Power Data Classiﬁcation: A Hybrid of a Novel
Local Time Warping and LSTM

Yuanlong Li, Member, IEEE, Han Hu, Member, IEEE, Yonggang Wen, Senior Member, IEEE, and Jun
Zhang Senior Member, IEEE

1

7
1
0
2

n
u
J

7

]
E
N
.
s
c
[

4
v
1
7
1
4
0
.
8
0
6
1
:
v
i
X
r
a

Abstract—In this paper, for the purpose of data centre energy
consumption monitoring and analysis, we propose to detect
the running programs in a server by classifying the observed
power consumption series. Time series classiﬁcation problem
has been extensively studied with various distance measurements
developed; also recently the deep learning based sequence models
have been proved to be promising. In this paper, we propose a
novel distance measurement and build a time series classiﬁcation
algorithm hybridizing nearest neighbour and long short term
memory (LSTM) neural network. More speciﬁcally, ﬁrst we
propose a new distance measurement termed as Local Time
Warping (LTW), which utilizes a user-speciﬁed set for local
warping, and is designed to be non-commutative and non-
dynamic programming. Second we hybridize the 1NN-LTW
and LSTM together. In particular, we combine the prediction
probability vector of 1NN-LTW and LSTM to determine the label
of the test cases. Finally, using the power consumption data from
a real data center, we show that the proposed LTW can improve
the classiﬁcation accuracy of DTW from about 84% to 90%. Our
experimental results prove that the proposed LTW is competitive
on our data set compared with existed DTW variants and its non-
commutative feature is indeed beneﬁcial. We also test a linear
version of LTW and it can signiﬁcantly outperform existed linear
runtime lower bound methods like LB Keogh. Furthermore, with
the hybrid algorithm, for the power series classiﬁcation task we
achieve an accuracy up to about 93%. Our research can inspire
more studies on time series distance measurement and the hybrid
of the deep learning models with other traditional models.

Index Terms—Time series classiﬁcation, time warping, recur-

rent neural network, long short term memory

I. INTRODUCTION

N OWADAYS, a growing number of data centres have been

built to support complicated computation and massive
storage required by various blooming applications [1]. Each
data center is typically equipped with hundreds of thousands
servers and requires many mega-watts electricity to power its
hosted servers and the auxiliary facilities [2]. An essential
problem is to monitor such a large amount of servers for
energy saving and maintaining the business continuity.

Monitoring technologies [3] can be divided into two cate-
gories: intrusive and non-intrusive. Intrusive technologies re-
quire the install of certain monitoring software which requires
the administration role of the system. Compared to the intru-
sive methods, non-intrusive methods are more ﬂexible, which
only require limited data for the monitoring and analysis.

Yuanlong Li, Han Hu and Yonggang Wen are with School of Computer
Engineering, Nanyang Technological University, Nanyang Avenue, Singapore
639798. Email: {liyuanl, hhu, ygwen}@ntu.edu.sg.

Jun Zhang is with School of Computer Science and Engineering,
jun-

South China University of Technology, Guangzhou, China. Email:
zhang@ieee.org.

In this paper, for the purpose of energy consumption mon-
itoring, we propose to detect the running program in a server
by analysing the observed power consumption series. The
power data can be measured without the administration right
of the server, which can be useful in collecting the power
related information of the servers for the purpose of energy
consumption analysis. The proposed classiﬁcation analysis can
only gain the type of the running program, avoiding any
possibility in accessing the privacy-related contents in the
server.

The proposed program detecting problem falls into the ﬁeld
of time series classiﬁcation. As a time series classiﬁcation
problem, the power data classiﬁcation problem can be chal-
lenging as the power series collected in detection may be only
a small piece of the whole power series of a program, with
incomplete and limited information. For this problem, the key
is to design an accurate and fast classiﬁcation algorithm.

Currently there are a few similar works on classifying
signals (like the power consumption signals studied here)
such as [4] [5] [6]. However, the technologies applied in
these literature are based on common spectral or statistical
features with classiﬁers such as nearest neighbour or neural
network. In a more general aspect, the time series classiﬁcation
problem has been extensively studied [7], among which the
most popular method is 1-nearest neighbour with dynamic
time warping (DTW). The major research line in time series
classiﬁcation has been the developing of various DTW based
distance measurements (variants such as [8] and enhancers
[9]); yet we ﬁnd that even though these measurements can be
better than the original DTW by certain degree for certain
cases, these variants all have been designed to incorporate
the dynamic programming idea of DTW (except some lower
bound methods like LB Keogh [10]) and all designed to be
commutative. Another line of research has also become no-
table recently, i.e. the long short term memory (LSTM) neural
network, which shows great modelling ability for sequential
data. In this work, we propose a novel classiﬁer with much
higher accuracy and based on the great efforts in the current
literature.

In this research, ﬁrstly, we propose a Local Time Warping
(LTW) time series distance measurement, which is a light
weight DTW variant that does not need the dynamic pro-
gramming procedure and is designed to be non-commutative.
LTW can be set to a linear runtime algorithm which can
perform almost as good as the DTW on our data set. Secondly,
instead of further enhancing the distance measurement which
can be much more complicated and time consuming, we

 
 
 
 
 
 
look into a less expensive solution, which is to develop a
hybrid algorithm of the 1-nearest neighbour with LTW (1NN-
LTW) and the recent deep learning model for time sequential
modelling. To do so, we ﬁrst utilize the state-of-art sequential
data modelling neural network LSTM [11] [12] to classify the
power series. Then we propose a new hybrid algorithm of the
proposed 1NN-LTW and the LSTM. Our study shows that both
1NN-LTW and LSTM can outperform the 1NN-DTW with
similar accuracy; however, these two algorithms have their
unique different natures and the accurately classiﬁed samples
of these two algorithms have signiﬁcant differences. The
hybrid algorithm of the two classiﬁers, termed as LSTM/LTW,
improves the classiﬁcation accuracy further easily.

The main contributions of this paper are summarized as

follows:

• We propose a new distance measurement LTW. LTW
has two unique features which are different from the
existing DTW variants: 1) LTW is based on simple
“local warping”, no dynamic programming procedure
is needed; 2) LTW is non-commutative and is ﬂexible
for the nearest neighbour classiﬁer for the time series
classiﬁcation problem. Our experimental results show that
for our problem, the proposed LTW can perform better
than DTW and its several different variants. Also our
experiment shows that the non-commutative feature of
LTW is beneﬁcial. Furthermore, the linear version of
LTW can perform almost as good as DTW on our data set.
These results show that for certain cases, a light weight
local warping distance measurement (such as the LTW)
may be good enough for the classiﬁcation task; however,
this does not mean that the proposed LTW can work for
all kinds of time series data sets.

• For the ﬁrst time, we develop a hybrid algorithm of 1NN-
DTW and LSTM termed as LSTM/LTW. The hybrid al-
gorithm is based on a well trained LSTM neural network.
Although the training procedure of the LSTM can be time
consuming, the classiﬁcation process can be fast in testing
with the LTW distance.

• Numerical experiments show that for the power data clas-
siﬁcation problem, with the LTW distance measurement,
the accuracy of the 1NN-LTW classiﬁer can be improved
from about 84% to about 90% compared to the 1NN-
DTW. With the hybrid algorithm LSTM/LTW, we achieve
the power consumption series classiﬁcation accuracy upto
about 93%, which proves that using the power consump-
tion series to detect the type of the running programs in
a server can be very accurate.

The remainder of this paper is organized as follows. In
Section II, we brieﬂy introduce the state-of-art time series
classiﬁcation algorithms. In Section III, we introduce the
experimental data collection design and some preliminary
analysis on the data. In Section IV, we introduce the new
proposed algorithm and in Section V we show the numerical
evaluation results and the analysis. In Section VI we conclude
the whole paper and introduce the future works.

2

Fig. 1. Illustration of the DTW distance measurement. In computing the DTW
distance between samples x and y, the DTW algorithm ﬁnds the best match
(shown by the dash lines) between the two series at different time steps.

II. RELATED WORKS

The power data classiﬁcation problem studied in this paper
can be taken as a time series classiﬁcation problem, which
has been studied extensively for the past decades. For this
problem, common classiﬁers like support vector machine
(SVM), k-nearest neighbour (KNN) with Euclidean distance
have been proved to be non-competitive to the DTW distance
measurement based method like 1NN-DTW [13]. Recently
there have been a lot of new methods which have been proved
to be as competitive as 1NN-DTW. On one hand, there are
many non-neural network based methods like Shapelet based
method, dictionary based methods, interval based methods and
ensembles of these methods. We will brief these methods
below. On the other hand, recently with the fast development
of deep learning [14], LSTM neural network has also been
proved to hold high modelling ability for sequential data. In
the following we will brieﬂy introduce LSTM.

A. Non-Neural Network Approaches

The most popular non-neural network time series classiﬁers
are nearest neighbour based method with various different
distance measurements. The most popular method is the 1NN-
DTW, which is a special k-nearest neighbour classiﬁer with
k = 1 and a special DTW distance measurement. For the 1-
NN classiﬁer, the common standard procedure to label of a
test sample given a set of training samples is as follows. First
the distances of the test sample to all the training samples
are computed; then the training sample that has the smallest
distance to the test sample is chosen and its label is assigned
to the test sample as the classiﬁcation result. In the above
procedure, the key is to utilize a proper distance measurement.
For 1NN-DTW, the DTW distance is used, which has superior
performance for time series data.

The DTW calculates the distance of two sequences x and y
in a manner of ﬁnding the best match between them, as shown
in Fig. 1. The idea is that sequential data often contain similar
ﬂuctuation patterns, however, a same pattern, when existed in
different sequences as sub-sequences, may be stretched, shrank
or delayed in the time axis. In this case, the DTW distance
measurement aims to warp the time axis non-linearly and ﬁnds
the best match between the two samples such that when a same
pattern exists in both sequences, the distance is smaller.

Mathematically,

the DTW distance is computed by the
following dynamic programming process. Denote D(i, j) as
the DTW distance between sub-sequences x[1 : j] and y[1 : j],
then the DTW distance between x and y can be computed by

(cid:1)(cid:1)(cid:2)(cid:1)3

the dynamic programming process with the following iterative
equation:

D(i, j) = min{D(i−1, j−1), D(i−1, j), D(i, j−1)}+|xi−yj|.

TABLE I
COLLECTED POWER SEQUENCES OF DIFFERENT PROGRAMS. MAPREDUCE
AND WEB SERVER ARE THE TWO MAJOR CLASSES, WHILE IN MAPREDUCE
THERE ARE MANY SUBCLASSES. IN TOTAL THERE ARE 13 CLASSES.

(1)
The time complexity to compute the DTW distance is O(nm),
where n and m are the length of x and y respectively. The
DTW distance measurement actually re-align the time step
index pairs in the computing of the distance. In practice,
usually a threshold w is used to restrict
the index offset
in the alignment, which can be critical to the classiﬁcation
results [15]. Also there are many study [16] working on
accelerating the computing speed of DTW, which results in
the fast DTW that can be computed in linear time of the
length of the sequences. In this paper, we follows the idea of
DTW but propose a new distance measurement, which can be
computed with a local warping index set without a dynamic
programming process and has a special non-communicative
nature that can be helpful.

There are many DTW variants proposed. We name only a
few here for the space constraint; one can refer to [7] for a
more complete review and comparison of the existing methods.
Move-Split-Merge [8] introduces move and split operation in
dynamic warping. Complexity Invariant distance (CID) [9] is
a weight modiﬁer which can be used to enhance any kind of
distance measurement, and is proved to be very useful when
using with DTW.

Besides DTW based method, there have been many new
different methods which look into the pattern of the time
series for classiﬁcation. For example, Shapelet [17] based
methods utilize the subsequences that can differentiate differ-
ent classes to do the classiﬁcation. Dictionary based methods
[18] transforms the series into discrete words in a dictionary
and then do the classiﬁcation. Interval Based Classiﬁers [19]
try to extract the feature from intervals in each time series
for the classiﬁcation. In this paper, we will focus on DTW
based methods and will not compare with these methods,
as proved in [7], these methods perform similar with DTW
based methods unless they are ensemble methods. Ensemble
methods are the kind of classiﬁer that combine multiple simple
classiﬁers which can be better than any single classiﬁer.
Currently, the existed ensemble methods are mainly based the
above listed classiﬁers, and we have not seen any work on
ensemble method of the above methods and neural network. In
this paper, we will thus propose a simple hybrid algorithm of
a nearest neighbour classiﬁer and neural network. The neural
network classiﬁer used in this paper is introduced below.

B. LSTM

LSTM is ﬁrst proposed by Hochreiter and Gers et al. as
an upgrade of the recurrent neural network (RNN) [20]. RNN
is used to handle sequential data with a special calculation
process following the time step increment, while traditional
neural network simply treats the sequence as a plain vector.
With such nature, RNN is suitable for modelling sequential
data. However, it suffers from a problem called diminishing
gradient, which is caused by the iterative process on the time
axis and makes the gradient used in the training process

Program

Word Count
Sorting
PI

Spark

MapReduce

MLlib

CrossValidator
Kmean
LR
SVM
Cosine similarity
PCA

Hadoop

Word Count
Sorting
PI

Web server data

Number of sequences
100
100
100
40
40
40
40
40
40
100
100
100
40

Class label
0
1
2
3
4
5
6
7
8
9
10
11
12

extremely small and causes training failure. To solve the
problem, the LSTM is proposed and it utilizes a memory core
to avoid the diminishing gradient. The details of the LSTM
neural network will be introduced in Section IV.

LSTM has shown great modelling power for sequential
data and has been successfully applied in various machine
learning ﬁelds like natural language process (NLP) [21], video
analysis [22] and etc. It is also noted that LSTM can be both
discriminative and generative. By discriminative, LSTM can
be used for classiﬁcation tasks while by generative, LSTM
can be used to generate similar sequences like the training
samples [23]. In this paper, we utilize the discriminative ability
of LSTM for our power data classiﬁcation task.

III. POWER SERIES DATA COLLECTION AND
PRELIMINARY ANALYSIS
In this section we present the power series data we collected
followed by some preliminary analysis on the data. We will
detail the simulation design rules for the data collection and the
data samples collected with some pretreatment. The proposed
preliminary analysis includes data visualization with different
dimension reduction methods, classiﬁcation results with some
canonical classiﬁers, and feature study.

A. Power Series Data Collection

We ﬁrst introduce the designing rules of the simulation
for data set collection. As a data-driven study on the power
series classiﬁcation methodology, we need to collect a set of
sample power data. The data collection should be designed
carefully to make sure that
the classiﬁcation problem is
neither trivial nor impossible to accomplish. In this sense, our
guiding line for power data collection is to collect “different”
and “similar” power series: By “different”, the power series
must be generated by different programs. By “similar”, the
different programs can have some similar features so that the
classiﬁcation algorithms need to be really discriminative.

Follow the above guideline, we collected in total 13 classes
of power data as shown in Table I (for convenience they are
labelled as 0,1,...,12 respectively). These data fall into two
major categories:

• Web server power data: usually ﬂuctuate in a continuous

pattern;

• Spark/Hadoop MapReduce programs: usually show stage-

pattern, e.g. the Map stage and the Reduce stage.

For the Hadoop/Spark category, we test different programs on
these platforms, some are the same for both platforms, such as
the “Word Count” program; some only exist on one platform,
for example, the “MLlib” programs on the Spark platform.
With such design, we can achieve the proposed “different”
and “similar” design goal.

Note that the collected data series are of different lengths
as the running duration can vary among different programs.
Although classiﬁcation methods like 1NN-DTW can deal with
power series of different lengths, to apply other canonical
methods, in the following we cut the collected series into ﬁxed
length sequences. It is also reasonable to label sub-sequences
instead of the complete power sequences of the programs as in
a blind test, we have no information about the start/end point
of a program. The detailed cutting method we utilize here is
as shown below.

The goal is to cut the power sequences into length 200
samples. To do so, ﬁrst we discard sequences with length
smaller than 200 time slots (time unit: 3 seconds). The left
number of sequences for each class is: [77, 31, 30, 35, 28,
7, 40, 14, 5, 100, 58, 36, 40]. Although some power data are
discarded, the total duration of the left sequences is about 199
hours and with the time unit being 3 seconds, the amount of
the data are still adequate for the study. Then these sequences
are further cut into length 200 sub-sequences in the following
way: For each sequence q of length n, we cut it into multiple
sequences q[0 : 200], q[50 : 250], .... , q[(n − 200) : n]. We
obtain 3200 test sequences in this cutting procedure, which are
used as the power data in our classiﬁcation study. Note that
these sequences are overlapped, as indicated by the cutting
method.

Furthermore, for the purpose of multi-fold tests, we divide
these samples into ﬁve folds F0-F4. Note that to avoid the
overlapping of the training data and the test data, the fold
partition is done before the sequence cutting. For each fold of
test, we use Fi,F(i+1)%5 as the test data, and the left folds as
the training data.

B. Preliminary Analysis

We do some preliminary analysis on the pretreated data. The
following analysis are meant to provide a basic understanding
of the power data in view of classiﬁcation.

1) Basic Characteristic Analysis Based on Visualization:
We use various dimension reduction methods to visualize the
data, which can help to identify if the power series can be
successfully classiﬁed to a certain degree. We utilize eight
different dimension reduction methods with scikit-learn [24]
and project the original ﬁxed length power sequences into
a 2-dimensional space. These dimension reduction methods
are PCA, LDA, LLE, modiﬁed LLE, Isomap, MDS, Spectral
Embedding and t-SNE, which are widely adopted dimension
reduction methods. The 2-dimensional codes of the power
data generated by these methods are shown in Fig. 2. We use
different colors to show samples from different classes.

4

however, it still shows that the power series classiﬁcation task
cannot be easily done.

2) Tackling the Classiﬁcation Problem with the Canonical
Classiﬁers: We test some canonical classiﬁers to tackle the
power series classiﬁcation problem. The canonical classiﬁers
tested here are listed as follows: Nearest Neighbours, Linear
SVM, RBF SVM, Decision Tree, Random Forest, AdaBoost,
Naive Bayes, LDA and QDA [24] . Parameter settings for
these classiﬁers are tuned manually. The classiﬁcation results
of these methods are shown in Table II.

From the results we can observe that for a 13-classes
classiﬁcation problem, the highest accuracy achieved by these
methods are about 60% (by Random forest). The classiﬁcation
accuracy is not promising (when compared to the 1NN-DTW
shown below), which actually proves that our power series
labelling problem is a typical time series classiﬁcation prob-
lem, as stated in [13], for such problem, canonical Euclidean
distance metric based classiﬁers cannot achieve good results
usually.

3) Feature Based Classiﬁcation Study: In general, as a sig-
nal classiﬁcation problem, the power series labelling problem
can be solved by ﬁrst extracting certain features from the raw
power series and then carry out the classiﬁcation with these
features. In this subsection, we study such possibility and test
power series classiﬁcation with the DFT [25] feature of the
original power sequences. With DFT, each power sequence
can be transformed into the spectrum space resulting a new
representation. The spectrum representation can be aligned as
a vector as the input to the classiﬁers. We test the classiﬁcation
result of 1NN-DTW with the raw data compared to with the
DFT feature. The classiﬁcation results are shown in Table III.
Note that for the 1NN-DTW, the maximum offset r is set to
15% of the sample length, which is manually tuned in the
experiment.

From Table III we can observe that the DFT features are not
helping. The reason is that classiﬁcation with the original data
can maximize the information used in classiﬁcation, while the
DFT feature is less informative.

To summary, we ﬁnd that the power series classiﬁcation
problem is not easy to tackle especially with the canonical
classiﬁers and with some common used features. In the fol-
lowing, we will propose a new distance measurement inspired
from DTW and combine it with the state-of-art sequence
modelling neural network LSTM.

IV. THE PROPOSED POWER SERIES CLASSIFICATION
ALGORITHM

In this section we present the proposed new power series
classiﬁcation algorithm which hybridizes a nearest neighbour
classiﬁer with a novel distance measurement and a LSTM clas-
siﬁer. In the following we ﬁrst introduce the two components
respectively and then present the hybrid algorithm.

A. Nearest Neighbour with the Local Time Warping (LTW)

From the Fig. 2 we can observe that the power series data
are not easy to distinguish after the dimension reduction. This
may be due to the short length (2 here) of the embedding code;

We propose a new classiﬁer which utilize a novel distance
measurement to compute the distance between two sequences
which we termed as Local Time Warping (LTW) as its warping

5

Fig. 2. Projection results with different manifold learning methods. The power series samples (200 dimensional data) are projected into 2-dimensional space
for visualization. The results show that the power series are difﬁcult to discriminate.

TABLE II
THE CLASSIFICATION RESULTS OF THE CANONICAL CLASSIFIERS. THESE CLASSIFIERS CAN ACHIEVE ACCURACY UP TO ABOUT 60%.

Test case
Fold 0
Fold 1
Fold 2
Fold 3
Fold 4

Nearest Neighbours
0.4346
0.4427
0.4420
0.4475
0.4673

Linear SVM RBF SVM Decision Tree
0.3175
0.3063
0.3283
0.3094
0.3301

0.5187
0.5105
0.5071
0.5004
0.5383

0.5747
0.5607
0.5901
0.5835
0.5610

Random Forest
0.5891
0.6050
0.6133
0.6378
0.6174

AdaBoost
0.4593
0.4192
0.4121
0.3813
0.3834

Naive Bayes
0.4406
0.4360
0.4016
0.4123
0.4278

LDA
0.4380
0.4377
0.4256
0.3707
0.4407

QDA
0.4092
0.4276
0.4248
0.4574
0.4786

TABLE III
CLASSIFICATION RESULTS OF THE 1NN-DTW WITH THE ORIGINAL
SERIES AND WITH THE DFT FEATURE.

Test case

Fold 0
Fold 1
Fold 2
Fold 3
Fold 4

1NN-DTW with
original series
0.8548
0.8393
0.8369
0.8329
0.8514

1NN-DTW with
DFT feature
0.7122
0.7029
0.6761
0.6998
0.6885

computation for each time step is done in a local window
without a dynamic programming procedure like DTW. The
LTW is developed to replace the DTW distance measurement
in the 1NN-DTW classiﬁer.

The idea behind LTW is as follows. Comparing the al-
gorithms of DTW and the Euclidean distance,
the major
difference in between is that there are a lot of “min” operators
in DTW. Such “min” operator actually is the key to the “warp-
ing” map between the two time series. Despite the warping
operation, DTW utilizes a dynamic programming procedure to
optimize the mapping. Note that dynamic programming is slow
and it is not directly optimizing the classiﬁcation accuracy. In
this case, what if we do not use the dynamic programming
procedure? We may try some low cost warping operations; is
it possible that such a distance measurement can be as good
as DTW? Here we propose the LTW to answer this question.
Also, note that the DTW is computed by a beautiful symmetric
formula which makes it commutative for the two time series in

computing the distance. What if we do not need the distance
measurement to be commutative? Can it be better with the
non-commutative feature? Our proposed LTW will also answer
this problem. The detailed design is shown below.

The LTW distance measurement is computed in the follow-
ing way. Suppose we have two sequences x and y, both of
length n. We deﬁne the LTW distance between x and y as:

dLT W
k

(x, y) =

n−k
(cid:88)

i=k+1

min(|xi − yi|, |xi − yi+1|, |xi − yi+k|),

LT WG(x, y) =

(cid:88)

k∈G

dLT W
k

(x, y).

(2)
(3)

As shown in (3), LTW works in the following manner. In
computing the distance between x and y (when we want to
ﬁnd a nearest neighbour of x), we set x as the base sequence
and test the similarity of y to x in the following way: with
a warping index set G, for k ∈ G, for time step i in x, we
compute the minimum absolute distance between xi and one
of yi, yi+1, yi+k; then we add these distance measures for
i = k + 1, ..., n − k and for k ∈ G up, which is the LTW
distance from y to x with warping index G. Note that (2) is a
linear algorithm (only three items to compare no matter how
large k is).

Note that the LT WG(x, y) distance is non-commutative,
which means that LT WG(x, y) (cid:54)= LT WG(y, x) can be true.
We use LT WG(x, y) to compute the nearest neighbour of
to ﬁnd the best match of x
sequence x,

in a sense that

PCALDALLEModified LLE0123456789101112IsomapMDSSpectralEmbeddingt-SNE6

With the node state, to further compute the output, an
output gate is ﬁrstly computed as:

ot = σ(Woxt + Uoht−1 + bo).

(8)

Finally the output of a LSTM node t is computed as:

ht = ot · tanh(Ct).

(9)

The output of all LSTM nodes are then added together
as the output of the LSTM layer:

h =

n−1
(cid:88)

t=0

ht.

(10)

• Logistic regression layer: In this layer the output of the
LSTM layer is used to compute the label of the test
sample in the following way. First we use the sof tmax
[27] function to compute the probability vector P with its
each entry representing the probability of the test sample
belonging to a class:

P = sof tmax(W h + b).

(11)

Then the prediction ypred is the class which achieves the
largest probability:

ypred = argmaxi(P).

(12)

To train the LSTM classiﬁer, the loss function is deﬁned
as the negative log-likelihood function with the label of the
training data y:

− L(θ, D) = −

|D|−1
(cid:88)

i=0

log(Py(i)|x(i), θ)),

(13)

where θ is the set of all the weight and bias parameters in
the LSTM neural network (which are adjusted in the training
process); D is a batch of training samples. Size of D can be
important for the performance of of the classiﬁer.

C. Hybridization of LSTM and 1NN-LTW

In this subsection we propose to combine the 1NN-LTW
classiﬁer and the LSTM classiﬁer. The underlying rational is
that both classiﬁer can achieve high classiﬁcation accuracy for
our problem but in very different manners: the 1NN-LTW is
a nearest neighbour classiﬁer, which is a data-based classiﬁer
without a training process; while LSTM is a training based
classiﬁer in which the training data are ﬁrstly used to build
a model and then the model is used to classify the test data.
In our experiments, both classiﬁers can perform promisingly
individually; however, our numerical simulation shows that
the accurately classiﬁed samples by the two classiﬁers have
signiﬁcant differences. In such sense, we propose to combine
the two algorithms to construct a even stronger classiﬁer.

The hybrid algorithm is designed in the following way.
Considering that in practice, the training of LSTM and the
computing of the distance matrix for 1NN-LTW can be both
time consuming, the hybrid algorithm is designed to be as
simple as possible. We ﬁrst obtain the two individual classiﬁers
CLT W (the nearest neighbour classiﬁer with CID enhanced
LTW) and CLST M (the trained LSTM classiﬁer). For each

Fig. 3. Illustration of the LSTM neural network. It contains three layers: the
input layer, the LSTM layer and the logistic regression layer. The output is a
13-dimensional vector which denotes the probability of the sample belonging
to each class.

among the other samples such as y. For comparison, the DTW
distance is obviously commutative. The non-commutative fea-
ture of LTW can be beneﬁcial, as our target is to ﬁnd the
nearest neighbour for each x. A non-commutative distance
measurement is enough to serve the purpose and can provide
more ﬂexibility by enforcing less constraints to the distance
measurement.

B. Long Short Term Memory Neural Network

We utilize the LSTM classiﬁer following [26] for our power
series classiﬁcation problem. The LSTM neural network con-
sists of an input layer, a LSTM layer and a logistic regression
layer as depicted in Fig. 3. The three layers function in the
following way respectively:

• Input layer: The input data sample, which is a length n
vector x, is ﬁrstly discretized into range [0, S]. Such an
operation is a smoothing operation to the original power
series, which can affect the performance of the LSTM.
Then each time step xt, t = 0, ..., n − 1 is enriched into
a m-dimensional vector xt which can ease the following
computation, i.e. xt = xt · e, where e is a m-dimensional
vector with all entries equal to 1. After the above process,
the new sequence x0, x1, ..., xn−1 is used as the input to
the LSTM layer.

• LSTM layer: The LSTM layer contains n LSTM node,
where each LSTM node t can output an m dimensional
code ht. The operation inside the LSTM node is shown
below. First, for each time step xt, the LSTM node needs
to compute a new state denoted by Ct. To compute Ct,
a candidate state C(cid:48)

t is ﬁrstly computed as:

C(cid:48)

t = tanh(Wcxt + Ucht−1 + bc).

(4)

Then two gates, an input gate it and a forget gate ft are
computed to update the new state:

it = σ(Wixt + Uiht−1 + bi).

ft = σ(Wf xt + Uf ht−1 + bf ).

(5)

(6)

Then the new state of the LSTM node is computed as:

Ct = it · Ct + ft · C(cid:48)
t.

(7)

(cid:1)(cid:2)(cid:3)(cid:4)(cid:5)(cid:6)(cid:7)(cid:8)(cid:9)(cid:10)(cid:8)(cid:7)(cid:11)(cid:6)(cid:10)(cid:2)(cid:6)(cid:8)(cid:12)(cid:13)(cid:14)(cid:6)(cid:5)(cid:10)(cid:15)(cid:8)(cid:6)(cid:7)(cid:5)(cid:8)(cid:3)(cid:16)(cid:5)(cid:14)(cid:8)(cid:9)(cid:8)(cid:6)(cid:10)(cid:7)(cid:6)(cid:12)(cid:6)(cid:17)(cid:8)(cid:13)(cid:5)(cid:18)(cid:9)(cid:6)(cid:1)(cid:1)(cid:19)(cid:20)(cid:21)(cid:22)(cid:6)(cid:8)(cid:2)(cid:13)(cid:18)(cid:23)(cid:10)(cid:2)(cid:24)(cid:11)(cid:6)(cid:25)(cid:12)(cid:13)(cid:14)(cid:6)(cid:2)(cid:1)(cid:6)(cid:10)(cid:7)(cid:6)(cid:8)(cid:2)(cid:13)(cid:18)(cid:23)(cid:8)(cid:10)(cid:2)(cid:5)(cid:18)(cid:6)(cid:12)(cid:6)(cid:3)(cid:26)(cid:27)(cid:6)(cid:17)(cid:8)(cid:13)(cid:5)(cid:18)(cid:9)(cid:16)(cid:6)(cid:2)(cid:1)(cid:4)(cid:6)(cid:28)(cid:17)(cid:8)(cid:9)(cid:12)(cid:24)(cid:8)(cid:6)(cid:18)(cid:17)(cid:8)(cid:9)(cid:6)(cid:23)(cid:10)(cid:29)(cid:29)(cid:8)(cid:9)(cid:8)(cid:2)(cid:5)(cid:6)(cid:5)(cid:10)(cid:15)(cid:8)(cid:6)(cid:7)(cid:5)(cid:8)(cid:3)(cid:7)(cid:11)(cid:6)(cid:6)(cid:6)(cid:6)(cid:6)(cid:30)(cid:1)(cid:2)(cid:1)(cid:31) (cid:5)(cid:1)!(cid:1)"(cid:1)#(cid:1)(cid:5)(cid:6)"(cid:19)(cid:18)(cid:24)(cid:10)(cid:7)(cid:5)(cid:10)(cid:13)(cid:6)(cid:9)(cid:8)(cid:24)(cid:9)(cid:8)(cid:7)(cid:7)(cid:10)(cid:18)(cid:2)(cid:6)$(cid:12)%(cid:8)(cid:9)(cid:6)&(cid:10)(cid:5)(cid:14)(cid:6)(cid:12)(cid:6)"’(cid:26)(cid:23)(cid:10)(cid:15)(cid:8)(cid:2)(cid:5)(cid:10)(cid:18)(cid:2)(cid:12)$(cid:6)(cid:18)(cid:4)(cid:5)(cid:3)(cid:4)(cid:5)()(cid:4)(cid:5)(cid:3)(cid:4)(cid:5)(cid:20)(cid:18)(cid:29)(cid:5)(cid:15)(cid:12)*(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)classiﬁer, we obtain the probability vector when classifying
some test time series x: pLT W (x) and pLST M (x), where
pLT W (x) is deﬁned as:

pLT W (x) =

(cid:80)m

i=1(m − i)vi
∆

,

(14)

where vi is a all zero vector except its value at the index of
the class of the ith neighbour obtained from CLT W is to be
1. ∆ is a normalization vector which is used to make sure that
the summation of the obtained probability vector equal to 1.
pLST M (x) is obtained equation (11).

With this two probability vector, we simply add them up
and the test series will be classiﬁed to be the class index with
the maximum probability, i.e.:

CLST M/LT W (x) = argmax(pLT W (x) + pLST M (x)). (15)

The detailed algorithm is shown in Algorithm 1.

Algorithm 1 Hybridization of 1NN-LTW and LSTM

1: Individual training process: build classiﬁer CLT W and

CLST M . Train CLST M with the training data.

2: For test time series x, compute the probability vector

pLT W (x) and pLST M (x).

3: Classify the test time series x according to (15) and get

the label lhybrid(x).

4: Output: lhybrid(x), as the predicted label for sample x.

V. NUMERICAL EVALUATION AND ANALYSIS

In this section we present the experimental results of the
above proposed algorithms and the analysis. We ﬁrst conduct
experiments to investigate the proposed LTW and compraed
it with various variants of DTW. Then we compare the
classiﬁcation accuracy of the proposed 1NN-LTW, LSTM
and their hybrid algorithm LSTM/LTW with the baseline
algorithm 1NN-DTW. For the base line algorithm 1NN-DTW,
the maximum warping offset w is manually tuned and set to
be 30. For the 1NN-LTW, the warping index set G of the LTW
distance is set to {1, 2, ..., 10}, and we will show analysis on
the affects of the set G. For the LSTM neural network, we
set the maximum number of epoch to be 50. For some key
parameters which can affect the performance of LSTM ,we
give a detailed discussion in the following parameter settings
study. Test data and codes for the LTW tests are available at
https://www.dropbox.com/s/lylsece6xysayw8/DataAndCode.zi
for
p?dl=0.
convenience, we will simply use test Fold i
to denote
the test with test samples in Fi and F(i+1)%5.

classiﬁcation

presenting

results,

the

In

A. Experimental Study on LTW

In this subsection, we conduct experiments to prove that
the proposed LTW is indeed a different distance measurement
from the existed DTW variants, and prove that
it works
better or nearly the same(for the linear version) to DTW
and its various state-of-art variants. We will also prove that
the non-communicative feature is indeed beneﬁcial. Note that
we will not try to use massive experimental data to prove

7

that the proposed LTW is superior than other DTW variants,
which is indeed not true not only because of the No Free
Launch theory, but also because these distance measurements
are mostly designed in a way without a training objective
to directly optimize the classiﬁcation accuracy: for example,
in DTW,
the dynamic programming process can optimize
the match; however, such optimization goal is different from
the classiﬁcation accuracy. In such sense, all these distance
measurements can suffer from model bias and when they are
applied to different data sets, their performance will deﬁnitely
vary. As proved in [7], only ensemble based methods can
signiﬁcantly outperform 1NN-DTW by more than 3%. In this
section, we will only compare LTW with the DTW with Man-
hattan distance (DTWm), DTW with Euclidean distance(we
will use this as the default DTW in this paper, as it has better
performance), and DTW variants MSM, LB Keogh, and the
enhancer CID as these methods performs good in general as
shown in [7]. Details are shown below.

First, we present

the comparison of 1NN-LTW (G =
{1, 2..., 10}) with 1NN-DTWm (warping window w = 30),
1NN-DTW (warping window w = 30), MSM (with its
threshold parameter c = 1.0). The results on our data set
are shown in Table IV. Parameter settings for 1NN-DTW and
MSM are tuned by manually. Clearly the performance of 1NN-
LTW is better.

Second, we compare the fast linear LTW with G = {10}
with the linear runtime lower bound method LB Keogh. We
test LB Keogh with window size w = 5 and w = 10
respectively. The results are shown in Table V and LTW with
G = {10} outperforms LB Keogh signiﬁcantly on our data
set. This proves that the fast linear version of LTW can still
perform quite good.

Third, we present the classiﬁcation with the CID enhanced
distance measurement. We show the result of CID(DTW) and
CID(LTW) with G = {1, 2..., 10} and G = {10} in Table
VI. Clearly, the CID can improve the performance of both
DTW and LTW for our data set. With CID modiﬁer, the LTW
is still slightly better than DTW; however, the advantage of
CID(LTW) over CID(DTW) becomes smaller, we believe that
it is reasonable as the accuracy is upper bounded and it will
be much more difﬁcult to further improve the accuracy when
the algorithm is already very accurate.

At last, we present the experimental results to show that
the non-commutative feature of LTW is indeed beneﬁcial. To
do so, we implement a simple commutative version of LTW
deﬁned as:

LT W com(x, y) = LT W (x, y) + LT W (y, x).

(16)

The experimental results compare the LTW with the LT W com
is shown in Table VII. Clearly, LTW outperforms its com-
mutative version signiﬁcantly. This proves that
the non-
commutative feature of LTW is indeed beneﬁcial.

B. The Classiﬁcation Accuracy Rate Comparison

The ﬁve-fold classiﬁcation accuracy results for the hybrid
algorithm LSTM/LTW are shown in Table VIII,compared with
the non-hybrid classiﬁers. Results of LSTM and LSTM/LTW
are based on 10 independent runs.

TABLE IV
CLASSIFICATION ACCURACY OF 1NN-LTW (G = {1, 2..., 10}) WITH
1NN-DTWM (w = 30), 1NN-DTW (w = 30), MSM (c = 1.0).

Test case
Fold 0
Fold 1
Fold 2
Fold 3
Fold 4

1NN-LTW({1,...,10})
0.8862
0.8854
0.8661
0.8809
0.8894

1NN-DTWm 1NN-DTW MSM
0.8294
0.8310
0.8699
0.8682
0.8515

0.7988
0.7958
0.8220
0.8118
0.8168

0.8480
0.8351
0.8347
0.8379
0.8442

TABLE V
COMPARISON OF 1NN-LTW (G = {10}) WITH 1NN-LB KEOGH
(w = 5, 10).

Fold 0
Fold 1
Fold 2
Fold 3
Fold 4

1NN-LTW(G = {10})
0.8829
0.8762
0.8444
0.8696
0.8838

LB Keogh (w = 5)
0.5866
0.5791
0.6081
0.6018
0.6053

LB Keogh (w = 10)
0.4542
0.4753
0.4592
0.4616
0.4326

From Table VIII we can observe that:

• The proposed LSTM classiﬁer shows similar accuracy
compared to 1NN-LTW and it also outperforms 1NN-
DTW on our data set.

• The hybrid algorithm LSTM/LTW can achieve higher
accuracy compared to 1NN-LTW and LSTM by an incre-
ment of about 3%, which proves that the hybrid algorithm
can indeed improve the classiﬁcation accuracy.

For the ﬁrst observation, we can see that LSTM, as a neural
network, can signiﬁcantly outperform the other canonical
classiﬁers like SVM, which proves its strong modelling ability
for sequential data. Note that a common neural network like
multilayer perceptron (MLP) can not perform as good as
LSTM. The performance of LSTM can be seriously affected
by the training settings, which we will discuss below.

For the second observation, we can see that the improvement
is small, which is reasonable as the baseline algorithms already
achieve a high accuracy individually, making it difﬁcult to
achieve large improvement for the hybrid algorithm. The
improvement caused by the hybrid algorithm will be shown
clearly in the following detailed analysis.

TABLE VI
CLASSIFICATION RESULTS WITH CID ENHANCED DISTANCE
MEASUREMENT.

Fold 0
Fold 1
Fold 2
Fold 3
Fold 4

1NN-CID(DTW)
0.8829
0.8854
0.8833
0.8950
0.8999

1NN-CID(LTW{1,...,10})
0.9041
0.8904
0.8684
0.9013
0.9031

1NN-CID(LTW{10})
0.8837
0.8912
0.8579
0.8887
0.8846

TABLE VII
COMPARISON OF LTW AND THE LT W com

1NN-LTW 1NN-LT W com

Fold 0
Fold 1
Fold 2
Fold 3
Fold 4

0.8862
0.8853
0.8661
0.8809
0.8894

0.7385
0.7029
0.7218
0.7294
0.7692

8

TABLE VIII
CLASSIFICATION ACCURACY OF 1NN-LTW,LSTM AND THE HYBRID
ALGORITHM LSTM/LTW COMPARED TO THE BASELINE ALGORITHM
1NN-DTW.

Test case
Fold 0
Fold 1
Fold 2
Fold 3
Fold 4

1NN-DTW 1NN-CID(LTW)

0.8548
0.8393
0.8369
0.8329
0.8514

0.9040
0.8903
0.8683
0.9013
0.9031

LSTM
0.8772±0.0153
0.8780±0.0130
0.8547±0.0171
0.8772±0.0077
0.8778±0.0061

LSTM/LTW
0.9267±0.0030
0.9115±0.0046
0.8923±0.0038
0.9167±0.0021
0.9256±0.0051

C. Analysis on the Accurately Classiﬁed Samples

In this subsection we analyse the accurately classiﬁed
samples of the power series and study the difference between
different classiﬁers. In doing so we will be able to identify
why and how the hybrid algorithm works.

Fig. 4 shows the predicted labels for the test samples in
Fold 0 of the 1NN-DTW, 1NN-LTW, LSTM and the hybrid
algorithm LSTM/LTW. Fig. 5 shows the accurately classiﬁed
samples for each class and for each algorithm. From Fig. 4
and 5 we can observe that:

• The proposed 1NN-LTW method performs similarly to
1NN-DTW, although 1NN-LTW can accurately predict
more test samples. This is reasonable as the two classi-
ﬁers are both nearest-neighbour classiﬁers and they have
similar measurement deﬁnition.

• The proposed LSTM classiﬁer shows certain degree of
difference compared to the other two nearest neighbour
the LSTM classiﬁer cannot
classiﬁers. One example,
predict any test samples from the Spark-MLlib-LR (class
label 5) and the Spark-MLlib-PCA (class label 8) classes,
while both 1NN-DTW and 1NN-LTW can; however,
LSTM performs better than the other algorithms on
classes Spark-MLlib-SVM (class label 6) and Hadoop-
WordCount (class label 9).

• The proposed LSTM/LTW classiﬁer can successfully
combine the advantages of LSTM and 1NN-LTW. Such
as for the Spark-MLlib-LR class and Hadoop-WordCount
classes, the hybrid algorithm achieve similar performance
to the better one of LSTM and 1NN-LTW.

• All the classiﬁers can successfully classify the test sam-
ples of the web server class, which is reasonable as the
web server program is of a completely different kind from
the other MapReduce programs.

The above results show the difference of the 1NN-LTW
and the LSTM classiﬁer which makes the hybrid algorithm
work. Although 1NN-LTW and the LSTM can achieve similar
accuracy, their accurately classiﬁed samples have signiﬁcant
differences. To make this more clearly, we compute the union-
accuracy accunion of the two classiﬁers as follows:

accunion =

|ALST M ∪ A1N N −LT W |
N

,

(17)

where ALST M and A1N N −LT W are the sets of the accurately
classiﬁed samples by LSTM and 1NN-LTW respectively; N
is the total number of test samples in this test fold. The union-
accuracy of the ﬁve fold tests are shown in Table IX. It can be
seen that the union-accuracy is between 94%-96%. It shows
the potentiality of a hybrid algorithm of the two classiﬁers.

9

Fig. 4. Classiﬁcation response for the different algorithms on Fold 0. The ground truth/predicted labels of the test samples are plotted against the index of
the sample. Clearly the hybrid algorithm gains the advantage of both LSTM and 1NN-LTW.

TABLE IX
THE UNION-ACCURACY OF THE 1NN-LTW AND THE LSTM CLASSIFIERS
IN THE FIVE FOLD TESTS.

Test case
Fold 0
Fold 1
Fold 2
Fold 3
Fold 4

accunion
0.9482
0.9489
0.9468
0.9541
0.9612

TABLE X
TEST RESULTS WITH DIFFERENT SETTINGS FOR THE WARPING INDEX SET
G IN 1NN-LTW

G = {1} G = {1, .., 4} G = {1, ..., 8} G = {1, ..., 12}

Fold 0
Fold 1
Fold 2
Fold 3
Fold 4

0.8166
0.8075
0.8138
0.8132
0.8103

0.8591
0.8552
0.8601
0.8668
0.8571

0.8727
0.8728
0.8684
0.8760
0.8878

0.8846
0.8904
0.8661
0.8802
0.8902

Note that the hybrid algorithm can only achieve accuracy
smaller than the union-accuracy, as the union-accuracy is
computed in an ideal manner.

D. Discussion on the Parameter Settings

In this subsection we discuss the parameters settings in the
above algorithms. First we study the parameter used in the
LTW measurement, the warping index set G. The test results
with different G settings are shown in Table X. It can be seen
that a proper G setting is needed as a set G too small can
deteriorate the performance. In our experiments we ﬁnd that
with a larger set G, the performance of LTW is more stable.
Note that increasing the size of G can cause higher computing
cost.

work is critical. In our experiments, we ﬁnd that an improper
setting can result a bad performance with accuracy lower than
50% for the LSTM. We ﬁnd the following key settings in the
LSTM classiﬁer, which we have tested and ﬁnd the proper set-
ting, although detailed experimental results are omitted here.
The hyper-parameter settings: three parameters are specially
tuned in our experiments, which are the batch size (we set
to 60), the dimension of the LSTM node (we set to 90), and
the discretized range parameter S (we set to 100). We also
tested two more different implementation variations of LSTM:
1)Adding a dropout layer, which is tested and not helpful in
our case. 2) More than one LSTM layers, which has been
tested and is also not helpful.

VI. CONCLUSION AND FUTURE WORKS
In this research, we study the server power consumption
series classiﬁcation problem used as a non-intrusive method
for data centre energy monitoring. First we propose a new
time series distance measurement termed as local time warping
(LTW) and build a hybrid algorithm of the 1-nearest neighbour
with LTW and the LSTM neural network. The proposed LTW
distance measurement is designed to be a light weight time
series measurement with local warping operations within a
predeﬁned warping index set, and it is designed to be non-
commutative. LTW can be taken as the simpliﬁed version
of DTW with only the warping operation (a series of “min”
operations). The LTW is proved to be better than DTW on
our data set and its non-commutative feature is proved to be
beneﬁcial. Also a linear version of LTW can perform almost
as good as the DTW on our data set. The proposed LTW
shows that for a certain time series classiﬁcation problem,
it is possible to use some light weight time series distance
measurement to achieve quite good classiﬁcation accuracy.

Second, we discuss the parameter settings for the LSTM
classiﬁer. Tuning of the hyper-parameters of the LSTM net-

Second we apply the state-of-art sequential data modelling
neural network LSTM to classify the power series. Our study

0.00.20.40.60.81.0Index of the test sample0.00.20.40.60.81.0Label of the test sample020040060080010001200024681012DTWGround truthPredicted020040060080010001200024681012LTW020040060080010001200024681012LSTM020040060080010001200024681012LSTM/LTW10

(a)

(b)

(c)

(d)

Fig. 5. Accurately classiﬁed samples for the different algorithms on Fold 0. Samples from the same class are drawn in the same sub-ﬁgure: (a)1NN-DTW; (b)
1NN-LTW; (c) LSTM; (d) LSTM/LTW. LSTM cannot classify some classes while the hybrid algorithm gains the advantage of both LSTM and 1NN-LTW.

show that LSTM can perform well compared to 1NN-LTW
with similar accuracy; however, these two algorithms have
their unique different natures and the accurately classiﬁed
samples of these two algorithms have signiﬁcant difference.
In this sense, we propose a hybrid algorithm of the two
classiﬁers termed as LSTM/LTW, which further improves
the accuracy. The proposed hybrid algorithm can achieve
classiﬁcation accuracy as high as 93% in our experiments.

For the future work, one interesting problem is to study
the case that the power series generated by multiple programs
thus with multiple labels. The problem is especially interesting
when we have the test data being the combination of different
programs (such as a pair of programs (A, B)) where this
special pair may not be seen in the training data, for example,
the training data may only contain samples generated by
program pairs like (B,C) and (A,C). In this case, the classiﬁer
should be able to recognize the new pair (A, B). Also, one
can try more complicated ensemble algorithms with LSTM
and other existed time series classiﬁcation algorithms.

REFERENCES

[1] “America’s data centers consuming and wasting growing amounts of
energy,” http://www.nrdc.org/energy/data-center-efﬁciency-assessment.
asp, accessed: 2015-07-01.

[2] L. A. Barroso and U. Holzle, “The case for energy-proportional com-

puting,” Computer, vol. 40, no. 12, pp. 33–37, 2007.

[3] J. Moore, J. Chase, K. Farkas, and P. Ranganathan, “Data center
workload monitoring, analysis, and emulation,” in Eighth Workshop on
Computer Architecture Evaluation using Commercial Workloads, 2005.
[4] A. Fehske, J. Gaeddert, and J. H. Reed, “A new approach to signal
classiﬁcation using spectral correlation and neural networks,” in New
Frontiers in Dynamic Spectrum Access Networks.
IEEE, 2005, pp.
144–150.

[5] S. S. Soliman and S.-Z. Hsue, “Signal classiﬁcation using statistical
moments,” IEEE Transactions on Communications, vol. 40, no. 5, pp.
908–916, 1992.

[6] M. Reaz, M. Hussain, and F. Mohd-Yasin, “Techniques of EMG signal
analysis: detection, processing, classiﬁcation and applications,” Biolog-
ical procedures online, vol. 8, no. 1, pp. 11–35, 2006.

[7] A. Bagnall, A. Bostrom, J. Large, and J. Lines, “The great time series
classiﬁcation bake off: An experimental evaluation of recently proposed
algorithms. extended version,” arXiv preprint arXiv:1602.01711, 2016.
[8] A. Stefan, V. Athitsos, and G. Das, “The move-split-merge metric for

0.00.20.40.60.81.0Time steps0.00.20.40.60.81.0Power050100150200501001502002503003504000501001502005010015020025030035040005010015020050100150200250300350400050100150200501001502002503003504000501001502005010015020025030035040005010015020050100150200250300350400050100150200501001502002503003504000501001502005010015020025030035040005010015020050100150200250300350400050100150200501001502002503003504000501001502005010015020025030035040005010015020050100150200250300350400050100150200501001502002503003504000.00.20.40.60.81.0Time steps0.00.20.40.60.81.0Power050100150200501001502002503003504000501001502005010015020025030035040005010015020050100150200250300350400050100150200501001502002503003504000501001502005010015020025030035040005010015020050100150200250300350400050100150200501001502002503003504000501001502005010015020025030035040005010015020050100150200250300350400050100150200501001502002503003504000501001502005010015020025030035040005010015020050100150200250300350400050100150200501001502002503003504000.00.20.40.60.81.0Time steps0.00.20.40.60.81.0Power05010015020050100150200250300350400050100150200501001502002503003504000501001502005010015020025030035040005010015020050100150200250300350400050100150200501001502002503003504000.00.20.40.60.81.05010015020025030035040005010015020050100150200250300350400050100150200501001502002503003504000.00.20.40.60.81.050100150200250300350400050100150200501001502002503003504000501001502005010015020025030035040005010015020050100150200250300350400050100150200501001502002503003504000.00.20.40.60.81.0Time steps0.00.20.40.60.81.0Power0501001502005010015020025030035040005010015020050100150200250300350400050100150200501001502002503003504000501001502005010015020025030035040005010015020050100150200250300350400050100150200501001502002503003504000501001502005010015020025030035040005010015020050100150200250300350400050100150200501001502002503003504000501001502005010015020025030035040005010015020050100150200250300350400050100150200501001502002503003504000501001502005010015020025030035040011

time series,” IEEE Transactions on Knowledge and Data Engineering,
vol. 25, no. 6, pp. 1425–1438, 2013.

[9] G. E. Batista, X. Wang, and E. J. Keogh, “A complexity-invariant
SIAM, 2011,

distance measure for time series.” in SDM, vol. 11.
pp. 699–710.

[10] T. Rakthanmanon, B. Campana, A. Mueen, G. Batista, B. Westover,
Q. Zhu, J. Zakaria, and E. Keogh, “Searching and mining trillions of time
series subsequences under dynamic time warping,” in Proceedings of the
18th ACM SIGKDD international conference on Knowledge discovery
and data mining. ACM, 2012, pp. 262–270.

[11] S. Hochreiter and J. Schmidhuber, “Long short-term memory,” Neural

computation, vol. 9, no. 8, pp. 1735–1780, 1997.

[12] F. A. Gers, J. Schmidhuber, and F. Cummins, “Learning to forget:
Continual prediction with LSTM,” Neural computation, vol. 12, no. 10,
pp. 2451–2471, 2000.

[13] X. Xi, E. Keogh, C. Shelton, L. Wei, and C. A. Ratanamahatana, “Fast
time series classiﬁcation using numerosity reduction,” in Proceedings of
the 23rd international conference on Machine learning. ACM, 2006,
pp. 1033–1040.

[14] G. E. Hinton and R. R. Salakhutdinov, “Reducing the dimensionality of
data with neural networks,” Science, vol. 313, no. 5786, pp. 504–507,
2006.

[15] C. A. Ratanamahatana and E. Keogh, “Everything you know about

dynamic time warping is wrong.” Citeseer, 2004.

[16] S. Salvador and P. Chan, “Toward accurate dynamic time warping in
linear time and space,” Intelligent Data Analysis, vol. 11, no. 5, pp.
561–580, 2007.

[17] T. Rakthanmanon and E. Keogh, “Fast shapelets: A scalable algorithm
for discovering time series shapelets,” in Proceedings of the 13th SIAM
international conference on data mining. SIAM, 2013, pp. 668–676.
[18] J. Lin, E. Keogh, L. Wei, and S. Lonardi, “Experiencing sax: a novel
symbolic representation of time series,” Data Mining and knowledge
discovery, vol. 15, no. 2, pp. 107–144, 2007.

[19] H. Deng, G. Runger, E. Tuv, and M. Vladimir, “A time series forest for
classiﬁcation and feature extraction,” Information Sciences, vol. 239, pp.
142–153, 2013.

[20] T. Mikolov, M. Karaﬁ´at, L. Burget, J. Cernock`y, and S. Khudanpur,
“Recurrent neural network based language model.” in INTERSPEECH,
vol. 2, 2010, p. 3.

[21] J. Donahue, L. Anne Hendricks, S. Guadarrama, M. Rohrbach, S. Venu-
gopalan, K. Saenko, and T. Darrell, “Long-term recurrent convolutional
networks for visual recognition and description,” in Proceedings of the
IEEE Conference on Computer Vision and Pattern Recognition, 2015,
pp. 2625–2634.

[22] J. Yue-Hei Ng, M. Hausknecht, S. Vijayanarasimhan, O. Vinyals,
R. Monga, and G. Toderici, “Beyond short snippets: Deep networks
for video classiﬁcation,” in Proceedings of the IEEE Conference on
Computer Vision and Pattern Recognition, 2015, pp. 4694–4702.
[23] T.-H. Wen, M. Gasic, N. Mrksic, P.-H. Su, D. Vandyke, and S. Young,
“Semantically conditioned LSTM-based natural language generation for
spoken dialogue systems,” arXiv:1508.01745, 2015.

[24] F. Pedregosa, G. Varoquaux, A. Gramfort, V. Michel, B. Thirion,
O. Grisel, M. Blondel, P. Prettenhofer, R. Weiss, V. Dubourg et al.,
“Scikit-learn: Machine learning in Python,” The Journal of Machine
Learning Research, vol. 12, pp. 2825–2830, 2011.

[25] C. Burrus and T. W. Parks, DFT/FFT and Convolution Algorithms:

theory and Implementation.

John Wiley & Sons, Inc., 1991.

[26] “LSTM networks

for

sentiment analysis,” http://deeplearning.net/

tutorial/lstm.html, accessed: 2016-05-01.

[27] D. W. Hosmer Jr and S. Lemeshow, Applied logistic regression.

John

Wiley & Sons, 2004.

