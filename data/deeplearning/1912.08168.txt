Diﬀerentiable programming and its applications to dynamical systems

Adri´an Hern´andez and Jos´e M. Amig´o∗
Centro de Investigaci´on Operativa, Universidad Miguel Hern´andez, Avenida de la Universidad s/n, 03202 Elche, Spain

0
2
0
2

y
a
M
2

]
S
D
.
h
t
a
m

[

2
v
8
6
1
8
0
.
2
1
9
1
:
v
i
X
r
a

Abstract
Diﬀerentiable programming is the combination of classical neural networks modules with algorithmic ones in an end-
to-end diﬀerentiable model. These new models, that use automatic diﬀerentiation to calculate gradients, have new
learning capabilities (reasoning, attention and memory). In this tutorial, aimed at researchers in nonlinear systems
with prior knowledge of deep learning, we present this new programming paradigm, describe some of its new features
such as attention mechanisms, and highlight the beneﬁts they bring. Then, we analyse the uses and limitations of
traditional deep learning models in the modeling and prediction of dynamical systems. Here, a dynamical system is
meant to be a set of state variables that evolve in time under general internal and external interactions. Finally, we
review the advantages and applications of diﬀerentiable programming to dynamical systems.

Keywords: Deep learning, diﬀerentiable programming, dynamical systems, attention, recurrent neural networks

1. Introduction

The increase in computing capabilities together with
new deep learning models has led to great advances in
several machine learning tasks [1, 2, 3].

Deep learning architectures such as Recurrent Neural
Networks (RNNs) and Convolutional Neural Networks
(CNNs), as well as the use of distributed representa-
tions in natural language processing, have allowed to
take into account the symmetries and the structure of
the problem to be solved.

However, a major criticism of deep learning remains,
namely, that it only performs perception, mapping in-
puts to outputs [4].

A new direction to more general and ﬂexible mod-
els is diﬀerentiable programming, that is, the combina-
tion of geometric modules (traditional neural networks)
with more algorithmic modules in an end-to-end diﬀer-
entiable model. As a result, diﬀerentiable programming
is a dynamic computational graph composed of diﬀer-
entiable functions that provides not only perception but
also reasoning, attention and memory. To eﬃciently cal-
culate derivatives, this approach uses automatic diﬀer-
entiation, an algorithmic technique similar to backprop-
agation and implemented in modern software packages
such as PyTorch, Julia, etc.

∗Corresponding author

Preprint submitted to Physica D

To keep our exposition concise, this tutorial is aimed
at researchers in nonlinear systems with prior knowl-
edge of deep learning; see [5] for an excellent intro-
duction to the concepts and methods of deep learning.
Therefore, this tutorial focuses right away on the lim-
itations of traditional deep learning and the advantages
of diﬀerential programming, with special attention to its
application to dynamical systems. By a dynamical sys-
tem we mean here and hereafter a set of state variables
that evolve in time under the inﬂuence of internal and
possibly external inputs.

Examples of diﬀerentiable programming techniques
that have been successfully developed in recent years
include

(i) attention mechanisms [6], which allow the model
to automatically search and learn which parts of a
source sequence are relevant to predict the target ele-
ment,

(ii) self-attention,
(iii) end-to-end Memory Networks [7], and
(iv) Diﬀerentiable Neural Computers (DNCs) [8],
which are neural networks (controllers) with an exter-
nal read-write memory.

As expected, in recent years there has been a grow-
ing interest in applying deep learning techniques to dy-
namical systems. In this regard, RNNs and Long Short-
Term Memories (LSTMs), specially designed for se-
quence modelling and temporal dependence, have been

May 5, 2020

 
 
 
 
 
 
successful in various applications to dynamical systems
such as model identiﬁcation and time series prediction
[9, 10, 11].

The performance of theses models (e.g.

encoder-
decoder networks), however, degrades rapidly as the
length of the input sequence increases and they are not
able to capture the dynamic (i.e., time-changing) inter-
dependence between time steps. The combination of
neural networks with new diﬀerentiable modules could
overcome some of those problems and oﬀer new oppor-
tunities and applications.

Among the potential applications of diﬀerentiable

programming to dynamical systems let us mention

(i) attention mechanisms to select the relevant time

steps and inputs,

(ii) memory networks to store historical data from dy-
namical systems and selectively use it for modelling and
prediction, and

(iii) the use of diﬀerentiable components in scientiﬁc

computing.
Despite some achievements, more work is still needed
to verify the beneﬁts of these models over traditional
networks.

Thanks to software libraries that facilitate auto-
matic diﬀerentiation, diﬀerentiable programming ex-
tends deep learning models with new capabilities (rea-
soning, memory, attention, etc.) and the models can be
eﬃciently coded and implemented.

In the following sections of this tutorial we introduce
diﬀerentiable programming and explain in detail why it
is an extension of deep learning (Section 2). We de-
scribe some models based on this new approach such as
attention mechanisms (Section 3.1), memory networks
and diﬀerentiable neural computers (Section 3.2), and
continuous learning (Section 3.3). Then we review the
use of deep learning in dynamical systems and their lim-
itations (Section 4.1). And, ﬁnally, we present the new
opportunities that diﬀerentiable programming can bring
to the modelling, simulation and prediction of dynami-
cal systems (Section 4.2). The conclusions and outlook
are summarized in Section 5.

2. From deep learning to diﬀerentiable program-

ming

In recent years, we have seen major advances in the
ﬁeld of machine learning. The combination of deep
neural networks with the computational capabilities of
Graphics Processing Units (GPUs) [12] has improved
the performance of several tasks (image recognition,
machine translation, language modelling, time series

2

prediction, game playing and more) [1, 2, 3].
Inter-
estingly, deep learning models and architectures have
evolved to take into account the structure of the prob-
lem to be resolved.

Deep learning is a part of machine learning that
is based on neural networks and uses multiple layers,
where each layer extracts higher level features from the
input. RNNs are a special class of neural networks
where outputs from previous steps are fed as inputs to
the current step [13, 14]. This recurrence makes them
appropriate for modelling dynamic processes and sys-
tems.

CNNs are neural networks that alternate convolu-
tional and pooling layers to implement translational in-
variance [15]. They learn spatial hierarchies of features
through backpropagation by using these building layers.
CNNs are being applied successfully to computer vision
and image processing [16].

Especially important is the use of distributed rep-
resentations as inputs to natural language processing
pipelines. With this technique, the words of the vocab-
ulary are mapped to an element of a vector space with a
much lower dimensionality [17, 18]. This word embed-
ding is able to keep, in the learned vector space, some
of the syntactic and semantic relationships presented in
the original data.

Let us recall that, in a feedforward neural network
(FNN) composed of multiple layers, the output (without
the bias term) at layer l, see Figure 1, is deﬁned as

xl+1 = σ(W l xl),

(1)

W l being the weight matrix at layer l. σ is the activation
function and xl+1, the output vector at layer l and the
input vector at layer l + 1. The weight matrices for the
diﬀerent layers are the parameters of the model.

Learning is the mechanism by which the parame-
ters of a neural network are adapted to the environment
in the training process. This is an optimization prob-
lem which has been addressed by using gradient-based
methods, in which given a cost function f : Rn → R,
the algorithm ﬁnds local minima w∗ = arg minw f (w)
updating each layer parameter wi j with the rule wi j :=
wi j − η∇wi j f (w), η > 0 being the learning rate.

In addition to regarding neural networks as universal
approximators, there is no sound theoretical explanation
for a good performance of deep learning. Several theo-
retical frameworks have been proposed:

(i) As pointed out in [19], the class of functions of
practical interest can be approximated with expo-
nentially fewer parameters than the generic ones.

(i) Programs are directed acyclic graphs.
(ii) Graph nodes are mathematical functions or vari-
ables and the edges correspond to the ﬂow of in-
termediate values between the nodes.

(iii) n is the number of nodes and l the number of input
variables of the graph, with 1 ≤ l < n. vi for
i ∈ {1, ..., n} is the variable associated with node i.
(iv) E is the set of edges in the graph. For each
(i, j) ∈ E we have i < j, therefore the graph is
topologically ordered.

(v) fi for i ∈ {(l+1), ..., n} is the diﬀerentiable function
computed by node i in the graph. αi for i ∈ {(l +
1), ..., n} contains all input values for node i.
(vi) The forward algorithm or pass, given input vari-
ables v1, ..., vl calculates vi = fi(αi) for i = {(l +
1), ..., n}.

(vii) The graph is dynamically constructed and com-
posed of parametrizable functions that are diﬀer-
entiable and whose parameters are learned from
data.

Then, neural networks are just a class of these diﬀer-
entiable programs composed of classical blocks (feed-
forward, recurrent neural networks, etc.) and new ones
such as diﬀerentiable branching, attention, memories,
etc.

Diﬀerentiable programming can be seen as a con-
tinuation of the deep learning end-to-end architectures
that have replaced, for example, the traditional linguistic
components in natural language processing [23, 24]. To
eﬃciently calculate the derivatives in a gradient descent,
this approach uses automatic diﬀerentiation, an algo-
rithmic technique similar but more general than back-
propagation.

Automatic diﬀerentiation, in its reverse mode and in
contrast to manual, symbolic and numerical diﬀeren-
tiation, computes the derivatives in a two-step process
[25, 26]. As described in [25] and rearranging the in-
dexes of the previous deﬁnition, a function f : Rn → Rm
is constructed with intermediate variables vi such that:
(i) variables vi−n = xi, i = 1, ..., n are the inputs vari-

ables.

(ii) variables vi, i = 1, ..., l are the intermediate vari-

ables.

(iii) variables ym−i = vl−i, i = m − 1, ..., 0 are the output

variables.

In a ﬁrst step, similar to the forward pass described
before, the computational graph is built populating in-
termediate variables vi and recording the dependencies.
In a second step, called the backward pass, derivatives
are calculated by propagating for the output y j being

3

Figure 1: Multilayer neural network.

Symmetry, locality and compositionality proper-
ties make it possible to have simpler neural net-
works.

(ii) From the point of view of information theory [20],
an explanation has been put forward based on how
much information each layer of the neural network
retains and how this information varies with the
training and testing process.

Although deep learning can implicitly implement
logical reasoning [21], it has limitations that make it dif-
ﬁcult to achieve more general intelligence [4]. Among
these limitations, we can highlight the following:

(i) It only performs perception, representing a map-

ping between inputs and outputs.

(ii) It follows a hybrid model where synaptic weights
perform both processing and memory tasks but
doesn’t have an explicit external memory.

(iii) It does not carry out conscious and sequential rea-
soning, a process that is based on perception and
memory through attention.

A path to a more general intelligence, as we will see
below, is the combination of geometric modules with
more algorithmic modules in an end-to-end diﬀeren-
tiable model. This approach, called diﬀerentiable pro-
gramming, adds new parametrizable and diﬀerentiable
components to traditional neural networks.

Diﬀerentiable programming, a broad term, is deﬁned
in [22] as a programming model (model of how a com-
puter program is executed), trainable with gradient de-
scent, where neural networks are truly functional blocks
with data-dependent branches and recursion.

Here, and for the purposes of this tutorial, we deﬁne
diﬀerentiable programming as a programming model
with the following characteristics:

considered, the adjoints vi = ∂y j
∂vi
inputs.

from the output to the

The reverse mode is more eﬃcient to evaluate for
functions with a large number of inputs (parameters)
: Rn → R,
and a small number of outputs. When f
as is the case in machine learning with n very large and
f the cost function, only one pass of the reverse mode
is necessary to compute the gradient ∇ f = ( ∂y
).
∂x1
In the last years, deep learning frameworks such
as PyTorch have been developed that provide reverse-
mode automatic diﬀerentiation [27]. The deﬁne-by-run
philosophy of PyTorch, whose execution dynamically
constructs the computational graph, facilitates the de-
velopment of general diﬀerentiable programs.

, ..., ∂y
∂xn

Diﬀerentiable programming is an evolution of classi-
cal (traditional) software programming where, as shown
in Table 1:

(i) Instead of specifying explicit instructions to the
computer, an objective is set and an optimizable
architecture is deﬁned which allows to search in a
subset of possible programs.

(ii) The program is deﬁned by the input-output data

and not predeﬁned by the user.

(iii) The algorithmic elements of the program have to
be diﬀerentiable, say, by converting them into dif-
ferentiable blocks.

Classical
Sequence of instructions
Fixed architecture
User deﬁned
Imperative programming Declarative programming
Intuitive

Diﬀerentiable
Sequence of diﬀ. primitives
Optimizable architecture
Data deﬁned

Abstract

Table 1: Diﬀerentiable vs classical programming.

RNNs, for example, are an evolution of feedforward
networks because they are classical neural networks in-
side a for-loop (a control ﬂow statement for iteration)
which allows the neural network to be executed repeat-
edly with recurrence. However, this for-loop is a prede-
ﬁned feature of the model. Diﬀerentiable programming
allows to dynamically constructs the graph and vary the
length of the loop. Then, the ideal situation would be to
augment the neural network with programming primi-
tives (for-loops, if branches, while statements, external
memories, logical modules, etc.) that are not predeﬁned
by the user but are parametrizable by the training data.
The trouble is that many of these programming prim-
itives are not diﬀerentiable and need to be converted

4

into optimizable modules. For instance, if the con-
dition a of an ”if” primitive (e.g., if a is satisﬁed do
y(x), otherwise do z(x)) is to be learned, it can be the
output of a neural network (linear transformation and
a sigmoid function) and the conditional primitive will
transform into a weighted combination of both branches
ay(x) + (1 − a)z(x). Similarly, in an attention module,
diﬀerent weights that are learned with the model are as-
signed to give a diﬀerent inﬂuence to each part of the
input. Figure 2 shows the computational graph of a con-
ditional branching.

Figure 2: Computational graph of diﬀerentiable branching.

The process of extending deep learning with diﬀeren-

tiable primitives would consist of the following steps:

(i) Select a new function that improves the classical
input-output transformation of deep learning, e.g.
attention, continuous learning, memories, etc.

(ii) Convert this function into a directed acyclic graph,
a sequence of parametrizable and diﬀerentiable
functions. For example, Figure 2 shows this se-
quence of operations used in attention for diﬀer-
entiable branching.

(iii) Integrate this new function into the base model.

In this way, using diﬀerentiable programming we can
combine traditional perception modules (CNN, RNN,
FNN) with additional algorithmic modules that provide
reasoning, abstraction and memory [28]. In the follow-
ing section we describe, by following this process, some
examples of this approach that have been developed in
recent years.

3. Diﬀerentiable learning and reasoning

3.1. Diﬀerentiable attention

One of the aforementioned limitations of deep learn-
ing models is that they do not perform conscious and
sequential reasoning, a process that is based on percep-
tion and memory through attention.

Reasoning is the process of consciously establishing
and verifying facts combining attention with new or ex-
isting information. An attention mechanism allows the
brain to focus on one part of the input or memory (im-
age, text, etc), giving less attention to others.

Attention mechanisms have provided and will pro-
vide a paradigm shift in machine learning. From tradi-
tional large-scale vector transformations to a more con-
scious process that focuses only on a set of elements,
e.g. decomposing a problem into a sequence of atten-
tion based reasoning operations [29].

Figure 3: Attention diagram.

One way to make this attention process diﬀerentiable
is to make it a convex combination of the input or mem-
ory, where all the steps are diﬀerentiable and the com-
bination weights are parametrizable.

As in [30], this diﬀerentiable attention process is de-
scribed as mapping a query and a set of key-value pairs
to an output:

att(q, s) =

T(cid:88)

i=1

αi(q, ki)Vi,

(2)

where, as seen in ﬁgure 3, ki and Vi are the key and the
value vectors from the source/memory s, and q is the
query vector. αi(q, ki) is the similarity function between
the query and the corresponding key and is calculated by
applying the softmax function:

αi =

exp(score(q, ki))
i(cid:48)=1 exp(score(q, ki(cid:48)))

(cid:80)T

.

(4)

The score function can be computed using a feedfor-

ward neural network:

score(q, ki) = Za tanh(Wa[q, ki])),

(5)

as proposed in [6], where Za and Wa are matrices to
be jointly learned with the rest of the model and [q, ki]
is a linear function or concatenation of q and ki. Also,
in [31] the authors use a cosine similarity measure for
content-based attention, namely,

score(q, ki) = cos((q, ki))

(6)

where ((q, ki)) denotes the angle between q and ki.

Then, diﬀerentiable attention can be seen as a sequen-
tial process of reasoning in which the task (query) is
guided by a set of elements of the input source (or mem-
ory) using attention.

The attention process can focus on:

(i) Temporal dimensions, e.g. diﬀerent time steps of

a sequence.

(ii) Spatial dimensions, e.g. diﬀerent regions of an im-

age.

(iii) Diﬀerent elements of a memory.
(iv) Diﬀerent features or dimensions of an input vec-

tor, etc.

Depending on where the process is initiated, we have:

(i) Top-down attention, initiated by the current task.
(ii) Bottom-up, initiated spontaneously by the source

or memory.

3.1.1. Attention mechanisms in seq2seq models

RNNs (see Figure 4) are a basic component of mod-
ern deep learning architectures, especially of encoder-
decoder networks. The following equations deﬁne the
time evolution of an RNN:

ht = f h(W ih xt + W hhht−1),

(7)

yt

= f o(W ho ht),
W ih, W hh and W ho being weight matrices. f h and f o are
the hidden and output activation functions while xt, ht
and yt are the network input, hidden state and output.

(8)

S o f tmax(zi) = exp(zi)
i(cid:48) exp(zi(cid:48) )

(cid:80)

to the score function score(q, ki) :

An evolution of RNNs are LSTMs [32], an RNN
structure with gated units, i.e.
regulators. LSTM are
composed of a cell, an input gate, an output gate and a
forget gate, and allow gradients to ﬂow unchanged. The

(3)

5

at time t, m is the size of the hidden state and f1 is
an RNN (or any of its variants).

(ii) A decoder, where st is the hidden state and whose
initial state s0 is initialized with the last hidden
state of the encoder hT .
It generates the output
sequence Y = (y1, y2, ..., yT (cid:48) ), yt ∈ Ro (the dimen-
sion o depending on the task), with

yt

= f2(st−1, yt−1),

(10)

where f2 is an RNN (or any of its variants) with an
additional softmax layer.

Because the encoder compresses all the information
of the input sequence in a ﬁxed-length vector (the ﬁnal
hidden state hT ), the decoder possibly does not take into
account the ﬁrst elements of the input sequence. The use
of this ﬁxed-length vector is a limitation to improve the
performance of the encoder-decoder networks. More-
over, the performance of encoder-decoder networks de-
grades rapidly as the length of the input sequence in-
creases [33]. This occurs in applications such as ma-
chine translation and time series predition, where it is
necessary to model long time dependencies.

The key to solve this problem is to use an atten-
tion mechanism.
In [6] an extension of the basic
encoder-decoder arquitecture was proposed by allowing
the model to automatically search and learn which parts
of a source sequence are relevant to predict the target
element. Instead of encoding the input sequence in a
ﬁxed-length vector, it generates a sequence of vectors,
choosing the most appropriate subset of these vectors
during the decoding process.

With the attention mechanism, the encoder is a bidi-
−→
hi =
rectional RNN [34] with a forward hidden state
←−
h i+1, xi). The
f1(
encoder state is represented as a simple concatenation
of the two states,

−→
h i−1, xi) and a backward one

←−
hi = f1(

hi = [

−→
hi;

←−
hi],

(11)

with i = 1, ..., T . The encoder state includes both the
preceding and following elements of the sequence, thus
capturing information from neighbouring inputs.

The decoder has an output

yt

= f2(st−1, yt−1, ct)

(12)

for t = 1, ..., T (cid:48).
f2 is an RNN with an additional soft-
max layer, and the input is a concatenation of yt−1 with
the context vector ct, which is a sum of hidden states of
the input sequence weighted by alignment scores:

6

Figure 4: Temporal structure of a recurrent neural network.

memory cell remembers values over arbitrary time inter-
vals and the three gates regulate the ﬂow of information
into and out of the cell.

An encoder-decoder network maps an input sequence
to a target one with both sequences of arbitrary length
[2]. They have applications ranging from machine
translation to time series prediction.

Figure 5: An encoder-decoder network.

More speciﬁcally, this mechanism uses an RNN (or
any of its variants, an LSTM or a GRU, Gated Recur-
rent Unit) to map the input sequence to a ﬁxed-length
vector, and another RNN (or any of its variants) to de-
code the target sequence from that vector (see Figure 5).
Such a seq2seq model features normally an architecture
composed of:

(i) An encoder which, given an input sequence X =

(x1, x2, ..., xT ) with xt ∈ Rn, maps xt to

ht = f1(ht−1, xt),
(9)
where ht ∈ Rm is the hidden state of the encoder

T(cid:88)

ct =

αtihi.

(13)

i=1
Similar to equation (4), the weight αti of each state hi is
calculated by

αti =

exp(score(st−1, hi))
i(cid:48)=1 exp(score(st−1, hi(cid:48)))

(cid:80)T

.

(14)

In this attention mechanism, the query is the state st−1
and the key and the value are the hidden states hi. The
score measures how well the input at position i and the
output at position t match. αti are the weights that im-
plement the attention mechanism, deﬁning how much of
each input hidden state should be considered when de-
ciding the next state st and generating the output yt (see
Figure 6).

Figure 6: An encoder-decoder network with attention.

As we have described previously, the score function
can be parametrized using diﬀerent alignment models
such as feedforward networks and the cosine similarity.
An example of a matrix of alignment scores can be
seen in Figure 7. This matrix provides interpretability
to the model since it allows to know which part (time-
step) of the input is more important to the output.

3.2. Other attention mechanisms and diﬀerentiable

neural computers

A variant of the attention mechanism is self-attention,
in which the attention component relates diﬀerent posi-
tions of a single sequence in order to compute a repre-
sentation of the sequence. In this way, the keys, values
and queries come from the same source. The mecha-
nism can connect distant elements of the sequence more
directly than using RNNs [35].

7

Figure 7: A matrix of alignment scores.

Another variant of attention are end-to-end memory
networks [7], which we describe in Section 4.2.2 and are
neural networks with a recurrent attention model over
an external memory. The model, trained end-to-end,
outputs an answer based on a set of inputs x1, x2, ..., xn
stored in a memory and a query.

Traditional computers are based on the von Neumann
architecture which has two basic components: the CPU
(Central Processing Unit), which carries out the pro-
gram instructions, and the memory, which is accessed
by the CPU to perform write/read operations. In con-
trast, neural networks follow a hybrid model where
synaptic weights perform both processing and memory
tasks.

Neural networks and deep learning models are good
at mapping inputs to outputs but are limited in their abil-
ity to use facts from previous events and store useful
information. Diﬀerentiable Neural Computers (DNCs)
[8] try to overcome these shortcomings by combining
neural networks with an external read-write memory.

As described in [8], a DNC is a neural network, called
the controller (playing the role of a diﬀerentiable CPU),
with an external memory, an N × W matrix. The DNC
uses diﬀerentiable attention mechanisms to deﬁne dis-
tributions (weightings) over the N rows and learn the
importance each row has in a read or write operation.

To select the most appropriate memory components
during read/write operations, a weighted sum w(i) is
used over the memory locations i = 1, ..., N. The atten-
tion mechanism is used in three diﬀerent ways:

(i) Access content (read or write) based on similarity.
(ii) Time ordered access (temporal links) to recover
the sequences in the order in which they were writ-
ten.

(iii) Dynamic memory allocation, where the DNC as-
signs and releases memory based on usage per-
centage.

At each time step, the DNC gets an input vector and
emits an output vector that is a function of the combina-
tion of the input vector and the memories selected.

DNCs, by combining the following characteristics,
have very promising applications in complex tasks that
require both perception and reasoning:

y j = tanh

(cid:88)

(wi j + αi jHi j(t))yi





i∈inputs



,


(15)

Hi j(t + 1) = ηyiy j + (1 − η)Hi j(t).
Then, during the initial training period, wi j and αi j
are trained using gradient descent and after this period,
the model keeps learning from ongoing experience.

(16)

(i) The classical perception capability of neural net-

4. Dynamical systems and diﬀerentiable program-

works.

ming

(ii) Read and write capabilities based on content sim-

ilarity and learned by the model.

(iii) The use of previous knowledge to plan and reason.
(iv) End-to-end diﬀerentiability of the model.
(v) Implementation using software packages with au-
tomatic diﬀerentiation libraries such as PyTorch,
Tensorﬂow or similar.

3.3. Meta-plasticity and continuous learning

The combination of geometric modules (classical
neural networks) with algorithmic ones adds new learn-
ing capabilities to deep learning models. In the previ-
ous sections we have seen that one way to improve the
learning process is by focusing on certain elements of
the input or a memory and making this attention diﬀer-
entiable.

Another natural way to improve the process of learn-
ing is to incorporate diﬀerentiable primitives that add
ﬂexibility and adaptability. A source of inspiration is
neuromodulators, which furnish the traditional synap-
tic transmission with new computational and processing
capabilities [36].

Unlike the continuous learning capabilities of ani-
mal brains, which allow animals to adapt quickly to
the experience, in neural networks, once the training
is completed, the parameters are ﬁxed and the network
stops learning. To solve this issue, in [37] a diﬀeren-
tiable plasticity component is attached to the network
that helps previously-trained networks adapt to ongoing
experience.

The process to introduce the diﬀerentiable plastic
component in the network is as follows. The activation
y j of neuron j has a conventional ﬁxed weight wi j and
a plastic component αi jHi j(t), where αi j is a structural
parameter tuned during the training period and Hi j(t) a
plastic component automatically updated as a function
of ongoing inputs and outputs. The equations for the ac-
tivation of y j with learning rate η, as described in [37],
are:

4.1. Modeling dynamical systems with neural networks
Dynamical systems deal with time-evolutionary pro-
cesses and their corresponding systems of equations.
At any given time, a dynamical system has a state that
can be represented by a point in a state space (mani-
fold). The evolutionary process of the dynamical sys-
tem describes what future states follow from the current
state. This process can be deterministic, if its entire fu-
ture is uniquely determined by its current state, or non-
deterministic otherwise [38] (e.g., a random dynamical
system [39]). Furthermore, it can be a continuous-time
process, represented by diﬀerential equations or, as in
this paper, a discrete-time process, represented by dif-
ference equations or maps. Thus,

ht = f (ht−1; θ)
for autonomous discrete-time deterministic dynamical
systems with parameters θ, and

(17)

ht = f (ht−1, xt; θ)
for non-autonomous discrete-time deterministic dynam-
ical systems driven by an external input xt.

(18)

Dynamical systems have important applications in
physics, chemistry, economics, engineering, biology
and medicine [40]. They are relevant even in day-to-day
phenomena with great social impact such as tsunami
warning, earth temperature analysis and ﬁnancial mar-
kets prediction.

Dynamical systems that contain a very large number
of variables interacting with each other in non-trivial
ways are sometimes called complex (dynamical) sys-
tems [41]. Their behaviour is intrinsically diﬃcult to
model due to the dependencies and interactions between
their parts and they have emergence properties arising
from these interactions such as adaptation, evolution,
learning, etc.

Here we consider discrete-time, deterministic and
non-autonomous (i.e., the time evolution depending also

8

on exogenous variables) dynamical systems as well as
the more general complex systems. Speciﬁcally, the dy-
namical systems of interest range from systems of dif-
ference equations with multiple time delays to systems
with a dynamic (i.e., time-changing) interdependence
between time steps. Notice that the former ones may
be rewritten as higher dimensional systems with time
delay 1.

On the other hand, in recent years deep learning mod-
els have been very successful in performing various
tasks such as image recognition, machine translation,
game playing, etc. When the amount of training data
is suﬃcient and the distribution that generates the real
data is the same as the distribution of the training data,
these models perform extremely well and approximate
the input-output relation.

In view of the importance of dynamical systems for
modeling physical, biological and social phenomena,
there is a growing interest in applying deep learning
techniques to dynamical systems. This can be done in
diﬀerent contexts, such as:

(i) Modeling dynamical systems with known struc-
ture and equations but non-analytical or complex
solutions [42].

(ii) Modeling dynamical systems without knowledge
of the underlying governing equations [43, 44]. In
this regard, let us mention that commercial initia-
tives are emerging that combine large amounts of
meteorological data with deep learning models to
improve weather predictions.

(iii) Modeling dynamical systems with partial or noisy

data [45].

A key aspect in modelling dynamical systems is tem-
poral dependence. There are two ways to introduce it
into a neural network [46]:

(i) A classical feedforward neural network with time
delayed states in the inputs but perhaps with an
unnecessary increase in the number of parameters.
(ii) A recurrent neural network (RNN) which, as
shown in Equations (7) and (8), has a temporal re-
currence that makes it appropriate for modelling
discrete dynamical systems of the form given in
Equations (17) and (18).

Thus, RNNs, specially designed for sequence mod-
elling [47], seem the ideal candidates to model, analyze
and predict dynamical systems in the broad sense used
in this tutorial. The temporal recurrence of RNNs, the-
oretically, allows to model and identify dynamical sys-
tems described with equations with any temporal depen-
dence.

To learn chaotic dynamics, recurrent radial basis
function (RBF) networks [48] and evolutionary algo-
rithms that generate RNNs have been proposed [49].
”Nonlinear Autoregressive model with exogenous in-
put” (NARX) [50] and boosted RNNs [51] have been
applied to predict chaotic time series.

However, a diﬃculty with RNNs is the vanishing gra-
dient problem [52]. RNNs are trained by unfolding
them into deep feedforward networks, creating a new
layer for each time step of the input sequence. When
backpropagation computes the gradient by the chain
rule, this gradient vanishes as the number of time-steps
increases. As a result, for long input-output sequences,
as depicted in Figure 8, RNNs have trouble modelling
long-term dependencies, that is, relationships between
elements that are separated by large periods of time.

Figure 8: Vanishing gradient problem in RNNs. Information sensitiv-
ity decays over time forgetting the ﬁrst input.

To overcome this problem, LSTMs were proposed.
LSTMs have an advantage over basic RNNs due to their
relative insensitivity to temporal delays and, therefore,
are appropriate for modeling and making predictions
based on time series whenever there exist temporary de-
pendencies of unknown duration. With the appropriate
number of hidden units and activation functions [10],
LSTMs can model and identify any non-linear dynami-
cal system of the form:

ht = f (xt, ..., xt−T , ht−1, ..., ht−T ),

yt

= g(ht),

(19)

(20)

f and g are the state and output functions while xt, ht
and yt are the system input, state and output.

LSTMs have succeeded in various applications to dy-
namical systems such as model identiﬁcation and time
series prediction [9, 10, 11].

9

An also remarkable application of the LSTM has
been machine translation [2, 53], using the encoder-
decoder architecture described in Section 3.1.1.

However, as we have seen, the decoder possibly does
not take into account the ﬁrst elements of the input se-
quence because the encoder compresses all the informa-
tion of the input sequence in a ﬁxed-length vector. Then,
the performance of encoder-decoder networks degrades
rapidly as the length of input sequence increases and
this can be a problem in time series analysis, where pre-
dictions are based upon a long segment of the series.

Furthermore, as depicted in Figure 9, a complex
dynamic may feature interdependencies between time
steps that vary with time. In this situation, the equation
that deﬁnes the temporal evolution may change at each
t ∈ 1, ..., T . For these dynamical systems, adding an
attention module like the one described in Equation 13
can help model such time-changing interdependencies.

applying this mechanism to dynamical systems model-
ing or prediction, it is necessary to decide the following
aspects:

(i) In which phase or phases of the model should the

attention mechanism be introduced?

(ii) What dimension is the mechanism going to focus

on? Temporal, spatial, etc.

(iii) What parts of the system will correspond to the

query, the key and the value?

One option, which is also quite illustrative, is to use
a dual-stage attention, an encoder with input attention
and a decoder with temporal attention, as pointed out in
[54].

Here we describe this option, in which the ﬁrst stage
extracts the relevant input features and the second se-
lects the relevant time steps of the model. In many dy-
namical systems there are long term dependencies be-
tween time steps and these dependencies can be dy-
namic, i.e., time-changing.
In these cases, attention
mechanisms learn to focus on the most relevant parts
of the system input or state.

Figure 9: Temporal interdependencies in a dynamical system.

4.2. Improving dynamical systems with diﬀerentiable

programming

Deep learning models together with graphic proces-
sors and large amounts of data have improved the mod-
eling of dynamical systems but this has some limitations
such as those mentioned in the previous section. The
combination of neural networks with new diﬀerentiable
algorithmic modules is expected to overcome some of
those shortcomings and oﬀer new opportunities and ap-
plications.

In the next three subsections we illustrate with exam-
ples the kind of applications of diﬀerentiable program-
ming to dynamical systems we have in mind, namely:
implementations of attention mechanisms, memory net-
works, scientiﬁc simulations and modeling in physics.

4.2.1. Attention mechanisms in dynamical systems

X = (x1, x2, ..., xT ) with xt ∈ Rn represents the input
sequence. T is the length of the time interval and n the
number of input features or dimensions. At each time
step t, xt = (x1

t , ..., xn
t ).

t , x2

Encoder with input attention

The encoder, given an input sequence X, maps ut to

ht = f1(ht−1, ut),
where ht ∈ Rm is the hidden state of the encoder at
time t, m is the size of the hidden state and f1 is an
RNN (or any of its variants). xt is replaced by ut, which
adaptively selects the relevant input features with

(21)

ut = (α1

t x1

t , α2

t x2

t , ..., αn

t xn
t ).

(22)

αk

t is the attention weight measuring the importance

of the k input feature at time t and is computed by

=

αk
t

(cid:80)T

exp(score(ht−1, xk))
i=1 exp(score(ht−1, xi))
2, ..., xk

1, xk

where xk = (xk
T ) is the k input feature series
and the score function can be computed using a feed-
forward neural network, a cosine similarity measure or
other similarity functions.

,

(23)

In the previous sections we have described the atten-
tion mechanism, which allows a task to be guided by a
set of elements of the input or memory source. When

Then, this ﬁrst attention stage extracts the relevant in-
put features, as seen in Figure 10 with the corresponding
query, keys and values.

10

Figure 10: Diagram of the input attention mechanism.

Decoder with temporal attention

Similar to the attention decoder described in Section

3.1.1, the decoder has an output

= f2(st−1, yt−1, ct)

yt
for t = 1, ..., T (cid:48).
f2 is an RNN (or any of its variants)
with an additional linear or softmax layer, and the in-
put is a concatenation of yt−1 with the context vector ct,
which is a sum of hidden states of the input sequence
weighted by alignment scores:

(24)

ct =

T(cid:88)

i=1

βi
t hi.

(25)

The weight βi

t of each state hi is computed using the
similarity function, score(st−1, hi), and applying a soft-
max function, as described in Section 3.1.1.

This second attention stage selects the relevant time
steps, as shown in Figure 11 with the corresponding
query, keys and values.

Further remarks

In [54], the authors deﬁne this dual-stage attention
RNN and show that the model outperforms a classical
model in time series prediction.

In [55], a comparison is made between LSTMs and
attention mechanisms for ﬁnancial time series forecast-
ing. It is shown there that an LSTM with attention per-
form better than stand-alone LSTMs.

A temporal attention layer is used in [56] to select rel-
evant information and to provide model interpretability,
an essential feature to understand deep learning models.
Interpretability is further studied in detail in [57], con-
cluding that attention weights partially reﬂect the im-
pact of the input elements on model prediction.

11

Figure 11: Diagram of the input attention mechanism.

Despite the theoretical advantages and some achieve-
ments, further studies are needed to verify the beneﬁts
of the attention mechanism over traditional networks.

4.2.2. Memory networks

Memory networks allow long-term dependencies in
sequential data to be learned thanks to an external mem-
ory component. Instead of taking into account only the
most recent states, memory networks consider the entire
list of entries or states.

Here we deﬁne one possible application of memory
networks to dynamical systems, following an approach
based on [7]. We are given a time series of histori-
cal data n1, ..., nT (cid:48) with ni ∈ Rn and the input series
x1, ..., xT with xt ∈ Rn the current input, which is the
query.

The set {ni} are converted into memory vectors {mi}
and output vectors {ci} of dimension d. The query xt is
also transformed to obtain a internal state ut of dimen-
sion d. These transformations correspond to a linear
transformation: Ani = mi, Bni = ci, Cxt = ut, being
A, B, C parameterizable matrices.

A match between ut and each memory vector mi is
computed by taking the inner product followed by a
softmax function:

pi
t

= S o f tmax(uT

t mi).

(26)

The ﬁnal vector from the memory, ot, is a weighted

sum over the transformed inputs {ci}:

ot =

(cid:88)

j

pi
t ci.

(27)

To generate the ﬁnal prediction yt, a linear layer is
applied to the sum of the output vector ot and the trans-
formed input ut and to the previous output yt−1:

yt

= W 1(ot + ut) + W 2yt−1
This model is diﬀerentiable end-to-end by learning
the matrices (the ﬁnal matrices W i ant the three transfor-
mation matrices A, B and C) to minimize the prediction
error.

(28)

In [58] the authors propose a similar model based on
memory networks with a memory component, three en-
coders and an autoregressive component for multivari-
ate time-series forecasting. Compared to non-memory
RNN models, their model is better at modeling and cap-
turing long-term dependencies and, moreover, it is in-
terpretable.

Taking advantage of the highlighted capabilities of
Diﬀerentiable Neural Computers (DNCs), an enhanced
DNC for electroencephalogram (EEG) data analysis is
proposed in [59]. By replacing the LSTM network con-
troller with a recurrent convolutional network, the po-
tential of DNCs in EEG signal processing is convinc-
ingly demonstrated.

4.2.3. Scientiﬁc simulation and physical modeling

Scientiﬁc modeling, as pointed out in [60], has tradi-

tionally employed three approaches:

(i) Direct modeling, if the exact function that relates

input and output is known.

(ii) Using a machine learning model. As we have
mentioned, neural networks are universal approx-
imators.

(iii) Using a diﬀerential equation if some structure of
the problem is known. For example, if the rate of
change of the unknown function is a function of
the physical variables.

Machine learning models have to learn the input-
output transformation from scratch and need a lot of
data. One way to make them more eﬃcient is to com-
bine them with a diﬀerentiable component suited to a
speciﬁc problem. This component allows speciﬁc prior
knowledge to be incorporated into deep learning models
and can be a diﬀerentiable physical model or a diﬀeren-
tiable ODE (ordinary diﬀerential equation) solver.

(i) Diﬀerentiable physical models.

Diﬀerentiable plasticity, as described in Section
3.3, can be applied to deep learning models of dy-
namical systems in order to help them adapt to on-
going data and experience.
As done in [37],
the plasticity component de-
scribed in Equations 15 and 16, can be introduced
in some layers of the deep learning architecture.

In this way, the model can continuously learn be-
cause the plastic component is updated by neural
activity.
DiﬀTaichi, a diﬀerentiable programming language
for building diﬀerentiable physical simulations,
is proposed in [62], integrating a neural network
controller with a physical simulation module.
A diﬀerentiable physics engine is presented in
[63]. The system simulates rigid body dynamics
and can be integrated in an end-to-end diﬀeren-
tiable deep learning model for learning the physi-
cal parameters.

(ii) Diﬀerentiable ODE solvers.

As described in [60], an ODE can be embedded
into a deep learning model. For example, the Euler
method takes in the derivative function and the ini-
tial values and outputs the approximated solution.
The derivative function could be a neural network.
This solver is diﬀerentiable and can be integrated
into a lager model that can be optimized using gra-
dient descent.
In [61] a diﬀerentiable model of a trebuchet is de-
scribed. In a classical trebuchet model, the param-
eters (the mass of the counterweight and the angle
of release) are fed into an ODE solver that cal-
culates the distance, which is compared with the
target distance.
In the extended model, a neural network is intro-
duced. The network takes two inputs, the target
distance and the current wind speed, and outputs
the trebuchet parameters, which are fed into the
simulator to calculate the distance. This distance
is compared with the target distance and the er-
ror is back-propagated through the entire model to
optimize the parameters of the network. Then, the
neural network is optimized so that the model can
achieve any target distance. Using this extended
model is faster than optimizing only the trebuchet.
This type of applications shows how combin-
ing diﬀerentiable ODE solvers and deep learning
models allows to incorporate previous structure to
the problem and makes the learning process more
eﬃcient.
We may conclude that combining scientiﬁc com-
puting and diﬀerentiable components will open
new avenues in the coming years.

5. Conclusions and future directions

Diﬀerentiable programming is the use of new diﬀer-
entiable components beyond classical neural networks.
This generalization of deep learning allows to have data

12

parametrizable architectures instead of pre-ﬁxed ones
and new learning capabilities such as reasoning, atten-
tion and memory.

The ﬁrst models created under this new paradigm,
such as attention mechanisms, diﬀerentiable neural
computers and memory networks, are already having a
great impact on natural language processing.

These new models and diﬀerentiable programming
are also beginning to improve machine learning appli-
cations to dynamical systems. As we have seen, these
models improve the capabilities of RNNs and LSTMs
in identiﬁcation, modeling and prediction of dynamical
systems. They even add a necessary feature in machine
learning such as interpretability.

However, this is an emerging ﬁeld and further re-
search is needed in several directions. To mention a few:

(i) More comparative studies between attention
mechanisms and LSTMs in predicting dynamical
systems.

(ii) Use of self-attention and its possible applications

to dynamical systems.

(iii) As with RNNs, a theoretical analysis (e.g., in the
framework of dynamical systems) of attention and
memory networks.

(iv) Clear guidelines so that scientists without ad-
vanced knowledge of machine learning can use
new diﬀerentiable models in computational sim-
ulations.

Acknowledgments. This work was ﬁnancially sup-
ported by the Spanish Ministry of Science,
Inno-
vation and Universities, grant MTM2016-74921-P
(AEI/FEDER, EU).

References

[1] Y. LeCun, Y. Bengio, G. Hinton, Deep learning, Nature 521

(2015) 436–44. doi:10.1038/nature14539.

[2] I. Sutskever, O. Vinyals, Q. V. Le, Sequence to sequence learn-

ing with neural networks, in: NIPS, 2014.
[3] D. Silver, J. Schrittwieser, K. Simonyan,

I. Antonoglou,
A. Huang, A. Guez, T. Hubert, L. R. Baker, M. Lai, A. Bolton,
Y. Chen, T. P. Lillicrap, F. F. C. Hui, L. Sifre, G. van den Driess-
che, T. Graepel, D. Hassabis, Mastering the game of go without
human knowledge, Nature 550 (2017) 354–359.

[4] G. Marcus, Deep learning: A critical appraisal, ArXiv

abs/1801.00631.

[5] I. Goodfellow, Y. Bengio, A. Courville, Deep Learning, MIT

Press, 2016, http://www.deeplearningbook.org.

[6] D. Bahdanau, K. Cho, Y. Bengio, Neural machine translation by

jointly learning to align and translate, ArXiv 1409.

[7] S. Sukhbaatar, A. Szlam, J. Weston, R. Fergus, End-to-end

memory networks, in: NIPS, 2015.

[8] A. Graves, G. Wayne, M. Reynolds, T. Harley, I. Danihelka,
A. Grabska-Barwinska, S. G. Colmenarejo, E. Grefenstette,
T. Ramalho, J. Agapiou, A. P. Badia, K. M. Hermann, Y. Zwols,
G. Ostrovski, A. Cain, H. King, C. Summerﬁeld, P. Blunsom,
K. Kavukcuoglu, D. Hassabis, Hybrid computing using a neu-
ral network with dynamic external memory, Nature 538 (2016)
471–476.

[9] Z. Wang, D. Xiao, F. Fang, R. Govindan, C. Pain, Y. Guo, Model
identiﬁcation of reduced order ﬂuid dynamics systems using
deep learning, International Journal for Numerical Methods in
Fluids 86. doi:10.1002/fld.4416.

[10] Y. Wang, A new concept using lstm neural networks for dynamic
system identiﬁcation, 2017, pp. 5324–5329. doi:10.23919/
ACC.2017.7963782.

[11] Y. Li, H. Cao, Prediction for tourism ﬂow based on lstm neu-
ral network, Procedia Computer Science 129 (2018) 277–283.
doi:10.1016/j.procs.2018.03.076.

[12] O. Yadan, K. Adams, Y. Taigman, M. Ranzato, Multi-gpu train-

ing of convnets, CoRR abs/1312.5853.

[13] A. Graves, M. Liwicki, S. Fern´andez, R. Bertolami, H. Bunke,
J. Schmidhuber, A novel connectionist system for unconstrained
handwriting recognition, IEEE Transactions on Pattern Analysis
and Machine Intelligence 31 (2009) 855–868.

[14] A. Sherstinsky, Fundamentals of recurrent neural network
(rnn) and long short-term memory (lstm) network, ArXiv
abs/1808.03314.

[15] Y. Lecun, L. Bottou, Y. Bengio, P. Haﬀner, Gradient-based
learning applied to document recognition, Proceedings of the
IEEE 86 (1998) 2278 – 2324. doi:10.1109/5.726791.
[16] R. Yamashita, M. Nishio, R. K. G. Do, K. Togashi, Convolu-
tional neural networks: an overview and application in radiol-
ogy, in: Insights into imaging, 2018.

[17] Y. Bengio, R. Ducharme, P. Vincent, C. Janvin, A neural
probabilistic language model, J. Mach. Learn. Res. 3 (2003)
1137–1155.
URL
944966

http://dl.acm.org/citation.cfm?id=944919.

[18] T. Mikolov, I. Sutskever, K. Chen, G. Corrado, J. Dean,
Distributed representations of words and phrases and their
in: Proceedings of the 26th International
compositionality,
Conference on Neural
Information Processing Systems -
Volume 2, NIPS’13, Curran Associates Inc., USA, 2013, pp.
3111–3119.
URL http://dl.acm.org/citation.cfm?id=2999792.
2999959

[19] H. W. Lin, M. Tegmark, Why does deep and cheap learning
work so well?, Journal of Statistical Physics doi:10.1007/
s10955-017-1836-5.

[20] R. Shwartz-Ziv, N. Tishby, Opening the black box of deep neural

networks via information, ArXiv abs/1703.00810.

[21] P. Hohenecker, T. Lukasiewicz, Ontology reasoning with deep

neural networks, ArXiv abs/1808.07980.

[22] F. Wang, Backpropagation with continuation callbacks : Foun-
dations for eﬃcient and expressive diﬀerentiable programming,
NIPS’18, 2018.

[23] L. Deng, Y. Liu, A Joint Introduction to Natural Language Pro-
cessing and to Deep Learning, Springer Singapore, Singapore,
2018, pp. 1–22.

[24] Y. Goldberg, Neural network methods

lan-
guage processing, Synthesis Lectures on Human Lan-
guage Technologies 10 (2017) 1–309.
doi:10.2200/
S00762ED1V01Y201703HLT037.

for natural

[25] A. G. Baydin, B. A. Pearlmutter, A. A. Radul, J. M. Siskind,
Automatic diﬀerentiation in machine learning: a survey, Journal
of Machine Learning Research 18 (153) (2018) 1–43.

13

URL http://jmlr.org/papers/v18/17-468.html
[26] F. Wang, X. Wu, G. M. Essertel, J. M. Decker, T. Rompf, De-
mystifying diﬀerentiable programming: Shift/reset the penulti-
mate backpropagator, ArXiv abs/1803.10228.

[27] A. Paszke, S. Gross, S. Chintala, G. Chanan, E. Yang, Z. DeVito,
Z. Lin, A. Desmaison, L. Antiga, A. Lerer, Automatic diﬀeren-
tiation in pytorch, in: NIPS-W, 2017.

[28] F. Yang, Z. Yang, W. W. Cohen, Diﬀerentiable learning of
logical rules for knowledge base reasoning (2017) 2316–2325.
URL http://dl.acm.org/citation.cfm?id=3294771.
3294992

[29] D. A. Hudson, C. D. Manning, Compositional attention net-
works for machine reasoning, in: Proceedings of the Interna-
tional Conference on Learning Representations (ICLR), 2018.

[30] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N.
Gomez, L. Kaiser, I. Polosukhin, Attention is all you need, in:
NIPS, 2017.

[31] A. Graves, G. Wayne, I. Danihelka, Neural turing machines,

ArXiv abs/1410.5401.

[32] S. Hochreiter, J. Schmidhuber, Long short-term memory, Neural
computation 9 (1997) 1735–80. doi:10.1162/neco.1997.
9.8.1735.

[33] K. Cho, B. van Merri¨enboer, D. Bahdanau, Y. Bengio, On the
properties of neural machine translation: Encoder–decoder ap-
proaches, in: Proceedings of SSST-8, Eighth Workshop on Syn-
tax, Semantics and Structure in Statistical Translation, Associa-
tion for Computational Linguistics, Doha, Qatar, 2014, pp. 103–
111. doi:10.3115/v1/W14-4012.
URL https://www.aclweb.org/anthology/W14-4012
[34] A. Graves, N. Jaitly, A. rahman Mohamed, Hybrid speech
recognition with deep bidirectional lstm, 2013 IEEE Workshop
on Automatic Speech Recognition and Understanding (2013)
273–278.

[35] G. Tang, M. M¨uller, A. Rios, R. Sennrich, Why self-attention? a
targeted evaluation of neural machine translation architectures,
in: EMNLP, 2018.

[36] A. Hernandez, J. M. Amig´o, Multilayer adaptive networks in
neuronal processing, The European Physical Journal Special
Topics 227 (2018) 1039–1049.

[37] T. Miconi, K. O. Stanley, J. Clune, Diﬀerentiable plastic-
ity: training plastic neural networks with backpropagation, in:
ICML, 2018.

[38] G. Layek, An Introduction to Dynamical Systems and Chaos,

2015. doi:10.1007/978-81-322-2556-0.
[39] L. Arnold, Random Dynamical Systems, 2003.
[40] T. Jackson, A. Radunskaya, Applications of Dynamical Sys-
tems in Biology and Medicine, Vol. 158, 2015. doi:10.1007/
978-1-4939-2782-1.

[41] C. Gros, Complex and adaptive dynamical systems. A primer.

3rd ed, Vol. 1, 2008. doi:10.1063/1.3177233.

[42] S. Pan, K. Duraisamy, Long-time predictive modeling of nonlin-
ear dynamical systems using neural networks, Complexity 2018
(2018) 4801012:1–4801012:26.

[43] P. Dben, P. Bauer, Challenges and design choices for global
weather and climate models based on machine learning, Geo-
scientiﬁc Model Development 11 (2018) 3999–4009. doi:
10.5194/gmd-11-3999-2018.

[44] K. Chakraborty, K. G. Mehrotra, C. K. Mohan, S. Ranka, Fore-
casting the behavior of multivariate time series using neural net-
works, Neural Networks 5 (1992) 961–970.

[45] K. Yeo, I. Melnyk, Deep learning algorithm for data-driven sim-
ulation of noisy dynamical system, Journal of Computational
Physics 376 (2019) 1212 – 1231. doi:https://doi.org/10.
1016/j.jcp.2018.10.024.

[46] K. S. Narendra, K. Parthasarathy, Identiﬁcation and control of

dynamical systems using neural networks, IEEE transactions on
neural networks 1 1 (1990) 4–27.

[47] B. Chang, M. Chen, E. Haber, E. H. Chi, AntisymmetricRNN:
A dynamical system view on recurrent neural networks, in: In-
ternational Conference on Learning Representations, 2019.
URL https://openreview.net/forum?id=ryxepo0cFX

[48] T. Miyoshi, H. Ichihashi, S. Okamoto, T. Hayakawa, Learning
chaotic dynamics in recurrent rbf network, 1995, pp. 588 – 593
vol.1. doi:10.1109/ICNN.1995.488245.

[49] Y. Sato, S. Nagaya, Evolutionary algorithms that generate re-
current neural networks for learning chaos dynamics, in: Pro-
ceedings of IEEE International Conference on Evolutionary
Computation, 1996, pp. 144–149. doi:10.1109/ICEC.1996.
542350.

[50] E. Diaconescu, The use of narx neural networks to predict
chaotic time series, WSEAS Transactions on Computer Re-
search 3.

[51] M. Assaad, R. Bon, H. Cardot, Predicting chaotic time series by
boosted recurrent neural networks, Vol. 4233, 2006, pp. 831–
840. doi:10.1007/11893257\_92.

[52] Y. Bengio, P. Simard, P. Frasconi, Learning long-term depen-
dencies with gradient descent is diﬃcult, IEEE transactions on
neural networks / a publication of the IEEE Neural Networks
Council 5 (1994) 157–66. doi:10.1109/72.279181.

[53] K. Cho, B. van Merrinboer, C. Gulcehre, F. Bougares,
H. Schwenk, Y. Bengio, Learning phrase representations us-
ing rnn encoder-decoder for statistical machine translationdoi:
10.3115/v1/D14-1179.

[54] Y. Qin, D. Song, H. Cheng, W. Cheng, G. Jiang, G. W. Cottrell,
A dual-stage attention-based recurrent neural network for time
series prediction, ArXiv abs/1704.02971.

[55] T. Hollis, A. Viscardi, S. E. Yi, A comparison of lstms and at-
tention mechanisms for forecasting ﬁnancial time series, ArXiv
abs/1812.07699.

[56] P. Vinayavekhin, S. Chaudhury, A. Munawar, D. J. Agravante,
G. D. Magistris, D. Kimura, R. Tachibana, Focusing on what
is relevant: Time-series learning and understanding using atten-
tion, 2018 24th International Conference on Pattern Recognition
(ICPR) (2018) 2624–2629.

[57] S. Serrano, N. A. Smith, Is attention interpretable?, in: ACL,

2019.

[58] Y.-Y. Chang, F.-Y. Sun, Y.-H. Wu, S. de Lin, A memory-network
based solution for multivariate time-series forecasting, ArXiv
abs/1809.02105.

[59] Y. Ming, D. Pelusi, C.-N. Fang, M. Prasad, Y.-K. Wang, D. Wu,
C.-T. Lin, Eeg data analysis with stacked diﬀerentiable neu-
ral computers, Neural Computing and Applications doi:10.
1007/s00521-018-3879-1.

[60] C. Rackauckas, M. Innes, Y. Ma, J. Bettencourt, L. White,
V. Dixit, Diﬀeqﬂux.jl - a julia library for neural diﬀerential equa-
tions, ArXiv abs/1902.02376.

[61] M. Innes, A. Edelman, K. Fischer, C. Rackauckus, E. Saba,
V. Shah, W. Tebbutt, Zygote: A diﬀerentiable programming sys-
tem to bridge machine learning and scientiﬁc computing, ArXiv
abs/1907.07587.

[62] Y. Hu, L. Anderson, T.-M. Li, Q. Sun, N. Carr, J. Ragan-Kelley,
F. Durand, Diﬀtaichi: Diﬀerentiable programming for physical
simulation, ArXiv abs/1910.00935.

[63] F. d. A. Belbute-Peres, K. A. Smith, K. R. Allen, J. B.
Tenenbaum, J. Z. Kolter, End-to-end diﬀerentiable physics for
learning and control, in: Proceedings of the 32Nd Interna-
tional Conference on Neural Information Processing Systems,
NIPS’18, Curran Associates Inc., USA, 2018, pp. 7178–7189.
URL http://dl.acm.org/citation.cfm?id=3327757.
3327820

14

