1
2
0
2

g
u
A
9
2

]
E
S
.
s
c
[

2
v
5
5
6
2
1
.
5
0
1
2
:
v
i
X
r
a

CodeNet: A Large-Scale AI for Code Dataset for
Learning a Diversity of Coding Tasks

Ruchir Puri1, David S. Kung1, Geert Janssen1, Wei Zhang1,
Giacomo Domeniconi1, Vladimir Zolotov1, Julian Dolby1, Jie Chen2,1,
Mihir Choudhury1, Lindsey Decker1, Veronika Thost2,1, Luca Buratti1,
Saurabh Pujar1, Shyam Ramji1, Ulrich Finkler1, Susan Malaika3, Frederick Reiss1

1IBM Research
2MIT-IBM Watson AI Lab
3IBM Worldwide Ecosystems

Abstract

Over the last several decades, software has been woven into the fabric of every
aspect of our society. As software development surges and code infrastructure of
enterprise applications ages, it is now more critical than ever to increase software
development productivity and modernize legacy applications. Advances in deep
learning and machine learning algorithms have enabled breakthroughs in computer
vision, speech recognition, natural language processing and beyond, motivating
researchers to leverage AI techniques to improve software development efﬁciency.
Thus, the fast-emerging research area of “AI for Code” has garnered new interest
and gathered momentum. In this paper, we present a large-scale dataset CodeNet,
consisting of over 14 million code samples and about 500 million lines of code
in 55 different programming languages, which is aimed at teaching AI to code.
In addition to its large scale, CodeNet has a rich set of high-quality annotations
to benchmark and help accelerate research in AI techniques for a variety of crit-
ical coding tasks, including code similarity and classiﬁcation, code translation
between a large variety of programming languages, and code performance (runtime
and memory) improvement techniques. Additionally, CodeNet provides sample
input and output test sets for 98.5% of the code samples, which can be used as
an oracle for determining code correctness and potentially guide reinforcement
learning for code quality improvements. As a usability feature, we provide several
pre-processing tools in CodeNet to transform source code into representations that
can be readily used as inputs into machine learning models. Results of code classi-
ﬁcation and code similarity experiments using the CodeNet dataset are provided as
a reference. We hope that the scale, diversity and rich, high-quality annotations of
CodeNet will offer unprecedented research opportunities at the intersection of AI
and Software Engineering.

1

Introduction

There is a growing trend towards leveraging AI for building tools that support software engineering
and development [1, 2]. AI can manipulate and generate computer code, but can it do so with
high quality? Many researchers are fascinated by this possibility, encouraged by AI successes in
other domains and tantalized by the vision of computers programming computers. Some recent
deep-learning models [3, 4] for code have received a lot of publicity: trained on vast amounts of
data and using novel architectures with billions of parameters, they sometimes generate surprisingly
plausible code.

Preprint. Under review.

 
 
 
 
 
 
Given the success of non-AI tools for code, why should we consider AI to augment or possibly
replace them? Firstly, AI can help reﬁne and re-tune the heuristics used by traditional coding tools.
Secondly, based on the training data from past experience, AI can help prioritize when there is more
than one sound answer [5]. Thirdly, an AI-based tool may handle incomplete or invalid code more
robustly, thus expanding its scope. Finally, AI can incorporate signals usually ignored by traditional
tools for code, such as the natural language in identiﬁers or comments.

In the enterprise environment, developers often face code written by large teams over many years
and geographies. Developers must manipulate such code to modernize it, ﬁx bugs, improve its
performance, evolve it when requirements change, make it more secure, and/or comply with regu-
lations. These tasks are challenging, and it is crucial to provide tool support for developers to be
more productive at performing them. It is well known that the latest advancements in deep learning
algorithms rely on best-of-breed datasets, such as ImageNet, to create increasingly complex and
powerful models. In this paper, we present "CodeNet", a ﬁrst-of-its-kind dataset in scale, diversity,
and quality, to accelerate the algorithmic advances in AI for Code.

To promote widespread adoption of CodeNet, we will be launching contests involving use cases based
on the dataset. The ﬁrst contest [6] will focus on diversity, inclusion and spurring interest among
aspiring data scientists. We are partnering with the Global Women in Data Science organization (with
presence in over 50 countries) founded by Stanford University [7] and targeting teams with at least
ﬁfty percent women. We are planning follow-up contests that target experienced AI practitioners.

The rest of the paper is organized as follows. Section 2 introduces the CodeNet dataset. Related
datasets are discussed in Section 3, and the differentiation of CodeNet with respect to these related
datasets is elaborated in Section 4. Section 5 describes how CodeNet was curated and Section 6
enumerates the usability features of CodeNet with several pre-processing tools to transform source
codes into representations that can be readily used as inputs into machine learning models. Section 7
discusses the upcoming CodeNet contest and Section 8 describes important baseline experiments
with the CodeNet dataset. Section 9 presents further uses of the CodeNet dataset and Section 10
concludes the paper.

2 The CodeNet Dataset

The CodeNet dataset consists of a large collection of code samples with extensive metadata. It
also contains documented tools to transform code samples into intermediate representations and to
access the dataset and make tailored selections. Our goal is to provide the community with a large,
high-quality curated dataset that can be used to advance AI techniques for source code.

CodeNet is derived from the data available on two online judge websites: AIZU [8] and AtCoder [9].
Online judge websites pose programming problems in the form of courses and contests. The dataset
consists of submissions to these problems, which are judged by an automated review process for
correctness. Problem descriptions, submission outcomes, and associated metadata are available via
various REST APIs.

Scale and Statistics. CodeNet contains a total of 13,916,868 submissions, divided into 4053
problems. Among the submissions, 53.6% (7,460,588) are accepted (compilable and pass the
prescribed tests), 29.5% are marked with wrong answer, and the remaining rejected due to their
failure to meet run time or memory requirements. To our knowledge, this is the largest dataset so
far among similar kinds. Submissions are in 55 different languages; 95% of them are coded in C++,
Python, Java, C, Ruby, and C#. C++ is the most common language, with 8,008,527 submissions (57%
of the total), of which 4,353,049 are accepted. With the abundance of code samples, users can extract
large benchmark datasets that are customized to their downstream use. See Figure 1 for a summary.

Diversity. The problems in CodeNet are mainly pedagogical and range from elementary exercises
to sophisticated problems that require advanced algorithms. The submitters range from beginners
to experienced coders. Some submissions are correct while others contain different types of errors,
accordingly labeled. The submissions are in many different languages.

Code Samples. Each code sample is a single ﬁle and includes inputting the test cases and printing out
the computed results. The ﬁle name uses standard extensions that denote the programming language,
e.g., .py for Python. The majority of code samples contain only one function, although submissions
to more complex problems might have several functions.

2

(a) Languages

(b) Status

Figure 1: Percentage of submissions per language (left) and per status (right).

Metadata. The metadata enables data queries and selections among the large collection of problems,
languages, and source ﬁles. The metadata is organized in a two level hierarchy. The ﬁrst is the
dataset level, which describes all problems. The second is the problem level, which details all the
submissions to a single problem. Metadata and data are separated in the dataset structure.

At the dataset level, a single CSV ﬁle lists all problems and their origins, along with the CPU time
and memory limits set for them. Additionally, every problem has an HTML ﬁle with a detailed
description of the problem, the requirements and constraints, and the IO examples.

At the problem level, every problem has a CSV ﬁle. The metadata for each submission is summarized
in Table 2 below, which lists the ﬁelds contained in each CSV ﬁle as well as the corresponding
descriptions.

2.1 How to read the CodeNet dataset

The data and metadata are organized in a rigorous directory structure. The top level Project_CodeNet
directory contains several sub-directories: data, metadata, problem_descriptions, and
derived. The code samples or submissions reside under the data directory. The data directory
is organized as (problem_id)/(language)/(submission), so the ﬁle path data/p00023/C++/
s006384060.cpp denotes a submission to problem p00023 in C++ with id s006384060. Detailed
statement of the problems can be found in problem_descriptions/(problem_id).html. The
meta data for the dataset is contained in the metadata directory. metadata/problem_list.csv
contains metadata for all the problems in the dataset, which is summarized in Table 1. metadata/
(problem_id).csv contains the metadata for all the submissions to problem problem_id, which is
described in Table 2. Each submission comes with cpu time, memory usage and status with possible
values described in Table 3. The derived directory contains information derived from the dataset,
such as near-duplicate information for submissions to speciﬁc languages, token sequences for code
samples, and information on identical problems.

Table 1: Metadata at the dataset level

name of column
id
name
dataset
time_limit
memory_limit
rating
tags
complexity

data type
string
string
string
int
int
int
string
string

unit

description

unique anonymized id of the problem
short name of the problem
original dataset, AIZU or AtCoder

none
none
none
millisecond maximum time allowed for a submission
KB
none
none
none

maximum memory allowed for a submission
rating, i.e., difﬁculty of the problem
list of tags separated by "|"; not used
degree of difﬁculty of the problem; not used

3

Table 2: Metadata at the problem level

name of column
submission_id
problem_id
user_id

data type
string
string
string

date

language

int

string

original_language

string

ﬁlename_ext

status
cpu_time
memory
code_size
accuracy

string

string
int
int
int
string

unit

none
none
none

seconds

none

none

none

none
millisecond
KB
bytes
none

description

unique anonymized id of the submission
anonymized id of the problem
anonymized user id of the submission
date and time of submission in the Unix
timestamp format (seconds since the epoch)
mapped language of the submission
(ex: C++14 ->C++)
original language speciﬁcation
extension of the ﬁlename that indicates
the programminglanguage used
acceptance status, or error type
execution time
memory used
size of the submission source code in bytes
number of tests passed; *Only for AIZU

Table 3: All the possible status values
abbreviation
status
CE
Compile Error
WA
Wrong Answer
Time Limit Exceeded
TLE
Memory Limit Exceeded MLE
Accepted
Judge Not Available
Output Limit Exceeded
Runtime Error
WA: Presentation Error
Waiting for Judging
Waiting for Re-judging
Internal Error
Judge System Error

AC
JNA
OLE
RE
PE
WJ
WR
IE

numeric code
0
1
2
3
4
5
6
7
8

Table 4 summarizes the metadata available for each code submission to a problem. Figure 2 gives the
distributions of problems based on number of submissions received.

Table 4: Submission metadata.

column
submission_id
problem_id
user_id
date
language
original_language
ﬁlename_ext
status
cpu_time
memory
code_size
accuracy

unit/example
s[0-9]{9}
p[0-9]{5}
u[0-9]{9}
seconds
C++
C++14
.cpp
Accepted
millisecond
kilobytes
bytes
4/4

description
anonymized id of submission
anonymized id of problem
anonymized user id
date and time of submission
consolidated programming language
original language
ﬁlename extension
acceptance status, or error type
execution time
memory used
source ﬁle size
passed tests (AIZU only)

Limitations. All code samples in CodeNet may not be extensively commented, and these comments
may be in multitude of languages. Therefore, AI techniques that rely on learning from preponderance
of comments in the code may face challenges. The code samples are solutions to high-school and

4

Figure 2: Number of problems providing at least X submissions. The bars show both the numbers of
accepted submissions (blue) and rejected submissions (orange).

beginning college level programming problems. This dataset is not suitable for users looking for code
with enterprise API’s and advanced design patterns.

3 Related Datasets

A wide variety of datasets for source code exist, with many targeting one or a small number of
tasks. Such tasks include clone detection, vulnerability detection [10, 11], cloze test [12], code
completion [13, 14], code repair [15], code-to-code translation, natural language code search [16],
text-to-code generation [17], and code summarization [16]. A detailed discussion of several of these
tasks and their respective datasets is available in CodeXGLUE [18], which is a collection of existing
datasets. CodeNet, on the other hand, is a new dataset curated from scratch, that aims to support a
broad set of use cases. Popular datasets of a similar kind are POJ-104 [19] (which is incorporated as
part of CodeXGLUE as well) and GCJ [20] (derived from Google Code Jam). We compare CodeNet
to these datasets in the following.

3.1 POJ-104

POJ-104 was collected from a pedagogical online judge system. The code samples are submissions
to 104 programming problems. With 500 submissions to each problem, there is a total of 52,000 code
samples in the dataset. This dataset has been used by many authors for code classiﬁcation [19] and
code similarity [21].

POJ-104 is faced with several limitations.

1. The code samples are in C and C++, but the two languages are not distinguished. Although they are
closely related, mixing them leads to parsing errors and a reduction of useful code samples [21].

2. Useful metadata such as the results of the judging system (acceptance, error types etc.) are missing.
Therefore, for certain applications where compilabilty or code correctness is important, additional
pre-processing efforts are needed and useful code samples are reduced [21]. The dataset does
not contain the problem statement, although some example problems are described in [22], and
information on how to execute the code samples is absent.

3. Some problems are identical (e.g., problems 26 and 62), and some submissions are near duplicates

of each other, although the percentage of such cases is low compared to other datasets.

3.2 GCJ

GCJ [20] was collected from the submissions to the Google Code Jam competitions from 2008 to
2020. Similar to CodeNet, the submissions cover a wide variety of programming languages, with
C++, Java, Python, and C being the predominant ones. The C++ subset has been extracted into a
POJ-104-like benchmark and used in some publications. This benchmark dataset, GCJ-297 [23],
has 297 problems and approximately 280K submissions. The number of submissions is imbalanced
among problems.

5

GCJ is advantageous over POJ-104 in size and language diversity, but we believe that an even
larger dataset such as CodeNet can better serve the community. GCJ contains neither metadata nor
information on identical problems and near duplicates.

4 CodeNet Differentiation

Table 5: Related datasets comparison

Total number of problems
Number of programming languages
Total number of code samples
C++/C subset data size (code samples)
Percentage of problems with test data
Task: Memory Consumption Prediction
Task: Runtime Performance Comparison
Task: Error Prediction
Task: Near duplicate prediction

CodeNet
4053
55
13,916,828
8,008,527
51%
Yes
Yes
Yes
Yes

GCJ
332
20
2,430,000
280,000
0%
No
No
No
No

POJ
104
2
52,000
52,000
0%
No
No
No
No

A high quality code dataset has certain desired properties. We constructed CodeNet according to
these requirements. In the following, we discuss how CodeNet differentiates itself from the existing
datasets along these lines. Table 5 is a comparison with related datasets.

Large scale. A useful dataset should contain a large number and variety of data samples to expose
the realistic and complex landscape of data distributions one meets in practice. CodeNet is the
largest dataset in its class - it has approximately 10 times more code samples than GCJ and its C++
benchmark is approximately 10 times larger than POJ-104.

Rich annotation. For the dataset class in question, it is important to include information beyond
which problem a code sample solves to enable a wide range of applications and use cases. It is useful
to know whether a code sample solves the problem correctly, and if not, the error category (e.g.,
compilation error, runtime error, and out-of-memory error). Since the source code is supposed to
solve a programming problem, it is advantageous to know the problem statement and have a sample
input for execution and a sample output for validation. All such extra information is part of CodeNet
but absent in GCJ and POJ-104.

Clean samples. For effective machine learning, the data samples are expected to be independent
and identically distributed (iid); otherwise, the resulting performance metric could be signiﬁcantly
inﬂated [24]. The existence of duplicate and/or near duplicate code samples makes the iid assumption
dubious. Hence, it is crucial to identify the near duplicates. The presence of identical problems in the
dataset poses an even bigger issue. In CodeNet, we analyzed the code samples for (near) duplication
and used clustering to ﬁnd identical problems. This information is made available as part of the
dataset release but it is absent in GCJ and POJ-104.

5 Construction of CodeNet

5.1 Collection of Code Samples

The CodeNet dataset contains problems, submissions, and metadata, scraped from the AIZU and
AtCoder online judging systems. For AIZU, we used the provided REST APIs to download all the
metadata. For AtCoder, due to the absence of a REST API, we scraped the problems, submissions,
and metadata directly from the web pages. We considered only public and non-empty submissions
that did not contain errors or inconsistencies in the metadata. We manually merged the information
from the two sources and adopted a uniﬁed format to create a single dataset.

6

5.2 Cleansing

Because data are collected from different sources, we apply a consistent character encoding (UTF-8)
on all raw data ﬁles. Additionally, we remove byte-order marks and use Unix-style line-feeds as the
line ending.

As indicated in section 4, we identify near-duplicates. We follow Allamanis [24] and use Jaccard
similarity [25] as a metric to score code pairs. Each code sample is tokenized and stored as a
bag-of-tokens multiset. In our case, we keep all tokens except comments and preprocessor directives.
We compute the set and multiset Jaccard indices and respectively use 0.9 and 0.8 as the near-duplicate
thresholds.

Besides similar code samples, identical problems are also likely because they have been gathered over
many decades. We go through the problem description ﬁles (in HTML format) and apply fdupes to
extract identical problem pairs. Additionally, using the near-duplicate information calculated for code
samples, we consider a problem pair to be a potential duplicate when the number of near-duplicate
code pairs exceeds a threshold. Clustering of duplicate problems is illustrated by the graphs in
Figure 3, where each node denotes a problem and an edge between two nodes is labeled by the
number of near-duplicate code pairs. Each connected graph is then a cluster of potential duplicate
problems and we manually inspect the problem descriptions to verify the correctness of this duplicate
detection.

Figure 3: An example of a near-duplicate problem graph.

5.3 Benchmark Datasets

CodeNet has a rich set of code samples, and the user can assemble a customized benchmark according
to his/her need. Following POJ-104, we extracted benchmark datasets from CodeNet in C++, Python,
and Java. The benchmark characteristics are shown in Table 6. For the C++ benchmarks, the number
of problems and their solutions are chosen to make the benchmark challenging. The benchmarks are
ﬁltered in the following ways. Each code sample is “unique” in the sense that it is not a near-duplicate
of another code sample. The same is true of each problem. Samples with a large fraction of dead code
are excluded. Each code sample has successfully passed through the tokenizer, the SPT generator,
and the graph generator, all described in the next section. This step is to ensure that proper processing
can be done to convert a code sample to a machine learning model input.

6 Code Representation and Tools

Machine learning with source code requires proper abstractions of the code. The abstractions are
instantiated as representations in speciﬁc formats. As a usability feature, we provide several pre-
processing tools to transform source codes into representations that can readily be used as inputs into
machine learning models. They are described as follows.

Tokenizer. We offer fast C implementations of tokenizers for C, C++, Java, Python, and JavaScript.
Additionally, the parse-tree generator described next can also produce token streams for C, C++, Java,
and Python and can easily be extended to more languages.

Simpliﬁed Parse Tree (SPT) Simpliﬁed parse trees are derived from parse trees generated using
ANTLR4 [26]. We traverse the ANTLR4 parse tree and remove internal nodes that only have one
child. By doing so, we maintain the essential structure of the parse tree while pruning out unnecessary
parser production rules. Finally, we adopt Aroma’s [27] naming convention: leaf nodes are named by

7

p1341p535p4243164p1620p56419p237p6216p261522p5844p85328their literal strings and internal nodes are named by a concatenation of their children’s names (only
reserved words are kept while others are replaced by a hash mark #). We produce features for each
node: (1) node type (token or parsing rule); (2) token type (e.g., an identiﬁer), when applicable; (3)
parsing rule type (e.g., an expression), when applicable; and (4) whether it is a reserved word. We
adopt an extensible JSON graph schema so that edges can be augmented with types when needed.
Currently, we support generating SPTs for four languages: C, C++, Java, and Python. Table 6
summarizes the SPT statistics for the four benchmarks.

Table 6: Benchmark statistics.
C++1000
1,000
500,000
188,449,294
187,949,294

C++1400
1,400
420,000
198,258,050
197,838,050

Python800
800
240,000
55,744,550
55,504,550

Java250
250
75,000
25,449,640
25,374,640

#problems
#samples
#SPT-nodes
#SPT-edges

Code graphs. We augment the tool chain with a code graph generator using WALA [28], a general
framework for program analysis. The backbone of a code graph is a system dependence graph, which
is an inter-procedural graph of program instructions (e.g. call, read) expressing control ﬂow and
data ﬂow information as edges. We also generate inter-procedural control ﬂow graphs, which are
control ﬂow graphs of all the methods in the program, stitched together to connect call sites with
target methods. Our code graph tool currently supports only Java and Python, but we plan to support
more languages such as Javascript.

7 CodeNet Challenge

The launch of CodeNet was well received by the AI community and the media, with coverage
from Forbes[29], VentureBeat[30], ZDNet[31] and others. Within a short span of 3 months, our
github received 1000 stars and has been forked over 119 times. Our vision is to use CodeNet as an
umbrella to curate AI for code datasets for widespread adoption and to drive innovation in AI for
code. To leverage the momentum of CodeNet, we will be launching CodeNet challenges to create
excitement in the AI community. The ﬁrst contest [6] is mainly pedagogical and targets aspiring
data scientists. In addition, we are partnering with the Global Women in Data Science organization
(with presence in over 50 countries) founded by Stanford University [7] to emphasize diversity and
inclusion (teams must have at least ﬁfty percent women). We will organize workshops to introduce
the topic, code similarity, and provide educational materials. This contest will be kicked off in late
September and the winner will be announced in early December, around the NeurIPS2021 time
frame. The conclusion of the ﬁrst contest will be followed by a contest that will target experienced AI
practitioners. Potential contest topics will revolve around practical and compelling use cases such as
code language translation, code repair, code performance improvement, and code memory reduction.

8 Experiments with the CodeNet Dataset

In this section, we report the results of a code classiﬁcation task, a similarity task, a generalization
task, and a token inference task, using the four benchmark datasets (see Table 6) extracted from
CodeNet. For this paper, these experiments are not meant to achieve the best-of-breed results using
the state of the art. Our intention is to provide a set of baseline results as a reference. The experiments
are typically performed on a Xeon machine using P100 or V100 GPUs. Code and scripts for these
experiments are in the model-experiments folder of the CodeNet repository [32].

8.1 Code Classiﬁcation

In the classiﬁcation task, each problem corresponds to a class: a code sample belongs to a class if it
is a submission to the corresponding problem. For each experiment, 20% of the code samples are
used for testing, while the rest are split in 4:1 for training and validation, respectively. We experiment
with a diverse set of machine learning methods: bag of tokens, sequence of tokens, BERT model, and
graph neural networks (GNNs).

8

1. MLP with bag of tokens. A code sample is represented by a vector of relative frequencies of
token occurrences. Only operator and keyword tokens are used. The model is a 3-layer multilayer
perceptron (MLP).

2. CNN with token sequence. We use the same set of tokens as above but retain their order to form
a sequence. All sequences have the same length under zero padding. The classiﬁcation model is a
convolutional neural network (CNN) with an initial token embedding layer.

3. C-BERT with token sequence. Treating a code sample as a piece of natural language text, we
build a C-BERT model [33] through pretraining on 10K top starred Github projects written in C.
We use the Clang C tokenizer and Sentencepiece to tokenize each code sample. The pretrained
model is ﬁne-tuned on each benchmark.

4. GNN with SPT. Based on the parse tree representation, we use graph convolutional networks
(GCN) [34] and graph isomorphism networks (GIN) [35] as well as their variants as the prediction
model. The variant adds a virtual node to the graph to enhance graph message passing [36].
5. GNN with Code Graph. We also apply GCN on the code graph representation of the code.

Table 7: Classiﬁcation accuracy (in %).

MLP w/ bag of tokens
CNN w/ token sequence
C-BERT
GNN (GCN)
GNN (GCN-V)
GNN (GIN)
GNN (GIN-V)
Code Graph+GCN

Java250
71.00±0.29
89.52±0.59
97.40±0.19
92.70±0.25
93.02±0.81
93.26±0.23
92.77±0.66
94.10±.001

Python800
67.80±0.15
87.46±0.25
97.09±0.18
93.82±0.16
94.30±0.15
94.17±0.19
94.54±0.12
87.80±.007

C++1000
68.26±0.21
93.96±0.18
93.79±0.01
95.76±0.12
96.09±0.17
96.34±0.15
96.64±0.10
N/A

C++1400
64.50±0.13
93.71±0.18
91.83±0.06
95.26±0.13
95.73±0.07
95.95±0.13
96.36±0.10
N/A

Table 7 summarizes the classiﬁcation accuracy for all models on all benchmarks. Despite the
simplicity of bag of tokens, it achieves well over 60% accuracy. Maintaining token ordering,
CNN with token sequence offers signiﬁcant improvement, reaching approximately 90% across all
benchmarks.

More complex neural models sometimes further improve the prediction performance, as witnessed by
C-BERT, which reaches approximately 97% for both Java and Python. It is interesting to note that
even though C-BERT is pre-trained with C programs, its performance on the two C++ benchmarks is
less impressive. We speculate that such a lower performance is related to programming practices. For
C++, it is common to have identical program construction, such as declaration of constants (e.g., pi
and epsilon) and data structures, appear across C++ submissions to different problems, but such a
practice is rare in Java and Python.

Overall, the GNN models exhibit competitive performance. They are consistently the top performers,
if not the best. The code graph representation slightly improves over the SPT representation on Java,
but performs less well on Python.

Further details of each model, along with the experiment environment, are given below.

8.1.1 Details of Experiments on Code Classiﬁcation

MLP with Bag of Tokens
One of the simplest representations of a code sample is a bag of tokens. Here, the code sample is
represented by a vector of relative frequencies of token occurrences in the source code. The vector is
computed by the following steps:

1. Convert a given source code into a sequence of tokens using a tokenizer (i.e., lexical analyzer).
2. From this sequence, remove the tokens considered not useful for code classiﬁcation.
3. Count the number of each token type in the reduced sequence and form a vector of counts.
4. Normalize the vector with respect to L2 norm.

We do not use all tokens available in the grammar of the programming language. Only some operators
and keywords are used. All identiﬁers, comments and literals are ignored. We also ignore some

9

operators and many keywords that in our opinion provide no signiﬁcant information on the algorithm
the source code implements.

The vector representing a bag of tokens has the same length for every code sample, which makes
it convenient for processing with a neural network. The vector is usually short, which makes
training of a neural network fast. However, in a bag-of-tokens representation, information about the
number of occurrences and position of each token is lost. Hence, the accuracy of a classiﬁer using a
bag-of-tokens representation is rather limited.

Table 8 provides results of code classiﬁcation of all four benchmarks. The columns give the benchmark
name, the test accuracy, the number of training epochs, the run time of each epoch, and the number
of token types considered. All networks are implemented using Keras API of TensorFlow machine
learning tool. Training is performed on a single V100 GPU, using Adam optimizer with learning rate
1e-3, and batches of 32 samples. In each experiment, 20% of the samples are used for testing, while
the rest are split in 4:1 for training and validation, respectively.

Table 8: Code classiﬁcation by MLP with bag of tokens.
Run time Number
Benchmark
tokens
sec/epoch
dataset
81
2
Java250
71
7
Python800
56
14
C++1000
56
12
C++1400

Accuracy Number
epochs
30
22
20
17

%%
71.00±0.29
67.80±0.15
68.26±0.21
64.50±0.13

Figure 4 shows the neural network used for solving the classiﬁcation problem for the C++1400
benchmark. The neural networks used for classiﬁcation of other benchmarks are similar to this one.
As we see in Table 8 their performance is quite similar.

Figure 4: MLP architecture for code classiﬁcation.

From Table 8 we see that training is rather fast, the reason being that the network is simple. In
spite of simplicity, this neural network performs very well. The 64.50±0.13% test accuracy for
C++1400 benchmark dataset is signiﬁcantly better than the potential 0.071% accuracy of random
guess. It indicates that the relative frequencies of source code tokens provide sufﬁcient information
for classifying code.

CNN with Token Sequence
The sequence-of-tokens representation retains more information of a code sample than the bag-of-

10

tokens representation. For our experiments on code classiﬁcation, we use the same set of tokens that
is used in the above bag-of-tokens approach. Similarly, we omit all comments and identiﬁers.

Table 9: Code classiﬁcation by CNN with token sequence.
Run time Number
Benchmark
tokens
sec/epoch
dataset
81
10
Java250
71
26
Python800
56
59
C++1000
56
60
C++1400

Accuracy Number
epochs
810
504
235
334

%%
89.52±0.59
87.46±0.25
93.96±0.18
93.71±0.18

Table 9 shows results of code classiﬁcation on all four benchmarks by using the sequence-of-tokens
representation. The columns give the benchmark name, the test accuracy, the number of training
epochs, the run time of each epoch, and the number of token types considered. All networks are
implemented using Keras API of TensorFlow machine learning tool. The training is performed on
four V100 GPUs, using Adam optimizer in data parallel mode with learning rate 1e-3, and batches of
512 samples. In each experiment, 20% of the samples are used for testing, while the rest are split in
4:1 for training and validation, respectively.

We have experimented with several types of neural networks. Figure 5 shows the neural network
we choose for the C++1400 benchmark. It is a multi-layer convolutional neural network. It uses
categorical encoding of source code tokens. For batching, the sequences of tokens are padded with
zeros.

Using this network we get a test accuracy 93.71±0.18% for C++1400 benchmark dataset, which is
signiﬁcantly better than the accuracy shown by the bag-of-tokens approach. The neural networks
used for classiﬁcation of other benchmarks are similar to the one shown in Figure 5. As we see in
Table 9, their performance is similar.

C-BERT with Token Sequence
The sequence-of-tokens representation can be used with other neural networks of increasing capacity.
We build a C-BERT model (a transformer model introduced in [33]) by pre-training on 10,000 top
starred GitHub open source projects written in C, where we use Clang C tokenizer and Sentencepiece
to tokenize the pre-training data. The C-BERT model is then ﬁne tuned on each classiﬁcation
benchmark. Additionally, we experiment with the POJ-104 dataset, which contains code examples in
C and C++.

C-BERT achieves appealing results on binary classiﬁcation and vulnerability detection with C source
code [10, 37]. However, it has not been used on multiclass classiﬁcation tasks or with other languages
such as C++, Java, and Python. Because we use sub-word tokenization and different programming
languages share common tokens, we could apply the C-BERT model directly on the benchmarks.

After pretraining, we ﬁne tune the model for ﬁve epochs on each benchmark, with a batch size 32 and
learning rate 2e-5. The ﬁne-tuning was done on two V100 GPUs and it took 30 minutes to four hours,
depending on the size of the dataset. The sub-word vocabulary size is 5,000. Contexts larger than
512 tokens were truncated.

Table 10 summarizes the accuracies C-BERT achives on the four CodeNet benchmarks as well as the
POJ-104 dataset. C-BERT achieves high accuracy and performs the best on Java and Python.

Table 10: C-BERT results (accuracy, in %) for code classiﬁcation.

C-BERT

POJ-104
98.41±0.01

C++1000
93.79±0.01

C++1400
91.83±0.06

Java250
97.40±0.19

Python800
97.09±0.18

The relatively low performance on C++ benchmarks is possibly related to the idiosyncrasies of the
dataset and certain programming practices. Manual inspection suggests that lack of detailed variable
names in C++ hurts the performance of the model, in problems appearing similar and having similar
solutions. Removing one of the similar problems improves the model performance on the other
problem. Moreover, one programming practice which could potentially confuse the models is that
certain C++ users copied common constants (e.g., pi and epsilon) and data structures (e.g., enums) to
all solutions they submitted. In many cases, these duplicate contents were not even used. We did not
observe such practices in Python and Java.

11

Figure 5: CNN architecture for code classiﬁcation.

GNN with SPT
We experiment with four types of GNNs with SPT-based graph representations of the source code:
the Graph Convolutional Network (GCN) [34], the Graph Isomorphism Network (GIN) [35], and a
virtual-node-included variant for each (denoted by -V). The variant adds a virtual node to the graph
to enhance graph message passing [36]. We use the Adam optimizer with learning rate 1e-3 for
training. All GNN models have ﬁve layers. We have experimented with more than 5 layers (i.e., 8
and 10), however deeper GNNs do not improve performance, as deeper GNNs might suffer from
the over-smoothing problem (i.e., node features become less distinguishable after many rounds of
message passing) [38].

We conduct 6/2/2 random split for each of the 4 benchmarks: i.e., 60% training data, 20% testing
data, and 20% validation data. We run ﬁve folds for each benchmark with early stop ”patience”
set 20 (i.e., stop only when validation loss has not decreased in the past 20 epochs). Our model
training typically converges within 200 epochs in a 1-fold run. We modiﬁed OGB [39] code-base with
PyTorch Geometric [40] back-end over PyTorch 1.6.0 [41] to run our experiments. The experiments
are conducted on one NVIDIA V100 GPU. For large benchmarks such as C++1000 and C++1400, it
takes about 1 week to ﬁnish a 5-fold run. We summarize model accuracy, training time over 5-folds,
and training epochs over 5-folds in Table 11. As we can see, adding a virtual node improves GNN
performance (both GCN and GIN). Overall, GIN and its variants work better than GCN and its

12

SoftMaxwhile(<{)}+=*;Convolution 15x512 with ReLUConvolution 5x320 with ReLUConvolution 1x256Dense layer 256x512 with ReLUDense layer 512x1024 with ReLUGlobal Max PoolingDropout layerDense layer 1024x1000variants, likely due to the fact that GIN theoretically generalizes the Weisfeiler-Lehman Isomorphism
Test and achieves maximum expressive power among GNNs [42].

For the detailed model, hyper-parameter setup, data splits and etc, please refer to https://github.
com/IBM/Project_CodeNet/tree/main/model-experiments/gnn-based-experiments.

Table 11: GNN (SPT) results for code classiﬁcation. Each task trains over 5-folds with early stopping
patience parameter set as 20. We record test accuracy (with standard deviation), total training time
over 5 folds, and total training epochs over 5 folds.

GCN

Java250
92.70±0.25
10.55 hrs
411 epochs
GCN-V 93.02±0.81

GIN

GIN-V

12.50 hrs
419 epochs
93.26±0.23
19.80 hrs
513 epochs
92.77±0.66
26.25 hrs
656 epochs

Python800
93.82±0.16
14.50 hrs
219 epochs
94.30 ±0.15
23.02 hrs
325 epochs
94.17±0.19
41.67 hrs
496 epochs
94.54±0.12
51.67 hrs
570 epochs

C++1000
95.76±0.12
47.96 hrs
228 epochs
96.09±0.17
61.55 hrs
287 epochs
96.34±0.15
116.67 hrs
441 epochs
96.64±0.10
142.25 hrs
496 epochs

C++1400
95.26±0.13
67.34 hrs
310 epochs
95.73±0.07
71.85 hrs
358 epochs
95.95±0.13
133.50 hrs
502 epochs
96.36±0.10
208.47 hrs
678 epochs

8.2 Code Similarity

In the similarity task, two pieces of code samples are considered similar if they solve the same problem
(type-4 similarity in [43]). Note that textual similarity does not guarantee similarity in functionality.
For example, programs that differ by only one token might behave very differently; hence, they are
not considered similar. For the token-based experiments, we treat the problem as binary classiﬁcation.
We use the same training, validation and testing split as in classiﬁcation. Code pairs are randomly
sampled within each subset. The number of similar pairs is the same as dissimilar ones. For the SPT
representation, we experiment with several popular techniques, including AROMA [27], MISIM [21],
and GMN [44]. The following contains more details about the models and methods.

1. MLP with bag of tokens. This model is the same as the one for code classiﬁcation, except that

the input is a concatenation of the two bag-of-tokens vectors from each program.

2. Siamese network with token sequence. The token sequence is the same as the one for code

classiﬁcation. The model is a Siamese network with two CNNs with shared weights.

3. SPT with handcrafted feature extraction: The method AROMA [27] uses normalized SPT
node names and handcrafted rules to extract feature vectors for each SPT. Then, similarity is
computed as a dot product of the extracted feature vectors.

4. GNN with SPT: With the same SPT, on the other hand, MISIM [21] uses a graph neural network
to extract high-level features, and uses the cosine similarity of the extracted features to compute
similarity. Additionally, we apply graph matching network (GMN) [44], which uses a cross-graph
attention mechanism to learn pair-wise structural similarity of graphs, on the SPT pairs to predict
similarity. The implementation is adapted from [45].

Table 12: Similarity accuracy (in %).

MLP w/ bag of tokens
Siamese w/ token sequence

Java250
81.80±0.06
89.70±0.18

Python800
86.61±0.08
94.67±0.12

C++1000
85.82±0.05
96.19±0.08

C++1400
86.54±0.07
96.56±0.07

Table 12 summarizes the classiﬁcation accuracy for the ﬁrst two models. The performance of bag
of tokens is modest, considering that the problem is a binary classiﬁcation with perfectly balanced
classes. On the other hand, the Siamese model signiﬁcantly outperforms bag of tokens, as expected.

Table 13 summarizes the MAP@R [46] score for two SPT-based approaches with solutions for 50%
problems used for training, 25% for validation, and 25% for test. MISIM GNN model is trained for

13

Table 13: Similarity MAP@R score.

Rule-based w/ SPT (AROMA)
GNN w/ SPT (MISIM)

Java250
0.19
0.64±0.007

Python800
0.19
0.65±0.003

C++1000
0.17
0.78±0.005

C++1400
0.15
0.77±0.002

1000 epochs. AROMA results in a relatively low score because the feature extraction is rule-based
and no model is learned, whereas MISIM uses a neural network to extract features through supervised
training.

Table 14: Similarity MAP@R score on Java250.

GNN w/ SPT (MISIM, structure only)
GNN w/ SPT (GMN, structure only)
GNN w/ SPT (GMN + MISIM node attributes)

(p4, s5)
0.472±0.023
0.679±0.056
0.985±0.015

(p3, s300)
0.194±0.010
0.432±0.035
0.794±0.036

(p10, s300)
0.096±0.009
0.256±0.015
0.780±0.026

Exploring further into the Java250 benchmark, Table 14 summarizes the MAP@R score with a variety
of test sets: (p4, s5), (p3, s300), and (p10, s300), indicating 4, 3, and 10 problems with 5, 300 and
300 solutions each respectively. Across all test sets, GMN outperforms MISIM if both are trained
with only the SPT structure; when combined with MISIM node attributes, GMN further improves the
score signiﬁcantly.

Figure 6: Test score on POJ-104 is 12% higher when a model is trained on C++1000 as compared to
a model trained on GCJ-297, even though the validation score for GCJ-297 model is 10% higher than
the validation score for C++1000 model.

Further details of each model, along with the experiment environment, are given below.

8.2.1 Details of Experiments on Code Similarity

MLP with Bag of Tokens
For experiments on code similarity analysis, we use the same bag of tokens as for code classiﬁcation.
The input to the neural network is constructed by concatenating two bags of tokens, one for each
source code ﬁle.

Table 15 provides results of code similarity analysis on all four benchmarks. The columns give the
benchmark name, the test accuracy, the number of training epochs, the number of samples in each
epoch, the run time of each epoch, the number of token types considered, and the number of test
samples. All networks are implemented using Keras API of TensorFlow machine learning tool. The
training is performed on a single V100 GPU, using Adam optimizer with learning rate 1e-3, and
batches of 256 samples.

14

0.250.350.450.550.650.750100200300400500Number of training epochsMean Average Precision @ R scorePOJ-104 (Test for GCJ-297)GCJ-297 (Validation)C++1000 (Validation)POJ-104 (Test for C++1000 )10%12%Benchmark
dataset
Java250
Python800
C++1000
C++1400

Table 15: Similarity analysis by MLP with bag of tokens.
Run time Number
tokens
sec/epoch
81
21
71
24
56
21
56
22

Accuracy Number
epochs
20
94
64
64

Size of
epoch
4,096,000
4,096,000
4,096,000
4,096,000

%%
81.80±0.06
86.61±0.08
85.82±0.05
86.54±0.07

N test
samples
512,000
512,000
512,000
512,000

Figure 7 shows the neural network used for code similarity analysis on the C++1400 benchmark. The
neural networks used for code similarity analysis on other benchmarks are similar to this one. As we
see in Table 15, their accuracy is similar.

Figure 7: MLP architecture for similarity analysis.

As we see in Table 15, the model accuracy is rather modest (<87%) for all benchmark datasets, which
is not very high for a binary classiﬁcation problem of a fully balanced dataset. Obviously, the bag of
tokens is too primitive and misses many important details necessary for identifying similarity.

Siamese Network with Token Sequence
For experiments on code similarity, we use the same sequence of tokens as for code classiﬁcation.
The neural network has two inputs, one for each source code ﬁle. After experimenting with various
neural network architectures, we select the siamese network for its good performance.

Table 16 provides results of code similarity analysis on all four benchmarks. The columns give the
benchmark name, the test accuracy, the number of training epochs, the number of samples in each
epoch, the run time of each epoch, the number of token types considered, and the number of test
samples. All networks are implemented using Keras API of TensorFlow machine learning tool. The
training is performed on four V100 GPUs, using Adam optimizer in data parallel mode with learning
rate 1e-3, and batches of 512 samples.

The neural network for the C++1400 benchmark is depicted in Figure 8. The siamese parts of the
network have the same structure and share all their weights. If the inputs are identical, so are the
outputs. Therefore, by construction, the network guarantees detecting similarity of identical source
code samples. The outputs of the siamese parts are compared by computing the absolute difference.

15

Source code file 1Source code file 2Dense layer 112x64 with ReLUDense layer 64x32 with ReLUDense layer 32x4 with ReLUSigmoidDense layer 4x1Bag of tokens of Bag of tokens of Table 16: Similarity analysis by Siamese network with token sequence.

Benchmark
dataset
Java250
Python800
C++1000
C++1400

Accuracy Number
epochs
29
110
123
144

%%
89.70±0.18
94.67±0.12
96.19±0.08
96.56±0.07

Size of
epoch
51,200
64,000
64,000
64,000

Run time Number
tokens
sec/epoch
75
114
71
89
56
89
56
96

N test
samples
512,000
512,000
512,000
512,000

The network shows 96.56±0.07% test accuracy for C++1400 benchmark dataset. We consider this a
good result, especially considering that the token sequence ignores all identiﬁers, comments, and
many keywords. The neural networks used for code similarity analysis of other benchmarks are
similar to the one shown in Figure 8. As we see in Table 16, their accuracy is quite similar.

SPT-based experiments
Following MISIM [21], the train, validation, and test datasets for the SPT-based experiments draw
from entirely different problems. In our experiments, we use 50% problems for training, 25% for
validation, and 25% for test. The train, validation, and test split used for the experiments can be
found at [47]. Similarity scores in Table 13 and Table 14 report mean and standard deviation of
MAP@R [46] values evaluated with models trained using ﬁve random seeds. The models are trained
on a Xeon(R) CPU E5-2680 v4, 2.4GHz, 256 GiB memory using a NVIDIA V100 GPU. The SPTs
used in these experiments have nodes annotated with attributes derived by combining SPT features
(refer to Section 6), following the context-aware semantic structure (CASS) proposed in [21].

AROMA experiments are performed using the implementation of MISIM given in the further details
section below [23] and the input (SPTs) used for these experiments can be found at [47]. Due to the
high memory requirement for computing MAP@R on the test set of CodeNet benchmarks, we had to
reduce the feature set of AROMA. We estimate that AROMA results can improve by 10–25% when
all features are used. AROMA is rule-based and no training is involved, hence we don’t report mean
and standard deviation in Table 13. For each of the four datasets – Java250, Python800, C++1000,
C++1400 – MISIM’s GNN model is trained for a total of 1000 epochs at a learning rate of 0.001
with Adam optimizer. Each epoch consists of 1000 iterations, and in each iteration, 16 problems
and 5 solutions per problem are randomly sampled, and all solution pairs are used for training as in
[21]. MISIM results for the four languages can be reproduced by downloading the MISIM code and
scripts [23] and using the provided CASS ﬁles [47] as input.

For the GMN experiments (row 2 and row 3 in Table 14), we adapt the implementation in [45] of
the GMN model [44] using SPTs [47] as graphs. We follow the recommendations in [44] for the
model conﬁguration, as they produce the best and stable results in our experiments. Speciﬁcally,
we use 5 layers of propagation with weight sharing across layers, dot-product similarity for the
cross-graph attention mechanism, and GRU layer to update node embeddings from the propagation
scheme. For GMN training, given the large set of SPT pairs, we adopt an approach similar to [21] of
randomly sampling 16 problems with 5 solutions each. We use triplet loss with approximate hamming
similarity [44] for each sample, which is formed using a similar pair combined with a dissimilar SPT.
After every 100 iterations with a batch size of 64, another set of 16 problems and 5 solutions are
sampled randomly for a total of 150,000 iterations (1500 sampled sets). GMN results could improve
further with more training iterations. We use Adam optimizer with a learning rate of 1e-4 for training.

The ﬁrst two rows of Table 14 compare similarity models trained on SPT graph structure only.
The ﬁrst row in the table adapts the MISIM GNN model by masking the node labels to allow the
model to learn structural features only. The second row uses the GMN [44] model with cross-graph
attention-based matching for structural similarity using a node vector dimension of 32 and graph
representation dimension of 128.

For the GMN+MISIM node attributes experiment, row 3 in Table 14, we allow the GMN model to
learn features based on both node attributes and the SPT structure. Accordingly, we replace the node
encoder in the GMN, an MLP, with an embedding layer, for generating node feature vectors. We
explore different node feature vector dimensions, such as 64, 100, 128, and found 100 to produce
good results for the given number of training iterations. All other parameter settings remain the same
as the structure only GMN experiments from row 2 of Table 14. The GMN results can be reproduced
using the Java250 CASS ﬁles available at [47].

16

Figure 8: Siamese architecture for similarity analysis.

MAP@R score [46] is computationally expensive for GMN models because an embedding has to be
computed for all SPT pairs in the test set, and hence Table 14 reports results on smaller sampled test
sets.

Details of MLM Experiment
Here we show how a masked language model (MLM) can be trained with CodeNet. We closely
follow the approach by Ankur Singh, documented in the blog [48]. The goal of the model is to infer
the correct token for an arbitrary masked-out location in the source text. We assume that in every text,
precisely one token is randomly masked. The original token at such position is then the golden label.

From each of the 1000 C++1000 problems, we randomly select 100 samples for training and another
100 for testing. Each C++ source ﬁle is tokenized into a vocabulary of 442 distinct tokens as
categorized in Table 17. For example, while is a keyword and strlen is a function literal.

This code snippet:

17

while({)==Global Max PoolingGlobal Max PoolingDropout layerDense layer 128x128 with ReLUDense layer 128x128 with ReLUDense layer 128x1Sigmoid|x−y|while(<{)Convolution 5x160 with ReLUConvolution 1x128Convolution 15x256 with ReLUTable 17: Token categories used for MLM.

Type
the keyword
the function
the identiﬁer
the punctuator
# or ##
0, 1
the token class

Count Description

95
280
42
16
2
2
5

all C++20 reserved words
function names in common header ﬁles
standard identiﬁers, like stderr, etc.
small set of punctuation symbols
the C pre-processor symbols
special case for these frequent constants
identiﬁer, number, operator, character, string

for (i = 0; i < strlen(s); i++) {}

will be tokenized to:

for ( id = 0 ; id < strlen ( id ) ; id operator ) { }

The tokenized source ﬁles are read into a pandas dataframe and processed by the Keras Text Vector-
ization layer, to extract a vocabulary and encode all token lines into vocabulary indices, including the
special “[mask]” token. Each sample has a ﬁxed token length of 256. The average number of tokens
per sample across the training set is 474. Short samples are padded with 0 and those that are too large
are simply truncated.

The model is trained with 100,000 samples in batches of 32 over ﬁve epochs, with a learning rate
of 0.001 using the Adam optimizer. We evaluate the trained model on a test set of 100,000 samples.
Each sample is pre-processed in the same way as the training samples and one token (never a padding)
is arbitrarily replaced by the “[mask]” symbol. Then, a prediction is generated and the top 1 and top
5 results are compared with the expected value. The achieved accuracies are top-1: 0.9104 (stddev:
0.002) and top-5: 0.9935 (stddev: 0.0005).

8.3 Generalization Across Datasets

Models trained on the CodeNet benchmark datasets can beneﬁt greatly from their high quality. To
demonstrate this, we compare C++1000 to one of the largest publicly available datasets of its kind,
GCJ-297 [23]. For the purpose of this comparison, we train the same MISIM model on C++1000 and
GCJ-297 and test the two trained models on a third, independent dataset - POJ-104. The result of this
comparison is plotted in Figure 6.

The x-axis of this plot is the number of training epochs used and the y-axis is the MAP@R score.
The MISIM model for both datasets is trained for 500 epochs and the MAP@R score for validation
and test is computed after every ten epochs. There are a total of four curves - a validation and a test
curve for GCJ-297 and a validation and a test curve for C++1000.

The training curves show that a 10% higher validation score can be achieved with GCJ-297 compared
to C++1000. However, when tested on POJ-104, the model trained on GCJ-297 achieves a 12% lower
score compared to the model trained on C++1000. We believe C++1000 has better generalization than
GCJ-297 mainly for two reasons: i) high data bias in GCJ-297 because the top 20 problems with the
most number of submissions account for 50% of all submissions and ii) cleaning and de-duplication
of submissions in CodeNet dataset (as described in Section 5.2).

8.4 Masked Language Modelling for Token Inference

A task such as code completion relies on the ability to predict a token at a certain position in a
sequence. To accomplish this we can build a masked language model (MLM) using a technique that
randomly masks out tokens in an input sequence and aims to correctly predict them in an as-yet-
unseen test set. We train a popular BERT-like attention model on the C++1000 CodeNet benchmark
after tokenization to a vocabulary of over 400 tokens and obtain a top-1 prediction accuracy of 0.9104
(stddev: 0.002) and a top-5 accuracy of 0.9935 (stddev: 0.0005).

18

9 Further Uses of CodeNet

The rich metadata and language diversity open CodeNet to a plethora of use cases. The problem-
submission relationship in CodeNet corresponds to type-4 similarity [43] and can be used for code
search and clone detection. The code samples in CodeNet are labeled with their acceptance status
so we can readily extract pairs of buggy and ﬁxed code for code repair [49, 50]. A large number
of code samples come with inputs so that we can execute the code to extract the CPU run time and
memory footprint, which can be used for regression studies and prediction.

CodeNet may also be used for program translation, given its wealth of programs written in a multitude
of languages. Translation between two programming languages is born out of a practical need to port
legacy codebases to modern languages in order to increase accessibility and lower maintenance costs.
With the help of neural networks, machine translation models developed for natural languages [51]
were adapted to programming languages, producing pivotal success [4]. One considerable challenge of
neural machine translation is that model training depends on large, parallel corpora that are expensive
to curate [52], especially for low-resource languages (e.g., legacy code). Recently, monolingual
approaches [53, 4] were developed to mitigate the reliance on parallel data, paving ways to build
models for languages with little translation. Compared with current popular data sets (e.g., [4, 54]),
CodeNet covers a much richer set of languages with ample training instances.

10 Conclusion

Artiﬁcial intelligence has made great strides in understanding human language. Computer scientists
have been fascinated by the possibility and tantalized by the vision of computers (AI) programming
computers. In this paper, we presented "CodeNet", a ﬁrst-of-its-kind very large-scale, diverse and
high-quality dataset to accelerate the algorithmic advances in AI for Code. This dataset is not
only unique in its scale, but also in the diversity of coding tasks it can help benchmark: from code
similarity and classiﬁcation for advances in code recommendation algorithms, and code translation
between a large variety of programming languages, to advances in code performance improvement
techniques. We hope that the scale, diversity and rich, high-quality annotations of CodeNet will offer
unprecedented research opportunities at the intersection of AI and Software Engineering.

11 Acknowledgements

We would like to acknowledge AIZU and AtCoder for making the code submissions publicly available.
We would like to thank the IBM Data Asset eXchange team for providing a platform to host the
CodeNet dataset. We would like to thank the Women in Data Science team at Stanford University
and the IBM Call for Code team for their collaboration in launching the CodeNet challenge.

12 Bibliography

[1] Miltiadis Allamanis, Earl T Barr, Premkumar Devanbu, and Charles Sutton. A survey of
machine learning for big code and naturalness. ACM Computing Surveys (CSUR), 51(4):1–37,
2018.

[2] Yanming Yang, Xin Xia, David Lo, and John Grundy. A survey on deep learning for software

engineering. arXiv preprint arXiv:2011.14597, 2020.

[3] Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde de Oliveira Pinto,
Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, Alex Ray, Raul
Puri, Gretchen Krueger, Michael Petrov, Heidy Khlaaf, Girish Sastry, Pamela Mishkin, Brooke
Chan, Scott Gray, Nick Ryder, Mikhail Pavlov, Alethea Power, Lukasz Kaiser, Mohammad
Bavarian, Clemens Winter, Philippe Tillet, Felipe Petroski Such, Dave Cummings, Matthias
Plappert, Fotios Chantzis, Elizabeth Barnes, Ariel Herbert-Voss, William Hebgen Guss, Alex
Nichol, Alex Paino, Nikolas Tezak, Jie Tang, Igor Babuschkin, Suchir Balaji, Shantanu Jain,
William Saunders, Christopher Hesse, Andrew N. Carr, Jan Leike, Josh Achiam, Vedant Misra,
Evan Morikawa, Alec Radford, Matthew Knight, Miles Brundage, Mira Murati, Katie Mayer,
Peter Welinder, Bob McGrew, Dario Amodei, Sam McCandlish, Ilya Sutskever, and Wojciech
Zaremba. Evaluating large language models trained on code, 2021.

19

[4] Marie-Anne Lachaux, Baptiste Roziere, Lowik Chanussot, and Guillaume Lample. Unsuper-

vised translation of programming languages. In NeurIPS, 2020.

[5] Zheng Wang and Michael O’Boyle. Machine learning in compiler optimization. Proceedings of

the IEEE, 106(11):1879–1901, 2018.

[6] http://ibm.biz/cfcsc-codenet.

[7] Women in data science. https://widsconference.org/.

[8] Yutaka Watanobe. Aizu online judge. https://onlinejudge.u-aizu.ac.jp.

[9] Atcoder. https://atcoder.jp/.

[10] Yunhui Zheng, Saurabh Pujar, Burn Lewis, Luca Buratti, Edward Epstein, Bo Yang, Jim Laredo,
Alessandro Morari, and Zhong Su. D2a: A dataset built for ai-based vulnerability detection
In Proceedings of the ACM/IEEE 43rd International
methods using differential analysis.
Conference on Software Engineering: Software Engineering in Practice, ICSE-SEIP ’21, New
York, NY, USA, 2021. Association for Computing Machinery.

[11] Yaqin Zhou, Shangqing Liu, Jingkai Siow, Xiaoning Du, and Yang Liu. Devign: Effective
vulnerability identiﬁcation by learning comprehensive programsemantics via graph neural
networks. In Advances in Neural Information Processing Systems, pages 10197–10207. NeurIPS
Foundation, 2019.

[12] Zhangyin Feng, Daya Guo, Duyu Tang, Nan Duan, Xiaocheng Feng, Ming Gonga, Linjun Shou,
Bing Qin, Ting Liu, and Daxin Jiang. Codebert: A pre-trained model for programming and
natural languages. arXiv preprint arXiv:2002.08155v4, 2020.

[13] Miltiadis Allamanis and Charles Sutton. Mining source code repositories at massive scale using
language modeling. In 10th Working Conference on Mining Software Repositories (MSR), page
207–216. IEEE, 2013.

[14] Veselin Raychev, Pavol Bielik, and Martin Vechev. Probabilistic model for code with decision

trees. ACM SIGPLAN Notices, 2016.

[15] Michele Tufano, Cody Watson, Gabriele Bavota, Massimiliano Di Penta, Martin White, and
Denys Poshyvanyk. An empirical study on learning bug-ﬁxing patches in the wild via neural
machine translation. In ACM Transactions on Software Engineering and Methodology (TOSEM),
pages 1–29, 2019.

[16] Hamel Husain, Ho-Hsiang Wu, Tiferet Gazit, Miltiadis Allamanis, and Marc Brockschmidt.
Codesearchnet challenge: Evaluating the state of semantic code search. arXiv preprint
arXiv:1909.09436v3, 2019.

[17] Srinivasan Iyer, Ioannis Konstas, Alvin Cheung, and Luke Zettlemoyer. Mapping language to

code in programmatic context. arXiv preprint arXiv:1808.09588, 2018.

[18] Shuai Lu, Daya Guo, Shuo Ren, Junjie Huang, Alexey Svyatkovskiy, Ambrosio Blanco, Colin
Clement, Dawn Drain, Daxin Jiang, Duyu Tang, Ge Li, Lidong Zhou, Linjun Shou, Long
Zhou, Michele Tufano, Ming Gong, Ming Zhou, Nan Duan, Neel Sundaresan, Shao Kun Deng,
Shengyu Fu, and Shujie Liu. Codexglue: A machine learning benchmark dataset for code
understanding and generation, 2021.

[19] Tal Ben-Nun, Alice Shoshana Jakobovits, and Torsten Hoeﬂer. Neural code comprehension: A
learnable representation of code semantics. In S. Bengio, H. Wallach, H. Larochelle, K. Grau-
man, N. Cesa-Bianchi, and R. Garnett, editors, Advances in Neural Information Processing
Systems 31, pages 3588–3600. Curran Associates, Inc., 2018.

[20] Farhan Ullah, Hamad Naeem, Sohail Jabbar, Shehzad Khalid, Muhammad Ahsan Latif, Fadi
Al-turjman, and Leonardo Mostarda. Cyber security threats detection in internet of things using
deep learning approach. IEEE Access, 7:124379–124389, 2019.

[21] Fangke Ye, Shengtian Zhou, Anand Venkat, Ryan Marcus, Nesime Tatbul, Jesmin Jahan Tithi,
Niranjan Hasabnis, Paul Petersen, Mattson. Timothy, Tim Kraska, Pradeep Dubey, Vivek Sarkar,
and Justin Gottschlich. Misim: A novel code similarity system, 2021.

[22] https://sites.google.com/site/treebasedcnn/home/problemdescription.

[23] gcj-dataset.

https://openreview.net/attachment?id=AZ4vmLoJft&name=

supplementary_material.

20

[24] Miltiadis Allamanis. The adverse effects of code duplication in machine learning models of
code. In Proceedings of the 2019 ACM SIGPLAN International Symposium on New Ideas, New
Paradigms, and Reﬂections on Programming and Software, Onward! 2019, page 143–153, New
York, NY, USA, 2019. Association for Computing Machinery.

[25] Wikipedia. Jaccard index — Wikipedia, the free encyclopedia. https://en.wikipedia.

org/wiki/Jaccard_index, 2020.

[26] Terence Parr. The Deﬁnitive ANTLR 4 Reference. Pragmatic Bookshelf, 2nd edition, 2013.
[27] Sifei Luan, Di Yang, Celeste Barnaby, Koushik Sen, and Satish Chandra. Aroma: code recom-
mendation via structural code search. Proceedings of the ACM on Programming Languages,
3(OOPSLA):1–28, Oct 2019.

[28] IBM T.J. Watson Research Center. Wala. https://github.com/wala/WALA, 2021.
[29] Forbes on codenet. https://www.forbes.com/sites/moorinsights/2021/06/04/ibm-
codenet-artificial-intelligence-that-can-program-computers-and-solve-a-
100-billion-legacy-code-problem/?sh=343813636cdc.

[30] Venturebeat on codenet.

https://venturebeat.com/2021/05/10/ibms-codenet-

dataset-aims-to-train-ai-to-tackle-programming-challenges/.

[31] Zdnet on codenet. https://www.zdnet.com/article/ibm-launches-autosql-watson-

orchestrate-codenet-enterprise-ai-tools-at-think/.

[32] Project codenet repository. https://github.com/IBM/Project_CodeNet.
[33] Luca Buratti, Saurabh Pujar, Mihaela Bornea, Scott McCarley, Yunhui Zheng, Gaetano Rossiello,
Alessandro Morari, Jim Laredo, Veronika Thost, Yufan Zhuang, and Giacomo Domeniconi.
Exploring software naturalness through neural language models, 2020.

[34] Thomas N. Kipf and Max Welling. Semi-supervised classiﬁcation with graph convolutional

networks. In ICLR, 2017.

[35] Keyulu Xu, Weihua Hu, Jure Leskovec, and Stefanie Jegelka. How powerful are graph neural

networks? In ICLR, 2019.

[36] Veronika Thost and Jie Chen. Directed acyclic graph neural networks. In ICLR, 2021.
[37] Shuai Lu, Daya Guo, Shuo Ren, Junjie Huang, Alexey Svyatkovskiy, Ambrosio Blanco, Colin B.
Clement, Dawn Drain, Daxin Jiang, Duyu Tang, Ge Li, Lidong Zhou, Linjun Shou, Long Zhou,
Michele Tufano, Ming Gong, Ming Zhou, Nan Duan, Neel Sundaresan, Shao Kun Deng,
Shengyu Fu, and Shujie Liu. Codexglue: A machine learning benchmark dataset for code
understanding and generation. CoRR, abs/2102.04664, 2021.

[38] Qimai Li, Zhichao Han, and Xiao-Ming Wu. Deeper insights into graph convolutional networks

for semi-supervised learning, 2018.

[39] Weihua Hu, Matthias Fey, Marinka Zitnik, Yuxiao Dong, Hongyu Ren, Bowen Liu, Michele
Catasta, and Jure Leskovec. Open graph benchmark: Datasets for machine learning on graphs.
arXiv preprint arXiv:2005.00687, 2020.

[40] Matthias Fey and Jan E. Lenssen. Fast graph representation learning with PyTorch Geometric.

In ICLR Workshop on Representation Learning on Graphs and Manifolds, 2019.

[41] Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan,
Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, Alban Desmaison, Andreas
Kopf, Edward Yang, Zachary DeVito, Martin Raison, Alykhan Tejani, Sasank Chilamkurthy,
Benoit Steiner, Lu Fang, Junjie Bai, and Soumith Chintala. Pytorch: An imperative style, high-
performance deep learning library. In H. Wallach, H. Larochelle, A. Beygelzimer, F. d'Alché-
Buc, E. Fox, and R. Garnett, editors, Advances in Neural Information Processing Systems 32,
pages 8024–8035. Curran Associates, Inc., 2019.

[42] Keyulu Xu, Weihua Hu, Jure Leskovec, and Stefanie Jegelka. How powerful are graph neural

networks?, 2019.

[43] Hitesh Sajnani. Large-Scale Code Clone Detection. PhD thesis, University of California, Irvine,

2016.

[44] Yujia Li, Chenjie Gu, Thomas Dullien, Oriol Vinyals, and Pushmeet Kohli. Graph matching
network for learning the similarity of graph structured objects. In International Conference on
Machine Learning (ICML), 2019.

21

[45] Graph-matching-networks.

https://github.com/Lin-Yijie/Graph-Matching-

Networks.

[46] Kevin Musgrave, Serge J. Belongie, and Ser-Nam Lim. A metric learning reality check. CoRR,

abs/2003.08505, 2020.

[47] Codenet dataset.

https://developer.ibm.com/exchanges/data/all/project-

codenet.

[48] Ankur Singh. "end-to-end masked language modeling with bert". https://keras.io/

examples/nlp/masked_language_modeling.

[49] Zimin Chen, Steve Kommrusch, Michele Tufano, Louis-Noël Pouchet, Denys Poshyvanyk, and
Martin Monperrus. Sequencer: Sequence-to-sequence learning for end-to-end program repair.
IEEE Transaction on Software Engineering, 2019.

[50] Michihiro Yasunaga and Percy Liang. Break-it-ﬁx-it: Unsupervised learning for program repair.

In International Conference on Machine Learning (ICML), 2021.

[51] Yonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V. Le, Mohammad Norouzi, Wolfgang
Macherey, Maxim Krikun, Yuan Cao, Qin Gao, Klaus Macherey, Jeff Klingner, Apurva Shah,
Melvin Johnson, Xiaobing Liu, Łukasz Kaiser, Stephan Gouws, Yoshikiyo Kato, Taku Kudo,
Hideto Kazawa, Keith Stevens, George Kurian, Nishant Patil, Wei Wang, Cliff Young, Jason
Smith, Jason Riesa, Alex Rudnick, Oriol Vinyals, Greg Corrado, Macduff Hughes, and Jeffrey
Dean. Google’s neural machine translation system: Bridging the gap between human and
machine translation. Preprint arXiv:1609.08144, 2016.

[52] Xinyun Chen, Chang Liu, and Dawn Song. Tree-to-tree neural networks for program translation.

In NeurIPS, 2018.

[53] Guillaume Lample, Alexis Conneau, Ludovic Denoyer, and Marc’Aurelio Ranzato. Unsuper-

vised machine translation using monolingual corpora only. In ICLR, 2018.

[54] Shuai Lu, Daya Guo, Shuo Ren, Junjie Huang, Alexey Svyatkovskiy, Ambrosio Blanco, Colin
Clement, Dawn Drain, Daxin Jiang, Duyu Tang, Ge Li, Lidong Zhou, Linjun Shou, Long
Zhou, Michele Tufano, Ming Gong, Ming Zhou, Nan Duan, Neel Sundaresan, Shao Kun Deng,
Shengyu Fu, and Shujie Liu. CodeXGLUE: A machine learning benchmark dataset for code
understanding and generation. Preprint arXiv:2102.04664, 2021.

22

