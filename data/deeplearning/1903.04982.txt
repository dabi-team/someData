9
1
0
2

r
a

M
3
1

]

G
L
.
s
c
[

2
v
2
8
9
4
0
.
3
0
9
1
:
v
i
X
r
a

A Capsule-uniﬁed Framewok of Deep Neural
Networks for Graphical Programming

Yujian Li
College of Computer Science
Faculty of Information Technology
Beijing University of Technology
Beijing, China 100124
liyujian@bjut.edu.cn

Chuanhui Shan
College of Computer Science
Faculty of Information Technology
Beijing University of Technology
Beijing, China 100124
chuanhuishan@emails.bjut.edu.cn

Abstract

Recently, the growth of deep learning has produced a large number of deep neural
networks. How to describe these networks uniﬁedly is becoming an important
issue. We ﬁrst formalize neural networks in a mathematical deﬁnition, give their
directed graph representations, and prove a generation theorem about the induced
networks of connected directed acyclic graphs. Then, using the concept of capsule
to extend neural networks, we set up a capsule-uniﬁed framework for deep learning,
including a mathematical deﬁnition of capsules, an induced model for capsule
networks and a universal backpropagation algorithm for training them. Finally, we
discuss potential applications of the framework to graphical programming with
standard graphical symbols of capsules, neurons, and connections.

1

Introduction

Artiﬁcial neural networks are mathematical models that are inspired by biological neural networks.
As a new era of neural networks, deep learning has become a powerful technology for artiﬁcial
intelligence. Since presented by Hinton et al. [1], it has made a great amount of successes in
image classiﬁcation, speech recognition, and natural language processing [2-5], inﬂuencing on both
academia and industry dramatically.

Essentially, deep learning is a collection of various methods for effectively training neural networks
with deep structures. A neural network is usually regarded as a hierarchical system composed of
many nonlinear computing units (or neurons, nodes). For example, a single-neuron network is the
MP model proposed by McCulloch and Pitts in 1943 [6], with the function of performing logical
operations. Although the MP model is unable to learn, it started the age of neural networks. In 1949,
Hebb ﬁrst proposed the idea of learning about biological neural networks [7]. In 1958, Rosenblatt
invented a model of perceptron and its learning algorithm [8]. Subsequently, Minsky’s criticism of
the perceptron brought the study of neural networks to a period of low tide [9], but not stagnating
completely. By the 1980s and 1990s, a worldwide research of neural networks came up with the
presentation of Hopﬁeld neural network [10], Boltzmann machine [11], multilayer perceptron [12],
and so on.

Multilayer perceptron (MLP) was once the most popular model of neural networks. A MLP consists
of an input layer, a number of hidden layers and an output layer, as shown in Figure 1. The depth
of it is the number of layers excluding the input layer. If the depth is greater than 2, a neural
network may be called “deep”. For training MLPs, backpropagation (BP) is certainly the most
well-known algorithm in common use [12, 13]. However, a majority of applications using MLPs
were implemented with few hidden layers, while gaining little empirical beneﬁt from additional
hidden layers. Certain solace might be found in universal approximation theorems [14, 15], stating

Preprint. Work in progress.

 
 
 
 
 
 
Figure 1: The structure of a MLP.

that a single-layer perceptron can approximate any multivariate continuous function arbitrarily well
with enough hidden units. BP seemed to work only for shallow networks, albeit principally for deep
networks. Not until 1991 was this problem on deep learning fully understood.

In 1991, Hochreiter made a milestone of deep learning [16], to formally indicate that typical deep
networks suffer from the problem of vanishing or exploding gradients. The problem says, cumulative
backpropagated error signals decay or explode exponentially in the number of layers, with conse-
quence of shrinking rapidly or growing out of bounds. This is the major reason why BP is hard to
train deep networks. Note that it could also be encountered as the long time lag problem in recurrent
neural networks [17].

To overcome the training difﬁculties in deep neural networks (DNNs), in 2006 Hinton et al. started the
new ﬁeld of deep learning with a two-stage strategy in two landmark papers [1, 18], to demonstrate
that unsupervised learning in shallow networks, e.g. contrastive divergence learning in restricted
Boltzmann machines, can facilitate supervised learning in DNNs, e.g. BP in deep autoencoders (AEs)
and deep MLPs. Furthermore, their pioneering work inspired many important techniques, including
max pooling [19], dropout [20], dropconnect [21], batch normalization [22], etc. Additionally, a
wealthy of historic accomplishments have been completed in applications such as image classiﬁcation
[2], speech recognition [3, 4], and natural language processing [5]. As a consequence, deep learning
brings about a new and striking wave of neural networks in both academia and industry.

Broadly speaking, deep neural networks include feedforward neural networks (FNNs) and recurrent
neural networks (RNNs). In this paper, we will focus on FNNs, for example, MLPs. Another kind of
FNNs is convolutional neural networks (CNNs) that share connections and weights locally. CNNs
were developed on the basis of neocognitrons [23, 24]. In 1998, LeCun et al. combined convolutional
layers with downsampling layers to design an early structure of modern CNNs, namely, LeNet
[25]. In 2012, Krizhevsky et al. used rectiﬁed linear units (ReLUs) and graphics processing units
(GPUs) to build a breakthrough model of CNNs, i.e. AlexNet [2], which won the 2012 ImageNet
Large Scale Visual Recognition Challenge. Since then, CNNs get into a rapid growth full of brilliant
achievements and great contributions in the development of deep learning. They not only take the lead
in competitions of image classiﬁcation and recognition as well as object localization and detection
[26-31], but also in a variety of application systems such as deep Q-networks [32], AlphaGo [33],
speech recognition [34], and machine translation [5].

Figure 2: Structure of a CNN.

2

Theoretically, a CNN can be regarded as a special MLP. It may consist of an input layer, alternating
convolutional and pooling layers, a fully connected layer, and an output layer, as shown in Figure 2.
In fact, convolutional layers (or "detection layers"), and pooling layers (or “downsampling layers”),
are special hidden layers. A convolutional kernel means the weight of a convolutional layer. Besides
LeNet and AlexNet, many variants of CNNs have been presented, such as VGGNet [26], GoogLeNet
[29], SqueezeNet [35], SPPNet [36], ResNet [27], DenseNet [37], FCN [38], Faster R-CNN [39],
Mask R-CNN [40], YOLO [41], SSD [42], and 3D CNN [43]. There have been so many models
of deep neural networks with different structures, even containing shortcut connections, parallel
connections, and nested structures. The problem of how to establish a uniﬁed framework for DNNs,
is becoming a progressively important issue.

In 2017 and 2018, Hinton et al. published two papers on capsule networks [44, 45], to overcome
the disadvantages of CNNs. Different from their motivation, we will use capsules to establish a
uniﬁed framework from a novel point of view. The uniﬁed framework has something to do with
computational graphs [46]. A computational graph visualizes data dependence relations as an acyclic
graph [47]. It may be composed of input vertices, non-input vertices, and dependence edges. Input
vertices stand for input variables, non-input vertices stand for operations or transformations. A
dependence edge stands for a directed relation between two vertices with one preceding the other.
In Figure 3, input vertices are labelled by "x1", "x2" and "x3", non-input vertices are labelled by
"+", "−", "×", and "/", and "sin". The vertex "×" has two preceding vertices "+" and "−". The
dependence edges are all linear relations. For example, the directed edge from "x1" to "+" has a
weight of 3, meaning that 3x1 will be input into the vertex "+". It can be seen that x4 = (2x2)/(3x3)
is the output of vertex "/", x5 = −x1 − 2x4 is the output of vertex "−", x6 = 3x1 + x2 is the output
of vertex "+", x7 = 2x6 × 0.5x5 is the output of vertex "×", and x8 = sin(0.3x7) is the output
of vertex "sin". If x1 = 2, x2 = 0 and x3 = 3, we have x4 = 0, x5 = −2, x6 = 6, x7 = −12,
x8 = sin(−3.6).

Figure 3: A computational graph that visualizes data dependence relations.

Note that a computational graph may not be a neural network. But a neural network can be viewed as
a special computational graph. Like a computational graph, a neural network may also be composed
of input vertices, non-input vertices, and dependences edges (or directed connections). However,
non-input vertices of a neural network are only chosen from a special set of transformations, which
are generally non-linear functions, such as sigmoid, tanh and ReLU. Usually, these transformations
are termed activation functions. The input of each activation function (or node) is taken as the
weighted sum of the outputs of its preceding nodes and a bias.

This paper is an extension of [48]. The remainder is organized as follows. In Section 2, we propose
a mathematical deﬁnition to formalize neural networks, give their directed graph representations,
and prove a generation theorem about the induced networks of connected directed acyclic graphs.
In Section 3, we use the concept of capsule to extend neural networks, deﬁne an induced model for
capsule networks, and establish a uniﬁed framework for deep learning with a universal backpropaga-
tion algorithm. In Section 4, we discuss potential applications of the uniﬁed framework to graphical
programming. Finally, we make conclusions with future work in Section 5.

3

2 Formalization of Neural networks

2.1 Mathematical deﬁnition

A neural network is a computational model composed of neurons (or nodes) and connections. Nodes
are divided into input neurons and non-input neurons. Input neurons represent real variables, e.g.
x1, x2, · · · , xn. A non-input neuron can receive signals through connections both from input neurons
and the outputs of other non-input neurons, and perform a weighted sum of these signals following a
transformation termed activation function.

Let X = {x1, x2, · · · , xn} stand for a set of real variables, and F for a set of activation functions.
On X and F , a neural network can be formally deﬁned as a 4-tuple net = (S, H, W, Y ). The 4-tuple
is termed a neural network if it can be recursively generated by the following four rules.

1) Rule of variable. For any z ∈ X, let yz = z. Let S = {z}, H = ∅, W = ∅, Y = {yz}. Then
net = (S, H, W, Y ) is a neural network.
2) Rule of neuron. For any nonempty subset S ⊆ X, ∀f ∈ F , ∀b ∈ R, construct a non-input
neuron h (cid:54)∈ X that depends on (f, b), select weighting connections wxi→h(xi ∈ S), and set
yh = f ((cid:80)
xi∈S wxi→hxi + b). Let H = {h}, W = {wxi→h|xi ∈ S}, and Y = {yh}. Then,
net = (S, H, W, Y ) is a neural network.

3) Rule of growth. Suppose net = (S, H, W, Y ) is a neural network. For any nonempty subset
N ⊆ S ∪ H, ∀f ∈ F , ∀b ∈ R, construct a non-input neuron h (cid:54)∈ S ∪ H that depends on (f, b),
select weighting connections wzj→h(zj ∈ N ), and set yh = f ((cid:80)
zj ∈N wzj→hyzj + b). Let S(cid:48) = S,
H (cid:48) = H ∪ {h}, W (cid:48) = W ∪ {wzj→h|zj ∈ N }, and Y (cid:48) = Y ∪ {yh}. Then, net(cid:48) = (S(cid:48), H (cid:48), W (cid:48), Y (cid:48))
is also a neural network.

k=1 Hk) ∪ {h}, W = ((cid:83)K

z∈N wz→hyz + b). Let S = (cid:83)K

k=1 Yk) ∪ {yh}, then net = (S, H, W, Y ) is also a neural network.

4) Rule of convergence. Suppose netk = (Sk, Hk, Wk, Yk)(1 ≤ k ≤ K) are K neural networks,
satisfying that ∀1 ≤ i (cid:54)= j ≤ K, (Si ∪ Hi) ∩ (Sj ∪ Hj) = ∅. For any nonempty subsets
Ak ⊆ Sk ∪ Hk(1 ≤ k ≤ K), N = (cid:83)K
k=1 Ak, ∀f ∈ F , ∀b ∈ R, construct a non-input neuron
h (cid:54)∈ (cid:83)K
k=1(Sk ∪ Hk) that depends on (f, b), select weighting connections wz→h(z ∈ N ), and set
yh = f ((cid:80)
k=1 Sk, H = ((cid:83)K
k=1 Wk) ∪
{wz→h|z ∈ N }, and Y = ((cid:83)K
In the above four rules, weighting connection wz→h is a data structure that contain the full information
of the directed connection z → h, including its strength, back-end neuron z and front-end neuron h.
Moreover, if a neuron h depends on (f, b), f is called the activation function of h, and b is called
the bias of h. Additionally, if the 4-tuple net = (S, H, W, Y ) is a neural network, S is called the set
of input neurons, H the set of non-input neurons, W the set of weighting connections, and Y the
set of outputs. Finally, the rule of neuron can be deduced from the rule of variable and the rule of
convergence. In fact, for any nonempty subset S = {xi1, xi2 , · · · , xir } ⊆ X, we can use the rule of
variable to generate r neural networks netr(cid:48) = ({xr(cid:48)}, ∅, ∅, {xr(cid:48)}) 1 ≤ r(cid:48) ≤ r. According to the rule
of convergence, we can also use the r networks to generate a new neural network net = (S, H, W, Y ),
where S = (cid:83)r
z∈S wz→hyz +b)}.
This is exactly the rule of neuron.

r(cid:48)=1 Sr(cid:48), H = {h}, W = {wz→h|z ∈ S}, and Y = {yh|yh = f ((cid:80)

Figure 4: (a) A trivial network; (b) A 1-1 network

2.2 Directed graph representation

Let X be a set of real variables and F be a set of activation functions. For any neural network
net = (S, H, W, Y ) on X and F , a directed acyclic graph Gnet = (V, E) can be constructed
with the set of vertices V = S ∪ H and the set of directed edges E = {z → h|wz→h ∈ W }.
Gnet = (V, E) is called the directed graph representation of net = (S, H, W, Y ). How to construct
such a graph representation is detailed in the following two cases.

4

Figure 5: Three 1-2 networks

Figure 6: Seven 1-3 networks

1)The case of X = {x1}
For x1 ∈ X, let yx1 = x1, S = {x1}, H = ∅, W = ∅, Y = {yx1}. According to the rule of variable,
net = (S, H, W, Y ) is a neural network, as shown in Figure 4(a). This network is called "trivial
network". It has only one input neuron, which does nothing.
For a nonempty subset S = {x1} ⊆ X, ∀f ∈ F , ∀b ∈ R, construct a non-input neuron h1 (cid:54)∈ S
that depends on (f, b), select weighting connection wx1→h1 , and set yh1 = f (wx1→h1x1 + b). Let
H = {h1}, W = {wx1→h1}, and Y = {yh1}. According to the rule of neuron, net = (S, H, W, Y )
is a neural network, as shown in Figure 4(b). This network is called "1-1 network". It has one input
neuron and one non-input neuron.

Using the rule of growth on the 1-1 network, three different neural networks can be generated next,
as shown in Figures 5(a-c). These networks are called "1-2 networks". Each of them has one input
neuron and two non-input neurons.

Using the rule of growth on the three 1-2 networks, twenty-one different neural networks can be
totally generated further. They are all 1-3 networks, with seven displayed in Figures 6(a-g). The
seven are generated from the network in Figure 5(a).

2)The case of X = {x1, x2}
For x1, x2 ∈ X, let yx1 = x1, yx2 = x2, S1 = {x1}, S2 = {x2}, H1 = H2 = ∅, W1 = W2 = ∅,
Y1 = {yx1} and Y2 = {yx2}. Using the rule of variable, net1 = ({x1}, ∅, ∅, {yx1}) and net2 =
({x2}, ∅, ∅, {yx2 }) are neural networks. Obviously, both of them are trivial networks.

5

Figure 7: A 2-1 network.

Figure 8: Seven 2-2 networks.

If S = {x1, x2}, ∀f ∈ F , ∀b ∈ R, construct a non-input neuron h1 (cid:54)∈ S that depends on (f, b), select
weighting connections wxi→h1(xi ∈ S), and set yh1 = f ((cid:80)
xi∈S wxi→h1xi + b). Let H = {h1},
W = {wx1→h1, wx2→h1 }, and Y = {yh1}. According to the rule of neuron, net = (S, H, W, Y ) is
a neural network. This is a 2-1 network, as depicted in Figure 7. Using the rule of growth on the 2-1
network, seven different 2-2 networks can be generated next, as depicted in Figures 8(a-g).

Using the rule of growth on the seven networks in Figure 8, one hundred and ﬁve different neural
networks can be totally generated further, with ﬁfteen shown in Figures 9(a-o). The ﬁfteen are all 2-3
networks, generated from the network in Figure 8(a).

Finally, the rule of convergence is necessary. In fact, it cannot generate all neural networks only using
the three rules of variable, neuron and growth. An example is the network in Figure 10(c). Without
the rule of convergence, this network cannot be generated from the two in Figures 10(a-b).

2.3

Induced network and its generation theorem

Suppose G = (V, E) is a connected directed acyclic graph, where V is the set vertices (nodes) and E
is the set of directed edges. For any vertex h ∈ V , let IN h = {z|z ∈ V, z → h ∈ E} be the set of
all vertices that precede h adjacently. Let OU T h = {z|z ∈ V, h → z ∈ E} be the set of vertices
that follow h adjacently. If INh = ∅, then h is called an input node of G. If OU Th = ∅, then h is
called an output node of G. Otherwise, h is called a hidden node of G. Let X stand for the set of
all input nodes, O for the set of all output nodes, and M for the set of all hidden nodes. Obviously,
V = X ∪ M ∪ O, and M = V − X ∪ O.

Let yh be the output of node h, and wz→h be the weighting connection from z to h. Then, a
computational model of graph G is deﬁned as follows:

1) ∀z ∈ X, yz = z.
2) ∀h ∈ M ∪ O, select f ∈ F and b ∈ R to compute yh = f ((cid:80)

z∈INh

wz→hyz + b).

Let S = X, H = M ∪ O, W = {wz→h|z → h ∈ E}, and Y = {yh|h ∈ V }. Then, netG =
(S, H, W, Y ) is called an induced network of graph G. We have the following generation theorem.

Generation Theorem: For any connected directed acyclic graph G = (V, E), its induced network
netG is a neural network that can be recursively generated by the rules of variable, neuron, growth,
and convergence.

Proof: By induction on |V | (i.e. number of vertices), we prove the theorem as follows.
1) When |V | = 1, we have |X| = 1 and |O| = 0, so the induced network netG is a neural network

6

Figure 9: Fifteen 2-3 networks.

that can be generated directly by the rule of variable.
2) When |V | = 2, we have |X| = 1 and |O| = 1, so the induced network netG is a neural network
that can be generated directly by the rule of growth.
3) Assume that the theorem holds for |V | ≤ n. When |V | = n + 1 ≥ 3, the induced network netG
has at least one output node h ∈ O. Let Eh = {z → h ∈ E} denote the set of edges heading
to the node h. Moreover, let V (cid:48) = V − {h} and E(cid:48) = E − Eh. Based on the connectedness of
G(cid:48) = (V (cid:48), E(cid:48)), we have two cases to discuss in the following:

i) If G(cid:48) = (V (cid:48), E(cid:48)) is connected, then applying the induction assumption for |V (cid:48)| ≤ n, the
induced network netG(cid:48) = (S(cid:48), H (cid:48), W (cid:48), Y (cid:48)) can be recursively generated by the rules of
variable, neuron, growth, and convergence. Let N = INh. In netG = (S, H, W, Y ),
we use f ∈ F and b ∈ R to stand for the activation function and bias of node h, and
wz→h(z ∈ N ) for the weighting connection from node z to the node h. Then, netG can

7

Figure 10: A necessary explanation for the rule of convergence.

be obtained by using the rule of growth on netG(cid:48), to generate the node h and its output
yh = f ((cid:80)

z∈N wz→hyz + b).

ii) Otherwise, G(cid:48) comprises a number of disjoint connected components Gk = (Vk, Ek)(1 ≤
k ≤ K). Using the induction assumption for |Vk| ≤ n(1 ≤ k ≤ K), the induced
network netGk = (Sk, Hk, Wk, Yk) can be recursively generated by the rules of variable,
neuron, growth, and convergence. Let Ak = (Sk ∪ Hk) ∩ INh, and N = (cid:83)K
k=1 Ak. In
netG = (S, H, W, Y ), we use f ∈ F and b ∈ R to stand for the activation function and
bias of the node h, and wz→h(z ∈ N ) for the weighting connection from node z to node
h. Then, netG can be obtained by using the rule of convergence on netGk (1 ≤ k ≤ K), to
generate the node h and its output yh = f ((cid:80)

z∈N wz→hyz + b).

As a result, the theorem always holds.

Note that in the generation theorem, the rule of neuron can be deleted, because it can be deduced
from the rule of variable and the rule of convergence.

3 Capsule framework of Deep learning

3.1 Mathematical deﬁnition of capsules

In 2017, Hinton et al. pioneered the idea of capsules and considered a nonlinear "squashing"
capsule [45], as shown in Figure 11. The capsule has n input vectors u1, u2, · · · , un, and n weight
matrices w1, w2, · · · , wn. Let ûi = wiui(1 ≤ i ≤ n) be prediction vectors. The total input is
s = (cid:80)
i ciwiui, where ci is a coupling coefﬁcient between low-level capsule i and the
capsule itself. The value of ci is determined by a dynamic routing process, satisfying (cid:80)
i ci = 1. The
output of the capsule v is a vector computed by the nonlinear squashing function

i ciûi = (cid:80)

v = squash(s) =

(cid:107)s(cid:107)2
1 + (cid:107)s(cid:107)2

s
(cid:107)s(cid:107)

.

(1)

Figure 11: A model of squashing capsule.

Figure 12: A model of general capsule.

8

From the viewpoint of mathematical models, a capsule is essentially an extension of traditional
activation functions. A traditional activation function has a scalar input and a scalar output. However,
a capsule can have a vector input and a vector output. More generally, a capsule may have a
tensor input and a tensor output. As shown in Figure 12, a capsule may have n input tensors
X1, X2, · · · , Xn, n weight tensors W1, W2, · · · , Wn, and a bias B. In addition, ⊗1, ⊗2, · · · , ⊗n
are n weighting operations, which may be taken as "identity transfer", "scalar multiplication", "dot
product", "matrix multiplication", "convolution", and so on. Meanwhile, Wi ⊗i Xi(1 ≤ i ≤ n) and
B must be tensors with the same dimension. The total input of the capsule is U = (cid:80)
i Wi ⊗i Xi + B,
and the output Y is a tensor computed by a nonlinear capsule function cap, namely,

Y = cap(U ) = cap(

(cid:88)

Wi ⊗i Xi + B).

i

(2)

According to Eq. (2), a capsule is a generalization of scalar activation function with scalar inputs.

3.2 Deﬁnition of Capsule Networks

For convenience, we use F to stand for a nonempty set of capsule functions, and T for the set of all
tensors.

Suppose G = (V, E) is a connected directed acyclic graph, where V denotes the set of vertices and
E denotes the set of directed edges. For any vertex H ∈ V, let INH be the set of all vertices that
precede H adjacently. Let OU TH be the set of vertices that follow H adjacently. If INH = ∅, then
H is called an input vertex of G. If OU TH = ∅, then H is called an output vertex of G. Otherwise, H
is called a hidden vertex of G. Let X stand for the set of all input vertices, O for the set of all output
vertices, and M for the set of all hidden vertices. Obviously, V = X ∪ M ∪ O, and M = V − X ∪ O.

Furthermore, let YH be the output of vertex H, and (WZ→H , ⊗Z→H ) be the tensor-weighting
connection from Z to H. If ∀H ∈ M ∪ O, ∀Z ∈ INH , WZ→H ⊗Z→H YZ and B are tensors with
the same dimension, then a tensor-computational model of graph G is deﬁned as follows:

1) ∀Z ∈ X , YZ = Z,

2) ∀H ∈ M ∪ O,

select cap

∈ F and B ∈

T to compute YH =

cap((cid:80)

Z∈INH

WZ→H ⊗Z→H YZ + B).

Let S = X , H = M ∪ O, W = {(WZ→H , ⊗Z→H )|Z → H ∈ E}, and Y = {YH |H ∈ V}.
Then, netG = (S, H, W, Y) is called a tensor-induced network of graph G. This network is also
called a capsule network, where the vertices stand for capsules, and the directed edges for weighting
connections. If netG = (S, H, W, Y) is a capsule network, S is called the set of input capsules, H
the set of non-input capsules, W the set of weighting connections, and Y the set of output tensors.

Using a capsule network, a MLP can be simpliﬁed as a directed acyclic path. For example, the MLP
in Figure 1 can be represented as the capsule path in Figure 13. In fact, this MLP has ﬁve layers: one
input layer, three hidden layers, and one output layer. On the whole, each layer can be thought of
as a capsule. Let X = (x1, x2, · · · , x5)T stand for the input capsule, Hi = (capi, Bi)(i = 1, 2, 3)
for the hidden capsules, and O = (cap4, B4) for the output capsule. The capsule function capi
is deﬁned by the elementwise activation function, and the capsule bias Bi by the bias vector.
If all the weighting operations ⊗X→H1, ⊗H1→H2 , ⊗H2→H3, and ⊗H3→O are all taken as ma-
trix multiplication "×", then we are easy to obtain (WX→H1, ⊗X→H1) = ((wX→H1
)7×5, ×),
(WH1→H2 , ⊗H1→H2) = ((wH1→H2
)7×7, ×) and
m,n
(WH3→O, ⊗H3→O) = ((wH3→O
)4×7, ×), which are the tensor-weighting connections from
H0 = X to H1, H1 to H2, H2 to H3 and H3 to O. Finally, let YHi(i = 1, 2, 3) stand for the
output vector of Hi, and YO for the output vector of O. Setting YHO = X and YH4 = YO, we have
YHi = capi(WHi−1→Hi × YHi−1 + Bi) for 1 ≤ i ≤ 4. This means that the MLP in Figure 1 can be
represented as the capsule path in Figure 13.

)7×7, ×), (WH2→H3 , ⊗H2→H3) = ((wH2→H3

m,n

m,n

m,n

Figure 13: A capsule structure of MLP, with "×" standing for matrix multiplication.

9

Besides MLPs, capsule networks can also be used to simplify the structures of other DNNs. For
example, the CNN in Figure 2 can be represented as the capsule path in Figure 14. In fact, this
CNN has 7 layers: one input layer, two convolutional layers, two downsampling (pooling) layers,
one fully connected layer, and one output layer. On the whole, each of the layers can be thought
of as a capsule. Let X stand for the input capsule, Hi = (capi, Bi)(i = 1, · · · , 5) for the hidden
capsules, and O = (cap6, B6) for the output capsule. The capsule functions cap1 and cap3 are
deﬁned by the elementwise ReLU function. cap2 and cap4 are deﬁned by downsampling "↓". cap5
is the identity function. cap6 is the softmax function. Moreover, the capsule biases Bi(i = 1, · · · , 6)
are each deﬁned by the bias tensor of the corresponding layer. Let both ⊗X→H1 and ⊗H2→H3 be
convolution operation "∗", both ⊗H1→H2 and ⊗H3→H4 be identity transfer "→", ⊗H4→H5 be tensor-
reshaping operation "(cid:47)", and ⊗H5→O be matrix multiplication "×". Then, (WX→H1, ⊗X→H1) =
(WX→H1, ∗),
(WH2→H3 , ⊗H2→H3) = (WH2→H3, ∗),
(WH3→H4 , ⊗H3→H4) = (””, →), (WH4→H5 , ⊗H4→H5) = (””, (cid:47)), and (WH5→O, ⊗H5→O) =
(WH5→O, ×), which are tensor-weighting connections from X to H1, H1 to H2, H2 to H3, H3 to
H4, H4 to H5, and H5 to O. Finally, we have

(WH1→H2 , ⊗H1→H2) = (””, →),






YH1 = cap1(WX→H1 ∗ X + B1) = ReLU(WX→H1 ∗ X + B1),
YH2 = cap2(→ YH1 + B2) =↓ YH1 + B2,
YH3 = cap3(WH2→H3 ∗ X + B3) = ReLU(WH2→H3 ∗ YH2 + B3),
YH4 = cap4(→ YH3 + B4) =↓ YH3 + B4,
YH5 = cap5((cid:47)YH4 + B5) = (cid:47)YH4 ,
YO = cap6(WH5→O × YH5 + B6) = sof tmax(WH5→O × YH5 + B6).

(3)

According to Eq. (3), the CNN in Figure 2 can be represented as the capsule path in Figure 14.

Figure 14: A Capsule structure of CNN, with "∗" standing for convolution operation, "→" for identity
transfer, "(cid:47)" for tensor reshaping, and "×" for matrix multiplication.

3.3 Types of capsule networks

As mentioned in Section 3.2, a MLP can be simpliﬁed as a path structure composed of capsules,
so can many CNNs (e.g. LeNet and VGGNet). Note that the path structure is a special case of
layered structure. Generally speaking, capsule networks can be divided into two types: layered
capsule networks and skip capsule networks. A layer capsule network can only have weighting
connections must between two adjacent layers (see Figure 15), whereas a skip capsule network may
have weighting connections between any two layers (see Figure 16).

Figure 15: A layered capsule network with 5 layers.

Suppose G = (V, E) is a connected directed acyclic graph, and netG = (S, H, W, Y) is a tensor-
induced network of graph G. We have V = S ∪ H and H = M ∪ O. The capsule network netG is
called "layered", if the set of hidden capsules M can be partitioned into a number of disjoint subsets
M1, M2, · · · , Mk, such that

10

Figure 16: A skip capsule network.

1. M = M1 ∪ M2 ∪ · · · ∪ Mk, M0 = S, Mk+1 = O,
2. ∀H → H (cid:48) ∈ E, ∃i ∈ {0, 1, · · · , k}, H ∈ Mi and H (cid:48) ∈ Mi+1,
3. ∀H, H (cid:48) ∈ Mi(i = 0, 1, · · · , k + 1), H → H (cid:48) (cid:54)∈ E and H (cid:48) → H (cid:54)∈ E.

If netG = (S, H, W, Y) is not "layered", it is called "skip".

In Figure 15, we are easy to have a layered struture: M0 = {X1, X2}, M1 = {H3, H4, H5},
M2 = {H6, H7}, M3 = {H8, H9, H10} and M4 = {O1, O2, O3}. However, in Figure 16, we
cannot have such a layered structure.

3.4 Universal backpropagation of capsule networks

A capsule network is a tensor- induced network of graph G = (V, E).
It can be denoted as
netG = (S, H, W, Y). Let the set of input capsules S = X = {X1, X2, · · · , Xn}. Suppose
O = {O1, O2, · · · , Om} is the set of output capsules. Then, H = V − S is the set of non-input
capsules, and M = V − X ∪ O = {H1, H2, · · · , Hl} is the set of hidden capsules.

If the total number of capsules |S ∪ H| ≥ 2, then for ∀H ∈ H, we have

(cid:26)UH = (cid:80)

Z∈INH

YH = capH (UH ) = capH ((cid:80)

WZ→H ⊗Z→H YZ + BH ,

Z∈INH

WZ→H ⊗Z→H YZ + BH ).

(4)

For any output node H ∈ O, let YH and TH be its actual output tensor and expected output tensor
respectively for the set of input capsules S = X . The loss function between the two tensors is deﬁned
as

Accordingly, the total loss function can be computed by

LH = Loss(YH , TH ).

L =

(cid:88)

H∈O

LH .

(5)

(6)

Let δH = ∂L
∂UH
rule, we are easy to further obtain:

denote the backpropagated error signal (or sensitivity) for capsule H. By the chain

∀H ∈ O,






δH
∂L
∂BH

∂L
∂WZ→H

= ∂L
∂UH
= ∂L
∂UH
= ∂L
∂UH

= ∂Loss(YH ,TH )
∂YH
· ∂UH
= δH ,
∂BH
·

∂UH
∂WZ→H

= δH ·

· ∂capH
∂UH

,

∂UH
∂WZ→H

.

∀H ∈ M,


δH



∂L
∂BH

∂L
∂WZ→H

= (cid:80)

= ∂L
∂UH
= (cid:80)
= ∂L
∂UH
= ∂L
∂UH

P ∈OU TH
· ∂UH
∂BH
·

P ∈OU TH
δP · ∂UP
∂YH
= δH ,

∂UH
∂WZ→H

= δH ·

∂UH
∂WZ→H

.

∂L
∂UP
· ∂capH
∂UH

· ∂UP
∂YH
,

· ∂YH
∂UH

11

(7)

(8)

Algorithm 1: One iteration of the universal backpropagation algorithm.
1) Select a learning rate η > 0,
2) ∀H ∈ M ∪ O, ∀Z ∈ INH , initialize WZ→H and BH ,
3) ∀H ∈ O, compute δH = ∂Loss(YH ,TH )
4) ∀H ∈ M, compute δH = (cid:80)
P ∈OU TH
∂UH
5) Compute ∆WZ→H = δH ·
∂WZ→H
6) Update WZ→H ← WZ→H − η · ∆WZ→H ,BH ← BH − η · ∆BH .

· ∂capH
∂UH
· ∂capH
δP · ∂UP
∂UH
∂YH
and ∆BH = δH ,

∂YH

,

,

Algorithm 2: A universal backpropagation algorithm on training pairs in many iterations.
Input: training pairs (X k, T k)(k = 1, · · · , K),
Output: WZ→H , ∆WZ→H , BH , ∆BH ,
Select a learning rate η > 0,
∀H ∈ M ∪ O, ∀Z ∈ INH , initialize WZ→H and BH ,
for iter=1:max_iter do
for ∀H ∈ O, do δk

H ,T k
H )

H

for ∀H ∈ M, do δk
P ∈OU T k
H
∂U k
∆WZ→H = (cid:80)K
H
∂WZ→H
WZ→H ← WZ→H − η · ∆WZ→H ,BH ← BH − η · ∆BH

k=1 δk

H

· ∂capk
end for
∂U k
H
· ∂capk
P · ∂U k
δk
P
∂U k
∂Y k
H
H
and ∆BH = (cid:80)K
k=1 δk
H

∂Y k
H

H = ∂Loss(Y k
H = (cid:80)
H ·

end for

end for

It should be noted that in formulae (7)-(8), the computation of ∂capH
∂UH
capsule function capH . For example, when is an elementwise sigmoid function, the result is:

depends on the speciﬁc form of

∂capH
∂UH

= sigmoid(UH )(1 − sigmoid(UH )).

(9)

Additionally,

∂UH
∂WZ→H

and ∂UP
∂YH

also depends on the speciﬁc choice of weighting operation ⊗Z→H .

Based on formulae (7)-(8), a universal backpropagation algorithm can be designed for capsule
networks, with one iteration detailed in Algorithm 1. This algorithm can have many variants from
different versions of gradient descent [49].
In practice, we need a training algorithm on pairs (X k, T k)(k = 1, · · · , K), where X k =
{X k
1 , X k
n }. For any H ∈ O, we have the loss between
H and T k
Y k
H ,

n} and T k = {T k

2 , · · · , X k

2 , · · · , T k

1 , T k

Lk

H = Loss(Y k

H , T k

H ).

and the total loss for the k-th pair:

Lk =

(cid:88)

H∈O

Lk

H .

Therefore, we can compute the total loss for all training pairs as follows

L =

K
(cid:88)

k=1

Lk =

K
(cid:88)

(cid:88)

k=1

H∈O

Lk

H .

(10)

(11)

(12)

Using gradient descent on formula (12), we can have a universal backpropagation algorithm on
training pairs in many iterations, which is detailed in Algorithm 2.

4 Potential Applications to Graphical Programming

The capsule framework can be potentially applied to designing a graphical programming platform
based on a set of standard graphical symbols for capsule networks and plain networks. Using this
platform, we can implement deep neural networks by drawing them directly instead of coding. The
platform is not only expected to make easier deep learning, but also to promote its development
signiﬁcantly.

12

4.1 Standard graphical symbols for capsule networks

In future applications, the capsule framework will be able to provide a theoretical basis for graphical
programming on deep learning. Under the framework, we can design a set of standard graphical
symbols to draw capsule networks, mainly including capsule symbols and connection symbols.

Figure 17: Examples of standard capsule symbols: (a) 1D-data capsule, (b) 2D-data capsule, (c)
2D-ReLU capsule, (d) 2D-maximum downsampling capsule, (e) 1D-identical capsule, (f) 1D-ReLU
capsule, (g) 1D-softmax capsule.

Table 1: Types and attributes of some standard capsule symbols.

Capsule type
1D-data capsule
2D-data capsule

2D-ReLU capsule

2D-maximum downsampling capsule

1D-identical capsule
1D-ReLU capsule
1D-softmax capsule

Attributes
(dimension M , data type T )
(height M , width N , channel number d,
data type T )
(height M , width N , channel number d,
data type T )
(height M , width N , channel number d,
downsampling window λ × τ , data type
T )
(dimension M , data type T )
(dimension M , data type T )
(dimension M , data type T )

Some of standard capsule symbols are shown in Figure 17 (a-g), such as 1D-data capsule, 2D-data
capsule, 2D-ReLU capsule, 2D-maximum downsampling capsule, 1D-identical capsule, 1D-ReLU
capsule, and 1D-softmax capsule. Their types and attributes are listed in Table 1, with more detail
given as follows.

1) 1D-data capsule is a structure of vector array for the input of a capsule network. It can be
represented as X = X = (x1, · · · , xM ), with 2 basic attributes: dimension M and data type T (e.g.
ﬂoat64).

2) A 2D-data capsule is a structure of matrix array for the input of a capsule network. It can be
represented as X = (X1, · · · , Xd), with 4 basic attributes: height M , width N , channel number d
and data type T . Moreover, each Xi(1 ≤ i ≤ d) is a matrix of size M × N and data type T .

3) A 2D-ReLU capsule transforms a matrix array input into another matrix array output with
elementwise ReLU. It has four basic attributes: height M , width N , channel number d and data type
T . Its input is a matrix array X = (X1, · · · , Xd), where each Xi is of size M × N and data type T .
Its output is also a matrix array Y = (Y1, · · · , Yd), deﬁned as:

Y = ReLU (X ) = (ReLU (X1), · · · , ReLU (Xd)).

(13)

4) A 2D-maximum downsampling capsule transforms a matrix array input into another matrix array
output with maximum downsampling function ↓λ,τ
max. It has ﬁve basic attributes: height M , width
N , channel number d, downsamplinged window λ × τ and data type T . Its input is a matrix array
X = (X1, · · · , Xd), where each Xi is of size M × N and data type T . Its output is also a matrix
array Y = (Y1, · · · , Yd), deﬁned as:

Y =↓λ,τ

max (X ) = (↓λ,τ

max (X1), · · · , ↓λ,τ

max (Xd)),

Yi =↓λ,τ

max (Xi) = (↓λ,τ

max (GXi

λ,τ )).

(14)

(15)

where GA
matrix, then GA

λ,τ denotes the non-overlapping block matrix of A divided by λ × τ . If A is a M × N
τ columns. The i, j-th block can be

λ,τ is a block matrix that contains M

λ rows and N

13

expressed as

GA

λ,τ (i, j) = (ast)λ×τ , (i − 1) × λ + 1 ≤ s ≤ i × λ, (j − 1) × τ + 1 ≤ t ≤ j × τ.

(16)

The maximum downsampling of GA

λ,τ (i, j) is deﬁned as

max (GA
↓λ,τ

λ,τ (i, j)) = max{ast|(i − 1) × λ + 1 ≤ s ≤ i × λ, (j − 1) × τ + 1 ≤ t ≤ j × τ }. (17)

The maximum downsampling of GA

λ,τ is deﬁned as

max GA
↓λ,τ

λ,τ = (↓λ,τ

max (GA

λ,τ (i, j))).

(18)

5) A 1D-identical capsule transforms a vector input into another vector output with the identical
function I. It has two basic attributes: dimension M and data type T . Its input is a vector X =
(X) = X of dimension M and data type T . Its output is also a vector Y = Y , deﬁned as:

Y = I(X) = X.

(19)

6) A 1D-ReLU capsule transforms a vector input into another vector output with elementwise ReLU.
It has two basic attributes: dimension M and data type T . Its input is a vector X = X of dimension
M and data type T . Its output is a vector Y = Y , deﬁned as:

Y = ReLU (X).

(20)

7) A 1D-softmax capsule transforms a vector input into another vector output with function sof tmax.
It has two basic attributes: dimension M and data type T . Its input is a vector X = X of dimension
M and data type T . Its output is a vector Y = Y , deﬁned as:

Y = sof tmax(X).

(21)

Figure 18: Examples of standard connection symbols: (a) convolutional connection, (b) transfer
connection, (c) reshaping connection, (d) full connection.

Table 2: Types and attributes of several types of standard connected graphical symbols.

Connection types

Convolutional
connection

Transfer
connection

Reshaping
connection

Full connection

Back-end structure
(height M , width N ,
channel number d,
data type T )

(height M , width N ,
channel number d,
data type T )
(height M , width N ,
channel number d,
data type T )
(dimension N , data
type T )

Attributes
Weight structure
(kernel number k,
height m, width n,
channel number d,
stride s, data type T )
(height M , width M ,
data type T )

(dimension M ,
channel number M ,
data type T )
(height M , width N ,
data type T )

Front-end structure
(height M − m + 1,
width N − n + 1,
channel number k,
data type T )
(height M , width N ,
channel number d,
data type T )
(dimension dM N ,
data type T )

(dimension N , data
type T )

Some of standard connection symbols are shown in Figure 18, such as "convolutional connection",
"transfer connection", "reshaping connection", and "full connection". Their types and attributes are
listed in Table 2 and further explained as follows.

1) A convolutional connection transforms the back-end capsule into the front-end capsule with
convolution operation. It has three attributes: back-end structure, weight structure and front-end

14

structure. The back-end structure is a matrix array X = (X1, · · · , Xd), where each Xi is of size
M × N and data type T . he weight structure is a k-kernel tensor array W = (W1, · · · , Wk) with
Wi(1 ≤ i ≤ k) representing the i-th convolutional kernel. Moreover, the tensor Wi = (W i
1, · · · , W i
d)
is a d-channel matrix array, where each W i
j of size m × n and data type T . The front-end structure
Y = (Y1, · · · , Yk) is also a matrix array, deﬁned as:

Y = W ∗ X = (W1, · · · , Wk) ∗ X = (W1 ∗ X , · · · , Wk ∗ X ),

where

Yi = Wi ∗ X =

d
(cid:88)

j=1

W i

j ∗ Xj.

(22)

(23)

Note that W i
their s-stride convolution C = (cij) = Am×n ∗ BM ×N is a matrix of size ( M −m
and data type T , with cij deﬁned as:

j ∗ Xj is calculated by matrix convolution. If Am×n and BM ×N are two matrices, then
s + 1)

s + 1) × ( N −n

cij =

m
(cid:88)

n
(cid:88)

u=1

v=1

auv · bis+m−u−1,js+n−v−1, 1 ≤ i ≤

M − m
s

+ 1, 1 ≤ j ≤

N − n
s

+ 1.

(24)

2) A transfer connection transforms the back end into the front end with identity operation. It has
three attributes: back-end structure, weight structure and front-end structure. The back-end structure
is a matrix array X = (X1, · · · , Xd), where each Xi is of size M × N and data type T . The weight
structure W = I is the identity matrix of size M × M and data type T . The front-end structure
Y = (Y1, · · · , Yd) is also a matrix array, deﬁned as:

Y = W × X = (W × X1, · · · , W × Xd) = (I × X1, · · · , I × Xd) = (X1, · · · , Xd) = X , (25)

where Yi = I × Xi = Xi(1 ≤ i ≤ d).

3) A reshaping connection transforms the back end into the front end with reshaping operation. It has
three attributes: back-end structure, weight structure and front-end structure. The back-end structure
is a matrix array X = (X1, · · · , Xd), where each Xi is of size M × N and data type T . The weight
structure is a d-channel vector array W = (W1, · · · , WM ), where Wi = (0, · · · , 1
, 0, · · · , 0)(1 ≤
(cid:124)(cid:123)(cid:122)(cid:125)
i

i ≤ M ) is an M -dimensional vector. The front-end structure Y is an N M d-dimensional vector,
deﬁned as:

Y = (Y1, · · · , YM d) = (Y1·1, Y2·1, · · · , YM ·1, · · · , Yi·j, · · · , Y1·d, · · · , YM ·d)

= W × X = W × (X1, · · · , Xd) = (W × X1, · · · , W × Xd)
= ((W1, · · · , WM ) × X1, · · · , (W1, · · · , WM ) × Xd)
= (W1 × X1, · · · , WM × X1, · · · , W1 × Xd, · · · , WM × Xd),

(26)

where Yi·j = Wi × Xj(1 ≤ i ≤ M, 1 ≤ j ≤ d) is the i-th row of Xj, and it is an N -dimensional
vector.

4) A full connection transforms the back end into the front end with matrix multiplication. It has
three attributes: back-end structure, weight structure and front-end structure. The back-end structure
is an N -dimensional vector X = X. The weight structure W = W is a matrix of size M × N and
data type T . The front-end structure Y = Y is an M -dimensional vector, deﬁned as:

Y = W × X = Y = W × X.

(27)

4.2 Standard graphical symbols for plain networks

As a special case of capsule networks, plain networks can also be implemented by drawing them
with standard graphical symbols, such as "data neuron", "ReLU neuron", "identical neuron", "arrow
amplifying connection", and "amplifying connection". Some of these graphical symbols are shown in
Figure 19(a-e).

A data neuron is a scalar input with data type T . A ReLU neuron transforms a scalar input x into
another scalar output y with activation function ReLU, deﬁned as:

y = ReLU (x).

(28)

15

Figure 19: Graphical symbols for plain networks: (a) data neuron; (b) ReLU neuron; (c) identical
neuron; (d) arrow amplifying connection; (e) amplifying connection.

An identical neuron plays the identical function, namely,

y = I(x) = x.

(29)

An (arrow) amplifying connection transforms the back end into the front end with ampliﬁcation. It
has three attributes: back-end scalar x, weight strength w, and front-end scalar y. The value of y is
computed by:

y = wx.

(30)

In case of no ambiguity, amplifying connection can be in place of arrow amplifying connection.

4.3 Graphical programming

Based on graphical symbols of capsules and their connections, we can design a graphical programming
platform to implement deep neural networks. For example, we can implement MLP and LeNet by
drawing the capsule graphs in Figures 20-21.

Figure 20: A graphical representation of the capsuled MLP, which is orderly composed of 1D-data
capsule (a), 1D-ReLU capsule (b), 1D-ReLU capsule (c), and 1D-identical capsule (d).

Figure 21: A graphical representation of the capsuled LeNet, which is orderly composed of 2D-data
capsule (a), 2D-ReLU capsule (b), 2D-maximum downsampling capsule (c), 2D-ReLU capsule
(d), 2D-maximum downsampling capsule (e), 1D-identical capsule (f), 1D-ReLU capsule (g), and
1D-softmax capsule (h).

As shown in Figure 20, the capsuled MLP consists of one 1D-data capsule, two 1D-ReLU capsules,
and one 1D-identical capsule, together with three full connections. The dimensions of the capsules
can be set to 2, 6, 4 and 2, with the same type of "ﬂoat64". Accordingly, the full connection between
module (a) and module (b) must have a back end with dimension=2 and data type = "ﬂoat64", a
weight matrix with height = 6, width = 2 and data type = "ﬂoat64", and a front end with dimension
= 6 and data type = "ﬂoat64". The full connection between module (b) and module (c) must have
a back end with dimension=6 and data type = "ﬂoat64", a weight matrix with height =4, width =6
and data type = "ﬂoat64", and a front end with dimension = 4 and data type = "ﬂoat64". The full
connection between module (c) and module (d) must have a back end with dimension=4 and data
type = "ﬂoat64", a weight matrix with height =2, width =4 and data type = "ﬂoat64", and a front end
with dimension = 2 and data type = "ﬂoat64". Therefore, the capsuled MLP is equivalent to the plain
MLP depicted in Figure 22.

As shown in Figure 21, the capsuled LeNet consists of one 2D-data capsule, two 2D-ReLU capsules,
two 2D-maximum downsampling capsules, one 1D-identical capsule, one 1D-ReLU capsule and
one 1D-softmax capsule, together with two convolutional connections, two transfer connections, one
reshaping connection and two full connections. If the LeNet is applied to the MNIST dataset [50],
module (a) can be a 2D-data capsule with height=28, weight=28, channel number=1, and data type =

16

Figure 22: A graphical representation of the plain MLP.

"ﬂoat64". Module (b) can be a 2D-ReLU capsule with height = 24, width = 24, channel number =
32, and data type = "ﬂoat64". The convolution connection between module (a) and module (b) can
have a back end with height=28, weight=28, channel number=1, and data type = "ﬂoat64", and a
weight structure with kernel number= 32, height = 5, width = 5, channel number = 1, stride = 1, and
data type = "ﬂoat64", and a front end with height = 24, width = 24, channel number = 32, and data
type = "ﬂoat64". The other modules and their connections can be determined compatibly in a similar
manner.

Figure 23: A graphical representation of AlexNet in tradition.

Figure 24: A graphical representation of AlexNet by standard symbols.

Like LeNet, we can implement AlexNet (see Figure 23) by drawing its capsule graph displayed
in Figure 24. Using more graphical symbols, we can similarly implement VGGNet, GoogLeNet,
ResNet, and so on. Moreover, we can even implement a deep neural network with ﬂexible connections
in Figure 25, and a deep capsule network with ﬂexible connections in Figure 26. Obviously, drawing
is easier, simpler and more convenient than coding in popular deep learning platforms such as Caffe,
TensorFlow, MXNet, and CNTK.

5 Conclusions

We have used capsule networks to establish a uniﬁed framework of deep learning. We not only
present a strict mathematical deﬁnition of neural networks, but also a generation theorem to clarify
the intrinsic relationship between feedforward neural networks and connected directed acyclic
graphs. As a generalization of neural networks, we obtain two types of capsule networks (i.e.
layered and skip) to represent and extend various deep learning models. Additionally, we derive a

17

Figure 25: A deep neural network with ﬂexible connections.

Figure 26: A deep capsule network with ﬂexible connections.

universal backpropagation algorithm for capsule networks, and demonstrate its potential applications
to graphical programming in combination with standard graphical symbols of plain networks and
capsule networks. On a graphical programming platform, we can implement a variety of deep neural
networks by drawing them directly, such as MLP, LeNet, AlexNet, and many others. Therefore, this
kind of platform will make easier deep learning with beneﬁcial complement to popular deep learning
frameworks, such as Caffe, TensorFlow, MXNet and CNTK.

As future work, we will design a sufﬁcient set of standard graphical symbols to describe deep neural
networks, and then implement a graphical programming platform to make their construction more
convenient and to facilitate their large-scale applications.

6 Acknowledgements

This work was supported by the National Natural Science Foundation of China under Grant 61876010.

References

[1] Hinton, G. E. & Salakhutdinov, R. R. (2006) Reducing the dimensionality of data with neural networks.
Nature 313(5786): 504-507.

[2] Krizhevsky, A., Sutskever, I. & Hinton, G.E. (2012) Imagenet classiﬁcation with deep convolutional neural
networks. In F. Pereira, C.J.C. Burges, L. Bottou and K.Q. Weinberger (eds.), Advances in neural information
processing systems 25, pp. 1097–1105. Cambridge, MA: MIT Press.

[3] Seide, F., Li, G. & Yu, D. (2011) Conversational speech transcription using context-dependent deep neural
networks. Twelfth Annual Conference of the International Speech Communication Association.

[4] Le, Q. V. (2013) Building high-level features using large scale unsupervised learning. Acoustics, Speech and
Signal Processing (ICASSP), 2013 IEEE International Conference on. IEEE, pp. 8595–8598.

[5] Gao, J., He, X. & Yih, W. et al. (2014) Learning continuous phrase representations for translation modeling.
Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long
Papers), pp. 1: 699-709.

[6] McCulloch, W. S & Pitts, W. (1943) A logical calculus of the ideas immanent in nervous activity. The bulletin
of mathematical biophysics 5(4): 115-133.

[7] Hebb, D. O. (2006) The organization of behavior: A neuropsychological theory. Psychology Press.

[8] Rosenblatt, F. (1958) The perceptron: a probabilistic model for information storage and organization in the
brain. Psychological review 65(6): 386.

18

[9] Minsky, M., Papert, S. A. & Bottou, L. (2017) Perceptrons: An introduction to computational geometry. MIT
press.

[10] Hopﬁeld, J. J. (1982) Neural networks and physical systems with emergent collective computational abilities.
Proceedings of the national academy of sciences 79(8): 2554-2558.

[11] Ackley, D. H., Hinton, G. E. & Sejnowski, T. J. (1985) A learning algorithm for Boltzmann machines.
Cognitive science 9(1): 147-169.

[12] Rumellhart, D.E. (1986) Learning internal representations by error propagation. Parallel distributed
processing: Explorations in the microstructure of cognition 1:319-362.

[13] Werbos, P. (1974) Beyond regression: New tools for prediction and analysis in the behavior science.
Unpublished Doctoral Dissertation Harvard University.

[14] Cybenko, G. (1989) Approximation by superpositions of a sigmoidal function. Mathematics of control,
signals and systems 2(4): 303-314.

[15] Funahashi, K. I. (1989) On the approximate realization of continuous mappings by neural networks. Neural
networks 2(3): 183-192.

[16] Hochreiter, S. (1991) Untersuchungen zu dynamischen neuronalen Netzen. Master’s Thesis, Institut Fur
Informatik, Technische Universitat, Munchen.

[17] Hochreiter, S., Bengio, Y. & Frasconi, P. et al. (2001) Gradient ﬂow in recurrent nets: the difﬁculty of
learning long-term dependencies. 28(2):237-243.

[18] Hinton, G.E., Osindero, S. & Teh, Y.W. (2006) A fast learning algorithm for deep belief nets. Neural
computation 18(7):1527-1554.

[19] Nagi, J., Ducatelle, F. & Di Caro, G. A. et al. (2011) Max-pooling convolutional neural networks for vision-
based hand gesture recognition. Signal and Image Processing Applications (ICSIPA), 2011 IEEE International
Conference on. IEEE, pp. 342–347.

[20] Wan, L., Zeiler, M. & Zhang, S. et al. (2013) Regularization of neural networks using dropconnect.
International Conference on Machine Learning, pp. 1058–1066.

[21] Hinton, G. E., Srivastava, N. & Krizhevsky, A. et al. (2012) Improving neural networks by preventing
co-adaptation of feature detectors. Computer Science 3(4): 212-223.

[22] Ioffe, S. & Szegedy, C. (2015) Batch normalization: accelerating deep network training by reducing internal
covariate shift. International Conference on International Conference on Machine Learning, JMLR.org, pp.
448–456.

[23] Fukushima, K. (1979) Neural network model for a mechanism of pattern recognition unaffected by shift in
position-Neocognitron. IEICE Technical Report, A, pp. 62(10): 658-665.

[24] Fukushima, K. (1980) Neocognitron: A selorganizing neural network model for a mechanism of pattern
recognition unaffected by shift in position. Biological Cybernetics 36(4):193-202.

[25] LeCun, Y., Bottou, L. & Bengio Y, et al. (1998) Gradient-based learning applied to document recognition.
Proceedings of the IEEE 86(11):2278-2324.

[26] Simonyan, K. & Zisserman, A. (2014) Very Deep Convolutional Networks for Large-Scale Image Recogni-
tion. Computer Science.

[27] He, K. Zhang, X. & Ren, S. et al. (2016) Deep residual learning for image recognition. Proceedings of the
IEEE conference on computer vision and pattern recognition, pp. 770–778

[28] Russakovsky, O., Deng, J. & Su, H. et al. (2015) Imagenet large scale visual recognition challenge.
International Journal of Computer Vision 115(3): 211-252.

[29] Szegedy, C. Liu, W. & Jia, Y. et al. (2015) Going deeper with convolutions. IEEE Conference on Computer
Vision and Pattern Recognition.

[30] Ding, C. & Tao, D. (2015) Robust face recognition via multimodal deep face representation. IEEE
Transactions on Multimedia 17(11): 2049-2058.

[31] Schroff, F., Kalenichenko, D. & Philbin, J. (2015) Facenet: A uniﬁed embedding for face recognition and
clustering. Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 815–823.

[32] Mnih, V., Kavukcuoglu, K. & Silver, D. et al. (2015) Human-level control through deep reinforcement
learning. Nature 518(7540): 529.

19

[33] Silver, D., Schrittwieser, J. & Simonyan, K. et al. (2017) Mastering the game of Go without human
knowledge. Nature 550(7676):354-359.

[34] Hinton, G., Deng, L. & Yu, D. et al. (2012) Deep neural networks for acoustic modeling in speech
recognition: The shared views of four research groups. IEEE Signal Processing Magazine 29(6): 82-97.

[35] Iandola, F. N., Han, S. & Moskewicz, M. W. et al. (2016) SqueezeNet: AlexNet-level accuracy with 50x
fewer parameters and <0.5MB model size. arXiv preprint arXiv:1602.0736.

[36] He, K., Zhang, X. & Ren, S. et al. (2014) Spatial pyramid pooling in deep convolutional networks for visual
recognition. European conference on computer vision, Springer, Cham, pp. 346–361.

[37] Huang, G., Liu, Z. & Weinberger, K.Q. et al. (2017) Densely connected convolutional networks. Proceedings
of the IEEE conference on computer vision and pattern recognition, pp. 1(2):3.

[38] Long, J., Shelhamer, E. & Darrell, T. (2015) Fully convolutional networks for semantic segmentation.
Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 3431–3440.

[39] Ren, S., He, K. & Girshick, R. et al. (2015) Faster r-cnn: Towards real-time object detection with region
proposal networks. Advances in neural information processing systems, pp. 91–99.

[40] He, K., Gkioxari, G. & Dollár, P. et al. (2017) Mask r-cnn. Computer Vision (ICCV), 2017 IEEE International
Conference on. IEEE, pp. 2980–2988.

[41] Redmon, J., Divvala, S. & Girshick, R. et al. (2016) You only look once: Uniﬁed, real-time object detection.
Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 779–788.

[42] Liu, W., Anguelov, D. & Erhan, D. et al. (2016) Ssd: Single shot multibox detector. European conference
on computer vision. Springer, Cham, pp. 21–37.

[43] Ji, S., Xu, W. & Yang, M. et al. (2013) 3D convolutional neural networks for human action recognition.
IEEE transactions on pattern analysis and machine intelligence 35(1): 221-231.

[44] Sabour, S., Frosst, N. & Hinton, G.E. (2017) Dynamic routing between capsules. In I. Guyon, U.V. Luxburg,
S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan and R. Garnett (eds.), Advances in Neural Information
Processing Systems 30, pp. 3859-3869. Cambridge, MA: MIT Press.

[45] Hinton, G. E., Sabour, S. & Frosst, N. (2018) Matrix capsules with EM routing. International Conference
on Representation Learning.

[46] Bauer, F. L. (1974) Computational Graphs and Rounding Error. Siam Journal on Numerical Analysis 11(1):
87-96.

[47] Griewank, A. & Walther, A. (2008) Evaluating derivatives: principles and techniques of algorithmic
differentiation (2. ed.). DBLP.

[48] Li, Y. & Shan, C. (2018) A Uniﬁed Framework of Deep Neural Networks by Capsules. arXiv preprint
arXiv:1805.03551

[49] Ruder, S. (2016) An overview of gradient descent optimization algorithms. arXiv preprint arXiv:1609.04747.

[50] http://yann.lecun.com/exdb/mnist/.

20

