0
2
0
2

b
e
F
5
2

]

C
D
.
s
c
[

3
v
7
0
2
4
0
.
8
0
9
1
:
v
i
X
r
a

Taming Unbalanced Training Workloads in Deep
Learning with Partial Collective Operations

Shigang Li
Department of Computer Science
ETH Zurich
shigang.li@inf.ethz.ch

Tal Ben-Nun
Department of Computer Science
ETH Zurich
talbn@inf.ethz.ch

Salvatore Di Girolamo
Department of Computer Science
ETH Zurich
salvatore.digirolamo@inf.ethz.ch

Dan Alistarh
IST Austria
dan.alistarh@ist.ac.at

Torsten Hoefler
Department of Computer Science
ETH Zurich
torsten.hoefler@inf.ethz.ch

Abstract
Load imbalance pervasively exists in distributed deep learn-
ing training systems, either caused by the inherent imbal-
ance in learned tasks or by the system itself. Traditional
synchronous Stochastic Gradient Descent (SGD) achieves
good accuracy for a wide variety of tasks, but relies on global
synchronization to accumulate the gradients at every train-
ing step. In this paper, we propose eager-SGD, which relaxes
the global synchronization for decentralized accumulation.
To implement eager-SGD, we propose to use two partial col-
lectives: solo and majority. With solo allreduce, the faster
processes contribute their gradients eagerly without waiting
for the slower processes, whereas with majority allreduce,
at least half of the participants must contribute gradients
before continuing, all without using a central parameter
server. We theoretically prove the convergence of the algo-
rithms and describe the partial collectives in detail. Exper-
iments are conducted on a variety of neural networks and
datasets. The results on load-imbalanced environments show
that eager-SGD achieves 2.64 × speedup (ResNet-50 on Ima-
geNet) over the asynchronous centralized SGD, and achieves
1.29 × speedup (ResNet-50 on ImageNet) and 1.27× speedup
(LSTM on UCF101) over the state-of-the-art synchronous
decentralized SGDs, without losing accuracy.

• Theory of computation → Parallel al-
CCS Concepts
gorithms; • Computing methodologies → Neural net-
works;

Keywords
stochastic gradient descent, distributed deep
learning, eager-SGD, workload imbalance, collective opera-
tions

1 Motivation
Deep learning models are on a steep trajectory to becoming
the most important workload on parallel and distributed com-
puter systems. Early convolutional networks demonstrated
groundbreaking successes in computer vision, ranging from
image classification to object detection [30, 52]. More recent
developments in recurrent and transformer networks enable

1

impressive results in video classification, natural language
processing for machine translation, question answering, text
comprehension, and synthetic text generation. The latter
models contain more than 1.5 billion parameters and take
weeks to train [15, 45]. Other demanding neural networks
are trained on the largest supercomputers to achieve scien-
tific breakthroughs [35, 41]. Furthermore, the models are
growing exponentially in size, OpenAI is predicting a 10x
growth each year [3] potentially leading to artificial general
intelligence. In order to support this development, optimiz-
ing the training procedure is most important.

The training procedure of deep learning is highly par-
allel but dominated by communication [10]. Most parallel
training schemes use data parallelism where full models are
trained with parts of the dataset and parameters are syn-
chronized at the end of each iteration. The total size of allre-
duce grows with the model size, which ranges from a few
megabytes [30] to several gigabytes [45] and grows quickly.
The allreduce operation is not atomic and it can be split into
layer-wise reductions, which can easily be overlapped with
the layer computation using non-blocking collectives [5, 25].
Yet, the optimal scaling of an allreduce of size S is at best
O (log P + S) in P processes [26, 43, 47]. Thus, growing pro-
cess counts will reduce the parallel efficiency and eventually
make the reduction a scaling bottleneck.

The communication aspects of deep learning have been in-
vestigated in many different contexts [47, 51], see the survey
for an overview [10]. In this work, we identify load imbalance
as an additional barrier to scalability. When some processes
finish the computation later than others, all processes will
wait for the last one at the blocking allreduce function. Load
imbalance can be caused by the system itself, for example,
when training on multi-tenant cloud systems [31, 32, 49] or
by system or network noise [27, 28] in high-performance
machines. A second, and more prominent cause of imbalance
is inherent imbalance in the computation that causes varying
load across different processes. While noise from the system
is generally low on well-maintained HPC machines [28], the
inherent load imbalance of the training workloads cannot

 
 
 
 
 
 
Our main contributions are:

• A detailed analysis of workload imbalance in deep

learning training.

• Definition and implementation of partial collectives,

specifically majority and solo allreduce.

• Eager-SGD for asynchronous decentralized distributed
training of neural networks with proof of convergence.
• An experimental study of convergence and training
speed for multiple networks, achieving 1.27× speedup
over synchronous SGD on a video classification task
without losing accuracy.

2 Load-Imbalance in Deep Learning
Load imbalance widely exists in the training of deep learning
models, which can be caused by either the applications or
the system itself [27, 28, 31, 32, 49].

2.1 Video Processing

Long short-term memory (LSTM) [23] is a type of unit cell
in Recurrent Neural Networks (RNN). In video classifica-
tion tasks, LSTMs are used [7, 18, 60] to process a sequence
of frames for a video as input (optionally following con-
volutional neural networks that preprocess the images to
features), and output a probability distribution over a set of
classes. Due to the recurrent structure of the network, the
computational overhead is proportional to the number of
frames in the input video.

Fig. 2a shows the video length distribution (the number
of frames) over all 9,537 videos in the training dataset of
UCF101 [53]. The video length is distributed between 29 and
1,776 frames, with a mean frame count of 187 and standard
deviation of 97. Fig. 2b shows the runtime distribution over
the 1,192 sampled batches in two epochs to train a 2,048-
wide single-layer LSTM model on video frame features. As
is standard in variable-length training, videos with similar
lengths are grouped into buckets for performance. The run-
time is distributed from 201 ms to 3,410 ms, with a mean
runtime of 1,235 ms and standard deviation of 706 ms. These
statistics above show that training an LSTM model for video
classification exhibits inherent load imbalance.

2.2 Language Processing

Transformers [57] are sequence-to-sequence models that
translate a sequence of words from one language to another.
Different from RNN, a Transformer network replaces the
recurrent structure with an attention mechanism. To train
the Transformer model, the computation overhead increases
with the length of the input (and output) sentences. Typically,
the sentences in the training dataset for a language model
have various lengths, and thus the workload is unbalanced
across different batches. Fig. 3 shows the runtime distribution
over the 20,653 randomly sampled batches in 1/3 epoch to
train a Transformer on the WMT16 dataset. The runtime is

Figure 1. Synch-SGD vs eager-SGD under load imbalance.
w(t ) are the weights in training step t.

easily be avoided. Natural language processing tasks have
sentences of highly varying length while video processing
tasks have videos with different number of frames. For ex-
ample, the training dataset of UCF101 [53] contains videos
that range from 29 to 1,776 frames.

Several researchers have shown that the training process
itself is quite robust with respect to bounded errors. In fact,
data augmentations such as Cutout [16] and Dropout [6]
introduce random errors and omissions into the training
process to improve generalization properties. Several pack-
ages take advantage of this robustness and employ three
techniques in tandem: (1) communicated weights are quan-
tized to more compact number representations [50, 54], (2)
only the most significant weights are sent during each allre-
duce [2, 47], and (3) updates are only sent to limited (random)
neighborhoods using gossip algorithms [40]. We propose to
exploit this robustness in a new way: we perform the allre-
duce eagerly in that we ignore the input gradients of pro-
cesses that come late in order to not delay all processes. The
communication partners are selected based on their work-
load (which can be randomized) and the allreduce itself is
performed with high-performance reduction topologies [26]
in logarithmic depth. We call our method eager Stochastic
Gradient Decent (eager-SGD), as a counterpart to synchro-
nous SGD (synch-SGD) [5, 9, 51]. Fig. 1 shows the difference
between synch-SGD and eager-SGD.

Specifically, we propose to relax the allreduce operation
to partial collectives in eager-SGD. A partial collective is an
asynchronous operation where a subset of the processes
can trigger and complete the collective operation. Absentee
processes follow a predefined protocol, such as contributing
potentially outdated data. We define two partial collectives
— solo allreduce, a wait-free operation that one process trig-
gers; and majority allreduce, in which the majority must
participate.

Our theoretical analysis shows that solo allreduce does
not guarantee bounded error, as necessary in SGD, yet em-
pirically converges in cases of moderate load imbalance.
Majority allreduce is proven to bound the error, but is not
completely wait-free. The statistical guarantee, however, is
sufficient to both train deep neural networks and avoid the
delays.

We show that solo and majority collectives are suitable

for different cases, depending on load imbalance severity.

2

(b) eager-W(1)W(1)W(2)W(2)Process0ProcessnW(0)idleidleW(1)W(1)W(2)W(2)Process0ProcessnW(0)synch-allreduceimTe(a) synch-SGsynch-allreducepartial-allreducepartial-allreduceimTeDSGDmodel [21] on ImageNet [14], on a standard Google Cloud
instance (n1-standard-16 with 2x Nvidia V100 GPUs). The
runtime is distributed from 399 ms to 1,892 ms with a mean of
454 ms and standard deviation of 116 ms. Since ResNet-50 on
ImageNet has the same input size for different batches, the
load imbalance is caused mainly by the system. Compared
with imbalanced applications (e.g., Transformer, LSTM), the
load imbalance on cloud servers is relatively light.

3 Distributed Deep Learning
Deep neural networks are continuously differentiable func-
tions that are composed of multiple operators, representable
by a directed acyclic graph [36]. The gradient of those func-
tions can be computed using the backpropagation algorithm [37],
processing the nodes in the DAG in a reverse topological
order. Deep learning frameworks, such as TensorFlow [1],
typically execute parallel operations in the DAG in arbitrary
order.

Algorithm 1 Distributed Minibatch SGD
1: for t = 0 to T do
2:
3:
4:
5:
6:
7:
8: end for

(cid:174)x, (cid:174)y ← Sample B elements from dataset
wt ← Obtain parameters from global view
(cid:174)z ← ℓ (cid:0)wt , (cid:174)x, (cid:174)y(cid:1)
(cid:1)
Gt ← 1
B ΣB
∆w ← U (cid:0)Gt , w(0, . . ., t ), t (cid:1)
Update global view of parameters to wt + ∆w

i =0 ∇ℓ (cid:0)wt , (cid:174)zi

Supervised deep neural network training typically in-
volves first-order optimization in the form of Stochastic Gra-
dient Descent (SGD) [48]. SGD optimizes the expected loss
value over the “true” distribution of input samples by de-
scending in the direction of a random subset of the training
samples (minibatch). In a distributed data-parallel setting,
the SGD algorithm (Algorithm 1) consists of multiple learner
processes, each of which updates a global view of the pa-
rameters w according to a different random minibatch at the
same time. Given an update rule U and local minibatch of
size B, the learners modify the global view of the parame-
ters by using an average of the gradients Gt obtained by the
agents.

A straightforward manner to maintain a global view is
using a Parameter Server (PS) architecture [13], where one
or several nodes assume the role of a PS, broadcasting up-
to-date weights (line 3) to learners prior to each step and
aggregating gradients from them (line 7). This enables the
PS to asynchronously update the global view [46], or require
a fraction of learners to send gradients before progressing
to the next step [22].

As the PS model is generally not scalable, another mode
of operation implements SGD using collective operations.
In such implementations, accumulating the gradients (line
7) is done via an allreduce operation, where each learner

(a) Video length distribution.

(b) Runtime distribution on a P100 GPU (batch size=16).

Figure 2. Load imbalance in the training of an LSTM model
on UCF101 [53].

Figure 3. Runtime distribution on a P100 GPU (batch size
= 64), using a Transformer model on WMT16.

Figure 4. Runtime distribution on Google Cloud with
2xV100 GPUs (batch size=256, ResNet-50 on ImageNet).

distributed from 179 ms to 3,482 ms with a mean of 475 ms
and standard deviation of 144 ms, which shows the inherent
load imbalance in language model training.

2.3 Training in the Cloud

Performance variability is common in cloud computing [31,
32, 49]. Fig. 4 shows the runtime distribution over the sam-
pled batches for 5 epochs of training for the classic ResNet-50

3

0500100015002000020040060080010001200140016001800Number of framesNumber of videos02550751000500100015002000250030003500Runtime (milliseconds)Number of batches01000200030004000500100015002000250030003500Runtime (milliseconds)Number of batches0500010000150002000040060080010001200140016001800Runtime (milliseconds)Number of batchesFigure 5. Adding control dependency in the computation
DAG, using a block of ResNet-50 as an example.

contains its own local view of the weights [10]. Horovod [51]
is one such implementation over the TensorFlow framework,
which also fuses several allreduce operations into one in
order to reduce overhead. However, due to the arbitrary order
of execution imposed by the frameworks, Horovod uses a
master process for negotiation communication (achieving
consensus on which parameters are sent).

A more scalable method, used in the Deep500 DSGD opti-
mizer [9], is to ensure an order of communication execution
by adding control dependencies into the computation DAG,
as shown in Fig. 5. In the backward pass, the allreduce op-
erations are executed in a specific order after finishing the
local gradient computation. We use the same method when
implementing eager-SGD. Note that synchronizing gradient
order can be avoided completely using non-blocking col-
lectives [42]. In this mode, each gradient communication
message is assigned to an agreed-upon numeric tag, and
multiple allreduce operations may be in-flight concurrently.
Prior to updating the local view of the weights, a waitall
command must be issued. All in all, these approaches reduce
overhead in imbalanced loads by overlapping communica-
tion and computation, but do not mitigate it completely.

4 Partial Collective Operations
A collective communication involves a set of processes that
cooperate to progress their internal state. Some of these
operations, e.g., allreduce, implicitly synchronize the partic-
ipants: the operation cannot terminate before the slowest
process joins it. We define these collectives as synchronous
and introduce a new class of partial collectives that relax
the synchronization. We now discuss two variants of partial
collectives: solo and majority.

4.1 Solo Collectives

A solo collective [17] is a wait-free operation, which forces
the slow processes to execute the collective as soon as there
is one process executing it. This process, called initiator,
is in charge of informing the others to join the collective.
While solo collectives remove the synchronization delays,
they change the semantics of collective operations, which

4

Figure 6. Solo collective activation (left) and process sched-
ule (right). Operations are represented by circles: blue =
send, green = receive, orange = computation, white = NOP.
A dashed border means the operation can be fired as soon
as one of its dependencies are satisfied.

may now be completed by using stale data from the slow
processes.

4.1.1 Schedule Activation

We define a schedule as a set of operations that a process
executes in order to globally progress the collective oper-
ation. In particular, a schedule is a directed acyclic graph
(DAG) where the vertices are operations and the edges are
happens-before dependencies among them. We define the
following operations:
• Point-to-point communications: sends and receives.
• Computations: simple computations defined between two
arrays of data items. The type of the data items is defined
according to the MPI basic types [42].

• Non-operations (NOP): complete immediately and are only

used to build dependencies.

Operations can be dependent on zero, one, or more other
operations (with and or or logic) of the same schedule.

The main difference between synchronous and solo col-
lectives is the time at which processes activate (i.e., starts
executing) their schedule. For synchronous collectives, the
schedule is executed only when a process reaches the col-
lective function call (e.g., MPI_Allreduce). We define this
activation as internal. For solo collectives, an external ac-
tivation is also possible: the processes start executing the
schedule because of an activation message received from the
initiator, which starts broadcasting it immediately after the
internal activation of its schedule. In particular, a solo col-
lective is composed of two schedules: one for broadcasting
the activation and the other one for executing the collective
operation.

In a solo collective, any process can become the initiator,
hence any process must be capable of broadcasting the acti-
vation message. The activation broadcast is implemented as
a modified version of the recursive doubling communication
scheme: this is equivalent to the union of P binomial trees
(optimal for small message broadcast, like the activation)
rooted at the different nodes.

Conv-BNConv-BN-ReLUConv-BNMax PoolAdditionforward passConv-BNConv-BN-ReLUConv-BNMax PoolAddition1backward passAll-reduce2All-reduceAll-reduceAll-reduce34controldependencyAllreduceActivationActivationAllreduceP0P1P2P3P3 scheduleS0R0S1R1S2R2S3R3R0R1S1S0S2R2R3C0C1S3N1N0Activation example Fig. 6 shows a solo allreduce example.
On the left, we show the global communications view that is
split in two phases: activation and allreduce. The highlighted
communication shows the activation path if the initiator is,
e.g., process P3. For the allreduce, we use a recursive doubling
implementation. Note that any collective implementation
that can be expressed as a schedule can be linked to the
activation phase. On the right we show the internal schedule
of process P3. An internal activation (i.e., P3 making the
function call explicitly) translates in the execution of NOP
0 (N0): this leads to the send operations S0 and S1 being
fired to start broadcasting the activation message and to the
execution of N1, which signals the activation of the allreduce
schedule. Alternatively, if P3 is not the initiator, it will receive
a message in receive R0 or R1: if the activation is received
by R0, then P3 has to forward the activation message to P1
with send S1 (i.e., P3 is an internal node of the activation
binomial tree). Also in this case NOP N1 will be executed,
leading to the execution of the allreduce schedule.

Multiple initiators Multiple processes may join the col-
lective at the same time: in this case we need to ensure that
the collective is executed only once. To address this issue,
we set the operations to be consumable, meaning that the
same operation cannot be executed twice. For example, let us
assume that nodes P2 and P3 reach their internal activation
at the same time. When P3 receives the activation message
from P2 (i.e., through R0) there are two possible cases: 1) S1
is still not consumed and then it is executed; 2) S1 has been
fired due to the internal activation and will not be executed
a second time. NOPs are also consumable, hence N1 (i.e., the
activation) can be executed only once.

Persistent schedules Processes can be asked to join a solo
collective only once before they reach their internal activa-
tion: once the schedule is executed, it needs to be re-created
by the application in order to be executed again. To enable
multiple asynchronous executions of solo collectives, we
introduce persistent schedules. Such schedules transparently
replicate themselves once executed, able to serve a new solo
collective without requiring application intervention. Multi-
ple executions of the same solo collective overwrite the data
in the receive buffer, which always contains the value of the
latest execution.

4.2 Majority Collectives

An issue of solo collectives is that if one or few processes are
always faster than the others, then the collective will always
complete by taking the stale data of the slower processes.
In cases like DNN training, this scenario may negatively
impact the convergence because the training will advance
only considering the updates of few processes. To overcome
this issue, we introduce majority collectives, which requires
at least half of the processes to join before completing. We
implement majority collectives by not letting any process

5

become the initiator, as in solo collectives. Instead, at each
execution of a persistent schedule, the processes designate an
initiator by randomly selecting a rank (consensus is achieved
by using the same seed for all the processes). When a pro-
cess joins the collective (i.e., internal activation), it checks
whether it is the designated initiator: only in that case it
keeps running the internal activation followed by the actual
collective schedule.

We now discuss how the above described implementation
can provide a statistical guarantee that at least half of the
processes on average contribute to the collective. Suppose
the same collective operation is called by many iterations,
such as in model training. We sort all the P processes by the
time they reach a collective operation. Since the probability
that any process is specified as the initiator is equal to 1/P,
the expectation of the randomly specified initiator is the P/2-
th process among the sorted processes, namely on average
half of the processes reach the collective operation earlier
than the initiator. For a workload distribution with one mode
and a tail, such as in Figs. 2, 3, and 4, the probability that
part of the processes reach the collective at a similar time to
the initiator is high; then, more than half of the processes on
average actively participate in the operation.

4.3 Asynchronous Execution by Library Offloading

The schedule of a partial collective can be asynchronously
executed with respect to the application. We develop fflib2,
a communication library that allows to express communica-
tion schedules and offload their execution to the library itself.
The schedule execution can take place on the application
thread (i.e., when the application enters the library), or on an
auxiliary thread. Once the application creates and commits a
schedule, the library starts executing all the operations that
have no dependencies. The remaining ones are executed as
their dependencies are satisfied.

4.4 Discussion

Offloading the schedule execution to the network interface
card (NIC) can provide different advantages such as asyn-
chronous execution, lower latency, and streaming processing.
Di Girolamo et al. [17] show how solo collectives can be of-
floaded to Portals 4 [8] NICs by using triggered operations.
This approach is limited by the amount of NIC resources
that bounds the number of times a persistent schedule can
be executed without application intervention. This limit can
be removed by implementing the schedule execution with
the sPIN programming model [24], which allows to execute
user-defined code on the NIC. A sPIN implementation of
fflib2 would then be able to replicate the schedule on-the-fly
upon completion.

Algorithm 2 Eager-SGD

1: b is local batchsize for P processes
2: for t = 0 to T do
3:
4:
5:

(cid:174)x, (cid:174)y ← Each process samples b elements from dataset
(cid:174)z ← ℓ (cid:0)wt , (cid:174)x, (cid:174)y(cid:1)
i =0 ∇ℓ (cid:0)wt , (cid:174)zi
G l oc al
b Σb
t
G дl obal
← 1
P par t ial _allr educe
(cid:16)
G дl obal
∆w ← U
, w(0, . . ., t ), t
wt +1 ← wt + ∆w

(cid:16)
G l oc al
t

← 1

6:

(cid:17)

(cid:17)

(cid:1)

t

t

7:
8:
9: end for

5 Eager-SGD algorithm
Algorithm 2 illustrates the main procedure of eager-SGD.
Instead of calling a synchronous allreduce in the distributed
optimizer (Fig. 5) to accumulate the gradients, eager-SGD
uses the partial allreduce operations (Line 7). Either solo or
majority allreduce can be used depending on the severity of
load imbalance.

t and Gp

Fig. 7 presents an example of how eager-SGD works with
partial collectives, in which wp
t represent the weights
and the gradients calculated on process p at training step t,
respectively, and U (G, w) represents the update rule. In step
t, suppose process P1 is faster than process P0. P1 finishes
the computation of G1
t and then triggers the partial allreduce
operation. Since P0 does not finish the computation of G0
t at
this time, it only passively contributes null gradients Gnull
to the partial allreduce at step t. After P0 finishes the compu-
tation of G0
t , it finds out that the partial allreduce at step t is
already finished by checking the results in the receive buffer.
P0 updates the weights of step t + 1 using G1
t stored in the
receive buffer of the partial allreduce and G0
t becomes the
stale gradients. The stale gradient G0
t is then stored in the
send buffer. If P0 does not catch up with P1 at step t + 1, P0
will passively participate in the partial allreduce again and
t . If P0 catches up with P1 at step t + 1 (as in the
contribute G0
case shown in Fig. 7), P0 will add G0
t +1 (calculated in
step t + 1) together, and contribute the accumulated gradi-
ents G0′
t +1 to the partial allreduce; P0 resets the send buffer
to Gnull after finishing allreduce.

t and G0

In severe load imbalance situations, some slower processes
may lag behind by more than one step. The data in the receive
buffer of the partial allreduce will then be overwritten and
only the latest data in the receive buffer can be seen, which
results in different weights on different processes. This may
result in slightly lower accuracy as shown in Section 7.2.2.
Thus, we periodically synchronize the models across all pro-
cesses to eliminate the side effect. Since we only synchronize
the models every few epochs, the overhead can be ignored.

6

Figure 7. Partial collective operations in eager-SGD.

6 Correctness and Convergence

Guarantees
6.1 System Model

We prove that, under a reasonable set of modeling assump-
tions, the eager-SGD algorithm will still converge. We as-
sume a system with P asynchronous processors indexed as
i ∈ {0, 1, . . . , P − 1}, which take steps at different speeds.

For simplicity, we break down the execution at each pro-
cessor into steps: at step t, we assume that each processor i
has collected a local view of the parameters, which we denote
by wi
t . We then proceed as follows: the processor computes
the gradient Gi
t on a randomly sampled mini-batch, with re-
spect to the local view wi
t , and enters an partial-allreduce
for the step, whose goal is to attempt to communicate its
current parameter updates to other processors. At the end
of this, the process obtains its next view of the parameters
t +1, which it will use in the following step t + 1.
wi
From a global perspective, we can split the execution in
serial fashion into rounds, where each round can be mapped
to the partial-allreduce of corresponding index. Without
loss of generality, we assume that each processor participates
in each round t, since it eventually submits an update to
the corresponding partial-allreduce, which we denote
by ADS(t), for asynchronous distributed sum. However, its
update may or may not be delivered to the other processors.
Each partial-allreduce has the following semantics:

• (Invocation) Each process i proposes a d-dimensional
t , corresponding to its current proposed update,

vector Ri
to ADS(t).

• (Response) Each process i receives a tuple ⟨Ut , si

t ⟩, where
Ut is the d-dimensional update to the parameter set cor-
responding to round t, as decided by the shared object
ADS(t), and si
t is a boolean stating whether the update by
process i has been included in Ut .

Computation threadCommunication threadGnullGt1Gt1Gt1wt0Gt1wt1P0Gt+11=Gt+10+Gt+11+Gt+11+Gt+11Gt+10'Gt0partial-allreducestep tstep t+1Gt0partial-allreduce( ,)Gt1Gt0Gt0sendbuff0recvbuff0sendbuff0sendbuff0recvbuff0sendbuff1recvbuff1sendbuff1recvbuff1wt+10P1Gt+10'Gt+10'Gt+10'U( ,)Gt1wt+11UWe can therefore rephrase the algorithm as having each
process invoke the ADS(t) object in each round, with its cur-
= false) then
rent update. If its update is not “accepted” (si
t
the processor simply adds it to its update in the next itera-
tion. The ADS objects we implement provide the following
guarantees.

Lemma 6.1. Each ADS object ensures the following:

1. (Liveness) The ADS(t) object eventually returns an out-

put at every invoking process.

2. (Safety) The output is consistent, in the sense that (1) it
is a correct average of a subset of the proposed updates
in the round; (2) the returned bits reflect its composition;
and (3) the output is the same at every invoking process.
3. (Quorum Size) The subset of proposed updates included
in the output is of size Q ≥ 1, where Q is a lower bound
parameter ensured by the algorithm.

4. (Staleness Bound) There exists a bounded parameter
τ such that any update by a process can be rejected by
the ADS objects for at most τ consecutive rounds from
the time it was generated before being accepted.

Proof. The proof of the above properties follows directly
from the structure of the partial-allreduce algorithm,
□
and is therefore skipped.

6.2 Convergence Proof

We now show that these properties are sufficient for eager-
SGD to ensure convergence for a standard class of smooth
non-convex objectives. In the following, all norms are ℓ2-
norms, unless otherwise stated.

Assumption 1 (Loss Function). We assume that our objec-
tive loss function f : Rd → R satisfies the following standard
properties:
• (Lower Bound) The function f is bounded from below, that
is, there exists a finite value m such that, ∀(cid:174)x ∈ Rd , f ((cid:174)x) ≥ m.

• (Smoothness) The function f is L-smooth, i.e.

∀ (cid:174)x, (cid:174)y ∈ Rd , ∥∇f (cid:0) (cid:174)x (cid:1) − ∇f (cid:0) (cid:174)y(cid:1) ∥ ≤ L∥ (cid:174)x − (cid:174)y∥ for L > 0.

Further, we make the following standard assumptions

about the gradients generated by the nodes:

Assumption 2 (Gradients). For any round t and processor i,
the gradients Gi
t generated by the processes satisfy the follow-
ing, where expectations are taken with respect to the random
data sampling at round t.
• (Unbiasedness) The stochastic gradients are unbiased esti-

mators of the true gradients:
∀(cid:174)x ∈ Rd , E (cid:2)Gi

t ((cid:174)x)(cid:3) = ∇f ((cid:174)x),

• (Second Moment Bound) There exists a constant M such

that

∀(cid:174)x ∈ Rd , E (cid:2)∥Gi
t

(cid:0) (cid:174)x (cid:1) ∥2(cid:3) ≤ M 2.

Analytic View of the Algorithm. Let us fix a global round
t + 1, and consider the view of an arbitrary process i, wi
t +1
at the beginning of this round. Recall that this view con-
sists of the view returned by the object ADS(t). Therefore,
by Lemma 6.1, this view must include the sum of at least Q
distinct gradients generated in each previous round, possibly
together with some additional gradients, some of which are
included in their corresponding round, and some of which
are delayed. Conversely, if we consider the gradients which
have been proposed to ADS objects by all nodes by time t
and are not included in this view, we have that there can be
at most P − Q such gradients for any previous round, up to
maximum time τ in the past. We formalize this observation
as follows.

Define recursively the auxiliary random variable Λt such

that for every round t ≥ 0,

Λt +1 = Λt −

α
P

P −1
(cid:213)

i=0

Gi

t (wi

t ),

where α > 0 is the learning rate, which we assume to be
constant. Without loss of generality, we set Λ0 = 0d . Intu-
itively, Λt would like to follow the “clean” SGD iteration, by
including all the gradients generated by the end of round
t. However, one technical issue is that these gradients are
generated not with respect to the model Λt −1 (which would
allow us to perform a standard SGD analysis) but with re-
spect to the partial views wi
t . We will overcome this obstacle
by leveraging the fact that the partial view wi
t cannot be too
far from Λt . More precisely, the discussion in the previous
paragraph implies:

Lemma 6.2. For any t ≥ 0 and process i, we have:

E[∥Λt − wi

t ∥2] ≤ α 2τ M 2(P − Q)/P 2.

Proof. Let δ j
t be a binary indicator random variable that is
true if the gradient generated by process j at iteration t is
not delivered by the ADS(t) object. Then, we have that:

∥Λt − wi

t ∥2 = ∥

= ∥

∞
(cid:213)

t =1

τ
(cid:213)

α

α

P
(cid:213)

j=1

P
(cid:213)

t =1

j=1

t G j
δ j

t /P ∥2

t G j
δ j

t /P ∥2

≤

τ
(cid:213)

t =1

(α 2/P 2)

P
(cid:213)

j=1

t ∥G j
δ j

t ∥2,

(1)

(2)

(3)

where we have used the properties stated in Lemma 6.1 (in
particular the Staleness Bound), and the triangle inequality.
Next, we notice that (1) the expected squared norm of each
of the missing gradients is bounded by M 2 (by the second
moment bound), and that (2) there can be at most P − Q
delayed gradients from each round (by the Quorum Size

7

bound). This finally implies the claimed inequality:
t ∥2] ≤ (α 2/P 2) (cid:205)τ

E[∥Λt − wi

E[∥Gi

j=1 δ j

(cid:205)P

t =1

t

t ∥2]

≤

α 2τ M 2(P − Q)/P 2.

(4)

(5)

□

Convergence Bound. Finally, we put all the machinery to-
gether to obtain the following convergence bound:

Theorem 6.3 (Eager-SGD Convergence). Consider an ar-
bitrary objective function f and gradient sampling scheme
satisfying Assumptions 1 and 2. Fix the success parameter
ϵ > 0. Then, if we execute the eager-SGD algorithm for con-
stant learning rate value
(cid:32)

√

√

(cid:33)

α ≤ min

ϵP
(cid:112)12L2τ M 2(P − Q)
(cid:17)
(cid:16) f (w0)−m
ϵ α

,

ϵP
(cid:112)4Lτ M 2(P − Q)

,

ϵ
12M 2L

for T = Θ
some iterate wt ⋆ with 1 ≤ t ≤ T such that

iterations, we are guaranteed to reach

E∥∇f (wt ⋆)∥2 ≤ ϵ.

Proof. We begin from the definition of Λt :

Λt +1 = Λt −

α
P

P −1
(cid:213)

i=0

Gi

t (wt ).

(6)

We will first prove the above statement for the iterate Λt ,
and then will extend the proof for wt . For simplicity, let us
denote Gt = (cid:205)P −1
t (wt ). We can use the Taylor expansion
of f (Λt +1) around Λt and the smoothness condition to obtain
the following inequality:

i=0 Gi

f (Λt +1) ≤ f (Λt ) + (Λt +1 − Λt )T ∇f (Λt ) + L
2
= f (Λt ) − α ∇f (Λt )T ∇f (Λt ) + α 2L
∥Gt ∥2+
2P 2
+ α(∇f (Λt ) − Gt /P)T ∇f (Λt ).

∥Λt +1 − Λt ∥2

We can therefore apply the expectation with respect to
the random sampling at step t, the second moment bound
assumption:

E [f (Λt +1)|Λt ] ≤ f (Λt ) − α ∥∇f (Λt )∥2 + α 2L
2

M 2

+α(∇f (Λt ) − ∇f (wt ))T ∇f (Λt ).

To bound the last term, we can now apply the Cauchy-
Schwarz inequality and the fact that the gradients are L-
Lipschitz:

E [f (Λt +1)|Λt ] ≤ f (Λt ) − α ∥∇f (Λt )∥2 + α 2L
2

+αL∥Λt − wt ∥∥∇f (Λt )∥.

M 2

To further bound the last term, we can apply the classic
inequality a2 + b2 ≥ 2ab together with Lemma 6.2 to obtain:
E [f (Λt +1)|Λt ] ≤ f (Λt ) − α ∥∇f (Λt )∥2 + α 2L
2

M 2
+α ∥∇f (Λt )∥2/2 + α 3L2τ M 2(P − Q)

.

2P 2

Rearranging terms and taking total expectation:

E (cid:2)∥∇f (Λt )∥2(cid:3) ≤

2E [f (Λt ) − f (Λt +1)]
α
+α 2τ L2M 2(P − Q)/P 2.

+ αM 2L

Summing across all t and dividing by T , we get:

E (cid:2)∥∇f (Λt )∥2(cid:3) ≤

min
1≤t ≤T

1
T

(cid:213)

t

E (cid:2)∥∇f (Λt )∥2(cid:3) ≤

≤

2 (f (Λ0) − m)
αT

+ αM 2L + α 2L2τ M 2(P − Q)/P 2.

We now study the set of conditions for each of the three
RHS terms to be less than ϵ/12. We have that it is sufficient
for the following three conditions to hold:

;

1. T ≥ 24(f (Λ0)−m)
α ϵ
ϵ
2. α ≤
12M 2L ;
√
√
3. α ≤

ϵ P
12L2τ M 2(P −Q )

.

All these conditions hold by assumption from the theorem
statement. We have therefore obtained that there exists t ⋆
such that ∥∇f (Λt ⋆)∥2 ≤ ϵ/4. However, by smoothness and
Lemma 6.2 we know that

E∥∇f (Λt ⋆) − ∇f (wt ⋆)∥2 ≤ α 2Lτ M 2(P − Q)/P 2 ≤ ϵ/4,
where we have used the assumption in the theorem state-
ment on the upper bound on α. Finally, we can apply the
classic inequality ∥a + b ∥2 ≤ 2(∥a∥2 + ∥b ∥2) to obtain that
E∥∇f (wt ⋆)∥2 ≤ ϵ.

□

Discussion We make the following observations regarding
the bound. First, we note that, since we analyze non-convex
objectives, we must settle for a weaker notion of convergence
than in the convex case (where we can prove convergence
to a global minimum): specifically, we prove that, for a given
sequence of learning rates, the algorithm will converge to
a point of negligible gradient. Second, we note the depen-
√
τ and (cid:112)(P − Q) for the number of iterations to
dence in
convergence, i.e.:

T ≥ Θ

(cid:32)

(f (w0) − m)(cid:112)τ (P − Q)
Pϵ 3/2

(cid:33)

.

Thus, we would like the maximum delay and the number
of “missed” gradients per round to be minimized. However,
obviously, having no stragglers would imply higher synchro-
nization cost. This suggests that, in practice, the algorithm

8

should trade off the additional performance cost of synchro-
nization with the slower convergence due to delayed gradient
information.

immediately; and majority allreduce has to wait for a ran-
domly specified process to trigger the operation, and thus it
is moderately delayed.

7 Evaluation
Experiments are conducted on the CSCS Piz Daint super-
computer with Cray Aries interconnect. Each XC50 compute
node contains a 12-core Intel Xeon E5-2690 CPU with 64 GiB
RAM, and one NVIDIA Tesla P100 GPU. The communication
library is Cray MPICH 7.7.2. We use one MPI process per
node and utilize the GPU for acceleration in all following
experiments. First, we evaluate the performance of the par-
tial collective operations using a microbenchmark. Then, we
use the different neural networks summarized in Table 1
to compare our eager-SGD with the state-of-the-art synch-
SGD implementations (Horovod [51] and Deep500 [9]), the
asynchronous centralized SGD [1], and the gossip-based
SGDs [4, 39], under simulated and real workload imbalance
environments.

Average

Figure 9.
latency comparison between
MPI_Allreduce and partial allreduce running on 32
processes by 64 iterations. Processes are linearly skewed by
injecting load imbalance from 1 ms to 32 ms.

For the partial collective operations, we refer to the initia-
tor together with the processes that arrive at the operation
before the initiator as the active processes, which contribute
the latest data (line 5 in Fig. 8). The other processes only
contribute null values (line 13 in Fig. 8). For solo allreduce,
since the fastest process is the initiator and all the processes
are fully skewed, the Number of Active Processes (NAP) is
around 1, as shown in Fig. 9. For majority allreduce, since the
initiator is randomly specified, the expectation of NAP is half
of the total processes. On average 16 out of 32 processes for
majority allreduce are active processes, which means half of
the processes contribute the latest data when the processes
are fully skewed.

7.2 Throughput and Convergence with Simulated

Workload Imbalance

We use three networks shown in Table 1, including a multi-
layer perceptron (MLP), ResNet-32, and ResNet-50, to evalu-
ate the performance of eager-SGD with simulated workload
imbalance. From the application perspective, these three net-
works have balanced workload during the distributed train-
ing, since each batch has equivalent workload. We manually
inject delays to simulate the load imbalance environment
caused by the training system, as discussed in Section 2.3.

7.2.1 Hyperplane Regression, Light Load Imbalance

We generate both training and validation datasets for a 8,192-
dimensional hyperplane regression using the equation: y =

Figure 8. Microbenchmark used to test the latency of the
collective operations.

7.1 Partial Allreduce Operations

We design a microbenchmark, shown in Fig. 8, to evalu-
ate the performance of partial allreduce operations (fflib2)
and MPI_Allreduce (Cray MPICH) with unbalanced work-
load. All the processes are linearly skewed before calling
the collective operations and the average latency among all
the processes is recorded. The microbenchmark is a special
case with severe load imbalance, which is useful to verify
the statistical guarantee of majority allreduce. Experimental
results on 32 processes are presented in Fig. 9. Compared
with MPI_Allreduce, solo and majority allreduce operations
reduce the latency by on average 53.32x and 2.46x, respec-
tively. This is because all the processes (except the slowest
one) for MPI_Allreduce are delayed; solo allreduce is not
delayed since the fastest process will trigger the operation

9

        1  sendbuff={0};   recvbuff={0};  //initialization2  pid = process ID;   psize = processes number;3  for(i = 0;  i < ITER;  i++) {4      usleep(pid * 1000);  //linearly skewed5      sendbuff = {1};  //assign useful values67      begin = MPI_Wtime();8          call  MPI/Solo/Majority_Allreduce(sendbuff, recvbuff);  9      latencypid = MPI_Wtime() - begin; 10    average_latency = sum(latencypid)/psize; 11       12    MPI_Barrier();  //synchronize before the next iteration13    sendbuff = {0};    recvbuff = {0};  //reset to dummy values                  14   } 0102064 B512 B4 KB32 KB256 KB4 MBMessage SizeLatency (milliseconds)MPI_AllreduceMajority_AllreduceSolo_Allreduce08162432Number of active processesNAP of Majority_AllreduceNAP of Solo_Allreduce+ standard deviation- standard deviationTable 1. Neural networks used for evaluation.

Tasks

Models

Parameters

Train data size Batch size Epochs

Processes

Hyperplane regression One-layer MLP
ResNet-32 [21]
Cifar-10
ResNet-50 [21]
ImageNet [14]
Inception+LSTM [61]
UCF101 [53]

8,193
467,194
25,559,081
34,663,525

32,768 points
50,000 images
1,281,167 images
9,537 videos

2,048
512
8,192
128

48
190
90
50

8
8
64
8

imbalance, the worse the performance of synch-SGD because
of the synchronization overhead. On the other hand, the
performance of eager-SGD is stable. Given that the through-
put on a single GPU node with batch size of 2,048 is 0.64
steps/s, eager-SGD with 400 ms load imbalance injection still
achieves 3.8x speedup in strong scaling on 8 GPU nodes.

Fig. 10b presents the validation loss (mean squared error)
as a function of the training time, which shows that eager-
SGD using solo allreduce converges with equivalent loss
value (around 4.7) to synch-SGD but significantly reduces
the training time. Since the processes are not severely skewed
and the stale gradients are added to the next training iteration
(as discussed in Section 5), using solo allreduce is enough for
convergence. When using majority allreduce, the throughput
of eager-SGD is lower than using solo allreduce (1.64 step/s
vs 1.37 step/s with 200 ms load imbalance injection).

7.2.2 ResNet-50 on ImageNet, Light Load Imbalance

Residual Network (ResNet) [21] is widely used in computer
vision tasks. To evaluate the performance of eager-SGD, we
use 64 processes with a total batch size of 8,192 to train
ResNet-50 on ImageNet for 90 epochs. To simulate the load
imbalance environment, we randomly select 4 processes out
of the 64 processes at every training step to inject a certain
amount of delay, according to the performance variability
on Cloud machines discussed in Section 2.3.

Fig. 11a presents the throughput comparison between
synch-SGD (Horovod and Deep500) and eager-SGD using
solo allreduce. With 300 and 460 ms load imbalance injection,
eager-SGD achieves 1.25x and 1.29x speedup over Deep500,
respectively; 1.14x and 1.27x speedup over Horovod, respec-
tively. Given that the throughput of a single GPU node with
batch size of 128 is 1.56 steps/s, eager-SGD running on 64
processes with 460 ms load imbalance injection still achieves
49.8x speedup in weak scaling.

Fig. 11b and Fig. 11c present the Top-1 train and test ac-
curacy as a function of the training time, respectively. We
train the model three times for each SGD, and obtain stable
accuracy results. For top-1 accuracy, Deep500 achieves 79.1%
train accuracy and 75.7% test accuracy, Horovod achieves
79.0% train accuracy and 75.8% test accuracy, while eager-
SGD using solo allreduce achieves 78.4% train accuracy and
75.2% test accuracy on average over different load imbalance
injections. Note that without model synchronization at every
10 epochs, the top-1 test accuracy of eager-SGD decreases

(a) Throughput comparison.

(b) Validation Loss Comparison.

Figure 10. Comparison between synch-SGD and eager-SGD
for hyperplane regression using 8 processes. "synch/eager-
SGD-200/300/400" represent 200/300/400 ms load imbalance
injection, respectively. Each point is at the boundary of one
epoch.

a0x0 +a1x1 + ... +a8191x8191 +noise, where (x0, x1, ..., x8191) is
the input vector and y is the label. An one-layer MLP is used
to learn the coefficients (a0, a1, ..., a8191) of the hyperplane.
We use 8 processes with the total batch size of 2,048 to train
the model for 48 epochs. To simulate the load imbalance
environment, we randomly select one process out of the 8
processes at every training step to inject a certain amount
of delay, according to the variability shown in Fig. 4.

The throughput comparison between synch-SGD (Deep500)
and eager-SGD (using solo allreduce) is shown in Fig. 10a.
With 200, 300, and 400 ms load imbalance injection, eager-
SGD achieves 1.50x, 1.75x, and 2.01x speedup over synch-
SGD, respectively. We observe that the more severe the load

10

1.21.41.61.82.02.22.401020304050EpochsThroughput (steps/second)synch-SGD-200 (Deep500)synch-SGD-300 (Deep500)synch-SGD-400 (Deep500)eager-SGD-200 (solo)eager-SGD-300 (solo)eager-SGD-400 (solo)0.0e+05.0e+61.0e+71.5e+70100200300400500600Training time (seconds)Validation losssynch-SGD-200 (Deep500)synch-SGD-300 (Deep500)synch-SGD-400 (Deep500)eager-SGD-200 (solo)eager-SGD-300 (solo)eager-SGD-400 (solo)~312 sfor all eager-SGD468.9 s625.3s545.1sloss=4.7loss=4.7loss=4.7loss=4.7Table 2. Throughput comparison with the asynchronous
centralized SGD and the gossip-based SGDs for ResNet-50
on ImageNet, using 64 processes (total batch size 8,192) under
load imbalance environment.

SGDs Asynch-PS [1] D-PSGD [39]

SGP [4]

eager-SGD

step/s

0.45

0.94

1.02

1.19

to 74.1%. For top-5 accuracy, synch-SGD achieves 92.6% test
accuracy, while eager-SGD using solo allreduce achieves
92.4% test accuracy on average. The experimental results on
ResNet-50 demonstrate that eager-SGD (solo) significantly
improves the training speed without losing accuracy for deep
neural networks in light load imbalance environment.

Table 2 presents the throughput comparison with the asyn-
chronous centralized SGD and the gossip-based SGDs for
ResNet-50 on ImageNet. We randomly select 4 processes out
of the 64 processes at every training step and inject 460 ms
delay for each selected process. Asynch-PS [1] is the asyn-
chronous Parameter-Sever-based (centralized) SGD provided
by TensorFlow. The throughput of Asynch-PS is the lowest
among all the SGD variants because of the performance
bottleneck on the sever. Compared with Asynch-PS, eager-
SGD achieves 2.64× speedup. D-PSGD [39] and SGP [4] are
gossip-based SGDs, which do not use global collective com-
munication primitives, such as Allreduce. Alternatively, each
process only communicates with its neighbors (two neigh-
bors for D-PSGD and SGP). However, all the processes need
to finish the communications of the current step before go-
ing to the next step. The Overlap SGP [4] can mitigate the
synchronization effect by overlapping communication and
computation. According to the parameter setup in [4], we
configure SGP to use one step of gradient computation to
overlap the communication, namely the communication syn-
chronization is delayed by one step. Using communication
topology optimizations [4, 40], each process can globally
propagate its local update using O (log P) steps. Note that
eager-SGD only uses O (1) step to globally propagate the lo-
cal update. As shown in Table 2, eager-SGD also outperforms
the gossip-based SGDs because of the feature of asynchrony.

7.2.3 ResNet-32 on Cifar-10, Severe Load Imbalance

To test the robustness of eager-SGD, we train ResNet-32 on
Cifar-10 with 8 processes for 190 epochs in a severe load
imbalance environment. All 8 processes are skewed by in-
jecting load imbalance from 50 ms to 400 ms at every training
step. The injection amount over the processes is shifted after
each step. Fig. 12 presents the test accuracy as a function of
the training time. Eager-SGD using solo allreduce has the
highest training speed but with lower test accuracy. Solo
allreduce only waits for the fastest process to inform the
other processes to participate in allreduce, but most of them
will contribute stale gradients. Majority allreduce can solve

(a) Throughput comparison. Each point is at the boundary of one
epoch.

(b) Top-1 training accuracy. Each point is at the boundary of one
epoch.

(c) Top-1 test accuracy. Each point is at the boundary of every 10
epochs.

Figure 11. Comparisons between synch-SGD and eager-
SGD for ResNet-50 on ImageNet using 64 processes.
"synch/eager-SGD-300/460" represent 300/460 ms load im-
balance injection, respectively.

11

0.91.01.11.20102030405060708090EpochsThroughput (steps/second)synch-SGD-300 (Deep500)synch-SGD-460 (Deep500)synch-SGD-300 (Horovod)synch-SGD-460 (Horovod)eager-SGD-300 (solo)eager-SGD-460 (solo)01020304050607080050100150200250187 m196 m213 m233 m249m252 m78.4%79.0%synch-SGD-300 (Deep500)synch-SGD-460 (Deep500)synch-SGD-300 (Horovod)synch-SGD-460 (Horovod)eager-SGD-300 (solo)eager-SGD-460 (solo)Training time (minutes)Top-1 train accuracy %01020304050607080050100150200250187 m196 m213 m233 m249m252 m75.2%75.8%synch-SGD-300 (Deep500)synch-SGD-460 (Deep500)synch-SGD-300 (Horovod)synch-SGD-460 (Horovod)eager-SGD-300 (solo)eager-SGD-460 (solo)Training time (minutes)Top-1 test accuracy %Figure 12. Top-1 test accuracy of synch-SGD (Horovod)
and eager-SGD for ResNet-32 on Cifar-10 using 8 processes.
Each point is at the boundary of every 10 epochs.

the lower accuracy problem caused by solo allreduce, which
achieves approximately equivalent accuracy to synch-SGD
with 1.29x speedup. The results demonstrate that eager-SGD
using majority allreduce is tolerant to severe load imbalance.

7.3 Case Study: Video Classification

As discussed in Section 2.1, LSTM on UCF101 for video clas-
sification has inherent workload imbalance because of differ-
ent workload for different batches. We use Inception v3 [55],
a CNN model, to extract a 2,048-wide feature from each
frame of the videos, and then pass the sequences of features
to an LSTM model. The training time reported in the paper
is only for the LSTM model, not including the preprocessing
time using Inception v3.

To evaluate eager-SGD, we use 8 processes with a total
batch size of 128 to train LSTM on UCF101 for 50 epochs
(more information is in Table 1). Fig. 13a and Fig. 13b present
the train and test accuracy as a function of the training
time, respectively. For each SGD, we train the model four
times and plot the curves using the average values. Colored
areas around the curves are confidence intervals with the
boundaries representing the standard deviation. Although
eager-SGD using solo allreduce achieves 1.64x speedup over
Horovod, it has lower accuracy. Eager-SGD (solo) achieves
on average 60.6% (up to 70.4%) top-1 test accuracy while
Horovod achieves on average 69.6%. This is because the
workload of the video model is severely unbalanced, and
solo allreduce introduces too many stale gradients. In con-
trast, eager-SGD using majority allreduce achieves 1.27x
speedup over Horovod with equivalent accuracy. For exam-
ple, Horovod achieves on average 69.6% top-1 test accuracy
(up to 72.2%) and 90.4% top-5 test accuracy (up to 91.9%),
while eager-SGD using majority allreduce achieves on aver-
age 69.7% top-1 test accuracy (up to 72.8%) and 90.0% top-5
test accuracy (up to 91.7%). Train accuracy results (in Fig. 13a)
show a similar trend as the test accuracy. Horovod achieves

(a) Training accuracy.

(b) Test accuracy.

Figure 13. Training results for LSTM on UCF101 using 8
processes. Each point is at the boundary of one epoch.

on average 86.1% top-1 train accuracy and 96.6% top-5 train
accuracy, while eager-SGD using majority allreduce achieves
on average 86.7% top-1 train accuracy and 96.1% top-5 train
accuracy. All the accuracy results are consistent with that
claimed in recent work [61]. The training speed and accuracy
for Deep500 (not plotted in figures) are similar to Horovod.
The results show that majority allreduce, with its statistical
guarantee, is sufficient to both speed up training and achieve
good accuracy.

The total training time for 50 epochs using a single GPU
node with batch size of 16 and 128 is 34,301 seconds and 6,314
seconds, respectively. In weak scaling, Synch-SGD (Horovod)
and eager-SGD using majority allreduce achieve 3.72x and
4.71x speedup on 8 GPU nodes, respectively. In strong scaling,
synch-SGD and eager-SGD using majority allreduce do not
have speedup on 8 GPU nodes; in contrast, eager-SGD using
solo allreduce achieves 1.12x speedup on 8 GPU nodes in
strong scaling, but with lower accuracy. Note that increasing
the batch size can further improve the speedup in strong
scaling for eager-SGD. However, large batch sizes commonly

12

01020304050607080901000200040006000800010000Training time (seconds)Test accuracy %synch-SGD (Horovod, top1)eager-SGD (solo, top1)eager-SGD (majority, top1)synch-SGD (Horovod, top5)eager-SGD (solo, top5)eager-SGD (majority, top5)3534 stop-1: 58.0%8607 stop-1: 90.0%11128 stop-1: 92.6%01020304050607080901000100020003000400050006000700080009000Training time (seconds)Train accuracy %synch-SGD (Horovod, top1)eager-SGD (solo, top1)eager-SGD (majority, top1)synch-SGD (Horovod, top5)eager-SGD (solo, top5)eager-SGD (majority, top5)96.1%96.6%90.0%86.7%86.1%80.8%9227 s5623 s7290 s01020304050607080900100020003000400050006000700080009000Training time (seconds)Test accuracy %9227 s9227 s5623 s5623 s7290 s7290 ssynch-SGD (Horovod, top1)eager-SGD (solo, top1)eager-SGD (majority, top1)synch-SGD (Horovod, top5)eager-SGD (solo, top5)eager-SGD (majority, top5)90.0%90.4%80.5%69.7%69.6%60.6%need carefully-tuned learning rate schedules to achieve good
accuracy [59], which is out of scope.

8 Related Work
Deep learning Parameter Server SGD implementations
use synchronous [20, 38], asynchronous [11, 13], stale- [22,
62], and approximate-synchronous [29] SGD, where the lat-
ter two limit the age and insignificance of the gradients,
respectively. For synchronous Parameter Server SGD, com-
munication granularity and scheduling optimizations [33]
are studied to better overlap communication and compu-
tation. In a decentralized setting, asynchrony is achieved
by performing communication on an explicit subset of the
learners, e.g., forming a ring [40] or a general graph [58]; or
on a random subset using Gossip Algorithms [12, 34]. These
modes achieve some degree of asynchrony, but take O (P)
or O (log P) (for ring or gossip-based schemes, respectively)
update steps to disseminate the information to all P learners.
To the best of our knowledge, this is the first work that imple-
ments asynchronous and stale-synchronous decentralized
SGD where the messages propagate to all nodes in one step.

Collective communication Several algorithms can be used
to implement allreduce operations, and the optimal algo-
rithm depends on network topology, number of processes,
and message size [56]. For large message sizes and large
number of processes, practical implementations employ the
ring-allreduce [19] or the Rabenseifner’s Algorithm [44]. In-
dependently from the specific algorithm, the semantic of
the allreduce implies processes synchronization. With eager-
SGD we relax this synchronization by using solo and major-
ity allreduce operations.

9 Conclusions
In this work, we show that load imbalance is prevalent in
deep learning problems with variable-length inputs, and
increasingly becoming an issue in cloud systems. To that
end, we propose eager-SGD: an asynchronous decentralized
version of SGD where fast processes contribute gradients
without waiting for slow processes. Using the resilience of
machine learning to bounded error, we implement two vari-
ants of partial collectives — solo and majority allreduce —
enabling this behavior without a central parameter server.
The experimental results reaffirm our theoretical analysis,
showing that eager-SGD using solo allreduce speeds up the
training process (1.29× and 2.64× faster than the synchro-
nous decentralized SGD and the asynchronous centralized
SGD on ImageNet, respectively) in light load imbalance envi-
ronments. As the load imbalance increases, the convergence
rate of solo allreduce degrades, in which case majority allre-
duce speeds up the training process (1.27× faster than the
synchronous decentralized SGD on UCF101) yet desirable
generalization.

13

The research can extend in different directions. Firstly,
the promising results make eager-SGD attractive for other
applications as well, such as language models and object
detection. Secondly, in order to provide different quorum
sizes, it is possible to construct a spectrum between solo,
majority, and full collectives. Lastly, partial collectives can
be used for other algorithms beyond SGD.

Acknowledgments
This project has received funding from the European Re-
search Council (ERC) under the European UnionâĂŹs Hori-
zon 2020 programme (grant agreement DAPP, No. 678880;
grant agreement No. 805223, ERC Starting Grant ScaleML).
We also would like to thank the Swiss National Supercom-
puting Center (CSCS) for providing the computing resources
and for their excellent technical support.

References
[1] Martín Abadi, Ashish Agarwal, Paul Barham, Eugene Brevdo, Zhifeng
Chen, Craig Citro, Greg S. Corrado, Andy Davis, Jeffrey Dean, Matthieu
Devin, Sanjay Ghemawat, Ian Goodfellow, Andrew Harp, Geoffrey
Irving, Michael Isard, Yangqing Jia, Rafal Jozefowicz, Lukasz Kaiser,
Manjunath Kudlur, Josh Levenberg, Dandelion Mané, Rajat Monga,
Sherry Moore, Derek Murray, Chris Olah, Mike Schuster, Jonathon
Shlens, Benoit Steiner, Ilya Sutskever, Kunal Talwar, Paul Tucker, Vin-
cent Vanhoucke, Vijay Vasudevan, Fernanda Viégas, Oriol Vinyals,
Pete Warden, Martin Wattenberg, Martin Wicke, Yuan Yu, and Xi-
aoqiang Zheng. 2015. TensorFlow: Large-Scale Machine Learning
on Heterogeneous Systems. https://www.tensorflow.org/ Software
available from tensorflow.org.

[2] Dan Alistarh, Torsten Hoefler, Mikael Johansson, Sarit Khirirat, Nikola
Konstantinov, and Cedric Renggli. 2018. The Convergence of Sparsi-
fied Gradient Methods. In Advances in Neural Information Processing
Systems 31. Curran Associates, Inc.

[3] Dario Amodei and Danny Hernandez. 2018. AI and Compute.

https://openai.com/blog/ai-and-compute/.

[4] Mahmoud Assran, Nicolas Loizou, Nicolas Ballas, and Michael Rabbat.
2018. Stochastic gradient push for distributed deep learning. arXiv
preprint arXiv:1811.10792 (2018).

[5] A. Awan, K. Hamidouche, J. Hashmi, and D. Panda. 2017. S-Caffe:
Co-designing MPI Runtimes and Caffe for Scalable Deep Learning on
Modern GPU Clusters.

[6] Jimmy Ba and Brendan Frey. 2013. Adaptive dropout for training deep
neural networks. In Advances in Neural Information Processing Systems.
3084–3092.

[7] Nicolas Ballas, Li Yao, Chris Pal, and Aaron Courville. 2015. Delving
Deeper into Convolutional Networks for Learning Video Representa-
tions. arXiv e-prints (2015). arXiv:1511.06432

[8] Brian W Barrett, Ron Brightwell, , E Ryan Grant, Scott Hemmert,
Kevin Pedretti, Kyle Wheeler, Keith D Underwood, R Reisen, Torsten
Hoefler, Arthur B Maccabe, and Trammell Hudson. 2018. The Portals
4.2 network programming interface. Sandia National Laboratories,
November 2018, Technical Report SAND2017-3825 (2018).

[9] T. Ben-Nun, M. Besta, S. Huber, A. N. Ziogas, D. Peter, and T. Hoefler.
2019. A Modular Benchmarking Infrastructure for High-Performance
and Reproducible Deep Learning. In 2019 IEEE International Parallel
and Distributed Processing Symposium (IPDPS). 66–77. https://doi.org/
10.1109/IPDPS.2019.00018

[10] T. Ben-Nun and T. Hoefler. 2018. Demystifying Parallel and Dis-
tributed Deep Learning: An In-Depth Concurrency Analysis. CoRR

abs/1802.09941 (Feb. 2018).

[11] Trishul Chilimbi, Yutaka Suzue, Johnson Apacible, and Karthik Kalya-
naraman. 2014. Project Adam: Building an Efficient and Scalable Deep
Learning Training System. In 11th USENIX Symposium on Operating
Systems Design and Implementation (OSDI 14). USENIX Association,
Broomfield, CO, 571–582. https://www.usenix.org/conference/osdi14/
technical-sessions/presentation/chilimbi

[12] Jeff Daily, Abhinav Vishnu, Charles Siegel, Thomas Warfel, and Vinay
Amatya. 2018. GossipGraD: Scalable Deep Learning using Gos-
sip Communication based Asynchronous Gradient Descent. CoRR
abs/1803.05880 (2018). arXiv:1803.05880 http://arxiv.org/abs/1803.
05880

[13] Jeffrey Dean, Greg Corrado, Rajat Monga, Kai Chen, Matthieu Devin,
Mark Mao, Andrew Senior, Paul Tucker, Ke Yang, Quoc V Le, and
Andrew Y. Ng. 2012. Large scale distributed deep networks. In Advances
in neural information processing systems. 1223–1231.

[14] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei.
2009.
Imagenet: A large-scale hierarchical image database. In Pro-
ceedings of the 2009 IEEE Conference on Computer Vision and Pattern
Recognition (CVPR). IEEE, 248–255.

[15] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova.
2018. BERT: Pre-training of Deep Bidirectional Transformers for Lan-
guage Understanding. CoRR abs/1810.04805 (2018). arXiv:1810.04805
http://arxiv.org/abs/1810.04805

[16] Terrance Devries and Graham W. Taylor. 2017.

Improved Regu-
larization of Convolutional Neural Networks with Cutout. CoRR
abs/1708.04552 (2017). arXiv:1708.04552 http://arxiv.org/abs/1708.
04552

[17] Salvatore Di Girolamo, Pierre Jolivet, Keith D Underwood, and Torsten
Hoefler. 2015. Exploiting offload enabled network interfaces. In 2015
IEEE 23rd Annual Symposium on High-Performance Interconnects. IEEE,
26–33.

[18] Jeff Donahue, Lisa Anne Hendricks, Sergio Guadarrama, Marcus
Rohrbach, Subhashini Venugopalan, Kate Saenko, and Trevor Darrell.
2014. Long-term Recurrent Convolutional Networks for Visual Recog-
nition and Description. CoRR abs/1411.4389 (2014). arXiv:1411.4389
http://arxiv.org/abs/1411.4389

[19] Andrew Gibiansky. 2017. Bringing HPC techniques to deep learn-
ing.(2017). URL http://research. baidu. com/bringing-hpc-techniques-
deep-learning (2017).

[20] Suyog Gupta, Wei Zhang, and Fei Wang. 2015. Model Accuracy and
Runtime Tradeoff in Distributed Deep Learning: A Systematic Study.
arXiv e-prints (Sep 2015). arXiv:1509.04210

[21] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. 2016. Deep
residual learning for image recognition. In Proceedings of the IEEE
conference on computer vision and pattern recognition. 770–778.
[22] Qirong Ho, James Cipar, Henggang Cui, Jin Kyu Kim, Seunghak
Lee, Phillip B. Gibbons, Garth A. Gibson, Gregory R. Ganger, and
Eric P. Xing. 2013. More Effective Distributed ML via a Stale Syn-
chronous Parallel Parameter Server. In Proceedings of the 26th Inter-
national Conference on Neural Information Processing Systems - Vol-
ume 1 (NIPS’13). Curran Associates Inc., USA, 1223–1231.
http:
//dl.acm.org/citation.cfm?id=2999611.2999748

[23] Sepp Hochreiter and Jürgen Schmidhuber. 1997. Long short-term

memory. Neural computation 9, 8 (1997), 1735–1780.

[24] Torsten Hoefler, Salvatore Di Girolamo, Konstantin Taranov, Ryan E
Grant, and Ron Brightwell. 2017. sPIN: High-performance streaming
Processing in the Network. In Proceedings of the International Confer-
ence for High Performance Computing, Networking, Storage and Analysis.
ACM, 59.

[25] Torsten Hoefler, Andrew Lumsdaine, and Wolfgang Rehm. 2007. Im-
plementation and performance analysis of non-blocking collective
operations for MPI. In Proceedings of the 2007 ACM/IEEE conference on
Supercomputing. ACM, 52.

14

[26] T. Hoefler and D. Moor. 2014. Energy, Memory, and Runtime Tradeoffs
for Implementing Collective Communication Operations. Journal of
Supercomputing Frontiers and Innovations 1, 2 (Oct. 2014), 58–75.
[27] T. Hoefler, T. Schneider, and A. Lumsdaine. 2009. The Effect of Network
Noise on Large-Scale Collective Communications. Parallel Processing
Letters (PPL) 19, 4 (Aug. 2009), 573–593.

[28] T. Hoefler, T. Schneider, and A. Lumsdaine. 2010. Characterizing the In-
fluence of System Noise on Large-Scale Applications by Simulation. In
International Conference for High Performance Computing, Networking,
Storage and Analysis (SC’10).

[29] Kevin Hsieh, Aaron Harlap, Nandita Vijaykumar, Dimitris Konomis,
Gregory R. Ganger, Phillip B. Gibbons, and Onur Mutlu. 2017. Gaia:
Geo-distributed Machine Learning Approaching LAN Speeds. In Pro-
ceedings of the 14th USENIX Conference on Networked Systems Design
and Implementation (NSDI’17). USENIX Association, Berkeley, CA,
USA, 629–647. http://dl.acm.org/citation.cfm?id=3154630.3154682

[30] G. Huang, Z. Liu, L. v. d. Maaten, and K. Q. Weinberger. 2017.
Densely Connected Convolutional Networks. In 2017 IEEE Confer-
ence on Computer Vision and Pattern Recognition (CVPR). 2261–2269.
https://doi.org/10.1109/CVPR.2017.243

[31] Alexandru Iosup, Nezih Yigitbasi, and Dick Epema. 2011. On the perfor-
mance variability of production cloud services. In 2011 11th IEEE/ACM
International Symposium on Cluster, Cloud and Grid Computing. IEEE,
104–113.

[32] Keith R Jackson, Lavanya Ramakrishnan, Krishna Muriki, Shane
Canon, Shreyas Cholia, John Shalf, Harvey J Wasserman, and
Nicholas J Wright. 2010. Performance analysis of high performance
computing applications on the amazon web services cloud. In 2nd IEEE
international conference on cloud computing technology and science.
IEEE, 159–168.

[33] Anand Jayarajan, Jinliang Wei, Garth Gibson, Alexandra Fedorova, and
Gennady Pekhimenko. 2019. Priority-based parameter propagation for
distributed DNN training. In Proceedings of the 2nd SysML Conference.
[34] Peter H. Jin, Qiaochu Yuan, Forrest N. Iandola, and Kurt Keutzer. 2016.
How to scale distributed deep learning? CoRR abs/1611.04581 (2016).
arXiv:1611.04581 http://arxiv.org/abs/1611.04581

[35] Thorsten Kurth, Sean Treichler, Joshua Romero, Mayur Mudigonda,
Nathan Luehr, Everett Phillips, Ankur Mahesh, Michael Matheson, Jack
Deslippe, Massimiliano Fatica, Prabhat, and Michael Houston. 2018.
Exascale Deep Learning for Climate Analytics. In Proceedings of the
International Conference for High Performance Computing, Networking,
Storage, and Analysis (SC ’18). IEEE Press, Piscataway, NJ, USA, Article
51, 12 pages. https://doi.org/10.1109/SC.2018.00054

[36] Y. LeCun, Y. Bengio, and G. Hinton. 2015. Deep learning. Nature 521,

7553 (2015), 436–444.

[37] Yann LeCun, Léon Bottou, Yoshua Bengio, and Patrick Haffner. 1998.
Gradient-based learning applied to document recognition. Proc. IEEE
86, 11 (1998), 2278–2324.

[38] Mu Li, David G. Andersen, Jun Woo Park, Alexander J. Smola, Amr
Ahmed, Vanja Josifovski, James Long, Eugene J. Shekita, and Bor-Yiing
Su. 2014. Scaling Distributed Machine Learning with the Parameter
Server. In Proceedings of the 11th USENIX Conference on Operating Sys-
tems Design and Implementation (OSDI’14). USENIX Association, Berke-
ley, CA, USA, 583–598. http://dl.acm.org/citation.cfm?id=2685048.
2685095

[39] Xiangru Lian, Ce Zhang, Huan Zhang, Cho-Jui Hsieh, Wei Zhang, and
Ji Liu. 2017. Can Decentralized Algorithms Outperform Centralized Al-
gorithms? A Case Study for Decentralized Parallel Stochastic Gradient
Descent. In Proceedings of the 31st International Conference on Neural
Information Processing Systems (NIPS’17). Curran Associates Inc., USA,
5336–5346. http://dl.acm.org/citation.cfm?id=3295222.3295285
[40] Xiangru Lian, Wei Zhang, Ce Zhang, and Ji Liu. 2018. Asynchronous
Decentralized Parallel Stochastic Gradient Descent. In Proceedings of
the 35th International Conference on Machine Learning (Proceedings of

[58] Pengtao Xie, Jin Kyu Kim, Yi Zhou, Qirong Ho, Abhimanu Kumar,
Yaoliang Yu, and Eric Xing. 2016. Lighter-communication Distributed
Machine Learning via Sufficient Factor Broadcasting. In Proceedings of
the Thirty-Second Conference on Uncertainty in Artificial Intelligence
(UAI’16). AUAI Press, Arlington, Virginia, United States, 795–804. http:
//dl.acm.org/citation.cfm?id=3020948.3021030

[59] Yang You, Zhao Zhang, Cho-Jui Hsieh, James Demmel, and Kurt
Imagenet training in minutes. In Proceedings of the

Keutzer. 2018.
47th International Conference on Parallel Processing. ACM, 1.

[60] Joe Yue-Hei Ng, Matthew Hausknecht, Sudheendra Vijayanarasimhan,
Oriol Vinyals, Rajat Monga, and George Toderici. 2015. Beyond Short
Snippets: Deep Networks for Video Classification. In Proceedings of the
IEEE Conference on Computer Vision and Pattern Recognition (CVPR).
[61] Joe Yue-Hei Ng, Matthew Hausknecht, Sudheendra Vijayanarasimhan,
Oriol Vinyals, Rajat Monga, and George Toderici. 2015. Beyond short
snippets: Deep networks for video classification. In Proceedings of the
IEEE conference on computer vision and pattern recognition. 4694–4702.
[62] Wei Zhang, Suyog Gupta, Xiangru Lian, and Ji Liu. 2015. Staleness-
arXiv preprint

aware async-sgd for distributed deep learning.
arXiv:1511.05950 (2015).

Machine Learning Research), Jennifer Dy and Andreas Krause (Eds.),
Vol. 80. PMLR, StockholmsmÃďssan, Stockholm Sweden, 3043–3052.
http://proceedings.mlr.press/v80/lian18a.html

[41] Amrita Mathuriya, Deborah Bard, Peter Mendygral, Lawrence Mead-
ows, James Arnemann, Lei Shao, Siyu He, Tuomas Kärnä, Diana Moise,
Simon J. Pennycook, Kristyn Maschhoff, Jason Sewall, Nalini Kumar,
Shirley Ho, Michael F. Ringenburg, Prabhat, and Victor Lee. 2018. Cos-
moFlow: Using Deep Learning to Learn the Universe at Scale. In Pro-
ceedings of the International Conference for High Performance Comput-
ing, Networking, Storage, and Analysis (SC ’18). IEEE Press, Piscataway,
NJ, USA, Article 65, 11 pages. https://doi.org/10.1109/SC.2018.00068
[42] Message Passing Interface Forum. 2015. MPI: A Message-Passing

Interface Standard Version 3.1.

[43] Pitch Patarasuk and Xin Yuan. 2009. Bandwidth Optimal All-reduce
Algorithms for Clusters of Workstations. J. Parallel Distrib. Comput.
69, 2 (Feb. 2009), 117–124. https://doi.org/10.1016/j.jpdc.2008.09.002
[44] Rolf Rabenseifner. 2004. Optimization of collective reduction opera-
tions. In International Conference on Computational Science. Springer,
1–9.

[45] Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei,
and Ilya Sutskever. 2018. Language Models are Unsupervised Mul-
titask Learners.
(2018).
https://d4mucfpksywv.cloudfront.net/
better-language-models/language-models.pdf

[46] B. Recht, C. Re, S. Wright, and F. Niu. 2011. Hogwild: A Lock-Free
Approach to Parallelizing Stochastic Gradient Descent. In Advances in
Neural Information Processing Systems 24. 693–701.

[47] Cédric Renggli, Dan Alistarh, and Torsten Hoefler. 2018. SparCML:
High-Performance Sparse Communication for Machine Learning.
CoRR abs/1802.08021 (2018). arXiv:1802.08021 http://arxiv.org/abs/
1802.08021

[48] Herbert Robbins and Sutton Monro. 1951. A Stochastic Approximation

Method. The Annals of Mathematical Statistics (1951).

[49] Jörg Schad, Jens Dittrich, and Jorge-Arnulfo Quiané-Ruiz. 2010. Run-
time measurements in the cloud: observing, analyzing, and reducing
variance. Proceedings of the VLDB Endowment 3, 1-2 (2010), 460–471.
[50] Frank Seide, Hao Fu, Jasha Droppo, Gang Li, and Dong Yu. 2014. 1-
Bit Stochastic Gradient Descent and its Application to Data-Parallel
Distributed Training of Speech DNNs. In Fifteenth Annual Conference
of the International Speech Communication Association.

[51] Alexander Sergeev and Mike Del Balso. 2018. Horovod: fast
and easy distributed deep learning in TensorFlow. arXiv preprint
arXiv:1802.05799 (2018).

[52] Karen Simonyan and Andrew Zisserman. 2014. Very deep convo-
lutional networks for large-scale image recognition. arXiv preprint
arXiv:1409.1556 (2014).

[53] Khurram Soomro, Amir Roshan Zamir, and Mubarak Shah. 2012.
UCF101: A dataset of 101 human actions classes from videos in the
wild. arXiv preprint arXiv:1212.0402 (2012).

[54] Nikko Strom. 2015. Scalable distributed DNN training using com-
modity GPU cloud computing. In Sixteenth Annual Conference of the
International Speech Communication Association.

[55] Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jon Shlens, and
Zbigniew Wojna. 2016. Rethinking the inception architecture for
computer vision. In Proceedings of the IEEE conference on computer
vision and pattern recognition. 2818–2826.

[56] Rajeev Thakur, Rolf Rabenseifner, and William Gropp. 2005. Opti-
mization of collective communication operations in MPICH. The
International Journal of High Performance Computing Applications 19,
1 (2005), 49–66.

[57] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion
Jones, Aidan N Gomez, Łukasz Kaiser, and Illia Polosukhin. 2017. At-
tention is all you need. In Advances in Neural Information Processing
Systems. 5998–6008.

15

A Artifact Appendix
A.1 Abstract

We provide source code of eager-SGD and scripts to run
experiments from the paper. This artifact is run on the Piz
Daint supercomputer. This artifact supports the paper by
making it possible to reproduce the figures and numbers in
this paper, and it can be validated by comparing the figures
and results that this artifactâĂŹs scripts generate with the
data from the paper.

A.2 Artifact check-list (meta-information)
• Algorithm: Eager Stochastic Gradient Descent (eager-SGD)
• Compilation: cmake, g++, Python 3.6
• Data set: ImageNet, CIFAR-10, UCF101
• Run-time environment: Cray MPICH 7.7.2, TensorFlow-gpu

r1.11, Horovod, mpi4py, Keras

• Hardware: Piz Daint Supercomputer (Intel Xeon E5-2690 CPU,

NVIDIA Tesla P100 GPU)

• Execution: SLURM job scheduler on Piz Daint
• Metrics: Execution time, training throughput, loss values, Top1

and Top5 accuracy

• Output: TXT files and Figures
• Experiments: Use the provided scripts in the artifact to build,

schedule jobs, and generate figures

• How much disk space required (approximately)?: 400 GB
• How much time is needed to prepare workflow (approxi-

mately)?: Assuming access to Piz Daint, 30 minutes

• How much time is needed to complete experiments (ap-
proximately)?: About 90 hours if each job can be scheduled to
run immediately. Considering the job queuing time, it may take
one week.

A.3 Description

A.3.1 How delivered

> cd $WORK/eager-SGD-artifact/eager-SGD
> pip install -r requirements.txt

3. Compile fflib2 and set the environment variable.

> cd $WORK/eager-SGD-artifact/eager-SGD/fflib2/lib
> cmake .. && make
> export LD_LIBRARY_PATH=$WORK/eager-SGD-artifact/

eager-SGD/fflib2/lib:$LD_LIBRARY_PATH
4. Configure a CMakelist file which will be used for compiling the
customized Tensorflow operators.

> vim $WORK/eager-SGD-artifact/eager-SGD/deep500/

deep500/frameworks/tensorflow/custom_operators/
CMakeLists.txt

Update include_directories and link_directories by the
path where fflib2 is installed. Set TENSORFLOW_PATH by the path
where TensorFlow is installed.

> export PYTHONPATH=$PYTHONPATH:$WORK/eager-SGD-

artifact/eager-SGD/deep500/

A.5 Experiment workflow

To run experiments, users run the provided scripts that will sched-
ule runs of the executable on Piz Daint via sbatch. To run jobs on
Piz Daint, one must put them on a queue and wait until they are
scheduled. Once these experiments finish, the results of execution
time, loss values, Top1 and Top5 accuracy will be output. We pro-
vide scripts that will compile these output results into TXTs using
Python, and from these TXTs, we have included scripts that will
use R to create the figures that we used in the paper.

A.6 Evaluation and expected result

Users are expected to reproduce the results in this paper, specifically
generating the figures in Section 7. Different versions of MPICH,
TensorFlow-gpu, Horovod, and mpi4py may lead to slightly variant
results compared with the numbers reported in the paper, but this
does not affect the general trends claimed in the paper.

Via Dropbox:
eager-SGD-artifact.zip?dl=0

https://www.dropbox.com/s/3k8xgw0rh0s0m7j/

1. Evaluate solo and majority allreduce and generate Fig. 9.

> cd $WORK/eager-SGD-artifact/test-scripts/allreduce

A.3.2 Hardware dependencies

This artifact uses Piz Daint supercomputer.

A.3.3 Software dependencies

To run the experiments, Python 3.6, TensorFlow-GPU, Horovod,
mpi4py, MPICH, and Keras are required. To plot out the figures,
RStudio and ggplot2 are required.

A.3.4 Data sets

Download the training and validation images of ImageNet from
http://www.image-net.org/challenges/LSVRC/2010/downloads
Download the binary version of CIFAR-10 from https://www.cs.
toronto.edu/~kriz/cifar-10-binary.tar.gz
Download UCF101 from https://www.crcv.ucf.edu/data/UCF101/
UCF101.rar

A.4 Installation
1. Download the artifact and move it to your personal $WORK direc-
tory. Extract the artifact using unzip.
2. Install the dependent Python modules.

-scripts

Submit the jobs.
> ./sbatch_jobs.sh
It may take about 10 minutes to finish the jobs, and then outputs

majority.txt, solo.txt, and mpi.txt. Next, run the file
statistics_summary.py to read the output files and calculate the
mean and standard deviation for the latency and results.

> python statistics_summary.py
Now it should generate latency_results_summary.txt. Next,
run the file ./plotRstudio/plot-Figure9.R (Rstudio and ggplot2
are required) to generate Fig. 9. Detailed steps are stated in
./plotRstudio/ReadMe.

The expected results are as follows:
a) For the average latency, Solo_Allreduce < Majority_

Allreduce < MPI_Allreduce, where "<" means "less than".
b) For the average results, Solo_Allreduce < Majority_
Allreduce < MPI_Allreduce, where "<" means "less than".

2. Train hyperplane regression and generate Fig. 10.

> cd ./$WORK/eager-SGD-artifact/test-scripts/

hyperplane-scripts

16

Submit the jobs.
> ./sbatch_jobs.sh
It may take about 20 minutes to finish the jobs, and then outputs

b) For the accuracy, eager-SGD (majority) is very close to synch-
SGD (horovod); however, the accuracy of eager-SGD (solo) is appar-
ently lower than eager-SGD (majority) and synch-SGD (horovod).

solo200.txt, solo300.txt, solo400.txt, dfive200.txt,
dfive300.txt, and dfive400.txt. Next, run the file statistics
_summary.py to summarize the throughput, runtime, and loss data.

> python statistics_summary.py
Now it should generate throughput_summary.txt and loss_
summary.txt. Next, run the file ./plotRstudio/plot-Figure10.R
to plot Fig. 10. Detailed steps are stated in ./plotRstudio/ReadMe.

The expected results are as follows:
a) For the throughput of training, eager-SGD achieves higher

throughput than synch-SGD.

b) Eager-SGD converges and achieves approximately equivalent

loss value to synch-SGD.

3. Train ResNet-50 on ImageNet and generate Fig. 11.

Generate the TensorFlow data format for ImageNet, which may

take several hours.

> python $WORK/eager-SGD-artifact/test-models/tf-

models-r1.11/official/data/build_imagenet_data.py

> cd $WORK/eager-SGD-artifact/test-scripts/imagenet

-scripts

Copy synchm.sh to the checkpoint directory. Submit the jobs.
> ./sbatch_jobs.sh
It may take about tens of hours to finish the jobs. It should
generate solo300.txt, solo460.txt, hvd300.txt, hvd460.txt,
dfive300.txt, and dfive460.txt, which contain the output data
of the jobs. Make sure all the jobs have been finished. Then, run
the file statistics_summary.py to read the output files and sum-
marize the throughput, runtime, and accuracy data.

> python statistics_summary.py
It generates imgnetthroughput_64p.txt, top1testimgnet_
64p_runtime.txt and top1trainimgnet_64p_runtime.txt. Next,
run the file ./plotRstudio/plot-Figure11.R to generate Fig. 11.
Detailed steps are stated in ./plotRstudio/ReadMe.

The expected results are as follows:
a) For the throughput of training, eager-SGD achieves higher

throughput than synch-SGD (Horovod and Deep500).

b) For the accuracy, eager-SGD converges and achieves approxi-

mately equivalent accuracy value to synch-SGD.

4. Train ResNet-32 on CIFAR-10 and generate Fig. 12.

> cd $WORK/eager-SGD-artifact/test-scripts/cifar10

-scripts

Copy synchm.sh to the checkpoint directory. Submit the jobs.
> ./sbatch_jobs.sh
It may take about several hours to finish the jobs, and then
outputs hvd.txt, major.txt, and solo.txt. Make sure all the jobs
have been finished. Then, run the file statistics_summary.py to
read the output files and summarize the runtime and accuracy data.

> python statistics_summary.py
It should generate cifar10_accuracy_runtime.txt. Next, run
the file ./plotRstudio/plot-Figure12.R to generate Fig. 12. De-
tailed steps are stated in ./plotRstudio/ReadMe.

The expected results are as follows:
a) For the runtime of training, eager-SGD (solo) < eager-SGD

(majority) < synch-SGD (horovod), where "<" means "less than".

17

5. Train LSTM on UCF101 and generate Fig. 13.

Extracting features from the raw data. It may take several hours.
> python $WORK/eager-SGD-artifact/test-models/lstm-

video-classification/extract_features.py

Submit the jobs.
> cd $WORK/eager-SGD-artifact/test-scripts/lstm-

scripts

> ./sbatch_jobs.sh
It may take several hours to finish the jobs. It should generate

hvd.iter1-4.txt, solo.iter1-4.txt, and majority.iter1-
4.txt, which contain the output data. Make sure all the jobs have
been finished. Then, run the file statistics_summary.py to read
the output files and summarize the runtime and accuracy data.

> python statistics_summary.py
It should generate test_statistics_summary.txt and train_

statistics_summary.txt. Next, run the file ./plotRstudio/
plot-Figure13.R to generate Fig. 13. Detailed steps are stated in
./plotRstudio/ReadMe.

The expected results are as follows:
a) For the runtime of training, eager-SGD (solo) < eager-SGD

(majority) < synch-SGD (horovod), where "<" means "less than".

b) For the accuracy, eager-SGD (majority) is very close to synch-
SGD (horovod); however, the accuracy of eager-SGD (solo) is lower
than eager-SGD (majority) and synch-SGD (horovod).

A.7 Experiment customization
Users can modify the scripts in the subdirectories of $WORK/eager
-SGD-artifact/test-scripts to customize the experiments.

Modify the line of #SBATCH --nodes=<number-of-nodes> to
change the number of nodes (processes). Modify the input param-
eter -bs=<batch-size-per-node> to change the batch size per
node. Note that the total-batch-size = number-of-nodes *
batch-size-per-node, which means the change of the number of
nodes and the batch size per node would change the total batch size.
However, different total batch sizes may lead to different results for
the train and test accuracy.

Modify the line of #SBATCH --time=<time-quota> to change

the time limit to run the job.

Users can also use eager-SGD to train other TensorFlow-based
models that are not listed in the paper. To achieve this, simply wrap
the TensorFlow optimizer using the eager-SGD optimizer, and then
use the eager-SGD optimizer instead to train the model.

A.8 Notes

Some jobs may not be scheduled to run for a surprising long time
because of the busy use of the machine. In this case, they may
be automatically cancelled by the job scheduler. This is usually
resolved by rescheduling the cancelled jobs using sbatch.

A.9 Methodology

Submission, reviewing and badging methodology:
http://cTuning.org/ae/submission-20190109.html
http://cTuning.org/ae/reviewing-20190109.html
https://www.acm.org/publications/policies/artifact-review-badging

