2
2
0
2

r
p
A
9
1

]

C
D
.
s
c
[

6
v
2
3
0
5
1
.
0
1
1
2
:
v
i
X
r
a

OneFlow: Redesign the Distributed Deep Learning Framework from Scratch

Jinhui Yuan 1 Xinqi Li 1 Cheng Cheng 1 Juncheng Liu 1 Ran Guo 1 Shenghang Cai 1 Chi Yao 1 Fei Yang 2
Xiaodong Yi 3 Chuan Wu 3 Haoran Zhang 4 Jie Zhao 5

ABSTRACT
Deep learning frameworks such as TensorFlow and PyTorch provide a productive interface for expressing and
training a deep neural network (DNN) model on a single device or using data parallelism. Still, they may not
be ﬂexible or efﬁcient enough in training emerging large models on distributed devices, which require more
sophisticated parallelism beyond data parallelism. Plugins or wrappers have been developed to strengthen these
frameworks for model or pipeline parallelism, but they complicate the usage and implementation of distributed
deep learning. Aiming at a simple, neat redesign of distributed deep learning frameworks for various paral-
lelism paradigms, we present OneFlow, a novel distributed training framework based on an SBP (split, broadcast
and partial-value) abstraction and the actor model. SBP enables much easier programming of data parallelism
and model parallelism than existing frameworks, and the actor model provides a succinct runtime mechanism
to manage the complex dependencies imposed by resource constraints, data movement and computation in dis-
tributed deep learning. We demonstrate the general applicability and efﬁciency of OneFlow for training various
large DNN models with case studies and extensive experiments. The results show that OneFlow outperforms
many well-known customized libraries built on top of the state-of-the-art frameworks. The code of OneFlow is
available at: https://github.com/Oneflow-Inc/oneflow.

INTRODUCTION

1
Deep learning (DL) models have become increasingly com-
plicated and large (Devlin et al., 2019; Brown et al., 2020;
Fedus et al., 2021; Kaplan et al., 2020). Severe challenges
arise for existing DL frameworks such as TensorFlow
(Abadi et al., 2016) and PyTorch (Paszke et al., 2019) for
training large-scale DL models, which were designed in
the early days without initially foreseeing the emerging re-
quirements, e.g., model/pipeline parallelism of large mod-
els (Brown et al., 2020; Huang et al., 2019; Wang et al.,
2019).

Depending on the structure of neural networks (NN) and
hardware conﬁguration, various parallelism schemes ﬁnd
their best usage (Ben-Nun & Hoeﬂer, 2019). Data paral-
lelism is especially suitable for DL models with a relatively
small set of parameters (usually less than tens of millions
of parameters), where near-linear speed-up can be achieved
once back propagation maximally overlaps with gradient/-
parameter communication (jea, 2021; Hashemi et al., 2019;

Research.

1OneFlow
Laboratory,
of Hong
yangf@zhejianglab.com.
Kong,
of Pennsylvania,
cwu@cs.hku.hk.
haorz@seas.upenn.edu. 5State Key Laboratory of Mathematical
Engineering and Advanced Computing, zjbc2005@163.com.
Correspondence to: Jinhui Yuan <yuanjinhui@oneﬂow.org>.

2Zhejiang
3The University
4University

Copyright 2021 by the author(s).

Peng et al., 2019; Jiang et al., 2020). Model parallelism
and pipeline parallelism are for models with a more sig-
niﬁcant number of parameters, which probably cannot ﬁt
into a single device or the communication cost is too high
for data parallelism. Stanza (Wu et al., 2018) and DLPlacer
(Pal et al., 2019) adopt data parallelism for training the con-
volutional layers and model parallelism for other layers
in convolutional neural network (CNN) models. OptCNN
(Jia et al., 2018) parallelizes CNN model training by split-
ting operations along batch and channel dimensions on ho-
mogeneous devices. Tofu (Wang et al., 2019) utilizes a
partition-n-reduce method to split a single operation into
sub-operations and deploy partitions on multiple GPUs.
FlexFlow (Jia et al., 2019) searches the SOAP (sample, op-
eration, attribute, parameter) space to exploit parallelism
within and across operations.

In the best case, a distributed DL framework should be able
to automatically generate the physical execution plan for
any chosen parallelism scheme, minimizing manual pro-
gramming efforts of users. Then a more advanced require-
ment is that the framework should be able to ﬁnd the most
appropriate parallelism strategy for any combination of NN
structure and hardware conﬁguration (Shazeer et al., 2018).
However, existing DL frameworks cannot even accomplish
the ﬁrst goal, i.e., ﬂexibly supporting various parallelism
strategies. This is the exact problem we aim to address
in this paper, with a novel redesign of distributed training

 
 
 
 
 
 
OneFlow: Redesign the Distributed Deep Learning Framework from Scratch

framework.

Some emerging open-source projects develop dedi-
cated systems or customized libraries for better sup-
port of model or pipeline parallelism.
For exam-
ple, HugeCTR (Oldridge et al., 2020) enables model
parallelism for large-scale click-through rate estimation.
Megatron-LMs (Shoeybi et al., 2020; Narayanan et al.,
2021) and DeepSpeed (dee, 2021; Rajbhandari et al., 2021;
2020) support model parallelism for pre-training large NLP
InsightFace (ins, 2021) trains large-scale face
models.
recognition models with model parallelism. However,
these systems are customized for speciﬁc applications, and
cannot be assembled together to constitute a general solu-
tion due to compatibility issues.

Wrappers or plugins have also been proposed to enhance
some mainstream DL frameworks (e.g., TensorFlow, Py-
Torch) for better support of more complex parallelism
schemes. Mesh-TensorFlow (Shazeer et al., 2018) and
GShard (Lepikhin et al., 2020) provide APIs for develop-
ers to express a wide range of parallel computation pat-
terns of DNNs on top of TensorFlow. GPipe (Huang et al.,
2019) and PipeDream (Narayanan et al., 2019) use pipelin-
ing across distributed devices to address the limited mem-
ory capacity on each device for training large DNNs on
TensorFlow and PyTorch respectively. FairScale (fairscale)
integrates techniques from Megatron-LM and DeepSpeed
to enable PyTorch with model parallelism and pipeline par-
allelism. Since the existing training frameworks were ini-
tially designed without forseeing such complicated paral-
lelism, incremental improvements over the frameworks of-
ten yield non-negligible system overhead and require sub-
stantial engineering efforts from users.

What would a generic design and efﬁcient implementation
of distributed DL frameworks be if we could know the
rapidly evolving large AI models and demand for various
parallelism schemes in advance? Could the system be sim-
pler and neater? In this paper, we explore such possibilities
and present OneFlow, a novel DNN training framework
built from scratch. OneFlow includes a holistic design
from the compiler to the runtime based on the actor model.
It adopts an SBP (split, broadcast and partial-value) ab-
straction, enabling various hybrids of data parallelism and
model parallelism in a much easier manner than existing
frameworks. The actor model provides a succinct runtime
mechanism to manage complex dependencies imposed by
resource constraints, data movement and computation in
distributed training.

We demonstrate the general applicability and efﬁciency
of OneFlow for training various large DNN models with
extensive experiments, comparing to many representative
state-of-the-art systems. The results show that, with a
much simpler and more generic implementation, OneFlow

d1

d2

compiler

f1

f2

f3

logical graph

d3

resource

d4

runtime

c11

c21

f11
d1

f12

f13
d3

b13

b12
d1

g

s

f21
d2

f22

f23
d4

b23

b22
d2

b11

r2

b21

c31

r1

c41

f31
d1

f12

f41
d2

f12

physical graph

Figure1. A typical DL framework which translates the logical
graph of a three-layer NN to a physical graph (or execution plan)
on 4 inter-connected devices.

achieves performance comparable to or slightly better than
that of the major customized libraries which are built on
top of the state-of-the-art frameworks.

2 BACKGROUND AND MOTIVATION
A DNN is typically expressed as a logical computation
graph of operators (abbreviated as op) in DL frameworks,
which is manually programmed or automatically converted
by a compiler into a physical graph composed of opti-
mized kernels for execution at runtime (Abadi et al., 2016).
Distributed training involves mandatory communication
ops for data (gradient, parameters, or activations) ex-
change among devices (Li et al., 2014; Goyal et al., 2017;
Chen et al., 2016a). The inter-device bandwidth is still one
or two orders of magnitude lower than that of data access
within a device (Jiang et al., 2020; Narayanan et al., 2019).
Therefore, a distributed DL framework should treat data
movement as a ﬁrst-class citizen as computation.

2.1 Distributing the Workload in Spatial Domain
Spatial Scheduling speciﬁes how to spread the ops across
multiple devices. Figure 1 illustrates a training job with
three computation ops f1, f2, f3 scheduled onto four inter-
connected devices d1, d2, d3, d4. f1 and f2 are executed on
d1 and d2 with data parallelism, and f3 runs on d3 and d4
with model parallelism. An all-gather communication op
g is inserted between {f12, f22} and {f13, f23} in the for-
ward pass, while a reduce-scatter communication op s is
required between {b13, b23} and {b12, b22} in the backward
pass. Two all-reduce collective communication ops r1 and
r2 are used to synchronize model parameters of f1 and f2.
Manually arranging the communication ops in such hybrid
parallelism case by case is labor-intensive, incurring signif-
icant obstacles in applying complex parallelism to new DL
models.

2.2 Distributing the Workload in Temporal Domain
Temporal Scheduling of dataﬂow in a DL job refers to
scheduling execution of ops in a particular order to maxi-

OneFlow: Redesign the Distributed Deep Learning Framework from Scratch

M1

O1

M2

O2

M1 executed
M2 executed

O1 executed

1

6

2

5

O2 executing

O1 executing

O2 executed

3

4

O1

1

2

3

ready?

yes

no

update

query

call O2

insert O2

counter state

waiting list

callback

scheduler

Figure2. An example where deadlock may result with the sched-
uler in existing frameworks.

mize hardware utilization and system throughput. The best
opportunity for performance improvement usually comes
from overlapping communication and computation when-
ever possible. Execution dependencies are enforced within
and across different
instances (each mini-batch corre-
sponds to an instance) on a physical graph when using syn-
chronous stochastic gradient descent training (Chen et al.,
2016a). In Figure 1, for example, forward ops f31 and f41
cannot be scheduled ahead of the all-reduce op r1. On the
other hand, data loading and pre-processing ops c31 and
c41 can be performed simultaneously while the devices are
processing the previous batch of data; back-propagation
{b11, b21} and the all-reduce op r2 can be executed in par-
allel, without hampering the correctness.

2.3 Managing the Complex Dependencies
In mainstream DL frameworks, both data and control de-
pendencies are represented with edges in the execution
graph (Abadi et al., 2016; Paszke et al., 2019; Chen et al.,
2015). Upon the completion of each op, the scheduler
updates dependencies of the remaining ops and identiﬁes
ops that are ready to run (whose dependencies have all
been resolved). Distributed DL often experiences increased
complexity of execution dependencies and resource con-
straints (Rajbhandari et al., 2020; Huang et al., 2019).

Dependencies caused by resource sharing. The sched-
uler has to decide an appropriate execution order to avoid
out-of-memory (OOM) errors or deadlocks when multiple
ops share the same resource. Consider a simple example
in Figure 2. M1 and M2 are two data movement ops serv-
ing two computing ops O1 and O2 on the same device, re-
spectively. O1 and O2 do not depend on each other and
O1 requires more device memory to execute than O2. M1
and M2 also need some device memory to store the output
data. After M1 and M2 have occupied their memory, the
free memory capacity can only satisfy O2 but not O1, while
both O1 and O2 are in the ready set of the scheduler (as in
TensorFlow’s) at the same time. If O1 is scheduled ﬁrst,
the memory is insufﬁcient; the system may either report an
OOM error or block the scheduling thread, and the latter
may cause a deadlock. To avoid this risk, it is better for
the framework to specify an appropriate execution order in
advance (e.g., adding control dependencies between ops in
TensorFlow). If the system leverages pipelining to overlap
data movement and computation, the issue becomes even
more severe, as M1 can execute simultaneously while O1

Figure3. Interaction between callback function and the scheduler.

is processing the previous piece of data in the above exam-
ple.Resource planning at compile-time and ﬂow control at
runtime are necessary for execution stability.

Dependencies caused by data movement. The existing
DL frameworks do not treat data movement (e.g., between
host and device memories) as a normal op in the graph.
As a result, the dependencies between data movement and
computation are not represented with edges in the computa-
tion graph. For example, TensorFlow wraps intra-node data
movements in callback functions and inserts them where
necessary. As a result, some dependencies are represented
by graph edges while others are described by the invoca-
tion of callback functions. In Figure 3, O2 is wrapped in
a callback function which is expected to be invoked on the
completion of O1. However, if O2 has other dependencies
such as the output of other ops or control dependencies, the
completion of O1 does not sufﬁce to invoke O2. To cor-
rectly schedule O2, the callback function should tell the
scheduler the completion of O1: if the scheduler returns
that all the other dependencies have been resolved, O2 can
be scheduled immediately; otherwise, O2 is inserted into a
waiting list and will be scheduled in the future when other
dependencies are resolved.

In the above example, the framework has to expose the in-
ternal scheduler to users so that the inserted callback func-
tions can correctly interact with the scheduler. However,
substantial engineering efforts are required to modify the
existing DL frameworks to achieve this, as none of the ex-
isting DL frameworks expose the underlying scheduler to
users yet. Ideally, the framework should represent all the
dependencies among all the ops (including data movement)
explicitly in the graph. Once this is achieved, the graph ex-
ecutor at runtime can also be greatly simpliﬁed.

2.4 Summary
We design OneFlow, with a compiler that can automatically
generate a physical graph for data parallelism, model par-
allelism and pipeline parallelism. The compiler supports
a full analysis of all types of dependencies (e.g., resource,
data movement and computation) at compile-time. Further-
more, we design a succinct runtime for OneFlow based on
actor model, which instantiates all types of dependencies
with a uniﬁed approach of message passing among actors.

OneFlow: Redesign the Distributed Deep Learning Framework from Scratch

split(0)

split(1)

broadcast

partial sum

1

2

Device0

1

3

Device1

3

4

2

4

1

3

1

3

2

4

2

4

1

1

0

2

1

0

1

4

Figure4. Example of 4 SBP signatures to map a 2×2 global tensor
to two devices. Each block in the ﬁgure indicates an entry of a
tensor.

3 THE COMPILER
OneFlow’s compiler takes a logical computation graph and
the assigned hardware conﬁguration as inputs and gener-
ates a physical graph describing the actual execution pro-
cedure. We assume each logical op is already assigned
with an attribute placement, indicating on which nodes (i.e.,
physical machines) and devices the logical op will be de-
ployed. Consequently, a global tensor (i.e., the input or
the output of a logical op) is also mapped to multiple local
tensors (i.e., the multiple correspondences on the devices
where the logical op is placed).

3.1 Specifying Parallelism of Each Tensor and Each

Operator among Assigned Devices

We design SBP, a mathematical abstraction specifying the
mapping between a global tensor and the corresponding lo-
cal tensors, including split (S in short), broadcast (B) and
partial-value (P). The example in Figure 4 demonstrates
how a global tensor with a shape of 2 × 2 is mapped to 2
local tensors under 4 types of SBP mappings (each referred
to as an SBP signature), namely split(0), split(1), broad-
cast, and partial-sum. split indicates that the local tensors
are obtained by splitting the global tensor along a certain
axis in a balanced manner. For example, the two tensors
in the ﬁrst column in Figure 4 are obtained by splitting the
global 2 × 2 tensor by row axis, while the two tensors in the
second column are resulted in by splitting the global tensor
by column axis. As shown by the third column of Figure
4, broadcast means that each local tensor is an exact copy
of the global tensor. As demonstrated by the last column of
Figure 4, partial-value indicates that the local tensors have
the same shape as the global tensor, and the global tensor
can be obtained by performing an element-wise reduction
operation (e.g., sum, max, etc.) over all the local tensors.

When SBP signatures of the input tensors of an op are given,
SBP signature of its output tensor can also be determined.
Take M atM ul as an example. Given a data tensor X and a
weight tensor W , SBP signature of their product Y = XW
can be inferred from those of X and W , as given in Table
1. For most operators, the rule for inferring the SBP of out-
put tensor from the SBP of input tensors is straightforward.
Take the ﬁrst case in Table 1 as an example, if X is split by
row (i.e., S(0)) and W is broadcast, the result Y will also

Table1. Valid SBP signatures for MatMul

X
S(0)
B
S(1)
P (sum)
B
B

W
B
S(1)
S(0)
B
P (sum)
B

Y = XW
S(0)
S(1)
P (sum)
P (sum)
P (sum)
B

node 0

B0

A0
split(0)

A0

B0

M0

Y0

B1

M1

Y1

Logical View

A0
split(0)

Y0
split(0)

Device0

M0

M0

Y0
split(0)

Boxing

B0

Device1

Y0

broadcast

B1
split(1)

M1

Y0

broadcast

B1
split(1)

M1

Y1
split(1)

Device0

Y1
split(1)

Device1

node 1

Figure5. Example showing data movement with a boxing op in-
serted, when translating a logical graph into a physical graph.

be split by row (i.e., S(0)). Currently, we provide the SBP
deduction rule for all the operators case by case and expect
to automate the process in the future. With SBP signatures
of an op’s inputs and outputs, the parallelism strategy of
the op is fully speciﬁed. For example, S(0), B for X, W
in the ﬁrst row of Table 1 correspond to data parallelism,
and B, S(1) for X, W in the second row indicates model
parallelism.

3.2 Modeling Data Routing
Producer and consumer of the same global tensor may pre-
fer different SBP signatures for the tensor. As illustrated in
Figure 5, two M atM ul ops are connected by a global ten-
sor Y0. S(0) is Y0’s inferred SBP signature by M atM ul0;
however, M atM ul1 expects its SBP signature to be B. In
this case, a data-routing op for re-arranging or transform-
ing the local tensors of Y0 is required between M atM ul0
and M atM ul1. In distributed DL, the data-routing op for
automatically transforming the intermediate local tensors is
usually one of the common collective communication prim-
itives such as all2all, broadcast, reduce-scatter, all-reduce,
all-gather, etc. We unify all such ops as a type of boxing
ops. In the example of Figure 5, the boxing op performs an
all-gather operation internally.

The inserted boxing op may or may not incur communica-
tion cost. Table 2 lists the data size transferred between suc-
cessive SBP signatures, when the input tensors and the out-
put tensors of the boxing op are on the same set or disjoint
sets of devices, respectively. Tensor transformation across
disjoint sets of devices always incurs communication costs,
while tensor transformation within the same set of devices
may not necessarily lead to data movement (e.g., B → S

OneFlow: Redesign the Distributed Deep Learning Framework from Scratch

Table2. Data size transferred between successive SBP signatures.
p1 (p2) is the number of devices where input (output) tensors are
placed. |T | is the size of the global tensor T .

SBP 1 → SBP 2
S(i) → S(i)
S(i) → S(j)
(i 6= j)

S → B

S → P
B → S
B → B
B → P

P → S

P → B

P → P

Cost (same)
0
p1−1
|T |
p1
all2all
(p1 − 1) · |T |
all-gather
0
0
0
0
(p1 − 1) · |T |
reduce-scatter
2(p1 − 1)·|T |
all-reduce
0

Cost (disjoint)
|T |

|T |

p2 · |T |

|T |
|T |
p2 · |T |
|T |

p1 · |T |

(p1 +p2 − 1) · |T |

p1 · |T |

in Table 2, since the output tensor can be directly obtained
from the input tensor located at the same device). This is
useful for deciding the optimal parallelism strategy, that is,
by selecting SBP signatures incurring the lowest communi-
cation costs.

3.3 Difference from GShard’s Abstractions
Our SBP abstractions bear some similarities to those in
GShard (Lepikhin et al., 2020),1 i.e., split (split in GShard)
and broadcast (replicate in GShard). GShard further adds
a shard annotation to generalize split to multi-dimensional
split. In OneFlow, we use multi-dimensional split that uni-
ﬁes the split and shard in GShard. Besides split, we also
generalize all other SBP signatures to multi-dimension. For
example, a matrix can has an SBP signature as (S(0), B),
in which S(0) speciﬁes the parallelism strategy at the level
of nodes while B indicates the parallelism strategy among
devices inside the same node. As the deduction rule shown
in Figure 3, with multi-dimensional SBP, more advanced
distributed matrix multiplication such as 2D SUMMA al-
gorithm (Xu et al., 2021) can be conveniently supported.

Further, we create the partial-value signature which
GShard does not consider, but is necessary to make the
annotation system complete. For example, Table 1 lists
all the valid SBP signatures for a matrix multiplication op
(Y = XW ). If X uses S(1) and W uses S(0), the signa-
ture of Y will be P(sum), which cannot be described by
either split (i.e., split and shard in GShard) or broadcast
(i.e., replicate in GShard). GShard suggests performing re-
duce to combine the partial data to obtain the ﬁnal result
immediately after the un-reduced data are generated. How-
ever, sometime, maintaining the intermediate result as the

1SBP and GShard are independently developed being unaware
of each other, which can be proved by tracking the commit logs
of OneFlow in GitHub.

Table3. Two valid two-dimensional SBP signatures for MatMul
W
(B, S(1))
(B, S(0))

Y = XW
(S(0), S(1))
(S(0), P )

X
(S(0), B)
(S(0), S(1))

Table4. Example program for implementing SBP signatures/par-
allelism of M atM ul0 and M atM ul1 in Figure 5.

f l o w

i m p o r t o n e f l o w a s
P0= f l o w . p l a c e m e n t ( ” c u d a ” , { 0 : [ 0 , 1 ] } )
P1= f l o w . p l a c e m e n t ( ” c u d a ” , { 1 : [ 0 , 1 ] } )
a 0 s b p = f l o w . s b p . s p l i t ( 0 )
b 0 s b p = f l o w . s b p . b r o a d c a s t
y 0 s b p = f l o w . s b p . b r o a d c a s t
b 1 s b p = f l o w . s b p . s p l i t ( 1 )

1
2
3
4
5
6
7
8
9 A0= f l o w . r a n d n ( 4 , 5 , p l a c e m e n t =P0 , s b p = a 0 s b p )
10 B0= f l o w . r a n d n ( 5 , 8 , p l a c e m e n t =P0 , s b p = b 0 s b p )
11 Y0= f l o w . matmul ( A0 , B0 )
12
13 Y0 = Y0 . t o g l o b a l ( p l a c e m e n t =P1 , s b p = y 0 s b p )
14 B1= f l o w . r a n d n ( 8 , 6 , p l a c e m e n t =P1 , s b p = b 1 s b p )
15 Y2= f l o w . matmul ( Y0 , B1 )

partial-value is more efﬁcient than immediately reducing
the partial results. With partial-value, OneFlow allows the
system to choose the optimal timing of inserting a boxing
op (i.e., a reduce or all-reduce op). Take Y = U × V × W
as an example. Suppose SBP signatures of U , V and W are
S(1), S(0) and B, respectively. According to Table 1, SBP
signature of the result of U ×V is P(sum). The partial result
can be multiplied by W , since the product of P (sum) and
B is valid and the resulting signature is P (sum). With-
out partial-value signature, a boxing op, which incurs ad-
ditional communication cost, must be inserted before per-
forming the second matrix multiplication.

3.4 The Programming Interface
The design objective of the programming interface is to
keep the operator APIs and the model description the
same between a single device version and a distributed
one. For different distributed strategies, users only need
to specify the placement and SBP signatures of some ten-
sors. Consider the example in Figure 5 where M atM ul0
and M atM ul1 use data and model parallelism, respec-
tively. The code snippet in Table 4 illustrates how One-
Flow achieves the respective parallelism. Two different
placements are created in line 2 and line 3, where cuda
indicates NVIDIA GPGPUs as accelerators, and {0 : [0, 1]}
and {1 : [0, 1]} denote node and device placements (the
number before the colon is the node ID and numbers in
square brackets are device IDs). SBP signatures are cre-
ated in lines 4-7. Lines 9, 10 and 14 specify the place-
ment and SBP attribute of tensor A0, B0 and B1, respec-
tively. In line 11, SBP signature of Y0 is then inferred (as
split(0)). However, the M atM ul1 at line 15 expects the
SBP signature of Y0 to be broadcast. Therefore, in line

OneFlow: Redesign the Distributed Deep Learning Framework from Scratch

13, the to global() method is used to add a boxing op be-
tween M atM ul0 and M atM ul1 as described in Section
3.2, which explicitly transforms the placement and SBP sig-
natures of tensor Y0.
In line 13, the to global() method
transforms the placement and SBP signature of tensor Y0
from split(0) to broadcast. We note that, since the place-
ments of input tensors of M atM ul0 and M atM ul1 are dif-
ferent, i.e., P 0 and P 1, respectively, the two ops actually
work with pipeline parallelism.

With its APIs, OneFlow does not require a user to program
with various low-level communication primitives, but the
user may need to specify appropriate placements and SBP
signatures for each tensor. Placement and parallelism strat-
egy making entails separate in-depth investigation, as stud-
ied in (Jia et al., 2019; Lepikhin et al., 2020; Wang et al.,
2019; Narayanan et al., 2019; Huang et al., 2019). After
OneFlow integrates those strategies to automatically infer
optimal placement and parallelism strategy, users will no
longer manually specify the attributes of tensors or explic-
itly call to global method.
4 THE RUNTIME

We adopt the actor model (Hewitt et al., 1973) in runtime
design. We use an actor as a thin wrapper for each op and
abstract the dependencies and resources dedicated to the op
as the actor’s state. Actors interact with each other through
message passing instead of function invocation. An actor’s
state is updated whenever it receives a message from others.
We show that the actor model can elegantly solve various
issues complicated to existing DL frameworks.

4.1 The Actor Model
An actor in our runtime is associated with 4 components:
• Registers. A register is simply a container holding mem-
ory addresses of tensors. An actor is usually associated
with two types of registers: in register, used for tensors con-
sumed by the actor, and out register, for tensors produced
by the actor.
• Messages. Actor communicate with others by exchang-
ing messages: a req message from a producer (i.e., the actor
generating an output) to a consumer (i.e., the actor utilizing
the output), notifying the consumer a register containing
newly generated tensor can be read, and an ack message
from a consumer to a producer indicating that the particu-
lar register is no longer required by the consumer.
• Actions. An action corresponds to the execution of an op
that an actor is bound to (e.g., launching a GPU kernel or
performing data movement).
• A state machine. Each actor keeps track of whether all
the dependencies are resolved.

We next discuss the mechanism inside each actor’s state
machine and the message passing protocol.

time0
a1

time1
a1

time2
a1

r11
r12
r13

r11
r12
r13

r11
r12
r13

r21
r22

r21
r22

r21
r22

a2

a2

a2

r31
r32

r31
r32

r31
r32

a3

a3

a3

Figure6. Pipelining example with OneFlow’s actor-based run-
time A blank block indicates a register containing no useful data.
A ﬁlled block denotes a register containing data useful to other
actors.

4.2 Explicit Representation of Resource Dependency
Counters for both in and out registers. Each actor allo-
cates a pre-determined number of out registers in the begin-
ning, amounting to a ﬁxed memory quota for each actor. If
an actor has used up its quota, the next action will not be
scheduled even all its input tensors have been ready, until
some memory previously allocated to the actor can be re-
cycled. To achieve such goal, we associate a counter with
each register. The zero initialized in counter records the
number of the tensors held by an in register which is ready
to be consumed, while the non-zero initialized out counter
represents free memory quota. Each action results in a
decrease of some out counter. Only when the in counter
equals to an expected non-zero values and the out counter
is non-zero (indicating it has free memory to use), can the
actor trigger an action.

In existing DL frameworks, the scheduler considers an op
can start once its input tensors are ready, without taking
into account whether it can later successfully acquire mem-
ory for the output. After the op is scheduled and only just
before executing the action, the runtime tries to allocate
memory for the op on the ﬂy, which, however, may suc-
ceed or not. With in counter and out counter, OneFlow
represents resource availability as an explicit dependency
for the scheduler to decide whether an op is ready to exe-
cute. Consequently, the resource planning at compile-time
and ﬂow control at runtime are made possible.

Reference counting with message passing. Besides the in
counter and out counter, we introduce an additional zero-
initialized reference counter for each out register recording
the number of consumers who are referencing its content.
A non-zero value of a reference counter for an out register
indicates the register is in use and the content can not be
modiﬁed. Therefore, the out counter depends on the refer-
ence counter. It turns out that the reference counter can be

OneFlow: Redesign the Distributed Deep Learning Framework from Scratch

updated according to a message passing protocol:
• A producer sends a req message to a consumer and in-
creases the reference counter of the out register relating to
the message by one. A change from zero to non-zero of a
reference counter results in the decrease of an out counter.

• On receiving a req message, the consumer knows an in
register becomes available and increases the in counter by
one.
• After using data from the in register, the consumer de-
creases the in counter by one and sends an ack message to
the producer.
• On receiving an ack message from the consumer, the pro-
ducer decreases the reference counter of the out register
relating to the ack message, indicating the elimination of a
reference on the out register. If the reference counter be-
comes zero again, the corresponding out counter increases
by one, indicating the corresponding out register can be re-
cycled for the future use.

In the above protocol, if an out register is being consumed
by some consumer, its reference counter must be non-zero
and it will be no longer used by the producer to put newly
generated tensors. Such a mutual exclusion property safely
enables a zero-copy mechanism: if a pair of producer and
consumer reside on the same device, the consumer can just
directly use the producer’s output as input, without making
another copy of the content as input.

4.3 Applications: pipelining and back pressure

Allowing the initial value of an out counter for a particular
register to be larger than one facilitates the processing of
different versions of data in parallel. Each actor runs inde-
pendently, acting as a natural stage in a pipeline. Multiple
versions of the same register can be deemed as a general-
ization of the double buffering technique used in traditional
DL frameworks (nvi, 2021) In Figure 6, actor1 has 3 out
registers; actor2 and actor3 have 2 out registers respec-
tively.
• At time0, actor1 produces a register r11, while actor2
and actor3 are idle because their in counters are zero.
• At time1, actor2 triggers an action because both its in
counter and out counter are non-zeros. At the same time,
actor1 and trigger an action again (on a different micro-
batch) because its out counter is still non-zero.
• At time2, actions of all 3 actors can be triggered since all
their requirements on registers are fulﬁlled.

Essentially, the actor-based protocol is equivalent to the
credit-based ﬂow control method in asynchronous transfer
mode networks (Kung et al., 1994).
It naturally enables
back pressure for resource preservation. If all its out reg-
isters are in use, a producer stops processing due to out
counter becoming zero and no available free out register

MQ

LMQ

1

4

actora

msg

3

actorc

msg

actord

9

actore

actorb

2

∗
∗

MQ: Message Queue
LMQ: Local Message Queue

actor message bus

5

8

7

6

actor message bus

node 0

CommNet CommNet

node 1

Figure7. An illustration of 3 message routing cases: sending mes-
sage to an actor on the same thread, sending message to an actor
on another thread in the same node, and sending message to an
actor on another node. The CommNet in the ﬁgure indicates the
low-level networking module in OneFlow.

1

11

22

43

63

node

thread

hardware queue

actor

Figure8. Encoding of an actor’s address.

to hold the new output tensor. Without this back pressure
mechanism (as in existing frameworks), a producer may
run out of memory quickly if the consumer blocks.

5 THE IMPLEMENTATION
We implement OneFlow using around 26K LoC in Python,
120K LoC in C++, and 10K LoC in CUDA. The actor run-
time uses 3K LoC of C++, and the compiler module is
implemented in 20K LoC of C++.2 In the following, we
present some implementation details of actor system.

Actor addressing and message routing. Similar to CUDA
stream in Nvidia GPGPUs, we also abstract other hardware
resources (e.g., network and CPUs) as FIFO queues. We
ensure no implicit dependency is brought by sharing re-
sources. For example, two separate CUDA streams are
created for copy engine and compute engine. To mini-
mize device context switch, OneFlow creates a dedicated
OS thread for each hardware queue and the actors using
the same queue (or hardware resource) are bound to the
same OS thread (e.g., actora and actorb in Figure 7). With
static binding among actor, device, OS thread and node,
OneFlow assigns a unique and hierarchically organized 64-
bit address (or equivalently, ID) for each actor as shown in
Figure 8; IDs of the device, OS thread and the node (where
the actor resides) can be parsed from some speciﬁc ﬁelds of
an actor ID. With this ID translation mechanism, attaching
the receiver actor’s ID with the message sufﬁces to route
the message to its destination.

In OneFlow, actors running on the same OS thread share a
FIFO message queue. For an actor to receive a message, the
message is ﬁrst put in the message queue of the correspond-
ing OS thread, which polls the queue repeatedly, fetches the
message and routes it to the intended receiver (e.g., case 3(cid:13)

2The code of OneFlow is available at: https://github.

com/Oneflow-Inc/oneflow.

OneFlow: Redesign the Distributed Deep Learning Framework from Scratch

)
s
/
s
e
l
p
m
a
s
(

t
u
p
h
g
u
o
r
h
T

 11000
 10000
 9000
 8000
 7000
 6000
 5000
 4000
 3000
 2000
 1000
 0

PyTorch dataloader
PyTorch DALI
PyTorch synthetic data
TensorFlow dataloader
TensorFlow DALI
TensorFLow synthetic data
OneFlow dataloader
OneFlow synthetic data

1

4
# of GPUs

8

Figure9. Throughput comparison with various frameworks and
data loaders: training ResNet50-V1.5 with mixed precision.

in Figure 7). There is also a local message queue on each
OS thread. The message sent to a receiver on the same OS
thread as the sender is put into a local message queue and
is directly processed by the receiver without being polled
by the OS thread (case 1(cid:13) in Figure 7).

Unifying the intra- and inter-node actor systems. We in-
troduce an abstraction layer, the actor message bus, that
provides a uniﬁed interface to route a message to its re-
ceiver no matter whether the receiver is on the same or an-
other node. In Figure 7, the message from actora to actord
travels along the logical path { 2(cid:13), 4(cid:13)}, while its actual path
is { 2(cid:13), 5(cid:13), 6(cid:13), 7(cid:13)}. Such abstraction hides low-level com-
munication across networks.

Different from existing frameworks and libraries which in-
sert Send and Recv ops at both sides of inter-node communi-
cation, OneFlow’s compiler only inserts a networking actor
at the consumer’s side for pulling data from the producer’s
node to the consumer’s node, once inter-node communica-
In Figure 7, suppose actore on node 1
tion is detected.
requires the output of actora on node 0; when generating
the physical graph, the compiler creates actord at node 1
whose sole responsibility is to pull the output of actora
from node 0 to node 1, so that actore can consume the data
as if the producer was on the same node.

6 EVALUATION
We demonstrate OneFlow’s generality, ﬂexibility and ef-
ﬁciency by implementing representative parallelisms and
comparing with state-of-the-art libraries in various cases.
Unless stated otherwise, we conduct experiments on a clus-
ter of 4 machines inter-connected by a 100Gbps RoCE net-
work. Each machine is equipped with 8 Nvidia Tesla V100
16G GPUs interconnected with NVLink.

6.1 Data-preprocessing Pipeline
In many scenarios such as training small DL models in
mixed precision mode with high-end GPGPUs, feeding
data to computation renders a bottleneck in DNN train-
ing (Kumar et al., 2020). Figure 9 compares the through-
put achieved by OneFlow and mainstream frameworks with

various data loaders. DALI is a plugin developed by Nvidia
for optimizing data loading for DL frameworks (nvi, 2021).
In “synthetic data” cases, we use fake data generated in
memory without the need for data loading from disks, rep-
resenting the respective ideal cases. Tensorﬂow and Py-
Torch’s data loaders are able to overlap data loading and
computation but perform much worse than using Nvidia
DALI. Unlike using customized plugin such as DALI, One-
Flow supports pipelining by just allocating two out regis-
ters for data loading, pre-processing and copying host to de-
vice ops as described in Section 4.3. Performance of One-
Flow’s data loader is close to that of the synthetic data case,
indicating perfect piplelining between data loading actors
and pre-processing actors. OneFlow achieves this without
additional engineering efforts like DALI.

training.

6.2 Data Parallelism
The existing DL frameworks have carried out the most
In
extensive optimization on data-parallel
the experiments of Figure 10, MXNet
is based on
Horovod (Sergeev & Balso, 2018); Tensorﬂow and Py-
Torch use their native communication strategies, which
lead to better performance than using Horovod. We ob-
serve that in the case of ResNet (He et al., 2016), One-
Flow not only outperforms the ofﬁcial TensorFlow, Py-
Torch and MXNet by 23%–31% with FP32 and 71%–
213% with FP16 (Micikevicius et al., 2018), but also out-
performs the highly optimized versions of these frame-
works (those preﬁxed by NGC, using the same script as
submitted by NVIDIA to MLPerf (Mattson et al., 2020)) by
9%–30% with FP32 and 8%–47% with FP16. In terms of
BERT (Devlin et al., 2019), OneFlow also achieves higher
training throughput than NGC versions by 9%–47% with
FP32 and around 55% with FP16. For each model, we carry
out a lot of performance optimization to ensure the through-
put of OneFlow on a single device comparable to or slightly
better than that of other frameworks. In this way, the scal-
ability of different frameworks can be compared based on
almost the same baseline. Note that the BERT implementa-
tion in MXNet does not perform gradient clipping, which
hence involves fewer computation. To perform a fair com-
parison between MXNet and OneFlow, we implement two
versions of BERT on OneFlow, with and without gradient
clipping, respectively.

6.3 Model Parallelism
We compare OneFlow with two customized DL libraries
supporting model parallelism training, as ofﬁcial versions
of TensorFlow and PyTorch do not support model paral-
lelism.

6.3.1 InsightFace
InsightFace (ins, 2021) is widely used to train huge face
recognition models, where model parallelism is necessary.
It supports model parallelism based on PyTorch with a com-
plicated customization. In contrast, OneFlow only needs

 
OneFlow: Redesign the Distributed Deep Learning Framework from Scratch

MXNet
TensorFlow
PyTorch
NGC TensorFlow
NGC MXNet
NGC PyTorch
oneFlow

 35000

 30000

 25000

 20000

 15000

 10000

)
s
/
s
e
g
a
m

i
(

t
u
p
h
g
u
o
r
h
T

MXNet
TensorFlow

N(cid:0)(cid:1) (cid:2)(cid:3)(cid:4)(cid:5)(cid:6)(cid:7)(cid:8)(cid:9)(cid:10)(cid:11) (cid:12)(cid:13) (cid:14)(cid:15)(cid:16)

(cid:17)(cid:18)(cid:19) (cid:20)(cid:21)(cid:22)(cid:23)(cid:24)(cid:25)(cid:26)(cid:27)(cid:28)(cid:29) (cid:30)(cid:31)  !"#

NGC MXNet
NGC PyTorch

O$%&’()

1

8

16

32

# of GPUs

(a) ResNet, FP32

 5000

 0

1

8

16

32

# of GPUs

(b) ResNet, FP16

 4500

 4000

 3500

 3000

 2500

 2000

 1500

 1000

)
s
/
s
e
c
n
e
t
n
e
s
(

t
u
p
h
g
u
o
r
h
T

M*+,- ./0 1234

NGC TensorFlow
NGC PyTorch

56789:; <= >?@A

BCDEFGH IJK LPQR

 500

 0

1

8

16

32

# of GPUs

(c) BERT-base, FP32

)
s
/
s
e
c
n
e
t
n
e
s
(

t
u
p
h
g
u
o
r
h
T

 5000
 4500
 4000
 3500
 3000
 2500
 2000
 1500
 1000
 500
 0

STUVW XYZ [\]^

NGC TensorFlow
NGC PyTorch

_‘abcde fg hijk

lmnopqr stu vwxy

1

8

16

32

# of GPUs

(d) BERT-base, FP16

)
s
/
s
e
g
a
m

i
(

t
u
p
h
g
u
o
r
h
T

 14000

 12000

 10000

 8000

 6000

 4000

 2000

 0

Figure10. Data parallelism training of ResNet and BERT-base.

PartialSum

PartialSum

Split(0)

PartialSum

PartialSum

Broadcast

SCE

∗SCE: Sparse Cross Entropy

Split(1)

Broadcast

SCE

Split(1)

Broadcast

SCE

Split(1)

Broadcast

SCE

Split(1)

Label

Label

Label

Label

softmax

softmax

softmax

softmax

 80
 70
 60
 50
 40
 30
 20
 10
 0

mnop qrs

OneFlow

OOM OOM OOM OOM OOM

 18000
 16000
 14000
 12000
 10000
 8000
 6000
 4000
 2000
 0

OOM OOM OOM OOM OOM

(cid:190)¿(cid:192)` ´ˆ˜

OneFlow

3(cid:29)(cid:30)

6(cid:31) 

1!"#

2$%&

5’() *+,-. /0478 9:;<= >?@AB

tuv

wxy

z{|} ~(cid:127)(cid:128)(cid:129) (cid:130)(cid:131)(cid:132)(cid:133) (cid:134)(cid:135)(cid:136)(cid:137)(cid:138) (cid:139)(cid:140)(cid:141)(cid:142)(cid:143) (cid:144)(cid:145)(cid:146)(cid:147)(cid:148) (cid:149)(cid:150)(cid:151)(cid:152)(cid:153)

[\]^_ ‘abcdefghijkl

‹›ﬁﬂ(cid:176) –†‡·(cid:181)¶•‚„”»…‰

CML

∗CML: Combined Margin Loss

CML

Matmul

Broadcast

Split(1)

Matmul

Broadcast

Split(1)

CML

Matmul

Broadcast

Split(1)

CML

Matmul

Broadcast

Split(1)

(a) Training time per iteration

(b) Memory footprint

F eatures

F eatures

F eatures

F eatures

W eight

W eight

W eight

W eight

Figure13. Model parallelism training: OneFlow vs. HugeCTR.

(a) MatMul, softmax and sparse cross entropy operators if the SBP
signature of weight matrix is S(1) .

Split(1)

div

Split(1)

div

Broadcast

Split(1)

div

Split(1)

div

Split(1)

RS

∗RM: Reduce Sum

Split(1)

RS

PartialSum

Split(1)

RS

Split(1)

RS

Split(1)

exp

Split(1)

sub

Split(1)

RM

∗RM: Reduce Max

Split(1)

Split(1)

exp

Split(1)

sub

RM

Split(1)

exp

Split(1)

sub

RM

Broadcast

Split(1)

PartialMax

Split(1)

exp

Split(1)

sub

RM

Split(1)

Split(1)

Split(1)

Split(1)

Split(1)

GPU0

GPU1

GPU2

GPU3

(b) The details of softmax op in the physical graph generated by
compiler.

Figure11. Implementing model parallelism in InsightFace on four
GPUs.

z{|}~(cid:127)(cid:128) (cid:129)(cid:130)(cid:131)(cid:132)(cid:133)(cid:134)(cid:135)(cid:136)(cid:137)(cid:138)(cid:139)(cid:140)(cid:141)

(cid:142)(cid:143)(cid:144)(cid:145)(cid:146)(cid:147)(cid:148) (cid:149)(cid:150)(cid:151)(cid:152)(cid:153)(cid:154)(cid:155)(cid:156)(cid:157)(cid:158)(cid:159)(cid:160)¡

¢£⁄¥ƒ§¤ '“«‹›ﬁﬂ(cid:176)–†‡·(cid:181)

¶•‚„”»… ‰(cid:190)¿(cid:192)`´ˆ˜¯˘˙¨(cid:201)

 2000

 1800

 1600

 1400

 1200

 1000

)
s
/
s
e
g
a
m

i
(

t
u
p
h
g
u
o
r
h
T

 800

 600

 400

 200

1

2

# of GPUs

4

8

 14000

 12000

 10000

)
s
/
s
e
g
a
m

i
(

t
u
p
h
g
u
o
r
h
T

˚¸(cid:204)˝˛ˇ— (cid:209)(cid:210)(cid:211)(cid:212)(cid:213)(cid:214)(cid:215)(cid:216)(cid:217)(cid:218)(cid:219)(cid:220)(cid:221)(cid:222)

(cid:223)(cid:224)Æ(cid:226)ª(cid:228)(cid:229) (cid:230)(cid:231)ŁØŒº(cid:236)(cid:237)(cid:238)(cid:239)(cid:240)æ(cid:242)(cid:243)

(cid:244)ı(cid:246)(cid:247)łøœ ß(cid:252)(cid:253)(cid:254)(cid:255)O(cid:0)(cid:1)(cid:2)(cid:3)(cid:4)(cid:5)(cid:6)(cid:7)

(cid:8)(cid:9)(cid:10)(cid:11)(cid:12)(cid:13)(cid:14) (cid:15)(cid:16)(cid:17)(cid:18)(cid:19)(cid:20)(cid:21)(cid:22)(cid:23)(cid:24)(cid:25)(cid:26)(cid:27)(cid:28)

 8000

 6000

 4000

 2000

 0

1

2

# of GPUs

4

8

(a) ResNet

(b) MobileFaceNet

Figure12. Model-parallel training: OneFlow vs. InsightFace.

to conﬁgure appropriate SBP signatures for M atM ul and
softmax ops that require model parallelism. Figure 11a il-
lustrates the transformation of local tensors on four GPUs
after setting SBP signature of the weight matrix as S(1).
Figure 11b demonstrates the details of a softmax op in the
physical graph generated by the compiler. Note that, there
are two reduce calculations within the softmax op. To mini-
mize the communication cost incurred by global reduction,
OneFlow ﬁrst carries out local reduction within a device
while performing the max and sum ops. In Figure 12, we

observe that OneFlow’s throughput slightly outperforms
InsightFace’s when training face recognition models with
ResNet and MobileFaceNet as backbone networks respec-
tively (Chen et al., 2018). The physical execution plans
used by both frameworks are essentially the same. How-
ever, the plan in InsightFace is generated with manual pro-
gramming, while the plan in OneFlow is automatically pro-
duced by the compiler. OneFlow signiﬁcantly eases the
programming burden of model parallelism.

6.3.2 HugeCTR
Wide & Deep Leaning (Cheng et al., 2016) is widely used
in recommender systems, e.g., for click-through rates esti-
mation. In production, to support click-through rates esti-
mation for billions of IDs, the embedding matrices become
too large for a single GPU’s memory to hold. Model par-
allelism on the embedding table is needed. As shown in
Figure 13, Wide & Deep learning built on OneFlow outper-
forms HugeCTR in NVIDIA Merlin (Oldridge et al., 2020),
a dedicated framework for training click-through rates esti-
mation model with model parallelism. OneFlowachieves
lower latency (i.e., per-iteration training time) and less
memory footprint compared to HugeCTR. HugeCTR runs
out of memory when the vocabulary size exceeds 51.2 mil-
lion. Different from HugeCTR that involves substantial
engineering efforts, with OneFlow, users only need to set
appropriate SBP signatures for the embedding table (i.e.,
S(0) for splitting the vocabulary IDs and S(1) for splitting
the hidden dimension), and our compiler will automatically
insert collective communication ops where necessary for
model parallelism.

6.4 Parallelizing the Optimizer
Memory redundancy of model states (such as gra-
dients,
in
Adam (Kingma & Ba, 2015)) in data parallelism can

parameters, momentum and

variances

 
 
 
 
 
 
L
C
D
E
F
G
H
I
J
K
M
N
P
Q
R
S
T
U
V
W
X
Y
Z
(cid:154)
(cid:155)
(cid:156)
(cid:157)
(cid:158)
(cid:159)
(cid:160)
¡
¢
£
⁄
¥
ƒ
§
¤
'
“
«
OneFlow: Redesign the Distributed Deep Learning Framework from Scratch

in

Params(fp32) Broadcast

in

Params(fp32) Split

in0

4Ψ bytes (Ψ = 8)

Device 0 Device 1

Params(fp32)

Params(fp32)

cast to fp16

cast to fp16

chain

forward

forward

broadcast

cast to fp16

cast to fp16

Params(fp16)

Params(fp16)

forward

boxing

logical graph rewrite pass

compile to physical graph

Params(fp16)

Params(fp16)

loss

loss

loss

 18
 16
 14
 12
 10
 8
 6
 4
 2

in1

forward

loss

(cid:240)æ(cid:242)(cid:243)(cid:244)ı(cid:246) (cid:247)łøœ ß(cid:252)(cid:253)(cid:254)

(cid:255)Z(cid:0)(cid:1)(cid:2)(cid:3)(cid:4) (cid:5)(cid:6)(cid:7)(cid:8)(cid:9)(cid:10)(cid:11)(cid:12) (cid:13)(cid:14)(cid:15) (cid:16)(cid:17)(cid:18)

O(cid:19)(cid:20)(cid:21)(cid:22)(cid:23)(cid:24) (cid:25)(cid:26)(cid:27)(cid:28) (cid:29)(cid:30)(cid:31) 

!"#$%&’ ()*+ ,-.

abcdefg hijk lmno

pqrstuv wxyz{|}~ (cid:127)(cid:128)(cid:129) (cid:130)(cid:131)(cid:132)

(cid:133)(cid:134)(cid:135)(cid:136)(cid:137)(cid:138)(cid:139) (cid:140)(cid:141)(cid:142)(cid:143) (cid:144)(cid:145)(cid:146)(cid:147)

(cid:148)(cid:149)(cid:150)(cid:151)(cid:152)(cid:153)(cid:154) (cid:155)(cid:156)(cid:157)(cid:158) (cid:159)(cid:160)¡

 900
 800

 700
 600

 500
 400

 300
 200

(a) Zero optimizer foward pass.

loss

Device 0

gradient(fp16)

boxing

reduce-scatter

Device 1

gradient(fp16)

loss

backward

params(fp16)

cast

cast

gradient(fp32)

gradient(fp32)

adam state V(fp32)

adam state V(fp32)

adam state M(fp32)

adam state M(fp32)

backward

params(fp16)

params(fp32)

Optimizer

Optimizer

params(fp32)

(b) Zero optimizer backward pass.

Figure14. Parallelizing the optimizer in OneFlow.

be signiﬁcantly reduced by sharding them across devices.
ZeRO-DP (Rajbhandari et al., 2020) leverages it to support
distributed training of large models on devices with limited
memory, with each device only holding part of the sharded
model states. When the full model states are required, an
all-gather communication primitive can be used. OneFlow
is able to implement the same idea with less engineering
efforts. Figure 14 illustrates the procedure of generating
the physical graph on two devices by OneFlow, while
implementing the same techniques as in ZeRO-DP with
mixed precision enabled (Micikevicius et al., 2018). First,
a conversion op (such as fp16 cast) is inserted. Second,
our framework conﬁgures SBP signatures of the input of
the cast op as S(0) and the output of the cast op as B.
Our compiler automatically generates the physical graph
for both forward pass (Figure 14a) and backward pass
(Figure 14b). Data routing ops are automatically inserted
where appropriate. ZeRO-DP’s implementation is based on
PyTorch, using about 2K LoC. OneFlow implements the
idea with 300 LoC, which is much simpler.

Figure 15 compares per-device memory footprint and
throughput when training GPT-2, with the activation check-
point (Chen et al., 2016b) on (i.e., opt on) or off (i.e., opt
off). We observe that OneFlow consumes less device mem-
ory but achieves higher throughput than ZeRO-DP, with or
without the activation checkpointing optimization.

6.5 Hybrid Parallelism
Megatron-LM (Shoeybi et al., 2020) is a customized li-
brary for pre-training large models such as GPT-3 based
on PyTorch. It supports data parallelism, model parallelism
and hybrid parallelism which combines data and model par-
allelism (amounting to the two-dimensional SBP described
in Section 3.3). It also implements activation checkpoint-
ing and synchronous pipeline with 1F1B pipeline sched-

¯˘ ˙

¨(cid:201) ˚

¸(cid:204) ˝

1/ 0

23 4

56 7

(cid:222) (cid:223)(cid:224) Æ(cid:226)ª(cid:228)(cid:229)(cid:230)(cid:231)Ł Ø Œº (cid:236)(cid:237)(cid:238)(cid:239)

L MN PQRSUVWX Y [\ ]^_‘

(a) Memory footprint

(b) Throughput

Figure15. Performance of optimizer
vs. ZeRO-DP.

sharding:

OneFlow

 1000

 800

 600

 400

 200

 0

 2500

 2000

 1500

 1000

 500

 0

œß(cid:252)(cid:253)(cid:254)(cid:255)M(cid:0)(cid:1)(cid:2)(cid:3)

]^_‘abcdefg

 20000

OneFlow

 15000

 10000

 5000

 0

OneFlow

¢£⁄¥ƒ

·(cid:181)¶•‚

˙¨(cid:201)˚¸(cid:204)

(cid:219)(cid:220)(cid:221)(cid:222)(cid:223)(cid:224)

1(cid:4)(cid:5)(cid:6)(cid:7)

(cid:20)(cid:21)(cid:22)(cid:23)(cid:24)

()*+,-

=>?@AB

§¤'“«‹›ﬁﬂ

„”»…‰(cid:190)¿(cid:192)`´

˝˛ˇ—(cid:209)(cid:210)(cid:211)(cid:212)(cid:213)(cid:214)

Æ(cid:226)ª(cid:228)(cid:229)(cid:230)(cid:231)ŁØŒ

 (cid:8)(cid:9)(cid:10)(cid:11)(cid:12)(cid:13)(cid:14)(cid:15)

(cid:25)(cid:26)(cid:27)(cid:28)(cid:29)(cid:30)(cid:31)!"#

./02345678

CDEFGHIJKL

(cid:176)–† ‡

ˆ˜¯ ˘

(cid:215)(cid:216)(cid:217) (cid:218)

º(cid:236)(cid:237) (cid:238)

(cid:16)(cid:17)(cid:18) (cid:19)

$%& ’

9:; <

NOP Q

(a) DP

(b) MP

´ˆ˜¯˘˙¨(cid:201)˚¸(cid:204)

OneFlow

 25000

 20000

 15000

 10000

 5000

 0

M*+,-./0134

OneFlow

hijkl

{|}~(cid:127)

(cid:143)(cid:144)(cid:145)(cid:146)(cid:147)

£⁄¥ƒ§

˝˛ˇ—(cid:209)

(cid:226)ª(cid:228)(cid:229)(cid:230)

(cid:247)łøœß

2(cid:11)(cid:12)(cid:13)(cid:14)

mnopqrstuv

(cid:128)(cid:129)(cid:130)(cid:131)(cid:132)(cid:133)(cid:134)(cid:135)(cid:136)(cid:137)

(cid:148)(cid:149)(cid:150)(cid:151)(cid:152)(cid:153)(cid:154)(cid:155)(cid:156)(cid:157)

¤'“«‹›ﬁﬂ(cid:176)–

(cid:210)(cid:211)(cid:212)(cid:213)(cid:214)(cid:215)(cid:216)(cid:217)(cid:218)(cid:219)(cid:220)

(cid:231)ŁØŒº(cid:236)(cid:237)(cid:238)(cid:239)(cid:240)æ

(cid:252)(cid:253)(cid:254)(cid:255) (cid:0)(cid:1)(cid:2)(cid:3)(cid:4)(cid:5)

(cid:15)(cid:16)(cid:17)(cid:18)(cid:19)(cid:20)(cid:21)(cid:22)(cid:23)(cid:24)(cid:25)

wxy z

(cid:138)(cid:139)(cid:140) (cid:141)(cid:142)

(cid:158)(cid:159)(cid:160) ¡¢

†‡· (cid:181)¶

(cid:221)(cid:222)(cid:223) (cid:224)Æ

(cid:242)(cid:243)(cid:244) ı(cid:246)

(cid:6)(cid:7)(cid:8) (cid:9)(cid:10)

(cid:26)(cid:27)(cid:28) (cid:29)(cid:30)

(c) DP combined with MP

(d) DP, MP combined with PP

Figure16. Per-iteration training time for training GPT-2 using
various parallelisms: OneFlow vs. Megatron-LM. The num-
bers listed for each experiment are respectively data-parallel-size,
tensor-model-parallel-size, pipeline-model-parallel-size, global
batch size, hidden-size, number-of-layers deﬁned in Megatron-
LM.

ule. We compare OneFlow and Megatron-LM for train-
ing GPT-2 under representative conﬁgurations in Figure
16. The four sub-ﬁgures demonstrates the experiment re-
sults for pure data parallelism, pure model parallelism,
hybrid of data parallelism and model parallelism, a com-
bination of data, model and pipeline parallelism. As a
generic framework, OneFlow implements all features that
Megatron-LM supports, such as the activation checkpoint-
ing and 1F1B pipeline schedule techniques and align all
the hyper-parameters. The physical execution plans of two
frameworks are essentially the same. However, OneFlow
performs more kernel fusions than Megatron-LM does. In
the result, OneFlow outperforms Megatron-LM even with
a single device. This is the major reason why OneFlow
achieves higher training efﬁciency in distributed cases over
the customized library.

7 CONCLUSION AND DISCUSSIONS
We propose a new distributed deep learning framework
OneFlow based on the concept of SBP and the actor model.
OneFlow overcomes the complexity and efﬁciency issues
of existing frameworks in supporting various parallelisms
for training large DL models. The compiler uses the con-

˛
ˇ
—
(cid:209)
(cid:210)
(cid:211)
(cid:212)
(cid:213)
(cid:214)
(cid:215)
(cid:216)
(cid:217)
(cid:218)
(cid:219)
(cid:220)
(cid:221)
T
8
9
:
;
<
=
>
?
@
A
B
C
D
E
F
G
H
I
J
K
(cid:239)
(cid:240)
æ
(cid:242)
(cid:243)
(cid:244)
ı
(cid:246)
(cid:247)
ł
ø
R
S
T
U
V
W
X
Y
Z
[
\
•
‚
„
”
»
…
‰
(cid:190)
¿
(cid:192)
`
L
(cid:31)
!
"
#
$
%
&
’
(
)
OneFlow: Redesign the Distributed Deep Learning Framework from Scratch

cise abstraction of SBP for automatically generating an ef-
fective execution plan for actors with both spatial and tem-
poral scheduling enabled. The actor model uniﬁes various
dependencies as message passing and naturally supports
pipelining, serving a novel mechanism for runtime of dis-
tributed DL frameworks. Finally, we show experiment re-
sults from a wide range of challenging tasks on real datasets
to demonstrate that the design presented in this paper is
more ﬂexible and efﬁcient than the existing ones.

Even though both OneFlow and Ray (Moritz et al., 2018)
use the concept of the actor, the granularities are different.
In Ray, a single actor is used to manage a complete neural
network while performing deep learning training. So far,
Ray can only act as a plugin to enable data-parallelism to
TensorFlow and PyTorch. It does not support model paral-
lelism and pipeline parallelism.

There are still a number of areas that we are actively
working on to improve OneFlow, including: (1) to enable
OneFlow with elastic scaling (Mai et al., 2020; Or et al.,
2020) and ﬁne-grained fault resilience (Wang et al., 2021;
Zaharia et al., 2013) besides the naive global checkpoint-
ing; (2) to implement auto placement and auto parallelism
by designing a more efﬁcient cost model, thus making it
easier to use.

ACKNOWLEDGEMENTS

We thank the anonymous reviewers of OSDI 2021, SOSP
2021 and MLSys 2022 for their helpful comments on the
paper. Developing a deep learning framework such as One-
Flow involves a large amount of engineering efforts. We
gratefully acknowledge contributions from our colleagues
within OneFlow Inc. and Zhejiang Lab., and from the users
of OneFlow. In particular, Wenxiao Zhang, Xiaoyu Zhang,
Binbin Han, Jianhao Zhang, Houjiang Chen, Luyang Zhao,
Yu Ouyang, Zekang Zheng, Xuan Xie, Yinggang Wang,
Yipeng Li, Fengwei Liu, Shijie Wang, Xiaoyu Xu, De-
peng Liang, Mingyang Liu, Shiyuan Shangguan, Jing Qiao,
Chong Niu, Wei Zhang, Xuefei Jiang contribute a lot of
code to OneFlow.

REFERENCES

Microsoft DeepSpeed.

https://github.com/

microsoft/DeepSpeed, 2021.

InsightFace

Project.

https://github.com/

deepinsight/insightface, 2021.

NVIDIA NCCL.

https://developer.nvidia.

com/nccl, 2021.

NVIDIA Data Loading Library (DALI).

https://

developer.nvidia.com/DALI, 2021.

Abadi, M., Barham, P., Chen, J., Chen, Z., Davis, A., Dean,
J., Devin, M., Ghemawat, S., Irving, G., Isard, M., Kud-
lur, M., Levenberg, J., Monga, R., Moore, S., Murray,
D. G., Steiner, B., Tucker, P., Vasudevan, V., Warden, P.,
Wicke, M., Yu, Y., and Zheng, X. TensorFlow: A Sys-
tem for Large-scale Machine Learning. In Proceedings
of the 12th USENIX Conference on Operating Systems
Design and Implementation, 2016.

Ben-Nun, T. and Hoeﬂer, T. Demystifying Parallel and Dis-
tributed Deep Learning: An In-Depth Concurrency Anal-
ysis. ACM Computing Surveys, 2019.

Brown, T., Mann, B., Ryder, N., Subbiah, M., Kaplan, J. D.,
Dhariwal, P., Neelakantan, A., Shyam, P., Sastry, G.,
Askell, A., Agarwal, S., Herbert-Voss, A., Krueger, G.,
Henighan, T., Child, R., Ramesh, A., Ziegler, D., Wu, J.,
Winter, C., Hesse, C., Chen, M., Sigler, E., Litwin, M.,
Gray, S., Chess, B., Clark, J., Berner, C., McCandlish,
S., Radford, A., Sutskever, I., and Amodei, D. Language
In Proceedings of Ad-
Models are Few-Shot Learners.
vances in Neural Information Processing Systems, 2020.

Chen, J., Monga, R., Bengio, S., and J´ozefowicz, R. Re-
visiting Distributed Synchronous SGD. arXiv preprint
arXiv:1604.00981, 2016a.

Chen, S., Liu, Y., Gao, X., and Han, Z. Mobilefacenets: Ef-
ﬁcient CNNs for Accurate Real-time Face Veriﬁcation
on Mobile Devices. In Proceedings of Chinese Confer-
ence on Biometric Recognition, 2018.

Chen, T., Li, M., Li, Y., Lin, M., Wang, N., Wang, M.,
Xiao, T., Xu, B., Zhang, C., and Zhang, Z. MXNet:
A Flexible and Efﬁcient Machine Learning Library for
In Proceedings of
Heterogeneous Distributed System.
LearningSys, 2015.

Chen, T., Xu, B., Zhang, C., and Guestrin, C. Training
Deep Nets with Sublinear Memory Cost. arXiv preprint
arXiv:1604.06174, 2016b.

Cheng, H.-T., Koc, L., Harmsen, J., Shaked, T., Chandra,
T., Aradhye, H., Anderson, G., Corrado, G., Chai, W.,
Ispir, M., Anil, R., Haque, Z., Hong, L., Jain, V., Liu, X.,
and Shah, H. Wide & Deep Learning for Recommender
Systems. In Proceedings of the 1st Workshop on Deep
Learning for Recommender Systems, 2016.

Devlin, J., Chang, M.-W., Lee, K., and Toutanova, K.
BERT: Pre-training of Deep Bidirectional Transformers
In Proceedings of the
for Language Understanding.
North American Chapter of the Association for Com-
putational Linguistics: Human Language Technologies,
2019.

fairscale.

https://
github.com/facebookresearch/fairscale.

Facebook Fairscale project.

OneFlow: Redesign the Distributed Deep Learning Framework from Scratch

Fedus, W., Zoph, B., and Shazeer, N. Switch Transform-
ers: Scaling to Trillion Parameter Models with Simple
and Efﬁcient Sparsity. arXiv preprint arXiv:2101.03961,
2021.

Goyal, P., Doll¨¢r, P., Girshick, R., Noordhuis, P.,
Wesolowski, L., Kyrola, A., Tulloch, A., Jia, Y., and He,
K. Accurate, Large Minibatch SGD: Training ImageNet
in 1 Hour. arXiv preprint arXiv:1706.02677, 2017.

Hashemi, S. H., Jyothi, S. A., and Campbell, R. H. TicTac:
Accelerating Distributed Deep Learning with Communi-
cation Scheduling. In Proceedings of Machine Learning
and Systems, 2019.

He, K., Zhang, X., Ren, S., and Sun, J. Deep Resid-
ual Learning for Image Recognition. In Proceedings of
the IEEE Conference on Computer Vision and Pattern
Recognition, 2016.

Hewitt, C., Bishop, P., and Steiger, R. A Universal Mod-
ular ACTOR Formalism for Artiﬁcial Intelligence.
In
Proceedings of the 3rd International Joint Conference
on Artiﬁcial Intelligence, 1973.

Huang, Y., Cheng, Y., Bapna, A., Firat, O., Chen, D., Chen,
M., Lee, H., Ngiam, J., Le, Q. V., Wu, Y., and Chen,
z. GPipe: Efﬁcient Training of Giant Neural Networks
using Pipeline Parallelism. In Proceedings of Advances
in Neural Information Processing Systems, 2019.

Jia, Z., Lin, S., Qi, C. R., and Aiken, A. Exploring Hidden
Dimensions in Parallelizing Convolutional Neural Net-
works. In Proceedings of the International Conference
on Machine Learning, 2018.

Jia, Z., Zaharia, M., and Aiken, A. Beyond Data and Model
Parallelism for Deep Neural Networks. In Proceedings
of Machine Learning and Systems, 2019.

Jiang, Y., Zhu, Y., Lan, C., Yi, B., Cui, Y., and Guo, C. A
Uniﬁed Architecture for Accelerating Distributed DNN
Training in Heterogeneous GPU/CPU Clusters. In Pro-
ceedings of the 14th USENIX Symposium on Operating
Systems Design and Implementation, 2020.

Kaplan, J., McCandlish, S., Henighan, T., Brown, T. B.,
Chess, B., Child, R., Gray, S., Radford, A., Wu, J., and
Amodei, D. Scaling Laws for Neural Language Models.
arXiv preprint arXiv:2001.08361, 2020.

Kingma, D. P. and Ba, J. Adam: A Method for Stochastic
In Proceedings of International Confer-

Optimization.
ence on Learning Representations, 2015.

Kumar, S., Bradbury, J., Young, C., Wang, Y. E., Lev-
skaya, A., Hechtman, B., Chen, D., Lee, H., Deveci,

M., Kumar, N., et al. Exploring the Limits of Concur-
rency in ML Training on Google TPUs. arXiv preprint
arXiv:2011.03641, 2020.

Kung, H. T., Blackwell, T., and Chapman, A. Credit-
based ﬂow control for atm networks: Credit update pro-
tocol, adaptive credit allocation and statistical multiplex-
ing. SIGCOMM Comput. Commun. Rev., 24(4):101–114,
October 1994.

Lepikhin, D., Lee, H., Xu, Y., Chen, D., Firat, O., Huang,
Y., Krikun, M., Shazeer, N., and Chen, Z. GShard: Scal-
ing Giant Models with Conditional Computation and Au-
tomatic Sharding. In Proceedings of International Con-
ference on Learning Representations, 2020.

Li, M., Andersen, D. G., Park, J. W., Smola, A. J., Ahmed,
A., Josifovski, V., Long, J., Shekita, E. J., and Su, B.-Y.
Scaling Distributed Machine Learning with the Parame-
ter Server. In Proceedings of the 11th USENIX Sympo-
sium on Operating Systems Design and Implementation,
2014.

Mai, L., Li, G., Wagenl¨ander, M., Fertakis, K., Brabete,
A.-O., and Pietzuch, P. KungFu: Making Training in
Distributed Machine Learning Adaptive. In Proceedings
of the 14th USENIX Symposium on Operating Systems
Design and Implementation, 2020.

Mattson, P., Cheng, C., Diamos, G., Coleman, C., Micike-
vicius, P., Patterson, D., Tang, H., Wei, G.-Y., Bailis, P.,
Bittorf, V., Brooks, D., Chen, D., Dutta, D., Gupta, U.,
Hazelwood, K., Hock, A., Huang, X., Kang, D., Kan-
ter, D., Kumar, N., Liao, J., Narayanan, D., Oguntebi, T.,
Pekhimenko, G., Pentecost, L., Janapa Reddi, V., Robie,
T., St John, T., Wu, C.-J., Xu, L., Young, C., and Za-
haria, M. MLPerf Training Benchmark. In Proceedings
of Machine Learning and Systems, 2020.

Micikevicius, P., Narang, S., Alben, J., Diamos, G., Elsen,
E., Garcia, D., Ginsburg, B., Houston, M., Kuchaiev, O.,
Venkatesh, G., and Wu, H. Mixed Precision Training.
In Proceedings of International Conference on Learning
Representations, 2018.

Moritz, P., Nishihara, R., Wang, S., Tumanov, A., Liaw, R.,
Liang, E., Paul, W., Jordan, M. I., and Stoica, I. Ray:
A Distributed Framework for Emerging AI Applications.
In Proceedings of the 13th USENIX Symposium on Op-
erating Systems Design and Implementation, 2018.

Narayanan, D., Harlap, A., Phanishayee, A., Seshadri, V.,
Devanur, N. R., Ganger, G. R., Gibbons, P. B., and Za-
haria, M. PipeDream: Generalized Pipeline Parallelism
for DNN Training. In Proceedings of the 27th ACM Sym-
posium on Operating Systems Principles, 2019.

OneFlow: Redesign the Distributed Deep Learning Framework from Scratch

Shoeybi, M., Patwary, M., Puri, R., LeGresley, P., Casper,
J., and Catanzaro, B. Megatron-LM: Training Multi-
Billion Parameter Language Models Using Model Par-
allelism. arXiv preprint arXiv:1909.08053, 2020.

Wang, M., Huang, C.-c., and Li, J. Supporting Very Large
Models Using Automatic Dataﬂow Graph Partitioning.
In Proceedings of the Fourteenth EuroSys Conference,
2019.

Wang, S., Liang, E., Oakes, E., Hindman, B., Luan, F. S.,
Cheng, A., and Stoica, I. Ownership: A Distributed Fu-
tures System for Fine-Grained Tasks. In Proceedings of
the 18th USENIX Symposium on Networked Systems De-
sign and Implementation, 2021.

Wu, X., Xu, H., Li, B., and Xiong, Y. Stanza: Distributed
Deep Learning with Small Communication Footprint.
arXiv preprint arXiv:1812.10624, 2018.

Xu, Q., Li, S., Gong, C., and You, Y. An efﬁcient 2d
method for training super-large deep learning models,
2021.

Zaharia, M., Das, T., Li, H., Hunter, T., Shenker, S., and
Stoica, I. Discretized streams: Fault-tolerant stream-
ing computation at scale. In Proceedings of the twenty-
fourth ACM Symposium on Operating Systems Princi-
ples, 2013.

Narayanan, D., Shoeybi, M., Casper, J., LeGresley, P., Pat-
wary, M., Korthikanti, V., Vainbrand, D., Kashinkunti,
P., Bernauer, J., Catanzaro, B., Phanishayee, A., and Za-
haria, M. Efﬁcient Large-Scale Language Model Train-
ing on GPU Clusters. arXiv preprint arXiv:2104.04473,
2021.

Oldridge, E., Perez, J., Frederickson, B., Koumchatzky,
N., Lee, M., Wang, Z.-H., Wu, L., Yu, F., Zamora, R.,
Yılmaz, O., Gunny, A. M., Nguyen, V. P., and Lee, S.
Merlin: A GPU Accelerated Recommendation Frame-
work. 2020.

Or, A., Zhang, H., and Freedman, M. Resource Elasticity

in Distributed Deep Learning. 2020.

Pal, S., Ebrahimi, E., Zulﬁqar, A., Fu, Y., Zhang, V., Mi-
gacz, S., Nellans, D., and Gupta, P. Optimizing Multi-
GPU Parallelization Strategies for Deep Learning Train-
ing. IEEE Micro, 2019.

Paszke, A., Gross, S., Massa, F., Lerer, A., Bradbury, J.,
Chanan, G., Killeen, T., Lin, Z., Gimelshein, N., Antiga,
L., Desmaison, A., Kopf, A., Yang, E., DeVito, Z., Rai-
son, M., Tejani, A., Chilamkurthy, S., Steiner, B., Fang,
L., Bai, J., and Chintala, S. PyTorch: An Imperative
Style, High-Performance Deep Learning Library. In Pro-
ceedings of Advances in Neural Information Processing
Systems, 2019.

Peng, Y., Zhu, Y., Chen, Y., Bao, Y., Yi, B., Lan, C., Wu,
C., and Guo, C. A Generic Communication Scheduler
for Distributed DNN Training Acceleration. In Proceed-
ings of the 27th ACM Symposium on Operating Systems
Principles, 2019.

Rajbhandari, S., Rasley, J., Ruwase, O., and He, Y. ZeRO:
Memory Optimizations Toward Training Trillion Param-
eter Models. In Proceedings of International Conference
for High Performance Computing, Networking, Storage
and Analysis, 2020.

Rajbhandari, S., Ruwase, O., Rasley, J., Smith, S., and
He, Y. ZeRO-Inﬁnity: Breaking the GPU Memory
Wall for Extreme Scale Deep Learning. arXiv preprint
arXiv:2104.07857, 2021.

Sergeev, A. and Balso, M. D. Horovod: Fast and Easy Dis-
tributed Deep Learning in TensorFlow. arXiv preprint
arXiv:1802.05799, 2018.

Shazeer, N., Cheng, Y., Parmar, N., Tran, D., Vaswani,
A., Koanantakool, P., Hawkins, P., Lee, H., Hong, M.,
Young, C., et al. Mesh-tensorﬂow: Deep learning for
supercomputers. In Proceedings of Advances in Neural
Information Processing Systems, 2018.

