Learning to Optimize Power Distribution Grids
using Sensitivity-Informed Deep Neural Networks

Manish K. Singh, Sarthak Gupta, Vassilis Kekatos
Bradley Dept. of ECE, Virginia Tech
Blacksburg, VA, 24061
Emails: {manishks,gsarthak,kekatos}@vt.edu

Guido Cavraro and Andrey Bernstein
National Renewanable Energy Laboratories
Golden, CO, 80401
Emails: {guido.cavraro,Andrey.Bernstein}@nrel.gov

0
2
0
2

l
u
J

7

]

C
O
.
h
t
a
m

[

1
v
8
2
7
3
0
.
7
0
0
2
:
v
i
X
r
a

Abstract—Deep learning for distribution grid optimization can
be advocated as a promising solution for near-optimal yet timely
inverter dispatch. The principle is to train a deep neural network
(DNN) to predict the solutions of an optimal power ﬂow (OPF),
thus shifting the computational effort from real-time to ofﬂine.
Nonetheless, before training this DNN, one has to solve a large
number of OPFs to create a labeled dataset. Granted the latter
step can still be prohibitive in time-critical applications, this work
puts forth an original technique for improving the prediction
accuracy of DNNs by taking into account the sensitivities of
the OPF minimizers with respect to the OPF parameters. By
expanding on multiparametric programming, it is shown that
although inverter control problems may exhibit dual degeneracy,
the required sensitivities do exist in general and can be computed
readily using the output of any standard quadratic program
(QP) solver. Numerical tests showcase that sensitivity-informed
deep learning can enhance prediction accuracy in terms of mean
square error (MSE) by 2-3 orders of magnitude at minimal
computational overhead. Improvements are more signiﬁcant in
the small-data regime, where a DNN has to learn to optimize
using a few examples. Beyond multiparametric QPs, the approach
is currently being generalized to parametric (non)-convex opti-
mization problems.

Index Terms—Multiparametric programming; reactive power
compensation; optimal power ﬂow; linearized distribution ﬂow.

I. INTRODUCTION

To cope with resource allocation problems with stringent
latency requirements, recent research advocates using learn-
ing models that are trained to solve optimization tasks; see
e.g., [1], [2]. The general workﬂow involves solving a large
number of such problems ofﬂine to create a dataset of problem
instances paired with their minimizers; train a learning model
to predict those minimizers; and use the model for promptly
ﬁnding near-optimal solutions in real time. References [1] and
[3] impose feasibility using Lagrangian decomposition, while
[4] and [5] build DNNs with structures mimicking algorithmic
iterates solving the original optimization.

In the context of power systems, there has recently been
an interesting interplay between deep learning and the OPF.
DNNs have been employed to predict OPF solutions under a
linearized DC [6], [7]; or the exact AC model [3], [8]. The
standard regression loss function is sometimes augmented by a
penalty on OPF constraint violations to promote feasibility [6],
[8], [3]. Alternatively, predicted solutions may be converted

Work partially supported by the US NSF grant 1751085.

to a physically implementable schedule upon projection using
a power ﬂow (PF) solver [6], [7]. A graph neural network
leveraging the connectivity of the power system is trained to
infer AC-OPF solutions in [9]. Focusing on OPF for inverter
control, reference [10] constructs a DNN to be used as a
digital twin of the electric feeder. To learn optimal inverter
control policies, DNN-based reinforcement learning schemes
have been suggested in [11], [12].

Instead of learning minimizers, DNNs have also been used
to predict the active constraints of an OPF [8], [13], [2].
References [8] and [13] train DNN-based classiﬁers to predict
individual or sets of active constraints. In [2],
the active
constraints of a linear programming (LP) DC-OPF are unveiled
in three steps: A DNN is ﬁrst trained to predict the optimal
locational prices are then
generation cost given demands;
computed as the DNN sensitivities with respect to demands;
and congested lines are identiﬁed via sparse-aware regression.
When the OPF is posed using a linearized grid model,
it gives rise to parametric LPs or QPs, and hence, power-
ful tools from multiparametric programming (MPP) become
relevant [14]. Such tools have been utilized before to study
congestion patterns in electricity markets [15]; for the proba-
bilistic characterization of primal/dual OPF solutions [16]; to
accelerate hosting capacity analyses [17]; or strategic invest-
ment [18]. MPP-aided methods however are meaningful only
when the related OPF enjoys few congestion patterns and/or
the application of interest is of a batch nature.

This work contributes to the literature of training DNNs to
solve the OPF in the following creative way. A DNN is trained
to ﬁt not only the OPF minimizer, but also its sensitivities
with respect to the problem parameters. Inspired by [19], we
cross-pollinate the idea of training sensitivity-aware DNNs
from solving differential equations to learning for optimization
problems in Section II. Advances in automatic differentiation
facilitate the efﬁcient computation of DNN sensitivities as
well as their integration into training. In Sections III and IV,
we particularize properties from MPP theory to the inverter
dispatch task at hand, so that the sensitivities of the OPF
solution can be computed readily. The numerical
tests of
Section V showcase that the proposed sensitivity-informed
DNN outperforms a plain DNN in terms of prediction accuracy
by 2-3 orders of magnitude. The paper is concluded with
generalizations of the method to more challenging OPF tasks.

 
 
 
 
 
 
Regarding notation, lower- (upper-) case boldface letters
denote column vectors (matrices). Sets are represented by
calligraphic symbols. Symbol (cid:62) stands for transposition, and
inequalities are understood element-wise. All-zero and all-one
vectors and matrices are represented by 0 and 1; the respective
dimensions are deducible from context. A symmetric positive
deﬁnite matrix is denoted as X (cid:31) 0.

II. SENSITIVITY-INFORMED DEEP LEARNING

Suppose a system operator has to routinely solve a convex

quadratic program (QP) over the resource vector x

xθ := arg min

1

2 x(cid:62)Ax − x(cid:62)Bθ
x
s.to Cx ≤ Dθ + e : λθ.

(1a)

(1b)

While (A (cid:31) 0, B, C, D, e) are kept ﬁxed for some time, the
parameter vector θ may be varying frequently. Let (xθ, λθ)
denote a pair of primal/dual solutions for a particular θ. This
multiparametric QP (MPQP) may be encountered in several
resource allocation tasks in cyber-physical systems in general,
and power systems in particular. Renditions of (1) may ap-
pear in real-time electricity markets and contingency analysis
in transmission systems; or in load disaggregation, demand
response, and DER management in distribution systems.

The application of interest here is dispatching inverters
through an approximate OPF that minimizes power losses
subject to voltage constraints. Vector x comprises the inverter
reactive power setpoints, while θ carries the loading conditions
(demand and solar generation) across feeder buses. Since θ
may change rapidly (rooftop solar generation can ﬂuctuate by
up to 15% of its rating within one minute [20]), an operator
may not be able to solve (1) exactly for thousands of feeders
hosting thousands of inverters each.

To expedite the process of computing inverter setpoints, the
operator may want to train a DNN that learns to solve (1). This
DNN would be fed with θ to compute a predictor ˆx(θ; w) of
xθ. The straightforward approach for training this DNN is
ﬁnding its weights w as the minimizers of

min
w

S
(cid:88)

s=1

(cid:107)ˆx(θs; w) − xθs(cid:107)2
2.

(2)

Problem (2) is solved ofﬂine and aims at ﬁtting the DNN out-
put to the OPF minimizer over S loading scenarios {θs}S
s=1,
which are deemed representative of the upcoming control
period. Before solving (2) however, the operator has to solve
S instances of (1) to build the labeled dataset {(θs, xθs )}S
s=1.
Accommodating a large S could be impractical
in time-
critical setups, where the DNN has to be re-trained every
30–60 min or so because parameters (A, B, C, D, e) change
too. This is relevant for inverter control due to grid topology
reconﬁgurations, changes in regulator settings and capacitor
statuses, and varying loading conditions that may alter the
linearization point of the nonlinear power ﬂow equations.

We improve on this process in two aspects: i) we reduce
S so the operator has to solve fewer OPFs; and ii) for each
OPF instance solved during training, we further exploit the

sensitivity of xs with respect to θs. Both aspects can expedite
training or increase the training dataset over the same training
time, while ii) is expected to yield better generalization to the
sought DNN and stabilize its solutions against variations in θ.
Inspired by [19] where a DNN was trained to solve differential
equations by incorporating information on its derivatives, here
we integrate sensitivity information of the optimizer xθ with
respect to the DNN input θ. To accomplish that, we propose
augmenting the training process of (2) as

min
w

S
(cid:88)

s=1

(cid:107)ˆx(θs; w) − xθs (cid:107)2

2 + (cid:107)ˆJ(θs; w) − Jθs(cid:107)2

F

(3)

where Jθ := ∇θxθ and ˆJ(θ; w) := ∇θ ˆx(θ; w) are the
Jacobian matrices of the minimizer and the DNN output
accordingly with respect to the parameter vector θ. Symbol
(cid:107) · (cid:107)F denotes the Frobenius norm. The optimization of (3)
aims at ﬁtting the values of the minimizers as well as their
sensitivities (partial derivatives) in terms of the DNN input θ.
By incorporating such physics-aware information, the DNN
is expected to predict well not only on the training scenarios
θs’s, but in a neighborhood around them too. We term the
DNN trained by (3) as sensitivity-informed DNN (SI-DNN)
versus the plain DNN (P-DNN) obtained from (2).

Having posed the sensitivity-informed deep learning task of
(3), the pertinent questions are if the OPF operator OPF : θ →
xθ is differentiable; and if so, whether its Jacobian matrix Jθ
can be found in a computationally efﬁcient way. The goal is
to be able to build the augmented training set {(θs, xθs, Jθs )}
at a minimal computational effort
in excess to the effort
for building the training set {(θs, xθs} for (2). To this end,
Section III ﬁrst poses optimal inverter dispatching in the form
of the MPQP in (1), and then Section IV adapts results from
multiparametric programming to compute Jθ efﬁciently.

III. OPTIMAL INVERTER DISPATCH

Consider the task of optimal reactive power compensation
by smart
inverters. Given (re)active loads along with the
active solar generation on every bus, the goal is to ﬁnd the
inverter setpoints for reactive power injections. Such setpoints
can be found through an OPF that minimizes ohmic power
losses on distribution lines, while satisfying voltage regulation
limitations across buses. Before formally posing this OPF, we
brieﬂy review the postulated feeder model.

In a single-phase radial feeder with N + 1 buses, the sub-
station is indexed by n = 0. Let vn be the voltage magnitude,
and pn + jqn the complex power injection at bus n. Collect
all but the substation injections and voltages in the N -length
vectors (p, q, v). Vectors (p, q) can be decomposed into non-
negative injections (pi, qi) from inverters, and withdrawals
(p(cid:96), q(cid:96)) from loads as

p = Fipi − F(cid:96)p(cid:96)
q = Fiqi − F(cid:96)q(cid:96).

(4a)

(4b)

Matrices Fi and F(cid:96) map buses with solar and loads to the N
feeder buses, respectively. If there are I buses hosting solar

and L buses hosting loads, then Fi ∈ {0, 1}N ×I and F(cid:96) ∈
{0, 1}N ×L. We assume that each bus may host at most one
(aggregate) load and at most one solar generator.

To simplify the task of dispatching inverters, we resort to an
approximate feeder model obtained upon linearizing the power
ﬂow equations at the ﬂat voltage proﬁle of unit magnitudes
and zero angles at all buses; see e.g., [17]. According to this
model, voltages depend linearly on power injections as

v(p, q) (cid:39) Rp + Xq + v01N

(5)

where the N ×N matrices (R, X) are symmetric positive def-
inite, and depend on the feeder topology and line impedances.
We are also interested in expressing ohmic losses with respect
to nodal injections. Adopting a second-order Taylor’s series
expansion, active losses on lines can be approximated as a
convex quadratic function of power injections as

L(p, q) (cid:39) 2p(cid:62)Rp + 2q(cid:62)Rq.

(6)

Based on the previous modeling, the reactive setpoints for

inverters can be found as the minimizers of

q(cid:62)Rq

min
qi
s.to − 0.03 · 1N ≤ Rp + Xq ≤ 0.03 · 1N

− ¯qi ≤ qi ≤ ¯qi.

(7a)

(7b)

(7c)

The approximate OPF of (7) minimizes ohmic losses over the
controllable inverter setpoints qi; note the ﬁrst summand of
(6) and the scaling by 2 have been ignored. Constraint (7b)
maintains voltages within ±3% per unit (pu). Although the
substation voltage v0 has been set at 1 pu, it can be appended
as an optimization variable of (7) without loss of generality.
Our methodology can incorporate voltage regulators as in [17];
they are ignored here to keep the exposition succinct. Inverter
setpoints are constrained in (7c) by the vector ¯qi of reactive
ratings, which is assumed known and ﬁxed.

Under varying grid conditions, an operator may want to
solve (7) routinely for different OPF instances. If the topology
remains ﬁxed, OPF instances differ in the values of loads
and solar (pi, p(cid:96), q(cid:96)). Deﬁne the vector of OPF inputs θ :=
[p(cid:62) q(cid:62)
(cid:96) ](cid:62). Due to the problem structure, active loads and solar
can appear collectively as p per (4a). To better distinguish the
optimization variable x = qi from the OPF parameters θ, let
us substitute (4b) in (7), and express the inverter dispatch of
(7) as the MPQP of (1) with the substitutions

A := 2F(cid:62)







C :=



i RFi (cid:31) 0,
−XFi
XFi
−II
II





,

B := 2F(cid:62)


D :=







i RF(cid:96) [0L×N IL]
[R − XF(cid:96)]
[−R + XF(cid:96)]
0I
0I
¯q(cid:62)
i





(cid:3) .

(8a)

(8b)

(8c)

e(cid:62) := (cid:2) 0.03 · 1(cid:62)

N 0.03 · 1(cid:62)

N ¯q(cid:62)
i

IV. COMPUTING THE JACOBIAN MATRIX VIA MPP
Consider solving (1) for a particular θ. The corresponding
optimal primal-dual pair (xθ, λθ) should satisfy the Karush-
Kuhn-Tucker (KKT) optimality conditions [21]. Among the

inequality constraints of (1), those satisﬁed with equality at
the optimum are termed active or binding constraints. Let
( ˜C, ˜D, ˜e, ˜λθ) be the row partitions of (C, D, e, λθ) associated
with active constraints. Likewise, let ( ¯C, ¯D, ¯e, ¯λθ) denote the
row partition related to inactive or non-binding constraints.
Leveraging complementarity slackness, the KKT conditions
can be expressed as [21]

Axθ + ˜C(cid:62) ˜λθ = Bθ

˜Cxθ = ˜Dθ + ˜e
¯Cxθ < ¯Dθ + ¯e

˜λθ > 0
¯λθ = 0.

(9a)

(9b)

(9c)

(9d)

(9e)

For (9d) in particular, it is assumed that binding constraints
relate to strictly positive dual variables ˜λθ > 0. The reverse
is unlikely to occur when θs’s are drawn at random. In the
unlikely case some of the entries of ˜λθ do equal zero, the OPF
sensitivity for this speciﬁc θ can be ignored and the DNN is
trained only to match the OPF minimizer xθ.

If there are A active constraints, the equalities of (9a)–(9b)
constitute a system of (I +A) linear equations over the (I +A)
unknowns (xθ, ˜λθ). Since A (cid:31) 0, the Lagrangian optimality
condition of (9a) yields

xθ = A−1(Bθ − ˜C(cid:62) ˜λθ).

(10)

Substituting (10) into (9b) provides

G˜λθ = ( ˜CA−1B − ˜D)θ − ˜e
(11)
where G := ˜CA−1 ˜C(cid:62). If matrix ˜C has linearly independent
rows, then G (cid:31) 0, and (11) has a unique solution. Otherwise,
there are inﬁnitely many ˜λθ’s satisfying (11) within a shift
invariance on the nullspace null( ˜C(cid:62)) = null(G).

For a general QP, the rows of ˜C are typically linearly
independent, thus satisfying the so termed linear indepen-
dence constraint qualiﬁcation (LICQ) [21]. Nonetheless, that
is not
the case for the inverter dispatch problem of (7)
where instances of linearly dependent active constraints occur
frequently. To see this, consider the feeder of Fig. 1, where
buses {1, 2} host inverters, bus 3 is a load bus, and 4 is a
zero-injection bus. Let xn denote the reactance for the line
feeding bus n. Then, matrix X can be found as [17]




X =

x1

x1
x1 x1 + x2
x1
x1

x1
x1





x1
x1
x1 + x3
x1 + x3 x1 + x3 + x4

x1
x1
x1 + x3

.





the product XFi selects the ﬁrst

Moreover,
two columns
of X corresponding to inverter locations. Now consider a
loading scenario that causes the voltage at bus 3 to be at
the lower limit. Being a zero-injection bus, the downstream
bus 4 shares the same voltage. Thus, the minimum voltage
constraints become active for buses 3 and 4. Assuming no
other constraints being active, matrix ˜C of (8b) reads as

˜C = −

(cid:21)

(cid:20)x1 x1
x1 x1

.

Fig. 1. A 5-bus feeder demonstrating the violation of LICQ for (7).

which is clearly row rank-deﬁcient. This counterexample em-
phasizes the need to explicitly address the case where the
solution of (1) or (7) fails to meet the LICQ.

Regardless of whether ˜C is full row-rank or not, the solution

to (11) lies in the polyhedron
˜Λθ := {˜λθ = G+( ˜CA−1B − ˜D)θ − G+˜e + u, ˜C(cid:62)u = 0}
where G+ is the pseudo-inverse of G = ˜CA−1 ˜C(cid:62). Substi-
tuting ˜λθ back in (10) and exploiting ˜C(cid:62)u = 0, we get

xθ = Jθθ + A−1 ˜C(cid:62)G+˜e.

(12)

where the sensitivity matrix is deﬁned as

Jθ := A−1B − A−1 ˜C(cid:62)G+( ˜CA−1B − ˜D).
(13)
Although there may be inﬁnitely many ˜λθ’s satisfying the
KKT conditions, the optimal primal solution of (1) is unique
if it exists. This is not surprising since (1) has a strictly convex
objective. Moreover, the solution can be expressed as an afﬁne
function of θ. Note that the parameters of this afﬁne function
depend on the set of active constraints, and this is indicated
by the subscript of θ on Jθ.

Because the triplet (θ, xθ, ˜λθ) should also satisfy (9d) and

(9c), there exists a u ∈ null( ˜C(cid:62)) so that θ satisﬁes

Fig. 2. Minute-based data for the total (feeder-wise) solar power generation
and active power demand (top panel); and the optimal reactive power injection
setpoints for ﬁve inverters (bottom panel).

G+( ˜CA−1B − D)θ > G+˜e − u
( ¯CJθ − ¯D)θ < ¯e − ¯CA−1 ˜C(cid:62)G+˜e

(14a)

(14b)

So far, we have characterized the solution to (1) for a
particular θ. Since we are interested in the sensitivity of the
OPF minimizer with respect to θ, we ask the natural question
whether (12) holds for all θ(cid:48)’s within a vicinity of this speciﬁc
θ. The answer to this question is on the afﬁrmative. To see
this, ﬁx u and perturb θ to get θ(cid:48) so it still satisﬁes (14).
Using θ(cid:48) in lieu of θ, construct xθ(cid:48) from (12), and ˜λθ(cid:48) from
(11). In doing so, we row-partition matrices assuming the
same constraints are active as for θ. This means we still use
matrix Jθ. We also set ¯λθ(cid:48) = 0. The constructed primal-
dual pair (xθ(cid:48), λθ(cid:48)) satisﬁes the KKT conditions of (1) for
θ(cid:48), and hence constitutes an optimal solution for this θ(cid:48). The
aforesaid process can be repeated for any θ(cid:48) in the vicinity
of the original θ because (14) are strict inequalities. In other
words, formula (12) is valid for all θ(cid:48)’s around θ and with
matrix Jθ remaining unaltered.

The latter reveals that the OPF operator from θ to xθ is
differentiable around θ. In addition, its Jacobian matrix is
provided in closed form using (13). Calculating Jθ entails:
i) Knowing the set of active constraints; ii) inverting matrix
A once; and iii) inverting matrix G. Although iii) is executed

Fig. 3. A modiﬁed IEEE 37-bus feeder showing additional solar generators.

once per θ, the computation is lightweight since the number of
active constraints A should be smaller than N . In a nutshell,
computing Jθ once (1) has been solved for a particular θ,
is much simpler than solving (1) per se. Hence, computing
sensitivities adds insigniﬁcant complexity in the process of
constructing the labeled dataset (θ, xθ, Jθ).

V. NUMERICAL TESTS

The performance of the developed approach was numer-
ically tested by solving the OPF in (7) for the IEEE 37-
bus feeder upon removing regulators, incorporating 5 solar
generators, and converting it to its single-phase equivalent; see
Fig. 3. We extracted minute-based load and solar generation

23401024048072096012001440Time [min]00.511.522.53Total solar and demands [p.u.]Solar generationPower demand024048072096012001440Time [min]-0.45-0.3-0.1500.150.30.45Inverter reactive power [p.u.]AVERAGE TEST MSE [IN 10−6 PU] AFTER 1,000 EPOCHS

TABLE I

FOR DIFFERENT HOURS OF THE DAY

Hour

5
12
16
20

12 OPF training scenarios
SI-DNN
P-DNN
0.04
27.9
50.4
1184.4
49.6
145.1
45.0
359.3

4 OPF training scenarios
SI-DNN
P-DNN
0.33
75.3
4755.3
7229.8
342.5
540.3
145.2
1500.6

data for June 1, 2018, from the Pecan Street dataset [22].
The feeder has 25 buses with non-zero load. The ﬁrst 75
non-zero load buses from the dataset were aggregated every
3 and normalized to obtain 25 load proﬁles. Similarly, 5
solar generation proﬁles were obtained. The normalized load
proﬁles for the 24-hr period were scaled so the 97-th percentile
of the total
load duration curve coincided with the total
nominal load. This scaling results in a peak aggregate load
being 1.1 times the total nominal load. We synthesized reactive
loads by scaling active demand to match the power factors of
the IEEE 37-bus feeder. The 5 solar generators were of equal
ratings and scaled to meet 75% of the total energy requirement
over the entire day. Figure 2 shows the total demand and
solar generation across the feeder (top), along with the optimal
reactive power injection for each of the ﬁve inverters (bottom).
The optimal setpoints were obtained by solving (7) using
YALMIP and Sedumi. In solving the 1,440 OPF instances, no
constraints were active for 914 instances. Out of the remaining
526 instances, the LICQ was not satisﬁed for 175 instances;
thus necessitating the approach pursued here.

Our tests compared the P-DNN and SI-DNN, both trained to
predict the minimizer of (7). For the ﬁrst set of tests, the DNNs
were assumed to be trained on an hourly basis. To evaluate
the potential beneﬁt of integrating sensitivity information, the
architecture, optimizer, and training epochs were kept identical
for the two DNNs. In fact, the architecture was chosen in favor
of the P-DNN. It was determined so that for a sufﬁciently
large training set, the P-DNN would be able to predict a near-
optimal inverter dispatch. This ensured a sufﬁciently rich, yet
not too complex architecture: Three hidden layers with 210,
210, and 350 neurons respectively, all using the rectiﬁed linear
unit (ReLU) activation. All results were generated using the
Adam optimizer with a learning rate of 0.01. The DNNs were
implemented using the TensorFlow library on Google colab.
For a given hour, the two DNNs were trained using 4 OPF
instances, obtained by sampling the upcoming hourly period
every 15 min. The remaining 56 grid instances were used to
test the two DNNs based on the mean square error (MSE)
(cid:107)xs − ˆxs(cid:107)2
2 on the predicted setpoints. To compare P-DNN and
SI-DNN under varying conditions, their training and testing
errors were evaluated over hours 5, 12, and 20, and shown in
Fig. 4. The tests show that even when P-DNN yields smaller
training errors, SI-DNN offers an improvement in testing error
by one or two orders of magnitude.

Fig. 4. Training and testing errors across epochs in terms of MSE for hours
5 (top), 12 (middle), and 20 (bottom). Training was performed using 15-min
smart meter data (4 OPF instances) to operate during the hour of interest.

AVERAGE TEST MSE [IN 10−6 PU] AND TRAINING TIME [IN SEC]
AFTER 1,000 EPOCHS FOR 10–12 AM (840 MIN-BASED SCENARIOS)

TABLE II

Training
Scenarios
5%
10%
20%

P-DNN

SI-DNN

MSE
1407.8
520.2
219.2

Time MSE
119.9
108.9
72.9
77.1
55.1
100.8

Time
118.7
79.7
113.4

random. To evaluate the effect of the dataset size, we trained
both DNNs using 4 and 12 scenarios for each of the hours 5,
12, 16, and 20. Table I reports the test MSEs attained after
1,000 epochs and averaged over the 10 Monte Carlo runs.
The results corroborates that the SI-DNN features a gain in
prediction accuracy over P-DNN by 1-3 orders of magnitude.
Its advantage is generally more pronounced when dealing with
smaller datasets, as expected.

We next conducted the previous test but now over 10 Monte
Carlo runs, each time selecting the training OPF scenarios at

For the last set of tests, the two DNNs were trained to
learn the OPF for an entire day. The training dataset was

02004006008001000Epochs10-610-410-2100102104MSE [× 10-6 p.u.]P-DNN testingP-DNN trainingSI-DNN testingSI-DNN training02004006008001000Epochs10-610-410-2100102104MSE [× 10-6 p.u.]02004006008001000Epochs10-2100102104106MSE [× 10-6 p.u.]constructed by sampling 5%, 10%, and 20% of the one-minute
data. To explicitly focus on periods of high variability, we
excluded the hours from midnight to 10 am from sampling.
Table II summarizes the MSEs obtained after 1,000 epochs
and averaged over 100 Monte Carlo draws of the training
dataset. The table also reports the average training time for
1,000 epochs for the two methodologies. The runtimes on
Google colab often varies with session. Thus, these times
can only be compared separately for each training scenario;
and not across scenarios. The results establish that the SI-
DNN consistently outperforms the P-DNN without incurring
any signiﬁcant increase in training time. For example, the SI-
DNN achieves an MSE of 119.9 · 10−6 pu using 5% of the
data, whereas the P-DNN achieves an MSE of 219.2 · 10−6 pu
although it has been trained by using 4 times more data (20%).

VI. CONCLUSIONS AND ONGOING WORK

A novel procedure for training DNNs that learn to optimize
has been put forth. Leaping beyond the general practice of
this
training DNNs via labeled parameter-minimizer pairs,
work ensures that the sensitivities of DNN predictions to inputs
are close to the respective sensitivities of the original opti-
mization task. Addressing QPs in particular, the sensitivities
required for training the DNN can be computed readily in
virtue of results from MPP theory. The application pursued has
been inverter reactive power control in distribution systems for
minimizing losses subject to voltage constraints. It has been
shown here that although for general QPs dual degeneracies
are rare, for the inverter dispatch task such degeneracies can be
encountered frequently. Fortunately, even for such instances,
the required Jacobian matrices do exist in general, and the
proposed approach can successfully compute them in closed
form. The efﬁcacy of the novel training method is demon-
strated via numerical tests that corroborate an improvement in
prediction accuracy by 2-3 orders of magnitude, as compared
to the traditional regression approach. The improvements are
more pronounced in the small-data regime, where a DNN has
to learn to optimize using few examples.

Prompted by these promising results, we are currently
generalizing this creative idea of including gradient informa-
tion into DNNs towards several exciting directions. Beyond
MPQPs that feature rich structure in their solutions, we are
interested in improving learning for resource allocation tasks
of the generic parametric form

xθ := arg min

f (x; θ)

x
s.to g(x; θ) ≤ 0 : λθ.

(Pθ)

Thanks to results from optimization, given (f, g, xθ, λθ) one
can compute the gradient ∇θf (xθ), and the Jacobian matrices
∇θxθ and ∇θλθ, regardless if (Pθ) is convex. Leveraging
these results, we are currently working towards: d1) incor-
porating sensitivities for learning to optimize different power
system optimization and monitoring tasks, including the AC
OPF and its various convex relaxations; d2) integrating dual
sensitivities and weight their deviations; and d3) predicting

binding constraints so that exact minimizers can be found by
solving an OPF over a condensed feasible set.

REFERENCES

[1] M. Eisen, C. Zhang, L. F. O. Chamon, D. D. Lee, and A. Ribeiro,
“Learning optimal resource allocations in wireless systems,” IEEE Trans.
Signal Process., vol. 67, no. 10, pp. 2775–2790, May 2019.

[2] Y. Chen and B. Zhang, “Learning to solve network ﬂow problems
[Online]. Available: https:

via neural decoding,” 2020, preprint.
//arxiv.org/abs/2002.04091

[3] F. Fioretto, T. W. Mak, and P. V. Hentenryck, “Predicting AC optimal
power ﬂows: Combining deep learning and lagrangian dual methods,”
in AAAI Conf. on Artiﬁcial Intelligence, New York, NY, Feb. 2020.
[4] H. Sun, X. Chen, Q. Shi, M. Hong, X. Fu, and N. D. Sidiropoulos,
“Learning to optimize: Training deep neural networks for interference
management,” IEEE Trans. Signal Process., vol. 66, no. 20, pp. 5438–
5453, Oct. 2018.

[5] L. Zhang, G. Wang, and G. B. Giannakis, “Real-time power system state
estimation and forecasting via deep unrolled neural networks,” IEEE
Trans. Signal Process., vol. 67, no. 15, pp. 4069–4077, Aug. 2019.
[6] X. Pan, T. Zhao, and M. Chen, “DeepOPF: Deep neural network for DC
optimal power ﬂow,” in Proc. IEEE Intl. Conf. on Smart Grid Commun.,
Beijing, China, Oct. 2019, pp. 1–6.

[7] A. Zamzam and K. Baker, “Learning optimal solutions for extremely
fast AC optimal power ﬂow,” 2019, preprint. [Online]. Available:
https://arxiv.org/abs/1910.01213

[8] N. Guha, Z. Wang, M. Wytock, and A. Majumdar, “Machine learning
for AC optimal power ﬂow,” 2019, climate Change Workshop at ICML
2019. [Online]. Available: https://arxiv.org/abs/1910.08842

[9] D. Owerko, F. Gama, and A. Ribeiro, “Optimal power ﬂow using graph
neural networks,” in Proc. IEEE Intl. Conf. on Acoustics, Speech, and
Signal Process., Barcelona, Spain, May 2020, pp. 5930–5934.

[10] Y. Chen, Y. Shi, and B. Zhang, “Input convex neural networks for
optimal voltage regulation,” 2020, (submitted). [Online]. Available:
https://arxiv.org/abs/2002.08684

[11] Q. Zhang, K. Dehghanpour, Z. Wang, F. Qiu, and D. Zhao,
“Multi-agent safe policy learning for power management of networked
microgrids,” 2020, (submitted). [Online]. Available: https://arxiv.org/
abs/1907.02091v3

[12] W. Wang, N. Yu, Y. Gao, and J. Shi, “Safe off-policy deep reinforcement
learning algorithm for Volt-VAR control in power distribution systems,”
IEEE Trans. Smart Grid, pp. 1–1, 2019.

[13] D. Deka and S. Misra, “Learning for DC-OPF: Classifying active sets
using neural nets,” in IEEE PowerTech, Milan, Italy, Jun. 2019, pp. 1–6.
[14] F. Borrelli, A. Bemporad, and M. Morari, “Geometric algorithm for
multiparametric linear programming,” Journal of Optimization Theory
and Applications, vol. 118, no. 3, pp. 515–540, Sep. 2003.

[15] Q. Zhou, L. Tesfatsion, and C.-C. Liu, “Short-term congestion forecast-
ing in wholesale power markets,” IEEE Trans. Power Syst., vol. 26,
no. 4, pp. 2185–2196, Nov. 2011.

[16] Y. Ji, R. J. Thomas, and L. Tong, “Probabilistic forecasting of real-time
LMP and network congestion,” IEEE Trans. Power Syst., vol. 32, no. 2,
pp. 831–841, Mar. 2017.

[17] S. Taheri, M. Jalali, V. Kekatos, and L. Tong, “Fast probabilistic
for active distribution systems,” IEEE
[Online]. Available:

hosting capacity analysis
Trans. Smart Grid, Feb. 2020,
https://arxiv.org/abs/2002.01980

(submitted).

[18] S. Taheri, V. Kekatos, and H. Veeramachaneni, “Strategic investment
in energy markets: A multiparametric programming approach,” IEEE
Trans. Smart Grid, Apr. 2020,
[Online]. Available:
https://arxiv.org/abs/2004.06483

(submitted).

[19] M. Raissi, P. Perdikaris, and G. E. Karniadakis, “Physics-informed
neural networks: A deep learning framework for solving forward and
inverse problems involving nonlinear partial differential equations,”
Journal of Computational Physics, vol. 378, pp. 686–707, 2019.
[20] S. Barker, A. Mishra, D. Irwin, E. Cecchet, P. Shenoy, and J. Albrecht,
“Smart*: An open data set and tools for enabling research in sustainable
homes,” in Proc. Workshop on Data Mining Applications in Sustainabil-
ity, Beijing, China, Aug. 2012.

[21] D. P. Bertsekas, Nonlinear Programming, 2nd ed.

Belmont, MA:

Athena Scientiﬁc, 1999.

[22] (2018) Dataport. Pecan Street

Inc.

[Online]. Available: https:

//dataport.cloud/

