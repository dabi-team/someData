Empirical Evaluation of Deep Learning
Models for Knowledge Tracing:
Of Hyperparameters and Metrics on
Performance and Replicability

Sami Sarsa
Aalto University
sami.sarsa@aalto.ﬁ

Juho Leinonen
Aalto University
juho.2.leinonen@aalto.ﬁ

Arto Hellas
Aalto University
arto.hellas@aalto.ﬁ

We review and evaluate a body of deep learning knowledge tracing (DLKT) models with openly
available and widely-used data sets, and with a novel data set of students learning to program. The
evaluated knowledge tracing models include Vanilla-DKT, two Long Short-Term Memory Deep Knowl-
edge Tracing (LSTM-DKT) variants, two Dynamic Key-Value Memory Network (DKVMN) variants,
and Self-Attentive Knowledge Tracing (SAKT). As baselines, we evaluate simple non-learning models,
logistic regression and Bayesian Knowledge Tracing (BKT). To evaluate how different aspects of DLKT
models inﬂuence model performance, we test input and output layer variations found in the compared
models that are independent of the main architectures. We study maximum attempt count options, in-
cluding ﬁltering out long attempt sequences, that have been implicitly and explicitly used in prior studies.
We contrast the observed performance variations against variations from non-model properties such as
randomness and hardware. Performance of models is assessed using multiple metrics, whereby we also
contrast the impact of the choice of metric on model performance. The key contributions of this work are
the following: Evidence that DLKT models generally outperform more traditional models, but not nec-
essarily by much and not always; Evidence that even simple baselines with little to no predictive value
may outperform DLKT models, especially in terms of accuracy – highlighting importance of selecting
proper baselines for comparison; Disambiguation of properties that lead to better performance in DLKT
models including metric choice, input and output layer variations, common hyperparameters, random
seeding and hardware; Discussion of issues in replicability when evaluating DLKT models, including
discrepancies in prior reported results and methodology. Model implementations, evaluation code, and
data are published as a part of this work.
Keywords: Knowledge Tracing, Deep Learning, Memory Networks, Attention-Based Models, Hyperpa-
rameter Optimization, Evaluation Metrics, Replicability

2
2
0
2

r
p
A
5

]

G
L
.
s
c
[

4
v
2
7
0
5
1
.
2
1
1
2
:
v
i
X
r
a

1

 
 
 
 
 
 
1.

INTRODUCTION

Knowledge tracing (KT) is a student modeling task where knowledge or skill is estimated based
on a trace of interactions with learning activities, such as course exercises. While new knowl-
edge tracing models and modiﬁcations of existing ones are presented regularly (Yudelson et al.,
2013; Piech et al., 2015; Zhang et al., 2017; Yeung and Yeung, 2018; Abdelrahman and Wang,
2019; Nakagawa et al., 2019; Pu et al., 2020), it is not always clear what the actual factors
that contribute to the performance of the proposed models are (Wilson et al., 2016; Lipton and
Steinhardt, 2018). To what extent do the results depend on the proposed model algorithms and
the used data, or are optimizations the actual key to the results?

When working with deep learning models – machine learning models that use multiple pro-
cessing layers for transforming an input to an output – the resulting models are often not trans-
parent and hence difﬁcult to interpret. Substantial performance differences can be observed
already due to the choice of hyperparameters that are used to train the models (Bouthillier et al.,
2021). Even the used hardware and software can inﬂuence the outcomes; for example, the doc-
umentation of PyTorch – an open source machine learning library – warns that “Completely
reproducible results are not guaranteed across PyTorch releases, individual commits, or differ-
ent platforms. Furthermore, results may not be reproducible between CPU and GPU executions,
even when using identical seeds”1. Deeper insight into the factors that contribute to differences
in performance between models can be gained through reproduction, replication and ablation.
Since the terms reproduction and replication are often used interchangeably, in this work by
reproducibility we denote using the same data and methodology – including code – by different
researchers, and by replicability we denote using the same methodology but re-implementing
the work and with different data and researchers, as is done in previous work (Patil et al., 2016;
Stevens, 2017). Although the need for replication and reproduction has been highlighted within
the educational data mining domain (Ihantola et al., 2015; Gardner et al., 2019), such studies are
still scarce.

In our work, we replicate earlier studies, reimplementing models ourselves following de-
scriptions available in the original articles. We review and evaluate a body of KT model algo-
rithms with a focus on KT algorithms that utilize Deep Learning Knowledge Tracing (hereafter
DLKT). As baselines, we use both non-learning models and Bayesian Knowledge Tracing and
a recent Logistic Regression -based model. We triangulate factors that contribute to the perfor-
mance of the models, including metric choice, input and output variations in model structure,
non-model properties (e.g. maximum input sequence lengths, random seeding and hardware)
and commonly used hyperparameters. The evaluations are conducted with seven datasets – six
open ones and a novel dataset made openly available as a part of this study – and seven metrics,
with the purpose of identifying and discussing performance differences between the models, the
datasets, and the metrics. Our research questions for this study are as follows.
RQ1 How do DLKT models compare to naive baselines and non deep-learning KT models?
RQ2 How do DLKT models perform on the same and different datasets as originally evaluated

with?

RQ3 What is the impact of variations in architecture and hyperparameters on DLKT models’

performance?

1https://pytorch.org/docs/stable/notes/randomness.html, accessed 2020-12-

01

2

2. BACKGROUND

2.1.

INTELLIGENT TUTORING SYSTEMS AND STUDENT MODELING

Intelligent tutoring systems are software systems designed to provide personalized tutoring to
students, supporting learning also in contexts where students have little to no access to human
tutors (Anderson et al., 1985; Nwana, 1990; VanLehn, 2011; Ma et al., 2014). Classic intelligent
tutoring systems have four main components; (1) a task environment, (2) a domain knowledge
module, (3) a student model, and (4) a pedagogical module (Corbett et al., 1997).

Students work within the task environment, where their actions are evaluated through the
domain knowledge module, which contains a structured representation of concepts, rules, and
strategies that the student is expected to learn. The student model contains details of the students’
actions within the task environment, an estimate of students’ current knowledge, and possibly
also other information about the students such as background and demographic information.
Finally, based on the information from the domain knowledge module and the student model,
the pedagogical module is used to decide on appropriate instructional interventions that will be
fed to the task environment. These interventions may include, among other things, providing
hints and suggestions for the current task, as well as guiding the students to appropriate tasks.

Reviews that compare intelligent tutoring systems and other environments have suggested
that intelligent tutoring systems outperform teacher-led large-group instruction, computer-based
instruction, and the use of textbooks or workbooks (Ma et al., 2014), and that intelligent tutoring
systems can be nearly as effective as human tutors (VanLehn, 2011; Ma et al., 2014).

Students who use intelligent tutoring systems learn mainly through solving problems within
the task environment. The problems are given based on the student model, which estimates what
the student knows based on students’ actions and other collected information (Chrysaﬁadi and
Virvou, 2013). There are multiple ways for estimating students’ knowledge, including model
tracing and knowledge tracing. In model tracing, the domain model is used to interpret students’
actions and follow the students step-by-step through the problem space, making adjustments if
needed. In knowledge tracing, the system attempts to monitor students’ changing knowledge as
the student works on problems (Corbett and Anderson, 1994), and, e.g., provides content related
to a new topic when the student has mastered the current topic. In this article, our focus is on
knowledge tracing.

2.2. KNOWLEDGE TRACING

Knowledge tracing (Corbett and Anderson, 1994) is an approach for student modeling in which
students’ actions within the task environment are used for estimating their current knowledge
with regard to each individual knowledge component2 that the intelligent tutoring system is
teaching. The estimates of current knowledge are typically probabilistic, where mastery of the
current topic is assumed when the models posit at least a 95% probability that the student has
mastered the topic.

In practice, most if not all of the knowledge tracing algorithms are used to create regression
models, which are used for estimating relationships between a dependent variable (in our con-
text, often knowledge) and one or more independent variables (in our context, often variables

2While the original article (Corbett and Anderson, 1994) uses the term rule for describing the concepts and
objectives that the student is expected to learn, we refer here to the term as knowledge component as it is more
commonly used in contemporary research on the topic.

3

describing students’ behavior or outcomes from the task environment). One particular family of
regression models that has been used in educational data mining is logistic regression models,
which are often used for estimating the probability of a given class (e.g. has mastered the topic
/ has not mastered the topic) based on a set of independent variables.

Here, we revisit models for knowledge tracing. The topics include probabilistic graphical

models, factor analysis models, and deep learning models.

2.2.1. Probabilistic Graphical Models

Probabilistic graphical models are probabilistic models that use graphs for presenting the condi-
tional dependencies between the studied variables. The most prominent probabilistic graphical
model for knowledge tracing is Bayesian Knowledge Tracing (Corbett and Anderson, 1994)
(BKT3). BKT assumes that knowledge is binary, i.e. that a student has either mastered or not
mastered the topic, and furthermore, that once a topic has been mastered, it cannot be unlearned
or forgotten. Interactions with the task environment are related to individual knowledge compo-
nents, and the interactions may produce an output, which is observable evidence that indicates
whether the student’s action was correct or incorrect.

Whenever the system produces an output based on a student’s actions, which represents
either correct or incorrect knowledge, the student’s knowledge of the knowledge component
is updated. In BKT, the update is based on four parameters (described more formally in 3.3),
which deﬁne the probability that the student had already learned the knowledge component
(prior learning), the probability that the student’s action led to the student learning the knowledge
component (transition), the probability that the student’s interaction with the task environment
produced a correct output by accident (guess), and the probability that the student’s interaction
with the task environment produced an incorrect output by accident (slip). In the original article,
data from all students were used to estimate optimal values for the four parameters.

A range of BKT variations has been published since the original article. For example, re-
searchers have added information on whether and how the student was helped during the pro-
cess (Chang et al., 2006; Lin and Chi, 2016), adjusted the probability of guessing or slipping
using additional factors (Baker et al., 2008), adjusted the prior learning and transition parame-
ters based on data from each individual student instead of all students (Yudelson et al., 2013),
and included information on the task difﬁculty to the process (Pardos and Heffernan, 2011).

In addition to the BKT variants, there exists other probabilistic graphical model -based
approaches for KT. For example, Partially Observable Markov Decision Processes (Rafferty
et al., 2011) have been used to model student behavior. Similarly, methods such as (Dynamic)
Bayesian Networks (Pearl, 1985; Pearl, 1988; Ghahramani, 1997) have been applied, e.g., for
estimating student’s knowledge (Rowe and Lester, 2010), for extending BKT to allow multiple
knowledge components (Pardos et al., 2008), for modeling dependencies between knowledge
components (K¨aser et al., 2014), and for adjusting the BKT prior learning parameter based on
additional factors (Pardos and Heffernan, 2010).

2.2.2. Factor Analysis Based Models

Factor analysis is a method for reducing the amount of observed variables into a potentially
lower number of unobserved variables called factors. The methods brieﬂy described next, Item

3The original article uses the term Knowledge Tracing for the approach. Over time, the approach has been

relabeled as Bayesian Knowledge Tracing by the scientiﬁc community.

4

Response Theory, Learning Factors analysis, and Performance Factors Analysis all use existing
data to estimate one or more latent (unobserved) variables.

Item Response Theory (IRT) (Hambleton and Swaminathan, 1985), from the ﬁeld of psycho-
metrics, is used for assessing student’s ability and item difﬁculty based on students’ responses
to, e.g., questionnaire items. As IRT models can be used to produce estimates of item difﬁ-
culty and student ability, they have been used for estimating the probability with which students
will answer questions correctly (Johns et al., 2006). However, the basic IRT model does not
provide means for dynamically updating students’ knowledge, and thus, the model has been
incorporated with knowledge tracing, resulting in models that outperform BKT (Khajah et al.,
2014; Khajah et al., 2014; Gonz´alez-Brenes et al., 2014). In addition, recent work on logistic
regression models incorporating more aspects from students’ behavior has been shown to also
outperform DLKT models (Gervet et al., 2020).

Other variants purposed for the knowledge tracing task are Learning Factors Analysis (Cen
et al., 2006) (LFA) and Performance Factors Analysis (Pavlik Jr et al., 2009) (PFA). LFA is com-
prised of three parts, which are (1) a parameter that quantiﬁes student’s ability, (2) difﬁculty of
the knowledge components deﬁned through a single parameter for each knowledge component,
and (3) learning rate or beneﬁt of frequency of prior practice for each knowledge component.
Similar to the basic IRT model, LFA has downsides in how it can be used for adaptive environ-
ments as it does not account for the correct or incorrect outputs from the task environment, i.e.
correct and incorrect responses by the student. PFA accounts for this by reconﬁguring the model
by making it more sensitive to student’s performance, and providing better means to adapt to
the student’s performance. Literature has provided evidence of PFA outperforming both LFA
and BKT (Pavlik Jr et al., 2009), although there also exists evidence on BKT performing on par
with PFA (Gong et al., 2010). In addition, it has been observed that, in particular tasks, PFA
performs almost on par with a DLKT model (Xiong et al., 2016), which we discuss next.

2.2.3. Deep Learning Models for Knowledge Tracing

Deep learning models are a set of machine learning models that are based on (artiﬁcial) neural
networks. Neural networks are networks of “neurons” organized in layers that are used as in-
formation processing systems for e.g. classiﬁcation tasks. Neurons in the neural networks are
mathematical functions that take a ﬁxed sized set, i.e. a vector, of inputs, have a weight (learn-
able parameter) for each input, and produce an output as a weighted sum of the inputs. The
output is then processed using an activation function (e.g. a sigmoid function).

The ﬁrst layer of a neural network is its input, i.e. the data that is fed to the network. A
single layer may contain an arbitrary amount of neurons and a neural network can consist of
one or more layers of neurons without an upper bound. Complex neural networks, i.e., ones that
are deeply layered, are often called deep learning models or deep networks. In contrast, neural
networks with a relatively simple layer structure with few layers are sometimes referred to as
shallow networks (Bianchini and Scarselli, 2014). The most common type of layer in a neural
network, one that is used commonly in the models explored in this paper, is the fully connected
layer (FCL), wherein each neuron (and weight) of the layer is connected to each of its inputs.

Training a neural network consists of providing training data to the network and adjusting
the weights of the neurons so that the overall network learns to produce a desired output. During
learning, the performance of the model is estimated using an error function that compares the
output of the model with the expected output, i.e. true outcome. Neural networks often require

5

extensive amounts of training data in order to produce generalizable results. This is especially
true when the training data is complex, such as when it contains sequential relations, e.g. tempo-
rality or word order. Thus, adjustments to neural networks have been made to better conform to
certain kinds of data relations. The Recurrent Neural Network (RNN) (Hochreiter and Schmid-
huber, 1997a) is a result of one such adjustment that has been widely adopted. In an RNN,
connections between neurons are constructed as a directed graph that can represent a temporal
sequence, and where the output of each neuron can be fed back to the network (similar to re-
cursion). RNNs have been shown to work well in various domains, including natural language
processing (Mikolov et al., 2011), stock market prediction (Kamijo and Tanigawa, 1990; Saad
et al., 1998), and protein sequence prediction (Saha and Raghava, 2006)

RNNs were ﬁrst used for the Knowledge Tracing task in a study by Piech et al. (Piech et al.,
2015). They showed that “Deep Knowledge Tracing” (DKT) outperformed BKT, despite model-
ing all the skills jointly instead of building a separate model for each skill. Since then, DKT has
been shown to perform well in a range of studies (Wilson et al., 2016; Lin and Chi, 2017; Mao
et al., 2018; Montero et al., 2018; Ding and Larson, 2019). This has also led to a range of DLKT-
based approaches that have been proposed and evaluated for the Knowledge Tracing task. Their
contributions include both proposing new model algorithms by introducing techniques from the
broader machine learning domain and also how to leverage different sorts of input into the mod-
els. For example, Zhang et al. (Zhang et al., 2017) introduce DKVMN, a memory network that
utilizes next skill input when predicting next attempt correctness, that is not present in the orig-
inal DKT model, and Abderahman et al. (Abdelrahman and Wang, 2019) further propose use
of memory network architecture in tandem with LSTMs in their SKVMN model. As a more
input oriented example, The models by Su et al. (Su et al., 2018) (EERNN) and Liu et al. (Liu
et al., 2019) (EKT) utilize textual content of exercises. Pandey et al. (Pandey and Karypis, 2019)
started a line of research on self-attentive models known as transformers for knowledge tracing
with their SAKT model. Further attention-based models include RKT (Pandey and Srivastava,
2020), context-aware AKT (Ghosh et al., 2020) that fuses DLKT with IRT, SAINT (Choi et al.,
2020) with an encoder-decoder architecture, time-aware encoder-decoder by Pu et al. (Pu et al.,
2020), and a time-aware version of SAINT called SAINT+ (Shin et al., 2021). Another line of
development has concentrated on graph neural networks with models such as GKT (Nakagawa
et al., 2019) that models learning of skills as a network and JKT (Song et al., 2021) which takes
a step further by model by modeling both skill and exercise relations. Many of these models
have been introduced since this study is started, which highlights the rapid development and
popularity of the ﬁeld, but also the need for high quality methodology and replication work for
fair comparisons and disambiguation of causes for performance differences.

2.3. REPLICATING WORK FROM OTHERS

Discussions on the importance of replicating work from others have stemmed from observations
that published results do not always hold up under scrutiny (Ioannidis, 2005b; Moonesinghe
et al., 2007). Researchers from various disciplines have voiced out their concerns about the state
of replicability in their ﬁelds, which include, for example, biology (Begley and Ellis, 2012),
computing education (Ahadi et al., 2016), educational data mining (Gardner et al., 2019), health
care (Ioannidis, 2005a), political science (Golden, 1995) and computational science (Peng,
2011). This concern was highlighted also in a multi-disciplinary survey of over 1500 researchers,
where about three quarters of the respondents considered that they could only trust at least half

6

of the papers in their ﬁeld (Baker, 2016). Whether published results hold may depend also on
the ﬁeld of science; for example, one study from psychology replicated 100 experimental and
correlational studies from three journals and found that only about one-third to one-half of the
original ﬁndings could be replicated (Collaboration et al., 2015), raising discussion about how
to conduct replication studies (Gilbert et al., 2016; Anderson et al., 2016).

Despite the need for replication studies, studies that attempt to replicate earlier ﬁndings
remain relatively rare (Muma, 1993; Makel et al., 2012). This rarity has been linked with valuing
innovation and original research (Mackey, 2012; Asendorpf et al., 2013; Ahadi et al., 2016) and
the concern that replications may not be publishable (Fanelli, 2011; Spellman, 2012; Ahadi
et al., 2016). Both of these issues may steer researchers away from conducting and publishing
replication studies. In the survey of over 1500 researchers, where relative lack of trust towards
earlier results was highlighted as one of the issues, over half of the surveyed researchers had
been unable to replicate an experiment from others (Baker, 2016), suggesting that there is a
dire need of sufﬁcient details for replication as well as a need to maintain quality. Indeed, calls
for more detailed reporting of experimental designs, requiring publication of data and research
materials, and rewarding replication attempts have been made (Asendorpf et al., 2013; Ioannidis
et al., 2014).

With replication studies, one could – for example – identify issues with how methodolog-
ical details are presented and consequently also highlight the importance of replication to the
community (Ihantola et al., 2015; Gardner et al., 2019). By reimplementing algorithms as a part
of the replication studies, one may also reveal bugs in existing research code, that could lead
researchers into faulty conclusions (Bhandari Neupane et al., 2019). Although DLKT models
often outperform older models, the relative performance of evaluated models may be inﬂuenced,
among other factors, by the context, the task, the data, the hyperparameter tuning and the chosen
metrics (Xiong et al., 2016; Lalwani and Agrawal, 2017; Mao et al., 2018; Ding and Larson,
2019). The relative impact of these factors could be revealed through replication studies.

2.4. EFFECTS OF METRICS ON REPORTED PERFORMANCE

Metrics for evaluating model performance have been extensively studied and developed, and
different metrics are used to evaluate different aspects of model performance. Relying on any
single metric alone is prone to provide misleading information on model performance (Council
et al., 2005). Plenty of discussion has concentrated on the goodness of accuracy and (ROC-)AUC
(Receiving Operator Characteristic Area Under Curve) as metrics, where some scrutinize the
former (Ling et al., 2003; Halimu et al., 2019) as misleading and others scrutinize the latter (Jeni
et al., 2013; Muschelli, 2020) as potentially masking bad performance. Both of these are popular
metrics in the Knowledge Tracing ﬁeld as well as other domains (Pahikkala et al., 2009; Huang
et al., 2019).

Within the ﬁeld of educational data mining, some studies have been conducted on the effect
of metric choice on model performance, although on different metrics and not so much on ac-
curacy and AUC. As an example, Pelanek at al. (Pel´anek, 2015) compare MAE (Mean Average
Error, RMSE (Root Mean Squared Error, LL (Log-likelihood aka binary cross-entropy and AUC
on BKT, PFA and Elo rating system adaptation (Pel´anek, 2014) models. A rather recent study
by Effenberger et al. (Effenberger and Pel´anek, 2020) compare the highly similar metrics MAE
and RMSE for multiple models for student modeling, including the mentioned Elo adaption, an
IRT model and Random Forest, and show that metric choice affects model performance and can

7

even affect model ranking. They also highlight other issues in methodological choices such as
ﬁltering choices of data in preprocessing.

2.5. RECENT COMPARISONS OF DEEP LEARNING MODELS FOR KNOWLEDGE TRAC-

ING

There are signs that the popularity of replication studies in educational data mining is on the rise.
This has been especially visible within the Educational Data Mining community, where recent
calls for papers at e.g. the Educational Data Mining conference have included reproducibility of
research as a topic of interest4.

A recent article by Gervet et al. (Gervet et al., 2020) compared different models that have
been used for KT. Their evaluated models included (1) different DLKT models, i.e. DKT (Piech
et al., 2015), SAKT (Pandey and Karypis, 2019) and a feedforward network; (2) regression
models, i.e. IRT, PFA (Pavlik Jr et al., 2009), DAS3H (Chofﬁn et al., 2019) and a logistic re-
gression model (LR); and (3) a variation of BKT called BKT+ (Khajah et al., 2016) that adds
individualization, forgetting and discovery of knowledge components. Additionally, they con-
ducted ablation studies to examine which features of different models could explain differences
in their performance. The metrics that were used to compare the models were AUC score and
RMSE, and the study used nine different datasets. The results showed that DKT and logistic
regression performed the best, with DKT being the best model in ﬁve datasets and LR being the
best in four. SAKT underperformed DKT in all of the datasets, which is contrary to prior results
by Pandey and Karypis (Pandey and Karypis, 2019), where SAKT performed signiﬁcantly bet-
ter compared to DKT. The results by Gervet et al. also suggest that dataset size affects model
performance: LR worked better for smaller datasets and DKT performed better for larger ones.
Additionally, DKT seemed to reach peak performance faster when compared to LR, for which
the performance continues to improve over a longer time.

In a similar study, Pandey et al. (Pandey et al., 2021) compared DLKT models. They in-
cluded DKT (Piech et al., 2015), DKVMN (Zhang et al., 2017), SAKT (Pandey and Karypis,
2019) and RKT (Pandey and Srivastava, 2020) in their comparison. The models were compared
using the EdNet dataset (Choi et al., 2020) by comparing accuracies and AUCs. The results
of Pandey et al. suggest that the self-attention based models (RKT and SAKT) perform better
compared to DKT and DKVMN.

Similar to Pandey et al. (Pandey et al., 2021), Mandalapu et al. (Mandalapu et al., 2021) used
the EdNet dataset (Choi et al., 2020) to compare different knowledge tracing models. They used
two versions of the EdNet dataset: the full data with 600,000 students and a pruned dataset with
50,000 students. They evaluated a baseline model (prediction based on probability of correct-
ness in training set), a logistic regression model, DKT (Piech et al., 2015), and SAKT (Pandey
and Karypis, 2019). The models were compared with the AUC metric. Based on the results of
Mandalapu et al. (Mandalapu et al., 2021), the logistic regression model very slightly outper-
formed DKT and SAKT (0.77 AUC for LR vs 0.76 AUC for DKT and SAKT) in the smaller
50,000 student dataset. In the full dataset, SAKT outperformed DKT (0.76 vs 0.72 AUC), while
the logistic regression model could not be trained with the full dataset. Contrary to Gervet et
al. (Gervet et al., 2020), Mandalapu et al. (Mandalapu et al., 2021) found that DKT’s perfor-
mance was worse with a larger dataset. Mandalapu et al. hypothesized that SAKT beneﬁts from

4See e.g. https://educationaldatamining.org/edm2021/call-for-papers/, ac-

cessed 2020-10-15

8

a larger dataset size. Interestingly, out of the two models present in both Pandey et al.’s and
Mandalapu et al.’s studies – SAKT and DKT –, SAKT performed approximately similarly in
both Mandalapu et al.’s and Pandey et al.’s studies achieving around 0.76 AUC, while there was
a difference in the performance of DKT, for which Mandalapu et al. got an AUC score of 0.72
and Pandey et al. an AUC score of around 0.742. Since the model and the data are the same,
the difference of around 0.022 AUC is likely due to differences in hyperparameter tuning or the
used hardware; in the present study, we also explore the effects of hyperparameter tuning and
hardware.

What is evident from all the comparisons is that the results of comparing different models
are affected by many factors. Interestingly, the authors of prior comparison studies mostly relied
on AUC scores to evaluate the models: AUC is the only performance metric present in all
three of (Gervet et al., 2020), (Pandey et al., 2021), and (Mandalapu et al., 2021), despite the
possibility of models performing differently with different metrics (Caruana and Niculescu-
Mizil, 2004; Council et al., 2005; Gunawardana and Shani, 2009; Sanyal et al., 2020).
In
addition, the differences between the best few models in the three comparison studies are not
particularly large:
the AUC scores between the best two models, for example, are typically
within 0.02 AUC of each other.

3. EVALUATED MODELS

3.1. OVERVIEW

We inspect three naive models, two commonly used baseline models, and three deep learning
knowledge tracing (DLKT) models. We also include three variants of the DLKT models that we
report as separate models; one of these is a novel variant created for the purposes of this study.
The models and their respective abbreviations are summarized in Table 1.

The models included in this study were chosen based on recentness and gained traction at
the beginning of this work in late 2019, where each selected model reported state-of-the-art
results and possibly also conﬂicting results for the previous state-of-the-art; the included DLKT
models have all been used also in more recent works that build on existing knowledge tracing
models (Liu et al., 2019; Trifa et al., 2019; Oya and Morishima, 2021). The naive models were
selected to provide a more comprehensive analysis of how different results can be achieved even
with simpler methods as well as to provide validity of the machine learning models’ usefulness,
while BKT and a logistic regression model were chosen as baselines to include non-DLKT
models for comparison. We note that modern variants of BKT may provide signiﬁcantly better
results than the baseline we use5, which includes an individualized BKT (Yudelson et al., 2013)
variant.

For our logistic regression baseline we use the Best-LR6 (GLR) model that builds on PFA,
IRT and DAS3H, and was the best performing logistic regression model in the recent study by
Gervet et al. (Gervet et al., 2020).

All the evaluated models, outlined in Table 1, accept inputs as sequences of exercise attempts
per student and output the probability of next attempt correctness for each attempt in the input.
An attempt consists of a skill id st and correctness ct at time step t, where time is an increasing

5https://github.com/myudelson/hmm-scalable, accessed 2020-03-01
6https://github.com/theophilee/learner-performance-prediction, accessed

2021-05-27

9

Table 1: Summary of evaluated models

Model

Shorthand

Details

Mean prediction
Next as previous
Next as previous N mean

Mean
NaP
NaPNM

A simple statistic
A simple baseline model
A slightly less simple baseline model

Bayesian Knowledge Tracing

BKT

Gervet et al. Logistic Regression

GLR

Commonly used as a baseline, predecessor
to DLKT models (Yudelson et al., 2013)
Logistic regression model with best input
feature combination in (Gervet et al., 2020)

e
v
i
a
N

T
K
L
D
-
n
o
N

(

) Long Short Term Memory (Re-
T
K
current Neural Network) Deep
L
Knowledge Tracing
D
Vanilla (Recurrent Neural Net-
work) Deep Knowledge Tracing
Dynamic Key-Value Memory Net-
work (MXNet implementation)
Dynamic Key-Value Memory Net-
work (as depicted in its respective
paper)

LSTM-DKT

First DLKT model (Piech et al., 2015)

Vanilla-DKT

DKVMN

First DLKT model along with LSTM-
DKT (Piech et al., 2015)
First DLKT model with separate next at-
tempt skills as input (Zhang et al., 2017)

DKVMN-Paper

Same as above

Self Attentive Neural Network

SAKT

LSTM-DKT with next skill input

LSTM-DKT-S+

A DLKT model based on Transformer neu-
ral network (Pandey and Karypis, 2019)
LSTM-DKT variation with added skill in-
put as in DKVMN and SAKT, presented in
this work

g
n
i
c
a
r
T
e
g
d
e
l
w
o
n
K
g
n
i
n
r
a
e
L
p
e
e
D

integer sequence. The common variables used in the subsequent descriptions of the models are
summarized in Table 2.

Next, we explore the architectures of the studied models with some mathematical detail
to portray a concrete picture of the differences between the models. We mainly follow the
mathematical notation from the original articles. Some differences to the notation are introduced
for instance to maintain consistency of variable deﬁnitions across the model descriptions in this
work; the model descriptions also include a less mathematical overview of how the models work
as well as illustrative ﬁgures of the DLKT model architectures. The used notation assumes basic
understanding of linear algebra and neural networks.

3.2. NAIVE BASELINES

3.2.1. Mean

Mean is our simplest model. It is also the only one of the naive models that is computed using
training data. The mean model takes the mean of the correctness values in training data and uses
the computed mean as prediction output. To keep the model as simple as possible, the mean is

10

Table 2: Common variables used in the descriptions of the evaluated models

Variable
T ∈ N
t ∈ {1..T }
S ∈ N
s ∈ {1..S}T
c ∈ {0, 1}T
st ∈ {1..S}
ct ∈ {0, 1}

yt ∈ [0, 1]

σ(x)
tanh(x)
x (cid:12) y

Description

Number of attempts in an attempt sequence
Time step of an attempt sequence
Number of distinct skills
Set of skill identiﬁers in an attempt sequence
Set of correctness values in an attempt sequence
Skill identiﬁer at time step (or attempt number) t
Attempt correctness at time step t (1 indicates correct and 0 incorrect)
Model output, i.e. prediction of attempt correctness produced at time step t
(yt is an estimate of ct+1 produced by the model)
Standard logistic function σ(x) = 1
Hyperbolic tangent tanh(x) = e2x−1
e2x+1
Element-wise product between x and y.

1+e−x

computed over all of the training data values, i.e. not per student or skill.

For metrics that operate on binary prediction values (correct or incorrect) instead of percent-
ages, the mean predictor model is equivalent to Majority Vote aka ZeroR (Zero Rate) classiﬁer
that predicts all values as the most common label in given data. This happens because the mean
prediction is rounded in order to compute the metric score. For metrics that are computed from
continuous values (e.g. RMSE), the models are not equivalent.

While the mean model can be considered to have no predictive power, for that reason specif-
ically, it is good for evaluating whether other models are useful in practice or not (Devasena
et al., 2011).

3.2.2. Next as Previous

Another naive model we use is predicting the correctness of a student’s answer at time t+1 to be
the same as the student’s previous answer correctness ct. Formally, NaP(t + 1) = ct. Similarly
to mean prediction, NaP has minimal predictive power and thus serves as an additional validity
check for more sophisticated models.

3.2.3. Next as Previous N’s Mean

For our ﬁnal naive baseline, we extend the Next as Previous model to compute the probability of
a student answering an exercise correct as the mean of N (or t if t < N ) previous correctnesses
so that NaPNM(t + 1) = mean(ct, .., cmax(t−N,1)). We report results for NaP3M and NaP9M.

3.3. BAYESIAN KNOWLEDGE TRACING

Bayesian Knowledge Tracing (BKT) (Corbett and Anderson, 1994) models student knowledge
as a latent, i.e. hidden, variable. It is a special case of the Hidden Markov Model, which is a

11

statistical model containing unobservable variable states where the probability of a state is de-
pendent only of the immediately previous state and no other states. In BKT, student knowledge
of a skill s is represented as a binary variable ms indicating skill mastery (either true or false).

The latent student state is determined by four parameters.
The ﬁrst is prior learning P (L0), which is the probability that a student has learned a skill
before attempting to apply it. Transition P (T ) is the probability of transition from not mastered
to mastered state for a student’s knowledge of a skill after an attempt to apply it. Guess P (G)
is the probability of applying a skill correctly by coincidence. Slip P (C) is the probability of
applying a skill incorrectly by coincidence. BKT models each skill separately and thus sets
these parameters individually for each skill. The inner workings of the BKT model is explained
in more detail in e.g. (Yudelson et al., 2013).

3.4. GERVET ET AL. BEST-LR

The Best-LR (GLR) model is a logistic regression model introduced in a recent Deep Learning
for Knowledge Tracing study (Gervet et al., 2020); the model was the best performing logistic
regression model within the study.

The model leverages knowledge tracing input features that are used in previous logistic re-
gression models for knowledge tracing with an additional feature of total success and error
counts for previous student attempts. The authors describe the model as “DAS3H (Chofﬁn
et al., 2019) without time-window features but augmented with total count features, or equiv-
alently PFA (Pavlik Jr et al., 2009) with rescaled count features, augmented with total counts
features and IRT student ability and question difﬁculty parameters”. Mathematical details of the
GLR model can be found in the original work.

3.5. DEEP LEARNING KNOWLEDGE TRACING

3.5.1. RNN-DKT

The ﬁrst DLKT model DKT (Deep Knowledge Tracing) (Piech et al., 2015) is an RNN, which
is a neural network that uses an internal memory, often referred to as kernel, to process sequen-
tial inputs such as student attempts at an exercise. An RNN for DLKT receives the encoded
sequences of previous attempts and skills as input and produces a probability of next attempt
correctness as output. The probability of next attempt correctness is output for each part of the
input sequence.

The original DKT model was tested with both a “vanilla” recurrent kernel, i.e., a simple
fully connected layer (FCL) with a hyperbolic tangent (tanh(x) = e2x−1
e2x+1) activation function,
and a more complex Long Short Term Memory (LSTM) (Hochreiter and Schmidhuber, 1997a)
recurrent kernel. The main advantage of LSTM over vanilla RNN is that LSTM is speciﬁcally
designed to not lose information across the recurrence over time for long input sequences by us-
ing a gate structure to control computation ﬂow over the recurrent network. In our experiments,
we refer to the DKT models that incorporate an RNN architecture, i.e. RNN-DKTs, by their
kernels, namely Vanilla-DKT and LSTM-DKT.

Mathematically speaking, an RNN kernel is an update equation f for its hidden state h at
timestep t that takes the current input xt and previous hidden state as parameters, i.e., ht =
f (xt, ht−1). In our context, xt is an embedded vector representation of skill st and correctness
ct for timestep t.

12

Figure 1: Architectures of Vanilla-DKT and LSTM-DKT models.

For our vanilla RNN, we used the SimpleRNN7 Keras implementation which consists of
multiplying inputs and weights, summing the result to weighted previous outputs, and applying
an activation function to the resulting sum. We use tanh as the activation function and thus, our
vanilla RNN kernel is ht = tanh(ht−1W h + xtW x + b), where W x and W h are trainable
matrices of input and hidden state weights and b is a bias vector. The RNN kernel output is
equal to ht for the vanilla RNN.

For the sake of conciseness, we hereafter use σ to denote the standard logistic function
f(x) = 1
1+e−x that converts real values into probabilities, i.e. values between 0 and 1. Also,
we use (cid:12) to denote element-wise product. For the LSTM kernel, which has multiple variations
(Hochreiter and Schmidhuber, 1997a; Gers and Schmidhuber, 2001; Graves, 2013), we use the
Keras kernel8 which is based on the Hochreiter and Schmidhuber (Hochreiter and Schmidhuber,
1997b) variant. The chosen LSTM kernel is a complex layer that comprises an input gate, a
forget gate, an output gate and a memory cell. The variables W g and U g in the following equa-
tions represent trainable input and hidden state weight matrices that are speciﬁc for computing
g, where g represents which part of the kernel the weights relate to. The LSTM kernel parts are
deﬁned as follows:

input gate: it = σ(xtW i + ht−1U i + bi)
forget gate: f t = σ(xtW f + ht−1U f + bf )
output gate: ot = tanh(xtW o + ht−1U o + bo)
memory cell: mt = f t (cid:12) mt−1 + it (cid:12) σ(xW m + ht−1U m + bm)
output vector: ht = ot (cid:12) tanh(mt)

The original RNN-DKT models consist of the recurrent kernel and an output layer that pro-
duces the outputs yt = σ(otW y + by) of the neural net. The RNN-DKT architectures for the
two kernels (Vanilla-RNN and LSTM) are depicted in Figure 1.

The RNN-DKT models predict the probabilities of a student having mastered skills given the
student’s exercise attempt sequence. For each attempt at time t, the attempt sequence contains

7https://keras.io/api/layers/recurrent_layers/simple_rnn/, accessed 2020-

01-15

8https://keras.io/api/layers/recurrent_layers/lstm/, accessed 2020-01-15

13

the resulting correctness of the attempt ct ∈ {0, 1}, where t ∈ {1..T }, and a skill id st ∈ {1..S},
where S is the number of distinct skill tags. Given that T denotes the number of exercise
attempts for a student, the neural network takes as input a sequence of student’s exercise attempt
tuples (st, ct) that are combined into integers 2st + ct and then converted into embedded one-
hot vectors xt ∈ R2S. The T × 2S sized sequence is then fed to a standard LSTM recurrent
neural network with T outputs yt ∈ (0, 1)S, one per each time step t. In the resulting output
vector, each vector dimension represents the probability of a student knowing the corresponding
skill. The model is trained by considering only the skill tag at time t + 1 with a binary cross
entropy loss L(yT · δ(st+1), ct+1) per student attempt where L denotes the binary cross entropy
function and δ denotes one-hotting. Note that T in the loss function denotes transpose and not
the number of attempts. Overall, this means that the model can improve itself for the upcoming
skill based on previous skills for each time step. A successful modiﬁcation (Yeung and Yeung,
2018), coined DKT+, has been created to improve the current timestep’s skill output in addition
to the next skill output. This modiﬁcation has been left out from the current study however,
since the differences in model performance are not reportedly signiﬁcant.

3.5.2. DKVMN

Dynamic Key-Value Memory Network (DKVMN) (Zhang et al., 2017) is an RNN similar to the
previously discussed DLKT models LSTM-DKT and Vanilla-DKT, however, its architecture
differs signiﬁcantly from these two models. DKVMN is built to leverage the additional infor-
mation of next attempt skill tag as an input in addition to the current skill tag and the current
attempt correctness. The architecture is heavily inspired by RNNs that use a read-write memory
recurrent kernel that incorporates an attention mechanism (Graves et al., 2014). Namely, these
RNNs are the Memory Network (Weston et al., 2014; Sukhbaatar et al., 2015) and the Key-Value
Memory Network (Miller et al., 2016) models, which were originally designed to solve natural
language processing problems. An additional difference is that the DKVMN, as presented orig-
inally, replaces the one-hot embedding of inputs used in LSTM-DKT with embedding matrices
(Gal and Ghahramani, 2016) to transform the inputs into dense vector representations.

In brief, the DKVMN consists of a recurrent kernel with read and write processes where read
denotes the process of generating values for output of the kernel, and the write process involves
updating the state of the recurrent kernel. The key inputs are used to compute attention weights
via a key memory layer, whereas value inputs are used in the write process. The computed
attention weights are applied to both read and write processes of the recurrent kernel. The
outputs of the read process of the kernel are then fed to a feed-forward network that produces
the model outputs, i.e., the probabilities of exercise correctness.

The DKVMN model involves two embedding layers to process the skill and correctness
inputs into ﬁxed-size dense vectors. A key embedding layer for the next skill id in the input
sequence, and a value embedding layer for concatenated current skill and correctness. We denote
the ith row of a weight matrix A as A(i), key embedding size as K and value embedding size
as V . Using a key embedding matrix Ek ∈ RS×K, key embeddings kt are obtained from
the matrix row that corresponds to the next skill in sequence, i.e. kt = Ek(st+1). Similarly,
value embeddings vt are obtained from a value embedding matrix Ev ∈ R2S×V so that vt =
Ev(2st + ct).

We describe the DKVMN recurrent kernel using two distinct steps. The ﬁrst step is the read
process, where an input of a single timestep is read and an output is produced. The second

14

(cid:80)N

step is the write process, in which the DKVMN recurrent kernel (value memory) is updated for
processing the next timestep. DKVMN has two separate memory layers, one for values and one
for keys, both with the same memory size M . The key memory M k ∈ RK×M , which is non-
recurrent, is used to compute attention weights wt by multiplying each key memory row with
the key embedding so that wt = (cid:0)softmax(ktM k(1)), .., softmax(ktM k(M ))(cid:1) ∈ RM , where
the softmax function is softmax(x)i = exi

j=1 exj for all values i ∈ {1..N } in vector x.

A read value vector rt is computed using the attention weights and current value memory
t ∈ RM ×V as (cid:80)M
i=1 wt(i)M v(i) ∈ RV , i.e. the read value is a weighted sum of value memory
M v
rows, where key memory provides the weights. Note that for the ﬁrst input, the read value is not
dependent on correctnesses, but only the skill id st+1 and the initial weights of the value memory.
The read value is then concatenated with the key embeddings and the concatenated vector is fed
to a dense, i.e., fully connected layer (FCL) of size S(cid:48) with weights W s ∈ RK+V ×S(cid:48), which
is called the summary layer as it is meant to summarize student knowledge state at time t.
Then, the summary layer output, i.e., the summary vector st = tanh(concat(kt, rt)W s + bs) ∈
(−1, 1)K+V is fed to a binary, i.e. one-neuron, sigmoid activated output layer with weights
W y ∈ RS(cid:48)×1 resulting in a probability prediction yt = σ(stW y + by) ∈ (0, 1) for exercise
correctness at time t + 1.

The write process of DKVMN involves value memory, attention weights, an addition layer
and an erase layer. The addition layer is a FCL with weights W a ∈ RV ×V with tanh activation.
The erase layer is a FCL with sigmoid activation and weights W e ∈ RV ×V . The addition and
erase layers both take value embeddings vt as input and respectively produce an addition vector
at = tanh(vtW a + ba) ∈ (−1, 1)V and an erase vector et = σ(vtW e + be) ∈ (0, 1)V as
output. Each value memory row is then updated by multiplying the value memory row with
complemented attention weighted erase vector values et summed with the addition vector at,
i.e. the updated value memory M v

t+1(i) = at + M v

t (i)(1 − wt(i)et).

During reimplementation and veriﬁcation of DKVMN, we noticed that the MXNet imple-
mentation found on the author’s GitHub page9 included a fully connected layer (FCL) with tanh
activation function for generating key vectors from key embeddings, which is not mentioned
in the original article nor in the GitHub documentation. Notably, we ﬁnd that the MXNet im-
plementation replaces the key memory layer described in the article with a regular dense layer
with softmax activation. The differences between the two models (the version presented in the
article and the version in the implementation) is shown in the Figure 2 that represents the model
architectures of the article version and MXNet version. Both the article version and the MXNet
version have been reimplemented for the present evaluation.

3.5.3. Self-Attentive Knowledge Tracing

Self-Attentive Knowledge Tracing (SAKT) (Pandey and Karypis, 2019) is an attentive recurrent
neural network that is based on the self-attentive neural network Transformer (Vaswani et al.,
2017). Similar to DKVMN, SAKT uses embedding layers to create dense vector representations
of encoded skill-correctness inputs and next skill key inputs. The key embeddings and value
embeddings in SAKT are created as described in the read process of DKVMN with the addition
of positional encoding to value embeddings. The positions are added to value embeddings to add
a notion of order in time, because unlike RNNs, attention networks do not operate sequentially.

9https://github.com/jennyzhang0215/DKVMN, accessed 2020-01-01

15

Figure 2: DKVMN architectures for the article and the MXNet implementation based on
https://github.com/jennyzhang0215/DKVMN.

SAKT leverages a self-attention layer to process sequential input without explicit ordering that
effectively reveals distance in time.

To be more precise, SAKT incorporates a self-attention layer called multi-head attention that
computes attention for each of its inputs simultaneously. For preventing the use of future value
inputs, SAKT uses a mask on the multi-head attention that constrains the attention computation
to only the preceding inputs. The self-attention layer outputs a vector of real values that is then
passed to a feed-forward network that outputs the attempt correctness predictions of the SAKT
model. As opposed to attention in DKVMN, while DKVMN leverages attention for its read
values per each timestep, SAKT applies attention jointly on the whole key and value sequences
that precede each attempt.

More formally, as in DKVMN, the inputs for SAKT are key embeddings kt and value em-
beddings vt for each timestep t. To add information of timesteps in SAKT, position embeddings
pt = W p(t) are added to value embeddings to create positioned value embeddings vp
t = pt+vt.
These embeddings are then passed on to the multi-head attention mechanism.

t = vp

t W v and value vectors va = vp

Multi-head attention ﬁrst projects its inputs, key and positioned value embeddings, and
projects them with linear dense layers W k ∈ RK×A and W v ∈ RV ×A to query vectors qa
t =
ktW k, key vectors ka
t W v, each of size A. Note that the key
embeddings serve as query inputs in multi-head attention terminology, and that the positioned
value embeddings serve as both key and value inputs for the multi-head attention. The projec-
tions are fed into a dot-product attention layer (Luong et al., 2015). Attention(qa
t ) =
t ·ka
softmax( qa
) · va, whose outputs are then concatenated and projected once more with a linear
t√
A
fully connected layer.

t , ka

t , va

SAKT processes the outputs of the multi-head attention with a feed-forward network that
consists of three fully connected layers, where the ﬁrst layer is ReLU activated and the second

16

Figure 3: Self-Attentive Knowledge Tracing (SAKT) architecture. In SAKT, value embeddings
are used for both value inputs va and key inputs ka in the attention layer, i.e. ka = va.

is linear. The ﬁnal layer is a one neuron sigmoid activated output layer for producing exercise
correctness probabilities. The model architecture and the attention mechanism are illustrated in
Figure 3.

3.6. MODEL VARIATIONS

Next, we describe and summarize the model variations that we include to the present study.
First, the RNN-DKT contains two kernel versions, a vanilla kernel and an LSTM kernel. The
kernel is the main component of RNN-DKT, and therefore we consider the two RNN-DKT
versions, Vanilla-DKT and LSTM-DKT, as separate models. Second, the DKVMN model has
a model variant that modiﬁes its recurrent behaviour. The inclusion of the two variations are
due to the observed differences between the original DKVMN article and the authors’ MXNet
implementation as discussed at the end of Section 3.5.2. Third, we also introduce our own
variation to LSTM-DKT that is based on the differences of DKVMN and SAKT when compared
to the original LSTM-DKT. Aside from the major architectural differences of the models, we
seek to quantify the effect of giving the LSTM-DKT model information about the next attempt
skill input, as is done in DKVMN and SAKT. The LSTM-DKT-S+ that includes key vector
inputs at time step t + 1 in the same fashion as in DKVMN and SAKT is shown next to the
original LSTM-DKT model in Figure 4.

In addition, we examine the impact of input and output processing variations for each model
and model variation, i.e. one-hot embedding versus using an embedding layer, and the skills-
to-scalar output in SAKT and DKVMN versus the output-per-skill layer in the original RNN-
DKT models. Whereas one-hot embedding produces a vector with a size relative to the number
of skills in a dataset which risks the vectors becoming impractically large, embedding layer
allows one to keep the embedding size smaller by learning how to represent categorical inputs as

17

Figure 4: Architectures of LSTM-DKT, described in 3.5.1, and LSTM-DKT-S+ that includes
information about the future skill in the input.

arbitrary sized real-valued vectors. Piech et al. (Piech et al., 2015) acknowledged this problem in
one-hot embedding already in their introduction of the RNN-DKT models. They proposed using
random low-dimensional Gaussian vector encodings to tackle the problem, which corresponds to
using embedding layer minus the learning. For the output layer variations there are two distinct
differences: The most obvious difference is that skills-to-scalar introduces an additional layer
with the activation function tanh to the recurrent outputs. The second, more subtle difference,
is in how the learned skills are represented and learned in the output layer(s). When training
the output-per-skill variant the skill weights are trained separately, only the output weights that
correspond to the next attempt skill is trained out of all of the skill weights. Conversely, for the
skills-to-scalar variant, all of the skill weights (input weights of the skill summary layer) are
trained for each output regardless of skill. The ﬁnal scalar output corresponds the probability of
the student getting the next attempt correct regardless of the skill unless the model is provided
with next skill information as input.

These variations are illustrated for the LSTM-DKT model in Figure 5.

In general, they
allow us to gain insight into how much of the models’ performance differences can be attributed
to minor model modiﬁcations that are interchangeable between the models as they affect only
pre- and post-sequential processing.

The model variations in input and output processing are minor architectural changes and
they can be easily implemented as hyperparameters. Thus, we consider the input and output
model variations as hyperparameters among other tunable model variables that may or may not
affect model performance. The hyperparameter optimizations are outlined in more detail in the
next Section.

18

Figure 5: Output layer and input encoding variations shown for the LSTM-DKT model. In the
present study, the same variations have been applied to the other models as well.

4. METHODOLOGY

4.1. DATASETS

For the purposes of our study, we use seven datasets (summarized in Table 3), which are as
follows: 1) ASSISTments 2009 updated10, 2) ASSISTments 201511, 3) ASSISTments 2017
Data Mining Competition12, 4) Statics 201113, 5) Synthetic-5 (k=2) and 6) Synthetic-5 (k=5) 14,
and 7) a new dataset called IntroProg15.

The ﬁrst six of the datasets have been previously used in knowledge tracing evaluations,
while the seventh one is a new dataset created for the purposes of this study. The ASSISTments
2009 updated, 2015, and 2017 come from the ASSISTments system (Heffernan et al., 2006;
Heffernan and Heffernan, 2014; Feng et al., 2009). The 2009 updated is an updated version
of the original 2009 dataset, accounting for DKT-speciﬁc evaluation related issues in the orig-
inal dataset (Xiong et al., 2016). The Statics 2011 dataset comes from an engineering statics
course (Steif and Bier, 2014), and the Synthetic-5 datasets are simulated datasets which were
originally used to test the LSTM-DKT model (Piech et al., 2015). The synthetic datasets have
been built using IRT to generate student responses for k ∈ 1..5 hidden exercise concepts for
a sequence of 50 exercises per student – for the present study, we include data for k = 2 and
k = 5 as these are pre-generated and publicly available. The k = 5 version has also been used
in later works

10Retrieved from https://github.com/jennyzhang0215/DKVMN/tree/master/data/

assist2009_updated, accessed 2020-01-15

11Retrieved

from

https://sites.google.com/site/assistmentsdata/home/

2015-assistments-skill-builder-data, accessed 2020-01-15

12Retrieved

from

https://sites.google.com/view/assistmentsdatamining/

dataset, accessed 2020-01-15

13Retrieved from https://github.com/jennyzhang0215/DKVMN/tree/master/data/

STATICS, accessed 2020-01-15

14Retrieved from https://github.com/chrispiech/DeepKnowledgeTracing/tree/

master/data/synthetic, accessed 2020-01-15
and
the

15Available

source

code

with

dlkt-anon-sources-and-res-zip

19

results

at

https://tinyurl.com/

The seventh dataset, IntroProg, contains information from a total of 3273 students. The
data comes from a 2 ECTS16 introductory programming course organized by a Nordic research-
oriented university.
In the course, students learn principles of procedural programming (i.e.
input/output, variables, conditionals, loops) and learn to work with basic data structures (lists,
maps). In total, the course has 61 programming exercises. The course is offered as an online
textbook with an integrated programming environment and an automated assessment system.
Whenever a student submits an exercise to the automated assessment system, the system records
the submission as well as information on the correctness of the attempt. Each entry in the data
includes a student id, an exercise id, and correctness of the attempt. An attempt is classiﬁed as
correct when all instructor-written tests pass for the submitted exercise. There is no limit for the
number of submissions to exercises. Only the best submission is considered when grading the
course, which means that the student can continue working on an exercise even after a failed
submission. This is in contrast to e.g. ASSISTments datasets, where students are given a new
exercise once they submit it, regardless of the correctness.

For data preprocessing, we follow the methodology of (Zhang et al., 2017).

If the data
does not include skill identiﬁers, exercise identiﬁers are used instead as skill identiﬁers. As an
exception, for the Statics dataset, we use exercise identiﬁers instead of skill identiﬁers. Data
rows that do not contain a skill identiﬁer, a user identiﬁer, or correctness are discarded from
the analyses. That is, every attempt must contain a skill identiﬁer (or an exercise identiﬁer for
datasets where such is used as skill identiﬁer), a user identiﬁer and a correctness value. Similarly,
for datasets where the correctness of an attempt was not a binary variable (ASSISTments 2015),
we only used the attempts that were correct or incorrect. Finally, if a student has at most one
attempt, data from the student is excluded as such data is impractical for training or evaluating
the models.

Table 3: Summary of the datasets used in our evaluation. “Max Att.” stands for maximum
attempts in the data for a single student, and “E. ids” stands for the total number of exercise
identiﬁers in the data.

Dataset

Max Att. Students Attempts Correct (%) E. ids Skill ids

ASSISTments 2009 (u)
ASSISTments 2015
ASSISTments 2017
Statics 2011
Synthetic-5 K=2
Synthetic-5 K=5
IntroProg

1261
632
3057
1181
50
50
1857

4151
19917
1709
333
4000
4000
3273

326k
683k
943k
189k
200k
200k
172k

214k (66%)
514k (72%)
351k (37%)
145k (77%)
137k (69%)
122k (61%)
84k (49%)

110
100
3162
1223
50
50
64

-
102
98
-
-
-

4.2. APPROACH

4.2.1. Metrics

Results are reported using seven metrics: Accuracy (ACC), Area Under the Curve (AUC), Preci-
sion, Recall, F1 score, Matthews Correlation Coefﬁcient (MCC), and Root-Mean-Square Error

16ECTS stands for European Credit Transfer System. One ECTS is approximately 25 to 30 hours of work,

although this naturally varies between students.

20

(RMSE). Multiple metrics are included, since e.g. (Caruana and Niculescu-Mizil, 2004) and
(Council et al., 2005) suggest that models can perform differently depending on the metric and
that using a single metric can lead to misguided judgement of model performance. Accuracy
is included as an intuitive metric that tells the overall correct prediction percentage. The AUC,
which is more precisely the Area Under the Curve of Receiving Operator Characteristic (AU-
ROC, or ROC-AUC), is a commonly used metric in DLKT models.
It is “equivalent to the
probability that the classiﬁer will rank a randomly chosen positive instance higher than a ran-
domly chosen negative instance.” (Fawcett, 2006). Precision gives insight on how many of the
predicted positive values were actually positive, whereas Recall tells how many of all positive
instances were correctly identiﬁed as positive. F1 score is the harmonic mean of Precision and
Recall and is an often used metric although susceptible to imbalances in data (Chicco and Ju-
rman, 2020). Matthews correlation coefﬁcient (MCC) provides high scores only if predictions
are accurate regardless of the rate of negative and positive elements in a dataset. RMSE is a
commonly used metric, which unlike the other metrics, is computed from raw prediction values.
These metrics are summarized in Table 4, where confusion matrix terms true positives (TP),
false positives (FP), true negatives (TN) and false negative (FN), are used for deﬁning some of
the metrics.

Table 4: Deﬁnitions for the used metrics. True Positive (TP), False Positive (FP), True Negative
(TN), and False Negative (FN) come from confusion matrix terminology.

Accuracy

T P +T N
T P +F P +T N +F N

Precision

Recall

F1

AUC

T P
T P +F P

T P
T P +F N

2 ∗ Precision∗Recall
Precision+Recall

Area under a plot line with recall on y-axis and false positive rate
(
F P +T N ) on x-axis for decision thresholds from 0 to 1

F P

MCC

√

T P ×T N −F P ×F N

(T P +F P )(T P +F N )(T N +F P )(T N +F N )

RMSE

(cid:113) (cid:80)N

i=1(ci−yi)2
N

For confusion matrix based metrics, we consider the attempt correctness ct value 1 as a
positive label and 0 as a negative label. To determine the polarity of predicted values, i.e. whether
a value is positive or negative (0 or 1), we use 0.5 as the decision threshold. This means that a
prediction yt is considered positive if it is equal or greater to 0.5. RMSE is an exception in the
used metrics as it does not use confusion matrix categories but is computed directly from model
prediction values. AUC is also distinct from most other included metrics as it is independent of
a decision boundary due to it being computed from a plot over all decision thresholds (from 0 to
1). Out of the seven metrics, AUC, and MCC take class imbalance into account, which can be
beneﬁcial on interpreting results from skewed datasets, although it can mask poor performance
by weighing underrepresented labels too heavily (Lobo et al., 2008; Jeni et al., 2013).

Our simplest baseline model, Mean (majority vote for binary metrics) is by deﬁnition unable

21

to produce meaningful AUC, MCC, Recall, Precision (when majority of labels are negative), and
by extension F1 scores. For AUC, this happens for the Mean since for each decision boundary,
every predicted value is on the same side of the decision boundary, wherefore Recall equals false
positive rate and thus AUC is a constant 0.5. MCC cannot be computed due to division by zero,
which also applies for Precision and Recall when the majority of labels are negative. When the
majority of labels are positive, Precision is meaningful but Recall will always be one.

4.2.2. Naive and non-DLKT model training

The naive models included in the study are outlined in Section 3.2. The naive models do not
require training apart from using training set to determine the mean correctness for the Mean
model. In addition, we use two non-DLKT baseline models BKT and GLR that are statistical
classiﬁcation models and contain parameters that need to be trained, i.e. learned. These are
presented in Sections 3.3 and 3.4.

For BKT, we use the publicly available hmm-scalable17 implementation by Yudelson,
which includes individualized BKT (Yudelson et al., 2013). We evaluate the available solvers
for the BKT implementation and pick the best results per dataset according to the RMSE metric.
We choose the best solver based on RMSE instead of AUC since by inspecting the results we
ﬁnd that there is minimal difference in AUC scores compared to choosing the best model by
AUC. However, especially for the ASSISTments 2015, the best model by AUC is signiﬁcantly
worse on most other metrics. Also, previous studies have shown that AUC is a less viable metric
for BKT than RMSE (Dhanani et al., 2014; Pel´anek, 2015). For other hyperparameters we use
the default settings. For GLR, we use the publicly available implementation18 by Gervet et al.
discussed in (Gervet et al., 2020). When training the model, we use the default L-BFGS solver
provided in the implementation.

We use 5-fold cross validation to evaluate the models, i.e. the data is divided into ﬁve parts,
and the models are then trained and evaluated ﬁve times using one part as the test set and the
other parts as the training set. Each part is used once in test, and four times in training. The
reported results are averaged over the ﬁve training and testing iterations. Dividing the data into
the parts is performed by student, meaning that data from each student is present only in one
part at a time and not divided over the parts.

In order to have the GLR model implementation comply with our methodology, we modiﬁed

the data preprocessing and added 5-fold cross validation.

4.2.3. DLKT Model training and hyperparameter optimization

All DLKT models are optimized using binary cross-entropy (log loss): −(ci log(yi) + (1 −
ci) log(1 − yi)), which is an often-used loss function for training neural networks. The machine
learning models are evaluated using 5-fold cross validation. The models are given one hundred
training iterations with early stopping, where early stopping is performed if ten consecutive iter-
ations do not improve the performance of the model, measured with binary cross-entropy19. A

17https://github.com/IEDMS/standard-bkt,https://github.com/myudelson/

hmm-scalable, accessed 2020-03-01

18https://github.com/theophilee/learner-performance-prediction, accessed

2021-05-27

19Typically, a “main” evaluation metric is selected for early stopping. Here, we deem the training metric more

suitable as we aim to measure the effect of evaluation metric choice.

22

Table 5: Overview of hyperparameters used for training DLKT models. Although there are
a maximum of two options per hyperparameter, the variation count is not a power of two since
some options cancel each other out. Also, not all hyperparameters affect every model, e.g. SAKT
is the only model that uses attention heads.

Hyperparameter

Recurrent layer size
Key embedding size
Value embedding size
Skill summary layer size
Input variation
Output variation
Learning rate
Dropout
Attention heads
Batch size
Random Seed
Optimizer

Options

{50, 100}
{20, 50}
{20, 50}
{50, 100}
{One-hot, Embedding}
{Output-per-skill, Skill summary + Scalar}
{0.01, 0.001}
{0.2}
{1, 5}
{32}
{13, 42}
{Nadam}

Variations

72-240 (depending on model)

validation set which consists of 10% of the training data is used to determine the early stopping.
Thus, the training data for each fold comprises 70% of the data while the remaining 20% is used
for evaluation.

Grid search is used to ﬁnd optimal hyperparameters for each model, summarized in Ta-
ble 5. The best hyperparameters are selected based on averaged AUC score in the 5-fold cross-
validation. The hyperparameters regarding model layers include recurrent layer size (memory
size for DKVMN), key embedding size, value embedding size and skill summary layer size.
Additionally, the number of attention heads is tuned for SAKT as it uses multi-head attention.
The use of layer sizes depends on the model variation as some models and/or their variations
either do not have a corresponding layer or the size is a set constant. For one-hot input model
variations, the embedding sizes are not applicable as one-hot size is a constant determined by
the count of distinct skill identiﬁers in a given dataset. Likewise, output-per-skill output layer
size is determined by the same distinct skill count to provide an output for each skill as the
name suggests. For model variations with such an output layer, a skill summary layer is not
used. The DKT models apart from LSTM-DKT-S+ do not have separate key inputs, and thus
key embedding size does not affect it.

Besides layer-speciﬁc hyperparameters, we tune the models for learning rate and max at-
tempt count. Although the latter is not necessary, it is used both in DKVMN and SAKT due
to implementation restrictions and possibly for faster training times. While the DKVMN article
does not mention max attempt count, the MXNet implementation by the authors uses 200 as
the max attempt count. SAKT authors mention using different max attempt counts for different
datasets, where the values for max attempt count range from 50 to 500. Both DKVMN and
SAKT implement max attempt count by splitting exceeding attempts as a new attempt sequence
recursively until no attempt sequence exceeds the max attempt count, effectively creating new
students into the dataset. We note that our implementations published as a part of this article do

23

not include this restriction and can be trained with arbitrarily large attempt sequences, although
the training time and space requirement increases signiﬁcantly without max attempt counts. For
the grid search, we use the max attempt count of 200 both for reproducibility purposes and to
reduce model training time.

We also include random number generator seed as a hyperparameter to provide a baseline for
evaluating the effect of hyperparameter tuning. The differences in the results of models whose
hyperparameters differ only regarding the random seed can be safely assumed to be caused
by randomness. This gives insight into what extent randomness may affect the 5-fold cross
validation results.

All DLKT models were trained using input batches of size of 32, 20% dropout and the
Nadam gradient descent optimizer (Dozat, 2016). While all of these may affect the results,
we decided to not tune them systematically for each model to keep the hyperparameter option
space reasonably sized. In addition to comparing the performance of the models where the best
hyperparameters are selected based on AUC, we explore selecting best hyperparameters with
other metrics. Target metric used for model training, for instance, can inﬂuence selected features
and model performance on other metrics (Sanyal et al., 2020). We analyze the effect of choosing
a metric in grid search by comparing the perceived performance loss in other metrics. By loss,
we mean the difference between the performance of the model with “best” hyperparameters
selected based on a given metric, and the best result achieved when selecting models with some
other metric. As an example, we compare the performance of a model that was selected based on
AUC with models selected by other metrics to determine how the performance over all metrics
differ between these models.

We acknowledge that we only included two options for each tuned hyperparameter. This,
along with the untuned options, leaves room for speculation whether more broad tuning would
provide signiﬁcantly different results. All in all, considering all datasets and model variations,
the number of trained and evaluated DLKT models totals 5,880 cross-validated models.

4.2.4. Maximum attempt cut

As brieﬂy mentioned, DKVMN and SAKT, which leverage maximum attempt counts due to the
restrictions in their original implementations, split input sequences according to the maximum
attempt count hyperparameter. This means that if the maximum attempt count is 100 and a
user has 950 attempts, the attempts would be split into 9 sets of 100 attempts and one with 50
attempts. Effectively, this multiplies the number of data points that the models are trained on,
since the models consider the different sets from the same student as completely new students.
Instead of having fewer students with longer attempt sequences, the models have more students
with some having unnatural starting points for their attempt sequences. As it is poorly explained
why this would be the preferred case, we test to what extent cutting off attempts that exceed
the maximum attempt count affect the results. We use the same 200 as maximum attempt count
splitting for maximum attempt count cutting. Further, we test the models for no maximum
attempt count. The effect of splitting or cutting is evaluated only for the best hyperparameter
options determined by the grid search using the hyperparameter options in Table 5.

4.2.5. Hardware and Software

The model training in this work was performed using computer resources within the Aalto Uni-
versity School of Science “Science-IT” project. All DLKT model training was conducted using

24

CPUs unless otherwise noted. The models were implemented in Python using TensorFlow ver-
sion 2.1.0.

Further, to evaluate the effect of hardware, the models were re-trained and evaluated using
GPUs. GPU evaluation was only conducted for the hyperparameter values deemed best by grid
search using CPUs. Due to implementation restrictions, as well as to compare machine learning
framework versions, the TensorFlow version was updated to 2.6.2 for GPU computation. Thus,
we also retrained the models with CPU on the new TensorFlow 2.6.2 version for the CPU versus
GPU comparison and present CPU results for both TensorFlow 2.1.0 and 2.6.2, while the GPU
results are presented only for TensorFlow 2.6.2.

5. RESULTS

In the following result tables, the best metric values for a given model have been selected by
picking the model with the highest AUC score. Thus, the actual highest scores for other metrics
may be higher than in the presented tables because the best model as determined by AUC might
not yield the best results for all metrics. Section 5.3 presents this aspect in greater detail. Full
evaluation results are available in the online repository alongside the source code20.

5.1. BASELINE MODELS AND DLKT MODELS

Here, we mostly focus on the performance of the models using AUC as the metric. Other metrics
are considered in more detail in subsequent sections. These results are outlined in Appendix A.
The DLKT models consistently produce better predictions than the simple baselines and
BKT. The GLR model reaches performance that is above that of the other baselines, including
BKT, consistently. For the IntroProg dataset (Table 12 in Appendix A), GLR is the best per-
forming model, and its performance is also on par with the performance of most DLKT models
on some metrics on the ASSISTments 2015 dataset (Table 10 in Appendix A) and the Statics
dataset (Table 13 in Appendix A), while falling behind DLKT models in other datasets.

None of the baselines apart from GLR reach AUC scores that are able to compete with the
scores of any of the evaluated DLKT models. With BKT, which is the second-best performing
baseline after GLR, the difference in AUC is smallest in the ASSISTments 2015, IntroProg and
Statics (Tables 10, 12 and 13 in Appendix A) where BKT falls behind the bottom DLKT model
AUC score by 2% points, 3.2% points, and 3.4% points respectively. In these three datasets, the
difference between BKT and the bottom DLKT model is greater than the difference between
the top DLKT model21 and the bottom DLKT model22; 1.1% points, 0.6% points and 1.7%
points for ASSISTments 2015, IntroProg and Statics respectively. Notably however, the best
performing model for IntroProg is not a DLKT model but GLR: the difference in AUC score
between GLR and the top-performing DLKT models LSTM-DKT and DKVMN is 1.6% points.
When considering BKT, it is in some cases unable to beat the naive baselines, in addition
to its poorer performance compared to the DLKT models or GLR. This is evidenced in the
Synthetic-K2 dataset (Table 14 in Appendix A), where BKT achieves a lower AUC score (0.635)
than the NaP3M (0.654) or the NaP9M (0.709).

20https://tinyurl.com/dlkt-anon-sources-and-res-zip
21LSTM-DKT for ASSISTments 2015, DKVMN and LSTM-DKT for IntroProg, and DKVMN for Statics
22SAKT for ASSISTments 2015, Vanilla-DKT for IntroProg, and SAKT for Statics

25

Overall, the DLKT models surpass GLR and other baselines as DLKT models outperform
the best-performing baseline GLR on 3 out of 5 real datasets and on both synthetic datasets.
A clear difference between model performances of the two best DLKT models (DKVMN and
LSTM-DKT) versus baselines can be seen for each metric in Figure 7, which shows scores
averaged over datasets.

5.2. MODEL PERFORMANCE ON DIFFERENT METRICS

In the previous Section 5.1, we focused on the performance of the models when using AUC
as the metric. Next, we look deeper into the differences in model performance when consider-
ing other metrics. These metrics are accuracy, precision, recall, F1, MCC, RMSE, which are
presented in Section 4.2.1.

As noted previously, the DLKT models overall outperform GLR as well as other baselines
when using AUC. The same observation can be made when studying the average performance of
the models over all datasets, presented in Figure 7 (the ﬁgure includes the two best performing
DLKT models and the baselines). When contrasted with AUC results, we however observe more
variation between the performances of the DLKT models and the baselines. When looking at
the results in Tables 12 and 13 in Appendix A for different metrics, GLR ranks among the best
on IntroProg and Statics datasets. In IntroProg, GLR receives the best scores for Accuracy, AUC
and MCC (0.4% points, 1.6% points, and 3.1% points respectively). In Statics, GLR receives
scores similar to most DLKT models. In the other datasets, DLKT models perform better on all
metrics, excluding precision and recall.

When ranking the models based on their performance in the different metrics, we observe
a degree of variation between the metrics and the datasets. For example, for the ASSISTments
2015 dataset (Table 10 in Appendix A), the simple Mean baseline has F1-score (0.849) which
is equal to the F1-Score of GLR, topping all DLKT models. Similarly, for the same dataset,
the Mean baseline’s accuracy (0.738) is close to the accuracy of the complex models, falling
short 1.3% points from the best performing model (LSTM-DKT). When studying the average
performance of the GLR and the best two DLKT models over all datasets, shown in Figure 7,
we see that the differences when measured with RMSE are somewhat small, for Accuracy and
F1 moderate, and for AUC and MCC the differences are rather clear.

Considering only the DLKT models, the inter-model performance differs depending on the
observed metric, and the differences partially depend on the dataset. For example, for the AS-
SISTments 2009 Updated dataset (Table 9 in Appendix A), the accuracy, F1-Score and RMSE
between the DLKT models, excluding SAKT, is at most 1% point, while AUC shows 1.6% point
difference between the best and worst DLKT models.

There are some cases where the ranking of the models depends on the chosen metric, al-
though the differences are often small. As an example, for the IntroProg dataset (Table 12
in Appendix A) when comparing SAKT and GLR, SAKT slightly outperforms in F1-Score
(0.759 vs 0.755), while GLR outperforms in e.g. AUC (0.843 vs 0.825). Similar observations
can be made also for the ASSISTments 2015 dataset (Table 10 in Appendix A), where SAKT
outperforms GLR in AUC (0.714 vs 0.702) and MCC (0.230 vs 0.204), while GLR slightly
outperforms SAKT in F1-Score (0.849 vs 0.846) and Accuracy (0.750 vs 0.748).

As for the top performing model all in all, there is no absolute victor, since no model domi-
nates all datasets and every metric. Although the differences are not major, we ﬁnd that LSTM-
DKT (both original and S+ versions) and DKVMN (MXNet version, i.e. not DKVMN-Paper)

26

perform consistently as the best or near the best over the evaluated models across the varying
metrics. For AUC, this can be seen in Figure 6. Also, the top-performing DLKT models receive
on average better scores for all metrics compared to the baseline GLR or other baselines as can
be seen in Figure 7.

On closer inspection into the DLKT models, we observe that LSTM-DKT and LSTM-DKT-
S+ hold the top AUC score on ASSISTments 2009, ASSISTments 2015 and ASSISTments
2017, while DKVMN holds the top AUC score on Statics, Synthetic-K2 (shared with SAKT)
and Synthetic-K5. Even though LSTM-DKT models are ranked ﬁrst on more datasets, the
only dataset where the performance difference can be considered other than marginal is AS-
SISTments 2017 where the difference in AUC score is 2.1% points. For all other datasets, the
difference between the LSTM-DKT models and DKVMN is at most 0.5% points.

Additionally, when considering the LSTM-DKT and LSTM-DKT-S+, we observe that in
most cases their performance is almost the same if not the same across all datasets and metrics.
The largest differences can be observed in ASSISTments 2017 in F1-Score, where the difference
is 0.4% points in favor of LSTM-DKT-S+, in IntroProg in MCC, where the difference is 0.4%
points in favor of LSTM-DKT, and in Synthetic-K2 in MCC, where the difference is 0.4% points
in favor of LSTM-DKT-S+.

All of the above model differences are derived from comparing models that have been hy-

perparameter tuned for the AUC score.

5.3. METRICS AND DETERMINING THE BEST MODEL HYPERPARAMETERS

In the previous parts, the comparisons were based on models that were tuned for the AUC score.
To consider the impact of tuning models for other metrics, we analyzed the model evaluation
results for all the hyperparameter variations for each model and dataset. This was conducted
to gain an overview of to what extent the model that is ranked the best depends on the metric
that is used to pick the best model out of the trained models. These results are summarized in
Figure 5.3, which shows the mean and max losses over our evaluated datasets and models for a
given metric when some other metric is used to pick the best trained model among models with
different hyperparameter options. For this metric comparison we included the results of maxi-
mum attempt count analysis, which is discussed in the next section. In the metric comparison,
in addition to the evaluation metrics, we also include log loss that was used in model training.

Overall, picking a model based on the performance measured using a speciﬁc metric most
likely does not mean that the model is the best when considering the other metrics. On aver-
age, the mean losses are small but noticeable, although in some cases the mean losses can be
measured in multiple percentage points. For example, when picking a model based on F1-score
and then looking at the MCC score, the mean loss is 1.5% points. For the other options, the
mean losses are under 1% point, and mostly under 0.5% points. When considering the max loss,
i.e. highest loss when optimizing a speciﬁc metric and considering another metric within all the
models and the datasets, the differences can be large. For example, when picking a model based
on Accuracy, F1-Score, RMSE, or Log loss, and then looking at the MCC, the maximum loss is
7.7% points. When picking a model based on Accuracy and then looking at the F1-Score, the
maximum loss is 3.0% points (similar to picking MCC and looking at Log loss). In most cases,
the maximum loss is under 2.5% points.

When focusing on AUC, which is often used as the main comparison metric in knowledge
tracing studies, also consequently in our replication study, we observe that if AUC is used for

27

Figure 6: Best AUC scores of each DLKT model per dataset.

Figure 7: Result comparison over the two best performing deep learning models over baselines.
Scores are averaged over all datasets

hyperparameter optimization, we sacriﬁce up to 2.5% points in MCC and up to 1.5% points in

28

Accuracy and F1-Score. In other words, using another hyperparameter set for the same model
and dataset would achieve 2.5% points higher MCC than the hyperparameter set used to obtain
the best AUC score.

To summarize, according to our results, there does not appear to be a metric that would be
optimal for optimizing all metrics, as all metrics when used for optimization risk losses for at
least one other metric. The optimization metric comparison does however suggest that F1-score
conveys the most risk, as evidenced by the mean losses which are the highest out of the studied
metrics. On the other hand, looking at the max losses, all of the metrics convey a risk.

Figure 8: Mean and maximum differences in metric scores compared to optimal over datasets
and DLKT models for using different metrics to select the “best” hyperparameters. The loss
indicates how much lower scores for other metrics can be expected when choosing a metric for
hyperparameter optimization.

5.4. OPTIMAL HYPERPARAMETERS AND MODEL VARIATIONS

Here we inspect the impact of hyperparameter tuning on model performance, including analyz-
ing the effect of the input and output model variations. The evaluated hyperparameter combina-
tions are outlined in Table 5 in Section 4.2.3.

The hyperparameter tuning results are presented as follows. First, we brieﬂy outline the
optimal hyperparameters in general, which is followed by input and output model variations
(output-per-skill vs skills-to-scalar output, one-hot input vs embedding layer, max attempt cut vs
max attempt split vs no max attempt count). To discern the magnitude of the effect of the chosen
hyperparameters, we also outline results related to random seed selection and used hardware
(CPU vs GPU).

5.4.1. Optimal Hyperparameters

The hyperparameters that yielded the best performance as measured by the AUC score are out-
lined in tables in Appendix B, one table per dataset. In general, the optimal hyperparameters are

29

model- and dataset-speciﬁc. When considering layer sizes, the recurrent layer size is more often
smaller (50) than larger (100) for all models apart from Vanilla-DKT, key and value layer sizes
do not show a pattern, and summary layer size appears more often smaller (50) for DKVMN
and DKVMN-Paper and larger (100) for other models when it is used.

As for the learning rates, the models most often perform better with 0.001 as the learning rate
when compared to the other option 0.01. The only exception is DKVMN-Paper, which always
performs better with 0.01 except for the two synthetic datasets. Also, in the ASSISTments 2017
dataset all models but SAKT and Vanilla-DKT perform better with 0.01 learning rate. The two
models, SAKT and Vanilla-DKT are also the only ones that perform best with learning rate
0.001 on all the evaluated datasets. The number of attention-heads for SAKT also differs across
datasets with 1 as the optimal for the datasets IntroProg and Statics and 5 for other datasets.

5.4.2.

Input and Output Model Variations

In order to triangulate the differences in the evaluated model architectures, we compared input
and output layer variations found between the model architectures that were compatible for all
of the models. We compared the output layer variations output-per-skill against skill summary
layer and a scalar output layer (skills-to-scalar output layers), and the input layer variations
one-hot input against embedding layer.

When comparing output-per-skill and skills-to-scalar output, for LSTM-DKT, Vanilla-DKT
and LSTM-DKT-S+, using an output-per-skill layer provided either as good or better results as
opposed to using skills-to-scalar output layers. For SAKT, the differences are minimal for the
best results, but with output-per-skill layer the SAKT model appears more robust to other hyper-
parameters as the worst scores are much higher than with skills-to-scalar output. On the other
hand, DKVMN shows an opposite effect compared to LSTM-DKT, Vanilla-DKT and LSTM-
DKT-S+ with sometimes clearly better performance for skills-to-scalar output layers. We high-
light a few of these ﬁndings in Tables 23 and 24 included in Appendix B. Notably, in the AS-
SISTments 2017 dataset (Table 23 in Appendix B), DKVMN performs clearly worse when using
output-per-skill than when using skills-to-scalar output layers (difference of 1.9% points), while
the difference for DKVMN-Paper is 3.8% points. The DKT models on the other hand perform
better with output-per-skill layer (LSTM-DKT 1.2% points, LSTM-DKT-S+ 0.5% points and
Vanilla-DKT 2.0% points) For ASSISTments 2015 (Table 24 in Appendix B) DKVMN differ-
ences are much smaller and DKVMN performs slightly better with output-per-skill layer unlike
in ASSISTments 2017. The results for LSTM-DKT, Vanilla-DKT and LSTM-DKT-S+ show a
similar pattern as in ASSISTments 2017. When considering the performance of the evaluated
models overall, skills-to-scalar output results have more variance than the output-per-skill re-
sults, as depicted by the standard deviation of the results in Tables 23 and 24 in Appendix B.
This holds especially for SAKT.

When considering one-hot input and embedding layer, the differences in performance again
depend on the model and the dataset, although the differences are often slight. There are also
a few exceptions, however, as shown in Tables 25 and 26 in Appendix B. In the Statics dataset
(Table 25 in Appendix B), where the differences are subtle, only SAKT performs worse when
using one-hot embedding (0.8% point difference), while DKVMN-Paper performs marginally
better when using one-hot embedding (0.7% points). On the other hand, in the ASSISTments
2009 Updated dataset (Table 26 in Appendix B), DKVMN-Paper, and SAKT perform signiﬁ-
cantly worse when using one-hot embedding (3.3% points and 4.6% points, respectively), and

30

Vanilla-DKT performs slightly worse (0.3% points). For the other models, the differences are
non-existent. This illustrates that for some models such as SAKT, the effect of the input encod-
ing is inﬂuenced by the data.

However, with one-hot inputs, the models often achieve higher minimum scores compared
to using embedding layers. In the ASSISTments 2009 Updated dataset, we see that LSTM-DKT
and LSTM-DKT-S+ achieve much higher minimum scores with one-hot inputs. And similarly,
Vanilla-DKT and SAKT have higher minimum scores for IntroProg dataset. This suggests that
while using embedding layers appears to be the go-to choice when seeking maximal perfor-
mance, using one-hot inputs can be a safer choice as they seem to be more robust regarding bad
choice of hyperparameters.

5.4.3. Maximum attempt count

We considered the maximum attempt count used in original DKVMN and SAKT studies in three
ways. The ﬁrst approach was to split the input data into multiple students if a student’s attempt
count exceeded the maximum attempt count, the second approach was to discard the excessive
attempts, and the third approach was to not use a maximum attempt count. We test the ﬁrst two
approaches with maximum attempt count 200 and 500. This analysis was conducted only for
the best hyperparameters selected using grid-search as outlined in 4.2.3. The comparison results
are shown in Table 28 in Appendix C.

When averaging over all the datasets, shown in Table 29 in Appendix C, the results suggest
that – on average – most models beneﬁt from using no maximum attempt count. SAKT and
DKVMN-Paper are the exceptions that are appear unhindered by the use of maximum attempt
count. The average differences between using and not using a maximum attempt count are subtle
for all the DLKT models, however.

Upon inspection of the results for individual datasets, we ﬁnd different patterns.

In the
ASSISTments 2015 dataset and in the Synthetic datasets (both K2 and K5), the differences are
negligible to non-existent. In the case of the Synthetic datasets, this is explainable with the data
having fewer attempts than the used maximum attempt counts. In other datasets, some noticeable
differences can be observed. As an example, the largest difference in the ASSISTments 2009
dataset for DKVMN-Paper is 1.8% points in AUC in favor of using maximum attempt count
(both cut 500 and split 200 perform similarly). For the ASSISTments 2017 dataset, the largest
difference can be observed for LSTM-DKT, where there is a 2.7% point difference in AUC in
favor of not using a maximum attempt count (when compared to cut 200). However, the drop in
AUC from no maximum attempt count to the best split result (200) and best cut result (500) is
1.0% and 1.1% points respectively. Similarly, for Vanilla-DKT in the Statics dataset, there is a
3.1% point difference in AUC in favor of not using a maximum attempt count (when compared
to cut 500). For no maximum attempt count versus best split (200), the difference is much
smaller (0.7% points) but quite large (2.4% points) versus the best cut (200).

Notable differences are present also in other metrics, for instance 3.9% point difference in
MCC for both DKVMN and LSMT-DKT in IntroProg (no maximum attempt count vs split
200). Furthermore, all the metrics do not show a certain approach consistently as the best. As
an example, when considering the Statics dataset, the best performing option for DKVMN in
terms of AUC or MCC is not using a maximum attempt count, while the best performing option
for DKVMN in terms of Accuracy, F1-Score or RMSE is using cut 200. Averaged over all
datasets, the metrics seem to tell a similar story as AUC. Although, there are slight differences,

31

e.g. DKVMN is best without maximum attempt count according to all the metrics apart from
F1-Score (precision and recall excluded).

5.4.4. Random seed, Hardware and Software Version

As the random seed and the used hardware can also inﬂuence the performance of the models, we
included two random seeds and two hardware types into the evaluation. As per software version
matching challenges for GPU computation within the computing resources at our disposal (and
also to examine potential effect of changing the machine learning framework version), we re-
sorted to using TensorFlow 2.6.2 for the GPU calculations (instead of TensorFlow 2.1.0 that we
used for CPU computation for the other analyses). To take this into account, when comparing
the GPU and CPU, we evaluated the models on CPU with both TensorFlow 2.1.0 and 2.6.2. This
evaluation is conducted as a control to which we can compare the results from tuning the other
hyperparameters that are more directly related to the models themselves.

Regarding random seed, the effects from changing the random seed are negligible (up to
0.1% points change) when looking at the best models. However, more considerable differences
between the random seeds can be observed for the results with suboptimal hyperparameters.
For instance, for SAKT with the ASSISTments 2015 dataset (Table 27 in Appendix B), the
difference for the worst models is 1.8% points. In effect, this indicates that the effect of the
random seed is larger for non-optimal hyperparameters, while the better models are more robust
to the effect of the random seed. Moreover, when considering the standard deviation of the
models’ performance, there are noticeable differences between the models. As an example, the
standard deviation of the models’ performance when using different random seeds is noticeably
higher for SAKT than for other models.

To consider the effect of hardware, we retrained the models on GPU with their optimal hy-
perparameters according to the grid search results for the AUC metric obtained on CPU trained
models. The GPU versus CPU results are shown in Table 30 which can be found in the Appendix
D. As noted previously, the GPU results were obtained using TensorFlow 2.6.2 as opposed to
2.1.0, which was used for the grid search. Due to this, in this evaluation, we also retrained the
models with the best hyperparameters on CPU using TensorFlow 2.6.2.

Mostly, the differences between the two computation unit types are again subtle, ranging
from 0.0 to 0.6% points (e.g. Statics dataset LSTM-DKT-S+ or SAKT in MCC score), similar
to the random seed comparison. The differences are more often close to zero than above 0.2%
points, especially for accuracy and RMSE. In general, we ﬁnd slightly more differences between
the TensorFlow versions on CPU than between CPU and GPU. The largest overall difference we
ﬁnd is 1.7% points (CPU-tf2.1.0 vs CPU-tf2.6.2) in F1 score in the ASSISTments 2017 dataset
for Vanilla DKT. With the same TensorFlow versions the difference is only 0.1% points, when
comparing CPU and GPU. For the same dataset and model, the AUC and MCC differences,
0.6% points and 0.9% points, are also notable.

5.5. MODEL PERFORMANCE AND PREVIOUSLY PUBLISHED RESULTS

Here, we outline our results in light of previously published results. Tables 6 (ASSISTments
2009 / ASSISTments 2009 updated) and 7 (Statics 2011) summarize observed model perfor-
mances in this and prior work: (Piech et al., 2015) (DKT), (Zhang et al., 2017) (DKVMN),
(Pandey and Karypis, 2019) (SAKT), (Gervet et al., 2020) (GLR). The tables include AUC
scores and use two datasets that are common between the articles. In addition, results from

32

models evaluated in this study are included from (Yeung and Yeung, 2018) (DKT+) into the
tables, although our present evaluation does not include LSTM-DKT+.

The results presented in this article agree with the previously reported results to some extent.
When considering the standard deviation of the model-speciﬁc results between the articles, we
note that there are considerable differences. For example, for SAKT, the standard deviation is
4.61% points in the ASSISTments 2009 Updated dataset, and 2.47% points in the Statics 2011
dataset. Similarly, for LSTM-DKT, the standard deviation is 2.69% points in the ASSISTments
2009 Updated dataset, and 1.03% points in the Statics 2011 dataset. For DKVMN, the standard
deviations are 0.4% points and 0.75% points for the ASSISTments 2009 Updated dataset and
the Statics 2011 dataset, respectively. We also observe a minor difference (0.4% points) in
Statics 2011 dataset for GLR but a large difference (4.3% points) in ASSISTments 2009 Updated
dataset.

Table 6: AUC score matrix for models trained on the ASSISTments2009 (Updated) dataset re-
ported on previous articles and this article, labeled as this.

Article

LSTM-DKT LSTM-DKT+ DKVMN

SAKT

BKT

DKT*
DKT+
DKVMN
SAKT
GLR
this

86
82.212
80.53
82.0
75.7
81.4

-
82.227
-
82.2
-
-

-
-
81.57
81.6
-
80.9

-
-
-
84.8
75.6
79.8

67
-
-
-
-
71.0

GLR

-
-
-
-
77.2
72.9

avg (sd)

81.3 (3.33)

82.2 (0.02)

81.4 (0.40)

80.1 (4.61)

69.0 (2.83)

75.1 (3.04)

A row contains an article identiﬁer (speciﬁed in Table 1) and the best AUC scores for models reported
in that article. Note that the high variance is partly explained by differences in the data used in training
the models in the different studies. Most notably, the scores in the DKT article, noted with an asterisk,
stems from using an older version of the data. Note also that our result for DKVMN in this table differs
from our best results, as the results for original output layer is used here for replication purposes.

Table 7: AUC score matrix models trained on the Statics 2011 dataset reported on deep learning
model papers and this article, labeled as this.

Article

LSTM-DKT LSTM-DKT+

DKVMN

SAKT

BKT

DKT+
DKVMN
SAKT
GLR
this

81.59
80.20
81.5
81.5
82.4

83.49
-
83.5
-
-

-
82.84
81.4
-
82.5

-
-
85.3
81.3
80.8

-
-
-
-
77.4

GLR

-
-
-
81.9
81.5

avg (sd)

81.7 (1.03)

83.5 (0.01)

82.25 (0.75)

82.5 (2.47)

77.4 (-)

81.7 (0.28)

A row contains an article name and best AUC scores for models reported in that article.

In addition to the results shown in Tables 6 (ASSISTments 2009 / ASSISTments 2009 up-
dated) and 7, we brieﬂy discuss the results from other datasets (all results are available in the
online repository).

When considering DKVMN results, we observe that the original article shows that DKVMN
outperforms LSTM-DKT, mostly by a few AUC percentage points and on all four datasets that

33

the models were evaluated on. In contrast, in our study when including the hyperparameter
optimizations and model variations (shown in Appendix B in Tables 16, 17, 18, 19, 20, 21,
and 22), DKVMN falls behind LSTM-DKT on the ASSISTments 2009 Updated, ASSISTments
2015, and ASSISTments 2017 datasets. Conversely, on Synthetic-5 DKVMN is the slightly
better performing model while in the original study DKVMN falls short of LSTM-DKT by
2.4% points.

For SAKT, we observe worse performance than reported in the original article, similar to the
Gervet et al (Gervet et al., 2020) study. While the SAKT article reports considerable improve-
ments on both LSTM-DKT and DKVMN on Statics, Assistments 2009, 2015 and 2017 datasets,
in our results, SAKT has worse performance than either DKVMN or LSTM-DKT on all but the
IntroProg (not present in original) and the Synthetic datasets (only K-5 in original), where the
differences are small to negligible.

We acknowledge that the tested hyperparameter combinations, although at times not reported
in the articles, might differ between the studies, which can explain some of the observed larger
differences. Some differences can also be attributed to variations in used data. It seems that
the data preprocessing differs between the studies as the ﬁnal used dataset sizes differ at times,
as seen in Table 8. For example, while both we and Gervet et al. (Gervet et al., 2020) used
the ASSISTments 2009 Updated dataset, the data in the repository for the GLR article was
considerably smaller (14% fewer lines) than the dataset at our disposal. In addition, we also
observe that the DKT+ article has a typo in the Statics dataset size (189,927), where the correct
dataset size is 189,297. The same typo appears also in the SAKT article.

Table 8: Number of interactions used in different datasets in our article result comparisons as
reported in the articles.

ASSISTments 2009 Updated ASSISTments 2015

Statics

DKT+
DKVMN
SAKT
GLR
this

328,291
325,637
328k
278,868
325,515

708,631
683,801
708,631
658,887
683,331

189,927
189,297
189,927
189,297
189,297

The differences between interaction counts 708,631 and 683,801 in ASSISTments 2015 in
DKT+ and SAKT when compared to DKVMN is due to the preprocessing step explained in
DKVMN article (Zhang et al., 2017), where correctness values other than 0 or 1 are excluded.
The data in our work has slightly fewer interactions than that of DKVMN due to our exclusion
of students with less than two attempts.

Note that when downloading the dataset from the ASSISTments site23, the only dataset that
is openly accessible is the corrected and collapsed skill builder dataset. In collapsed dataset, the
number of interactions is 346,860, which does not directly match any of the datasets used in the
studies outlined in Table 8. This could be due to small differences in preprocessing between the
studies. Furthermore, we acknowledge that some of the publicly available datasets may have
changed over time – as an example, the ASSISTments site and the available datasets have been

23https://sites.google.com/site/assistmentsdata/home/assistment-2009-2010-data/

skill-builder-data-2009-2010, accessed 2021-04-01

34

updated even during the present work24.

6. DISCUSSION

6.1. EVALUATION RESULTS - NO SILVER BULLET

Overall, when comparing the model performance, no single model consistently outperformed
all the other models in all the metrics. The best performance is observed for the DLKT models,
especially LSTM-DKT and DKVMN. The GLR also performed on par with the DLKT models
in some datasets, while falling behind in others. The performance of BKT is typically better
than the naive baseline models (mean and variants of next as previous), but worse than DLKT
models and GLR. We acknowledge, however, that our BKT implementation is not heavily tuned.
In addition, we point out that the naive baselines included in this study performed relatively well
in one of the datasets, which showcases the beneﬁt of including simple baselines when reporting
results of more complex models. In ASSISTments 2015, the F1-score of Mean baseline model
is the same as for the best performing model which is GLR. This indicates that in such cases, the
usefulness of the best models can be questionable even if they outperform other complex models
when they can not signiﬁcantly outperform extremely simple statistical models. The observed
high performance of the naive baselines in ASSISTments 2015 is possibly due to high skew in
combination with relatively low number of attempts per student.

When considering the evolution of the knowledge tracing ﬁeld, our evaluation and recent
other evaluations of DLKT models (Gervet et al., 2020; Mandalapu et al., 2021; Pandey et al.,
2021) show that the introduction of DLKT to the ﬁeld has clearly advanced the performance of
knowledge tracing. The major contributing factor seems to be the use of deep learning method-
ologies in general, and recent work that has focused on further exploration of deep learning for
knowledge tracing. Thus, the move from simpler models to deep learning models has shown
robust and veriﬁed improvements in knowledge tracing performance. Multiple more recent ap-
proaches appear promising (Nakagawa et al., 2019; Liu et al., 2019; Ghosh et al., 2020; Choi
et al., 2020; Cheng et al., 2020; Pandey and Srivastava, 2020; Oya and Morishima, 2021; Shin
et al., 2021; Song et al., 2021), and many claim signiﬁcant performance improvements, but their
results still require veriﬁcation via replication studies.

Many of these newer models include slightly different inputs, such as skill and previous
correctness as separate inputs (Choi et al., 2020), additional time related inputs (Shin et al.,
2021) or leveraging both exercise and skill labels (Song et al., 2021), giving rise to the question
of whether and how much the older architectures would also beneﬁt from such input additions.
We agree with this direction of exploring new inputs since DLKT models are powerful models
that are likely to beneﬁt from such additional information. Also, similar input modiﬁcation
can be seen in the move from DKT (Piech et al., 2015) to models that incorporate next skill
id as additional input as in DKVMN (Zhang et al., 2017) and SAKT (Pandey and Karypis,
2019), although this addition does not appear to provide performance boost as evidenced by our
experiments.

As a minor note, in our experiments, SAKT did not live up to the expectations laid out
in the original article where the model was introduced. This is in line with prior results by

24e.g. comparing the present – 2021/12/15 – version of

the ASSISTments

site with https:

//sites.google.com/site/assistmentsdata/home/assistment-2009-2010-data/
skill-builder-data-2009-2010, accessed 2021-04-01

35

Gervet et al. (Gervet et al., 2020) where SAKT also underperformed compared to the original
article (Pandey and Karypis, 2019). However, newer studies that continued on the path started by
SAKT of using self-attentive models have shown promising results: for example, RKT (Pandey
and Srivastava, 2020), AKT (Ghosh et al., 2020) and SAINT+ (Shin et al., 2021).

Creating new models by introducing new types of inputs is a direction that has been wit-
nessed also earlier in the KT ﬁeld. As an example, Performance Factors Analysis (Pavlik Jr
et al., 2009) is an improvement over Learning Factors Analysis (Cen et al., 2006) that includes
student ability as a separate input, achieving better performance. Similarly, there is a wide va-
riety ways how BKT models can be improved by adding new inputs (Khajah et al., 2016), e.g.,
adding item difﬁculty as an input (Pardos and Heffernan, 2011). Effectively, as suggested by
Khajah et al (Khajah et al., 2016), deep learning models can leverage regularities in the data
that prior models cannot without adding such capability through explicitly added input features.
As the deep learning models themselves are capable of performing feature engineering (LeCun
et al., 2015), adding new inputs can provide them an easier starting point for this process.

On the other hand, one of the challenges of the introduction of DLKT models is the de-
crease in interpretability. As deep learning models are complex layered structures – effectively
partially black boxes – the intricacies of the models are not easy to understand (Molnar, 2020).
During training, connections between neurons in the layers are re-weighted and re-evaluated to
optimize the output. This effectively means that there is a vast amount of trainable parameters,
i.e., weights. For example, most of the models evaluated in this study have tens of thousands of
weights that are updated during training. This leads to difﬁculty in for instance interpreting the
effect of individual input features on the model outputs. Possible remedies for this include fea-
ture visualization, concept detection and ﬁnding inﬂuential instances (Molnar, 2020), although
these often require forming hypotheses of what might work and what might not work and they
remain heuristics for feature importance.

In contrast, when considering BKT or GLR, interpreting the results is more straightforward.
For example, manual ﬁne tuning of BKT through introduction of parameters can lead to perfor-
mance comparable with DLKT models (Khajah et al., 2016), in addition to the state of being
able to understand how the parameters are used. Similarly, when using GLR, ranking the im-
portance of the features is straightforward. This has implications as understanding the models
helps us to understand why some models perform better than others, as well as to understand –
for example – contextual factors that contribute to the performance.

Due to this, DLKT models can lead to a disconnect between the use of learning theories and
knowledge tracing. One could even see a link between using deep learning for knowledge tracing
and using machine learning for natural language processing, where one of the famous quotes is
“Every time I ﬁre a linguist, the performance of the speech recognizer goes up” (Frederick
Jelinek in the 1980s).
Key takeaways:
• No single model consistently outperforms all other models in all metrics. In our evaluation
results, DKVMN and LSTM-DKT are most often ranked at the top and have the best
performance on average.

• DLKT models in general come with an increase in model performance, at the cost of
model interpretability. New models seem to emphasize leveraging inputs differently or
adding additional inputs.

• Naive baseline models that are easy to implement and interpret – that is, models with little

36

to no predictive power – can help understand the relative performance of more complex
models.

• A prominent area of improving the performance of DLKT models is introducing different

approaches to processing input and providing new input features.

6.2. METRICS IN REPORTING AND TRAINING - NOT JUST AUC

A multitude of metrics for model evaluation exist and relying on a single metric can easily lead to
misguided judgement of performance (Council et al., 2005). The usefulness of one metric over
another depends on the task at hand, and as Gunawardana et al. (Gunawardana and Shani, 2009)
state, “The decision on the proper evaluation metric is often critical, as each metric may favor
a different algorithm”. For instance, in identifying at-risk students, recall can be considered
preferable over precision, since it can be argued that ﬁnding struggling students is preferable
over ﬁnding non-struggling students. Misidentifying a well-performing student as a struggling
student is not as costly as vice-versa, as the downside is that a well-performing student might be
offered additional support that they do not need. Thus, in this case, tuning precision and recall
in favor of recall might be more favorable than tuning precision and recall in favor of precision
or with equal weights.

In knowledge tracing, we are not predicting dropping a course but rather single attempts at
exercises, which makes the weighing of false positives and false negatives less clear. Although,
without delving deeper into the matter, when considering the case of early intervention to help
struggling students, similarly to retention prediction, it might be better to intervene more often
than not. But the interventions still need to happen accurately enough to keep them valuable in
the minds of learners.

DLKT studies often compare the performance of models using the AUC (ROC-AUC) metric
which is also popular in other domains (e.g. medicine (Kim et al., 2017; Huang et al., 2019)
and natural language processing (Pahikkala et al., 2009)). How AUC compares to accuracy,
another popular metric, has drawn a lot of attention. Some formal and quantitative studies have
shown AUC to be consistent with and more discriminative than accuracy (Ling et al., 2003;
Huang and Ling, 2005; Halimu et al., 2019), and thus it has been claimed as superior. This,
however, does not show that accuracy is worse than AUC, merely that it is less likely than AUC
to show differences between the models. There is no guarantee that better AUC translates to
better model (Jeni et al., 2013; Ozenne et al., 2015; Dhanani et al., 2014).

An often heard critique of accuracy (and also F1 metric), as opposed to e.g. AUC or MCC, is
that high accuracy and F1 score can be a product of skewed data where positive labels outnumber
negative labels. In such a case, the model may only predict positive labels well (Chicco and
Jurman, 2020) and receive seemingly good metric scores, while still performing poorly with the
minority class. AUC solves this issue by effectively accounting for skew in data. However, AUC
with skewed data has also been argued to be a poor combination, since, in AUC, the minority
class has the same impact on the metric score as the majority class (Ferri et al., 2009). Indeed,
there is evidence that AUC may mask poor model performance (Jeni et al., 2013; Ozenne et al.,
2015) and that AUC is less suitable than RMSE, for instance for BKT model evaluation (Dhanani
et al., 2014).

One remedy to problems in AUC could be the visualization of the whole curve instead of
reporting merely the area under it. However, drawing conclusions from the visualized curve
also becomes more difﬁcult at the same time, since comparing curves is not as straightforward

37

as numbers.

We argue that if we provide model accuracy and F1 scores alongside mean prediction with
equally high scores, it easily breaks the illusion that a model with high accuracy is good. Ac-
curacy and F1 scores can be good metrics with little risk of misinterpretation even on highly
skewed datasets when mean or majority vote baseline is presented as comparison.

This leads to a dilemma in choosing the metrics to report and to determine model rankings.
As an example of the metric choice dilemma, in our results for the ASSISTments 2015 dataset,
DLKT models hold the best AUC and MCC scores by a signiﬁcant margin. On the other hand,
both the GLR and BKT models achieve almost the same performance on most other metrics.
Also, the simple mean baseline is not far from the DLKT models in terms of accuracy, and
the mean baseline holds the best F1-score tied with GLR. The DLKT models outperform GLR
and BKT only on MCC and AUC which are the metrics often presented as alternatives to the
“misleading” accuracy and other metrics that can be inﬂuenced by data skew.

In a related study, Effenberger et al. (Effenberger and Pel´anek, 2020) evaluated the metrics
MAE (mean absolute error) and RMSE (root mean squared error) in detail for student modeling.
Similarly to our case, they reported cases where the choice of metric affected model ranking and
also drew attention to the possibility of picking a “suitable” metric for a newly proposed model
to make it appear better in comparison to previous models. They also showed that besides
metric choice, the computation methodology (RMSE over whole data vs average over RMSEs
per student data) of the metric may also affect model ranking.

Choosing a metric is not solely a problem in reporting, but also in hyperparameter tuning.
When considering which metric to use when selecting the best hyperparameters for models via
grid search, we observed that different hyperparameters may be chosen as the best when the
metric used to choose them is changed. Consequently, the model with best hyperparameters
according to one metric may not be optimal when considering other metrics. This problem is
also noted by Sanyal et al. (Sanyal et al., 2020), who inspected how optimizing feature selection
on different metrics inﬂuenced model performance and observed that metric selection can lead
to large performance differences between datasets and selected features.

This further highlights the importance of choosing and understanding metrics for model

comparison.

With scant rigorously studied information on evaluation metrics for knowledge tracing, and
especially for DLKT, based on previous studies and our results, we suggest providing multiple
metrics for evaluation. At least one unaffected by skew (e.g. AUC, MCC) and one affected
by skew (e.g. Accuracy, F1-score) as proposed by (Jeni et al., 2013).
In addition a generic
error metric, such as RMSE, should be included for reliability, which is also suggested in (Liu
et al., 2011). Providing Area Under Precision-Recall Curve (AUC-PC), which unlike AUC,
is affected by skew but similarly to AUC is not affected by a decision threshold, could be a
potential main metric to replace AUC since it has been shown to mask poor performance less
than AUC (Ozenne et al., 2015; Saito and Rehmsmeier, 2015).

Key takeaways:
• Even though AUC is one of the most widely used metrics for evaluating model perfor-

mance, with skewed data, it can mask poor model performance.

• Relying on a single metric in model evaluations can lead to misinformed decisions on
model quality. Metrics that account and do not account for data imbalance should be used
in model evaluations.

38

• The model that receives the best scores on a certain metric does not necessarily achieve
the best results for other metrics. Thus, the metric that is used to determine the best
model or best model hyperparameters is important to choose well, as well as to report for
transparency.

6.3. HYPERPARAMETERS AND ARCHITECTURE VARIATIONS - MIND YOUR RANDOM-

NESS

Overall, hyperparameter tuning had a signiﬁcant impact on model performance. This is to be ex-
pected as hyperparameter tuning heavily affects both overall model complexity (e.g. layer sizes
affect the number of trainable parameters in a model) and model training itself (e.g. learning
rate). Even though we explored a relatively small hyperparameter space (2 options per hy-
perparameter, as shown in Table 5), the differences between the worst and best hyperparameter
combinations can be over 20% points (see e.g. SAKT in Table 25). The relative impact of hyper-
parameter tuning on the model performance depended on the model, as some models were less
susceptible to their hyperparameters (see e.g. Tables 23-25). For instance, DKVMN is relatively
stable over the hyperparameter combinations, while e.g., Vanilla-DKT is much less so.

In our exploration of the effect of different input (one-hotting vs. use of embedding layer)
and output (skill summary layer and skill-to-scalar output vs. output per skill) variations for
DLKT models, we identify two main ﬁndings. First, while for many datasets and models there
are mostly no differences in performance when using one-hot embedding when compared to
using embedding layers. There are some exceptions, where using embedding layers consider-
ably outperforms one-hotting up to 4.6% point difference (for SAKT in the ASSISTments 2009
Updated dataset in Table 26). This leaves little reason to one-hot inputs as opposed to using
embedding layers, especially as one-hotting can cause memory problems when training models
if the one-hot embeddings become large due to high number of skills in data. On the other hand,
while embedding layers appear to be the go-to choice when seeking maximal performance, us-
ing one-hot inputs may be a safer choice as they seem to be more robust regarding bad choice
of hyperparameters in some datasets. Second, when considering output variations, using the
more recent output version (skill layer and skills-to-scalar output layers) showed more variance
in model performance compared to the output per skill layer used in the ﬁrst DLKT model DKT.
We did not ﬁnd indications that one would consistently be better than the other.

We also considered the impact of a maximum attempt count that has been incorporated in
both DKVMN and SAKT (although the DKVMN article does not mention this). Both DKVMN
and SAKT split student attempt sequence by a maximum attempt count, but neither article dis-
cussed this to an extent or analyzed the effect of the approach. To better understand how using a
maximum attempt count affects KT model performance, we analyzed the effect of using maxi-
mum attempt count to split the student assignment sequences into smaller chunks thus artiﬁcially
increasing the number of total students. This analysis was conducted by re-training our models
using the best hyperparameters according to our grid search tuning with an additional hyper-
parameter: maximum attempt count ﬁlter method (none vs cut vs split). None indicates that
no maximum attempt count was applied, cut indicates that the attempts after the maximum at-
tempt count were discarded, and split effectively divides sequences longer than the maximum
attempt count into multiple sequences that have at most maximum attempt count as their length.
When testing the effect of the maximum attempt count, we used 200 and 500 as the choices for
maximum attempt count.

39

Overall, as shown in Appendix C, we found signiﬁcant differences (e.g. up to 3.1% points
AUC) in model performances depending on the ﬁlter method and some differences between the
maximum attempt count 200 and 500, although not in all models in all datasets. For instance in
the Statics dataset, no model is unaffected by the additional hyperparameter tuning. The Syn-
thetic datasets were not inﬂuenced by the ﬁlter as the attempt sequences are shorter than 200.
These results showcase that maximum attempt count should not be overlooked when tuning hy-
perparameters and comparing knowledge tracing models. This is further emphasized by SAKT
and Vanilla-DKT appearing to beneﬁt from applying the maximum attempt count (split or cut)
in comparison to other models. The other models perform better with no maximum attempt
count while SAKT and Vanilla-DKT are on average much less affected. In other words, tuning
maximum attempt count can bias evaluation in favor of certain models and this is something that
should be accounted for and discussed. We do not however, suggest that using maximum attempt
count should be completely discouraged as it does speed up model training and reduce model
space requirements, although there appears to be little other beneﬁt according to our evaluation.
To quantify the effect of variation in performance due to properties not related to models,
we also explored the effect of random seed and hardware (CPU vs GPU). First, we found that
the values used as random seeds have very slight effects on model performance (see Table 27),
although the effects are considerable on poor choice of hyperparameters. This indicates that the
danger of randomness affecting model performance is present but mitigated by a large hyperpa-
rameter space for tuning. Second, we found mostly little to no differences in model performance
depending on the hardware that was used to run the model (CPU or GPU). Some differences are
larger, however. The biggest is 0.6% points difference for MCC and F1-Score, while the biggest
difference in AUC is merely 0.3% points. On the other hand, we observed a tad more and larger
differences between TensorFlow versions, with the largest being 1.7% points in F1-Score and
the largest AUC score difference is 0.6% points. Thus, although unlikely based on our results, it
is possible to have large performance difference when tuning hyperparameters on one machine
learning framework version and then using the hyperparameters on another version another.
Consequently, one should not take granted that the tuned hyperparameters work the same in
another setting.

Even though the performance differences between the best models are usually negligible
(e.g. 0.1% points AUC) when considering one of these non-model properties, the combination
of the effects of hardware, seed, and hyperparameter tuning could easily change the ranking
of the best performing models. This suggests that minor improvements in performance could
be due to random chance, and thus to regard some model as the new “state-of-the-art”, the
improvement in e.g. AUC scores should be considerable and consistent across multiple datasets.
Based on our results, we would be careful to consider even a 1% point increase in performance
as a true improvement in terms of model architecture unless such an improvement was repeated
in multiple studies and contexts.

Key takeaways:
• Hyperparameter tuning has a signiﬁcant impact on model performance. Our results indi-

cate that optimal hyperparameters are model- and dataset-speciﬁc.

• Input and output variations, which we used as hyperparameters but which could be pre-

and post-processing steps, also impact model performance.

• Maximum attempt count ﬁltering strategy inﬂuences model performance, and its effect

depends on used model and dataset.

40

• Randomness (e.g. random seed, hardware) and machine learning framework version can
affect model performance, although the observed impact in our evaluations is mostly very
slight.

6.4. DATA - NO ONE MODEL TO RULE THEM ALL

Overall, some models appear to be more capable of beneﬁting from big data than others, and
the dataset and its underlying distributions has a clear impact on model performance. Data
size is certainly a factor in DLKT model generalizability across datasets due to deep learning
models’ symbiotic relation with big data that follows from the models’ tendency to overﬁt. In
our results, we noticed no clear pattern with data size affecting model performance, and no
model outperformed all the other models in all datasets. GLR and BKT fare relatively well
compared to DLKT models in the ASSISTments 2015 dataset with the most students (19917)
and the second most attempts (683k) but poorly in the ASSISTments 2017 dataset with the most
attempts (943k). Also the SAKT model performs on par with the other DLKT models only in
the IntroProg dataset which has the least attempts (172k) our datasets.

Other studies have come to different conclusions, which highlights that the impact of the data
is still an open question. For example, Mandalapu et al. (Mandalapu et al., 2021) note that SAKT
In the same study though, a non-deep-
performs better than LSTM-DKT in larger datasets.
learning logistic regression model beat both SAKT and LSTM-DKT even when supposedly
deep learning excels on large data. Apart from data quantity, the underlying distribution of the
data has a great inﬂuence on the performance of the models. Gervet et al. (Gervet et al., 2020)
suggest that the number of learners per learning item or knowledge component (KC) is more
important than the total number of interactions, and that some models beneﬁt more from large
amount of training data than others.

Similar to other studies, we also used synthetic (Synthetic-K5 and Synthetic-K2) datasets
in our study. One interesting phenomenon we noticed (see Figure 6) is that the AUCs for the
synthetic datasets seem to be more stable between different deep knowledge tracing models,
i.e. the differences in AUC and other metrics are smaller, but still large compared to logistic
regression and other baselines.

Our present work followed the methodology most commonly used in knowledge tracing
studies where data from all students is used for training the models (accounting for train/test
validation splits etc.). Some prior work has however suggested that one way to improve model
performance would be to use a subset of the available data (Faraway and Augustin, 2018) – for
example only a part of the students – to train the models, since some students seem to produce
higher quality data than others (Alexandron et al., 2019; Yudelson et al., 2014), which can lead
to model performance improvement over all students (Yudelson et al., 2014). This topic should
also be further explored.

On a more general level, evaluations of knowledge tracing models typically rely on individ-
ual datasets. There are options in the machine learning domain that could potentially be used
to better beneﬁt from existing data; as an example, one could beneﬁt from the use of domain
adaptation and transfer learning techniques that have been successfully used in, for example,
natural language processing (Zhuang et al., 2020). Some work already exists in adapting these
methodologies for knowledge tracing (Cheng et al., 2020). However, we envision pre-trained
knowledge tracing models similar to GPT-3 (Brown et al., 2020) that could then be ﬁne-tuned
with context-speciﬁc data.

41

Key takeaways:
• No single model outperformed all other models on all datasets.
• Even the best DLKT models did not always yield superior performance compared to other
models in all datasets. The best model and also model type (DLKT vs non-DLKT) is
dependent on data.

• When considering the adoption of knowledge tracing, to ensure the best ﬁt for a speciﬁc
context, evaluate multiple models in that context instead of choosing the most recent state-
of-the-art model.

6.5. REPLICATION PROCESS AND FINDINGS - THE DEVIL IS IN THE DETAILS

One of the key aspects of science is providing sufﬁcient details of the used methodology that
allows tracing the steps of the researchers to conduct similar research and to improve on it.
Replication studies can be conducted in different ways (Ihantola et al., 2015; Patil et al., 2016),
where one is seeking to reproduce earlier ﬁndings with the same data and the same method-
ology. In such a case, the objective is to examine the methodology to determine whether the
steps are explained clearly enough and to explore whether there are considerations that were
omitted from the original study. Earlier studies have suggested, however, that keeping the same
data and methodology, but changing the researchers, can already lead to challenges with repli-
cation (Ihantola et al., 2015). Another approach to replication would be seeking to replicate the
effect found in the original study with new data and possibly new or improved methodologies.
In this case, the objective could be to study whether the data has an impact on the effect, and
whether the effect generalizes beyond the original context.

To summarize our ﬁndings from the replication process, we found multiple issues, which are
as follows. First, we observed differences between a model description in a published article
and an associated code repository. Second, we observed methodological differences between
a published article and an associated code repository. Third, we identiﬁed differences in data
set sizes between articles, even though the data sets have been labeled the same and thus also
likely understood to be the same. Finally, fourth, in some cases, we were unable to reach similar
performance as reported in the original articles.

There are naturally a multitude of explanations for our ﬁndings. As an example, for the ﬁrst
and the second case, it is possible that the code that authors have added to their repository is an
earlier version of their work, and does not represent the version reported in the article. This situ-
ation can be problematic however, as others may directly rely on the available implementations,
instead of reimplementing the work based on the article. For the third case, it is possible that
there are differences in data preprocessing steps that are not sometimes fully explained in the
articles. We did, however, also observe a case where it is likely that data sizes were originally
mistyped in an article and then copy-pasted to another article by different authors. For the fourth
case, it is possible that some methodological steps have been omitted in the article, which could
lead to better results. In this case, however, others have also struggled to replicate the earlier
performance.

These ﬁndings follow the trend visible also in other ﬁelds, where researchers have identiﬁed
problems with replicating results from prior published studies (Baker, 2016; Collaboration et al.,
2015; Ioannidis, 2005b; Moonesinghe et al., 2007).

There are a lot of positive signals as well. As an example, authors often had placed their
research code and used data in repositories for inspection, and there are multiple open datasets

42

that can be used to evaluate model performance. Some authors also reported the hyperparameter
variations and preprocessing steps they had used when training the models, and the articles
often included evaluations of proposed models against other recently proposed models over
evaluating only against simpler baselines that are easy to outperform. One commendable work
in this line of work is centralized algorithm and data repositories such as DataShop (Koedinger
et al., 2010), which help reproduction, although considering replication, using the same data
and methodology can disallow further examination of implementation details, which may lead
to overlooking issues in implementation.

Our replication results show that old models can easily surpass newer ones and vice versa
given the right hyperparameters. When introducing a new model that outperforms others, it
is important to report the used hyperparameter options for both the new and the old models,
and to verify that the better results are not due to more rigorous hyperparameter tuning for
the introduced model. Similarly, data preprocessing decisions such as splitting student data
based on maximum attempt count or omitting data by minimum attempt count can impact model
performance, and even non-model speciﬁc properties such as hardware has an inﬂuence over the
results.

Examples of the difﬁculty in replicating prior work can be easily found by looking at the
reported performance of models. Some papers agree on the results of some models on certain
datasets and disagree on the results of others. The degree of variance in observed performances is
also relatively high, although the biggest differences could be explainable by the use of different
versions of data.

In order to replicate machine learning work well, as much information as possible about the
original work is beneﬁcial, which is why we would like to emphasize the importance of source
code accessibility. When the source code for a work is easy to ﬁnd and analyze, many aspects of
a machine learning model can be viewed that are often missing from a scientiﬁc paper. Whether
missing pieces of implementation details are due to lack of rigor, estimated importance or a
limitation of the publishing platform (e.g. paper length), they might include key components
that explain why one model is better than the other.

This raises an interesting point about replicating machine learning work in general. Different
types of expertise are needed to effectively work in the ML domain. On one hand, one needs
to have sufﬁcient mathematical skills to understand mathematical notations which are the most
common way to communicate new ML models in academic publications. On the other hand, also
good skills are required in programming and the frameworks being used in order to understand
the source code of the models when those models are available openly.
In most works, the
mathematical notation of the model present in the publication hopefully matches the source
code of the model. When this is not the case, it is not evident whether the researchers replicating
the work should follow the mathematical notation (which likely includes the novel aspects of the
model) or the source code (which likely was used to compute the results presented in the article).
It is a good question whether conference organizers and journal editors should require reviewers
to also review the source code and evaluate whether it matches the mathematical notation in the
publication. We acknowledge that this can be a highly time-consuming process, however.

Key takeaways:
• In our study, we found multiple discrepancies in algorithm and data descriptions, which

have the potential to inﬂuence study outcomes.

• Publishing source code and data is a good practice that should be continued. Preferably,
links to code and data should be included in articles for ease of access and transparency.

43

Further, we recommend linking to a speciﬁc version of code and data to verify that the
linked code is indeed the deﬁnitive version used in the study and to allow for further
improvements while keeping the connection to the original article.

• When publishing studies, include methodological details that allow replication, including
data preprocessing steps, evaluated hyperparameter options, and other speciﬁcs of model
training.

• When presenting a new state-of-the-art model, other evaluated models should be evalu-
ated with the same rigor (e.g. data preprocessing, hyperparameter tuning) as the proposed
model to clarify that improvements stem from the proposed model algorithm and not from
other factors.

6.6. LIMITATIONS OF WORK

Here, we summarize the key limitations of this work. First, we acknowledge that there are
a wide variety of knowledge tracing models, including newer ones, that were not included in
the empirical evaluation. Multiple of these are brieﬂy discussed in section 2.2.3. Limiting the
number of evaluated models was a deliberate choice, as we meticulously reimplemented the
algorithms as well as compared and contrasted the available implementations with the details
outlined in the respective articles.

Second, while we implemented most of the models compared in this study, for the baseline
Bayesian Knowledge Tracing we used Yudelson’s implementation that is available on GitHub25.
Similarly, for the logistic regression, we used the Best-LR (GLR) model with slight modiﬁca-
tions by Gervet et al. (Gervet et al., 2020). Their model, too, is available on GitHub26. Thus,
our results related to BKT and GLR are not full replications (where the model would be reim-
plemented from scratch). We decided not to reimplement the models as our focus in this work is
on scrutinizing DLKT models, furthermore BKT and GLR were only used as baselines to which
we can compare the DLKT models.

Third, we have used seven datasets in this study, six publicly available ones and one novel
dataset (IntroProg) which is published alongside this work. Thus, our results are only applicable
to these datasets, and do not necessarily extend to other datasets. As an example, many recent
studies have included the EdNet dataset (Choi et al., 2020) in their evaluations, including the
studies by Mandalapu et al. (Mandalapu et al., 2021) and Pandey et al. (Pandey et al., 2021).
When compared to the datasets used in this study, the EdNet dataset is larger and thus may
allow the models to utilize information in a way that is not possible in the present datasets.

Fourth, we acknowledge that our explored hyperparameter space is not very extensive, and
it is possible that further tuning could have realized better gains. All in all, in our study, there
are between 72 and 240 hyperparameter variations per model, and in total, we evaluated 5,880
cross-validated DLKT models. With the resources available to us, signiﬁcantly extending the
explored hyperparameter space would have led to far longer model training time.

Fifth, although we explored a range of hyperparameters, we did not look into item-aware
input and output, which also could inﬂuence the results. For an exploration of this aspect of
knowledge tracing models, see e.g. studies by Gervet et al. (Gervet et al., 2020) and Vie and
Kashima (Vie and Kashima, 2019).

25https://github.com/myudelson/hmm-scalable, accessed 2020-03-01
26https://github.com/theophilee/learner-performance-prediction, accessed 2021-05-

27

44

Finally, we acknowledge that when selecting best hyperparameter values for models, we
selected them based on AUC, similarly as is done in e.g. (Zhang et al., 2017). As discussed in
Section 5.3, using another metric to select the best models would have led to slightly different
results. We decided against extensive exploration of selection metric when reporting results,
as it would have led to more complex reporting (effectively multiplying the reported results by
the number of metrics), and as AUC is commonly used as a principal metric in the knowledge
tracing community.

7. CONCLUSION

In this article, we reviewed models for knowledge tracing, evaluating the performance of eleven
models. We evaluated deep learning knowledge tracing (DLKT) models (Vanilla-DKT, LSTM-
DKT, LSTM-DKT-S+, DKVMN, DKVMN-Paper, SAKT) and baseline models (Mean, Next as
Previous, Next as Previous N’s Mean, BKT, GLR). Out of these, LSTM-DKT-S+ is our own
variant of LSTM-DKT, which takes next skills into account as inputs. For the DKVMN, the
two versions are presented as the version in the article and the version in the repository differed
from each other. See Table 1 for details of the models. All models were evaluated against
seven datasets using seven metrics. For deep learning knowledge tracing models, we evaluated
the impact of input and output variations (one-hot embedding versus embedding layer; output-
per-skill layer versus skill layer and skills-to-scalar output layers), and also maximum attempt
count handling on model performance. The effect of non-model properties such as hardware and
random seed were examined as a baseline to which model improvements could be compared to.
The deep learning knowledge tracing models were reimplemented for this study.

The motivation of our study was ﬁve-fold. First, to re-evaluate the proposed models of earlier
studies by reimplementing them and comparing them to each other and to simple baselines. Sec-
ond, to highlight how the choice of metric used in reporting affect perceived model performance
and hyperparameter tuning. Third, to show how hyperparameters and variations in model input
and output architecture can have an effect on models’ performance. Fourth, to explore model
performance across different contexts (datasets). Fifth, to emphasize the importance of replica-
tion studies and give pointers on how to make such studies easier, as well as to give pointers
to help make results of future KT studies more robust. We also publish our implementations,
datasets and evaluation code for use in future research.

To summarize, our research questions and their answers are as follows:
RQ1 How do DLKT models compare to naive baselines and non deep-learning KT mod-
els? Answer: The evaluated DLKT models generally outperform the baseline models. DLKT
models in general outperform the non-DLKT baselines BKT and the logistic regression model
GLR. GLR does, however, achieve performance on par with some of the DLKT models on two
datasets and is the best performing model on one dataset. BKT is on par with the DLKT mod-
els and GLR on one dataset, in all metrics but AUC and MCC, but BKT fares worse on other
datasets. Naive baselines mostly perform poorly when compared to the more complex models,
but on one dataset, the accuracy of the best DLKT models is not much better than the Mean
model, and the Mean model also achieves better F1-Score than the DLKT models. This high-
lights both the performance of the DLKT models as well as the importance of including naive
baseline models into KT model comparisons to verify usefulness of models.

RQ2 How do DLKT models perform on the same and different datasets as originally eval-
uated with? Answer: In our evaluations, LSTM-DKT, LSTM-DKT-S+ and DKVMN had the

45

best performance on average out of the evaluated models, but the differences in performance of
any of the DLKT models were not great. We found that the relative performance of the models
depended on the context, i.e. the dataset. We did not ﬁnd differences in performance between
LSTM-DKT and our variant with additional next skill input LSTM-DKT-S+, but we found that
the DKVMN version implemented based on the DKVMN authors’ repository performed on av-
erage slightly better than the DKVMN-Paper version, which is the version introduced in the
article. We found considerable variance between previously reported results for our evaluated
models, especially for SAKT, which is in line with other prior comparison studies. When com-
paring the results over different metrics, we observed that the ranking of the models can differ
depending on the inspected metric, which highlights the importance of reporting multiple met-
rics for model evaluation. We also analyzed the effect of using different metrics for hyperpa-
rameter tuning and found the same pattern there; the model which receives the best score on one
metric does not always receive the best score on another metric.

RQ3 What is the impact of variations in architecture and hyperparameters on DLKT mod-
els’ performance? Answer: Overall, the impact of variations in hyperparameters contributes
signiﬁcantly to model performance. The extent to which model performance depends on hy-
perparameters could be seen as a quality factor of the models, where more robust models are
less dependent on extensive hyperparameter tuning. Explored variations in model architecture,
one hot vs embedding layer for input, and output per skill layer vs skill layer and skills-to-
scalar output layers, showed some variation in performance, up to 4.6% point AUC. Non-model
properties, i.e. hardware, machine learning framework version, and random seed, had mostly
negligible impact on model performance. Although, we found some cases where machine learn-
ing framework version and the hardware led to over 0.5% point difference in results in various
metrics. Maximum attempt count ﬁltering (no ﬁltering, cut, split – split has been used implicitly
and explicitly in some previous work) had also a noticeable impact in model performance, and
should be taken into account and reported if used when training models.

As a part of the work, we reimplemented the deep learning knowledge tracing algorithms fol-
lowing the details presented in the respective articles. During the implementation, we observed
inconsistencies between algorithm descriptions and implementations, as well as inconsistencies
between the reported dataset sizes. As an example, the architecture of the DKVMN differed
between the article and the implementation, and the ASSISTments 2015 dataset had nearly 10%
difference in size between the articles likely due to varying preprocessing steps.

Furthermore, we highlight the need to identify the sources for empirical gains, which has
been pointed out to be a concern within the machine learning discipline (Lipton and Steinhardt,
2018): in our evaluations, we observed that some of the previous ﬁndings reported in the litera-
ture may have more to do with hyperparameter tuning than proposed neural network structures.
We call out others to also perform similar studies where KT models are evaluated through
replication and reimplementation of the models, where the preprocessing of the data and any
possible hyperparameter tuning is explicitly stated and performed with the same level of rigor
for all compared models.

ACKNOWLEDGEMENTS

We acknowledge the computational resources provided by the Aalto Science-IT project. We
are grateful for the grant by the Media Industry Research Foundation of Finland which partially
funded this work. We thank the reviewers for their valuable comments that helped improved this

46

manuscript.

REFERENCES

ABDELRAHMAN, G. AND WANG, Q. 2019. Knowledge tracing with sequential key-value memory net-
works. In Proceedings of the 42nd International ACM SIGIR Conference on Research and Develop-
ment in Information Retrieval - SIGIR’19. ACM Press, New York, New York, USA, 175–184.

AHADI, A., HELLAS, A., IHANTOLA, P., KORHONEN, A., AND PETERSEN, A. 2016. Replication in
computing education research: Researcher attitudes and experiences. In Proceedings of the 16th Koli
Calling International Conference on Computing Education Research. Koli Calling ’16. Association
for Computing Machinery, New York, NY, USA, 2–11.

ALEXANDRON, G., YOO, L. Y., RUIP ´EREZ-VALIENTE, J. A., LEE, S., AND PRITCHARD, D. E. 2019.
Are MOOC learning analytics results trustworthy? with fake learners, they might not be! Interna-
tional journal of artiﬁcial intelligence in education 29, 4, 484–506.

ANDERSON, C. J., BAHN´IK, ˇS., BARNETT-COWAN, M., BOSCO, F. A., CHANDLER, J., CHARTIER,
C. R., CHEUNG, F., CHRISTOPHERSON, C. D., CORDES, A., CREMATA, E. J., ET AL. 2016. Re-
sponse to comment on “estimating the reproducibility of psychological science”. Science 351, 6277,
1037–1037.

ANDERSON, J. R., BOYLE, C. F., AND REISER, B. J. 1985. Intelligent tutoring systems. Sci-

ence 228, 4698, 456–462.

ASENDORPF, J. B., CONNER, M., DE FRUYT, F., DE HOUWER, J., DENISSEN, J. J., FIEDLER, K.,
FIEDLER, S., FUNDER, D. C., KLIEGL, R., NOSEK, B. A., ET AL. 2013. Recommendations for
increasing replicability in psychology. European Journal of Personality 27, 2, 108–119.

BAKER, M. 2016. Is there a reproducibility crisis? Nature 533, 452–454.

BAKER, R. S., CORBETT, A. T., AND ALEVEN, V. 2008. More accurate student modeling through
contextual estimation of slip and guess probabilities in bayesian knowledge tracing. In International
conference on intelligent tutoring systems. Springer, 406–415.

BEGLEY, C. G. AND ELLIS, L. M. 2012. Drug development: Raise standards for preclinical cancer

research. Nature 483, 7391, 531–533.

BHANDARI NEUPANE, J., NEUPANE, R. P., LUO, Y., YOSHIDA, W. Y., SUN, R., AND WILLIAMS,
P. G. 2019. Characterization of leptazolines a–d, polar oxazolines from the cyanobacterium leptolyn-
gbya sp., reveals a glitch with the “willoughby–hoye” scripts for calculating nmr chemical shifts.
Organic letters 21, 20, 8449–8453.

BIANCHINI, M. AND SCARSELLI, F. 2014. On the complexity of neural network classiﬁers: A compar-
ison between shallow and deep architectures. IEEE Transactions on Neural Networks and Learning
Systems 25, 8, 1553–1565.

BOUTHILLIER, X., DELAUNAY, P., BRONZI, M., TROFIMOV, A., NICHYPORUK, B., SZETO, J., MO-
HAMMADI SEPAHVAND, N., RAFF, E., MADAN, K., VOLETI, V., ET AL. 2021. Accounting for
variance in machine learning benchmarks. Proceedings of Machine Learning and Systems 3.

BROWN, T. B., MANN, B., RYDER, N., SUBBIAH, M., KAPLAN, J., DHARIWAL, P., NEELAKANTAN,
A., SHYAM, P., SASTRY, G., ASKELL, A., ET AL. 2020. Language models are few-shot learners.
arXiv preprint arXiv:2005.14165.

CARUANA, R. AND NICULESCU-MIZIL, A. 2004. Data mining in metric space: an empirical analysis
of supervised learning performance criteria. In Proceedings of the tenth ACM SIGKDD international
conference on Knowledge discovery and data mining. 69–78.

47

CEN, H., KOEDINGER, K., AND JUNKER, B. 2006. Learning factors analysis–a general method for
cognitive model evaluation and improvement. In International Conference on Intelligent Tutoring
Systems. 164–175.

CHANG, K.-M., BECK, J. E., MOSTOW, J., AND CORBETT, A. 2006. Does help help? a Bayes net
approach to modeling tutor interventions. In AAAI2006 Workshop on Educational Data Mining.

CHENG, S., LIU, Q., AND CHEN, E. 2020. Domain adaption for knowledge tracing. arXiv preprint

arXiv:2001.04841.

CHICCO, D. AND JURMAN, G. 2020. The advantages of the Matthews correlation coefﬁcient (MCC)

over F1 score and accuracy in binary classiﬁcation evaluation. BMC genomics 21, 1, 1–13.

CHOFFIN, B., POPINEAU, F., BOURDA, Y., AND VIE, J.-J. 2019. Das3h: modeling student learning and
forgetting for optimally scheduling distributed practice of skills. arXiv preprint arXiv:1905.06873.

CHOI, Y., LEE, Y., CHO, J., BAEK, J., KIM, B., CHA, Y., SHIN, D., BAE, C., AND HEO, J. 2020.
Towards an appropriate query, key, and value computation for knowledge tracing. In Proceedings of
the Seventh ACM Conference on Learning@ Scale. 341–344.

CHOI, Y., LEE, Y., SHIN, D., CHO, J., PARK, S., LEE, S., BAEK, J., BAE, C., KIM, B., AND HEO, J.
2020. EdNet: A large-scale hierarchical dataset in education. In International Conference on Artiﬁcial
Intelligence in Education. Springer, 69–73.

CHRYSAFIADI, K. AND VIRVOU, M. 2013. Student modeling approaches: A literature review for the

last decade. Expert Systems with Applications 40, 11, 4715–4729.

COLLABORATION, O. S. ET AL. 2015. Estimating the reproducibility of psychological science. Sci-

ence 349, 6251, aac4716.

CORBETT, A. T. AND ANDERSON, J. R. 1994. Knowledge tracing: Modeling the acquisition of proce-

dural knowledge. User modeling and user-adapted interaction 4, 4, 253–278.

CORBETT, A. T., KOEDINGER, K. R., AND ANDERSON, J. R. 1997. Intelligent tutoring systems. In

Handbook of human-computer interaction. Elsevier, 849–874.

COUNCIL, N. R., COMMITTEE, C. R., ET AL. 2005. Chapter 3: Principles for developing metrics.
In Thinking strategically: the appropriate use of metrics for the climate change science program.
National Academies Press.

DEVASENA, C. L., SUMATHI, T., GOMATHI, V., AND HEMALATHA, M. 2011. Effectiveness evaluation
of rule based classiﬁers for the classiﬁcation of iris data set. Bonfring International Journal of Man
Machine Interface 1, Special Issue Inaugural Special Issue, 05–09.

DHANANI, A., LEE, S. Y., PHOTHILIMTHANA, P. M., AND PARDOS, Z. 2014. A comparison of error
metrics for learning model parameters in bayesian knowledge tracing. In Workshop Approaching
Twenty Years of Knowledge Tracing (BKT20y). Citeseer. 8–9.

DING, X. AND LARSON, E. C. 2019. Why deep knowledge tracing has less depth than anticipated.

DOZAT, T. 2016. Incorporating nesterov momentum into adam.

EFFENBERGER, T. AND PEL ´ANEK, R. 2020. Impact of methodological choices on the evaluation of
student models. In International Conference on Artiﬁcial Intelligence in Education. Springer, 153–
164.

FANELLI, D. 2011. Negative results are disappearing from most disciplines and countries. Scientomet-

rics 90, 3, 891–904.

FARAWAY, J. J. AND AUGUSTIN, N. H. 2018. When small data beats big data. Statistics & Probability

Letters 136, 142–145.

48

FAWCETT, T. 2006. An introduction to ROC analysis. Pattern recognition letters 27, 8, 861–874.

FENG, M., HEFFERNAN, N., AND KOEDINGER, K. 2009. Addressing the assessment challenge with an
online system that tutors as it assesses. User Modeling and User-Adapted Interaction 19, 3, 243–266.
FERRI, C., HERN ´ANDEZ-ORALLO, J., AND MODROIU, R. 2009. An experimental comparison of per-

formance measures for classiﬁcation. Pattern Recognition Letters 30, 1, 27–38.

GAL, Y. AND GHAHRAMANI, Z. 2016. A theoretically grounded application of dropout in recurrent

neural networks. In Advances in neural information processing systems. 1019–1027.

GARDNER, J., YANG, Y., BAKER, R. S., AND BROOKS, C. 2019. Modeling and experimental design
for mooc dropout prediction: A replication perspective. In Proceedings of The 12th International
Conference on Educational Data Mining (EDM 2019). ERIC.

GERS, F. A. AND SCHMIDHUBER, E. 2001. Lstm recurrent networks learn simple context-free and

context-sensitive languages. IEEE Transactions on Neural Networks 12, 6, 1333–1340.

GERVET, T., KOEDINGER, K., SCHNEIDER, J., MITCHELL, T., ET AL. 2020. When is deep learning the
best approach to knowledge tracing? JEDM— Journal of Educational Data Mining 12, 3, 31–54.

GHAHRAMANI, Z. 1997. Learning dynamic bayesian networks. In International School on Neural Net-

works, Initiated by IIASS and EMFCSC. Springer, 168–197.

GHOSH, A., HEFFERNAN, N., AND LAN, A. S. 2020. Context-aware attentive knowledge tracing. In
Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery & Data
Mining. 2330–2339.

GILBERT, D. T., KING, G., PETTIGREW, S., AND WILSON, T. 2016. Comment on “estimating the

reproducibility of psychological science”. Science 351, 6277, 1037.

GOLDEN, M. A. 1995. Replication and non-quantitative research. PS: Political Science & Politics 28, 03,

481–483.

GONG, Y., BECK, J. E., AND HEFFERNAN, N. T. 2010. Comparing knowledge tracing and performance
factor analysis by using multiple model ﬁtting procedures. In International conference on intelligent
tutoring systems. Springer, 35–44.

GONZ ´ALEZ-BRENES, J., HUANG, Y., AND BRUSILOVSKY, P. 2014. General features in knowledge
tracing to model multiple subskills, temporal item response theory, and expert knowledge. In The 7th
International Conference on Educational Data Mining. University of Pittsburgh, 84–91.

GRAVES, A. 2013. Generating sequences with recurrent neural networks. arXiv preprint

arXiv:1308.0850.

GRAVES, A., WAYNE, G., AND DANIHELKA, I. 2014. Neural turing machines. arXiv preprint

arXiv:1410.5401.

GUNAWARDANA, A. AND SHANI, G. 2009. A survey of accuracy evaluation metrics of recommendation

tasks. Journal of Machine Learning Research 10, 12.

HALIMU, C., KASEM, A., AND NEWAZ, S. S. 2019. Empirical comparison of area under ROC curve
(AUC) and mathew correlation coefﬁcient (mcc) for evaluating machine learning algorithms on im-
balanced datasets for binary classiﬁcation. In Proceedings of the 3rd international conference on
machine learning and soft computing. 1–6.

HAMBLETON, R. K. AND SWAMINATHAN, H. 1985. Item response theory: Principles and applications.

Springer.

HEFFERNAN, N. T. AND HEFFERNAN, C. L. 2014. The assistments ecosystem: Building a platform
that brings scientists and teachers together for minimally invasive research on human learning and
teaching. International Journal of Artiﬁcial Intelligence in Education 24, 4, 470–497.

49

HEFFERNAN, N. T., TURNER, T. E., LOURENCO, A. L., MACASEK, M. A., NUZZO-JONES, G., AND
KOEDINGER, K. R. 2006. The assistment builder: Towards an analysis of cost effectiveness of its
creation. In Flairs Conference. 515–520.

HOCHREITER, S. AND SCHMIDHUBER, J. 1997a. Long short-term memory. Neural computation 9, 8,

1735–1780.

HOCHREITER, S. AND SCHMIDHUBER, J. 1997b. LSTM can solve hard long time lag problems. In

Advances in neural information processing systems. 473–479.

HUANG, J. AND LING, C. X. 2005. Using AUC and accuracy in evaluating learning algorithms. IEEE

Transactions on knowledge and Data Engineering 17, 3, 299–310.

HUANG, L., SHEA, A. L., QIAN, H., MASURKAR, A., DENG, H., AND LIU, D. 2019. Patient clustering
improves efﬁciency of federated machine learning to predict mortality and hospital stay time using
distributed electronic medical records. Journal of biomedical informatics 99, 103291.

IHANTOLA, P., VIHAVAINEN, A., AHADI, A., BUTLER, M., B ¨ORSTLER, J., EDWARDS, S. H., ISO-
HANNI, E., KORHONEN, A., PETERSEN, A., RIVERS, K., RUBIO, M. A., SHEARD, J., SKUPAS,
B., SPACCO, J., SZABO, C., AND TOLL, D. 2015. Educational data mining and learning analytics
in programming: Literature review and case studies. In Proceedings of the 2015 ITiCSE on Working
Group Reports. ITICSE-WGR ’15. ACM, New York, NY, USA, 41–63.

IOANNIDIS, J. P. 2005a. Contradicted and initially stronger effects in highly cited clinical research.

Jama 294, 2, 218–228.

IOANNIDIS, J. P. 2005b. Why most published research ﬁndings are false. PLoS Med 2, 8, e124.

IOANNIDIS, J. P., MUNAFO, M. R., FUSAR-POLI, P., NOSEK, B. A., AND DAVID, S. P. 2014. Publi-
cation and other reporting biases in cognitive sciences: detection, prevalence, and prevention. Trends
in Cognitive Sciences 18, 5, 235–241.

JENI, L. A., COHN, J. F., AND DE LA TORRE, F. 2013. Facing imbalanced data–recommendations for
the use of performance metrics. In 2013 Humaine association conference on affective computing and
intelligent interaction. IEEE, 245–251.

JOHNS, J., MAHADEVAN, S., AND WOOLF, B. 2006. Estimating student proﬁciency using an item
response theory model. In International conference on intelligent tutoring systems. Springer, 473–
480.

KAMIJO, K. AND TANIGAWA, T. 1990. Stock price pattern recognition-a recurrent neural network ap-

proach. In 1990 IJCNN International Joint Conference on Neural Networks. 215–221 vol.1.

K ¨ASER, T., KLINGLER, S., SCHWING, A. G., AND GROSS, M. 2014. Beyond knowledge tracing:
Modeling skill topologies with bayesian networks. In International conference on intelligent tutoring
systems. Springer, 188–198.

KHAJAH, M., LINDSEY, R. V., AND MOZER, M. C. 2016. How deep is knowledge tracing? In Proceed-

ings of the 9th International Conference on Educational Data Mining (EDM 2016). ERIC.

KHAJAH, M., WING, R., LINDSEY, R., AND MOZER, M. 2014. Integrating latent-factor and knowledge-
tracing models to predict individual differences in learning. In Educational Data Mining 2014. Cite-
seer.

KHAJAH, M. M., HUANG, Y., GONZ ´ALEZ-BRENES, J. P., MOZER, M. C., AND BRUSILOVSKY, P.
2014. Integrating knowledge tracing and item response theory: A tale of two frameworks. In CEUR
Workshop Proceedings. Vol. 1181. University of Pittsburgh, 7–15.

KIM, S. J., CHO, K. J., AND OH, S. 2017. Development of machine learning models for diagnosis of

glaucoma. PloS one 12, 5, e0177726.

50

KOEDINGER, K. R., BAKER, R. S., CUNNINGHAM, K., SKOGSHOLM, A., LEBER, B., AND STAMPER,
J. 2010. A data repository for the EDM community: The PSLC DataShop. Handbook of educational
data mining 43, 43–56.

LALWANI, A. AND AGRAWAL, S. 2017. Few hundred parameters outperform few hundred thousand. In
Proceedings of the 10th International Conference on Educational Data Mining, EDM. Vol. 17. ERIC,
448–453.

LECUN, Y., BENGIO, Y., AND HINTON, G. 2015. Deep learning. nature 521, 7553, 436–444.

LIN, C. AND CHI, M. 2016. Intervention-BKT: incorporating instructional interventions into bayesian
knowledge tracing. In International conference on intelligent tutoring systems. Springer, 208–218.

LIN, C. AND CHI, M. 2017. A comparisons of BKT, RNN and LSTM for learning gain prediction. In

International Conference on Artiﬁcial Intelligence in Education. Springer, 536–539.

LING, C. X., HUANG, J., ZHANG, H., ET AL. 2003. AUC: a statistically consistent and more discrimi-

nating measure than accuracy. In Ijcai. Vol. 3. 519–524.

LIPTON, Z. C. AND STEINHARDT, J. 2018. Troubling trends in machine learning scholarship. arXiv

preprint arXiv:1807.03341.

LIU, C., WHITE, M., AND NEWELL, G. 2011. Measuring and comparing the accuracy of species distri-

bution models with presence–absence data. Ecography 34, 2, 232–243.

LIU, Q., HUANG, Z., YIN, Y., CHEN, E., XIONG, H., SU, Y., AND HU, G. 2019. EKT: Exercise-aware
knowledge tracing for student performance prediction. IEEE Transactions on Knowledge and Data
Engineering 33, 1, 100–115.

LIU, Q., TONG, S., LIU, C., ZHAO, H., CHEN, E., MA, H., AND WANG, S. 2019. Exploiting cognitive
structure for adaptive learning. In Proceedings of the 25th ACM SIGKDD International Conference
on Knowledge Discovery & Data Mining. 627–635.

LOBO, J. M., JIM ´ENEZ-VALVERDE, A., AND REAL, R. 2008. AUC: a misleading measure of the per-
formance of predictive distribution models. Global ecology and Biogeography 17, 2, 145–151.

LUONG, M.-T., PHAM, H., AND MANNING, C. D. 2015. Effective approaches to attention-based neural

machine translation. arXiv preprint arXiv:1508.04025.

MA, W., ADESOPE, O. O., NESBIT, J. C., AND LIU, Q. 2014. Intelligent tutoring systems and learning

outcomes: A meta-analysis. Journal of educational psychology 106, 4, 901.

MACKEY, A. 2012. Why (or why not), when and how to replicate research. Replication research in

applied linguistics 2146.

MAKEL, M. C., PLUCKER, J. A., AND HEGARTY, B. 2012. Replications in psychology research how

often do they really occur? Perspectives on Psychological Science 7, 6, 537–542.

MANDALAPU, V., GONG, J., AND CHEN, L. 2021. Do we need to go deep? knowledge tracing with big

data. arXiv preprint arXiv:2101.08349.

MAO, Y., LIN, C., AND CHI, M. 2018. Deep learning vs. bayesian knowledge tracing: Student models

for interventions. JEDM— Journal of Educational Data Mining 10, 2, 28–54.

MIKOLOV, T., KOMBRINK, S., BURGET, L., ˇCERNOCK ´Y, J., AND KHUDANPUR, S. 2011. Extensions
of recurrent neural network language model. In 2011 IEEE International Conference on Acoustics,
Speech and Signal Processing (ICASSP). 5528–5531.

MILLER, A., FISCH, A., DODGE, J., KARIMI, A.-H., BORDES, A., AND WESTON, J. 2016. Key-value

memory networks for directly reading documents. arXiv preprint arXiv:1606.03126.

MOLNAR, C. 2020. Interpretable machine learning. Lulu. com.

51

MONTERO, S., ARORA, A., KELLY, S., MILNE, B., AND MOZER, M. 2018. Does deep knowledge

tracing model interactions among skills?. International Educational Data Mining Society.

MOONESINGHE, R., KHOURY, M. J., AND JANSSENS, A. C. J. 2007. Most published research ﬁndings

are false – but a little replication goes a long way. PLoS Med 4, 2, e28.

MUMA, J. R. 1993. The need for replication. Journal of Speech, Language, and Hearing Research 36, 5,

927–930.

MUSCHELLI, J. 2020. Roc and auc with a binary predictor: a potentially misleading metric. Journal of

classiﬁcation 37, 3, 696–708.

NAKAGAWA, H., IWASAWA, Y., AND MATSUO, Y. 2019. Graph-based knowledge tracing: Modeling
student proﬁciency using graph neural network. In IEEE/WIC/ACM International Conference on Web
Intelligence. WI ’19. ACM, New York, NY, USA, 156–163.

NWANA, H. S. 1990. Intelligent tutoring systems: an overview. Artiﬁcial Intelligence Review 4, 4, 251–

277.

OYA, T. AND MORISHIMA, S. 2021. LSTM-SAKT: LSTM-encoded SAKT-like transformer for
answer correctness prediction. arXiv preprint

knowledge tracing, 2nd place solution for riiid!
arXiv:2102.00845.

OZENNE, B., SUBTIL, F., AND MAUCORT-BOULCH, D. 2015. The precision–recall curve overcame the
optimism of the receiver operating characteristic curve in rare diseases. Journal of clinical epidemi-
ology 68, 8, 855–859.

PAHIKKALA, T., PYYSALO, S., BOBERG, J., J ¨ARVINEN, J., AND SALAKOSKI, T. 2009. Matrix rep-
resentations, linear transformations, and kernels for disambiguation in natural language. Machine
Learning 74, 2, 133–158.

PANDEY, S. AND KARYPIS, G. 2019. A self-attentive model for knowledge tracing.

PANDEY, S., KARYPIS, G., AND SRIVASTAVA, J. 2021. An empirical comparison of deep learning

models for knowledge tracing on large-scale dataset. arXiv preprint arXiv:2101.06373.

PANDEY, S. AND SRIVASTAVA, J. 2020. RKT: Relation-aware self-attention for knowledge tracing. In
Proceedings of the 29th ACM International Conference on Information & Knowledge Management.
1205–1214.

PARDOS, Z., HEFFERNAN, N., RUIZ, C., AND BECK, J. 2008. The composition effect: Conjuntive or
compensatory? an analysis of multi-skill math questions in its. In Educational Data Mining 2008.

PARDOS, Z. A. AND HEFFERNAN, N. T. 2010. Modeling individualization in a bayesian networks im-
plementation of knowledge tracing. In International Conference on User Modeling, Adaptation, and
Personalization. Springer, 255–266.

PARDOS, Z. A. AND HEFFERNAN, N. T. 2011. KT-IDEM: Introducing item difﬁculty to the knowl-
edge tracing model. In International conference on user modeling, adaptation, and personalization.
Springer, 243–254.

PATIL, P., PENG, R. D., AND LEEK, J. T. 2016. A statistical deﬁnition for reproducibility and replica-

bility. BioRxiv, 066803.

PAVLIK JR, P. I., CEN, H., AND KOEDINGER, K. R. 2009. Performance Factors Analysis–A New

Alternative to Knowledge Tracing. Online Submission.

PEARL, J. 1985. Bayesian netwcrks: A model cf self-activated memory for evidential reasoning. In
Proceedings of the 7th Conference of the Cognitive Science Society, University of California, Irvine,
CA, USA. 15–17.

52

PEARL, J. 1988. Probabilistic Reasoning in Intelligent Systems: Networks of Plausible Inference. Morgan

Kaufmann.

PEL ´ANEK, R. 2014. Application of time decay functions and the elo system in student modeling. In

Educational Data Mining 2014. Citeseer.

PEL ´ANEK, R. 2015. Metrics for evaluation of student models. Journal of Educational Data Mining 7, 2,

1–19.

PENG, R. D. 2011. Reproducible research in computational science. Science 334, 6060, 1226–1227.

PIECH, C., BASSEN, J., HUANG, J., GANGULI, S., SAHAMI, M., GUIBAS, L. J., AND SOHL-
DICKSTEIN, J. 2015. Deep knowledge tracing. In Advances in neural information processing sys-
tems. 505–513.

PU, S., YUDELSON, M., OU, L., AND HUANG, Y. 2020. Deep knowledge tracing with transformers. In

International Conference on Artiﬁcial Intelligence in Education. Springer, 252–256.

RAFFERTY, A. N., BRUNSKILL, E., GRIFFITHS, T. L., AND SHAFTO, P. 2011. Faster teaching by
pomdp planning. In International Conference on Artiﬁcial Intelligence in Education. Springer, 280–
287.

ROWE, J. P. AND LESTER, J. C. 2010. Modeling user knowledge with dynamic bayesian networks in in-
teractive narrative environments. In Sixth Artiﬁcial Intelligence and Interactive Digital Entertainment
Conference.

SAAD, E. W., PROKHOROV, D. V., AND WUNSCH, D. C. 1998. Comparative study of stock trend
prediction using time delay, recurrent and probabilistic neural networks. IEEE Transactions on neural
networks 9, 6, 1456–1470.

SAHA, S. AND RAGHAVA, G. P. S. 2006. Prediction of continuous b-cell epitopes in an antigen using

recurrent neural network. Proteins: Structure, Function, and Bioinformatics 65, 1, 40–48.

SAITO, T. AND REHMSMEIER, M. 2015. The precision-recall plot is more informative than the ROC

plot when evaluating binary classiﬁers on imbalanced datasets. PloS one 10, 3, e0118432.

SANYAL, D., BOSCH, N., AND PAQUETTE, L. 2020. Feature selection metrics: Similarities, differences,

and characteristics of the selected models. International Educational Data Mining Society.

SHIN, D., SHIM, Y., YU, H., LEE, S., KIM, B., AND CHOI, Y. 2021. SAINT+: Integrating tempo-
ral features for ednet correctness prediction. In LAK21: 11th International Learning Analytics and
Knowledge Conference. 490–496.

SONG, X., LI, J., TANG, Y., ZHAO, T., CHEN, Y., AND GUAN, Z. 2021. JKT: A joint graph convolu-

tional network based deep knowledge tracing. Information Sciences 580, 510–523.

SPELLMAN, B. A. 2012. Introduction to the special section data, data, everywhere... especially in my ﬁle

drawer. Perspectives on Psychological Science 7, 1, 58–59.

STEIF, P. AND BIER, N. 2014. OLI engineering statics-fall 2011, February 2014.

STEVENS, J. R. 2017. Replicability and reproducibility in comparative psychology. Frontiers in psychol-

ogy 8, 862.

SU, Y., LIU, Q., LIU, Q., HUANG, Z., YIN, Y., CHEN, E., DING, C., WEI, S., AND HU, G. 2018.
Exercise-enhanced sequential modeling for student performance prediction. In Proceedings of the
AAAI Conference on Artiﬁcial Intelligence. Vol. 32.

SUKHBAATAR, S., WESTON, J., FERGUS, R., ET AL. 2015. End-to-end memory networks. In Advances

in neural information processing systems. 2440–2448.

53

TRIFA, A., HEDHILI, A., AND CHAARI, W. L. 2019. Knowledge tracing with an intelligent agent, in an

e-learning platform. Education and Information Technologies 24, 1, 711–741.

VANLEHN, K. 2011. The relative effectiveness of human tutoring, intelligent tutoring systems, and other

tutoring systems. Educational Psychologist 46, 4, 197–221.

VASWANI, A., SHAZEER, N., PARMAR, N., USZKOREIT, J., JONES, L., GOMEZ, A. N., KAISER, Ł.,
AND POLOSUKHIN, I. 2017. Attention is all you need. In Advances in neural information processing
systems. 5998–6008.

VIE, J.-J. AND KASHIMA, H. 2019. Knowledge tracing machines: Factorization machines for knowl-
edge tracing. In Proceedings of the AAAI Conference on Artiﬁcial Intelligence. Vol. 33. 750–757.

WESTON, J., CHOPRA, S., AND BORDES, A. 2014. Memory networks. arXiv preprint arXiv:1410.3916.

WILSON, K. H., XIONG, X., KHAJAH, M., LINDSEY, R. V., ZHAO, S., KARKLIN, Y., VAN INWEGEN,
E. G., HAN, B., EKANADHAM, C., BECK, J. E., ET AL. 2016. Estimating student proﬁciency: Deep
learning is not the panacea. In In Neural Information Processing Systems, Workshop on Machine
Learning for Education. 3.

XIONG, X., ZHAO, S., VAN INWEGEN, E. G., AND BECK, J. E. 2016. Going deeper with deep knowl-

edge tracing. International Educational Data Mining Society.

YEUNG, C.-K. AND YEUNG, D.-Y. 2018. Addressing two problems in deep knowledge tracing via
prediction-consistent regularization. In Proceedings of the Fifth Annual ACM Conference on Learning
at Scale. 1–10.

YUDELSON, M., FANCSALI, S., RITTER, S., BERMAN, S., NIXON, T., AND JOSHI, A. 2014. Better

data beats big data. In Educational Data Mining 2014. Citeseer.

YUDELSON, M. V., KOEDINGER, K. R., AND GORDON, G. J. 2013. Individualized bayesian knowledge
tracing models. In International conference on artiﬁcial intelligence in education. Springer, 171–180.

ZHANG, J., SHI, X., KING, I., AND YEUNG, D.-Y. 2017. Dynamic key-value memory networks for
knowledge tracing. In Proceedings of the 26th international conference on World Wide Web. 765–
774.

ZHUANG, F., QI, Z., DUAN, K., XI, D., ZHU, Y., ZHU, H., XIONG, H., AND HE, Q. 2020. A compre-

hensive survey on transfer learning. Proceedings of the IEEE 109, 1, 43–76.

54

A. MODEL COMPARISON RESULTS

Table 9: Results for ASSISTments 2009 Updated dataset.

Model

Acc

AUC

Precision

Recall

F1

MCC

RMSE

Vanilla-DKT
LSTM-DKT
LSTM-DKT-S+
DKVMN
DKVMN-Paper
SAKT
GLR
BKT
Mean
NaP
NaP 3 Mean
NaP 5 Mean
NaP 9 Mean

.757±.009
.761±.009
.761±.011
.758±.011
.755±.011
.752±.015
.711±.029
.699±.010
.633±.058
.713±.025
.681±.022
.697±.026
.694±.028

.809±.010
.814±.011
.814±.010
.809±.010
.806±.010
.798±.008
.729±.032
.710±.029
.500±.000
.686±.025
.695±.034
.698±.034
.694±.035

.766±.021
.767±.017
.765±.018
.765±.020
.760±.023
.759±.017
.715±.033
.716±.033
.633±.058
.768±.038
.733±.033
.736±.033
.727±.032

.892±.011
.898±.018
.901±.018
.895±.014
.902±.014
.895±.031
.894±.045
.857±.015
1.000±.000
.776±.035
.771±.039
.803±.050
.816±.058

.824±.014
.827±.016
.827±.016
.825±.016
.825±.015
.821±.021
.794±.038
.780±.024
.774±.043
.772±.036
.751±.036
.768±.041
.769±.044

.448±.020
.455±.021
.455±.021
.450±.018
.443±.019
.434±.015
.324±.074
.313±.039
.000±.000
.373±.051
.288±.061
.315±.057
.301±.061

.403±.006
.400±.006
.400±.006
.403±.006
.405±.006
.408±.008
.438±.014
.448±.004
.484±.019
.535±.023
.485±.016
.470±.015
.463±.015

Table 10: Results for ASSISTments 2015 dataset.

Model

Acc

AUC

Precision

Recall

F1

MCC

RMSE

Vanilla-DKT
LSTM-DKT
LSTM-DKT-S+
DKVMN
DKVMN-Paper
SAKT
GLR
BKT
Mean
NaP
NaP 3 Mean
NaP 5 Mean
NaP 9 Mean

.749±.028
.751±.027
.751±.027
.750±.028
.750±.027
.748±.028
.750±.031
.747±.026
.738±.033
.690±.036
.701±.036
.704±.035
.712±.033

.720±.021
.725±.020
.725±.020
.723±.020
.718±.022
.714±.023
.702±.027
.694±.020
.500±.000
.594±.013
.602±.024
.624±.024
.624±.023

.767±.027
.769±.025
.769±.026
.769±.026
.769±.028
.767±.029
.763±.031
.761±.026
.738±.033
.786±.032
.778±.033
.771±.033
.769±.032

.943±.011
.943±.013
.943±.013
.941±.013
.943±.008
.942±.007
.959±.010
.955±.009
1.000±.000
.792±.034
.830±.029
.850±.025
.868±.022

.846±.020
.847±.021
.847±.020
.846±.021
.847±.020
.846±.020
.849±.022
.847±.018
.849±.022
.789±.033
.803±.031
.808±.029
.816±.027

.230±.015
.239±.013
.238±.015
.239±.015
.237±.017
.230±.021
.204±.017
.209±.012
.000±.000
.190±.026
.176±.027
.158±.025
.162±.022

.414±.019
.413±.019
.412±.019
.413±.019
.414±.019
.415±.019
.416±.021
.421±.015
.440±.017
.556±.032
.484±.026
.463±.023
.454±.021

55

Table 11: Results for ASSISTments 2017 dataset.

Model

Acc

AUC

Precision

Recall

F1

MCC

RMSE

Vanilla-DKT
LSTM-DKT
LSTM-DKT-S+
DKVMN
DKVMN-Paper
SAKT
GLR
BKT
Mean
NaP
NaP 3 Mean
NaP 5 Mean
NaP 9 Mean

.681±.015
.692±.017
.692±.016
.680±.011
.678±.012
.672±.014
.659±.004
.645±.016
.627±.005
.591±.001
.630±.005
.637±.004
.641±.004

.703±.009
.723±.010
.723±.010
.704±.009
.696±.009
.661±.022
.648±.005
.623±.002
.500±.000
.562±.002
.573±.002
.600±.002
.608±.003

.615±.028
.630±.019
.626±.019
.611±.037
.617±.031
.617±.026
.602±.008
.565±.020
.000±.000
.451±.007
.505±.004
.518±.005
.528±.005

.374±.049
.412±.039
.420±.040
.377±.044
.342±.071
.298±.082
.254±.007
.279±.018
.000±.000
.450±.007
.414±.007
.387±.008
.356±.008

.464±.044
.498±.035
.502±.035
.466±.045
.438±.068
.397±.084
.357±.009
.373±.013
.000±.000
.450±.007
.455±.005
.443±.006
.425±.007

.271±.020
.302±.016
.302±.014
.270±.026
.257±.032
.235±.036
.205±.007
.183±.009
.000±.000
.124±.004
.182±.005
.187±.005
.186±.006

.453±.008
.446±.009
.446±.009
.452±.006
.454±.006
.462±.004
.467±.002
.475±.006
.484±.001
.640±.001
.534±.002
.503±.002
.488±.001

Table 12: Results for IntroProg dataset.

Model

Acc

AUC

Precision

Recall

F1

MCC

RMSE

Vanilla-DKT
LSTM-DKT
LSTM-DKT-S+
DKVMN
DKVMN-Paper
SAKT
GLR
BKT
Mean
NaP
NaP 3 Mean
NaP 5 Mean
NaP 9 Mean

.752±.028
.757±.027
.755±.027
.756±.028
.754±.027
.754±.029
.761±.007
.713±.012
.513±.013
.716±.006
.708±.008
.707±.008
.705±.009

.821±.012
.827±.013
.826±.014
.827±.015
.825±.013
.825±.015
.843±.008
.789±.012
.500±.000
.716±.006
.767±.009
.772±.010
.774±.012

.756±.036
.754±.032
.752±.030
.757±.030
.752±.033
.743±.023
.754±.002
.738±.087
.000±.000
.708±.005
.696±.005
.693±.006
.687±.006

.746±.048
.762±.056
.761±.060
.755±.055
.759±.058
.776±.068
.757±.008
.684±.011
.000±.000
.710±.005
.713±.006
.717±.006
.723±.006

.751±.042
.758±.044
.756±.045
.756±.043
.755±.045
.759±.045
.755±.004
.708±.041
.000±.000
.709±.005
.704±.006
.704±.006
.704±.006

.483±.029
.491±.030
.487±.029
.490±.031
.486±.029
.482±.034
.522±.014
.423±.012
.000±.000
.431±.012
.416±.017
.414±.017
.410±.018

.410±.016
.406±.016
.406±.017
.406±.018
.407±.016
.407±.020
.402±.005
.436±.004
.500±.000
.533±.006
.466±.006
.456±.006
.450±.007

56

Table 13: Results for Statics dataset.

Model

Acc

AUC

Precision

Recall

F1

MCC

RMSE

Vanilla-DKT
LSTM-DKT
LSTM-DKT-S+
DKVMN
DKVMN-Paper
SAKT
GLR
BKT
Mean
NaP
NaP 3 Mean
NaP 5 Mean
NaP 9 Mean

.804±.017
.807±.016
.806±.015
.807±.017
.803±.016
.801±.018
.805±.006
.786±.022
.765±.007
.705±.004
.729±.005
.740±.004
.751±.005

.815±.014
.824±.013
.823±.013
.825±.012
.812±.013
.808±.017
.815±.004
.774±.028
.500±.000
.589±.005
.640±.007
.652±.008
.661±.009

.828±.017
.832±.014
.832±.014
.835±.017
.831±.015
.827±.019
.826±.005
.803±.028
.765±.007
.807±.004
.801±.004
.797±.003
.791±.004

.938±.013
.937±.017
.936±.018
.932±.011
.933±.013
.936±.012
.943±.006
.953±.012
1.000±.000
.808±.004
.859±.005
.886±.005
.916±.007

.880±.014
.881±.014
.881±.013
.881±.014
.879±.013
.878±.014
.881±.005
.872±.013
.867±.005
.808±.004
.829±.004
.839±.004
.849±.005

.372±.021
.383±.030
.383±.034
.393±.023
.374±.028
.363±.027
.378±.007
.305±.070
.000±.000
.179±.010
.180±.015
.176±.018
.170±.019

.369±.015
.365±.013
.365±.013
.364±.014
.369±.013
.371±.015
.369±.004
.393±.013
.424±.004
.543±.004
.453±.003
.435±.003
.422±.003

Table 14: Results for Synthetic-K2 dataset.

Model

Acc

AUC

Precision

Recall

F1

MCC

RMSE

Vanilla-DKT
LSTM-DKT
LSTM-DKT-S+
DKVMN
DKVMN-Paper
SAKT
GLR
BKT
Mean
NaP
NaP 3 Mean
NaP 5 Mean
NaP 9 Mean

.804±.003
.805±.003
.806±.003
.806±.003
.806±.002
.806±.002
.729±.004
.692±.004
.685±.005
.644±.001
.680±.003
.688±.003
.707±.003

.869±.003
.871±.002
.871±.002
.872±.002
.872±.002
.872±.002
.786±.002
.635±.005
.500±.000
.585±.003
.654±.004
.681±.004
.709±.005

.851±.004
.848±.006
.851±.005
.852±.003
.853±.003
.855±.006
.750±.003
.700±.005
.685±.005
.738±.002
.753±.002
.754±.002
.764±.003

.867±.007
.872±.004
.868±.003
.867±.003
.865±.002
.862±.005
.906±.005
.961±.000
1.000±.000
.744±.002
.794±.004
.808±.004
.827±.006

.858±.003
.860±.002
.859±.002
.860±.002
.859±.002
.859±.001
.821±.003
.810±.003
.813±.003
.741±.002
.773±.003
.780±.003
.794±.004

.542±.007
.541±.008
.545±.006
.546±.005
.547±.005
.548±.006
.308±.013
.132±.002
.000±.000
.171±.006
.235±.008
.247±.010
.289±.008

.367±.002
.366±.002
.366±.002
.365±.002
.365±.002
.365±.002
.418±.002
.455±.002
.465±.002
.597±.001
.489±.002
.465±.001
.448±.001

57

Table 15: Results for Synthetic-K5 dataset.

Model

Acc

AUC

Precision

Recall

F1

MCC

RMSE

Vanilla-DKT
LSTM-DKT
LSTM-DKT-S+
DKVMN
DKVMN-Paper
SAKT
GLR
BKT
Mean
NaP
NaP 3 Mean
NaP 5 Mean
NaP 9 Mean

.750±.002
.752±.003
.751±.002
.754±.003
.753±.003
.753±.003
.649±.004
.633±.002
.608±.003
.565±.004
.579±.003
.596±.004
.610±.005

.822±.002
.827±.002
.826±.002
.829±.002
.829±.003
.828±.002
.683±.006
.633±.002
.500±.000
.543±.003
.560±.004
.581±.005
.605±.006

.805±.003
.807±.004
.805±.003
.812±.003
.812±.003
.809±.008
.666±.004
.641±.004
.608±.003
.641±.004
.645±.004
.655±.004
.664±.004

.778±.004
.779±.003
.779±.008
.775±.005
.773±.007
.777±.004
.848±.005
.899±.010
1.000±.000
.647±.004
.682±.006
.710±.006
.725±.007

.791±.003
.793±.003
.792±.004
.793±.003
.792±.004
.793±.002
.746±.002
.748±.002
.756±.002
.644±.004
.663±.005
.681±.005
.693±.005

.481±.004
.485±.005
.483±.004
.491±.006
.490±.006
.488±.008
.220±.011
.164±.006
.000±.000
.086±.007
.103±.005
.133±.009
.163±.010

.409±.001
.406±.002
.406±.001
.404±.002
.405±.002
.405±.002
.465±.001
.476±.000
.488±.001
.659±.003
.551±.002
.522±.002
.504±.002

58

B. BEST HYPERPARAMETERS

The layer sizes in the tables are as follows: recurrent layer size (attention layer size for SAKT),
key-embedding layer size, value-embedding layer size and summary layer size. Hyperparame-
ters that are not used for a model are denoted by the dash symbol “-”. As an example, if a model
has no key-embeddings or a summary layer, recurrent layer size of 100 and value-embedding
layer size of 20, the layer sizes are shown as 100,-,20,-.

Table 16: Best hyperparameters for DLKT models in ASSISTments 2009 Updated dataset

Model

DKVMN

DKVMN-Paper LSTM-DKT LSTM-DKT-S+

SAKT

Vanilla-DKT

0.809
AUC
13
Seed
0.001
Init lr
50,20,50,-,
Layer sizes
-
N heads
One-hot input
True
Output per skill True

0.806
13
0.01
50,20,20,100
-
False
False

0.814
42
0.001
50,-,20,-,
-
False
True

0.814
42
0.001
50,-,20,-,
-
False
True

0.798
13
0.001
50,50,50,50
5
False
False

0.809
13
0.001
100,-,50,-,
-
False
True

Table 17: Best hyperparameters for DLKT models in ASSISTments 2015 dataset

Model

DKVMN

DKVMN-Paper LSTM-DKT LSTM-DKT-S+

SAKT

Vanilla-DKT

0.723
AUC
13
Seed
0.001
Init lr
50,20,20,-,
Layer sizes
-
N heads
One-hot input
False
Output per skill True

0.718
42
0.01
50,50,50,100
-
False
False

0.725
13
0.001
100,-,50,-,
-
False
True

0.725
13
0.001
100,-,50,-,
-
False
True

0.714
13
0.001
100,-,-,50
5
True
False

0.72
42
0.001
50,-,50,-,
-
False
True

Table 18: Best hyperparameters for DLKT models in ASSISTments 2017 dataset

Model

DKVMN

DKVMN-Paper LSTM-DKT LSTM-DKT-S+

SAKT

Vanilla-DKT

AUC
Seed
Init lr
Layer sizes
N heads
One-hot input
Output per skill

0.704
13
0.01
100,20,50,100
-
True
False

0.696
42
0.01
50,50,50,100
-
False
False

0.723
42
0.01
100,-,20,-,
-
False
True

0.723
42
0.01
100,-,20,-,
-
False
True

0.661
42
0.001
50,20,20,-,
5
False
True

0.703
42
0.001
100,-,50,-,
-
False
True

59

Table 19: Best hyperparameters for DLKT models in IntroProg dataset

Model

DKVMN

DKVMN-Paper LSTM-DKT LSTM-DKT-S+

SAKT

Vanilla-DKT

AUC
0.827
Seed
13
Init lr
0.001
Layer sizes
50,20,20,-,
N heads
-
False
One-hot input
Output per skill True

0.825
13
0.01
50,50,20,50
-
False
False

0.827
42
0.001
50,-,50,-,
-
False
True

0.826
42
0.01
50,-,20,-,
-
False
True

0.825
13
0.001
50,-,-,100
1
True
False

0.821
42
0.001
50,-,20,-,
-
False
True

Table 20: Best hyperparameters for DLKT models in Statics dataset

Model

DKVMN

DKVMN-Paper LSTM-DKT LSTM-DKT-S+

SAKT

Vanilla-DKT

0.825
AUC
13
Seed
0.01
Init lr
50,20,20,-,
Layer sizes
-
N heads
One-hot input
True
Output per skill True

0.812
13
0.01
50,50,50,-,
-
True
True

0.824
13
0.001
50,-,-,-,
-
True
True

0.823
13
0.001
50,-,-,-,
-
True
True

0.808
42
0.001
50,-,-,-,
1
True
True

0.815
42
0.001
100,-,50,-,
-
False
True

Table 21: Best hyperparameters for DLKT models in Synthetic-K2 dataset

Model

DKVMN

DKVMN-Paper LSTM-DKT LSTM-DKT-S+

SAKT

Vanilla-DKT

AUC
Seed
Init lr
Layer sizes
N heads
One-hot input
Output per skill

0.872
42
0.001
50,50,20,50
-
True
False

0.872
42
0.001
50,50,20,50
-
False
False

0.871
42
0.001
50,-,-,100
-
True
False

0.871
42
0.001
50,-,20,100
-
False
False

0.872
42
0.001
100,20,20,100
5
False
False

0.869
42
0.001
100,-,-,-,
-
True
True

Table 22: Best hyperparameters for DLKT models in Synthetic-K5 dataset

Model

DKVMN

DKVMN-Paper LSTM-DKT LSTM-DKT-S+

SAKT

Vanilla-DKT

AUC
Seed
Init lr
Layer sizes
N heads
One-hot input
Output per skill

0.829
42
0.001
50,50,20,50
-
True
False

0.829
13
0.001
50,50,50,50
-
False
False

0.827
42
0.001
50,-,-,-,
-
True
True

60

0.826
13
0.001
50,-,-,-,
-
True
True

0.828
42
0.001
100,20,20,100
5
False
False

0.822
13
0.001
100,-,20,-,
-
False
True

Table 23: Output-per-skill effect on AUC in ASSISTments 2017 dataset

Model

DKVMN

DKVMN-Paper

LSTM-DKT

LSTM-DKT-S+

SAKT

Vanilla-DKT

Output-per-skill Max Max-Min

Min

Sd

False
True
False
True
False
True
False
True
False
True
False
True

0.704
0.685
0.696
0.658
0.711
0.723
0.718
0.723
0.659
0.661
0.683
0.703

0.038
0.011
0.058
0.012
0.048
0.010
0.032
0.007
0.138
0.038
0.126
0.090

0.666
0.674
0.638
0.646
0.663
0.713
0.686
0.716
0.521
0.623
0.557
0.613

0.012
0.002
0.019
0.002
0.010
0.002
0.005
0.002
0.051
0.005
0.036
0.029

Table 24: Output-per-skill effect on AUC in ASSISTments 2015 dataset

Model

DKVMN

DKVMN-Paper

LSTM-DKT

LSTM-DKT-S+

SAKT

Vanilla-DKT

Output-per-skill Max Max-Min

Min

Sd

False
True
False
True
False
True
False
True
False
True
False
True

0.720
0.723
0.718
0.716
0.703
0.725
0.720
0.725
0.714
0.713
0.699
0.720

0.006
0.009
0.032
0.029
0.020
0.005
0.016
0.005
0.194
0.060
0.076
0.056

0.714
0.714
0.686
0.687
0.683
0.720
0.704
0.720
0.520
0.653
0.623
0.664

0.001
0.003
0.010
0.006
0.003
0.002
0.003
0.002
0.062
0.013
0.021
0.019

Table 25: One-hot-input effect on AUC in IntroProg dataset

Model

DKVMN

DKVMN-Paper

LSTM-DKT

LSTM-DKT-S+

SAKT

Vanilla-DKT

One-hot-input Max Max-Min

Min

Sd

0.003
0.003
0.024
0.012
0.007
0.007
0.002
0.003
0.265
0.219
0.102
0.088

0.824
0.824
0.801
0.806
0.820
0.819
0.824
0.823
0.552
0.606
0.719
0.733

0.001
0.001
0.005
0.003
0.002
0.003
0.001
0.001
0.055
0.065
0.017
0.018

False
True
False
True
False
True
False
True
False
True
False
True

0.827
0.827
0.825
0.818
0.827
0.826
0.826
0.826
0.817
0.825
0.821
0.821

61

Table 26: One-hot input effect on AUC in ASSISTments 2009 Updated dataset

Model

DKVMN

DKVMN-Paper

LSTM-DKT

LSTM-DKT-S+

SAKT

Vanilla-DKT

One-hot-input Max Max-Min

Min

Sd

False
True
False
True
False
True
False
True
False
True
False
True

0.809
0.809
0.806
0.773
0.814
0.814
0.814
0.814
0.798
0.752
0.809
0.806

0.010
0.010
0.094
0.062
0.077
0.026
0.072
0.014
0.253
0.207
0.098
0.101

0.799
0.799
0.712
0.711
0.737
0.788
0.742
0.800
0.545
0.545
0.711
0.705

0.003
0.004
0.035
0.017
0.013
0.011
0.010
0.005
0.071
0.074
0.022
0.023

Table 27: Random seed effect on AUC in ASSISTments 2015 dataset

Model

DKVMN

DKVMN-Paper

LSTM-DKT

LSTM-DKT-S+

SAKT

Vanilla-DKT

Random seed Max Max-Min

Min

Sd

13
42
13
42
13
42
13
42
13
42
13
42

0.723
0.723
0.718
0.718
0.725
0.724
0.725
0.725
0.714
0.713
0.719
0.720

0.009
0.009
0.031
0.032
0.042
0.027
0.021
0.018
0.176
0.193
0.096
0.091

0.714
0.714
0.687
0.686
0.683
0.697
0.704
0.707
0.538
0.520
0.623
0.629

0.002
0.002
0.009
0.009
0.012
0.011
0.004
0.003
0.049
0.062
0.024
0.022

62

C. MAX ATTEMPT FILTER CUT VERSUS SPLIT

Dataset

ASSISTments 2009
Updated

Model

DKVMN

DKVMN-Paper

LSTM-DKT

LSTM-DKT-S+

SAKT

Vanilla-DKT

ASSISTments 2015 DKVMN

DKVMN-Paper

LSTM-DKT

LSTM-DKT-S+

Table 28: Student attempt split effect

Max attempt ﬁlter Acc AUC Prec Recall

F1

MCC RMSE

.759
.753
.758
.758
.758
.745
.749
.756
.755
.754
.763
.756
.763
.761
.759
.762
.756
.762
.761
.759
.743
.744
.746
.752
.746
.758
.752
.757
.757
.753
.755
.754
.755
.750
.754
.753
.753
.753
.750
.753
.755
.755
.754
.751
.754
.754
.754
.754

.812
.809
.811
.809
.811
.788
.802
.806
.806
.804
.817
.813
.816
.814
.815
.817
.813
.815
.814
.815
.787
.795
.790
.798
.792
.810
.808
.808
.809
.806
.725
.725
.725
.723
.725
.719
.718
.719
.718
.719
.727
.727
.727
.725
.727
.727
.727
.727

.763
.758
.762
.765
.762
.750
.750
.759
.760
.759
.766
.761
.768
.767
.764
.765
.761
.765
.765
.764
.756
.748
.756
.759
.754
.761
.758
.759
.766
.761
.772
.772
.772
.769
.772
.771
.772
.773
.769
.772
.772
.772
.772
.769
.772
.772
.772
.772

.890
.880
.889
.895
.891
.889
.886
.891
.902
.888
.892
.878
.888
.898
.889
.893
.878
.892
.901
.888
.865
.879
.871
.895
.880
.893
.876
.893
.892
.882
.945
.945
.945
.941
.945
.946
.943
.941
.943
.943
.945
.946
.945
.943
.944
.946
.946
.945

.822
.814
.821
.825
.821
.814
.812
.820
.825
.818
.824
.815
.823
.827
.822
.824
.815
.824
.827
.821
.807
.808
.809
.821
.812
.822
.813
.821
.824
.817
.850
.850
.850
.846
.849
.849
.849
.848
.847
.849
.850
.850
.850
.847
.850
.850
.850
.850

.454
.452
.452
.450
.452
.413
.441
.446
.443
.441
.463
.458
.463
.455
.455
.462
.457
.460
.455
.455
.416
.430
.424
.434
.423
.451
.449
.449
.448
.441
.240
.237
.240
.239
.238
.233
.236
.238
.237
.238
.241
.240
.241
.239
.241
.239
.238
.240

.401
.406
.402
.403
.403
.411
.409
.404
.405
.406
.398
.404
.399
.400
.401
.398
.404
.399
.400
.401
.415
.412
.413
.408
.413
.403
.407
.404
.403
.406
.410
.411
.410
.413
.411
.412
.413
.413
.414
.413
.410
.410
.410
.413
.410
.410
.410
.410

- & -
Cut & 200
Cut & 500
Split & 200
Split & 500
- & -
Cut & 200
Cut & 500
Split & 200
Split & 500
- & -
Cut & 200
Cut & 500
Split & 200
Split & 500
- & -
Cut & 200
Cut & 500
Split & 200
Split & 500
- & -
Cut & 200
Cut & 500
Split & 200
Split & 500
- & -
Cut & 200
Cut & 500
Split & 200
Split & 500
- & -
Cut & 200
Cut & 500
Split & 200
Split & 500
- & -
Cut & 200
Cut & 500
Split & 200
Split & 500
- & -
Cut & 200
Cut & 500
Split & 200
Split & 500
- & -
Cut & 200
Cut & 500

63

Dataset

Model

Max attempt ﬁlter Acc AUC Prec Recall

F1

MCC RMSE

Table 28: Student attempt split effect

ASSISTments 2015

LSTM-DKT-S+

SAKT

Vanilla-DKT

ASSISTments 2017 DKVMN

DKVMN-Paper

LSTM-DKT

LSTM-DKT-S+

SAKT

Vanilla-DKT

IntroProg

DKVMN

DKVMN-Paper

.751
.755
.752
.753
.752
.748
.751
.752
.753
.752
.749
.753
.684
.666
.676
.680
.680
.681
.667
.673
.678
.676
.697
.676
.685
.692
.690
.697
.676
.686
.692
.690
.660
.652
.657
.672
.665
.679
.667
.670
.681
.675
.765
.753
.761
.756
.763
.764
.752
.760
.754

.725
.726
.715
.715
.715
.714
.714
.722
.721
.721
.720
.721
.712
.690
.708
.704
.703
.701
.685
.699
.696
.693
.734
.707
.724
.723
.722
.734
.708
.726
.723
.721
.650
.659
.664
.661
.651
.698
.687
.692
.703
.691
.847
.833
.843
.827
.842
.845
.832
.842
.825

.769
.772
.769
.769
.769
.767
.772
.770
.769
.770
.767
.769
.626
.612
.627
.611
.615
.630
.638
.642
.617
.624
.631
.620
.632
.630
.625
.630
.622
.633
.626
.626
.600
.604
.612
.617
.609
.627
.625
.632
.615
.613
.761
.763
.761
.757
.760
.762
.763
.761
.752

.943
.945
.948
.949
.948
.942
.939
.946
.948
.947
.943
.949
.380
.391
.416
.377
.384
.348
.341
.367
.342
.335
.450
.435
.461
.412
.433
.452
.433
.462
.420
.427
.261
.312
.327
.298
.287
.342
.365
.367
.374
.350
.754
.763
.756
.755
.755
.749
.759
.752
.759

.847
.850
.849
.849
.849
.846
.847
.849
.849
.849
.846
.849
.472
.477
.500
.466
.471
.448
.444
.467
.438
.433
.525
.511
.533
.498
.510
.526
.511
.534
.502
.507
.363
.411
.426
.397
.383
.443
.460
.464
.464
.443
.757
.763
.758
.756
.757
.756
.761
.757
.755

.238
.241
.225
.227
.227
.230
.234
.229
.227
.228
.230
.228
.283
.263
.287
.270
.272
.271
.261
.276
.257
.256
.321
.289
.312
.302
.303
.321
.291
.315
.302
.302
.207
.221
.234
.235
.219
.266
.261
.269
.271
.254
.529
.504
.522
.490
.521
.527
.502
.520
.486

.412
.410
.413
.413
.413
.415
.414
.412
.412
.412
.414
.412
.450
.460
.455
.452
.453
.453
.461
.457
.454
.456
.443
.455
.450
.446
.447
.444
.455
.449
.446
.447
.466
.468
.467
.462
.465
.454
.461
.459
.453
.456
.399
.408
.402
.406
.401
.400
.409
.403
.407

Split & 200
Split & 500
- & -
Cut & 200
Cut & 500
Split & 200
Split & 500
- & -
Cut & 200
Cut & 500
Split & 200
Split & 500
- & -
Cut & 200
Cut & 500
Split & 200
Split & 500
- & -
Cut & 200
Cut & 500
Split & 200
Split & 500
- & -
Cut & 200
Cut & 500
Split & 200
Split & 500
- & -
Cut & 200
Cut & 500
Split & 200
Split & 500
- & -
Cut & 200
Cut & 500
Split & 200
Split & 500
- & -
Cut & 200
Cut & 500
Split & 200
Split & 500
- & -
Cut & 200
Cut & 500
Split & 200
Split & 500
- & -
Cut & 200
Cut & 500
Split & 200

64

Table 28: Student attempt split effect

Model

Max attempt ﬁlter Acc AUC Prec Recall

F1

MCC RMSE

Dataset

IntroProg

DKVMN-Paper
LSTM-DKT

LSTM-DKT-S+

SAKT

Vanilla-DKT

Statics

DKVMN

DKVMN-Paper

LSTM-DKT

LSTM-DKT-S+

SAKT

Vanilla-DKT

.761
.765
.753
.761
.757
.763
.764
.751
.759
.755
.761
.761
.749
.758
.754
.760
.760
.748
.756
.752
.758
.813
.828
.805
.807
.807
.807
.826
.801
.803
.801
.805
.822
.798
.807
.799
.802
.819
.798
.806
.799
.802
.828
.798
.801
.800
.805
.817
.794
.804
.800

.841
.846
.833
.843
.827
.842
.845
.832
.842
.826
.841
.843
.830
.839
.825
.839
.840
.827
.836
.821
.835
.836
.830
.823
.825
.828
.823
.827
.812
.812
.813
.819
.812
.803
.824
.811
.807
.804
.806
.823
.813
.811
.835
.808
.808
.810
.822
.798
.791
.815
.811

.762
.761
.763
.760
.754
.760
.760
.759
.758
.752
.757
.748
.751
.750
.743
.750
.759
.760
.758
.756
.759
.841
.852
.832
.835
.837
.832
.849
.825
.831
.831
.824
.840
.817
.832
.822
.821
.837
.818
.832
.822
.826
.853
.824
.827
.829
.829
.835
.815
.828
.824

.746
.755
.764
.757
.762
.756
.753
.767
.756
.761
.754
.768
.778
.767
.776
.767
.744
.756
.746
.746
.742
.931
.950
.933
.932
.927
.938
.951
.938
.933
.928
.948
.959
.948
.937
.942
.947
.961
.946
.936
.941
.940
.947
.937
.936
.930
.938
.960
.945
.938
.938

.754
.758
.763
.758
.758
.758
.756
.763
.757
.756
.756
.757
.764
.759
.759
.758
.751
.758
.752
.751
.750
.884
.898
.880
.881
.880
.881
.897
.878
.879
.877
.881
.896
.878
.881
.877
.880
.894
.878
.881
.877
.879
.898
.876
.878
.877
.880
.893
.875
.880
.877

.518
.530
.504
.522
.491
.521
.527
.501
.519
.487
.517
.521
.497
.516
.482
.515
.519
.495
.511
.483
.511
.422
.379
.394
.393
.407
.393
.368
.369
.374
.384
.375
.334
.351
.383
.364
.362
.315
.353
.383
.364
.371
.382
.362
.363
.378
.383
.306
.335
.372
.371

.402
.400
.408
.402
.406
.401
.400
.409
.403
.406
.402
.403
.411
.405
.407
.404
.404
.412
.406
.410
.405
.360
.348
.367
.364
.365
.366
.350
.371
.369
.372
.367
.355
.374
.365
.373
.372
.358
.374
.365
.372
.370
.348
.373
.371
.372
.367
.360
.379
.369
.373

Split & 500
- & -
Cut & 200
Cut & 500
Split & 200
Split & 500
- & -
Cut & 200
Cut & 500
Split & 200
Split & 500
- & -
Cut & 200
Cut & 500
Split & 200
Split & 500
- & -
Cut & 200
Cut & 500
Split & 200
Split & 500
- & -
Cut & 200
Cut & 500
Split & 200
Split & 500
- & -
Cut & 200
Cut & 500
Split & 200
Split & 500
- & -
Cut & 200
Cut & 500
Split & 200
Split & 500
- & -
Cut & 200
Cut & 500
Split & 200
Split & 500
- & -
Cut & 200
Cut & 500
Split & 200
Split & 500
- & -
Cut & 200
Cut & 500
Split & 200
Split & 500

65

Dataset

Model

Max attempt ﬁlter Acc AUC Prec Recall

F1

MCC RMSE

Table 28: Student attempt split effect

Synthetic-K2

DKVMN

DKVMN-Paper

LSTM-DKT

LSTM-DKT-S+

SAKT

Vanilla-DKT

Synthetic-K5

DKVMN

DKVMN-Paper

LSTM-DKT

LSTM-DKT-S+

SAKT

.806
.806
.806
.806
.806
.806
.806
.806
.806
.806
.805
.805
.805
.805
.805
.806
.806
.806
.806
.806
.806
.806
.806
.806
.806
.805
.805
.805
.804
.805
.754
.754
.754
.754
.754
.753
.753
.753
.753
.753
.752
.752
.752
.752
.752
.751
.751
.751
.751
.751
.753

.872
.872
.872
.872
.872
.872
.872
.872
.872
.872
.871
.871
.871
.871
.871
.871
.871
.871
.871
.871
.872
.872
.872
.872
.872
.869
.869
.869
.869
.869
.829
.829
.829
.829
.829
.829
.829
.829
.829
.829
.827
.827
.827
.827
.827
.826
.826
.826
.826
.826
.828

.852
.852
.852
.852
.852
.853
.853
.853
.853
.853
.848
.848
.848
.848
.848
.851
.851
.851
.851
.851
.855
.855
.855
.855
.855
.850
.850
.850
.851
.850
.812
.812
.812
.812
.812
.812
.812
.812
.812
.812
.807
.807
.807
.807
.807
.805
.805
.805
.805
.805
.808

.867
.867
.867
.867
.867
.865
.865
.865
.865
.865
.872
.872
.872
.872
.872
.868
.868
.868
.868
.868
.862
.862
.862
.862
.862
.867
.867
.867
.867
.867
.775
.775
.775
.775
.775
.773
.773
.773
.773
.773
.779
.779
.779
.779
.779
.779
.779
.779
.779
.779
.780

.860
.860
.860
.860
.860
.859
.859
.859
.859
.859
.860
.860
.860
.860
.860
.859
.859
.859
.859
.859
.859
.859
.859
.859
.859
.859
.859
.859
.858
.859
.793
.793
.793
.793
.793
.792
.792
.792
.792
.792
.793
.793
.793
.793
.793
.792
.792
.792
.792
.792
.794

.546
.546
.546
.546
.546
.547
.547
.547
.547
.547
.541
.541
.541
.541
.541
.545
.545
.545
.545
.545
.548
.548
.548
.548
.548
.542
.542
.542
.542
.542
.491
.491
.491
.491
.491
.490
.490
.490
.490
.490
.485
.485
.485
.485
.485
.483
.483
.483
.483
.483
.488

.365
.365
.365
.365
.365
.365
.365
.365
.365
.365
.366
.366
.366
.366
.366
.366
.366
.366
.366
.366
.365
.365
.365
.365
.365
.367
.367
.367
.367
.367
.404
.404
.404
.404
.404
.405
.405
.405
.405
.405
.406
.406
.406
.406
.406
.406
.406
.406
.406
.406
.405

- & -
Cut & 200
Cut & 500
Split & 200
Split & 500
- & -
Cut & 200
Cut & 500
Split & 200
Split & 500
- & -
Cut & 200
Cut & 500
Split & 200
Split & 500
- & -
Cut & 200
Cut & 500
Split & 200
Split & 500
- & -
Cut & 200
Cut & 500
Split & 200
Split & 500
- & -
Cut & 200
Cut & 500
Split & 200
Split & 500
- & -
Cut & 200
Cut & 500
Split & 200
Split & 500
- & -
Cut & 200
Cut & 500
Split & 200
Split & 500
- & -
Cut & 200
Cut & 500
Split & 200
Split & 500
- & -
Cut & 200
Cut & 500
Split & 200
Split & 500
- & -

66

Dataset

Synthetic-K5

Model

SAKT

Vanilla-DKT

Table 28: Student attempt split effect

Max attempt ﬁlter Acc AUC Prec Recall

F1

MCC RMSE

Cut & 200
Cut & 500
Split & 200
Split & 500
- & -
Cut & 200
Cut & 500
Split & 200
Split & 500

.753
.753
.753
.753
.749
.749
.749
.750
.749

.828
.828
.828
.828
.822
.822
.822
.822
.822

.808
.808
.809
.808
.803
.803
.803
.805
.803

.780
.780
.777
.780
.777
.777
.777
.778
.777

.794
.794
.793
.794
.790
.790
.790
.791
.790

.488
.488
.488
.488
.478
.478
.478
.481
.478

.405
.405
.405
.405
.409
.409
.409
.409
.409

Table 29: Student attempt split effect averaged over datasets

Max attempt ﬁlter Acc AUC Prec Recall F1

MCC RMSE

Model

DKVMN

DKVMN-Paper

LSTM-DKT

- & -
Cut & 200
Cut & 500
Split & 200
Split & 500
- & -
Cut & 200
Cut & 500
Split & 200
Split & 500
- & -
Cut & 200
Cut & 500
Split & 200
Split & 500

LSTM-DKT-S+ - & -

SAKT

Vanilla-DKT

Cut & 200
Cut & 500
Split & 200
Split & 500
- & -
Cut & 200
Cut & 500
Split & 200
Split & 500
- & -
Cut & 200
Cut & 500
Split & 200
Split & 500

.775
.774
.774
.772
.773
.773
.777
.775
.771
.773
.773
.773
.772
.772
.771
.772
.772
.772
.771
.771
.766
.770
.768
.768
.768
.771
.771
.770
.770
.768

.792
.796
.797
.792
.792
.787
.788
.790
.788
.783
.806
.805
.807
.800
.802
.805
.805
.807
.801
.800
.775
.787
.785
.784
.778
.787
.793
.792
.791
.786

.777
.779
.780
.775
.776
.771
.773
.774
.771
.769
.784
.784
.785
.781
.781
.784
.783
.785
.781
.780
.758
.769
.767
.765
.761
.771
.775
.773
.773
.769

.424
.410
.419
.411
.418
.411
.406
.412
.405
.411
.422
.407
.416
.414
.416
.420
.404
.416
.413
.415
.397
.399
.400
.397
.401
.410
.394
.402
.404
.404

.398
.400
.401
.401
.400
.402
.402
.403
.403
.403
.399
.401
.401
.400
.401
.399
.401
.401
.400
.401
.405
.403
.406
.405
.405
.402
.404
.405
.404
.404

.762
.759
.759
.759
.760
.758
.758
.757
.757
.758
.763
.760
.760
.761
.760
.762
.759
.759
.760
.760
.754
.755
.753
.755
.754
.758
.756
.755
.757
.756

.805
.798
.802
.798
.801
.797
.795
.797
.794
.796
.806
.799
.802
.802
.802
.804
.797
.802
.801
.802
.787
.791
.788
.787
.787
.798
.790
.791
.794
.794

67

D. HARDWARE COMPARISON CPU VERSUS GPU

Both the CPU and GPU results here were obtained with a later TensorFlow version than the
hyperparameter tuning results, wherefore the CPU results may not exactly match the results in
other result tables. Also the CPU hardware may differ from the other results as the models were
trained and evaluated in a computation cluster with multiple different CPU nodes with varying
CPU types.

Table 30: Hardware result comparison

Hardware

Acc AUC Prec Recall

F1

MCC RMSE

Dataset

ASSISTments 2009
Updated

Model

DKVMN

CPU-tf2.1.0
CPU-tf2.6.2
GPU

DKVMN-Paper CPU-tf2.1.0
CPU-tf2.6.2
GPU
CPU-tf2.1.0
CPU-tf2.6.2
GPU

LSTM-DKT

SAKT

LSTM-DKT-S+ CPU-tf2.1.0
CPU-tf2.6.2
GPU
CPU-tf2.1.0
CPU-tf2.6.2
GPU
CPU-tf2.1.0
CPU-tf2.6.2
GPU
CPU-tf2.1.0
CPU-tf2.6.2
GPU

Vanilla-DKT

ASSISTments 2015 DKVMN

DKVMN-Paper CPU-tf2.1.0
CPU-tf2.6.2
GPU
CPU-tf2.1.0
CPU-tf2.6.2
GPU

LSTM-DKT

SAKT

LSTM-DKT-S+ CPU-tf2.1.0
CPU-tf2.6.2
GPU
CPU-tf2.1.0
CPU-tf2.6.2
GPU
CPU-tf2.1.0
CPU-tf2.6.2
GPU
CPU-tf2.1.0
CPU-tf2.6.2
GPU

Vanilla-DKT

ASSISTments 2017 DKVMN

DKVMN-Paper CPU-tf2.1.0
CPU-tf2.6.2
GPU

68

.758
.757
.757
.755
.755
.754
.761
.759
.759
.761
.759
.759
.752
.752
.752
.757
.758
.758
.750
.750
.750
.750
.750
.750
.751
.750
.750
.751
.751
.750
.748
.749
.749
.749
.749
.749
.680
.680
.680
.678
.678
.678

.809
.808
.808
.806
.806
.805
.814
.811
.811
.814
.810
.810
.798
.799
.799
.809
.810
.811
.723
.723
.722
.718
.722
.721
.725
.724
.724
.725
.724
.723
.714
.714
.714
.720
.721
.721
.704
.702
.702
.696
.695
.695

.765
.765
.765
.760
.763
.762
.767
.765
.765
.765
.765
.765
.759
.755
.756
.766
.768
.766
.769
.769
.769
.769
.769
.769
.769
.769
.769
.769
.768
.770
.767
.767
.767
.767
.770
.769
.611
.614
.612
.617
.616
.616

.895
.894
.894
.902
.893
.894
.898
.898
.897
.901
.898
.899
.895
.904
.902
.892
.889
.894
.941
.941
.941
.943
.941
.942
.943
.943
.943
.943
.944
.941
.942
.944
.943
.943
.938
.939
.377
.367
.372
.342
.344
.343

.825
.825
.825
.825
.823
.823
.827
.826
.826
.827
.826
.826
.821
.823
.823
.824
.824
.825
.846
.846
.846
.847
.846
.846
.847
.847
.847
.847
.847
.846
.846
.846
.846
.846
.845
.846
.466
.459
.462
.438
.438
.438

.450
.448
.448
.443
.442
.440
.455
.451
.451
.455
.451
.451
.434
.433
.434
.448
.450
.449
.239
.237
.239
.237
.238
.235
.239
.237
.238
.238
.236
.239
.230
.230
.230
.230
.236
.236
.270
.268
.268
.257
.257
.256

.403
.404
.404
.405
.405
.405
.400
.402
.402
.400
.402
.402
.408
.408
.408
.403
.403
.402
.413
.413
.413
.414
.413
.414
.413
.413
.413
.412
.413
.413
.415
.415
.416
.414
.414
.414
.452
.453
.453
.454
.454
.454

Table 30: Hardware result comparison

Dataset

Model

Hardware

Acc AUC Prec Recall

F1

MCC RMSE

ASSISTments 2017

LSTM-DKT

CPU-tf2.1.0
CPU-tf2.6.2
GPU

SAKT

LSTM-DKT-S+ CPU-tf2.1.0
CPU-tf2.6.2
GPU
CPU-tf2.1.0
CPU-tf2.6.2
GPU
CPU-tf2.1.0
CPU-tf2.6.2
GPU
CPU-tf2.1.0
CPU-tf2.6.2
GPU

Vanilla-DKT

DKVMN

DKVMN-Paper CPU-tf2.1.0
CPU-tf2.6.2
GPU
CPU-tf2.1.0
CPU-tf2.6.2
GPU

LSTM-DKT

SAKT

LSTM-DKT-S+ CPU-tf2.1.0
CPU-tf2.6.2
GPU
CPU-tf2.1.0
CPU-tf2.6.2
GPU
CPU-tf2.1.0
CPU-tf2.6.2
GPU
CPU-tf2.1.0
CPU-tf2.6.2
GPU

Vanilla-DKT

DKVMN

DKVMN-Paper CPU-tf2.1.0
CPU-tf2.6.2
GPU
CPU-tf2.1.0
CPU-tf2.6.2
GPU

LSTM-DKT

SAKT

LSTM-DKT-S+ CPU-tf2.1.0
CPU-tf2.6.2
GPU
CPU-tf2.1.0
CPU-tf2.6.2
GPU
CPU-tf2.1.0
CPU-tf2.6.2
GPU
CPU-tf2.1.0
CPU-tf2.6.2
GPU

Vanilla-DKT

DKVMN

69

IntroProg

Statics

Synthetic-K2

.692
.689
.689
.692
.690
.689
.672
.669
.669
.681
.683
.683
.756
.756
.756
.754
.755
.755
.757
.755
.755
.755
.753
.754
.754
.754
.754
.752
.754
.753
.807
.807
.807
.803
.802
.802
.807
.806
.806
.806
.806
.807
.801
.801
.801
.804
.804
.805
.806
.806
.806

.723
.719
.719
.723
.719
.719
.661
.655
.656
.703
.708
.709
.827
.826
.826
.825
.824
.824
.827
.824
.824
.826
.823
.824
.825
.824
.824
.821
.823
.822
.825
.823
.823
.812
.809
.809
.824
.823
.821
.823
.822
.822
.808
.808
.811
.815
.818
.819
.872
.872
.872

.630
.620
.619
.626
.619
.621
.617
.609
.610
.615
.612
.613
.757
.757
.757
.752
.757
.756
.754
.751
.752
.752
.753
.753
.743
.755
.754
.756
.749
.746
.835
.837
.837
.831
.832
.832
.832
.832
.832
.832
.833
.835
.827
.828
.830
.828
.830
.832
.852
.852
.852

.412
.418
.417
.420
.422
.414
.298
.292
.294
.374
.398
.397
.755
.753
.753
.759
.753
.755
.762
.763
.761
.761
.752
.755
.776
.753
.753
.746
.764
.766
.932
.929
.929
.933
.930
.930
.937
.936
.934
.936
.934
.932
.936
.935
.930
.938
.936
.934
.867
.868
.868

.498
.499
.497
.502
.502
.496
.397
.391
.393
.464
.481
.480
.756
.755
.755
.755
.754
.755
.758
.757
.756
.756
.752
.754
.759
.753
.753
.751
.756
.756
.881
.880
.880
.879
.878
.878
.881
.881
.880
.881
.881
.881
.878
.878
.878
.880
.880
.880
.860
.860
.860

.302
.297
.295
.302
.298
.296
.235
.227
.229
.271
.279
.280
.490
.490
.490
.486
.487
.487
.491
.488
.488
.487
.484
.485
.482
.483
.483
.483
.485
.483
.393
.394
.394
.374
.373
.373
.383
.382
.382
.383
.384
.390
.363
.364
.370
.372
.378
.380
.546
.546
.546

.446
.448
.448
.446
.448
.448
.462
.463
.463
.453
.451
.451
.406
.406
.406
.407
.407
.407
.406
.407
.407
.406
.408
.408
.407
.408
.407
.410
.408
.409
.364
.365
.365
.369
.371
.371
.365
.366
.366
.365
.366
.366
.371
.371
.371
.369
.368
.367
.365
.365
.365

Table 30: Hardware result comparison

Dataset

Model

Hardware

Acc AUC Prec Recall

F1

MCC RMSE

.806
.806
.806
.805
.805
.805
.806
.806
.806
.806
.806
.806
.804
.804
.803
.754
.754
.754
.753
.753
.753
.752
.751
.751
.751
.751
.751
.753
.752
.753
.750
.748
.749

.872
.872
.872
.871
.871
.870
.871
.871
.871
.872
.872
.872
.869
.868
.868
.829
.829
.829
.829
.829
.829
.827
.825
.825
.826
.825
.825
.828
.828
.828
.822
.822
.821

.853
.852
.852
.848
.853
.852
.851
.853
.853
.855
.857
.857
.851
.854
.856
.812
.812
.812
.812
.813
.813
.807
.803
.804
.805
.804
.804
.809
.817
.807
.805
.804
.804

.865
.867
.867
.872
.865
.865
.868
.865
.866
.862
.860
.860
.867
.860
.857
.775
.775
.775
.773
.771
.771
.779
.782
.780
.779
.780
.780
.777
.764
.779
.778
.774
.777

.859
.859
.859
.860
.859
.859
.859
.859
.859
.859
.858
.858
.858
.857
.857
.793
.793
.793
.792
.791
.791
.793
.792
.792
.792
.792
.792
.793
.789
.793
.791
.789
.790

.547
.546
.546
.541
.545
.544
.545
.546
.547
.548
.549
.549
.542
.543
.544
.491
.491
.491
.490
.490
.490
.485
.482
.483
.483
.482
.482
.488
.492
.486
.481
.478
.479

.365
.365
.365
.366
.366
.366
.366
.365
.365
.365
.366
.366
.367
.368
.367
.404
.404
.404
.405
.405
.405
.406
.407
.407
.406
.407
.407
.405
.406
.405
.409
.409
.409

Synthetic-K2

DKVMN-Paper CPU-tf2.1.0
CPU-tf2.6.2
GPU
CPU-tf2.1.0
CPU-tf2.6.2
GPU

LSTM-DKT

Synthetic-K5

SAKT

LSTM-DKT-S+ CPU-tf2.1.0
CPU-tf2.6.2
GPU
CPU-tf2.1.0
CPU-tf2.6.2
GPU
CPU-tf2.1.0
CPU-tf2.6.2
GPU
CPU-tf2.1.0
CPU-tf2.6.2
GPU

Vanilla-DKT

DKVMN

DKVMN-Paper CPU-tf2.1.0
CPU-tf2.6.2
GPU
CPU-tf2.1.0
CPU-tf2.6.2
GPU

LSTM-DKT

SAKT

LSTM-DKT-S+ CPU-tf2.1.0
CPU-tf2.6.2
GPU
CPU-tf2.1.0
CPU-tf2.6.2
GPU
CPU-tf2.1.0
CPU-tf2.6.2
GPU

Vanilla-DKT

70

