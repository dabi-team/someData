Differentiable Logic Machines

Matthieu Zimmer, Xuening Feng, Claire Glanois, Zhaohui Jiang, Jianyi Zhang, Paul Weng, Senior Member, IEEE,
Dong Li, Member, IEEE, Jianye Hao, Member, IEEE, and Wulong Liu, Member, IEEE

1

1
2
0
2
c
e
D
2
1

]
I

A
.
s
c
[

4
v
9
2
5
1
1
.
2
0
1
2
:
v
i
X
r
a

Abstract—The integration of reasoning, learning, and decision-
making is key to build more general AI systems. As a step
in this direction, we propose a novel neural-logic architecture
that can solve both inductive logic programming (ILP) and deep
reinforcement learning (RL) problems. Our architecture deﬁnes
a restricted but expressive continuous space of ﬁrst-order logic
programs by assigning weights to predicates instead of rules.
Therefore, it is fully differentiable and can be efﬁciently trained
with gradient descent. Besides, in order to solve more efﬁciently
RL problems, we propose a novel critic architecture that enables
actor-critic algorithms. Compared to state-of-the-art methods on
both ILP and RL problems, our proposition achieves excellent
performance, while being able to provide a fully interpretable
solution and scaling much better, especially during the testing
phase.

I. INTRODUCTION

F OLLOWING the successes of deep learning and deep

reinforcement learning, a research trend [1–3], whose
goal is to combine reasoning, learning, and decision-making
into one architecture has become very active. This research
may unlock the next generation of artiﬁcial intelligence (AI)
[4, 5]. Simultaneously, a second research trend has ﬂourished
under the umbrella term of explainable AI [6]. This trend
is fueled by the realization that solutions obtained via deep
learning-based techniques are difﬁcult to understand, debug,
and deploy.

Neural-logic approaches (see Section IV) have been pro-
posed to integrate reasoning and learning, notably via ﬁrst-
order logic and neural networks. Recent works have demon-
strated good achievements by using differentiable methods
to learn a logic program [7] or by applying a logical in-
ductive bias to create a neural-logic architecture [1]. The
latter approach obtains the best performance at the cost of
interpretability, while the former can yield an interpretable
solution, but at the cost of scalability. Since interpretability
is crucial, especially in high-stake domains, one important
research problem regards the design of an efﬁcient method
with good performance while preserving interpretability.

In this paper, we propose a novel neural-logic architecture
(see Section II for background notions and Section III for
our proposition) that offers a better tradeoff in terms of in-
terpretability vs performance and scalability. This architecture
deﬁnes a continuous relaxation over ﬁrst-order logic expres-
sions deﬁned on input predicates. In contrast to most previous

M. Zimmer, X. Feng, C. Glanois, Z. Jiang and P. Weng are with the Uni-
versity of Michigan-Shanghai Jiao Tong University Joint Institute, Shanghai
Jiao Tong University, China.

P. Weng is also with the Department of Automation, Shanghai Jiao Tong

University, China.

D. Li, J. Hao and W. Liu are with Huawei Noah’s Ark Lab, China.
Corresponding author: paul.weng@sjtu.edu.cn

approaches (see Section IV), one key idea is to assign learnable
weights on predicates instead of template rules, which allows
for a much better scalability. We also introduce several training
techniques to ﬁnd interpretable solutions. Besides, for deep
reinforcement learning (RL), we propose an adapted critic
to train our architecture in an actor-critic scheme for faster
convergence.

We

our

experimentally

proposition with
compare
previously-proposed neuro-logic architectures on inductive
logic programming (ILP) and RL tasks (see Section V).
Our architecture achieves state-of-the-art performances in
ILP and RL tasks while maintaining interpretability and
achieving better scalability. More precisely, our proposition is
superior to all interpretable methods in terms of success rates,
computational time, and memory consumption. Compared to
non-interpretable ones, our method compares favorably, but
can ﬁnd fully-interpretable solutions (i.e., logic program) that
are faster and use less memory during the testing phase.

The contributions of this paper can be summarized as
follows: (1) a novel neural-logic architecture that can produce
an interpretable solution and that can scale better than state-
of-the-art methods, (2) an algorithm to train this architecture
and obtain an interpretable solution, (3) an adapted critic for
the deep RL setting, and (4) a thorough empirical evaluation in
both ILP and RL tasks. Hence, to the best of our knowledge,
our approach is the ﬁrst neuro-logic method able to output at
the end of training a fully-interpretable solution for complex
tasks like Path.

II. BACKGROUND

In this section, we present the necessary notions in inductive
logic programming and reinforcement learning. We also recall
neural logic machines, since our work is based on it.

Notations: For any ﬁnite set X, let ∆pXq denote the set
of probability distributions over X. For any n P N, let rns
denote the set t1, 2, . . . , nu.

A. Inductive Logic Programming

Inductive Logic Programming (ILP) [8] refers to the prob-
lem of learning a logic program that entails a given set
of positive examples and does not entail a given set of
negative examples. This logic program is generally written in
(a fragment of) ﬁrst-order logic.

First-Order Logic (FOL) is a formal language deﬁned with
several elements: constants, variables, functions, predicates,
and formulas. Constants correspond to the objects in the
domain of discourse. Let C denote the set of m constants. They
will be denoted in lowercase. Variables refer to unspeciﬁed
objects. They will be denoted in uppercase. Like previous

 
 
 
 
 
 
works, we consider a fragment of FOL without any functions.
An r-ary predicate P can be thought of as a relation between
r constants, which can be evaluated as true T or false F.
Predicates will be denoted in uppercase. Let P denote the
set of predicates used for a given problem. An atom is an r-
ary predicate with its arguments P px1, ¨ ¨ ¨ , xrq where xi’s are
either variables or constants. A formula is a logical expression
composed of atoms, logical connectives (e.g., negation (cid:32),
conjunction ^, disjunction _, implication Ð), and possibly
existential D and universal @ quantiﬁers.

Since solving an ILP task involves searching an expo-
nentially large space, this problem is generally handled by
focusing on formulas of restricted forms, such as a subset of
if-then rules, also referred to as clauses. A deﬁnite clause is
a rule of the form:

B Ð A1 ^ . . . ^ Ak

which means that the head atom B is implied by the conjunc-
tion of the body atoms A1, . . . , Ak. More general rules can
be deﬁned by allowing logical operations (e.g., disjunction or
negation). A ground rule (resp. ground atom) is a rule (resp.
atom) whose variables have been all replaced by constants.

In ILP tasks, given some initial

input predicates (e.g.,
ZeropXq, SuccpX, Y q for natural numbers), and a target
predicate (e.g., Even), the goal is to learn a logical formula
deﬁning the target predicate. It is usually judicious to proceed
incrementally and introduce in P some auxiliary predicates,
for which we also have to learn an explicit and consistent
deﬁnition. Below, we show a simple example of such predicate
invention with created predicate Succ2:

EvenpXq Ð ZeropXq _

Succ2pX, Y q ^ EvenpY q

`

˘

Succ2pX, Y q Ð SuccpX, Zq ^ SuccpZ, Y q

B. Reinforcement Learning

The Markov Decision Process (MDP) model [9] is deﬁned
as a tuple pS, A, T, r, µ, γq, where S is a set of states, A
is a set of actions, T : S ˆ A Ñ ∆pSq is a transition
function, r : S ˆ A Ñ R is a reward function, µ P ∆pSq
is a distribution over initial states, and γ P r0, 1q is a discount
factor. A (stationary Markov) policy π : S Ñ ∆pAq is a
mapping from states to distributions over actions; πpa | sq
stands for the probability of taking action a given state s.
We consider parametrized policies πθ with parameter θ (e.g.,
neural networks). The aim in discounted MDP settings, is to
ﬁnd a policy that maximizes the expected discounted total
reward:

” 8ÿ

ı

Jpθq “ Eµ,T,πθ

γtrpst, atq

(1)

t“0

where Eµ,T,πθ is the expectation w.r.t. distribution µ, transi-
tion function T , and policy πθ. The state value function of a
policy πθ for a state s is deﬁned by:

V θpsq “ ET,πθ

” 8ÿ

ı
γtrpst, atq | s0 “ s

(2)

t“0

2

where ET,πθ is the expectation w.r.t. transition function T and
policy πθ. The action value function is deﬁned by:

Qθps, aq “ ET,πθ

” 8ÿ

ı
γtrpst, atq | s0 “ s, a0 “ a

t“0

and the advantage function is deﬁned by:

Aθps, aq “ Qθps, aq ´ V θpsq.

(3)

(4)

Reinforcement learning (RL) [10], which is based on MDP,
is the problem of learning a policy that maximizes the expected
discounted sum of rewards without knowing the transition and
reward functions. Policy gradient (PG) methods constitute a
widespread approach for tackling RL problems in continuous
or large state-action spaces. They are based on iterative updates
of the policy parameter in the direction of a (policy) gradient
expressed as:

∇θJpθq “ E

ps,aq„dπθ rAθps, aq∇θ log πθpa | sqs

where the expectation is taken w.r.t to dπθ , the stationary
distribution of the Markov chain induced by policy πθ. Algo-
rithms like REINFORCE [11] that estimate this gradient via
Monte Carlo sampling suffer from high variance. To address
this issue, actor-critic (AC) schemes [12] have been proposed.
In such a framework, both an actor (πθ) and a critic (e.g.,
Aθ or V θ) are jointly learned. Using a critic to estimate the
policy gradient reduces variance, but at the cost of introducing
some bias.

Proximal Policy Optimization (PPO) [13] is a state-of-the-
art actor-critic algorithm, which optimizes a clipped surrogate
objective function JPPOpθq:

8ÿ

t“0

minpωtpθqA

¯θpst, atq, clippωtpθq, (cid:15)qA

¯θpst, atqq,

(5)

where ¯θ is the current policy parameter, ωtpθq “ πθ pat|stq
π ¯θ pat|stq ,
and clipp¨, (cid:15)q is the function to clip between r1 ´ (cid:15), 1 ` (cid:15)s.
This surrogate objective was motivated as an approximation
of that used in Trust Region Policy Optimization (TRPO) [14],
which was introduced to ensure monotonic improvement after
a policy parameter update. Some undeniable advantages of
PPO over TRPO lies in its simplicity and lower computational
and sample complexity.

C. Neural Logic Machines

Neural Logic Machines (NLM) [1] are neural networks
designed with a strong architectural inductive bias to solve
ILP problems. An NLM is comprised of learnable computation
units organized into layers with depth L, each layer having
a breadth B (see Figure 1). An NLM processes predicates
represented as tensors. For any b-ary predicate P , its corre-
sponding tensor P , denoted in bold, is of dimension b with
shape rm, . . . , ms. A value P ri1, . . . , ibs (with ij P rms and
j P rbs) in r0, 1s is interpreted as the probability of the truth
value of the corresponding grounded predicate.

Apart from layer 0, which directly corresponds to the
initial predicates, a computation unit at breadth b P rBs and
layer l P rLs takes as input b-ary predicates and outputs new

3

using SLPs, NLM cannot provide an interpretable solution
after training.

III. DIFFERENTIABLE LOGIC MACHINES

In this section, we present our novel neural-logic archi-
tecture, called Differentiable Logic Machines (DLM), which
offers a good trade-off between expressivity and trainability.

A. Internal Logic

Inspired by NLM [1], DLM follows a similar architecture
(see Figure 1) with three important novelties in order to make
the network more interpretable and more expressive. First, the
computation units implemented as MLPs are replaced by logic
modules. Besides, in addition to the three operations used in
NLM (expansion, reduction, and permutation), we introduce
two novel operations called negation and preservation. We
describe those three new points next:
Logic module: In DLM, a logic module at layer l and breadth
b takes as inputs a tensor that stacks the tensor representations
of all the predicates in the set deﬁned in Ql
b. Like NLM, a
module outputs nO new predicates, but in DLM, half of them
(nO{2) are deﬁned with a fuzzy conjunction and the other
half (nO{2) with a fuzzy disjunction. The conjunctions and
disjunctions are expressed over nA atoms from Ql
b. The ten-
sor representation of a conjunctive predicate P l
bpX1, . . . , Xbq
computed by a module with a fuzzy and (for nA “ 2, which
easily extends to nA ą 2) is obtained as follows:

¨

˛

¨

˛

ÿ

˝

P l

b “

wP P

‚d

˝

ÿ

P PQl
b

P 1PQl
b

wP 1P 1

‚

(6)

where d is the component-wise product, wP P r0, 1s and
wP 1 P r0, 1s are learnable weights for selecting predicates
P and P 1 such that
wP 1 “ 1.
Similarly, the tensor representation of a disjunctive predicate
computed by a module with a fuzzy or is deﬁned as follows:

wP “ 1 and

P 1PQl
b

P PQl
b

ř

ř

ř

P l

b “ Q ` Q1 ´ Q d Q1
wP P and Q1 “

ř

P PQl
b

wP 1P 1. Different
where Q “
weights wP ’s (or wP 1) are learned for each conjunction or
disjunction, as a softmax of parameters θP (with temperature
τ as a hyperparameter):

P 1PQl
b

(7)

wP “

ř

exppθP {τ q

P 1PQl
b

exppθP 1{τ q

.

(8)

Negation: To increase the diversity and the expressivity of
the logic program that is learned, we introduce the negation
operation, which is used to augment a set of predicates. After
applying this operation on a set of predicates P, the resulting
set of considered predicates, denoted ηpPq, contains their
negations. On tensor representations, the negation is simply
computed via the involutive function f pxq “ 1 ´ x applied
component-wisely. In DLM, the negation operation is applied
in half of the conjunctions and disjunctions such that the ﬁrst
half of the atoms in those conjunctions and disjunctions are
enforced to be a negation of a predicate from the previous
layer, i.e., for nA “ 2, the ﬁrst sum of (6) and (7) is deﬁned

Fig. 1: High-level architecture of NLM zoomed in around breadth b,
where boxes represent computation units (except for layer 0), blue
arrows correspond to reduction, and yellow arrows to expansion.
The arguments of the predicates corresponding to the inputs of a
computation unit can be permuted.

in

its

not

any

role

play

value,

b-ary predicate

b is denoted pP l
b.

b-ary predicates. In order to deﬁne its input predicates, we
introduce notation P l
b to be the set of all predicates of arity
b at layer l P rLs. This set can be augmented with three
operations: expansion, reduction, and permutation:
Expansion: Any b-ary predicate P can be expanded
ˆP , where the last argument
into a b+1-ary predicate
does
i.e.,
truth
ˆP pX1, . . . , Xb`1q :“ P pX1, . . . , Xbq. The set of expanded
predicates obtained from P l
Reduction: Any b+1-ary predicate P can be reduced
its
into a
last
universal)
existential
i.e., ˇP pX1, . . . , Xbq “ DXb`1P pX1, . . . , Xb`1q
quantiﬁer,
@Xb`1P pX1, . . . , Xb`1q.
(resp.
“
Those operations on tensors are performed by a max
(resp. min)
i.e.,
ˇP pi1, . . . , ibq “ maxj P pi1, . . . , ib, jq (resp. ˇP pi1, . . . , ibq “
minj P pi1, . . . , ib, jq). The set of reduced predicates obtained
from P l
Permutation: Let Sb be the set of all permutations of rbs for
b P rBs. For a given b-ary predicate and a given permutation
σ P Sb, PσpX1, . . . , Xbq :“ P pXσp1q, . . . , Xσpbqq. Permuting
arguments allow to build more expressive formulas.

ˇP by marginalizing out

b`1 is denoted qP l

ˇP pX1, . . . , Xbq

argument with

corresponding

dimension,

(resp.

b`1.

the

on

an

b Y

b “ tPσ | P P P l´1
0 “ H and qP l

The input predicates of a computation unit at layer l and
pP l´1
breadth b are the elements of Ql
b´1 Y
qP l´1
b`1, σ P Sbu. By convention, pP l
B`1 “ H.
The input tensor of a computation unit at layer l and breadth
b is the tensor obtained by stacking the tensor representation
of the predicates in Ql
b. A computation unit outputs nO new
predicates, where each one is computed with a single-layer
perceptron (SLP) on a ﬁxed grounding of all predicates in
Ql
b.
Thus, NLM offers an expressive neural network architecture
that simulates forward chaining and where predicate invention
is performed via SLPs. At a high-level, the architecture is
deﬁned by setting the number of layers L, the breadth B,
and the number nO of output predicates of the computation
units. Interestingly, the network is independent of the number
of objects and therefore a trained network can be applied
on new instances with any number of objects. However, by

b and Rl

b “ ηpQl

bq instead of Ql

b. In summary, half of the
b, the other half
b, and similarly,

over Rl
conjunctions (nO{4) are deﬁned only on Ql
b instead of Ql
(nO{4) has half of its atoms in Rl
for the disjunctions.
Preservation: To add more ﬂexibility in the architecture, we
b with T (resp. F) for the conjunctions
augment Ql
(resp. disjunctions). Thus, after the preservation operation, the
sums of (6) (resp. (7)) are deﬁned over tTu Y Ql
b or tTu Y Rl
b
(resp. tFu Y Ql
b). The rationale for introducing
those constant predicates T or F is notably to allow a predicate
at one layer to be preserved for a later layer. For instance, for
nA “ 2, if wT “ 1 in (6) (resp. wF “ 1 in (7)), the predicate in
the conjunction with T (resp. disjunction with F) is preserved
as input to the next layer.

b or tFu Y Rl

Example 1. Consider a blocks world environment with 4
objects: 3 blocks pu, v, wq and the ﬂoor pf loorq. Assuming
that only two initial predicates are available, the following
facts tOnpu, vq, Onpv, f loorq, Onpw, f loorq, Toppuq, Toppwqu
are encoded by two tensors. The ﬁrst tensor encoding the
unary predicate ToppXq is a vector of length 4, since there
are 4 objects. The second tensor for the binary predicate
OnpX, Y q is a 4 ˆ 4-matrix. Those two tensors will feed
the DLM network at layer 0 on different breadths (1 and 2
respectively).
We ﬁrst

layer of
breadth 1 (l “ 1, b “ 1). By assumption,
there is no
expansion. Since the previous layer l ´ 1 with breadth
b ` 1 is not empty, the reduction operation generates two
predicates OnR1pXq Ð @Y, OnpX, Y q and OnR2pXq Ð
DY, OnpX, Y q. Hence, the possible (positive) input predicates
R1
1 are tToppXq, OnR1pXq, OnR2pXqu. Since we deal with
unary predicates, the permutation operation does not play a
role.

focus on what happens in the ﬁrst

Now, consider the ﬁrst layer of breadth 2 (l “ 1, b “ 2). By
assumption, there is no reduction. The expansion operation
generates one predicate T opEpX, Y q whose truth value is
given by T oppXq. Therefore, after the permutation operation,
R1
2 “ tOnpX, Y q, TopEpX, Y q, OnpY, Xq, TopEpY, Xqu.
Assume that we want to detect the objects that are neither
the ﬂoor, nor on the top of a stack (i.e., v here) with a unary
predicate, which we call BlockNotToppXq. This predicate can
be invented as a conjunction with a negative ﬁrst atom.
As |R1
length 4 due
to the preservation operation. To build BlockNotToppXq Ð
(cid:32)T oppXq^OnR2pXq, wP should be zero everywhere except
for selecting (cid:32)T oppXq in tTu Y ηpRl
bq. Accordingly, wP 1
should be zero everywhere except
for selecting OnR2pXq
inside tTu Y Rl
b.

1| “ 3, wP and wP 1 are vectors of

Following NLM, we keep the probabilistic interpretation
of the tensor representations of the predicates in DLM. This
interpretation justiﬁes the application of statistical machine
learning techniques to train DLM via cross-entropy minimiza-
tion for supervised tasks (i.e., ILP) and via policy gradient for
RL tasks. In this interpretation, using the fuzzy conjunction
and disjunction can be understood as making the assumption
of probabilistic independence between the truth values of any
pairs of atoms in Rl
b. This may seem a strong assumption,

however this is not detrimental since we want to learn a logic
program operating on Boolean tensors.

4

a) Complexity: In one module, the number of parameters
grows as OppnAnOq where p is the number of input predicates
of the module, nA is the number of atoms used in a predicate,
and nO is the number of output predicates of the module.
In comparison with related works (see Section IV), to obtain
the same expressivity, the alternative approach BILP [7] (and
thus its extension to RL, NLRL [2]) would need Op
ˆnOq
because the weights are deﬁned for all the nA-combinations of
p predicates. In contrast, dNL-ILP and NLM would be better
with only OppnOq. However, the modules of dNL-ILP and
NLM are not really comparable to those of our model or BILP.
Indeed, the NLM modules are not interpretable, and the dNL-
ILP architecture amounts to learning a CNF formula, where
a module corresponds to a component of that formula. While
expressive, the space of logic program induced in dNL-ILP is
much less constrained than in our architecture, making it much
harder to train, as shown in our experiments (see Table I).

p
nA

`

˘

Assuming that B (the maximum arity) is a small con-
2q parameters where
stant, the complete DLM has OpLnAnO
L is the maximum number of layers. NLM has instead
2q parameters. The complexity of a forward pass is
OpLnO
OpmBLnAnO
2q where m is the number of objects. For NLM,
it is OpmBLnO
2q. Once a logic program is extracted (see
Section III-D), we can reduce the forward pass complexity to
OpmBpq where p is the number of element in the graph. Note
that, by construction, p ď LnO

2 ď LnAnO

2.

B. Training

DLM deﬁnes a continuous relaxation over ﬁrst-order logic
programs. DLM expressivity can not only be controlled
by setting hyperparameters L, B, nA, and nO but also by
restricting the inputs of logic modules (e.g., no negation or no
existential or universal quantiﬁers). A priori knowledge can
be injected in this architecture by choosing different values
for B at each layer, different values for nO in each module,
or removing some inputs of logic modules. Note that DLM is
independent of the number of objects: it can be trained on a
small number of objects and generalize to a larger number.

For supervised learning tasks, with positive and negative
examples, the loss is simply a binary cross-entropy loss. As
reported in previous neural-logic works, this loss function
generally admits many local optima. Besides, there may be
global optima that reside in the interior of the continuous
relaxation (i.e., not interpretable). Therefore, if we train our
model with a standard supervised (or RL training) technique,
there is no reason that an interpretable solution would be
obtained, if we manage to completely solve the task at all.

In order to help training and guide the model towards an
interpretable solution, we propose to use three techniques: (a)
inject some noise in the softmax deﬁned in (8), (b) decrease
temperature τ during training, and (c) use dropout. For the
noise, we use a Gumbel distribution. Thus, the softmax in (8)
is replaced by a Gumbel-softmax [15]:

wP “

ř

expppGP ` θP q{τ q

P 1PQl
b

expppGP 1 ` θP 1q{τ q

(9)

5

is in the i-th position of P . Outputs P i
r ’s are then combined
with another GRU to provide an output for arity r. All those
outputs are then given as inputs to a single-layer perceptron
that estimates the value of the current state. Note that this
critic, like DLM, is independent of the number of objects.

For the actor, the output should ideally correspond to a
predicate that evaluates to true for only one action and false for
all other actions, which corresponds to a deterministic policy.
For instance, in a blocks world domain, the target predicate
would be movepX, Y q and would be true for only one pair of
objects, corresponding to the optimal action, and false for all
other pairs. While not impossible to achieve (at least for certain
small tasks), an optimal deterministic policy may involve an
unnecessarily complex logic program. Indeed, for instance, for
the blocks world domain, in many states, there are several
equivalent actions, which the deterministic policy would have
to order.

Thus, as done in previous works, we separate the reasoning
part and the decision-making part. The reasoning part follows
the architecture presented in Figure 1, which provides a tensor
representing the target predicate corresponding to the actions.
A component of this tensor can be interpreted as whether the
respective action is good or not. The decision part takes as
input this tensor and outputs a probability distribution over
the actions using a softmax with ﬁxed low temperature. Note
that this temperature used for the actions is different from that
used in (9).

D. Logic Program Extraction

During evaluation and deployment, both the time and space
complexity will increase quickly as the number of objects
increases. To speed up inference and have an interpretable
model, we post-process the trained model to extract the logical
formula instead of using it directly. For each used module,
we replace the Gumbel-softmax (9) by an argmax to choose
the predicates deterministically. The fuzzy operations can then
be replaced by their corresponding Boolean ones. Formula
extraction can be done recursively from the output of the
model. All the non-selected inputs predicates coming from the
previous layer do not need to be computed. A graph containing
only the selected predicates is built from the output to the
input predicate. The extracted interpretable model can then
operate on Boolean tensors, which further saves space and
computation time.

IV. RELATED WORK

The literature aiming at integrating reasoning, learning, and
possibly decision-making is very rich. Our work is related
to statistical relational AI [17], which aims at combining
relational reasoning and learning. However, a key difference
is that our focus is to learn a logic program, although we
have a probabilistic interpretation of predicate tensors. Our
work is also related to relational RL [18–20], whose goal is
to combine RL with FOL representation and ILP. To the best
of our knowledge, such approach does not scale as well as
those resorting to neural networks. Thus, the investigation of
neural approaches to tackle this integration has become very

Fig. 2: Architecture of our critic with B “ 2. Additional observable
predicates with lower arity can be introduced in the deeper layers
by concatenation. The architecture generalizes to higher depth by
introducing more initials GRUs, for instance, 3 GRUs for depth 3,
etc. The number of parameters of the critic is independent of the
number of objects and only depends on the number of predicates.

where GP (and GP 1) are i.i.d. samples from a Gumbel distri-
bution Gumbelp0, βq. The injection of noise during training
corresponds to a stochastic smoothing technique: optimizing
by injecting noises with gradient descent amounts to perform-
ing stochastic gradient descent on a smoothed loss function.
This helps avoid early convergence to a local optimum and
ﬁnd a global optimum. The decreasing temperature favors the
convergence towards interpretable solutions. To further help
learn an interpretable solution, we additionally use dropout
during our training procedure. Dropout helps learn more inde-
pendent weights, which promotes interpretability. The scale β
of the Gumbel distribution and the dropout probability is also
decreased with the temperature during learning.

C. Actor-Critic

For RL tasks, the objective function is deﬁned from the
sparse reward obtained when a task is solved. Here, the goal
is to learn a policy that maximizes the expected cumulative
rewards. To the best of our knowledge, all previous neural-
logic works1 rely on REINFORCE [11] instead of an actor-
critic (AC) algorithm, which can be generally much more
sample-efﬁcient. One reason may be the difﬁculty of designing
a neural network architecture for the critic that can directly
receive as inputs the initial predicates.

To solve this issue, we propose a recurrent neural network
architecture for the critic that can directly take as inputs the
initial predicates. The critic estimates the value of a current
state described by the initial predicates, which are represented
by a tensor. An r-ary predicate can be represented as a tensor
of dimension r with shape rm, . . . , ms. The set of r-ary
predicates can be represented as a tensor Pr of dimension
r`1 with shape rm, . . . , m, |P 0
r |s. The architecture is depicted
in Figure 2. For arity r, r recurrent heads, implemented as a
Gated Recurrent Unit (GRU) [16], read the r-ary predicates,
the i-th head reading the i-th slice of tensor Pr, yielding
an output P i
r |s. Intuitively, the i-th head
computes for each object o and each predicate P a summary
of the objects that o is in relation with according to P when o

r of shape rm, |P 0

1Although not discussed in [2]’s paper, we found in their source code an
attempt to apply an AC scheme, but they do so by converting states into
images, which may not only be unsuitable for some problems, but may also
lose information during the conversion and prevent good generalization.

P2(X,Y)GRUGRUGRUP2(Y,X)P1(X)MLPP0(·)V(s)+++: concatenation+Q1(X)Q0(·)active in recent years (e.g., [21–23]). For space reasons, we
only discuss the recent works closest to ours below.

a) (Differentiable) ILP and their extensions to RL: In-
ductive Logic Programming (ILP) [8, 24] aims to extract lifted
logical rules from examples. Since traditional ILP systems
can not handle noisy, uncertain or ambiguous data, they have
been extended and integrated into neural and differentiable
frameworks. For instance, [7] proposed BILP, a model based
on a continuous relaxation of the logical reasoning process,
such that the parameters can be trained via gradient descent,
by expressing the satisﬁability problem of ILP as a binary
classiﬁcation problem. This relaxation is deﬁned by assigning
weights to templated rules. [2] adapted BILP to RL problems
using vanilla policy gradient. Despite being interpretable, this
approach does not scale well in terms of both memory and
computation, which is notably due to how the relaxation is
deﬁned.

[25] proposed differentiable Neural Logic ILP (dNL-ILP),
another ILP solver where in contrast to BILP, weights are
placed on predicates like in our approach. Their architecture
is organized as a sequence of one layer of neural conjunction
functions followed by one layer of neural disjunction functions
to represent expressions in Disjunctive Normal Form, which
provides high expressivity. In this architecture, conjunctions
and disjunctions are deﬁned over all predicates of any arity
in contrast
to DLM and NLM. [25] did not provide any
experimental evaluation of dNL-ILP on any standard ILP
benchmarks. But, in our experiments, our best effort to eval-
uate it suggests that dNL-ILP performs worse than BILP. We
believe this is due to the too generic form imposed on the
logic program to be learned. [26] extended their model to RL
and showed that initial predicates can be learned from images
if sufﬁcient domain knowledge under the form of auxiliary
rules is provided to the agent. However, they do not show that
their approach can learn good policies without this domain
knowledge.

Another way to combine learning and logic reasoning is
to introduce some logical architectural inductive bias, as in
Neural Logic Machine (NLM) [1]. This approach departs from
previous ones by learning rules with single-layer perceptrons
(SLP), which prevents this method to provide any ﬁnal inter-
pretable solution. NLMs can generalize and its inference time
is signiﬁcantly improved compared to BILP; by avoiding rules
templates as in traditional neuro-symbolic approaches, it also
gains in expressivity. Our architecture is inspired by NLM, but
we use interpretable modules instead of SLPs.

b) Other neuro-symbolic approaches: In order to com-
bine probabilistic logic reasoning and neural networks, [3]
propose DeepProbLog, which extends ProbLog [27], a prob-
abilistic logic language, with neural predicates. While this
approach is shown to be capable of program induction, it is
not obvious how to apply it for solving generic ILP problems,
since partially-speciﬁed programs need to be provided.

Another line of work in relational reasoning speciﬁcally
targets Knowledge-Base reasoning. Although these works
have demonstrated huge gain in scalability (w.r.t number of
predicates or entities), they are usually less concerned about

6

predicate invention2. Some recent works [28, 29] extend the
multi-hop reasoning framework to ILP problems. The latter
work is able to learn more expressive rules, with the use of
nested attention operators. In the KB completion literature, a
recurrent idea is to jointly learn sub-symbolic embeddings of
entities and predicates, which are then used for approximate
inference. However, the expressivity remains too limited for
more complex ILP tasks and these works are typically more
data-hungry.

V. EXPERIMENTAL RESULTS

In this section, we experimentally compare our architecture
with previous state-of-the-art on ILP and RL tasks. Since DLM
is a neuro-symbolic architecture, we only compare with neuro-
symbolic approaches. For ILP, we evaluate BILP [7], NLM [1],
dNL-ILP [30], and our architecture DLM on the family tree
and graph reasoning tasks used in BILP and NLM. Differential
architectures such as MEM-NN [31] or DNC [32] are not
included, since they have been shown to be inferior on ILP
tasks compared to NLM and they furthermore do not provide
any interpretable solutions. Also, the approaches in multi-hop
reasoning [28, 29] are also left out because although they can
scale well, the rules they can learn are much less expressive,
which prevent them to solve any complex ILP tasks in an
interpretable way. Besides, DeepProbLog [3], which is based
on backward chaining, is not included since it is not easy to
perform predicate invention with it. For RL, we compare DLM
with the best baselines as measured on the ILP tasks, namely
NLM and NLRL [2], which is an extension of BILP to RL, on
several variants of blocks world tasks from NLRL and NLM,
in addition to two other tasks Sorting and Path from NLM.
More details about the ILP and RL tasks are given below and
in Appendix A. The speciﬁcations of the computers used for
training are provided in Appendix B-A.

We present three series of experiments. The ﬁrst two series
demonstrate how well our method performs on ILP and RL
tasks in terms of success rates and in terms of computational
times and memory usage during training and testing compared
to the other baselines. The last series of experiments is an
justiﬁes the different components (i.e.,
ablation study that
critic, Gumbel-softmax, dropout) of our method.

A. ILP Tasks

Since the authors of BILP did not release their source code,
we only reported the performance of BILP given in the NLM
paper, and could not compute its percentage of successful
seeds nor its training/testing time. For NLM and dNL-ILP, we
use the source codes shared by their authors. We will share the
source code of DLM in the ﬁnal version of the submission.

a) Task Performance: For the ILP tasks, we report in
Table I the success rates of the different methods on two
family tree and graph reasoning. In the family
domains:
tree domain, different
tasks are considered corresponding
target predicates to be learned from an input
to different

2Typically, rules learned in multi-hop reasoning are chain-like rules (i.e.,
paths on graphs), which form a subset of Horn clauses: QpX, Y q Ð
P1pX, Z1q ^ P2pZ1, Z2q ^ ¨ ¨ ¨ PrpZr´1, Zrq.

7

TABLE I: Success rates (%) and percentage of successful seeds of dNL-ILP, BILP, NLM, and DLM on the family tree and graph reasoning
tasks.

Family Tree m “ 20 M “ 100 PSS m “ 20 M “ 100 m “ 20 M “ 100 PSS m “ 20

dNL-ILP

BILP

NLM

DLM (Ours)

M “ 100

HasFather
HasSister
IsGrandparent
IsUncle
IsMGUncle

100
100
100
97.32
99.09

100
100
100
96.77
N/A

100
40
80
0
0

100
100
100
100
100

100
100
100
100
100

100
100
100
100
100

100
100
100
100
100

100
100
100
90
20

100 p˘0q
100 p˘0q
100 p˘0q
100 p˘0q
100 p˘0.01q

100 p˘0q
100 p˘0q
100 p˘0q
100 p˘0q
100 p˘0.08q

Graph

m “ 10 M “ 50

PSS m “ 10 M “ 50 m “ 10 M “ 50

PSS m “ 10

M “ 50

AdjacentToRed
4-Connectivity
6-Connectivity
1-OutDegree
2-OutDegree

100
91.36
92.80
82.00
83.39

100
85.30
N/A
78.44
8.24

100
0
0
0
0

100
100
100
100
N/A

100
100
100
100
N/A

100
100
100
100
100

100
100
100
100
100

90
100
60
100
100

100 p˘0.02q
100 p˘0q
100 p˘0.00q
100 p˘0q
100 p˘0q

100 p˘0.01q
100 p˘0q
100 p˘0q
100 p˘0q
100 p˘0q

PSS

100
100
100
100
70

PSS

90
100
90
100
100

1 PSS: Percentage of Successful Seeds reaching 100% of success rates on the training instances.

graph where nodes representing individuals are connected with
relations: IsMotherpX, Y q, IsFatherpX, Y q, IsSonpX, Y q, and
IsDaughterpX, Y q. The target predicates are HasFather, Has-
Sister, IsGrandParent, IsUncle, and IsMGUncle (i.e., maternal
great uncle). In the graph reasoning domain, the different
target predicates to be learned from an input graph are
AdjacentToRed, 4-Connectivity, 6-Connectivity, 1-OutDegree,
2-OutDegree (see Appendix A-A for their deﬁnitions).

The success rates are computed as the average over 250
random instances (i.e., family tree or graph) with the best
model over 10 random seeds. Those 10 random seeds are also
used to compute the Percentage of Successful Seeds (PSS),
which is reported in Table I.

In Table I, we also report the performance results of BILP
and NLM as given by [1]. For dNL-ILP, [30] did not evaluate
their method in any standard ILP tasks. Using their source
code, we did our best to ﬁnd the best set of hyperparameters
(see Appendix B-C) for each ILP task. N/A means that the
method ran out of memory. For dNL-ILP, the memory issue
comes from the fact that, both learning auxiliary predicates and
increasing the number of variables of predicates will increase
memory consumption sharply with a growing number of nodes
(see details in Appendix B-C1).

The experimental results demonstrate that previous inter-
pretable methods dNL-ILP and BILP do not scale to difﬁcult
ILP tasks and to larger number of objects. However, our
method can solve all the ILP tasks like NLM, while our
method can in addition provide an interpretable rule in contrast
to NLM. Moreover, we can observe that DLM is more stable,
the other methods
in terms of successful seeds,
including NLM.

than all

b) Computational Performance: We compare now the
different algorithms with respect to computational times and
memory usage during training (see Table X in Appendix) and
testing (see Figure 3).

B. RL Tasks

We evaluate NLRL, NLM and our method DLM on 6 RL
domains: On the ﬁrst three, Stack, Unstack, and On [2], the

in IsGrandParent (top) and 2-
Fig. 3: Comparison during test
outdegree (bottom): (left) Computational time; (right) Memory usage.
On 2-outdegree, NLM is rapidly out of memory and dNL-ILP does
not achieve a 100% success rate.

agent is trained to learn the binary predicate MovepX, Y q
which moves block X on block (or ﬂoor) Y . The observable
predicates are: IsFloorpXq, ToppXq, and OnpX, Y q with an
additional predicate OnGoalpX, Y q for the On task only. In
Stack, the agent needs to stack all the blocks whatever their
order. In Unstack, the agent needs to put all the blocks on
the ﬂoor. In On, the agent needs to reach the goal speciﬁed
by onGoal. The last three domains are Sorting, Path and
Blocksworld [1]. In Sorting, the agent must learn SwappX, Y q
where X and Y are two elements of a list
to sort. The
binary observable predicates are SmallerIndex, SameIndex,
GreaterIndex, SmallerValue, SameValue and GreaterValue. In
Path, the agent is given a graph as a binary predicate with
a source node and a target node as two unary predicates. It
must learn the shortest path with an unary predicate GoTopXq
where X is the destination node. In Blocksworld, it also learns
MovepX, Y q. This environment is the most complex one, it
features a target world and a source world with numbered
blocks, which makes the number of constants to be 2pm ` 1q

2468101214persons(×10)051015202530Time(s)dNL-ILPNLMDLM(Ours)3456789101112persons(×10)0200040006000800010000Memory(MiB)dNL-ILPNLMDLM(Ours)5101520253035persons(×10)0102030405060708090Time(s)dNL-ILPNLMDLM(Ours)5101520253035persons(×10)0100020003000400050006000Memory(MiB)dNL-ILPNLMDLM(Ours)where m is the number of blocks and 1 corresponds to the
ﬂoor. The agent is rewarded if both worlds match exactly.
The binary observable predicates are SameWorldID, Smaller-
WorldID, LargerWorldID, SameID, SmallerID, LargerID, Left,
SameX, Right, Below, SameY, and Above.

All those domains are sparse reward RL problems. Since the
ﬁrst three domains are relatively simple, they can be trained
and evaluated on ﬁxed instances with a ﬁxed number of blocks.
In contrast, for the last three domains, the training and testing
instances are generated randomly. Those last three domains,
which are much harder than the ﬁrst three, also require training
with curriculum learning, which was also used by [1]. The
difﬁculty of a lesson is deﬁned by the number of objects.
Further details about the training with curriculum learning are
provided in Appendix B-B. After training, we evaluate the
learned model on instances of size m “ 10, but also M “ 50
to assess its generalizability.

Table II provides the success rates of all the algorithms
on different RL tasks. For our architecture, we provide the
results whether we learn an interpretable policy (DLM) or not
(nIDLM). Each subrow of an RL task corresponds to some
instance(s) on which a trained model is evaluated.

The experimental results show that NLRL does not scale
to harder RL tasks, as expected. On Sorting and Path where
we can learn a fully-interpretable policy, DLM is much better
than NLM in terms of computational time and memory usage
during testing. Thus, our architecture is always superior to
NLRL and better than NLM.

For the harder RL tasks (Path, Blocksworld), our method
can reach similar or better performances with a non-
interpretable policy, i.e., if we do not enforce the convergence
to an interpretable policy. However, obtaining an interpretable
policy with curriculum learning (CL) reveals to be difﬁcult:
there is a contradiction between learning to solve a lesson
and converging to a ﬁnal interpretable policy that generalizes.
Indeed, on the one hand, we can learn an interpretable policy
for a lesson with a small number of objects, however that
policy will probably not generalize and keep training that
interpretable policy on the next
lesson is hard since the
softmaxes are nearly argmaxes. On the other hand, we can
learn to solve all the lessons with a non-interpretable policy,
but that ﬁnal policy is hard to turn into an interpretable one,
because of the many local optima in the loss landscape. This
training difﬁculty explains why we did not manage to learn
an interpretable policy for Path and Blocksworld.

Moreover, as we know that the ﬁxed-sized architecture is not
expressive enough to learn complicated formulas for Path or
Blocksworld, we developed a more elaborate training method:
we progressively stack several DLMs. In Table II, we denote
this method as DLM+incr. We train a ﬁrst DLM, make the
temperatures of the softmax tend to a low number, then freeze
its parameters and extract the interpretable formulas. Finally,
we stack a new DLM that takes as inputs the initial predicates
given in the problem plus all the built predicates present in
the previously learned DLM. This process is repeated several
times. The strong advantage of this method is that we can
perform inference with deeper architectures due to the fact
that the modules in the frozen DLM do not need to be fully

8

computed. We can extract and compute only the necessary
ones (see Section III-D). The method helped on Path but
Blocksworld was still too difﬁcult.

We also evaluated DLM+incr on an imitation learning sce-
nario on Blocksworld where the agent tries to copy a working
policy given by a teacher (possibly the one learned by nIDLM
which does not enforce interpretability). With DLM+incr, un-
like vanilla DLM, we were able to extract a good interpretable
policy. It needed a stacking of 4 DLM of depth 8, which shows
the difﬁculty of ﬁnding an interpretable formula for this task.
We leave for future work the investigation of alternative RL
training methods that scale better than CL for sparse-reward
RL problems like Path and Blocksworld.

C. Examples of Interpretable Rules and Policies

As illustration for ILP, we provide the logic program learned
by our method on the task IsGrandParent. We used L “ 5
layers, B “ 3 breadth, nA “ 2 atoms, and nO “ 8 outputs per
logic modules. For better legibility, we give more meaningful
names to the learned rules and remove the expansions and
reductions:

IsChild1pa, bq Ð IsSonpa, bq _ IsDaughterpa, bq
IsChild2pa, bq Ð IsSonpa, bq _ IsDaughterpa, bq
IsGCPpa, b, cq Ð IsChild2pa, cq ^ IsChild2pc, bq
IsGPC1pa, b, cq Ð IsChild1pc, aq ^ IsChildpb, cq
IsGPC2pa, b, cq Ð IsGPC1pa, b, cq _ IsGCPpb, a, cq
IsGPpa, bq Ð DC, IsGPC2pa, b, Cq ^ DC, IsGPC2pa, b, Cq
IsGrandParentpa, bq Ð IsGPpa, bq ^ IsGPpa, bq

The logic program extracted from the trained DLM has redun-
dant parts (e.g., P ^ P ), because we used a relatively large
architecture to ensure sufﬁcient expressivity. Note that being
able to learn interpretable solutions with a large architecture
is a desirable feature when the designer does not know the
solution beforehand. Redundancy could be reduced by using
a smaller architecture, otherwise the redundant parts (e.g.,
P ^ P ) could easily be removed by post-processing the
extracted logic program, as we did. After simpliﬁcation, the
solution is given by the following program, which shows that
the target predicate has been perfectly learned:

IsChildpa, bq Ð IsSonpa, bq _ IsDaughterpa, bq
IsGPC1pa, b, cq Ð IsChildpc, aq ^ IsChildpb, cq
IsGrandParentpa, bq Ð DC, IsGPC1pa, b, Cq

As an illustration for RL, we provide the simpliﬁed logic
program learned by our method on the task On, which corre-
sponds to the output of the reasoning part:

Movepa, bq Ð pOnGoalpa, bq _ IsFloorpbqq^

(cid:32)Onpa, bq ^ Toppaq.

Using this program, the decision-making part (stochastically)
moves blocks to the ﬂoor and moves the good block on its goal
position when it can. The complete logic program is provided
in Appendix C-A, where we provide examples for other tasks
as well.

Being able to ﬁnd solutions in a large architecture is a
desirable feature when the designer does not know the solution
before hand. Besides, note that we directly output an inter-
pretable logic program. In contrast, with previous interpretable

9

TABLE II: Success rates (%) and average rewards of NLRL, NLM, and DLM on RL tasks for the best seeds.

NLRL NLM nIDLM DLM DLM+incr

Success rate

Average rewards
NLRL NLM nIDLM DLM DLM+incr

Unstack

5 variations

Stack

On

Sorting

Path

5 variations

5 variations

m “ 10
M “ 50

m “ 10
M “ 50

Blocks world m “ 10
M “ 50

Blocks world m “ 10
(imitation) M “ 50

100

100

100

97
N/A

N/A

N/A

100

100

100

100
100

100
100

100
100

100

100

100

100
100

100
100

100
100

100

100

100

100
100

´
´

´
´

´
´

100

100

100

100
100

72.8
24.4

´
´

100
100

0.914 0.920

0.920

0.920

0.920

0.877 0.920

0.920

0.920

0.920

0.885 0.896

0.896

0.896

0.896

0.866 0.939
N/A 0.556

N/A 0.970
0.970

N/A 0.888
0.153

0.937
0.556

0.970
0.970

0.904
0.159

0.939
0.559

´
´

´
´

´
´

0.939
0.559

0.695
0.206

´
´

0.894
0.230

N/A: Out of memory issues.

´: Could not extract a working interpretable policy for the given architecture size.

TABLE III: Average ratio of successful seeds on all the tasks of
graphs and family trees leading to a 100% success rate during testing
with interpretable rules. Score computed with 5 seeds for each task.

Successful seeds (%)

Softmax without noise
Constant β and dropout prob.
DLM - Dropout
Gaussian noise
DLM

58
68
70
80
95

models, it can achieve comparable results up to some com-
plexity level, but it generalizes better on problems with few
instances, and more importantly, it scales much better in terms
of computational times and memory usage during testing.

Learning a fully-interpretable policy in RL for more com-
plex tasks is a hard problem. Solving it calls for alternative
training methods that deal with sparse rewards (e.g., Hindsight
Experience Replay [33]), which we plan to explore next in our
future work.

models, logic rules with high weights are extracted to be
inspected. However, those rules may not generalize because
weights are usually not concentrated on one element.

ACKNOWLEDGEMENTS

This work is supported by a funding (HF2020055001) from

Huawei.

D. Ablation study

REFERENCES

In the following part, we report the performance of our
model by removing some of its features. We tried to train our
model by using only softmax without injecting noise, without
decreasing the noise over time, without having a dropout
noise and ﬁnally by replacing the Gumbel distribution with
a Gaussian one. In those experiments, during evaluation, we
are still using an argmax to retrieve the interpretable rules.
Table III shows that all our choices help our model to reach
interpretable rules.

We also performed an ablation study on the effects of using
a critic both in NLM and in DLM. In both architectures,
using a critic improved learning speed, which demonstrates
the quality of our critic. We also evaluated different critic
architectures, the GRU-based critic was found to perform the
best. For space reasons, we provide further details with plots
in Appendix C-C.

VI. CONCLUSION

We proposed a novel neural-logic architecture that is ca-
pable of learning a fully-interpretable solution,
logic
program. It obtains state-of-the-art results for inductive logic
programming tasks, while retaining interpretability and scaling
much better. For reinforcement learning tasks, it is superior to
previous interpretable models. Compared to non-interpretable

i.e.,

[1] H. Dong, J. Mao, T. Lin, C. Wang, L. Li, and D. Zhou,
“Neural Logic Machines,” in International Conference
on Learning Representations, Apr. 2019.

[2] Z. Jiang and S. Luo, “Neural Logic Reinforcement Learn-
ing,” in International Conference on Machine Learning,
Apr. 2019.

[3] R. Manhaeve, S. Dumanˇci´c, A. Kimmig, T. Demeester,
and L. De Raedt, “Deepproblog: Neural probabilistic
logic programming,” in Neural Information Processing
Systems, 2018.

[4] B. M. Lake, T. D. Ullman, J. B. Tenenbaum, and S. J.
Gershman, “Building machines that learn and think like
people,” Behavioral and Brain Sciences, vol. 40, 2017.
[5] G. Marcus, “Deep learning: A critical appraisal,” arXiv:

1801.00631, 2018.

[6] A. Barredo Arrieta, N. D´ıaz-Rodr´ıguez, J. D. Ser, A. Ben-
netot, S. Tabik, A. Barbado, S. Garcia, S. Gil-Lopez,
D. Molina, R. Benjamins, and et al., “Explainable artiﬁ-
cial intelligence (XAI): Concepts, taxonomies, opportu-
nities and challenges toward responsible ai,” Information
Fusion, vol. 58, pp. 82–115, 2020.

[7] R. Evans and E. Grefenstette, “Learning explanatory
rules from noisy data,” Journal of AI Research, 2018.
[8] S. Muggleton, “Inductive logic programming,” New Gen-
eration Computing, vol. 8, no. 4, p. 295–318, Feb 1991.

10

[9] R. Bellman, “A Markovian decision process,” Journal of

mathematics and mechanics, pp. 679–684, 1957.
[10] R. S. Sutton and A. G. Barto, Reinforcement learning:

An introduction. MIT press, 2018.

G. Pinkas, H. Poon, and G. Zaverucha, “Neural-symbolic
learning and reasoning: A survey and interpretation,”
arXiv:1711.03902, 2017.
[Online]. Available: http:
//arxiv.org/abs/1711.03902

[11] R. J. Williams, “Simple statistical gradient-following al-
gorithms for connectionist reinforcement learning,” Ma-
chine Learning, vol. 8, no. 3, pp. 229–256, 1992.
[12] V. Konda and J. Tsitsiklis, “Actor-critic algorithms,”
in Advances in neural information processing systems,
vol. 12. MIT Press, 2000.

[24] A. Cropper, S. Dumanˇci´c, and S. H. Muggleton, “Turn-
ing 30: New ideas in inductive logic programming,” in
International Joint Conference on Artiﬁcial Intelligence,
2020, pp. 4833–4839.

[25] A. Payani and F. Fekri, “Learning algorithms via neural

logic networks,” arXiv:1904.01554, 2019.

[13] J. Schulman, F. Wolski, P. Dhariwal, A. Radford, and
O. Klimov, “Proximal Policy Optimization Algorithms,”
arXiv preprint: 1707.06347, Aug. 2017.

[26] ——, “Incorporating relational background knowledge
into reinforcement learning via differentiable inductive
logic programming,” arXiv:2003.10386, 2020.

[14] J. Schulman, S. Levine, P. Abbeel, M. Jordan, and
P. Moritz, “Trust region policy optimization,” in Inter-
national Conference on Machine Learning, 2015.
[15] E. Jang, S. Gu, and B. Poole, “Categorical reparameter-
ization with gumbel-softmax,” in International Confer-
ence on Learning Representations, 2017.

[16] K. Cho, B. van Merrienboer, C. Gulcehre, D. Bahdanau,
F. Bougares, H. Schwenk, and Y. Bengio, “Learning
phrase representations using rnn encoder-decoder for sta-
tistical machine translation,” in Conference on Empirical
Methods in Natural Language Processing, 2014.
[17] L. De Raedt, K. Kersting, S. Natarajan, and D. Poole,
Statistical Relational AI: Logic, Probability and Compu-
tation. Morgan & Claypool Publishers, 2016.

[18] S. Dˇzeroski, L. De Raedt, and K. Driessens, “Relational
learning,” Machine Learning, vol. 43,
[Online]. Available: https:

reinforcement
no. 1, pp. 7–52, 2001.
//doi.org/10.1023/A:1007694015589

[19] P. Tadepalli, R. Givan, and K. Driessens, “Relational
reinforcement learning : An overview,” in Proceedings
of the International Conference on Machine Learning’04
workshop on relational reinforcement learning, 2004.

[20] M. van Otterlo, “Solving relational and ﬁrst-order
logical markov decision processes: A survey,” in
Reinforcement Learning, M. Wiering and M. van
Otterlo, Eds. Springer Berlin Heidelberg, 2012, vol. 12,
pp. 253–292, series Title: Adaptation, Learning, and
Optimization.
[Online]. Available: http://link.springer.
com/10.1007/978-3-642-27645-3 8

[21] L. de Raedt, S. Dumanˇci´c, R. Manhaeve, and G. Marra,
“From Statistical Relational to Neuro-Symbolic Artiﬁ-
cial Intelligence,” in Proceedings of the Twenty-Ninth
International Joint Conference on Artiﬁcial Intelligence.
Yokohama, Japan: International Joint Conferences on
Artiﬁcial Intelligence Organization, Jul. 2020, pp. 4943–
4950.

[27] L. De Raedt, A. Kimmig, and H. Toivonen, “Problog: a
probabilistic prolog and its application in link discovery,”
in Proceedings of the 20th international joint conference
on Artiﬁcal intelligence, ser. International Joint Confer-
ence on Artiﬁcial Intelligence’07. Morgan Kaufmann
Publishers Inc., Jan 2007, p. 2468–2473.

[28] F. Yang, Z. Yang, and W. W. Cohen, “Differentiable
learning of logical rules for knowledge base reasoning,”
in Neural Information Processing Systems, 2017.
[29] Y. Yang and L. Song, “Learn to explain efﬁciently via
neural logic inductive learning,” in International Confer-
ence on Learning Representations, 2020.

[30] A. Payani and F. Fekri, “Inductive logic program-
logic networks,”

ming via differentiable deep neural
arXiv:1906.03523, 2019.

[31] S. Sukhbaatar, A. Szlam, J. Weston, and R. Fergus,
“End-to-end memory networks,” in Neural Information
Processing Systems, 2015.

[32] A. Graves, G. Wayne, M. Reynolds, T. Harley,
I. Danihelka, A. Grabska-Barwi´nska, S. Colmenarejo,
E. Grefenstette, T. Ramalho, J. Agapiou, and et al.,
“Hybrid computing using a neural network with dy-
namic external memory,” Nature, vol. 538, no. 7626, p.
471–476, 2016.

[33] M. Andrychowicz, F. Wolski, A. Ray, J. Schneider,
R. Fong, P. Welinder, B. McGrew, J. Tobin, P. Abbeel,
and W. Zaremba, “Hindsight experience replay,” in Neu-
ral Information Processing Systems, 2017, pp. 5049–
5059.

[22] A. d’Avila Garcez, M. Gori, L. C. Lamb, L. Seraﬁni,
M. Spranger,
“Neural-symbolic
and S. N. Tran,
computing: An effective methodology for principled
reasoning,”
integration
arXiv:1905.06088 [cs], May 2019, arXiv: 1905.06088.
[Online]. Available: http://arxiv.org/abs/1905.06088
[23] T. R. Besold, A. d. Garcez, S. Bader, H. Bowman,
P. Domingos, P. Hitzler, K.-U. Kuehnberger, L. C.
Lamb, D. Lowd, P. M. V. Lima, L. de Penning,

of machine

learning

and

11

APPENDIX A
TASKS DESCRIPTION

A. ILP

1) Family Tree: For family tree tasks, they have the same
IsMotherpX, Y q,
backgroung predicates:
IsSonpX, Y q and IsDaughterpX, Y q. IsFatherpX, Y q is T rue
when Y is X’s father. The other three predicates have the
similar meaning.

IsFatherpX, Y q,

‚ HasFather: HasFatherpXq is T rue when X has father. It

can be expressed by:

HasFatherpXq Ð DY, IsFatherpX, Y q

‚ 6-Connectivity: 6-ConnectivitypX, Y q is T rue if there
exists a path between node X and node Y within 6 edges.
It can be expressed by:

6-ConnectivitypX, Y q Ð DZ, pHasEdgepX, Y q_

Distance2pX, Y q _ Distance3pX, Y q_
pDistance2pX, Zq ^ Distance2pZ, Y qq_
pDistance3pX, Zq ^ Distance2pZ, Y qq_
pDistance3pX, Zq ^ Distance3pZ, Y qqq

Distance2pX, Y q Ð DZ, pHasEdgepX, Zq ^ HasEdgepZ, Y qq
Distance3pX, Y q Ð DZ, pHasEdgepX, Zq ^ Distance2pZ, Y qq

‚ 1-Outdegree: 1-OutdegreepXq is T rue if there the out-
degree of node X is exactly 1. It can be expressed by:

‚ HasSister: HasSisterpXq is T rue when X has at least

one sister. It can be expressed by:

1-OutdegreepXq Ð DY, @Z, pHasEdgepX, Y q

^ (cid:32)HasEdgepX, Zqq

HasSisterpXq Ð DY, IsSisterpX, Y q
IsSisterpX, Y q Ð DZ, pIsDaughterpZ, Y q ^ IsMotherpX, Zqq

‚ IsGrandparent: IsGrandparentpX, Y q is T rue when Y is

X’s grandparent. It can be expressed by:

‚ 2-Outdegree: 2-OutdegreepXq is T rue if there the outdegree of

node X is exactly 2. It can be expressed by:

2-OutdegreepXq Ð DY, @Z, KpHasEdgepX, Y q

^ (cid:32)HasEdgepX, Zq ^ (cid:32)HasEdgepX, Kqq

IsGrandparentpX, Y q Ð DZ, ppIsSonpY, Zq ^ IsFatherpX, Zqq
_pIsDaughterpY, Zq ^ IsMotherpX, Zqqq

B. RL

‚ IsUncle: IsUnclepX, Y q is T rue when Y is X’s uncle.

It can be expressed by:

IsUnclepX, Y q Ð DZ, ppIsMotherpX, Zq ^ IsBrotherpZ, Y qqq
_ pIsFatherpX, Zq ^ IsBrotherpZ, Y qq

IsBrotherpX, Y q Ð DZ, ppIsSonpZ, Y q ^ IsSonpZ, Xqq

In Path, the ﬁrst lesson starts with 3 objects and ﬁnish on
10. In Sorting, we start with 3 objects and ﬁnish on 15. In
Blocksworld, we start with 2 blocks (6 objects) and ﬁnish 12
blocks (26 objects). In those 3 domains, the decreasing of the
temperature and noise to obtain an interpretable policy is only
applied during the last lesson.

_ pIsSonpZ, Y q ^ IsDaughterpZ, Xqqq

In the NLRL tasks, curriculum learning is not needed: the

‚ IsMGUncle: IsMGUnclepX, Y q is T rue when Y is X’s

maternal great uncle. It can be expressed by:

IsMGUnclepX, Y q Ð DZ, pIsMotherpX, Zq ^ IsUnclepZ, Y qq

2) Graph: For graph tasks, HasEdge task have the same
background predicates: HasEdgepX, Y q. HasEdgepX, Y q is
T rue when there is an undirected edge between node X and
node Y .

‚ AdjacentToRed: AdjacentToRedpXq is T rue if node X
has an edge with a red node. In this task, it also use
ColorspX, Y q as another background predicate besides
HasEdgepX, Y q. ColorpX, Y q is T rue when the color
of node X is Y . It can be expressed by:

AdjacentToRedpXq Ð DY, pHasEdgepX, Y q ^ ColorpY, redqq

‚ 4-Connectivity: 4-ConnectivitypX, Y q is T rue if there
exists a path between node X and node Y within 4 edges.
It can be expressed by:

4-ConnectivitypX, Y q Ð DZ, pHasEdgepX, Y q_

Distance2pX, Y q _ pDistance2pX, Zq ^ HasEdgepZ, Y qq_

pDistance2pX, Zq ^ Distance2pZ, Y qqq

Distance2pX, Y q Ð DZ, pHasEdgepX, Zq ^ HasEdgepZ, Y qq

number of blocks during training is always 4.

For the RL scenarios with DLM-incr, the decreasing of
the temperature and noise to obtain an interpretable policy is
applied at every lesson and we stack a DLM after each lesson.
In the imitation learning scenario with DLM-incr, curricu-
lum learning is not needed: the number of blocks during
training is always 7. The teacher produces the trajectories to
learn. A new DLM is stacked only if the extracted formula is
better than the previous one. If it is not, the parameters of the
last DLM are reinitialized.

APPENDIX B
EXPERIMENTAL SET-UP

A. Computer Speciﬁcations

The experiments are ran by one CPU thread and one GPU
unit on the computer with speciﬁcations shown in Table IV.

TABLE IV: Computer speciﬁcation.

Attribute

CPU
Threads
Memory
GPU

Speciﬁcation

2 ˆ Intel(R) Xeon(R) CPU E5-2678 v3
48
64GB (4ˆ16GB 2666)
4 ˆ GeForce GTX 1080 Ti

B. Curriculum Learning

Every 10 epochs, we test the performance of the agent
over 100 instances with a deterministic policy and a stochastic
policy. If one of them reaches 100% then it can move to the
next lesson. Our agents are trained only on one lesson at a
time.

APPENDIX C
MORE DETAILS ON EXPERIMENTS

A. Examples of Interpretable Rules or Policies

As illustration for RL, we provide the logic program learned
by our method on the task On, which corresponds to the output
of the reasoning part:

12

C. Hyperparameters

1) Hyperparameters for dNL-ILP: For dNL-ILP, we train
each task with at most 80, 000 iterations. Moreover, at each
iteration, we use a new family tree or graph as training data,
which is randomly generated from the same data generator in
NLM and DLM, as backgrounds for training the model.
For task HasFather, IsGrandparent and AdjacentToRed, dNL-
ILP can achieve 100% accuracy without learning any auxiliary
predicates. For other ILP tasks, it has to learn at least one
auxiliary predicate to induct the target. In practice, the perfor-
mance decrease with increasing number of auxiliary predicates
or variables, therefore here we only use at most one auxiliary
predicate and at most three variables. Table V is the notions for
all the hyperparameters for testing dNL-ILP. Table VI shows
hyperparameters for deﬁning rules in dNL-ILP that achieve
the best performance.

TABLE V: Notions for hyperparameters used in testing [30]’s work.

Hyperparameter

Explanation

Narg
Nvar
Nterms
Fam
Ntrain
T
Nf ilter
lr
Nepoch
Niter
Mterms
θmean
θmax
β1
β2
(cid:15)

The number of arguments
The number of variables
The number of terms
amalgamate function
The number of nodes for training
The number of forward chain
The number of tests for rules ﬁlter
Learning rate
Maximum number of epochs for training model
The number of iterations for one epoch
Maximum number of terms in each clause
Fast convergence total loss threshold MEAN
Fast convergence total loss threshold MAX
ADAM β1
ADAM β2
ADAM (cid:15)

IsGrandparent,

Moreover, we use the same Ntrain from NLM to
train dNL-ILP (i.e. We set Ntrain “ 20 for HasFather,
IsUncle and IsMGUncle. And
HasSister,
we set Ntrain “ 10 for AdjacentToRed, 4-Connectivity,
6-Connectivity, 1-Outdegree and 2-Outdegree). Other hyper-
parameters, such as hyperparameters for optimizer, remain the
same for all tasks and consistent with Payani et al.’s github
code (https://github.com/apayani/ILP). For Family Tree tasks,
we set lr “ 0.01 when the loss from the last step is greater
than 2, otherwise we set lr “ 0.005. For Graph tasks, we set
lr “ 0.01. Other hyperparameters are shown in TableVII.

2) Hyperparameters for DLM: We have used ADAM with
learning rate of 0.005, 5 trajectories, with a clip of 0.2 in the
PPO loss, λ “ 0.9 in GAE and a value function clipping of
0.2. For the softmax over the action distribution, we used a
temperature of 0.01.

Pred1pa, bq Ð OnGoalpb, aq _ IsFloorpaq
Pred2pa, bq Ð (cid:32)Onpa, bq ^ Toppaq
Pred3pa, bq Ð Pred1pb, aq ^ Pred2pa, bq
Pred4pa, bq Ð Pred1pb, aq ^ Pred1pb, aq
Movepa, bq Ð Pred3pa, bq ^ Pred4pa, bq

Using this program, the decision-making part (stochastically)
moves blocks to the ﬂoor and moves the good block on its
goal position when it can.

Here are other examples on the family tree domain:

Pred1paq Ð DB, IsFatherpa, Bq ^ DB, IsMotherpa, Bq
Pred2paq Ð DB, IsFatherpa, Bq _ DB, IsMotherpa, Bq
Pred3paq Ð DB, IsMotherpa, Bq _ DB, IsMotherpa, Bq
Pred4paq Ð Pred1paq _ Pred2paq
Pred5paq Ð Pred3paq _ Pred3paq
Pred6paq Ð Pred4paq ^ Pred5paq
Pred7paq Ð Pred6paq _ Pred6paq
Pred8paq Ð Pred6paq _ Pred6paq
HasFatherpaq Ð Pred7paq ^ Pred8paq

Pred1pa, bq Ð IsDaughterpb, aq ^ IsMotherpa, bq
Pred2pa, bq Ð IsDaughterpb, aq ^ IsFatherpa, bq
Pred3pa, bq Ð IsDaughterpb, aq _ IsMotherpa, bq
Pred4pa, bq Ð DC, Pred2pb, a, Cq ^ DC, Pred1pb, a, Cq
Pred5pa, bq Ð DC, Pred3pb, a, Cq ^ DC, Pred1pb, a, Cq
Pred6paq Ð DB, Pred4pa, Bq ^ DB, Pred5pa, Bq
Pred7paq Ð Pred6paq _ Pred6paq
HasSisterpaq Ð Pred7paq ^ Pred7paq

Pred1pa, bq Ð IsSonpb, aq ^ IsSonpb, aq
Pred2pa, bq Ð IsDaughterpb, aq _ IsSonpb, aq
Pred3pa, bq Ð (cid:32)IsSonpb, aq _ IsMotherpb, aq
Pred4pa, bq Ð IsFatherpa, bq ^ IsFatherpa, bq
Pred5pa, b, cq Ð (cid:32)IsMotherpa, bq ^ IsMotherpa, bq
Pred6pa, bq Ð (cid:32)IsSonpb, aq ^ IsDaughterpb, aq
Pred7paq Ð DB, Pred1pa, Bq _ DB, Pred1pa, Bq
Pred8pa, bq Ð DC, Pred5pa, b, Cq _ DC, Pred5pa, b, Cq
Pred9pa, bq Ð (cid:32)DC, Pred6pb, a, Cq ^ DC, Pred4pb, a, Cq
Pred10pa, b, cq Ð (cid:32)Pred2pb, aq _ Pred3pa, bq
Pred11pa, bq Ð Pred8pa, bq ^ Pred7pb, aq
Pred12pa, b, cq Ð (cid:32)Pred9pa, bq _ Pred10pb, c, aq
Pred13pa, bq Ð (cid:32)Pred11pa, bq ^ @C, Pred12pa, b, Cq
IsUnclepa, bq Ð Pred13pa, bq ^ Pred13pa, bq

13

TABLE VI: Hyperparameters for deﬁning and learning dNL-ILP rules for each task.

Task

T

Auxiliary

Target

Narg Nvar Nterms

Fam Narg Nvar Nterms

Fam

HasFather
HasSister
IsGrandparent
IsUncle
IsMGUncle
AdjacentToRed
4-Connectivity
6-Connectivity
1-Outdegree
2-Outdegree

1
2
1
7
7
2
7
7
3
3

´
2
´
2
2
´
2
2
´
2

´
1
´
1
1
´
1
1
´
0

´
1
´
3
1
´
1
1
´
1

´
eq
´
or
or
´
or
or
´
eq

1
1
2
2
2
1
2
2
1
1

1
1
1
1
2
1
1
2
2
3

2
1
4
4
2
2
5
7
2
2

or
max
eq
eq
or
eq
eq
eq
eq
eq

TABLE VII: Hyperparameters for training dNL-ILP.

Nepoch Nf ilter Niter Mterms

θmean

θmax

400

3

200

6

0.5

0.5

β1

0.9

β2

(cid:15)

0.99

1e-6

TABLE VIII: Hyperparameters of the noise in DLM.

Starting from Exponential decay

Approximate ﬁnal value

Supervised

temperature τ of the Gumbel distributions
scale β of the Gumbel distributions
dropout probability

RL

temperature τ of the Gumbel distributions
scale β of the Gumbel distributions
dropout probability

1
1
0.1

1
0.1
0.01

0.995
0.98
0.98

0.995
0.98
0.98

0.5
0.005
0.0005

task-dependent
task-dependent
task-dependent

TABLE IX: Architectures for DLM.

Depth

Breadth

nO

nA

IO residual1

Family Tree

Graph

NLRL tasks

General Algorithm

HasFather
HasSister
IsGrandparent
IsUncle
IsMGUncle

AdjacentToRed
4-Connectivity
6-Connectivity
1-OutDegree
2-OutDegree

Unstack
Stack
On

Sorting
Path
Path (DLM-incr)

Blocksworld (nIDLM)
Blocksworld (imitation)

5
5
5
5
9

5
5
9
5
7

4
4
4

4
8
6

8
8

3
3
3
3
3

3
3
3
3
4

2
2
2

3
3
3

2
2

8
8
8
8
8

8
8
8
8
8

8
8
8

8
8
8

8
8

2
2
2
2
2

2
2
2
2
2

2
2
2

2
2
3

2
2

(cid:88)

1 Input-Output residual connections: As in NLM, all the input predicates of the DLM are
given as input of every module. Similarly, every output of each module is given to the ﬁnal
predicate of the DLM.

14

B. More computational time and space

TABLE X: Computational cost of dNL-ILP, BILP, NLM, and DLM
on the family tree and graph reasoning tasks.

IsGrandparent

dNL-ILP
T

BILP
M T M T

NLM

M

DLM (Ours)

T

M

Training
m “ 10
m “ 20
m “ 30
m “ 40
m “ 50
m “ 60
m “ 70
m “ 80
m “ 90
m “ 100
m “ 110
m “ 120
m “ 130

Training
m “ 5
m “ 10
m “ 15
m “ 20
m “ 25
m “ 30
m “ 35

30
201
27
1
30
1
42
5
99
13
198
31
358
65
596
119
192
932
303 1390
464 1994
656 2771
915 3751
1247 4964

22
966
22
1
22
1
42
3
112
9
314
24
47
682
93 1332

70 1629
1357
1
24
4
1
70
4
1
188
5
2
414
5
3
820
7
3
1341
8
4
2089
8
6
3123
11
8
4434
13
10
6093
17
13
21
8079
16
27 10056
19
N/A

N/A

2522
3
6
9
N/A
N/A
N/A
N/A

844 3238
2
2
3
5
10
19
33

78
844
4342
N/A
N/A
N/A
N/A

382
2
24
24
74
124
226
344
500
724
1002
1321
1710
2161

1372
4
40
254
732
1751
3594
6666

1 T: time (s), M: Memory (MB).
2 DLM used depth 4, breadth 3 for IsGrandparent, and depth 6, breadth

4 for 2-Outdegree.

C. Ablation Study: Critic

Fig. 4: Learning performance with or without critics with NLM and
DLM.

050100150200episodes0.00.51.0testingperformanceNLMwithoutcriticNLMwithcritic0100020003000episodes0.00.51.0testingperformanceDLMwithoutcriticDLMwithcritic