1
2
0
2

n
u
J

4
1

]
L
M

.
t
a
t
s
[

2
v
0
6
0
7
0
.
2
0
1
2
:
v
i
X
r
a

Achieving Eﬃciency in Black-box Simulation of Distribution Tails

with Self-structuring Importance Samplers

Anand Deo

Karthyek Murthy

Abstract. Motivated by the increasing adoption of models which facilitate greater automation in risk
management and decision-making, this paper presents a novel Importance Sampling (IS) scheme for
measuring distribution tails of objectives modeled with enabling tools such as feature-based decision
rules, mixed integer linear programs, deep neural networks, etc. Conventional eﬃcient IS approaches
suﬀer from feasibility and scalability concerns due to the need to intricately tailor the sampler to the un-
derlying probability distribution and the objective. This challenge is overcome in the proposed black-box
scheme by automating the selection of an eﬀective IS distribution with a transformation that implicitly
learns and replicates the concentration properties observed in less rare samples. This novel approach
is guided by a large deviations principle that brings out the phenomenon of self-similarity of optimal
IS distributions. The proposed sampler is the ﬁrst to attain asymptotically optimal variance reduction
across a spectrum of multivariate distributions despite being oblivious to the underlying structure. The
large deviations principle additionally results in new distribution tail asymptotics capable of yielding
operational insights. The applicability is illustrated by considering product distribution networks and
portfolio credit risk models informed by neural networks as examples.

Keywords: Tail risks;
importance sampling; black-box; variance reduction;
asymptotics; feature maps; decision rules; neural networks; portfolio credit risk.

large deviations; tail

1. Introduction

In addition to being an integral part of quantitative risk management, the need to estimate and control
tail risks is inherent in managing operations requiring high levels of service or reliability guarantees. The
variety of contexts for which chance-constrained and risk-averse optimization formulations are employed
serve as a testimony to the importance of tail risk management in operations research. Naturally,
this signiﬁcance is retained in the numerous operations and risk management models which are being
enriched with the use of algorithmic feature-mapping tools (such as neural networks, kernels, etc.,)
employed to facilitate a greater degree of automation and expressivity in mapping data to decisions.
Modeling mortgage risk with deep neural networks (Sirignano et al. (2018)), incorporating contextual
side information into decision making with integrated prediction-optimization approaches (Ban & Rudin
(2019), Bertsimas & Kallus (2020), Elmachtoub & Grigas (2020)), and the sample application contexts
in Ferreira et al. (2016), Harsha et al. (2019), Bazier-Matte & Delage (2020), Cao & Shen (2019),
Oroojlooyjadid et al. (2020), Lin et al. (2020) serve as illustrative examples. With the increasing adoption
of these expressive classes of models, it is imperative that the risk management practice seeks to measure
and manage the tail risks associated with their use.

In a similar vein, considerations of certifying safety, fairness and robustness in settings deploying
automation have led to a number of applications seeking to measure tail risks in avenues extending
beyond operations and quantitative risk management as well. Assessing the safety of automation in
driving and other intelligent physical systems get naturally cast in terms of evaluating expectations
restricted to distribution tails (Zhao et al. (2017), O’ Kelly et al. (2018), Uesato et al. (2019), Huang

Singapore University of Technology and Design, Singapore 487372

E-mail address: deo avinash@sutd.edu.sg, karthyek murthy@sutd.edu.sg.

1

 
 
 
 
 
 
2

EFFICIENCY IN BLACK-BOX IMPORTANCE SAMPLING

et al. (2018), Arief et al. (2020)), as is the case with evaluating severity of algorithmic biases on minority
sub-populations (Williamson & Menon (2019), Duchi & Namkoong (2020), Jeong & Namkoong (2020)).
Motivated by the importance and the challenges arising in measuring tail risks in these applications, this
paper considers the estimation of distribution tails of a rich class of performance functionals speciﬁed
with tools such as feature-based decision rules, linear programs, deep neural networks, etc., that serve
as key ingredients in these models.

To illustrate the challenges in the tail estimation tasks we consider, suppose that L(x) denotes the
loss (or cost) incurred when the uncertain variables aﬀecting the system, modeled by a random vector X,
realize the value x ∈ Rd. For example, L(X) may denote the losses associated with a portfolio exposed
to risk factors X, or may capture the inventory cost incurred with a decision rule for procurement
which depends on some or all of the components of X. In general, we take L(·) to be modeling a
suitable performance measure of interest. The distribution of L(X) is not analytically tractable even
in elementary models, and related measures such as its mean, or tail risk measures such as Value-at-
Risk, Conditional Value-at-Risk, etc., are typically estimated from sampled observations. Estimation and
subsequent optimization of tail risks via simulation becomes computationally expensive however, as naive
sample averaging requires about p−1
u ε−2δ−1 samples to achieve a relative precision of ε in tasks requiring
the estimation of pu := P (L(X) ≥ u) with 1 − δ conﬁdence. This prohibitive sample and computational
complexity point to the need for eﬃcient samplers whose complexity do not grow as severely with pu
decreasing to zero.

Application contexts where tail risk measurement is of central importance, such as those arising
in ﬁnancial engineering, actuarial risk, system availability, etc., have facilitated the development of
variance reduction techniques which aim to tackle this diﬃculty. Prominent examples include the use of
importance sampling and splitting, potentially in combination with other variance reduction tools such as
control variates, stratiﬁcation, conditional Monte Carlo, etc; see Glasserman (2004), Asmussen & Glynn
(2007). In particular, Importance Sampling (IS) is seen as the primary method for combating rarity
of relevant samples in diverse scientiﬁc disciplines and is shown to oﬀer remarkable variance reduction
in various ﬁnancial engineering, actuarial risk, queueing, and reliability models. The underlying idea
behind IS is to accelerate the occurrences of the target risk event in simulation by sampling from an
alternate distribution which places a much greater emphasis on the risk scenarios of interest. Observed
samples are then suitably reweighed to eliminate the bias introduced. We shall refer to this alternate
sampling distribution as IS distribution hereafter.

Eﬀective use of IS in the instances above rely, however, on carefully leveraging the structure of the
problem and the underlying distribution. The choice of IS distribution in tail estimation is often guided
by carefully examining the large deviations behaviour of the random vector X, as relevant to the chosen
performance functional L(·). A number of queuing and actuarial risk models, where the extreme event of
interest is often amenable to be written in terms of ﬁrst passage of a random walk or a Markov chain into
a “failure” set, serve as excellent illustrations for how one may exploit large deviations characterizations
to reap substantial variance reduction beneﬁts, see Heidelberger (1995), Juneja & Shahabuddin (2006),
Blanchet & Mandjes (2009), Blanchet & Lam (2012).

Key challenges in the settings considered. Such explicit reliance on large deviations charac-
terizations to carefully select the IS distribution, while a source of strength, also helps showcase the
challenges one may face in estimation tasks pertaining to events in which the value of an optimization
problem or that of a performance functional speciﬁed in terms of a neural network is unusually high. As
we explain in Section 2.2, large deviations characterizations for such events are known only in limited in-
stances and only speciﬁc examples, such as in Blanchet et al. (2019), Arief et al. (2020), Bai et al. (2020),
enjoy eﬃcient samplers as a consequence. Straightforward application of conventional approaches based
on exponential twisting are not known to be eﬀective even in elementary instances such as project evalu-
ation (PERT) networks with independent activity durations, and require intricate tailoring based on the

EFFICIENCY IN BLACK-BOX IMPORTANCE SAMPLING

3

objective L(·) and the distribution of X (see Juneja et al. (2007)). Even in these restricted instances and
in the ﬁnancial engineering applications above, exploiting certain special properties (such as multivariate
normality, independence, etc.) has been crucial and there is limited understanding otherwise.

We next highlight that it is often impractical to invoke the speciﬁc form of the cost functional L(·) or
that of the distribution of the random vector X, in their entirety, as is typically required in eﬃcient IS
estimation. Upon identifying an IS distribution family, selection of optimal candidate within the family
often requires solving a non-trivial optimization problem whose objective is informed by the distribution
of X and the constraints are informed by the functional L(·). The resulting formulation may be convex,
e.g.., Glasserman et al. (2000), Glasserman & Li (2005), or may additionally possess a combinatorial
structure Glasserman et al. (2008), or is solved by a mixed integer program Bai et al. (2020), in special
instances. Such nuanced selection is impractical if the functional L(·) is speciﬁed, for example, in terms
of a mixed integer linear program with several constraints (or) a deep neural network. Adaptive black-
box optimization approaches, based on criteria such as estimator variance Lemaire & Pag`es (2010), or
cross-entropy Rubinstein & Kroese (2013), may suﬀer systematic under-estimation see (Arief et al. 2020,
App. E) and, at best, can result in only as much variance reduction oﬀered by the best within the chosen
IS distribution family. The task of selecting a good IS family to begin with remains a major challenge.

The proposed approach towards automating the selection of IS distributions. In order to
overcome the above challenges, we propose an entirely novel importance sampling approach in which the
selection of an eﬀective IS distribution is automated by learning from simulated observations which are
less rare. To provide a high-level view of the method, suppose that X 1, . . . , X n are independent copies
of the random vector X. The change of measure in the proposed IS scheme is eﬀected by transforming
the samples as in, X i (cid:55)→ T (X i), where T (·) is a simple transformation which is not explicitly dependent
on the loss L(·) or the distribution of the random vector X. This distribution oblivious nature of the
transformation, along with the fact that generating samples X i does not involve any change of measure,
obviate the diﬃculties highlighted above. Despite its simple and scalable implementation, we show
that the proposed IS scheme achieves asymptotically optimal variance reduction across a spectrum of
multivariate distributions (involving light, heavy tails) and for a rich class of objectives modeled with
tools as diverse as mixed integer linear programs, deep neural networks, etc.

The rationale behind the procedure and a veriﬁcation of its eﬃciency are oﬀered by developing a
tail modeling framework and associated large deviations characterizations which are applicable for a
broad class of objectives motivated by the settings considered. The large deviations treatment reveals
a phenomena unexploited in this context, namely, the self-similarity of optimal IS distributions. This
notion serves as a key ingredient in understanding the rationale behind the proposed sampler and is
explained as follows: For a given u > 0, suppose that Pu denotes the theoretically optimal IS distribution
for estimating P (L(X) > u); in other words, Pu is just the law of X conditioned on the event {L(X) >
u}. We show that the distributions Pl and Pu are similar in the sense that they concentrate their mass
on identical sets, upon suitably scaling, even if the level l > 0 is only a fraction of the level u. Figure 1
below oﬀers an illustration of this self-similarity property.

The distribution-oblivious transformation T (·) employed in the IS scheme is carefully chosen such
that it exploits this self-similarity property. Irrespective of the underlying probability distribution, we
show that the employed transformation T (·) replicates the concentration properties of the theoretically
optimal IS distribution by learning from observations which are not as rare. The notion of self-similarity
utilized here is based on the theory of large deviations and is of diﬀerent nature compared to the weak
convergence based notion used widely in the statistical estimation of extreme events in Embrechts et al.
(1997), Resnick (1987), de Haan & Ferreira (2010). A weak convergence based IS distribution selection is
explored in Deo & Murthy (2020) and is not suited to result in asymptotically optimal variance reduction
showcased in this paper.

The main contributions of this paper can be summarized as follows.

4

EFFICIENCY IN BLACK-BOX IMPORTANCE SAMPLING

Figure 1. Illustration of the notion of self-similarity of optimal IS distributions: Sam-
ples from the distributions Pl, Pu (displayed in blue and red respectively) reveal that
they share similar concentration properties for three distribution choices of X informed
by a Gaussian copula with correlation ρ. The levels l, u are such that the probabilities of
L(X) exceeding these levels are approximately 10−3 and 10−5.5. The contours (drawn in
green) represent level sets of L(x) = 1(cid:124)(Ax − b)+ derived from a ReLU neural network
with weights given by the matrix A with rows (0.3, 1), (1, 0.3), (0, 1.1), (1.1, 0) and vector
b = 0.

1) (Novel & generically applicable IS) We present an entirely novel IS scheme which exhibits
asymptotically optimal variance reduction in black-box IS estimation of distribution tails of
objectives of the form L(X). To the best of our knowledge, this is the ﬁrst instance to do so at
a generality which expands the scope of applicability of eﬃcient IS to settings where (a) L(·) is
modeled by tools such as mixed integer linear and quadratic programs, feature maps informed by
neural networks, etc., (see Assumption 1, Examples 2.1 - 2.3); and (b) the uncertainty is driven
by a wide variety of multivariate distributions (see Tables 1 - 3).

2) (Tail modeling framework and asymptotics) Building on the large deviations based ap-
proach introduced in de Valk (2016), we present a tail modeling framework for X which allows
characterization of the rates at which the distribution tail of L(X) decays. The asymptotic pre-
sented in Theorem 4.1 reveals how the distribution of X and the model speciﬁed by L(·) inﬂuence
the rates of decay of the distribution tail. We illustrate how these results can be of interest, in
their own right, by deriving operational insights for minimizing network failure probabilities in
product distribution networks (see Section 4.2).

3) (Automation in the selection of IS distribution) We utilize these large-deviations char-
acterizations to bring out the self-similarity of optimal IS distributions (Proposition 5.1) and
verify the variance reduction properties (Theorems 5.2 and 7.2). The self-similarity property
serves to guide the automated approach towards selection of IS distribution. While a variety
of methods exist for searching optimal parameters within a chosen IS distribution family (see,
for eg., Rubinstein & Kroese (2013), Ahamed et al. (2006), Lemaire & Pag`es (2010), Bai et al.
(2020), He et al. (2021)), to the best of our knowledge, this is the ﬁrst paper to exhibit an
automated approach for tackling the complementary and more challenging problem of selection
of IS distribution families with optimal variance reduction properties.

4) (Applications) We demonstrate the utility of the IS scheme and the large deviations charac-
terizations in the evaluation of the probability of (a) large losses in a credit risk setting modeled
with a deep neural network, (b) large delays in contextual routing, and (c) failures in distri-
bution networks (Sections 4.2, 6, 8 and Appendix E). The risk events in these settings can be
re-expressed to coincide signiﬁcantly with an event of the form {L(X) > u}, and the reported
variance reduction serve to showcase the versatility and robustness of the IS scheme.

With the repertoire of models considered in quantitative risk management expanding to include ma-
chine learning based approaches, the proposed IS scheme serves as an entirely novel addition that extends

EFFICIENCY IN BLACK-BOX IMPORTANCE SAMPLING

5

the scope of applicability of the line of research pursued in Glasserman et al. (2000, 2002), Glasserman
& Li (2005), Bassamboo et al. (2008), Glasserman et al. (2008), Liu (2015) to the class of models which
employ algorithmic approaches such as neural networks. To the best of our knowledge, the treatment in
Section 6 presents the ﬁrst provably eﬃcient IS scheme in a portfolio risk estimation problem in which
the relationships between the underlying risk factors is captured by a neural network. While most IS
schemes rely on exploiting the structure of Gaussian or t-copula models as applicable for the problem con-
sidered, the proposed black-box IS scheme oﬀers asymptotically optimal variance reduction in a greater
generality. More broadly, the black-box nature and the ease of implementation point to the suitability
for the IS scheme to serve as a vehicle for variance reduction in the Value-at-Risk related estimation
tasks considered in Glynn (1996), Glasserman et al. (2000), Sun & Hong (2010), Hong et al. (2014), He
et al. (2021).

The proposed IS scheme also serves as an entirely novel addition to the IS methods introduced in
Blanchet et al. (2019), Bai et al. (2020), Arief et al. (2020), which comprise the ﬁrst instances of veriﬁably
eﬃcient variance reduction schemes for estimating distribution tails motivated from optimization and
learning contexts. As we explain in Section 2.2, the technique used in Blanchet et al. (2019) is speciﬁc to
a linear program arising in modeling product distribution networks. The approaches introduced in Bai
et al. (2020), Arief et al. (2020) utilize the dominating point machinery for normal distributions in order
to tackle piecewise linear and black-box objectives in Gaussian environments. In contrast, the proposed
IS scheme is applicable as long as the model informed by L(·) and the distribution of X are suﬃciently
regular, thus substantially expanding the scope and applicability.

The rest of the paper is organized as follows. Upon introducing the problem in Section 2.1, we
provide an overview of related importance sampling literature in Section 2.2 and present the proposed IS
scheme in Section 2.3. The tail modeling framework introduced in Section 3 is used to derive the large
deviations asymptotics for distribution tails in Section 4. Section 5 serves to bring out the notion of
self-similarity of optimal IS distributions and presents the main result verifying the asymptotic optimal
variance reduction properties of the proposed IS scheme. An application to the portfolio credit risk
setting is presented in Section 6. Results which help understand the variance reduction properties of the
IS scheme in heavy-tailed settings are presented in Section 7 and the results of numerical experiments
are furnished in Section 8. Key ideas behind the proofs of main results are presented in Appendix A.
Technical proofs and additional useful examples are presented in the subsequent supplementary material.

2. The problem considered and the proposed IS procedure

Vectors are written in boldface to enable diﬀerentiation from scalars. For any a = (a1, . . . , ad) ∈
Rd, b = (b1, . . . , bd) ∈ Rd and c ∈ R, we have |a| = (|a1|, . . . , |ad|), ab = (a1b1, . . . , adbd), a/b =
(a1/b1, . . . , ad/bd), a ∨ b = (max{a1, b1}, . . . , max{ad, bd}), ab = (ab1
d ), a−1 = (1/a1, . . . , 1/ad),
log a = (log a1, . . . , log ad), ca = (ca1, . . . , cad ), denoting the respective component-wise operations. Let
Rd

+ = {x ∈ Rd : x ≥ 0} denote the positive orthant and Rd

++ denote its interior.

1 , . . . , abd

2.1. A description of the problem considered. Suppose that L(x) denotes the cost incurred when
the uncertain variables aﬀecting the system, modeled by a random vector X, realize the value x ∈ Rd.
While the loss L(·) may be expressed as a linear combination of uncertain variables in some simple
settings, the inherent nature of managing operations under resource constraints often results in L(·)
expressed suitably as the value of an optimization formulation and/or may require evaluation of a feature
map represented, for example, by a neural network. We consider the task of estimating the probabilities
or expectations associated with tail risk events of the form {L(X) ≥ u}, for a threshold u suitably
large. The need for having a control over likelihoods of these risk scenarios is inherently present in many
operational settings aﬀected by uncertainty, due to the need to keep the costs below a target risk level
(or) to meet a service-level agreement which ensures that a target quality of service is met. In many

6

EFFICIENCY IN BLACK-BOX IMPORTANCE SAMPLING

applications, high losses are experienced when the random vector X takes undesirably high values in the
positive orthant; for example, large travel durations in vehicle routing instances leading to large delays.
Thus, without loss of generality, we take the set specifying risk scenarios, {x ∈ supp(X) : L(x) ≥ u},
to be a subset of the positive orthant Rd
+; here supp(X) denotes the support of the distribution of X.
In many of the motivating contexts, it is impractical to invoke the speciﬁc form of the function L(x)
to facilitate eﬃcient estimation of the distribution tails. Therefore, our interest is to design eﬃcient
estimation schemes which not only oﬀer substantial variance reduction, but do so while requiring only
black-box access to a function L(·) satisfying Assumption 1 below.

Assumption 1. The function L : Rd → R satisﬁes the following conditions:

a) the set {x ∈ supp(X) : L(x) ≥ u} is contained in Rd
b) for any sequence {xn}n≥1 of Rd

+ satisfying xn → x, we have

+ for all suﬃciently large u; and

lim
n→∞

L(nxn)

nρ = L∗(x),

where ρ is a positive constant and the limiting function L∗ : Rd
{x ∈ Rd

+ : L∗(x) > 0} is non-empty.

+ → R is such that the cone

Assumption 1b merely speciﬁes asymptotic homogeneity, which implies that larger the value of u,
farther is the target rare set from the origin. The set {x ∈ Rd
+ : L∗(x) > 0} is necessarily a cone follows
from the observation that L∗(cx) = limn→∞ n−ρL(ncx) = cρ limn→∞(cn)−ρL(cnx) = cρL∗(x), for any
c > 0 and x ∈ Rd
+. Assumption 1 can be readily veriﬁed to hold in many useful instances, some of
which are demonstrated in the Examples 2.1 - 2.3 below. To focus on the convergence requirement in
Assumption 1b, we take the support of X to be bounded from below these examples.

Example 2.1 (Piecewise aﬃne functions, value of mixed integer linear programs). Suppose that L(·)
can be written as

L(x) = sup
θ∈Θ

{θ(cid:124)x + r(θ)},

(1)

where Θ is a bounded subset of Rd and r : Θ → R is a bounded function which serves to capture
terms, if any, which do not involve the random vector X. In the case where Θ is a ﬁnite set, L(·) could
(cid:124)
represent a piecewise aﬃne function as in, L(x) = maxk=1,...,K{θ
kx + rk}, where K is a positive integer,
θk ∈ Rd and rk ∈ R, for k = 1, . . . , K. If the set Θ is described by linear and/or integer constraints
and if the function r(·) is aﬃne, we have that (1) is a linear (or) a mixed integer linear program. With
the notation used in Assumption 1, we have ρ = 1 and L∗(x) = maxθ∈Θ θ(cid:124)x for the example L(·) in
(1); see (Rockafellar & Wets 1998, Proposition 7.29). The requirements in Assumption 1 are met, for
example, if at least one vector in the collection Θ lies outside the negative orthant Rd
−; or, in other
words, if sup{θk : (θ1, . . . , θd) ∈ Θ, k = 1, . . . , d} > 0. Objectives in many planning problems, such
as project evaluation and review networks, linear assignment or matching, traveling salesman problem,
vehicle routing problem, max-ﬂow, minimum cost ﬂow, etc., satisfy this requirement either in the native
formulation or in the respective dual formulation.

Example 2.2 (Piecewise quadratic functions). As a natural extension to the piecewise linear functions
treated in Example 2.1, one may also consider piecewise quadratic functions of the form,

L(x) = max

k=1,...,K

(cid:8)xT Qkx + cT

k x(cid:9) ,

(2)

where K is a positive integer, {Qk : k = 1, . . . , K} are (d × d)-symmetric matrices, and ck ∈ Rd, for
k = 1, . . . , K. As long as the matrices Qk are not all identically zero, we have ρ = 2 and L∗(x) =
max{xT Qkx : k = 1, . . . , K} in this example. When the support of X is bounded from below, the
requirements in Assumption 1 are automatically met if, for example, at least one of the eigen values
of the matrices in the collection {Qk : k = 1, . . . , K} is positive. On the other hand, if we have

EFFICIENCY IN BLACK-BOX IMPORTANCE SAMPLING

7

minimum instead of maximum in (2), the requirements are easily checked to be satisﬁed if the collection
{Qk : k = 1, . . . , K} is positive semideﬁnite.

Example 2.3 (Models using contextual information via feature maps & decision rules). Suppose that
L(·) is written as a composition of functions as in,

L(x) = c (θ(cid:124)Φ(x) + θ0) ,
where c : R → R is an objective measuring the cost incurred by plugging in a speciﬁc decision rule
or a function approximation based on the feature map Φ : Rd → Rm (see Ban & Rudin (2019) for
an example utility of feature-based decision rules in newsvendor models). In simple settings, one may
take the feature map to be merely Φ(x) = x, or, may include cross-terms in the feature vector as in
x = (x1, . . . , xd) (cid:55)→ (xi, xixj : i, j = 1, . . . , d). Motivated by the proliferation of deep neural networks in
learning expressive and eﬃcient feature maps LeCun et al. (2015), one may consider the feature map Φ
to be deﬁned in terms of several function compositions deﬁned recursively as in,

Φ(x) = LK(x), Lk(x) = (AkLk−1(x) − bk)+, k = 1, . . . , K,

and L0(x) = (A0x − b0)+.

(3)

In the above, the operation (a)+ = a ∨ 0, K is a positive integer and for each k ≤ K, Ak ∈ Rnk×nk−1,
bk ∈ Rnk are weight parameters in a neural network with nk ≥ 1 rectiﬁed linear activation units (ReLU)
in the k-th layer. We have nK =: m as the dimension of the resulting feature map. Refer Sirignano
et al. (2018) for a treatment of their utility in identifying relevant features in the context of modeling
mortgage default risk. For the map Φ(·) considered in (3), we have
(cid:0) · · · A1(A0x)+(cid:1)+(cid:1)+

, for every sequence {xn}n≥1 of Rd satisfying xn → x.

n−1Φ(nxn) → (cid:0)AK

In general, suppose the feature map Φ is such that n−pΦ(nxn) → Φ∗(x), for some p > 0 and every
sequence {xn}n≥1 of Rd
+ satisfying xn → x. Then for the desired convergence in Assumption 1b, we have,
and ρ = pq, if, for example, c(·) is such that c(u)/uq →
L∗(x) = c+
c+ as u → ∞, c(u)/|u|q → c− as u → −∞, with constants q, c+, c− satisfying q > 0, min{c+, c−} > 0.
One may include an additional composition to consider models of the form,

(cid:2)(cid:0)θ(cid:124)Φ∗(x)(cid:1)−(cid:3)q

(cid:2)(cid:0)θ(cid:124)Φ∗(x)(cid:1)+(cid:3)q

+ c−

L(s, ε) = min
θ∈Θ

θ(cid:124)c(cid:0)Φ(s), ε(cid:1),

(4)

where s is seen as contextual side information, Φ(·) is a feature map that models the dependence of cost
vector c in terms of the side information s and additional uncertainty ε, and Θ describes the constraints;
see, for example, Elmachtoub & Grigas (2020) for details and Section 8.1 for a contextual shortest-path
example. Here suppose that the feature map Φ(·) is as above and the cost mapping c is positive and
satisﬁes n−ρc(npsn, nεn) → c∗(s, ε), for sn → s, εn → ε and some ρ, p > 0. If we let x = (s, ε), we have
Assumption 1(b) satisﬁed with L∗(s, ε) = minθ∈Θ θ(cid:124)c∗(cid:0)Φ∗(s), ε(cid:1).

The above examples are only meant to be indicative of the breadth of functions L(·) which satisfy
Assumption 1 and are not exhaustive. One can identify more functionals L(·) which satisfy Assumption
1 by taking linear combinations or compositions suitably from the above class based on the modeling
needs. The requirements in Assumption 1 can be recast naturally if a particular application requires the
set quantifying risky scenarios, {x : L(x) ≥ u}, to be a subset in a diﬀerent orthant. Since Assumption
1 does not require convexity, the analysis and algorithms developed in this paper are applicable even if
the set {x : L(x) ≥ u} is non-convex.

Given a loss L(·) satisfying Assumption 1 above, our objective is to allow fast Monte Carlo evaluation
of the probabilities of the risk event, P (L(X) ≥ u), or expectations restricted to this event, for a suitably
large value of u appropriate for the reliability requirements in the application considered. We consider
this estimation task supposing that the probability distribution of X is a member of a distribution class
with suﬃciently regular tail behaviour, in a sense to be made precise in following Section 3. Many
commonly used continuous distribution families belong to the considered class, as we demonstrate with

8

EFFICIENCY IN BLACK-BOX IMPORTANCE SAMPLING

examples in Tables 1 to 3 in Appendix B. It is well-known that estimating such quantities related to
u ε−2δ−1 samples, where
distribution tails merely by computing sample averages would require about p−1
pu := P (L(X) ≥ u) is the probability of the target risk event, ε > 0 is the relative precision required and
1 − δ is the target conﬁdence level (see (Asmussen & Glynn 2007, Chapter 6)). Such a computational
requirement is prohibitive when pu is small, which is often the case in settings with high service quality
or reliability requirements.

2.2. Related literature on eﬃcient importance sampling. The idea of using large deviations to
come up with eﬀective IS distributions and verify its variance reduction properties goes back to Siegmund
(1976). Intense pursuit of this strategy has resulted in a remarkable body of literature motivated by
queueing, actuarial and reliability applications. Since the distribution tail events introduced in Section
2.1 do not necessarily possess the Markovian structure prevalent in these contexts, we refer the readers
to the review articles Heidelberger (1995), Juneja & Shahabuddin (2006), Blanchet & Lam (2012) and
references therein, for IS methods which speciﬁcally exploit the structure present in independent (or)
Markovian settings.

The ubiquity of Gaussian random vectors, along with the ease with which it is amenable to be treated
via Laplace principle, has led to another class of eﬃcient IS schemes which utilize the properties of
multivariate normal distributions. Considering the problem of estimating probabilities of excess losses of a
portfolio incurred in a time interval, Glasserman et al. (2000) proposes an IS scheme for estimating the tail
probabilities of the Delta-Gamma quadratic approximation of the portfolio loss. A suitably exponentially
twisted IS distribution, which is again multivariate normal, is shown to result in asymptotically optimal
variance reduction. Glasserman & Li (2005), Glasserman et al. (2008), Liu (2015) develop eﬃcient IS
schemes for portfolio credit risk settings where dependence between loan defaults is introduced via a
normal copula. To tackle settings where the loan defaults have extremal dependence, Bassamboo et al.
(2008) develops an IS algorithm where suitable exponential twisting is applied to a single shock factor
which induces large defaults.

Motivated by the problem of cascading failures in power networks, Blanchet et al. (2019) showcases
the ﬁrst instance of an eﬃcient IS method for estimating the tail probabilities of a linear program based
performance functional. Here variance reduction is veriﬁed assuming that the underlying distribution
is multivariate normal and the IS scheme is reliant on the speciﬁc structure of the linear program
involved. Considering the challenge of evaluating safety and robustness in safety-critical systems which
employ prediction models such as random forests and neural networks, Bai et al. (2020) verify that an
appropriately chosen mixture of multivariate normal distributions is eﬃcient in estimating P (g(X) ≥ u),
where g(·) is piecewise linear and X is multivariate normal.

Most of the above approaches are rendered inapplicable however in the presence of heavy-tailed ran-
dom variables distributed with log-normal, Weibull (or) power law tails. Considering the example of
independent sums, Asmussen et al. (2000) illustrate the diﬃculties of applying IS to heavy-tailed prob-
lems and propose that one may use a IS distribution whose tail is an order of magnitude heavier than
that of the summands. However, this is shown to not perform well in practice (see (Asmussen & Glynn
2007, p.176)). Juneja & Shahabuddin (2002), Huang & Shahabuddin (2004), Juneja et al. (2007) develop
the idea of hazard rate twisting, a variant of exponential twisting in which the twisting rate is taken to be
proportional to hazard rate, and demonstrate its potential in handling independent light and heavy-tailed
random variables together and objectives more general than the sum. Selecting an IS distribution which
emphasizes the behaviour suggested by large-deviations has been found to be eﬀective in elementary
heavy-tailed settings as well, as illustrated by Asmussen & Kroese (2006), Dupuis et al. (2007). Dupuis
et al. (2007) introduces a mixture family which replicates the dynamics of the “big jump principle”
often reported in heavy-tailed large deviations. The optimal mixture parameters are chosen by solving
a limiting control problem. With the exception of Blanchet & Rojas-Nandayapa (2011), Asmussen et al.
(2011) which consider correlated sums, it is instructive to note that most IS methods involving a number

EFFICIENCY IN BLACK-BOX IMPORTANCE SAMPLING

9

of heavy-tailed random variables are applicable only when the random variables involved are mutually
independent. A strategy for handling heavy-tailed random vectors, in the context of scenario generation
for chance constraints, is developed in Blanchet et al. (2020).

Ensuring signiﬁcant variance reduction in all the above instances require carefully tailoring the choice
of the IS distribution to the structure of the model speciﬁed by L(·) and the underlying probability
distribution. As highlighted earlier, the analytical tractability required for this exercise has not been
shown to extend to functionals L(·) written in terms of optimization objectives. In an eﬀort to overcome
some of these diﬃculties in black-box Gaussian environments, Arief et al. (2020) proposes to use deep
learning classiﬁers to learn certain boundary points of an outer approximation to the rare set, namely the
dominating points (see Sadowsky & Bucklew (1990), Hashorva & H¨usler (2003), Honnappa et al. (2018)).
If the underlying distribution is multivariate normal, then an IS mixture distribution with means chosen
as dominant points has been shown to result in an upper bounding estimate with controlled relative
error. The IS scheme to be introduced in Section 2 aims to overcome the challenges in a wider variety
of tail estimation tasks which are suﬃciently regular.

2.3. The proposed importance sampling method. The proposed importance sampling (IS) pro-
cedure for the fast evaluation of the tail risk probabilities, pu := P (L(X) > u), where L(·) is taken
to satisfy Assumption 1, is presented in Algorithm 1 below. Assuming only an oracle access to the
evaluations of loss L(·), the parameter ρ in Assumption 1, the probability density of X and its samples,
Algorithm 1 returns an estimator ¯ζN (u) for pu without requiring further information on the structure
of the loss L(·) or that of the probability distribution. It does so by ﬁrst drawing a nominal number of
independent samples {X i : i = 1, . . . , N } of the random vector X, and by employing a multiplicative
transformation as in,

where l is a positive hyper-parameter choice in Algorithm 1 and κ : Rd → Rd is given by,

T (x) = x × (cid:0)u/l(cid:1)κ(x)

,

κ(x) :=

log(1 + |x|)
ρ(cid:107) log(1 + |x|)(cid:107)∞

.

(5)

(6)

In other words, the samples for the proposed IS procedure are taken as, Zi := T (X i),
i = 1, . . . , N.
The bias resulting from counting the fraction of samples Zi lying in the target rare set, instead of that
of X i, is adjusted by multiplying with the respective likelihood ratio term Li as in,

¯ζN (u) =

1
N

N
(cid:88)

i=1

LiI (L(Zi) ≥ u) .

(7)

As with any IS procedure (see (Asmussen & Glynn 2007, Chapter 5)), the likelihood ratio term Li is
simply the ratio between the probability densities of X and Z evaluated at Zi. Consequently, as veriﬁed
in Proposition 2.4 below, the resulting estimator ¯ζN (u) has no bias. Further, with a standard change of
variables formula involving the Jacobian of the transformation Li can be written conveniently as in (8)
- (9a) in Algorithm 1 which require only access to evaluations of the probability density of X; see the
proof of Proposition 2.4 in the appendix for computations leading to (9a).

Proposition 2.4. Suppose that X is a random vector with probability density fX (·). Then for any
u > 0, the estimator ¯ζN (u) is unbiased. In other words, E[¯ζN (u)] = pu. Moreover there exists a map
T −1 : Rd → Rd such that T ◦ T −1(x) = x for almost every x ∈ Rd.

The choice of the transformation T (·) in (5), which implicitly speciﬁes the IS density, is guided by
the self-similarity properties of the probability distribution of X made concrete by means of the large
deviations framework developed in Sections 3 - 4. Building on this framework, an account on the
rationale behind the choice of the IS transformation (5) and its variance reduction properties is oﬀered
in Section 5. Roughly speaking, the transformation T (·) suitably replicates the concentration properties

10

EFFICIENCY IN BLACK-BOX IMPORTANCE SAMPLING

Algorithm 1: Self-structuring IS procedure for estimating P (L(X) > u)

Input: Threshold u, N independent samples X 1, . . . , X N of X, hyper-parameter l
Procedure:
1. Transform the samples: For each sample i = 1, . . . , N, compute the transformation,

where κ(X i) is evaluated as in (6).

Zi = T (X i) := X i(u/l)κ(X i),

2. Compute the associated likelihood: For each transformed sample Zi, compute the
respective likelihood ratio as,

fX (Zi)
fX (X i)
where fX (·) is the probability density of X and J : Rd → R+ is the Jacobian of the
transformation T (·) given as follows:

i = 1, . . . , N,

J(X i),

Li :=

J(x) :=

(cid:35)

˜Ji(x)

×

(cid:34) d
(cid:89)

i=1

(u/l)1(cid:124)κ(x)
maxi=1,...,d ˜Ji(x)

,

where ˜Ji(x) := 1 +

ρ−1 log(u/l)
(cid:107) log(1 + |x|)(cid:107)∞

|xi|
1 + |xi|

,

i = 1, . . . , d.

3. Return the output estimator: Return the IS average computed as in,

¯ζN (u) =

1
N

N
(cid:88)

i=1

LiI (L(Zi) ≥ u) .

(8)

(9a)

(9b)

of the theoretically optimal IS distribution by learning from observations which are not as rare. This
is facilitated by taking the parameter l such that l (cid:28) u and the event {L(X) ≥ l}, though also a tail
risk event, is much more frequently observed in the initial samples when compared to the target event
{L(X) ≥ u}. Even if the parameter l is relatively negligible when compared to the level u, we show the
variance of the resulting IS estimator is small as in,

var[ ¯ζN (u) ] = o (cid:0)p2−ε

u N −1(cid:1) ,

(10)

as the estimation task is made more challenging by taking pu → 0. The relationship (10) holds for
any arbitrary choice of ε > 0 and for any choice of l = l(u) which is taken to be slowly varying in u
and satisﬁes limu→∞ l(u) = +∞; see Theorem 5.2 in Section 5 for a precise statement of the variance
reduction result and Section 3.1 for the deﬁnition and examples of slowly varying functions. Unlike
traditional IS approaches which require solving a non-trivial optimization problem, the selection of IS
density within the implicitly speciﬁed IS distributions family is simpliﬁed to that of selecting the single
parameter l. The robust variance reduction guarantee, obtained for any l which is slowly varying in u,
enables us to conﬁne the search for a good choice of the hyper-parameter l to be within a relatively
narrow range. One may then execute the selection of l by means of cross-validation (or) by incorporating
adaptive stochastic approximation schemes such as in Lemaire & Pag`es (2010); see Section 8 for an
example using cross-validation.

Contrast the reduced variance of the IS estimator in (10) with that of the naive sample average
which merely counts the fraction of samples {X i : i = 1, . . . , N } in the target rare set.
In the case
of naive sample average, the variance is pu(1 − pu)N −1 and the coeﬃcient of variation grows as in
u N −1/2, as pu → 0. Thanks to (10), the coeﬃcient of variation of the proposed IS estimator grows
p−1
only as o(p−ε
u N −1/2) where ε can be arbitrarily small, thus requiring only a negligible fraction of samples
compared to that required by the naive sample average. Any estimator which meets the relative error

EFFICIENCY IN BLACK-BOX IMPORTANCE SAMPLING

11

guarantee in (10) is said to oﬀer asymptotically optimal variance reduction and is referred to as logarith-
mically eﬃcient. Please refer (Asmussen & Glynn 2007, Chapter 6) for a discussion on the signiﬁcance
on logarithmic eﬃciency and why it is a natural and pragmatic eﬃciency criterion for estimation tasks
pertaining to rare events.

3. A nonparametric tail modeling description and associated LDP

Our objective in Sections 3 - 5 is to develop a large deviations asymptotic for the tail probabilities
of the form P (L(X) ≥ u) and subsequently use it to verify the eﬃciency of the proposed Algorithm 1
without having to make restrictive assumptions for the distribution of X.

3.1. Preliminaries: Regularly varying functions (Class RV). A function f : R+ → R+ is said to
be regularly varying with index ρ ∈ R if for every x > 0,

= xρ.

lim
n→∞

f (nx)
f (n)
When referring to (11), we write f ∈ RV, or, f ∈ RV(ρ) if there is a need to explicitly specify the
exponent ρ. The function f (x) = xρ is a canonical example of the class RV(ρ). If ρ = 0, then f is
speciﬁcally referred as slowly varying. Some examples of slowly varying functions include log(1 + x),
log log(e + x), (1 + log(1 + x))a where a ∈ R, exp(log(x)a) where a ∈ (0, 1), or any function f satisfying
limx→∞ f (x) = c ∈ (0, ∞). A function f ∈ RV(ρ) can be written as f (x) = (cid:96)(x)xρ, for some slowly
varying (cid:96)(·) and ρ ∈ R. Evidently, (11) is a characteristic of all homogeneous functions and univariate
polynomials. By allowing ρ to be an arbitrary real number and (cid:96)(·) to be any slowly varying function,
the class RV possesses substantially improved modeling power. See, for example, Feller (1971), Borovkov
(2008) for a detailed treatment of the properties of the class RV.

(11)

3.2. Assumptions on the probability distribution of X. Let ¯Fi(xi) := P (Xi > xi) and Λi(xi) :=
− log ¯Fi(xi) respectively denote the complementary c.d.f (also known as survival function) and the cu-
mulative hazard function of the component Xi in X = (X1, . . . , Xd). The marginal components Xi are
required to satisfy Assumption 2 below.

Assumption 2. For i ∈ {1, . . . , d}, the marginal components Xi are such that Λi is continuous, strictly
increasing in an interval of the form (x0, ∞), and Λi ∈ RV(αi) for some αi ∈ (0, ∞).

Common examples of distributions which satisfy Assumption 2 are as follows: standard exponential
distribution where Λi(x) = x satisﬁes Λi ∈ RV(1); standard normal distribution where Λi(x) = x2/2 −
log x(1 + o(1)), as x → ∞, satisﬁes Λi ∈ RV(2); Weibull distribution with shape parameter k ∈ (0, ∞),
where Λi(x) = xk, satisﬁes Λi ∈ RV(k). Other examples of parametric families, along with respective
tail parameters αi, are given in Table 1 in Appendix B. This large class includes distributions which are
light-tailed and as well as heavy-tailed distributions of the Weibull type. The case where the marginal
distributions possess even heavier tails, such as log-normal, pareto, regularly varying distributions, etc.
are treated later in Section 7.

To describe the joint distribution of X, we ﬁrst consider the standardizing transformation,

Y = (Y1, . . . , Yd) := Λ(X),

where Λ(x) := (Λ1(x1), . . . , Λd(xd)),

which “standardize” the marginal distributions to that of standard exponential.

Lemma 3.1. The marginal distributions of the components Yi, for i = 1, . . . , d, are identical and is given
by, P (Yi > yi) = exp (−yi) , for yi > 0.

As with the wide-spread practice of modeling joint distributions in terms of copulas (Embrechts et al.
(2001), Nelsen (2010)), this standardization restricts the focus to the dependence structure without
getting distracted by the potential non-identical nature of marginal distributions of X.

12

EFFICIENCY IN BLACK-BOX IMPORTANCE SAMPLING

Assumption 3. The probability density of Y := Λ(X) admits the form

fY (y) = p(y) exp(−ϕ(y)),

where ϕ(·), p(·) satisfy the following: There exists a limiting function I : Rd
n−ε log p(nyn) → 0,
n−1ϕ(nyn) → I(y)

and

+ → R+ such that,

(12)

(13)

for any sequence {yn}n≥1 of Rd

+ satisfying yn → y (cid:54)= 0, and ε > 0.

The nonparametric nature of the Assumption 3 suggests that a wide variety of dependence models
satisfy Assumption 3. Indeed, most commonly used distribution families such as multivariate normal,
multivariate t, elliptical densities, archimedean copula models, exponential family with any regularly
varying suﬃcient statistic, extreme value distributions, suitable members of generalized linear models,
log-concave densities, etc. can be veriﬁed to satisfy Assumption 3. Table 3 in Appendix B is intended
to oﬀer a sample of distribution families which satisfy the marginal and joint distribution conditions
in Assumption 2 - 3 and to serve as a quick reference for the limiting function I(·) in Assumption 3.
Appendix B also contains a suﬃcient condition, directly in terms of the probability density of X, under
which Assumptions 2 - 3 are guaranteed to hold.

Example 3.2 (Gaussian copula). Suppose that Y has a joint distribution given by a Gaussian copula
with correlation matrix R. Given a copula with density c : [0, 1]d → [0, 1], the respective probability
density fY (·) can be expressly computed as, fY (y) = c(cid:0)1 − exp(−y)(cid:1) exp(−1(cid:124)y), Therefore

fY (y) = [det(R)]−1/2 exp (cid:0)−1(cid:124)y − 2−1g(y)(cid:124)(R−1 − I)g(y)(cid:1) ,
where g(y) := ( ¯Φ−1(e−y1 ), . . . , ¯Φ−1(e−yd )) and ¯Φ(·) := 1 − Φ(·) is the complementary c.d.f. of the
standard normal variable. Thus, in this example, we have from the notation in (12) that p(y) =
[det(R)]−1/2 and ϕ(y) = −1(cid:124)y − 2−1g(y)(cid:124)(R−1 − I)g(y). Since ¯Φ−1(p) = −2 log p (1 + o(1)), as p → 0,
we have g(ny)/(n) → (y1/2
), and subsequently, ϕ(ny)/ϕ(n) → (y1/2)(cid:124)R−1y1/2, compactly, as
n → ∞. We therefore have the limiting I(·) in Assumption 3 as I(y) := (y1/2)(cid:124)R−1y1/2.

, . . . , y1/2

(cid:3)

d

1

3.3. Tail large deviations principle with I(·) as the rate function. In this section, we unravel a
large deviations principle associated with a sequence derived from a random vector Y whose probability
density is of the form stated in Assumption 3. A sequence of random vectors ξn is said to satisfy a
large-deviations principle with rate function J(·) if,

lim sup
n→∞

1
n

log P (ξn ∈ F ) ≤ − inf
x∈F

J(x)

and

lim inf
n→∞

1
n

log P (ξn ∈ G) ≥ − inf
x∈G

J(x),

for every closed subset F and open subset G. Theorem 3.3 below establishes the LDP which is useful in
the context considered.

Theorem 3.3 (Tail LDP). Suppose that Y is a random vector whose probability density admits the
form (12), where the functions ϕ(·), p(·) satisfy the convergences in (13) for any sequence {yn}n≥1 of
Rd
+ satisfying yn → y (cid:54)= 0, and ε > 0. Then the sequence {n−1Y : n ≥ 1} satisﬁes the large deviations

principle with rate function I(·).

The following useful properties of the limiting function I : Rd

+ → R in (13) are deduced from the

conditions in Assumption 3 and the conclusion in Theorem 3.3.

Lemma 3.4. Suppose that Assumption 3 holds. Then

a) I(·) is continuous, I(0) = 0 and I(x) > 0 for all x ∈ R+
b) I(·) is homogeneous: that is, I(λx) = λI(x), for any λ > 0, x ∈ R+
d ;
c) I(·) has compact level sets; speciﬁcally, inf x∈Rd

d \ {0};

+:xi>c I(x) = c, for all c ≥ 0 and i = 1, . . . , d.

EFFICIENCY IN BLACK-BOX IMPORTANCE SAMPLING

13

Conversely, any function I : Rd

+ → R+ which satisﬁes the above conditions can be used to readily
specify a joint distribution for Y which has standard exponential marginals and satisﬁes the tail LDP.
This is veriﬁed, for instance, by considering Y for which P (Y > y) = exp(− inf z>y I(z)). While the
rate function I(·) is unique for a given distribution for which the tail LDP holds, it is instructive to note
that there may be multiple distributions which give rise to the same limit I(·). Indeed, the relation “has
the same rate function I(·) in the tail LDP” is an equivalence relation, and every function I(·) satisfying
properties (a) - (c) in Lemma 3.4 speciﬁes a equivalence class of distributions for the random vector
Y . Thus, the nonparametric nature of the limiting function I(·) oﬀers a great amount of expressive
power in capturing the joint dependence features observed in the the tail regions. The hazard functions
Λ1(·), . . . , Λd(·) in Assumption 2, on the other hand, oﬀer ﬂexibility in terms of specifying marginal
distributions with various tail strengths.

Figure 3. Illustration of the level sets of I(·) capturing diﬀerent strengths of the posi-
tive (indicated (+)) or negative (indicated (-)) tail correlations between the components
of Y = (Y1, Y2). Range of axes =[0,5]

Gaussian copula (+)

Gaussian copula (-)

Clayton copula (+)

Clayton copula (-)

4. Large deviations analysis for the distribution tail of L(X)

4.1. Asymptotics for P (L(X) ≥ u). In this section, we characterize the exponential rate at which
P (L(X) ≥ u) decays as in,

P (L(X) ≥ u) = exp{−t(u)[I ∗ + o(1)]},

as u → ∞,

where the function t(u), which grows to inﬁnity as u → ∞, is identiﬁed in terms of the marginal hazard
functions Λ1, . . . , Λd described in Assumption 2, and the constant I ∗ is identiﬁed in terms of the marginal
tail parameters α = (α1, . . . , αd) and the limiting functions L∗(·) and I(·) in Assumptions 1 and 3. In
order to state the result, let us deﬁne Λmin : R+ → R+ and ˆq : R+ → Rd

+ as,

Λmin(u) := min

i=1,...,d

Λi(u)

and

ˆq(t) :=

q(t1)
(cid:107)q(t1)(cid:107)∞

,

(14)

where q : Rd
specifying the left-continuous inverse of the hazard function Λi(·).

+ → Rd denotes the component-wise inverse q(y) = (q1(y1), . . . , qd(yd)), with qi(yi) = Λ←

i (yi)

Theorem 4.1 (Tail asymptotic). Suppose that the marginal distributions of the components X1, . . . , Xd
of the random vector X = (X1, . . . , Xd) satisfy Assumption 2 and the standardized vector Y = Λ(X) is
such that the tail LDP in Theorem 3.3 holds. Further suppose that the limit q∗ := limt→∞ ˆq(t) exists.
Then, for any L(·) satisfying Assumption 1,

log P (L(X) ≥ u) = −Λmin

(cid:0)u1/ρ(cid:1)(cid:2)I ∗ + o(1)(cid:3),

as u → ∞; here the non-negative constant I ∗ is given by,

I ∗ := inf (cid:8)I(y) : L∗(cid:0)q∗y1/α(cid:1) ≥ 1, y ≥ 0(cid:9).

(15)

(16)

012345012345Gaussian Copula - 3ositive Correlation012345012345Gaussian Copula - 1egative Correlation012345012345ClDyton CopulD - 3ositive Dependence012345012345ClDyton CopulD - 1egDtive Dependence14

EFFICIENCY IN BLACK-BOX IMPORTANCE SAMPLING

As a consequence of Theorem 3.3, the tail asymptotic in (15) holds automatically for any joint distri-
bution speciﬁed via Assumptions 2 - 3. While there is a rich literature on tail risk probabilities of the
form P (X1 + · · · + Xd > u) where X1, . . . , Xd are independent, the treatment for more general objectives
which arise in modeling operations exist only for speciﬁc instances. As examples, we have Glasserman
et al. (2000) deriving asymptotics of the form (15) for a speciﬁc quadratic objective L(·) motivated from
the delta-gamma approximation of portfolio losses. Likewise, Juneja et al. (2007), Blanchet et al. (2019),
Ahn & Kim (2018), Bai et al. (2020) derive asymptotics of the form (15) considering piecewise linear
L(·) motivated from settings requiring evaluation of the likelihood of excessive project delays, cascading
failures in product distribution and banking networks, safety in intelligent physical systems, etc. These
results rely, however, on exploiting the speciﬁc structure of L(·) and the distributional assumptions such
as X being multivariate normal, or elliptical, or possessing independent components. On the other hand,
Theorem 4.1 is applicable for L(·) as general as the instances considered in Examples 2.1 - 2.3 and across
a broad spectrum of distributions.

Remark 4.2 (Suﬃcient conditions on existence of q∗). Let ri(x) := Λmin(x)/Λi(x), i = 1, . . . , d, and
α∗ := mini=1,...,d αi. With q := Λ←, the limit q∗ = (q∗

d), when exists, satisﬁes,

1, . . . , q∗
ri(x)1/α∗ ,

q∗
i = lim
x→∞

(17)

for i in {1, . . . , d}; see the discussion at the end of Appendix D for additional explanation. Then for any
i such that αi > α∗, the limit in (17) expressly evaluates to q∗
i = 0. For all i such that αi = α∗, we have
that the limit in (17) exists if Λi(x) = xα∗ (ci + o(1)), for some positive constant ci; or more generally
if | d
dx ri(x)| = O(x−(1+ε)), for some ε > 0. As the latter condition merely restricts the magnitude of
oscillations of the ratio Λmin(x)/Λi(x), we have that the limit q∗ exists for commonly used parametric
distribution families.

To interpret the tail asymptotic (15), ﬁrst note that the occurrence of Λmin(·) := mini=1,...,d Λi(·) in
the denominator in (15) is aligned with the phenomenon that the “heaviest tail wins”. This observation
is well-known within the specialized context of sums of random variables, see, for example, (Hult et al.
2012, Example 8.17). Thus, as is expected, the presence of an heavier tail results in larger probability
for P (L(X) ≥ u). With q∗ characterized as in (17) in terms of the ratio ri(x) := Λmin(x)/Λi(x),
the appearance of q∗ in (15) captures the diﬀerences in tail heaviness of the marginal distributions of
X1, . . . , Xd. In the simpler case where all the components are identically distributed, we have q∗ = 1. If,
for example, X1 is the component with the heaviest tail in X = (X1, X2) and if P (X1 > x)/P (X2 > x) =
O(1) as x → ∞, then q∗ = (1, c) for some constant c ∈ (0, 1); if on the other hand, Λ1(x)/Λ2(x) → ∞,
then q∗ = (1, 0). The same description is applicable in higher dimensions where d > 2. The constant I ∗,
which characterizes the rate speciﬁc to the given event {L(X) ≥ u}, is referred as the large-deviations
exponent. Any point y which attains the inﬁmum in (16) is referred as the minimum rate point.

4.2. An example application to distribution networks. In this section, we discuss an application
of the tail asymptotic (15) to the linear programming model considered in Blanchet et al. (2019) for
distributing a commodity among various nodes in a network. A brief description of the model, motivated
from excess load management in electric power grids and cloud-computing settings, is as follows. Consider
a directed graph G = (V, E), where V = {1, . . . , d} represents the set of nodes in the network and
E = {(i, j) : ∃ edge from i to j} is the set of directed edges. Each vertex i is endowed with a supply si
and incurs a random demand Di. While every vertex i tries to serve its demand, it distributes its excess
demand to its neighbouring nodes if Di exceeds si. Let aij be the fraction of demand excess of node i
distributed to node j. Note that if (i, j) (cid:54)∈ E, then aij = 0, and that (cid:80)d
j=1 aij = 1; that is all of the excess
demand from node i is distributed to its neighbours. The objective is to estimate the probability that the
total amount of load not served at their originating regions exceed a tolerance threshold. In the event of
such exceedance, the network is said to have failed. Denoting A := {aij : i, j ∈ V }, D := (D1, . . . , Dd),

EFFICIENCY IN BLACK-BOX IMPORTANCE SAMPLING

15

and M := Id − A, the total excess demand in the network is the value of the following LP:

L(D, s) := max {y(cid:124) (D − s) : M y ≤ 1, y ≥ 0} .

In the above, Id denotes the (d × d) identity matrix. See Blanchet et al. (2019) for the steps involved in
arriving at this formulation and the desirable invariance properties associated with it.

For notational convenience, we write the supply vector s = (s1, . . . , sd) as s = uθ, where u = (cid:80)d

i=1 si
is the total supply at all nodes and θ = s/u is the vector in the unit simplex Ld
1 specifying relative
1 := {θ ≥ 0 : 1(cid:124)θ = 1}. It is evident from the above formulation
supply levels at each node. Here Ld
that the network failure event gets rarer when the total supply u is increased. Proposition 4.3 below
utilizes the tail asymptotics derived in Theorem 4.1 to arrive at a large-deviations approximation of the
probability of network failure and uses it to characterize failure-mitigating relative supply levels. The
tolerance threshold k(u), which speciﬁes the network failure event as,

may be a ﬁxed positive constant, or even taken to grow as in k(u) = o(u) as u → ∞.

N F uθ := {L(D, uθ) ≥ k(u)},

Proposition 4.3. Suppose that the matrix A is irreducible and the assumptions on X in Theorem 4.1
hold when we let X = D. Then, with the supply levels s = (s1, . . . , sd) re-expressed as s = uθ with u > 0
and θ ∈ Ld

1, we have the following for any choice of k(·) satisfying k(u)/u → 0 as u → ∞:

lim
u→∞

log P (N F uθ)
Λmin(u)

= − min

i=1,...,d

(θi/q∗

i )α∗

(18)

where α∗ := mini=1,...,d αi. Moreover the choice θ∗ = q∗
(cid:107)q∗(cid:107)1
(18). Consequently, given any ε > 0, there exists δ such that for all suﬃciently large u,

uniquely minimizes the right-hand side of

log P (N F s)
inf s∈Su log P (N F s)

< 1 + ε

if and only if the supply vector s lies in the collection {s > 0 : s/u ∈ Ld

1, (cid:107)s/u − θ∗(cid:107) < δ}.

For the selection of node-wise supply levels s which result in a low probability for network failure, a
central planner may utilize Proposition 4.3 by narrowing the selection choices to supply levels which are
roughly proportional to the relative tail heaviness q∗ of the demand incurred in the respective nodes.
Interestingly, such a selection is robust from the perspective of keeping the network failure probability
low, as it is agnostic to a) the dependence structure between the demands encountered in diﬀerent nodes
and b) the actual speciﬁcation of the marginal distributions. The selection criterion instead depends
only on the relative tail heaviness captured by the entries in the vector q∗. In the speciﬁc example where
the demand D is Gaussian, we have the least risky allocation θ∗ given by θ∗
j=1 σj, where σi
denotes the standard deviation of the demand Di originating at node i.

i = σi/ (cid:80)d

5. Variance reduction properties of Algorithm 1

Recall that the IS estimator ¯ζN (u) and Algorithm 1 were introduced in Section 2.3 for the purpose of
estimating the probability pu := P {L(X) ≥ u}. In short, ¯ζN (u) is the sample mean computed from N
independent replications of the random variable,

ζ(u) := L(Z)I (L(Z) ≥ u) ,

where Z := T (X) and L(Z) := J(X)fX (Z)/fX (X), with J(·) computed as in (9a). Let

M2,u := E(cid:2) ζ(u)2 (cid:3)
denote the second moment of ζ(u). With ¯ζN (u) being the average of N independent samples of ζ(u),
the variance of ¯ζN (u) is given by (cid:0)M2,u − p2
(cid:1) N −1. Theorem 5.2, stated towards the end of this section,

u

16

EFFICIENCY IN BLACK-BOX IMPORTANCE SAMPLING

utilizes the utilize the large deviations machinery developed in Sections 3 - 4 to establish that the second
moment M2,u is suitably small and the family {ζ(u) : u > 0} is log-eﬃcient.

In the passage below, we ﬁrst develop an understanding behind the eﬀectiveness of the IS transforma-
tion T (·). As is well known, the general idea behind the use of importance sampling is to obtain samples
for the random vector X from an alternative probability distribution under which the event of interest
{L(X) ≥ u} is not as rare. Therefore the eﬀectiveness of an IS scheme is directly determined by the
choice of the alternative distribution employed. Theoretically speaking, the best such choice is simply
the conditional distribution,

P (X ∈ dx | L(X) ≥ u) =

fX (x)
P (L(X) ≥ u)

dx,

(19)

upon obtaining samples from which the variance of the resulting IS estimator is zero (see Theorem 1.2 in
(Asmussen & Glynn 2007, Chapter 5)). This choice is however not practical as its speciﬁcation relies on
the unknown quantity P (L(X) ≥ u). Despite this limitation, the zero-variance IS distribution in (19) is
often utilized as a guide for identifying a good choice of IS distribution. Indeed, most veriﬁably eﬀective
IS schemes seek to identify a proposal IS distribution which possess relevant aspects of the respective
zero-variance IS distribution and approximate it in a suitable manner; see, for eg., (Asmussen & Glynn
2007, Chapter 6), Juneja & Shahabuddin (2002).

In our context of estimating P (L(X) ≥ u), Proposition 5.1 below explains how the IS random vector
Z = T (X) used in Algorithm 1 induces a distribution which is asymptotically similar to the zero-variance
distribution (19). In order to state Proposition 5.1, let l(u) denote the hyper-parameter choice employed
in Algorithm 1 for the task of estimating pu. For brevity, deﬁne

t(u) := Λmin

(cid:0)u1/ρ(cid:1)

and

˜t(u) := Λmin

(cid:0)l(u)1/ρ(cid:1).

Proposition 5.1 (Self-similarity of optimal IS distributions). Suppose that X satisﬁes the conditions in
Theorem 4.1, the loss L(·) satisﬁes Assumption 1 and the resulting limiting function L∗(·) is such that
L∗(q∗x) is not identically zero for x ∈ R+
d . Further, let Λ(·) be strictly monotone and the choice l = l(u)
in Algorithm 1 be such that l(u)/u → γ for some γ ∈ (0, 1) as u → ∞. Then the optimal IS distribution
(19) and the distribution of Z conditioned on {L(Z) ≥ u} are asymptotically similar in the following
sense: There exist random vectors ξzv

u , ξu such that

(cid:1),
Law of X | L(X) ≥ u = Law of q(cid:0)t(u)ξzv
(cid:1),
Law of Z | L(Z) ≥ u = Law of q(cid:0)t(u)ξu

u

and the probability densities of random vectors ξzv
trate their measure on the identical set of rate points as in,

u , ξu, denoted respectively by f zv

u (·) and f is

u (·), concen-

u (z) = exp {−t(u) [I(z) − I ∗ + o(1)]} ,
f zv
u (z) = exp (cid:8)−˜t(u) [I(z) − I ∗ + o(1)](cid:9) ,
f is
as u → ∞, for every z in the respective supports. In turn, for any ε > 0, there exist u0 such that for all
u > u0, the support sets of ξzv

u , ξu are contained in {z : I(z) > I ∗ + ε}.

(20b)

(20a)

In the above statement, I ∗ is identiﬁed as in (16). Observe that the functions t(u), ˜t(u) increase to
inﬁnity as u → ∞. Therefore we have from the approximation for f zv
u (·) in (20a) that the zero variance
distribution concentrates its measure in the region (cid:8)q(cid:0)t(u)z(cid:1) : z ∈ Rd
+, I(z) < I ∗ + ε(cid:9) , where ε > 0 can
be made arbitrarily small as u → ∞. Interestingly, the approximation for f is
u (·) in (20b) asserts that the
IS vector Z = T (X), conditioned on lying in the rare set, also concentrates its mass in the same region,
albeit at a slower rate ˜t(u) < t(u).
A numerical illustration. Figure 5 below provides a pictorial illustration of this identical measure
concentration property by plotting samples from the conditional distributions observed for the loss
L(x) = 0.5(x1 + x2). A ﬁxed number of samples from the zero-variance IS distribution are plotted

EFFICIENCY IN BLACK-BOX IMPORTANCE SAMPLING

17

in red colour in Figures 5(A) - 5(C) below considering diﬀerent distribution choices for X = (X1, X2). In
particular, (X1, X2) are taken to have identical normal marginal distributions in Figure 5(A), exponen-
tial marginal distributions in Figure 5(B), and heavier-tailed Weibull marginal distributions for which
α = (0.5, 0.5) in Figure 5(C). To illustrate cases where the concentration of conditional distributions
happen in diﬀerent regions, the joint distributions are taken to be given by i) a Gaussian copula with
correlation coeﬃcient = 0.5 in Figures 5(A) - 5(B), and ii) independent copula in Figure 5(C). In order
to facilitate an easy comparison across these diﬀerent choices of joint distributions, the numbers u and
l(u) are taken to be such that P (L(X) ≥ u) = 10−5 and P (L(X) ≥ l(u)) = 10−2 in each of these cases.
An identical number of samples of the IS random vector Z | L(Z) > u, computed from the chosen values
of (u, l(u)) for each of the above distributions, are plotted in blue colour in the respective sub-ﬁgures in
Figure 5. In this setup, the following observations are readily inferred from Figure 5.

Figure 5. Figures (A) - (C) plot independent samples from the zero-variance distribu-
tion (in red) and that of the IS vector Z | L(Z) ≥ u (in blue) and illustrate their identical
concentration behaviour. Contours indicate the level sets of the respective joint distri-
butions. Figures (D) - (F) indicate the respective histograms for κ(X) | L(Z) ≥ u
involved in the transformation Z = T (X).

(A)
correlation= 0.5

Gaussian

marginals,

(B) Exponential marginals,
correlation= 0.5

(C) Weibull marginals with
α = (0.5, 0.5), uncorrelated

The zero variance and the IS samples tend to concentrate in the same neighbourhoods in all the three
cases considered in Figures 5(A) - 5(C) as asserted by Proposition 5.1. Regardless of the distinctions in
the regions where the zero variance distribution concentrates, the IS transformation Z = T (X) replicates
the concentration in the same neighbourhood by implicitly learning from the samples which are not as
rare. Indeed, the asymptotics (20a) - (20b) makes this rigorous.

To gain intuition behind this phenomenon, we ﬁrst see that the multiplicative factor (u/l)κ(x) (cid:29)
1 in the transformation T (x) = (u/l)κ(x)x ensures that the IS vector T (X) is more likely to take
more extreme values than X. Here the exponent κ(X) ensures that the components are relatively
magniﬁed only to the extent necessary.
Indeed, a quick examination by applying the deﬁnition of
κ(x) in (6) to the red points in the respective cases in Figure 5 reveals the following observation: The
distribution of κ(X) | L(Z) ≥ u concentrates in the neighbourhood of the points {(1, 0), (0, 1)} in Figure
5(F), unlike those in Figures 5(D) - 5(F) where its concentration is in the vicinity of (1, 1). While

012345678x012345678y 05101520x05101520y 020406080100120140160x020406080100120140160y 18

EFFICIENCY IN BLACK-BOX IMPORTANCE SAMPLING

a naive multiplication by the factor (u/l) will result in both components (X1, X2) being magniﬁed,
the introduction of (u/l)κ(X) lets the conditional distribution of Z concentrate appropriately near the
axes in the heavier-tailed case in Figure 5(C). Thus the transformation T (·) is crucial in adjusting the
magniﬁcation of diﬀerent components of X such that the transformed vector Z = T (X) concentrates
measure in the regions deemed suitable by the zero-variance IS distribution. To develop a precise
(cid:3)
understanding, we refer readers to the proof of Proposition 5.1.

Interestingly, this strategy of informing concentration properties of the zero variance IS distribution
by means of the IS transformation T (·) results in low variance guarantees for the IS estimator returned
by Algorithm 1 even if the hyper-parameter l is taken to be negligibly small when compared to u. This is
the content of Theorem 5.2 below. In order to state Theorem 5.2, let us introduce a regularity condition
on the marginal distributions of X.

Assumption 4. There exists x0 > 0 such that for every i = 1, . . . , d, the cumulative hazard function,
Λi(x) = − log P (Xi > x), is either a convex or concave function over the interval x ∈ [x0, ∞).

The condition in Assumption 4 is readily satisﬁed for the examples in Tables 1 - 2 in Appendix B and

for other commonly used probability distributions.

Theorem 5.2 (Logarithmic eﬃciency). Suppose X satisﬁes Assumptions 2 - 4 and the limit q∗ exists.
For the loss L(·), suppose that L(·) satisﬁes Assumption 1 and the limiting function L∗(·) is such that
L∗(q∗x) is not identically zero for x ∈ R+
d . Then for any choice of parameter l in the IS transforma-
tion (5) which is taken to be slowly varying in u, the family of estimators {ζ(u) : u > 0} is logarithmically
eﬃcient in estimating pu := P (L(X) ≥ u) : that is,

lim
u→∞

log M2,u
log p2
u

= 1.

(21)

Since E[ζ(u)] = pu (see Proposition 2.4), it is evident that the second moment M2,u ≥ p2

u. With this
lower bound serving as the best-attainable second moment, Theorem 5.2 asserts that this benchmark
lower bound is met in the logarithmic scale by the proposed family of estimators: precisely, M2,u =
), as u → ∞, for any choice of ε > 0. In terms of the variance of the IS estimator ¯ζN (u) returned
o(p2−ε
u
by Algorithm 1, we have the reduced variance in (10) as a consequence of Theorem 5.2.

6. Application to Portfolio Credit Risk

Eﬃcient IS schemes for estimating excess loss probabilities of a portfolio of loans have been considered
in Glasserman & Li (2005), Bassamboo et al. (2008), Glasserman et al. (2008). A salient feature of
these approaches is the ﬂexibility to have correlated loan defaults informed suitably via Gaussian or
extremal copula models. The repertoire of loan default probability models considered in the literature
have naturally expanded to include machine learning based approaches aiming to capture more intricate
interactions between the underlying covariates; see, for example, Sirignano & Giesecke (2019), Sirignano
et al. (2018) and references therein. The treatment in this section capitalizes on the generic applicability
of the proposed IS scheme to demonstrate how eﬃcient samplers can be similarly devised in this setting.

To introduce the default model studied here, consider a portfolio of m loans indexed by {1, . . . , m}
belonging to J ≥ 1 types. For any i ∈ {1, . . . , m}, let t(i) ∈ {1, . . . , J} denote the type of loan i, Yi
denote the indicator random variable that loan i defaults over a ﬁxed horizon of interest, ei denote the
exposure upon its default, and vi ∈ Rk denote loan-speciﬁc factors (such as original interest rate, original
loan-to-value, original debt-to-income ratios, FICO score, pre-payment penalty, etc.) which are ﬁxed for
a given loan. The average loss incurred by the portfolio is Lm := m−1 (cid:80)m
i=1 eiYi. For a given q ∈ (0, 1),
our objective is to estimate the probability of the excess loss event,

Em := {Lm ≥ q¯em},

(22)

EFFICIENCY IN BLACK-BOX IMPORTANCE SAMPLING

19

which is the event that the incurred loss exceeds a given fraction of the maximum loss. To restrict the
focus to main ideas, we take the exposure ei to be ﬁxed for every i ∈ {1, . . . , m} and satisfy ei ∈ (0, e0],
where e0 < ∞ is the maximum exposure level. If we let ¯em := m−1 (cid:80)m
i=1 ei denote the average of the
exposures, then it is clear that Lm ∈ (0, ¯em).

The joint distribution of the default variables Y1, . . . , Ym is taken to be determined by the loan-speciﬁc
variables v1, . . . , vm and some common stochastic factors X ∈ Rd
+ which aﬀect all loans. The common
factors X may capture region-level economic eﬀects, such as those given by unemployment level, median
income, etc., whose evolution is uncertain over the time horizon of interest. See Sirignano et al. (2018) for
an account of the factors useful in modeling mortgage defaults. Conditioned on X, the default indicators
Y1, . . . , Ym are taken to be independent and the respective conditional default probabilities are speciﬁed
by the family of functions {Wj}j=1,...,J as in,

P (Yi = 1 | X) =

exp (cid:0)Wt(i)(X, vi) − γ(cid:1)
1 + exp (cid:0)Wt(i)(X, vi) − γ(cid:1)
almost surely; in the above expression, Wj : Rd × Rk → R and γ is a parameter modeling the rarity of
loan defaults. While the functions Wj(·) are typically modeled as members of parametric families such as
in Example 6.1 below, we only require Assumption 5 below which is merely a restatement of Assumption
1b suitably adapted to this portfolio credit risk setting.

i = 1, . . . , m,

(23)

Assumption 5. For j ∈ {1, . . . , J}, the function Wj : Rd × Rk → R is such that for any sequence
{xn, vn}n≥1 of Rd

+ satisfying (xn, vn) → (x, v), we have
Wj(nxn, vn)
nρ

= W ∗

j (x),

lim
n→∞
j : Rd

where ρ is a positive constant and W ∗
W ∗

j (x) > 0} is not empty.

+ → R is the limiting function such that the cone {x ∈ Rd

+ :

Example 6.1 (Logit loan default model in Sirignano & Giesecke (2019)). Suppose J = 1 and the
conditional default probability is modeled as in (23) with the function W1(·) given by a ReLU neural
network described recursively as in,

W1(x, v) = θ(cid:124)LK(x, v) + θ0, Lk(x, v) = (AkLk−1(x, v) − bk)+, k = 1, . . . , K,

where L0(x, v) = (x, v) is the concatenated column vector comprising the entries of x followed by
the entries of v, K is a positive integer denoting the number of layers, and for any k ∈ {1, . . . , K},
Ak, bk respectively denote a properly dimensioned weight matrix and weight vector. For the case J ≥ 1,
one may similarly consider functions {W1, . . . , WJ }, potentially with diﬀerences in the number of layers
and weight matrices. Next for k ∈ {1, . . . , K}, let Bk denotes the sub-matrix formed by the ﬁrst
d rows of Ak. In terms of Assumption 5, we have ρ = 1 and the limit W ∗
1 (·) given by, W ∗
1 (x) =
θ(cid:124)(cid:0)BK
+ : W ∗
1 (x) > 0} is non-empty if at
least one of the entries of the column vector θ(cid:124)BKBK−1 . . . B1 is positive.

, for this example. The cone {x ∈ Rd

(cid:0)BK−1 · · · (cid:0)B1x(cid:1)+(cid:1)+(cid:1)+

The task of estimating P (Em) is particularly challenging in portfolios composed of high quality loans
with small default probabilities. This rarity is speciﬁed, for example, in the default probabilities by
letting the parameter γ be large in (23). In order to study how an IS scheme fares when the target event
becomes increasingly rare, we embed the given problem in the sequence of estimation problems indexed
by m, with m → ∞ and the respective γ in (23) and the exposures satisfying

γm−η → c,

¯em → ¯e,

1
m

(cid:88)

i:t(i)=j

ei → ¯cj,

(24)

for some positive constants c, η, ¯e, ¯cj, j = 1, . . . , J. Similar asymptotic frameworks form the basis of
analysis in Glasserman & Li (2005), Bassamboo et al. (2008), Glasserman et al. (2008); see Deo &
Juneja (2020) for a detailed exposition on the appropriateness of this regime in the context of logit

20

EFFICIENCY IN BLACK-BOX IMPORTANCE SAMPLING

default models. We impose a mild technical requirement that the parameter q in (22) does not lie in the
set {¯e−1 (cid:80)
j∈I ¯cj : I ⊆ {1, . . . , J}}. This unrestritive condition is common in the literature for ease of
analysis; see Bassamboo et al. (2008), Glasserman et al. (2007).

We ﬁrst present a tail asymptotic for the excess loss probability P (Em) in Theorem 6.2 below as an
application of Theorem 4.1. In order to state the result, let Jm denote the collection of subsets of the
set {1, . . . , J} whose collective exposure exceeds the speciﬁed threshold; in other words, Jm := {I ⊆
{1, . . . , J} : (cid:80)

Wt(i)(x, vi),

and Am := {Lcr(X) > c(1 − εm)mη} ,

(25)

i: t(i)∈I ei ≥ mq¯em}. Let
Lcr(x) := max
I∈Jm

min
t(i)∈I

where (εm : m ≥ 1) is a sequence decreasing to zero as m → ∞. With the event Am amenable to be
treated via Theorem 4.1, Theorem 6.2 below establishes that the events Em and Am coincide as m → ∞
and uses this observation to establish the asymptotic for P (Am) and P (Em).

Theorem 6.2 (Tail asymptotic for P (Em)). Suppose that the conditional default probabilities are spec-
iﬁed as in (23), with the functions W1(·), . . . , WJ (·) satisfying Assumption 5. In addition, suppose that
the convergences in (24) hold and X satisﬁes the conditions in Theorem 4.1 with α∗ := mini=1,...,d αi
satisfying α∗ < ρ(1 + η−1). Then as m → ∞,

P (cid:0)Em \ Am
log P (Em) ∼ log P (Am) = −Λmin

(cid:1) = o (P (Em))

(cid:0)mη/ρ(cid:1)(cid:0)c(cid:48)Icr + o(1)(cid:1),

and

(26)

for any sequence εm → 0 and εmmηr/ρ → ∞, where r < ρ(1 + η−1) − α∗. Here the constant c(cid:48) := cα∗/ρ
and Icr are identiﬁed as,

Icr := inf (cid:8)I(y) : max

I∈J

min
k∈I

W ∗

k (q∗y1/α) ≥ 1, y ≥ 0(cid:9),

(27)

in terms of J := {I ⊆ {1, . . . , J} : (cid:80)

k∈I ¯ck ≥ q¯e} and q∗ speciﬁed as in Theorem 4.1.

Since the IS procedure introduced in Section 2.3 is readily applicable for the purpose of estimating
P (Am), thanks to (26), one may suitably modify the IS scheme to result in an eﬃcient IS scheme
for estimating the desired excess loss probability P (Em). Algorithm 2 below illustrates how this can
be achieved in the setting which otherwise requires, as demonstrated in Glasserman et al. (2008), a
relatively complex methodology to tackle the combinatorial structure embedded in the problem. To adapt
Algorithm 1 to this setting, we begin with the generation of samples of the IS variate by applying the IS
transformation Z = T (X) as in Step 1 of Algorithm 1. Next, to ensure that the defaults are observed
appropriately often in the IS scheme given a realization of Z, we obtain samples of the default variables
Y1, . . . , Ym from the conditional distribution Pis( · |Z) described as follows: Conditioned on the realization
of Z, we take Y1, . . . , Ym as independent Bernoulli random variables with Pis(Yi = 1|Z) = ˜pi(Z), where
˜pi(z) is deﬁned as follows: For i = 1, . . . , m, deﬁne

˜pi(z) :=

pi(z) exp(λei)
1 + pi(z)(exp(λei) − 1)

,

(28)

where pi(z) := P (Yi = 1|X = z) denotes the given loan default probability in (23) and λ is chosen to be
the unique solution of the optimization problem, minγ≥0{−γq + ψm(γ, x)}, with ψm(·, z) denoting the
conditional cumulant of mLm written as follows:

ψm(θ, z) =

1
m

log E(cid:2) exp(θmLm)| X = z(cid:3) =

1
m

m
(cid:88)

i=1

log (cid:0)1 + pi(z)(exp(θei) − 1)).

(29)

This prescription for conditional sampling for the default variables Y1, . . . , Ym is the same as that
in the literature, see Glasserman & Li (2005), Glasserman et al. (2008). In turn, the probabilities in
(28) constitute a speciﬁc instance of the well-known exponential twisting procedure applied to Bernoulli
random variables; see Sadowsky & Bucklew (1990), Juneja & Shahabuddin (2006) for detailed accounts

EFFICIENCY IN BLACK-BOX IMPORTANCE SAMPLING

21

on exponential twisting. Under this chosen IS conditional distribution, we have that the conditional
expectation of Lm equals the loss level q¯em, which explains why the event Em := {Lm ≥ q¯em} is
observed more often under the distribution Pis(·).

While the above conditional sampling is conventional, it may be worthwhile to observe that the
selection of a suitable IS distribution for the common factors X is signiﬁcantly challenging: In the
instance described in Example 6.1, this task corresponds to sampling from regions where the ReLU
network takes suitably large values. This non-trivial question is however automatically handled by the
IS transformation T (·) in the prescribed Algorithm 2 below. Given a realization of conditional default
probabilities by means of X, the conditional sampling as described above helps in steering the samples
of Y1, . . . , Ym to be more likely in satisfying {Lm ≥ q¯em}, if necessary.

Algorithm 2: IS algorithm for estimating the excess loss probability P (Em)

Input: Loss threshold q ∈ (0, 1), N independent samples X 1, . . . , X N of X, loan-speciﬁc values

{v1, . . . , vm}, u := γ, hyper-parameter l.

Procedure:
1. Transform the samples and compute associated likelihood: Compute IS samples
Z1, . . . , Zn and their likelihoods L1, . . . LN as described in Steps 1 - 2 of Algorithm 1.
2. Obtain samples of portfolio loss: For k = 1, . . . , N, do the following steps:

a) Generate the loan default variables Y1,k, . . . , Ym,k, where for i = 1, . . . , m, Yi,k are independent
Bernoulli random variable with success probability ˜pi(Zk) in (28); speciﬁcally, the parameter
λm(Zk) in (28) is chosen to be the unique solution of minθ≥0{−θq¯em + ψm(θ, Zk)}.

b) Set Lm,k := m−1 (cid:80)m
c) Compute the conditional likelihood associated with the collection {Y1,k, . . . , Ym,k} by letting

i=1 eiYi,k to be the portfolio loss.

Ly

k := exp (−m[λm(Zk)Lm,k − ψm(λm(Zk), Zk)]) .

3. Return the output estimator. Return the IS average computed as in,

¯ζN (m) =

1
N

N
(cid:88)

k=1

LkLy
k

I(Lm,k ≥ q¯em).

Proposition 6.3 (Log-eﬃciency of Algorithm 2). Suppose that X satisﬁes the conditions in Theorem
5.2 and the density of X is bounded away from 0 on compact subsets of R+
d . For the conditional default
probabilities speciﬁed in (23), let the functions W1(·), . . . , WJ (·) satisfy Assumption 5 and the resulting
constant Icr in (27) is ﬁnite. Then under the asymptotic regime in (24) we have

lim
m→∞

log E[¯ζN (m)2]
log P (Em)2 = 1,

N ≥ 1.

for any choice of the parameter l which is slowly varying in m. In other words, the family of unbiased
estimators returned by Algorithm 2 is logarithmically eﬃcient in estimating P (Em).

7. Log-efficiency in the presence of heavier-tailed distributions

Here we present the counterpart for Theorems 4.1 and 5.2 when one or more of the components of
X1, . . . , Xd are heavier-tailed than considered in Assumption 2. Interestingly, the same Algorithm 1 is
shown to oﬀer asymptotically optimal variance reduction in the presence of heavier-tailed distributions.
As in Section 3, we write Λi(x) = − log P (Xi > x), for x ∈ R. In addition, let
¯Λi(x) := − log P (log Xi > x) = Λi ◦ exp(x), x ∈ R.

Assumption 6. For any i ∈ {1, . . . , d} for which Λi does not satisfy Assumption 2, ¯Λi is continuous
and strictly increasing in the interval (x0, ∞), and ¯Λi ∈ RV(αi), for some αi ∈ [1, ∞).

22

EFFICIENCY IN BLACK-BOX IMPORTANCE SAMPLING

Assumption 6 enriches Assumption 2 by including the possibility that, if the hazard function for
Xi is not regularly varying, then the hazard function for log Xi is instead regularly varying. This
immediately brings commonly used heavier-tailed distributions such as log-normal, pareto and regularly
varying distributions under the framework considered. Indeed, if Xi is log-normally distributed, we have
¯Λi(x) = x2/2 − log x(1 + o(1)) satisfying ¯Λi ∈ RV(2). Instead, if Xi is a pareto or regularly varying
random variable, we have ¯Λi(x) = αx − log L(ex), for some α > 0 and a slowly varying function L(·);
in this case, ¯Λi ∈ RV(1) (see Table 2). Since the case where all the components X1, . . . , Xd satisfy
Assumption 2 is treated in the sections before, we proceed without any loss of generality by assuming
here that there exists at least one component Xi for which Assumption 2 is not satisﬁed. Analogous to
the deﬁnition (14) in Section 4.1, let us assign

ˆq(t) :=

log q(t1)
(cid:107) log q(t1)(cid:107)∞

,

and

q∗ := lim
t→∞

ˆq(t),

(30)

if the limit exists. Here, as in Section 4.1, q : Rd
(q1(y1), . . . , qd(yd)), with qi(yi) = Λ←
We proceed assuming that the loss L(·) satisﬁes the following variation of Assumption 1.

+ → Rd denotes the component-wise inverse q(y) =
i (yi) specifying the left-continuous inverse of the hazard function.

Assumption 7. Suppose that the function L : Rd → R+ satisﬁes Assumption 1 and the limiting function
L∗(·) is such that

n−1 log L∗ (exp(nx)) = ¯L∗(x),

lim
n→∞

for all x ∈ Rd

+ and some limiting function ¯L∗ : Rd

+ → R+.

For instance, in the examples considered earlier in Section 2, we have the resulting ¯L∗(x) = max{xi :
i = 1, . . . , d} in Example 2.1 and ¯L∗(x) = 2 max{xi : i = 1, . . . , d} in Example 2.2. We have the following
counterparts to Theorems 4.1 & 7.2 in the presence of heavier tailed distributions.

Theorem 7.1 (Tail asymptotic). Suppose that the marginal distributions of the components X1, . . . , Xd
of the random vector X = (X1, . . . , Xd) satisfy Assumption 6 and the standardized vector Y = Λ(X) is
such that the tail LDP in Theorem 3.3 holds. Further suppose that the limit q∗ in (30) exists. Then for
any L(·) satisfying Assumption 7,

log P (L(X) ≥ u) = −Λmin(u)(cid:0)I ∗ + o(1)(cid:1),

u → ∞

(31)

where the non-negative constant I ∗ := inf (cid:8)I(y) : ¯L∗(cid:0)q∗y1/α(cid:1) ≥ 1, y ≥ 0(cid:9).

Complementing the rich literature on tail asymptotics for sums and maxima of independent heavy-
tailed variables, we have Mainik & R¨uschendorf (2010), Kley et al. (2016), Kley & Kl¨uppelberg (2016)
treating tail asymptotics of the form P (L(X) > u) in speciﬁc linear portfolio and reinsurance risk
settings involving dependent heavy-tailed random variables. Speciﬁcally, these tail asymptotics invoke
the structure present in multivariate regularly varying distributions to derive sharper tail asymptotics
compared to (31). The asymptotic (31) is however applicable more generally and for a broader class of
distributions which also includes log-normal, logweibull distributions and dependent regularly varying
distributions (such as those arising with the use of Gaussian copula) in which the components exhibit
asymptotic tail independence.

Theorem 7.2 (Logarithmic eﬃciency of Algorithm 1 in the presence of heavier tails). Suppose that
the random vector X satisﬁes Assumptions 3 - 4, 6 and the limit q∗ in (30) exists. For the loss L(·),
suppose that L(·) satisﬁes Assumption 7 and the resulting limiting function ¯L∗(·) is such that ¯L∗(q∗x)
is not identically zero for x ∈ R+
d . Taking ρ = 1 in (6) and the choice l in the IS transformation (5)
to be slowly varying in u, the second moment M2,u satisﬁes (21) and therefore the family of estimators
{ζ(u) : u > 0} is logarithmically eﬃcient.

EFFICIENCY IN BLACK-BOX IMPORTANCE SAMPLING

23

8. Numerical Experiments

8.1. Illustration with the contextual shortest path problem. Here we employ the IS scheme for
the contextual shortest path problem considered in Elmachtoub & Grigas (2020). The goal is to travel
from the north-west corner to the south-east corner of a 5 × 5 grid. From any given vertex, only edges
which travel south or east are available. Associated with each edge j (enumerated as {1, . . . 40}) is a
traversing cost Cj(S, ε) determined by contextual side information S and an additional random vector
ε. The context S ∈ R5 is taken to have independent Weibull marginals, with Fi(x) = 1 − exp(−x0.5),
for 1 ≤ i ≤ 5. The cost Cj(S, ε) is given by,

C(S, ε) = (cid:2)5−1/2(BS + 3)deg + 1(cid:3)ε,

(32)

where B is a ﬁxed 40 × 5 matrix, deg is a positive constant that allows nonlinear dependence on S,
and the independent noise ε has i.i.d. components uniformly distributed on [1 − a, 1 + a]. The loss
L(C) = L(S, ε) is given as in (4), where Θ is the shortest path polytope on the considered 5 × 5 grid.

Experiment 1: We consider the estimation of pu := P (L(C) ≥ u), for values of u resulting
in pu ∈ [10−3, 10−6]. The probability of large travel cost is estimated from averaging N = 103 i.i.d
samples of the estimator LI(L( ˜C) ≥ u), where the IS cost vector ˜C = C(T (S), ε) and the likelihood
ratio Li is as deﬁned in (8) - (9b). Since S and ε are independent, ˆζN (u) is an unbiased estimator
for pu := P (L(C) ≥ u). Letting Vu denote the resulting estimator variance, we plot log Vu against
log pu in Figure 7(A) for parameter choices a ∈ {0.1, 0.25}, deg = 1.3. The hyperparameter l is selected
via cross-validation (see the explanation below on Figure 7(C)). Note that the plot is approximately a
straight line with a slope of 2, supporting the conclusion Vu = p2+o(1)
from Theorem 5.2. Contrast this
with naive sample averaging, where the variance scales as pu(1 + o(1)).

u

Experiment 2: Here we consider the predictive setting where a routing decision ˆθ( ˆC) =
arg minθ∈Θ θ(cid:124) ˆC is obtained by plugging in the cost ˆC predicted from the realized contextual side in-
formation S. Note that while the realized cost C depends on both S and ε, the realization of ε is not
available at the time of decision-making. Suppose that similar to (Elmachtoub & Grigas 2020, Section
(cid:111)
6.1), the cost is predicted from a linear model ˆC = ˆAS; we take ˆA ∈ arg min
,
estimated from historical data, (S1, C1), . . . , (S1000, C1000). The total cost realized by deploying the de-
cision ˆθ(C) is then given by [ ˆθ( ˆC)](cid:124)C, where the edge-traversal costs C satisfy (32).

i=1 (cid:107)Ci − ASi(cid:107)2

(cid:110)
10−3 (cid:80)103

2

A risk manager is then naturally interested in evaluating tail risk probabilities such as in pu :=
P ([w( ˆC)](cid:124)C > u), that have to be borne from deploying the routing decision ˆθ( ˆC). Drawing N = 2×103
i.i.d. samples S1, . . . , SN for this purpose (drawn independently of those used to estimate ˆA), we ﬁrst
generate the transformed tuple (Zi, ˆDi, ˜Ci)N
i=1; here Zi = T (Si) is the IS vector, ˜Ci = C(Zi, εi) and
ˆDi = ˆAZi serve as the realized and predicted costs, and ˆw( ˆDi) denotes the respective shortest paths,
for i ≤ N . Our IS estimator for the probability pu is then ˆζ1,N (u) = 1
i=1 LiI([ ˆw( ˆDi)](cid:124) ˜Ci) ≥ u).
N
The parameter l = 240 is selected via cross validation. We plot log Vu (denoting the log variance) against
log pu in Figure 7(B) for parameter choices deg ∈ {1.3, 2}, a = 0.25. As before, we observe that the plot
is approximately a straight line with a slope of 2.

(cid:80)N

Figure 7(C) presents additional details on cross-validation by plotting the logarithm of estimator
variance, log Vu, observed (in red) for diﬀerent choices of hyper-parameter l considered. In Figure 7(C),
the level u is such that pu ≈ 10−6. The high degree of variance reduction (exceeding 99.99%) in the
interval l ∈ (200, 450) demonstrates that the estimator variance is not unduly sensitive to the choice of
parameter l. Furthermore, the sample variances estimated from N = 2×103 samples closely approximate
the true variance (green crosses). These observations indicate the limited sample requirement involved
in cross-validation. Please refer E for additional details.
8.2. Illustration with the portfolio credit risk model. For the portfolio credit risk model considered
in Section 6, we take a pool of m = 3000 loans of a single type, each with an exposure ei = 1. As in
Section 8.1, the common factors X ∈ R5 are taken to possess standard Weibull marginal distributions.

24

EFFICIENCY IN BLACK-BOX IMPORTANCE SAMPLING

(a) Estimation of P (L(C) > u)

(b) Estimation of P ([w( ˆC)](cid:124)C > u)

(c) Illustration of cross validation

Figure 7. Variance of the proposed IS estimator, illustrated via a log-log plot, for the
contextual shortest path problem. Lines (solid and dashed) represent a polynomial ﬁt
to the variance values marked via crosses.

The dependence is informed by a Gaussian copula whose non-zero oﬀ-diagonal entries of the correlation
matrix R are taken to be [R]i,j = 0.2, for |j − i| = 1. We consider cases where the conditional default
probabilities are given by (a) the logit model in (23) and (b) the discrete default intensity of the form,
P (Yi = 1 | X) = 1 − exp(−eW (X)−γ). The function W (x) = 1(cid:124)(Ax − b)+ is informed by a ReLU neural
network with 1 hidden layer with weights Ai,j = 1/5 for all (i, j) and b = 0.

Taking the loss threshold q = 0.2 in (22), we aim to estimate the probability of excess default loss
pγ := P (Em); the parameter γ in the conditional default probabilities dictate the rarity of loan defaults
and is varied in the experiments so that the respective pγ varies from 10−1 to 10−9. In Figure 8 below,
we report log Vγ against log pγ, where Vγ is the sample variance of the IS estimator computed using
Algorithm 2. For sub-exponential covariates, cross validation of estimation variance (for both the logit
and default intensity models) in the interval (0.9, 2.4) results in the hyper-parameter choice l = 1.5.
Likewise, cross-validation over the interval (0.8, 1.9) in the case of super-exponential covariates results
in the hyper-parameter choice l = 1.4. As in Section 8.1, the ratio between the logarithm of variance of
IS estimator and that of the naive estimator (without IS) ranges over the interval (1.6, 2), which points
to the IS estimator possessing negligible variance compared to that of sample averaging without IS.

Acknowledgements. Support from Singapore Ministry of Education’s AcRF Tier 2 grant MOE2019-
T2-2-163 is gratefully acknowledged.

References

Ahamed, T. P. I., Borkar, V. S., & Juneja, S. 2006. Adaptive Importance Sampling Technique for Markov Chains

Using Stochastic Approximation. Operations Research, 54(3), 489–504.

Ahn, Dohyun, & Kim, Kyoung-Kuk. 2018. Eﬃcient Simulation for Expectations over the Union of Half-Spaces.

ACM Trans. Model. Comput. Simul., 28(3).

3.03.54.04.55.0Log excess probability56789Log variancea=0.25a=0.1----------2.53.03.54.04.55.05.56.0 Log excess probability45678910 Log variancedeg=2deg=1.3---------------100200300400500600Lower level l10.09.59.08.5Log variance Estimated varianceTrue VarianceEFFICIENCY IN BLACK-BOX IMPORTANCE SAMPLING

25

(a) Logit loan default probabilities

(b) Intensity based default probabilities

Figure 8. Plots displaying the logarithm of sample variance, log Vγ, of the IS estima-
tor versus log pγ, where pγ is excess default loss probability estimated. For the sub-
exponential case, the tail parameter of the Weibull distribution is taken to equal 0.8,
while for the super-exponential case, this equals 1.2

Arief, M, Zhiyuan, H, Kumar, GKS, Bai, Y, He, S, Ding, W, Lam, H, & Zhao, D. 2020. Deep Probabilistic
Accelerated Evaluation: A Certiﬁable Rare-Event Simulation Methodology for Black-Box Autonomy.

Asmussen, S., & Glynn, P.W. 2007. Stochastic Simulation: Algorithms and Analysis. Springer New York.

Asmussen, Søren, Blanchet, Jos´e, Juneja, Sandeep, & Rojas-Nandayapa, Leonardo. 2011. Eﬃcient simulation of

tail probabilities of sums of correlated lognormals. Annals of Operations Research, 189(1), 5–23.

Asmussen, Søren, & Kroese, Dirk P. 2006.

Improved algorithms for rare event simulation with heavy tails.

Advances in Applied Probability, 38(2), 545–558.

Asmussen, Søren, Binswanger, Klemens, & Højgaard, Bjarne. 2000. Rare events simulation for heavy-tailed

distributions. Bernoulli, 6(2), 303–322.

Bai, Yuanlu, Huang, Zhiyuan, Lam, Henry, & Zhao, Ding. 2020. Rare-Event Simulation for Neural Network and

Random Forest Predictors.

Ban, Gah-Yi, & Rudin, Cynthia. 2019. The Big Data Newsvendor: Practical Insights from Machine Learning.

Operations Research, 67(1), 90–108.

Bassamboo, Achal, Juneja, Sandeep, & Zeevi, Assaf. 2008. Portfolio Credit Risk with Extremal Dependence:

Asymptotic Analysis and Eﬃcient Simulation. Operations Research, 56(3), 593–606.

Bazier-Matte, Thierry, & Delage, Erick. 2020. Generalization bounds for regularized portfolio selection with
market side information. INFOR: Information Systems and Operational Research, 58(2), 374–401.

Beer, Gerald, Rockafellar, RT, & Wets, Roger J-B. 1992. A characterization of epi-convergence in terms of

convergence of level sets. Proceedings of the American Mathematical Society, 11(3), 753–761.

Bertsimas, Dimitris, & Kallus, Nathan. 2020. From Predictive to Prescriptive Analytics. Management Science,

66(3), 1025–1044.

Blanchet, J, & Lam, H. 2012. State-dependent importance sampling for rare-event simulation: An overview and

recent advances. Surveys in Operations Research and Management Science, 17(1), 38 – 59.

Blanchet, J, & Mandjes, M. 2009. Chapter 5. Rare Event Simulation for Queues. In: Rubino, Gerardo, & Tuﬃn,

Bruno (eds), Rare event simulation using Monte Carlo methods. John Wiley & Sons.

Blanchet, Jose, Li, Juan, & Nakayama, Marvin K. 2019. Rare-Event Simulation for Distribution Networks.

Operations Research, 67(5), 1383–1396.

Blanchet, Jose, Zhang, Fan, & Zwart, Bert. 2020. Optimal Scenario Generation for Heavy-tailed Chance Con-

strained Optimization.

Blanchet, Jose H., & Rojas-Nandayapa, Leonardo. 2011. Eﬃcient simulation of tail probabilities of sums of

dependent random variables. Journal of Applied Probability, 48(A), 147–164.

Borovkov, Aleksandr Alekseevich. 2008. Asymptotic analysis of random walks. Vol. 118. Cambridge University

Press.

-2-4-8-10-6Log default probability-2-4-6-8-10-12-14-16-18Log varianceLogit default probabiltiesSub-exponential CovariatesSuper-exponential Covariates-2-34567-8-9-10Log default probability-2-4-6-8-10-12-14-16Log varianceDefault intensity probabiltiesSub-exponential CovariatesSuper-exponential Covariates----26

EFFICIENCY IN BLACK-BOX IMPORTANCE SAMPLING

Cao, Ying, & Shen, Zuo-Jun Max. 2019. Quantile forecasting and data-driven inventory management under

nonstationary demand. Operations Research Letters, 47(6), 465 – 472.

de Haan, Laurens, & Ferreira, Ana. 2010. Extreme Value Theory: An Introduction (Springer Series in Operations

Research and Financial Engineering). 1st edition. edn. Springer.

de Valk, Cees. 2016. Approximation and estimation of very small probabilities of multivariate extreme events.

Extremes, 19(4), 687–717.

Dembo, Amir, & Zeitouni, Ofer. 1998. Large Deviations Techniques and Applications. Springer.

Deo, A., & Murthy, K. 2020. Optimizing tail risks using an importance sampling based extrapolation for heavy-

tailed objectives. In: 2020 IEEE 59th Conference on Decision and Control (CDC).

Deo, Anand, & Juneja, Sandeep. 2020. Credit Risk: Simple Closed-Form Approximate Maximum Likelihood

Estimator. Operations Research.

Duchi, John, & Namkoong, Hongseok. 2020. Learning Models with Uniform Performance via Distributionally

Robust Optimization.

Dupuis, Paul, Leder, Kevin, & Wang, Hui. 2007.

Importance Sampling for Sums of Random Variables with

Regularly Varying Tails. ACM Trans. Model. Comput. Simul., 17(3).

Elmachtoub, Adam N., & Grigas, Paul. 2020. Smart “Predict, then Optimize”.

Embrechts, Paul, Kl¨uppelberg, Claudia, & Mikosch, Thomas. 1997. Modelling Extremal Events. Springer Berlin

Heidelberg.

Embrechts, Paul, Lindskog, Filip, & McNeil, Alexander. 2001. Modelling dependence with copulas. Rapport

technique.

Feller, William. 1971. An introduction to probability theory and its applications. Vol. II. Second edition. New

York: John Wiley & Sons Inc.

Ferreira, K J, Lee, B, & Simchi-Levi, D. 2016. Analytics for an Online Retailer: Demand Forecasting and Price

Optimization. Manufacturing & Service Operations Management, 18(1), 69–88.

Glasserman, Paul. 2004. Monte Carlo methods in ﬁnancial engineering. New York: Springer.

Glasserman, Paul, & Li, Jingyi. 2005. Importance Sampling for Portfolio Credit Risk. Management Science,

51(11), 1643–1656.

Glasserman, Paul, Heidelberger, Philip, & Shahabuddin, Perwez. 2000. Variance Reduction Techniques for

Estimating Value-at-Risk. Management Science, 46(10), 1349–1364.

Glasserman, Paul, Heidelberger, Philip, & Shahabuddin, Perwez. 2002. Portfolio Value-at-Risk with Heavy-Tailed

Risk Factors. Mathematical Finance, 12(3), 239–269.

Glasserman, Paul, Kang, Wanmo, & Shahabuddin, Perwez. 2007. Large Deviations in Multifactor Portfolio

Credit Risk. Mathematical Finance, 17(3), 345–379.

Glasserman, Paul, Kang, Wanmo, & Shahabuddin, Perwez. 2008. Fast Simulation of Multifactor Portfolio Credit

Risk. Operations Research, 56(5), 1200–1217.

Glynn, Peter W. 1996. Importance sampling for Monte Carlo estimation of quantiles. Pages 180–185 of: Mathe-
matical Methods in Stochastic Simulation and Experimental Design: Proceedings of the 2nd St. Petersburg
Workshop on Simulation. Publishing House of St. Petersburg University.

Harsha, Pavithra, Natarajan, Ramesh, & Subramanian, Dharmashankar. 2019. A prescriptive machine learning

framework to the price-setting newsvendor problem.

Hashorva, Enkelejd, & H¨usler, J¨urg. 2003. On multivariate Gaussian tails. Annals of the Institute of Statistical

Mathematics, 55(3), 507–522.

He, Shengyi, Jiang, Guangxin, Lam, Henry, & Fu, Michael C. 2021. Adaptive Importance Sampling for Eﬃcient

Stochastic Root Finding and Quantile Estimation. arXiv preprint arXiv:2102.10631.

Heidelberger, Philip. 1995. Fast Simulation of Rare Events in Queueing and Reliability Models. ACM Trans.

Model. Comput. Simul., 5(1), 43–85.

Hong, L. Jeﬀ, Hu, Zhaolin, & Liu, Guangwu. 2014. Monte Carlo Methods for Value-at-Risk and Conditional

Value-at-Risk: A Review. ACM Trans. Model. Comput. Simul., 24(4).

Honnappa, Harsha, Pasupathy, Raghu, & Jaiswal, Prateek. 2018. Dominating Points of Gaussian Extremes.

EFFICIENCY IN BLACK-BOX IMPORTANCE SAMPLING

27

Huang, Z., & Shahabuddin, P. 2004. A uniﬁed approach for ﬁnite-dimensional, rare-event Monte Carlo simulation.

Pages 1616–1624 of: Proceedings of the 2004 Winter Simulation Conference, 2004., vol. 2.

Huang, Z., Lam, H., LeBlanc, D. J., & Zhao, D. 2018. Accelerated Evaluation of Automated Vehicles Using

Piecewise Mixture Models. IEEE Transactions on Intelligent Transportation Systems, 19(9), 2845–2855.

Hult, Henrik, Lindskog, Filip, Hammarlid, Ola, & Rehn, Carl Johan. 2012. Risk and Portfolio Analysis. Springer

New York.

Jeong, Sookyo, & Namkoong, Hongseok. 2020. Robust causal inference under covariate shift via worst-case
subpopulation treatment eﬀects. Pages 2079–2084 of: Proceedings of Thirty Third Conference on Learning
Theory. Proceedings of Machine Learning Research, vol. 125. PMLR.

Juneja, S., & Shahabuddin, P. 2006. Chapter 11 Rare-Event Simulation Techniques: An Introduction and Recent
Advances. Pages 291 – 350 of: Henderson, Shane G., & Nelson, Barry L. (eds), Simulation. Handbooks in
Operations Research and Management Science, vol. 13. Elsevier.

Juneja, S., Karandikar, R. L., & Shahabuddin, P. 2007. Asymptotics and Fast Simulation for Tail Probabilities

of Maximum of Sums of Few Random Variables. ACM Trans. Model. Comput. Simul., 17(2).

Juneja, Sandeep, & Shahabuddin, Perwez. 2002. Simulating Heavy Tailed Processes Using Delayed Hazard Rate

Twisting. ACM Trans. Model. Comput. Simul., 12(2), 94–118.

Kley, Oliver, & Kl¨uppelberg, Claudia. 2016. Bounds for randomly shared risk of heavy-tailed loss factors.

Extremes, 19(4), 719–733.

Kley, Oliver, Kl¨uppelberg, Claudia, & Reinert, Gesine. 2016. Risk in a Large Claims Insurance Market with

Bipartite Graph Structure. Operations Research, 64(5), 1159–1176.

Langen, Hans-Joachim. 1981. Convergence of Dynamic Programming Models. Mathematics of Operations Re-

search, 6(4), 493–512.

LeCun, Yann, Bengio, Yoshua, & Hinton, Geoﬀrey. 2015. Deep learning. Nature, 521, 436 – 444.

Lemaire, Vincent, & Pag`es, Gilles. 2010. Unconstrained recursive importance sampling. Ann. Appl. Probab.,

20(3), 1029–1067.

Lin, Shaochong, Chen, Frank, Li, Yanzhi, & Shen, Zuo-Jun Max. 2020. Procurement of New Products: Data-

Driven Newsvendor with Proﬁt Risk. Available at SSRN 3540448.

Liu, Guangwu. 2015. Simulating Risk Contributions of Credit Portfolios. Operations Research, 63(1), 104–121.

Mainik, Georg, & R¨uschendorf, Ludger. 2010. On optimal portfolio diversiﬁcation with respect to extreme risks.

Finance and Stochastics, 14(4), 593–623.

Nelsen, Roger B. 2010. An Introduction to Copulas. Springer Publishing Company, Incorporated.

O’ Kelly, M, Sinha, A, Namkoong, H, Tedrake, R, & Duchi, J. 2018. Scalable End-to-End Autonomous Vehi-
cle Testing via Rare-event Simulation. Pages 9827–9838 of: Advances in Neural Information Processing
Systems, vol. 31.

Oroojlooyjadid, Afshin, Snyder, Lawrence V., & Tak´aˇc, Martin. 2020. Applying deep learning to the newsvendor

problem. IISE Transactions, 52(4), 444–463.

Resnick, Sidney. 2007. Heavy-Tail Phenomena: Probabilistic and statistical modeling. Springer, New York.

Resnick, Sidney I. 1987. Extreme Values, Regular Variation and Point Processes. Springer New York.

Rockafellar, R. Tyrrell, & Wets, Roger J. B. 1998. Variational Analysis. Springer, Berlin, Heidelberg.

Rubinstein, Reuven Y, & Kroese, Dirk P. 2013. The cross-entropy method: a uniﬁed approach to combinatorial

optimization, Monte-Carlo simulation and machine learning. Springer Science & Business Media.

Sadowsky, J. S., & Bucklew, J. A. 1990. On large deviations theory and asymptotically eﬃcient Monte Carlo

estimation. IEEE Transactions on Information Theory, 36(3), 579–588.

Salinetti, Gabriella, & Wets, Roger J-B. 1981. On the convergence of closed-valued measurable multifunctions.

Transactions of the American Mathematical Society, 266(1), 275–289.

Siegmund, D. 1976. Importance Sampling in the Monte Carlo Study of Sequential Tests. Ann. Statist., 4(4),

673–684.

Sirignano, Justin, & Giesecke, Kay. 2019. Risk Analysis for Large Pools of Loans. Management Science, 65(1),

107–121.

28

EFFICIENCY IN BLACK-BOX IMPORTANCE SAMPLING

Sirignano, Justin, Sadhwani, Apaar, & Giesecke, Kay. 2018. Deep Learning for Mortgage Risk.

Sun, Lihua, & Hong, L. Jeﬀ. 2010. Asymptotic representations for importance-sampling estimators of value-at-risk

and conditional value-at-risk. Operations Research Letters, 38(4), 246 – 251.

Uesato, J, Kumar, A, Szepesvari, C, Erez, T, Ruderman, A, Anderson, K, Dvijotham, K, Heess, N, & Kohli,
P. 2019. Rigorous Agent Evaluation: An Adversarial Approach to Uncover Catastrophic Failures.
In:
International Conference on Learning Representations.

Varadhan, S. R. S. 1988. Large deviations and applications. Pages 1–49 of: Hennequin, Paul-Louis (ed), ´Ecole
d’ ´Et´e de Probabilit´es de Saint-Flour XV–XVII, 1985–87. Berlin, Heidelberg: Springer Berlin Heidelberg.
Williamson, Robert, & Menon, Aditya. 2019. Fairness risk measures. Pages 6786–6797 of: Proceedings of the

36th International Conference on Machine Learning, vol. 97. PMLR.

Zhao, D., Lam, H., Peng, H., Bao, S., LeBlanc, D. J., Nobukawa, K., & Pan, C. S. 2017. Accelerated Evaluation
of Automated Vehicles Safety in Lane-Change Scenarios Based on Importance Sampling Techniques. IEEE
Transactions on Intelligent Transportation Systems, 18(3), 595–607.

EFFICIENCY IN BLACK-BOX IMPORTANCE SAMPLING

1

Supplementary material for the paper “Achieving Eﬃciency in Black-box Simulation of
Distribution Tails with Self-Structuring Importance Samplers”

The appendix is organised as follows: In Appendix A, we present the proofs of the main results The-
orems 3.3 - 5.2. Appendix B serves to illustrate the wide applicability of the tail modeling framework
with examples and suﬃcient conditions for Assumptions 2 - 3. Proofs of the results relating to distribu-
tion network and portfolio credit risk applications, namely Propositions 4.3, 6.3 and Theorem 6.2, are
presented in Appendix C. Proofs of technical results (Lemma 3.1 - 3.4, Proposition 5.1 and Theorems
7.1 - 7.2) are given in Appendix D. Appendix E concludes with an additional numerical experiment on
distribution networks.

Appendix A. Proofs of main results

The following deﬁnitions and notational convention are used in the proofs: For r > 0 and x ∈ Rd,
let Br(x) = {y ∈ Rd : (cid:107)y − x(cid:107) ≤ r} denote the metric ball of radius r around x. Unless speciﬁed
explicitly, (cid:107)x(cid:107) = maxi=1,...,d |xi| denotes the (cid:96)∞-norm. Let S d−1 = {x ∈ Rd : (cid:107)x(cid:107) = 1} denote the
unit sphere and Rd
++ = {x ∈ Rd : x > 0} denote the interior of the positive orthant. For M > 0, let
BM := {x ∈ Rd

+ : (cid:107)x(cid:107) ≤ M }. For A ⊆ Rd, let cl(A) denote its closure, int(A) denote its interior,

χA(x) :=

(cid:40)

0,

if x ∈ A

+∞,

otherwise,

and

[A]1+r := {x + y ∈ Rd

+ : x ∈ A, (cid:107)y(cid:107) ≤ r},

respectively denote the characteristic function and the set of points in Rd
(0, ∞) from A. For f : Rd → R, α ∈ R, M > 0, let
α (f ) = {x ∈ Rd

and Ξα,M (f ) = lev+

+ : f (x) ≥ α}

lev+

α (f ) ∩ BM ∩ Rd

++

+ lying within a distance r ∈

denote the super-level sets of f restricted, respectively, to the positive orthant and to the bounded
subset BM ∩ Rd
d (t)), and
q∗ = limt→∞[q(t)/q∞(t)], if the limit exists. Throughout the proofs, we suppose u > x0 as speciﬁed in
Assumption 1(a). Deﬁne Lu : Rd

++. Recall that Y = Λ(X), the component-wise inverse q(t) = (Λ←

++ → R and fLD : Rd

1 (t), . . . , Λ←

Lu(x) := u−1L(q(t(u)x)),
(cid:0)u1/ρ(cid:1)

t(u) := Λmin

+ → R as,
fLD(y) := L∗(q∗y1/α), where

and

q∞(t) := (cid:107)q(t)(cid:107)∞,

(A.1a)

(A.1b)

Write Diag(a) for a diagonal matrix with diagonal entries a1, . . . , ad and sgn(x) for the vector of signs
of x. To avoid clutter in the expressions, deﬁne

Yu := t(u)−1Y , ψu := Λ ◦ T −1 ◦ q,

and

cρ(u) := (l/u)1/ρ.

where the parameter u in the symbol ψu is explicitly indicated to remind the role of u in T −1.
Proof of Theorem 3.3. A suﬃcient condition (see (Dembo & Zeitouni 1998, Theorem 4.1.11)) to verify
the existence of LDP is to show that for all x ∈ Rd
++,

− I(x) = inf
δ>0

lim sup
t→∞

1
t

log P

(cid:18) Y
t

(cid:19)

∈ Bδ(x)

= inf
δ>0

lim inf
t→∞

1
t

log P

(cid:18) Y
t

(cid:19)

∈ Bδ(x)

.

(A.2)

Fix any ε, M ∈ (0, ∞) and x ∈ (0, M )d. Since fY (y) = p(y) exp(−ϕ(y)),

P (Y /t ∈ Bδ(x)) =

(cid:90)

y/t∈Bδ(x)

p(y) exp(−ϕ(y))dy = td

(cid:90)

z∈Bδ(x)

p(tz)e−ϕ(tz)dz.

Recall that the continuous convergences in Assumption 3 imply the following uniform convergences over
compact sets not containing the origin (see (Rockafellar & Wets 1998, Theorem 7.14)):

n−1ϕ(nx) n→∞−−−−→ I(x) and n−1 log p(nx) n→∞−−−−→ 0.

(A.3)

2

EFFICIENCY IN BLACK-BOX IMPORTANCE SAMPLING

Due to this local uniform convergence and the continuity of I, there exist δ0, t0 ∈ (0, ∞) such that,

(cid:12)
(cid:12)
(cid:12)
(cid:12)

ϕ(tz)
t

− I(x)

(cid:12)
(cid:12)
(cid:12)
(cid:12)

≤

(cid:12)
(cid:12)
(cid:12)
(cid:12)

ϕ(tz)
t

− I(z)

(cid:12)
(cid:12)
(cid:12)
(cid:12)

+ |I(z) − I(x)| ≤ ε/2, for all z ∈ Bδ(x)

and exp(−εt/2) ≤ p(tz) ≤ exp(εt/2), whenever t > t0, δ < δ0 and Bδ0 (x) does not contain the origin.
Thus, given ε, M and x ∈ (0, M )d, there exist δ0, t0 ∈ (0, ∞) such that for all t > t0 and δ ∈ (0, δ0),

exp (−t(I(x) + ε)) ≤ fY (tz) ≤ exp (−t(I(x) − ε)) , uniformly over z ∈ Bδ(x);

(A.4)

Then tdVol(Bδ(x)) exp(−t(I(x) + (cid:15))) ≤ P (t−1Y ∈ Bδ(x)) ≤ tdVol(Bδ(x)) exp(−t(I(x) − (cid:15))). Since
P (Y /t ∈ Bδ(x)) is increasing in δ and these bounds hold for any δ < δ0,

−I(x) − (cid:15) ≤ inf
δ>0

lim inf
t→∞

1
t

log P (t−1Y ∈ Bδ(x)) ≤ inf
δ>0

lim sup
t→∞

1
t

log P (Y /t ∈ Bδ(x)) ≤ −I(x) + (cid:15).

Since the choices ε, M ∈ (0, ∞) are arbitrary, (A.2) holds.

(cid:3)

A.1. Proof of Theorem 4.1. For functions f and g, let f ∧ g (resp. f ∨ g) denote their point-wise
minimum (resp. maximum).

Lemma A.1. Under Assumption 2, we have q∞(t(u)) = u1/ρ. Therefore when Assumption 1 additionally
holds, the events {L(X) ≥ u} and {Yu ∈ lev+
1 (Lu)} coincide.

Proof. Consider increasing real valued functions f1, f2. By the deﬁnition of left-continuous inverses,
i=1fi)← =
i = Λi (see

2 (y). From induction, (∨d
, given increasing functions f1, . . . , fd. Since {Λi : i = 1, . . . , d} are continuous, q←

(f1 ∨ f2)←(y) = inf{u : f1(u) ≥ y} ∧ inf{u : f2(u) ≥ y} = f ←
∧d
(de Haan & Ferreira 2010, Excercise 1.1 (a))). Consequently,

1 (y) ∧ f ←

i=1f ←
i

q←
∞(t) = min

i=1,. . . d

q←
i (t) = Λmin(t).

(A.5)

Then q∞(Λmin(x)) = x for all x ∈ (x0, ∞) due to the strict monotonicity in Assumption 2. Since q = Λ←
is injective, {Yu ∈ A} = {X ∈ q(t(u)A)}, for any measurable A. With Lu(x) := u−1L(q(t(u)x)),

{q(t(u)y) : y ∈ lev+

1 (Lu)} = {q(t(u)y) : L(q(t(u)y)) ≥ u} = {x : L(x) > u} ∩ supp(X).

(cid:3)

Lemma A.2. If Assumptions 1 and 2 hold and the limit q∗ exists, the sequence of functions {Lu : u > 0}
converge continuously to fLD on Rd
++. Consequently, for any α, ε, M, K > 0, there exists u0 large enough
such that for all u > u0,

Ξα,M (Lu) ⊆ (cid:2)Ξα,M (fLD)(cid:3)1+ε/2

and

Ξα+ε,M (fLD) ⊆ [Ξα+ε/2,M (Lu)]1+ε/K

Proof. 1) Consider any sequences {un} ⊂ R+, {xn} ⊂ Rd

+ satisfying un ↑ ∞, xn → x > 0.We rewrite,

Lun(xn) = u−1

n L

(cid:16)

q(t(un)xn)/q(t(un)1) · ˆq(t(un))u1/ρ

n

(cid:17)

.

Consider any δ > 0 such that Bδ(x) does not include 0. As Λi ∈ RV(αi), we have qi ∈ RV(1/αi)
(see Part 9 of (de Haan & Ferreira 2010, Proposition B.1.9)). Due to the uniform convergence
limt→∞ q(ty)/q(t1) = y1/α over y ∈ Bδ(x) (see (de Haan & Ferreira 2010, Proposition B.1.4)),

q(t(un)xn)/q(t(un)1) → x1/α, and ˆq(t(un)) → q∗,

Applying triangle inequality, pn = q(t(u)xu)/q(t(u)1)ˆq(t(u)) → x1/αq∗. Continuous convergence

Lun (xn) := u−1

n L(u1/ρn

n

pn) → L∗(q∗x1/α) =: fLD(x)

follows from Assumption 1(b).

2) We next prove the claims on the set inclusions using the notions of lim sup, lim inf of a sequence of
sets deﬁned in the Kuratowski sense (see (Rockafellar & Wets 1998, Chapter 1)). Taking the ambient

EFFICIENCY IN BLACK-BOX IMPORTANCE SAMPLING

3

space X = Rd

++ in (Beer et al. 1992, Theorem 3.1), we obtain

lim sup
u

Ξβ(Lu) ⊆ Ξβ(fLD),

lim inf
u

Ξβu (Lu) ⊇ Ξβ(fLD) for some βu (cid:37) β,

(A.6)

as a consequence of above veriﬁed Lun(xn) → fLD(x). Refer the construction in the proof of (Beer et al.
1992, Theorem 3.1) for using ﬁxed level β in the ﬁrst set inclusion and considering increasing sequence
βu in the second set inclusion in (A.6). Now, setting β = α in (A.6), using the equivalence between (v)b
and (vii)b in (Salinetti & Wets 1981, Theorem 2.2),
Ξα,M (Lu) ⊆ (cid:2)Ξα,M (fLD)(cid:3)1+ε/2

++ ⊆ (cid:2)Ξα,M (fLD)(cid:3)1+ε/2

∩ Rd

.

Set β = α + ε, and let βu (cid:37) α + ε be selected as in (A.6). Therefore, for all large enough u, Ξβu,M (Lu) ⊆
Ξα+ε/2,M (Lu). From (A.6), Ξα+ε,M (fLD) ⊆ lim inf u Ξβu (Lu) ⊆ lim inf u Ξα+ε/2,M (Lu). Further, from
the equivalence between (vii)a and (v)a in (Salinetti & Wets 1981, Theorem 2.2),
++ ⊆ [Ξα+ε/2,M (Lu)]1+ε/K.

Ξα+ε,M (fLD) ⊆ [Ξα+ε/2,M (Lu)]1+ε/K ∩ Rd

(cid:3)

Corollary A.3. Suppose that Assumptions 1 and 2 hold and the limit q∗ exists. Then for any α, ε, M >
0, there exists u0 large enough such that for all u > u0,

lev+

α (Lu) ∩ BM ⊆ (cid:2)Ξα,M (fLD)(cid:3)1+ε

and

Ξα+ε,M (fLD) ⊆ Ξα,M (Lu).

(A.7)

Consequently, lim inf n→∞ χlev+

α (Lun )(xn) ≥ χlev+

α (fLD)(x), for any xn → x and un → ∞.

Proof. Notice that for ε > 0, lev+

α (Lu) ∩ BM ⊆ [Ξα,M (Lu)]1+ε/2, as a consequence of the deﬁnitions
at the beginning of Section A. The ﬁrst set inclusion now follows from the deﬁnition of the [A](1+r) for
a set A ⊂ Rd

+. For the second set inclusion, observe that for K > 0 (to be chosen imminently),

for all large enough u. Further, for any x, y,

Ξα+ε,M (fLD) ⊆ [Ξα+ε/2,M (Lu)]1+ε/K ∩ Rd

++

|Lu(x) − Lu(y)| ≤ |Lu(y) − fLD(y)| + |fLD(y) − fLD(x)| + |fLD(x) − Lu(x)|.

Recall that Lu → fLD uniformly on Rd
++ ∩ B2M (see for example, (Rockafellar & Wets 1998, Theorem
7.14)). Hence the ﬁrst and third terms above may be made less than ε/6 for a large enough u for any
choices of x, y ∈ [Ξα+ε/2,M (Lu)]1+ε/K ∩ Rd
++. Next, fLD is uniformly continuous over B2M . Therefore,
there exists a κ > 0, such that for x, y ∈ B2M , whenever (cid:107)x − y(cid:107) ≤ κ, |fLD(x) − fLD(y)| ≤ ε/6.
Consider any K ≥ ε/κ. For any y ∈ [Ξα+ε/2,M (Lu)]1+ε/K ∩ Rd
++, there exists x ∈ Ξα+ε/2,M (Lu) such
that (cid:107)x − y(cid:107) ≤ κ, and consequently,

|Lu(x) − Lu(y)| ≤ ε/6 + ε/6 + ε/6 ≤ ε/2.

Therefore, whenever y ∈ [Ξα+ε/2,M (Lu)]1+ε/K ∩ Rd
To verify the conclusion on characteristic functions, we proceed as follows: The bound in the statement
is immediate if x ∈ lev+
α (fLD)(x) = 0 in that case. Consider the case where xn → x /∈
lev+
α (fLD)]1+δ = ∅, because of the continuity of
fLD and lev+

α (fLD), as χlev+
α (fLD). Then for a suitably small δ > 0, Bδ(x) ∩ [lev+

α (fLD) being a closed set. Fix M > (cid:107)x(cid:107) + δ. With un → ∞, we have

++, Lu(y) ≥ α. Hence Ξα+ε,M (fLD) ⊆ Ξα,M (Lu).

lev+

α (Lun ) ∩ BM ⊆ (cid:2)Ξα,M (fLD)(cid:3)1+δ

⊆ (cid:2)lev+

α (fLD) ∩ BM

(cid:3)1+δ

, n > n0

for suﬃciently large n0, due to the ﬁrst inclusion in (A.7). For n1 chosen large enough to ensure
{xn}n>n1 ⊆ Bδ(x), we have {xn : n > n1}∩[lev+
α (Lun )∩BM , for
all n > n2 := max{n0, n1}. Since M > (cid:107)x(cid:107)+δ and (cid:107)xn(cid:107) ≤ (cid:107)x(cid:107)+δ for n > n2, xn /∈ lev+
α (Lun )∩(Rd\BM )
either. Therefore xn /∈ lev+
(cid:3)

α (fLD)]1+δ = ∅. Consequently, xn /∈ lev+

α (Lun )(xn) = ∞ for n > n2.

α (Lun ) and χlev+

4

EFFICIENCY IN BLACK-BOX IMPORTANCE SAMPLING

Lemma A.4. Suppose that f, g : Rd

+ → R are continuous. For any α, δ, M positive,

inf
x∈[Ξα,M (f )]1+ε

g(x) ≥

inf
x∈int(Ξα+ε,M (f ))

g(x) ≤

for all ε suitably small.

inf
α (f )∩BM
inf
α (f )∩BM

x∈lev+

x∈lev+

g(x) − δ

and

g(x) + δ,

Proof. Observe that the sequence of sets Xn := {[Ξα,M (f )]1+1/n}n≥1 are uniformly bounded in n and
∪i≥1Xi is relatively compact. Further, Xn (cid:38) lev+
α (f ) ∩ BM in the Kuratowski sense. Therefore, from
(Langen 1981, Theorem 2.2 (iii)), for all small enough (cid:15),

inf
x∈[Ξα,M (f )]1+(cid:15)

g(x) ≥

inf
x∈lev+
α ∩BM

g(x) − δ.

In a similar spirit, due to the continuity of f (·), one has Ξα+1/n,M (f ) (cid:37) Ξα,M (f ). Then, upon an
application of (Langen 1981, Theorem 2.2 (iii)),

inf
x∈Ξα+ε,M (f )

g(x) ≤

inf
x∈Ξα,M (f )

g(x) + δ.

The statement then follows from the continuity of g(·).
Proof of Theorem 4.1. Fix any δ, M > 0. Observe that I(·) and L∗(·) are continuous. As a consequence
of Lemma A.4, there exists ε > 0 suitably small such that

(cid:3)

inf
p∈[Ξ1,M (fLD)]1+(cid:15)

I(p) ≥

inf
p∈Ξ1,M (fLD)

I(p) − δ, and

inf
p∈int(Ξ1+ε,M (fLD))

I(p) ≤

inf

I(p) + δ.

p∈lev+

1 (fLD)∩BM

(A.8a)

(A.8b)

Large deviations upper bound. Due to Lemma A.1,

From Corollary A.3, P (cid:0)Yu ∈ lev+
Since the expansion set [A]1+r is closed for any A ⊆ Rd

1 (Lu) ∩ BM

P (cid:0)L(X) ≥ u(cid:1) ≤ P (cid:0)Yu ∈ lev+

1 (Lu) ∩ BM

(A.9)
(cid:1) ≤ P (cid:0)Yu ∈ (cid:2)Ξ1,M (fLD)(cid:3)1+ε(cid:1), for all u suﬃciently large.

M

(cid:1) + P (cid:0)Yu ∈ Bc

(cid:1).

+, the set [Ξα,M (fLD)]1+ε is closed. Therefore,

lim sup
u→∞

1
t(u)

log P (cid:0)Yu ∈ lev+

1 (Lu) ∩ BM

(cid:1) ≤ lim sup

log P (cid:0)t(u)−1Y ∈ (cid:2)Ξ1,M (fLD)(cid:3)1+ε(cid:1)

u→∞

1
t(u)
inf
p∈[Ξ1,M (fLD)]1+ε
inf

p∈lev+

1 (fLD)∩BM

≤ −

≤ −

I(p)

I(p) + δ,

where the second inequality follows from the Tail LDP in Theorem 3.3 and the third inequality is a
consequence of the choice of ε satisfying (A.8a). Since δ > 0 is arbitrary,

lim sup
u→∞
where I ∗ := inf p∈lev+

1
t(u)

log P (cid:0)Yu ∈ lev+

1 (Lu) ∩ BM

(cid:1) ≤ −

inf

I(p) ≤ −I ∗,

p∈lev+

1 (fLD)∩BM

1 (fLD) I(p). A similar application of Theorem 3.3 results in

lim sup
u→∞

1
t(u)

log P (Yu ∈ Bc

M ) ≤ − inf
p∈cl(Bc

M )

I(p) = −M,

(A.10)

where the latter inequality follows from Lemma 3.4d and the continuity of I(·). Combining these conclu-
sions with that in (A.9) and (Dembo & Zeitouni 1998, Lemma 1.2.15),

lim sup
u→∞

1
t(u)

log P (L(X) ≥ u) ≤ − min {M, I ∗} .

Since M can be made arbitrarily large, we have lim supu→∞ t(u)−1 log P (L(X) ≥ u) ≤ −I ∗.
Large deviations lower bound. We have from Lemma A.1 and the deﬁnition of Ξα,M that
P (L(X) ≥ u) ≥ P (Yu ∈ Ξ1,M (Lu)). For ε satisfying (A.8b), it follows from the second set inclusion

EFFICIENCY IN BLACK-BOX IMPORTANCE SAMPLING

5

in Corollary A.3 that P (L(X) ≥ u) ≥ P (Yu ∈ int (Ξ1+ε,M (fLD))), for all suﬃciently large enough u.
Then as an application of the tail LDP in Theorem 3.3,

lim inf
u→∞

1
t(u)

log P (L(X) ≥ u) ≥ −

inf
p∈int(Ξ1+ε,M (fLD))

I(p) ≥ −

inf

I(p) − δ,

p∈lev+

1 (fLD)∩BM

where the latter inequality is a consequence of ε satisfying (A.8b). Since M, δ are arbitrary,

lim inf
u→∞

1
t(u)

log P (L(X) ≥ u) ≥ −

inf
p∈lev+
1 (fLD)

I(p) = −I ∗.

(cid:3)

A.2. Proof of Proposition 2.4 and useful bounds on the inverse of T (·).. Proof of Proposi-
tion 2.4: Let kρ(u) := log(u/l). It is suﬃcient to show that the determinant of the Jacobian of the map
x (cid:55)→ T (x) equals J(x). In that case, the density of Z, denoted by fZ(·), is fZ(T (x)) = fX (x)/J(x);
consequently, the likelihood ratio between the distributions of Z and that of X (or the Radon-Nikodym
derivative evaluated at the samples Z1 . . . Zn) is given as in (9a). So the rest of the veriﬁcation is
devoted to checking that J(·) indeed equals the determinant of the Jacobian JacT (·) = ∂T /∂x. To this
end, deﬁne ψ(x) = log |x| + kρ(u)κ(x) and observe that T (x) = Diag(sgn(x))eψ(x). Now, following the
chain rule for Jacobians,

Jac(x) = Diag(sgn(x))Diag(eψ(x))Jacψ(x), for almost every x, and

Jacψ(x) = Diag

(cid:19)

(cid:18) sgn(x)
|x|

+ ρ−1kρ(u)

(cid:20)

(cid:18)

Diag

(cid:18)

= Diag

sgn(x)

(cid:20) 1
|x|

+

ρ−1kρ(u)
(1 + |x|)(cid:107) log(1 + |x|)(cid:107)∞

sgn(x)
(1 + |x|)(cid:107) log(1 + |x|)(cid:107)∞
(cid:21)(cid:19)

(cid:19)

(cid:20)

− ρ−1kρ(u)

−

log(1 + |x|)
(cid:107) log(1 + |x|)(cid:107)2
∞
log(1 + |x|)
(cid:107) log(1 + |x|)(cid:107)2
∞

(e∗/(1 + |x|))(cid:124)

(cid:21)

(e∗/(1 + |x|))(cid:124)

(cid:21)

where e∗
Now, recall that if M = A + uv(cid:124), then |M | = (1 + u(cid:124)A−1v)|A|. Set

i = sgn(xi) if |xi| = (cid:107)x(cid:107)∞, and e∗

i = 0 otherwise. Notice that for this component, (cid:107)x(cid:107)∞ = |xi|.

u = log(1 + |x|)/(cid:107) log(1 + |x|)(cid:107)2

(cid:18)

A = Diag

sgn(x)

(cid:20) 1
|x|

+

Then, almost everywhere,

∞,

v = −(kρ(u)e∗/(1 + |x|))(cid:124), and
(cid:21)(cid:19)
kρ(u)
(1 + |x|)(cid:107) log(1 + |x|)(cid:107)∞

.

1 + u(cid:124)A−1v = 1 −

ρ−1kρ(u)(cid:107)x(cid:107)∞
(cid:107)1 + |x|(cid:107)∞(cid:107) log(1 + |x|)(cid:107)∞ + kρ(u)(cid:107)x(cid:107)∞

, and

To complete the veriﬁcation, observe |Diag(eψ(x))| = (cid:81)d
We next show that T (·) is onto. Fix a y ∈ Rd

|A| =

d
(cid:89)

i=1

1
|xi|

×

(cid:19)

d
(cid:89)

(cid:18)

i=1

1 +

ρ−1kρ(u)|xi|
(1 + |xi|)(cid:107) log(1 + |x|)(cid:107)∞
i=1 |xi| · (u/l)1(cid:124)κ(x).
+. Let I = {i : yi = (cid:107)y(cid:107)∞}. Deﬁne the set,

.

S = {x : xi = (cid:107)y(cid:107)∞cρ(u) for all i ∈ I, xi ∈ [0, (cid:107)y(cid:107)∞cρ(u)], i (cid:54)∈ I},

(A.11)

where recall cρ(u) := (l/u)1/ρ. Notice that for all x ∈ S,

T i(x) =






xi[cρ(u)]
(cid:107)y(cid:107)∞

− log(1+xi)
log(1+cρ (u)(cid:107)y(cid:107)∞ )

for i (cid:54)∈ I,

for i ∈ I.

Restricted to x ∈ S, T i(x) is only a function of xi for i (cid:54)∈ I. Fixing i /∈ I, see that T i(x) < yi,
if xi < yicρ(u); and T i(x) = (cid:107)y(cid:107)∞ > yi if xi = (cid:107)y(cid:107)∞cρ(u). Since T i are all continuous maps, by the
intermediate value theorem, there exists some x(cid:48) ∈ [ycρ(u), (cid:107)y(cid:107)∞cρ(u)]d such that T (x(cid:48)) = y. The above
argument also shows that if x1 (cid:54)= x(cid:48), T (x1) (cid:54)= T (x(cid:48)) = y, that is, T (·) is 1 ↔ 1. Since T (·) is symmetric
(cid:3)
about the origin, one can similarly extend the proof to the case where y ∈ Rd.

6

EFFICIENCY IN BLACK-BOX IMPORTANCE SAMPLING

Lemma A.5. For any y ∈ Rd

+ satisfying (cid:107)y(cid:107)∞ ≥ 1/cρ(u),

ycρ(u) ≤ T −1(y) ≤ min

(cid:110)

y[cρ(u)]

log y

log (cid:107)y(cid:107)∞ ∨1, (cid:107)y(cid:107)∞cρ(u)

(cid:111)

.

(A.12)

Proof. We verify the bounds by exhibiting x(cid:48) sandwiched component-wise between the left and right

hand sides of (A.12) and satisfying T (x(cid:48)) = y. For any y as in the statement, we ﬁrst set

˜xi =

(cid:40)

yi [cρ(u)]
1

log yi
log (cid:107)y(cid:107)∞

if yi ≥ 1,
otherwise,

(A.13)

for i = 1, . . . , d. See that ˜xi ∈ [1, (cid:107)y(cid:107)∞cρ(u)]. This is because of the following two observations: 1)
log ˜xi = (1 + log cρ(u)/ log (cid:107)y(cid:107)∞) log yi ≥ 0, when (cid:107)y(cid:107)∞ ≥ 1/cρ(u) > 1; and 2) likewise,

log ˜xi − log((cid:107)y(cid:107)∞cρ(u)) =

1 +

(cid:18)

(cid:19)

log cρ(u)
log (cid:107)y(cid:107)∞

(log yi − log (cid:107)y(cid:107)∞) ≤ 0.

Set I = {i : yi = (cid:107)y(cid:107)∞}. With the set S deﬁned as in (A.11), we thus have ˜x = (˜x1, . . . , ˜xd) ∈ S and
˜xi ≥ 1 for all i. Since log(1 + t)/ log(t) is decreasing over t ≥ 1, we have

T i(x) = xi[cρ(u)]− log(1+xi)

(cid:107) log(1+x)(cid:107)∞ ≥ xi[cρ(u)]

− log xi
log (cid:107)x(cid:107)∞ ,

(A.14)

for any x such that xi ≥ 1, i = 1, . . . , d. With ˜x deﬁned via (A.13), we obtain the following from the
bound in (A.14) and the observation (log ˜xi)/(log (cid:107)˜x(cid:107)∞) = (log yi)/(log (cid:107)y(cid:107)∞) :

T i(˜x) =

(cid:40)

T i(yi[cρ(u)]
T i(1)

log yi
log (cid:107)y(cid:107)∞ )

if yi ≥ 1,
otherwise,

≥

(cid:40)

yi
1

if yi ≥ 1,
otherwise.

For i /∈ I, the map T i(x), when restricted to x ∈ S, is a function only of xi and satisﬁes T i(˜x) ≥ yi
(regardless of whether yi > 1 or yi < 1). Applying the intermediate value theorem component-wise, we
see that there exists some x(cid:48)
i) = yi for all i (cid:54)∈ I.
(cid:3)
Setting x(cid:48)

i = (cid:107)y(cid:107)∞cρ(u) for i ∈ I, we obtain T (x(cid:48)) = y. Hence the bounds (A.12) hold.

i in the interval [yicρ(u), ˜xi] containing yi such that Ti(x(cid:48)

Lemma A.6. Suppose that Assumptions 2 - 3 hold, l(u) is slowly varying in u, and limu→∞ l(u) = ∞.
Then for any γ > 0, the below convergence holds uniformly over p in compact subsets of Rd

++ :

(cid:107)ψu(t(u)p)(cid:107)∞ = o(t(u)), as u → ∞.

Proof. Recall ψu := Λ ◦ T −1 ◦ q, cρ(u) := (l(u)/u)1/ρ and q∞(t(u)) := (cid:107)q(t(u))(cid:107)∞. Fix any M >
0, γ ∈ (0, M ) and p ∈ BM \ Bγ. As limu→∞ l(u) = +∞, q ∈ RV, and q∞(t(u)) = u1/ρ (see Lemma A.1),

(cid:107)q(t(u)p)(cid:107)∞cρ(u) = l(u)1/ρ (cid:107)q(t(u)p)(cid:107)∞
(cid:107)q(t(u))(cid:107)∞

→ ∞.

(A.15)

Then from (A.12), T −1
monotone and (cid:107)p(cid:107)∞ ≤ M . With q∞(t(u)) = u1/ρ,

i

(q(t(u)p)) ≤ (cid:107)q(t(u)p)(cid:107)∞cρ(u) ≤ (cid:107)q(t(u)M 1)(cid:107)∞cρ(u), for i = 1, . . . , d, as q is

q(t(u)M 1)
q(t(u))
Since q ∈ RV(1/α), we have q(t(u)M 1)cρ(u) ≤ M 1/αl(u)1/ρ(1 + o(1)). Therefore,

q(t(u)M 1)
q(t(u))

q(t(u))
q∞(t(u))

q(t(u)M 1)cρ(u) =

l(u)1/ρ ≤

l(u)1/ρ.

T −1
i

(q(t(u)p)) ≤ (cid:107)q(t(u)M 1)(cid:107)∞cρ(u) ≤ max
i=1,...,d

M 1/αil(u)1/ρ(1 + o(1)),

uniformly over p ∈ BM \ Bγ(0). Recall Λ ∈ RV(α) is monotone. Write ¯α = maxi αi. Then for δ > 0,

(cid:107)ψu(t(u)p))(cid:107)∞ = max
i=1,...,d

Λi

(cid:0)T −1

i

(q(t(u)p))(cid:1) ≤ (1 + δ)M0l(u) ¯α/ρ+δ,

for all u suﬃciently large (see (de Haan & Ferreira 2010, Proposition B.1.9 (7))). Here M0 > 0 is a
(cid:3)
suitably large constant. Since t(u) = Λmin(u1/ρ), noting l(u) ¯α/ρ+δ = o(t(u)) completes the proof.

EFFICIENCY IN BLACK-BOX IMPORTANCE SAMPLING

7

A.3. Proof of Theorem 5.2. Unless explicitly speciﬁed, the only assumption made in the proofs below
is that X has a probability density fX (·). As a consequence, the hazard rates

λi(x) = fXi(x)/Λi(x),

i = 1, . . . , d

are well-deﬁned. In the above, fXi(·) denotes the probability density of the component Xi.

Lemma A.7. The second moment M2,u = E[exp(−t(u)Fu(Yu))], where Fu : Rd

+ → R ∪ {+∞} is,

Fu(p) := au(p) + bu(p) − 2d

log t(u)
t(u)

+ χlev+

1 (Lu)(p),

for u > 0, and au : Rd

+ → R, bu : Rd

+ → R are deﬁned as follows:

au(p) =

bu(p) =

1
t(u)

1
t(u)

[log fY (ψu(t(u)p) − log fY (t(u)p)] ,

(cid:2)log λi(T −1

i

(cid:35)
(q(t(u)p))) − log λi(qi(t(u)pi))(cid:3) − log J(T −1(q(t(u)p)))

.

(cid:34) d

(cid:88)

i=1

Proof. Since the change of measure is eﬀected by the map Z = T (X),
(cid:20)(cid:18) fX (X)
fZ(X)

(cid:35)
I(L(Z) ≥ u)

(cid:34)(cid:18) fX (Z)
fZ(Z)

M2,u = E

= E

(cid:19)2

(cid:19)

(cid:21)
I(L(X) ≥ u)

(cid:90)

=

L(x)≥u

fX (x)
fX (T −1(x))

J(T −1(x))fX (x)dx.

(A.16)

Changing variables from x to y = q−1(x), we have fX (x) = (cid:81)d
fXi(x)/ ¯FXi(x) is the hazard rate of Xi. Thus,

i=1 λi(xi)fY (q−1(x)), where λi(x) =

(cid:90)

M2,u =

L(q(y))≥u

d
(cid:89)

i=1

λi(qi(yi))

λi(T −1

i

(qi(yi)))

fY (y)
fY (ψu(y)))

J(T −1(q(y)))fY (y)dy

= t(u)d

(cid:90)

p∈lev+

1 (Lu)

fY (t(u)p)
fY (ψu(t(u)p)))
(cid:123)(cid:122)
(cid:125)
(cid:124)
(a)

d
(cid:89)

i=1
(cid:124)

λi(qi(t(u)pi))

λi(T −1

i

(q(t(u)p)))
(cid:123)(cid:122)
(b)

J(T −1(q(t(u)p)))

fY (t(u)p)dp,

(cid:125)

Since Yu = t(u)−1Y , fY (t(u)p) = t(u)dfYu (p). Checking the terms labeled (a) and (b) in the above
expression equal exp(−t(u)au(p)) and exp(−t(u)bu(p)), respectively, we obtain
(cid:105)(cid:111)(cid:105)

(cid:110)

(cid:104)

(cid:104)
au(Yu) + bu(Yu) + χlev+

1 (Lu)(Yu)

.

(cid:3)

M2,u = t(u)2dE

exp

−t(u)

Thanks to Lemma A.5 - A.6, the terms in Lemma A.7 enjoy the following bounds.

Lemma A.8. Suppose that Assumptions 2 - 3 are satisﬁed and l(u) is slowly varying in u. Then au(p) ≥
I(p) + o(1), as u → ∞, uniformly over p in compact subsets of Rd

++.

Proof. Under Assumption 3, uniformly over ˆy := y/(cid:107)y(cid:107) on S d−1 ∩ Rd
+,

−

log fY ((cid:107)y(cid:107)ˆy)
(cid:107)y(cid:107)

→ I(ˆy), as (cid:107)y(cid:107) → ∞ (replacing n there by (cid:107)y(cid:107)) .

(A.17)

Fix any M > 0, γ ∈ (0, M ) and ε ∈ (0, 1). From the monotonicity of Λ, the lower bound in (A.12), and
(A.15), (cid:107)ψu(t(u)p)(cid:107)∞ := (cid:107)Λ(T −1(q(t(u)p)))(cid:107)∞ ≥ (cid:107)Λ (q(t(u)p)cρ(u)) (cid:107)∞ → ∞, for any p ∈ BM \ Bγ,
as u → ∞. Then due to (A.17) and the upper bound in (A.12),

− log fY (ψ(t(u)p)) ≤ (cid:107)ψ(t(u)p)(cid:107)∞

(cid:2)

sup
ˆy∈S d−1∩Rd
+

I(ˆy)) + ε(cid:3) ≤ εt(u)(M1 + ε)),

8

EFFICIENCY IN BLACK-BOX IMPORTANCE SAMPLING

for all suﬃciently large u; here M1 := sup{I(ˆy) : ˆy ∈ S d−1 ∩ Rd
the regularity properties of I in Lemma 3.4). Observe that from the upper bound in (A.4),

+} is a ﬁnite positive constant (due to

− log fY ((t(u)p)) ≥ t(u)(I(p) − ε),

uniformly over p ∈ BM \ Bγ and all u suﬃciently large. Combining the above displayed bounds on
(cid:3)
− log fY (·) terms, we obtain from the deﬁnition of au(·) that au(p) ≥ I(p) − ε − ε(M1 + ε).

Lemma A.9. Suppose that Assumptions 2 and 4 are satisﬁed and l(u) is slowly varying in u. Then
lim inf u→∞ bu(p) ≥ 0, where the convergence is uniform over p ∈ Rd
+.

Proof. Recall the deﬁnitions of J(x) and ˜Ji(x) in (9a) - (9b). Since κ(x) in (6) satisﬁes κ(x) ∈ [0, 1]d,
we have 1(cid:124)κ(x) ≤ d. Next observe that for all t ≥ 0, t/[(1 + t) log(1 + t)] ≤ e. Therefore for x ∈ Rd
++,

d
(cid:89)

i=1

˜Ji(x) ≤

(cid:20)

1 +

d
(cid:89)

i=1

ρ−1 log(u/l)
log(1 + |x|i)

|xi|
1 + |xi|

(cid:21)

≤ (cid:0)1 + eρ−1 log(u/l)(cid:1)d

.

Moreover, max{ ˜Ji(x), . . . , ˜Jd(x)} ≥ 1. Combining these observations we obtain that
J(x) ≤ (cid:2)1 + eρ−1 log(u/l)(cid:3)d

(u/l)d, x ∈ Rd

++.

(A.18)

i

log λi

To bound the terms involving hazard rates λi(·), we proceed as follows: Due to Assumption 4, λi(·) is
eventually monotone for any i. From Lemma A.5, if λi is eventually decreasing, λi(T −1
(q(t(u)p)) >
If λi is eventually increasing, the bound T −1(q(t(u)p)) ≥ q(t(u)p)cρ(u) from (A.12)
λi(qi(t(u)pi)).
implies λi(T −1
(qi(t(u)pi))) ≥ λi(qi(t(u)pi)cρ(u)). In either case,
(q(t(u)p))(cid:1) − log λi
(cid:0)T −1

(cid:0)qi(t(u)pi)cρ(u)(cid:1) − log λi
(A.19)
Since Λi ∈ RV(αi) and λi = Λ(cid:48)
i is monotone, λi ∈ RV(αi − 1) (see (de Haan & Ferreira 2010, Proposition
B.1.9 (11))). Given ε > 0, an application of Potter’s bounds (de Haan & Ferreira 2010, Proposition B.1.9
(7)) yields λi(qi(t(u)pi)cρ(u))/λi(qi(t(u)pi)) ≥ cρ(u)αi−1+(cid:15), for all u suﬃciently large. Then from (A.19),
(cid:0)qi(t(u)pi)(cid:1) ≥ (αi −1+ε) log cρ(u), for i = 1, . . . , d. Since cρ(u) := (l/u)1/ρ,
log λi
we obtain the following by combining this bound with (A.18):

(cid:0)qi(t(u)pi)(cid:1) ≥ log λi

(q(t(u)p))(cid:1)−log λi

(cid:0)qi(t(u)pi)(cid:1).

(cid:0)T −1

i

i

i

bu(p) ≥ −ρ−1 log(u/l)
t(u)

inf
p∈Rd
+

d
(cid:88)

i=1

(αi − 1 + ε) − d

log (cid:0)1 + eρ−1 log(u/l)(cid:1)
t(u)

− d

log(u/l)
t(u)

→ 0,

where the convergence follows from noting t(u) := Λmin(u1/ρ) and log(u/l) = o(t(u)).

(cid:3)

Lemma A.10. Suppose that Assumptions 1 and 2 are satisﬁed. Then for all suﬃciently large u,
lev+

+ \ Bγ, for some γ > 0.

1 (Lu) ⊆ Rd

Proof. Recall the deﬁnition fLD(y) := L∗(q∗y1/α). The function fLD(·) is therefore continuous and
bounded on the unit sphere S d−1 ∩ Rd

i = 0 if αi > minj αj, we have for all c > 0,

+. Since q∗

fLD(cy) = L∗(q∗(cy)1/α) = L∗(c1/α∗ q∗y1/α) = cρ/α∗ L∗(q∗y1/α) = cρ/α∗ fLD(y) > 0,
from the homogeneity of L∗. Combining these observations, we obtain that for any γ > 0,
supy∈Bγ fLD(y) < γρ/α∗ M1, where M1 > maxy∈B1 fLD(y). Choosing γ < (2M1)−α∗/ρ ensures
supy∈Bγ fLD(y) < 1/2. Thus [Ξ1,γ(fLD)]1+ε ∩ Bγ = ∅, if ε > 0 is chosen suitably small. Then from
the inclusion lev+
1 (Lu) ∩ Bγ = ∅, for all u
(cid:3)
suﬃciently large.

1 (Lu) ∩ Bγ ⊆ [Ξ1,γ(fLD)]1+ε from Corollarly A.3, we have lev+

Recall from Lemma A.7 that the second moment M2,u = E (cid:2)exp (cid:0) − t(u)Fu(Yu)(cid:1)(cid:3) .

Lemma A.11. Suppose that Assumptions 1 - 4 are satisﬁed and l(u) is taken to be slowly varying in u.
Then there exists u1 suﬃciently large such that for all u > u1, inf p∈Rd

Fu(p) ≥ 0.

+

Checking this non-negativity in Lemma A.11, while is executed along similar lines as in the proofs of
Lemma A.8 - A.9, is technically more involved. Its proof is therefore given later in Section D, which is

1 (Lu)(·) in Corollary A.3,
log t(un)
t(un)

+ χlev+

EFFICIENCY IN BLACK-BOX IMPORTANCE SAMPLING

9

devoted to technical results that are repetitive in terms of the key ideas involved. We now prove the key
variance reduction result, namely, Theorem 5.2.

Proof of Theorem 5.2. From Lemma A.7, we have M2,u = E [exp{−t(u)Fu(Yu)}] . Deﬁne the function
F : Rd

+ → R ∪ {+∞} as,

F (p) := I(p) + χlev+

1 (fLD)(p).

Since lev+
1 (fLD) is closed and I(·) is continuous, F (·) is lower semi-continuous. Consider sequences
{un} ⊆ R+, {pn} ⊆ Rd satisfying un → ∞ and pn → p. Due to Lemma A.10, there exists γ, n0 > 0 such
that lev+
1 (Lun ) ∩ Bγ = ∅, for all n > n0. Suppose p /∈ Bγ/2. Then from the uniform convergences of
au(·), bu(·) in Lemma A.8 - A.9 and that of χlev+

lim inf
n→∞

Fun (pn) := lim inf
n→∞

aun (pn) + bun(pn) − 2d

1 (Lun )(pn) ≥ F (p).

(A.20)

On the other hand, if p ∈ Bγ/2, we have {pn : n ≥ n1} ⊆ Bγ for some n1 > n0. Since lev+
1 (Lun )∩Bγ = ∅
for all n > n1, we obtain inf n≥n1 Fun (pn) = ∞. Thus, regardless of the membership of p (in the ball
Bγ/2), (A.20) holds. From Lemma A.11, we deduce that there exists n2 > n1 satisfying that the family
{Fun : n ≥ n2} comprises non-negative valued functions. Recall from Theorem 3.3 that the sequence
{Yun : n ≥ 1} satisﬁes LDP with rate function I(·). Then due to a general version of Varadhan’s integral
lemma (see (Varadhan 1988, Theorem 2.2)), we obtain from (A.20) that

lim sup
n→∞

1
t(un)

log E (cid:2)exp (cid:8) − t(un)Fun (Yun )(cid:9)(cid:3) ≤ − inf
p∈Rd

{F (p) + I(p)} = −2

inf
p∈lev+
1 (fLD)

I(p).

Since M2,u = E [exp{−t(u)Fu(Yu)}] , combining this conclusion with the bounds M2,u ≥ p2
lim inf u→∞[t(u)−1 log p2

u] ≥ −2I ∗ from Theorem 4.1, we obtain (21).

u and
(cid:3)

Appendix B. Sufficient conditions and examples for the tail models considered

This section serves to illustrate the variety of multivariate distribution families which come under
the tail modeling framework considered and to provide suﬃcient conditions. In Tables 1 - 2 below, we
provide some examples of univariate distribution families which satisfy either the marginal assumptions
in Assumption 2, or its heavier-tailed counterpart in Assumption 6.

Table 1. Examples of some marginal distributions satisfying Assumption 2 and their
right tail parameter α. Larger the parameter α, the lighter the respective tail is.

Distribution families
Exponential, Erlang, Gumbel, Logistic
Gamma, Chi-squared, phase-type
Gaussian, Chi, mixtures of Gaussians, Rayleigh
Weibull with shape parameter k
Generalized-gamma with shape parameter k

Tail parameter α
1
1
2
k
k

Table 2. Examples of some heavy-tailed marginal distributions satisfying Assumption
6 and the respective right tail parameter α. Larger the parameter α, relatively lighter is
the respective tail.

Distribution families
Lognormal
Generalized Pareto, Student’s t, Regularly varying
Log-Laplace, Frechet, Lomax, Log-logistic, Cauchy

Tail parameter γ
2
1
1

10

EFFICIENCY IN BLACK-BOX IMPORTANCE SAMPLING

The notion of multivariate regular variation turns out to be convenient in understanding the condition
+ → R+ is

in Assumption 3 in terms of the probability density of X. We say that a function f : Rd
multivariate regularly varying if for any sequence xn of Rd

+ satisfying xn → x (cid:54)= 0,

lim
n→∞

n−1f (h(n)xn) = f ∗(x),

(B.1)

for some limiting f ∗ : Rd
+ → (0, ∞) and a component-wise increasing h(t) = (h1(t), . . . , hd(t)) satisfying
hi ∈ RV(1/ρi), ρi > 0, i = 1, . . . , d. It follows from (B.1) that f ∗(·) satisﬁes f ∗(s1/ρx) = sf ∗(x). In the
above, the notation s1/ρ is to be interpreted as the vector s1/ρ = (s1/ρ1, . . . , s1/ρd ). When referring to
(B.1), we write f ∈ MRV, or more speciﬁcally, f ∈ MRV(f ∗, h) if there is a need to explicitly specify
the scaling functions h(·) and the respective limit function f ∗(·).

For instance, the function f : R2
2 , satisﬁes
f ∈ MRV(f ∗, h) with f ∗(x) = x2.5
and h(t) = (t1/2.5, t2). See Table 3 below for some useful
examples which arise in the context of tail modeling and Resnick (2007) for a detailed treatment. The
following result on MRV functions is required in the subsequent proofs. Let Id(x) = x.

+ → R+ deﬁned as in, f (x) = x2.5
1 + x0.5
2

1 (1 − exp(−x2)) + x0.5

Lemma B.1. Suppose g : Rd
+ → Rd
s > 0, deﬁne ˜gs(t) := g(st) and vs,β(x) := sx1/β. Then for f : Rd
so long as f ∈ MRV(u∗ ◦ vs,β, ˜gs) for some s > 0.

+ is such that gi ∈ RV(βi) is monotone, for i ∈ {1, . . . , d}. For
+ → R, we have f ◦ g ∈ MRV(u∗, Id),

Proof. Consider any M > 0. Since gi ∈ RV(βi), i = 1, . . . , d, and are monotone, we have gi(tx)/gi(t) →
xβi, uniformly over x ∈ [0, M ]. Consequently for s > 0 and xn → x ∈ [0, M ]d, we obtain g(sn ·
s−1xn)/g(sn) = (s−1x)β(1 + o(1)). Therefore,

n−1(f ◦ g)(nxn) = n−1f

(cid:18) g(ns · s−1xn)
g(ns)

(cid:19)

g(ns)

=

f (cid:0)(s−1x)β(1 + o(1))g(ns)(cid:1)
n

= u∗(x)(1 + o(1)),

where the last equality follows from f ∈ MRV(u∗ ◦ vs,β, ˜gs).

Suppose that the support of X contains Rd

(cid:3).
+. Propositions B.2 - B.3 below give suﬃcient conditions

under which Assumption 3 is satisﬁed.

Proposition B.2 (Suﬃcient conditions on the density of X). Suppose that the marginal distributions
of X satisfy either Assumptions 2 and 4, and the density of X when written in the form,

fX (x) = exp(−ψ(x)),

for x ∈ R+
d ,

(B.2)

satisﬁes ψ ∈ MRV(ψ∗, h). Then X satisﬁes Assumptions 3. In particular, the hazard functions Λ =
(Λ1, . . . , Λd) in Assumption 2 and the limiting function I(·) in Assumption 3 are related to h and ψ∗ as
follows: there exists c ∈ Rd

++ such that
I(x) = ψ∗(cx1/α)

and

h(x) = q(x)(c−1 + o(1)),

as (cid:107)x(cid:107) → ∞, and α = (α1, . . . , αd) ∈ Rd

++ is such that hi ∈ RV(1/αi), i = 1, . . . , d.

Proof.
Since Y = Λ(X) and X = q(Y ), we have (12) satisﬁed with ϕ(y) = ψ(q(y)) and
i[λi(qi(yi))]−1, as a consequence of change of variables. Observe that Λi(t) = (cid:82) t
p(y) = (cid:81)
−∞ λi(x)dx,
for monotone λi. Therefore using (de Haan & Ferreira 2010, Proposition B.1.9(11)), tλi(t)/Λi(t) → αi
and λi ∈ RV(αi − 1). This implies log λi ◦ qi ∈ RV(0). Since the support of X contains Rd
+, whenever
yn → y ∈ R+

d , n−(cid:15) log p(nyn) = o(1) for all (cid:15) > 0. Therefore Assumption 3 holds if ϕ ∈ MRV(I, Id).

To see the latter, recall that ϕ(y) = ψ(q(y)). Substituting g(y) = q(y) and β = 1/α in Lemma B.1,
a suﬃcient condition for ϕ ∈ MRV(I, Id) is that ψ ∈ MRV(I ◦ vs,1/α, qs), for some s > 0. Under
the proposition assumptions, ψ ∈ MRV(ψ∗, h). Equating parameters, I(x) = ψ∗(cx1/α), and h(x) =
q(x)(c−1 + o(1)), where c = s−1/α ∈ Rd
(cid:3)

++.

To identify suﬃcient conditions in the presence of heavier tailed distributions, let L denote the collec-
tion of indices of the components (X1, . . . , Xd) which satisfy the lighter tailed assumption in Assumption

EFFICIENCY IN BLACK-BOX IMPORTANCE SAMPLING

11

2. For i /∈ L, we have the respective Xi satisfying the heavier tailed assumption in Assumption 6. Let
Z = (Z1, . . . , Zd) be deﬁned as follows:

Zi =

(cid:40)

log(1 + Xi)
Xi

if Xi > 0 and i /∈ L,
otherwise.

(B.3)

Proposition B.3 (Suﬃcient conditions in the presence of heavier tails). Suppose that the marginal
distributions of X satisfy Assumptions 4 and 6. Let the probability density of Z = (Z1, . . . , Zd) in (B.3),
when written in the form,

fZ(z) = exp(− ˆψ(z)),

for z > 0,

satisfy ˆψ ∈ MRV(ψ∗, h). Then X satisﬁes Assumption 3.
limiting I(·) in Assumption 3 are related to h and ψ∗ as follows: there exists c ∈ Rd

In particular, hazard function Λ and the

++ such that

I(x) = ψ∗(cx1/α)

and

as (cid:107)x(cid:107) → ∞, and α = (α1, . . . , αd) ∈ Rd

(cid:40)

hi(xi) =

i + o(1))

qi(xi)(c−1
log(qi(xi))(c−1
++ is such that hi ∈ RV(1/αi), i = 1, . . . , d.

i + o(1)))

if i ∈ L,

otherwise,

Proof of Proposition B.3. For i ∈ {1, . . . , d}, let ˜Λi and ˜λi denote the hazard function and hazard
rate of Zi, respectively. Let ˜qi := ˜Λ←
i . To rewrite the density of Y in terms of that of Z, see that
˜Λi(z) = Λi(exp(z) − 1), when i /∈ L. Using a change of variables,

fY (y) =

1
˜λi(˜qi(yi))
Recall that under the assumptions of the proposition, ˜Λi ∈ RV(αi) and the support of Z contains R+
d .
Since ˜Λi(x) ∼ Λi(exp(x)) as x → ∞, we obtain the desired conclusion following the steps in the proof of
Proposition B.2 with p(y) = 1/(cid:81)d
˜λi(˜qi(yi)) and ϕ(y) = ˆψ(˜q(y)).
(cid:3)

− ˆψ(˜q(y))

exp

(cid:81)d

i=1

.

(cid:17)

(cid:16)

i=1

Remark B.4. If the density of X is written in the form (B.2) in the positive orthant, then the respective
density for Z in the positive orthant is given by fZ(z) = exp(− ˆψ(z)), where

(cid:124)
ˆψ(z) := ψ ◦ E(z) − 1
Hz,

with the map E : (x1, . . . , xd) (cid:55)→ (cid:0)E1(x1), . . . , Ed(xd)(cid:1) and the vector 1H ∈ Rd

+ deﬁned as follows:

Ei(xi) :=

(cid:40)

xi
exp(xi) − 1

if i ∈ L,

if i /∈ L,

and

1H =

(cid:40)

0

1

if i ∈ L,

if i /∈ L.

(B.4)

Then one can restate the condition in Proposition B.3, directly in terms of density of X, as follows: If
(cid:124)
ˆψ(z) = ψ ◦ E(z) − 1
Hz ∈ MRV(ψ∗, h), then the conclusion in Proposition B.3 holds.
Example B.5 (Multivariate t distribution). Suppose X is distributed according to multivariate t dis-
tribution with density, fX (x) = cρ exp(−ψ(x)), where
(cid:18)

(cid:19)

ψ(x) =

log

1 +

ρ + d
2

x(cid:124)Σ−1x
ρ

,

ρ is a suitable positive integer and cρ is the respective normalizing constant. Since the marginals of
a multivariate t distribution are heavy-tailed, we have E(x) = exp(x) − 1. With Z = log(1 + X),
fZ(z) = e−(cid:107)z(cid:107)1fX (ez − 1), due to change of variables. Then in this case,

ˆψ(z) = −(cid:107)z(cid:107)1 +

ρ + d
2

(cid:18)

log

1 +

(ez − 1)Σ−1(ez − 1)
ρ

(cid:19)

.

Thus ˆψ ∈ MRV(ψ∗, h), where ψ∗(z) := (ρ + d)(cid:107)z(cid:107)∞ − (cid:107)z(cid:107)1 and h(t) = t1.

(cid:3)

Example 3.2 in Section 3 and Example B.5 above serve as illustrations for the sequence of steps in-
volved in verifying memberships of commonly used multivariate distributions and copula models in the

12

EFFICIENCY IN BLACK-BOX IMPORTANCE SAMPLING

considered tail modeling framework. Table 3 below is intended to serve as a reference for identifying the
limiting function I(·) in Assumption 3 (or) the respective function ψ∗(·) which arise in the characteriza-
tions in Propositions B.2 - B.3. Thanks to standardization, the limiting function I(·) is unique despite
ψ∗(·) depending on the speciﬁc scaling function h(·) employed.

Appendix C. Proofs of results on applications to distribution network and credit risk

Proof of Proposition 4.3: With s = uθ, observe from the proof of (Blanchet et al. 2019, Thm. 1),

P (max

i

(Di − uθi) > k(u)) ≤ P (N F uθ) ≤ P (max

i

(Di − uθi) > 0)

Since k(u) = o(u), we have for any ε > 0 that there exists u0 > 0 satisfying k(u)/u < ε for all u > u0.
Recall that θ > 0. Letting L(x; θ) = maxd

i=1(xi/θi), we obtain the following from the above bounds:

P (L(D; θ) ≥ u(1 + ε)) ≤ P (N F uθ) ≤ P (L(D; θ) ≥ u),

for all u > u0. Since L(nx; θ)/n = (cid:107)θ−1x(cid:107)∞ for x ∈ Rd

+, we arrive at the following from Theorem 4.1:

−I ∗(θ) ≤ lim inf
u→∞

log P (N F uθ)
Λmin(u(1 + ε))

≤ lim sup
u→∞

log P (N F uθ)
Λmin(u)
+ : maxi y1/αi

≤ −I ∗(θ),

where I ∗(θ) := inf{I(y) : y ∈ Aθ} and Aθ := {y ∈ Rd
mini=1,...,d (θi/q∗
the observation that Λmin ∈ RV. To verify this expression for I ∗(θ), we proceed by letting ci := (θi/q∗
Since Aθ = ∪i{y ∈ Rd
Lemma 3.4(d). Note that q∗
Therefore I ∗(θ) = mini=1,...,d (θi/q∗

i /θi ≥ 1}. If we show I ∗(θ) =
q∗
i )α∗ , then (18) would follow from the above bounds due to the arbitrariness of ε > 0 and
i )αi.
+ : yi ≥ ci}, we obtain I ∗(θ) = inf y∈Aθ I(y) = mini ci as a consequence of
(cid:54)= α∗ (see (de Haan & Ferreira 2010, Proposition B.1.9)).

i = 0 if αi
i )α∗ .

i

For verifying θ∗ is the unique minimizer of the right-hand side of (18), see that θi ≤ q∗

i /(cid:107)q∗(cid:107)1 for
at least one i when θ ∈ Ld
1. Further, this
inequality is tight for θ∗ = q∗/(cid:107)q∗(cid:107)1, and therefore, the maximum is attained at θ∗. To see uniqueness
of the solution, notice that if θ (cid:54)= θ∗, and θ ∈ Ld
i /(cid:107)q∗(cid:107)1 for some
(cid:15) > 0. Then I ∗(θ) < I ∗(θ∗), which would be a contradiction to the optimality of θ∗.

1, then for at least one i, θi ≤ (1 − (cid:15)) q∗

1. Therefore I ∗(θ) = mini(θi/q∗

i )α∗ ≤ (cid:107)q∗(cid:107)−α∗

for all θ ∈ Ld

1

1 ∩ Rd

To see the ﬁnal conclusion, we ﬁrst argue that the convergence in (18) is uniform over θ in compact
subsets of Ld
++. Deﬁne Iu(θ) = [Λmin(u)]−1 log P (N F uθ). Suppose θu → θ as u → ∞. Notice
that for each u, Iu(θ) is monotone in θ, that is if θ1 > θ2, then Iu(θ1) > Iu(θ2). Further, for any
δ > 0, there exists a u0, such that u > u0 implies θu ∈ [θ − δ1, θ + δ1]. Therefore, for u > u0,
Iu(θu) ∈ [Iu(θ − δ1), Iu(θ + δ1)]. Now, from the tail LDP in Proposition 4.3, notice that for all large
enough u (say u > u1), Iu(θ − δ1) ≥ I ∗(θ − δ1) − ε/2 and Iu(θ + δ1) ≤ I ∗(θ + δ1) + ε/2. This suggests
that for u suﬃciently large, I ∗(θ − δ1) − ε/2 ≤ Iu(θu) ≤ I ∗(θ + δ1) + ε/2. Since I ∗(·) is a continuous
function of θ, δ can be selected small enough so that for all large enough u, |Iu(θu) − I ∗(θ)| ≤ ε. Thus,
1) to I ∗. We consequently have the inﬁma
Iu converges continuously (and therefore uniformly over Ld
converge (see (Rockafellar & Wets 1998, Theorem 7.31)) as in,

inf
1 ∩ Rd

++

θ∈Ld

log P (N F uθ)
Λmin(u)

→ − inf
θ∈Ld
1

I ∗(θ) = −I ∗(θ∗).

Therefore given any ε > 0, there exists u0 large enough such that for all u > u0,
I ∗(θ)(1 + ε/4)
I ∗(θ∗)(1 − ε/4)

I ∗(θ)(1 − ε/4)
I ∗(θ∗)(1 + ε/4)

log P (N F s)
inf s∈Su log P (N F s)

≤

≤

for any s ∈ Su. The result follows from continuity of I ∗ and θ∗ serving as its unique minimizer.

(cid:3)

1(Footnotes for Table 3) µ, Σ are the location, scale parameters, U is uniformly distributed on the unit sphere in Rd and
is independent of R; includes the special cases of factor models if Σ = Γ(cid:124)Γ + σ2Id for some factor matrix Γ ∈ Rk×d with
k < d, and graphical models where R is Gaussian and the inverse covariance matrix is sparse.
2student-distributed with d, ρ degrees of freedom

EFFICIENCY IN BLACK-BOX IMPORTANCE SAMPLING

13

Table 3. Some commonly used density families which satisfy Assumptions 2 - 3, along
with their limiting functions I(·) and ψ∗(·) (where applicable). Certain constants are
written as c or c (if c ∈ Rd) to minimize clutter

Density families

Elliptical densities 1

1) Multivariate normal:
mean = µ, covariance Σ

2) Multivariate t−
distribution2: R ∼ Fd,ρ

3) R is light-tailed with
p.d.f. fR(r) = exp(−g(r)),
for g ∈ RV(k), k > 0

4) R is heavy-tailed with
p.d.f. fR(r) = exp(−g(r)),
for g ◦ exp ∈ RV(k), k > 1

Limiting function
ψ∗(x)
given by X d= µ + RΣ1/2U :
x(cid:124)Σ−1x

Respective
copula family

Gaussian
copula

Student-t
copula

Limiting I(x)
in Assumption 3b

(x1/2)(cid:124)R−1x1/2, with
R = correlation matrix
ρ )(cid:107)x(cid:107)∞ − 1

ρ (cid:107)x(cid:107)1

(1 + d

(ρ + d)(cid:107)x(cid:107)∞ − (cid:107)x(cid:107)1

(x(cid:124)Σ−1x)k/2

(cid:107)x(cid:107)k
∞

Elliptical
copula family

Elliptical
copula family

((x1/k)(cid:124)R−1x1/k)k/2,
with R = correlation
matrix

(cid:107)x(cid:107)∞

Exponential family with p.d.f. fX (x) ∝ g(x) exp (cid:0)η(cid:124)T (x)(cid:1)

Minimal, light-tailed:
η(cid:124)T ∈ MRV(T ∗

η , n1)
η (·), g ∈ RV

for some T ∗

T ∗
η (x)

-

Generalized linear models: ξ = (X, Y ) with fY | X (y | x) ∝ b(y) exp
X with p.d.f in exponential family
b ∈ MRV, fx(x) = e−ψ(x)
ψ + u ∈ MRV(ψ∗, (h, r))
u(x, y) = (cid:96)−1(βT x)(cid:124)Ty(y),
hi ∈ RV(αi), ri ∈ RV(βi)

ψ∗(x, y)

-

T ∗
η (cx1/k) with
k as in f ∈ RV(k)
for f (n) = η(cid:124)T (n1)

(cid:16)

(cid:96)−1(βT x)(cid:124)Ty(y)

(cid:17)

ψ∗ ◦ π−1

1 , with

π1(x) = (xα, yβ)

Logconcave densities with p.d.f. fX (x) = exp(−ψ(x))
convex ψ ∈ MRV

ψ∗(·) limit with
scaling h(t) = Λ−1(t)

Archimedian copula family with C(u) = φ−1(φ(u1) + . . . + φ(ud))

-

-

-

Mixtures of K normal
variables with
covariances Σ1, . . . , Σk

-

-

-

Gumbel
φ(u) = (− log u)θ

Clayton,
φ(u) = (t−θ−1)

θ

Independence

minK

i=1 x(cid:124)Σ−1
k x

-

cψ∗ ◦ π−1

(cid:107)x(cid:107)θ

(1 + θd)(cid:107)x(cid:107)∞ − θ(cid:107)x(cid:107)1

(cid:107)x(cid:107)1

(x1/2)(cid:124)R−1

k∗ x1/2,
where k∗ minimizes

{(x1/2)(cid:124)R−1

k x1/2 : k ≤ K}

14

EFFICIENCY IN BLACK-BOX IMPORTANCE SAMPLING

C.1. Proofs of Theorem 6.2 and Proposition 6.3. Let [J] denote the set {1, . . . , J}. For any ε > 0,
j ∈ [J], I ⊆ [J], and x ∈ Rd

+, we deﬁne,

Cx,ε := {i ∈ [m] : Wt(i)(x, vi) > γ(1 − ε)}

sε(x) :=

1
m

(cid:88)

ei,

i∈Cx,ε

em(I) :=

1
m

(cid:88)

ei,

i∈[m]:t(i)∈I

wj(x) := min

i∈[m]:t(i)=j

Wt(i)(x, vi), and

e∞(I) :=

e∗
m = max
I /∈Jm

(cid:88)

j∈I

¯cj,

em(I).

(C.1)

(C.2)

Lemma C.1. P (Em | X) ≤ exp(−0.5mγεme−1
0
sεm(·) is as deﬁned in (C.1).

(q¯em − sεm (X))+ + exp(−0.5γεm)), almost surely, where

Proof. For any x ∈ Rd

+, let Px(·) denote the conditional law of the default variables (Y1, . . . , Ym)
given X = x; let Ex[·] denote the associated expectation. For any λ > 0, we obtain from Markov’s
inequality, Px(Lm > q¯em) ≤ exp (−mλq¯em + log Ex[exp(mλLm)]) , due to the independence of the
default variables Y1, . . . , Ym given X. Letting ψm(λ, x) := 1
i=1 log Ex [exp(λeiYi)] and gm(x) =
m
supλ>0 {λq¯em − ψm(λ, x)} , we obtain

(cid:80)m

Px(Lm > q¯em) ≤ exp (−mgm(x)) ,

(C.3)

as a consequence. With Px(Yi = 1) given as in (23), we have
(cid:18)

(cid:19)

ψm(λ, x) =

log

1 +

exp(λei) − 1
1 + exp(γ − Wt(i)(x, vi))

m
(cid:88)

1
m

≤

1
m

i=1
(cid:88)

i∈Cx,ε

log (1 + exp(λei) − 1) +

1
m

(cid:88)

(cid:18)

log

1 +

i /∈Cx,ε

exp(λe0) − 1
1 + exp(γ − γ(1 − ε))

(cid:19)

,

where we have used that Wt(i)(x, vi) ≤ γ(1 − ε) for every i /∈ Cx,ε and that ei ≤ (0, e0]. Then,

ψm(λ, x) ≤ λsε(x) + log (1 + exp(λe0 − γε)) ,

from the deﬁnition of sε(x). If q¯em > sε(x), we obtain a lower bound for gm(x) by considering the
speciﬁc value λ = λ∗

0 γε as below:

m(x) := 0.5e−1
gm(x) ≥ λ∗

m(x)q¯em − λ∗

m(x)sε(x) − log (1 + exp(λ∗

m(x)e0 − γε))

≥ 0.5e−1

0 γε (q¯em − sε(x)) − exp(−0.5γε).

If q¯em ≤ sε(x), a trivial bound gm(x) ≥ 0 is obtained by letting λ = λ∗
0.5e−1

0 γε (q¯em − sε(x))+ − exp(−0.5γεm). Combining this with (C.3) yields the desired result.
Recall from the deﬁnitions that Jm := {I ⊆ [J] : em(I) ≥ q¯em} and J := {I ⊆ [J] : e∞(I) ≥ q¯e}.

m(x) = 0. Thus, gm(x) ≥
(cid:3)

Lemma C.2. There exists a positive integer m0 such that Jm = J for all m > m0. Consequently,

inf
m>m0

(q¯em − e∗

m) > 0

and

inf
m>m0,I∈Jm

(em(I) − q¯em) > 0.

Proof. Notice that under the model assumptions stated in Section 6, ¯em → ¯e and q¯e (cid:54)∈ {e∞(I) : I ⊆

[J]}. The latter implies that there exists some δ1 > 0 such that

min
I⊆[J]

|e∞(I) − q¯e| > δ1.

(C.4)

Further, for all j ∈ [J], m−1 (cid:80)
there exists m0 suitably large such that for all m > m0,

i:t(i)=j ei → ¯cj. Consider any δ ∈ (0, δ1/2). Due to these convergences,

em(I) ∈ (e∞(I) − δ, e∞(I) + δ), and ¯em ∈ (¯e − δ, ¯e + δ).

(C.5)

uniformly over I ⊆ [J]. Since q ∈ (0, 1), the above bounds imply that e∞(I) ≥ q¯e − 2δ for any I ∈ Jm.
With δ < δ1/2, e∞(I) > q¯e − δ1, or equivalently, e∞(I) ≥ q¯e (due to (C.4)). Therefore Jm ⊆ J for

EFFICIENCY IN BLACK-BOX IMPORTANCE SAMPLING

15

all m > m0. Similarly if I ∈ J is such that e∞(I) ≥ q¯e, we automatically have e∞(I) ≥ q¯e + δ1 (due
to (C.4)). Similarly from (C.5), em(I) ≥ q¯em + δ1 − 2δ ≥ q¯em. Therefore J ⊆ Jm for all m > m0.
Combining the two inclusions result in the desired conclusion that J = Jm.

With e∗

m deﬁned as in (C.2) and Jm = J for all m > m0, we have e∗

m = maxI(cid:54)∈J em(I) for all m > m0.
Again for δ ∈ (0, δ1/2), we obtain from (C.5) that e∗
m = maxI(cid:54)∈J em(I) < maxI(cid:54)∈J e∞(I) + δ1/2 for all
m > m0. However maxI(cid:54)∈J e∞(I) < q¯e − δ1 due to (C.4).Therefore e∗
m < q¯e − δ1/2. Since q¯e < q¯em + δ,
we arrive at the conclusion that q¯em − e∗
m > δ1/2 − δ, for all m > m0. Recalling that δ < δ1/2, the ﬁrst
inequality in the lemma statement stands veriﬁed. Observing that em(I) > q¯em for any I ∈ Jm, the
(cid:3)
second inequality follows from completely analogous arguments.

Proof of Theorem 6.2: We treat the terms in the bound, P (Em) ≤ P (Am) + P (Em \ Am), separately.
k (q∗x1/α).
Step 1) To obtain an upper bound for P (Am): Deﬁne L∗
For any sequence {xn} ⊂ Rd
cr(x).
To see this, recall the deﬁnition of Lcr(x) in (25). From the continuous convergence of Wi(·, v) in
Assumption 5, supI⊂[J],t(i)∈I |n−ρWi(nxn, vi) − W ∗
t(i)(x)| → 0 as n → ∞. Consequently, for any I ⊂ [J],
mini:t(i)∈I n−ρWi(nxn, vi) → minj∈I W ∗

+ satisfying xn → x (cid:54)= 0, we ﬁrst show that n−ρLcr(nxn) → L∗

cr(x) := maxI∈J mink∈I W ∗

j (x). Since Jm = J for all m > m0 (see Lemma C.2),
n−ρLcr(nxn) → L∗

cr(x),

(C.6)

for any xn → x (cid:54)= 0. Since Am := {Lcr(X) > c(1 − εm)mη} and εm (cid:38) 0, this continuous convergence
implies that one can apply the asymptotics from Theorem 4.1 to evaluate P (Am) as below:

lim
m→∞

P (Am)
Λmin(c1/ρmη/ρ)

= lim
m→∞

log P (Lcr(X) ≥ cmη)
Λmin(c1/ρmη/ρ)

= −Icr,

(C.7)

where Icr is deﬁned as in (27).
Step 2) To show P (Em \ Am) = o(P (Am)) : Recall that Am = {Lcr(X) > c(1 − εm)mη}. For any
x ∈ Rd

+ such that Lcr(x) ≤ c(1 − εm)mη, we have from the deﬁnition of Lcr(·) that,

For such x, the collection J := {j ∈ [J] : wj(x) > c(1 − ε)mη} is not a member of Jm. Hence,

max
J∈Jm

min
j∈J

wj(x) ≤ c(1 − εm)mη.

sεm(x) =

1
m

(cid:88)

i∈Cx,εm

ei ≤

1
m

(cid:88)

(cid:88)

j∈J

{i:t(i)=j}

ei ≤ e∗
m,

where e∗

m is deﬁned as in (C.2). Thus we have from Lemma C.1 that
(cid:16)

P (Em \ Am) ≤ exp

−0.5mγεme−1
0

(q¯em − e∗

(cid:17)
m)+ + exp(−0.5γεm)

.

(C.8)

Recall that γ = cmη(1 + o(1)) and εm is chosen such that mη(1−α∗/ρ)+1εm → ∞. Therefore,

lim
m→∞

mγεm
Λmin(mη/ρ)

= lim
m→∞

m1+ηεm
mα∗η/ρ(1+o(1))

= ∞.

Since inf m>m0(q¯em − e∗

m) > 0 for m0 large (see Lemma C.2(b)), we obtain from (C.8),
m exp(−0.5γεm) − 0.5mγεme−1
0

(q¯em − e∗

m)+

lim sup
m→∞

log P (Em \ Am)
Λmin(mη/ρ)

≤ lim sup
m→∞

Λmin(mη/ρ)
Combining (C.9) and (C.7), we arrive at P (Em \ Am) = o(P (Am)).
Step 3) To obtain a matching lower bound: Choose δm such that δm (cid:38) 0 and δmγ → ∞. Consider
any ﬁxed x ∈ Bm := {Lcr(x) ≥ γ(1 + δm)}. Notice that there exists a set I ∈ Jm satisfying

= −∞.

(C.9)

min
i:t(i)∈I

Wt(i)(x, vi) > γ(1 + δm).

For the chosen x and the resulting index set I, we have for all i ∈ I,

P (Yi = 1 | X = x) =

1
1 + exp(γ − Wt(i)(x, vi))

≥

1
1 + exp(−γδm)

≥ 1 − δ,

16

EFFICIENCY IN BLACK-BOX IMPORTANCE SAMPLING

where δ > 0 is suitably small. The resulting conditional loss for the chosen x is given by,

E[Lm | X = x] =

1
m

m
(cid:88)

i=1

eiP (Yi = 1 | X = x) ≥

1
m

(cid:88)

i:t(i)∈I

eiP (Yi = 1 | X = x) ≥ (1 − δ)em(I).

Since I ∈ Jm, inf m>m0 (em(I) − q¯em) > 0 as a consequence of Lemma C.2. As a result, E[Lm | X =
x] ≥ q¯em + κ, for some κ > 0. Given a realization of X, Lm = (cid:80)m
i=1 eiYi is the sum of m independent
random variables. So for any ε > 0 and x ∈ Bm,

P (Lm ≥ q¯em | X = x) ≥ P (Lm ≥ E[Lm | X = x] − κ | X = x) ≥ 1 − ε,

for all m suﬃciently large due to concentration properties of independent sums (see, for example, Can-
telli’s inequality). Therefore,

P (Em ∩ Bm) =

(cid:90)

x∈Bm

P (Em | X = x)dF (x) ≥ inf

x∈Bm

P (Em | X = x)P (Bm) ≥ (1 − ε)P (Bm).

To conclude the proof, notice that P (Em ∩ Bm) ≤ P (Em) ≤ P (Am) + o(P (Am)), where Am := {Lcr(x) ≥
cmη(1 − εm)} and deﬁne Bm := {x : Lcr(x) ≥ γ(1 + δm)}. Since εm, δm → 0 and γ = cmη(1 + o(1)),
log P (Bm) = log P (Am)(1 + o(1)) as m → ∞. Combining this with the observation in (C.7), we obtain
the following from Λmin ∈ RV(α∗) :

log P (Em) = −Λmin(c1/ρmη/ρ)(Icr + o(1)) = cα∗/ρΛmin(mη/ρ)(Icr + o(1)).

(cid:3)

Proof of Proposition 6.3. First, we write the second moment of the IS estimator as (see (Glasserman
et al. 2008, Appendix, Pg. 1) for details on how to arrive at the ﬁrst expression below),

M2,m = E

(cid:20)
exp {−mLmλm(X) + mψm(X, λm(X))}

fX (X)
fZ(X)

(cid:21)
I(Em)

≤ E

(cid:124)

(cid:20) fX (X)
fZ(X)
(cid:123)(cid:122)
I1,m

(cid:21)
I(Am)

(cid:20)
exp {−mqλm(X)¯em + mψm(X, λm(X))}

+ E

(cid:125)

(cid:124)

(cid:123)(cid:122)
I2,m

fX (X)
fZ(X)

I(Em \ Am)

(cid:21)
,

(cid:125)

where we use the non-positivity of the term in the exponent and drop I(Em) in I(Em ∩ Am) to arrive at
the term labelled I1,m. Note that I1,m equals the second moment of the IS estimator in Algorithm 1 when
used for the problem of estimating P (Am). Here Am = {Lcr(X) > um} and um := cmη(1 − εm) → ∞
as m → ∞. Recall the speciﬁc choice u = cmη employed in Algorithm 2; in addition, l is taken to be
slowly varying in m. Due to the continuous convergence in (C.6) and these choices of l and u, all the
requirements in Theorem 5.2 stand satisﬁed under the assumptions stated in Proposition 6.3. Therefore,
log I1,m
log P (Am)2 = 1,

lim
m→∞

(C.10)

due to Theorem 5.2. To bound I2,m, we obtain the following from the steps leading to (C.8):

I2,m ≤ exp

(cid:110)

−mγεme−1
0

(q¯em − e∗

(cid:111)
m)+ + exp(−0.5γεm)

(cid:20)

E

fX (X)
fX (T −1(X))

J(T −1(X))I(Em \ Am)

(cid:21)

.

(C.11)

Since u = cmη(1+o(1)) and l is slowly varying in u, we have (u/l)1/ρ ≤ (1+δ)mη/ρ(1+δ) as a consequence
of Potter’s bounds (de Haan & Ferreira 2010, Proposition B.1.9(7)). Then from the uniform bound for
the Jacobian J(·) in (A.18), we have J(T −1(x)) ≤ mK, for some suitably large constant K ∈ (0, ∞).
Since fX (·) is bounded below on compact subsets of Rd
fX (T −1(x)) ≤ M , for some M large

+, we have

fX (x)

EFFICIENCY IN BLACK-BOX IMPORTANCE SAMPLING

17

enough. Combining these observations with (C.11),
m exp(−0.5γεm) − mγεme−1
0

lim sup
m→∞

log I2,m
Λmin(mη/ρ)

≤

(q¯em − e∗

m)+ + log M + K log m + log P (Em \ Am)

Λmin(mη/ρ)

≤

2m exp(−0.5γεm) − 2mγεme−1
0

(q¯em − e∗

m)+ + log M + K log m

Λmin(mη/ρ)

= −∞,

where the last inequality follows from (C.9). Together with (C.10) and the asymptotic for log P (Em) in
the statement of Theorem 6.2, we thus arrive at

lim inf
m→∞

log M2,m
log P (Em)2 ≥ 1.

Since the second moment M2,m also satisﬁes M2,m ≥ P (Em)2, we obtain the desired limit.

(cid:3)

Appendix D. Proofs of Technical Results

Proof of Lemma 3.1. Since Yi := Λi(Xi) and Λi(xi) := log(1 − F (xi)), we obtain P (Yi ≥ y) =
(cid:3)
P (Λi(Xi) ≥ y) = P (Fi(Xi) ≥ 1 − e−y) = e−y.
Proof of Lemma 3.4. a) Observe that I(·) is the limit of continuously converging functions n−1ϕ(nx).
Therefore, from (Rockafellar & Wets 1998, Theorem 7.14), I(·) is continuous. Parts b), c), d) of the
lemma statement follow directly as a consequence of Theorem 3.3 and (de Valk 2016, Proposition 3). (cid:3)

D.1. Proof of self-similarity in Proposition 5.1. Proposition 5.1 oﬀers additional understanding on
the map T and is not utilized towards establishing the main results on variance reduction. Lemma D.1
below serves as a stepping stone in verifying Proposition 5.1. Recall the deﬁnition ˜t(u) := Λmin(l(u)1/ρ)
in Section 5.

Lemma D.1. Suppose that Assumption 2 is satisﬁed and l(u)/u → γ, for some γ ∈ (0, 1), as u → ∞.
Then we have the following uniform convergence over compact subsets of Rd

+ \ {0} :

ψu

(cid:0)t(u)p(cid:1)
˜t(u)

= p(1 + o(1)), as u → ∞.

(D.1)

Proof. Consider sequences {un}n≥1, {pn}n≥1 ⊂ Rd
+ \ {0} as
n → ∞. Case 1: Suppose p > 0. Since cρ(u) := [l(u)/u]1/ρ → γ1/ρ and q ∈ RV(1/α), we have from the
deﬁnition of κ(·) in (6) that limn→∞ κ(q(t(un)pn)) = α∗/(ρα); here α∗ := mini αi. From (5),
(cid:20) un
l(un)

+ such that un → ∞ and pn → p ∈ Rd

= q(t(un)pn)(1 + o(1)), as n → ∞.

ρα (cid:1) = q(t(un)pn)γ

T (cid:0)q(t(un)pn)γ

ρα (1+o(1))

(cid:21) α∗

α∗
αρ

α∗

Then due to the continuity of T −1, we have T −1(cid:0)q(t(un)pn)(cid:1) = q(t(un)pn)γ
RV(α), q = Λ←, and t(u) = Λmin(u1/ρ), we obtain uniformly over p in compact subsets of Rd

α∗
ρα (1 + o(1)). Since Λ ∈

++,

ψu

(cid:1)

(cid:0)t(un)pn
˜t(un)

:=

Λ ◦ T −1 ◦ q(cid:0)t(un)pn

Λmin

(cid:0)l(un)1/ρ(cid:1)
Λmin(u1/ρ
n )
(cid:0)u1/ρ
α∗
ρ Λmin

n

= γ

α∗
ρ p

γ

(cid:1)

=

Λ(cid:0)q(t(un)pn)γ

α∗

ρα (1 + o(1))(cid:1)

Λ(cid:0)q(t(un))(cid:1)

Λ ◦ q(t(un))
(cid:0)(γun)1/ρ(1 + o(1))(cid:1)

Λmin

(1 + o(1)) = p(1 + o(1)).

(cid:1)

(D.2)

due to the uniform convergence in (de Haan & Ferreira 2010, Proposition B.1.4).
Case 2: Suppose p = (p1, . . . , pd) is such that the subset of indices I := {i : pi = 0} is non-empty.
Since p (cid:54)= 0, I is a strict subset of {1, . . . , d}. Since pi = 0 for i ∈ I, we have qi(t(un)pn) = o(qi(t(un))).
Following the same reasoning as in Case 1, we arrive at T −1

(cid:0)q(t(un)pn)(cid:1) = o(qi(t(un)pn)) and

ψu,i

(cid:1)

(cid:0)t(un)pn
˜t(un)

Λi

=

(cid:0)o(qi(t(un)pn))(cid:1)
(cid:0)qi(t(un))(cid:1)
Λi

γ

i
Λmin(u1/ρ
n )
ρ Λmin(u1/ρ
n )

α∗

(1 + o(1)) → 0,

18

EFFICIENCY IN BLACK-BOX IMPORTANCE SAMPLING

as a consequence for i ∈ I. Since pi = 0 for i ∈ I, (D.1) holds for the i-th component, when i ∈ I. For
(cid:3)
i /∈ I, we have ψu,i(t(un)pn) = γα∗/ρpi(1 + o(1)), exactly following the reasoning in Case 1.
Proof of Proposition 5.1. 1) Veriﬁcation of (20a). Recall from Lemma A.1 that the events
{L(X) ≥ u} and {Yu ∈ lev+
1 (Lu) denote the conditional
realization. Since Yu := t(u)−1Λ(X) and q = Λ←, we have

1 (Lu)} coincide. Let ξzv

u := Yu | Yu ∈ lev+

and

u is f zv

u ) = lev+

Law(cid:0)q(t(u)ξzv

P (cid:0)L(X) ≥ u(cid:1) = P (cid:0)Yu ∈ lev+

1 (Lu)(cid:1)
1 (Lu), the pdf of ξzv

u )(cid:1) = Law(cid:0)X | L(X) ≥ u(cid:1).
For any p ∈ supp(ξzv
u (p) := t(u)dfY (t(u)p)/P (L(X) ≥ u). Since
the support avoids the origin for large values of u (see A.10), (A.4) and the tail asymptotic in Theo-
rem 4.1 yield (20a). An application of Corollary A.3 and Lemma A.4 imply that for all large enough u,
1 (fLD) I(p) − ε. Therefore for all large enough u, I(p) > I ∗ − ε for all p in
1 (Lu) I(p) ≥ inf p∈lev+
inf p∈lev+
the support of ξzv
u .
2) Veriﬁcation of (20b). Recall ψu := Λ ◦ T −1 ◦ q and Lu(·) from (A.1a). The map ψu is invertible
almost everywhere, thanks to the invertibility of T , Λ and q := Λ←. Speciﬁcally, ψ−1
u = Λ◦T ◦q satisﬁes
ψu ◦ ψ−1

u = Id almost everywhere. For notational ease, let

φu(z) := t(u)−1ψ−1

u (t(u)z)

and

ξu := φu(Yu)

(cid:12)
(cid:12) φu(Yu) ∈ lev+
(cid:12)

1 (Lu)

denote the conditional realization. As Z := T (X) = T (q(t(u)Yu)), we then have

P (cid:0)L(Z) ≥ u(cid:1) = P (cid:0)φu(Yu) ∈ lev+
Here supp(ξu) = {φu(z) : Lu(φu(z)) ≥ 1} = lev+

1 (Lu)(cid:1)

f is
u (p) =

1
Jφu (φ−1
u (p))

fY u (φ−1
u (p))
P (cid:0)φu(Yu) ∈ lev+

1 (Lu)(cid:1) =

and

Law (q(t(u)ξu)) = Law (Z | L(Z) ≥ u)) .

1 (Lu). Due to change of variables, the pdf of ξu is
1 (Lu)(cid:1) , p ∈ lev+

t(u)dfY (t(u)φ−1
u (p))
u (p))P (cid:0)φu(Yu) ∈ lev+

Jφu (φ−1

1 (Lu),

where Jφu (·) is the Jacobian of the transformation φ(·). Following the same Jacobian derivation in the
proof of Lemma A.7, we have

Jφu (φ−1

u (p)) = t(u)d

d
(cid:89)

i=1

λi(qi(t(u)pi))

λi(T −1

i

(q(t(u)p)))

J(T −1(q(t(u)p))),

where J(·) is the Jacobian of the transformation T identiﬁed in (9a). For any ε > 0, we obtain following
the proof of Lemma A.9 that log Jφu (φ−1
1 (Lu) avoids
the origin (see Lemma A.10), there exists δ1 > 0 such that (cid:107)q(t(u)p)(cid:107)∞ ≥ δ1u1/ρ for all z ∈ supp(ξu).
Therefore (cid:107)t(u)φ−1
u (p)(cid:107)∞ = (cid:107)Λ ◦ T −1(q(t(u)p))(cid:107)∞ → ∞, as u → ∞. Then using (A.17),
(cid:0)t(u)φ−1
u (p)
t(u)
u (p)(cid:107)∞

u (p)) = o(u(cid:15)), as u → ∞. Since supp(ξu) = lev+

u (p)(cid:1)(1 + o(1)),

(1 + o(1)) = I(cid:0)φ−1

(cid:18) φ−1
(cid:107)φ−1

u (p)(cid:107)∞I

u (p)(cid:1)

= (cid:107)φ−1

log fY

−

(cid:19)

due to the homogeneity of I(·) in Lemma 3.4b. Since ˜t(u) := Λmin(l(u)1/ρ) = γ
combine all the above observations to obtain,

α∗
ρ t(u)(1 + o(1)), we

−

u (p)

log f is
˜t(u)

∼ γ− α∗

= γ− α∗

(cid:34)

ρ

−

−d

log fY

u (p)(cid:1)

(cid:0)t(u)φ−1
log t(u)
t(u)
t(u)
u (p)(cid:1) + t(u)−1 log P (cid:0)φu(Yu) ∈ lev+
u (p) = t(u)−1ψu(t(u)p), we obtain from (D.1) that,

ρ (cid:2)I(cid:0)φ−1

log Jφu (φ−1

t(u)

+

u (p))

log P (cid:0)φu(Yu) ∈ lev+

1 (Lu)(cid:1)

(cid:35)

t(u)

+

1 (Lu)(cid:1) + o(1)(cid:3) (1 + o(1)),

(D.3)

as u → ∞. As φ−1

φ−1

u (p) =

˜t(u)
t(u)

ψu(t(u)p)
˜t(u)

= γ

α∗
ρ p(1 + o(1))

and φu(p) = γ− α∗

ρ p(1 + o(1)),

(D.4)

uniformly over compact subsets of Rd

+ \ {0}.

For simplifying the term involving P (cid:0)φu(Yu) ∈ lev+

1 (Lu)(cid:1), we observe the following from the tail LDP
for {Yu}u>0 in Theorem 3.3 and the contraction principle involving converging sequence of functions in

EFFICIENCY IN BLACK-BOX IMPORTANCE SAMPLING

19

(Dembo & Zeitouni 1998, Theorem 4.2.23): As u → ∞, the family {φu(Yu) : u > 0} satisﬁes an LDP
with rate function

I (cid:48)(y) = inf (cid:8)I(x) : y = γ− α∗

ρ x(cid:9) = γ

α∗
ρ I(y),

due to the homogeneity of I(·). Recall that Theorem 4.1 derives a tail asymptotic for log P (L(X) ≥ u) =
log P (Yu ∈ lev+
1 (Lu)). Equipped with the above LDP for φu(Yu), one can repeat the steps in the proof
of Theorem 4.1 to similarly arrive at

t(u)−1 log P (L(Z) ≥ u) = t(u)−1 log P (cid:0)φu(Yu) ∈ lev+

1 (Lu)(cid:1) → −

inf
y∈lev+
1 (fLD)

I (cid:48)(y) = −γ

α∗
ρ I ∗.

Substituting this observation and (D.4) in (D.3), we have from the homogeneity of I(·),

−˜t(u)−1 log f is

u (p) = γ− α∗

ρ (cid:2)I(cid:0)γ

α∗

ρ p(cid:1) − γ

α∗

ρ I ∗(cid:3) = I(p) − I ∗ + o(1).

(cid:3)

1 (Gu) ∩ BM ⊂ [lev+

D.2. Proof of Lemma A.11. Step 1- Develop an asymptotic upper bound for ψu(t(u)p): Deﬁne
the function Gu(x) = u−1L(xu1/ρ). Then, under Assumption 1, lev+
1 (L∗) ∩ BM ]1+ε
for all large enough u. Notice that from the continuity and ρ-homogeneity of the limit L∗, the 1-
level set of L∗ is disjoint from Bδ1−ε(0), for some small enough δ1, ε. Thus, for all large enough u,
Gu(x) ≥ 1 =⇒ (cid:107)x(cid:107)∞ ≥ δ1 for some δ1 > 0. Write
(cid:18) q(t(u)p)
u1/ρ

(cid:18) q(t(u)p)
u1/ρ
Then for all large enough u, lev+
1 (Lu) ⊆ {p : (cid:107)q(t(u)p)(cid:107)∞ ≥ δ1u1/ρ}. Since (u/l) = o(u), for all
large enough u, (cid:107)q(t(u)p)(cid:107)∞ > 1/cρ(u). Recall that from Lemma A.5, whenever (cid:107)y(cid:107)∞ ≥ 1/cρ(u),
T −1(y) ≤ y[cρ(u)]
log (cid:107)y(cid:107)∞ ∨1. Therefore for all u > u0 where u0 is suﬃciently large, T −1(q((t(u)p)) ≤
exp(log q(t(u)p)ru(p))∨1, where

Lu(p) = u−1L

= Gu

u1/ρ

(cid:19)

(cid:19)

log y

.

(cid:18)

ru(p) =

1 +

log cρ(u)
log (cid:107)q(t(u)p)(cid:107)∞

(cid:19)

∈ (0, 1), since (cid:107)q(t(u)p)(cid:107)∞ > 1/cρ(u).

From the monotonicity of Λ, it follows that ψu(t(u)p) ≤ Λ (cid:0)elog q(t(u)p)ru(p)(cid:1) ∨ Λ(1).
Step 2 - Derive partial upper bounds: To this end, for k ∈ [d], deﬁne the set

Ek,u =

(cid:110)

p : Λk

(cid:16)

elog qk(t(u)p)ru(p)(cid:17)

≥ Λj

(cid:16)

elog qj (t(u)p)ru(p)(cid:17)

(cid:111)

, ∀j (cid:54)= k

∩ lev+

1 (Lu).

This is the set of all p, such that the kth component of Λ(elog q(t(u)p)ru(p)) is the largest, and therefore
achieves the maximum in (cid:107)Λ (cid:0)elog q(t(u)p)ru(p)(cid:1) (cid:107)∞. Therefore, ∪kEk,u = lev+
1 (Lu) (since the maximum
(cid:0)elog qk(t(u)p)ru(p)(cid:1) ∨
in the (cid:107) · (cid:107)∞ norm is achieved for some k ∈ [d]). For p ∈ Ek,u, (cid:107)ψu(t(u)p)(cid:107)∞ ≤ Λk
maxk Λk(1). By deﬁnition, over Ek,u, for all j ∈ [d],
(cid:16)
elog qk(t(u)pk)ru(p)(cid:17)

(cid:16)
elog qj (t(u)pj )ru(p)(cid:17)

(D.5)

≥ Λj

Λk

Step 3 - Establish a lower bound for elog qk(t(u)pk)ru(p): Given p, let i(p) be the index which
achieves the maximum in q(t(u)p) (if there are multiple, select one arbitrarily). Then, we have that
qi(p)(t(u)pi(p)) ≥ δ1u1/ρ. Recall that cρ(u) = (l/u)1/ρ. Therefore,

qi(p)(t(u)pi(p))c

log qi(p)(t(u)pi(p) )
log (cid:107)q(t(u)p)(cid:107)∞

ρ

(u) = qi(p)(t(u)pi(p))cρ(u) ≥ δ1l1/ρ.

Let κ be such that κ maxj αj < minj αj. Then, notice that for all k

minj αj
αk

− κ > 0.

(D.6)

Since Λ ∈ RV(α), an application of (de Haan & Ferreira 2010, Proposition B.1.9 (1)) shows that for
all κ > 0, there exists an xj such that for all x > xj, Λ−1
k (Λj(x)) ≥ xαj /αk−κ. Further since l → ∞
as u → ∞, there exists a u2,k, such that for all u > u2,k, for κ selected as in (D.6), δ1l1/ρ > maxj xj.
Therefore, for all u > u2,k, Λ−1
k (Λj(δ1l1/ρ)) ≥ (δ1l1/ρ)αj /αk−κ for all j ∈ [d]. Now, for all p ∈ Ek,u, for

20

EFFICIENCY IN BLACK-BOX IMPORTANCE SAMPLING

all u > u0 ∨ u1 ∨ u2,k,
elog qk(t(u)p)ru(p)(cid:17)
(cid:16)

≥ Λ−1

k (Λi(p)(qi(p)(t(u)pi(p)))) since p ∈ Ek,u

≥ Λ−1

k (Λi(p)(δ1l1/ρ)) ≥ (δ1l1/ρ)

minj αj
αk

−κ by choice u > max{u1, u2,k}.

(D.7)

Notice that since l → ∞ as u → ∞, the RHS above can be made arbitrarily large by appropriate choice
of u.

Step 4 - Evaluate and combine partial bounds: Fix δ > 0. Observe that due to the monotonicity of
(cid:0)Λk
(cid:0)elog qk(t(u)pk))ru(p)(cid:1)(cid:1) ≥ exp(log qj(t(u)pj)ru(p)).
Λi, (D.5) implies that for all p ∈ Ek,u, for all j, Λ−1
Next, observe that from an application of (de Haan & Ferreira 2010, Proposition B.1.9 (1)), Λ−1
j (Λk(x)) ≤
minj αj
−κ >
x
αk
maxj xj,k. Therefore, for u > u0 ∨ u1 ∨ u2,k ∨ u3,k, inf p∈Ek,u exp(log(qk(t(u)pk)ru(p)) ≥ maxj xj,k. Then,
(cid:16)

αk (1+δ)
αj (1−δ) whenever x ≥ xj,k. Now choose u large enough (say u > u3,k) so that (δ1l1/ρ)

(cid:19)

(cid:16)

j

elog qk(t(u)pk))ru(p)(cid:17)(cid:17)

, for all j ∈ [d].

log qk(t(u)pk)ru(p)

≥ Λ−1
j

Λk

exp

(cid:18) αk(1 + δ)
αj(1 − δ)

Thus, for all j,

(cid:18)

exp

log qk(t(u)pk)ru(p) −

αj(1 − δ)
αk(1 + δ)

log qj(t(u)pj)ru(p)

≥ 1.

(cid:19)

With ru(p) > 0, this requires that for all j,

log qk(t(u)pk)
log qj(t(u)pj)

≥

αj(1 − δ)
αk(1 + δ)

.

Upon selecting the j which achieves the maximum in (cid:107)q(t(u)p)(cid:107)∞, notice that

log qk(t(u)pk)
log (cid:107)q(t(u)p)(cid:107)∞

≥

min αj(1 − δ)
αk(1 + δ)

and therefore, since cρ(u) = o(1),

qk(t(u)pk)[cρ(u)]

log qk (t(u)pk )
log (cid:107)q(t(u)p)(cid:107)∞ ≤ qk(t(u)pk)[cρ(u)]

minj αj (1−δ)

αk (1+δ) ≤ qk(t(u)pk)ε1/αk where ε is arbitrary.

Therefore, Λk(eqk(t(u)pk)ru(p)) ≤ Λk(qk(t(u)pk)ε1/αk ) ≤ εt(u)pk. Now, whenever u > uk, for p ∈ Ek,u,
(cid:107)ψu(t(u)p)(cid:107)∞ ≤ εt(u)pk ≤ εt(u)(cid:107)p(cid:107)∞, uniformly over p ∈ Ek,u. By symmetry, for u > maxk(u0 ∨ u1 ∨
u2,k ∨ u3,k), uniformly over p ∈ lev+
1 (Lu), (cid:107)ψu(t(u)p)(cid:107)∞ ≤ εt(u)(cid:107)p(cid:107)∞.
Step 5 - Establish non-negativity: With the bound on (cid:107)ψu(t(u)p)(cid:107)∞ established, from (A.17) and
Lemma A.9 respectively, for ε > 0 for all large enough u,

au(p) ≥ t(u)(cid:107)p(cid:107)∞(1 − ε),

log t(u)
t(u)

≤ ε

and

bu(p) ≥ −εt(u).

From Lemma A.10, p ∈ lev+
enough u. Finally, notice that χlev+

1 (Lu) =⇒ (cid:107)p(cid:107)∞ > γ. Thus, Fu(p) ≥ 0 over p ∈ lev+

1 (Lu)(p) = ∞, for p (cid:54)∈ lev+

1 (Lu). This completes the proof.

1 (Lu) for all large
(cid:3)

D.3. Proof of log-eﬃciency in the presence of heavier tails. Recall that L is the collection of
indices of components (X1, . . . , Xd) which satisfy the lighter tailed assumption in Assumption 2. Then
H := {1, . . . , d} \ L denotes the heavy-tailed components.
Proof of Theorem 7.1. Under Assumption 6, ¯Λi ∈ RV(αi) for i ∈ H. Its respective inverse is,

¯qi := ¯Λ←

i = log qi ∈ RV(1/αi),

due to (de Haan & Ferreira 2010, Proposition B.1.9(9)). Let ¯q(y) := (¯q1(y1), . . . qd(yd)). Deﬁne the
following counterparts to quantities Lu, fLD, t(u) and q∞(t) used in the proof of Theorem 4.1:

¯Lu(x) :=

log L(e¯q(t(u)x))
log u

and

¯fLD(x) := ¯L∗(q∗x1/α),

where t(u) := min

i

¯Λi(log u) = Λmin(u),

and

¯q∞(t) := max
i=1,...,d

¯qi(t).

(D.8a)

(D.8b)

EFFICIENCY IN BLACK-BOX IMPORTANCE SAMPLING

21

∞ = mini ¯Λi, we have ¯q∞(t(u)) = log u (see Lemma A.1 and (A.5)). Letting Yu := t(u)−1Y =

Since ¯q←
t(u)−1Λ(X), we have the following equivalence of events,

{L(X) > u} = (cid:8)Yu ∈ lev+

1 ( ¯Lu)(cid:9) ,

from the deﬁnition of ¯Lu and injectivity of ¯q = ¯Λ←.

As before, we proceed by showing continuous convergence of Lu to ¯fLD, as u → ∞. For this purpose,
++ such that un → ∞, xn → x > 0. Since ¯q∞(t(u)) = log u,

consider sequences {un}n≥1 ⊂ R+, {xn} ⊂ Rd
¯qi ∈ RV(1/αi) for i ∈ H, and ¯qi ∈ RV(0), ˆqi∗ = 0 for i ∈ L,

¯q(t(un)xn)
log un

=

¯q(t(un)xn)
¯q(t(un)1)

ˆq(t(un)) → x1/αq∗

(D.9)

from (30) and (de Haan & Ferreira 2010, Proposition B.1.9(4)), as n → ∞. Consequently,
log L (cid:0)exp{q∗x1/α log un(1 + o(1))}(cid:1)
log un

log L (exp{¯q(t(un)xn)})
log un

¯Lun(xn) =

=

,

uniformly over x in compact subsets. Letting e(n, x) := enx/(cid:107)enx(cid:107)∞ be the unit vector, we have

L(enxn ) = L((cid:107)enxn (cid:107)∞e(n, xn)) = (cid:107)enx(cid:107)ρ

∞L∗(e(n, x))(1 + o(1)) = L∗(enx)(1 + o(1)),

where the second equality follows from the compact convergence of L(·) in Assumption 1 and the last
equality is due to the homogeneity L∗(cx) = cρL∗(x). Then from Assumption 7,
q∗x1/α(cid:17)

log L∗ (cid:0)exp{q∗x1/α log un}(cid:1) (1 + o(1))
log un

¯Lun (xn) =

=: ¯fLD(x).

→ ¯L∗ (cid:16)

Then as a consequence of the continuous convergence Lu → ¯fLD above, we obtain the following from
exactly the same reasoning in the proofs of Lemma A.2 and Corollary A.3: given ε, M > 0, there exists
u0 large enough such that for all u > u0,

lev+

1 ( ¯Lu) ∩ BM ⊆ (cid:2)Ξ1,M ( ¯fLD)(cid:3)1+ε

, and

inf
n:un>u

χlev+

1 ( ¯Lun )(xn) ≥ χlev+

1 ( ¯fLD)(x),

(D.10)

for any xn → x and un → ∞. Thanks to these set inclusions, the conclusion in Theorem 7.1 follows by
repeating the steps in the proof of Theorem 4.1 with Lu, fLD replaced by ¯Lu, ¯fLD.
(cid:3)

To analyse the variance of the IS estimator, we have the following analogue to Lemma D.1.

Lemma D.2. Suppose that Assumption 6 holds, the parameter l in (5) is taken to be slowly varying in
u, and ρ = 1 in (6). Then uniformly over compact subsets of Rd

+ \ {0},

ψu

(cid:0)t(u)p(cid:1)
t(u)

(cid:18)

= p1H

1 −

(cid:19)α

1
(cid:107)q∗p1/α(cid:107)∞

(1 + o(1)), as u → ∞,

(D.11)

where the vector 1H is the indicator vector (for the heavy-tailed components) deﬁned as in (B.4).

Proof. With ρ = 1, we have cρ(u) = (l(u)/u). For x ∈ Rd
−1

+, T (x) ≤ ˜T (x) := (1 + x)[cρ(u)]−κ(x),
component-wise. Note that ˜T
(x) = x[cρ(u)]κ(x−1) − 1.
This yields T −1(x) ≥ (x[cρ(u)]κ(x−1) − 1)+. Combining this with the upper bound in (A.12), we arrive
at the following: For any p ∈ Rd

+, δ > 0 there exists u0 large enough such that for all u > u0,

◦ ˜T (x) = x for x ∈ Rd

+ when we take ˜T

−1

[exp {¯q(t(u)p)ru(p)} − 1]+ ≤ T −1(q(t(u)p)) ≤ exp {¯q(t(u)p)ru(p)}

p ∈ Bδ(p),

where ru(p) := 1 +

log cρ(u)
(cid:107)¯q(t(u)p)(cid:107)∞

= 1 −

1 + o(1)
(cid:107)p1/αq∗(cid:107)∞

[due to (D.9)].

Since ψu := Λ ◦ T −1 ◦ q and Λ is increasing component wise,

Λ(cid:0) [exp {¯q(t(u)p)ru(p)} − 1]+ (cid:1) ≤ ψu(t(u)p) ≤ Λ(cid:0) exp {¯q(t(u)p)ru(p)} (cid:1).

(D.12)

22

EFFICIENCY IN BLACK-BOX IMPORTANCE SAMPLING

As Λi ◦ exp = ¯Λi ∈ RV(αi) for i ∈ H and ¯qi := ¯Λ←

i , the term

Λi

(cid:0) exp {¯qi(t(u)pi)ru(pi)} (cid:1) = ru(pi)αi ¯Λi ◦ ¯qi(t(u)pi)(1 + o(1)) = rαi

u (pi)t(u)pi(1 + o(1)),

for i ∈ H,

On the other hand when i ∈ L, we have Λi ∈ RV(αi) and qi ∈ RV(1/αi). In this case,

(cid:0) exp {¯qi(t(u)pi)ru(pi)} (cid:1) = Λi

Λi

qi(t(u)p)ru(pi)(cid:17)
(cid:16)

= O(t(u)1−(cid:107)p1/αq∗(cid:107)−1

∞ +o(1)).

for i ∈ L

Due to the above deduction that ru(p) = 1 − (cid:107)p1/αq∗(cid:107)−1
uniformly over compact subsets, (D.12) results in,

∞ (1 + o(1)). Since the above convergences

lim
u→∞

t(u)−1ψu,i(t(u)p) =

(cid:0)1 − (cid:107)p1/αq∗(cid:107)−1

∞

(cid:1)αi

(cid:40)

pi
0

for i ∈ H,

for i ∈ L.

(cid:3)

Proof of Theorem 7.2: Following the reasoning in the proof of Theorem 7.2, notice that the second
moment of the IS estimator may be written as M2,u = t2d(u)E (cid:2)exp (cid:8)−t(u) ¯Fu( ¯Y u)(cid:9)(cid:3), where

¯Fu(p) = au(p) + bu(p) + χlev+

1 ( ¯Lu)(p).

(D.13)

(cid:16)

Here, au(p) and bu(p) are as deﬁned in Lemma A.7. Following Lemma D.2 and the proof of Lemma A.8,
++.
we obtain au(p) ≥ I(p) − I
We have from Assumption 6 that for i ∈ H, Λ ◦ exp ∈ RV(αi), for some αi ≥ 1. Therefore, Λi ∈ RV(0)
whenever i (cid:54)∈ L. Now, since λi(·) are monotone, (de Haan & Ferreira 2010, Proposition B.1.9 (7)) implies
that λi ∈ RV(γi − 1). Here, γi = 0 if i ∈ H. Following the steps in the proof of Lemma A.8, bound the
product (for large enough u) in (b) as

+ o(1). uniformly over compact subsets of Rd

(cid:0)1 − 1/(cid:107)q∗p1/α(cid:107)(cid:1)α(cid:17)

p1H

d
(cid:89)

i=1

λi(qi(t(u)pi))

λi(T −1

i

(q(t(u)p)))

J(T −1
i

(q(t(u)p))) ≤ exp

log(u/l)

(cid:32)

(cid:34) d

(cid:88)

i=1

γiκi(q(t(u)p)) + o(1)

(D.14)

(cid:35)(cid:33)

Whenever L (cid:54)= [d] and i ∈ L, it is easy to see that ¯q∗
i = 0. For all such i, κi(q(t(u)p)) = o(1). Further,
for all i ∈ H, γi = 0. Finally, observe that with ¯Λi ∈ RV(αi) for i ∈ H, we have log u = O(t(u)). Now,
(D.14) suggests bu(p) ≥ −t(u)ε for all large enough u. Noting the convergences in (D.10), and repeating
the arguments from the proof of Theorem 5.2, replacing Fu(p) there by ¯Fu(p),

lim sup
u→∞

[Λmin(u)]−1 log M2,u ≤ −

inf
1 ( ¯fLD)
p∈lev+

2I(p) + I

(cid:16)

p1H

(cid:16)

1 − 1/(cid:107)q∗ · p1/α(cid:107)∞

(cid:17)α(cid:17)

+ 2ε.

Due to the homogeneity of I(·) (see Lemma 3.4(b)), it can be seen that the above inﬁmum occurs at the
boundary, (cid:107)q∗ · p1/α(cid:107)∞ = 1, and therefore, lim supu→∞
Veriﬁcation of Remark 4.2: For any f, g ∈ RV(p) that are eventually strictly increasing and satisfying
limx→∞ f (x)/g(x) = c ∈ (0, ∞), we ﬁrst show that limx→∞ g←(x)/f ←(x) = c1/p. For this purpose,
observe limt→∞ g←(tx)/g←(t) = x1/p uniformly over x ∈ (c/2, 2c) as t → ∞. Setting t = g(f ←(x)), we
have t → ∞ and f (f ←(x))/g(f ←(x)) → c as x → ∞. Therefore,
(cid:19)

Λmin(u) log M2,u ≤ −2I ∗ + 2ε.

(cid:3)

(cid:18)

1

g←(x) = g←

g(f ←(x)) ·

∼ c1/pf ←(x).

f (f ←(x))
g(f ←(x))

This veriﬁes the claim g←(x)/f ←(x) → c1/p. To see (17) as a consequence, ﬁx any i ∈ {1, . . . , d} such
i (x) > 0. Setting f = qi and g(·) = (cid:107)q(·)(cid:107)∞, we have f ← = Λi, g← = Λmin (see (A.5)).
that q∗
i exists and q∗
Since Λmin ∈ RV(α∗), q∗
i = 0, the conclusion is immediate from the
diﬀering rates of growths of the numerator and the denominator. Finally to verify the suﬃcient condition
on the derivative, consider any sequence {xn} ⊂ R increasing to inﬁnity. Since |r(cid:48)
i(x)| ≤ M x−(1+ε) for
suitable constants M, ε > 0,

i = (limx→∞ Λmin(x)/Λi(x))1/α∗ . If q∗

|ri(xm+n) − ri(xm)| ≤

(cid:90) xm+n

xm

|r(cid:48)

i(x)|dx ≤ ε−1M x−ε
m ,

for all suﬃciently large m. Therefore the sequence {ri(xn) : n ≥ 1} is Cauchy and is convergent.

EFFICIENCY IN BLACK-BOX IMPORTANCE SAMPLING

23

Appendix E. Illustration with the product distribution network model

For the distribution network model in Section 4.2, we take the demand vector to possess marginal
distributions speciﬁed by ¯Fi(x) = 1 − exp(−xαi), i = 1, . . . , d. The tail parameters α1, . . . , α(cid:98)d/2(cid:99) are set
to equal 0.8 (subexponential tails) while α(cid:98)d/2(cid:99)+1, . . . , αd are set to equal 1.2 (superexponential tails),
thus introducing heterogeneity. The dependence across demands arising in diﬀerent nodes is modeled by
a Gaussian copula in Example 3.2 with correlation matrix whose oﬀ-diagonal entries are 0.05. We report
results for the following two network structures, namely, 1) complete network and 2) cyclic network, each
corresponding to an extreme in terms of interconnectedness of the network. The complete network is a
fully connected network and the non-zero entries in the connectivity matrix A are given by aij = 1/(d−1)
for i (cid:54)= j. The cyclic network, on the other hand, is a sparsely connected network in which the non-zero
entries are given by aij = 1 for j = (i mod d) + 1.

Keeping the threshold for network failure k(u) ﬁxed to equal 1, the variance of the IS estimator
is observed at diﬀerent levels of rarity by varying the total supply u over an interval for which the
respective probability of network failure pu = P (N F u) decreases from roughly 10−4 to 10−9. The nodes
are endowed with identical supplies corresponding to the case where s = u × 1/d. Taking the same
N = 5 × 103 independent demand samples D1, . . . , D5000 in all instances, the probability of network
failure is estimated using the IS estimator ¯ζN (u) = 1
i=1 Li( ˜Di)I(L( ˜Di, s) ≥ 1), where ˜Di is given
N
by the transformation ˜Di = T (Di) in (5); here, for every choice of u considered, the hyper-parameter
l = 4 is chosen as a result of performing cross-validation of the observed estimator variance over choices
of l = c + ln u in the interval c ∈ (−2.5, 5.5).

(cid:80)N

Figure 9 below reports log Vu against log pu, where Vu is the sample variance of the IS estimator and
pu = P (N F u) is the network failure probability. Figure 11 below presents details additionally on cross-
validation by plotting the logarithm of estimator variance, log Vu, observed (in red) for diﬀerent choices of
hyper-parameter l considered. The robust variance reduction (exceeding 99.98%) in the interval l ∈ (2, 8)
demonstrates that the estimator variance is not highly sensitive to the choice of parameter l. Moreover,
the sample variance estimated from N = 5 × 103 (these are kept the same as l is varied) samples closely
approximates the true variance (to within a factor of 2), and therefore, performing the cross-validation
task has small sample requirements. While the naive sample averaging would result in log-variance versus
log pu plot with slope ≈ 1, the proposed IS scheme results in a line with slope ≈ 2 in Figure 9, thus
supporting the asymptotic optimal variance reduction in Theorem 5.2. The observed variance reduction
ranges from 98.5% to 99.995%.

Figure 9. Sample variance of the IS estimator illustrated by plotting log Vu against
log pu. The network with d = 5, 10, 20 nodes are either (A) fully connected or (B) sparsely
connected cyclic network.

(a) Fully Connected Network

(b) Cyclic Network

-4-567-8-9 Log excess probability-6-8-10-12-14-16 Log varianceComplete Networkd=5d=10d=20   -   --4-567-8-9 Log excess probability-6-8-10-12-14-16 Log varianceRing Networkd=5d=10d=20--24

EFFICIENCY IN BLACK-BOX IMPORTANCE SAMPLING

Figure 11. Plot of log Vu against the lower level l. The solid blue and dashed red
curves represent curves ﬁt to the log Vu and its sample estimate using the IS estimator,
respectively. The black +’s represent the values of the estimated variance, while the
green x’s represent the true variance of the estimator

246810Lower level l−15.8−15.6−15.4−15.2−15.0−14.8−14.6Log variance True varianceEstimated variance