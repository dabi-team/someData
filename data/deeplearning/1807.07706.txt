0
2
0
2

b
e
F
7
1

]

G
L
.
s
c
[

5
v
6
0
7
7
0
.
7
0
8
1
:
v
i
X
r
a

Efﬁcient Probabilistic Inference in the Quest for
Physics Beyond the Standard Model

Atılım Güne¸s Baydin,1 Lukas Heinrich,2 Wahid Bhimji,3 Lei Shao,4
Saeid Naderiparizi,5 Andreas Munk,5 Jialin Liu,3 Bradley Gram-Hansen,1 Gilles Louppe6
Lawrence Meadows,4 Philip Torr,1 Victor Lee,4 Prabhat,3 Kyle Cranmer,7 Frank Wood5
1University of Oxford, 2CERN, 3Lawrence Berkeley National Lab, 4Intel Corporation
5University of British Columbia, 6University of Liege, 7New York University

Abstract

We present a novel probabilistic programming framework that couples directly to
existing large-scale simulators through a cross-platform probabilistic execution
protocol, which allows general-purpose inference engines to record and control ran-
dom number draws within simulators in a language-agnostic way. The execution of
existing simulators as probabilistic programs enables highly interpretable posterior
inference in the structured model deﬁned by the simulator code base. We demon-
strate the technique in particle physics, on a scientiﬁcally accurate simulation of
the τ (tau) lepton decay, which is a key ingredient in establishing the properties of
the Higgs boson. Inference efﬁciency is achieved via inference compilation where
a deep recurrent neural network is trained to parameterize proposal distributions
and control the stochastic simulator in a sequential importance sampling scheme,
at a fraction of the computational cost of a Markov chain Monte Carlo baseline.

1

Introduction

Complex simulators are used to express causal generative models of data across a wide segment of the
scientiﬁc community, with applications as diverse as hazard analysis in seismology [50], supernova
shock waves in astrophysics [36], market movements in economics [74], and blood ﬂow in biology
[73]. In these generative models, complex simulators are composed from low-level mechanistic
components. These models are typically non-differentiable and lead to intractable likelihoods, which
renders many traditional statistical inference algorithms irrelevant and motivates a new class of
so-called likelihood-free inference algorithms [49].

There are two broad strategies for this type of likelihood-free inference problem. In the ﬁrst, one uses a
simulator indirectly to train a surrogate model endowed with a likelihood that can be used in traditional
inference algorithms, for example approaches based on conditional density estimation [57, 71, 78, 86]
and density ratio estimation [30, 35]. Alternatively, approximate Bayesian computation (ABC)
[82, 88] refers to a large class of approaches for sampling from the posterior distribution of these
likelihood-free models, where the original simulator is used directly as part of the inference engine.
While variational inference [22] algorithms are often used when the posterior is intractable, they are
not directly applicable when the likelihood of the data generating process is unknown [85].

The class of inference strategies that directly use a simulator avoids the necessity of approximating
the generative model. Moreover, using a domain-speciﬁc simulator offers a natural pathway for
inference algorithms to provide interpretable posterior samples. In this work, we take this approach,
extend previous work in universal probabilistic programming [44, 87] and inference compilation
[64, 66] to large-scale complex simulators, and demonstrate the ability to execute existing simulator
codes under the control of general-purpose inference engines. This is achieved by creating a cross-

33rd Conference on Neural Information Processing Systems (NeurIPS 2019), Vancouver, Canada.

 
 
 
 
 
 
Figure 1: Top left: overall framework where the PPS is controlling the simulator. Bottom left:
probabilistic execution of a single trace. Right: LSTM proposals conditioned on an observation.

platform probabilistic execution protocol (Figure 1, left) through which an inference engine can
control simulators in a language-agnostic way. We implement a range of general-purpose inference
engines from the Markov chain Monte Carlo (MCMC) [25] and importance sampling [34] families.
The execution framework we develop currently has bindings in C++ and Python, which are languages
of choice for many large-scale projects in science and industry. It can also be used by any other
language that support ﬂatbuffers1 pending the implementation of a lightweight front end.

We demonstrate the technique in a particle physics setting, introducing probabilistic programming as
a novel tool to determine the properties of particles at the Large Hadron Collider (LHC) [1, 29] at
CERN. This is achieved by coupling our framework with SHERPA2 [42], a state-of-the-art Monte
Carlo event generator of high-energy reactions of particles, which is commonly used with Geant43
[5], a toolkit for the simulation of the passage of the resulting particles through detectors. In particular,
we perform inference on the details of the decay of a τ (tau) lepton measured by an LHC-like detector
by controlling the SHERPA simulation (with minimal modiﬁcations to the standard software), extract
posterior distributions, and compare to ground truth. To our knowledge this is the ﬁrst time that
universal probabilistic programming has been applied in this domain and at this scale, controlling a
code base of nearly one million lines of code. Our approach is scalable to more complex events and
full detector simulators, paving the way to its use in the discovery of new fundamental physics.

2 Particle Physics and Probabilistic Inference

Our work is motivated by applications in particle physics, which studies elementary particles and their
interactions using high-energy collisions created in particle accelerators such as the LHC at CERN.
In this setting, collision events happen many millions of times per second, creating cascading particle
decays recorded by complex detectors instrumented with millions of electronics channels. These
experiments then seek to ﬁlter the vast volume of (petabyte-scale) resulting data to make discoveries
that shape our understanding of fundamental physics.

The complexity of the underlying physics and of the detectors have, until now, prevented the
community from employing likelihood-free inference techniques for individual collision events.
However, they have developed sophisticated simulator packages such as SHERPA [42], Geant4 [5],
Pythia8 [80], Herwig++ [16], and MadGraph5 [6] to model physical processes and the interactions of
particles with detectors. This is interesting from a probabilistic programming point of view, because

1 https://google.github.io/flatbuffers/
https://sherpa.hepforge.org/

2 Simulation of High-Energy Reactions of Particles.

3 Geometry and Tracking. https://geant4.web.cern.ch/

2

SimulatorProbabilistic programming systemSHERPA (C++)PPL + (Python)......ExecuterequestExecutereplySamplerequestSamplereplyObserverequestObservereplySimulator executionProbprog trace recording and controlSTARTA1A2A3ENDSTARTA1A2A3ENDSamplerequestSamplereplyx1A2xN-1AN0A1LSTMq(x1|y)q(x2|x1,y)q(xN|x1:N-1,y)………3D-CNNyobsysimINFERENCEp(x1)p(x2|x1)p(xN|x1:N-1)…x1x2xNSampled random variables (trace)Prior distributionsor likelihoodsProposaldistributionsProposallayersLSTM inputs:- Observ. embed.- Address embed.- Sample embed.SIMULATIONAddressesA1A2AN……p(y|x1:N)Simulation outputObserved outputthese simulators are essentially very accurate generative models implementing the Standard Model of
particle physics and the passage of particles through matter (i.e., particle detectors). These simulators
are coded in Turing-complete general-purpose programming languages, and performing inference in
such a setting requires using inference techniques developed for universal probabilistic programming
that cannot be handled via more traditional inference approaches that apply to, for example, ﬁnite
probabilistic graphical models [59]. Thus we focus on creating an infrastructure for the interpretation
of existing simulator packages as probabilistic programs, which lays the groundwork for running
inference in scientiﬁcally-accurate probabilistic models using general-purpose inference algorithms.

The τ Lepton Decay. The speciﬁc physics setting we focus on in this paper is the decay of a τ lepton
particle inside an LHC-like detector. This is a real use case in particle physics currently under active
study by LHC physicists [2] and it is also of interest due to its importance to establishing the properties
of the recently discovered Higgs boson [1, 29] through its decay to τ particles [12, 33, 47, 48]. Once
produced, the τ decays to further particles according to certain decay channels. The prior probabilities
of these decays or “branching ratios” are shown in Figure 8 (appendix).

3 Related Work

3.1 Probabilistic Programming

Probabilistic programming languages (PPLs) extend general-purpose programming languages with
constructs to do sampling and conditioning of random variables [87]. PPLs decouple model speciﬁca-
tion from inference: a model is implemented by the user as a regular program in the host programming
language, specifying a model that produces samples from a generative process at each execution. In
other words, the program produces samples from a joint prior distribution p(x, y) = p(y|x)p(x) that
it implicitly deﬁnes, where x and y denote latent and observed random variables, respectively. The
program can then be executed using a variety of general-purpose inference engines available in the
PPL to obtain p(x|y), the posterior distribution of latent variables x conditioned on observed vari-
ables y. Universal PPLs allow the expression of unrestricted probability models in a Turing-complete
fashion [43, 90, 91], in contrast to languages such as Stan [28, 39] that target the more restricted
model class of probabilistic graphical models [59]. Inference engines available in PPLs range from
MCMC-based lightweight Metropolis Hastings (LMH) [90] and random-walk Metropolis Hastings
(RMH) [63] to importance sampling (IS) [11] and sequential Monte Carlo [34]. Modern PPLs such
as Pyro [20] and Edward2 [32, 83, 84] use gradient-based inference engines including variational
inference [53, 58] and Hamiltonian Monte Carlo [54, 70] that beneﬁt from modern deep learning
hardware and automatic differentiation [18] features provided by PyTorch [72] and TensorFlow
[3] libraries. Another way of making use of gradient-based optimization is to combine IS with
deep-learning-based proposals trained with data sampled from the probabilistic program, resulting in
the inference compilation (IC) algorithm [64] that enables amortized inference [40].

3.2 Data Analysis in Particle Physics

Inference for an individual collision event in particle physics is often referred to as reconstruction [62].
Reconstruction algorithms can be seen as a form of structured prediction: from the raw event data they
produce a list of candidate particles together with their types and point-estimates for their momenta.
The variance of these estimators is characterized by comparison to the ground truth values of the latent
variables from simulated events. Bayesian inference on the latent state of an individual collision is
rare in particle physics, given the complexity of the latent structure of the generative model. Until now,
inference for the latent structure of an individual event has only been possible by accepting a drastic
simpliﬁcation of the high-ﬁdelity simulators [4, 7–10, 15, 23, 27, 37, 38, 46, 60, 67, 68, 79, 81]. In
contrast, inference for the fundamental parameters is based on hierarchical models and probed at the
population level. Recently, machine learning techniques have been employed to learn surrogates for
the implicit densities deﬁned by the simulators as a strategy for likelihood-free inference [24].

Currently particle physics simulators are run in forward mode to produce substantial datasets that
often exceed the size of datasets from actual collisions within the experiments. These are then reduced
to considerably lower dimensional datasets of a handful of variables using physics domain knowledge,
which can then be directly compared to collision data. Machine learning and statistical approaches
for classiﬁcation of particle types or regression of particle properties can be trained on these large
pre-generated datasets produced by the high-ﬁdelity simulators developed over many decades [13, 56].

3

The ﬁeld is increasingly employing deep learning techniques allowing these algorithms to process
high-dimensional, low-level data [14, 17, 31, 55, 75]. However, these approaches do not estimate the
posterior of the full latent state nor provide the level of interpretability our probabilistic inference
framework enables by directly tying inference results to the latent process encoded by the simulator.

4 Probabilistic Inference in Large-Scale Simulators

In this section we describe the main components of our probabilistic inference framework, including:
(1) a novel PyTorch-based [72] PPL and associated inference engines in Python, (2) a probabilistic
programming execution protocol that deﬁnes a cross-platform interface for connecting models and
inference engines implemented in different languages and executed in separate processes, (3) a
lighweight C++ front end allowing execution of models written in C++ under the control of our PPL.

4.1 Designing a PPL for Existing Large-Scale Simulators

A shortcoming of the state-of-the-art PPLs is that they are not designed to directly support existing
code bases, requiring one to implement any model from scratch in each speciﬁc PPL. This limitation
rules out their applicability to a very large body of existing models implemented as domain-speciﬁc
simulators in many ﬁelds across academia and industry. A PPL, by deﬁnition, is a programming
language with additional constructs for sampling random values from probability distributions and
conditioning values of random variables via observations [44, 87]. Domain-speciﬁc simulators in
particle physics and other ﬁelds are commonly stochastic in nature, thus they satisfy the behavior
random sampling, albeit generally from simplistic distributions such as the continuous uniform. By
interfacing with these simulators at the level of random number sampling (via capturing calls to the
random number generator) and introducing a construct for conditioning, we can execute existing
stochastic simulators as probabilistic programs. Our work introduces the necessary framework
to do so, and makes these simulators, which commonly represent the most accurate models and
understanding in their corresponding ﬁelds, subject to Bayesian inference using general-purpose
inference engines. In this setting, a simulator is no longer a black box, as all predictions are directly
tied into the fully-interpretable structured model implemented by the simulator code base.
To realize our framework, we implement a universal PPL called pyprob,4 speciﬁcally designed to
execute models written not only in Python but also in other languages. Our PPL currently has two
families of inference engines:5 (1) MCMC of the lightweight Metropolis–Hastings (LMH) [90] and
random-walk Metropolis–Hastings (RMH) [63] varieties, and (2) sequential importance sampling (IS)
[11, 34] with its regular (i.e., sampling from the prior) and inference compilation (IC) [64] varieties.
The IC technique, where a recurrent neural network (NN) is trained in order to provide amortized
inference to guide (control) a probabilistic program conditioning on observed inputs, forms our main
inference method for performing efﬁcient inference in large-scale simulators. Because IC training
and inference uses dynamic reconﬁguration of NN modules [64], we base our PPL on PyTorch
[72], whose automatic differentiation feature with support for dynamic computation graphs [18] has
been crucial in our implementation. The LMH and RMH engines we implement are specialized for
sampling in the space of execution traces of probabilistic programs, and provide a way of sampling
from the true posterior and therefore provide a baseline—at a high computational cost.
A probabilistic program can be expressed as a sequence of random samples (xt, at, it)T
t=1, where
xt, at, and it are respectively the value, address,6 and instance (counter) of a sample, the execution
of which describes a joint probability distribution between latent (unobserved) random variables
x := (xt)T

t=1 and observed random variables y := (yn)N

n=1 given by

p(x, y) :=

T
(cid:89)

t=1

fat (xt|x1:t−1)

N
(cid:89)

n=1

gn(yn|x≺n) ,

(1)

4 https://github.com/pyprob/pyprob
5 The selection of these families was motivated by working with
existing simulators through an execution protocol (Section 4.2) precluding the use of gradient-based inference
6 An “address” is a
engines. We plan to extend this protocol in future work to incorporate differentiability.
label uniquely identifying each sampling or conditioning event in the execution of the program. In our system it
is based on a concatenation of stack frames (Table 1) leading up to the point of each random number draw, and it
also includes a sufﬁx identifying the type of associated probability distribution.

4

where fat(·|x1:t−1) denotes the prior probability distribution of a random variable with address at
conditional on all preceding values x1:t−1, and gn(·|x≺n) is the likelihood density given the sample
values x≺n preceding observation yn. Once a model p(x, y) is expressed as a probabilistic program,
we are interested in performing inference in order to get posterior distributions p(x|y) of latent
variables x conditioned on observed variables y.

Inference engines of the MCMC family, designed to work in the space of probabilistic execution
traces, constitute the gold standard for obtaining samples from the true posterior of a probabilistic
program [63, 87, 90]. Given a current sequence of latents x in the trace space, these work by making
proposals x(cid:48) according to a proposal distribution q(x(cid:48)|x) and deciding whether to move from x to x(cid:48)
based on the Metropolis–Hasting acceptance ratio of the form
p(x(cid:48))q(x|x(cid:48))
p(x)q(x(cid:48)|x))

α = min{1,

(2)

} .

Inference engines in the IS family use a weighted set of samples {(wk, xk)K
empirical approximation of the posterior distribution: ˆp(x|y) = (cid:80)K
where δ is the Dirac delta function. The importance weight for each execution trace is

k=1 wkδ(xk − x)/ (cid:80)K

k=1} to construct an
j=1 wj,

wk =

N
(cid:89)

n=1

gn(yn|xk

1:τk(n))

T k
(cid:89)

t=1

fat(xk
qat,it(xk

t |xk
t |xk

1:t−1)
1:t−1)

,

(3)

where qat,it (·|xk
1:t−1) is known as the proposal distribution and may be identical to the prior fat (as
in regular IS). In the IC technique, we train a recurrent NN to receive the observed values y and
return a set of adapted proposals qat,it(xt|x1:t−1, y) such that the approximate posterior q(x|y) is
close to the true posterior p(x|y). This is achieved by using a Kullback–Leibler divergence training
objective Ep(y) [DKL (p(x|y) || q(x|y; φ))] as
(cid:90)
p(x|y)
q(x|y; φ)

dx dy = Ep(x,y) [− log q(x|y; φ)] + const. ,

p(x|y) log

L(φ) :=

p(y)

(4)

(cid:90)

y

x

where φ represents the NN weights. The weights φ are optimized to minimize this objective by
continually drawing training pairs (x, y) ∼ p(x, y) from the probabilistic program (the simulator).
In IC training, we may designate a subset of all addresses (at, it) to be “controlled” (learned) by the
NN, leaving all remaining addresses to use the prior fat as proposal during inference. Expressed in
simple terms, taking an observation y (an observed event that we would like to recreate or explain
with the simulator) as input, the NN learns to control the random number draws of latents x during
the simulator’s execution in such a way that makes the observed outcome likely (Figure 1, right).

at−1,it−1

The NN architecture in IC is based on a stacked LSTM [52] recurrent core that gets executed for
as many time steps as the probabilistic trace length. The input to this LSTM in each time step is
a concatenation of embeddings of the observation f obs(y), the current address f addr(at, it), and
the previously sampled value f smp
(xt−1). f obs is a NN speciﬁc to the domain (such as a 3D
convolutional NN for volumetric inputs), f smp are feed-forward modules, and f addr are learned
address embeddings optimized via backpropagation for each (at, it) pair encountered in the program
execution. The addressing scheme at is the main link between semantic locations in the probabilistic
program [90] and the inputs to the NN. The address of each sample or observe statement is supplied
over the execution protocol (Section 4.2) at runtime by the process hosting and executing the model.
The joint proposal distribution of the NN q(x|y) is factorized into proposals in each time step qat,it,
whose type depends on the type of the prior fat. In our experiments in this paper (Section 5) the
simulator uses categorical and continuous uniform priors, for which IC uses, respectively, categorical
and mixture of truncated Gaussian distributions as proposals parameterized by the NN. The creation
of IC NNs is automatic, i.e., an open-ended number of NN modules are generated by the PPL
on-the-ﬂy when a simulator address at is encountered for the ﬁrst time during training [64]. These
modules are reused (either for inference or undergoing further training) when the same address is
encountered in the lifetime of the same trained NN.

A common challenge for inference in real-world scientiﬁc models, such as those in particle physics,
is the presence of large dynamic ranges of prior probabilities for various outcomes. For instance,
some particle decays are ∼104 times more probable than others (Figure 8, appendix), and the prior
distribution for a particle momentum can be steeply falling. Therefore some cases may be much

5

more likely to be seen by the NN during training relative to others. For this reason, the proposal
parameters and the quality of the inference would vary signiﬁcantly according to the frequency of the
observations in the prior. To address this issue, we apply a “prior inﬂation” scheme to automatically
adjust the measure of the prior distribution during training to generate more instances of these unlikely
outcomes. This applies only to the training data generation for the IC NN, and the unmodiﬁed original
model prior is used during inference, ensuring that the importance weights (Eq. 3) and therefore the
empirical posterior are correct under the original model.

4.2 A Cross-Platform Probabilistic Execution Protocol

To couple our PPL and inference engines with simulators in a language-agnostic way, we introduce
a probabilistic programming execution protocol (PPX)7 that deﬁnes a schema for the execution of
probabilistic programs. The protocol covers language-agnostic deﬁnitions of common probability
distributions and message pairs covering the call and return values of (1) program entry points (2)
sample statements, and (3) observe statements (Figure 1, left). The implementation is based on
ﬂatbuffers,8 which is an efﬁcient cross-platform serialization library through which we compile the
protocol into the ofﬁcially supported languages C++, C#, Go, Java, JavaScript, PHP, Python, and
TypeScript, enabling very lightweight PPL front ends in these languages—in the sense of requiring
only an implementation to call sample and observe statements over the protocol. We exchange these
ﬂatbuffers-encoded messages over ZeroMQ9 [51] sockets, which allow seamless communication
between separate processes in the same machine (using inter-process sockets) or across a network
(using TCP).

Connecting any stochastic simulator in a supported language involves only the redirection of calls to
the random number generator (RNG) to call the sample method of PPX using the corresponding
probability distribution as the argument, which is facilitated when a simulator-wide RNG interface is
deﬁned in a single code ﬁle as is the case in SHERPA (Section 4.3). Conditioning is achieved by
either providing an observed value for any sample at inference time (which means that the sample
will be ﬁxed to the observed value) or adding manual observe statements, similar to Pyro [20].

Besides its use with our Python PPL, the protocol deﬁnes a very ﬂexible way of coupling any PPL
system to any model so that these two sides can be (1) implemented in different programming
languages and (2) executed in separate processes and on separate machines across networks. Thus we
present this protocol as a probabilistic programming analogue to the Open Neural Network Exchange
(ONNX)10 project for interoperability between deep learning frameworks, in the sense that PPX is
an interoperability project between PPLs allowing language-agnostic exchange of existing models
(simulators). Note that, more than a serialization format, the protocol enables runtime execution of
probabilistic models under the control of inference engines in different PPLs. We are releasing this
protocol as a separately maintained project, together with the rest of our work in Python and C++.

4.3 Controlling SHERPA’s Simulation of Fundamental Particle Physics

We demonstrate our framework with SHERPA [42], a Monte Carlo event generator of high-energy
reactions of particles, which is a state-of-the-art simulator of the Standard Model developed by the
particle physics community. SHERPA, like many other large-scale scientiﬁc projects, is implemented
in C++, and therefore we implement a C++ front end for our protocol.11 We couple SHERPA to the
front end by a system-wide rerouting of the calls to the RNG, which is made easy by the existence
of a third-party RNG interface (External_RNG) already present in SHERPA. Through this setup,
we can repurpose, with little effort, any stochastic simulation written in SHERPA as a probabilistic
generative model in which we can perform inference.

Random number draws in C++ simulators are commonly performed at a lower level than the actual
prior distribution that is being simulated. This applies to SHERPA where the only samples are from
the standard uniform distribution U (0, 1), which subsequently get used for different purposes using
transformations or rejection sampling. In our experiments (Section 5) we work with all uniform
samples except for a problem-speciﬁc single address that we know to be responsible for sampling
from a categorical distribution representing particle decay channels. The modiﬁcation of this address

7 https://github.com/pyprob/ppx
9 http://zeromq.org/

10 https://onnx.ai/

8 http://google.github.io/flatbuffers/

11 https://github.com/pyprob/pyprob_cpp

6

Figure 2: Top histograms: RMH and IC posterior results where a Channel 2 decay event (τ → ντ π−)
is the mode of the posterior distribution. Note that the eight variables shown are just a subset of the
full latent state of several thousand addresses (Figure 5, appendix). Vertical lines indicate the point
sample of the single GT trace supplying the calorimeter observation in each row. Bottom plots: trace
joint log-probability, Gelman–Rubin diagnostic, autocorrelation results belonging to the posterior in
the ﬁrst row.

to use the proper categorical prior allows an effortless application of prior inﬂation (Section 4.1) to
generate training data equally representing each channel.

Rejection sampling [41] sections in the simulator pose a challenge for our approach, as they deﬁne
execution traces that are a priori unbounded; and since the IC NN has to backpropagate through
every sampled value, this makes the training signiﬁcantly slower. Rejection sampling is key to the
application of Monte Carlo methods for evaluating matrix elements [61] and other stages of event
generation in particle physics; thus an efﬁcient treatment of this construction is primal. We address
this problem by implementing a novel trace evaluation scheme which works by annotating the sample
statements within long-running rejection sampling loops with a boolean ﬂag called replace, which,
when set true, enables a rejection-sampling-speciﬁc behavior for the given sample address. The
simplest correct approach is to exclude these replace addresses from IC inference (i.e., proposing
for these from the prior) and treat them as regular raw addresses in MCMC. Other approaches include
amortization schemes where during IC NN training we only consider the last (thus accepted) instance
ilast of any address (at, it) that fall within a rejection sampling loop. The results presented in this
paper use the former simple mode. Efﬁcient handling of rejection sampling in universal PPLs [45, 69],
and nested inference in general [76, 77], constitute an active area of research with several alternative
approaches currently being formulated with varying degrees of complexity and sample efﬁciency that
are beyond the scope of this paper.

5 Experiments

An important decay of the Higgs boson is to τ leptons, whose subsequent decay products interact
in the detector. This constitutes a rich and realistic case to simulate, and directly connects to an
important line of current research in particle physics. During simulation, SHERPA stochastically
generates a set of particles to which the initial τ lepton will decay—a “decay channel”—and samples

7

32101230.00.20.40.60.81.01.2 pxICRMH32101230.00.20.40.60.8 pyICRMH43444546470.00.10.20.30.4 pzICRMH01020300.00.20.40.60.81.0Decay ChannelICRMH0102030400.0000.0250.0500.0750.1000.1250.1500.175FSP Energy 1ICRMH0102030400.000.020.040.060.080.100.120.14FSP Energy 2ICRMH010203040FSP 1 Energy [GeV]0510152025303540FSP 2 Energy [GeV]ICRMH0.00.51.01.52.02.53.00.00.51.01.52.0METICRMHx3210123y3210123z0.02.55.07.510.012.515.0Observed Calorimeter32101230.00.20.40.60.8ICRMH32101230.00.10.20.30.40.50.60.7ICRMH43444546470.000.050.100.150.200.250.300.35ICRMH01020300.00.20.40.60.81.0ICRMH0102030400.000.050.100.150.200.250.30ICRMH0102030400.0000.0250.0500.0750.1000.1250.150ICRMH010203040FSP 1 Energy [GeV]0510152025303540FSP 2 Energy [GeV]ICRMH0.00.51.01.52.02.53.00.000.250.500.751.001.251.501.75ICRMHx3210123y3210123z0.02.55.07.510.012.515.032101230.00.10.20.30.4ICRMH32101230.00.20.40.60.8ICRMH43444546470.00.10.20.30.4ICRMH01020300.00.20.40.60.81.0ICRMH0102030400.0000.0250.0500.0750.1000.1250.1500.175ICRMH0102030400.000.050.100.150.20ICRMH010203040FSP 1 Energy [GeV]0510152025303540FSP 2 Energy [GeV]ICRMH0.00.51.01.52.02.53.00.00.20.40.60.8ICRMHx3210123y3210123z0.02.55.07.510.012.515.032101230.00.20.40.60.81.01.2ICRMH32101230.00.20.40.60.8ICRMH43444546470.000.050.100.150.200.250.300.35ICRMH01020300.00.20.40.60.81.0ICRMH0102030400.000.050.100.150.20ICRMH0102030400.000.020.040.060.080.100.120.14ICRMH010203040FSP 1 Energy [GeV]0510152025303540FSP 2 Energy [GeV]ICRMH0.00.51.01.52.02.53.00.00.51.01.52.02.5ICRMHx3210123y3210123z0.02.55.07.510.012.515.03210123Momentum [GeV/c]0.00.20.40.60.81.0ICRMH3210123Momentum [GeV/c]0.00.20.40.60.81.01.2ICRMH4344454647Momentum [GeV/c]0.000.050.100.150.200.250.300.35ICRMH01020300.00.20.40.60.81.0ICRMH010203040Energy [GeV]0.000.050.100.150.200.25ICRMH010203040Energy [GeV]0.0000.0250.0500.0750.1000.1250.1500.175ICRMH010203040FSP 1 Energy [GeV]0510152025303540FSP 2 Energy [GeV]ICRMH0.00.51.01.52.02.53.0Missing ET0.00.51.01.52.02.5ICRMHx3210123y3210123z0.02.55.07.510.012.515.0010000002000000300000040000005000000600000070000008000000Iteration215.0212.5210.0207.5205.0202.5200.0197.5195.0Log probabilityPosterior, RMH (initialized with random trace from prior)Posterior, RMH (initialized with ground truth trace)102103104105106107Iteration100101102R-hatchannel_indexmother_momentum_xmother_momentum_ymother_momentum_z14 most frequent addresses102103104105106Iteration0.00.20.40.60.81.0Autocorrelationchannel_indexmother_momentum_xmother_momentum_ymother_momentum_z14 most frequent addressesthe momenta of these particles according to a joint density obtained from underlying physical theory.
These particles then interact in the detector leading to observations in the raw sensor data. While
Geant4 is typically used to model the interactions in a detector, for our initial studies we implement a
fast, approximate, stochastic detector simulation for a calorimeter with longitudinal and transverse
segmentation (with 20×35×35 voxels). The detector deposits most of the energy for electrons and π0
into the ﬁrst layers and charged hadrons (e.g., π±) deeper into the calorimeter with larger ﬂuctuations.

Figure 2 presents posterior distributions of a selected subset of random variables in the simulator for
ﬁve different test cases where the mode of the posterior is a channel-2 decay (τ → ντ π−). Test cases
are generated by sampling an execution trace from the simulator prior, giving us a “ground truth trace”
(GT trace), from which we extract the simulated raw 3D calorimeter as a test observation. We run
our inference engines taking only these calorimeter data as input, giving us posteriors over the entire
latent state of the simulator, conditioned on the observed calorimeter using a physically-motivated
Poisson likelihood. We show RMH (MCMC) and IC inference results, where RMH serves as a
baseline as it samples from the true posterior of the model, albeit at great computational cost. For
each case, we establish the convergence of the RMH posterior to the true posterior by computing
the Gelman–Rubin (GR) convergence diagnostic [26, 89] between two MCMC chains conditioned
on the same observation, one starting from the GT trace and one starting from a random trace
sampled from the prior.12 As an example, in Figure 2 (bottom) we show the joint log-probability, GR
diagnostic, and autocorrelation plots of the RMH posterior (with 7.7M traces) belonging to the test
case in the ﬁrst row. The GR result indicates that the chains converged around 106 iterations, and the
autocorrelation result indicates that we need approximately 105 iterations to accumulate each new
effectively independent sample from the true posterior. These RMH baseline results incur signiﬁcant
computational cost due to the sequential nature of the sampling and the large number of iterations one
needs to accumulate statistically independent samples. The example we presented took 115 compute
hours on an Intel E5-2695 v2 @ 2.40GHz CPU node.

We present IC posteriors conditioned on the same observations in Figure 2 and plot these together
with corresponding RMH baselines, showing good agreement in all cases. These IC posteriors were
obtained in less than 30 minutes in each case, representing a signiﬁcant speedup compared with
the RMH baseline. This is due to three main strengths of IC inference: (1) each trace executed by
the IC engine gives us a statistically independent sample from the learned proposal approximating
the true posterior (Equation 4) (cf. the autocorrelation time of 105 in RMH); following from this
independence, (2) IC inference does not necessitate a burn-in period (cf. 106 iterations to convergence
in GR for RMH); and (3) IC inference is embarrassingly parallelizable. These features represent
the main motivation to incorporate IC in our framework to make inference in large-scale simulators
computationally efﬁcient and practicable. The results presented were obtained by running IC inference
in parallel on 20 compute nodes of the type used for RMH inference, using a NN with 143,485,048
parameters that has been trained for 40 epochs with a training set of 3M traces sampled from the
simulator prior, lasting two days on 32 CPU nodes. This time cost for NN training needs to be
incurred only once for any given simulator setup, resulting in a trained inference NN that enables
fast, repeated inference in the model speciﬁed by the simulator—a concept referred to as “amortized
inference”. Details of the 3DCNN–LSTM architecture used are in Figure 9 (appendix).

In the last test case in Figure 2 we show posteriors corresponding to a calorimeter observation of
a Channel 22 event (τ → ντ K −K −K +), a type of decay producing calorimeter depositions with
similarity to Channel 2 decays and with extremely low probability in the prior (Figure 8, appendix),
therefore representing a difﬁcult case to infer. We see the posterior uncertainty in the true (RMH)
posterior of this case, where Channel 2 is the mode of the posterior with a small probability mass
on Channel 22 among other channels. We see that the IC posterior is able to reproduce this small
probability mass on Channel 22 with success, thanks to the “prior inﬂation” scheme with which we
train IC NNs. This leads to a proposal where Channel 22 is the mode, which later gets adjusted by
importance weighting (Equation 3) to match the true posterior result (Figure 7, appendix). Our results
demonstrate the feasibility of Bayesian inference in the whole latent space of this existing simulator
deﬁning a potentially unbounded number of addresses, of which we encountered approximately 24k
during our experiments (Table 1 also Figure 5, appendix). To our knowledge, this is the ﬁrst time a
PPL system has been used with a model expressed by an existing state-of-the-art simulator at this
scale.

12 The GR diagnostic compares estimated between-chains and within-chain variances, summarized as the ˆR
metric which approaches unity as the chains converge on the target distribution.

8

6 Conclusions

We presented the ﬁrst step in subsuming the vast existing body of scientiﬁc simulators, which are
causal, generative models that often reﬂect the most accurate understanding in their respective ﬁelds,
into a universal probabilistic programming framework. The ability to scale probabilistic inference to
large-scale simulators is of fundamental importance to the ﬁeld of probabilistic programming and
the wider modeling community. It is a hard problem that requires innovations in many areas such as
model–PPL interface, handling of priors with long tails, amortization of rejection sampling routines
[69], addressing schemes, IC network architectures, and distributed training and inference [19] which
make it difﬁcult to cover in depth in a single paper.

Our work allows one to use existing simulator code bases to perform model-based machine learning
with interpretability, where the simulator is no longer used as a black box to generate synthetic
training data, but as a highly structured generative model that the simulator’s code already speciﬁes.
Bayesian inference in this setting gives results that are highly interpretable, where we get to see the
exact locations and processes in the model that are associated with each prediction and the uncertainty
in each prediction. With this novel framework providing a clearly deﬁned interface between domain-
speciﬁc simulators and probabilistic machine learning techniques, we expect to enable a wide range
of applied work straddling machine learning and ﬁelds of science and engineering. In the particle
physics setting, our ultimate aim is to run the inference stage of this approach on collision data from
real detectors by implementing a full LHC physics analysis together with the full posterior, so that
it can be exploited for discovery of new physics via simulations that contain processes beyond the
current Standard Model.

Acknowledgments

We thank the anonymous reviewers for their constructive comments that helped us improve this paper
signiﬁcantly. This research used resources of the National Energy Research Scientiﬁc Computing
Center (NERSC), a U.S. Department of Energy Ofﬁce of Science User Facility operated under
Contract No. DE-AC02-05CH11231. This work was partially supported by the NERSC Big Data
Center; we acknowledge Intel for their funding support. KC, LH, and GL were supported by the
National Science Foundation under the awards ACI-1450310. Additionally, KC was supported by the
National Science Foundation award OAC-1836650. BGH is supported by the EPRSC Autonomous
Intelligent Machines and Systems grant. AGB and PT are supported by EPSRC/MURI grant
EP/N019474/1 and AGB is also supported by Lawrence Berkeley National Lab. FW is supported by
DARPA D3M, under Cooperative Agreement FA8750-17-2-0093, Intel under its LBNL NERSC Big
Data Center, and an NSERC Discovery grant.

References

[1] G. Aad, T. Abajyan, B. Abbott, J. Abdallah, S. Abdel Khalek, A. A. Abdelalim, O. Abdinov,
R. Aben, B. Abi, M. Abolins, and et al. Observation of a new particle in the search for the
Standard Model Higgs boson with the ATLAS detector at the LHC. Physics Letters B, 716:1–29,
Sept. 2012.

[2] G. Aad et al. Reconstruction of hadronic decay products of tau leptons with the ATLAS

experiment. Eur. Phys. J., C76(5):295, 2016.

[3] M. Abadi, P. Barham, J. Chen, Z. Chen, A. Davis, J. Dean, M. Devin, S. Ghemawat, G. Irving,
M. Isard, et al. Tensorﬂow: A system for large-scale machine learning. In 12th USENIX
Symposium on Operating Systems Design and Implementation (OSDI 16), pages 265–283,
2016.

[4] V. M. Abazov et al. A precision measurement of the mass of the top quark. Nature, 429:638–642,

2004.

[5] J. Allison, K. Amako, J. Apostolakis, P. Arce, M. Asai, T. Aso, E. Bagli, A. Bagulya, S. Banerjee,
G. Barrand, B. Beck, A. Bogdanov, D. Brandt, J. Brown, H. Burkhardt, P. Canal, D. Cano-Ott,
S. Chauvie, K. Cho, G. Cirrone, G. Cooperman, M. Cortés-Giraldo, G. Cosmo, G. Cuttone,

9

G. Depaola, L. Desorgher, X. Dong, A. Dotti, V. Elvira, G. Folger, Z. Francis, A. Galoyan,
L. Garnier, M. Gayer, K. Genser, V. Grichine, S. Guatelli, P. Guèye, P. Gumplinger, A. Howard,
I. Hˇrivnáˇcová, S. Hwang, S. Incerti, A. Ivanchenko, V. Ivanchenko, F. Jones, S. Jun, P. Kai-
taniemi, N. Karakatsanis, M. Karamitros, M. Kelsey, A. Kimura, T. Koi, H. Kurashige, A. Lech-
ner, S. Lee, F. Longo, M. Maire, D. Mancusi, A. Mantero, E. Mendoza, B. Morgan, K. Mu-
rakami, T. Nikitina, L. Pandola, P. Paprocki, J. Perl, I. Petrovi´c, M. Pia, W. Pokorski, J. Quesada,
M. Raine, M. Reis, A. Ribon, A. R. Fira, F. Romano, G. Russo, G. Santin, T. Sasaki, D. Sawkey,
J. Shin, I. Strakovsky, A. Taborda, S. Tanaka, B. Tomé, T. Toshito, H. Tran, P. Truscott, L. Urban,
V. Uzhinsky, J. Verbeke, M. Verderi, B. Wendt, H. Wenzel, D. Wright, D. Wright, T. Yamashita,
J. Yarba, and H. Yoshida. Recent developments in GEANT4. Nuclear Instruments and Meth-
ods in Physics Research Section A: Accelerators, Spectrometers, Detectors and Associated
Equipment, 835(Supplement C):186 – 225, 2016.

[6] J. Alwall, R. Frederix, S. Frixione, V. Hirschi, F. Maltoni, O. Mattelaer, H.-S. Shao, T. Stelzer,
P. Torrielli, and M. Zaro. The automated computation of tree-level and next-to-leading order
differential cross sections, and their matching to parton shower simulations. Journal of High
Energy Physics, 2014(7):79, 2014.

[7] J. Alwall, A. Freitas, and O. Mattelaer. The Matrix Element Method and QCD Radiation. Phys.

Rev., D83:074010, 2011.

[8] J. R. Andersen, C. Englert, and M. Spannowsky. Extracting precise Higgs couplings by using

the matrix element method. Phys. Rev., D87(1):015019, 2013.

[9] P. Artoisenet, P. de Aquino, F. Maltoni, and O. Mattelaer. Unravelling tth via the Matrix

Element Method. Phys. Rev. Lett., 111(9):091802, 2013.

[10] P. Artoisenet and O. Mattelaer. MadWeight: Automatic event reweighting with matrix elements.

PoS, CHARGED2008:025, 2008.

[11] M. S. Arulampalam, S. Maskell, N. Gordon, and T. Clapp. A tutorial on particle ﬁlters for
online nonlinear/non-Gaussian Bayesian tracking. IEEE Transactions on Signal Processing,
50(2):174–188, 2002.

[12] A. Askew, P. Jaiswal, T. Okui, H. B. Prosper, and N. Sato. Prospect for measuring the CP phase

in the hτ τ coupling at the LHC. Phys. Rev., D91(7):075014, 2015.

[13] L. Asquith et al. Jet Substructure at the Large Hadron Collider : Experimental Review. 2018.

[14] A. Aurisano, A. Radovic, D. Rocco, A. Himmel, M. Messier, E. Niner, G. Pawloski, F. Psihas,
A. Sousa, and P. Vahle. A convolutional neural network neutrino event classiﬁer. Journal of
Instrumentation, 11(09):P09001, 2016.

[15] P. Avery et al. Precision studies of the Higgs boson decay channel H → ZZ → 4l with MEKD.

Phys. Rev., D87(5):055006, 2013.

[16] M. Bähr, S. Gieseke, M. A. Gigg, D. Grellscheid, K. Hamilton, O. Latunde-Dada, S. Plätzer,
P. Richardson, M. H. Seymour, A. Sherstnev, et al. Herwig++ physics and manual. The
European Physical Journal C, 58(4):639–707, 2008.

[17] P. Baldi, P. Sadowski, and D. Whiteson. Searching for exotic particles in high-energy physics

with deep learning. Nature Communications, 5:4308, 2014.

[18] A. G. Baydin, B. A. Pearlmutter, A. A. Radul, and J. M. Siskind. Automatic differentiation in
machine learning: a survey. Journal of Machine Learning Research (JMLR), 18(153):1–43,
2018.

[19] A. G. Baydin, L. Shao, W. Bhimji, L. Heinrich, L. F. Meadows, J. Liu, A. Munk, S. Naderiparizi,
B. Gram-Hansen, G. Louppe, M. Ma, X. Zhao, P. Torr, V. Lee, K. Cranmer, Prabhat, and
F. Wood. Etalumis: Bringing probabilistic programming to scientiﬁc simulators at scale. In
Proceedings of the International Conference for High Performance Computing, Networking,
Storage, and Analysis (SC19), November 17–22, 2019, 2019.

10

[20] E. Bingham, J. P. Chen, M. Jankowiak, F. Obermeyer, N. Pradhan, T. Karaletsos, R. Singh,
P. Szerlip, P. Horsfall, and N. D. Goodman. Pyro: Deep universal probabilistic programming.
Journal of Machine Learning Research, 2018.

[21] C. M. Bishop. Mixture density networks. Technical Report NCRG/94/004, Neural Computing

Research Group, Aston University, 1994.

[22] D. M. Blei, A. Kucukelbir, and J. D. McAuliffe. Variational inference: A review for statisticians.

Journal of the American Statistical Association, 112(518):859–877, 2017.

[23] S. Bolognesi, Y. Gao, A. V. Gritsan, K. Melnikov, M. Schulze, N. V. Tran, and A. Whitbeck. On
the spin and parity of a single-produced resonance at the LHC. Phys. Rev., D86:095031, 2012.

[24] J. Brehmer, K. Cranmer, G. Louppe, and J. Pavez. A Guide to Constraining Effective Field

Theories with Machine Learning. Phys. Rev., D98(5):052004, 2018.

[25] S. Brooks, A. Gelman, G. Jones, and X.-L. Meng. Handbook of Markov Chain Monte Carlo.

CRC press, 2011.

[26] S. P. Brooks and A. Gelman. General methods for monitoring convergence of iterative simula-

tions. Journal of Computational and Graphical Statistics, 7(4):434–455, 1998.

[27] J. M. Campbell, R. K. Ellis, W. T. Giele, and C. Williams. Finding the Higgs boson in decays to
Zγ using the matrix element method at Next-to-Leading Order. Phys. Rev., D87(7):073005,
2013.

[28] B. Carpenter, A. Gelman, M. D. Hoffman, D. Lee, B. Goodrich, M. Betancourt, M. Brubaker,
J. Guo, P. Li, A. Riddell, et al. Stan: A probabilistic programming language. Journal of
Statistical Software, 76(i01), 2017.

[29] S. Chatrchyan, V. Khachatryan, A. M. Sirunyan, A. Tumasyan, W. Adam, E. Aguilo, T. Bergauer,
M. Dragicevic, J. Erö, C. Fabjan, and et al. Observation of a new boson at a mass of 125 GeV
with the CMS experiment at the LHC. Physics Letters B, 716:30–61, Sept. 2012.

[30] K. Cranmer, J. Pavez, and G. Louppe. Approximating likelihood ratios with calibrated discrimi-

native classiﬁers. arXiv preprint arXiv:1506.02169, 2015.

[31] L. de Oliveira, M. Kagan, L. Mackey, B. Nachman, and A. Schwartzman. Jet-images – deep

learning edition. Journal of High Energy Physics, 2016(7):69, 2016.

[32] J. V. Dillon, I. Langmore, D. Tran, E. Brevdo, S. Vasudevan, D. Moore, B. Patton, A. Alemi,
M. Hoffman, and R. A. Saurous. Tensorﬂow distributions. arXiv preprint arXiv:1711.10604,
2017.

[33] A. Djouadi. The Anatomy of electro-weak symmetry breaking. I: The Higgs boson in the

standard model. Phys. Rept., 457:1–216, 2008.

[34] A. Doucet and A. M. Johansen. A tutorial on particle ﬁltering and smoothing: Fifteen years

later. Handbook of Nonlinear Filtering, 12(656-704):3, 2009.

[35] R. Dutta, J. Corander, S. Kaski, and M. U. Gutmann. Likelihood-free inference by penalised

logistic regression. arXiv preprint arXiv:1611.10242, 2016.

[36] E. Endeve, C. Y. Cardall, R. D. Budiardja, S. W. Beck, A. Bejnood, R. J. Toedte, A. Mezzacappa,
and J. M. Blondin. Turbulent magnetic ﬁeld ampliﬁcation from spiral SASI modes: implications
for core-collapse supernovae and proto-neutron star magnetization. The Astrophysical Journal,
751(1):26, 2012.

[37] J. S. Gainer, J. Lykken, K. T. Matchev, S. Mrenna, and M. Park. The Matrix Element Method:
Past, Present, and Future. In Proceedings, 2013 Community Summer Study on the Future of
U.S. Particle Physics: Snowmass on the Mississippi (CSS2013): Minneapolis, MN, USA, July
29-August 6, 2013, 2013.

[38] Y. Gao, A. V. Gritsan, Z. Guo, K. Melnikov, M. Schulze, and N. V. Tran. Spin determination of

single-produced resonances at hadron colliders. Phys. Rev., D81:075022, 2010.

11

[39] A. Gelman, D. Lee, and J. Guo. Stan: A Probabilistic Programming Language for Bayesian
Inference and Optimization. Journal of Educational and Behavioral Statistics, 40(5):530–543,
2015.

[40] S. J. Gershman and N. D. Goodman. Amortized inference in probabilistic reasoning.
Proceedings of the 36th Annual Conference of the Cognitive Science Society, 2014.

In

[41] W. R. Gilks and P. Wild. Adaptive rejection sampling for Gibbs sampling. Applied Statistics,

pages 337–348, 1992.

[42] T. Gleisberg, S. Hoeche, F. Krauss, M. Schonherr, S. Schumann, F. Siegert, and J. Winter. Event

generation with SHERPA 1.1. Journal of High Energy Physics, 02:007, 2009.

[43] N. Goodman, V. Mansinghka, D. M. Roy, K. Bonawitz, and J. B. Tenenbaum. Church: a

language for generative models. arXiv preprint arXiv:1206.3255, 2012.

[44] A. D. Gordon, T. A. Henzinger, A. V. Nori, and S. K. Rajamani. Probabilistic programming. In

Proceedings of the Future of Software Engineering, pages 167–181. ACM, 2014.

[45] B. Gram-Hansen, C. S. de Witt, R. Zinkov, S. Naderiparizi, A. Scibior, A. Munk, F. Wood,
M. Ghadiri, P. Torr, Y. W. Te, A. G. Baydin, and T. Rainforth. Efﬁcient bayesian inference
for nested simulators. In Second Symposium on Advances in Approximate Bayesian Inference
(AABI), Vancouver, Canada, 8 December 2019, 2019.

[46] A. V. Gritsan, R. Röntsch, M. Schulze, and M. Xiao. Constraining anomalous Higgs boson cou-
plings to the heavy ﬂavor fermions using matrix element techniques. Phys. Rev., D94(5):055023,
2016.

[47] B. Grzadkowski and J. F. Gunion. Using decay angle correlations to detect CP violation in the

neutral Higgs sector. Phys. Lett., B350:218–224, 1995.

[48] R. Harnik, A. Martin, T. Okui, R. Primulando, and F. Yu. Measuring CP violation in h → τ +τ −

at colliders. Phys. Rev., D88(7):076009, 2013.

[49] F. Hartig, J. M. Calabrese, B. Reineking, T. Wiegand, and A. Huth. Statistical inference for
stochastic simulation models–theory and application. Ecology Letters, 14(8):816–827, 2011.

[50] A. Heinecke, A. Breuer, S. Rettenberger, M. Bader, A.-A. Gabriel, C. Pelties, A. Bode, W. Barth,
X.-K. Liao, K. Vaidyanathan, et al. Petascale high order dynamic rupture earthquake simulations
on heterogeneous supercomputers. In Proceedings of the International Conference for High
Performance Computing, Networking, Storage and Analysis, pages 3–14. IEEE Press, 2014.

[51] P. Hintjens. ZeroMQ: messaging for many applications. O’Reilly Media, Inc., 2013.

[52] S. Hochreiter and J. Schmidhuber. Long short-term memory. Neural Computation, 9(8):1735–

1780, 1997.

[53] M. D. Hoffman, D. M. Blei, C. Wang, and J. Paisley. Stochastic variational inference. The

Journal of Machine Learning Research, 14(1):1303–1347, 2013.

[54] M. D. Hoffman and A. Gelman. The No-U-turn Sampler: Adaptively Setting Path Lengths in

Hamiltonian Monte Carlo. Journal of Machine Learning Research, 15(1):1593–1623, 2014.

[55] B. Hooberman, A. Farbin, G. Khattak, V. Pacela, M. Pierini, J.-R. Vlimant, M. Spiropulu,
W. Wei, M. Zhang, and S. Vallecorsa. Calorimetry with Deep Learning: Particle Classiﬁcation,
Energy Regression, and Simulation for High-Energy Physics, 2017. Deep Learning in Physical
Sciences (NIPS workshop). https://dl4physicalsciences.github.io/files/nips_
dlps_2017_15.pdf.

[56] G. Kasieczka. Boosted Top Tagging Method Overview. In 10th International Workshop on Top

Quark Physics (TOP2017) Braga, Portugal, September 17-22, 2017, 2018.

12

[57] D. P. Kingma, T. Salimans, R. Jozefowicz, X. Chen, I. Sutskever, and M. Welling. Improved
variational inference with inverse autoregressive ﬂow.
In D. D. Lee, M. Sugiyama, U. V.
Luxburg, I. Guyon, and R. Garnett, editors, Advances in Neural Information Processing Systems
29, pages 4743–4751. Curran Associates, Inc., 2016.

[58] D. P. Kingma and M. Welling. Auto-encoding variational Bayes.

arXiv preprint

arXiv:1312.6114, 2013.

[59] D. Koller and N. Friedman. Probabilistic graphical models: principles and techniques. MIT

press, 2009.

[60] K. Kondo. Dynamical Likelihood Method for Reconstruction of Events With Missing Momen-

tum. 1: Method and Toy Models. J. Phys. Soc. Jap., 57:4126–4140, 1988.

[61] F. Krauss. Matrix elements and parton showers in hadronic interactions. Journal of High Energy

Physics, 2002(08):015, 2002.

[62] W. Lampl, S. Laplace, D. Lelas, P. Loch, H. Ma, S. Menke, S. Rajagopalan, D. Rousseau,
S. Snyder, and G. Unal. Calorimeter Clustering Algorithms: Description and Performance.
Technical Report ATL-LARG-PUB-2008-002. ATL-COM-LARG-2008-003, CERN, Geneva,
Apr 2008.

[63] T. A. Le. Inference for higher order probabilistic programs. Masters Thesis, University of

Oxford, 2015.

[64] T. A. Le, A. G. Baydin, and F. Wood.

Inference compilation and universal probabilistic
programming. In Proceedings of the 20th International Conference on Artiﬁcial Intelligence
and Statistics (AISTATS), volume 54 of Proceedings of Machine Learning Research, pages
1338–1348, Fort Lauderdale, FL, USA, 2017. PMLR.

[65] Y. LeCun, L. Bottou, Y. Bengio, and P. Haffner. Gradient-based learning applied to document

recognition. Proceedings of the IEEE, 86(11):2278–2324, 1998.

[66] M. Lezcano Casado, A. G. Baydin, D. Martinez Rubio, T. A. Le, F. Wood, L. Heinrich,
G. Louppe, K. Cranmer, W. Bhimji, K. Ng, and Prabhat. Improvements to inference compilation
In Neural Information
for probabilistic programming in large-scale scientiﬁc simulators.
Processing Systems (NIPS) 2017 workshop on Deep Learning for Physical Sciences (DLPS),
Long Beach, CA, US, December 8, 2017, 2017.

[67] T. Martini and P. Uwer. Extending the Matrix Element Method beyond the Born approximation:

Calculating event weights at next-to-leading order accuracy. JHEP, 09:083, 2015.

[68] T. Martini and P. Uwer. The Matrix Element Method at next-to-leading order QCD for hadronic

collisions: Single top-quark production at the LHC as an example application. 2017.

[69] S. Naderiparizi, A. ´Scibior, A. Munk, M. Ghadiri, A. G. Baydin, B. Gram-Hansen, C. S. de Witt,
R. Zinkov, P. H. Torr, T. Rainforth, Y. W. Teh, and F. Wood. Amortized rejection sampling in
universal probabilistic programming. arXiv preprint arXiv:1910.09056, 2019.

[70] R. M. Neal. MCMC Using Hamiltonian dynamics. Handbook of Markov Chain Monte Carlo,

2011.

[71] G. Papamakarios, T. Pavlakou, and I. Murray. Masked autoregressive ﬂow for density estimation.

In Advances in Neural Information Processing Systems, pages 2338–2347, 2017.

[72] A. Paszke, S. Gross, S. Chintala, G. Chanan, E. Yang, Z. DeVito, Z. Lin, A. Desmaison,
L. Antiga, and A. Lerer. Automatic differentiation in PyTorch. In NIPS 2017 Autodiff Workshop:
The Future of Gradient-based Machine Learning Software and Techniques, Long Beach, CA,
US, December 9, 2017, 2017.

[73] P. Perdikaris, L. Grinberg, and G. E. Karniadakis. Multiscale modeling and simulation of brain

blood ﬂow. Physics of Fluids, 28(2):021304, 2016.

13

[74] M. Raberto, S. Cincotti, S. M. Focardi, and M. Marchesi. Agent-based simulation of a ﬁnan-
cial market. Physica A: Statistical Mechanics and its Applications, 299(1):319 – 327, 2001.
Application of Physics in Economic Modelling.

[75] E. Racah, S. Ko, P. Sadowski, W. Bhimji, C. Tull, S.-Y. Oh, P. Baldi, et al. Revealing fundamental
physics from the daya bay neutrino experiment using deep neural networks. In Machine Learning
and Applications (ICMLA), 2016 15th IEEE International Conference on, pages 892–897. IEEE,
2016.

[76] T. Rainforth. Nesting probabilistic programs.

In Conference on Uncertainty in Artiﬁcial

Intelligence (UAI), 2018.

[77] T. Rainforth, R. Cornish, H. Yang, A. Warrington, and F. Wood. On nesting Monte Carlo

estimators. In International Conference on Machine Learning (ICML), 2018.

[78] D. J. Rezende and S. Mohamed. Variational inference with normalizing ﬂows. arXiv preprint

arXiv:1505.05770, 2015.

[79] D. Schouten, A. DeAbreu, and B. Stelzer. Accelerated Matrix Element Method with Parallel

Computing. Comput. Phys. Commun., 192:54–59, 2015.

[80] T. Sjöstrand, S. Mrenna, and P. Skands. Pythia 6.4 physics and manual. Journal of High Energy

Physics, 2006(05):026, 2006.

[81] D. E. Soper and M. Spannowsky. Finding physics signals with shower deconstruction. Phys.

Rev., D84:074002, 2011.

[82] M. Sunnåker, A. G. Busetto, E. Numminen, J. Corander, M. Foll, and C. Dessimoz. Approximate

Bayesian computation. PLoS Computational Biology, 9(1):e1002803, 2013.

[83] D. Tran, M. W. Hoffman, D. Moore, C. Suter, S. Vasudevan, and A. Radul. Simple, distributed,
and accelerated probabilistic programming. In Advances in Neural Information Processing
Systems, pages 7598–7609, 2018.

[84] D. Tran, A. Kucukelbir, A. B. Dieng, M. Rudolph, D. Liang, and D. M. Blei. Edward: A library

for probabilistic modeling, inference, and criticism. arXiv preprint arXiv:1610.09787, 2016.

[85] D. Tran, R. Ranganath, and D. Blei. Hierarchical implicit models and likelihood-free variational

inference. In Advances in Neural Information Processing Systems, pages 5523–5533, 2017.

[86] B. Uria, M.-A. Côté, K. Gregor, I. Murray, and H. Larochelle. Neural autoregressive distribution

estimation. Journal of Machine Learning Research, 17(205):1–37, 2016.

[87] J.-W. van de Meent, B. Paige, H. Yang, and F. Wood. An Introduction to Probabilistic Program-

ming. arXiv e-prints, Sep 2018.

[88] R. D. Wilkinson. Approximate Bayesian computation (ABC) gives exact results under the as-
sumption of model error. Statistical Applications in Genetics and Molecular Biology, 12(2):129–
141.

[89] D. Williams. Probability with Martingales. Cambridge University Press, 1991.

[90] D. Wingate, A. Stuhlmueller, and N. Goodman. Lightweight implementations of probabilistic
programming languages via transformational compilation. In Proceedings of the Fourteenth
International Conference on Artiﬁcial Intelligence and Statistics, pages 770–778, 2011.

[91] F. Wood, J. W. Meent, and V. Mansinghka. A new approach to probabilistic programming

inference. In Artiﬁcial Intelligence and Statistics, pages 1024–1032, 2014.

14

Efﬁcient Probabilistic Inference in the Quest for Physics Beyond the Standard Model
(Supplementary Material)

(a) Latent probabilistic structure of the 10 most frequent trace types.

(b) Latent probabilistic structure of the 25 most frequent traces types.

(c) Latent probabilistic structure of the 100 most frequent traces types.

(d) Latent probabilistic structure of the 250 most frequent traces types.

Figure 3: Interpretability of the latent structure of the τ lepton decay process, automatically extracted
from SHERPA executions via the probabilistic execution protocol. Showing model structure with
increasing detail by taking an increasing number of most common trace types into account. Node
labels denote address IDs (A1, A2, etc.) that correspond to uniquely identiﬁable parts of model
execution such as those in Table 1. Addresses A1, A2, A3 correspond to momenta px, py, pz, and A6
corresponds to the decay channel. Edge labels denote the frequency an edge is taken, normalized
per source node. Red: controlled; green: rejection sampling; blue: observed; yellow: uncontrolled.
Note: the addresses in these graphs are “aggregated”, meaning that we collapse all instances it of
addresses (at, it) into the same node in the graph, i.e., representing loops in the execution as cycles in
the graph, in order to simplify the presentation. This gives us ≤60 aggregated addresses representing
the transitions between a total of approximately 24k addresses (at, it) in the simulator.

15

STARTA11.000A21.000A31.000A41.000A51.000A61.000A261.0000.784A90.2160.464A100.5360.437A150.378A110.185A161.000A200.672A170.328A211.000A221.000A231.000A241.000A251.000END1.000A121.000A131.000A141.0001.0000.500A180.500A191.0001.000STARTA11.000A21.000A31.000A41.000A51.000A61.000A260.925A70.068A270.0070.788A90.2120.461A100.4190.1150.0060.470A150.319A110.208A310.0040.017A160.966A390.017A200.608A170.392A211.000A221.000A231.000A241.000A251.000END1.000A121.000A131.0000.149A140.8510.8510.1490.500A180.500A191.0000.8510.149A81.0000.1250.875A401.000A411.000A421.000A431.0001.000A281.000A291.000A301.0000.2500.7501.000STARTA11.000A21.000A31.000A41.000A51.000A61.000A260.814A70.091A270.0950.789A90.2110.143A100.1310.0850.6410.510A150.266A110.192A310.0320.043A160.917A390.0400.012A200.599A170.3860.004A211.000A221.000A231.000A241.000A251.000END1.000A121.000A131.0000.211A140.785A450.0050.7570.2090.0340.500A180.500A191.0000.7870.208A460.005A81.0000.1250.8750.057A400.943A411.000A421.0000.146A430.8540.9430.057A281.000A291.000A301.0000.2210.7790.8460.077A320.077A331.000A341.000A350.250A440.750A361.0000.464A370.536A381.0000.4000.6001.0000.5000.500A471.0000.5000.500STARTA11.000A21.000A31.000A41.000A51.000A61.000A260.723A70.098A270.1790.803A90.1970.076A100.0640.0730.7870.548A150.219A110.188A310.0450.084A160.842A390.0750.030A200.562A170.3970.009A570.003A211.000A221.000A231.000A241.000A251.000END1.000A121.000A131.0000.249A140.724A450.019A560.0070.6640.2460.0880.0020.500A180.500A191.0000.7180.2390.010A460.021A480.0070.005A81.0000.1250.8750.107A400.893A410.959A550.041A421.0000.177A430.8230.0080.8840.107A281.000A291.000A301.0000.2200.7800.7450.128A320.1280.0790.0350.0050.0050.876A471.0000.8330.1671.000A331.000A341.000A350.196A440.804A361.0000.559A370.441A381.0000.5080.0480.4441.000A491.000A501.0000.538A510.4620.538A520.4620.500A530.500A541.0000.4620.5381.000A581.000A591.000A601.0000.3330.667(a) Prior execution p(x, y).

(b) Posterior execution p(x|y) conditioned on a given calorimeter observation y.

Figure 4: Interpretability of the latent probabilistic structure of the τ lepton decay simulator code,
automatically extracted from 10,000 SHERPA executions via the probabilistic execution protocol.
The ﬂow is probabilistic at the shown nodes and deterministic along the edges. Edge labels denote the
frequency an edge is taken, normalized per source node. Red: controlled; green: rejection sampling;
blue: observed; yellow: uncontrolled. Note: the addresses in these graphs are “aggregated”, meaning
that we collapse all instances it of addresses (at, it) into the same node in the graph, i.e., representing
loops in the execution as cycles in the graph, in order to simplify the presentation. This gives us ≤60
aggregated addresses representing the transitions between a total of approximately 24k addresses
(at, it) in the simulator.

16

STARTA11.000A21.000A31.000A41.000A51.000A61.000A260.723A70.098A270.1790.803A90.1970.076A100.0640.0730.7870.548A150.219A110.188A310.0450.084A160.842A390.0750.030A200.562A170.3970.009A570.003A211.000A221.000A231.000A241.000A251.000END1.000A121.000A131.0000.249A140.724A450.019A560.0070.6640.2460.0880.0020.500A180.500A191.0000.7180.2390.010A460.021A480.0070.005A81.0000.1250.8750.107A400.893A410.959A550.041A421.0000.177A430.8230.0080.8840.107A281.000A291.000A301.0000.2200.7800.7450.128A320.1280.0790.0350.0050.0050.876A471.0000.8330.1671.000A331.000A341.000A350.196A440.804A361.0000.559A370.441A381.0000.5080.0480.4441.000A491.000A501.0000.538A510.4620.538A520.4620.500A530.500A541.0000.4620.5381.000A581.000A591.000A601.0000.3330.667STARTA11.000A21.000A31.000A41.000A51.000A61.000A261.000A70.000A270.0000.800A90.2000.006A100.9940.0000.0000.500A150.500A110.000A310.0000.499A160.001A390.4990.000A201.000A170.0000.000A570.000A211.000A221.000A231.000A241.000A251.000END1.000A121.000A131.0000.000A141.000A450.000A560.0001.0000.0000.0000.0000.500A180.500A191.0001.0000.0000.000A460.000A480.0000.000A81.0000.1250.8750.000A401.000A411.000A550.000A421.0000.000A431.0000.0001.0000.000A280.000A290.000A300.0000.0000.0000.0000.000A320.0000.0000.000 0.0000.000A470.0000.0000.0000.000A330.000A340.000A350.000A440.000A360.0000.000A370.000A380.0000.000 0.0000.000A490.000A500.0000.000A510.0000.000A520.0000.000A530.000A540.0000.0000.0000.000A580.000A590.000A600.0000.0000.000Figure 5: Example posterior over the entire latent state of the SHERPA simulator, conditioned on a
single observed calorimeter. For the observation used, the posteriors presented in this ﬁgure contain
approximately 6k addresses out of a total of approximately 24k addresses in the whole simulator. The
histograms shown in Figure 2 are only a subset of this collection. Note: presenting this many plots in
a single ﬁgure is challenging and a better plotting code is pending.

Figure 6: Steps of constructing the IC posterior for a Channel 2 GT event (τ → ντ π−, ﬁrst test case
in Figure 2). The IC proposal (top row) is produced by the trained inference network. It is then
weighted using Equation 3, giving IC posterior (middle row). The corresponding true posterior from
RMH (MCMC) baseline is given below (bottom row). Note that the shown variables are just a subset
of the full latent variables available in each case.

17

32101230.00.20.40.60.81.01.2 px32101230.00.20.40.60.8 py43444546470.000.050.100.150.200.250.30 pz01020300.00.20.40.60.81.0Decay Channel0102030400.000.010.020.030.040.050.06FSP Energy 10102030400.000.020.040.060.08FSP Energy 2010203040FSP 1 Energy [GeV]0510152025303540FSP 2 Energy [GeV]0.00.51.01.52.02.53.00.00.20.40.60.8METx3210123y3210123z0.02.55.07.510.012.515.0Observed Calorimeter32101230.00.20.40.60.81.01.232101230.00.20.40.60.843444546470.00.10.20.30.401020300.00.20.40.60.81.00102030400.0000.0250.0500.0750.1000.1250.1500.1750102030400.000.020.040.060.080.100.120.14010203040FSP 1 Energy [GeV]0510152025303540FSP 2 Energy [GeV]0.00.51.01.52.02.53.00.00.51.01.52.0x3210123y3210123z0.02.55.07.510.012.515.03210123Momentum [GeV/c]0.00.20.40.60.81.01.23210123Momentum [GeV/c]0.00.20.40.60.84344454647Momentum [GeV/c]0.000.050.100.150.200.250.300.3501020300.00.20.40.60.81.0010203040Energy [GeV]0.0000.0250.0500.0750.1000.1250.1500.175010203040Energy [GeV]0.000.020.040.060.080.100.120.14010203040FSP 1 Energy [GeV]0510152025303540FSP 2 Energy [GeV]0.00.51.01.52.02.53.0Missing ET0.00.51.01.52.0x3210123y3210123z0.02.55.07.510.012.515.0Figure 7: Steps of constructing the IC posterior for a Channel 22 GT event (τ → ντ K −K −K +, last
test case in Figure 2). The IC proposal (top row) is produced by the trained inference network. It is
then weighted using Equation 3, giving IC posterior (middle row). The corresponding true posterior
from RMH (MCMC) baseline is given below (bottom row). Note that the shown variables are just a
subset of the full latent variables available in each case. The effect of “prior inﬂation” can be seen
in the proposal mode of Channel 22 which the NN proposes as the most likely (i.e., mode of the
proposal). However after importance weighting the IC posterior matches the true posterior from
RMH (MCMC) where Channel 22 has very low (but non-zero) posterior probability due to the prior
model.

τ −

W −

ντ

ντ

e/µ−

τ −

νe/µ

W −

π0

π−

d

d

d

u

γ

γ

Figure 8: Top: branching ratios of the τ lepton, effectively the prior distribution of the decay channels
in SHERPA. Note that the scale is logarithmic. Bottom: Feynman diagrams for τ decays illustrating
that these can produce multiple detected particles.

18

32101230.00.20.40.60.81.01.21.4 px32101230.000.250.500.751.001.251.50 py43444546470.00.10.20.30.4 pz01020300.00.20.40.60.81.0Decay Channel0102030400.000.020.040.060.080.10FSP Energy 10102030400.0000.0250.0500.0750.1000.1250.1500.175FSP Energy 2010203040FSP 1 Energy [GeV]0510152025303540FSP 2 Energy [GeV]0.00.51.01.52.02.53.00.000.250.500.751.001.251.50METx3210123y3210123z0.02.55.07.510.012.515.0Observed Calorimeter32101230.00.20.40.60.81.032101230.00.20.40.60.81.01.243444546470.000.050.100.150.200.250.300.3501020300.00.20.40.60.81.00102030400.000.050.100.150.200.250102030400.0000.0250.0500.0750.1000.1250.1500.175010203040FSP 1 Energy [GeV]0510152025303540FSP 2 Energy [GeV]0.00.51.01.52.02.53.00.00.51.01.52.02.5x3210123y3210123z0.02.55.07.510.012.515.03210123Momentum [GeV/c]0.00.20.40.60.81.03210123Momentum [GeV/c]0.00.20.40.60.81.04344454647Momentum [GeV/c]0.000.050.100.150.200.250.300.3501020300.00.20.40.60.81.0010203040Energy [GeV]0.0000.0250.0500.0750.1000.1250.1500.175010203040Energy [GeV]0.000.020.040.060.080.100.120.14010203040FSP 1 Energy [GeV]0510152025303540FSP 2 Energy [GeV]0.00.51.01.52.02.53.0Missing ET0.00.51.01.52.02.5x3210123y3210123z0.02.55.07.510.012.515.010−410−310−210−1100101102103104105106BranchingRatioντe−¯νeντµ−¯νµντπ−ντK−ντπ−π0ντK−KSντK−KLντπ−KSντπ−KLντK−π0ντπ−π0π0ντπ+π−π−ντπ−KSπ0ντπ−KLπ0ντK−π+π−ντπ0π0K−ντK−K+π−ντK−KSπ0ντK−KLπ0ντπ−KSKLντπ−KSKSντπ−KLKLντK−K−K+ντπ+π−π−π0ντπ−π0π0π0ντK−π0π0π0ντKSπ−π0π0ντKLπ−π0π0ντK−K+π−π0ντπ−ηντK−ηντπ−ηηντηπ−π0ντπ−π−π+π0π0ντπ−π0π0π0π0ντK−π0π0π0π0ντπ−π−π+π0π0π0ντπ−π−π−π+π+π0Figure 9: Training and validation losses of the IC inference NNs used for the results presented in
Section 5. The network has 143,485,048 parameters and has been trained for 40 epochs. Network
conﬁguration: an LSTM with 512 hidden units; an observation embedding of size 256, encoded with
a 3D convolutional NN (CNN) [65] with layer conﬁguration Conv3D(1, 64, 3)–Conv3D(64, 64, 3)–
MaxPool3D(2)–Conv3D(64, 128, 3)–Conv3D(128, 128, 3)–Conv3D(128, 128, 3)– MaxPool3D(2)–
FC(2048, 256). We use previous sample embeddings of size 4 given by single-layer NNs, and address
embeddings of size 64. The proposal layers are two-layer NNs, the output of which are either a
mixture of ten truncated normal distributions [21] (for uniform continuous priors) or a categorical
distribution (for categorical priors). We use ReLU nonlinearities in all NN components.

19

0.000.250.500.751.001.251.501.752.00Trace1e821012345LossTraining LossValidation LossTable 1: Examples of addresses in the τ lepton decay problem in SHERPA (C++). Only the ﬁrst 6
addresses are shown out of a total of 24,382 addresses encountered over 1,602,880 executions to
collect statistics.
Address ID Full address

A1

A2

A3

A4

A5

A6

[forward(xt:: xarray_container<xt:: uvector<double, std:: allocator<double> >, (xt::
lay-
out_type)1, xt:: svector<unsigned long, 4ul, std:: allocator<unsigned long>, true>, xt:: xten-
sor_expression_tag>)+0x5f; SherpaGenerator:: Generate()+0x36; SHERPA:: Sherpa:: Gener-
ateOneEvent(bool)+0x2fa; SHERPA:: Event_Handler:: GenerateEvent(SHERPA:: eventtype::
code)+0x44d; SHERPA:: Event_Handler:: GenerateHadronDecayEvent(SHERPA:: eventtype::
code&)+0x45f; ATOOLS:: Random:: Get(bool, bool)+0x1d5; probprog_RNG:: Get(bool,
bool)+0xf9]_Uniform_1
[forward(xt:: xarray_container<xt:: uvector<double, std:: allocator<double> >, (xt::
lay-
out_type)1, xt:: svector<unsigned long, 4ul, std:: allocator<unsigned long>, true>, xt:: xten-
sor_expression_tag>)+0x5f; SherpaGenerator:: Generate()+0x36; SHERPA:: Sherpa:: Gener-
ateOneEvent(bool)+0x2fa; SHERPA:: Event_Handler:: GenerateEvent(SHERPA:: eventtype::
code)+0x44d; SHERPA:: Event_Handler:: GenerateHadronDecayEvent(SHERPA:: eventtype::
code&)+0x477; ATOOLS:: Random:: Get(bool, bool)+0x1d5; probprog_RNG:: Get(bool,
bool)+0xf9]_Uniform_1
[forward(xt:: xarray_container<xt:: uvector<double, std:: allocator<double> >, (xt::
lay-
out_type)1, xt:: svector<unsigned long, 4ul, std:: allocator<unsigned long>, true>, xt:: xten-
sor_expression_tag>)+0x5f; SherpaGenerator:: Generate()+0x36; SHERPA:: Sherpa:: Gener-
ateOneEvent(bool)+0x2fa; SHERPA:: Event_Handler:: GenerateEvent(SHERPA:: eventtype::
code)+0x44d; SHERPA:: Event_Handler:: GenerateHadronDecayEvent(SHERPA:: eventtype::
code&)+0x48f; ATOOLS:: Random:: Get(bool, bool)+0x1d5; probprog_RNG:: Get(bool,
bool)+0xf9]_Uniform_1
[forward(xt:: xarray_container<xt:: uvector<double, std:: allocator<double> >, (xt::
lay-
out_type)1, xt:: svector<unsigned long, 4ul, std:: allocator<unsigned long>, true>, xt:: xten-
sor_expression_tag>)+0x5f; SherpaGenerator:: Generate()+0x36; SHERPA:: Sherpa:: Gener-
ateOneEvent(bool)+0x2fa; SHERPA:: Event_Handler:: GenerateEvent(SHERPA:: eventtype::
code)+0x44d; SHERPA:: Event_Handler:: GenerateHadronDecayEvent(SHERPA:: eventtype::
code&)+0x8f4; ATOOLS:: Particle:: SetTime()+0xd; ATOOLS:: Flavour:: GenerateLifeTime()
const+0x35; ATOOLS:: Random:: Get()+0x18b; probprog_RNG:: Get()+0xde]_Uniform_1
[forward(xt:: xarray_container<xt:: uvector<double, std:: allocator<double> >, (xt::
lay-
out_type)1, xt:: svector<unsigned long, 4ul, std:: allocator<unsigned long>, true>, xt:: xten-
sor_expression_tag>)+0x5f; SherpaGenerator:: Generate()+0x36; SHERPA:: Sherpa:: Gener-
ateOneEvent(bool)+0x2fa; SHERPA:: Event_Handler:: GenerateEvent(SHERPA:: eventtype::
code)+0x44d; SHERPA:: Event_Handler:: GenerateHadronDecayEvent(SHERPA:: eventtype::
code&)+0x982; SHERPA:: Event_Handler:: IterateEventPhases(SHERPA:: eventtype:: code&,
double&)+0x1d2; SHERPA:: Hadron_Decays:: Treat(ATOOLS:: Blob_List*, double&)+0x975;
SHERPA:: Decay_Handler_Base:: TreatInitialBlob(ATOOLS:: Blob*, METOOLS:: Ampli-
tude2_Tensor*, std:: vector<ATOOLS:: Particle*, std:: allocator<ATOOLS:: Particle*> >
const&)+0x1ab1; SHERPA:: Hadron_Decay_Handler:: CreateDecayBlob(ATOOLS:: Parti-
cle*)+0x4cd; PHASIC:: Decay_Table:: Select() const+0x76e; ATOOLS:: Random:: Get(bool,
bool)+0x1d5; probprog_RNG:: Get(bool, bool)+0xf9]_Uniform_1
[forward(xt:: xarray_container<xt:: uvector<double, std:: allocator<double> >, (xt::
lay-
out_type)1, xt:: svector<unsigned long, 4ul, std:: allocator<unsigned long>, true>, xt:: xten-
sor_expression_tag>)+0x5f; SherpaGenerator:: Generate()+0x36; SHERPA:: Sherpa:: Gen-
erateOneEvent(bool)+0x2fa; SHERPA:: Event_Handler:: GenerateEvent(SHERPA:: event-
type:: code)+0x44d; SHERPA:: Event_Handler:: GenerateHadronDecayEvent(SHERPA:: event-
type:: code&)+0x982; SHERPA:: Event_Handler:: IterateEventPhases(SHERPA:: eventtype::
code&, double&)+0x1d2; SHERPA:: Hadron_Decays:: Treat(ATOOLS:: Blob_List*, dou-
ble&)+0x975; SHERPA:: Decay_Handler_Base:: TreatInitialBlob(ATOOLS:: Blob*, METOOLS::
Amplitude2_Tensor*, std:: vector<ATOOLS:: Particle*, std:: allocator<ATOOLS:: Parti-
cle*> > const&)+0x1ab1; SHERPA:: Hadron_Decay_Handler:: CreateDecayBlob(ATOOLS::
Particle*)+0x4cd; PHASIC:: Decay_Table:: Select() const+0x9d7; ATOOLS:: Random::
GetCategorical(std:: vector<double, std:: allocator<double> > const&, bool, bool)+0x1a5;
probprog_RNG:: GetCategorical(std:: vector<double, std:: allocator<double> > const&, bool,
bool)+0x111]_Categorical(length_categories:38)_1

20

