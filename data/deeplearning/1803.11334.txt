Cache-Enabled Dynamic Rate Allocation via Deep
Self-Transfer Reinforcement Learning

Zhengming Zhang, Yaru Zheng, Meng Hua, Yongming Huang and Luxi Yang
School of Information Science and Engineering, Southeast University, Nanjing 210096, China
Email: {zmzhang, yrzheng, mhua, huangym and lxyang}@seu.edu.cn

8
1
0
2

r
a

M
0
3

]
I

N
.
s
c
[

1
v
4
3
3
1
1
.
3
0
8
1
:
v
i
X
r
a

Abstract—Caching and rate allocation are two promising
approaches to support video streaming over wireless network.
However, existing rate allocation designs do not fully exploit
the advantages of the two approaches. This paper investigates
the problem of cache-enabled QoE-driven video rate allocation
problem. We establish a mathematical model for this problem,
and point out that it is difﬁcult to solve the problem with
traditional dynamic programming. Then we propose a deep
reinforcement learning approaches to solve it. First, we model
the problem as a Markov decision problem. Then we present
a deep Q-learning algorithm with a special knowledge transfer
process to ﬁnd out effective allocation policy. Finally, numerical
results are given to demonstrate that the proposed solution can
effectively maintain high-quality user experience of mobile user
moving among small cells. We also investigate the impact of
conﬁguration of critical parameters on the performance of our
algorithm.

Index Terms—Bit rate allocation, caching, deep reinforcement

learning, transfer learning.

I. INTRODUCTION

Due to the exponential growth of the number of smart
mobile equipments and innovative high-rate mobile data ser-
vices (such as videos streaming for mobile gaming and road
condition monitoring), the networks should accommodate the
overwhelming wireless trafﬁc demands [1], [2]. In fact it can
be predicted that the mobile video trafﬁc will be over one
third of mobile data trafﬁc by the end of 2018 [3]. People’s
demand for video loading speed and video clarity is endless,
thus the viewers’ Quality-of-Experience (QoE) is an important
indicator of the mobile communication network performance.
Mobile users often encounter video lag or sudden blur when
using mobile devices to watch videos. The reason for these
two cases is that the video is divided into small video chunks
(usually of the same length) by special algorithms [4]. If the
current network is of low quality, the video sender will reduce
the video resolution of the next few seconds to ensure that
users can continue to watch the video [5], [6]. In this way,
the quality can not be guaranteed. If the user fasts forward the
video, and the paragraph has not been loaded, then the video
playback will be interrupted until the corresponding paragraph
is cached. The goal of using the adaptive bit rates allocation
algorithm is to provide a better user experience and to reduce
network bandwidth usage. We will focus on three factors that
affect the user experience: quality (quantiﬁed by video rates),
packet dropping and video frozenness duration.

times can bring popular contents closer to users, and hence
improves users’ QoE [8]. Although caching effectiveness has
been extensively explored in the literature [7], [8], [9], they
do not consider the inﬂuence of bit rate allocation to the
effectiveness of caching.

In this paper, we adapt the ideas underlying the success of
QoE-driven dynamic video rate allocation to the cache-enabled
wireless network. There are two important and practical as-
sumption in our paper.

Assumption 1 : The bit rate received by the user is lied
in a large discrete space which almost can be considered as a
continuous range.

Assumption 2 : The central network has ﬁnite caching
capability but knows the size and status of the buffer of the
user’s mobile device [10], [11].

Most of the existing studies assume that the bit rate is in
discrete space, and the dimension of the space is low so that all
the values can be traversed within a ﬁnite time [6], [10]–[12].
This can be achieved by uniformly quantizing the candidate
regions where the bit rate is located. The main reason for
doing so is to simplify the complex real-world model and to
facilitate the solution of the problem. In practice, however, the
bit rate received by the user is lied in a large discrete space
which almost can be considered as a continuous range.

In our work, we maximize the bit rate of video chunks
that are actually consumed in a period of time, and minimize
packet dropping and video frozenness duration. We track the
network capacity and the buffer state like [10]. However,
unlike previous studies, the capacity of the network and buffer
state are considered as continuous values. We formulate the
problem as a Markov Decision Process (MDP),
then we
present a deep Q-learning approach with normalized advantage
functions and special knowledge transfer process to solve it.
The main contributions of this paper are summarized as

follows:

(i) We deﬁne a more realistic QoE criteria that consider the
total accumulation bit rates, packet dropping and video freeze
duration for cache-enabled video streaming.

(ii) We convert

the allocation problem into a Markov
decision problem which extends the network capacity and
candidate bit rate from discrete space to continuous space.
And we point that the states transition probability cannot be
obtained directly.

Recently, caching at base stations has been used as a promis-
ing approach for video streaming [7]. Caching during off-peak

(iii) We propose a way to go beyond the traditional algo-
rithms we call it deep self-transfer reinforcement learning. We

 
 
 
 
 
 
get (sub)optimal dynamic video rate allocation policy by using
this approach.

The rest of this paper is organized as follow. In Section II,
we introduce existing rate adaptation approaches. Section III
the system model is presented. Section IV, we proposed the
deep reinforcement learning algorithms in which the solution
of bitrate allocation policy for video streaming is veriﬁed.
Finally, numerical results and discussion are presented in
Section V, and a conclusion is reached in Section VI.

II. RELATED WORK

Improving the QoE of adaptive video streaming has been
the main focus of many researchers in recent years. Although
caching methods used to optimize the video transmission, the
audience’s requirements for the video quality are endless, the
sites providing wireless video services have to compromise
between the quality and transmission speed.

Model : Numerous models have been proposed to address
real-time adaptation of multimedia contents over wireless
channels for a better QoE. A novel cache-enabled video
rate control model that balances the needs for video rate
smoothness and energy cost saving is proposed in [11]. In
this work, they consider the playback rate in their QoE model,
but the transmission delay and caching delay are ignored. A
logarithmic QoE model derived from experimental results is
used in [13] and they formulate the content cache management
for video streaming into a convex optimization to give their
problem an analytical framework. A QoE-aware video rate
adaptation model is also proposed in [10]. In this work the
user’s buffer state is modeled as a vector and the value of
network capacity is discretized as a ﬁnite set. To describe
the dynamic evolution of the entire network, a large number
of studies have modeled the problem as a Markov decision
problem [10], [14]–[16]. We emphasize that the above studies
ignore the impact of delay on QoE and cache performance
and their MDP models are built in discrete action space and
discrete state space.

Algorithm : Convex optimization algorithms are used in
[11], [13] to solve their allocation problems. Their algorithms
are simple and efﬁcient but are not valid for complex and
dynamic scenes. Optimal policy for dynamic MDP model in
[14] is based on a fast heuristic. The state transition function
is given in detail (this is not practical in the real world)
and standard value iteration method [17] is used to obtain
the optimal policy. In [16] the optimal problem is solved
via dynamic programming [18]. In [10] similar approaches
which based on value iteration and Q function are used to
solve the MDP problem. It is worth emphasizing that the
methods used in these studies are suitable to solve discrete
dynamic programming problems and are not usbale for large
scale decision space and continuous space.

Our study considers the cache-enabled video rate allocation
MDP problem in continuous state space and action space, and
we solve it using continuous deep Q-learning approach. This
approach has three key technologies. They are continuous deep

Base Station

Base Station

Video Download

Video Playback

Base Station

Cell

Mobile User

Fig. 1: Video streaming architecture.

reinforcement learning [19]–[21], imitation learning [22], [23]
and transfer learning [24].

Continuous deep reinforcement learning: Deep reinforce-
ment learning has received considerable attention in recent
years and has been applied to a wide variety of real-world
robotic control tasks [19] including learning policies to play
go game [25]. To incorporate the beneﬁts of value function
estimation into continuous deep reinforcement learning, policy
network and value network should be established [20]. A
different point of view is put forward in [21], the authors com-
plete the task by learning a single network that outputs both
the value function and policy. We are inspired by this work
to create a deep Q-learning approach to solve our problem
without any prior knowledge about the state transition.

Imitation learning: This kind of algorithms consider the
problem of acquiring skills from observing demonstrations.
Behavioral cloning and inverse reinforcement learning are two
main lines of work within imitation learning. There two kinds
of imitation learning can improve the learning efﬁciency of
reinforcement learning [21], [23].

Transfer learning: This kind of algorithms consider the
problem of learning policies with applicability and re-use
beyond a single task. They can acquire a multitude of skills
faster than what it would take to acquire each of the skills
independently, and let the artiﬁcial intelligence revolution from
virtual to reality [24].

While there past works have led to a wide range of im-
pressive results, they consider each skill separately, and do
not fully exploit the advantages of the three key technologies.
These led us to integrate these technologies with their respec-
tive advantages and get a more powerful algorithm.

III. MODEL AND PROBLEM FORMULATION

A highway high-mobility scenario with multiple base sta-
tions (BSs) deployed is shown in Fig. 1. We consider the
downlink of a cache-enabled content-centric wireless network
which provides full coverage of all driving sections and high
data rate in each of the coverage areas. The BSs are equipped
with cache storing some subsets of video contents. We assume
the contents stored in the cache are given [26]. The video
stream is divided into N chunks and the mth chunk has a
length of ∆Tm seconds. The network consisting of F overlaid
small cell base stations and each of them is connected to
the video server via optical ﬁbers. The indices of F BSs are

orderly included in a set B = {1, 2, · · · , F }. The caching
system contains L ﬁles W = {W1, W2, · · · , WL}. The user
moving on a segment of highway which is covered by B. Both
BSs and mobile user are equipped with directional antennas.
We deﬁne the following variables to describe our problem:
state space, action space, state transition probabilities and
reward function. The state space is a ﬁnite set of states. The
action space is a ﬁnite set of actions. The state transition
probabilities is the probability that action a in state s at time t
. And the reward function is the immediate
will lead to state s
, due
reward received after transitioning from state s to state s
to action a .

′

′

A. The state space

Consider time slots indexed by T = {t1, t2, · · · , tℵ} where
ℵ is the number of slots. The time that the system stays at
each slot is ∆tx = tx+1 − tx. We assume that the network
capacity is a random variable following a certain distribution
and its value at time tx is Ctx ∈ R+. Let ntx as the requested
content and B(tx) ∈ RM as the buffer state where M is large
enough. The element bi(tx) of B(tx) is the bitrate of the ith
video chunk and it must meet two conditions [10], the ﬁrst is
FIFO constraint which means if bi = 0 then bj = 0, j > i, and
i bi < U ,
the second is buffer length constraint which means
where U is the total size of the buffer.

Let Stx denote the current state of the system at time tx,
the state Stx is the combination of Ctx , ntx and Btx . The state
space S is composed of all the possible combination of the
states.

P

S =

Stx = (Ctx, ntx, Btx)|Ctx ∈ R+, ntx ∈ W, Btx ∈ RM

n

B. The action space

o(1)

When the system is in state Stx, the set of possible actions
Atx
is the set of bit rates which would be allocated to the user
j
from the base station j. Speciﬁcally, Atx
j = (b1, b2, · · · , bk),
where bi = b is the bit rate of the ith video chunk. The bit rate
b should satisfy bmin ≤ b ≤ bmax, and the number of video
chunks that can be successfully transmitted is

k =

Ctx
b (cid:23)

(cid:22)

(2)

where ⌊ ⌋ indicates a round-down operation. Therefore, the
action space can be deﬁned as the possible combination of
Atx
j

i.e., A = ∪Atx
j .

Ctx

f reeze =

C. The state transition probabilities

The state transition probability from state Stj to state Stk

can be given by

pj,k = pj,k(Ctx , ntx, Btx )

(3)

Remark 1: Although the expression of the transition prob-
ability is given here, the mapping relation cannot be used
directly to solve problems, because it is not available in the
real world and is difﬁcult to estimate accurately.

D. The reward function

A reward function deﬁnes the goal in a reinforcement learn-
ing problem. In the rate allocation problem, the objective is to
select proper action for each system state to optimal the overall
performance of the network and video streaming quality. We
divide the reward function into two parts, cache miss cost
and video streaming quality. Cache miss cost measures the
cost of downloading data from the server. Video streaming
quality is used to measure the delay caused by video wireless
transmission.

Cache miss cost: First, we consider the overhead caused by
data download from the remote sever. Let c(ntx) denote the
cost for fetching content ntx via the backhaul link, depending
on the content size. Then the cache miss cost is given by

Ctx

n,dl = 1(ntx /∈ C)c(ntx)

(4)

where C is the cache set, 1(·) denotes the indicator function.
Video streaming quality: Now we consider the reward of
the transmission. There three kinds of rewards of it, video
playback quality utx, packet loss cost Ctx
loss and video freeze
cost Ctx
f reeze. The video playback quality is represented by the
accumulated rate of the video chunks watched in the duration
∆tx by the viewer.

utx(Stx , Atx) =

ek

bi

Xi=1

(5)

where
k is number of video chunks pushed from the buffer
in ∆tx. For the user in the cell Cj the base station Bj will
transfer the video clip of size Zj to the user, and Zj can be

e

M

given as Zj =
bm∆Tm. Assume that the buffer size of
the user at that time is U tx ≤ U , and Ttx is the time spent
emptying U tx. Then the packet loss cost and the video freeze
cost can be given by

Pm=1

M

∆Tm, if Zj + U tx > U

Pm=τ +1
0, otherwise

(6)

Ctx

loss = 




∆tx − 1(ntx /∈ C)

Tn − Ttx −

if ∆tx > 1(ntx /∈ C)

e

0, otherwise

e

τ

∆Tm,
τ

Pm=1
Tn + Ttx +

∆Tm

Pm=1

(7)





where τ is the number of video blocks that can be received
successfully by the user,
Tn is the time taken to download the
video ntx from the remote server.

Finally we combine cache miss cost and video streaming
quality as our QoE model to get the reward of the user at tx.

e

rtx
QoE =

4

Xi=1

λiri

(8)

where r1 = −Ctx
n,dl, r2 = utx, r3 = −Ctx
f reeze
and λi are associated weights for the ri. Given the cost of
each slot, the total reward throughout the whole process is

loss, r4 = −Ctx

ℵ

N

R =

j rtx
γtx

QoE

(9)

Xn=1
where N is the total number of requests sent by the user, γtx
j
is the decay factor of the reward where j ∈ B

Xx=1

E. Problem formulation

The global objective is to ﬁnd the optimal bit rate allocation
proﬁle that maximizes the system reward throughout the whole
process. The considered problem can be formulated as

ℵ

N

min
Atx
j

lim sup
L→∞

−1
Lℵ

j rtx
γtx

QoE{tx∈T,j∈B}

(10)

Xx=1

Xn=1
where L means that the system is running L times and each
time has ℵ ∈ N+ slots. Here Atx
is the optimization variable,
j
it is also action of the agent at time tx. The system total reward
is the objective function.

Optimality analysis: The cache-enabled bit rate allocation
problem (10) is an inﬁnite horizon cost MDP and it is well
known to be difﬁcult problem due to the curse of dimen-
sionality. While dynamic programming approaches represent a
systematic approach for MDPs, there are usually not practical
due to the curse of dimensionality and they generally need the
state transition probabilities which are not available in practice.
Existing works do not solve problem (10) and the optimal
allocation solution remains unknown and is highly nontrivial.
As for the problem (10), we can obtain the optimal bit rate

allocation policy π∗ by solving the Bellman equation [27].

Lemma 1 (Bellman equation :): There exist a value function

V (·) and a scalar δ satisfying :

δ + V

Stx

= max

A∈A n

π(Stx|Atx)(rtx

QoE + γtxE[V (Stx+1)])

(cid:1)

(cid:0)

o(11)
Where the expectation E[·] is with respect to the probability
distribution of the request arrival ntx and the network capacity
Ctx . δ = V ∗ is the optimal value to Problem and the optimal
policy π∗ achieving the optimal value V ∗ is given by

µ∗(Stx ) = arg max

π(Stx|Atx )(rtx

QoE + γtxE[V (Stx+1)])

A∈A n
Proof : Please refer to Appendix A.

o(12)

Algorithm design: Closed solution almost does not exist to
the inﬁnite horizon cost MDPs without any knowledge of state
transition probabilities, and standard brute-force algorithms are
usually impractical for implementation due to the curse of
dimensionality. Therefore, it is of great interest to develop low-
complexity suboptimal solutions, which can adapt to different
initial state and complex environment. A deep neural network
is built to approximate the allocation policy whose input is
any possible system state and output is appropriate action. It
has three advantages, ﬁrst it can use historical data to training
off-line, second the trained deep neural network can be used
online, third it has good generalization performance.

IV. SOLVE DECISION PROCESS PROBLEM

In this section, we ﬁrst introduce deep Q-learning that can
work in continuous space. Then we embed a special imitation
learning algorithm and a transfer learning algorithm into it to
make our deep Q-learning more efﬁcient.

Continuous deep Q-learning: The Q function Qπ(st, at)
is used in many reinforcement learning algorithms, it corre-
sponds to a policy π and is deﬁned as the expected return
after taking an action at in state st and following the policy
π thereafter.

Qπ(st, at) = Eri≥t,si>t∼E,ai>t∼π [Rt|st, at]
Q-learning
greedy
=
arg maxa Q (st, at). Let θQ parametrize the Q function
and the loss function is :

µ (st)

policy

uses

(13)

the

L

θQ
(cid:0)
where

(cid:1)

= Est∼ρβ ,at∼β,rt∼E

Q

h(cid:0)

st, at|θQ
(cid:0)

(cid:1)

− yt

2

i

(cid:1)

yt = rt

QoE (st, at) + γQ (st+1, µ (st+1))

(14)

(15)

and ρ is a stochastic distribution, β is a different stochastic
policy. We optimize the Q function by using gradient descent
with ∂L(θQ)
∂θQ :

∂L(θQ)
∂θQ

= Est∼ρβ ,at∼β,rt∼E
= Est∼ρβ ,at∼β,rt∼E

Q(st, st|θQ) − yt

2

∇θQ
h
∇θQQ(st, at|θQ)
(cid:2)

(cid:0)

(cid:1)

i
Q(st, at|θQ) − yt
(cid:0)

(cid:1)(cid:3)(16)
To use Q-learning for continuous problem, we should also
deﬁne the value function V π (st) and advantage function
Aπ (st, at):

Ra

π (st, at) Qπ (st, at) da
V π (st) =
Aπ (st, at) = Qπ (st, at) − V π (st)
Then a neural network that separately outputs a value function
term V and an advantage term A is used to represent the
Q function, and the network is parameterized as a quadratic
function [20]:

(17)

Q
A

s, a|θQ
s, a|θA
(cid:0)
(cid:0)

(cid:1)
(cid:1)

s|θV
s, a|θA
= A
2 (a − µ (s|θµ))T P
= − 1
(cid:0)
(cid:0)
(cid:1)

+ V

(a − µ (s|θµ))

s|θP
(cid:1)
(cid:0)

(cid:1)

(18)
where P(s|θP ) is a state dependent positive deﬁnite square ma-
trix, and it is parameterized by P(s|θP ) = L(s|θL)T L(s|θL),
and L(s|θL) is a lower triangular matrix. We have three
variables that are parameterized differently µ (s|θµ), V
and P(s|θP ), they are three neural network and we use target
(cid:1)
networks to combine them [28].

s|θV
(cid:0)

Imitation learning: As large amounts of on-policy expe-
rience are required in addition to good off-policy samples,
we should improve the sampling efﬁciency of the algorithm.
For the video rate allocation problem in continuous action
space some actions, for example, discrete values obtained by
sampling in the action space at equal intervals [10], are useful
but not easily sampled. For this reason we propose an imitation
learning we called it imitated sampling learning (ISL).

e

ei obeys a probability distribution β and

ei = 1.

Algorithm 1 ISL

1: Initialize: Initialize the set I = {bmin, bmin + ∆b, bmin +
and d is a ﬁxed

2∆b, · · · , bmax} where ∆b = bmax−bmin
constant.

d

2: Step 1: Sample a random minbatch of K samples

{(Si, Ai, ri, Si+1)}K

i=1 from replay buffer Σ.

3: Step 2: Using there samples get a policy πθ(Ai|Si)
4: Step 3: Using the policy πθ(Ai|Si) get new set (

i i.e.,
Si,
5: Step 4: Return aggregate dataset Σ ← Σ ∪
e

Si, A′
i)
Ai = (b, · · · , b), b ∈ I. Then get a
Ai,
e
e

and discrete A′
new dataset

Si+1)

Σ = (

ri,

Σ.

e

e

e

e

In Algorithm 1, the Initialize gets a candidate bit rate set
like [12]. Step 2 provides a learned model πθ and this model
will be used get new samples. Step 3 and Step 4 can be seen
as a sample imagination rollout [21]. Q-learning inherently
requires noisy on-policy actions to succeed. It implies that
the policy must be allowed to make its own mistakes during
training, which might involve taking undesirable. ISL can
avoid this problem while still allowing for a large amount
of on-policy exploration is to generate synthetic on-policy
trajectories under the learned model πθ.

ISL synthesizes a series of operations by discretizing At
j
and use them to get a lot of state action trajectories and add
them to the replay buffer. This will effectively augments the
amount of experience available for Q-learning, and improve
its data efﬁciency substantially.

Self-Transfer learning: Using pseudo-rewards in reinforce-
ment learning can make other auxiliary predictions that serve
to focus the agent on important aspects of the task. It also
accelerates the acquisition of a useful representation [29]. Our
QoE model will focus on different aspects when we choose
different λi. This inspired us to choose different λi to improve
the generalization ability of our agent. And we give this ability
to the original agent by using self-transfer learning (STL).

In Algorithm 2, we construct different reward set by using
different ei from a noise process N. For each reward ri, eiri
can be seen as the main part of the ri, and (1 − ei)ri is
considered to be a noise added to ri, and vice versa. This
inspired us to propose a method which called reward lifting
scheme similar to lifting-schemed wavelet transform [30] to
obtain the main components of the rewards. The reward lifting
scheme has three main steps: split, predict, and update, which
correspond to Step 3, Step 4, and Step 5 in Algorithm 2,
respectively.

STL synthesizes a series of pseudo-rewards and use them to
train a virtual agent. Then copies its knowledge to the original
agent. This can improve training efﬁciency and generalization
performance. The training method to our deep self-transfer
reinforcement learning is described in Algorithm 3.

In Algorithm 3, Γ is the total number of training times, r
is the learning rate. We use Adam [31] and the rectiﬁed non-
linearity [32] to learn the parameters for our neural network
with a learning rate of 10−3 . We set the mini-batch sizes is 64

Algorithm 2 STL

P(s, a|θ
G(r|θG).
e
e
2: Step 1: Copy
V (s|θV ).

1: Initialize: Randomly initialize normalized networks
µ(s|θeµ) and a prediction network

V (s|θ

eV ),

eP ),

P(s, a|θ

eP ) and
e

V (s|θ

eV ) to P(s, a|θP ) and

3: Step 2: Using there

e

e

µ(s|θµ) get dataset Σ

′

=

{(Si, Ai, ri = (r1, r2, ..., rd), Si+1)}.

e
4: Step 3: Split each ri: Split(ri) = (eiri, (1 − ei)ri) where

d

5: Step 4: Use G(r|θG) for prediction error: errori = (1 −
ei)ri − G(eir|θG). And train G(r|θG) using Split(ri).

Pi=1

6: Step 5: Update

ri = eiri + ((1 − ei)ri − errori)/2. And

get Σ = {(Si, Ai,
b

ri, Si+1)}.

d

Pi=1

7: Step 6: Use Σ train
update θP ← θ

V (s|θV ) and
P(s, a|θA),
b
eV .
eP and θV ← θ
e
µ(s|θµ).
8: Return P(s, a|θP ), V (s|θV ) and

e

µ(s|θµ). And

e

e

Algorithm 3 Deep Self-Transfer Reinforcement Learning for
Cache-Enabled Video Rate Allocation

s, a|θQ
(cid:0)

1: Initialize: Use STL and (18) initialize normalized Q
. And freeze P(s, a|θP ) and V (s|θV ).
= θQ.
(cid:1)

network Q
Initialize target network Q′ with weight θQ′
Initialize replay buffer Σ = ∅.
2: For episode = 1 ,· · · , Γ do:
3:

Initialize a random process N for action exploration.
Get a initial observation state st1 .
For x = 1 ,· · · , ℵ do:

Select action Atx according to the current policy and

exploration method Atx = µ (Stx|θµ) + N .
Execute Atx and observe rtx and Stx+1.
Store (Stx, Atx , rtx, Stx+1) in Σ.
If mod(episode ·ℵ + x ) = 0

Run Algorithm ISL.

End If
Sample
{(Si, Ai, ri, Si+1)}K

a

random minbatch
i=1 from Σ.

of K samples

Set yi = ri + γV
Update θQ by minimizing the loss function L =

.
(cid:17)

(cid:16)

Si+1|θQ′

1
K

K

yi − Q

Si, Ai|θQ
(cid:0)
Update the target network θQ′

(cid:1)(cid:1)

Pi=1 (cid:0)

2

= rθQ + (1 − r)θQ′

.

4:
5:

6:

7:

8:
9:

10:

11:
12:

13:

14:

15:

End For

16:
17: End For

and the replay buffer size is 106. Fig. 2 shows the workﬂow
of the deep self-transfer reinforcement learning in detail. STL
µ(s|θµ) with
is ﬁrst executed to get P(s, a|θP ), V (s|θV ) and
good generalization performance. This corresponds to the left
part of Fig. 2. When the STL is sufﬁciently trained,
the
parameters of the STL will be passed to the deep neural
network which located in the right of Fig. 2 and the ISL will
run at the appropriate time. Fig. 3 is the structure of the deep
self-transfer reinforcement learning neural network. µ (s|θµ),
and L(s|θL), each of the three neural networks has
V
three hidden layers with 128, 64 and 32 units, respectively.
The output layer of µ(s|θµ) uses Sigmoid activation function.
The training clock controls the time of knowledge transfer.

s|θV
(cid:0)

e

(cid:1)

Analysis of Algorithm 3: The training of the parameters of
the deep neural network in algorithm 3 uses Adam based on
the ﬁrst-order gradient information which makes the neural
network update sufﬁciently stable. Similar to [25], despite
lacking any theoretical convergence guarantees, our method
is able to train large neural networks using reinforcement
learning signal and gradient descent in stable manner. There
′θ and Qθ, only the
are two neural networks to calculate Q, Q
target Q-network calculates Q values according to the next
state with the target network’s output. The using of target Q-
network can cut off the dependencies of training samples and
improve stability and convergence.

A major challenge of transfer learning in different tasks is
catastrophic forgetting [33]. It means that the tendency of a
neural network to completely and abruptly forget previously
learned information upon learning new information. We teach
a learned model parameter to a new task via lateral connection
[24]. Since the trained model will be freezed in network design
(line 1 in Algorithm 3), the way parameters are optimized for
back-propagation does not affect the network that has been
learned. This kind of neural network design naturally avoids
the appearance of catastrophic forgetting.

The challenge of exploration bring us to learn a effective
policy in continuous action spaces can be treated by adding
noise sampled from a noise process N (line 6 in Algorithm
3). The main reason is the off-policies algorithms such as Al-
gorithm 3 can treat the problem of exploration independently
from the learning algorithm.

Notice that in order to solve (10) by using Bellman equation
the knowledge of the transition probability is needed, but the
formulated Markovian domain herein lacks the state transition
the traditional
mapping as state in Remark 1. Therefore,
approaches cannot be used to solve our problem. Although
we do not know the transition probability, our algorithm is
still valid, we prove this by Theorem 1.

Theorem 1 (Effectiveness): The changes in the distribution
of ntx and Ctx do not cause Algorithm 3 to fail to update
its parameters, no matter what the distribution function of ntx
and Ctx is Algorithm 3 is valid.

Proof : Please refer to Appendix B.
We can ﬁnd that the optimal policy π∗ is existed from
lemma 1. A popular measure of the performance of a rein-
forcement learning algorithm is its regret relative to executing

TABLE I
SIMULATION PARAMETERS

Parameter
Number of base station F
The length of each video chunk ∆Tm (s)
Available rates (Mb/s)
Capacity space (Mb/s)
Buffer size U (Mb)
Random process N
Average sojourn time E[Ωj] (s)

Value
20
10
{0} ∪ [2, 10]
[2, 80]
180
Gauss process
60

the optimal policy in current MDP. The regret interests in the
difference (in rewards during learning) between an optimal
policy and a reinforcement learner:

∆(π, π∗) = lim
T →∞

j rtx
γtx

QoE(Stx, Atx|π)−

T

1
T (cid:18)
T

Px=0

Px=0
j rtx
γtx

QoE(Stx, Atx |π∗)

(cid:19)

(19)

From a value function point of view (21) we can be deﬁne the
regret as:

i

Regret(π, π∗) = E

V π(S) − V π∗
h
Where the expectation E[·] is with respect to the state S. We
give a regret bound of our learning algorithm following the
assumption of the estimation errors Qπ(S, A) − V ∗(S) are
uniformly random in [−1, 1] [34].

(S)

(20)

Theorem 2 (Regret bound): Consider a state S in which
all the true optimal action values are equal at Q∗(S, A) =
V ∗(S). Suppose that the estimation errors Qπ(S, A) − V ∗(S)
are independently uniformly randomly in [−1, 1]. Then,

Regret(π, π∗) ≤ 3

(21)

Proof : Please refer to Appendix C.

V. PERFORMANCE EVALUATION

In this section, simulation results are provided to illustrate
the effectiveness of the proposed algorithm. There are another
four solutions used for comparison. Speciﬁcally, actor-critic
solution [35], normalized advantage function (NAF) without
deep neural network [36], normalized advantage function with
deep neural network [21] and random rate allocation solution.
We show the performances in terms of the buffer size and the
maximum network capacity. The Zipf distribution is used in
our scenario, which is used to describe the popularity of the
contents in user requirements. This distribute predicts that out
of a population of N elements, the frequency of elements of
rank k is:

f (k, z, N ) =

(22)

1/kz

N

(1/iz)

where z is the zipf factor, in our scenario z = 0.8 [37]. The
default parameter values are shown in Table I.

Pi=1

Fig. 4 shows the training process of our deep self-transfer
reinforcement learning. The training clock is 1500 means that
the self-transfer neural network run 1500 times. Then the deep

(cid:53)(cid:72)(cid:83)(cid:79)(cid:68)(cid:92)(cid:3)(cid:69)(cid:88)(cid:73)(cid:73)(cid:72)(cid:85)

(cid:46)(cid:81)(cid:82)(cid:90)(cid:79)(cid:72)(cid:71)(cid:74)(cid:72)
(cid:55)(cid:85)(cid:68)(cid:81)(cid:86)(cid:73)(cid:72)(cid:85)

(cid:51)(cid:68)(cid:85)(cid:68)(cid:80)(cid:72)(cid:87)(cid:72)(cid:85)(cid:86)
(cid:41)(cid:85)(cid:72)(cid:72)(cid:93)(cid:72)

(cid:54)(cid:3)(cid:15)(cid:3)(cid:68)(cid:3)(cid:15)(cid:3)(cid:85)(cid:20)(cid:3)(cid:15)(cid:3)(cid:85)(cid:21)(cid:3)(cid:15)(cid:3)(cid:17)(cid:17)(cid:17)(cid:3)(cid:15)(cid:3)(cid:85)(cid:81)(cid:3)(cid:15)(cid:3)(cid:3)(cid:3)(cid:54)(cid:10)

(cid:48)(cid:68)(cid:76)(cid:81)(cid:3)(cid:49)(cid:72)(cid:87)(cid:90)(cid:82)(cid:85)(cid:78)

(cid:54)(cid:83)(cid:79)(cid:76)(cid:87)

(cid:55)(cid:68)(cid:85)(cid:74)(cid:72)(cid:87)(cid:3)(cid:49)(cid:72)(cid:87)(cid:90)(cid:82)(cid:85)(cid:78)

(cid:47)

(cid:541)

(cid:57)

(cid:72)(cid:20)(cid:85)(cid:3)(cid:20)(cid:3)(cid:15)(cid:3)(cid:17)(cid:17)(cid:17)(cid:15)(cid:3)(cid:72)(cid:81)(cid:85)(cid:3)(cid:81)

(cid:47)

(cid:541)

(cid:57)

(cid:51)(cid:3)(cid:32)(cid:3)(cid:47)(cid:55)(cid:47)

(cid:51)(cid:85)(cid:72)(cid:71)(cid:76)(cid:70)(cid:87)(cid:76)(cid:82)(cid:81)(cid:3)
(cid:49)(cid:72)(cid:87)(cid:90)(cid:82)(cid:85)(cid:78)

(cid:39)(cid:76)(cid:86)(cid:70)(cid:85)(cid:72)(cid:87)(cid:76)(cid:93)(cid:76)(cid:81)(cid:74)
(cid:44)(cid:32)(cid:94)(cid:69)(cid:20)(cid:15)(cid:69)(cid:21)(cid:15)(cid:17)(cid:17)(cid:17)(cid:15)(cid:69)(cid:80)(cid:68)(cid:91)(cid:96)

(cid:40)(cid:91)(cid:72)(cid:70)(cid:88)(cid:87)(cid:76)(cid:82)(cid:81)(cid:3)(cid:68)(cid:70)(cid:87)(cid:76)(cid:82)(cid:81)(cid:3)
(cid:68)(cid:10)

(cid:68)(cid:10)(cid:3)(cid:32)(cid:3)(cid:69)(cid:76)

(cid:54)(cid:3)(cid:15)(cid:3)(cid:68)(cid:3)(cid:15)(cid:3)(cid:85)(cid:3)(cid:15)(cid:3)(cid:54)(cid:10)

(cid:60)(cid:72)(cid:86)

(cid:44)(cid:80)(cid:76)(cid:87)(cid:68)(cid:87)(cid:76)(cid:82)(cid:81)(cid:3)
(cid:47)(cid:72)(cid:68)(cid:85)(cid:81)(cid:76)(cid:81)(cid:74)(cid:3)(cid:731)

(cid:54)(cid:3)(cid:15)(cid:3)(cid:68)(cid:10)(cid:3)(cid:15)(cid:3)(cid:85)(cid:10)(cid:3)(cid:15)(cid:3)(cid:54)(cid:10)(cid:10)

(cid:53)(cid:72)(cid:83)(cid:79)(cid:68)(cid:92)(cid:3)(cid:69)(cid:88)(cid:73)(cid:73)(cid:72)(cid:85)

(cid:36)(cid:74)(cid:74)(cid:85)(cid:72)(cid:74)(cid:68)(cid:87)(cid:72)

(cid:54)(cid:3)(cid:15)(cid:3)(cid:68)(cid:3)(cid:15)(cid:3)(cid:85)(cid:3)(cid:15)(cid:3)(cid:54)(cid:10)

(cid:48)(cid:68)(cid:76)(cid:81)(cid:3)(cid:49)(cid:72)(cid:87)(cid:90)(cid:82)(cid:85)(cid:78)

(cid:55)(cid:68)(cid:85)(cid:74)(cid:72)(cid:87)(cid:3)(cid:49)(cid:72)(cid:87)(cid:90)(cid:82)(cid:85)(cid:78)

(cid:16)(cid:11)(cid:68)(cid:3)(cid:16)(cid:3)(cid:541)(cid:12)(cid:55)(cid:51)(cid:11)(cid:68)(cid:3)(cid:16)(cid:3)(cid:541)(cid:12)(cid:18)(cid:21)

(cid:72)(cid:85)(cid:85)(cid:82)(cid:85)(cid:20)(cid:3)(cid:15)(cid:3)(cid:17)(cid:17)(cid:17)(cid:3)(cid:15)(cid:3)(cid:72)(cid:85)(cid:85)(cid:82)(cid:85)(cid:81)(cid:3)

(cid:47)

(cid:541)

(cid:57)

(cid:47)

(cid:541)

(cid:57)

(cid:36)

(cid:57)(cid:3)

(cid:14)

(cid:52)(cid:3)

(cid:56)(cid:83)(cid:71)(cid:68)(cid:87)(cid:72)

(cid:85)(cid:3)

(cid:57)(cid:3)(cid:10)

(cid:47)(cid:82)(cid:86)(cid:86)(cid:3)(cid:73)(cid:88)(cid:81)(cid:70)(cid:87)(cid:76)(cid:82)(cid:81)
(cid:45)(cid:3)(cid:32)(cid:3)(cid:62)(cid:11)(cid:3)(cid:85)(cid:3)(cid:14)(cid:3)(cid:534)(cid:57)(cid:3)(cid:10)(cid:3)(cid:12)(cid:3)(cid:16)(cid:3)(cid:52)(cid:3)(cid:64)(cid:21)

(cid:51)(cid:3)(cid:32)(cid:3)(cid:47)(cid:55)(cid:47)

(cid:16)(cid:11)(cid:68)(cid:3)(cid:16)(cid:3)(cid:541)(cid:12)(cid:55)(cid:51)(cid:11)(cid:68)(cid:3)(cid:16)(cid:3)(cid:541)(cid:12)(cid:18)(cid:21)

(cid:57)(cid:3)

(cid:14)
(cid:52)(cid:3)

(cid:36)

(cid:57)(cid:3)(cid:10)

(cid:47)(cid:82)(cid:86)(cid:86)(cid:3)(cid:73)(cid:88)(cid:81)(cid:70)(cid:87)(cid:76)(cid:82)(cid:81)
(cid:45)(cid:3)(cid:32)(cid:3)(cid:62)(cid:11)(cid:3)(cid:85)(cid:3)(cid:14)(cid:3)(cid:534)(cid:57)(cid:3)(cid:10)(cid:3)(cid:12)(cid:3)(cid:16)(cid:3)(cid:52)(cid:3)(cid:64)(cid:21)

Fig. 2: Deep self-transfer reinforcement learning.

(cid:47)

(cid:541)

(cid:57)

(cid:47)

(cid:541)

(cid:57)

Training Clock

(cid:39)(cid:72)(cid:72)(cid:83)(cid:3)(cid:49)(cid:72)(cid:88)(cid:85)(cid:68)(cid:79)(cid:3)
(cid:49)(cid:72)(cid:87)(cid:90)(cid:82)(cid:85)(cid:78)

(cid:55)(cid:85)(cid:68)(cid:76)(cid:81)(cid:76)(cid:81)(cid:74)(cid:3)
(cid:38)(cid:79)(cid:82)(cid:70)(cid:78)

(cid:51)(cid:68)(cid:85)(cid:68)(cid:80)(cid:72)(cid:87)(cid:72)(cid:85)(cid:86)(cid:3)
(cid:41)(cid:85)(cid:72)(cid:72)(cid:93)(cid:72)

(cid:46)(cid:81)(cid:82)(cid:90)(cid:79)(cid:72)(cid:71)(cid:74)(cid:72)
(cid:55)(cid:85)(cid:68)(cid:81)(cid:86)(cid:73)(cid:72)(cid:85)

Fig. 3: The structure of deep self-transfer reinforcement learn-
ing neural network.

reinforcement neural network start work. We can ﬁnd that
compared with the deep neural network without STL, using
knowledge transfer make our agent get higher rewards.

We gain insight

into the convergence behaviors of our
approach. We compare it to other methods in terms of per-
formance. Fig. 2 shows the convergence behavior of our
algorithm. After 2000 training, the agent has mastered the
knowledge of the whole network system. We can ﬁnd that
our proposed algorithm is obviously better than the another
four methods.

We let the buffer size change from 80Mb to 230Mb to
compare the effect of the buffer size of the ﬁve methods.
Fig. 3 demonstrates the impact of the buffer size on our
approach. It can be found that as the buffer size increases,
the average reward gained by the user is increasing because

Fig. 4: Training of deep self-transfer reinforcement learning.

buffer size becomes larger means that more video chunks can
be downloaded at the appropriate time. However, it is noticed
that this performance improvement is becoming smaller as
the buffer size becomes larger, which means that the average
reward tends to converge as the buffer size increases. We can
see from Fig. 4 that in the process of increasing buffer size,
our algorithm is always better than the other four algorithms.
Fig. 5 demonstrates the impact of the maximum network
capacity on the ﬁve approaches. In order to compare the
performance of different algorithms, we set the user’s storage
space as 180Mb. It is shown that as the maximum network
capacity increase, our algorithm has always been able to
maintain a higher QoE than the another three methods it
can also maintain a higher QoE when the maximum network
capacity is low.

VI. CONCLUSION

Existing researches on cache-enabled video rate adaptive
allocation do not take into account the continuous system
capacity and candidate rate. In this paper, we leverage MDP

Fig. 5: Convergence behavior.

Fig. 6: The impact of buffer size on our algorithm

Fig. 7: Performance comparison with different buffer size

Fig. 8: Performance comparison with different maximum net-
work capacity

to model the cache-enabled video rate allocation problem and
propose a QoE-driven dynamic video rate adaptation method
via deep continuous Q-learning scheme. We study the impacts
of the buffer size and the maximum network capacity on our
approach, and the performance evaluation results have shown
that our approach can greatly improve the QoE. Future work
is going to consider a more efﬁcient scheme by adding more
prior information to the model.

APPENDIX A
PROOF OF LEMMA 1

By Propositions 4.2.1, 4.2.3, and 4.2.5 in [27], the optimal
system reward of problem (10) is the same for all initial
Stx
) to the following Bellman
states and the solution (δ, V
equation exists.
(cid:1)

(cid:0)

δ + V (S) =

π(S|A)(rtx

QoE + γtx

max
A∈A




XS′ ∈S

p(S′

|S, A)V (S′

The transition probability is given by



(23)

))




|S, A) ∆= p(Stx+1 = S′

p(S′
= En,C [p(Stx+1 = S′

|Stx = S, Atx = A)

|Stx = S, Atx = A), ntx = n, Ctx = C]
(24)

Substituting (31) into (30) leads to (11).

APPENDIX B
PROOF OF THEOREM 1

Since ntx and Ctx affect the state transition probabilities,
x=0 γtxp(Stx = S|St0 , A). Then use policy
we let dµ(S) =
gradient [38] and cost function L (line 14 in Algorithm 3) we
get

P

∞

∂L
∂θ

=

dµ(S)

∂µ(S, A)
∂θ

[Aµ(S, A) + V µ(S)]

(25)

XS

XA
The gradient is no terms of the form ∂dµ(S)
, so the effect of
policy changes on the distribution of states does not appear.

∂θ

APPENDIX C
PROOF OF THEOREM 2

Deﬁne σA = Qπ(S, A)−Qπ∗

(S, A). σA is a uniform random
variable in [−1, 1]. As we us the Sigmoid function as the
activation function, the output of µ(S) belong to [0, 1]. We
quantize the interval [0, 1] into m values at the interval 1/m.
Thus we get m discrete actions. When m tends to inﬁnity we
can get continuous actions. Because the estimation errors are
independent, we can derive

P (max

A

σA ≤ σ) = P (σ1 ≤ σ, σ2 ≤ σ, ..., σm ≤ σ)

m

=

P (σi ≤ σ)

Yi=1

(26)

The function P (σi ≤ σ) is the cumulative distribution function
of σi, which here is deﬁned as

P (σi ≤ σ) = 


0, if σ ≤ 1
(1 + σ)/2, if 0<σ < 1
1, if σ ≥ 1

(27)

Then we get that



m

P (max

A

σA ≤ σ) =

P (σi ≤ σ)

Yi=1

0, if σ ≤ −1
[(1 + σ)/2]m , if − 1 < σ < 1
1, if σ > 1

= 



The expectation of max

A

σA can be given as

(28)

E[max

A

σA] =

1

Z
−1

σfσ(σ)dσ

(29)

where fσ is the probability density function of σ, it can be
2 [(1 + σ)/2]m−1. Note that we use
written as fσ(σ) = m
Sigmoid function as the activation function of the µ (S) and
L(S). Thus

P(S) = L(S)T L(S) ≤ 1.

(30)

Aπ (S, A) = −

1
2
This implies that

(A − µ (S))T P (S) (A − µ (S)) ≥ −2 (31)

V π(S) − V π∗
E
= E
(cid:2)
≤ E
(cid:2)
= E[max
(cid:2)

(S)
Qπ(S, A) − Aπ(S) − V π∗
(cid:3)
Qπ(S, A) − V π∗
+ 2
σA] + 2

(S)
(cid:3)

A

(S)
(cid:3)

(32)

[(1 + σ)/2]m mσ−1
m+1

=
h
= m−1
m+1 + 2

i (cid:12)
(cid:12)
(cid:12)
(cid:12)

1

−1

+ 2

In our cache-enabled bit rate allocation problem, the actions
lie in the continuous space thus when m → ∞

V π(S) − V π∗

E

h

≤ 3

(S)
i

(33)

We complete the proof.

REFERENCES

[1] K. Zheng, L. Zhao, J. Mei, and M. Dohler, “10 gb/s hetsnets with
millimeter-wave communications: access and networking - challenges
and protocols,” IEEE Communications Magazine, vol. 53, no. 1, pp.
222–231, 2015.

[2] M. Jaber, M. A. Imran, R. Tafazolli, and A. Tukmanov, “5g backhaul
challenges and emerging research directions: A survey,” IEEE Access,
vol. 4, pp. 1743–1766, 2016.

[6] Z. Li, X. Zhu, J. Gahm, R. Pan, H. Hu, A. C. Begen, and D. Oran,
“Probe and adapt: Rate adaptation for http video streaming at scale,”
IEEE Journal on Selected Areas in Communications, vol. 32, no. 4, pp.
719–733, 2014.

[7] J. Qiao, Y. He, and X. S. Shen, “Proactive caching for mobile video
streaming in millimeter wave 5g networks,” IEEE Transactions on
Wireless Communications, vol. 15, no. 10, pp. 7187–7198, Oct 2016.

[8] B. Zhou, Y. Cui, and M. Tao, “Optimal dynamic multicast scheduling for
cache-enabled content-centric wireless networks,” IEEE Transactions on
Communications, vol. 65, no. 7, pp. 2956–2970, July 2017.

[9] A. Liu and V. K. N. Lau, “Cache-enabled opportunistic cooperative
mimo for video streaming in wireless systems,” IEEE Transactions on
Signal Processing, vol. 62, no. 2, pp. 390–402, Jan 2014.

[10] Y. Chen, F. Zhang, K. Wu, and Q. Zhang, “Qoe-aware dynamic video
rate adaptation,” in IEEE Global Communications Conference, 2016, pp.
1–6.

[11] J. Xie, R. Xie, T. Huang, J. Liu, and Y. Liu, “Energy-efﬁcient cache
resource allocation and qoe optimization for http adaptive bit rate stream-
ing over cellular networks,” in 2017 IEEE International Conference on
Communications (ICC), May 2017, pp. 1–6.

[12] Y. Wang, X. Zhou, M. Sun, L. Zhang, and X. Wu, “A new qoe-driven
video cache management scheme with wireless cloud computing in
cellular networks,” Mobile Networks & Applications, vol. 22, no. 1, pp.
72–82, 2017.

[13] W. Zhang, Y. Wen, Z. Chen, and A. Khisti, “Qoe-driven cache manage-
ment for http adaptive bit rate streaming over wireless networks,” IEEE
Transactions on Multimedia, vol. 15, no. 6, pp. 1431–1445, Oct 2013.
[14] W. Bao and S. Valentin, “Bitrate adaptation for mobile video streaming
based on buffer and channel state,” in IEEE International Conference
on Communications, 2015, pp. 3076–3081.

[15] A. Bokani, S. A. Hoseini, M. Hassan, and S. S. Kanhere, “Implemen-
tation and evaluation of adaptive video streaming based on markov
decision process,” in ICC 2016 - 2016 IEEE International Conference
on Communications, 2016, pp. 1–6.

[16] M. Pesce, D. Munaretto, and M. Zorzi, “A markov decision model for
source video rate allocation and scheduling policies in mobile networks,”
in Ad Hoc NETWORKING Workshop, 2014, pp. 119–125.

[17] M. L. Puterman, Markov decision processes: discrete stochastic dynamic

programming.

John Wiley & Sons, 2014.

[18] D. P. Bertsekas, D. P. Bertsekas, D. P. Bertsekas, and D. P. Bertsekas,
Dynamic programming and optimal control. Athena scientiﬁc Belmont,
MA, 1995, vol. 1, no. 2.

[19] T. P. Lillicrap, J. J. Hunt, A. Pritzel, N. Heess, T. Erez, Y. Tassa,
D. Silver, and D. Wierstra, “Continuous control with deep reinforcement
learning,” Computer Science, vol. 8, no. 6, p. A187, 2015.

[20] J. Schulman, S. Levine, P. Moritz, M. I. Jordan, and P. Abbeel, “Trust

region policy optimization,” Computer Science, pp. 1889–1897, 2015.

[21] S. Gu, T. Lillicrap, I. Sutskever, and S. Levine, “Continuous deep q-
learning with model-based acceleration,” in International Conference
on Machine Learning, 2016, pp. 2829–2838.

[22] S. Schaal, “Is imitation learning the route to humanoid robots?” Trends

in Cognitive Sciences, vol. 3, no. 6, p. 233, 1999.

[23] Y. Duan, M. Andrychowicz, B. C. Stadie, J. Ho, J. Schneider,
I. Sutskever, P. Abbeel, and W. Zaremba, “One-shot imitation learning,”
2017.

[24] A. A. Rusu, N. C. Rabinowitz, G. Desjardins, H. Soyer, J. Kirkpatrick,
K. Kavukcuoglu, R. Pascanu, and R. Hadsell, “Progressive neural
networks,” 2016.

[25] D. Silver, A. Huang, C. J. Maddison, A. Guez, L. Sifre, G. V. D.
Driessche, J. Schrittwieser,
I. Antonoglou, V. Panneershelvam, and
M. Lanctot, “Mastering the game of go with deep neural networks and
tree search,” Nature, vol. 529, no. 7587, p. 484, 2016.

[26] D. Liu and C. Yang, “Energy efﬁciency of downlink networks with
caching at base stations,” IEEE Journal on Selected Areas in Commu-
nications, vol. 34, no. 4, pp. 907–922, 2015.

[27] D. P. Bertsekas, Dynamic Programming and Optimal Control. Athena

[3] C. V. N. Index, “Global mobile data trafﬁc forecast update, 2012-2017,”

Scientiﬁc, 1995.

2013.

[4] N. Carlsson, D. Eager, V. Krishnamoorthi, and T. Polishchuk, “Opti-
mized adaptive streaming of multi-video stream bundles,” IEEE Trans-
actions on Multimedia, vol. 19, no. 7, pp. 1637–1653, 2017.

[5] S. Akhshabi, A. C. Begen, and C. Dovrolis, “An experimental evaluation
of rate-adaptation algorithms in adaptive streaming over http,” in ACM
Conference on Multimedia Systems, 2011, pp. 157–168.

[28] V. Mnih, K. Kavukcuoglu, D. Silver, A. A. Rusu, J. Veness, M. G.
Bellemare, A. Graves, M. Riedmiller, A. K. Fidjeland, and G. Ostrovski,
“Human-level control through deep reinforcement learning.” Nature, vol.
518, no. 7540, p. 529, 2015.

[29] M. Jaderberg, V. Mnih, W. M. Czarnecki, T. Schaul, J. Z. Leibo, D. Sil-
ver, and K. Kavukcuoglu, “Reinforcement learning with unsupervised
auxiliary tasks,” 2016.

[30] C. Chrysaﬁs and A. Ortega, “Line-based, reduced memory, wavelet
image compression,” IEEE Transactions on Image Processing, vol. 9,
no. 3, pp. 378–389, Mar 2000.

[31] D. P. Kingma and J. Ba, “Adam: A method for stochastic optimization,”

Computer Science, 2014.

[32] X. Glorot, B. Antoine, and Y. Bengio, “Deep sparse rectiﬁer networks,”

Learning/statistics & Optimisation, 2010.

[33] M. Mccloskey and N. J. Cohen, “Catastrophic interference in con-
nectionist networks: The sequential learning problem,” Psychology of
Learning & Motivation, vol. 24, pp. 109–165, 1989.

[34] S. Thrun and A. Schwartz, “Issues in using function approximation
for reinforcement learning,” Proceedings of the Fourth Connectionist
Models Summer School, 1993.

[35] D. P. Bertsekas, J. N. Tsitsiklis, and A. Volgenant, “Neuro-dynamic
programming,” Encyclopedia of Optimization, vol. 27, no. 6, pp. 1687–
1692, 2011.

[36] S. Gu, E. Holly, T. Lillicrap, and S. Levine, “Deep reinforcement
learning for robotic manipulation with asynchronous off-policy updates,”
in 2017 IEEE International Conference on Robotics and Automation
(ICRA), May 2017, pp. 3389–3396.

[37] M. Cha, H. Kwak, P. Rodriguez, Y. Y. Ahn, and S. Moon, “Analyzing
the video popularity characteristics of large-scale user generated content
systems,” IEEE/ACM Transactions on Networking, vol. 17, no. 5, pp.
1357–1370, Oct 2009.

[38] R. S. Sutton, “Policy gradient methods for reinforcement learning with
function approximation,” Submitted to Advances in Neural Information
Processing Systems, vol. 12, pp. 1057–1063, 1999.

