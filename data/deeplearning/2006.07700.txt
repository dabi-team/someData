1
2
0
2

l
u
J

9
2

]

R
C
.
s
c
[

3
v
0
0
7
7
0
.
6
0
0
2
:
v
i
X
r
a

Defensive Approximation: Securing CNNs using Approximate
Computing

Amira Guesmi
amira.guesmi@stud.enis.tn
ENIS, University of Sfax
Tunisia

Mouna Baklouti
ENIS, University of Sfax
Tunisia

Ihsen Alouani
Ihsen.Alouani@uphf.fr
IEMN, Polytechnic University
Hauts-De-France
France

Tarek Frikha
ENIS, University of Sfax
Tunisia

Nael Abu-Ghazaleh
University of California Riverside
USA

Khaled N. Khasawneh
kkhasawn@gmu.edu
George Mason University
USA

Mohamed Abid
ENIS, University of Sfax
Tunisia

ABSTRACT
In the past few years, an increasing number of machine-learning
and deep learning structures, such as Convolutional Neural Net-
works (CNNs), have been applied to solving a wide range of real-life
problems. However, these architectures are vulnerable to adversar-
ial attacks: inputs crafted carefully to force the system output to a
wrong label. Since machine-learning is being deployed in safety-
critical and security-sensitive domains, such attacks may have cata-
strophic security and safety consequences. In this paper, we propose
for the first time to use hardware-supported approximate comput-
ing to improve the robustness of machine learning classifiers. We
show that our approximate computing implementation achieves
robustness across a wide range of attack scenarios. Specifically, we
show that successful adversarial attacks against the exact classi-
fier have poor transferability to the approximate implementation.
The transferability is even poorer for the black-box attack scenar-
ios, where adversarial attacks are generated using a proxy model.
Surprisingly, the robustness advantages also apply to white-box at-
tacks where the attacker has unrestricted access to the approximate
classifier implementation: in this case, we show that substantially
higher levels of adversarial noise are needed to produce adversarial
examples. Furthermore, our approximate computing model main-
tains the same level in terms of classification accuracy, does not
require retraining, and reduces resource utilization and energy con-
sumption of the CNN. We conducted extensive experiments on
a set of strong adversarial attacks; We empirically show that the
proposed implementation increases the robustness of a LeNet-5
and an Alexnet CNNs by up to 99% and 87%, respectively for strong
transferability-based attacks along with up to 50% saving in energy

Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation
on the first page. Copyrights for components of this work owned by others than ACM
must be honored. Abstracting with credit is permitted. To copy otherwise, or republish,
to post on servers or to redistribute to lists, requires prior specific permission and/or a
fee. Request permissions from permissions@acm.org.
ASPLOS â€™21, April 19â€“23, 2021, Virtual, USA
Â© 2021 Association for Computing Machinery.
ACM ISBN 978-1-4503-8317-2/21/04. . . $15.00
https://doi.org/10.1145/3445814.3446747

consumption due to the simpler nature of the approximate logic.
We also show that a white-box attack requires a remarkably higher
noise budget to fool the approximate classifier, causing an average
of 4 ğ‘‘ğµ degradation of the PSNR of the input image relative to the
images that succeed in fooling the exact classifier.

CCS CONCEPTS
â€¢ Computer systems organization â†’ Embedded systems; Re-
dundancy; Robotics; â€¢ Networks â†’ Network reliability.

KEYWORDS
Deep neural network, adversarial example, security, approximate
computing.

ACM Reference Format:
Amira Guesmi, Ihsen Alouani, Khaled N. Khasawneh, Mouna Baklouti,
Tarek Frikha, Mohamed Abid, and Nael Abu-Ghazaleh. 2021. Defensive
Approximation: Securing CNNs using Approximate Computing. In Proceed-
ings of the 26th ACM International Conference on Architectural Support for
Programming Languages and Operating Systems (ASPLOS â€™21), April 19â€“23,
2021, Virtual, USA. ACM, New York, NY, USA, 14 pages. https://doi.org/10.
1145/3445814.3446747

1 INTRODUCTION
Convolutional neural networks (CNNs) and other deep learning
structures provide state-of-the-art performance in many application
domains, such as computer vision [58, 62], natural language process-
ing [16], robotics [56], autonomous driving [3], and healthcare [43].
With the rapid progress in CNNâ€™s development and deployment,
they are increasing concerns about their vulnerability to adversarial
attacks: maliciously designed imperceptible perturbations injected
within the data that cause CNNs to misclassify. Adversarial attacks
have been demonstrated in real-world scenarios [19, 31, 69], mak-
ing this vulnerability a serious threat to safety-critical and other
applications that rely on CNNs.

Several software-based defenses have been proposed against
Adversarial Machine Learning (AML) attacks [46, 48, 53], but more
advanced attack strategies [10, 40] also continue to evolve that
demonstrate vulnerability of some of these defenses. Moreover,

 
 
 
 
 
 
ASPLOS â€™21, April 19â€“23, 2021, Virtual, USA

Amira Guesmi, Ihsen Alouani, Khaled N. Khasawneh, Mouna Baklouti, Tarek Frikha, Mohamed Abid, and Nael Abu-Ghazaleh

many of the proposed defenses introduce substantial overheads
to either the training or the inference operation of CNNs [46, 70].
These overheads increase the computational requirements of these
systems, which is already a significant challenge driving substan-
tial research into algorithmic and hardware techniques to improve
CNN performance and energy-efficiency [17]. Thus, finding new
approaches to harden systems against AML without heavy over-
heads in both design-time and run-time is an area of substantial
need.

In this paper, we propose a new hardware based approach to
improve the robustness of machine learning (ML) classifiers. Specif-
ically, we show that Approximate Computing (AC), designed to
improve the performance and power consumption of algorithms
and processing elements, can substantially improve CNN robust-
ness to AML. Our technique, which we call defensive approximation
(DA), substantially enhances the robustness of CNNs to adversarial
attacks. We show that for a variety of attack scenarios, and utiliz-
ing a range of algorithms for generating adversarial attacks, DA
provides substantial robustness even under the assumptions of a
powerful attacker with full access to the internal classifier structure.
Importantly, DA does not require retraining or fine-tuning, allowing
pre-trained models to benefit from its robustness and performance
advantages by simply replacing the exact multiplier implementa-
tions with approximate ones. The approximate classifier achieves
similar accuracy to the exact classifier for Lenet-5 and Alexnet.

DA also benefits from the conventional advantages of AC, re-
sulting in a less complex design that is both faster and more energy
efficient. Other defenses such as Defensive Quantization (DQ) [36]
also provide energy efficiency benefits in addition to robustness.
However, we show that because it is input-dependent DA achieves
twice higher robustness to attacks than DQ.

We carry out several experiments to better understand the ro-
bustness advantages of DA. We show that the unpredictable and
input-dependent variations introduced by AC improve the CNN
resilience to adversarial perturbations. Experimental results show
that DA has a confidence enhancement impact on non-adversarial
examples; we believe that this is due to our AC multiplier which
adds input dependent approximation with generally higher magni-
tude at large multiplication values. In fact, the AC-induced noise in
the convolution layer is shown to be higher in absolute value when
the input matrix is highly correlated to the convolution filter, and by
consequence highlights further the features. This observation at the
feature map propagates through the model and results in enhanced
classification confidence, i.e., the difference between the 1ğ‘ ğ‘¡ class
and the "runner-up". Intuitively and as shown by prior work [14],
enhancing the confidence furthers the classifierâ€™s robustness.At
the same time, we observe negligible accuracy loss compared to a
conventional CNN implementation on non-adversarial inputs while
providing considerable power savings.

In summary, the contributions of the paper are:

â€¢ We build an aggressively approximate floating point mul-
tiplier that injects data-dependent noise within the convo-
lution calculation. Subsequently, we used this approximate
multiplier to implement an approximate CNN hardware ac-
celerator (Section 4.3).

â€¢ To the best of our knowledge, we are the first to lever-
age AC to enhance CNN robustness to adversarial attacks
without the need for re-training, fine-tuning, nor input pre-
processing. We investigate the capacity of AC to help de-
fending against adversarial attacks in Section 5.3.

â€¢ We empirically show that the proposed approximate imple-
mentation reduces the success rate of adversarial attacks by
an average of 87% and 71.5% in Lenet-5 and Alexnet CNNs
respectively.

â€¢ We evaluate the approximate classifiers against powerful
attackers with white-box access. We observe that attackers
require substantially higher adversarial perturbations to fool
the approximate classifier.

We believe that DA is highly practical; it can be deployed without
retraining or fine-tuning, achieving comparable classification per-
formance to exact classifiers. In addition to security advantages,
DA improves performance by reducing latency and energy making
it an attractive choice even in Edge device settings (Appendix 8.2).

2 BACKGROUND
This section first presents an overview of adversarial attacks fol-
lowed by introducing AC.

2.1 Adversarial Attacks
Deep learning techniques gained popularity in recent years and
are now deployed even in safety-critical tasks, such as recognizing
road signs for autonomous vehicles [13]. Despite their effective-
ness and popularity, CNN-powered applications are facing a critical
challenge â€“ adversarial attacks. Many studies [20, 24, 65, 69] have
shown that CNNs are vulnerable to carefully crafted inputs de-
signed to fool them, very small imperceptible perturbations added
to the data can completely change the output of the model. In com-
puter vision domain, these adversarial examples are intentionally
generated images that look almost exactly the same as the original
images, but can mislead the classifier to provide wrong prediction
outputs. Other work [38] claimed that adversarial examples are not
a practical threat to ML in real-life scenarios. However, physical
adversarial attacks have recently been shown to be effective against
CNN based applications in real-world [7].
Minimizing Injected Noise: Its essential for the adversary to
minimize the added noise to avoid detection. For illustration pur-
poses, consider a CNN used for image classification. More formally,
given an original input image ğ‘¥ and a target classification model
ğ‘“ () ğ‘ .ğ‘¡ . ğ‘“ (ğ‘¥) = ğ‘™, the problem of generating an adversarial example
ğ‘¥ âˆ— can be formulated as a constrained optimization [71]:

ğ‘¥ âˆ— = arg min

ğ‘¥ âˆ—

D (ğ‘¥, ğ‘¥ âˆ—), ğ‘ .ğ‘¡ . ğ‘“ (ğ‘¥ âˆ—) = ğ‘™ âˆ—, ğ‘™ â‰  ğ‘™ âˆ—

(1)

Where D is the distance metric used to quantify the similarity
between two images and the goal of the optimization is to minimize
this added noise, typically to avoid detection of the adversarial
perturbations. ğ‘™ and ğ‘™ âˆ— are the two labels of ğ‘¥ and ğ‘¥ âˆ—, respectively:
ğ‘¥ âˆ— is considered as an adversarial example if and only if the label
of the two images are different (ğ‘“ (ğ‘¥) â‰  ğ‘“ (ğ‘¥ âˆ—)) and the added noise
is bounded (D (ğ‘¥, ğ‘¥ âˆ—) < ğœ– where ğœ– â©¾ 0).
Distance Metrics: The adversarial examples and the added pertur-
bations should be visually imperceptible by humans. Since it is hard

Defensive Approximation: Securing CNNs using Approximate Computing

ASPLOS â€™21, April 19â€“23, 2021, Virtual, USA

to model humansâ€™ perception, researchers proposed three metrics
to approximate humansâ€™ perception of visual difference, namely ğ¿0,
ğ¿2, and ğ¿âˆ [10]. These metrics are special cases of the ğ¿ğ‘ norm:

âˆ¥ğ‘¥ âˆ¥ğ‘ =

(cid:33) 1

ğ‘

|ğ‘¥ğ‘– |ğ‘

(cid:32) ğ‘›
âˆ‘ï¸

ğ‘–=1

(2)

These three metrics focus on different aspects of visual significance.
ğ¿0 counts the number of pixels with different values at correspond-
ing positions in the two images. ğ¿2 measures the Euclidean distance
between the two images ğ‘¥ and ğ‘¥ âˆ—. ğ¿âˆ measures the maximum dif-
ference for all pixels at corresponding positions in the two images.
The consequences of an adversarial attack can be dramatic. For
example, misclassification of a stop sign as a yield sign or a speed
limit sign could lead to material and human damages. Another
possible situation is when using CNNs in financial transactions and
automatic bank check processing â€“ using handwritten character
recognition algorithms to read digits from bank cheques or using
neural networks for amount and signature recognition [42]. An
attacker could easily fool the model to predict wrong bank account
numbers or amount of money or even fake a signature. A dangerous
situation, especially with such large sums of money at stake.

2.2 Approximate Computing
The speed of new generations of computing systems, from embed-
ded and mobile devices to servers and computing data centers, has
been drastically climbing in the past decades. This development was
made possible by the advances in integrated circuits (ICs) design
and driven by the increasingly high demand for performance in
the majority of modern applications. However, this development
is physically reaching the end of Mooreâ€™s law, since TSMC and
Samsung are releasing 5 ğ‘›ğ‘š technology [44]. On the other hand,
a wide range of modern applications is inherently fault-tolerant
and may not require the highest accuracy. This observation has
motivated the development of approximate computing (AC), a com-
puting paradigm that trades power consumption with accuracy.
The idea is to implement inexact/AC elements that consume less
energy, as far as the overall application tolerates the imprecision
level in computation. This paradigm has been shown promising
for inherently fault-tolerant applications such as deep/ML, big data
analytics, and signal processing. Several AC techniques have been
proposed in the literature and can be classified into three main
categories based on the computing stack layer they target: software,
architecture, and circuit level [6].

In this paper, we consider AC for a totally new objective; en-
hancing CNNs robustness to adversarial attacks, without losing the
initial advantages of AC.

3 THREAT MODEL
We assume an attacker attempting to conduct adversarial attacks
to fool a classifier in a variety of attack scenarios.

3.1 Adversary Knowledge (Attacks Scenarios)
In this work, we consider three attack scenarios:
Transferability Attack. We assume the adversary is aware of the
exact classifier internal model; its architecture and parameters. The

adversary uses the exact classifier to create adversarial examples.
Thus, we explore whether these examples transfer effectively to
the approximate classifier (DA classifier).
Black-box Attack. We assume the attacker has access only to the
input/output of the victim classifier (which is our approximate
classifier) and has no information about its internal architecture.
The adversary first uses the results of querying the victim to reverse
engineer the classifier and create a substitute CNN model. With
the substitute model, the attacker can attempt to generate different
adversarial examples to attack the victim classifier.
White-box Attack. We assume a powerful attacker who has full
knowledge of the victim classifierâ€™s architecture and parameters
(including the fact that it uses AC). The attacker uses this knowledge
to create adversarial examples.

3.2 Adversarial Example Generation
We consider several adversarial attack generation algorithms for
our attack scenarios, including some of the most recent and potent
evasion attacks. Generally, in each algorithm, the attacker tries to
evade the system by adjusting malicious samples during the infer-
ence phase, assuming no influence over the training data. However,
as different defenses have started to be deployed that specifically
target individual adversarial attack generation strategies, new algo-
rithms have started to be deployed that bypass these defenses. For
example, methods such as defensive distillation [53] and automa-
tion detection [25] were introduced and demonstrate robustness
against the Fast gradient sign attack [20]. However, the new ğ¶&ğ‘Š
attack was able to bypass these defenses [10]. Thus, demonstrating
robustness against a range of these attacks provides confidence that
a defense is effective in general, against all known attack strategies,
rather than against a specific strategy.

These attacks can be divided into three categories: Gradient-
based attacks relying on detailed model information, including the
gradient of the loss w.r.t. the input. Score-based attacks rely on the
predicted scores, such as class probabilities or logits of the model.
On a conceptual level, these attacks use the predictions to estimate
the gradient numerically. Finally, decision-based attacks rely only
on the final output of the model and minimizing the adversarial
examplesâ€™ norm. The attacks are summarized in Table 1.

Table 1: Summary of the used attack methods. Notice that
the strength estimation is based on [2].

Method

Category

Norm Learning

Strength

FGSM [20]
PGD [41]
JSMA [54]
C&W [10]
DF [45]
LSA [47]
BA [8]
HSJ [11]

gradient-based
gradient-based
gradient-based
gradient-based
gradient-based
Score-based
Decision-based
Decision-based

ğ¿âˆ
ğ¿âˆ
ğ¿0
ğ¿2
ğ¿2
ğ¿2
ğ¿2
ğ¿2

One shot
Iterative
Iterative
Iterative
Iterative
Iterative
Iterative
Iterative

***
****
***
*****
****
***
***
*****

ASPLOS â€™21, April 19â€“23, 2021, Virtual, USA

Amira Guesmi, Ihsen Alouani, Khaled N. Khasawneh, Mouna Baklouti, Tarek Frikha, Mohamed Abid, and Nael Abu-Ghazaleh

4 DEFENSIVE APPROXIMATION:

IMPLEMENTING APPROXIMATE CNNS
We propose to leverage approximate computing to improve the
robustness of ML classifiers, such as CNNs, against adversarial at-
tacks. We call this general approach Defensive Approximation
(DA). The closest approach to DA is the perturbation-based de-
fense [14, 34] that either adds noise or otherwise filter the input
data to try to interfere with any adversarial modifications to the
input of a classifier. However, our approach advances the state-
of-the-art by injecting perturbations throughout the classifier and
directly by the approximate hardware, thereby enhancing both
robustness and power efficiency.

Technically, the approximate design process is driven by two

main considerations:

(a) Injecting significant noise that can influence the CNN be-

havior, and allows by-product power gains.

(b) Keeping the cumulative noise magnitude under control to

avoid impacting the baseline accuracy of the CNN.

For consideration (b), we purpose to an implementation that
replaces the conventional mantissa multiplier in floating point mul-
tipliers, with a simpler approximate design. This choice is backed by
the study in [49, 50], which show that errors in the exponent part
might have a drastic impact on CNNs accuracy. Consideration (a)
means that the approximate design needs to be aggressive to induce
significant noise. Therefore, as detailed in the next subsection, we
chose the corner case, i.e., the most aggressive approximate design
[22, 23], and used it to replace the mantissa computation logic in a
basic floating point multiplier.

In this section, we present our approximate multiplier design

and analyze its properties.

4.1 Approximate Floating Point Multiplier
ML structures, such as CNNs, often rely on computationally ex-
pensive operations, e.g., convolutions that are composed of multi-
plications and additions. Floating-point multiplications consume
most of the processing energy in both inference and training of
CNNs [5, 22]. Although approximate computation can be intro-
duced in different ways (with likely different robustness benefits),
DA leverages a new approximate 32-bit floating-point multiplier,
which we call approximate floating-point multiplier (Ax-FPM). The
IEEE 754-2008 compliant floating-point format binary numbers are
composed of three parts: a sign, an exponent, and a mantissa (also
called fraction) [1]. The sign is the most significant bit, indicating
whether the number is positive or negative. In a single-precision
format, the following 8 bits represent the exponent of the binary
number ranging from âˆ’126 to 127. The remaining 23 bits repre-
sent the fractional part (mantissa). For most of the floating number
range, the normalized format is:

ğ‘£ğ‘ğ‘™ = (âˆ’1)sign Ã— 2ğ‘’ğ‘¥ğ‘âˆ’ğ‘ğ‘–ğ‘ğ‘  Ã— (1.ğ‘“ ğ‘Ÿğ‘ğ‘ğ‘¡ğ‘–ğ‘œğ‘›)
A floating-point multiplier (FPM) consists mainly of three units:
mantissa multiplier, exponent adder, and a rounding unit. The man-
tissa multiplication consumes 81% of the overall power of the mul-
tiplier [67].

(3)

Ax-FPM is designed based on a mantissa multiplication unit
that is constructed using approximate full adders (FA). The FAs are

aggressively approximated to inject computational noise within the
circuit. We describe Ax-FPM by first presenting the approximate
FA design, and then the approximate mantissa multiplier used to
build the Ax-FPM.

Figure 1: An illustration of a 4 Ã— 4 array multiplier architec-
ture.

To build a power-efficient and a higher performance FPM, we pro-
pose to replace the mantissa multiplier by an approximate mantissa
multiplier; an array multiplier constructed using approximate FAs.
We selected an array multiplier implementation because it is consid-
ered one of the most power-efficient among conventional multiplier
architectures [26]. In the array architecture, multiplication is im-
plemented through the addition of partial products generated by
multiplying the multiplicand with each bit of multiplier using AND
gates, as shown in Figure 1.

Figure 2: Logic diagram of (a) exact Full Adder, (b) AMA5
(used in this work).

Specifically, we build an array multiplier based on an approx-
imate mirror adder (AMA5) [23] in place of exact FAs. The ap-
proximation of a conventional FA is performed by removing some
internal circuitry, thereby resulting in power and resource reduc-
tion at the cost of introducing errors. Consider a FA with (A,B, ğ¶ğ‘–ğ‘›)
as inputs and (ğ‘†ğ‘¢ğ‘š, ğ¶ğ‘œğ‘¢ğ‘¡ ) as outputs (ğ¶ here refers to carry). For
any input combination, the logical approximate expressions for ğ‘†ğ‘¢ğ‘š
and ğ¶ğ‘œğ‘¢ğ‘¡ are: ğ‘†ğ‘¢ğ‘š = ğµ and ğ¶ğ‘œğ‘¢ğ‘¡ = ğ´. The AMA5 design is consti-
tuted by only two buffers (see Figure 2), leading to the latency and
energy savings relative to the exact design, but more importantly,
introduce errors into the computation. It is worth noting that these
errors are data dependent, appearing for specific combinations of
the inputs, and ignoring the carry in value, making the injected
noise difficult to predict or model.

When trying to evaluate the proposed Ax-FPM, we were inter-
ested in studying its behavior under small input numbers ranging
between âˆ’1 and +1 since most of the internal operations within
CNNs are in this range.

ğ´ğµğ¶ğ‘–ğ‘›ğ‘†ğ‘¢ğ‘šğ¶ğ‘œğ‘¢ğ‘¡ğ¶ğ‘œğ‘¢ğ‘¡ğ‘†ğ‘¢ğ‘šğ´ğµğ¶ğ‘–ğ‘›(a)(b)Defensive Approximation: Securing CNNs using Approximate Computing

ASPLOS â€™21, April 19â€“23, 2021, Virtual, USA

We measure the introduced error as the difference of the output
of the approximate multiplier and the exact multiplier. The results
are shown in Figure 3 using 100 million randomly generated multi-
plications across the input range from âˆ’1 to 1. Three trends can be

Figure 3: Noise introduced by the approximate multiplier
while the operands in [0, 1].

observed that will be used later to help understanding the impact
of the approximation using Ax-FPM on CNN security: (i) The first
is the data-dependent discontinuity of the approximation-induced
errors, (ii) We noticed that in 96% of the cases, the approximate
multiplication results in higher absolute values than the exact mul-
tiplication: For positive products, the approximate result is higher
than the exact result, and for negative product results the approxi-
mate result is lower than the exact result, and (iii) In general, we
notice that the larger the multiplied numbers, the larger the error
magnitude added to the approximate result. As shown later, these
observations will help understand the mechanism that we think is
behind robustness enhancement.

Figure 4: Convolution result of the filter and each input im-
age using exact and approximate convolution.

performed on the layerâ€™s input data using a kernel matrix that is
convoluted (piece-wise multiplied) against the input data to produce
a feature map.

As we slide the filter over the input from left to right and top to
bottom whenever the filter coincides with a similar portion of the
input, the convolution result is high, and the neuron will fire. The
weight matrix filters out portions of the input image that does not
align with the filter, and the approximate multiplier helps improve
this process by further increasing the output when a feature is
detected. In Figure 4, we run an experiment where we choose a
filter and six different images with different degrees of similarity to
the chosen filter (1 to 6 from the least to the most similar), and we
perform the convolution operation. We notice that the approximate
convolution using Ax-FPM delivers higher results for similar inputs
and lower results for dissimilar inputs. We can also notice that the
higher the similarity, the higher the gap between the exact and Ax-
FPM approximation results. Therefore, by using the approximate
convolution, the main features of the image that are important in
the image recognition are retained and further highlighted with
higher scores that will later help increase the confidence of the
classification result, as explained in Section 6.

4.3 AC Design Space Exploration
For the sake of comparison, we proceed to a design space explo-
ration considering the accuracy of the multipliers and resource
utilization as optimization objectives. The comparison of our de-
sign with the optimal approximate design given by the exploration
and referred to as HEAP led to a clearly dominant choice in favor
of our design given the following perspectives:

(i) Resource and power efficiency: Given its aggressive de-
sign, Ax-FPM achieves the lowest power consumption and resource
utilization.

(ii) Robustness: While other approximate designs had a positive
impact on CNNs robustness, we noticed that Ax-FPM achieves the
highest enhancement.

(iii) Accuracy: CNNs are highly approximation-tolerant, and
even aggressive multiplier design didnâ€™t impact overall accuracy.
Ax-FPM, as well as HEAP have insignificant impact on accuracy.

While these observations give an insight on the potential gen-
eralizability of AC positive impact, Ax-FPM is a non dominated
design outperforming other design in both robustness, power con-
sumption and resource utilization, while having the same overall
accuracy level. Hence, in the remainder of the paper, we will focus
only on Ax-FPM based DA. Further details and results can be found
in the Appendix.

4.2 Approximate Convolution
In order to understand the impact of AC at larger scales than the
individual multiplication, we track the impact on convolution op-
erations. The approximate CNN is built using the approximate
convolution operations as building blocks. The activation functions
and the pooling layers which do not use multiplication are simi-
lar to the conventional CNN. Convolution layers enable CNNs to
extract data-driven features rather than relying on manual feature-
extraction in classical ML systems. The convolution operation is

5 CAN DA HELP IN DEFENDING AGAINST

ADVERSARIAL ATTACKS?

In this section, we empirically explore the robustness properties of
DA under a number of threat models. We first explore the transfer-
ability of adversarial attacks where we evaluate whether attacks
crafted for exact CNNs transfer to approximate CNNs. We then
consider direct attacks against approximate CNNs in both black
and white-box settings.

ÎµASPLOS â€™21, April 19â€“23, 2021, Virtual, USA

Amira Guesmi, Ihsen Alouani, Khaled N. Khasawneh, Mouna Baklouti, Tarek Frikha, Mohamed Abid, and Nael Abu-Ghazaleh

5.1 Experimental Setup
The first benchmark we use is the LeNet-5 CNN architecture [32]
along with the MNIST database [33], which implements a classifier
for handwritten digit recognition. The MNIST consists of 60,000
training and 10,000 test images with 10 classes corresponding to
digits. Each digit example is represented as a gray-scale image of
28 Ã— 28 pixels, where each feature corresponds to a pixel intensity
normalized between 0 and 1. We also use the AlexNet image classifi-
cation CNN [30] along with the CIFAR-10 database [29]. CIFAR-10
consists of 60,000 images, of dimension 64 Ã— 64 Ã— 3 each and con-
tains ten different classes. LeNet-5 consists of two convolutional
layers, two max-pooling layers, and two fully connected layers.
AlexNet uses five convolution layers, three max-pooling layers,
and three fully connected layers. The rectified linear unit (ReLU)
was used as the activation function in this evaluation, along with a
dropout layer to prevent overfitting. For both models, the output
layer is a special activation function called softmax that will assign
probabilities to each class.

Our implementations are built using the open source ML frame-
work PyTorch [55]. We use the Adam optimization algorithm to
train the LeNet-5 classifier. For Alexnet, we use Stochastic Gradient
Descent (SGD) with a learning rate equal to 0.01 and 0.001, respec-
tively. Note that no retraining is applied in DA, we rather use the
same hyper-parameters obtained from the original (exact) classifier;
we simply replace the multipliers with the approximate multiplier.
Our reference exact CNNs are conventional CNNs based on exact
convolution layers with the format Conv2d provided by PyTorch. In
contrast, the approximate CNNs emulate the 32-bit Ax-FPM func-
tionality and replace the multiplication in the convolution layers
with Ax-FPM in order to assess the behavior of the approximate
classifier.

Since we are simulating a cross-layer (approximate) implemen-
tation from gate-level up to system-level, the experiments (forward
and backward gradient) are highly time consuming, which limited
our experiments to the two datasets MNIST and CIFAR-10.

Except for the black-box setting where the attacker trains his
own reverse-engineered proxy/substitute model, the approximate
and exact classifiers share the same pre-trained parameters and the
same architecture; they differ only in the hardware implementation
of the multiplier.

5.2 Do Adversarial Attacks on an Exact CNN
Transfer to an Approximate CNN ?

Attack Scenario. In this setting, the attacker has full knowledge
of the classifier architecture and hyper-parameters, but without
knowing that the model uses approximate hardware. An example of
such scenario could be in case an attacker correctly guesses the used
architecture based on its widespread use for a given application
(e.g., LeNet-5 in digit recognition), but is unaware of the use of DA
as illustrated in Figure 5.
Transferability Analysis. The classifier generates adversarial ex-
amples using the set of algorithms in Table 1 and assume that the
exact classifier from Lenet-5 trained on the MNIST dataset. Notice
that the hyperparameters, as well as the structure of the network,
are the same between the exact and the approximate classifiers. The

Figure 5: Transferrability attack scenario.

attacker then tests the adversarial examples against the approxi-
mate classifier. Table 2 presents the attacks respective success rates.
We notice that the DA considerably reduces the transferability of
the malicious samples and, by consequence, increases the robust-
ness of the classifier to this attack setting. We observed that the
robustness against transferability is very high, and reaches 99% for
ğ¶&ğ‘Š attack.

Table 2: Attacks transferability success rates for MNIST.

Attack method

Exact LeNet-5 Approximate LeNet-5

FGSM
PGD
JSMA
C&W
DF
LSA
BA
HSJ

100%
100%
100%
100%
100%
100%
100%
100%

12%
28%
9%
1%
17%
18%
17%
2%

We repeat the experiment for AlexNet with CIFAR-10 dataset.
For the same setting, the success of different adversarial attacks
is shown in Table 3. While more examples succeed against the
approximate classifier, we see that the majority of the attacks do not
transfer. Thus, DA offers built-in robustness against transferability
attacks.

Table 3: Attacks transferability success rates for CIFAR-10.

Attack method

Exact AlexNet Approximate AlexNet

FGSM
PGD
JSMA
C&W
DF
LSA
BA
HSJ

100%
100%
100%
100%
100%
100%
100%
100%

38%
31%
32%
17%
35%
36%
37%
12%

Notice that, unlike other state-of-the-art defenses, our defense
mechanism protects the network without relying on the attack
details or the model specification and without any training beyond
that of the original classifier. Unlike most of the perturbation-based
defenses that degrade the classifierâ€™s accuracy on non-adversarial
inputs, our defense strategy significantly improves the classification

(cid:1501)(cid:48)(cid:82)(cid:71)(cid:72)(cid:79)(cid:3)(cid:1501)(cid:43)(cid:68)(cid:85)(cid:71)(cid:90)(cid:68)(cid:85)(cid:72)(cid:38)(cid:79)(cid:72)(cid:68)(cid:81)(cid:3)(cid:86)(cid:68)(cid:80)(cid:83)(cid:79)(cid:72)(cid:36)(cid:87)(cid:87)(cid:68)(cid:70)(cid:78)(cid:72)(cid:85)(cid:3)(cid:78)(cid:81)(cid:82)(cid:90)(cid:79)(cid:72)(cid:71)(cid:74)(cid:72)(cid:40)(cid:91)(cid:68)(cid:70)(cid:87)(cid:3)(cid:38)(cid:79)(cid:68)(cid:86)(cid:86)(cid:76)(cid:73)(cid:76)(cid:72)(cid:85)(cid:36)(cid:87)(cid:87)(cid:68)(cid:70)(cid:78)(cid:3)(cid:86)(cid:70)(cid:72)(cid:81)(cid:68)(cid:85)(cid:76)(cid:82)(cid:36)(cid:71)(cid:89)(cid:72)(cid:85)(cid:86)(cid:68)(cid:85)(cid:76)(cid:68)(cid:79)(cid:3)(cid:40)(cid:91)(cid:68)(cid:80)(cid:83)(cid:79)(cid:72)Defensive Approximation: Securing CNNs using Approximate Computing

ASPLOS â€™21, April 19â€“23, 2021, Virtual, USA

robustness with no baseline accuracy degradation, as we will show
in Section 8.1.

5.3 Can We Attack an Approximate CNN?
In the remaining attack models, we assume that the attacker has
direct access to the approximate CNN. We consider both black-box
and white-box settings.

Black-box Attack. In a black-box setting, the attacker has no
access to the classifier architecture, parameters, and the hardware
platform but can query the classifier with any input and obtain
its output label. In a typical black-box attack, the adversary uses
the results of many queries to the target model to reverse engineer
it. Specifically, the adversary trains a substitute (or proxy) using
the labeled inputs obtained from querying the original model (see
Figure 6). We also conduct a black box attack on the exact classifier
and evaluate how successful the black box attack is in fooling it.
Essentially, we are comparing the black-box transferability of the
reverse-engineered models to the original models for both the exact
and the approximate CNNs.

Figure 6: Black-box attack scenario.

In Table 4, we present the attack success ratios for the exact CNN
and the approximate/DA CNN. DA increases resilience to adver-
sarial attacks across various attacks and for both single-step and
iterative ones: it achieves 73% classification success on adversarial
examples in the worst case and the defense succeeded in up to 100%
of the examples generated by C&W, PGD, and HSJ respectively.

Table 4: Black-box attacks success rates for MNIST.

Attack method

Exact LeNet-5 Approximate LeNet-5

FGSM
PGD
JSMA
C&W
DF
LSA
BA
HSJ

100%
100%
100%
100%
100%
100%
100%
100%

22%
0%
13%
0%
25%
26%
27%
0%

White-box Attack. In this setting, the attacker has access to the
approximate hardware along with the victim model architecture and
parameters, as shown in Figure 7. In particular, the adversary has
full knowledge of the defender model, its architecture, the defense
mechanism, along with full access to approximate gradients used to
build the gradient-based attacks. In essence, the attacker is aware
of our defense, and can adapt around it with full knowledge of the
model and the hardware, which is a recommended methodology
for evaluating new defenses [9].

Figure 7: White-box attack scenario.

In this scenario, we assume a powerful attacker with full ac-
cess to the approximate classifierâ€™s internal model and can query
it indefinitely to directly create adversarial attacks. Although DA
in production would normally reduce execution time, in our ex-
periments, we emulate the 32-bit Ax-FPM functionality within the
approximate classifier. As a result, this makes inference extremely
slow: on average, it takes 5 to 6 days to craft one adversarial ex-
ample on an 8th Gen Intel core i7-8750H processor with NVIDIA
GeForce GTX 1050. This led us to limit the white-box experiments;
we use only two of the most efficient attacks in our benchmark:
ğ¶&ğ‘Š and DeepFool attacks, and for a limited number of examples
selected randomly from our test set for different classes.

In a white box attack, with an unconstrained noise budget, an
adversary can always eventually succeed in causing an image to
misclassify. Thus, robustness against this type of attack occurs
through the magnitude of the adversarial noise to be added: if this
magnitude is high, this may exceed the ability of the attacker to
interfere, or cause the attack to be easily detectable.

Figures 8 and 9, respectively, present different measures of ğ¿2
for adversarial examples crafted using DF and ğ¶&ğ‘Š attacking both
a conventional CNN and an approximate CNN. We notice that
the distance between a clean image and the adversarial example
generated under DA is much larger than the distance between a
clean sample and the adversarial example generated for the exact
classifier. On average, a difference of 5.12 for ğ¿2-DeepFool attacks
and 1.23 for ğ¿2-C&W attack. This observation confirms that DA is
more robust to adversarial perturbations since the magnitude of
the adversarial noise has to be significantly higher for DF to fool
DA successfully.

To understand the implication of this higher robustness in terms
of observable effects on the input image, we also show the Peak
Signal to Noise Ratio (PSNR) and the Mean Square Error (MSE) in
Figures 10 and 11; these are two common measures of the error
introduced in a reconstruction of an image. Specifically, MSE repre-
sents the average of the squares of the "errors" between the clean
image and the adversarial image. The error is the amount by which

(cid:1501)(cid:48)(cid:82)(cid:71)(cid:72)(cid:79)(cid:3)(cid:1501)(cid:43)(cid:68)(cid:85)(cid:71)(cid:90)(cid:68)(cid:85)(cid:72)(cid:36)(cid:87)(cid:87)(cid:68)(cid:70)(cid:78)(cid:72)(cid:85)(cid:3)(cid:78)(cid:81)(cid:82)(cid:90)(cid:79)(cid:72)(cid:71)(cid:74)(cid:72)(cid:36)(cid:83)(cid:83)(cid:85)(cid:82)(cid:91)(cid:76)(cid:80)(cid:68)(cid:87)(cid:72)(cid:3)(cid:38)(cid:79)(cid:68)(cid:86)(cid:86)(cid:76)(cid:73)(cid:76)(cid:72)(cid:85)(cid:36)(cid:87)(cid:87)(cid:68)(cid:70)(cid:78)(cid:3)(cid:86)(cid:70)(cid:72)(cid:81)(cid:68)(cid:85)(cid:76)(cid:82)(cid:91)(cid:20)(cid:91)(cid:21)(cid:91)(cid:22)(cid:17)(cid:17)(cid:17)(cid:17)(cid:91)(cid:81)(cid:73)(cid:11)(cid:91)(cid:20)(cid:12)(cid:73)(cid:11)(cid:91)(cid:21)(cid:12)(cid:73)(cid:11)(cid:91)(cid:22)(cid:12)(cid:17)(cid:17)(cid:17)(cid:17)(cid:73)(cid:11)(cid:91)(cid:81)(cid:12)(cid:55)(cid:85)(cid:68)(cid:76)(cid:81)(cid:3)(cid:68)(cid:3)(cid:86)(cid:88)(cid:69)(cid:86)(cid:87)(cid:76)(cid:87)(cid:88)(cid:87)(cid:72)(cid:3)(cid:80)(cid:82)(cid:71)(cid:72)(cid:79)(cid:3)(cid:38)(cid:79)(cid:72)(cid:68)(cid:81)(cid:3)(cid:86)(cid:68)(cid:80)(cid:83)(cid:79)(cid:72)(cid:40)(cid:91)(cid:68)(cid:70)(cid:87)(cid:3)(cid:38)(cid:79)(cid:68)(cid:86)(cid:86)(cid:76)(cid:73)(cid:76)(cid:72)(cid:85)(cid:36)(cid:71)(cid:89)(cid:72)(cid:85)(cid:86)(cid:68)(cid:85)(cid:76)(cid:68)(cid:79)(cid:3)(cid:40)(cid:91)(cid:68)(cid:80)(cid:83)(cid:79)(cid:72)(cid:1501)(cid:48)(cid:82)(cid:71)(cid:72)(cid:79)(cid:3)(cid:1501)(cid:43)(cid:68)(cid:85)(cid:71)(cid:90)(cid:68)(cid:85)(cid:72)(cid:38)(cid:79)(cid:72)(cid:68)(cid:81)(cid:3)(cid:86)(cid:68)(cid:80)(cid:83)(cid:79)(cid:72)(cid:36)(cid:87)(cid:87)(cid:68)(cid:70)(cid:78)(cid:72)(cid:85)(cid:3)(cid:78)(cid:81)(cid:82)(cid:90)(cid:79)(cid:72)(cid:71)(cid:74)(cid:72)(cid:36)(cid:83)(cid:83)(cid:85)(cid:82)(cid:91)(cid:76)(cid:80)(cid:68)(cid:87)(cid:72)(cid:3)(cid:38)(cid:79)(cid:68)(cid:86)(cid:86)(cid:76)(cid:73)(cid:76)(cid:72)(cid:85)(cid:3)(cid:36)(cid:87)(cid:87)(cid:68)(cid:70)(cid:78)(cid:3)(cid:86)(cid:70)(cid:72)(cid:81)(cid:68)(cid:85)(cid:76)(cid:82)(cid:36)(cid:71)(cid:89)(cid:72)(cid:85)(cid:86)(cid:68)(cid:85)(cid:76)(cid:68)(cid:79)(cid:3)(cid:40)(cid:91)(cid:68)(cid:80)(cid:83)(cid:79)(cid:72)ASPLOS â€™21, April 19â€“23, 2021, Virtual, USA

Amira Guesmi, Ihsen Alouani, Khaled N. Khasawneh, Mouna Baklouti, Tarek Frikha, Mohamed Abid, and Nael Abu-Ghazaleh

Figure 8: Required perturbations by DeepFool attack, mea-
sured using ğ¿2 distance, to generate MNIST adversarial ex-
amples for approximate and exact classifiers.

the values of the original image differ from the distorted image.
PSNR is an expression for the ratio between the maximum possible
value (power) of a signal and the power of distorting noise that
affects the quality of its representation. It is given by the follow-
(cid:17). The lower the PSNR, the
ing equation: ğ‘ƒğ‘†ğ‘ ğ‘… = 20 log10
higher the image quality degradation is.

(cid:16) ğ‘€ğ´ğ‘‹ğ‘¥
âˆš
ğ‘€ğ‘†ğ¸

We notice that the adversarial examples generated for DA is more
noisy than adversarial examples generated for an exact classifier.
The PSNR difference reaches 4ğ‘‘ğµ for C&W and 7.8ğ‘‘ğµ for DeepFool.
Moreover, on average, the DA-dedicated adversarial examples have
6 times, and 3 times more MSE than the exact classifier-dedicated
adversarial examples for ğ¶&ğ‘Š and DeepFool attacks, respectively.

Figure 9: Required perturbations by C&W attack, measured
using ğ¿2 distance, to generate MNIST adversarial examples
for approximate and exact classifiers.

We can conclude that DA provides substantial built-in robustness
for all three attack models we considered. Attacks generated against
an exact model do not transfer successfully to DA. Black-box attacks
also achieve a low success rate against DA. Finally, even white-box
attacks require substantial increases in the injected noise to fool DA.
Next, we probe deeper into DAâ€™s internal behavior to provide some
intuition and explanation for these observed robustness advantages.

Figure 10: MSE and PSNR values for the generated adversar-
ial examples using DeepFool method when attacking the ap-
proximate and the exact classifiers.

Figure 11: MSE and PSNR values for the generated adversar-
ial examples using ğ¿2-C&W method when attacking the ap-
proximate and the exact classifiers.

6 HOW DOES DA HELP CNN ROBUSTNESS?
In this section, we probe into the DA classifierâ€™s operation to at-
tempt to explain the robustness advantages we observed empirically
in the previous section. While the explainability of deep neural net-
works models is a known hard problem, especially under adversarial
settings [61], we attempt to provide an overview of the mechanisms
that we think are behind the DA impact on security. We study the
impact of the approximation on CNNsâ€™ confidence and general-
ization property. We follow this analysis in the Appendix with a
mathematical argument explaining the observed robustness based
on recent formulations by Lecuyer et al. [34].

The output of the CNN is computed using the softmax function,
which normalizes the outputs from the fully connected layer into
a likelihood value for each output class. Specifically, this function
takes an input vector and returns a non-negative probability distri-
bution vector of the same dimension corresponding to the output
classes. In this section, we examine the impact of approximation on
the observed classifier confidence. We compare the output scores
of an exact and an approximate classifier for a set of 1000 repre-
sentative samples selected from the MNIST dataset: 100 randomly
selected from each class. We define the classification confidence, ğ¶,
as the difference between the true class ğ‘™â€™s score and the "runner-
up" class score, i.e., the class with the second-highest score. ğ¶ is
expressed by ğ¶ = ğ‘œğ‘¢ğ‘¡ğ‘ğ‘¢ğ‘¡ [ğ‘™] âˆ’ ğ‘šğ‘ğ‘¥ ğ‘—â‰ ğ‘™ {ğ‘œğ‘¢ğ‘¡ğ‘ğ‘¢ğ‘¡ [ ğ‘—]}. The confidence
ranges from 0 when the classifier gives equal likelihood to the top
two or more classes, to 1 when the top class has a likelihood of 1,
and all other classes 0.

Defensive Approximation: Securing CNNs using Approximate Computing

ASPLOS â€™21, April 19â€“23, 2021, Virtual, USA

We plot the cumulative distribution of confidence for both classi-
fiers in Figure 12. DA images have higher confidence; for example,
in images classified by the exact classifier, less than 20% had higher
than 0.8 confidence. On the other hand, for the approximate classi-
fier, 74.5% of the images reached that threshold.

in DA makes its decision boundary randomly different from the
initial model.

Table 5: Comparing attacks transferability success rates for
CIFAR-10 when using DA and DQ.

Attack method

Exact

Full Weight-only

DA: DQ:

DQ:

FGSM
PGD
C&W

100%
100%
100%

38%
31%
17%

60%
74%
68%

61%
73%
68%

7.2 BFloat16
The Bfloat16 (Brain Floating Point) [28] is a truncated version of the
32-bit IEEE 754 single-precision floating-point format (float32). It is
composed of one sign bit, eight exponent bits, and seven mantissa
bits giving the range of a full 32-bit number but in the data size
of a 16-bit number. Bfloat16 is used in machine learning applica-
tions and intended to reduce the storage requirements and increase
computing speed without losing precision.

To evaluate the impact of Bfloat16 on deep neural networks
robustness, we use Pytorch framework [55] to implement Bfloat16-
based CNN architectures and test them for MNIST and CIFAR-
10 benchmarks. We notice that using Bfloat16 achieves the same
prediction accuracy as the full precision 32-bit floating point. No
remarkable change was noticed at the output of the convolutional
layers nor in the confidence of the models. We believe this is due to
the nature of the noise introduced by reducing data representation
accuracy. A more detailed discussion can be found in Section 9.

Figure 13 shows the noise resulting from multiplying 100 million
randomly generated Bfloat16 numbers compared to their corre-
sponding 32-bit floating point. This figure shows that, in contrast
with our approximate multiplier (Figure 3), Bfloat16 multiplication
results in mostly negative noise with orders of magnitude lower
than DA-induced noise. Moreover, the Bfloat16 noise is not input-
dependent, and has no specific impact on the model confidence.
Accordingly, no improvement in the robustness was noticed when
attacking the Bfloat16 model using FGSM, PGD and ğ¶&ğ‘Š .

Figure 13: Noise introduced by multiplying Bfloat16 while
the operands âˆˆ [0, 1].

Figure 12: Cumulative distribution of confidence.

Compared to the baseline feature maps generated from an exact
convolution operation, for the same pre-trained weights, our ap-
proximate convolution highlights further the features. Recall that
the multiplier injected noise is higher when input numbers are
higher (i.e., there is a high similarity between the kernel and the
input data) and lower when the inputs are lower (when the similar-
ity is small), as shown in Figure 4. We believe that these enhanced
features continue to propagate through the model resulting in a
higher probability for the predicted class. This higher confidence
requires thereby higher noise to decrease the true labelâ€™s likelihood,
and increase another labelâ€™s.

7 HOW DOES DA COMPARE TO OTHER
REDUCED PRECISION TECHNIQUES?

In this section, we investigate the impact of other reduced precision
techniques on robustness. We first compare DA to DQ, and then
study the impact of using Bfloat16 data representation on the system
performance and robustness.

7.1 Defensive Quantization
Defensive quantization [36] was proposed to jointly optimize the
efficiency and robustness of deep learning models. A 4-bit quan-
tized model was trained on CIFAR-10 using the Dorefa-Net method
[72]. The model architecture is detailed in Appendix B. We consider
two ways of quantization: (i) Weight quantization, where only the
weights are quantized, and (ii) Full quantization where the weights
of each convolutional and dense block and the output of each acti-
vation function are quantized. In Table 5, we report transferability
between exact (32-bit floating point), approximate model (using
DA), fully quantized and weight-only quantized model. We notice
that DA is almost two times more robust against transferability
attacks than DQ under FGSM, PGD and C&W attacks. We believe
that this is due to the difference in terms of noise distribution be-
tween DA and DQ. In fact, while DQ-induced noise tends to make
the initial decision boundary smoother, the input-dependent noise

ASPLOS â€™21, April 19â€“23, 2021, Virtual, USA

Amira Guesmi, Ihsen Alouani, Khaled N. Khasawneh, Mouna Baklouti, Tarek Frikha, Mohamed Abid, and Nael Abu-Ghazaleh

8 BASELINE PERFORMANCE IMPLICATIONS
8.1 Impact on Model Accuracy
It is important for a defense mechanism that aims to enhance robust-
ness against adversarial attacks to keep at least an acceptable per-
formance level for clean inputs. In fact, considerably reducing the
baseline accuracy, or creating an exploding or vanishing gradient
impact that makes the model sensitive to other types of noise under-
mines the model reliability. In our proposed approach, we maintain
the same level of recognition rate even with the approximate noise
in the calculations. Counter-intuitively, this data-dependent noise
helps to better highlight the inputâ€™s important features used in the
recognition and does not affect the classification process. A drop
of 0.01% in the recognition rate for the case of LeNet-5 and 1% for
AlexNet is recorded as mentioned in Table 8.

Table 6: Accuracy results of Float32, approximate model,
fully quantized, weight-only quantized and Bfloat16 mod-
els.

Used Multiplier

MNIST CIFAR-10

Float32
Approximate (DA)
Fully quantized
Weight-only quantized
Bfloat16

97.93%
97.67%
-
-
97.93

81%
80%
80%
80%
81%

8.2 Impact on Performance and Energy

Consumption

Here we show the additional benefit of using AC, especially in the
context of power-limited devices, such as mobile, embedded, and
Edge devices. The experiments evaluate normalized energy and
delay achieved by the proposed approximate multiplier compared
to a conventional baseline multiplier. Multipliers are implemented
using 45 ğ‘›ğ‘š technology via the Predictive Technology Model (PTM)
using the Keysight Advanced Design System (ADS) simulation
platform [27].

Table 7: Energy and delay Comparison.

Multiplier
Exact multiplier
Ax-FPM
Bfloat16

Average energy Average delay

1
0.487
0.4

1
0.29
0.4

Table 7 compares the energy and delay for the approximate
multiplier and the Bfloat16 multiplier normalized to a conventional
multiplier.

Bfloat16 data representation is shown in Figure 14, with 1 sign bit,
a 8-bit exponent and a 7-bit fraction. The used Bfloat16 multiplier
has similar architecture to that of Ax-FPM. However for the man-
tissa multiplier we use the conventional Booth multiplier instead of
the array multiplier. Ax-FPM achieves up to 51% and 71% savings

Figure 14: Contrast between IEEE 754 Single-precision 32-bit
floating-point format and Bfloat16 format.

in energy and delay, respectively compared to the baseline multi-
plier. Bfloat16 implementation results in comparable savings with
around 60% lower power and delay. However, this comes without
robustness advantages, as mentioned earlier.

Unlike most of the state-of-the-art defense strategies that lead to
power, resource, or timing overhead, DA results in saving energy
and area.

9 DISCUSSION
This work tackles the problem of robustness to adversarial attacks
from a new perspective: approximation in the underlying hardware.
DA exploits the inherent fault tolerance of deep learning systems
[49] to provide resilience while also obtaining the by-product gains
of AC in terms of energy and resources. Our empirical study shows
promising results in terms of robustness across a wide range of
attack scenarios. We notice that AC-induced perturbations tend
to help the classifier generalize and enhances its confidence. We
believe that this observation is possibly due to the specific AC mul-
tiplier we used where the introduced noise is input-dependent and
non-uniform. When we observe the effect on the convolution layer,
we see higher absolute values when the inputs are similar to the
convolution filter. This observation at the feature map propagates
through the model and results in enhanced classification confi-
dence, i.e., the difference between the 1ğ‘ ğ‘¡ and runner-up classes.
This aspect of confidence enhancement resembles the smoothing
effect observed by some recently proposed randomization tech-
niques [34].

We believe that our study makes important contributions in
demonstrating the general potential of approximate computing
in this new dimension. However, we believe that substantial fur-
ther research remains which we hope to tackle in our future work:
(1) More work is needed to carefully understand the relationship
between the patterns of induced noise and the observed robust-
ness to adversarial attacks to guide the selection of approximation
approaches; (2) We would also like to explore whether there is
additional protection that results from adapting the approximation
function over time; (3) We believe that DA is orthogonal to some
of the other AML defenses and, deployed together, they may result
in even higher protection against AML; (4) Some AML defenses
unintentionally make the model more susceptible to privacy related
attacks [64]; we believe that DA does not have a similar effect and

Defensive Approximation: Securing CNNs using Approximate Computing

ASPLOS â€™21, April 19â€“23, 2021, Virtual, USA

would like to study its implication on privacy-preserving in the
future; and (5) We would like to explore DA in the context of other
learning structures beyond CNNs.

DA has two important advantages compared to DQ: (1) DA re-
sults in input-dependent noise, while DQ results in a deterministic
network that can be efficiently reverse engineered and undermined
by adaptive white-box attacks; (2) DQ requires retraining/fine-
tuning the model to avoid drastic accuracy drop, while DA does
not require retraining.

Compared to DA, when proceeding to Bfloat16 quantization, we
did not notice any improvement in the model robustness, which
could be explained by the fact that Bfloat16 results in uniform low-
amplitude noise distribution that is not sufficient to impact CNNs
behavior.

While we considered full precision floating point CNNs, we
believe that DA can also apply to quantized and sparse networks [4,
39, 66] with similar impact on security. Prior work [50] shows
that quantized networks tolerate errors, implying that DA can
potentially be deployed without degrading accuracy.

Our experimental setup is based on simulating a cross-layer im-
plementation from gate-level up to system-level. Therefore, the
experiments are computationally and time demanding, which lim-
ited our experiments to the two datasets MNIST and CIFAR-10.
This limitation was the same for techniques that require Monte
Carlo Simulations such as [34]. Our observations held for 5-layer-
CNN (LeNet) and 8-layer-CNN (AlexNet). In future work, we are
planning to solve the simulation time limitation by a real hardware
implementation, which facilitate evaluating DA for larger networks.

10 RELATED WORK
Several defense mechanisms were proposed to combat adversarial
attacks and can be categorized as follows:
Adversarial Training (AT). AT is one of the most explored de-
fenses against adversarial attacks. The main idea can be traced back
to [20], in which models were hardened by including adversarial
examples in the training data set of the model. As a result, the
trained model classifies evasive samples with higher accuracy. Var-
ious attempts to combine AT with other methods have resulted in
better defense approaches such as cascade adversarial training [46],
principled training [63]. Nonetheless, AT is not effective when the
attacker uses a different attack strategy than the one used to train
the model [60]. Moreover, adversarial training is much more com-
putationally intensive than training a model on the training data set
only because generating evasive samples needs more computation
and model fitting is more challenging (takes more epochs) [68].
Input Preprocessing (IP). Input preprocessing depends on apply-
ing transformations to the input to remove the adversarial pertur-
bations [15, 51]. Examples of transformation are denoising auto-
encoders [21], the median, averaging, and Gaussian low-pass fil-
ters [51], and JPEG compression [15]. However, it was shown that
these defenses are insecure under strong white-box attacks [12]; if
the attacker knows the specific used transformation, it can be taken
into account when creating the attack. Furthermore, preprocessing
requires additional computation on every input.
Gradient Masking (GM). GM relies on applying regularization to
the model to make its output less sensitive to input perturbations.

Papernot et al. proposed defensive distillation [53], which is based
on increasing the generalization of the model by distilling knowl-
edge out of a large model to train a compact model. Nonetheless,
distillation was found weak against ğ¶&ğ‘Š attack [10]. Nayebi and
Surya [48] purposed to use saturating networks that use a loss
function that promotes the activations to be in their saturating
regime. [59] proposed to regularize the gradient input by penaliz-
ing variations in the modelâ€™s output with respect to changes in the
input during the training of differentiable models. Nonetheless, GM
approaches found to make white-box attacks harder and vulnera-
ble against black-box attacks [52, 68]. Furthermore, they require
re-training of pre-trained networks.
Randomization-based Defenses. These techniques are the clos-
est to our work [14, 18, 34, 37, 57]. Liu et al. [37] suggest to ran-
domize the entire DNN and predict using an ensemble of multiple
copies of the DNN. Lecuyer et al. [34] also suggest to add random
noise to the first layer of the DNN and estimate the output by a
Monte Carlo simulation. These techniques offer a bounded theo-
retical guarantee of robustness. From a practical perspective, none
of these works has been evaluated at scale or with realistic imple-
mentations. For example, Raghunathan et al. [57] evaluate only a
tiny neural network. Other works [14, 34] consider scalability but
require high overhead to implement the defense (specifically, to
estimate the model output which requires running a heavy Monte
Carlo simulation involving a number of different runs of the CNN).
Our approach is different since not only our noise does not require
overhead but comes naturally from the simpler and faster AC im-
plementation. Moreover, while these techniques require additional
training, DA is a drop-in replacement of the hardware without spe-
cific training requirements, and with no changes to the architecture
nor the parameters of the CNN.

11 CONCLUSIONS
To the best of our knowledge, this is the first work that proposes
the use of hardware-supported approximation as a defense strat-
egy against adversarial attacks for CNNs. We propose a CNN im-
plementation based on Ax-FPM, an energy-efficient approximate
floating-point multiplier. While AC is used in the literature to re-
duce the energy and delay of CNNs, we show that AC also enhances
their robustness to adversarial attacks. The proposed defense is,
on average, 87% more robust against strong grey-box attacks and
87.5% against strong black-box attacks than a conventional CNN
for the case of MNIST dataset, with negligible loss in accuracy. The
approximate CNN achieves a significant reduction in power and
delay of 50% and 67%, respectively.

ACKNOWLEDGMENTS
We would like to thank the reviewers and shepherd for the feedback
and suggestions that have substantially improved the paper. This
work was partially supported by NSF grants CNS-1646641, CNS-
1619322 and CNS-1955650.

A APPENDIX: DESIGN SPACE EXPLORATION
Accuracy. HEAP [22] is the result of a design space exploration
among combinations of different approximate full adders leading
to minimal accuracy loss. The selection process of optimal circuit

ASPLOS â€™21, April 19â€“23, 2021, Virtual, USA

Amira Guesmi, Ihsen Alouani, Khaled N. Khasawneh, Mouna Baklouti, Tarek Frikha, Mohamed Abid, and Nael Abu-Ghazaleh

design was performed after an exhaustive accuracy evaluation of
all possible configurations.The metrics used to evaluate the accu-
racy of different combinations are the mean relative error distance
| Ë†ğ‘Œ âˆ’ğ‘Œ |
(MRED) [35] ğ‘€ğ‘…ğ¸ğ· = 1
ğ‘›
ğ‘Œ
ror distance (NMED): ğ‘ ğ‘€ğ¸ğ· = 1
(cid:205)ğ‘›
. Where ğ‘Œ is the exact
ğ‘›
result, Ë†ğ‘Œ is the approximate result, and ğ‘ƒğ‘šğ‘ğ‘¥ is the maximum prod-
uct.

, and the normalized mean er-

| Ë†ğ‘Œ âˆ’ğ‘Œ |
ğ‘ƒğ‘šğ‘ğ‘¥

(cid:205)ğ‘›

ğ‘–=1

ğ‘–=1

Table 8: Accuracy results of the LeNet-5 CNN and different
multipliers.

Multiplier
Exact multiplier
HEAP [22]
Ax-FPM

CNN Accuracy MRED NMED

97.93%
97.86%
97.67%

0
0.12
0.33

0
0.03
0.08

Table 8 includes the accuracy results of the exact multiplier
and the two approximate multipliers. Ax-FPM is substantially less
accurate than HEAP. However, when we evaluate CNN accuracy
(using the recognition rate), we notice a negligible accuracy loss
for both approximate multipliers, confirming the inherent error-
resiliency of CNNs.

Figure 15: Noise introduced by the approximate multiplier
while the operands âˆˆ [0, 1] (a). for Ax-FPM, (b). for HEAP
[22].

Measuring the error introduced by the Ax-FPM and the HEAP
for small inputs ranging between 0 and 1, see Figure 15, we notice
that, in addition to the smaller magnitude of error when using the
HEAP, a different pattern was found: less data dependency and for
only 34% of the cases, the HEAP results were higher than the exact
multiplier results.

In order to further investigate the impact of different approx-
imate multipliers on the convolution layer output, we produce
different heat maps. As shown in Figure 16, we can see that the Ax-
FPM is further highlighting the important features by increasing
their scores whereas the HEAP is lowering their scores.
Energy consumption and Delay. We also compare the gain in
terms of energy and delay achieved by the two approximate man-
tissa multipliers. Multipliers are implemented using 45 ğ‘›ğ‘š technol-
ogy via the Predictive Technology Model (PTM) using the Keysight
Advanced Design System (ADS) simulation platform [27].

Figure 16: Heat maps of final convolution layer output when
using Exact multiplier, Ax-FPM, and HEAP [22].

Table 9: Energy and delay of different 24 Ã— 24 approximate
multipliers normalized to a conventional 24 Ã— 24 multiplier.

Multiplier
Exact multiplier
HEAP
Ax-FPM

Average energy Average delay

1
0.49
0.395

1
0.46
0.235

As shown in Table 9, the mantissa multiplier in Ax-FPM achieves
a considerable gain in performance and energy saving compared to
the HEAP. A significant reduction in the delay was also achieved.
Robustness. We tried to assess different models transferability, the
adversarial examples were generated to fool the exact model. In
table 10, we present the percentage of adversarial examples that
successfully deceived the approximate models. While the use of the
HEAP increased the robustness of the model, the Ax-FPM achieved
better results.

Table 10: Attacks transferability success rates for MNIST.

Attack

Exact-based HEAP-based Ax-FPM-based

FGSM
PGD
JSMA
C&W
DF
LSA
BA
HSJ

100%
100%
100%
100%
100%
100%
100%
100%

16%
30%
42%
6%
18%
41%
22%
4%

12%
28%
9%
1%
17%
18%
17%
2%

All these results confirm that Ax-FPM is a better choice in terms

of robustness and resource and power efficiency.

B APPENDIX: DEFENSIVE QUANTIZATION:

MODELS ARCHITECTURE

We present models architectures in Table 11: For the fully quantized
model, ConvolutionQuant, DenseQuant and reluQuant designate
respectively a convolution layer with quantized weights, a dense
layer with quantized weights and the relu activation function with
its output quantized. For the weight quantized model architecture,
the ConvolutionQuant and DenseQuant designate respectively a
convolution layer with quantized weights and a dense layer with
quantized weights.

a.b.Exact Ax-FPM HEAP Defensive Approximation: Securing CNNs using Approximate Computing

ASPLOS â€™21, April 19â€“23, 2021, Virtual, USA

Table 11: Fully quantized and Weight quantized models ar-
chitecture.

Fully Quantized

Weight Quantized

ConvolutionQuant
BatchNorm
reluQuant

ConvolutionQuant
BatchNorm
relu

ConvolutionreluQuant ConvolutionreluQuant

MaxPooling
BatchNorm
reluQuant
ConvolutionQuant
BatchNorm
reluQuant
ConvolutionQuant
MaxPooling
BatchNorm
reluQuant
ConvolutionQuant
BatchNorm
reluQuant
ConvolutionQuant
MaxPooling
BatchNorm
reluQuant
DenseQuant
BatchNorm
reluQuant
DenseQuant
BatchNorm
reluQuant
Dense
softmax

MaxPooling
BatchNorm
relu
ConvolutionQuant
BatchNorm
relu
ConvolutionQuant
MaxPooling
BatchNorm
relu
ConvolutionQuant
BatchNorm
relu
ConvolutionQuant
MaxPooling
BatchNorm
relu
DenseQuant
BatchNorm
relu
DenseQuant
BatchNorm
relu
Dense
softmax

REFERENCES
[1] 2008. IEEE Standard for Floating-Point Arithmetic. IEEE Std 754-2008 (2008),

1â€“70.

[2] N. Akhtar and A. Mian. 2018. Threat of Adversarial Attacks on Deep Learning in

Computer Vision: A Survey. IEEE Access 6 (2018), 14410â€“14430.

[3] Mohammed Al-Qizwini, Iman Barjasteh, Hothaifa Al-Qassab, and Hayder Radha.
2017. Deep learning algorithm for autonomous driving using GoogLeNet. In 2017
IEEE Intelligent Vehicles Symposium (IV). IEEE, 89â€“96.

[4] Karim M. A. Ali, Ihsen Alouani, Abdessamad Ait El Cadi, Hamza Ouarnoughi,
and Smail Niar. 2020. Cross-layer CNN Approximations for Hardware Implemen-
tation. In Applied Reconfigurable Computing. Architectures, Tools, and Applications,
Fernando RincÃ³n, JesÃºs Barba, Hayden K. H. So, Pedro Diniz, and JuliÃ¡n Caba
(Eds.). Springer International Publishing, Cham, 151â€“165.

[5] I. Alouani, H. Ahangari, O. Ozturk, and S. Niar. 2018. A Novel Heterogeneous
Approximate Multiplier for Low Power and High Performance. IEEE Embedded
Systems Letters 10, 2 (2018), 45â€“48.

[6] Filipe Betzel, Karen Khatamifard, Harini Suresh, David J. Lilja, John Sartori, and
Ulya Karpuzcu. 2018. Approximate Communication: Techniques for Reducing
Communication Bottlenecks in Large-Scale Parallel Systems. ACM Comput. Surv.
51, 1, Article 1 (Jan. 2018), 32 pages. https://doi.org/10.1145/3145812

[7] Adith Boloor, Xin He, Christopher Gill, Yevgeniy Vorobeychik, and Xuan Zhang.
2019. Simple Physical Adversarial Examples against End-to-End Autonomous
Driving Models. arXiv:1903.05157 [cs.RO]

[8] Wieland Brendel, Jonas Rauber, and Matthias Bethge. 2017. Decision-Based Ad-
versarial Attacks: Reliable Attacks Against Black-Box Machine Learning Models.
arXiv:1712.04248 [stat.ML]

[9] Nicholas Carlini, Anish Athalye, Nicolas Papernot, Wieland Brendel, Jonas
Rauber, Dimitris Tsipras, Ian Goodfellow, Aleksander Madry, and Alexey Kurakin.
2019. On Evaluating Adversarial Robustness. arXiv:1902.06705 [cs.LG]

[10] Nicholas Carlini and David A. Wagner. 2016. Towards Evaluating the Robustness
of Neural Networks. CoRR abs/1608.04644 (2016). arXiv:1608.04644 http://arxiv.
org/abs/1608.04644

[11] Jianbo Chen and Michael I. Jordan. 2019. Boundary Attack++: Query-Efficient
Decision-Based Adversarial Attack. CoRR abs/1904.02144 (2019). arXiv:1904.02144
http://arxiv.org/abs/1904.02144

[12] Jiefeng Chen, Xi Wu, Vaibhav Rastogi, Yingyu Liang, and Somesh Jha. 2019.
Towards understanding limitations of pixel discretization against adversarial
attacks. In 2019 IEEE European Symposium on Security and Privacy (EuroS&P).
IEEE, 480â€“495.

[13] Dan C. Ciresan, Ueli Meier, Jonathan Masci, and JÃ¼rgen Schmidhuber. 2012. Multi-
column deep neural network for traffic sign classification. Neural networks : the
official journal of the International Neural Network Society 32 (2012), 333â€“8.
[14] Jeremy Cohen, Elan Rosenfeld, and Zico Kolter. 2019. Certified Adversarial Ro-
bustness via Randomized Smoothing. In Proceedings of the 36th International Con-
ference on Machine Learning (Proceedings of Machine Learning Research, Vol. 97),
Kamalika Chaudhuri and Ruslan Salakhutdinov (Eds.). PMLR, Long Beach, Cali-
fornia, USA, 1310â€“1320. http://proceedings.mlr.press/v97/cohen19c.html
[15] Nilaksh Das, Madhuri Shanbhogue, Shang-Tse Chen, Fred Hohman, Li Chen,
Michael E Kounavis, and Duen Horng Chau. 2017. Keeping the bad guys out:
Protecting and vaccinating deep learning with jpeg compression. arXiv preprint
arXiv:1705.02900 (2017).

[16] Li Deng and Yang Liu. 2018. Deep learning in natural language processing.

Springer.

[17] Yunbin Deng. 2019. Deep Learning on Mobile Devices - A Review. CoRR
abs/1904.09274 (2019). arXiv:1904.09274 http://arxiv.org/abs/1904.09274
[18] Guneet S. Dhillon, Kamyar Azizzadenesheli, Zachary C. Lipton, Jeremy Bern-
stein, Jean Kossaifi, Aran Khanna, and Anima Anandkumar. 2018. Stochastic
Activation Pruning for Robust Adversarial Defense. CoRR abs/1803.01442 (2018).
arXiv:1803.01442 http://arxiv.org/abs/1803.01442

[19] Ivan Evtimov, Kevin Eykholt, Earlence Fernandes, Tadayoshi Kohno, Bo Li, Atul
Prakash, Amir Rahmati, and Dawn Song. 2017. Robust Physical-World Attacks
on Machine Learning Models. CoRR abs/1707.08945 (2017). arXiv:1707.08945
http://arxiv.org/abs/1707.08945

[20] Ian J. Goodfellow, Jonathon Shlens, and Christian Szegedy. 2014. Explaining and

Harnessing Adversarial Examples. arXiv:1412.6572 [stat.ML]

[21] Shixiang Gu and Luca Rigazio. 2014. Towards deep neural network architectures

robust to adversarial examples. arXiv preprint arXiv:1412.5068 (2014).

[22] Amira Guesmi, Ihsen Alouani, Mouna Baklouti, Tarek Frikha, Mohamed Abid,
and Atika Rivenq. 2019. HEAP: A Heterogeneous Approximate Floating-Point
Multiplier for Error Tolerant Applications. In Proceedings of the 30th International
Workshop on Rapid System Prototyping (RSPâ€™19) (New York, NY, USA) (RSP â€™19).
ACM, New York, NY, USA, 36â€“42. https://doi.org/10.1145/3339985.3358495
[23] V. Gupta, D. Mohapatra, A. Raghunathan, and K. Roy. 2013. Low-Power Digital
Signal Processing Using Approximate Adders. IEEE Transactions on Computer-
Aided Design of Integrated Circuits and Systems 32, 1 (Jan 2013), 124â€“137. https:
//doi.org/10.1109/TCAD.2012.2217962

[24] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. 2015. Deep Residual
Learning for Image Recognition. CoRR abs/1512.03385 (2015). arXiv:1512.03385
http://arxiv.org/abs/1512.03385

[25] Dan Hendrycks and Kevin Gimpel. 2016. Early Methods for Detecting Adversarial

Images. arXiv:1608.00530 [cs.LG]

[26] R. Hrbacek, V. Mrazek, and Z. Vasicek. 2016. Automatic design of approximate
circuits by means of multi-objective evolutionary algorithms. In 2016 International
Conference on Design and Technology of Integrated Systems in Nanoscale Era (DTIS).
1â€“6. https://doi.org/10.1109/DTIS.2016.7483885

[27] Nanoscale Integration and Modeling (NIMO) Group. 2012. Predictive technology

model (PTM) website. Retrieved April 8, 2019 from http://ptm.asu.edu

[28] Dhiraj Kalamkar, Dheevatsa Mudigere, Naveen Mellempudi, Dipankar Das, Kunal
Banerjee, Sasikanth Avancha, Dharma Teja Vooturi, Nataraj Jammalamadaka,
Jianyu Huang, Hector Yuen, Jiyan Yang, Jongsoo Park, Alexander Heinecke,
Evangelos Georganas, Sudarshan Srinivasan, Abhisek Kundu, Misha Smelyanskiy,
Bharat Kaul, and Pradeep Dubey. 2019. A Study of BFLOAT16 for Deep Learning
Training. arXiv:1905.12322 [cs.LG]

[29] Alex Krizhevsky. 2009. Learning multiple layers of features from tiny images.

Technical Report.

[30] Alex Krizhevsky, Ilya Sutskever, and Geoffrey E. Hinton. 2012. ImageNet Classi-
fication with Deep Convolutional Neural Networks. In Proceedings of the 25th
International Conference on Neural Information Processing Systems - Volume 1
(Lake Tahoe, Nevada) (NIPSâ€™12). Curran Associates Inc., Red Hook, NY, USA,
1097â€“1105.

[31] Alexey Kurakin, Ian J. Goodfellow, and Samy Bengio. 2016. Adversarial examples
in the physical world. CoRR abs/1607.02533 (2016). arXiv:1607.02533 http:
//arxiv.org/abs/1607.02533

[32] Y. Lecun, L. Bottou, Y. Bengio, and P. Haffner. 1998. Gradient-based learning
applied to document recognition. Proc. IEEE 86, 11 (Nov 1998), 2278â€“2324. https:
//doi.org/10.1109/5.726791

[33] Yann LeCun and Corinna Cortes. 2010. MNIST handwritten digit database.
http://yann.lecun.com/exdb/mnist/. (2010). http://yann.lecun.com/exdb/mnist/
[34] M. Lecuyer, V. Atlidakis, R. Geambasu, D. Hsu, and S. Jana. 2019. Certified Robust-
ness to Adversarial Examples with Differential Privacy. In 2019 IEEE Symposium
on Security and Privacy (SP). 656â€“672.

ASPLOS â€™21, April 19â€“23, 2021, Virtual, USA

Amira Guesmi, Ihsen Alouani, Khaled N. Khasawneh, Mouna Baklouti, Tarek Frikha, Mohamed Abid, and Nael Abu-Ghazaleh

[35] J. Liang, J. Han, and F. Lombardi. 2013. New Metrics for the Reliability of Approx-
imate and Probabilistic Adders. IEEE Trans. Comput. 62, 9 (Sep. 2013), 1760â€“1771.
https://doi.org/10.1109/TC.2012.146

[36] Ji Lin, Chuang Gan, and Song Han. 2019. Defensive Quantization: When Efficiency

Meets Robustness. arXiv:1904.08444 [cs.LG]

[37] Xuanqing Liu, Minhao Cheng, Huan Zhang, and Cho-Jui Hsieh. 2017. Towards
Robust Neural Networks via Random Self-ensemble. arXiv:1712.00673 [cs.LG]
[38] Jiajun Lu, Hussein Sibai, Evan Fabry, and David Forsyth. 2017. NO Need to
Worry about Adversarial Examples in Object Detection in Autonomous Vehicles.
arXiv:1707.03501 [cs.CV]

[39] Yufei Ma, N. Suda, Yu Cao, J. Seo, and S. Vrudhula. 2016. Scalable and modularized
RTL compilation of Convolutional Neural Networks onto FPGA. In 2016 26th
International Conference on Field Programmable Logic and Applications (FPL). 1â€“8.
[40] Aleksander Madry, Aleksandar Makelov, Ludwig Schmidt, Dimitris Tsipras, and
Adrian Vladu. 2017. Towards Deep Learning Models Resistant to Adversarial
Attacks. arXiv:1706.06083 [stat.ML]

[41] Aleksander Madry, Aleksandar Makelov, Ludwig Schmidt, Dimitris Tsipras, and
Adrian Vladu. 2017. Towards Deep Learning Models Resistant to Adversarial
Attacks. arXiv:1706.06083 [stat.ML]

[42] Mohammad Badrul Alam Miah, Mohammad Abu Yousuf, Md. Sohag Mia, and
Md. Parag Miya. 2015. Article: Handwritten Courtesy Amount and Signature
Recognition on Bank Cheque using Neural Network. International Journal of
Computer Applications 118, 5 (May 2015), 21â€“26. Full text available.

[43] Riccardo Miotto, Fei Wang, Shuang Wang, Xiaoqian Jiang, and Joel T Dudley. 2018.
Deep learning for healthcare: review, opportunities and challenges. Briefings in
bioinformatics 19, 6 (2018), 1236â€“1246.

[44] S. K. Moore. 2019. Another step toward the end of Mooreâ€™s law: Samsung and
TSMC move to 5-nanometer manufacturing - [News]. IEEE Spectrum 56, 6 (June
2019), 9â€“10. https://doi.org/10.1109/MSPEC.2019.8727133

[45] Seyed-Mohsen Moosavi-Dezfooli, Alhussein Fawzi, and Pascal Frossard. 2015.
DeepFool: a simple and accurate method to fool deep neural networks.
arXiv:1511.04599 [cs.LG]

[46] Taesik Na,

Jong Hwan Ko, and Saibal Mukhopadhyay. 2017.

Cas-
cade Adversarial Machine Learning Regularized with a Unified Embedding.
arXiv:1708.02582 [stat.ML]

[47] Nina Narodytska and Shiva Prasad Kasiviswanathan. 2016. Simple Black-Box
Adversarial Perturbations for Deep Networks. CoRR abs/1612.06299 (2016).
arXiv:1612.06299 http://arxiv.org/abs/1612.06299

[48] Aran Nayebi and Surya Ganguli. 2017. Biologically inspired protection of deep

networks from adversarial attacks. arXiv:1703.09202 [stat.ML]

[49] M. A. Neggaz, I. Alouani, P. R. Lorenzo, and S. Niar. 2018. A Reliability Study on
CNNs for Critical Embedded Systems. In 2018 IEEE 36th International Conference
on Computer Design (ICCD). 476â€“479.

[50] M. A. Neggaz, I. Alouani, S. Niar, and F. Kurdahi. 2019. Are CNNs Reliable Enough
for Critical Applications? An Exploratory Study. IEEE Design Test (2019), 1â€“1.
https://doi.org/10.1109/MDAT.2019.2952336

[51] Margarita Osadchy, Julio Hernandez-Castro, Stuart Gibson, Orr Dunkelman,
and Daniel PÃ©rez-Cabo. 2017. No bot expects the DeepCAPTCHA! Introducing
immutable adversarial examples, with applications to CAPTCHA generation.
IEEE Transactions on Information Forensics and Security 12, 11 (2017), 2640â€“2653.
[52] Nicolas Papernot, Patrick McDaniel, Ian Goodfellow, Somesh Jha, Z Berkay
Celik, and Ananthram Swami. 2017. Practical black-box attacks against machine
learning. In Proceedings of the 2017 ACM on Asia conference on computer and
communications security. 506â€“519.

[53] N. Papernot, P. McDaniel, X. Wu, S. Jha, and A. Swami. 2016. Distillation as a
Defense to Adversarial Perturbations Against Deep Neural Networks. In 2016
IEEE Symposium on Security and Privacy (SP). 582â€“597. https://doi.org/10.1109/
SP.2016.41

[54] Nicolas Papernot, Patrick D. McDaniel, Somesh Jha, Matt Fredrikson, Z. Berkay
Celik, and Ananthram Swami. 2015. The Limitations of Deep Learning in

Adversarial Settings. CoRR abs/1511.07528 (2015). arXiv:1511.07528 http:
//arxiv.org/abs/1511.07528

[55] Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory
Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, Alban
Desmaison, Andreas KÃ¶pf, Edward Yang, Zach DeVito, Martin Raison, Alykhan
Tejani, Sasank Chilamkurthy, Benoit Steiner, Lu Fang, Junjie Bai, and Soumith
Chintala. 2019. PyTorch: An Imperative Style, High-Performance Deep Learning
Library. arXiv:1912.01703 [cs.LG]

[56] Harry A Pierson and Michael S Gashler. 2017. Deep learning in robotics: a review

of recent research. Advanced Robotics 31, 16 (2017), 821â€“835.

[57] Aditi Raghunathan, Jacob Steinhardt, and Percy Liang. 2018. Certified Defenses

against Adversarial Examples. arXiv:1801.09344 [cs.LG]

[58] Joseph Redmon and Ali Farhadi. 2016. YOLO9000: Better, Faster, Stronger.

arXiv:1612.08242 [cs.CV]

[59] Andrew Slavin Ross and Finale Doshi-Velez. 2018. Improving the adversarial
robustness and interpretability of deep neural networks by regularizing their
input gradients. In Thirty-second AAAI conference on artificial intelligence.
[60] Pouya Samangouei, Maya Kabkab, and Rama Chellappa. 2018. Defense-gan:
Protecting classifiers against adversarial attacks using generative models. arXiv
preprint arXiv:1805.06605 (2018).

[61] Wojciech Samek. 2019. Explainable AI: interpreting, explaining and visualizing

deep learning. Vol. 11700. Springer Nature.

[62] Karen Simonyan and Andrew Zisserman. 2014. Very Deep Convolutional Net-

works for Large-Scale Image Recognition. arXiv:1409.1556 [cs.CV]
[63] Aman Sinha, Hongseok Namkoong, and John Duchi. 2017.

Certify-
ing Some Distributional Robustness with Principled Adversarial Training.
arXiv:1710.10571 [stat.ML]

[64] Liwei Song, Reza Shokri, and Prateek Mittal. 2019. Privacy Risks of Securing
Machine Learning Models against Adversarial Examples. Proceedings of the 2019
ACM SIGSAC Conference on Computer and Communications Security (Nov 2019).
https://doi.org/10.1145/3319535.3354211

[65] Christian Szegedy, Wojciech Zaremba, Ilya Sutskever, Joan Bruna, Dumitru Erhan,
Ian Goodfellow, and Rob Fergus. 2013. Intriguing properties of neural networks.
arXiv:1312.6199 [cs.CV]

[66] Enzo Tartaglione, Skjalg LepsÃ¸ y, Attilio Fiandrotti, and Gianluca Francini.
2018. Learning sparse neural networks via sensitivity-driven regularization.
In Advances in Neural Information Processing Systems 31, S. Bengio, H. Wallach,
H. Larochelle, K. Grauman, N. Cesa-Bianchi, and R. Garnett (Eds.). Curran Asso-
ciates, Inc., 3878â€“3888. http://papers.nips.cc/paper/7644-learning-sparse-neural-
networks-via-sensitivity-driven-regularization.pdf

[67] J. Y. F. Tong, D. Nagle, and R. A. Rutenbar. 2000. Reducing power by optimizing
the necessary precision/range of floating-point arithmetic. IEEE Transactions
on Very Large Scale Integration (VLSI) Systems 8, 3 (June 2000), 273â€“286. https:
//doi.org/10.1109/92.845894

[68] Florian TramÃ¨r, Alexey Kurakin, Nicolas Papernot, Ian Goodfellow, Dan Boneh,
and Patrick McDaniel. 2017. Ensemble adversarial training: Attacks and defenses.
arXiv preprint arXiv:1705.07204 (2017).

[69] Valerio Venceslai, Alberto Marchisio, Ihsen Alouani, Maurizio Martina, and
Muhammad Shafique. 2020. NeuroAttack: Undermining Spiking Neural Networks
Security through Externally Triggered Bit-Flips. arXiv:2005.08041 [cs.CR]
[70] Cihang Xie, Yuxin Wu, Laurens van der Maaten, Alan L Yuille, and Kaiming He.
2019. Feature denoising for improving adversarial robustness. In Proceedings of
the IEEE Conference on Computer Vision and Pattern Recognition. 501â€“509.
[71] Xiaoyong Yuan, Pan He, Qile Zhu, Rajendra Rana Bhat, and Xiaolin Li.
2017. Adversarial Examples: Attacks and Defenses for Deep Learning. CoRR
abs/1712.07107 (2017). arXiv:1712.07107 http://arxiv.org/abs/1712.07107
[72] Shuchang Zhou, Yuxin Wu, Zekun Ni, Xinyu Zhou, He Wen, and Yuheng Zou.
2018. DoReFa-Net: Training Low Bitwidth Convolutional Neural Networks with
Low Bitwidth Gradients. arXiv:1606.06160 [cs.NE]

