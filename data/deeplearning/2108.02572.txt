1
2
0
2

g
u
A
3

]

G
L
.
s
c
[

1
v
2
7
5
2
0
.
8
0
1
2
:
v
i
X
r
a

SINGA-Easy: An Easy-to-Use Framework for MultiModal
Analysis

Naili Xing1, Sai Ho Yeung1, Chenghao Cai1,4, Teck Khim Ng1, Wei Wang1, Kaiyuan Yang1, Nan
Yang1, Meihui Zhang2, Gang Chen3, Beng Chin Ooi1

1National University of Singapore, Singapore

2Beijing Institute of Technology, China

3Zhejiang University, China

4National University of Singapore (Suzhou) Research Institute, China

{xingnl,yeungsh,caich,ngtk,wangwei,yangky,yangn,ooibc}@comp.nus.edu.sg
meihui_zhang@bit.edu.cn
cg@zju.edu.cn

ABSTRACT
Deep learning has achieved great success in a wide spectrum of
multimedia applications such as image classification, natural lan-
guage processing and multimodal data analysis. Recent years have
seen the development of many deep learning frameworks that pro-
vide a high-level programming interface for users to design models,
conduct training and deploy inference. However, it remains chal-
lenging to build an efficient end-to-end multimedia application
with most existing frameworks. Specifically, in terms of usability, it
is demanding for non-experts to implement deep learning models,
obtain the right settings for the entire machine learning pipeline,
manage models and datasets, and exploit external data sources
all together. Further, in terms of adaptability, elastic computation
solutions are much needed as the actual serving workload fluctu-
ates constantly, and scaling the hardware resources to handle the
fluctuating workload is typically infeasible. To address these chal-
lenges, we introduce SINGA-Easy, a new deep learning framework
that provides distributed hyper-parameter tuning at the training
stage, dynamic computational cost control at the inference stage,
and intuitive user interactions with multimedia contents facilitated
by model explanation. Our experiments on the training and de-
ployment of multi-modality data analysis applications show that
the framework is both usable and adaptable to dynamic inference
loads. We implement SINGA-Easy on top of Apache SINGA and
demonstrate our system with the entire machine learning life cycle.

CCS CONCEPTS
• Information systems → Multimedia information systems;
• Computing methodologies → Machine learning; • Human-
centered computing → Systems and tools for interaction de-
sign.

KEYWORDS
deep learning, data analytics, multimedia application, distributed
training, dynamic inference

1 INTRODUCTION
Deep learning has been successfully adopted in a variety of mul-
timedia applications such as image classification, speech recog-
nition and news recommendation. Driven by the increasing de-
mand of real-world multimedia applications and an unprecedented
growth of big data, many Deep Learning (DL) techniques and sys-
tems [6, 44, 45] have been developed to facilitate the development
of AI applications. Although human-level performance has been

achieved in areas like computer vision [47], natural language pro-
cessing [13] and speech processing [46], the mass adoption of AI
applications remains elusive due to two major challenges.

The first challenge is usability [37]. Many AutoML frameworks
have been developed to improve usability. These include Auto-
WEKA [22], H20 AutoML [24], Auto-Sklearn [11], Auto-Pytorch
[49], and Auto-Keras [20]. Among these frameworks, some pro-
vide functionalities for hyper-parameter tuning that can work
with large datasets, and some support good user interaction and
experience. However, most of them do not take into account both.
Specifically, for hyper-parameters tuning, most DL model train-
ing processes focus on searching for the best hyper-parameter
configuration using a set of stochastic gradient descent based opti-
mization algorithms [3, 4, 21]. Many AutoML systems [25, 28] use
Bayesian Optimization (BO) [33, 40] to tune the hyper-parameters
automatically. This is often time consuming as they need to eval-
uate many different combinations of hyper-parameters to obtain
the best configuration. To accelerate the searching process, AI-as-
a-Service platforms such as Rafiki [43] tune the hyper-parameters
of the SGD algorithms in a distributed manner, but it pays little
attention to the model (or architecture) related hyper-parameters.
In terms of user interaction and experience, DL systems typi-
cally hide implementation details and appear like a black-box to
users. To provide better interaction and user control, an easy-to-
use APIs for managing the ML job in a finer-grained manner is
required. In addition, a good model explanation solution [34, 36, 38]
is also much needed in real-world applications, especially for those
high-stakes applications. For example, in the X-ray based medical
diagnosis [12, 30, 42], making a wrong decision may lead to cata-
strophic consequences, and meanwhile, providing explainable AI
solutions [14, 18, 41] can also engender user trust. Auto-WEKA
and H20 AutoML provide graphical user interfaces for datasets,
models and task management. In addition, H20 AutoML provides
a number of model explanation functions based on variable impor-
tance and dependency. However, most of these existing systems
do not provide full support for automatic hyper-parameter tuning,
good user interaction and model explanation.

The second challenge is adaptability – inference services have
to support elastic computation control in real-time, as it is not
practical and often infeasible to scale the computational resources
of the system to handling peak workloads. In practice, the users
may want to obtain more prediction results within a stipulated
time and meanwhile can tolerate a slight decrease in accuracy. A
conventional static model takes a fixed amount of computation and
thus can not trade off accuracy and efficiency dynamically, which

 
 
 
 
 
 
therefore is unable to handle the fluctuating workload. Clipper [8]
focuses on the prediction serving by introducing a new framework
and explores several optimization techniques such as caching and
model selection to improve latency and accuracy. However, it sim-
ply drops the instances if they cannot be processed within the time
limit in the presence of high workloads. Model-Switching [48] - an
online scheduler on top of Clipper, can select and switch to a dif-
ferent serving model based on the budget dynamically, which can
achieve higher effective accuracy. Nevertheless, multiple models
need to be trained beforehand and loaded to memory to support
runtime model selection, which incurs additional overheads. No-
tably, the ability to efficiently and effectively adapting the serving
model size to the current workload and computational resources
available is still lacking in most systems.

To address these two challenges, namely the usability and adapt-
ability, an easy-to-use deep learning framework supporting auto-
matic hyper-parameter selection, distributed training, dataset and
model management, model explanation, and elastic computation
control is required. We design and implement such a system on
top of Apache SINGA [29]. The main contributions of this work
are summarized as follows:

• We present an end-to-end open-sourced DL framework
called SINGA-Easy, which is developed to facilitate the adop-
tion of DL algorithms and inference services by domain-
specific multimedia application users. SINGA-Easy can au-
tomatically tune training jobs for pre-built complex models
and adapt the model size when facing high inference work-
loads. It also provides an intuitive APIs to manage the whole
DL/ML life cycle and retrieve the inference result from var-
ious perspectives.

• At the training stage, we propose a new auto-tuning frame-
work combining both a distributed hyper-parameter tuning
policy and an adaptive regularization method, which reduce
the effort required for training an efficient and accurate
model. We also integrate to our system a novel technique
to adapt the serving model size dynamically.

• At the inference stage, we focus on the evaluation metric ef-
fective accuracy and propose a new scheduling algorithm to
adapt the model size to the current workload in real-time for
achieving higher effective accuracy, and meanwhile satisfy
user-defined response time requirement under the available
computational resources. The algorithm also reduces man-
ual effort in the deployment, scaling and workload balancing
of the service.

• To facilitate the adoption of multimedia applications, we
integrate commonly used algorithms and provide an easy-
to-use APIs. We also provide an option for users to evaluate
model performance from the model explanation perspective
provided by LIME [36] and Grad-CAM [38].

• We demonstrate the usability and adaptability of our SINGA-

Easy by conducting experiments on various real-world datasets
and showcasing several multimedia applications.

The remainder of the paper is structured as follows. Section 2 in-
troduces the system architecture and the dataflow between the sys-
tem components. Section 3 introduces the dynamic model serving
framework. Section 4 presents system usability. Section 5 presents

Figure 1: SINGA-Easy system architecture overview.

the experimental study. We review the related work in Section 6
and conclude the paper in Section 7.

2 SYSTEM ARCHITECTURE
This section introduces the system architecture of SINGA-Easy.
The software stack is illustrated in Figure 1 and Figure 2 respec-
tively. The system consists of the frontend layer, the backend layer,
and the storage layer. Specifically, the frontend layer provides differ-
ent HTTP APIs to manage both data and tasks. Users can interact
with the framework via either Python SDK-Client or Web UI. In the
following subsections, we will introduce the other layers in detail.

2.1 Backend Layer
SINGA-Easy is built on top of the base architecture of Apache
SINGA. The backend of the overall system comprises five essen-
tial components: Admin, Training Worker, Advisor, Predictor and
Inference Worker.

Admin is the core component of the system’s control plane,
which exposes HTTP APIs for users to manage the whole ML
lifecycle. Upon receiving requests from users via RESTful APIs, it
deploys a number of workers for model training and serving, and
stores information of the user-defined tasks into a Metadata Store.
When a worker is deployed, the worker pulls the information of
the task from the Metadata Store and starts the task.

Training Worker trains models by conducting trials proposed
by a corresponding Advisor Worker. The computational kernel
of Training Worker supports various DL libraries in addition to
Apache SINGA e.g., PyTorch [31], and TensorFlow [1]. Figure 2
shows the stack diagram with Apache SINGA as the DL frame-
work, where the upper layers are constructed based on the lower
layers. For example, the component model is defined using Layer,
Autograd, Opt and Operator, etc. Each of them is built on top of
the basic data structures of Apache SINGA such as Tensor and
Communicator. We introduce the technical details of the training
part in Section 2.4.

Advisor performs hyper-parameter tuning by conducting multi-
ple trials on a training job. In each trial, it proposes the training
configuration, i.e., knobs of the model and training algorithm, to

AdminPredictorInference WorkerAdvisorTraining WorkerInference Deployment FrontendLayerBackendLayerStorageLayerWeb UI (ReactJS)DL Life Cycle ManagementMetadata Store(PostgreSQL)In-Memory Cache(Redis)File System(NFS/HDFS)Metadata & LogMessage queue(Kafka)ExamplesHyper-Parameter TuningTrainingDataTrainingDeploymentConsoleUser’s RequestRESTfulSINGA-Easy: An Easy-to-Use Framework for MultiModal Analysis

Figure 2: SINGA-Easy software stack.

be used by Training Worker. For the implementation of the Advisor,
we adopt the Bayesian Optimization of Scikit-Optimize toolbox1.
Predictor is designed for ensemble modelling, which stands
between users and Inference Workers. Predictor receives requests,
e.g., one or many images to be classified, from the users, then
forwards the requests to a number of Inference Workers and collects
the prediction results. Inference Worker manages trained models
for the inference jobs, which receives the request forwarded by
the Predictor and performs prediction. The technical details of
inference are discussed in Section 3.

2.2 Storage Layer
The storage layer contains the following components for caching
and storage of data:

Metadata Store is a centralized and persistent database used to
store the metadata of the whole system such as user metadata,
job metadata, worker metadata and model templates. We adopt
PostgreSQL2 in our system.

In-Memory Cache is an in-memory data structure store used for
fast asynchronous communication between Training Workers and
Advisor at the training stage. Redis3 is used as the in-memory data
store.

Message Queue is a file-based data store used for supporting
asynchronous communication between Inference Workers and Pre-
dictors for Inference Jobs. Apache Kafka4 is used in our system.

2.3 Workflow
SINGA-Easy allows users to manage the whole ML life cycle and
retrieve the prediction results from inference services. The user
firstly uploads the model, dataset, or annotation file to Admin,
which will be stored into a distributed storage (NFS). The metadata
will be stored into Metadata Store. When use send requests to
start training, Admin launches one Advisor and multiple Training
Workers. Advisor stores training configurations into In-Memory
Cache, and Training Workers will conduct training accordingly.

In each iteration, Training Workers report the training accuracy
to In-Memory Cache, which is then used by Advisor to generate new
configurations for the next training iteration. After completing
a training job, one Predictor and multiple Inference Workers will

1Scikit-Optimize: https://scikit-optimize.github.io/stable
2PostgreSQL: https://www.postgresql.org/
3Redis: https://redis.io
4kafka: https://kafka.apache.org

Figure 3: Training Worker: distributed model training.

be created. The user can retrieve the Predictor inference service’s
URL from Admin to use the model inference services. All these
components communicate with each other via a message queue.

2.4 Elastic Inference
Model slicing [5] is a general technique to enable deep learning
models to support elastic computation. Specifically, each layer of
the model is divided into equal-sized contiguous computational
groups. During both training and inference, there is a single param-
eter slice rate 𝑟 that dynamically controls the fraction of groups
involved in the computation for all the layers in the model, namely
the model width. In particular, these groups are trained dynami-
cally to build up representations residually. The first group learns
the base representation. Each subsequent group learns on top of
all its preceding groups. As a result, during inference, we can sup-
port accuracy-efficiency trade-offs by dynamically slicing a subnet
of a certain width, where only the parameters of the activated
groups are involved in computation. Theoretically, the number of
parameters and computation measured in FLOPs are both roughly
quadratic to the slice rate 𝑟 [5], e.g., a slice rate of 0.5 can achieve
up to four times speedup. Therefore, we can support elastic infer-
ence by introducing the model slicing technique to the training
stage. Specifically, we can train the model with multiple slice rates
beforehand. At the inference stage, the model can be switched to
different sub-models adaptively.

The overall training process is illustrated in Figure 3. We train
the model with model slicing to render the ability of elastic com-
putation. For efficiency, the system trains the sub-models in a
distributed manner by training these sub-model instances of differ-
ent slice rates in a pool of workers. After a few training iterations,
all workers merge their local copy of weights and update them
globally. SINGA-Easy reuses the distributed hyper-parameter tun-
ing component of Apache SINGA and adds an adaptive Gaussian
Mixture (GM) regularization technique [27] to further improve the
prediction performance of the model.

3 DYNAMIC MODEL SERVING
To support dynamic model serving, we further propose a sched-
uling algorithm based on the elastic inference enabled via model
slicing. At the inference stage, the user can send multiple requests,
each of which corresponds to one prediction task. A prediction
task may contains multiple instances to be processed by the serv-
ing model. As shown in Figure 4(a), each Client sends instances
and a global deadline constraint 𝐷 to Predictor. The Predictor and

Apache SINGASINGA-EasyONNXModelAutogradLayerOperatorOptTensorOperatorCommunicatorDevice SocketSchedulerMemPoolCPUGPUEthernetNVLinkMPINCCLHardwareBackendPythonInterfaceWeb UIAdminPredictorStorage (Metadata, Files)AdvisorTrain Worker Inference Worker ClientBackendSINGA-AUTOFrontendAdvisorTraining WorkerSliced DL ModelGM RegularizationNFS / HDFS Storage GM RegularizationGM RegularizationTraining DataParams Group 1Params Group 2Params Group 3Training WorkerSliced DL ModelTraining WorkerSliced DL ModelTraining DataTraining Data(a) Single model serving

(b) Multiple model serving

Figure 4: Inference stage: each message in the message
queue contains one instance and the global deadline con-
straint for this prediction task.

Inference Workers will then work cooperatively to produce the
inference result. The Predictor wrap each instance to be a message
in the form of 3-tuple < 𝐼𝑛𝑠𝑡𝑎𝑛𝑐𝑒, 𝐷,𝑇 𝑎𝑠𝑘𝐼𝐷 >, it then pushes all
messages to a message queue as a producer. The Inference Workers
read message of the same prediction task from the queue as con-
sumers, divide the instances into mini-batches, adjust the model
size according to the scheduling algorithm and then conduct the
inference. The mini-batch is the minimum scheduling unit, which
can contain one or more instances, e.g., one video for the video
objection detection task or several texts for the text classification
task. To fully utilize the GPU parallel computing capability, the
size of the mini-batch is typically set to the largest possible. The
algorithm is outlined in Algorithm 1. For ease of reference, we
summarize all the variables used in subsequent sections in Table 1.
To evaluate the effectiveness of the proposed dynamic model
serving, we adopt the effective accuracy [48] as the evaluation met-
ric, which is fraction of correctly processed instances returned
before a predefined deadline. Specifically, denoting the prediction
accuracy of a given model as 𝑝 and the fraction of instances that
can be processed by the model before the deadline 𝐷 as 𝑓 𝑇 , ef-
fective accuracy 𝑝𝑒 𝑓 𝑓 can then be computed by 𝑝𝑒 𝑓 𝑓 = 𝑝 ∗ 𝑓 𝑇 .
Briefly, effective accuracy takes into account both the accuracy and
efficiency of the serving model, as 𝑝 represents the prediction accu-
racy. Given the deadline, 𝑓 𝑇 is determined by the efficiency of the
model inference. Notably, 𝑓 𝑇 can also be seen as the throughput
within the given time frame. The goal of the scheduling algorithm
is formally defined as follows.

Given the number of instances to be processed 𝑁 and a global
deadline constraint 𝐷, the scheduling algorithm is to maximize the
effective accuracy under the available computational resources in the
model serving system.

We develop two scheduling algorithms in two respective scenar-
ios, where the available computational resources can support either
only one model in Section 3.1 or multiple models in Section 3.2. In
what follows, we denote that sub-model 𝑚𝑖 indexed by the slice
rate 𝑟𝑖 has a prediction accuracy 𝑝𝑖 and takes 𝑡𝑖 time on average to

Table 1: Summary of variables.

Symbol Meaning

𝑚𝑖
𝑟𝑖
𝑝𝑖
𝑡𝑖
𝑝𝑒 𝑓 𝑓
𝑖
𝐾
𝑛𝑖
(cid:99)𝑊𝑖
𝑁
𝑆𝑚𝑏

𝑁𝑚𝑏
𝐷
𝑊𝑒𝑥𝑝
𝑇𝑖
𝑇𝑓 𝑎𝑠𝑡
𝑇𝑠𝑙𝑜𝑤

The 𝑖-th sub-model
Slice rate of 𝑚𝑖
Accuracy of 𝑚𝑖
Time for 𝑚𝑖 to process a mini-batch
Effective accuracy of 𝑚𝑖
Number of trained sub-models
Number of mini-batches assigned to 𝑚𝑖
The maximum workload∗ that 𝑚𝑖 can handle
Number of instances in one prediction task
Number of instances in a mini-batch,
which is fixed on each prediction task.
Number of mini-batch, 𝑁𝑚𝑏 = 𝑁 / 𝑆𝑚𝑏
User-defined deadline to process 𝑁 instances
Expected workload of the system, 𝑊𝑒𝑥𝑝 = 𝑁 / 𝐷
Time spent by 𝑚𝑖 to process 𝑁 instances
𝑇𝑖 of the fastest sub-model
𝑇𝑖 of the slowest sub-model

*Workload: the number of instances to be processed per second.

process a mini-batch of a fixed number of instances. Typically, the
full model (i.e., 𝑟𝑖 = 1.0) has the highest accuracy while the low-
est efficiency. For both scenarios, there are 𝐾 sub-models trained
beforehand. The 𝑁 instances are divided into 𝑁𝑚𝑏 mini-batches
(each with 𝑁
instances) by Inference Worker. The sub-model 𝑚𝑖
𝑁𝑚𝑏
takes 𝑇𝑖 to process all 𝑁𝑚𝑏 mini-batches. Formally, the scheduling
algorithms are to determine the best scheduling policy denoted
as [𝑛1...𝑛𝑚], where 𝑛𝑖 is the number of mini-batches assigned to
sub-model 𝑚𝑖 .

3.1 Single Model Serving
In the first scenario where only one single model can be deployed
in the system, Algorithm 2 is adopted to dynamically adapt the
model size to meet the deadline requirement and to obtain the best
effective accuracy. The situations are summarized as follows:

• If 𝐷 ≤ 𝑇𝑓 𝑎𝑠𝑡 (see Figure 5 where 𝐷 ≤ 4), dropping mini-
batch will be unavoidable. In this case, the scheduler will
schedule all mini-batches to the fastest sub-model to mini-
mize the drop rate.

• If 𝐷 ≥ 𝑇𝑠𝑙𝑜𝑤 (see Figure 5 where 𝐷 ≥ 24), the scheduler will
schedule all mini-batches to the slowest but most accurate
sub-model.

• If 𝑇𝑓 𝑎𝑠𝑡 < 𝐷 < 𝑇𝑠𝑙𝑜𝑤 (see Figure 5 where 4 < 𝐷 < 24), more
than one sub-model is needed to achieve the best effective
accuracy.

For a single sub-model 𝑚𝑖 , it’s effective accuracy is defined as:

(cid:40)

𝑝𝑒 𝑓 𝑓
𝑖

=

𝑝𝑖, 𝑊𝑒𝑥𝑝 ≤ (cid:99)𝑊𝑖
((cid:99)𝑊𝑖 / 𝑊𝑒𝑥𝑝 ) ∗ 𝑝𝑖, 𝑊𝑒𝑥𝑝 > (cid:99)𝑊𝑖

(1)

When the expected workload 𝑊𝑒𝑥𝑝 is higher than the maximal
workload that a single sub-model can handle, i.e., (cid:99)𝑊𝑖 , the effec-
tive accuracy decreases since the sub-model cannot process all
instances before the deadline. In this case, the effective accuracy

MessageQueuewith a single partitionInference WorkerElastic ModelPredictorclientclientClientsRetrieve and dividemessage: < instance, deadline constraint, task ID >Task2Task2Task1Task1Task1Task1D2D2D1D1D1D1Ins1Ins2Ins3Ins4Ins5Ins6SchedulerMini-batch-list = [mb1,mb2…]mb11. Instances to be processed2.User-defined deadline DInference WorkerPredictorclientclientClientsInference WorkerMini-batch of instances with D1 and task1Mini-batch of instances with D1 and task1Task1Task1MessageQueuewith 3 partitionsTask2Task2Task2Task1Task1Task1D2D2D2D1D1D1Ins1Ins2Ins3Ins4Ins5Ins6Task2Task2Task1Task1Task1Task1D2D2D1D1D1D1Ins1Ins2Ins3Ins4Ins5Ins6SINGA-Easy: An Easy-to-Use Framework for MultiModal Analysis

Algorithm 1 Model Serving Predictor and Inference Worker
Input: 𝜇 = [𝑚1, . . . , 𝑚𝐾 ], 𝜌 = [𝑝1, . . . , 𝑝𝐾 ], 𝑆𝑚𝑏 , user’s requests
Output: Prediction results
1: function Predictor
2:

while True do

Receive user’s request with instances, 𝐷
generate task ID for this prediction task
message: < 𝐼𝑛𝑠𝑡𝑎𝑛𝑐𝑒, 𝐷,𝑇 𝑎𝑠𝑘𝐼𝐷 > ← each instance
Send the messages to the message queue

7:
end while
8: end function
9: function Inference Worker(𝜇, 𝜌, 𝑆𝑚𝑏 )
Get 𝜏 = [𝑡1, . . . , 𝑡𝐾 ] according to 𝑆𝑚𝑏
10:
while True do

11:

Retrieve 𝑁 messages with the same ID from the queue
Retrieve instances and 𝐷 from message tuples
Divide the instances into 𝑁𝑚𝑏 mini-batches of size 𝑆𝑚𝑏
Store the mini-batches as a list 𝛾
𝜂, 𝑝𝑒 𝑓 𝑓 (𝜂) ← Scheduler(𝜌, 𝜏, 𝐷, 𝑁𝑚𝑏 )
for 𝑛𝑖 in 𝜂 do

for j in [1, . . . , 𝑛𝑖 ] do

𝑅 ← 𝜇 [𝑖].prediction(𝛾 .𝑝𝑜𝑝 ())
Send 𝑅 as prediction results to the user

end for

3:

4:

5:

6:

12:

13:

14:

15:

16:

17:

18:

19:

20:

21:

22:

end for
23:
end while
24: end function

can be improved using multiple sub-models. The optimization ob-
jective now becomes a combinatorial optimization problem that
maximizes the follows:

𝑝𝑒 𝑓 𝑓 ( [𝑛1, . . . , 𝑛𝐾 ]) =

𝐾
∑︁

𝑖=1

(

𝑛𝑖
𝑁𝑚𝑏

∗ 𝑝𝑖 )

(2)

Moreover, 𝑛𝑖 is the number of mini-batches assigned to the 𝑖-th
sub-model, and at most 𝑁𝑚𝑏 mini-batches can be scheduled, we
thus have the following bound functions:

𝑛𝑖 ≥ 0 (𝑖 = 1, 2...𝐾)
𝑛𝑖 ∈ Z (𝑖 = 1, 2...𝐾)

where Z is the set of integers. We also have:

𝐾
∑︁

𝑖=1

𝑛𝑖 ≤ 𝑁𝑚𝑏

(3)

(4)

(5)

Additionally, the total time to process all of the scheduled mini-
batches is limited by 𝐷:

𝐾
∑︁

𝑖=1

(𝑛𝑖 ∗ 𝑡𝑖 ) ≤ 𝐷

(6)

The maximization of the objective function Eq. (2) with the
constraints of Eq. (3), (4), (4), (5) and (6) can be formulated as a
Integer Linear Programming (ILP) problem, which can be solved by
either the classical linear programming-based Branch-and-Bound
(B&B) method or Dynamic Programming (DP). Although DP can
find the optimal solution in polynomial time, the solution may

Figure 5: The total Time required to process all four mini-
batches using different sub-models.

not be precise as 𝐷 needs to be discretized, which forms a limited
number of sub-problems. Thus, the B&B method is used prior to
DP, which is shown in Algorithm 2. Specifically, the B&B method
takes the following steps: <s1> An initial linear programming
problem 𝑋0 is constructed by grouping Eq. (2), (5), (3) and (6).
Then 𝑋0 is pushed to the problem queue 𝜙. <s2> Retrieve a linear
programming problem 𝑋 from 𝜙 and get its optimal solution 𝛽 by
applying linear programming method. <s3> If all elements in 𝛽 are
integers, then 𝛽 satisfies Eq. (4) and will be a feasible solution of
the ILP problem. If 𝛽 leads to a higher 𝑝𝑒 𝑓 𝑓 , then 𝛽 will be used to
update 𝜂. <s4> If 𝛽 contains non-integers, eg. 𝛽 [𝑖] is a float number,
then two new sub-problems are generated by merging the problem
𝑋 and two respective new constraints, namely 𝑛𝑖 ≥ 𝑖𝑛𝑡 (𝛽 [𝑖]) + 1
and 𝑛𝑖 ≤ 𝑖𝑛𝑡 (𝛽 [𝑖]), which are then pushed into 𝜙. Steps <s2>,
<s3> and <s4> are iterated until 𝜙 is empty. Finally, the optimal
𝜂 can be obtained. If the B&B method fails to find the optimal 𝜂,
the ILP problem will be approximated as a classical 2-dimensional
unbounded knapsack problem and solved by DP.

Our experiments further confirm that the optimal solution of
Eq. (2) is also the best scheduling policy that achieves the highest
effective accuracy. For example, as shown in Figure 5, when the
user sets 𝐷 = 8, sub-model 1 can only serve one mini-batch within
the deadline. Sub-models 3 and 4 can meet the deadline but with
relatively low accuracy. The best scheduling policy is a combi-
nation of sub-models 2, 3 and 4 that will provide relatively high
accuracy and achieve a zero-drop rate. Specifically, mini-batch 1 is
assigned to sub-model 2. The model is then switched to sub-model
3 to process mini-batch 2 after processing mini-batch 1. Finally,
mini-batch 3 and 4 are processed using sub-model 4.

To further save the time spent on running the scheduling algo-
rithm, we precompute and store combinations of 𝐷 and 𝑁𝑚𝑏 in
In-Memory Cache to accelerate decision making.

3.2 Multiple Model Serving
In the second scenario where multiple models can be loaded to
the system, as shown in Figure 4(b), the Producer will partition
instances to different queues. Each queue is served by a dedicated
Inference Worker. The models in other Inference Workers are repli-
cated from the first Inference Worker. The model in each Inference
Worker is able to switch between sub-models. The global effective
accuracy is defined as the average of effective accuracy of each
Inference Worker. Suppose there are 𝑏 Inference Workers, the global
effective accuracy= 1
. The global best effective accuracy
𝑏
is equal to the average of the local best effective accuracy. To get
the best local effective accuracy, each Inference Worker runs Algo-
rithm 2 separately under the original deadline constraint 𝐷 and
the number of mini-batches 𝑁𝑚𝑏 in the corresponding queue.

𝑗=1 𝑝𝑒 𝑓 𝑓
(cid:205)𝑏

𝑗

124641246212461124610123456789101112131415161718192021222324Fastest sub model 4Sub model 3Sub model 2Slowest sub model 1Combination of sub modelsMini-batch 1Mini-batch 2Mini-batch 3Mini-batch 4Deadline constraint D = 4Deadline constraint D = 8Deadline constraint D = 24Algorithm 2 Scheduling Algorithm
Input: 𝜌 = [𝑝1, . . . , 𝑝𝐾 ], 𝜏 = [𝑡1, . . . , 𝑡𝐾 ], 𝐷, 𝑁𝑚𝑏
Output: 𝜂 = [𝑛1, . . . , 𝑛𝐾 ], theoretical 𝑝𝑒 𝑓 𝑓 (𝜂)
1: function Scheduler(𝜌, 𝜏, 𝐷, 𝑁𝑚𝑏 )
2:

𝜂 ← [0, . . . , 0]
if 𝐷 ≤ 𝑁𝑚𝑏 × Min(𝜏) then 𝜂 [Argmin(𝜏)] ← 𝑁𝑚𝑏
end if
if 𝐷 ≥ 𝑁𝑚𝑏 × Max(𝜏) then 𝜂 [Argmax(𝜏)] ← 𝑁𝑚𝑏
end if
if 𝑁𝑚𝑏 × Min(𝜏) < 𝐷 < 𝑁𝑚𝑏 × Max(𝜏) then
𝜙 ← Queue([{Eq.(2), (3), (5), (6)}])
while 𝜙 ≠ [] do
𝑋 ← 𝜙 .Pop
𝛽 ← Linear_Programming(𝑋 )
if 𝑝𝑒 𝑓 𝑓 (𝛽) > 𝑝𝑒 𝑓 𝑓 (𝜂) then

if ∀𝑖. 𝛽 [𝑖] ∈ Z then 𝜂 ← 𝛽
else select 𝑖 such that 𝛽 [𝑖] ∉ Z

𝜙 .Push(𝑋 ∪ {𝑛𝑖 ≥ Int(𝛽 [𝑖]) + 1})
𝜙 .Push(𝑋 ∪ {𝑛𝑖 ≤ Int(𝛽 [𝑖])})

end if

end if
end while
// If B&B fails to find a solution
if 𝜂 = [0, . . . , 0] then

𝜂 ← 2D_Unbounded_Knapsack_DP(𝜌, 𝜏, 𝐷, 𝑁𝑚𝑏 )

end if

3:

4:

5:

6:

7:

8:

9:

10:

11:

12:

13:

14:

15:

16:

17:

18:

19:

20:

21:

22:

23:

24:

end if
return 𝜂, 𝑝𝑒 𝑓 𝑓 (𝜂)

25:
26: end function

(a) Inference time of different sub-models.

(b) Illustration of the impact of the scheduling algorithm on Effective
Accuracy (ResNet-50 trained with model slicing on NIH Chest X-rays).

Figure 7: Inference time and effective accuracy.

improve the reliability of inference, we have also implemented two
model explanation techniques, i.e., Grad-CAM [38] and LIME [36].

5 EXPERIMENTAL EVALUATION
To evaluate the usability and adaptability of SINGA-Easy, we con-
duct experiments on different multi-media datasets using various
DL network architectures. SINGA-Easy is deployed on clusters
equipped with GTX1080Ti GPUs and two models of CPUs. The
CPU model deployed in Admin node is Intel Xeon CPU E5-2620
v4 @ 2.10GHz. The CPU model deployed in the other nodes is
Intel Xeon CPU E5-1650 v4 @ 3.60GHz. All nodes are connected
via Ethernet at 1 Gbit/s. We run all services in Docker and use
Kubernetes 1.6 as the cluster manager.

5.1 Experimental Setup
In this section, we introduce the training details, namely the datasets
adopted for the evaluation and the training results.

Figure 6: SINGA-Easy APIs for supporting X-ray image clas-
sification.

4 SYSTEM USABILITY
To improve the usability of ML and DL models in multimedia appli-
cations such as medical image classification, food recognition, di-
etary management, question answering, and speech classification,
SINGA-Easy provides built-in models using third-party libraries
based on PyTorch [31], TensorFlow [1] and Scikit-learn [32]. Table
2 lists representative models for six common multimedia tasks.

Figure 6 shows an example on the use of SINGA-Easy’s APIs
to quickly develop applications with the supported models. To

50001000015000200002500030000Ingesting rate(#instances/second)1020304050Inference time per mini-batch(millisecond)Sub-model with ri = 1Sub-model with ri = 0.75Sub-model with ri = 0.5Sub-model with ri = 0.25050001000015000200002500030000Wexp=N/D (# instances / deadline constraint)0.10.20.30.40.50.60.70.80.9Effective accuracySub-model with ri = 1, pi = 79.36Sub-model with ri = 0.75, pi = 71.88Sub-model with ri = 0.5, pi = 70.94Sub-model with ri = 0.25, pi = 65.12Combination of sub-modelsSINGA-Easy: An Easy-to-Use Framework for MultiModal Analysis

(a) Throughput

(b) Tail latency

(c) Ingesting rate on scheduled sub-models.

Figure 8: Adaptability Experiments in SINGA-Easy

Table 2: Models for multimedia applications.

Table 3: Accuracy and inference time of sliced ResNet-50.

Model

Scenario

VGG [39], ResNet [16]
Mask RCNN [15]
Deep Speech [2]
BERT [10]
Random Forest [17]
XGBoost [7]

Image Classification
Object Detection
Speech Recognition
Question and Answering
Tabular Classification
Tabular Regression

X-Ray

ImageNet12
Slice
𝑇𝑎𝑣𝑒
𝑇𝑎𝑣𝑒
Acc.
Rate
49.40
75.09
1
45.12
38.08
73.74
0.75
34.56
22.95
71.09
0.5
22.72
17.82
63.91
0.25
15.68
*𝑇𝑎𝑣𝑒 : average inference time (ms) to process a mini-batch.

CIFAR-10
𝑇𝑎𝑣𝑒
12.48
9.92
6.41
3.24

Acc.
91.13
88.41
85.19
79.71

Acc.
79.37
71.88
70.94
65.12

5.1.1 Datasets. We evaluate the training and inference efficiency
of SINGA-Easy using model ResNet-50 on three image classifica-
tion datasets, namely CIFAR [23], ILSVRC 2012 [9] and NIH Chest
X-rays dataset5. To demonstrate the general support of SINGA-
Easy for different applications, we train Inception-ResNet-v2 and
yoloV3 [35] on five food datasets for food image classification and
detection and visualize the results. We use 50,000 training images
and 10,000 test images from the CIFAR dataset. Each CIFAR image
is resized to 32 × 32. We use 1.2 million training images and 50,000
test images drawn from 1,000 classes in the ILSVRC 2012 dataset.
And resize them to 244 × 244. We also use 5,234 training images
and 634 test images from the NIH Chest X-rays dataset. Each image
of the X-rays dataset is classified as "healthy" or "unhealthy" and
is normalized from 1600 × 1125 to 244 × 244. We also use five
Singapore food datasets. The number of classes in each dataset is
55, 101, 172, 231 and 256, respectively. Each class contains 300 to
500 images of size 624 × 1024.

5.1.2 Training Results. ResNet-50 is trained on each dataset with
SGD. Specifically, we train 100/100/300 epochs on CIFAR-10/ILSVRC
2012/NIH X-arays with a batch size of 128/64/64, respectively. We
summarize the statistics of the trained models in Table 3.

In order to demonstrate the scalability of SINGA-Easy, we en-
abled 2, 4, 8, 16, 32 and 64 parallel workers in a cluster and tested
the wall time for the parallel workers to complete 64 training jobs

5NIH Chest X-rays: www.kaggle.com/nih-chest-xrays/data

in total. Each worker was allowed to use at most 12 logical pro-
cessors and 32GB of memory. For each training job, a multi-layer
perceptron with random hyper-parameters, i.e. the initial learning
rate, the number of hidden layers and the dimensionality of hidden
layers, was trained using 1,000 gray images drawn from 10 classes.
Each image was of size 50×50 pixels. Figure 10 shows the wall time
with respect to the number of parallel workers. It demonstrated
that multiple training jobs can be effectively run in parallel in our
system.

5.2 Dynamic Model Serving Evaluation
The adaptability of SINGA-Easy is evaluated using ResNet-50
trained on dataset NIH Chest X-rays. In the following experiments,
we set the mini-batch size 𝑆𝑚𝑏 to 32 for the evaluation.

We first measure the inference time 𝑡𝑖 to process a single mini-
batch with different sub-models. Then we measure the effective
accuracy of SINGA-Easy under the first scenario where only one
single model can be loaded to the system.

To measure the actual inference time, we enable GPU warm-up
and GPU/CPU synchronization. We also use torch.cuda.Event to
capture the time before and after model inference. Specifically, we
record the inference time with different ingesting rates from 32 to
25,000 instances/second as shown in Figure 7(a). Then the inference
time is averaged to obtain the 𝑡𝑖 of sub-model 𝑚𝑖 . Results in Table 3
show that both the accuracy and inference time decreases with a
smaller slice rate, which is consistent with the previous discussion.

50001000015000200002500030000Ingesting rate(#instances/second)010203040506070Throughput(mini-batches processed/second)Sub-model with ri = 1Sub-model with ri = 0.75Sub-model with ri = 0.5Sub-model with ri = 0.25Combination of sub-modelsTheoretical value50001000015000200002500030000Ingesting rate(#instances/second)05101520Tail Latency(second)Sub-model with ri = 1Sub-model with ri = 0.75Sub-model with ri = 0.5Sub-model with ri = 0.25Combination of sub-modelsTheoretical valueDeadline constrain=8s500010000150002000025000Ingesting rate(#instances/second)0100200300400500600700800# Mini-batchesdeadline constraint D=8sSub-model with ri = 1Sub-model with ri = 0.75Sub-model with ri = 0.5Sub-model with ri = 0.25Figure 9: Two multimedia applications(X-ray based diagnosis and Food Detection) developed using SINGA-Easy. The correct-
ness of X-ray explanation is confirmed by the overlap between the explanation map and the ground truth.

the sub-model of a slice rate 0.75 and 0.5 have similar accuracy,
while the sub-model of a slice rate 0.5 is much faster, the scheduling
algorithm tends to use the later sub-model for achieving higher
effective accuracy.

For the second scenario, where multiple models can be loaded
to the system, SINGA-Easy can have multiple elastic models and
can generate multiple combinations of sub-models. In contrast,
Model-Switching can only have fixed combinations of models.

In conclusion, the experiments on effective accuracy, through-
put, latency, and sub-model combinations confirm that the model
trained with the model slicing technique and our proposed sched-
uling algorithm support dynamic workloads via finer-grained elas-
tic computation control. It further illustrates the adaptability of
SINGA-Easy.

5.3 Multimedia Applications
We further demonstrate the usability of SINGA-Easy on various
applications. Due to the space limit, we showcase representative
examples in Figure 9. The Singapore Food Detection component
has been used to develop FoodLG app6, which is customized for
healthcare applications such as pre-diabetes management and diet
recommendation. For the training dataset, we crowdsource to
knowledge users using CDAS[26] for labelling. Medical applica-
tions like X-ray-based diagnosis is shown in Figure 9, the GradCam
map highlights the unhealthy areas with warm colors (red and
purple). The LIME map circles the unhealthy areas with yellow
color. The explanation maps can assist clinicians in verifying the
correctness of the diagnosis, e.g., whether explanation maps match
is in line with their diagnosis.

6 RELATED WORK
In this section, we review the related work of ML/DL systems and
framework. Their are highly accessible and could be used to extend
our SINGA-Easy.

PyTorch [31] can achieve automated ML using the Auto PyTorch
library [49], but it does not provide the system infrastructure for
ML life cycle management in multimedia applications. SINGA-Easy
can be used to facilitate the PyTorch models.

Microsoft NNI7 is a ML framework supporting model compres-
sion. However, it does not provide elastic inference capabilities

6http://foodlg.com/
7Microsoft NNI: https://github.com/microsoft/nni

Figure 10: Scalability test of multiple workers.

To measure the effective accuracy of SINGA-Easy, we set the
deadline constraint to 𝐷 = 8𝑠 and gradually increase the number
of ingested instances 𝑁 from 32 to 30,000. As shown in Figure 7(b),
the model equipped with the scheduler can adapt to the workload
by switching between sub-models, which leads to higher effective
accuracy. Specifically, when 𝑊𝑒𝑥𝑝 = 100, the serving model is the
full model (i.e., the slice rate 𝑟 = 1.0). When 𝑊𝑒𝑥𝑝 = 10, 000, the
serving model is switched to the smaller model of a slice rate 0.5
to avoid dropping instances. When 𝑊𝑒𝑥𝑝 = 18, 000, where even
the smallest sub-model of a slice rate 0.25 can not process all in-
stances within the time limit. In such scenarios, the serving model
is switched to the smallest sub-model to maximize the through-
put. As shown in Figure 8(a), the system dynamically adapt the
model size to increase the throughput until it reaches the maximum
throughput, which is the same as the fastest sub-model.

We also measure the latency of the system, which is shown in
Figure 8(b). Specifically, when the ingesting rate is low, since all
sub-models now can meet the deadline, the scheduler will adopt
the sub-model of slice-rate 1.0 for higher accuracy. When the
ingesting rate reach around 10,000, both the sub-models of a slice-
rate 1.0 and 0.75 cannot process all instances before the deadline.
The combination of sub-models however, can meet the deadline
constraint until the ingesting rate reach 18,000, where the serving
model will entirely switch to the sub-model of a slice rate 0.25.

To better illustrates the combinations of the scheduled sub-
models under different instance ingesting rates, we present detailed
assignment of the mini-batches to the sub-models in Figure 8(c).
We can observer that when the ingesting rate is low, the model
assigns all mini-batches to the sub-model of a slice rate 1.0. Since

Ground truthGround truthClassified to Unhealthy, explained with LIMEClassified to Unhealthy, explained with GradCamSINGA-Easy: An Easy-to-Use Framework for MultiModal Analysis

to the models. While the slice-rate in SINGA-Easy is more under-
standable.

Hopswork [19] is a data science platform for the design and op-
eration of data analytics applications. The system applies HopsFS,
a highly scalable distributed file system, to improve its efficiency.
while our system focuses more on the usability to AI applications.
In summary, there are indeed many data analytics systems devel-
oped in recent years. Our SINGA-Easy is designed to improve the
usability and adaptability in developing multimedia applications.

7 CONCLUSIONS
In this paper, we introduced SINGA-Easy - a learning system focus-
ing on usability and adaptability. SINGA-Easy was built on top of
Apache SINGA. It assists users in managing data and models, and
developing AI applications. We have used SINGA-Easy to develop
multi-media applications such as a chest X-ray image explanation
function and food detection system. We showed that SINGA-Easy
is highly extendable as it can be used with various third-party
machine learning models.

Moving forward, we note that there exist other bottlenecks in
data science such as data loading, visualization, cleaning, labeling,
and data transformation. Future extensions to SINGA-Easy may
include such data science supporting modules.

Acknowledgement: We thank the anonymous reviewers for
their constructive comments and NUS colleagues for their com-
ments and contributions. This research is supported by Singa-
pore Ministry of Education Academic Research Fund Tier 3 under
MOE’s official grant number MOE2017-T3-1-007. Meihui Zhang’s
work is supported by the National Natural Science Foundation of
China (62050099).

REFERENCES
[1] Martín Abadi, Paul Barham, Jianmin Chen, Zhifeng Chen, Andy Davis, Jeffrey
Dean, Matthieu Devin, Sanjay Ghemawat, Geoffrey Irving, Michael Isard, Manju-
nath Kudlur, Josh Levenberg, Rajat Monga, Sherry Moore, Derek Gordon Murray,
Benoit Steiner, Paul A. Tucker, Vijay Vasudevan, Pete Warden, Martin Wicke,
Yuan Yu, and Xiaoqiang Zheng. 2016. TensorFlow: A System for Large-Scale
Machine Learning. In 12th USENIX Symposium on Operating Systems Design and
Implementation (OSDI). USENIX Association, Savannah, GA, USA, 265–283.
[2] Dario Amodei, Sundaram Ananthanarayanan, Rishita Anubhai, Jingliang Bai,
Eric Battenberg, Carl Case, Jared Casper, Bryan Catanzaro, Jingdong Chen,
Mike Chrzanowski, Adam Coates, Greg Diamos, Erich Elsen, Jesse H. Engel,
Linxi Fan, Christopher Fougner, Awni Y. Hannun, Billy Jun, Tony Han, Patrick
LeGresley, Xiangang Li, Libby Lin, Sharan Narang, Andrew Y. Ng, Sherjil Ozair,
Ryan Prenger, Sheng Qian, Jonathan Raiman, Sanjeev Satheesh, David Seetapun,
Shubho Sengupta, Chong Wang, Yi Wang, Zhiqian Wang, Bo Xiao, Yan Xie, Dani
Yogatama, Jun Zhan, and Zhenyao Zhu. 2016. Deep Speech 2: End-to-End Speech
Recognition in English and Mandarin. In Proceedings of the 33rd International
Conference on Machine Learning (ICML). ACM, New York, NY, USA, 173–182.
[3] Léon Bottou. 2010. Large-Scale Machine Learning with Stochastic Gradient De-
scent. In 19th International Conference on Computational Statistics (COMPSTAT).
Springer, Paris, France, 177–186.

[4] Léon Bottou. 2012. Stochastic Gradient Descent Tricks. In Neural Networks:
Tricks of the Trade - Second Edition. Springer, Heidelberg, Berlin, 421–436.
[5] Shaofeng Cai, Gang Chen, Beng Chin Ooi, and Jinyang Gao. 2019. Model Slicing
for Supporting Complex Analytics with Elastic Inference Cost and Resource
Constraints. Proceedings of the VLDB Endowment 13, 2 (2019), 86–99.

[6] Shaofeng Cai, Jinyang Gao, Meihui Zhang, Wei Wang, Gang Chen, and
Beng Chin Ooi. 2019. Effective and Efficient Dropout for Deep Convolu-
tional Neural Networks. CoRR abs/1904.03392 (2019). arXiv:1904.03392 http:
//arxiv.org/abs/1904.03392

[7] Tianqi Chen and Carlos Guestrin. 2016. XGBoost: A Scalable Tree Boosting
System. In Proceedings of the 22nd ACM SIGKDD International Conference on
Knowledge Discovery and Data Mining. ACM, San Francisco, CA, USA, 785–794.
[8] Daniel Crankshaw, Xin Wang, Giulio Zhou, Michael J. Franklin, Joseph E. Gon-
zalez, and Ion Stoica. 2017. Clipper: A Low-Latency Online Prediction Serving

System. In 14th USENIX Symposium on Networked Systems Design and Imple-
mentation, NSDI 2017, Boston, MA, USA, March 27-29, 2017. USENIX Association,
613–627.

[9] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Fei-Fei Li. 2009. Im-
(2009), 248–255. https:

ageNet: A large-scale hierarchical image database.
//doi.org/10.1109/CVPR.2009.5206848

[10] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. BERT:
Pre-training of Deep Bidirectional Transformers for Language Understanding.
In Proceedings of the Conference of the North American Chapter of the Association
for Computational Linguistics: Human Language Technologies (NAACL-HLT).
NAACL, Minneapolis, MN, USA, 4171–4186.

[11] Matthias Feurer, Aaron Klein, Katharina Eggensperger, Jost Tobias Springenberg,
Manuel Blum, and Frank Hutter. 2015. Efficient and Robust Automated Machine
Learning. In Advances in Neural Information Processing Systems: Annual Con-
ference on Neural Information Processing Systems. MIT Press, Montreal, Quebec,
Canada, 2962–2970.

[12] Kenneth R Foster, Robert Koprowski, and Joseph D Skufca. 2014. Machine
learning, medical diagnosis, and biomedical engineering research-commentary.
Biomedical engineering online 13 (2014), 94.

[13] Matt Gardner, Joel Grus, Mark Neumann, Oyvind Tafjord, Pradeep Dasigi,
Nelson F. Liu, Matthew E. Peters, Michael Schmitz, and Luke Zettlemoyer. 2018.
AllenNLP: A Deep Semantic Natural Language Processing Platform. CoRR
abs/1803.07640 (2018), 6 pages.

[14] Randy Goebel, Ajay Chander, Katharina Holzinger, Freddy Lécué, Zeynep Akata,
Simone Stumpf, Peter Kieseberg, and Andreas Holzinger. 2018. Explainable
AI: The New 42?. In Machine Learning and Knowledge Extraction - Second IFIP
International Cross-Domain Conference. Springer, Hamburg, Germany, 295–303.
[15] Kaiming He, Georgia Gkioxari, Piotr Dollár, and Ross B. Girshick. 2017. Mask R-
CNN. In IEEE International Conference on Computer Vision (ICCV). IEEE, Venice,
Italy, 2980–2988.

[16] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. 2016. Deep Residual
Learning for Image Recognition. In 2016 IEEE Conference on Computer Vision
and Pattern Recognition (CVPR). IEEE, Las Vegas, NV, USA, 770–778.

[17] Tin Kam Ho. 1995. Random decision forests. In Third International Conference on
Document Analysis and Recognition (ICDAR). IEEE, Montreal, Canada, 278–282.
[18] Andreas Holzinger, Chris Biemann, Constantinos S. Pattichis, and Douglas B.
Kell. 2017. What do we need to build explainable AI systems for the medical
domain? CoRR abs/1712.09923 (2017), 28 pages. http://arxiv.org/abs/1712.09923
[19] Mahmoud Ismail, Ermias Gebremeskel, Theofilos Kakantousis, Gautier Berthou,
and Jim Dowling. 2017. Hopsworks: Improving User Experience and Devel-
opment on Hadoop with Scalable, Strongly Consistent Metadata. In 37th IEEE
International Conference on Distributed Computing Systems (ICDCS). IEEE, At-
lanta, GA, USA, 2525–2528.

[20] Haifeng Jin, Qingquan Song, and Xia Hu. 2019. Auto-Keras: An Efficient Neural
Architecture Search System. In Proceedings of the 25th ACM SIGKDD International
Conference on Knowledge Discovery & Data Mining (KDD). ACM, Anchorage,
AK, USA, 1946–1956.

[21] Diederik P. Kingma and Jimmy Ba. 2015. Adam: A Method for Stochastic
Optimization. In 3rd International Conference on Learning Representations (ICLR).
ICLR Press, San Diego, CA, USA, 15 pages.

[22] Lars Kotthoff, Chris Thornton, Holger H. Hoos, Frank Hutter, and Kevin Leyton-
Brown. 2017. Auto-WEKA 2.0: Automatic model selection and hyperparameter
optimization in WEKA. Journal of Machine Learning Research 18 (2017), 25:1–
25:5.

[23] Alex Krizhevsky and Geoffrey Hinton. 2009. Learning multiple layers of features

from tiny images. (2009).

[24] Erin LeDell and Sebastien Poirier. 2020. H2O AutoML: Scalable Automatic
Machine Learning. In 7th ICML Workshop on Automated Machine Learning
(AutoML). ACM, Virtual Conference, 16 pages.

[25] Sijia Liu, Parikshit Ram, Deepak Vijaykeerthy, Djallel Bouneffouf, Gregory
Bramble, Horst Samulowitz, Dakuo Wang, Andrew Conn, and Alexander G.
Gray. 2020. An ADMM Based Framework for AutoML Pipeline Configuration.
In The Thirty-Fourth AAAI Conference on Artificial Intelligence (AAAI). AAAI,
New York, NY, USA, 4892–4899.

[26] Xuan Liu, Meiyu Lu, Beng Chin Ooi, Yanyan Shen, Sai Wu, and Meihui Zhang.
2012. CDAS: A Crowdsourcing Data Analytics System. Proc. VLDB Endow. 5, 10
(2012), 1040–1051. https://doi.org/10.14778/2336664.2336676

[27] Zhaojing Luo, Shaofeng Cai, Jinyang Gao, Meihui Zhang, Kee Yuan Ngiam, Gang
Chen, and Wang-Chien Lee. 2018. Adaptive lightweight regularization tool for
complex analytics. In IEEE 34th International Conference on Data Engineering
(ICDE). 485–496.

[28] Jorge G. Madrid, Hugo Jair Escalante, Eduardo F. Morales, Wei-Wei Tu, Yang
Yu, Lisheng Sun-Hosoya, Isabelle Guyon, and Michèle Sebag. 2019. Towards
AutoML in the presence of Drift: first results. CoRR abs/1907.10772 (2019), 14
pages. http://arxiv.org/abs/1907.10772

[29] Beng Chin Ooi, Kian-Lee Tan, Sheng Wang, Wei Wang, Qingchao Cai, Gang
Chen, Jinyang Gao, Zhaojing Luo, Anthony K. H. Tung afnd Yuan Wafng, Zhon-
gle Xie, Meihui Zhang, and Kaiping Zheng. 2015. SINGA: A Distributed Deep

Learning Platform. In Proceedings of the 23rd Annual ACM Conference on Multi-
media Conference. ACM, Brisbane, Australia, 685–688.

[30] Akin Özçift and Arif Gülten. 2011. Classifier ensemble construction with rotation
forest to improve medical diagnosis performance of machine learning algorithms.
Computer Methods and Programs in Biomedicine 104, 3 (2011), 443–451.
[31] Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gre-
gory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga,
Alban Desmaison, Andreas Köpf, Edward Yang, Zachary DeVito, Martin Raison,
Alykhan Tejani, Sasank Chilamkurthy, Benoit Steiner, Lu Fang, Junjie Bai, and
Soumith Chintala. 2019. PyTorch: An Imperative Style, High-Performance Deep
Learning Library. In Advances in Neural Information Processing Systems 32: An-
nual Conference on Neural Information Processing Systems. MIT Press, Vancouver,
BC, Canada, 8024–8035.

[32] Fabian Pedregosa, Gaël Varoquaux, Alexandre Gramfort, Vincent Michel,
Bertrand Thirion, Olivier Grisel, Mathieu Blondel, Peter Prettenhofer, Ron
Weiss, Vincent Dubourg, Jake VanderPlas, Alexandre Passos, David Cournapeau,
Matthieu Brucher, Matthieu Perrot, and Edouard Duchesnay. 2011. Scikit-learn:
Machine Learning in Python. J. Mach. Learn. Res. 12 (2011), 2825–2830.
[33] Martin Pelikan, David E Goldberg, Erick Cantú-Paz, et al. 1999. BOA: The
Bayesian optimization algorithm. In Proceedings of the genetic and evolutionary
computation conference (GECCO). ACM, Orlando, Florida, USA, 525–532.
[34] Vitali Petsiuk, Abir Das, and Kate Saenko. 2018. RISE: Randomized Input Sam-
pling for Explanation of Black-box Models. In British Machine Vision Conference
2018 (BMVC). British Machine Vision Association, Newcastle, UK, 151.
[35] Joseph Redmon and Ali Farhadi. 2018. YOLOv3: An Incremental Improvement.
CoRR abs/1804.02767 (2018). arXiv:1804.02767 http://arxiv.org/abs/1804.02767
[36] Marco Túlio Ribeiro, Sameer Singh, and Carlos Guestrin. 2016. "Why Should I
Trust You?": Explaining the Predictions of Any Classifier. In Proceedings of the
22nd ACM SIGKDD International Conference on Knowledge Discovery and Data
Mining. ACM, San Francisco, CA, USA, 1135–1144.

[37] Yuji Roh, Geon Heo, and Steven Euijong Whang. 2021. A Survey on Data
Collection for Machine Learning: A Big Data - AI Integration Perspective. IEEE
Transactions on Knowledge and Data Engineering 33, 4 (2021), 1328–1347.
[38] Ramprasaath R. Selvaraju, Michael Cogswell, Abhishek Das, Ramakrishna Vedan-
tam, Devi Parikh, and Dhruv Batra. 2017. Grad-CAM: Visual Explanations from
Deep Networks via Gradient-Based Localization. In IEEE International Confer-
ence on Computer Vision (ICCV). IEEE, Venice, Italy, 618–626.

[39] Karen Simonyan and Andrew Zisserman. 2015. Very Deep Convolutional Net-
works for Large-Scale Image Recognition. In 3rd International Conference on

Learning Representations (ICLR). ICLR Press, San Diego, CA, USA, 15 pages.
[40] Jasper Snoek, Hugo Larochelle, and Ryan P. Adams. 2012. Practical Bayesian
Optimization of Machine Learning Algorithms. In Advances in Neural Informa-
tion Processing Systems: 26th Annual Conference on Neural Information Processing
Systems. MIT Press, Lake Tahoe, Nevada, United States, 2960–2968.

[41] Danding Wang, Qian Yang, Ashraf Abdul, and Brian Y Lim. 2019. Designing
theory-driven user-centric explainable AI. In Proceedings of the CHI conference
on human factors in computing systems. ACM, Glasgow, Scotland, UK, 1–15.
[42] Fei Wang, Rainu Kaushal, and Dhruv Khullar. 2020. Should health care demand
interpretable artificial intelligence or accept “black box” medicine? Annals of
Internal Medicine 172, 1 (2020), 59–60.

[43] Wei Wang, Jinyang Gao, Meihui Zhang, Sheng Wang, Gang Chen, Teck Khim
Ng, Beng Chin Ooi, Jie Shao, and Moaz Reyad. 2018. Rafiki: Machine Learning
as an Analytics Service System. Proceedings of the VLDB Endowment 12, 2 (2018),
128–140.

[44] Wei Wang, Xiaoyan Yang, Beng Chin Ooi, Dongxiang Zhang, and Yueting
Zhuang. 2016. Effective deep learning-based multi-modal retrieval. The VLDB
Journal 25, 1 (2016), 79–101.

[45] Wei Wang, Meihui Zhang, Gang Chen, HV Jagadish, Beng Chin Ooi, and Kian-
Lee Tan. 2016. Database meets deep learning: Challenges and opportunities.
ACM SIGMOD Record 45, 2 (2016), 17–22.

[46] Shinji Watanabe, Takaaki Hori, Shigeki Karita, Tomoki Hayashi, Jiro Nishitoba,
Yuya Unno, Nelson Enrique Yalta Soplin, Jahn Heymann, Matthew Wiesner,
Nanxin Chen, Adithya Renduchintala, and Tsubasa Ochiai. 2018. ESPnet: End-
to-End Speech Processing Toolkit. In Proceedings of the 19th Annual Conference
of the International Speech Communication Association (Interspeech). Elsevier,
Hyderabad, India, 2207–2211.

[47] Yang You, Zhao Zhang, Cho-Jui Hsieh, James Demmel, and Kurt Keutzer. 2018.
ImageNet Training in Minutes. In Proceedings of the 47th International Conference
on Parallel Processing (ICPP). ACM, Eugene, OR, USA, 1:1–1:10.

[48] Jeff Zhang, Sameh Elnikety, Shuayb Zarar, Atul Gupta, and Siddharth Garg. 2020.
Model-Switching: Dealing with Fluctuating Workloads in Machine-Learning-as-
a-Service Systems. In 12th USENIX Workshop on Hot Topics in Cloud Computing,
HotCloud 2020, July 13-14, 2020, Amar Phanishayee and Ryan Stutsman (Eds.).
USENIX Association.

[49] Lucas Zimmer, Marius Lindauer, and Frank Hutter. 2020. Auto-PyTorch Tab-
ular: Multi-Fidelity MetaLearning for Efficient and Robust AutoDL. CoRR
abs/2006.13799 (2020), 1–15.

