Policy Design for Active Sequential Hypothesis
Testing using Deep Learning

Dhruva Kartik, Ekraam Sabir, Urbashi Mitra and Prem Natarajan

8
1
0
2

t
c
O
1
1

]
T
I
.
s
c
[

1
v
9
5
8
4
0
.
0
1
8
1
:
v
i
X
r
a

Abstract—Information theory has been very successful in ob-
taining performance limits for various problems such as commu-
nication, compression and hypothesis testing. Likewise, stochastic
control theory provides a characterization of optimal policies
for Partially Observable Markov Decision Processes (POMDPs)
using dynamic programming. However, ﬁnding optimal policies
for these problems is computationally hard in general and thus,
heuristic solutions are employed in practice. Deep learning can
be used as a tool for designing better heuristics in such problems.
In this paper, the problem of active sequential hypothesis testing
is considered. The goal is to design a policy that can reliably
infer the true hypothesis using as few samples as possible by
adaptively selecting appropriate queries. This problem can be
modeled as a POMDP and bounds on its value function exist in
literature. However, optimal policies have not been identiﬁed and
various heuristics are used. In this paper, two new heuristics are
proposed: one based on deep reinforcement learning and another
based on a KL-divergence zero-sum game. These heuristics are
compared with state-of-the-art solutions and it is demonstrated
using numerical experiments that the proposed heuristics can
achieve signiﬁcantly better performance than existing methods
in some scenarios.

I. INTRODUCTION

Information theory provides us with a quantitative frame-
work [1], [2] to analyze various notions associated with infor-
mation processing such as communication and storage. Some
of the major advances in data storage and communication have
been facilitated by information theory. Information theory has
also been very successful in identifying performance limits
such as channel capacity and compression rate. It guarantees
the existence of policies that achieve optimal performance but
in many cases, ﬁnding these optimal policies (like capacity-
achieving encoding and decoding schemes) can be a difﬁcult
task. Many problems in statistics, such as hypothesis testing,
also have strong connections with information theory.

Stochastic control theory [3], [4] provides us with a frame-
work to analyze sequential decision-making problems under
uncertainty. Many real world decision-making problems can
be modeled as Markov Decision Processes (MDPs) or Partially
Observable Markov Decision Processes (POMDPs). It has
been widely used in the areas of artiﬁcial intelligence, robotics
and ﬁnance. Stochastic control theory provides us with strong
tools such as Dynamic Programming (DP) that can help us
characterize optimal solutions for these decision problems.
For instance, optimal solutions for MDPs with complete

D. Kartik, and U. Mitra are with the Department of Electrical Engineering,
University of Southern California, Los Angeles, CA 90089 (e-mail: mokha-
sun@usc.edu; ubli@usc.edu).

E. Sabir, and P. Natarajan are with the USC Information Sciences Institute,

Marina Del Rey, CA 90292 (e-mail: esabir@isi.edu; pnataraj@isi.edu).

model information can be computed efﬁciently using dynamic
programming [4]. This efﬁciency, however, does not extend
to POMDPs. It is known that ﬁnding optimal solutions for
POMDPs in general is a PSPACE-hard problem [5].

Because of the computational hardness of these problem,
various heuristic solutions are employed in practice. For exam-
ple, Point Based Value Iteration [6] is a well-known heuristic
for POMDPs. In this work, we examine the possibility of
using deep learning to design better heuristics for problems in
information and control theory. Deep learning is an emerging
branch of machine learning and has found tremendous success
in the areas of image and text processing [7]. Deep neural net-
works are universal approximators [8] and these networks can
be trained in a supervised manner using the backpropagation
algorithm [9]. Deep neural networks have lately been used to
solve problems in communication [10], [11] and reinforcement
learning [12].

In this work, we consider the problem of active sequential
hypothesis testing, which involves a combination of informa-
tion and control theory. The aim of active hypothesis testing
is to infer an unknown hypothesis based on observations.
The agent can adaptively make queries to obtain observations
and we seek to design a sequential query selection policy
that can reliably infer the underlying hypothesis using few
queries. We deﬁne a notion of conﬁdence and reformulate
this problem as a conﬁdence maximization problem in a ﬁxed
sample-size setting. The asymptotic version of the conﬁdence
maximization problem can be seen as an inﬁnite-horizon,
average-reward MDP.

We design heuristics for this conﬁdence maximization prob-
lem using deep neural networks. We ﬁrst examine a design
framework based on Recurrent Neural Networks (RNNs).
RNNs are a category of neural networks with a recurring
neural unit which maintains a hidden state vector for each
input instance. They have been used for solving sequential
problems with success [13], [14]. Their ability to store long-
term dependencies of sequences within hidden states makes
them apt for the task. One of the most popular and successful
variants of recurrent networks are Long-Short Term Memory
(LSTM) networks [15] which maintain their state using forget,
input and output gates. The underlying structure of recurrent
networks ﬁts naturally for active hypothesis testing. We ex-
plore an LSTM architecture that can be trained simultaneously
to adaptively select queries as well as learn to infer the true
hypothesis based on observations obtained. We observe that
the model manages to learn to infer the hypothesis for a given
set of observations but fails to learn the query selection policy.

 
 
 
 
 
 
We discuss the details of this architecture in Appendix A.

We then design a heuristic based on deep reinforcement
learning [12]. In this heuristic, the agent simulates the MDP
associated with the conﬁdence maximization problem. Based
on its simulated experience, the agent tries to learn the optimal
query selection policy using deep reinforcement
learning.
We observe in our numerical experiments that this heuristic
policy comes very close to optimality. The details of this
approach are discussed in Section IV. In addition to the
neural network based heuristics, we introduce a heuristic based
on a KL-divergence zero-sum game. This policy is adaptive
and also achieves near-optimal performance in our numerical
experiments. The details of the policy are discussed in Section
V-C.

The rest of the paper is organized as follows. In Section I-A,
we describe the prior works related to active hypothesis testing
and deep learning. In Section I-B, we discuss the mathematical
notation used in this paper. The conﬁdence maximization
problem is formulated in Section II and expressed as an
MDP in Section III. The deep reinforcement learning approach
for active hypothesis testing is discussed in Section IV. In
Section V we describe a few policies from prior works and
compare them with our designed policies based on numerical
experiments. We conclude the paper in Section VI.

A. Related Work

Active hypothesis testing was ﬁrst formulated by Chernoff
in [16] inspired by Wald’s Sequential Probability Ratio Test
(SPRT) [17]. Thereafter,
this work has been extended in
various ways. In [18], the problem of multihypothesis testing
is considered in both ﬁxed sample size and sequential settings.
In [19], a Bayesian setting is examined with a random stopping
time and is formulated as a POMDP. Upper and lower bounds
on the optimal value function of this POMDP were derived and
some heuristic policies were proposed. All these works provide
heuristic policies that are asymptotically optimal. However,
optimal policies for the non-asymptotic formulations are not
known. Furthermore, most of the heuristics proposed in these
works are almost open-loop and randomized policies. This
motivates us to seek better heuristics.

The idea of using deep neural networks to solve POMDPs
is relatively less explored. Reinforcement
learning usually
assumes perfect state observability. In [20], [21], the authors
learning under partial
aim to perform deep reinforcement
observability. They use a combination of convolutional neural
networks and recurrent neural networks to achieve this. In this
model, the agent does not have model information and thus,
cannot directly make Bayesian belief updates. The network
model in [22] is very similar to our deep Q-network. However,
the model
in [22] cannot be used directly for hypothesis
testing due to some issues discussed in Section IV-D. We make
appropriate modiﬁcations to rectify these issues. To the best
of our knowledge, deep neural networks have not been used in
the context of active hypothesis testing and our neural network
design framework is the ﬁrst of its kind.

ρ(1)

Nature

h = 1

h = 2

u2

u1

u2

y2

...

...

...

...

y1

y2

Agent

u1

Nature

y1

Agent

u1

u2

u1

u2

u1

u2

u1

u2

...

...

...

...

...

...

...

...

Fig. 1: Agent’s choices and subsequent observations repre-
sented as a tree. Every instance of the probability space can
be uniquely represented by a path in this tree.

B. Notation

Random variables/vectors are denoted by upper case bold-
face letters, their realization by the corresponding lower case
letter. We use calligraphic fonts to denote sets (e.g. U) and ∆U
is the probability simplex over a ﬁnite set U. In general, sub-
scripts are used as time indices. There is an exception (ρj(n))
to this convention where the subscript denotes the hypothesis
and n denotes time. For time indices n1 ≤ n2, Xn1:n2 is the
abbreviated notation for the variables (Xn1, Xn1+1, ..., Xn2).
For a strategy g, we use Pg[·] and Eg[·] to indicate that the
probability and expectation depend on the choice of g. The
Shannon entropy of a discrete distribution p over a space Y
is given by

H(p) = −

p(y) log p(y).

(1)

(cid:88)

y∈Y

The Kullback-Leibler divergence between distributions p and
q is given by

D(p||q) =

(cid:88)

y∈Y

p(y) log

p(y)
q(y)

.

(2)

II. PROBLEM FORMULATION
Let H ⊂ N be a ﬁnite set of hypotheses and let H be the
true hypothesis. At each time n ∈ N, the agent can perform
an experiment Un ∈ U and obtain an observation Yn ∈ Y.
The relation between Un and Yn is given by

Yn = ξ(H, Un, Wn),

(3)

where Wn is a collection of independent primitive random
variables. Thus, all the observations are independent condi-
tioned on the hypothesis and the experiment. The probability
of observing y after performing an experiment u under hypoth-
esis h is denoted by pu
h(y). For simplicity, let us also assume
that the sets U and Y are ﬁnite.

The information available at the agent at time n is

In = {U1:n−1, Y1:n−1}.

(4)

log p
1−p

10

5

0

−5

−10

0

0.2

0.4

0.6

0.8

1

p

Fig. 2: The logit function is the inverse of the logistic sigmoid
function 1/(1+e−x). It is widely used in statistics and machine
learning to quantify conﬁdence level [23].

Actions of the agent at time n can be functions of In (see Fig.
1). Let the experiment selection policy be

Un = gn(In).

(5)

The sequence of all the policies {gn} is denoted by g which
is referred to as a strategy. Let the collection of all such
strategies be G. Using the available information, the agent
forms a posterior belief ρ(n) on H at time n which is given
by

ρh(n) = P[H = h | Y1:n−1, U1:n−1].

(6)

Deﬁnition 1 (Bayesian Log-Likelihood Ratio): The Bayesian
log-likelihood ratio Ch(ρ) associated with an hypothesis h ∈
H is deﬁned as

Ch(ρ) := log

ρh
1 − ρh

.

(7)

The Bayesian log-likelihood ratio (BLLR) is the logarithm of
the ratio of the probability that hypothesis h is true versus
the probability that hypothesis h is not true. BLLR can be
interpreted as a conﬁdence level on hypothesis h being true in
logit form, which is also referred to as log-odds in statistics
[23]. The logit function is the inverse of the logistic sigmoid
function. Notice that the posterior belief ρh and BLLR are
related by the bijective increasing logit function (See Fig. 2).
The objective is to design an experiment selection strategy
g for the agent such that the conﬁdence level CH on the true
hypothesis H increases as quickly as possible. In other words,
the total reward after acquiring N observations is the average
rate of increase in the conﬁdence level on the true hypothesis
H and is given by

CH(ρ(N + 1)) − CH(ρ(1))
N
More explicitly, we seek to design a policy g that maximizes

(8)

.

the asymptotic expected reward R(g) which is deﬁned as

R(g) := lim
N→∞

inf

1
N

Eg [CH(ρ(N + 1)) − CH(ρ(1))] .

Since the initial conﬁdence CH(ρ(1)) is a constant, we can
ignore it for large values of N . Henceforth, we refer to this
problem as the Expected Conﬁdence Maximization (ECM)
problem.

Remark 1: Generally, the objective is to maximize the decay
rate of Bayesian error probability [18], or to use a stopping
time and optimize a linear combination of expected stopping
time and expected error probability [18], [19]. Our problem
formulation is mathematically different from these frameworks
but conceptually, all the formulations aim to capture the same
phenomenon which is to infer the true hypothesis quickly and
reliably. The precise mathematical relationship between these
formulations is yet to be understood and is an avenue for future
work.

To describe an upper bound on the optimal performance of
the conﬁdence maximization problem, we state the following
theorem without proof.

Theorem 1: For any query selection policy g and any

hypothesis h, we have

lim
N→∞

sup

1
N

where

Eg [CH(ρ(N + 1)) | H = h] ≤ R∗
h,

(9)

R∗

i := max
α∈∆U

min
j(cid:54)=i

(cid:88)

u

αuD(pu

i ||pu

j ).

(10)

Further, if the underlying hypothesis H = h, we have

lim
N→∞

sup

1
N

[Ch(ρ(N + 1))] ≤ R∗
h,

(11)

with probability 1.

The upper bound on the expected conﬁdence rate can
be obtained using dynamic programming for inﬁnite-horizon,
average reward MDPs and the same inequality in an almost
sure sense can be obtained with the help of Strong Law of
Large Numbers (SLLN).

III. MARKOV DECISION PROCESS FORMULATION

In this section, we show that the problem of maximizing
R(g) can be formulated as an inﬁnite-horizon, average-cost
MDP problem. The state of the MDP is the posterior belief
ρ(n). The agent’s observation and action spaces are the same
as in Section II. The posterior belief is updated using Bayes’
rule. Thus, if Un = u and Yn = y, we have

ρh(n + 1) =

ρh(n)pu
h(y)
h(cid:48) ρh(cid:48)(n)pu

h(cid:48)(y)

.

(cid:80)

(12)

For convenience, let us denote the Bayes’ update in (12) by

ρ(n + 1) = F (ρ(n), Un, Yn).

(13)

Thus, we have

P[ρ(n + 1) = F (ρ(n), u, y) | In, Un = u]

= P[Yn = y | In, Un = u] =

(cid:88)

h∈H

ρh(n)pu

h(y).

Clearly, the dynamics of this system are controlled Markovian.
The expectation of the average conﬁdence rate under a strategy
g is given by

RN (g)

.
=

=

1
N
1
N

Eg [CH(ρ(N + 1)) − CH(ρ(1))]

(14)

N
(cid:88)

Eg

[CH(ρ(n + 1)) − CH(ρ(n))]

(15)

n=1
N
(cid:88)

Eg

n=1

r(ρ(n), Un, Yn).

(16)

=:

1
N

Thus, the instantaneous reward for this MDP is r(ρ, u, y),
i.e. if the state is ρ, the experiment performed is u and the
observation is y, then the instantaneous reward is given by

Fig. 3: The agent performs a query u at some state ρ. The
environments simulates the belief update using u and ρ to
generate the update belief ρ(cid:48) and its associated reward r.

the agent updates its target policy g. This iterative simulated
learning process, schematically illustrated in Figure 3,
is
repeated until a convergence criterion is met. We elucidate this
methodology in greater detail in the following sub-sections.

r(ρ, u, y) = C(F (ρ, u, y)) − C(ρ),

(17)

A. Discounted Reward Formulation

where for any belief state ρ

C(ρ) =

(cid:88)

i∈H

ρi log

ρi
1 − ρi

=

(cid:88)

i∈H

ρiCi(ρ).

(18)

We refer to the function C(ρ) as Average Bayesian Log-
Likelihood Ratio (ABLLR). Note that this is almost identical
to the notion of average log-likelihood ratio U (ρ) = −C(ρ)
in [24], which was used to design a greedy heuristic for the
active hypothesis testing problem.

The objective in this MDP problem is to ﬁnd a strategy g∗

that maximizes the following average expected reward

R(g) = lim
N→∞

inf

1
N

N
(cid:88)

n=1

Eg(r(ρ(n), Un, Yn)).

(19)

IV. DEEP Q-LEARNING FOR HYPOTHESIS TESTING

In this section, we describe our deep learning approach for
policy design for active sequential hypothesis testing. We use
a variant of the Deep Q-Network (DQN) introduced in [12],
which is a learning agent that combines reinforcement learning
with deep neural networks. It is an adaptation of a popular off-
policy Temporal Difference (TD) learning algorithm, known
as Q-learning [25].

We create an artiﬁcial environment

that simulates the
Bayesian belief update (12) over multiple episodes. The du-
ration (N ) of each episode is ﬁxed. At the beginning of each
episode, the underlying hypothesis H is randomly selected
with probability ρ(1) and it remains ﬁxed over the episode’s
duration. At any given time, the agent interacts with this
environment by making a query (u) based on the current state
(ρ) using an appropriate exploration strategy (e.g. (cid:15)-greedy
exploration [25]). The environment then reveals the next state
(ρ(cid:48)) and its associated reward (r) to the agent. We refer to
(ρ, u, ρ(cid:48), r) as an experience tuple. Using this information,

The Q-learning algorithm is designed for a discounted
reward MDP formulation. Therefore, we ﬁrst convert our
average reward formulation in Section III to a discounted
reward formulation with a discount factor γ < 1. For an
experiment selection policy g, let

Rd(g) := Eg

(cid:34) ∞
(cid:88)

(cid:35)
γn−1r(ρ(n), Un, Yn)

,

(20)

n=1

be the total discounted reward. Since r(ρ, u, y) is uniformly
bounded, the discounted reward Rd(g) is also bounded and
well-deﬁned for any policy g. Our objective now is to ﬁnd a
strategy g∗ that maximizes Rd(g).

Remark 2: When the state and action spaces of an MDP are
ﬁnite, it is well-known [4] that the discounted reward and the
average reward formulations are equivalent for a sufﬁciently
large discount factor γ. However, the state space herein is
uncountably inﬁnite and this equivalence may not necessarily
hold. Nonetheless, we observe in our numerical experiments
that the solution to the discounted reward formulation is near-
optimal with respect to the average reward formulation.

B. Action-value Function

The action-value function [25] for a policy g is deﬁned as

qg(ρ, u) := Eg

(cid:34) ∞
(cid:88)

(cid:35)
γkrk+1 | ρ(1) = ρ, U1 = u

,

(21)

k=0

where rn = r(ρ(n), Un, Yn). Let g∗ be an optimal policy
with respect to the discounted reward formulation and let
q∗(ρ, u) be its corresponding action-value function. Then the
optimal action-value function satisﬁes the ﬁxed point equation,
also known as Bellman optimality equation [25],

q∗(ρ, u) = E[r(ρ, u, Y) + max
u(cid:48)

q∗(F (ρ, u, Y), u(cid:48))],

(22)

Error (MSE) loss is minimized

L(θ) = (Q(cid:48)(ρ, u) − Qθ(ρ, u))2.

(24)

This naive update rule can make the network unstable because
it closely ﬁts the network to the updated Q-value Q(cid:48)(ρ, u) for
the current state-action pair (ρ, u) but the Q-values associated
with other state-action pairs may be disturbed. To ensure
this does not happen, a method known as experience replay
[12] is employed. At each time, the agent stores experience
tuples (ρ, u, ρ(cid:48), r) in its memory D. Whenever the weights
are updated, a random mini-batch B of experience tuples is
sampled from the memory and the MSE loss is minimized
using gradient descent over this mini-batch with the following
loss function

L(θ) =

(cid:88)

(ρ,u,ρ(cid:48),r)∈B

(Q(cid:48)(ρ, u) − Qθ(ρ, u))2.

(25)

D. Additional Challenges

Generally, it is necessary to explore all the states to learn
the state transition and reward structure. However, since the
state space is uncountably inﬁnite, we cannot possibly explore
all the states. Therefore, we choose a large value for (cid:15) (≈ 0.8)
so that the state space is sufﬁciently explored. We observe in
our numerical experiments that training over a large number of
episodes results in an efﬁcient query selection policy despite
this exploration issue.

Another challenge is that as the belief on the true hypothesis
gets close to 1, the belief on all the alternate hypotheses
becomes very small. Improvement in the conﬁdence level, i.e.
the instantaneous reward r is very sensitive to the belief on
alternate hypotheses. Thus, the DQN fails to select optimal
queries when the belief on alternate hypotheses is too small.
To counter this, we normalize the belief on the alternate set of
hypotheses and augment it to the belief vector. The normalized
alternate belief is denoted by ˜ρ and is given by

˜ρj =

ρj
1 − ρi

,

(26)

where i is the most likely hypothesis with respect to the belief
ρ and j (cid:54)= i.

The overall Deep Q-learning algorithm for active sequential
hypothesis testing is described in Algorithm 1. The agent
and the environment operate in an interleaved manner. Their
combined behavior is captured by Algorithm 1. The comment
on each instruction speciﬁes whether the instruction is meant
for the agent (A) or the artiﬁcial environment (E). The Q-value
update is denoted by QUPθ and is given by

QUPθ(ρ, u, ρ(cid:48), r)

= Qθ(ρ, u) + ζ[r + γ max

u(cid:48)

Qθ(ρ(cid:48), u(cid:48)) − Qθ(ρ, u)].

Note that ζ is a small constant less than 1. Note that the letter α
is generally used in place of ζ. We select ζ to avoid notational
conﬂict with distribution α∗

i which will be introduced later.

Fig. 4: The neural network takes the belief vector ρ as the
input and outputs the Q-values for each action u. The hidden
layers are fully connected with non-linear activation. Only the
ﬁnal layer has linear activation.

for every belief state ρ and query u. Note that the source
of randomness in the ﬁxed point equation is the variable
Y and the expectation is with respect
to the distribution
(cid:80)
h(y). Further, if a policy g is such that, for every

h∈H ρhpu
belief state ρ,

g(ρ) = arg max

u

q∗(ρ, u),

(23)

then g is an optimal policy with respect to the discounted
reward formulation. Thus, ﬁnding an optimal policy g∗ can
be reduced to ﬁnding the optimal action-value function q∗.
The optimal action-value function can be obtained using the
Q-learning algorithm in [25] when the state and action spaces
are ﬁnite. However, the state space is inﬁnite in our case and
thus, we need a different approach to ﬁnd the optimal action-
value function.

C. Action-value Function as a Deep Neural Network

The ﬁrst challenge in performing Q-learning with an inﬁnite
state space is to ﬁnd an appropriate representation for the
action-value function. Notice that the posterior belief ρ is a
ﬁnite-dimensional vector and the action space is ﬁnite. We
can thus represent the action-value function as a deep neural
network which takes posterior belief ρ as an input and outputs
the action-value vector of dimension |U| as illustrated in Figure
4. The neural network is parameterized by a ﬁnite collection
of weights θ and henceforth, we refer to the output of this
neural network as Qθ(ρ, u).

The second challenge lies in making the Q-learning updates.
We would ideally like to make an update of the following form

Q(cid:48)(ρ, u) ← Q(ρ, u) + ζ[r + γ max
u(cid:48)

Q(ρ(cid:48), u(cid:48)) − Q(ρ, u)],

where (ρ, u, ρ(cid:48), r) is an experience tuple. Notice, however,
that the action-value function is characterized by a collection
of weights (θ) and thus, one has to update these weights so
that the neural network outputs the corresponding updated Q-
values Q(cid:48)(ρ, u). To achieve this, we can modify the weights θ
using gradient descent such that the following Mean-Squared

Algorithm 1 Deep Q-learning algorithm for active sequential
hypothesis testing
1: Initialize memory D to capacity K
2: Initialize DQN with random weights θ
3: for episode = 1, EpiNum do
4:

(cid:46) A
(cid:46) A

Randomly select H with prob. ρ(1)
Initialize state ρ = ρ(1)
for n = 1, N do

5:
6:
7:
8:
9:
10:

11:
12:
13:
14:
15:
16:

17:
18:
19:
20:
21:
22:

With probility (cid:15), select random query u
Otherwise, select u = arg maxu Qθ(ρ, u)
Perform query u
Generate Y = ξ(H, u, W)
Update belief ρ(cid:48) = F (ρ, u, Y)
Compute reward r = C(ρ(cid:48)) − C(ρ)
Reveal ρ(cid:48) and r to A
Store (ρ, u, ρ(cid:48), r) in D
Assign ρ ← ρ(cid:48)
Sample random minibatch B from D
Duplicate DQN θ(cid:48) ← θ
for epoch = 1, EpochNum do

for each ( ˆρ, ˆu, ˆρ(cid:48), ˆr) in B do

Q(cid:48)( ˆρ, ˆu) ← QUPθ( ˆρ, ˆu, ˆρ(cid:48), ˆr)
Perform gradient descent step on
(Q(cid:48)( ˆρ, ˆu) − Qθ(cid:48)( ˆρ, ˆu))2

(cid:46) E
(cid:46) A

(cid:46) A
(cid:46) A
(cid:46) A
(cid:46) E
(cid:46) E
(cid:46) E
(cid:46) E
(cid:46) A
(cid:46) A
(cid:46) A
(cid:46) A

(cid:46) A
(cid:46) A

(cid:46) A

end for

end for
Assign θ ← θ(cid:48)

23:
24:
25:
26:
27: end for
28: return DQN θ

end for

V. NUMERICAL EXPERIMENTS

In this section, we numerically compare our DQN model
with other popular heuristics used for active hypothesis testing.
We also propose a new heuristic based on a Kullback-Leibler
divergence zero-sum game and demonstrate numerically that
this heuristic’s performance is close to the maximum achiev-
able conﬁdence rate. We ﬁrst brieﬂy describe all the heuristics
we use in our experiments.

A. Extrinsic Jensen-Shannon (EJS) Divergence

Extrinsic Jensen-Shannon divergence as a notion of infor-
mation was ﬁrst introduced in [26]. Using our notation, EJS
for a query u at some belief state ρ is simply the expected
instantaneous reward, i.e.

EJS(ρ, u) = E[C(F (ρ, u, Y)) − C(ρ)].

(27)

Notice that the only random variable in the expression above
is Y and the expectation is with respect to the distribution
(cid:80)
h(y) on Y. The EJS heuristic selects the experiment

h∈H ρhpu

u that maximizes EJS(ρ, u) for a given state ρ.

B. Open Loop Veriﬁcation (OPE)

Open loop veriﬁcation policy is the most widely used
policy in prior literature [19], [18]. In this heuristic, the agent

ﬁrst explores for a while using an appropriate exploration
strategy. Whenever the conﬁdence on some hypothesis i is
large enough, i.e. ρi > ¯ρ, the queries are randomly selected
in an open-loop manner from the distribution α∗
i which is
deﬁned as

α∗

i := arg max
α∈∆U

min
j(cid:54)=i

(cid:88)

u

αuD(pu

i ||pu

j ),

(28)

where ∆U is the set of all distributions over the set of
experiments U. We refer to this phase as the veriﬁcation phase.
In our implementation, we use EJS for the exploration phase
and set the threshold ¯ρ = 0.7.

C. KL-divergence Zero-sum Game (HEU)

This heuristic is similar to OPE but the query selection
policy in the veriﬁcation phase is adaptive. For each hypothesis
i, we can formulate a zero-sum game [27] in which the ﬁrst
player (maximizing) selects an experiment u ∈ U and the
second player (minimizing) selects an alternate hypothesis
j ∈ ˜Hi := H \ {i}. The payoff for this zero-sum game is
the KL-divergence D(pu
j ). Whenever ρi > ¯ρ, the agent
picks an experiment u that maximizes

i ||pu

Pi(ρ, u) :=

(cid:88)

j(cid:54)=i

˜ρjD(pu

i ||pu

j ),

where ˜ρj = ρj/1 − ρi. This strategy can be interpreted as the
ﬁrst player’s best-response when the second player uses the
mixed strategy ˜ρj to select an alternate hypothesis. Note that
the mixed strategy α∗
i used in OPE is an equilibrium strategy
for the maximizing player.

D. Simulation Setup

To simulate these heuristics, we ﬁrst consider a simple
setup with three hypotheses and two queries. The conditional
distributions pu
i (y) for each of these queries are illustrated in
Figure 5.

y = 0
0.8
0.2
0.8

y = 1
0.2
0.8
0.2

h0
h1
h2

y = 0
0.8
0.8
0.2

y = 1
0.2
0.2
0.8

h0
h1
h2

(a) Query u1

(b) Query u2

Fig. 5: Conditional distributions pu

i (y) for each query

The queries are designed such that when H = h0, the agent
is forced to make both queries u1 and u2. This is because
hypotheses h0 and h2 are indistinguishable under query u1 and
similarly, hypotheses h0 and h1 are indistinguishable under
query u2. However, when H = h1, the agent can eliminate
h0 and h2 simultaneously using query u1 alone and similarly,
when H = h2, the agent only needs to perform the query u2.
We observe that all the four heuristics manage to learn
this scheme of query selection. However, the rate at which
conﬁdence is maximized is different for each heuristic. We
illustrate the evolution of expected conﬁdence rate RN under

Fig. 6: Evolution of expected conﬁdence rate RN under
hypothesis h0 in the ﬁrst setup with queries u1 and u2. Note
the subpar performance of OPE in this setup.

Fig. 8: Evolution of expected conﬁdence rate RN under
hypothesis h0 in the second setup with additional queries u3
and u4. Note the subpar performance of OPE and EJS in this
setup.

hypothesis h0 in Figure 6. The heuristics DQN, EJS and
HEU come very close to the maximum achievable rate. OPE
eventually achieves maximal rate but very slowly.

In the second experimental setup, we include two additional
queries u3 and u4 characterized by the distributions in Figure
7. When H = h0 the queries u3 and u4 together can eliminate
at a much faster rate than u1 and u2. Intuitively,
this is
because when the agent performs u3 and observes y = 1, the
belief on h1 decreases drastically because y = 1 is extremely
unlikely under hypothesis h1. Similarly, u4 is very effective
in eliminating h2.

y = 0
0.8
1 − δ
0.8

y = 1
0.2
δ
0.2

h0
h1
h2

h0
h1
h2

y = 1
0.2
0.2
δ

y = 0
0.8
0.8
1 − δ
(b) Query u4

(a) Query u3
Fig. 7: Conditional distributions pu
δ = 0.0000001.

i (y) for each query. Here,

The evolution of expected conﬁdence rate under hypothesis
h0 with additional experiments u3 and u4 is shown in Figure
8. The heuristics DQN, HEU and OPE select queries u3 and
u4 under hypothesis h0. But the greedy heuristic EJS usually
selects only u1 and u2 and fails to realize that queries u3
and u4 are more effective under hypothesis h0. The greedy
EJS approach fails because queries u3 and u4 are constructed
in such way that they are optimal over longer horizons but
are sub-optimal over shorter horizons. Thus the assumption
required for asymptotic optimality of EJS in [26] does not
hold in this setup. This also demonstrates that DQN is not

simply selecting its queries greedily and manages to learn the
long-term consequences of selecting queries.

VI. CONCLUSION

In this paper, we considered the problem of active sequential
hypothesis testing. We deﬁned a notion of conﬁdence, called
Bayesian log-likelihood ratio and reformulated the hypothesis
testing problem as a conﬁdence maximization problem which
can be seen as an inﬁnite-horizon, average-reward MDP over a
ﬁnite-dimensional belief space. We proposed a deep reinforce-
ment learning based policy design framework for this MDP.
We also proposed a heuristic based on a KL-divergence zero-
sum game. Using numerical experiments, we compared these
heuristics with those in prior works and demonstrated that our
designed heuristics perform signiﬁcantly better than existing
methods in some scenarios.

ACKNOWLEDGMENT

This research was supported, in part, by National Science
Foundation under Grant NSF CNS-1213128, CCF-1410009,
CPS-1446901, Grant ONR N00014-15-1-2550, and Grant
AFOSR FA9550-12-1-0215. We also thank Ashutosh Nayyar
for his contribution in formulating and solving the MDP.

REFERENCES

[1] Claude Elwood Shannon, “A mathematical theory of communication,”
ACM SIGMOBILE mobile computing and communications review, vol.
5, no. 1, pp. 3–55, 2001.

[2] Thomas M Cover and Joy A Thomas, Elements of information theory,

John Wiley & Sons, 2012.

[3] Panganamala Ramana Kumar and Pravin Varaiya, Stochastic systems:
Estimation, identiﬁcation, and adaptive control, vol. 75, SIAM, 2015.
[4] Dimitri P Bertsekas, Dynamic programming and optimal control, vol. 1,

Athena scientiﬁc Belmont, MA, 2005.

01020304050TimeHorizonN0.280.300.320.340.360.380.400.42ExpectedConﬁdenceRateRNDQNEJSHEUOPEOptimalRate020406080100TimeHorizonN0.20.40.60.81.01.21.4ExpectedConﬁdenceRateRNDQNHEUOPEEJSOptimalRate[5] Christos H Papadimitriou and John N Tsitsiklis, “The complexity of
markov decision processes,” Mathematics of operations research, vol.
12, no. 3, pp. 441–450, 1987.

[6] Joelle Pineau, Geoff Gordon, Sebastian Thrun, et al., “Point-based value
iteration: An anytime algorithm for pomdps,” in IJCAI, 2003, vol. 3,
pp. 1025–1032.

[7] Yann LeCun, Yoshua Bengio, and Geoffrey Hinton, “Deep learning,”

nature, vol. 521, no. 7553, pp. 436, 2015.

[8] Kurt Hornik, Maxwell Stinchcombe, and Halbert White, “Multilayer
feedforward networks are universal approximators,” Neural networks,
vol. 2, no. 5, pp. 359–366, 1989.

[9] Martin T Hagan and Mohammad B Menhaj,

“Training feedforward
networks with the marquardt algorithm,” IEEE transactions on Neural
Networks, vol. 5, no. 6, pp. 989–993, 1994.
[10] Nariman Farsad and Andrea Goldsmith,

“Detection algorithms
arXiv preprint

for communication systems using deep learning,”
arXiv:1705.08044, 2017.

[11] Nariman Farsad, Milind Rao, and Andrea Goldsmith, “Deep learning for
joint source-channel coding of text,” arXiv preprint arXiv:1802.06832,
2018.

[12] Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei A Rusu,
Joel Veness, Marc G Bellemare, Alex Graves, Martin Riedmiller, An-
dreas K Fidjeland, Georg Ostrovski, et al., “Human-level control through
deep reinforcement learning,” Nature, vol. 518, no. 7540, pp. 529, 2015.
[13] Ilya Sutskever, Oriol Vinyals, and Quoc V Le, “Sequence to sequence
in Advances in neural information

learning with neural networks,”
processing systems, 2014, pp. 3104–3112.

[14] Kyunghyun Cho, Bart Van Merri¨enboer, Caglar Gulcehre, Dzmitry Bah-
danau, Fethi Bougares, Holger Schwenk, and Yoshua Bengio, “Learning
phrase representations using rnn encoder-decoder for statistical machine
translation,” arXiv preprint arXiv:1406.1078, 2014.

[15] Sepp Hochreiter and J¨urgen Schmidhuber, “Long short-term memory,”

Neural computation, vol. 9, no. 8, pp. 1735–1780, 1997.

[16] Herman Chernoff, “Sequential design of experiments,” The Annals of

Mathematical Statistics, vol. 30, no. 3, pp. 755–770, 1959.
[17] Abraham Wald, Sequential analysis, Courier Corporation, 1973.
[18] Sirin Nitinawarat, George K Atia, and Venugopal V Veeravalli, “Con-
IEEE Transactions on

trolled sensing for multihypothesis testing,”
Automatic Control, vol. 58, no. 10, pp. 2451–2464, 2013.

[19] Mohammad Naghshvar, Tara Javidi, et al., “Active sequential hypothesis
testing,” The Annals of Statistics, vol. 41, no. 6, pp. 2703–2738, 2013.
[20] Matthew Hausknecht and Peter Stone, “Deep recurrent q-learning for
partially observable mdps,” CoRR, abs/1507.06527, vol. 7, no. 1, 2015.
“Qmdp-net: Deep
learning for planning under partial observability,” in Advances in Neural
Information Processing Systems, 2017, pp. 4694–4704.

[21] Peter Karkus, David Hsu, and Wee Sun Lee,

[22] Maxim Egorov, “Deep reinforcement learning with pomdps,” 2015.
[23] David W Hosmer Jr, Stanley Lemeshow, and Rodney X Sturdivant,
Applied logistic regression, vol. 398, John Wiley & Sons, 2013.
[24] Mohammad Naghshvar, Active learning and hypothesis testing, Ph.D.

thesis, UC San Diego, 2013.

[25] Richard S Sutton, Andrew G Barto, Francis Bach, et al., Reinforcement

learning: An introduction, MIT press, 1998.

[26] Mohammad Naghshvar and Tara Javidi,

“Extrinsic jensen-shannon
divergence with application in active hypothesis testing,” in Information
Theory Proceedings (ISIT), 2012 IEEE International Symposium on.
IEEE, 2012, pp. 2191–2195.

[27] Martin J Osborne and Ariel Rubinstein, A course in game theory, MIT

press, 1994.

[28] Ekraam Sabir, Stephen Rawls, and Prem Natarajan, “Implicit language
model in lstm for ocr,” in Document Analysis and Recognition (ICDAR),
2017 14th IAPR International Conference on. IEEE, 2017, vol. 7, pp.
27–31.

APPENDIX A
RECURRENT NEURAL NETWORK ARCHITECTURE

The ﬁrst goal is to verify if the internal state of an LSTM
can maintain hypothesis information. The model is a simpler
version of the recurrent network shown in Figure 10, which
takes a sequence of random queries and its results as input.
This model is compared against Maximum A Posteriori (MAP)

Fig. 9: This plot compares the performance of the RNN
network vs the MAP rule.

Fig. 10: The LSTM network with query selection.

rule for hypothesis classiﬁcation which is optimal for any
input, in Figure 9. The performance of LSTM comes close
to that of MAP, which clearly shows that its hidden state
maintains hypothesis information.

We examine if LSTMs can learn query selection as well.
Our model architecture in Figure 10 predicts a query at each
time-step which in turn is used to produce an input for the next
time-step. True hypothesis is provided for training the model,
but optimal query selection for any time-step is unknown.
The model is expected to learn this implicitly. There are two
practical issues with this architecture. Query result is produced
by a non-differentiable black-box making the output and input
of consecutive time-steps disconnected. This prevents explicit
learning of query selection. However, it is known that recurrent
networks can learn implicit tasks [28]. Second, query selection
is a soft decision made by the model, whereas a discrete
decision is preferred. Experiments show that the model fails
to learn query selection.

An improvement to this architecture can be made if hard
decisions can be incorporated in a model. A discrete decision
from the model also solves the problem of explicit query
selection, since the output and input at consecutive time-
steps can be connected. This direction of research leads to
reinforcement learning, which requires further investigation.

