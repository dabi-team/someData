A Theory of Regularized Markov Decision Processes

Matthieu Geist 1 Bruno Scherrer 2 Olivier Pietquin 1

9
1
0
2

n
u
J

4

]

G
L
.
s
c
[

2
v
5
7
2
1
1
.
1
0
9
1
:
v
i
X
r
a

Abstract
Many recent successful (deep) reinforcement
learning algorithms make use of regularization,
generally based on entropy or Kullback-Leibler di-
vergence. We propose a general theory of regular-
ized Markov Decision Processes that generalizes
these approaches in two directions: we consider
a larger class of regularizers, and we consider
the general modiﬁed policy iteration approach,
encompassing both policy iteration and value it-
eration. The core building blocks of this theory
are a notion of regularized Bellman operator and
the Legendre-Fenchel transform, a classical tool
of convex optimization. This approach allows for
error propagation analyses of general algorithmic
schemes of which (possibly variants of) classical
algorithms such as Trust Region Policy Optimiza-
tion, Soft Q-learning, Stochastic Actor Critic or
Dynamic Policy Programming are special cases.
This also draws connections to proximal convex
optimization, especially to Mirror Descent.

1. Introduction

Many reinforcement learning algorithms make use of some
kind of entropy regularization, with various motivations,
such as improved exploration and robustness. Trust Region
Policy Optimization (TRPO) (Schulman et al., 2015) is a pol-
icy iteration scheme where the greedy step is penalized with
a Kullback-Leibler (KL) penalty between two consecutive
policies. Dynamic Policy Programming (DPP) (Azar et al.,
2012) is a reparametrization of a value iteration scheme
regularized by a KL penalty between consecutive policies.
Soft Q-learning, eg. (Fox et al., 2016; Schulman et al., 2017;
Haarnoja et al., 2017), uses a Shannon entropy regulariza-
tion in a value iteration scheme, while Soft Actor Critic
(SAC) (Haarnoja et al., 2018a) uses it in a policy iteration
scheme. Value iteration has also been combined with a

1Google Research, Brain Team. 2Universit´e de Lorraine, CNRS,
Inria, IECL, F-54000 Nancy, France. Correspondence to: Matthieu
Geist <mfgeist@google.com>.

Proceedings of the 36 th International Conference on Machine
Learning, Long Beach, California, PMLR 97, 2019. Copyright
2019 by the author(s).

Tsallis entropy (Lee et al., 2018), with the motivation of
having a sparse regularized greedy policy. Other approaches
are based on a notion of temporal consistency equation,
somehow extending the notion of Bellman residual to the
regularized case (Nachum et al., 2017; Dai et al., 2018;
Nachum et al., 2018), or on policy gradient (Williams, 1992;
Mnih et al., 2016).

This non-exhaustive set of algorithms share the idea of us-
ing regularization, but they are derived from sometimes
different principles, consider each time a speciﬁc regular-
ization, and have ad-hoc analysis, if any. Here, we propose
a general theory of regularized Markov Decision Processes
(MDPs). To do so, a key observation is that (approximate)
dynamic programming, or (A)DP, can be derived solely
from the core deﬁnition of the Bellman evaluation opera-
tor. The framework we propose is built upon a regularized
Bellman operator, and on an associated Legendre-Fenchel
transform. We study the theoretical properties of these regu-
larized MDPs and of the related regularized ADP schemes.
This generalizes many existing theoretical results and pro-
vides new ones. Notably, it allows for an error propagation
analysis for many of the aforementioned algorithms. This
framework also draws connections to convex optimization,
especially to Mirror Descent (MD).

A uniﬁed view of entropy-regularized MDPs has already
been proposed by Neu et al. (2017). They focus on reg-
ularized DP through linear programming for the average
reward case. Our contribution is complementary to this
work (different MDP setting, we do not regularize the same
quantity, we do not consider the same DP approach). Our
use of the Legendre-Fenchel transform is inspired by Men-
sch & Blondel (2018), who consider smoothed ﬁnite hori-
zon DP in directed acyclic graphs. Our contribution is also
complementary to this work, that does not allow recover-
ing aforementioned algorithms nor analyzing them. After
a brief background, we introduce regularized MDPs and
various related algorithmic schemes based on approximate
modiﬁed policy iteration (Scherrer et al., 2015), as well as
their analysis. All proofs are provided in the appendix.

2. Background

In this section, we provide the necessary background for
building the proposed regularized MDPs. We write ∆X the

 
 
 
 
 
 
A Theory of Regularized MDPs

set of probability distributions over a ﬁnite set X and Y X
the set of applications from X to the set Y . All vectors are
column vectors, except distributions, for left multiplication.
We write (cid:104)·, ·(cid:105) the dot product and (cid:107) · (cid:107)p the (cid:96)p-norm.

We’ll make use of the following properties (Hiriart-Urruty
& Lemar´echal, 2012; Mensch & Blondel, 2018).

Proposition 1. Let Ω be strongly convex, we have the fol-
lowing properties.

2.1. Unregularized MDPs

An MDP is a tuple {S, A, P, r, γ} with S the ﬁnite1 state
space, A the ﬁnite action space, P ∈ ∆S×A
the Markovian
transition kernel (P (s(cid:48)|s, a) denotes the probability of tran-
siting to s(cid:48) when action a is applied in state s), r ∈ RS×A
the reward function and γ ∈ (0, 1) the discount factor.

S

A policy π ∈ ∆S
A associates to each state a distribution over
actions. The associated Bellman operator is deﬁned as, for
any function v ∈ RS ,
∀s ∈ S, [Tπv](s) = Ea∼π(.|s)

(cid:2)r(s, a) + γEs(cid:48)|s,a[v(s(cid:48))](cid:3) .

This operator is a γ-contraction in supremum norm and its
unique ﬁxed-point is the value function vπ. With rπ(s) =
Ea∼π(.|s)[r(s, a)] and Pπ(s(cid:48)|s) = Ea∼π(.|s)[P (s(cid:48)|s, a)]),
the operator can be written as Tπv = rπ + γPπv. For
any function v ∈ RS , we associate the function q ∈ RS×A,
q(s, a) = r(s, a) + γEs(cid:48)|s,a[v(s(cid:48))].

Thus,
the Bellman operator can also be written as
[Tπv](s) = (cid:104)π(·|s), q(s, ·)(cid:105) = (cid:104)πs, qs(cid:105). With a slight abuse
of notation, we will write Tπv = (cid:104)π, q(cid:105) = ((cid:104)πs, qs(cid:105))s∈S .

From this evaluation operator, one can deﬁne the Bellman
optimality operator as, for any v ∈ RS ,

T∗v = max

π

Tπv.

This operator is also a γ-contraction in supremum norm,
and its ﬁxed point is the optimal value function v∗. From
the same operator, one can also deﬁne the notion of a policy
being greedy respectively to a function v ∈ RS :

π(cid:48) ∈ G(v) ⇔ T∗v = Tπ(cid:48)v ⇔ π(cid:48) ∈ argmax

π

Tπv.

Given this, we could derive value iteration, policy iteration,
modiﬁed policy iteration, and so on. Basically, we can
do all these things from the core deﬁnition of the Bellman
evaluation operator. We’ll do so from a notion of regularized
Bellman evaluation operator.

2.2. Legendre-Fenchel transform

Let Ω : ∆A → R be a strongly convex function. The
Legendre-Fenchel transform (or convex conjugate) of Ω is
Ω∗ : RA → R, deﬁned as

∀qs ∈ RA, Ω∗(qs) = max
πs∈∆A

(cid:104)πs, qs(cid:105) − Ω(πs).

1We assume a ﬁnite space for simplicity of exposition, our

results extend to more general cases.

i Unique maximizing argument: ∇Ω∗ is Lipschitz and
satisﬁes ∇Ω∗(qs) = argmaxπs∈∆A (cid:104)πs, qs(cid:105) − Ω(πs).

ii Boundedness: if there are constants LΩ and UΩ such
that for all πs ∈ ∆A, we have LΩ ≤ Ω(πs) ≤ UΩ, then
maxa∈A qs(a) − UΩ ≤ Ω∗(qs) ≤ maxa∈A qs(a) − LΩ.

iii Distributivity: for any c ∈ R (and 1 the vector of ones),

we have Ω∗(qs + c1) = Ω∗(qs) + c.

iv Monotonicity: qs,1 ≤ qs,2 ⇒ Ω∗(qs,1) ≤ Ω∗(qs,2).

(cid:80)

A classical example is the negative entropy Ω(πs) =
(cid:80)
a πs(a) ln πs(a). Its convex conjugate is the smoothed
maximum Ω∗(qs) = ln (cid:80)
a exp qs(a) and the unique
maximizing argument is the usual softmax ∇Ω∗(qs) =
exp qs(a)
b exp qs(b) . For a positive regularizer, one can consider
Ω(πs) = (cid:80)
a πs(a) ln πs(a) + ln |A|, that is the KL di-
vergence between πs and a uniform distribution. Its con-
vex conjugate is Ω∗(qs) = ln (cid:80)
1
|A| exp qs(a), that is the
Mellowmax operator (Asadi & Littman, 2017). The max-
imizing argument is still the softmax. Another less usual
example is the negative Tsallis entropy (Lee et al., 2018),
Ω(πs) = 1
2 − 1). The analytic convex conjugate is
more involved, but it leads to the sparsemax as the maximiz-
ing argument (Martins & Astudillo, 2016).

2 ((cid:107)πs(cid:107)2

a

3. Regularized MDPs

The core idea of our contribution is to regularize the Bellman
evaluation operator. Recall that [Tπv](s) = (cid:104)πs, qs(cid:105). A
natural idea is to replace it by [Tπ,Ωv](s) = (cid:104)πs, qs(cid:105) −
Ω(πs). To get the related optimality operator, one has to
perform state-wise maximization over πs ∈ ∆A, which
gives the Legendre-Fenchel transform of [Tπ,Ωv](s). This
deﬁnes a smoothed maximum (Nesterov, 2005). The related
maximizing argument deﬁnes the notion of greedy policy.

3.1. Regularized Bellman operators

We now deﬁne formally these regularized Bellman oper-
ators. With a slight abuse of notation, we write Ω(π) =
(Ω(πs))s∈S (and similarly for Ω∗ and ∇Ω∗).
Deﬁnition 1 (Regularized Bellman operators). Let Ω :
∆A → R be a strongly convex function. For any v ∈ RS
deﬁne q ∈ RS×A as q(s, a) = r(s, a) + γEs(cid:48)|s,a[v(s(cid:48))].
The regularized Bellman evaluation operator is deﬁned as

Tπ,Ω : v ∈ RS → Tπ,Ωv = Tπv − Ω(π) ∈ RS ,

A Theory of Regularized MDPs

that is, state-wise, [Tπ,Ωv](s) = (cid:104)πs, qs(cid:105) − Ω(πs). The
regularized Bellman optimality operator is deﬁned as

The function v∗,Ω is indeed the optimal value function,
thanks to the following result.

T∗,Ω : v ∈ RS → T∗,Ωv = max
π∈∆S
A

Tπ,Ωv = Ω∗(q) ∈ RS ,

that is, state-wise, [T∗,Ωv](s) = Ω∗(qs). For any function
v ∈ RS , the associated unique greedy policy is deﬁned as

π(cid:48) = GΩ(v) = ∇Ω∗(q) ⇔ Tπ(cid:48),Ωv = T∗,Ωv,

that is, state-wise, π(cid:48)

s = ∇Ω∗(qs).

To be really useful, these operators should satisfy the same
properties as the classical ones. It is indeed the case (we
recall that all proofs are provided in the appendix).
Proposition 2. The operator Tπ,Ω is afﬁne and we have the
following properties.

i Monotonicity: let v1, v2 ∈ RS such that v1 ≥ v2. Then,

Tπ,Ωv1 ≥ Tπ,Ωv2 and T∗,Ωv1 ≥ T∗,Ωv2.

Theorem 1 (Optimal regularized policy). The policy
π∗,Ω = GΩ(v∗,Ω) is the unique optimal regularized policy,
in the sense that for all π ∈ ∆S

A, vπ∗,Ω,Ω = v∗,Ω ≥ vπ,Ω.

When regularizing the MDP, we change the problem at
hand. The following result relates value functions in
(un)regularized MDPs.
Proposition 3. Assume that LΩ ≤ Ω ≤ UΩ. Let π be any
policy. We have that vπ − UΩ
1−γ 1 and
1−γ 1 ≤ v∗,Ω ≤ v∗ − LΩ
v∗ − UΩ

1−γ 1 ≤ vπ,Ω ≤ vπ − LΩ
1−γ 1.

Regularization changes the optimal policy, the next result
shows how it performs in the original MDP.
Theorem 2. Assume that LΩ ≤ Ω ≤ UΩ. We have that

v∗ −

UΩ − LΩ
1 − γ

≤ vπ∗,Ω ≤ v∗.

ii Distributivity: for any c ∈ R, we have that

3.3. Related Works

Tπ,Ω(v + c1) = Tπ,Ωv + γc1
and T∗,Ω(v + c1) = T∗,Ωv + γc1.

iii Contraction: both operators are γ-contractions in supre-

mum norm. For any v1, v2 ∈ RS ,

(cid:107)Tπ,Ωv1 − Tπ,Ωv2(cid:107)∞ ≤ γ(cid:107)v1 − v2(cid:107)∞
and (cid:107)T∗,Ωv1 − T∗,Ωv2(cid:107)∞ ≤ γ(cid:107)v1 − v2(cid:107)∞.

3.2. Regularized value functions

The regularized operators being contractions, we can deﬁne
regularized value functions as their unique ﬁxed-points. No-
tice that from the following deﬁnitions, we could also easily
derive regularized Bellman operators on q-functions.
Deﬁnition 2 (Regularized value function of policy π).
Noted vπ,Ω, it is deﬁned as the unique ﬁxed point of the
operator Tπ,Ω: vπ,Ω = Tπ,Ωvπ,Ω. We also deﬁne the asso-
ciated state-action value function qπ,Ω as

qπ,Ω(s, a) = r(s, a) + γEs(cid:48)|s,a[vπ,Ω(s(cid:48))]

with vπ,Ω(s) = Ea∼π(.|s)[qπ,Ω(s, a)] − Ω(π(.|s)).

Thus, the regularized value function is simply the unreg-
ularized value of π for the reward rπ − Ω(π), that is
vπ,Ω = (I − γPπ)−1(rπ − Ω(π)).
Deﬁnition 3 (Regularized optimal value function). Noted
v∗,Ω, it is the unique ﬁxed point of the operator T∗,Ω: v∗,Ω =
T∗,Ωv∗,Ω. We also deﬁne the associated state-action value
function q∗,Ω(s, a) as

q∗,Ω(s, a) = r(s, a) + γEs(cid:48)|s,a[v∗,Ω(s(cid:48))]

with v∗,Ω(s) = Ω∗(q∗,Ω(s, .)).

Some of these results already appeared in the literature, in
different forms and with speciﬁc regularizers. For example,
the contraction of T∗,Ω (Prop. 2) was shown in various
forms, e.g. (Fox et al., 2016; Asadi & Littman, 2017; Dai
et al., 2018), as well as the relation between (un)regularized
optimal value functions (Th. 2), e.g. (Lee et al., 2018; Dai
et al., 2018). The link to Legendre-Fenchel has also been
considered before, e.g. (Dai et al., 2018; Mensch & Blondel,
2018; Richemond & Maginnis, 2017).

The core contribution of Sec. 3 is the regularized Bellman
operator, inspired by Nesterov (2005) and Mensch & Blon-
del (2018). It allows building in a principled and general
way regularized MDPs, and generalizing existing results
easily. More importantly, it is the core building block of reg-
ularized (A)DP, studied in the next sections. The framework
and analysis we propose next rely heavily on this formalism.

4. Regularized Modiﬁed Policy Iteration

Having deﬁned the notion of regularized MDPs, we still
need algorithms that solve them. As the regularized Bell-
man operators have the same properties as the classical ones,
we can apply classical dynamic programming. Here, we
consider directly the modiﬁed policy iteration approach (Put-
erman & Shin, 1978), that we regularize (reg-MPI for short):

(cid:40)

πk+1 = GΩ(vk)
vk+1 = (Tπk+1,Ω)mvk

.

(1)

Given an initial v0, reg-MPI iteratively performs a regu-
larized greedy step to get πk+1 and a partial regularized
evaluation step to get vk+1.

A Theory of Regularized MDPs

With m = 1, we retrieve a regularized value iteration algo-
rithm, that can be simpliﬁed as vk+1 = T∗,Ωvk (as πk+1 is
greedy resp. to vk, we have Tπk+1,Ωvk = T∗,Ωvk). With
m = ∞, we obtain a regularized policy iteration algorithm,
that can be simpliﬁed as πk+1 = GΩ(vπk,Ω) (indeed, with a
slight abuse of notation, (Tπk,Ω)∞vk−1 = vπk,Ω).

Before studying the convergence and rate of convergence
of this general algorithmic scheme (with approximation),
we discuss its links to state of the art algorithms (and more
generally how it can be practically instantiated).

4.1. Related algorithms

Most existing schemes consider the negative entropy as the
regularizer. Usually, it is also more convenient to work with
q-functions. First, we consider the case m = 1. In the exact
case, the regularized value iteration scheme can be written

qk+1(s, a) = r(s, a) + γEs(cid:48)|s,a[Ω∗(qk(s(cid:48), ·))].

In the entropic case, Ω∗(qk(s, ·)) = ln (cid:80)
a exp qk(s, a). In
an approximate setting, the q-function can be parameter-
ized by parameters θ (for example, the weights of a neural
network), write ¯θ the target parameters (computed during
the previous iteration) and ˆE the empirical expectation over
sampled transitions (si, ai, ri, s(cid:48)
i), an iteration amounts to
minimize the expected loss

J(θ) = ˆE

(ˆqi − qθ(si, ai))2(cid:105)
(cid:104)
i, ·)).

with ˆqi = ri + γΩ∗(q¯θ(s(cid:48)

(2)

Getting a practical algorithm may require more work, for ex-
ample for estimating Ω∗(q¯θ(s(cid:48)
i, ·)) in the case of continuous
actions (Haarnoja et al., 2017), but this is the core principle
of soft Q-learning (Fox et al., 2016; Schulman et al., 2017).
This idea has also been applied using the Tsallis entropy as
the regularizer (Lee et al., 2018).

Alternatively, assume that qk has been estimated. One
could compute the regularized greedy policy analytically,
πk+1(·|s) = ∇Ω∗(qk(s, ·)). Instead of computing this for
any state-action couple, one can generalize this from ob-
served transitions to any state-action couple through a pa-
rameterized policy πw, by minimizing the KL divergence
between both distributions:

J(w) = ˆE[KL(πw(·|si)||∇Ω∗(qk(si, .)))].

(3)

This is done in SAC (Haarnoja et al., 2018a), with
(and thus ∇Ω∗(qk(s, .)) =
an entropic regularizer
exp qk(s,·)
a exp qk(s,a) ). This is also done in Maximum A Posteriori
(cid:80)
Policy Optimization (MPO) (Abdolmaleki et al., 2018b)
with a KL regularizer (a case we discuss Sec. 5), or by Ab-
dolmaleki et al. (2018a) with more general “conservative”
greedy policies.

Back to SAC, qk is estimated using a TD-like approach, by
minimizing2 for the current policy π:

J(θ) = ˆE[(ˆqi − qθ(si, ai))2]

(4)

with ˆqi = ri + γ(Ea∼π(·|s(cid:48)

i)[q¯θ(s(cid:48)

i, a)] − Ω(π(·, s(cid:48)

i)).

For SAC, we have Ω(π(·, s)) = Ea∼π(·|s)[ln π(a|s)] specif-
ically (negative entropy). This approximate evaluation step
corresponds to m = 1, and SAC is therefore more a VI
scheme than a PI scheme, as presented by Haarnoja et al.
(2018a) (the difference with soft Q-learning lying in how the
greedy step is performed, implicitly or explicitly). It could
be extended to the case m > 1 in two ways. One possibility
is to minimize m times the expected loss (4), updating the
target parameter vector ¯θ between each optimization, but
keeping the policy π ﬁxed. Another possibility is to replace
the 1-step rollout of Eq. (4) by an m-step rollout (similar to
classical m-step rollouts, up to the additional regularizations
correcting the rewards). Both are equivalent in the exact
case, but not in the general case.

Depending on the regularizer, Ω∗ or ∇Ω∗ might not be
known analytically. In this case, one can still solve the
greedy step directly. Recall that the regularized greedy
policy satisﬁes πk+1 = maxπ Tπ,Ωvk. In an approximate
setting, this amounts to maximize3

J(w) = ˆE (cid:2)Ea∼πw(·|si)[qk(si, a)] − Ω(πw(·|si)(cid:3) .

(5)

This improvement step is used by Riedmiller et al. (2018)
with an entropy, as well as by TRPO (up to the fact that the
objective is constrained rather than regularized), with a KL
regularizer (see Sec. 5).

To sum up, for any regularizer Ω, with m = 1 one can
concatenate greedy and evaluation steps as in Eq. (2), with
m ≥ 1 one can estimate the greedy policy using either
Eqs. (3) or (5), and estimate the q-function using Eq. 4, ei-
ther performed m times repeatedly or combined with m-step
rollouts, possibly combined with off-policy correction such
as importance sampling or Retrace (Munos et al., 2016).

4.2. Analysis

We analyze the propagation of errors of the scheme depicted
in Eq. (1), and as a consequence, its convergence and rate of
convergence. To do so, we consider possible errors in both
the (regularized) greedy and evaluation steps,

(cid:40)

(cid:15)(cid:48)
k+1
Ω

πk+1 = G
(vk)
vk+1 = (Tπk+1,Ω)mvk + (cid:15)k+1

,

(6)

2Actually, a separate network is used to estimate the value

function, but it is not critical here.

3One could add a state-dependant baseline to qk, eg. vk, this

does not change the maximizer but can reduce the variance.

A Theory of Regularized MDPs

(cid:15)(cid:48)
k+1
(vk) meaning that for any policy π, we
with πk+1 = G
Ω
have Tπ,Ωvk ≤ Tπk+1,Ωvk + (cid:15)(cid:48)
k+1. The following analysis
is basically the same as the one of Approximate Modiﬁed
Policy Iteration (AMPI) (Scherrer et al., 2015), thanks to
the results of Sec. 3 (especially Prop. 2).

The distance we bound is the loss lk,Ω = v∗,Ω − vπk,Ω. The
bound will involve the terms d0 = v∗,Ω − v0 and b0 =
v0 − Tπ1,Ωv0. It requires also deﬁning the following.
Deﬁnition 4 (Γ-matrix (Scherrer et al., 2015)). For n ∈ N∗,
Pn is the set of transition kernels deﬁned as 1) for any set of
n policies {π1, . . . , πn}, (cid:81)n
i=1(γPπi) ∈ Pn and 2) for any
α ∈ (0, 1) and (P1, P2) ∈ Pn ×Pn, αP1 +(1−α)P2 ∈ Pn.
Any element of Pn is denoted Γn.

We ﬁrst state a point-wise bound on the loss. This is the
same bound as for AMPI, generalized to regularized MDPs.
Theorem 3. After k iterations of scheme (6), we have

lk,Ω ≤ 2

k−1
(cid:88)

∞
(cid:88)

i=1

j=i

Γj|(cid:15)k−i| +

k−1
(cid:88)

∞
(cid:88)

i=0

j=i

Γj|(cid:15)(cid:48)

k−i| + h(k)

with h(k) = 2 (cid:80)∞

j=k Γj|d0| or h(k) = 2 (cid:80)∞

j=k Γj|b0|.

Next, we provide a bound on the weighted (cid:96)p-norm of the
loss, deﬁned for a distribution ρ as (cid:107)lk(cid:107)p
p,ρ = ρ|lk|p. Again,
this is the AMPI bound generalized to regularized MDPs.
Corollary 1. Let ρ and µ be distributions. Let p, q and q(cid:48)
such that 1
q(cid:48) = 1. Deﬁne the concentrability coefﬁcients
q = 1−γ
j=i γj maxπ1,...,πj
. After
γi
k iterations of scheme (6), the loss satisﬁes

ρPπ1 Pπ2 ...Pπj
µ

q + 1
(cid:80)∞

(cid:13)
(cid:13)
(cid:13)q,µ

(cid:13)
(cid:13)
(cid:13)

C i

(cid:107)lk,Ω(cid:107)p,ρ ≤ 2

+

k−1
(cid:88)

i=1

k−1
(cid:88)

i=0

γi
1 − γ

(C i
q)

1

p (cid:107)(cid:15)k−i(cid:107)pq(cid:48),µ

γi
1 − γ

(C i
q)

1

p (cid:107)(cid:15)(cid:48)

k−i(cid:107)pq(cid:48),µ + g(k)

with g(k) = 2γk

1−γ (C i
q)

1

p min((cid:107)d0(cid:107)pq(cid:48),µ, (cid:107)b0(cid:107)pq(cid:48),µ).

As this is the same bound (up to the fact that it deals with
regularized MDPs) as the one of AMPI, we refer to Scherrer
et al. (2015) for a broad discussion about it. It is similar to
other error propagation analyses in reinforcement learning,
and generalizes those that could be obtained for regularized
value or policy iteration. The factor m does not appear in
the bound. This is also discussed by Scherrer et al. (2015),
but basically this depends on where the error is injected. We
could derive a regularized version of Classiﬁcation-based
Modiﬁed Policy Iteration (CBMPI, see Scherrer et al. (2015)
again) and make it appear.

So, we get the same bound for reg-MPI that for unregu-
larized AMPI, no better nor worse. This is a good thing,

as it justiﬁes considering regularized MDPs, but it does no
explain the good empirical results of related algorithms.

With regularization, policies will be more stochastic than in
classical approximate DP (that tends to produce determin-
istic policies). Such stochastic policies can induce lower
concentrability coefﬁcients. We also hypothesize that regu-
larizing the greedy step helps controlling the related approx-
imation error, that is the (cid:107)(cid:15)(cid:48)
k−i(cid:107)pq(cid:48),µ terms. Digging this
question would require instantiating more the algorithmic
scheme and performing a ﬁnite sample analysis of the re-
sulting optimization problems. We left this for future work,
and rather pursue the general study of solving regularized
MDPs, with varying regularizers now.

5. Mirror Descent Modiﬁed Policy Iteration

Solving a regularized MDP provides a solution that differs
from the one of the unregularized MDP (see Thm. 2). The
problem we address here is estimating the original optimal
policy while solving regularized greedy steps. Instead of
considering a ﬁxed regularizer Ω(π), the key idea is to pe-
nalize a divergence between the policy π and the policy
obtained at the previous iteration of an MPI scheme. We
consider more speciﬁcally the Bregman divergence gener-
ated by the strongly convex regularizer Ω.

Let π(cid:48) be some given policy (typically πk, when computing
πk+1), the Bregman divergence generated by Ω is

Ωπ(cid:48)

s

(πs) = DΩ(πs||π(cid:48)
s)

= Ω(πs) − Ω(π(cid:48)

s) − (cid:104)∇Ω(π(cid:48)

s), πs − π(cid:48)

s(cid:105).

For example, the KL divergence is generated by the negative
a πs(a) ln πs(a)
entropy: KL(πs||π(cid:48)
s(a) . With a slight
π(cid:48)
abuse of notation, as before, we will write

s) = (cid:80)

Ωπ(cid:48)(π) = DΩ(π||π(cid:48)) = Ω(π)−Ω(π(cid:48))−(cid:104)∇Ω(π(cid:48)), π −π(cid:48)(cid:105).

This divergence is always positive, it satisﬁes Ωπ(cid:48)(π(cid:48)) = 0,
and it is strongly convex in π (so Prop. 1 applies).

We consider a reg-MPI algorithmic scheme with a Bregman
divergence replacing the regularizer. For the greedy step,
we simply consider πk+1 = GΩπk

(vk), that is

πk+1 = argmax

(cid:104)qk, π(cid:105) − DΩ(π||πk).

π

This is similar to the update of the Mirror Descent (MD)
algorithm in its proximal form (Beck & Teboulle, 2003),
with −qk playing the role of the gradient in MD. There-
fore, we will call this approach Mirror Descent Modiﬁed
Policy Iteration (MD-MPI). For the partial evaluation step,
we can regularize according to the previous policy πk,
)mvk, or according to the cur-
that is vk+1 = (Tπk+1,Ωπk
)mvk. As
rent policy πk+1, that is vk+1 = (Tπk+1,Ωπk+1

A Theory of Regularized MDPs

Ωπk+1 (πk+1) = 0, this simpliﬁes as vk+1 = (Tπk+1)mvk,
that is a partial unregularized evaluation.

To sum up, we will consider two general algorithmic
schemes based on a Bregman divergence, MD-MPI types 1
and 2 respectively deﬁned as

(cid:40)

πk+1 = GΩπk
(vk)
vk+1 = (Tπk+1,Ωπk

)mvk

,

(cid:40)

πk+1 = GΩπk
vk+1 = (Tπk+1)mvk

(vk)

and both initialized with some v0 and π0.

5.1. Related algorithms

(qk(s, ·)) = ln (cid:80)

To derive practical algorithms, the recipes provided in
Sec. 4.1 still apply, just replacing Ω by Ωπk . If m = 1,
greedy and evaluation steps can be concatenated (only for
MD-MPI type 1). In the general case (m ≥ 1) the greedy
policy (for MD-MPI types 1 and 2) can be either directly
estimated (Eq. (5)) or trained to generalize the analytical
solution (Eq. (3)). The partial evaluation can be done using
a TD-like approach, either done repeatedly while keeping
the policy ﬁxed or considering m-step rollouts. Speciﬁ-
cally, in the case of a KL divergence, one could use the fact
that Ω∗
a πk(a|s) exp qk(s, a) and that
πk
(qk(s, ·)) = πk(·|s) exp qk(s,·)
∇Ω∗
(cid:80)
πk

a πk(a|s) exp qk(s,a) .
This general algorithmic scheme allows recovering state
of the art algorithms. For example, MD-MPI type 2
with m = ∞ and a KL divergence as the regularizer is
TRPO (Schulman et al., 2015) (with a direct optimization
of the regularized greedy step, as in Eq. (5), up to the use of
a constraint instead of a regularization). DPP can be seen as
a reparametrization4 of MD-MPI type 1 with m = 1 (Azar
et al., 2012, Appx. A). MPO (Abdolmaleki et al., 2018b)
is derived from an expectation-maximization principle, but
it can be seen as an instantiation of MD-MPI type 2, with
a KL divergence, a greedy step similar to Eq. (3) (up to
additional regularization) and an evaluation step similar to
Eq. (4) (without regularization, as in type 2, with m-step
return and with the Retrace off-policy correction). This also
generally applies to the approach proposed by Abdolmaleki
et al. (2018a) (up to an additional subtelty in the greedy step
consisting in decoupling updates for the mean and variance
in the case of a Gaussian policy).

5.2. Analysis

Here, we propose to analyze the error propagation of MD-
MPI (and thus, its convergence and rate of convergence).
We think this is an important topic, as it has only been partly

4Indeed, if one see MD-MPI as a Mirror Descent approach, one
can see DPP as a dual averaging approach, somehow updating a
kind of cumulative q-functions directly in the dual. However, how
to generalize this beyond the speciﬁc DPP algorithm is unclear,
and we let it for future work.

studied for the special cases discussed in Sec. 5.1. For exam-
ple, DPP enjoys an error propagation analysis in supremum
norm (yet it is a reparametrization of a special case of MD-
MPI, so not directly covered here), while TRPO or MPO
are only guaranteed to have monotonic improvements, un-
der some assumptions. Notice that we do not claim that
our analysis covers all these cases, but it will provide the
key technical aspects to analyze similar schemes (much like
CBMPI compared to AMPI, as discussed in Sec. 4.2 or
by Scherrer et al. (2015); where the error is injected changes
the bounds).

In Sec. 4.2, the analysis was a straightforward adaptation of
the one of AMPI, thanks to the results of Sec. 3 (the regular-
ized quantities behave like their unregularized counterparts).
It is no longer the case here, as the regularizer changes over
iterations, depending on what has been computed so far. We
will notably need a slightly different notion of approximate
regularized greediness.

Deﬁnition 5 (Approximate Bregman divergence-regular-
ized greediness). Write Jk(π) the (negative) optimiza-
tion problem corresponding to the Bregman divergence-
regularized greediness (that is, negative regularized Bell-
man operator of π applied to vk):

Jk(π) = (cid:104)−qk, π(cid:105) + DΩ(π||πk) = −Tπ,Ωπk

vk.

We write πk+1 ∈ G
πk+1 satisﬁes

(cid:15)(cid:48)
k+1
Ωπk

(vk) if for any policy π the policy

(cid:104)∇Jk(πk+1), π − πk+1(cid:105) + (cid:15)(cid:48)

k+1 ≥ 0.

(cid:15)(cid:48)
k+1
Ωπk

(vk) means that πk+1 is (cid:15)(cid:48)

In other words, πk+1 ∈ G
k+1-
close to satisfying the optimality condition, which might be
slightly stronger than being (cid:15)(cid:48)
k+1-close to the optimal (as
for AMPI or reg-MPI). Given this, we consider MD-MPI
with errors in both greedy and evaluation steps, type 1

(cid:40)

(cid:15)(cid:48)
k+1
Ωπk

(vk)
πk+1 = G
vk+1 = (Tπk+1,Ωπk

)mvk + (cid:15)k+1

and type 2

(cid:40)

(cid:15)(cid:48)
k+1
Ωπk

πk+1 = G
vk+1 = (Tπk+1)mvk + (cid:15)k+1

(vk)

.

The quantity we are interested in is v∗ − vπk , that is sub-
optimality in the unregularized MDP, while the algorithms
compute new policies with a regularized greedy operator.
So, we need to relate regularized and unregularized quanti-
ties when using a Bregman divergence based on the previous
policy. The next lemma is the key technical result that allows
analyzing MD-MPI.

A Theory of Regularized MDPs

Lemma 1. Assume that πk+1 ∈ G
Def. 5. Then, the policy πk+1 is (cid:15)(cid:48)
ized greedy policy, in the sense that for any policy π

(vk), as deﬁned in
k+1-close to the regular-

(cid:15)(cid:48)
k+1
Ωπk

Tπ,Ωπk

vk − Tπk+1,Ωπk

vk ≤ (cid:15)(cid:48)

k+1.

Moreover, we can relate the (un)regularized Bellman oper-
ators applied to vk. For any policy π (so notably for the
unregularized optimal policy π∗), we have
vk ≤ (cid:15)(cid:48)

k+1 + DΩ(π||πk) − DΩ(π||πk+1),

Tπvk − Tπk+1,Ωπk

Tπvk − Tπk+1 vk ≤ (cid:15)(cid:48)

k+1 + DΩ(π||πk) − DΩ(π||πk+1).

k = (Tπk,Ωπk−1

)mvk−1 = v∗ − (vk − (cid:15)k) and d2

We’re interested in bounding the loss lk = v∗ − vπk , or
some related quantity, for each type of MD-MPI. To do so,
we introduce quantities similar to the ones of the AMPI anal-
ysis (Scherrer et al., 2015), deﬁned respectively for types 1
and 2: 1) The distance between the optimal value func-
tion and the value before approximation at the kth iteration,
d1
k =
k = v∗ − (Tπk,Ωπk−1
v∗ − (Tπk )mvk−1 = v∗ − (vk − (cid:15)k); 2) The shift between
the value before approximation and the policy value a itera-
tion k, s1
)mvk−1 − vπk = (vk − (cid:15)k) − vπk
and s2
k = (Tπk )mvk−1 − vπk = (vk − (cid:15)k) − vπk ; 3) the
Bellman residual at iteration k, b1
vk and
b2
k = vk − Tπk+1vk.
For both types (h ∈ {1, 2}), we have that lh
k, so
bounding the loss requires bounding these quantities, which
is done in the following lemma (quantities related to both
types enjoy the same bounds).
Lemma 2. Let k ≥ 1, deﬁne xk = (I −γPπk )(cid:15)k+(cid:15)(cid:48)
yk = −γPπ∗ (cid:15)k + (cid:15)(cid:48)
DΩ(π∗||πk+1). We have for h ∈ {1, 2} :

k+1 and
k+1, as well as δk(π∗) = DΩ(π∗||πk) −

k = vk − Tπk+1,Ωπk

k = dh

k + sh

bh
k ≤ (γPπk )mbh
k ≤ (γPπk )m(I − γPπk )−1bh
sh

k−1 + xk,

k−1 and

k+1 ≤ γPπ∗ dh
dh

k + yk +

m−1
(cid:88)

j=1

(γPπk+1 )jbh

k + δk(π∗).

the same as the ones of
These bounds are almost
AMPI (Scherrer et al., 2015, Lemma 2), up to the addi-
tional δk(π∗) term in the bound of the distance dh
k. One
can notice that summing these terms gives a telescopic
sum: (cid:80)K−1
k=0 δk(π∗) = DΩ(π∗||π0) − DΩ(π∗||πK) ≤
DΩ(π∗||π0) ≤ supπ DΩ(π||π0). For example, if DΩ
is the KL divergence and π0 the uniform policy, then
(cid:107) supπ DΩ(π||π0)(cid:107)∞ = ln |A|. This suggests that we must
bound the regret LK deﬁned as

Lk =

K
(cid:88)

k=1

K
(cid:88)

lk =

(v∗ − vπk ).

k=1

Theorem 4. Deﬁne RΩπ0
= (cid:107) supπ DΩ(π||π0)(cid:107)∞, after
K iterations of MD-MPI, for h = 1, 2, the regret satisﬁes

K
(cid:88)

k−1
(cid:88)

∞
(cid:88)

LK ≤ 2

Γj|(cid:15)k−i| +

K
(cid:88)

k−1
(cid:88)

∞
(cid:88)

Γj|(cid:15)(cid:48)

k−i|

k=2

i=1

j=i

k=1

i=0

j=i

+

K
(cid:88)

k=1

h(k) +

1 − γK
(1 − γ)2 RΩπ0

1.

with h(k) = 2 (cid:80)∞

j=k Γj|d0| or h(k) = 2 (cid:80)∞

j=k Γj|b0|.

From this,we can derive an (cid:96)p-bound for the regret.
Corollary 2. Let ρ and µ be distributions over states. Let p,
q and q(cid:48) be such that 1
q(cid:48) = 1. Deﬁne the concentrability
coefﬁcients C i
q as in Cor. 1. After K iterations, the regret
satisﬁes

q + 1

(cid:107)LK(cid:107)p,ρ ≤ 2

K
(cid:88)

k−1
(cid:88)

k=2

i=1

K
(cid:88)

k−1
(cid:88)

k=1

i=0

+

+ g(k) +

γi
1 − γ

(C i
q)

1

p (cid:107)(cid:15)k−i(cid:107)pq(cid:48),µ

γi
1 − γ

(C i
q)

1

p (cid:107)(cid:15)(cid:48)

k−i(cid:107)pq(cid:48),µ

1 − γK
(1 − γ)2 RΩπ0

.

with g(k) = 2 (cid:80)K

k=1

γk
1−γ (C k
q )

1

p min((cid:107)d0(cid:107)pq(cid:48),µ, (cid:107)b0(cid:107)pq(cid:48),µ).

This result bounds the regret, while it is usually the loss that
is bounded. Both can be related as follows.
Proposition 4. For any p ≥ 1 and distribution ρ, we have
min1≤k≤K (cid:107)v∗ − vπk (cid:107)1,ρ ≤ 1

K (cid:107)LK(cid:107)p,ρ.

This means that if we can control the average regret, then we
can control the loss of the best policy computed so far. This
suggests that practically we should not use the last policy,
but this best policy.

From Cor. 2 can be derived the convergence and rate of
convergence of MD-MPI in the exact case.
Corollary 3. Both MD-MPI type 1 and 2 enjoy the follow-
ing rate of convergence, when no approximation is done
((cid:15)k = (cid:15)(cid:48)

k = 0),

1
K

(cid:107)LK(cid:107)∞ ≤

1 − γK
(1 − γ)2

2γ(cid:107)v∗ − v0(cid:107)∞ + RΩπ0
K

.

In classical DP and in regularized DP (see Cor. 1), there is
a linear convergence rate (the bound is 2γK
1−γ (cid:107)v∗ − v0(cid:107)∞),
while in this case we only have a logarithmic convergence
rate. We also pay an horizon factor (square dependency in
1
1−γ instead of linear). This is normal, as we bound the
regret instead of the loss. Bounding the regret in classical
DP would lead to the bound of Cor. 3 (without the RΩπ0
term).

A Theory of Regularized MDPs

The convergence rate of the loss of MD-MPI is an open
question, but a sublinear rate is quite possible. Compared
to classical DP, we slow down greediness by adding the
Bregman divergence penalty. Yet, this kind of regularization
is used in an approximate setting, where it favors stability
empirically (even if studying this further would require
much more work regarding the (cid:107)(cid:15)(cid:48)
k(cid:107) term, as discussed
in Sec. 4.2).

As far as we know, the only other approach that studies a
DP scheme regularized by a divergence and that offers a
convergence rate is DPP, up to the reparameterization we
discussed earlier. MD-MPI has the same upper-bound as
DPP in the exact case (Azar et al., 2012, Thm. 2). However,
DPP bounds the loss, while we bound a regret. This means
that if the rate of convergence of our loss can be sublinear, it
is superlogarithmic (as the rate of the regret is logarithmic),
while the rate of the loss of DPP is logarithmic.

To get more insight on Cor. 2, we can group the terms
differently, by grouping the errors.

Corollary 4. With the same notations as Cor. 2, we have

1
K

(cid:107)Lk(cid:107)p,ρ ≤

K−1
(cid:88)

i=1
1
K

+

γi
1 − γ

(cid:18)

(C i
q)

1
p

2EK−i + E(cid:48)

K−i

K

g(k) +

1 − γK
(1 − γ)2 RΩπ0

(cid:19)

,

with Ei = (cid:80)i

j=1 (cid:107)(cid:15)j(cid:107)pq(cid:48),µ and E(cid:48)

i = (cid:80)i

j=1 (cid:107)(cid:15)(cid:48)

j(cid:107)pq(cid:48),µ.

Compared to the bound of AMPI (Scherrer et al., 2015,
Thm. 7), instead of propagating the errors, we propagate
the sum of errors over previous iterations normalized by
the total number of iterations. So, contrary to approximate
DP, it is no longer the last iterations that have the highest
inﬂuence on the regret. Yet, we highlight again the fact that
we bound a regret, and bounding the regret of AMPI would
provide a similar result.

Our result is similar to the error propagation of DPP (Azar
et al., 2012, Thm. 5), except that we sum norms of errors,
instead of norming a sum of errors, the later being much
better (as it allows the noise to cancel over iterations). Yet,
as said before, DPP is not a special case of our framework,
but a reparameterization of such one. Consequently, while
we estimate value functions, DPP estimate roughly at itera-
tion k a sum of k advantage functions (converging to −∞
for any suboptimal action in the exact case). As explained
before, where the error is injected does matter. Knowing
if the DPP’s analysis can be generalized to our framework
(MPI scheme, (cid:96)p bounds) remains an open question.

To get further insight, we can express the bound using dif-
ferent concentrability coefﬁcients.
Corollary 5. Deﬁne the concentrability coefﬁcient C l,k

q as

q = (1−γ)2
C l,k

γl−γk

(cid:80)k−1
i=l

(cid:80)∞

j=i cq(j), the regret then satisﬁes

(cid:107)LK(cid:107)p,ρ ≤ 2

+

K−1
(cid:88)

i=1

K−1
(cid:88)

i=0

γ − γi+1
(1 − γ)2 (C 1,i+1

q

)

1

p (cid:107)(cid:15)K−i(cid:107)pq(cid:48),µ

1 − γi+1
(1 − γ)2 (C 0,i+1

q

)

1

p (cid:107)(cid:15)(cid:48)

K−i(cid:107)pq(cid:48),µ + f (k)

with f (k) = γ−γK+1
1−γK
(1−γ)2 RΩπ0

.

(1−γ)2 (C 1,K+1

q

1

p min((cid:107)d0(cid:107)pq(cid:48),µ, (cid:107)b0(cid:107)pq(cid:48),µ)+

)

We observe again that contrary to ADP, the last iteration
does not have the highest inﬂuence, and we do not enjoy a
decrease of inﬂuence at the exponential rate γ towards the
initial iterations. However, we bound a different quantity
(regret instead of loss), that explains this behavior. Here
again, bounding the regret in AMPI would lead to the same
). Moreover, sending p and K
bound (up to the term RΩπ0
to inﬁnity, deﬁning (cid:15) = supj (cid:107)(cid:15)j(cid:107)∞ and (cid:15)(cid:48) = supj (cid:107)(cid:15)(cid:48)
j(cid:107)∞,
K (cid:107)LK(cid:107)∞ ≤ 2γ(cid:15)+(cid:15)(cid:48)
we get lim sup
(1−γ)2 , which is the classical
K→∞
asymptotical bound for approximate value and policy itera-
tions (Bertsekas & Tsitsiklis, 1996) (usually stated without
greedy error). It is generalized here to an approximate MPI
scheme regularized with a Bregman divergence.

1

6. Conclusion

We have introduced a general theory of regularized MDPs,
where the usual Bellman evaluation operator is modiﬁed
by either a ﬁxed convex function or a Bregman divergence
between consecutive policies. For both cases, we proposed
a general algorithmic scheme based on MPI. We shown how
many (variations of) existing algorithms could be derived
from this general algorithmic scheme, and also analyzed
and discussed the related propagation of errors.

We think that this framework can open many perspectives,
among which links between (approximate) DP and prox-
imal convex optimization (going beyond mirror descent),
temporal consistency equations (roughly regularized Bell-
man residuals), regularized policy search (maximizing the
expected regularized value function), inverse reinforcement
learning (thanks to uniqueness of greediness in this regular-
ized framework) or zero-sum Markov games (regularizing
the two-player Bellman operators). We develop more these
points in the appendix.

This work also lefts open questions, such as combining the
propagation of errors with a ﬁnite sample analysis, or what
speciﬁc regularizer one should choose for what context.
Some approaches also combine a ﬁxed regularizer and a
divergence (Akrour et al., 2018), a case not covered here
and worth being investigated.

A Theory of Regularized MDPs

References

Abdolmaleki, A., Springenberg, J. T., Degrave, J., Bohez, S.,
Tassa, Y., Belov, D., Heess, N., and Riedmiller, M. Rela-
tive entropy regularized policy iteration. arXiv preprint
arXiv:1812.02256, 2018a.

Abdolmaleki, A., Springenberg, J. T., Tassa, Y., Munos,
R., Heess, N., and Riedmiller, M. Maximum a posteri-
ori policy optimisation. In International Conference on
Learning Representations (ICLR), 2018b.

Akrour, R., Abdolmaleki, A., Abdulsamad, H., Peters, J.,
and Neumann, G. Model-free trajectory-based policy
optimization with monotonic improvement. The Journal
of Machine Learning Research (JMLR), 19(1):565–589,
2018.

Haarnoja, T., Zhou, A., Abbeel, P., and Levine, S. Soft
actor-critic: Off-policy maximum entropy deep reinforce-
ment learning with a stochastic actor. In International
Conference on Machine Learning (ICML), 2018a.

Haarnoja, T., Zhou, A., Hartikainen, K., Tucker, G., Ha,
S., Tan, J., Kumar, V., Zhu, H., Gupta, A., Abbeel, P.,
et al. Soft actor-critic algorithms and applications. arXiv
preprint arXiv:1812.05905, 2018b.

Hiriart-Urruty, J.-B. and Lemar´echal, C. Fundamentals of
convex analysis. Springer Science & Business Media,
2012.

Lee, K., Choi, S., and Oh, S. Sparse markov decision pro-
cesses with causal sparse tsallis entropy regularization for
reinforcement learning. IEEE Robotics and Automation
Letters, 3(3):1466–1473, 2018.

Asadi, K. and Littman, M. L. An alternative softmax opera-
tor for reinforcement learning. In International Confer-
ence on Machine Learning (ICML), 2017.

Levine, S. Reinforcement Learning and Control as Proba-
bilistic Inference: Tutorial and Review. arXiv preprint
arXiv:1805.00909, 2018.

Azar, M. G., G´omez, V., and Kappen, H. J. Dynamic policy
programming. Journal of Machine Learning Research
(JMLR), 13(Nov):3207–3245, 2012.

Beck, A. and Teboulle, M. Mirror descent and nonlinear
projected subgradient methods for convex optimization.
Operations Research Letters, 31(3):167–175, 2003.

Bertsekas, D. P. and Tsitsiklis, J. N. Neuro-Dynamic Pro-
gramming. Athena Scientiﬁc, 1st edition, 1996. ISBN
1886529108.

Dai, B., Shaw, A., Li, L., Xiao, L., He, N., Liu, Z., Chen, J.,
and Song, L. Sbeed: Convergent reinforcement learning
with nonlinear function approximation. In International
Conference on Machine Learning (ICML), 2018.

Finn, C., Levine, S., and Abbeel, P. Guided cost learning:
Deep inverse optimal control via policy optimization. In
International Conference on Machine Learning (ICML),
2016.

Fox, R., Pakman, A., and Tishby, N. Taming the noise in
reinforcement learning via soft updates. In Conference
on Uncertainty in Artiﬁcial Intelligence (UAI), 2016.

Fu, J., Luo, K., and Levine, S. Learning robust rewards with
adversarial inverse reinforcement learning. In Interna-
tional Conference on Representation Learning, 2018.

Haarnoja, T., Tang, H., Abbeel, P., and Levine, S. Rein-
forcement learning with deep energy-based policies. In
International Conference on Machine Learning (ICML),
2017.

Martins, A. and Astudillo, R. From softmax to sparsemax: A
sparse model of attention and multi-label classiﬁcation. In
International Conference on Machine Learning (ICML),
2016.

Mensch, A. and Blondel, M. Differentiable dynamic pro-
gramming for structured prediction and attention.
In
International Conference on Machine Learning (ICML),
2018.

Mnih, V., Badia, A. P., Mirza, M., Graves, A., Lillicrap,
T., Harley, T., Silver, D., and Kavukcuoglu, K. Asyn-
chronous methods for deep reinforcement learning. In
International Conference on Machine Learning (ICML),
2016.

Morgenstern, O. and Von Neumann, J. Theory of games and
economic behavior. Princeton university press, 1953.

Munos, R., Stepleton, T., Harutyunyan, A., and Bellemare,
M. Safe and efﬁcient off-policy reinforcement learning.
In Advances in Neural Information Processing Systems
(NIPS), pp. 1054–1062, 2016.

Nachum, O., Norouzi, M., Xu, K., and Schuurmans, D.
Bridging the gap between value and policy based rein-
forcement learning. In Advances in Neural Information
Processing Systems (NIPS), 2017.

Nachum, O., Chow, Y., and Ghavamzadeh, M. Path consis-
tency learning in tsallis entropy regularized mdps. arXiv
preprint arXiv:1802.03501, 2018.

Nemirovski, A. Prox-method with rate of convergence
o(1/t) for variational inequalities with lipschitz contin-
uous monotone operators and smooth convex-concave

A Theory of Regularized MDPs

Williams, R. J. Simple statistical gradient-following algo-
rithms for connectionist reinforcement learning. Machine
learning, 8(3-4):229–256, 1992.

Ziebart, B. D., Maas, A. L., Bagnell, J. A., and Dey, A. K.
Maximum entropy inverse reinforcement learning. In
AAAI Conference on Artiﬁcial Intelligence (AAAI), 2008.

saddle point problems. SIAM Journal on Optimization,
15(1):229–251, 2004.

Nesterov, Y. Smooth minimization of non-smooth functions.
Mathematical programming, 103(1):127–152, 2005.

Nesterov, Y. Primal-dual subgradient methods for convex
problems. Mathematical programming, 120(1):221–259,
2009.

Neu, G., Jonsson, A., and G´omez, V. A uniﬁed view of
entropy-regularized Markov decision processes. arXiv
preprint arXiv:1705.07798, 2017.

Ng, A. Y., Harada, D., and Russell, S. Policy invariance
under reward transformations: Theory and application to
reward shaping. In International Conference on Machine
Learning (ICML), 1999.

Perolat, J., Scherrer, B., Piot, B., and Pietquin, O. Approx-
imate dynamic programming for two-player zero-sum
markov games. In International Conference on Machine
Learning (ICML), 2015.

Peters, J., Mulling, K., and Altun, Y. Relative entropy policy
search. In AAAI Conference on Artiﬁcial Intelligence,
2010.

Puterman, M. L. and Shin, M. C. Modiﬁed policy itera-
tion algorithms for discounted markov decision problems.
Management Science, 24(11):1127–1137, 1978.

Richemond, P. H. and Maginnis, B. A short variational
proof of equivalence between policy gradients and soft q
learning. arXiv preprint arXiv:1712.08650, 2017.

Riedmiller, M., Hafner, R., Lampe, T., Neunert, M., De-
grave, J., Wiele, T., Mnih, V., Heess, N., and Springen-
berg, J. T. Learning by playing solving sparse reward
tasks from scratch. In International Conference on Ma-
chine Learning (ICML), pp. 4341–4350, 2018.

Scherrer, B., Ghavamzadeh, M., Gabillon, V., Lesner, B.,
and Geist, M. Approximate modiﬁed policy iteration and
its application to the game of tetris. Journal of Machine
Learning Research (JMLR), 16:1629–1676, 2015.

Schulman, J., Levine, S., Abbeel, P., Jordan, M., and Moritz,
In International

P. Trust region policy optimization.
Conference on Machine Learning (ICML), 2015.

Schulman, J., Chen, X., and Abbeel, P. Equivalence be-
tween policy gradients and soft q-learning. arXiv preprint
arXiv:1704.06440, 2017.

Sutton, R. S., McAllester, D. A., Singh, S. P., and Mansour,
Y. Policy gradient methods for reinforcement learning
with function approximation. In Advances in neural in-
formation processing systems (NIPS), 2000.

A Theory of Regularized MDPs

This appendices provide the proofs for all stated results (Appx. A to C) and discuss in more details the perspectives
mentioned in Sec. 5 (Appx. D).

A. Proofs of section 3

In this section, we prove the results of Sec. 3. We start with the properties of the regularized Bellman operators.

Proof of Proposition 2. We can write Tπ,Ωv = rπ − Ω(π) + γPπv, it is obviously afﬁne (in v). Then, we show that the
operators are monotonous. For the evaluation operator, we have

v1 ≥ v2 ⇒ Tπv1 ≥ Tπv2 ⇔ Tπ,Ωv1 = Tπv1 − Ω(π) ≥ Tπv2 − Ω(π) = Tπ,Ωv2.

For the optimality operator, we have

v1 ≥ v2 ⇒ ∀s ∈ S,

qs,1 ≥ qs,2

⇒ ∀s ∈ S, Ω∗(qs,1) ≥ Ω∗(qs,2)
⇔ ∀s ∈ S,
⇔ T∗,Ωv1 ≥ T∗,Ωv2.

[T∗,Ωv1](s) ≥ [T∗,Ωv2](s)

by Prop. 1

Then, we show the distributivity property. For the evaluation operator, we have

Tπ,Ω(v + c1) = Tπ(v + c1) − Ω(π) = Tπv + γc1 − Ω(π) = Tπ,Ωv + γc1.

For the optimality operator, for any s ∈ S, we have

[T∗,Ω(v + c1)](s) = Ω∗(qs + γc1)
= Ω∗(qs) + γc
= [T∗,Ωv](s) + γc.

by Prop. 1

Lastly, we study the contraction of both operators. For the evaluation operator, we have

Tπ,Ωv1 − Tπ,Ωv2 = Tπv1 − Ω(π) − (Tπv2 − Ω(π)) = Tπv1 − Tπv2.

So, the contraction is the same as the one of the unregularized operator. For the optimality operator, we have that

(cid:107)T∗,Ωv1 − T∗,Ωv2(cid:107)∞ = max
s∈S

|[T∗,Ωv1](s) − [T∗,Ωv2](s)| .

Pick s ∈ S, and without loss of generality assume that [T∗,Ωv1](s) ≥ [T∗,Ωv2](s). Write also π1 = GΩ(v1) and π2 = GΩ(v2).
We have

|[T∗,Ωv1](s) − [T∗,Ωv2](s)| = [T∗,Ωv1](s) − [T∗,Ωv2](s)

= [Tπ1,Ωv1](s) − [Tπ2,Ωv2](s)
≤ [Tπ1,Ωv1](s) − [Tπ1,Ωv2](s)
≤ γ(cid:107)v1 − v2(cid:107)∞.

as T∗,Ωv2 = Tπ2,Ωv2 ≥ Tπ1,Ωv2,

The stated result follows immediately.

Then, we show that in a regularized MDP, the policy greedy respectively to the optimal value function is indeed the optimal
policy, and is unique.

Proof of Theorem 1. The uniqueness of π∗,Ω is a consequence of the strong convexity of Ω, see Prop. 1. On the other hand,
by deﬁnition of the greediness, we have

π∗,Ω = GΩ(v∗,Ω) ⇔ Tπ∗,Ω,Ωv∗,Ω = T∗,Ωv∗,Ω = v∗,Ω.

This proves that v∗,Ω is the value function of π∗,Ω. Next, for any function v ∈ RS and any policy π, we have

A Theory of Regularized MDPs

Using monotonicity, we have that

T∗,Ωv ≥ Tπ,Ωv.

∗,Ωv = T∗,Ω(T∗,Ωv) ≥ T∗,Ω(Tπ,Ωv) ≥ Tπ,Ω(Tπ,Ωv) = T 2
T 2

π,Ωv.

By direct induction, for any n ≥ 1, T n

∗,Ωv ≥ T n

π,Ωv. Taking the limit as n → ∞, we conclude that v∗,Ω ≥ vπ,Ω.

Next, we relate regularized and unregularized value functions (for a given policy, and for the optimal value function).

Proof of Proposition 3. We start by linking (un)regularized values of a given policy. Let v ∈ RS . As Tπ,Ωv = Tπv − Ω(π),
we have that

Tπv − UΩ1 ≤ Tπ,Ωv ≤ Tπv − LΩ1.

We work on the left inequality ﬁrst. We have

π,Ωv = Tπ,Ω(Tπ,Ωv) ≥ Tπ,Ω(Tπv − UΩ1) ≥ Tπ(Tπv − UΩ1) − UΩ1 = T 2
T 2

π v − γUΩ1 − UΩ1.

By direct induction, for any n ≥ 1,

Taking the limit as n → ∞ we obtain

π,Ω ≥ T n
T n

π v −

n−1
(cid:88)

k=0

γkUΩ1.

vπ,Ω ≥ vπ −

UΩ
1 − γ

1.

The proof is similar for the right inequality. Next, we link the (un)regularized optimal values. As a direct corollary of
Prop. 1, for any v ∈ RS we have

Then, the proof is the same as above, switching evaluation and optimality operators.

T∗v − UΩ1 ≤ T∗,Ωv ≤ T∗v − LΩ1.

Lastly, we show how good is the optimal policy of the regularized MDP for the original problem (unregularized MDP).

Proof of Theorem 2. The right inequality is obvious, as for any π, v∗ ≥ vπ. For the left inequality,

v∗ ≤ v∗,Ω +

UΩ
1 − γ

= vπ∗,Ω,Ω +

≤ vπ∗,Ω +

UΩ
1 − γ
UΩ
1 − γ

−

by Prop. 3

by Thm. 1

by Prop. 3.

LΩ
1 − γ

B. Proofs of section 4

The results of section 4 do not need to be proven. Indeed, we have shown in Sec. 3 that all involved quantities of regularized
MDPs satisfy the same properties as their unregularized counterpart. Therefore, the proofs of these results are identical to
the proofs provided by Scherrer et al. (2015), up to the replacement of value functions, Bellman operators, and so on, by
their regularized counterparts. The proofs for Mirror Descent Modiﬁed Policy Iteration (Sec. 5) are less straightforward.

A Theory of Regularized MDPs

C. Proofs of section 5

As a prerequisite of Lemma 1, we need the following result.
Lemma 3 (Three-point identity). Let π be any policy, we have that

(cid:104)∇Ω(πk) − ∇Ω(πk+1), π − πk+1(cid:105) = DΩ(π||πk+1) − DΩ(π||πk) + DΩ(πk+1||πk).

Proof. This is the classical three-point identity of Bregman divergences, and can be checked by calculus:

DΩ(π||πk+1) − DΩ(π||πk) + DΩ(πk+1||πk) = Ω(π) − Ω(πk+1) − (cid:104)∇Ω(πk+1), π − πk+1(cid:105)

− Ω(π) + Ω(πk) + (cid:104)∇Ω(πk), π − πk(cid:105)
+ Ω(πk+1) − Ω(πk) − (cid:104)∇Ω(πk), πk+1 − πk(cid:105)

= (cid:104)∇Ω(πk) − ∇Ω(πk+1), π − πk+1(cid:105).

Now, we can prove the key lemma of MD-MPI.

Proof of Lemma 1. Let Jk be as deﬁned in Def. 5,

and let πk+1 ∈ G

(cid:15)(cid:48)
k+1
Ωπk

(vk), that is (cid:104)∇Jk(πk+1), π − πk+1(cid:105) + (cid:15)(cid:48)

k+1 ≥ 0. By convexity of Jk, for any policy π, we have

Jk(π) = (cid:104)−qk, π(cid:105) + DΩ(π||πk) = −Tπ,Ωπk

vk,

Jk(π) − Jk(πk+1) ≥ (cid:104)∇J(πk+1), π − πk+1(cid:105)

≥ −(cid:15)(cid:48)

k+1

vk + Tπk+1,Ωπk
⇔ −Tπ,Ωπk
⇔ Tπ,Ωπk
vk − Tπk+1,Ωπk
This is the ﬁrst result stated in Lemma 1.

vk ≥ −(cid:15)(cid:48)
vk ≤ (cid:15)(cid:48)

k+1

k+1

by convexity of Jk

as πk+1 ∈ G

(cid:15)(cid:48)
k+1
Ωπk

(vk)

using Jk(π) = −Tπ,Ωπk

vk

Next, we relate (un)regularized quantities. We start with the following decomposition

Taking the gradient of Jk (by using the deﬁnition of the Bregman divergence), we get

(cid:104)−qk, π(cid:105) = (cid:104)−qk, πk+1(cid:105) + (cid:104)−qk, π − πk+1(cid:105).

∇Jk(πk+1) = −qk + ∇Ω(πk+1) − ∇Ω(πk)

⇔ −qk = ∇Jk(πk+1) + ∇Ω(πk) − ∇Ω(πk+1).

Injecting Eq. (8) into Eq. (7), we get

(cid:104)−qk, π(cid:105) = (cid:104)−qk, πk+1(cid:105) + (cid:104)∇Jk(πk+1) + ∇Ω(πk) − ∇Ω(πk+1), π − πk+1(cid:105)

= (cid:104)−qk, πk+1(cid:105) + (cid:104)∇Jk(πk+1), π − πk+1(cid:105)
(cid:125)

(cid:124)

(cid:123)(cid:122)
≥−(cid:15)(cid:48)

k+1

+ (cid:104)∇Ω(πk) − ∇Ω(πk+1), π − πk+1(cid:105)
(cid:125)
(cid:123)(cid:122)
three-point identity (Lemma 3)

(cid:124)

≥ (cid:104)−qk, πk+1(cid:105) − (cid:15)(cid:48)

k+1 + DΩ(π||πk+1) − DΩ(π||πk) + DΩ(πk+1||πk)

⇔ Tπvk ≤ Tπk+1vk + (cid:15)(cid:48)

k+1 + DΩ(π||πk) − DΩ(π||πk+1) − DΩ(πk+1||πk),

(7)

(8)

(9)

where we used in the last inequality the fact that (cid:104)qk, π(cid:105) = Tπvk. From the deﬁnition of the regularized Bellman operator,
we have that Tπk+1vk − DΩ(πk+1||πk) = Tπk+1,Ωπk

vk, so Eq. (9) is equivalent to the second result of Lemma 1:

As the Bregman divergence is positive, −DΩ(π||πk+1) ≤ 0, and thus Eq. (9) implies the last result of Lemma 1:

Tπvk ≤ Tπk+1,Ωπk

vk + (cid:15)(cid:48)

k+1 + DΩ(π||πk) − DΩ(π||πk+1).

Tπvk ≤ Tπk+1vk + (cid:15)(cid:48)

k+1 + DΩ(π||πk) − DΩ(π||πk+1).

This concludes the proof.

A Theory of Regularized MDPs

Next, we prove the bounds for bh

k, sh

k and dh

k, for h = 1, 2 (if the bounds are the same, the proofs differ).

Proof of Lemma 2. We start by bounding the quantities for MD-MPI type 1. First, we consider the Bellman residual:

vk

vk + (cid:15)(cid:48)

b1
k = vk − Tπk+1,Ωπk
(a)
= vk − Tπk vk + Tπk,Ωπk
(b)
≤ vk − Tπk,Ωπk−1
= vk − (cid:15)k − Tπk,Ωπk−1
(c)
= (vk − (cid:15)k) − Tπk,Ωπk−1
(d)
= (Tπk,Ωπk−1
= (Tπk,Ωπk−1
= (γPπk )m(vk−1 − Tπk,Ωπk−1
= (γPπk )mb1

k−1 + xk.

vk − Tπk+1,Ωπk

vk

k+1
vk + γPπk (cid:15)k + (cid:15)k − γPπk (cid:15)k + (cid:15)(cid:48)

k+1

(vk − (cid:15)k) + xk

)mvk−1 − Tπk,Ωπk−1
)mvk−1 − (Tπk,Ωπk−1

(Tπk,Ωπk−1
)mTπk,Ωπk−1

)mvk−1 + xk

vk−1 + xk

vk−1) + xk

In the previous equations, we used the following facts:

(a) Tπk vk = Tπk,Ωπk

vk as Ωπk (πk) = 0.

(b) We used two facts. First, Tπk vk ≥ Tπk vk − Ωπk−1 (πk) = Tπk,Ωπk−1
k+1.

vk − Tπk+1,Ωπk

Tπk,Ωπk

vk ≤ (cid:15)(cid:48)

vk, as Ωπk−1(πk) ≥ 0. Second, by Lemma 1,

(c) We used two facts. First, generally speaking, we have Tπ,Ω(v1 + v2) = Tπ,Ωv1 + γPπv2 (as Tπ,Ω is afﬁne). Second, by

deﬁnition xk = (I − γPπk )(cid:15)k + (cid:15)(cid:48)

k+1.

(d) By deﬁnition, vk = (Tπk,Ωπk−1

)mvk−1 + (cid:15)k.

Next, we bound the shift s1
k:

)mvk−1 − vπk
)mvk−1 − vπk,Ωπk−1

s1
k = (Tπk,Ωπk−1
= (Tπk,Ωπk−1
(a)
≤ (Tπk,Ωπk−1
(b)
= (Tπk,Ωπk−1
= (Tπk,Ωπk−1
= (γPπk )m(vk−1 − (Tπk,Ωπk−1
∞
(cid:88)

)mvk−1 − vπk,Ωπk−1
)mvk−1 − (Tπk,Ωπk−1
)mvk−1 − (Tπk,Ωπk−1

+ vπk,Ωπk−1

− vπk

)∞vk−1
)m(Tπk,Ωπk−1

)∞vk−1

)∞vk−1)

= (γPπk )m

((Tπk,Ωπk−1

)jvk−1 − (Tπk,Ωπk−1

)j+1vk−1)

j=0
∞
(cid:88)

((Tπk,Ωπk−1

)jvk−1 − (Tπk,Ωπk−1

)jTπk,Ωπk−1

vk−1)

j=0
∞
(cid:88)

(γPπk )j(vk−1 − Tπk,Ωπk−1

vk−1)

= (γPπk )m

= (γPπk )m

j=0
= (γPπk )m(I − γPπk )−1b1

k−1.

In the previous equations, we used the following facts:

A Theory of Regularized MDPs

(a) Generally speaking, if Ω ≥ 0 then vπ,Ω − vπ ≤ 0 (see Prop. 3).
(b) With a slight abuse of notation, for any v ∈ RS , vπ,Ω = T ∞

π,Ωv.

Then we bound the distance d1
k:
d1
k+1 = v∗ − (Tπk+1,Ωπk

)mvk

= Tπ∗ v∗ − Tπ∗ vk + Tπ∗ vk − Tπk+1,Ωπk
= γPπ∗ (v∗ − vk) + Tπ∗ vk − Tπk+1,Ωπk
(a)
≤ γPπ∗ (v∗ − vk) + (cid:15)(cid:48)

vk + Tπk+1,Ωπk
vk + Tπk+1,Ωπk

vk − (Tπk+1,Ωπk
vk − (Tπk+1,Ωπk

)mvk
)mvk

k+1 + δk(π∗) + Tπk+1,Ωπk

vk − (Tπk+1,Ωπk

)mvk

(b)
= γPπ∗ (v∗ − vk) + (cid:15)(cid:48)

k+1 + δk(π∗) +

m−1
(cid:88)

(γPπk+1)jb1
k

j=1

= γPπ∗ (v∗ − vk) + γPπ∗ (cid:15)k − γPπ∗ (cid:15)k + (cid:15)(cid:48)

k+1 + δk(π∗) +

m−1
(cid:88)

(γPπk+1)jb1
k

j=1

(c)
= γPπ∗ d1

k + yk + δk(π∗) +

m−1
(cid:88)

(γPπk+1)jb1
k.

j=1

In the previous equations, we used the following facts:

(a) By Lemma 1,

(b) For this step, we used:

Tπ∗ vk − Tπk+1,Ωπk

vk ≤ (cid:15)(cid:48)

k+1 + δk(π∗).

Tπk+1,Ωπk

vk − (Tπk+1,Ωπk

)mvk =

=

=

=

m−1
(cid:88)

((Tπk+1,Ωπk

j=1

m−1
(cid:88)

((Tπk+1,Ωπk

j=1

)jvk − (Tπk+1,Ωπk

)j+1vk)

)jvk − (Tπk+1,Ωπk

)jTπk+1,Ωπk

vk)

m−1
(cid:88)

(γPπk+1 )j(vk − Tπk+1,Ωπk

vk)

j=1

m−1
(cid:88)

(γPπk+1)jb1
k

j=1

(c) By deﬁnition of d1

k = v∗ − (vk − (cid:15)k) and yk = −γPπ∗ (cid:15)k + (cid:15)(cid:48)

k+1.

The proofs for the quantities involved in MD-MPI type 2 are similar, even if their deﬁnition differ. First, we consider the
Bellman residual
b2
k = vk − Tπk+1 vk

vk

vk − Tπk+1,Ωπk

(a)
≤ vk − Tπk+1 vk + Ωπk (πk+1) = vk − Tπk+1,Ωπk
(b)
= vk − Tπk vk + Tπk,Ωπk
(c)
≤ vk − Tπk vk + (cid:15)(cid:48)
= vk − (cid:15)k − Tπk vk + γPπk (cid:15)k + (cid:15)k − γPπk (cid:15)k + (cid:15)(cid:48)
(d)
= (vk − (cid:15)k) − Tπk vk(vk − (cid:15)k) + xk
(e)
= (Tπk )mvk−1 − Tπk (Tπk )mvk−1 + xk = (γPπk )m(vk−1 − Tπk vk−1) + xk = (γPπk )mb2

k+1

k+1

vk

k−1 + xk.

A Theory of Regularized MDPs

In the previous equations, we used the following facts:

(a) This is because Ωπk (πk+1) ≥ 0 and by deﬁnition of Tπ,Ωv = Tπv − Ω(π).

(b) It uses the fact that Tπk vk = Tπk,Ωπk

vk (as Ωπk (πk) = 0).

(c) This is by Lemma 1.

(d) This is by deﬁnition of xk = (cid:15)k − γPπk (cid:15)k + (cid:15)(cid:48)

k+1.

(e) This is by deﬁnition of vk = (Tπk )mvk−1 + (cid:15)k.

Then, we bound the shift s2

k, the technique being the same as before:

k = (Tπk )mvk−1 − vπk
s2

= (Tπk )mvk−1 − (Tπk )m+∞vk−1

= (γPπk )m

= (γPπk )m

∞
(cid:88)

j=0
∞
(cid:88)

((Tπk )jvk−1 − (Tπk )j+1vk−1)

(γPπk )j(vk−1 − Tπk vk−1)

j=0
= (γPπk )m(I − γPπk )−1b2

k−1.

To ﬁnish with, we prove the bound on the distance

d2
k+1 = v∗ − (Tπk+1)mvk
= Tπ∗ v∗ − Tπ∗ vk
(cid:125)
(cid:123)(cid:122)
(cid:124)
=γPπ∗ (v∗−vk)

+ Tπ∗ vk − Tπk+1vk
(cid:125)

(cid:123)(cid:122)
(cid:124)
≤(cid:15)(cid:48)
k+1+δk(π∗) by Lemma 1

+ Tπk+1vk − (Tπk+1 vk)m
(cid:125)
(cid:123)(cid:122)

(cid:124)

=(cid:80)m−1

j=1 (γPπk+1 )j b2

k

= γPπ∗ (v∗ − vk) + γPπ∗ (cid:15)k
(cid:124)
(cid:125)
(cid:123)(cid:122)
=γPπ∗ (v∗−(vk−(cid:15)k))=γPπ∗ d2
k

−γPπ∗ (cid:15)k + (cid:15)(cid:48)
(cid:123)(cid:122)
(cid:124)
=yk

k+1
(cid:125)

+δk(π∗) +

= γPπ∗ d2

k + yk +

m−1
(cid:88)

j=1

(γPπk+1)jb2

k + δk(π∗).

m−1
(cid:88)

(γPπk+1 )jb2
k

j=1

Now, we will show the component-wise bound on the regret Lk of Thm. 4

Proof of Theorem 4. The proof is similar to the one of AMPI (Scherrer et al., 2015, Lemma 2), up to the additional term
δk(π∗) and to the different bounded quantity. We will make use of the notation Γ, deﬁned in Def. 4, and we will write Γ∗ if
only the stochastic kernel induced by the optimal policy π∗ is involved. In other words, we write Γj

∗ = (γPπ∗ )j.

From Lemma 2, we have that

k+1 ≤ Γ∗dh
dh

k + yk +

m−1
(cid:88)

j=1

Γjbh

k + δk(π∗).

As the bound is the same for h = 1, 2, we remove the upperscript (and reintroduce it only when necessary, that is when
going back to the core deﬁnition of these quantities). By direct induction, we get

dk ≤

k−1
(cid:88)

j=0

(cid:32)

Γk−1−j

∗

yj +

m−1
(cid:88)

l=1

(cid:33)

Γlbj + δj(π∗)

+ Γkd0.

Therefore, the loss lk can be bounded as (deﬁning Lk at the same time)

A Theory of Regularized MDPs

lk = dk + sk ≤

Γk−1−j

∗

(yj +

k−1
(cid:88)

j=0
(cid:124)

m−1
(cid:88)

l=1
(cid:123)(cid:122)
=Lk

Γlbj) + Γkd0 + sk

+

k−1
(cid:88)

j=0

Γk−1−j

∗

δj(π∗).

(cid:125)

The loss Lk is exactly of the same form as the one of AMPI, we’ll take advantage of this later. From this bound on the loss
lk, we can bound the regret as:

Lk =

K
(cid:88)

k=1

lk ≤

K
(cid:88)

k=1

Lk +

K
(cid:88)

k−1
(cid:88)

k=1

j=0

Γk−1−j

∗

δj(π∗).

(10)

We will ﬁrst work on the last double sum. For this, we deﬁne ∆j(π∗) = (cid:80)j

k=0 δk(π∗). We have

K
(cid:88)

k−1
(cid:88)

k=1

j=0

Γk−1−j

∗

δj(π∗) =

=

=

=

K−1
(cid:88)

k
(cid:88)

k=0

j=0

K−1
(cid:88)

k
(cid:88)

Γk−j
∗

δj(π∗)

Γj

∗δk−j(π∗)

k=0

j=0

K−1
(cid:88)

K−1
(cid:88)

j=0

k=j

Γj

∗δk−j(π∗)

K−1
(cid:88)

Γj
∗

K−1−j
(cid:88)

j=0

k=0

δk(π∗) =

K−1
(cid:88)

j=0

Γj

∗∆K−1−j(π∗).

For the last line, we used the fact that Γ∗ only involves the Pπ∗ transition kernel, that does not depend on any iteration. Now,
we can bound the term ∆k(π∗), for any k ≥ 0 as follows. Let write RΩπ0

= (cid:107) supπ DΩ(π||π0)(cid:107)∞, we have

∆k(π∗) =

k
(cid:88)

j=0

δj(π∗) =

k
(cid:88)

(DΩ(π∗||πj) − DΩ(π∗||πj+1)) = DΩ(π∗||π0) − DΩ(π∗||πk+1) ≤ DΩ(π∗||π0) ≤ RΩπ0

1.

j=0

Given the deﬁnition of Γ, we have that Γj1 = γj1, so

K
(cid:88)

k−1
(cid:88)

k=1

j=0

Γk−1−j

∗

δj(π∗) ≤

K−1
(cid:88)

j=0

Γj
∗RΩπ0

1 =

K−1
(cid:88)

j=0

γj1RΩπ0

=

1 − γK
1 − γ

RΩπ0

1.

(11)

Next, we work on the term Lk. As stated before, it is exactly the same as the one of the AMPI analysis, and the proof
of Scherrer et al. (2015, Lemma 2) applies almost readily, we do not repeat it fully here. The only difference that appears
and that induces a slight modiﬁcation of the bound (on Lk)) is how b0 and d0 are related, that will modify the ηk term of the
original proof. We will link b0 and d0 for both types of MD-MPI.

For MD-MPI type 1 (with the natural convention that (cid:15)0 = 0), we have that

b1
0 = v0 − Tπ1,Ωπ0

v0 and d1

0 = v∗ − (v0 − (cid:15)0) = v∗ − v0.

The Bellman residual can be written as

b1
0 = v0 − Tπ1,Ωπ0

v0

(cid:124)

= v0 − v∗ + Tπ∗ v∗ − Tπ∗ v0
(cid:125)
(cid:123)(cid:122)
=(I−γPπ∗ )(−d1
0)
0) + (cid:15)(cid:48)
0) + (cid:15)(cid:48)

≤ (I − γPπ∗ )(−d1
≤ (I − γPπ∗ )(−d1

1 + DΩ(π∗||π0)
1 + RΩπ0

1,

+ Tπ∗ v0 − Tπ1,Ωπ0

v0
(cid:125)
(cid:123)(cid:122)
1+δ0(π∗) by Lemma 1

(cid:124)
≤(cid:15)(cid:48)

where we used in the penultimate line the fact that δ0(π∗) = DΩ(π∗||π0) − DΩ(π∗||π1) ≤ DΩ(π∗||π0) and in the last line
the same bounding as before. So, the link between b1

0 is the same as for AMPI, up to the additional RΩπ0

0 and d1

1 term.

A Theory of Regularized MDPs

For MD-MPI type 2, we have (with the same convention)

0 = v0 − Tπ1 v0 and d2
b2

0 = v∗ − (v0 − (cid:15)0) = v∗ − v0.

Working on the Bellman residual

b2
0 = v0 − Tπ1v0

(cid:124)

= v0 − v∗ + Tπ∗ v∗ − Tπ∗ v0
(cid:125)
(cid:123)(cid:122)
=(I−γPπ∗ )(−d2
0)
0) + (cid:15)(cid:48)

≤ (I − γPπ∗ )(−d2

+ Tπ∗ v0 − Tπ1v0
(cid:125)

(cid:123)(cid:122)
(cid:124)
≤(cid:15)(cid:48)
1+δ0(π∗) by Lemma 1
1 + DΩ(π∗||π0) ≤ (I − γPπ∗ )(−d2

0) + (cid:15)(cid:48)

1 + RΩπ0

1.

So, we have the same bound.

Combining this with the part of the proof that does not change, and that we do not repeat, we get the following bound on Lk:

Lk ≤ 2

k−1
(cid:88)

∞
(cid:88)

i=1

j=i

Γj|(cid:15)k−i| +

k−1
(cid:88)

∞
(cid:88)

i=0

j=i

Γj|(cid:15)(cid:48)

k−i| + h(k) +

∞
(cid:88)

Γj1RΩπ0

,

j=k
(cid:124)
= γk

(cid:123)(cid:122)
1−γ 1RΩπ0

(cid:125)

(12)

with h(k) being deﬁned as

h(k) = 2

∞
(cid:88)

j=k

Γj|d0| or h(k) = 2

∞
(cid:88)

j=k

Γj|b0|.

Combining Eqs (11) and (12) into Eq. (10), we can bound the regret:

K
(cid:88)

k=1

K
(cid:88)

LK ≤

≤

Lk +

K
(cid:88)

k−1
(cid:88)

k=1

j=0

Γk−1−j

∗

δj(π∗)



2

k−1
(cid:88)

∞
(cid:88)

Γj|(cid:15)k−i| +

k=1

i=1

j=i

k−1
(cid:88)

∞
(cid:88)

i=0

j=i

Γj|(cid:15)(cid:48)

k−i| + h(k) +



1RΩπ0

 +

γk
1 − γ

1 − γK
1 − γ

RΩπ0

1

K
(cid:88)

k−1
(cid:88)

∞
(cid:88)

= 2

Γj|(cid:15)k−i| +

K
(cid:88)

k−1
(cid:88)

∞
(cid:88)

Γj|(cid:15)(cid:48)

k−i| +

k=2

i=1

j=i

k=1

i=0

j=i

K
(cid:88)

k=1

h(k) +

1 − γK
(1 − γ)2 RΩπ0

1.

This concludes the proof.

To prove Cor. 2, we will need a result from Scherrer et al. (2015), that we recall ﬁrst.
Lemma 4 (Lemma 6 of Scherrer et al. (2015)). Let I and (Ji)i∈I be sets of non-negative integers, {I1, . . . , In} be a
partition of I, and f and (gi)i∈I be functions satisfying

(cid:88)

(cid:88)

|f | ≤

Γj|gi| =

n
(cid:88)

(cid:88)

(cid:88)

Γj|gi|.

i∈I

j∈Ji

l=1

i∈Il

j∈Ji

Then, for all p, q such that 1

p + 1

q = 1 and for all distributions ρ and µ, we have

(cid:107)f (cid:107)p,ρ ≤

n
(cid:88)

l=1

(Cq(l))

1
p sup
i∈Il

(cid:107)gi(cid:107)pq(cid:48),µ

(cid:88)

(cid:88)

γj

i∈Il

j∈Ji

with the following concentrability coefﬁcients,

Cq(l) =

(cid:80)

(cid:80)

i∈Il
(cid:80)

j∈Ji
(cid:80)

i∈Il

j∈Ji

γjcq(j)
γj

, where cq(j) = max
π1,...,πj

(cid:13)
(cid:13)
(cid:13)
(cid:13)

ρPπ1Pπ2 . . . Pπj
µ

(cid:13)
(cid:13)
(cid:13)
(cid:13)q,µ

.

Now, we can prove the stated result.

A Theory of Regularized MDPs

Proof of Corollary 2. The proof is an application of Lemma 4 to Thm. 4. We deﬁne I = {1, 2, . . . , K 2 + K + 1} and the
associated trivial partition (that is, Ii = {i}). For each i ∈ I we deﬁne

gi =

and Ji =










2(cid:15)k−i(cid:48)
(cid:15)(cid:48)
k−i(cid:48)
2d0 or 2b0
1−γK
(1−γ)2 RΩπ0
{i(cid:48), . . . }
{i(cid:48), . . . }
{k, . . . }
{0}

1

if i = i(cid:48) + (k−2)(k−1)
2
if i = i(cid:48) + k(k−1)
if i = K 2 + k with 1 ≤ k ≤ K
if i = K 2 + K + 1

2 + K(K−1)

2

with 2 ≤ k ≤ K and 1 ≤ i(cid:48) ≤ k − 1

+ 1 with 1 ≤ k ≤ K and 0 ≤ i(cid:48) ≤ k − 1

if i = i(cid:48) + (k−2)(k−1)
2
if i = i(cid:48) + k(k−1)
if i = K 2 + k with 1 ≤ k ≤ K
if i = K 2 + K + 1

2 + K(K−1)

2

with 2 ≤ k ≤ K and 1 ≤ i(cid:48) ≤ k − 1

+ 1 with 1 ≤ k ≤ K and 0 ≤ i(cid:48) ≤ k − 1

.

With this, Thm. 4 rewrites as

K2+K+1
(cid:88)

(cid:88)

(cid:88)

Γj|gi|.

LK ≤

l=1

i∈Il

j∈Ji

The results follows by applying Lemma 4 and using the fact that (cid:80)

j≥i γj = γi

1−γ

The proof of Prop. 4 is a basic application of the H¨older inequality.

Proof of Proposition 4. We write ◦ the Hadamard product. Recall that lk = v∗ − vπk ≥ 0 and that LK = (cid:80)K
that the sequence vπk is not necessarily monotone, even without approximation error. On one side, we have that

k=1 lk. Notice

(cid:107)LK(cid:107)1,ρ = ρLK =

K
(cid:88)

k=1

ρlk ≥ K min
1≤k≤K

ρlk = K min
1≤k≤K

(cid:107)lk(cid:107)1,ρ.

On the other side, with q such that 1

p + 1

q = 1 we have that

ρLK = (cid:104)ρ

1

p + 1

q , Lk(cid:105) = (cid:104)ρ

1
q , ρ

1

p ◦ LK(cid:105) ≤ (cid:107)ρ

1

q (cid:107)q(cid:107)ρ

1

p ◦ LK(cid:107)p = (cid:107)LK(cid:107)p,ρ,

where we used the H¨older inequality. Combining both equations provides the stated result.

Cor. 3 is a direct consequence of Cor. 2.

Proof of Corollary 3. Taking the limit of Cor. 2 as p → ∞, when the errors are null, gives

(cid:107)LK(cid:107)∞ ≤ 2

K
(cid:88)

k=1

γk
1 − γ

min((cid:107)d0(cid:107)∞, (cid:107)b0(cid:107)∞) +

1 − γK
(1 − γ)2 RΩπ0

≤ 2γ

1 − γk
(1 − γ)2 (cid:107)d0(cid:107)∞ +

1 − γK
(1 − γ)2 RΩπ0

,

where we used the fact that (cid:80)K

k=1 γk = γ 1−γK

1−γ . The result follows by grouping terms and using d0 = v∗ − v0.

The proof of Cor. 4 is mainly a manipulation of sums.

Proof of Corollary 4. From Cor. 2, we have that

(cid:107)LK(cid:107)p,ρ ≤ 2

K
(cid:88)

k−1
(cid:88)

k=2

i=1

γi
1 − γ

(C i
q)

1

p (cid:107)(cid:15)k−i(cid:107)pq(cid:48),µ +

K
(cid:88)

k−1
(cid:88)

k=1

i=0

γi
1 − γ

(C i
q)

1

p (cid:107)(cid:15)(cid:48)

k−i(cid:107)pq(cid:48),µ. + g(k) +

1 − γK
(1 − γ)2 RΩπ0

.

A Theory of Regularized MDPs

We only need to work on the ﬁrst two sums. To shorten the notations, write αi = γi
i = (cid:107)(cid:15)(cid:48)
β(cid:48)

k−i(cid:107)pq(cid:48),µ. We have:

1−γ (C i
q)

1

p , βi = (cid:107)(cid:15)k−i(cid:107)pq(cid:48),µ and

K
(cid:88)

k−1
(cid:88)

k=2

i=1

αiβk−i +

K
(cid:88)

k−1
(cid:88)

k=1

i=0

αiβ(cid:48)

k−i =

K−1
(cid:88)

k
(cid:88)

k=1

i=1

αiβk+1−i +

K−1
(cid:88)

k
(cid:88)

k=0

i=0

αiβ(cid:48)

k+1−i

K−1
(cid:88)

i=1

K−1
(cid:88)

αi

αi

K−1
(cid:88)

k=i

K−i
(cid:88)

=

=

βk+1−i +

K−1
(cid:88)

αi

K−1
(cid:88)

i=0

k=i

β(cid:48)
k+1−i

K−1
(cid:88)

αi

K−i
(cid:88)

β(cid:48)
k.

βk +

i=1

k=1

i=0

k=1

The result follows by reinjecting this in Cor. 2 after having replaced αi, βi and β(cid:48)
i.

The proof of Cor. 5 basically consists in the application of Lemma 4 to a rewriting of Thm. 4.

Proof of Corollary 5. By Thm. 4, we have

K
(cid:88)

k−1
(cid:88)

∞
(cid:88)

LK ≤ 2

Γj|(cid:15)k−i| +

K
(cid:88)

k−1
(cid:88)

∞
(cid:88)

Γj|(cid:15)(cid:48)

k−i| +

k=2

i=1

j=i

k=1

i=0

j=i

K
(cid:88)

k=1

h(k) +

1 − γK
(1 − γ)2 RΩπ0

1.

(13)

We start by rewriting the two ﬁrst sums. For the ﬁrst one, we have

K
(cid:88)

k−1
(cid:88)

∞
(cid:88)

k=2

i=1

j=i

Γj|(cid:15)k−i| =

K
(cid:88)

k−1
(cid:88)

∞
(cid:88)

Γj|(cid:15)i|

k=2

i=1

j=k−i

K−1
(cid:88)

k
(cid:88)

∞
(cid:88)

Γj|(cid:15)i|

=

=

j=k+1−i

i=1


k=1

K−1
(cid:88)

K−1
(cid:88)

∞
(cid:88)



Γj

 |(cid:15)i|







i=1

k=i

j=k+1−i


K−1
(cid:88)

=

K−i
(cid:88)

∞
(cid:88)

Γj

 |(cid:15)i|

i=1

k=1

j=k

K−1
(cid:88)

=





i
(cid:88)

∞
(cid:88)

i=1

k=1

j=k



Γj

 |(cid:15)K−i|.

Similarly, we have that

K
(cid:88)

k−1
(cid:88)

∞
(cid:88)

Γj|(cid:15)(cid:48)

k−i| =

K−1
(cid:88)





i
(cid:88)

∞
(cid:88)


 |(cid:15)(cid:48)

k−i|.

Γj

k=1

i=0

j=i

i=0

k=0

j=k

Thus, the bound on the loss can be writen as

LK ≤ 2

K−1
(cid:88)





i
(cid:88)

∞
(cid:88)

i=1

k=1

j=k



Γj

 |(cid:15)K−i| +

K−1
(cid:88)





i
(cid:88)

∞
(cid:88)

Γj

i=0

k=0

j=k


 |(cid:15)(cid:48)

k−i| + 2





K
(cid:88)

∞
(cid:88)

k=1

j=k



Γj

 (|d0| or |b0|) +

1 − γK
(1 − γ)2 RΩπ0

1.

In order to apply Lemma 4 to this bound, we consider I = {1, 2, . . . 2K + 1} and the associated trivial partition Ii = {i}.

For each i ∈ I, we deﬁne:

A Theory of Regularized MDPs

if 1 ≤ i ≤ K − 1
if K ≤ i ≤ 2K − 1
if i = 2K
if i = 2K + 1

gi =

and Ji =



2(cid:15)K−i

(cid:15)(cid:48)
2K−i
2|d0| or 2|b0|

1−γK
(1−γ)2 RΩπ0
1

(cid:83)i

(cid:83)i−K
(cid:83)K

{0}

k=1{k, k + 1, . . . }
k=0 {k, k + 1, . . . }
k=1{k, k + 1, . . . }

if 1 ≤ i ≤ K − 1
if K ≤ i ≤ 2K − 1
if i = 2K
if i = 2K + 1

.

With this, Eq. (13) rewrites as

2K+1
(cid:88)

(cid:88)

(cid:88)

Γj|gi|.

LK ≤

i∈Il
With the cq term deﬁned in Lemma 4, using the fact that (cid:80)k−1
coefﬁcient,

l=1

i=l

j∈Ji

(cid:80)∞

j=i γj = γl−γk

(1−γ)2 , as well as the following concentrability

we get the following bound on the regret:

C l,k

q =

(1 − γ)2
γl − γk

k−1
(cid:88)

∞
(cid:88)

i=l

j=i

cq(j),

(cid:107)LK(cid:107)p,ρ ≤ 2

K−1
(cid:88)

γ − γi+1
(1 − γ)2 (C 1,i+1

q

i=1
γ − γK+1
(1 − γ)2 (C 1,K+1

q

+

1

1

p (cid:107)(cid:15)k−i(cid:107)pq(cid:48),µ +

)

K−1
(cid:88)

i=0

1

q

)

p (cid:107)(cid:15)(cid:48)

1 − γi+1
(1 − γ)2 (C 0,i+1
1 − γK
(1 − γ)2 RΩπ0

.

k−i(cid:107)pq(cid:48),µ

)

p min((cid:107)d0(cid:107)pq(cid:48),µ, (cid:107)b0(cid:107)pq(cid:48),µ) +

D. Perspectives on regularized MDPs

Here, we discuss in more details the perspectives brieﬂy mentioned in Sec. 6.

D.1. Dynamic programming and optimization

We have shown how MPI regularized by a Bregman divergence is related to Mirror Descent. The computation of the
regularized greedy policy is similar to a mirror descent step, where the q-function plays the role of the negative subgradient.
In this sense, the policy lives in the primal space while the q-function lives in the dual space. It would be interesting to take
inspiration from proximal convex optimization to derive new dynamic programming approaches, for example based on Dual
Averaging (Nesterov, 2009) or Mirror Prox (Nemirovski, 2004).

For example, consider the case of a ﬁxed regularizer. We have seen that it leads to a different solution than the original one
(see Thm. 2). Usually, one considers a scaled negative entropy as such a regularizer5, Ω(π(·|s)) = α (cid:80)
a π(a|s) ln π(a|s).
The choice of this parameter is important practically and problem-dependent: too high and the solution of the regularized
problem will be very different from the original one, too low and the algorithm will not beneﬁt from the regularization. A
natural idea is to vary the weight of the regularizer over iterations (Peters et al., 2010; Abdolmaleki et al., 2018a; Haarnoja
et al., 2018b), much like a learning rate in a gradient descent approach.

More formally, in our framework, write Ωk = αkΩ and consider the following weighted reg-MPI scheme,

(cid:40)

(cid:15)(cid:48)
k+1
Ωk

(vk)
πk+1 = G
vk+1 = (Tπk+1,Ωk )mvk + (cid:15)k+1

,

5This is the case for SAC or soft Q-learning, for example. Sometimes, it is the reward that is scaled, but both are equivalent.

A Theory of Regularized MDPs

(cid:15)(cid:48)
k+1
Ωk

(vk) as deﬁned in Sec. 4.2: for any policy π, Tπ,Ωk vk ≤ Tπk+1,Ωk vk + (cid:15)(cid:48)

with G
developed previously, one can obtain easily the following result.
Theorem 5. Deﬁne RΩ = (cid:107) supπ Ω(π)(cid:107)∞. Assume that the series (αk)k≥0 is positive and decreasing, and that the
regularizer Ω is positive (without loss of generality). After K iterations of the preceding weighted reg-MPI scheme, the
regret satisﬁes

k+1. By applying the proof techniques

K
(cid:88)

k−1
(cid:88)

∞
(cid:88)

LK ≤ 2

Γj|(cid:15)k−i| +

K
(cid:88)

k−1
(cid:88)

∞
(cid:88)

Γj|(cid:15)(cid:48)

k−i| +

k=2

i=1

j=i

k=1

i=0

j=i

K
(cid:88)

k=1

h(k) +

1 − γK
(1 − γ)2 RΩ

K−1
(cid:88)

k=0

αk1.

with h(k) = 2 (cid:80)∞

j=k Γj|d0| or h(k) = 2 (cid:80)∞

j=k Γj|b0|.

Proof. The proof is similar to the one of MD-MPI, as it bounds the regret, but also simpler as it does not require Lemma 1.
The principle is still to bound the distance dk and the shift sk, that both require bounding the Bellman residual bk. From this,
one can bound the loss lk = dk + sk, and then the regret LK = (cid:80)K
k=1 lk. We only specify what changes compared to the
previous proofs.

Bounding the shift sk is done as before, and one get the same bound:

sk = (Tπk,Ωk−1)mvk−1 − vπk ≤ (γPπk )m(I − γPπk )bk−1.

For the distance dk, we have

dk+1 = v∗ − (Tπk+1,Ωk )mvk
= Tπ∗ v∗ − Tπ∗ vk
(cid:125)
(cid:123)(cid:122)
(cid:124)
=γPπ∗ (v∗−vk)

+ Tπ∗ vk − Tπ∗,Ωk vk
(cid:123)(cid:122)
(cid:125)
=Ωk(π∗)≤αkRΩ1

(cid:124)

+ Tπ∗,Ωk vk − Tπk+1,Ωk vk
(cid:125)
(cid:123)(cid:122)
≤(cid:15)(cid:48)
k+1

(cid:124)

+Tπk+1,Ωk vk − (Tπk+1,Ωk )mvk.

The rest of the bounding is similar to MD-MPI, and we get

dk+1 ≤ γPπ∗ dk + yk + αkRΩ1 +

m−1
(cid:88)

(γPπk+1)jbk.

j=1

So, this is similar to the bound of MD-MPI, with the term αkRΩ1 replacing the term δk(π∗). For the Bellman residual, we
have

bk = vk − Tπk+1,Ωk vk = vk − Tπk,Ωk vk
(cid:125)
(cid:123)(cid:122)
(cid:124)
≤vk−Tπk ,Ωk−1 vk

+ Tπk,Ωk vk − Tπk+1,Ωk vk
(cid:123)(cid:122)
(cid:125)
(cid:15)(cid:48)
k+1

(cid:124)

where we used the facts that αk ≤ αk−1 and that Ω ≥ 0 for bounding the ﬁrst term:

−Tπk,Ωk vk = −Tπk vk + αkΩ(πk) ≤ −Tπk vk + αk−1Ω(πk) = −Tπk,Ωk−1vk.

The rest of the bounding is as before and gives bk ≤ (γPπk )mbk−1 + xk. From these bounds, one can bound Lk as
previously.

From Thm. 5, we can obtain an (cid:96)p-bound on the regret, and from this the rate of convergence of the average regret. For
MD-MPI, the rate of convergence was in O( 1
K ). Here, it depends on the weighting of the regularizer. For example, if αk is
in O( 1
). This
illustrates the kind of things that can be done with the proposed framework of regularized MDPs.

), then the average regret will be in O( 1√
K

k ), then the average regret will be in O( ln K

K ). If αk is in O( 1√

k

D.2. Temporal consistency equations

Thanks to Ω, the regularized greedy policies are unique, and thus is the regularized optimal policy. The pair of optimal
policy and optimal value function can be characterized as follows.
Corollary 6. The optimal policy and optimal value function in a regularized MDP are the unique functions satisfying

∀s ∈ S,

(cid:40)

v∗,Ω(s) = Ω∗ (cid:0)r(s, ·) + γEs(cid:48)|s,·[v∗,Ω(s(cid:48))](cid:1)
π∗,Ω(·|s) = ∇Ω∗ (cid:0)r(s, ·) + γEs(cid:48)|s,·[v∗,Ω(s(cid:48))](cid:1)

.

Proof. This is a direct consequence of Thm. 1

A Theory of Regularized MDPs

This provides a general way to estimate the optimal policy. For example, if Ω is the negative entropy, this set of equations
simpliﬁes to

∀(s, a) ∈ S × A v∗,Ω(s) = r(s, a) + γEs(cid:48)|s,a[v∗,Ω(s(cid:48))] − ln π∗,Ω(a|s).

This has been used by Nachum et al. (2017) or Dai et al. (2018), where it is called “temporal consistency equation”, to
estimate the optimal value-policy pair by minimizing the related residual,

Es,a

(cid:104)(cid:0)r(s, a) + γEs(cid:48)|s,a[v∗,Ω(s(cid:48))] − ln π∗,Ω(a|s) − v∗,Ω(s)(cid:1)2(cid:105)

.

This idea has also been extended to Tsallis entropy (Nachum et al., 2018). In this case, the set of equations does not simplify
as nicely as with the Shannon entropy. Instead, the approach consists in considering the Lagrangian derived from the
Legendre-Fenchel transform (and the resulting temporal consistency equation involves Lagrange multipliers, that have to
be learnt too). This idea could be extended to other regularizers. One could also replace the regularizer Ω by a Bregman
divergence, and estimate the optimal policy by solving a sequence of temporal consistency equations.

D.3. Regularized policy gradient

Policy search approaches often combine policy gradient with an entropic regularization, typically to prevent the policy from
becoming too quickly deterministic (Williams, 1992; Mnih et al., 2016). The policy gradient theorem (Sutton et al., 2000)
can easily be extended to the proposed framework.

Let ν be a (user-deﬁned) state distribution, the classical policy search approach consists in maximizing J(π) =
Es∼ν[vπ(s)] = νvπ. This principle can easily be extended to regularized MDPs, by maximizing

JΩ(π) = νvπ,Ω.

Write dν,π the γ-weighted occupancy measure induced by the policy π when the initial state is sampled from ν, deﬁned as
dν,π = (1 − γ)ν(I − γPπ)−1 ∈ ∆S . Slightly abusing notations, we’ll also write dν,π(s, a) = dν,π(s)π(a|s).
Theorem 6 (Policy gradient for regularized MDPs). The gradient of JΩ is

∇JΩ(π) =

1
1 − γ

Es,a∼dν,π

(cid:20)(cid:18)

qπ,Ω(s, a) −

(cid:19)

∂Ω(π(.|s))
∂π(a|s)

(cid:21)

∇ ln π(a|s)

.

Proof. We have that

∇JΩ(π) =

(cid:88)

s∈S

ν(s)∇vπ,Ω(s).

We have to study the gradient of the value function. For this, the nabla-log trick is useful: ∇π = π∇ ln π.

∇vπ,Ω(s) = ∇

(cid:32)

(cid:88)

a∈A

π(a|s)(r(s, a) + γEs(cid:48)|s,a[vπ,Ω(s(cid:48))]) − Ω(π(.|s))

(cid:33)

=

=

(cid:88)

a∈A
(cid:88)

a∈A

(cid:0)∇π(a|s)qπ,Ω(s, a) + π(a|s)γEs(cid:48)|s,a[∇vπ,Ω(s(cid:48))](cid:1) − ∇Ω(π(.|s))

π(a|s) (cid:0)qπ,Ω(s, a)∇ ln π(a|s) − ∇Ω(π(.|s)) + γEs(cid:48)|s,a[∇vπ,Ω(s(cid:48))](cid:1) .

So, the components of ∇vπ,Ω are the (unregularized) value functions corresponding to the rewards being the components of
qπ,Ω(s, a)∇ ln π(a|s) − ∇Ω(π(.|s)). Consequently,

JΩ(π) =

1
1 − γ

(cid:88)

s∈S

dν,π(s)

(cid:88)

a∈A

π(a|s)(qπ,Ω(s, a)∇ ln π(a|s) − ∇Ω(π(.|s))).

Using the chain-rule, we have that

A Theory of Regularized MDPs

∇Ω(π(.|s)) =

(cid:88)

a∈A

∂Ω(π(.|s))
∂π(a|s)

∇π(a|s) =

(cid:88)

a∈A

∂Ω(π(.|s))
∂π(a|s)

π(a|s)∇ ln π(a|s).

Injecting this in the previous result concludes the proof.

Even in the entropic case, it might be not exactly the same as the usual regularized policy gradient, notably because our
result involves the regularized q-function. Again, the regularizer Ω could be replaced by a Bregman divergence, to ultimately
estimate the optimal policy of the original MDP. It would be interesting to compare empirically the different resulting policy
gradients approaches, we left this for future work.

D.4. Regularized inverse reinforcement learning

Inverse reinforcement learning (IRL) consists in ﬁnding a reward function that explains the behavior of an expert which is
assumed to act optimally. It is often said that it is an ill-posed problem. The classical example is the null reward function
that explains any behavior (as all policies are optimal in this case).

We argue that in this regularized framework, the problem is not ill-posed, because the optimal policy is unique, thanks to the
regularization. For example, if one consider the negative entropy as the regularizer, with a null reward, the optimal policy
will be that of maximum entropy, so the uniform policy, and it is unique.

Notice that if for a reward, the associated regularized optimal policy is unique, the converse is not true. For example, the
uniform policy is optimal for any constant reward. More generally, reward shaping (Ng et al., 1999) still holds true for
regularized MDPs (this being thanks to the results of Sec. 3, again).

This being said, assume that the model (dynamic, discount factor, regularizer) and that the optimal regularized policy π∗,Ω
are known. It is possible to retrieve a reward function such that π∗,Ω is the unique optimal policy.
Proposition 5. Let ˆq ∈ RS×A be any function satisfying

then the reward ˆr(s, a) deﬁned as

∀s ∈ S,

π∗,Ω(·|s) = ∇Ω∗(ˆq(s, ·)),

∀(s, a) ∈ S × A,

ˆr(s, a) = ˆq(s, a) − γEs(cid:48)|s,a[Ω∗(ˆq(s, ·))]

= ˆq(s, a) − γEs(cid:48)|s,a[Ea(cid:48)∼π∗,Ω(·|s(cid:48))[ˆq(s, a)] − Ω(π∗,Ω(·|s))]

has π∗,Ω as the unique corresponding optimal policy.

Proof. First, recall (see Prop. 1) that if for any q, there is a unique regularized greedy policy, the converse is not true (simply
by the fact that for any v ∈ RS , we have ∇Ω∗(q(s, ·) + v(s)) = ∇Ω∗(q(s, ·))). By assumption and by uniqueness of
regularized greediness, π∗,Ω is the unique regularized policy corresponding to ˆq. Then, with the above deﬁned reward
function, ˆq is unique the solution of the regularized Bellman optimality equation. This shows the stated result.

If this result shows that IRL is well-deﬁned in a regularized framework, it is not very practical (for example, in the entropic
case, it tells that ˆr(s, a) = ln π∗,Ω(s, a) is such a reward function). Yet, we think that the proposed general framework could
lead to more practical algorithm.

For example, many IRL algorithms are based on the maximum-entropy principle, eg. (Ziebart et al., 2008; Finn et al., 2016;
Fu et al., 2018). This maximum-entropy IRL framework can be linked to probabilistic inference, that can itself be shown
to be equivalent, in some speciﬁc cases (deterministic dynamics), to entropy-regularized reinforcement learning (Levine,
2018). We think this to be an interesting connection, and maybe that our proposed regularized framework could allow to
generalize or analyze some of these approaches.

D.5. Regularized zero-sum Markov games

A Theory of Regularized MDPs

A zero-sum Markov game can be seen as a generalization of MDPs. It is a tuple {S, A1, A2, P, r, γ} with S the state
space common to both players, Aj the action space of player j, P ∈ ∆S×A1×A2
the transition kernel (P (s(cid:48)|s, a1, a2) is
the probability of transiting to state s(cid:48) when player 1 played a1 and player 2 played a2 in s), r ∈ RS×A1×A2
the reward
function of both players (one tries to maximize it, the other one to minimize it) and γ the discount factor. We write µ ∈ ∆S
A1
a policy of the maximizer, and ν ∈ ∆S

A2 a policy of the minimizer.
As in the case of classical MDPs, everything can be constructed from an evaluation operator, deﬁned as

S

[Tµ,νv](s) =

(cid:88)

(cid:88)

a1

a2

(cid:32)

µ(a1|s)ν(a2|s)

r(s, a1, a2) + γ

p(s(cid:48)|s, a1, a2)v(s(cid:48))

,

(cid:33)

(cid:88)

s(cid:48)

of ﬁxed point vµ,ν. From this, the following operators are deﬁned:

Tµv = min

ν

Tµ,νv

T v = max

µ

Tµv

Tνv = max

Tµ,νv

µ
ˆTv = min
ν

Tνv.

We also deﬁne the greedy operator as µ ∈ G(v) ⇔ T v = Tµv = minν Tµ,νv. Thanks to the Von Neumann’s minimax
theorem (Morgenstern & Von Neumann, 1953), we have that

and the optimal value function satisﬁes

ˆT v = T v

v∗ = min

ν

max
µ

vµ,ν = max

µ

min
ν

vµ,ν.

The modiﬁed policy iteration for this kind of games is

(cid:40)

µk+1 ∈ G(vk)
vk+1 = T m

µk+1

vk

.

(14)

As in MDPs, we can regularize the evaluation operator, and construct regularized zero-sum Markov games from this. Let Ω1
and Ω2 be two strongly convex reguralizers on ∆A1 and ∆A2 , and deﬁne the regularized evaluation operator as

[Tµ,ν,Ωv](s) = [Tµ,νv](s) − Ω1(µ(.|s)) + Ω2(ν(.|s)).

From this, we can construct a theory of regularized zero-sum Markov games as it was done for MDPs. The Von Neumann’s
minimax theorem does not only hold for afﬁne operators, but for convex-concave operators, so we’re ﬁne.

Notably, the unregularized error propagation analysis of (14) by Perolat et al. (2015) could be easily adapted to the
regularized case (much like how the analysis of AMPI directly led to the analysis of Sec. 4.2, thanks to the results of Sec. 3).
We left its extension to regularization with a Bregman divergence as future work.

