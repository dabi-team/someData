A tensorized logic programming language for large-scale data

Ryosuke Kojima1, Taisuke Sato2
1 Department of Biomedical Data Intelligence, Graduate School of Medicine,
Kyoto University, Kyoto, Japan.
2 AI research center (AIRC)
National Institute of Advanced Industrial Science and Technology (AIST), Tokyo, Japan.

9
1
0
2

n
a
J

0
2

]

G
L
.
s
c
[

1
v
8
4
5
8
0
.
1
0
9
1
:
v
i
X
r
a

Abstract

We introduce a new logic programming language T-PRISM
based on tensor embeddings. Our embedding scheme is
a modiﬁcation of the distribution semantics in PRISM,
one of the state-of-the-art probabilistic logic programming
languages, by replacing distribution functions with multi-
dimensional arrays, i.e., tensors. T-PRISM consists of two
parts: logic programming part and numerical computation
part. The former provides ﬂexible and interpretable model-
ing at the level of ﬁrst order logic, and the latter part provides
scalable computation utilizing parallelization and hardware
acceleration with GPUs. Combing these two parts provides
a remarkably wide range of high-level declarative modeling
from symbolic reasoning to deep learning.
To embody this programming language, we also introduce a
new semantics, termed tensorized semantics, which combines
the traditional least model semantics in logic programming
with the embeddings of tensors. In T-PRISM, we ﬁrst derive
a set of equations related to tensors from a given program
using logical inference, i.e., Prolog execution in a symbolic
space and then solve the derived equations in a continuous
space by TensorFlow.
Using our preliminary implementation of T-PRISM, we have
successfully dealt with a wide range of modeling. We have
succeeded in dealing with real large-scale data in the declar-
ative modeling. This paper presents a DistMult model for
knowledge graphs using the FB15k and WN18 datasets.

1. Introduction
Logic programming provides concise expressions of
knowledge and has been proposed as means for rep-
resenting and modeling various types of data for real-
world AI systems. For example,
to deal with uncer-
tain and noisy data, probabilistic logic programming
(PLP) has been extensively studied (Kimmig et al. 2011;
Wang, Mazaitis, and Cohen 2013; Sato and Kameya 2008).
PLP systems allow users to ﬂexibly and clearly describe
stochastic dependencies and relations between entities using
logic programming. Also in the different context, to han-
dle a wide range of applications, the uniﬁcation of neu-
ral networks and approximate reasoning by symbol em-
beddings into continuous spaces has recently been pro-

posed (Manhaeve et al. 2018; Rockt¨aschel and Riedel 2017;
Evans and Grefenstette 2018).

In this paper, we tackle a task of combining symbolic
reasoning and multi-dimensional continuous-space embed-
dings of logical constructs such as clauses, and explore a
new approach to compile a program written in a declarative
language into a procedure of numerical calculation suitable
for large-scale data. Such languages are expected to be in-
terpretable as a programming language while efﬁciently exe-
cutable in the level of numerical calculation like vector com-
putation. Aiming at this goal, we introduce tensorized se-
mantics, a novel algebraic semantics interfacing a symbolic
reasoning layer and the numeric computation layer, and pro-
pose a new modeling language “T-PRISM”. It is based on an
existing probabilistic logic programming language PRISM
(Sato and Kameya 2008) and implements the tensorized se-
mantics for large-scale datasets.

Thus the ﬁrst contribution of this paper is the introduc-
tion of a new semantics, tensorized semantics. The current
PRISM has the distribution semantics (Sato 1995) that prob-
abilistically generalizes the least model semantics in logic
programming to coherently assign probabilities to the log-
ical constructs. Likewise tensorized semantics assigns ten-
sors1 to the logical constructs based on the least model se-
mantics. Both of PRISM and T-PRISM programs are char-
acterized by a set of equations for the assigned quantities.

One may be able to view T-PRISM as one approach
to an equation-level interface to connect logic program-
ming and continuous-space embeddings. Another approach
is possible, predicate-level interface, such as DeepProblog
(Manhaeve et al. 2018). Their approach is to provide spe-
cial predicates connecting neural networks and probabilistic
logic programming by ProbLog. It implementationally sep-
arates neural networks from probabilistic models but syntac-
tically integrates, for example, image recognition and logi-
cal reasoning using estimated labels. This approach, unlike
our approach, does not allow constants and predicates to
have corresponding vectors (tensors) representations in neu-
ral networks.

The second contribution of this paper is an implementa-
tion methodology of the T-PRISM’s tensorized semantics.

Copyright c(cid:13) 2019, Association for the Advancement of Artiﬁcial
Intelligence (www.aaai.org). All rights reserved.

1 In this paper, the term “tensor” is used as a multi-dimensional

array interchangeably.

 
 
 
 
 
 
T-PRISM’s new semantics guides an internal data structure
to connect symbolic inference by Prolog execution and nu-
merical calculation by TensorFlow (Abadi et al. 2016). This
internal data structure represents tensor equations for ten-
sorized semantics and is an expanded PRISM’s explanation
graph, explanation graphs with Einstein’s notation, a set
of logical equations (bi-implications) that allows repeated
sum-product operations for a speciﬁed number of times,
called Einstein’s notation. This expansion is intended for
parallelization and enhance the applicability of T-PRISM to
large-scale data.

2. Tensorized semantics
T-PRISM’s semantics, “tensorized least model semantics”
or “tensorized semantics” for short, is a tensorized version of
existing PRISM semantics. In PRISM, the semantics assigns
probability masses to ground atoms in the Herbrand base of
a program whereas in T-PRISM, multi-dimensional arrays
are assigned to them.

Formally,

a T-PRISM program is
a ﬁve-tuple
hF, C, qF , TF , Ri. A primary part of
the program
DB = F ∪ C is a Prolog (deﬁnite clause) program
where F is a set of tensor atoms (see below), and C is a
set of deﬁnite clauses whose head contains no tensor atom.
Theoretically, we always equate a set of clauses with the
set of all ground instances and allow DB to be countably
inﬁnite.

In addition to this setting, we introduce a speciﬁc type of
atoms called tensor atoms represented by tensor/2 which
declare multi-dimensional arrays called embedded tensors.
A tensor atom has an index as a list of Prolog constants in the
second argument to access the entries of the declared multi-
dimensional array. For example, tensor(x, [i, j]) repre-
sents a tensor atom having a ground atom x and an index
[i,j]. It declares a second-order tensor (matrix) X spec-
iﬁed by x whose index (i, j) is expressed by [i,j]. To
make this correspondence mathematically rigorous, we de-
ﬁne a function qF (·) from tensor atoms to embedded tensors
such that qF (tensor(x, [i, j])) = Xi,j . In general, an in-
dex of an n-order tensor is represented as an n-length list
of Prolog constants. Note that the domain of qF (.) can be
extended to programs without tensor atoms. In such case,
qF (x) is deﬁned as one (a scalar value) when a ground atom
x is logically true in Prolog. This treatment is consistent with
PRISM in a part of vanilla Prolog.

In this paper, as already shown, we use italic fonts for
indices appearing in mathematical expressions to distinguish
them from the corresponding ones appearing in a program.
So for a matrix X speciﬁed by x, we write an entry Xi,j for
the index [i,j]. Also to extract indices, we deﬁne TF such
that, for instance, TF (tensor(x, [i, j])) = (i, j).

In the concrete numerical calculation, we have to give the
range of indices, i.e., the size of embedded tensors. A T-
PRISM program contains a function R(i) that determines
the range of an index [i] as 1, 2, ..., R(i). Thus, in the
above example, the size of a matrix Xi,j is determined as
R(i) × R(j).

Once a T-PRISM program is given, a set of tensor equa-
tions are determined from the program. Consider a clause

having a head H. Let H ← Wℓ(ℓ = 1, . . . , L) be an enu-
meration of clauses about H in the program. For each H,
logical equations2 of the following form, which holds in the
least model of the program, are derived.

H ⇔ W1 ∨ W2 ∨ ... ∨ WL

Wℓ

def
= B1ℓ ∧ B2ℓ ∧ ... ∧ BMℓℓ

(1)

where B∗ℓ is a ground atom in the body Wℓ.

The tensorized semantics is obtained by expanding the
mappings qF (·) and TF (·) over F respectively to q(·) and
T (·) for the entire Herbrand base. It assigns various orders
of tensors to atoms as their denotation. They are inductively
deﬁned, starting from tensor atoms, by applying two rules
below to Equation (1).

• Disjunction rule

Given a disjunction of tensor atoms, its embeddings is de-
ﬁned as the summation of the tensor atoms, A and B:

q(A ∨ B)

def
= q(A) + q(B).

• Conjunction rule

Given a conjunction of tensor atoms with associated
multi-dimensional arrays for embedding, embeddings of
the conjunction is deﬁned by the Einstein’s notation rule3.
In the case where subscripts overlap in the same term,
the rule is to take a sum for that subscript. Note the non-
overlap indices are not eliminated by this operation. We
call the overlapping index a dummy index whereas a non-
overlapping index as a free index. A set of free indices is
assigned to T (A ∧ B). For example, put T (A) = (i, j)
and T (B) = (j, k). T (A ∧ B) = (i, k) is computed
by this rule, and a matrix is embedded to q(A ∧ B). As
a result, its i, k-th element q(A ∧ B)ik is computed as
Pj q(A)ij · q(B)jk, i.e., this computation coincides with
matrix multiplication. When the index is empty, the re-
sulting value is a scalar (number) and the value of a con-
junction of scalar embedding atoms is the product of their
values.

By applying the disjunction and conjunction rules to
Equation (1) recursively, we can derive the following tensor
equations:

q(H) = q(W1) + q(W2) + ... + q(WL)
q(Wℓ) = einsumq,T (B1ℓ, B2ℓ, ..., BMℓℓ)

(2)

where einsumq,T is a sum-product operation indicated by
the Einstein’s notation in the conjunction rule. The derived
set of tensor equations has a least solution (as all coefﬁ-
cients are non-negative and all derived equations have up-
per bounds)(Sato 2017) and we consider it as the denotation
of the given T-PRISM program in the tensorized semantics.

2 Logical equivalence seems a more appropriate term, but we

use equation for intuitiveness.

3 Although a general formulation of Einstein’s notation distin-
guishes between covariance and contravariance, indicated by sub-
script and superscript indices respectively, their distinction is irrel-
evant to our semantics, and hence ignored in this paper.

In this paper, we assume Equation (2) is well-deﬁned, i.e.,
there is no mismatch of tensor dimensions.

Non-linear operations are prerequisites for a wider range
of applications including deep neural networks. To make
them available, T-PRISM provides operator/1 indicat-
ing a non-linear operation. This built-in predicate occurs in
an explanation graph similarly to tensor/2. The second
equation in Equation (2) is modiﬁed as follows:

q(Wℓ) =op1 ◦ op2 ◦ ... ◦ opOℓ

1: index_list(v(_),[[i]]).
2: index_list(r(_),[[i]]).
3: :-set_index_range(i,20).
4:
5: rel(S,R,O):-
6:
7:
8:

tensor(v(S),[i]),
tensor(v(O),[i]),
tensor(r(R),[i]).

◦ einsumq,T (B1ℓ, B2ℓ, ..., BMℓℓ).

(3)

Figure 1: simple DistMult program in T-PRISM

where op1, op2, ..., opOk are non-linear operations, B∗ℓ is
a ground atom in the body Wℓ except for non-linear op-
erations, and ◦ represents their function synthesis. Note
that nonlinear operations and function synthesis are non-
commutative. For example, let A and B be tensor atoms.
B ⇔ operator(f ) ∧ operator(g) ∧ A is interpreted as
q(B) = f ◦g◦q(A). While operator(f )∧operator(g)∧A
and operator(g) ∧ operator(f ) ∧ A are logically equiva-
lent, they are interpreted as different equations: f (g(q(A)))
and g(f (q(B))), respectively.

3. T-PRISM

Theoretically T-PRISM is an implementation of the ten-
sorized semantics with a speciﬁc execution mechanism. The
main idea of T-PRISM program execution is that the sym-
bolic part of program execution including recursion is pro-
cessed by Prolog search and the numerical and parallel part
such as loop for sum-product computation is carried out by
TensorFlow. More concretely in T-PRISM, a program is ﬁrst
compiled to an explanation graph with Einstein’s notation, a
set of logical equations like Equation (1), translated to a set
of tensor equations like Equation (2) and Equation (3). By
removing redundant repetitions, Einstein’s notation enables
sum-product computation much faster than that in PRISM
which uses sum and product operations naively. Hereafter,
we simply call it explanation graph. This explanation graph
is obtained by using tabled search in the exhaustive search
for all proofs for the top-goal G and by applying dynamic
programming to them (Zhou, Sato, and Shen 2008).

To train parameters in embedded tensors, T-PRISM sup-
ports (stochastic) gradient decent algorithms using an auto-
differential mechanism in TensorFlow. We compute the loss
of a dataset D represented as a set of goals to learn pa-
rameters. T-PRISM assumes the total loss is deﬁned to be
the sum of individual goal loss as follows:Loss(D) =
PG∈D Loss(G). Usually, Loss(G) is a function of q(G).
For example, when parameter training uses a negative log
likelihood loss function, the goal loss function is deﬁned as
Loss(G) = − log q(G) where q(G) stands for the likeli-
hood of G. For the convenience of modeling, the T-PRISM
system offers related built-ins for parameter training and
custom loss functions written by Python programs with Ten-
sorFlow. Utilizing this T-PRISM system based on tensorized
semantics with non-linear operators, various models includ-
ing matrix computation, deep neural networks, and the mod-
els written by recursive programs can be designed.

section

explains

4. T-PRISM programs for knowledge graphs
T-
This
us-
PRISM through
that, we
ing DistMult
present
datasets:
the FB15k and WN18 datasets
(Bordes et al. 2014;
Bordes et al. 2013).

(Yang et al. 2015), After
experiments with

programs
of
graph modeling

knowledge

learning

sample

real

The T-PRISM system provides a special predicate
tensor/2 to deﬁne TF and qF . Also it provides
set index range/2 to deﬁne R. Using these built-in
predicates, T-PRISM programs are written just like Prolog
programs.

In addition to these special predicates, our preliminary im-
plementation requires to declare all tensor atoms used in a
program using a built-in predicates index list/2. The
ﬁrst argument speciﬁes a tensor atom, and the second one
speciﬁes a list of the second arguments of tensor/2s that
occur during program execution, i.e., its possible use of in-
dices for computing a cost function.

We ﬁrst look at a simple T-PRISM program used for link
prediction in knowledge graphs. A knowledge graph con-
sists of triples (subject, relation, object), represented by a
ground atom rel(subject, relation, object) using the rel/3
predicate. So logically speaking, a knowledge graph is noth-
ing but a set of ground atoms.

For link prediction in knowledge graphs, we choose Dist-
Mult model (Yang et al. 2015), one of the standard knowl-
edge graph models, for its simplicity. In DistMult, entities s,
o, r (s for subject, o for object, r for relation) are encoded as
N-dimensional real vectors, s, o, and r respectively, and the
prediction is made based on the score of (s, r, o) computed
by:

f (s, r, o) = X
i

sirioi.

(4)

In T-PRISM, a DistMult model is written like Figure 1.
The lines 1 and 2 introduce v(_) and r(_). They are em-
bedded vectors with an index i. The lines 3 says the range
of index i is 0 ≤i< 20. The lines 5-8 represent an equa-
tion for sum-product computation for the three embedded
vectors required for computing the scores in Equation (4).

Next, we apply T-PRISM to link prediction in large
size datasets. For more efﬁcient computation, we rewrite
a DistMult program as in Figure 2. This program out-
puts the o-dimensional vector, and computes scores for
all entities at once (the number of entities in FB15k is

1: index_list(v(_),[[i]]).
2: index_list(v,[[o,i]]).
3: index_list(r(_),[[i]]).
4: :-set_index_range(i,256).
5: :-set_index_range(o,14951).
6:
7: rel(S,R,O):-
8:
9:
10:

tensor(v(S),[i]),
tensor(v,[o,i]),
tensor(r(R),[i]).

Figure 2: DistMult program for large-scale data

MRR
HIT@10
HIT@1

FB15K WN18
54.16% 60.62%
75.88% 86.00%
42.22% 47.04%

Table 1: DistMult (Figure 2) performance using T-PRISM

14951). Furthermore, we use a mini-batch method, an es-
sential method in deep learning and other optimization prob-
lems from the viewpoint of learning speed and performance
(Goodfellow, Bengio, and Courville 2016). Note that O is a
singleton variable, i.e., not used in the Prolog search and
construction of tensor equations; however, it is instantiated
as labels and required to compute a loss function described
as below.

For our link prediction experiment, we employ a loss

function for the knowledge graph as follows:
Loss(rel(S, R, O)) = [f (S, R, O) − f (S, R, O′) − γ]+

where [·]+ means a hinge loss max(0, ·) and a negative sam-
ple O′ is uniformly sampled from all entities. We use this
hinge loss function with L2 regularization and its learning
parameters set to λ = 1.0e − 5 and γ = 1.

In this experiment, we use two standard datasets: WN18
from WordNet and FB15k from the Freebase. The purpose
here is not to outperform known models for knowledge
graphs but rather to demonstrate that T-PRISM can imple-
ment them very compactly with a declarative program.

For FB15K, we use the following settings: the batch size
M = 512, the number of negative sampling for one positive
sample Nneg = 100 and select as a learning rate optimiza-
tion method Adam (Kingma and Ba 2014), with the initial
learning rate 0.001. For WN18, we set M to 256 and other
setting is the same as the case of FB15K.

This experiment shows that while the DistMult model
written in PRISM (Kojima and Sato 2018) is incapable of
dealing with these datasets due to their sheer data size, the
T-PRISM implementation of a DistMult model can han-
dle them and achieves reasonable prediction performance as
listed in Table 1. Thanks to a GPU, this experiment is ﬁn-
ished in 5 hours4.

4 NVIDIA Tesla P40 GPU was used.

Conclusion
We proposed an innovative tensorized logic language T-
PRISM together with its tensorized semantics, enabling
declarative tensor modeling for a large-scale data. We ex-
plained how programs are compiled into tensor equations
by way of explanation graphs with Einstein’s notation us-
ing Prolog’s tabled search, and executed on TensorFlow ef-
ﬁciently. T-PRISM supports a rich array of non-linear oper-
ations and custom cost functions for parameter training. Us-
ing our preliminary T-PRISM system, we also demonstrated
the modeling of knowledge graphs. Future work includes ap-
plying T-PRISM to real world problems and showing the ef-
fectiveness of logic-based declarative modeling.

References
[Abadi et al. 2016] Abadi, M.; Barham, P.; Chen, J.; Chen, Z.;
Davis, A.; Dean, J.; Devin, M.; Ghemawat, S.; Irving, G.; Isard,
M.; et al. 2016. Tensorﬂow: a system for large-scale machine
learning. In OSDI, volume 16, 265–283.

[Bordes et al. 2013] Bordes, A.; Usunier, N.; Garcia-Duran, A.;
Weston, J.; and Yakhnenko, O. 2013. Translating embeddings for
modeling multi-relational data. In Advances in Neural Information
Processing Systems, 2787–2795.

[Bordes et al. 2014] Bordes, A.; Glorot, X.; Weston, J.; and Bengio,
Y. 2014. A semantic matching energy function for learning with
multi-relational data. Machine Learning 94(2):233–259.

[Evans and Grefenstette 2018] Evans, R., and Grefenstette, E.
2018. Learning explanatory rules from noisy data. Journal of Arti-
ﬁcial Intelligence Research 61:1–64.

[Goodfellow, Bengio, and Courville 2016] Goodfellow, I.; Bengio,
2016. Deep Learning. MIT Press.

Y.; and Courville, A.
http://www.deeplearningbook.org.

[Kimmig et al. 2011] Kimmig, A.; Demoen, B.; De Raedt, L.;
Costa, V. S.; and Rocha, R. 2011. On the implementation of
the probabilistic logic programming language problog. Theory and
Practice of Logic Programming 11(2-3):235–262.

[Kingma and Ba 2014] Kingma, D., and Ba, J.

A method for
arXiv:1412.6980.

stochastic optimization.

2014. Adam:
arXiv preprint

[Kojima and Sato 2018] Kojima, R., and Sato, T. 2018. Learning
to rank in prism. International Journal of Approximate Reasoning
93:561 – 577.

[Manhaeve et al. 2018] Manhaeve, R.; Dumanˇci´c, S.; Kimmig, A.;
Demeester, T.; and De Raedt, L. 2018. Deepproblog: Neural prob-
abilistic logic programming. arXiv preprint arXiv:1805.10872.
[Rockt¨aschel and Riedel 2017] Rockt¨aschel, T., and Riedel, S.
2017. End-to-end differentiable proving. In Advances in Neural
Information Processing Systems, 3788–3800.

[Sato and Kameya 2008] Sato, T., and Kameya, Y. 2008. New ad-
vances in logic-based probabilistic modeling by PRISM. In Prob-
abilistic Inductive Logic Programming. Springer. 118–155.

[Sato 1995] Sato, T. 1995. A statistical learning method for logic
programs with distribution semantics. In Proceedings of Interna-
tionall Conference on Logic Programming. Citeseer.

[Sato 2017] Sato, T. 2017. A linear algebraic approach to datalog
evaluation. Theory and Practice of Logic Programming 17(3):244–
265.

[Wang, Mazaitis, and Cohen 2013] Wang, W. Y.; Mazaitis, K.; and
Cohen, W. W. 2013. Programming with personalized pagerank: a

locally groundable ﬁrst-order probabilistic logic. In Proceedings of
the 22nd ACM International Conference on Information & Knowl-
edge Management, 2129–2138. ACM.

[Yang et al. 2015] Yang, B.; Yih, W.; He, X.; Gao, J.; and Deng, L.
2015. Embedding entities and relations for learning and inference
in knowledge bases. In Proceedings of the International Confer-
ence on Learning Representations (ICLR).

[Zhou, Sato, and Shen 2008] Zhou, N.-F.; Sato, T.; and Shen, Y.-D.
2008. Linear tabling strategies and optimization. Theory and Prac-
tice of Logic Programming 8(1):81–109.

