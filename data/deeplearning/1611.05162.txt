7
1
0
2

v
o
N
3
2

]

G
L
.
s
c
[

4
v
2
6
1
5
0
.
1
1
6
1
:
v
i
X
r
a

Net-Trim: Convex Pruning of Deep Neural Networks
with Performance Guarantee

Alireza Aghasi, Afshin Abdi, Nam Nguyen and Justin Romberg∗

Abstract

We introduce and analyze a new technique for model reduction for deep neural net-
works. While large networks are theoretically capable of learning arbitrarily complex
models, overﬁtting and model redundancy negatively aﬀects the prediction accuracy and
model variance. Our Net-Trim algorithm prunes (sparsiﬁes) a trained network layer-wise,
removing connections at each layer by solving a convex optimization program. This pro-
gram seeks a sparse set of weights at each layer that keeps the layer inputs and outputs
consistent with the originally trained model. The algorithms and associated analysis are
applicable to neural networks operating with the rectiﬁed linear unit (ReLU) as the non-
linear activation. We present both parallel and cascade versions of the algorithm. While
the latter can achieve slightly simpler models with the same generalization performance,
the former can be computed in a distributed manner. In both cases, Net-Trim signiﬁ-
cantly reduces the number of connections in the network, while also providing enough
regularization to slightly reduce the generalization error. We also provide a mathemati-
cal analysis of the consistency between the initial network and the retrained model. To
analyze the model sample complexity, we derive the general suﬃcient conditions for the
recovery of a sparse transform matrix. For a single layer taking independent Gaussian
random vectors of length N as inputs, we show that if the network response can be de-
scribed using a maximum number of s non-zero weights per node, these weights can be
learned from O(s log N ) samples.

1

Introduction

In the context of universal approximation, neural networks can represent functions of arbitrary
complexity when the network is equipped with suﬃciently large number of layers and neurons
[17]. Such model ﬂexibility has made the artiﬁcial deep neural network a pioneer machine

∗A. Aghasi was previously with the Department of Mathematical Sciences, IBM T.J. Watson Research
Center and is currently with the Georgia State School of Business. N. Nguyen is with the IBM T.J. Watson
Research Center. A. Abdi and J. Romberg are with the Department of Electrical and Computer Engineering,
Georgia Institute of Technology.
Contact: aaghasi@gsu.edu

1

 
 
 
 
 
 
learning tool over the past decades (see [20] for a comprehensive review of deep networks).
Basically, given unlimited training data and computational resources, deep neural networks
are able to learn arbitrarily complex data models.

In practice, the capability of collecting huge amount of data is often restricted. Thus,
learning complicated networks with millions of parameters from limited training data can
easily lead to the overﬁtting problem. Over the past years, various methods have been
proposed to reduce overﬁtting via regularizing techniques and pruning strategies [19, 14,
21, 24]. However, the complex and non-convex behavior of the underlying model barricades
the use of theoretical tools to analyze the performance of such techniques.

In this paper, we present an optimization framework, namely Net-Trim, which is a layer-
wise convex scheme to sparsify deep neural networks. The proposed framework can be viewed
from both theoretical and computational standpoints. Technically speaking, each layer of a
neural network consists of an aﬃne transformation (to be learned by the data) followed by
a nonlinear unit. The nested composition of such mappings forms a highly nonlinear model,
learning which requires optimizing a complex and non-convex objective. Net-Trim applies to
a network which is already trained. The basic idea is to reduce the network complexity layer
by layer, assuring that each layer response stays close to the initial trained network.

More speciﬁcally, the training data is transmitted through the learned network layer by
layer. Within each layer we propose an optimization scheme which promotes weight sparsity,
while enforcing a consistency between the resulting response and the trained network re-
sponse. In a sense, if we consider each layer response to the transmitted data as a checkpoint,
Net-Trim assures the checkpoints remain roughly the same, while a simpler path between
the checkpoints is discovered. A favorable leverage of Net-Trim is the possibility of convex
formulation, when the ReLU is employed as the nonlinear unit across the network.

Figure 1 demonstrates the pruning capability of Net-Trim for a sample network. The
neural network used for this example classiﬁes 200 points positioned on the 2D plane into
two separate classes based on their label. The points within each class lie on nested spirals to
classify which we use a neural network with two hidden layers of each 200 neurons (the reader
is referred to the Experiments section for more technical details). Figures 1(a), (b) present
the weighted adjacency matrix and partial network topology, relating the hidden layers before
and after retraining. With only a negligible change to the overall network response, Net-Trim
is able to prune more than 93% of the links among the neurons, and bring a signiﬁcant model
reduction to the problem. Even when the neural network is trained using sparsifying weight
regularizers (here, dropout [21] and (cid:96)1 penalty), application of the Net-Trim yields a major
additional reduction in the model complexity as illustrated in Figures 1(c) and 1(d).

Net-Trim is particularly useful when the number of training samples is limited. While
overﬁtting is likely to occur in such scenarios, Net-Trim reduces the complexity of the model
by setting a signiﬁcant portion of weights at each layer to zero, yet maintaining a similar
relationship between the input data and network response. This capability can also be viewed
from a diﬀerent perspective, that Net-Trim simpliﬁes the process of determining the network

2

81

100

81

100

⋮

⋮

⋮

⋮

81

⇒

100

(b)

81

⇒

100

(d)

⋮

⋮

⋮

⋮

⇒

(a)

⇒

(c)

Figure 1: Net-Trim pruning performance on classiﬁcation of points within nested spirals; (a) left:
the weighted adjacency matrix relating the two hidden layers after training; right: the adjacency
matrix after the application of Net-Trim causing more than 93% of the weights to vanish; (b) partial
network topology relating neurons 81 to 100 of the hidden layers, before and after retraining; (c) left:
the adjacency matrix after training the network with dropout and (cid:96)1 regularization; right: Net-Trim
is yet able to ﬁnd a model which is over 7 times sparser than the model on the left; (d) partial network
topology before and after retraining for panel (c)

size. In other words, the network used at the training phase can be oversized and present
more degrees of freedom than what the data require. Net-Trim would automatically reduce
the network size to an order matching the data.

Finally, a favorable property of Net-Trim is its post-processing nature. It simply processes
the network layer-wise responses regardless of the training strategy used to build the model.
Hence, the proposed framework can be easily combined with the state-of-the-art training
techniques for deep neural networks.

3

5010015020020406080100120140160180200-9-8-7-6-5-4-3-2-1015010015020020406080100120140160180200-8-7-6-5-4-3-2-10125010015020020406080100120140160180200-9-8-7-6-5-4-3-2-1015010015020020406080100120140160180200-9-8-7-6-5-4-3-2-1011.1 Previous Work

In the recent years there has been increasing interest in the mathematical analysis of deep
networks. These eﬀorts are mainly in the context of characterizing the minimizers of the
underlying cost function. In [4], the authors show that some deep forward networks can be
learned accurately in polynomial time, assuming that all the edges of the network have random
weights. They propose a layer-wise algorithm where the weights are recovered sequentially at
each layer. In [18], Kawaguchi establishes an exciting result showing that regardless of being
highly non-convex, the square loss function of a deep neural network inherits interesting
geometric structures.
In particular, under some independence assumptions, all the local
minima are also the global ones. In addition, the saddle points of the loss function possess
special properties which guide the optimization algorithms to avoid them.

The geometry of the loss function is also studied in [10], where the authors bring a connec-
tion between spin-glass models in physics and fully connected neural networks. On the other
hand, Giryes et al. recently provide the link between deep neural networks and compressed
sensing [15], where they show that feedforward networks are able to preserve the distance
of the data at each layer by using tools from compressed sensing. There are other works on
formulating the training of feedforward networks as an optimization problem [7, 6, 5]. The
majority of cited works approach to understand the neural networks by sequentially studying
individual layers, which is also the approach taken in this paper.

On the more practical side, one of the notorious issues with training a complicated deep
neural network concerns overﬁtting when the amount of training data is limited. There have
been several approaches trying to address this issue, among those are the use of regularizations
such as (cid:96)1 and (cid:96)2 penalty [19, 14]. These methods incorporate a penalty term to the loss
function to reduce the complexity of the underlying model. Due to the non-convex nature of
the underlying problem, mathematically characterizing the behavior of such regularizers is an
impossible task and most literature in this area are based on heuristics. Another approach is
to apply the early stopping of training as soon as the performance of a validation set starts
to get worse.

More recently, a new way of regularization is proposed by Hinton et. al.

[21] called
Dropout. It involves temporarily dropping a portion of hidden activations during the train-
ing. In particular, for each sample, roughly 50% of the activations are randomly removed on
the forward pass and the weights associated with these units are not updated on the backward
pass. Combining all the examples will result in a representation of a huge ensemble of neural
networks, which oﬀers excellent generalization capability. Experimental results on several
tasks indicate that Dropout frequently and signiﬁcantly improves the classiﬁcation perfor-
mance of deep architectures. More recently, LeCun et al. proposed an extension of Dropout
named DropConnect [24]. It is essentially similar to Dropout, except that it randomly re-
moves the connections rather than the activations. As a result, the procedure introduces a
dynamic sparsity on the weights.

4

The aforementioned regularization techniques (e.g (cid:96)1, Dropout, and DropConnect) can
be seen as methods to sparsify the neural network, which result in reducing the model com-
plexity. Here, sparsity is understood as either reducing the connections between the nodes
or decreasing the number of activations. Beside avoiding overﬁtting, computational favor of
sparse models is preferred in applications where quick predictions are required.

1.2 Summary of the Technical Contributions

Our post-training scheme applies multiple convex programs with the (cid:96)1 cost to prune the
weight matrices on diﬀerent layers of the network. Formally, we denote Y (cid:96)−1 and Y (cid:96) as the
given input and output of the (cid:96)-th layer of the trained neural network, respectively. The
mapping between the input and output of this layer is performed via the weight matrix W(cid:96)
and the nonlinear activation unit σ: Y (cid:96)
). To perform the pruning at this layer,
Net-Trim focuses on addressing the following optimization:

= σ(W ⊺

(cid:96) Y (cid:96)−1

U ∥U ∥1
min

s.t.

∥σ (U ⊺Y (cid:96)−1

) − Y (cid:96)

∥F ≤ (cid:15),

(1)

where (cid:15) is a user-speciﬁc parameter that controls the consistence of the output Y (cid:96) before
and after retraining and ∥ U ∥1 is the sum of absolute entries of U , which is essentially the (cid:96)1
norm to enforce sparsity on U .

When σ(.) is taken to be the ReLU, we are able to provide a convex relaxation to (1). We
will show that ˆW(cid:96), the solution to the underlying convex program, is not only sparser than
W(cid:96), but the error accumulated over the layers due to the (cid:15)-approximation constraint will not
signiﬁcantly explode. In particular in Theorem 1 we show that if ˆY (cid:96) is the (cid:96)-th layer after
retraining, i.e., ˆY (cid:96)

), then for a network with normalized weight matrices

⊺ ˆY (cid:96)−1

= σ(

ˆW(cid:96)

(cid:96)

ˆY

∥

− Y (cid:96)

≤ (cid:96)(cid:15).

∥
F

Basically, the error propagated by the Net-Trim at any layer is at most a multiple of (cid:15). This
property suggests that the network constructed by the Net-Trim is sparser, while capable of
achieving a similar outcome. Another attractive feature of this scheme is its computational
distributability, i.e., the convex programs could be solved independently.

Also, in this paper we propose a cascade version of Net-Trim, where the output of the
retraining at the previous layer is fed to the next layer as the input of the optimization. In
particular, we present a convex relaxation to

U ∥U ∥1
min

s.t.

∥σ (U ⊺ ˆY

(cid:96)−1

) − Y (cid:96)

∥

≤ (cid:15)(cid:96),

F

where ˆY (cid:96)−1 is the retrained output of the ((cid:96) − 1)-th layer and (cid:15)(cid:96) has a closed form expression
to maintain feasibility of the resulting program. Again, for a network with normalized weight

5

matrices, in Theorem 2 we show that

(cid:96)

ˆY
∥

− Y (cid:96)

∥

F

≤ (cid:15)1γ((cid:96)−1)/2.

Here γ > 1 is a constant inﬂation rate that can be arbitrarily close to 1 and controls the
magnitude of (cid:15)(cid:96). Because of the more adaptive pruning, cascade Net-Trim may yield sparser
solutions at the expense of not being computationally parallelizable.

Finally, for redundant networks with limited training samples, we will discuss that a
simpler network (in terms of sparsity) with identical performance can be explored by setting
(cid:15) = 0 in (1). We will derive general suﬃcient conditions for the recovery of such sparse
model via the proposed convex program. As an insightful case, we show that when a layer
is probed with standard Gaussian samples (e.g., applicable to the ﬁrst layer), learning the
simple model can be performed with much fewer samples than the layer degrees of freedom.
More speciﬁcally, consider X ∈ RN ×P to be a Gaussian matrix, where each column represents
an input sample, and W a sparse matrix, with at most s nonzero terms on each column,
from which the layer response is generated, i.e., Y = σ(W ⊺X). In Theorem 3 we state that
when P = O(s log N ), with overwhelming probability, W can be accurately learned through
the proposed convex program.

As will be detailed, the underlying analysis steps beyond the standard measure concen-
tration arguments used in the compressed sensing literature (cf. §8 in [12]). We contribute
by establishing concentration inequalities for the sum of dependent random matrices.

1.3 Notations and Organization of the Paper

The remainder of the paper is structured as follows. In Section 2, we formally present the
network model used in the paper. The proposed pruning schemes, both the parallel and
cascade Net-Trim are presented and discussed in Section 3. The material includes insights
on developing the algorithms and detailed discussions on the consistency of the retraining
schemes. Section 4 is devoted to the convex analysis of the proposed framework. We derive the
unique optimality conditions for the recovery of a sparse weight matrix through the proposed
convex program. We then use this tool to derive the number of samples required for learning a
sparse model in a Gaussian sample setup. In Section 5 we report some retraining experiments
and the improvement that Net-Trim brings to the model reduction and robustness. Finally,
Section 6 presents some discussions on extending the Net-Trim framework, future outlines
and concluding remarks.

As a summary of the notations, our presentation mainly relies on multidimensional cal-
culus. We use bold characters to denote vectors and matrices. Considering a matrix A and
the index sets Γ1, and Γ2, we use AΓ1,∶ to denote the matrix obtained by restricting the rows
of A to Γ1. Similarly, A∶,Γ2 denotes the restriction of A to the columns speciﬁed by Γ2, and
AΓ1,Γ2 is the submatrix with the rows and columns restricted to Γ1 and Γ2, respectively.

6

Given a matrix X = [xm,n] ∈ RM ×N , we use ∥X∥1 ≜ ∑

N
n=1 ∣xm,n∣ to denote the sum of
matrix absolute entries1 and ∥X∥F to denote the Frobenius norm. For a given vector x, ∥x∥0
denotes the cardinality of x, supp x denotes the set of indices with non-zero entries from x,
and suppc x is the complement set. Mainly in the proofs, we use the notation x+ to denote
max(x, 0). The max(., 0) operation applied to a vector or matrix acts on every component
individually. Finally, following the MATLAB convention, the vertical concatenation of two
vectors a and b (i.e., [a⊺, b⊺

⊺) is sometimes denoted by [a; b] in the text.
]

M
m=1 ∑

2 Feed Forward Network Model

In this section, we introduce some notational conventions related to a feed forward network
model, which will be used frequently in the paper. Considering a feed forward neural network,
we assume to have P training samples xp, p = 1, ⋯, P , where xp ∈ RN is an input to the
network. We stack up the samples in a matrix X ∈ RN ×P , structured as

X = [x1, ⋯, xP ] .

The ﬁnal output of the network is denoted by Z ∈ RM ×P , where each column zp ∈ RM of Z
is a response to the corresponding training column xp in X. We consider a network with L
layers, where the activations are taken to be rectiﬁed linear units. Associated with each layer
(cid:96), we have a weight matrix W(cid:96) such that

Y ((cid:96))

= max (W ⊺

(cid:96) Y ((cid:96)−1), 0) ,

(cid:96) = 1, ⋯, L,

(2)

and

Y (0)

= X, Y (L)

(3)
Basically, the outcome of the (cid:96)-th layer is Y ((cid:96))
∈ RN(cid:96)×P , which is generated by applying the
adjoint of W(cid:96) ∈ RN(cid:96)−1×N(cid:96) to Y ((cid:96)−1) and going through a component-wise max(., 0) operation.
Clearly in this setup N0 = N and NL = M . A trained neural network as outlined in (2)
L
(cid:96)=1, X). Figure 2(a) demonstrates the architecture of the
and (3) is represented by T N ({W(cid:96)}
proposed network.

= Z.

For the sake of theoretical analysis, throughout the paper we focus on networks with

normalized weights as follows.

L
Deﬁnition 1. A given neural network T N ({W(cid:96)}
(cid:96)=1, X) is link-normalized when ∥W(cid:96)∥1 = 1
for every layer (cid:96) = 1, ⋯, L.

1The formal induced norm ∥X∥1 has a diﬀerent deﬁnition, however, for a simpler formulation we use a

similar notation

7

Y (1)

Y (L−1)

X
x1,1 ⋯ x1,P
x2,1 ⋯ x2,P

⎡
⎢
⎢
⎢
⎢
⎢
⎢
⎢
⎢
⎢
⎢
xN,1 ⋯ xN,P
⎣

⋯

⋮

⋮

⎤
⎥
⎥
⎥
⎥
⎥
⎥
⎥
⎥
⎥
⎥
⎦

→

W1

X

Y (1)

Y (L−1)

⋯

WL

Z

Z

→

z1,1 ⋯ z1,P
⎡
⎢
⎢
z2,1 ⋯ z2,P
⎢
⎢
⎢
⋮
⋯
⎢
⎢
zM,1 ⋯ zM,P
⎣

⋮

⎤
⎥
⎥
⎥
⎥
⎥
⎥
⎥
⎦

ˆW1

ˆY (1)

ˆY (L−1)

⋯

ˆWL

ˆZ

⋯

(a)

⇒ X
(b)

Figure 2: (a) Network architecture and notations; (b) the main retraining idea: keeping the layer
outcomes close to the initial trained model while ﬁnding a simpler path relating each layer input to
the output

A general network in the form of (2) can be converted to its link-normalized version
by replacing W(cid:96) with W(cid:96)/∥W(cid:96)∥1, and Y ((cid:96)+1) with Y ((cid:96)+1)
(cid:96)
j=0 ∥Wj∥1. Since max(αx, 0) =
/ ∏
α max(x, 0) for α > 0, any weight processing on a network of the form (2) can be applied to
the link-normalized version and later transferred to the original domain via a suitable scaling.
Subsequently, all the results presented in this paper are stated for a link-normalized network.

3 Pruning the Network

Our pruning strategy relies on redesigning the network so that for the same training data
each layer outcomes stay more or less close to the initial trained model, while the weights
associated with each layer are replaced with sparser versions to reduce the model complexity.
Figure 2(b) presents the main idea, where the complex paths between the layer outcomes are
replaced with simple paths.

Consider the ﬁrst layer, where X = [x1, ⋯, xP ] is the layer input, W = [w1, ⋯, wM ] the
layer coeﬃcient matrix, and Y = [ym,p] the layer outcome. We require the new coeﬃcient
matrix ˆW to be sparse and the new response to be close to Y . Using the sum of absolute
entries as a proxy to promote sparsity, a natural strategy to retrain the layer is addressing

8

the nonlinear program

ˆW = arg min

U

∥U ∥1

s.t.

∥max (U ⊺X, 0) − Y ∥F ≤ (cid:15).

(4)

Despite the convex objective, the constraint set in (4) is non-convex. However, we may
approximate it with a convex set by imposing Y and ˆY = max(
ˆW ⊺X, 0) to have similar
activation patterns. More speciﬁcally, knowing that ym,p is either zero or positive, we enforce
the max(., 0) argument to be negative when ym,p = 0, and close to ym,p elsewhere. To present
the convex formulation, for V = [vm,p] we use the notation

∑
m,p∶ ym,p>0

mxp − ym,p)

(u⊺
u⊺
mxp ≤ vm,p

2

≤ (cid:15)2

f or m, p ∶ ym,p = 0

⇐⇒ U ∈ C(cid:15)(X, Y , V ).

(5)

⎧⎪⎪
⎨
⎪⎪⎩

Based on this deﬁnition, a convex proxy to (4) is

ˆW = arg min

U

∥U ∥1

s.t. U ∈ C(cid:15)(X, Y , 0).

(6)

Basically, depending on the value of ym,p, a diﬀerent constraint is imposed on u⊺
mxp to
emulate the ReLU operation. For a simpler formulation throughout the paper, we use a
similar notation as C(cid:15)(X, Y , V ) for any constraints of the form (5) parametrized by given
X, Y , V and (cid:15).

As a ﬁrst step towards establishing a retraining framework applicable to the entire network,
we show that the solution of (6) satisﬁes the constraint in (4) and the outcome of the retrained
layer stays controllably close to Y .

Proposition 1. Let ˆW be the solution to (6). For ˆY = max(
layer response, ∥

ˆY − Y ∥F ≤ (cid:15).

ˆW ⊺X, 0) being the retrained

Based on the above exploratory, we propose two schemes to retrain the neural network; one
explores a computationally distributable nature and the other proposes a cascading scheme
to retrain the layers sequentially. The general idea which originates from the relaxation in
(6) is referred to as the Net-Trim, speciﬁed by the parallel or cascade nature.

3.1 Parallel Net-Trim

The parallel Net-Trim is a straightforward application of the convex program (6) to each layer
in the network. Basically, each layer is processed independently based on the initial model
input and output, without taking into account the retraining result from the previous layer.
Speciﬁcally, denoting Y ((cid:96)−1) and Y ((cid:96)) as the input and output of the (cid:96)-th layer of the initially

9

Algorithm 1 Parallel Net-Trim

← max (W ⊺

1: Input: X, (cid:15) > 0, and link-normalized W1, ⋯, WL
2: Y (0)
← X
3: for (cid:96) = 1, ⋯, L do
Y ((cid:96))
4:
5: end for
6: for all (cid:96) = 1, ⋯, L do
7:
8: end for
9: Output: ˆW1, ⋯, ˆWL

ˆW(cid:96) ← arg minU ∥U ∥1

(cid:96) Y ((cid:96)−1), 0)

% generating link-normalized layer outcomes

s.t. U ∈ C(cid:15) (Y ((cid:96)−1), Y ((cid:96)), 0)

% retraining

trained neural network (see equation (2)), we propose to relearn the coeﬃcient matrix W(cid:96)
via the convex program

ˆW (cid:96) = arg min

U

∥U ∥1

s.t. U ∈ C(cid:15) (Y ((cid:96)−1), Y ((cid:96)), 0) .

(7)

The optimization (7) can be independently applied to every layer in the network and
hence computationally distributable. The pseudocode for the parallel Net-Trim is presented
as Algorithm 1.

With reference to the constraint in (7), if we only retrain the (cid:96)-th layer, the output of the
retrained layer is in the (cid:15)-neighborhood of that before retraining. However, when all the layers
are retrained through (7), an immediate question would be whether the retrained network
produces an output which is controllably close to the initially trained model. In the following
theorem, we show that the retrained error does not blow up across the layers and remains a
multiple of (cid:15).

Theorem 1. Given a link-normalized network T N ({W(cid:96)}
as sketched in (2) and (3), consider retraining each layer individually via

L

(cid:96)=1, X) with layer outcomes Y ((cid:96))

ˆW(cid:96) = arg min

U

∥U ∥1

s.t. U ∈ C(cid:15)(cid:96) (Y ((cid:96)−1), Y ((cid:96)), 0) .

(8)

For the retrained network T N ({

ˆW(cid:96)}

L

(cid:96)=1, X) with layer outcomes ˆY

((cid:96))

ˆY

∥

− Y ((cid:96))

∥

(cid:96)
∑
j=1

(cid:15)j.

≤

F

((cid:96))

= max(

⊺
ˆW
(cid:96)

ˆY

((cid:96)−1)

, 0),

(9)

When all the layers are retrained with a ﬁxed parameter (cid:15) (as in Algorithm 1), the following
corollary simply bounds the overall discrepancy.

10

Corollary 1. Using Algorithm 1, the ultimate network outcome obeys

(L)

ˆY

∥

− Y (L)

≤ L(cid:15).

∥
F

We would like to note that the network conversion to a link-normalized version is only
for the sake of presenting the theoretical results in a more compact form. In practice such
conversion is not necessary and to retrain layer (cid:96) we can take (cid:15) = (cid:15)r∥Y ((cid:96))
∥F , where (cid:15)r plays a
similar role as (cid:15) for a link-normalized network.

3.2 Cascade Net-Trim

Unlike the parallel scheme, where each layer is retrained independently, in the cascade ap-
proach the outcome of a retrained layer is used to retrain the next layer. To better explain
the mechanics, consider starting the cascade process by retraining the ﬁrst layer as before,
through

ˆW1 = arg min

U

∥U ∥1

s.t. U ∈ C(cid:15)1 (X, Y (1), 0) .

(10)

= max(

Setting ˆY (1)
⊺X, 0) to be the outcome of the retrained layer, to retrain the second
layer, we ideally would like to address a similar program as (10) with ˆY (1) as the input and
Y (2) being the output reference, i.e.,

ˆW1

U ∥U ∥1
min

s.t. U ∈ C(cid:15)2 (

ˆY (1), Y (2), 0) .

(11)

However, there is no guarantee that program (11) is feasible, that is, there exists a matrix
W = [w1, ⋯, wN2] such that

∑
m,p>0

m,p∶ y(2)

⎧⎪⎪⎪⎪
⎪⎪⎪⎪⎩

⎨

(w⊺

m ˆy(1)

p − y(2)
m,p)

w⊺

m ˆy(1)

p ≤ 0

2

≤ (cid:15)2
2

f or m, p ∶ y(2)

m,p = 0

.

(12)

If instead of ˆY (1) the constraint set (11) was parameterized by Y (1), a natural feasible point
would have been W2. Now that ˆY (1) is a perturbed version of Y (1), the constraint set needs
to be slacked to maintain the feasibility of W2. In this context, one may easily verify that

W2 ∈ C(cid:15)2 (

ˆY (1), Y (2), W ⊺
2

ˆY (1)

)

as long as

(cid:15)2
2 ≥ ∑
m,p∶ y(2)

m,p>0

(w⊺

2,m ˆy(1)

2
p − y(2)
m,p)

(13)

(14)

,

11

where w2,m is the m-th column of W2. Basically the constraint set in (13) is a slacked
version of the constraint set in (12), where the right hand side quantities in the corresponding
inequalities are suﬃciently extended to maintain the feasibility of W2.

Following this line of argument, in the cascade Net-Trim we propose to retrain the ﬁrst
layer through (10). For every subsequent layer, (cid:96) = 2, ⋯, L, the retrained weighting matrix is
obtained via

ˆW(cid:96) = arg min

U

∥U ∥1

s.t. U ∈ C(cid:15)(cid:96) (

ˆY

((cid:96)−1)

, Y ((cid:96)), W ⊺
(cid:96)

ˆY

((cid:96)−1)

) ,

(15)

where for W(cid:96) = [w(cid:96),1, ⋯, w(cid:96),N(cid:96)] and γ(cid:96) ≥ 1,

(cid:15)2
(cid:96) = γ(cid:96) ∑
m,p∶ y((cid:96))

m,p>0

(w⊺

(cid:96),m ˆy((cid:96)−1)

p

− y((cid:96))

m,p)

2

.

The constants γ(cid:96) ≥ 1 (referred to as the inﬂation rates) are free parameters, which control
the sparsity of the resulting matrices. After retraining the (cid:96)-th layer we set

ˆY ((cid:96))

= max (

ˆW(cid:96)

⊺ ˆY ((cid:96)−1), 0) ,

and use this outcome to retrain the next layer. Algorithm 2 presents the pseudo-code to
implement the cascade Net-Trim for (cid:15)1 = (cid:15) and a constant inﬂation rate, γ, across all the
layers.

Algorithm 2 Cascade Net-Trim

ˆW1

⊺X, 0)

s.t. U ∈ C(cid:15)(X, Y , 0)

1: Input: X, (cid:15) > 0, γ > 1 and link-normalized W1, ⋯, WL
2: Y ← max (W ⊺
1 X, 0)
3: ˆW1 ← arg minU ∥U ∥1
4: ˆY ← max(
5: for (cid:96) = 2, ⋯, L do
Y ← max(W ⊺
(cid:96) Y , 0)
6:
(cid:15) ← (γ ∑m,p∶ym,p>0(w⊺
ˆW(cid:96) ← arg minU ∥U ∥1
⊺
ˆW
ˆY ← max(
(cid:96)

(cid:96),m ˆyp − ym,p)

s.t. U ∈ C(cid:15)(

ˆY , Y , W ⊺
(cid:96)

ˆY , 0)

ˆY )

1/2

8:

7:

)

2

9:
10: end for
11: Output: ˆW1, ⋯, ˆWL

% w(cid:96),m is the m-th column of W(cid:96)

In the following theorem, we prove that the outcome of the retrained network produced

by Algorithm 2 is close to that of the network before retraining.

12

L
Theorem 2. Given a link-normalized network T N ({W(cid:96)}
consider retraining the ﬁrst layer as (10) and the subsequent layers via (15), such that

(cid:96)=1, X) with layer outcomes Y ((cid:96)),

ˆY ((cid:96))
the retrained network

= max(

ˆW(cid:96)

⊺ ˆY ((cid:96)−1), 0), ˆY (1)

= max(

⊺X, 0) and γ(cid:96) > 1. For T N ({

(w⊺

(cid:96),m ˆy((cid:96)−1)

p

− y((cid:96))

m,p)

2

,

(cid:15)2
(cid:96) = γ(cid:96) ∑
m,p∶ y((cid:96))

m,p>0
ˆW1

((cid:96))

ˆY

∥

− Y ((cid:96))

≤ (cid:15)1

∥
F

¿
`
`
`(cid:192)

(cid:96)
∏
j=2

γj.

ˆW(cid:96)}

L
(cid:96)=1, X) being

(16)

When (cid:15)1 = (cid:15) and all the layers are retrained with a ﬁxed inﬂation rate (as in Algorithm 2),
the following corollary of Theorem 2 bounds the network overall discrepancy.

Corollary 2. Using Algorithm 2, the ultimate network outcome obeys

(L)

ˆY

∥

− Y (L)

∥

(L−1)
2

(cid:15).

≤ γ

F

Similar to the parallel Net-Trim, the cascade Net-Trim can also be performed without a

link-normalization by simply setting (cid:15) = (cid:15)r∥Y (1)

∥F .

3.3 Retraining the Last Layer

Commonly, the last layer in a neural network is not subject to an activation function and a
standard linear model applies, i.e., Y (L)
L Y (L−1). This linear outcome may be directly
exploited for regression purposes or pass through a soft-max function to produce the scores
for a classiﬁcation task.

= W ⊺

In this case, to retrain the layer we simply need to seek a sparse weight matrix under the
constraint that the linear outcomes stay close before and after retraining. More speciﬁcally,

ˆWL = arg min

U

∥U ∥1

s.t.

∥U ⊺Y (L−1)

− Y (L)

∥F ≤ (cid:15)L.

In the case of cascade Net-Trim,

ˆWL = arg min

U

∥U ∥1

s.t.

∥U ⊺ ˆY

(L−1)

− Y (L)

∥

≤ (cid:15)L,

F

and the feasibility of the program is established for

L = γL ∥W ⊺
(cid:15)2

L

ˆY

(L−1)

− Y (L)

∥

2

F

,

γ ≥ 1.

(17)

(18)

(19)

It can be shown that the results stated earlier in Theorems 1 and 2 regarding the overall
discrepancy of the network generalize to a network with linear activation at the last layer.

13

Proposition 2. Consider a link-normalized network T N ({W(cid:96)}
linear model applies to the last layer.

L
(cid:96)=1, X), where a standard

(a) If the ﬁrst L − 1 layers are retrained according to the process stated in Theorem 1 and

the last layer is retrained through (17), then

(L)

ˆY
∥

− Y (L)

∥

L
∑
(cid:96)=1

(cid:15)j.

≤

F

(b) If the ﬁrst L − 1 layers are retrained according to the process stated in Theorem 2 and

the last layer is retrained through (18) and (19), then

(L)

ˆY
∥

− Y (L)

∥

≤ (cid:15)1

F

¿
`
`
`(cid:192)

L
∏
j=2

γj.

While the cascade Net-Trim is designed in way that infeasibility is never an issue, one
can take a slight risk of infeasibility in retraining the last layer to further reduce the overall
discrepancy. More speciﬁcally, if the value of (cid:15)L in (18) is replaced with κ(cid:15)L for some κ ∈ (0, 1),
we may reduce the overall discrepancy by the factor κ, without altering the sparsity pattern
of the ﬁrst L−1 layers. It is however clear that in this case there is no guarantee that program
(18) remains feasible and multiple trials may be needed to tune κ. We will refer to κ as the
risk coeﬃcient and will present some examples in Section 5, which use it as a way to control
the ﬁnal discrepancy in a cascade framework.

4 Convex Analysis and Model Learning

In this section we will focus on redundant networks, where the mapping between a layer input
and the corresponding output can be established via various weight matrices. As an example,
this could be the case when insuﬃcient training samples are used to train a large network.
We will show that in this case, if the relation between the layer input and output can be
established via a sparse weight matrix, under some conditions such matrix could be uniquely
identiﬁed through the core Net-Trim program in (6).

As noted above, in the case of a redundant layer, for a given input X and output Y , the
relation Y = max(W ⊺X, 0) can be established via more than one W . In this case we hope
to ﬁnd a sparse W by setting (cid:15) = 0 in (6). For this value of (cid:15) our central convex program
reduces to

ˆW = arg min

U

∥U ∥1

s.t. {

u⊺
u⊺

mxp = ym,p f or m, p ∶ ym,p > 0
f or m, p ∶ ym,p = 0
mxp ≤ 0

,

which decouples into M convex programs, each searching for the m-th column in ˆW :

ˆwm = arg min

w

∥w∥1

s.t. {

w⊺xp = ym,p f or p ∶ ym,p > 0
w⊺xp ≤ 0
f or p ∶ ym,p = 0

.

14

For a more concise representation, we drop the m index and given a vector y ∈ RP focus on
the convex program

w ∥w∥1
min

s.t. {

X ⊺
X ⊺

∶,Ωw = yΩ
∶,Ωcw ⪯ 0

, where Ω = {p ∶ yp > 0}.

(20)

In the remainder of this section we analyze the optimality conditions for (20), and show how
they can be linked to the identiﬁcation of a sparse solution. The program can be cast as

w,s ∥w∥1
min

s.t.

˜X [

w
s ] = y,

s ⪯ 0,

(21)

where

˜X = [

X ⊺
∶,Ω
X ⊺

0
∶,Ωc −I]

and y = [

yΩ
0 ] .

For a general ˜X, not necessarily structured as above, the following result states the suﬃcient
conditions under which a sparse pair (w∗, s∗
Proposition 3. Consider a pair (w∗, s∗
) ∈ (Rn1, Rn2), which is feasible for the convex pro-
gram (21). If there exists a vector Λ = [Λ(cid:96)] ∈ Rn1+n2 in the range of ˜X ⊺ with entries satisfying

) is the unique minimizer to (21).

−1 < Λ(cid:96) < 1 (cid:96) ∈ suppc w∗
(cid:96) ∈ suppc s∗
0 < Λn1+(cid:96)

{

,

Λ(cid:96) = sign(w∗
(cid:96) )
Λn1+(cid:96) = 0

{

(cid:96) ∈ supp w∗
(cid:96) ∈ supp s∗

,

(22)

and for ˜Γ = supp w∗
pair (w∗, s∗

∪ {n1 + supp s∗
) is the unique solution to (21).

} the restricted matrix ˜X ∶,˜Γ is full column rank, then the

The proposed optimality result can be related to the unique identiﬁcation of a sparse
w∗ from rectiﬁed observations of the form y = max(X ⊺w∗, 0). Clearly, the structure of the
feature matrix X plays the key role here, and the construction of the dual certiﬁcate stated
in Proposition 3 entirely relies on that. As an insightful case, we show that when X is a
Gaussian matrix (that is, the elements of X are i.i.d values drawn from a standard normal
distribution), learning w∗ can be performed with much fewer samples than the layer degrees
of freedom.

Theorem 3. Let w∗
∈ RN be an arbitrary s-sparse vector, X ∈ RN ×P a Gaussian matrix
representing the samples and µ > 1 a ﬁxed value. Given P = (11s + 7)µ log N observations of
the type y = max(X ⊺w∗, 0), with probability exceeding 1 − N 1−µ the vector w∗ can be learned
exactly through (20).

The standard Gaussian assumption for the feature matrix X allows us to relate the
number of training samples to the number of active links in a layer. Such feature structure

15

could be a realistic assumption for the ﬁrst layer of the neural network. As shown in the
proof of Theorem 3, because of the dependence of the set Ω to the entries in X, the standard
concentration of measure framework for independent random matrices is not applicable here.
Instead, we will need to establish concentration bounds for the sum of dependent random
matrices.

Because of the contribution the weight matrices have to the distribution of Y (1), ⋯, Y (L),
without restrictive assumptions, a similar type of analysis for the subsequent layers seems
signiﬁcantly harder and left as a possible future work. Yet, Theorem 3 is a good reference
for the number of required training samples to learn a sparse model for Gaussian (or approx-
imately Gaussian) samples. While we focused on each decoupled problem individually, for
observations of the type Y = max(W ∗⊺X, 0), using the union bound, an exact identiﬁcation
of W ∗ can be warranted as a corollary of Theorem 3.

Corollary 3. Consider an arbitrary matrix W ∗
m∥0,
and 0 < sm ≤ smax for m = 1, ⋯, M . For X ∈ RN ×P being a Gaussian matrix, set Y =
max(W ∗⊺X, 0).
If µ > (1 + logN M ) and P = (11smax + 7)µ log N , for (cid:15) = 0, W ∗ can be
accurately learned through (6) with probability exceeding

M ] ∈ RN ×M , where sm = ∥w∗

1, ⋯, w∗

= [w∗

1 −

M
∑
m=1

N 1−µ 11smax+7
11sm+7 .

4.1 Pruning Partially Clustered Neurons

As discussed above, in the case of (cid:15) = 0, program (6) decouples into M smaller convex
problems, which could be addressed individually and computationally cheaper. Clearly, for
(cid:15) ≠ 0 a similar decoupling does not produce the formal minimizer to (6), but such suboptimal
solution may yet signiﬁcantly contribute to the pruning of the layer.

Basically, in retraining W ∈ RN ×M corresponding to a large layer with a large num-
ber of training samples, one may consider partitioning the output nodes into Nc clusters
Nc
C1, C2, ⋯, CNc such that ∪
k=1Ck = {1, 2, ⋯, M }, and solve an individual version of (6) for each
cluster focusing on the underlying target nodes:

˜W∶,Ck = arg min

U

∥U ∥1

s.t. U ∈ C(cid:15)k (X, YCk,∶ , 0).

(23)

Solving (23) for each cluster provides the retrained submatrix associated with that cluster.
The values of (cid:15)k in (23) are selected in a way that ultimately the overall layer discrepancy is
upper-bounded by (cid:15). In this regard, a natural choice would be

√

(cid:15)k = (cid:15)

∣Ck∣
M

.

16

While ˆW , acquired through (6), and ˜W are only identical in the case of (cid:15) = 0, the idea of
clustering the output neurons into multiple groups and retraining each sublayer individually
can signiﬁcantly help with breaking down large problems into computationally tractable ones.
Some examples of Net-Trim with partially clustered neurons (PCN) will be presented in the
experiments section. Clearly, the most distributable case is choosing a single neuron for each
partition (i.e., Ck = {k} and Nc = M ), which results in the smallest sublayers to retrain.

4.2

Implementing the Convex Program

As discussed earlier, Net-Trim implementation requires addressing optimizations of the form

U ∥U ∥1
min

s.t.

⎧⎪⎪
⎨
⎪⎪⎩

∑
m,p∶ ym,p>0

mxp − ym,p)

(u⊺
u⊺
mxp ≤ vm,p

2

≤ (cid:15)2

f or m, p ∶ ym,p = 0

,

(24)

where U = [u1, ⋯, uM ] ∈ RN ×M , X = [x1, ⋯, xP ] ∈ RN ×P , Y = [ym,p] ∈ RM ×P and V =
[vm,p] ∈ RM ×P . By the construction of the problem, all elements of Y are non-negative. In
this section we represent (24) in a matrix form, which can be fed into standard quadratically
constrained solvers. For this purpose we try to rewrite (24) in terms of

u = vec(U ) ∈ RM N ,

where the vec(.) operator converts U into a vector of length M N by stacking its columns on
top of one another. Also corresponding to the subscript index sets {(m, p) ∶ ym,p > 0} and
{(m, p) ∶ ym,p = 0} we deﬁne the complement linear index sets
Ω = {(m − 1)P + p ∶ ym,p > 0}, Ωc

= {(m − 1)P + p ∶ ym,p = 0}.

Denoting I M as the identity matrix of size M ×M , using basic properties of the Kronecker

product it is straightforward to verify that

mxp = u⊺
u⊺

(I M ⊗ X)∶,(m−1)P +p .

Basically, u⊺
Subsequently, denoting y = vec(Y ⊺

mxp is the inner product between vec(U ) and column (m − 1)P + p of I M ⊗ X.
), we can rewrite (24) in terms of u as

) and v = vec(V ⊺

where

and

min
u

∥u∥1

s.t. {

u⊺Qu + 2q⊺u ≤ ˜(cid:15)
P u ⪯ c

,

Q = (I M ⊗ X)∶,Ω (I M ⊗ X)

⊺
∶,Ω ,

q = − (I M ⊗ X)∶,Ω yΩ,

˜(cid:15) = (cid:15)2

− y⊺

ΩyΩ

P = (I M ⊗ X)

⊺
∶,Ωc ,

c = vΩc.

17

(25)

(26)

(27)

Using the formulation above allows us to cast (24) as (25), where the unknown is a vector
instead of a matrix.

We can apply an additional change of variable to make (25) adaptable to standard quadrat-
ically constrained convex solvers. For this purpose we deﬁne a new vector ˜u = [u+; −u−
],
where u−

= min(u, 0). This variable change naturally yields
∥u∥1 = 1⊺ ˜u.

u = [I, −I]˜u,

The convex program (25) is now cast as the quadratic program

1⊺ ˜u s.t.

min
˜u

˜u⊺ ˜Q˜u + 2˜q⊺ ˜u ≤ ˜(cid:15)
˜P ˜u ⪯ c
˜u ⪰ 0

⎧⎪⎪⎪
⎨
⎪⎪⎪⎩

,

(28)

where

˜Q = [

1
−1

−1
1 ] ⊗ Q,

˜q = [

q
−q] ,

˜P = [P −P ] .

Once ˜u∗, the solution to (31) is found, we can obtain u∗ (the solution to (25)) through the
= [I, −I]˜u∗. Reshaping u∗ to a matrix of size N × M ultimately returns U ∗, the
relation u∗
matrix solution to (24).

As an alternative implementation technique, we can solve the Net-Trim in regularized
form. More speciﬁcally, if the quadratic constraint in (24) is brought to the objective via a
regularization parameter λ, the resulting convex program decouples into M smaller programs
of the form

ˆwm = arg min

u

∥u∥1 + λ ∑

p∶ ym,p>0

(u⊺xp − ym,p)

2

s.t. u⊺xp ≤ vm,p,

for p ∶ ym,p = 0, (29)

each recovering a column of ˆW . Such decoupling of the regularized form is computationally
attractive, since it makes the trimming task extremely distributable among parallel processing
units by recovering each column of ˆW on a separate unit.

We can formulate the program in a standard form by introducing the index sets

Ωm = {p ∶ ym,p > 0}, Ωc

m = {p ∶ ym,p = 0}.

Denoting the m-th row of Y by y⊺
rewrite (29) in terms of u as

m and the m-th row of V by v⊺

m, one can equivalently

min
u

∥u∥1 + u⊺Qmu + 2q⊺

mu s.t. P mu ⪯ cm,

(30)

where

Qm = λX ∶,ΩmX ⊺

∶,Ωm

,

qm = −λX ∶,ΩmymΩm = −λXym, P m = X ⊺

∶,Ωc
m

,

cm = vmΩc

m

.

(31)

18

Using a similar variable change as ˜u = [u+; −u−
standard quadratic program

], the convex program (31) is now cast as the

min
˜u

˜u⊺ ˜Qm ˜u + (1 + 2˜qm)

⊺ ˜u s.t.

˜P m
−I ] ˜u ⪯ [

[

cm
0 ] ,

(32)

where

˜Qm = [

1
−1

−1
1 ] ⊗ Qm,

˜qm = [

qm
−qm

] ,

˜P m = [P m −P m] .

Aside from the variety of convex solvers that can be used to address (32), we are speciﬁcally
interested in using the alternating direction method of multipliers (ADMM). In fact the
main motivation to translate (29) into (32) is the availability of ADMM implementations for
problems in the form of (32) that are reasonably fast and scalable (e.g., see [13]). We have
made the implementation of regularized Net-Trim publicly available online2.

5 Experiments

In this section we present some learning and retraining examples to highlight the performance
of the Net-Trim. To train our networks we use the H2O package for deep learning [3, 8], which
is equipped with the well-known pruning and regularizing tools such as the dropout and (cid:96)1-
penalty. To address the Net-Trim in the constrained form, standard quadratic solvers such as
the IBM ILOG CPLEX [11] and Gurobi [16] can be employed. For large scale experiments
using the ADMM solver, the reader is referred to the presentation of this work at NIPS 2017.

5.1 Basic Classiﬁcation: Data Points on Nested Spirals

For a better demonstration of the Net-trim performance in terms of model reduction, mean
accuracy and cascade vs. parallel retraining frameworks, here we focus on a low dimensional
dataset. We speciﬁcally look into the classiﬁcation of two set of points lying on nested spirals
as shown in Figure 3(a). The dataset is embedded into the H2O package and publicly available
along with the module.

As an initial experiment, we consider a network of size 2⋅200⋅200⋅2, which indicates the use
of two hidden layers of 200 neurons each, i.e., W1 ∈ R2×200, W2 ∈ R200×200 and W3 ∈ R200×2.
After training the model, a contour plot of the soft-max outcome, indicating the classiﬁer, is
depicted in Figure 3(b). We apply the cascade Net-Trim for (cid:15) = 0.01×∥Y (1)
∥F (the network is
not link normalized), γ = 1.1 and the ﬁnal risk coeﬃcient κ = 0.35. To evaluate the diﬀerence
2The code for the regularized Net-Trim implementation using the ADMM scheme can be accessed online

at: https://github.com/DNNToolBox/Net-Trim-v1

19

(c)

(a)

(b)

⇒

(e)

Figure 3: Classifying two set of data points on nested spirals; (a) the points corresponding to each
class with diﬀerent colors; (b) the soft-max contour (0.5 level-set) representing the neural net classiﬁer;
(c) the classiﬁer after applying the Net-Trim (d) a plot of the network weights corresponding to the
last layer, before (on the left side) and after (on the right side) retraining

between the network output before and after retraining, we deﬁne the relative discrepancy

(cid:15)rd =

∥Z −

ˆZ∥F

∥Z∥F

,

(33)

where Z = W3Y (2) and ˆZ =
ˆW3 ˆY (2) are the network outcomes before the soft-max operation.
In this case (cid:15)rd = 0.046. The classiﬁer after retraining is presented in Figure 3(c), which shows
minor diﬀerence with the original classiﬁer in panel (b). The number of nonzero elements in
W1, W2 and W3 are 397, 39770 and 399, respectively. After retraining, the active entries in
ˆW1, ˆW2 and ˆW3 reduce to 362, 2663 and 131 elements, respectively. Basically, at the expense
of a slight model discrepancy, a signiﬁcant reduction in the model complexity is achieved.
Figures 1(a) and 3(e) compare the cardinalities of the second and third layer weights before
and after retraining.

As a second experiment, we train the neural network with dropout and (cid:96)1 penalty to
produce a readily simple model. The number of nonzero elements in W1, W2 and W3 turn

20

-1.5-1-0.500.511.5-1.5-1-0.500.511.5-1.5-1-0.500.511.5-1.5-1-0.500.511.5-1.5-1-0.500.511.5-1.5-1-0.500.511.50100200300400-3-2-10120100200300400-50510o
i
t
a
r

y
t
i
s
r
a
p
s

o
i
t
a
r

y
t
i
s
r
a
p
s

(cid:15)rd
(a)

(cid:15)rd
(d)

(cid:15)rd
(b)

(cid:15)rd
(e)

(cid:15)rd
(c)

(cid:15)rd
(f)

Figure 4: Sparsity ratio as a function of overall network relative mismatch for the cascade (ﬁrst row)
and parallel (second row) schemes

out to be 319, 6554 and 304, respectively. Using a similar (cid:15) as the ﬁrst experiment, we apply
the cascade Net-Trim, which produces a retrained model with (cid:15)rd = 0.0183 (the classiﬁers are
visually identical and not shown here). The number of active entries in ˆW1, ˆW2 and ˆW3
are 189, 929 and 84, respectively. Despite the use of model reduction tools (dropout and (cid:96)1
penalty) in the training phase, the Net-Trim yet zeros out a large portion of the weights in
the retraining phase. The second layer weight-matrix densities before and after retraining are
visually comparable in Figure 1(c).

We next perform a more extensive experiment to evaluate the performance of the cascade
Net-Trim against the parallel version. Using the spiral data, we train three networks each
with two hidden layers of sizes 50 ⋅ 50, 75 ⋅ 75 and 100 ⋅ 100. For the parallel retraining, we
ﬁx a value of (cid:15), retrain each model 20 times and record the mean layer sparsity across these
experiments (the averaging among 20 experiments is to remove the bias of local minima in
the training phase). A similar process is repeated for the cascade case, where we consistently
use γ = 1.1 and κ = 1. We can sweep the values of (cid:15) in a range to generate a class of curves
relating the network relative discrepancy to each layer mean sparsity ratio, as presented in
Figure 4. Here, sparsity ratio refers to the ratio of active elements to the total number of
elements in the weight matrix.

A natural observation from the decreasing curves is allowing more discrepancy leads to
more level of sparsity. We also observe that for a constant discrepancy (cid:15)rd, the cascade
Net-Trim is capable of generating rather sparser networks. The contrast in sparsity is more

21

0.050.10.150.20.250.300.20.40.60.81Layer 1: 2x50Layer 1: 2x75Layer 1: 2x1000.050.10.150.20.250.300.20.40.60.81Layer 2: 50x50Layer 2: 75x75Layer 2: 100x1000.050.10.150.20.250.300.20.40.60.81Layer 3: 50x2Layer 3: 75x2Layer 3: 100x20.050.10.150.200.20.40.60.81Layer 1: 2x50Layer 1: 2x75Layer 1: 2x1000.050.10.150.200.20.40.60.81Layer 2: 50x50Layer 2: 75x75Layer 2: 100x1000.050.10.150.200.20.40.60.81Layer 3: 50x2Layer 3: 75x2Layer 3: 100x2apparent in the third layer (panel (c) vs. panel (f)). We would like to note that using κ < 1
makes the contrast even more tangible, however for the purpose of long-run simulations, here
we chose κ = 1 to avoid any possible infeasibility interruptions. Finally, an interesting obser-
vation is the rather dense retrained matrices associated with the ﬁrst layer. Apparently, less
pruning takes place at the ﬁrst layer to maximally bring the information and data structure
into the network.

In Table 1 we have listed some retraining scenarios for networks of diﬀerent sizes trained
with dropout. Across all the experiments, we have used the cascade Net-Trim to retrain the
networks and chosen (cid:15) small enough to warrant an overall relative discrepancy below 0.02.
On the right side of the table, the number of active elements for each layer is reported, which
indicates the signiﬁcant model reduction for a negligible discrepancy.

Table 1: Number of active elements within each layer, before and after Net-Trim for a network
trained with Dropout

Trained Network

Network Size Layer 1 Layer 2 Layer 3
2 ⋅ 50 ⋅ 50 ⋅ 2
2 ⋅ 75 ⋅ 75 ⋅ 2
2 ⋅ 125 ⋅ 125 ⋅ 2
2 ⋅ 175 ⋅ 175 ⋅ 2
2 ⋅ 200 ⋅ 200 ⋅ 2

2483
5594
15529
30395
39668

99
149
250
349
400

100
150
250
350
399

Net-Trim Retrained Network
Layer 3
Layer 1 Layer 2
54
72
96
116
113

467
710
3477
1743
1991

98
149
247
348
399

Table 2: Number of active elements within each layer, before and after Net-Trim for a network
trained with Dropout and an (cid:96)1-penalty

Trained Network

Network Size Layer 1 Layer 2 Layer 3
2 ⋅ 50 ⋅ 50 ⋅ 2
2 ⋅ 75 ⋅ 75 ⋅ 2
2 ⋅ 125 ⋅ 125 ⋅ 2
2 ⋅ 175 ⋅ 175 ⋅ 2
2 ⋅ 200 ⋅ 200 ⋅ 2

1604
2867
5316
9580
8700

95
135
226
320
382

58
96
126
171
134

Net-Trim Retrained Network
Layer 3
Layer 1 Layer 2
46
62
60
61
70

342
651
751
906
606

54
90
95
136
109

Table 2 reports another set of sample experiments, where dropout and (cid:96)1 penalty are
simultaneously employed in the training phase to prune the network. Going through a simi-
lar cascade retraining, while keeping (cid:15)rd below 0.02, we have reported the level of additional
model reduction that can be achieved. Basically, the Net-Trim post processing module uses
the trained model (regardless of how it is trained) to further reduce its complexity. A com-
parison of the network weight histograms before and after retraining may better highlight the

22

⇒

(a)

⇒

(b)

Figure 5: The weight histogram of the middle layer before and after retraining; (a) the middle layer
histogram of a 2 ⋅ 50 ⋅ 50 ⋅ 2 network trained with dropout (left) vs. the histogram after Net-Trim
(right); (b) similar plots as panel (a) for a 2 ⋅ 200 ⋅ 200 ⋅ 2 network

Net-Trim performance. Figure 5 compares the middle layer weight histograms for a pair of
experiments reported in Table 1.

5.2 Character Recognition

In this section we apply Net-Trim to the problem of classifying hand-written digits. For
this purpose we use a fraction of the mixed national institute of standards and technology
(MNIST) dataset. The set contains 60,000 training samples and 10,000 test instances. This
classiﬁcation problem has been well-studied in the literature, and error rates of almost 0.2%
have been achieved using the full training set [24]. However, here we focus on the problem of
training the models with limited samples (a fraction of the data) and show how the Net-Trim
stabilizes this process.

For this problem we apply the parallel Net-Trim. Also, in order to maximally distribute
the problem among independent computational nodes, we use the PCN Net-Trim with one
target node. Basically, at every layer, an individual Net-Trim program is solved for every
output neuron. We consider four experiments with 200, 300, 500 and 1000 training samples.
For every sample set of size P , we simply select the ﬁrst P rows of the original MNIST training
set. Similarly, our test corresponds to the ﬁrst 1000 samples of the original test set.

For the experiments with 200 and 300 training samples we use two hidden layers of each
1024 neurons. For the cases of 500 and 1000 samples, three hidden layers of each 1024 neurons
are used. Table 3 summarizes the test accuracies after the initial training.

To retrain the perceptron corresponding to neuron m of layer (cid:96), we use (cid:15) = (cid:15)r∥Y ((cid:96))

m,∶∥, where
(cid:15)r ∈ {0.002, 0.005, 0.01}. With such individual neuron retraining the layer response would also

23

-15-10-505050100-40-30-20-100100100020003000-15-10-5050100020003000-30-20-1001001234×104Table 3: A summary of the network architecture, sparsity ratio (SR) and classiﬁcation accuracy
(CA) for various training sample sizes

Sample Size (P )
Hidden Units
Retraining
SR (Layer 1)
SR (Layer 2)
SR (Layer 3)
SR (Layer 4)
CA
CA (5% noise)
CA (10% noise)

200
1024 ⋅ 1024

300
1024 ⋅ 1024

Before After Before After Before
0.71
0.98
0.99
–
82.2
75.7
61.5

0.68
0.98
0.99
–
77.5
62.7
46.1

0.26
0.17
0.26
–
82.6
78.9
70.4

0.19
0.16
0.18
–
77.7
70.0
55.0

500
1024 ⋅ 1024 ⋅ 1024
After
0.39
0.24
0.20
0.31
86.5
80.5
65.5

0.73
0.98
0.98
0.99
86.1
78.8
62.1

1000
1024 ⋅ 1024 ⋅ 1024
Before After
0.57
0.27
0.29
0.43
89.8
74.2
52.9

0.76
0.98
0.98
0.99
89.2
62.1
39.2

−

ˆY ((cid:96))

∥F ≤ (cid:15)r∥Y ((cid:96))

obey ∥Y ((cid:96))
∥F . We used three (cid:15)r values to run a simple cross-validation
test and ﬁnd a combination of the retrained layers which produces a better network in terms
of accuracy and complexity. For the 200 and 300-sample networks (cid:15)r = 0.005 across all three
layers produced the best networks. For the 1000-sample network using (cid:15)r = 0.01 across all the
layers, and for the 500-sample network the (cid:15)r sequence (0.005, 0.01, 0.005, 0.01) for the layers
one through four seemed the better combinations.

Table 3 also reports the layer-wise sparsity ratio before and after retraining. We can
observe the substantial improvement in sparsity ratio gained after the retraining. To see how
this reduction in the model helps with the classiﬁcation accuracy, we report the identiﬁcation
success rate for the test data as well as the cases of data being contaminated with 5% and
10% Gaussian noise. We can see that the pruning scheme improves the accuracy especially
when the data are noisy. Basically, as expected by reducing the model complexity the network
becomes more robust to the outliers and noisy samples.

For a deeper study of the improvement Net-Trim brings to handling noisy data, in Figure
6 we have plotted the classiﬁcation accuracy against the noise level for the four trained
networks. The Net-Trim improvement in accuracy becomes more noticeable as the noise level
in the data increases. Finally, Figure 7 reports a selected set of the test samples, where
the original network prediction was false and reducing the network complexity through the
Net-Trim corrected the prediction.

6 Discussion and Conclusion

In linear regression problems, a well-known (cid:96)1 regularization tool to promote sparsity on the
resulting weights is the LASSO (least absolute shrinkage and selection operator [22]). The
convexity of the initial regression problem allows us to conveniently add the (cid:96)1 penalty to the

24

)

%

(

y
c
a
r
u
c
c
a

)

%

(

y
c
a
r
u
c
c
a

noise percentage
(a)

)

%

(

y
c
a
r
u
c
c
a

)

%

(

y
c
a
r
u
c
c
a

noise percentage
(b)

noise percentage
(c)

noise percentage
(d)

Figure 6: Classiﬁcation accuracy for various noise levels before and after Net-Trim retraining: (a)
200 training samples; (b) 300 training samples; (c) 500 training samples; (d) 1000 training samples

Figure 7: Examples of digits at diﬀerent noise levels, which are misidentiﬁed using the initial trained
network (with 1000 samples), but correctly identiﬁed after applying Net-Trim; ﬁrst and second row
initial identiﬁcations, left to right are 5, 6, 8, 8, 7, 7, 3, 6; the correct identiﬁed digits after applying
the Net-Trim are 8, 5, 3, 3, 9, 9, 2, 8

25

0246810121420406080OriginalRetrained024681012145060708090OriginalRetrained024681012145060708090OriginalRetrained0246810121420406080100OriginalRetrainedproblem. The convex structure of the (cid:96)1-constrained problem helps relating the number of
training samples to the active features in the model, as mainly presented in the compressed
sensing literature [12]. In the neural networks, such process is not easy as the initial learning
problem is highly non-convex. In this case, not much could be said in terms of characterizing
the minimizers and the quality of the resulting models after adding an (cid:96)1 regularizer.

By taking a layer-wise modeling strategy in Net-Trim, we have been able to overcome the
non-convexity issue and present our results in an algorithmic and analytical way. The post-
processing nature of the problem allows the algorithm to be conveniently blended with the
state-of-the-art learning techniques in neural networks. Basically, regardless of the process
being taken to train the model, Net-Trim can be considered as an additional post-processing
step to reduce the model order and further improve the stability and prediction accuracy.

Similar to the LASSO, the proposed framework can handle both over and under-determined
training cases and basically prevent networks of arbitrary size from overﬁtting. As expected,
Net-Trim performs a more considerable pruning job on redundant networks, where the model
degrees of freedom are more than those imposed by the data.

The Net-Trim pruning technique is not only limited to the parallel and cascade frameworks
presented in this paper. As the method follows a layer-wise retraining strategy, it can be
applied partially to a selected number of layers in the network. Even a hybrid retraining
framework applying the parallel and cascade schemes to diﬀerent subsets of the layers could
be generally considered. To determine the optimal free parameters in the retrained model,
cross validation methods can be used once the retraining scheme is set up.

From an optimization point of view, Net-Trim includes a series of constrained quadratic
programs, which can be handled via any standard convex solver. A desirable characteristic of
the resulting programs is the availability of at least one feasible point, which can be employed
as an initialization for some solvers or minimization methods.

While the focus of this paper was solely an (cid:96)1 regularization, with some slight modiﬁcation
to the formulation, other regularizations such as the (cid:96)2 and max norm, or a combination of
convex penalties such as (cid:96)1 + (cid:96)2 may be considered. In the majority of the cases, a slight
change to the objective (e.g., replacing ∥U ∥1 with ∥U ∥F or ∥U ∥1 + λ∥U ∥F ) brings the desired
structure to the retrained models.

In the case of big data, where the network size and the training samples are large, Net-
Trim presents a good compatibility. As stated in the paper, not only the parallel scheme
allows retraining the layers individually and through independent computational resources,
but also within a single layer retraining framework we can consider a PCN approach to cast
the problem as a series of smaller independent programs. Yet, in cases that the number of
training samples are huge, an additional reduction in computational load would be to use the
validation set instead of the training set in the Net-Trim. In a standard training phase, the
update on the network weights stops once the prediction error within a reference validation
set starts to increase. This set is often much smaller than the original training set and may
be replaced with the training data in Net-Trim.

26

7 Appendix: Proofs of the Main Results

7.1 Proof of Proposition 1
If ˆW = [ ˆw1, ⋯, ˆwM ] is a solution to (6), then the feasibility of the solution requires
( ˆw⊺

mxp − ym,p)

≤ (cid:15)2

2

∑
m,p ∶ ym,p>0

and

Consider ˆY = [ˆym,p], then

ˆw⊺

mxp ≤ 0

if

ym,p = 0.

∥Y −

ˆY ∥

2
F =

M
∑
m=1

P
∑
p=1

(ym,p − ˆym,p)

2

(34)

(35)

= ∑

m,p ∶ ym,p>0

(ym,p − ˆym,p)

2

+ ∑

m,p ∶ ym,p=0

(ym,p − ˆym,p)

= ∑

m,p ∶ ym,p>0

(ym,p − ( ˆw⊺

mxp)

+

2
)

=

∑

m,p ∶ ym,p>0, ˆw⊺

mxp>0

(ym,p − ˆw⊺

2
mxp)

+

∑

m,p ∶ ym,p>0, ˆw⊺

mxp≤0

2

y2
m,p.

(36)

Here since ym,p ≥ 0, the second equality is partitioned into two summations separated by the
values of ym,p being zero or strictly greater than zero. The second resulting sum vanishes in
the third equality since from (35), ˆym,p = max( ˆw⊺
mxp, 0) = 0 when ym,p = 0. For the second
term in (36) we use the basic algebraic identity

2

. (37)

.

(38)

∑

m,p ∶ ym,p>0, ˆw⊺

mxp≤0

y2
m,p =

∑

m,p ∶ ym,p>0, ˆw⊺

mxp≤0

(ym,p − ˆw⊺

2
mxp)

+ 2ym,p( ˆw⊺

mxp) − ( ˆw⊺

mxp)

Combining (37) and (36) results in

∥Y −

ˆY ∥

2
F = ∑

m,p ∶ ym,p>0

(ym,p − ˆw⊺

2
mxp)

+
m,p ∶ ym,p>0, ˆw⊺

∑

mxp≤0

2ym,p( ˆw⊺

mxp) − ( ˆw⊺

2
mxp)

From (34), the ﬁrst sum in (38) is upper bounded by (cid:15)2. In addition,

2ym,p( ˆw⊺

mxp) − ( ˆw⊺

mxp)

2

≤ 0,

when ym,p > 0 and ˆw⊺

mxp ≤ 0, which together yield ∥Y −

ˆY ∥

F ≤ (cid:15)2 as expected.
2

27

7.2 Proof of Theorem 1

We prove the theorem by induction. For (cid:96) = 1, the claim holds as a direct result of Proposition
1. Now suppose the claim holds up to the ((cid:96) − 1)-th layer,

((cid:96)−1)

ˆY

∥

− Y ((cid:96)−1)

∥

(cid:96)−1
∑
j=1

εj,

≤

F

(39)

we show that (9) will hold for the layer (cid:96) as well. The outcome of the (cid:96)-th layer before and
after retraining obeys

m,p = (w⊺
y((cid:96))

my((cid:96)−1)
p

)

+

and

m,p = ( ˆw⊺
ˆy((cid:96))

m ˆy((cid:96)−1)
p

)

+

,

(40)

m,p and ˆy((cid:96))

where y((cid:96))
m,p are entries of Y ((cid:96)) and ˆY ((cid:96)), the m-th columns of W(cid:96) and ˆW(cid:96) are denoted
by wm and ˆwm (we have dropped the (cid:96) subscripts in the column notation for brevity), and
the p-th columns of Y ((cid:96)−1) and ˆY ((cid:96)−1) are denoted by y((cid:96)−1)
. We also deﬁne the
quantities

and ˆy((cid:96)−1)
p

p

m,p = ( ˆw⊺
˜y((cid:96))

my((cid:96)−1)
p

)

+

,

which form a matrix ˜Y

((cid:96))

. From Proposition 1, we have

((cid:96))

˜Y

∥

− Y ((cid:96))

∥

≤ (cid:15)(cid:96).

F

On the other hand,

+
)
+ ˆw⊺

m,p = ( ˆw⊺
ˆy((cid:96))
= ( ˆw⊺
≤ ( ˆw⊺
≤ ˜y((cid:96))

m ˆy((cid:96)−1)
p
my((cid:96)−1)
p
my((cid:96)−1)
p
m,p + ∣ ˆw⊺

p

p

− y((cid:96)−1)

m (ˆy((cid:96)−1)
+
+ ( ˆw⊺
)
m (ˆy((cid:96)−1)

+
))
m (ˆy((cid:96)−1)
− y((cid:96)−1)
− y((cid:96)−1)
)∣ ,

p

p

p

p

(41)

(42)

+
))

where in the last two inequalities we used the sub-additivity of the max(., 0) function and the
inequality max(x, 0) ≤ ∣x∣. In a similar fashion we have

+
)

+

m,p = ( ˆw⊺
˜y((cid:96))
≤ ( ˆw⊺
≤ ˆy((cid:96))

my((cid:96)−1)
p
m ˆy((cid:96)−1)
p
m,p + ∣ ˆw⊺

+ ( ˆw⊺
)
m (ˆy((cid:96)−1)

− ˆy((cid:96)−1)
m (y((cid:96)−1)
− y((cid:96)−1)
)∣ ,

p

p

p

p

+
))

28

which together with (42) asserts that ∣ˆy((cid:96))

m,p − ˜y((cid:96))

m,p∣ ≤ ∣ ˆw⊺

m(ˆy((cid:96)−1)

p

− y((cid:96)−1)

p

)∣ or

ˆY

∥

((cid:96))

((cid:96))

˜Y

−

≤ ∥

∥

F

ˆW

⊺
(cid:96) (

ˆY

((cid:96)−1)

− Y ((cid:96)−1)

)∥

F

≤ ∥

ˆW(cid:96)∥F ∥

ˆY

((cid:96)−1)

− Y ((cid:96)−1)

∥

.

F

(43)

As ˆW(cid:96) is the minimizer of (8) and W(cid:96) is a feasible point (i.e., W(cid:96) ∈ C(cid:15)(cid:96)(Y ((cid:96)−1), Y ((cid:96)), 0)), we
have

ˆW(cid:96)∥F ≤ ∥

ˆW(cid:96)∥1 ≤ ∥W(cid:96)∥1 = 1,

∥

(44)

which with reference to (43) yields

ˆY
∥

((cid:96))

((cid:96))

˜Y

−

ˆY

≤ ∥

∥

F

((cid:96)−1)

− Y ((cid:96)−1)

≤

∥
F

(cid:96)−1
∑
j=1

εj.

Finally, the induction proof is completed by applying the triangle inequality and then using
(41),

((cid:96))

ˆY

∥

− Y ((cid:96))

∥

F

ˆY

≤ ∥

((cid:96))

((cid:96))

˜Y

−

∥

F

((cid:96))

˜Y

+ ∥

− Y ((cid:96))

∥

(cid:96)
∑
j=1

εj.

≤

F

7.3 Proof of Theorem 2

For (cid:96) ≥ 2 we relate the upper-bound of ∥
struction of the network:

ˆY ((cid:96))

− Y ((cid:96))

∥F to ∥

ˆY ((cid:96)−1)

− Y ((cid:96)−1)

∥F . By the con-

((cid:96))

ˆY

∥

− Y ((cid:96))

∥

2

F

N(cid:96)
∑
m=1

P
∑
p=1

=

(( ˆw⊺

m ˆy((cid:96)−1)
p

)

+

− (w⊺

my((cid:96)−1)
p

+
)

2
)

(( ˆw⊺

m ˆy((cid:96)−1)
p

)

+

− w⊺

my((cid:96)−1)
p

2
)

= ∑
m,p ∶ y((cid:96))

m,p>0

(( ˆw⊺

m ˆy((cid:96)−1)
p

)

+

2
)

,

(45)

+ ∑
m,p ∶ y((cid:96))

m,p=0

where the m-th columns of W(cid:96) and ˆW(cid:96) are denoted by wm and ˆwm, respectively (we have
dropped the (cid:96) subscripts in the column notation for brevity), and the p-th columns of Y ((cid:96))
and ˆY
+. For the ﬁrst
term in (45) we have

are denoted by y((cid:96)−1)

. Also, as before y((cid:96))

and ˆy((cid:96)−1)
p

m,p = (w⊺

my((cid:96)−1)

((cid:96))

)

p

p

(( ˆw⊺

m ˆy((cid:96)−1)
p

)

+

− w⊺

my((cid:96)−1)
p

2
)

=

∑
m,p ∶ y((cid:96))

m,p>0

∑
m,p>0, ˆw⊺

m,p ∶ y((cid:96))

m ˆy((cid:96)−1)

p

m ˆy((cid:96)−1)
p

( ˆw⊺
>0

− w⊺

my((cid:96)−1)
p

2
)

+

∑
m,p>0, ˆw⊺

m,p ∶ y((cid:96))

m ˆy((cid:96)−1)

p

my((cid:96)−1)
p

(w⊺
≤0

2
)

.

(46)

29

However, for the elements of the set {(m, p) ∶ y((cid:96))

m,p > 0, ˆw⊺

m ˆy((cid:96)−1)
p

≤ 0}:

(w⊺

my((cid:96)−1)
p

)

2

= ( ˆw⊺
≤ ( ˆw⊺

m ˆy((cid:96)−1)
p

m ˆy((cid:96)−1)
p

− w⊺
− w⊺

my((cid:96)−1)
p

my((cid:96)−1)
p

2
)
2
)

+ 2 ( ˆw⊺

m ˆy((cid:96)−1)
p

) (w⊺

my((cid:96)−1)
p

) − ( ˆw⊺

m ˆy((cid:96)−1)
p

)

2

,

using which in (46) yields

(( ˆw⊺

m ˆy((cid:96)−1)
p

+
)

− w⊺

my((cid:96)−1)
p

)

2

∑
m,p ∶ y((cid:96))

m,p>0

( ˆw⊺

m ˆy((cid:96)−1)
p

− w⊺

my((cid:96)−1)
p

)

2

≤ ∑
m,p ∶ y((cid:96))

m,p>0

(w⊺

m ˆy((cid:96)−1)
p

− w⊺

my((cid:96)−1)
p

2
)

(47)

≤ γ(cid:96) ∑
m,p ∶ y((cid:96))
= (cid:15)2
(cid:96) .

m,p>0

Here, the second inequality is a direct result of the feasibility condition

ˆY
ˆW(cid:96) ∈ C(cid:15)(cid:96) (

((cid:96)−1)

, Y ((cid:96)), W(cid:96)

ˆY

((cid:96)−1)

) .

A second outcome of the feasibility is

ˆw⊺

m ˆy((cid:96)−1)
p

≤ w⊺

m ˆy((cid:96)−1)
p

,

(48)

(49)

for any pair (m, p) that obeys y((cid:96))
≤ 0). We can apply max(., 0)
(as an increasing and positive function) to both sides of (49) and use it to bound the second
term in (45) as follows:

m,p = 0 (or equivalently, w⊺

my((cid:96)−1)

p

(( ˆw⊺

m ˆy((cid:96)−1)
p

2

+
)

)

∑
m,p ∶ y((cid:96))

m,p=0

((w⊺

m ˆy((cid:96)−1)
p

)

+

2
)

((w⊺

my((cid:96)−1)
p

+ w⊺

m ˆy((cid:96)−1)
p

− w⊺

my((cid:96)−1)
p

2

+
)

)

((w⊺

my((cid:96)−1)
p

)

+

+ (w⊺

m ˆy((cid:96)−1)
p

− w⊺

my((cid:96)−1)
p

2

+
)

)

((w⊺

m ˆy((cid:96)−1)
p

− w⊺

my((cid:96)−1)
p

+
)

2
)

(w⊺

m ˆy((cid:96)−1)
p

− w⊺

my((cid:96)−1)
p

)

2

.

(50)

≤ ∑
m,p ∶ y((cid:96))

m,p=0

= ∑
m,p ∶ y((cid:96))

m,p=0

≤ ∑
m,p ∶ y((cid:96))

m,p=0

= ∑
m,p ∶ y((cid:96))

m,p=0

≤ ∑
m,p ∶ y((cid:96))

m,p=0

30

The ﬁrst and second terms in (45) are bounded via (47) and (50) and therefore

((cid:96))

ˆY
∥

− Y ((cid:96))

∥

2

F

(w⊺

m ˆy((cid:96)−1)
p

− w⊺

my((cid:96)−1)
p

)

2

(w⊺

m ˆy((cid:96)−1)
p

− w⊺

my((cid:96)−1)
p

)

2

+ ∑
m,p ∶ y((cid:96))

m,p=0

≤ γ(cid:96) ∑
m,p ∶ y((cid:96))

≤ γ(cid:96) ∑
m,p

m,p>0
(w⊺

m ˆy((cid:96)−1)
p

− w⊺

my((cid:96)−1)
p

)

2

2

F
2

= γ(cid:96) ∥W ⊺
(cid:96) (

ˆY

((cid:96)−1)

− Y ((cid:96)−1)

)∥

≤ γ(cid:96) ∥W(cid:96)∥

2
F ∥

ˆY

((cid:96)−1)

− Y ((cid:96)−1)

((cid:96)−1)

= γ(cid:96) ∥

ˆY

− Y ((cid:96)−1)

∥

2

F

.

∥

F

(51)

Based on Proposition 1, the outcome of the ﬁrst layer obeys ∥
together with (51) conﬁrm (16).

ˆY (1)

− Y (1)

F ≤ (cid:15)2
2

1, which

∥

7.4 Proof of Proposition 2

For part (a), from Theorem 1 we have ∥

(L−1)

ˆY

− Y (L−1)

∥F ≤ ∑

L−1
(cid:96)=1 (cid:15)(cid:96). Furthermore,

(L)

ˆY

∥

− Y (L)

∥

F

⊺
ˆW
L

ˆY

= ∥

(L−1)

− Y (L)

∥

⊺
ˆW
L

ˆY

≤ ∥

(L−1)

−

ˆW

F
⊺
L Y (L−1)

ˆW

⊺
L Y (L−1)

− Y (L)

∥F

+ ∥

∥

F

ˆW

⊺
L ∥F ∥

ˆY

≤ ∥

(L−1)

− Y (L−1)

∥

+ (cid:15)L

F

L
∑
(cid:96)=1

≤

(cid:15)(cid:96),

where for the last inequality we used a similar chain of inequalities as (44).

31

To prove part (b), for the ﬁrst L−1 layers, ∥

(L−1)

ˆY

−Y (L−1)

∥F ≤ (cid:15)1

√

L−1
(cid:96)=1 γ(cid:96), and therefore

∏

(L)

ˆY

∥

− Y (L)

∥

F

⊺
ˆW
L

ˆY

= ∥

(L−1)

√γL ∥W ⊺

L

ˆY

≤

− Y (L)
(L−1)

∥

F

− W ⊺

L Y (L−1)

∥

F

√γL ∥WL∥F ∥
ˆY

≤

(L−1)

− Y (L−1)

∥

F

≤

√γL ∥
ˆY
¿
`
`(cid:192)

L
∏
(cid:96)=1

≤ (cid:15)1

(L−1)

− Y (L−1)

∥

F

γ(cid:96).

Here, the ﬁrst inequality is thanks to (18) and (19).

7.5 Proof of Proposition 3

Since program (21) has only aﬃne inequality constraints, the Karush–Kuhn–Tucker (KKT)
optimality conditions give us necessary and suﬃcient conditions for an optimal solution. The
pair (w∗, s∗

) is optimal if and only if there exists η ∈ Rn2 and ν such that

ηks∗

k = 0, k = 1, . . . , n2,
ηk ≥ 0, k = 1, . . . , n2,
⊺

∂∥w∗
η

∥1

] .

˜X

ν ∈ [

∥1 denotes the subgradient of the (cid:96)1 norm evaluated at w∗; the last expression
Above, ∂∥w∗
above means that the ﬁrst n1 entries of ˜X
ν must match the sign of w∗
(cid:96) for indexes (cid:96) with
(cid:96) /= 0, and must have magnitude no greater than 1 for indexes (cid:96) with w∗
w∗
(cid:96) = 0. The existence
of such (η, ν) is compatible with the existence of a Λ meeting the conditions in (22), by
taking Λ =

We now argue the conditions for uniqueness. Let w∗, s∗, Λ be as above. Suppose (w′, s′

˜X

ν.

)

⊺

⊺

is a feasible point with ∥w′

∥1 = ∥w∗

∥1. Since Λ is in the row space of ˜X, we know that

Λ⊺

w∗
[
s∗

− w′
− s′ ] = 0,
∥1, we must also have Λ⊺

and since Λ⊺
properties stated in (22), the support (set of nonzero entries) ˜Γ of [w∗; s∗

] = ∥w∗

] = ∥w∗

[w∗; s∗

[w′; s′

∥1. Therefore by the
] must

] and [w′; s′

32

be the same. Since these points obey the same equality constraints in the program (21), and
˜X ∶,˜Γ has full column rank, it must be true that [w′; s′

] = [w∗; s∗

].

The interested reader is referred to [1, 2] for examples of more comprehensive uniqueness

proofs for convex problems with both equality and inequality constraints.

7.6 Proof of Theorem 3
For a more convenient notation we use Γ = supp w∗. Also, in all the formulations, sub-matrix
selection precedes the transpose operation, e.g., X ⊺

⊺.

∶,Ω = (X ∶,Ω)

Clearly since the samples are random Gaussians, with probability one the set {p ∶ X ⊺

∶,pw∗

0} is an empty set, and following the notation in (21) and (22), suppcs∗
reference to the setup in Proposition 3

=
= ∅. Also, with

˜X = [

X ⊺
0
∶,Ω
∶,Ωc −I] .
X ⊺

To establish the full column rank property of ˜X ∶,˜Γ for ˜Γ = supp w∗
}, we only
need to show that X Γ,Ω is of full row rank (thanks to the identity block in ˜X). Also, to
satisfy the equality requirements in (22), we need to ﬁnd a vector ξ such that

∪ {N + supp s∗

X Γ,Ω X Γ,Ωc

[

0

−I ] [

ξΩ
ξΩc

sign(w∗
Γ)
0

] = [

] .

(52)

This equation trivially yields ξΩc = 0. In the remainder of the proof we will show that when
P is suﬃciently larger than ∣Γ∣ = s, the smallest eigenvalue of X Γ,ΩX ⊺
Γ,Ω is bounded away
from zero (which automatically establishes the full row rank property for X Γ,Ω). Also, based
on such property, we can select ξΩ to be the least squares solution
ξΩ ≜ X ⊺

Γ,Ω (X Γ,ΩX ⊺

sign(w∗

Γ,Ω)

(53)

Γ),

−1

which satisﬁes the equality condition in (52). To verify the conditions stated in (22) and
complete the proof, we will show that when P is suﬃciently large, with high probability
∥X Γc,ΩξΩ∥∞ < 1. We do this by bounding the probability of failure via the inequality

P {∥X Γc,ΩξΩ∥∞ ≥ 1} ≤ P {∥X Γc,ΩξΩ∥∞ ≥ 1 ∣ ∥ξΩ∥ ≤ τ } + P {∥ξΩ∥ > τ )},

(54)

for some positive τ , which is a simple consequence of

P {E1} = P {E1∣E2} P {E2} + P {E1∣E c

2 } P {E c

2 } ≤ P {E1∣E2} + P {E c

2 } ,

generally holding for two event E1 and E2. Without the ﬁltering of the Ω set, standard
concentration bounds on the least squares solution can help establishing the unique optimality
conditions (e.g., see [9]). Here also, we proceed by bounding each term on the right hand side
of (54) individually, while the bounding process requires taking a diﬀerent path because of
the dependence Ω brings to the resulting random matrices.

33

- Step 1. Bounding P{∥ξΩ∥ > τ }:

By the construction of ξΩ in(53), clearly

∥ξΩ∥

2

= sign(w∗
Γ)

⊺

(X Γ,ΩX ⊺

Γ,Ω)

−1

sign(w∗

Γ).

(55)

Technically speaking, to bound the expression x⊺Ax, where x is a ﬁxed vector and A is a self
adjoint random matrix, we normally need the entries of A to be independent of the elements
in x. While such independence does not hold in (55) (because of the dependence of Ω to the
entries of w∗
Γ), we are still able to proceed with bounding by rewriting sign(w∗
Γ) = Λw∗1,
where

Taking into account the facts that Λw∗ = Λ−1

Λw∗ = diag (sign(w∗
w∗ and w∗

Γ)) .
n ≠ 0 for n ∈ Γ, we have

∥ξΩ∥

2

= 1⊺

(Λw∗X Γ,ΩX ⊺

Γ,ΩΛw∗)

−1

1,

(56)

where now the matrix and vector independence is maintained. The special structure of Λw∗
does not cause a change in the eigenvalues and

eig {Λw∗X Γ,ΩX ⊺

Γ,ΩΛw∗} = eig {X Γ,ΩX ⊺

Γ,Ω} ,

where eig{.} denotes the set of eigenvalues. Now conditioned on X Γ,ΩX ⊺
the magnitude of ξΩ as

Γ,Ω ≻ 0, we can bound

2

∥ξΩ∥

−1

Γ,ΩΛw∗)

= 1⊺
(Λw∗X Γ,ΩX ⊺
≤ λmax ((Λw∗X Γ,ΩX ⊺
= s (λmin (X Γ,ΩX ⊺

Γ,Ω))

−1

1
−1

Γ,ΩΛw∗)

) 1⊺1

,

(57)

where λmax and λmin denote the maximum and minimum eigenvalues. To lower bound
λmin (X Γ,ΩX ⊺
Γ,Ω), we focus on the matrix eigenvalue results associated with the sum of
random matrices. For this purpose, consider the independent sequence of random vectors
P
{xp}
p=1, where each vector contains i.i.d standard normal entries. We are basically interested
in concentration bounds for

λmin

⎛
⎜
⎝

∑
pw∗>0

p ∶ x⊺

xpx⊺
p

.

⎞
⎟
⎠

(58)

When the summands are independent self adjoint random matrices, we can use standard
Bernstein type inequalities to bound the minimum or maximum eigenvalues [23]. However,
as the summands in (58) are dependent in the sense that they all obey x⊺
> 0, such results

pw∗

34

are not directly applicable. To establish the independence, we can look into an equivalent
formulation of (58) as

λmin

P
∑
p=1

⎛
⎝

xpx⊺
p

,

⎞
⎠

(59)

where xp are independently drawn from the distribution

gX (x) =

⎧⎪⎪
⎨
⎪⎪⎩

1√
(2π)s exp (−
1
2 δD(x)

1

2 x⊺x) x⊺w∗
x⊺w∗

> 0
≤ 0

.

s
i=1 δD(xi) denotes the s-dimensional Dirac delta function, and is probabilis-
Here, δD(x) = ∏
tically in charge of returning a zero vector in half of the draws. We are now theoretically able
to apply the following result, brought from [23], to bound the smallest eigenvalue:

Theorem 4. (Matrix Bernstein3) Consider a ﬁnite sequence Zp of independent, random,
self-adjoint matrices with dimension s. Assume that each random matrix satisﬁes

Then, for all t ≤ 0,

E(Zp) = 0,

and λmin(Zp) ≥ R almost surely.

P

⎧⎪⎪
λmin
⎨
⎪⎪⎩

⎛
⎝

∑
p

Zp

⎞
⎠

⎫⎪⎪
≤ t
⎬
⎪⎪⎭

≤ s exp (

−t2
2σ2 + 2Rt/3 ) ,

where σ2

= ∥ ∑p E(Z2

p)∥.

To more conveniently apply Theorem 4, we can use a change of variable which markedly
simpliﬁes the moment calculations required for the Bernstein inequality. For this purpose,
consider R to be a rotation matrix which maps w∗ to the ﬁrst canonical basis [1, 0, ⋯, 0]
∈ Rs.
Since

⊺

⎫⎪⎪
⎬
⎪⎪⎭
we can focus on random vectors up = Rxp which follow the simpler distribution

Rxpx⊺

P
∑
p=1

P
∑
p=1

xpx⊺
p

pR⊺

= eig

⎧⎪⎪
⎨
⎪⎪⎩

⎧⎪⎪
⎨
⎪⎪⎩

⎫⎪⎪
⎬
⎪⎪⎭

eig

,

gU (u) ≜

⎧⎪⎪
⎨
⎪⎪⎩

1√
(2π)s exp (−
1
2 δD(u)

1

2 u⊺u) u1 > 0
u1 ≤ 0

.

Here, u1 denotes the ﬁrst entry of u, and we used the basic property R−1
= R⊺ along with
the rotation invariance of the Dirac delta function to derive gU (u) from gX (x). Using the
Bernstein inequality, we can now summarize everything as the following concentration result
(proved later in the section):

3The original version of the theorem bounds the maximum eigenvalue. The present version can be easily

derived using, λmin(Z) = −λmax(−Z) and P{λmin(∑p Z p) ≤ t} = P{λmax(∑p −Z p) ≥ −t}.

35

(60)

(61)

P
Proposition 4. Consider a sequence of independent {up}
p=1 vectors of length s, where each
vector is drawn from the distribution gU (u) in (61). For all t ≤ 0,
−t2
P (s + 3/2) − t/3 ) .

≤ s exp (

P
∑
p=1

upu⊺
p

(62)

P

≤

⎛
⎝

⎞
⎠

⎫⎪⎪
P
2 + t
⎬
⎪⎪⎭

⎧⎪⎪
λmin
⎨
⎪⎪⎩

Combining the lower bound in (57) with the concentration result (62) certify that when

P + 2t > 0 and t ≤ 0,

P

⎧⎪⎪
∥ξΩ∥ ≥
⎨
⎪⎪⎩

√

2s
P + 2t

⎫⎪⎪
⎬
⎪⎪⎭

≤ s exp (

−t2
P (s + 3/2) − t/3 ) .

(63)

- Step 2. Bounding P{∥X Γc,ΩξΩ∥∞ ≥ 1 ∣ ∥ξΩ∥ ≤ τ }:

Considering the conditioned event {∥X Γc,ΩξΩ∥∞ ≥ 1 ∣ ∥ξΩ∥ ≤ τ }, we note that the set Ω is
constructed by selecting columns of X that satisfy X ⊺
Γc = 0,
the index set Ω, technically corresponds to the columns p where X ⊺
In other
words, none of the entries of the sub-matrix X Γc,∶ contribute to the selection of Ω. Noting
this, conditioned on given ξΩ, the entries of the vector X Γc,ΩξΩ are i.i.d random variables
distributed as N (0, ∥ξΩ∥

> 0. However, since w∗
Γ,pw∗

Γ > 0.

) and

∶,pw∗

2

P {∥X Γc,ΩξΩ∥∞ ≥ 1 ∣ ∥ξΩ∥ ≤ τ } = P

∣Γc∣
⋃
n=1

⎧⎪⎪
⎨
⎪⎪⎩

∣zn∣ ≥

1
∥ξΩ∥

RRRRRRRRRRR

∥ξΩ∥ ≤ τ

,

⎫⎪⎪
⎬
⎪⎪⎭

(64)

where {zn} are i.i.d standard normals. Using the union bound and the basic inequality
P{∣zn∣ ≥ a} ≤ exp(−a2

/2) valid for a ≥ 0, we get

P {∥X Γc,ΩξΩ∥∞ ≥ 1 ∣ ∥ξΩ∥ ≤ τ } ≤ (N − s) exp (−

1
2τ 2 ) .

√

For τ =

2s(P + 2t)

−1 we can combine (65) and (63) with reference to (54) to get

P {∥X Γc,ΩξΩ∥∞ ≥ 1} ≤ s exp (

−t2

P (s + 3/2) − t/3 ) + (N − s) exp (−

P + 2t
4s ) .

(65)

(66)

To select the free parameter t we make the argument of the two exponentials equal to get
√

3s + 4 −

t∗

=

45s2 + 84s + 25
12s + 2

P,

for which the right hand side expression in (66) reduces to N exp(−(4s)
on the given value of t∗, it is easy to verify that for all P ≥ 0 and s ≥ 1, the conditions t∗
and P + 2t∗

)). Based
≤ 0
> 0 are satisﬁed. Moreover some basic algebra reveals that for all P ≥ 0 and s ≥ 1
P
11s + 7

P + 2t∗
4s

≤ −

−

.

−1

(P + 2t∗

36

Therefore, for µ > 1, setting P = (11s + 7)µ log N guarantees that

P {∥X Γc,ΩξΩ∥∞ ≥ 1} ≤ N 1−µ.

7.6.1 Proof of Proposition 4

P
To use Theorem 4, we focus on a sequence {Zp}
E(uu⊺
Eg(un1

−
). In all the steps discussed below, we need to calculate cross moments of the type
2 ⋯uns

p=1 of the random matrices Z = uu⊺

s ) for u = [ui] ∈ Rs distributed as

1 un2

gU (u) =

⎧⎪⎪
⎨
⎪⎪⎩

1√
(2π)s exp (−
1
2 δD(u)

1

2 u⊺u) u1 > 0
u1 ≤ 0

.

For the proposed distribution, the matrix of second order moments can be conveniently cal-
culated as

D = E(uu⊺

) =

1
2

I.

The matrix uu⊺ is a rank one positive semideﬁnite matrix, which has only one nonzero
eigenvalue. Using the Weyl’s inequality we get

λmin (Z) = λmin (uu⊺

− D) ≥ λmin(uu⊺

) + λmin(−D) = −

1
2

.

(67)

Furthermore,

p) = E ((uu⊺
for which we can calculate the expectation term as

E (Z2

2
)

) − D2,

E ((uu⊺

)

2

) =

s + 2
2

I.

Here, we used the following simple lemma:
Lemma 1. Given a random vector u = [ui] ∈ Rs, with i.i.d entries ui ∼ N (0, 1):
E ((uu⊺

) = (s + 2)I.

)

2

It is now easy to observe that

P
∑
p=1

XXXXXXXXXXX

E (Z2
p)

XXXXXXXXXXX

= P λmax (E ((uu⊺

)

2

) − D2

) = P (

s + 2

2 −

1
4 ) =

P
2 (s +

3
2 ) .

Now, using (67) and (68) we can apply Theorem 4 to bound the smallest eigenvalue as

∀t ≤ 0 ∶ P

⎧⎪⎪
λmin
⎨
⎪⎪⎩

P
∑
p=1

⎛
⎝

upu⊺

p − P D⎞
⎠

≤ t

⎫⎪⎪
⎬
⎪⎪⎭

≤ s exp (

−t2
P (s + 3/2) − t/3 ) .

37

(68)

(69)

Since P D is a multiple of the identity matrix, eig{∑
and therefore

p=1 upu⊺
P

p − P D} = eig{∑

p=1 upu⊺
P

p} − P /2

P

⎧⎪⎪
λmin
⎨
⎪⎪⎩

P
∑
p=1

⎛
⎝

upu⊺
p

⎞
⎠

≤

⎫⎪⎪
P
2 + t
⎬
⎪⎪⎭

= P

⎧⎪⎪
λmin
⎨
⎪⎪⎩

P
∑
p=1

⎛
⎝

upu⊺

p − P D⎞
⎠

⎫⎪⎪
≤ t
⎬
⎪⎪⎭

(70)

which gives the probability mentioned in (62).

7.6.2 Proof of Lemma 1

The (i, j) element of the underlying matrix can be written as

((uu⊺

)

2

)i,j = uiuj

s
∑
k=1

u2
k,

therefore,

E (uiuj

s
∑
k=1

u2
k) = {

0
E (u4

i + u2

i ∑k≠i u2

i ≠ j
k) i = j = {

0
i ≠ j
s + 2 i = j

.

(71)

Here, we used the facts that E(u4

i ) = 3 and ∑k≠i u2

k = s − 1.

References

[1] A. Aghasi and J. Romberg. Convex cardinal shape composition. SIAM Journal on

Imaging Sciences, 8(4):2887–2950, 2015.

[2] A. Aghasi and J. Romberg. Learning shapes by convex composition. Under Review,

arXiv preprint arXiv:1602.07613, 2016.

[3] Spencer Aiello, Tom Kraljevic, Petr Maj, and H2O.ai team. h2o: R Interface for H2O,

2016. R package version 3.10.0.6.

[4] S. Arora, A. Bhaskara, R. Ge, and T. Ma. Provable bounds for learning some deep rep-
resentations. In Proceedings of the 31st International Conference on Machine Learning,
2014.

[5] O. Aslan, X. Zhang, and D. Schuurmans. Convex deep learning via normalized kernels.
In Proceedings of the 27th International Conference on Neural Information Processing
Systems, pages 3275–3283, 2014.

[6] F. Bach. Breaking the curse of dimensionality with convex neural networks. Technical

report, 2014.

38

[7] Y. Bengio, N. Le Roux, P. Vincent, O. Delalleau, and P. Marcotte. Convex neural
networks. In Proceedings of the 18th International Conference on Neural Information
Processing Systems, pages 123–130, 2005.

[8] A. Candel, E. LeDell, V. Parmar, and A. Arora. Deep Learning with H2O. H2O.ai Inc.,

ﬁfth edition, September 2016.

[9] E. Candès and B. Recht. Simple bounds for recovering low-complexity models. Mathe-

matical Programming, 141(1-2):577–589, 2013.

[10] A. Choromanska, M. Henaﬀ, M. Mathieu, G.B. Arous, and Y. LeCun. The loss surfaces
of multilayer networks. In Proceedings of the 18th International Conference on Artiﬁcial
Intelligence and Statistics, 2015.

[11] IBM ILOG CPLEX. V12. 1: User’s manual for cplex. International Business Machines

Corporation, 46(53):157, 2009.

[12] S. Foucart and H. Rauhut. A mathematical introduction to compressive sensing, volume 1.

Springer, 2013.

[13] E. Ghadimi, A. Teixeira, I. Shames, and M. Johansson. Optimal parameter selection
for the alternating direction method of multipliers (admm): quadratic problems. IEEE
Transactions on Automatic Control, 60(3):644–658, 2015.

[14] F. Girosi, M. Jones, and T. Poggio. Regularization theory and neural networks architec-

tures. Neural computation, 7(2):219–269, 1995.

[15] R. Giryes, G. Sapiro, and A.M. Bronstein. Deep neural networks with random gaussian
weights: A universal classiﬁcation strategy? IEEE Transactions on Signal Processing,
64(13):3444–3457, 2016.

[16] Inc. Gurobi Optimization. Gurobi optimizer reference manual, 2015.

[17] K. Hornik, M. Stinchcombe, H. White D. Achlioptas, and F. McSherry. Multilayer
feedforward networks are universal approximators. Neural networks, 2(5):359–366, 1989.

[18] K. Kawaguchi. Deep learning without poor local minima. In Preprint, 2016.

[19] S. Nowlan and G. Hinton. Simplifying neural networks by soft weight-sharing. Neural

computation, 4(4):473–493, 1992.

[20] J. Schmidhuber. Deep learning in neural networks: An overview. Neural Networks,

61:85–117, 2015.

39

[21] N. Srivastava, G. Hinton, A. Krizhevsky, I. Sutskever, and R. Salakhutdinov. Dropout: a
simple way to prevent neural networks from overﬁtting. The Journal of Machine Learning
Research, 15(1):1929–1958, 2014.

[22] R. Tibshirani. Regression shrinkage and selection via the lasso. Journal of the Royal

Statistical Society. Series B (Methodological), pages 267–288, 1996.

[23] J. A. Tropp. User-friendly tail bounds for sums of random matrices. Foundations of

computational mathematics, 12(4):389–434, 2012.

[24] L. Wan, M. Zeiler, S. Zhang, Y. LeCun, and R. Fergus. Regularization of neural networks
In Proceedings of the 33rd International Conference on Machine

using dropconnect.
Learning, 2016.

40

