2
2
0
2

l
u
J

3
1

]
L
P
.
s
c
[

1
v
6
4
9
5
0
.
7
0
2
2
:
v
i
X
r
a

Distribution Theoretic Semantics for Non-Smooth
Differentiable Programming

PEDRO H. AZEVEDO DE AMORIM, Cornell University, United States
CHRISTOPHER LAM, University of Illinois at Urbana-Champaign, United States

With the wide spread of deep learning and gradient descent inspired optimization algorithms, differentiable
programming has gained traction. Nowadays it has found applications in many different areas as well, such as
scientific computing, robotics, computer graphics and others. One of its notoriously difficult problems consists
in interpreting programs that are not differentiable everywhere.

In this work we define 𝜆𝛿 , a core calculus for non-smooth differentiable programs and define its semantics
using concepts from distribution theory, a well-established area of functional analysis. We also show how
𝜆𝛿 presents better equational properties than other existing semantics and use our semantics to reason
about a simplified ray tracing algorithm. Further, we relate our semantics to existing differentiable languages
by providing translations to and from other existing differentiable semantic models. Finally, we provide a
proof-of-concept implementation in PyTorch of the novel constructions in this paper.

Additional Key Words and Phrases: Differentiable Programming, Distribution Theory, Denotational Semantics

1 INTRODUCTION

The field of differentiable programming languages seeks to add differentiation operators, such
as gradients, to programming languages. Originally motivated by its use in machine learning
frameworks such as Tensorflow [Abadi et al. 2016], these techniques have seen a considerable rise
in interest in the past years.

In the context of neural networks, differentiable programming is used to easily implement
gradient descent algorithms. It can quickly find the local minimum of a carefully selected objective
function over a large data set, yet can be succinctly represented by the following two line program:

𝑤0 = 0

𝑤𝑛+1 = 𝑤𝑛 − 𝛾 ∇𝐹 (𝑤𝑛)

Many languages can already easily implement the program above for a broad class of objective
functions 𝐹 , but in doing so they have outpaced the theoretical understanding of differentiable
programming. This is an expected consequence, as the gradient of a function is only defined for
points in which the function is differentiable. Unfortunately, it is very common for real-world
applications to make use of non-smooth functions. The ReLU function depicted in Figure 1, for
instance is a non-smooth function widely used to train neural networks.

The example above illustrates how the practice of differentiable programming has outpaced its
theory. Much has been done in understanding syntactic aspects of differentiation in programming
languages; automatic differentiation (AD) techniques have seen steady progress since the 80s [Beck
and Fischer 1994; Griewank et al. 1989; Pearlmutter and Siskind 2008]. On the denotational side,
however, the programming languages community lags behind in handling features that users of
these languages take for granted.

Authors’ addresses: Pedro H. Azevedo de Amorim, Computer Science, Cornell University, Ithaca, New York, 14850, United
States, pamorim@cs.cornell.edu; Christopher Lam, Computer Science, University of Illinois at Urbana-Champaign, Cham-
paign, Illinois, 61802, United States, lam30@illinois.edu.

2022. XXXX-XXXX/2022/7-ART $15.00
https://doi.org/10.1145/nnnnnnn.nnnnnnn

, Vol. 1, No. 1, Article . Publication date: July 2022.

 
 
 
 
 
 
2

Pedro H. Azevedo de Amorim and Christopher Lam

−1

1

𝑦

−1

𝑥

1

Fig. 1. ReLU activation function

This disparity has practical and negative consequences as many automatic differentiation al-
gorithms present unsound behavior that breaks the equational theory of the language when the
program being differentiated uses certain seemingly harmless features.

One of these features is conditionals, which can easily create points of non-differentiability as

the ReLU activation function shows:

ReLU(x) = if x < 0 then 0 else x
One way of fixing this is by syntactically restricting which programs can be differentiated. This

is an insufficient solution, as many common applications heavily rely on these use-cases.

Many attempts have been made to deal with non-smooth differentiability, but they all either
present certain non-intuitive restrictions on which non-smooth programs can be written or break
the equational theory of the language. Concretely, many of the expected equations that if-statements
are supposed to satisfy are not validated by these semantics, such as:

Theorem 1.1.

if 𝑏 then 𝑡 else 𝑡

=

𝑡
(cid:74)

.

(cid:75)

(cid:75)

(cid:74)

Our work uses ideas from distribution theory — a subject relevant to functional analysis, partial
differential equations and mathematical physics — to give a novel approach to non-smooth differ-
entiability where conditionals have a more intuitive equational theory. Our semantics is the first
one that validates Theorem 1.11 while still allowing for a large set of boolean predicates that can be
conditioned on. Furthermore, as we will show in Section 4, the existing semantics for non-smooth
differentiation behave unsoundly when you have the interaction of integration and differentiation.
These problematic interactions are both common in fields such as graphics, robotics, and physical
simulators, and also completely mathematically sound in our semantics.

In this work we present 𝜆𝛿 , a simply-typed 𝜆-calculus extended with constructions motivated
by distribution theory, define its denotational semantics using diffeological spaces (see 2), and
implement a proof-of-concept library in PyTorch.

We chose to use diffeological spaces because they are well-understood mathematical objects
that can accommodate higher-order programming, differentiation and distribution-theoretic ideas.
However, we believe that there might be other interesting semantical models for 𝜆𝛿 , specially when
taking into account computability issues; see Section 7 for more details.

This paper assumes significant background in distribution theory, and uses the standard nota-
tion from this field. See Appendix A for a self-contained introduction on this topic. We heavily

1In 𝜆𝛿 syntax the theorem statement looks a bit different, see Section 7 for more details.

, Vol. 1, No. 1, Article . Publication date: July 2022.

Distribution Theoretic Semantics for Non-Smooth Differentiable Programming

3

recommend reading this appendix for those that are unfamiliar with this material, as we will be
making heavy use of it in our contributions.

Summary of contributions. 𝜆𝛿 is the first language that combines conditionals, higher-order
functions, differentiability, and has a denotational semantics. Our main contributions are as follow-
ing:

• We define 𝜆𝛿 , an extension of the simply-typed 𝜆-calculus with differentiation and distribution-

theoretic primitives.(§5)

• We use the category Diff to interpret its denotational semantics and prove Theorem 6.4.(§6)
• We use our semantics to reason about several non-smooth programs, such as a modern

differentiable ray tracing algorithm. (§7.3)

• We embed a smooth 𝜆-calculus of [Huot et al. 2020] in 𝜆𝛿 and show how to relate their AD

transform with our syntax and semantics. (§8.1)

• We propose a constructive semantics for 𝜆𝛿 using ideas from constructive topology[Sherman

et al. 2019, 2020].(§8.1)

• We have implemented a proof-of-concept library in PyTorch which explores the ideas pre-

sented in this work. (§9)

2 MATHEMATICAL PRELIMINARIES: DIFFEOLOGICAL SPACES

When dealing with differentiable programming it is expected that the category in which we
interpret our language has a canonical notion of smoothness. A well-known such category is Man,
the category of manifolds and smooth maps. Even though manifolds are extremely well-understood
at this point, Man cannot be used to interpret a smooth 𝜆-calculus, as it is not cartesian closed,
meaning that it cannot interpret higher-order programs.

This limitation was already noted by mathematicians which led to the development of diffeological
spaces. 2 Though its definition might seem foreign to someone who is only used to working with
smooth manifolds, diffeological spaces are deeply related to them, as they can be defined as a
certain category of sheaves over Man.

The soon to be defined category Diff has many desired categorical properties such as complete-

ness, cocompleteness and cartesian closure; indeed, it is even a quasitopos.
Definition 2.1. A diffeological space is a pair (𝑋, P𝑈
number 𝑛 and open set 𝑈 ⊆ R𝑛, a set of plots P𝑈

𝑋 ⊆ 𝑈 → 𝑋 such that.

𝑋 ), where 𝑋 is a set and for every natural

• Every constant function is a plot
• If 𝑓 : 𝑉 → 𝑈 is a smooth function and 𝑝 ∈ P𝑈
• Let 𝑓 : 𝑈 → 𝑋 be a function, if for every point 𝑢 ∈ 𝑈 there exists an open set 𝑉 ⊆ 𝑈 such

𝑋 then 𝑝 ◦ 𝑓 ∈ P𝑉
𝑋

that 𝑓 |𝑉 ∈ P𝑉

𝑋 then 𝑓 ∈ P𝑈
𝑋

We say that a function 𝑓 : (𝑋, P𝑈
𝑌 . We call the set P𝑈
𝑋 , 𝑓 ◦ 𝑝 ∈ P𝑈
P𝑈
Definition 2.2. The category Diff has diffeological spaces as morphisms and Diff morphisms as
arrows.

𝑌 ) is a Diff morphism if for every plot 𝑝 : 𝑈 → 𝑋 ∈

𝑋 ) → (𝑌, P𝑈
𝑋 a diffeology.

Note that this construction is very similar to the quasi Borel category for higher-order probability
theory defined by Heunen et al. [Heunen et al. 2017]. Unsurprisingly, they both arise from similar
categorical machinery.

2c.f. the Diffeology textbook [Iglesias-Zemmour 2013] for a presentation on the subject

, Vol. 1, No. 1, Article . Publication date: July 2022.

4

Pedro H. Azevedo de Amorim and Christopher Lam

Let us work through a couple of examples to get a better feel of how to work with diffeological

spaces.
Example 2.3. The pair (R𝑛, {𝑓 : 𝑈 → R𝑛 | 𝑓 is smooth)}) is a diffeological space for every 𝑛 > 0.
The example above shows that the common definition of smoothness is used when defining the

R diffeological space. This construction can be generalized to an arbitrary manifold.
Example 2.4. Let 𝑀 be a manifold, then (𝑀, {𝑓 : 𝑈 → 𝑀 | 𝑓 ∈ Man(𝑈 , 𝑀)}) is a diffeological
space. Every smooth function 𝑓
: 𝑀 → 𝑁 between manifolds is also a Diff morphism. This
construction is actually a functor 𝜄 : Man → Diff.
Lemma 2.5 ([Iglesias-Zemmour 2013]). The functor 𝜄 is full and faithful.

What the theorem above implies is that when you only have ground types the smooth functions
are exactly what you would expect them to be. In particular Man(R𝑛, R𝑚) = Diff (R𝑛, R𝑚). This is
what makes Diff such a nice category to interpret differentiable programs — it simply conservatively
extends the familiar category Man.
Example 2.6. The pair (1, P𝑈
open 𝑈 .

1 ) is a diffeological space where P𝑈

1 is the singleton set for every

Lemma 2.7. Let 𝑋 be a set, the set {𝑓 : 𝑈 → R | 𝑓 is constant} is a diffeology.

Proof. The proof follows by unfolding the definition of diffeology. This is called the constant
□

diffeology.

Lemma 2.1. Let 𝑋 and 𝑌 be diffeological spaces equipped with the constant diffeology. Every function

𝑓 : 𝑋 → 𝑌 is a Diff morphism.

Proof. For every plot 𝑝 : 𝑈 → 𝑋 , the function 𝑓 ◦ 𝑝 is constant.

□

2.0.1 Cartesian Closed structure. As it was mentioned above, in order to interpret higher-order
programs we need Diff to be cartesian closed.
Definition 2.8 (Products). Let (𝑋1, P𝑈
𝑋1
𝑋2, P𝑈
Definition 2.9 (Closure). Let (𝑋, P𝑈
𝑌 , P𝑈
Diff (𝑈 × 𝑋, 𝑌 )}

) and (𝑋2, P𝑈
) be two diffeological spaces, then (𝑋1 ×
𝑋2
= {𝑝 : 𝑈 → 𝑋1×𝑋2 | 𝑝 ◦𝜋𝑖 ∈ P𝑈
, 𝑖 ∈ {1, 2}}
𝑋𝑖

𝑌 ) be two diffeological spaces, then (𝑋 ⇒
𝑋 ⇒𝑌 = {𝑝 : 𝑈 → Diff (𝑋, 𝑌 ) | (𝑢, 𝑥) ↦→ 𝑝 (𝑢)(𝑥) ∈

𝑋 ⇒𝑌 ) is an internal hom in Diff, where P𝑈

) is a cartesian product in Diff, where P𝑈

𝑋1×𝑋2
𝑋 ) and (𝑌, P𝑈

𝑋1×𝑋2

These constructions will be used in Section 6 to interpret product types and function types.

2.0.2 Distribution theory in Diff. There are other cartesian closed categories that also have a notion
of differentiability. However, we are working with Diff because it accommodates the distribution
theoretic machinery we need in order to define our semantics. The following lemma is used to
define distribution objects inside Diff.
Lemma 2.10 ([Iglesias-Zemmour 2013]). Let (𝑋, P𝑈
(𝑌, P𝑈

𝑋 ) be a diffeological space and 𝑌 ⊆ 𝑋 . The pair

𝑋 |𝑌 ) is a diffeological space, where P𝑈

𝑋 |𝑌 = {𝑓 : 𝑈 → 𝑋 ∈ P𝑈

𝑋 | 𝑓 (𝑈 ) ⊆ 𝑌 }.

For the categorically minded reader: since Diff is a quasitopos, the theorem above can be

generalized to arbitrary strong monomorphims.

This theorem makes it easy to equip D (R𝑛) and D ′(R𝑛) with diffeologies: they are simply the
appropriate subobjects of R𝑛 ⇒ R and D (R𝑛) ⇒ R, respectively. Concretely, the plots in D (R𝑛)
are the functions 𝑝 : 𝑈 → D (R𝑛) such that 𝑝 is a plot in P𝑈

.

The theorem below is what allows us to lift smooth programs to distributions

R𝑛⇒R

, Vol. 1, No. 1, Article . Publication date: July 2022.

Distribution Theoretic Semantics for Non-Smooth Differentiable Programming

5

Lemma 2.11 ([Kock and Reyes 2004]). The map T : 𝐶∞ (R𝑛) → D ′(R𝑛) is smooth

The next theorem is what allows us to interpret the 𝛿 distribution.

Theorem 2.12. The map 𝛿 : R𝑛 → D ′(R𝑛) is smooth.

Proof. The map 𝜂 : R𝑛 → (D (R𝑛) ⇒ R) such that 𝜂 (𝑟, 𝑓 ) = 𝑓 (𝑟 ) is smooth by the Cartesian
closed structure of Diff and we can conclude that 𝛿 is smooth by the fact that 𝜂 (𝑟 ) is linear and,
therefore, an element of D ′(R𝑛)
□

Remark 2.13. It is important to note that the idea of using distribution theory to define semantics
of differentiable programming languages does not rely on an specific category. We chose Diff for
the sake of convenience. The category of convenient vector spaces [Blute et al. 2012], for instance,
also has objects for distributions. This means that the ideas presented in this paper may be used in
other categories as well.

3 BACKGROUND

It is not hard to see that if-statements can easily define non-differentiable behavior. Before
we explain the subtleties of conditionals in differentiable programming languages we motivate
non-smooth differentiable programming languages. We argue that making it impossible to define
non-smooth programs may introduce unnecessary complexity to models while allowing non-
smooth behavior may also simplify certain models.

Something that commonly occurs in systems that make use of differentiable components is
having a model that, modulo its points of non-differentiability, works as intended. A common next
step is to come up with a smooth variant for it, the sigmoid 𝑆 (𝑥) = 1
1+𝑒−𝑥 function being a smooth
variant of the ReLU function. Unfortunately, in more convoluted cases the smooth approximation
does not have a nice closed-form expression, making the model more complex. Besides, as we will
see in Section 3.1, in many cases, the tools used to make the model smooth implicitly use ideas
from distribution theory.

On the other hand, in physics, non-smoothness has been historically used as a simplifying agent.
In rigid body physics, for instance, it is standard to assume that collisions occur instantaneously or
that electrical charges are point-mass. With differentiable programming being used in physical sim-
ulators [Hu et al. 2020] it seems natural that differentiable programming tools should accommodate
these commonly used modeling principles.

What the existing semantics of non-smooth differentiability show us is that we cannot rely on
all of our intuitions about differentiability. One of their drawbacks is that, while they prove that
their semantics has the expected behavior modulo a null measure set, they break the equational
theory of conditionals and add unnecessary non-terminating behavior to programs.

Our approach is more powerful in comparison; we offer a semantics that both allows for con-
ditional statements as well as a method of differentiating those conditional statements without
introducing undefined behavior. We do this by introducing distributions into our semantics and
syntax.

As differentiable programming finds applications in fields other than machine learning, such as
robotics, computer vision, computer graphics and scientific computing [Degrave et al. 2019; Innes
et al. 2019; Li et al. 2021, 2018b], we have empirical evidence that non-smoothness may result in
better models. Recently Li et al. [Li et al. 2018a] have shown how by using ideas from distributions
theory they were able to come up with a better differentiable ray-tracing algorithm.

Next, we will use the ReLU function to present some of the subtleties that if-statements introduce
in differentiable programming languages. ReLU is differentiable almost everywhere, except at 𝑥 = 0.

, Vol. 1, No. 1, Article . Publication date: July 2022.

6

Pedro H. Azevedo de Amorim and Christopher Lam

1

𝑦

−1

𝑥

1

−1

Fig. 2. Heaviside function

When we differentiate this function while ignoring this problematic point we get something that
looks like the Heaviside function depicted in Figure 2. This means that any solution that wants to
accommodate non-smooth but continuous functions and higher-order derivatives must also deal
with discontinuous functions.

To our knowledge, there are two approaches to deal with differentiability of if-statements. The
first is simply removing conditionals from the language and only allowing for the construction
of infinitely differentiable functions. While this approach is formally valid, it loses the ability to
construct the widely used ReLU activation function, severely limiting the expressiveness of the
language. The second approach is somewhat more interesting: differentiate down both branches
and leave the derivative as a piecewise function. While this approach appears appealing, it is in fact
vulnerable to another family of pathological counterexamples. Consider the following function:

id(x) = if x != 0 then x else 0
This function is semantically identical to the the identity function over R, and as such, its derivative
should be equal to 1 at all points. However, the derivative resulting from the construction specified
above results in the derivative at x = 0 to be erroneously equal to 0.

(1)

There have been forays into formalizing some of the ad-hoc solutions. Huot et al. [Huot et al.
2020] take the first described approach; they disallow conditionals and implement differentiation
as a macro outside of the language syntax. Adabi and Plotkin [Abadi and Plotkin 2019] introduce
non-terminating behaviour at the points of non-smoothness and only guarantee correctness of
differentiation at the points that the program terminates. Furthermore, they avoid the problematic
program 1 by imposing a restriction on which predicates can be expressed in their language.

3.1 Why Distribution Theory?

In this section we try to give intuition as to why even though our semantics is radically different
from existing semantics for differentiable programming, it is still closely related to a couple of
methods that practioners currently employ to deal with non-smoothness. In fact, later in section
8.1, we show that we can actually embed a previous language directly into our semantics.

The existing semantics of differentiable programming that guarantee correctness of differentiation
modulo a null-measure set rely on the assumption that, if you want to run gradient descent you
may either start with a random initial point or, at every iteration, add a small random noise to the
current point. This procedure combined with these correctness theorems allows you to show that
with probability 1 you will always get the correct derivative.

, Vol. 1, No. 1, Article . Publication date: July 2022.

Distribution Theoretic Semantics for Non-Smooth Differentiable Programming

7

How would you give semantics to these stochastic operations? This is achieved by integrating
the almost-smooth function 𝑓 by the probability measure 𝜇 corresponding to the random noise,
which is equal to

∫

𝑓 𝜑 d𝑥, where 𝜑 is the probability distribution function of 𝜇

This construction looks very similar to the way distribution theory would interpret this quasi-

smooth function.

Another way practioners deal with non-smoothness is by finding a smooth approximation to
their model. There are a few methods that allow you to define a smooth variant of a model 3.
Unfortunately, in general, these methods do not give you a closed form solution. One of these
methods relies on the following theorem:

Theorem 3.1. Let 𝑓 : R𝑛 → R and 𝑔 : R𝑛 → R be two measurable functions. If 𝑔 is smooth and
𝑓 (𝑦)𝑔(𝑥 − 𝑦)𝑑𝑦 is defined everywhere

compactly supported then the convolution (𝑓 ★ 𝑔)(𝑥) = ∫ ∞
and is smooth as well.

−∞

By using the theorem above and choosing an appropriate function 𝑔 it is possible to find a smooth
approximation of 𝑓 . That being said, by closer inspection, if you choose 𝑔 to be our defined bump
function we are, once again, inadvertently getting the same interpretation as the one our semantics
would give.

These two examples illustrate that even though our semantics originates from a completely

different starting point, it captures some of the methods already being used by practioners.

A more direct consequence of basing our semantics on distribution theory is that this formalism
provides a very precise language in which one can model non-smooth behavior. If one were to use
the existing semantics to reason about the examples shown in Section 4 there would be fundamental
flaws in their interpretation. For instance, in the bouncing ball example the derivative of the velocity
with respect to time (i.e. acceleration) would be constant everywhere except at the collision point, in
which case it would either be undefined or an arbitrary value. Both alternatives do not capture the
whole point of the model, which is that the ball is bouncing rather than accelerating at a constant
rate.

A more egregious example is presented by Li et al. [Li et al. 2018a] where it is shown that by
considering non-smooth aspects you improve on the state-of-the-art for differentiable rendering
techniques.

For these reasons we believe that distribution theory provides a useful interpretation of differen-

tiable programming.

4 DISTRIBUTION THEORY FOR NON-SMOOTH MODELING

In this section we show how distribution theory can be used in non-smooth modeling. As we
have already mentioned, one of the main drawbacks of using a semantics which ignores the points
of non-smoothness is that it does not properly account for the interaction of differentiation and
integration.

Indeed, the popular implementations of integration in automatic differentiation systems use
variants of the Monte Carlo method. Therefore, by linearity, the AD procedure distributes the deriv-
ative over the various terms of the sum, resulting in unsound behavior. By contrast, a distribution
theoretic approach - like ours - can safely and soundly apply linearity of differentiation.

3See Pierucci [Pierucci 2017] for a thorough presentation on some of these methods

, Vol. 1, No. 1, Article . Publication date: July 2022.

8

Pedro H. Azevedo de Amorim and Christopher Lam

𝑣 (𝑡)

20

10

−10

−20

1

2

3

4

𝑡

5

Fig. 3. Velocity of a bouncing ball under constant acceleration

4.1 Coin Flipping Optimization

In the context of probabilistic programming it is often required to optimize the expected value
of a family of random variables. This can be achieved by using gradient-descent methods and,
therefore, it requires to compute the derivative of the function

∫

𝑝 ↦→

𝑓 (𝑝, 𝑥) d𝑥

Where 𝑓 (𝑝, −) is a probability density function (pdf). Under certain conditions, derivatives
distribute over integrals, reducing the problem of computing the derivative of the integral to
computing the derivative of the function 𝑓 (−, 𝑥).

As a simple example, consider a 𝑝-biased coin such that we want to optimize its expected heads

frequency. Therefore we have to differentiate the integral ∫ 1
0
As we mentioned above, the partial semantics gives 𝜕𝑓𝑝 (𝑥)

𝑓𝑝 (𝑥) d𝑥, where 𝑓𝑝 (𝑥) = 10≤𝑥 ≤𝑝 .
𝜕𝑝 = 0 and, therefore, its integral would
be 0 as well, making it impossible to move away from the initial guess. On the other hand, by
considering its distributional derivative, we get 𝛿𝑝−𝑥 and its integral over the interval [0, 1] will be
10≤𝑥 ≤1 [Bangaru et al. 2021]. Once again we see that the partial semantics is inadequate to correctly
model non-smooth behavior.

4.2 Bouncing Ball

Assume that we drop a ball from a height ℎ with constant acceleration 𝑔 and that it collides with

the ground instantaneously. Its velocity is given by the plot depicted in Figure 3.

As we can see, there are points of discontinuity at every natural number greater than 0. This
example shows an important difference between existing semantics for differentiable programming
and distribution theory. If you consider a language that does not have distribution theoretic
primitives, the acceleration of the system is simply 𝑔, completely ignoring the effects of the
collisions. It is possible to show that the distributional derivative of 𝑣 (𝑡) is 𝑔 + (cid:205)∞
𝑛=0 𝛿2𝑛+1, where
each 𝛿 captures a time of collision.

Even though this is a simple physical system, variants of it can be used in differential motion
planning algorithms. In such applications it is important to take into consideration the interaction
of differentiation and instantaneous collisions [Bangaru et al. 2021].

4.3 Differentiable Ray Tracing

In recent years, differentiable methods have found applications in computer graphics. One possi-
ble application is to use gradient descent on the rendering algorithm so that you may differentiate
the output image with respect to certain scene parameters so that you may, for instance, generate
an adversarial picture for an image classifier. Therefore, in order for this method to work the whole

, Vol. 1, No. 1, Article . Publication date: July 2022.

Distribution Theoretic Semantics for Non-Smooth Differentiable Programming

9

image generating process needs to be differentiable, in particular the rendering algorithm. Li et
al. [Li et al. 2018a] define a differentiable ray tracing algorithm which fundamentally relies on
non-smooth behavior. In this example we will present a simplification of their model.

Consider a 2D environment parametrized by a continuous space Φ (e.g. camera position) filled
with triangles with different characteristics and with spatial coordinates also captured by Φ. In
such a setup the value of a pixel is given by an integral

∫ ∫

𝐼 =

𝑓 (𝑥, 𝑦, Φ)𝑑𝑥𝑑𝑦

In this simple setup the scene function 𝑓 is simply the sum of functions 𝑓𝑖 for each triangle in
the scene multiplied by the indicator function of each triangle. In this case, each 𝑓𝑖 captures certain
properties of the triangles (e.g. its transparency) that are relevant to the rendering of the image and
therefore should only be “active” within the boundaries of the triangle:

∫ ∫ ∑︁

𝐼 =

𝑖

1𝑖 𝑓𝑖 (𝑥, 𝑦, Φ)𝑑𝑥𝑑𝑦

Therefore, when trying to reason about the gradient of 𝐼 we must be careful around the points
of non-smoothness in the functions 1𝑖 𝑓𝑖 (𝑥, 𝑦, Φ). Following Li et al. [Li et al. 2018a] we have the
equation:

∫ ∫

∑︁

𝐼 =

𝑖

1𝑖 𝑓𝑖 (𝑥, 𝑦, Φ)𝑑𝑥𝑑𝑦

We may now take the gradient of 𝐼 :

(cid:18)∫ ∫

∇

1𝑖 𝑓𝑖 (𝑥, 𝑦, Φ)𝑑𝑥𝑑𝑦

(cid:19)

=

∫ ∫

∫ ∫

∇ (1𝑖 𝑓𝑖 (𝑥, 𝑦, Φ)) 𝑑𝑥𝑑𝑦 =

1𝑖 ∇𝑓𝑖 (𝑥, 𝑦, Φ)𝑑𝑥𝑑𝑦 +

∫ ∫

𝑓𝑖 (𝑥, 𝑦, Φ)∇1𝑖𝑑𝑥𝑑𝑦

(2)

(3)

(4)

(5)

What Li et al. have used in their analysis is the fact that the gradient of the indicator function will
add a Dirac delta to their integral, which the authors show can be further simplified. This contrasts
with the partial semantics, which would simply ignore the non-smoothness and differentiate
𝑓𝑖 (𝑥, 𝑦, Φ) without taking into consideration the non-smoothness introduced by the indicator
function. Furthermore, this would require commuting the derivative with the integral, which is
unsound in this case. Indeed, the authors show that by only using the partial semantics you get a
qualitatively worse rendered image. See Section 7.3 for a formal elaboration on how a distribution-
theoretic approach resolves this.

5 SYNTAX AND TYPE SYSTEM

Figure 4 presents the syntax and type system of 𝜆𝛿 : it is a simply typed 𝜆-calculus extended with
types D (R𝑛) and D ′(R𝑛) for test functions and distributions, respectively, and some distribution
theoretic primitives inhabiting them, which are highlighted.

We have a differentiation operation, a way of sending a smooth function 𝑓 to its canonical
distribution 𝑇𝑓 , which we call lift in our syntax, and a way of defining bump functions so that
we may compute the value of a distribution “around a point”. Furthermore, we use the fact that

, Vol. 1, No. 1, Article . Publication date: July 2022.

10

Pedro H. Azevedo de Amorim and Christopher Lam

𝜏 := R | R+ | Pred(R𝑛) | N |𝜏 × 𝜏 | D ′(R𝑛) | D (R𝑛) | 𝜏 −→ 𝜏

𝑡, 𝑢 := 𝑥 | 𝑟 | (𝑡, 𝑢) | let (𝑥, 𝑦) = 𝑡 in 𝑢 |

let 𝑥 = 𝑡 in 𝑢 | 𝜆𝑥 : 𝜏 .𝑡 | 𝑡 𝑢 | lift(𝑡) | 1𝑡 (𝑢) |

| 𝑡 + . 𝑢 | 𝑡 ∗. 𝑢 | ⟨𝑡, 𝑢⟩ | 𝜑𝑛 (𝑐, 𝑟 ) |𝛿𝑡

𝜕𝑡
𝜕𝑥𝑖
it 𝑡 𝑢 | arithmetic | comparators

|

Fig. 4. Syntax and type system - distribution theoretic concepts highlighted in red

distributions can be equipped with a vector space structure and add to our syntax addition and
multiplication by a scalar.

To encode conditionals we allow multiplying a smooth function by an indicator function 1𝑏 —
the function such that 1𝑏 (𝑥) = 1 if 𝑏 (𝑥) is true and 0 otherwise. This construction and the vector
space structure allows us to define piecewise smooth functions and define conditionals.

We have also added N to the type system and a structural recursion combinator to encode
iterators. Intuitively, it 𝑡 𝑢 𝑛 composes 𝑢 with itself 𝑛 times and is applies this composition to 𝑡.
Note that this iteration procedure is memory-less, i.e. for every step of the iteration the function 𝑢
does not know in which step it currently is. However, given a function 𝑓 : 𝜏 × N → 𝜏, the regular
iterator can be used to implement a memoryful one:

it (𝑡0, 0) (𝜆(𝑡, 𝑛). (𝑓 (𝑡, 𝑛), 𝑛 + 1))
Note that we do not allow to differentiate with respect to a higher-order argument. Instead these

features should be used to facilitate the manipulation of functions and distributions.

Example 5.1 (Non-smooth programs). Given two smooth programs 𝑡 and 𝑢 and a boolean predicate

𝑏, we define the program if 𝑏 then 𝑡 else 𝑢 as the distribution 1𝑏𝑡 + .1¬𝑏𝑢

An unexpected consequence of the way we encode if-statements and the typing rule for the
indicator function is that we lose the ability to nest conditionals. However, this is not problematic
as in the absence of recursive programs, every nested if-statement can be expressed as a single
multi-branch if-statement, which is something we can encode as the sum of several indicator
functions, each indicator corresponding to an if-statement branch.

That being said, this syntactic restriction is a consequence of a limitation of our model, as
morphisms 𝐴 → 𝐵 need to be smooth. However, by Theorem A.6, every measurable function can
be made into a distribution, which suggests that it is theoretically possible to have a more standard
presentation of if-statements. For the sake of brevity we will not focus on these issues.

Figure 12 presents the typing rules. As it is usually the case, contexts Γ are lists of variables and
their respective types. We write Γ ⊢ 𝑡 : 𝜏 to mean that the program 𝑡 has type 𝜏 under context Γ.

Example 5.2. We can write the ReLU function as the following program:

· ⊢ 1𝜆𝑥:R.𝑥 ≥0(𝜆𝑥 : R.𝑥) : D ′(R)
Example 5.3. We can write a function that transforms a function 𝑓 : R𝑚 → R2 into a pair of

functions of the type R𝑚 → R with the following:

· ⊢ 𝜆𝑓 .(𝜆𝑥 .let (𝑎, 𝑏) = 𝑓 𝑥 in 𝑎, 𝜆𝑥 .let (𝑎, 𝑏) = 𝑓 𝑥 in 𝑏)

, Vol. 1, No. 1, Article . Publication date: July 2022.

Distribution Theoretic Semantics for Non-Smooth Differentiable Programming

11

Lift

Γ ⊢ 𝑡 : R𝑛 −→ R
Γ ⊢ lift(𝑡) : D ′(R𝑛)

Differentiation
Γ ⊢ 𝑡 : D ′(R𝑛)
𝜕𝑡
𝜕𝑥𝑖

Γ ⊢

𝑖 ∈ {1, ..., 𝑛}

: D ′(R𝑛)

Indicator Function
Γ ⊢ 𝑡1 : Pred(R𝑛)

Γ ⊢ 𝑡2 : R𝑛 → R

Γ ⊢ 1𝑡1 (𝑡2) : D ′(R𝑛)

Distribution Application
Γ ⊢ 𝑡1 : D ′(R𝑛)

Γ ⊢ 𝑡2 : D (R𝑛)

Γ ⊢ ⟨𝑡1, 𝑡2⟩ : R

Bump Function
Γ ⊢ 𝑐 : R𝑛

Γ ⊢ 𝑟 : R+

𝑛 ∈ N

Γ ⊢ 𝜑𝑛 (𝑐, 𝑟 ) : D (R𝑛)

Dirac delta
Γ ⊢ 𝑡 : R𝑛
Γ ⊢ 𝛿𝑡 : D ′(R𝑛)

Fig. 5. Selected typing rules

By repeatedly applying the above example, we can transform an arbitrary function of type

R𝑚 → R𝑛 to an 𝑛-tuple of functions R𝑚 → R.

Example 5.4. We can write a function that takes the derivative of distribution 𝑓 : D ′(R) at some

point 𝑥 and applies to it the bump function centered at 𝑥 with radius 𝜀 as the following:

𝑑𝑒𝑟 : D ′(R) → R → R → R
(cid:28) 𝜕𝑓
𝜕𝑥

𝑑𝑒𝑟 𝑓 𝑥 𝜀 =

, 𝜑 (𝑥, 𝜀)

(cid:29)

where we use Haskell-like syntactic sugar for convenience.

Example 5.5. We can write the gradient descent algorithm running for 𝑛 iterations, starting at
a point 𝑥0, for a distribution 𝑓 and using 𝜀 as the radius of the bump function as the following
program:

𝑔𝑟𝑎𝑑𝐷𝑒𝑠𝑐 : D ′(R) → R → R → N → R
𝑔𝑟𝑎𝑑𝐷𝑒𝑠𝑐 𝑓 𝑥0 𝜀 =

it 𝑥0 (𝜆𝑥𝑛 : R𝑛.𝑥𝑛 − (𝑑𝑒𝑟 𝑓 𝑥 𝜀))

Where again we use Haskell-like syntactic sugar.

In the example above we have defined a gradient descent algorithm for functions R → R which

can be used to maximize or minimize the given input function in certain circumstances.

The situation is a bit subtler when working with functions R𝑛 → R𝑚, since elements of type
D ′(R𝑛) are, in a way, generalized functions with codomain R, making it unclear at first how to
define their gradients using distributions. We get around this by using the universal property of
products to factor every function 𝑓 : R𝑛 → R𝑚 into 𝑚 functions 𝑓𝑖
: R𝑛 → R and defining its
gradient using the 𝑚-tuple of distributional derivatives of 𝑓𝑖 , as shown in Example 5.3.

6 SEMANTICS

To every type 𝜏 in our language we associate a diffeological space

program Γ ⊢ 𝑡 : 𝜏 gives rise to a morphism
(cid:74)
structural induction on the typing derivation. For the sake of clarity, we will write
Γ ⊢ 𝑡 : 𝜏

𝜏
(cid:74)

→

Γ

(cid:75)

(cid:75)

𝑡
(cid:74)

(cid:75)

and every well typed
(cid:75)
in Diff. As usual, we define the semantics by
instead of

𝜏
(cid:74)

(cid:74)

(cid:75)

, Vol. 1, No. 1, Article . Publication date: July 2022.

12

Pedro H. Azevedo de Amorim and Christopher Lam

(cid:75)

(cid:75)

R
(cid:75)
(cid:74)
R+
(cid:74)
Pred(R𝑛)
(cid:74)
N
(cid:74)
𝜏1 × 𝜏2
(cid:74)
D (R𝑛)
(cid:74)
D ′(R𝑛)
(cid:74)
𝜏1 → 𝜏2
(cid:74)

(cid:75)

(cid:75)

(cid:75)

(cid:75)

(cid:75)

= R
= R+
= Pred(R𝑛)
= N
𝜏2
𝜏1
×
=
(cid:74)
(cid:75)
(cid:74)
= D (R𝑛)
= D ′(R𝑛)
𝜏1
⇒
=
(cid:74)

(cid:75)

(cid:75)

𝜏2
(cid:74)

(cid:75)

Fig. 6. Type interpretation

6.1 Interpreting types and non-standard booleans

Figure 6 shows the semantics of well-typed terms. Since Diff is cartesian closed, we interpret the
simply typed fragment of our language using the standard constructions. The distribution theoretic
types are interpreted as explained in Section A. To interpret the type N we will use the fact that
Diff is cocomplete and therefore can interpret inductive types — i.e. initial algebras for polynomial
functors. Furthermore, we will use the initiality of inductive types to define the semantics of the it
combinator. Interpreting B is a bit subtler.

Non-standard booleans. In cartesian categories with coproducts, booleans are usually defined as
the coproduct 1 + 1, where 1 is the unit for the cartesian strucuture, and if-statements are defined
by the universal property of coproducts. Unfortunately, if we define
= 1 + 1 in Diff, the only
smooth predicates R𝑛 → 1 + 1 are the constant functions, which is obviously not expressive enough.
In order to remedy this we will assume that we have types Pred(R𝑛) whose elements are
measurable subsets of R𝑛. In an earlier version 𝜆𝛿 we had a type B of boolean that were equipped
with a diffeology such that functions R𝑛 → B were the measurable functions. The problem with
this approach is that since we were using the closed structure of Diff to interpret predicates,
we could partially apply predicates. Unfortunately, currying predicates makes it fairly easy to
write non-smooth programs. By using this new approach we can avoid the problems of previous
approaches. The key ingredient is equipping Pred(R𝑛) with the constant diffeology.

B
(cid:75)

(cid:74)

Theorem 6.1. B = (Pred(R𝑛), P𝑈

Pred(R𝑛), is a diffeological space.

Pred(R𝑛) ), where PPred(R𝑛)

𝑈

are the constant functions 𝑈 →

Proof. This is just a special case of Lemma 2.7.

□

Something appealing about this construction is that it is possible to equip Pred(R𝑛) with a

boolean algebra structure.

Theorem 6.2. Pred(R𝑛) is a boolean algebra.
Proof. This is a direct consequence of Lemma 2.1.

□

The reason why we care about these non-standard booleans is because they validate the following

theorem:

Theorem 6.3. The function 1 : Pred(R𝑛) × (R𝑛 → R) → D ′(R𝑛) is a Diff morphism.

, Vol. 1, No. 1, Article . Publication date: July 2022.

Distribution Theoretic Semantics for Non-Smooth Differentiable Programming

13

(𝛾) = 𝛾 (𝑥)

𝑥
(cid:74)

(cid:75)

(𝑡1, 𝑡2)

(cid:74)

(cid:75)

(𝛾) = (

𝑡1
(cid:74)

(cid:74)
𝑡 𝑢
(cid:74)

let (𝑥, 𝑦) = 𝑡 in 𝑢

(𝛾) =

(cid:75)

(cid:115) 𝜕𝑡
𝜕𝑥𝑖

(𝛾,

𝑡
𝑢
(cid:74)
(cid:75)
(cid:74)
(cid:123) (𝛾) =

(cid:75)
𝜕(

(𝛾,

𝑢
(cid:74)

(cid:75)

(𝛾) =

(cid:75)
(𝛾))

(𝛾))

(cid:75)
lift(𝑡)

𝑡
(cid:74)

(cid:74)

(𝛾))

𝑡
(cid:75)
(cid:74)
𝜕𝑥𝑖

𝑡 + . 𝑢
(cid:74)

(cid:75)

(𝛾) =

(𝛾) +

𝑡
(cid:74)

(cid:75)

(𝛾)

𝑢
(cid:74)

(cid:75)

(𝛾),

𝑡2
(cid:75)
(cid:74)
𝜆𝑥 : 𝜏 .𝑡
(cid:74)

(𝛾))

(cid:75)
(𝛾) = 𝜆𝑥 .

(𝛾, 𝑥)

𝑡
(cid:74)
(cid:75)
(𝛾) = 𝑇1

(𝛾 )

(cid:75)

𝑢
(cid:74)

(cid:75)

(𝛾 )

𝑡
(cid:74)

(cid:75)
1𝑡 (𝑢)

(cid:75)

(𝛾) = T
𝑡
(cid:74)

(cid:75)

(𝛾 )

(cid:75)

(cid:74)

(𝛾) ∗

(𝛾)

𝑢
(cid:74)

(cid:75)

𝑡 ∗ . 𝑢
(cid:74)

(cid:75)
𝜑𝑛 (𝑐, 𝑟 )
(cid:74)

(𝛾) =

𝑡
(cid:75)
(cid:74)
𝑐
(𝛾) = 𝜑 (cid:74)
𝑟
(cid:74)

(cid:75)

(𝛾 )
(𝛾 )

(cid:75)
(cid:75)

it 𝑡 𝑢

(cid:74)

(cid:75)

⟨𝑡, 𝑢⟩

(𝛾) = ⟨

(cid:75)
(cid:74)
(𝛾) = 𝑅𝑒𝑐N (

𝑡
(cid:74)

(𝛾),

(cid:75)
(𝛾))(

(𝛾)⟩

(cid:75)
(𝛾))

𝑢
(cid:74)
𝑢
(cid:74)

(cid:75)

𝑡
(cid:74)

(cid:75)

(𝛾) = 𝛿

𝛿𝑡
(cid:74)
Fig. 7. Denotational semantics for 𝜆𝛿

𝑡
(cid:74)

(𝛾 )

(cid:75)

(cid:75)

This theorem is used to interpret the 1 operator of 𝜆𝛿 and can be proven analogously to Theo-

rem 2.11.

6.2 Semantics of well-typed terms

Figure 7 shows the semantics of our constructions. It is mostly standard: the simply typed
structure is interpreted by the cartesian closed structure of Diff presented in Section A, the iterator
is defined as the unique arrow given by the object N equipped with the appropriate arrows being
an initial algebra4. Given an element 𝑥 of a diffeological space 𝑋 and an endofunction 𝑓 : 𝑋 → 𝑋 ,
we name the arrow given by initiality 𝑅𝑒𝑐N 𝑥 𝑓 .

The cartesian closed structure of Diff and the fact that the distribution theoretic types are
defined as subobjects means that the distribution application is also smooth. The smoothness of
the distributional derivative follows by a similar proof. Finally, the validity of the interpretations of
𝛿, lift and 1 are given by Theorem 2.12, Theorem A.6 and Theorem 6.3.

Note that the construction of our non-standard booleans impose the predicates in our language
to be measurable sets. In practice, however, it is extremely hard to define a non-measurable set,
making this restriction almost non-existent.

To reiterate, even though Diff might look a bit too abstract at times, morphisms R𝑛 → R𝑚 are
exactly the smooth functions between R𝑛 and 𝑅𝑚. Furthermore, since D ′(R𝑛) is also exactly equal
to the set of distributions over R𝑛, the vast catalog of theorems from distribution theory can be
used to reason about programs.

6.3 Well-behaved conditionals

As we illustrate in Appendix 9 with PyTorch’s max function, the current handling of the dif-
ferentiation of non-smooth programs and conditionals in production environments can lead to
unexpected aberrant behavior. The key problem that gives rise to this behavior is that Theorem 1.1
does not hold in existing semantics in the context of differentiation:
Our semantics validates this equation with the following theorem:

4known as catamorphisms in the functional programming community

, Vol. 1, No. 1, Article . Publication date: July 2022.

14

Pedro H. Azevedo de Amorim and Christopher Lam

Theorem 6.4. For every context Γ and well-typed programs Γ ⊢ 𝑏 : Pred(R𝑛), Γ ⊢ 𝑡, 𝑢, 𝑒 : R𝑛 → R,
(𝑟 )then we have the
𝑏
if
(cid:75)
(cid:74)
equality

(𝑟 ) = 𝑓 𝑓 implies
1𝑏𝑒 + .1¬𝑏𝑢

(𝑟 ) = 𝑡𝑡 implies

(𝑟 ) and

(𝑟 ) =

𝑢
(cid:74)
.

𝑡
(cid:74)
=

𝑏
(cid:74)

𝑒
(cid:74)

𝑡
(cid:74)

(cid:75)

(cid:75)

(cid:75)

(cid:75)

(𝑟 ) =
(cid:75)
lift(𝑡)
(cid:74)
=

(cid:75)
1𝑏𝑡 + .1¬𝑏𝑡

(cid:74)
Proof.

(cid:75)
1𝑏𝑒 + .1¬𝑏𝑢

(cid:74)

(cid:75)

(cid:74)

=

lift(𝑡)

(cid:75)

(cid:74)

(cid:75)

This theorem implies Theorem 1.1, resolving this issue.

□

7 CASE STUDIES

7.1 Bouncing Ball

Let us consider a variant of the example presented in Section 4 where there is a ball moving at
constant velocity 𝑣 perpendicular to a wall and it elastically hits a wall at time 𝑡𝑤𝑎𝑙𝑙 and moves in
the opposite direction with velocity −𝑣. The program that computes the velocity is given by:

By unfolding the definitions we can easily show (cid:113)
delta is modelling the time of collision with the wall.

𝑢 = 1𝑡 <=𝑡𝑤𝑎𝑙𝑙 (𝜆𝑡 . 𝑣) + .1𝑡 >𝑡𝑤𝑎𝑙𝑙 (𝜆𝑡 . − 𝑣)

(6)
𝜕𝑢
𝜕𝑡 (cid:121) = 𝛿𝑡𝑤𝑎𝑙𝑙 . As it is usually the case, the Dirac

Existing semantics of differentiable programming fail in two ways when trying to model phenom-
ena with non-smooth behavior like this. The first is to make the whole phenomenon inexpressible
by disallowing any form of non-smooth behavior. This means that 6 and other simple physical
phenomena are completely inexpressible in these semantics. Nonetheless, this approach was used
by [Huot et al. 2020] and [Ehrhard and Regnier 2003] among others.

The second approach is somewhat more interesting, in that it utilizes a form of partial semantics
that completely ignores the point of collision, such as the semantics of [Abadi and Plotkin 2019]. In
their semantics, it is possible to express a form of 6 as the following program:

However, by differentiating the program above using their semantics we get

𝑀 = if 𝑡 < 0 then 𝑣 else − 𝑣

(cid:115) d𝑀
d𝑡

(cid:123) =

(cid:40)0
if 𝑡 ≠ 0
⊥ otherwise

Were we to attempt to model the position of the ball using the velocity with these existing
semantics, the model would not be able to distinguish between the simple physical reality of the
ball bouncing off of the wall and the ball passing through it. More recently, Matthijs Vakar has also
posted some preliminary work on extending this flavor of partiality semantics on the arXiv, but
their approach still fails to model this phenomena. Even the approach in [Sherman et al. 2020] with
its advances in modeling non-smooth functions is unable to distinguish between these situations
as they also force the function into undefined behavior at points of discontinuity.

7.2 Derivatives of Intergrals

In this example we will show how our syntax and semantics can properly deal with the interaction
of differentiation and integration. We illustrate this by coming back to the example in Section 4.1
showing that the equation d
d𝑝

∫ 10≤𝑥 ≤𝑝 d𝑥 = 10≤𝑥 ≤1 holds in our semantics.

First, we present a method of computing integrals of compactly supported functions using our
semantics. Let 𝐾1 ⊂ 𝐾2 be two compact subsets of R𝑛 such that 𝐾1 is strictly contained in 𝐾2. There
exists a smooth function 𝜓 such that 0 ≤ 𝜓 ≤ 1, 𝜓 |𝐾1 ≡ 1 and 𝜓 |R𝑛\𝐾2 ≡ 0 [Lee 2013]. Because this
function is smooth and has compact support, it allows us to effectively construct a test function that

, Vol. 1, No. 1, Article . Publication date: July 2022.

Distribution Theoretic Semantics for Non-Smooth Differentiable Programming

15

is constant on any arbitrary compact set. Intuitively, this function is 1 on 𝐾1, transitions smoothly
on from 1 to 0 on 𝐾2 \ 𝐾1, and is 0 everywhere else.

We will denote this function as 𝜓 𝐾2
𝐾1

. If we apply a distribution 𝑇𝑓 to it we get:

∫

R𝑛
∫

⟨𝑇𝑓 ,𝜓 𝐾2
𝐾1

⟩ =

𝑓 (𝑥)𝜓 𝐾2
𝐾1

(𝑥) d𝑥 =

∫

𝐾1

𝑓 (𝑥) d𝑥 +

𝑓 (𝑥)𝜑𝐾2
𝐾1

(𝑥) d𝑥

𝐾2\𝐾1

Importantly, if 𝑓 has compact support, then by carefully choosing 𝐾1 such that supp(𝑓 ) ⊆ 𝐾1,

we can get the equality ⟨𝑇𝑓 ,𝜓 𝐾2
𝐾1

⟩ = ∫
𝐾1

𝑓 (𝑥) d𝑥.

We want to compute the derivative of 𝜆𝑝. ∫ 10<𝑥 <𝑝 , which, as we explained, can be computed

by the term

.

𝑡 =

𝜕
𝜕𝑡

(lift(𝜆 𝑡 . ⟨1(𝜆 𝑥 . 0<𝑥 <𝑡 ),𝜓 𝐾2

[0,1]⟩))

Now it is easy to show that
Note that while we do not explicitly have the 𝜓 𝐾2
(cid:75)
terms in our syntax, if we add constructors
𝐾1
to 𝜆𝛿 where 𝐾1 and 𝐾2 are cubes or spheres such that 𝐾1 ⊂ 𝐾2 then we would already be able

𝜓 𝐾2
𝐾1
to program many interesting examples, as we will see in the next section.

10≤𝑥 ≤1(𝜆𝑥 .1)

𝑡
(cid:74)

=

(cid:75)

(cid:74)

As many applications heavily rely on the interaction of integrals and differentiation[Bangaru
et al. 2020, 2021; Li et al. 2018a], it is important to develop semantics that can soundly justify these
equations. Indeed, this kind of reasoning is frequently used by practioners, as illustrated by Bangaru
and Michel et al. [Bangaru et al. 2021], where these ideas have been used to automatically rewrite
programs in the differentiable programming language TEG. It is important to note that their system
cannot properly reason about the example above.

While the programming model imposed by working with distributions cannot compute the value
of a function at a point, we can approximate it by applying a bump function with a sufficiently small
radius. Unfortunately this becomes problematic when considering what an operational semantics
to such language would look like, as applying a test function to a distribution that originated from
a smooth function is integrating their product, meaning that an operational semantics for 𝜆𝛿 would
require computing integrals.

For implementation purposes, however, the point above is not too problematic, as there are
many algorithms that can efficiently compute approximations to arbitrary integrals — we have
implemented a small library to demonstrate the use of distributions and key concepts from our
language in a software artifact. See Appendix 9 for more details.

7.3 Validating Equations for a Ray Tracing Algorithm

We have already argued in Section 4.3 that using a non-distribution-theoretic semantics makes
it impossible to validate the equations used by Li et al. In this section we are going to show how
we can use 𝜆𝛿 and its semantics to correctly reason about a simplified implementation of the ray
tracing algorithm.

As we have shown above, 𝜆𝛿 provides a simple way of computing the integrals of compactly

supported functions.

, Vol. 1, No. 1, Article . Publication date: July 2022.

16

Pedro H. Azevedo de Amorim and Christopher Lam

𝑖𝑛𝑡𝑒𝑔𝑟𝑎𝑙 : D ′(R2) → R2 → R2 → R
𝑖𝑛𝑡𝑒𝑔𝑟𝑎𝑙 𝑇 (𝑥1, 𝑦1) (𝑥2, 𝑦2) = ⟨𝑇 ,𝜓 𝐾2
𝐾1

⟩,

Where 𝐾1 = [𝑥1, 𝑥2] × [𝑦1, 𝑦2] and 𝐾2 = [2𝑥1, 2𝑥2] × [2𝑦1, 2𝑦2]. Next, we need to write a program
that computes the integrand 𝑓 (𝑥, 𝑦, Φ). For the sake of convenience, we extend 𝜆𝛿 with lists which
is semantically valid because Diff is cocomplete and, therefore, can interpret inductive types and
their inductive principles, which for lists is the familiar fold. Syntactically 𝜆𝛿 will look exactly like a
regular 𝜆-calculus equipped with inductive types; see Huot et al. [Huot et al. 2020] for more details.
With this extension we can write a program that, given a list of triangles and their characteristic
functions, returns the sum of distributions (cid:205)𝑖 1𝛼𝑖 𝑓𝑖 , where 𝛼𝑖 is the predicate for the 𝑖-th triangle
and 𝑓𝑖 is its characteristic function:

𝑐ℎ𝑎𝑟 𝐹𝑢𝑛𝑐 : Φ → [(Pred(R𝑛), R2 × Φ → R)] → D ′(R2)
𝑐ℎ𝑎𝑟 𝐹𝑢𝑛𝑐
𝑐ℎ𝑎𝑟 𝐹𝑢𝑛𝑐 𝜑 ((𝑓𝑖, 𝛼𝑖 ) :: 𝑡𝑙) = 1𝛼𝑖 (𝜆𝑟 . 𝑓𝑖 (𝑟, 𝜑)) +. (𝑐ℎ𝑎𝑟 𝐹𝑢𝑛𝑐 𝜑 𝑡𝑙)
Once again, note that the program above is structuraly recursive and thus can be implemented
with a fold without having to make use of full recursion. We can now implement the pixel value
function 𝐼 :

[ ] = 0

𝐼 : [(Pred(R2), R2 × Φ → R)] → R2 → R2 → (Φ → R)
𝐼 𝑙 (𝑥1, 𝑥2) (𝑦1, 𝑦2) = 𝜆𝜑. 𝑖𝑛𝑡𝑒𝑔𝑟𝑎𝑙 (𝑐ℎ𝑎𝑟 𝐹𝑢𝑛𝑐 𝜑 𝑙) (𝑥1, 𝑥2) (𝑦1, 𝑦2)

Assuming that the support of 𝑐ℎ𝑎𝑟 𝐹𝑢𝑛𝑐 𝜑 𝑙 is a subset of the integral domain, it is easy to show
that the semantics of I is exactly the formula presented by Li et al. By unfolding the semantics of
∇(lift (𝐼 𝑙 (𝑥1, 𝑥2) (𝑦1, 𝑦2))) we can see where adopting a non-distributional semantics would be
problematic:

∇(lift (𝐼 𝑙 (𝑥1, 𝑥2) (𝑦1, 𝑦2))
∫ (𝑥2,𝑦2)

(cid:75)

(𝑐ℎ𝑎𝑟 𝐹𝑢𝑛𝑐 𝜑 𝑙) = ∇

(cid:74)
∇

(𝑥1,𝑦1)

(cid:74)
∫ (𝑥2,𝑦2)

∑︁

(𝑥1,𝑦1)

𝑖

= ∇

(lift 𝐼 𝑙 (𝑥1, 𝑥2) (𝑦1, 𝑦2))

=

(cid:75)

1𝛼𝑖 (𝜆𝑟 . 𝑓𝑖 (𝑟, 𝜑)) =

∑︁

∇

𝑖

∫ (𝑥2,𝑦2)

(𝑥1,𝑦1)

1𝛼𝑖 (𝜆𝑟 . 𝑓𝑖 (𝑟, 𝜑))

Existing AD algorithms will always commute with integrals — as they are implemented using
finite sums — even when the Leibniz theorem 5 does not hold. Fortunately, distributions do not
suffer from this drawback, so our semantics would be able to commute ∇ and ∫ and soundly apply
the last steps of the reasoning done in Section 4.3.

The incompatibility of regular AD and integration has been observed by Bangaru, Michel et al in
recent work[Bangaru et al. 2021], where they have coined the terms "discretize-then-differentiate"
and "differentiate-then-discretize" to contrast the standard approach to AD with the distribution-
theoretic one.

5https://en.wikipedia.org/wiki/Leibniz_integral_rule

, Vol. 1, No. 1, Article . Publication date: July 2022.

Distribution Theoretic Semantics for Non-Smooth Differentiable Programming

17

8 TRANSLATING TO AND FROM OTHER DIFFERENTIABLE LANGUAGES
8.1 Embedding a Smooth 𝜆-calculus

It is an important question to understand how our language and semantics relate to existing
semantics of differentiable programming. In this section we will show how the language proposed
by [Huot et al. 2020] can be soundly embedded in our language and how their AD program
transformation relates to our differentiation operation.

In their work they define a simply typed 𝜆-calculus with one base type for the real numbers and
smooth primitives (e.g. the sine function). Their differentiation program transformation follows the
dual number approach to AD, where each input carries an extra parameter corresponding to the
derivative in that argument. This is achieved by defining the following type transformation:

D (R) = R × R

D (𝜏 × 𝜏) = D (𝜏) × D (𝜏)
D (𝜏 → 𝜏) = D (𝜏) → D (𝜏)
They also define a transformation D (−) at the term level and prove that if Γ ⊢ 𝑡 : 𝜏 then
D (Γ) ⊢ D (𝑡) : D (𝜏). Their translation is elegant and can be shown to be functorial, which is a
consequence of its compositionality. Their (simplified) correctness property is:

Lemma 8.1 ([Huot et al. 2020]). If 𝑥 : R ⊢ 𝑡 : R then for every smooth function 𝑓
(𝑓 , 𝑓 ′);

) ′), where (−) ′ is the derivative operation.

= (𝑓 ;

D (𝑡)

, (𝑓 ;

: R → R,

(cid:74)

(cid:75)

𝑡
(cid:74)

(cid:75)

𝑡
(cid:74)

(cid:75)

Their full correctness theorem takes into account arbitrary open term such that the inputs and
outputs are smooth manifolds. For sake of presentation, we focus on this simplified case. Due to
their language also being a 𝜆-calculus that is interpreted Diff, the identity translation is well-typed
and so is their AD translation. Now, we can state the theorem:

Theorem 8.2. Let 𝑥 : R ⊢ 𝑡 : R be a well-typed program in the original language. Consider the terms
· ⊢ 𝑡1 = 𝜆𝑥 . 𝜋1(D (𝑡)(𝑥, 1)) : R → R and 𝑡2 = 𝜆𝑥 . 𝜋2(D (𝑡)(𝑥, 1)) : R → R, then

(cid:115)(lift(𝜆𝑥 . 𝑡),

𝜕
𝜕𝑥

(lift(𝜆𝑥 . 𝑡)))(cid:123) =

(lift(𝑡1), lift(𝑡2))

(cid:75)

(cid:74)

Proof. The proof is a straightforward application of Theorem A.9 and Lemma 8.1 with 𝑓 =
□

𝜆𝑥 . 𝑥.

By using the general soundness theorem of [Huot et al. 2020], slightly massaging the statement
above and modifying the programs 𝑡1 and 𝑡2 it is possible to prove a similar version of the theorem
above for open terms 𝑥1 : R, · · · , 𝑥𝑛 : R ⊢ 𝑡 : R𝑚.

8.2 Constructive Semantics

An important aspect of the semantics presented by [Sherman et al. 2019] is that their language
has datatypes both open and compact subsets of topological spaces, which in their language are
type constructors OShape and KShape. Due to idiosyncrasies of constructive topology, they also
make heavy use of the type OShape 𝐸 × KShape 𝐸, which they call OKShape. In the original papers
[Sherman et al. 2019, 2020] they go over the formalities that make their semantics work and be
computable. In this section we are only interested in their provided API to program with these
spaces which, in particular, makes it possible to compute integrals over compact domains of R𝑛.
This suggests that it should be possible to translate 𝜆𝛿 into their metalanguage. We define such a
translation

𝛿 , starting with the type translation depicted in Figure 8.
−
(cid:77)

(cid:76)

, Vol. 1, No. 1, Article . Publication date: July 2022.

18

Pedro H. Azevedo de Amorim and Christopher Lam

𝛿 = ℜ
R
(cid:77)
(cid:76)
𝜏2
𝜏1
𝜏1 × 𝜏2
𝛿 ×
𝛿 =
𝛿
(cid:77)
(cid:77)
(cid:76)
(cid:76)
(cid:77)
(cid:76)
𝜏2
𝜏1
𝜏1 → 𝜏2
𝛿 →
𝛿 =
𝛿
(cid:77)
(cid:76)
(cid:77)
(cid:76)
(cid:77)
(cid:76)
Pred(R𝑛)
𝛿 = OKShape(ℜ𝑛)
(cid:77)
𝛿 = (ℜ𝑛 → ℜ) × (OKShape ℜ𝑛)
D (R𝑛)
(cid:77)
(cid:76)
D ′(R𝑛)
𝛿 → ℜ
𝛿 =
(cid:77)
(cid:77)

D (R𝑛)

(cid:76)

(cid:76)

(cid:76)

Fig. 8. Type translation into 𝜆𝑠

(cid:76)

(cid:76)

𝜆𝑥 . (𝜑 𝑥)(

𝑟 , makecube_n 𝑟 𝑐)

𝐾

∫

1𝑏 𝑡

𝜑𝑐
𝑟
(cid:76)
lift 𝑡

𝛿 = (𝜑𝑐
(cid:77)
𝛿 = 𝜆(𝜑, 𝐾).
(cid:77)
𝛿 = 𝜆(𝜑, 𝐾). let 𝐾 ′ = (
(cid:77)
𝛿𝑐
𝛿 = 𝜆(𝜑, 𝐾). 𝜑 𝑐
(cid:77)
(cid:76)
𝜕𝑡
𝛿 = 𝜆(𝜑, 𝐾). −
𝜕𝑥𝑖 (cid:77)

𝑡
(cid:76)

𝑏
(cid:76)

(cid:76)

𝛿 𝑥)
(cid:77)

𝑡
(cid:76)
𝛿 ∩ 𝐾) in
(cid:77)

𝛿 ((derivative𝑖 𝜑), 𝐾)
(cid:77)

∫

𝐾 ′

(𝜆𝑥 .

𝑡
(cid:76)

𝛿 (𝑥) ∗ (indicator 𝐾 ′ 𝑥) )
(cid:77)

Fig. 9. Selected term translations into 𝜆𝑠

Since their semantics is based on constructive topology, they only have a datatype for the
constructive real numbers ℜ. The type constructors of the simply-typed 𝜆 calculus are standard.
The interesting aspects are the distribution theoretic primitives. We interpret predicates as open
sets corresponding to their indicator functions. Test functions are interpreted as pairs of a function
a compact set, i.e. its support. Then, as it is standard, distributions are functions from test functions
to real numbers.

We present parts of the term translation in Figure 9 and, once again, the translation for the
lambda calculus syntax is trivial. The interesting aspects are some of the distribution theoretic
primitives. The primitive test functions 𝜑𝑐
𝑟 are mapped to their mathematical counterparts 𝜑𝑐
𝑟 ,
which are simply a multiplication of exponentials, and to the hypercube centered around 𝑐 and
with sides of length 𝑟 , which can be defined using the primitives6

unit_cube : KShape ℜ
product : KShape ℜ𝔪 → KShape ℜ𝔪 → KShape ℜ𝔪+𝔫
translate : ℜ𝑛 → KShape ℜ𝑛 → KShape ℜ𝑛
scale : ℜ → KShape ℜ𝑛 → KShape ℜ𝑛

These primitives are also available for OShape, making it possible to define an OKShape ℜ𝑛 for
𝑛-dimensional hypercubes.

6https://github.com/psg-mit/marshall/blob/master/examples/stoneworks/krep.asd
https://github.com/psg-mit/marshall/blob/master/examples/stoneworks/orep.asd
https://github.com/psg-mit/marshall/blob/master/examples/stoneworks/okrep.asd

, Vol. 1, No. 1, Article . Publication date: July 2022.

Distribution Theoretic Semantics for Non-Smooth Differentiable Programming

19

The lift primitive uses the iterated integral primitive:

This is simply an iterated application of their primitive

integraln : OKShape ℜ𝑛 → (ℜ𝑛 → ℜ) → ℜ

integral : OKShape ℜ → (ℜ → ℜ) → ℜ
for one dimensional integration over compact support in order to support higher dimensional
integration. We use the syntactic sugar ∫ ≜ integral𝑛.

In order to translate the indicator function we use their primitive

∩ : (𝑂𝐾𝑆ℎ𝑎𝑝𝑒 ℜ𝑛) → (𝑂𝑆ℎ𝑎𝑝𝑒 ℜ𝑛) → (𝑂𝐾𝑆ℎ𝑎𝑝𝑒 ℜ𝑛)
which computes the intersection of a compact subset with an open subset, resulting in a
compact subset. Then, we simply compute the integral over this intersection of the function
𝑓 (𝑥) ∗ (indicator (𝑏 ∩ 𝐾) 𝑥), where the function indicator : 𝑂𝑆ℎ𝑎𝑝𝑒 ℜ𝑛 → ℜ𝑛 → ℜ returns 1
when 𝑥 is in the interior of the shape, and 0 when outside its boundary. Note that for points at the
boundary, this function returns an indeterminate value.

Dirac deltas are translated the standard way. In order to translate derivatives we make use of the

primitive:

derivative : (ℜ → ℜ) → (ℜ → ℜ)
which, by making use of partial application of the test function, allows us to define partial
derivatives of functions ℜ𝑛 → ℜ as the authors show in their paper [Sherman et al. 2020], which
we denote by the syntactic sugar derivative𝑖 . Just an example, a partial derivative of a function
𝑓 : ℜ2 → ℜ can be defined as 𝜆𝑥 . derivative (𝜆𝑦. 𝑓 𝑥 𝑦). The other distributive theoretic primitives
such as distribution application and the vector space structure use the standard translation.

Theorem 8.3. If Γ ⊢𝜆𝛿

𝛿 ⊢𝜆𝑠
(cid:77)
Proof. The proof follows by induction on the typing derivation Γ ⊢𝜆𝛿

𝑡 : 𝜏 then

𝛿 :
(cid:77)

𝛿 .
(cid:77)

𝜏
(cid:76)

𝑡
(cid:76)

Γ

(cid:76)

judgements of the API functions defined above.

𝑡 : 𝜏, using the type
□

We could also prove a similar theorem here as we did in 8.2, where the proof would be almost
identical, save for the fact that soundness of differentiation is given by construction of the primitive
here rather than as a meta-property of a defined macro.

This illustrates that there are other semantic domains that can soundly give semantics to 𝜆𝛿 .
This exercise should not be interpreted as our language not extending the semantics defined by
Sherman et al. Instead, we see this as a valuable addition to their semantics, since previous work
has shown how ignoring jump discontinuities – as it is the case in their base semantics – ignores
important physical interactions in your model, as demonstrated in [Bangaru et al. 2021; Li et al.
2018a]. Furthermore, since this semantics is fully computable and numerically stable, it sidesteps
the undesirable property of noncomputable integrals in the Diff semantics.

9 IMPLEMENTATION

We have implemented a proof-of-concept library for the main concepts in 𝜆𝛿 in PyTorch7. The
distributional derivative is defined in terms of regular derivatives, which opens two possibilities for
implementation: we either define an automatic differentiation procedure suited to distributions, or
we use an out-of-the-box AD procedure and apply it to the test functions. There are advantages and
disadvantages to both approaches. By construction, the distributions definable in 𝜆𝛿 have an easy
to characterize normal-form. Most of the syntactic constructions have an easy to define interaction

7https://github.com/pytorch/pytorch

, Vol. 1, No. 1, Article . Publication date: July 2022.

20

Pedro H. Azevedo de Amorim and Christopher Lam

with derivatives — e.g. the derivative of a lift is the lift of the derivative — with the exception of
the multiplication of a smooth function by a predicate. The feasibility of directly differentiating a
predicate is conditional on its complexity. One way around this problem is by only allowing simple
predicates that have well-known derivatives. For example, the derivative of the predicate 𝜆𝑥 . 𝑥 ≤ 0
is 𝛿0. Unfortunately when dealing with higher-dimensional objects it becomes harder to compute
these derivatives. The advantage of this approach is that it is closer to heart to an important selling
point of AD methods: it is possible to share computations between the computation of a function
and its derivatives.

While the test function approach does not share this computation sharing property, it has the
advantage of being incredibly easy to implement. They are by definition infinitely differentiable
and are simple enough to easily apply any existing AD method to implement their differentiation.
It is for this reason that we take the test function approach in our library.

Another key aspect of our library is distribution application as it relies on higher-dimensional
integration. This feature goes out of the scope of PyTorch’s standard library, so we have used
torchquad’s8 Monte Carlo integration implementation. Because we are using Monte Carlo integra-
tion methods, our distribution application is not strictly deterministic and has some variance from
application to application. This instability further compounds at higher derivatives, though it can
be tamed to a degree by sampling a larger volume of points for the Monte Carlo method. However,
because this implementation is meant to be more of a toy implementation of a few key features of
our language rather than an optimized compiler, we have not attempted to optimize for further
performance.

Our implementation resolves some of the strange behaviour that arises from differentiating
conditionals (such as the maximum function) in PyTorch. For instance, consider the following three
functions:

def sillyID_1(x):

return max(0, x) - max(-x, 0)

def sillyID_2(x):

return max(x, 0) - max(-x, 0)

def sillyID_3(x):

return max(0, x) - max(0, -x)

These are each a different way of implementing the identity function which appear to be
equivalent save for the order of the arguments. However, each of these functions will return a
different derivative at the origin when fed into PyTorch’s AD implementation. More specifically,
the first will return 1, the second will return 0, and the third will return 2. Our implementation
resolves this inconsistency modulo the noise from numerical integration.

Finally, if we were to implement an actual compiler to our language, we would avoid using
integration as much as possible, as it is inefficient and susceptible to floating point errors. In an
actual optimizing compiler for 𝜆𝛿 we could rewrite distribution applications ⟨lift 𝑡, 𝜑𝑛 (𝑐, 𝑟 )⟩ as
𝑡 𝑐, which would save computation when evaluating away from boundaries of discontinuity. We
could apply a similar procedure for differentiation and apply normal AD algorithms far from these
boundaries.

8https://github.com/esa/torchquad

, Vol. 1, No. 1, Article . Publication date: July 2022.

Distribution Theoretic Semantics for Non-Smooth Differentiable Programming

21

10 RELATED WORK

Higher-order
Functions

Higher
Derivatives

Non-Smooth
Conditionals

Theorem 6.4

✗

✓

✓

✗

✓

[Abadi
and
Plotkin 2019]

[Huot et al.
2020]

[Sherman
et al. 2020]

[Bangaru
et al. 2021]

[Ehrhard
and Regnier
2003]

This Work

✓

✓

✓

✓

✓

✓

✓

✓(Partiality)

✗

✗

✗(Locally
Lipschitz)

✓

✗

✓

N/A

N/A

✓

N/A

✓

Recursive languages. Abadi and Plotkin [Abadi and Plotkin 2019] define a first-order programming
language with a reverse-mode AD construct and while loops. They define the semantics of their
language using the fact that the set of infinitely differentiable partial functions R𝑛 ⇁ R𝑚 forms a
pointed CPO. They define an operational semantics that implements an AD algorithm and prove it
adequate with respect to their denotational semantics. Their language does not support higher-
order functions and uses non-termination to deal with jump discontinuities. In order to prove the
correctness of automatic differentiation everywhere the program terminates they work with partial
predicates 𝑝 : 𝑅𝑛 ⇁ {𝑡𝑡, 𝑓 𝑓 } such that both 𝑝−1({𝑡𝑡 }) and 𝑝−1({𝑓 𝑓 }) are open sets. However,
adding non-terminating behavior at points of discontinuity loses the expressive power required to
describe models such as those in 4.

Semantics for Macro-Based AD. Huot et al. [Huot et al. 2020] also use the category Diff to give
semantics to a differentiable 𝜆-calculus. They define a global program transformation corresponding
to an implementation of forward-mode automatic differentiation which they show corresponds
to the semantic differentiation by a logical relations argument. While they use the same semantic
model we do, their language lacks the distribution theoretic machinery developed here. As a result,
their language lacks a conditional construct beyond pattern matching on tuples, which severely
limits the programs expressible. In fact, none of the examples presented in Section 4 are expressible
in their language. Furthermore, as we have shown in Section 8.1, our semantics can be seen as a
conservative extension of their semantics.

Matthijs Vakar has a long line of work on defining languages where an automatic differentiation
operator is sound. This began with his collaboration with Huot and Staton [Huot et al. 2020], and
has consistently used a smooth model similar to the one described therein with the same limitations
on conditional statements. However, in [Vakar 2020] posted on arXiv (and thus we only tenatively
include it given its lack of peer review), he presents a model of a differentiable programming
language with a sign construct, which gives rise to non-smooth behavior in programs. He uses
an approach similar to [Abadi and Plotkin 2019] in that he leaves sign a partial function that is
undefined at zero, which gives rise to the same expressiveness issues. It is unclear at the moment if
there is a sound translation from his language into ours, since our programs are total.

, Vol. 1, No. 1, Article . Publication date: July 2022.

22

Pedro H. Azevedo de Amorim and Christopher Lam

Constructive Semantics. While most of the existing approaches (described in 3) cannot handle
differentiating directly at points of discontinuity, the semantics defined by Sherman et al. [Sher-
man et al. 2020] uses ideas from constructive topology to interpret a differentiable language that
admits higher-order functions and locally-Lipschitz functions that may be differentiated. What
distinguishes their approach from ours is that to interpret non-smooth programs they use the
idea of subgradient, formally expressed by the concept of Clarke derivatives. A consequence of
using Clarke derivatives is that the higher derivatives of their non-differentiable locally-Lipschitz
functions are undefined.

To interpret the higher-order fragment they use a sheaf-theoretic construction which is similar
to the one used for Diff. They define a tangent bundle functor using Kan extensions, similar to the
construction presented by Staton et al. [Staton et al. 2016] for the Giry monad. A consequence of
their semantics is that even though their semantic category has coproducts and therefore, admits
pattern matching, the only predicates R𝑛 → 2 available are the constant ones, because morphisms
are continuous and R𝑛 is connected but 2 is not. This prohibits expressing discontinuous functions
such as the Heaviside function in their language. Furthermore, even though they would satisfy
a kind of Theorem 6.4, it would be trivially true, since the predicate has to be constant. In short,
while their language uses impressive mathematical machinery to constructively express a much
larger class of functions than previous differentiable languages, local Lipschitz continuity is not
sufficient for many useful conditionals, and Clarke derivatives fail to capture the true behavior of
even those locally-Lipschitz continuous functions at higher derivatives.

In Section 8.2 we have shown how it is possible to embed 𝜆𝛿 in their language. We also see this
as our language subsuming their original semantics, since non distribution theoretic semantics for
nondifferentiable programs will inevitably ignore jump discontinuities.

Differential Linear Logic. Since the turn of the century, kickstarted by Ehrhard and Regnier
[Ehrhard and Regnier 2003], much work has been done in bridging the gap between differentiation
and logic. This has led to the discovery of differential linear logic (DiLL), the differential 𝜆-calculus
and the categorical formulation of differentiation. Though much work has been done on these
categorical formalisms, it is still not clear how these models could handle certain features that are
expected from the programming languages community – recursion and if-statements being two
of those. That being said, some models of the differential 𝜆-calculus are connected to distribution
theory. Kerjean and Tasson [Kerjean and Tasson 2018] have defined a model of differential linear
logic where the exponential !𝐴 is interpreted as the compactly-supported distributions over 𝐴.
Further research is needed to understand if there is a treatment of DiLL that can handle non-
compactly-supported distributions.

Distribution Theory in Computer Science. The idea of using distribution theory to interpret jump
discontinuities has also been used in [Nilsson 2003] to write a Haskell library for functional reactive
programming. However, their approach required two restrictive preconditions: the program had to
be provided with the locations of the discontinuities, and the number of discontinuities had to be
finite.

Finally, a distribution-theoretic semantics is at the core of the differentiable language TEG
[Bangaru et al. 2021] — once again showing that ideas from distribution theory are already used by
practioners of differentiable programming. They have defined an untyped, first-order language that
has both differentials as well as integral primitives. With their distribution theoretic semantics they
focus on reasoning equationally about the differentiation of the integral of non-smooth functions.
The main drawbacks of their language when compared to 𝜆𝛿 is that the fact that they are untyped
create some restrictions on the programs they can write. Besides, since 𝜆𝛿 has higher-order functions
it provides more expressive primitives to the programmer. It would be interesting future work to

, Vol. 1, No. 1, Article . Publication date: July 2022.

Distribution Theoretic Semantics for Non-Smooth Differentiable Programming

23

extend the TEG language with a type system and higher-order functions so that 𝜆𝛿 could be seen
as its idealized core calculus.

11 FUTURE WORK AND CONCLUSION

We have defined a denotational semantics for 𝜆𝛿 , a higher-order differentiable language extended
with distribution theoretic primitives which provides a solution to the if-statement problem in
differentiable programming. Our semantics is the first that validates certain expected if-statements
equations. We highlight the fact that there might other interesting models to the calculus presented
here.

For future work we would like to better understand how our semantics could be used to study
the solution of linear partial differential equations. Something similar was done by Kerjean [Ker-
jean 2018] using compactly supported distributions. By adding recursion to 𝜆𝛿 and defining its
denotational semantics we would have syntax to express the solution of differential equations using
the fixed-point operator of our language, making the connections to physics simulators even more
explicit. Furthermore, we conjecture that by allowing non-terminating behavior it might be easier
to get a better categorical understanding of what distributions are, as test functions might simply
be smooth functions, instead of compactly supported ones.

Another promising line of work that requires further research is developing a theory of probability
inside Diff. Many modern Bayesian inference engines rely on differentiable programming. Therefore,
it is paramount to develop a theory that encompasses both differential and probabilistic primitives.

REFERENCES
Martín Abadi, Paul Barham, Jianmin Chen, Zhifeng Chen, Andy Davis, Jeffrey Dean, Matthieu Devin, Sanjay Ghemawat,
Geoffrey Irving, Michael Isard, et al. 2016. Tensorflow: A system for large-scale machine learning. In Operating Systems
Design and Implementation (OSDI)).

Martín Abadi and Gordon D Plotkin. 2019. A simple differentiable programming language. In Principles of Programming

Languages (POPL).

Sai Praveen Bangaru, Tzu-Mao Li, and Frédo Durand. 2020. Unbiased warped-area sampling for differentiable rendering.

ACM Transactions on Graphics (TOG) 39, 6 (2020), 1–18.

Sai Praveen Bangaru, Jesse Michel, Kevin Mu, Gilbert Bernstein, Tzu-Mao Li, and Jonathan Ragan-Kelley. 2021. Systematically

differentiating parametric discontinuities. ACM Transactions on Graphics (TOG) 40, 4 (2021), 1–18.

Thomas Beck and Herbert Fischer. 1994. The if-problem in automatic differentiation. J. Comput. Appl. Math. 50, 1-3 (1994),

119–131.

Richard Blute, Thomas Ehrhard, and Christine Tasson. 2012. A convenient differential category. Cahiers de topologie et

géométrie différentielle catégoriques 53, 3 (2012), 211–232.

Jonas Degrave, Michiel Hermans, Joni Dambre, et al. 2019. A Differentiable Physics Engine for Deep Learning in Robotics.

Frontiers in Neurorobotics 13 (2019).

Thomas Ehrhard and Laurent Regnier. 2003. The differential lambda-calculus. Theoretical Computer Science 309, 1-3 (2003),

1–41.

Françoise Golse. 2010. Distributions, analyse de Fourier, équations aux dérivées partielles. Cours de l’École Polytechnique.

Accessible à l’adresse http://www. math. polytechnique. fr/golse/mat431. html (2010).

Andreas Griewank et al. 1989. On automatic differentiation. Mathematical Programming: recent developments and applications

6, 6 (1989), 83–107.

Chris Heunen, Ohad Kammar, Sam Staton, and Hongseok Yang. 2017. A convenient category for higher-order probability

theory. In Symposium on Logic in Computer Science (LICS).

Yuanming Hu, Luke Anderson, Tzu-Mao Li, Qi Sun, Nathan Carr, Jonathan Ragan-Kelley, and Frédo Durand. 2020. DiffTaichi:
Differentiable Programming for Physical Simulation. In International Conference on Learning Representations (ICLR).
Mathieu Huot, Sam Staton, and Matthijs Vákár. 2020. Correctness of Automatic Differentiation via Diffeologies and

Categorical Gluing.. In Foundations of Software Science and Computation Structures (FoSSaCS).

Patrick Iglesias-Zemmour. 2013. Diffeology. Vol. 185. American Mathematical Soc.
Mike Innes, Alan Edelman, Keno Fischer, Chris Rackauckas, Elliot Saba, Viral B Shah, and Will Tebbutt. 2019. A differentiable
programming system to bridge machine learning and scientific computing. arXiv preprint arXiv:1907.07587 (2019).

Marie Kerjean. 2018. A logical account for linear partial differential equations. In Logic in Computer Science (LICS).

, Vol. 1, No. 1, Article . Publication date: July 2022.

24

Pedro H. Azevedo de Amorim and Christopher Lam

Marie Kerjean and Christine Tasson. 2018. Mackey-complete spaces and power series–a topological model of differential

linear logic. Mathematical Structures in Computer Science 28, 4 (2018), 472–507.

Anders Kock and Gonzalo E Reyes. 2004. Categorical distribution theory; heat equation. arXiv preprint math/0407242 (2004).
John M Lee. 2013. Smooth manifolds. In Introduction to Smooth Manifolds. Springer, 1–31.
Li Li, Stephan Hoyer, Ryan Pederson, Ruoxi Sun, Ekin D. Cubuk, Patrick Riley, and Kieron Burke. 2021. Kohn-Sham Equations
as Regularizer: Building Prior Knowledge into Machine-Learned Physics. Phys. Rev. Lett. 126 (Jan 2021), 036401. Issue 3.
https://doi.org/10.1103/PhysRevLett.126.036401

Tzu-Mao Li, Miika Aittala, Frédo Durand, and Jaakko Lehtinen. 2018a. Differentiable monte carlo ray tracing through edge

sampling. ACM Transactions on Graphics (TOG) 37, 6 (2018), 1–11.

Tzu-Mao Li, Michaël Gharbi, Andrew Adams, Frédo Durand, and Jonathan Ragan-Kelley. 2018b. Differentiable programming
for image processing and deep learning in Halide. ACM Trans. Graph. (Proc. SIGGRAPH) 37, 4 (2018), 139:1–139:13.
Henrik Nilsson. 2003. Functional automatic differentiation with dirac impulses. In International Conference on Functional

Programming (ICFP).

Barak A Pearlmutter and Jeffrey Mark Siskind. 2008. Reverse-mode AD in a functional framework: Lambda the ultimate

backpropagator. ACM Transactions on Programming Languages and Systems (TOPLAS) 30, 2 (2008), 1–36.

Federico Pierucci. 2017. Nonsmooth optimization for statistical learning with structured matrix regularization. Ph. D.

Dissertation. Université Grenoble Alpes.

Benjamin Sherman, Jesse Michel, and Michael Carbin. 2019. Sound and robust solid modeling via exact real arithmetic and

continuity. Proceedings of the ACM on Programming Languages 3, ICFP (2019), 1–29.

Benjamin Sherman, Jesse Michel, and Michael Carbin. 2020. 𝜆𝑆 : Computable semantics for differentiable programming with

higher-order functions and datatypes. In Principles of Programming Languages (POPL).

Sam Staton, Frank Wood, Hongseok Yang, Chris Heunen, and Ohad Kammar. 2016. Semantics for probabilistic programming:

higher-order functions, continuous distributions, and soft constraints. In Logic in Computer Science (LICS).

Matthijs Vakar. 2020. Denotational Correctness of Foward-Mode Automatic Differentiation for Iteration and Recursion.

https://doi.org/10.48550/ARXIV.2007.05282

A DISTRIBUTION THEORY

The original motivation of distribution theory was studying the solutions of linear partial
differential equations. Many constructions for functions with codomain R have a distribution
theoretic analogue, which is why they are usually referred to as “generalized functions”. In our case
we are interested in the theory of differentiability of distributions, as they allow us to differentiate
certain functions with jump discontinuities or other points of non-differentiability. Since distribution
theory is not commonly used by researchers in programming languages this section serves as a
self-contained introduction to the subject — see Golse [Golse 2010] for a more detailed presentation.

A.1 Definitions
Definition A.1. Let 𝑋 be a topological space and 𝑓 : 𝑋 → R be a function. We define the support
of 𝑓 as supp(𝑓 ) = {𝑥 | 𝑓 (𝑥) ≠ 0}, where 𝐴 is the topological closure of a subset 𝐴.

Loosely speaking, the support of a real-valued function is the set of points where the function is
non-zero. We say that the support is compact if it is bounded. Throughout this section 𝑈 ⊆ R𝑛 will
always be assumed to be an open set.

𝑐 (𝑈 ) is the set of compactly supported infinitely differentiable functions 𝑓 :
Definition A.2. 𝐶∞
𝑈 → R, we may also call this set D (𝑈 ). The elements of this set are usually referred to as test
functions.

It is easy to see that test functions are closed under addition and scalar multiplication. The
smoothness and compactness requirements are essential in the construction of distributions. Note
that the smoothness requirement rules out most compactly supported functions. For our purposes
there is a particular class of test functions which are the most useful ones and are easy to construct.

, Vol. 1, No. 1, Article . Publication date: July 2022.

Distribution Theoretic Semantics for Non-Smooth Differentiable Programming

25

Fig. 10. Bump function centered at (0,0)

Fig. 11. First derivative of a bump function

Example A.3. Let 𝑐 ∈ R and 𝑟 ∈ R+. The bump function 𝜑𝑐

𝑟 is the following:

1
𝑥 −𝑐
𝑟

) 2

1−(

𝜑𝑐
𝑟 =

(cid:40)
𝑒−
0

if 𝑐 − 𝑟 < 𝑥 < 𝑐 + 𝑟
otherwise

Intuitively speaking, 𝜑𝑐
𝑟 is an infinitely differentiable bump of radius 𝑟 and centered at point 𝑐.
𝜑 0
1 is pictured in figure 10. Additionally, because bump functions are closed under multiplication,
we can easily extend this definition to the multivariate case:

Example A.4. Let 𝑐 = (𝑐1, 𝑐2, . . . , 𝑐𝑛) ∈ R𝑛 and 𝑟 ∈ R+. The multivariate bump function 𝜑𝑐
is the following:

𝑟 : R𝑛 → R

𝑟 (𝑥1, 𝑥2, . . . , 𝑥𝑛) = 𝜑𝑐1
𝜑𝑐

𝑟 (𝑥1)𝜑𝑐2
The set of test functions when equipped with function addition and multiplication by a scalar
forms a vector space. More specifically, this means that we can normalize the volume of the image
of the test function regardless of the radius or degree of differentiation.

𝑟 (𝑥2) . . . 𝜑𝑐𝑛

𝑟 (𝑥𝑛)

With these definitions in mind we define distributions as the set of continuous linear functionals
D ′(𝑈 ) = D (𝑈 ) ⊸ R. In the literature it is common to use the letters 𝑇 and 𝑆 to represent
distributions, and ⟨𝑇 , 𝜑⟩ to denote the application of a distribution 𝑇 to a test function 𝜑. It is
important to note that the test functions having compact support is fundamental when defining
derivatives of distributions, as illustrated by Theorem A.8. The set of distributions over an open
set forms a vector space, where addition is defined as ⟨𝑇1 + 𝑇2, 𝜑⟩ = ⟨𝑇1, 𝜑⟩ + ⟨𝑇2, 𝜑⟩ and scalar
multiplication is defined as ⟨𝛼𝑇 , 𝜑⟩ = 𝛼 ⟨𝑇 , 𝜑⟩. This construction will be used in Section 5 to encode
if-statements.

There are two particular families of distributions that should be mentioned, as they are the most

important ones for our semantics.

Example A.5. Let 𝑢 ∈ 𝑈 , we define the Dirac delta distribution 𝛿𝑢 : D ′(𝑈 ) as ⟨𝛿𝑢, 𝜑⟩ = 𝜑 (𝑢)

This next class of distributions is why we call them “generalized functions”, as every sufficiently

nice function into R can be lifted to a distribution:
Theorem A.6 ([Golse 2010]). Let 𝑓
∫
𝑈
Remark A.7. Something peculiar about distributions is that we lose the ability to compute their
values at a point, as they must be fed a continuation to output a real number. However, this is not

𝑓 (𝑥)𝜑 (𝑥)𝑑𝑥 is a distribution. Note that this definition uses the Lebesgue integral.

: 𝑈 → R be a locally integrable function, then ⟨𝑇𝑓 , 𝜑⟩ =

, Vol. 1, No. 1, Article . Publication date: July 2022.

26

Pedro H. Azevedo de Amorim and Christopher Lam

too big a problem as we can use bump functions (c.f. Figure 10 for an example) centered around a
point with support contained in a sphere of radius 𝜀 which approximates to an arbitrary precision
the value of a function at a point.
Theorem A.8. Let 𝑓 : 𝑈 → R be a locally integrable function, 𝑢 ∈ 𝑈 a point where 𝑓 is continuous
and 𝜓𝜀 a family of positive test functions with volume 1 such that its support is included in the
𝑢-centered 𝜀-radius sphere, then lim𝜀→0⟨𝑇𝑓 ,𝜓𝜀⟩ = 𝑓 (𝑢).
A.1.1 Differentiation. For every distribution𝑇 ∈ D ′(𝑈 ), where 𝑈 ⊆ R𝑛 is an open set we can define
its partial derivative as the distribution 𝜕𝑇
⟩. As such, distributional
𝜕𝑥𝑖
derivatives are heavily reliant on the derivatives of bump functions, the first of which is pictured in
11.

, 𝜑⟩ = −⟨𝑇 , 𝜕𝜑
𝜕𝑥𝑖

such that ⟨ 𝜕𝑇
𝜕𝑥𝑖

We can show that this definition of differentiation extends usual differentiation in the following

sense:
Theorem A.9. For every differentiable function 𝑓 ,

𝜕𝑇𝑓
𝜕𝑥𝑖

= 𝑇 𝜕𝑓
𝜕𝑥𝑖

.

Proof. The proof follows by using integration by parts and the fact that the test functions have
□

compact support.

This definition allows us to understand how distribution theory can be used to interpret the

derivative of if-statements. Consider the following example:
Example A.10. Let 𝐻 (𝑥) = if 𝑥 < 0 then 0 else 1 be the Heaviside function. One can easily show
using the definition above that d𝑇𝐻
Example A.11. The function

d𝑥 = 𝛿0.

𝑓 (𝑥) = if 𝑥 < 0 then 0 else 𝑥
is the ReLU function frequently used as the activation function in neural networks. This function
is differentiable almost everywhere except at 𝑥 = 0. However, we can show that its distributional
derivative is the Heaviside function defined above. Additionally, because bump functions are
symmetric over their centered point, it is easy to see that the distributional derivative centered at 0
of the ReLU function is 1
2 regardless of the radius of the region tested.

It is also important to note that, when dealing with nice piecewise continuous functions, the
distributional derivative is somewhat "stable" with regards to the non-distributional derivative. For
example, in piecewise continuous functions 𝑓 : R → R, the symbolic distributional derivative of
the distribution 𝑓 is nearly identical to the symbolic piecewise derivative of the function 𝑓 save for
Dirac delta distributions being injected at the points of discontinuity scaled by the magnitude of
the discontinuity.
Theorem A.12 ([Golse 2010]). Let 𝑓 : R → R be a piecewise differentiable function with discontinu-
ities:

𝑎1 < 𝑎2 < · · · < 𝑎𝑛

The distributional derivative of 𝑓 is given by

𝑓 ′ = 𝑓 ′|R\{𝑎1,··· ,𝑎𝑛 } +

𝑛
∑︁

𝑘=1

(cid:32)

(cid:33)

lim
𝑥→𝑎+
𝑘

𝑓 (𝑎𝑘 ) − lim
𝑥→𝑎−
𝑘

𝑓 (𝑎𝑘 )𝑓 (𝑎𝑘 )

𝛿𝑎𝑘 ,

When dealing with functions with a higher-dimensional domain the situation is a bit subtler but,

under certain conditions, it can still be done analytically.

B TYPING RULES

, Vol. 1, No. 1, Article . Publication date: July 2022.

Distribution Theoretic Semantics for Non-Smooth Differentiable Programming

27

Variable
(𝑥, 𝜏) ∈ Γ
Γ ⊢ 𝑥 : 𝜏

Unpair
Γ ⊢ 𝑡1 : 𝜏1 × 𝜏2

Γ, 𝑥 : 𝜏1, 𝑦 : 𝜏2 ⊢ 𝑡2 : 𝜏3

Γ ⊢ let (𝑥, 𝑦) = 𝑡1 in 𝑡2 : 𝜏3

𝜆-Abstraction

Γ, 𝑥 : 𝜏1 ⊢ 𝑡 : 𝜏2
Γ ⊢ 𝜆𝑥 : 𝜏1.𝑡 : 𝜏1 −→ 𝜏2

Application
Γ ⊢ 𝑡1 : 𝜏1 −→ 𝜏2

Γ ⊢ 𝑡2 : 𝜏1

Γ ⊢ 𝑡1 𝑡2 : 𝜏2

Indicator Function
Γ ⊢ 𝑡1 : Pred(R𝑛)

Γ ⊢ 𝑡2 : R𝑛 → R

Γ ⊢ 1𝑡1 (𝑡2) : D ′(R𝑛)

Differentiation
Γ ⊢ 𝑡 : D ′(R𝑛)
𝜕𝑡
𝜕𝑥𝑖

Γ ⊢

: D ′(R𝑛)

Pair
Γ ⊢ 𝑡1 : 𝜏1

Γ ⊢ 𝑡2 : 𝜏2

Γ ⊢ (𝑡1, 𝑡2) : 𝜏1 × 𝜏2

Lift

Γ ⊢ 𝑡 : R𝑛 −→ R
Γ ⊢ lift(𝑡) : D ′(R𝑛)

𝑖 ∈ {1, ..., 𝑛}

Iteration
Γ ⊢ 𝑡0 : 𝜏

Γ ⊢ 𝑡 : 𝜏 → 𝜏

Γ ⊢ it 𝑡0 𝑡 : N → 𝜏

Scalar Multiplication
Γ ⊢ 𝛼 : R

Γ ⊢ 𝑡2 : D ′(R𝑛)

Γ ⊢ 𝛼𝑡2 : D ′(R𝑛)

Distribution Addition
Γ ⊢ 𝑡1 : D ′(R𝑛)

Γ ⊢ 𝑡2 : D ′(R𝑛)

Γ ⊢ 𝑡1 + .𝑡2 : D ′(R𝑛)

Distribution Application
Γ ⊢ 𝑡1 : D ′(R𝑛)

Γ ⊢ 𝑡2 : D (R𝑛)

Γ ⊢ ⟨𝑡1, 𝑡2⟩ : R

Bump Function
Γ ⊢ 𝑐 : R𝑛

Γ ⊢ 𝑟 : R+

𝑛 ∈ N

Γ ⊢ 𝜑𝑛 (𝑐, 𝑟 ) : D (R𝑛)

Dirac delta
Γ ⊢ 𝑡 : R𝑛
Γ ⊢ 𝛿𝑡 : D ′(R𝑛)

Fig. 12. Typing rules

, Vol. 1, No. 1, Article . Publication date: July 2022.

