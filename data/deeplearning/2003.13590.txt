0
2
0
2

r
p
A
1

]
I

A
.
s
c
[

2
v
0
9
5
3
1
.
3
0
0
2
:
v
i
X
r
a

Suphx: Mastering Mahjong with Deep
Reinforcement Learning∗

Junjie Li,1 Sotetsu Koyamada,2 Qiwei Ye,1
Guoqing Liu,3 Chao Wang,4 Ruihan Yang,5 Li Zhao,1
Tao Qin,1 Tie-Yan Liu,1 Hsiao-Wuen Hon1

1Microsoft Research Asia
2Kyoto University
3University of Science and Technology of China
4Tsinghua University
5Nankai University

Abstract

Artiﬁcial Intelligence (AI) has achieved great success in many domains,
and game AI is widely regarded as its beachhead since the dawn of AI. In
recent years, studies on game AI have gradually evolved from relatively
simple environments (e.g., perfect-information games such as Go, chess,
shogi or two-player imperfect-information games such as heads-up Texas
hold’em) to more complex ones (e.g., multi-player imperfect-information
games such as multi-player Texas hold’em and StartCraft II). Mahjong
is a popular multi-player imperfect-information game worldwide but very
challenging for AI research due to its complex playing/scoring rules and rich
hidden information. We design an AI for Mahjong, named Suphx, based
on deep reinforcement learning with some newly introduced techniques
including global reward prediction, oracle guiding, and run-time policy
adaptation. Suphx has demonstrated stronger performance than most top
human players in terms of stable rank and is rated above 99.99% of all the
oﬃcially ranked human players in the Tenhou platform. This is the ﬁrst
time that a computer program outperforms most top human players in
Mahjong.

1

Introduction

Building superhuman programs for games is a long-standing goal of artiﬁcial
intelligence (AI). Game AI has made great progress in past two decades (2, 3,

∗This work was conducted at Microsoft Research Asia. The 2nd, 4th, 5th, and 6th authors

were interns at Microsoft Research Asia then.

1

 
 
 
 
 
 
11, 13, 15, 16, 18). Recent studies have gradually evolved from relatively simple
perfect-information or two-player games (e.g., shogi, chess, Go, and heads-up
Texas hold’em) to more complicated imperfect-information multi-player ones
(e.g., contract bridge (12), Dota (1), StarCraft II (21), and multi-player Texas
hold’em (4)).

Mahjong, a multi-round tile-based game with imperfect information and
multiple players, is very popular with hundreds of millions of players worldwide.
In each round of a Mahjong game, four players compete with each other towards
the ﬁrst completion of a winning hand. Building a strong Mahjong program
raises great challenges to the current studies on game AI.

First, Mahjong has complicated scoring rules. Each game of Mahjong contains
multiple rounds, and the ﬁnal ranking (and so the reward) of a game is determined
by the accumulated round scores of those rounds. The loss of one round does
not always mean that a player plays poorly for that round (e.g., the player may
tactically lose the last round to ensure rank 1 of the game if he/she has a big
advantage in previous rounds), and so we cannot directly use the round score
as a feedback signal for learning. Furthermore, Mahjong has a huge number
of possible winning hands. Those winning hands can be very diﬀerent from
each other, and diﬀerent hands result in diﬀerent winning round scores of the
round. Such scoring rules are much more complex than previously studied games
including chess, Go, etc. A professional player needs to carefully choose what
kind of winning hand to form in order to trade oﬀ the winning probability and
winning score of the round.

Second, in Mahjong each player has up to 13 private tiles in his/her hand
which are not visible to other players, and there are 14 titles in the dead wall,
which are invisible to all the players throughout the game, and 70 tiles in the
live wall, which will become visible once they are drawn and discarded by the
players. As a result, on average, for each information set (a decision point of
a player), there are more than 1048 hidden states that are indistinguishable to
him/her. Such a large set of hidden information makes Mahjong a much more
diﬃcult imperfect-information game than previously studied ones such as Texas
hold’em poker. It is hard for a Mahjong player to determine which action is
good only based on his/her own private tiles, since the goodness of an action
highly depends on the private tiles of other players and the wall tiles that are
invisible to everyone. Consequently, it is also diﬃcult for an AI to connect the
reward signal to the observed information.

Third, the playing rule of Mahjong is complex: (1) there are diﬀerent types
of actions including Riichi, Chow, Pong, Kong, discard, and (2) the regular order
of plays can be interrupted when making a meld (Pong or Kong), going Mahjong
(declaring a winning hand), or robbing a Kong. Because each player can have
up to 13 private tiles, it is hard to predict those interruptions, and therefore
we even cannot build a regular game tree; even if we build a game tree, such
a tree will have a huge number of paths between the consecutive actions of a
player. This prevents the direct application of previously successful techniques
for games such as Monte-Carlo tree search (14, 15) and counterfactual regret
minimization (3, 4).

2

Due to the above challenges, although there are several attempts (7–9, 20),

the best Mahjong AI is still far behind top human players.

In this work, we build Suphx (short for Super Phoenix), an AI system
for 4-player Japanese Mahjong (Riichi Mahjong), which has one of the largest
Mahjong communities in the world. Suphx adopts deep convolutional neural
networks as its models. The networks are ﬁrst trained through supervised
learning from the logs of human professional players and then boosted through
self-play reinforcement learning (RL), with the networks as the policy. We use
the popular policy gradient algorithm (17) for self-play RL and introduce several
techniques to address the aforementioned challenges.

1. Global reward prediction trains a predictor to predict the ﬁnal reward
(after several future rounds) of a game based on the information of the
current and previous rounds. This predictor provides eﬀective learning
signals so that the training of the policy network can be performed. In
addition, we design look-ahead features to encode the rich possibilities of
diﬀerent winning hands and their winning scores of the round, as a support
to the decision making of our RL agent.

2. Oracle guiding introduces an oracle agent that can see the perfect infor-
mation including the private tiles of other players and the wall tiles. This
oracle agent is a super strong Mahjong AI due to the (unfair) perfect
information access. In our RL training process, we gradually drop the
perfect information from the oracle agent, and ﬁnally convert it to a normal
agent which only takes observable information as input. With the help of
the oracle agent, our normal agent improves much faster than standard
RL training which only leverages observable information.

3. As the complex playing rules of Mahjong lead to an irregular game tree
and prevent the application of Monte-Carlo tree search techniques, we
introduce parametric Monte-Carlo policy adaptation (pMCPA) to improve
the run-time performance of our agent. pMCPA gradually modiﬁes and
adapts the oﬄine-trained policy to a speciﬁc round in the online playing
stage when the round goes on and there is more information observable
(such as public tiles discarded by four players).

We evaluated Suphx on the most popular and competitive Mahjong platform,
Tenhou (19), which has more than 350,000 active users. Suphx reached 10 dan
at Tenhou, and its stable rank, which describes the long-term average of the
performance of a player, surpasses most top human players.

2 Overview of Suphx

In this section, we ﬁrst describe the decision ﬂow of Suphx, and then the network
structures and features used in Suphx.

3

Model

Functionality

Discard model Decide which tile to discard in normal situations
Riichi model
Chow model
Pong model
Kong model

Decide whether to declare Riichi
Decide whether/what to make a Chow
Decide whether to make a Pong
Decide whether to make a Kong

Table 1: Five models in Suphx

2.1 Decision Flow

Due to the complex playing rules of Mahjong, Suphx learns ﬁve models to handle
diﬀerent situations: the discard model, the Riichi model, the Chow model, the
Pong model, and the Kong model, as summarized in Table 1.

Besides these ﬁve learned models, Suphx employs another rule-based winning
model to decide whether to declare a winning hand and win the round.
It
basically checks whether a winning hand can be formed from a tile discarded by
other players or drawn from the wall, and then makes decisions according to the
following simple rules:

• If this is not the last round of the game, declare and win the round;

• If this is the last round of the game,

– If after declaring a winning hand the accumulated round score of the
whole game is the lowest among the four players, do not declare;

– Otherwise, declare and win the round.

There are two kinds of situations that a Mahjong player needs to take actions,

so does our AI Suphx (see Figure 1):

• The Draw situation: Suphx draws a tile from the wall.

If its private
tiles can form a winning hand with the drawn tile, the winning model
decides whether to declare winning. If yes, it declares and the round is
over. Otherwise,

1. Kong step: If the private tiles can make an ClosedKong or AddKong
with the drawn tile, the Kong model decides whether to make the
ClosedKong or AddKong. If not, go to the Riichi step; otherwise,
there are two sub cases:

(a) If it is an ClosedKong, make the ClosedKong and go back to the

Draw situation.

(b) If it is an AddKong, other players may win the round using
this AddKong tile. If some other player wins, the round is over;
otherwise, make the AddKong and go back to the Draw situation.

4

2. Riichi step: If the private tiles can make Riichi with the drawn tile,
the Riichi model decides whether to declare Riichi. If not, go to the
Discard step; otherwise, declare Riichi and then go to the Discard
step.

3. Discard step: The discard model chooses a tile to discard. After that,
it is the other players’ turn to take actions or the round ends with no
wall tiles left.1

• The Other-Discard situation: Other players discard a tile. If Suphx can
form a winning hand with this tile, the winning model decides whether to
declare winning. If yes, it declares and the round is over. Otherwise, it
checks whether a Chow, Pong, or Kong can be made with the discarded
tile. If not, it is the other players’ turn to take actions; otherwise, the
Chow, Pong, or Kong model decides what action to take:

1. If no action is suggested by the three models, it is the turn for other
players to take actions or the round ends with no wall tiles left.

2. If one or more actions are suggested, Suphx proposes the action
with the highest conﬁdence score (outputted by those models). If
the proposed action is not interrupted by higher-priority actions
from other players, Suphx takes the action, and then goes to the
Discard step in the ﬁrst situation. Otherwise, the proposed action is
interrupted and it becomes the turn for other players to take actions.

Figure 1: Decision ﬂow of Suphx.

1If a round ends with no wall tiles left, this is also known as an exhaustive draw.

5

​Draw a tile​Can Win​Yes​Winning Model: Win or not?​YesRound overCan ClosedKong or AddKong​Yes​Kong Model:Kong or not?​No​Can RiichiRiichi Model:Riichi or not?​YesDeclare a Riichi State​YesDiscard Model:​Discard a tileOther players take actions, or round ends with no wall tiles leftMake Kong​Other playerdiscards a tile​Can Win​Winning Model: Win or not?​Yes​Round over​Yes​Chow Model:Whether/What to Chow?Pong Model:Whether to Pong?Kong Model:Whether to Kong?​#actions suggested>=1Proposed action interrupted by other players?Other players take actions​Can Chow​Can Pong​Can Kong​Yes​YesYes​Win by other players(AddKong)?​No​​No​No​Yes​Yes0​Yes​NoYesTake the proposed actionNoNoThe action is Kong?YesPropose the action with the largest confidence score​2.2 Features and Model Structures

Since deep convolutional neural networks (CNNs) have demonstrated powerful
representation capability and been veriﬁed in playing games like chess, shogi
and Go, Suphx also adopts deep CNNs as the model architecture for its policy.
Diﬀerent from board games like Go and Chess, the information (as shown
in Figure 2) available to players in Mahjong is not naturally in the format of
images. We carefully design a set of features to encode the observed information
into channels that can be digested by CNNs.

Figure 2: Example of a state. Mahjong’s state contains several types of infor-
mation: (1) the tile set including private tiles, open hand, and doras, (2) the
sequence of discarded tiles, (3) integer features including accumulated round
scores of the four players and the number of tiles left in the live wall, and (4)
categorical features including round id, dealer, counters of repeat dealer, and
Riichi bets.

As there are 34 unique tiles in Japanese Mahong, we use multiple 34 × 1
channels to represent a state. As shown in Fig. 3, we use four channels to encode
the private tiles of a player. Open hand, doras, and the sequence of discarded
tiles are encoded similarly into other channels. Categorical features are encoded
into multiple channels, with each channel being either all 0’s or all 1’s. Integer
features are partitioned into buckets and each bucket is encoded using a channel
of either all 0’s or all 1’s.

In addition to the directly observable information, we also design some look-

6

Figure 3: Encoding of private tiles. We encode the private tiles of a player into
four channels. There are four rows and 34 columns, with each row corresponding
to one channel, and each column indicating one type of tile. The m-th column in
the n-th channel means whether there are n tiles of the m-th type in the hand.

ahead features, which indicate the probability and round score of winning a
hand if we discard a speciﬁc tile from the current hand tiles and then draw tiles
from the wall to replace some other hand tiles. In Japanese Mahjong, a winning
hand of 14 tiles contains four melds and one pair. There are 89 kinds of melds
and 34 kinds of pairs, which lead to a huge number of diﬀerent possible winning
hands. Furthermore, diﬀerent hands result in diﬀerent winning scores of the
round according to the complex scoring rules.2 It is impossible to enumerate all
the combinations of diﬀerent discarding/drawing behaviors and winning hands.
Therefore, to reduce computational complexity, we make several simpliﬁcations
while extracting look-ahead features: (1) We perform depth ﬁrst search to ﬁnd
possible winning hands. (2) We ignore opponents’ behaviors and only consider
drawing and discarding behaviors of our own agent. With those simpliﬁcations,
we obtain 100+ look-ahead features, with each feature corresponding to a 34-
dimensional vector. For example, a feature represents whether discarding a
speciﬁc tile can lead to a winning hand of 12,000 round score with replacing 3
hand tiles by tiles drawn from the wall or discarded by other players.

In Suphx, all the models (i.e., the discard/Riichi/Chow/Pong/Kong models)
use similar network structures (Figures 4 and 5), except the dimensions of the
input and output layers (Table 2). The discard model has 34 output neurons
corresponding to 34 unique tiles, the Richii/Chow/Pong/Kong models have
only two output neurons corresponding to whether or not to take a certain
action. In addition to the state information and look-ahead features, the inputs
of Chow/Pong/Kong models also contain information about what tiles to make
a Chow/Pong/Kong. Note that there is no pooling layer in our models, because
every column of a channel has its semantic meaning and pooling will lead to
information loss.

2A common list of diﬀerent scoring patterns could be found at http://arcturus.su/wiki/

List_of_yaku

7

0000100000011100001110000001000001000010000000000000000000000100000100001000000000000000000000000000010000000000000000000000000000000000Represented with 4 channelsPrivate Hand TilesencodeDiscard Riichi Chow Pong Kong

Input
Output

34 × 838

34

34 × 958
2

Table 2: Input/out dimensions of diﬀerent models

Figure 4: Structure of the discard model

Figure 5: Structures of the Riichi, Chow, Pong, and Kong models

3 Learning Algorithm

The learning of Suphx contains three major steps. First, we train the ﬁve models
of Suphx by supervised learning, using (state, action) pairs of top human players
collected from the Tenhou platform. Second, we improve the supervised models
through self-play reinforcement learning (RL), with the models as policy. We
adopt the popular policy gradient algorithm (Section 3.1) and introduce global
reward prediction (Section 3.2) and oracle guiding (Section 3.3) to handle the
unique challenges of Mahjong. Third, during online playing, we employ run-time
policy adaptation (Section 3.4) to leverage the new observations on the current
round in order to perform even better.

3.1 Distributed Reinforcement Learning with Entropy Reg-

ularization

The training of Suphx is based on distributed reinforcement learning. In partic-
ular, we employ the policy gradient method and leverage importance sampling
to handle the staleness of trajectories due to asynchronous distributed training:

L (θ) = E

s,a∼πθ(cid:48)

(cid:20) πθ(a|s)
πθ(cid:48)(a|s)

(cid:21)
Aπθ (s, a)

,

(1)

where θ(cid:48) is (the parameters of) an old policy generating trajectories for training,
θ is the latest policy to update, and Aπθ (s, a) is the advantage of action a at

8

CNN Architecture3x1 conv256inputDx34x13x1 conv2563x1 conv256Repeat 50x1x1 conv1output+3x1 conv256inputDx34x13x1 conv2563x1 conv256Repeat 50x3x1 conv32output+2 layers of fc (1024, 256)CNN Architecture3x1 conv256inputDx34x13x1 conv2563x1 conv256Repeat 50x1x1 conv1output+3x1 conv256inputDx34x13x1 conv2563x1 conv256Repeat 50x3x1 conv32output+2 layers of fc (1024, 256)Figure 6: Distributed RL system in Suphx

state s with respect to policy πθ.

We ﬁnd that RL training is sensitive to the entropy of the policy. If the
entropy is too small, RL training converges quickly and self-play does not
signiﬁcantly improve the policy; if the entropy is too large, RL training becomes
unstable and the learned policy is of large variance. Therefore, we regularize the
entropy of policy during the RL training as follows:

∇θJ (πθ) = E

s,a∼πθ(cid:48)

(cid:20) πθ(s, a)
πθ(cid:48)(s, a)

(cid:21)
∇θ log πθ (a|s) Aπθ (s, a)

+ α∇θH(πθ)

(2)

where H(πθ) is the entropy of policy πθ and α > 0 is a trade-oﬀ coeﬃcient.
To ensure stable exploration, we dynamically adjust α to increase/decrease the
entropy term if the entropy of our policy is smaller/larger than the target Htarget
in a recent period:

α ← α + β(cid:0)Htarget − ¯H(πθ)(cid:1),

(3)

where ¯H(πθ) is the empirical entropy of the trajectories in a recent period, and
β > 0 is a small step size.

The distributed RL system used by Suphx is shown in Figure 6. The system
consists of multiple self-play workers, each containing a set of CPU-based Mahjong
simulators and a set of GPU-based inference engines to generate trajectories.
The update of the policy πθ is decoupled from the generation of the trajectories:
a parameter server is used to update the policy using multiple GPUs based on a
replay buﬀer. During training, every Mahjong simulator randomly initializes a
game with our RL agent as a player and other three opponents. When any of
the four players needs to take an action, the simulator sends the current state
(represented by a feature vector) to a GPU inference engine, which then returns
an action to the simulator. GPU inference engines pull the up-to-date policy πθ
from the parameter server in a regular basis, to ensure that the self-play policy
is close enough to the latest policy πθ.

9

Experience replay bufferLatest policyUpdateFetch new policy parametersevery ~1000 mini-batchesParameter serverInference enginesSelf-play workerssaMahjong simulatorsMahjong simulatorsMahjong simulatorsMahjong simulatorsMahjong simulatorsInference enginesInference engines3.2 Global Reward Prediction

In Mahjong, each game contains multiple rounds, e.g., 8-12 rounds in Tenhou.3
A round starts with 13 private tiles dealt to each player, in turn players draw
and discard tiles, the round ends until one of the players completes a winning
hand or no tile is left in the wall, and then each player gets a round score. For
example, the player who forms a winning hand gets a positive round score, and
the others get zero or negative round scores. When all the rounds end, each
player gets a game reward based on the rank of the accumulated round scores.
Players receive round scores at the end of each round, and receive game
rewards after 8-12 rounds. However, neither round scores nor game rewards are
good signals for RL training:

• Since multiple rounds in the same game share the same game reward,
using game rewards as a feedback signal cannot diﬀerentiate well-played
rounds and poorly-played rounds. Therefore, one should better measure
the performance of each round separately.

• While the round score is computed for each individual round, it may not
be able to reﬂect the goodness of the actions, especially for top professional
players. For example, in the last one or two rounds of a game, the rank-1
player with a big lead in terms of accumulated round scores will usually
become more conservative, and may purposely let rank-3 or rank-4 players
win this round so that he/she can safely keep rank 1 overall. That is,
a negative round score may not necessarily mean a poor policy: it may
sometimes reﬂect certain tactics and thus correspond to a fairly good
policy.

Therefore, to provide eﬀective signal for RL training, we need to appropriately
attribute the ﬁnal game reward (a global reward) to each round of the game.
For this purpose, we introduce a global reward predictor Φ, which predicts the
ﬁnal game reward given the information of the current round and all previous
rounds of this game. In Suphx, the reward predictor Φ is a recurrent neural
network, more speciﬁcally, a two-layer gated recurrent unit (GRU) followed by
two fully-connected layers, as shown in Figure 7.

The training data for this reward predictor Φ come from the logs of top
human players in Tenhou, and Φ is trained by minimizing the following mean
square error:

min

1
N

N
(cid:88)

i=1

1
Ki

Ki(cid:88)

(Φ(x1

i , · · · , xj

i ) − Ri)2,

j=1

(4)

where N denotes the number of games in the training data, Ri denotes the ﬁnal
game reward of the i-th game, Ki denotes the number of rounds in the i-th game,
xk
i denotes the feature vector of the k-th round in the i-th game, including the

3The number of rounds of a game is not ﬁxed, depending on the win/loss of each round.

More details can be found at https://tenhou.net/man/.

10

Figure 7: Reward predictor: GRU network

score of this round, the current accumulated round score, the dealer position,
the counters of repeat dealer and Riichi bets.

When Φ is well trained, for a self-play game with K rounds, we use Φ(xk) −

Φ(xk−1) as the reward of the k-th round for RL training.

3.3 Oracle Guiding

There is rich hidden information in Mahjong (e.g., the private tiles of other
players and the wall tiles). Without access to such hidden information, it is
hard to take good actions. This is a fundamental reason why Mahjong is a
diﬃcult game. In this situation, although an agent can learn a policy by means
of reinforcement learning, the learning could be very slow. To speed up the RL
training, we introduce an oracle agent, which can see all the perfect information
about a state: (1) private tiles of the player, (2) open (previously discarded) tiles
of all the players, (3) other public information such as the accumulated round
scores and Riichi bets, (4) private tiles of the other three players, and (5) the
tiles in the wall. Only (1)(2) and (3) are available to the normal agent,4 while (4)
and (5) are additional “perfect" information that is only available to the oracle.
With the (unfair) access to the perfect information, the oracle agent will
easily become a master of Mahjong after RL training. Here the challenge is how
to leverage the oracle agent to guide and accelerate the training of our normal
agent. According to our study, simple knowledge distillation does not work well:
it is diﬃcult for a normal agent, who only has limited information access, to
mimic the behavior of a well-trained oracle agent, who is super strong and far
beyond the capacity of a normal agent. Therefore, we need a smarter way to
guide our normal agent with the oracle.

4“Normal” here means that the agent has no access to the perfect information.

11

Round 1 infoRound 2 infoRound k-1 infoRound k infoGRUGRUGRUGRUGRUGRUGRUGRU……2 layers of fully-connected layerPredicted RewardRNN network for reward predictorTo this end, there might be diﬀerent approaches. In Suphx, what we do is to
ﬁrst train the oracle agent through reinforcement learning, using all the features
including the prefect ones. Then we gradually drop out the perfect features so
that the oracle agent will eventually transit to a normal agent:

L (θ) = E

s,a∼πθ(cid:48)

(cid:20) πθ(a|[xn(s), δtxo(s)])
πθ(cid:48)(a|[xn(s), δtxo(s)])

(cid:21)
Aπθ ([xn(s), δtxo(s)], a)

,

(5)

where xn(s) denote the normal features and xo(s) the additional perfect features
of state s, and δt is the dropout matrix at the t-th iteration whose elements are
Bernoulli variables with P (δt(i, j) = 1) = γt. We gradually decay γt from 1 to 0.
When γt = 0, all the prefect features are dropped out and the model transits
from the oracle agent to a normal agent.

After γt becomes zero, we continue the training of the normal agent for a
certain number of iterations. We adopt two tricks during the continual training.
First, we decay the learning rate to one tenth. Second, we reject some state-action
pairs if the importance weight is larger than a pre-deﬁned threshold. According
to our experiments, without these tricks, the continual training is not stable and
does not lead to further improvements.

3.4 Parametric Monte-Carlo Policy Adaptation

The strategy of a top human player will be very diﬀerent when his/her initial
hand (private tiles) varies. For example, he/she will play aggressively to win
more given a good initial hand, and conservatively to lose less given a poor initial
hand. This is very diﬀerent from previous games including Go and StarCraft.
Therefore, we believe that we could build a stronger Mahjong agent if we are
able to adapt the oﬄine-trained policy in the run time.

Monte-Carlo tree search (MCTS) is a well established technique in games
like Go (14) for run-time performance improvement. Unfortunately, as afore-
mentioned, the playing order of Mahjong is not ﬁxed and it is hard to build a
regular game tree. Therefore, MCTS cannot be directly applied to Mahjong.
In this work, we design a new method, named parametric Monte-Carlo policy
adaptation (pMCPA).

When a round begins and the initial private hand is dealt to our agent, we

adapt the oﬄine-trained policy to this given initial hand as follows:

1. Simulations: Randomly sample private tiles for the three opponents and
wall tiles from the pool of tiles excluding our own private tiles, and then
use the oﬄine-trained policy to roll out and ﬁnish the whole trajectory. In
total, K trajectories are generated in this way.

2. Adaptation: Perform gradient updates using the rollout trajectories to

ﬁnetune the oﬄine policy.

3. Inference: Use the ﬁnetuned policy to play against other players in this

round.

12

Let h denote the private hand tiles of our agent at a round, θo denote the
parameters of the policy trained oﬀ-line, and θa the parameters of the new policy
adapted to this round. Then we have

θa = arg max

θ

(cid:88)

τ ∼θo T (h)

R(τ )

p(τ ; θ)
p(τ ; θo)

,

(6)

where T (h) is the set of trajectories with preﬁx h, and p(τ ; θ) is the probability
of policy θ generating trajectory τ .

According to our study, the number K of simulations/trajectories does not
need to be very large and pMCPA does not need to collect statistics for all
the following states for this round. Since pMCPA is a parametric method, the
updated policy (using the K simulations) can lead to updated estimation of those
states not visited in the simulations as well. That is, such run-time adaptation
can help to generalize our knowledge obtained from limited simulations to unseen
states.

Please note that the policy adaptation is performed for each round indepen-
dently. That is, after we adapt the policy of our agent in the current round, for
the next round, we will restart from the oﬄine-trained policy once again.

4 Oﬄine Evaluation

In this section, we report the eﬀectiveness of each technical component of Suphx
through oﬄine experiments.

4.1 Supervised Learning

In Suphx, the ﬁve models were ﬁrst trained through supervised learning separately.
Each training sample is a state-action pair collected from human professional
players with state serving as the input and action serving as the label for
supervised learning. For example, for the training of the discard model, the
input of a sample is all the observable information (and the look-head features)
of a state, and the label is the action taken by a human player, i.e., the tile
discarded at this state.

The training data sizes and the test accuracy are reported in Table 3. The
sizes of the validation data and test data are 10K and 50K respectively, for all
the models. Since the discard model addresses a 34-class classiﬁcation problem,
we collected more training samples for it. As can be seen from the table, we
achieve an accuracy of 76.7% for the discard model, 85.7% for the Riichi model,
95.0% for the Chow model, 91.9% for the Pong model, and 94.0% for the Kong
model. We also list the accuracy achieved by previous works (6) as a reference.5

5We would like to point out that our numbers are not directly comparable to previous

numbers due to diﬀerent training/test data and model structures.

13

Model
Discard model
Riichi model
Chow model
Pong model
Kong model

Training Data Size Test Accuracy Previous Accuracy (6)

15M
5M
10M
10M
4M

76.7%
85.7 %
95.0%
91.9 %
94.0 %

68.8 %
-
90.4%
88.2%
-

Table 3: Results for supervised learning

4.2 Reinforcement Learning

To demonstrate the value of each RL component in Suphx, we trained several
Mahjong agents:

• SL: the supervised learning agent. This agent (with all the ﬁve models)
was trained in a supervised way, as illustrated in the previous subsection.

• SL-weak: an under-trained version of the SL agent, which serves as oppo-

nent models while evaluating other agents.

• RL-basic: the basic version of the reinforcement learning agent. In RL-
basic, the discard model was initialized with the SL discard model and
then boosted through the policy gradient method with round scores as
reward and entropy regularization. The Riichi, Chow, Pong, and Kong
models remain the same as those of the SL agent.6

• RL-1: the RL agent that enhances RL-basic with global reward prediction.
The reward predictor was trained through supervised learning with human
game logs from Tenhou.

• RL-2: the RL agent that further enhances RL-1 with oracle guiding. Please
note that in both RL-1 and RL-2, we also only trained the discard model
using RL, and left the other four models the same as those of the SL agent.

The initial private tiles have large randomness and will greatly impact the
win/loss of a game. To reduce the variance caused by initial private tiles, during
the oﬄine evaluation, we randomly generated one million games. Each agent
plays against 3 SL-weak agents on these games. In such a setting, the evaluation
of one agent took 20 Tesla K80 GPUs for two days. For the evaluation metric,
we computed the stable rank of an agent following the rules of Tenhou (see
Appendix C). To reduce the variance of stable rank, for each agent, we randomly
sampled 800K games from the one million games, for 1000 times.

Figure 8 shows the interquartile ranges7 of stable ranks over the 1000 sam-
plings for those agents. Note that for fair comparison, each RL agent was trained

6These four models can also be improved through reinforcement learning, although not as
signiﬁcant as the discard model. Therefore, to reduce training time, we inherit the SL models.
7According to Wikipedia, "In descriptive statistics, the interquartile range (IQR) is a
measure of statistical dispersion, being equal to the diﬀerence between 75-th and 25-th

14

using 1.5 million games. The training of each agent costs 44 GPUs (4 Titan XP
for the parameter server and 40 Tesla K80 for self-play workers) and two days.
As can be seen, RL-basic leads to good improvement over SL, RL-1 outperforms
RL-basic, and RL-2 brings additional gains against RL-1. These experimental
results clearly demonstrate the value of reinforcement learning, as well as the
additional values of global reward prediction and oracle guiding.

Figure 8: Statistics of stable rank over one million games. The plot shows the
three quartile values of a distribution along with extreme values. The “whiskers”
extend to points that lie within 1.5 IQRs of the lower and upper quartile, and
then observations that fall outside this range are displayed independently.

With the global reward predictor distributing the game reward to each round,
the trained agent can better maximize the ﬁnal game reward instead of the round
score. For example, in Figure 9, our agent (the south player) has a big lead with
good hand in the last round of the game. Based on the current accumulated
round scores of the four players, winning this round gets only marginal reward
while losing this round will lead to a big punishment. Therefore instead of
playing aggressively to win this round, our agent plays conservatively, chooses
the safest tile to discard, and ﬁnally gets the ﬁrst place/rank for this game. In
contrast, RL-basic discards another tile to win the round, which brings a big
risk of losing the 1-st rank of the entire game.

percentiles, or between upper and lower quartiles, IQR = Q3 - Q1. In other words, the IQR is
the ﬁrst quartile subtracted from the third quartile; these quartiles can be clearly seen on a
box plot on the data. It is a trimmed estimator, deﬁned as the 25% trimmed range, and is a
commonly used robust measure of scale."

15

Figure 9: With global reward prediction, our agent (the south player) plays
conservatively when its accumulated round score has a big lead in the last round
of a game, even if its hand tiles are good and it has a certain probability to win
this round. The RL-basic agent discards the red-boxed tile to win this round,
but discarding this tile is risky since the same tile has not been discarded by
any player in this round. In contrast, RL-1 and RL-2 agents play in a defense
mode and discard the blue-boxed tile, which is a safe tile because the same tile
has just been discarded by the west player.

4.3 Evaluation of Run-Time Policy Adaptation

In addition to testing the enhancement to oﬄine RL training, we also tested the
run-time policy adaptation. The experimental setting is described as follows.

When a round begins and the private tiles are dealt to our agent,

1. Data generation: We ﬁx the hand tiles of our agent and simulate 100K
trajectories. In each trajectory, the hand tiles of the other three players
and wall tiles are randomly generated, and we use four copies of our agent
to roll out and ﬁnish the trajectory.

2. Policy adaptation: We ﬁne-tune and update the policy trained oﬄine over

16

those 100K trajectories by using the basic policy gradient method.

3. Test of the adapted policy: Our agent plays against the other three players
using the updated policy on another 10K test set where the private tiles of
our agent are still ﬁxed. As the initial private tiles of our agent is ﬁxed,
the performance of the adapted agent on this test set can tell whether such
run-time policy adaptation really makes our agent adapt and work better
for the current private tiles.

Please note that run-time policy adaptation is time consuming due to the
roll-outs and online learning. Therefore, at the current stage, we only tested
this technique on hundreds of initial rounds. The wining rate of the adapted
version of RL-2 against its non-adapted version is 66%, which demonstrates the
advantage of run-time policy adaptation.

Policy adaption makes our agent work better for the current private hand,
especially at the last 1 or 2 rounds of a game. Figure 10 shows an example of the
last round of a game. Through simulations the agent learns to know that while
it is easy to win this round with a nice round score, this is unfortunately not
enough to avoid ending the game with the 4-th place. Thus, after adaptation,
the agent plays more aggressively, takes more risks, and eventually wins the
round with a much larger round score and successfully avoid ending the game
with the 4-th place.

5 Online Evaluation

To evaluate the real performance of our Mahjong AI Suphx8, we let it play on
Tenhou.net, the most popular online platform for Japanese Mahjong. Tenhou
has two major rooms, the expert room and the phoenix room. The expert room
is open to AI and human players of 4 dan and above, while the phoenix room is
only open to human players of 7+ dan. According to this policy, Suphx can only
play in the expert room.

Suphx played 5000+ games in the expert room and achieved 10 dan in terms
of record rank9 and 8.74 dan in terms of stable rank.10 It is the ﬁrst and only
AI in Tenhou that achieves 10 dan in terms of record rank.

We compare Suphx with several AI/human players in Table 4:

• Bakuuchi (10): This is a Mahjong AI designed by the University of Tokyo
based on Monte Carlo simulation and opponent modeling. It does not use
reinforcement learning.

8Suphx is equivalent to RL-2 trained with about 2.5 million games. Given that Tenhou.net
has time constraint for each action, run-time policy adaptation was not integrated into Suphx
while testing on Tenhou.net since it is time consuming. We believe the integration of run-time
policy adaptation will further improve Suphx.

9Record rank is the highest rank a player has ever achieved in Tenhou. As shown in
Appendix B, the rank of a player is dynamic and usually changes over time. For example, if
he/she does not play well recently, his/her rank will drop.

10See Appendix C for the deﬁnition of stable rank.

17

Figure 10: In this example, to move out of the 4-th place of the game, the agent
needs to win more than 12,000 round score in this round. Through simulations,
the agent learns to know that discarding the red-boxed tile is easy to win this
round; however, the corresponding winning round score will be less than 12,000.
After adaptation, the agent discards the blue-boxed tile, which leads to lower
probability of winning but more than 12,000 winning round score once it wins.
By doing so, it takes risk and successfully move out of the 4-th place.

• NAGA11: This is a Mahjong AI designed by Dwango Media Village based
It does not use reinforcement

on deep convolutional neural networks.
learning either.

• We also compare Suphx with top human players who have achieved 10 dan
in terms of record rank. To be fair, we only compare their game playing
in the expert room after they achieved 10 dan. Since these top human
players spent the majority of their time in the phoenix room (partly due to
its more friendly scoring rules) and only played occasionally in the expert
room after they achieved 10 dan, we can hardly calculate a reliable stable

11https://dmv.nico/ja/articles/mahjong_ai_naga/

18

rank for each individual of them.12 Therefore, we treat them as one macro
player to make a statistically reasonable comparison.

We can see that in terms of stable rank, Suphx is about 2 dan better than
Bakuuchi and NAGA, the two best Mahjong AIs before Suphx. Although these
top human players have achieved the same record rank (10 dan) as Suphx,
they are not as strong as Suphx in terms of stable rank. Figure 11 plots the
distributions of record ranks13 of current active users in Tenhou, which shows
that Suphx is above 99.99% human players in Tenhou.

Bakuuchi
NAGA
Top human
Suphx

#Game
30,516
9,649
8,031
5,760

record rank
9 dan
8 dan
10 dan
10 dan

Stable Rank
6.59
6.64
7.46
8.74

Table 4: Comparison with other AIs and top human players.

Figure 11: Distributions of record ranks of human players in Tenhou. Each bar
indicates the number of human players above a certain level in Tenhou.

As discussed in Appendix B, the record rank sometimes cannot reﬂect the
true level of a player: for example, there are 100+ players with a record rank
of 10 dan in Tenhou’s history , but their true levels can be very diﬀerent. The

12We chose to compare with the performance of top human players according to their game
playing in the expert room but not the phoenix room because these two rooms have diﬀerent
scoring rules and the stable ranks are not directly comparable.

13Phoenix is a honorable title in Tenhou, when a 10-dan player gets 4000 ranking points.
There are only 13 players (and 14 accounts) in Tenhou’s history won this honorable title for
4-player Mahjong.

19

stable rank is more stable (by its deﬁnition) and of ﬁner granularity than the
record rank; however, it could also be of large variance, especially when a player
has not played enough number of games in Tenhou. Thus, in order to make more
informative and reliable comparisons, we proceed as follows. For each AI/human
player, we randomly sample K games from its/his/her logs in the expert room
and compute the stable rank using those K games. We do such sampling for N
times and show the statistics of the corresponding N stable ranks of each player
in Figure 12. As can be seen, Suphx surpasses both the other two AIs and the
average performance of top human professional players with a big margin.

Figure 12: Statistics of stable ranks with K = 2000 and N = 5000.

We further show more statistics of those AI/human players in Table 5. We

have several interesting observations from the table:

• Suphx is very strong at defense and has very low deal-in rate. This is

conﬁrmed by the comments from top human players on Suphx.14

• Suphx has very low 4-th rank rate, which is the key to get a high stable

rank in Tenhou according to its scoring rules.

Suphx has developed its own playing styles, which are well recognized by top
human players. For example, Suphx is very good at keeping safe tiles, prefers

14A quote from a pheonix human player can be found at https://twitter.com/Futokunaio_

Sota/status/1142399895577325568

20

       6 W D E O H  5 D Q N 6 X S K [ 7 R S  + X P D Q 1 $ * $ % D N X X F K LBakuuchi

1st
Rank
28.0%
NAGA 25.6%
28.0%
29.3%

Top human
Suphx

2nd
Rank
26.2%
27.2%
26.8%
27.5%

3rd
Rank
23.2%
25.9%
24.7%
24.4%

Deal-in
Rate

4th
Rank
22.4%
21.1%
20.5%
18.7% 22.83% 10.06%

Win
Rate
23.07% 12.16%
22.69% 11.42%
-

-

Table 5: More statistics: rank distribution and win/deal-in rate

winning hand with half-ﬂush15, etc. Figure 13 is an example that Suphx keeps a
safe tile to balance future attack and defense16.

6 Conclusion and Discussions

Suphx is the strongest Mahjong AI system up to date, and is also the ﬁrst
Mahjong AI that surpassed most top human players in Tenhou.net, a famous
online platform for Japanese Mahjong. Because of the complexity and unique
challenges of Mahjong, we believe that even though Suphx has performed very
well, there is still a lot of space for further improvement.

• We introduced global reward prediction in Suphx. In the current system,
the reward predictor takes limited information as its input. Clearly, more
information will lead to better reward signal. For example, if a round is very
easy to win due to our good luck of initial hand tiles, winning this round
does not reﬂect the superiority of our policy and should not be rewarded
too much; in contrast, winning a diﬃcult round should be rewarded more.
That is, game diﬃculty should be taken into consideration while designing
reward signals. We are investing how to leverage prefect information (e.g.,
by comparing the private initial hands of diﬀerent players) to measure the
diﬃculty of a round/game and then boost the reward predictor.

• We introduced the concept of oracle guiding, and instantiated this concept
using the gradual transition from an oracle agent to a normal agent by
means of perfect feature dropout.
In addition to this, there could be
other approaches to leverage the perfect information. For one example,
we can simultaneously train an oracle agent and a normal agent, let the
oracle agent distill its knowledge to the normal agent while constraining
the distance between these two agents. According to our preliminary
experiments, this approach also works quite well. For another example,
we can consider designing an oracle critic, which provides more eﬀective

15https://en.wikipedia.org/wiki/Japanese_Mahjong_yaku
16Game

replay

found

can

be

at

https://tenhou.net/3/?log=

2019070722gm-0029-0000-3bee4a7e&tw=3.

21

Figure 13: Suphx keeps a safe tile at state st to balance future attack and
defense. Although it is safe to discard the red-boxed tile at state st (actually
this is indeed the tile that most human players would discard), Suphx keeps this
tile in hand; instead, it discards the blue-boxed tile, which might slow down the
process of forming a winning hand. Doing so leads to more ﬂexibility in future
states and can better balance future attack and defense. Consider a future state
st+k in which another player declares Riichi that is unexpected to our agent. In
this case, Suphx can discard the safe tile kept at state st and does not break
the winning hand it tries to form. In contrast, if Suphx discards this safe tile at
state st, it has no other safe tiles to discard at st+k, and thus may have to break
a meld or pair in its hand tiles that are close to a winning hand, consequently
resulting in smaller winning probability.

state-level instant feedback (instead of round-level feedback) to accelerate
the training of the policy function based on the perfect information.

• For run-time policy adaptation, in the current system of Suphx, we did

22

simulations at the beginning of each round when private tiles were dealt to
our agent. Actually we can also do simulations after each tile is discarded
by any player. That is, instead of only adapting the policy to the initial
hands, we can continue the adaptation as the game goes on and more and
more information becomes observable. Doing so should be able to further
improve the performance of our policy. Moreover, since we gradually adapt
our policy, we do not need too many samplings and roll-outs at each step.
In other words, we can amortize the computational complexity of policy
adaptation over the entire round. With this, it is even possible to use policy
adaption in the online playing with aﬀordable computational resources.

Suphx is an agent that constantly learns and improves. Today’s achievement
of Suphx on Tenhou.net is just the beginning. Looking forward, we will introduce
more novel technologies to Suphx, and continue to push the frontier of Mahjong
AI and imperfect-information game playing.

Most real world problems such as ﬁnance market predication and logistic
optimization share the same characteristics with Mahjong rather than Go/chess
- complex operation/reward rules, imperfect information, etc. We believe our
techniques designed in Suphx for Mahjong, including global reward prediction,
oracle guiding and parametric Monte-Carlo policy adaptation, have a great
potential to beneﬁt for a wide range of real-world applications.

Acknowledgement

We would like to sincerely thank Tsunoda Shungo and Tenhou.net for providing
the expert game playing logs and online platform for our experiments. We
would like to thank players on Tenhou.net for playing games with Suphx. We
would like to thank MoYuan for helping us to collect the statistics of human
professional players. We would also like to thank our interns Hao Zheng and
Xiaohong Ji, as well as our colleagues Yatao Li, Wei Cao, Weidong Ma, and Di
He for their contributions to developing the learning algorithms and training
system of Suphx.

References

1. Christopher Berner, Greg Brockman, Brooke Chan, Vicki Cheung, Prze-
mysław Dębiak, Christy Dennison, David Farhi, Quirin Fischer, Shariq
Hashme, Chris Hesse, et al. Dota 2 with large scale deep reinforcement
learning. arXiv preprint arXiv:1912.06680, 2019.

2. Michael Bowling, Neil Burch, Michael Johanson, and Oskari Tammelin.
Heads-up limit Hold’Em poker is solved. Commun. ACM, 60(11):81–88,
October 2017.

23

3. Noam Brown and Tuomas Sandholm. Superhuman AI for heads-up no-limit
poker: Libratus beats top professionals. Science, 359(6374):418–424, January
2018.

4. Noam Brown and Tuomas Sandholm. Superhuman ai for multiplayer poker.

Science, 365(6456):885–890, 2019.

5. European Mahjong Association. Rules for Japanese Mahjong.

http:

//mahjong-europe.org/portal/images/docs/Riichi-rules-2016-EN.
pdf.

6. Shiqi Gao, Fuminori Okuya, Yoshihiro Kawahara, and Yoshimasa Tsuruoka.
Supervised learning of imperfect information data in the game of mahjong
via deep convolutional neural networks. Information Processing Society of
Japan, 2018.

7. Shiqi Gao, Fuminori Okuya, Yoshihiro Kawahara, and Yoshimasa Tsuruoka.
Building a computer mahjong player via deep convolutional neural networks.
June 2019.

8. Moyuru Kurita and Kunihito Hoki. Method for constructing artiﬁcial intelli-
gence player with abstraction to markov decision processes in multiplayer
game of mahjong. April 2019.

9. N Mizukami and Y Tsuruoka. Building a computer mahjong player based on
monte carlo simulation and opponent models. In 2015 IEEE Conference on
Computational Intelligence and Games (CIG), pages 275–283, August 2015.

10. Naoki Mizukami and Yoshimasa Tsuruoka. Building a computer mahjong
player based on monte carlo simulation and opponent models. In 2015 IEEE
Conference on Computational Intelligence and Games (CIG), pages 275–283.
IEEE, 2015.

11. Matej Moravčík, Martin Schmid, Neil Burch, Viliam Lisý, Dustin Morrill,
Nolan Bard, Trevor Davis, Kevin Waugh, Michael Johanson, and Michael
Bowling. DeepStack: Expert-level artiﬁcial intelligence in heads-up no-limit
poker. Science, 356(6337):508–513, May 2017.

12. Jiang Rong, Tao Qin, and Bo An. Competitive bridge bidding with deep
neural networks. In Proceedings of the 18th International Conference on
Autonomous Agents and MultiAgent Systems, pages 16–24. International
Foundation for Autonomous Agents and Multiagent Systems, 2019.

13. David Silver, Aja Huang, Chris J Maddison, Arthur Guez, Laurent Sifre,
George van den Driessche, Julian Schrittwieser, Ioannis Antonoglou, Veda
Panneershelvam, Marc Lanctot, Sander Dieleman, Dominik Grewe, John
Nham, Nal Kalchbrenner, Ilya Sutskever, Timothy Lillicrap, Madeleine Leach,
Koray Kavukcuoglu, Thore Graepel, and Demis Hassabis. Mastering the
game of go with deep neural networks and tree search. Nature, 529(7587):484–
489, January 2016.

24

14. David Silver, Aja Huang, Chris J Maddison, Arthur Guez, Laurent Sifre,
George Van Den Driessche, Julian Schrittwieser, Ioannis Antonoglou, Veda
Panneershelvam, Marc Lanctot, et al. Mastering the game of go with deep
neural networks and tree search. nature, 529(7587):484, 2016.

15. David Silver, Thomas Hubert, Julian Schrittwieser, Ioannis Antonoglou,
Matthew Lai, Arthur Guez, Marc Lanctot, Laurent Sifre, Dharshan Kumaran,
Thore Graepel, Timothy Lillicrap, Karen Simonyan, and Demis Hassabis. A
general reinforcement learning algorithm that masters chess, shogi, and go
through self-play. Science, 362(6419):1140–1144, December 2018.

16. David Silver, Julian Schrittwieser, Karen Simonyan, Ioannis Antonoglou, Aja
Huang, Arthur Guez, Thomas Hubert, Lucas Baker, Matthew Lai, Adrian
Bolton, Yutian Chen, Timothy Lillicrap, Fan Hui, Laurent Sifre, George
van den Driessche, Thore Graepel, and Demis Hassabis. Mastering the game
of go without human knowledge. Nature, 550(7676):354–359, October 2017.

17. Richard S Sutton, David A McAllester, Satinder P Singh, and Yishay
Mansour. Policy gradient methods for reinforcement learning with function
approximation. In Advances in neural information processing systems, pages
1057–1063, 2000.

18. Gerald Tesauro. Temporal diﬀerence learning and TD-Gammon. Commun.

ACM, 38(3):58–68, 1995.

19. Shingo Tsunoda. Tenhou. https://tenhou.net/. Accessed: 2019-6-17.

20. Dwango Media Village. NAGA: Deep learning mahjong AI. https://dmv.

nico/ja/articles/mahjong_ai_naga/. Accessed: 2019-6-29.

21. Oriol Vinyals, Igor Babuschkin, Wojciech M Czarnecki, Michaël Mathieu,
Andrew Dudzik, Junyoung Chung, David H Choi, Richard Powell, Timo
Ewalds, Petko Georgiev, et al. Grandmaster level in starcraft ii using
multi-agent reinforcement learning. Nature, 575(7782):350–354, 2019.

25

Appendix A: Rules of Mahjong

Mahjong is a tile-based game that was developed in China hundreds of years ago
and is now popular worldwide with hundreds of millions of players. The game
of Mahjong itself has numerous variations across the world, and Mahjong in
diﬀerent regions is diﬀerent in both the rules and the tiles used. In this work, we
focus on 4-player Japanese Mahjong (Riichi Mahjong) considering that Japanese
Mahjong is very popular with a professional league17 of top players in Japan
and its rules (5) are clearly deﬁned and well accepted. Since the playing/scoring
rules of Japanese Mahjong are very complex and the focus of this work is not
to give a comprehensive introduction to them, here we make some selections
and simpliﬁcations and give a brief introduction to those rules. Comprehensive
introductions can be found at Mahjong International League rules (5).

There are 136 tiles in Japanese Mahjong, consisting of 34 diﬀerent kinds of
tiles, with four of each kind. The 34 tiles are consists of three suit, Bamboo,
Character and Dot, each from 1 to 9, and 7 diﬀerent Honour tiles. A game
contains multiple rounds and ends when one player loses all points, or some
winning condition is triggered.

Each player in a game will start with 25,000 points, and one of four players is
designated as the dealer. At the beginning of each round, all the tiles are shuﬄed
and arranged into four walls, each with 34 tiles. 52 tiles are dealt to 4 players
(13 for each player as his/her private hand), 14 tiles form the dead wall, which
are never played except when players declare Kongs and draw a replacement
tile, and the remaining 70 tiles form the live wall. The 4 players take turns to
draw and discard tiles.18 The player who ﬁrst builds a complete hand with at
least one yaku wins the round and gets certain round score calculated by the
rewarding rules:

• A complete hand is a set of 4 melds plus a pair. A meld can be a Pong
(three identical tiles), a Kong (four identical tiles), and a Chow (three
consecutive simple tiles of the same suit). The pair consists of any two
identical tiles, but it cannot be mixed with the four melds. A player
can make a Chow/Pong/Kong from (1) a tile drawn from the wall by
himself/herself, in which case this meld is concealed to others, or (2) a tile
discarded by other players, in which case this meld is exposed to others. If
a Kong is made from a tile drawn from the wall, it is called ClosedKong.
After a player makes a Kong, he/she needs to draw an additional tile from
the dead wall for replacement. A special case called AddKong converts
an exposed Pong into a Kong when a player draws a tile that matches an
exposed Pong he/she has.

• A yaku is a certain pattern of players’ tiles or a special condition. Yaku
is the main factor to determine round score, and the value of yaku varies
from diﬀerent patterns. A winning hand could contain several diﬀerent

17https://m-league.jp/
18The turns can be interrupted by a Kong/Kong and declaring a winning hand.

26

Level

Rookie
9Kyu
8Kyu
7Kyu
6Kyu
5Kyu
4Kyu
3Kyu
2Kyu
1Kyu
1Dan
2Dan
3Dan
4Dan
5Dan
6Dan
7Dan
8Dan
9Dan
10Dan
Phoenix

Base
Ranking Pts.

1st

2nd

3rd

4th

Level Up
Ranking Pts.

Level Down

0
0
0
0
0
0
0
0
0
0
200
400
600
800
1000
1200
1400
1600
1800
2000

+20
@Normal Room

+10
@Normal Room

+40
@Advanced Room

+10
@Advanced Room

+0

+50
@Expert Room

+20
@Expert Room

+60
@Phoenix Room

+30
@Phoenix Room

Honorable Title

0
0
0
0
0
0
0
0
-15
-30
-45
-60
-75
-90
-105
-120
-135
-150
-165
-180

20
20
20
20
40
60
80
100
100
100
400
800
1200
1600
2000
2400
2800
3200
3600
4000

-
-
-
-
-
-
-
-
-
-
Yes
Yes
Yes
Yes
Yes
Yes
Yes
Yes
Yes
Yes
-

Table 6: Tenhou ranking systems: diﬀerent levels and their requirements

yaku’s and the ﬁnal round score will be accumulated across all the yaku’s in
hand. Diﬀerent variations of Japanese Majong have diﬀerent yaku patterns.
A common list of yaku consist of 40 diﬀerent types. Furthermore, dora,
a special tile determined by rolling dice before drawing tiles, provides
additional points as a bonus.

The last tile that a player forms a winning hand together with his/her private
tiles can come from (1) a tile from the wall drawn by himself/herself or (2) a tile
discarded by other players. For the ﬁrst case, all other players will lose points to
the winner. For the second case, the player who discards the tile will lose points
to the winner.

A special yaku is that player can declare Riichi when his/her hand is only
one tile away from a winning hand. Once declaring a Riichi, the player can only
claim winning hand from either a self-drawn tile or a tile discarded by other
players, and he/she is not allowed to change his/her hand tiles any more.19

The ﬁnal ranking points a player gets from a game is determined by his/her
level and the rank of his/her accumulated round score over the multiple rounds
of the game, as shown in Table 6.

Appendix B: Tenhou Ranking Rules

Tenhou uses the Japanese Martial Arts Ranking System20, which starts from
rookie, 9 kyuu down to 1 kyuu, and then 1 dan up to 10 dan. Players earn/lose

19http://mahjong.wikidot.com/riichi
20https://en.wikipedia.org/wiki/Dan_(rank)

27

ranking points when they win/lose ranked games. The amount earned or lost
depends on the game result (which ranges from 1 to 4), the current level of the
player, and the room that the player is in. There are four types of room in
Tenhou, normal room, advanced room, expert room and phoenix room. The
punishment (i.e., the negative ranking points) of losing a game is the same across
these rooms, but the reward (i.e., the positive ranking points) of winning a game
are diﬀerent. The ranking system is designed to punish high-level players if
he/she loses a game: a player of a higher level will lose more point for the 4-th
rank of game than that of a lower-level player, e.g., -180 for a 10-dan player vs.
-120 for a 6-dan player.

When a player plays a game, he/she wins/loses some ranking points from
the game, and his/her total ranking points change. If the total ranking points
increase and reach the requirement of the next level, his/her rank increases 1
level; if the total ranking points decrease to 0, his/her rank decreases 1 level.
When his/her rank level changes, he/she will get initial ranking points at the
new level. The details of the ranking points across diﬀerent rooms and levels
are listed in Table 6. Therefore, the rank of a player is not stable and often
change over time. We use record rank to denote the highest rank a player has
ever achieved in Tenhou.

Appendix C: Stable Rank

Tenhou uses stable rank to evaluate the long-term average performance of a
player. The stable rank in the expert room is calculated as follows.21

Let n1 denote the number of games a player gets the highest accumulated
round scores, n4 the number of games he/she gets the lowest accumulated
round scores, and n2 and n3 the numbers of games for the second/third highest
accumulated round scores. Then the stable rank of the player in terms of dan is

5 × n1 + 2 × n2
n4

− 2.

(7)

Since the accumulated round score of a game depends not only on the skills
of the player but also on the private tiles of the four players and the wall tiles,
the stable rank could be of large variance due to the randomness in the hidden
information. Furthermore, when playing in Tenhou, opponents are randomly
allocated by the Tenhou system, which brings in additional randomness. Thus,
for a player in Tenhou, it is usually assumed that at least a few thousands of
games are needed to get a relatively reliable stable rank.

21https://tenhou.net/man/#RANKING

28

