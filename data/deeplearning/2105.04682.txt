Value Iteration in Continuous Actions, States and Time

Michael Lutter 1 2 Shie Mannor 1 3 Jan Peters 2 Dieter Fox 1 4 Animesh Garg 1 5

1
2
0
2

y
a
M
0
1

]

G
L
.
s
c
[

1
v
2
8
6
4
0
.
5
0
1
2
:
v
i
X
r
a

Abstract

Classical value iteration approaches are not appli-
cable to environments with continuous states and
actions. For such environments, the states and
actions are usually discretized, which leads to an
exponential increase in computational complexity.
In this paper, we propose continuous ﬁtted value
iteration (cFVI). This algorithm enables dynamic
programming for continuous states and actions
with a known dynamics model. Leveraging the
continuous-time formulation, the optimal policy
can be derived for non-linear control-aﬃne dy-
namics. This closed-form solution enables the
eﬃcient extension of value iteration to continuous
environments. We show in non-linear control ex-
periments that the dynamic programming solution
obtains the same quantitative performance as deep
reinforcement learning methods in simulation but
excels when transferred to the physical system.
The policy obtained by cFVI is more robust to
changes in the dynamics despite using only a deter-
ministic model and without explicitly incorporat-
ing robustness in the optimization. Videos of the
physical system are available at https://sites.
google.com/view/value-iteration.

1. Introduction
Reinforcement learning (RL) maximizes the scalar rewards 𝑟
by trying diﬀerent actions u selected by the policy 𝜋. For a
deterministic policy and deterministic dynamics, the discrete
time optimization is described by

𝜋∗ (x) = arg max

𝜋

∞
∑︁

𝑖=0

𝛾𝑖𝑟 (x𝑖, u𝑖),

(1)

with the dynamics x𝑖+1 = 𝑓 (x𝑖, u𝑖), the state x and dis-
counting factor 𝛾 ∈ [0, 1) (Bellman, 1957; Puterman, 1994).
The discrete time framework has been very successful to

1NVIDIA 2Technical University of Darmstadt 3Technion, Israel
Institute of Technology 4University of Washington 5University of
Toronto & Vector Institute. Correspondence to: Michael Lutter
<michael@robot-learning.de>.

Proceedings of the 37𝑡 ℎ International Conference on Machine
Learning, PMLR 108, 2021. Copyright 2021 by the author(s).

Figure 1. Value iteration (VI) can compute the optimal value func-
tion 𝑉 ∗ and policy 𝜋∗ for discrete state and action environments.
Fitted value iteration extends VI to continuous states and discrete
actions. Our proposed continuous ﬁtted value iteration enables
value iteration for continuous states and continuous actions, e.g., a
car with steering rather than a car moving left, right, up and down.

learn policies for various applications including robotics
(Silver et al., 2017; OpenAI et al., 2019; Da et al., 2020).
However, the ﬁxed pre-determined time discretization is a
limitation when dealing with physical phenomena evolving
in continuous time. For such environments, the time-step can
be chosen to ﬁt the environment. For example, in robotics
the control frequency is only limited by the sampling fre-
quencies of sensors and actuators, which is commonly much
higher than the control frequency of deep RL agents. To
avoid the ﬁxed time steps, we use the continuous-time RL
objective to frame an optimization that is agnostic to the
time-discretization. Furthermore, this formulation simpliﬁes
the optimization as many robot dynamics are control aﬃne
at the continuous-time limit.

To solve the continuous-time RL problem, we use classical
value iteration where the dynamics and reward function are
known. For robotics, this setting is commonly a reasonable
assumption. The reward is known as it is manually tuned
to achieve the task without violating system constraints.
The equations of motion and system parameters of robots
are approximately known. The model is not perfect as
the actuators are not ideal and the physical parameters
vary slightly. However, the overall system behavior is
approximately correct. The same assumption is also used
within the vast sim2real literature to enable the sample
eﬃcient transfer to the physical world (Ramos et al., 2019;
Chebotar et al., 2019; Xie et al., 2021).

In this paper we show for the continuous-time RL that

(1) classical value iteration (Bellman, 1957) can be ex-
tended to environments with continuous actions and states
if the dynamics are control aﬃne and the reward separable.
Learning the value function 𝑉 is suﬃcient as the policy can

Value IterationFitted Value IterationContinuous Fitted Value Iteration 
 
 
 
 
 
Continuous Fitted Value Iteration

Figure 2. The optimal value function 𝑉 ∗ and policy 𝜋∗ and roll outs of the torque-limited pendulum computed by DP cFVI, RTDP cFVI
and the deep RL baselines with uniform initial state distribution. cFVI learns a symmetric and smooth optimal policy that can swing-up
the pendulum from both sides. Especially the DP variant has a very sharp ridge leading to the upward pointing pendulum. The baselines
do not achieve a symmetric swing-up and usually prefer a one-sided swing-up.

be deduced from 𝑉 in the continuous-time limit. Therefore,
one does not need the policy optimization used by the pre-
dominant actor-critic approaches (Schulman et al., 2015;
Lillicrap et al., 2015) or the discretization required by the
classical algorithms (Bellman, 1957; Sutton et al., 1998).
(2) the proposed approach can be successfully applied to
learn the optimal policy for real-world under-actuated sys-
tems using only the approximate dynamics model.
(3) We provide an in-depth quantitative and qualitative eval-
uation with comparisons to actor-critic algorithms. Using
value iteration on the compact state domain obtains a more
robust policy when transferred to the physical systems.

Summary of contributions. We extend value iteration
to continuous actions as well as the extensive quantitative
and qualitative evaluation on two physical systems. The
latter is in contrast to prior work (Schulman et al., 2017;
Haarnoja et al., 2018; Lillicrap et al., 2015; Harrison* et al.,
2017; Mandlekar* et al., 2017), which mainly focuses on the
quantitative comparison in simulation. Instead, we focus on
the qualitative evaluation on the physical Furuta pendulum
and Cartpole to understand the diﬀerences in performance.

In the following, we summarize continuous-time RL (Sec-
tion 2) and introduce continuous ﬁtted value iteration (Sec-
tion 3). Section 4 describes suitable network representations
for a Lyapunov value function. Finally, we analyze the
performance on the physical systems in Section 5.

2. Problem Statement
We consider the deterministic, continuous-time RL problem.
The inﬁnite horizon optimization is described by

𝜋∗ (x0) = arg max

𝜋

𝑉 ∗(x0) = max
u

∫ ∞

0
∫ ∞

0

exp(−𝜌𝑡) 𝑟𝑐 (x𝑡 , u𝑡 ) 𝑑𝑡

exp(−𝜌𝑡) 𝑟𝑐 (x𝑡 , u𝑡 ) 𝑑𝑡

with x(𝑡) = x0 +

∫ 𝑡

0

𝑓𝑐 (x𝜏, u𝜏) 𝑑𝜏

(2)

(3)

(4)

with the discounting constant 𝜌 ∈ (0, ∞], the reward 𝑟𝑐 and
the dynamics 𝑓𝑐 (Kirk, 1970). Notably, the discrete reward
and discounting can be described using the continuous-
time counterparts, i.e., 𝑟 (x, u) = Δ𝑡 𝑟𝑐 (x, u) and 𝛾 =
exp(−𝜌 Δ𝑡) with the sampling interval Δ𝑡. The continuous-
time discounting 𝜌 is, in contrast to the discrete discounting
factor 𝛾, agnostic to sampling frequencies. In the continuous-
time case, the Q-function does not exist (Doya, 2000).
The deterministic continuous-time dynamics model 𝑓𝑐 is
assumed to be non-linear w.r.t. the system state x but aﬃne
w.r.t. the action u. Such dynamics model is described by

(cid:164)x = a(x; 𝜃) + B(x; 𝜃)u,

(5)

with the non-linear drift a, the non-linear control matrix
B and the system parameters 𝜃. Robot dynamics models
are naturally expressed in the continuous-time formulation
and many are control aﬃne. Furthermore, this special case
has received ample attention in the existing literature due to
its wide applicability (Doya, 2000; Kappen, 2005; Todorov,
2007). The reward is separable into a non-linear state reward
𝑞𝑐 and the action cost 𝑔𝑐 described by

𝑟𝑐 (x, u) = 𝑞𝑐 (x) − 𝑔𝑐 (u).

(6)

The action penalty 𝑔𝑐 is non-linear, positive deﬁnite and
strictly convex. This separability is common for robot control
problems as rewards are composed of a state component
quantifying the distance to a desired state and an action
penalty. The action cost penalizes non-zero actions to avoid
bang-bang control from being optimal and is convex to have
an unique optimal action.

3. Continuous Fitted Value Iteration
First, we summarize the existing value iteration approaches
and present the theory enabling us to extend value iteration
to continuous action spaces. Afterwards, we introduce our
proposed algorithm to learn the value function.

−7.50.07.5˙θ[rad/s]V∗(x)DPcFVI(ours)RTDPcFVI(ours)SAC-USAC-U&UDRDDPG-UDDPG-U&UDRPPO-UPPO-U&UDR±π−π/20+π/2±πθ[rad]−7.50.07.5˙θ[rad/s]π∗(x)±π−π/20+π/2±πθ[rad]±π−π/20+π/2±πθ[rad]±π−π/20+π/2±πθ[rad]±π−π/20+π/2±πθ[rad]±π−π/20+π/2±πθ[rad]±π−π/20+π/2±πθ[rad]±π−π/20+π/2±πθ[rad]Continuous Fitted Value Iteration

3.1. Value Iteration Preliminaries
Value iteration (VI) is an approach to compute the optimal
value function for a discrete time, state and action MDP
with known dynamics and reward (Bellman, 1957). This
approach iteratively updates the value function of each state
using the Bellman optimality principle described by

𝑉 𝑘+1 (x𝑡 ) = max
u0...ℓ

ℓ−1
∑︁

𝑖=0

𝛾𝑖𝑟 (x𝑡+𝑖, u𝑖) + 𝛾ℓ𝑉 𝑘 (x𝑡+ℓ),

with the number of look-ahead steps ℓ. VI is proven to
converge to the optimal value function (Puterman, 1994) as
the update is a contraction described by

(cid:107)𝑉 𝑘+1
𝑖

− 𝑉 𝑘+1
𝑗

(cid:107) ≤ 𝛾ℓ (cid:107)𝑉𝑖 − 𝑉 𝑗 (cid:107).

(7)

For discrete actions, the greedy action is determined by
evaluating all possible actions. However, VI is impractical
for larger MDPs as the computational complexity grows
exponentially with the number of states and actions. This is
especially problematic for continuous spaces as discretiza-
tion leads to an exponential increase of state-action tuples.

VI can be applied to continuous state spaces and discrete
actions by using a function approximator instead of tabular
value function. Fitted value iteration (FVI) computes the
value function target using the VI update and minimizes the
ℓ𝑝-norm between the target and the approximation 𝑉 𝑘 (x; 𝜓).
This approach is described by

𝑉tar (x𝑡 ) = max
u

𝜓𝑘+1 = arg min

𝑟 (x𝑡 , u) + 𝛾𝑉 𝑘 ( 𝑓 (x𝑡 , u); 𝜓𝑘 )
∑︁

(cid:107)𝑉tar (x) − 𝑉 𝑘 (x; 𝜓)(cid:107)

𝑝
𝑝

𝜓

x∈D

(8)

(9)

with the parameters 𝜓𝑘 at iteration 𝑘 and the ﬁxed dataset D.
The convergence proof of VI does not generalize to FVI
as the ﬁtting of the value function is not necessarily a
contraction (Boyan & Moore, 1994; Baird, 1995; Tsitsiklis
& Van Roy, 1996; Munos & Szepesvári, 2008). Despite
these theoretical limitations, many authors proposed model-
free variants using the Q-function for discrete action MDPs,
e.g., ﬁtted Q-iteration (FQI) (Ernst et al., 2005), Regularized
FQI (Farahmand et al., 2009), Neural FQI (Riedmiller, 2005)
and DQN (Mnih et al., 2015). The resulting algorithms were
empirically successful and solved Backgammon (Tesauro,
1992) and the Atari games (Mnih et al., 2015).

For continuous actions, current RL methods use policy
iteration (PI) rather than VI (Schulman et al., 2015; Lillicrap
et al., 2015; ?). PI evaluates the value function of the
current policy and hence, uses the action of the policy to
compute the value function target. Therefore, PI circumvents
the maximization required of VI (Equation 8). To update
the policy, PI uses an additional optimization to improve
the policy via policy gradients. Therefore, PI requires an
additional optimization compared to VI approaches.

Algorithm 1 Continuous Fitted Value Iteration (cFVI)
Input: Dynamics Model 𝑓𝑐 (x, u) & Dataset D
Result: Value Function 𝑉 ∗(x; 𝜓∗)

for k = 0 . . . N do

0

𝛽 exp(−𝛽𝑡) 𝑅𝑡 𝑑𝑡 + exp(−𝛽𝑇)𝑅𝑇
0 exp(−𝜌𝜏) 𝑟𝑐 (x𝜏, u𝜏)𝑑𝜏 + exp(−𝜌𝑡)𝑉 𝑘 (x𝑡 )

// Compute Value Target for x ∈ D:
𝑉tar (x𝑖) = ∫ 𝑇
𝑅𝑡 = ∫ 𝑡
x𝜏 = x𝑖 + ∫ 𝜏
u𝜏 = ∇ ˜𝑔 (cid:0)B(x𝜏)∇𝑥𝑉 𝑘 (x𝜏))(cid:1)

𝑓𝑐 (x𝑡 , u𝑡 )𝑑𝑡

0

// Fit Value Function:
𝜓𝑘+1 = arg min𝜓 (cid:205)

x∈ D (cid:107)𝑉tar (x) − 𝑉 (x; 𝜓)(cid:107) 𝑝

if RTDP cFVI then

// Add samples from 𝜋𝑘+1 to FIFO buﬀer D
D 𝑘+1 = ℎ(D 𝑘 , {x𝑘+1

. . . x𝑘+1

𝑁 })

0

end if
end for

3.2. Deriving the Analytic Optimal Policy
One cannot directly apply FVI to continuous actions due
to the maximization in Equation 8. To compute the value
function target, one would need to solve an optimization
in each iteration. To extend value iteration to continuous
actions, we show that one can solve this maximization
analytically for the considered continuous-time problem.
This solution enables the eﬃcient computation of the value
function target and extends FVI to continuous actions.

Theorem 1. If the dynamics are control aﬃne (Equation 5),
the reward is separable w.r.t. to state and action (Equation 6)
and the action cost 𝑔𝑐 is positive deﬁnite and strictly convex,
the continuous-time optimal policy 𝜋𝑘 is described by

𝜋𝑘 (x) = ∇ ˜𝑔𝑐

(cid:16)

B(x)𝑇 ∇𝑥𝑉 𝑘 (cid:17)

(10)

where ˜𝑔 is the convex conjugate of 𝑔 and ∇𝑥𝑉 𝑘 is the
Jacobian of current value function 𝑉 𝑘 w.r.t. the system state.

Proof Sketch. The detailed proof is provided in the appendix.
This derivation follows the work of Lutter et. al. (2019),
which generalized the special cases initially described by
Lyshevski (1998) and Doya (2000) to a wider class of reward
functions. The value iteration target (Equation 8) can be
expressed by using the Taylor series for the value function
𝑉 (x𝑡+1) and substituting Equation 5 and Equation 6. This
reformulated value target is described by

𝑉tar = max
u

(cid:2)𝛾∇𝑥𝑉𝑇 (a + Bu) + 𝛾 O (.) + 𝑞𝑐 − 𝑔𝑐(cid:3) Δ𝑡 + 𝛾𝑉

with the higher order terms O (Δ𝑡, x, u). In the continuous-
time limit the higher order terms disappear and 𝛾 = 1.

Continuous Fitted Value Iteration

Therefore, the optimal action is described by

u∗

𝑡 = arg max

∇𝑥𝑉𝑇 B(x𝑡 ) u − 𝑔𝑐 (u).

(11)

u

Equation 11 can be solved analytically as 𝑔𝑐 is strictly
convex and hence ∇𝑔𝑐 (u) = w is invertible, i.e., u =
[∇𝑔𝑐]−1 (w) (cid:66) ∇ ˜𝑔𝑐 (w) with the convex conjugate ˜𝑔. The
solution of Equation 11 is described by

B𝑇 ∇𝑥𝑉 ∗ − ∇𝑔𝑐 (u) (cid:66) 0 ⇒ u∗ = ∇ ˜𝑔𝑐

(cid:16)

B𝑇 ∇𝑥𝑉 𝑘 (cid:17)

.

(cid:3)
Unrolling the closed form policy over time performs
hill-climbing on the value function, where the step-size
corresponds to the time-discretization. The inner part
B(x)𝑇 ∇𝑥𝑉 𝑘 determines the direction of steepest ascent
and the action cost rescales this direction. For example, a
zero action cost for all admissible actions makes the optimal
policy take the largest possible actions, i.e., the policy is
bang-bang controller. A quadratic action cost linearly re-
scales the gradient. A barrier shaped cost clips the gradients.
A complete guide on designing the action cost to shape the
optimal policy can be found in Lutter et. al. (2019). The
step-size, which corresponds to the control frequency of
the discrete time controller, determines the convergence to
the value function maximum. If the step size is too large
the system becomes unstable and does not converge to the
maximum. For most real world robotic systems with nat-
ural frequencies below 5Hz and control frequencies above
100Hz, the resulting step-size is suﬃciently small to achieve
convergence to the value function maximum. Therefore, 𝜋∗
can be used for discrete time controllers. Furthermore, the
continuous-time policy can be used for intermittent control
(event-based control) where interacting with the system oc-
curs at irregular time-steps and each interaction is associated
with a cost (Aström, 2008).

3.3. Learning the Optimal Value Function
Using the closed-form policy that analytically solves the
maximization, one can derive a computationally eﬃcient
value iteration update for continuous states and actions.
Substituting u∗ (Equation 10) into the value iteration target
computation (Equation 8) yields

𝑉tar (x𝑡 ) = 𝑟

(cid:16)

x𝑡 , ∇ ˜𝑔

(cid:16)

B(x𝑡 )𝑇 ∇𝑥𝑉 𝑘 (cid:17)(cid:17)
(cid:16)

+ 𝛾𝑉 𝑘 (x𝑡+1; 𝜓𝑘 )
(cid:17)

with x𝑡+1 = 𝑓

x𝑡 , ∇ ˜𝑔(B(x𝑡 )𝑇 ∇𝑥𝑉 𝑘 )

.

Combined with ﬁtting the value function to the target value
(Equation 9), these two steps constitute the value function
update of cFVI. Repeatedly computing the value target and
ﬁtting the value function leads to the optimal value function
and policy.

For the continuous-time limit, the computation of the naive
value function target should be adapted as the convergence

Figure 3. Tracked swing-up of the Furuta pendulum for best and
worst roll out. DP cFVI can consistently swing-up the pendulum
with minimal variance between roll outs. DP cFVI achieves much
better performance compared to most baselines. Only PPO with
domain randomization achieves comparable results. The remaining
baselines are shown in the appendix.

decreases with decreasing time steps. As Δ𝑡 decreases, the 𝛾
increases, i.e., 𝛾 = limΔ𝑡→0 exp(−𝜌Δ𝑡) = 1. Therefore, the
contraction coeﬃcient of VI decreases exponentially with
increasing sampling frequencies. This slower convergence
is intuitive as higher sample frequencies eﬀectively increase
the number of steps to reach the goal.

N-Step Value Function Target To increase the computa-
tional eﬃciency cFVI, the exponentially weighted n-step
value function target

𝑉tar (x) =

∫ 𝑇

0

𝛽 exp(−𝛽𝑡) 𝑅𝑡 𝑑𝑡 + exp(−𝛽𝑇)𝑅𝑇 ,

exp(−𝜌𝜏) 𝑟𝑐 (x𝜏, u𝜏)𝑑𝜏 + exp(−𝜌𝑡)𝑉 𝑘 (x𝑡 ),

𝑅𝑡 =

∫ 𝑡

0

and the exponential decay constant 𝛽, can be used. The inte-
grals can be solved using any ordinary diﬀerential equation
solver with ﬁxed or adaptive step-size. We use the explicit
Euler integrator with ﬁxed steps to solve the integral for all
samples in parallel using batched operations on the GPU. The
nested integrals can be computed eﬃciently by recursively
splitting the integral and reusing the estimate of the previous
step. In practice we treat 𝛽 as a hyperparameter and select
𝑇 such that the weight of the 𝑅𝑇 is exp (−𝛽𝑇) (cid:66) 10−4.

The convergence rate improves as this target computation
corresponds to the multi-step value target. The n-step target
increases the contraction rate as this rate is proportional to
𝛾ℓ. This approach is the continuous-time counterpart of
the discrete eligibility trace of TD(𝜆) with 𝜆 = exp(−𝛽Δ𝑡)
(Sutton et al., 1998). With respect to deep RL this discounted
n-step value target is similar to the generalized advantage
estimation (GAE) of PPO (Schulman et al., 2015; 2017) and
model-based value expansion (MVE) (Feinberg et al., 2018;

Continuous Fitted Value Iteration

Buckman et al., 2018). GAE and MVE have shown that
the 𝑛-step target increases the sample eﬃciency and lead to
faster convergence to the optimal policy.

Oﬄine and Online cFVI The proposed approach is oﬀ-
policy as the samples in the replay memory do not need to
originate from the current policy 𝜋𝑘 . Therefore, the dataset
can either consist of a ﬁxed dataset (i.e., batch or oﬄine RL)
or be updated within each iteration. In the oﬄine dynamic
programming case, the dataset contains samples from the
compact state domain X. We refer to the oﬄine variant as
DP cFVI. In the online case, the replay memory is updated
with samples generated by the current policy 𝜋𝑘 . Every
iteration the states of the previous 𝑛-rollouts are added to the
data and replace the oldest samples. This online update of
state distribution performs real-time dynamic programming
(RTDP) (Barto et al., 1995). We refer to the online variant
as RTDP cFVI. The pseudo code of DP cFVI and RTDP
cFVI is summarized in Algorithm 1.

4. Value Function Hypothesis Space
The previous sections focused on learning the optimal value
function independent of the value function representation.
In this section, we focus on the value function representation.

Value Function Representation Most recent approaches
use a fully connected network to approximate the value
function. However, for the many tasks the hypothesis space
of the value function can be narrowed. For control tasks,
the state cost is often a negative distance measure between
x𝑡 and the desired state xdes. Hence, 𝑞𝑐 is negative deﬁnite,
i.e., 𝑞(x) < 0 ∀ x ≠ xdes and 𝑞(xdes) = 0. These
properties imply that 𝑉 ∗ is a negative Lyapunov function, as
𝑉 ∗ is negative deﬁnite, 𝑉 ∗ (xdes) = 0 and ∇𝑥𝑉 ∗ (xdes) = 0
(Khalil & Grizzle, 2002). With a deep network a similar
representation can be achieved by
𝑉 (x; 𝜓) = − (x − xdes)𝑇 L(x; 𝜓)L(x; 𝜓)𝑇 (x − xdes)

with L being a lower triangular matrix with positive diag-
onal. This positive diagonal ensures that LL𝑇 is positive
deﬁnite. Simply applying a ReLu activation to the last
layer of a deep network is not suﬃcient as this would also
zero the actions for the positive values and ∇𝑥𝑉 ∗(xdes) = 0
cannot be guaranteed. The local quadratic representation
guarantees that the gradient and hence, the action, is zero at
the desired state. However, this representation can also not
guarantee that the value function has only a single extrema
at xdes as required by the Lyapunov theory. In practice, the
local regularization of the quadratic structure to avoid high
curvature approximations is suﬃcient as the global structure
is deﬁned by the value function target. L is the mean of a
deep network ensemble with 𝑁 independent parameters 𝜓𝑖.
The ensemble mean smoothes the initial value function and
is diﬀerentiable. Similar representations have been used by
prior works in the safe reinforcement learning community

Figure 4. Tracked swing-up of the cartpole for the best and worst
roll out. DP cFVI and RTDP cFVI can achieve the task. In the
failure case, cFVI excels as the policy remains in the center. In
contrast the deep RL baselines move the cart between the joint
limits in case of failure. All baselines are shown in the appendix.

(Berkenkamp et al., 2017; Richards et al., 2018; Kolter &
Manek, 2019; Chang et al., 2019; Bharadhwaj et al., 2021).

Gradient Projection of State Transformations State trans-
formations should be explicitly incorporated in the value
function and should not be contained in the environment,
as for example in the openAI Gym (Brockman et al., 2016).
If the transformation is implicit, ∇𝑥𝑉 might not be sen-
sible. For example, the standard feature transform for a
continuous revolute joint maps the joint state x = [𝜃, (cid:164)𝜃]
to z = [sin(𝜃), cos(𝜃), (cid:164)𝜃]. In this case the transformation
ℎ(x) must be included in the value function as the trans-
formed state z is on the tube shaped manifold . Hence, the
naive gradient of 𝑉 might not be in the manifold tangent
space.

If the state transform is explicitly incorporated in the value
function, this problem does not occur. The transformation
can be included explicitly by 𝑉 (𝑥; 𝜓) = 𝑓 (ℎ(x); 𝜓) and
∇𝑥𝑉 (𝑥; 𝜓) = 𝜕 𝑓 (ℎ(x); 𝜓)/𝜕ℎ 𝜕ℎ(x)/𝜕x. In this case, the
gradient of the transformed state is projected to the tangent
space of the feature transform. Therefore, the value function
gradient points in a plausible direction.

5. Experiments
In the following the experimental setup and results are
described. The exact experiment speciﬁcation, all qualitative
plots and an additional ablation study on model ensembles
is provided in the appendix.

5.1. Experimental Setup
Systems The algorithms are compared using the stan-
dard non-linear control benchmark, the swing-up of under-
actuated systems. Speciﬁcally, we apply the algorithms to
the torque-limited pendulum, cartpole (Figure 4) and Furuta
pendulum (Figure 3). For the physical systems the dynamics
model of the manufacturer is used (Quanser, 2018). The
control frequency is optimized for each algorithm. The task
is considered solved, if the pendulum angle 𝛼 is below ±5◦
degree for the last second.

Continuous Fitted Value Iteration

Figure 5. The learning curves for DP cFVI and RTDP cFVI averaged over 5 seeds. The shaded area displays the min/max range between
seeds. DP cFVI achieves consistent learning of the optimal policy with very low variance between seeds. RTDP cFVI has a higher
variation and learns slower compared to DP cFVI. RTDP cFVI needs to discover the steady-state state distribution ﬁrst. Especially for the
Furuta pendulum the range increases between seeds as minor changes in the policy lead to large changes of state-distribution.

Table 1. Average rewards on the simulated and physical systems. The ranking describes the decrease in reward compared to the best result
averaged on all systems. The initial state distribution during training is noted by 𝜇. The dynamics are either deterministic model 𝜃 ∼ 𝛿(𝜃)
or sampled using uniform domain randomization 𝜃 ∼ U (𝜃). During evaluation the roll outs start with the pendulum pointing downwards.
DP cFVI obtains high rewards on all systems and is the highest ranking algorithm compared to the baselines.

Algorithm

𝜇

𝜃

U U ( 𝜃)
DP cFVI (ours)
RTDP cFVI (ours) U U ( 𝜃)

SAC
SAC & UDR
SAC
SAC & UDR

DDPG
DDPG & UDR
DDPG
DDPG & UDR

PPO
PPO & UDR
PPO
PPO & UDR

N U ( 𝜃)
N 𝛿 ( 𝜃))
U U ( 𝜃)
U U ( 𝜃)

N U ( 𝜃)
N 𝛿 ( 𝜃))
U U ( 𝜃)
U U ( 𝜃)

N U ( 𝜃)
N 𝛿 ( 𝜃))
U U ( 𝜃)
U U ( 𝜃)

Simulated Pendulum

Simulated Cartpole

Success
[%]

Reward
[𝜇 ± 1.96𝜎]

Success
[%]

Reward
[𝜇 ± 1.96𝜎]

Simulated Furuta Pendulum
Success
[%]

Reward
[𝜇 ± 1.96𝜎]

Physical Cartpole
Reward
[𝜇 ± 1.96𝜎]

Success
[%]

Physical Furuta Pendulum Average
Ranking
Reward
Success
[𝜇 ± 1.96𝜎]
[%]
[%]

100.0
100.0

100.0
100.0
100.0
100.0

100.0
100.0
100.0
100.0

100.0
100.0
100.0
100.0

−030.5 ± 000.8
−031.1 ± 001.4

−031.1 ± 000.1
−032.9 ± 000.6
−030.6 ± 001.4
−031.4 ± 002.5

−031.1 ± 000.4
−032.5 ± 000.5
−031.5 ± 000.7
−032.5 ± 003.6

−032.0 ± 000.2
−032.3 ± 000.6
−033.4 ± 004.7
−035.6 ± 003.1

100.0
100.0

100.0
100.0
100.0
100.0

98.0
100.0
100.0
100.0

100.0
100.0
99.0
100.0

−024.2 ± 002.1
−024.9 ± 001.6

−026.9 ± 003.2
−029.7 ± 004.6
−024.2 ± 001.4
−024.2 ± 001.3

−050.4 ± 285.6
−027.4 ± 002.3
−028.2 ± 005.5
−027.2 ± 001.0

−031.5 ± 007.2
−084.0 ± 007.8
−039.7 ± 045.7
−044.8 ± 021.4

100.0
100.0

100.0
100.0
100.0
100.0

100.0
100.0
100.0
100.0

100.0
100.0
100.0
100.0

−027.7 ± 001.6
−040.1 ± 002.7

−029.3 ± 001.5
−032.0 ± 001.1
−028.1 ± 002.0
−028.1 ± 001.3

−030.5 ± 003.5
−034.6 ± 009.8
−030.0 ± 001.7
−032.1 ± 001.5

−081.1 ± 018.3
−040.9 ± 004.6
−038.2 ± 013.1
−048.5 ± 006.2

73.3
100.0

00.0
100.0
53.3
40.0

06.7
00.0
06.7
00.0

00.0
00.0
00.0
40.0

−143.7 ± 210.4
−101.1 ± 029.0

−518.6 ± 028.1
−394.8 ± 382.8
−144.5 ± 204.0
−296.4 ± 418.9

−536.7 ± 262.7
−517.9 ± 117.6
−459.4 ± 248.3
−318.1 ± 063.4

−287.9 ± 068.8
−435.4 ± 111.9
−183.8 ± 018.0
−143.8 ± 016.1

100.0
00.0

86.7
100.0
100.0
100.0

46.7
86.7
100.0
100.0

33.3
46.7
60.0
100.0

−082.1 ± 007.6
−008.8
−1009.9 ± 004.5 −240.4

−330.7 ± 799.0
−181.4 ± 157.9
−350.8 ± 433.3
−092.3 ± 064.1

−614.1 ± 597.8
−192.7 ± 404.8
−146.6 ± 218.3
−156.7 ± 246.4

−718.7 ± 456.1
−935.7 ± 711.6
−755.3 ± 811.0
−080.6 ± 010.8

−148.4
−092.3
−076.1
−042.4

−242.6
−119.3
−092.9
−068.8

−240.9
−338.6
−206.1
−044.0

Baselines The performance is compared to the actor-critic
deep RL methods: DDPG (Lillicrap et al., 2015), SAC
(Haarnoja et al., 2018) and PPO (Schulman et al., 2017).
We compare two diﬀerent initial state distributions 𝜇. For
𝜇 = U, the initial pendulum angle 𝛼0 is sampled uniformly
𝛼0 ∼ U (−𝜋, +𝜋). For 𝜇 = N , the initial angle is sampled
from a Gaussian distribution with the pendulum facing down-
wards 𝛼0 ∼ N (±𝜋, 𝜎). The uniform sampling avoids the
exploration problem and generates a larger state distribution
of the optimal policy. In addition we augment each baseline
with uniform domain randomization (Muratore et al., 2018).

5.2. Simulation Results
The average rewards of our method - Continuous Fitted
Value Iteration (cFVI) and the baselines are summarized
in Table 1. The learning curves are shown in Figure 5. In
simulation, DP cFVI, the oﬄine version of cFVI with ﬁxed
dataset, achieves the best rewards for all three systems and
marginally outperforms SAC in terms of average reward.
It’s important to point out that identical mean rewards do
not imply a similar qualitative performance. For example,
DP cFVI and SAC-U achieve identical mean reward on the
cartpole despite having very diﬀerent closed-loop dynamics
(Figure 11 - Appendix). Notably, RTDP cFVI, which uses
the state distribution of the current policy rather than a
ﬁxed dataset, solves the tasks for all three systems. For the

pendulum and the cartpole, the reward is comparable to the
best performing algorithms. While for the Furuta pendulum,
the reward is lower (higher) than the best (worst) performing
Deep RL baselines.

The learning curves highlight that the variance between seeds
is very low for DP cFVI. It is important to note that Figure 5
shows the min-max range of the diﬀerent seeds rather than
the conﬁdence intervals or the standard error. Therefore, the
exact weight initialization and sampling of the ﬁxed dataset,
which is diﬀerent for each seed, do not have a large impact
on the learning progress or obtained solution. For RTDP
cFVI the variance between seed increases and the learning
speed decreases compared to DP cFVI. The pendulum is
an outlier for RTDP cFVI, as the state-distribution is nearly
independent of the current policy if the pendulum angle is
initialized uniformly. Especially for the Furuta pendulum
the variance increases and learning speed decreases. For this
system slightly diﬀerent policies can cause vastly diﬀerent
state distributions due to the low condition number of the
mass-matrix. Therefore, RTDP cFVI takes longer to reach a
stationary distribution and increase the obtained reward.

In general it is important to point out that the exploration
for RTDP cFVI is more challenging compared to other
deep RL methods as RTDP cFVI uses a higher control
frequency. Due to the shorter steps simple Gaussian noise

0255075100Episode−40−200RewardPendulumDPcFVI-λ=0.85RTDPcFVI-λ=0.85050100150200250Episode−150−100−500RewardCartpoleDPcFVI-λ=0.90RTDPcFVI-λ=0.45050100150200250Episode−400−300−200−1000RewardFurutaPendulumDPcFVI-λ=0.95RTDPcFVI-λ=0.40Continuous Fitted Value Iteration

Figure 6. The simulated (blue) and real world (red) roll outs for the Furuta pendulum controlled by cFVI and the baselines. The deviation
from the dashed line denotes the joint velocity. The distribution shift is clearly visible as the blue and red trajectories do not overlap, e.g.,
cFVI requires more pre-swings on the physical system. For PPO-U the distribution shift causes random exploration of the complete
state-domain. cFVI achieves the best qualitative performance as it can swing-up the pendulum from both directions and achieves nearly
identical roll-outs. In contrast the deep RL baselines have higher distribution shift due to the dynamics mismatch. The trajectories are also
less consistent and vary signiﬁcantly between roll outs. The plots for all baselines and the physical cartpole are shown in the appendix.

averages out and does not lead to a large exploration in state
space. Therefore, the performance and variance of RTDP
cFVI could be improved using a more coherent exploration
strategy in future work.

physical systems and outperforms the baselines. Only PPO-
U UDR achieves comparable performance on both physical
systems. On average the performance of the deep RL variants
increases with with larger initial state distribution.

5.3. Ablation Study - 𝑁-step Value Targets
The learning curves, averaged over 5 seeds, for diﬀerent 𝑛-
step value targets are shown in Figure 7. When increasing 𝜆,
which implicitly increases the 𝑛-step horizon (section 3.3),
the convergence rate to the optimal value function increases.
This increased learning speed is expected as Equation 7
shows that the convergence rate depends on 𝛾𝑛 with 𝛾 < 1.
While the learning speed measured in iterations increases
with 𝜆, the computational complexity also increases. Longer
horizons require to simulate 𝑛 sequential steps increasing the
required wall-clock time. Therefore, the computation time
increases exponentially with increasing 𝜆. For example the
forward roll out in every iteration of the pendulum increases
exponentially from 0.4s (𝜆 = 0.01) to 56.4s (𝜆 = 0.99)1.
For the Furuta pendulum and the cartpole extremely long
horizons of 100+ steps start to diverge as the value function
target over ﬁts to the untrained value function. For RTDP
cFVI, the horizons must smaller, i.e., 10 - 20 steps. For
longer horizons the predicted rollout overﬁts to the value
function outside of the current state distribution, which
prevents learning or leads to pre-mature convergence (see
Appendix Figure 9). Therefore, DP cFVI works bests with
𝜆 ∈ [0.85, 0.95] and RTDP cFVI with 𝜆 ∈ [0.45 − 0.55].

5.4. Physical Experiments
The quantitative results of the physical experiments are
summarized in Table 12, with additional plots and baselines
in the appendix. DP cFVI achieves high reward on both

1Wall-clock time on an AMD 3900X and a NVIDIA RTX 3090
2Task Video available at: https://sites.google.com/

view/value-iteration

The trajectories of the Furuta Pendulum are shown in Figure 6.
The distribution shift between the physical and simulated
trajectories is clearly visible as the trajectories largely deviate.
DP cFVI achieves a highly repeatable performance on the
noisy physical system. All 15 roll-outs have a similar
trajectory. The baselines require domain randomization and
uniform initial distribution to achieve a reward comparable
to DP cFVI. All other deep RL baselines have a high variance
between roll-outs. For example the simulation gap causes
PPO-U to randomly explore the complete state space. RTDP
cFVI does not achieve the swing-up but fails gracefully as
it stabilizes the pendulum on a stable limit cycle and does
not hit the joint limits. Furthermore, this limit cycle is
highly repetitive and the variance in reward is low. For many
robotic tasks, it is preferable to fail gracefully rather than
hitting the limits and solving the task.

On the physical cartpole, DP cFVI obtains a higher reward
than RTDP cFVI, if the swing-up is successful. However,
RTDP cFVI has a higher average reward across all roll-
outs as DP cFVI only achieves a 73% success rate. If DP
cFVI fails, the resulting behavior is a deterministic limit-
cycle where the cart is centered. The policy decelerates the
pendulum, but due to the backslash in the linear actuator
the applied force is not suﬃcient. Most deep RL baselines
do not achieve a consistent swing-up. Only PPO-U UDR,
SAC-U achieve a repeatable high reward. SAC-N UDR
achieves the swing-up but always hits the joint limit and
obtains a relative low reward. The failure cases of all deep
RL baselines are stochastic and repeatedly hit the joint
limit. If the pendulum over-swings the cart starts to oscillate

˙α[rad/s]-30.0+0.0+30.0−π/2−π/40+π/4+π/2Joint0DPcFVI(ours)˙α[rad/s]-30.0+0.0+30.0−π/2−π/40+π/4+π/2RTDPcFVI(ours)˙α[rad/s]-30.0+0.0+30.0−π/2−π/40+π/4+π/2PPO-U˙α[rad/s]-30.0+0.0+30.0−π/2−π/40+π/4+π/2PPO-U&UDR˙α[rad/s]-30.0+0.0+30.0−π/2−π/40+π/4+π/2SAC-U˙α[rad/s]-30.0+0.0+30.0−π/2−π/40+π/4+π/2SAC-U&UDR−3π/4−π/2−π/4+π/4+π/2+3π/4±πJoint1SimulatedTrajectoriesRealTrajectoriesDesiredStateInitialState−3π/4−π/2−π/4+π/4+π/2+3π/4±π−3π/4−π/2−π/4+π/4+π/2+3π/4±π−3π/4−π/2−π/4+π/4+π/2+3π/4±π−3π/4−π/2−π/4+π/4+π/2+3π/4±π−3π/4−π/2−π/4+π/4+π/2+3π/4±πContinuous Fitted Value Iteration

Figure 7. The learning curves averaged over 5 seeds for the 𝑛-step value function target. The shaded area displays the min/max range
between seeds. The step count is selected such that 𝜆𝑛 (cid:66) 10−4. Increasing the horizon of the value function target increases the
convergence rate to the optimal value function. For very long horizons the learning diverges as it over ﬁts to the current value function
approximation. Furthermore, the performance of the optimal policy also increases with roll out length.

between both joint limits. Notably, domain randomization
does not improve the performance on the physical cartpole.
For this system, the simulation gap originates from backslash
and friction in the actuator rather than the uncertainty of
the model parameters. As the actuation is not simulated,
these parameters cannot be randomized. For the cartpole,
the larger initial state distribution is required to achieve
high rewards on the physical system. All baselines with a
gaussian initial state distribution achieve only a low reward.
See appendix for qualitative simulated and physical results.

6. Related Work
Continuous-time RL started with the seminal work of Doya
(2000). Since then, various approaches have been proposed
to solve the Hamilton-Jacobi-Bellman (HJB) diﬀerential
equation with the machine learning toolset. These methods
can be divided into trajectory and state-space based methods.

Trajectory based methods solve the stochastic HJB along a
trajectory to obtain the optimal trajectory. For example path
integral control uses the non-linear, control-aﬃne dynamics
with quadratic action costs to simplify the HJB to a linear
partial diﬀerential equation (Kappen, 2005; Todorov, 2007;
Theodorou et al., 2010; Pan et al., 2014). This diﬀerential
equation can be transformed to a path-integral using the
Feynman-Kac formulae. The path-integral can then be
solved using Monte Carlo sampling to obtain the optimal
state and action sequence. Recently, this approach has been
used in combination with deep networks (Rajagopal et al.,
2016; Pereira et al., 2019; 2020).

State-space based methods solve the HJB globally to obtain
a optimal non-linear controller applicable on the complete
state domain. Classical approaches discretize the contin-
uous spaces into a grid and solve the HJB or the robust
Hamilton-Jacobi-Isaac (HJI) using a PDE solver (Bansal
et al., 2017). To overcome the curse of dimensionality of
the grid based methods, machine learning methods that use
function approximation and sampling have been proposed.
For example, regression based approaches solved the HJB
by ﬁtting a radial-basis-function networks (Doya, 2000),
deep networks (Tassa & Erez, 2007; Lutter et al., 2019;
Kim et al., 2020), kernels (Hennig, 2011) or polynomial
functions (Yang et al., 2014; Liu et al., 2014) to minimize

the HJB residual. The naive optimization of this objective,
while successful for diﬀerent PDEs (Raissi et al., 2017a;b;
2020), does not work for the HJB as the exact location of
the boundary condition is unknown. Therefore, authors
used various optimization tricks such as annealing the noise
(Tassa & Erez, 2007) or the discounting (Lutter et al., 2019)
to obtain the optimal value function.

In this work we propose a DP based algorithm. Instead of try-
ing to solve the HJB PDE directly via regression as previous
learning-based methods, we leverage the continuous-time do-
main and solve the HJB by iteratively applying the Bellman
optimality principle. The resulting algorithm has a simpler
and more robust optimization compared to the previous
approaches using direct regression.

7. Conclusion and Future Work
We proposed continuous ﬁtted value iteration (cFVI). This
algorithm enables dynamic programming with known model
for problems with continuous states and actions without dis-
cretization. Therefore, cFVI avoids the curse of dimensional-
ity associated with discretization and the policy optimization
of actor-critic approaches. Exploiting the insights from the
continuous-time formulation, the optimal action can be com-
puted analytically. This closed-form solution permits the
eﬃcient computation of the value function target and en-
ables the extension of value iteration. The non-linear control
experiments showed that value iteration on the compact state
space has the same quantitative performance as deep RL
methods in simulation. On the sim2real tasks, DP cFVI
excels compared deep RL algorithms that include domain
randomization and use uniform initial state distribution.

In future work, we plan to extend cFVI to (1) oﬄine/batch
model-based RL, (2) stochastic maximum entropy policies
and (3) explicit model robustness. cFVI uses a ﬁxed data
set and hence, can be combined with learning control-aﬃne
models to the oﬄine RL benchmark (Gulcehre et al., 2020;
Mandlekar et al., 2019). We plan to extend cFVI to maximum
entropy and stochastic policies to improve the exploration of
RTDP cFVI. Finally, we plan to incorporate robustness w.r.t.
to changes in dynamics by using the adversarial continuous
RL formulation culminating in the Hamilton-Jacobi-Isaacs
diﬀerential equation rather than the HJB (Isaacs, 1999).

0255075100Episode−40−200RewardPendulum0255075100125150Episode−150−100−500RewardCartpole050100150200250Episode−400−300−200−1000RewardFurutaPendulumn=2/λ=0.01n=4/λ=0.10n=7/λ=0.25n=13/λ=0.50n=17/λ=0.60n=23/λ=0.70n=35/λ=0.80n=66/λ=0.90n=122/λ=0.95n=459/λ=0.99Continuous Fitted Value Iteration

Acknowledgements

M. Lutter was an intern at Nvidia during this project. A. Garg
was partially supported by CIFAR AI Chair. We also want to
thank Fabio Muratore, Joe Watson and the ICML reviewers for
their feedback. Furthermore, we want to thank the open-source
projects SimuRLacra (Muratore, 2020), MushroomRL (D’Eramo
et al., 2020), NumPy (Harris et al., 2020) and PyTorch (Paszke
et al., 2019).

References

Aström, K. J. Event based control. In Analysis and design of
nonlinear control systems, pp. 127–147. Springer, 2008.

Baird, L. Residual algorithms: Reinforcement learning with
function approximation. In Machine Learning Proceed-
ings 1995, pp. 30–37. Elsevier, 1995.

Bansal, S., Chen, M., Herbert, S., and Tomlin, C. J. Hamilton-
Jacobi reachability: A brief overview and recent advances.
2017.

Barto, A. G., Bradtke, S. J., and Singh, S. P. Learning
to act using real-time dynamic programming. Artiﬁcial
intelligence, 72(1-2):81–138, 1995.

Bellman, R. Dynamic Programming. Princeton University

Press, USA, 1957. ISBN 0691146683.

Berkenkamp, F., Turchetta, M., Schoellig, A., and Krause,
A. Safe model-based reinforcement learning with stability
guarantees. In Advances in neural information processing
systems, pp. 908–918, 2017.

Bharadhwaj, H., Kumar, A., Rhinehart, N., Levine, S.,
Shkurti, F., and Garg, A. Conservative safety critics for
exploration. In International Conference on Learning
Representations (ICLR), 2021.

Boyan, J. and Moore, A. Generalization in reinforcement
learning: Safely approximating the value function. Ad-
vances in neural information processing systems, 7:369–
376, 1994.

Brockman, G., Cheung, V., Pettersson, L., Schneider, J.,
Schulman, J., Tang, J., and Zaremba, W. Openai gym.
arXiv preprint arXiv:1606.01540, 2016.

Buckman, J., Hafner, D., Tucker, G., Brevdo, E., and Lee,
H. Sample-eﬃcient reinforcement learning with stochas-
tic ensemble value expansion. In Advances in Neural
Information Processing Systems, pp. 8224–8234, 2018.

Chang, Y.-C., Roohi, N., and Gao, S. Neural lyapunov
control. In Advances in Neural Information Processing
Systems, pp. 3245–3254, 2019.

Chebotar, Y., Handa, A., Makoviychuk, V., Macklin, M.,
Issac, J., Ratliﬀ, N., and Fox, D. Closing the sim-to-real
loop: Adapting simulation randomization with real world
experience, 2019.

Da, X., Xie, Z., Hoeller, D., Boots, B., Anandkumar, A., Zhu,
Y., Babich, B., and Garg, A. Learning a Contact-Adaptive
Controller for Robust, Eﬃcient Legged Locomotion. In
Conference on Robot Learning (CoRL), 2020.

D’Eramo, C., Tateo, D., Bonarini, A., Restelli, M., and
Peters, J. Mushroomrl: Simplifying reinforcement learn-
https://github.com/MushroomRL/
ing research.
mushroom-rl, 2020.

Doya, K. Reinforcement learning in continuous time and

space. Neural computation, 12(1):219–245, 2000.

Ernst, D., Geurts, P., and Wehenkel, L. Tree-based batch
mode reinforcement learning. Journal of Machine Learn-
ing Research, 6(Apr):503–556, 2005.

Farahmand, A. M., Ghavamzadeh, M., Szepesvári, C., and
Mannor, S. Regularized ﬁtted q-iteration for planning in
continuous-space markovian decision problems. In 2009
American Control Conference, pp. 725–730. IEEE, 2009.

Feinberg, V., Wan, A., Stoica, I., Jordan, M. I., Gonzalez,
J. E., and Levine, S. Model-based value estimation
for eﬃcient model-free reinforcement learning. arXiv
preprint arXiv:1803.00101, 2018.

Gulcehre, C., Wang, Z., Novikov, A., Paine, T. L., Col-
menarejo, S. G., Zolna, K., Agarwal, R., Merel, J.,
Mankowitz, D., Paduraru, C., et al. Rl unplugged: Bench-
marks for oﬄine reinforcement learning. arXiv preprint
arXiv:2006.13888, 2020.

Haarnoja, T., Zhou, A., Abbeel, P., and Levine, S. Soft
actor-critic: Oﬀ-policy maximum entropy deep reinforce-
ment learning with a stochastic actor. arXiv preprint
arXiv:1801.01290, 2018.

Harris, C. R., Millman, K. J., van der Walt, S. J., Gommers,
R., Virtanen, P., Cournapeau, D., Wieser, E., Taylor, J.,
Berg, S., Smith, N. J., Kern, R., Picus, M., Hoyer, S., van
Kerkwĳk, M. H., Brett, M., Haldane, A., del Río, J. F.,
Wiebe, M., Peterson, P., Gérard-Marchant, P., Sheppard,
K., Reddy, T., Weckesser, W., Abbasi, H., Gohlke, C., and
Oliphant, T. E. Array programming with NumPy. Nature,
2020.

Harrison*, J., Garg*, A., Ivanovic, B., Zhu, Y., Savarese,
S., Fei-Fei, L., and Pavone (* equal contribution), M.
AdaPT: Zero-Shot Adaptive Policy Transfer for Stochastic
In International Symposium on
Dynamical Systems.
Robotics Research (ISRR). Springer STAR, 2017.

Continuous Fitted Value Iteration

Hennig, P. Optimal reinforcement learning for gaussian
systems. In Advances in Neural Information Processing
Systems, pp. 325–333, 2011.

Robotic manipulation dataset through human reasoning
In IEEE International Conference on
and dexterity.
Intelligent Robots and Systems (IROS), 2019.

Isaacs, R. Diﬀerential games: a mathematical theory with
applications to warfare and pursuit, control and optimiza-
tion. Courier Corporation, 1999.

Kappen, H. J. Linear theory for control of nonlinear stochas-
tic systems. Physical review letters, 95(20):200201, 2005.

Khalil, H. K. and Grizzle, J. W. Nonlinear systems, volume 3.

Prentice hall Upper Saddle River, NJ, 2002.

Kim, J., Shin, J., and Yang, I. Hamilton-jacobi deep
q-learning for deterministic continuous-time systems
arXiv preprint
with lipschitz continuous controls.
arXiv:2010.14087, 2020.

Kirk, D. E. Optimal control theory: an introduction. Courier

Corporation, 1970.

Kolter, J. Z. and Manek, G. Learning stable deep dynamics
models. In Advances in Neural Information Processing
Systems (NeurIPS), 2019.

Lillicrap, T. P., Hunt, J. J., Pritzel, A., Heess, N., Erez,
T., Tassa, Y., Silver, D., and Wierstra, D. Continuous
control with deep reinforcement learning. arXiv preprint
arXiv:1509.02971, 2015.

Liu, D., Wang, D., Wang, F.-Y., Li, H., and Yang, X. Neural-
network-based online hjb solution for optimal robust
guaranteed cost control of continuous-time uncertain
nonlinear systems. IEEE transactions on cybernetics, 44
(12):2834–2847, 2014.

Lutter, M., Belousov, B., Listmann, K., Clever, D., and Peters,
J. HJB optimal feedback control with deep diﬀerential
value functions and action constraints. In Conference on
Robot Learning (CoRL), 2019.

Lyshevski, S. E. Optimal control of nonlinear continuous-
time systems: design of bounded controllers via general-
ized nonquadratic functionals. In Proceedings of the 1998
American Control Conference, volume 1, pp. 205–209.
IEEE, 1998.

Mandlekar*, A., Zhu*, Y., Garg*, A., Fei-Fei, L., and
Savarese (* equal contribution), S. Adversarially Ro-
bust Policy Learning through Active Construction of
Physically-Plausible Perturbations. In IEEE International
Conference on Intelligent Robots and Systems (IROS),
2017.

Mandlekar, A., Booher, J., Spero, M., Tung, A., Gupta, A.,
Zhu, Y., Garg, A., Savarese, S., and Fei-Fei, L. Scaling
robot supervision to hundreds of hours with roboturk:

Mnih, V., Kavukcuoglu, K., Silver, D., Rusu, A. A., Veness,
J., Bellemare, M. G., Graves, A., Riedmiller, M., Fidje-
land, A. K., Ostrovski, G., et al. Human-level control
through deep reinforcement learning. Nature, 518(7540):
529, 2015.

Munos, R. and Szepesvári, C. Finite-time bounds for ﬁtted
value iteration. Journal of Machine Learning Research, 9
(May):815–857, 2008.

Muratore, F. Simurlacra - a framework for reinforce-
ment learning from randomized simulations. https:
//github.com/famura/SimuRLacra, 2020.

Muratore, F., Treede, F., Gienger, M., and Peters, J. Domain
randomization for simulation-based policy optimization
with transferability assessment. In Conference on Robot
Learning, pp. 700–713, 2018.

OpenAI, Andrychowicz, M., Baker, B., Chociej, M., Joze-
fowicz, R., McGrew, B., Pachocki, J., Petron, A., Plappert,
M., Powell, G., Ray, A., Schneider, J., Sidor, S., Tobin,
J., Welinder, P., Weng, L., and Zaremba, W. Learning
dexterous in-hand manipulation, 2019.

Pan, Y., Theodorou, E. A., and Kontitsis, M. Model-based
path integral stochastic control: A bayesian nonparametric
approach. arXiv preprint arXiv:1412.3038, 2014.

Paszke, A., Gross, S., Massa, F., Lerer, A., Bradbury, J.,
Chanan, G., Killeen, T., Lin, Z., Gimelshein, N., Antiga,
L., Desmaison, A., Kopf, A., Yang, E., DeVito, Z., Raison,
M., Tejani, A., Chilamkurthy, S., Steiner, B., Fang, L.,
Bai, J., and Chintala, S. Pytorch: An imperative style,
high-performance deep learning library. In Advances in
Neural Information Processing Systems 32. 2019.

Pereira, M., Wang, Z., Exarchos, I., and Theodorou, E.
Learning deep stochastic optimal control policies using
forward-backward sdes. In Robotics: science and systems,
2019.

Pereira, M. A., Wang, Z., Exarchos, I., and Theodorou,
E. A. Safe optimal control using stochastic barrier func-
tions and deep forward-backward sdes. arXiv preprint
arXiv:2009.01196, 2020.

Puterman, M. L. Markov decision processes: discrete
stochastic dynamic programming. John Wiley & Sons,
1994.

Quanser. Quanser courseware and resources. https://www.
quanser.com/solution/control-systems/, 2018.

Continuous Fitted Value Iteration

Raissi, M., Perdikaris, P., and Karniadakis, G. E. Physics
informed deep learning (part i): Data-driven solutions of
nonlinear partial diﬀerential equations. arXiv preprint
arXiv:1711.10561, 2017a.

Theodorou, E., Buchli, J., and Schaal, S. Reinforcement
learning of motor skills in high dimensions: A path
integral approach. In 2010 IEEE International Conference
on Robotics and Automation, pp. 2397–2403. IEEE, 2010.

Todorov, E. Linearly-solvable markov decision problems. In
Advances in neural information processing systems, pp.
1369–1376, 2007.

Tsitsiklis, J. N. and Van Roy, B. Feature-based methods for
large scale dynamic programming. Machine Learning,
22(1-3):59–94, 1996.

Xie, Z., Da, X., van de Panne, M., Babich, B., and Garg,
A. Dynamics Randomization Revisited: A Case Study
In IEEE International
for Quadrupedal Locomotion.
Conference on Robotics and Automation (ICRA), 2021.

Yang, X., Liu, D., and Wang, D. Reinforcement learning
for adaptive optimal control of unknown continuous-time
nonlinear systems with input constraints. International
Journal of Control, 87(3):553–566, 2014.

Raissi, M., Perdikaris, P., and Karniadakis, G. E. Physics
informed deep learning (part ii): data-driven discovery
of nonlinear partial diﬀerential equations. arXiv preprint
arXiv:1711.10566, 2017b.

Raissi, M., Yazdani, A., and Karniadakis, G. E. Hidden
ﬂuid mechanics: Learning velocity and pressure ﬁelds
from ﬂow visualizations. Science, 367(6481):1026–1030,
2020.

Rajagopal, K., Balakrishnan, S. N., and Busemeyer, J. R.
Neural network-based solutions for stochastic optimal
control using path integrals. IEEE transactions on neural
networks and learning systems, 28(3):534–545, 2016.

Ramos, F., Possas, R. C., and Fox, D. Bayessim: adap-
tive domain randomization via probabilistic inference for
robotics simulators, 2019.

Richards, S. M., Berkenkamp, F., and Krause, A. The
lyapunov neural network: Adaptive stability certiﬁcation
for safe learning of dynamical systems. arXiv preprint
arXiv:1808.00924, 2018.

Riedmiller, M. Neural ﬁtted q iteration–ﬁrst experiences with
a data eﬃcient neural reinforcement learning method. In
European Conference on Machine Learning, pp. 317–328.
Springer, 2005.

Schulman, J., Moritz, P., Levine, S., Jordan, M., and Abbeel,
P. High-dimensional continuous control using generalized
advantage estimation. arXiv preprint arXiv:1506.02438,
2015.

Schulman, J., Wolski, F., Dhariwal, P., Radford, A., and
Klimov, O. Proximal policy optimization algorithms.
arXiv preprint arXiv:1707.06347, 2017.

Silver, D., Schrittwieser, J., Simonyan, K., Antonoglou,
I., Huang, A., Guez, A., Hubert, T., Baker, L., Lai, M.,
Bolton, A., et al. Mastering the game of go without human
knowledge. Nature, 550(7676):354, 2017.

Sutton, R. S., Barto, A. G., et al.

Introduction to rein-
forcement learning, volume 135. MIT press Cambridge,
1998.

Tassa, Y. and Erez, T. Least squares solutions of the hjb equa-
tion with neural network value-function approximators.
IEEE transactions on neural networks, 18(4):1031–1041,
2007.

Tesauro, G. Practical issues in temporal diﬀerence learning.

Machine learning, 8(3):257–277, 1992.

Continuous Fitted Value Iteration

8. Appendix

Proof of Theorem 1

Theorem. If the dynamics are control aﬃne (Equation 5),
the reward is separable w.r.t. to state and action (Equation 6)
and the action cost 𝑔𝑐 is positive deﬁnite and strictly convex,
the continuous time optimal policy 𝜋𝑘 w.r.t. 𝑉 𝑘 is described
by

Therefore, the value function update can be rewritten by
substituting the optimal action, i.e.,

𝑉tar (x𝑡 ) = 𝑟

(cid:16)

x𝑡 , ∇ ˜𝑔

(cid:16)

B(x𝑡 )𝑇 ∇𝑥𝑉 𝑘 (cid:17)(cid:17)

with

x𝑡+1 = 𝑓

(cid:16)

x𝑡 , ∇ ˜𝑔(B(x𝑡 )𝑇 ∇𝑥𝑉 𝑘 )

.

+ 𝛾𝑉 𝑘 (x𝑡+1; 𝜓𝑘 )
(cid:17)

(cid:3)

𝜋𝑘 (x) = ∇ ˜𝑔𝑐

(cid:16)

B(x)𝑇 ∇𝑥𝑉 𝑘 (cid:17)

(12)

Experimental Setup

with the convex conjugate ˜𝑔 of 𝑔 and the Jacobian of 𝑉 w.r.t.
the system state ∇𝑥𝑉.

Proof. This proof follows the derivation of Lutter et. al.
(2019). This prior work derived the optimal policy 𝜋∗ using
the Hamilton Jacobi Bellman diﬀerential equation (HJB)
and generalized the special case described by Doya (2000).
The value iteration update (Equation 8) is deﬁned as

𝑉tar (x𝑡 ) = max
u

𝑟 (x𝑡 , u) + 𝛾𝑉 ( 𝑓 (x𝑡 , u); 𝜓𝑘 ).

Substituting the assumptions and using the Taylor expansion
of the Value function, i.e.,

𝑉 ( 𝑓 (x𝑡 , u)) = 𝑉 (x𝑡 ) + ∇𝑥𝑉𝑇 𝑓𝑐 (x𝑡 , u) Δ𝑡 + O (Δ𝑡, x𝑡 , u) Δ𝑡,

this update can be rewritten - omitting all functional depen-
dencies for brevity - as

𝑉tar = max
u
= max
u

𝑟 + 𝛾𝑉 + 𝛾∇𝑥𝑉𝑇 𝑓𝑐Δ𝑡 + 𝛾OΔ𝑡
(cid:2)𝛾∇𝑥𝑉𝑇 (a + Bu) + 𝛾 O − 𝑔𝑐(cid:3) Δ𝑡 + 𝛾𝑉 + 𝑞𝑐 Δ𝑡

with the higher order Terms O (Δ𝑡, x𝑡 , u). Therefore, the
optimal action is deﬁned as

u∗

𝑡 = arg max

𝛾∇𝑥𝑉𝑇 (a + Bu) + 𝛾 O (Δ𝑡, x𝑡 , u) − 𝑔𝑐 (u).

u

time

limit,

continuous

In the
the higher order
terms O (Δ𝑡, x𝑡 , u) disappear as these depend on Δ𝑡, i.e., i.e.,
limΔ𝑡→0 O (Δ𝑡, x𝑡 , u) = 0. The action is also independent of
the discounting as limΔ𝑡→0 𝛾 = 1. Therefore, the continuous
time optimal action is deﬁned as

u∗

𝑡 = arg max

∇𝑥𝑉𝑇 B(x)u𝑡 − 𝑔𝑐 (u).

u

This optimization can be solved analytically as 𝑔𝑐 is strictly
convex and hence ∇𝑔𝑐 (u) = w is invertible, i.e., u =
[∇𝑔𝑐]−1 (w) (cid:66) ∇ ˜𝑔𝑐 (w) with the convex conjugate ˜𝑔. The
optimal action is described by

B(x)𝑇 ∇𝑥𝑉 ∗ − ∇𝑔𝑐 (u) (cid:66) 0 ⇒ u∗ = ∇ ˜𝑔𝑐

(cid:16)

B(x)𝑇 ∇𝑥𝑉 𝑘 (cid:17)

.

Systems The performance of the algorithms is evaluated
using the swing-up the torque-limited pendulum, cartpole
and Furuta pendulum. The physical cartpole (Figure ??)
and Furuta pendulum (Figure ??) are manufactured by
Quanser (2018). For simulation, we use the equations of
motion and physical parameters of the supplier. Both systems
have very diﬀerent characteristics. The Furuta pendulum
consists of a small and light pendulum (24g, 12.9cm) with
a strong direct-drive motor. Even minor diﬀerences in the
action cause large changes in acceleration due to the large
ampliﬁcation of the mass-matrix inverse. Therefore, the
main source of uncertainty for this system is the uncertainty
of the model parameters. The cartpole has a longer and
heavier pendulum (127g, 33.6cm). The cart is actuated
by a geared cogwheel drive. Due to the larger masses the
cartpole is not so sensitive to the model parameters. The
main source of uncertainty for this system is the friction
and the backlash of the linear actuator. The systems are
simulated and observed with 500Hz. The control frequency
varies between algorithm and is treated as hyperparameter.

Baselines The control performance is compared to Deep De-
terministic Policy Gradients (DDPG) (Lillicrap et al., 2015),
Soft Actor Critic (SAC) (Haarnoja et al., 2018) and Proximal
Policy Optimization (PPO) (Schulman et al., 2017). The
baselines are augmented with uniform domain randomiza-
tion (UDR) (Muratore et al., 2018). For the experiments the
open-source implementations of MushroomRL (D’Eramo
et al., 2020) are used. We compare two diﬀerent initial
state distributions (𝜇). First, the initial pendulum angle
𝜃0 is sampled uniformly, i.e. 𝜃0 ∼ U (−𝜋, +𝜋). Second,
the initial angle is sampled from a Gaussian distribution
with the pendulum facing downwards, i.e., 𝜃0 ∼ N (±𝜋, 𝜎).
The uniform sampling avoids the exploration problem and
generates a larger state distribution of the optimal policy.

Reward Function The desired state for all tasks is the
upward pointing pendulum at xdes = 0. The state re-
ward is described by 𝑞𝑐 (x) = −(z − zdes)𝑇 Q(z − zdes)
with the positive deﬁnite matrix Q and the transformed
state z. For continuous joints the joint state is trans-
formed to 𝑧𝑖 = 𝜋2 sin(𝑥𝑖). The action cost is described
by 𝑔𝑐 (u) = −2 β umax/𝜋 log cos(𝜋 u/(2 umax)) with the
actuation limit umax and the positive constant β. This barrier

Continuous Fitted Value Iteration

Figure 8. The learning curves averaged over 5 seeds for the 𝑛-step value function target. The shaded area displays the min/max range
between seeds. The step count is selected such that 𝜆𝑛 (cid:66) 10−4. Increasing the horizon of the value function target increases the
convergence rate to the optimal value function. For very long horizons the learning diverges as it over ﬁts to the current value function
approximation. Furthermore, the performance of the optimal policy also increases with roll out length.

Figure 9. The learning curves averaged over 5 seeds for the diﬀerent model architectures. The shaded area displays the min/max range
between seeds. All network architectures are capable of learning the value function and policy for most of the tasks. The locally quadratic
network architecture increases learning speed compared to the baselines. The structured architecture acts as an inductive bias that shapes
the exploration. The global maximum of the locally quadratic value function is guaranteed at xdes and hence the initial policy performs
hill-climbing towards this point.

shaped cost bounds the optimal actions. The corresponding
policy is shaped by ∇ ˜𝑔(w) = 2 umax/𝜋 tan−1 (w/β). For
the experiments, the reward parameters are

Pendulum: Qdiag = [01.0, 0.1] ,
Cartpole: Qdiag = [25.0, 1.0, 0.5, 0.1] ,
Furuta Pendulum: Qdiag = [01.0, 5.0, 0.1, 0.1] ,

𝛽 = 0.5
𝛽 = 0.1
𝛽 = 0.1

Evaluation The rewards are evaluated using 100 roll outs
in simulation and 15 roll outs on the physical system. If not
noted otherwise, each roll out lasts 15𝑠 and starts with the
pendulum downward . This duration is much longer than
the required time to swing up. The pendulum is considered
balancing, if the pendulum angle is below ±5◦ degree for
every sample of the last second.

Extended Experimental Results

Ablation Study - 𝑁-step Value Targets

The learning curves for the ablation study highlighting the
importance are shown in Figure 8. This ﬁgure contains in
contrast to Figure 7 also RTDP cFVI. When increasing 𝜆,
which implicitly increases the 𝑛-step horizon (Section 3.3),
the convergence rate to the optimal value function increases.
This increased learning speed is expected as Equation 7
shows that the convergence rate depends on 𝛾𝑛 with 𝛾 < 1.

While the learning speed measured in iterations increases
with 𝜆, the computational complexity also increases. Longer
horizons require to simulate 𝑛 sequential steps increasing the
required wall-clock time. Therefore, the computation time
increases exponentially with increasing 𝜆. For example the
forward roll out in every iteration of the pendulum increases
exponentially from 0.4s (𝜆 = 0.01) to 56.4s (𝜆 = 0.99).
For the Furuta pendulum and the cartpole extremely long
horizons of 100+ steps start to diverge as the value function
target over ﬁts to the untrained value function. For RTDP
cFVI, the horizons must smaller, i.e., 10 - 20 steps. For longer
horizons the predicted rollout overﬁts to the value function
outside of the current state distribution, which prevents
learning or leads to pre-mature convergence. This is very
surprising as even for the true model long time-horizons
can be counterproductive due to the local approximation of
the value function. Therefore, DP cFVI works bests with
𝜆 ∈ [0.85, 0.95] and RTDP cFVI with 𝜆 ∈ [0.45 − 0.55].

Ablation Study - Model Architecture

To evaluate the impact of the locally quadratic architecture
described by

𝑉 (x; 𝜓) = − (x − xdes)𝑇 L(x; 𝜓)L(x; 𝜓)𝑇 (x − xdes) ,

where L is a lower triangular matrix with positive diagonal,
we compare this architecture to a standard multi-layer percep-
tron with and without feature transformation. The learning

−40−200RewardDPcFVIPendulum−150−100−500RewardCartpole−400−2000RewardFurutaPendulum0255075100Episode−40−200RewardRTDPcFVI0255075100125150Episode−150−100−500Reward050100150200250Episode−400−2000Rewardn=2/λ=0.01n=4/λ=0.10n=7/λ=0.25n=13/λ=0.50n=17/λ=0.60n=23/λ=0.70n=35/λ=0.80n=66/λ=0.90n=122/λ=0.95n=459/λ=0.99−60−40−200RewardDPcFVIDPcFVIPendulum−150−100−500RewardCartpole−400−2000RewardFurutaPendulumQuadraticFeatureMLPQuadraticMLPFeatureMLPMLP0255075100Episode−60−40−200RewardRTDPcFVIRTDPcFVI0255075100125150Episode−150−100−500Reward050100150200250Episode−400−2000RewardContinuous Fitted Value Iteration

Figure 10. The learning curves averaged over 5 seeds with diﬀerent model ensemble sizes 𝑁. The shaded area displays the min/max range
between seeds. The performance of the optimal policy is not signiﬁcantly aﬀected by the model ensemble. For the cartpole and especially
the Furuta pendulum, the larger model ensembles stabilize the training and achieve faster learning and exhibit less variations between
seeds.

curves for the ablation study highlighting the importance of
the network architecture are shown in Figure 9. The reward
curves are averaged over 5 seeds and visualize the maxi-
mum range between seeds. For most systems all network
architecture are able to learn a good policy. The locally
quadratic value function is on average the best performing ar-
chitecture. The structure acts as a inductive bias that shapes
the exploration and leads to faster learning. The global
maximum is guaranteed to be at xdes as L(x; 𝜓)L(x; 𝜓)𝑇
is positive deﬁnite. Therefore, the initial policy directly
performs hill-climbing towards the balancing position. Only
for the cartpole the other network architectures fail. For this
system, these architectures learn a local optimal solution of
balancing the pendulum downwards. This local optima the
conservative solution as the cost associated with the cart
position is comparatively high to avoid the cart limits on
the physical system. Therefore, stabilizing the pendulum
downwards is better compared to swinging the pendulum up
and failing at the balancing. The locally quadratic network
with the feature transform, learns the optimal policy for the
cartpole. This architecture avoids the local solution as the
network structure guides the exploration to be optimistic and
the feature transform simpliﬁes the value function learning
to learn a successful balancing.

Ablation Study - Model Ensemble

The learning curves for diﬀerent model ensemble sizes
are shown in Figure 10. The model ensemble does not
signiﬁcantly aﬀect the performance of the ﬁnal policy but
reduces the variance in learning speed between seeds. The
variance between seeds also increases. The reduced variance
for the model ensembles is caused by the smoothing of the
network initialization. The mean across diﬀerent initial
weights lets the initialization be more conservative compared
to a single network. For the comparatively small value
function networks (i.e., 2-3 layers deep and 64 - 128 units
wide), we prefer the network ensembles as the computation
time does not increase when increasing the ensemble size.
If the individual networks are batched and evaluated on the
GPU the computation time does not increase. The network
ensembles could also be evaluated at 500Hz for the real-time
control experiments using an Intel i7 9900k.

020406080100120140Episode−40−200RewardPendulum020406080100120140Episode−150−100−500RewardCartpole050100150200250Episode−400−300−200−1000RewardFurutaPendulumNetworkEnsembleN=1NetworkEnsembleN=2NetworkEnsembleN=3NetworkEnsembleN=4NetworkEnsembleN=5