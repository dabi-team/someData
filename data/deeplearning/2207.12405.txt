2
2
0
2

l
u
J

5
2

]

R
C
.
s
c
[

1
v
5
0
4
2
1
.
7
0
2
2
:
v
i
X
r
a

Versatile Weight Attack via Flipping Limited Bits

Jiawang Bai, Baoyuan Wu, Zhifeng Li, and Shu-tao Xia

1

Abstract—To explore the vulnerability of deep neural networks (DNNs), many attack paradigms have been well studied, such as the
poisoning-based backdoor attack in the training stage and the adversarial attack in the inference stage. In this paper, we study a novel
attack paradigm, which modiﬁes model parameters in the deployment stage. Considering the effectiveness and stealthiness goals, we
provide a general formulation to perform the bit-ﬂip based weight attack, where the effectiveness term could be customized depending
on the attacker’s purpose. Furthermore, we present two cases of the general formulation with different malicious purposes, i.e., single
sample attack (SSA) and triggered samples attack (TSA). To this end, we formulate this problem as a mixed integer programming
(MIP) to jointly determine the state of the binary bits (0 or 1) in the memory and learn the sample modiﬁcation. Utilizing the latest
technique in integer programming, we equivalently reformulate this MIP problem as a continuous optimization problem, which can be
effectively and efﬁciently solved using the alternating direction method of multipliers (ADMM) method. Consequently, the ﬂipped critical
bits can be easily determined through optimization, rather than using a heuristic strategy. Extensive experiments demonstrate the
superiority of SSA and TSA in attacking DNNs.

Index Terms—Bit-ﬂip, weight attack, deep neural networks, vulnerability, mixed integer programming.

(cid:70)

1 INTRODUCTION

D EEP neural networks (DNNs) have achieved state-of-

the-art performance in many applications, including
computer vision [1], natural language processing [2], and
robotic manipulation [3]. However, many works [4], [5] have
revealed that DNNs are vulnerable to a range of attacks,
which has attracted great attention, especially for security-
critical applications (e.g., face recognition [6], medical diag-
nosis [7], and autonomous driving [8], [9]). For example,
backdoor attack [10], [11] manipulates the behavior of the
DNN model by mainly poisoning some training data in the
training stage; adversarial attack [12], [13] aims to fool the
DNN model by adding malicious imperceptible perturba-
tions onto the input in the inference stage.

Besides the above backdoor attack and adversarial at-
tack, weight attack [14], as a novel attack paradigm, has not
been well studied. It assumes that the attacker has full access
to the memory of a device, such that he/she can directly
change the parameters of a deployed model to achieve some
malicious purposes. For example, Rakin et al. [15] proposed
to crush a fully functional DNN model by converting it to
a random output generator. Since the weight attack neither
modiﬁes the training data nor controls the training process
and performs in the deployment stage, it is difﬁcult for both
the service provider and the user to realize the existence
of the attack. In practice, since the deployed DNN model
is stored as binary bits in the memory, the attacker can
modify the model parameters using some physical fault
injection techniques, such as Row Hammer Attack [16], [17]

•

Jiawang Bai and Shu-tao Xia are with the Tsinghua Shenzhen Interna-
tional Graduate School, Tsinghua University, Shenzhen 518057, China.
E-mail: {bjw19@mails.tsinghua.edu.cn, xiast@sz.tsinghua.edu.cn}
• Baoyuan Wu is with the School of Data Science, Chinese University
of Hong Kong, Shenzhen, and the Secure Computing Lab of Big Data,
Shenzhen Research Institute of Big Data, 518100, China.
E-mail: wubaoyuan@cuhk.edu.cn

• Zhifeng Li is with Tencent Data Platform, Shenzhen 518057, China.

E-mail: michaelzﬂi@tencent.com

• Corresponding authors: Baoyuan Wu and Shu-tao Xia

Fig. 1. Demonstration of our proposed attack against a deployed DNN
in the memory. By ﬂipping critical bits (marked in red), our method can
achieve some malicious purposes. For example, single sample attack
(SSA) can manipulate the behavior of the DNN on an attacked sample
without any sample modiﬁcation while making little difference to other
samples; triggered samples attack (TSA) can mislead the triggered sam-
ples into the target class while not signiﬁcantly reducing the prediction
accuracy of original samples.

and Laser Beam Attack [18]. These techniques can precisely
ﬂip any bit of the data in the memory. Some previous
works [15], [19], [20] have demonstrated that it is feasible to
change the model weights via ﬂipping weight bits to achieve
some malicious purposes. Their efforts partially focused
on identifying the critical bits in a large number of model
parameters. However, these methods mostly used some
heuristic strategies. For example, the attack in [15] combined
gradient ranking and progressive search to identify the
critical bits for ﬂipping.

Original Decision BoundaryClass 0Class 1TSASSA010110010100110001101001111101010010111000110101101001010100100110011000111010011001011001001110101110101100000110010100Attacker: identify and flip critical bits010110010100110001101001101101010010111000110101101001010110100110011000111011011001011001001110101110101100000110010100DNN in the memoryAn Attacked SampleTriggered Samples 
 
 
 
 
 
This work also focuses on the bit-level weight attack
against DNNs in the deployment stage, whereas with two
different goals, including effectiveness and stealthiness. The
effectiveness requires that the attacked DNN can meet the
attacker-speciﬁed malicious purpose, while the stealthiness
encourages that the attacked DNN behaves normally on
samples except the attacked sample(s). We formulate these
two goals as two terms in the objective function, respec-
tively, where the effectiveness loss can be customized de-
pending on the attacker’s purpose. This general formulation
results in a versatile weight attack. Speciﬁcally, we treat each
bit in the memory as a binary variable and the modiﬁcation
on the samples as a continuous operator, and our task is to
determine the state of each bit (i.e., 0 or 1) and the learn-
able parameter of the sample modiﬁcation. Accordingly, it
can be formulated as a mixed integer programming (MIP)
problem. To further improve the stealthiness, we also limit
the number of ﬂipped bits, which can be formulated as a
cardinality constraint.

Considering practical attack scenarios, we specify the
general formulation as two types of attack: single sample
attack (SSA) and triggered samples attack (TSA), by intro-
ducing different effectiveness goals. SSA aims at misclassi-
fying a speciﬁc sample to a predeﬁned target class without
any sample modiﬁcation, while TSA is to misclassify the
samples embedded with a designed trigger. Besides, both
attacks expect that the attacked DNN can meet the stealth-
iness goal to make the attack undetectable. Their goals
are demonstrated in Fig. 1. They are special cases of the
above general formulation and can be formulated as MIP
problems.

However, how to solve the MIP problem with a car-
dinality constraint is a challenging problem. Fortunately,
inspired by an advanced optimization method, the (cid:96)p-box
ADMM [21], this problem can be reformulated as a con-
tinuous optimization problem, which can further be efﬁ-
ciently and effectively solved by the alternating direction
method of multipliers (ADMM) [22], [23]. Consequently, the
ﬂipped bits can be determined through optimization rather
than the heuristic strategy, which makes our attack more
effective. Note that we also conduct experiments against
the quantized DNN models, following the setting in some
related works [15], [19]. Extensive experiments demonstrate
the superiority of the proposed method over several ex-
isting weight attacks 1. For example, in attacking an 8-
bit quantized ResNet-18 model on ImageNet, on average,
SSA achieves a 100% attack success rate with 7.37 bit-
ﬂips and 0.09% accuracy degradation of the rest unspeciﬁc
inputs, and TSA achieves 95.63% attack success rate with
3.4 bit-ﬂips and 0.05% accuracy degradation of the original
samples. Moreover, we also demonstrate that the proposed
method is also more resistant to existing defense methods.

This work builds upon the preliminary conference paper
[24], which primarily focuses on attacking a speciﬁed sam-
ple without modifying the attacked sample. The new major
contributions are summarized as follows.

1) We expand the attack in the conference paper to a
general formulation, which can be ﬂexibly speciﬁed to

1. Our

code

is

available

at: https://github.com/jiawangbai/

Versatile-Weight-Attack

TABLE 1
Comparison between three attack paradigms in terms of attack stage,
modiﬁed object, and adopted technique.

2

Adversarial
Attack

Backdoor
Attack

Weight
Attack

Stage

Inference

Training

Deployment

Modiﬁed
Object

Technique

Test Sample

Imperceptible
Noise

Training
Data

Trigger

Model
Parameters

Bit-ﬂip

apply in different attack scenarios. This general for-
mulation consists of identifying the ﬂipped bits and
learning the modiﬁcation on the samples, resulting in
a MIP problem.

2) Based on the general formulation, we propose a new
type of attack, i.e., TSA. Compared to SSA, TSA aims
at misclassifying all samples with the learned trigger,
which may be suitable to the scenarios that require
various malicious inputs.

3) Utilizing the general framework for solving this MIP
problem, we conduct more experiments and ablation
studies to verify the proposed attacks. In particular, in
the new attack scenario, TSA reduces the number of
bit-ﬂips greatly compared to state-of-the-art methods.

2 RELATED WORK

Our work is related to various attack paradigms that explore
the vulnerability of DNNs, including adversarial attack,
backdoor attack, and weight attack. We compare these three
attack paradigms from three aspects in Table 1 and discuss
them brieﬂy in what follows.

2.1 Adversarial Attack

Adversarial attack modiﬁes samples in the inference stage
by adding small perturbations that remain imperceptible to
the human vision system [25]. Since adversarial attack only
modiﬁes inputs while keeping the model unchanged, it has
no effect on the benign samples. Besides the basic white-box
attack, the black-box attack is regarded as a greater threat,
including transfer-based methods [26], [27] and query-based
methods [28], [29]. Many works [13], [30] have also shown
the existence of the image-agnostic universal perturbations.
Inspired by its success in the classiﬁcation, it also has been
extended to other tasks, including image captioning [31],
retrieval [32], person re-identiﬁcation [33], etc. To mitigate
the potential threat of the adversarial attack, recent studies
have demonstrated many defense methods, including the
preprocessing-based defense [34], the detection-based de-
fense [35], and the adversarial learning-based defense [36].

2.2 Backdoor Attack

Backdoor attack happens in the training stage and requires
that the attacker can tamper with the training data even the
training process [37], [38]. Through poisoning some training
samples with a trigger, the attacker can control the behavior
of the attacked DNN in the inference stage. For example,
images with reﬂections are misclassiﬁed into a target class,

while benign images are classiﬁed normally [39]. Recent
clean-label attacks [40], [41] improve previous methods by
only poisoning training examples while leaving their la-
bels unchanged. Besides, instead of using the ﬁxed trigger,
dynamic backdoor attacks [42], [43] generate the trigger
varying from input to input, which makes the attacked mod-
els more stealthy. Many defense methods against backdoor
attack have been proposed, such as the preprocessing-based
defense [44], the model reconstruction-based defense [45],
and the trigger synthesis-based defense [46].

2.3 Weight Attack

Weight attack [47], [48], [49] modiﬁes model parameters in
the deployment stage, which is the studied paradigm in this
work. It received extensive attention, since the robust model
weight is the cornerstone of employing DNNs in many
security-critical applications [50]. Firstly, two schemes are
proposed in [47] to modify model parameters for misclassi-
ﬁcation without and with considering stealthiness, which
is dubbed single bias attack (SBA) and gradient descent
attack (GDA) respectively. After that, Trojan attack [48] and
model-reuse attack [51] were proposed, which inject mali-
cious behavior to the DNN by retraining the model. These
methods require to change lots of parameters. Recently, fault
sneaking attack (FSA) [52] was proposed, which aims to
misclassify certain samples into a target class by modify-
ing the DNN parameters with two constraints, including
maintaining the classiﬁcation accuracy of other samples and
minimizing parameter modiﬁcations.

Bit-Flip based Attack. Recently, some physical fault
injection techniques [16], [17], [18] were proposed, which
can be adopted to precisely ﬂip any bit in the memory. Those
techniques promote researchers to study how to modify
model parameters at the bit-level. As a branch of weight
attack, the bit-ﬂip based attack was ﬁrstly explored in [15]. It
proposed an untargeted attack that can convert the attacked
DNN to a random output generator with several bit-ﬂips.
Besides, the targeted bit Trojan (TBT) [19] injects the fault
into DNNs by ﬂipping some critical bits. Speciﬁcally, the at-
tacker ﬂips the identiﬁed bits to force the network to classify
all samples embedded with a trigger to a certain target class,
while the network operates with normal inference accuracy
with benign samples. Most recently, the targeted bit-ﬂip
attack (T-BFA) [20] achieves malicious purposes without
modifying samples. Speciﬁcally, T-BFA can mislead samples
from single source class or all classes to a target class by
ﬂipping the identiﬁed weight bits. It is worth noting that
the above bit-ﬂip based attacks leverage heuristic strategies
to identify critical weight bits. How to ﬁnd critical bits for
the bit-ﬂip based attack method is still an important open
question.

To mitigate the affect of the weight attack, many works
have investigated the defense mechanisms. Previous studies
[20], [53] observed that increasing the network capacity can
improve the robustness against the bit-ﬂip based attack.
Moreover, the training strategies in [53], [54], [55], [56] can
improve the robustness of the model parameters. There
exist other works considering defense in the inference stage,
such as weight reconstruction-based defense [57], detection-
based defense [58], [59], and Error Correction Codes (ECC)-

based defense [60], [61]. For a more comprehensive compar-
ison, we also evaluate the resistance of attack methods to
defense strategies in [20], [53].

3

3 METHODOLOGY

3.1 Preliminaries

Storage and Calculation of Quantized DNNs. Currently,
it is a widely-used technique to quantize DNNs before de-
ploying on devices for efﬁciency and reducing storage size.
For each weight in l-th layer of a Q-bit quantized DNN, it
will be represented and then stored as the signed integer in
two’s complement representation (v = [vQ; vQ−1; ...; v1] ∈
{0, 1}Q) in the memory. Attacker can modify the weights
of DNNs through ﬂipping the stored binary bits. In this
work, we adopt the layer-wise uniform weight quantization
scheme similar to Tensor-RT [62]. Accordingly, each binary
vector v can be converted to a real number by a function
h(·), as follow:

h(v) = (−2Q−1 · vQ +

Q−1
(cid:88)

i=1

2i−1 · vi) · ∆l,

(1)

where l indicates which layer the weight is from, ∆l > 0 is
a known and stored constant which represents the step size
of the l-th layer weight quantizer.

Notations. We denote a Q-bit quantized DNN-based
classiﬁcation model as f : X → Y, where X ∈ Rd being the
input space and Y ∈ {1, 2, ..., K} being the K-class output
space. Assuming that the last layer of this DNN model
is a fully-connected layer with B ∈ {0, 1}K×C×Q being
the quantized weights, where C is the dimension of last
layer’s input. Let Bi,j ∈ {0, 1}Q be the two’s complement
representation of a single weight and Bi ∈ {0, 1}C×Q
denotes all the binary weights connected to the i-th output
neuron. Given a test sample xi with the ground-truth label
yi, f (xi; Θ, B) ∈ [0, 1]K is the output probability vector
and g(xi; Θ) ∈ RC is the input of the last layer, where Θ
denotes the model parameters except the last layer.

Attack Scenario. In this paper, we focus on the white-
box bit-ﬂip based weight attack, which was ﬁrst introduced
in [15]. Speciﬁcally, we assume that the attacker has full
knowledge of the model (including it’s architecture, param-
eters, and parameters’ location in the memory), and can
precisely ﬂip any bit in the memory. Besides, we also assume
that attackers can have access to a small portion of benign
samples, but they can not tamper the training process and
the training data. We consider two basic goals for our attack:
effectiveness and stealthiness. The effectiveness corresponds
to a higher attack success rate, while stealthiness requires
reducing the effect on samples except the attacked sam-
ple(s). We consider the targeted attack with the purpose of
misclassifying sample(s) to a target class t, since the targeted
attack is more challenging than the untargeted attack.

3.2 General Formulation

In this section, we give the general formulation of the
proposed weight attack. As mentioned in Section 3.1, we
assume that the attacker can get access to two sample
sets to perform attack: D1 = {(xi, yi)}N1
i=1 contributes to

achieve the attack effectiveness and D2 = {(xi, yi)}N2
i=1
helps to keep the attack stealthiness. Let the vectorized
binary parameters b ∈ {0, 1}V being a part of the attacked
model (e.g., the parameters of the last layer B) which the
attacker intends to modify, and ˆb ∈ {0, 1}V corresponds to
the modiﬁed version of b. The dimension V will be speciﬁed
later. When performing attack, we only modify b and the
remaining parameters of the attacked model are ﬁxed. Note
that we will omit the ﬁxed parameters of the attacked model
for clarity in the below formulas. Let φ with the learnable
parameter q represent the modiﬁcation operator on D1,
which is supposed to be differentiable w.r.t. q. The overall
objective function is below:

λ1L1(φ(D1; q), t; ˆb) + λ2L2(D2; ˆb),

min
ˆb,q

s.t. ˆb ∈ {0, 1}V , dH (b, ˆb) ≤ k,

(2)

where dH (·, ·) denotes the Hamming distance, and λ1, λ2 >
0 are the trade-off parameters. The loss L1 is used to
ensure the attack effectiveness, which can be customized
according to the attacker’s purpose. By minimizing L1, the
attacked model can misclassify the attacked sample(s) (e.g.,
a speciﬁed sample or samples with a trigger) into a target
class t, as shown in Fig. 1. We will detail the loss L1 later for
two weight attack versions, respectively.

The attack may be easily detectable using only L1, since
the attacked model behaves abnormally even on samples
except the attacked sample(s). Therefore, we utilize the
loss L2 to ensure the attack stealthiness. The loss L2 is
formulated as follows,
L2(D2; ˆb) =

(cid:96)(f (xi; ˆb), yi),

(cid:88)

(xi,yi)∈D2
where fj(xi; ˆb) indicates the posterior probability of xi
w.r.t. class j. (cid:96)(·, ·) is speciﬁed by the cross entropy loss.

Besides, to better meet our goal, a straightforward ad-
ditional approach is reducing the magnitude of the modi-
ﬁcation. In this paper, we constrain the number of bit-ﬂips
less than k. Physical bit ﬂipping techniques can be time-
consuming as discussed in [52], [63]. Moreover, such tech-
niques lead to abnormal behaviors in the attacked system
(e.g., suspicious cache activity of processes), which may be
detected by some physical detection-based defenses [64]. As
such, limiting the number of bit-ﬂips is critical to make the
attack more efﬁcient and practical.

3.3 Single Sample Attack

In this section, based on the above general formulation, we
introduce our ﬁrst type of attack: single sample attack (SSA).
Attacker’s Goals. For SSA, the goals of effectiveness and
stealthiness can be described as follows. The effectiveness
requires that the attacked model can misclassify a speciﬁc
sample to a predeﬁned target class without any sample
modiﬁcation, and the stealthiness requires that the predic-
tion accuracy of other samples will not be signiﬁcantly re-
duced. SSA aims at misclassifying a speciﬁc sample, whose
goal may correspond to the attacker’s requirement in some
scenarios. For example, the attacker wants to manipulate
the behavior of the intrusion detection system on a speciﬁc
input. Moreover, as the minimal malicious requirement,

(3)

4

misclassifying a single sample may result in a low number
of bit-ﬂips, which will be demonstrated in our experiments.
Loss for Ensuring Effectiveness. Recall that SSA aims
at forcing a speciﬁc image to be classiﬁed as the target class
by modifying the model parameters at the bit-level. To this
end, the most straightforward way is maximizing the logit
of the target class while minimizing that of the source class.
For a sample x with the ground-truth label s, the logit of
a class can be directly determined by the input of the last
layer g(x; Θ) and weights connected to the node of that
class. Accordingly, we can modify weights only connected to
the source and target class to fulﬁll our purpose. Moreover,
for SSA, we specify the set D1 as {(x, s)} and use x and
s straightly in the below formulations for clarity. We have
no modiﬁcation on the attacked sample, i.e., φ in Eq. (2) is
speciﬁed as φ({(x, s)}, q) = {(x, s)} and will be omitted
below. The loss for ensuring effectiveness is as follows:
(x, s, t; ˆBs, ˆBt) = max (cid:0)τ − p(x; Θ, ˆBt) + δ, 0(cid:1)
+ max (cid:0)p(x; Θ, ˆBs) − τ + δ, 0(cid:1),

LSSA
1

(4)

where p(x; Θ, ˆBi) = [h( ˆBi,1); h( ˆBi,2); ...; h( ˆBi,C)](cid:62)g(x; Θ)
denotes the logit of class i (i = s or i = t), h(·) is the function
p(x; Θ, Bi), and δ ∈ R
max
deﬁned in Eq. (1), τ =
i∈{0,...,K}\{s}
indicates a slack variable, which will be speciﬁed in later
experiments. The ﬁrst term of LSSA
aims at increasing the
logit of the target class, while the second term is to decrease
the logit of the source class. The loss LSSA
is 0 only when
the output on target class is more than τ + δ and the output
on source class is less than τ − δ. That is, the prediction on
x of the target model is the predeﬁned target class t.

1

1

Loss for Ensuring Stealthiness. For SSA, we use L2
to ensure the stealthiness. L2 deﬁned by Eq. (3) can be
rewritten for SSA using ˆBs and ˆBt, as follows:

L2(D2; ˆBs, ˆBt) =

(cid:88)

(cid:96)(f (xi; ˆBs, ˆBt), yi).

(xi,yi)∈D2

(5)

Overall Objective for SSA. In conclusion, the ﬁnal

objective function for SSA is as follows:

min
ˆBs, ˆBt

λ1LSSA
1

(x, s, t; ˆBs, ˆBt) + λ2L2(D2; ˆBs, ˆBt),

s.t. ˆBs ∈ {0, 1}C×Q, ˆBt ∈ {0, 1}C×Q,
dH (Bs, ˆBs) + dH (Bt, ˆBt) ≤ k.

(6)

Note that
the above objective function is one of spe-
cial forms of Eq. (2), where φ is the identity function.
ˆBs, ˆBt ∈ {0, 1}C×Q are two variables we want to optimize,
corresponding to the weights of the fully-connected layer
w.r.t. class s and t, respectively,
in the attacked DNN
model. B ∈ {0, 1}K×C×Q denotes the weights of the fully-
connected layer of the original DNN model. Therefore, b in
Section 3.2 corresponds to the reshaped and concatenated
Bs and Bt and ˆb could be the reshaped and concatenated
ˆBs and ˆBt, respectively. The size of b and ˆb is 2CQ (i.e.,
V = 2CQ).

3.4 Triggered Samples Attack

In this section, we present an attack considering embedding
the inputs with a learned trigger, namely, triggered samples
attack (TSA).

Attacker’s Goals. For TSA, the effectiveness goal means
that the attacked model can classify the samples embedded
with a designed trigger (e.g., a square patch) to a target
class, and the stealthiness goal expects that the attacked
model performs accurate classiﬁcation on most inputs when
the trigger is removed. Compared to SSA, TSA can mislead
the attacked model on any samples with the trigger, which
may be suitable to the scenarios that require various ma-
licious inputs. For example, in the context of autonomous
driving, the attacker wishes that autonomous cars recognize
all road signs with the trigger as the stop sign. Besides, the
normal behavior of the attacked model when the trigger is
absent makes the attack undetectable.

Loss for Ensuring Effectiveness. Since TSA utilizes the
trigger to perform attack, we can suppose that two sample
sets D1 and D2 share the same data, i.e., D = D1 = D2
and N = N1 = N2. We use D for clarity below. To achieve
the effectiveness goal, we present how to embed the inputs
with a trigger ﬁrstly. We suppose that the trigger is a patch
and its area is given by an attacker-speciﬁed mask m ∈
[0, 1]d. We deﬁne the function φ to generate the triggered
samples, as follows.

φ(D; q) = {((1 − m) ⊗ xi + m ⊗ q, yi) | (xi, yi) ∈ D},

(7)
where ⊗ indicates the element-wise product and q ∈ Rd
is the trigger. Because the modiﬁcation on the sample is
differentiable w.r.t. q, the trigger can be optimized using the
gradient method to achieve a more powerful attack. Note
that TSA aims at misclassifying the samples with the trigger
from all classes, which is different from SSA. Therefore, we
optimize the parameters of the last layer B and ˆB is the
modiﬁed B. Based on the above modiﬁcation, we deﬁne the
following objective to achieve the targeted misclassiﬁcation
on the triggered sample.
LT SA
1
(cid:88)

(φ(D; q), t; ˆB)

(cid:96)(f ((1 − m) ⊗ xi + m ⊗ q; ˆB), t),

(8)

=

(xi,yi)∈D

where (cid:96)(·) is the cross entropy loss. We can update the
trigger q and ˆB alternatively to ﬁnd the trigger and the
bit-ﬂips to minimize the above loss.

Loss for Ensuring Stealthiness. Since we modify B for

TSA, L2 deﬁned by Eq. (3) can be rewritten as below.

3.5 An Effective Optimization Method

5

To solve the challenging MIP problem (2), we adopt the
generic solver for integer programming, dubbed (cid:96)p-Box
ADMM [21]. The solver presents its superior performance in
many tasks, e.g., model pruning [65], clustering [66], MAP
inference [67], adversarial attack [68], etc. It proposed to
replace the binary constraint equivalently by the intersection
of two continuous constraints, as follows

ˆb ∈ {0, 1}V ⇔ ˆb ∈ (Sb ∩ Sp),

(11)

2 ||2

||ˆb − 1

2 = V

where Sb = [0, 1]V indicates the box constraint, and Sp =
{ˆb :
4 } denotes the (cid:96)2-sphere constraint.
Utilizing (11), Problem (2) can be equivalently reformu-
lated. Besides, for binary vector b and ˆb, there exists a
nice relationship between Hamming distance and Euclidean
distance: dH (b, ˆb) = ||b − ˆb||2
2. The reformulated objective is
as follows:

min
ˆb,q,u1∈Sb,u2∈Sp,u3∈R+

λ1L1(φ(D1; q); ˆb) + λ2L2(D2; ˆb),

s.t. ˆb = u1,ˆb = u2, ||b − ˆb||2

2 − k + u3 = 0,

(12)

where two extra variables u1, u2 ∈ RV are introduced to
split the constraints w.r.t. ˆb. Besides, the non-negative slack
variable u3 ∈ R+ is used to transform ||b − ˆb||2
2 − k ≤ 0
in (2) into ||b − ˆb||2
2 − k + u3 = 0. The above constrained
optimization problem can be efﬁciently solved by the alter-
nating direction method of multipliers (ADMM) [69].

Following the standard procedure of ADMM, we ﬁrstly
present the augmented Lagrangian function of the above
problem, as follows:

L(ˆb, q, u1, u2, u3, z1, z2, z3)

=λ1L1(φ(D1; q); ˆb) + λ2L2(D2; ˆb)
+z(cid:62)
+c1(u1) + c2(u2) + c3(u3)

1 (ˆb − u1) + z(cid:62)

2 (ˆb − u2) + z3(||b − ˆb||2

2 − k + u3)

L2(D; ˆB) =

(cid:88)

(cid:96)(f (xi; ˆB), yi).

(xi,yi)∈D

(9)

+

||ˆb − u1||2

2 +

||ˆb − u2||2

2 +

ρ2
2

ρ1
2

ρ3
2

(||b − ˆb||2

2 − k + u3)2,
(13)

Overall Objective for TSA. We summarize the objective

function of TSA as below.

λ1LT SA
1

(φ(D; q), t; ˆB) + λ2L2(D; ˆB)

min
ˆB,q
s.t. ˆB ∈ {0, 1}K×C×Q, dH (B, ˆB) ≤ k.

(10)

Note that we specify q as the learnable trigger pattern
and the modiﬁcation operator φ as embedding this trigger
(see Eq. (7)). Since the weights of the fully-connected layer
ˆB ∈ {0, 1}K×C×Q is the variable we want to optimize, b in
Section 3.2 corresponds to the reshaped B and ˆb could be the
reshaped ˆB. The size of b and ˆb is KCQ (i.e., V = KCQ).

where z1, z2 ∈ RV and z3 ∈ R are dual variables, and
ρ1, ρ2, ρ3 > 0 are penalty factors, which will be spec-
iﬁed later. c1(u1) = I{u1∈Sb}, c2(u2) = I{u2∈Sp}, and
c3(u3) = I{u3∈R+} capture the constraints Sb, Sp and R+,
respectively. The indicator function I{a} = 0 if a is true;
otherwise, I{a} = +∞. Based on the augmented Lagrangian
function, the primary and dual variables are updated itera-
tively, with r indicating the iteration index.
1, zr
).
3), (u1, u2, u3) are independent, and

Given (ˆbr, qr, zr
2, zr

3), update (ur+1

Given (ˆbr, q, zr

, ur+1
2

, ur+1
3

1, zr

2, zr

1

Algorithm 1 Continuous optimization for the proposed bit-
ﬂip based weight attack
Input: The original quantized DNN model f with weights
Θ, B; speciﬁed L1, L2, and φ; target class t; auxiliary sample
set D1 and D2; hyper-parameters λ1, λ2, and k.
Output: ˆb.
1: Initial u0
1, z0
1, u0
2: while not converged do
Update ur+1
3:
Update ˆbr+1 as Eq. (15);
Update ˆqr+1 as Eq. (16);
Update zr+1
1
r ← r + 1.

3, ˆb0, q0 and let r ← 0;

and ur+1

and zr+1

as Eq. (14);

as Eq. (17);

, ur+1
2

, zr+1
2

3, z0

2, u0

2, z0

4:
5:
6:
7:
8: end while

3

3

1

they can be optimized in parallel, as follows:

2 ||ˆbr − u1||2

2

2 ||ˆbr − u2||2

2






ur+1

1 = arg min
u1∈Sb

1)(cid:62)(ˆbr − u1) + ρ1

(zr
= PSb (ˆbr + zr
),
1
ρ1
2)(cid:62)(ˆbr − u2) + ρ2
(zr

2 = arg min
u2∈Sp

ur+1

),

ur+1
3 = arg min
u3∈R+

= PSp (ˆbr + zr
2
ρ2
3(||b − ˆbr||2
zr
2 (||b − ˆbr||2

+ ρ3

= PR+(−||b − ˆbr||2

2 − k + u3)

2 − k + u3)2
),

2 + k − zr

3
ρ3

Given (ˆbr+1, qr+1, ur+1
, zr+1
3

), update (zr+1
,
). The dual variables are updated by the gradient

, ur+1
2

, ur+1
3

1

1

zr+1
2
ascent method, as follows

6






zr+1
1 = zr
zr+1
2 = zr
zr+1
3 = zr

1 + ρ1(ˆbr+1 − ur+1
2 + ρ2(ˆbr+1 − ur+1
3 + ρ3(||b − ˆbr+1||2

),
),
2 − k + ur+1

1

2

3

(17)

).

, ur+1
3

, ur+1
2

Remarks. 1) Note that

since the dual variables
(ur+1
) are updated in parallel, and the primal
1
variables (ˆb, q) are also updated in parallel, the above algo-
rithm is a two-block ADMM algorithm. We summarize this
algorithm in Algorithm 1. 2) Except for the update of ˆbr+1
and qr+1, other updates are very simple and efﬁcient. The
computational cost of the whole algorithm will be analyzed
in Section 4.4.2. 3) Due to the inexact solution to ˆbr+1
and qr+1 using gradient descent, the theoretical conver-
gence of the whole ADMM algorithm cannot be guaranteed.
However, as demonstrated in many previous works [69],
[70], [71], the inexact two-block ADMM often shows good
practical convergence, which is also the case in our later
experiments. Besides, the numerical convergence analysis
is presented in Section 4.4.3. 4) The proper adjustment
of (ρ1, ρ2, ρ3) could accelerate the practical convergence,
which will be speciﬁed later.

4 EXPERIMENTS

(14)

¯a

√

n
2

where PSb (a) = min((1, max(0, a)) with a ∈ Rn is the
||a|| + 1
projection onto the box constraint Sb; PSp (a) =
2
with ¯a = a − 1
2 indicates the projection onto the (cid:96)2-sphere
constraint Sp [21]; PR+(a) = max(0, a) with a ∈ R indicates
the projection onto R+.
,ur+1
2

3), update ˆbr+1 and qr.
,zr
Although there is no closed-form solution to ˆbr+1, it can
be easily updated by the gradient descent method, as
L1(φ(D1; q), t; ˆb) and L2(D2; ˆb) are differentiable w.r.t. ˆb,
as follows

Given (ur+1

,ur+1
3

1,zr

2,zr

1

1

, zr

1, zr

2, zr
3)

, ur+1
3

∂L(ˆb, qr, ur+1

, ur+1
2
∂ˆb

ˆbr+1 ← ˆbr − η ·

(cid:12)
(cid:12)
(cid:12)ˆb=ˆbr
(15)
where η > 0 denotes the step size. Note that we can run
multiple steps of gradient descent in the above update.
Both the number of steps and η will be speciﬁed in later
experiments. Besides, the detailed derivation of ∂L/∂ˆb for
SSA and TSA can be found in Appendix.

,

As mentioned in Section 3.2, we suppose that φ is differ-
entiable w.r.t. q and thus L1(φ(D1; q); ˆb) is differentiable
w.r.t. q. We can update q using the gradient descent method
as follows

1

, zr

1, zr

, ur+1
3

∂L(ˆbr, q, ur+1

, ur+1
2
∂q

qr+1 ← qr −ζ ·

(cid:12)
(cid:12)
(cid:12)q=qr
(16)
where ζ > 0 denotes the step size. The update of q is kept
pace with ˆb, i.e., we update q for one step for each update
of ˆb. Note that the update of q is kept pace with ˆb, using
same backward pass.

2, zr
3)

,

4.1 Evaluation Setup

4.1.1 Datasets

We conduct experiments on CIFAR-10 [72] and ImageNet
[73]. CIFAR-10 consists of 50K training samples and 10K
testing samples with 10 classes. ImageNet contains 1.2M
samples from the training set and 50K samples from the
validation set, categorized into 1,000 classes. For the meth-
ods in Section 4.2 , we randomly select 1,000 images from
each dataset as the evaluation set. Speciﬁcally, for each of the
10 classes in CIFAR-10, we perform attacks on the 100 ran-
domly selected validation images from the other 9 classes.
For ImageNet, we randomly choose 50 target classes. For
each target class, we perform attacks on 20 images randomly
selected from the rest classes in the validation set. For the
methods in Section 4.3, we select all 10 classes for CIFAR-
10 and randomly select 5 classes for ImageNet as the target
classes. For each target class, we use all testing samples (10K
images for CIFAR-10 and 50K images for ImageNet) with
the generated trigger to evaluate the attack performance.

Besides, for all methods in Section 4.2 and 4.3 except
GDA which does not employ auxiliary samples, we provide
128 and 512 auxiliary samples on CIFAR-10 and ImageNet,
respectively, which corresponds to the size of D2 (N2) for
SSA and the size of D (N ) for TSA.

4.1.2 Target Models

According to the setting in [19], [20], we adopt two popular
network architectures: ResNet [1] and VGG [74] for evalu-
ation. On CIFAR-10, we perform experiments on ResNet-20

7

TABLE 2
Results of ﬁve attack methods across different bit-widths and architectures on CIFAR-10 and ImageNet (bold: the best; underline: the second
best). The mean and standard deviation of PA-ACC and Nﬂip are calculated by attacking the 1,000 images. Our method is denoted as SSA.

Dataset Method

Target
Model

FT
T-BFA
FSA
GDA
SSA
FT
T-BFA
FSA
GDA
SSA

FT
T-BFA
FSA
GDA
SSA
FT
T-BFA
FSA
GDA
SSA

ResNet 8-bit

ACC: 92.16%

ResNet 4-bit

ACC: 91.90%

ResNet 8-bit

ACC: 69.50%

ResNet 4-bit

ACC: 66.77%

0
1
-
R
A
F
I
C

t
e
N
e
g
a
m

I

PA-ACC
(%)

85.01±2.90
87.56±2.22
88.38±2.28
86.73±3.50
88.20±2.64
84.37±2.94
86.46±2.80
87.73±2.36
86.25±3.59
87.82±2.60

59.33±0.93
68.71±0.36
69.27±0.15
69.26±0.22
69.41±0.08
15.65±4.52
65.86±0.42
66.44±0.21
66.54±0.22
66.69±0.07

ASR
(%)

100.0
98.7
98.9
99.8
100.0
100.0
97.9
98.4
99.8
100.0

100.0
79.3
99.7
100.0
100.0
100.0
80.4
99.9
100.0
100.0

Nﬂip

1507.51±86.54
9.91±2.33
185.51±54.93
26.83±12.50
5.57±1.58
392.48±47.26
8.80±2.01
76.83±25.27
14.08±7.94
5.25±1.09

277424.29±12136.34
24.57±20.03
441.21±119.45
18.54±6.14
7.37±2.18
135854.50±21399.94
24.79±19.02
157.53±33.66
11.45±3.82
7.96±2.50

Target
Model

VGG 8-bit

ACC: 93.20%

VGG 4-bit

ACC: 92.61%

VGG 8-bit

ACC: 73.31%

VGG 4-bit

ACC: 71.76%

PA-ACC
(%)

84.31±3.10
89.83±3.92
88.80±2.86
85.51±2.88
86.06±3.17
83.31±3.76
88.74±4.52
87.58±3.06
85.08±2.82
85.91±3.29

62.08±2.33
73.09±0.12
73.28±0.03
73.29±0.02
73.28±0.03
17.76±1.71
71.49±0.15
71.69±0.09
71.73±0.03
71.73±0.03

ASR
(%)

98.7
96.7
96.8
100.0
100.0
94.5
96.2
97.5
100.0
100.0

100.0
84.5
100.0
100.0
100.0
100.0
84.3
100.0
100.0
100.0

Nﬂip

11298.74±830.36
14.53±3.74
253.92±122.06
21.54±6.79
7.40±2.72
2270.52±324.69
11.23±2.36
75.03±29.75
10.31±3.77
6.26±2.37

1729685.22±137539.54
363.78±153.28
1030.03±260.30
197.05±49.85
69.89±18.42
1900751.70±37329.44
350.33±158.57
441.32±111.26
107.18±28.70
69.72±18.84

and VGG-16. On ImageNet, we use the pre-trained ResNet-
182 and VGG-163 network. We quantize all networks to
the 4-bit and 8-bit quantization level using the layer-wise
uniform weight quantization scheme, which is similar to the
one involved in the Tensor-RT solution [62].

4.1.3 Evaluation Metrics

We adopt three metrics to evaluate the attack performance,
i.e., the post attack accuracy (PA-ACC), the attack success
rate (ASR), and the number of bit-ﬂips (Nﬂip). PA-ACC
denotes the post attack accuracy on the original validation
set. ASR is deﬁned as the ratio of attacked samples that are
successfully attacked into the target class. To be speciﬁc, we
calculate ASR using all 1,000 attacked samples for the meth-
ods in Section 4.2 and all testing samples with the trigger
for the methods in Section 4.3. Therefore, we can calculate
an ASR after 1,000 attacks for the methods in Section 4.2
and obtain an ASR after an attack for the methods in Section
4.3. Nﬂip is the number of bit-ﬂips required for an attack. A
better attack performance corresponds to a higher PA-ACC
and ASR, while a lower Nﬂip. Besides, we also show the
accuracy of the original model, denoted as ACC.

4.1.4 Defense Methods

Besides attacking the standard training models, we also test
the resistance of all attacks to two defense methods: piece-
wise clustering [53] and larger model capacity [20], [53].

He et al. [53] proposed a novel training technique,
called piece-wise clustering, to enhance the network ro-
bustness against the bit-ﬂip based attack. Such a training
technique introduces an additional weight penalty to the

2. Downloaded
resnet18-5c106cde.pth
3. Downloaded

vgg16 bn-6c64b313.pth

from https://download.pytorch.org/models/

from https://download.pytorch.org/models/

inference loss, which has the effect of eliminating close-
to-zero weights [53]. We test the resistance of all attack
methods to the piece-wise clustering. We conduct experi-
ments with the 8-bit quantized ResNet on CIFAR-10 and
ImageNet. Following the ideal conﬁguration in [53], the
clustering coefﬁcient, which is a hyper-parameter of piece-
wise clustering, is set to 0.001 in our evaluation.

Previous studies [20], [53] observed that increasing the
network capacity can improve the robustness against the
bit-ﬂip based attack. Accordingly, we evaluate all attack
methods against the models with a larger capacity using
the 8-bit quantized ResNet on both datasets. Similar to the
strategy in [53], we increase the model capacity by varying
the network width (i.e., 2× width in our experiments).

We conduct experiments with the 8-bit quantized ResNet
on CIFAR-10 and ImageNet to evaluate the attack per-
formance of all attack methods against these two defense
methods. Besides the three metrics in Section 4.1.3, we
also present the number of increased Nﬂip compared to
the model without defense (i.e., results in Table 2 and 4),
denoted as ∆Nﬂip.

4.2 Results of SSA

4.2.1 Baseline Methods

We compare our SSA with GDA [47], FSA [52], and T-
BFA [20]. Since GDA [47] and FSA [52] are originally de-
signed for attacking the full-precision network, we adapt
these two methods to attack the quantized network by
applying quantization-aware training [75]. We adopt the
(cid:96)0-norm for FSA [47] and modiﬁcation compression for
GDA [52] to reduce the number of the modiﬁed parameters.
Among three types of T-BFA [20], we compare to the most
comparable method: the 1-to-1 stealthy attack scheme. The
purpose of this attack scheme is to misclassify samples of
a single source class into the target class while maintaining

TABLE 3
Results of all attack methods against the models with defense on CIFAR-10 and ImageNet (bold: the best; underline: the second best). The mean
and standard deviation of PA-ACC and Nﬂip are calculated by attacking the 1,000 images. Our method is denoted as SSA. ∆Nﬂip denotes the
increased Nﬂip compared to the corresponding result in Table 2.

8

Defense

Dataset

Method

l

g
n
i
r
e
t
s
u
C
e
s
i
w
-
e
c
e
i
P

y
t
i
c
a
p
a
C

l
e
d
o
M

r
e
g
r
a
L

CIFAR-10

ImageNet

CIFAR-10

ImageNet

FT
T-BFA
FSA
GDA
SSA
FT
T-BFA
FSA
GDA
SSA

FT
T-BFA
FSA
GDA
SSA
FT
T-BFA
FSA
GDA
SSA

ACC
(%)

91.01

63.62

94.29

71.35

PA-ACC
(%)

84.06±3.56
85.82±1.89
86.61±2.51
84.12±4.77
87.30±2.74
43.44±2.07
62.82±0.27
63.26±0.21
63.14±0.48
63.52±0.14

86.46±2.84
91.16±1.42
90.70±2.37
89.83±3.02
90.96±2.63
63.51±1.29
70.84±0.30
71.30±0.04
71.30±0.05
71.30±0.04

ASR
(%)

99.5
98.6
98.6
100.0
100.0
92.2
90.1
99.5
100.0
100.0

100.0
98.7
98.5
100.0
100.0
100.0
88.9
100.0
100.0
100.0

Nﬂip

1893.55±68.98
45.51±9.47
246.11±75.36
52.76±16.29
18.93±7.11
762267.56±52179.46
273.56±191.29
729.94±491.83
107.59±31.15
51.11±4.33

2753.43±188.27
17.91±4.64
271.27±65.18
48.96±21.03
8.79±2.44
507456.61±34517.04
40.23±27.29
449.70±106.42
20.01±6.04
8.48±2.52

∆Nﬂip

386.04
35.60
60.60
25.93
13.36
484843.27
248.99
288.73
89.05
43.74

1245.92
8.00
85.76
22.13
3.22
230032.32
15.66
8.49
1.47
1.11

the prediction accuracy of other samples. Besides, we take
the ﬁne-tuning (FT) of the last fully-connected layer using
the objective deﬁned in Eq. (6) without considering the
constraints as a basic attack and present its results.

4.2.2 Implementation Details of SSA
For each attack, we ﬁx λ1 as 1 and adopt a strategy for jointly
searching λ2 and k. Speciﬁcally, for an initially given k, we
search λ2 from a relatively large initial value and divide it by
2 if the attack does not succeed. The maximum search times
of λ2 for a ﬁxed k is set to 8. If it exceeds the maximum
search times, we double k and search λ2 from the relatively
large initial value. The maximum search times of k is set to
4. On CIFAR-10, the initial k and λ2 are set to 5 and 100. On
ImageNet, λ2 is initialized as 104; k is initialized as 5 and
50 for ResNet and VGG, respectively. On CIFAR-10, the δ in
L1 is set to 10. On ImageNet, the δ is set to 3 and increased
to 10 if the attack fails. u1 and u2 are initialized as b and
u3 is initialized as 0. z1 and z2 are initialized as 0 and z3
is initialized as 0. ˆb is initialized as b. During each iteration,
the number of gradient steps for updating ˆb is 5 and the step
size is set to 0.01 on both datasets. Hyper-parameters (ρ1, ρ2,
ρ3) (see Eq. (17)) are initialized as (10−4, 10−4, 10−5) on both
datasets, and increased by ρi ← ρi × 1.01, i = 1, 2, 3 after
each iteration. The maximum values of (ρ1, ρ2, ρ3) are set to
(50, 50, 5) on both datasets. Besides the maximum number of
iterations (i.e., 2,000), we also set another stopping criterion,
i.e., ||ˆb − u1||2

2 ≤ 10−4 and ||ˆb − u2||2

2 ≤ 10−4.

4.2.3 Main Results

Due to the absence of the training data, the PA-ACC of
FT is also poor. These results indicate that ﬁne-tuning the
trained DNN as an attack method is infeasible. Although T-
BFA ﬂips the second-fewest bits under three cases, it fails to
achieve a higher ASR than GDA and FSA. In terms of PA-
ACC, SSA is comparable to other methods. Note that the PA-
ACC of SSA signiﬁcantly outperforms that of GDA, which
is the most competitive w.r.t. ASR and Nﬂip among all the
baseline methods. The PA-ACC of GDA is relatively poor,
because it does not employ auxiliary samples. Achieving the
highest ASR, the lowest Nﬂip, and the comparable PA-ACC
demonstrates that our optimization-based method is more
superior to other heuristic methods (T-BFA and GDA).

Results on ImageNet. The results on ImageNet are
shown in Table 2. It can be observed that GDA shows very
competitive performance compared to other methods. How-
ever, our method obtains the highest PA-ACC, the fewest
bit-ﬂips (less than 8), and a 100% ASR in attacking ResNet.
For VGG, our method also achieves a 100% ASR with the
fewest Nﬂip for both bit-widths. The Nﬂip results of our
method are mainly attributed to the cardinality constraint
on the number of bit-ﬂips. Moreover, for our method, the
average PA-ACC degradation over four cases on ImageNet
is only 0.06%, which demonstrates the stealthiness of our
attack. When comparing the results of ResNet and VGG,
an interesting observation is that all methods require sig-
niﬁcantly more bit-ﬂips for VGG. One reason is that VGG
is much wider than ResNet. Similar to the claim in [53],
increasing the network width contributes to the robustness
against the bit-ﬂip based attack.

Results on CIFAR-10. The results of all methods on CIFAR-
10 are shown in Table 2. Our method achieves a 100% ASR
with the fewest Nﬂip for all the bit-widths and architectures.
FT modiﬁes the maximum number of bits among all meth-
ods since there is no limitation of parameter modiﬁcations.

4.2.4 Resistance to Defense Methods

Resistance to Piece-wise Clustering. For our method, the
initial k is set to 50 on ImageNet and the rest settings are the
same as those in Section 4.2.2. The results of the resistance

9

Fig. 2. Results of SSA with different parameters λ2 (λ1 is ﬁxed at 1), k, and the number of auxiliary samples N2 on CIFAR-10. Regions in shadow
indicate the standard deviation of attacking the 1,000 images.

Fig. 3. Visualization of decision boundaries of the original model and the post attack models. The attacked sample from Class 3 is misclassiﬁed into
the Class 1 by FSA, GDA, and SSA (ours).

to the piece-wise clustering of all attack methods are shown
in Table 3. It shows that the model trained with piece-wise
clustering can improve the number of required bit-ﬂips for
all attack methods. However, our method still achieves a
100% ASR with the least number of bit-ﬂips on both two
datasets. Compared with other methods, SSA achieves the
fewest ∆Nﬂip on ImageNet and the best PA-ACC on both
datasets. These results demonstrate the superiority of our
method over other methods when attacking models trained
with piece-wise clustering.

Resistance to Larger Model Capacity. All settings of
our method are the same as those used in Section 4.2.2.
The results are presented in Table 3. We observe that all
methods require more bit-ﬂips to attack the model with the
2× width. To some extent, it demonstrates that the wider
network with the same architecture is more robust against
the bit-ﬂip based attack. However, our method still achieves
a 100% ASR with the fewest Nﬂip and ∆Nﬂip. Moreover,
when comparing the two defense methods, we ﬁnd that
piece-wise clustering performs better than the model with
a larger capacity in terms of ∆Nﬂip. However, piece-wise
clustering training also causes the accuracy decrease of the
original model (e.g., from 92.16% to 91.01% on CIFAR-10).

4.2.5 Ablation Studies
We perform ablation studies on parameters λ2 (λ1 is ﬁxed
at 1), k, and the number of auxiliary samples N2. We use the
8-bit quantized ResNet on CIFAR-10 as the representative
for analysis. We discuss the attack performance of SSA

under different values of λ2 while k is ﬁxed at 20, and
under different values of k while λ2 is ﬁxed at 10. To
analyze the effect of N2, we conﬁgure N2 from 25 to 800
and keep other settings the same as those in Section 4.2.2.
The results are presented in Fig. 2. We observe that our
method achieves a 100% ASR when λ2 is less than 20. As
expected, the PA-ACC increases while the ASR decreases
along with the increase of λ2, since larger λ2 encourages
more stealthy attack. The plot of parameter k presents that
k can exactly limit the number of bit-ﬂips, while other attack
methods do not involve such constraint. This advantage is
critical since it allows the attacker to identify limited bits to
perform an attack when the budget is ﬁxed. As shown in the
ﬁgure, the number of auxiliary samples less than 200 has a
marked positive impact on the PA-ACC. It’s intuitive that
more auxiliary samples can lead to a better PA-ACC. The
observation also indicates that SSA still works well without
too many auxiliary samples.

4.2.6 Visualization of Decision Boundary
To further compare FSA and GDA with our method, we vi-
sualize the decision boundaries of the original and the post
attack models in Fig. 3. We adopt a four-layer Multi-Layer
Perceptron trained with the simulated 2-D Blob dataset
from 4 classes. The original decision boundary indicates
that the original model classiﬁes all data points almost
perfectly. The attacked sample is classiﬁed into Class 3 by
all methods. Visually, GDA modiﬁes the decision boundary
drastically, especially for Class 0. However, our method

15102050100200260708090100PA-ACC / ASR (%)101520Nflip51015202530k7580859095100PA-ACC / ASR (%)051015202530Nflip2550100200400800N2859095100PA-ACC / ASR (%)0510NflipASRPA-ACCNflipOriginal ACC=98.0%FSA PA-ACC=89.4% Nflip=97GDA PA-ACC=61.3% Nflip=9SSA(ours) PA-ACC=91.6% Nflip=7Class 0Class 1 (Target Class)Class 2Class 3 (Source Class)Attacked SampleTABLE 4
Results of three attack methods across different bit-widths and architectures on CIFAR-10 and ImageNet (bold: the best). The mean and standard
deviation of TA, PA-ACC, and Nﬂip are calculated by attacking into 10 and 5 target classes on CIFAR-10 and ImageNet, respectively. Our method
is denoted as TSA.

10

Dataset Method

0
1
-
R
A
F
I
C

t
e
N
e
g
a
m

I

FT
TBT
TSA
FT
TBT
TSA

FT
TBT
TSA
FT
TBT
TSA

Target
Model

ResNet 8-bit
TA: 92.16%

ResNet 4-bit
TA: 91.90%

ResNet 8-bit
TA: 69.50%

ResNet 4-bit
TA: 66.77%

PA-ACC
(%)

84.23±6.67
87.52±3.69
88.09±4.59
82.38±6.39
86.23±3.10
87.69±2.74

69.24±0.07
65.99±0.54
69.45±0.02
66.50±0.14
63.35±0.32
64.88±0.81

ASR
(%)

84.25±14.08
85.60±17.55
96.06±2.25
79.73±18.42
86.99±11.99
96.48±1.75

73.38±9.43
94.99±0.86
95.63±2.38
0.05±0.10
95.42±0.43
99.98±0.03

Nﬂip

18.9±9.72
63.7±4.88
4.5±1.50
18.5±9.88
27.6±5.48
4.6±0.66

961.8±293.12
605.4±18.69
3.4±1.52
705.8±492.44
268.2±11.65
11.2±2.28

Target
Model

VGG 8-bit
TA: 93.20%

VGG 4-bit
TA: 92.61%

VGG 8-bit
TA: 73.31%

VGG 4-bit
TA: 71.76%

PA-ACC
(%)

19.77±10.79
78.30±24.84
92.04±3.04
30.75±14.60
83.87±2.75
89.00±4.25

71.20±0.56
73.01±0.05
72.89±0.53
68.90±0.80
71.30±0.02
69.04±3.38

ASR
(%)

97.97±2.22
47.65±24.41
95.59±1.57
95.96±6.35
61.72±10.57
94.22±3.10

84.09±3.85
97.48±2.39
99.85±0.26
43.28±8.88
75.95±42.42
99.99±0.03

Nﬂip

2130.7±829.60
602.7±15.62
6.6±2.50
2271.1±597.21
266.4±19.71
5.4±1.91

9845.8±1608.45
4631.0±106.84
164.6±53.80
14319.8±3783.75
4359.4±614.54
223.8±57.73

modiﬁes the decision boundary mainly around the attacked
sample. Althoug FSA is comparable to ours visually in
Fig. 3, it ﬂips 10× bits than GDA and SSA. In terms of
the numerical results, SSA achieves the best PA-ACC and
the fewest Nﬂip. This ﬁnding veriﬁes that our method can
achieve a successful attack even only tweaking the original
classiﬁer.

4.3 Results of TSA

4.3.1 Baseline Methods

We compare our TSA with TBT [19], which consists of two
steps: trigger generation and weight bits identiﬁcation. TBT
has been proven to be effective, especially for ﬂipping only
several vulnerable bits [19]. We also present the results of
ﬁne-tuning (FT) the last fully-connected layer and optimize
the trigger using the objective deﬁned in Eq. (10), without
considering the constraints. We keep the same trigger design
for FT, TBT, and TSA, i.e., 40 × 40 for ImageNet and 10 × 10
for CIFAR-10 square trigger at the bottom right, as shown
in Fig. 5.

4.3.2 Implementation Details of TSA
We set λ2 as 1 and adjust λ1 to obtain a better trade-off
between effectiveness and stealthiness for TSA. On CIFAR-
10, we set λ1 as 100 and search the best k from a relatively
small initial value. Speciﬁcally, we start from k = 5 and
double k if ASR calculated on the auxiliary sample set D is
less than 98%. The maximum search times of k is set to 4.
On ImageNet, we set λ1 = 2 × 104 and k = 50 for ResNet
and λ1 = 3 × 104 and k = 500 for VGG. For both datasets, ˆb
is initialized as b and q is initialized randomly. During each
iteration, the number of gradient steps for updating ˆb and q
is 5. The step size of updating ˆb is set to 0.001, and the step
size of updating q is set to 1 at the ﬁrst 1,000 iterations and
thereafter decreased to 0.1. The initial values of (u1, u2, u3),
(z1, z2, z3), and (ρ1, ρ2, ρ3) are same as the settings in Sec-
tion 4.2.2. The maximum values of (ρ1, ρ2, ρ3) are set to (100,
100, 10) on both datasets. Besides the maximum number of
iterations (i.e., 3,000), we also set another stopping criterion,
i.e., ||ˆb − u1||2
2 ≤ 10−6 for ResNet

2 ≤ 10−6 and ||ˆb − u2||2

on ImageNet and ||ˆb − u1||2
for others.

2 ≤ 10−4 and ||ˆb − u2||2

2 ≤ 10−4

4.3.3 Main Results
Results on CIFAR-10. The results of TSA and the compared
methods are shown in Table 4. It shows that FT achieves a
higher ASR for VGG, but the accuracy on original samples
of the attacked model drop drastically and the Nﬂip is
unacceptable. For the proposed TBT in [19], the performance
in attacking ResNet is comparable with other methods, but
its ASR is too poor for VGG. It can be observed that TSA
achieves the least Nﬂip (less than 7) for all the bit-widths
and architectures, which can be attributed to the proposed
constraint on the number of ﬂipped bits (see Eq. 10). Besides,
TSA can also achieve the highest PA-ACC and a satisfactory
ASR (greater than 94%) in all cases. All results verify the
superiority of TSA compared with FT and TBT.

Results on ImageNet. Table 5 shows the results on
ImageNet. Because there is no limitation on the number
of ﬂipped bits, the FT achieves most Nﬂip among three
methods in all cases. TBT shows its very competitive perfor-
mance in attacking both architectures. However, our method
can balance three metrics better, i.e., TSA can achieve a
comparable PA-ACC, the highest ASR, and the least Nﬂip.
Especially in attacking 8-bits quantized ResNet, TSA im-
proves greatly in terms of Nﬂip with the highest PA-ACC
and ASR. For TSA, we also observe that the Nﬂip in attacking
VGG is signiﬁcantly more than that in attacking ResNet.
This ﬁnding can further verify the effect of the model width
against the bit-ﬂip based attack.

4.3.4 Resistance to Defense Method
Resistance to Piece-wise Clustering. We evaluate three
methods in attacking models training with the piece-wise
clustering and keep all settings as those in Section 4.3.2.
The results are shown in Table 5. It shows that the defense
is effective against the bit-ﬂip based attack, resulting in a
lower ASR or a more Nﬂip. On CIFAR-10, the ASR of FT
and TBT is less than 50%. Besides, it is worth noting that the
PA-ACC of the attacked model on ImageNet for the FT and
TBT is only 0.10 %, i.e., the target model is converted into a
random output generator. It also indicates the effectiveness

TABLE 5
Results of three attack methods against the models with defense on CIFAR-10 and ImageNet (bold: the best). The mean and standard deviation of
TA, PA-ACC, and Nﬂip are calculated by attacking into 10 and 5 target classes on CIFAR-10 and ImageNet, respectively. Our method is denoted
as TSA. ∆Nﬂip denotes the increased Nﬂip compared to the corresponding result in Table 4

11

Defense

Dataset

Method

e
s
i
w
-
e
c
e
i
P

g
n
i
r
e
t
s
u
C

l

l
e
d
o
M

r
e
g
r
a
L

y
t
i
c
a
p
a
C

CIFAR-10

ImageNet

CIFAR-10

ImageNet

FT
TBT
TSA
FT
TBT
TSA

FT
TBT
TSA
FT
TBT
TSA

ACC
(%)

91.01

63.62

94.29

71.35

PA-ACC
(%)

90.06±1.45
88.97±2.59
87.46±5.47
0.10±0.00
0.10±0.00
56.83±2.57

69.17±5.53
90.22±3.80
90.22±3.56
6.19±4.95
66.54±0.34
70.97±0.58

ASR
(%)

39.78±18.39
41.27±31.47
92.08±5.66
100.00±0.00
100.00±0.00
99.94±0.03

35.46±23.25
62.17±31.42
95.24±3.52
97.53±2.66
92.16±0.59
97.96±4.01

Nﬂip

∆Nﬂip

14.2±11.23
124.8±14.42
9.9±5.73
1047.6±13.24
1190.6±26.49
63.6±5.73

936.4±190.54
123.5±10.20
9.3±2.72
1824.8±136.07
1224.4±20.92
11.0±7.97

-4.7
61.1
5.4
85.8
585.2
60.2

917.5
59.8
4.8
863.0
619.0
7.6

Fig. 4. Results of TSA with different parameters λ1 (λ2 is ﬁxed at 1), k, and the number of auxiliary samples N on CIFAR-10. Regions in shadow
indicate the standard deviation of attacking into 10 target classes.

ACC and ASR on both datasets. Comparing the results of
two defense methods on ImageNet, we ﬁnd that the piece-
wise clustering can provide more effective defense than the
larger model capacity, which is consistent with the ﬁnding
in Section 4.2.4.

4.3.5 Ablation Studies
We investigate the effect of the λ1 (λ2 is ﬁxed at 1), k, and the
number of auxiliary samples N for the proposed TSA. We
show the results on CIAFR-10 using 8-bit quantized ResNet
in Fig. 4 under various values of λ1 while k is 20, and
various values of k while λ1 is 100. We also study different
size of the auxiliary sample set from 10 to 400 and keep other
settings as those in Section 4.3.2. It can be seen from the
ﬁgure that larger λ1 encourages a higher ASR but a lower
PA-ACC and more bit-ﬂips, which is in agreement with the
designed objective function in Eq. 10. When λ1 is selected
from 20 to 100, TSA can achieve a relatively better trade-off
between the three metrics. The plot of parameter k presents
that Nﬂip increases with the increase of k when k is less
than 20, which illustrates the effectiveness of the proposed
constraint on the number of ﬂipped bits. We also ﬁnd that
k = 15 is a feasible setting in attacking 8-bit quantized
ResNet on CIAFR-10. As shown in the plot of parameter
N , a larger number of auxiliary samples leads to a higher

Fig. 5. Visualization of samples embedded with a trigger generated by
TSA. Left: an image from CIFAR-10 with 10×10 trigger. Right: an image
from ImageNet with 40×40 trigger.

of the piece-wise clustering defense. However, our TSA can
still achieve both a satisfactory PA-ACC and ASR with the
least Nﬂip in all cases. These results demonstrate that the
proposed method can still identify critical bits and generate
a trigger effectively, even though model training with the
piece-wise clustering is more robust.

Resistance to Larger Model Capacity. The results of
attacking the models with 2× width are presented in Table 5.
The increased Nﬂip in all cases illustrates that larger model
capacity can improve the model robustness against these
three attacks to some extent. Among three methods, TSA
can achieve the least Nﬂip and ∆Nﬂip with the highest PA-

15102050100200160708090100PA-ACC / ASR (%)02468Nflip51015202530k2030405060708090100PA-ACC / ASR (%)01234567Nflip102550100200400N80859095100PA-ACC / ASR (%)0510NflipASRPA-ACCNflipTABLE 6
Results of TSA with various trigger sizes against 8-bit quantized
ResNet on CIFAR-10. The mean and standard deviation of all metrics
calculated by attacking into 10 target classes.

TABLE 8
Results of SSA with different target classes in attacking 8-bit quantized
ResNet on CIFAR-10. The mean and standard deviation of PA-ACC
and Nﬂip are calculated by attacking the 100 images.

12

Trigger Size
6×6
8×8
10×10
12×12
14×14
16×16

PA-ACC (%)
51.84±17.87
75.40±6.90
88.82±2.73
91.43±1.29
92.00±0.47
92.15±0.02

ASR (%)
95.02±3.94
95.18±3.03
95.67±2.32
98.61±1.39
99.82±0.17
99.99±0.01

Nﬂip
14.9±3.30
9.6±1.56
4.3±1.49
1.5±1.91
0.4±1.20
0.1±0.30

TABLE 7
Results of TSA with different trigger locations against 8-bit quantized
ResNet on CIFAR-10. The mean and standard deviation of all metrics
calculated by attacking into 10 target classes.

Trigger Location
bottom left
bottom right
top left
top right

PA-ACC (%)
88.93±3.43
88.92±2.38
90.81±1.26
90.86±1.45

ASR (%)
96.62±1.83
96.46±2.16
98.56±1.13
96.50±2.47

Nﬂip
4.0±2.49
4.7±0.90
3.2±2.18
3.0±1.55

ASR when N is less than 200, while has little effect on PA-
ACC and Nﬂip. The rationale behind this ﬁnding may be that
more auxiliary samples make a better generalization of the
generated trigger and ﬂipped bits on the unseen samples.

4.3.6 Trigger Design

We delve into the trigger design for the TSA in this part.
We ﬁrstly visualize the generated trigger with the above
settings in Fig. 5. The visualization shows that the trigger
area is relatively small in the whole image, especially for
the sample from ImageNet, resulting in a stealthy attack. We
also investigate how the trigger size and location inﬂuence
the attack effectiveness in this part. We conduct experiments
using various trigger sizes and keep other settings in line
with Section 4.3.2. The results are shown in Table 6. With
the increase of the trigger size, the attack performance is
improved overall, including a higher PA-ACC, a higher
ASR, and a less Nﬂip. Meanwhile, a larger trigger size means
more modiﬁcation on the sample, leading to poor stealthi-
ness. The results of TSA with different trigger locations are
shown in Table 7. It shows that TSA can achieve satisfactory
attack performance in all cases. However, an interesting
observation is that TSA achieves a relatively better result
when the trigger is located at the top left. This may be
because the important features on CIFAR-10 are mainly near
the top left, which is consistent with the ﬁnding in [19].

4.4 A Closer Look at the Proposed Method

4.4.1 Effect of the Target Class

We study the effect of the target class for the proposed
method in this section. Table 8 shows the results of SSA
varying target class on CIAFR-10, where we attack 100 sam-
ples from other classes into each target class. For different
target class, SSA always achieves a 100.0% ASR and about
88% PA-ACC with a few bit-ﬂips. It veriﬁes that different
target class has a little effect on the attack performance
for SSA. The results of TSA are shown in Table 9. For the
TSA, an interesting ﬁnding is that the attack performance
changes a lot across target classes, which is different from

Target Class
0
1
2
3
4
5
6
7
8
9

PA-ACC (%) ASR (%)

87.95±2.66
88.49±2.48
88.22±2.81
87.99±2.76
87.80±2.73
88.18±2.61
88.13±2.58
88.26±2.69
88.55±2.43
88.45±2.54

100.0
100.0
100.0
100.0
100.0
100.0
100.0
100.0
100.0
100.0

Nﬂip
5.61±1.62
5.61±1.62
5.61±1.62
5.61±1.62
5.61±1.62
5.61±1.62
5.56±1.56
5.56±1.56
5.51±1.50
5.50±1.50

TABLE 9
Results of TSA with different target classes in attacking 8-bit quantized
ResNet on CIFAR-10.

Target Class
0
1
2
3
4
5
6
7
8
9

PA-ACC (%) ASR (%) Nﬂip

89.58
91.87
92.01
80.41
90.46
78.30
91.10
89.20
90.75
87.17

96.50
96.11
97.42
96.94
94.04
98.87
92.75
97.79
91.95
98.26

5
2
2
5
4
7
4
5
5
6

SSA. Therefore, the difﬁculty of attacking different target
classes is different. For example, TSA achieves a relatively
high PA-ACC and ASR with only 2 bit-ﬂips in attacking into
class 2; the PA-ACC is only 78.30% with the highest bit-ﬂips
in attacking into class 5.

4.4.2 Complexity Analysis

The computational complexity of SSA consists of
two
parts, the forward and backward pass. In terms of the
forward pass, since Θ, B{1,...,K}\{s,t} are ﬁxed during the
optimization, their involved terms, including g(x; Θ) and
p(x; Θ, Bi)|i(cid:54)=s,t, are calculated only one time. The main
cost from ˆBs and ˆBt is O(2(N2 + 1)C 2Q) per iteration,
as there are N2 + 1 samples. In terms of the backward
pass, the main cost is from the update of ˆbr+1, which is
O(2(N2 + 1)CQ) per iteration in the gradient descent. Since
all other updates are very simple, their costs are omitted
here. Thus, the overall computational cost is O(cid:0)Touter ·
Tinner · [2(N2 + 1)CQ · (C + 1)](cid:1)
, with Touter being the
iteration of the overall algorithm and Tinner indicating the
number of gradient steps in updating ˆbr+1. As shown in
Section 4.4.3, SSA always converges very fast in our exper-
iments, thus Touter is not very large. As demonstrated in
Section 4.2.2, Tinner is set to 5 in our experiments. In short,
the proposed method can be optimized very efﬁciently.

For TSA, since the optimization involves updating the
input variable (i.e., the trigger q), the main costs are the
forward process and computing the gradients of L w.r.t. ˆb
and q. Similar to the analysis for SSA, the cost of updating
ˆb per iteration is O(2KN CQ) in the gradient descent. The
costs of the forward process and computing the gradients of
L w.r.t. q depends on the attacked model f , which are the

13

5 CONCLUSION AND FUTURE WORK
In this work, we have explored a novel attack paradigm that
the weights of a deployed DNN can be slightly changed
via bit ﬂipping in the memory to achieve some malicious
purposes. Firstly, we present the general formulation con-
sidering the attack effectiveness and stealthiness and for-
mulate it as a mixed integer programming (MIP) problem
since the weights are stored as binary bits in the memory.
Based on the general formulation, we propose two types of
attack: SSA and TSA. while the predictions on other samples
are not signiﬁcantly inﬂuenced. Furthermore, we solve the
MIP problem with an effective and efﬁcient continuous
algorithm. Since the critical bits are determined through
optimization, SSA and TSA can achieve the attack goals by
ﬂipping a few bits under different experimental settings.

Future studies of the bit-ﬂip based attack include, but
are not limited to: 1) Specifying the proposed framework
as other types of attack with different malicious purposes;
2) Extending it to the different tasks, e.g., face recognition
and autonomous driving; 3) Exploring more strict settings
than the whit-box one; 4) Using our method to evaluate the
parameter robustness of models with different architectures,
training strategies, etc., due to its least number of bit-
ﬂips. We hope that this work can draw more attention to
the security of the deployed DNNs and encourage further
investigation on the defense against the bit-ﬂip based attack.

REFERENCES

[1] K. He, X. Zhang, S. Ren, and J. Sun, “Deep residual learning for

image recognition,” in CVPR, 2016.

[2] B. Zhang, D. Xiong, and J. Su, “Neural machine translation with
deep attention,” IEEE transactions on pattern analysis and machine
intelligence, vol. 42, no. 1, pp. 154–163, 2018.
S. Levine, C. Finn, T. Darrell, and P. Abbeel, “End-to-end training
of deep visuomotor policies,” The Journal of Machine Learning
Research, vol. 17, no. 1, pp. 1334–1373, 2016.

[3]

[4] C. Szegedy, W. Zaremba, I. Sutskever, J. Bruna, D. Erhan, I. Good-
fellow, and R. Fergus, “Intriguing properties of neural networks,”
in ICLR, 2014.

[5] T. Gu, K. Liu, B. Dolan-Gavitt, and S. Garg, “Badnets: Evaluating
backdooring attacks on deep neural networks,” IEEE Access, vol. 7,
pp. 47 230–47 244, 2019.

[6] Y. Dong, H. Su, B. Wu, Z. Li, W. Liu, T. Zhang, and J. Zhu, “Efﬁcient
decision-based black-box adversarial attacks on face recognition,”
in CVPR, 2019.
S. G. Finlayson, J. D. Bowers, J. Ito, J. L. Zittrain, A. L. Beam, and
I. S. Kohane, “Adversarial attacks on medical machine learning,”
Science, vol. 363, no. 6433, pp. 1287–1289, 2019.

[7]

[8] K. Eykholt, I. Evtimov, E. Fernandes, B. Li, A. Rahmati, C. Xiao,
A. Prakash, T. Kohno, and D. Song, “Robust physical-world at-
tacks on deep learning visual classiﬁcation,” in CVPR, 2018.
[9] A. Arnab, O. Miksik, and P. H. Torr, “On the robustness of seman-
tic segmentation models to adversarial attacks,” IEEE Transactions
on Pattern Analysis and Machine Intelligence, vol. 42, no. 12, pp.
3040–3053, 2019.

[10] A. Saha, A. Subramanya, and H. Pirsiavash, “Hidden trigger

backdoor attacks,” in AAAI, 2020.

[11] C. Xie, K. Huang, P.-Y. Chen, and B. Li, “Dba: Distributed backdoor

attacks against federated learning,” in ICLR, 2019.

[12] I. J. Goodfellow, J. Shlens, and C. Szegedy, “Explaining and har-

nessing adversarial examples,” in ICLR, 2015.

[13] S.-M. Moosavi-Dezfooli, A. Fawzi, O. Fawzi, and P. Frossard,

“Universal adversarial perturbations,” in CVPR, 2017.

[14] J. Breier, X. Hou, D. Jap, L. Ma, S. Bhasin, and Y. Liu, “Practical

fault attack on deep neural networks,” in CCS, 2018.

[15] A. S. Rakin, Z. He, and D. Fan, “Bit-ﬂip attack: Crushing neural

network with progressive bit search,” in ICCV, 2019.

[16] M. Agoyan, J.-M. Dutertre, A.-P. Mirbaha, D. Naccache, A.-L.

Ribotta, and A. Tria, “How to ﬂip a bit?” in IOLTS, 2010.

(a) SSA on CIFAR-10

(b) SSA on ImageNet

(c) TSA on CIFAR-10

(d) TSA on ImageNet

Fig. 6. Numerical convergence analysis of SSA and TSA on CIFAR-
10 and ImageNet, respectively. We present the values of ||ˆb − u1||2
2,
||ˆb − u2||2
2 and λ2L1 + λ2L2 at different iterations in attacking 8-bit
quantized ResNet. Note that the maximum number of iterations is set to
2,000 and 3,000 for SSA and TSA, respectively.

same with the complexities of the forward and backward
pass in the standard model training. Moreover, considering
the very small auxiliary sample set (128 on CIFAR-10 and
512 on ImageNet), TSA could be optimized with an accept-
able time.

4.4.3 Numerical Convergence Analysis

We present the numerical convergence of SSA and TSA in
Fig. 6. Note that ||ˆb − u1||2
2 characterize the
degree of satisfaction of the box and (cid:96)2-sphere constraint,
respectively. λ1L1 + λ2L2 means the attack performance to
some extent, where a lower value corresponds to a better
attack.

2 and ||ˆb − u2||2

For the two examples of SSA on CIFAR-10 and Ima-
geNet, the values of both indicators ﬁrst increase, then drop,
and ﬁnally close to 0. Another interesting observation is that
λ1L1 + λ2L2 ﬁrst decreases evidently and then increases
slightly. Such ﬁndings illustrate the optimization process of
SSA. In the early iterations, modifying the model parameters
tends to achieve the effectiveness and stealthiness goals; in
the late iterations, ˆb is encouraged to satisfy the box and l2-
sphere constraint. We also observe that both examples stop
when meeting ||ˆb − u1||2
2 ≤ 10−4
and do not exceed the maximum number of iterations.

2 ≤ 10−4 and ||ˆb − u2||2

||ˆb − u1||2

2 and ||ˆb − u2||2

For the TSA,

2 start from
relatively high values and could be optimized to be lower
than the stop threshold. At the end of the optimization, TSA
can modify the model parameters and generate a trigger
to achieve a low value of λ1L1 + λ2L2 on both datasets.
Similar to the SSA, the optimization can stop within the
maximum number of iterations. All the above numerical
results demonstrate the fast convergence of our proposed
method in practice.

05001000# iterations0.00.20.40.6||bu1||22   /   ||bu2||22303540455011+22050010001500# iterations0.0000.0020.0040.0060.0080.010||bu1||22   /   ||bu2||221245512460124651247011+2205001000# iterations0.0000.0050.0100.0150.0200.025||bu1||22   /   ||bu2||220.51.01.52.02.511+22010002000# iterations106105104103102101||bu1||22   /   ||bu2||2210010110210311+22||bu1||22||bu2||2211+22[17] B. Selmke, S. Brummer, J. Heyszl, and G. Sigl, “Precise laser fault
injections into 90 nm and 45 nm sram-cells,” in CARDIS, 2015.
[18] Y. Kim, R. Daly, J. Kim, C. Fallin, J. H. Lee, D. Lee, C. Wilkerson,
K. Lai, and O. Mutlu, “Flipping bits in memory without accessing
them: An experimental study of dram disturbance errors,” ACM
SIGARCH Computer Architecture News, vol. 42, no. 3, pp. 361–372,
2014.

[19] A. S. Rakin, Z. He, and D. Fan, “Tbt: Targeted neural network

attack with bit trojan,” in CVPR, 2020.

[20] A. S. Rakin, Z. He, J. Li, F. Yao, C. Chakrabarti, and D. Fan,
“T-bfa: Targeted bit-ﬂip adversarial weight attack,” arXiv preprint
arXiv:2007.12336, 2020.

[21] B. Wu and B. Ghanem, “(cid:96)p-box admm: A versatile framework
for integer programming,” IEEE transactions on pattern analysis and
machine intelligence, vol. 41, no. 7, pp. 1695–1708, 2018.

[22] R. Glowinski and A. Marroco, “Sur l’approximation, par ´el´ements
ﬁnis d’ordre un, et la r´esolution, par p´enalisation-dualit´e d’une
classe de probl`emes de dirichlet non lin´eaires,” ESAIM: Mathemat-
ical Modelling and Numerical Analysis-Mod´elisation Math´ematique et
Analyse Num´erique, vol. 9, no. R2, pp. 41–76, 1975.

[23] D. Gabay and B. Mercier, “A dual algorithm for the solution of
nonlinear variational problems via ﬁnite element approximation,”
Computers & mathematics with applications, vol. 2, no. 1, pp. 17–40,
1976.

[24] J. Bai, B. Wu, Y. Zhang, Y. Li, Z. Li, and S.-T. Xia, “Targeted attack
against deep neural networks via ﬂipping limited weight bits,” in
ICLR, 2021.

[25] N. Akhtar and A. Mian, “Threat of adversarial attacks on deep
learning in computer vision: A survey,” IEEE Access, vol. 6, pp.
14 410–14 430, 2018.

[26] D. Wu, Y. Wang, S.-T. Xia, J. Bailey, and X. Ma, “Skip connections
matter: On the transferability of adversarial examples generated
with resnets,” in ICLR, 2020.

[27] W. Chen, Z. Zhang, X. Hu, and B. Wu, “Boosting decision-based
black-box adversarial attacks with random sign ﬂip,” in ECCV,
2020.

[28] P. Zhao, S. Liu, P.-Y. Chen, N. Hoang, K. Xu, B. Kailkhura, and
X. Lin, “On the design of black-box adversarial examples by lever-
aging gradient-free optimization and operator splitting method,”
in CVPR, 2019.

[29] M. Andriushchenko, F. Croce, N. Flammarion, and M. Hein,
“Square attack: a query-efﬁcient black-box adversarial attack via
random search,” in ECCV, 2020.

[30] K. R. Mopuri, A. Ganeshan, and R. V. Babu, “Generalizable data-
free objective for crafting universal adversarial perturbations,”
IEEE transactions on pattern analysis and machine intelligence, vol. 41,
no. 10, pp. 2452–2465, 2018.

[31] Y. Xu, B. Wu, F. Shen, Y. Fan, Y. Zhang, H. T. Shen, and W. Liu,
“Exact adversarial attack to image captioning via structured out-
put learning with latent variables,” in CVPR, 2019.

[32] J. Bai, B. Chen, Y. Li, D. Wu, W. Guo, S.-t. Xia, and E.-h. Yang,
“Targeted attack for deep hashing based retrieval,” in ECCV, 2020.
[33] S. Bai, Y. Li, Y. Zhou, Q. Li, and P. H. Torr, “Adversarial metric
attack and defense for person re-identiﬁcation,” IEEE Transactions
on Pattern Analysis and Machine Intelligence, 2020.

[34] C. Xie, J. Wang, Z. Zhang, Z. Ren, and A. Yuille, “Mitigating

adversarial effects through randomization,” ICLR, 2018.

[35] W. Xu, D. Evans, and Y. Qi, “Feature squeezing: Detecting adver-

sarial examples in deep neural networks,” NDSS, 2017.

[36] Y. Carmon, A. Raghunathan, L. Schmidt, J. C. Duchi, and P. Liang,
“Unlabeled data improves adversarial robustness,” in NeurIPS,
2019.

[37] Y. Liu, A. Mondal, A. Chakraborty, M. Zuzak, N. Jacobsen,
D. Xing, and A. Srivastava, “A survey on neural trojans.” in
ISQED, 2020.

[38] Y. Li, B. Wu, Y. Jiang, Z. Li, and S.-T. Xia, “Backdoor learning: A

survey,” arXiv preprint arXiv:2007.08745, 2020.

[39] Y. Liu, X. Ma, J. Bailey, and F. Lu, “Reﬂection backdoor: A natural

backdoor attack on deep neural networks,” ECCV, 2020.

[40] A. Turner, D. Tsipras, and A. Madry, “Label-consistent backdoor

attacks,” arXiv preprint arXiv:1912.02771, 2019.

14

[43] T. A. Nguyen and A. T. Tran, “Wanet - imperceptible warping-

based backdoor attack,” in ICLR, 2021.

[44] Y. Liu, Y. Xie, and A. Srivastava, “Neural trojans,” in ICCD, 2017.
[45] K. Liu, B. Dolan-Gavitt, and S. Garg, “Fine-pruning: Defending
against backdooring attacks on deep neural networks,” in RAID,
2018.

[46] B. Wang, Y. Yao, S. Shan, H. Li, B. Viswanath, H. Zheng, and
B. Y. Zhao, “Neural cleanse: Identifying and mitigating backdoor
attacks in neural networks,” in IEEE S&P, 2019, pp. 707–723.
[47] Y. Liu, L. Wei, B. Luo, and Q. Xu, “Fault injection attack on deep

neural network,” in ICCAD, 2017.

[48] Y. Liu, S. Ma, Y. Aafer, W. Lee, J. Zhai, W. Wang, and X. Zhang,

“Trojaning attack on neural networks,” in NDSS, 2018.

[49] S. Hong, P. Frigo, Y. Kaya, C. Giuffrida, and T. Dumitras, , “Terminal
brain damage: Exposing the graceless degradation in deep neural
networks under hardware fault attacks,” in USENIX Security Sym-
posium, 2019.

[50] T.-W. Weng, P. Zhao, S. Liu, P.-Y. Chen, X. Lin, and L. Daniel,
“Towards certiﬁcated model robustness against weight perturba-
tions,” in AAAI, 2020.

[51] Y. Ji, X. Zhang, S. Ji, X. Luo, and T. Wang, “Model-reuse attacks on

deep learning systems,” in CCS, 2018.

[52] P. Zhao, S. Wang, C. Gongye, Y. Wang, Y. Fei, and X. Lin, “Fault
sneaking attack: A stealthy framework for misleading deep neural
networks,” in ACM DAC, 2019.

[53] Z. He, A. S. Rakin, J. Li, C. Chakrabarti, and D. Fan, “Defending
and harnessing the bit-ﬂip based adversarial weight attack,” in
CVPR, 2020.

[54] D. Stutz, N. Chandramoorthy, M. Hein, and B. Schiele, “Bit error
robustness for energy-efﬁcient dnn accelerators,” in MLSys, 2021.
robustness:
adversarial
Energy-efﬁcient and secure dnn accelerators,” arXiv preprint
arXiv:2104.08323, 2021.

“Random and

[55] ——,

error

bit

[56] X. Sun, Z. Zhang, X. Ren, R. Luo, and L. Li, “Exploring the
vulnerability of deep neural networks: A study of parameter
corruption,” in AAAI, 2021.

[57] J. Li, A. S. Rakin, Y. Xiong, L. Chang, Z. He, D. Fan, and
C. Chakrabarti, “Defending bit-ﬂip attack through dnn weight
reconstruction,” in ACM DAC, 2020.

[58] Z. He, T. Zhang, and R. Lee, “Sensitive-sample ﬁngerprinting of

deep neural networks,” in CVPR, 2019.

[59] J. Li, A. S. Rakin, Z. He, D. Fan, and C. Chakrabarti, “Radar: Run-
time adversarial weight attack detection and accuracy recovery,”
arXiv preprint arXiv:2101.08254, 2021.

[60] H. Guan, L. Ning, Z. Lin, X. Shen, H. Zhou, and S.-H. Lim, “In-

place zero-space memory protection for cnn,” in NeurIPS, 2019.

[61] K. Huang, P. H. Siegel, and A. A. Jiang, “Functional error correc-

tion for reliable neural networks,” in ISIT, 2020.

[62] S. Migacz, “8-bit inference with tensorrt,” in GPU technology con-

ference, 2017.

[63] V. Van Der Veen, Y. Fratantonio, M. Lindorfer, D. Gruss, C. Mau-
rice, G. Vigna, H. Bos, K. Razavi, and C. Giuffrida, “Drammer:
Deterministic rowhammer attacks on mobile platforms,” in CCS,
2016.

[64] D. Gruss, M. Lipp, M. Schwarz, D. Genkin,

Jufﬁnger,
S. O’Connell, W. Schoechl, and Y. Yarom, “Another ﬂip in the wall
of rowhammer defenses,” in IEEE S&P, 2018.

J.

[65] T. Li, B. Wu, Y. Yang, Y. Fan, Y. Zhang, and W. Liu, “Compress-
ing convolutional neural networks via factorized convolutional
ﬁlters,” in CVPR, 2019.

[66] A. Bibi, B. Wu, and B. Ghanem, “Constrained k-means with
general pairwise and cardinality constraints,” arXiv preprint
arXiv:1907.10410, 2019.

[67] B. Wu, L. Shen, T. Zhang, and B. Ghanem, “Map inference via
l2-sphere linear program reformulation,” International Journal of
Computer Vision, pp. 1–24, 2020.

[68] Y. Fan, B. Wu, T. Li, Y. Zhang, M. Li, Z. Li, and Y. Yang, “Sparse
adversarial attack via perturbation factorization,” in ECCV, 2020.
[69] S. Boyd, N. Parikh, and E. Chu, Distributed optimization and statis-
tical learning via the alternating direction method of multipliers. Now
Publishers Inc, 2011.

[41] S. Zhao, X. Ma, X. Zheng, J. Bailey, J. Chen, and Y.-G. Jiang, “Clean-
label backdoor attacks on video recognition models,” in CVPR,
2020.

[70] E. G. Gol’shtein and N. Tret’yakov, “Modiﬁed lagrangians in
convex programming and their generalizations,” in Point-to-Set
Maps and Mathematical Programming. Springer, 1979, pp. 86–97.

[42] A. Nguyen and A. Tran, “Input-aware dynamic backdoor attack,”

in NeurIPS, 2020.

[71] J. Eckstein and D. P. Bertsekas, “On the douglas—rachford split-
ting method and the proximal point algorithm for maximal mono-

tone operators,” Mathematical Programming, vol. 55, no. 1-3, pp.
293–318, 1992.

[72] A. Krizhevsky, G. Hinton et al., “Learning multiple layers of

features from tiny images,” Technical report, 2009.

[73] O. Russakovsky, J. Deng, H. Su, J. Krause, S. Satheesh, S. Ma,
Z. Huang, A. Karpathy, A. Khosla, M. Bernstein et al., “Imagenet
large scale visual recognition challenge,” International journal of
computer vision, vol. 115, no. 3, pp. 211–252, 2015.

[74] K. Simonyan and A. Zisserman, “Very deep convolutional net-

works for large-scale image recognition,” in ICLR, 2015.

[75] B. Jacob, S. Kligys, B. Chen, M. Zhu, M. Tang, A. Howard,
H. Adam, and D. Kalenichenko, “Quantization and training of
neural networks for efﬁcient integer-arithmetic-only inference,” in
CVPR, 2018.

APPENDIX A
UPDATE ˆb BY GRADIENT DESCENT
Here, we derive the gradient of L w.r.t. ˆb, which is adopted
to update ˆbr+1 by gradient descent. In the following two
parts, we present the derivations for the SSA and TSA,
respectively.

A.1 Update ˆb for SSA
For SSA, b corresponds to the reshaped and concatenated
Bs and Bt and ˆb could be the reshaped and concatenated
ˆBs and ˆBt, respectively. Based on the speciﬁed objective
function for SSA, the derivation consists of the following
parts.

Derivation of ∂LSSA

1

(x, s, t; ˆBs, ˆBt)/∂ˆb.

For clarity,

Thus, we obtain that

∂LSSA
1

(x,s,t; ˆBs, ˆBt)

∂ˆb

= (cid:2)

Reshape

(cid:0) ∂LSSA
1

(cid:0) ∂LSSA
1

Reshape

15

(cid:1);

(cid:1)(cid:3),

(23)

(x, s, t; ˆBs, ˆBt)

∂ ˆBs

(x, s, t; ˆBs, ˆBt)

∂ ˆBt

where Reshape(·) elongates a matrix to a vector along the
column.

Derivation of ∂L2(D2; ˆBs, ˆBt)/∂ˆb. For clarity, here we

ﬁrstly repeat the following deﬁnition

L2(D2; ˆBs, ˆBt) =

(cid:88)

(cid:96)(f (xi; ˆBs, ˆBt), yi),

(xi,yi)∈D2

(24)

where fj(xi; ˆBs, ˆBt) indicates the posterior probability of
xi w.r.t. class j. D2 = {(xi, yi)}N2
i=1 denotes the auxiliary
sample set. (cid:96)(·, ·) is speciﬁed as the cross entropy loss. Then,
we have

∂L2(D2; ˆBs, ˆBt)
∂ ˆBs
(cid:20)
(cid:0)I(yi = s) − fs(xi; ˆBs, ˆBt)(cid:1) ·

(cid:88)

=

(xi,yi)∈D2

∂p(xi; Θ, ˆBs)
∂ ˆBs

(cid:21)
,

(25)

∂p(xi; Θ, ˆBt)
∂ ˆBt

(cid:21)
,

(26)

here we ﬁrstly repeat some deﬁnitions,

LSSA
1

(x, s, t; ˆBs, ˆBt) = max (cid:0)τ − p(x; Θ, ˆBt) + δ, 0(cid:1)
+ max (cid:0)p(x; Θ, ˆBs) − τ + δ, 0(cid:1),

(18)

=

∂L2(D2; ˆBs, ˆBt)
∂ ˆBt
(cid:20)
(cid:0)I(yi = t) − ft(xi; ˆBs, ˆBt)(cid:1) ·

(cid:88)

(xi,yi)∈D2

p(x; Θ, ˆBi) = [h( ˆBi,1);h( ˆBi,2); ...; h( ˆBi,C)](cid:62)g(x; Θ), (19)

where I(a) = 1 of a is true, otherwise I(a) = 0. Thus, we
obtain

h(v) = (−2Q−1 · vQ +

Q−1
(cid:88)

i=1

2i−1 · vi) · ∆l.

(20)

∂L2(D2; ˆBs, ˆBt)
∂ˆb

=

(cid:20)

Reshape

Reshape

(cid:0) ∂L2(D2; ˆBs, ˆBt)
∂ ˆBs
(cid:0) ∂L2(D2; ˆBs, ˆBt)
∂ ˆBt

(cid:1);

(cid:21)
.

(cid:1)

(27)

Then, we obtain that
∂p(x; Θ, ˆBi)
∂ ˆBi

= [g1(x; Θ) · (

∇h( ˆBi,1)
∂ ˆBi,1
∇h( ˆBi,C)
∇ ˆBi,C

)(cid:62); ...;

)(cid:62)],

gC(x; Θ) · (

Derivation of ∂L(ˆb)/∂ˆb. According to the augmented
Lagrangian function, and utilizing Eqs. (23) and (27), we
obtain the gradient of L w.r.t. ˆb for SSA, as follows.

(21)

∇h(v)

∇v = [20; 21, . . . , 2Q−2; −2Q−1] · ∆l is a constant,
where
and here l indicates the last layer; gj(x; Θ) denotes the j-th
entry of the vector g(x; Θ).
Utilizing (21), we have

∂LSSA
1

∂LSSA
1

(x,s,t; ˆBs, ˆBt)
∂ ˆBs
(x,s,t; ˆBs, ˆBt)
∂ ˆBt

=

=

(cid:40)

(cid:40) ∂p(x;Θ, ˆBs)

∂ ˆBs
0, otherwise
− ∂p(x;Θ, ˆBt)
∂ ˆBt
0, otherwise

, if p(x; Θ, Bs) > τ −δ

,

, if p(x; Θ, Bt) < τ +δ
.

(22)

∂L(ˆb)
∂ˆb

=

∂LSSA
1

(x, s, t; ˆBs, ˆBt)

∂L2(D2; ˆBs, ˆBt)
∂ˆb
+z1 + z2 + ρ1(ˆb − u1) + ρ2(ˆb − u2)
+2(ˆb − b) · (cid:2)z3 + ρ3||ˆb − b||2

2 − k + u3

∂ˆb

(cid:3).

+

(28)

A.2 Update ˆb for TSA
For TSA, b corresponds to the reshaped B and ˆb could
be the reshaped ˆB. We ﬁrstly give the derivation of
(φ(D; q), t; ˆB)/∂ˆb and ∂L2(D; ˆB)/∂ˆb, and then the
∂LT SA
1
derivation of ∂L(ˆb)/∂ˆb.

16
Derivation of ∂L(ˆb)/∂ˆb. According to the augmented
Lagrangian function, and utilizing Eqs. (32) and (36), we
obtain the gradient of L w.r.t. ˆb for TSA, as follows.
∂L2(D2; ˆB)
∂ˆb
+z1 + z2 + ρ1(ˆb − u1) + ρ2(ˆb − u2)
+2(ˆb − b) · (cid:2)z3 + ρ3||ˆb − b||2

(φ(D; q), t; ˆB))

∂L(ˆb)
∂ˆb

∂LT SA
1

2 − k + u3

(37)

∂ˆb

(cid:3).

=

+

Derivation of ∂LT SA

(φ(D; q), t; ˆB)/∂ˆb.
1
here we ﬁrstly repeat the following deﬁnition.

For clarity,

(φ(D; q), t; ˆB)

LT SA
1
(cid:88)

=

(cid:96)(f ((1 − m) ⊗ xi + m ⊗ q; ˆB), t),

(29)

(xi,yi)∈D

where f ((1 − m) ⊗ xi + m ⊗ q; ˆB) indicates the poste-
rior probability of xi with a trigger w.r.t. class j. D =
{(xi, yi)}N
i=1 denotes the auxiliary sample set. (cid:96)(·, ·) is spec-
iﬁed as the cross entropy loss.

Utilizing Eq. (21), we obtain that

∂LT SA
1

(φ(D; q), t; ˆB)

∂ ˆBj






=

(cid:80)

(xi,yi)∈D

(cid:20)
(cid:0)1 − f ((1 − m) ⊗ xi + m ⊗ q; ˆB))

· ∂p((1−m)⊗xi+m⊗q;Θ, ˆBj )
∂ ˆBj
(cid:20)

(cid:21)
, if t = j

(cid:80)

(xi,yi)∈D

− f ((1 − m) ⊗ xi + m ⊗ q; ˆB)

· ∂p((1−m)⊗xi+m⊗q;Θ, ˆBj )
∂ ˆBj

(cid:21)
, if t (cid:54)= j

.

(30)

Then, we have

∂LT SA
1

(φ(D; q), t; ˆB)

(cid:20) ∂LT SA
1

=

∂ ˆB
(φ(D; q), t; ˆB)
∂ ˆB0

∂LT SA
1

; ...;

(φ(D; q), t; ˆB)
∂ ˆBK

(cid:21)
.

(31)

Thus, we have

∂LT SA
1

(φ(D; q), t; ˆB))

∂ˆb

= Reshape

(cid:20) ∂LT SA
1

(φ(D; q), t; ˆB)

∂ ˆB

(cid:21)
,

(32)

where Reshape(·) ﬂattens a 3-D tensor to a vector.

Derivation of ∂L2(D; ˆB)/∂ˆb.
ﬁrstly repeat the following deﬁnition.

For clarity, here we

L2(D; ˆB) =

(cid:88)

(cid:96)(f (xi; ˆB), yi).

(xi,yi)∈D

(33)

Utilizing Eq. (21), we obtain that

∂L2(D; ˆB)
∂ ˆBj
(cid:88)

(cid:20)
(cid:0)I(yi = j) − f (xi; ˆB)) ·

=

(xi,yi)∈D

Then, we have

(34)

∂p(xi; Θ, ˆBj)
∂ ˆBj

(cid:21)
.

∂L2(D; ˆB)
∂ ˆB

=

(cid:20) ∂L2(D; ˆB)
∂ ˆBj

; ...;

∂L2(D; ˆB)
∂ ˆBK

(cid:21)
.

Thus, we have

∂L2(D2; ˆB)
∂ˆb

= Reshape

(cid:20) ∂L2(D; ˆB)
∂ ˆB

(cid:21)
.

(35)

(36)

