Machine Learning Techniques for Software Quality
Assurance: A Survey

Safa Omri
Institute of Theretical Informatics
Karlsruhe Institute of Technology
Karlsruhe, Germany
safa.omri@kit.edu

Carsten Sinz
Institute of Theretical Informatics
Karlsruhe Institute of Technology
Karlsruhe, Germany
carsten.sinz@kit.edu

1
2
0
2

r
p
A
9
2

]
E
S
.
s
c
[

1
v
6
5
0
4
1
.
4
0
1
2
:
v
i
X
r
a

Abstract—Over the last years, machine learning techniques
have been applied to more and more application domains, in-
cluding software engineering and, especially, software quality as-
surance. Important application domains have been, e.g., software
defect prediction or test case selection and prioritization. The
ability to predict which components in a large software system
are most likely to contain the largest numbers of faults in the next
release helps to better manage projects, including early estimation
of possible release delays, and affordably guide corrective actions
to improve the quality of the software. However, developing
robust fault prediction models is a challenging task and many
techniques have been proposed in the literature. Closely related to
estimating defect-prone parts of a software system is the question
of how to select and prioritize test cases, and indeed test case
prioritization has been extensively researched as a means for
reducing the time taken to discover regressions in software. In
this survey, we discuss various approaches in both fault prediction
and test case prioritization, also explaining how in recent studies
deep learning algorithms for fault prediction help to bridge the
gap between programs’ semantics and fault prediction features.
We also review recently proposed machine learning methods for
test case prioritization (TCP), and their ability to reduce the cost
of regression testing without negatively affecting fault detection
capabilities.

Index Terms—software testing, fault prediction, machine learn-

ing, test cases prioritization, deep learning

I. INTRODUCTION

Nowadays, software quality assurance is overall the most
expensive activity for nearly all software developing com-
panies [1], since team members need to spend a signiﬁcant
amount of their time inspecting the entire software in detail
rather than, for example, implementing new features. Regres-
sion testing is one of the important software maintenance
activities that let the software tester to ensure the quality
and reliability of modiﬁed program. However, as software
evolves and becomes increasingly complex, the number of
tests that are required to ensure correct functionality also
grows, leading to a commensurate increase in the time taken
to execute the test suite. Since it often may take too much
time to re-execute all of the tests for every change made to
the system, it might be more efﬁcient to ﬁrst predict where
which part of a software are most likely to be fault-prone and
then prioritize the test cases in that part. If defect prediction
can accurately predict the class that is most likely to be buggy,
a tool can prioritize tests to rapidely detect the defects in that

class. The more accurate a defect prediction is, the smaller
the subset of the test suite needed to ﬁnd potential bugs.
Therefore, software quality assurance activities, such as source
code inspection, assist developers in ﬁnding potential bugs and
allocating their testing efforts. They have a great inﬂuence on
producing high quality reliable software. Numerous research
studies have analyzed software fault prediction techniques to
help prioritize software testing and debugging. Software fault
prediction is a process of building classiﬁers to anticipate
which software modules or code areas are most likely to fail.
Most of these techniques focus on designing features (e.g.
complexity metrics) that correlate with potentially defective
code. Object-oriented metrics were initially suggested by
Chidamber and Kemerer [2]. Basili et al. [3] and Briand et al.
[4] were among the ﬁrst to use such metrics to validate and
evaluate fault-proneness. Subramanyam and Krishnan [5] and
Tang et al. [6] showed that these metrics can be used as early
indicators of externally visible software quality. D’Ambros
et al. have compared popular fault prediction approaches for
software systems [7], namely, process metrics [8], previous
faults [9] and source code metrics [3]. Nagappan et al. [10]
presented empirical evidence that code complexity metrics
can predict post-release faults. Omri et al.’s work [11] takes
into consideration not only code complexity metrics but also
the faults detected by static analysis tools to build accurate
pre-release fault predictors. Numerous research studies have
analyzed code churn (number of lines of code added, removed,
etc.) as a variable for predicting faults in large software
systems [12]–[14]. All these research studies have gone into
carefully designing features which are able to discriminate
defective code from non-defective code such as code size,
code complexity (e.g. Halstead, McCabe, CK features), code
churn metrics (e.g. the number of code lines changed), or
process metrics. Most defect prediction approaches consider
defect prediction as a binary classiﬁcation problem that can
be solved by classiﬁcation algorithms, e.g., Support Vector
Machines (SVM), Naive Bayes (NB), Decision Trees (DT),
or Neural Networks (NN). Such approaches simply classify
source code changes into two categories: fault-prone or not
fault-prone.

Those approaches, however, do not sufﬁciently capture the
syntax and different levels of semantics of source code, which

 
 
 
 
 
 
is an important capability for building accurate prediction
models. Speciﬁcally, in order to make accurate predictions,
features need to be discriminative: capable of distinguish-
ing one instance of code region from another. The existing
traditional features cannot distinguish code regions with dif-
ferent semantics but similar code structure. For example, in
Figure 1, there are two Java ﬁles, both of which contain a
for statement, a remove function and an add function.
The only difference between the two ﬁles is the order of
the remove and add function. File2.java will produce a
NoSuchElementException when the function is called
with an empty queue. Using traditional features to represent
these two ﬁles, their feature vectors are identical, because
these two ﬁles have the same source code characteristics in
terms of lines of code, function calls, raw programming tokens,
etc. However, the semantic content is different. Features that
can distinguish such semantic differences should enable the
building of more accurate prediction models. To bridge the gap
between programs’ semantic information and features used
for defect prediction, some approaches propose to leverage
a powerful representation-learning algorithm, namely deep
learning, to capture the semantic representation of programs
automatically and use this representation to improve defect
prediction.

Fig. 1. A motivating example: File2.java will exhibit an exception when
the function is called with an empty queue.

After allocating the testing efforts based on fault prediction
techniques, we need to know how much test cases needed to be
executed within the predicted fault-prone components. Within
a software development process, software testing consumes
a longer time in execution and can be the most expensive
phase. Researchers have developed a number of regression
testing techniques [15] to reduce the time required to detect
regression. These techniques include test case selection, where
information about the current set of changes (i.e., the classes
or methods that were modiﬁed) is used to deﬁne a subset
of tests that can be used to detect possible regressions. Test
case minimization aims to ﬁnd test cases that are redundant or
irrelevant with respect to new tests that may perform similar
actions. Finally, Test-case prioritization aims to determine the
tests that are most likely to detect an error and to put them
in the ﬁrst priority in order to enable a quick detection of
potential regressions. The use of test case prioritization (TCP)
seems to improve the feasibility of tests in the software testing
activity [16]. Test case prioritization (TCP) techniques reduce
the cost of testing using all test cases. Costs are reduced
by parallelizing debugging and testing activities [17]. The
advantage of prioritizing test cases is that it enables continuous

testing until resources are consumed or until all test cases are
executed, although the most important test cases are always ex-
ecuted ﬁrst [18]. The techniques for prioritizing test cases are
classiﬁed into two groups: code-based and model-based tech-
niques. The code-based TCP techniques use the source code of
the application to prioritize Test Cases. The model-based TCP
techniques use the model that shows the required behavior of
the system to prioritize Test Cases. Numerous techniques are
proposed and developed for the prioritization of test cases.
The most commonly used approaches are coverage-based
[19], error-based, model-based, history-based, change-based,
similarity-based, gene-based, etc. [17], [20], [21]. However,
traceability links between code and test cases are not always
available or easily accessible when the test cases correspond
to system tests. For example, in system testing, Test Cases are
designed to test the whole system rather than simple units of
code. Therefore, test case selection and prioritization must be
handled differently, and the use of historical data on test case
failures and successes has been suggested as an alternative
[22]. Under the assumption that test cases that have failed in
the past are more likely to fail in the future, these test cases
are scheduled ﬁrst in new regression testing cycles for history-
based test case prioritization [23]. Since the time needed for
the test cases varies strongly, not all tests can be executed,
so that a test case selection is necessary. Although algorithms
have recently been proposed [23], [24], these two aspects of
regression testing, the test-case selection and the history-based
prioritization, can hardly be solved by using only non-adaptive
methods. Non-adaptive methods may not be able to detect
changes in the importance of some test cases compared to
others because they use systematic prioritization algorithms.
To overcome these problems, Spieker et al. [25] propose a new
lightweight test case selection and prioritization approach to
regression testing based on reinforcement learning and neural
networks. Reinforcement Learning is well adapted to design
an adaptive method that is able to learn from its experience
with the execution environment. This adaptive method can
gradually improve its efﬁciency by monitoring the effects of
its actions. By using a neural network that works with both the
selected test cases and the order in which they are executed,
the method tends to select and prioritize test cases that have
been successfully used to detect errors in previous regression
testing cycles and prioritize them so that the most promising
ones are executed ﬁrst. The use of machine learning in TCP
has become more and more interesting in the last years. By
using learning algorithms the system can use the knowledge
from previous tests and adapt to changing applications. Test
cases deﬁne the priority with continuous actions and machine
learning techniques offer easy adaptation to changes.

In this survey, we describe how defect prediction helps
to obtain an early estimate of software component’s fault-
proneness in order to guide software quality assurance towards
inspecting and testing the components most likely to contain
faults, with prioritizing the test cases. We review the different
common machine learning techniques and the different deep
learning technologies used in software quality assurance to

predict faults, and provide a survey on the state-of-the-art in
deep learning methods applied to software defect prediction.
We also investigate some of the studies that use machine
learning in the process of test case prioritization.

II. SOFTWARE DEFECT PREDICTION PROCESS

Fault prediction is an active research area in the ﬁeld
of software engineering. Many techniques and metrics have
been developed to improve fault prediction performance. In
recent decades, numerous studies have examined the realm of
software fault prediction. Figure 2 brieﬂy shows the history of
software fault prediction studies in about the last 20 years.

TABLE I
COMMON METRICS USED TO ASSESS THE PERFORMANCE OF
CLASSIFICATION MODELS

Metric

Formula

Accuracy

T P +T N
T P +T N +F P +F N

Precision

Recall

F1 score

T P
T P +F P

T P
T P +F N

2T P
2T P +F P +F N

Interpretation
Overall performance
of model
How accurate the positive
predictions are
Coverage of actual
positive sample
Hybrid metric useful for
unbalanced classes

model is the difference between the expected prediction and
the correct model that we try to predict for given data points.
(2) Variance: the variance of a model is the variability of
the model prediction for given data points. (3) Bias/variance
tradeoff : the simpler the model, the higher the bias, and the
more complex the model, the higher the variance. Figure 4
shows a brief summary of how underﬁtting, overﬁtting and a
suitable ﬁt looks like for the three commonly used techniques
regression, classiﬁcation and deep learning. Once the model
has been chosen, it is trained on the entire dataset and tested on
the test dataset. Most defect prediction approaches take defect
prediction as a binary classiﬁcation problem. After ﬁtting the
models, the test data is fed into the trained classiﬁer (the best-
ﬁt prediction model), which can predict whether the ﬁles are
buggy or clean. Afterwards, in order to assess the performance
of the selected model, quality metrics are computed. To have
a more complete picture when assessing the performance of a
model, a confusion matrix is used. It is deﬁned as shown in
Figure 5. We summarize the metrics for the performance of
classiﬁcation models in Table I.

Fig. 2. History of Software Defect Prediction

A. Within-Project Defect Prediction

As the process shows in Figure 3, the ﬁrst step is to collect
source code repositories from software archives. The second
step is to extract features from the source code repositories
and the commits contained therein. There are many traditional
features deﬁned in past studies, which can be categorized into
two kinds:

1) Metrics used as input for the common machine learning

techniques:
code metrics (e.g., McCabe features and CK features)
and process metrics (e.g., change histories).

2) Metrics used as input for the deep learning techniques:
an abstract representation of the source code (e.g., AST,
control ﬂow, data ﬂow, etc...)

The extracted features represent the train and test dataset. To
select the best-ﬁt defect prediction model, the most commonly
used method is called k-fold cross-validation that splits the
training data into k groups to validate the model on one
group while training the model on the k − 1 other groups,
all of this k times. The error is then averaged over the k
runs and is named cross-validation error. The diagnostics of
the model is based on these features: (1) Bias: the bias of a

Within-project defect prediction uses training data and test
data that are from the same project. Many machine learn-
ing algorithms have been adopted for within-project defect
prediction, including Support Vector Machines (SVM) [27],
Bayesian Belief Networks [28], Naive Bayes (NB) [29], De-
cision Trees (DT) [30], [31], [32], Neural Networks (NN) [33],
or Dictionary Learning [34]. Elish et al. [27] evaluated the fea-
sibility of SVM in predicting defect-prone software modules,
and they compared SVM against eight statistical and machine
learning models on four NASA datasets. Their results showed
that SVM is generally better than, or at least competitive with
other models, e.g., Logistic Regression, Bayesian techniques,
etc. Amasaki et al. [28] used a Bayesian Belief Network to
predict the ﬁnal quality of a software product. They evaluated
their approach on a closed project, and the results showed
that their proposed method can predict bugs that the Software
Reliability Growth Model (SRGM) cannot handle. Wang et
al. [32] and Khoshgoftaar et al. [31] examined the perfor-
mance of tree-based machine learning algorithms on defect
prediction. Their results indicate that tree-based algorithms can
generate good predictions. Tao et al. [29] proposed a Naive
Bayes based defect prediction model, and they evaluated the

Fig. 3. Software Defect Prediction Process

well as traditional features, they built prediction models by
using three typical machine learning algorithms, i.e., ADTree,
Naive Bayes, and Logistic Regression. Their experimental
results show that the learned DBN-based semantic features
consistently outperform the traditional defect prediction fea-
tures on these machine learning classiﬁers. Most of the above
approaches are designed for ﬁle-level defect prediction. For
change-level defect prediction, Mockus and Weiss [36] and
Kamei et al. [37] predicted the risk of a software change
by using change measures, e.g., the number of subsystems
touched, the number of ﬁles modiﬁed, the number of added
lines, and the number of modiﬁcation requests. Kim et al. [38]
used the identiﬁers in added and deleted source code and the
words in change logs to classify changes as being fault-prone
or not fault-prone. Jiang et al. [39] and Xia et al. [40] built
separate prediction models with characteristic features and
meta features for each developer to predict software defects
in changes. Tan et al. [41] improved change classiﬁcation
techniques and proposed online defect prediction models for
imbalanced data. Their approach uses time sensitive change
classiﬁcation to address the incorrect evaluation introduced by
cross-validation. McIntosh et al. [42] studied the performance
of change-level defect prediction as software systems evolve.
Change classiﬁcation can also predict whether a commit is
buggy or not [43], [44], [45]. In Wang et al.’s work [35],
they also compare the DBN-based semantic features with the
widely used change-level defect prediction features, and ther
results suggest that the DBN-based semantic features can also
outperform change-level features.

However, sufﬁcient defect data is often unavailable for many
projects and companies. This raises the need for cross-project
bug localization, i.e., the use of data from one project to help
locate bugs in another project.

B. Cross-Project Defect Prediction

Due to the lack of data, it is often difﬁcult to build accurate
models for new projects. Recently, more and more papers
studied the cross-project defect prediction problem, where the
training data and test data come from different projects.

Fig. 4. Fitting Model Diagnostics [26]

Fig. 5. Confusion Matrix

proposed approach on 11 datasets from the PROMISE defect
data repository. Their experimental results showed that the
Naive Bayes based defect prediction models could achieve
better performance than J48 (decision tree) based prediction
models. Jing et al. [34] introduced the dictionary learning
technique to defect prediction. Their cost-sensitive dictionary
learning based approach could signiﬁcantly improve defect
prediction in their experiments. Wang et al. [35] used a Deep
Belief Network (DBN) to generate semantic features for ﬁle-
level defect prediction tasks. In Wang et al.’s work [35], to
evaluate the performance of DBN-based semantic features as

Some studies ( [46], [47], [48]) have been done on eval-
uating cross-project defect prediction against within-project
defect prediction and show that cross-project defect prediction
is still a challenging problem. He et al. [49] showed the
feasibility to ﬁnd the best cross-project models among all
available models to predict defects on speciﬁc projects. Turhan
et al. [50] proposed a nearest-neighbor ﬁlter to improve cross-
project defect prediction. Zimmermann et al. [48] evaluated
the performance of cross-project defect prediction on 12
projects and their 622 combinations. They found that
the
defect prediction models at that time could not adapt well to
cross-project defect prediction. Li et al. [51] proposed defect
prediction via convolutional neural networks (DP-CNN). Their
work differs from the above-mentioned approaches in that they
utilize deep learning technique (i.e., CNN) to automatically
generate discriminative features from source code, rather than
manually designing features which can capture semantic and
structural
information of programs. Their features lead to
more accurate predictions. The state-of-the-art cross-project
defect prediction is proposed by Nam et al. [52], who adopted
a state-of-the-art transfer learning technique called Transfer
Component Analysis (TCA). They further improved TCA
as TCA+ by optimizing TCA’s normalization process. They
evaluated TCA+ on eight open-source projects, and the results
show that their approach signiﬁcantly improves cross-project
defect prediction. Xia et al. [40] proposed HYDRA, which
leverages a genetic algorithm and ensemble learning (EL)
to improve cross-project defect prediction. HYDRA requires
massive training data and a portion (5%) of labeled data from
test data to build and train the prediction models. TCA+
[52] and HYDRA [40] are the two state-of-the-art techniques
for cross-project defect prediction. However, in Wang et al.’s
work [53], they only use TCA+ as baseline for cross-project
defect prediction. This is because HYDRA requires that the
developers manually inspect and label 5% of the test data,
while in real-world practice, it is very expensive to obtain
labeled data from software projects, which requires the de-
velopers’ manually inspection, and the ground truth might
not be guaranteed. Most of the above existing cross-project
approaches are examined for ﬁle-level defect prediction only.
Recently, Kamei et al. [54] empirically studied the feasibility
of change level defect prediction in a cross-project context.
Wang et al. [53] examines the performance of Deep Be-
lief Network (DBN)-based semantic features on change-level
cross-project defect prediction tasks. The main differences
between this and existing approaches for within-project defect
prediction and cross-project defect prediction are as follows.
First, existing approaches to defect prediction are based on
manually encoded traditional features which are not sensitive
to the programs’ semantic information, while Wang et al.’s
approach automatically learns the semantic features using a
DBN and uses these features to perform defect prediction
tasks. Second, since Wang et al.’s method requires only the
source code of the training and test projects, it is suitable for
both within-project and cross-project defect prediction. The
semantic features can capture the common characteristics of

defects, which implies that the semantic features trained from
one project can be used to predict a different project, and thus
is applicable in cross-project defect prediction.

Deep learning-based approaches require only the source
code of the training and test projects, and are therefore suitable
for both within-project and cross-project defect prediction. In
the next session, we explain, based on recent research, how
effective and accurate fault-prediction models developed using
deep learning techniques are.

III. DEEP LEARNING IN SOFTWARE DEFECT PREDICTION

Recently, deep learning algorithms have been adopted to im-
prove research tasks in software engineering. The most popular
deep learning techniques are: Deep Belief Networks (DBN),
Recurrent Neural Networks, Convolutional Neural Networks
and Long Short Term Memory (LSTM), see Table II. Yang et
al. [55] propose an approach that leverages deep learning to
generate new features from existing ones and then use these
new features to build defect prediction models. Their work
was motivated by the weaknesses of logistic regression (LR),
which is that LR cannot combine features to generate new
features. They used a Deep Belief Network (DBN) to generate
features from 14 traditional change level features, including
the following: number of modiﬁed subsystems, modiﬁed direc-
tories, modiﬁed ﬁles, code added, code deleted, lines of code
before/after the change, ﬁles before and after the change, and
several features related to developers’ experience [55]. The
work of Wang et al. [53] differs from the above study mainly
in three aspects. First, they use a DBN to learn semantic
features directly from source code, while Yang et al. use
relations among existing features. Since the existing features
cannot distinguish between many semantic code differences,
the combination of these features would still fail to capture
semantic code differences. For example, if two changes add the
same line at different locations in the same ﬁle, the traditional
features cannot distinguish between the two changes. Thus,
the generated new features, which are combinations of the
traditional features, would also fail to distinguish between the
two changes.

How to explain deep learning results is still a challenging
question in the AI community. To interpret deep learning
models, Andrej et al. [63] used character level language mod-
els as an interpretable testbed to explain the representations
and predictions of a Recurrent Neural Network (RNN). Their
qualitative visualization experiments demonstrate that RNN
models could learn powerful and often interpretable long-range
interactions from real-world data. Radford et al. [64] focus
on understanding the properties of representations learned by
byte-level recurrent language models for sentiment analysis.
Their work reveals that there exists a sentiment unit in the
well-trained RNNs (for sentiment analysis) that has a direct
inﬂuence on the generative process of the model. Speciﬁcally,
simply ﬁxing its value to be positive or negative can generate
samples with the corresponding positive or negative sentiment.
The above studies show that to some extent deep learning
models are interpretable. However, these two studies focused

TABLE II
COMMON MACHINE LEARNING AND DEEP LEARNING TECHNIQUES USED IN SOFTWARE DEFECT PREDICTION

Techniques

Deﬁnition

RNN

LSTM

CNN

RNNs are called recurrent because
they perform the same task for
every element of a sequence,
with the output being depended on
the previous computations.

A long short-term memory (LSTM)
network is a type of RNN model
that avoids the vanishing gradient
problem by adding ’forget’ gates.
CNN is a class of deep neural network,
it uses convolution in place of general
matrix multiplication in at least one
of their layers.

Advantages
- Possibility of processing
input of any length
- Model size not increasing
with size of the input
- Computation takes into
account historical information

- Remembering information
for a long periods of time

- It automatically detects the
important features without any
human supervision.

Stacked
Auto-Encoder

A stacked autoencoder is a neural
network consist several layers of sparse
autoencoders where output of each hidden
layer is connected to the input of the
successive hidden layer.

- Possible use of pre-trained layers
from another model, to apply
transfer learning
- It does not require labeled inputs
to enable learning

DBN

DBN is an unsupervised probabilistic
deep learning algorithm.

- Only needs a small labeled dataset
- It is a solution to the
vanishing gradient problem

LR is used to describe data and
to explain the relationship between
one dependent binary variable and
independent variables.

- Easy to implement
- Very efﬁcient to train

Drawbacks
- Slow computation
- Difﬁculty of accessing
information from a
long time ago
- Cannot consider any future
input for the current state

- It takes longer to train
- It requires more memory
to train

Ref.

[56]

[57], [58]

- need a lot of training data.
- High computational cost.

[51], [59], [60]

- Computationally expensive
to train
- Extremely uninterpretable
- The underlying math is more
complicated
- Prone to overﬁtting, though
this can be mitigated
via regularization

- It overlooks the structural
information of programs

- It cannot combine different
features
to generate new features.
- It performs well only when
input features and output
labels are in linear relation

[61], [62]

[35]

[37]

SVM is a supervised learning model.
It can be used for both regression
and classiﬁcation tasks.

DT is a decision support tool that uses a
tree-like graph or model of decisions
and their possible consequences.

- Using different kernel function it
gives better prediction result
- Less computation power
Tree based methods empower
predictive models with high
accuracy, stability and
ease of interpretation.

- Not suitable for large number
of software metrics

[27]

- Construction of decision tree
is complex

[30], [31], [32]

Logistic
Regression

SVM

Decision Tree

on interpreting RNNs on text analysis. Wang et al. [53] lever-
ages a different deep learning model, Deep Belief Networks
(DBN), to analyze the ASTs of source code. The DBN adopts
different architectures and learning processes from RNNs. For
example, an RNN (e.g., LSTM) can,
in principle, use its
memory cells to remember long-range information that can be
used to interpret data it is currently processing, while a DBN
does not have such memory cells. Thus, it is unknown whether
DBN models share the same properties (w.r.t interpretability)
as RNNs. Many studies used a topic model [65] to extract
semantic features for different tasks in software engineering (
[66], [67], [68]). Nguyen et al. [67] leveraged a topic model to
generate features from source code for within-project defect
prediction. However, their topic model handles each source
ﬁle as an unordered token sequence. Thus,
the generated
information in a source
features cannot capture structural
ﬁle. A just-in-time defect prediction technique was proposed
by Kamei et al. which leverages the advantages of Logistic
Regression (LR) [37]. However, logistic regression has two
weaknesses. First, in logistic regression, the contribution of
each feature is calculated independently, which means that
LR cannot combine different features to generate new ones.
For example, given two features x and y,
if x × y is a
highly relevant feature, it is not enough to input only x and
y because logistic regression cannot generate the new feature
x × y. Second, logistic regression performs well only when
input features and output labels are in linear relation. Due to

these two weaknesses, the selection of input features becomes
crucial when using logistic regression. The bad selection
of features may result
in a non-linear relation for output
labels, leading to bad training performance or even training
failure. This severe problem leads some studies to adopt Deep
Belief Network (DBN), which is one of the state-of-the-art
deep learning approaches. The biggest advantage of DBN, as
shown in Table II, over logistic regression is that DBNs can
generate a more expressive feature set from the initial feature
set. We summarizes in Table II the most commonly used
machine learning and deep learning techniques in software
defect prediction.

IV. MACHINE LEARNING-DRIVEN TEST CASE
PRIORITIZATION

Test case prioritization aims to decrease the cost of
regression testing by ﬁnding a test case ordering that
maximizes the fault detection capability of the test suite. It
seeks to ﬁnd the optimal permutation of the sequence of test
cases. It does not involve selection of test cases, and assumes
that all the test cases may be executed in the order of the
permutation it produces, but that testing may be terminated
at some arbitrary point during the testing process. More
formally, the prioritization problem is deﬁned as follows [15]:

Deﬁnition: test case prioritization requirements:

T, a test suite

TABLE III
TEST CASES PRIORITIZATION APPROACHES USING MACHINE LEARNING TECHNIQUES

Techniques

Used Metrics

Neural Networks

Bayesian Network

Bayesian Network

Bayesian Network

- Duration of test cases,
- Last execution,
- History of failures

- Changes of the source code,
- Coverage degree,
- Fault-proneness of the software
- Quality metrics,
- Code change,
- Coverage degree
- Similarity on code coverage,
- Probability of failure

Genetic algorithm

- Code coverage

Genetic algorithm

- Code degree of statements,
- Multiple conditions

Genetic algorithm

SVM

SVM

K-means

- Fault proneness of software modules,
- Faults probability of each test case
- Coverage degree of requirements,
- Failure count,
- Failure age,
- Priority of failures,
- Cost of executing tests,
- Test case description
- Code coverage,
- Textual similarity between tests and changes,
- Test-failure history,
- Fault history,
- Test age
- Fault detection capability

Advantages

Test cases priorization done
with continuous actions,
Adaptive

Drawbacks
Needs a long
history of tests, needs
more data processing if
the test history is long

Hight value of APFD
(Average Percentage Faults Detected)

Requires a reasonable
number of available faults

Improves performance of detecting
faults earlier

Time-consuming

Achieves full coverage,
Faster fault detection

Adaptive, Simple

Considers severities of faults,
time of execution and structural
coverage
More efﬁcient than the
non-adaptive approach

Doesn’t use real faults

Do not scale well
with complexity

Do not scale well
with complexity

No optimization if there is
no time to execute test cases

Ref.

[25]

[78]

[79]

[80]

[81]

[82]

[73]

Failure detection rate is increased
compared to a random order

Based on human intuition

[83]

Convenient for industrial settings

Integration of multiple
techniques

[70]

Good for large scale testing

Dependent on initial values

[84]

P T , the set of permutations of T
f , a function that gives a numerical score for T (cid:48) ∈ P T
Problem:
Find T (cid:48) such that

(∀T (cid:48)(cid:48))(T (cid:48)(cid:48) ∈ P T )(T (cid:48)(cid:48) (cid:54)= T (cid:48))[f (T (cid:48)) ≥ f (T (cid:48)(cid:48))].

In order to maximize the fault detection capability of the test
suite, an appropriate f function must be chosen to select the
permutation T (cid:48) that ﬁnds all regressions as soon as possible.
Since we cannot know about either the existence or location of
faults prior to running the test suite, f can only be a surrogate
for actual fault detection. Most previous research on test case
prioritization has tried to ﬁnd an implementation of f that
most closely approximates fault detection, using a variety of
strategies [15], [69].

Machine learning for software testing: Machine learning
algorithms are increasingly being considered in the context of
software testing. Busjaeger and Xie [70] use machine learning
and multiple heuristic techniques to prioritize test cases in
an industrial environment. Through combining different data
sources and learning agnostic prioritization, this work makes
an important step towards deﬁning a general framework for
automatically learning the prioritization of test cases. Spieker
et al. [25] offer a lightweight learning method that uses a
single data source, namely the test case failure history based
on reinforcement learning and artiﬁcial neural networks. Chen
et al. [71] uses semi-supervised clustering for regres-sion test

selection. The drawback of this approach can be a higher
computational complexity. Some other approaches include
active learning for test classiﬁcation [72], the combination of
machine learning and program slicing for regression test case
prioritization [32], learning agent-based test case prioritization
[73], or clustering approaches [74]. Groce et al. [75] proposed
reinforcement learning (RL) for automated testing of soft-
ware APIs in combination with adaptation-based programming
(ABP). The combination of RL and ABP selects successive
calls to the API with the goal of increasing test coverage.
Moreover, Reichstaller et al. [76] use RL to generate test cases
for risk-based interoperability tests. We summarize in Table III
the approaches with the common used techniques in test cases
prioritization. A number of machine learning techniques are
used to prioritize test cases. The most commonly used tech-
niques are Neural Networks, Bayesian Networks and genetic
algorithms. Spieker et al. [25] show that neural networks are
very good classiﬁers and are used to group test cases based on
similar properties. This simpliﬁes the process of ordering test
cases for the test cases priorization process. Bayesian network
was used in numerous approaches [80], [79], [78], are well
adapted to calculate probabilities which are used to order test
cases. In addition, other researchers [73], [82], [81] use genetic
algorithms which order test cases that execute many iterations
with three steps each: selection of some of the test cases based
on a ﬁtness function that uses some predeﬁned characteristics
for selection; crossover and mutation. Neural networks and

Bayesian networks are very adaptable to the problem and
very efﬁcient in integrating many features. Numerous other
machine learning methods have also proven to be efﬁcient
in TCP. Support Vector Machines (SVM) were used in some
approaches [83], [70], SVM are also effective in integrating
different attributes while K-nearest neighbor, logistic regres-
sion and k-means can be used in clustering test cases [84].

V. CONCLUSION

With the ever-increasing scale and complexity of modern
software, software reliability assurance has become a sig-
niﬁcant challenge. To enhance the reliability of software,
we consider predicting potential code defects in software
implementations a beneﬁcial direction, which has the potential
to dramatically reduce the workload of software maintenance.
Speciﬁcally, we see the highest potential in a defect prediction
framework which utilizes deep learning algorithms for auto-
mated feature generation from source code with the semantic
and structural information preserved. Moreover, our survey
corroborates the feasibility of machine learning techniques in
the ﬁled of test cases prioritization.

REFERENCES

[1] R. Rana, M. Staron, J. Hansson, and M. Nilsson, “Defect prediction over
software life cycle in automotive domain state of the art and road map
for future,” in 9th International Conference on Software Engineering
and Applications (ICSOFT-EA), 2014.

[2] S. R. Chidamber and C. F. Kemerer, “A metrics suite for object oriented

design,” IEEE Trans. Softw. Eng., 1994.

[3] V. R. Basili, L. C. Briand, and W. L. Melo, “A validation of object-
oriented design metrics as quality indicators,” IEEE Trans. Softw. Eng.,
1996.

[4] L. C. Briand, J. W¨ust, S. V. Ikonomovski, and H. Lounis, “Investigating
quality factors in object-oriented designs: An industrial case study,” in
Proceedings of the 21st International Conference on Software Engineer-
ing, 1999.

[5] R. Subramanyam and M. S. Krishnan, “Empirical analysis of CK
metrics for object-oriented design complexity: Implications for software
defects,” IEEE Trans. Softw. Eng., 2003.

[6] M.-H. Tang, M.-H. Kao, and M.-H. Chen, “An empirical study on object-
oriented metrics,” in Proceedings of the 6th International Symposium on
Software Metrics, 1999.

[7] M. D’Ambros, M. Lanza, and R. Robbes, “Evaluating defect prediction
approaches: A benchmark and an extensive comparison,” Empirical
Softw. Engg., 2012.

[8] R. Moser, W. Pedrycz, and G. Succi, “A comparative analysis of
the efﬁciency of change metrics and static code attributes for defect
prediction,” in Proceedings of the 30th International Conference on
Software Engineering, 2008.

[9] S. Kim, T. Zimmermann, E. J. Whitehead Jr., and A. Zeller, “Predicting
faults from cached history,” in Proceedings of the 29th International
Conference on Software Engineering, 2007.

[10] N. Nagappan, T. Ball, and A. Zeller, “Mining metrics to predict compo-
nent failures,” in Proceedings of the 28th International Conference on
Software Engineering, 2006.

[11] S. Omri, P. Montag, and C. Sinz, “Static analysis and code complexity
metrics as early indicators of software defects,” Journal of Software
Engineering and Applications, 2018.

[12] T. M. Khoshgoftaar, E. B. Allen, N. Goel, A. Nandi, and J. McMullan,
“Detection of software modules with high debug code churn in a very
large legacy system,” in Proceedings of the The Seventh International
Symposium on Software Reliability Engineering, 1996.

[14] S. Omri, C. Sinz, and P. Montag, “An enhanced fault prediction model
for embedded software based on code churn, complexity metrics, and
static analysis results.”
ICSEA 2019 : The Fourteenth International
Conference on Software Engineering Advances.

[15] S. Yoo and M. Harman, “Regression testing minimization, selection and

prioritization: A survey,” Softw. Test. Verif. Reliab., p. 67–120, 2012.

[16] G. Rothermel, R. H. Untch, Chengyun Chu, and M. J. Harrold,
“Test case prioritization: an empirical study,” in Proceedings IEEE
International Conference on Software Maintenance - 1999 (ICSM’99).
’Software Maintenance for Business Change’ (Cat. No.99CB36360),
1999, pp. 179–188.

[17] H. Do, S. Mirarab, L. Tahvildari, and G. Rothermel, “The effects
of time constraints on test case prioritization: A series of controlled
experiments,” IEEE Transactions on Software Engineering, pp. 593–
617, 2010.

[18] G. Rothermel, R. H. Untch, Chengyun Chu, and M. J. Harrold, “Priori-
tizing test cases for regression testing,” IEEE Transactions on Software
Engineering, pp. 929–948, 2001.

[19] D. Nardo, N. Alshahwan, L. Briand, and Y. Labiche, “Coverage-
based regression test case selection, minimization and prioritization: A
case study on an industrial system,” Software Testing, Veriﬁcation and
Reliability, 2015.

[20] N. G¨okc¸e, F. Belli, M. Eminli, and B. Dincer, “Model-based test
case prioritization using cluster analysis: A soft-computing approach,”
Turkish Journal of Electrical Engineering and Computer Sciences, 2014.
[21] M. Khatibsyarbini, M. Isa, D. Jawawi, and R. Tumeng, “Test case
prioritization approaches in regression testing: A systematic literature
review,” Information and Software Technology, 2017.

[22] Jung-Min Kim and A. Porter, “A history-based test prioritization tech-
nique for regression testing in resource constrained environments,” in
Proceedings of the 24th International Conference on Software Engi-
neering. ICSE 2002, 2002, pp. 119–129.

[23] D. Marijan, A. Gotlieb, and S. Sen, “Test case prioritization for
continuous regression testing: An industrial case study,” in 2013 IEEE
International Conference on Software Maintenance, 2013, pp. 540–543.
[24] T. B. Noor and H. Hemmati, “A similarity-based approach for test
case prioritization using historical failure data,” in 2015 IEEE 26th
International Symposium on Software Reliability Engineering (ISSRE),
2015, pp. 58–68.

[25] H. Spieker, A. Gotlieb, D. Marijan, and M. Mossige, “Reinforcement
learning for automatic test case prioritization and selection in continuous
integration.” Association for Computing Machinery, 2017.

[26] A.

Amidi,
[Online]. Available:
cheatsheet-machine-learning-tips-and-tricks

cheatsheet-machine-learning-tips-and-tricks,

2018.
https://stanford.edu/∼shervine/teaching/cs-229/

[27] K. O. Elish and M. O. Elish, “Predicting defect-prone software modules

using support vector machines,” J. Syst. Softw., 2008.

[28] S. Amasaki, Y. Takagi, O. Mizuno, and T. Kikuno, “A bayesian belief
network for assessing the likelihood of fault content,” in Proceedings of
the 14th International Symposium on Software Reliability Engineering,
2003.

[29] T. Wang and W. Li, “Naive bayes software defect prediction model,”
in 2010 International Conference on Computational Intelligence and
Software Engineering.

[30] N. Gayatri, N. Savarimuthu, and A. Reddy, “Feature selection using
decision tree induction in class level metrics dataset for software defect
predictions,” Lecture Notes in Engineering and Computer Science, 2010.
[31] T. M. Khoshgoftaar and N. Seliya, “Tree-based software quality estima-
tion models for fault prediction,” in Proceedings of the 8th International
Symposium on Software Metrics, 2002.

[32] J. Wang, B. Shen, and Y. Chen, “Compressed c4.5 models for software
the 2012, 12th International

defect prediction,” in Proceedings of
Conference on Quality Software.

[33] Elhampaikari, M. M.richter, and Guentherruhe, “Defect prediction using
case-based reasoning: an attribute weighting technique based upon
sensitivity analysis in neural network,” International Journal of Software
Engineering and Knowledge Engineering, 2012.

[34] X.-Y. Jing, S. Ying, Z.-W. Zhang, S.-S. Wu, and J. Liu, “Dictionary
learning based software defect prediction,” in Proceedings of the 36th
International Conference on Software Engineering, 2014.

[13] N. Nagappan and T. Ball, “Use of relative code churn measures to
predict system defect density,” in Proceedings of the 27th International
Conference on Software Engineering, 2005.

[35] S. Wang, T. Liu, and L. Tan, “Automatically learning semantic features
for defect prediction,” in Proceedings of the 38th International Confer-
ence on Software Engineering, 2016.

[36] A. Mockus and D. M. Weiss, “Predicting risk of software changes,” Bell

[60] A. Phan, L. Nguyen, and L. Bui, “Convolutional neural networks over

Labs Technical Journal, 2000.

[37] Y. Kamei, E. Shihab, B. Adams, A. E. Hassan, A. Mockus, A. Sinha,
and N. Ubayashi, “A large-scale empirical study of just-in-time quality
assurance,” IEEE Trans. Softw. Eng., 2013.

[38] S. Kim, E. J. Whitehead, and Y. Zhang, “Classifying software changes:

Clean or buggy?” IEEE Trans. Softw. Eng., 2008.

[39] T. Jiang, L. Tan, and S. Kim, “Personalized defect prediction,” in Pro-
ceedings of the 28th IEEE/ACM International Conference on Automated
Software Engineering, 2013.

[40] X. Xia, D. Lo, X. Wang, and X. Yang, “Collective personalized
change classiﬁcation with multiobjective search,” IEEE Transactions on
Reliability, 2016.

[41] M. Tan, L. Tan, S. Dara, and C. Mayeux, “Online defect prediction for
imbalanced data,” in Proceedings of the 37th International Conference
on Software Engineering, 2015.

[42] S. McIntosh and Y. Kamei, “Are ﬁx-inducing changes a moving target? a
longitudinal case study of just-in-time defect prediction,” in Proceedings
of the 40th International Conference on Software Engineering, 2018.

[43] H. Perl, S. Dechand, M. Smith, D. Arp, F. Yamaguchi, K. Rieck, S. Fahl,
and Y. Acar, “Vccﬁnder: Finding potential vulnerabilities in open-source
projects to assist code audits,” in Proceedings of the 22nd ACM SIGSAC
Conference on Computer and Communications Security, 2015.

[44] L. Prechelt and A. Pepper, “Why software repositories are not used for
defect-insertion circumstance analysis more often: A case study,” Inf.
Softw. Technol., 2014.

[45] A. Habib and M. Pradel, “Neural bug ﬁnding: A study of opportunities

and challenges,” CoRR, 2019.

[46] B. A. Kitchenham, E. Mendes, and G. H. Travassos, “Cross versus
within-company cost estimation studies: A systematic review,” IEEE
Trans. Softw. Eng., 2007.

[47] T. Menzies, Z. Milton, B. Turhan, B. Cukic, Y. Jiang, and A. Bener,
“Defect prediction from static code features: Current results, limitations,
new approaches,” Autom. Softw. Eng., 2010.

[48] T. Zimmermann, N. Nagappan, H. Gall, E. Giger, and B. Murphy,
“Cross-project defect prediction: A large scale experiment on data vs.
domain vs. process,” in Proceedings of the 7th Joint Meeting of the
European Software Engineering Conference and the ACM SIGSOFT,
2009.

[49] Z. He, F. Peters, T. Menzies, and Y. Yang, “Learning from open-
source projects: An empirical study on defect prediction,” in ACM
IEEE International Symposium on Empirical Software Engineering and
Measurement, 2013.

[50] B. Turhan, T. Menzies, A. B. Bener, and J. Di Stefano, “On the relative
value of cross-company and within-company data for defect prediction,”
Empirical Softw. Engg., 2009.

[51] J. Li, P. He, J. Zhu, and M. R. Lyu, “Software defect prediction via
convolutional neural network,” in IEEE International Conference on
Software Quality, Reliability and Security (QRS), 2017.

[52] J. Nam, S. J. Pan, and S. Kim, “Transfer defect learning,” in Proceedings
of the International Conference on Software Engineering, 2013.
[53] S. Wang, T. Liu, J. Nam, and L. Tan, “Deep semantic feature learning for
software defect prediction,” IEEE Transactions on Software Engineering,
2018.

[54] Y. Kamei, T. Fukushima, S. Mcintosh, K. Yamashita, N. Ubayashi,
and A. E. Hassan, “Studying just-in-time defect prediction using cross-
project models,” Empirical Softw. Engg., 2016.

[55] X. Yang, D. Lo, X. Xia, Y. Zhang, and J. Sun, “Deep learning for
just-in-time defect prediction,” in Proceedings of the IEEE International
Conference on Software Quality, Reliability and Security, 2015.
[56] J. Wang and C. Zhang, “Software reliability prediction using a deep
learning model based on the RNN encoder-decoder,” Reliab. Eng. Syst.
Saf., 2018.

[57] H. K. Dam, T. Pham, S. W. Ng, T. Tran, J. Grundy, A. Ghose, T. Kim,
and C.-J. Kim, “Lessons learned from using a deep tree-based model
for software defect prediction in practice,” in Proceedings of the 16th
International Conference on Mining Software Repositories, 2019.
[58] K. H. Dam, T. Pham, S. W. Ng, T. Tran, J. Grundy, A. K. Ghose, T. Kim,
and C.-J. Kim, “A deep tree-based model for software defect prediction,”
ArXiv, 2018.

[59] L. Mou, G. Li, L. Zhang, T. Wang, and Z. Jin, “Convolutional neural
networks over tree structures for programming language processing,” in
Proceedings of the Thirtieth AAAI Conference on Artiﬁcial Intelligence,
2016.

control ﬂow graphs for software defect prediction,” 2018.

[61] C. Manjula and L. Florence, “Deep neural network based hybrid
approach for software defect prediction using software metrics,” Cluster
Computing, 2019.

[62] H. Tong, B. Liu, and S. Wang, “Software defect prediction using stacked
denoising autoencoders and two-stage ensemble learning,” Information
and Software Technology, 2017.

[63] A. Karpathy, J. Johnson, and F. F. Li, “Visualizing and understanding

recurrent networks,” Cornell Univ. Lab., 2015.

[64] A. Radford, R. Jozefowicz, and I. Sutskever, “Learning to generate

reviews and discovering sentiment,” 2017.

[65] D. M. Blei, A. Y. Ng, and M. I. Jordan, “Latent dirichlet allocation,” J.

Mach. Learn. Res., 2003.

[66] T.-H. Chen, S. W. Thomas, M. Nagappan, and A. E. Hassan, “Explaining
software defects using topic models,” in Proceedings of the 9th IEEE
Working Conference on Mining Software Repositories, 2012.

[67] T. T. Nguyen, T. N. Nguyen, and T. M. Phuong, “Topic-based defect
prediction (nier track),” in Proceedings of the 33rd International Con-
ference on Software Engineering, 2011.

[68] X. Xie, W. Zhang, Y. Yang, and Q. Wang, “Dretom: Developer recom-
mendation based on topic models for bug resolution,” in Proceedings
of the 8th International Conference on Predictive Models in Software
Engineering, 2012.

[69] A. Gonzalez-Sanchez, R. Abreu, H. Gross, and A. J. C. van Gemund,
“Prioritizing tests for fault localization through ambiguity group reduc-
tion,” in 2011 26th IEEE/ACM International Conference on Automated
Software Engineering (ASE 2011), 2011, pp. 83–92.

[70] B. Busjaeger and T. Xie, “Learning for test prioritization: An industrial
case study,” in Proceedings of the 2016 24th ACM SIGSOFT Interna-
tional Symposium on Foundations of Software Engineering, 2016, p.
975–980.

[71] S. Chen, Z. Chen, Z. Zhao, B. Xu, and Y. Feng, “Using semi-supervised
clustering to improve regression test selection techniques,” in 2011
Fourth IEEE International Conference on Software Testing, Veriﬁcation
and Validation, 2011, pp. 1–10.

[72] J. F. Bowring, J. M. Rehg, and M. J. Harrold, “Active learning for
automatic classiﬁcation of software behavior,” SIGSOFT Softw. Eng.
Notes, p. 195–205, Jul. 2004.

[73] S. Abele and P. G¨ohner, “Improving proceeding test case prioritization
with learning software agents,” in Proceedings of the 6th International
Conference on Agents and Artiﬁcial
- Volume 2.
Setubal, PRT: SCITEPRESS - Science and Technology Publications,
Lda, 2014, p. 293–298. [Online]. Available: https://doi.org/10.5220/
0004920002930298

Intelligence

[74] G. Chaurasia, S. Agarwal, and S. S. Gautam, “Clustering based novel
test case prioritization technique,” in 2015 IEEE Students Conference
on Engineering and Systems (SCES), 2015, pp. 1–5.

[75] A. Groce, A. Fern, J. Pinto, T. Bauer, A. Alipour, M. Erwig, and
C. Lopez, “Lightweight automated testing with adaptation-based pro-
gramming,” in 2012 IEEE 23rd International Symposium on Software
Reliability Engineering, 2012, pp. 161–170.

[76] A. Reichstaller, B. Eberhardinger, A. Knapp, W. Reif, and M. Gehlen,
“Risk-based interoperability testing using reinforcement learning,” 2016.
[77] M. Veanes, P. Roy, and C. Campbell, “Online testing with reinforcement

learning,” 2006, pp. 240–253.

[78] S. Mirarab and L. Tahvildari, “A prioritization approach for software test
cases based on bayesian networks,” in Proceedings of the 10th Interna-
tional Conference on Fundamental Approaches to Software Engineering,
2007, p. 276–290.

[79] T. Joachims, “Optimizing search engines using clickthrough data,” in
Proceedings of the Eighth ACM SIGKDD International Conference on
Knowledge Discovery and Data Mining, 2002, p. 133–142.

[80] X. Zhao, Z. Wang, X. Fan, and Z. Wang, “A clustering-bayesian network
based approach for test case prioritization,” in 2015 IEEE 39th Annual
Computer Software and Applications Conference, 2015, pp. 542–547.

[81] P. Konsaard and L. Ramingwong, “Total coverage based regression test
case prioritization using genetic algorithm,” in 2015 12th International
Conference on Electrical Engineering/Electronics, Computer, Telecom-
munications and Information Technology (ECTI-CON), 2015, pp. 1–6.
[82] A. A. Ahmed, M. Shaheen, and E. Kosba, “Software testing suite priori-
tization using multi-criteria ﬁtness function,” in 2012 22nd International
Conference on Computer Theory and Applications (ICCTA), 2012, pp.
160–166.

[83] R. Lachmann, S. Schulze, M. Nieke, C. Seidl, and I. Schaefer, “System-
level test case prioritization using machine learning,” in 2016 15th
IEEE International Conference on Machine Learning and Applications
(ICMLA), 2016, pp. 361–368.

[84] J. Chen, L. Zhu, T. Chen, D. Towey, F.-C. Kuo, R. Huang, and Y. Guo,
“Test case prioritization for object-oriented software: An adaptive ran-
dom sequence approach based on clustering,” Journal of Systems and
Software, 2017.

