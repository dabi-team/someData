Towards Mixed Optimization for
Reinforcement Learning with Program Synthesis

Surya Bhupatiraju * 1 Kumar Krishna Agrawal * 1 Rishabh Singh 1

8
1
0
2

l
u
J

3

]

G
L
.
s
c
[

2
v
3
0
4
0
0
.
7
0
8
1
:
v
i
X
r
a

Abstract

Deep reinforcement learning has led to several
recent breakthroughs, though the learned policies
are often based on black-box neural networks.
This makes them difﬁcult to interpret, and to
impose desired speciﬁcation or constraints dur-
ing learning. We present an iterative framework,
MORL, for improving the learned policies using
program synthesis. Concretely, we propose to use
synthesis techniques to obtain a symbolic repre-
sentation of the learned policy, which can be de-
bugged manually or automatically using program
repair. After the repair step, we distill the policy
corresponding to the repaired program, which is
further improved using gradient descent. This pro-
cess continues until the learned policy satisﬁes the
constraints. We instantiate MORL for the simple
CartPole problem and show that the programmatic
representation allows for high-level modiﬁcations
which in turn leads to improved learning of the
policies.

1. Introduction

There have been many recent successes in using deep re-
inforcement learning (DRL) to solve challenging problems
such as learning to play Go and Atari games (Silver et al.,
2016; 2017; Mnih et al., 2015). While the effectiveness of
reinforcement learning methods in these domains has been
impressive, they have some shortcomings. These learned
policies are based on black-box deep neural networks which
are difﬁcult to interpret. Additionally, it is challenging to
impose and validate certain desirable policy speciﬁcations,
such as worst-case guarantees or safety constraints. This
makes it difﬁcult to debug and improve these policies, there-
fore hindering their use for safety-critical domains.

*Equal contribution 1Google Brain, USA. Correspondence to:
Surya Bhupatiraju <sbhupatiraju@google.com>, Kumar Krishna
Agrawal <agrawalk@google.com>.

Published at the ICML workshop Neural Abstract Machines &
Program Induction v2 (NAMPI) — Workshop Paper, Stockholm,
Sweden, 2018. Copyright 2018 by the author(s).

There has been some recent work on using program synthe-
sis techniques to interpret learned policies using higher-level
programs (Verma et al., 2018) and decision trees (Bastani
et al., 2018). The key idea in PIRL (Verma et al., 2018)
is to ﬁrst train a DRL policy using standard methods and
then use an imitation learning-like approach to search for a
program in a domain-speciﬁc language (DSL) that conforms
to the behavior traces sampled from the policy. Similarly,
VIPER (Bastani et al., 2018) uses imitiation learning (a mod-
iﬁed form of the DAGGER algorithm (Ross et al., 2011)) to
extract a decision tree corresponding to the learned policy.
The main goal of these works is to extract a symbolic high-
level representation of the policy (as a DSL program or a
decision tree) which is more interpretable and also amenable
for program veriﬁcation techniques.

We build upon these recent advances to propose an iterative
framework for learning interpretable and safe policies. The
main steps in the workﬂow of our framework are as follows.
We start with a random initial policy π0. We use program
synthesis techniques similar to PIRL and VIPER to learn a
symbolic representation of the learned policy as a program
P0. After obtaining a programmatic representation of the
policy, we perform program repair (Weimer et al., 2009; Job-
stmann et al., 2005) to obtain a repaired program P (cid:48)
0 that sat-
isﬁes some set of constraints. Note that the program repair
step can be performed either automatically using a safety
speciﬁcation constraint or it can be performed manually by
a human expert that modiﬁes P0 to remove undesirable be-
haviors (or add desired behaviors). We then use behavioral
cloning (Bratko et al., 1995) to obtain the corresponding
improved policy π(cid:48)
0, which is then further improved using
standard gradient descent to obtain π1. This process of im-
proving policies from πt → Pt → P (cid:48)
t → πt+1 is
repeated until achieving desirable performance and safety
guarantees. We name this iterative procedure a mixed opti-
mization scheme for reinforcement learning, or MORL.

t → π(cid:48)

As a ﬁrst step towards a full realization of MORL, we
present a simple instantiation of our framework for the
CartPole (Barto et al., 1983) problem. We demonstrate
the efﬁcacy of our approach to learn near-optimal policies,
while enabling the user to better interpret the learned policy.
In addition, we argue that the scheme has a natural interpre-

 
 
 
 
 
 
Towards Mixed Optimization for Reinforcement Learning with Program Synthesis

Figure 1. An overview of the proposed method. We decompose policy learning into alternating between policy optimization and program
repair. Starting from a black-box policy πt, we consider the following steps (1) Synthesis, which generates a program Pt corresponding
to the policy πt. The program is sampled from an underlying Domain Speciﬁc Language (DSL) D (2) Repair, which corresponds to
debugging the program ,allowing us to impose high-level constraints on the learned program. (3) Imitation corresponds to distilling the
program back into a reactive representation. (4) Policy Optimization in this case corresponds to gradient-based policy optimization.

tation and can be readily extended to capture more notions
of policy improvement and discuss the potential beneﬁts
and obstacles of using such an approach.

This paper makes the following key contributions:

• We propose a simple framework for iterative policy
reﬁnement by performing repair at the level of pro-
grammatic representation of learned policies.

• We instantiate the framework for the CartPole problem
and show the effectiveness of performing modiﬁcations
in the symbolic representation.

2. Mixed Optimization for Reinforcement

Learning

Our goal is to improve policy learning by decomposing the
usual gradient-based optimization scheme into an iterative
two-stage algorithm. In this context, we view improvement
as either making the policies (1) safe – to ensure perfor-
mance under safety, (2) interpretable – allowing some level
of introspection into the policy’s decisions, (3) sample ef-
ﬁcient, or (4) alignment with priors. While there are other
notions of improvement, for the remainder of the paper, we
focus on sample efﬁciency as a notion of policy improve-
ment. We include a discussion of the other approaches as
they apply to our framework.

2.1. Problem Deﬁnition

Consider the typical Markov decision process (MDP) setup
(S, A, R, T , ρ0, γ), with a state space S, an action space
A, a reward function R, the transition dynamics of the
environment T , the initial starting state distribution ρ0, and
the discount factor γ. The goal will be to ﬁnd a policy, or
function π : S → A, that achieves the maximum expected
reward. Normally, the reward design and speciﬁcation for a
task T corresponds to deﬁning the reward function R(s, a),
such that an optimal policy π∗ solves the task.

An alternative view of solving the task could be deﬁned as
having access to an oracle policy π or a ﬁxed number of
trajectories from it. In this setting, our goal is learning a
policy by imitation learning, which would also equivalently
solve the task. In this work, we focus on improving policy
learning using imitation learning (Abbeel & Ng, 2004; Ho
& Ermon, 2016), though the framework is more general and
extends well to reinforcement learning.

We consider a symbolic representation D (such as a DSL)
that is expressive enough to represent different policies. The
synthesis problem can then be deﬁned as learning a program
P ∈ D such that ∀s ∈ S : π(s) ≈ P (s), i.e. the learned
program P produces approximately the same output actions
as the actions produced by the policy π for all (or a sampled
set of) input states S.

2.2. Model

In MORL we maintain two representations of a policy:

• a reactive, black-box policy where we represent the
policy as a differentiable function, such as a neural net-
work, allowing us to use gradient-based optimization
methods like TRPO (Schulman et al., 2015) or PPO
(Schulman et al., 2017).

• a symbolic program, which represents the policy as
an interpretable program. The symbolic program rep-
resentations are amenable for analysis and transforma-
tions using automated program veriﬁcation and repair
techniques, or human inspection.

With these intermediate representations, we alternate be-
tween the following; the ﬁrst step allows us to ﬁnetune poli-
cies in function space and the second allows us to impose
constraints or incorporate human debugging. The procedure
(Fig 1) consists of four key steps, as detailed below.

Synthesis: Given a task T, we consider a Domain Speciﬁc
Language D, such that there exists some program P ∈ D

Towards Mixed Optimization for Reinforcement Learning with Program Synthesis

Figure 2. Evaluating the usefulness of maintaining differentiable,
and symbolic representations of the policy. Each plot corre-
sponds to ﬁnetuning a policy cloned from a program (in this
case decision trees) with TRPO (averaged over 5 runs). In this
case, Near-Optimal is obtained by manual debugging of the
Intermediate policy, which is obtained from Worst policy.

Figure 3. An important step in the algorithm is alternating between
symbolic and policy representations. Here we plot the convergence
rate of randomly initialized policies to the program behavior. In
this work, we used simple behavioral cloning to retrain the policies.
We note that more sample efﬁcient algorithms would be able to
emulate the behavior from the program more quickly.

that is a sufﬁcient representation of the task. In the ﬁrst
step of MORL, we seek to synthesize such a program that
is equivalent to the policy π. A programmatic representa-
tion of the policy allows us to leverage approaches such as
program repair and veriﬁcation to provide guarantees for
the underlying policy. For this step, and in the scope of
this paper, we assume that we can utilize existing program
synthesis methods such as VIPER or PIRL, so we do not
attempt to perform this step explicitly. We focus on the
following steps in the MORL scheme.

Repair: In this step, we modify the synthesized program
accordingly to satisfy constraints imposed either on D, or
on the synthesized program P . This step allows us to mean-
ingfully debug the policy, either through human-in-the-loop
veriﬁcation for interpretability, or through automated pro-
gram repair techniques that involve deﬁning Constraint Sat-
isfaction Problems (CSP) typically solved using SAT/SMT
solvers (Singh et al., 2013). For the scope of this paper,
we mimic the repair process by manually modifying the
initial program to obtain three programs that achieve three
different levels of success at the task of interest.

Imitation: Following the program synthesis and repair
steps, we distill (Rusu et al., 2015) the program back into a
reactive policy using imitation learning. Given that we have
access to an oracle P
t , we ﬁnd that we reliably imitate the
program (Ross et al., 2011). Note that it is possible to stop
the optimization here. Indeed, we observe that a user may
end the procedure of MORL here, if certain performance or
safety bounds have been reached, and may skip the last step.

(cid:48)

Policy Optimization Finally, we ﬁnetune the policy using
gradient descent. We posit that by optimizing in both pro-
gram space and over the space of policies in a differentiable
space, we are able to better escape local minima while still
maintaining an underlying intuition for how the policy is
performing from the inspection of the program.

3. Experiments

We evaluate our framework on the CartPole-v0 problem in
the OpenAI Gym environment for discrete control (Brock-
man et al., 2016). We present a ﬁrst simple instantiation
of the framework to showcase its usefulness compared to
direct reinforcement learning. In our preliminary evaluation,
we evaluate the following research questions:

• Does program repair lead to faster convergence?

• Does programmatic representation help humans pro-

vide better repair insights?

To this effect, we train an initial policy π0 (Worst) that
performs poorly, and then extract the corresponding sym-
bolic representation P0. For the symbolic representation,
we chose VIPER’s (Bastani et al., 2018) decision tree
representation of the policy. We then modify the symbolic
program to get a new program P (cid:48)
0, which performs better
than the original program by repairing certain values in
the decision tree. This is followed by behavioural cloning
to obtain π(cid:48)
0), which is optimized to
obtain π1.

0 (corresponding to P (cid:48)

Towards Mixed Optimization for Reinforcement Learning with Program Synthesis

PIRL ﬁrst trains a DRL policy for a domain and then uses
an imitation learning like approach to generate speciﬁca-
tions (input-output behaviors) for the synthesis problem. It
then uses a Bayesian optimization technique to search for
programs in a DSL that conforms to the speciﬁcation. It
iteratively builds up new behaviors by executing the initial
policy as an oracle to obtain outputs for inputs that were not
originally sampled but are observed in executing the learnt
programs. It maintains a family of programs consistent with
the speciﬁcation and chooses the one as output that achieves
the maximum reward on the task.

VIPER uses a modiﬁed form of the DAGGER initiation learn-
ing algorithm to extract a decision tree corresponding to the
learnt policy. It then uses program veriﬁcation techniques
to validate correctness, stability, and robustness properties
of the extracted programs (represented as decision trees).

While previous approaches stop at learning a veriﬁable sym-
bolic representation of policies, our framework aims at iter-
ative improvement of policies. In particular, if the extracted
symbolic program does not satisfy certain desirable veriﬁca-
tion constraints, unlike previous approaches, our framework
allows for repairing the programs in symbolic space and
distilling the programs to policies for further optimization.

5. Discussion and Future Work

We presented a preliminary instantiation of the MORL frame-
work showing the beneﬁts of learning a symbolic represen-
tation of the policy. Namely, that by optimizing the policy
by iterating between two representations, we were able to
converge faster to near-optimal performance starting with a
poor initialization.

There are a number of assumptions we make in this paper
in order to instantiate our framework. While the MORL
framework is general enough to encapsulate many differ-
ent approaches of synthesis, repair, and imitation, we only
consider the simplest forms of these. For instance, we hand-
design the candidate repaired programs, and use a simple
supervised approach for imitation learning. Each of these
aspects could be signiﬁcantly scaled up to be used for larger
programs and for more complicated tasks. While CartPole
was a simple sandbox for which we could test symbolic
programs, for more complicated tasks, automated program
repair and veriﬁcation techniques would be more efﬁcient.

Reward design (Clark & Amodei, 2016) and safety
(Hadﬁeld-Menell et al., 2017) is another exciting research
direction. Note that we can instead use the reward func-
tion R as the program representation for MORL; this would
instead provide a procedure for more interpretable or veriﬁ-
able inverse reinforcement learning.

Figure 4. Debugging Worst (red) to Intermediate (green).
In one step of debugging the policy, we ﬁx the policy to make the
cart shift in the same direction as the pole.

To simulate the iterative optimization of the framework,
we perform two different modiﬁcations of the program
repair step to obtain P 1
(Intermediate) and P 2
0
0
(Near-optimal) that have different characteristics in
terms of repair improvements. For example, the modiﬁ-
cation to obtain program P 1
0 from P0 is shown in Fig 4,
where we manually provide the insight of making the cart
shift in the same direction as the pole.

In our experiments, we ﬁrst ﬁnd the average perfor-
mance of each of the levels of policies across 25 runs.
The Worst policy gets an average reward of 9.28, the
Intermediate policy gets an average reward of 104.0,
and the Near-optimal policy gets an average reward of
200.0. When we attempt to distill the programs to continu-
ous policies π, we ﬁnd that each of the resulting levels of
policies get 10.64, 66, and 185, respectively, as shown in
Figure 3 after 15000 epochs. Lastly, when we take the result-
ing distilled policies and then ﬁnetune these with TRPO, we
ﬁnd that the resulting average rewards are 38.65, 79.03, and
and 176.8 after 25 episodes of training with 10 trajectories
of length 200. In Figure 2, we run TRPO for a total of 250
episodes to see the limiting behavior.

From our results, we validate our hypothesis that under bad
initialization (Worst), TRPO takes an order of magnitude
longer to converge to near-optimal policy, when compared
to policies initialized after program repair. We believe that
providing high-level insights programmatically can help
policies discover better or safer behaviors.

4. Related Work

Our framework is inspired from the recent works of
PIRL (Verma et al., 2018) and VIPER (Bastani et al., 2018)
in using program synthesis to learn symbolic interpretable
representations of learnt policies, and then using program
veriﬁcation to verify certain properties of the program.

Towards Mixed Optimization for Reinforcement Learning with Program Synthesis

Rusu, Andrei A, Colmenarejo, Sergio Gomez, Gulcehre,
Caglar, Desjardins, Guillaume, Kirkpatrick, James, Pas-
canu, Razvan, Mnih, Volodymyr, Kavukcuoglu, Koray,
and Hadsell, Raia. Policy distillation. arXiv preprint
arXiv:1511.06295, 2015.

Schulman, John, Levine, Sergey, Abbeel, Pieter, Jordan,
Michael, and Moritz, Philipp. Trust region policy opti-
mization. In International Conference on Machine Learn-
ing, pp. 1889–1897, 2015.

Schulman, John, Wolski, Filip, Dhariwal, Prafulla, Radford,
Alec, and Klimov, Oleg. Proximal policy optimization
algorithms. arXiv preprint arXiv:1707.06347, 2017.

Silver, David, Huang, Aja, Maddison, Chris J., Guez,
Arthur, Sifre, Laurent, van den Driessche, George, Schrit-
twieser, Julian, Antonoglou, Ioannis, Panneershelvam,
Vedavyas, Lanctot, Marc, Dieleman, Sander, Grewe, Do-
minik, Nham, John, Kalchbrenner, Nal, Sutskever, Ilya,
Lillicrap, Timothy P., Leach, Madeleine, Kavukcuoglu,
Koray, Graepel, Thore, and Hassabis, Demis. Mastering
the game of go with deep neural networks and tree search.
Nature, 529(7587):484–489, 2016.

Silver, David, Hubert, Thomas, Schrittwieser, Julian,
Antonoglou, Ioannis, Lai, Matthew, Guez, Arthur, Lanc-
tot, Marc, Sifre, Laurent, Kumaran, Dharshan, Graepel,
Thore, Lillicrap, Timothy P., Simonyan, Karen, and Has-
sabis, Demis. Mastering chess and shogi by self-play
with a general reinforcement learning algorithm. CoRR,
abs/1712.01815, 2017.

Singh, Rishabh, Gulwani, Sumit, and Solar-Lezama, Ar-
mando. Automated feedback generation for introductory
programming assignments. In PLDI, pp. 15–26, 2013.

Verma, Abhinav, Murali, Vijayaraghavan, Singh, Rishabh,
Kohli, Pushmeet, and Chaudhuri, Swarat. Programmati-
cally interpretable reinforcement learning. arXiv preprint
arXiv:1804.02477, 2018.

Weimer, Westley, Nguyen, ThanhVu, Le Goues, Claire,
and Forrest, Stephanie. Automatically ﬁnding patches
using genetic programming. In ICSE, pp. 364–374. IEEE
Computer Society, 2009.

References

Abbeel, Pieter and Ng, Andrew Y. Apprenticeship learn-
ing via inverse reinforcement learning. In Proceedings
of the twenty-ﬁrst international conference on Machine
learning, pp. 1. ACM, 2004.

Barto, A. G., Sutton, R. S., and Anderson, C. W. Neuron-
like adaptive elements that can solve difﬁcult learning
control problems. IEEE Transactions on Systems, Man,
and Cybernetics, SMC-13(5):834–846, Sept 1983. ISSN
0018-9472.

Bastani, Osbert, Pu, Yewen, and Solar-Lezama, Armando.
Veriﬁable reinforcement learning via policy extraction.
arXiv preprint arXiv:1805.08328, 2018.

Bratko, Ivan, Urbanˇciˇc, Tanja, and Sammut, Claude. Be-
havioural cloning: phenomena, results and problems.
IFAC Proceedings Volumes, 28(21):143–149, 1995.

Brockman, Greg, Cheung, Vicki, Pettersson, Ludwig,
Schneider, Jonas, Schulman, John, Tang, Jie, and
arXiv preprint
Zaremba, Wojciech. Openai gym.
arXiv:1606.01540, 2016.

Clark, Jack and Amodei, Dario. Faulty reward func-
tions in the wild. https://blog.openai.com/faulty-reward-
functions/, 2016.

Hadﬁeld-Menell, Dylan, Milli, Smitha, Abbeel, Pieter, Rus-
sell, Stuart J, and Dragan, Anca. Inverse reward design.
In Advances in Neural Information Processing Systems,
pp. 6768–6777, 2017.

Ho, Jonathan and Ermon, Stefano. Generative adversarial
imitation learning. In Advances in Neural Information
Processing Systems, pp. 4565–4573, 2016.

Jobstmann, Barbara, Griesmayer, Andreas, and Bloem, Rod-
In CAV, pp. 226–
erick. Program repair as a game.
238, Berlin, Heidelberg, 2005. Springer-Verlag. doi:
10.1007/11513988 23. URL http://dx.doi.org/
10.1007/11513988_23.

Mnih, Volodymyr, Kavukcuoglu, Koray, Silver, David,
Rusu, Andrei A., Veness, Joel, Bellemare, Marc G.,
Graves, Alex, Riedmiller, Martin A., Fidjeland, Andreas,
Ostrovski, Georg, Petersen, Stig, Beattie, Charles, Sadik,
Amir, Antonoglou, Ioannis, King, Helen, Kumaran, Dhar-
shan, Wierstra, Daan, Legg, Shane, and Hassabis, Demis.
Human-level control through deep reinforcement learn-
ing. Nature, 518(7540):529–533, 2015.

Ross, St´ephane, Gordon, Geoffrey, and Bagnell, Drew. A
reduction of imitation learning and structured prediction
to no-regret online learning. In AISTATS, pp. 627–635,
2011.

