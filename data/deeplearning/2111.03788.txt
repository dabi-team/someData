1
2
0
2

v
o
N
6

]

G
L
.
s
c
[

1
v
8
8
7
3
0
.
1
1
1
2
:
v
i
X
r
a

d3rlpy: An Ofﬂine Deep Reinforcement Learning
Library

Takuma Seno1,2, Michita Imai1
Keio University1, Sony AI2
{seno,michita}@ailab.ics.keio.ac.jp

Abstract

In this paper, we introduce d3rlpy, an open-sourced ofﬂine deep reinforcement
learning (RL) library for Python. d3rlpy supports a number of ofﬂine deep RL
algorithms as well as online algorithms via a user-friendly API. To assist deep
RL research and development projects, d3rlpy provides practical and unique fea-
tures such as data collection, exporting policies for deployment, preprocessing
and postprocessing, distributional Q-functions, multi-step learning and a conve-
nient command-line interface. Furthermore, d3rlpy additionally provides a novel
graphical interface that enables users to train ofﬂine RL algorithms without cod-
ing programs. Lastly, the implemented algorithms are benchmarked with D4RL
datasets to ensure the implementation quality. The d3rlpy source code can be found
on GitHub: https://github.com/takuseno/d3rlpy.

1

Introduction

Deep reinforcement learning (RL) has been showing signiﬁcant advancements in numerous domains
such as gaming [28, 10], robotics [24] and autonomous driving [20]. While RL algorithms has
a potential to solve complex tasks, active data collection is a major challenge especially for the
environments where the interaction is expensive. To this problem, ofﬂine RL [25], where the
algorithms ﬁnd the good policy within the previously collected static dataset, is recently getting more
attentions.

Although the recent deep RL papers open-sourced their experiment codes, the implementations are
scattered across different repositories and the repositories are not usually providing user-friendly
APIs, which makes it difﬁcult for practitioners and researchers to incoporate the algorithms into their
projects. Moreover, the non-standardized implementations makes it difﬁcult for researchers to track
the exact implementation difference between algorithms. On the other hand, there are already many
libraries that provide a collection of deep RL algorithms [15, 4, 31]. However, they are designed for
online RL paradigms and not providing complete supports for ofﬂine RL in terms of algorithms and
interfaces.

In this paper, we introduce d3rlpy, an ofﬂine deep RL library for Python. d3rlpy provides many
of online and ofﬂine RL algorithms built with PyTorch [32]. The key features of the d3rlpy are as
follows:

Ofﬂine RL support d3rlpy is designed to support ofﬂine RL paradigms as well as the standard
online RL. The available algorithms are listed in Table 1. All algorithms support the ofﬂine
training interface and the online training interface so that users can easily pretrain ofﬂine
algorithms and ﬁne-tune the policy in online.

User-friendly API To make the state-of-the-art algorithms available for wide variety of users,
d3rlpy provides a user-friendly API through the scikit-learn [33] style interface. For deep

Ofﬂine Reinforcement Learning Workshop at Neural Information Processing Systems, 2021.

 
 
 
 
 
 
discrete action
(cid:88)
(cid:88)

(cid:88)
(cid:88)

(cid:88)

algorithm
DQN[28]
Double DQN[35]
DDPG[26]
TD3[14]
SAC[17, 5]
BCQ[13, 11]
BEAR[22]
CQL[23]
AWAC[30]
CRR[36]
PLAS[37]
PLAS+P[37]
TD3+BC[12]

continuous action

proposed as ofﬂine RL

(cid:88)
(cid:88)
(cid:88)
(cid:88)
(cid:88)
(cid:88)
(cid:88)
(cid:88)
(cid:88)
(cid:88)
(cid:88)

(cid:88)
(cid:88)
(cid:88)
(cid:88)
(cid:88)
(cid:88)
(cid:88)
(cid:88)

Table 1: A list of supported deep RL algorithms.

RL researchers, the interface is ﬂexible enough to customize neural network models, change
optimizers and train with their own datasets and environments.

Practical features Unlike the most of the existing libraries that focus on reproducing the paper
results, d3rlpy also provides practical and unique features that allow users to integrate the
powerful trained policies into their research or development projects.

Futhermore, d3rlpy also provides MINERVA, a novel tool with the graphical user interface (GUI) for
an out-of-the-box ofﬂine RL training. MINERVA enables users to upload datasets, train ofﬂine RL
algoirthms and export the trained policies for deployment without coding.

This paper is organized as follows: Section 2 presents related works on deep RL libraries, Section 3
presents the library design of the d3rlpy, Section 4 presents the practical and unique features of the
d3rlpy, Section 5 introduces the MINERVA, a GUI ofﬂine RL training tool, Section 6 presents the
benchmark scores in ofﬂine RL and Section 7 provides a conclusion.

2 Related work

A number of libraries are proposed in the deep RL ﬁeld. ChainerRL [15] provides many of deep RL
algorithms with the faithful reproduction results, which enables deep RL researchers to experiment
their new algorithms and compare with baselines. Dopamine [4] is focusing on DQN variants [28,
6, 19] to make them available for researchers as baseline implementations. Tonic [31] aims to
provide many of continuous control algorithms with large-scale benchmark results. There are two
design differences between d3rlpy and the existing libraries. First, the d3rlpy is the ﬁrst library
that fully supports ofﬂine RL algorithms. Second, d3rlpy provides the simple and easy interface
for practitioners such as application engineers as well as researchers while the existing libraries are
focusing on research purposes where users need to understand how to compose them.

Regarding the GUI tool in RL, ChainerRL Visualizer [15] is proposed as a debugging tool for the
ChainerRL. With ChainerRL Visualizer, the users can introspect RL policies by 1-step environment
execution and plotting details of the model outputs. Vizarel [8] provides an interface for visualizing
evaluation episodes and data distribution of the replay buffer [27]. Although the existing GUI tools
were designed for debugging RL models, there were no existing tools capable of RL training. The
MINERVA is the ﬁrst tool that offers the RL training capability by leveraging the d3rlpy as a backend.

3 Design of d3rlpy

3.1 Library interface

d3rlpy provides scikit-learn’s style API to make the use of this library as easy as possible. In terms
of the library design, there are two main differences from the existing libraries. First, d3rlpy has
an interface for ofﬂine RL training, which takes a dedicated RL dataset structure, MDPDataset,

2

Figure 1: The illustration of module components in d3rlpy. Algorithm takes EncoderFactory,
QFunctionFactory, OptimizerFactory, Scaler, ActionScaler and RewardScaler to instan-
tiate AlgorithmImpl and Model modules. MDPDataset and OpenAI Gym-styled environment can
be used to train policies.

described in Section 3.3. Second, the all methods for training such as fit and fit_online are
implemented in the algorithm modules to make d3rlpy as user-friendly as possible by reducing the
number of modules to use in training. Moreover, the neural network architectures are automatically
selected from MLP and the convolutional model [28] depending on the observation, which allows
the users to start training without composing the neural network models unless using the customized
architectures. These design choices are expected to lower the bar to start using this library.

Since d3rlpy supports both ofﬂine and online training, the seamless transition from ofﬂine training to
online ﬁne-tuning is easily realized. As ﬁne-tuning the policies trained in ofﬂine is demanded, but is
still a challenging problem [30], this seamless transition supports the further resarches by allowing
RL researchers to easily conduct ﬁne-tuning experiments.

import d3rlpy
import gym

# prepare MDPDataset object
dataset, env = d3rlpy.datasets.get_dataset("hopper-medium-v0")

# prpare algorithm object
sac = d3rlpy.algos.SAC(actor_learning_rate=3e-4, use_gpu=0, ...)

# start offline training
sac.fit(

dataset,
n_steps=500000,
eval_episodes=dataset.episodes,
scorers={

"environment": d3rlpy.metrics.evaluate_on_environment(env),
"average_value": d3rlpy.metrics.average_value_estimation_scorer,

}

)

# seamless transition to online training
sac.fit_online(env, n_steps=500000, eval_env=gym.make("Hopper-v2"))

3

EncoderFactory VectorEncoderFactory, …OptimizerFactory SGDFactory, AdamFactory, …Algorithm DQN, DDPG, SAC, BEAR, CQL, PLAS, …AlgorithmImpl DQNImpl, DDPGImpl, SACImpl, BEARImpl, CQLImpl, PLASImpl, …MDPDatasetEnvironment (OpenAI Gym interface)QFunctionFactory MeanQFunctionFactory, …Model DiscreteMeanQFunction, ContinuousMeanQFunction, SquashedNormalPolicy, …. . .Scaler StandardScaler, MinMaxScaler, …ActionScaler MinMaxActionScalerRewardScaler StandardRewardScaler, …Figure 2: The diagram of the MDPDataset. The MDPDataset consists of a list of Episode object that
includes Transition objects representing transition tuples. The Transition object has pointers to
the previous and the next transitions.

3.2 Algorithms

Figure 1 depicts a module components in d3rlpy. The Algorithm has a hierarchical design that
instantiates AlgorithmImpl inside itself. This hierarchy is to give high-level user-friendly APIs
such as fit method to Algorithm and low-level APIs such as update_actor and update_critic
to AlgorithmImpl. The main motivation of this hierarchical API system is to increase module
resusability of the algoirthms when the new algorithm only requires high-level changes. For example,
delayed policy update of TD3, which updates policy parameters every two gradient steps, can be
implemented by adjusting the frequency of update_actor method calls in the high-level module
without changing the low-level logics.

The users can customize neural network models through EncoderFactory module and change opti-
mizers through OptimizerFactory module. Scaler, ActionScaler and RewardScaler are mod-
ules for preprocessing and postprocessing, which is described in Section 4.2. QFunctionFactory
provides the distributional Q-function options, which is described in Section 4.4. MDPDataset is a
module to represent RL datasets described in Section 3.3.

3.3 MDPDataset

d3rlpy provides MDPDataset module internally used to represent an ofﬂine RL dataset. Figure 2 de-
picts a diagram of the MDPDataset module. The hierarchy of the Episode module and Transition
module is convenient for episode-wise data division for training and testing. The Transition
module is implemented with Cython, which allows Python programs to use C++ extentions, to
optimize memory copy at mini-batch sampling. By exploiting the pointers to the previous and the
next transitions and the low-overhead implementation in Cython, the multi-step return calculation [34]
and frame stacking [28] are done at batch sampling without slowing the training. Especially, the
advantage of the sampling time frame stacking is to reduce CPU memory usage by placing single
frame images on the memory instead of stacked images.

4 Practical features

4.1 Dataset collection

In ofﬂine RL research projects, data collection to create a new dataset plays an important role
especially when users need to develop policies for new tasks. d3rlpy supports this data collection
by giving OpenAI Gym [3] style environment and the replay buffer module to the algorithm object.
For the diverse sets of dataeset creation, the data collection can be done with and without parameter
updates, which corresponds to the dataset with a static policy and a non-stationary policy respectively.

4

MDPDatasetEpisodeEpisode…Transition observation action reward next_observation next_action next_rewarddoubled linked listTransition observation action reward next_observation next_action next_rewardEpisodeenv = gym.make("Hopper-v2")
sac = d3rlpy.algos.SAC()
buffer = d3rlpy.online.buffers.ReplayBuffer(1000000, env=env)

# collect data without training
sac.collect(env, buffer, n_steps=1000000)
# collect data with training
# sac.fit_online(env, buffer, n_steps=1000000)

# convert replay buffer to MDPDataset
dataset = buffer.to_mdp_dataset()

4.2 Preprocessing and postprocessing

By exploiting the static dataset in ofﬂine RL training, d3rlpy provides various preprocessing and post-
processing methods in a handy way. For observation preprocessing, normalization, standardization
and pixel (division by 255) are available. Especially, the observation standardization has been shown
to improve the policy performance in ofﬂine RL setting [12]. Regarding the action preprocessing,
normalization is available and the action output from the trained policy will be denormalized to the
original scale as postprocessing. Lastly, reward preprocessing supports normalization, standard-
ization, clip and constant multiplication. The reward preprocessing is still underexplored in RL
researches, however, the reward scale is known as an important factor affecting the actor-critic policy
performance [18].

cql = d3rlpy.algos.CQL(

scaler="standard",
action_scaler="min_max",
reward_scaler="standard", # standardize rewards

# standardize observations
# normalize actions and postprocess outputs

)

4.3 Export policy for deployment

Once the policy is ready for deployment, d3rlpy provides the policy export functionality. The main
advantage of this feature is that users can deploy the trained policy without the d3rlpy dependency, and
the exported formats support different programming languages other than Python and are optimized
for inference. d3rlpy supports two machine learning model formats, TorchScript [32] and ONNX [2].
The TorchScript is a serialized model format provided by PyTorch, which is optimized and executable
in C++ and Python programs only with the PyTorch dependency. The ONNX is an open format
built to represent machine learning models with various language supports such as Python, C++ and
JavaScript. To reduce the users’ implementation efforts, this exported policy includes the tensor
operations for preprocessing and postprocessing described above so that the programs can use the
exported policy without processing inputs and outputs.

cql = d3rlpy.algos.CQL()
cql.fit(...)
cql.save_policy("policy.pt")
cql.save_policy("policy.onnx")

# export as TorchScript
# export as ONNX

4.4 Distributional Q-function

d3rlpy supports distributional Q-functions, Quantile Regression (QR) [7] and Implicit Quantile Net-
work (IQN) [6]. The distributional Q-functions have achieved dramatical performance improvements
by capturing the variance of returns instead of learning the expected Q-values. Unlike conventional
RL libraries that implement distributional Q-functions as DQN-variants, d3rlpy enables users to use
them with all algorithms.

5

(a) Dataset Management

(b) Training Management

Figure 3: The MINERVA. With the use of the MINERVA, the users can easily upload their datasets,
train ofﬂine RL algorithms and export the trained policies as TorchScript and ONNX format for
deployment.

# use Quantile Regression Q-function in CQL training
cql = d3rlpy.algos.CQL(q_func_factory="qr")

4.5 Multi-step learning

Multi-step learning is known as a simple and powerful approach to improve RL agent performance [34,
19]. In d3rlpy, the multi-step returns are computed when sampling a mini-batch and cut at the end of
episodes if the episode length is shorter than the return horizon, which allows the algorithm to use the
all transition tuples to train. The computation is implemented in Cython for the minimal overhead.

# use N=3 in multi-step learning for Q-function update
sac = d3rlpy.algos.SAC(n_steps=3)

4.6 Command-line interface

A convenient command-line interface (CLI) are also available in d3rlpy. This CLI is capable of
visualizing the logged metrics data, executing the trained policies to record videos and exporting the
policies for deployment described in Section 4.3. When executing or exporting the trained policies,
the serialized algorithm metadata saved at the beginning of the training is used to construct the trained
models. The metadata format is described in Appendix A.

# plot training metrics by matplotlib
$ d3rlpy plot d3rlpy_logs/.../environment.csv
# record videos of evaluation episodes
$ d3rlpy record d3rlpy_logs/.../model_500000.pt --env-id Hopper-v2
# export trained policy as TorchScript
$ d3rlpy export d3rlpy_logs/.../model_500000.pt --format torchscript

5 MINERVA: Graphical user interface for ofﬂine RL

The MINERVA is a GUI tool powered by the d3rlpy, which enables users to upload datasets,
train ofﬂine RL algorithms and export the trained policy as TorchScript and ONNX formats. The
main difference from the conventional visualizers for RL is that the MINERVA is capable of not
only visualizing data, but also training policies by focusing on the ofﬂine training paradigm. The
MINERVA is built as a Web interface with the d3rlpy as a backend with an ability to manage multiple
training jobs in parallel. The dataset format is deﬁned as CSV text ﬁle (additionally with image ﬁles

6

Dataset
halfcheetah-random-v0
walker2d-random-v0
hopper-random-v0
halfcheetah-medium-v0
walker2d-medium-v0
hopper-medium-v0
halfcheetah-medium-replay-v0
walker2d-medium-replay-v0
hopper-medium-replay-v0
halfcheetah-medium-expert-v0
walker2d-medium-expert-v0
hopper-medium-expert-v0

SAC
30.8
2.9
0.8
25.1
7.1
0.8
42.0
8.1
0.6
-0.4
0.0
11.7

TD3
31.9
3.6
1.1
31.0
0.1
0.8
36.1
8.1
10.5
-1.7
0.9
8.7

AWAC
18.4
0.4
10.9
40.5
64.7
40.4
42.0
24.2
28.8
18.4
55.6
51.2

BCQ
2.2
4.1
10.6
40.2
45.8
40.2
39.1
16.0
16.9
58.1
52.3
112.5

BEAR
2.3
4.9
10.1
36.9
56.8
33.5
37.6
12.9
26.2
45.3
62.4
83.3

CQL
29.7
5.3
10.9
41.7
75.2
46.2
40.1
18.8
30.6
9.0
69.7
100.9

CRR
22.0
3.6
10.6
41.8
53.4
43.4
41.3
26.7
37.3
13.1
60.0
19.7

PLAS
27.0
6.7
10.6
40.2
33.4
68.7
44.1
22.2
16.2
82.8
98.1
110.4

PLAS+P
28.5
7.6
11.0
41.8
66.2
58.4
45.1
3.4
11.0
66.2
84.3
99.7

TD3+BC
11.0
2.7
11.0
42.6
74.9
86.7
43.0
25.3
31.0
86.9
96.0
112.0

Table 2: Normalized scores collected with the ﬁnal trained policy in D4RL. The scores are averaged
over 3 random seeds.

Dataset
halfcheetah-random-v0
walker2d-random-v0
hopper-random-v0
halfcheetah-medium-v0
walker2d-medium-v0
hopper-medium-v0
halfcheetah-medium-replay-v0
walker2d-medium-replay-v0
hopper-medium-replay-v0
halfcheetah-medium-expert-v0
walker2d-medium-expert-v0
hopper-medium-expert-v0

SAC
33.9
23.7
14.9
31.2
16.5
3.5
47.0
31.2
30.9
5.7
13.9
28.9

TD3
33.9
18.7
4.4
33.5
15.1
13.7
42.0
27.3
50.1
4.0
17.0
49.5

AWAC
23.0
7.9
11.7
42.3
80.1
102.5
43.9
50.9
98.5
27.4
112.0
112.8

BCQ
2.3
6.5
11.0
41.0
76.1
100.5
42.3
34.9
42.1
86.5
88.9
114.5

BEAR
3.3
11.9
11.1
37.8
76.3
97.4
39.7
34.5
58.8
63.7
91.1
112.8

CQL
34.8
15.5
11.2
42.6
82.4
100.6
46.3
49.5
65.2
24.4
110.9
114.7

CRR
23.6
10.8
15.0
42.4
72.7
95.0
42.9
44.6
86.3
23.6
98.1
87.5

PLAS
33.0
21.4
11.0
40.7
67.6
98.5
44.6
45.3
53.4
102.7
108.1
113.6

PLAS+P
34.9
22.7
11.1
42.4
80.8
101.3
46.1
44.9
51.9
93.5
111.4
114.7

TD3+BC
12.5
21.3
11.2
43.4
82.5
100.8
44.6
53.4
64.5
99.9
111.2
114.0

Table 3: Normalized scores collected with the best trained policy in D4RL. The scores are averaged
over 3 random seeds.

for image observations) described in Appendix B. Whether the action-space is discrete or continuous
is automatically determined based on the dataset and the algorithm options shown to the users depend
on the detected action-space.

Figure 3 shows example screenshots of a dataset management page and an experiment management
page. In the dataset management page depicted in Figure 3(a), the dataset metadata such as dataset
size and observation shapes, contents and data statistics are displayed. The training management page
depicted in Figure 3(b) shows a list of training jobs and collected metrics to compare multiple trials.

6 Benchmark

6.1 Ofﬂine RL

We evaluate the implemented algorithms on the D4RL benchmark of OpenAI Gym MuJoCo tasks [3,
9]. We train each algorithm for 500K gradient steps and evaluate every 5000 steps to collect
evaluation performance in environments for 10 episodes. The details of hyperparameters are reported
in Appendix C.

The ﬁnal performance results are reported in Table 2. The scores are normalized to the range
between 0 and 100 [9]. TD3+BC performs well across all datasets despite its simple implementation.
Interestingly, PLAS and PLAS+P performed closely to TD3+BC although those were not often
compared in the previous researches.

The existing works for ofﬂine RL algorithms usually report only ﬁnal performance results. However,
each algorithm has different trends of overﬁtting datasets, which makes it difﬁcult to evaluate the
potential performance in comparison for the same gradient steps. In this paper, we also report the best
performance results in Table 3. This performance metrics will be more important once we establish a
method to detect policy overﬁtting. Surprisingly, Table 3 shows that the best performance of AWAC
and CRR, the algorithms with supervised regression policy training, is signiﬁcantly better than the

7

(a) Multi-step Learning

(b) Distributional Q-function

Figure 4: Training curves on continuous control tasks with d3rlpy’s training features enabled. (a)
Multi-step learning performance in Walker2d-v2 environment. x-axis represents environment steps
and y-axis represents average returns in evaluation episodes. The scores are averaged over 3 random
seeds. (b) Distributional Q-function performance in HalfCheetah-v2 environment.

ﬁnal performance. This suggests that the supervised regression policy training algorithms potentially
perform well, but overﬁtting is more critical in those algorithms.

The raw scores and side-by-side comparisons against reference scores are reported in Appendix D.

6.2 Online RL with multi-step learning and distributional Q-functions

We evaluate d3rlpy’s training features with MuJoCo tasks in online training scenarios. In this
experiment, we evaluate multi-step learning described in Section 4.5 and distributional Q-functions
described in Section 4.4. The multi-step learning and distributional Q-functions are used with
the standard SAC algorithm without changing hyperparameter. We train each algorithm for 1M
environment steps and evaluate every 10K steps to collect evaluation performance in environments
for 10 episodes.

Figure 4 shows the result of continuous control tasks. In particular environments, simple modiﬁcations
by enabling multi-step learning or distributional Q-functions improve the performance from the
standard algorithm. This suggests that it is possible that users achieve better performance than SOTA
scores reported in their papers only with d3rlpy. The rest of results are reported in Appendix E and
Appendix F.

7 Conclusion

In this paper, we introduced an ofﬂine deep reinforcement learning library, d3rlpy, and its GUI
interface, MINERVA. d3rlpy provides user-friendly APIs and practical features for the research and
development projects. Also, the implemented algorithms are benchmarked with D4RL datasets,
which makes d3rlpy comparable to the existing and future works.

In the future work, we will continuously add new RL algorithms and features based on demands
from the open-source community and RL research trends. Speciﬁcally, we will support off-policy
evaluation methods [29] since evaluating the trained policies only with static datasets is important
for the practical applications. We will also benchmark discrete-control algorithms such as CQL and
BCQ with Atari 2600 datasets [1, 16] since we only benchmarked continuous-control algorithms in
this paper.

Hopefully, d3rlpy will contribute to the standardization of ofﬂine RL implementations and push ofﬂine
RL algorithms to the more diverse applications. And, we believe that d3rlpy solves reproducibility
issues [18] by providing the exact scripts used for benchmarking.

8

00.20.40.60.81.0million step0100020003000400050006000average returnWalker2d-v2SACSAC (N=3)SAC (N=5)00.20.40.60.81.0million step020004000600080001000012000average returnHalfCheetah-v2SACQR-SACIQN-SACAcknowledgement

This work is supported by Information-technology Promotion Agency, Japan (IPA), Exploratory IT
Human Resources Project (MITOU Program) in the ﬁscal year 2020. We would like to express our
genuine gratitude for the contributions made by the voluntary contributors. We would also like to
thank our users who have provided constructive feedbacks and insights. Lastly, we would like to
thank Yu Ishihara and Shunichi Sekiguchi from Sony R&D Center Tokyo for their fruitful comments
about the paper.

References

[1] Rishabh Agarwal, Dale Schuurmans, and Mohammad Norouzi. An optimistic perspective
on ofﬂine reinforcement learning. In International Conference on Machine Learning, pages
104–114. PMLR, 2020.

[2] Junjie Bai, Fang Lu, Ke Zhang, et al. Onnx: Open neural network exchange. https://github.

com/onnx/onnx, 2019.

[3] Greg Brockman, Vicki Cheung, Ludwig Pettersson, Jonas Schneider, John Schulman, Jie Tang,

and Wojciech Zaremba. Openai gym. arXiv preprint arXiv:1606.01540, 2016.

[4] Pablo Samuel Castro, Subhodeep Moitra, Carles Gelada, Saurabh Kumar, and Marc G. Belle-

mare. Dopamine: A Research Framework for Deep Reinforcement Learning. 2018.

[5] Petros Christodoulou.

Soft actor-critic for discrete action settings.

arXiv preprint

arXiv:1910.07207, 2019.

[6] Will Dabney, Georg Ostrovski, David Silver, and Rémi Munos. Implicit quantile networks for
distributional reinforcement learning. In International conference on machine learning, pages
1096–1105. PMLR, 2018.

[7] Will Dabney, Mark Rowland, Marc Bellemare, and Rémi Munos. Distributional reinforce-
ment learning with quantile regression. In Proceedings of the AAAI Conference on Artiﬁcial
Intelligence, volume 32, 2018.

[8] Shuby Deshpande, Benjamin Eysenbach, and Jeff Schneider.

Interactive visualization for

debugging rl. arXiv preprint arXiv:2008.07331, 2020.

[9] Justin Fu, Aviral Kumar, Oﬁr Nachum, George Tucker, and Sergey Levine. D4rl: Datasets for

deep data-driven reinforcement learning. arXiv preprint arXiv:2004.07219, 2020.

[10] Florian Fuchs, Yunlong Song, Elia Kaufmann, Davide Scaramuzza, and Peter Dürr. Super-
human performance in gran turismo sport using deep reinforcement learning. IEEE Robotics
and Automation Letters, 6(3):4257–4264, 2021.

[11] Scott Fujimoto, Edoardo Conti, Mohammad Ghavamzadeh, and Joelle Pineau. Benchmarking
batch deep reinforcement learning algorithms. arXiv preprint arXiv:1910.01708, 2019.
[12] Scott Fujimoto and Shixiang Shane Gu. A minimalist approach to ofﬂine reinforcement learning.

arXiv preprint arXiv:2106.06860, 2021.

[13] Scott Fujimoto, David Meger, and Doina Precup. Off-policy deep reinforcement learning
without exploration. In International Conference on Machine Learning, pages 2052–2062.
PMLR, 2019.

[14] Scott Fujimoto, Herke van Hoof, and David Meger. Addressing function approximation error in
actor-critic methods. In Proceedings of the 35th International Conference on Machine Learning,
pages 1587–1596, 2018.

[15] Yasuhiro Fujita, Prabhat Nagarajan, Toshiki Kataoka, and Takahiro Ishikawa. Chainerrl: A deep

reinforcement learning library. Journal of Machine Learning Research, 22(77):1–14, 2021.
[16] Caglar Gulcehre, Ziyu Wang, Alexander Novikov, Tom Le Paine, Sergio Gomez Colmenarejo,
Konrad Zolna, Rishabh Agarwal, Josh Merel, Daniel Mankowitz, Cosmin Paduraru, et al. Rl
unplugged: Benchmarks for ofﬂine reinforcement learning. arXiv e-prints, pages arXiv–2006,
2020.

[17] Tuomas Haarnoja, Aurick Zhou, Kristian Hartikainen, George Tucker, Sehoon Ha, Jie Tan,
Vikash Kumar, Henry Zhu, Abhishek Gupta, Pieter Abbeel, et al. Soft actor-critic algorithms
and applications. arXiv preprint arXiv:1812.05905, 2018.

9

[18] P Henderson, R Islam, P Bachman, J Pineau, D Precup, and D Meger. Deep reinforcement

learning that matters. arxiv 2017. arXiv preprint arXiv:1709.06560, 2017.

[19] Matteo Hessel, Joseph Modayil, Hado Van Hasselt, Tom Schaul, Georg Ostrovski, Will Dab-
ney, Dan Horgan, Bilal Piot, Mohammad Azar, and David Silver. Rainbow: Combining
improvements in deep reinforcement learning. In Thirty-second AAAI conference on artiﬁcial
intelligence, 2018.

[20] Alex Kendall, Jeffrey Hawke, David Janz, Przemyslaw Mazur, Daniele Reda, John-Mark Allen,
Vinh-Dieu Lam, Alex Bewley, and Amar Shah. Learning to drive in a day. In 2019 International
Conference on Robotics and Automation (ICRA), pages 8248–8254. IEEE, 2019.

[21] Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint

arXiv:1412.6980, 2014.

[22] Aviral Kumar, Justin Fu, George Tucker, and Sergey Levine. Stabilizing off-policy q-learning

via bootstrapping error reduction. arXiv preprint arXiv:1906.00949, 2019.

[23] Aviral Kumar, Aurick Zhou, George Tucker, and Sergey Levine. Conservative q-learning for

ofﬂine reinforcement learning. arXiv preprint arXiv:2006.04779, 2020.

[24] Joonho Lee, Jemin Hwangbo, Lorenz Wellhausen, Vladlen Koltun, and Marco Hutter. Learning

quadrupedal locomotion over challenging terrain. Science robotics, 5(47), 2020.

[25] Sergey Levine, Aviral Kumar, George Tucker, and Justin Fu. Ofﬂine reinforcement learning:
Tutorial, review, and perspectives on open problems. arXiv preprint arXiv:2005.01643, 2020.
[26] Timothy P Lillicrap, Jonathan J Hunt, Alexander Pritzel, Nicolas Heess, Tom Erez, Yuval Tassa,
David Silver, and Daan Wierstra. Continuous control with deep reinforcement learning. In
International Conference on Learning Representation, 2016.

[27] Long-Ji Lin. Self-improving reactive agents based on reinforcement learning, planning and

teaching. Machine learning, 8(3-4):293–321, 1992.

[28] Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei A Rusu, Joel Veness, Marc G
Bellemare, Alex Graves, Martin Riedmiller, Andreas K Fidjeland, Georg Ostrovski, et al.
Human-level control through deep reinforcement learning. Nature, 518(7540):529–533, 2015.
[29] Oﬁr Nachum, Yinlam Chow, Bo Dai, and Lihong Li. Dualdice: Behavior-agnostic estimation
of discounted stationary distribution corrections. arXiv preprint arXiv:1906.04733, 2019.
[30] Ashvin Nair, Murtaza Dalal, Abhishek Gupta, and Sergey Levine. Accelerating online rein-

forcement learning with ofﬂine datasets. arXiv preprint arXiv:2006.09359, 2020.

[31] Fabio Pardo. Tonic: A deep reinforcement learning library for fast prototyping and benchmark-

ing. arXiv preprint arXiv:2011.07537, 2020.

[32] Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan,
Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, et al. Pytorch: An imperative
style, high-performance deep learning library. arXiv preprint arXiv:1912.01703, 2019.
[33] Fabian Pedregosa, Gaël Varoquaux, Alexandre Gramfort, Vincent Michel, Bertrand Thirion,
Olivier Grisel, Mathieu Blondel, Peter Prettenhofer, Ron Weiss, Vincent Dubourg, et al. Scikit-
learn: Machine learning in python. the Journal of machine Learning research, 12:2825–2830,
2011.

[34] Richard S Sutton, Andrew G Barto, et al. Introduction to reinforcement learning, volume 2.

MIT Press Cambridge, 1998.

[35] Hado van Hasselt, Arthur Guez, and David Silver. Deep reinforcement learning with double

q-learning. corr abs/1509.06461 (2015). arXiv preprint arXiv:1509.06461, 2015.

[36] Ziyu Wang, Alexander Novikov, Konrad ˙Zołna, Jost Tobias Springenberg, Scott Reed, Bobak
Shahriari, Noah Siegel, Josh Merel, Caglar Gulcehre, Nicolas Heess, et al. Critic regularized
regression. arXiv preprint arXiv:2006.15134, 2020.

[37] Wenxuan Zhou, Sujay Bajracharya, and David Held. Latent action space for ofﬂine reinforce-

ment learning. In Conference on Robot Learning, 2020.

10

Appendix

A Serialized metadata

The metadata is serialized in JSON text format. The example of serialized metadata used by the CLI
is shown as follows:

{

"algorithm": "CQL",
"observation_shape": [

17

],
"action_size": 6
"action_scaler": null,
"actor_encoder_factory": {

"type": "vector",
"params": {

"hidden_units": [

256,
256,
256

],
"activation": "relu",
"use_batch_norm": false,
"dropout_rate": null,
"use_dense": false

}

},
"actor_learning_rate": 0.0001,
"actor_optim_factory": {
"optim_cls": "Adam",
"betas": [

0.9,
0.999

],
"eps": 1e-08,
"weight_decay": 0,
"amsgrad": false

},
.
.
.

}

Using this serialized data, the algorithm object can be constructed in the program, which is especially
convenient when continuing the training from the trained models:

# construct CQL algorithm object from the serialized data
cql = d3rlpy.algos.CQL.from_json("d3rlpy_logs/.../params.json")
# load the trained models
cql.load_model("d3rlpy_logs/.../model_500000.pt")
# resume training
cql.fit(...)

11

B Dataset format for MINERVA

The example dataset is shown below. In this case, the observation is 2-dimensional vector and the
action is 3-dimensional continuous vector.

,0.030
,0.032

,1.120
,0.241

episode,observation:0,observation:1,action:0,action:1,action:2,reward
0
0
.
.
1
1
.
.

,1.120
,0.312

,0.030
,0.032

,0.1
,1.4

,1.5
,0.3

.0.4
.1.0

,0.2
,1.4

,0.1
,1.2

,1.3
.0.3

,1.0
,0.0

,0.0
,1.0

The example of image observation dataset is shown below. In this case, the observation is pixel and
the action is discrete. When uploading the image observation datasets, zipped image ﬁles need to be
additionally uploaded along with the CSV ﬁle.

,1
,0

,0.0
,1.0

,image1.png
,image2.png

episode,observation:0,action:0,reward
0
0
.
.
1
1
.
.

,image13.png
,image14.png

,0.0
,1.0

,1
,0

C Experimental Details

Table 4 shows hyperparameters used in benchmarking. We used discount factor of 0.99, target update
rate of 5e-3 and an Adam optimizer [21] across all algorithms. The default architecture was MLP
with hidden layers of [256, 256] unless we explicitly explain it.

Table 4: Hyperparameters used in benchmarking.

Algorithm Hyperparameter

Critic learning rate
Actor learning rate
Mini-batch size
Critic learning rate
Actor learning rate
Mini-batch size
Policy noise
Policy noise clipping
Policy update frequency
Critic learning rate
Actor learning rate
Mini-batch size
λ [30]
Actor hidden units
Actor weight decay
Critic learning rate
Actor learning rate
VAE learning rate

SAC

TD3

AWAC

BCQ

Value
3e-4
3e-4
256
3e-4
3e-4
256
0.2
(-0.5, 0.5)
2
3e-4
3e-4
1024
1
[256, 256, 256, 256]
1e-4
1e-3
1e-3
1e-3

12

Mini-batch size
λ [13]
Critic hidden units
Actor hidden units
VAE encoder hidden units
VAE decoder hidden units
VAE latent size
Perturbation range
Action samples
Critic learning rate
Actor learning rate
VAE learning rate
α learning rate
Mini-batch size
VAE encoder hidden units
VAE decoder hidden units
VAE latent size
MMD σ
MMD kernel
MMD action samples
α threshold
Target action samples
Evaluation action samples
Pretraining steps
Critic learning rate
Actor learning rate
Fixed α
Mini-batch size
Critic hidden units
Actor hidden units
Action samples
Critic learning rate
Actor learning rate
Mini-batch size
Action samples
Advantage type
Weight type
Critic learning rate
Actor learning rate
VAE learning rate
Mini-batch size
Critic hidden units
Actor hidden units
VAE encoder hidden units
VAE decoder hidden units
λ [13]
VAE latent size
VAE pretraining steps
Critic learning rate
Actor learning rate
VAE learning rate
Mini-batch size
Critic hidden units
Actor hidden units
VAE encoder hidden units
VAE decoder hidden units
λ [13]
VAE latent size
VAE pretraining steps

100
0.75
[400, 300]
[400, 300]
[750, 750]
[750, 750]
2 × |A|
0.05
100
3e-4
1e-4
3e-4
1e-3
256
[750, 750]
[750, 750]
2 × |A|
20
laplacian (gaussian for HalfCheetah datasets)
4
0.05
10
100
40000
3e-4
1e-4
5 (10 for medium datasets)
256
[256, 256, 256]
[256, 256, 256]
10
3e-4
3e-4
256
4
mean
binary
1e-3
1e-4
1e-4
100
[400, 300]
[400, 300]
[750, 750] ([128, 128] for medium-replay datasets)
[750, 750] ([128, 128] for medium-replay datasets)
1.0
2 × |A|
500000
1e-3
1e-4
1e-4
100
[400, 300]
[400, 300]
[750, 750] ([128, 128] for medium-replay datasets)
[750, 750] ([128, 128] for medium-replay datasets)
1.0
2 × |A|
500000

13

BEAR

CQL

CRR

PLAS

PLAS+P

TD3+BC

Perturbation range
Critic learning rate
Actor learning rate
Mini-batch size
Policy noise
Policy noise clipping
Policy update frequency
α
Observation preprocess

0.05
3e-4
3e-4
256
0.2
(-0.5, 0.5)
2
2.5
standardization

D Additional Benchmark Results

Dataset
halfcheetah-random-v0
walker2d-random-v0
hopper-random-v0
halfcheetah-medium-v0
walker2d-medium-v0
hopper-medium-v0
halfcheetah-medium-replay-v0
walker2d-medium-replay-v0
hopper-medium-replay-v0
halfcheetah-medium-expert-v0
walker2d-medium-expert-v0
hopper-medium-expert-v0

SAC
3546.6
136.6
5.8
2831.6
325.9
6.6
4928.7
375.1
0.1
-331.0
2.7
360.9

TD3
3674.7
166.2
16.0
3568.4
5.3
4.2
4197.9
375.5
321.0
-490.3
43.8
262.0

AWAC
2005.6
20.0
335.7
4749.3
2972.2
1294.2
4922.4
1112.2
918.6
2008.8
2556.3
1647.2

BCQ
-1.1
189.4
326.3
4715.7
2103.1
1286.8
4577.5
734.0
528.8
6927.8
2401.6
3641.6

BEAR
-0.3
228.3
308.7
4304.5
2607.3
1071.2
4385.2
594.7
831.9
5338.8
2886.3
2692.2

CQL
3401.6
244.0
334.3
4892.3
3451.7
1483.7
4703.6
864.8
974.4
842.1
3209.4
3262.0

CRR
2452.4
165.6
323.8
4911.4
2451.8
1391.2
4852.8
1229.6
1193.3
1342.7
2341.2
621.0

PLAS
3076.1
316.9
323.4
4707.4
1532.9
2220.9
5198.6
1021.7
506.3
9995.3
4505.3
3571.6

PLAS+P
3260.1
348.4
336.6
4906.0
3039.7
1881.8
5322.2
159.4
336.4
7947.5
3872.6
3223.3

TD3+BC
1092.7
126.0
339.2
5014.3
3442.1
2800.0
5056.9
1164.4
989.9
10510.4
4410.2
3624.4

Table 5: The raw scores collected with the ﬁnal trained policy in D4RL. The scores are averaged over
3 random seeds.

Dataset
halfcheetah-random-v0
walker2d-random-v0
hopper-random-v0
halfcheetah-medium-v0
walker2d-medium-v0
hopper-medium-v0
halfcheetah-medium-replay-v0
walker2d-medium-replay-v0
hopper-medium-replay-v0
halfcheetah-medium-expert-v0
walker2d-medium-expert-v0
hopper-medium-expert-v0

SAC
3932.2
1088.3
466.2
3593.5
759.9
94.8
5560.7
1431.6
985.7
428.7
640.7
920.4

TD3
3931.2
859.3
124.3
3880.9
694.7
424.5
4933.1
1253.4
1609.3
214.1
783.5
1590.4

AWAC
2570.4
365.0
360.3
4966.1
3680.0
3316.3
5174.3
2336.3
3184.9
3122.6
5144.8
3650.5

BCQ
6.2
298.6
336.9
4816.2
3495.2
3251.2
4966.3
1601.8
1350.4
10459.6
4081.5
3704.9

BEAR
129.7
548.1
341.1
4408.9
3505.8
3148.2
4652.9
1586.2
1893.9
7630.9
4184.1
3651.5

CQL
4037.0
712.5
344.2
5005.2
3784.4
3252.9
5467.8
2272.7
2100.4
2755.0
5091.1
3713.5

CRR
2644.2
496.7
467.5
4979.9
3346.5
3072.2
5049.9
2050.0
2787.2
2648.1
4506.3
2828.5

PLAS
3816.7
984.4
336.6
4777.2
3103.2
3186.0
5256.2
2080.6
1718.3
12469.9
4964.3
3677.3

PLAS+P
4047.6
1042.2
342.4
4979.5
3710.9
3276.5
5438.9
2062.2
1669.2
11327.3
5104.1
3712.5

TD3+BC
1271.6
980.3
344.0
5105.1
3787.4
3260.2
5259.0
2453.1
2080.5
12124.2
5108.4
3690.4

Table 6: The raw scores collected with the best trained policy in D4RL. The scores are averaged over
3 random seeds.

14

e
r
e
w
L
Q
C

f
o

s
e
r
o
c
s

e
c
n
e
r
e
f
e
r

e
h
T
)
†
(

.

L
R
4
D
n
i

y
c
i
l
o
p

d
e
n
i
a
r
t

l
a
n
ﬁ
e
h
t

h
t
i

w
d
e
t
c
e
l
l
o
c

s
e
r
o
c
s

d
e
z
i
l
a
m
r
o
n

e
c
n
e
r
e
f
e
r

e
h
t

t
s
n
i
a
g
a

n
o
s
i
r
a
p
m
o
c

e
d
i
s
-
y
b
-
e
d
i
s

e
h
T

:
7

e
l
b
a
T

t
s
e
b
e
h
t
g
n
i
k
c
i
p
y
b
d
e
t
r
o
p
e
r

e
r
e
w
P
+
S
A
L
P
f
o
s
e
r
o
c
s

e
c
n
e
r
e
f
e
r

e
h
T
)
‡
(

.

b
u
H

t
i

G
n
i
d
e
t
s
e
g
g
u
s

s
r
e
t
e
m
a
r
a
p
r
e
p
y
h
e
h
t
h
t
i

w
n
o
i
t
a
t
n
e
m
e
l
p
m

i

s
’
r
o
h
t
u
a

e
h
t
h
t
i

w
d
e
t
c
e
l
l
o
c

.
s
e
u
l
a
v
n
e
s
o
h
c

e
h
t

e
d
i
v
o
r
p

t
o
n

d
i
d

r
o
h
t
u
a

e
h
t

e
s
u
a
c
e
b

r
e
t
e
m
a
r
a
p

e
h
t

d
e
x
ﬁ
e
w
e
l
i
h
w

]
7
3
[

t
e
s
a
t
a
d

h
c
a
e

r
o
f
l
e
d
o
m
n
o
i
t
a
b
r
u
t
r
e
p

e
h
t

f
o
r
e
t
e
m
a
r
a
p

a

g
n
i
p
e
e
e
w
s

f
o

t
l
u
s
e
r

]
2
1
[

f
e
R

y
p
l
r
3
d

‡
]
7
3
[

f
e
R

y
p
l
r
3
d

]
7
3
[

f
e
R

y
p
l
r
3
d

y
p
l
r
3
d

]
9
[

f
e
R

y
p
l
r
3
d

]
9
[

f
e
R

y
p
l
r
3
d

]
0
3
[

f
e
R

y
p
l
r
3
d

C
B
+
3
D
T

P
+
S
A
L
P

S
A
L
P

L
Q
C

R
A
E
B

Q
C
B

C
A
W
A

t
e
s
a
t
a
D

4
.
1

2
.
0
1

0
.
1
1

8
.
2
4

7
.
9
7

5
.
9
9

3
.
3
4

2
.
5
2

4
.
1
3

9
.
7
9

1
.
1
0
1

2
.
2
1
1

7
.
2

0
.
1
1

0
.
1
1

6
.
2
4

9
.
4
7

7
.
6
8

0
.
3
4

3
.
5
2

0
.
1
3

9
.
6
8

0
.
6
9

8
.
6

3
.
8
2

3
.
3
1

2
.
2
4

9
.
6
6

9
.
6
3

7
.
5
4

3
.
4
1

9
.
1
5

3
.
9
9

7
.
4
9

0
.
2
1
1

7
.
8
0
1

6
.
7

5
.
8
2

0
.
1
1

8
.
1
4

2
.
6
6

4
.
8
5

1
.
5
4

4
.
3

0
.
1
1

2
.
6
6

3
.
4
8

7
.
9
9

1
.
3

8
.
5
2

5
.
0
1

3
.
9
3

6
.
4
4

9
.
2
3

9
.
3
4

2
.
0
3

9
.
7
2

6
.
6
9

6
.
9
8

7
.
6

0
.
7
2

6
.
0
1

2
.
0
4

4
.
3
3

7
.
8
6

1
.
4
4

2
.
2
2

2
.
6
1

8
.
2
8

1
.
8
9

†
f
e
R

2
.
1

5
.
8
2

6
.
0
1

8
.
8
3

7
.
8
4

2
.
1
3

9
.
4
4

5
.
5
2

1
.
0
3

3
.
1
1

5
.
5
7

3
.
5

7
.
9
2

9
.
0
1

7
.
1
4

2
.
5
7

2
.
6
4

1
.
0
4

8
.
8
1

6
.
0
3

0
.
9

7
.
9
6

0
.
1
1
1

4
.
0
1
1

0
.
0
0
1

9
.
0
0
1

3
.
7

1
.
5
2

4
.
1
1

7
.
1
4

1
.
9
5

1
.
2
5

6
.
8
3

2
.
9
1

7
.
3
3

4
.
3
5

1
.
0
4

3
.
6
9

3
.
2

9
.
4

1
.
0
1

9
.
6
3

8
.
6
5

5
.
3
3

6
.
7
3

9
.
2
1

2
.
6
2

3
.
5
4

4
.
2
6

3
.
3
8

2
.
2

9
.
4

6
.
0
1

7
.
0
4

1
.
3
5

5
.
4
5

2
.
8
3

0
.
5
1

1
.
3
3

7
.
4
6

5
.
7
5

2
.
2

1
.
4

6
.
0
1

2
.
0
4

8
.
5
4

2
.
0
4

1
.
9
3

0
.
6
1

9
.
6
1

1
.
8
5

3
.
2
5

9
.
0
1
1

5
.
2
1
1

2
.
2

1
.
5

6
.
9

4
.
7
3

1
.
0
3

0
.
2
7

−

−

−

8
.
6
3

7
.
2
4

9
.
0
8

4
.
0

4
.
8
1

9
.
0
1

5
.
0
4

7
.
4
6

4
.
0
4

0
.
2
4

2
.
4
2

8
.
8
2

4
.
8
1

6
.
5
5

2
.
1
5

0
v
-
y
a
l
p
e
r
-

m
u
i
d
e
m
-
h
a
t
e
e
h
c
f
l
a
h

0
v
-
y
a
l
p
e
r
-

m
u
i
d
e
m
-
d
2
r
e
k
l
a
w

0
v
-
y
a
l
p
e
r
-

m
u
i
d
e
m

-
r
e
p
p
o
h

0
v
-
t
r
e
p
x
e
-
m
u
i
d
e
m
-
h
a
t
e
e
h
c
f
l
a
h

0
v
-
t
r
e
p
x
e
-
m
u
i
d
e
m
-
d
2
r
e
k
l
a
w

0
v
-
t
r
e
p
x
e
-
m
u
i
d
e
m

-
r
e
p
p
o
h

0
v
-
m
o
d
n
a
r
-
h
a
t
e
e
h
c
f
l
a
h

0
v
-
m
o
d
n
a
r
-
d
2
r
e
k
l
a
w

0
v
-
m
o
d
n
a
r
-
r
e
p
p
o
h

0
v
-
m
u
i
d
e
m
-
h
a
t
e
e
h
c
f
l
a
h

0
v
-
m
u
i
d
e
m
-
d
2
r
e
k
l
a
w

0
v
-
m
u
i
d
e
m

-
r
e
p
p
o
h

15

E Additional Multi-step Learning Results

We additionally evaluated the multi-step learning feature with tasks provided by Bullet 1, an open-
sourced simulator as well as other MuJoCo tasks. Interestingly, we observed more performance
improvements in Bullet tasks. This different trend indicates that Bullet tasks could be more dif-
ﬁcult tasks than MuJoCo tasks so that the powerful methods can obtain signiﬁcant performance
improvements with Bullet tasks.

Figure 5: Training curves on three different MuJoCo and Bullet tasks for multi-step learning.

F Additional Distributional Q-function Results

We additionally evaluated the distributional Q-function feature with tasks provided by Bullet as well
as other MuJoCo tasks. Just same as multi-step learning results, we also observed more performance
improvements in Bullet tasks.

Figure 6: Training curves on three different MuJoCo and Bullet tasks for distributional Q-functions.

1https://github.com/bulletphysics/bullet3

16

00.20.40.60.81.0million step0500100015002000250030003500average returnHopper-v2SACSAC (N=3)SAC (N=5)00.20.40.60.81.0million step020004000600080001000012000average returnHalfCheetah-v2SACSAC (N=3)SAC (N=5)00.20.40.60.81.0million step0100020003000400050006000average returnWalker2d-v2SACSAC (N=3)SAC (N=5)00.20.40.60.81.0million step05001000150020002500average returnHopperBulletEnv-v0SACSAC (N=3)SAC (N=5)00.20.40.60.81.0million step10000100020003000average returnHalfCheetahBulletEnv-v0SACSAC (N=3)SAC (N=5)00.20.40.60.81.0million step05001000150020002500average returnWalker2DBulletEnv-v0SACSAC (N=3)SAC (N=5)00.20.40.60.81.0million step0500100015002000250030003500average returnHopper-v2SACQR-SACIQN-SAC00.20.40.60.81.0million step020004000600080001000012000average returnHalfCheetah-v2SACQR-SACIQN-SAC00.20.40.60.81.0million step010002000300040005000average returnWalker2d-v2SACQR-SACIQN-SAC00.20.40.60.81.0million step05001000150020002500average returnHopperBulletEnv-v0SACQR-SACIQN-SAC00.20.40.60.81.0million step10000100020003000average returnHalfCheetahBulletEnv-v0SACQR-SACIQN-SAC00.20.40.60.81.0million step0500100015002000average returnWalker2DBulletEnv-v0SACQR-SACIQN-SAC