Melding the Data-Decisions Pipeline: Decision-Focused Learning for
Combinatorial Optimization

Bryan Wilder, Bistra Dilkina, Milind Tambe
Center for Artiﬁcial Intelligence in Society, University of Southern California
{bwilder, dilkina, tambe}@usc.edu

8
1
0
2

v
o
N
1
2

]

G
L
.
s
c
[

2
v
4
0
5
5
0
.
9
0
8
1
:
v
i
X
r
a

Abstract

Creating impact in real-world settings requires artiﬁcial in-
telligence techniques to span the full pipeline from data, to
predictive models, to decisions. These components are typ-
ically approached separately: a machine learning model is
ﬁrst trained via a measure of predictive accuracy, and then its
predictions are used as input into an optimization algorithm
which produces a decision. However, the loss function used to
train the model may easily be misaligned with the end goal,
which is to make the best decisions possible. Hand-tuning
the loss function to align with optimization is a difﬁcult and
error-prone process (which is often skipped entirely).
We focus on combinatorial optimization problems and in-
troduce a general framework for decision-focused learning,
where the machine learning model is directly trained in con-
junction with the optimization algorithm to produce high-
quality decisions. Technically, our contribution is a means
of integrating common classes of discrete optimization prob-
lems into deep learning or other predictive models, which are
typically trained via gradient descent. The main idea is to use
a continuous relaxation of the discrete problem to propagate
gradients through the optimization procedure. We instantiate
this framework for two broad classes of combinatorial prob-
lems: linear programs and submodular maximization. Experi-
mental results across a variety of domains show that decision-
focused learning often leads to improved optimization perfor-
mance compared to traditional methods. We ﬁnd that standard
measures of accuracy are not a reliable proxy for a predictive
model’s utility in optimization, and our method’s ability to
specify the true goal as the model’s training objective yields
substantial dividends across a range of decision problems.

Introduction
The goal in many real-world applications of artiﬁcial in-
telligence is to create a pipeline from data, to predictive
models, to decisions. Together, these steps enable a form
of evidence-based decision making which has transforma-
tive potential across domains such as healthcare, scientiﬁc
discovery, transportation, and more (Horvitz and Mitchell
2010; Horvitz 2010). This pipeline requires two technical
components: machine learning models and optimization al-
gorithms. Machine learning models use the data to predict

Copyright c(cid:13) 2019, Association for the Advancement of Artiﬁcial
Intelligence (www.aaai.org). All rights reserved.

unknown quantities; optimization algorithms use these pre-
dictions to arrive at a decision which maximizes some objec-
tive. Our concern here is combinatorial optimization, which
is ubiquitous in real-world applications of artiﬁcial intelli-
gence, ranging from matching applicants to public housing
to selecting a subset of movies to recommend. We focus
on common classes of combinatorial problems which have
well-structured continuous relaxations, e.g., linear programs
and submodular maximization. A vast literature has been de-
voted to combinatorial optimization (Korte et al. 2012). Im-
portantly though, optimization is often insufﬁcient without
the broader pipeline because the objective function is un-
known and must predicted via machine learning.

While machine learning has witnessed incredible growth
in recent years, the two pieces of the pipeline are treated
entirely separately by typical training approaches. That is,
a system designer will ﬁrst train a predictive model using
some standard measure of accuracy, e.g., mean squared er-
ror for a regression problem. Then, the model’s predictions
are given as input to the optimization algorithm to produce a
decision. Such two-stage approaches are extremely common
across many domains (Wang et al. 2006; Fang et al. 2016;
Mukhopadhyay et al. 2017; Xue et al. 2016). This process
is justiﬁed when the predictive model is perfect, or near-so,
since completely accurate predictions also produce the best
decisions. However, in complex learning tasks, all models
will make errors and the training process implicitly trades
off where these errors will occur. When prediction and opti-
mization are separate, this tradeoff is divorced from the goal
of the broader pipeline: to make the best decision possible.
We propose a decision-focused learning framework
which melds the data-decisions pipeline by integrating pre-
diction and optimization into a single end-to-end system.
That is, the predictive model is trained using the quality
of the decisions which it induces via the optimization al-
gorithm. Similar ideas have recently been explored in the
context of convex optimization (Donti, Amos, and Kolter
2017), but to our knowledge ours is the ﬁrst attempt to train
machine learning systems for performance on combinato-
rial decision-making problems. Combinatorial settings raise
new technical challenges because the optimization problem
is discrete. However, machine learning systems (e.g., deep
neural networks) are often trained via gradient descent.

Our ﬁrst contribution is a general framework for training

 
 
 
 
 
 
machine learning models via their performance on combina-
torial problems. The starting point is to relax the combinato-
rial problem to a continuous one. Then, we analytically dif-
ferentiate the optimal solution to the continuous problem as
a function of the model’s predictions. This allows us to train
using a continuous proxy for the discrete problem. At test
time, we round the continuous solution to a discrete point.

Our second contribution is to instantiate this framework
for two broad classes of combinatorial problems: linear pro-
grams and submodular maximization problems. Linear pro-
gramming encapsulates a number of classical problems such
as shortest path, maximum ﬂow, and bipartite matching.
Submodular maximization, which reﬂects the intuitive phe-
nomena of diminishing returns, is also ubiquitous; applica-
tions range from social networks (Kempe, Kleinberg, and
Tardos 2003) to recommendation systems (Viappiani and
Boutilier 2010). In each case, we resolve a set of technical
challenges to produce well-structured relaxations which can
be efﬁciently differentiated through.

Finally, we give an extensive empirical investigation,
comparing decision-focused and traditional methods on a
series of domains. Decision-focused methods often improve
performance for the pipeline as a whole (i.e., decision qual-
ity) despite worse predictive accuracy according to standard
measures. Intuitively, the predictive models trained via our
approach focus speciﬁcally on qualities which are important
for making good decisions. By contrast, more generic meth-
ods produce predictions where error is distributed in ways
which are not aligned with the underlying task.

Problem description
We consider combinatorial optimization problems of the
form maxx∈X f (x, θ), where X is a discrete set enumer-
ating the feasible decisions. Without loss of generality, X ⊆
{0, 1}n and the decision variable x is a binary vector. The
objective f depends on a parameter θ ∈ Θ. If θ were known
exactly, a wide range of existing techniques could be used
to solve the problem. In this paper, we consider the chal-
lenging (but prevalent) case where θ is unknown and must
be inferred from data. For instance, in bipartite matching, x
represents whether each pair of nodes were matched and θ
contains the reward for matching each pair. In many appli-
cations, these afﬁnities are learned from historical data.

Speciﬁcally, the decision maker observes a feature vector
y ∈ Y which is correlated with θ. This introduces a learn-
ing problem which must be solved prior to optimization. As
in classical supervised learning, we formally model y and θ
as drawn from a joint distribution P . Our algorithm will ob-
serve training instances (y1, θ1)...(yN , θN ) drawn iid from
P . At test time, we are give a feature vector y correspond-
ing to an unobserved θ. Our algorithm will use y to predict
a parameter value ˆθ. Then, we will solve the optimization
problem maxx f (x, ˆθ) to obtain a decision x∗. Our utility is
the objective value that x∗ obtains with respect to the true
but unknown parameter θ, f (x∗, θ).

Let m : Y → Θ denote a model mapping observed
features to parameters. Our goal is to (using the training
data) ﬁnd a model m which maximizes expected perfor-

mance on the underlying optimization task. Deﬁne x∗(θ) =
arg maxx∈X f (x, θ) to be the optimal x for a given θ. The
end goal of the data-decisions pipeline is to maximize

E
y,θ∼P

[f (x∗(m(y)), θ)]

(1)

The classical approach to this problem is a two-stage
method which ﬁrst learns a model using a task-agnostic
loss function (e.g., mean squared error) and then uses
the learned model to solve the optimization problem. The
model class will have its own parameterization, which we
denote by m(y, ω). For instance, the model class could
consist of deep neural networks where ω denotes the
weights. The two-stage approach ﬁrst solves the problem
minω Ey,θ∼P [L(θ, m(y, ω))], where L is a loss function.
Such a loss function measures the overall “accuracy” of the
model’s predictions but does not speciﬁcally consider how
m will fare when used for decision making. The question we
address is whether it is possible to do better by speciﬁcally
training the model to perform well on the decision problem.

Previous work
There is a growing body of research at the interface of
machine learning and discrete optimization (Vinyals, For-
tunato, and Jaitly 2015; Bertsimas and Dunn 2017; Khalil
et al. 2017b; Khalil et al. 2017a). However, previous work
largely focuses on either using discrete optimization to ﬁnd
an accuracy-maximizing predictive model or using machine
learning to speed up optimization algorithms. Here, we pur-
sue a deeper synthesis; to our knowledge, this work is the
ﬁrst to train predictive models using combinatorial optimiza-
tion performance with the goal of improving decision mak-
ing.

The closest work to ours in motivation is (Donti, Amos,
and Kolter 2017), who study task-based convex optimiza-
tion. Their aim is to optimize a convex function which de-
pends on a learned parameter. As in their work, we use the
idea of differentiating through the KKT conditions. How-
ever, their focus is entirely on continuous problems. Our
discrete setting raises new technical challenges, highlighted
below. Elmachtoub and Grigas (2017) also propose a means
of integrating prediction and optimization; however, their
method applies strictly to linear optimization and focuses
on linear predictive models while our framework applies to
nonlinear problems with more general models (e.g., neu-
ral networks). Finally, some work has noted that two-stage
methods lead to poor optimization performance in speciﬁc
domains (Beygelzimer and Langford 2009; Ford et al. 2015).
Our work is also related to recent research in structured
prediction (Belanger, Yang, and McCallum 2017; Tu and
Gimpel 2018; Niculae et al. 2018; Djolonga and Krause
2017). which aims to make a prediction lying in a discrete
set. This is fundamentally different than our setting since
their goal is to predict an external quantity, not to optimize
and ﬁnd the best decision possible. However, structured pre-
diction sometimes integrates a discrete optimization prob-
lem as a module within a larger neural network. The clos-
est such work technically to ours is (Tschiatschek, Sahin,

and Krause 2018), who design a differentiable algorithm for
submodular maximization in order to predict choices made
by users. Their approach is to introduce noise into the stan-
dard greedy algorithm, making the probability of outputting
a given set differentiable. There are two key differences be-
tween our approaches. First, their approach does not apply to
the decision-focused setting because it maximizes the like-
lihood of a ﬁxed set but cannot optimize for ﬁnding the best
set. Second, exactly computing gradients for their algorithm
requires marginalizing over the k! possible permutations of
the items, forcing a heuristic approximation to the gradient.
Our approach allows closed-form differentiation.

Some deep learning architectures differentiate through
gradient descent steps, related to our approach in the sub-
modular setting. Typically, previous approaches explicitly
unroll T iterations of gradient descent in the computational
graph (Domke 2012). However, this approach is usually em-
ployed for unconstrained problems where each iteration is
a simple gradient step. By contrast, our combinatorial prob-
lems are constrained, requiring a projection step to enforce
feasibility. Unrolling the projection step may be difﬁcult,
and would incur a large computational cost. We instead ex-
ploit the fact that gradient ascent converges to a local opti-
mum and analytically differentiate via the KKT conditions.

General framework

Our goal is to integrate combinatorial optimization into the
loop of gradient-based training. That is, we aim to directly
train the predictive model m by running gradient steps on the
objective in Equation 1, which integrates both prediction and
optimization. The immediate difﬁculty is the dependence on
x∗(m(y, ω)). This term is problematic for two reasons. First,
it is a discrete quantity since x∗ is a decision from a binary
set. This immediately renders the output nondifferentiable
with respect to the model parameters ω. Second, even if x∗
were continuous, it is still deﬁned as the solution to an op-
timization problem, so calculating a gradient requires us to
differentiate through the argmax operation.

We resolve both difﬁculties by considering a continuous
relaxation of the combinatorial decision problem. We show
that for a broad class of combinatorial problems, there are
appropriate continuous relaxations such that we can analyti-
cally obtain derivatives of the continuous optimizer with re-
spect to the model parameters. This allows us to train any
differentiable predictive model via gradient descent on a
continuous surrogate to Equation 1. At test time, we solve
the true discrete problem by rounding the continuous point.
More speciﬁcally, we relax the discrete constraint x ∈ X
to the continuous one x ∈ conv(X ) where conv denotes the
convex hull. Let x(θ) = arg maxx∈conv(X ) f (x, θ) denote
the optimal solution to the continuous problem. To train our
predictive model, we would like to compute gradients of the
whole-pipeline objective given by Equation 1, replacing the
discrete quantity x∗ with the continuous x. We can obtain a
stochastic gradient estimate by sampling a single (y, θ) from
the training data. On this sample, the chain rule gives

df (x(ˆθ), θ)
dω

=

df (x(ˆθ), θ)
dx(ˆθ)

dx(ˆθ)
dˆθ

dˆθ
dω

The ﬁrst term is just the gradient of the objective with
respect to the decision variable x, and the last term is the
gradient of the model’s predictions with respect to its own
internal parameterization.

The key is computing the middle term, which measures
how the optimal decision changes with respect to the pre-
diction ˆθ. For continuous problems, the optimal continuous
decision x must satisfy the KKT conditions (which are suf-
ﬁcient for convex problems). The KKT conditions deﬁne a
system of linear equations based on the gradients of the ob-
jective and constraints around the optimal point. Is is known
that by applying the implicit function theorem, we can dif-
ferentiate the solution to this linear system (Gould et al.
2016; Donti, Amos, and Kolter 2017). In more detail, re-
call that our continuous problem is over conv(X ), the con-
vex hull of the discrete feasible solutions. This set is a poly-
tope, which can be represented via linear equalities as the
set {x : Ax ≤ b} for some matrix A and vector b. Let (x, λ)
be pair of primal and dual variables which satisfy the KKT
conditions. Then differentiating the conditions yields that

(cid:20) ∇2

xf (x, θ)

AT

diag(λ)A diag(Ax − b)

(cid:21)

(cid:21) (cid:20) dx
dθ
dλ
dθ

(cid:21)

(cid:20) d∇xf (x,θ)
dθ
0

=

(2)

By solving this system of linear equations, we can ob-
tain the desired term dx
dθ . However, the above approach is
a general framework; our main technical contribution is to
instantiate it for speciﬁc classes of combinatorial problems.
Speciﬁcally, we need (1) an appropriate continuous relax-
ation, along with a means of solving the continuous opti-
mization problem and (2) efﬁcient access to the terms in
Equation 2 which are needed for the backward pass (i.e.,
gradient computation). We provide both ingredients for two
broad classes of problems: linear programming and submod-
ular maximization. In each setting, the high-level challenge
is to ensure that the continuous relaxation is differentiable, a
feature not satisﬁed by naive alternatives. We also show how
to efﬁciently compute terms needed for the backward pass,
especially for the more intricate submodular case.

Linear programming
The ﬁrst setting that we consider is combinatorial problems
which can be expressed as a linear program with equality
and inequality constraints in the form

max θT x s.t. Ax = b, Gx ≤ h

(3)

Example problems include shortest path, maximum ﬂow,
bipartite matching, and a range of other domains. For in-
stance, in a shortest path problem θ contains the cost for
traversing each edge, and we are interested in problems
where the true costs are unknown and must be predicted.
Since the LP can be regarded as a continuous problem (it

just happens that the optimal solutions in these example do-
mains are integral), we could attempt to apply Equation 2
and differentiate the solution. This approach runs into an im-
mediate difﬁculty: the optimal solution to an LP may not be
differentiable (or even continuous) with respect to θ. This
is because the optimal solution may “jump” to a different
vertex. Formally, the left-hand side matrix in Equation 2 be-
comes singular since ∇2
xf (x, θ) is always zero. We resolve
this challenge by instead solving the regularized problem

max θT x − γ||x||2

2 s.t. Ax = b, Gx ≤ h

(4)

which introduces a penalty proportional to the squared
norm of the decision vector. This transforms the LP into a
strongly concave quadratic program (QP). The Hessian is
given by ∇2
xf (x, θ) = −2γI (where I is the identity ma-
trix), which renders the solution differentiable under mild
conditions (see supplement for proof):

Theorem 1. Let x(θ) denote the optimal solution of Prob-
lem 4. Provided that the problem is feasible and all rows of A
are linearly independent, x(θ) is differentiable with respect
to θ almost everywhere. If A has linearly dependent rows,
removing these rows yields an equivalent problem which is
differentiable almost everywhere. Wherever x(θ) is differen-
tiable, it satisﬁes the conditions in Equation 2.

Moreover, we can control the loss that regularization can

cause on the original, linear problem:
Theorem 2. Deﬁne D = maxx,y∈conv(X ) ||x − y||2 as the
squared diameter of the feasible set and OP T to be the op-
timal value for Problem 3. We have θ(cid:62)x(θ) ≥ OP T − γD.

Together, these results give us a differentiable surrogate
which still enjoys an approximation guarantee relative to the
integral problem. Computing the backward pass via Equa-
tion 2 is now straightforward since all the relevant terms are
easily available. Since ∇xθ(cid:62)x = θ, we have d∇xf (x,θ)
= I.
All other terms are easily computed from the optimal primal-
dual pair (x, λ) which is output by standard QP solvers. We
can also leverage a recent QP solver (Amos and Kolter 2017)
which maintains a factorization of the KKT matrix for a
faster backward pass. At test time, we simply set γ = 0
to produce an integral decision.

dθ

Submodular maximization

We consider problems where the underlying objective to
maximize a set function f : 2V → R, where V is a ground
set of items. A set function is submodular if for any A ⊆ B
and any v ∈ V \B, f (A∪{v})−f (A) ≥ f (B∪{v})−f (B).
We will restrict our consideration to submodular functions
which are monotone (f (A ∪ {v}) − f (A) ≥ 0 ∀A, v) and
normalized f (∅) = 0. This class of functions contains
many combinatorial problems which have been considered
in machine learning and artiﬁcial intelligence (e.g., inﬂu-
ence maximization, facility location, diverse subset selec-
tion, etc.). We focus on the cardinality-constrained optimiza-
tion problem max|S|≤k f (S), though our framework easily
accommodates more general matroid constraints.

Continuous relaxation: We employ the canonical con-
tinuous relaxation for submodular set functions, which as-
sociates each set function f with its multilinear extension
F (Calinescu et al. 2011). We can view a set function as
deﬁned on the domain {0, 1}|V |, where each element is an
indicator vector which the items contained in the set. The ex-
tension F is a continuous function deﬁned on the hypercube
[0, 1]|V |. We interpret a given fraction vector x ∈ [0, 1]|V |
as giving the marginal probability that each item is included
in the set. F (x) is the expected value of f (S) when each
item i is included in S independently with probability xi.
In other words, F (x) = (cid:80)
i(cid:54)∈S 1 − xi.
While this deﬁnition sums over exponentially many terms,
arbitrarily close approximations can be obtained via ran-
dom sampling. Further, closed forms are available for many
cases of interest (Iyer, Jegelka, and Bilmes 2014). Impor-
tantly, well-known rounding algorithms (Calinescu et al.
2011) can convert a fractional point x to a set S satisfying
E[f (S)] ≥ F (x); i.e., the rounding is lossless.

S⊆V f (S) (cid:81)

i∈S xi

(cid:81)

As a proxy for the discrete problem max|S|≤k f (S), we
can instead solve maxx∈conv(X ) F (x), where X = {x ∈
{0, 1}|V | : (cid:80)
i xi ≤ k}. Unfortunately, F is not in general
concave. Nevertheless, many ﬁrst-order algorithms still ob-
tain a constant factor approximation. For instance, a variant
of the Frank-Wolfe algorithm solves the continuous maxi-
mization problem with the optimal approximation ratio of
(1 − 1/e) (Calinescu et al. 2011; Bian et al. 2017).

However, non-concavity complicates the problem of dif-
ferentiating through the continuous optimization problem.
Any polynomial-time algorithm can only be guaranteed to
output a local optimum, which need not be unique (com-
pared to strongly convex problems, where there is a single
global optimum). Consequently, the algorithm used to se-
lect x(θ) might return a different local optimum under an
inﬁnitesimal change to θ. For instance, the Frank-Wolfe al-
gorithm (the most common algorithm for continuous sub-
modular maximization) solves a linear optimization problem
at each step. Since (as noted above), the solution to a linear
problem may be discontinuous in θ, this could render the
output of the optimization problem nondifferentiable.

We resolve this difﬁculty through a careful choice of opti-
mization algorithm for the forward pass. Speciﬁcally, we use
apply projected stochastic gradient ascent (SGA), which has
recently been shown to obtain a 1
2 -approximation for con-
tinuous submodular maximization (Hassani, Soltanolkotabi,
and Karbasi 2017). Although SGA is only guaranteed to ﬁnd
a local optimum, each iteration applies purely differentiable
computations (a gradient step and projection onto the set
conv(X )), and so the ﬁnal output after T iterations will be
differentiable as well. Provided that T is sufﬁciently large,
this output will converge to a local optimum, which must
satisfy the KKT conditions. Hence, we can apply our gen-
eral approach to the local optimum returned by SGA. The
following theorem shows that the local optima of the multi-
linear extension are differentiable:
Theorem 3. Suppose that x∗ is a local maximum of the mul-
tilinear extension, i.e,., ∇xF (x∗, θ) = 0 and ∇2
xF (x∗, θ) (cid:31)
0. Then, there exists a neighborhood I around x∗ such that

the maximizer of F (·, θ) within I ∩conv(X ) is differentiable
almost everywhere as a function of θ, with dx(θ)
satisfying
dθ
the conditions in Equation 2.

We remark that Theorem 3 requires a local maximum,
while gradient ascent may in theory ﬁnd saddle points. How-
ever, recent work shows that random perturbations ensure
that gradient ascent quickly escapes saddle points and ﬁnds
an approximate local optimum (Jin et al. 2017).

Efﬁcient backward pass: We now show how the terms
needed to compute gradients via Equation 2 can be efﬁ-
ciently obtained. In particular, we need access to the opti-
mal dual variable λ as well as the term d∇xF (x,θ)
. These
were easy to obtain in the LP setting but the submodular
setting requires some additional analysis. Nevertheless, we
show that both can be obtained efﬁciently.

dθ

Optimal dual variables: SGA only produces the opti-
mal primal variable x, not the corresponding dual variable λ
which is required to solve Equation 2 in the backward pass.
We show that for cardinality-constrained problems, we can
obtain the optimal dual variables analytically given a primal
solution x. Let λL
i be the dual variable associated with the
constraint xi ≥ 0, λU
i xi ≤ k.
By differentiating the Lagrangian, any optimum satisﬁes

i with xi ≤ 1 and λS with (cid:80)

i + λS

i = 0 ∀i

∇xif (x) − λL

i + λU
where complementary slackness requires that λL

i = 0 if
xi > 0 and λU
i = 0 if xi < 1. Further, it is easy to see
that for all i with 0 < xi < 1, ∇xif (x) must be equal.
Otherwise, x could not be (locally) optimal since we could
increase the objective by ﬁnding a pair i, j with ∇xif (x) >
∇xj f (x), increasing xi, and decreasing xj. Let ∇∗ denote
the shared gradient value for fractional entries. We can solve
the above equation and express the optimal dual variables as

λS = −∇∗, λL
where the expressions for λL

i = ∇xif − λS
i apply only when
xi = 0 and xi = 1 respectively (otherwise, complementary
slackness requires these variables be set to 0).

i = λS − ∇xif, λU
i and λU

Computing d

dθ ∇xF(x, θ): We show that this term can
be obtained in closed form for the case of probabilistic cov-
erage functions, which includes many cases of practical in-
terest (e.g. budget allocation, sensor placement, facility lo-
cation, etc.). However, our framework can be applied to arbi-
trary submodular functions; we focus here on coverage func-
tions just because they are particularly common in applica-
tions. A coverage function takes the following form. There
a set of items U , and each j ∈ U has a weight wj. The al-
gorithm can choose from a ground set V of actions. Each
action ai covers each item j independently with probability
θij. We consider the case where the probabilities θ are be un-
known and must be predicted from data. For such problems,
the multilinear extension has a closed form

F (x, θ) =

(cid:32)

wj

1 −

(cid:89)

i∈V

(cid:88)

j∈U

(cid:33)

1 − xijθij

and we can obtain the expression

d
dθkj

∇xiF (x, θ) =

(cid:40)

−θijxk
(cid:81)

k(cid:54)=i 1 − xkθkj

(cid:81)

(cid:96)(cid:54)=i,k 1 − x(cid:96)θ(cid:96)j

if k (cid:54)= i
otherwise.

Experiments
We conduct experiments across a variety of domains in or-
der to compare our decision-focused learning approach with
traditional two stage methods. We start out by describing
the experimental setup for each domain. Then, we present
results for the complete data-decisions pipeline in each do-
main (i.e., the ﬁnal solution quality each method produces
on the optimization problem). We ﬁnd that decision-focused
learning almost always outperforms two stage approaches.
To investigate this phenomenon, we show more detailed re-
sults about what each model learns. Two stage approaches
typically learn predictive models which are more accurate
according to standard measures of machine learning ac-
curacy. However, decision-focused methods learn qualities
which are important for optimization performance even if
this leads to lower accuracy in an overall sense.

Budget allocation: We start with a synthetic domain
which allows us to illustrate how our methods differ from
traditional approaches and explore when improved decision
making is achievable. This example concerns budget allo-
cation, a submodular maximization problem which models
an advertiser’s choice of how to divide a ﬁnite budget k be-
tween a set of channels. There is a set of customers R and
the objective is f (S) = (cid:80)
u∈S(1 − θuv), where
θuv is the probability that advertising on channel u will
reach customer v. This is the expected number of customers
reached. Variants on this problem have been the subject of a
great deal of research (Alon, Gamzu, and Tennenholtz 2012;
Soma et al. 2014; Miyauchi et al. 2015).

v∈R 1 − (cid:81)

In our problem, the matrix θ is not known in advance and
must be learned from data. The ground truth matrices were
generated using the Yahoo webscope (Yahoo 2007) dataset
which logs bids placed by advertisers on a set of phrases. In
our problem, the phrases are channels and the accounts are
customers. Each instance samples a random subset of 100
channels and 500 customers. For each edge (u, v) present in
the dataset, we sample θuv uniformly at random in [0,0.2].
For each channel u, we generate a feature vector from that
channel’s row of the matrix, θu via complex nonlinear func-
tion. Speciﬁcally, θu is passed through a 5-layer neural net-
work with random weight matrices and ReLU activations to
obtain a feature vector yu. The learning task is to reconstruct
θu from yu. The optimization task is to select k channels in
order to maximize the number of customers reached.

Bipartite matching: This problem occurs in many do-
mains; e.g., bipartite matching has been used to model the
problem of a public housing programs matching housing re-
sources to applicants (Benabbou et al. 2018) or platforms
matching advertisers with users (Bellur and Kulkarni 2007).
In each of these cases, the reward to matching any two nodes
is not initially known, but is instead predicted from the fea-
tures available for both parties. Bipartite matching can be
formulated as a linear program, allowing us to apply our

Table 1: Solution quality of each method for the full data-decisions pipeline.

Budget allocation

k =

5

10

20

NN1-Decision
NN2-Decision
NN1-2Stage
NN2-2Stage
RF-2Stage
Random

49.18 ± 0.24
44.35 ± 0.56
32.13 ± 2.47
9.69 ± 0.05
48.81 ± 0.32
9.69 ± 0.04

72.62 ± 0.33
67.64 ± 0.62
45.63 ± 3.76
18.93 ± 0.10
72.40 ± 0.43
18.92 ± 0.09

98.95 ± 0.46
93.59 ± 0.77
61.88 ± 4.10
36.16 ± 0.18
98.82 ± 0.63
36.13 ± 0.14

Matching

−

2.50 ± 0.56
6.15 ± 0.38
2.99 ± 0.76
3.49 ± 0.32
3.66 ± 0.26
2.45 ± 0.64

Diverse recommendation

5

10

20

15.81 ± 0.50
13.34 ± 0.77
4.08 ± 0.16
11.63 ± 0.43
7.71 ± 0.18
8.19 ± 0.19

29.81 ± 0.85
26.32 ± 1.38
8.42 ± 0.29
22.79 ± 0.66
15.73 ± 0.34
16.15 ± 0.35

52.43 ± 1.23
47.79 ± 1.96
19.16 ± 0.57
42.37 ± 1.02
31.25 ± 0.64
31.68 ± 0.71

decision-focused approach. The learning problem is to use
node features to predict whether each edge is present or ab-
sent (a classiﬁcation problem). The optimization problem is
to ﬁnd a maximum matching in the predicted graph.

Our experiments use the cora dataset (Sen et al. 2008).
The nodes are scientiﬁc papers and edges represent citation.
Each node’s feature vector indicating whether each word in
a vocabulary appeared in the paper (there are 1433 such fea-
tures). The overall graph has 2708 nodes. In order to con-
struct instances for the decision problem, we partitioned the
complete graph into 27 instances, each with 100 nodes, us-
ing metis (Karypis and Kumar 1998). We divided the nodes
in each instance into the sides of a bipartite graph (of 50
nodes each) such that the number of edges crossing sides
was maximized. The learning problem is much more chal-
lenging than before: unlike in budget allocation, the features
do not contain enough information to reconstruct the citation
network. However, a decision maker may still beneﬁt from
leveraging whatever signal is available.

Diverse recommendation: One application of submod-
ular optimization is to select diverse sets of item, e.g. for
recommendation systems or document summarization. Sup-
pose that each item i is associated with a set of topics t(i).
Then, we aim to select a set of k items which collectively
cover as many topics as possible: f (S) = (cid:12)
(cid:12). Such
formulations have been used across recommendation sys-
tems (Ashkan et al. 2015), text summarization (Takamura
and Okumura 2009), web search (Agrawal et al. 2009) and
image segmentation (Prasad, Jegelka, and Batra 2014).

i∈S t(i)(cid:12)

(cid:12)(cid:83)

In many applications, the item-topic associations t(i) are
not known in advance. Hence, the learning task is to pre-
dict a binary matrix θ where θij is 1 if item i covers topic
j and 0 otherwise. The optimization task is to ﬁnd a set of
k items maximizing the number of topics covered according
to θ. We consider a recommendation systems problem based
on the Movielens dataset (GroupLens 2011) in which 2113
users rate 10197 movies (though not every user rated every
movie). The items are the movies, while the topics are the
top 500 actors. In our problem, the movie-actor assignments
are unknown, and must be predicted only from user ratings.
This is a multilabel classiﬁcation problem where we attempt
to predict which actors are associated with each movie. We
randomly divided the movies into 101 problem instances,
each with 100 movies. The feature matrix y contains the rat-
ings given by each of the 2113 users for the 100 movies in
the instance (with zeros where no rating is present).

Algorithms and experimental setup:

In each domain,

we randomly divided the instances into 80% training and
20% test. All results are averaged over 30 random splits. Our
decision-focused framework was instantiated using feed-
forward, fully connected neural networks as the underlying
predictive model. All networks used ReLU activations. We
experimented with networks with 1 layer, representing a re-
stricted class of models, and 2-layer networks, where the
hidden layer (of size 200) gives additional expressive power.
We compared two training methods. First, the decision-
focused approach proposed above. Second, a two stage ap-
proach that uses a machine learning loss function (mean
squared error for regression tasks and cross-entropy loss
for classiﬁcation). This allows us to isolate the impact of
the training method since both use the same underlying ar-
chitecture. We experimented with additional layers but ob-
served little beneﬁt for either method. All networks were
trained using Adam with learning rate 10−3. We refer to
the 1-layer decision focused network as NN1-Decision and
the 1-layer two stage network as NN1-2Stage (with analo-
gous names for the 2-layer networks). We also compared to a
random forest ensemble of 100 decisions trees (RF-2Stage).
Gradient-based training cannot be applied to random forests,
so benchmark represents a strong predictive model which
can be used by two stage approaches but not by our frame-
work. Lastly, we show performance for a random decision.
Solution quality: Table 1 shows the solution quality that
each approaches obtains on the full pipeline; i.e., the ob-
jective value of its decision evaluated using the true param-
eters. Each value is the mean (over the 30 iterations) and
a bootstrapped 95% conﬁdence interval. For the budget al-
location and diverse recommendation tasks, we varied the
budget k. The decision-focused methods obtain the highest-
performance across the board, tied with random forests on
the synthetic budget allocation task.

We now consider each individual domain, starting with
budget allocation. Both decision-focused methods substan-
tially outperform the two-stage neural networks, obtaining
at least 37% greater objective value. This demonstrates that
with ﬁxed predictive architecture, decision-focused learn-
ing can greatly improve solution quality. NN1-Decision per-
forms somewhat better than NN2-Decision, suggesting that
the simpler class of models is easier to train. However, NN1-
2Stage performs signiﬁcantly worse than NN1-Decision,
indicating that alignment between training and the deci-
sion problem is highly important for simple models to suc-
ceed. RF-2Stage performs essentially equivalently to NN1-
Decision. This is potentially surprising since random for-

Table 2: Accuracy of each method according to standard measures.

Budget allocation

MSE

NN1-Decision
NN2-Decision
NN1-2Stage
NN2-2Stage
RF-2Stage

0.8673e-02 ± 1.83e-04
1.7118e-02 ± 2.65e-04
0.0501e-02 ± 2.67e-06
0.0530e-02 ± 2.27e-06
0.0354e-02 ± 4.17e-06

Matching

CE

AUC

0.994 ± 0.002
0.689 ± 0.004
0.696 ± 0.001
0.223 ± 0.005
0.693 ± 0.000

0.501 ± 0.011
0.560 ± 0.006
0.499 ± 0.013
0.498 ± 0.007
0.500 ± 0.000

Diverse recommendation

CE

AUC

1.053 ± 0.005
1.004 ± 0.022
0.703 ± 0.001
0.690 ± 0.000
0.689 ± 0.000

0.593 ± 0.003
0.577 ± 0.008
0.389 ± 0.003
0.674 ± 0.004
0.500 ± 0.000

Figure 1: (a) ground truth (b) NN1-2Stage (c) NN1-Decision

est are a much more expressive model class. As we will
see later, much of the random forest’s success is due to the
fact that the features in this synthetic domain are very high-
signal; indeed, they sufﬁce for near-perfect reconstruction.
The next two domains, both based on real data, explore low-
signal settings where highly accurate recovery is impossible.
In bipartite matching, NN2-Decision obtains the high-
est overall performance, making nearly over 70% more
matches than the next best method (RF-2Stage, followed
closely by NN2-2Stage). Both 1-layer models perform ex-
tremely poorly, indicating that the more complex learning
problem requires a more expressive model class. However,
the highly expressive RF-2Stage does only marginally better
than NN2-2Stage, demonstrating the critical role of aligning
training and decision making.

In the diverse recommendation domain, NN1-Decision
has the best performance,
followed closely by NN2-
Decision. NN2-2Stage trails by 23%, and NN1-2Stage per-
forms extremely poorly. This highlights the importance of
the training method within the same class of models: NN1-
Decision obtains approximately 2.7 times greater objective
value than NN1-2Stage. RF-2Stage also performs poorly in
this domain, and is seemingly unable to extract any signal
which boosts decision quality above that of random.

Exploration of learned models: We start out by show-
ing the accuracy of each method according to standard mea-
sures, summarized in Table 2. For classiﬁcation domains (di-
verse recommendation, matching), we show cross-entropy
loss (which is directly optimized by the two stage networks)
and AUC. For regression (the budget allocation domain), we

Figure 2: Left: our method’s predicted total out-weight for
each item. Right: predictions from two stage method.

show mean squared error (MSE). For budget allocation and
diverse recommendation, we ﬁxed k = 10.

The two-stage methods are, in almost all cases, signiﬁ-
cantly more accurate than the decision-focused networks de-
spite their worse solution quality. Moreoever, no accuracy
measure is well-correlated with solution quality. On bud-
get allocation, the two decision-focused networks have the
worst MSE but the best solution quality. On bipartite match-
ing, NN2-2Stage has better cross-entropy loss but much
worse solution quality than NN2-Decision. On diverse rec-
ommendation, NN2-2Stage has the best AUC but worse so-
lution quality than either decision-focused network.

This incongruity raises the question of what differentiates
the predictive models learned via decision-focused train-
ing. We now show more a more detailed exploration of
each model’s predictions. Due to space constraints, we fo-
cus on the simpler case of the synthetic budget allocation
task, comparing NN1-Decision and NN1-2Stage. However,
the higher-level insights generalize across domains (see the
supplement for more detailed visualizations).

Figure 1 shows each model’s predictions on an example
instance. Each heat map shows a predicted matrix θ, where
dark entries correspond to a high prediction and light entries
to low. The ﬁrst matrix is the ground truth. The second ma-
trix is the prediction made by NN1-2Stage, which matches
the overall sparsity of the true θ but fails to recover almost all
of the true connections. The last matrix corresponds to NN1-
Decision and appears completely dissimilar to the ground
truth. Nevertheless, these seemingly nonsensical predictions
lead to the best quality decisions.

To investigate the connection between predictions and
decision, Figure 2 aggregates each model’s predictions at
the channel level. Formally, we examine the predicted out-
weight for each channel u, i.e., the sum of the row θu. This
is a coarse measure of u’s importance for the optimiza-
tion problem; channels with connections to many customers

abc0.0000.0250.0500.0750.1000.1250.1500.17513.013.514.014.515.0Predicted out-weight051015Ground truthr2=0.940.000.050.100.150.20Predicted out-weight01020Ground truthr2=0.64are more likely to be good candidates for the optimal set.
Surprisingly, NN1-Decision’s predicted out-weights are ex-
tremely well correlated with the ground truth out-weights
(r2 = 0.94). However, the absolute magnitude of its predic-
tions are skewed: the bulk of channels have low outweight
(less than 1), but NN1-Decision’s predictions are all at least
13. By contrast NN1-2Stage has poorer correlation, making
it less useful for identifying the outliers which comprise the
optimal set. However, it better matches the values of low
out-weight channels and hence attains better MSE. This il-
lustrates how aligning the model’s training with the opti-
mization problem leads it to focus on qualities which are
speciﬁcally important for decision making, even if this com-
promises accuracy elsewhere.

Acknowledgments: This work was supported by the
Army Research Ofﬁce (MURI W911NF1810208) and a Na-
tional Science Foundation Graduate Research Fellowship.

References

[Agrawal et al. 2009] Agrawal, R.; Gollapudi, S.; Halverson,
A.; and Ieong, S. 2009. Diversifying search results.
In
WSDM, 5–14. ACM.
[Alon, Gamzu, and Tennenholtz 2012] Alon, N.; Gamzu, I.;
and Tennenholtz, M. 2012. Optimizing budget allocation
among channels and inﬂuencers. In WWW.
[Amos and Kolter 2017] Amos, B., and Kolter, J. Z. 2017.
Optnet: Differentiable optimization as a layer in neural net-
works. In ICML.
[Ashkan et al. 2015] Ashkan, A.; Kveton, B.; Berkovsky, S.;
and Wen, Z. 2015. Optimal greedy diversity for recommen-
dation. In IJCAI, 1742–1748.
[Belanger, Yang, and McCallum 2017] Belanger, D.; Yang,
B.; and McCallum, A. 2017. End-to-end learning for struc-
tured prediction energy networks. In ICML.
[Bellur and Kulkarni 2007] Bellur, U., and Kulkarni, R.
2007. Improved matchmaking algorithm for semantic web
services based on bipartite graph matching. In ICWS, 86–93.
IEEE.
[Benabbou et al. 2018] Benabbou, N.; Chakraborty, M.; Ho,
X.-V.; Sliwinski, J.; and Zick, Y. 2018. Diversity constraints
in public housing allocation. In AAMAS, 973–981.
[Bertsimas and Dunn 2017] Bertsimas, D., and Dunn, J.
2017. Optimal classiﬁcation trees. Machine Learning
106(7):1039–1082.
and
[Beygelzimer and Langford 2009] Beygelzimer, A.,
Langford, J. 2009. The offset tree for learning with partial
labels. In KDD.
[Bian et al. 2017] Bian, A. A.; Mirzasoleiman, B.; Buhmann,
J. M.; and Krause, A. 2017. Guaranteed non-convex op-
timization: Submodular maximization over continuous do-
mains. In AISTATS.
[Calinescu et al. 2011] Calinescu, G.; Chekuri, C.; P´al, M.;
and Vondr´ak, J. 2011. Maximizing a monotone submodular
function subject to a matroid constraint. SIAM Journal on
Computing 40(6):1740–1766.

[Djolonga and Krause 2017] Djolonga, J., and Krause, A.
2017. Differentiable learning of submodular models.
In
NIPS, 1013–1023.
[Domke 2012] Domke, J.
2012. Generic methods for
optimization-based modeling. In Artiﬁcial Intelligence and
Statistics, 318–326.
[Donti, Amos, and Kolter 2017] Donti, P.; Amos, B.; and
Kolter, J. Z. 2017. Task-based end-to-end model learning
in stochastic optimization. In NIPS.
[Elmachtoub and Grigas 2017] Elmachtoub, A. N., and Gri-
gas, P. 2017. Smart “predict, then optimize”. arXiv preprint
arXiv:1710.08005.
[Fang et al. 2016] Fang, F.; Nguyen, T. H.; Pickles, R.; Lam,
W. Y.; Clements, G. R.; An, B.; Singh, A.; Tambe, M.;
Lemieux, A.; et al. 2016. Deploying paws: Field optimiza-
tion of the protection assistant for wildlife security. In IAAI,
3966–3973.
[Ford et al. 2015] Ford, B.; Nguyen, T.; Tambe, M.; Sintov,
N.; and Delle Fave, F. 2015. Beware the soothsayer: From
attack prediction accuracy to predictive reliability in security
games. In International Conference on Decision and Game
Theory for Security.
[Gould et al. 2016] Gould, S.; Fernando, B.; Cherian, A.;
Anderson, P.; Cruz, R. S.; and Guo, E. 2016. On dif-
ferentiating parameterized argmin and argmax problems
with application to bi-level optimization. arXiv preprint
arXiv:1607.05447.
[GroupLens 2011] GroupLens. 2011. Movielens dataset.
[Hassani, Soltanolkotabi, and Karbasi 2017] Hassani,
Soltanolkotabi, M.; and Karbasi, A.
Methods for Submodular Maximization. In NIPS.
[Horvitz and Mitchell 2010] Horvitz, E., and Mitchell, T.
2010. From data to knowledge to action: A global enabler
for the 21st century. Computing Community Consortium 1.
[Horvitz 2010] Horvitz, E. 2010. From data to predictions
and decisions: Enabling evidence-based healthcare. Com-
puting Community Consortium 6.
[Iyer, Jegelka, and Bilmes 2014] Iyer, R. K.; Jegelka, S.; and
Bilmes, J. A. 2014. Monotone closure of relaxed constraints
in submodular optimization. In UAI.
[Jin et al. 2017] Jin, C.; Ge, R.; Netrapalli, P.; Kakade, S. M.;
and Jordan, M. I. 2017. How to escape saddle points efﬁ-
ciently. In ICML.
[Karypis and Kumar 1998] Karypis, G., and Kumar, V. 1998.
A fast and high quality multilevel scheme for partitioning
irregular graphs. SIAM Journal on Scientiﬁc Computing
20(1):359–392.
[Kempe, Kleinberg, and Tardos 2003] Kempe, D.; Klein-
berg, J.; and Tardos, ´E. 2003. Maximizing the Spread of
Inﬂuence Through a Social Network. In KDD.
[Khalil et al. 2017a] Khalil, E. B.; Dai, H.; Zhang, Y.; Dilk-
ina, B.; and Song, L. 2017a. Learning combinatorial opti-
mization algorithms over graphs. In NIPS.
[Khalil et al. 2017b] Khalil, E. B.; Dilkina, B.; Nemhauser,

H.;
2017. Gradient

G. L.; Ahmed, S.; and Shao, Y. 2017b. Learning to run
heuristics in tree search. In IJCAI.
[Korte et al. 2012] Korte, B.; Vygen, J.; Korte, B.; and Vy-
2012. Combinatorial optimization, volume 2.
gen, J.
Springer.
[Miyauchi et al. 2015] Miyauchi, A.; Iwamasa, Y.; Fuku-
naga, T.; and Kakimura, N. 2015. Threshold inﬂuence model
for allocating advertising budgets. In ICML, 1395–1404.
[Mukhopadhyay et al. 2017] Mukhopadhyay, A.; Vorobey-
chik, Y.; Dubey, A.; and Biswas, G. 2017. Prioritized allo-
cation of emergency responders based on a continuous-time
incident prediction model. In AAMAS, 168–177.
[Niculae et al. 2018] Niculae, V.; Martins, A. F.; Blondel,
M.; and Cardie, C. 2018. Sparsemap: Differentiable sparse
structured inference. In ICML.
[Prasad, Jegelka, and Batra 2014] Prasad, A.; Jegelka, S.;
and Batra, D. 2014. Submodular meets structured: Finding
diverse subsets in exponentially-large structured item sets.
In NIPS.
[Sen et al. 2008] Sen, P.; Namata, G.; Bilgic, M.; Getoor, L.;
Galligher, B.; and Eliassi-Rad, T. 2008. Collective classiﬁ-
cation in network data. AI magazine 29(3):93.
[Soma et al. 2014] Soma, T.; Kakimura, N.; Inaba, K.; and
Kawarabayashi, K.-i. 2014. Optimal budget allocation: The-
In ICML, 351–
oretical guarantee and efﬁcient algorithm.
359.
[Takamura and Okumura 2009] Takamura, H., and Oku-
mura, M. 2009. Text summarization model based on maxi-
mum coverage problem and its variant. In EACL.
[Tschiatschek, Sahin, and Krause 2018] Tschiatschek,
S.;
Sahin, A.; and Krause, A. 2018. Differentiable submodular
maximization. In IJCAI.
[Tu and Gimpel 2018] Tu, L., and Gimpel, K. 2018. Learn-
ing approximate inference networks for structured predic-
tion. In ICLR.
[Viappiani and Boutilier 2010] Viappiani, P., and Boutilier,
C. 2010. Optimal bayesian recommendation sets and my-
opically optimal choice query sets. In NIPS.
[Vinyals, Fortunato, and Jaitly 2015] Vinyals, O.; Fortunato,
M.; and Jaitly, N. 2015. Pointer networks. In NIPS.
[Wang et al. 2006] Wang, H.; Xie, H.; Qiu, L.; Yang, Y. R.;
Zhang, Y.; and Greenberg, A. 2006. Cope: trafﬁc engineer-
ing in dynamic networks. In Sigcomm, volume 6, 194.
[Xue et al. 2016] Xue, Y.; Davies, I.; Fink, D.; Wood, C.; and
Gomes, C. P. 2016. Avicaching: A two stage game for bias
reduction in citizen science. In AAMAS, 776–785.
[Yahoo 2007] Yahoo. 2007. Yahoo! webscope dataset ydata-
ysm-advertiser-bids-v1 0. http://research.yahoo.
com/Academic_Relations.

Proofs
Proof of Theorem 1. We start with the case where all rows
of A are linearly independent. Here, the result follows eas-
ily from Theorem 1 of (Amos and Kolter 2017) since the

Hessian matrix is γI and hence guaranteed to be positive
deﬁnite.

When A has linearly dependent rows, we argue that these
rows can be removed without changing the feasible region.
Consider two rows ai and aj such that for all x, a(cid:62)
i x =
ca(cid:62)
j x for some scalar c. We are guaranteed that the prob-
lem is feasible, meaning that there exists an x which sat-
isﬁes both constraints simultaneously. For this x, we have
a(cid:62)
i x = bi and a(cid:62)
j x, we must
have bi = cbj. Accordingly, constraint i is satisﬁed if and
only if constraint j is satisﬁed, and so removing one of the
constraints leaves the feasible set unchanged. Applying this
argument inductively yields the theorem.

j x = bj. But since a(cid:62)

i x = ca(cid:62)

Proof of Theorem 2. Let xmax = arg maxy∈conv(X ) ||y||2.
We have that

θ(cid:62)x(θ) = max

y

≥ max

y

= max

y

(cid:2)θ(cid:62)y − γ||y||2(cid:3) + ||x(θ)||2
(cid:2)θ(cid:62)y(cid:3) − γ||xmax||2 + γ||x(θ)||2
(cid:2)θ(cid:62)y(cid:3) + γ (cid:0)||x(θ)||2 − ||xmax||2(cid:1)

≥ OP T − γ||x(θ) − xmax||2
≥ OP T − γD

where the second inequality uses the reverse triangle in-

equality.
Proof of Theorem 3. Since X = {x ∈ {0, 1}|V | : (cid:80)
i xi ≤
k}, conv(X ) is described by the two inequality constraints
−Ix ≤ 0 and 1(cid:62)x ≤ k. It is easy to see that the correspond-
ing constraint matrix A has full row rank. Even though F
is not concave, any stationary point (x, λ) must satisfy the
KKT conditions. By applying the implicit function theorem
to differentiate these equations, we get the form

(cid:20) ∇2

xF (x, θ)

AT

diag(λ)A diag(Ax − b)

(cid:21) (cid:20) dx
dθ
dλ
dθ

(cid:21)

=

(cid:21)

(cid:20) d∇xf (x,θ)
dθ
0

So long as the right hand side matrix is invertible al-
most everywhere, the implicit function theorem guarantees
that dx
dθ exists in a neighborhood of x and satisﬁes the
above conditions. Note that at a local maximum, we have
∇2
xF (x, θ) (cid:31) 0, implying that the Hessian matrix must be
invertible. Accordingly, it is easy to show that the RHS ma-
trix is nonsingular by applying the same logic as (Amos and
Kolter 2017) (Theorem 1).

Visualizations
We now show more detailed analysis of the predictions made
by each model in the other two domains: diverse recommen-
dation and bipartite matching. The general trends are sim-
ilar to those observed in the main paper for budget alloca-
tion (although the results are somewhat messier for the real-
data domains). We see that the decision-focused neural net-
work makes apparently nonsensical predications. However,
the out-weight that it predicts for each item is better corre-
lated with the ground truth than for the two stage method.

Figure 3: Diverse recommendation predictions. Top to
bottom: ground truth, our method’s prediction (by NN2-
Decision), two stage prediction (by NN2-2Stage)

Figure 4: Diverse recommendation predicted outweight ac-
cording to NN2-Decision (right) and NN2-2Stage (left).

Figure 5: Bipartite matching predictions. Left
to right:
ground truth adjacency matrix, our method’s prediction
(NN2-Decision), two stage prediction (NN2-2Stage).

Figure 6: Bipartite matching predicted outweight according
to NN2-Decision (right) and NN2-2Stage (left).

250300350400Predicted out-weight0.02.55.07.510.0Ground truthr2=0.34250251252253254Predicted out-weight0.02.55.07.510.0Ground truthr2=0.1825.025.526.0Predicted out-weight0510Ground truthr2=0.13246Predicted out-weight0510Ground truthr2=-0.0