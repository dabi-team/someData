Attack as Defense: Characterizing Adversarial Examples using
Robustness
Guangke Chen
ShanghaiTech University
China

Zhe Zhao
ShanghaiTech University
China

Jingyi Wang
Zhejiang University
China

Yiwei Yang
ShanghaiTech University
China

Fu Song
ShanghaiTech University
China

Jun Sun
Singapore Management University
Singapore

1
2
0
2

r
a

M
3
1

]

R
C
.
s
c
[

1
v
3
3
6
7
0
.
3
0
1
2
:
v
i
X
r
a

ABSTRACT
As a new programming paradigm, deep learning has expanded its
application to many real-world problems. At the same time, deep
learning based software are found to be vulnerable to adversarial
attacks. Though various defense mechanisms have been proposed
to improve robustness of deep learning software, many of them
are ineffective against adaptive attacks. In this work, we propose
a novel characterization to distinguish adversarial examples from
benign ones based on the observation that adversarial examples are
significantly less robust than benign ones. As existing robustness
measurement does not scale to large networks, we propose a novel
defense framework, named attack as defense (A2D), to detect adver-
sarial examples by effectively evaluating an exampleâ€™s robustness.
A2D uses the cost of attacking an input for robustness evaluation
and identifies those less robust examples as adversarial since less
robust examples are easier to attack. Extensive experiment results
on MNIST, CIFAR10 and ImageNet show that A2D is more effective
than recent promising approaches. We also evaluate our defence
against potential adaptive attacks and show that A2D is effective
in defending carefully designed adaptive attacks, e.g., the attack
success rate drops to 0% on CIFAR10.

ACM Reference Format:
Zhe Zhao, Guangke Chen, Jingyi Wang, Yiwei Yang, Fu Song, and Jun
Sun. 2021. Attack as Defense: Characterizing Adversarial Examples using
Robustness. In Proceedings of ACM Conference (Conferenceâ€™17). ACM, New
York, NY, USA, 17 pages. https://doi.org/10.1145/nnnnnnn.nnnnnnn

1 INTRODUCTION
Deep learning (DL) has arguably become a new programming par-
adigm which takes over traditional software programs in many
areas. For instance, it has achieved state-of-the-art performance
in real-world tasks such as autonomous driving [1], medical di-
agnostics [59] and cyber-security [63]. Despite the success, DL
software are still far from dependable (especially for safety- and
security-critical systems) and, like traditional software, they must

Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation
on the first page. Copyrights for components of this work owned by others than ACM
must be honored. Abstracting with credit is permitted. To copy otherwise, or republish,
to post on servers or to redistribute to lists, requires prior specific permission and/or a
fee. Request permissions from permissions@acm.org.
Conferenceâ€™17, July 2017, Washington, DC, USA
Â© 2021 Association for Computing Machinery.
ACM ISBN 978-x-xxxx-xxxx-x/YY/MM. . . $15.00
https://doi.org/10.1145/nnnnnnn.nnnnnnn

be properly tested, and defended in the presence of malicious inputs.
In particular, DL software are known to be brittle to adversarial
examples [10, 21, 67], i.e., by adding a slight perturbation on an
input, a well-trained DL model could be easily fooled.

There is a huge body of work proposing various attack and
defense mechanisms with regard to adversarial examples, from
both the security [6, 21, 31, 50] and software engineering commu-
nity [38, 66, 69, 73, 74]. Since the adversarial attack L-BFGS was
introduced [67], many sophisticated adversarial attacks have been
proposed, such as Fast Gradient Sign Method (FGSM) [21], Iter-
ative Gradient Sign Method (BIM) [31], Jacobian-based Saliency
Map Attack (JSMA) [50], Local Search Attack (LSA) [46], Decision-
Based Attack (DBA) [6], DeepFool [45], and Carlini and Wagnerâ€™s
attack (C&W) [10]. As countermeasures, defense mechanisms are
proposed to improve robustness in the presence of adversarial at-
tacks. Examples include adversarial training [21, 42, 57], defensive
distillation [51] and feature squeezing [82]. These works consti-
tute important steps in exploring the defense mechanisms, yet have
various limitations and are shown to be insufficient [2, 9, 10, 24, 71].
Some efforts also have been made to distinguish adversarial
examples from benign ones and reject those potentially adversarial
ones [73, 74]. A usual underlying assumption is that adversarial and
benign examples differ in a certain subspace distribution, e.g., kernel
density [19], local intrinsic dimensionality [41], manifold [43], logits
values [55], etc. Although these characterizations provide some
insights on the adversarial subspace, they are far from reliably
discriminating adversarial examples alone. Worse yet, attackers can
easily break existing defense mechanisms by specifically designed
adaptive attacks [8, 9, 71].

In this work, we propose a novel characterization, i.e., robustness,
to distinguish adversarial examples from benign ones. Our main
observation is that adversarial examples (crafted by most existing
attack methods) are significantly less robust than benign ones. Dif-
ferent from previous statistical characterizations [19, 41, 43, 55], the
difference in robustness comes from two inherent characteristics
of model training and adversarial example generation. First, the
model training progressively minimizes the loss of each example
with respect to the model. Thus, a benign example is often trained
to be robust against a model with good generalization. As a result,
benign examples are in general relatively far away from the deci-
sion boundary. Second, most adversarial attacks aim to generate
human-imperceptible perturbations which often result in much less
robust just-cross-boundary adversarial examples. The contrasting

 
 
 
 
 
 
Conferenceâ€™17, July 2017, Washington, DC, USA

Zhe Zhao, Guangke Chen, Jingyi Wang, Yiwei Yang, Fu Song, and Jun Sun

robustness characteristics make it suitable to distinguish adversarial
examples from benign ones.

However, it is still an open research problem on how to effectively
measure an inputâ€™s robustness with respect to a DL model. Exist-
ing robustness measurement methods (e.g., CLEVER [79] which
calculates a minimum perturbation required to change an inputâ€™s
label) are often too computationally expensive. In this work, we
propose to utilize the robustness difference from a reversed angle.
Our intuition is that it is easier to employ a successful adversarial
attack on an input that is less robust. Thus, we propose a novel
defense, named attack as defense (A2D), to effectively detect ad-
versarial examples from benign ones. Given an input example, we
apply different kinds of attacks to attack the input and measure
how â€˜easyâ€™ it is to employ a successful attack. An input example
is considered less robust and thus more likely to be adversarial, if
the attacks are easier to succeed. Note that the effectiveness of our
defense (A2D) relies on a set of attacks whose attack cost (easiness)
can be quantitatively measured, which will guide the selection of
attacks from a large number of adversarial attacks in the literature.
Compared to existing adversarial example detection approaches
(which are mostly proven to be ineffective [2, 8â€“10, 24, 71]), our
A2D framework has the following advantages. First, A2D utilizes
the inherent robustness difference caused by the contrasting charac-
teristics of model training and adversarial example generation. To
circumvent the defense, an attack needs to generate robust adver-
sarial examples which in general might induce human-perceptible
large distortion. Second, A2D is essentially an ensemble approach
using the robustness information obtained from different kinds
of attacks which is hard to bypass at once. We evaluated A2D
with a carefully selected set of attacks on three popular datasets
MNIST [34], CIFAR10 [30] and ImageNet [71]. Experiment results
show that A2D can be more effective than recently proposed de-
tection algorithms [19, 41, 73, 74] and remain effective even in the
white-box adversarial setting. We further show that A2D is effective
against adaptive attacks. That is, we show that A2D (combined with
a complementary detection approach for detecting large-distortion
adversarial examples [43] and adversarial training) is very effec-
tive against specifically designed adaptive attacks, e.g., the attack
success rate (ASR) drops from 72% to 0% using our defense with
adversarial training on CIFAR10, and the ASR drops from 100% to
0% on MNIST. We remark that many existing defenses combined
with adversarial training result in lower robustness than adversarial
training on its own [71].

In a nutshell, we make the following contributions:

â€¢ We propose a novel characterization to distinguish adversar-

ial examples from benign ones via robustness.

â€¢ We present detection approaches based on the characteriza-
tion, which can utilize existing attacks and do not need to
modify or retrain the protected model.

â€¢ We conduct extensive experiments to test our observations
and our defense, which outperforms recent promising detec-
tion algorithms.

â€¢ We thoroughly discuss possible adaptive attacks to our de-
fense and evaluate them to our defense integrated with a
complementary detection approach and adversarial training.
The integrated defense is very promising.

2 BACKGROUND
2.1 Adversarial Attacks
In this work, we target deep neural network (DNN) for classification
tasks. We denote a DNN by ğ‘“ : ğ‘‹ â†’ ğ¶, which maps each input
ğ‘¥ âˆˆ ğ‘‹ to a certain label ğ‘ âˆˆ ğ¶. We denote the ground-truth label
of an input ğ‘¥ by ğ‘ğ‘¥ . Given a DNN ğ‘“ and a benign input ğ‘¥ (which
means ğ‘“ (ğ‘¥) = ğ‘ğ‘¥ ), the attackerâ€™s goal is to craft a perturbation Î”ğ‘¥
(measured in different ğ¿ğ‘› norms [10]) for the input ğ‘¥ such that the
DNN ğ‘“ classifies the example Ë†ğ‘¥ = ğ‘¥ + Î”ğ‘¥ as a different label ğ‘“ ( Ë†ğ‘¥),
i.e., ğ‘“ ( Ë†ğ‘¥) â‰  ğ‘“ (ğ‘¥). Such Ë†ğ‘¥ is called an adversarial example.

In the literature, an extensive number of adversarial attacks have
been proposed [6, 10, 11, 14, 18, 21, 26, 31, 44, 45, 47, 49, 50, 67, 72].
We briefly introduce some representative attacks that will be used
for robustness evaluation in our work.
FGSM. Fast Gradient Sign Method (FGSM) [21] uses a loss function
ğ½ (ğ‘¥, ğ‘ğ‘¥ ) (e.g. the cross-entropy loss) describes the cost of classifying
ğ‘¥ as label ğ‘ğ‘¥ , and maximizes the loss to implement an untarget
attack by performing one step gradient ascend from the input ğ‘¥
with a ğ¿âˆ distance threshold ğœ–. Formally, a potential adversarial
example Ë†ğ‘¥ is crafted as follows:

Ë†ğ‘¥ = ğ‘¥ + ğœ– Ã— sign(âˆ‡ğ‘¥ ğ½ (ğ‘¥, ğ‘ğ‘¥ ))
where âˆ‡ğ‘¥ is the partial derivative of ğ‘¥, and sign(Â·) is a sign function
such that sign(ğ‘) is +1 if ğ‘ > 0, âˆ’1 if ğ‘ < 0 and 0 if ğ‘ = 0.
BIM. Basic Iterative gradient Method (BIM) [31] is an iterative
version of FGSM. For each iteration, BIM performs FGSM with
a small step size ğ›¼ and clips the result so that it stays in the ğœ–-
neighbourhood of the input sample. The ğ‘–th iteration is updated by
as follows:

ğ‘¥ğ‘–+1 = clipğœ–,ğ‘¥ (ğ‘¥ğ‘– + ğ›¼ Ã— sign(âˆ‡ğ‘¥ ğ½ (ğ‘¥ğ‘–, ğ‘ğ‘¥ )))
where ğ‘¥ 0 = ğ‘¥, and the iterative process can repeat several times.

The perturbation of FGSM and BIM is restricted by the ğ¿âˆ norm,
measuring the largest change between Ë†ğ‘¥ and ğ‘¥ (i.e. âˆ¥ğ‘¥ âˆ’ Ë†ğ‘¥ âˆ¥âˆ â‰¤ ğœ–).
We could derive ğ¿2 norm (i.e. âˆ¥ğ‘¥ âˆ’ Ë†ğ‘¥ âˆ¥2 â‰¤ ğœ–) attacks by,

Ë†ğ‘¥ = ğ‘¥ + ğœ– Ã— âˆ‡ğ‘¥ ğ½ (ğ‘¥,ğ‘ğ‘¥ )
âˆ‡ğ‘¥ âˆ¥ğ½ (ğ‘¥,ğ‘ğ‘¥ ) âˆ¥2
Similarly, FGSM and BIM can be adapted from untarget attacks to
target ones which specify the target label of an adversarial example.
Compared to fixed step size, there are some optimization-based
attack methods that seek to find adversarial examples with the min-
imal perturbation, such as L-BFGS [67] and C&W [10]. In addition,
JSMA [50] seeks to modify the smallest number of pixels, which is
an attack method with the ğ¿0 norm.

2.2 Robustness
A DNN ğ‘“ is (locally) robust with respect to an input ğ‘¥ and an
ğ¿ğ‘ norm distance threshold ğœ– if for every example Ë†ğ‘¥ such that
âˆ¥ğ‘¥ âˆ’ Ë†ğ‘¥ âˆ¥ğ‘ â‰¤ ğœ–, ğ‘“ (ğ‘¥) = ğ‘“ ( Ë†ğ‘¥) holds. Several approaches have been
proposed to certify robustness, based on SAT/SMT/MILP solv-
ing [16, 28], abstraction refinement [17, 75, 76], and abstract inter-
pretation [20, 61, 62]. Though these approaches feature theoretical
guarantees, they are limited in scalability and efficiency, hence
fail to work for large models in practice. To improve scalability, a
few approaches aiming to achieve statistical guarantees, by claim-
ing robustness with certain probability [4, 56, 79]. Among them,
CLEVER [79] score is an effective metric to estimate robustness

Attack as Defense: Characterizing Adversarial Examples using Robustness

Conferenceâ€™17, July 2017, Washington, DC, USA

by sampling the norm of gradients and fitting a limit distribution
using extreme value theory. For each input ğ‘¥, CLEVER is able to
calculate a minimal perturbation Î”ğ‘¥ needed such that Ë†ğ‘¥ = ğ‘¥ + Î”ğ‘¥
becomes an adversarial example of ğ‘¥. The CLEVER score is an
attack-independent robustness metric for large scale neural net-
works. Readers can refer to [79] for details. In this work, we use
the CLEVER score to compare the robustness of adversarial and
benign examples.

2.3 Problem Formulation
We focus on the detection of adversarial examples as motivated by
many relevant works [19, 41, 73, 74]. The problem is: given an input
example ğ‘¥ to a DNN model ğ‘“ , how to effectively decide whether ğ‘¥ is
benign or adversarial? The fundamental problem is how to better
characterize adversarial examples. Our solution is to use robustness.

Threat Model. We consider a challenging defense scenario which
assumes that the adversary knows all the information of the model
under attack, namely, white-box attacks. Besides, we assume the
detector has access to a set of benign examples, but knows nothing
about how the adversary generates adversarial examples. We also
assume the detector can use various attacks (for robustness evalu-
ation). These assumptions are reasonable in practice, as there are
many publicly available datasets and source of attacks.

3 CHARACTERIZATION
3.1 Robustness: Adversarial vs. Benign
Our detection approach is based on the observation that adversarial
examples are much less robust than benign ones. To understand
the underlying reason, we briefly recap the processes of DL model
training and adversarial example generation. Training a DL model
typically takes multiple epochs. For each epoch, the training dataset
is partitioned into multiple batches and each batch of input exam-
ples is trained once. After each batch, the parameters are updated,
e.g., via stochastic gradient descent. Once all the epochs finish, the
DL model is ready for testing. During training, each example in the
dataset goes through a number of epochs. Consequently, it is often
the case that the trained DL model achieves good generalization
results. Therefore, as illustrated in Figure 1 (left-part), under a rea-
sonable distance threshold ğœ–, most examples in the ğœ–-neighborhood
of a benign example ğ‘¥ are also benign while adversarial examples
are relatively far away from the benign example ğ‘¥.

In the process of adversarial example generation, the attacker
crafts a perturbation Î”ğ‘¥ for a benign example ğ‘¥ such that the
resulting example Ë†ğ‘¥ = ğ‘¥ +Î”ğ‘¥ is adversarial. During the generation of
adversarial examples, attackers often neglect robustness due to the
pursuit of other attributes, such as minimal perturbation, invisibility,
target label classification and query efficiency. These attributes and
robustness are often incompatible, and thus difficult to achieve
simultaneously. Consequently, adversarial examples with small
distortion are very close to the decision boundary [74] and most
examples in the ğœ– â€²-neighbourhood of the adversarial example Ë†ğ‘¥ are
benign (with respect to the original example ğ‘¥). This is illustrated in
Figure 1 (right-part), which is the zoom in of the ğœ– â€²-neighbourhood
of the adversarial example Ë†ğ‘¥ in Figure 1 (left-part).

Figure 1: An illustration of robustness of adversarial and
benign examples. The left-part depicts ğœ–-neighbourhood
of a benign example ğ‘¥ and the right-part depicts ğœ–-
neighbourhood of the adversarial example Ë†ğ‘¥, where each
triangle denotes an example that can be correctly classified
into the label ğ‘ğ‘¥ and each cross denotes an example that can-
not be correctly classified into the label ğ‘ğ‘¥ .

The above observation can be quantified using robustness. We
conduct a quantitative robustness comparison of benign and adver-
sarial examples using the CLEVER score on MNIST and CIFAR10
datasets. For each dataset, we choose the first 100 images from
the test dataset as subjects. Adversarial examples are generated by
applying four representative attacks: FGSM, BIM, JSMA, and C&W.
All the models and attack tools are taken from [19, 41], where the
parameters are presented in Table 5 in the supplementary mate-
rial. The CLEVER scores, in the form of a confidence interval of
90% significance level [12], are shown in Table 1. Column Label for
CLEVER shows the label type for computing the CLEVER scores,
where Target-2/5 denotes the top-2/5 label of the example, and LLC
(least likely class) is the label with the smallest probability. Column
Benign examples shows the CLEVER scores of the benign examples.
Columns FGSM, BIM, JSMA and C&W show the CLEVER scores
of the adversarial examples generated by FGSM, BIM, JSMA and
C&W, respectively. Columns ğœ† show the ratio of the CLEVER scores
of benign examples to that of the adversarial ones for each attack.
The last column Avg. ğœ† shows the ratio of the CLEVER scores of
benign examples to that of all the adversarial examples.

We can observe that the clever scores of benign examples are
much larger than that of adversarial ones for both MNIST and
CIFAR10 datasets, though the difference varies from attacks and
datasets. This indicates that the difference of robustness between
adversarial and benign examples is significant, thus confirms our
observation. We also observe that the ratios ğœ† using untarget/target-
2 label for computing the CLEVER scores are larger than the ones
using other labels. This is because that the label of untarget or
target-2 for each adversarial example is often the label of its be-
nign counterpart, which also confirms our observation that most
examples in ğœ–-neighbourhood of each adversarial example have the
same label of the benign counterpart.

3.2 Attack Cost: Adversarial vs. Benign
Based on the above observation, one could design adversarial ex-
ample detection approaches similar to other characterizations like
label change rate [74]. However, existing techniques for robustness
certification (with statistical guarantees) still have limited scalabil-
ity, and hence are not able to handle large models efficiently. For

ğ‘¥à·œğ‘¥à·œğ‘¥ğœ–ğœ–â€²Benign examplesAdversarial examplesConferenceâ€™17, July 2017, Washington, DC, USA

Zhe Zhao, Guangke Chen, Jingyi Wang, Yiwei Yang, Fu Song, and Jun Sun

Table 1: CLEVER scores with confidence interval of 90% significance level

Dataset

MNIST

CIFAR10

Label for
CLEVER
Untarget
Target-2
Target-5
LLC
Untarget
Target-2
Target-5
LLC

Benign examples

3.5572 Â± 0.3342
3.6711 Â± 0.3296
3.8303 Â± 0.3113
3.8372 Â± 0.3097
0.3851 Â± 0.1850
0.4141 Â± 0.1806
0.4657 Â± 0.1913
0.4829 Â± 0.1913

Adversarial examples

FGSM
0.1093 Â± 0.0506
0.1148 Â± 0.0427
0.2047 Â± 0.0431
0.2390 Â± 0.0421
0.2743 Â± 0.1627
0.2971 Â± 0.1675
0.3389 Â± 0.1675
0.3572 Â± 0.1713

ğœ†
32.55
31.98
18.71
16.06
1.40
1.39
1.37
1.35

BIM
0.0256 Â± 0.0031
0.0258 Â± 0.0031
0.1582 Â± 0.0084
0.1647 Â± 0.0071
0.0329 Â± 0.0033
0.0380 Â± 0.0044
0.0971 Â± 0.0117
0.1091 Â± 0.0132

ğœ†
138.95
142.29
24.21
23.30
11.71
10.90
4.80
4.43

JSMA
0.0550 Â± 0.0060
0.0558 Â± 0.0063
0.1898 Â± 0.0096
0.2120 Â± 0.0076
0.0128 Â± 0.0021
0.0129 Â± 0.0021
0.0610 Â± 0.0061
0.0918 Â± 0.0095

ğœ†
64.68
65.79
20.18
18.10
30.09
32.10
7.63
5.26

C&W
0.0004 Â± 0.0001
0.0004 Â± 0.0001
0.1384 Â± 0.0043
0.1406 Â± 0.0045
0.0005 Â± 0.0002
0.0005 Â± 0.0002
0.0925 Â± 0.0168
0.1035 Â± 0.0180

ğœ†
8893
9178
27.68
27.29
770
828
5.03
4.67

Avg.
ğœ†
74.77
74.62
22.17
20.29
4.81
4.75
3.16
2.92

(a) Score vs. time on MNIST

(b) Score vs. time on CIFAR10

Figure 2: CLEVER score vs. attack time using JSMA

instance, on a single GTX 1080 GPU, the cost of computing the
CLEVER score is near: 450 seconds for each MNIST example and
1150 seconds for each CIFAR10 example using untarget, 50 sec-
onds for each MNIST example and 128 seconds for each CIFAR10
example using target-2/5.

To effectively and efficiently detect adversarial examples, we
propose a novel detection approach, named attack as defense (A2D
for short), which uses the cost of attacking an example to test its
robustness. The underlying assumption is that the more robust the
example is, the more difficult (a larger attack cost) it is to attack.
The implication is that we can decide whether an input example is
likely to be adversarial by utilizing off-the-shelf attacks.

To leverage attack cost to detect adversarial examples, the first
problem needs to be tackled is how to select attacks for defense. In
general, the attack cost should be able to be quantified and reflect
inputsâ€™ robustness. As a result, FGSM is not suitable since it simply
performs one-step perturbation. In contrast, iterative attacks (such
as BIM, JSMA, and C&W) that iteratively search for adversarial
examples with least distortion could be leveraged, as the costs of
such attacks can be quantified and are relevant to inputsâ€™ robustness.
We illustrate this observation using JSMA. JSMA calculates a
saliency map based on the Jacobian matrix to model the impact
that each pixel imposes on the classification result. During each
iteration, JSMA uses a greedy algorithm that modifies certain pixels
to increase the probability of the target label. The process is repeated
until finding an adversarial example or reaching the termination
criteria. The attack cost (time and iteration) of JSMA depends on
the robustness of each example. For an example ğ‘¥ that is less robust
than another one ğ‘¥ â€², an adversarial example of ğ‘¥ can be quickly
constructed using less time/iteration than the one of ğ‘¥ â€². To further
test this observation, we compare the attack time of JSMA on 100
MNIST and 100 CIFAR10 examples whose CLEVER scores range
from 0 to 0.3. The results are reported as scatter plots in Figure 2(a)
and Figure 2(b), which confirm our observation.

Figure 3: Euclidean distances of the average number of iter-
ations between each pair of sets of examples

The next problem is then how to characterize attack costs for
different kinds of attacks. The most direct indicator of attack costs
is the attack time as demonstrated in Figure 2. The attack time of
different examples can reflect their robustness. However, the attack
time is easily affected by the real-time performance of computing
devices in a physical environment, which makes the variance of
attack time intolerable. Therefore, in this work, we propose to use
the number of iterations of the attacks as the indicator of the attack
costs. For an iterative attack, the number of iterations is positively
correlated with attack time (cf. the supplementary material).

To demonstrate the effectiveness of the attack costs for charac-
terizing adversarial examples, we choose 5 types of images (Benign,
FGSM, BIM, DeepFool and C&W), each of which randomly select
1,000 samples, and divided each type of images into two indepen-
dent sets. Then we use JSMA to attack these images and record
the number of iterations required. To show the difference in attack
costs, we calculate the average Euclidean distance of the number
of iterations between each pair of sets of examples, the results are
presented in Figure 3. We can see that for different types of exam-
ples (adversarial vs. benign), the distance is enormous. While for
the same types of examples (adversarial vs. adversarial or benign
vs. benign), the distance is close to zero. It is worth mentioning
that for the examples generated by different attacks, the distance
is also very similar, meaning that even if the adversarial examples

0.000.050.100.150.200.250.30CLEVER score0.00.10.20.30.40.5Time                                CLEVER score0.00.20.40.60.81.0Time0.00      0.05          0.10         0.15          0.20         0.25          0.30                               BenignBenignFGSMFGSMBIMBIMDeepFoolDeepFoolC&WC&WBenignBenignFGSMFGSMBIMBIMDeepFoolDeepFoolC&WC&W0.0000.0310.8880.8860.9570.9570.9410.9450.9690.9690.0310.0000.9190.9170.9870.9880.9720.9751.0001.0000.8880.9190.0000.0020.0680.0690.0530.0560.0810.0810.8860.9170.0020.0000.0700.0710.0550.0580.0830.0830.9570.9870.0680.0700.0000.0010.0150.0120.0130.0130.9570.9880.0690.0710.0010.0000.0160.0130.0120.0120.9410.9720.0530.0550.0150.0160.0000.0040.0280.0280.9450.9750.0560.0580.0120.0130.0040.0000.0250.0250.9691.0000.0810.0830.0130.0120.0280.0250.0000.0000.9691.0000.0810.0830.0130.0120.0280.0250.0000.0000.00.20.40.60.81.0Attack as Defense: Characterizing Adversarial Examples using Robustness

Conferenceâ€™17, July 2017, Washington, DC, USA

are generated by different attacks, they are also â€œcognate" examples
and have similar attack costs.

Utilizing the diversity of attack methods, an ensemble detection
method can be constructed jointly. As we mentioned before, iter-
ative attacks have the potential to be used as defense, so we can
integrate multiple iterative attacks to derive a more robust defense.
Different attacks have the ability to capture different characteriza-
tions. For example, JSMA crafts adversarial examples based on the
ğ¿0 norm, while BIM is based on the ğ¿âˆ norm, thus they measure
the robustness of inputs under different distance metrics. Thanks
to various types of attacks, ensemble multiple attacks will make
the defense more reliable and difficult to bypass.

4 DETECTION APPROACH
In this section, we consider how to detect adversarial examples
by leveraging attack costs. In this work, we propose two effective
detection approaches that are based on ğ‘˜-nearest neighbors (K-NN)
and standard score (Z-score), respectively. The former requires
both benign and adversarial examples, while the latter requires
only benign examples.

Hereafter, we sometimes denote by attackğ‘‘ the attack that is

used as defense, i.e., to generate attack costs.

4.1 K-NN based Detection Approach
Assume that we have two disjoint sets: ğµ the set of benign examples
and ğ´ the set of adversarial examples.

Single detector. Let us consider the attackğ‘‘ ğ‘œ. The attack cost ğ›¼ğ‘¦
of attacking an example ğ‘¦ using the attackğ‘‘ ğ‘œ is regarded as the
fingerprint of ğ‘¦. We can generate a set of fingerprints {ğ›¼ğ‘¦ | ğ‘¦ âˆˆ
ğ´ âˆª ğµ} from the examples ğ‘¦ âˆˆ ğ´ âˆª ğµ by utilizing the attackğ‘‘ ğ‘œ.
For each unknown input ğ‘¥ and parameter ğ¾, we first compute the
attack cost ğ›¼ğ‘¥ of the input ğ‘¥ using the attackğ‘‘ ğ‘œ and then identify
the ğ¾-nearest neighbors ğ‘ğ¾ = {ğ›¼ğ‘¦ğ‘–
| 1 â‰¤ ğ‘– â‰¤ ğ¾ } of ğ›¼ğ‘¥ from the
set {ğ›¼ğ‘¦ | ğ‘¦ âˆˆ ğ´ âˆª ğµ}. The set ğ‘ğ‘˜ is partitioned into two subsets:
ğ´ğ‘¥ = {ğ‘¦ âˆˆ ğ´ | ğ›¼ğ‘¦ âˆˆ ğ‘ğ¾ } and ğµğ‘¥ = {ğ‘¦ âˆˆ ğµ | ğ›¼ğ‘¦ âˆˆ ğ‘ğ¾ }. The input
ğ‘¥ is classified as adversarial if |ğ´ğ‘¥ | > |ğµğ‘¥ |, namely, the number
of adversarial examples is larger than that of benign ones in K-
neighbourhood of the input ğ‘¥.

Ensemble detector. The K-NN based detection approach can be
easily generalized from one attackğ‘‘ to multiply attacksğ‘‘ ğ‘œ1, Â· Â· Â· , ğ‘œğ‘›,
leading to a more robust detector. Under this setting, the fingerprint
of an example ğ‘¦ is a vector of attack costs, (cid:174)ğ›¼ğ‘¦ = (ğ›¼ 1
ğ‘¦, Â· Â· Â· , ğ›¼ğ‘›
ğ‘¦),
where for every 1 â‰¤ ğ‘— â‰¤ ğ‘›, ğ›¼ ğ‘—
ğ‘¦ is the attack cost of the example
ğ‘¦ by utilizing the attackğ‘‘ ğ‘œ ğ‘— . Consequently, we can generate a set
of fingerprints { (cid:174)ğ›¼ğ‘¦ | ğ‘¦ âˆˆ ğ´ âˆª ğµ} from the examples ğ‘¦ âˆˆ ğ´ âˆª ğµ by
utilizing the attacksğ‘‘ ğ‘œ1, Â· Â· Â· , ğ‘œğ‘›. Similar to the single attack setting,
for each unknown input ğ‘¥ and parameter ğ¾, we identify the ğ¾-
nearest neighbors ğ‘ğ¾ = { (cid:174)ğ›¼ğ‘¦ğ‘–
| 1 â‰¤ ğ‘– â‰¤ ğ¾ } of the fingerprint (cid:174)ğ›¼ğ‘¥
of the input ğ‘¥ and partition ğ‘ğ‘˜ into two subsets: ğ´ğ‘¥ = {ğ‘¦ âˆˆ ğ´ |
(cid:174)ğ›¼ğ‘¦ âˆˆ ğ‘ğ¾ } and ğµğ‘¥ = {ğ‘¦ âˆˆ ğµ | (cid:174)ğ›¼ğ‘¦ âˆˆ ğ‘ğ¾ }. The input ğ‘¥ is classified
as adversarial if |ğ´ğ‘¥ | > |ğµğ‘¥ |.

4.2 Z-Score based Detection Approach
Z-score is a well-known concept in statistics for measuring a sample
in terms of its relationship to the mean and standard deviation of

ğ‘–âˆ’ğœ‡
a dataset [33]. The Z-score of a sample ğ‘– is defined by: ğ‘§ =
ğœ ,
where ğœ‡ is the sample mean and ğœ is the sample standard deviation.
Intuitively, the score ğ‘§ indicates how many standard deviations
that the sample ğ‘– is far away from the sample mean. Our Z-Score
based detection approach leverages the distribution of attack costs
of benign examples to check whether an example ğ‘¦ is adversarial
or not. Thus, it is likely more robust with respect to unseen attacks.
Single detector. Let us consider a set ğµ of benign examples and
an attackğ‘‘ ğ‘œ as defense. We can compute the distribution of at-
tack costs of the examples in ğµ. Assume that the distribution is
an approximately normal distribution ğ‘ (ğœ‡, ğœ2). Otherwise, we can
transform it by applying the Box-Cox power transformation [5].
ğ›¼ğ‘¦ âˆ’ğœ‡
Thus, the Z-score ğ‘§ğ‘¦ of an example ğ‘¦ is defined as: ğ‘§ğ‘¦ =
ğœ ,
where ğ›¼ğ‘¦ denotes the cost of attacking ğ‘¦ using the attackğ‘‘ ğ‘œ. For
a given ratio â„ of the sample standard deviation as the threshold,
based on our observation that adversarial examples are less robust
than benign ones, an input ğ‘¥ is classified to adversarial if ğ‘§ğ‘¥ < â„,
i.e., ğ‘¥ is â„ standard deviations away from the sample mean.
Ensemble detector. We can also generalize this approach from
one attackğ‘‘ to multiply attacksğ‘‘ ğ‘œ1, Â· Â· Â· , ğ‘œğ‘›. For each attackğ‘‘ ğ‘œ ğ‘— , we
can construct a Z-Score based detector ğ‘‘ ğ‘— , resulting in detectors
ğ‘‘1, Â· Â· Â· , ğ‘‘ğ‘›. The ensemble detector determines whether an input
is adversarial or not by taking into account the results of all the
detectors ğ‘‘1, Â· Â· Â· , ğ‘‘ğ‘›. Consider ğ‘˜ â‰¤ ğ‘›, the ensemble detector classi-
fies an input to benign if ğ‘˜ detectors classify the input to benign,
otherwise adversarial. The ensemble detector would have high true
positive rates when ğ‘˜ = ğ‘›, high true negative rates when ğ‘˜ = 1.

5 EVALUATION
In this section, we evaluate our approach on three widely used
datasets MNIST, CIFAR10 and ImageNet. The experiments are de-
signed to answer the following research questions:
RQ1. How to select effective attacks for defense?
RQ2. How effective are the selected attacks for defense?
RQ3. How effective and efficient is A2D (i.e., detection)?

5.1 RQ1: Attack Selection
We answer RQ1 by comparing attack costs of adversarial and be-
nign examples. Adversarial examples are crafted in the same setting
as in Section 3.1, using the first 1,000 images from MNIST.

For defense, we choose eight attacks: FGSM, BIM, BIM2 (BIM
under ğ¿2 norm), JSMA, C&W, L-BFGS, LSA, DBA from Foolbox [54],
recommended by [71]. For ease of reading, an attack used as a
defense is marked by a subscript ğ‘‘, e.g., FGSMğ‘‘ . According to
the results of Table 1, we use untarget attack as defense, unless
the attack only supports target attack for which we use target-2
attack. We use the default parameters of Foolbox except that BIMğ‘‘
and BIM2ğ‘‘ immediately terminate when an adversarial example is
found, and DBAğ‘‘ terminates when the MSE between the adversarial
example and its original version is less than 0.02, otherwise the
number of iterations is fixed, namely, the attack costs of all the
examples will be similar.

The results in terms of attack time are reported as box plots in
Figures 4. It is not surprising that the attack time of adversarial and
benign examples using FGMSğ‘‘ are similar, as it is a one-step attack.
Though the results show the variation between different attacks

Conferenceâ€™17, July 2017, Washington, DC, USA

Zhe Zhao, Guangke Chen, Jingyi Wang, Yiwei Yang, Fu Song, and Jun Sun

(a) FGSMğ‘‘

(b) BIMğ‘‘

(c) BIM2ğ‘‘

(d) JSMAğ‘‘

(e) C&Wğ‘‘

(f) L-BFGSğ‘‘

(g) LSAğ‘‘

(h) DBAğ‘‘

Figure 4: Attack time of benign and adversarial examples, where ğ‘¦-axis means seconds

(a) BIMğ‘‘

(b) BIM2ğ‘‘

(c) JSMAğ‘‘

(d) DBAğ‘‘

Figure 5: Attack iterations comparison

and defenses, the differences are often significant when an iterative
attack is used as defense, e.g., BIMğ‘‘ , BIM2ğ‘‘ , JSMAğ‘‘ , L-BFGSğ‘‘ and
DBAğ‘‘ . We find that the differences are not stable when C&Wğ‘‘ and
LSAğ‘‘ are used. This is because that C&Wğ‘‘ implements a binary
search to minimize distortion and may stop searching when there
is a bottleneck, while LSAğ‘‘ suffers from the lower attack success
rate which causes it to have many outliers.

Based on the above results, considering the efficiency and dis-
crepancy in attack time between benign and adversarial examples,
BIMğ‘‘ , BIM2ğ‘‘ , JSMAğ‘‘ , and DBAğ‘‘ will be used as defense in the
follow-up experiments. These four methods cover both white-box
and black-box attacks, as well as different distance metrics ğ¿0, ğ¿2
and ğ¿âˆ. It should be noted that with an amount of adversarial
attacks being proposed (more than 2,000 papers in 2020), it is im-
possible to evaluate all of them. In this work, we only analyze the
pros and cons of the above methods and choose suitable attacks.

The above experiments demonstrate how to quickly select a suit-
able attack as defense. In order to ensure the reliability of BIMğ‘‘ ,
BIM2ğ‘‘ , JSMAğ‘‘ , and DBAğ‘‘ , we also analyze the numbers of itera-
tions. The results are reported as box plots in Figure 5, where the
maximal number of iterations of BIMğ‘‘ and BIM2ğ‘‘ is increased from
the default value 10 to 500 in order to obtain a more significant dif-
ference. Remark that we did not tune parameters, these widely used
parameters are sufficient to achieve expected results. Fine-tuning
parameters may yield better results. We can observe that the differ-
ences in the numbers of iterations are consistent with that of attack
time, therefore. Since the number of iterations does not depend on
computing devices, thus, we will use the number of iterations as
the indicator of attack costs in the follow-up experiments.

Answer to RQ1: Both attack time and the number of iterations
can be used to select effective attacks for defense, while non-
iterative attacks are not effective.

5.2 RQ2: Effectiveness of Attacks as Defense
We answer RQ2 by comparing our approach with four promising
approaches as baselines. The evaluation metric used here is AUROC
(area under the receiver operating characteristic curve), which is
one of the most important evaluation metrics for measuring the
performance of classification indicators. The large the AUROC, the
better the approach for detecting adversarial examples.

The first one [19], denoted by BL1, uses a Gaussian Mixture
Model to model outputs from the final hidden layer of a CNN,
which was considered to be the most effective defense on MNIST
among ten detections in [9]. The second one [41], denoted by BL2,
uses local intrinsic dimension to represent and distinguish adver-
sarial subspace and claims to be better than BL1. BL3 [74] uses
label change rate through model mutation testing to distinguish
adversarial examples. BL4 [73] dissects the middle layer outputs
to construct a fault tolerance approach. We emphasize that BL3
and BL4 [73] were published in ICSEâ€™19 and ICSEâ€™20, respectively.
BL2 is implemented on BL1 which is implemented on Keras (called
Env1), while BL3 and BL4 is implemented on Pytorch (called Env2).
Since the performance of these four defenses may vary due to plat-
forms, CNN models and attack settings, for a fair comparison, we
implement our approach in both Env1 and Env2 and conduct com-
parison directly using the same target models and attacks provided
by each of them. (Parameters are given in Tables 5 and 6 in the
supplementary material.)

Table 2 shows the results in AUROC, where the best one is
highlighted in bold font. Note that BL1, BL2 and BL3 only support
the MNIST and CIFAR10 datasets, thus there are no results on the
ImageNet dataset. Though BL4 considered all these datasets, its
open source tool only supports the ImageNet dataset when we
conduct experiments. Overall, we can observe that our approach
outperforms the baselines in most cases. It is worth mentioning
that our defense parameters are the same in both environments,
which shows its universality, namely, users do not need to adjust
parameters for a specific DL model or platform.

Among the four defenses JSMAğ‘‘ , BIMğ‘‘ , BIM2ğ‘‘ and DBAğ‘‘ on
MNIST and CIFAR10, BIMğ‘‘ performs better than the others in
almost all the cases, while DBAğ‘‘ performs worse than the others
in most cases. This is due to that DBAğ‘‘ is a black-box attack which
is less powerful than the other white-box attacks. An interesting
phenomenon is that the AUROC of JSMAğ‘‘ and DBAğ‘‘ on ImageNet
is better than on MNIST and CIFAR10. This is because that for
images with large dimension, each perturbation generated by JSMA
and DBA is smaller than that of BIM, resulting in a fine-grained
attack as well as a fine-grained indicator of examplesâ€™ robustness.

Benign   FGSM BIM   JSMA  C&W0.00650.00800.00950.01100.01250.0080.0160.0240.0320.040Benign   FGSM BIM   JSMA  C&W0.0060.0120.0180.0240.030Benign   FGSM BIM   JSMA  C&W0.91.82.73.64.5Benign   FGSM BIM   JSMA  C&W510152025Benign   FGSM BIM   JSMA  C&W3691215Benign   FGSM BIM   JSMA  C&W0.51.01.52.02.5Benign   FGSM BIM   JSMA  C&W2.44.87.29.612.0Benign   FGSM BIM   JSMA  C&WBenign FGSM   BIM   JSMA C&W80160240320400Benign FGSM BIM  JSMA C&W100200300400500Benign FGSM BIM   JSMA  C&W80160240320400Benign FGSM BIM    JSMA C&W100200300400500Attack as Defense: Characterizing Adversarial Examples using Robustness

Conferenceâ€™17, July 2017, Washington, DC, USA

Table 2: AUROC comparison of our approach over baselines

Env1

MNIST

CIFAR10

Env2

MNIST

CIFAR10

ImageNet

Attack
FGSM
BIM
JSMA
C&W
FGSM
BIM
JSMA
C&W

Attack
FGSM
JSMA
DeepFool
C&W
BB
FGSM
JSMA
DeepFool
C&W
FGSM
JSMA
DeepFool
C&W

JSMAğ‘‘
0.9653
0.9986
0.9923
1.0
0.6537
0.8558
0.9459
0.9905
JSMAğ‘‘
0.9665
0.9971
0.9918
0.9456
0.9746
0.8808
0.9774
0.9832
0.8842
0.973
0.9962
0.9958
0.9873

BIMğ‘‘
0.9922
0.9996
0.9922
1.0
0.712
0.8636
0.955
0.9984
BIMğ‘‘
0.9883
0.9984
0.9971
0.9870
0.9895
0.8994
0.9890
0.9898
0.9176
0.9763
0.9805
0.9793
0.9731

BIM2ğ‘‘
0.9883
0.9995
0.9914
1.0
0.6474
0.861
0.9526
0.9988
BIM2ğ‘‘
0.9846
0.9974
0.9951
0.9769
0.9852
0.8998
0.9873
0.9902
0.9175
0.9782
0.99
0.9892
0.9801

DBAğ‘‘
0.9504
0.9625
0.9497
0.9672
0.6977
0.8276
0.9452
0.9833
DBAğ‘‘
0.9595
0.984
0.9587
0.8672
0.9535
0.8746
0.9566
0.9769
0.9004
0.9625
0.9937
0.9891
0.9924

BL1
0.8267
0.9786
0.9855
0.9794
0.7015
0.8255
0.8421
0.9217
BL3
0.9617
0.9941
0.9817
0.9576
0.9677
0.8617
0.9682
0.9614
0.9063
-
-
-
-

BL2
0.9161
0.9695
0.9656
0.9502
0.7891
0.8496
0.9475
0.9799
BL4
-
-
-
-
-
-
-

-
0.9617
0.9695
0.9924
0.9636

One may find that BL2 performs better than the others on CIFAR10
adversarial examples crafted by FGSM. This may be because that the
performance of the model is too poor as its accuracy is only 80.3%
on the testing dataset (cf. Table 5 in the supplementary material).
Due to the poor performance of the CIFAR10 model, most attacks
of benign examples can be achieved easily, hence the attack costs
of adversarial examples generated by FGSM are close to benign
examples. This problem could be alleviated by using state-of-the-art
models (such as the model in Env2) or improving the robustness of
the model (such as adversarial training, cf. Section 6.2.2).

Answer to RQ2: Against most attacks on 2 popular platforms
and 3 widely-used datasets, the selected white-box attacks JSMAğ‘‘ ,
BIMğ‘‘ and BIM2ğ‘‘ are more effective than the baselines.

5.3 RQ3: Effectiveness and Efficiency of A2D
We answer RQ3 by applying our K-NN and Z-score based detectors
to check benign examples and adversarial examples generated by
the attacks from BL3 in Env2. We do not consider other baselines,
which only considered the results of AUROC or do not provide cost
analysis. In order to avoid overfitting, we selected different data as
the training set and the test set.

5.3.1 Effectiveness. K-NN based detectors. For each dataset, each
attackğ‘‘ of BIMğ‘‘ , BIM2ğ‘‘ , JSMAğ‘‘ and DBAğ‘‘ as defense and each
attack ğ‘ in Env2, we construct a K-NN based detector through the
attack costs of 1,000 benign examples and 1,000 attack ğ‘ crafted
adversarial examples using the defense attackğ‘‘ . We also construct a
K-NN based ensemble detector END, which consists of 1,000 benign
examples and 1,000 adversarial examples, where each attack of
Env2 contributes 1000 / N adversarial examples (N is the number
of attacks). We set ğ¾ = 100. Results on tuning ğ¾ and ratio between
benign, adversarial examples and classification algorithms are given
in Section C in the supplementary material.

The results are shown in Figures 6(a), 6(c) and 6(e). In general, on
average, the accuracies of detectors JSMAğ‘‘ , BIMğ‘‘ , BIM2ğ‘‘ , DBAğ‘‘

(a) K-NN, MNIST

(b) Z-score, MNIST

(c) K-NN, CIFAR10

(d) Z-score, CIFAR10

(e) K-NN, ImageNet

(f) Z-score, ImageNet

Figure 6: Accuracy of detectors on different inputs

and END are: 90.84%, 98.09%, 96.17%, 87.42% and 99.36% for MNIST,
86.31%, 87.90%, 87.55%, 85.23% and 92.31% for CIFAR10, 93.44%,
94.08%, 95.08%, 91.64% and 94.48% for ImageNet, We find that DBAğ‘‘
performs worse than others on most cases, which is consistent with
AUROC (cf. Table 2). It is worth noting that though the ensemble
detector END does not always achieve the best performance, it has
the highest average accuracy. Thus, it balances the performances
of individual detectors and is more robust.
Z-score based detectors. For each dataset, and each attackğ‘‘ of
BIMğ‘‘ , BIM2ğ‘‘ , JSMAğ‘‘ and DBAğ‘‘ as defense, we construct a Z-score
based detector using the normal distribution of attack costs of 1,000
benign examples via attackğ‘‘ , resulting in detectors BIMğ‘‘ , BIM2ğ‘‘ ,
JSMAğ‘‘ and DBAğ‘‘ . The threshold â„ is -1.281552, which yields 10%
false positive rate on the 1,000 benign examples. The ensemble
detector named by END consists of these four detectors. It classifies
an input as benign if no less than 2 detectors classify the input as
benign, otherwise adversarial, namely, ğ‘˜ = 2. Results on tuning ğ‘˜
and â„ are given in Section C in the supplementary material.

The results are shown in Figures 6(b), 6(d) and 6(f). We can
observe that they are able to achieve comparable or even better
accuracy than K-NN based detectors, although Z-score based detec-
tors only use benign examples, whereas K-NN based detectors use
both benign and adversarial examples.

5.3.2 Efficiency. For a fair comparison with BL3, we report the
detection costs of the Z-score based detectors here, although it may
be slightly worse than K-NN based detector. The reason is that
the threshold of Z-score detectors can be easily adjusted to ensure
that detection accuracy on benign examples is close to the baseline.

FGSMJSMADeepfoolC&WBBBenign0.00.20.40.60.81.0JSMAdBIMdBIM2dDBAdEND0.00.20.40.60.81.0JSMAdBIMdBIM2dDBAdENDFGSMJSMADeepfoolC&WBBBenignFGSMJSMADeepfoolC&WBenign0.00.20.40.60.81.0JSMAdBIMdBIM2dDBAdEND0.00.20.40.60.81.0JSMAdBIMdBIM2dDBAdENDFGSMJSMADeepfoolC&WBenign0.00.20.40.60.81.0JSMAdBIMdBIM2dDBAdENDFGSMJSMADeepfoolC&WBenign0.00.20.40.60.81.0FGSMJSMADeepfoolC&WBenignJSMAdBIMdBIM2dDBAdENDConferenceâ€™17, July 2017, Washington, DC, USA

Zhe Zhao, Guangke Chen, Jingyi Wang, Yiwei Yang, Fu Song, and Jun Sun

Table 3: Cost analysis of our detector with accuracy

Dataset

MNIST

CIFAR10

ImageNet

Detector
BL3
JSMAğ‘‘
BIMğ‘‘
BIM2ğ‘‘
DBAğ‘‘
BL3
JSMAğ‘‘
BIMğ‘‘
BIM2ğ‘‘
DBAğ‘‘
BL3
JSMAğ‘‘
BIMğ‘‘
BIM2ğ‘‘
DBAğ‘‘

#adv Accadv
96.4%
66
95.4%
20
16
99.8%
99.6%
38
88.2%
92
90.6%
67
92.6%
6
14
93.0%
92.7%
29
87.7%
252
-
-
95.4%
6
95.2%
1
96.7%
1
93.9%
143

#benign
463
240
148
352
319
376
33
65
129
744
-
67
4
7
451

Threshold Accbenign

-
53
122
189
195
-
13
35
71
409
-
11
2
2
219

89.7%

â‰¥ 89.7%

74.0%

â‰¥ 74.0%

-
88.6%
89.6%
88.5%
88.0%

As both our method and the baseline BL3 are query-intensive, we
compare the number of queries for efficiency comparison.

The results are reported in Table 3. Columns #adv and #benign
give the number of queries to the model for adversarial and benign
examples on average. Columns Accadv and Accbenign respectively
give the accuracy for adversarial and benign examples on average.
By limiting the accuracy on benign examples to the one of BL3,
we observe that all the white-box defenses (i.e., JSMAğ‘‘ , BIMğ‘‘ and
BIM2ğ‘‘ ) outperform BL3 in terms of the number of queries on both
MNIST and CIFAR10. Furthermore, they also achieve better accu-
racy than BL3 on CIFAR10, while both BIMğ‘‘ and BIM2ğ‘‘ achieve bet-
ter accuracy than BL3 on MNIST. We also provide the results on Im-
ageNet in Table 3. The results show that on ImageNet, A2D can still
detect adversarial examples effectively and efficiently. BIMğ‘‘ /BIM2ğ‘‘
are able to detect adversarial examples using one query. This demon-
strates that our method is also efficient on high-resolution images.
We finally remark that BL3 does not support ImageNet and the
other baselines either provide only AUROC without constructing a
detector or do not provide cost analysis.

It is not surprising that the cost of DBAğ‘‘ is the largest one, as
it is a label-only black-box attack. It is important to mention that
black-box attacks used as defense do not need any information of
the models, hence using black-box attacks as defense preserve the
privacy of the models while its effectiveness is still acceptable.

We emphasize that there is still space for optimization. One latent
optimization is to add an upper bound on the number of iterations,
as adversarial examples often need fewer iterations than benign. If
the number of iterations reaches the bound and the attack fails, the
input can be considered as benign. This optimization can reduce
the number of iterations (hence queries) for benign examples and
both true and false positive rates will not be affected.

Discussion. Here we briefly discuss how our approach can be used
in practice as different detectors have different accuracies. Consid-
ering the tradeoff between the efficiency and accuracy, one can use
JSMAğ‘‘ , BIMğ‘‘ or BIM2ğ‘‘ as defense according to the dimension of
images. If one expects a more reliable and higher accurate detector,
an ensemble detector such as the END detector can be used. If the
privacy of the model matters, a black-box attack based detector
such as DBAğ‘‘ is better.

Answer to RQ3: A2D is able to efficiently and effectively detect
adversarial examples with lower false positive rate. It is consid-
erably more effective and efficient than the baseline BL3.

5.4 Threats to Validity
The threats to validity of our study include external and internal
threats. The selection of the subject datasets and target models
could be one of the external threats. We tried to counter this issue
by using 3 widely-used datasets and 5 pre-trained models from
well-established works.

The second external threat is the works we chose for compar-
ison. To mitigate this threat, we compare with the works from
both the artificial intelligence community (e.g., BL1 and BL2) and
the software engineering community (e.g., BL3 and BL4). BL1 is
the most effective defense on MNIST among ten detections in [9],
while BL2 is claimed better than BL1 [41]. Both BL1 and BL2 are
widely-used for comparison in the literature [24, 35, 40, 58]. BL3 and
BL4 are state-of-the-art methods from the perspective of software
engineering. It is worth noting that the comparison of baselines
was conducted on the repositories and parameters provided by the
original authors, to reproduce their best performance, although it
may be unfair to our method.

A further external threat is the knowledge of the adversary. The
same to baselines, we evaluated our approach against the original
models and assume that the adversary is unaware of the existence
of the detection. In practice, the adversary may learn that A2D
has been used via social engineering or other methods, and use a
more threatening, specified attack method, called adaptive attacks
in [71]. We discuss and evaluate adaptive attacks against our defense
method in Section 6. We do not perform the same adaptive attack
on the baselines, as adaptive attacks are usually designed for each
specific defense method.

The internal threat mainly comes from the selection of attacks
as defense. We approximate model robustness of examples by the
cost of attacking them while attacks may differ in their capabil-
ity. To mitigate this threat, we studied various attacks, covering
white-box and black-box attacks, and ğ¿0, ğ¿2 and ğ¿âˆ norm based
attacks. Experimental results indicate that our defense performs
well regardless of the selected attacks, although a minor difference
can be observed. Based on this, we conclude that our answers to
research questions should generally hold.

6 ON ADAPTIVE ATTACKS
Lots of effective defenses have been shown to be ineffective in the
presence of adaptive attacks [8â€“10, 24, 71]. Thus, adaptive attacks
are the main threat to defense approaches. In this section, we study
possible adaptive attacks to our defense.

6.1 Potential Bypass Approaches
Increasing Attack Costs. A straightforward approach that
6.1.1
may be used to bypass our defense is to increase the attack costs
so that the attack costs of adversarial and benign examples are
similar. To increase attack costs of adversarial examples, one can
incorporate attack costs into the loss function used to identify

Attack as Defense: Characterizing Adversarial Examples using Robustness

Conferenceâ€™17, July 2017, Washington, DC, USA

MNIST

Table 4: Robustness vs. confidence of adversarial examples
0
â‰ˆ 0
1.01
1.71
â‰ˆ 0
1.37
0.41

ğœ…
CLEVER Score
No. of Iterations
ğ¿2 distance
CLEVER Score
No. of Iterations
ğ¿2 distance

2
0.11
10.36
1.91
0.07
8.53
0.52

4
0.14
20.28
2.11
0.08
17.29
0.67

6
0.14
31.29
2.32
0.09
24.47
0.82

8
0.17
42.59
2.53
0.13
34.4
0.99

CIFAR10

adversarial examples. For instance, the adversary could change the
loss function to

ğ½ â€²(ğ‘¥) = ğ½ (ğ‘¥) + ğ›½ Â· max(cost âˆ’ attack_cost(ğ‘¥), 0)
where ğ½ (ğ‘¥) is the original loss function, ğ›½ is a parameter for bal-
ancing two terms of ğ½ â€²(ğ‘¥), cost is the expected attack cost such
as the mean of attack costs of benign examples or even the thresh-
old of our Z-score based detection approach, and attack_cost(ğ‘¥)
denotes the attack cost of ğ‘¥ via some attacks. Minimizing the new
loss increases the attack cost until exceeding cost.
Discussion. This adaptive attack to our defense is infeasible if not
impossible. First, the function attack_cost(ğ‘¥) non-differentiable,
consequently, the loss function ğ½ â€²(ğ‘¥) cannot be solved via gradient-
based algorithms. Second, non-gradient-based iterative attacks have
to run some attacks internally during each iteration in order to
check if the attack succeeds or not. This definitely results in high
computational complexity, thus becoming infeasible .

Increasing Robustness. An alternative approach that may be
6.1.2
used to bypass our defense is to increase the robustness of adver-
sarial examples, aiming to indirectly increase the attack costs. How-
ever, it is non-trivial to directly control the robustness of adversarial
examples. We propose to increase the confidence/strength of adver-
sarial examples, initially considered by Carlini and Wagner [10] for
increasing transferability of adversarial examples between different
models. Confidence is controlled by introducing a parameter ğœ… into
the loss function ğ½ (ğ‘¥), thus, the loss function becomes

ğ½ ğœ… (ğ‘¥) = max(ğ½ (ğ‘¥), âˆ’ğœ…)
where the larger the parameter ğœ…, the higher the confidence of the
adversarial example.

The relation between robustness and confidence of adversarial
examples is confirmed by the following experiment. We mount the
C&W attack on the previous 100 MNIST and 100 CIFAR10 images
under the same setting as [8], by varying the value of ğœ… and mea-
suring the robustness using the CLEVER scores. The results are
reported in Table 4. The experiment results show that the adver-
sary is able to increase the robustness of adversarial examples by
increasing the confidence. Therefore, high-confidence adversarial
examples have the potential to bypass our defense.
Discussion. This adaptive attack is feasible, but will introduce large
distortion into adversarial examples when ğœ… increases, observed
from Table 4. As our defense changes neither inputs nor models, it
can be seamlessly combined with other defenses to defend against
this adaptive attack.
â€¢ The first method is to combine with other defenses that are
aimed at detecting adversarial examples with large distortion
(e.g., [36, 43, 55]). This would be able to detect a wide spectrum
of adversarial examples.

(a) A2D + AE

(b) A2D + AT

Figure 7: Adaptive attack results

â€¢ The second method is to combine with adversarial training [21,
42, 67] which enhances the DL model. Indeed, a successful attack
to an adversarially trained model often introduces large distor-
tion while the adaptive attack also introduces large distortion
to bypass our defense, consequently, the distortion becomes too
large to be human-perceptible.

6.2 Evaluation of Adaptive Attacks
Since the first adaptive attack is infeasible, we only evaluate the
second one which is implemented based on C&W [8]. We evaluate
this adaptive attack by varying the parameter ğœ… from 0 to 20.

To evaluate the effectiveness of our defense combined with
other defenses, we consider the autoencoder based detector (AE
for short) [43] and PGD adversarial training (AT for short) [42].
The AE trains a classifier ğ‘“ğ‘ğ‘’ based on benign examples in order to
detect any adversarial examples with large distortion by checking
if ğ‘‘ (ğ‘¥, ğ‘“ğ‘ğ‘’ (ğ‘¥)) is greater than a different threshold ğœ, where ğ‘‘ is a
distance function, e.g., the mean squared error âˆ¥ğ‘¥ âˆ’ ğ‘“ğ‘ğ‘  (ğ‘¥)âˆ¥2.

2

D with AE. For ease of evaluation, we conduct experi-
6.2.1 A
ments using the MNIST dataset under the same settings as [43]
which provides a trained AE. In our experiments, the maximal ğ¿2
norm distortion is 8.4 which is approximated from the maximal
ğ¿âˆ norm distortion 0.3 in Madryâ€™s challenges [32]. Note that our
maximal ğ¿2 distortion allows perturbations to be greater than the
maximal ğ¿âˆ distortion for some pixels. Such large perturbations
are often challenging for defense. We use BIMğ‘‘ as defense and the
corresponding Z-score based detector which only requires benign
examples. Thus, it is a relatively weaker defense. We denote by A2D
our detector and A2D + AE the combined detector.
Results. The results are reported in Figure 7(a). From Figure 7(a) ,
we can observe that without any defense, the attack success rate
(ASR) is always 100%. With the increase of ğœ…, the detection rate
of our defense A2D decreases slightly. Specifically, A2D is able to
detect all of the adversarial examples when ğœ… â‰¤ 15, while only
about 3% of adversarial examples can bypass A2D when ğœ… = 20. We
also observe that both the ğ¿2 distortion and detection rate of AE
increase with the increase of ğœ…. About 21% of adversarial examples
can bypass AE when ğœ… = 0, while all adversarial examples can be
detected by AE when ğœ… = 20. Therefore, the ASR is always 0% when
the combined defense A2D + AE is applied.
Summary. The above results demonstrate the benefit of combining
two complementary defenses. Although an adaptive attack can
slightly reduce the effectiveness of A2D by increasing robustness
of adversarial examples, the combination of A2D and AE is able to
completely defend against such adaptive attack.

05 101520Confidence in Carlini L2 attack0%20%40%60%80%100%Adaptive attack success rateNo defenseA2DAEA2D+AE0.01.63.24.86.48.0L2 DistortionL2 Distortion05101520Confidence in Carlini L2 attack0%20%40%60%80%100%Adaptive attack success rateNo defenseA2D012345L2 DistortionL2 DistortionConferenceâ€™17, July 2017, Washington, DC, USA

Zhe Zhao, Guangke Chen, Jingyi Wang, Yiwei Yang, Fu Song, and Jun Sun

Figure 8: Adversarial examples for different ğœ…

(a) Benign

(b)

(c)

(d)

Figure 9: Adversarial examples on the models without AT
(b), with AT (c) and with A2D + AT (d)

Case study. Figure 8 shows adversarial examples of a target attack
from 4 to 0 with different ğœ…. The perturbation for ğœ… = 20 is about
twice larger than that for ğœ… = 0 and can be easily detected by AE.
More images are given in Section D in the supplementary material.

2

D with AT. For ease of evaluation, we conduct experiments
6.2.2 A
using the CIFAR10 dataset under the same settings as [42] which
provides an adversarially trained DL model. In our experiments,
the maximal ğ¿2 norm distortion is 1.6 which is approximated from
the maximal ğ¿âˆ norm distortion 0.03 in Madryâ€™s challenges [32].
We use the same Z-score based detector as in Section 6.2.1.

Results. The results are shown in Figure 7(b). Note that adversarial
training (AT) is not a detector, so â€˜No defenseâ€™ is equivalent to â€˜ATâ€™.
We can observe that AT is not very promising when ğœ… is smaller, e.g.,
72% ASR for ğœ… = 0. With the increase of ğœ… (i.e., increasing robustness
of adversarial examples), the ASR drops to 21% when ğœ… = 10 and 0%
ASR when ğœ… â‰¥ 15. This is because that finding adversarial examples
with distortion limited to the maximal ğ¿2 threshold 1.6 becomes
more difficult for the adversarially trained model. Recall that our
defense A2D is good at detecting adversarial examples with small
distortion (i.e., low-confidence). Therefore, the combined defense
is very effective. For instance, all adversarial examples with ğœ… = 0
can be detected by A2D, hence the ASR drops from 72% to 0%. The
adaptive attack achieves no more than 3% ASR on the adversarially
trained model.

Summary. To bypass our defense on adversarially trained models,
the adversary has to introduce much large distortion. When per-
turbations are limited to human-imperceptible, it becomes difficult
to bypass our defense on adversarially trained models.

Case study. Figure 9 shows adversarial examples of a target at-
tack from â€˜airplaneâ€™ to â€˜catâ€™. Without any defense, an adversarial
example with less distortion can be crafted, cf. Figure 9(b). With
AT, it requires more distortion to craft an adversarial example, cf.
Figure 9(c). If both A2D and AT are enabled, it requires much more
distortion to craft adversarial examples, cf. Figure 9(d). Now the
distortion is too large to be human-perceptible, and we can clearly
see the silhouettes of â€˜catsâ€™ on the adversarial example. More images
are given in Section D in the supplementary material.

7 RELATED WORK
As a new type of software system, neural networks have received ex-
tensive attention over the last five years. We classify existing works
along three dimensions: adversarial attack, adversarial defense, and
neural network testing & verification.
Adversarial attack. Adversarial attacks aim to misjudge the neural
network by adding perturbations that are imperceptible to humans.
We have introduced common attacks in Section 2. We selected mul-
tiple types of methods to generate adversarial examples, FGSM [21],
BIM [31], JSMA [50], DeepFool [45], C&W [10] and substitute model
attack [48]. We also selected multiple attacks as defense. Meanwhile,
several adversarial examples have the ability to carry out attacks
in the real environment [18, 27, 31], and pose threats to neural
networks, which is a new programming paradigm.
Adversarial defense. A typical defense method is adversarial
training [21, 31, 42, 67], which produces adversarial examples and
injects them into training data. Another type of defenses protects
models by pre-processing the input data [7, 23, 81] or projects po-
tential adversarial examples onto the benign data manifold before
classifying them [43, 64]. Detection is another defense approach to
adversarial examples. If an input is detected as adversarial, it will
be rejected without being fed to the model [19, 35, 41, 82]. However,
it has been shown that most defense methods except adversarial
training can be easily bypassed by adaptive attack with backward
pass differentiable approximation and expectation over transfor-
mation [2, 3, 9, 71]. Until today this problem is still of considerable
interest. However, most detection methods are not effective on
high-resolution images (e.g., ImageNet dataset) [19, 41, 43, 74] or
do not consider adaptive attacks [19, 41, 43, 74, 74]. Our defense
method is effective on 3 widely-used datasets covering both low-
and high-resolution images. We also evaluated our defense against
potential adaptive attacks and demonstrate its effectiveness.
Neural network testing & verification. Some works look for
vulnerabilities in neural networks from the perspective of soft-
ware testing. For example, DeepXplore [52] proposed a testing
technique to find adversarial examples guided neuron coverage.
After that, a series of coverage criteria have been proposed for neu-
ral network testing [29, 38, 65]. Other testing methods also have
been adapted to test neural networks such as concolic testing [66],
mutation testing [39, 60], and so on [37, 80]. Some testing meth-
ods focus on different application scenarios of neural networks,
including DeepTest [69], DeepRoad [84], Deepbillboard [86] and
object-relevance metamorphic testing [68]. Some other works fo-
cus on testing the neural network at the architecture level [85] or
the deep learning library itself [77]. We do not use testing criteria
to model the robustness of examples, as testing criteria are not
necessarily correlated with robustness [13, 83].

Various formal verification techniques have been proposed to
verify robustness property against neural networks [4, 15â€“17, 20,
22, 25, 28, 53, 61, 62, 70, 75, 78]. Formal verification provides prov-
able or theoretic guarantees, and robustness is also the source of
our defense approach. However, formal verification suffers from
the scalability problem, due to the high computational complexity.
Therefore, we used attack cost instead.

[74] and [73] are very close to our defense approach. They also
considered the problem of identifying adversarial examples from

Benign=0L2=1.99=5L2=2.63=10L2=3.19=15L2=3.61=20L2=3.87BenignL2=0.31L2=1.39L2=2.13Attack as Defense: Characterizing Adversarial Examples using Robustness

Conferenceâ€™17, July 2017, Washington, DC, USA

the perspective of software engineering, by leveraging mutation
testing and model anatomy respectively. However, both of them
have to modify the original model, while our defense approach
does not, hence is easy to deploy. When using white-box attacks as
defense, the model only needs to provide an interface for logits and
gradients, rather than model parameters. When using black-box
attacks as defense, the model only needs to provide the classification
results, thus protecting the privacy of the model. Inspired by [8, 24,
40, 71], we discussed and evaluated adaptive attacks to our defense.
However, [74] and [73] do not consider adaptive attacks, hence it
is unclear whether they are still effective under adaptive attacks.

8 CONCLUSION
We have proposed a novel characterization of adversarial examples
via robustness. Based on the characterization, we proposed a novel
detection approach, named attack as defense (A2D), which utilizes
existing attacks to measure examplesâ€™ robustness. We conducted
extensive experiments to evaluate our observations and detection
approach A2D, showing that it outperforms four recent promising
approaches. We also thoroughly discussed the main threat (i.e.,
adaptive attacks) to our defense and evaluated them to our defense.
By combing our defense with an existing defense and adversarial
training, the results are very promising, e.g., the ASR drops from
72% to 0% on CIFAR10, and drops from 100% to 0% on MNIST.

REFERENCES
[1] Apollo. An open, reliable and secure software platform for autonomous driving

systems. http://apollo.auto, 2018.

[2] Anish Athalye, Nicholas Carlini, and David A. Wagner. Obfuscated gradients
give a false sense of security: Circumventing defenses to adversarial examples.
In Proceedings of the 35th International Conference on Machine Learning, pages
274â€“283, 2018.

[3] Anish Athalye, Logan Engstrom, Andrew Ilyas, and Kevin Kwok. Synthesizing
robust adversarial examples. In Proceedings of the 35th International Conference
on Machine Learning, pages 284â€“293, 2018.

[4] Osbert Bastani, Yani Ioannou, Leonidas Lampropoulos, Dimitrios Vytiniotis,
Aditya V. Nori, and Antonio Criminisi. Measuring neural net robustness with
constraints. In NIPS, pages 2613â€“2621, 2016.

[5] George EP Box and David R Cox. An analysis of transformations. Journal of the

Royal Statistical Society: Series B (Methodological), 26(2):211â€“243, 1964.

[6] Wieland Brendel, Jonas Rauber, and Matthias Bethge. Decision-based adver-
sarial attacks: Reliable attacks against black-box machine learning models. In
International Conference on Learning Representations, 2018.

[7] Jacob Buckman, Aurko Roy, Colin Raffel, and Ian Goodfellow. Thermometer
encoding: One hot way to resist adversarial examples. In International Conference
on Learning Representations, 2018.

[8] Nicholas Carlini and David Wagner. Magnet and â€œefficient defenses against
adversarial attacks" are not robust to adversarial examples. CoRR, abs/1711.08478,
2017.

[9] Nicholas Carlini and David A. Wagner. Adversarial examples are not easily
In Proceedings of the 10th ACM

detected: Bypassing ten detection methods.
Workshop on Artificial Intelligence and Security, pages 3â€“14, 2017.

[10] Nicholas Carlini and David A. Wagner. Towards evaluating the robustness of
neural networks. In Proceedings of IEEE Symposium on Security and Privacy (S&P),
pages 39â€“57, 2017.

[11] Pin-Yu Chen, Yash Sharma, Huan Zhang, Jinfeng Yi, and Cho-Jui Hsieh. EAD:
elastic-net attacks to deep neural networks via adversarial examples. In Proceed-
ings of the Thirty-Second AAAI Conference on Artificial Intelligence (AAAI), pages
10â€“17, 2018.

[12] Frederik Michel Dekking, Cornelis Kraaikamp, Hendrik Paul LopuhaÃ¤, and Lu-
dolf Erwin Meester. A Modern Introduction to Probability and Statistics: Under-
standing why and how. Springer Science & Business Media, 2005.

[13] Yizhen Dong, Peixin Zhang, Jingyi Wang, Shuang Liu, Jun Sun, Jianye Hao, Xinyu
Wang, Li Wang, Jinsong Dong, and Ting Dai. An empirical study on correlation
between coverage and robustness for deep neural networks.

[14] Yuchao Duan, Zhe Zhao, Lei Bu, and Fu Song. Things you may not know about
adversarial example: A black-box adversarial image attack. CoRR, abs/1905.07672,
2019.

[15] Krishnamurthy Dvijotham, Robert Stanforth, Sven Gowal, Timothy A. Mann,
and Pushmeet Kohli. A dual approach to scalable verification of deep networks.
CoRR, abs/1803.06567, 2018.

[16] RÃ¼diger Ehlers. Formal verification of piece-wise linear feed-forward neural
In Proceedings of the 15th International Symposium on Automated

networks.
Technology for Verification and Analysis, pages 269â€“286, 2017.

[17] Yizhak Yisrael Elboher, Justin Gottschlich, and Guy Katz. An abstraction-based
framework for neural network verification. In Proceedings of The 32nd Interna-
tional Conference on Computer-Aided Verification (CAV), 2020.

[18] Kevin Eykholt, Ivan Evtimov, Earlence Fernandes, Bo Li, Amir Rahmati, Chaowei
Xiao, Atul Prakash, Tadayoshi Kohno, and Dawn Song. Robust physical-world at-
tacks on deep learning visual classification. In Proceedings of 2018 IEEE Conference
on Computer Vision and Pattern Recognition, pages 1625â€“1634, 2018.

[19] Reuben Feinman, Ryan R Curtin, Saurabh Shintre, and Andrew B Gardner. De-
tecting adversarial samples from artifacts. arXiv preprint arXiv:1703.00410, 2017.
[20] Timon Gehr, Matthew Mirman, Dana Drachsler-Cohen, Petar Tsankov, Swarat
Chaudhuri, and Martin T. Vechev. AI2: safety and robustness certification of
neural networks with abstract interpretation. In Proceedings of the 2018 IEEE
Symposium on Security and Privacy, pages 3â€“18, 2018.

[21] Ian Goodfellow, Jonathon Shlens, and Christian Szegedy. Explaining and har-
nessing adversarial examples. In Proceedings of the 3th International Conference
on Learning Representations (ICLR), 2015.

[22] Divya Gopinath, Guy Katz, Corina S. Pasareanu, and Clark Barrett. Deepsafe: A
data-driven approach for assessing robustness of neural networks. In Proceedings
of the 16th International Symposium on Automated Technology for Verification and
Analysis, pages 3â€“19, 2018.

[23] Chuan Guo, Mayank Rana, Moustapha Cisse, and Laurens Van Der Maaten.
Countering adversarial images using input transformations. arXiv preprint
arXiv:1711.00117, 2017.

[24] Warren He, James Wei, Xinyun Chen, Nicholas Carlini, and Dawn Song. Adver-
sarial example defense: Ensembles of weak defenses are not strong. In Proceedings
of the 11th USENIX Workshop on Offensive Technologies, 2017.

[25] Xiaowei Huang, Marta Kwiatkowska, Sen Wang, and Min Wu. Safety verification
of deep neural networks. In Proceedings of the 29th International Conference on
Computer Aided Verification, pages 3â€“29, 2017.

[26] Andrew Ilyas, Logan Engstrom, Anish Athalye, and Jessy Lin. Black-box adver-
sarial attacks with limited queries and information. In Proceedings of the 35th
International Conference on Machine Learning, pages 2142â€“2151, 2018.

[27] Steve T.K. Jan, Joseph Messou, Yen-Chen Lin, Jia-Bin Huang, and Gang Wang.
Connecting the digital and physical world: Improving the robustness of adver-
sarial attacks. In AAAI, 2019.

[28] Guy Katz, Clark W. Barrett, David L. Dill, Kyle Julian, and Mykel J. Kochender-
fer. Reluplex: An efficient SMT solver for verifying deep neural networks. In
Proceedings of the 29th International Conference on Computer Aided Verification,
pages 97â€“117, 2017.

[29] Jinhan Kim, Robert Feldt, and Shin Yoo. Guiding deep learning system testing
In 2019 IEEE/ACM 41st International Conference on

using surprise adequacy.
Software Engineering (ICSE), pages 1039â€“1049. IEEE, 2019.

[30] Alex Krizhevsky. Learning multiple layers of features from tiny images. Technical

report, 2009.

[31] Alexey Kurakin, Ian Goodfellow, and Samy Bengio. Adversarial examples in the
physical world. In Proceedings of International Conference on Learning Represen-
tations, 2017.

[32] Madry Lab. MNIST and cifar10 adversarial examples challenges. https://github.

com/MadryLab, September 2020.

[33] Richard J. Larsen and Morris L. Marx. An Introduction to Mathematical Statistics

and Its Applications. Prentice Hall, 2011.

[34] Yann LeCun, Corinna Cortes, and Christopher JC Burges. The mnist database of

handwritten digits, 1998.

[35] Kimin Lee, Kibok Lee, Honglak Lee, and Jinwoo Shin. A simple unified framework
for detecting out-of-distribution samples and adversarial attacks. In Advances in
Neural Information Processing Systems, pages 7167â€“7177, 2018.

[36] Jiajun Lu, Theerasit Issaranon, and David A. Forsyth. Safetynet: Detecting and
rejecting adversarial examples robustly. In Proceedings of the IEEE International
Conference on Computer Vision, pages 446â€“454, 2017.

[37] Lei Ma, Felix Juefei-Xu, Minhui Xue, Bo Li, Li Li, Yang Liu, and Jianjun Zhao.
Deepct: Tomographic combinatorial testing for deep learning systems. In Xinyu
Wang, David Lo, and Emad Shihab, editors, 26th IEEE International Conference on
Software Analysis, Evolution and Reengineering, SANER 2019, Hangzhou, China,
February 24-27, 2019, pages 614â€“618. IEEE, 2019.

[38] Lei Ma, Felix Juefei-Xu, Fuyuan Zhang, Jiyuan Sun, Minhui Xue, Bo Li, Chunyang
Chen, Ting Su, Li Li, Yang Liu, Jianjun Zhao, and Yadong Wang. Deepgauge:
multi-granularity testing criteria for deep learning systems. In Proceedings of
the 33rd ACM/IEEE International Conference on Automated Software Engineering,
pages 120â€“131, 2018.

[39] Lei Ma, Fuyuan Zhang, Jiyuan Sun, Minhui Xue, Bo Li, Felix Juefei-Xu, Chao Xie,
Li Li, Yang Liu, Jianjun Zhao, and Yadong Wang. Deepmutation: Mutation testing
In 29th IEEE International Symposium on Software
of deep learning systems.

Conferenceâ€™17, July 2017, Washington, DC, USA

Zhe Zhao, Guangke Chen, Jingyi Wang, Yiwei Yang, Fu Song, and Jun Sun

Reliability Engineering, pages 100â€“111, 2018.

[40] Shiqing Ma, Yingqi Liu, Guanhong Tao, Wen-Chuan Lee, and Xiangyu Zhang.
NIC: detecting adversarial samples with neural network invariant checking. In
26th Annual Network and Distributed System Security Symposium, NDSS 2019, San
Diego, California, USA, February 24-27, 2019, 2019.

[41] Xingjun Ma, Bo Li, Yisen Wang, Sarah M. Erfani, Sudanthi N. R. Wijewickrema,
Grant Schoenebeck, Dawn Song, Michael E. Houle, and James Bailey. Character-
izing adversarial subspaces using local intrinsic dimensionality. In Proceedings of
the 6th International Conference on Learning Representations, 2018.

[42] Aleksander Madry, Aleksandar Makelov, Ludwig Schmidt, Dimitris Tsipras, and
Adrian Vladu. Towards deep learning models resistant to adversarial attacks. In
Proceedings of International Conference on Learning Representations, 2018.
[43] Dongyu Meng and Hao Chen. Magnet: A two-pronged defense against adversarial
examples. In Proceedings of the 2017 ACM SIGSAC Conference on Computer and
Communications Security, pages 135â€“147, 2017.

[44] Seyed-Mohsen Moosavi-Dezfooli, Alhussein Fawzi, Omar Fawzi, and Pascal
Frossard. Universal adversarial perturbations. In Proceedings of 2017 IEEE Con-
ference on Computer Vision and Pattern Recognition, pages 86â€“94, 2017.

[45] Seyed-Mohsen Moosavi-Dezfooli, Alhussein Fawzi, and Pascal Frossard. Deep-
fool: A simple and accurate method to fool deep neural networks. In Proceedings
of 2016 IEEE Conference on Computer Vision and Pattern Recognition, pages 2574â€“
2582, 2016.

[46] Nina Narodytska and Shiva Prasad Kasiviswanathan. Simple black-box adversar-
ial attacks on deep neural networks. In 2017 IEEE Conference on Computer Vision
and Pattern Recognition Workshops, pages 1310â€“1318, 2017.

[47] Anh Mai Nguyen, Jason Yosinski, and Jeff Clune. Deep neural networks are easily
fooled: High confidence predictions for unrecognizable images. In Proceedings of
2015 IEEE Conference on Computer Vision and Pattern Recognition, pages 427â€“436,
2015.

[48] Nicolas Papernot, Patrick D. McDaniel, and Ian J. Goodfellow. Transferability
in machine learning: from phenomena to black-box attacks using adversarial
samples. CoRR, abs/1605.07277, 2016.

[49] Nicolas Papernot, Patrick D. McDaniel, Ian J. Goodfellow, Somesh Jha, Z. Berkay
Celik, and Ananthram Swami. Practical black-box attacks against machine
learning. In Proceedings of the 2017 ACM on Asia Conference on Computer and
Communications Security, pages 506â€“519, 2017.

[50] Nicolas Papernot, Patrick D. McDaniel, Somesh Jha, Matt Fredrikson, Z. Berkay
Celik, and Ananthram Swami. The limitations of deep learning in adversarial
settings. In Proceedings of IEEE European Symposium on Security and Privacy
(EuroS&P), pages 372â€“387, 2016.

[51] Nicolas Papernot, Patrick D. McDaniel, Xi Wu, Somesh Jha, and Ananthram
Swami. Distillation as a defense to adversarial perturbations against deep neural
networks. In IEEE Symposium on Security and Privacy, pages 582â€“597, 2016.
[52] Kexin Pei, Yinzhi Cao, Junfeng Yang, and Suman Jana. Deepxplore: Automated
whitebox testing of deep learning systems. In Proceedings of the 26th Symposium
on Operating Systems Principles, pages 1â€“18, 2017.

[53] Luca Pulina and Armando Tacchella. An abstraction-refinement approach to
verification of artificial neural networks. In Proceedings of the 22nd International
Conference on Computer Aided Verification (CAV), pages 243â€“257, 2010.

[54] Jonas Rauber, Wieland Brendel, and Matthias Bethge. Foolbox: A python tool-
box to benchmark the robustness of machine learning models. arXiv preprint
arXiv:1707.04131, 2017.

[55] Kevin Roth, Yannic Kilcher, and Thomas Hofmann. The odds are odd: A statistical
test for detecting adversarial examples. In Proceedings of the 36th International
Conference on Machine Learning, pages 5498â€“5507, 2019.

[56] Wenjie Ruan, Xiaowei Huang, and Marta Kwiatkowska. Reachability analysis of
deep neural networks with provable guarantees. In Proceedings of the Twenty-
Seventh International Joint Conference on Artificial Intelligence, pages 2651â€“2659,
2018.

[57] Ali Shafahi, Mahyar Najibi, Amin Ghiasi, Zheng Xu, John P. Dickerson, Christoph
Studer, Larry S. Davis, Gavin Taylor, and Tom Goldstein. Adversarial training
for free! In Proceedings of the 32nd Annual Conference on Neural Information
Processing Systems, pages 3353â€“3364, 2019.

[58] Shawn Shan, Emily Wenger, Bolun Wang, Bo Li, Haitao Zheng, and Ben Y Zhao.
Gotta catchâ€™em all: Using honeypots to catch adversarial attacks on neural net-
works.
In Proceedings of the 2020 ACM SIGSAC Conference on Computer and
Communications Security, pages 67â€“83, 2020.

[59] Dinggang Shen, Guorong Wu, , and Heung-Il Suk. Deep learning in medical
image analysis. Annual Review of Biomedical Engineering, 19:221â€“248, 2017.
[60] Weijun Shen, Jun Wan, and Zhenyu Chen. Munn: Mutation analysis of neural
networks. In 2018 IEEE International Conference on Software Quality, Reliability
and Security Companion (QRS-C), pages 108â€“115. IEEE, 2018.

[61] Gagandeep Singh, Timon Gehr, Matthew Mirman, Markus PÃ¼schel, and Martin T.
Vechev. Fast and effective robustness certification. In Proceedings of the Annual
Conference on Neural Information Processing Systems (NeurIPS), pages 10825â€“
10836, 2018.

[62] Gagandeep Singh, Timon Gehr, Markus PÃ¼schel, and Martin T. Vechev. An
abstract domain for certifying neural networks. Proc. ACM Program. Lang.,

3(POPL):41:1â€“41:30, 2019.

[63] Wei Song, Heng Yin, Chang Liu, and Dawn Song. Deepmem: Learning graph
neural network models for fast and robust memory forensic analysis. In Pro-
ceedings of the 2018 ACM SIGSAC Conference on Computer and Communications
Security, pages 606â€“618, 2018.

[64] Yang Song, Taesup Kim, Sebastian Nowozin, Stefano Ermon, and Nate Kushman.
Pixeldefend: Leveraging generative models to understand and defend against
adversarial examples. arXiv preprint arXiv:1710.10766, 2017.

[65] Youcheng Sun, Xiaowei Huang, and Daniel Kroening. Testing deep neural net-

works. CoRR, abs/1803.04792, 2018.

[66] Youcheng Sun, Min Wu, Wenjie Ruan, Xiaowei Huang, Marta Kwiatkowska, and
Daniel Kroening. Concolic testing for deep neural networks. In Proceedings of
the 33rd ACM/IEEE International Conference on Automated Software Engineerin,
pages 109â€“119, 2018.

[67] Christian Szegedy, Wojciech Zaremba, Ilya Sutskever, Joan Bruna, Dumitru Erhan,
Ian Goodfellow, and Rob Fergus. Intriguing properties of neural networks. In
Proceedings of the 2nd International Conference on Learning Representations (ICLR),
2014.

[68] Yongqiang Tian, Shiqing Ma, Ming Wen, Yepang Liu, Shing-Chi Cheung, and
Xiangyu Zhang. Testing deep learning models for image analysis using object-
relevant metamorphic relations. arXiv preprint arXiv:1909.03824, 2019.

[69] Yuchi Tian, Kexin Pei, Suman Jana, and Baishakhi Ray. Deeptest: automated
testing of deep-neural-network-driven autonomous cars. In Proceedings of the
40th International Conference on Software Engineering, pages 303â€“314, 2018.
[70] Vincent Tjeng, Kai Xiao, and Russ Tedrake. Evaluating robustness of neural

networks with mixed integer programming. 2019.

[71] Florian TramÃ¨r, Nicholas Carlini, Wieland Brendel, and Aleksander Madry. On

adaptive attacks to adversarial example defenses. CoRR, abs/2002.08347, 2020.

[72] Chun-Chen Tu, Pai-Shun Ting, Pin-Yu Chen, Sijia Liu, Huan Zhang, Jinfeng Yi,
Cho-Jui Hsieh, and Shin-Ming Cheng. Autozoom: Autoencoder-based zeroth or-
der optimization method for attacking black-box neural networks. In Proceedings
of the 33rd AAAI Conference on Artificial Intelligence, pages 742â€“749, 2019.
[73] Huiyan Wang, Jingwei Xu, Chang Xu, Xiaoxing Ma, and Jian Lu. Dissector: Input
validation for deep learning applications by crossing-layer dissection. In The
42th International Conference on Software Engineering, 2020.

[74] Jingyi Wang, Guoliang Dong, Jun Sun, Xinyu Wang, and Peixin Zhang. Adver-
sarial sample detection for deep neural network through model mutation testing.
In Proceedings of the 41st International Conference on Software Engineering, pages
1245â€“1256. IEEE Press, 2019.

[75] Shiqi Wang, Kexin Pei, Justin Whitehouse, Junfeng Yang, and Suman Jana. Ef-
ficient formal safety analysis of neural networks.
In Proceedings of Annual
Conference on Neural Information Processing Systems (NeurIPS), pages 6367â€“6377,
2018.

[76] Shiqi Wang, Kexin Pei, Justin Whitehouse, Junfeng Yang, and Suman Jana. Formal
security analysis of neural networks using symbolic intervals. In Proceedings of
the 27th USENIX Security Symposium on Security, pages 1599â€“1614, 2018.
[77] Zan Wang, Ming Yan, Junjie Chen, Shuang Liu, and Dongdi Zhang. Deep learning
library testing via effective model generation. In Proceedings of the 28th ACM
Joint Meeting on European Software Engineering Conference and Symposium on
the Foundations of Software Engineering, pages 788â€“799, 2020.

[78] Tsui-Wei Weng, Huan Zhang, Hongge Chen, Zhao Song, Cho-Jui Hsieh, Luca
Daniel, Duane S. Boning, and Inderjit S. Dhillon. Towards fast computation of
certified robustness for relu networks. In Proceedings of the 35th International
Conference on Machine Learning, pages 5273â€“5282, 2018.

[79] Tsui-Wei Weng, Huan Zhang, Pin-Yu Chen, Jinfeng Yi, Dong Su, Yupeng Gao,
Cho-Jui Hsieh, and Luca Daniel. Evaluating the robustness of neural networks: An
extreme value theory approach. In Proceedings of the 6th International Conference
on Learning Representations (ICLR), 2018.

[80] Matthew Wicker, Xiaowei Huang, and Marta Kwiatkowska. Feature-guided
black-box safety testing of deep neural networks.
In Proceedings of the 24th
International Conference on Tools and Algorithms for the Construction and Analysis
of Systems, pages 408â€“426, 2018.

[81] Cihang Xie, Jianyu Wang, Zhishuai Zhang, Zhou Ren, and Alan Yuille. Mitigating
adversarial effects through randomization. arXiv preprint arXiv:1711.01991, 2017.
[82] Weilin Xu, David Evans, and Yanjun Qi. Feature squeezing: Detecting adversarial
examples in deep neural networks. In Proceedings of the 25th Annual Network
and Distributed System Security Symposium, 2018.

[83] Shenao Yan, Guanhong Tao, Xuwei Liu, Juan Zhai, Shiqing Ma, Lei Xu, and
Xiangyu Zhang. Correlations between deep neural network model coverage
criteria and model quality.
In Proceedings of the 28th ACM Joint Meeting on
European Software Engineering Conference and Symposium on the Foundations of
Software Engineering, pages 775â€“787, 2020.

[84] Mengshi Zhang, Yuqun Zhang, Lingming Zhang, Cong Liu, and Sarfraz Khurshid.
Deeproad: Gan-based metamorphic testing and input validation framework for
autonomous driving systems. In 2018 33rd IEEE/ACM International Conference on
Automated Software Engineering (ASE), pages 132â€“142. IEEE, 2018.

[85] Yuhao Zhang, Luyao Ren, Liqian Chen, Yingfei Xiong, Shing-Chi Cheung, and
Tao Xie. Detecting numerical bugs in neural network architectures. In Proceedings

Attack as Defense: Characterizing Adversarial Examples using Robustness

Conferenceâ€™17, July 2017, Washington, DC, USA

Table 5: Parameters of attacks for Env1
Dataset

MNIST

CIFAR10

Attack Method
FGSM
BIM
JSMA
C&W
FGSM
BIM
JSMA
C&W

Parameters
ğœ– = 0.3
ğœ– = 0.3, ğ›¼ = 0.01
ğœƒ = 1, ğ›¾ = 0.1
ğœ… = 0, ğ‘ = 0.02
ğœ– = 0.05
ğœ– = 0.05, ğ›¼ = 0.005
ğœƒ = 1, ğ›¾ = 0.1
ğœ… = 0, ğ‘ = 0.02

Table 6: Parameters of attacks for Env2

Dataset

MNIST

CIFAR10

ImageNet

Attack Method
FGSM
JSMA
DeepFool
C&W
BB
FGSM
JSMA
DeepFool
C&W
FGSM
JSMA
DeepFool
C&W

Parameters
ğœ– = 0.35
ğœƒ = 1, ğ›¾ = 0.12
overshoot=0.02
ğœ… = 0, ğ‘ = 0.6
Sub model+FGSM, ğœ– = 0.35
ğœ– = 0.05
ğœƒ = 1, ğ›¾ = 0.12
overshoot=0.02
ğœ… = 0, ğ‘ = 0.6
ğœ– = 0.016, ğ¿2 norm

Foolbox default parameters

of the 28th ACM Joint Meeting on European Software Engineering Conference and
Symposium on the Foundations of Software Engineering, pages 826â€“837, 2020.
[86] Husheng Zhou, Wei Li, Zelun Kong, Junfeng Guo, Yuqun Zhang, Bei Yu, Ling-
ming Zhang, and Cong Liu. Deepbillboard: Systematic physical-world testing of
autonomous driving systems. In 2020 IEEE/ACM 42nd International Conference
on Software Engineering (ICSE), pages 347â€“358. IEEE, 2020.

A ENVIRONMENTS
For reproductivity of this work, all the information of the target
models and attack parameters used in our experiments are given
below.

For Env1, we directly use models and attack methods provided
by BL1 [19], the DL model for MNIST is LeNet, the DL model for
CIFAR10 is a deep 12-layer convnet. The accuracy of the target
model on training/testing dataset is 99.6%/99.1% for MNIST and
87.3%/80.3% for CIFAR10. BL2 [41] uses the models and attack code
segments provided by BL1, so Env1 is the environment used by
these two baselines. It should be noted that there are two slightly
different BIM implementations in Env1, and we use the â€˜bim-aâ€™.

For Env2, we integrated the models and datasets used by BL3 [74]
and BL4 [73]. These two papers were published in ICSEâ€™19 and
ICSEâ€™20 respectively, and both use PyTorch as the platform. BL3 pro-
vides models of MNIST (LeNet) and CIFAR10 (GooglLeNet), the ac-
curacy of the target model on training/testing dataset is 98.5%/98.3%
for MNIST and 99.7%/90.5% for CIFAR10, and BL4 provides a model
of ImageNet (ResNet101), which has a top-1 accuracy rate 77.36%
on the validation set. In addition, BL4 only provides one attack that
is ğ¿2 norm adoption of FGSM with 0.016 as the attack step. In order
to make the results more comprehensive, we used the three attacks
JSMA, DeepFool and C&W, implemented in Foolbox, to generate
adversarial examples with default parameters.

(a) Time vs. â™¯iter on MNIST

(b) Time vs. â™¯iter on CIFAR10

Figure 10: Attack time vs. iterations (â™¯iter) using JSMA

Table 7: Comparison for the impact of classifier parameters

Parameter

K-value

Ratio between
benign and
adversarial
examples

Z-Score

Statistic
Ensemble

Classifier

Value
5
10
25
50
100
150
200
1:0.5
1:0.8
1:1
1:1.2
1:2
-1
-1.281552
-1.644854
-1.959964
-2
1
2
3
4
K-NN
SVM
DTC
RFC

Accbenign Accadv
0.9373
0.9465
0.9545
0.962
0.9685
0.9715
0.9753
0.9643
0.9658
0.9685
0.9705
0.9768
0.9918
0.987
0.9763
0.9635
0.962
0.8835
0.987
0.994
0.9963
0.9685
0.9643
0.9078
0.9068

0.988
0.986
0.986
0.985
0.979
0.977
0.976
0.981
0.979
0.979
0.977
0.971
0.875
0.926
0.963
0.979
0.98
0.972
0.926
0.883
0.775
0.979
0.975
0.994
0.994

B ATTACK TIME VS. NUMBER OF

ITERATIONS

For an iterative attack, the number of iterations is positively corre-
lated with attack time. This is justified by the scatter plots shown
in Figure 10(a) and Figure 10(b), which compare the attack time and
number of iterations by JSMA on the 100 MNIST and 100 CIFAR10
examples.

C RESULTS OF PARAMETER TUNING
We report results of turning parameters on the target model from
Env1 using the MNIST images.

We vary the value of ğ¾ from 5 to 200 for the K-NN based ensemble
detector. The results are shown in Table 7, where the bold one
denotes the value used in the previous experiments. Column Accadv
denotes the accuracy on adversarial examples. Column Accbenign
denotes the accuracy on benign examples. We observe that both
true and false positive rates slightly increase with the increase of ğ¾.

01020304050607080#Iter0.00.10.20.30.40.5Time020406080100#Iter0.00.10.20.30.40.50.60.70.8TimeConferenceâ€™17, July 2017, Washington, DC, USA

Zhe Zhao, Guangke Chen, Jingyi Wang, Yiwei Yang, Fu Song, and Jun Sun

We also vary the ratio between benign and adversarial examples.
As shown in Table 7, with the decrease of the ratio, both true and
false positive rates slightly increase.

For the Z-score based ensemble detector, we vary the value of the
threshold â„ from -1, -1.281552, -1.644854 and -2, which respectively
correspond to about 15%, 10%, 5% and 2.3% false positive rates of
1,000 benign samples. From Table 7, we observe that the smaller
the threshold â„, the lower the false positive rate, but the higher
the true positive rate. We also vary the parameter ğ‘˜ in the Z-score
based ensemble detector. Recall that the ensemble detector classi-
fies an input to benign if ğ‘˜ detectors classify the input to benign,
otherwise adversarial. We observe from Table 7 that both true and
false positive rates increase with the increase of ğ‘˜.

In summary, the above parameters can be used to balance the
true and false positive rates, namely, the true positive rate could be
improved at the cost of false positive rate.

Finally, instead of K-NN, we also tried the support vector machine
(SVM), decision tree (DTC) and random forest (RFC) classification

algorithms. We use the implementations of scikit-learn with the
default parameters1. The results in Table 7 show that similar ac-
curacy can be obtained using different classification algorithms.
This implies that our detection approach is generic in terms of
classification algorithms.

D MORE EXAMPLES
Figure 11 shows adversarial examples under the attack in Sec-
tion 6.2.1 with ğœ… = 20, where each of them except for the benign
one targets one of the other labels. All these adversarial examples
are successfully detected by A2D + AE.

Figures 12 and 13 show adversarial examples on the adversarially
training model with ğœ… = 0 and ğœ… = 10 in Section 6.2.2, where each
of them except for the benign ones targets one of the other labels.
All the adversarial examples in Figure 12 (i.e., ğœ… = 0) can be detected
by our defense A2D. Only 3 adversarial examples in Figure 13 (i.e.,
ğœ… = 10) cannot be detected by A2D + AT.
1https://scikit-learn.org

Attack as Defense: Characterizing Adversarial Examples using Robustness

Conferenceâ€™17, July 2017, Washington, DC, USA

Figure 11: Targeted adversarial examples for each label pair of images on the MNIST model, ğœ… = 20

BenignL2=4.65L2=2.63L2=3.48L2=3.23L2=2.68L2=2.46L2=2.26L2=3.87L2=3.27L2=3.4BenignL2=2.52L2=1.95L2=1.53L2=1.86L2=2.46L2=2.1L2=1.95L2=2.15L2=3.55L2=2.58BenignL2=2.47L2=3.79L2=3.17L2=3.2L2=4.78L2=3.11L2=4.22L2=3.23L2=4.27L2=2.48BenignL2=3.31L2=1.8L2=2.88L2=3.42L2=1.8L2=4.05L2=3.88L2=4.18L2=2.94L2=2.97BenignL2=2.43L2=2.99L2=1.97L2=2.62L2=1.7L2=4.22L2=4.71L2=2.94L2=4.14L2=2.71BenignL2=2.47L2=3.74L2=2.48L2=2.82L2=2.85L2=3.75L2=2.72L2=2.81L2=3.4L2=2.12BenignL2=3.38L2=2.52L2=3.64L2=3.46L2=4.33L2=3.46L2=2.39L2=3.58L2=2.84L2=4.82BenignL2=3.44L2=2.64L2=3.68L2=3.77L2=1.92L2=2.26L2=3.77L2=2.47L2=3.77L2=3.5BenignL2=3.17L2=3.43L2=3.97L2=2.08L2=1.54L2=1.82L2=2.09L2=3.2L2=2.89L2=1.72BenignConferenceâ€™17, July 2017, Washington, DC, USA

Zhe Zhao, Guangke Chen, Jingyi Wang, Yiwei Yang, Fu Song, and Jun Sun

Figure 12: Targeted adversarial examples for each label pair of images on the adversarially trained model, ğœ… = 0

BenignL2=1.08L2=0.77L2=1.39L2=1.06L2=1.48L2=1.44L2=1.54L2=1.23L2=1.48L2=1.45BenignL2=1.34L2=0.89L2=1.74L2=0.72L2=0.89L2=1.69L2=1.14L2=0.68L2=1.08L2=1.86BenignL2=0.71L2=0.65L2=1.19L2=1.12L2=0.81L2=1.61L2=1.59L2=0.97L2=0.71L2=0.54BenignL2=0.04L2=0.29L2=0.53L2=0.58L2=1.13L2=0.81L2=0.53L2=0.4L2=0.38L2=0.57BenignL2=0.7L2=0.31L2=0.35L2=0.24L2=0.84L2=1.06L2=0.73L2=0.86L2=0.3L2=0.25BenignL2=0.64L2=0.78L2=0.99L2=1.04L2=1.39L2=0.91L2=0.84L2=0.62L2=0.5L2=0.83BenignL2=1.18L2=1.19L2=1.25L2=2.98L2=2.75L2=2.89L2=2.64L2=2.34L2=2.52L2=2.5BenignL2=2.97L2=2.39L2=0.92L2=1.0L2=1.64L2=1.68L2=1.84L2=1.34L2=1.9L2=1.84BenignL2=0.44L2=1.9L2=1.46L2=1.95L2=1.97L2=2.15L2=1.7L2=1.77L2=1.86L2=1.46BenignAttack as Defense: Characterizing Adversarial Examples using Robustness

Conferenceâ€™17, July 2017, Washington, DC, USA

Figure 13: Targeted adversarial examples for each label pair of images on the adversarially trained model, ğœ… = 10

BenignL2=1.83L2=1.87L2=2.13L2=1.89L2=2.4L2=2.39L2=2.38L2=2.22L2=2.26L2=2.72BenignL2=2.34L2=2.27L2=3.24L2=2.16L2=2.29L2=3.22L2=2.52L2=1.9L2=2.44L2=2.89BenignL2=1.98L2=1.94L2=2.94L2=2.05L2=2.28L2=2.96L2=2.91L2=1.81L2=1.66L2=1.36BenignL2=0.96L2=1.11L2=1.42L2=1.65L2=2.19L2=1.79L2=1.33L2=1.0L2=1.11L2=1.62BenignL2=1.29L2=0.96L2=1.08L2=0.97L2=1.5L2=2.12L2=1.79L2=1.55L2=1.37L2=1.27BenignL2=1.48L2=1.32L2=1.93L2=2.11L2=2.27L2=2.11L2=1.67L2=1.33L2=1.19L2=1.84BenignL2=2.55L2=2.31L2=2.49L2=3.84L2=3.46L2=3.98L2=3.26L2=4.37L2=3.29L2=3.3BenignL2=3.84L2=3.09L2=1.88L2=2.13L2=3.11L2=3.15L2=3.4L2=2.82L2=3.66L2=2.95BenignL2=1.66L2=2.62L2=2.16L2=3.11L2=3.14L2=2.99L2=2.84L2=2.85L2=2.77L2=2.32Benign