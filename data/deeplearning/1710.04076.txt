7
1
0
2

t
c
O
0
1

]

O
R
.
s
c
[

1
v
6
7
0
4
0
.
0
1
7
1
:
v
i
X
r
a

Deep Semantic Abstractions of
Everyday Human Activities
On Commonsense Representations of Human Interactions

Jakob Suchan1 and Mehul Bhatt1,2

1 Spatial Reasoning. www.spatial-reasoning.com
EASE CRC: Everyday Activity Science and Engineering., http://ease-crc.org
University of Bremen, Germany

2 Machine Perception and Interaction Lab., https://mpi.aass.oru.se
Centre for Applied Autonomous Sensor Systems (AASS)
¨Orebro University, Sweden

Abstract. We propose a deep semantic characterisation of space and mo-
tion categorically from the viewpoint of grounding embodied human-object
interactions. Our key focus is on an ontological model that would be adept
to formalisation from the viewpoint of commonsense knowledge representa-
tion, relational learning, and qualitative reasoning about space and motion
in cognitive robotics settings. We demonstrate key aspects of the space &
motion ontology and its formalisation as a representational framework in
the backdrop of select examples from a dataset of everyday activities. Fur-
thermore, focussing on human-object interaction data obtained from RGBD
sensors, we also illustrate how declarative (spatio-temporal) reasoning in the
(constraint) logic programming family may be performed with the developed
deep semantic abstractions.

1

Introduction

Cognitive robotics technologies and machine perception & interaction systems
involving an interplay of space, dynamics, and (embodied) cognition necessitate
capabilities for explainable reasoning, learning, and control about space, events,
actions, change, and interaction (Bhatt, 2012). A crucial requirement in this con-
text pertains to the semantic interpretation of multi-modal human behavioural
data (Bhatt, 2013; Bhatt and Kersting, 2017), with objectives ranging from
knowledge acquisition and data analyses to hypothesis formation, structured
relational learning, learning by demonstration etc. Towards this, the overall fo-
cus & scope of our research is on the processing and semantic interpretation of
dynamic visuo-spatial imagery with a particular emphasis on the ability to ab-
stract, reason, and learn commonsense knowledge that is semantically founded
in qualitative spatial, temporal, and spatio-temporal relations and patterns.
We propose that an ontological characterisation of human-activities — e.g., en-
compassing (embodied) spatio-temporal relations and motion patterns— serves

 
 
 
 
 
 
2

Suchan and Bhatt

as a bridge between high-level conceptual categories (e.g., pertaining to human-
object interactions) on the one-hand, and low-level / quantitative sensory-motor
data on the other.

Deep Semantics – The Case of Dynamic Visuo-Spatial Imagery

The high-level semantic interpretation and qualitative analysis of dynamic visuo-
spatial imagery requires the representational and inferential mediation of com-
monsense abstractions of space, time, action, change, interaction and their mu-
tual interplay thereof. In this backdrop, deep visuo-spatial semantics denotes
the existence of declaratively grounded models —e.g., pertaining to space, time,
space-time, motion, actions & events, spatio-linguistic conceptual knowledge—
and systematic formalisation supporting capabilities such as:
(a). mixed quan-
titative qualitative spatial inference and question answering (e.g., about con-
(b). non-
sistency, qualiﬁcation and quantiﬁcation of relational knowledge);
monotonic spatial reasoning (e.g., for abductive explanation);
(c). relational
learning of spatio-temporally grounded concepts;
(d). integrated inductive-
abductive spatio-temporal inference;
(e). probabilistic spatio-temporal infer-
(f ). embodied grounding and simulation from the viewpoint of cognitive
ence;
linguistics (e.g., for knowledge acquisition and inference based on natural lan-
guage).
Recent perspectives on deep (visuo-spatial) semantics encompass methods for
declarative (spatial) representation and reasoning —e.g., about space and mo-
tion— within frameworks such as constraint logic programming (rule-based
spatio-temporal inference (Bhatt et al., 2011b; Suchan et al., 2014)), answer-
set programming (for non-monotonic spatial reasoning (Walega et al., 2015;
Bhatt and Loke, 2008)), description logics (for spatio-terminological reason-
ing (Bhatt et al., 2009)), inductive logic programming (for inductive-abductive
spatio-temporal learning (Dubba et al., 2011, 2015)) and other specialised forms
of commonsense reasoning based on expressive action description languages for
modelling space, events, action, and change (Bhatt, 2012; Bhatt and Loke, 2008).
In general, deep visuo-spatial semantics driven by declarative spatial represen-
tation and reasoning pertaining to dynamic visuo-spatial imagery is relevant
and applicable in a variety of cognitive interaction systems and assistive tech-
nologies at the interface of (spatial) language, (spatial) logic, and (visuo-spatial)
cognition.

Deep Semantics, and Reasoning about Human-Robot Interactions

The starting point of our work is from formal commonsense representation and
reasoning techniques developed in the ﬁeld of Artiﬁcial Intelligence. Here, the
core focus of the overall research goal is on the question:

How can everyday activity tasks be formally represented in terms of spatio-
temporal descriptions (that are augmented by knowledge about objects and
environments) such that it enables robotic agents to execute everyday ma-
nipulation tasks appropriately?.

Deep Semantic Abstractions of Everyday Human Activities

3

Fig. 1: A Sample Activity – “Passing a Cup” (RGB and Corresponding Depth Data)

We particularly focus on an ontological and formal characterisation of space and
motion from a human-centered, commonsense formal modeling and computa-
tional viewpoint, i.e., space, as it is interpreted within the AI subdiscipline of
knowledge representation and reasoning, commonsense reasoning, spatial cogni-
tion & computation, and more broadly, within spatial information theory (Aiello
et al., 2007; Bhatt et al., 2011a; Bhatt, 2012; Bhatt et al., 2013; Cohn and Renz,
2007; Renz and Nebel, 2007). Whereas the main focus of this paper is on the on-
tological and representational aspects, we emphasise that this is strongly driven
by computational considerations focussing on:
(a). developing general methods
and tools for commonsense reasoning about space and motion categorically from
the viewpoint of commonsense cognitive robotics in general, but human-object
interactions occurring in the context of everyday activities in particular;
(b).
founded on the established ontological model, developing models, algorithms
and tools for reasoning about space and motion, and making them available
as extensions knowledge representation (KR) based declarative spatio-temporal
reasoning systems, e.g., constraint logic programming based CLP(QS) (Bhatt
et al., 2011b), or answer-set programming based ASPMT(QS) (Walega et al.,
2015).

2 Commonsense Reasoning about Space and Change:

Background and Related Work

Commonsense spatio-temporal relations and patterns (e.g. left, touching, part of,
during, collision) oﬀer a human-centered and cognitively adequate formalism for
logic-based automated reasoning about embodied spatio-temporal interactions
involved in everyday activities such as ﬂipping a pancake, grasping a cup, or
opening a tea box (Bhatt et al., 2013; Worgotter et al., 2012; Spranger et al.,
2014, 2016).
Qualitative, multi-modal, and multi-domain3 representations of spatial, tempo-
ral, and spatio-temporal relations and patterns, and their mutual transitions can

3 Multi-modal in this context refers to more than one aspect of space, e.g., topol-
ogy, orientation, direction, distance, shape. Multi-domain denotes a mixed domain

Entities (E)

arbitrary
circles,
cuboids, spheres

rectangles,
polygons,

axis-aligned
gles and cuboids

rectan-

2D point, circle, poly-
gon with 2D line
oriented
2D/3D vectors

points,

circles,
cuboids,

circles,
cuboids,

4

Suchan and Bhatt

Spatial Domain (QS) Formalisms

Spatial Relations (R)

Mereotopology

RCC-5, RCC-8 (Ran-

dell et al., 1992)

Rectangle & Block
algebra
(Guesgen,
1989)

disconnected (dc), external contact (ec), par-
tial overlap (po), tangential proper part (tpp),
non-tangential proper part (ntpp), proper part
(pp), part of (p), discrete (dr), overlap (o),
contact (c)
proceeds, meets, overlaps, starts, during, ﬁn-
ishes, equals

Orientation

LR
(Scivos
Nebel, 2004)

and

left, right, collinear, front, back, on

Distance, Size

Dynamics, Motion

OPRA
2006)

(Moratz,

QDC
et al., 1995)

(Hern´andez

Space-Time Histories
Hayes (1985); Haz-
arika (2005b)

facing towards, facing away, same direction,
opposite direction
adjacent, near, far, smaller, equi-sized, larger rectangles,

moving: towards, away, parallel; growing /
shrinking: vertically, horizontally; passing: in
front, behind;
rota-
tion: left, right, up, down, clockwise, couter-
clockwise

splitting / merging;

polygons,
spheres
rectangles,
polygons,
spheres

Table 1: The Spatio-Temporal Domain QS for Abstracting Everyday Human Activities

provide a mapping and mediating level between human-understandable natural
language instructions and formal narrative semantics on the one hand (Eppe
and Bhatt, 2013; Bhatt et al., 2013), and symbol grounding, quantitative trajec-
tories, and low-level primitives for robot motion control on the other (see Fig.
1). By spatio-linguistically grounding complex sensory-motor trajectory data
(e.g., from human-behaviour studies) to a formal framework of space and mo-
tion, generalized (activity-based) qualitative reasoning about dynamic scenes,
spatial relations, and motion trajectories denoting single and multi-object path
& motion predicates can be supported (Eschenbach and Schill, 1999). For in-
stance, such predicates can be abstracted within a region based 4D space-time
framework (Hazarika, 2005a; Bennett et al., 2000a,b), object interactions (Davis,
2008, 2011), and spatio-temporal narrative knowledge (Tyler and Evans, 2003;
Eppe and Bhatt, 2013; Davis, 2013). An adequate qualitative spatio-temporal
representation can therefore connect with low-level constraint-based movement
control systems of robots (Bartels et al., 2013), and also help grounding symbolic
descriptions of actions and objects to be manipulated (e.g., natural language in-
structions such as cooking recipes (Tellex, 2010)) in the robots perception.

3 Embodied Interactions in Space-Time: Towards

Commonsense Abstractions of Everyday Activities

3.1 Humans, Objects, and Interactions in Space-Time

Activities and interactions are characterised based on visuo-spatial domain-
objects O = {o1, o2, ..., oi} representing the visual elements in the scene, i.e,
people and objects.

ontology involving points, line-segments, polygons, and regions of space, time, and
space-time (Hazarika, 2005a).

Deep Semantic Abstractions of Everyday Human Activities

5

Fig. 2: Declarative Model of Human-Body Posture

The Qualitative Spatio-Temporal Ontology (QS) is characterised by the
basic spatial and temporal entities (E) that can be used as abstract represen-
tations of domain-objects and the relational spatio-temporal structure (R) that
characterises the qualitative spatio-temporal relationships amongst the entities
in (E). Towards this, domain-objects (O) are represented by their spatial and
temporal properties, and abstracted using the following basic spatial entities:

– points are triplets of reals x, y, z;
– oriented-points consisting of a point p and a vector v;
– line-segments consisting of two points p1, p2 denoting the start and the end

point of the line-segment;

– poly-line consisting of a list of vertices (points) p1, ..., pn, such that the line

is connecting the vertices is non-self-intersecting;

– polygon consisting of a list of vertices (points) p1, ..., pn, (spatially ordered

counter-clockwise) such that the boundary is non-self-intersecting;

and the temporal entities:

– time-points are a real t
– time-intervals are a pair of reals t1, t2, denoting the start and the end point

of the interval.

The dynamics of human activities are represented by 4-dimensional regions in
space-time (sth) representing people and object dynamics by a set of spatial
entities in time, i.e. ST H = (εt1 , εt2, εt3 , ..., εtn ), where εt1 to εtn denotes the
spatial primitive representing the object o at the time points t1 to tn.

DeepSemanticAbstractionsofEverydayHumanActivities5person(full_body,[upper_body,lower_body]).person(upper_body,[head,left_arm,...])....body_part(left_upper_arm,joint(shoulder_left),joint(elbow_left))....joint(spine_base,joint(id(0)).joint(spine_mid,joint(id(1))).joint(neck,id(2)).joint(head,id(3))....joint(thumb_right,id(24)).Fig.2:DeclarativeModelofHuman-BodyPosturetationsofdomain-objectsandtherelationalspatio-temporalstructure(R)thatcharacterisesthequalitativespatio-temporalrelationshipsamongsttheentitiesin(E).Towardsthis,domain-objects(O)arerepresentedbytheirspatialandtemporalproperties,andabstractedusingthefollowingbasicspatialentities:–pointsaretripletsofrealsx,y,z;–oriented-pointsconsistingofapointpandavectorv;–line-segmentsconsistingoftwopointsp1,p2denotingthestartandtheendpointoftheline-segment;–poly-lineconsistingofalistofvertices(points)p1,...,pn,suchthatthelineisconnectingtheverticesisnon-self-intersecting;–polygonconsistingofalistofvertices(points)p1,...,pn,(spatiallyorderedcounter-clockwise)suchthattheboundaryisnon-self-intersecting;andthetemporalentities:–time-pointsarearealt–time-intervalsareapairofrealst1,t2,denotingthestartandtheendpointoftheinterval.Thedynamicsofhumanactivitiesarerepresentedby4-dimensionalregionsinspace-time(sth)representingpeopleandobjectdynamicsbyasetofspatialentitiesintime,i.e.STH=("t1,"t2,"t3,...,"tn),where"t1to"tndenotesthespatialprimitiverepresentingtheobjectoatthetimepointst1totn.Spatio-TemporalCharacteristicsofHumanActivitiesDynamicsofhumanactivitiesareabstractedusing4-dimensionalregionsinspace-time,i.e.6

Suchan and Bhatt

discrete(o1, o2)

overlapping(o1, o2)

inside(o1, o2)

moving(o)

stationary(o)

growing(o)

shrinking(o)

parallel(o1, o2)

merging(o1, o2)

splitting(o1, o2)

curved(o)

cyclic(o)

moving into(o1, o2) moving out(o1, o2)

attached(o1, o2)

Fig. 3: Commonsense Spatial Reasoning with Spatio-Temporal Histories Represent-
ing Dynamics in Everyday Human Activities

Spatio-Temporal Characteristics of Human Activities Dynamics of
human activities are abstracted using 4-dimensional regions in space-time, i.e.
space-time histories (sth) representing people and object dynamics. Based on
the space-time histories of domain-objects, we deﬁne the following functions for
spatio-temporal properties of objects:

– position: O× T → R × R × R, gives the 3D position (x,y,z) of an object o at

a time-point t;

– size: O× T → R, gives the size of an object o at a time-point t;
– distance: O× O× T → R, gives the distance between two objects o1 and o2

at a time-point t;

– angle: O× O× T → R, gives the angle between two objects o1 and o2 at a

time-point t;

for static spatial properties, and

– movement velocity: O× T × T → R, gives the amount of movement of an

object o between two time-points t1 and t2;

– movement direction: O× T × T → R, gives the direction of movement of an

object o between two time-points t1 and t2;

– rotation: O× T × T → R, gives the rotation of an object o between two

time-points t1 and t2;

for dynamic spatio-temporal properties.
Spatio-temporal relationships (R) between the basic entities in E may be char-
acterised with respect to arbitrary spatial and spatio-temporal domains such as

TimeSpaceSpaceTimeSpaceSpaceTimeSpaceSpaceTimeSpaceSpaceTimeSpaceSpaceTimeSpaceSpaceTimeSpaceSpaceTimeSpaceSpaceTimeSpaceSpaceTimeSpaceSpaceTimeSpaceSpaceTimeSpaceSpaceTimeSpaceSpaceTimeSpaceSpaceTimeSpaceSpaceDeep Semantic Abstractions of Everyday Human Activities

7

Interaction (Θ)
pick up(P, O)
put down(P, O)
reach f or(P, O)
passing over(P1, P2, O) a person P1 is passing an object O to another person P2.

Description
a person P picks up an object O.
a person P puts down an object O.
a person P is reaching for an object O.

Table 2: Sample Interactions Involved in Everyday Human Activities

mereotopology, orientation, distance, size, motion, rotation (see Table 1 for a
list of considered spatio-temporal abstractions).

Declarative Model of Human Body Pose The human body is represented
using a declarative model of the human body (see Fig. 2), within this model we
ground the human body in 3d-data of skeleton joints and body-parts obtained
from RGB-D sensing. Body-parts may be abstracted using regions and line-
segments, and joints may be abstracted using points. As such, Body pose can be
declaratively abstracted by the spatio-temporal conﬁguration of the body-parts,
using the position of body-parts and the angle between skeleton joints.

Spatio-temporal ﬂuents are used to describe properties of the world, i.e. the
predicates holds-at(φ, t) and holds-in(φ, δ) denote that the ﬂuent φ holds at time
point t, resp. in time interval δ. Fluents are determined by the data from the
depth sensing device and represent qualitative relations between domain-objects,
i.e. spatio-temporal ﬂuents denote, that a relation r ∈ R holds between basic
spatial entities ε of a space-time history at a time-point t. Dynamics of the
domain are represented based on changes in spatio-temporal ﬂuents (see Fig. 3),
e.g., two objects approaching each other can be deﬁned as follows.

holds-in(approaching(oi, oj), δ) ⊃ during(ti, δ) ∧ during(tj, δ)∧
before(ti, tj) ∧ (distance(oi, oj, ti) > distance(oi, oj, tj)).

(1)

Interactions Interactions Θ = {θ1, θ2, ..., θi} describe processes that change
the spatio-temporal conﬁguration of objects in the scene, at a time point t or
in a time interval δ; these are deﬁned by the involved spatio-temporal dynamics
in terms of changes in the status of st-histories caused by the interaction, i.e.
the description consists of (dynamic) spatio-temporal relations of the involved
st-histories, before, during and after the interaction (See Table 2 for exemplary
interactions). We use occurs-at(θ, t), and occurs-in(θ, δ) to denote that an inter-
action θ occurred at a time point t or in an interval δ, e.g., a person reaching
for an object can be deﬁned as follows.

holds-in(reach for(oi, oj), δ) ⊃ person(oi)∧

holds-in(approaching(body part(hand, oi), oj), δi)∧
holds-in(touches(body part(hand, oi), oj), δj)∧
meets(δi, δj) ∧ starts(δi, δ) ∧ ends(δj, δ).

(2)

8

Suchan and Bhatt

Activities

Making sandwich,
Making tea,
Making salad,
Making cereals

Interactions Instances
cut
pour

Cucumbers, Onions, Tomatoes, Sandwich
Dressing on the plate, Tea in the cup, Juice in the glass, Water in
the glass, Coﬀee in the cup, Cereal in the bowl, Milk in the bowl
Cup of water / coﬀee / tea,
Cup from the cupboard, Slices of bread from the packet, Vegeta-
bles/Fruits from the basket, Basket from the kitchen plane, Tea
bag from the box
Sugar in the cup, Tea Bag in the cup

pass
pick

put

Table 3: Exemplary Activities from the Dataset of Human Activities

4 Application: Grounding of Everyday Activities

We demonstrate the above model for grounding everyday activities in percep-
tual data obtained from RGB-D sensing. 4 The model has been implemented
within (Prolog based) constraint logic programming based on formalisations of
qualitative space in CLP(QS) (Bhatt et al., 2011b). Using the presented model
it is possible to generate grounded sequences of interactions performed within
the course of an activity.

The presented activity is part of a larger dataset on everyday human activities
(see Table 3), including RGB and RGB-D data for from diﬀerent viewpoints of
human-human, and human-object interactions.

Sample Activity: “Pass Cup of Water” The activity of passing a cup
of water is characterised with respect to the interactions between the humans
and their environment, i.e. objects the human uses in the process of passing the
cup. Each of these interactions is deﬁned by its spatio-temporal characteristics,
in terms of changes in the spatial arrangement in the scene (as described in
Sec. 3). As an result we obtain a sequence of interactions performed within the
track of the particular instance of the activity, grounded in the spatio-temporal
dynamics of the scenario. As an example consider the sequence depicted in ﬁg.
1, the interactions in this sequence can be described as follows:

Person1 reaches for the cup, picks up the cup, and moves the hand together
with the cup towards Person2. Person2 grasps the cup and Person1 releases
the cup.

The data we obtain from the RGB-D sensor consists of 3D positions of skeleton
joints for both persons and the tabletop objects for each time-point.

4 RGB-D Data (video, depth, body skeleton): We collect data using Microsoft Kinect
v2 which provides RGB and depth data. The RGB stream has a resolution of
1920x1080 pixel at 30 Hz and the depth sensor has a resolution of 512x424 pix-
els at 30 Hz. Skeleton tracking can track up to 6 persons with 25 joints for each
person. Further we use the point-cloud data to detect objects on the table using
tabletop object segmentation.

Deep Semantic Abstractions of Everyday Human Activities

9

Grounded Interaction Sequence Based on the sensed body-pose data and
the detected objects, a sequence of interactions can be queried from the example
sequences using Prologs interactive query answering mode.

This results in all interactions identiﬁed in the example sequence and their re-
spective grounding with respect to the spatio-temporal dynamics constituting
the interaction,

This grounding of the activity may be used for interpretation and learning from
the observed activities and the involved spatio-temporal dynamics, e.g., in the
example above the person is passing the cup over the laptop, which is safe when
the cup is empty, but in the case that the cup is ﬁlled with water one would pass
it around the laptop.

5 Summary and Outlook

Deep semantics denotes the existence of declaratively grounded models —e.g.,
pertaining to space, time, space-time, motion, actions & events, spatio-linguistic
conceptual knowledge— and systematic formalisation supporting KR-based ca-
pabilities such as abstraction, learning, reasoning, embodied simulation. Rooted
in this concept of deep (visuo-spatial) semantics, this paper presents an ontologi-
cal and formal representational framework aimed at grounding embodied human-
object interactions in a commonsense cognitive robotics setting. The model is
illustrated with select RGBD datasets corresponding to representative activities
from a larger dataset of everyday activities; as preliminary application, we also

DeepSemanticAbstractionsofEverydayHumanActivities9at(joint(id(0),person(id(0),tracking_status(2),pos_3d(point(-0.280572,-0.0300787,2.15142)),time_point(2385869011))))).at(joint(id(0),person(id(1),tracking_status(2),pos_3d(point(0.605924,-0.162173,2.04098)),time_point(2385869011)))))....at(object(id(0)),type(cup),pos_3d(point(0.667643,-0.213097,1.83488)),time_point(2385869011))....GroundedInteractionSequenceBasedonthesensedbody-posedataandthedetectedobjects,asequenceofinteractionscanbequeriedfromtheexamplesequencesusingPrologsinteractivequeryansweringmode.?-grounded_interaction(occurs_in(Interaction,Interval),Grounding).Thisresultsinallinteractionsidentiﬁedintheexamplesequenceandtheirre-spectivegroundingwithrespecttothespatio-temporaldynamicsconstitutingtheinteraction,Interaction=pick_up(person(P),object(cup)),Interval=interval(t1,t3),Grounding=[occurs_at(grasp(body_part(right_hand,person(id(0))),object(cup)),timepoint(t1),holds_in(attached(body_part(right_hand,person(id(0))),object(cup)),interval(t2,t6)),holds_in(move_up(body_part(right_hand,person(id(0)))),interval(t2,t3))];Interaction=pass_over(person(P),person(Q),object(cup)),Interval=interval(t4,t7),Grounding=[holds_in(approaching(body_part(right_hand,person(id(0))),person(id(1))),interval(t4,t5)),holds_in(approaching(body_part(right_hand,person(id(1))),object(cup)),interval(t4,t5)),occurs_at(grasp(body_part(right_hand,person(id(1))),object(cup)),timepoint(t6),occurs_at(release(body_part(right_hand,person(id(0))),object(cup)),timepoint(t7),...];...Thisgroundingoftheactivitymaybeusedforinterpretationandlearningfromtheobservedactivitiesandtheinvolvedspatio-temporaldynamics,e.g.,intheexampleabovethepersonispassingthecupoverthelaptop,whichissafewhenthecupisempty,butinthecasethatthecupisﬁlledwithwateronewouldpassitaroundthelaptop.5SummaryandOutlookDeepsemanticsdenotestheexistenceofdeclarativelygroundedmodels—e.g.,pertainingtospace,time,space-time,motion,actions&events,spatio-linguisticconceptualknowledge—andsystematicformalisationsupportingKR-basedca-pabilitiessuchasabstraction,learning,reasoning,embodiedsimulation.Rootedinthisconceptofdeep(visuo-spatial)semantics,thispaperpresentsanontologi-calandformalrepresentationalframeworkaimedatgroundingembodiedhuman-objectinteractionsinacommonsensecognitiveroboticssetting.ThemodelisillustratedwithselectRGBDdatasetscorrespondingtorepresentativeactivitiesfromalargerdatasetofeverydayactivities;aspreliminaryapplication,wealsoshowhowtheformalmodelcanbedirectlyappliedforcommonsensereason-ingwithconstraintlogicprogramming,withaparticularfocusonspace-timehistoriesandmotionpatterns.DeepSemanticAbstractionsofEverydayHumanActivities9at(joint(id(0),person(id(0),tracking_status(2),pos_3d(point(-0.280572,-0.0300787,2.15142)),time_point(2385869011))))).at(joint(id(0),person(id(1),tracking_status(2),pos_3d(point(0.605924,-0.162173,2.04098)),time_point(2385869011)))))....at(object(id(0)),type(cup),pos_3d(point(0.667643,-0.213097,1.83488)),time_point(2385869011))....GroundedInteractionSequenceBasedonthesensedbody-posedataandthedetectedobjects,asequenceofinteractionscanbequeriedfromtheexamplesequencesusingPrologsinteractivequeryansweringmode.?-grounded_interaction(occurs_in(Interaction,Interval),Grounding).Thisresultsinallinteractionsidentiﬁedintheexamplesequenceandtheirre-spectivegroundingwithrespecttothespatio-temporaldynamicsconstitutingtheinteraction,Interaction=pick_up(person(P),object(cup)),Interval=interval(t1,t3),Grounding=[occurs_at(grasp(body_part(right_hand,person(id(0))),object(cup)),timepoint(t1),holds_in(attached(body_part(right_hand,person(id(0))),object(cup)),interval(t2,t6)),holds_in(move_up(body_part(right_hand,person(id(0)))),interval(t2,t3))];Interaction=pass_over(person(P),person(Q),object(cup)),Interval=interval(t4,t7),Grounding=[holds_in(approaching(body_part(right_hand,person(id(0))),person(id(1))),interval(t4,t5)),holds_in(approaching(body_part(right_hand,person(id(1))),object(cup)),interval(t4,t5)),occurs_at(grasp(body_part(right_hand,person(id(1))),object(cup)),timepoint(t6),occurs_at(release(body_part(right_hand,person(id(0))),object(cup)),timepoint(t7),...];...Thisgroundingoftheactivitymaybeusedforinterpretationandlearningfromtheobservedactivitiesandtheinvolvedspatio-temporaldynamics,e.g.,intheexampleabovethepersonispassingthecupoverthelaptop,whichissafewhenthecupisempty,butinthecasethatthecupisﬁlledwithwateronewouldpassitaroundthelaptop.5SummaryandOutlookDeepsemanticsdenotestheexistenceofdeclarativelygroundedmodels—e.g.,pertainingtospace,time,space-time,motion,actions&events,spatio-linguisticconceptualknowledge—andsystematicformalisationsupportingKR-basedca-pabilitiessuchasabstraction,learning,reasoning,embodiedsimulation.Rootedinthisconceptofdeep(visuo-spatial)semantics,thispaperpresentsanontologi-calandformalrepresentationalframeworkaimedatgroundingembodiedhuman-objectinteractionsinacommonsensecognitiveroboticssetting.ThemodelisillustratedwithselectRGBDdatasetscorrespondingtorepresentativeactivitiesfromalargerdatasetofeverydayactivities;aspreliminaryapplication,wealsoshowhowtheformalmodelcanbedirectlyappliedforcommonsensereason-ingwithconstraintlogicprogramming,withaparticularfocusonspace-timehistoriesandmotionpatterns.DeepSemanticAbstractionsofEverydayHumanActivities9at(joint(id(0),person(id(0),tracking_status(2),pos_3d(point(-0.280572,-0.0300787,2.15142)),time_point(2385869011))))).at(joint(id(0),person(id(1),tracking_status(2),pos_3d(point(0.605924,-0.162173,2.04098)),time_point(2385869011)))))....at(object(id(0)),type(cup),pos_3d(point(0.667643,-0.213097,1.83488)),time_point(2385869011))....GroundedInteractionSequenceBasedonthesensedbody-posedataandthedetectedobjects,asequenceofinteractionscanbequeriedfromtheexamplesequencesusingPrologsinteractivequeryansweringmode.?-grounded_interaction(occurs_in(Interaction,Interval),Grounding).Thisresultsinallinteractionsidentiﬁedintheexamplesequenceandtheirre-spectivegroundingwithrespecttothespatio-temporaldynamicsconstitutingtheinteraction,Interaction=pick_up(person(P),object(cup)),Interval=interval(t1,t3),Grounding=[occurs_at(grasp(body_part(right_hand,person(id(0))),object(cup)),timepoint(t1),holds_in(attached(body_part(right_hand,person(id(0))),object(cup)),interval(t2,t6)),holds_in(move_up(body_part(right_hand,person(id(0)))),interval(t2,t3))];Interaction=pass_over(person(P),person(Q),object(cup)),Interval=interval(t4,t7),Grounding=[holds_in(approaching(body_part(right_hand,person(id(0))),person(id(1))),interval(t4,t5)),holds_in(approaching(body_part(right_hand,person(id(1))),object(cup)),interval(t4,t5)),occurs_at(grasp(body_part(right_hand,person(id(1))),object(cup)),timepoint(t6),occurs_at(release(body_part(right_hand,person(id(0))),object(cup)),timepoint(t7),...];...Thisgroundingoftheactivitymaybeusedforinterpretationandlearningfromtheobservedactivitiesandtheinvolvedspatio-temporaldynamics,e.g.,intheexampleabovethepersonispassingthecupoverthelaptop,whichissafewhenthecupisempty,butinthecasethatthecupisﬁlledwithwateronewouldpassitaroundthelaptop.5SummaryandOutlookDeepsemanticsdenotestheexistenceofdeclarativelygroundedmodels—e.g.,pertainingtospace,time,space-time,motion,actions&events,spatio-linguisticconceptualknowledge—andsystematicformalisationsupportingKR-basedca-pabilitiessuchasabstraction,learning,reasoning,embodiedsimulation.Rootedinthisconceptofdeep(visuo-spatial)semantics,thispaperpresentsanontologi-calandformalrepresentationalframeworkaimedatgroundingembodiedhuman-objectinteractionsinacommonsensecognitiveroboticssetting.ThemodelisillustratedwithselectRGBDdatasetscorrespondingtorepresentativeactivitiesfromalargerdatasetofeverydayactivities;aspreliminaryapplication,wealsoshowhowtheformalmodelcanbedirectlyappliedforcommonsensereason-ingwithconstraintlogicprogramming,withaparticularfocusonspace-timehistoriesandmotionpatterns.10

Suchan and Bhatt

show how the formal model can be directly applied for commonsense reason-
ing with constraint logic programming, with a particular focus on space-time
histories and motion patterns.
Immediate next steps involve expanding the scope of everyday activities from
table-top or kitchen based scenarios to situations involving indoor mobility and
abstractions for the representation of social interactions between humans and
mobile agents. This will enable to further enhance the scope of the ontology and
corresponding spatio-temporal relations. Furthermore, the demonstrated appli-
cations of the ontology of space & motion are currently preliminary; next steps
here involve integration with state of the art robot control platforms such as
ROS; this will be accomplished via integration into the ExpCog commonsense
cognition robotics platform for experimental / simulation purposes, and within
openEASE as a state of the art cognition-enabled control of robotic control plat-
form for real robots.5

Acknowledgements. We acknowledge funding by the Germany Research Foundation
(DFG) via the Collabortive Research Center (CRC) EASE – Everyday Activity Science and
Engineering (http://ease-crc.org). This paper builds on, and is a condensed version of, a
workshop contribution (Suchan and Bhatt, 2017) at the ICCV 2017 conference. We also
acknowledge the support of Vijayanta Jain in preparation of part of the overall activity
dataset; toward this, the help of Omar Moussa, Hubert Kloskoski, Thomas Hudkovic,
Vyyom Kelkar as subjects is acknowledged.

Bibliography

M. Aiello, I. E. Pratt-Hartmann, and J. F. v. Benthem. Handbook of Spatial Logics.
Springer-Verlag New York, Inc., Secaucus, NJ, USA, 2007. ISBN 978-1-4020-5586-7.
G. Bartels, I. Kresse, and M. Beetz. Constraint-based movement representation
grounded in geometric features. In Proceedings of the IEEE-RAS International Con-
ference on Humanoid Robots, Atlanta, Georgia, USA, October 15–17 2013.

B. Bennett, A. G. Cohn, P. Torrini, and S. M. Hazarika. Describing rigid body motions
in a qualitative theory of spatial regions. In Proceedings of the Seventeenth National
Conference on Artiﬁcial Intelligence and Twelfth Conference on on Innovative Ap-
plications of Artiﬁcial Intelligence, pages 503–509, 2000a.

B. Bennett, A. G. Cohn, P. Torrini, and S. M. Hazarika. A foundation for region-based
qualitative geometry. In Proceedings of the 14th European Conference on Artiﬁcial
Intelligence, pages 204–208, 2000b.

M. Bhatt. Reasoning about space, actions and change: A paradigm for applications of
spatial reasoning. In Qualitative Spatial Representation and Reasoning: Trends and
Future Directions. IGI Global, USA, 2012. ISBN ISBN13: 9781616928681.

M. Bhatt. Between sense and sensibility: Declarative narrativisation of mental mod-
els as a basis and benchmark for visuo-spatial cognition and computation focussed
collaborative cognitive systems. CoRR, abs/1307.3040, 2013.

M. Bhatt and K. Kersting. Semantic Interpretation of Multimodal Human Behaviour
Data: Making Sense of Events, Activities, Processes. KI - K¨unstliche Intelligenz /
Artiﬁcial Intelligence, 2017.

5 ExpCog – http://www.commonsenserobotics.org

openEASE – http://ease.informatik.uni-bremen.de/openease/

Deep Semantic Abstractions of Everyday Human Activities

11

M. Bhatt and S. Loke. Modelling dynamic spatial systems in the situation
doi: 10.
URL http://www.tandfonline.com/doi/abs/10.1080/

Spatial Cognition & Computation, 8(1-2):86–130, 2008.

calculus.
1080/13875860801926884.
13875860801926884.

M. Bhatt, F. Dylla, and J. Hois. Spatio-terminological inference for the design of
ambient environments. In Spatial Information Theory, 9th International Conference,
COSIT 2009, Aber Wrac’h, France, September 21-25, 2009, Proceedings, volume
5756 of Lecture Notes in Computer Science, pages 371–391. Springer, 2009. ISBN
978-3-642-03831-0. doi: 10.1007/978-3-642-03832-7 23.

M. Bhatt, H. Guesgen, S. W¨olﬂ, and S. Hazarika. Qualitative spatial and tempo-
ral reasoning: Emerging applications, trends, and directions. Spatial Cognition &
Computation, 11(1):1–14, 2011a.

M. Bhatt, J. H. Lee, and C. P. L. Schultz. CLP(QS): A declarative spatial reasoning
framework. In Spatial Information Theory - 10th International Conference, COSIT
2011, Belfast, ME, USA, September 12-16, 2011. Proceedings, pages 210–230, 2011b.
doi: 10.1007/978-3-642-23196-4 12.

M. Bhatt, C. Schultz, and C. Freksa. The ‘Space’ in Spatial Assistance Systems: Con-
ception, Formalisation and Computation. In T. Tenbrink, J. Wiener, and C. Clara-
munt, editors, Representing space in cognition: Interrelations of behavior, language,
and formal models. Series: Explorations in Language and Space. 978-0-19-967991-1,
Oxford University Press, 2013.

A. G. Cohn and J. Renz. Qualitative spatial reasoning. In F. van Harmelen, V. Lifschitz,

and B. Porter, editors, Handbook of Knowledge Representation. Elsevier, 2007.

E. Davis. Pouring liquids: A study in commonsense physical reasoning. Artif. Intell.,

172(12-13):1540–1578, 2008.

E. Davis. How does a box work? a study in the qualitative dynamics of solid objects.

Artif. Intell., 175(1):299–345, 2011.

E. Davis. Qualitative spatial reasoning in interpreting text and narrative. Spatial

Cognition & Computation, 13(4):264–294, 2013.

Interleaved
K. S. R. Dubba, M. Bhatt, F. Dylla, D. C. Hogg, and A. G. Cohn.
inductive-abductive reasoning for learning complex event models.
In S. Mug-
gleton, A. Tamaddoni-Nezhad, and F. A. Lisi, editors, Inductive Logic Program-
ming - 21st International Conference, ILP 2011, Windsor Great Park, UK, July
31 - August 3, 2011, Revised Selected Papers, volume 7207 of Lecture Notes in
Computer Science, pages 113–129. Springer, 2011.
ISBN 978-3-642-31950-1. doi:
10.1007/978-3-642-31951-8 14.

K. S. R. Dubba, A. G. Cohn, D. C. Hogg, M. Bhatt, and F. Dylla. Learning relational
event models from video. J. Artif. Intell. Res. (JAIR), 53:41–90, 2015. doi: 10.1613/
jair.4395.

M. Eppe and M. Bhatt. Narrative based postdictive reasoning for cognitive robotics.
In COMMONSENSE 2013: 11th International Symposium on Logical Formalizations
of Commonsense Reasoning, 2013.

C. Eschenbach and K. Schill. Studying spatial cognition - a report on the dfg workshop

on ”the representation of motion”. KI, 13(3):57–58, 1999.

H. W. Guesgen. Spatial reasoning based on Allen’s temporal logic. Technical Report

TR-89-049. International Computer Science Institute Berkeley, 1989.

P. J. Hayes. Naive physics I: ontology for liquids. In J. R. Hubbs and R. C. Moore,
editors, Formal Theories of the Commonsense World. Ablex Publishing Corporation,
Norwood, NJ, 1985.

12

Suchan and Bhatt

S. M. Hazarika. Qualitative Spatial Change : Space-Time Histories and Continuity.
PhD thesis, The University of Leeds, School of Computing, 2005a. Supervisor -
Anthony Cohn.

S. M. Hazarika. Qualitative spatial change: space-time histories and continuity. PhD

thesis, The University of Leeds, 2005b.

D. Hern´andez, E. Clementini, and P. Di Felice. Qualitative distances. Springer, 1995.
R. Moratz. Representing relative direction as a binary relation of oriented points. In

ECAI, pages 407–411, 2006.

D. A. Randell, Z. Cui, and A. G. Cohn. A spatial logic based on regions and connection.

KR, 92:165–176, 1992.

J. Renz and B. Nebel. Qualitative spatial reasoning using constraint calculi. In Hand-

book of Spatial Logics, pages 161–215. 2007.

A. Scivos and B. Nebel. The Finest of its Class: The Natural, Point-Based Ternary
Calculus LR for Qualitative Spatial Reasoning. In C. Freksa et al. (2005), Spatial
Cognition IV. Reasoning, Action, Interaction: International Conference Spatial Cog-
nition. Lecture Notes in Computer Science Vol. 3343, Springer, Berlin Heidelberg,
volume 3343, pages 283–303, 2004.

M. Spranger, J. Suchan, M. Bhatt, and M. Eppe. Grounding dynamic spatial relations
for embodied (robot) interaction. In PRICAI 2014: Trends in Artiﬁcial Intelligence
- 13th Paciﬁc Rim International Conference on Artiﬁcial Intelligence, Gold Coast,
QLD, Australia, December 1-5, 2014. Proceedings, pages 958–971, 2014. doi: 10.
1007/978-3-319-13560-1 83. URL http://dx.doi.org/10.1007/978-3-319-13560-1 83.
M. Spranger, J. Suchan, and M. Bhatt. Robust Natural Language Processing - Com-
bining Reasoning, Cognitive Semantics and Construction Grammar for Spatial Lan-
guage. In IJCAI 2016: 25th International Joint Conference on Artiﬁcial Intelligence.
AAAI Press, 2016.

J. Suchan and M. Bhatt. Commonsense Scene Semantics for Cognitive Robotics: To-
wards Grounding Embodied Visuo-Locomotive Interactions. In ICCV 2017 Work-
shop: Vision in Practice on Autonomous Robots (ViPAR), International Conference
on Computer Vision (ICCV), 2017.

J. Suchan, M. Bhatt, and P. E. Santos. Perceptual narratives of space and motion for
semantic interpretation of visual data. In Computer Vision - ECCV 2014 Workshops
- Zurich, Switzerland, September 6-7 and 12, 2014, Proceedings, Part II, pages 339–
354, 2014. doi: 10.1007/978-3-319-16181-5 24.

S. Tellex. Natural language and spatial reasoning. PhD thesis, Massachusetts Institute

of Technology, 2010.

A. Tyler and V. Evans. The semantics of English prepositions: spatial scenes, embodied

meaning and cognition. Cambridge University Press, Cambridge, 2003.

P. A. Walega, M. Bhatt, and C. P. L. Schultz. ASPMT(QS): Non-Monotonic Spatial
Reasoning with Answer Set Programming Modulo Theories. In Logic Programming
and Nonmonotonic Reasoning - 13th International Conference, LPNMR 2015, Lex-
ington, KY, USA, September 27-30, 2015. Proceedings, pages 488–501, 2015. doi:
10.1007/978-3-319-23264-5 41.

F. Worgotter, E. E. Aksoy, N. Kruger, J. Piater, A. Ude, and M. Tamosiunaite. A

simple ontology of manipulation actions based on hand-object relations. 2012.

