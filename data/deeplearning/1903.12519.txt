AProvableDefenseforDeepResidualNetworksMatthewMirman1GagandeepSingh1MartinVechev1AbstractWepresentatrainingsystem,whichcanprovablydefendsigniﬁcantlylargerneuralnetworksthanpreviouslypossible,includingResNet-34andDenseNet-100.Ourapproachisbasedondifferentiableabstractinterpretationandintroducestwonovelconcepts:(i)abstractlayersforﬁne-tuningtheprecisionandscalabilityoftheabstraction,(ii)aﬂexibledomainspeciﬁclanguage(DSL)fordescribingtrainingobjectivesthatcombineabstractandconcretelosseswitharbitraryspeciﬁcations.OurtrainingmethodisimplementedintheDiffAIsystem.1.IntroductionRecentworkhasshownthatneuralnetworksaresusceptibletoadversarialattacksSzegedyetal.(2013):small,imperceptibleperturbationswhichcausethenetworktomisclassifytheinput.Thishasledtogrowinginterestintrainingprocedurestoproducerobustnetworks(Gu&Rigazio,2014;Zhengetal.,2016),newadversarialattacks(Papernotetal.,2016;Moosavi-Dezfoolietal.,2017;Xiaoetal.,2018;Athalye&Sutskever,2017;Evtimovetal.,2017),aswellasdefenseswhichusetheseattacksduringtraining(Goodfellowetal.,2014;Tram`eretal.,2017;Yuanetal.,2017;Huangetal.,2015;Madryetal.,2018;Dongetal.,2018).Whilenetworksdefendedusingattacksmaybeexperimentallyrobust,ithasbeenshownthatingeneralmoredataisneeded(Schmidtetal.,2018)andthatthisstyleoftrainingissampleinefﬁcient(Khoury&Hadﬁeld-Menell,2018).Further,whiledetectingadvarsarialattacks(Rozsaetal.,2016;Bhagojietal.,2017;Feinmanetal.,2017;Grosseetal.,2017)appearsapromisingcontingency,Carlini&Wagner(2017)foundthatmanyofthesetechniqueswereinsufﬁcient.Thelistofpossibleattacksisextensive(e.g.,(Akhtar&Mian,2018))andconstantlyexpanding,motivatingtheneedformethodswhichcanensurethatneuralnetworksare1DepartmentofComputerScience,ETHZurich,Zurich,Switzerland.Correspondenceto:MatthewMirman<matthew.mirman@inf.ethz.ch>.Preliminarywork.Copyright2020bytheauthors.provablyrobustagainsttheseattacks.Katzetal.(2017)developedaneuralnetworkveriﬁcationsystembasedonSMTsolvers,howeveritonlyscaledtosmallnetworks.Gehretal.(2018)introducedabstractinterpretation(Cousot&Cousot,1977)asamethodforverifyingmuchlargernetworks.However,asthesizeofnetworksthatveriﬁcationsystemscouldhandleincreased,itbecameclearthatveriﬁablerobustnesscouldbesigniﬁcantlyimprovedbyemployingprovablyrobusttraining.Theﬁrstattemptsfortrainingprovablyrobustnetworks(Raghunathanetal.,2018;Kolter&Wong,2017;Dvijothametal.,2018)scaledtosmallsizeswithatmosttwoconvolutionallayers.Laterworksawthedevelopmentoftwomethods:(i)thedual-methodinthecaseofWongetal.(2018),and(ii)differentiableabstractinterpretationintroducedbyMirmanetal.(2018)(DiffAI)andusedinGowaletal.(2018)(IBP)andWangetal.(2018)(MixTrain).Whilethesepushedtheboundaryintermsofprovableveriﬁedrobustnessandnetworksize(withnetworksofupto230kneurons),scalingaprovabledefensetoafullImageNetsizednetworkremainsakeychallenge.Inparticular,ResNet-34representsanimportantmilestonetoachievingthisgoalasitisthesmallestresidualnetworkproposedbyHeetal.(2016).Toaddressthischallenge,weintroduceanovelapproachtorobustness,onewherethenetworkitselfisdesignedtobeprovablyrobustsimilartoattemptswhichaimtodesignnetworkstobeexperimentallyrobustbyconstruction(Cisseetal.,2017;Sabouretal.,2017).Inparticular,weintroducetheparadigmof“programmingtoprove”,longknowntotheprogramminglanguagescommunity(Delahaye,2000;Wagneretal.,2013),asatechniqueforcreatingprovablyrobustarchitectures.WeshowhowtointegratethisideawithDiffAI,resultinginasystemthancantrainaprovablyrobustResNet-34(asmallerresnetisshowninFig.1).MainContributionsOurmaincontributionsare:•Theconceptofanabstractlayerwhichhasnoeffectonstandardnetworkexecutionbutimprovesprovablyrobustlearning.•Adomainspeciﬁclanguage(DSL)forspecifyingsophisticatedtrainingobjectives.•Acompleteimplementationandevaluationofourmethod.OurexperimentalresultsindicatethearXiv:1903.12519v2  [cs.LG]  7 Jan 2020AProvableDefenseforDeepResidualNetworksCorrelateMax32Conv31,1DecorrMin8Conv31,1Conv11,0Conv31,1+rDecorrMin4Conv31,1Conv11,0Conv31,1+rDecorrAllConv31,1Conv11,0Conv31,1+rConv31,1Conv11,0Conv31,1+rCorrelateMax10FC500Figure1.ResNet-TinywithAbstractLayers.LayerswithdarkorangeontheirrightincludeaReLU,andthespherewithanrinitisalsoaReLU.Convks,pisaconvolutionwithakernelsizeofk,astrideofs,andapaddingp,Thisnethas311kneuronsand18mparameters.approachcanachieveprovablerobustnessfornetworksanorderofmagnitudelargerthanpriorwork.2.BackgroundonRobustTrainingWenowprovidenecessarybackgroundontrainingneuralnetworkstobeprovablyrobustagainstadversarialexamples.AneuralnetworkNθ:Rd→Rkmapsad-dimensionalinputtoak-dimensionaloutputbasedonlearnedweightsθ.LetB(cid:15)(x)bethe‘∞-ballofradius(cid:15)aroundaninputx∈Rd.AnetworkNθiscalled(cid:15)-robustaroundapointx∈Rdif∀˜x∈B(cid:15)(x),Nθ(˜x)i>Nθ(˜x)jwherei,j∈{1,...,k}andj6=i.Thegoalofarobusttrainingprocedureistolearnaθsuchthat:(i)Nθassignsthecorrectclassyitoeachtrainingexamplexi,and(ii)Nθis(cid:15)-robustaroundeachexamplexi.DifferentiableAbstractInterpretationInthisworkweleveragethedifferentiableabstractinterpretationframeworkintroducedbyMirmanetal.(2018).Here,oneveriﬁesneuralnetworkrobustnessandformulatesprovabilitylossesbyconstructingsoundoverapproximationsusingabstractinterpretation(Cousot&Cousot,1977).Wenowintroducethenecessarytermsusedlaterinthepaper.Deﬁnition2.1.AnabstractdomainDconsistsof:(a)abstractelementsrepresentingasetofconcretepointsinP(Rp)forp∈N,(b)aconcretizationfunctionγ:D→P(Rp)mappinganabstractelementd∈Dtothesetofconcretepointsitrepresents,and(c)asetofabstracttransformersT#approximatingtheconcretetransformerTinP(Rp),i.e.,T(γ(d))⊆γ(T#(d)).Ourapproachadditionallyrequirestheexistenceofanabstractionfunctionα:P(Rp)→DmappingthesetofconcretepointsinRptoanabstractelementd∈D.AbstracttransformerscomposeandhencebydeﬁningabstracttransformersforeachbasicoperationinaneuralnetworkN,wecanderiveanoverallabstracttransformerT#NfortheentireN.WeapplyabstractinterpretationtocomputeT#N(α(B(cid:15)(x))),describingasupersetofthex3:=x1+x2x4:=x1−x2x3:=x1+x2x4:=x1−x2h1=h0,1ih2=h0,1ih3=h0,2ih4=h0,2ih1=h0,0,(1,0)ih2=h0,0,(0,1)ih3=h0,0,(1,1)ih4=h0,0,(1,-1)i(a)Box(b)HybridZonotopeFigure2.Comparingtheprecisionoftheafﬁnetransformersinthe(a)Boxand(b)HybridZonotopedomains.possibleoutputsofNforallinputsinB(cid:15)(x)whichcanbeusedtocomputeanabstractlossasinMirmanetal.(2018).HybridZonotopeDomainInthisworkweusetheHybridZonotopeDomainasdescribedbyMirmanetal.(2018).Thisdomain,introducedoriginallybyGoubault&Putot(2008),isageneralizationoftwodomains:(i)thesimpleBoxdomain(theBoxdomainisalsoreferredtoasintervalboundpropagationinGowaletal.(2018))and,(ii)thebasezonotopedomainGhorbaletal.(2009).Themainbeneﬁtofhybridzonotopesisthattheyallowformoreﬁne-grainedcontrolofanalysisprecisionandperformance.TheHybridZonotopedomainassociateswitheverycomputedresultv(e.g.,aneuron)inthenetwork,atriplethv=h(hC)v,(hB)v,(hE)viwhereh=hhC,hV,hEiandisreferredtoasthehybridzonotopeoverallpvariables.Here,(hC)v∈Risacenterpoint,(hB)v∈R≥0isanon-negativeuncorrelatederrorcoefﬁcient(similartotheBoxdomain),and(hE)v∈Rmarethecorrelatederrorcoefﬁcientsthenumbermofwhichdeterminetheaccuracyofthedomain.Thesecoefﬁcientsdeﬁneanafﬁnefunctionbhwhichisparameterizedbythecorrelatederrortermse∈[−1,1]mandanuncorrelatederrortermAProvableDefenseforDeepResidualNetworksβ∈[−1,1]p:bh(β,e)=(h1(β,e),...,hp(β,e))where:chv(β,e)=(hC)v+(hB)v·βv+(hE)v·eDifferentvariablessharethecorrelatederrortermswhichintroducesdependenciesbetweenvariablesmakingover-approximationsmoreprecisethanthoseproducedwiththeBoxdomain(whichdoesnottrackdependencies).Formally,theconcretizationfunctionγHofahybridzonotopehisγH(h)={bh(β,e)|β∈[−1,1]p,e∈[−1,1]m}.AboxbcanbeexpressedasahybridzonotopehwithhC=bC(thebox’scenter),hB=bB(thebox’sradius)andm=0.Descriptionsofourhybridzonotopetransformers(e.g.,ReLU),canbefoundinMirmanetal.(2018).IntervalconcretizationForoperationssuchasconstructinganabstractlossorbuildingheuristicsinabstractlayers,itisnecessarytodeterminetheboundsofahybridzonotopehforthei-thvariableusingintervalconcretization:ιH(h)i=[(hC)i−(cid:15)H(h)i,(hC)i+(cid:15)H(h)i]where(cid:15)H(h)i=(hB)i+Pmj=1|(hE)i,j|isthetotalerror.Example:Boxvs.HybridZonotopeFig.2showsanafﬁnetransformationoninputsabstractedinboththeBoxandtheHybridZonotopedomains.TheboxrepresentationinFig.2(a)onlycontainsthecenterandtheuncorrelatederrorcoefﬁcientswhereasthehybridzonotoperepresentationinFig.2(b)alsocontainsnon-zerocorrelatederrorcoefﬁcients.Theafﬁnetransformationcreatesdependencybetweenx3andx4astheyareassignedvaluesusingafﬁneexpressionsdeﬁnedoverthesamevariablesx1andx2.TheBoxdomaincannotcapturethisandasaresultitsoutputislessprecise(containsmoreconcretepoints)thantheoneproducedwithHybridZonotopedomain.3.AbstractLayersforVeriﬁableNetworksProgramveriﬁcationofteninvolvesboththeadditionoferasableannotations(B¨ohmeetal.,2008)andprogramtransformationstomaketheresulting(semanticallyequivalentprogram)moresuitableforveriﬁcation(Wagneretal.,2013).Thatis,unlikestandardtransformationswhichaimtoproduceaprogramthatrunsfaster,here,thegoalistoproduceamoreveriﬁableprogram.Ourkeyinsightistoleveragethis“programmingtoprove”paradigminasimilarfashionwhendesigningrobustneuralnetworks.Basedonthisinsight,wedescribethenovelconceptofAbstractLayers.Theselayersarespeciﬁcallyprovidedbythenetworkdesignerbutdifferfromtraditionalconcretelayersinthattheyhavenoeffectontheconcreteexecution.Instead,theyonlyaffecttheanalysisofthenetwork,i.e.,theyonlymodifyabstractelementsthatpropagatethroughthelayers(e.g.,boxesorhybridzonotopes).WedescribetwotypesofabstractlayersdesignedtotunetheprecisionandscalabilityoftheanalysiswiththeHybridZonotopedomain.Forallabstractlayers,wedescribetheireffectonagivenhybridzonotopehwithmcorrelatederrorcoefﬁcients,producinganewhybridzonotopeh0.Forourabstractlayers,itholdsthath0C=hC.3.1.CorrelationlayersAcorrelationlayerincreasestheprecisionoftheanalysisinsuccessivelayersbyproducinganewhybridzonotopeh0whichcontainsmorecorrelatederrorcoefﬁcientsthantheoriginalinputh.Wenotethatherewehaveγ(h0)=γ(h),thatis,bothhybridzonotopesactuallyrepresentthesamesetofpoints.However,theadvantageofh0overhisthath0containsmoreshareddependenciesbetweendifferentdimensions(variables)thanh,meaningthatsuccessivestepsoftheanalysisusingh0willbeproducemorepreciseresultsthanthosesamestepsusingh.Informally,acorrelationlayerselectsasetofdimensionindices(variables)Pandcreates|P|newcorrelatederrorcoefﬁcients.Foreachselectedvariablei∈P,weintroduceonecorrelatederrorcoefﬁcientwhosevalueisthatofthevariable’suncorrelatederrorcoefﬁcient.Allotherremainingcorrelatedcoefﬁcients(atotalof|P|−1)foriaresetto0.ForallvariablesnotinP,theirnewcorrelatederrorcoefﬁcientsareallsetto0.Moreformally,givenh,wedeﬁneh0asfollows:h0B,i=hB,ii/∈Ph0B,i=0i∈Ph0E,i,j=hE,i,j∀0≤i<p,0≤j<mh0E,i,m+t=hB,ii∈P∧t=|P<i|h0E,i,m+t=0∀t<|P|.i/∈P∨t6=|P<i|HereweuseP<itodenotethesubsetofPwhereeachelementissmallerthani.Next,wedeﬁnefourvariantsofacorrelationlayerbasedonthechoiceofthesetP.CorrelateAllcorrelatesalluncorrelatedcoefﬁcientsinallpdimensionstherebyaddingpcorrelatederrorcoefﬁcients.Formally,itusesP={i|0≤i<p}.CorrelateFixedkcorrelateskﬁxeddimensions,chosenbytakingeverypkoftheﬂattenedlistofdimensionindices.Formally,wehaveP={bi·pkc|0≤i<k}.CorrelateMaxkcorrelatestheﬁrstkdimensionswhoseintervalconcretization(seeSection2)hasthelargestupperboundvalue.Thisheuristicaimstoimproveprecisionwhilestillkeepingtheanalysisscalable.Formally,wehaveAProvableDefenseforDeepResidualNetworksx1x2x3x4x5x6x4x5x6x7x8x9x7x8x9x10x11x12[-1,1][-1,1][-1,1]InputlayerFirsthiddenlayerθ1CorrelateMax2Secondhiddenlayerθ2DecorrelateMin1Outputlayerθ3h1=h0,1ih2=h0,1ih3=h0,1iθ1h4=h0,0.75ih5=h0,1.25ih6=h0,0.5ih4=h0,0,(0.75,0)ih5=h0,0,(0,1.25)ih6=h0,0.5,(0,0)iθ2h7=h0,0,(1,−1)ih8=h0,0,(1,0.5)ih9=h0,0,(0,0)ih7=h0,1,(1)ih8=h0,0.5,(1)ih9=h0,0,(0)iθ3h10=h0,0,(1)ih11=h0,0.5,(0.5)ih12=h0,0,(0)iFigure3.Ouranalysisonanexampletoynetworkaugmentedwithabstractlayers.P={i|UB(ιH(h)i)∈TOPk(UB(ιH(h)))}whereTOPkreturnstheklargestelementsandUBreturnstheupperboundofaninterval.CorrelateMaxPoolc,w,h,scorrelatesdimensionschosenusingMaxPooling(Krizhevskyetal.,2012).WeapplyMaxPoolingwithkernelsize(c,w,h)andstridesonafunctionfdeﬁnedoverh.Ifthecorrelationisappliedbeforetheﬁrstlayerthenf=hCotherwisef=hB.Formally,P={i|f(i)∈MAXPOOLc,w,h,s(f)}.3.2.DecorrelationlayersThepurposeofdecorrelationlayersisoppositethatofcorrelationlayers:toreducethenumberofcorrelatedcoefﬁcientssotomakeanalysisforsuccessivelayersmoreefﬁcientbutlessprecise.Concretely,adecorrelationlayerremovescorrelatederrorcoefﬁcientsandaddstheirabsolutesumtothevalueofuncorrelatederrorcoefﬁcientsineachdimension.WenowintroducetwochoicesforthesetP,eachdeﬁningdifferentdimensionstobedecorelated:DecorrelateAllproducesahybridzonotopewithnocorrelatedcoefﬁcientsinanydimensionandineachdimension,theuncorrelatedcoefﬁcientisdeﬁnedas:h0B,i=hB,i+m−1Xj=0|hE,i,j|DecorrelateMinkisbasedonaheuristictominimizethelossofprecisionduetodecorrelationbyremovingm−kcorrelatederrorcoefﬁcientswhoseabsolutesuminalldimensionsisthesmallest.Asaresult,kcorrelatedcoefﬁcientsremainintheoutput.Formally,wedeﬁnePtobetheindicesofthem−ksmallestelementsofthesequence{Pp−1i=0|hE,i,j|}j=0...m−1.Then,thenewzonotopeh0,wherei∈[0,p),isdeﬁnedas:h0B,i=hB,i+Pj∈P|hE,i,j|h0E,i,j=hE,i,tt/∈P∧j=t−|P<t|Informally,intheﬁrstequation,weaccumulateallcorrelatedcoefﬁcientschosenforremoval(thatis,thesetP)intotheuncorrelatedcoefﬁcient,whilethesecondequationensurestheremainingcorrelatedcoefﬁcientsareshiftedtobenexttoeachother(orderispreserved).3.3.DeepLossFordeeperneuralnetworkssuchasResNet-18,itispossiblefornaiveabstractionimprecisiontogrowexponentiallytothepointwhereoverﬂowoccursbeforetheﬁnallossiscalculated,makingoptimizingthatlossfutile.Insuchcases,wewouldlikethenetworktoproducemorepreciseresultsinintermediatelayers,beforeanoverﬂowoccurs.Astheselayersdonothavethesamenumberofneuronsastargetclassiﬁcations,wecannotoptimizeusingastandardprovabilityloss.Instead,alossonagenericheuristicforprovabilitymustbedeﬁnedontheoutputofaspeciﬁclayer.Asthislossdoesnoteffectconcreteexecutionandoperatesusingtheabstractelementfromaspeciﬁclayer,wealsoconsideritaformofanabstractlayer.Wedeﬁnethefollowinglossesonanintervalconcretizationcinndimensions:Llb,f,i(c)=max{f(cj,2−ci,1)|cj,1≤ci,1}Lub,f,i(c)=max{f(ci,2−cj,1)|ci,2≤cj,2}Ldeep,f(d)=12nPn−1i=0(Llb,f,i(ι(d))+Lub,f,i(ι(d)))wherefisapositiveactivationandLdeep,fcombinestheﬁrsttwolossesforeachdimensionofanarbitraryabstractelementd.Intuitively,thislossmeasuresandsumsforeachdimensiontheworstoffendingoverlapbetweentheconcretizationlowerboundinthatdimensionandtheupperAProvableDefenseforDeepResidualNetworksboundofanyotherdimension,andvisaversa.Inourexperiments,weusedReLUforf.Anaiveimplementationoftheabovelosswouldrequiren2computations(andpotentiallyn2spaceonaGPU),whichcouldbeproblematicgiventhatthelossisintendedtobeusedontheoutputofanintermediate,andpresumablyquitewide,layer.Whileamatrixmultiplicationwouldalsotypicallyinvolveusinguptoann2sizedmatrix,thislossisintendedtobeusedbetweenconvolutionswhichtypicallypermitssigniﬁcantlywideroutputsthroughutilizingsigniﬁcantlysmallerkernels.WeimplementitleveragingonedimensionalMaxPoolandSortsothatmarshalingbetweentheCPUandGPUisnotrequired,andsuchthatalgorithmicoptimizationstoMaxPoolcanbeleveragedtopotentially1provideanO(nlogn)implementation.3.4.ExamplenetworkwithabstractlayersFig.3showsouranalysiswithabstractlayersonanexampletoyfeedforwardnetwork.Theneuralnetworkcontainsthreeneuronsperlayer.WeaddaCorrelateMax2andaDecorrelateMin1abstractlayersaftertheﬁrstandsecondhiddenlayerrespectively.Weshowthe3-dimensionalshapespropagatedthrougheachlayeralongwiththecorrespondinghybridzonotopebasedencoding.Thetop,middle,andbottomneuronsineachlayerrepresentthex,y,andz-directionsintheshapes.OuranalysisabstractstheinputregionwithaBoxandpropagatesitthroughtheﬁrsthiddenlayer.Aftercorrelationsareadded,theabstractionisshowninblue(beforecorrelations,theshapeisgray).CorrelateMax2changestheencodingoftheabstractelementobtainedaftertheﬁrsthiddenlayerbycreatingcorrelatederrorcoefﬁcientsforneuronsx4andx5whoseupperboundislargerthanthatforx6.Theintroductionofcorrelatederrorcoefﬁcientsincreasestheprecisionoftheresultobtainedafterapplyingthetransformersforthesecondhiddenlayerastheresultingshapeisnolongerabox.Wenotethatneuronx9issetto0intheresult.DecorrelateMin1removesthesecondcorrelatederrorcoefﬁcientasitsabsolutesumoverallneuronsissmaller.Theabsolutevalueofthiscoefﬁcientisaddedtotheuncorrelatedcoefﬁcientineachdimension.Thischangestheconcretizationoftheabstractelementbyremovingdependenciesmakingitlessimprecisewhileincreasingscalability.Theoutputlayertransformationsnextproducearesultmoreprecisethantheboxobtainedwithouttheabstractlayers.1ProvidedanoptimalimplementationofMaxPool(a)δ=0.2(a)δ=0.35Figure4.Fivetwo-dimensionalabstractions(orangeboxes)producedbyαgwheregisSample(δ,Normal,Box)4.SpecifyingTrainingObjectivesWeintroduceadomainspeciﬁclanguage(DSL)forspecifyingtrainingobjectivesandparameterscheduling.Forexample,itcancapturethetraininglossandschedulingproposedbyGowaletal.(2018)(IBP).4.1.SpecifyingSchedulesWedescribetwoconstructorsfordescribingascheduleusedtoadjustthevaluesoftrainingparameters(e.g.,sizeoftheballsaroundimagesusedintraining)dynamically,leadingtoimprovedresults.Ascheduleisafunctionwhichusesthecurrenttrainingtimestepcorrespondingtothefractionalnumberofepochscompleted(e.g.,completing25000ofthe50000examplesfromtheﬁrstepochonCIFAR10wouldprovideatime-stepvalueof0.5).Theconstructorsbelowdescribehowto(recursively)buildthisfunction.Lin(a,b,m,n)speciﬁestheparametervalueshouldbethestartvalueafortheﬁrstmepochs.Then,linearparameterannealingbetweenstartvalueaandendvaluebovernepochsshouldbeusedtodeterminetheparametervalue.Until(m,s1,s2)speciﬁesthattheﬁrst-scheduleconstructors1willbeusedtodeterminetheparametervalueuntilmepochsarereached,andthenthesecond-schedules2willbeusedbutwillbegiventhetimewithmepochssubtracted.4.2.SpecifyingTrainingGoalsWenextdescribethegoal-constructorsfordescribinghowtobuildtheabstractionfunctionandtrainingloss.AttimestepsoftraininganetworkNonanexampleowithatargetlabel(t),foreachgoalconstructor(g)intheabstractsyntaxtree(AST),webuild:(i)anabstractionfunction(αg)whichtakestheinputboxfortrainingspeciﬁedbythelower(l)andupperbound(u)vectorsasinputandreturnsanabstractelementd=αg(l,u),and(ii)alossfunctionlossg(d,t).Beforetraining,theuserprovidesthegoalwhichisparsedintoanAST(gU)andatrainingwidth((cid:15)).Thelossusedtotrainis,foradatasetwithvaluesintherangeofatob:lossgU(T#N(αgU(max(o−(cid:15),a),min(o+(cid:15),b))),t).Table1formalizesourgoalconstructors,describedbelow:Pointreturnsthecenteroftheinputboxspeciﬁedbylandufortrainingandusesthecrossentropyloss.NormalreturnsapointsampledfromthenormalAProvableDefenseforDeepResidualNetworksTable1.Ourdifferentgoalconstructorsfortraining.Eachassumestheexistenceofatargetlabelt.Goalconstructorabstractelementd=α(l,u)loss(d,t)Pointl+u2cross-entropy(d,t)NormalMAX(MIN(0.5·(u−l)·NORMALRAND(0)+l,l),u)cross-entropy(d,t)UniformMAX(MIN(0.5·(u−l)·(UNIFORMRAND(2)−1)+l,l),u)cross-entropy(d,t)IFGSMkFGSM(k,l,u,t)cross-entropy(d,t)Boxhl+u2,l−u2,0icross-entropy(d,t)Mix(g1,g2,λ)(d1,d2)=αg1(l,u)×αg2(l,u)(1-λ)·lossg1(d1,t)+λ·lossg2(d2,t)Sub(δ,g)αg(0.5·(u+l−δ·(u−l)),0.5·(u+l+δ·(u−l),t)lossg(d,t)Sample(δ,r,gs,gt)αgt(b−0.5·δ(u−l),b+0.5·δ(u−l),t)wherelossgt(d,t)b=CENTER(Sub(1−r·δ,gs))BiSample(g1,g2)αg2(l0,u0,t)wherelossg2(d,t)l0=0.5·(l+u)−|UB(αg1(l,u))-0.5·(l+u)|andu0=0.5·(l+u)+|UB(αg1(l,u))-0.5·(l+u)|Table2.Trainingschemesusedfortheexperiments.Thosewithsubscript18orLareusedbyResNet-18orResNetLargerespectively.NameTrainingSchemeBaselineMix(Point,Sub(Lin(0,1,150,10),Box),Lin(0,0.5,150,10))InSampMix(Point,Sample(Lin(0,1,150,10),0.5,Normal,Box),Lin(0,0.5,150,10))InSampLPAMix(Point,Sub(Lin(0,1,150,10),Sample(Lin(0,1,150,10),0.5,Normal,Box)),Lin(0,0.5,150,10))AdvkISMix(Sub(Lin(0,1,20,20),IFGSMk),Sample(Lin(0,1,150,10),0.5,Normal,Box),Lin(0,0.5,150,10))AdvkISLPAMix(Sub(Lin(0,1,20,20),IFGSMk),Sub(Lin(0,1,150,10),Sample(Lin(0,1,150,10),0.5,Normal,Box)),Lin(0,0.5,150,10))AdvkISLPAUSMix(Sub(Lin(0,1,20,20),IFGSMk),Sub(Lin(0,1,150,10),Sample(Lin(0,1,150,10),Uniform1,Box)),Lin(0,0.35,150,10))BaselineS18Mix(Point,Sub(Lin(0,1,200,40),Box),Lin(0,0.5,200,40))InSampS18Mix(Point,Sample(Lin(0,1,200,40),0.5,Normal,Box),Lin(0,0.5,200,40))AdvkISS18Mix(Sub(Lin(0,1,20,20),IFGSMk),Sample(Lin(0,1,200,40),0.5,Normal,Box),Lin(0,0.5,200,40))BaselineS18Mix(Point,Sub(Lin(0,1,200,40),Box),Lin(0,0.5,200,40))InSampS18Mix(Point,Sample(Lin(0,1,200,40),0.5,Normal,Box),Lin(0,0.5,200,40))AdvkISS18Mix(Sub(Lin(0,1,20,20),IFGSMk),Sample(Lin(0,1,200,40),0.5,Normal,Box),Lin(0,0.5,200,40))AdvkISLPAR18Mix(Sub(Lin(0,1,20,20),IFGSMk),Sub(Lin(0,1,200,40),Sample(Lin(0,1,200,40),1,Uniform,Box)),Lin(0,0.5,200,40))InSampLPAR34Mix(Point,Sub(Lin(0,1,200,40),Sample(Lin(0,1,200,40),1,Uniform,Box)),Lin(0,0.5,200,40))AdvkISLPAD100Mix(IFGSMk,Sub(Lin(0,1,150,50),Sample(Lin(0,1,150,50),1,Uniform,Box)),Lin(0,0.5,150,50))BiAdvLMix(IFGSM2,BiSample(Sub(Lin(0,1,150,30),IFGSM3),Lin(0,0.6,200,30))distributionaroundtheinputbox(viathefunctionNORMALRAND)andclippedtothatbox.Itusesthecrossentropyloss.IFGSMkuseskiterationsofFGSMtoﬁndanadversarialexampleintheinputboxandusesthecorrespondingpointfortraining.Thecrossentropylossisused.Boxreturnsahybridzonotopeabstractelementwithnocorrelatederrorcoefﬁcientsabstractingtheboxbetweenthelower(l)andupper-bound(u).Thelossconcretizestheabstractelementdandreturnsthemaximumcrossentropylossonapointintheconcretization(Gowaletal.,2018).Mix(g1,g2,λ)takestwogoalconstructorsg1andg2andaﬂoatλasinputs.Theabstractelementusedfortrainingisthecartesianproductoftheabstractionsd1andd2oftheinputboxing1andg2.Thelosslinearlycombinesthelossfunctionsfromg1andg2usingλ.Sub(δ,g)takesaﬂoatδandagoalconstructorgasinputs.Itcomputestheabstractelementfortrainingbycallingtheabstractionfunctionαgoftheconstructorgusingthenewboundsl0=0.5·(u+l−δ·(u−l))andu0=0.5·(u+l+δ·(u−l)).Theinsightbehindthisconstructoristousetrainingelementsconstructedfromboxesthatoverlapwiththeinputbox.Theoutputlossisthelossfromg.Sample(δ,gs,gt)usesSub(1−δ,gs)toﬁndapointb,bytakingthecenterofthereturnedtrainingelement,andpassesl0=b−0.5·δ(u−l)andu0=b+0.5·δ(u−l)totheabstractionfunctionαgtofgt.Theoutputlossisfromgt.ThisisvisualizedinFig.4.BiSample(g1,g2)usestheabstractelementαg1(l,u,t)fortheinputing1andcomputesl0=0.5·(l+u)−|(UB(αg1(l,u)))−0.5·(l+u)|andu0=0.5·(l+u)+|(UB(αg1(l,u)))−0.5·(l+u)|.AProvableDefenseforDeepResidualNetworksTheoutputelementisαg2(l0,u0,t).Itusesg2’sloss.Wenotethatﬂoatingpointparameterssuchasδandλusedintheconstructorsabovecanuseschedulingconstructors.4.3.ExampleTrainingSchemesEarlier,weobservedthatourtrainingDSLcouldbeusedtospecifycomplextrainingschemessuchasIBP.Inparticular,IBPuseslinearparameterannealingonboththeepsilonusedintrainingandtheweightoftheprovabilityloss,togetherwithacrossentropybasedlossfunctioninsteadofthehingelossdesignedbyMirmanetal.(2018)(DiffAI).Usingthiscustomization,IBPimprovesontheresultsofDiffAIwhilestillusingtheintervaldomainfortrainingasdonebyDiffAI.InourDSL,thistrainingschemecouldbewrittenas:Mix(Point,Sub(Lin(0,1,150,10),Box),Lin(0,0.5,150,10)).InTable2,weshowanumberofexampletrainingschemescapturedasexpressionsinourDSL.Wefoundthefollowingschemestobeparticularlyuseful(theseareevaluatednext):InSampinterpolatesbetweentrainingonrandompointsintheL∞(cid:15)-Ballandanabstractboxsurroundingtheexample.Theideaisthatitmightbeeasiertotrainanetworkonapointtobe(cid:15)robustifinsteadofitbeingonly(cid:15)−µrobustalready,everypointaroundinthe(cid:15)boxarounditisalso(cid:15)−µrobustforsmallµ>0.InSampLPAisthesameasInSamp,butalsousesschedulingforthesizeofthesamplingdomain,bysurroundingitwithSub.Theideaisthatusingthesamplingdomainisakindofadversarialtraining,anditmightbeeasierforthenetworktolearnthestandarddatasetﬁrst.5.ExperimentalSetupOursystem,andthecodeforreproducingexperiments,ispubliclyavailableathttps://github.com/eth-sri/diffai.WeimplementedthissystemusingPyTorch-0.4.1.WeranallexperimentsusingGeForceRTX2080TiGPUs.Wedonotuseweightnormalizationreparameterizationandclipping.Todemonstratetheeffectivenessofourtechnique,weevaluateusingthemostchallengingdatasetcommonlyusedforprovableveriﬁcationtasks,CIFAR-10(Krizhevsky,2009).Wealsousethelargestcommonlyusedepsilon,(cid:15)=0.031373∼8/255.Allaccuraciesandveriﬁablerobustnesspercentagesusethefull10,000imagetestset.Toaugmentthedatasetandmakeiteasiertolearn,randomcroppingwithapaddingof4wasused(thismaintainsimagesize)aswellasrandomhorizontalﬂipping.WhileIBPandMixTrainpresentedimprovementstorobusttraining,wedidnotcompareagainstthesesystems.ForIBP,thepubliccodedidnotcontainresidualnetworksthoughwewereabletointegratetheproposedtrainingimprovementsintoDiffAI.ForMixTain,thecodebasewasunavailable,andwechosenottoperformnormalizationonthedatasetpriortousage.Instead,weaddaﬁxedlayertoeachnetworkinordertomakeattackandveriﬁcationepsilonseasiertocompareacrossdifferentsystems.Fortestingtheattackedaccuracy,weusedMI-FGSM(Dongetal.,2018)withµ=0.8,20iterations,andastepsizeof0.0031373.Totestveriﬁablerobustness,weusedDiffAI’sbuilt-inHybrid-Zonotopedomain(describedearlier).5.1.EvaluatedNetworksAbriefoverviewofthenetworksizesweevaluateonandtheirtrainingspeedunderwellperformingtrainingschemes,isshowninTable3.Tothebestofourknowledge,noothersystemcantrainasdeepandaslargeprovablenetworksasoursystem.IntheAppendix,weprovidethecompletetableforalltrainingschemes.Next,wegiveabriefdescriptionofthesenetworksandourtrainingparameters.ResNet-Tinyisawideresidualnetwork,12layersdeep,similartotheResNetdescribedbyWongetal.(2018)butwithmoreandwiderlayersshowninFig.1.Ithas50%moreneuronsthanthelargestCIFAR10networktrainedviaIBPorWangetal.(2018).Forthisnetworkwealwaysuseaninitiallearningrateof0.001withascheduleasusedbyIBP,andAdamoptimization(Kingma&Ba,2014).WealsouseanL2regularizationconstantof0.01.SkipNet-18isan18layerdeepnetworkwith4residualconnectionsadaptedfromPyTorch’svisionlibrary.Forthisnetworkwealwaysuseaninitiallearningrateof0.1andaschedulewheretherateismultipliedby0.1atsteps10,20,250and300.InsteadofAdam,standardSGDisused.TheL2regularizationconstantissetto0.0005.ResNet-18andResNet-34are18and34layer(respectively)deepresidualnetworksadaptedfromPyTorch’svisionlibrary.Forthesenetworkwealwaysuseaninitiallearningrateof0.1andasimilarschedulewheretherateismultipliedby0.1atsteps10,20,250,300,and350,andabatchsizeof200.WealsouseSGDhere,butdonotuseanyregularization.DenseNet-100isanetworkwith99layers,andmanyresidualconnections,adaptedfromthemodelsproposedbyHuangetal.(2017).Toourknowledge,thisisthelargestnetworkintermsofdepthandthenumberofneuronstohavebeenprovablytrainedsofar.Forthisnetwork,wealwaysuseaninitiallearningrateof0.1andaschedulewheretherateismultipliedby0.1atsteps20,50,200,250,and300.WealsouseSGDhere,andnoregularization,andabatchsizeof50.Duetoitssize,itisonlyveriﬁedusingtheBoxdomainandnotwiththehybridZonotopedomain.AProvableDefenseforDeepResidualNetworksTable3.Listofnetworks,andthetrainingschemesthatachievedthehighestprovablerobustness.NetNeuronsParamsAbstractLayersTrainingSchemes/epochAccuracy%VeriﬁedRobustness%ResNet-Tiny312k18mManyFixedAdv1ISLPAUS30340.223.2SkipNet-18558k15mNoneAdv5ISS1826028.421.2ResNet-Large639k66mLargeComboBiAdvL52738.13.0ResNet-18558k15mNoneAdv5ISLPAR1823332.322.3ResNet-34967k25mNoneInSampLPAR3417635.119.5DenseNet-1004.5m748kNoneAdv5ISLPAD10072736.421.95.2.AbstractLayersToevaluatetheeffectofabstractlayers,weinvestigatedavarietyofconﬁgurationsfortheabovenetworks.Forallofournetworks,weuseCorrelateAllbeforethelastlinearlayerduringbothtrainingandtesting.Thishastheeffectofnotcausinganylossofaccuracybythatlinearlayerbeforetheconcretizationofthelossfunction.Nonemeansthatnoadditionalabstractlayersareused.FewComboforResNet-Tiny,hasaCorrelateMax32layerbeforetheﬁrstlayer,aDecorrelateMin8aftertheﬁrstlayer,aDecorrelateMin4aftertheﬁrstwideresidualblock,aDecorrelateAllafterthesecondwideresidualblock,andaCorrelateMax10beforethefullyconnectedlayers.ManyFixedforResNet-Tiny,hasaCorrelateMax32layerbeforetheﬁrstlayer,aCorrelateFixed16thenDecorrelateMin16aftertheﬁrstlayer,aCorrelateFixed8thenDecorrelateMin8aftertheﬁrstandsecondwideblocks,andaCorrelateFixed4thenDecorrelateMin4afterthethirdwideblock,andaDecorrelateAllafterthefourth.ComboforSkipNet-18,haspairsofCorrelateFixedkandDecorrelateMinb0.5kcwithk=20,10,5afterlayers3,4and5respectivelyandusesDeepLossafterthefourthlayerwithaweightscheduleofUntil(90,Lin(0,0.2,50,40),0).LargeComboforResNet-Large,hasaCorrelateFixed4thenDecorrelateMin4beforewideresidualblocks1,2,3,and4.Beforethewideresidualblock5,weplaceDecorrelateMin2.ItusesDeepLossafterblock2and5,withweightschedulesofUntil(1,0,Lin(0.5,0,50,3))andUntil(24,Lin(0,0.1,20,4),Lin(0.1,0,50)).AcompletedescriptionofeachnetworkwitheachabstractlayercombinationcanbefoundintheAppendix,alongwithatableshowingitsperformanceforeverytrainingscheme.6.ExperimentalResultsWenowdemonstratehowourtrainingschemesshowninTable2(anddiscussedearlier)canbeusedtotrainprovablyrobustnetworksofsizesanorderofmagnitudelargerthanpriorwork.Weadditionallyshowhowabstractlayerscanbeusedtofurtherpushtheenvelopeofprovablerobustness.Table4.ComparisonofabstractlayersonResNet-Tiny.Layerss/epochAcc%Attck%Ver%None13029.421.417.7FewCombo22029.021.919.6ManyFixed34528.921.419.2ComparingTrainingSchemesandAbstractLayersToevaluatewhichcombinationsoftrainingschemesandabstractlayersprovidethebestresults,weﬁrsttrainedResNet-Tinyusingfourtrainingschemesbothwithoutabstractlayers,andwiththeabstractlayersetupdescribedbyFewCombo.Wetrainedeachfor400epochs.ThecompleteresultsareincludedintheAppendix,hereweshowtheaccuracyandveriﬁedrobustnessinFigure5.Wecanobservethatusingabstractlayersimprovesbothprovablerobustnessandaccuracywhenamorecomplextrainingschemeisused,andthatbeneﬁtsexistforprovablerobustnessaswell.Whentheobjectiveisonlytomaximizeprovablerobustness,thetrainingschemes(thereareseveral)whichutilizeInSampandabstractlayers,areoptimal.Withoutabstractlayers,inclusionsamplingaloneappearstohaveabeneﬁtonaccuracywithoutsigniﬁcantdetrimenttoprovablerobustness.Tofurtherinvestigatetheeffectofabstractlayers,wecomparetheresultsoftrainingthreeconﬁgurationsofabstractlayersonResNet-Tiny.ThesecanbeseeninTable4,whichshowstheresultsonthetestsetaftertrainingwithAdv1ISLPAfor350epochs.Inthisexperiment,wecanseethatManyFixedactuallydoesnotperformaswellasFewComboinanymetric,whileforveriﬁedrobustnessbothnetworkswithabstractlayersoutperformthenetworkwithoutabstractlayers.WhileManyFixedcontainsmanymoreabstractlayersandusesmorecorrelation(thusmakingitsigniﬁcantlyslowertotrain),thelayersinFewCombohavebeenchosenmoreselectively.ManyFixcontainsmultipleiterationsofCorrelateFixkimmediatelybeforeDecorrelateMinkofdecreasingsize.WehypothesizethatplacingaCorrelateFiximmediatelybeforeDecorrelateMindiminishestheutilityofDecorrelateMin’sheuristic.AsuncorrelatederrorcoefﬁcientstendtoaccumulateandgrowwhilecorrelatedAProvableDefenseforDeepResidualNetworksBaselineInSampInSampLPAAdv3InSampLPA28%30%32%34%32.933.630.128.232.73031.128.4(a)TestAccuracyNoAbstractLayersAbstractLayersBaselineInSampInSampLPAAdv3InSampLPA19%20%21%19.619.319.219.219.220.320.720.4(b)VeriﬁedRobustnessFigure5.AtoycomparisonofAccuracy(a)andVeriﬁedRobustness(b)oftrainingschemeswithsimilarparameterstoBaselineonResNet-Tinywithandwithoutabstractlayers.Table5.ComparisonoftraininginstancesonSkipNet-18.SchemeLayerss/epochAcc%Attck%Ver%Baseline18None15210.2--InSamp18None10228.523.420.5Adv5IS18None26028.423.821.2InSamp18Combo34229.52318.5errortermstendtoshrink,asaddlepointisgeneratedwhereinthenetworkwouldneedtomaximizeerrorcoefﬁcients(andthusdecreasingaccuracy)forneuronsdecidedpreviouslytobeimportant(whichwillbecomecorrelated)andminimizeerrorcoefﬁcients(andthusincreasingaccuracy)forneuronsthatwillbedecidedtobeunimportantinordertokeepthecoefﬁcientsfromswitching.Insummary,FewComboismoreefﬁcientandmoreaccuratethanManyFixedandisakeyexampleforthenecessityofthe“programmingtoprove”methodology.ScalingtoSkipNet-18InordertobuildadefenseschemecapableoftrainingSkipNet-18wefounditnecessaryto,ataminimum,useInSamp18training.Table5demonstratestheresultoftrainingSkipNet-18withavarietyoftrainingschemesfor400epochs2.OnecanobservethatBaselinedivergedandwhileInSamp18withoutabstractlayersorDeepLosswasabletotrainaSkipNet-18modelwithhighestprovablerobustness,thehighestaccuracywasobtainedusingthesametrainingschemeandComboabstractlayers.ThetrainingschemeAdv5IS18achievedabettercompromisebetweenprovablerobustnessandaccuracy.LargerAndMoreComplexArchitecturesWhilethemajorityofourcomparisonswereperformedonResNet-TinyandSkipNet-18weuseourschemestoscaletosigniﬁcantlylargernetworks.Toshowthis,wedesignedalargerwideresidualnetwork,ResNet-Large,with70kmoreneuronsthanSkipNet-18and66millionparameters(more2Baselinewasstoppedearlyat350epochsasithadclearlyfailedtotrain.thanfourtimesasmanyasSkipNet-18).Here,wefounditnecessarytouseacombinationofpreviouslyevaluatedtechniques,inadditiontotwoDeepLosslayers.ForthisnetworkweusedtheBiAdvLtrainingscheme,whichconstructsabstractboxesfromadversarialattacks.Astrainingthisnetworkwassigniﬁcantlymoreexpensive,taking527secondsperepoch,wehaltedtrainingafter100epochs.TheresultsforthisnetworkcanbeseeninTable3.Whileneithertheaccuracynorveriﬁablerobustnessareparticularlycompetitivewithsmallernetworks,thisisthedeepestnetwork(byshortestpathfrominputtooutput)tohaveprovedrobustforacompetitiveepsilonvalueandthatalsocomeswithnon-trivialaccuracy.WhileResNet-34andDenseNet-100havelongerpathsfrominputtooutput,theyalsohaveveryshortpathswhichmeansthattheycouldpotentiallylearnasmallandprovablyrobustnetworkﬁrstasaneasiersub-problem.Ondeepernetworksandlargernetworksthathavemoreresidualconnections,wefoundthatabstractlayerswerenotasnecessaryfortraining.Here,wehypothesizethatthenetworkcanprovablylearnthesmallernetworkwithouttheresiduallayersﬁrst,andthenusethemaspossiblewhentheydonottooseriouslyhurtorprovability.Table3alsoshowstheresultsoftrainingResNet-18,ResNet-34,andDenseNet-100.Thelargest,DenseNet-100is4.5timesthenumberofneuronstoappearinanyotherpaperatthetimeofthispublicationtohaveanon-trivialnumberofpointsveriﬁedtoberobust.7.ConclusionWeintroducedamethodfortrainingprovablyrobustnetworksbasedonthenovelconceptofabstractlayersandadomainspeciﬁclanguageforspecifyingcomplextrainingobjectives.Ourexperimentalevaluationdemonstratesthatourapproachiseffectiveintrainingprovablyrobustnetworksthatareanorderofmagnitudelargerthanthoseconsideredinpriorwork.AProvableDefenseforDeepResidualNetworksReferencesAkhtar,N.andMian,A.Threatofadversarialattacksondeeplearningincomputervision:Asurvey.arXivpreprintarXiv:1801.00553,2018.Athalye,A.andSutskever,I.Synthesizingrobustadversarialexamples.arXivpreprintarXiv:1707.07397,2017.Bhagoji,A.N.,Cullina,D.,andMittal,P.Dimensionalityreductionasadefenseagainstevasionattacksonmachinelearningclassiﬁers.arXivpreprint,2017.B¨ohme,S.,Leino,K.R.M.,andWolff,B.Hol-boogie—aninteractiveproverfortheboogieprogram-veriﬁer.InTheoremProvinginHigherOrderLogics,pp.150–166,2008.Carlini,N.andWagner,D.A.Adversarialexamplesarenoteasilydetected:Bypassingtendetectionmethods.CoRR,abs/1705.07263,2017.URLhttp://arxiv.org/abs/1705.07263.Cisse,M.,Bojanowski,P.,Grave,E.,Dauphin,Y.,andUsunier,N.Parsevalnetworks:Improvingrobustnesstoadversarialexamples.InInternationalConferenceonMachineLearning,pp.854–863,2017.Cousot,P.andCousot,R.Abstractinterpretation:auniﬁedlatticemodelforstaticanalysisofprogramsbyconstructionorapproximationofﬁxpoints.InSymposiumonPrinciplesofProgrammingLanguages(POPL),1977.Delahaye,D.Atacticlanguageforthesystemcoq.InInternationalConferenceonLogicforProgrammingArtiﬁcialIntelligenceandReasoning,pp.85–95.Springer,2000.Dong,Y.,Liao,F.,Pang,T.,Su,H.,Zhu,J.,Hu,X.,andLi,J.Boostingadversarialattackswithmomentum.InProceedingsoftheIEEEConferenceonComputerVisionandPatternRecognition,pp.9185–9193,2018.Dvijotham,K.,Gowal,S.,Stanforth,R.,Arandjelovic,R.,O’Donoghue,B.,Uesato,J.,andKohli,P.Trainingveriﬁedlearnerswithlearnedveriﬁers.arXivpreprintarXiv:1805.10265,2018.Evtimov,I.,Eykholt,K.,Fernandes,E.,Kohno,T.,Li,B.,Prakash,A.,Rahmati,A.,andSong,D.Robustphysical-worldattacksondeeplearningmodels.arXivpreprintarXiv:1707.08945,2017.Feinman,R.,Curtin,R.R.,Shintre,S.,andGardner,A.B.Detectingadversarialsamplesfromartifacts.arXivpreprintarXiv:1703.00410,2017.Gehr,T.,Mirman,M.,Tsankov,P.,DrachslerCohen,D.,Vechev,M.,andChaudhuri,S.Ai2:Safetyandrobustnesscertiﬁcationofneuralnetworkswithabstractinterpretation.InSymposiumonSecurityandPrivacy(SP),2018.Ghorbal,K.,Goubault,E.,andPutot,S.Thezonotopeabstractdomaintaylor1+.InInternationalConferenceonComputerAidedVeriﬁcation(CAV),2009.Goodfellow,I.J.,Shlens,J.,andSzegedy,C.Explainingandharnessingadversarialexamples.arXivpreprintarXiv:1412.6572,2014.Goubault,E.andPutot,S.Perturbedafﬁnearithmeticforinvariantcomputationinnumericalprogramanalysis.arXivpreprintarXiv:0807.2961,2008.Gowal,S.,Dvijotham,K.,Stanforth,R.,Bunel,R.,Qin,C.,Uesato,J.,Mann,T.,andKohli,P.Ontheeffectivenessofintervalboundpropagationfortrainingveriﬁablyrobustmodels.arXivpreprintarXiv:1810.12715,2018.Grosse,K.,Manoharan,P.,Papernot,N.,Backes,M.,andMcDaniel,P.Onthe(statistical)detectionofadversarialexamples.arXivpreprintarXiv:1702.06280,2017.Gu,S.andRigazio,L.Towardsdeepneuralnetworkarchitecturesrobusttoadversarialexamples.arXivpreprintarXiv:1412.5068,2014.He,K.,Zhang,X.,Ren,S.,andSun,J.Deepresiduallearningforimagerecognition.InProceedingsoftheIEEEconferenceoncomputervisionandpatternrecognition,pp.770–778,2016.Huang,G.,Liu,Z.,vanderMaaten,L.,andWeinberger,K.Q.Denselyconnectedconvolutionalnetworks.InProceedingsoftheIEEEConferenceonComputerVisionandPatternRecognition,2017.Huang,R.,Xu,B.,Schuurmans,D.,andSzepesv´ari,C.Learningwithastrongadversary.arXivpreprintarXiv:1511.03034,2015.Katz,G.,Barrett,C.,Dill,D.L.,Julian,K.,andKochenderfer,M.J.Reluplex:Anefﬁcientsmtsolverforverifyingdeepneuralnetworks.InInternationalConferenceonComputerAidedVeriﬁcation,2017.Khoury,M.andHadﬁeld-Menell,D.Onthegeometryofadversarialexamples.arXivpreprintarXiv:1811.00525,2018.Kingma,D.P.andBa,J.Adam:Amethodforstochasticoptimization.arXivpreprintarXiv:1412.6980,2014.AProvableDefenseforDeepResidualNetworksKolter,J.Z.andWong,E.Provabledefensesagainstadversarialexamplesviatheconvexouteradversarialpolytope.arXivpreprintarXiv:1711.00851,2017.Krizhevsky,A.Learningmultiplelayersoffeaturesfromtinyimages.2009.Krizhevsky,A.,Sutskever,I.,andHinton,G.E.Imagenetclassiﬁcationwithdeepconvolutionalneuralnetworks.InAdvancesinneuralinformationprocessingsystems,pp.1097–1105,2012.Madry,A.,Makelov,A.,Schmidt,L.,Tsipras,D.,andVladu,A.Towardsdeeplearningmodelsresistanttoadversarialattacks.2018.Mirman,M.,Gehr,T.,andVechev,M.Differentiableabstractinterpretationforprovablyrobustneuralnetworks.InInternationalConferenceonMachineLearning(ICML),2018.Moosavi-Dezfooli,S.-M.,Fawzi,A.,Fawzi,O.,andFrossard,P.Universaladversarialperturbations.In2017IEEEConferenceonComputerVisionandPatternRecognition(CVPR),pp.86–94.Ieee,2017.Papernot,N.,McDaniel,P.,Jha,S.,Fredrikson,M.,Celik,Z.B.,andSwami,A.Thelimitationsofdeeplearninginadversarialsettings.InSecurityandPrivacy(EuroS&P),2016IEEEEuropeanSymposiumon,pp.372–387.IEEE,2016.Raghunathan,A.,Steinhardt,J.,andLiang,P.Certiﬁeddefensesagainstadversarialexamples.arXivpreprintarXiv:1801.09344,2018.Rozsa,A.,Rudd,E.M.,andBoult,T.E.Adversarialdiversityandhardpositivegeneration.InProceedingsoftheIEEEConferenceonComputerVisionandPatternRecognitionWorkshops,pp.25–32,2016.Sabour,S.,Frosst,N.,andHinton,G.E.Dynamicroutingbetweencapsules.InAdvancesinNeuralInformationProcessingSystems,pp.3856–3866,2017.Schmidt,L.,Santurkar,S.,Tsipras,D.,Talwar,K.,andMadry,A.Adversariallyrobustgeneralizationrequiresmoredata.arXivpreprintarXiv:1804.11285,2018.Szegedy,C.,Zaremba,W.,Sutskever,I.,Bruna,J.,Erhan,D.,Goodfellow,I.J.,andFergus,R.Intriguingpropertiesofneuralnetworks.arXivpreprintarXiv:1312.6199,2013.Tram`er,F.,Kurakin,A.,Papernot,N.,Goodfellow,I.,Boneh,D.,andMcDaniel,P.Ensembleadversarialtraining:Attacksanddefenses.arXivpreprintarXiv:1705.07204,2017.Wagner,J.,Kuznetsov,V.,andCandea,G.-overify:Optimizingprogramsforfastveriﬁcation.InPresentedaspartofthe14thWorkshoponHotTopicsinOperatingSystems,SantaAnaPueblo,NM,2013.USENIX.Wang,S.,Chen,Y.,Abdou,A.,andJana,S.Mixtrain:Scalabletrainingofformallyrobustneuralnetworks.arXivpreprintarXiv:1811.02625,2018.Wong,E.,Schmidt,F.,Metzen,J.H.,andKolter,J.Z.Scalingprovableadversarialdefenses.arXivpreprintarXiv:1805.12514,2018.Xiao,C.,Li,B.,Zhu,J.-Y.,He,W.,Liu,M.,andSong,D.Generatingadversarialexampleswithadversarialnetworks.arXivpreprintarXiv:1801.02610,2018.Yuan,X.,He,P.,Zhu,Q.,Bhat,R.R.,andLi,X.Adversarialexamples:Attacksanddefensesfordeeplearning.arXivpreprintarXiv:1712.07107,2017.Zheng,S.,Song,Y.,Leung,T.,andGoodfellow,I.Improvingtherobustnessofdeepneuralnetworksviastabilitytraining.InProceedingsoftheieeeconferenceoncomputervisionandpatternrecognition,pp.4480–4488,2016.AProvableDefenseforDeepResidualNetworksA.EvaluationTable6.Thenetworkscomparedandtheirsizesandspeedsunderdifferenttrainingschemes.NetworkNameNeuronsParametersAbstractLayersTrainingSchemeBatchSizeSecondsPerEpochResNet-Tiny31179618415231NoneBaseline50106InSamp50106InSampLPA50107Adv1ISLPA50161Adv3ISLPA50130FewComboBaseline50209InSamp50205InSampLPA50206Adv1ISLPA50220Adv3ISLPA50265ManyFixedAdv1ISLPA50347SkipNet-1855808015626634NoneBaseline18200152InSamp18200102Adv5IS18200260ComboInSamp18100342ResNet-Large63997665819474LargeComboBiAdvL50527ResNet-Large558k18mResNet-18Adv5ISLPAR18200233ResNet-Large967k25mResNet-34InSampLPAR34200176Table7.ComparisonofdifferentabstractlayersandtrainingschemesonResNet-Tiny.TrainSchemeAbstractLayersSecondsPerEpochStandardAccuracy%AttackedAccuracy%VeriﬁedRobust%BaselineNone10532.923.719.6FewCombo20932.724.119.2InSampNone10633.624.719.3FewCombo2053023.220.3InSampLPANone10730.122.519.2FewCombo20631.12320.7Adv3ISLPANone16128.222.219.2FewCombo26728.422.520.4AProvableDefenseforDeepResidualNetworksB.FurtherAccuracyResultsforMNISTTable8.NetworksforMNIST.NetworkNameNeuronsParametersDepth(ReLUs)FFNN5001199105ConvSmall3604896063ConvMed48041664063ConvBig4806419747626ConvLargeIBP17581654264026TruncatedVGG151040131097065Table9.MNISTwith0.1NetworkStandardAccuracyPGDAccuracyHBoxProvabilityFFNN93.3%90.8%88.9%ConvSmall97.8%96.2%95.5%ConvMed97.8%96.3%95.5%ConvBig98.5%97.2%95.6%ConvLargeIBP98.7%97.5%95.8%TruncatedVGG98.9%97.7%95.6%Table10.MNISTwith(cid:15)=0.3.NetworkStandardAccuracyPGDAccuracyHBoxProvabilityFFNN80.2%73.4%62.6%ConvSmall96.9%93.6%89.1%ConvMed96.6%93.1%89.3%ConvBig97.0%95.2%87.8%ConvLargeIBP97.2%95.4%88.8%TruncatedVGG96.5%94.4%87.6%AProvableDefenseforDeepResidualNetworksC.FurtherAccuracyResultsforCIFAR10Table11.NetworksforCIFAR10.NetworkNameNeuronsParametersDepth(ReLUs)FFNN5003487105ConvSmall48521253183ConvMed62442149183ConvBig6246424668586ConvLargeIBP22957669635546TruncatedVGG197120170430185Table12.CIFAR10with(cid:15)=3/255.NetworkStandardAccuracyPGDAccuracyHBoxProvabilityFFNN45.1%37.0%33.1%ConvSmall56.1%46.2%42.4%ConvMed56.9%46.6%43.2%ConvBig61.9%51.4%45.0%ConvLargeIBP61.1%51.4%44.5%TruncatedVGG62.3%51.4%45.5%Table13.CIFAR10with(cid:15)=8/255.NetworkStandardAccuracyPGDAccuracyHBoxProvabilityFFNN33.5%23.8%19.0%ConvSmall42.6%30.5%24.9%ConvMed43.6%30.3%24.7%ConvBig46.0%34.2%25.2%ConvLargeIBP46.2%34.7%27.2%TruncatedVGG45.9%34.4%27.0%AProvableDefenseforDeepResidualNetworksD.NetworksandAbstractLayersFigure6.ResNet-Tiny,NoneNormalizemean=[0.4914,0.4822,0.4465]std=[0.2023,0.1994,0.201]Conv2D,filters=16,kernel_size=[3,3],input_shape=[32,32,3],stride=[1,1],padding=1ReLUParNet1Conv2D,filters=16,kernel_size=[1,1],input_shape=[32,32,16],stride=[1,1],padding=0ParNet2Conv2D,filters=16,kernel_size=[3,3],input_shape=[32,32,16],stride=[1,1],padding=1ReLUConv2D,filters=16,kernel_size=[3,3],input_shape=[32,32,16],stride=[1,1],padding=1ParSumReLUParNet1Conv2D,filters=32,kernel_size=[1,1],input_shape=[32,32,16],stride=[1,1],padding=0ParNet2Conv2D,filters=32,kernel_size=[3,3],input_shape=[32,32,16],stride=[1,1],padding=1ReLUConv2D,filters=32,kernel_size=[3,3],input_shape=[32,32,32],stride=[1,1],padding=1ParSumReLUParNet1Conv2D,filters=32,kernel_size=[1,1],input_shape=[32,32,32],stride=[1,1],padding=0ParNet2Conv2D,filters=32,kernel_size=[3,3],input_shape=[32,32,32],stride=[1,1],padding=1ReLUConv2D,filters=32,kernel_size=[3,3],input_shape=[32,32,32],stride=[1,1],padding=1ParSumReLUParNet1Conv2D,filters=32,kernel_size=[1,1],input_shape=[32,32,32],stride=[1,1],padding=0ParNet2Conv2D,filters=32,kernel_size=[3,3],input_shape=[32,32,32],stride=[1,1],padding=1ReLUConv2D,filters=32,kernel_size=[3,3],input_shape=[32,32,32],stride=[1,1],padding=1ParSumReLUParNet1Conv2D,filters=32,kernel_size=[1,1],input_shape=[32,32,32],stride=[1,1],padding=0ParNet2Conv2D,filters=32,kernel_size=[3,3],input_shape=[32,32,32],stride=[1,1],padding=1ReLUConv2D,filters=32,kernel_size=[3,3],input_shape=[32,32,32],stride=[1,1],padding=1ParSumReLULinearout=500ReLUCorrelateAllonly_train=FalseLinearout=10AProvableDefenseforDeepResidualNetworksFigure7.ResNet-Tiny,FewComboNormalizemean=[0.4914,0.4822,0.4465]std=[0.2023,0.1994,0.201]CorrMaxKonly_train=Truenum_correlate=32Conv2D,filters=16,kernel_size=[3,3],input_shape=[32,32,3],stride=[1,1],padding=1ReLUDecorrMinonly_train=Truek=8num_to_keep=TrueParNet1Conv2D,filters=16,kernel_size=[1,1],input_shape=[32,32,16],stride=[1,1],padding=0ParNet2Conv2D,filters=16,kernel_size=[3,3],input_shape=[32,32,16],stride=[1,1],padding=1ReLUConv2D,filters=16,kernel_size=[3,3],input_shape=[32,32,16],stride=[1,1],padding=1ParSumReLUDecorrMinonly_train=Truek=4num_to_keep=TrueParNet1Conv2D,filters=32,kernel_size=[1,1],input_shape=[32,32,16],stride=[1,1],padding=0ParNet2Conv2D,filters=32,kernel_size=[3,3],input_shape=[32,32,16],stride=[1,1],padding=1ReLUConv2D,filters=32,kernel_size=[3,3],input_shape=[32,32,32],stride=[1,1],padding=1ParSumReLUConcretizeonly_train=TrueParNet1Conv2D,filters=32,kernel_size=[1,1],input_shape=[32,32,32],stride=[1,1],padding=0ParNet2Conv2D,filters=32,kernel_size=[3,3],input_shape=[32,32,32],stride=[1,1],padding=1ReLUConv2D,filters=32,kernel_size=[3,3],input_shape=[32,32,32],stride=[1,1],padding=1ParSumReLUParNet1Conv2D,filters=32,kernel_size=[1,1],input_shape=[32,32,32],stride=[1,1],padding=0ParNet2Conv2D,filters=32,kernel_size=[3,3],input_shape=[32,32,32],stride=[1,1],padding=1ReLUConv2D,filters=32,kernel_size=[3,3],input_shape=[32,32,32],stride=[1,1],padding=1ParSumReLUParNet1Conv2D,filters=32,kernel_size=[1,1],input_shape=[32,32,32],stride=[1,1],padding=0ParNet2Conv2D,filters=32,kernel_size=[3,3],input_shape=[32,32,32],stride=[1,1],padding=1ReLUConv2D,filters=32,kernel_size=[3,3],input_shape=[32,32,32],stride=[1,1],padding=1ParSumReLUCorrMaxKonly_train=Truenum_correlate=10Linearout=500ReLUCorrelateAllonly_train=FalseLinearout=10AProvableDefenseforDeepResidualNetworksFigure8.ResNet-Tiny,ManyFixedNormalizemean=[0.4914,0.4822,0.4465]std=[0.2023,0.1994,0.201]CorrMaxKonly_train=Truenum_correlate=32Conv2D,filters=16,kernel_size=[3,3],input_shape=[32,32,3],stride=[1,1],padding=1ReLUCorrFixonly_train=Truek=16DecorrMinonly_train=Truek=16num_to_keep=TrueParNet1Conv2D,filters=16,kernel_size=[1,1],input_shape=[32,32,16],stride=[1,1],padding=0ParNet2Conv2D,filters=16,kernel_size=[3,3],input_shape=[32,32,16],stride=[1,1],padding=1ReLUConv2D,filters=16,kernel_size=[3,3],input_shape=[32,32,16],stride=[1,1],padding=1ParSumReLUCorrFixonly_train=Truek=8DecorrMinonly_train=Truek=8num_to_keep=TrueParNet1Conv2D,filters=32,kernel_size=[1,1],input_shape=[32,32,16],stride=[1,1],padding=0ParNet2Conv2D,filters=32,kernel_size=[3,3],input_shape=[32,32,16],stride=[1,1],padding=1ReLUConv2D,filters=32,kernel_size=[3,3],input_shape=[32,32,32],stride=[1,1],padding=1ParSumReLUCorrFixonly_train=Truek=8DecorrMinonly_train=Truek=8num_to_keep=TrueParNet1Conv2D,filters=32,kernel_size=[1,1],input_shape=[32,32,32],stride=[1,1],padding=0ParNet2Conv2D,filters=32,kernel_size=[3,3],input_shape=[32,32,32],stride=[1,1],padding=1ReLUConv2D,filters=32,kernel_size=[3,3],input_shape=[32,32,32],stride=[1,1],padding=1ParSumReLUCorrFixonly_train=Truek=4DecorrMinonly_train=Truek=4num_to_keep=TrueParNet1Conv2D,filters=32,kernel_size=[1,1],input_shape=[32,32,32],stride=[1,1],padding=0ParNet2Conv2D,filters=32,kernel_size=[3,3],input_shape=[32,32,32],stride=[1,1],padding=1ReLUConv2D,filters=32,kernel_size=[3,3],input_shape=[32,32,32],stride=[1,1],padding=1ParSumReLUConcretizeonly_train=TrueParNet1Conv2D,filters=32,kernel_size=[1,1],input_shape=[32,32,32],stride=[1,1],padding=0ParNet2Conv2D,filters=32,kernel_size=[3,3],input_shape=[32,32,32],stride=[1,1],padding=1ReLUConv2D,filters=32,kernel_size=[3,3],input_shape=[32,32,32],stride=[1,1],padding=1ParSumReLULinearout=500ReLUCorrelateAllonly_train=FalseLinearout=10AProvableDefenseforDeepResidualNetworksFigure9.SkipNet-18,NoneNormalizemean=[0.4914,0.4822,0.4465]std=[0.2023,0.1994,0.201]Conv2D,filters=64,kernel_size=[3,3],input_shape=[32,32,3],stride=[1,1],padding=1ReLUConv2D,filters=64,kernel_size=[3,3],input_shape=[32,32,64],stride=[1,1],padding=1ReLUConv2D,filters=64,kernel_size=[3,3],input_shape=[32,32,64],stride=[1,1],padding=1ReLUConv2D,filters=64,kernel_size=[3,3],input_shape=[32,32,64],stride=[1,1],padding=1ReLUConv2D,filters=64,kernel_size=[3,3],input_shape=[32,32,64],stride=[1,1],padding=1ReLUParNet1Conv2D,filters=128,kernel_size=[3,3],input_shape=[32,32,64],stride=[2,2],padding=1ReLUConv2D,filters=128,kernel_size=[3,3],input_shape=[16,16,128],stride=[1,1],padding=1ParNet2Conv2D,filters=128,kernel_size=[1,1],input_shape=[32,32,64],stride=[2,2],padding=0ParSumReLUConv2D,filters=128,kernel_size=[3,3],input_shape=[16,16,128],stride=[1,1],padding=1ReLUConv2D,filters=128,kernel_size=[3,3],input_shape=[16,16,128],stride=[1,1],padding=1ReLUParNet1Conv2D,filters=256,kernel_size=[3,3],input_shape=[16,16,128],stride=[2,2],padding=1ReLUConv2D,filters=256,kernel_size=[3,3],input_shape=[8,8,256],stride=[1,1],padding=1ParNet2Conv2D,filters=256,kernel_size=[1,1],input_shape=[16,16,128],stride=[2,2],padding=0ParSumReLUConv2D,filters=256,kernel_size=[3,3],input_shape=[8,8,256],stride=[1,1],padding=1ReLUConv2D,filters=256,kernel_size=[3,3],input_shape=[8,8,256],stride=[1,1],padding=1ReLUParNet1Conv2D,filters=512,kernel_size=[3,3],input_shape=[8,8,256],stride=[2,2],padding=1ReLUConv2D,filters=512,kernel_size=[3,3],input_shape=[4,4,512],stride=[1,1],padding=1ParNet2Conv2D,filters=512,kernel_size=[1,1],input_shape=[8,8,256],stride=[2,2],padding=0ParSumReLUConv2D,filters=512,kernel_size=[3,3],input_shape=[4,4,512],stride=[1,1],padding=1ReLUConv2D,filters=512,kernel_size=[3,3],input_shape=[4,4,512],stride=[1,1],padding=1ReLULinearout=512ReLULinearout=512ReLUCorrelateAllonly_train=FalseLinearout=10AProvableDefenseforDeepResidualNetworksFigure10.SkipNet-18,ComboNormalizemean=[0.4914,0.4822,0.4465]std=[0.2023,0.1994,0.201]Conv2D,filters=64,kernel_size=[3,3],input_shape=[32,32,3],stride=[1,1],padding=1ReLUConv2D,filters=64,kernel_size=[3,3],input_shape=[32,32,64],stride=[1,1],padding=1ReLUConv2D,filters=64,kernel_size=[3,3],input_shape=[32,32,64],stride=[1,1],padding=1ReLUConv2D,filters=64,kernel_size=[3,3],input_shape=[32,32,64],stride=[1,1],padding=1ReLUConv2D,filters=64,kernel_size=[3,3],input_shape=[32,32,64],stride=[1,1],padding=1ReLUParNet1Conv2D,filters=128,kernel_size=[3,3],input_shape=[32,32,64],stride=[2,2],padding=1ReLUConv2D,filters=128,kernel_size=[3,3],input_shape=[16,16,128],stride=[1,1],padding=1ParNet2Conv2D,filters=128,kernel_size=[1,1],input_shape=[32,32,64],stride=[2,2],padding=0ParSumReLUCorrFixonly_train=Truek=20DecorrMinonly_train=Truek=10num_to_keep=TrueConv2D,filters=128,kernel_size=[3,3],input_shape=[16,16,128],stride=[1,1],padding=1ReLUConv2D,filters=128,kernel_size=[3,3],input_shape=[16,16,128],stride=[1,1],padding=1ReLUCorrFixonly_train=Truek=10DecorrMinonly_train=Truek=5num_to_keep=TrueDeepLossonly_train=Truebw=Until(90,Lin(%s,%s,%s,%s),0.0)act=<functionreluat0x10b94d620>ParNet1Conv2D,filters=256,kernel_size=[3,3],input_shape=[16,16,128],stride=[2,2],padding=1ReLUConv2D,filters=256,kernel_size=[3,3],input_shape=[8,8,256],stride=[1,1],padding=1ParNet2Conv2D,filters=256,kernel_size=[1,1],input_shape=[16,16,128],stride=[2,2],padding=0ParSumReLUCorrFixonly_train=Truek=5DecorrMinonly_train=Truek=2num_to_keep=TrueConv2D,filters=256,kernel_size=[3,3],input_shape=[8,8,256],stride=[1,1],padding=1ReLUConv2D,filters=256,kernel_size=[3,3],input_shape=[8,8,256],stride=[1,1],padding=1ReLUParNet1Conv2D,filters=512,kernel_size=[3,3],input_shape=[8,8,256],stride=[2,2],padding=1ReLUConv2D,filters=512,kernel_size=[3,3],input_shape=[4,4,512],stride=[1,1],padding=1ParNet2Conv2D,filters=512,kernel_size=[1,1],input_shape=[8,8,256],stride=[2,2],padding=0ParSumReLUConv2D,filters=512,kernel_size=[3,3],input_shape=[4,4,512],stride=[1,1],padding=1ReLUConv2D,filters=512,kernel_size=[3,3],input_shape=[4,4,512],stride=[1,1],padding=1ReLULinearout=512ReLULinearout=512ReLUCorrelateAllonly_train=FalseLinearout=10AProvableDefenseforDeepResidualNetworksFigure11.ResNet-Large,LargeComboNormalizemean=[0.4914,0.4822,0.4465]std=[0.2023,0.1994,0.201]Conv2D,filters=16,kernel_size=[3,3],input_shape=[32,32,3],stride=[1,1],padding=1ReLUCorrMaxKonly_train=Truenum_correlate=4ParNet1Conv2D,filters=16,kernel_size=[1,1],input_shape=[32,32,16],stride=[1,1],padding=0ParNet2Conv2D,filters=16,kernel_size=[3,3],input_shape=[32,32,16],stride=[1,1],padding=1ReLUConv2D,filters=16,kernel_size=[3,3],input_shape=[32,32,16],stride=[1,1],padding=1ParSumReLUCorrMaxKonly_train=Truenum_correlate=4DecorrMinonly_train=Truek=4num_to_keep=TrueParNet1Conv2D,filters=32,kernel_size=[1,1],input_shape=[32,32,16],stride=[1,1],padding=0ParNet2Conv2D,filters=32,kernel_size=[3,3],input_shape=[32,32,16],stride=[1,1],padding=1ReLUConv2D,filters=32,kernel_size=[3,3],input_shape=[32,32,32],stride=[1,1],padding=1ParSumReLUCorrMaxKonly_train=Truenum_correlate=4DecorrMinonly_train=Truek=4num_to_keep=TrueParNet1Conv2D,filters=32,kernel_size=[1,1],input_shape=[32,32,32],stride=[1,1],padding=0ParNet2Conv2D,filters=32,kernel_size=[3,3],input_shape=[32,32,32],stride=[1,1],padding=1ReLUConv2D,filters=32,kernel_size=[3,3],input_shape=[32,32,32],stride=[1,1],padding=1ParSumReLUDeepLossonly_train=Truebw=Until(1,0.0,Lin(0.5,0,50,3))ParNet1Conv2D,filters=32,kernel_size=[1,1],input_shape=[32,32,32],stride=[1,1],padding=0ParNet2Conv2D,filters=32,kernel_size=[3,3],input_shape=[32,32,32],stride=[1,1],padding=1ReLUConv2D,filters=32,kernel_size=[3,3],input_shape=[32,32,32],stride=[1,1],padding=1ParSumReLUCorrMaxKonly_train=Truenum_correlate=4DecorrMinonly_train=Truek=4num_to_keep=TrueParNet1Conv2D,filters=64,kernel_size=[1,1],input_shape=[32,32,32],stride=[1,1],padding=0ParNet2Conv2D,filters=64,kernel_size=[3,3],input_shape=[32,32,32],stride=[1,1],padding=1ReLUConv2D,filters=64,kernel_size=[3,3],input_shape=[32,32,64],stride=[1,1],padding=1ParSumReLUCorrMaxKonly_train=Truenum_correlate=4DecorrMinonly_train=Truek=2num_to_keep=TrueParNet1Conv2D,filters=64,kernel_size=[1,1],input_shape=[32,32,64],stride=[1,1],padding=0ParNet2Conv2D,filters=64,kernel_size=[3,3],input_shape=[32,32,64],stride=[1,1],padding=1ReLUConv2D,filters=64,kernel_size=[3,3],input_shape=[32,32,64],stride=[1,1],padding=1ParSumReLUDeepLossonly_train=Truebw=Until(24,Lin(0,0.1,20,4),Lin(0.1,0,50,0))ParNet1Conv2D,filters=64,kernel_size=[1,1],input_shape=[32,32,64],stride=[1,1],padding=0ParNet2Conv2D,filters=64,kernel_size=[3,3],input_shape=[32,32,64],stride=[1,1],padding=1ReLUConv2D,filters=64,kernel_size=[3,3],input_shape=[32,32,64],stride=[1,1],padding=1ParSumReLULinearout=1000ReLUCorrelateAllonly_train=FalseLinearout=10AProvableDefenseforDeepResidualNetworksAProvableDefenseforDeepResidualNetworksFigure12.ResNet-18,NoneNormalizemean=[0.4914,0.4822,0.4465]std=[0.2023,0.1994,0.201]Conv2D,filters=64,kernel_size=[3,3],input_shape=[32,32,3],stride=[1,1],padding=1ReLUParNet1Conv2D,filters=64,kernel_size=[3,3],input_shape=[32,32,64],stride=[1,1],padding=1ReLUConv2D,filters=64,kernel_size=[3,3],input_shape=[32,32,64],stride=[1,1],padding=1ParNet2ParSumReLUParNet1Conv2D,filters=64,kernel_size=[3,3],input_shape=[32,32,64],stride=[1,1],padding=1ReLUConv2D,filters=64,kernel_size=[3,3],input_shape=[32,32,64],stride=[1,1],padding=1ParNet2ParSumReLUParNet1Conv2D,filters=128,kernel_size=[3,3],input_shape=[32,32,64],stride=[2,2],padding=1ReLUConv2D,filters=128,kernel_size=[3,3],input_shape=[16,16,128],stride=[1,1],padding=1ParNet2Conv2D,filters=128,kernel_size=[1,1],input_shape=[32,32,64],stride=[2,2],padding=0ParSumReLUParNet1Conv2D,filters=128,kernel_size=[3,3],input_shape=[16,16,128],stride=[1,1],padding=1ReLUConv2D,filters=128,kernel_size=[3,3],input_shape=[16,16,128],stride=[1,1],padding=1ParNet2ParSumReLUParNet1Conv2D,filters=256,kernel_size=[3,3],input_shape=[16,16,128],stride=[2,2],padding=1ReLUConv2D,filters=256,kernel_size=[3,3],input_shape=[8,8,256],stride=[1,1],padding=1ParNet2Conv2D,filters=256,kernel_size=[1,1],input_shape=[16,16,128],stride=[2,2],padding=0ParSumReLUParNet1Conv2D,filters=256,kernel_size=[3,3],input_shape=[8,8,256],stride=[1,1],padding=1ReLUConv2D,filters=256,kernel_size=[3,3],input_shape=[8,8,256],stride=[1,1],padding=1ParNet2ParSumReLUParNet1Conv2D,filters=512,kernel_size=[3,3],input_shape=[8,8,256],stride=[2,2],padding=1ReLUConv2D,filters=512,kernel_size=[3,3],input_shape=[4,4,512],stride=[1,1],padding=1ParNet2Conv2D,filters=512,kernel_size=[1,1],input_shape=[8,8,256],stride=[2,2],padding=0ParSumReLUParNet1Conv2D,filters=512,kernel_size=[3,3],input_shape=[4,4,512],stride=[1,1],padding=1ReLUConv2D,filters=512,kernel_size=[3,3],input_shape=[4,4,512],stride=[1,1],padding=1ParNet2ParSumReLULinearout=512ReLULinearout=512ReLUCorrelateAllonly_train=FalseLinearout=10AProvableDefenseforDeepResidualNetworksFigure13.ResNet-34,NoneNormalizemean=[0.4914,0.4822,0.4465]std=[0.2023,0.1994,0.201]Conv2D,filters=64,kernel_size=[3,3],input_shape=[32,32,3],stride=[1,1],padding=1ReLUParNet1Conv2D,filters=64,kernel_size=[3,3],input_shape=[32,32,64],stride=[1,1],padding=1ReLUConv2D,filters=64,kernel_size=[3,3],input_shape=[32,32,64],stride=[1,1],padding=1ParNet2ParSumReLUParNet1Conv2D,filters=64,kernel_size=[3,3],input_shape=[32,32,64],stride=[1,1],padding=1ReLUConv2D,filters=64,kernel_size=[3,3],input_shape=[32,32,64],stride=[1,1],padding=1ParNet2ParSumReLUParNet1Conv2D,filters=64,kernel_size=[3,3],input_shape=[32,32,64],stride=[1,1],padding=1ReLUConv2D,filters=64,kernel_size=[3,3],input_shape=[32,32,64],stride=[1,1],padding=1ParNet2ParSumReLUParNet1Conv2D,filters=128,kernel_size=[3,3],input_shape=[32,32,64],stride=[2,2],padding=1ReLUConv2D,filters=128,kernel_size=[3,3],input_shape=[16,16,128],stride=[1,1],padding=1ParNet2Conv2D,filters=128,kernel_size=[1,1],input_shape=[32,32,64],stride=[2,2],padding=0ParSumReLUParNet1Conv2D,filters=128,kernel_size=[3,3],input_shape=[16,16,128],stride=[1,1],padding=1ReLUConv2D,filters=128,kernel_size=[3,3],input_shape=[16,16,128],stride=[1,1],padding=1ParNet2ParSumReLUParNet1Conv2D,filters=128,kernel_size=[3,3],input_shape=[16,16,128],stride=[1,1],padding=1ReLUConv2D,filters=128,kernel_size=[3,3],input_shape=[16,16,128],stride=[1,1],padding=1ParNet2ParSumReLUParNet1Conv2D,filters=128,kernel_size=[3,3],input_shape=[16,16,128],stride=[1,1],padding=1ReLUConv2D,filters=128,kernel_size=[3,3],input_shape=[16,16,128],stride=[1,1],padding=1ParNet2ParSumReLUParNet1Conv2D,filters=256,kernel_size=[3,3],input_shape=[16,16,128],stride=[2,2],padding=1ReLUConv2D,filters=256,kernel_size=[3,3],input_shape=[8,8,256],stride=[1,1],padding=1ParNet2Conv2D,filters=256,kernel_size=[1,1],input_shape=[16,16,128],stride=[2,2],padding=0ParSumReLUParNet1Conv2D,filters=256,kernel_size=[3,3],input_shape=[8,8,256],stride=[1,1],padding=1ReLUConv2D,filters=256,kernel_size=[3,3],input_shape=[8,8,256],stride=[1,1],padding=1ParNet2ParSumReLUParNet1Conv2D,filters=256,kernel_size=[3,3],input_shape=[8,8,256],stride=[1,1],padding=1ReLUConv2D,filters=256,kernel_size=[3,3],input_shape=[8,8,256],stride=[1,1],padding=1ParNet2ParSumReLUAProvableDefenseforDeepResidualNetworksParNet1Conv2D,filters=256,kernel_size=[3,3],input_shape=[8,8,256],stride=[1,1],padding=1ReLUConv2D,filters=256,kernel_size=[3,3],input_shape=[8,8,256],stride=[1,1],padding=1ParNet2ParSumReLUParNet1Conv2D,filters=256,kernel_size=[3,3],input_shape=[8,8,256],stride=[1,1],padding=1ReLUConv2D,filters=256,kernel_size=[3,3],input_shape=[8,8,256],stride=[1,1],padding=1ParNet2ParSumReLUParNet1Conv2D,filters=256,kernel_size=[3,3],input_shape=[8,8,256],stride=[1,1],padding=1ReLUConv2D,filters=256,kernel_size=[3,3],input_shape=[8,8,256],stride=[1,1],padding=1ParNet2ParSumReLUParNet1Conv2D,filters=512,kernel_size=[3,3],input_shape=[8,8,256],stride=[2,2],padding=1ReLUConv2D,filters=512,kernel_size=[3,3],input_shape=[4,4,512],stride=[1,1],padding=1ParNet2Conv2D,filters=512,kernel_size=[1,1],input_shape=[8,8,256],stride=[2,2],padding=0ParSumReLUParNet1Conv2D,filters=512,kernel_size=[3,3],input_shape=[4,4,512],stride=[1,1],padding=1ReLUConv2D,filters=512,kernel_size=[3,3],input_shape=[4,4,512],stride=[1,1],padding=1ParNet2ParSumReLUParNet1Conv2D,filters=512,kernel_size=[3,3],input_shape=[4,4,512],stride=[1,1],padding=1ReLUConv2D,filters=512,kernel_size=[3,3],input_shape=[4,4,512],stride=[1,1],padding=1ParNet2ParSumReLULinearout=512ReLULinearout=512ReLUCorrelateAllonly_train=FalseLinearout=10AProvableDefenseforDeepResidualNetworksFigure14.DenseNet-100,NoneNormalizemean=[0.4914,0.4822,0.4465]std=[0.2023,0.1994,0.201]Conv2D,filters=24,kernel_size=[3,3],input_shape=[32,32,3],stride=[1,1],padding=1SkipNet1SkipNet2ReLUConv2D,filters=48,kernel_size=[1,1],input_shape=[32,32,24],stride=[1,1],padding=0ReLUConv2D,filters=12,kernel_size=[3,3],input_shape=[32,32,48],stride=[1,1],padding=1SkipCatdim=1SkipNet1SkipNet2ReLUConv2D,filters=48,kernel_size=[1,1],input_shape=[32,32,36],stride=[1,1],padding=0ReLUConv2D,filters=12,kernel_size=[3,3],input_shape=[32,32,48],stride=[1,1],padding=1SkipCatdim=1SkipNet1SkipNet2ReLUConv2D,filters=48,kernel_size=[1,1],input_shape=[32,32,48],stride=[1,1],padding=0ReLUConv2D,filters=12,kernel_size=[3,3],input_shape=[32,32,48],stride=[1,1],padding=1SkipCatdim=1SkipNet1SkipNet2ReLUConv2D,filters=48,kernel_size=[1,1],input_shape=[32,32,60],stride=[1,1],padding=0ReLUConv2D,filters=12,kernel_size=[3,3],input_shape=[32,32,48],stride=[1,1],padding=1SkipCatdim=1SkipNet1SkipNet2ReLUConv2D,filters=48,kernel_size=[1,1],input_shape=[32,32,72],stride=[1,1],padding=0ReLUConv2D,filters=12,kernel_size=[3,3],input_shape=[32,32,48],stride=[1,1],padding=1SkipCatdim=1SkipNet1SkipNet2ReLUConv2D,filters=48,kernel_size=[1,1],input_shape=[32,32,84],stride=[1,1],padding=0ReLUConv2D,filters=12,kernel_size=[3,3],input_shape=[32,32,48],stride=[1,1],padding=1SkipCatdim=1SkipNet1SkipNet2ReLUConv2D,filters=48,kernel_size=[1,1],input_shape=[32,32,96],stride=[1,1],padding=0ReLUConv2D,filters=12,kernel_size=[3,3],input_shape=[32,32,48],stride=[1,1],padding=1SkipCatdim=1SkipNet1SkipNet2ReLUConv2D,filters=48,kernel_size=[1,1],input_shape=[32,32,108],stride=[1,1],padding=0ReLUConv2D,filters=12,kernel_size=[3,3],input_shape=[32,32,48],stride=[1,1],padding=1SkipCatdim=1SkipNet1SkipNet2ReLUConv2D,filters=48,kernel_size=[1,1],input_shape=[32,32,120],stride=[1,1],padding=0ReLUConv2D,filters=12,kernel_size=[3,3],input_shape=[32,32,48],stride=[1,1],padding=1SkipCatdim=1SkipNet1SkipNet2ReLUConv2D,filters=48,kernel_size=[1,1],input_shape=[32,32,132],stride=[1,1],padding=0ReLUConv2D,filters=12,kernel_size=[3,3],input_shape=[32,32,48],stride=[1,1],padding=1SkipCatdim=1SkipNet1SkipNet2ReLUAProvableDefenseforDeepResidualNetworksConv2D,filters=48,kernel_size=[1,1],input_shape=[32,32,144],stride=[1,1],padding=0ReLUConv2D,filters=12,kernel_size=[3,3],input_shape=[32,32,48],stride=[1,1],padding=1SkipCatdim=1SkipNet1SkipNet2ReLUConv2D,filters=48,kernel_size=[1,1],input_shape=[32,32,156],stride=[1,1],padding=0ReLUConv2D,filters=12,kernel_size=[3,3],input_shape=[32,32,48],stride=[1,1],padding=1SkipCatdim=1SkipNet1SkipNet2ReLUConv2D,filters=48,kernel_size=[1,1],input_shape=[32,32,168],stride=[1,1],padding=0ReLUConv2D,filters=12,kernel_size=[3,3],input_shape=[32,32,48],stride=[1,1],padding=1SkipCatdim=1SkipNet1SkipNet2ReLUConv2D,filters=48,kernel_size=[1,1],input_shape=[32,32,180],stride=[1,1],padding=0ReLUConv2D,filters=12,kernel_size=[3,3],input_shape=[32,32,48],stride=[1,1],padding=1SkipCatdim=1SkipNet1SkipNet2ReLUConv2D,filters=48,kernel_size=[1,1],input_shape=[32,32,192],stride=[1,1],padding=0ReLUConv2D,filters=12,kernel_size=[3,3],input_shape=[32,32,48],stride=[1,1],padding=1SkipCatdim=1SkipNet1SkipNet2ReLUConv2D,filters=48,kernel_size=[1,1],input_shape=[32,32,204],stride=[1,1],padding=0ReLUConv2D,filters=12,kernel_size=[3,3],input_shape=[32,32,48],stride=[1,1],padding=1SkipCatdim=1ReLUConv2D,filters=108,kernel_size=[1,1],input_shape=[32,32,216],stride=[1,1],padding=0AvgPool2DSkipNet1SkipNet2ReLUConv2D,filters=48,kernel_size=[1,1],input_shape=[17,17,108],stride=[1,1],padding=0ReLUConv2D,filters=12,kernel_size=[3,3],input_shape=[17,17,48],stride=[1,1],padding=1SkipCatdim=1SkipNet1SkipNet2ReLUConv2D,filters=48,kernel_size=[1,1],input_shape=[17,17,120],stride=[1,1],padding=0ReLUConv2D,filters=12,kernel_size=[3,3],input_shape=[17,17,48],stride=[1,1],padding=1SkipCatdim=1SkipNet1SkipNet2ReLUConv2D,filters=48,kernel_size=[1,1],input_shape=[17,17,132],stride=[1,1],padding=0ReLUConv2D,filters=12,kernel_size=[3,3],input_shape=[17,17,48],stride=[1,1],padding=1SkipCatdim=1SkipNet1SkipNet2ReLUConv2D,filters=48,kernel_size=[1,1],input_shape=[17,17,144],stride=[1,1],padding=0ReLUConv2D,filters=12,kernel_size=[3,3],input_shape=[17,17,48],stride=[1,1],padding=1SkipCatdim=1SkipNet1SkipNet2ReLUConv2D,filters=48,kernel_size=[1,1],input_shape=[17,17,156],stride=[1,1],padding=0ReLUConv2D,filters=12,kernel_size=[3,3],input_shape=[17,17,48],stride=[1,1],padding=1SkipCatdim=1SkipNet1SkipNet2ReLUConv2D,filters=48,kernel_size=[1,1],input_shape=[17,17,168],stride=[1,1],padding=0AProvableDefenseforDeepResidualNetworksReLUConv2D,filters=12,kernel_size=[3,3],input_shape=[17,17,48],stride=[1,1],padding=1SkipCatdim=1SkipNet1SkipNet2ReLUConv2D,filters=48,kernel_size=[1,1],input_shape=[17,17,180],stride=[1,1],padding=0ReLUConv2D,filters=12,kernel_size=[3,3],input_shape=[17,17,48],stride=[1,1],padding=1SkipCatdim=1SkipNet1SkipNet2ReLUConv2D,filters=48,kernel_size=[1,1],input_shape=[17,17,192],stride=[1,1],padding=0ReLUConv2D,filters=12,kernel_size=[3,3],input_shape=[17,17,48],stride=[1,1],padding=1SkipCatdim=1SkipNet1SkipNet2ReLUConv2D,filters=48,kernel_size=[1,1],input_shape=[17,17,204],stride=[1,1],padding=0ReLUConv2D,filters=12,kernel_size=[3,3],input_shape=[17,17,48],stride=[1,1],padding=1SkipCatdim=1SkipNet1SkipNet2ReLUConv2D,filters=48,kernel_size=[1,1],input_shape=[17,17,216],stride=[1,1],padding=0ReLUConv2D,filters=12,kernel_size=[3,3],input_shape=[17,17,48],stride=[1,1],padding=1SkipCatdim=1SkipNet1SkipNet2ReLUConv2D,filters=48,kernel_size=[1,1],input_shape=[17,17,228],stride=[1,1],padding=0ReLUConv2D,filters=12,kernel_size=[3,3],input_shape=[17,17,48],stride=[1,1],padding=1SkipCatdim=1SkipNet1SkipNet2ReLUConv2D,filters=48,kernel_size=[1,1],input_shape=[17,17,240],stride=[1,1],padding=0ReLUConv2D,filters=12,kernel_size=[3,3],input_shape=[17,17,48],stride=[1,1],padding=1SkipCatdim=1SkipNet1SkipNet2ReLUConv2D,filters=48,kernel_size=[1,1],input_shape=[17,17,252],stride=[1,1],padding=0ReLUConv2D,filters=12,kernel_size=[3,3],input_shape=[17,17,48],stride=[1,1],padding=1SkipCatdim=1SkipNet1SkipNet2ReLUConv2D,filters=48,kernel_size=[1,1],input_shape=[17,17,264],stride=[1,1],padding=0ReLUConv2D,filters=12,kernel_size=[3,3],input_shape=[17,17,48],stride=[1,1],padding=1SkipCatdim=1SkipNet1SkipNet2ReLUConv2D,filters=48,kernel_size=[1,1],input_shape=[17,17,276],stride=[1,1],padding=0ReLUConv2D,filters=12,kernel_size=[3,3],input_shape=[17,17,48],stride=[1,1],padding=1SkipCatdim=1SkipNet1SkipNet2ReLUConv2D,filters=48,kernel_size=[1,1],input_shape=[17,17,288],stride=[1,1],padding=0ReLUConv2D,filters=12,kernel_size=[3,3],input_shape=[17,17,48],stride=[1,1],padding=1SkipCatdim=1ReLUConv2D,filters=150,kernel_size=[1,1],input_shape=[17,17,300],stride=[1,1],padding=0AvgPool2DSkipNet1SkipNet2ReLUConv2D,filters=48,kernel_size=[1,1],input_shape=[9,9,150],stride=[1,1],padding=0ReLUAProvableDefenseforDeepResidualNetworksConv2D,filters=12,kernel_size=[3,3],input_shape=[9,9,48],stride=[1,1],padding=1SkipCatdim=1SkipNet1SkipNet2ReLUConv2D,filters=48,kernel_size=[1,1],input_shape=[9,9,162],stride=[1,1],padding=0ReLUConv2D,filters=12,kernel_size=[3,3],input_shape=[9,9,48],stride=[1,1],padding=1SkipCatdim=1SkipNet1SkipNet2ReLUConv2D,filters=48,kernel_size=[1,1],input_shape=[9,9,174],stride=[1,1],padding=0ReLUConv2D,filters=12,kernel_size=[3,3],input_shape=[9,9,48],stride=[1,1],padding=1SkipCatdim=1SkipNet1SkipNet2ReLUConv2D,filters=48,kernel_size=[1,1],input_shape=[9,9,186],stride=[1,1],padding=0ReLUConv2D,filters=12,kernel_size=[3,3],input_shape=[9,9,48],stride=[1,1],padding=1SkipCatdim=1SkipNet1SkipNet2ReLUConv2D,filters=48,kernel_size=[1,1],input_shape=[9,9,198],stride=[1,1],padding=0ReLUConv2D,filters=12,kernel_size=[3,3],input_shape=[9,9,48],stride=[1,1],padding=1SkipCatdim=1SkipNet1SkipNet2ReLUConv2D,filters=48,kernel_size=[1,1],input_shape=[9,9,210],stride=[1,1],padding=0ReLUConv2D,filters=12,kernel_size=[3,3],input_shape=[9,9,48],stride=[1,1],padding=1SkipCatdim=1SkipNet1SkipNet2ReLUConv2D,filters=48,kernel_size=[1,1],input_shape=[9,9,222],stride=[1,1],padding=0ReLUConv2D,filters=12,kernel_size=[3,3],input_shape=[9,9,48],stride=[1,1],padding=1SkipCatdim=1SkipNet1SkipNet2ReLUConv2D,filters=48,kernel_size=[1,1],input_shape=[9,9,234],stride=[1,1],padding=0ReLUConv2D,filters=12,kernel_size=[3,3],input_shape=[9,9,48],stride=[1,1],padding=1SkipCatdim=1SkipNet1SkipNet2ReLUConv2D,filters=48,kernel_size=[1,1],input_shape=[9,9,246],stride=[1,1],padding=0ReLUConv2D,filters=12,kernel_size=[3,3],input_shape=[9,9,48],stride=[1,1],padding=1SkipCatdim=1SkipNet1SkipNet2ReLUConv2D,filters=48,kernel_size=[1,1],input_shape=[9,9,258],stride=[1,1],padding=0ReLUConv2D,filters=12,kernel_size=[3,3],input_shape=[9,9,48],stride=[1,1],padding=1SkipCatdim=1SkipNet1SkipNet2ReLUConv2D,filters=48,kernel_size=[1,1],input_shape=[9,9,270],stride=[1,1],padding=0ReLUConv2D,filters=12,kernel_size=[3,3],input_shape=[9,9,48],stride=[1,1],padding=1SkipCatdim=1SkipNet1SkipNet2ReLUConv2D,filters=48,kernel_size=[1,1],input_shape=[9,9,282],stride=[1,1],padding=0ReLUConv2D,filters=12,kernel_size=[3,3],input_shape=[9,9,48],stride=[1,1],padding=1SkipCatdim=1SkipNet1SkipNet2AProvableDefenseforDeepResidualNetworksReLUConv2D,filters=48,kernel_size=[1,1],input_shape=[9,9,294],stride=[1,1],padding=0ReLUConv2D,filters=12,kernel_size=[3,3],input_shape=[9,9,48],stride=[1,1],padding=1SkipCatdim=1SkipNet1SkipNet2ReLUConv2D,filters=48,kernel_size=[1,1],input_shape=[9,9,306],stride=[1,1],padding=0ReLUConv2D,filters=12,kernel_size=[3,3],input_shape=[9,9,48],stride=[1,1],padding=1SkipCatdim=1SkipNet1SkipNet2ReLUConv2D,filters=48,kernel_size=[1,1],input_shape=[9,9,318],stride=[1,1],padding=0ReLUConv2D,filters=12,kernel_size=[3,3],input_shape=[9,9,48],stride=[1,1],padding=1SkipCatdim=1SkipNet1SkipNet2ReLUConv2D,filters=48,kernel_size=[1,1],input_shape=[9,9,330],stride=[1,1],padding=0ReLUConv2D,filters=12,kernel_size=[3,3],input_shape=[9,9,48],stride=[1,1],padding=1SkipCatdim=1ReLUAvgPool2DCorrelateAllonly_train=FalseLinearout=10CorrelateMax32Conv31,1DecorrMin8Conv31,1Conv11,0Conv31,1+rDecorrMin4Conv31,1Conv11,0Conv31,1+rDecorrAllConv31,1Conv11,0Conv31,1+rConv31,1Conv11,0Conv31,1+rCorrelateMax10FC500arXiv:1903.12519v2  [cs.LG]  7 Jan 2020