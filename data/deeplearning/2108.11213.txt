ACCOMONTAGE: ACCOMPANIMENT ARRANGEMENT VIA PHRASE
SELECTION AND STYLE TRANSFER

Jingwei Zhao
Music X Lab, NYU Shanghai
jz4807@nyu.edu

Gus Xia
Music X Lab, NYU Shanghai
gxia@nyu.edu

1
2
0
2

g
u
A
5
2

]

D
S
.
s
c
[

1
v
3
1
2
1
1
.
8
0
1
2
:
v
i
X
r
a

ABSTRACT

Accompaniment arrangement is a difﬁcult music genera-
tion task involving intertwined constraints of melody, har-
mony, texture, and music structure. Existing models are
not yet able to capture all these constraints effectively, es-
pecially for long-term music generation. To address this
problem, we propose AccoMontage, an accompaniment
arrangement system for whole pieces of music through
unifying phrase selection and neural style transfer. 1 We
focus on generating piano accompaniments for folk/pop
songs based on a lead sheet (i.e., melody with chord pro-
gression). Speciﬁcally, AccoMontage ﬁrst retrieves phrase
montages from a database while recombining them struc-
turally using dynamic programming. Second, chords of
the retrieved phrases are manipulated to match the lead
sheet via style transfer. Lastly, the system offers controls
over the generation process. In contrast to pure learning-
based approaches, AccoMontage introduces a novel hybrid
pathway, in which rule-based optimization and deep learn-
ing are both leveraged to complement each other for high-
quality generation. Experiments show that our model gen-
erates well-structured accompaniment with delicate tex-
ture, signiﬁcantly outperforming the baselines.

1. INTRODUCTION

Accompaniment arrangement refers to the task of recon-
ceptualizing a piece by composing the accompaniment part
given a lead sheet (a lead melody with a chord progres-
sion). When designing the texture and voicing of the ac-
companiment, arrangers are simultaneously dealing with
the constraints from the original melody, chord progres-
sion, and other structural information. This constrained
composition process is often modeled as a conditioned
generation problem in music automation.

Despite recent promising advances in deep music gen-
erative models [1–7], existing methods cannot yet gen-
erate musical accompaniment while capturing the afore-
mentioned constraints effectively. Speciﬁcally, most al-

1 Codes and demos at https://github.com/zhaojw1998/AccoMontage.

© J. Zhao, and G. Xia. Licensed under a Creative Commons
Attribution 4.0 International License (CC BY 4.0). Attribution: J. Zhao,
and G. Xia, “AccoMontage: Accompaniment Arrangement via Phrase
Selection and Style Transfer”, in Proc. of the 22nd Int. Society for Music
Information Retrieval Conf., Online, 2021.

gorithms fall short in preserving the ﬁne granularity and
structure of accompaniment in the long run. Also, it is dif-
ﬁcult to explicitly control the generation process. We argue
that these limits are mainly due to the current generation
from scratch approach. In composition practice, however,
arrangers often resort to existing pieces as accompaniment
references. For example, a piano accompanist can impro-
vise through off-the-shelf textures while transferring them
into proper chords, which is essentially re-harmonizing a
reference piece to ﬁt a query lead sheet. In this way, the co-
herence and structure of the accompaniment are preserved
from the reference pieces, and musicians also have control
over what reference to choose.

To this end, we contribute AccoMontage, a generalized
template-based approach to 1) given a lead sheet as the
query, search for proper accompaniment phrases as the ref-
erence; 2) re-harmonize the selected reference via style
transfer to accompany the query. We model the search
stage as an optimization problem on the graph, where
nodes represent candidate phrases in the dataset and edges
represent inter-phrase transitions. Node scores are deﬁned
in a rule-based manner to reveal query-reference ﬁtness,
while edge scores are learned by contrastive learning to
reveal smoothness of phrase transitions. As for the re-
harmonization stage, we adopt the chord-texture disentan-
glement and transfer method in [8, 9].

The current system focuses on arranging piano accom-
paniments for a full-length folk or pop song. Experi-
mental results show that the generated accompaniments
not only harmonize well with the melody but also con-
tain more intra-phrase coherence and inter-phrase structure
compared to the baselines. In brief, our contributions are:

• A generalized template-based approach: A novel
hybrid approach to generative models, where search-
ing and deep learning are both leveraged to comple-
ment each other and enhance the overall generation
quality. This strategy is also useful in other domains.

• The AccoMontage system: A system capable
of generating long-term and structured accompani-
ments for full-length songs. The arranged accompa-
niments have state-of-the-art quality and are signif-
icantly better than existing pure learning-based and
template-based baselines.

• Controllable music generation: Users can control
the generation process by pre-ﬁltering of two texture
features: rhythm density and voice number.

 
 
 
 
 
 
2. RELATED WORK

3. METHODOLOGY

We review three topics related to symbolic accompaniment
arrangement:
template-
based arrangement, and music style transfer.

conditional music generation,

2.1 Conditional Music Generation

Conditional music generation takes various forms, such as
generating chords conditioned on the melody [10,11], gen-
erating melody on the underlying chords [6, 7], and gen-
erating melody from metadata and descriptions [12].
In
particular, accompaniment arrangement refers to generat-
ing accompaniment conditioned on the lead sheet, and this
topic has recently drawn much research attention. We even
see tailored datasets for piano arrangement tasks [13].

For accompaniment arrangement, existing models that
show satisﬁed arrangement quality typically apply only to
short clips. GAN and VAE-based models are used to main-
tain inter-track music dependency [14–16], but limit music
generation within 4 to 8 bars. Another popular approach
is to generate longer accompaniment in a seq2seq manner
with attention [5, 6, 8], but can easily converge to repeti-
tive textural patterns in the long run. On the other hand,
models that arrange for complete songs typically rely on a
library of ﬁxed elementary textures and often fail to gener-
alize [17–19]. This paper aims to unite both high-quality
and long-term accompaniment generation in one system,
where “long-term” refers to full songs (32 bars and more)
with dependencies to intra-phrase melody and chord pro-
gression, and inter-phrase structure.

2.2 Template-based Accompaniment Arrangement

The use of existing compositions to generate music is not
an entirely new idea. Existing template-based algorithms
include learning-based unit selection [20, 21], rule-based
matching [17, 18], and genetic algorithms [19]. For ac-
companiment arrangement, a common problem lies in the
difﬁculty to ﬁnd a perfectly matched reference especially
when the templates contain rich textures with non-chordal
tones. Some works choose to only use basic accompani-
ment patterns to avoid this issue [17–19]. In contrast, our
study addresses this problem by applying the style trans-
fer technique on a selected template to improve the ﬁtness
between the accompaniment and the lead sheet. We name
our approach after generalized template matching.

2.3 Music Style transfer

Music style transfer [22] is becoming a popular ap-
proach for controllable music generation. Through music-
representation disentanglement and manipulation, users
can transfer various factors of a reference music piece, in-
cluding pitch contour, rhythm pattern, chord progression,
polyphonic texture, etc [1, 8]. Our approach can be seen as
an extension of music style transfer in which the “reference
search” step is also automated.

The AccoMontage system uses a generalized template-
based approach for piano accompaniment arrangement.
The input to the system is a lead sheet of a complete
folk/pop song with phrase labels, which we call a query.
The search space of the system is a MIDI dataset of piano-
arranged pop songs. In general, we can derive the chord
progression and phrase labels of each song in the dataset
by MIR algorithms. In our case, the chords are extracted
by [13] and the phrases are labeled manually [23]. We refer
to each phrase of associated accompaniment, melody, and
chords as a reference. For the rest of this section, we ﬁrst
introduce the feature representation of the AccoMontage
system in Section 3.1, and then describe the main pipeline
algorithms in Section 3.2 and 3.3. Finally, we show how to
further control the arrangement process in Section 3.4.

3.1 Feature Representation

Given a lead sheet as the query, we represent it as a se-
quence of ternary tuples:

q = {(cid:0)qmel

i

, qchord
i

, qlabel
i

(cid:1)}n

i=1,

(1)

i

where qmel
, the melody feature of query phrase i, is a se-
quence of 130-D one-hot vectors with 128 MIDI pitches
plus a hold and a rest state [24]; qchord
, the chord feature
aligned with qmel
, is a sequence of 12-D chromagram vec-
tors [1, 2]; qlabel
is a phrase label string denoting within-
song repetition and length in bar, such as A8, B8, etc. [23].
n is the number of phrases in lead sheet q.

i

i

i

We represent the accompaniment reference space as a

collection of tuples:

r = {(cid:0)rmel

i

, rchord
i

, racc
i

(cid:1)}N

i=1,

(2)

i

, and rchord
i

where rmel
are the melody and the chord fea-
ture of the i-th reference phrase, represented in the same
format as in the query phrases; racc
is the accompani-
ment feature, which is a 128-D piano-roll representation
the same as [8]. N is the volume of the reference space.

i

3.2 Phrase Selection

Assuming there are n phrases in the query lead sheet, we
aim to ﬁnd a reference sequence:

x = [x1, x2, · · · , xn],

(3)

where we match reference xi to the i-th phrase qi in our
query; xi ∈ r and has the same length as qi.

Given the query’s phrase structure, the reference space
forms a graph of layered structures shown as Figure 1.
Each layer consists of equal-length reference phrases and
consecutive layers are fully connected to each other. Each
node in graph describes the ﬁtness between xi and qi, and
each edge evaluates the transition from xi to xi+1. A com-
plete selection of reference phrases corresponds to a path
that traverses through all layers. To evaluate a path, We
design a ﬁtness model and a transition model as follows.

3.2.1 Phrase Fitness Model

We rely on the phrase ﬁtness model to evaluate if a refer-
ence accompaniment phrase matches a query phrase. For-
mally, we deﬁne the ﬁtness model f (xi, qi) as follows:

f (xi, qi) = αsim(xrhy

, qrhy
i
+ βsim(T(xchord

i

i

)
), T(qchord
i

(4)

)),

i

and qrhy
i

where sim(·, ·) measures the similarity between two in-
puts. In our work, we use the cosine similarity. T(·) is
the Tonal Interval Vector (TIV) operator that maps a chro-
magram to a 12-D tonal interval space whose geometric
properties concur with harmonic relationships of the tonal
system [25]. xrhy
are both rhythm features, which
condense the original 130-D melody feature to 3-D that
denotes an onset of any pitch, a hold state, and rest [1].
xchord
are chord features (chromagram) deﬁned
i
in Section 3.1 and we further augment the reference space
by transposing phrases to all 12 keys. While computing
the similarity, we consider the rhythm feature and TIV as
2-D matrices each with channel number 3 and 12. We cal-
culate the cosine similarity of both features by feeding in
their channel-ﬂattened vectors.

and qchord
i

Note that in Eq (4), we compare only the rhythm and
chord features for query-reference matching. The underly-
ing assumption is that if lead sheet A is similar to another
lead sheet B in rhythm and chord progression, then B’s
accompaniment will be very likely to ﬁt A as well.

3.2.2 Transition Model

We exploit the transition model to reveal the inter-phrase
transition and structural constraints. Formally, we deﬁne
the transition score between two reference accompaniment
phrases t(xi, xi+1) as follows:

Figure 1. Phrase selection on the graph. Based on the
lead sheet with an AABB phrase structure, the search space
forms a graph with four consecutive layers. Graph nodes
are assigned with similarity scores, and edges with transi-
tion scores. The form term is part of the transition score.

where xi and xi+1 are supposed to be consecutive pairs. S
is a collection of k samples which contains xi+1 and other
k − 1 randomly selected phrases from reference space r.
Following [20], we choose k = 5.

For the form term form(xi, xi+1), we introduce this
term to bias a more well-structured transition. Concretely,
if query phrases qi and qi+1 share the same phrase label,
we would prefer to also retrieve equal-labeled references,
i.e., accompaniments with recapitulated melody themes.
To maximize such likelihood, we deﬁne the form term:

form(xi, xi+1) = 1

{qlabel
i

i+1 } · 1

=qlabel

{xmel

i ≈xmel

i+1},

(8)

where we deﬁne xmel
cosine similarity is greater than 0.99.

i ≈ xmel

i+1 if and only if their step-wise

t(xi, xi+1) = sim(W1xtxt

i

, W2xtxt

i+1)

+ form(xi, xi+1).

(5)

3.2.3 Model Inference

The ﬁrst term in Eq (5) aims to reveal the transition
naturalness of the polyphonic texture between two adja-
cent phrases. Instead of using rule-based heuristics to pro-
cess texture information, we resort to neural representation
learning and contrastive learning. Formally, let xtxt
i denote
the feature vector that represents the accompaniment tex-
ture of xacc

. It is computed by:

i

i = Enctxt
xtxt

θ (xacc
i

),

(6)

where the design of Enctxt
θ (·) is adopted from the chord-
texture representation disentanglement model in [8]. This
texture encoder regards piano-roll inputs as images and
uses a CNN to compute a rough “sketch” of polyphonic
texture that is not sensitive to mild chord variations.
To reveal whether two adjacent textures (xtxt

i+1)
follow a natural transition, we use a contrastive loss L to
simultaneously train the weight matrix W in Eq (5) and
ﬁne-tune Enctxt

θ (·) (with parameter θ) in Eq (6):

, xtxt

i

L(W, θ) = 1 −

(cid:80)

exp(sim(W1xtxt
x∈S exp(sim(W1xtxt

i

i

, W2xtxt

i+1))
, W2xtxt

k ))

, (7)

The reference space forms a layered graph with consecu-
tive layers fully connected to each other. In Figure 1, we
leverage the transition model to assign weights of edges
and the ﬁtness model to assign weights of nodes. Thus, the
phrase selection is formulated as:

x∗ = argmax
x1,x2,··· ,xn

δ

n
(cid:88)

i=1

f (xi, qi) + γ

n−1
(cid:88)

i=1

t(xi, xi+1),

(9)

where f (·) and t(·) are as deﬁned in Eq (4) and Eq (5), and
δ and γ are hyper-parameters.

We optimize Eq (9) by dynamic programming to re-
trieve the Viterbi path x∗ as the optimal solution [26]. The
time complexity is O(nN 2), where n is the number of
query phrases and N is the volume of the reference space.
In summary, the phrase selection algorithm enforces
strong structural constraints (song-level form and phrase-
level ﬁtness) as well as weak harmonic constraints (chord
term in Eq (4)) to the selection of accompaniment refer-
ence. We argue that this is a good compromise because
strong harmonic constraints can potentially “bury” well-
structured references due to unmatched chord when our

Multi-case basedsearch𝑥𝑥1…𝑥𝑥2𝑥𝑥3𝑥𝑥4………Form TermSimilarity Score𝑞𝑞1𝑞𝑞2𝑞𝑞3𝑞𝑞4dataset is limited. To maintain a better harmonic ﬁtness,
we resort to music style transfer.

3.3 Style Transfer

Table 1. Length Distribution of POP909 Phrase

bars

<4

4

Phrases

1338

3591

5~7

855

8

>8

3796

1402

The essence of style transfer is to transfer the chord se-
quence of a selected reference phrase while keeping its tex-
ture. To this end, we adopt the chord-texture disentangle-
ment VAE framework by [8]. The VAE consists of a chord
encoder Encchd and a texture encoder Enctxt. Encchd
takes in a two-bar chord progression under one-beat res-
olution and exploits a bi-directional GRU to approximate
a latent chord representation zchd. Enctxt is introduced in
Section 3.2.2 and it extracts a latent texture representation
ztxt. The decoder Dec takes the concatenation of zchd and
ztxt and decodes the music segment using the same archi-
tecture invented in [9]. Sustaining texture input and vary-
ing chords, the whole model works like a conditional VAE
which re-harmonizes texture based on the chord condition.
In our case, to re-harmonize the selecetd accompani-
to query lead sheet qi, the style transfer works

ment xacc
in a pipeline as follows:

i

zchd = Encchd(qchord
ztxt = Enctxt(xacc
),

i

i

),

(10)

x(cid:48)
i = Dec(zchd, ztxt),

where x(cid:48)
ment arrangement result is x∗(cid:48) = [x∗(cid:48)

i is the re-harmonized result. The ﬁnal accopmani-

2 , · · · , x∗(cid:48)
n ].

1 , x∗(cid:48)

3.4 Controllability

In the phrase selection stage, we essentially traverse a route
on the graph. Intuitively, we can control generation of the
whole route by assigning the ﬁrst node. In our case, we
ﬁlter reference candidates for the ﬁrst phrase based on tex-
tural properties. The current design has two ﬁlter criteria:
rhythm density and voice number. we deﬁne three inter-
vals low, medium, and high for both properties and mask
the references that do not fall in the expected interval.

• Rhythm Density (RD): the ratio of time steps with

note onsets to all time steps;

• Voice Number (VN): the average number of notes

that are simultaneously played.

4. EXPERIMENT

4.1 Dataset

We collect our reference space from POP909 dataset [13]
with the phrase segmentation created by [23]. POP909
contains piano arrangements of 909 popular songs created
by professional musicians, which is a great source of del-
icate piano textures. Each song has a separated melody,
chord, and accompaniment MIDI track. We only keep the
pieces with 2
4 meters and quantize them at 16th notes
(chords at 4th). This derives 857 songs segmented into
11032 phrases. As shown in Table 1, we have four-bar and

4 and 4

eight-bar phrases in majority, which makes sense for pop-
ular songs. We also use POP909 to ﬁne-tune our transition
model, during which we randomly split the dataset (at song
level) into training (95%) and validation (5%) sets.

At inference time, the query lead sheets come from the
Nottingham Dataset [27], a collection of ~1000 British and
American folk tunes. We also adopt 2
4 pieces quan-
tized at 16th (chords at 4th). We label their phrase seg-
mentation by hand, where four-bar and eight-bar phrases
are also the most common ones.

4 and 4

4.2 Architecture Design

We develop our model based on the chord-texture disen-
tanglement model proposed by [8], which comprises a tex-
ture encoder, a chord encoder, and a decoder. The tex-
ture encoder consists of a convolutional layer with kernel
size 12 × 4 and stride 1 × 4 and a bi-directional GRU en-
coder [24]. The convolutional layer is followed by a ReLU
activation [28] and max-pooling with kernel size 4 × 1 and
stride 4 × 1. The chord encoder is a bi-directional GRU en-
coder. The decoder is consistent with PianoTree VAE [9],
a hierarchical architecture for polyphonic representation
learning. The architecture of Enctxt(·) in the proposed
transition model is the same as the texture encoder illus-
trated above. We directly take the chord-texture disentan-
glement model with pre-trained weights as our style trans-
fer model. We ﬁne-tune the transition model with W1 and
W2 in Eq (7) as trainable parameters.

4.3 Training

Our model is trained with a mini-batch of 128 piano-
roll pairs for 50 epochs using Adam optimizer [29] with
a learning rate from 1e-4 exponentially decayed to 5e-
6. Note that each piano-roll pair contains 2 consecutive
piano-rolls and 4 randomly sampled ones. We ﬁrst pre-
train a chord-texture disentanglement model and initialize
Enctxt(·) using weights of the texture encoder in the pre-
trained model. Then we update all the parameters of the
proposed transition model using contrastive loss L in Eq
(7). We set both α and β in Eq (4) to 0.5. During infer-
ence, we set δ and γ in Eq (9) to 0.3 and 0.7.

4.4 Generated Examples

To this end, we show two long-term accompaniment ar-
rangement examples by the Accomontage system. The
ﬁrst one is illustrated in Figure 2, in which we show a
whole piece (32-bar music) piano arrangement (the bot-
tom two staves) base on the lead sheet (the top stave). We
see that the generated accompaniment matches with the
melody and has a natural ﬂow on its texture. Moreover,
it follows the A8A8B8B8 structure of the melody.

Figure 2. Accompaniment arrangement for Castles in the Air from Nottingham Dataset by AccoMontage. The 32-bar song
has an A8A8B8B8 phrase structure which is captured during accompaniment arrangement. Second melodies and texture
variations are also introduced to manifest music ﬂow. Here we highlight some texture re-harmonization of 7th chords.

Figure 3. Pre-Filtering Control on Rhythm Density and Voice Number

The second example shows that our controls on rhythm
density and voice number are quite successful. To better
illustrate, we switch to a piano-roll representation in Fig-
ure 3, where 9 arranged accompaniments for the same lead
sheet is shown in a 3 by 3 grid. The rhythm density control
increases from left to right, while the voice number control
increases from top to bottom. We can see that both controls
have a signiﬁcant inﬂuence on the generated results.

4.5 Evaluation

4.5.1 Baseline Models

The AccoMontage system is a generalized template-based
model that leverages both rule-based optimization and
deep learning to complement each other. To evaluate, we
introduce a hard template-based and a pure learning-based
baseline to compare with our model. Speciﬁcally, the base-

line model architectures are as follows:

Hard Template-Based (HTB): The hard template-
based model also retrieves references from existing accom-
paniment, but directly applies them without any style trans-
fer. It uses the same phrase selection architecture as our
model while skipping the style transfer stage.

Pure Learning-Based (PLB): We adopt the accompa-
niment arrangement model in [8], a seq2seq framework
combining Transformer [30] and chord-texture disentan-
glement. We consider [8] the current state-of-the-art al-
gorithm for controllable accompaniment generation due to
its tailored design of harmony and texture representations,
sophisticated neural structure, and convincing demos. The
input to the model is a lead sheet and its ﬁrst four-bar ac-
companiment. The model composes the rest by predicting
every four bars based on the current lead sheet and previ-
ous four-bar accompaniment.

Texture re-harmonized with D7 chordPreservation of phrase structureSyncopated pattern preserved when phrase transitions from A to B, while more variations introducedTexture re-harmonized with G7 chordMinor texture variation thatmanifests music flowTexture re-harmonized with A7 chordCounter melody introduced when phrase transitions from A to ARhythm Density (RD)Voice Number (VN)low VN, low RDmedium VN, low RDhigh VN, low RDlow VN, medium RDmedium VN, medium RDhigh VN, medium RDlow VN, high RDmedium VN, high RDhigh VN, high RDlowhighhighFigure 4. Subjective Evaluation Results.

4.5.2 Subjective Evaluation

We conduct a survey to evaluate the musical quality of
the arrangement performance of all models.
In our sur-
vey, each subject listens to 1 to 3 songs randomly selected
from a pool of 14. All 14 songs are randomly selected from
the Nottingham Dataset, 12 of which have 32 bars and the
other two 24 and 16 bars. Each song has three accompa-
niment versions generated by our and the baseline models.
The subjects are required to rate all three accompaniment
versions of one song based on three metrics: coherence,
structure, and musicality. The rating is base on a 5-point
scale from 1 (very poor) to 5 (excellent).

• Coherence: If the accompaniment matches the lead

melody in harmony and texture;

• Structure:If the accompaniment ﬂows dynamically

with the structure of the melody;

• Musicality: Overall musicality of accompaniment.

A total of 72 subjects (37 females and 35 males) par-
ticipated in our survey and we obtain 67 effective ratings
for each metric. As in Figure 4, the heights of bars repre-
sent the means of the ratings and the error bars represent
the MSEs computed via within-subject ANOVA [31]. We
report a signiﬁcantly better performance of our model than
both baselines in coherence and structure (p < 0.05), and a
marginally better performance in musicality (p = 0.053).

4.5.3 Objective Evaluation

In the phrase selection stage, we leverage a self-supervised
contrastive loss (Eq (7)) to enforce a smooth textural tran-
sition among reference phrases. We expect a lower loss for
true adjacent phrase pairs than in other situations. Mean-
while, true consecutive pairs should hold a similar texture
pattern with smaller differences in general properties.

We investigate the contrastive loss (CL) and the dif-
ference of rhythm density (RD) and voice number (VN)
among three types of phrase pairs from the validation set.
Namely, Random, Same Song, and Adjacent. Between to-
tally randomly pairing and strict adjacency, Same Song
refers to randomly selecting two phrases (not necessarily
adjacent) from one song. Results are shown in Figure 5.

Figure 5. Evaluation of Transition Model. The contrastive
loss (CL) and differences of RD and VN are calculated for
three types of phrase pairs. A consistent decreasing trend
illustrates reliable discernment of smooth transition.

Table 2. Ranking Accuracy and Mean Rank

Metric

Phrase Acc.

Song Acc. Rank@50

Value

0.2425

0.3769

5.8003

For contrastive loss and each property, we see a consis-
tent decreasing trend from Random to Same Song and to
Adjacent. Speciﬁcally, we see the upper quartile of Adja-
cent is remarkably lower than the lower quartile of Random
for CL, which indicates a reliable textural discernment that
ensures smooth phrase transitions. This is also proved by
the metric of ranking accuracy and mean rank [20], where
the selection rank of the true adjacent phrase out of k − 1
randomly selected phrases (Rank@k) is calculated. We
follow [20] and adopt Rank@50, and the results are shown
in Table 2. Phrase Acc. and Song Acc. each refers to the
accuracy that the top-ranked phrase is Adjacent or belongs
to the Same Song. The high rank of adjacent pairs illus-
trates our model’s reliability to explore smooth transitions.

5. CONCLUSION

In conclusion, we contribute a generalized template-based
algorithm for the accompaniment arrangement problem.
The main novelty lies in the methodology that seamlessly
combines deep generation and search-based generation. In
speciﬁc, searching is used to optimize the high-level struc-
ture, while neural style transfer is in charge of local co-
herency and melody-accompaniment ﬁtness. Such a top-
down hybrid strategy is inspired by how human musicians
arrange accompaniments in practice. We aim to bring a
new perspective not only to music generation, but to long-
term sequence generation in general. Experiments show
that our AccoMontage system signiﬁcantly outperforms
pure learning-based and template-based methods, being
capable of rendering well-structured and ﬁne-grained ac-
companiment for full-length songs.

CoherencyStructureMusicality1.01.52.02.53.03.54.04.5RatingOursPLBHTB0.40.50.60.70.80.91.01.11.2CLRDVNIndex0.000.250.500.751.001.251.501.752.00Random Same Song AdjacentContrastive ValueDifference Value6. ACKNOWLEDGEMENT

The authors wish to thank Yixiao Zhang for his contri-
bution to ﬁgure framing and proofreading. We thank Li-
wei Lin and Junyan Jiang for providing feedback on initial
drafts of this paper and additional editing.

7. REFERENCES

[1] R. Yang, D. Wang, Z. Wang, T. Chen, J. Jiang, and
G. Xia, “Deep music analogy via latent representation
disentanglement,” arXiv preprint arXiv:1906.03626,
2019.

[2] K. Chen, G. Xia, and S. Dubnov, “Continuous melody
generation via disentangled short-term representations
and structural conditions,” in 2020 IEEE 14th Inter-
national Conference on Semantic Computing (ICSC),
2020, pp. 128–135.

[3] C.-Z. A. Huang, A. Vaswani, J. Uszkoreit, N. Shazeer,
I. Simon, C. Hawthorne, A. M. Dai, M. D. Hoff-
man, M. Dinculescu, and D. Eck, “Music transformer,”
arXiv preprint arXiv:1809.04281, 2018.

[4] P. Dhariwal, H. Jun, C. Payne, J. W. Kim, A. Radford,
and I. Sutskever, “Jukebox: A generative model for
music,” arXiv preprint arXiv:2005.00341, 2020.

[5] Y. Ren, J. He, X. Tan, T. Qin, Z. Zhao, and T.-Y. Liu,
“Popmag: Pop music accompaniment generation,” in
Proceedings of the 28th ACM International Conference
on Multimedia, 2020, pp. 1198–1206.

[6] H. Zhu, Q. Liu, N. J. Yuan, C. Qin, J. Li, K. Zhang,
G. Zhou, F. Wei, Y. Xu, and E. Chen, “Xiaoice band:
A melody and arrangement generation framework for
pop music,” in Proceedings of the 24th ACM SIGKDD
International Conference on Knowledge Discovery &
Data Mining, 2018, pp. 2837–2846.

[7] L.-C. Yang, S.-Y. Chou, and Y.-H. Yang, “Midinet:
A convolutional generative adversarial network for
symbolic-domain music generation,” arXiv preprint
arXiv:1703.10847, 2017.

[8] Z. Wang, D. Wang, Y. Zhang, and G. Xia, “Learning in-
terpretable representation for controllable polyphonic
music generation,” arXiv preprint arXiv:2008.07122,
2020.

[9] Z. Wang, Y. Zhang, Y. Zhang, J. Jiang, R. Yang,
J. Zhao, and G. Xia, “Pianotree vae: Structured
representation learning for polyphonic music,” arXiv
preprint arXiv:2008.07118, 2020.

[12] Y. Zhang, Z. Wang, D. Wang, and G. Xia, “Butter:
A representation learning framework for bi-directional
music-sentence retrieval and generation,” in Proceed-
ings of the 1st Workshop on NLP for Music and Audio
(NLP4MusA), 2020, pp. 54–58.

[13] Z. Wang, K. Chen, J. Jiang, Y. Zhang, M. Xu, S. Dai,
X. Gu, and G. Xia, “Pop909: A pop-song dataset
for music arrangement generation,” arXiv preprint
arXiv:2008.07142, 2020.

[14] H.-W. Dong, W.-Y. Hsiao, L.-C. Yang, and Y.-H. Yang,
“Musegan: Multi-track sequential generative adversar-
ial networks for symbolic music generation and accom-
paniment,” in Proceedings of the AAAI Conference on
Artiﬁcial Intelligence, vol. 32, no. 1, 2018.

[15] H.-M. Liu and Y.-H. Yang, “Lead sheet generation and
arrangement by conditional generative adversarial net-
work,” in 2018 17th IEEE International Conference on
Machine Learning and Applications (ICMLA).
IEEE,
2018, pp. 722–727.

[16] B. Jia, J. Lv, Y. Pu, and X. Yang, “Impromptu accom-
paniment of pop music using coupled latent variable
model with binary regularizer,” in 2019 International
Joint Conference on Neural Networks (IJCNN). IEEE,
2019, pp. 1–6.

[17] P.-C. Chen, K.-S. Lin, and H. H. Chen, “Automatic
accompaniment generation to evoke speciﬁc emotion,”
in 2013 IEEE International Conference on Multimedia
and Expo (ICME).

IEEE, 2013, pp. 1–6.

[18] Y.-C. Wu and H. H. Chen, “Emotion-ﬂow guided mu-
sic accompaniment generation,” in 2016 IEEE Inter-
national Conference on Acoustics, Speech and Signal
Processing (ICASSP).

IEEE, 2016, pp. 574–578.

[19] C.-H. Liu and C.-K. Ting, “Polyphonic accompani-
ment using genetic algorithm with music theory,” in
2012 IEEE Congress on Evolutionary Computation.
IEEE, 2012, pp. 1–7.

[20] M. Bretan, G. Weinberg, and L. Heck, “A unit selection
methodology for music generation using deep neural
networks,” arXiv preprint arXiv:1612.03789, 2016.

[21] G. Xia, “Expressive collaborative music performance
via machine learning,” Jul 2018. [Online]. Available:
https://kilthub.cmu.edu/articles/thesis/Expressive_
Collaborative_Music_Performance_via_Machine_
Learning/6716609/1

[10] I. Simon, D. Morris, and S. Basu, “Mysong: automatic
accompaniment generation for vocal melodies,” in Pro-
ceedings of the SIGCHI conference on human factors
in computing systems, 2008, pp. 725–734.

[11] H. Lim, S. Rhyu, and K. Lee, “Chord generation
from symbolic melody using blstm networks,” arXiv
preprint arXiv:1712.01011, 2017.

[22] S. Dai, Z. Zhang, and G. G. Xia, “Music style transfer:
A position paper,” arXiv preprint arXiv:1803.06841,
2018.

[23] S. Dai, H. Zhang, and R. B. Dannenberg, “Auto-
matic analysis and inﬂuence of hierarchical structure
on melody, rhythm and harmony in popular music,”
arXiv preprint arXiv:2010.07518, 2020.

[24] A. Roberts, J. Engel, C. Raffel, C. Hawthorne, and
D. Eck, “A hierarchical latent vector model for learning
long-term structure in music,” in International Confer-
ence on Machine Learning. PMLR, 2018, pp. 4364–
4373.

[25] G. Bernardes, D. Cocharro, M. Caetano, C. Guedes,
and M. E. Davies, “A multi-level tonal interval space
for modelling pitch relatedness and musical conso-
nance,” Journal of New Music Research, vol. 45, no. 4,
pp. 281–294, 2016.

[26] G. D. Forney, “The viterbi algorithm,” Proceedings of

the IEEE, vol. 61, no. 3, pp. 268–278, 1973.

[27] E.

Foxley,

“Nottingham database,”

[EB/OL],
https://ifdo.ca/~seymour/nottingham/nottingham.html
Accessed May 25, 2021.

[28] V. Nair and G. E. Hinton, “Rectiﬁed linear units im-

prove restricted boltzmann machines,” in Icml, 2010.

[29] D. P. Kingma and J. Ba, “Adam: A method for stochas-

tic optimization,” 2017.

[30] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit,
L. Jones, A. N. Gomez, L. Kaiser, and I. Polo-
sukhin, “Attention is all you need,” arXiv preprint
arXiv:1706.03762, 2017.

[31] H. Scheffe, The analysis of variance.

John Wiley &

Sons, 1999, vol. 72.

