1

Stable Online Computation Ofﬂoading via
Lyapunov-guided Deep Reinforcement Learning

Suzhi Bi∗, Liang Huang†, Hui Wang∗, and Ying-Jun Angela Zhang‡
∗College of Electronics and Information Engineering, Shenzhen University, Shenzhen, China
†College of Computer Science and Technology, Zhejiang University of Technology, Hangzhou, China
‡Department of Information Engineering, The Chinese University of Hong Kong, Hong Kong SAR
E-mail: {bsz,wanghsz}@szu.edu.cn, liang.huang08@gmail.com, yjzhang@ie.cuhk.edu.hk

1
2
0
2

b
e
F
5

]
I

N
.
s
c
[

1
v
6
8
2
3
0
.
2
0
1
2
:
v
i
X
r
a

Abstract—In this paper, we consider a multi-user mobile-edge
computing (MEC) network with time-varying wireless channels
and stochastic user task data arrivals in sequential time frames.
In particular, we aim to design an online computation ofﬂoading
algorithm to maximize the network data processing capability
subject to the long-term data queue stability and average power
constraints. The online algorithm is practical in the sense that the
decisions for each time frame are made without the assumption
of knowing future channel conditions and data arrivals. We
formulate the problem as a multi-stage stochastic mixed integer
non-linear programming (MINLP) problem that jointly deter-
mines the binary ofﬂoading (each user computes the task either
locally or at the edge server) and system resource allocation
decisions in sequential time frames. To address the coupling
in the decisions of different time frames, we propose a novel
framework, named LyDROO, that combines the advantages of
Lyapunov optimization and deep reinforcement learning (DRL).
Speciﬁcally, LyDROO ﬁrst applies Lyapunov optimization to
decouple the multi-stage stochastic MINLP into deterministic
per-frame MINLP subproblems of much smaller size. Then, it
integrates model-based optimization and model-free DRL to solve
the per-frame MINLP problems with very low computational
complexity. Simulation results show that the proposed LyDROO
achieves optimal computation performance while satisfying all
the long-term constraints. Besides, it induces very low execution
latency that is particularly suitable for real-time implementation
in fast fading environments.

I. INTRODUCTION

The emerging mobile-edge computing (MEC) technology
allows WDs to ofﬂoad intensive computation tasks to the edge
server (ES) in the vicinity to reduce the computation energy
and time cost [1]. Compared to the naive scheme that ofﬂoads
all the tasks for edge execution, opportunistic computation
ofﬂoading, which dynamically assigns tasks to be computed
either locally or at the ES, has shown signiﬁcant performance
improvement under time-varying network conditions, such
as wireless channel gains [2], harvested energy level [3],
service availability [4], and task input-output dependency [5].
In general, it requires solving a mixed integer non-linear
programming (MINLP) that jointly determines the binary
ofﬂoading (i.e., either ofﬂoading the computation or not) and
the communication/computation resource allocation (e.g., task
ofﬂoading time and local/edge CPU frequencies) decisions [5]–
[7]. To tackle the prohibitively high computational complexity
of solving the MINLP problems, many works have proposed
reduced-complexity sub-optimal algorithms, such as local-
search based heuristics [5], [6], decomposition-oriented search

[6], and convex relaxations of the binary variables [7], [14], etc.
However, aside from performance losses, the above algorithms
still require a large number of numerical iterations to produce
a satisfying solution. It is therefore too costly to implement
the conventional optimization algorithms in a highly dynamic
MEC environment, where the MINLP needs to be frequently
re-solved once the system parameters, such as wireless link
quality, vary.

The recent development of data-driven deep reinforcement
learning (DRL) provides a promising alternative to tackle
the online computation ofﬂoading problem. In a nutshell, the
DRL framework takes a model-free approach that uses deep
neural networks (DNNs) to directly learn the optimal mapping
from the “state” (e.g., time-varying system parameters) to the
“action” (e.g., ofﬂoading decisions and resource allocation) to
maximize the “reward” (e.g., data processing rate). Example
implementations include using deep Q-learning network (DQN)
[8], double DQN [9], actor-critic DRL [10], [11]. In particular,
our previous work [11] proposes a hybrid framework, named
DROO (Deep Reinforcement learning-based Online Ofﬂoad-
ing), to combine the advantages of conventional model-based
optimization and model-free DRL methods. The integrated
learning and optimization approach leads to more robust and
faster convergence of the online training process, thanks to the
accurate estimation of reward values corresponding to each
sampled action.

Apart from optimizing the computation performance, it is
equally important to guarantee stable system operation, such as
data queue stability and average power consumption. However,
most of the existing DRL-based methods do not impose long-
term performance constraints (e.g., [8]–[11]). Instead, they
resort to heuristic approaches that discourage unfavorable
actions in each time frame by introducing penalty terms
related to, for example, packet drop events [9] and energy
consumption [8]. A well-known framework for online joint
utility maximization and stability control is Lyapunov optimiza-
tion [12]. It decouples a multi-stage stochastic optimization to
sequential per-stage deterministic subproblems, while providing
theoretical guarantee to long-term system stability. Some
recent works have applied Lyapunov optimization to design
computation ofﬂoading strategy in MEC networks (e.g., [13]–
[15]). However, it still needs to solve a hard MINLP in each
per-stage subproblem to obtain the joint binary ofﬂoading and
resource allocation decisions. To tackle the intractability, some

 
 
 
 
 
 
works have designed reduced-complexity heuristics, such as
continuous relaxation in [14] and decoupling heuristic in [15].
This, however, suffers from the similar performance-complexity
tradeoff dilemma as in [5]–[7].

In this paper, we consider a multi-user MEC network
with an ES assisting the computation of N WDs, where
the computation task data arrive at the WDs’ data queues
stochastically in sequential time frames. We aim to design
an online computation ofﬂoading algorithm, in the sense
that the decisions for each time frame are made without
knowing the future channel conditions and task arrivals. The
objective is to maximize the network data processing capability
under the long-term data queue stability and average power
constraints. To tackle the problem, we propose a Lyapunov-
guided Deep Reinforcement learning (DRL)-based Online
Ofﬂoading (LyDROO) framework that combines the advantages
of Lyapunov optimization and DRL. In particular, we ﬁrst apply
Lyapunov optimization to decouple the multi-stage stochastic
MINLP into per-frame deterministic MINLP problems. Then
in each frame, we integrate model-based optimization and
model-free DRL to solve the per-frame MINLP problems with
very low computational complexity. Simulation results show
that the proposed LyDROO algorithm converges very fast
to the optimal computation rate while meeting all the long-
term stability constraints. Compared to a myopic benchmark
algorithm that greedily maximizes the computation rate in each
time frame, the proposed LyDROO achieves a much larger
stable capacity region that can stabilize the data queues under
much heavier task data arrivals.

II. SYSTEM MODEL AND PROBLEM FORMULATION

We consider an MEC network with an ES assisting the
computation of N WDs in sequential time frames of equal
duration T . Within the tth time frame, we denote At
i (in bits)
as the raw task data arrival at the data queue of the ith WD.
We assume that At
i follows an i.i.d. exponential distribution
with mean λi, for i = 1, · · · , N . We denote the channel gain
between the ith WD and the ES as ht
i. Under the block fading
assumption, ht
i remains constant within a time frame but varies
independently across different frames.

In the tth time frame, suppose that a tagged WD i processes
Dt
i bits data and produces a computation output at the end
of the time frame. Within each time frame, we assume that
the WDs adopt a binary computation ofﬂoading rule [1] that
process the raw data either locally at the WD or remotely at
the ES. The ofﬂoading WDs share a common bandwidth W
for transmitting the task data to the ES in a TDMA manner.
We use a binary variable xt
i to denote the ofﬂoading decision.
i = 0), we
i , which is upper bounded
. The raw data (in bits) processed locally and the

denote the local CPU frequency as f t
by f max
i
consumed energy within the time frame are [1]
T, ∀xt
i T /φ, Et

i,L = κ (cid:0)f t
(1)
respectively. Here, parameter φ > 0 denotes the number of
computation cycles needed to process one bit of raw data and
κ > 0 denotes the computing energy efﬁciency parameter.

When the ith WD processes the data locally (xt

i,L = f t

i = 0,

Dt

(cid:1)3

i

2

and τ t

i ∈ [0, 1] and (cid:80)N

Otherwise, when the data is ofﬂoaded for edge execution
(xt
i = 1), we denote P t
i as the transmit power constrained
i ≤ P max
by the maximum power P t
i T as the amount
i
of time allocated to the ith WD for computation ofﬂoading.
Here, τ t
i ≤ 1. The energy consumed on
i τ t
data ofﬂoading is Et
i T .
Similar to [3] and [6], we neglect the delay on edge computing
and result downloading such that the amount of data processed
at the edge within the time frame is
Et
i,Oht
W τ t
i
τ t
vu
i T N0

i=1 τ t
i,O = P t

i T , such that P t

i = Et

i,O/τ t

i = 1,

, ∀xt

i,O =

log2

1 +

Dt

i T

(2)

(cid:18)

(cid:19)

(cid:44) (1 − xt

where vu ≥ 1 represents the rate loss due to communication
overhead and N0 denotes the noise power.
i,O and Et
i)Dt
i

iDt
i,L +
i,O denote the bits computed and energy consumed in time
i and power consumption

Let Dt
i
iEt
xt
frame t. We deﬁne computation rate rt
et
i in the tth time frame as
i)f t
i

(cid:44) (1 − xt

i,L + xt

i)Et

(1 − xt
φ

+ xt
i

W τ t
i
vu

log2

(cid:18)

1 +

(cid:19)

,

et
i,Oht
i
τ t
i N0

(3)

= (1 − xt

i)κ (cid:0)f t

i

(cid:1)3

+ xt

iet

i,O,

rt
i =

et
i =

=

Dt
i
T
Et
i
T
(cid:44) Et

i,O

where et
i,O/T . For simplicity of exposition, we assume
T = 1 without loss of generality in the following derivations.
Let Qi(t) denote the queue length of the ith WD at the

beginning of the tth time frame such that
(cid:111)
(cid:110)
Qi(t) − ˜Dt

i = min (Qi(t), Dt

Qi(t + 1) = max
where ˜Dt
i) and Qi(1) = 0. In the following
derivation, we enforce the data causality constraint Dt
i ≤ Qi(t),
implying that Qi(t) ≥ 0 holds for any t. Thus, the queue
dynamics is simpliﬁed as

, i = 1, 2, · · · , (4)

i + At

i, 0

t=1

1
K

(cid:80)K

i + At
i,

i = 1, 2, · · · .

Qi(t + 1) = Qi(t) − Dt

(5)
E [Qi(t)] < ∞ holds for Qi(t), we refer
If limK→∞
to the queue as strongly stable, where the expectation is taken
with respect to the random channel fading and task data arrivals
[12]. By the Little’s law, a strongly stable data queue translates
to a ﬁnite processing delay of each task data bit.

In this paper, we aim to design an online algorithm to
maximize the long-term average weighted sum computation rate
of all the WDs under the data queue stability and average power
constraints. We denote xt = [xt
1, · · · , τ t
N ],
(cid:3), and let x =
O = (cid:2)et
f t = [f t
O}K
t=1, f = {f t}K
{xt}K
t=1. We
formulate the problem as a multi-stage stochastic MINLP

1,O, · · · , et
t=1 and eO = {et

1, · · · , f t
t=1, τ = {τ t}K

N ], τ t = [τ t
N,O

N ] and et

1, · · · , xt

lim
K→∞

1/K · (cid:80)K

t=1

(cid:80)N

i=1cirt

i

(6a)

(6b)

(6c)

(cid:19)

≤ Qi(t), ∀t, i,

(6d)

+ xt

iet

i,O

(cid:105)

≤ γi, ∀i,

(6e)

(6f)

i ∈ {0, 1} , ∀i, t,
(cid:18)

log2

(1 − xt

1 +

i,Oht
et
i
τ t
i N0
(cid:1)3
i)κ (cid:0)f t

i

E [Qi(t)] < ∞, ∀i,

maximize
x,τ ,f ,eO
subject to
(cid:80)N
i=1τ t
(1 − xt
φ

lim
K→∞
lim
K→∞
i , f t
τ t

i ≤ 1, ∀t, xt
i)f t
iW τ t
xt
i
i
vu
E

+

1/K · (cid:80)K
1/K · (cid:80)K

t=1

(cid:104)

t=1
i,O ≥ 0, f t

i , et

, et
(6g)
Here, ci denotes the ﬁxed weight of the ith WD. (6c) denotes

i,O ≤ P max

i ≤ f max
i

τ t
i , ∀i, t.

i

i = et
the ofﬂoading time constraint. Notice that τ t
i,O = 0 must
hold at the optimum if xt
i = 0. Similarly, f t
i = 0 must hold if
xt
i = 1. (6d) corresponds to the data causality constraint. (6e)
corresponds to the average power constraint and γi > 0 is the
power threshold. (6f) are the data queue stability constraints.
Under the stochastic channels and data arrivals, it is hard to
satisfy the long-term constraints when the decisions are made
in each time frame without future knowledge. Besides, the fast-
varying channel condition requires real-time decision-making
in each short time frame, e.g., within the channel coherence
time. In the following, we propose a novel LyDROO framework
that solves (6) with both high robustness and efﬁciency.

III. LYAPUNOV-BASED MULTI-STAGE DECOUPLING

In this section, we apply the Lyapunov optimization to
decouple (6) into per-frame deterministic problems. To cope
with the average power constraints (6e), we introduce N virtual
energy queues {Yi(t)}N
i=1, one for each WD. Speciﬁcally, we
set Yi(1) = 0 and update the queue as
i − νγi, 0(cid:1) ,
Yi(t + 1) = max (cid:0)Yi(t) + νet
(7)
for i = 1, · · · , N, t = 1, 2, · · · , where et
i in (3) is the energy
consumption at the tth time frame and ν is a positive scaling
factor. Intuitively, when the virtual energy queues are stable,
the average power consumption et
i does not exceed γi, and
thus the constraints in (6e) are satisﬁed.

We deﬁne Z(t) = {Q(t), Y(t)} as the total queue backlog,
where Q(t) = {Qi(t)}N
i=1. Then, we
introduce the Lyapunov function L (Z(t)) and Lyapunov drift
∆L (Z(t)) as [12]

i=1 and Y(t) = {Yi(t)}N

(8)

(cid:16)(cid:80)N

L (Z(t)) = 0.5

i=1Qi(t)2 + (cid:80)N

i=1Yi(t)2(cid:17)
,
∆L (Z(t)) = E {L (Z(t + 1)) − L (Z(t)) |Z(t)} .
To maximize the time average computation rate while stabiliz-
ing Z(t), we use the drift-plus-penalty minimization approach
[12]. Speciﬁcally, we seek to minimize an upper bound on the
following drift-plus-penalty expression at every time frame t:
(9)
where V > 0 is an “importance” weight to scale the penalty.
The following Theorem 1 derives an upper bound of Λ (Z(t)).
Theorem 1: Given any queue backlog Z(t), the drift-plus-

Λ (Z(t)) (cid:44) ∆L (Z(t)) − V · (cid:80)N

i |Z(t)(cid:9) ,

E (cid:8)cirt

i=1

penalty in (9) is upper bounded as

Λ (Z(t)) ≤ ˆB + (cid:80)N
(cid:8)Yi(t)E (cid:2)et
+ (cid:80)N

i=1Qi(t)E (cid:2)(cid:0)At
i − γi|Z(t)(cid:3) − V E (cid:2)cirt

i − Dt
i

i=1

(cid:1) |Z(t)(cid:3)

i |Z(t)(cid:3)(cid:9) ,

(10)

where ˆB is a ﬁnite constant.

in the online technical report [16].

Proof : Due to the page limit, please refer the detailed proof
(cid:4)
In the tth time frame, we apply the technique of opportunistic
expectation minimization [12]. That is, we observe the queue
backlogs Z(t) and decide the joint ofﬂoading and resource
allocation control action accordingly to minimize the upper
bound in (10). By removing the constant terms from the
observation at the beginning of the tth time frame, the algorithm
decides the actions by maximizing the following:
i − (cid:80)N
i=1Yi(t)et
i=1 (Qi(t) + V ci) rt
i,
(11)
where rt
i are in (3). Intuitively, it tends to increase the
computation rates of WDs that have a long data queue backlog

(cid:80)N
i and et

3

or a large weight, while penalizing those that have exceeded the
average power threshold. We introduce an auxiliary variable
i,O for each WD i and denote rt
rt
. Taking
into account the per-frame constraints, we solve the following
deterministic per-frame subproblem in the tth time frame
i=1 (Qi(t) + V ci) rt

O = (cid:8)rt

i=1Yi(t)et

i − (cid:80)N

(12a)

(cid:80)N

(cid:9)N

i=1

i,O

i

maximize
O ,rt
O

xt,τ t,f t,et

subject to
(cid:80)N

i=1τ t
i ≤ 1,
i /φ ≤ Qi(t), rt
f t
W τ t
i
vu
f t
, et
i ≤ f max
i
i ∈ {0, 1} , τ t
xt

rt
i,O ≤

log2

1 +

i,O ≤ Qi(t), ∀i,
(cid:19)
(cid:18)
et
i,Oht
i
τ t
i N0
i,O ≤ P max
τ t
i , ∀i,
i
i , et
i , f t
i,O ≥ 0, ∀i.

, ∀i,

(12b)

(12c)

(12d)

(12e)

(12f)

(12g)

Notice that the above constraints (12d) and (12e) are equivalent
to (6d), because there is exactly one non-zero term in the left-
hand side of (6d) at the optimum. It can be shown that, for a
feasible (6) under the data arrival rates and power constraints,
we can satisfy all long-term constraints in (6) by solving the
per-frame subproblems in an online fashion, where the detailed
proof is omitted due to the page limit. Then, the remaining
difﬁculty lies in solving the MINLP (12) in each time frame.

i, Qi(t), Yi(t)}N

IV. LYAPUNOV-GUIDED DRL FOR ONLINE OFFLOADING
Recall that to solve (12) in the tth time frame, we observe
ξt (cid:44) {ht
i=1, consisting of the channel gains
i}N
i=1 and the system queue states {Qi(t), Yi(t)}N
{ht
i=1, and
accordingly decide the control action {xt, yt}, including the
binary ofﬂoading decision xt and the continuous resource
allocation yt (cid:44) (cid:8)τ t

i,O
Suppose that the value of xt in (12) are given, we denote the
i = 1 as Mt
1, and the complementary
0. Then, the remaining optimal

index set of users with xt
user set with xt
resource allocation problem to optimize yt is as following
jf t
at

j /φ − Yj(t)κ (cid:0)f t

i = 0 as Mt

i,O, rt

i , f t

i , et

(13a)

(cid:1)3(cid:111)

(cid:9)N

i=1

(cid:80)

(cid:110)

.

j

maximize
O ,rt
τ t,f t,et
O

j∈Mt
0

+ (cid:80)

i∈Mt
1

subject to

(cid:8)at

irt

i,O − Yi(t)et

i,O

(cid:9)

(cid:80)

i ≤ 1, et
τ t
i , ∀i ∈ Mt
τ t
i,O ≤ P max
1,
i
i∈M1
f t
j /φ ≤ Qj(t), f t
, ∀j ∈ Mt
j ≤ f max
0,
j
i,O ≤ Qi(t), ∀i ∈ Mt
rt
1

(13b)

(13c)

(13d)

(13e)

(13f)

(cid:19)

(cid:18)

1 +

log2

(13g)

rt
i,O ≤

W τ t
i
vu

, ∀i ∈ Mt
1,

i,Oht
et
i
τ t
i N0
(cid:44) Qi(t) + V ci is a parameter. A close observation
where at
i
shows that although (12) is a non-convex optimization problem,
the resource allocation problem (13) is in fact a convex problem
if xt is ﬁxed. Here, we denote G (cid:0)xt, ξt(cid:1) as the optimal value
of (12) by optimizing yt given the ofﬂoading decision xt and
parameter ξt, i.e., the optimal value of (13), which can be
efﬁciently obtained from off-the-shelf or customized convex
algorithms. In this sense, solving (12) is equivalent to ﬁnding
the optimal ofﬂoading decision (xt)∗, where

(cid:0)xt(cid:1)∗

= arg maximize
xt∈{0,1}N
In general, obtaining (xt)∗ requires enumerating 2N ofﬂoading
decisions, which leads to signiﬁcantly high computational

(14)

G (cid:0)xt, ξt(cid:1) .

4

Fig. 1: The schematics of the proposed LyDROO algorithm.

complexity even when N is moderate (e.g., N = 10). Other
search based methods, such as branch-and-bound and block
coordinate descent, are also time-consuming when N is large.
In practice, neither method is applicable to online decision-
making under fast-varying channel condition. Leveraging the
DRL technique, we propose a LyDROO algorithm to construct
a policy π that maps from the input ξt to the optimal action
(xt)∗, i.e., π : ξt (cid:55)→ (xt)∗, with very low complexity, e.g., tens
of milliseconds execution delay for N = 10. As illustrated in
Fig. 1, LyDROO consists of four main modules that operate
in a sequential and iterative manner as detailed below.

1) Actor Module: The actor module consists of a DNN and
an action quantizer. Here, we apply a fully-connected multi-
layer perceptron of two hidden layers with 120 and 80 hidden
neurons, respectively. Besides, we use a sigmoid activation
function at the output layer. At the beginning of the tth time
frame, we denote the parameter of the DNN as θt, which is
randomly initialized following the standard normal distribution
when t = 1. With the input ξt, the DNN outputs a relaxed
ofﬂoading decision ˆxt ∈ [0, 1]N , where the input-out relation
is expressed as

Πθt : ξt (cid:55)→ ˆxt = (cid:8)ˆxt

i ∈ [0, 1], i = 1, · · · , N (cid:9) .

(15)
We then quantize the continuous ˆxt into Mt feasible candidate
binary ofﬂoading actions, denoted as
(cid:9) , (16)
ΥMt : ˆxt (cid:55)→ Ωt = (cid:8)xt
where Ωt denotes the set of candidate ofﬂoading actions in
the tth time frames. Notice that the number of binary actions
Mt = |Ωt| is a time-dependent design parameter.

j ∈ {0, 1}N , j = 1, · · · , Mt

j|xt

j

A good quantization function should balance the exploration-
exploitation tradeoff
in generating the ofﬂoading action to
(cid:9)’s should be
ensure good training convergence. Intuitively, (cid:8)xt
close to ˆxt (measured by Euclidean distance) to make effective
use of the DNN’s output and meanwhile sufﬁciently separate
to avoid premature convergence to sub-optimal solution in
the training process. Here, we apply a noisy order-preserving
(NOP) quantization method, which can generate any Mt ≤ 2N
candidate actions. The NOP method generates the ﬁrst Mt/2
actions (Mt is assumed an even number) by applying the
order-preserving quantizer (OPQ) in [11] to ˆxt.1 To obtain

1Please refer to [16] for detailed quantization procedures.

the remaining Mt/2 actions, we ﬁrst generate a noisy version
of ˆxt denoted as ˜xt = Sigmoid (ˆxt + n), where the random
Gaussian noise n ∼ N (0, IN ) with IN being an identity
matrix, and Sigmoid (·) is the element-wise Sigmoid function
that bounds each entry of ˜xt within (0, 1). Then, we produce
the remaining Mt/2 actions xt
m, for m = Mt/2 + 1, · · · , Mt,
by applying the OPQ to ˜xt.

2) Critic Module: Followed by the actor module, the critic
module evaluates {xt
i} and selects the best ofﬂoading action
xt. LyDROO leverages the model information to evaluate the
binary ofﬂoading action by analytically solving the optimal
resource allocation problem, which leads to more robust and
faster convergence of the DRL training process. Speciﬁcally,
LyDROO selects the best action xt as
G (cid:0)xt

(17)

j, ξt(cid:1) ,

xt = arg max
j ∈Ωt

xt

j, ξt(cid:1) is obtained by optimizing the resource
where G (cid:0)xt
allocation given xt
j in (13). Intuitively, a larger Mt = |Ωt|
results in better solution performance, but a higher execution
delay. To balance the performance-complexity tradeoff, we
propose here an adaptive procedure to set a time-varying Mt.
Denote mt ∈ [0, Mt − 1] as the index of the best action
xt ∈ Ωt. We deﬁne m∗
t = mod(mt, Mt/2), which represents
the order of xt among either the Mt/2 noise-free or the noise-
added candidate actions. The key idea is that m∗
t gradually
decreases as the actor DNN gradually approaches the optimal
policy over time. In practice, we set a maximum M1 = 2N
initially and update Mt every δM ≥ 1 time frames as
(cid:1) + 1, N (cid:1) .

(18)
The additional 1 in the ﬁrst term within the min operator allows
Mt to increase over time.

Mt = 2 · min (cid:0)max (cid:0)m∗

t−1, · · · , m∗

t−δM

3) Policy Update Module: LyDROO uses (cid:0)ξt, xt(cid:1) as a
labeled input-output sample for updating the policy of the DNN.
In particular, we maintain a replay memory that only stores
the most recent q data samples. In practice, with an initially
empty memory, we start training the DNN after collecting more
than q/2 data samples. Then, the DNN is trained periodically
once every δT time slots to avoid model over-ﬁtting. When
mod (t, δT ) = 0, we randomly select a batch of data samples
{(ξτ , xτ ) , τ ∈ S t}, where S t denotes the time indices of the

........................︙ ︙ arg Max︙ Training samplesSample random batchCompute optimal resource allocation problem (13)NOP Action QuantizationAction selectionReplay MemoryLabeled sampleTrain the DNN1) Actor module2) Critic module3) Policy update module{,}ttξx11{,}ttξx22{,}ttξx{,}jjξx,ttxy1,(),()NttiiiihQtYtξttξˆtx1txttMx,tttMGxξ1,ttGxξtx4) Queueing module1{(t),(t)}NiiiQY1{}tNiih1{A}tNii1{,}ttiiNiDeInput of DNNData processed and energy consumptionChannel gainsData arrivalsExecute the joint actionJoint actionUpdate queuesselected samples. We then update the parameter of the DNN
by minimizing its average cross-entropy loss function over
the data samples. When the training completes, we update the
parameter of the actor module in the next time frame to θt+1.
4) Queueing Module: As a by-product of the critic module,
we obtain the optimal resource allocation yt associated with xt.
Accordingly, the system executes the joint computation ofﬂoad-
ing and resource allocation action {xt, yt}, which processes
data {Dt
i=1 as given in (3).
Based on {Dt
i=1 observed
in the tth time frame, the queueing module then updates the
data and energy queues {Qi(t + 1), Yi(t + 1)}N
i=1 using (5)
and (7) at the beginning of the (t + 1)th time frame. With the
wireless channel gains observation {ht+1
}N
i=1, the system feeds
, Qi(t + 1), Yi(t + 1)(cid:9)N
the input parameter ξt+1 = (cid:8)ht+1
to the DNN and starts a new iteration from Step 1).

i=1 and consumes energy {et

i=1 and the data arrivals {At

i, et

i}N

i}N

i}N

i}N

i=1

i

i

With the above actor-critic-update loop, the DNN consis-
tently learns from the best and most recent state-action pairs,
leading to a better policy πθt that gradually approximates the
optimal mapping to solve (14). We summarize the pseudo-code
of LyDROO in Algorithm 1. We can efﬁciently obtain the best
ofﬂoading solution xt in line 8 by solving convex resource
allocation problem in (13). Besides, the training process of the
DNN in line 10-13 is performed in parallel with the execution
of task ofﬂoading and computation, and thus does not incur
additional training delay overhead.

Algorithm 1: LyDROO algorithm for solving (6).

input

: Parameters V , {γi, wi}N
interval δM ;

i=1, K, training interval δT , Mt update

output : Control actions (cid:8)xt, yt(cid:9)K
t=1;
Initialize the DNN with random parameters θ1 and empty replay memory,

M1 ← 2N ;

Initial data and energy queue Qi(1) = Yi(1) = 0, for i = 1, · · · , N ;
for t = 1, 2, . . . , K do

Observe the input ξt = (cid:8)ht, Qi(t), Yi(t)(cid:9)N
(18) if mod (t, δM ) = 0;
Generate a relaxed ofﬂoading action ˆxt = Πθt
Quantize ˆxt into binary actions (cid:8)xt
method;

i|i = 1, · · · , Mt

i=1 and update Mt using
(cid:0)ξt(cid:1) with the DNN;

(cid:9) using the NOP

Compute G (cid:0)xt
Select the best solution xt with (17) and execute the joint action

i, ξt(cid:1) by optimizing yt

i in (13) for each xt
i;

(cid:0)xt, yt(cid:1);

Update the replay memory by adding (ξt, xt);
if mod (t, δT ) = 0 then

Uniformly sample a data set {(ξτ , xτ ) | τ ∈ St} from the

memory;

Train the DNN with {(ξτ , xτ ) | τ ∈ St} and update θt;

end
t ← t + 1;
Update {Qi(t), Yi(t)}N
(cid:111)N

(cid:110)

observation

At−1
i

i=1

i=1 based on (cid:0)xt−1, yt−1(cid:1) and data arrival

using (5) and (7).

1

2

3

4

5

6

7

8

9

10

11

12

13

14

15

16 end

V. SIMULATIONS

In this section, we use simulations to evaluate the perfor-
mance of the proposed LyDROO algorithm. All the compu-
tations are evaluated on a TensorFlow 2.0 platform with an
Intel Core i5-4570 3.2GHz CPU and 12 GB of memory. We
assume that the average channel gain ¯hi follows a path-loss
model ¯hi = Ad
, i = 1, · · · , N , where Ad = 3
denotes the antenna gain, fc = 915 MHz denotes the carrier
frequency, de = 3 denotes the path loss exponent, and di

(cid:16) 3×108
4πfcdi

(cid:17)de

5

i

i

= 0.3 GHz, P max

in meters denotes the distance between the ith WD and the
ES. hi follows an i.i.d. Rician distribution with line-of-sight
link gain equal to 0.3¯hi. The noise power N0 = W υ0 with
noise power spectral density υ0 = −174 dBm/Hz. Unless
otherwise stated, we consider N = 10 WDs equally spaced
with di = 120 + 15(i − 1), for i = 1, · · · , N . The weight
wi = 1.5 if i is an odd number and wi = 1 otherwise.
Other parameter values are set as: T = 1 second, W = 2
MHz, f max
= 0.1 watt, γi = 0.08 watt,
V = 20, φ = 100, ν = 1000, q = 1024, δT = 10, δM = 32,
|S t| = 32. For performance comparison, we consider two
benchmark methods: 1) Lyapunov-guided Coordinate Decent
(LyCD): The only difference with LyDROO is that LyCD
applies the coordinate decent (CD) method [6] to solve (12),
which iteratively applies one-dimensional search to update
the binary ofﬂoading decision vector xt. We have veriﬁed
through extensive simulations that the CD method achieves
close-to-optimal performance of solving (12). Therefore, we
consider LyCD as a target benchmark of LyDROO. 2) Myopic
optimization [11]: The Myopic method neglects the data queue
backlogs and maximizes the weighted sum computation rate
in each time frame t by solving

maximize
O ,rt
O

xt,τ t,f t,et

(cid:80)N

i=1cirt

i

i, ∀i,

l=1 el

l=1 el

subject to

(12c) − (12g), et

i ≤ tγi − (cid:80)t−1

i ≤ tγi − (cid:80)t−1
where et
i ensures that the average power
constraint in (6e) is satisﬁed up to the tth time frame and
(cid:8)el

i|l < t(cid:9) is the known past energy consumptions.
In Fig. 2(a)-(c), we evaluate the convergence of proposed
LyDROO and the two benchmark methods. We consider two
data arrival rates with λi = 2.5 and 3 Mbps for all i, and plot
the average data queue length, and average power consumption,
and weighted sum computation rate over time. We consider
i.i.d. random realizations in 10, 000 time frames, where each
point in the ﬁgure is a moving-window average of 200 time
frames. In Fig. 2(a), we observe that for a low data arrival rate
λi = 2.5, all the schemes maintain the data queues stable and
achieve similar computation rate performance. Besides, they
all satisfy the average power constraint 0.08 W in Fig. 2(b).
Meanwhile, they achieve the identical rate performance in
Fig. 2(c). When we increase λi to 3, the average data queue
length of the Myopic method increases almost linearly with
time, indicating that the data arrival rate has surpassed the
computation capacity (i.e., achievable sum computation rate).
On the other hand, both the LyCD and LyDROO methods
can stabilize the data queues. In between, the LyCD method
maintains lower data queue length over all time frames. The
data queue length of LyDROO experiences quick increase
when t ≤ 3, 000. However, as the embedded DNN gradually
approaches the optimal policy, it quickly drops and eventually
converges to the similar queue length and rate performance as
the LyCD method after around t = 7, 500, indicating its fast
convergence even under highly dynamic queueing systems.

In Fig. 2(d), we vary the data arrival rate λi from 2.5 to
3.2 Mbps, and plot the performance after convergence (points
with inﬁnite data queue length are not plotted). We omit the
results for λi > 3.2 because we observe that none of the three

6

Fig. 2: Convergence performance (a)-(c) and the impact of data arrival rate λi (d) of different schemes.

schemes can maintain queue stability, i.e., arrival rate surpasses
the achievable sum computation rate. All the three schemes
satisfy the average power constraints under different λi. The
data queues are stable with LyCD and LyDROO under all the
considered λi, while the queue lengths of the Myopic scheme
become inﬁnite when λi > 2.7. The results show that both
LyDROO and LyCD achieve much larger stable capacity region
than the Myopic method, and thus are more robust under heavy
workload. We also observe that LyCD and LyDROO achieve
identical computation rate performance in all the considered
cases. This is because when the data queues are long-term
stable, the average computation rate of the ith WD (departures
rate of the data queue) equals the data arrival rate λi, and
thus the achievable average weighted sum computation rate is
(cid:80)N
i=1 ciλi for both schemes. In fact, this also indicates that
both LyDROO and LyCD achieve the optimal computation
rate performance in all the considered setups.

We further examine the computational complexity. Here,
we consider a ﬁxed total network workload 30 Mbps and
equally allocate λi = 30/N to each WD for N ∈ {10, 20, 30}.
The locations of the N WDs are evenly spaced within
[120, 255] meters distance to the ES. The two LyDROO and
LyCD method achieve similar computation rate performance
for all N and all
the long-term constraints are satisﬁed.
In terms of execution delay, LyDROO and LyCD take on
average {0.021, 0.108, 0.156} and {0.27, 2.57, 8.02} seconds
to generate an ofﬂoading decision for N ∈ {10, 20, 30},
respectively. LyCD consumes acceptable latency when N = 10,
but signiﬁcantly long latency when N = 30, e.g., around
50 times longer than that of LyDROO. Because the channel
coherence time of a common indoor IoT system is no larger
than several seconds, the long execution latency makes LyCD
costly even infeasible in a practical MEC system with online
ofﬂoading decision. The proposed LyDROO algorithm, in
contract, incurs very short latency overhead, e.g., around 3%
overhead when the time frame is 5 seconds for N = 30.
Therefore, the LyDROO algorithm can be efﬁciently applied
in an MEC system under fast channel variation.

VI. CONCLUSIONS
In this paper, we have studied an online stable computation
ofﬂoading problem in a multi-user MEC network under stochas-
tic wireless channel and task data arrivals. We formulated a
multi-stage stochastic MINLP problem that maximizes the
average weighted sum computation rate of all the WDs under

long-term queue stability and average power constraints. To
tackle the problem, we proposed a LyDROO framework that
combines the advantages of Lyapunov optimization and DRL.
We show that the proposed approach can achieve optimal
computation rate performance meanwhile satisfying all the
long-term constraints. Besides, its incurs very low execution
delay in generating an online action, and converges within
relatively small number of iterations.

REFERENCES

[1] Y. Mao, C. You, J. Zhang, K. Huang, and K. B. Letaief, “A survey on
mobile edge computing: the communication perspective,” IEEE Commun.
Surveys Tuts., vol. 19, no. 4, pp. 2322-2358, Aug. 2017.

[2] W. Zhang, Y. Wen, K. Guan, D. Kilper, H. Luo, and D. O. Wu, “Energy-
optimal mobile cloud computing under stochastic wireless channel,” IEEE
Trans. Wireless Commun., vol. 12, no. 9, pp. 4569-4581, Sep. 2013.
[3] C. You, K. Huang, and H. Chae, “Energy efﬁcient mobile cloud
computing powered by wireless energy transfer,” IEEE J. Sel. Areas
Commun., vol. 34, no. 5, pp. 1757-1771, May 2016.

[4] S. Bi, L. Huang, and Y. J. Zhang, “Joint optimization of service
caching placement and computation ofﬂoading in mobile edge computing
systems,” IEEE Trans. Wireless Commun., vol. 19, no. 7, pp. 4947-4963,
Jul. 2020.

[5] J. Yan, S. Bi, Y. J. Zhang, and M. Tao, “Optimal task ofﬂoading
and resource allocation in mobile-edge computing with inter-user task
dependency,” IEEE Trans. Wireless Commun., vol. 19, no. 1, pp. 235-250,
Jan. 2020.

[6] S. Bi and Y. J. Zhang, “Computation rate maximization for wireless
powered mobile-edge computing with binary computation ofﬂoading,”
IEEE Trans. Wireless Commun., vol. 17, no. 6, pp. 4177-4190, Jun. 2018.
[7] T. Q. Dinh, J. Tang, Q. D. La, and T. Q. Quek, “Ofﬂoading in mobile
edge computing: task allocation and computational frequency scaling,”
IEEE Trans. Commun., vol. 65, no. 8, pp. 3571-3584, Aug. 2017.
[8] M. Min, L. Xiao, Y. Chen, P. Cheng, D. Wu, and W. Zhuang, “Learning-
based computation ofﬂoading for IoT devices with energy harvesting,”
IEEE Trans. Veh. Technol., vol. 68, no. 2, pp. 1930-1941, Feb. 2019.
[9] X. Chen, H. Zhang, C. Wu, S. Mao, Y. Ji, and M. Bennis, “Optimized
computation ofﬂoading performance in virtual edge computing systems
via deep reinforcement learning”, IEEE Internet Things J., vol. 6, no. 3,
pp. 4005-4018, Jun. 2019.

[10] Y. Wei, F. R. Yu, M. Song, and Z. Han, “Joint optimization of caching,
computing, and radio resources for fog-enabled IoT using natural actor-
critic deep reinforcement learning,” IEEE Internet Things J., vol. 6, no. 2,
pp. 2061-2073, Apr. 2019.

[11] L. Huang, S. Bi, and Y. J. Zhang, “Deep reinforcement learning for online
computation ofﬂoading in wireless powered mobile-edge computing
networks,” IEEE Trans. Mobile Compt., vol. 19, no. 11, pp. 2581-2593,
Nov. 2020.

[12] M. J. Neely, “Stochastic network optimization with application to com-
munication and queueing systems,” Synthesis Lectures on Communication
Networks, vol. 3, no. 1, pp. 1-211, 2010.

[13] Y. Mao, J. Zhang, and K. B. Letaief, “Dynamic computation ofﬂoading
for mobile-edge computing with energy harvesting devices,” IEEE J. Sel.
Areas in Commun., vol. 34, no. 12, pp. 3590-3605, Dec. 2016.

10002000300040005000600070008000900010000Average Data Queue Length (Mb)2004006008001000(cid:1)(cid:1)(cid:2)(cid:3)(cid:4)(cid:5)(cid:6)(cid:7)(cid:7)(cid:3)(cid:4)(cid:8)(cid:5)(cid:9)(cid:4)(cid:10)(cid:11)(cid:12)(cid:13)(a)Time Frames10002000300040005000600070008000900010000510152025(cid:1)(cid:1)(cid:14)(cid:1)(cid:15)900095001000030405010002000300040005000600070008000900010000Average Power Consumption (watt)0.0740.0760.0780.08(cid:1)(cid:1)(cid:2)(cid:3)(cid:4)(cid:5)(cid:6)(cid:7)(cid:7)(cid:3)(cid:4)(cid:8)(cid:5)(cid:9)(cid:4)(cid:10)(cid:11)(cid:12)(cid:13)(b)Time Frames100020003000400050006000700080009000100000.0650.070.0750.08(cid:1)(cid:1)(cid:14)(cid:1)(cid:15)10002000300040005000600070008000900010000Weighted Sum Computation Rate (Mbps)3536373839(cid:1)(cid:1)(cid:2)(cid:3)(cid:4)(cid:5)(cid:6)(cid:7)(cid:7)(cid:3)(cid:4)(cid:8)(cid:5)(cid:9)(cid:4)(cid:10)(cid:11)(cid:12)(cid:13)(c)Time Frames1000200030004000500060007000800090001000030313233(cid:1)(cid:1)(cid:14)(cid:1)(cid:15)2.52.62.72.82.933.13.20100200300Average Data Queue Length (Mb)(cid:1)(cid:2)(cid:3)(cid:4)(cid:5)(cid:5)(cid:1)(cid:2)(cid:6)(cid:3)(cid:7)(cid:2)(cid:8)(cid:9)(cid:10)(cid:11)2.52.62.72.82.933.13.20.0650.070.0750.08Average Energy Consumption (watt)(cid:12)d(cid:14) (cid:15)(cid:16)(cid:16)(cid:10)(cid:17)(cid:13)(cid:18) (cid:4)(cid:13)(cid:19)(cid:20) (cid:1)(cid:1)2.52.62.72.82.933.13.2303234363840Average Weighted Sum Computation Rate (Mbps)[14] J. Du, F. R. Yu, X. Chu, J. Feng, and G. Lu, “Computation ofﬂoading
and resource allocation in vehicular networks based on dual-side cost
minimization,” IEEE Trans. Veh. Technol., vol. 68, no. 2, pp. 1079-1092,
Feb. 2019.

[15] C. Liu, M. Bennis, M. Debbah, and H. V. Poor, “Dynamic task ofﬂoading
and resource allocation for ultra-reliable low-latency edge computing,”
IEEE Trans. Commun., vol. 67, no. 6, pp. 4132-4150, Jun. 2019.
[16] S. Bi, L. Huang, H. Wang, and Y. J. Zhang, “Lyapunov-guided
stable online computation of-
deep reinforcement
ﬂoading in mobile-edge computing networks,” online available
https://arxiv.org/abs/2010.01370

learning for

7

