8
1
0
2

r
p
A
7
1

]
I

A
.
s
c
[

1
v
8
5
4
6
0
.
4
0
8
1
:
v
i
X
r
a

Deep Probabilistic Programming Languages: A Qualitative Study

Guillaume Baudart
IBM Research
guillaume.baudart@ibm.com

Martin Hirzel
IBM Research
hirzel@us.ibm.com

Louis Mandel
IBM Research
lmandel@us.ibm.com

ABSTRACT
Deep probabilistic programming languages try to combine the ad-
vantages of deep learning with those of probabilistic programming
languages. If successful, this would be a big step forward in ma-
chine learning and programming languages. Unfortunately, as of
now, this new crop of languages is hard to use and understand. This
paper addresses this problem directly by explaining deep proba-
bilistic programming languages and indirectly by characterizing
their current strengths and weaknesses.

CCS CONCEPTS
• Theory of computation → Probabilistic computation;
• Computing methodologies → Neural networks;
• Software and its engineering → Domain specific languages;

KEYWORDS
DL, PPL, DSL

1 INTRODUCTION
A deep probabilistic programming language (PPL) is a language
for specifying both deep neural networks and probabilistic models.
In other words, a deep PPL draws upon programming languages,
Bayesian statistics, and deep learning to ease the development of
powerful machine-learning applications.

For decades, scientists have developed probabilistic models in
various fields of exploration without the benefit of either dedicated
programming languages or deep neural networks [12]. But since
these models involve Bayesian inference with often intractable
integrals, they sap the productivity of experts and are beyond the
reach of non-experts. PPLs address this issue by letting users express
a probabilistic model as a program [15]. The program specifies how
to generate output data by sampling latent probability distributions.
The compiler checks this program for type errors and translates it
to a form suitable for an inference procedure, which uses observed
output data to fit the latent distributions. Probabilistic models show
great promise: they overtly represent uncertainty [6] and they have
been demonstrated to enable explainable machine learning even in
the important but difficult case of small training data [21, 26, 30].
Over the last few years, machine learning with deep neural net-
works (deep learning, DL) has become enormously popular. This is
because in several domains, DL solves what was previously a vexing
problem [10], namely manual feature engineering. Each layer of
a neural network can be viewed as learning increasingly higher-
level features. In other words, the essence of DL is automatic hier-
archical representation learning [22]. Hence, DL powered recent
breakthrough results in accurate supervised large-data tasks such
as image recognition [20] and natural language translation [33].
Today, most DL is based on frameworks that are well-supported,
efficient, and expressive, such as TensorFlow [1] and PyTorch [11].

These frameworks provide automatic differentiation (users need not
manually calculate gradients for gradient descent), GPU support
(to efficiently execute vectorized computations), and Python-based
embedded domain-specific languages [18].

Deep PPLs, which have emerged just recently [29–32], aim to
combine the benefits of PPLs and DL. Ideally, programs in deep
PPLs would overtly represent uncertainty, yield explainable models,
and require only a small amount of training data; be easy to write
in a well-designed programming language; and match the break-
through accuracy and fast training times of DL. Realizing all of
these promises would yield tremendous advantages. Unfortunately,
this is hard to achieve. Some of the strengths of PPLs and DL are
seemingly at odds, such as explainability vs. automated feature
engineering, or learning from small data vs. optimizing for large
data. Furthermore, the barrier to entry for work in deep PPLs is
high, since it requires non-trivial background in fields as diverse
as statistics, programming languages, and deep learning. To tackle
this problem, this paper characterizes deep PPLs, thus lowering the
barrier to entry, providing a programming-languages perspective
early when it can make a difference, and shining a light on gaps
that the community should try to address.

This paper uses the Stan PPL as a representative of the state of
the art in regular (not deep) PPLs [9]. Stan is a main-stream, mature,
and widely-used PPL: it is maintained by a large group of developers,
has a yearly StanCon conference, and has an active forum. Stan is
Turing complete and has its own stand-alone syntax and semantics,
but provides bindings for several languages including Python.

Most importantly, this paper uses Edward [31] and Pyro [32] as
representatives of the state of the art in deep PPLs. Edward is based
on TensorFlow and Pyro is based on PyTorch. Edward was first
released in mid-2016 and has a single main maintainer, who is fo-
cusing on a new version. Pyro is a much newer framework (released
late 2017), but seems to be very responsive to community questions.
This paper characterizes deep PPLs by explaining them (Sec-
tions 2, 3, and 4), comparing them to each other and to regular
PPLs and DL frameworks (Section 5), and envisioning next steps
(Section 6). Additionally, the paper serves as a comparative tutorial
to both Edward and Pyro. To this end, it presents examples of in-
creasing complexity written in both languages, using deliberately
uniform terminology and presentation style. By writing this paper,
we hope to help the research community contribute to the exciting
new field of deep PPLs, and ultimately, combine the strengths of
both DL and PPLs.

2 PROBABILISTIC MODEL EXAMPLE
This section explains PPLs using an example that is probabilistic
but not deep. The example, adapted from Section 9.1 of [3], is about
learning the bias of a coin. We picked this example because it is
simple, lets us introduce basic concepts, and shows how different
PPLs represent these concepts.

 
 
 
 
 
 
Figure 1: Graphical model for biased coin tosses. Circles rep-
resent random variables. The white circle for θ indicates that
it is latent and the gray circle for xi indicates that it is ob-
served. The arrow represents dependency. The rounded rec-
tangle is a plate, representing N distributions that are IID.

We write xi = 1 if the result of the ith coin toss is head and
xi = 0 if it is tail. We assume that individual coin tosses are inde-
pendent and identically distributed (IID) and that each toss follows
a Bernoulli distribution with parameter θ : p(xi = 1 | θ ) = θ and
p(xi = 0 | θ ) = 1 − θ . The latent (i.e., unobserved) variable θ is the
bias of the coin. The task is to infer θ given the results of previously
observed coin tosses, that is, p(θ | x1, x2, . . . , xN ). Figure 1 shows
the corresponding graphical model. The model is generative: once
the distribution of the latent variable has been inferred, one can
draw samples to generate data points similar to the observed data.

We now present this simple example in Stan, Edward, and Pyro.
In all these languages we follow a Bayesian approach: the program-
mer first defines a probabilistic model of the problem. Assumptions
are encoded with prior distributions over the variables of the model.
Then the programmer launches an inference procedure to automat-
ically compute the posterior distributions of the parameters of the
model based on observed data. In other words, inference adjusts
the prior distribution using the observed data to give a more pre-
cise model. Compared to other machine-learning models such as
deep neural networks, the result of a probabilistic program is a
probability distribution, which allows the programmer to explicitly
visualize and manipulate the uncertainty associated with a result.
This overt uncertainty is an advantage of PPLs. Furthermore, a
probabilistic model has the advantage that it directly describes the
corresponding world based on the programmer’s knowledge. Such
descriptive models are more explainable than deep neural networks,
whose representation is big and does not overtly resemble the world
they model.

Figure 2(a) solves the biased-coin task in Stan, a well-established
PPL [9]. This example uses PyStan, the Python interface for Stan.
Lines 2-13 contain code in Stan syntax in a multi-line Python string.
The data block introduces observed random variables, which are
placeholders for concrete data to be provided as input to the in-
ference procedure, whereas the parameters block introduces latent
random variables, which will not be provided and must be inferred.
Line 4 declares x as a vector of ten discrete random variables, con-
strained to only take on values from a finite set, in this case, either
0 or 1. Line 7 declares θ as a continuous random variable, which can
take on values from an infinite set, in this case, real numbers be-
tween 0 and 1. Stan uses the tilde operator (~) for sampling. Line 10
samples θ from a uniform distribution (same probability for all val-
ues) between 0 and 1. Since θ is a latent variable, this distribution is
merely a prior belief, which inference will replace by a posterior dis-
tribution. Line 12 samples the xi from a Bernoulli distribution with
parameter θ . Since the xi are observed variables, this sampling is

2

really a check for how well the model corresponds to data provided
at inference time. One can think of sampling an observed variable
like an assertion in verification [15]. Line 15 specifies the data and
Lines 17-18 run the inference using the model and the data. By
default, Stan uses a form of Monte-Carlo sampling for inference [9].
Lines 20-22 extract and print the mean and standard deviation of
the posterior distribution of θ .

Figure 2(b) solves the same task in Edward [31]. Line 2 samples θ
from the prior distribution, and Line 3 samples a vector of random
variables from a Bernoulli distribution of parameter θ , one for each
coin toss. Line 5 specifies the data. Lines 7-8 define a placeholder
that will be used by the inference procedure to compute the poste-
rior distribution of θ . The shape and size of the placeholder depends
on the inference procedure. Here we use the Hamiltonian Monte-
Carlo inference HMC, the posterior distribution is thus computed
based on a set of random samples and follows an empirical dis-
tribution. The size of the placeholder corresponds to the number
of random samples computed during inference. Lines 9-11 launch
the inference. The inference takes as parameter the prior:posterior
pair theta:qtheta and links the data to the variable x. Lines 13-16
extract and print the mean and standard deviation of the posterior
distribution of θ .

Figure 2(c) solves the same task in Pyro [32]. Lines 2-7 define
the model as a function coin. Lines 3-5 sample θ from the prior
distribution, and Lines 6-7 sample a vector of random variable x
from a Bernoulli distribution of parameter θ . Pyro stores random
variables in a dictionary keyed by the first argument of the func-
tion pyro.sample. Lines 9-10 define the data as a dictionary. Line 12
conditions the model using the data by matching the value of the
data dictionary with the random variables defined in the model.
Lines 13-15 apply inference to this conditioned model, using im-
portance sampling. Compared to Stan and Edward, we first define
a conditioned model with the observed data before running the
inference instead of passing the data as an argument to the infer-
ence. The inference returns a probabilistic model, post, that can be
sampled to extract the mean and standard deviation of the posterior
distribution of θ in Lines 17-19.

The deep PPLs Edward and Pyro are built on top of two popular
deep learning frameworks TensorFlow [1] and PyTorch [11]. They
benefit from efficient computations over large datasets, automatic
differentiation, and optimizers provided by these frameworks that
can be used to efficiently implement inference procedures. As we
will see in the next sections, this design choice also reduces the gap
between DL and probabilistic models, allowing the programmer
to combine the two. On the other hand, this choice leads to piling
up abstractions (Edward/TensorFlow/Numpy/Python or Pyro/Py-
Torch/Numpy/Python) that can complicate the code. We defer a
discussion of these towers of abstraction to Section 5.

Variational Inference
Inference, for Bayesian models, computes the posterior distribu-
tion of the latent parameters θ given a set of observations x, that
is, p(θ | x). For complex models, computing the exact posterior
distribution can be costly or even intractable. Variational inference
turns the inference problem into an optimization problem and tends

θxiN1 # Model
2 coin_code = """

data {
int<lower=0,upper=1> x[10];

}
parameters {
real<lower=0,upper=1> theta;

3

4

5

6

7

}
8
9 model {

10

11

12

theta ~ uniform(0,1);
for (i in 1:10)

x[i] ~ bernoulli(theta);

}"""

13
14 # Data
15 data = {'x': [0, 1, 0, 0, 0, 0, 0, 0, 0, 1]}
16 # Inference
17 fit = pystan.stan(model_code=coin_code,

data=data, iter=1000)

18
19 # Results
20 samples = fit.extract()['theta']
21 print("Posterior mean:", np.mean(samples))
22 print("Posterior stddev:", np.std(samples))

1 # Model
2 theta = Uniform(0.0, 1.0)
3 x = Bernoulli(probs=theta, sample_shape=10)
4 # Data
5 data = np.array([0, 1, 0, 0, 0, 0, 0, 0, 0, 1])
6 # Inference
7 qtheta = Empirical(

tf.Variable(tf.ones(1000) ∗ 0.5))

8
9 inference = ed.HMC({theta: qtheta},
data={x: data})

10
11 inference.run()
12 # Results
13 mean, stddev = ed.get_session().run(

[qtheta.mean(),qtheta.stddev()])

14
15 print("Posterior mean:", mean)
16 print("Posterior stddev:", stddev)

1 # Model
2 def coin():
3

theta = pyro.sample("theta", Uniform(

Variable(torch.Tensor([0])),
Variable(torch.Tensor([1])))

pyro.sample("x", Bernoulli(

4

5

6

theta ∗ Variable(torch.ones(10)))

7
8 # Data
9 data = {"x": Variable(torch.Tensor(

[0, 1, 0, 0, 0, 0, 0, 0, 0, 1]))}

10
11 # Inference
12 cond = pyro.condition(coin, data=data)
13 sampler = pyro.infer.Importance(cond,

num_samples=1000)

14
15 post = pyro.infer.Marginal(sampler, sites=["theta"])
16 # Result
17 samples = [post()["theta"].data[0] for _ in range(1000)]
18 print("Posterior mean:", np.mean(samples))
19 print("Posterior stddev:", np.std(samples))

(a) Stan

(b) Edward

(c) Pyro

Figure 2: Probabilistic model for learning the bias of a coin.

1 # Inference Guide
2 qalpha = tf.Variable(1.0)
3 qbeta = tf.Variable(1.0)
4 qtheta = Beta(qalpha, qbeta)
5 # Inference
6 inference = ed.KLqp({theta: qtheta}, {x: data})
7 inference.run()

1 # Inference Guide
2 def guide():
3

4

qalpha = pyro.param("qalpha", Variable(torch.Tensor([1.0]), requires_grad=True))
qbeta = pyro.param("qbeta", Variable(torch.Tensor([1.0]), requires_grad=True))
pyro.sample("theta", Beta(qalpha, qbeta))

5
6 # Inference
7 svi = SVI(cond, guide, Adam({}), loss="ELBO", num_particles=7)
8 for step in range(1000):
9

svi.step()

(a) Edward

(b) Pyro

Figure 3: Variational Inference for learning the bias of a coin.

to be faster and more adapted to large datasets than sampling-based
Monte-Carlo methods [4].

Variational infence tries to find the member q∗(θ ) of a family Q
of simpler distribution over the latent variables that is the closest
to the true posterior p(θ | x). The fitness of a candidate is measured
using the Kullback-Leibler (KL) divergence from the true posterior,
a similarity measure between probability distributions.

q∗(θ ) = argmin
q(θ )∈ Q

KL(q(θ ) || p(θ | x))

It is up to the programmer to choose a family of candidates, or
guides, that is sufficiently expressive to capture a close approxima-
tion of the true posterior, but simple enough to make the optimiza-
tion problem tractable.

Both Edward and Pyro support variational inference. Figure 3
shows how to adapt Figure 2 to use it. In Edward (Figure 3(a)), the

programmer defines the family of guides by changing the shape
of the placeholder used in the inference. Lines 2-4 use a beta dis-
tribution with unknown parameters α and β that will be com-
puted during inference. Lines 6-7 do variational inference using the
Kullback-Leibler divergence. In Pyro (Figure 3(b)), this is done by
defining a guide function. Lines 2-5 also define a beta distribution
with parameters α and β. Lines 7-9 do inference using Stochastic
Variational Inference, an optimized algorithm for variational infer-
ence. Both Edward and Pyro rely on the underlying framework to
solve the optimization problem. Probabilistic inference thus closely
follows the scheme used for training procedures of DL models.

This section gave a high-level introduction to PPLs and intro-
duced basic concepts (generative models, sampling, prior and pos-
terior, latent and observed, discrete and continuous). Next, we turn
our attention to deep learning.

3

the softmax computes a vector π of scores that add up to one. The
higher the value of πl , the more strongly the classifier predicts
label l. Using the output of the MLP, the argmax extracts the label l
with highest score.

Traditional methods to train such a neural network incrementally
update the latent parameters of the network to optimize a loss func-
tion via gradient descent [28]. In the case of hand-written digits, the
loss function is a distance metrics between observed and predicted
labels. The variables and computations are non-probabilistic, in
that they use concrete values rather than probability distributions.

Deep PPLs can express Bayesian neural networks with probabilis-
tic weights and biases [5]. One way to visualize this is by replacing
rectangles with circles for latent variables in Figure 4(a) to indicate
that they hold probability distributions. Figure 4(b) shows the cor-
responding graphical model, where the latent variable θ denotes
all the parameters of the MLP: W h , bh , W y , by .

Bayesian inference starts from prior beliefs about the parameters
and learns distributions that fit observed data (such as, images and
labels). We can then sample concrete weights and biases to obtain
a concrete MLP. In fact, we do not need to stop at a single MLP:
we can sample an ensemble of as many MLPs as we like. Then,
we can feed a concrete image to all the sampled MLPs to get their
predictions, followed by a vote.

Figure 5(a) shows the probabilistic MLP example in Edward.
Lines 3-4 are placeholders for observed variables (i.e., batches of
images x and labels l). Lines 5-9 defines the MLP parameterized by θ ,
a dictionary containing all the network parameters. Lines 10-14
sample the parameters from the prior distributions. Line 15 defines
the output of the network: a categorical distribution over all possible
label values parameterized by the output of the MLP. Line 17-23
define the guides for the latent variables, initialized with random
numbers. Later, the variational inference will update these during
optimization, so they will ultimately hold an approximation of
the posterior distribution after inference. Lines 25-29 set up the
inference with one prior:posterior pair for each parameter of the
network and link the output of the network to the observed data.
Figure 5(b) shows the same example in Pyro. Lines 2-11 contain
the basic neural network, where torch.nn.Linear wraps the low-level
linear algebra. Lines 3-6 declare the structure of the net, that is, the
type and dimension of each layer. Lines 7-10 combine the layers
to define the network. It is possible to use equivalent high-level
TensorFlow APIs for this in Edward as well, but we refrained from
doing so to illustrate the transition of the parameters to random
variables. Lines 14-20 define the model. Lines 15-18 sample priors
for the parameters, associating them with object properties created
by torch.nn.Linear (i.e., the weight and bias of each layer). Line 19
lifts the MLP definition from concrete to probabilistic. We thus
obtain a MLP where all parameters are treated as random variables.
Line 20 conditions the model using a categorical distribution over
all possible label values. Lines 26-31 define the guide for the latent
variables, initialized with random numbers, just like in the Edward
version. Line 33 sets up the inference.

After the inference, Figure 6 shows how to use the posterior
distribution of the MLP parameters to classify unknown data. In
Edward (Figure 6(a)), Lines 2-4 draw several samples of the param-
eters from the posterior distribution. Then, Lines 5-6 execute the

(a) Computational graph for non-probabilistic MLP.

(b) Graphical model for probabilistic MLP.

Figure 4: Multi-layer perceptron (MLP) for classifying
images. Circles and squares are probabilistic and non-
probabilistic variables. Black rectangles are pure functions.
Arrows represent dependencies and forward data flow.

3 PROBABILISTIC MODELS IN DL
This section explains DL using an example of a deep neural network
and shows how to make that probabilistic. The task is multiclass
classification: given input features x, e.g., an image of a handwritten
digit [23] comprising nx = 28 · 28 pixels, predict a label l, e.g., one
of ny = 10 digits. Before we explain how to solve this task using DL,
let us clarify some terminology. In cases where DL uses different
terminology from PPLs, this paper favors the PPL terminology. So
we say inference for what DL calls training; predicting for what DL
calls inferencing; and observed variables for what DL calls train-
ing data. For instance, the observed variables for inference in the
classifier task are the image x and label l.

Among the many neural network architectures suitable for this
task, we chose a simple one: a multi-layer perceptron (MLP [28]).
We start with the non-probabilistic version. Figure 4(a) shows an
MLP with a 2-feature input layer, a 3-feature hidden layer, and a
2-feature output layer; of course, this generalizes to wider (more
features) and deeper (more layers) MLPs. From left to right, there is
a fully-connected subnet where each input feature xi contributes to
each hidden feature hj , multiplied with a weight wji and offset with
a bias bj . The weights and biases are latent variables. Treating the
input, biases, and weights as vectors and a matrix lets us cast this
subnet in linear algebra xW h + bh , which can run efficiently via
vector instructions on CPUs or GPUs. Next, a rectified linear unit
ReLU (z) = max(0, z) computes the hidden feature vector h. The
ReLU lets the MLP discriminate input spaces that are not linearly
separable. The hidden layer exhibits both the advantage and dis-
advantage of deep learning: automatically learned features that
need not be hand-engineered but would require non-trivial reverse
engineering to explain in real-world terms. Next, another fully-
connected subnet hW y + by computes the output layer y. Then,

4

hw00*w01*b0+w10*w11*b1+w20*w21*b2+ReLUReLUReLUw00*w01*b0+w02*w10w11*b1+w12*y1*input layer(nx= 2)hidden layer(nh= 3)output layer(ny= 2)hhhhhhhhyyyyyyyysoftmaxlp0p1argmaxh0h1h2x0x1y0MLPqlNxpMLP1 # Model
2 class MLP(nn.Module):
def __init__(self):
3

super(MLP, self).__init__()
self.lh = torch.nn.Linear(nx, nh)
self.ly = torch.nn.Linear(nh, ny)

def forward(self, x):

4

5

6

7

8

9

16

17

18

19

h = F.relu(self.lh(x.view((−1, nx))))
log_pi = F.log_softmax(self.ly(h), dim=−1)
return log_pi

10
11 mlp = MLP()
12 def v0s(∗shape): return Variable(torch.zeros(∗shape))
13 def v1s(∗shape): return Variable(torch.ones(∗shape))
14 def model(x, l):
15

theta = { 'lh.weight': Normal(v0s(nh, nx), v1s(nh, nx)),
'lh.bias': Normal(v0s(nh), v1s(nh)),
'ly.weight': Normal(v0s(ny, nh), v1s(ny, nh)),
'ly.bias': Normal(v0s(ny), v1s(ny)) }
lifted_mlp = pyro.random_module("mlp", mlp, theta)()
pyro.observe("obs", Categorical(logits=lifted_mlp(x)), one_hot(l))

20
21 # Inference Guide
22 def vr(name, ∗shape):
23

return pyro.param(name,

Variable(torch.randn(∗shape), requires_grad=True))

24
25 def guide(x, l):
26

qtheta = {

27

28

29

30

'lh.weight': Normal(vr("Wh_m", nh, nx), F.softplus(vr("Wh_s", nh, nx))),
'lh.bias': Normal(vr("bh_m", nh), F.softplus(vr("bh_s", nh))),
'ly.weight': Normal(vr("Wy_m", ny, nh), F.softplus(vr("Wy_s", ny, nh))),
'ly.bias': Normal(vr("by_m", ny), F.softplus(vr("by_s", ny))) }

return pyro.random_module("mlp", mlp, qtheta)()

31
32 # Inference
33 inference = SVI(model, guide, Adam({"lr": 0.01}), loss="ELBO")

(b) Pyro

1 batch_size, nx, nh, ny = 128, 28 ∗ 28, 1024, 10
2 # Model
3 x = tf.placeholder(tf.float32, [batch_size, nx])
4 l = tf.placeholder(tf.int32, [batch_size])
5 def mlp(theta, x):
6

h = tf.nn.relu(tf.matmul(x, theta["Wh"]) + theta["bh"])
yhat = tf.matmul(h, theta["Wy"]) + theta["by"]
log_pi = tf.nn.log_softmax(yhat)
return log_pi

9
10 theta = {

'Wh': Normal(loc=tf.zeros([nx, nh]), scale=tf.ones([nx, nh])),
'bh': Normal(loc=tf.zeros(nh), scale=tf.ones(nh)),
'Wy': Normal(loc=tf.zeros([nh, ny]), scale=tf.ones([nh, ny])),
'by': Normal(loc=tf.zeros(ny), scale=tf.ones(ny)) }

14
15 lhat = Categorical(logits=mlp(theta, x))
16 # Inference Guide
17 def vr(∗shape):
18
19 qtheta = {

return tf.Variable(tf.random_normal(shape))

'Wh': Normal(loc=vr(nx, nh), scale=tf.nn.softplus(vr(nx, nh))),
'bh': Normal(loc=vr(nh), scale=tf.nn.softplus(vr(nh))),
'Wy': Normal(loc=vr(nh, ny), scale=tf.nn.softplus(vr(nh, ny))),
'by': Normal(loc=vr(ny), scale=tf.nn.softplus(vr(ny))) }

23
24 # Inference
25 inference = ed.KLqp({ theta["Wh"]: qtheta["Wh"],

26

27

28

29

theta["bh"]: qtheta["bh"],
theta["Wy"]: qtheta["Wy"],
theta["by"]: qtheta["by"] },
data={lhat: l})

(a) Edward

7

8

11

12

13

20

21

22

3

4

5

Figure 5: Probabilistic multilayer perceptron for classifying images.

1 def predict(x):
2

theta_samples = [ { "Wh": qtheta["Wh"].sample(), "bh": qtheta["bh"].sample(),
"Wy": qtheta["Wy"].sample(), "by": qtheta["by"].sample() }
for _ in range(args.num_samples) ]

yhats = [ mlp(theta_samp, x).eval()

6
7 mean = np.mean(yhats, axis=0)

for theta_samp in theta_samples ]

8

return np.argmax(mean, axis=1)

1 def predict(x):
2

sampled_models = [ guide(None)

yhats = [ model(Variable(x)).data

for _ in range (args.num_samples) ]

3

4

for model in sampled_models ]
5
6 mean = torch.mean(torch.stack(yhats), 0)

7

return np.argmax(mean, axis=1)

(a) Edward

(b) Pyro

Figure 6: Predictions by the probabilistic multilayer perceptron.

MLP with each concrete model. Line 7 computes the score of a label
as the average of the scores returned by the MLPs. Finally, Line 8
predicts the label with the highest score. In Pyro (Figure 6(b)), the
prediction is done similarly but we obtain multiple versions of the
MLP by sampling the guide (Line 2-3), not the parameters.

This section showed how to use probabilistic variables as build-
ing blocks for a DL model. Compared to non-probabilistic DL, this

5

approach has the advantage of reduced overfitting and accurately
quantified uncertainty [5]. On the other hand, this approach re-
quires inference techniques, like variational inference, that are
more advanced than classic back-propagation. The next section
will present the dual approach, showing how to use neural net-
works as building blocks for a probabilistic model.

4 DL IN PROBABILISTIC MODELS
This section explains how deep PPLs can use non-probabilistic
deep neural networks as components in probabilistic models. The
example task is learning a vector-space representation. Such a rep-
resentation reduces the number of input dimensions to make other
machine-learning tasks more manageable by counter-acting the
curse of dimensionality [10]. The observed random variable is x, for
instance, an image of a hand-written digit with nx = 28 · 28 pixels.
The latent random variable is z, the vector-space representation,
for instance, with nz = 4 features. Learning a vector-space repre-
sentation is an unsupervised problem, requiring only images but
no labels. While not too useful on its own, such a representation is
an essential building block. For instance, it can help in other image
generation tasks, e.g., to generate an image for a given writing
style [30]. Furthermore, it can help learning with small data, e.g.,
via a K-nearest neighbors approach in vector space [2].

Each image x depends on the latent representation z in a com-
plex non-linear way (i.e., via a deep neural network). The task is to
learn this dependency between x and z. The top half of Figure 7(a)
shows the corresponding graphical model. The output of the neural
network, named decoder, is a vector µ that parameterizes a Bernoulli
distribution over each pixel in the image x. Each pixel is thus asso-
ciated to a probability of being present in the image. Similarly to
Figure 4(b) the parameter θ of the decoder is global (i.e., shared by
all data points) and is thus drawn outside the plate. Compared to
Section 3 the network here is not probabilistic, hence the square
around θ .

The main idea of the VAE [19, 27] is to use variational inference to
learn the latent representation. As for the examples presented in the
previous sections, we need to define a guide for the inference. The
guide maps each x to a latent variable z via another neural network.
The bottom half of Figure 7(a) shows the graphical model of the
guide. The network, named encoder, returns, for each image x, the
parameters µz and σz of a Gaussian distribution in the latent space.
Again the parameter ϕ of the network is global and not probabilistic.
Then inference tries to learn good values for parameter θ and ϕ,
simultaneously training the decoder and the encoder, according to
the data and the prior beliefs on the latent variables (e.g., Gaussian
distribution).

After the inference, we can generate a latent representation of an
image with the encoder and reconstruct the image with the decoder.
The similarity of the two images gives an indication of the success
of the inference. The model and the guide together can thus be seen
as an autoencoder, hence the term variational autoencoder.

Figure 7(b) shows the VAE examples in Edward. Lines 4-12 define
the decoder: a simple 2-layers neural network similar to the one
presented in Section 3. The parameter θ is initialized with random
noise. Line 13 samples the priors for the latent variable z from a
Gaussian distribution. Lines 14-15 define the dependency between x
and z, as a Bernoulli distribution parameterized by the output of
the decoder. Lines 17-29 define the encoder: a neural network with
one hidden layer and two distinct output layers for µz and σz .
The parameter ϕ is also initialized with random noise. Lines 30-31
define the inference guide for the latent variable, that is, a Gaussian
distribution parameterized by the outputs of the encoder. Line 33

6

sets up the inference matching the prior:posterior pair for the latent
variable and linking the data with the output of the decoder.

Figure 7(c) shows the VAE example in Pyro. Lines 2-12 define
the decoder. Lines 13-19 define the model. Lines 14-16 sample the
priors for the latent variable z. Lines 18-19 define the dependency
between x and z via the decoder. In contrast to Figure 5(b), the de-
coder is not probabilistic, so there is no need for lifting the network.
Lines 34-37 define the guide as in Edward linking z and x via the
decoder defined Lines 21-33. Line 39 sets up the inference.

This example illustrates that we can embed non-probabilistic DL
models inside probabilistic programs and learn the parameters of
the DL models during inference. Sections 2, 3, and 4 were about
explaining deep PPLs with examples. The next section is about
comparing deep PPLs with each other and with their potential.

5 CHARACTERIZATION
This section attempts to answer the following research question:
At this point in time, how well do deep PPLs live up to their poten-
tial? Deep PPLs combine probabilistic models, deep learning, and
programming languages in an effort to combine their advantages.
This section explores those advantages grouped by pedigree and
uses them to characterize Edward and Pyro.

Before we dive in, some disclaimers are in order. First, both
Edward and Pyro are young, not mature projects with years of
improvements based on user experiences, and they enable new
applications that are still under active research. We should keep
this in mind when we criticize them. On the positive side, early
criticism can be a positive influence. Second, since getting even
a limited number of example programs to support direct side-by-
side comparisons was non-trivial, we kept our study qualitative.
A respectable quantitative study would require more programs
and data sets. On the positive side, all of the code shown in this
paper actually runs. Third, doing an outside-in study risks missing
subtleties that the designers of Edward and Pyro may be more
expert in. On the positive side, the outside-in view resembles what
new users experience.

5.1 Advantages from Probabilistic Models
Probabilistic models support overt uncertainty: they give not just a
prediction but also a meaningful probability. This is useful to avoid
uncertainty bugs [6], track compounding effects of uncertainty,
and even make better exploration decisions in reinforcement learn-
ing [5]. Both Edward and Pyro support overt uncertainty well, see
e.g. the lines under the comment “# Results” in Figure 2.

Probabilistic models give users a choice of inference procedures:
the user has the flexibility to pick and configure different approaches.
Deep PPLs support two primary families of inference procedures:
those based on Monte-Carlo sampling and those based on varia-
tional inference. Edward supports both and furthermore flexible
compositions, where different inference procedures are applied to
different parts of the model. Pyro supports primarily variational
inference and focuses less on Monte-Carlo sampling. In comparison,
Stan makes a form of Monte-Carlo sampling the default, focusing
on making it easy-to-tune in practice [8].

Probabilistic models can help with small data: even when in-
ference uses only small amount of labeled data, there have been

(a) Graphical models.

1 batch_size, nx, nh, nz = 256, 28 ∗ 28, 1024, 4
2 def vr(∗shape): return tf.Variable(0.01 ∗ tf.random_normal(shape))
3 # Model
4 X = tf.placeholder(tf.int32, [batch_size, nx])
5 def decoder(theta, z):
6

hidden = tf.nn.relu(tf.matmul(z, theta['Wh']) + theta['bh'])
mu = tf.matmul(hidden, theta['Wy']) + theta['by']
return mu

7

8
9 theta = { 'Wh': vr(nz, nh),

'bh': vr(nh),
'Wy': vr(nh, nx),
'by': vr(nx) }

12
13 z = Normal(loc=tf.zeros([batch_size, nz]), scale=tf.ones([batch_size, nz]))
14 logits = decoder(theta, z)
15 x = Bernoulli(logits=logits)
16 # Inference Guide
17 def encoder(phi, x):
18

x = tf.cast(x, tf.float32)
hidden = tf.nn.relu(tf.matmul(x, phi['Wh']) + phi['bh'])
z_mu = tf.matmul(hidden, phi['Wy_mu']) + phi['by_mu']
z_sigma = tf.nn.softplus(

tf.matmul(hidden, phi['Wy_sigma']) + phi['by_sigma'])

10

11

19

20

21

22

return z_mu, z_sigma

23
24 phi = { 'Wh': vr(nx, nh),

28

25

27

26

'bh': vr(nh),
'Wy_mu': vr(nh, nz),
'by_mu': vr(nz),
'Wy_sigma': vr(nh, nz),
'by_sigma': vr(nz) }
29
30 loc, scale = encoder(phi, X)
31 qz = Normal(loc=loc, scale=scale)
32 # Inference
33 inference = ed.KLqp({z: qz}, data={x: X})

4

5

6

7

8

9

10

15

16

17

18

23

24

25

26

27

28

29

30

31

36

1 # Model
2 class Decoder(nn.Module):
def __init__(self):
3

super(Decoder, self).__init__()
self.lh = nn.Linear(nz, nh)
self.lx = nn.Linear(nh, nx)
self.relu = nn.ReLU()

def forward(self, z):

hidden = self.relu(self.lh(z))
mu = self.lx(hidden)
return mu

11
12 decoder = Decoder()
13 def model(x):
14

z_mu = Variable(torch.zeros(x.size(0), nz))
z_sigma = Variable(torch.ones(x.size(0), nz))
z = pyro.sample("z", dist.Normal(z_mu, z_sigma))
pyro.module("decoder", decoder)
mu = decoder.forward(z)
pyro.sample("xhat", dist.Bernoulli(mu), obs=x.view(−1, nx))

19
20 # Inference Guide
21 class Encoder(nn.Module):
def __init__(self):
22

super(Encoder, self).__init__()
self.lh = torch.nn.Linear(nx, nh)
self.lz_mu = torch.nn.Linear(nh, nz)
self.lz_sigma = torch.nn.Linear(nh, nz)
self.softplus = nn.Softplus()

def forward(self, x):

hidden = F.relu(self.lh(x.view((−1, nx))))
z_mu = self.lz_mu(hidden)
z_sigma = self.softplus(self.lz_sigma(hidden))
return z_mu, z_sigma

32
33 encoder = Encoder()
34 def guide(x):
35

pyro.module("encoder", encoder)
z_mu, z_sigma = encoder.forward(x)
pyro.sample("z", dist.Normal(z_mu, z_sigma))

37
38 # Inference
39 inference = SVI(model, guide, Adam({"lr": 0.01}), loss="ELBO")

(b) Edward

(c) Pyro

Figure 7: Variational autoencoder for encoding and decoding images.

high-profile cases where probabilistic models still make accurate
predictions [21]. Working with small data is useful to avoid the cost
of hand-labeling, to improve privacy, to build personalized mod-
els, and to do well on underrepresented corners of a big-data task.
The intuition for how probabilistic models help is that they can
make up for lacking labeled data for a task by domain knowledge
incorporated in the model, by unlabeled data, or by labeled data
for other tasks. There are some promising initial successes of using

deep probabilistic programming on small data [26, 30]; at the same
time, this remains an active area of research.

Probabilistic models can support explainability: when the com-
ponents of a probabilistic model correspond directly to concepts
of a real-world domain being modeled, predictions can include an
explanation in terms of those concepts. Explainability is useful
when predictions are consulted for high-stakes decisions, as well
as for transparency around bias [7]. Unfortunately, the parameters

7

xNzµdecoderxNzµzencoderszfqmodel pq(x | z)guide qf(z | x)of a deep neural network are just as opaque with as without proba-
bilistic programming. There is cause for hope though. For instance,
Siddharth et al. advocate disentangled representations that help
explainability [30]. Overall, the jury is still out on the extent to
which deep PPLs can leverage this advantage from PPLs.

5.2 Advantages from Deep Learning
Deep learning is automatic hierarchical representation learning [22]:
each unit in a deep neural network can be viewed as an automati-
cally learned feature. Learning features automatically is useful to
avoid the cost of engineering features by hand. Fortunately, this DL
advantage remains true in the context of a deep PPL. In fact, a deep
PPL makes the trade-off between automated and hand-crafted fea-
tures more flexible than most other machine-learning approaches.
Deep learning can accomplish high accuracy: for various tasks,
there have been high-profile cases where deep neural networks
beat out earlier approaches with years of research behind them.
Arguably, the victory of DL at the ImageNet competition in 2012
ushered in the latest craze around DL [20]. Record-breaking ac-
curacy is useful not just for publicity but also to cross thresholds
where practical deployments become desirable, for instance, in
machine translation [33]. Since a deep PPL can use deep neural
networks, in principle, it inherits this advantage from DL [31].
However, even non-probabilistic DL requires tuning, and in our
experience with the example programs in this paper, the tuning
burden is exacerbated with variational inference.

Deep learning supports fast inference: even for large models and
a large data set, the wall-clock time for a batch job to infer pos-
teriors is short. The fast-inference advantage is the result of the
back-propagation algorithm [28], novel techniques for paralleliza-
tion [25] and data representation [17], and massive investments in
the efficiency of DL frameworks such as TensorFlow and PyTorch,
with vectorization on CPU and GPU. Fast inference is useful for it-
erating more quickly on ideas, trying more hyperparameter during,
and wasting fewer resources. Tran et al. measured the efficiency of
the Edward deep PPL, demonstrating that it does benefit from the
efficiency advantage of the underlying DL framework [31].

5.3 Advantages from Programming Languages
Programming language design is essential for composability: bigger
models can be composed from smaller components. Composabil-
ity is useful for testing, team-work, and reuse. Conceptually, both
graphical probabilistic models and deep neural networks compose
well. On the other hand, some PPLs impose structure in a way that
reduces composability; fortunately, this can be mitigated [16]. Both
Edward and Pyro are embedded in Python, and, as our example
programs demonstrate, work with Python functions and classes.
For instance, users are not limited to declaring all latent variables
in one place; instead, they can compose models, such as MLPs, with
separately declared latent variables. Edward and Pyro also work
with higher-level DL framework modules such as tf.layers.dense or
torch.nn.Linear, and Pyro even supports automatically lifting those
to make them probabilistic. Edward and Pyro also do not prevent
users from composing probabilistic models with non-probabilistic
code, but doing so requires care. For instance, when Monte-Carlo

8

sampling runs the same code multiple times, it is up to the pro-
grammer to watch out for unwanted side-effects. One area where
more work is needed is the extensibility of Edward or Pyro itself [8].
Finally, in addition to composing models, Edward also emphasizes
composing inference procedures.

Not all PPLs have the same expressiveness: some are Turing com-
plete, others not [15]. For instance, BUGS is not Turing complete,
but has nevertheless been highly successful [13]. The field of deep
probabilistic programming is too young to judge which levels of
expressiveness are how useful. Edward and Pyro are both Turing
complete. However, Edward makes it harder to express while-loops
and conditionals than Pyro. Since Edward builds on TensorFlow,
the user must use special APIs to incorporate dynamic control con-
structs into the computational graph. In contrast, since Pyro builds
on PyTorch, it can use native Python control constructs, one of the
advantages of dynamic DL frameworks [24].

Programming language design affects conciseness: it determines
whether a model can be expressed in few lines of code. Conciseness
is useful to make models easier to write and, when used in good
taste, easier to read. In our code examples, Edward is more concise
than Pyro. Pyro arguably trades conciseness for structure, making
heavier use of Python classes and functions. Wrapping the model
and guide into functions allows compiling them into co-routines,
an ingredient for implementing efficient inference procedures [14].
In both Edward and Pyro, conciseness is hampered by the Bayesian
requirement for explicit priors and by the variational-inference
requirement for explicit guides.

Programming languages can offer watertight abstractions: they
can abstract away lower-level concepts and prevent them from
leaking out, for instance, using types and constraints [8]. Consider
the expression xW [1] + b[1] from Section 3. At face value, this looks
like eager arithmetic on concrete scalars, running just once in the
forward direction. But actually, it may be lazy (building a computa-
tional graph) arithmetic on probability distributions (not concrete
values) of tensors (not scalars), running several times (for different
Monte-Carlo samples or data batches), possibly in the backward di-
rection (for back-propagation of gradients). Abstractions are useful
to reduce the cognitive burden on the programmer, but only if they
are watertight. Unfortunately, abstractions in deep PPLs are leaky.
Our code examples directly invoke features from several layers of
the technology stack (Edward or Pyro, on TensorFlow or PyTorch,
on NumPy, on Python). Furthermore, we found that error mes-
sages rarely refer to source code constructs. For instance, names of
Python variables are erased from the computational graph, making
it hard to debug tensor dimensions, a common cause for mistakes.
It does not have to be that way. For instance, DL frameworks are
good at hiding the abstraction of back-propagation. More work is
required to make deep PPL abstractions more watertight.

6 CONCLUSION AND OUTLOOK
This paper is a study of two deep PPLs, Edward and Pyro. The study
is qualitative and driven by code examples. This paper explains how
to solve common tasks, contributing side-by-side comparisons of
Edward and Pyro. The potential of deep PPLs is to combine the ad-
vantages of probabilistic models, deep learning, and programming
languages. In addition to comparing Edward and Pyro to each other,

this paper also compares them to that potential. A quantitative
study is left to future work. Based on our experience, we confirm
that Edward and Pyro combine three advantages out-of-the-box:
the overt uncertainty of probabilistic models; the hierarchical rep-
resentation learning of DL; and the composability of programming
languages.

Following are possible next steps in deep PPL research.

• Choice of inference procedures: Especially Pyro should support
Monte-Carlo methods at the same level as variational inference.
• Small data: While possible in theory, this has yet to be demon-

strated on Edward and Pyro, with interesting data sets.

• High accuracy: Edward and Pyro need to be improved to simplify

the tuning required to improve model accuracy.

• Expressiveness: While Turing complete in theory, Edward should

adopt recent dynamic TensorFlow features for usability.

• Conciseness: Both Edward and Pyro would benefit from reducing

the repetitive idioms of priors and guides.

• Watertight abstractions: Both Edward and Pyro fall short on this

goal, necessitating more careful language design.

• Explainability: This is inherently hard with deep PPLs, necessi-

tating more machine-learning innovation.

In summary, deep PPLs show great promises and remain an active
field with many research opportunities.

REFERENCES
[1] Martín Abadi, Paul Barham, Jianmin Chen, Zhifeng Chen, Andy Davis, Jeffrey
Dean, Matthieu Devin, Sanjay Ghemawat, Geoffrey Irving, Michael Isard, Man-
junath Kudlur, Josh Levenberg, Rajat Monga, Sherry Moore, Derek G. Murray,
Benoit Steiner, Paul Tucker, Vijay Vasudevan, Pete Warden, Martin Wicke, Yuan
Yu, and Xiaoqiang Zheng. 2016. TensorFlow: A System for Large-Scale Machine
Learning. In Operating Systems Design and Implementation (OSDI). 265–283. https:
//www.usenix.org/conference/osdi16/technical-sessions/presentation/abadi
[2] Petr Babkin, Md. Faisal Mahbub Chowdhury, Alfio Gliozzo, Martin Hirzel, and
Avraham Shinnar. 2017. Bootstrapping Chatbots for Novel Domains. In Workshop
at NIPS on Learning with Limited Labeled Data (LLD). https://lld-workshop.
github.io/papers/LLD_2017_paper_10.pdf

[3] David Barber. 2012. Bayesian Reasoning and Machine Learning. Cambridge

University Press. http://www.cs.ucl.ac.uk/staff/d.barber/brml/

[4] David M. Blei, Alp Kucukelbir, and Jon D. McAuliffe. 2017. Variational inference:
A review for statisticians. J. Amer. Statist. Assoc. 112, 518 (2017), 859–877. https:
//arxiv.org/abs/1601.00670

[5] Charles Blundell, Julien Cornebise, Koray Kavukcuoglu, and Daan Wierstra. 2015.
Weight Uncertainty in Neural Network. In International Conference on Machine
Learning (ICML). 1613–1622. http://proceedings.mlr.press/v37/blundell15.html
[6] James Bornholt, Todd Mytkowicz, and Kathryn S. McKinley. 2014. Uncertain<T>:
A First-order Type for Uncertain Data. In Conference on Architectural Support
for Programming Languages and Operating Systems (ASPLOS). 51–66. https:
//doi.org/10.1145/2541940.2541958

[7] Flavio P. Calmon, Dennis Wei, Bhanukiran Vinzamuri, Karthikeyan Nate-
Optimized Pre-
Information
http://papers.nips.cc/paper/

san Ramamurthy, and Kush R. Varshney. 2017.
Processing
(NIPS). 3995–4004.
Processing Systems
6988-optimized-pre-processing-for-discrimination-prevention.pdf

for Discrimination

In Neural

Prevention.

[8] Bob Carpenter. 2017. Hello, world! Stan, PyMC3, and Edward. (2017). http://
andrewgelman.com/2017/05/31/compare-stan-pymc3-edward-hello-world/ (Re-
trieved February 2018).

[9] Bob Carpenter, Andrew Gelman, Matt Hoffman, Daniel Lee, Ben Goodrich,
Michael Betancourt, Michael A. Brubaker, Jiqiang Guo, Peter Li, and Allen Riddell.
2017. Stan: A probabilistic programming language. Journal of Statistical Software
76, 1 (2017), 1–37. https://www.jstatsoft.org/article/view/v076i01

[10] Pedro Domingos. 2012. A Few Useful Things to Know About Machine Learning.
Communications of the ACM (CACM) 55, 10 (Oct. 2012), 78–87. https://doi.org/
10.1145/2347736.2347755

[11] Facebook. 2016. PyTorch. (2016). http://pytorch.org/ (Retrieved February 2018).
[12] Zoubin Ghahramani. 2015. Probabilistic machine learning and artificial intelli-
gence. Nature 521, 7553 (May 2015), 452–459. https://www.nature.com/articles/
nature14541

[13] W. R. Gilks, A. Thomas, and D. J. Spiegelhalter. 1994. A Language and Program
for Complex Bayesian Modelling. The Statistician 43, 1 (Jan. 1994), 169–177.
[14] Noah D. Goodman and Andreas Stuhlmüller. 2014. The Design and Implementa-
tion of Probabilistic Programming Languages. (2014). http://dippl.org (Retrieved
February 2018).

[15] Andrew D. Gordon, Thomas A. Henzinger, Aditya V. Nori, and Sriram K. Rajamani.
2014. Probabilistic Programming. In ICSE track on Future of Software Engineering
(FOSE). 167–181. https://doi.org/10.1145/2593882.2593900

[16] Maria I. Gorinova, Andrew D. Gordon, and Charles Sutton. 2018. SlicStan: Im-
proving Probabilistic Programming using Information Flow Analysis. In Work-
shop on Probabilistic Programming Languages, Semantics, and Systems (PPS).
https://pps2018.soic.indiana.edu/files/2017/12/SlicStanPPS.pdf

[17] Suyog Gupta, Ankur Agrawal, Kailash Gopalakrishnan, and Pritish Narayanan.
2015. Deep learning with limited numerical precision. In International Confer-
ence on Machine Learning (ICML). 1737–1746. http://proceedings.mlr.press/v37/
gupta15.pdf

[18] Paul Hudak. 1998. Modular domain specific languages and tools. In International
Conference on Software Reuse (ICSR). 134–142. https://doi.org/10.1109/ICSR.1998.
685738

[19] Diederik P. Kingma and Max Welling. 2013. Auto-encoding variational Bayes.

(2013). https://arxiv.org/abs/1312.6114

[20] Alex Krizhevsky,

Ilya Sutskever, and Geoffrey E. Hinton. 2012.

Ima-
geNet Classification with Deep Convolutional Networks. In Advances in
Neural Information Processing Systems (NIPS).
http://papers.nips.cc/paper/
4824-imagenet-classification-with-deep-convolutional-neural-networks
[21] Brenden M. Lake, Ruslan Salakhutdinov, and Joshua B. Tenenbaum. 2015. Human-
level concept learning through probabilistic program induction. Science 350 (Dec.
2015), 1332–1338. Issue 6266. http://science.sciencemag.org/content/350/6266/
1332

[22] Yann LeCun, Yoshua Bengio, and Geoffrey Hinton. 2015. Deep Learning. Nature

521, 7553 (May 2015), 436–444. https://www.nature.com/articles/nature14539

[23] Yann LeCun, Corinna Cortes, and Christopher J.C. Burges. 1998. The MNIST
Database of Handwritten Digits. (1998). http://yann.lecun.com/exdb/mnist/
(Retrieved February 2018).

[24] Graham Neubig, Chris Dyer, Yoav Goldberg, Austin Matthews, Waleed Ammar,
Antonios Anastasopoulos, Miguel Ballesteros, David Chiang, Daniel Clothiaux,
Trevor Cohn, Kevin Duh, Manaal Faruqui, Cynthia Gan, Dan Garrette, Yangfeng
Ji, Lingpeng Kong, Adhiguna Kuncoro, Gaurav Kumar, Chaitanya Malaviya, Paul
Michel, Yusuke Oda, Matthew Richardson, Naomi Saphra, Swabha Swayamdipta,
and Pengcheng Yin. 2017. DyNet: The Dynamic Neural Network Toolkit. (2017).
[25] Feng Niu, Benjamin Recht, Christopher Ré, and Stephen J. Wright.
Hogwild: A Lock-Free Approach to Parallelizing Stochas-
2011.
Pro-
tic Gradient Descent.
Systems
cessing
http://papers.nips.cc/paper/
4390-hogwild-a-lock-free-approach-to-parallelizing-stochastic-gradient-descent
[26] Danilo J. Rezende, Shakir Mohamed, Ivo Danihelka, Karol Gregor, and Daan
Wierstra. 2016. One-shot Generalization in Deep Generative Models. In Interna-
tional Conference on Machine Learning (ICML). 1521–1529. http://proceedings.
mlr.press/v48/rezende16.html

In Conference

Information

on Neural

693–701.

(NIPS).

[27] Danilo J. Rezende, Shakir Mohamed, and Daan Wierstra. 2014. Stochastic Back-
propagation and Approximate Inference in Deep Generative Models. In Interna-
tional Conference on Machine Learning (ICML). 1278–1286. http://proceedings.
mlr.press/v32/rezende14.html

[28] David E. Rumelhart, Geoffrey E. Hinton, and Ronald J. Williams. 1986. Learning
representations by back-propagating errors. Nature 323 (Oct. 1986), 533–536.
https://doi.org/doi:10.1038/323533a0

[29] John Salvatier, Thomas V. Wiecki, and Christopher Fonnesbeck. 2016. Probabilis-
tic programming in Python using PyMC3. PeerJ Computer Science 2 (April 2016),
e55. https://doi.org/10.7717/peerj-cs.55

[30] N. Siddharth, Brooks Paige, Jan-Willem van de Meent, Alban Desmai-
son, Noah D. Goodman, Pushmeet Kohli, Frank Wood, and Philip Torr.
Learning Disentangled Representations with Semi-Supervised
2017.
Information Pro-
Deep Generative Models.
cessing
Systems
http://papers.nips.cc/paper/
7174-learning-disentangled-representations-with-semi-supervised-deep\
-generative-models.pdf

In Advances

5927–5937.

in Neural

(NIPS).

[31] Dustin Tran, Matthew D. Hoffman, Rif A. Saurous, Eugene Brevdo, Kevin Murphy,
and David M. Blei. 2017. Deep Probabilistic Programming. In International
Conference on Learning Representations (ICLR). https://arxiv.org/abs/1701.03757

[32] Uber. 2017. Pyro. (2017). http://pyro.ai/ (Retrieved February 2018).
[33] Yonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V. Le, Mohammad Norouzi,
Wolfgang Macherey, Maxim Krikun, Yuan Cao, Qin Gao, Klaus Macherey, Jeff
Klingner, Apurva Shah, Melvin Johnson, Xiaobing Liu, Lukasz Kaiser, Stephan
Gouws, Yoshikiyo Kato, Taku Kudo, Hideto Kazawa, Keith Stevens, George Kurian,
Nishant Patil, Wei Wang, Cliff Young, Jason Smith, Jason Riesa, Alex Rudnick,
Oriol Vinyals, Greg Corrado, Macduff Hughes, and Jeffrey Dean. 2016. Google’s
neural machine translation system: Bridging the gap between human and ma-
chine translation. (2016). https://arxiv.org/abs/1609.08144

9

