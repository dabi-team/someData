8
1
0
2

l
u
J

8

]

G
L
.
s
c
[

1
v
6
1
8
2
0
.
7
0
8
1
:
v
i
X
r
a

ImprovingDeepLearningthroughAutomaticProgrammingMaster’sThesisinComputerScienceDangHaTheHienMay14,2014Halden,Norwaywww.hiof.no 
 
 
 
 
 
AbstractDeeplearninganddeeparchitecturesareemergingasthebestmachinelearningmeth-odssofarinmanypracticalapplicationssuchasreducingthedimensionalityofdata,imageclassiﬁcation,speechrecognitionorobjectsegmentation....Infact,manyleadingtechnologycompaniessuchasGoogle,MicrosoftorIBMareresearchingandusingdeeparchitecturesintheirsystemstoreplaceothertraditionalmodels.Therefore,improvingtheperformanceofthesemodelscouldmakeaverystrongimpactintheareaofmachinelearning.However,deeplearningisaveryfast-growingresearchdomainwithmanycoremethodologiesandparadigmsjustdiscoveredoverthelastfewyears.Thisthesiswillﬁrstserveasashortsummaryofdeeplearning,whichtriestoincludeallofthemostimpor-tantideasinthisresearcharea.Basedonthisknowledge,wesuggested,andconductedsomeexperimentstoinvestigatethepossibilityofimprovingthedeeplearningbasedonautomaticprogramming(ADATE).Althoughourexperimentsdidproducegoodresults,therearestillmanymorepossibilitiesthatwecouldnottryduetolimitedtimeaswellassomelimitationsofthecurrentADATEversion.Ihopethatthisthesiscanpromotefutureworkonthistopic,especiallywhenthenextversionofADATEcomesout.ThisthesisalsoincludesashortanalysisofthepowerofADATEsystem,whichcouldbeveryusefulforotherresearcherswhowanttoknowwhatitiscapableof.Keywords:DeepLearning,AutomaticProgramming,ADATE,NeuralNetworks,Ma-chineLearningiAcknowledgmentsFirstandforemost,IwouldliketothankmythesissupervisorProf.RolandOlssonfortheadvice,support,andkindnessthathehasgiventomeoverthepastyear.Withouthim,Ideﬁnitelycouldnotcompletethisthesis.Manypartsofthisthesisweredonebybothofus.Therefore,Iused“we”asthesubjectinthisthesistoindicatethatfact.Iwouldalsoliketothankmyfellowstudents,especiallyKristinLarsen,whohasallowedmetouseherexperimentresultinmythesis.Inaddition,Iwanttothankmyfamilyandfriends,whowerealwaysbesideandsupportmeindiﬃculttimes.Lastbutnotleast,Iwanttothankmybestfriend,NguyetThanh,whotookmeoutofmylonelinessandencouragedmetokeepfollowingmydream.iiiPrerequisitesAlthoughmachinelearningaswellasneuralnetworksanddeeplearningisaveryspecial-izedﬁeld,whichcoversmanydiﬀerentaspectsofmathematics,physics,andbiology...,webelievethatagraduatedstudentincomputersciencecouldeasilyunderstandthemainideasandresultsinthisreport.However,togetadeeperunderstanding,thereadershouldhavebasicknowledgeaboutprobability,statistics,linearalgebra,andcalculus.HeorsheshouldalsobefamiliarwithbasicmathematicsoptimizationconceptslikeTaylorseriesandsteepestgradientdescent.vContentsAbstractiAcknowledgmentsiiiPrerequisitesvListofFiguresxiListofTablesxiiiListingsxv1Introduction11.1MachineLearning.................................11.2ArtiﬁcialNeuralNetworks............................21.3DeepArchitecturesandDeepLearning.....................31.4ResearchQuestionandMethodology......................41.5ReportOutline..................................52TheNeedforDeepLearningandDeepArchitectures72.1DeepArchitecturesonCircuitProblem.....................72.2TheChallengesintrainingDeepNeuralNetworks..............82.3DominanceofDeepLearning..........................83DeepArchitecturesandRelatedModels113.1DeepNeuralNetworks..............................113.2BoltzmannMachines-RelatedModels.....................133.3AutoEncoders...................................164Gradient-BasedTrainingAlgorithmsforNeuralNetworks254.1First-orderMethods...............................254.2Second-orderMethods..............................284.3LineSearchvs.TrustRegionStrategy.....................314.4InitializationAnalysis..............................334.5Cascadecorrelationalgorithm..........................335OverﬁttingandRegularization355.1Introduction....................................355.2RegularizationOverview.............................39viiviiiCONTENTS6ActivationFunctions476.1LogisticSigmoidfunction............................476.2Hyperbolictangentanditsscaledversion...................486.3Softsignfunction.................................496.4RectiﬁerandSoftplusfunction.........................496.5MaxoutFunction.................................527IntroductiontoADATE557.1AShortIntroductiontoADATE........................557.2AShortAnalysisofthePowerofADATESystem...............568ADATEExperiments678.1SelectingTarget..................................678.2BuildingTinyDatasets.............................698.3DesignandImplementation...........................728.4WritingSpeciﬁcationﬁle.............................808.5ExperimentResults................................818.6OverﬁttingProblem...............................859ConclusionandFutureWorks899.1FutureWorks...................................90Bibliography94ANeuralNetworkLibraries95A.1MatrixLibrary..................................95A.2Randomlibrary..................................102A.3NeuralNetworkLibrary.............................103BADATESpeciﬁcationforInitializationExperiment115ListofFigures1.1Atypicalfeedforwardneuralnetwork.....................31.2Activationofanodejthatlayerkthiscalculatedbyxjk=ϕ(Pni=1xi(k−1)wijk)32.1Diﬃcultyonobjectclassiﬁcationtask.....................92.2Hand-designedfeaturesexamples........................92.3Deeplearningapplications............................103.1Deepneuralnetworksrepresentinputinmulti-levelfeaturerepresentations123.2Pretrainingprocessusingauto-encoder.....................123.3(Takenfrom[40])Left:AgeneralBoltzmannmachine.Thetoplayerrepresentsavectorofstochasticbinary“hidden”featuresandthebottomlayerrepresentsavectorofstochasticbinary“visible”variables.Right:ArestrictedBoltzmannmachinewithnohiddentohiddenandnovisibletovisibleconnections.................................133.4(Takenfrom[41])Left:DeepBeliefNetwork(DBN),withthetoptwolayersforminganundirectedgraphandtheremaininglayersformabeliefnetwithdirected,top-downconnectionsRight:DeepBoltzmannMachine(DBM),withbothvisible-to-hiddenandhidden-to-hiddenconnectionsbutwithnowithin-layerconnections.AlltheconnectionsinaDBMareundi-rected........................................153.5(Takenfrom[41])DBNmodelthatwasusedforMNISTdataset......163.6Atypicalautoencoder..............................173.7Takenfrom[28]Eachsquareintheﬁgureaboveshowsthe(normbounded)inputimagexthatmaximallyactivesoneof100hiddenunits.Weseethatthediﬀerenthiddenunitshavelearnedtodetectedgesatdiﬀerentpositionsandorientationsintheimage...........................203.8Takenfrom[48]Left:regularautoencoderwithweightdecay.Thelearnedﬁlterscannotcaptureinterestingstructureintheimage.Right:adenois-ingautoencoderwithadditiveGaussiannoise(σ=0.5)learnsGabor-likelocalorientededgedetectors.Verysimilartotheﬁlterslearnedbysparseautoencoder....................................213.9Takenfrom[39]Resultsaresortedinascendingorderofclassiﬁcationerroronthetestset.Bestperformerandmodelswhosediﬀerencewiththebestperformerwasnotstatisticallysigniﬁcantareinbold.Noticehowtheav-erageJacobiannorm(beforeﬁne-tuning)appearscorrelatedwiththeﬁnaltesterror.SATistheaveragefractionofsaturatedunitsperexample....22ixxLISTOFFIGURES3.10(Takenfrom[16])PretrainingconsistsoflearningastackofRBMs,eachhavingonlyonelayeroffeaturedetectors.ThelearnedfeatureactivationsofoneRBMareusedasthe“data”fortrainingthenextRBMinthestack.Afterthepretraining,theRBMsare“unrolled”tocreateadeepautoen-coder,whichisthenﬁne-tunedusingbackpropagationoferrorderivatives.233.11(Takenfrom[16])(A)Thetwodimensionalcodesfor500digitsofeachclassproducedbytakingtheﬁrsttwoprincipalcomponentsofall60,000trainingimages.(B)Thetwodimensionalcodesfoundbya784−1000−500−250−2autoencoder....................................245.1UnderﬁttingandOverﬁttinginclassiﬁcationtask,takenfrom[30]......365.2UnderﬁttingandOverﬁttinginregressiontask,takenfrom[30].......365.3Errorcurvesincrossvalidationmethods....................375.4Posteriordistribution...............................395.5AddingGaussiannoisetoinputs,takenfrom[15]...............425.6CNNarchitectureusedinMNISTproblem[24]................446.1Thesigmoid,tanh,andscaledtanhfunctions.................486.2Takenfrom[4],tanhversusthesoftsign,whichconvergespolynomiallyinsteadofexponentiallytowardsitsasymptotes................496.3Takenfrom[10],Sparsepropagationofactivationsandgradientsinnet-workofrectiﬁerunits.Theinputselectsasubsetofactiveneuronsandcomputationislinearinthissubset.......................516.4Takenfrom[10],Rectiﬁerandsoftplusactivationfunctions.Thesecondoneisasmoothversionoftheﬁrst.......................516.5Takenfrom[11],Graphicaldepictionofhowthemaxoutactivationfunctioncanimplementtherectiﬁedlinear,absolutevaluerectiﬁer,andapproximatethequadraticactivationfunction.Thisdiagramis2Dandonlyshowshowmaxoutbehaveswitha1Dinput,butinmultipledimensionsamaxoutunitcanapproximatearbitraryconvexfunctions...................537.1TheequivalentdecisiontreeoftheADATEsolutionforedgedetectionprob-lem;LeftbranchesaretheTruecases......................587.2Takenfrom[21],mobilenavigationexample:ItemAisselected,andtheuserhaspressedtheup-button.Themodelhastopredictwhichitemtheuserintendstoselect...............................608.1ElasticdeformationandTinyDigits.Firstline:ahand-designedpatternforthedigit“0”anditsdeformedversions.Twobottomlines:examplesofgenerateddigitsfrom0to9.Notethatallofthesedeformedimagesareintelligible.....................................708.2Fromtoptobottom:MNIST,CIFAR-10andSVHNdatasets........718.3Generalﬂowchartofgradient-basedtrainingalgorithmsforNeuralNetworks798.4Histogramof10000instancesdrawnfromthetanh(tanh(randn(.)))distri-bution.......................................828.5Learningcurvesfornegativeloglikelihoodtrainingcost...........868.6Learningcurvesonnegativeloglikelihoodvalidationcost..........86LISTOFFIGURESxi8.7Learningcurvesofsparse3,sparse*,andnormalized*testedon4-layerneu-ralnetworksusinga0.05learningrate.Eachmethodwasappliedfor10diﬀerentinitializationseeds............................868.8Experimentontestingoverﬁttingsources:Architecture,Batchsize,Learn-ingRate,MomentumandWeightDecay.TheﬁrstvalueforeachfactorwasthevaluethatusedduringtheADATEtrainingprocess...........878.9Experimentontestingoverﬁttingondatasets.................87ListofTables8.1ParametersusedwhentrainingonMNISTdataset..............688.2Testerrorratesafterﬁrst100iterationsfordiﬀerentlearningratesandinitializations...................................688.3Testerrorratesafterﬁrst100iterationsfordiﬀerentinitializationsatthesamelearningrates................................698.4Settingsusedfortheexperiments........................838.5Validationerrorratesafterspeciﬁctrainingepochsfordiﬀerentnetworkdepth848.6Averagetesterror(10initializationseeds)forbestvalidationfordiﬀerentnetworkstructures................................858.7Gridsearchforthetwoconstantsinsparse-3..................85xiiiListings7.1Cleanversionofthebestsynthesizedprogramforedgedetectionproblem.577.2Takenfrom[21]PseudocodeforthebestADATE’ssolution........607.3OneADATE’ssolutionforCARproblem...................617.4TypicalADATE’ssolutionforSPEEDproblem................617.5TypicalADATE’ssolutionforWinequalityproblem.............627.6AnADATE’ssolutionforsortingproblem...................647.7TheoriginalandsynthesizederrorEstimatefunctionusedinEBPalgorithm658.1implementationofassertFun()andassertFuns()................768.2datatypeforonelayerneuralnetwork.....................778.3Paramsdatatypewhichshowsallavailabletrainingoptions.........778.4Originalsparseandgeneratedsparse-3initializationfunctions.Thef(.)functionisusedtogeneratealistofweightsforanode.Theseweightsarethenrandomlyassignedtoitsincomingconnections.............81xvChapter1IntroductionMachinelearningisahugeresearchareabasedonmanyideasinmathematics,physics,andbiology....Therefore,inthisﬁrstpart,weonlytrytogiveaverybriefintroductiontomachinelearningaswellasexplanationsforsomerelatedimportantspecializedterms,whichhopefullycouldhelponewhosemajorisnotmachinelearningunderstandthereport.Besides,wealsointroduceartiﬁcialneuralnetworksanddeeplearning-themainﬁeldthatweareworkingoninourthesis-theiradvantagesandpracticalapplicationsasourmotivations.Finally,wepresentourresearchquestion,researchplan,andoutlineoftherestofthethesis.1.1MachineLearningInthebook“MachineLearning”[27],Michell1997deﬁned“Theﬁeldofmachinelearningisconcernedwiththequestionofhowtoconstructcomputerprogramsthatautomati-callyimprovewithexperience...AcomputerprogramissaidtolearnfromexperienceEwithrespecttosomeclassoftasksTandperformancemeasureP,ifitsperformanceattasksinT,asmeasuredbyP,improveswithexperienceE”and“Machinelearningisinherentlyamultidisciplinaryﬁeld.Itdrawsonresultsfromartiﬁcialintelligence,proba-bilityandstatistics,computationalcomplexitytheory,controltheory,informationtheory,philosophy,psychology,neurobiology,andotherﬁelds”.Anothermoremodernandpracticaldeﬁnitionisusedinthebook“DataMining-PracticalMachineLearningToolsandTechniques”[51],wheretheauthorsstatethat“Machinelearningprovidesthetechnicalbasisofdatamining”and“Datamining...istobuildcomputerprogramsthatsiftthroughdatabasesautomatically,seekingregularitiesorpatterns,...,generalizetomakeaccuratepredictionsonfuturedata”.Ingeneral,amachinelearningsystemisbasedonthreemainparts:trainingdata,model,andtraining(i.elearning)algorithm.Eachmodelhasasetofparametersthatwecanusethetrainingalgorithmtotrain(i.eﬁt)theseparameterstothetrainingdata,andapplythatlearnedmodelonanotherseparatedtestdatatomeasureitsrealperfor-mance.Therearetwomaintypesoflearning:supervisedandunsupervisedlearning.Insupervisedlearning,thetrainingdatacontainsboththeinputandthedesiredoutput(e.g.imagesandtheirclassiﬁcations),andthetrainingalgorithmstrytoconstructamapping(throughadaptingmodel’sparameters)thatdeﬁnestheoutputpatternsintermsoftheinputpatterns.Inunsupervisedlearning,thedesiredoutputisunknown,andthetrainingalgorithmstrytodiscoverthestructureindata,orevengeneratenewsimilardata.12Chapter1.IntroductionBesides,therearemanydiﬀerenttypesofmodel,fromthesimplestonessuchasZeroR(justguessthemostlikelyanswer-usuallyusedasthebaselineperformance)orLinearRegressiontothemuchmorecomplexmodelssuchasSupportVectorMachine(SVM)orDeepNeuralNetworks(DNNs).Eachmodelhasadiﬀerentassumption(i.e.inductivebias)aboutthepossibleinput-outputmappings(insupervisedlearning)orpossibledatadistribution(inunsupervisedlearning).Thisisbecausetheproblemwearedealingwithinmachinelearningisknowntobe“illposed”(referstochapter5),whichcannotbesolvedunlesssomepriorinformationisasssumed.Forexample,insupervisedlearning,themappingfunctionmaynotevenexist,orwedonothaveenoughinformationfromthetrainingexamplestoreconstructthatmapping,ortheunavoidablepresenceofnoiseinthetrainingexamplescanmakeaperfectﬁttedmodeluselessinpractice[14].Aneasyexampleforinductivebiasinlinearregressionisthatweassumethattheinputandthedesiredoutputarelinearlydependent,sothatwecanrepresentthemappingfrominputtooutputthroughalinearequation.Thetrainingalgorithmforitthereforejustsearchesforthe“best”equation.Inthisthesis,wefocusonsupervisedlearningalgorithminArtiﬁcialNeuralNetworks(ANNs)andDeepNeuralNetworks(DNNs),whichareverypowerfulmodels.However,wealsointroducesomeunsupervisedlearningalgorithmsandmodels,whicharecommonlyusedinlearningprocessofDNNs.1.2ArtiﬁcialNeuralNetworksTheartiﬁcialneuralnetworks(ANNs)havebeeninspiredinpartbytheobservationthatbiologicallearningsystemsarebuiltofverycomplexwebsofinterconnectedneurons.Artiﬁcialneuralnetworksarebuiltoutofadenselyinterconnectedsetofsimpleunits,whereeachunittakesanumberofreal-valuedinputs(possiblytheoutputsofotherunits)andproducesasinglereal-valuedoutput(whichmaybecometheinputtomanyotherunits)[27].WhileANNsarelooselymotivatedbybiologicalneuralsystems,therearemanycomplexitiestobiologicalneuralsystemsthatarenotmodeledbyANNs,andmanyfeaturesoftheANNsareknowntobeinconsistentwithbiologicalsystems[27].Forexample,weconsiderhereANNswhoseindividualunitsoutputasingleconstantvalue,whereasbiologicalneuronsoutputacomplextimeseriesofspikes.Therearemanydiﬀerenttypesofneuralnetworkssofar.Inthisreport,theterm“Neu-ralNetworks”isusedtoimplicitlyrefertothe“ArtiﬁcialFeedForwardNeuralNetworks”ifthereisnootherexplicitexplanation.Afeedforwardneuralnetworkisanartiﬁcialneuralnetworkwhereconnectionsbetweentheunitsdonotformadirectedcycle,andthatthereforecanbevisualizedasamultiplelayersnetwork.InFigure1.1,weshowedatypicalfeedforwardneuralnetworkwith2hiddenlayers(i.ethelayersbetweeninputandoutputlayers).Themappingbetweeninputandoutputiscalculatedbyfeedingtheinputintotheinputnodes,goingthroughthehiddennodesandtotheoutputnodes.Thevalue(i.eactivation)ofeachintermediatenodeisproducedbyﬁrstweightedsummingofallitspredecessorsthengoingthroughanactivationfunction(asdescribedinFigure1.2,pleaserefertoChapter6formoredetails).ManydiﬀerentactivationfunctionscanbeusedinANNs,thesimplestoneisthesgn()function:return+1wheninputisgreaterthan0andreturn-1otherwise;whichisusedinperceptronnetworks.Thecurrentlypopularactivationfunctionsarethetanh(),thesigmoidfunction(f(x)=11+e−x)andthesoftmaxfunction.Thesefunctionsarepopularpartlybecauseoftheirdiﬀerentiability,sowecan1.3.DeepArchitecturesandDeepLearning3easilycomputetheirgradients,whichiscrucialingradient-basedtrainingalgorithm.Figure1.1:AtypicalfeedforwardneuralnetworkFigure1.2:Activationofanodejthatlayerkthiscalculatedbyxjk=ϕ(Pni=1xi(k−1)wijk)ThetrainingprocessinANNsistoadaptalloftheconnections’weighttothetrain-ingdata,byminimizingacostfunction(i.e.objectivefunction)–whichisusuallyameansquareerror(MSE),crossentropy,ornegativeloglikelihoodfunction.TherearemanydiﬀerenttrainingalgorithmsforANNs,includingevolutionstrategyorgeneticalgo-rithms.However,gradient-basedmethodsarethemostpopular.Thesemethodsareusedtominimizethecostfunctionf(x)byiterativelyfollowingasearchdirectiondeﬁnedbythegradient(orafunctionofgradient)oftheffunctionatthecurrentpoint(detailsofthesemethodsareprovidedatChapter4.Inthisthesis,weonlyfocusonstudyingthesegradient-basedmethodsasthemainoptimizationapproachfordeeplearning.1.3DeepArchitecturesandDeepLearningDeeparchitecturesaremodelsthatarecomposedofmultiplelevelsofnon-linearopera-tions,suchasinDeepNeuralNetworks(i.e.neuralnetworkswithmorethan3hidden4Chapter1.Introductionlayers),DeepBeliefNets(DBN),orDeepAutoEncoders.Theoreticalresultssuggestthatinordertolearnthekindofcomplicatedfunctionsthatcanrepresenthigh-levelabstrac-tions(e.g.,invision,language,andotherAI-leveltasks),wemayneeddeeparchitectures[1].However,until2006,trainingthesedeeparchitectureswasbelievedtobediﬃcultduetothewell-knownvanishinggradientproblem.Hintonetal.2006[16]haveintroducedanewlearningproceduretosuccessfullytacklethisproblem.Afterthat,diﬀerentdeeparchitecturemodelsaswellasnewlearningalgorithmshavebeenproposedandappliedsuccessfullyinmanyareas,beatingthestate-of-the-artincertainapplicationssuchasindimensionalityreduction,modelingtextures,modelingmotion,objectsegmentation,informationretrieval,robotics,naturallanguageprocessing...[1].Inchapter3,wegiveanoverviewofthesedeeparchitecturemodels:whyweneeddeeparchitecturesandthechallengesoftrainingthesemodels.Wealsobrieﬂyintroducetheirpopulartrainingalgo-rithms.Infact,deeparchitecturesareemergingasthebestmachinelearningmodelsofarinmanypracticalapplicationssuchasreducingthedimensionalityofdata[16],imageclassiﬁcation[22]orspeechrecognition[17]...webelievethatimprovingjustoneofthesemodels’architecturesortrainingalgorithm,intermsoftheirperformanceorcomputationalcost,couldmakeaverystrongimpactinthemachinelearningarea.1.4ResearchQuestionandMethodology1.4.1ResearchquestionInthisthesis,weintendtostudydeeparchitectures(especiallydeepneuralnetworks)andtheirlearningalgorithms,therebyunderstandthechallengesregardingtrainingthesemodels.Basedontheknowledge,wecouldinvestigatethepossibilityofimprovingthesemodelsoralgorithmsusingADATE(AutomaticDesignofAlgorithmsThroughEvolution)[31].However,deeplearningisknowntorunextremelyslowly(sometimestakesweekstoﬁnish),especiallyonbigdataset.ADATE,onthecontrary,needstheprogramtorunfast(usuallylessthantenseconds),becauseithastoevaluateatleastmillionsofdiﬀerentprograminstancestoﬁndthebestversion.Therefore,touseADATEtoimprovedeeplearning,wehavetotraindeeparchitecturesonaverysmalldataset.Thebiggestchallengeisthatwehavetomakesurethattheimprovedversionofthatalgorithmcanstillbeusefulinotherdataset.Moreover,deeparchitecturesandtheirtrainingalgorithmsareimplementedasaverycomplexprogram,whichdeﬁnitelycouldnotbecompletelygeneratedbyADATE.Therefore,wehavetochooseasmallpartofthatprogram,whichcansigniﬁcantlyimprovethewholeperformanceifitisimproved.Basically,attheendofthisthesis,weneedtoanswerthefollowingresearchquestions:RQ1HowcantheADATEsystemimprovetheperformanceofdeeplearning?Secondaryrelevantresearchquestionsare:RQ1.1WhichpartofdeeplearningispossibletobeimprovedbyADATE?RQ1.2WhatdatasetissuitableforusinginADATEtoimprovedeeplearning?RQ1.3Couldtheimprovedversionofdeeplearningbeusedinotherdataset?RQ1.4Howcanweimplementcorrectlydeeparchitecturesandtheirtrainingalgo-rithmsinStandardML(SML)?1.5.ReportOutline51.4.2MethodologyIngeneral,machinelearningisanexperimentalscience,inwhichmanymodelsoralgo-rithmsarebasedonheuristicequations,whicharelearnedfromexperiments.Thisthesisisnotanexception.Toanswertheseaboveresearchquestions,wehavetoexperimentwithdiﬀerentpossiblesolutions,asmuchaspossible,toﬁndoutthebestone.First,wehavetogetadeepunderstandingofdeeparchitecturesandtheirtrainingalgorithms.ThenwechooseapartthatismostlikelytobeimprovedusingADATE.Finally,wechoosethemostsuitabledatasetanduseADATEtoimprovethealgorithmonthatdatasetautomatically.1.5ReportOutlineChapter2,3,4,5and6providethenecessarybackgroundaboutdeeplearning:•Inchapter2,wepresentedanswerthequestionwhyweneeddeeplearning,inboththeoreticalandempiricalways.•Inchapter3,wegaveanoverviewofdeeparchitecturesandchallengesoftrainingthesemodels.Wealsointroducedbrieﬂyabouttheirpopulartrainingalgorithms.•Inchapter4,wesummarizeddiﬀerentpopulargradient-basedtrainingalgorithmsforNeuralNetworks.•Inchapter5,wepresentedtheoverﬁttingproblem,andintroduceddiﬀerentregular-izationtechniquesasthepossiblesolutions.•Inchapter6,wesummarizedrecentwell-knownresearchesaboutactivationfunctionsusedindeeplearning.Chapter7providesashortintroductionaboutADATEsystem,aswellasananalysisofitspower.Besides,basedonknowledgesprovidedinpreviouschapters,thischapteralsosuggestsdiﬀerentdeeplearningalgorithmsthatcanprobablybeimprovedbyADATE.Chapter8describesourexperimentswithADATEindetail,fromchoosingthetargetalgorithm,buidingtinydataset,totestingresultsandanalyzingoverﬁttingproblem.Thischapteraimsatansweringalltheresearchquestionsposedabove.Chapter9presentsourconclusionaboutthisproject,aswellassuggestionsforfutureworks.Chapter2TheNeedforDeepLearningandDeepArchitecturesDeeplearningisasetofalgorithmsinmachinelearningthatattempttolearnlayeredmodelsofinputs(deeparchitectures).Thelayersinsuchmodelscorrespondtodistinctlevelsofconcepts,wherehigher-levelconceptsaredeﬁnedfromlower-levelones,andthesamelower-levelconceptscanhelptodeﬁnemanyhigher-levelconcepts.Beforetheinventionofpre-training,whichmakesdeeplearningmorefeasible,mostofavailablelearningalgorithmscorrespondtoshallowarchitectures.However,asweallknow,themammalbrainisorganizedinadeeparchitecture[42].Thebrainalsoappearstoprocessinformationthroughmultiplestagesoftransformationandrepresentation.Thisisparticularlyclearintheprimatevisualsystem,withitssequenceofprocessingstages:detectionofedges,primitiveshapes,andmovinguptograduallymorecomplexvisualshapes[42].Theoretically,somefunctionscannotbeeﬃcientlyrepresented(intermsofthenumberoftunableelements)byarchitecturesthataretooshallow.Infact,functionsthatcanbecompactlyrepresentedbyadepthkarchitecturemightrequireanexponentialnumberofcomputationalelementstoberepresentedbyadepthk−1architecture.Therefore,poorgeneralizationmaybeexpectedwhenusinganineﬃcientlydeeparchitectureforrepresentingsomefunctions[1].2.1DeepArchitecturesonCircuitProblemIn[3],Bengioetal.2007havesummarizedtheadvantagesofdeeparchitecturesoncircuitproblems.Accordingtothem,complexitytheoryofcircuitsstronglysuggeststhatdeeparchitecturescanbemuchmoreeﬃcient(sometimesexponentially)thanshallowarchi-tectures,intermsofthenumberofcomputationalelementsrequiredtorepresentsomefunctions.Forexample,theparityfunctionwithdinputs(XORproblem)requiresO(2d)examplesandparameterstoberepresentedbyaGaussianSVM(Bengioetal.,2006),O(d2)parametersforaone-hidden-layerneuralnetwork,O(d)parametersandunitsforamulti-layernetworkwithO(log2d)layers,andO(1)parameterswitharecurrentneuralnetwork.Moregenerally,booleanfunctions(suchasthefunctionthatcomputesthemul-tiplicationoftwonumbersfromtheird-bitrepresentation)expressiblebyO(logd)layersofcombinatoriallogicwithO(d)elementsineachlayermayrequireO(2d)elementswhenexpressedwithonly2layers(Utgoﬀ&Stracuzzi,2002;Bengio&LeCun,2007).78Chapter2.TheNeedforDeepLearningandDeepArchitectures2.2TheChallengesintrainingDeepNeuralNetworksBoththeoreticalandexperimentalevidencesuggeststhattrainingdeeparchitecturesismuchmorediﬃcultthantrainingshallowones.In[1,3,7],theauthorsshowedthatgradient-basedtrainingprocessondeepsupervisedmulti-layerneuralnetworks(startingfromrandominitialization)usuallygetsstuckin“apparentlocaloptimaorplateaus”,whichendsupwithapoorerperformanceresultcomparedtotheshallowones’.Thisphenomenoncanbeexplainedbythevanishinggradient,localoptima,andpathologicalcurvatureproblems,whichknowntobemuchmoreseriousindeeparchitectures.Theseproblemspreventedusfromusingdeeplearningforaverylongtimeuntiltheappearanceofpre-trainingmethods.Hintonetal.hascompletelychangedthestorywhenintroducingDeepBeliefNetworksandgreedylayer-wiseunsupervisedpre-trainingmethodsin2006[18].Afterthat,manysuccessfuldeeplearningmethodshavebeenintroduced(Bengioetal.,2007;Vincentetal.,2008;Westonetal.,2008;Leeetal.,2008),butallofthemuseacommonideawithHinton’sone:theDNNsareﬁrstpre-trainedbyanunsupervisedpre-trainingalgorithm,andthenﬁne-tunedbyotherclassicalsupervisedlearningmethods.Inthenextchapter,besidesdeeparchitectures,manyofthemostpopularmodelsusedinpre-trainingprocessofdeeplearningwillalsobeintroduced.Becauseifwecouldimproveoneofthesepre-trainingbuilding-blockmodel,wecouldimprovetheperformanceofdeeplearningingeneral.Besidespre-training,somerecentresearcheshaveshownthatlearningdeepnetworkscanstillbedonefairlywellusingotherclassicallearningmethodssuchasHessian-freeOp-timization(2nd-ordermethod)orevenastandardstochasticgradientdescent(1st-ordermethod)[26,46].Theyarguedthatwhilebadlocaloptimadoexistindeep-networks,inpracticetheydonotseemtoposeasigniﬁcantthreat.Instead,thediﬃcultyisbetterexplainedbyregionsofpathologicalcurvature(e.g.longnarrowvalley)intheobjec-tivefunction[26].However,intheirexperiments,theycouldonlysuccessfullytraindeepautoencodersusingtheirmethods,anddidnotmentionaboutothertypesofdeeparchi-tectures.Afterconductingsomeexperiments,weconcludedthatpre-trainingisstillamuchbetterapproachthananyothermethods,inallofdeeplearningapplications.How-ever,therearestillsomeexceptions,wherewecantrainadeeparchitecturewithouttheneedofpre-training,suchasusingdataaugmentation[5]orconvolutionalneuralnetworks(section5.2.6).2.3DominanceofDeepLearningInhispresentationaboutdeeplearning[29],AndrewNg.–aleadingresearcherinthedeeplearningarea,hasstatedthat“Thisistheﬁrsttimeasingletypeofmodelcancompetealmostallofthepreviousstate-of-the-artresultsinmachinelearning”.Oneofthemostimportantabilitieswhichbringsthepowertodeeplearningisthatitcanautomaticallyextractusefulfeaturesinalmostanytypeofinputdomain(e.g.Images/Video,Audio,Text,...).Figure2.1showstheimportanceoffeaturelearning,whereusingrawinputdirectlymakethetasksonit(e.g.labels,classiﬁcation,imagesearch,...)becomealmostimpossible.Whatweneedisabetterwaytopresentinputs:featurerepresentations.Forexamples,todetectiftheimagesinﬁgure2.1isaboutmotortype,weneedtoknow:isthereanywheel?Isthereahandlebar?...Imagepresentedintermofthesefeaturesismucheasiertoprocess.However,wedonothavewheeldetectoror2.3.DominanceofDeepLearning9handlebardetector.Fordecades,thousandsofexpertsweretryingtohand-designfeaturestocapturevariousstatisticalpropertiesoftheimage/audio/text....SomeofthesefeaturesaredemonstratedinFigure2.2.Figure2.1:DiﬃcultyonobjectclassiﬁcationtaskFigure2.2:Hand-designedfeaturesexamplesDeeplearning,ontheotherhand,canautomaticallylearnevenbetterfeaturesthanhand-designedfeaturesonmanydiﬀerentinputdomains.Thismakesdeeplearningbe-comespowerfulandgeneral-purposemodel.Figure2.3showssomeexamplesofthesediﬀerenttasksthatdeeplearningoutperformedanypreviousmethods.Seeingitspoten-tialinreal-lifeapplications,manybigtechcompanieshaverecentlyhireddeeplearningleadingresearchers.InMar,2013,GooglehiredGeoﬀreyHinton–theonewhoenableddeeplearningin2006byhisdiscoveryofunsupervisedpre-training–andhisteamtomakeAIaReality(asintheannouncement).FacebookalsodecidedtohireprominentNYUprofessorYannLeCun–theonewhodiscoveredconvolutionalneuralnetworks–asthenewdirectoroftheirAIlab.AcquisitionofDeepMind–aLondon-basedartiﬁcialintelligence–byGoogleinJan2014,whichcostmorethan$500million,couldshowtheheatofdeeplearning.10Chapter2.TheNeedforDeepLearningandDeepArchitecturesFigure2.3:DeeplearningapplicationsChapter3DeepArchitecturesandRelatedModelsInthischapter,wegiveabriefintroductionaboutdiﬀerentwell-knowndeeparchitec-tures.AlthoughwemainlyfocusonDeepNeuralNetworksanditsrelatedmodels,itisworthknowingothermodelssuchasBoltzmannMachines,DeepBeliefNetworks,orAu-toEncoders,whichareusedinpre-trainingprocessofdeeplearning.Thischapterissplitintothreegeneraltypesofmodels:DeepNeuralNetworks,BoltzmannMachines-relatedmodels,andAutoEncodersmodels.3.1DeepNeuralNetworksBeingafundamentaldeeparchitecture,DeepNeuralNetworkissimplyaneuralnetworkwithmanyhiddenlayers.Thisisaveryoldideabutcouldnotbeusedpopularlybecauseofdiﬃcultiesinitstrainingalgorithm.Afterappearanceofunsupervisedpre-training,deepneuralnetworkshaveﬂourishedandbecomeoneofthemostpopularmachinelearningmodelsnowadays.AsshowninFigure3.1,deepneuralnetworksarecomposedofmulti-layerofnon-linearoperations.Moreover,iflearnedinagoodway,deepneuralnetworkscanrepresentinputthroughmulti-levelfeaturerepresentations,wherefeaturesinhigherlayermodelhigherlevelabstractionsininput.Layer-wiseUnsupervisedPre-trainningGreedylayer-wiseunsupervisedpre-trainingisthemostimportantstrategyindeeplearn-ing.Itallowsustotrainverydeepneuralnetworksveryeﬀectively.Thisstrategycontainstwomainstages:•Pre-training:Weﬁrstpre-trainonelayeroftheneuralnetworksatatime(i.e.layer-wise)inagreedyway.Todothis,weneedabuildingblockmodel.Wepre-traineachlayerofthedeepneuralnetworkbytrainingabuildingblockmodelforeachlayerandstackthemoneaboveanother.Diﬀerentbuildingblockmodelsforthispurposecanbeused.TheﬁrstappearedoneisRBMproposedbyHintonin2006.Sincethen,manydiﬀerentbuildingblockmodelshavebeenintroduced,suchassparseauto-encoders,denoisingauto-encoders,orcontractiveauto-encoders.Figure3.2illustratesthispre-trainingprocessusinganauto-encoder.1112Chapter3.DeepArchitecturesandRelatedModelsFigure3.1:Deepneuralnetworksrepresentinputinmulti-levelfeaturerepresentationsFigure3.2:Pretrainingprocessusingauto-encoder•Fine-tuning:Afterpre-training,wecanuseanystandardbackpropagationmethodstoﬁne-tunethatpretraineddeepneuralnetwork.Thetrainingtaskisnowmucheasierthantrainingfromarandominitializednetwork.TraditionalApproachesforDeeplearningBesidesthepretrainingstrategy,researchersalsotriedtoﬁndatraditionalwaytododeeplearning.Martens2010[26]hasdevelopeda2nd-orderoptimizationmethodbasedonthe“Hessian-free”approach,andapplyittotrainingdeepautoencodersuccessfully.Ilyaetal.2013[46]provedthatadeepautoencodercouldalsobetrainedwith1st-ordermethodlikestochasticgradientdescent,usingwell-chosenrandominitializationschemescalledandvariousformsofmomentum-basedacceleration[46].TheyusedavariationofmomentumcalledNesterov’sAcceleratedGradienttoimprovetheconvergencerateguarantee.BothMartensandIlyaetal.used“sparseinitialization”intheirmethods.However,thesemethodsarenotgoodattrainingageneraldeepneuralnetworks,wherethepre-trainingstillamuchbetterstrategy.3.2.BoltzmannMachines-RelatedModels13Figure3.3:(Takenfrom[40])Left:AgeneralBoltzmannmachine.Thetoplayerrepresentsavectorofstochasticbinary“hidden”featuresandthebottomlayerrepresentsavectorofstochasticbinary“visible”variables.Right:ArestrictedBoltzmannmachinewithnohiddentohiddenandnovisibletovisibleconnections.3.2BoltzmannMachines-RelatedModelsThemostimportantmodelofthistypeisRestrictedBoltzmannMachines(RBM),whichisusedasabuilding-blockmodelintheunsupervisedpre-trainingprocessintroducedbyHintonin2006.Afterthat,otherresearchershaveintroducedsomenewtypesofautoencoders,whichcanreplacetheRBM’sroleinpre-trainingprocess.Actually,theRBMmodelisverysimilartoanautoencoder.Thissimilaritywillbepresentedinthenextsectionaboutautoencoders.3.2.1BoltzmannMachinesABoltzmannmachineisatypeofenergy-basedmodels,whichmaintainsanenergyfunc-tiontodeﬁneaprobabilitydistributionofeachconﬁgurationofthevariablesofinterest.ItisakindofstochasticrecurrentneuralnetworkinventedbyGeoﬀreyHintonandTerrySejnowski.AsdisplayedinFigure.3.3,aBoltzmannmachineisanetworkofsymmetri-callycoupledstochasticbinaryunits.Itcontainsasetofvisibleunitsvvv∈0,1D,andasetofhiddenunitshhh∈0,1P.Theenergyofthestatevvv,hhhisdeﬁnedas:E(vvv,hhh;θ)=−12vvvTLvLvLv−12hhhTJhJhJh−vvvTWhWhWh(3.1)whereθ={WWW,LLL,JJJ}arethemodelparameters,whichrepresentvisible-to-hidden,visible-to-visible,andhidden-to-hiddensymmetricinteractionterms.(Wehaveomittedthebiastermsforclarityofpresentation)Theprobabilitythatthemodelassignstoavisiblevectorvvvis:p(vvv;θ)=1Z(θ)Xhhhexp(−E(vvv,hhh;θ))Z(θ)=XvvvXhhhexp(−E(vvv,hhh;θ))(3.2)14Chapter3.DeepArchitecturesandRelatedModelsTheparameterupdates,originallyderivedbyHintonandSejnowski(1983),thatareneededtoperformgradientascentinthelog-likelihoodcanbeobtainedfromEq.3.2:∆WWW=α(EPdata[vvvhThThT]−EPmodel[vvvhhhT])∆LLL=α(EPdata[vvvvTvTvT]−EPmodel[vvvvvvT])∆JJJ=α(EPdata[hhhhThThT]−EPmodel[hhhhhhT])(3.3)Exactmaximumlikelihoodlearninginthismodelisintractablebecauseexactcompu-tationofboththedata-dependentexpectationsandthemodel’sexpectationstakesatimethatisexponentialinthenumberofhiddenunits.However,settingbothJJJ=0andLLL=0wouldintroducethewell-knownrestrictedBoltzmannmachine(RBM)(Smolensky,1986)(seeFig.3.3,rightpanel),whichcanbelearnedeﬃcientlyusingContrastiveDivergence(CD)(Hinton,2002).3.2.2RestrictedBoltzmannMachines-RBMARBMisaBoltzmannmachine,whichisrestrictedbyomittingintra-layerconnections(e.g.hidden-hiddenandvisible-visibleconnections).Thisrestrictionallowsustoobtainexactsamplesfromtheconditionaldistributionp(hhh|vvv;θ)andp(vvv|hhh;θ),thanktoinde-pendencebetweenhidden-hiddenandvisible-visiblenodes.Therefore,wecansampleconﬁgurationofhbasedonstateofvandviceversa.TheparameterupdatesforRBMcanbederivedasfollowing(includingbiasterms):∆wij=α(hvihjidata−hvihjimodel)(3.4)∆bi=α(hviidata−hviimodel)(3.5)∆bj=α(hhiidata−hhiimodel)(3.6)Wecancomputehvihjidatabyclampingthevisibleunitsatthedatavectorvvvandthencomputetheexpectedvalueofhhheasily.Tocomputehvihjimodel,wecanﬁrstclampthevisibleunitsatdatavectorvvv,thensamplingthehiddenunits,thensamplingthevisibleunits,andrepeatingthisprocedureinﬁnitelymanytimes.Afterinﬁnitelymanyiterations,themodelwillforgetitsstartingpointandwecansamplefromitsequilibriumdistribution.However,ithasbeenshownthatthisexpectationcanbeapproximatedwellinﬁnitetimebyrunningthesamplingchainforonlyafewsteps.ThismethodiscalledContrastiveDivergence(CD).Usingthesederivatives,wecanmaximizethelog-likelihoodfunction,whichleadstodecreasetheenergyoftrainingcases(increaseprobability)andincreasetheenergyofotherunseencases.ThemainreasonRBMareinterestingisitscapabilityofpre-trainingadeepnetwork.3.2.3DeepBeliefNets-DBNDeepBeliefNetsareprobabilisticgenerativemodelsthatarecomposedofmultiplelayersofstochastic,latent(hidden)variables.Thetoptwolayershaveundirected,symmetric3.2.BoltzmannMachines-RelatedModels15Figure3.4:(Takenfrom[41])Left:DeepBeliefNetwork(DBN),withthetoptwolayersforminganundirectedgraphandtheremaininglayersformabeliefnetwithdirected,top-downconnec-tionsRight:DeepBoltzmannMachine(DBM),withbothvisible-to-hiddenandhidden-to-hiddenconnectionsbutwithnowithin-layerconnections.AlltheconnectionsinaDBMareundirected.connectionsbetweenthemandformanassociativememory.Thelowerlayersreceivetop-down,directedconnectionsfromthelayerabove(seeFigure.3.4)Basically,DBNsextendtheRBMarchitecturetomultiplehiddenlayers,wheretheweightsinlayerhlaretrainedbykeepingalltheweightsinthelowerlayersconstantandtakingasdatatheactivitiesofthehiddenunitsatlayerl−1.Therefore,DBNisastackedRBMs,whicharetrainedgreedilyandinsequence.Itcanbeprovedthateachtimeweaddanotherlayeroffeatures(i.e.RBM)weimproveavariationallowerboundonthelogprobabilityofthetrainingdata.Fine-tuningforgenerationAfterlearningmanylayersoffeatures,wecanﬁne-tunethefeaturestoimprovegenerationusingthe“wake-sleep”algorithm.First,wedoastochasticbottom-uppasstoadjustthetop-downweightstobegoodatreconstructingthefeatureactivitiesinthelayerbelow,thendoingafewiterationsofsamplinginthetoplevelRBMtoadjusttheweightsinthetop-levelRBM.Finally,wedoastochastictop-downpasstoadjustthebottom-upweightstobegoodatreconstructingthefeatureactivitiesinthelayerabove.Fine-tuningfordiscriminationAfterpre-trainingtheDBNs,wecanunfoldthatDBNintoadeepneuralnetwork,andusestandardbackpropagationtoﬁne-tunethemodelforbetterdiscrimination.ApplyingDBNondigitrecognitionHinton2006.hasdesignedaDBNshowninﬁgure3.5tolearnthejointdistributionofdigitimagesanddigitlabels.Themodellearnstogeneratecombinationsoflabelsandimages.Toperformrecognition,theystartwithaneutralstateofthelabelunitsanddoanup-passfromtheimagefollowedbyafewiterationsofthetop-levelassociativememory.FortheMNISTdataset(including70,000digitimages),theyachieved1.25%errorrateontestset(usinggenerativeﬁne-tuning)and1.15%(usingbackpropagationﬁne-tuning)16Chapter3.DeepArchitecturesandRelatedModelsFigure3.5:(Takenfrom[41])DBNmodelthatwasusedforMNISTdataset3.3AutoEncodersAclassicalautoencoder(i.e.autoassociator)isaspecialtypeoffeedforwardneuralnetworkwhichistrainedtoencodetheinputinsomerepresentationsothattheinputcanbereconstructedfromthatrepresentation.Inotherwords,wesetthetargetvaluestobeequaltotheinputs.Thisisanunsupervisedlearningtaskbecausewedonotuselabelsinformationduringthetrainingprocess.AtypicalautoencoderisshowninFigure3.6,whichisafeedforwardnetworkwithtwoweight-layers(usuallyconstrainedtobeequal).3.3.1BasicAutoencoder(AE)Anautoencodercontainstwoparts:•Encoder:isafunctionfthatmapsaninputx∈Rdxtohiddenrepresentationh(x)∈Rdh.Ithastheform:h=f(x)=sf(Wx+bh)(3.7)wheresfisaactivationfunction,typicallyalogisticsigmoidfunction.Theencoderisparametrizedbyadh×dxweightmatrixW,andabiasvectorbh∈Rdh•Decoder:Thedecoderfunctiongmapshiddenrepresentationhbacktoarecon-structiony:y=g(h)=sg(W0h+by)(3.8)wheresgisthedecoder’sactivationfunction,typicallyeithertheidentity(yieldinglinearreconstruction)orasigmoid.Thedecoder’sparametersareabiasvectorby∈Rdx,andmatrixW0.Theweightmatricesofencoderanddecoderareusuallytied,inwhichW0=WT.3.3.AutoEncoders17Figure3.6:AtypicalautoencoderAutoencodertrainingconsistsinﬁndingparametersθ={W,bh,by}thatminimizethereconstructionerroronatrainingsetofexamplesDn,whichcorrespondstominimizingthefollowingobjectivefunction:JAE(θ)=Xx∈DnL(x,g(f(x)))(3.9)WhereListhereconstructionerror.TypicalchoicesincludethesquarederrorL(x,y)=kx−yk2usedincasesoflinearreconstruction1;andthecross-entropylosswhensgisthesigmoidandinputsarein[0,1]:L(x,y)=−Pdxi=1xilog(yi)+(1−xi)log(1−yi).Wecanuseanykindofback-propagationoptimizationmethodspresentedinChapter4totrainanautoencoder.Asshownabove,theautoencodertriestolearnthefunctiong(f((x)))≈x.Inotherwords,itistryingtolearnanapproximationtotheidentityfunction,soastooutputˆxthatissimilartox.However,byplacingconstraintsonthenetwork,suchasbylimitingthenumberofhiddenunits,wecandiscoverinterestingstructureaboutthedata.Forexample,ifthenumberofhiddenunits(e.g.50)issmallerthanthenumberofinputunits(e.g.100),thenetworkisforcedtolearnacompressedrepresentationoftheinput.Becauseitmusttrytoreconstructtheinputx∈R100fromitshiddenactivationsrepresentationh(2)∈R50.Iftheinputwerecompletelyrandomthenthiscompressiontaskwouldbe1ThischoicegivesthehiddenrepresentationthatverysimilartoPCA’s18Chapter3.DeepArchitecturesandRelatedModelsverydiﬃcult.However,ifthereisstructureinthedata,forexample,ifsomeoftheinputfeaturesarecorrelated,thenthismodelwillbeabletodiscoversomeofthosecorrelations.Iftheoutputunitsarelinear(i.e.linearreconstruction)andthemeansquarederrorcriterionisusedtotrainthenetwork,thenthatsimpleautoencoderoftenendsuplearningalow-dimensionalrepresentationverysimilartoPCA’s.2.However,ifthehiddenunitsarenon-linear,theautoencoderbehavesverydiﬀerenctlyfromPCA,withtheabilitytocapturemulti-modalaspectsoftheinputdistribution[1].Anautoencoderwithmanyhiddenlayersiscalleddeepautoencoder.Deepautoencoderisaspecialtypeofdeepneuralnetworkandusefulindimensionalityreduction,whichwillbeintroducedshortlyinthefollowingsection.Besides,manydiﬀerenttypesofautoencoderhavebeenintroducedrecently,whichareextremelyusefulindeeplearningandfeatureextraction.Inthissection,Iwillpresentconciselyseveralmostinterestingkindsofautoencoder,whichcanbeusedinpre-trainingprocess(replacingtheRBMmodel)orfeatureextraction.Inthenextchapter,Iwilldiscussmoreaboutgreedylayer-wisepre-trainingprocess,whichisoneofthemostimportantinventionintheareaofdeeplearning.3.3.2SimilaritybetweenautoencoderandRBMsTheRBMsandbasicclassicalautoencodersareverysimilarintheirfunctionalform,althoughtheirinterpretationandtheproceduresusedfortrainingthemarequitediﬀer-ent[48].Morespeciﬁcally,thedeterministicfunctionthatmapsfrominputtohiddenrepresentationisthesameforbothmodels.Oneimportantdiﬀerenceisthatdeterminis-ticautoencodersusereal-valuedmeanastheirhiddenrepresentationwhereasstochasticRBMssampleabinaryhiddenrepresentationfromthatmean.However,aftertheirinitialpretraining,thewaylayersofRBMsaretypicallyusedinpracticewhenstackedinadeepneuralnetworkisbypropagatingthesereal-valuedmeans,whichismoreinlinewiththedeterministicautoencoderinterpretation.Thereconstructionerrorofanautoencodercanalsobeseenasanapproximationofthelog-likelihoodgradientinanRBM,inawaythatissimilartotheapproximationmadebyusingContrastiveDivergenceupdatesforRBMs[2].ThissimilaritycanexplainwhyinitializingadeepnetworkbystackingautoencodersyieldsalmostasgoodaclassiﬁcationperformanceaswhenstackingRBMs[3].However,manydiﬀerenttypesofautoencoderhavebeenintroducedrecently,whichcanoutperformtheRBMmodelindiﬀerentbenchmarks.3.3.3SparseautoencoderAsshownabove,withoutanyconstraint,theautoencoderwilltrytolearntheidentityfunction.Toﬁxthisproblem,wecanusediﬀerenttypeofconstraint.Onecommonconstraintusedinbasicclassicalautoencoderislimitingthenumberofhiddenunits.Wecanalsoaddweight−decayintotheobjectivefunction,whichfavorssmallweightsby2Principalcomponentanalysis(PCA)isastatisticalprocedurethatusesorthogonaltransformationtoconvertasetofobservationsofpossiblycorrelatedvariablesintoasetofvaluesoflinearlyuncorrelatedvariablescalledprincipalcomponents.Itisaveryoldprocedure(KarlPearson,1901)andusuallyusedindimensionalityreduction-Wiki3.3.AutoEncoders19addingthetermλPijW2ijJAE+wd(θ)=(Xx∈DnL(x,g(f(x))))+λXijW2ij(3.10)However,thisweight-decayconstraintalonedoesnothelpmuchandmustbeusedtogetherwithotherconstraints.Oneveryusefulconstraintissparsityconstraintonthehiddenunits,whichcanforceautoencoderstodiscoverinterestingstructureinthedata,evenifthenumberofhiddenunitsislarge[28]Informally,wewillthinkofaneuronasbeing“active”(oras“ﬁring”)ifitsoutputvalueiscloseto1,orasbeing“inactive”ifitsoutputvalueiscloseto0.Wewouldliketoconstraintheneuronstobeinactivemostofthetime.Toimposethisconstrain,weﬁrstcalculatetheaverageactivationofhiddenunitsoverthetrainingset.Lettheactivationofhiddenunitjwhenthenetworkisgivenaspeciﬁcinputxishj(x)andadatasetwithmtrainingexamples,theaverageactivationofhiddenunitjcanbecalculatedby:ˆρj=1mmXi=1hj(x(i))(3.11)Wewouldliketo(approximately)enforcetheconstraintˆρj=ρ(3.12)whereρisasparsityparameter,typicallyasmallvalueclosetozero(sayρ=0.05).Inotherwords,wewouldliketheaverageactivationofeachhiddenneuronjtobecloseto0.05(say).Tosatisfythisconstraint,thehiddenunit’sactivationsmustmostlybenear0.Toachievethis,wewilladdanextrapenaltytermtoouroptimizationobjectivethatpenalizesˆrhojdeviatingsigniﬁcantlyfromρ.Manychoicesofthepenaltytermcangivereasonableresults.AtypicaloneistheKullback-Leibler(KL)divergence.KL-divergenceisastandardfunctionformeasuringhowdiﬀerenttwodiﬀerentdistributionsare.ThispenaltyfunctionhasthepropertythatKL(ρkˆρj)=0ifˆρj=ρ,andotherwise,itincreasesmonotonicallyasˆρjdivergesfromρ.Thissparsitypenaltytermcanbecalculatedusingfollowingequation:dhXj=1KL(ρkˆρj)=dhXj=1ρlogρˆρj+(1−ρ)log1−ρ1−ˆρj(3.13)Ouroverallcostfunctionisnow:JsparseAE(θ)=JAE(θ)+βdhXj=1KL(ρkˆρj)(3.14)whereJAE(θ)isasdeﬁnedinbasicclassicalautoencoder,andβcontrolstheweightofthesparsitypenaltyterm.ToincorporatetheKL-divergencetermintobackpropagationoptimizationprocess,thereisasimple-to-implementtrickinvolvingonlyasmallcodechange,whichisexplainedclearlyin[28]Thesparseautoencoderisverygoodatlearningusefulrepresentations/featuresfromdiﬀerentinputdomains(suchasimage,audio,...),andthereforeitisusuallyusedasfeature20Chapter3.DeepArchitecturesandRelatedModelsextractor.Byﬁrstusingsparseautoencodertoextractfeaturesfromrawinput,andthentrainingatypicalclassiﬁersuchasaneuralnetworkorSVMonthesefeatures,wecangetaverygoodresultondiﬀerenttaskssuchasMNISTandCIFAR-10.Figure3.7showstheﬁltersthatlearnedbysparseautoencoderwhentrainedonMNISTdataset.Figure3.7:Takenfrom[28]Eachsquareintheﬁgureaboveshowsthe(normbounded)inputimagexthatmaximallyactivesoneof100hiddenunits.Weseethatthediﬀerenthiddenunitshavelearnedtodetectedgesatdiﬀerentpositionsandorientationsintheimage.3.3.4DenoisingautoencoderWehaveseenthatthereconstructioncriterionaloneisunabletoguaranteetheextrac-tionofusefulfeaturesasitcanleadtotheobvioussolution“simplycopytheinput”orsimilarlyuninterestingones.Toovercomethatproblem,onestrategyistoconstraintherepresentation:thetraditionalbottleneck(i.e.limitingthenumberofhiddenunits)andthemorerecentsparserepresentationsbothfollowthisstrategy.AnotherdiﬀerentstrategyintroducedbyVincentetal.2010[48]isthatwecanchangethereconstructioncriterionforabothmorechallengingandmoreinterestingobjective:cleaningpartiallycorruptedinput,orinshortdenoising.Vicentetal.2010suggestedthat“agoodrepresentationisonethatcanbeobtainedrobustlyfromacorruptedinputandthatwillbeusefulforrecoveringthecorrespondingcleaninput”.Theyexpectthatahigher-levelrepresentationshouldberatherstableandrobustundercorruptionsoftheinput,andperformingthedenoisingtaskwellrequiresextractingfeaturesthatcaptureusefulstructureintheinputdistribution.Basedonthatstrategy,Vincentetal.2010introducedaverysimplevariantofthebasicautoencoder,whichiscalleddenoisingautoencoder(DAE).DAEistrainedtoreconstructaclean“repaired”inputfromacorruptedversionofit.Eachtimeatrainingexamplexispresented,adiﬀerentcorruptedversionˆxofitisgeneratedaccordingtoastochasticmappingˆx∼qD(ˆx|x).Diﬀerentcorruptiontypescanbeconsidered.Butthetwomostcommonlyusedcorrup-tionsareadditiveisotropicGaussiannoise:ˆx=x+(cid:15),(cid:15)∼N(0,σ2I)andbinarymaskingnoise,whereafractionvofinputcomponents(randomlychosen)havetheirvaluesetto0.Thedegreeofcorruption(σorv)controlsthedegreeofregularization.3.3.AutoEncoders21Thedenoisingautoencoderisabletolearnusefulfeaturesindiﬀerentinputdomainslikesparseautoencoder(Figure3.8).Besides,wecanstackmanylayersofdenoisingautoencoderstopre-trainadeepneuralnetwork,similarlytothewaywestackRBMs.Thispre-trainingmethodevenyieldsbetterresultsinmanydiﬀerentdatasets[48].Figure3.8:Takenfrom[48]Left:regularautoencoderwithweightdecay.Thelearnedﬁlterscannotcaptureinterestingstructureintheimage.Right:adenoisingautoencoderwithadditiveGaussiannoise(σ=0.5)learnsGabor-likelocalorientededgedetectors.Verysimilartotheﬁlterslearnedbysparseautoencoder.3.3.5ContractiveautoencoderSalahRifaietal.2011[39]proposedanewvariantofautoencodercalledContractiveautoencoder(CAE).Theyintroducedanewpenaltyterm,whichencouragesthehiddenrepresentationtoberobusttosmallchangesoftheinputaroundthetrainingexamples.Theyhypothesizedthatwhereastheproposedpenaltytermencouragesthelearnedfeaturestobelocallyinvariantwithoutanypreferenceforparticulardirections,whenitiscombinedwithareconstructionerrororlikelihoodcriterion,weobtaininvarianceinthedirectionsthatmakesenseinthecontextofthegiventrainingdata,i.e.,thevariationsthatarepresentedinthedatashouldalsobecapturedinthelearnedrepresentation,buttheotherdirectionsmaybecontractedinthelearnedrepresentation[39].Ifinputx∈Rdxismappedbyencodingfunctionftohiddenrepresentationh∈Rdh,thentoencouragerobustnessoftherepresentationf(x)foratraininginputx,theyproposedtheFrobeniusnormoftheJacobianJf(x).Thispenaltyisthesumofsquaresofallpartialderivativesoftheextractedfeatureswithrespecttoinputdimensions:kJf(x)k2F=Xij(∂hj(x)∂xi)2(3.15)BypenalizingkJf(x)k2F,wewanttokeepalltheﬁrstderivativesofthehiddenunitsrespecttoinputunitstobesmall,whichinducesﬂatnessinmappingfunction.Inotherwords,weencouragethemappingtothefeaturespacetobecontractiveintheneigh-borhoodofthetrainingdata.Theﬂatnesswillimplyaninvarianceorrobustnessoftherepresentationforsmallvariationsoftheinput.TheobjectivefunctionforCAEisnow:JCAE(θ)=Xx∈Dn(L(x,g(f(x)))+λkJf(x)k2F)(3.16)Computingthenewpenaltyanditsgradientissimilartoandhasaboutthesamecostascomputingthereconstructionerroranditsgradient.Pleasecheckthe[39]formoredetails.22Chapter3.DeepArchitecturesandRelatedModelsTheCAEhascloserelationshipwithweightdecay,sparseAE,andDAE.•Weightdecay:TheFrobeniusnormoftheJacobianbecomestheL2weightdecayinthecaseofalinearencoder(i.e.whensfistheidentityfunction).Inthiscase,JCAE=JAE+wd.•Sparseautoencoders:sparseautoencodersencouragemajorityofthehiddenrepre-sentationclosetozero.Forthesefeaturestobeclosetozero,theymusthavebeencomputedintheleftsaturatedpartofthesigmoidnon-linearity,whichisalmostﬂatwithatinyﬁrstderivative.ThisyieldsacorrespondingsmallentryintheJacobianJf(x).•Denoisingautoencoders:Robustnesstoinputperturbationswasalsooneofthemotivationsofthedenoisingautoencoder.However,theCAEandtheDAEdiﬀerinthewaytheyencouragetherobustness.TheCAEcanbeusedasthebuildingblockinthelayer-wisepre-trainingstrategy,justliketheDAE.Itgivesslightlybetterresultsonsomedatasetsandsettings.Figure3.9showperformancecomparisonofdiﬀerentautoencodersforunsupervisedpre-traininga1000-unitsonehiddenlayernetworkonMNISTandCIFAR10-bw(grayscaleversionofCIFAR10)datasets.•RBM-binary:RestrictedBoltzmannMachinetrainedbyContrastiveDivergence,•AE:Basicautoencoder,•AE+wd:Autoencoderwithweight-decayregularization.•DAE-g:DenoisingautoencoderwithGaussiannoise,•DAE-b:Denoisingautoencoderwithbinarymaskingnoise.Figure3.9:Takenfrom[39]Resultsaresortedinascendingorderofclassiﬁcationerroronthetestset.Bestperformerandmodelswhosediﬀerencewiththebestperformerwasnotstatisticallysig-niﬁcantareinbold.NoticehowtheaverageJacobiannorm(beforeﬁne-tuning)appearscorrelatedwiththeﬁnaltesterror.SATistheaveragefractionofsaturatedunitsperexample.3.3.AutoEncoders233.3.6DeepAutoEncodersAdeepautoencoderisjustanautoencoderwithmanyhiddenlayers(Figure3.10).Thismodelisextremelyusefulinreducingthedimensionalityofdata,wherewewanttotrans-formthehigh-dimensionaldataintoalow-dimensionalcode.Byreducingdimensionality,othertasksonhigh-dimensionaldatawillbecomemucheasiersuchasclassiﬁcation,vi-sualization,communication,orstorage.ThemodelhasbeenintroducedlongagobutcouldonlybecomepopularbyHinton’sdiscoveryofpre-trainingstrategy.ThismodelcanbeconsideredasanonlineargenerationofPrincipalcomponentanalysis(PCA),whichisasimpleandwidelyusedmethodfordimensionalityreductiontask.In[16],Hintonshownhowadeepautoencodercouldbetrainedusingpre-trainingstrategyFigure3.10.Ofcourse,wecanreplaceRBMmodelinhismethodbyanothertypeofbuildingblockmodels.Figure3.10:(Takenfrom[16])PretrainingconsistsoflearningastackofRBMs,eachhavingonlyonelayeroffeaturedetectors.ThelearnedfeatureactivationsofoneRBMareusedasthe“data”fortrainingthenextRBMinthestack.Afterthepretraining,theRBMsare“unrolled”tocreateadeepautoencoder,whichisthenﬁne-tunedusingbackpropagationoferrorderivativesFigure3.11showshowadeepautoencoderproducesmuchbettercompressedcodescomparedtoPCA.ThisexperimentconductedbyHintonetal.[16],inwhichaverydeepautoencoder784−1000−500−250−2wasusedonMNISTdataset.Wecanseethatatwo-dimensionalautoencoderproducedabettervisualizationofthedatathandidtheﬁrsttwoprincipalcomponents.24Chapter3.DeepArchitecturesandRelatedModelsFigure3.11:(Takenfrom[16])(A)Thetwodimensionalcodesfor500digitsofeachclassproducedbytakingtheﬁrsttwoprincipalcomponentsofall60,000trainingimages.(B)Thetwodimensionalcodesfoundbya784−1000−500−250−2autoencoderChapter4Gradient-BasedTrainingAlgorithmsforNeuralNetworksInneuralnetworks,optimizationtechniquesplayanimportantrole,especiallyinsuper-visedlearning.Theyareusedtoexplorethehypothesisspaceandﬁndthebestconﬁgu-ration,whichminimizesanobjectivefunction(e.g.mean-squareerror,cross-entropyer-ror...).First-orderoptimizationmethodssuchasSteepestGradientDescent(SGD)usedtobethemostpopularmethodduetotheirsimpleimplementationsandcomputationaleﬃciency,comparedtothesecond-ordermethods(e.g.Newton’smethods).However,ﬁrst-ordermethodsareeasytobetrappedinlocalminimaandexhibitslowconvergence,duetotheproblemsdescribedbelow.Tohelpﬁrst-ordermethodsovercomethoseprob-lems,momentum-relatedtechniquesandwell-designedinitializationmethodshavebeenproposedrecently[46].ThesetechniquesaresuccessfullyexperimentedontrainingdeepautoencodersandRecurrentNeuralNetworks(RNN),whichwasbelievedtobeimpos-sible(evenwith2nd-ordermethods).Theseimprovementscouldpotentiallychangethe“wrong”beliefabouttheabilityofﬁrst-ordermethods.Ontheotherhand,someofthesecond-othermethodsarecomputationallyinfeasiblebutshowmuchbetterconvergencecharacteristicsbecausetheytakeintoaccountthecur-vatureoftheerrorspace.Thereasonofitsin-feasibilityisthecomputationoftheinverseofHessianmatrix(O(n3)),whichcouldbetackledbycomputingtheinversematrixdi-rectly(asinquasi-Newton’smethod);approximationofit(asinGauss-Newton’smethod);orusinganincompleteoptimization(asinHessian-Freemethod).Thischapterattemptstogiveacompleteoverviewaboutallcurrentwell-knownop-timizationtechniquesappliedinNeuralNetwork,theirbelievedshortcomingsaswellasadvantages.4.1First-orderMethodsAllﬁrst-ordermethodsbasesontheﬁrst-orderTaylorseriesexpansiontoapproximatetheobjectivefunctionF(i.e.costfunction)atcurrentstateofweightconﬁgurationw:F(w+∆w)≈F(w)+∇F(w)∆w∇F(w):gradientvectorofF(w)(4.1)2526Chapter4.Gradient-BasedTrainingAlgorithmsforNeuralNetworksTherefore,ifwegofromtheweightconﬁgurationwtothenextconﬁgurationatthedirectionof∆w=−α∇F(w)(αislearningrate),wewillget:F(wnext)=F(w−α∇F(w))≈F(w)−α∇F(w)2≤F(w)ifαsmallenoughThecostfunctionisdecreasedasweupdatetheweight.However,thereasoningpresentedhereisapproximateanditistrueonlyforsmallenoughlearningrates.4.1.1SteepestGradientDescentSteepestGradientDescent(SGD)isastandardﬁrst-ordermethod,whichdirectlyusestheproofabove.ThismethodassumesthatthecostfunctionF(w)decreasesfastestifwegointhedirectionofnegativegradientofF(w).TheSGDconvergesslowlytotheoptimalsolution,andthelearning-rateparameterαhasaprofoundinﬂuenceonitsconvergencebehaviors.Ifthelearningrateissmall,themethodismorestablebutconvergesslowly.Ifthelearningrateislarge,itcouldfollowsazigzagging(oscillatorypath)totheoptimalsolution,orevendivergeifitistoolarge.However,thesizeoflearningratesdependsontheshapeoftheerrorplaneatcurrentpoint,sowecouldnoteasilydeterminetheappropriatelearningratesatdiﬀerentpointoflearningprocess.Inpractice,weusuallysetthelearningrateslargeratthebeginninganddecreaseitdownwhenreachingtheoptimalsolution.Thesecond-ordermethodstacklethisproblembyusingthecurvatureinformationoftheerrorplanetosomehow“adjust”thelearningrates.4.1.2StochasticandMini-BatchesGradientDescentInsteepestgradientdescent(i.e.batchgradientdescent)method,thegradientvectorofthecostfunctionFiscalculatedbysummingupthegradientsofeachtrainingcase.Inotherwords,wehavetogothroughthewholetrainingsettomakeasingleupdateontheweightsmatrix.Updatingthiswayisineﬃcientifthetrainingsetislarge,whichisusuallyhappensinpractice.Thestochastic(i.e.online)gradientdescentapproximatesthegradientvector∇Fbycalculatethegradientofeachtrainingcaseanduseittoupdatetheweightsmatrixrightaway.Thismethodleadstoazigzaggingsearchingpath,becausewearenotfollowingthesteepestgradientoftheerrorplane.Thestochasticgradientdescenttendstoworkbetterthanthesteepestgradientdescentonlargedatasetswhereeachiterationofgradientdescentisveryexpensive.Besides,ifeachtrainingcaseisusedonlyonetime,anynewtrainingcasesaresuppliedeveryday,thismethodiscapableofforgettingtheoldtrainingcase.Thereisacompromisebetweenthebatchandstochasticmethod,whichisoftencalled”mini-batches”,wherethetruegradientisapproximatedbyasumoverasmallnumberoftrainingcases.4.1.3MomentumandNesterov’sAcceleratedGradientTheSGDmethodusuallygettroublewithplateausorlongnarrowvalleys(i.e.pathologicalcurvature)intheobjectivefunction.Itisbecausethegradientistoosmall(e.g.inplateau)orthecurvatureistoohigh(e.g.longnarrowvalley).Themomentummethodisusedtoacceleratetheoptimizationalongdirectionsoflowbutpersistentreduction(in4.1.First-orderMethods27plateau),similarlytothewaythesecond-ordermethodsacceleratetheoptimizationalonglow-curvaturedirections(butsecond-ordermethodsalsodeceleratetheoptimizationalonghighcurvaturedirections,whichisnotdonebymomentummethods)[45]Themomentummethodmaintainsavelocityvectorvtwhichisupdatedasfollows:vt+1=µvt−α∇F(wt)wt+1=wt+vt+1Themomentumdecaycoeﬃcientµ∈[0;1)controlstherateatwhicholdgradientsarediscarded.Itsphysicalinterpretationisthe“friction”ofthesurfaceoftheobjectivefunction,anditsmagnitudehasanindirecteﬀectonthemagnitudeofthevelocity[45].AvariantofmomentumknownasNesterov’sacceleratedgradient(Nesterov,1983)(NAG)hasbeenanalyzedwithcertainschedulesofthelearningrateandofthemomentumdecaycoeﬃcientµ,andwasshownbyNesterov(1983)toexhibitthebetterconvergencerateO(1/T2)versustheO(1/T)ofSGD[46].NAGformulas:vt+1=µvt−α∇F(wt+µvt)wt+1=wt+vt+1Whilemomentummethodcomputethegradientupdatefromthecurrentpositionwt,NAGﬁrstperformsapartialupdatetowt,computingwt+µvt,butmissingtheyetunknowncorrection.ThisdiﬀerenceseemstoallowNAGtochangevinaquickerandmoreresponsiveway,lettingitbehavemorestablythanmomentuminmanysituations,especiallyforhighervaluesofµ[46].Choosinganappropriatescheduleforthemomentumµandlearningrateαisdiﬃcultandusuallytunedinaheuristicway.Asshowedin[46],Ilyaet.al.2013havetraineddeepautoencodersbyusingthesefollowingformulas:µ=min(1−2−1−log2(bt/250c+1),µmax)(4.2)whereµmaxwaschosenfrom{0.999,0.995,0.99,0.9,0}.Foreachpointofµmax,thelearningrateαischosenfrom{0.05,0.01,0.005,0.001,0.0005,0.0001}.Besides,theyfounditbeneﬁcialtoreduceµto0.9duringtheﬁnal1000parameterupdatesoftheoptimizationwithoutreducingthelearningrate.(see[46]formoreinformationaboutmomentumscheduleanditseﬀectonconvergencerateandqualityofﬁnalresult)4.1.4SparseInitializationConvexobjectivefunctionsF(w)areinsensitivetotheinitialparametersetting,sincetheoptimizationwillalwaysrecovertheoptimalsolution,merelytakinglongertimeforworseinitializations.But,giventhatmostobjectivefunctionsF(w)thatwewanttooptimizearenon-convex,theinitializationhasaprofoundimpactontheoptimizationandonthequalityofthesolution.Ilyaet.al.2013shownthatappropriateinitializationsplayanevengreaterrolethanpreviouslybelievedforbothdeepandrecurrentneuralnetworks[46].Unfortunately,itisdiﬃculttodesigngoodrandominitializationsfornewmodels,soitisimportanttoexperimentwithmanydiﬀerentinitializations[45].Aninitializationschemethatwassuccessfullyusedtotraindeepandrecurrentneuralnetworksin[46]iscalledsparseinitialization.Inthatinitializationscheme,eachrandomunitisconnectedto15randomlychosenunitsinthepreviouslayer,whoseweightsare28Chapter4.Gradient-BasedTrainingAlgorithmsforNeuralNetworksdrawnfromaunitGaussian,andthebiasesaresettozero.Theintuitivejustiﬁcationisthatthetotalamountofinputtoeachunitwillnotdependonthesizeofthepreviouslayerandhencetheywillnotaseasilysaturate.Meanwhile,becausetheinputstoeachunitarenotallrandomlyweightedblendsoftheoutputsofmany100sor1000sofunitsinthepreviouslayer,theywilltendtobequalitativelymore“diverse”intheirresponsetoinputs.4.1.5ResilientBackPropagationResilientBackProbagation(Rprop)isaﬁrst-orderlocaladaptivelearningscheme,per-formingsupervisedbatchlearninginmulti-layerneuralnetworks[38].ThebasicprincipleofRpropistoeliminatetheharmfulinﬂuenceofthesizeofthepartialderivativeontheweightstep:smallinplateausandlargeinravines.Itusesonlythesignofthederivativetoindicatethedirectionoftheweightupdate.Thesizeoftheweightchangeisexclusivelydeterminedbyaweight-speciﬁc,so-called“update-value”∆ij:w(t)ij=−∆(t)ijifδE(t)δwij>0+∆(t)ijifδE(t)δwij<00else(4.3)ThesecondstepofRproplearningistodeterminethenewupdate-values∆(t)ij.Thisisbasedonasign-dependentadaptationprocess:∆(t)ij=−η+∗∆(t−1)ijifδE(t−1)δwij∗δE(t)δwij>0+η−∗∆(t)ijifδE(t−1)δwij∗δE(t)δwij<0∆(t−1)ijelse(4.4)Where0<η−<1<η+.Theη+isempiricallysetto1.2andη−to0.5[38]Theadaptive-ruleworksasfollows:everytimethepartialderivativeofthecorrespondingweightwijchangesitssign,whichindicatesthatthelastupdatewastoobigandthealgorithmhasjumpedoveralocalminimum,theupdate-value∆(t)ijisdecreasedbythefactorη−.Ifthederivativeretainsitssign.Theupdatevalueisslightlyincreasedinordertoaccelerateconvergenceinshallowregions.Generally,theRpropmethodconvergesfasterthanstandardgradientdescentmethod.Besides,wedonothavetosetanymeta-parameters(suchaslearningrates)forRproptoobtainoptimalconvergencetimes.AnotherinterestingpropertyofRpropisthatthesizeoftheweight-stepisonlydepen-dentonthesequenceofsigns,notonthemagnitudeofthederivative.Therefore,learningisspreadequallyallovertheentirenetworks:weightsneartheinputlayerhavetheequalchancetogrowandlearnasweightsneartheoutputlayer.Thispropertycanhelptoovercomethevanishinggradientproblemwhentrainingdeepneuralnetworks.4.2Second-orderMethodsAllsecond-ordermethodsbaseontheideaofminimizingthequadraticapproximationofthecostfunction(usingthesecond-orderTaylorseriesexpansion).However,eachofthesemethodsbelowarediﬀerentonhowtheycalculate(orapproximate)theinversionofHessianmatrix.4.2.Second-orderMethods294.2.1Newton’sMethodsThisisthestandardsecond-ordermethods.Speciﬁcally,usingasecond-orderTaylorseriesexpansionofthecostfunctionF()aroundthepointw,wewillhave:F(w+∆w)≈F(w)+∇F(w)∆w+12∆wTH∆w(4.5)WithHistheHessian(i.e.curvature)matrixofF(w).WeattaintheextremeofFwhenitsderivativewithrespectto∆wisequalto0.Therefore,wehavetosolvethefollowingequation:∇F(w)+H∆w=0⇔∆w=−H−1∇F(w)However,ﬁndingtheinverseoftheHessianinhigh-dimensionalspacecanbeanex-pensiveoperationO(n3).Insuchcases,insteadofdirectlyinvertingtheHessian,wecancalculateitasasolutionofthesystemoflinearequations:H∆w=−∇F(w).Thiscanbesolvedbyusingiteratemethodslikeconjugategradient(describedbelow,usedinHessian-Freemethod).However,conjugategradientrequiresHessianmatrixbeapositivedeﬁnitematrix,whichisnotguaranteedduringtrainingprocess.Besides,wecanalsoapproximatetheinversionofHessianmatrixdirectlyfromchangesinthegradient,whichisusedinquasi-Newton’smethods(e.g.DFP,BFGS,L-BFGS,...).4.2.2Gauss-Newton’smethodTheGauss-Newtonalgorithmisamethodused(andcanonlybeused)tosolvenon-linearleastsquaresproblemsandcanbeseenasamodiﬁcationofNewton’smethod.InsteadofcomputingtheHessianmatrix,itusestheJacobianmatrixtoapproximateit.Therefore,thismethodhastheadvantagethatsecondderivatives,whichcanbechallengingtocompute,arenotrequired.Inleastsquaresproblem,givenmtrainingexampleswhichtheirerrorcostarecalcu-latedbymfunctionsr1..mwehavetoﬁndtheminimumofthesumofsquares:F(w)=mXi=1r2i(w)(4.6)Diﬀerentiatingtheaboveequationwithrespecttowj(anelementofw),wehavetheelementsofgradientvectorofF():∇F(w)j=2mXi=1ri∂ri∂wj(4.7)Diﬀerentiatingtheabovegradientelementrespecttowk,wecouldgettheelementsoftheHessianmatrix:Hjk=2mXi=1(∂ri∂wj∂ri∂wk+ri∂2ri∂wj∂wk)(4.8)TheGauss-Newtonmethodisobtainedbyignoringthesecond-orderderivativeterms(thesecondterminaboveexpression).SotheHessianisapproximatedby:Hjk=2mXi=1(∂ri∂wj∂ri∂wk)=2mXi=1JijJik(4.9)30Chapter4.Gradient-BasedTrainingAlgorithmsforNeuralNetworkswhereJij=∂ri∂wjareentriesoftheJacobianJrmatrix.Therfore,Gauss-Newton’smethodsapproximateHessianby:H≈2JTrJr(4.10)ThisapproximationofHisusuallycalledasGauss-NewtonmatrixG,andthisisusedinHessian-FreemethodinsteadofHmatrixbecauseitisguaranteedtobepositivesemi-deﬁnite,whichavoidstheproblemofnegativecurvature.4.2.3Quasi-Newton’smethodsInquasi-Newtonmethods,theHessianmatrixdoesnotneedtobecomputed.TheHessianisupdatedbyanalyzingsuccessivegradientvectorsinstead.Themostcommonquasi-NewtonalgorithmsarecurrentlytheSR1formula(forsymmetricrankone),theBHHHmethod,thewidespreadBFGSmethod(suggestedindependentlybyBroyden,Fletcher,Goldfarb,andShanno,in1970),anditslow-memoryextension,L-BFGS.IfwecallBisanapproximationofHessian,applytheﬁrst-orderTaylorseriesexpansiononthegradientofcostfunctionF(),wehavethefollowingsecantequation:∇F(w+∆w)=∆F(w)+B∆w(4.11)Thevariousquasi-Newtonmethodsdiﬀerintheirchoiceofthesolutiontothesecantequa-tion.Mostmethods(butwithexceptions,suchasBroyden’smethod)seekasymmetricsolutionBT=B4.2.4ConjugategradientmethodInmathematics,theconjugategradient(CG)methodisaniterativemethodtosolvethesystemsoflinearequations.InHessian-Freeoptimizationmethod,theyuseconjugategradientmethodtosolvethelinearequation:H∆w=−∇F(w).Insteadofsolvingthisdirectly,CGmethodtriestominimizethisobjectivefunction,whichbecomessmallerwhenwecomeclosertothesolution.E(∆w)=12∆wTH∆w−∆wTb(4.12)Therefore,wehavetoﬁnd∆wthatminimizethefunctionE().Brieﬂy,CGﬁndsthesolutionbyusingasequenceofsteps,eachofwhichﬁndstheminimumalongonedirection.Besides,itmakessurethatnewdirectionis“conjugate”tothepreviousdirectionssoyoudonotmessuptheminimizationyoualreadydid.Infact,the“conjugate”meansthatasyougointhenewdirection,youdonotchangethegradientsinthepreviousdirections.4.2.5Hessian-FreeoptimizationmethodHFdiﬀersfromotherNewton’smethodsonlybecauseitisperforminganincompleteoptimization(viaun-convergedConjugategradient)ofapproximationofF()[26].TheﬁrstmainthingaboutHessian-Freeoptimizationisthatitusesconjugategradientmethodtoﬁndthe∆w(directiontogo)fromthelinearequationH∆w=−∇F(w)insteadofcomputingtheinversionofHessianmatrix(directlyorindirectly).Besides,inthismethod,wedonotwaitfortheCGtototallyconverge.4.3.LineSearchvs.TrustRegionStrategy31ThesecondthingisthatweuseGauss-NewtonmatrixG(insteadofHessian)becauseitisguaranteedtobepositivesemi-deﬁnite,andexperimentallyshowedtobeconsistentlybetterthanH.ThethirdvitalimportantthinginHFmethodisitsdampingtechnique(alsocalledstructuralregularization).HFmethodcouldﬁndadirectionwithextremelylowcurvatureandwillelecttomoveveryfaralongit,andpossiblywelloutsideoftheregionwhereTaylorseriesexpansionisasensibleapproximation.Therefore,weaddthedampingparameterλtocontrolhow“conservative”theapproximationis,byaddingtheconstantλkdk2tothecurvatureestimateforeachdirectiond.wecanalsousethe“Levenberg-Marquardt”styleheuristicforadjustingλovertime.Theﬁnalimportantthingisaboutthesparseinitializationschemedescribedabove.4.3LineSearchvs.TrustRegionStrategyIniterativeoptimizationtechniques(includingalltypeofﬁrst-orderandsecond-ordermethods),thereistwomainstrategies:linesearchandtrustregion.Inlinesearchap-proach,oneﬁrstﬁndsthedescentdirection(oftheobjectivefunctionf)andthencomputesanappropriatestepsize.Intrustregionapproach,oneﬁrstdeterminesastepsize(thesizeofthetrustregion)andthenﬁndsthestepdirection.Generally,thelinesearchapproachisusuallyusedtoadaptthelearningrateinﬁrst-ordermethods,whilethetrustregionapproachisusedasadamping/regularizationtechniqueinsecond-ordermethods.4.3.1LineSearchInlinesearchapproach,thestepdirectionisﬁrstcomputedbyothermethods,suchasgradientdescent,Newton’smethod,orQuasi-Newtonmethod....Thestepsizethencanbedeterminedeitherexactlyorinexactlyusingmanydiﬀerentrules.In[43],Shi2004hassummarizedandanalyzedsevendiﬀerentlinesearchrules.Inthissection,weprovidethebasicideasofthefourmostpopularrules,whichare:minimizationrule,Limitedminimizationrule,approximateminimizationrule,andArmijorule.AssumethatFisourobjectivefunction,wkisweightsconﬁgurationatstepkth,∆wkischosendirectionatstepkth,αkisstepsize(orlearningrateifusingwithﬁrst-ordermethods).Theαkcanbecomputedexactlyorinexactlyusingthesefollowingrules:1.Minimizationrule:Inthisrule,wetrytoﬁndthestepsizeαkthatcanminimizetheobjectivefunctionFinchosendirection∆wk.ThisruleisimplicitlyusedbyConjugategradientmethod.αk=argminα>0F(wk+α∆wk)(4.13)2.Limitedminimizationrule:Thisruleissimilartotheﬁrstrule,exceptthatwelimitthemaximumstepsizebysk=−∇F(∆wk)T∆wk||∆wk||2.Notethat∇F(∆wk)T∆wkistheexpecteddecrementinFifthestepis∆wk,approximatedbyﬁrstorderTaylorseries.αk=argminα∈[0,sk]F(wk+α∆wk)(4.14)3.Approximateminimizationrule:Inthisrule,wetrytogoasfaraspossible,aslongastheobjectivefunctionisstillgettingdecreased.Intheminimizationrule,wego32Chapter4.Gradient-BasedTrainingAlgorithmsforNeuralNetworksdirectlytotheglobalminimaalongthechosendirectionoftheobjectivefunction,whileinthisrule,wegotothenearestlocalminimaonthatdirection.αk=min(α|∇F(wk+α∆wk)T∆wk=0,α>0)(4.15)4.Armijorule:ThisruleensuresthatthesteplengthαkdecreasesF“suﬃciently”,byusingthisinequality:F(wk)−F(wk+αk∆wk)≥−c1αk∇F(wk)T∆wk(4.16)ThisinequalityensuresthattherealdecrementinFisbiggerthanaproportionc1oftheexpecteddecrement.c1isusuallychosentobequitesmall,say10−4.However,becausetheaboveinequalityisalwayssatisﬁedwhenαissmallenough.Therefore,topreventchoosingtoosmallstepsize,weusuallyaddthefollowingcurvatureconditiontoensuresthattheslopehasbeenreducedsuﬃciently.∇F(wk+αk∆wk)T∆wk≥c2∇F(wk)T∆wk(4.17)Inpractice,besidesusinglinesearchtocontrolthelearningrateinﬁrst-ordermethods,thelearningrateisalsousuallysetbyαk=ab+k,witha,barepredeﬁnedconstants.4.3.2TrustRegionInalmosteveryiterativeoptimizationtechniques,theobjectivefunctionisapproximatedusingacertainmodelfunction(e.g.quadraticinsecond-ordermethods).However,thatapproximationisonlygood(trusted)inalimitedregionaroundthesamplepoint-thetrustregion.Thetrustregionapproachesrestrictsthestepsizebyﬁrstcomputethetrustregion,andthenﬁndthebestdirectionwithinthatregion.Thetrustregionisexpandedwhentheapproximationisﬁttheobjectivefunctionwell,andcontractedotherwise.Thisisalsoknownastherestrictedstepmethod.Themodelﬁtisusuallyevaluatedbycomparingtheratioofexpectedimprovementfromthemodelapproximationwiththeactualimprovementobservedintheobjectivefunction.Thismethodisusuallyusedwiththesecond-ordermethodssuchasNewton’s,Gauss-NewtonorHessian-Freeoptimization.Generally,insecond-ordermethods,weﬁndthedescentdirection∆wbysolvingthefollowingequation:H∆w=∇F(w)(4.18)WithHistheHessianmatrixoritsapproximation.Torestrictthestepsize,thetrustregionmethodinsteadsolvesthefollowingequation:(H+λI)∆w=∇F(w)(4.19)WithIistheidentitymatrix,andλisthedampingparameterthatcontrolsthetrust-regionsize.Geometrically,thattermaddsaparaboloidcenteredat∆w=0tothequadraticform,resultinginasmallerstep.Iftheλislargeenough,theHessianmatrixwillbeignored,andthestepwillbetakenapproximatelyinthedirectionofthegradient.InLevenberg-Marquardtalgorithm,MarquardtreplacetheidentitymatrixIwiththediagonalmatrixconsistingofthediagonalelementsofH.Thisapproachcanscaleeach4.4.InitializationAnalysis33componentofthegradientaccordingtothecurvature,sothatthereislargermovementalongthedirectionswherethegradientissmaller:(H+λdiag(I))∆w=∇F(w)(4.20)Thedampingfactorλisadjustedbylookingattheratioρ=∆Factual∆fpred.In[26],Martens2010hassuccessfullyusedthisdampingmethod(usingidentitymetrix)withHessian-Freeoptimizationtotraindeepautoencoders.HeusedLevenberg-Marquardtstyleheuristicforadjustingλdirectly:ifρ<14:λ←32λelseifρ>34:λ←23λ4.4InitializationAnalysisIn[9],X.GlorotandY.Bengio2010haveprovidedaprofoundanalysisabouttheinﬂuenceofthenon-linearactivationsfunctions,costfunctiontypes,andinitializationmethods,onusingstandardgradientdescentfortrainingdeepneuralnetworks.Theyshowedthatthelogisticsigmoidactivationisunsuitedfordeepnetworkswithrandominitializationbecauseofitsmeanvalue,whichcandriveespeciallythetophiddenlayerintosaturation.Besides,theyfoundthatthelogisticregressionorconditionallog-likelihoodcostfunctioncoupledwithsoftmaxoutputsworkedmuchbetter(forclassiﬁcationproblems)thanthequadraticcost.Regardingtheinitialization,theyexplainedwhythestandardrandominitializationcouldleadtovanishinggradientproblemintrainingdeepneuralnetworks.Inthestandardrandominitialization,thebiasesisinitializedtobe0,andtheWijateachlayerissampledfromtheuniformdistribution:U[−1√n,1√n],wherenisthesizeofthepreviouslayerandU[−a,a]istheuniformdistributionintheinterval(−a,a).Theyprovedthatstandardinitializationwouldleadtovarianceoftheweightswiththefollowingproperty:niVar[Wi]=13(4.21)Whereniisthelayersize(assumingthatalllayershavethesamesize),andWiisweightsoflayerith.Thiswillcausethevarianceoftheback-propagatedgradienttobedependentonthelayer(anddecreasing).Wepreferthefollowingconditions,whichcankeepinformationﬂowing:∀i,niVar[Wi]=1∀i,ni+1Var[Wi]=1Toapproximatelysatisfytheobjectivesofmaintainingactivationvariancesandback-propagatedgradientsvarianceasonemovesupordownthenetwork,X.GlorotandY.Bengio2010haveproposedthefollowingnormalizedinitialization:W∼U[−√6√nj+nj+1,√6√nj+nj+1](4.22)4.5CascadecorrelationalgorithmCascade-Correlationisasupervisedlearningalgorithmforneuralnetworks.Insteadofjustadjustingtheweightsinanetworkofﬁxedtopology,Cascade-Correlationbeginswitha34Chapter4.Gradient-BasedTrainingAlgorithmsforNeuralNetworksminimalnetwork,thenautomaticallytrainsandaddsnewhiddenunitsonebyone,creatingamulti-layerstructure[8].Onceanewhiddenunithasbeenaddedtothenetwork,itsinput-sideweightsarefrozen.Thisunitthenbecomesapermanentfeature-detectorinthenetwork,availableforproducingoutputsorforcreatingother,morecomplexfeaturedetectors.Thisapproachhasseveraladvantages:itlearnsveryquickly,thenetworkdeterminesitsownsizeandtopology,itretainsthestructureithasbuiltevenifthetrainingsetchanges,anditrequiresnoback-propagationoferrorsignalsthroughtheconnectionsofthenetwork([8]).Chapter5OverﬁttingandRegularization5.1IntroductionIngeneral,bytrainingneuralnetworksusingasetofexampleswithinputandoutputpattern(i.e.supervisedlearning),wearetryingtoconstructamappingthatdeﬁnestheoutputpatternintermsoftheinputpatterns.However,theinformationcontentofthetrainingexamplesisordinarilynotsuﬃcientitselftoreconstructthatmapping,whichleadstothepossibilityofoverﬁtting[14].Hadamard(1902)hasdeﬁnedtheterm“well-posed”toindicatethatwhetheraprob-lemcouldbesolvedonacomputerusingastable(e.ireproducibleanduniqueresult)algorithmornot.Ifaproblemisnot“well-posed”,itissaidtobeill-posed.Theproblemofreconstructingthemappingfbetweeninputandoutputissaidtobewell-posedifHadamard’sthreeconditionsaresatisﬁed:•Existence:themappingfunction“f”exists.•Uniqueness:themappingfunctionisunique.•Continuity:slightchangeintheinputonlymakealimitedchangeintheoutput.However,inthecontextofsupervisedlearning,Hadamard’sconditionsareviolatedforthefollowingreasons[14]:•Themappingfunctionmaynotexist(acertainoutputmaynotexistforanyinput).•Wetypicallydonothaveasmuchinformationfromtrainingexamplesasweneedtoreconstructanuniquemapping.•Theunavoidablepresenceofnoiseintrainingexamplescouldleadtotheviolationonthecontinuitycriterion.Thereisnowaytoovercomethesediﬃcultiesunlesssomepriorinformationabouttheinput-outputmappingisavailable.Infact,whenweareconstructinganeuralnetworkforaspeciﬁctask(e.g.classiﬁcation,regression...),wehavealreadymadesomeassumptions(i.e.priorinformation,inductivebias)aboutwhatthesolutionshouldbe,suchas:numberofhiddenunits,typeofunits,numberoflayers...Regularizationisalsoasetofmethodstoembedpriorinformationintothelearningprocess.Themostcommonformofpriorinformationinvolvestheassumptionthattheinput-outputmappingfunctionissmooth(i.e.similarinputsproducesimilaroutputs)andsimple(Occam’sRazor).3536Chapter5.OverﬁttingandRegularizationFigure5.1:UnderﬁttingandOverﬁttinginclassiﬁcationtask,takenfrom[30]Figure5.2:UnderﬁttingandOverﬁttinginregressiontask,takenfrom[30]5.1.1Overﬁtting-WhatIsIt?Theoverﬁttingproblemhappenswhenamodelistrainedtoﬁtthetrainingdatasowellthatitlosesthegeneralityproperties(abilitytopredicttheoutputforanunseeninput).Thetrainingdatacontainsinformationabouttheregularitiesinthemappingfrominputtooutput.However,italsocontainssamplingerror,whichcomesfromthewaytheparticulartrainingcasesarechosen(notrepresentative),orthetechniqueweusetocollectthesetrainingcases.Therefore,whenweﬁtthemodeltothetrainingdata,itcannottellwhichregularitiesarerealandwhicharecausedbysamplingerror.Soifthemodelisveryﬂexible(i.e.powerful),itcanmodelthesamplingerrorreallywell,andfailtogeneralizetounseenexamples.InFigure5.1and5.2[30],youcanseehowthemodelsclassifyorﬁtthetrainingdataverywellinoverﬁttingcases,buttheyareprobablynotthegoodsolutions.Ingeneral,wepreferthesolutionthatis“smooth”,simple,andabletoexplainthetrainingdatawellenough,ratherthanacomplexwell-ﬁttedsolution.ThisinductivebiascanbeexplainedusingtheOccam’sRazor,whichwillbedescribedlater.Ontheotherhand,theunderﬁttingproblemhappenswhenthemodelcannotﬁtthetrainingdata.Ithappensbecausethemodelhaslowrepresentationalpowercomparedtocomplexityoftheproblem,orbecausethelearningalgorithmgetsstuck(e.g.inlocalminimaorravine).Thisisaseriousproblemwhentrainingdeeporrecurrentneuralnetworkswhicharebelievedtobehardtotrain,usingstandardlearningmethods.5.1.Introduction37Figure5.3:Errorcurvesincrossvalidationmethods5.1.2HowtoDetectOverﬁtting?Acommonmethodtodetectoverﬁttingistousevalidationset[27,30].Firstly,wesplitthewholedatasetintothreeparts:trainingdata,validationdata,andtestdata(typically60%,20%,and20%respectively).Then,weusethetrainingdatatotrainthemodel,usethevalidationdatatotunemeta-parameters(i.e.learningrate,momentum...),andusetestdatatotesttherealperformanceofmodeloverunseenexamples.Todetectoverﬁtting,wecouldslotthetwocurves:trainingerrorandvalidationerror(errorcalculatedonvalidationset)asshowninFigure5.4.Clearly,theoverﬁttingproblemhappenswhenthevalidationerrorstarttoincreasewhilethetrainingerrorisstillgettingimproved.5.1.3HowtoSolveOverﬁtting?Therearemanywaystopreventoverﬁtting.1.Getmoredata.Thisisthebestwaytoovercomeoverﬁtting,becausemoredataprobablyprovidesmoreinformationaboutsolutionandweakentheinﬂuenceofsamplingerror.However,collectingdataistypicallycostly(intermsoftimeandlabor).Besides,trainingwithmoredatawouldrequiremorecomputationalpower,whichcouldbeimpossibleinsomecases.2.Combinediﬀerentmodels.Wecanlearnmanymodelswithdiﬀerentforms,ortrainthemodelondiﬀerentsubsetsofthetrainingdata(i.e.bagging),andaveragepre-dictionsfromthesemodels.38Chapter5.OverﬁttingandRegularization3.Limitthemodelcapacity(i.e.representationalpower)toenoughtoﬁtthetrueregularitiesandnotenoughtoﬁtthespuriousregularities(samplingerrors).Wecancontrolthecapacityofaneuralnetworksinmanyways:1.Earlystopping:Startwithsmallweightsandstoplearningbeforeitoverﬁts.2.Architecture:Limitthenumberofhiddenlayersorthenumberofunitsperlayer.Besides,weightsinthenetworkcanbesharedasinconvolutionalneuralnetwork3.Weight-decay:Penalizelargeweightsusingpenaltiesofconstraintsontheirsquaredvalues(L2penalty)orabsolutevalues(L1penalty).4.Noise:Addnoisetotheinput,weights,orthenodeactivities.5.DropOutandDropConnect:Randomlyselectedsubsetsofactivationsorweightsaresettozerowithineachlayer.5.1.4FromtheOccam’sRazorPointofViewOccam’s(orOckham’s)razorisaprincipleattributedtothe14thcenturylogicianandFranciscanfriarWilliamofOckham[20].OckhamwasthevillageintheEnglishcountyofSurreywherehewasborn.Theprinciplestatesthat“Entitiesshouldnotbemultipliedunnecessarily”.ManyscientistshaveadoptedorreinventedOccam’sRazor,asinLeibniz’s”identityofobservables”andIsaacNewtonstatedtherule:“Wearetoadmitnomorecausesofnaturalthingsthansuchasarebothtrueandsuﬃcienttoexplaintheirappear-ances”.StephenHawkingwritesinABriefHistoryofTime:“Wecouldstillimaginethatthereisasetoflawsthatdetermineseventscompletelyforsomesupernaturalbeing,whocouldobservethepresentstateoftheuniversewithoutdisturbingit.However,suchmodelsoftheuniversearenotofmuchinteresttousmortals.ItseemsbettertoemploytheprincipleknownasOccam’srazorandcutoutallthefeaturesofthetheorythatcannotbeobserved”.Themostusefulstatementoftheprincipleforscientistsis“whenyouhavetwocom-petingtheoriesthatmakeexactlythesamepredictions,thesimpleroneisthebetter”,orastrongerformwhichisrelevanttomachinelearningﬁelds:“Ifyouhavetwotheoriesthatbothexplaintheobservedfacts,thenyoushouldusethesimplestuntilmoreevidencecomesalong”.Inmachinelearning,Occam’srazorcanbeviewedasaninductivebiasduringthelearningprocess.Thistheoryexplainswhywepreferasimplerelativelyﬁttedmodelthanacomplexwell-ﬁttedmodeloverthetrainingdata.5.1.5FromtheBayesianpointofviewFromtheBayesianpointofview,manyregularizationtechniquescorrespondtoimpos-ingcertainpriordistributionsonmodelparameters.Forexample,theL2regularizationassumesthatpriordistributionsofweightsinneuralnetworkarezero-meanGaussian.5.2.RegularizationOverview39Figure5.4:PosteriordistributionTheBayesianframeworkassumesthatwealwayshaveapriordistributionforeverything,butthispriormaybeveryvague.Whenweobservesomedata,wecombineourpriordistributionwithalikelihoodtermtogetaposteriordistribution.Thelikelihoodtermtakesintoaccounthowprobabletheobserveddatatobepredictedbythemodel.Duringthelearningprocess,thelikelihoodtermwillﬁghtsagainstthepriorandwithenoughdata,thelikelihoodtermsalwayswins.However,ifwedonothaveenoughdata,priordistributionwillkeepthesolutionreasonable.Aneasyexampleisabouttossingcoinwhenwetrytopredicttheprobabilitypofproducingheadofacoin.Supposeweobserve100tossesandthereare53heads,sothemaximumlikelihoodanswer(thevalueofpthatmakestheobservationof53headsand47tails)willbep=0.53.However,whatifweonlytossedthecoinoneandwegot1head?Clearly,p=1isnotagoodanswerbecausewedonothaveenoughinformation.Abettersolutionforthisproblemistoimplyapriordistributionforthecoin,suchasuniformor0.5-meanGaussiandistribution.Wethencombinetheprobabilityofobservingaheadwiththatpriordistributiontogettheposteriordistribution.Therefore,choosingtherightpriorinformationcouldhelpthemaximizeposteriormethodovercometheoverﬁttinginmaximizelikelihoodmethodwhenlackingoftrainingdata.5.2RegularizationOverviewRegularization,inmathematicsandstatisticsandparticularlyintheﬁeldsofmachinelearning,referstoaprocessofintroducingpriorinformationinordertosolveanill-posedproblemortopreventoverﬁtting.Inmachinelearning,regularizationtechniquesareusuallyusedtoconstraintheweightsofnetworktobesmall(L2method),sparse(L1method),orsharedoverdiﬀerentparts(Convolutionalneuralnetwork).Besides,wecanalsoaddnoisetoweightornodeactivities.NewpublishedmethodssuchasDropoutorDropconnectcouldalsodothejobparticularlysuccessfullyinmanycases.5.2.1Least-SquaredMethodasRegularizationWhenusingtheLeast-Squaredcostfunctiontomaximizethelikelihoodbetweenmodel’spredictionsandthetargetvalues,weareactuallydoingasimpleregularizationwiththepriorinformationisthatthetargetsolutionisgeneratedbyaddingtheGaussiannoisetotheoutputoftheneuralnetwork([27,15]).Supposethatwehaveyc=f(inputc,W)istheoutputofthenetandtcisthetargetvalue.Therefore,theprobabilitydensityofthetargetvalueisgivenbythenetwork’s40Chapter5.OverﬁttingandRegularizationoutputplusGaussiannoise:p(tc|yc)=1√2πσ2e−(tc−yc)22σ2p(t|y)=Ycexamples1√2πσ2e−(tc−yc)22σ2lnp(t|y)=Xcln1√2πσ2−(tc−yc)22σ2Therefore,minimizingthesquarederroristhesameasmaximizingthelogprobabilityunderaGaussiannoise.WML=argmaxWXcln1√2πσ2−(tc−yc)22σ2=argminWXc(tc−yc)22σ25.2.2L2regularizationL2regularizationisastandardweightpenaltymethod,whichiswidelyusedinleast-squaredcostfunction.Itaddsanextratermtothecostfunctionthatpenalizesthesquaredweights.However,thereisalsoasimilarregularizationtermwhichcanbeusedwithcross-entropycostfunction(incasetheoutputunitislogisticorsoftmaxnode).C=E+λ2Xiw2iC:FinalcostfunctionE:Squarederror-Likelihoodtermλ2Xiw2i:L2regularizationtermTheL2regularizationattempttokeeptheweightssmallunlesstheyhavebigerrorderivatives.Thismethodpreventsthenetworkfromusingweightsthatitdoesnotneedwhichcanimprovegeneralizationandmakeasmoothermodelbecauseithelpstostopthenetworkfromﬁttingthesamplingerror.Forexample,ifthenetworkhastwoverysimilarinputs,itpreferstoputhalftheweightoneachratherthanalltheweightonone.FromtheBayesianpointofview,thisregularizationmethodisequivalenttoassumingazero-meanGaussianpriorforthenetwork’sweights[15].lnp(W|D)=lnp(D|W)+lnp(W)−lnp(D)=⇒WML=argminW12σ2DXc(tc−yc)2+12σ2WXiw2iW:NetworkweightsD:Trainingdatalnp(D):IndependentfromW5.2.RegularizationOverview41TheﬁrstterminaboveequationcomefromtheassumptionthatthemodelmakesaGaussianprediction.Andthesecondtermassumesazero-meanGaussianpriorfortheweights.5.2.3L1regularizationSometimes,itworksbettertopenalizetheabsolutevaluesoftheweightsinsteadofthesquared.ThisiscalledL1regularization.C=E+λ2Xi|wi|(5.1)Thismethodiswidelyusedinsparsemodeling,apopularandeﬀectivemodelusinginimageprocessing.Itpushesmanyweightsinnetworktobecomeexactlyequaltozero,whichyieldssparsemodels-easiertointerpret.Besides,theL1alsooutperformtheL2penaltywhenirrelevantfeaturesarepresentedintrainingdata,becauseitcanlearntocompletelyignorethem.However,thisL1regularizationtermmakesthecostfunctioninequationabovenon-diﬀerentiable.Thus,wecannotusethestandardoptimizationmethodlikegradientdescenttoﬁndtheglobalminimuminthesamewaythatisdoneinL2penalty.Besides,L2andL1penalty,sometimes,wecanusediﬀerentweightpenaltythatallowslargeweightsbutpushessmallweightstobecomezero.5.2.4WeightconstraintsInsteadofpenalizethesquaredweightsseparately;wecanputaconstraintonthemaxi-mumsquaredlengthoftheincomingweightvectorofeachunit.Ifanupdateviolatesthisconstraint,wescaledownthevectorofincomingweightstoallowedlength.Thismethodhasseveraladvantagesoverweightpenalties:Itiseasiertosetasensiblethresholdandcanpreventhiddenunitsgettingstucknearzeroaswellasweightsexploding[15].Thisismoreeﬀectivethanaﬁxedpenaltyatpushingirrelevantweightstowardszero.Besides,usingaconstraintratherthanapenaltypreventsweightsfromgrowingverylargenomat-terhowlargetheproposedweight-updateis.Thismakesitpossibletostartwithaverylargelearningratewhichdecaysduringlearning,thusallowingafarmorethoroughsearchoftheweight-spacethanmethodsthatstartwithsmallweightsanduseasmalllearningrate[19].Thismethodhasbeenusedtogetherwithdropoutin[19]andgaveaverygoodresultsonmanydiﬀerentapplications(seeDropoutsection).5.2.5AddingnoiseInfact,addingGaussiannoisetotheinputsisequivalenttousingL2regularization.WehavethevarianceofthenoiseisampliﬁedbythesquaredweightbeforegoingintothenextlayerasshowedinFigure5.5.Thismakesanadditivecontributiontothesquarederror,sominimizingthesquarederrortendstominimizethesquaredweightswhentheinputsarenoisy.However,addingGaussiannoisetotheweightsofamultilayernon-linearneuralnet-workisnotexactlyequivalenttousinganL2penaltyandcouldbebetterespeciallyin42Chapter5.OverﬁttingandRegularizationFigure5.5:AddingGaussiannoisetoinputs,takenfrom[15]recurrentnetworks.AlexGravesshowedthatrecurrentnetworkssigniﬁcantlybetterifnoiseisaddedtotheweights[12].Anotherwayistoaddnoisetothenodeactivities[15].Supposethatwehaveamultilayerneuralnetworkcomposedoflogisticunitsandtrainedbybackpropagation.Wecanmakethelogisticunitsbinaryandstochastic(sampledfromitsactivities)ontheforwardpass,butdothebackwardpassasinoriginalbackpropagationmethod.Thiswayproducetheworseresultontrainingset,andrequireconsiderablymoretimetotrain,butitdoessigniﬁcantlybetterontestset.5.2.6ConvolutionalneuralnetworkWhenapplyingfully-connectedmultilayerneuralnetworkonlearningcomplexhigh-dimensionalnon-linearmappingsuchasimagerecognitionorspeechrecognition,wetraditionallyhavetousehand-designedfeatureextractortogathersrelevantinformationfromtheinputandeliminatesirrelevantvariabilities.Feeding“raw”inputsdirectlyintothenetworkandletitlearnfeatureextractorautomaticallyismoreinterestingbutcausesmanyproblems[24].Firstly,becausetypicalimagesorspokenwordscontainatleastseveralhundredvariables.Therefore,aﬁrstfully-connectedlayerwith,sayafew100units,wouldalreadycontainseveral10,000weights,whichleadstooverﬁttingifthetrainingdataisscarce.Secondly,unstructurednetshavenobuilt-ininvariancewithrespecttotranslations,orlocaldistor-tionsoftheinputs(e.g.size,slant,orpositionvariations).Inprinciple,afully-connectednetworkofsuﬃcientsizecouldlearntoproduceoutputsthatareinvariantwithrespecttosuchvariations.However,learningsuchataskwouldprobablyresultinmultipleunitswithidenticalweightpatternspositionedatvariouslocationsintheinput.Besides,learningtheseweightconﬁgurationsrequiresaverylargenumberoftraininginstancestocoverthespaceofpossiblevariations[24].Thirdly,afully-connectedarchitecturesentirelyignorethetopologyoftheinput(i.e.theinputvariablescanbepresentedinanyﬁxedorderwithoutaﬀectingtheoutcomeofthetraining).TheCNNsovercometheseaboveproblemsbydesigninganetworkarchitecturethatcontainsacertainamountofapriorknowledgeabouttheproblem[6].CNNscombinethreearchitecturalideastoensuresomedegreeofshiftanddistortioninvariance:localreceptiveﬁelds,sharedweights(orweightreplication)andspatialortemporalsubsampling[24],whichgivethesefollowingadvantages:5.2.RegularizationOverview431.Localreceptiveﬁelds:neuronscanextractelementaryvisualfeaturessuchasori-entededges,end-points,corners.ThiscomesfromtheHubelandWiesel’sdiscoveryoflocally-sensitive,orientation-selectiveneuronsinthecat’svisualsystem.2.Sharedweights:theelementaryfeaturedetectorsthatareusefulononepartoftheimagearelikelytobeusefulacrosstheentireimage.Thisknowledgecanbeappliedbyforcingasetofunits,whosereceptiveﬁeldsarelocatedatdiﬀerentplacesontheimage,tohaveidenticalweightvectors.Theoutputsofsuchasetofneuronsconstituteafeaturemap.Inorderwords,unitsinafeaturemapareconstrainedtoperformthesameoperationondiﬀerentpartsoftheimage.Aconvolutionallayerisusuallycomposedofseveralfeaturemaps(withdiﬀerentweightvectors),sothatmultiplefeaturescanbeextractedateachlocation.3.Subsampling:onceafeaturehasbeendetected,itsexactlocationbecomelessim-portant,aslongasitsapproximatepositionrelativetootherfeaturesispreserved.Therefore,eachconvolutionallayerisfollowedbyanadditionallayerthatperformsalocalaveraging,andasubsampling,reducingtheresolutionofthefeaturemap,andreducingthesensitivityoftheoutputtoshiftsanddistortions.Successivelayersofconvolutionsandsubsamplingaretypicallyalternated,resultingina“bi-pyramid”:ateachlayer,thenumberoffeaturemapsisincreasedasthespatialresolutionisdecreased.Besides,theweightsharingtechniquehastheinterestingsideeﬀectofreducingthenumberoffreeparameters,therebyreducingthe“capacity”ofthemachineandimprovingitsgeneralizationability.AverysuccessfulCNNarchitectureusedinMNISTproblemhasbeenintroducedbyY.LeCunetal.1990,whichgivesoneofthebestperformancessofar(0.4%errorrate)ifcombinedwithelasticdeformations,andearly-stopping(1.0%errorrateifusingCNNalone)(Figure5.6).However,CNNarchitectureisusuallydesignedinaheuristicway.Recently,manynewmethodshavebeenintroducedtoeﬀectivelytraindeepneuralnet-works,suchasusinggenerativepre-training,Hessian-freeoptimization,orevenastandardgradientdescentwithwell-designedinitializationandmomentum.Therefore,wenowcantrainafully-connecteddeepneuralnetworkswithmillionsofparameterstoextractuse-fulfeaturesfromthetrainingimagesautomatically.ADNNhasbeenusedforMNISTproblemwithrawinputandprovided1.25%errorrates[16].Recently,E.Hinton,AlexandIlyaSutskever2012[23]haveprovidedanamazingresultonusingDeepCNNonclassifyingimage.Theytrainedalarge,deepCNNtoclassifythe1.2millionhigh-resolutionimagesintheImageNetLSVRC-2010contestintothe1000diﬀerentclasses.Onthetestdata,theyachievedtop-1andtop-5errorratesof37.5%and17.0%whichisconsiderablybetterthanthepreviousstate-of-the-art.Theyalsoused“dropout”method(describedbelow)toreduceoverﬁttingandthatwasprovedtobeveryeﬀective.5.2.7DropoutHintonetal.2012haveproposedaveryeﬀectiveregularizationmethodcalledDropout.Itcanreduceoverﬁttingbypreventingcomplexco-adaptions(afeaturedetectorisonly44Chapter5.OverﬁttingandRegularizationFigure5.6:CNNarchitectureusedinMNISTproblem[24]helpfulinthecontextofseveralotherspeciﬁcfeaturedetectors)onthetrainingdata[19].Oneachpresentationofeachtrainingcase,eachhiddenunitisrandomlyomittedfromthenetworkwithacertainprobability,say0.5,soahiddenunitcannotrelyonotherhiddenunitsbeingpresent.Attesttime,theyusethe“meannetwork”thatcontainsallofthehiddenunitsbutwiththeiroutgoingweightshalvedtocompensateforthefactthattwiceasmanyofthemareactive.Anotherwaytoviewthedropoutprocedureisasaveryeﬃcientwayofperformingmodelaveragingwithneuralnetworks.Wecouldthinkofitastrainingdiﬀerentnetworkforeachpresentationofeachtrainingcasebutallofthesenetworkssharethesameweightsforthehiddenunitsthatarepresent.Usingtogetherwithweightconstraint,Hintonetal.2012havetestedthismethodonmanydiﬀerentdataset.OnMNISTdataset,thebestpublishedresultforastandardfeedforwardneuralnetwork(withoutaddingpriorknowledge,preprocessingorgenerativepre-training)is1.6%,whichcouldbereducedto1.3%using50%dropoutandweightconstraints,andto1.1%byalsodroppingoutarandom20%ofthepixels.OnTIMIT,awidelyusedbenchmarkforrecognitionofcleanspeechwithasmallvocabulary,using50%dropoutwithdeep,pre-trained,feedforwardneuralnetworksreducestherecognitionerrorratefrom22.7%to19.7%.Thatisarecordformethodsthatdonotuseanyinformationaboutspeakeridentity.Lately,E.Hinton,AlexandIlyaSutskever2012[23]haveusedadeepconvolutionalneuralnetworkswithdropoutmethodtoproducethebestresultonImagenetdataset(classifyhigh-resolutionimagesinto1000diﬀerentclasses).5.2.8DropconnectLately,agenerationofDropoutcalledDropconnecthasbeenintroducedbyLiWanetal.,2013[50].Thismethodsetsarandomlyselectedsubsetofweightswithinthenetworktozero.Eachunitthusreceivesinputfromarandomsubsetofunitsinpreviouslayer,insteadofdroppingoutrandomunitsasindropoutmethod.InMNISTdataset,thismethodgivesaslightlybetterresultinsomecases.However,itconvergesmoreslowly5.2.RegularizationOverview45thanDropout.ThesamethinghappenstootherdatasetsuchasCIFAR-10,SVHNandNORB.Ingeneral,theDropconnectmethodcanslightlyoutperformtheDropoutmethod,butitneedsmoretimetoconverge.Chapter6ActivationFunctionsWhatmakesthedeepneuralnetworksbecomeverypowerfulanduniversalmodelistheactivationfunction.Adeepneuralnetworkcontainmultiplelayersoflineartransformationcanberepresentedbyasimpleone-layerneuralnetwork.Thenonlinearactivationfunctioniswhatgivesneuralnetworkstheirnonlinearcapabilities[25].Theactivationfunctionisgenerallychosentobemonotonic.Therearemanydiﬀerenttypesofactivationfunctionsthathavebeenproposed.Sigmoidisoneofthemostcommonform,whichisamono-tonicallyincreasingfunctionthatasymptotesatsomeﬁnitevalueas±∞isapproached.Inthischapter,wewillpresentthemotivationbehindthesigmoidfunction,someofitsvariants,andintroducesomenewactivationformscomingfromrecentresearches.6.1LogisticSigmoidfunctionThelogisticsigmoidfunctionisgivenbyf(x)=11+exp(−x)(Figure6.1).Oneimportantmotivationforthisformoffunctionistheoutputofthelogisticsigmoidfunctioncanbeinterpretedasposteriorprobabilities[27].Forexample,weconsiderbuildingadiscriminantfunctionforatwo-classproblemusinglogisticregression,whichhasform:y=f(WTx+b)(6.1)wewanttopredictthetwo-classlabelyfromtheinputx,giventhattheclass-conditionaldensitiesaregivenbyGaussiandistributionswithequalcovariancematricesΣ1=Σ2=Σ,sothat:P(x|Ck)=1(2π)d/2|Σ|1/2exp{−12(x−µk)TΣ−1(x−µk)}(6.2)UsingBayes’theorem,theposteriorprobabilityofmembershipofclassC1isgivenby:P(C1|x)=p(x|C1)P(C1)p(x|C1)P(C1)+p(x|C2)P(C2)(6.3)Leta=lnp(x|C1)P(C1)p(x|C2)P(C2)(6.4)wewillseethattheposteriorprobabilitycanbeexpressedasthelogisticsigmoidfunctionofa:P(C1|x)=11+exp(−a)(6.5)4748Chapter6.ActivationFunctionsFigure6.1:Thesigmoid,tanh,andscaledtanhfunctionsIfnowwesubstitutetheexpressionsfortheclass-conditionaldensitiesfrom6.2into6.4,weobtain:a=WTx+b(6.6)whereW=Σ−1(µ1−µ2)b=−12µT1Σ−1µ1+12µT2Σ−1µ2+lnP(C1)P(C2)Therefore,wecanseethatthelogisticsigmoidactivationfunctionallowstheoutputsofthediscriminantfunctionin6.1tobeinterpretedasposteriorprobabilities.6.2HyperbolictangentanditsscaledversionTheHyperbolictangentortanhfunctionisarescalingofthelogisticsigmoid,suchthatitsoutputsrangefrom−1to1insteadof0to1asinlogisticsigmoid.Lets(x)isthelogisticsigmoidfunction,wecanrepresentthetanh(x)functionasalineartransformedversionofs(x):tanh(x)=ex−e−xex+e−x=2s(2x)−1(6.7)Inneuralnetwork,thetanhfunctionismorepopularbecauseofitssymmetryabouttheorigin(i.e.zero-mean).Inotherwords,thetanharemorelikelytoproduceoutputs(whichareinputstothenextlayer)thatareonaverageclosetozero.Moreover,thelogisticfunctionhasbeenshowntoslowdownthelearningprocessbecauseofitsnone-zeromeanthatinducesimportantsingularvaluesintheHessian[25].6.3.Softsignfunction49Figure6.2:Takenfrom[4],tanhversusthesoftsign,whichconvergespolynomiallyinsteadofexponentiallytowardsitsasymptotesGlorotandBengio2010[9]didadeepinvestigationontheeﬀectofusingactivationfunctionsondeepneuralnetwork.Theyshowedthatwhenusingsigmoidactivationon5-layersnetwork,thelasthiddenlayerquicklysaturatesat0(slowingdownalllearning),butthenslowlydesaturatesaroundepoch100.Thehyperbolictangentnetworksdonotsuﬀerfromthatkindofsaturationbehavior.However,withrandomweightinitialization,thesaturationphenomenonoccursequentiallystartingwithﬁrstlayerandpropagatingupinthenetwork.Lecunetal.1998alsorecommendedascaledversionoftanh:f(x)=1.7159tanh(23x).Theconstantsinthisfunctionarechosensothat,whenusedwithnormalizedinputs(e.g.zeromean,uncorrelated,andunitcovariance),thevarianceoftheoutputswillalsobecloseto1.Inparticular,thisfunctionhasthepropertiesthat(a)f(±1)=±1,(b)thesecondderivativeisamaximumatx=1,and(c)theeﬀectivegainiscloseto1.6.3SoftsignfunctionBergstraetal.2009[4]hasproposedanewactivationfunctioncalledsoftsign:f(x)=x1+|x|.Thesoftsignissimilartothehyperbolictangent(itsrangeis−1to1)butitstailsarequadraticpolynomialsratherthanexponentials,i.e.,itapproachesitsasymptotesmuchslower[9].InGlorotandBengio2010experiment,theyshowedthatsaturationdoesnotoccuronelayeroftheotherindeepsoftsignnetworkslikeforthehyperbolictangentnetworks.Itisfasteratthebeginningandthenslow,andalllayersmovetogethertowardslargerweights[9].Withoutpre-training,thesoftsignnetworksperformbetterthanthetanhorlogisticsigmoidnetworksondiﬀerentdatasetssuchasMNIST,Shapeset,CIFAR10,...6.4RectiﬁerandSoftplusfunctionManydiﬀerencesexistbetweenneuralnetworkmodelsusedbymachinelearningre-searchersandthoseusedbycomputationalneuroscientists.Glorotetal.2011[10]wanted50Chapter6.ActivationFunctionstobridge(inpart)amachinelearning/neurosciencegapintermsofactivationfunctionandsparsity.Theyhavesuccessfullyappliedtherectiﬁerfunctionsuggestedinneuro-scienceintomachinelearningneuralnetworkmodels,whichcouldperformbetterthantanhorsigmoidnetworksinsomeparticularsettings.Therearetwomainneuroscienceobservationsthatinspiredtheirworks:•Studiesonbrainenergyexpensesuggestthatneuronsencodeinformationinasparseanddistributedway(AttwellandLaughlin,2001),estimatingthepercentageofneu-ronsactiveatthesametimetobebetween1and4%.However,withoutadditionalregularization,suchasanL1penalty,ordinaryfeedforwardneuralnetsdonothavethisproperty.•Acommonbiologicalmodelofneuron,theleakyintegrate-and-ﬁre(orLIF)(DayanandAbott,2001)isverydiﬀerentfromthelogisticsigmoidortanhfunctionusedinmachinelearning.Moreover,theywerealsoinspiredparticularlybythesparserepresentationslearnedbysparseauto-encoder.However,theyarguedthatwhenusingthesparsitypenaltytoinducethesparserepresentations,theneuronsenduptakingsmallbutnon-zeroactivation.Theywanttobuildtrulysparserepresentations,whichgivesrisetorealzerosofactivations.Combiningalloftheseideas,theyendedupusingtherectiﬁerneurons,whichintro-ducedintheneuroscienceliteraturebyBushandSenowski1995:f(x)=max(0,x).Thisactivationfunctionshowsthefollowingadvantages:•Therectiﬁeractivationfunctionallowsanetworktoobtainsparserepresentationseasily.Forexample,afteruniforminitializationoftheweights,around50%ofhiddenunitscontinuousoutputvaluesarerealzeros,andthisfractioncanbeeasilyincreasewithsparsity-inducingregularization.•Theonlynon-linearlityinthenetworkcomesfromthepathselectionassociatedwithindividualneuronsbeingactiveornot(illustratedinFigure6.3).Foragiveninput,onlyasubsetofneuronsisactive.Oncethissubsetofneuronsisselected,theoutputisalinearfunctionoftheinput.Wecanseethemodelasanexponentialnumberoflinearmodelsthatshareparameters.Becauseofthislinearity,gradientsﬂowwellonactivepathsofneurons(thereisnogradientvanishingeﬀectduetoactivationnon-linearitiesofsigmoidortanhunits).However,thereisonepotentialproblem,thatthehardsaturationat0mayhurtoptimizationbyblockinggradientback-propagation.Theauthorshaveevaluatedthisbyinvestigatingthesoft-plusactivation:f(x)=log(1+ex),assmoothversionoftherectiﬁer.However,experimentalresultssuggestedthathardzeroscouldactuallyhelpsupervisedtraining.Anotherproblemcouldariseduetotheunboundedbehavioroftheactivations.Therefore,theauthorsusedtheL1weightdecayontheactivationvalues,whichalsopromotesadditionalsparsity.Experimentalresultsin[10]showedthatrectiﬁerperformsbetterthanothertraditionalactivationfunctionsonsupervisedtraining(1.43%onMNISTcomparedto1.57%oftanhnetwork).However,withunsupervisedpre-training,thediﬀerenceisnotsigniﬁcant.Glorotetal.2011[10]alsopresentedashortsummaryofsparserepresentationadvantages,whichisworthmentioninghere.6.4.RectiﬁerandSoftplusfunction51Figure6.3:Takenfrom[10],Sparsepropagationofactivationsandgradientsinnetworkofrectiﬁerunits.TheinputselectsasubsetofactiveneuronsandcomputationislinearinthissubsetFigure6.4:Takenfrom[10],Rectiﬁerandsoftplusactivationfunctions.Thesecondoneisasmoothversionoftheﬁrst52Chapter6.ActivationFunctions•Informationdisentangling:Adenserepresentationishighlyentangledbecauseal-mostanychangeintheinputmodiﬁesmostoftheentriesintherepresentationvector.Instead,ifarepresentationisbothsparseandrobusttosmallinputchanges,thesetofnon-zerofeaturesisalmostalwaysroughlyconservedbysmallchangesoftheinput.•Eﬃcientvariable-sizerepresentation:Diﬀerentinputsmaycontaindiﬀerentamountsofinformationandwouldbemoreconvenientlyrepresentedusingavariable-sizedata-structure,whichiscommonincomputerrepresentationsofinformation.Vary-ingthenumberofactiveneuronsallowsamodeltocontroltheeﬀectivedimension-alityoftherepresentationforagiveninputandtherequiredprecision.•Linearseparability:Sparserepresentationsaremorelikelytobelinearlyseparablesimplybecausetheinformationisrepresentedinahigh-dimensionalspace.6.5MaxoutFunctionGoodfellowetal.[11]proposedmaxoutactivationfunction,whichyieldsstateoftheartperformanceonMNIST,CIFAR10andCIFAR100datasetswhenappliedonconvolutionalneuralnetworkswithdropout(0.45%,11.58%,and38.57%respectively).Thisisaverysimplemodel,whichdesignedtobothfacilitatesoptimizationbydropoutandimprovestheaccuracyofdropout’sfastapproximatemodelaveragingtechnique.Asintroducedinchapter5aboutregularizationtechniques,Dropout(Hintonetal.,2012)providesaninexpensiveandsimplemeansofbothtrainingalargeensembleofmodelsthatshareparametersandapproximatelyaveragingtogetherthesemodels’predictions.Whiledropoutisknowntoworkwellinpractice,ithasnotpreviouslybeendemonstratedtoactuallyperformmodelaveragingfordeeparchitectures[11].Therefore,theauthorstriedtodesignamodelthatenhancesdropout’sabilitiesasamodelaveragingtechnique.Themaxoutnetworkissimplyafeed-forwardneuralnetworkordeepconvolutionalneuralnetwork,whichusesanewtypeofactivationfunction:themaxoutunit.Givenaninputx∈Rd(xmaybeatraininginputorahiddenlayer’sstate),amaxouthiddenlayerimplementsthefunction:hi(x)=maxj∈[1,k]zij(6.8)wherezij=xTW...ij+bijandW∈Rd×m×kNotethatinmaxoutnetwork,eachlayercontainskdiﬀerentweightmatrixinsteadofonlyoneweightmatrixinfeedforwardnetwork.Thisideaﬁtswellinconvolutionalnetworkwherewehavemultiplefeaturemapsoneachlayer.Inaconvolutionalnetwork,amax-outfeaturemapcanbeconstructedbytakingthemaximumacrosskaﬃnefeaturemaps(i.e.,poolacrosschannels,inadditionspatiallocations).Whentrainingwithdropout,weperformtheelementwisemultiplicationwiththedropoutmaskimmediatelypriortothemultiplicationbytheweightsinallcases-wedonotdropinputstothemaxoperator.Asinglemaxoutunitcanbeinterpretedasmakingapiecewiselinearapproximationtoanarbitraryconvexfunction.Therefore,maxoutnetworkslearnnotjusttherelationshipbetweenhiddenunits,butalsotheactivationfunctionofeachhiddenunit.Themaxoutabandonsmanyofthemainstaysoftraditionalactivationfunctiondesign.Therepresenta-tionitproducesisnotsparseatall.Moreover,maxoutislocallylinearalmosteverywhere,6.5.MaxoutFunction53Figure6.5:Takenfrom[11],Graphicaldepictionofhowthemaxoutactivationfunctioncanimplementtherectiﬁedlinear,absolutevaluerectiﬁer,andapproximatethequadraticactivationfunction.Thisdiagramis2Dandonlyshowshowmaxoutbehaveswitha1Dinput,butinmultipledimensionsamaxoutunitcanapproximatearbitraryconvexfunctions.whilemanypopularactivationfunctionshavesigniﬁcantcurvature.However,itisveryrobust,easytotrainwithdropout,andachievesexcellentperformance.Chapter7IntroductiontoADATEInthischapter,weonlyintroducedtheADATEsystemverybrieﬂy,whichcouldhelpyouunderstandthemainideasbehindit.Formoredetailsabouthowitsinternalalgorithmswork,pleasereferto[33,34].Besides,writingspeciﬁcationﬁleforADATEisalsoanart,whichneedsbothknowledgeandexperience.Thepaper[32]couldbeaveryusefulsourceforanyonewantingtowriteagoodspeciﬁcation.AcompleteUserManualisalsoavailable,whichprovidesmanypracticalinformationabouthowtouseADATE[47].7.1AShortIntroductiontoADATEADATE[35]hasbeendevelopedbyProf.RolandOlsson,toautomaticallygeneratepurelyfunctionalprograms.Forexample,ithasbeenemployedtoimprovestate-of-the-artSATsolvers[36].Toevolveasolutiontoaproblem,thesystemneedsaspeciﬁcationﬁlethatdeﬁnesdatatypesandauxiliaryfunctions,anumberoftrainingandvalidationinputexamples,andanevaluationfunctionthatisusedtogradeandselectpotentialsolutionsduringevolution.Additionally,thespeciﬁcationﬁlemaycontainaninitialprogramfromwhichevolutionwillstart.Ofcourse,itispossibletostarttheevolutionfromanygivenprogram,forexampletosearchforimprovementsforthebestknownprogramforagivenproblem.Theprogramsareconstructedusingalimitednumberofso-calledatomicprogramtransformation.Themostimportantonesareasfollows.•R(Replacement)-Apartofanexistingprogramisreplacedbyanewlysynthesizedexpression.Duetotheextremelyhighnumberofexpressionsthatcanbesynthesized,Rtransformationsarecombinatoriallyexpensive.•REQ(ReplacementpreservingEquality)-AnRtransformationthatdoesnotmaketheprogramworseaccordingtothegivenevaluationfunction.REQtransformationsarequiteusefulduetotheirabilitytoexploreplateausinthesearchlandscape.•ABSTR(Abstraction)-LikeREQtransformations,theseneutraltransformationsexisttoaidthesysteminexploringplateaus.IncontrasttothegeneralREQtransformation,ABSTRtransformationshavetheveryspeciﬁctaskofintroducingnewfunctionsintheprogrambyfactoringoutapieceofcodeandreplacingitwithafunctioncall.Thisgivesthesystemtheimportantabilityofinventingneededhelpfunctionsontheﬂy,somethingwhichhasproventobeanextremelyusefulfeature.5556Chapter7.IntroductiontoADATEAtomicprogramtransformationsarecomposedtocompoundprogramtransformationsusinganumberofdiﬀerentheuristicstoavoidcommoncasesofinﬁniterecursion,un-necessarytransformationsetc.Forexample,afteranABSTRtransformation,thenewlyintroducedfunctionshouldbeusedinsomewaybyafollowingRoraREQtransforma-tion.Moredetailsontheatomicprogramtransformationsandtheheuristicsemployedtocombinethemcanbefoundin[35].Eachtimeanewprogramiscreatedbyacompoundtransformation,itisconsideredforinsertionintotheso-calledkingdom[49].Asinallevolutionarysystems,individualswithgoodevaluationvaluesarepreferred,butinADATE,thesyntacticcomplexitiesoftheindividualsalsoplayanimportantrole.AccordingtowhatiscommonlyknownasOccam’sRazor,simpletheoriesshouldusuallybepreferredovermorecomplexones,asthesimplertheoriestendtobemoregeneral.ThisprincipleisutilizedbyADATEtoreducetheamountofoverﬁtting,inthatsmallprogramsarepreferred,andifalargeprogramistobeallowedinthekingdom,ithastobebetterthanallprogramssmallerthanit[35].Inotherwords,anewprogramwillonlybeallowedtobeinsertedintothekingdomifallotherprogramsinthekingdomareeitherlargerorworsethanit.Eachtimeaprogramisaddedtothekingdom,allprogramsinthekingdombothlargerandworsethanthanthenewoneareremoved.HavingdescribedthemostimportantcomponentsoftheADATEsystem,weconcludethissectionbygivingabriefoverviewoftheoverallevolutionoccuringinarunofthesystem.1.Initiatethekingdomwiththesingleprogramgivenasthestartprograminthespeciﬁcationﬁle(eitheranemptyprogramorsomeprogramthatistobeoptimized).Inadditiontotheactualprograms,thesystemalsomaintainsanintegervalueCPforeachprogramP,calledthecostlimitoftheprogram.Fornewprogramsthisvalueissettotheinitialvalue1000.2.SelecttheprogramPwiththelowestCPvaluefromthekingdom.3.ApplyCPcompoundprogramtransformationstotheselectedprogram,yieldingCPnewprograms.4.Trytoinserteachofthecreatedprogramsintothekingdominaccordancewiththesize-evaluationorderingdescribedabove.5.DoublethevalueofCP,andrepeatfromstep2.Theaboveloopisrepeateduntiltheuserterminatesthesystem.TheADATEsys-temhasnobuilt-interminationcriteriaanditisuptotheusertomonitortheevolvingprogramsandhaltthesystemwheneverheconsiderstheevolvedresultsgoodenough.7.2AShortAnalysisofthePowerofADATESystemADATEisageneral-purposeautomaticprogrammingsystem,whichcanbeusedindiﬀer-entways.Besidesbeingwellknownforitsabilityofmeta-learningor“learnhowtolearn”,ADATEcanalsobeappliedasatraditionalmachinelearningmodelonclassiﬁcationorregressionproblems.Moreover,ithasshoweditssuperiorﬁttingpowerinmanycases,comparedtoothertraditionalapproaches.7.2.AShortAnalysisofthePowerofADATESystem57Inthissection,IwilldoashortanalysisofthepowerofADATEsystemindiﬀerentusecases,basedmainlyonthestructureofitssynthesizedprograms.Inotherwords,IwillbasicallydoablackboxanalysiswhereIignoretheADATE’sinternalalgorithmandonlyfocusonwhatkindofprogramsitwouldprobablysynthesizeindiﬀerentusecases.Formoreinformationabouthowitdoesthe“magic”,pleasereferto[33,34].7.2.1ADATEonClassiﬁcationProblemsClassiﬁcationistheproblemofpredictingthevalueofacategoricalvariablebasedonotherexplanatoryvariables.Analgorithmthatimplementsclassiﬁcationiscalledaclassiﬁer.Inthissection,Iwillfurthersplitthisproblemintwodiﬀerenttypes:whenexplanatoryvariablesarecontinuousorcategorical.BecausetheADATEtendstosynthesizeprogramsindiﬀerentstructurewhenappliedtodiﬀerenttype.ContinuousExplanatoryVariablesAnexampleforthistypeisanedgedetectionproblem,whichisdonebyKristinLarsen1ashermasterthesis.Inthisproblem,theclassiﬁerhastopredictifthemiddlepixelisonanedgeofa5x5pixelsimage.Theexplanatoryvariablesareintensityof24pixelssurroundingthemiddleone(T1toT24).Theintensityvaluerangeisanintegernumberin[0..255].Aftertrainedforaprolongedtime(morethantwoweeks),thebestADATE’ssynthesizedprogram(testedonseparatevalidationset)isshowedbelow2:1funf(T1,T2,T3,T4,T5,3T6,T7,T8,T9,T10,T11,T12,T13,T14,T15,5T16,T17,T18,T19,T20,T21,T22,T23,T24)=7letfungx=9realLess(x,11)in11casecaseg(T12)of13false=>g(15caseg(T8)offalse=>17caseg(T18)offalse=>T719|true=>realSubtract(21T18,caseg(T17)offalse=>T1323|true=>T4)25|true=>caseg(T1)of27false=>caseg(T10)of29false=>T81KristinisalsoamasterstudentatHiof.Herthesisisnotﬁnishedyet.2Thiscodehasbeenmodiﬁedfromtheoriginalonetomakeitclean58Chapter7.IntroductiontoADATE|true=>realSubtract(T9,T12)31|true=>T19)33|true=>trueoffalse=>g(T16)35|true=>trueendCode7.1:CleanversionofthebestsynthesizedprogramforedgedetectionproblemWecaneasilyrecognizethattheaboveprogramhasasimilarstructuretoadecisiontree.However,asexperimentedbyKristinLarsen,therandomforest-anensemblelearningmethodforclassiﬁcationbyconstructingamultitudeofdecisiontrees-wasoutperformedbythatprogram.Figure7.1illustratestheequivalentdecisiontreeofthatprogram.Howcouldthecode7.1outperformthedecisiontree,randomforest,orevendeepneural T12<11 T8<11 True False T1<11 T19<11 T16<11 T10<11 T8<11 T9-T12<11 True True True False T16<11 True True False T16<11 True T18<11 T17<11 T18-T4<11 T18-T13<11 True False T16<11 True True False T16<11 True T7<11 True False T16<11 True Figure7.1:TheequivalentdecisiontreeoftheADATEsolutionforedgedetectionproblem;LeftbranchesaretheTruecasesnetworkclassiﬁersasshownintheKristin’sexperiment?IhypothesizethatthatiscomefromthesefollowingabilitiesofADATE’ssynthesizedclassiﬁer:•Featureextraction:TheADATEcandiscoverfeatureextractorswhentheyareneeded.Inﬁgure7.1,youcanseethatADATEhasdiscoveredthreenewfeatures:T9−T12,T18−T4andT18−T13ashighlightedingreenboxes.Inthiscase,ADATEonlylearnedlinearfeatureextractors(i.e.linearﬁlters).However,itisca-pableoflearningmuchmorecomplexfeatureextractors.Inthenextexampleaboutfour-wayitem-to-itemnavigationalgorithm,youwillseethismoreclearly.Thisabil-ityisextremelyusefulwhentheexplanatoryvariablesarehighlycorrelated.Thereisawell-knownproblem(maybeconsideredasaparadox)instatisticsandmachinelearningliteraturethatusingallavailableexplanatoryvariablestobuildapredictivemodelcanhurtitsperformanceverybad.Thisproblemoccurswhensomeoftheseexplanatoryvariablesarenotindependentandhighlycorrelated.Onesimpleandfamousexampleofthisisthecollinearityprobleminmultivariatelinearregressionmodel,whentwoormoreexplanatoryvariableshavelinearrelationship.7.2.AShortAnalysisofthePowerofADATESystem59Wecanovercome(orminimize)thiscorrelationprobleminmanyways,suchaseliminatingredundantvariablesmanuallyorautomatically,normalizingtheinput,ordoingfeatureextractiontoextractusefulfeaturefromhigh-dimensionalandhighlycorrelatedinputs.Thelastsolutionhasbeenshowntobethebestone,especiallyincomputervisionwherewehaveveryhigh-dimensionalinputs(images)withhighlycorrelatedcomponents(pixels).Thisexplainswhydeeplearningandfeatureextrac-tionhavedominatedinmanycomputervisiontaskssuchasdigitsrecognitionsorobjectclassiﬁcation.Pleaserefertochapter3andchapterchap:autoencoderaboutdeeparchitecturesandautoencodersformoreinformationaboutthis.•DivideandConquer:TheADATEcansplittheinputspaceintomanysub-spaces,andbuilddiﬀerentfeatureextractorsindiﬀerentsub-spaces.Thisisaverypowerfulability,whichmayexplainwhyADATEcaneasilysynthesizeclassiﬁerthatﬁtverywelltomanydiﬀerentproblems.However,thisalsomakesADATE’ssynthesizedprogramseasytobeoverﬁtted.Todemonstratethispower,takeexampleofanclassiﬁcationproblemwheretheposteriordistributionofthetargetvariableycon-ditionedontheinputx:P(y|x)isdiﬀerentindiﬀerentsub-spacesofinput,wewillneedtoﬁtdiﬀerentmodeltoeachofthesesub-spacestomakethebestclassiﬁer.EveniftheP(y|x)isdistributedconsistentlyintheinputdomain,butifitisaverycomplexfunction,dividingtheinputdomainintomanysub-spacesandtryingtoapproximatethatcomplexfunctionineachsub-spacesusingmuchmoresimplefunctionsisalsoaverygoodstrategy.ThisisasimilarstrategyemployedbytheMaxoutnetworkwhereitusespiecewiselinearfunctionstoapproximatetheacti-vationfunctionforeachneuralnode3.Moreover,inclassiﬁcationproblemswheretherelationshipsbetweenexplanatoryvariablesarediﬀerentindiﬀerentinput’ssub-spaces,wealsoneedtohavediﬀerentfeatureextractorsindiﬀerentinputdomains.Generally,theADATEactuallybuildsanensembleofmanydiﬀerentmodelsindiﬀer-entinput’ssub-spaces.ThisdivideandconquerstrategynotonlybringstheﬁttingpowertotheADATEsystem,butalsointroducesoveﬁttingproblem.Splittingtheinputspacetoomuchwillincreasethechanceofdiscoveringaccidentalrelationships(betweenexplanatoryvariablesandtargetvariableaswellasamongexplanatoryvariables)ofADATEsystem,especiallywhenwehavelow-granularitytrainingdata(i.e.notenoughdata).Ofcourse,theOccam’srazoremployedinADATEsearchingstrategy,andagoodvalidationsetcouldbeveryhelpfulinthiscase.Nevertheless,empiricalresultsshowedthatingeneral,usingADATEtobuildaclassiﬁerforahighdimensionalitycontinuousinputdomainismorelikelytocreateanoverﬁttedsolution,comparedtootherusecasesofADATE.Youwillseeinthenextsectionthat,onregressionproblem,ADATEusuallydoesnotdividetheinputspaceinitssolutions,whichmakeithardertobeoverﬁtted.•GoodSearchingStrategy:EvolutionstrategyusedinADATE,asfarasIknow,isoneofthebestoptimizationtechniqueinmachinelearning,whichdoesnotsuﬀerfromthelocaloptimaandothersimilarproblemsingreedyoptimizationmethods3Pleaserefertochapteraboutactivationfunction60Chapter7.IntroductiontoADATEsuchasID3indecisiontreeorgradient-basedmethodsinneuralnetworks.Indeed,ID3algorithmisaverygreedyapproachwhereitchoosethebestnodeateachtime,whilethegradient-basedoptimizationmethodscanonlydiscovertheoptimathatarecloseenoughtotheinitialpoint,i.e.,itcanonlysearchforalocalparameterspace.Theevolutionstrategy,ontheotherhand,cancoversmuchlargerparameterspace.However,thisoptimizationtechniquerunsmuchslowerthantheothers,asatrade-oﬀforitsperformance.•CompactRepresentation:ADATEcanrepresentacomplexdecisiontreeinaverycompactwaybyusinghelperfunctionsornestedcaseinstructions,whichcouldallowittolearnaverycomplextreerepresentedinonlyafewlinesofcode.ThismakesADATEpowerfulbuteasytooverﬁt,eveniftheOccam’srazoremployedasaregularizationinADATE.AshortprogramispreferredinADATEsearchingstrategy,butevenashortprogramcouldrepresentaverycomplexmodel.AnothergoodexampleforthefeatureextractionabilityofADATEsystemisitsso-lutionforthefour-wayitem-to-itemnavigationproblem,wherewehavetobuildamodeltopredictwhichitemthattheuserwanttoselect,providingcurrentselecteditem,listofavailableitemsandthenavigationdirection[21].Figure7.2illustratestheproblemclearly,whilethebestADATE’ssolutionisshowninCode7.2.Figure7.2:Takenfrom[21],mobilenavigationexample:ItemAisselected,andtheuserhaspressedtheup-button.Themodelhastopredictwhichitemtheuserintendstoselectfunctionf(2listofpoints,directionofmovement,4currentpos):point6beginbestp=badinitialvalue8foreachpointpinlistofpointsdovectorv=linefromcurrentpostop10ifanglebetweenvanddirectionofmovement<1245degrees7.2.AShortAnalysisofthePowerofADATESystem61and14distance(currentpos,p)<distance(currentpos,bestp)16thenbestp=p18returnbestpendCode7.2:Takenfrom[21]PseudocodeforthebestADATE’ssolutionYoucanseethattheADATEhasdiscoveredtwonewfeaturesforeachpointp:theanglebetweenthevectorv(vectorfromcurrentselecteditemtothepointp)andthemovementdirection;andthediﬀerencebetweenthedistancesbetweenselectedpointandthecurrentpointandthebestpointsofar(thefunctionforcalculatingtheEuclideandistanceisgivenasprimitivefunction).Ifthesefeaturesareusedwhenbuildingadecisiontree(insteadofthepoint’scoordinates)todecidewhetherweshouldselectthecurrentpointasthebestpointsofar,orignoreit,itprobablybecomesaveryeasylearningproblem.However,itisveryhardtobuildnewusefulfeaturesfordecision-makingmanually.Therefore,ADATE’sabilityofdiscoveringnewusefulfeaturesisextremelyvaluable.CategoricalExplanatoryVariablesInthistypeofproblem,alloftheexplanatoryvariablesarecategorical.AsimpletoyexampleforthisistheCARproblem,whereyouhavetopredicttheriskofbeingstolenofacar,basedonitsfollowingattributes:thecarmodel,isithasansecurityalarm?,isitparkedinurbanorruralarea?,isitparkedonstreetoringarage?.TheADATEsolutionforthistypeofproblemwilllookexactlylikeadecisiontree.OneofitssolutionisshowninCode7.3.However,thankstoitsbetteroptimizationstrategy,theADATEwillprobablycomeupwithabetterdecisiontree,comparedtoadecisiontreelearnedbyatraditionalapproachlikeID3.1funf(X0model,X1alarm,X2area,X3parking)=caseX0modelof3modelopel=>(caseX3parkingof5parkingstreet=>rischigh|parkinggarage=>risclow7)|modelvolvo=>risclowCode7.3:OneADATE’ssolutionforCARproblem7.2.2ADATEonRegressionProblemsRegressionistheproblemofpredictingvalueofacontinuousvariablebasedonotherexplanatoryvariables.Inthisproblem,ADATEusuallysynthesizessolutionsthatcanbegeneralizedbetterthaninclassiﬁcationproblem.Tounderstandthisphenomenon,Ihaveinvestigatedthefollowingexamples.Theﬁrstsimpleexampleistheproblemofpredictingthevelocityofafallingballwhenithitstheground,giventheheightHofitsstartingpoint.Thecorrectanswerforthisproblemis√19.6H.AtypicalADATE’ssolutionforthisproblemisshowninCode7.4.funfH=2realAdd(62Chapter7.IntroductiontoADATErealMultiply(4H,tanh(tanh(sqrt(tanh(tanh(tanh(tanh(tanh(H))))))))6),realAdd(8tanh(H),sqrt(3)10))Code7.4:TypicalADATE’ssolutionforSPEEDproblemAsyoucansee,theADATEtriedtoapproximatethecorrectfunctionbyarelativelycom-plexprogramwithsomeredundantoperations.However,whencomparingthisprogramtoadenseneuralnetworkswithonly10hiddennodes,thesolutioninCode7.4ismuchsimpler.Adenseneuralnetworkswithonly1input,10hiddennodes,and1outputwhenrepresentedina“program”formneeds42operations4,whiletheADATE’ssolutionabovecontainsonly13operations(someoperationsareevenredundantandcanbediscardedwithoutaﬀectingthefunctionmuch)5.Therefore,aneuralnetworkhasmuchmorefree-dominitsparameter’sspace,whichcouldexplainwhyADATE’ssolutionforthisSPEEDproblemcanbegeneralizedmuchbetterthananeuralnetwork.IhypothesizethattheCode7.4canberepresentedbyaverydeepandsparseneuralnetwork,whichisthecurrenttrends,andthebestwayindesigningneuralnetworknowadays.TheCode7.4containsrealMultiplyandsqrtfunction,whichcannotbedirectlytranslatedintoaneuralnetwork.However,Ibelievethatifweonlymakelinearoperations(i.e.realAddandrealSubstract)andanactivationfunction(e.g.tanhorsigmoidfunction)availableasprimitivefunctions,ADATEwillgenerateasolutionthatcanberepresentedexactlybyaverydeepandsparseneuralnetwork.Besides,multiplication,division,sqrt,ormanyotherarithmeticopera-tionscanbeeﬃcientlyrepresentedbyasmallone-layerneuralnetwork,whichisprovenby[37].Letcheckmyhypothesisonadiﬀerentproblem,whichcontainsmoreexplanatoryvariables.Thereisawinequalityproblemwherewehavetopredictthequalityofwinebasedonitselevenattributes:Fixedacidity,Volatileacidity,Citricacid,Residualsugar,Chlorides,Freesulfurdioxide,Totalsulfurdioxide,Density,Ph,Sulphates,andAlcohol.Alloftheseexplanatoryvariablesarecontinuous.OnetypicalADATE’ssolutionforthisproblemisshowninCode7.5.Youcanseethatthesynthesizedmodellookverylikeadeepsparseneuralnetwork.IfassumedthattherealMultiplyandrealDivideoperationsinthatcodecanbeapproximatedbyasmallshallowneuralnetwork,wecanactuallyapproximatethatffunctionbyaﬁve-layersandverysparseneuralnetworks.1funf(3Fixedacidity,Volatileacidity,5Citricacid,Residualsugar,7Chlorides,42∗10linearoperationsand10activationfunctionsfortheﬁrstlayer,and11∗1linearoperations+1activationfunctionfortheoutputlayer5Infact,theformulaH∗tanh(tanh(p(tanh(tanh(tanh(tanh(tanh(H))))))))canbeapproximatedbytwosimplelinearequations:y=0ifH<0andy=0.57143HifH≥07.2.AShortAnalysisofthePowerofADATESystem63Freesulfurdioxide,9Totalsulfurdioxide,Density,11Ph,Sulphates,13Alcohol)=15realMultiply(Alcohol,17realAdd(Volatileacidity,19sigmoid(realAdd(21Totalsulfurdioxide,realDivide(23realSubtract(Volatileacidity,Residualsugar),25sigmoid(Volatileacidity))27))29))Code7.5:TypicalADATE’ssolutionforWinequalityproblemAnotherveryimportantdiscoverywhenIinvestigatedmanydiﬀerentADATE’ssolutionsforregressionproblemsisthattheADATEsystemusuallydoesnotsynthesizeprogramsthatsplittheinputspaceintodiﬀerentsub-spaces,asincaseofclassiﬁcationproblems.Withoutthesedivideandconquerability,theADATEhaslostaveryimportantpartofitsﬁttingpower.Thislost,however,makestheADATEsystemmuchhardertobeoverﬁtted.Generally,onregressionproblems,theADATEhastobuildasinglemodelforthewholeinputspace,insteadofbuildingdiﬀerentsimplemodelfordiﬀerentpartsoftheinputdomain.ThismayexplainwhythegeneralityofADATE’ssolutionsforregressionproblemisbetterthatitssolutionsforclassiﬁcationproblems.TheevolutionstrategyusedinADATEcanbeapossiblefactorwhichcausesthisphe-nomenon.Theevolutionstrategysearchesforthesmallestprogramsﬁrst.Afterthat,itgeneratesotherbiggerprogramsbyapplyingdiﬀerentkindsoftransformationandmuta-tion.Inregressionproblem,weneedthefunctionftoreturnacontinuousvalue.Therefore,thesmallestprogramswhichreturncontinuousvalueswillbemorelikelytobepickedforlatertransformingandmutating.IfADATEtriestoadda“case”instructionintoanex-istingprogram,thatcaseinstructionhastoconditiononacontinuousvariable,whichcannotserveasabranchingstatement.Ofcourse,theADATEcanﬁrstapplyafunctionthatreturnsabooleanvaluelike“realLess”function,andintroducethecaseinstructionafterthat.However,becausebooleanvaluescannotbeusedinanequationofreal-valuevari-ables,theADATEusuallyignoresthesetransformations.Inotherwords,iftheADATEwantstoaddabranchingstatementintoacurrentprogram,itmustallowatwo-steptransformation,whereitcanaddaboolean-returnfunctionandacaseinstructionatthesametime.Becauseifit,almostanycaseinstructioninADATE’ssolutionsforregres-sionproblemsservesasandeclarationinstructionfortemporaryvariablesinsteadofabranchingstatement.Ofcourse,theADATEsystemcantakeadvantagesofcategoricalexplanatoryvariablestogeneratebranchingstatementiftheyexist.However,intypicalregressionproblems,alloftheexplanatoryvariablesarecontinuous.64Chapter7.IntroductiontoADATE7.2.3ADATEonClassicalalgorithmADATEhasbeenprovenabletogeneratethecorrectalgorithmsformanydiﬀerentclassicalproblems,suchas:insertinganddeletinganodeinbinarytree,generatingallpermutationsofalist,orsortingalist.Wewouldprobablyaskwhyitcouldgeneratethecorrectanswers,insteadofanapproximateanswer,forthistask.Ibelievethatthefollowingreasonsmayanswerthequestion:•Existence:ADATEcandiscoverthecorrectsolutionsfortheseproblemsmaybejustbecausetheyexist.Intypicalclassiﬁcationorregressionproblems,thereisusuallyaprobabilisticmodelbehindthescene,insteadofadeterministicmodel.Therefore,synthesizinganexactalgorithmforthesetasksisimpossible.•SuitableRepresentation:ADATErepresentsitsanswerina“functionalprogram”form,whichisverysuitableforexpressingalgorithms.Besides,thesealgorithmshavecorrectanswersthatcanberepresentedinaveryshortfunctionalprogram,whichisexactlywhattheADATEevolutionstrategyprefers.•NoConstant:IhypothesizethatADATEisnotverygoodatgeneratingaconstantnumberinitsprogram.Itmaybebecauseaconstantnumberthatisusefulincurrentprogramwillbecomecompletelyirrelevantinitstransformations.Inotherwords,unlikeotheroperations,aconstantnumberdependsverymuchontheprogramcon-text,andwhenwechangetheprogramjustalittlebit,itbecomesuseless.However,youprobablywillnotseeanyrandomconstantnumberinclassicalalgorithms,whichmayhelptheADATEverymuch.Code7.6illustratesoneofthebestADATE’ssolutionforsortingproblem,whereyouhavetosortalistofintegernumberintoascendingorder.funf(V1)=2caseV1ofnil=>V14|(V11::V1’)=>let6fung(V2)=caseV2of8nil=>(V11::nil)|(V21::V2’)=>10case(V11<V21)oftrue=>(V11::V2’)12|false=>(V21::g(V2’))14ing(f(V1’))16endCode7.6:AnADATE’ssolutionforsortingproblemThiscodeishardtounderstandatﬁrstsight.Butifwelookcloser,wecanseethatthevariableV11isincludedintheclosureoftheg(.)function.Therefore,wecanchangethefunctioncallg(f(V10))intog(V11,f(V10)).Now,thattailrecursionistryingtoapplythegfunctiononallcomponentsofV1.Forexample,ifV1=A1::A2::A3::A4::nil,thattailrecursionisequivalenttothisfunctioncall:g(A1,g(A2,g(A3,g(A4,nil))))(7.1)7.2.AShortAnalysisofthePowerofADATESystem65Nowifwechangethenameoffunctiong(x,V)intoinsert(x,V),theequation7.1isnowequivalenttoinsert(A1,insert(A2,insert(A3,insert(A4,nil))))(7.2)Wecanseethatiftheinsert(x,V)functionreturnsasortedlistbyinsertxintoV,giventhatVisasortedlist,thenthealgorithmiscorrect.Lookatthebodyofinsert(x,V)functioninCode7.66,youcanseethatitcomparextotheﬁrstelementyinV.Ifxsmallerthany,thenitreturnthelistx::V.However,ifxisequalorgreaterthany,thenywillbetheﬁrstelementinthereturnlist.Therestofreturnlistwillbesortedagainbycallinginsert(x,V0)withV0isthevectorVafterremovingitsﬁrstelementy.ThisrecursioncontinuesuntilxsmallerthanyorVbecomesnil.Therefore,thisADATE’ssolutionforsortingproblemsisindeedacorrectalgorithm.7.2.4ADATEonMeta-learningOnespecialabilityofADATEthatyoucannotﬁndinmostofothermachinelearningtoolsisthatitcandometa-learningor“learnhowtolearn”.ItmeansthatwecanuseADATEtoimproveotheralgorithmsormachinelearningmethods.TheevolutionstrategythatemployedinADATEisveryﬂexiblewhichallowittoworkinverydiﬀerentways.Mostofmachinelearningoptimizationmethodsneedawell-deﬁnedcontinuousobjec-tivefunction,whichrepresentsaclearrelationshipbetweenthecurrentmodel’sparametersconﬁgurationanditsperformance.Basedonthatfunction,theyusesomesearchingstrat-egytooptimizetheparameters,suchasgradient-basedmethods.Forexample,themeansquarederrorsisacommonobjectivefunctioninregression,whilenegativeloglikelihoodisusedinclassiﬁcationproblems.Theneedofcontinuousobjectivefunctionlimitsthepossibleproblemsthatthesemethodscanbeappliedon.Ontheotherhand,theADATEsystemusesﬁtnessfunctioninitssearchingstrategy.Althoughtheﬁtnessfunctioncanbeconsideredasaspecialtypeofobjectivefunction,itisverydiﬀerentfromothertypes.Generally,itisnotneededtobeacontinuousfunction,andtherelationshipbetweenamodelanditsﬁtnessmeasurementcanbeveryvague.Therefore,weusuallycannotgetanygradientinformationfromaﬁtnessfunction.OneexamplethatcandemonstratethisADATEﬂexibilityclearlyisusingADATEtoimprovedecisiontreepruning[13].Inthisproblem,theauthorswantedtoimproveanerrorbasedpruning(EBP)algorithmusedinC4.5decisiontree.StartingfromtheinitialffunctionthatisanaiveimplementationofEBP,ADATEhasdiscoveredabetterversionofit.Theoriginalffunctionisrelativelylongandcomplicatedfunction.Therefore,IonlychooseasmallpartofittodemonstratetheADATEﬂexibility.Code7.7showstheoriginalandimprovedversionoferrorEstimatefunctionusedinEBPalgorithm.Thisfunctiontakestwoarguments:n–thenumberofinstancesthatreachagiventreenode,andc–thenumberoftheseinstancesthatarecorrectlyclassiﬁedbythesubtreecorrespondingtothenode.(*OriginalerrorEstimate(.)function*)2funerrorEstimate((c,n):real∗real):real=let4vale=(n−c)/nvalz=0.696valz2=z∗zvalval1=(e/n)−((e∗e)/n))+(z2/(4.0∗n∗n)6theg(V2)functionistheinsert(x,V)functionwithxisV11andVisV266Chapter7.IntroductiontoADATE8valval2=(e+z2/(2.0∗n))+z∗(sqrtval1)valval3=1.0+z2/n10inval2/val312end(*ImprovederrorEstimate(.)function*)14funerrorEstimate((c,n):real∗real):real=let16valv1=tanh(tanh(tanh((n−c)/n)))valv2=sqrt(tanh(tanh(sqrt(n))))18valv3=tanh(sqrt(sqrt(c)))in20(v1+v2)/v3endCode7.7:TheoriginalandsynthesizederrorEstimatefunctionusedinEBPalgorithmThiscanbeconsideredasaregressionproblem,wherewewanttopredict(i.e.esti-mate)theerrorvalue,basedonthetwoexplanatoryvariablescandn.TheADATE’ssolutionforthisproblemcanalsoberepresentedbyaneuralnetwork.However,wecannotdirectlyuseneuralnetworksorothermachinelearningmethodsonthisproblem,becausethereisnoclearrelationshipbetweentheoutputofthefunctionerrorEstimate(.)anditsperformance.InADATE,thaterrorEstimate(.)functioniscalledinthedecisiontreecode,whichthenistestedinseveraldiﬀerenttrainingdatasetstocheckitsperformance.Ingeneral,theADATEcanbe,andhasbeen,successfullyusedtoimproveothermachinelearningmethods,especiallywhentheyincludeheuristicfunctions.However,thecurrentversionofADATEhassomefollowinglimitations,whichcanpossiblybeﬁxedinfuture:•SlowSearchingStrategy:Evolutionstrategyisaverygoodbutslowoptimizationmethod.Therefore,weusuallyneedtodesignsmallartiﬁcialdatasetstotraintheADATEﬁrst,beforegeneralizingitssolutiontothereal-worldproblems.However,insomecases,itssolutionsforthesmalldatasetscannotbegeneralizedwelltootherbiggerdatasets.Thismaybecomesfromabaddesignofartiﬁcialdatasets,orbadADATE’sconﬁguration.•Synthesizedprogramcan’tbecalled:IncurrentADATEversion,outsideoftheADATE-MLpart,thesynthesizedprogramcannotbecalled.ThismeansthatyouhavetoimplementthewholeoriginalalgorithminADATE-MLifyouwanttocallthesynthesizedprograminyouralgorithm.Thisworkisnottrivial,becauseADATE-MLisasimpliﬁedversionofStandard-ML,whichisaverysmallandlimitedlanguage.However,thislimitationisgoingtobeﬁxedsoon.InthenextADATEversion,thesynthesizedprogrammayevenbecalledfromCorotherexternalprograms,whichwillmakeADATEmucheasiertouseandexperiment.ThischangecanalsospeeduptheADATEsystem,becauseprogramswritteninCusuallyrunmuchfasterthantheirMLversions.•Hardtounderstand:OnereasonthatmakesADATE’simprovedversionofothermachinelearningmethodsunpopularisthatweusuallycannotunderstandcom-pletelythesynthesizedprogram.Itmeansthatwecannotprovethecorrectnessofthatprograminamathematicalway,whichisusuallydesiredbyotherresearchers.Chapter8ADATEExperimentsThischapterpresentsourexperimentsasaprocess,whichcanansweralloftheresearchquestionsposedinChapter1.Basedondeeplearningknowledgethatwesummarizedinpreviouschapters,weanalyzedwhywechosetheinitializationpartofdeeplearningalgorithmtoexperimentﬁrst.Afterthat,wepresentedhowwebuilttinydatasetsandneuralnetworkslibraryforADATE.AfterrunningtheADATEsystemtoevolvetheinitialsparseinitializationscheme,wegotthesparse-3program,whichwillbeanalyzedcarefullyinthischapter.Ashortanalysisofoverﬁttingproblemofsynthesizedprogramsisalsopresentedattheendofthechapter.8.1SelectingTargetThepreviouschaptersaboutdeeplearningpresentmanydeeplearningmethods,whichcouldpossiblybeimprovedbyADATE.Themostvaluableoneifwecouldimproveistheunsupervisedpre-trainingmethod.However,thisisacomplexandtime-consumingprocess,whichishardtoworkonfortheﬁrstexperiment.Wewantedtoselectasimplebuteﬀectivemethod,whichdoesnothavedependencyrelationshipwithotherpartsofthedeeplearningprocess.Themostpotentialtargetsare:•Autoencodermodels:manydiﬀerenttypesofautoencodershavebeenintroducedinchapter3.Byaddinganextratermintotheobjectivefunction,wecangetamuchbetterversionofautoencoders.WecanuseADATEtodiscoveranewbetterterm.However,weneedtotakethegradientofthenewobjectivefunctionautomatically,whichisrelativelyhardanderror-prone.•Optimizationmethods:chaper4showsthattheneuralnetworksperformancecanbeimprovedsigniﬁcantlybychangingtheinitializationschemeorthemomentumschedulewithoutusingpre-trainingmethods.WhileimprovingmomentumscheduleiscurrentlynotpossiblewithcurrentversionofADATE,theinitializationschemeisaseperatepartofthetrainingprocessandcaneasilybeimprovedbyADATE.•Regularizationmethods:chapter5presentsdropoutanddropconnect,thetwoverynewbuteﬀectiveregularizationmethods.However,wehavetocalltheregularizationmethodforeachiterationofthetrainingprocess,whichisnotpossibleincurrentADATEversion.6768Chapter8.ADATEExperiments•Activationfunction:chapter6showsthatbychangingtheactivationfunction,wecanimprovethenetwork’sperformance.However,becausewehavetotakethegradientofthenewactivationfunctionautomatically,thisisnotaneasytarget.Therefore,theinitializationschemewaschosenforourﬁrstexperimentwithADATE.Otherpartsofthedeeplearningprocessarealsopotential,butwehavetowaituntilthenextversionofADATE,whenwecancallthesynthesizedprogramfromoutsideoftheADATEpart.8.1.1CheckingtheEﬀectivenessofInitializationSchemesBeforestartingtoimprovetheinitializationscheme,wehadtocheckifchangingitmakesasigniﬁcantdiﬀerenceinthenetwork’sperformance.Inthisexperiment,wetrainedadeepneuralnetworkwith3hiddenlayers,theircorrespondingsizesare:500,500and2000,usingsteepestgradientdescentwithmomentumsuggestedin[46].L2weightdecaywasalsousedandﬁxedat10−5.Thebatchsizeischosenat200asin[46].Basically,allhyper-parametersinourexperimentareﬁxed,exceptforthelearningrateandinitializationmethodaspresentedintable8.1.Table8.1:ParametersusedwhentrainingonMNISTdatasetParametersConﬁgurationDataset:MNISTNetworkstructure:[784500500200010]Costfunction:NegativeloglikelihoodMomentumschedule:µ=min(1−2−1−log2(bt/250c+1),µmax)Momentummax:0.999L2weightdecay:10−5Batchsize:200Learningrates:chosenfrom[0.050.010.0050.0010.00050.0001]Initializationmethods:Normal;Normalized;SparseIntheﬁrstexperiment,wetrainedthenetworkforallpossiblecombinationsoflearningrateandinitializationmethodandstopthetrainingprocessaftertheﬁrst100epochs.Theresultisexpectedtobenoisybecauseofrandominitializationand100epochsarenotenoughtogetthetrainingprocesstobesaturated,especiallywithsmalllearningrate.However,wecanstillseethediﬀerenceinperformanceofdiﬀerentinitializationmethodsondiﬀerentlearningrate.Table8.2:Testerrorratesafterﬁrst100iterationsfordiﬀerentlearningratesandinitializationsInitializationLearningrates0.050.010.0050.0010.00050.0001Normal:4.33%2.89%3.92%6.57%7.89%10.76%Normalized:4.44%3.17%4.46%7.28%8.27%11.21%Sparse:3.86%2.94%3.47%4.6%5.26%7.56%∆Normal-Sparse:1.45%±1.34%∆Normalized-Sparse:1.86%±1.43%8.2.BuildingTinyDatasets69Intable8.2,wecanseethatsparseinitializationconsistentlyoutperformsothermeth-ods,regardlessofwhatlearningrateisusing.Inthesecondexperiment,wecheckedperformanceofdiﬀerentinitializationmethodsat0.01learningrate,whichisthebestlearningrateforalloftheminpreviousexperiment.Foreachinitializationmethod,wetrainedthenetwork5timesinattempttoreducenoiseproducedbyrandominitialization.Theresultintable8.3showsthesamethingintheﬁrstexperiment,butwithlowernoise.Weagainseethatsparseinitializationcaneasilyoutperformotherinitializationmethods,atthesameordiﬀerentlearningrate.Table8.3:Testerrorratesafterﬁrst100iterationsfordiﬀerentinitializationsatthesamelearningratesInitializationLearningrates0.010.010.010.010.01Normal:3.08%2.96%3.05%3.17%3.12%Normalized:3.11%3.16%3.11%3.19%3.01%Sparse:2.70%2.56%2.60%2.45%2.69%∆Normal-Sparse:0.48%±0.139%∆Normalized-Sparse:0.516%±0.164%Thesetwoaboveexperimentsprovedthatinitializationschemeisanessentialpartofthetrainingprocess.Inaddition,ifwecanimproveitbyADATE,wecanimprovethenetwork’sperformancesigniﬁcantly.8.2BuildingTinyDatasetsADATEusuallyneedstogenerateandevaluateatleasthundredsofthousandorevenmillionsofdiﬀerentprogramsbeforepossiblydiscoveringthebestones.Therefore,usinghugedatasetlikeMNISTdirectlyisdeﬁnitelyimpossible.OthersmallerdatasetssuchasCurvesorUSPScouldnothelpalso.Whatweneedisatinydataset,onwhichtrainingaDNNcostsleastthantenseconds.Besides,onecrucialpropertythatthetinydatasethastopossessisthataninitializationschemethatperformswellonitcanbegeneralizedandperformaswellonotherrealandbiggerdatasets.8.2.1TinyDigitsInourﬁrstexperiment,aftersearchingformanypossibilities,weﬁnallyendedupusingtheTinyDigitsdataset.TinyDigitsisa10x10digitsimagesdataset,whichisgeneratedusingtheelasticdeformation.Wechosetogenerateourownsyntheticdatasetconsistingof10x10pixelimagesofdigitsthataredistortedusingelasticdeformations.Togeneratethisso-calledTinyDigitsdataset,weﬁrsthand-designedthreediﬀerent8x8pixelpatternsforeachdigit.Fromthesepatterns,weextendedtheirbordertoget10x10pixelimages,andranelasticdeformationtoauto-generateothertrainingexamples.WeusedthefollowingparametersfortheelasticdeformationalgorithminTinyDigits:•αandσ:parametersforelasticdistortions;setto3and7respectively.(seeSimardetal.2013[44]formoredetails).70Chapter8.ADATEExperimentsFigure8.1:ElasticdeformationandTinyDigits.Firstline:ahand-designedpatternforthedigit“0”anditsdeformedversions.Twobottomlines:examplesofgenerateddigitsfrom0to9.Notethatallofthesedeformedimagesareintelligible•β:arandomanglefrom[−β,β]isusedforrotation;settoπ12.•γ:arandomscalingfrom[1−γ100,1+γ100]isusedforhorizontalandverticalscaling;setto15.TheelasticdeformationandtheTinyDigitsdatasetareillustratedinFigure8.1.8.2.2TinyUSPSAftertestingsomeADATE-generatedinitializationschemes,wesuspectedthattheyareoverﬁtedtotheTinyDigitstask.Totestthishypothesis,weneedmorediﬀerenttinydatasets,becausetestingontheoriginalMNISTistime-consuming1.TinyUSPSwasourﬁrstandverysimplesolution.USPSistheUSPostalServicehandwrittendigitsrecognitioncorpus.Itcontainsnormalizedgreyscaleimagesofsize16x16,dividedintoatrainingsetof7291imagesandatestsetof2007images.Wetookthecenter10x10pixelsoftheseUSPSimagestocreatetheTinyUSPSdataset.Ofcourse,doingthiswaycoulddiscardsomeusefulinformationfordistinguishingdiﬀerentnumbers,whichmakesthetaskharder.However,thisdatasetwasindeedveryusefulandhelpedusrecognizetheoverﬁttingproblemclearly.8.2.3CompressedMNIST,Cifar-10,andSVHNThreeofthemostfamousdatasetsforimagerecognitiontaskareMNIST,CIFAR-10andSVHN:•TheMNISTisahandwrittendigitsdataset,whichhasatrainingsetof60,000examples,andatestsetof10,000examples.Thedigitshavebeensize-normalizedandcenteredinaﬁxed-sizeimage.•TheCIFAR-10datasetisanobjectrecognitiondatasetwhichconsistsof6000032x32colourimagesin10classes,with6000imagesperclass.Thereare50000trainingimagesand10000testimages.1weneedtore-optimizemanyhyper-parametersontheMNISTdatasetﬁrst,andthentraintheneuralnetworkforatleast10timestocheckifthediﬀerencesarestatisticalsigniﬁcant8.2.BuildingTinyDatasets71•TheStreetViewHouseNumbers(i.e.SVHN)isareal-worldimagedatasetwhichcanbeseenassimilarinﬂavortoMNIST(e.g.,theimagesareofsmallcroppeddigits),butcomesfromasigniﬁcantlyharder,unsolved,realworldproblem(recognizingdigitsandnumbersinnaturalsceneimages).SVHNisobtainedfromhousenumbersinGoogleStreetViewimages.Figure8.2:Fromtoptobottom:MNIST,CIFAR-10andSVHNdatasetsMNIST,Cifar-10,andSVHNdatasetscontainmuchbiggerimagescomparedtotheUSPS(28x28,32x32,and32x32respectively).Therefore,taking10x10centerpixelsoftheimageswouldmakethemunrecognizable,andchangethetaskcompletely.Onepossiblesolutionisthatwecantrainadeepautoencodertocompressthesedatasets.ForMNIST,wehaveusedthenetwork:[784-1000-500-100-500-1000-784]and[784-1000-500-30-500-1000-784]tocreatethecompressedMNIST100andcompressedMNIST30respectively.OthersimilarnetworkarchitecturewasalsousedforCifar-10andSVHN.Currently,weusetheHintonimplementationofdeepautoencoder,whichusesstakedRBMforpre-trainingandConjugateGradientforﬁne-tuning.Ofcourse,thereareother72Chapter8.ADATEExperimentsnewerandbetterwaystotrainadeepautoencoder,suchasusingstakeddenoisingau-toencoderorsparseautoencoder.However,theHinton’simplementationiseasytouseandstillagoodone.8.3DesignandImplementationAsmentionedearlier,aneuralnetworklibraryhastobeimplementedinSMLbeforebeingabletotakeadvantageofADATEtoimprovesomepartsofit.Withinthischapter,IwillreporthowIhaddesignedanddevelopedthelibrary.Thisisindeedatime-consuminganderror-proneprocess,onwhichIdidmakeseveralmistakes.Therefore,Iwanttosharemyexperienceonthisprocess,whichcouldbeveryhelpfulforotherstudentsorresearchers.Ittookmemonthstoovercomeallofthesemistakes.BecauseSMLdoesnotsupportmatrixoperationsandsomeusefulrandomnumbergenerators,Ihadtobuildtheselibrariesmyselfalso.Ialsobuildasmallunit-testlibrarytotestmyimplementation.AllthecodesmentionedinthissectionareprovidedinAppendixA8.3.1MatrixlibraryMatrixoperationsareessentialpartofmanymachinelearningalgorithms,whichbaseheavilyonlinearalgebra.AftersearchingforaneﬃcientmatrixlibraryforSMLandcouldnotﬁndany,wehavedecidedtoimplementanewone.Thislibraryrepresentsamatrixasalistoflist,whichsupportscommonoperatorsonmatrix,includingmultiply,dotmultiply,sum,...Besides,thereisalsoprintfunctionstohelpdebuggingprocesseasier.Storingamatrixaslistoflist,insteadofarrayofarray,makestheimplementationeﬃcientandﬁtnaturallyinSMLlanguage.ManyfunctionswereinspiredbythestandardListlibraryofSML.Besides,allmatrixoperatorsinthislibraryhavetheequivalentcomplexitycomparedtowhatcouldbedoneusingarrayofarrayimplementation.Webelievethatwiththislibrary,wecanre-implementmanydiﬀerentmachinelearningalgorithmseasily.However,wearetryingtoreplacethismatrixlibrarybythestandardBLASlibrary,whichcouldprovidemuchhighermatrixoperations’performance.PleasechecktheChap-ter9aboutfutureworksformoredetails.MatrixdatastructureInthislibrary,thematrixdatatypeisdeﬁnedas:1type’avector=’alistdatatype’amatrix=3COLMATRIXof(’avectorlist∗int∗int)|ROWMATRIXof(’avectorlist∗int∗int)Therearetwomatrixtypes:COLMATRIX-matrixthatstoredaslistofcolumnvectors;andROWMATRIX-matrixthatstoredaslistofrowvectors.Thisstoringwaycouldmakemanymatrixoperationsbecomeaseﬃcientasusingarrayofarray.ChangingbetweenCOLMATRIXandROWMATRIXtypehascomplexityO(n∗m)withnandmarethenumberofrowsandcolumnsofthematrix.However,wecaneasilytransposeamatrix(andswitchthematrixtypeatthesametime)instantlyatO(1)complexity.A8.3.DesignandImplementation73matrixalsohasitssizeinitsdatastructure-thelasttwonumberinthetuple-tomakecheckingsizeoperationsmoreeﬃcient.WealsoprovideacheckValidfunctiontocheckifamatrixisvalidornot.However,ifweareusingthislibraryasaseparatedmodule,thematrixdatatypeishidedandcanonlybecreatedusingoneoftheprovidedinitializationfunctions,whichmakessurethatallthematricesarevalidatalltime.Thesementionedfunctionsabovehavethefollowingsignatures:valchangeType:’amatrix−>’amatrix2valtranspose:’amatrix−>’amatrixvalcheckValid:’amatrix−>bool4valsize:’amatrix−>int∗intMatrixinthislibrarycanbeeitheranintmatrixorarealmatrix.However,becauseSMLisastatictypelanguage,manymatrixoperationshaveanintandarealversion.Weprovidesixdiﬀerentwaystoinitializeeitheracolumnorarowmatrix,whichhavethefollowingsignatures:valzeroesRealCols:int∗int−>realmatrix2valzeroesRealRows:int∗int−>realmatrixvalzeroesIntCols:int∗int−>intmatrix4valzeroesIntRows:int∗int−>intmatrixvalfromList2Cols:’alist∗int∗int−>’amatrix6valfromList2Rows:’alist∗int∗int−>’amatrixTherearealsotwoprintingfunctions,oneforrealmatrixandoneforintmatrix,tosupportdebugging:valprintMatrixReal:realmatrix−>unit2valprintMatrixInt:intmatrix−>unitThislibraryusestwoexceptions:WrongMatrixTypeandUnmatchedDimension.ItraisestheWrongMatrixTypeexceptionwhentheinputmatrixisnotatthedesiredtype,andraisestheUnmatchedDimensionwhenthedimensionofamatrixitselfisunmatched(notavalidmatrix)orthetwoinputmatrices’dimensionsareunmatched(asinmatrixmultiplication).ScalaroperatorsScalaroperatorsaretheoperatorsthataﬀectallmatrixelementsinthesameway,suchasaddanumbertothewholematrix.Allscalaroperatorsinthislibraryareimplementedbasedonthefunctionmapfm,whichapplyftoallelementsinthematrixm.ThisfunctionisverysimilartothefunctionList.map,andindeedimplementedbasedonit:funmapVectorsfvs=2List.map(fnv=>List.mapfv)vs4funmapf(COLMATRIX(vs,rows,cols))=COLMATRIX(mapVectorsfvs,rows,cols)6|mapf(ROWMATRIX(vs,rows,cols))=ROWMATRIX(mapVectorsfvs,rows,cols)74Chapter8.ADATEExperimentsUsingthismapfunction,wecaneasilyimplementanyscalaroperators,forexample:1funaddScalarInt(m,x:int)=map(fna=>a+x)mfunaddScalarReal(m,x:real)=map(fna=>a+x)m3funmulScalarInt(m,x:int)=map(fna=>a∗x)mfunmulScalarReal(m,x:real)=map(fna=>a∗x)mMatrixElement-wiseoperatorsMatrixelement-wiseoperatorsaretheoperatorsthattaketwoequal-sizematricesandcombinetheirelementsatthesamepositionbyanarbitraryfunctiontoanewcreateanewmatrix.Allmatrixelement-wiseoperatorsinthislibraryarebasedonthemergefunction,whichtakeacombiningfunctionandtwoinputmatricestoproduceanewone.ItusefunctionmergeVectortomergetwovector,andmergeVectorstomergetwolistofvectors:funmergeVectorfv1v2=2case(v1,v2)of([],[])=>[]4|(hdv1::v1’,hdv2::v2’)=>f(hdv1,hdv2)::(mergeVectorfv1’v2’)6|=>raiseUnmatchedDimension8funmergeVectorsfvs1vs2=mergeVector(fn(v1,v2)=>mergeVectorfv1v2)10vs1vs212funmergef(m1,m2)=case(m1,m2)of14(COLMATRIX(vs1,r1,c1),COLMATRIX(vs2,r2,c2))=>COLMATRIX(mergeVectorsfvs1vs2,r1,c1)16|(ROWMATRIX(vs1,r1,c1),ROWMATRIX(vs2,r2,c2))=>ROWMATRIX(mergeVectorsfvs1vs2,r1,c1)18|=>raiseWrongMatrixTypeUsingthismergefunction,wecaneasilyimplementanyothermatrixelement-wiseoper-ators,suchas:valdotMulMatrixInt=merge(fn(a:int,b)=>a∗b)2valdotMulMatrixReal=merge(fn(a:real,b)=>a∗b)valaddMatrixInt=merge(fn(a:int,b)=>a+b)4valaddMatrixReal=merge(fn(a:real,b)=>a+b)MatrixmultiplicationMatrixmultiplicationisthemosttime-consumingoperationinmanyalgorithms.There-fore,Wetriedtomakethisoperationasfastaspossible.ThislibraryonlyallowsmultiplyaROWMATRIXtoaCOLMATRIX,andraisetheexceptionWrongMatrixTypeinallothercases.Becauseonlyinthatcase,wecanmultiplythetwolistoflistrightawaytoproducethedesiredmatrixproduct.Ofcourse,wecanusechangeType()functionto8.3.DesignandImplementation75changethematrixtypetothedesiredone.However,thatprocesscostsO(n∗m)andwedecidedtoforcetheusersdoitmanually,tomakesurethattheyareawareofthatoverheadcomputingcost.Besides,userscanchoosetoproduceaROWMATRIXoraCOLMATRIXasthematrixresult.Ibelievethatwiththisﬂexibility,youusuallydonothavetochangethematrixtypeinmostalgorithms.Moreover,thetransposeoperationinthislibraryliterallycostnothingwhilecostingO(n∗m)ifusingarrayofarrayim-plementation.Therefore,usingtransposefunctioncancompensateforusingchangeTypefunction.Implementationofmatrixmultiplicationisalittlebitmorecomplicatedthantheabovefunctions.Itusesthreehelperfunctions:foldl2Vectors,mulVectorsVectorandmulVectors.Thefoldl2VectorsisverysimilartotheList.foldlfunction,buttaketwoequal-sizeinputvectorsandfoldthemusinganinputfunction.ThemulVectorsVectorfunctionmultipliesalistofvectorstoavector,andthemulVectorsmultipliestwolistsofvector.Thesignatureofthematrixmultiplicationfunctionsare:valmulMatrixIntR:intmatrix∗intmatrix−>intmatrix2valmulMatrixIntC:intmatrix∗intmatrix−>intmatrixvalmulMatrixRealR:realmatrix∗realmatrix−>realmatrix4valmulMatrixRealC:realmatrix∗realmatrix−>realmatrix8.3.2RandomExtlibraryBecauseSMLdoesnothaveanybuilt-infunctionforGaussianrandomnumbergenera-tororrandompermutationoperator,whichisneededinneuralnetworkimplementation,wehadtoimplementthemmyself.FortheGaussianrandomgenerator,weusedtheBox–Mullermethod,whichisverywell-knownandpopular.Forgeneratingrandomper-mutation(i.e.shuﬄing),theFisher–Yatesmethodwasused.•Box–Mullermethod:isapseudo-randomnumbersamplingmethodforgeneratingpairsofindependent,standard,normallydistributed(zeroexpectation,unitvari-ance)randomnumbers,givenasourceofuniformlydistributedrandomnumbers.Therearetwowaystoimplementthismethod:takesamplesfromuniformdistribu-tionontheinterval(0,1]or[-1,+1].Inourimplementation,weusedthesecondone,whichtakestwosamplesfromtheuniformdistributionontheinterval[-1,+1]andmapsthemtotwostandard,normallydistributedsamplesU(0,1)•Fisher–Yatesmethod:alsoknownastheKnuthshuﬄe(afterDonaldKnuth),isanalgorithmforgeneratingarandompermutationofaﬁniteset–inplainterms,forrandomlyshuﬄingtheset.TheFisher–Yatesshuﬄeisunbiased,sothateverypermutationisequallylikely.Fisher–Yatesshuﬄingissimilartorandomlypickingnumberedticketsoutofahatwithoutreplacementuntiltherearenoneleft.8.3.3SmlunitlibraryBuildingMatrix,RandomExt,andNeuralnetworklibraryrequireimplementingmanydiﬀerentmathematicalformula,whichareveryhardtodebugifanymistakeismade.Therefore,unittestisamustinthisproject.Aftersearchingandcouldnotﬁndanysuitableunit-testlibraryforSML,wehadtobuildourownone,whichissimplebut76Chapter8.ADATEExperimentsfunctional.Thislibrarysupportscomparingdiﬀerentdatatype,measuringrunningtime,andsettingtimeout(foravoidingendlessloop).ThemostimportantfunctioninthislibraryisassertFun(),whichletustestoneinput-outputpair(i.e.testcase)whenapplyingtheinputintoaspeciﬁcfunction.BecauseSMLisstatictypelanguagewhichdoesnotsupporttypeinference,theassertFun()functionalsoneedsustoprovidetheisEqual()function,whichhelpsitcomparethedesiredout-putandthefunctionresult.Eachtestcasealsohasacorrespondingexplanation,whichwillbeprintedoutwhiletestingtohelpdebuggingeasier.ThelibraryalsoprovidetheassertFuns()functionwhichletustestalistoftestcase(i.e.testsuits)foraspeciﬁcfunction.UsingassertFun()andassertFuns()functions,wecaneasilycreateappropriatetestingfunctionfordiﬀerentoutputdatatypes.//−−−−−−assertFun()function−−−−−−//2funassertFunisEqualf(input,output,desc)=let4funrunFun()=let6valtimer=Timer.startRealTimer()valresult=f(input)8valtime=ptime(Timer.checkRealTimertimer)in10(result,time)end12val(result,time)=runFun();in14assertisEqual(result,output,descˆ"("ˆtimeˆ")")16end18//−−−−−−assertFuns()function−−−−−//funassertFunsisEqualfdesctestcases=20letvalassert=assertFunisEqualf;22funassertAll[]=()|assertAll(testcase::testcases)=24(asserttestcase;assertAll(testcases))in26(print("--------"ˆdescˆ"---------\n");assertAll(testcases))28endCode8.1:implementationofassertFun()andassertFuns()Everylibrarythatwehaveimplementedforthisthesishasitsownunit-testﬁletocheckitscorrectness.Wheneveranyfunctionismodiﬁedoradded,weupdatetheunit-testﬁletoreﬂectthedesiredchanges,andruntheseﬁlestocheckthatallexistingfunctionsareworkingwell,andnewmodiﬁcationdoesnotinterferewiththem.8.3.4NeuralNetworkLibraryNeuralnetworkisinfactageneraltermtoindicateatypeofmodel,whichconsistsmanydiﬀerenttypesoftechniquesfrominitializationschemes,trainingalgorithms,toregularizationmethods...Tobuildaﬂexibleandextendablelibrary,wemodularizedtheneuralnetworktrainingprocessintodiﬀerentparts,whichdiﬀerenttechniquescanbe8.3.DesignandImplementation77chosentoapplyon.Wealsodesignedanewappropriatedatastructureforneuralnetworkmodel,whichhelpsdevelopingprocessbecomeeasier.DatatypeNeuralnetworkisalayeredmodel,whichareconnectedbyadaptiveweights.Basedontheseproperties,werepresentaneuralnetworkbyalistoflayer,whereeachlayerconsistsofonelayerofweight(notlayerofnode),andhasthefollowingdatatype:typennlayer={2input:realmatrix,(*ROWMATRIX*)output:realmatrix,(*ROWMATRIX*)4GradW:realmatrix,(*COLMATRIX*)GradB:realvector,6B:realvector,W:realmatrix,(*COLMATRIX*)8actType:actType}Code8.2:datatypeforonelayerneuralnetworkAsyoucansee,eachlayerconsistsofinputandoutputvector,whichrepresentactivationactivitiesofinputandoutputnodesofthatlayer.Besides,WandBrepresenttheweightsandbiasmatrix.Becausediﬀerentlayercanhavediﬀerenttypeofactivationfunction,thereisactTypeﬁeldtokeepthisinformation.WealsoincludethegradWandgradBmatrix,whicharethegradientscalculatedbyaparticulartrainingalgorithm.Moreover,becausetherearediﬀerenttechniquesandtrainingoptionsthatyoucanchoosefrombeforetraining,wecreatedaparamsdatatype,whichcontainsallthesein-formationandcanbesetthroughfunctionsetParams().Figure8.3showstheparamsdatatypeandallavailabletrainingoptionssofar.1typeparams={batchsize:int,(*numberoftrainingcasesperbatch*)3nBatches:int,(*numberofbatches*)testsize:int,(*numberoftestingcases*)5lambda:real,(*momentumcoefficient*)momentumSchedule:bool,(*use/nouseMarten’smomentumschedule*)7maxLambda:real,(*maxmomentumusedinmomentumschedule*)lr:real,(*learningrate*)9costType:costType,(*costfunctiontypesupport:NLL|MSE|CE|PER*)11initType:initType,(*initializationtypesupport:SPARSE|NORMAL|NORMALISED*)13actType:actType,(*activationfunctiontypesupport:SIGM|TANH|LINEAR*)15layerSizes:intlist,(*structureofnetwork*)initWs:realmatrixoptionlist,(*pre-initializedWsmatrices*)17initBs:realvectoroptionlist,(*pre-initializedBsmatrices*)nItrs:int,(*numberofiterations/epoches*)19wdType:wdType,(*weightdecaytypesupport:L0|L1|L2*)21wdValue:real,(*weightdecayvalue*)verbose:bool(*printtraininginformationornot*)23}Code8.3:Paramsdatatypewhichshowsallavailabletrainingoptions78Chapter8.ADATEExperimentsModulesThegeneralneuralnetworktrainingprocessaremodularizedasshowninﬁgure8.3.Im-plementationoftheseprocessboxesarethefollowingfunctions:•Pre-processingandbatchescreating:implementedbyfunctionreadData,whichcanreadadatasetcontainedinaCSVﬁles,andsplititintoalistofbatches.•Networkarchitecture:isdeﬁnedinparamsvariableandsetbyfunctionsetParams().•InitializeNetwork:eachtypeofinitializationschemeisimplementedindiﬀerentfunction.Eachfunctiontakesresponsibilityofinitializingonenetworklayer,andwillbecalledbyfunctioninitLayers()togeneratethewholeinitializednetwork.WecanalsopredeﬁnetheinitializedneuralnetworksbyusingsetParams(),whichisveryusefulwhenusingADATEtoimprovetheinitializationscheme.•Forwardpropagation:implementedbyfprop1layer()andfprop()functions,whichdoforwardpropagatingforonelayerandthewholenetworkrespectively.•Computecost:implementedincomputeCost()function,whichcurrentlysupports:meansquareerrors(MSE),crossentropy(CE),andNLL(negativeloglikelihood).Thisfunctionisalsousedtocomputetheerrorratesofthenetwork.•Computesearchdirectionandstepsize:currently,thisneuralnetworklibraryonlysupportstheSGDtrainingalgorithmtogetherwithmomentum.Thistrainingpro-cessiscomputedthroughbackpropagation,whichpropagatesthegradientofcostfunctionfromthelasttotheﬁrstlayer.Thisprocessisimplementedbybprop1layer()andbprop()forbackpropagatingonelayerandforthewholenetworkrespectively.•Updateweights:implementedbyupdate1layer()andupdate()functions,whichcanupdateonelayerandthewholenetworkrespectively.•Stopcriteria:currentlythetrainingprocessisstoppedafteraspeciﬁcnumberofepochs(storedinthenItrsﬁeldofparams).However,therearetwooptionsforthereturnvalue:performanceofthelasttrainednetwork(byusingtrainNN()function)orthebestnetworkduringthetrainingprocess(byusingtrainBest()function).8.3.5ImportantWarning!WehighlyrecommendthatyoushoulduseMltoninsteadofStandardMLofNewJer-sey(i.e.SML/NJ)forcompilingﬁnalSMLcode.Mltonisawhole-programoptimizingcompilerforSML,whichcanproduceveryfastexecutablesﬁle(comparedtootherSMLcompiler).Inourexperience,MltonproduceexecutableﬁlethatrunatleastﬁvetimesfasterthanSML/NJ.Moreimportantly,theﬂoating-pointoperationsinMltonareequiv-alenttowhichofMatlaborC++.WeusuallyimplementnewalgorithminMatlabﬁrst,becauseitiseasierandrunfaster.Afterthat,were-implementitinSML.Therefore,weneedthesetwoimplementationstoproduceexactlythesameresult.However,iftheSML/NJisused,itisalmostimpossible.However,forwritingcode,westillrecommendusingSML/NJ,becauseitsupportsaniceREPL(Read–eval–printloop)-aninteractiveprogrammingenvironment-whichyou8.3.DesignandImplementation79 Start Dataset Pre-processing - Split to Data/Validation/Test set - Add auto-generated data (e.g. rotate, skew, elastic deform…) - Normalize attributes’ values Create batches - Choose batch sizes Initialize network - Random initialize - Normalized random initialize - Sparse initialize - Stacked Restrict Boltzmann machine - Stacked Autoassociators - Supervised pre-training Network architecture - Number of layers; nodes per layers - Type of nodes (e.g. tanh, sigm…) Add noise - Gaussian noise = L2 regularization - Rotate, skew, elastic deform… Forward propagation - Dropout - Make hidden nodes stochastic binary Stop? - Early stopping - Overfitting check on validation set Compute cost - Mean square error - Cross-entropy/softmax - L1/L2 weight decay Compute search direction and step size - Gradients and back propagated gradients - Learning rate (decay schedule, line search) - Hessian matrix - Hessian approximation (BFGS, Gaussian-Newton matrix, Hessian-free method…) - Trust region Update weights - Constrained weights - Resilient back propagation No End Yes Figure8.3:Generalﬂowchartofgradient-basedtrainingalgorithmsforNeuralNetworks80Chapter8.ADATEExperimentscannotﬁndinMlton.Moreover,youshoulduseEmacsastheSMLtexteditor.IthasSMLmodethatprovidesgoodsyntaxhighlighting,indentation,andintegrationwiththeSMLenvironment.8.4WritingSpeciﬁcationﬁleThankstotheNeuralNetworklibrary,writingspeciﬁcationﬁlewasprettysimple.Thef(.)functionisthefunctionthatreturnsalistofinitializedweightsforeachnodeintheneuralnetwork.Theoriginalf(.)wasthesparseinitializationscheme.Theonlyproblemwasthatthef(.)functioniswritteninADATE-MLpart,whichdoesnotsupporttheSMLlistdatatypewhichisusedintheneuralnetworklibrary.Therefore,weneededhelperfunctionsthattransformstheADATE-MLdatatypetoSMLdatatype.TheneuralnetworkinADATE-MLdeﬁnedasaweightMatrixlistdatatype:1datatypereallist=rnil|consrofreal∗reallist3datatypereallistlist=rlnil|consrlofreallist∗reallistlist5datatypeweightMatrix=weightMatrixofreal∗reallistlist7datatypeweightMatrixlist=wnil|conswofweightMatrix∗weightMatrixlistwhileinneuralnetworklibrary,theneuralnetworkisrepresentedbyalistofmatrix,whichinturnisalistoflistalso.ThefollowingfunctionswasusedtotransformtheweightMatrixlistdatatypeintolistofmatrixdatatype.1(*convertADATElisttypetoMLlist*)funtoRealListListList(Ws:weightMatrixlist):(real∗reallistlist)list=3letfuntoRealList(rs):reallist=5casersofrnil=>[]7|consr(r,rs’)=>r::toRealList(rs’)funtoRealListList(rls:reallistlist):reallistlist=9caserlsofrlnil=>[]11|consrl(rl,rls’)=>toRealList(rl)::toRealListList(rls’)in13caseWsofwnil=>[]15|consw((nInputs,rls),Ws’)=>(nInputs,toRealListList(rls))::toRealListListList(Ws’)17endfuntoWeightList(Ws:(real∗reallistlist)list)19:realMatrix.matrixoptionlist=let21funinitWeightMatrix(argas(n,W):(real∗reallistlist)):realMatrix.matrixoption=23letvalnInputs=Real.floor(n)25funinitSparseList((ids,initializedWeights,nInputs,I):8.5.ExperimentResults81(intlist∗reallist∗int∗int))27:reallist=case(ids,I<=nInputs)of29(,false)=>[]|([],true)=>310.0::initSparseList(ids,initializedWeights,nInputs,I+1)|(idx::ids’,true)=>33if(idx=I)thenhd(initializedWeights)35::initSparseList(ids’,tl(initializedWeights),37nInputs,I+1)else390.0::initSparseList(ids,initializedWeights,41nInputs,I+1)funinitSparseVects(nInputs,vs)=43casevsof[]=>[]45|v::vs’=>initSparseList((Randomext.randperm(length(v),nInputs)),47v,nInputs,1)::(initSparseVects(nInputs,vs’))49inSOME(Matrix.fromVectors2Cols(initSparseVects(nInputs,W),51(nInputs,length(W))))end53incaseWsof55[]=>[]|W::Ws’=>initWeightMatrix(W)::toWeightList(Ws’)57endFormoredetailaboutthespeciﬁcationﬁle,pleaserefertoAppendixB8.5ExperimentResultsAsetof5trainingand5validationTinyDigitsdatasetsweregeneratedandusedforthisexperiment.Eachtrainingdatasetconsistsof900trainingexamples,whilevalidationdatasetsconsistof600examples.WewereusingasmallDNN(100−80−80−200−10),whichtakesonlyaround1secondforeachepoch.Inourexperiment,wewereusingtanhastheactivationfunctionandsoft-maxwithnegativeloglikelihoodastheoutputunitandcostfunction.Thenetworkstructureandthenumberofepochsareﬁxed.Marten’ssparseinitializationwasusedasthestartingpointforADATE.Afterseveraldaysoflearning,ADATEhadsynthesizedacompletelynewinitializationscheme,whichwecallsparse-3.Theoriginalsparseinitializationandthesparse-3initializationcodegeneratedbyADATEareshowninCode8.4.1//−−Originalsparseinitialization−−//funf(NInputs,NOutputs,LayerType):reallist=3letfunh(N:real):reallist=5case0.0<Noffalse=>rnil82Chapter8.ADATEExperiments−0.8−0.6−0.4−0.200.20.40.60.8050100150200250300Figure8.4:Histogramof10000instancesdrawnfromthetanh(tanh(randn(.)))distribution7|true=>consr(randn(0.0,1.0),h(N−1.0))in9h15.0end11//−−−−−−−−Sparse−3function−−−−−−−−//funf(NInputs,NOutputs,LayerType)=13consr(0.456463462775,15consr(˜1.43515478736,consr(tanh(tanh(randnormal(0.0,1.0))),rnil)17))Code8.4:Originalsparseandgeneratedsparse-3initializationfunctions.Thef(.)functionisusedtogeneratealistofweightsforanode.Theseweightsarethenrandomlyassignedtoitsincomingconnections8.5.1Sparse-3descriptionAsshowninCode8.4,ADATEhascreatedatotallynewinitializationschemewhereeachnodeonlyhas3non-zeroincomingweights,Twoofthemaresettothetwoconstants0.456463462775and−1.43515478736.Thethirdvalueisdrawnfromanewdistribution:tanh(tanh(randn(.))),inwhichrandn(.)isthenormaldistributionwithµ=0,σ2=1.Thisdistributionlookslikeaninvertedbellcurve,sometimescalled“thewellcurve”,whichisbi-modalandusuallyappearsineconomicandsocialphenomena.Ahistogramofthisnewdistributionisshowninﬁgure8.4.Atﬁrstsight,onecouldimaginethatthesparse-3methodmaybeoverﬁttedtothear-tiﬁciallygeneratedTinyDigitsdatasets.Startingwithverylargeweightsatthebeginningofneuralnetworktrainingcouldcreateconﬁgurationsofweights,thatmightbeusefulonTinyDigitsbutunlikelytobeusefulonotherdatasets.Erhanetal.,2009alsosuggested8.5.ExperimentResults83thatsamplingfromafat-taileddistributioninordertoinitializeadeeparchitecturecouldactuallyhurttheperformanceofadeeparchitecture[7].However,asweshallseeshortlyinthenextsection,theperformanceofsparse-3methodontheMNISTdatasetisstatisticallyequivalent2tosparseandnormalizedinitialization,butconvergesmuchfaster.Moreover,whileotherintializationmethodsneedL2regular-izationtoovercomeoverﬁtting,sparse-3doesnotneedit.Withoutweightdecay,sparseandnormalizedmethodsareoutperformedbysparse-32.8.5.2Sparse-3TestingMethodologyWeexperimentedonMNIST,awell-known28x28handwrittendigitsimagesdatasetcomposedof60000trainingexamplesand10000testexamples.Theoriginaltrainingsetwasfurthersplitintoa50000-examplestrainingsetanda10000-examplesvalidationsetinourexperiments.Forsparseandnormalizedinitialization,thelearningrateandl2costpenaltyhyper-parametersareoptimized,chosenfrom[0.05,0.02,0.01,0.005]and[[10−4,10−5,10−6]]respectively.Thesparse-3initializationistestedwithoutusingl2regularization3.WeusedthemomentumschedulesuggestedbyIlyaSutskeveretal.2012[46].AllexperimentsonMNISTused“mini-batches”withabatchcardinalityof200trainingexamples.Allotherhyper-parametersareshownintable8.4.Eachinitializationmethodwasexperimentallyevaluatedonnetworkstructureswithdiﬀerentdepthsusing10diﬀerentrandominitializationseeds.Forthepurposeofcomparison,wealsotestedtheperformanceofsparseandnormalizedinitializationona4-layernetworkwithoutl2regularization.Table8.4:SettingsusedfortheexperimentsHyper-parametersConﬁgurationDataset:MNIST4layersnetwork:[5005002000]5layersnetwork:[200015001000500]6layersnetwork:[1500200015001000500]Activationfunction:tanh(.)Costfunction:NegativeloglikelihoodMomentumschedule:µ=min(1−2−1−log2(bt/250c+1),µmax)Momentummax:µmax=0.999L2weightdecay:[10−4,10−5,10−6]Batchcardinality:200Learningrates:[0.05,0.02,0.01,0.005]TheConvergenceSpeedAdvantageofSparse-3Anobviousadvantagewhentrainingsparse-3initializednetworksisthatthenetworksconvergemuchfasterthanforanyotherinitializationmethod,atleasttwiceasfast.Thediﬀerenceisevenbiggerasthenetworksbecomedeeper.AsyoucanseeinTable8.5,the2underthenullhypothesistestwithp=0.0053l2regularizationcouldhurtsparse-3performance84Chapter8.ADATEExperimentsTable8.5:ValidationerrorratesafterspeciﬁctrainingepochsfordiﬀerentnetworkdepthEpochs1510203040504-layersNormalized8.90%6.44%4.70%3.30%2.62%2.27%2.22%Normalized*8.92%6.38%4.49%3.32%2.53%2.30%2.22%Sparse8.64%4.59%3.38%2.88%2.35%2.22%2.11%Sparse*8.66%4.68%3.43%2.86%2.31%2.24%2.20%Sparse-36.62%3.51%2.79%2.47%2.20%2.14%1.97%5-layersNormalized8.12%4.86%3.45%2.56%2.13%2.06%2.03%Sparse8.24%3.86%3.05%2.63%2.03%2.07%2.05%Sparse-35.76%2.90%2.48%2.10%2.00%1.88%1.83%6-layersNormalized7.89%4.45%3.19%2.53%2.24%2.17%2.04%Sparse8.46%3.59%3.28%2.61%2.04%2.03%2.02%Sparse-35.35%2.78%2.29%1.85%1.78%1.81%1.76%validationerrorratesofsparse-3atthebeginningofthetrainingprocessaremuchlowerthanfortheotherinitializations.Until50epochs,sparse-3hasaclearadvantageoverothermethods.However,asthetrainingprocesscomestoaround100epochs,sparse-3islosingitsdominanceandletothermethod(withhelpfroml2regularization)catchup.Thiscanbeexplainedasfollows.Afterafewdozenepochs,thel2regularizationstartstomakeeﬀectonthenetworkgeneralization,whichleadstobettervalidationerrorrates.Thel2,however,isnotasuitableregularizerforsparse-3andcouldhurtitsperformance.Sparse-3AdvantageRegardingOptimizationandGeneralizationTheexperimentalresultsinTable8.5showtheperformanceofsparse-3,sparse,andnor-malizedinitializationfordiﬀerentneuralnetworkarchitectures.Itisobviousfromthetablethatsparse-3convergesmuchfaster.Forexample,sparse-3reachesavalidationerrorrateof1.85%afteronly20epochsfora6-layerarchitecturewhereasthecloseststate-of-the-artcompetitor,sparse,onlyreaches2.02%after50epochs.Moreover,withouthelpfroml2regularization,sparse-3signiﬁcantlyoutperformssparseandnormalizedinitialization.Tobetterunderstanditsadvantage,wealsocomparedthelearningcurvesofthesemethodswithoutusingl2regularizationandataﬁxed0.05learningrate.Asweallknow,theerrorsurfaceofdeeparchitecturesisverynon-convexandhardtooptimizewithmanylocalminima.AscanbeseeninFigure8.5,thesparse-3learningcurvesfortrainingaswellasvalidationerroraremuchsmootherandconvergefasterthanthoseforsparse*andnormalization*.Thissuggeststhatsparse-3initializationputsusinaregionofparameterspacewhereoptimizationiseasier.Notethataddingal2regularizationtermtothetrainingcostwillmaketheoptimiza-tionprocessbecomeharder(i.e.longer)inreturnforbettergeneralization.Therefore,trainingwithsparse-3initializednetworksisindeedmuchfasterandeasier.Moreover,thisadvantageisalsomagniﬁedwhenthenetworkgetsdeeper.Thispropertycouldbeveryusefulintrainingverydeepneuralnetworkslikeautoencoders,whereunderﬁttingisabigtrouble.Infact,theoriginalsparseinitializationmethodwasinventedbyMartens(2010)toovercomeexactlythatproblem.8.6.OverﬁttingProblem85Table8.6:Averagetesterror(10initializationseeds)forbestvalidationfordiﬀerentnetworkstructuresNetworkStandardNormalizedNormalized*SparseSparse*Sparse-3[500−500−2000]2.17%11.74%1.871.73%1.84%1.73%[2000−1500−1000−500]-21.64%3-1.66%-1.63%[1500−2000−1500−1000−500]-1.68%-1.62%-1.60%*:l2regularizationwasnotused.-:resultsarenotrelevant.1:producedbyErhanet.al.2009[7].Networkstructurewasalsooptimized.2:Erhanet.al.2009statedthattheywereunabletoeﬀectivelytrain5-layermodelsusingstandardinitialization.WhileX.GlorotandY.Bengio(2010)produced1.76%errorrates.3:producedbyX.GlorotandY.Bengio(2010)[9].Thenetworkstructureisunknown.Ourexperimentsshowedslightlyworseresults.Figure8.6alsoshowsthatwithoutl2regularization,sparse-3yieldsbettergeneraliza-tion(validationerrors)thanothermethods.Table8.5alsoconﬁrmsthispoint,atleastduringtheﬁrst50epochs.CombiningFigure8.5andFigure8.6,wecanseethatatthesametrainingcostlevel,sparse-3yieldsalowertestcost.Inthissense,sparse-3appearstobringaneﬀecttothatofaregularizer,probablyduetoitssparsity.Thiscouldalsoexplainwhysparse-3doesnotneedl2regularization.Sparse-3OpimizationWhenlookingatthesparse-3initialization,onewouldprobablyaskifthetwoconstantsinsparse-3alreadyareoptimal.AfterconductingasmallgridsearchontheMNISTdatasetwitha4-layerneuralnetwork,weconcludedthattheseconstantsalreadyarerelativelyoptimal,despitebeingoptimizedfortheTinyDigitsdataset.Thus,theconstantsdonotneedtobechangedwhengoingfromTinyDigitstoMNISTeveniftheneuralnetworksforthelatterareabouttwoordersofmagnitudebigger.Table8.7:Gridsearchforthetwoconstantsinsparse-3Variations−1.435154787360.456463462775-0.2-0.10.00.10.2-0.061.88%1.85%1.87%1.64%1.69%-0.031.98%1.75%1.90%1.82%1.75%0.001.89%1.76%1.68%1.72%1.82%0.031.78%1.98%1.77%1.84%1.76%0.061.88%2.01%1.81%1.88%1.72%8.6OverﬁttingProblemSparse-3initializationwasinventedbyADATEafterarelativelyshortrun.Aftersparse-3,wehavetriedtorunADATEformuchlongertime.AlthoughnewgeneratedprogramsperformverywellonTinyDigitsdataset,theyarealmostuselesswhenappliedonMNIST.Weattemptedtosolvethisproblemusingdiﬀerentsolutions.However,wedidnotknowexactlywhatthemaincauseofoverﬁttingwas.WehadtriedtoincreasethenumberoftinyDigitstrainingdatasets,makethevalidationsetsbigger,andincreasethenumberofepochs...However,theyturnedouttobeaverywrongway.86Chapter8.ADATEExperiments3.3in(a)b5010015020025030000.050.10.150.20.250.3nll−trainingEpochs  5010015020025030000.050.10.150.20.25nll−validationEpochs  sparse3sparse*normalized*sparse3sparse*normalized*Figure8.5:Learningcurvesfornegativeloglikelihoodtrainingcost3.3in(a)b5010015020025030000.050.10.150.20.250.3nll−trainingEpochs  5010015020025030000.050.10.150.20.25nll−validationEpochs  sparse3sparse*normalized*sparse3sparse*normalized*Figure8.6:LearningcurvesonnegativeloglikelihoodvalidationcostFigure8.7:Learningcurvesofsparse3,sparse*,andnormalized*testedon4-layerneuralnetworksusinga0.05learningrate.Eachmethodwasappliedfor10diﬀerentinitializationseeds.Afterthat,wedecidedtomakeamoresystematicexperimentoncheckingwhattherealoverﬁttingsourceis.Thefollowingsweresuspected:architecture,batchsize,learningrate,weightdecay,momentum,anddatasets.Inourﬁrstexperiment,theﬁrstﬁvesuspectedfactorsweretested.WetestedthebestADATE-generatedprogramatthattimeontheTinyDigitsandcomparetheresulttothesparseinitialization.Wechangedthesefactors’valueateachruntoseeiftheADATE-generatedprogramoveﬁtstothesettingsthatusedduringitstrainingprocess.Asshowninﬁgure8.8,theADATE-generatedprogramdoesnotoverﬁttoanyoftheabovefactors.Therefore,weconductedanewexperimenttocheckifitoverﬁtstothetinyDigitsdatasets.NotethatwecancheckthisusingtheoriginalMNISTdatasetdirectly.However,trainingneuralnetworkonMNISTtakesalotoftime,includingoptimizingmanyhyper-parameters,waitingformanyepochs,andtrainingnetworkforatleastﬁvetimestoassuretheresult.Besides,MNISTcontains28x28images,whichisdiﬀerentfromtinyDigits.Therefore,wehavetochangethenetworkstructureandsomeotherhyper-parameterstobeabletotrainonMNIST.Thismakesitimpossibletoknowwherethe8.6.OverﬁttingProblem87overﬁttingcomesfrom:thediﬀerenceindatasetsorthediﬀerenceinhyper-parameters.Therefore,weusedthetinyUSPSandcompressedMNISTdatasetinthisexperiment.Theﬁgure8.9showsthatthedatasetsareobviouslythemainsourceofoverﬁtting,wheretheADATE-generatedprogramoutperformthesparseinitializationontinyDigitsdataset,butbeingmuchworseontheothertinydatasets.ThissuggeststhatweshouldmixtheTinyDigitsdatasetwiththeTinyUSPSorcompressedMNIST100tomakeADATEhardertooverﬁttothetrainingdataset.OptionsADATESparseOptionsADATESparse80 80 2003.20%4.27%103.20%4.33%80 80 802.60%4.27%52.53%3.40%100 100 1003%3.67%153.27%4.27%100 100 2004.47%4.27%203.33%4%100 200 2002.87%3.27%253.53%4.13%200 200 2002.67%4%303.67%4.53%OptionsADATESparseOptionsADATESparse0.053.20%4.20%0.993.20%4.53%0.0253.33%3.73%0.953.20%4.60%0.014.20%4.73%0.93.20%3.73%0.0752.80%4.07%03.53%3.80%0.12.53%3.53%OptionsADATESparse10^-53.20%4.07%10^-43.20%3.73%03.20%4.53%Learning RateWeight DecayMomentumArchitectureBatch sizeFigure8.8:Experimentontestingoverﬁttingsources:Architecture,Batchsize,LearningRate,MomentumandWeightDecay.TheﬁrstvalueforeachfactorwasthevaluethatusedduringtheADATEtrainingprocessRandom SeedsADATESparse3SparseADATESparse3SparseADATESparse3SparseADATESparse3Sparse13.20%3.00%3.67%4.78%4.55%4.62%3.12%2.87%2.77%3.53%3.14%2.87%22.67%3.80%4.00%4.55%4.16%4.16%2.75%2.58%2.71%3.53%2.95%2.88%33.27%4.20%3.93%4.93%4.31%4.24%2.98%2.91%2.75%3.59%3.05%3.11%43.80%3.33%3.60%4.08%4.70%3.85%3.21%2.66%2.61%3.61%3.12%2.86%52.40%4.13%4.07%4.78%4.55%4.24%3.03%2.84%2.37%3.45%2.88%3%Mean3.07%3.69%3.85%4.62%4.45%4.22%3.02%2.77%2.64%3.54%3.03%2.94%ArchitectureBatchsizeMaxEpochsTinyDigitsTinyUSPSCompressedMnist1001010200CompressedMnist3080 - 80 - 20080 - 80 - 20080 - 80 - 20050-50-100200100100100100Figure8.9:ExperimentontestingoverﬁttingondatasetsChapter9ConclusionandFutureWorksThisthesisaimsattwomainpurposes:introducingdeeplearninganditsstate-of-the-artalgorithms,andconductingADATEexperimentstoimprovedeeplearning.Chapter3to6fulﬁltheﬁrstpurposebygivingashortintroductiontodeeplearningandsummarizingmanyrecentimportantdiscoverieswhichleadtothedeeplearning’sﬂour-ishingsuchas:unsupervisedpre-trainingstrategyordiﬀerenttypesofnewoptimization,regularizationmethods,andactivationfunctionsdesignedparticularlyfordeeplearning.ThisknowledgeisextremelyusefulforconductingADATEexperiments,especiallyfordecidingwhichpartofdeeplearningwecouldimprove.Chapter8fulﬁlsthesecondpurposeandanswerstheresearchquestionposedatthebeginningofthethesis:“HowcantheADATEsystemimprovetheperformanceofdeeplearning?”.ThequestionisansweredinaprocesswherewehavesuccesfullydesignedseveraldiﬀerenttinydatasetsforADATE,implementedaneuralnetworkslibraryinSMLlanguage,andsynthesizedabrandnewsparse-3initializationscheme.Despiteitssimplic-ity,ourexperimentresultshaveprovedtheadvantageofsparse-3onclassiﬁcationtaskoverotherexistinginitializationmethods.Thesparse-3candoubletheconvergencespeedofdeeplearning.Thismightsuggestthatdeeplearningisstillanewsubjectwithmanyaspectswestilldonotdeeplyunderstand.Therefore,automaticprogrammingcanhelpusovercomeoursubjectivejudgments,breakourbelief,andcomeupwithstrangebuteﬀectivealgorithms.ThisisonlyourﬁrsttryonusingADATEtoimprovedeeplearningalgorithms,andtherearemanyotherpotentialpossibilities,suchasactivationfunction,objectivefunction,regularizationterm,learningrateschedule,ormomentumformula...Afterthediscoveryofsparse-3,wealsoconductedotherexperimentsbuttheresultswerenotasgood.WedidananalysisforthisandrecognizedthedatasetoverﬁttingproblemwhenusingthetinyDigitsdataset.Othertinydatasetshavebeenbuilttoassesstheoverﬁtting.However,becauseofthetimelimitation,wecouldnotcompletefurtherexperimentswiththesenewdatasetsforthisthesis.Besides,thelimitationoftheADATEcurrentversionthatwecannotcallthesynthesizedprogramfromoutsideoftheADATEpartmakesithardtoexperimentwithotherpartsofdeeplearning.Weexpectthatbasedondeeplearningknowledgeandtheneuralnetworkslibraryprovidedinthisthesis,otherresearcherswhoﬁnditinterestingcanconductfutureexperimentseasily,especiallywhenthenextversionofADATEisavailable.8990Chapter9.ConclusionandFutureWorks9.1FutureWorksForfutureexperiments,wesuggestthesefollowingdevelopmentdirections•ImproveNeuralNetworklibraryperformance:IfwecanmaketheSMLneuralnet-worklibraryrunfaster,wecouldopenupnewpossibilities.WecantrainADATEwithbiggerdatasetsordeeperneuralnetworksinashortertime.WecouldeventrainADATEonreal-worlddataset,whichcanhelpitovercometheoverﬁttingprob-lemongenerateddatasets.OneimprovementthatwecandoﬁrstisinsteadofusingtheSMLmatrixlibrary,wecouldusetheBLAS(BasicLinearAlgebraSubpro-grams)library.BLASisahigh-performancelow-levellinearalgebralibrarywhichsupportsbasiclinearalgebraoperationssuchascopying,vectordotproducts,linearcombinations,andmatrixmultiplication.ThereareseveralBLASimplementationsthatoptimizedforspeciﬁcarchitectures(e.g.Intel,AMD,ARM...).Itisusedasabuilding-blockinalmostanyhigh-performancescientiﬁccomputinglanguagessuchasMATLAB,R,orNumpy.UsingBLASimplementationcouldspeedupmatrixmultiplicationoperation,whichisusedheavilyinneuralnetwork,10x-100xfasterthanusingnormalloopimplementation(dependsonCPUarchitecturesandsizeofthematrices).Currently,weintendtouseOpenBLAS,aBLASimplementationthatisoptimizedfordiﬀerentIntelandAMDarchitectures.Toachievebestperformance,wesuggestcompilingtheOpenBLASlibrarytodynamiclibrary(e.g.*.dllor*.soﬁles)foreachcomputerintheclustertogetitoptimizedfordiﬀerentarchitectures.•Overﬁttingproblem:ThemostseriousproblemthatwehavetodealwithwhenusingADATEtoimprovedeeplearningistheoverﬁtting.Wesuggesttwomainapproaches,whichcouldpossiblyhelpovercomethisproblem.First,wecanimprovethequalityofthetrainingdatasets.Byimprovingperformanceofneuralnetworklibrary,wecantrainwithbiggerdatasets,orevenwithreal-worlddatasets,whichcouldreducetheoverﬁtting.Wecouldalsousesomekindsofautoencoderstocreatecompressedversionofhigh-dimensionalityinput.Thesecondapproachforthisproblemisthatwecanmaketheoverﬁttinghappenaswhatwewant.WecanprovethatbyusingADATE,wecantuneapartofdeeplearningtobeoptimizedforaspeciﬁctaskinanacceptabletime.•StateoftheartAlgorithms:Despitebeingaverynewresearcharea,thedeeplearn-ingliteratureisdevelopingatextremespeedanddominatingallothermethodsinmachinelearning,especiallyforhigh-levelabstractiontasks.Supportedbyanexpandingandveryactiveresearchcommunity,therearenewdeeplearningalgo-rithmsandtechnologiesintroducedeveryyear.Therefore,tomaximizetheabilityofADATEonimprovingdeeplearning,weneedtokeepupwithnewtechnologiesindeeplearning.Thebestwaytodothisistouseastandarddeeplearninglibrary,whichisupdatedfrequentlywithallnewcentralmethods.WesuggestPylearn2forthispurpose.ItisamachinelearninglibrarywritteninPythonanddevelopedbythewell-knownLISAlab(UniversityofMontreal).Itconsistsofmanystate-of-the-artdeeplearningalgorithms,andsupportedbyoneofthemostactivedeeplearningresearchlab.Bibliography[1]YoshuaBengio.Learningdeeparchitecturesforai.FoundationsandtrendsR(cid:13)inMachineLearning,2(1):1–127,2009.[2]YoshuaBengioandOlivierDelalleau.Justifyingandgeneralizingcontrastivediver-gence.NeuralComputation,21(6):1601–1621,2009.[3]YoshuaBengioandPascalLamblin.Greedylayer-wisetrainingofdeepnetworks.Advancesinneural...,(1),2007.[4]JamesBergstra,GuillaumeDesjardins,PascalLamblin,andYoshuaBengio.Quadraticpolynomialslearnbetterimagefeatures.Technicalreport,TechnicalRe-port1337,Departementd’InformatiqueetdeRechercheOperationnelle,UniversitedeMontreal,2009.[5]DanClaudiuCiresan,UeliMeier,LucaMariaGambardella,andJuergenSchmid-huber.Deepbigsimpleneuralnetsexcelonhandwrittendigitrecognition.arXivpreprintarXiv:1003.0358,2010.[6]BBLeCunandJSDenker.Handwrittendigitrecognitionwithaback-propagationnetwork.Advancesinneural...,1990.[7]DumitruErhan,Pierre-AntoineManzagol,YoshuaBengio,SamyBengio,andPascalVincent.Thediﬃcultyoftrainingdeeparchitecturesandtheeﬀectofunsupervisedpre-training.InInternationalConferenceonArtiﬁcialIntelligenceandStatistics,pages153–160,2009.[8]SEFahlmanandCLebiere.Thecascade-correlationlearningarchitecture.1989.[9]XGlorotandYBengio.Understandingthediﬃcultyoftrainingdeepfeedforwardneuralnetworks.InternationalConferenceon...,pages1–8,2010.[10]XavierGlorot,AntoineBordes,andYoshuaBengio.Deepsparserectiﬁernetworks.15:315–323,2011.[11]IanJGoodfellow,DavidWarde-Farley,MehdiMirza,AaronCourville,andYoshuaBengio.Maxoutnetworks.arXivpreprintarXiv:1302.4389,2013.[12]AlexGraves,MarcusLiwicki,SantiagoFern´andez,RomanBertolami,HorstBunke,andJ¨urgenSchmidhuber.Anovelconnectionistsystemforunconstrainedhandwrit-ingrecognition.IEEEtransactionsonpatternanalysisandmachineintelligence,31(5):855–68,May2009.9192BIBLIOGRAPHY[13]Stig-ErlandHansenandRolandOlsson.Improvingdecisiontreepruningthroughautomaticprogramming.pages31–40,2007.[14]SimonHaykin.NeuralNetworksandLearningMachines.PearsonInternationalEdi-tion,3rdedition,2009.[15]Hinton.NeuralNetworksforMachineLearningcourse,2013.[16]GEHintonandRRSalakhutdinov.Reducingthedimensionalityofdatawithneuralnetworks.Science(NewYork,N.Y.),313(5786):504–7,July2006.[17]GeoﬀreyHinton,LiDeng,DongYu,GeorgeDahl,Abdel-rahmanMohamed,NavdeepJaitly,VincentVanhoucke,PatrickNguyen,TaraSainath,andBrianKingsbury.DeepNeuralNetworksforAcousticModelinginSpeechRecognition.pages1–27,2012.[18]GeoﬀreyEHinton,SimonOsindero,andYee-WhyeTeh.Afastlearningalgorithmfordeepbeliefnets.Neuralcomputation,18(7):1527–1554,2006.[19]GeoﬀreyEHinton,NitishSrivastava,AlexKrizhevsky,IlyaSutskever,andRuslanRSalakhutdinov.Improvingneuralnetworksbypreventingco-adaptationoffeaturedetectors.arXivpreprintarXiv:1207.0580,2012.[20]SugiharaHiroshi.WhatisOccam’sRazor?1996.[21]MorganJakobsenandRolandOlsson.Generationofafour-wayitem-to-itemnaviga-tionalgorithmusingautomaticprogramming.pages1310–1315,2006.[22]AKrizhevsky.Convolutionaldeepbeliefnetworksoncifar-10.Unpublishedmanuscript,pages1–9,2010.[23]AlexKrizhevsky,IlyaSutskever,andGeoﬀHinton.Imagenetclassiﬁcationwithdeepconvolutionalneuralnetworks.pages1106–1114,2012.[24]YLeCunandYBengio.Convolutionalnetworksforimages,speech,andtimeseries....handbookofbraintheoryandneuralnetworks,pages1–14,1995.[25]YannLeCun,L´eonBottou,GenevieveBOrr,andKlaus-RobertM¨uller.Eﬃcientbackprop.InNeuralnetworks:Tricksofthetrade,pages9–50.Springer,1998.[26]JamesMartens.Deeplearningviahessian-freeoptimization.pages735–742,2010.[27]TomMMitchell.MachineLearning.McGrawHill,1997.[28]AndrewNg.Cs294alecturenotes:Sparseautoencoder,2010.[29]AndrewNg.Deeplearning,self-taughtlearningandunsupervisedfeaturelearning,2012.[30]AndrewNg.MachineLearningcourse,2012.[31]J.R.Olsson.TheADATESystemhomepage,1994.[32]JRolandOlsson.Theartofwritingspeciﬁcationsfortheadateautomaticprogram-mingsystem.InProceedingsofthe3rdAnnualConferenceonGeneticProgrammin,pages278–283.Citeseer,1998.BIBLIOGRAPHY93[33]RolandOlsson.Inductivefunctionalprogrammingusingincrementalprogramtrans-formation;and,Executionoflogicprogramsbyiterative-deepeningA*SLD-treesearch.PhDthesis,UniversityofOslo,DepartmentofInformatics,1994.[34]RolandOlsson.Inductivefunctionalprogrammingusingincrementalprogramtrans-formation.Artiﬁcialintelligence,74(1):55–81,1995.[35]RolandOlsson.Inductivefunctionalprogrammingusingincrementalprogramtrans-formation.ArtiﬁcialIntelligence,74(1):55–81,1995.[36]RolandOlssonandArneLøkketangen.Usingautomaticprogrammingtogeneratestate-of-the-artalgorithmsforrandom3-sat.JournalofHeuristics,19(5):819–844,2013.[37]Jin-SongPei,EricCMai,andJosephPWright.Mappingsomefunctionsandfourarithmeticoperationstomultilayerfeedforwardneuralnetworks.pages693512–693512,2008.[38]MartinRiedmillerandIRprop.Rprop-descriptionandimplementationdetails.(Jan-uary),1994.[39]SalahRifai,PascalVincent,XavierMuller,XavierGlorot,andYoshuaBengio.Con-tractiveauto-encoders:Explicitinvarianceduringfeatureextraction.pages833–840,2011.[40]RuslanSalakhutdinovandGeoﬀreyHinton.Deepboltzmannmachines.International...,(3):448–455,2009.[41]RuslanSalakhutdinovandGeoﬀreyHinton.AneﬃcientlearningprocedurefordeepBoltzmannmachines.NeuralComputation,2012.[42]ThomasSerre,GabrielKreiman,MinjoonKouh,CharlesCadieu,UlfKnoblich,andTomasoPoggio.Aquantitativetheoryofimmediatevisualrecognition.Progressinbrainresearch,165(06):33–56,January2007.[43]Zhen-JunShi.Convergenceoflinesearchmethodsforunconstrainedoptimization.AppliedMathematicsandComputation,157(2):393–405,October2004.[44]PatriceYSimard,DaveSteinkraus,andJohnCPlatt.BestPracticesforConvolu-tionalNeuralNetworksAppliedtoVisualDocumentAnalysis.(Icdar):1–6,2003.[45]ISutskever.TrainingRecurrentNeuralNetworks.2013.[46]IlyaSutskever,JamesMartens,GeorgeDahl,andGeoﬀreyHinton.Ontheimportanceofinitializationandmomentumindeeplearning.cs.utoronto.ca,(2010),2012.[47]GeirVattekar.Adateusermanual.Technicalreport,Technicalreport,OstfoldUni-versityCollege,2006.[48]PascalVincent,HugoLarochelle,YoshuaBengio,andPierre-AntoineManzagol.Ex-tractingandcomposingrobustfeatureswithdenoisingautoencoders.pages1096–1103,2008.94BIBLIOGRAPHY[49]CarlvonLinn´eandJohannJoachimLange.SystemaNaturae:PerRegnaTriaNat-urae,SecundumClasses,Ordines,Genera,Species,CumCharacteribus,Diﬀerentiis,Synonymis,Locis,volume3.Curt,1770.[50]LWanandMatthewZeiler.RegularizationofNeuralNetworksusingDropConnect.Proceedingsofthe...,(1),2013.[51]IanH.Witten,EibeFrank,andMarkA.Hall.DataMining-PracticalMachineLearningToolsandTechniques.ElsevierInc.,3rdedition,2011.AppendixANeuralNetworkLibrariesA.1MatrixLibraryA.1.1MatrixSignature(*2File:matrix.smlContent:matrixlibraryusingListofListtype4Author:DangHaTheHien,Hiof,hdthe@hiof.noConventionforvariablenames:6matrix:m,m1,m2...vector:v,v1,v2...8listofvectors:vs,v1s,v2s...10*)signatureMATRIX=12sigtype’avector=’alist14type’amatrixexceptionWrongMatrixType16exceptionUnmatchedDimension(*basicfunctionsfordatatypematrix*)18valchangeType:’amatrix−>’amatrixvalcheckValid:’amatrix−>bool20valtranspose:’amatrix−>’amatrixvaltransposechangeType:’amatrix−>’amatrix22valtoVector:’amatrix−>’alistvalsize:’amatrix−>int∗int24(*functionstocreatenewmatrix*)valinitCols:’a∗(int∗int)−>’amatrix26valinitRows:’a∗(int∗int)−>’amatrixvaluniRandRows:Random.rand−>real∗(int∗int)−>realmatrix28valuniRandCols:Random.rand−>real∗(int∗int)−>realmatrixvalfromVector2Cols:’alist∗(int∗int)−>’amatrix30valfromVector2Rows:’alist∗(int∗int)−>’amatrixvalfromVectors2Cols:’alistlist∗(int∗int)−>’amatrix32valfromVectors2Rows:’alistlist∗(int∗int)−>’amatrix(*functionsfordebuging*)34valprintMatrixReal:realmatrix−>unitvalprintMatrixInt:intmatrix−>unit36(*functionsforoutput*)valprintfMatrixReal:string∗realmatrix−>unit9596ChapterA.NeuralNetworkLibraries38valprintfMatrixInt:string∗intmatrix−>unit(*supportedoperatorsonvector*)40valmergeVector:((’a∗’a)−>’b)−>(’avector∗’avector)−>’bvectorvalmergeVect2Matrix:((’a∗’a)−>’b)−>(’avector∗’amatrix)42−>’bmatrixvalmergeVect’2Matrix:((’a∗’a)−>’b)−>(’avector∗’amatrix)44−>’bmatrixvaladdVect2MatrixReal:reallist∗realmatrix−>realmatrix46valaddVect2MatrixInt:intlist∗intmatrix−>intmatrix(*supportedoperatorsonmatrix*)48(*scalaroperator*)valmap:(’a−>’b)−>’amatrix−>’bmatrix50valaddScalarInt:intmatrix∗int−>intmatrixvalmulScalarInt:intmatrix∗int−>intmatrix52valaddScalarReal:realmatrix∗real−>realmatrixvalmulScalarReal:realmatrix∗real−>realmatrix54(*accumulateoperator*)valfoldl:((’a∗’b)−>’b)−>’b−>(’amatrix)−>’bvector56valsumInt:(intmatrix)−>intvectorvalsumReal:(realmatrix)−>realvector58(*matrixoperator*)valmerge:((’a∗’b)−>’c)−>(’amatrix∗’bmatrix)60−>’cmatrixvaldotMulMatrixInt:intmatrix∗intmatrix−>intmatrix62valdotMulMatrixReal:realmatrix∗realmatrix−>realmatrixvaladdMatrixInt:intmatrix∗intmatrix−>intmatrix64valaddMatrixReal:realmatrix∗realmatrix−>realmatrix(*realmultiplyoperation*)66valmulMatrixIntR:intmatrix∗intmatrix−>intmatrixvalmulMatrixIntC:intmatrix∗intmatrix−>intmatrix68valmulMatrixRealR:realmatrix∗realmatrix−>realmatrixvalmulMatrixRealC:realmatrix∗realmatrix−>realmatrix70endA.1.2MatrixStructurestructureMatrix:>MATRIX=2structtype’avector=’alist4datatype’amatrix=COLMATRIXof(’avectorlist∗int∗int)|ROWMATRIXof(’avectorlist∗int∗int)6exceptionWrongMatrixTypeexceptionUnmatchedDimension8funchangeVectorsvs=let10fungetHeadsvs=casevsof12[]=>([],[])|v::vs’=>casevof14[]=>([],[])|hdv::tlv=>16casegetHeads(vs’)of(hdvs,tlvs)=>(hdv::hdvs,tlv::tlvs)18incasegetHeads(vs)of20([],)=>[]|(hdvs,tlvs)=>hdvs::changeVectors(tlvs)A.1.MatrixLibrary9722endfunchangeType(COLMATRIX(vs,rows,cols))=24ROWMATRIX(changeVectors(vs),rows,cols)|changeType(ROWMATRIX(vs,rows,cols))=26COLMATRIX(changeVectors(vs),rows,cols)funtransposem=28casemofCOLMATRIX(vs,rows,cols)=>changeType(ROWMATRIX(vs,cols,rows))30|ROWMATRIX(vs,rows,cols)=>changeType(COLMATRIX(vs,cols,rows))32funtransposechangeTypem=casemof34COLMATRIX(vs,rows,cols)=>ROWMATRIX(vs,cols,rows)|ROWMATRIX(vs,rows,cols)=>COLMATRIX(vs,cols,rows)36funfromVectors2List[]=[]|fromVectors2List(v::vs)=v@fromVectors2List(vs)38funtoVectorm=40casemofROWMATRIX(vs,rows,cols)=>fromVectors2List(vs)42|COLMATRIX(vs,rows,cols)=>fromVectors2List(changeVectors(vs))44funsizeVectorsvs=let46valnElems=List.foldl(fn(v,lengthv)=>case(lengthv,(List.length(v)=lengthv))of48(0,)=>List.length(v)|(,true)=>List.length(v)50|(,false)=>˜1)0vs52valnVectors=List.length(vs)in54(nElems,nVectors)end56funcheckValid(COLMATRIX(vs,rows,cols))=58letval(nElems,nVectors)=sizeVectors(vs)in60if(nElems=rows)andalso(nVectors=cols)thentrueelsefalseend62|checkValid(ROWMATRIX(vs,rows,cols))=letval(nElems,nVectors)=sizeVectors(vs)64inif(nElems=cols)andalso(nVectors=rows)thentrueelsefalseend66funinitVector(value,n,acc)=68casenof0=>acc70|n=>initVector(value,n−1,value::acc)funinitVectors(value,rows,cols)=72letfunreduce(cols,acc)=74casecolsof0=>acc76|cols=>reduce(cols−1,initVector(value,rows,[])::acc)78inreduce(cols,[])98ChapterA.NeuralNetworkLibraries80endfuninitCols(value,(rows,cols))=COLMATRIX(initVectors(value,rows,cols),82rows,cols)funinitRows(value,(rows,cols))=transposechangeType(initCols(value,84(cols,rows)))86fununiRandVectorsRandState(nVectors,length,max)=let88fununiRandList(n)=ifn=0then[]90else(max∗(2.0∗(Random.randRealRandState)−1.0))::uniRandList(n−1)fununiRandVectors(nVectors)=92ifnVectors=0then[]elseuniRandList(length)::uniRandVectors(nVectors−1)94inuniRandVectors(nVectors)96end98fununiRandRowsRandState(max,(rows,cols))=ROWMATRIX(uniRandVectorsRandState(rows,cols,max),rows,cols)100fununiRandColsRandState(max,(rows,cols))=COLMATRIX(uniRandVectorsRandState(cols,rows,max),rows,cols)102funprintInfom=104casemofCOLMATRIX(,rows,cols)=>106print("Collummatrix"ˆInt.toString(rows)ˆ"*"ˆInt.toString(cols)ˆ"\n")108|ROWMATRIX(,rows,cols)=>print("Rowmatrix"ˆInt.toString(rows)ˆ"*"110ˆInt.toString(cols)ˆ"\n")funprintMatrixprintElemprintEndm=112letfunprintVectorv=114casevof[]=>()116|last::[]=>printEndlast|head::v’=>(printElemhead;printVectorv’)118incasemof120ROWMATRIX(vs,rows,cols)=>(printInfom;List.appprintVectorvs)122|COLMATRIX(vs,rows,cols)=>(printInfom;List.appprintVector(changeVectorsvs))124end126funreal2strx=ifx>=0.0thenReal.toString(x)128else"-"ˆReal.toString(˜x)funint2strx=130ifx>=0thenInt.toString(x)else"-"ˆInt.toString(˜x)132valprintMatrixReal=printMatrix(fna=>print(real2str(a)ˆ","))134(fna=>print(real2str(a)ˆ"\n"))valprintMatrixInt=printMatrix(fna=>print(int2str(a)ˆ","))A.1.MatrixLibrary99136(fna=>print(int2str(a)ˆ"\n"))138funprintfMatrixReal(fname,m)=letvalfout=TextIO.openOut(fname)140in((printMatrix(fna=>TextIO.output(fout,(real2str(a)ˆ",")))142(fna=>TextIO.output(fout,(real2str(a)ˆ"\n")))m);144TextIO.flushOut(fout);TextIO.closeOut(fout))146end148funprintfMatrixInt(fname,m)=letvalfout=TextIO.openOut(fname)150in((printMatrix(fna=>TextIO.output(fout,(int2str(a)ˆ",")))152(fna=>TextIO.output(fout,(int2str(a)ˆ"\n")))m);154TextIO.flushOut(fout);TextIO.closeOut(fout))156end158funsize(COLMATRIX(,rows,cols))=(rows,cols)|size(ROWMATRIX(,rows,cols))=(rows,cols)160funfromList2Vectors(a,nElems,nVectors)=162letfunfromList2Vector(a,nElems)=164case(a,nElems)of(,0)=>(a,[])166|([],nElems)=>raiseUnmatchedDimension|(x::a’,nElems)=>casefromList2Vector(a’,nElems−1)of168(a’’,acc)=>(a’’,x::acc)in170casenVectorsof0=>[]172|nVectors=>casefromList2Vector(a,nElems)of(a’,v)=>v::fromList2Vectors(a’,nElems,nVectors−1)174end176funfromVector2Rows(a,(rows,cols))=ifList.length(a)<>rows∗cols178thenraiseUnmatchedDimensionelseROWMATRIX(fromList2Vectors(a,cols,rows),rows,cols)180funfromVector2Cols(a,(rows,cols))=transposechangeType(fromVector2Rows(a,(cols,rows)))182funfromVectors2Cols(vs,(rows,cols))=184ifcheckValid(COLMATRIX(vs,rows,cols))thenCOLMATRIX(vs,rows,cols)elseraiseUnmatchedDimension186funfromVectors2Rows(vs,(rows,cols))=ifcheckValid(ROWMATRIX(vs,rows,cols))thenROWMATRIX(vs,rows,cols)188elseraiseUnmatchedDimension190funmapVectorsfvs=List.map(fnv=>List.mapfv)vs192funmapf(COLMATRIX(vs,rows,cols))=COLMATRIX(mapVectorsfvs,rows,cols)100ChapterA.NeuralNetworkLibraries|mapf(ROWMATRIX(vs,rows,cols))=ROWMATRIX(mapVectorsfvs,rows,cols)194funaddScalarInt(m,x:int)=map(fna=>a+x)m196funaddScalarReal(m,x:real)=map(fna=>a+x)mfunmulScalarInt(m,x:int)=map(fna=>a∗x)m198funmulScalarReal(m,x:real)=map(fna=>a∗x)m200funmergeVectorf(v1,v2)=case(v1,v2)of202([],[])=>[]|(hdv1::v1’,hdv2::v2’)=>f(hdv1,hdv2)::(mergeVectorf(v1’,v2’))204|=>raiseUnmatchedDimension206funmergeVectorsfvs1vs2=mergeVector(fn(v1,v2)=>mergeVectorf(v1,v2))(vs1,vs2)208funmergef(m1,m2)=210case(m1,m2)of(COLMATRIX(vs1,rows1,cols1),COLMATRIX(vs2,rows2,cols2))=>212COLMATRIX(mergeVectorsfvs1vs2,rows1,cols1)|(ROWMATRIX(vs1,rows1,cols1),ROWMATRIX(vs2,rows2,cols2))=>214ROWMATRIX(mergeVectorsfvs1vs2,rows1,cols1)|=>raiseWrongMatrixType216218valdotMulMatrixInt=merge(fn(a:int,b)=>a∗b)valdotMulMatrixReal=merge(fn(a:real,b)=>a∗b)220valaddMatrixInt=merge(fn(a:int,b)=>a+b)valaddMatrixReal=merge(fn(a:real,b)=>a+b)222funmergeVect2Vectsf(v1,v2s)=224casev2sof[]=>[]226|v2::v2s’=>(mergeVectorf(v1,v2))::(mergeVect2Vectsf(v1,v2s’))228funmergeVect2Matrixf(vx,m)=casemof230COLMATRIX(vs,rows,cols)=>COLMATRIX(mergeVect2Vectsf(vx,vs),rows,cols)|ROWMATRIX(vs,rows,cols)=>ROWMATRIX(mergeVect2Vectsf(vx,vs),rows,cols)232valaddVect2MatrixReal=mergeVect2Matrix(fn(v1:real,v2)=>v1+v2)valaddVect2MatrixInt=mergeVect2Matrix(fn(v1:int,v2)=>v1+v2)234funmergeVect’2Vectsf(v1,v2s)=236letfuncurfab=f(a,b)238incase(v1,v2s)of240([],[])=>[]|(x::v1’,v2::v2s’)=>(List.map(curfx)v2)::(mergeVect’2Vectsf(v1’,v2s’))242|=>raiseUnmatchedDimensionend244funmergeVect’2Matrixf(vx,m)=246casemofA.1.MatrixLibrary101COLMATRIX(vs,rows,cols)=>COLMATRIX(mergeVect’2Vectsf(vx,vs),rows,cols)248|ROWMATRIX(vs,rows,cols)=>ROWMATRIX(mergeVect’2Vectsf(vx,vs),rows,cols)250funfoldlfaccm=letfunfoldlVectors(vs)=252casevsof[]=>[]254|v::vs’=>(List.foldlfaccv)::foldlVectors(vs’)in256casemofCOLMATRIX(vs,rows,cols)=>foldlVectors(vs)258|ROWMATRIX(vs,rows,cols)=>foldlVectors(vs)end260valsumInt=foldl(fn(x,acc:int)=>acc+x)0valsumReal=foldl(fn(x,acc:real)=>acc+x)0.0262funfoldl2Vectorsfacc(v1,v2)=264letfunfoldl(v1,v2,acc)=case(v1,v2)of266(hdv1::v1’,hdv2::v2’)=>foldl(v1’,v2’,f(hdv1,hdv2,acc))|([],[])=>acc268|=>raiseUnmatchedDimensionin270foldl(v1,v2,acc)end272funmulVectorsmul2VectorsFun(v1s,v2s)=274letfunmulVectorsVector(v1s,v2)=276casev1sof[]=>[]278|v1::v1s’=>(mul2VectorsFun(v1,v2))::(mulVectorsVector(v1s’,v2))280incasev2sof282[]=>[]|v2::v2s’=>(mulVectorsVector(v1s,v2))::(mulVectorsmul2VectorsFun(v1s,v2s’))284endvalmul2VectorsInt=foldl2Vectors(fn(x,y,acc:int)=>acc+x∗y)0286valmul2VectorsReal=foldl2Vectors(fn(x,y,acc:real)=>acc+x∗y)0.0valmulVectorsInt=mulVectorsmul2VectorsInt288valmulVectorsReal=mulVectorsmul2VectorsReal290funmulMatrixmulVectorsFunreturnRow((ROWMATRIX(vs1,rows1,cols1)),(COLMATRIX(vs2,rows2,cols2)))=292ifcols1<>rows2thenraiseUnmatchedDimension294elseifreturnRowthen(ROWMATRIX(mulVectorsFun(vs2,vs1),rows1,cols2))296else(COLMATRIX(mulVectorsFun(vs1,vs2),rows1,cols2))298|mulMatrixmulVectorsFunreturnRow(,)=raiseWrongMatrixTypevalmulMatrixIntC=mulMatrixmulVectorsIntfalse300valmulMatrixIntR=mulMatrixmulVectorsInttruevalmulMatrixRealC=mulMatrixmulVectorsRealfalse102ChapterA.NeuralNetworkLibraries302valmulMatrixRealR=mulMatrixmulVectorsRealtrueendA.2RandomlibraryA.2.1RandomSignature1signatureRANDOMEXT=sig3valrandnormal:Random.rand−>real∗real−>realvalrandperm:Random.rand−>int∗int−>intlist5endA.2.2RandomStructure1valcachedrandnormal=ref0.0;valranduselast=reffalse;3funrandnormalRandState(mean,std)=let5funboxmuller()=let7valx1=2.0∗(Random.randRealRandState)−1.0valx2=2.0∗(Random.randRealRandState)−1.09valw=x1∗x1+x2∗x2in11if(w<1.0)thenlet13valv=Math.sqrt((˜2.0∗Math.ln(w))/w)in15(randuselast:=true;cachedrandnormal:=x2∗v;17mean+x1∗v∗std)end19elseboxmuller()21endin23case!randuselastoftrue=>(randuselast:=false;mean+!cachedrandnormal∗std)25|false=>boxmuller()end27funrandpermRandState(m,n)=29letfunfisheryates(a,i,n,m)=31if(i=norelsei=m)thenaelse33letvalrandj=i+Real.floor((Random.randRealRandState)35∗Real.fromInt(n−i))valj=if(randj=n)thenn−1elserandj37valtmp=Array.sub(a,j)in39(Array.update(a,j,Array.sub(a,i));A.3.NeuralNetworkLibrary103Array.update(a,i,tmp);41fisheryates(a,i+1,n,m))end43funseqArray(a,i,n)=ifi=nthena45else(Array.update(a,i,i+1);seqArray(a,i+1,n))47funarrayToList(a,i,l)=if((i=Array.length(a))orelse(l=0))then[]49elseArray.sub(a,i)::arrayToList(a,i+1,l−1)valarrayperm=fisheryates(seqArray(Array.array(n,0),0,n)51,0,n,m)vallistperm=arrayToList(arrayperm,0,m)53inListMergeSort.sort(fn(a,b)=>a>b)listperm55endA.3NeuralNetworkLibraryA.3.1NeuralNetworkSignature1signatureNN=sig3type’amatrix=’aMatrix.matrixtype’avector=’aMatrix.vector5datatypecostType=NLL|MSE|CE|PERdatatypeinitType=SPARSE|NORMAL|NORMALISED7datatypeactType=SIGM|TANH|LINEARdatatypewdType=L0|L1|L29typennlayer={input:realMatrix.matrix,(*ROWMATRIX*)output:realMatrix.matrix,(*ROWMATRIX*)11GradW:realMatrix.matrix,(*COLMATRIX*)GradB:realMatrix.vector,13B:realMatrix.vector,W:realMatrix.matrix,(*COLMATRIX*)15actType:actType}17typeparams={batchsize:int,(*numberoftrainingcasesperbatch*)nBatches:int,(*numberofbatches*)19testsize:int,(*numberoftestingcases*)lambda:real,(*momentumcoefficient*)21momentumSchedule:bool,maxLambda:real,23lr:real,(*learningrate*)costType:costType,(*costfunctiontype*)25initType:initType,(*initializationtype*)actType:actType,(*activationfunctiontype*)27layerSizes:intlist,(*structureofnetwork*)initWs:realMatrix.matrixoptionlist,29(*preinitializedWsmatrices*)initBs:realMatrix.vectoroptionlist,31(*preinitializedBsmatrices*)nItrs:int,(*numberofiterations/epoches*)33wdType:wdType,wdValue:real,35verbose:bool104ChapterA.NeuralNetworkLibraries}37typefileNames={datatrain:string,labelstrain:string,39datatest:string,labelstest:string41}43exceptionInputErrorexceptionNotSupported45valsetParams:params−>unit47valrun:Random.rand−>params∗fileNames−>nnlayerlist∗realvalreadData:string∗int∗int∗int−>realmatrixlist49valtrainNN:nnlayerlist∗realmatrixlist∗realmatrixlist−>nnlayerlist51valtrainBest:nnlayerlist∗realmatrixlist∗realmatrixlist∗realmatrixlist∗realmatrixlist−>real∗nnlayerlist53valupdate:nnlayerlist−>nnlayerlistvalupdate1layer:nnlayer−>nnlayer55valcomputeCost:realmatrix∗realmatrix∗costType−>real∗realmatrix57valbprop:nnlayerlist∗realmatrix−>nnlayerlist∗realmatrix59valbprop1layer:nnlayer∗realmatrix−>nnlayer∗realmatrix61valfprop:nnlayerlist∗realmatrix−>nnlayerlist∗realmatrix63valfprop1layer:nnlayer∗realmatrix−>nnlayer∗realmatrix65valinitLayers:Random.rand−>intlist∗realmatrixoptionlist∗realvectoroptionlist−>nnlayerlist67valinitLayer:Random.rand−>int∗int∗actType∗realmatrixoption∗realvectoroption−>nnlayer69endA.3.2NeuralNetworkStructure1structureNN:>NN=struct3type’amatrix=’aMatrix.matrixtype’avector=’aMatrix.vector5exceptionInputErrorexceptionNotSupported7(*nnlayer:neuralnetworklayertypeaneuralnetworkisalistofnnlayervariables.9eachlayercontainsneccesaryinformationfortrainingandpredictingprocess11*)(*costType:supportedcostfunctiontype13-Negativeloglikelyhood:onlyusewhenyou’reusingoneHotlabels.-Meansquareerror15-Crossentropy-Errorpercentage-forbenchmarkonly17*)datatypecostType=NLL|MSE|CE|PER19(*initType:supportedinitializationmethods-SparseA.3.NeuralNetworkLibrary10521-Normal-Normalized23*)datatypeinitType=SPARSE|NORMAL|NORMALISED25(*actType:supportedactivationfunction-Sigmoid27-Tanh*)29datatypeactType=SIGM|TANH|LINEAR(*wdType:supportedweightdecaytype31-L0:noweightdecay-L1:L1weightdecay33-L2:L2weightdecay*)35datatypewdType=L0|L1|L2typennlayer={input:realmatrix,(*ROWMATRIX*)37output:realmatrix,(*ROWMATRIX*)GradW:realmatrix,(*COLMATRIX*)39GradB:realvector,B:realvector,41W:realmatrix,(*COLMATRIX*)actType:actType43}typeparams={batchsize:int,(*numberoftrainingcasesperbatch*)45nBatches:int,(*numberofbatches*)testsize:int,(*numberoftestingcases*)47lambda:real,(*momentumcoefficient*)momentumSchedule:bool,49maxLambda:real,lr:real,(*learningrate*)51costType:costType,(*costfunctiontype*)initType:initType,(*initializationtype*)53actType:actType,(*activationfunctiontype*)layerSizes:intlist,(*structureofnetwork*)55initWs:realmatrixoptionlist,(*preinitializedWsmatrices*)initBs:realvectoroptionlist,(*preinitializedBsmatrices*)57nItrs:int,(*numberofiterations/epoches*)wdType:wdType,59wdValue:real,verbose:bool61}valparams:paramsref=63ref{batchsize=10,(*numberoftrainingcasesperbatch*)nBatches=12,(*numberofbatches*)65testsize=30,(*numberoftestingcases*)lambda=0.0,(*momentumcoefficient*)67momentumSchedule=false,maxLambda=0.0,69lr=0.005,(*learningrate*)costType=MSE,(*costfunctiontype*)71initType=NORMAL,(*initializationtype*)actType=SIGM,(*activationfunctiontype*)73layerSizes=[4,8,3],(*structureofnetwork*)nItrs=40,(*numberofiterations/epoches*)75initWs=[],initBs=[],77wdType=L0,wdValue=0.0,106ChapterA.NeuralNetworkLibraries79verbose=false};81typefileNames={datatrain:string,labelstrain:string,83datatest:string,labelstest:string85}(*allinputfilenames*)87valfileNames:fileNames={datatrain="data_train.csv",89labelstrain="labels_train.csv",datatest="data_test.csv",91labelstest="labels_test.csv"}9395(*--------------------Functionsforreadinginputs--------------*)(*parseLine:parselineofstring97input-line:stringofnumbersseperatedbycommas99output-listofrealnumbers101*)funsetParams(p:params)=103params:=pfunparseLine(line)=105letfungetFirstNumnil=nil107|getFirstNum(x::xs)=ifx=#","then[]elsex::getFirstNum(xs);in109caselineof[]=>[]111|#","::line’=>parseLine(line’)|#""::line’=>parseLine(line’)113|=>let115valnumStr=implode(getFirstNum(line))valnum=valOf(Real.fromString(numStr))117innum::parseLine(List.drop(line,size(numStr)))end119end(*readData:readinputdata121input-nAtts:EachlinehasnAttsnumbersseperatedbycommas123-batchsize:numberoflinestocreateamatrix-nBatches:totalnumberofbatches125output-listofbatches(eachbatchisaROWMATRIX)127*)funreadData(fileName,nAtts,batchsize,nBatches)=129letfunreadLines(fh,nlines)=131case(TextIO.endOfStreamfh,nlines=0)of(,true)=>[]133|(false,false)=>(parseLine(explode(valOf(TextIO.inputLinefh))))135::readLines(fh,nlines−1)|(true,false)=>raiseInputErrorA.3.NeuralNetworkLibrary107137funreadBatches(fh,nBatches)=ifnBatches>0139thenMatrix.fromVectors2Rows(readLines(fh,batchsize),(batchsize,nAtts))141::readBatches(fh,nBatches−1)else(TextIO.closeInfh;[])143inreadBatches(TextIO.openInfileName,nBatches)145end(*-------------------Functionsfortrainingprocess--------------*)147funinitSparseWRandState(nInputs,nOutputs)=149letvalnconn=4;151funinitaNode(nInputs)=let153funinitSparseList(ids,i,n)=case(ids,i<=n)of155(,false)=>[]|([],true)=>0.0::initSparseList(ids,i+1,n)157|(idx::ids’,true)=>if(idx=i)then159Randomext.randnormalRandState(0.0,1.0)::initSparseList(ids’,i+1,n)161else0.0::initSparseList(ids,i+1,n)163ininitSparseList(Randomext.randpermRandState(nconn,nInputs)165,1,nInputs)end167funinitSparseVects(nInputs,nOutputs)=if(nOutputs>0)then169initaNode(nInputs)::initSparseVects(nInputs,nOutputs−1)else[]171inMatrix.fromVectors2Cols(initSparseVects(nInputs,nOutputs),173(nInputs,nOutputs))end175(*initLayer:initializeaneurralnetworklayer177input:-nInputs:numberofincommingnodes179-nOutputs:numberofoutgoingnodesoutput:181-ainitializedlayer*)183funinitLayerRandState(nInputs,nOutputs,actType,initW,initB):nnlayer=let185valinput=Matrix.initRows(0.0,(1,1))valoutput=Matrix.initRows(0.0,(1,1))187valGradW=Matrix.initCols(0.0,(nInputs,nOutputs))valGradB=Matrix.toVector(Matrix.initRows(0.0,(nOutputs,1)))189valB=caseinitBofNONE=>Matrix.toVector(Matrix.initRows(0.0,(nOutputs,1)))191|SOMEv=>vvalW=caseinitWof193NONE=>(case#initType(!params)of108ChapterA.NeuralNetworkLibrariesSPARSE=>initSparseWRandState(nInputs,nOutputs)195|NORMAL=>Matrix.uniRandColsRandState(1971.0/Math.sqrt(Real.fromInt(nInputs)),(nInputs,nOutputs))199|=>raiseNotSupported)|SOMEm=>m201in{input=input,output=output,GradW=GradW,203GradB=GradB,B=B,W=W,actType=actType}end205(*initLayers:initthewholeneuralnetwork207input-layerSizes:structureofneuralnetwork209output-alistofinitializedlayer(e.g.ainitlaizednetwork)211NOTE:-ifcostfunctionisNLLorCE,213lastlayershouldnotuseanyactivationfunction*)215funinitLayersRandState(layerSizes,initWs,initBs)=let217val(initW,initWs’)=caseinitWsof[]=>(NONE,[])219|W::initWs’=>(W,initWs’)val(initB,initBs’)=caseinitBsof221[]=>(NONE,[])|B::initBs’=>(B,initBs’)223incaselayerSizesof225[]=>[]|last::[]=>[]227|nInputs::nOutputs::[]=>if(#costType(!params)=NLL)orelse(#costType(!params)=CE)then229initLayerRandState(nInputs,nOutputs,LINEAR,initW,initB)::[]else231initLayerRandState(nInputs,nOutputs,#actType(!params),initW,initB)::[]233|nInputs::nOutputs::layerSizes’=>initLayerRandState(nInputs,nOutputs,#actType(!params),initW,initB)235::initLayersRandState(nOutputs::layerSizes’,initWs’,initBs’)end237funsigmx=239ifx>13.0then1.0elseifx<˜13.0then0.0241else1.0/(1.0+Math.exp(˜x))(*fprop1layer:forwardpropagate1layer243input:-layer:layertobepropagated245-input:inputdatatobepropagatedoutput:247-(propagatedlayer,propagatedinput)*)249funfprop1layer(layer:nnlayer,input)=A.3.NeuralNetworkLibrary109let251valz=Matrix.addVect2MatrixReal(#B(layer),Matrix.mulMatrixRealR(input,#W(layer)))253valoutput=case#actType(layer)of255SIGM=>Matrix.mapsigmz|TANH=>Matrix.mapMath.tanhz257|LINEAR=>zin259({input=input,output=output,GradW=#GradW(layer),GradB=#GradB(layer),B=#B(layer),W=#W(layer),261actType=#actType(layer)},output)end263(*fprop:fpropagatethewholenetworkinput:265-layers:neuralnetworktobeforwardpropagated-input:inputtothefirstlayer(trainingdata)267output:-(propagatednetwork,finalpropagatedinput)269**NotethatreturingnetworkhastheinversedorderoflayersThiswillbeinversedagainwhenusingbpropfunction271*)funfprop(layers,input)=273List.foldl(fn(layer,(layers’,input))=>casefprop1layer(layer,input)of275(fpropedLayer,output)=>(fpropedLayer::layers’,output))([],input)layers277(*bprop1layer:backwardpropagate1layer279input:-layer:layertobepropagated281-gradInput:inputgradienttobepropagatedoutput:283-(propagatedlayer,propagatedgradient)*)285funbprop1layer(layer:nnlayer,gradInput)=let287vallambda=#lambda(!params)valwdValue=#wdValue(!params)289fundervSigmx=x∗(1.0−x)fundervTanhx=1.0−x∗x291funmomentumUpdate(a,b)=lambda∗a+((1.0−lambda)∗b)funsignx=ifx>0.0then1.0else˜1.0293valgradFunc=case#actType(layer)of295SIGM=>Matrix.mapdervSigm(#output(layer))|TANH=>Matrix.mapdervTanh(#output(layer))297|LINEAR=>Matrix.initRows(1.0,Matrix.size(#output(layer)))299valgradOutput=Matrix.dotMulMatrixReal(gradInput,gradFunc)valgradOutputC=Matrix.changeType(gradOutput)301valoldGradW=#GradW(layer)valoldGradB=#GradB(layer)303valGradWnoWd=Matrix.mulMatrixRealC(Matrix.transpose(#input(layer)),gradOutputC)305valGradW=case#wdType(!params)ofL0=>GradWnoWd307|L1=>Matrix.merge(fn(a,b)=>a+wdValue∗signb)110ChapterA.NeuralNetworkLibraries(GradWnoWd,#W(layer))309|L2=>Matrix.merge(fn(a,b)=>a+wdValue∗b)(GradWnoWd,#W(layer))311valGradB=Matrix.sumReal(gradOutputC)valnewGradW=Matrix.mergemomentumUpdate(oldGradW,GradW)313valnewGradB=Matrix.mergeVectormomentumUpdate(oldGradB,GradB)valpropedGrad=Matrix.mulMatrixRealR(315gradOutput,Matrix.transpose(#W(layer)))in317({input=#input(layer),output=#output(layer),GradW=newGradW,GradB=newGradB,B=#B(layer),W=#W(layer),319actType=#actType(layer)},propedGrad)end321(*bprop:backpropagatethewholenetwork323input:-layers:neuralnetworkwithinversedorderoflayerscreatedbyfprop325-gradInput:gradienttothelastlayeroutput:327-(propagatednetwork,finalpropagatedgradient)**Notethattheinputlayershastobeininversedlayerorder329Whichistheorderinthenetworkreturnedfromfpropfunction*)331funbprop(layers,gradInput)=List.foldl(fn(layer,(layers’,gradInput))=>333casebprop1layer(layer,gradInput)of(bpropedLayer,gradOutput)=>335(bpropedLayer::layers’,gradOutput))([],gradInput)layers337(*computeCost:computetraining/validatingcostinput:339-output:predictedresultfromtheneuralnetwork-target:thedesiredtarget341-costType:typeofcost-MSE/NLL/PERoutput:343-(errors,gradient)*)345vali:intref=ref0;funcomputeCost(output,target,costType)=347letvalnSamples=Real.fromInt(#1(Matrix.size(output)))349valmax=Matrix.foldl(fn(x,acc)=>ifx>accthenxelseacc)˜10000.0351funmeanv=(List.foldl(fn(x,acc)=>acc+x)0.0v)/Real.fromInt(List.length(v))353funcalMSE()=let355valdiff=Matrix.merge(fn(a,b)=>a−b)(output,target)valerrors=Matrix.sumReal(Matrix.map(fna=>0.5∗a∗a)diff)357valgradient=Matrix.map(fna=>a/nSamples)diff359in(meanerrors,gradient)361endfunsoftmax(input)=363letvalmaxInput=maxinputA.3.NeuralNetworkLibrary111365valexpInput=Matrix.mergeVect’2Matrix(fn(a,b)=>Math.exp(b−a))367(maxInput,input)valsumExp=Matrix.sumRealexpInput369inMatrix.mergeVect’2Matrix(fn(a,b)=>b/a)371(sumExp,expInput)end373funcalNLL()=let375val=i:=(!i+1)valsoftmaxOutput=softmax(output)377valerrors=Matrix.sumReal(Matrix.merge(fn(a,b)=>˜(Math.ln(a)∗b))379(softmaxOutput,target))valgradient=Matrix.merge(fn(a,b)=>(˜a+b)/nSamples)381(target,softmaxOutput)in383(meanerrors,gradient)end385funcalCE()=let387valerrors=Matrix.sumReal(Matrix.merge389(fn(a,b)=>Math.ln(1.0+Math.exp(a))−a∗b)(output,target))391valgradient=Matrix.merge(fn(a,b)=>(sigm(a)−b)/nSamples)393(output,target)in395(meanerrors,gradient)end397funequalReal(a,b)=Real.abs(a−b)<0.0000000001funcalPER()=399letvalmaxnonzeros=Matrix.foldl401(fn(x,acc)=>if(x>accandalsonot(equalReal(x,0.0)))403thenxelseacc)˜20000.0405valnSamples=Real.fromInt(#1(Matrix.size(output)))valmaxOutput=maxoutput407valmaxTarget=maxnonzeros(Matrix.merge(fn(a,b)=>a∗b)(output,target))409valanswers=Matrix.mergeVector(fn(x,y)=>equalReal(x,y))(maxOutput,maxTarget)411valnRights=List.foldl(fn(x,acc)=>ifxthenacc+1elseacc)4130answersin415(1.0−(Real.fromInt(nRights)/nSamples),Matrix.initRows(0.0,(1,1)))417end419incasecostTypeof421MSE=>calMSE()|NLL=>calNLL()112ChapterA.NeuralNetworkLibraries423|CE=>calCE()|PER=>calPER()425end(*update1layer:update1layer427input-layer:alayertobeupdated429output-updatedlayer431*)funupdate1layer(layer:nnlayer)=433letvalnewW=Matrix.merge(fn(a,b)=>a−(#lr(!params))∗b)435(#W(layer),#GradW(layer))valnewB=Matrix.mergeVector(fn(a,b)=>a−(#lr(!params))∗b)437(#B(layer),#GradB(layer))in439{input=#input(layer),output=#output(layer),GradW=#GradW(layer),GradB=#GradB(layer),actType=#actType(layer),W=newW,B=newB}441end443(*update:updatethewholenetwork*)445funupdate(layers)=List.foldr(fn(a,acc)=>update1layer(a)::acc)[]layers447(*449Supportmethodfortrainingneuralnetwork*)451vali:intref=ref0;funtrain1Batch(layers,input,target)=453letval(fpropedLayers,output)=fprop(layers,input)455val=i:=(!i+1)val(errors,grad)=computeCost(output,target,457#costType(!params))val(bpropedLayers,)=bprop(fpropedLayers,grad)459val=if#verbose(!params)thenprint(Real.toString(errors)ˆ"\n")else()461inupdate(bpropedLayers)463endfuntrainBatches(layers,inputs,targets)=465case(inputs,targets)of([],[])=>layers467|(input::inputs’,target::targets’)=>trainBatches(train1Batch(layers,input,target),469inputs’,targets’)|=>raiseMatrix.UnmatchedDimension471(*trainNN:trainneuralnetworkinput:473-data_train:listofbatchesoftrainingcases-target_train:listofbatchesofdesiredtarget475output:-trainedneuralnetwork477*)funtrainNN(startLayers,datatrain,targettrain)=479letfuntrainEpoches(layers,nItrs)=A.3.NeuralNetworkLibrary113481letval=if#verbose(!params)thenprint("******epochs:"ˆInt.toString(nItrs)ˆ"*******\n")else()483inifnItrs=0thenlayers485elsetrainEpoches(487trainBatches(layers,datatrain,targettrain),nItrs−1)489endin491trainEpoches(startLayers,#nItrs(!params))end493(*495TrainNNontrainingsetandpickthebestresultonvalidationset*)497funtrainBest(startLayers,datatrain,targettrain,datavalidation,targetvalidation)=499letfuntrainEpoches(layers,nItrs,bestErr)=501letval=if#verbose(!params)thenprint("******epochs:"ˆInt.toString(nItrs)ˆ"*******\n")else()503val(,output)=fprop(layers,hd(datavalidation))val(errors,)=computeCost(output,hd(targetvalidation),PER)505valbestErr=ifbestErr<errorsthenbestErrelseerrorsval=if#verbose(!params)thenprint507("BestValidationErr="ˆReal.toString(bestErr)ˆ"\n")else()in509ifnItrs=0then(bestErr,layers)else511trainEpoches(trainBatches(layers,datatrain,targettrain)513,nItrs−1,bestErr)end515intrainEpoches(trainBatches(startLayers,datatrain,targettrain),517#nItrs(!params),1.0)end519(*run:readinputdataandtrainaneuralnetwork*)521funrunRandState(p:params,fs:fileNames)=let523val=params:=pval=if#verbose(!params)thenprint525("*********Readingdata********\n")else()valdatatrain=readData(#datatrain(fs),527hd(#layerSizes(!params)),#batchsize(!params),#nBatches(!params))529vallabelstrain=readData(#labelstrain(fs),List.last(#layerSizes(!params)),531#batchsize(!params),#nBatches(!params))valdatatest=readData(#datatest(fs),533hd(#layerSizes(!params)),#testsize(!params),1)535vallabelstest=readData(#labelstest(fs),List.last(#layerSizes(!params)),537#testsize(!params),1)val=if#verbose(!params)thenprint114ChapterA.NeuralNetworkLibraries539("******Donereadingdata,starttraining*****\n")else()valstartLayers=initLayersRandState(#layerSizes(!params),541#initWs(!params),#initBs(!params))543valtrainedLayers=trainNN(startLayers,datatrain,labelstrain)val(,output)=fprop(trainedLayers,hd(datatest))545val(errors,)=computeCost(output,hd(labelstest),PER)547in(trainedLayers,errors)549endendAppendixBADATESpeciﬁcationforInitializationExperiment2datatypeaUnit=aUnit4datatypereallist=rnil|consrofreal∗reallist6datatypereallistlist=rlnil|consrlofreallist∗reallistlist8datatypeweightMatrix=weightMatrixofreal∗reallistlist10datatypeweightMatrixlist=wnil|conswofweightMatrix∗weightMatrixlistdatatypelayerType=visHid|hidHid1|hidHid2|hidOut12funrconstLess((X,C):real∗rconst):bool=14caseCofrconst(Compl,StepSize,Current)=>realLess(X,Current)16funrandnormal((Mean,Std):real∗real):real=18letfunboxmuller(Dummy:aUnit):real=20caserealSubtract(realMultiply(2.0,(aRand0)),1.0)ofX122=>caserealSubtract(realMultiply(2.0,(aRand0)),1.0)ofX224=>caserealAdd(realMultiply(X1,X1),realMultiply(X2,X2))ofW=>caserealLess(W,1.0)of26true=>realAdd(Mean,realMultiply(Std,28realMultiply(X1,sqrt(realDivide(realMultiply(˜2.0,ln(W)),W)))))30|false=>boxmulleraUnitin32boxmulleraUnitend34funrand2()=362.0∗(aRand0)−1.0(*Normalizedinitialization*)38funf(NInputs,NOutputs,LayerType)=115116ChapterB.ADATESpeciﬁcationforInitializationExperimentlet40funh(N:real):reallist=case0.0<Nof42false=>rnil|true=>44consr(realDevide(realMultiply(Math.sqrt(6.0),rand2()),Math.sqrt(realAdd(NInputs,NOutputs))),46h(realSubtract(N,1.0)))in48hNInputsend5052funinitW((NInputs,NOutputs,LayerType):real∗real∗layerType):weightMatrix=54letfuninitSparseVects(I:real):reallistlist=56caserealLess(I,NOutputs)offalse=>rlnil58|true=>consrl(f(NInputs,NOutputs,LayerType),60initSparseVects(I+1.0))in62weightMatrix(NInputs,initSparseVects0.0)end64funmain((Layer1size,Layer2size,Layer3size,Layer4size,Layer5size):66real∗real∗real∗real∗real):weightMatrixlist=consw(initW(Layer1size,Layer2size,visHid),68consw(initW(Layer2size,Layer3size,hidHid1),consw(initW(Layer3size,Layer4size,hidHid2),70consw(initW(Layer4size,Layer5size,hidOut),wnil))))72%%74valNumInputs=76casegetCommandOption"--numInputs"ofSOMES=>caseInt.fromStringSofSOMEN=>N7880valNumIterations=casegetCommandOption"--numIterations"ofSOMES=>82caseInt.fromStringSofSOMEN=>N84valTrainParams:NN.params={batchsize=10,(*numberoftrainingcasesperbatch*)86nBatches=90,(*numberofbatches*)testsize=4500,(*numberoftestingcases*)88lambda=0.99,(*momentumcoefficient*)momentumSchedule=false,(*momentumschedule*)90maxLambda=0.0,(*maxmomentum*)lr=0.05,(*learningrate*)92costType=NN.NLL,(*costfunctiontype*)initType=NN.SPARSE,(*initializationtype*)94actType=NN.TANH,(*activationfunctiontype*)117layerSizes=[100,80,80,200,10],(*structureofnetwork*)96nItrs=NumIterations,(*numberofiterations/epoches*)initWs=[],(*initweightmatrices*)98initBs=[],wdType=NN.L2,(*weightdecaytype*)100wdValue=0.00001,verbose=false102};104valValidationParams:NN.params={batchsize=10,(*numberoftrainingcasesperbatch*)106nBatches=90,(*numberofbatches*)testsize=4500,(*numberoftestingcases*)108lambda=0.99,(*momentumcoefficient*)momentumSchedule=false,(*momentumschedule*)110maxLambda=0.0,(*maxmomentum*)lr=0.05,(*learningrate*)112costType=NN.NLL,(*costfunctiontype*)initType=NN.SPARSE,(*initializationtype*)114actType=NN.TANH,(*activationfunctiontype*)layerSizes=[100,80,80,200,10],(*structureofnetwork*)116nItrs=80,(*numberofiterations/epoches*)initWs=[],(*initweightmatrices*)118initBs=[],wdType=NN.L2,(*weightdecaytype*)120wdValue=0.00001,verbose=false122};124(*alltrainingandvalidationusethesameNetworkstructure*)126valInputs=[(100.0,80.0,80.0,200.0,10.0),128(100.0,80.0,80.0,200.0,10.0),(100.0,80.0,80.0,200.0,10.0),130(100.0,80.0,80.0,200.0,10.0),(100.0,80.0,80.0,200.0,10.0),132(100.0,80.0,80.0,200.0,10.0),(100.0,80.0,80.0,200.0,10.0),134(100.0,80.0,80.0,200.0,10.0),(100.0,80.0,80.0,200.0,10.0),136(100.0,80.0,80.0,200.0,10.0),(100.0,80.0,80.0,200.0,10.0),138(100.0,80.0,80.0,200.0,10.0),(100.0,80.0,80.0,200.0,10.0),140(100.0,80.0,80.0,200.0,10.0),(100.0,80.0,80.0,200.0,10.0),142(100.0,80.0,80.0,200.0,10.0),(100.0,80.0,80.0,200.0,10.0),144(100.0,80.0,80.0,200.0,10.0),(100.0,80.0,80.0,200.0,10.0),146(100.0,80.0,80.0,200.0,10.0)]148valTestinputs=[(100.0,80.0,80.0,200.0,10.0),150(100.0,80.0,80.0,200.0,10.0),(100.0,80.0,80.0,200.0,10.0),152(100.0,80.0,80.0,200.0,10.0),118ChapterB.ADATESpeciﬁcationforInitializationExperiment(100.0,80.0,80.0,200.0,10.0),154(100.0,80.0,80.0,200.0,10.0),(100.0,80.0,80.0,200.0,10.0),156(100.0,80.0,80.0,200.0,10.0),(100.0,80.0,80.0,200.0,10.0),158(100.0,80.0,80.0,200.0,10.0),(100.0,80.0,80.0,200.0,10.0),160(100.0,80.0,80.0,200.0,10.0),(100.0,80.0,80.0,200.0,10.0),162(100.0,80.0,80.0,200.0,10.0),(100.0,80.0,80.0,200.0,10.0),164(100.0,80.0,80.0,200.0,10.0),(100.0,80.0,80.0,200.0,10.0),166(100.0,80.0,80.0,200.0,10.0),(100.0,80.0,80.0,200.0,10.0),168(100.0,80.0,80.0,200.0,10.0)]170)172valinputfiles=[("/local/data1.csv","/local/labels1.csv","/local/datavalid1.csv","/local/labelsvalid1.csv"),174("/local/data2.csv","/local/labels2.csv","/local/datavalid2.csv","/local/labelsvalid2.csv"),176("/local/data3.csv","/local/labels3.csv","/local/datavalid3.csv","/local/labelsvalid3.csv"),178("/local/data4.csv","/local/labels4.csv","/local/datavalid4.csv","/local/labelsvalid4.csv"),180("/local/data5.csv","/local/labels5.csv","/local/datavalid5.csv","/local/labelsvalid5.csv"),182("/local/data6.csv","/local/labels6.csv","/local/datavalid6.csv","/local/labelsvalid6.csv"),184("/local/data7.csv","/local/labels7.csv","/local/datavalid7.csv","/local/labelsvalid7.csv"),186("/local/data8.csv","/local/labels8.csv","/local/datavalid8.csv","/local/labelsvalid8.csv"),188("/local/data9.csv","/local/labels9.csv","/local/datavalid9.csv","/local/labelsvalid9.csv"),190("/local/data10.csv","/local/labels10.csv","/local/datavalid10.csv","/local/labelsvalid10.csv"),192("/local/data11.csv","/local/labels11.csv",194"/local/datavalid11.csv","/local/labelsvalid11.csv"),("/local/data12.csv","/local/labels12.csv",196"/local/datavalid12.csv","/local/labelsvalid12.csv"),("/local/data13.csv","/local/labels13.csv",198"/local/datavalid13.csv","/local/labelsvalid13.csv"),("/local/data14.csv","/local/labels14.csv",200"/local/datavalid14.csv","/local/labelsvalid14.csv"),("/local/data15.csv","/local/labels15.csv",202"/local/datavalid15.csv","/local/labelsvalid15.csv"),("/local/data16.csv","/local/labels16.csv",204"/local/datavalid16.csv","/local/labelsvalid16.csv"),("/local/data17.csv","/local/labels17.csv",206"/local/datavalid17.csv","/local/labelsvalid17.csv"),("/local/data18.csv","/local/labels18.csv",208"/local/datavalid18.csv","/local/labelsvalid18.csv"),("/local/data19.csv","/local/labels19.csv",210"/local/datavalid19.csv","/local/labelsvalid19.csv"),119("/local/data20.csv","/local/labels20.csv",212"/local/datavalid20.csv","/local/labelsvalid20.csv"),];214funreadData(data,target,datavalid,targetvalid)=216(NN.readData(data,hd(#layerSizes(TrainParams)),218#batchsize(TrainParams),#nBatches(TrainParams)),NN.readData(target,220List.last(#layerSizes(TrainParams)),#batchsize(TrainParams),#nBatches(TrainParams)),222NN.readData(datavalid,hd(#layerSizes(TrainParams)),224#testsize(TrainParams),1),NN.readData(targetvalid,226List.last(#layerSizes(TrainParams)),#testsize(TrainParams),1))228(*realinputdatafortrainingprocess*)valinputdata=Array.fromList(List.mapreadDatainputfiles)230(*convertADATElisttypetoMLlist*)232funtoRealListListList(Ws:weightMatrixlist):(real∗reallistlist)list=let234funtoRealList(rs):reallist=casersof236rnil=>[]|consr(r,rs’)=>r::toRealList(rs’)238funtoRealListList(rls:reallistlist):reallistlist=caserlsof240rlnil=>[]|consrl(rl,rls’)=>toRealList(rl)::toRealListList(rls’)242incaseWsof244wnil=>[]|consw(weightMatrix(nInputs,rls),Ws’)=>246(nInputs,toRealListList(rls))::toRealListListList(Ws’)end248funtoWeightListRandState(Ws:(real∗reallistlist)list):realMatrix.matrixoptionlist=250letfuninitWeightMatrix(argas(n,W):(real∗reallistlist))252:realMatrix.matrixoption=let254valnInputs=Real.floor(n)funinitSparseList((ids,initializedWeights,nInputs,I):256(Int32.intlist∗reallist∗Int32.int∗Int32.int)):reallist=258case(ids,I<=nInputs)of(,false)=>[]260|([],true)=>0.0::initSparseList(ids,initializedWeights,nInputs,I+1)262|(idx::ids’,true)=>if(idx=I)then264hd(initializedWeights)::initSparseList(ids’,266tl(initializedWeights),nInputs,I+1)120ChapterB.ADATESpeciﬁcationforInitializationExperiment268else0.0::initSparseList(ids,270initializedWeights,nInputs,I+1)272funinitSparseVects(nInputs,vs)=casevsof274[]=>[]|v::vs’=>initSparseList(276(randpermRandState(length(v),nInputs)),v,nInputs,1)::278(initSparseVects(nInputs,vs’))in280SOME(Matrix.fromVectors2Cols(initSparseVects(nInputs,W),(nInputs,length(W))))282endin284caseWsof[]=>[]286|W::Ws’=>initWeightMatrix(W)::toWeightListRandState(Ws’)end288valAbstracttypes=[]290valRejectfuns=[]funrestoretransformD=D292funcompiletransformD=Dvalprintsyntedprogram=Print.printdec’294296valFunstouse=[298"false","true","realLess","realAdd","realSubtract","realMultiply",300"tanh","tor","rconstLess",302"rand_normal","0",304"aRand","rnil","consr"306]308funto(G:real):LargeInt.int=Real.toLargeIntIEEEReal.TONEAREST(G∗1.0e14)310structureGrade:GRADE=312structtypegrade=LargeInt.int314valNONE=LargeInt.maxIntvalzero=LargeInt.fromInt0316valop+=LargeInt.+valcomparisons=[LargeInt.compare]318valN=LargeInt.fromInt1000000∗LargeInt.fromInt1000000valsignificantComparisons=[fn(E1,E2)320=>LargeInt.compare(E1divN,E2divN)]322funtoString(G:grade):string=Real.toString(Real.fromLargeIntG/1.0E14)324valpack=LargeInt.toString121326fununpack(S:string):grade=328caseLargeInt.fromStringSofSOMEG=>G330valpostprocess=fnX=>X332valtoRealOpt=NONE334end336valInputs=take(NumInputs,Inputs)338funoutputevalfun(exactlyOne(I:int,:(real∗real∗real∗real∗real),WeightList:weightMatrixlist))=[340letval=NN.setParams342(ifI<Int64.fromIntNumInputsthenTrainParams344elseValidationParams)valRandState=Random.rand(10,Int64.toIntI)346val(datatrain,labelstrain,datatest,labelstest)=Array.sub(inputdata,Int64.toIntI)348valWs=toWeightListRandState(toRealListListList(WeightList))valstartLayers=NN.initLayersRandState(#layerSizes(TrainParams),Ws,[])350valtrainedLayers=NN.trainNN(startLayers,datatrain,labelstrain)val(,output)=NN.fprop(trainedLayers,hd(datatest))352val(errors,)=NN.computeCost(output,hd(labelstest),NN.PER)354val()=(p"\noutput_eval_fun:I=";printint64I;356p"errors=";printrealerrors;p"\n"358)360iniferrors>1.0E30orelsenot(Real.isFiniteerrors)then362{numCorrect=0:int,numWrong=1:int,grade=to1.0E30}else364{numCorrect=1,numWrong=0,grade=toerrors}end366]368exceptionMaxSyntComplExnvalMaxSyntCompl=(370casegetCommandOption"--maxSyntacticComplexity"ofNONE=>150.0372|SOMES=>caseReal.fromStringSofSOMEN=>N)handleEx=>raiseMaxSyntComplExn374376funrlEq(rnil,rnil)=true|rlEq(rnil,consr(,))=false378|rlEq(consr(,),)=false|rlEq(consr(X1,Xs1),consr(Y1,Ys1))=380realeq(X1,Y1)andalsorlEq(Xs1,Ys1)122ChapterB.ADATESpeciﬁcationforInitializationExperiment382funrllEq(rlnil,rlnil)=true384|rllEq(rlnil,consrl(,))=false|rllEq(consrl(,),)=false386|rllEq(consrl(X1,Xs1),consrl(Y1,Ys1))=rlEq(X1,Y1)andalsorllEq(Xs1,Ys1)388funwmEq(weightMatrix(X,Xss),weightMatrix(Y,Yss))=390realeq(X,Y)andalsorllEq(Xss,Yss)392funwlEq(wnil,wnil)=true|wlEq(wnil,consw(,))=false394|wlEq(consw(,),)=false|wlEq(consw(X1,Xs1),consw(Y1,Ys1))=396wmEq(X1,Y1)andalsowlEq(Xs1,Ys1)398valAllAtOnce=falsevalOnlyCountCalls=false400valTimeLimit:Int.int=10000000valmaxtimelimit=fn()=>Word64.fromIntTimeLimit:Word64.word402valmaxtesttimelimit=fn()=>Word64.fromIntTimeLimit:Word64.wordvaltimelimitbase=fn()=>realTimeLimit404funmaxsyntacticcomplexity()=MaxSyntCompl406funminsyntacticcomplexity()=0.0valUsetestdataformaxsyntacticcomplexity=false408valmainrangeeq=wlEq410valFilenameextension="numIterations"ˆInt.toStringNumIterationsˆ412"numInputs"ˆInt.toStringNumInputs414valResolution=NONEvalStochasticMode=false416valNumberofoutputattributes:Int64.int=4418funterminate(Nc,G)=false