2
2
0
2

n
u
J

0
3

]

G
L
.
s
c
[

1
v
7
8
9
4
1
.
6
0
2
2
:
v
i
X
r
a

Lookback for Learning to Branch

Prateek Gupta, University of Oxford, The Alan Turing Institute
Elias B. Khalil, University of Toronto
Didier Chetélat, Polytechnique Montréal
Maxime Gasse, Mila, Polytechnique Montréal
Yoshua Bengio, Mila, Université de Montréal, CIFAR Fellow
Andrea Lodi, CERC, Polytechnique Montréal, and Cornell Tech and Technion - IIT
M. Pawan Kumar, University of Oxford

pgupta@robots.ox.ac.uk
khalil@mie.toronto.edu
didier.chetelat@polymtl.ca
maxime.gasse@polymtl.ca
yoshua.bengio@mila.quebec
andrea.lodi@cornell.edu
pawan@robots.ox.ac.uk

Abstract

The expressive and computationally inexpensive bipartite Graph Neural Networks (GNN)
have been shown to be an important component of deep learning based Mixed-Integer Linear
Program (MILP) solvers. Recent works have demonstrated the eﬀectiveness of such GNNs
in replacing the branching (variable selection) heuristic in branch-and-bound (B&B) solvers.
These GNNs are trained, oﬄine and on a collection of MILPs, to imitate a very good but
computationally expensive branching heuristic, strong branching. Given that B&B results
in a tree of sub-MILPs, we ask (a) whether there are strong dependencies exhibited by the
target heuristic among the neighboring nodes of the B&B tree, and (b) if so, whether we
can incorporate them in our training procedure. Speciﬁcally, we ﬁnd that with the strong
branching heuristic, a child node’s best choice was often the parent’s second-best choice. We
call this the “lookback” phenomenon. Surprisingly, the typical branching GNN of Gasse
et al. (2019) often misses this simple “answer". To imitate the target behavior more closely
by incorporating the lookback phenomenon in GNNs, we propose two methods: (a) target
smoothing for the standard cross-entropy loss function, and (b) adding a Parent-as-Target
(PAT) Lookback regularizer term. Finally, we propose a model selection framework to
incorporate harder-to-formulate objectives such as solving time in the ﬁnal models. Through
extensive experimentation on standard benchmark instances, we show that our proposal
results in up to 22% decrease in the size of the B&B tree and up to 15% improvement in the
solving times.

1 Introduction

Many real-world decision making problems are naturally formulated as Mixed-Integer Linear Programs
(MILPs), for example, process integration (Kantor et al., 2020), production planning (Pochet & Wolsey,
2006), urban traﬃc management (Foster & Ryan, 1976; Fayazi & Vahidi, 2018), data center resource
management (Nowatzki et al., 2013), and auction design (Abrache et al., 2007) to name a few. In some
applications, such as urban traﬃc management (Fayazi & Vahidi, 2018), these MILPs need to be solved
frequently (e.g., every second) with only a slight change in the speciﬁcations. Similarly in data center resource
management (Nowatzki et al., 2013), the available machines and tasks that must be served evolve over
time, prompting repeated assignments through MILP solving. Even with a linear objective function and
linear constraints, the requirement that some decision variables must take on integer values makes MILPs
NP-Hard (Papadimitriou & Steiglitz, 1982).

As a result, the Branch-and-Bound (B&B) algorithm (Land & Doig, 1960) is used in the modern solvers to
eﬀectively prune the search space of the MILPs to ﬁnd the global optimum. B&B proceeds by recursively
splitting the search space and solving the linear relaxation of the resulting sub-problems, the solution of
which serves as an informative bound to prune the search space. The algorithm continues until a solution
with integral decision variables is found and proven optimal. Quite naturally, the sequence of sub-problems

1

 
 
 
 
 
 
resulting from the decisions at each step of the algorithm can be visualized as a tree; every node (a sub-MILP)
has a parent except the root node.

While seemingly simple, the B&B algorithm must repeatedly take decisions that are crucial for its eﬃciency
in solving MILPs, such as the choice of decision variable over which to split at each iteration, a problem
known as variable selection. Even though the worst-case time-complexity of the B&B algorithm is exponential
in the size of the problem (Wolsey, 1988), it has been successfully applied in practical settings thanks to the
careful design of a number of eﬀective search heuristics.

Modern solvers are conﬁgured with expert-designed heuristics and are aimed at solving general MILPs.
However, assuming that MILPs come from a speciﬁc distribution, there has been a recent surge in research
related to statistical approaches to learning such heuristics (He et al., 2014; Alvarez et al., 2017; Khalil et al.,
2016; Gasse et al., 2019; Zarpellon et al., 2020; Gupta et al., 2020; Huang et al., 2022).

Figure 1: (a) Frequency of the “lookback” property when (i) instances are solved using strong branching
heuristic (Ground truth), and (ii) corresponding frequency with which GNNs would have respected this
property on Ground truth. (b)-(e) GNNs trained with the proposed techniques (GNN-PAT) have higher
accuracy on the validation dataset. (f) GNN-PAT solves test instances faster than other baselines resulting
in (g) less time as compared to GNNs.

Gasse et al. (2019) proposed to use Graph Neural Network (GNN) for the variable selection problem to imitate
branching decisions of a computationally expensive strong branching heuristic that yields the smallest B&B

2

trees in practice. The GNN operates on a bipartite representation of a MILP in which variables and constraints
are nodes and an edge indicates the presence of a variable in a constraint. Such a bipartite representation has
several advantages. First, it is able to capture the most crucial invariances of MILPs, namely, permutation
invariance to the ordering of decision variables/constraints, and, by way of feature engineering, scale invariance
to the scaling of constraints or objective coeﬃcients. Second, due to the shared parametric representation,
the model can be applied to MILPs of arbitrary size. Thus, GNNs have been extensively used to process
MILPs as inputs to a neural network aimed at imitating various superior heuristic decisions (Huang et al.,
2022; Zarpellon et al., 2020; Nair et al., 2020), thereby outperforming solvers with oﬀ-the-shelf heuristics.

The training of GNNs consists primarily of two steps: (a) Dataset collection: data, i.e., sub-problems and
corresponding strong branching decisions, are collected oﬄine by solving reasonably-sized MILPs while
invoking, with some exploration probability, either a less eﬃcient but faster heuristic, or the strong branching
heuristic, and (b) Model Training: assuming independent and identical distribution (i.i.d.) of the collected
dataset, GNNs are trained to minimize standard cross-entropy loss between the predictions of GNNs and the
strong branching target.

So far, previous works on learning to branch have tried
imitating the strong branching decisions at each node in
isolation, thereby ignoring the statistical resemblance in
the behavior of the neighboring nodes. However, it often
happens that strong branching’s top choice at a node was
the second best choice at the node’s parent, a phenomenon
we will refer to onwards as the “lookback” property of
strong branching. Figure 1(a) shows how frequently this
occurs in some standard benchmark instances, labeled
for convenience as cauctions (Combinatorial Auction),
setcover (Minimum Set Cover), facilities (Capacitated
Facility Location), and indset (Maximum Independent
Set). Given the importance of decisions closer to the root
node, Figure 2 further shows that this phenomenon is
quite prevalent in the top of the tree. Although it appears
quite common empirically, such property has never been
explicitly pointed out in the previous literature.

Since imitation learning aims to mimic the expert as closely
as possible, and the expert exhibits this lookback prop-
erty, it follows that successful imitation should exhibit the
lookback property as well. However, as can be seen in
Figures 1(a) and 2, there is a big gap between the frac-
tion of times GNNs respect the lookback condition and
the fraction of times it happens in the strong branching
oracle. We therefore propose in this work two approaches
to encourage it in imitation frameworks for learning to
branch, and analyze its eﬀects on solving performance.

Figure 2: (Maximum Independent Set) Frequency
of the lookback property per depth-decile, one of
the 10 equally divided portions by depth of the
B&B tree. Ground truth is obtained by solving
small instances using strong branching heuristic.
Retrospectively, GNNs do not respect the look-
back property well enough. For similar statistics
on other benchmark problem families, see Ap-
pendix A.

A diﬃculty is that our proposed methods introduce new hyperparameters, which raises the question of how to
select them eﬃciently. Gupta et al. (2020) used validation accuracy on a few thousand observations collected
by solving MILPs of slightly bigger size than the training instances. We argue that such a selection strategy
doesn’t serve various objectives that a practitioner might have. Thus, we also design a better model selection
framework to respect such varied requirements. As shown in Figure 1(b)-(e), our resulting combined method
(GNN-PAT) increases the validation accuracy on each benchmark, which in turn lead to decreases in the size
of the ﬁnal B&B tree by up to 22% and in the running time by up to 15% (Figure 1(g)) solving most of the
instances fastest among the baselines (Figure 1(f)).

To summarize, the contributions of this paper are as follows.

3

• First, we demonstrate through numerical experiments that the “lookback” phenomenon often occurs

in practice (see Figure 1 and 2).

• Second, we propose two ways of exploiting this phenomenon, namely through target softening, and

through a regularizer term that encourages the GNN to exhibit this phenomenon.

• Third, we propose a model selection framework to incorporate harder-to-formulate practical objectives

such as minimum solving times.

The paper is divided as follows. Sections 2 and 3 review the literature and preliminary notation and
deﬁnitions, respectively. Section 4 proposes techniques to exploit the lookback phenomenon in imitation
learning. Section 5 details our proposed hyperparameter selection framework. Then, Section 6 presents
experimental results that show the beneﬁts of these adjustments. Finally, in Section 7, we discuss implications
of our proposals, limitations, and potential future directions, concluding in Section 8.

2 Related Work

The problem of variable selection in B&B based MILP solving has been studied quite extensively. While the
gold standard strong branching heuristic yields the smallest B&B trees, due to the high computational cost
per iteration it is impractical (see Section 3 for details). Thus, in its early years, the focus of research has
been on hand-designed heuristic methods (Applegate et al., 1995; Linderoth & Savelsbergh, 1999) that are
faster and suﬃciently good. As a result, reliability pseudocost branching (RPB) (Achterberg et al., 2005)
became the preferred branching strategy to solve general MILPs. RPB combines the lookahead strengths of
strong branching heuristic with faster estimations oﬀered through computationally inexpensive measures of
performance (such as pseudocosts (Linderoth & Savelsbergh, 1999)).

In the last decade, researchers have proposed machine learning (ML) methods to imitate the strong branching
heuristic, thereby leveraging the computationally inexpensive nature of the learned functions. For exam-
ple, Alvarez et al. (2017) used extremely randomized trees on the data collected oﬄine, whereas Khalil et al.
(2016) used support vector machines to imitate the strong branching ranking from the ﬁrst few hundred
nodes explored in the B&B tree. We refer the reader to Lodi & Zarpellon (2017) for a detailed survey on
ML-based branching strategies.

Both Alvarez et al. (2017) and Khalil et al. (2016) learn a classiﬁer on hand-designed features. Recently,
however, the promise of representation learning through neural networks has been exploited to propose
deep learning based branching strategies (Gasse et al., 2019; Zarpellon et al., 2020; Gupta et al., 2020;
Nair et al., 2020). Speciﬁcally, Gupta et al. (2020) explored the relation between the original MILP and
subsequent sub-MILPs in a B&B tree to design a CPU-eﬃcient neural network architecture. However, this
relationship has more to do with the B&B characteristics than the oracle behavior. Similarly, Zarpellon et al.
(2020) explored learning suitable representations based on the evolution of the B&B tree. To the best of
our knowledge, the parent-child lookback property has not been explored in the context of training machine
learning based variable selection strategies.

More recently, works have investigated reinforcement learning formulations for learning to branch. Sun
et al. (2020) used a simple evolutionary strategies approach, but they only obtained improvements on very
homogeneous benchmarks, while reporting subpar results on the harder benchmarks of Gasse et al. (2019)
used in this work. In parallel, Etheve et al. (2020) proposed a Q-learning approach on value functions
representing subtree size, and Scavuzzo et al. (2022) reinterpreted their approach as reinforcement learning
on a tree-shaped Markov decision processes. Although an interesting step forward, these approaches are
nonetheless not competitive with imitation learning methods, which remain the state of the art. In general,
reinforcement learning faces unusual challenges in this context, including that poor decisions lead to longer,
rather than shorter episodes, and also that transitions between states are also particularly computationally
slow, since they involve solving linear programs. For these reasons, like most works, we chose to focus on
the more successful strategy of imitation learning. Nonetheless, it is plausible that encouraging a lookback
property in a reinforcement learning policy could lead to similar beneﬁts to those explored in the context of
this work.

4

3 Preliminaries

A MILP is a mathematical optimization problem characterized by a linear objective function and linear
constraints in variables x. A generic representation of a MILP is as follows:

c(cid:124)x,

min
x

s.t. Ax ≤ b, x ∈ Zp × Rn−p,

(1)

where c ∈ Rn is the vector of cost coeﬃcients, A ∈ Rm×n is the matrix of constraint coeﬃcients, b ∈ Rm is a
vector of constant terms of constraints, and p of the decision variables are constrained to take integer values.

The B&B algorithm proceeds as follows. The Linear Programming (LP) relaxation of the MILP, obtained by
relaxing the integer constraints on the discrete variables, is solved to obtain a lower bound on the global
optimum, thereby resulting in x∗ as the optimal solution. If x∗ has integral values for the integer-constrained
decision variables, then it is integer-optimal and the algorithm terminates. If not, one of the decision variables
i 6∈ Z, i ≤ p}, is selected to split the MILP in two sub-MILPs.
with fractional value, i ∈ C, such that C ∈ {i | x∗
The resulting sub-MILPs are obtained by adding additional constraints xi ≤ bx?
i e, respectively.
We denote as C the set of branching candidates, while the variable xi is termed as a branching variable. The
algorithm proceeds recursively in this fashion by selecting the next sub-MILPs to operate on.

i c and xi ≥ dx?

Denoting the optimal value of Eq. (1) by P , and using the superscripts 0 to denote the parent MILP, − to
denote the child MILP obtained by adding the lower bound to the branching variable, and + otherwise. The
strong branching heuristic selects the variable xsb that has the maximum potential to improve the bound,
namely

xsb = arg max

i∈C

(cid:2) max{P − − P 0, (cid:15)LP } · max{P + − P 0, (cid:15)LP }(cid:3),

where (cid:15)LP is a small enough value to prevent the scores to collapse to 0 because of no improvement on either
side of the branching.

4 Methodology

In this section, we describe our proposals to incorporate dependencies between successive nodes. A bipartite
graph representation of a MILP is denoted by G ∈ (V, E, C), where V ∈ Rn×dv , E ∈ Rk×de, and C ∈ Rm×dc .
Here, V is the matrix of features for n decision variables, C is the matrix of features for m constraints, and
E is the matrix of features for k variable-constraint pairs. The terms dv, dc, and de are the numbers of input
features.

The data is collected by using the strong branching heuristic to solve instances of manageable size, thereby
, the set of candidate decision variables, Ci, with
yielding N graphical representations of MILPs, {Gi}N
fractional value at a node i, and their corresponding strong branching scores si ∈ R|Ci|
. We use si,j to denote
≥0
the strong branching score of the jth candidate in Ci. We denote the strong branching target chosen during
the solving procedure by yi, and the set of second best strong branching variables by Zi = arg max
j6=yi si,j.
Thus, Zi may include variables which have the same strong branching score as yi if there is a tie, else it
includes all the variables with the second best strong branching score, which can be more than one.

i=1

i , s0

i , s1
i

i , G1

We denote the parent graph by a superscript 0 and the child node by a superscript 1. Thus, the parent
bipartite graph of the i-th observation is denoted by G0
. Note that we drop
i
the superscripts whenever we do not need a distinction between parent and child nodes. We use D =
{(G0
Deﬁning a GNN by a function fθ that is deﬁned over the parameter space Θ, Gasse et al. (2019) proposed to
ﬁnd the optimal parameters by empirical risk minimization of the cross-entropy loss between the predictions
and the strong branching target over the dataset D. Thus, if there are n decision variables in Gi, fθ(Gi) ∈ Rn
represents the scores predicted by the function fθ. Denoting yi as a one-hot encoded vector with the value of

) | i ∈ {1, 2, 3, ..., N }} to denote the entire dataset.

and the child graph by G1
i

5

1 at yi and 0 elsewhere, θ?
y

is determined by solving

θ?
y

= arg min

θ

1

N

N
X

i=1

wi · CE(fθ(Gi), yi),

(2)

where CE is the cross-entropy loss function, and wi is the relative importance given to the observation i,
which may depend on the depth (Gupta et al., 2020).

4.1 Second-best (cid:15)-smooth loss target

Keeping the cross-entropy loss function as is, we modify the target to a smooth label zi, i.e., instead of
one-hot encoded vector yi, zi carries a value of 1 − (cid:15) at the index of the strong branching target yi, while the
value of (cid:15) is equally distributed among the second best strong branching decisions in Zi. Thus, we obtain the
optimal parameters as

θ?
z

= arg min

θ

1

N

N
X

i=1

wi · CE(fθ(Gi), zi).

(3)

A modiﬁed target such as zi in Eq. (3) yields parameters that tend to preserve the ranking of the second
most important decisions. While intuitive and simple to implement with minimal changes in the existing
framework, θ?
z

is still not aware of their parent behavior.

4.2 Parent-As-Target (PAT) lookback loss term

Here, we are interested in incorporating the relation between parent and child outputs as it happens under
the strong branching oracle. In doing so, we expect the learned parameters to more appropriately represent
the strong branching behavior. Deﬁning the lookback condition Li at the node i as

Li =

(1,
0,

y1
i ∈ Z 0
i
otherwise,

(4)

we consider an additional term to Eq. (2) or Eq. (3) that enforces fθ to follow the same ordering between
parent-child nodes whenever Li = 1. We call this Parent-As-Target (PAT) lookback term, designed to enforce
)[C1
similarity between the logits at the parent node for the candidates C1
],
i
i
with
, denoted by fθ(G1
and the logits at the child node for the same candidates C1
i
i
the target yi as

of the child node, denoted as fθ(G0
i
yP AT

). Thus, we obtain θ?

θ?
yP AT

= arg min

θ

1

N

N
X

i=1

h
CE(fθ(G1
i

wi·

), y1
i

)+

N

PN
i

1{Li = 1}

1{Li = 1}·λP AT ·CE(fθ(G1
i

), fθ(G0
i

)[C1
i

i
])

, (5)

or we obtain θ?

zP AT

with the target zi as

θ?
zP AT

= arg min

θ

1

N

N
X

i=1

h

wi·

CE(fθ(G1
i

), z1
i

)+

N

PN
i

1{Li = 1}

1{Li = 1}·λP AT ·CE(fθ(G1
i

), fθ(G0
i

)[C1
i

i
])

. (6)

The ﬁrst term in the brackets represents the usual cross-entropy loss which favors getting the top strong
branching variable right. The second term favors aligning the predicted scores of the second-best variable
in the parent node whenever it is the best in the child node i. This term is active only when the lookback
condition is satisﬁed, i.e., when 1{Li = 1} is true. Here, λP AT is the relative importance given to the
lookback condition, whenever it holds.

6

5 Model Selection

The dataset D is collected on instances of manageable size such that a reasonable number of observations
are collected within a certain time budget (e.g., 1 day). We label these instances as Small. Given various
hyperparameters involved in training machine learning models, a standard practice is to select a model
with the best validation accuracy. However, owing to the sequential nature of the branching decisions, the
validation accuracy is not necessarily indicative of the models’ runtime performance. For example, due to bad
branching decisions early on in the search, the models might later face sub-MILPs that are not representative
of the training dataset, thereby leading to even worse decisions. Alternatively, a model might make reasonably
good guesses of the strong branching variables, but the overall running time may be dominated by the solving
time of the intermediate LP relaxations.

In general, a practitioner is interested in using the learned models to solve problems that are potentially
bigger than those used for data collection and training. We label these instances as Big. To obtain some
estimates of a model’s ability to generalize to Big instances, we solve K randomly generated Medium instances
of intermediate size by using each of the learned branching strategies, b ∈ B, to guide the MILP solver within
a time limit of T seconds per instance. The aggregate performance (e.g., arithmetic mean, geometric mean,
etc.) of the branching models can be evaluated in terms of (a) solving time, denoted by t(b), across K
instances, e.g., 1-shifted geometric mean, (b) node counts, n(b, B), across the instances that are solved by all
the strategies in B, or (c) number of solved instances within the time limit, denoted by w(b). Similarly, one
can also deﬁne a metric based on the optimality gap of unsolved instances.

In exploring generalization metrics, we need to further distinguish between diﬀerent objectives that a
practitioner might have. For example, (a) Minimum solving time: the branching strategy that solves the
instances as fast as possible; (b) Maximum number of instances solved: a branching strategy that solves the
most instances to optimality within a certain time budget, irrespective of whether the strategy solved the
instances fastest or not; or (c) Minimum node count: a branching strategy that yields the smallest trees.

Thus, even though the models are trained to mimic the strong branching oracle, thereby expecting to
yield the smallest tree, we incorporate harder-to-formulate objectives by selecting the learned model using
a combination of aggregate performance measures. Thus, to incorporate the objective (a), we select the
branching strategy as per

0 = {j | j ∈ B,

B

t(j) ≤ min
B

t(b) + (cid:15)t},

b?
time

= arg min
b∈B0

n(b, B

0),

(7)

where we introduce (cid:15)t to account for the variability that might result from the hardware-dependent solving
time measurement. Thus, (cid:15)t = 1 considers all the branching strategies b such that t(b) < minb∈B t(b) + 1.
The objective corresponding to solving the largest number of instances in the minimum amount of time can
be formulated as

0 = {j | j ∈ B, w(j) = max

B

w(b)},

00 = {j | j ∈ B

0

,

B

t(b) + (cid:15)t},

(8)

B
t(j) ≤ min
B0

b?
solved−time

= arg min
b∈B00

n(b, B

00 ),

which is slightly diﬀerent than the objective to select the strategy with the minimum time that also solves
the most instances,

0 = {j | j ∈ B,

B

00 = {j | j ∈ B

0

B

t(j) ≤ min
B

t(b) + (cid:15)},

, w(j) = max
B0

w(b)},

(9)

b?
time−solved

= arg min
b∈B00

n(b, B

00).

7

We discuss other possible formulations in Appendix B.

6 Experiments

We consider four benchmark problem families, namely Combinatorial Auctions, Minimum Set Covering,
Capacitated Facility Location, and Maximum Independent Set, to evaluate the performance of our proposed
methods. These are the same problems that have been used extensively in the “learning to branch"
literature (Gupta et al., 2020; Scavuzzo et al., 2022; Etheve et al., 2020; Sun et al., 2020) since introduced
in Gasse et al. (2019). Speciﬁcally, we collect a dataset D for each of the problem families by solving the
Small instances using SCIP (Gleixner et al., 2018) with the strong branching heuristic. Our models are
trained to minimize the objective functions as described in Equations (2), (3), (5), and (6). Due to the space
constraints, we leave the instance size, dataset collection, and training speciﬁcations to Appendices C, D,
and E, respectively.

Baselines. To demonstrate the utility of the proposed models, we consider two types of widely used branching
strategies: (a) Reliability Pseudocost Branching (RPB): Given the online statistical learning aspect of this
heuristic, it has been shown to be the most promising among all. The commercial solvers use this as a
default branching strategy; (b) TunedRPB: Given that we are focused on learning a branching strategy
suitable for problem sets coming from a ﬁxed distribution, we search through the parameters of RPB to
select the ones suited best for the problem family. Speciﬁcally, we run a grid search on two RPB parameters
representing a trade-oﬀ between run time and the iterative performance (see Appendix F); We select the
best performing parameters using the model selection framework from Section 5, making this tuned heuristic
directly comparable to our method; (c) Graph Neural Networks (GNN): As proposed by Gasse et al. (2019),
and widely used in the community, we use GNNs trained on the same dataset as our proposed models. These
models have been shown to be the best among all the other machine learning based models.

Evaluation. We replace the variable selection heuristic in SCIP Gleixner et al. (2018) with the strategy
to be evaluated. For each of the four problem families, we solve 100 randomly generated instances across
three scales: Small, Medium, and Big (see Appendix C). Since Combinatorial Auctions’ big instances are
solved fairly quickly, we extend the evaluation to slightly bigger instances. Increasing scale is expected to
increase the running time of the B&B algorithm. All Small and Medium instances used for evaluation are
diﬀerent from those used for training and model selection. Given the NP-Hard nature of the problems, we
used the time limit of 45 minutes per instance to solve these instances using SCIP (Gleixner et al., 2018). See
Appendix G for the speciﬁcations of the hardware used for evaluation.

Evaluation Metrics. As per the standard practices in the MILP community, the performance of B&B solvers
is benchmarked across the following metrics: (a) Time: 1-shifted geometric mean1 of solving time of all the
instances, irrespective of whether the instance was solved to optimality or not; (b) Nodes: 1-shifted geometric
mean of the number of nodes of the commonly solved instances (denoted by c in parenthesis for clarity) across
all branching strategies; note that this is a hardware-independent measure of performance; and (c) Wins:
number of instances that were solved (to optimality) the fastest by the branching strategy; (d) Solved: Total
number of instances solved within the time limit; and (e) Time: 1-shifted geometric mean of solving time of
the commonly solved instances. The commonly solved instances are a subset of instances that have been
solved to optimality by all the branching strategies.

Model Hyperparameters. We consider a grid search over the following hyperparameters: (a) loss-target ∈
{y, z}, where z refers to the modiﬁed loss function proposed in Section 4.1 and y the typical loss function
that focuses only on the top strong branching variable; (b) λl2 ∈ {0.0, 0.01, 0.1, 1.0}, the l2-regularization
penalty; and (c) λP AT ∈ {0.01, 0.1, 0.2, 0.3} to deﬁne the strength of the PAT lookback loss term proposed
in Section 4.2. While we consider λl2 for both θy and θz, for θP AT we consider the best performing model
among all the hyperparameters {loss-target, λl2, λP AT }. For each hyperparameter conﬁguration, we train ﬁve

1For a complete deﬁnition, refer to Appendix A.3 in Achterberg (2007)

8

randomly seeded models as described in Appendix E. Finally, θy represents the baseline GNN from Gasse
et al. (2019) without any of our proposed modiﬁcations.

Figure 3: Maximum (across the hyperparameters) mean validation accuracy (1-standard deviation) of the
proposed models is better than the baseline GNNs (θy). We see that the models trained with smoothed
target (θz) and those with PAT lookback loss term (θP AT ) result in better validation accuracy.

6.1 Results

Hyperparameter Selection. Figure 3 shows validation accuracy of the best performing hyperparameters for
θy, θz, and θP AT according to the validation accuracy on collected dataset D. However, to select the models
based on the generalization performance, we solve 100 randomly generated medium instances and gather the
metrics as described above. Table 1 shows the selection of hyperparameters for θP AT as per various criteria
(see Appendix H for the best parameters for θy and θz). We observe that the models selected by validation
accuracy are not always preferred by the selection criteria deﬁned on the evaluation of medium instances.
Second, we observe that Eq. (8) and (9) may or may not have consensus among them; a strategy might have
the fastest solving times for all instances except one, but other strategies might solve all the instances in just
slightly more time. Third, we observe that the time limit per instance, T , does play a role in model selection;
both facilities and indset have a diﬀerent preference. Finally, we observe that even though the modiﬁed target
z is not preferred by all the problem families, there is a consensus for the use of PAT lookback loss term.

Table 1: Best performing hyperparameters {loss-target, λl2, λP AT } for θP AT . We see that loss-target = z is
preferred by some problem families, while the PAT lookback term is preferred by all. In addition, the model
selection criteria does impact the chosen hyperparameters.

Problem family

Validation Accuracy Eq. (8)(T = 30mins) Eq. (9)(T = 30mins) Eq. (8)(T = 15mins)

cauctions
setcover
facilities
indset

{y, 0.0, 0.01}
{y, 0.0, 0.1}
{z, 0.0, 0.0}
{y, 0.0, 0.1}

{z, 0.0, 0.1}
{y, 0.0, 0.1}
{z, 0.0, 0.1}
{z, 0.01, 0.2}

{z, 0.0, 0.1}
{y, 0.0, 0.1}
{z, 0.0, 0.1}
{y, 0.1, 0.3}

{z, 0.0, 0.1}
{y, 0.0, 0.1}
{z, 0.0, 0.3}
{y, 0.1, 0.3}

Validation performance on Medium instances. To see the interplay between our proposals in Section 4 and
the model selection framework proposed in Section 5, we compare the performance of the selected models
using Eq. (8) (T=30mins) for each of θy, θz, θyP AT , and θzP AT . Figure 4 compares their performance on
medium instances with respect to Time and Nodes. To accommodate diﬀerent scales of time and node, we

9

Figure 4: We plot the range-normalized (range is speciﬁed in parenthesis) Time and Node performance of
the selected models as per Eq. (8). The centered "X" black mark shows the ﬁnally selected models that
will be used for evaluating the performance on bigger instances. The points with "red" outline shows the
performance of the models selected according to the best validation accuracy (∗we omit such models for
indset as it distorts the scale of the plot; see Appendix I for complete data)

plot the range-normalized values of these measures and show the range of these measures in parentheses. The
centered black marks show the ﬁnal models selected as per Eq. (8). The models chosen as per validation
accuracy are shown with transparent marks with red border.

We make the following observations.
First, the PAT lookback term is ben-
eﬁcial for the generalization perfor-
mance most of the time. We ob-
serve that for the facilities set of
problems, the current GNNs already
respect the lookback condition suﬃ-
ciently well that the proposed modi-
ﬁcations do not yield signiﬁcant im-
provements compared to θy. Second,
we see that the cauctions and set-
cover models perform equally well
with respect to time, thereby making
the node count an important crite-
rion for identifying better branching
models. This is especially impor-
tant because time measurements are
hardware-dependent and thus not
as reliable. Third, central to the
motivation of our model selection
framework, the models chosen as per
validation accuracy do not fare well
on practical objectives such as Time
and Nodes.

Table 2: Performance of branching strategies on Big evaluation
instances. The best performing numbers are in bold. Refer to Section 6
for metrics. Since all of the Big Combinatorial Auctions instances were
solved to optimality, we extend the evaluation to Bigger instances
(∗since only 5 instances were solved using FSB, we omit it here; See
Appendix J for the full results including those on Big instances.)

Model
fsb∗
rpb
tunedRPB
gnn
gnn-PAT (ours)

fsb
rpb
tunedRPB
gnn
gnn-PAT (ours)

fsb
rpb
tunedRPB
gnn
gnn-PAT (ours)

Time
n/a
626.81
644.20
507.06
477.26

2700
1883.32
1851.83
1708.99
1601.28

917.39
737.66
751.27
646.03
581.91

Time (c) Wins

n/a
434.92
450.06
333.59
310.22

n/a
1
0
14
69
Combinatorial Auction (Bigger)

Solved
n/a
80
80
80
84

n/a
1213.57
1168.58
991.07
892.85

0
1
0
5
54
Set Covering

0
47
48
54
59

758.19
607.19
619.27
525.80
471.20

8
3
2
16
66
Capacitated Facility Location

85
92
92
92
95

Nodes (c)
n/a
17 979
18 104
17 145
16 388

n/a
58 766
58 155
39 535
38 385

50
104
97
293
304

fsb
rpb
tunedRPB
gnn
gnn-PAT (ours)

2700
1984.91
2016.85
1207.62
1035.32

0
0
n/a
32
0
888.04
33
0
952.25
65
14
279.71
70
56
233.97
Maximum Independent Set

n/a
12 407
12 940
7934
6122

instances.
on Big
Evaluation
Given that
sets of Small and
Medium instances have already
been used in training and selection
of the ﬁnal models, we leave the evaluation on additional sets of unseen instances from these families to

10

Appendix K. Here, we evaluate the performance of the selected models (as per Eq. (8)(T=30mins)) on bigger
instances. We use the same instance scaling scheme as proposed by Gasse et al. (2019) (see Appendix C).
Table 2 shows various evaluation metrics as computed from the evaluation of 100 randomly generated Big
instances. Since Big instances of Combinatorial Auctions are solvable by all the strategies, we extend the
scale of these instances to Bigger instances. Speciﬁcally, we observe that θP AT (GNN-PAT) outperforms the
baseline model (GNN) in all the problem families on all fronts – Time, Wins, Solved and Nodes. As an
example, for Maximum Independent Set problems, we observe a 15% decrease in Time and a 22% decrease in
Nodes. Notably, GNN-PAT increases the number of “Solved” instances by 4 to 5 for all but one problem
family. Solving additional instances to optimality is a testament to the improved branching decisions brought
about by GNN-PAT.

Finally, as noticed above, the learned models for Maximum Independent Set might result in a diﬀerent
hyperparameter conﬁguration based on the selection criterion. Therefore, we compare the performance of the
GNN-PAT models that are selected by each of the Eqs. (8) and (9) against GNN in Table 3. We observe
that as per the selection criterion of Eq. (8), the branching strategy solved the most number of instances.
However, as Eq. (9) prefers the strategy with overall lower running time, we observe superior performance of
the branching strategy on Time of commonly solved instances. These observations conﬁrm that the proposed
model selection approach yields the expected outcomes on unseen test instances.

Table 3: Performance measures on branching strategies selected as per
diﬀerent criteria speciﬁed in Eqs. (8) and (9) and the speciﬁed time limit
per instance T . We observe that each criterion supports the respective
measure on scaled-up instances.

Similarly, we look at the eﬀect
of the time limit T on the se-
lection criterion. The models
selected as per Eq. (8) for fa-
cilities diﬀer depending on the
speciﬁed time limit T (see Ta-
ble 1). We look at the how
the time limit might aﬀect gen-
eralization performance in Ta-
ble 3. Speciﬁcally, we observe
that insuﬃcient time to eval-
uate Medium instances may
lead to suboptimal hyperpa-
rameters. To conclude, we’ve
shown that the model selection criterion will impact generalization performance signiﬁcantly.

Solved
65
70
66
Maximum Independent Set

92
95
94
Capacitated Facility Location

gnn
gnn-PAT (Eq. (8)(T=30mins))
gnn-PAT (Eq. (8)(T=15mins))

Nodes

29 573
23 574
21 162

314
326
388

Model
gnn
gnn-PAT (Eq. 8)
gnn-PAT (Eq. 9)

Time
1207.62
1035.32
1063.12

646.03
581.91
635.04

753.81
621.42
613.96

562.20
503.85
551.53

Time (c) Wins

1
38
31

13
58
24

7 Discussion

An objective in this paper was to imitate the strong branching behavior more closely by taking advantage of
its lookback property. The proposals in Section 4 are aimed at doing so. A post-hoc analysis shows up to
16% improvement in GNNs ability to follow the lookback property (see Appendix L). Such improvements are
evident at various distances from the root node (see Appendix M).

We can argue that the proposals in Section 4 are regularizers or inductive biases. Although the modiﬁed
target and the PAT lookback term were inspired to induce the required oracle behavior, owing to the lack
of interpretability of GNNs, we cannot attribute the GNN-PATs’ performance improvements to the ability
to follow the lookback property with 100% certainty. However, if we consider that the proposals did cause
the observed improvements, we might consider it as an inductive bias. Inductive biases are deﬁned as any
pre-coded knowledge about the target behavior, such as neural network architecture, choice of features, or
loss function.

Further, the PAT lookback loss term is similar to the objective function in knowledge distillation (Hinton
et al., 2015), i.e., we are minimizing the cross-entropy between the logits of one model (child node) and
the other model (parent node). Whereas in the original distillation framework, there are two separate
models (teacher and student) that act on the same inputs, the PAT lookback term can be understood as a
form of cross-distillation that acts through the same model but on diﬀerent observations which share some

11

characteristics (e.g., output logits). Thus, we can understand that the PAT lookback term distills knowledge
from the parent node to the child node, as was intended.

Finally, our model selection framework enables us to incorporate complex metrics like Time and Nodes into
hyperparameter selection criterion. We hope that this framework will help aligning the research in machine
learning methods for MILP solvers with practitioners’ varied objectives.

Limitations. As illustrated in Figure 7, the GNNs for facilities instances are capable of capturing the
lookback condition 80% of the time. Since it is not possible to consider all possible problem families and
their varied formulations, we cannot make a deﬁnitive claim on whether our proposed modiﬁcations will be
useful all the time. Therefore, we recommend checking for the prevalence of the lookback condition to get
some idea of expected improvement.

Further, due to time and resource constraints, we restricted the evaluation for model selection to just 100
Medium instances. However, for a more robust selection, we suggest using larger set of instances. One can
also consider setting the size of Medium instances such that majority of them are solved, thereby resulting in
robust selection of better performing hyperparameters; see Table 1 on how the best hyperparameters vary
according to time limit per instance T and Table 3 for the eﬀect of T on generalization performance.

Finally, we emphasize that the model selection criteria is very much dependent on how these models will be
deployed. For example, a practitioner might only be concerned with solving maximum number of instances
(to optimality) while ensuring minimum optimality gap in the unsolved instances. This objective can be
formulated as a diﬀerent criteria. Considering all such formulations is beyond the scope of our work.

Future work. Although we did not specify the minimum optimality gap as the objective of the branching
strategies, we run a post-hoc analysis to compare 1-shifted geometric mean of optimality gaps of the commonly
unsolved instances (lower is better). Figure 5 shows that, except for setcover instances, the proposed branching
strategies are able to close larger gaps than the rest. We acknowledge that, depending on the use, the
optimality gap might be of primary importance to the practitioner. We think that the exploration of
optimality gap as a secondary objective could be an important future work.

As evident from the gaps in Figure 7 (Ap-
pendix L) and Figure 8 (Appendix M), we
plan to design more ways to incorporate
the lookback condition explored in this
work. While we studied how the parent
and child nodes in a B&B tree are related
with respect to a simple PAT lookback
condition, there still exists several ways
in which such nodes can be related (see
Appendix N for another example). Thus,
the design of machine learning algorithms
to discover and exploit such dependencies
could be an important direction for fu-
ture research. Moreover, such machine
learning-aided discoveries can be equally
important for the MILP community to
inspire the design of novel heuristics or
improve the existing manually-designed
heuristics applicable to general instances.

8 Conclusion

Figure 5: Post-hoc analysis of optimality gap of commonly un-
solved instances (number is shown in parenthisis next to the
problem family label) shows that θP AT is able to achieve the best
optimality gap (except for setcover) even though it is not the
primary objective speciﬁed in the model training.

With the huge gap between the performance of deep learning based heuristics and the oracle heuristics, we
expect that the research eﬀorts might require more in-depth investigation of how to imbue these models with
the same “reasoning" as the oracles themselves. In this line of thought, we investigated how the parent-child

12

nodes of a B&B tree are related to each other under the oracle heuristic. We found that quite often, the
parent’s second best choice is the child’s best choice. To incorporate this lookback condition into model
training, we designed two methods to align the models more closely with the strong branching oracle’s
behavior. We believe that this investigative approach to imitating oracle behavior could be a useful way to
close the gap between machine learning and the oracle heuristics.

Broader Impact Statement

This paper continues the exploration of the use of machine learning techniques for the most critical step in
branch-and-bound methods, i.e., variable selection. Branch and bound is the method of choice for solving a
myriad of discrete optimization problems appearing in all sort of applications (a few named in the introduction
of this paper) and it is the basic scheme of all commercial and open-source discrete optimization solvers. Thus,
the impact of the research in the area is potentially very high, not only from a methodological perspective
but also in terms of day to day challenges that we all face, including drug discoveries and climate change.

This paper signiﬁcantly advances the research in the area by observing for the ﬁrst time a hidden pattern,
the lookback phenomenon, in the statistical evolution of the most successful heuristic for variable selection.
The paper proposes several methods to exploit such a phenomenon and makes a signiﬁcant step forward on
the use of ML for discrete optimization.

References

Jawad Abrache, Teodor Gabriel Crainic, Michel Gendreau, and Monia Rekik. Combinatorial auctions. Annals

of Operations Research, 153(1):131–164, 2007.

Tobias Achterberg. Constraint Integer Programming. Doctoral thesis, Technische Universität Berlin,
Fakultät II - Mathematik und Naturwissenschaften, Berlin, 2007. URL http://dx.doi.org/10.14279/
depositonce-1634.

Tobias Achterberg, Thorsten Koch, and Alexander Martin. Branching rules revisited. Operations Research
ISSN 0167-6377. doi: https://doi.org/10.1016/j.orl.2004.04.002. URL

Letters, 33(1):42 – 54, 2005.
http://www.sciencedirect.com/science/article/pii/S0167637704000501.

Alejandro Marcos Alvarez, Quentin Louveaux, and Louis Wehenkel. A machine learning-based approximation

of strong branching. INFORMS Journal on Computing, 29(1):185–195, 2017.

David Applegate, Robert Bixby, Vašek Chvátal, and William Cook. Finding cuts in the TSP. Technical

report, DIMACS, 1995.

Egon Balas and Andrew Ho. Set covering algorithms using cutting planes, heuristics, and subgradient

optimization: a computational study. In Combinatorial Optimization, pp. 37–60. Springer, 1980.

Gérard Cornuéjols, Ranjani Sridharan, and Jean-Michel Thizy. A comparison of heuristics and relaxations
for the capacitated plant location problem. European journal of operational research, 50(3):280–297, 1991.

Marc Etheve, Zacharie Alès, Côme Bissuel, Olivier Juan, and Saﬁa Kedad-Sidhoum. Reinforcement learning
for variable selection in a branch and bound algorithm. In International Conference on Integration of
Constraint Programming, Artiﬁcial Intelligence, and Operations Research, pp. 176–185. Springer, 2020.

Seyed Alireza Fayazi and Ardalan Vahidi. Mixed-integer linear programming for optimal scheduling of
autonomous vehicle intersection crossing. IEEE Transactions on Intelligent Vehicles, 3(3):287–299, 2018.

Brian A Foster and David M Ryan. An integer programming approach to the vehicle scheduling problem.

Journal of the Operational Research Society, 27(2):367–384, 1976.

Maxime Gasse, Didier Chételat, Nicola Ferroni, Laurent Charlin, and Andrea Lodi. Exact combinatorial
optimization with graph convolutional neural networks. In Advances in Neural Information Processing
Systems, pp. 15554–15566, 2019.

13

Ambros Gleixner, Michael Bastubbe, Leon Eiﬂer, Tristan Gally, Gerald Gamrath, Robert Lion Gottwald,
Gregor Hendel, Christopher Hojny, Thorsten Koch, Marco E. Lübbecke, Stephen J. Maher, Matthias
Miltenberger, Benjamin Müller, Marc E. Pfetsch, Christian Puchert, Daniel Rehfeldt, Franziska Schlösser,
Christoph Schubert, Felipe Serrano, Yuji Shinano, Jan Merlin Viernickel, Matthias Walter, Fabian Wegschei-
der, Jonas T. Witt, and Jakob Witzig. The SCIP Optimization Suite 6.0. Technical report, Optimization
Online, July 2018. URL http://www.optimization-online.org/DB_HTML/2018/07/6692.html.

Prateek Gupta, Maxime Gasse, Elias Khalil, Pawan Mudigonda, Andrea Lodi, and Yoshua Bengio. Hybrid
models for learning to branch. Advances in neural information processing systems, 33:18087–18097, 2020.

He He, Hal III Daumé, and Jason Eisner. Learning to search in branch-and-bound algorithms. In Advances

in Neural Information Processing Systems 27, pp. 3293–3301, 2014.

Geoﬀrey Hinton, Oriol Vinyals, and Jeﬀ Dean. Distilling the knowledge in a neural network. arXiv preprint

arXiv:1503.02531, 2015.

Zeren Huang, Kerong Wang, Furui Liu, Hui-Ling Zhen, Weinan Zhang, Mingxuan Yuan, Jianye Hao, Yong
Yu, and Jun Wang. Learning to select cuts for eﬃcient mixed-integer programming. Pattern Recognition,
123:108353, 2022.

Ivan Kantor, Jean-Loup Robineau, Hür Bütün, and François Maréchal. A mixed-integer linear programming
formulation for optimizing multi-scale material and energy integration. Frontiers in Energy Research, 8:49,
2020.

Elias B. Khalil, Pierre Le Bodic, Le Song, George Nemhauser, and Bistra Dilkina. Learning to branch in
mixed integer programming. In Proceedings of the Thirtieth AAAI Conference on Artiﬁcial Intelligence,
pp. 724–731, 2016.

Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint

arXiv:1412.6980, 2014.

A. H. Land and A. G. Doig. An automatic method of solving discrete programming problems. Econometrica,

28(3):pp. 497–520, 1960.

Kevin Leyton-Brown, Mark Pearson, and Yoav Shoham. Towards a universal test suite for combinatorial
auction algorithms. In Proceedings of the 2nd ACM conference on Electronic commerce, pp. 66–76, 2000.

Jeﬀ Linderoth and Martin Savelsbergh. A computational study of search strategies for mixed integer

programming. INFORMS Journal on Computing, 11:173–187, 05 1999. doi: 10.1287/ijoc.11.2.173.

Andrea Lodi and Giulia Zarpellon. On learning and branching: a survey. TOP, 25:207–236, 2017.

Vinod Nair, Sergey Bartunov, Felix Gimeno, Ingrid von Glehn, Pawel Lichocki, Ivan Lobov, Brendan
O’Donoghue, Nicolas Sonnerat, Christian Tjandraatmadja, Pengming Wang, et al. Solving mixed integer
programs using neural networks. arXiv preprint arXiv:2012.13349, 2020.

Tony Nowatzki, Michael Ferris, Karthikeyan Sankaralingam, Cristian Estan, Nilay Vaish, and David Wood.
Optimization and mathematical modeling in computer architecture. Synthesis Lectures on Computer
Architecture, 8(4):1–144, 2013.

Christos H. Papadimitriou and Kenneth Steiglitz. Combinatorial Optimization: Algorithms and Complexity.

Prentice-Hall, Inc., USA, 1982. ISBN 0131524623.

Adam Paszke, Sam Gross, Soumith Chintala, Gregory Chanan, Edward Yang, Zachary DeVito, Zeming Lin,

Alban Desmaison, Luca Antiga, and Adam Lerer. Automatic diﬀerentiation in pytorch. 2017.

Yves Pochet and Laurence A Wolsey. Production planning by mixed integer programming. Springer Science &

Business Media, 2006.

14

Lara Scavuzzo, Feng Yang Chen, Didier Chételat, Maxime Gasse, Andrea Lodi, Neil Yorke-Smith, and Karen

Aardal. Learning to branch with tree mdps. arXiv preprint arXiv:2205.11107, 2022.

Haoran Sun, Wenbo Chen, Hui Li, and Le Song. Improving learning to branch via reinforcement learning.

2020.

Laurence A. Wolsey. Integer Programming. Wiley-Blackwell, 1988.

Giulia Zarpellon, Jason Jo, Andrea Lodi, and Yoshua Bengio. Parameterizing branch-and-bound search trees

to learn branching policies. arXiv preprint arXiv:2002.05120, 2020.

Appendix

A Lookback property as a function of depth-decile

Figure 6: The gap between the frequency of the lookback condition per depth-decile for the strong branching
oracle (Ground truth) and the traditionally trained GNNs (GNN) presents an opportunity to improve the
GNN models. See Section 4 for the dataset collection procedure.

15

B Model Selection Objectives

To incorporate the objective of maximum solved instances, we select the branching strategy as per,

B

0 = {j | j ∈ B, w(j) = max

B

w(b)},

b? = arg min

n(b, B

0)

b∈B0

(10)

Finally, to select the strategy only with the minimum node count, we select the branching strategy as per,

b? = arg min

n(b, B)

b∈B

(11)

C Instance Speciﬁcations

We follow the procedure outlined by Gasse et al. (2019) to generate and scale the instances. A well commented
and functional code to generate these instances can be found on the authors’ Github repo2. For convenience,
we describe the procedure here.

C.1 Combinatorial Auction

These instances are generated following the arbitrary scheme described in the section 4.3 of Leyton-Brown
et al. (2000). The scalable parameters are the number of items and the number of bids.

Table 4: Parameters for Combinatorial Auctions

Instance Size

number of items

number of bids

use

Small-Training
Small-Validation
Medium-Validation
Small
Medium
Big
Bigger

100
100
200
100
200
300
350

500
500
1000
500
1000
1500
1750

Dataset Collection & Training
Dataset Collection & Validation
Validation Evaluation
Test Evaluation
Test Evaluation
Test Evaluation
Test Evaluation

C.2 Set Covering

These instances are generated using the method described in Balas & Ho (1980). The scalable parameters
are number of items, where the number of sets are ﬁxed to 1000.

Table 5: Parameters for Minimum Weighted Set Cover. Number of sets is ﬁxed to 1000.

Instance Size

number of items

use

Small-Training
Small-Validation
Medium-Validation
Small
Medium
Big

500
500
1000
500
1000
2000

Dataset Collection & Training
Dataset Collection & Validation
Validation Evaluation
Test Evaluation
Test Evaluation
Test Evaluation

2https://github.com/ds4dm/learn2branch/blob/master/01_generate_instances.py

16

C.3 Capcitated Facility Location

These instances are generated the meethod described by Cornuéjols et al. (1991). Fixing the number of
facilities to 100, the scalable parameter is the number of customers.

Table 6: Parameters for Capcitated Facility Location. Number of facilities is ﬁxed to 100.

Instance Size

number of customers

use

Small-Training
Small-Validation
Medium-Validation
Small
Medium
Big

100
100
200
100
200
400

Dataset Collection & Training
Dataset Collection & Validation
Validation Evaluation
Test Evaluation
Test Evaluation
Test Evaluation

C.4 Maximum Independent Set

These instances are generated by formulating the maximum independent set problem on a randomly generated
Barabási-Albert with edge probability of 0.4. The scalable parameter is the number of nodes.

Table 7: Parameters for Maximum Independent Set. Aﬃnity is ﬁxed to 4.

Instance Size

number of nodes

use

Small-Training
Small-Validation
Medium-Validation
Small
Medium
Big

750
750
1000
750
1000
1500

Dataset Collection & Training
Dataset Collection & Validation
Validation Evaluation
Test Evaluation
Test Evaluation
Test Evaluation

D Data Generation

For each of the problem family, we generate 10,000 Small random instances to collect training data, 2,000
Small random instances to collect the validation data, and 20 Medium instances for model selection. We
use SCIP Gleixner et al. (2018) with strong branching heuristic to solve the randomly generated instances
and collect data of the form D = {(G0
) | i ∈ {1, 2, 3, ..., N }}, as described in the main text. We
i , G1
collected a total of 150,000 training observations and 30,000 validation observations.

i , s1
i

i , s0

The hand-engineered features to the GNNs are same as Gasse et al. (2019). For a full description of these
features, pleasee see Section 2 in Supplementary material of Gupta et al. (2020).

E Training Speciﬁcations

Our models are all implemented in PyTorch (Paszke et al., 2017). Following Gupta et al. (2020), we didn’t
change any of the training parameters, for example, we used the Adam (Kingma & Ba, 2014) optimizer with
the learning rate of 1e−3, training batch size of 32, and a learning rate scheduler to reduce the learning rate
by a factor of 0.2 in the absence of any improvement in the validation loss for the last 15 epochs Moreover,
we use the early stopping criterion to stop the training if the validation loss doesn’t improve over 30 epochs.
We validate the performance of model on the validation dataset after every epoch consisting of 10K random
training samples. For each problem family, we trained models with ﬁve random seeds.

17

F TunedRPB

We consider two parameters in RPB to negotiate the trade-oﬀ between the compactness of the resulting B&B
tree and the computational time. Broadly, RPB performs strong branching on MaxLookahead candidates
until it has collected enough information to reliably act on it. The selection of candidates is prioritized by the
reliability of pseudoscores (statistically learned score to estimate bound improvement per fractional rounding
of the variable) collected during the strong branching operations. If the minimum psuedoscore obtained
by withere rounding up or rounding down of the integer-constrained vairable is less than MaxReliable, the
candidate is deemed unreliable, thereby prioritizing it in the next rows. Thus, we observe the following
trade-oﬀ by varying these two parameters: (i) MaxReliable: lower values prefer faster solving times at the
expense of larger trees, and (ii) MaxLookahead: higher values prefer shorter trees at the expense of more
computational time.

To have a version of RPB that is trained on the training instances of interest, we run a hyperparameter
grid-search on the following values -

1. MaxLookahead: {6, 7, 8, 9, 10, 11}

2. MaxReliable: {3, 4, 5, 6, 7, 8}

Speciﬁcally, we followed the procedure as described in Section 5 to solve 100 randomly generated medium
instances, and select the best performing hyperparameters according to Eq. (8).

G Evaluation Speciﬁcation

The evaluation on Big instances is performed by using SCIP 6.0.1 installed on an Intel(R) Xeon(R) CPU
E5-2650 v4 @ 2.20GHz. The learned neural networks, as branching models, are run on NVIDIA-TITAN Xp
GPU card with CUDA 10.1. All of the main evaluations are done, as is a standard practice, while ensuring
that the ratio of the number of cores on the machine to the load average is more than 4. This condition
ensures that each process on the machine has at least 4 CPUs at a time.

The model selection is carried out by solving Medium instances on the shared cluster as speciﬁed by Nair
et al. (2020) (see Section 12.7). Speciﬁcally, we solve a benchmark MIPLIP problem (‘vpm2.mps‘) every
60 seconds to collect the solving time statistics. Given the solving times of the benchmark problem on the
reference machine, we recalibrate the solving time of the instance, which is used as the ﬁnal running time. We
solve 20 independently generated Medium instances with ﬁve seeds resulting in 100 independent evaluations.

H Hyperparameters & Model Selection

We use the ridge regression to penalize the parameters θy and θz. In addition to λl2, θP AT searches over
target and λP AT . Following values were used for the hyperparameter search -

1. λl2 = {0.01, 0.1, 1.0}

2. λP AT = {0.01, 0.1, 0.2, 0.3}

3. target = {y, z}

Finally, Table 8 shows the best performing hyperparameters according to Eq. (8) for θy, θz, and θP AT . As
observed in Gupta et al. (2020), we found that the l2-regularization is useful for the performance of indset
models.

18

Table 8: Best performing hyperparameters according to Eq. (8)

problem family

cauctions
setcover
facilities
indset

θy{λl2}
0.0
0.0
0.0
0.1

θz{λl2}
0.0
0.0
0.0
0.1

θP AT {target, λl2, λP AT }
{z, 0.0, 0.1}
{y, 0.0, 0.1}
{z, 0.0, 0.1}
{z, 0.01, 0.2}

I Performance on medium validation instances

Table 9 shows the data that was used to plot the points in Figure 4. Due to the huge variability in the
performance metrics of indset we omit the performance of θaccuracy from Figure 4.

Table 9: Data for Figure 4

Model Metric

cauctions

setcover

facilities

indset

θy

θz

θyP AT

θzP AT

θaccuracy

Time
Nodes

Time
Nodes

Time
Nodes

Time
Nodes

Time
Nodes

14.66
1077

14.82
1089

13.83
1031

13.65
1022

14.22
1083

38.21
1546

38.08
1527

37.92
1503

37.89
1513

37.92
1503

159.60
421

157.64
437

160.13
429

153.81
420

159.59
421

73.16
3102

76.61
3335

74.80
3201

71.32
2943

572.39
9430

J Full performance of Combinatorial Auction models on Big and Bigger instances

Since Big Combinatorial Auction instances are solved by all the strategies, we show them in Table 10. We
extend the size of these instances to Bigger and show the evaluation in the main paper without FSB as there
are only 5 instances solved distorting the comparison of Time (c) and Node (c). See Table 11 for the full
results.

Table 10: Performance of variable selection strategies on Big instances for Combiatorial Auctions instances.
See Appendix C for the instance scaling parameters.

Model
fsb
rpb
tunedRPB
gnn
gnn-PAT (ours)

Time
2075.60
202.78
205.57
139.30
141.63

Time (c) Wins
1643.78
113.47
112.80
72.17
73.31

0
1
0
64
35

Solved
53
100
100
100
100

Nodes (c)
336
4640
4611
4142
3754

Table 11: Performance of variable selection strategies on Bigger instances for Combinatorial Auctions
instances. See Appendix C for the instance scaling parameters.

Model
fsb
rpb
tunedRPB
gnn
gnn-PAT (ours)

Time
2623.50
626.81
644.20
507.06
477.26

Time (c) Wins
1618.62
174.88
188.22
126.94
130.72

0
1
0
14
69

Solved
5
80
80
80
84

Nodes (c)
200
5719
6191
5630
5757

Optimality Gap (16)
0.033 102
0.009 555
0.009 605
0.008 307
0.007 844

19

K Performance on small and medium instances

Table 12 shows the performance of the selected strategies as per Eq. (8) on small and medium instances. This
table is a counterpart to the Table 2.

Table 12: Performance of branching strategies on evaluation instances. We report geometric mean of solving
times, number of times a method won (in solving time) over total ﬁnished runs, and geometric mean of
number of nodes. Refer to section 6 for more details. The best performing results are in bold.

Model
fsb
rpb
tunedRPB
gnn
gnn-PAT (ours)

fsb
rpb
tunedRPB
gnn
gnn-PAT (ours)

fsb
rpb
tunedRPB
gnn
gnn-PAT (ours)

fsb
rpb
tunedRPB
gnn
gnn-PAT (ours)

Small

Time

5.85
3.89
3.72
2.10
2.18

Wins
0
0
0
82
18

Solved
100
100
100
100
100

Nodes

6
11
11
71
72

Time
127.56
25.31
24.70
13.15
13.25

Combinatorial Auction

Medium

Wins
0
0
0
58
42

Solved
100
100
100
100
100

Nodes
72
696
591
693
654

26.17
13.41
13.63
9.37
9.03

41.61
36.58
37.56
27.20
29.46

626.33
58.03
56.99
35.04
31.96

0
0
0
5
95

3
3
1
78
15

0
0
1
28
71

100
100
100
100
100

100
100
100
100
100

93
100
100
100
100

531.47
91.33
93.60
64.81
58.48

17
54
48
136
134
Set Covering

14
22
21
113
112

264.67
206.28
211.77
146.41
159.95

Capacitated Facility Location

54
702
697
1000
455

1634.40
144.92
143.88
76.53
72.00

Maximum Independent Set

0
0
0
1
99

3
1
2
64
30

0
0
2
36
62

75
100
100
100
100

98
100
100
100
100

60
100
100
100
100

117
1119
1131
1030
997

73
147
140
320
329

46
770
748
795
789

L Post-hoc analysis of the lookback property

Figure 7: Statistics of the lookback condition as observed when the problems are solved using the strong
branching oracle (Ground truth). We also show how many times traditional GNNs (GNN) and our proposed
GNNs (PAT-GNN) respect the lookback condition on the same dataset. Note that the displayed statistics
are on the oﬄine dataset and do not reﬂect the ﬁnal inference-time performance of the models. We ﬁnd that
the small improvements shown here results in large gains in the inference time performance (see Section 6)

20

M Post-hoc depth-decile analysis of the lookback property

Figure 8: The gap between the frequency of the lookback condition per depth-decile for the strong branching
oracle (Ground truth) and the traditionally trained GNNs (GNN) presents an opportunity to improve the
GNN models (PAT-GNN). See Section 4 for the dataset collection procedure.

N Same strong branching decisions at the sibling nodes

Any MILP solving procedure results in a tree of sub-problems, where each node shares some characteristics
with its parent. This line of resemblance can eventually be traced all the way back to the original MILP. For
example, the bipartite graph structure remains the same throughout the tree, a fact exploited by Gupta et al.
(2020) to design CPU-eﬃcient GNN-based models for learning to branch.

We investigate more of such dependencies among the B&B tree nodes of problem instances from the
benchmark problem families (Gasse et al., 2019) (see Appendix C for more details), which we label as
cauctions (Combinatorial Auctions), setcover (Minimum Set Cover), facilities (Capacitated Facility Location),
and indset (Maximum Independent Set).

Speciﬁcally, we investigate the frequency with which sibling nodes in the B&B tree have the same strong
branching decision. Figure 9 shows that this condition happens between 3-7% of the times on the small
instances that were used to collect approximately 30K observations. The SOTA GNNs (Gasse et al., 2019)

21

are not able to capture this dependency as well. However, given that this condition is fairly less frequent
relative to the lookback condition, we do not explore this condition in our current work.

Figure 9: Frequency with which sibling nodes in a B&B tree have the same strong branching decisions
(Ground truth), and the fraction of times this condition is satisﬁed by GNNs trained as per Gasse et al.
(2019).

22

