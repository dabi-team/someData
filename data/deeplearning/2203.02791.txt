Deep Q-Learning Based Resource Allocation in
Interference Systems With Outage Constraint

Saniul Alam, Sadia Islam, Muhammad R. A. Khandaker, Senior Member, IEEE, Risala T. Khan, Senior
Member, IEEE, Faisal Tariq, Senior Member, IEEE, and Apriana Toding

1

2
2
0
2

r
a

M
5

]
I

N
.
s
c
[

1
v
1
9
7
2
0
.
3
0
2
2
:
v
i
X
r
a

Abstract—This correspondence considers the resource allo-
cation problem in wireless interference channel
(IC) under
link outage constraints. Since the optimization problem is non-
convex in nature, existing approaches to ﬁnd the optimal power
allocation are computationally intensive and thus practically
infeasible. Recently, deep reinforcement
learning has shown
promising outcome in solving non-convex optimization problems
with reduced complexity. In this correspondence, we utilize
a deep Q-learning (DQL) approach which interacts with the
wireless environment and learns the optimal power allocation
of a wireless IC while maximizing overall sum-rate of the system
and maintaining reliability requirement of each link. We have
used two separate deep Q-networks to remove the inherent
instability in learning process. Simulation results demonstrate
that the proposed DQL approach outperforms existing geometric
programming based solution.

Index Terms—Deep reinforcement learning, Q-learning, deep
Q-learning, deep Q-network, wireless interference channel, re-
source allocation.

I. INTRODUCTION

Resource allocation tasks such as optimal power allocation
has a signiﬁcant effect on the capacity and performance of
wireless communication [1]. Traditionally, resource allocation
problems have been approached and tackled by established
numerical optimization approaches such as interference pric-
ing [2], water ﬁlling algorithms [3], fractional programming
[4] and weighted minimum mean-squared error (WMMSE)
minimization [5]. Most of these algorithms are mathematically
formulated to optimize a certain system performance metrics
such as sum-rate, minimum rate or mean-squared error. These
algorithms generally adopted iterative approaches where they
take certain parameters like channel realization as input and
produce the optimum resource allocation strategy.

Even though these established resource and power allocation
methods have improved the performance to some extent, they
come with their own set of problems. These non-convex opti-
mization problems are NP-hard and thus incur high computa-
tional cost. As a result, many of the algorithms and solutions
become infeasible for real world deployment. Moreover, as
it takes time to compute the optimum allocation, it also has
negative effect on the latency performance of the system
which is a key performance metric for current and future
communication systems [6]. Despite the high computational
cost, these algorithms have to be recomputed frequently since
the channel is time varying and the allocation may not remain
usable for long.

tool for providing viable solution to major challenges such as
channel decoding and estimation [7] as well as optimal power
allocation [8].

Deep reinforcement

learning (DRL), a member of ML
family, has been considered as an attractive method which can
learn the optimal solution of the resource allocation problem
by interacting with the wireless environment. Reinforcement
learning (RL) generally outperforms traditional approaches in
most of the cases [9]. The work in [10] uses deep Q-learning
(DQL) for centralized downlink power allocation scheme
which maximizes the network throughput in a multi cell
network. The work in [11] also uses multi agent DQL to ﬁnd
optimal power allocation in a wireless cellular network, where
each transmitter receives channel state information (CSI) from
several neighbours and learns to adjust its transmit power
accordingly. A deep Q-network (DQN) was used to overcome
the instability problem of DL. In [12], the authors proposed
faded-experience trust-region policy optimization (FE-TRPO)
base power allocation algorithm which exploits continuous
DRL. The work demonstrated that compared to the traditional
WMMSE techniques, it signiﬁcantly decreases computational
complexity while maintaining similar performance.

While most of these works use DRL for maximizing the
overall system throughput, the schemes do not always guar-
antee reliability for each individual user. As a consequence,
users with low channel gain may not be able to transmit at
all while user with the best channel gain may be allowed
to transmit at maximum power resulting in unfairness in the
system. To address this limitation, this work exploits DQN
with a reliability outage requirement threshold for each indi-
vidual user. The DQN ﬁnds the relationship between available
channel information and the solution of the power allocation
problem. A DQN agent is then used to ﬁnd the collective
power allocation policy of the users that maximizes the overall
system throughput while maintaining reliability of each user.
The main contribution of this paper can be summarised as:

• Firstly,

the resource allocation problem of a wireless
interference network under probabilistic constraints has
been solved using geometric programming (GP) (follow-
ing [13]).

• Then, a DRL-based solution has been developed in or-
der to reduce computational complexity and make the
solution practicable. Two separate deep Q-networks have
been used in the DRL solution to remove the inherent
instability of the learning process.

Machine learning (ML) techniques has recently been con-
sidered in wireless communication research as an attractive

• Simulation results demonstrate that the proposed DRL
approach signiﬁcantly outperforms the GP approach.

 
 
 
 
 
 
2

II. SYSTEM MODEL

Reward rt

Tx 1

We consider an interference channel (IC) as shown in Fig. 1,
where K single-antenna transmitters are communicating with
K single-antenna receivers. The direct channel between the
kth transmitter-receiver pair is denoted by hk,k while hk,j
denotes the IC between transmitter j and receiver k. Let us
denote xk as the signal transmitted by transmitter k and yk
as the corresponding received signal at receiver k. Then yk is
given by

State St

Policy π

at

Take Action at

Agent

Observe State St

Rx 1

Rx K

Rx 2

T x   2

T x   K

r o n m e n t

n v i

E

yk = hkkxk +

hkj xj + nk.

(1)

Fig. 1. Deep reinforcement learning for IC.

j6=k
X

0, σ2
k

Here nk ∼ N
for σk > 0 represents the additive
channel noise. We assume that pk is the transmission power
of transmitter k. Note that the symbol transmitted by different
transmitters are independent of each other. Then the signal to
interference-plus-noise ratio (SINR) can be expressed as

(cid:1)

(cid:0)

γk ,

|hkk|2 pk
j6=k |hkj |2 pj + σ2

k

,

(2)

where σ2
k denotes the noise power at receiver k. Thus the
achievable rate at the kth receiver under Gaussian channel
fading is given by

P

Then the chance SINR outage requirement in (4b) can be
expressed as

Ok =Pr{γk ≤ γ0}, ∀k,
≈Pr{|hkk|2 pk ≤ γ0

|hkj |2 pj}, ∀k,

(6)

j6=k
X

Note that the outage probability Ok can be interpreted as the
fraction of time the kth transmitter/receiver pair experiences
an outage due to fading. Interestingly, it has been shown
analytically in [14], [13] that the outage probability (6) can
be expressed in closed form using the following Lemma.

Rk = log2

1 +

|hkk|2 pk
j6=k |hkj |2 pj + σ2

k !

.

(3)

Lemma 1. If x1, x2 . . . , xN are independent random variables
following exponential distribution with means µi , E[xi] =
1/λi, then we have

A. Problem Formulation

P

Pr

x1 ≤

Our aim is to maximize the overall system throughput of the
communication links by optimally allocating transmit power
among the active links such that the worst-user’s mutual-
information is maximized under link outage constraints. Ac-
cordingly, we formulate the following optimization problem:

max
{pk}

min
k

Rk

s.t. Pr{γk ≤ γ0} ≤ p0, ∀k,
Pmin ≤ pk ≤ Pmax, ∀k,

(4a)

(4b)

(4c)

where Pmax and Pmin are the maximum and minimum trans-
mit power budgets of the users, respectively, γ0 is the mini-
mum SINR requirement that needs to be maintained by each
user to ensure reliability, and the tolerable outage probability is
denoted by p0. Note that problem (4) is a non-convex problem
with fractional objective and the nonlinear probabilistic con-
straints, and hence the exactly optimal solution is non-trivial.
In the following, we will ﬁrst develop an acceptable solution
to the problem following traditional alternating approaches,
and then following ML techniques.

For ease of exposition, let us consider an interference-
limited system such that the noise term in (2) can be ignored.
Thus, the SINR in (2) reduces to

N

N

= 1 −

xi

!

i=2
X

i=2  
Y

1
1 + λ1

λi !

.

(7)

Proof. The proof follows similar lines as in [13, Appendix I].
(cid:4)
Therefore, we omit the proof here for brevity.

Using Lemma 1 for an interference-limited scenario, we can

express the outage-probability in (6) as

Ok = 1 −

1
1 + γ0|hkj|2pj
|hkk|2Pk

.

j6=k
Y

(8)

Thus the outage constraint (4b) can be expressed as

(1 − p0)

1 +

j6=k (cid:18)
Y

γ0|hkj |2pj
|hkk|2pk (cid:19)

≤ 1.

(9)

It can be shown that the left-hand side of the inequality (9) is,
in fact, a posynomial function of the powers p1, p2, . . . , pK.
Thus we can express the problem as

max
{pk}

min
k

Rk

s.t. (1 − p0)

1 +

j6=k (cid:18)
Y
≤ 1, ∀k,

≤ 1, ∀k.

Pmin
pk
pk
Pmax

(10a)

γ0|hkj |2pj
|hkk|2pk (cid:19)

≤ 1, ∀k,

(10b)

(10c)

(10d)

˜γk ,

|hkk|2 pk
j6=k |hkj |2 pj

.

P

(5)

The problem is still non-convex due to the fractional objective
function as well as the constraint (10b), however, a solution
can be obtained based on the Perron–Frobenius theorem for

 
 
the maximum eigenvalue of the matrix {H | [H]i,j = |hi,j|2}
that has non-negative elements (channel gains in this case)
[15]. Note that due to the monotonicity of the log function,
problem (10) can be equivalently solved by replacing the
objective function by the corresponding SINR values. Now,
by introducing a slack variable η, we can reformulate problem
(10) as

max
η,{pk}

s.t.

η

η

j6=k |hkj|2 pj
|hkk|2 pk

(cid:16)P

(1 − p0)

1 +

j6=k (cid:18)
Y
≤ 1, ∀k,

≤ 1, ∀k.

Pmin
pk
pk
Pmax

≤ 1, ∀k,

(cid:17)
γ0|hkj|2pj
|hkk|2pk (cid:19)

(11a)

(11b)

≤ 1, ∀k,

(11c)

(11d)

(11e)

It can be observed that constraints (11b) and (11c) are ho-
mogeneous with respect to powers thereby depend only on
the power ratios. Accordingly, problem (11) is a geometric
program (GP) with respect to {pk}, therefore, it can be solved
globally and efﬁciently using interior-point methods for geo-
metric programming. We solve the problem using geometric
programming and consider it as a baseline to compare the
results of our DRL method developed in the next section.

III. REINFORCEMENT LEARNING FOR RESOURCE
OPTIMISATION

Fig. 1 demonstrates the basic structure of RL in the system
model under consideration. QL is a widely used model-free
RL technique, which determines the quality of action and
informs the RL agent which action will be optimum in each
circumstances.

Lets S denotes possible states of the environment and A
denotes a set of discrete actions. Here, s ∈ S is a speciﬁc
state containing environmental features. For each time step
t, agent receives a state st ∈ S from the environment and
takes a speciﬁc action at from the action space A. The
agent follows a policy π which is the probability of choosing
an action at, given the current state being st. After the
execution of the action, the agent receives a reward rt and
state transition happens from state st to state s(t+1). Then the
process forms an experience e(t+1) =
s(t), a(t), r(t), s(t+1))
which summarizes the interaction with the environment. This
process is repeated until the agent reaches the terminal state
after which the process restarts again. The goal of QL is to
ﬁnd an optimal policy π which, at time t, maximizes the future
cumulative reward, denoted as [10]:

(cid:0)

∞

Rt =

γkrt+k.

(12)

k=0
X
Here, γ ∈ (0, 1] is a factor which denotes the priority of future
rewards in comparison with the current reward. We deﬁne an
action-value function Qπ(s, a), which is the expected return

starting from state s, taking an action a, while following policy
π [11]:

3

Qπ(s, a) = Eπ

R(t) | s(t) = s, a(t) = a
h

i

.

(13)

The Q-function satisﬁes the Bellman equation given below:

Q∗(s, a) = R(s, a) + γ

P a

ss′ max
a′∈A

Q∗ (s′, a′) .

(14)

s′∈S
X
The QL algorithm uses a lookup table (Q-table) to store the
output of Q-function called Q-values. The agent uses an ǫ-
greedy policy in order to choose action. According to the ǫ-
greedy policy, at each state the agent either chooses to explore
the environment by choosing action randomly or exploit Q-
table and selects the action with the maximum Q-value.

The problem with traditional QL is that it stores the Q-
values in tabular form. So, for high dimensional state spaces,
QL becomes infeasible quickly since its space requirement
for the Q-table becomes impractical. In this correspondence,
we use DQN which is a combination of traditional QL and
a deep neural network (DNN). Instead of using a table to
store the Q-values, DQN uses a DNN which receives sates
and provides Q-values as output for each possible action from
that state. To avoid correlation between input data, a buffer of
experience called replay memory is also created. The DQN is
trained with training data sampled randomly from the replay
memory. This technique is known as experience replay. In
DQL, we approximate the optimal action-value function by
using a neural network, Q(s, a; θ) ≈ Q∗(s, a). Here, Q(s, a; θ)
is called the DQN and θ is a parameter of the neural network.
The MSE of the Bellman equation is reduced by the iterative
update, which is used to train the Q-network.

A. Proposed DQL Approach

In our proposed DQL approach, we exploit a single DQN
agent which communicates with the environment in order to
learn the optimal power of the users. The agent, instead of
assigning more power to the user with best channel condition,
least
tries to maintain reliability and assign each user at
minimum power, so that
they can maintain the minimum
required SINR threshold γ0. Once reliability is ensured, the
agent gives more emphasis on maximizing the global sum-rate
instead of trying to maximize the rate of each individual users.
The resource sharing scheme is performed in two phases, ﬁrst,
in the training or learning stage and then in the implementation
or testing phase. In the learning phase, the DQN agent access
the readily available system performance reward and adjusts
its action to achieve an optimal policy by updating the DQN.
In implementation phase, the agent receives observation from
environment and selects the action according to the Q-value
received at the output given by the trained DQN.

1) State Space: The state space S consists of power allo-
cation information of other users from the previous step and
channel realization of the user (hkj ), which follows a certain
distribution. We assume at the beginning of each time step t
that the users’ channel information is instantaneously available
at the transmitter.

2) Action Space: The resource allocation design using RL
comes down to the transmission power control for each user.
So, the total number of actions depend on the number of power
levels we are using. Even though transmission power takes
continuous values in most cases for reducing complexity, in
this paper, we limit the power control options to a ﬁxed number
of power levels chosen from the interval [Pmax, Pmin]. We
assume that the action space A has n power levels.

3) Reward Design: Due to inherent ﬂexibility in reward
design, RL is an appealing tool for solving problems that
are hard to optimize using traditional methods. In the original
problem formulation in section II, we set the design objective
to maximize the overall sum-rate ensuring reliability of each
user (i.e. SIN R ≥ γ0). Accordingly, we deﬁne the reward
function for each agent as per the achievable rate of each user,
Rk. So the reward rt at each time step becomes:

rt = Rk.

(15)

To ensure that the reliability constraint is satisﬁed, the con-
dition in (11c) is checked during each reward calculation. If
the condition is not fulﬁlled, the algorithm simply declares
the power choice of that user as invalid and sets the reward to
zero. Thus, the overall reward design can be expressed as

Rt =

Rk,
0,

(

if (1 − p0)
otherwise.

j6=k

Q

(cid:16)

1 + γ0|hkj|

2pj
|hkk|2pk

≤ 1

(16)

(cid:17)

B. Training the DNN

The objective of training design is to exploit a trained DQN
to replay the experience in learning the resource allocation
policy. The DQN takes an observation form the environment
st as input and outputs the possible actions. Multiple episodes
are run to train the DQN and the agent uses the ǫ-greedy
policy to explore the environment. After the transition of the
environment due to the actions taken as well as the change
of channel information, the agent saves the transition tuple
(s(t), a(t), r(t), s(t+1)) in a replay memory. A mini batch of
transition tuple or experience is sampled randomly from the
replay memory. The mini batch is used to train the DQN and
stochastic gradient-descent method is used for updating the
DQN. The goal is to minimize the sum-squared error:

R(t+1) + γ max

a′ Q

X h

s(t+1), a′; θ−
(cid:16)

(cid:17)

− Q

s(t), a(t); θ
(cid:16)

2

.

(17)
(cid:17)i

Here θ− is the parameter of target Q-network which is a DQN
with frozen weights. The target Q-network parameters are
initialized by duplicating the parameters of training DQN and
after a certain number of episodes they are updated to mirror
the parameters of the training DQN. The reason for using
two DQN is that if we use same Q-network for generating
the target Q-value and the predicted Q-value, the learning
becomes really unstable. So, 2 DQNs are used to ensure a
stable learning. The overall training procedure is summarized
in Algorithm 1.

Algorithm 1 Resource Allocation with DRL

1: Initialize the environment E (users’ channel information)

4

2: Initialize the Q-network with action space A.
3: for each episode do
Reset environment.
4:
Initialize t = 0.
repeat

5:

6:
7:

8:

9:
10:

11:

12:

13:

Observe state St.
Choose action at from A using ǫ-greedy strategy.
Agent takes action at and receives reward rt.
State transition happens and agent receives next-state
St+1.
Store (s(t), a(t), r(t), s(t+1)) in replay memory.
Sample mini-batch from replay memory.
Using the stochastic gradient descent method, opti-
mize error between Q-network and learning targets
deﬁned in (17).
t=t+1.

until t > maximum step per episode.

14:
15:
16: end for

C. Testing the DNN

In testing stage, at each time step t, the agent receives
channel realization st and ǫ is set to the value from the very
last training step. The agent selects an action a(t), which has
the maximum Q-value according to the DQN. After that the
agent starts transmission with the power level according to
their selected action. Since the training procedure described
in Algorithm 1 is computationally expensive, it can be per-
formed ofﬂine for a large number of channel conditions, over
many episodes and the computationally inexpensive, testing or
implementation phases can be executed online for the actual
network deployment.

IV. NUMERICAL SIMULATIONS

In this section, we compare the analytical results obtained
in Section II with those of the proposed DQN algorithm.
The channel coefﬁcients are generated using the standards of
Gaussian IC [1]. The channel coefﬁcients follow a Rayleigh
fading distribution with zero mean and unit variance. We
consider minimum SINR threshold γ0 as −10 dB and Pmin as
0 dB. We use a dense DQN with three hidden layers consisting
of 200, 100 and 40 neurons. We also use the rectiﬁed linear
unit (ReLU), f (x) = max(0, x) as activation function and
RMSProp optimizer is used for updating the weights of the
network using learning rate of 0.001 and updating the DQN
over 3000 episode. The proposed method is compared with
several methods namely: a GP method discussed in Section
II, a WMMSE algorithm developed in [5], a supervised DNN
[16] and a WMMSE based supervised learning method [8].

The Fig. 2 shows the sum rate performance of the DQN in a
4 user scenario with outage probability p0 = 0.3. We can see
that the proposed method performs signiﬁcantly better than
the existing GP algorithm [13]. The WMMSE approach in [5]
performs better since it does not maintain reliability constraint.

Supervised
WMMSE 
WMMSE-Supervised 
GP
DQN

e
t
a
r
-

m
u
S

3.0

2.5

2.0

1.5

1.0

e
t
a
r
-

m
u
S

e
g
a
r
e
v
A

1.2

1.0

0.8

0.6

0.4

0.2

5

Pmax=4 dB
Pmax=6 dB
Pmax=8 dB

2

3

4

5

6
Pmax

7

8

9

10

2

3

4
No of user

5

6

Fig. 2. Sum-rate versus transmit power budget Pmax.

Fig. 4. Average sum-rate versus the number of user pairs K.

Our proposed approach is capable of obtaining better result
than the approach in [5], if we ignore the reliability constraint
during reward design as shown in Fig. 2.

e
t
a
r
-

m
u
S

1.7

1.6

1.5

1.4

1.3

1.2

1.1

1.0

0.9

DQN
GP

0.20

0.22

0.24
0.26
Outage Constraint

0.28

0.30

Fig. 3. Sum-rate versus outage probability threshold p0.

Fig. 3 shows the performance of our proposed method over
varying outage probability p0. We make the comparison con-
sidering Pmax as 4 dB. It can be seen from the ﬁgure that the
proposed DQN method signiﬁcantly outperforms traditional
GP method. It can also be observed that the sum-rate of
the proposed method increases with p0, while in traditional
approaches performance decreases. This can be explained with
the reward design (16). When p0 increases, due to the term
(1 − p0) the overall condition term decreases and thus offers
higher probability of becoming less then or equal to one
and full-ﬁlling the condition for receiving reward resulting in
increased sum-rate performance.

Fig. 4 shows average sum rate against different number of
users. As expected, the average sum-rate performance of the
DQN decreases as the number of user increases in the system
since higher number of users generate additional interference.
This effect can be compensated by increasing transmit power
budget as evident from the Fig. 4.

V. CONCLUSION

In this paper, we have developed a DRL based resource
allocation scheme which successfully learns the power al-
location of a wireless interference channel and guarantees

reliability requirements for each user. We exploit a DQN with
experience to learn the relationship between users’ channel
information and power allocation. We have demonstrated the
superiority of the proposed approach by comparing the sum-
rate performance with that of the existing GP-based solution.

REFERENCES

[1] M. R. A. Khandaker and Y. Rong, “Interference MIMO relay channel:
Joint power control and transceiver-relay beamforming,” IEEE Trans.
Signal Process., vol. 60, no. 12, pp. 6509–6518, 2012.

[2] F. Wang, M. Krunz, and S. Cui, “Price-based spectrum management in
cognitive radio networks,” IEEE J. Sel. Topics Signal Process., vol. 2,
no. 1, pp. 74–87, 2008.

[3] Y. Hu and A. Ribeiro, “Adaptive distributed algorithms for optimal
random access channels,” IEEE Trans. Wireless Commun., vol. 10, no. 8,
pp. 2703–2715, 2011.

[4] K. Shen and W. Yu, “Fractional programming for communication
systems—part i: Power control and beamforming,” IEEE Trans. Signal
Process., vol. 66, no. 10, pp. 2616–2630, 2018.

[5] Q. Shi, M. Razaviyayn, Z.-Q. Luo, and C. He, “An iteratively weighted
mmse approach to distributed sum-utility maximization for a mimo
interfering broadcast channel,” IEEE Trans. Signal Process., vol. 59,
no. 9, pp. 4331–4340, 2011.

[6] F. Tariq, M. R. A. Khandaker, K.-K. Wong, M. A. Imran, M. Bennis,
and M. Debbah, “A speculative study on 6G,” IEEE Wireless Commu-
nications, vol. 27, no. 4, pp. 118–125, 2020.

[7] T. Li, M. R. A. Khandaker, F. Tariq, K. Wong, and R. T. Khan, “Learning
the wireless V2I channels using deep neural networks,” in IEEE 90th
Vehicular Technology Conference (VTC-Fall), 2019.

[8] J. Gao, M. R. A. Khandaker, F. Tariq, K.-K. Wong, and R. T. Khan,
“Deep neural network based resource allocation for V2X communica-
tions,” in Proc. IEEE 90th Vehicular Technol. Conf. (VTC-Fall), 2019.
[9] Q. Mao, F. Hu, and Q. Hao, “Deep learning for intelligent wireless
networks: A comprehensive survey,” IEEE Commun. Surveys Tuts.,
vol. 20, no. 4, pp. 2595–2621, 2018.

[10] K.

I. Ahmed and E. Hossain, “A deep Q-learning method for
downlink power allocation in multi-cell networks,” arXiv preprint
arXiv:1904.13032, 2019.

[11] Y. S. Nasir and D. Guo, “Multi-agent deep reinforcement learning for
dynamic power allocation in wireless networks,” IEEE J. Sel. Areas
Commun., vol. 37, no. 10, pp. 2239–2250, 2019.

[12] M. G. Khoshkholgh and H. Yanikomeroglu, “Faded-experience trust re-
gion policy optimization for model-free power allocation in interference
channel,” IEEE Wireless Commun. Lett., pp. 1–1, 2020.

[13] S. Kandukuri and S. Boyd, “Optimal power control in interference-
limited fading wireless channels with outage-probability speciﬁcations,”
IEEE Trans. Wireless Commun., vol. 1, pp. 46–55, 2002.

[14] Y.-D. Yao and A. Sheikh, “Outage probability analysis for microcell
mobile radio systems with cochannel
interferers in Rician/Rayleigh
fading environment,” Electronics Lett., vol. 26, pp. 864–866, June 1990.
[15] D. Mitra, An Asynchronous Distributed Algorithm for Power Control in
Cellular Radio Systems. Boston, MA: Springer US, 1994, pp. 177–186.

 
 
 
 
[16] Sun et al., “Learning to optimize: Training deep neural networks for
wireless resource management,” in IEEE Int. Wksp. Signal Process. Adv.
Wireless Commun. (SPAWC), 2017.

6

