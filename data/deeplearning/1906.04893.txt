9
1
0
2

n
u
J

2
1

]

G
L
.
s
c
[

1
v
3
9
8
4
0
.
6
0
9
1
:
v
i
X
r
a

Eﬃcient and Accurate Estimation of Lipschitz
Constants for Deep Neural Networks

Mahyar Fazlyab Alexander Robey Hamed Hassani
Manfred Morari George J. Pappas ∗

Abstract

Tight estimation of the Lipschitz constant for deep neural networks (DNNs) is useful in many ap-
plications ranging from robustness certiﬁcation of classiﬁers to stability analysis of closed-loop systems
with reinforcement learning controllers. Existing methods in the literature for estimating the Lipschitz
constant suﬀer from either lack of accuracy or poor scalability. In this paper, we present a convex op-
timization framework to compute guaranteed upper bounds on the Lipschitz constant of DNNs both
accurately and eﬃciently. Our main idea is to interpret activation functions as gradients of convex po-
tential functions. Hence, they satisfy certain properties that can be described by quadratic constraints.
This particular description allows us to pose the Lipschitz constant estimation problem as a semideﬁnite
program (SDP). The resulting SDP can be adapted to increase either the estimation accuracy (by cap-
turing the interaction between activation functions of diﬀerent layers) or scalability (by decomposition
and parallel implementation). We illustrate the utility of our approach with a variety of experiments on
randomly generated networks and on classiﬁers trained on the MNIST and Iris datasets. In particular,
we experimentally demonstrate that our Lipschitz bounds are the most accurate compared to those in
the literature. We also study the impact of adversarial training methods on the Lipschitz bounds of the
resulting classiﬁers and show that our bounds can be used to eﬃciently provide robustness guarantees.

1 Introduction

A function f : Rn
L

0 such that

≥

Rm is globally Lipschitz continuous on

→

X ⊆

Rn if there exists a nonnegative constant

−
The smallest such L is called the Lipschitz constant of f . The Lipschitz constant is the maximum ratio
between variations in the output space and variations in the input space of f and thus is a measure of
sensitivity of the function with respect to input perturbations.

∈ X

(cid:107) ≤

−

(cid:107)

f (x)
(cid:107)

f (y)

L

x
(cid:107)

y

for all x, y

.

(1)

When a function f is characterized by a deep neural network (DNN), tight bounds on its Lipschitz
constant can be extremely useful in a variety of applications.
In classiﬁcation tasks, for instance, L can
be used as a certiﬁcate of robustness of a neural network classiﬁer to adversarial attacks if it is estimated
tightly [33]. In deep reinforcement learning, tight bounds on the Lipschitz constant of a DNN-based controller
can be directly used to analyze the stability of the closed-loop system. Lipschitz regularity can also play a
key role in derivation of generalization bounds [5]. In these applications and many others, it is essential to
have tight bounds on the Lipschitz constant of DNNs. However, as DNNs have highly complex and non-
linear structures, estimating the Lipschitz constant both accurately and eﬃciently has remained a signiﬁcant
challenge.

Our contributions. In this paper we propose a novel convex programming framework to derive tight bounds
on the global Lipschitz constant of deep feed-forward neural networks. Our framework yields signiﬁcantly
more accurate bounds compared to the state-of-the-art and lends itself to a distributed implementation,
leading to eﬃcient computation of the bounds for large-scale networks.

∗The authors are with the Department of Electrical and Systems Engineering, University of Pennsylvania. Email:

{mahyarfa,arobey1,hassani,morari,gpappas}@seas.upenn.edu.

1

 
 
 
 
 
 
Our approach. We use the fact that nonlinear activation functions used in neural networks are gradients
of convex functions; hence, as operators, they satisfy certain properties that can be abstracted as quadratic
constraints on their input-output values. This particular abstraction allows us to pose the Lipschitz esti-
mation problem as a semideﬁnite program (SDP), which we call LipSDP. A striking feature of LipSDP is
its ﬂexibility to span the trade-oﬀ between estimation accuracy and computational eﬃciency by adding or
removing extra decision variables. In particular, for a neural network with (cid:96) layers and a total of n hid-
den neurons, the number of decision variables can vary from (cid:96) (least accurate but most scalable) to O(n2)
(most accurate but least scalable). As such, we derive several distinct yet related formulations of LipSDP
that span this trade-oﬀ. To scale each variant of LipSDP to larger networks, we also propose a distributed
implementation.

Our results. We illustrate our approach in a variety of experiments on both randomly generated networks
as well as networks trained on the MNIST [22] and Iris [10] datasets. First, we show empirically that our
Lipschitz bounds are the most accurate compared to all other existing methods of which we are aware. In
particular, our experiments on neural networks trained for MNIST show that our bounds almost coincide with
the true Lipschitz constant and outperform all comparable methods. For details, see Figure 2a. Furthermore,
we investigate the eﬀect of two robust training procedures [20, 23] on the Lipschitz constant for networks
trained on the MNIST dataset. Our results suggest that robust training procedures signiﬁcantly decrease
the Lipschitz constant of the resulting classiﬁers. Moreover, we use the Lipschitz bound for two robust
training procedures to derive non-vacuous lower bounds on the minimum adversarial perturbation necessary
to change the classiﬁcation of any instance from the test set. For details, see Figure 3.

Related work. The problem of estimating the Lipschitz constant for neural networks has been studied
in several works. In [33], the authors estimate the global Lipschitz constant of DNNs by the product of
Lipschitz constants of individual layers. This approach is scalable and general but yields trivial bounds. We
are only aware of two other methods that give non-trivial upper bounds on the global Lipschitz constant
In [9],
of fully-connected neural networks and can scale to networks with more than two hidden layers.
Combettes and Pesquet derive bounds on Lipschitz constants by treating the activation functions as non-
expansive averaged operators. The resulting algorithm scales well with the number of hidden units per layer,
but very poorly (in fact exponential) with the number of layers. In [36], Virmaux and Scaman decompose
the weight matrices of a neural network via singular value decomposition and approximately solve a convex
maximization problem over the unit cube. Notably, estimating the Lipschitz constant using the method
in [36] is intractable even for small networks; indeed, the authors of [36] use a greedy algorithm to compute
a bound, which may underapproximate the Lipschitz constant. Bounding Lipschitz constants for the speciﬁc
case of convolutional neural networks (CNNs) has also been addressed in [4, 5, 42].

Using Lipschitz bounds in the context of adversarial robustness and safety veriﬁcation has also been
addressed in several works [30, 37, 38]. In particular, in [38], the authors convert the robustness analysis
problem into a local Lipschitz constant estimation problem, where they estimate this local constant by a set
of independently and identically sampled local gradients. This algorithm is scalable but is not guaranteed to
provide upper bounds. In a similar work, the authors of [37] exploit the piece-wise linear structure of ReLU
functions to estimate the local Lipschitz constant of neural networks.
In [12], the authors use quadratic
constraints and semideﬁnite programming to analyze local (point-wise) robustness of neural networks. In
contrast, our Lipschitz bounds can be used as a global certiﬁcate of robustness and are agnostic to the choice
of the test data.

1.1 Motivating applications

We now enumerate several applications that highlight the importance of estimating the Lipschitz constant
of DNNs accurately and eﬃciently.

Robustness certiﬁcation of classiﬁers. In response to fragility of DNNs to adversarial attacks, there
has been considerable eﬀort in recent years to improve the robustness of neural networks against adversarial
attacks and input perturbations [15, 20, 21, 23, 25, 41]. In order to certify and/or improve the robustness of
neural networks, one must be able to bound the possible outputs of the neural network over a region of input
space. This can be done either locally around a speciﬁc input [6,11,12,14,19,20,28,29,32,34,39,40], or globally
by bounding the sensitivity of the function to input perturbations, i.e., the Lipschitz constant [17, 27, 33, 38].

2

Indeed, tight upper bounds on the Lipschitz constant can be used to derive non-vacuous lower bounds on
the magnitudes of perturbations necessary to change the decision of neural networks. Finally, an eﬃcient
computation of these bounds can be useful in either assessing robustness after training [12,28,29] or promoting
robustness during training [20, 35]. In the experiments section, we explore this application in depth.

Stability analysis of closed-loop systems with learning controllers. A central problem in learning-
based control is to provide stability or safety guarantees for a feedback control loop when a learning-enabled
component, such as a deep neural network, is introduced in the loop [3, 7, 18]. The Lipschitz constant of a
neural network controller bounds its gain. Therefore a tight estimate can be useful for certifying the stability
of the closed-loop system.
Notation. We denote the set of real n-dimensional vectors by Rn, the set of m
n-dimensional matrices
by Rm
++ the sets of n-by-n
×
symmetric, positive semideﬁnite, and positive deﬁnite matrices, respectively. The p-norm (p
1) is denoted
Rm
n is the largest singular value of W . We denote the
by
i-th unit vector in Rn by ei. We write diag(a1, ..., an) for a diagonal matrix whose diagonal entries starting
in the upper left corner are a1,

n, and the n-dimensional identity matrix by In. We denote by Sn, Sn

R+. The (cid:96)2-norm of a matrix W

(cid:107) · (cid:107)p : Rn

×
+, and Sn

, an.

→

≥

∈

×

· · ·

2 LipSDP: Lipschitz certiﬁcates via semideﬁnite programming

2.1 Problem statement
Consider an (cid:96)-layer feed-forward neural network f (x) : Rn0
equations:

→

Rn(cid:96)+1 described by the following recursive

x0 = x,

xk+1 = φ(W kxk + bk) for k = 0,

f (x) = W (cid:96)x(cid:96) + b(cid:96).

(2)

×

∈

∈

Rnk+1

Rn0 is an input to the network and W k

Rnk+1 are the weight matrix and
Here x
bias vector for the k-th layer. The function φ is the concatenation of activation functions at each layer, i.e.,
ϕ(xn)](cid:62). In this paper, our goal is to ﬁnd tight bounds on the Lipschitz
it is of the form φ(x) = [ϕ(x1)
constant of the map x
0 such
that
(cid:107)2 ≤

f (x) in (cid:96)2-norm. More precisely, we wish to ﬁnd the smallest constant L2 ≥
x

The main source of diﬃculty in solving this problem is the presence of the nonlinear activation functions.
To combat this diﬃculty, our main idea is to abstract these activation functions by a set of constraints that
they impose on their input and output values. Then any property (including Lipschitz continuity) that is
satisﬁed by our abstraction will also be satisﬁed by the original network.

· · ·
(cid:107)2 for all x, y

f (x)
(cid:107)

(cid:55)→
L2(cid:107)

Rn0 .

f (y)

−

−

∈

∈

y

, (cid:96)

1,

−
· · ·
nk and bk

2.2 Description of activation functions by quadratic constraints

In this section, we introduce several deﬁnitions and lemmas that characterize our abstraction of nonlinear
activation functions. These results are crucial to the formulation of an SDP that can bound the Lipschitz
constants of networks in Section 2.3.

Deﬁnition 1 (Slope-restricted non-linearity) A function ϕ : R
0

α < β <

if

≤

∞

ϕ(y)
y

α

≤

ϕ(x)
x

−
−

β

≤

x, y

∀

∈

R is slope-restricted on [α, β] where

(3)

→

R.

(cid:55)→

ϕ(x) is at least α and at most β (see Figure 1). By multiplying all sides of (3) by (y

The inequality in (3) simply states that the slope of the chord connecting any two points on the curve of the
x)2,
function x
x)2. By the left
we can write the slope restriction condition as α(y
−
inequality, the operator ϕ(x) is strongly monotone with parameter α [31], or equivalently the anti-derivative
function
ϕ(x)dx is strongly convex with parameter α. By the right-hand side inequality, ϕ(x) is one-sided
Lipschitz with parameter β. Altogether, the preceding inequalities state that the anti-derivative function

ϕ(x))(y

(ϕ(y)

x)2

β(y

x)

−

−

−

≤

−

≤

ϕ(x)dx is α-strongly convex and β-smooth.

(cid:82)

(cid:82)

3

Note that all activation functions used in deep learning satisfy the slope restriction condition in (3) for
. For instance, the ReLU, tanh, and sigmoid activation functions are all slope restricted

α < β <

≤

some 0
with α = 0 and β = 1. More details can be found in [12].
Deﬁnition 2 (Incremental Quadratic Constraint [1]) A function φ : Rn
tal quadratic constraint deﬁned by

S2n if for any Q

and x, y

Rn,

∞

Rn satisﬁes the incremen-

→

(4)

Q ⊂
x
φ(x)

(cid:20)

y
φ(y)

(cid:62)

Q

(cid:21)

−
−

∈ Q
x
φ(x)

−
−

(cid:20)

∈

y
φ(y)

0.

≥

(cid:21)

In the above deﬁnition,

is the set of all multiplier matrices that characterize φ, and is a convex cone
exp(xn)](cid:62) is the
by deﬁnition. As an example, the softmax operator φ(x) = (
n
gradient of the convex function ψ(x) = log(
i=1 exp(xi)). This function is smooth and strongly convex
with paramters α = 0 and β = 1 [8]. For this class of functions, it is known that the gradient function
φ(x) =

ψ(x) satisﬁes the quadratic inequality [24]

n
i=1 exp(xi))−

1[exp(x1)

· · ·

(cid:80)

(cid:80)

Q

∇

x
φ(x)

(cid:20)

−
−

y
φ(y)

(cid:21)

(cid:62)

2αβIn
−
(α + β)In
(cid:20)

(α + β)In

2In (cid:21) (cid:20)
−

x
φ(x)

y
φ(y)

−
−

≥

(cid:21)

0.

(5)

Therefore, the softmax operator satisﬁes the incremental quadratic constraint deﬁned by
where M the middle matrix in the above inequality.

=

λM
{

|

λ

,

0
}

≥

Q

To see the connection between incremental quadratic constraints and slope-restricted nonlinearities, note

that (3) can be equivalently written as the single inequality

Multiplying through by (y

−

(

ϕ(y)
y

ϕ(x)
x

α)(

−

ϕ(y)
y

ϕ(x)
x

β)

0.

≤

−

−
−

−
−

x)2 and rearranging terms, we can write (6) as

x
ϕ(x)

(cid:20)

(cid:62)

y
ϕ(y)

(cid:21)

−
−

(cid:20)

−
α + β

2αβ α + β

x
ϕ(x)

(cid:21) (cid:20)

y
ϕ(y)

−
−

≥

(cid:21)

0,

2

−

(6)

(7)

which, in view of Deﬁnition 2, is an incremental quadratic constraint for ϕ. From this perspective, incremental
quadratic constraints generalize the notion of slope-restricted nonlinearities to multi-variable vector-valued
nonlinearities.

Repeated nonlinearities. Now consider the vector-valued function φ(x) = [ϕ(x1)
ϕ(xn)](cid:62) obtained by
Rn. By exploiting the fact that the
applying a slope-restricted function ϕ component-wise to a vector x
same function ϕ is applied to each component, we can characterize φ(x) by O(n2) incremental quadratic
constraints. In the following lemma, we provide such a characterization.
R is slope-restricted on [α, β]. Deﬁne the set
Lemma 1 Suppose ϕ : R

· · ·

∈

→

Tn =

T
{

∈

Sn

|

n

T =

λiieie(cid:62)i +

i=1
(cid:88)
∈ Tn the vector-valued function φ(x) = [ϕ(x1)
x
2αβT
−
φ(x)
(α + β)T

(α + β)T
2T

x
φ(x)

y
φ(y)

i<j
(cid:88)1
≤
≤

(cid:62)

n

−
−

· · ·
y
φ(y)

−
−

λij(ei −

ej)(ei −

ej)(cid:62), λij ≥

0

.
}

ϕ(xn)](cid:62) : Rn

Rn satisﬁes

→

0 for all x, y

≥

Rn.

∈

(8)

(9)

(cid:20)

(cid:20)

(cid:21)

−

(cid:21) (cid:20)
Concretely, this lemma captures the coupling between neurons in a neural network by taking advantage
of two particular structures: (a) the same activation function is applied to each hidden neuron and (b)
all activation functions are slope-restricted on the same interval [α, β]. In this way, we can write the slope
restriction condition in (1) for any pair of activation functions in a given neural network. A conic combination
of these constraints would yield (9), where λij are the coeﬃcients of this combination. See Figure 1 for an
illustrative description.

(cid:21)

We will see in the next section that the matrix T that parameterizes the multiplier matrix in (9) appears
as a decision variable in an SDP, in which the objective is to ﬁnd an admissible T that yields the tightest
bound on the Lipschitz constant.

4

Then for any T

Figure 1: An illustrative description of encoding activation functions by quadratic constraints.

2.3 LipSDP for single-layer neural network

To develop an optimization problem to estimate the Lipschitz constant of a fully-connected feed-forward
neural network, the key insight is that the Lipschitz condition in (1) is in fact equivalent to an incremental
quadratic constraint for the map x
f (x) characterized by the neural network. By coupling this to the
incremental quadratic constraints satisﬁed by the cascade combination of the activation functions [13], we
can develop an SDP to minimize an upper bound on the Lipschitz constant of f . This result is formally
stated in the following theorem.

(cid:55)→

Theorem 1 (Lipshitz certiﬁcates for single-layer neural networks) Consider a single-layer neural
network described by f (x) = W 1φ(W 0x + b0) + b1. Suppose φ(x) : Rn
ϕ(xn)], where
ϕ is slope-restricted in the sector [α, β]. Deﬁne
Tn as in (8). Suppose there exists a ρ > 0 such that the
matrix inequality

Rn = [ϕ(x1)

· · ·

→

M (ρ, T ) :=

(cid:34)

−

2αβW 0(cid:62)T W 0

−
(α + β)T W 0

ρIn0

(α + β)W 0(cid:62)T
2T + W 1(cid:62)W 1(cid:35) (cid:22)

0,

(10)

holds for some T

∈ Tn. Then

f (x)

f (y)

(cid:107)2 ≤

√ρ

x
(cid:107)

−
Theorem 1 provides us with a suﬃcient condition for L2 = √ρ to be an upper bound on the Lipschitz
constant of f (x) = W 1φ(W 0x+b0)+b1. In particular, we can ﬁnd the tightest bound by solving the following
optimization problem:

−

∈

(cid:107)

Rn0.

−
(cid:107)2 for all x, y

y

where the decision variables are (ρ, T )
convex. Hence, (11) is an SDP, which can be solved numerically for its global minimum.

∈

subject to M (ρ, T )
and T
R+ × Tn. Note that M (ρ, T ) is linear in ρ and T and the set

∈ Tn,

(cid:22)

0

(11)

Tn is

minimize ρ

2.4 LipSDP for multi-layer neural networks

We now consider the multi-layer case. Assuming that all the activation functions are the same, we can write
the neural network model in (2) compactly as

Bx = φ(Ax + b)

and f (x) = Cx + b(cid:96),

(12)

5

-505-1-0.500.51(xj,'(xj))<latexit sha1_base64="7KVgF9Hepxz+AlzTMESMIJp61E0=">AAAB/XicbZDLSsNAFIZP6q3WW7zs3ARboQUpSTe6LLhxWcFeoA1hMp20YyeTMDMp1lJ8FTcuFHHre7jzbZy0WWjrDwMf/zmHc+b3Y0alsu1vI7e2vrG5ld8u7Ozu7R+Yh0ctGSUCkyaOWCQ6PpKEUU6aiipGOrEgKPQZafuj67TeHhMhacTv1CQmbogGnAYUI6UtzzwplR+8+4veGIl4SFOuVEqeWbSr9lzWKjgZFCFTwzO/ev0IJyHhCjMkZdexY+VOkVAUMzIr9BJJYoRHaEC6GjkKiXSn8+tn1rl2+lYQCf24subu74kpCqWchL7uDJEayuVaav5X6yYquHKnlMeJIhwvFgUJs1RkpVFYfSoIVmyiAWFB9a0WHiKBsNKBFXQIzvKXV6FVqzqab2vFej2LIw+ncAZlcOAS6nADDWgChkd4hld4M56MF+Pd+Fi05oxs5hj+yPj8AXzPk+0=</latexit><latexit sha1_base64="7KVgF9Hepxz+AlzTMESMIJp61E0=">AAAB/XicbZDLSsNAFIZP6q3WW7zs3ARboQUpSTe6LLhxWcFeoA1hMp20YyeTMDMp1lJ8FTcuFHHre7jzbZy0WWjrDwMf/zmHc+b3Y0alsu1vI7e2vrG5ld8u7Ozu7R+Yh0ctGSUCkyaOWCQ6PpKEUU6aiipGOrEgKPQZafuj67TeHhMhacTv1CQmbogGnAYUI6UtzzwplR+8+4veGIl4SFOuVEqeWbSr9lzWKjgZFCFTwzO/ev0IJyHhCjMkZdexY+VOkVAUMzIr9BJJYoRHaEC6GjkKiXSn8+tn1rl2+lYQCf24subu74kpCqWchL7uDJEayuVaav5X6yYquHKnlMeJIhwvFgUJs1RkpVFYfSoIVmyiAWFB9a0WHiKBsNKBFXQIzvKXV6FVqzqab2vFej2LIw+ncAZlcOAS6nADDWgChkd4hld4M56MF+Pd+Fi05oxs5hj+yPj8AXzPk+0=</latexit><latexit sha1_base64="7KVgF9Hepxz+AlzTMESMIJp61E0=">AAAB/XicbZDLSsNAFIZP6q3WW7zs3ARboQUpSTe6LLhxWcFeoA1hMp20YyeTMDMp1lJ8FTcuFHHre7jzbZy0WWjrDwMf/zmHc+b3Y0alsu1vI7e2vrG5ld8u7Ozu7R+Yh0ctGSUCkyaOWCQ6PpKEUU6aiipGOrEgKPQZafuj67TeHhMhacTv1CQmbogGnAYUI6UtzzwplR+8+4veGIl4SFOuVEqeWbSr9lzWKjgZFCFTwzO/ev0IJyHhCjMkZdexY+VOkVAUMzIr9BJJYoRHaEC6GjkKiXSn8+tn1rl2+lYQCf24subu74kpCqWchL7uDJEayuVaav5X6yYquHKnlMeJIhwvFgUJs1RkpVFYfSoIVmyiAWFB9a0WHiKBsNKBFXQIzvKXV6FVqzqab2vFej2LIw+ncAZlcOAS6nADDWgChkd4hld4M56MF+Pd+Fi05oxs5hj+yPj8AXzPk+0=</latexit><latexit sha1_base64="7KVgF9Hepxz+AlzTMESMIJp61E0=">AAAB/XicbZDLSsNAFIZP6q3WW7zs3ARboQUpSTe6LLhxWcFeoA1hMp20YyeTMDMp1lJ8FTcuFHHre7jzbZy0WWjrDwMf/zmHc+b3Y0alsu1vI7e2vrG5ld8u7Ozu7R+Yh0ctGSUCkyaOWCQ6PpKEUU6aiipGOrEgKPQZafuj67TeHhMhacTv1CQmbogGnAYUI6UtzzwplR+8+4veGIl4SFOuVEqeWbSr9lzWKjgZFCFTwzO/ev0IJyHhCjMkZdexY+VOkVAUMzIr9BJJYoRHaEC6GjkKiXSn8+tn1rl2+lYQCf24subu74kpCqWchL7uDJEayuVaav5X6yYquHKnlMeJIhwvFgUJs1RkpVFYfSoIVmyiAWFB9a0WHiKBsNKBFXQIzvKXV6FVqzqab2vFej2LIw+ncAZlcOAS6nADDWgChkd4hld4M56MF+Pd+Fi05oxs5hj+yPj8AXzPk+0=</latexit>(xi,'(xi))<latexit sha1_base64="GllpdvBtNW9LH6e2gGOX+rXlP0k=">AAAB/XicbZDLSsNAFIZP6q3WW7zs3ARboQWRpBtdFty4rGAv0IYwmU7aoZNJmJkUayi+ihsXirj1Pdz5Nk7aLLT1h4GP/5zDOfP7MaNS2fa3UVhb39jcKm6Xdnb39g/Mw6O2jBKBSQtHLBJdH0nCKCctRRUj3VgQFPqMdPzxTVbvTIiQNOL3ahoTN0RDTgOKkdKWZ55Uqg8evehPkIhHNONareKZZfvSnstaBSeHMuRqeuZXfxDhJCRcYYak7Dl2rNwUCUUxI7NSP5EkRniMhqSnkaOQSDedXz+zzrUzsIJI6MeVNXd/T6QolHIa+rozRGokl2uZ+V+tl6jg2k0pjxNFOF4sChJmqcjKorAGVBCs2FQDwoLqWy08QgJhpQMr6RCc5S+vQrt+6Wi+q5cbjTyOIpzCGVTBgStowC00oQUYHuEZXuHNeDJejHfjY9FaMPKZY/gj4/MHebWT6w==</latexit><latexit sha1_base64="GllpdvBtNW9LH6e2gGOX+rXlP0k=">AAAB/XicbZDLSsNAFIZP6q3WW7zs3ARboQWRpBtdFty4rGAv0IYwmU7aoZNJmJkUayi+ihsXirj1Pdz5Nk7aLLT1h4GP/5zDOfP7MaNS2fa3UVhb39jcKm6Xdnb39g/Mw6O2jBKBSQtHLBJdH0nCKCctRRUj3VgQFPqMdPzxTVbvTIiQNOL3ahoTN0RDTgOKkdKWZ55Uqg8evehPkIhHNONareKZZfvSnstaBSeHMuRqeuZXfxDhJCRcYYak7Dl2rNwUCUUxI7NSP5EkRniMhqSnkaOQSDedXz+zzrUzsIJI6MeVNXd/T6QolHIa+rozRGokl2uZ+V+tl6jg2k0pjxNFOF4sChJmqcjKorAGVBCs2FQDwoLqWy08QgJhpQMr6RCc5S+vQrt+6Wi+q5cbjTyOIpzCGVTBgStowC00oQUYHuEZXuHNeDJejHfjY9FaMPKZY/gj4/MHebWT6w==</latexit><latexit sha1_base64="GllpdvBtNW9LH6e2gGOX+rXlP0k=">AAAB/XicbZDLSsNAFIZP6q3WW7zs3ARboQWRpBtdFty4rGAv0IYwmU7aoZNJmJkUayi+ihsXirj1Pdz5Nk7aLLT1h4GP/5zDOfP7MaNS2fa3UVhb39jcKm6Xdnb39g/Mw6O2jBKBSQtHLBJdH0nCKCctRRUj3VgQFPqMdPzxTVbvTIiQNOL3ahoTN0RDTgOKkdKWZ55Uqg8evehPkIhHNONareKZZfvSnstaBSeHMuRqeuZXfxDhJCRcYYak7Dl2rNwUCUUxI7NSP5EkRniMhqSnkaOQSDedXz+zzrUzsIJI6MeVNXd/T6QolHIa+rozRGokl2uZ+V+tl6jg2k0pjxNFOF4sChJmqcjKorAGVBCs2FQDwoLqWy08QgJhpQMr6RCc5S+vQrt+6Wi+q5cbjTyOIpzCGVTBgStowC00oQUYHuEZXuHNeDJejHfjY9FaMPKZY/gj4/MHebWT6w==</latexit><latexit sha1_base64="GllpdvBtNW9LH6e2gGOX+rXlP0k=">AAAB/XicbZDLSsNAFIZP6q3WW7zs3ARboQWRpBtdFty4rGAv0IYwmU7aoZNJmJkUayi+ihsXirj1Pdz5Nk7aLLT1h4GP/5zDOfP7MaNS2fa3UVhb39jcKm6Xdnb39g/Mw6O2jBKBSQtHLBJdH0nCKCctRRUj3VgQFPqMdPzxTVbvTIiQNOL3ahoTN0RDTgOKkdKWZ55Uqg8evehPkIhHNONareKZZfvSnstaBSeHMuRqeuZXfxDhJCRcYYak7Dl2rNwUCUUxI7NSP5EkRniMhqSnkaOQSDedXz+zzrUzsIJI6MeVNXd/T6QolHIa+rozRGokl2uZ+V+tl6jg2k0pjxNFOF4sChJmqcjKorAGVBCs2FQDwoLqWy08QgJhpQMr6RCc5S+vQrt+6Wi+q5cbjTyOIpzCGVTBgStowC00oQUYHuEZXuHNeDJejHfjY9FaMPKZY/gj4/MHebWT6w==</latexit>↵'(xj) '(xi)xj xi <latexit sha1_base64="bXpVOSwBhezioWcgI+XxN1XSrkk=">AAACLHicbZDLSgMxFIYzXmu9jbp0E6yCLiwz3eiy0I3LCrYVOqWcSc/Y2MzFJFMsQx/Ija8iiAuLuPU5TNsBrz8EvvznHJLz+4ngSjvOxFpYXFpeWS2sFdc3Nre27Z3dpopTybDBYhHLax8UCh5hQ3Mt8DqRCKEvsOUPatN6a4hS8Ti60qMEOyHcRDzgDLSxunbt0AOR9IF6Au+o1wsksMwbgkz6/Pi+e3ty+nXhJ+PMWKeGxnm/jxoOu3bJKTsz0b/g5lAiuepd+9nrxSwNMdJMgFJt10l0JwOpORM4LnqpwgTYAG6wbTCCEFUnmy07pkfG6dEgluZEms7c7xMZhEqNQt90hqD76ndtav5Xa6c6OO9kPEpSjRGbPxSkguqYTpOjPS6RaTEyAExy81fK+mDy0ibfognB/b3yX2hWyq7hy0qpWs3jKJB9ckCOiUvOSJVckDppEEYeyBN5JRPr0Xqx3qz3eeuClc/skR+yPj4Be/+nuQ==</latexit><latexit sha1_base64="bXpVOSwBhezioWcgI+XxN1XSrkk=">AAACLHicbZDLSgMxFIYzXmu9jbp0E6yCLiwz3eiy0I3LCrYVOqWcSc/Y2MzFJFMsQx/Ija8iiAuLuPU5TNsBrz8EvvznHJLz+4ngSjvOxFpYXFpeWS2sFdc3Nre27Z3dpopTybDBYhHLax8UCh5hQ3Mt8DqRCKEvsOUPatN6a4hS8Ti60qMEOyHcRDzgDLSxunbt0AOR9IF6Au+o1wsksMwbgkz6/Pi+e3ty+nXhJ+PMWKeGxnm/jxoOu3bJKTsz0b/g5lAiuepd+9nrxSwNMdJMgFJt10l0JwOpORM4LnqpwgTYAG6wbTCCEFUnmy07pkfG6dEgluZEms7c7xMZhEqNQt90hqD76ndtav5Xa6c6OO9kPEpSjRGbPxSkguqYTpOjPS6RaTEyAExy81fK+mDy0ibfognB/b3yX2hWyq7hy0qpWs3jKJB9ckCOiUvOSJVckDppEEYeyBN5JRPr0Xqx3qz3eeuClc/skR+yPj4Be/+nuQ==</latexit><latexit sha1_base64="bXpVOSwBhezioWcgI+XxN1XSrkk=">AAACLHicbZDLSgMxFIYzXmu9jbp0E6yCLiwz3eiy0I3LCrYVOqWcSc/Y2MzFJFMsQx/Ija8iiAuLuPU5TNsBrz8EvvznHJLz+4ngSjvOxFpYXFpeWS2sFdc3Nre27Z3dpopTybDBYhHLax8UCh5hQ3Mt8DqRCKEvsOUPatN6a4hS8Ti60qMEOyHcRDzgDLSxunbt0AOR9IF6Au+o1wsksMwbgkz6/Pi+e3ty+nXhJ+PMWKeGxnm/jxoOu3bJKTsz0b/g5lAiuepd+9nrxSwNMdJMgFJt10l0JwOpORM4LnqpwgTYAG6wbTCCEFUnmy07pkfG6dEgluZEms7c7xMZhEqNQt90hqD76ndtav5Xa6c6OO9kPEpSjRGbPxSkguqYTpOjPS6RaTEyAExy81fK+mDy0ibfognB/b3yX2hWyq7hy0qpWs3jKJB9ckCOiUvOSJVckDppEEYeyBN5JRPr0Xqx3qz3eeuClc/skR+yPj4Be/+nuQ==</latexit><latexit sha1_base64="bXpVOSwBhezioWcgI+XxN1XSrkk=">AAACLHicbZDLSgMxFIYzXmu9jbp0E6yCLiwz3eiy0I3LCrYVOqWcSc/Y2MzFJFMsQx/Ija8iiAuLuPU5TNsBrz8EvvznHJLz+4ngSjvOxFpYXFpeWS2sFdc3Nre27Z3dpopTybDBYhHLax8UCh5hQ3Mt8DqRCKEvsOUPatN6a4hS8Ti60qMEOyHcRDzgDLSxunbt0AOR9IF6Au+o1wsksMwbgkz6/Pi+e3ty+nXhJ+PMWKeGxnm/jxoOu3bJKTsz0b/g5lAiuepd+9nrxSwNMdJMgFJt10l0JwOpORM4LnqpwgTYAG6wbTCCEFUnmy07pkfG6dEgluZEms7c7xMZhEqNQt90hqD76ndtav5Xa6c6OO9kPEpSjRGbPxSkguqYTpOjPS6RaTEyAExy81fK+mDy0ibfognB/b3yX2hWyq7hy0qpWs3jKJB9ckCOiUvOSJVckDppEEYeyBN5JRPr0Xqx3qz3eeuClc/skR+yPj4Be/+nuQ==</latexit>xi<latexit sha1_base64="pmg+gnVv+HU4HLbjrH6Vq/vhhsQ=">AAAB7HicbZA9TwJBEIbn8AvxC7W02QgmVuSORkoSG0tM5COBC9lb9mDD3t5ld85ILvwGGwuNsfUH2flvXOAKBd9kkyfvzGRn3iCRwqDrfjuFre2d3b3ifung8Oj4pHx61jFxqhlvs1jGuhdQw6VQvI0CJe8lmtMokLwbTG8X9e4j10bE6gFnCfcjOlYiFIyitdrVp6GoDssVt+YuRTbBy6ECuVrD8tdgFLM04gqZpMb0PTdBP6MaBZN8XhqkhieUTemY9y0qGnHjZ8tl5+TKOiMSxto+hWTp/p7IaGTMLApsZ0RxYtZrC/O/Wj/FsOFnQiUpcsVWH4WpJBiTxeVkJDRnKGcWKNPC7krYhGrK0OZTsiF46ydvQqde8yzf1yvNRh5HES7gEq7Bgxtowh20oA0MBDzDK7w5ynlx3p2PVWvByWfO4Y+czx8VwY4o</latexit><latexit sha1_base64="pmg+gnVv+HU4HLbjrH6Vq/vhhsQ=">AAAB7HicbZA9TwJBEIbn8AvxC7W02QgmVuSORkoSG0tM5COBC9lb9mDD3t5ld85ILvwGGwuNsfUH2flvXOAKBd9kkyfvzGRn3iCRwqDrfjuFre2d3b3ifung8Oj4pHx61jFxqhlvs1jGuhdQw6VQvI0CJe8lmtMokLwbTG8X9e4j10bE6gFnCfcjOlYiFIyitdrVp6GoDssVt+YuRTbBy6ECuVrD8tdgFLM04gqZpMb0PTdBP6MaBZN8XhqkhieUTemY9y0qGnHjZ8tl5+TKOiMSxto+hWTp/p7IaGTMLApsZ0RxYtZrC/O/Wj/FsOFnQiUpcsVWH4WpJBiTxeVkJDRnKGcWKNPC7krYhGrK0OZTsiF46ydvQqde8yzf1yvNRh5HES7gEq7Bgxtowh20oA0MBDzDK7w5ynlx3p2PVWvByWfO4Y+czx8VwY4o</latexit><latexit sha1_base64="pmg+gnVv+HU4HLbjrH6Vq/vhhsQ=">AAAB7HicbZA9TwJBEIbn8AvxC7W02QgmVuSORkoSG0tM5COBC9lb9mDD3t5ld85ILvwGGwuNsfUH2flvXOAKBd9kkyfvzGRn3iCRwqDrfjuFre2d3b3ifung8Oj4pHx61jFxqhlvs1jGuhdQw6VQvI0CJe8lmtMokLwbTG8X9e4j10bE6gFnCfcjOlYiFIyitdrVp6GoDssVt+YuRTbBy6ECuVrD8tdgFLM04gqZpMb0PTdBP6MaBZN8XhqkhieUTemY9y0qGnHjZ8tl5+TKOiMSxto+hWTp/p7IaGTMLApsZ0RxYtZrC/O/Wj/FsOFnQiUpcsVWH4WpJBiTxeVkJDRnKGcWKNPC7krYhGrK0OZTsiF46ydvQqde8yzf1yvNRh5HES7gEq7Bgxtowh20oA0MBDzDK7w5ynlx3p2PVWvByWfO4Y+czx8VwY4o</latexit><latexit sha1_base64="pmg+gnVv+HU4HLbjrH6Vq/vhhsQ=">AAAB7HicbZA9TwJBEIbn8AvxC7W02QgmVuSORkoSG0tM5COBC9lb9mDD3t5ld85ILvwGGwuNsfUH2flvXOAKBd9kkyfvzGRn3iCRwqDrfjuFre2d3b3ifung8Oj4pHx61jFxqhlvs1jGuhdQw6VQvI0CJe8lmtMokLwbTG8X9e4j10bE6gFnCfcjOlYiFIyitdrVp6GoDssVt+YuRTbBy6ECuVrD8tdgFLM04gqZpMb0PTdBP6MaBZN8XhqkhieUTemY9y0qGnHjZ8tl5+TKOiMSxto+hWTp/p7IaGTMLApsZ0RxYtZrC/O/Wj/FsOFnQiUpcsVWH4WpJBiTxeVkJDRnKGcWKNPC7krYhGrK0OZTsiF46ydvQqde8yzf1yvNRh5HES7gEq7Bgxtowh20oA0MBDzDK7w5ynlx3p2PVWvByWfO4Y+czx8VwY4o</latexit>'(xi)<latexit sha1_base64="2RSP5OM1rKvlHPSZBvlymYbyN3E=">AAAB9XicbZDLTgIxFIbP4A3xhrp00wgmuCEzbHRJ4sYlJnJJYCSd0oGGTjtpOyiZ8B5uXGiMW9/FnW9jgVko+CdNvvznnJzTP4g508Z1v53cxubW9k5+t7C3f3B4VDw+aWmZKEKbRHKpOgHWlDNBm4YZTjuxojgKOG0H45t5vT2hSjMp7s00pn6Eh4KFjGBjrYdyb4JVPGKVpz67LPeLJbfqLoTWwcugBJka/eJXbyBJElFhCMdadz03Nn6KlWGE01mhl2gaYzLGQ9q1KHBEtZ8urp6hC+sMUCiVfcKghft7IsWR1tMosJ0RNiO9Wpub/9W6iQmv/ZSJODFUkOWiMOHISDSPAA2YosTwqQVMFLO3IjLCChNjgyrYELzVL69Dq1b1LN/VSvV6FkcezuAcKuDBFdThFhrQBAIKnuEV3pxH58V5dz6WrTknmzmFP3I+fwBh5JHB</latexit><latexit sha1_base64="2RSP5OM1rKvlHPSZBvlymYbyN3E=">AAAB9XicbZDLTgIxFIbP4A3xhrp00wgmuCEzbHRJ4sYlJnJJYCSd0oGGTjtpOyiZ8B5uXGiMW9/FnW9jgVko+CdNvvznnJzTP4g508Z1v53cxubW9k5+t7C3f3B4VDw+aWmZKEKbRHKpOgHWlDNBm4YZTjuxojgKOG0H45t5vT2hSjMp7s00pn6Eh4KFjGBjrYdyb4JVPGKVpz67LPeLJbfqLoTWwcugBJka/eJXbyBJElFhCMdadz03Nn6KlWGE01mhl2gaYzLGQ9q1KHBEtZ8urp6hC+sMUCiVfcKghft7IsWR1tMosJ0RNiO9Wpub/9W6iQmv/ZSJODFUkOWiMOHISDSPAA2YosTwqQVMFLO3IjLCChNjgyrYELzVL69Dq1b1LN/VSvV6FkcezuAcKuDBFdThFhrQBAIKnuEV3pxH58V5dz6WrTknmzmFP3I+fwBh5JHB</latexit><latexit sha1_base64="2RSP5OM1rKvlHPSZBvlymYbyN3E=">AAAB9XicbZDLTgIxFIbP4A3xhrp00wgmuCEzbHRJ4sYlJnJJYCSd0oGGTjtpOyiZ8B5uXGiMW9/FnW9jgVko+CdNvvznnJzTP4g508Z1v53cxubW9k5+t7C3f3B4VDw+aWmZKEKbRHKpOgHWlDNBm4YZTjuxojgKOG0H45t5vT2hSjMp7s00pn6Eh4KFjGBjrYdyb4JVPGKVpz67LPeLJbfqLoTWwcugBJka/eJXbyBJElFhCMdadz03Nn6KlWGE01mhl2gaYzLGQ9q1KHBEtZ8urp6hC+sMUCiVfcKghft7IsWR1tMosJ0RNiO9Wpub/9W6iQmv/ZSJODFUkOWiMOHISDSPAA2YosTwqQVMFLO3IjLCChNjgyrYELzVL69Dq1b1LN/VSvV6FkcezuAcKuDBFdThFhrQBAIKnuEV3pxH58V5dz6WrTknmzmFP3I+fwBh5JHB</latexit><latexit sha1_base64="2RSP5OM1rKvlHPSZBvlymYbyN3E=">AAAB9XicbZDLTgIxFIbP4A3xhrp00wgmuCEzbHRJ4sYlJnJJYCSd0oGGTjtpOyiZ8B5uXGiMW9/FnW9jgVko+CdNvvznnJzTP4g508Z1v53cxubW9k5+t7C3f3B4VDw+aWmZKEKbRHKpOgHWlDNBm4YZTjuxojgKOG0H45t5vT2hSjMp7s00pn6Eh4KFjGBjrYdyb4JVPGKVpz67LPeLJbfqLoTWwcugBJka/eJXbyBJElFhCMdadz03Nn6KlWGE01mhl2gaYzLGQ9q1KHBEtZ8urp6hC+sMUCiVfcKghft7IsWR1tMosJ0RNiO9Wpub/9W6iQmv/ZSJODFUkOWiMOHISDSPAA2YosTwqQVMFLO3IjLCChNjgyrYELzVL69Dq1b1LN/VSvV6FkcezuAcKuDBFdThFhrQBAIKnuEV3pxH58V5dz6WrTknmzmFP3I+fwBh5JHB</latexit>'(yi)<latexit sha1_base64="RG1iAtaTSYwavALhRg+orYL4LFU=">AAAB9XicbZDLTgIxFIbPeEW8oS7dNIIJbsgMG12SuHGJiVwSGEmndKCh02naDmYy4T3cuNAYt76LO9/GArNQ8E+afPnPOTmnfyA508Z1v52Nza3tnd3CXnH/4PDouHRy2tZxoghtkZjHqhtgTTkTtGWY4bQrFcVRwGknmNzO650pVZrF4sGkkvoRHgkWMoKNtR4r/SlWcsyq6YBdVQalsltzF0Lr4OVQhlzNQemrP4xJElFhCMda9zxXGj/DyjDC6azYTzSVmEzwiPYsChxR7WeLq2fo0jpDFMbKPmHQwv09keFI6zQKbGeEzViv1ubmf7VeYsIbP2NCJoYKslwUJhyZGM0jQEOmKDE8tYCJYvZWRMZYYWJsUEUbgrf65XVo12ue5ft6udHI4yjAOVxAFTy4hgbcQRNaQEDBM7zCm/PkvDjvzseydcPJZ87gj5zPH2NskcI=</latexit><latexit sha1_base64="RG1iAtaTSYwavALhRg+orYL4LFU=">AAAB9XicbZDLTgIxFIbPeEW8oS7dNIIJbsgMG12SuHGJiVwSGEmndKCh02naDmYy4T3cuNAYt76LO9/GArNQ8E+afPnPOTmnfyA508Z1v52Nza3tnd3CXnH/4PDouHRy2tZxoghtkZjHqhtgTTkTtGWY4bQrFcVRwGknmNzO650pVZrF4sGkkvoRHgkWMoKNtR4r/SlWcsyq6YBdVQalsltzF0Lr4OVQhlzNQemrP4xJElFhCMda9zxXGj/DyjDC6azYTzSVmEzwiPYsChxR7WeLq2fo0jpDFMbKPmHQwv09keFI6zQKbGeEzViv1ubmf7VeYsIbP2NCJoYKslwUJhyZGM0jQEOmKDE8tYCJYvZWRMZYYWJsUEUbgrf65XVo12ue5ft6udHI4yjAOVxAFTy4hgbcQRNaQEDBM7zCm/PkvDjvzseydcPJZ87gj5zPH2NskcI=</latexit><latexit sha1_base64="RG1iAtaTSYwavALhRg+orYL4LFU=">AAAB9XicbZDLTgIxFIbPeEW8oS7dNIIJbsgMG12SuHGJiVwSGEmndKCh02naDmYy4T3cuNAYt76LO9/GArNQ8E+afPnPOTmnfyA508Z1v52Nza3tnd3CXnH/4PDouHRy2tZxoghtkZjHqhtgTTkTtGWY4bQrFcVRwGknmNzO650pVZrF4sGkkvoRHgkWMoKNtR4r/SlWcsyq6YBdVQalsltzF0Lr4OVQhlzNQemrP4xJElFhCMda9zxXGj/DyjDC6azYTzSVmEzwiPYsChxR7WeLq2fo0jpDFMbKPmHQwv09keFI6zQKbGeEzViv1ubmf7VeYsIbP2NCJoYKslwUJhyZGM0jQEOmKDE8tYCJYvZWRMZYYWJsUEUbgrf65XVo12ue5ft6udHI4yjAOVxAFTy4hgbcQRNaQEDBM7zCm/PkvDjvzseydcPJZ87gj5zPH2NskcI=</latexit><latexit sha1_base64="RG1iAtaTSYwavALhRg+orYL4LFU=">AAAB9XicbZDLTgIxFIbPeEW8oS7dNIIJbsgMG12SuHGJiVwSGEmndKCh02naDmYy4T3cuNAYt76LO9/GArNQ8E+afPnPOTmnfyA508Z1v52Nza3tnd3CXnH/4PDouHRy2tZxoghtkZjHqhtgTTkTtGWY4bQrFcVRwGknmNzO650pVZrF4sGkkvoRHgkWMoKNtR4r/SlWcsyq6YBdVQalsltzF0Lr4OVQhlzNQemrP4xJElFhCMda9zxXGj/DyjDC6azYTzSVmEzwiPYsChxR7WeLq2fo0jpDFMbKPmHQwv09keFI6zQKbGeEzViv1ubmf7VeYsIbP2NCJoYKslwUJhyZGM0jQEOmKDE8tYCJYvZWRMZYYWJsUEUbgrf65XVo12ue5ft6udHI4yjAOVxAFTy4hgbcQRNaQEDBM7zCm/PkvDjvzseydcPJZ87gj5zPH2NskcI=</latexit>yi<latexit sha1_base64="WmgSYaT4KZFZh7wSYgzHlBn+laY=">AAAB7HicbZBNT8JAEIan+IX4hXr0shFMPJGWix5JvHjExAIJNGS7bGHDdtvsTk1Iw2/w4kFjvPqDvPlvXKAHBd9kkyfvzGRn3jCVwqDrfjulre2d3b3yfuXg8Oj4pHp61jFJphn3WSIT3Qup4VIo7qNAyXup5jQOJe+G07tFvfvEtRGJesRZyoOYjpWIBKNoLb8+G4r6sFpzG+5SZBO8AmpQqD2sfg1GCctirpBJakzfc1MMcqpRMMnnlUFmeErZlI5536KiMTdBvlx2Tq6sMyJRou1TSJbu74mcxsbM4tB2xhQnZr22MP+r9TOMboNcqDRDrtjqoyiTBBOyuJyMhOYM5cwCZVrYXQmbUE0Z2nwqNgRv/eRN6DQbnuWHZq3VKuIowwVcwjV4cAMtuIc2+MBAwDO8wpujnBfn3flYtZacYuYc/sj5/AEZsI4x</latexit><latexit sha1_base64="WmgSYaT4KZFZh7wSYgzHlBn+laY=">AAAB7HicbZBNT8JAEIan+IX4hXr0shFMPJGWix5JvHjExAIJNGS7bGHDdtvsTk1Iw2/w4kFjvPqDvPlvXKAHBd9kkyfvzGRn3jCVwqDrfjulre2d3b3yfuXg8Oj4pHp61jFJphn3WSIT3Qup4VIo7qNAyXup5jQOJe+G07tFvfvEtRGJesRZyoOYjpWIBKNoLb8+G4r6sFpzG+5SZBO8AmpQqD2sfg1GCctirpBJakzfc1MMcqpRMMnnlUFmeErZlI5536KiMTdBvlx2Tq6sMyJRou1TSJbu74mcxsbM4tB2xhQnZr22MP+r9TOMboNcqDRDrtjqoyiTBBOyuJyMhOYM5cwCZVrYXQmbUE0Z2nwqNgRv/eRN6DQbnuWHZq3VKuIowwVcwjV4cAMtuIc2+MBAwDO8wpujnBfn3flYtZacYuYc/sj5/AEZsI4x</latexit><latexit sha1_base64="WmgSYaT4KZFZh7wSYgzHlBn+laY=">AAAB7HicbZBNT8JAEIan+IX4hXr0shFMPJGWix5JvHjExAIJNGS7bGHDdtvsTk1Iw2/w4kFjvPqDvPlvXKAHBd9kkyfvzGRn3jCVwqDrfjulre2d3b3yfuXg8Oj4pHp61jFJphn3WSIT3Qup4VIo7qNAyXup5jQOJe+G07tFvfvEtRGJesRZyoOYjpWIBKNoLb8+G4r6sFpzG+5SZBO8AmpQqD2sfg1GCctirpBJakzfc1MMcqpRMMnnlUFmeErZlI5536KiMTdBvlx2Tq6sMyJRou1TSJbu74mcxsbM4tB2xhQnZr22MP+r9TOMboNcqDRDrtjqoyiTBBOyuJyMhOYM5cwCZVrYXQmbUE0Z2nwqNgRv/eRN6DQbnuWHZq3VKuIowwVcwjV4cAMtuIc2+MBAwDO8wpujnBfn3flYtZacYuYc/sj5/AEZsI4x</latexit><latexit sha1_base64="WmgSYaT4KZFZh7wSYgzHlBn+laY=">AAAB7HicbZBNT8JAEIan+IX4hXr0shFMPJGWix5JvHjExAIJNGS7bGHDdtvsTk1Iw2/w4kFjvPqDvPlvXKAHBd9kkyfvzGRn3jCVwqDrfjulre2d3b3yfuXg8Oj4pHp61jFJphn3WSIT3Qup4VIo7qNAyXup5jQOJe+G07tFvfvEtRGJesRZyoOYjpWIBKNoLb8+G4r6sFpzG+5SZBO8AmpQqD2sfg1GCctirpBJakzfc1MMcqpRMMnnlUFmeErZlI5536KiMTdBvlx2Tq6sMyJRou1TSJbu74mcxsbM4tB2xhQnZr22MP+r9TOMboNcqDRDrtjqoyiTBBOyuJyMhOYM5cwCZVrYXQmbUE0Z2nwqNgRv/eRN6DQbnuWHZq3VKuIowwVcwjV4cAMtuIc2+MBAwDO8wpujnBfn3flYtZacYuYc/sj5/AEZsI4x</latexit>-505-1-0.500.51(xi,'(xi))<latexit sha1_base64="GllpdvBtNW9LH6e2gGOX+rXlP0k=">AAAB/XicbZDLSsNAFIZP6q3WW7zs3ARboQWRpBtdFty4rGAv0IYwmU7aoZNJmJkUayi+ihsXirj1Pdz5Nk7aLLT1h4GP/5zDOfP7MaNS2fa3UVhb39jcKm6Xdnb39g/Mw6O2jBKBSQtHLBJdH0nCKCctRRUj3VgQFPqMdPzxTVbvTIiQNOL3ahoTN0RDTgOKkdKWZ55Uqg8evehPkIhHNONareKZZfvSnstaBSeHMuRqeuZXfxDhJCRcYYak7Dl2rNwUCUUxI7NSP5EkRniMhqSnkaOQSDedXz+zzrUzsIJI6MeVNXd/T6QolHIa+rozRGokl2uZ+V+tl6jg2k0pjxNFOF4sChJmqcjKorAGVBCs2FQDwoLqWy08QgJhpQMr6RCc5S+vQrt+6Wi+q5cbjTyOIpzCGVTBgStowC00oQUYHuEZXuHNeDJejHfjY9FaMPKZY/gj4/MHebWT6w==</latexit><latexit sha1_base64="GllpdvBtNW9LH6e2gGOX+rXlP0k=">AAAB/XicbZDLSsNAFIZP6q3WW7zs3ARboQWRpBtdFty4rGAv0IYwmU7aoZNJmJkUayi+ihsXirj1Pdz5Nk7aLLT1h4GP/5zDOfP7MaNS2fa3UVhb39jcKm6Xdnb39g/Mw6O2jBKBSQtHLBJdH0nCKCctRRUj3VgQFPqMdPzxTVbvTIiQNOL3ahoTN0RDTgOKkdKWZ55Uqg8evehPkIhHNONareKZZfvSnstaBSeHMuRqeuZXfxDhJCRcYYak7Dl2rNwUCUUxI7NSP5EkRniMhqSnkaOQSDedXz+zzrUzsIJI6MeVNXd/T6QolHIa+rozRGokl2uZ+V+tl6jg2k0pjxNFOF4sChJmqcjKorAGVBCs2FQDwoLqWy08QgJhpQMr6RCc5S+vQrt+6Wi+q5cbjTyOIpzCGVTBgStowC00oQUYHuEZXuHNeDJejHfjY9FaMPKZY/gj4/MHebWT6w==</latexit><latexit sha1_base64="GllpdvBtNW9LH6e2gGOX+rXlP0k=">AAAB/XicbZDLSsNAFIZP6q3WW7zs3ARboQWRpBtdFty4rGAv0IYwmU7aoZNJmJkUayi+ihsXirj1Pdz5Nk7aLLT1h4GP/5zDOfP7MaNS2fa3UVhb39jcKm6Xdnb39g/Mw6O2jBKBSQtHLBJdH0nCKCctRRUj3VgQFPqMdPzxTVbvTIiQNOL3ahoTN0RDTgOKkdKWZ55Uqg8evehPkIhHNONareKZZfvSnstaBSeHMuRqeuZXfxDhJCRcYYak7Dl2rNwUCUUxI7NSP5EkRniMhqSnkaOQSDedXz+zzrUzsIJI6MeVNXd/T6QolHIa+rozRGokl2uZ+V+tl6jg2k0pjxNFOF4sChJmqcjKorAGVBCs2FQDwoLqWy08QgJhpQMr6RCc5S+vQrt+6Wi+q5cbjTyOIpzCGVTBgStowC00oQUYHuEZXuHNeDJejHfjY9FaMPKZY/gj4/MHebWT6w==</latexit><latexit sha1_base64="GllpdvBtNW9LH6e2gGOX+rXlP0k=">AAAB/XicbZDLSsNAFIZP6q3WW7zs3ARboQWRpBtdFty4rGAv0IYwmU7aoZNJmJkUayi+ihsXirj1Pdz5Nk7aLLT1h4GP/5zDOfP7MaNS2fa3UVhb39jcKm6Xdnb39g/Mw6O2jBKBSQtHLBJdH0nCKCctRRUj3VgQFPqMdPzxTVbvTIiQNOL3ahoTN0RDTgOKkdKWZ55Uqg8evehPkIhHNONareKZZfvSnstaBSeHMuRqeuZXfxDhJCRcYYak7Dl2rNwUCUUxI7NSP5EkRniMhqSnkaOQSDedXz+zzrUzsIJI6MeVNXd/T6QolHIa+rozRGokl2uZ+V+tl6jg2k0pjxNFOF4sChJmqcjKorAGVBCs2FQDwoLqWy08QgJhpQMr6RCc5S+vQrt+6Wi+q5cbjTyOIpzCGVTBgStowC00oQUYHuEZXuHNeDJejHfjY9FaMPKZY/gj4/MHebWT6w==</latexit>(yi,'(yi))<latexit sha1_base64="OwRczwPm0a80lXMROfDF4JnscN8=">AAAB/XicbZDLSsNAFIZP6q3WW7zs3ARboQWRpBtdScGNywr2Am0Ik+mkHTqZhJlJIZbiq7hxoYhb38Odb+OkzUJbfxj4+M85nDO/HzMqlW1/G4W19Y3NreJ2aWd3b//APDxqyygRmLRwxCLR9ZEkjHLSUlQx0o0FQaHPSMcf32b1zoQISSP+oNKYuCEachpQjJS2PPOkUk09etGfIBGPaMa1WsUzy/alPZe1Ck4OZcjV9Myv/iDCSUi4wgxJ2XPsWLlTJBTFjMxK/USSGOExGpKeRo5CIt3p/PqZda6dgRVEQj+urLn7e2KKQinT0NedIVIjuVzLzP9qvUQF1+6U8jhRhOPFoiBhloqsLAprQAXBiqUaEBZU32rhERIIKx1YSYfgLH95Fdr1S0fzfb3cuMnjKMIpnEEVHLiCBtxBE1qA4RGe4RXejCfjxXg3PhatBSOfOYY/Mj5/AHw5k+s=</latexit><latexit sha1_base64="OwRczwPm0a80lXMROfDF4JnscN8=">AAAB/XicbZDLSsNAFIZP6q3WW7zs3ARboQWRpBtdScGNywr2Am0Ik+mkHTqZhJlJIZbiq7hxoYhb38Odb+OkzUJbfxj4+M85nDO/HzMqlW1/G4W19Y3NreJ2aWd3b//APDxqyygRmLRwxCLR9ZEkjHLSUlQx0o0FQaHPSMcf32b1zoQISSP+oNKYuCEachpQjJS2PPOkUk09etGfIBGPaMa1WsUzy/alPZe1Ck4OZcjV9Myv/iDCSUi4wgxJ2XPsWLlTJBTFjMxK/USSGOExGpKeRo5CIt3p/PqZda6dgRVEQj+urLn7e2KKQinT0NedIVIjuVzLzP9qvUQF1+6U8jhRhOPFoiBhloqsLAprQAXBiqUaEBZU32rhERIIKx1YSYfgLH95Fdr1S0fzfb3cuMnjKMIpnEEVHLiCBtxBE1qA4RGe4RXejCfjxXg3PhatBSOfOYY/Mj5/AHw5k+s=</latexit><latexit sha1_base64="OwRczwPm0a80lXMROfDF4JnscN8=">AAAB/XicbZDLSsNAFIZP6q3WW7zs3ARboQWRpBtdScGNywr2Am0Ik+mkHTqZhJlJIZbiq7hxoYhb38Odb+OkzUJbfxj4+M85nDO/HzMqlW1/G4W19Y3NreJ2aWd3b//APDxqyygRmLRwxCLR9ZEkjHLSUlQx0o0FQaHPSMcf32b1zoQISSP+oNKYuCEachpQjJS2PPOkUk09etGfIBGPaMa1WsUzy/alPZe1Ck4OZcjV9Myv/iDCSUi4wgxJ2XPsWLlTJBTFjMxK/USSGOExGpKeRo5CIt3p/PqZda6dgRVEQj+urLn7e2KKQinT0NedIVIjuVzLzP9qvUQF1+6U8jhRhOPFoiBhloqsLAprQAXBiqUaEBZU32rhERIIKx1YSYfgLH95Fdr1S0fzfb3cuMnjKMIpnEEVHLiCBtxBE1qA4RGe4RXejCfjxXg3PhatBSOfOYY/Mj5/AHw5k+s=</latexit><latexit sha1_base64="OwRczwPm0a80lXMROfDF4JnscN8=">AAAB/XicbZDLSsNAFIZP6q3WW7zs3ARboQWRpBtdScGNywr2Am0Ik+mkHTqZhJlJIZbiq7hxoYhb38Odb+OkzUJbfxj4+M85nDO/HzMqlW1/G4W19Y3NreJ2aWd3b//APDxqyygRmLRwxCLR9ZEkjHLSUlQx0o0FQaHPSMcf32b1zoQISSP+oNKYuCEachpQjJS2PPOkUk09etGfIBGPaMa1WsUzy/alPZe1Ck4OZcjV9Myv/iDCSUi4wgxJ2XPsWLlTJBTFjMxK/USSGOExGpKeRo5CIt3p/PqZda6dgRVEQj+urLn7e2KKQinT0NedIVIjuVzLzP9qvUQF1+6U8jhRhOPFoiBhloqsLAprQAXBiqUaEBZU32rhERIIKx1YSYfgLH95Fdr1S0fzfb3cuMnjKMIpnEEVHLiCBtxBE1qA4RGe4RXejCfjxXg3PhatBSOfOYY/Mj5/AHw5k+s=</latexit>↵'(yi) '(xi)yi xi <latexit sha1_base64="71vqPKjY+TK9gITBZZq4AVlX2HI=">AAACLHicbZBNS8NAEIY3flu/qh69LFZBD0rSix4LXjxWsLbQlDLZTpqlmw93N8US8oO8+FcE8WARr/4OtzWCtg4sPPPODLPzeongStv22FpYXFpeWV1bL21sbm3vlHf37lScSoYNFotYtjxQKHiEDc21wFYiEUJPYNMbXE3qzSFKxePoVo8S7ITQj7jPGWgjdctXRy6IJADqCrynbs+XwDJ3CDIJ+Mmoy0/PfpIHk+SZkc4M5UW/hxqOuuWKfW5Pg86DU0CFFFHvll/cXszSECPNBCjVduxEdzKQmjOBeclNFSbABtDHtsEIQlSdbHpsTo+N0qN+LM2LNJ2qvycyCJUahZ7pDEEHarY2Ef+rtVPtX3YyHiWpxoh9L/JTQXVMJ87RHpfItBgZACa5+StlARi/tPG3ZExwZk+eh7vquWP4plqp1Qo71sgBOSQnxCEXpEauSZ00CCOP5Jm8kbH1ZL1a79bHd+uCVczskz9hfX4BfAOnuQ==</latexit><latexit sha1_base64="71vqPKjY+TK9gITBZZq4AVlX2HI=">AAACLHicbZBNS8NAEIY3flu/qh69LFZBD0rSix4LXjxWsLbQlDLZTpqlmw93N8US8oO8+FcE8WARr/4OtzWCtg4sPPPODLPzeongStv22FpYXFpeWV1bL21sbm3vlHf37lScSoYNFotYtjxQKHiEDc21wFYiEUJPYNMbXE3qzSFKxePoVo8S7ITQj7jPGWgjdctXRy6IJADqCrynbs+XwDJ3CDIJ+Mmoy0/PfpIHk+SZkc4M5UW/hxqOuuWKfW5Pg86DU0CFFFHvll/cXszSECPNBCjVduxEdzKQmjOBeclNFSbABtDHtsEIQlSdbHpsTo+N0qN+LM2LNJ2qvycyCJUahZ7pDEEHarY2Ef+rtVPtX3YyHiWpxoh9L/JTQXVMJ87RHpfItBgZACa5+StlARi/tPG3ZExwZk+eh7vquWP4plqp1Qo71sgBOSQnxCEXpEauSZ00CCOP5Jm8kbH1ZL1a79bHd+uCVczskz9hfX4BfAOnuQ==</latexit><latexit sha1_base64="71vqPKjY+TK9gITBZZq4AVlX2HI=">AAACLHicbZBNS8NAEIY3flu/qh69LFZBD0rSix4LXjxWsLbQlDLZTpqlmw93N8US8oO8+FcE8WARr/4OtzWCtg4sPPPODLPzeongStv22FpYXFpeWV1bL21sbm3vlHf37lScSoYNFotYtjxQKHiEDc21wFYiEUJPYNMbXE3qzSFKxePoVo8S7ITQj7jPGWgjdctXRy6IJADqCrynbs+XwDJ3CDIJ+Mmoy0/PfpIHk+SZkc4M5UW/hxqOuuWKfW5Pg86DU0CFFFHvll/cXszSECPNBCjVduxEdzKQmjOBeclNFSbABtDHtsEIQlSdbHpsTo+N0qN+LM2LNJ2qvycyCJUahZ7pDEEHarY2Ef+rtVPtX3YyHiWpxoh9L/JTQXVMJ87RHpfItBgZACa5+StlARi/tPG3ZExwZk+eh7vquWP4plqp1Qo71sgBOSQnxCEXpEauSZ00CCOP5Jm8kbH1ZL1a79bHd+uCVczskz9hfX4BfAOnuQ==</latexit><latexit sha1_base64="71vqPKjY+TK9gITBZZq4AVlX2HI=">AAACLHicbZBNS8NAEIY3flu/qh69LFZBD0rSix4LXjxWsLbQlDLZTpqlmw93N8US8oO8+FcE8WARr/4OtzWCtg4sPPPODLPzeongStv22FpYXFpeWV1bL21sbm3vlHf37lScSoYNFotYtjxQKHiEDc21wFYiEUJPYNMbXE3qzSFKxePoVo8S7ITQj7jPGWgjdctXRy6IJADqCrynbs+XwDJ3CDIJ+Mmoy0/PfpIHk+SZkc4M5UW/hxqOuuWKfW5Pg86DU0CFFFHvll/cXszSECPNBCjVduxEdzKQmjOBeclNFSbABtDHtsEIQlSdbHpsTo+N0qN+LM2LNJ2qvycyCJUahZ7pDEEHarY2Ef+rtVPtX3YyHiWpxoh9L/JTQXVMJ87RHpfItBgZACa5+StlARi/tPG3ZExwZk+eh7vquWP4plqp1Qo71sgBOSQnxCEXpEauSZ00CCOP5Jm8kbH1ZL1a79bHd+uCVczskz9hfX4BfAOnuQ==</latexit>xj<latexit sha1_base64="Qmcs1an4x+++o9Cy9ZfnlIj1FDM=">AAAB7HicbZA9TwJBEIbn/ET8Qi1tNoKJFbmjkZLExhITD0iAkL1lDlb29i67e0Zy4TfYWGiMrT/Izn/jAlco+CabPHlnJjvzBong2rjut7OxubW9s1vYK+4fHB4dl05OWzpOFUOfxSJWnYBqFFyib7gR2EkU0igQ2A4mN/N6+xGV5rG8N9ME+xEdSR5yRo21/MrT4KEyKJXdqrsQWQcvhzLkag5KX71hzNIIpWGCat313MT0M6oMZwJnxV6qMaFsQkfYtShphLqfLZadkUvrDEkYK/ukIQv390RGI62nUWA7I2rGerU2N/+rdVMT1vsZl0lqULLlR2EqiInJ/HIy5AqZEVMLlCludyVsTBVlxuZTtCF4qyevQ6tW9Szf1cqNeh5HAc7hAq7Ag2towC00wQcGHJ7hFd4c6bw4787HsnXDyWfO4I+czx8XRo4p</latexit><latexit sha1_base64="Qmcs1an4x+++o9Cy9ZfnlIj1FDM=">AAAB7HicbZA9TwJBEIbn/ET8Qi1tNoKJFbmjkZLExhITD0iAkL1lDlb29i67e0Zy4TfYWGiMrT/Izn/jAlco+CabPHlnJjvzBong2rjut7OxubW9s1vYK+4fHB4dl05OWzpOFUOfxSJWnYBqFFyib7gR2EkU0igQ2A4mN/N6+xGV5rG8N9ME+xEdSR5yRo21/MrT4KEyKJXdqrsQWQcvhzLkag5KX71hzNIIpWGCat313MT0M6oMZwJnxV6qMaFsQkfYtShphLqfLZadkUvrDEkYK/ukIQv390RGI62nUWA7I2rGerU2N/+rdVMT1vsZl0lqULLlR2EqiInJ/HIy5AqZEVMLlCludyVsTBVlxuZTtCF4qyevQ6tW9Szf1cqNeh5HAc7hAq7Ag2towC00wQcGHJ7hFd4c6bw4787HsnXDyWfO4I+czx8XRo4p</latexit><latexit sha1_base64="Qmcs1an4x+++o9Cy9ZfnlIj1FDM=">AAAB7HicbZA9TwJBEIbn/ET8Qi1tNoKJFbmjkZLExhITD0iAkL1lDlb29i67e0Zy4TfYWGiMrT/Izn/jAlco+CabPHlnJjvzBong2rjut7OxubW9s1vYK+4fHB4dl05OWzpOFUOfxSJWnYBqFFyib7gR2EkU0igQ2A4mN/N6+xGV5rG8N9ME+xEdSR5yRo21/MrT4KEyKJXdqrsQWQcvhzLkag5KX71hzNIIpWGCat313MT0M6oMZwJnxV6qMaFsQkfYtShphLqfLZadkUvrDEkYK/ukIQv390RGI62nUWA7I2rGerU2N/+rdVMT1vsZl0lqULLlR2EqiInJ/HIy5AqZEVMLlCludyVsTBVlxuZTtCF4qyevQ6tW9Szf1cqNeh5HAc7hAq7Ag2towC00wQcGHJ7hFd4c6bw4787HsnXDyWfO4I+czx8XRo4p</latexit><latexit sha1_base64="Qmcs1an4x+++o9Cy9ZfnlIj1FDM=">AAAB7HicbZA9TwJBEIbn/ET8Qi1tNoKJFbmjkZLExhITD0iAkL1lDlb29i67e0Zy4TfYWGiMrT/Izn/jAlco+CabPHlnJjvzBong2rjut7OxubW9s1vYK+4fHB4dl05OWzpOFUOfxSJWnYBqFFyib7gR2EkU0igQ2A4mN/N6+xGV5rG8N9ME+xEdSR5yRo21/MrT4KEyKJXdqrsQWQcvhzLkag5KX71hzNIIpWGCat313MT0M6oMZwJnxV6qMaFsQkfYtShphLqfLZadkUvrDEkYK/ukIQv390RGI62nUWA7I2rGerU2N/+rdVMT1vsZl0lqULLlR2EqiInJ/HIy5AqZEVMLlCludyVsTBVlxuZTtCF4qyevQ6tW9Szf1cqNeh5HAc7hAq7Ag2towC00wQcGHJ7hFd4c6bw4787HsnXDyWfO4I+czx8XRo4p</latexit>yj<latexit sha1_base64="GXeTI3hCKBcAvNjSYFW2ns3xRoM=">AAAB7HicbZDNTgIxFIVv8Q/xD3XpphFMXJEZNrIkceMSEwdIYEI6pQOVTmfSdkwmE57BjQuNcesDufNtLDALBU/S5Mu596b3niARXBvH+Ualre2d3b3yfuXg8Oj4pHp61tVxqijzaCxi1Q+IZoJL5hluBOsnipEoEKwXzG4X9d4TU5rH8sFkCfMjMpE85JQYa3n1bPRYH1VrTsNZCm+CW0ANCnVG1a/hOKZpxKShgmg9cJ3E+DlRhlPB5pVhqllC6IxM2MCiJBHTfr5cdo6vrDPGYazskwYv3d8TOYm0zqLAdkbETPV6bWH+VxukJmz5OZdJapikq4/CVGAT48XleMwVo0ZkFghV3O6K6ZQoQo3Np2JDcNdP3oRus+Favm/W2q0ijjJcwCVcgws30IY76IAHFDg8wyu8IYle0Dv6WLWWUDFzDn+EPn8AGM2OKg==</latexit><latexit sha1_base64="GXeTI3hCKBcAvNjSYFW2ns3xRoM=">AAAB7HicbZDNTgIxFIVv8Q/xD3XpphFMXJEZNrIkceMSEwdIYEI6pQOVTmfSdkwmE57BjQuNcesDufNtLDALBU/S5Mu596b3niARXBvH+Ualre2d3b3yfuXg8Oj4pHp61tVxqijzaCxi1Q+IZoJL5hluBOsnipEoEKwXzG4X9d4TU5rH8sFkCfMjMpE85JQYa3n1bPRYH1VrTsNZCm+CW0ANCnVG1a/hOKZpxKShgmg9cJ3E+DlRhlPB5pVhqllC6IxM2MCiJBHTfr5cdo6vrDPGYazskwYv3d8TOYm0zqLAdkbETPV6bWH+VxukJmz5OZdJapikq4/CVGAT48XleMwVo0ZkFghV3O6K6ZQoQo3Np2JDcNdP3oRus+Favm/W2q0ijjJcwCVcgws30IY76IAHFDg8wyu8IYle0Dv6WLWWUDFzDn+EPn8AGM2OKg==</latexit><latexit sha1_base64="GXeTI3hCKBcAvNjSYFW2ns3xRoM=">AAAB7HicbZDNTgIxFIVv8Q/xD3XpphFMXJEZNrIkceMSEwdIYEI6pQOVTmfSdkwmE57BjQuNcesDufNtLDALBU/S5Mu596b3niARXBvH+Ualre2d3b3yfuXg8Oj4pHp61tVxqijzaCxi1Q+IZoJL5hluBOsnipEoEKwXzG4X9d4TU5rH8sFkCfMjMpE85JQYa3n1bPRYH1VrTsNZCm+CW0ANCnVG1a/hOKZpxKShgmg9cJ3E+DlRhlPB5pVhqllC6IxM2MCiJBHTfr5cdo6vrDPGYazskwYv3d8TOYm0zqLAdkbETPV6bWH+VxukJmz5OZdJapikq4/CVGAT48XleMwVo0ZkFghV3O6K6ZQoQo3Np2JDcNdP3oRus+Favm/W2q0ijjJcwCVcgws30IY76IAHFDg8wyu8IYle0Dv6WLWWUDFzDn+EPn8AGM2OKg==</latexit><latexit sha1_base64="GXeTI3hCKBcAvNjSYFW2ns3xRoM=">AAAB7HicbZDNTgIxFIVv8Q/xD3XpphFMXJEZNrIkceMSEwdIYEI6pQOVTmfSdkwmE57BjQuNcesDufNtLDALBU/S5Mu596b3niARXBvH+Ualre2d3b3yfuXg8Oj4pHp61tVxqijzaCxi1Q+IZoJL5hluBOsnipEoEKwXzG4X9d4TU5rH8sFkCfMjMpE85JQYa3n1bPRYH1VrTsNZCm+CW0ANCnVG1a/hOKZpxKShgmg9cJ3E+DlRhlPB5pVhqllC6IxM2MCiJBHTfr5cdo6vrDPGYazskwYv3d8TOYm0zqLAdkbETPV6bWH+VxukJmz5OZdJapikq4/CVGAT48XleMwVo0ZkFghV3O6K6ZQoQo3Np2JDcNdP3oRus+Favm/W2q0ijjJcwCVcgws30IY76IAHFDg8wyu8IYle0Dv6WLWWUDFzDn+EPn8AGM2OKg==</latexit>'(xj)<latexit sha1_base64="oI6G4J0ZwbR7XFQ0Ze3vRM+PxDI=">AAAB9XicbZDLTsJAFIZP8YZ4Q126mQgmuCEtG1mSuHGJiVwSqGQ6TGFkOm1mpihpeA83LjTGre/izrdxCl0o+CeTfPnPOTlnfi/iTGnb/rZyG5tb2zv53cLe/sHhUfH4pK3CWBLaIiEPZdfDinImaEszzWk3khQHHqcdb3Kd1jtTKhULxZ2eRdQN8EgwnxGsjXVf7k+xjMas8jR4uCwPiiW7ai+E1sHJoASZmoPiV38YkjigQhOOleo5dqTdBEvNCKfzQj9WNMJkgke0Z1DggCo3WVw9RxfGGSI/lOYJjRbu74kEB0rNAs90BliP1WotNf+r9WLt192EiSjWVJDlIj/mSIcojQANmaRE85kBTCQztyIyxhITbYIqmBCc1S+vQ7tWdQzf1kqNehZHHs7gHCrgwBU04Aaa0AICEp7hFd6sR+vFerc+lq05K5s5hT+yPn8AYQKRug==</latexit><latexit sha1_base64="oI6G4J0ZwbR7XFQ0Ze3vRM+PxDI=">AAAB9XicbZDLTsJAFIZP8YZ4Q126mQgmuCEtG1mSuHGJiVwSqGQ6TGFkOm1mpihpeA83LjTGre/izrdxCl0o+CeTfPnPOTlnfi/iTGnb/rZyG5tb2zv53cLe/sHhUfH4pK3CWBLaIiEPZdfDinImaEszzWk3khQHHqcdb3Kd1jtTKhULxZ2eRdQN8EgwnxGsjXVf7k+xjMas8jR4uCwPiiW7ai+E1sHJoASZmoPiV38YkjigQhOOleo5dqTdBEvNCKfzQj9WNMJkgke0Z1DggCo3WVw9RxfGGSI/lOYJjRbu74kEB0rNAs90BliP1WotNf+r9WLt192EiSjWVJDlIj/mSIcojQANmaRE85kBTCQztyIyxhITbYIqmBCc1S+vQ7tWdQzf1kqNehZHHs7gHCrgwBU04Aaa0AICEp7hFd6sR+vFerc+lq05K5s5hT+yPn8AYQKRug==</latexit><latexit sha1_base64="oI6G4J0ZwbR7XFQ0Ze3vRM+PxDI=">AAAB9XicbZDLTsJAFIZP8YZ4Q126mQgmuCEtG1mSuHGJiVwSqGQ6TGFkOm1mpihpeA83LjTGre/izrdxCl0o+CeTfPnPOTlnfi/iTGnb/rZyG5tb2zv53cLe/sHhUfH4pK3CWBLaIiEPZdfDinImaEszzWk3khQHHqcdb3Kd1jtTKhULxZ2eRdQN8EgwnxGsjXVf7k+xjMas8jR4uCwPiiW7ai+E1sHJoASZmoPiV38YkjigQhOOleo5dqTdBEvNCKfzQj9WNMJkgke0Z1DggCo3WVw9RxfGGSI/lOYJjRbu74kEB0rNAs90BliP1WotNf+r9WLt192EiSjWVJDlIj/mSIcojQANmaRE85kBTCQztyIyxhITbYIqmBCc1S+vQ7tWdQzf1kqNehZHHs7gHCrgwBU04Aaa0AICEp7hFd6sR+vFerc+lq05K5s5hT+yPn8AYQKRug==</latexit><latexit sha1_base64="oI6G4J0ZwbR7XFQ0Ze3vRM+PxDI=">AAAB9XicbZDLTsJAFIZP8YZ4Q126mQgmuCEtG1mSuHGJiVwSqGQ6TGFkOm1mpihpeA83LjTGre/izrdxCl0o+CeTfPnPOTlnfi/iTGnb/rZyG5tb2zv53cLe/sHhUfH4pK3CWBLaIiEPZdfDinImaEszzWk3khQHHqcdb3Kd1jtTKhULxZ2eRdQN8EgwnxGsjXVf7k+xjMas8jR4uCwPiiW7ai+E1sHJoASZmoPiV38YkjigQhOOleo5dqTdBEvNCKfzQj9WNMJkgke0Z1DggCo3WVw9RxfGGSI/lOYJjRbu74kEB0rNAs90BliP1WotNf+r9WLt192EiSjWVJDlIj/mSIcojQANmaRE85kBTCQztyIyxhITbYIqmBCc1S+vQ7tWdQzf1kqNehZHHs7gHCrgwBU04Aaa0AICEp7hFd6sR+vFerc+lq05K5s5hT+yPn8AYQKRug==</latexit>'(yj)<latexit sha1_base64="I7R4a32uvuL7gDjagUmI6UlCRsU=">AAAB9XicbZDLTsJAFIZP8YZ4Q126mQgmuCEtG1iSuHGJiVwSqGQ6TGFkOm1mppim4T3cuNAYt76LO9/GAbpQ8E8m+fKfc3LO/F7EmdK2/W3ltrZ3dvfy+4WDw6Pjk+LpWUeFsSS0TUIeyp6HFeVM0LZmmtNeJCkOPE673vRmUe/OqFQsFPc6iagb4LFgPiNYG+uhPJhhGU1YJRk+XpeHxZJdtZdCm+BkUIJMrWHxazAKSRxQoQnHSvUdO9JuiqVmhNN5YRArGmEyxWPaNyhwQJWbLq+eoyvjjJAfSvOERkv390SKA6WSwDOdAdYTtV5bmP/V+rH2G27KRBRrKshqkR9zpEO0iACNmKRE88QAJpKZWxGZYImJNkEVTAjO+pc3oVOrOobvaqVmI4sjDxdwCRVwoA5NuIUWtIGAhGd4hTfryXqx3q2PVWvOymbO4Y+szx9iipG7</latexit><latexit sha1_base64="I7R4a32uvuL7gDjagUmI6UlCRsU=">AAAB9XicbZDLTsJAFIZP8YZ4Q126mQgmuCEtG1iSuHGJiVwSqGQ6TGFkOm1mppim4T3cuNAYt76LO9/GAbpQ8E8m+fKfc3LO/F7EmdK2/W3ltrZ3dvfy+4WDw6Pjk+LpWUeFsSS0TUIeyp6HFeVM0LZmmtNeJCkOPE673vRmUe/OqFQsFPc6iagb4LFgPiNYG+uhPJhhGU1YJRk+XpeHxZJdtZdCm+BkUIJMrWHxazAKSRxQoQnHSvUdO9JuiqVmhNN5YRArGmEyxWPaNyhwQJWbLq+eoyvjjJAfSvOERkv390SKA6WSwDOdAdYTtV5bmP/V+rH2G27KRBRrKshqkR9zpEO0iACNmKRE88QAJpKZWxGZYImJNkEVTAjO+pc3oVOrOobvaqVmI4sjDxdwCRVwoA5NuIUWtIGAhGd4hTfryXqx3q2PVWvOymbO4Y+szx9iipG7</latexit><latexit sha1_base64="I7R4a32uvuL7gDjagUmI6UlCRsU=">AAAB9XicbZDLTsJAFIZP8YZ4Q126mQgmuCEtG1iSuHGJiVwSqGQ6TGFkOm1mppim4T3cuNAYt76LO9/GAbpQ8E8m+fKfc3LO/F7EmdK2/W3ltrZ3dvfy+4WDw6Pjk+LpWUeFsSS0TUIeyp6HFeVM0LZmmtNeJCkOPE673vRmUe/OqFQsFPc6iagb4LFgPiNYG+uhPJhhGU1YJRk+XpeHxZJdtZdCm+BkUIJMrWHxazAKSRxQoQnHSvUdO9JuiqVmhNN5YRArGmEyxWPaNyhwQJWbLq+eoyvjjJAfSvOERkv390SKA6WSwDOdAdYTtV5bmP/V+rH2G27KRBRrKshqkR9zpEO0iACNmKRE88QAJpKZWxGZYImJNkEVTAjO+pc3oVOrOobvaqVmI4sjDxdwCRVwoA5NuIUWtIGAhGd4hTfryXqx3q2PVWvOymbO4Y+szx9iipG7</latexit><latexit sha1_base64="I7R4a32uvuL7gDjagUmI6UlCRsU=">AAAB9XicbZDLTsJAFIZP8YZ4Q126mQgmuCEtG1iSuHGJiVwSqGQ6TGFkOm1mppim4T3cuNAYt76LO9/GAbpQ8E8m+fKfc3LO/F7EmdK2/W3ltrZ3dvfy+4WDw6Pjk+LpWUeFsSS0TUIeyp6HFeVM0LZmmtNeJCkOPE673vRmUe/OqFQsFPc6iagb4LFgPiNYG+uhPJhhGU1YJRk+XpeHxZJdtZdCm+BkUIJMrWHxazAKSRxQoQnHSvUdO9JuiqVmhNN5YRArGmEyxWPaNyhwQJWbLq+eoyvjjJAfSvOERkv390SKA6WSwDOdAdYTtV5bmP/V+rH2G27KRBRrKshqkR9zpEO0iACNmKRE88QAJpKZWxGZYImJNkEVTAjO+pc3oVOrOobvaqVmI4sjDxdwCRVwoA5NuIUWtIGAhGd4hTfryXqx3q2PVWvOymbO4Y+szx9iipG7</latexit>nonnegative combintation (x)=264'(x1)...'(xn)375<latexit sha1_base64="guxvl0q2bU+xVoPoliEsRkzbL0M=">AAACNXicbZDLSgMxFIYzXmu9VV26CVbBbmSmG7sRCm5cuKhgL9CUksmctsFMZkgypWXoS7nxPVzpwoUibn0F04uirQcCH/9/Djnn92PBtXHdZ2dpeWV1bT2zkd3c2t7Zze3t13SUKAZVFolINXyqQXAJVcONgEasgIa+gLp/dzn2631Qmkfy1gxjaIW0K3mHM2qs1M5dH5O4x08HhQviQ5fL1A+pUXwwIn2qJk7bK2BCMOkHkdFT+nZkgYAMfkaO27m8e+ZOCi+CN4M8mlWlnXskQcSSEKRhgmrd9NzYtFKqDGcCRlmSaIgpu6NdaFqUNATdSidXj/CJVQLciZR90uCJ+nsipaHWw9C3nXbBnp73xuJ/XjMxnVIr5TJODEg2/aiTCGwiPI4QB1wBM2JogTLF7a6Y9aiizNigszYEb/7kRagVzzzLN8V8uTSLI4MO0RE6RR46R2V0hSqoihi6R0/oFb05D86L8+58TFuXnNnMAfpTzucX/a+riw==</latexit><latexit sha1_base64="guxvl0q2bU+xVoPoliEsRkzbL0M=">AAACNXicbZDLSgMxFIYzXmu9VV26CVbBbmSmG7sRCm5cuKhgL9CUksmctsFMZkgypWXoS7nxPVzpwoUibn0F04uirQcCH/9/Djnn92PBtXHdZ2dpeWV1bT2zkd3c2t7Zze3t13SUKAZVFolINXyqQXAJVcONgEasgIa+gLp/dzn2631Qmkfy1gxjaIW0K3mHM2qs1M5dH5O4x08HhQviQ5fL1A+pUXwwIn2qJk7bK2BCMOkHkdFT+nZkgYAMfkaO27m8e+ZOCi+CN4M8mlWlnXskQcSSEKRhgmrd9NzYtFKqDGcCRlmSaIgpu6NdaFqUNATdSidXj/CJVQLciZR90uCJ+nsipaHWw9C3nXbBnp73xuJ/XjMxnVIr5TJODEg2/aiTCGwiPI4QB1wBM2JogTLF7a6Y9aiizNigszYEb/7kRagVzzzLN8V8uTSLI4MO0RE6RR46R2V0hSqoihi6R0/oFb05D86L8+58TFuXnNnMAfpTzucX/a+riw==</latexit><latexit sha1_base64="guxvl0q2bU+xVoPoliEsRkzbL0M=">AAACNXicbZDLSgMxFIYzXmu9VV26CVbBbmSmG7sRCm5cuKhgL9CUksmctsFMZkgypWXoS7nxPVzpwoUibn0F04uirQcCH/9/Djnn92PBtXHdZ2dpeWV1bT2zkd3c2t7Zze3t13SUKAZVFolINXyqQXAJVcONgEasgIa+gLp/dzn2631Qmkfy1gxjaIW0K3mHM2qs1M5dH5O4x08HhQviQ5fL1A+pUXwwIn2qJk7bK2BCMOkHkdFT+nZkgYAMfkaO27m8e+ZOCi+CN4M8mlWlnXskQcSSEKRhgmrd9NzYtFKqDGcCRlmSaIgpu6NdaFqUNATdSidXj/CJVQLciZR90uCJ+nsipaHWw9C3nXbBnp73xuJ/XjMxnVIr5TJODEg2/aiTCGwiPI4QB1wBM2JogTLF7a6Y9aiizNigszYEb/7kRagVzzzLN8V8uTSLI4MO0RE6RR46R2V0hSqoihi6R0/oFb05D86L8+58TFuXnNnMAfpTzucX/a+riw==</latexit><latexit sha1_base64="guxvl0q2bU+xVoPoliEsRkzbL0M=">AAACNXicbZDLSgMxFIYzXmu9VV26CVbBbmSmG7sRCm5cuKhgL9CUksmctsFMZkgypWXoS7nxPVzpwoUibn0F04uirQcCH/9/Djnn92PBtXHdZ2dpeWV1bT2zkd3c2t7Zze3t13SUKAZVFolINXyqQXAJVcONgEasgIa+gLp/dzn2631Qmkfy1gxjaIW0K3mHM2qs1M5dH5O4x08HhQviQ5fL1A+pUXwwIn2qJk7bK2BCMOkHkdFT+nZkgYAMfkaO27m8e+ZOCi+CN4M8mlWlnXskQcSSEKRhgmrd9NzYtFKqDGcCRlmSaIgpu6NdaFqUNATdSidXj/CJVQLciZR90uCJ+nsipaHWw9C3nXbBnp73xuJ/XjMxnVIr5TJODEg2/aiTCGwiPI4QB1wBM2JogTLF7a6Y9aiizNigszYEb/7kRagVzzzLN8V8uTSLI4MO0RE6RR46R2V0hSqoihi6R0/oFb05D86L8+58TFuXnNnMAfpTzucX/a+riw==</latexit>x y (x)  (y) > 2↵ T(↵+ )T(↵+ )T 2T x y (x)  (y)  0<latexit sha1_base64="xE4TKIfpLj8g4gjosp4+9y8sOeo=">AAACtHiclVHBThsxEPVuaUtDW9Jy7MUiFCWqgnZzaDkicekRpIQgxSGddSaJhdfr2t4qq1W+sLfe+Jt6NxGCwIWRLD+9N288nkm0FNZF0V0Qvtp5/ebt7rvG3vsPH/ebnz5f2Sw3HAc8k5m5TsCiFAoHTjiJ19ogpInEYXJ7XunDP2isyFTfFRrHKcyVmAkOzlOT5t8jluBcqDJJwRmxXC27BWWMMr0Q7WWnW99Fh6Ga3qfcMJdpuuWj3R4DqRdQCQ5onx7T9pr5VjOdflV3mzr2vv6j6i/vh7I5/qbR0aTZik6iOuhTEG9Ai2ziYtL8x6YZz1NUjkuwdhRH2o1LME5wiasGyy1q4Lcwx5GHClK047Ie+op+9cyUzjLjj3K0Zh86SkitLdLEZ/o+F3Zbq8jntFHuZqfjUiidO1R8/dAsl9RltNognQqD3MnCA+BG+F4pX4AB7vyeG34I8faXn4Kr3kns8WWvdXa6Gccu+UIOSZvE5Ac5Iz/JBRkQHsTBMPgVQPg9ZCEPcZ0aBhvPAXkUofoPmdvT4g==</latexit><latexit sha1_base64="xE4TKIfpLj8g4gjosp4+9y8sOeo=">AAACtHiclVHBThsxEPVuaUtDW9Jy7MUiFCWqgnZzaDkicekRpIQgxSGddSaJhdfr2t4qq1W+sLfe+Jt6NxGCwIWRLD+9N288nkm0FNZF0V0Qvtp5/ebt7rvG3vsPH/ebnz5f2Sw3HAc8k5m5TsCiFAoHTjiJ19ogpInEYXJ7XunDP2isyFTfFRrHKcyVmAkOzlOT5t8jluBcqDJJwRmxXC27BWWMMr0Q7WWnW99Fh6Ga3qfcMJdpuuWj3R4DqRdQCQ5onx7T9pr5VjOdflV3mzr2vv6j6i/vh7I5/qbR0aTZik6iOuhTEG9Ai2ziYtL8x6YZz1NUjkuwdhRH2o1LME5wiasGyy1q4Lcwx5GHClK047Ie+op+9cyUzjLjj3K0Zh86SkitLdLEZ/o+F3Zbq8jntFHuZqfjUiidO1R8/dAsl9RltNognQqD3MnCA+BG+F4pX4AB7vyeG34I8faXn4Kr3kns8WWvdXa6Gccu+UIOSZvE5Ac5Iz/JBRkQHsTBMPgVQPg9ZCEPcZ0aBhvPAXkUofoPmdvT4g==</latexit><latexit sha1_base64="xE4TKIfpLj8g4gjosp4+9y8sOeo=">AAACtHiclVHBThsxEPVuaUtDW9Jy7MUiFCWqgnZzaDkicekRpIQgxSGddSaJhdfr2t4qq1W+sLfe+Jt6NxGCwIWRLD+9N288nkm0FNZF0V0Qvtp5/ebt7rvG3vsPH/ebnz5f2Sw3HAc8k5m5TsCiFAoHTjiJ19ogpInEYXJ7XunDP2isyFTfFRrHKcyVmAkOzlOT5t8jluBcqDJJwRmxXC27BWWMMr0Q7WWnW99Fh6Ga3qfcMJdpuuWj3R4DqRdQCQ5onx7T9pr5VjOdflV3mzr2vv6j6i/vh7I5/qbR0aTZik6iOuhTEG9Ai2ziYtL8x6YZz1NUjkuwdhRH2o1LME5wiasGyy1q4Lcwx5GHClK047Ie+op+9cyUzjLjj3K0Zh86SkitLdLEZ/o+F3Zbq8jntFHuZqfjUiidO1R8/dAsl9RltNognQqD3MnCA+BG+F4pX4AB7vyeG34I8faXn4Kr3kns8WWvdXa6Gccu+UIOSZvE5Ac5Iz/JBRkQHsTBMPgVQPg9ZCEPcZ0aBhvPAXkUofoPmdvT4g==</latexit><latexit sha1_base64="xE4TKIfpLj8g4gjosp4+9y8sOeo=">AAACtHiclVHBThsxEPVuaUtDW9Jy7MUiFCWqgnZzaDkicekRpIQgxSGddSaJhdfr2t4qq1W+sLfe+Jt6NxGCwIWRLD+9N288nkm0FNZF0V0Qvtp5/ebt7rvG3vsPH/ebnz5f2Sw3HAc8k5m5TsCiFAoHTjiJ19ogpInEYXJ7XunDP2isyFTfFRrHKcyVmAkOzlOT5t8jluBcqDJJwRmxXC27BWWMMr0Q7WWnW99Fh6Ga3qfcMJdpuuWj3R4DqRdQCQ5onx7T9pr5VjOdflV3mzr2vv6j6i/vh7I5/qbR0aTZik6iOuhTEG9Ai2ziYtL8x6YZz1NUjkuwdhRH2o1LME5wiasGyy1q4Lcwx5GHClK047Ie+op+9cyUzjLjj3K0Zh86SkitLdLEZ/o+F3Zbq8jntFHuZqfjUiidO1R8/dAsl9RltNognQqD3MnCA+BG+F4pX4AB7vyeG34I8faXn4Kr3kns8WWvdXa6Gccu+UIOSZvE5Ac5Iz/JBRkQHsTBMPgVQPg9ZCEPcZ0aBhvPAXkUofoPmdvT4g==</latexit>where x = [x0(cid:62) x1(cid:62)
b, A, B and C are given by [12]

· · ·

x(cid:96)(cid:62)](cid:62) is the concatenation of the input and the activation values, and the matrices

W 0
0
0 W 1
...
...
0
0

. . .
0
. . .
0
...
. . .
. . . W (cid:96)

1

−

. . .

0 W (cid:96)

, b =

0
0

...
0





b0(cid:62)
(cid:104)

0
0
, B = 
...

0



b(cid:96)

−

· · ·

(cid:105)

(cid:3)

A = 




0

C =

(cid:2)

In1
0
...
0

0
In2
...
0

. . .
. . .
. . .
. . .

1(cid:62)

(cid:62) .

0
0
...
In(cid:96)



,






(13)

The particular representation in (12) facilitates the extension of LipSDP to multiple layers, as stated in the
following theorem.

Theorem 2 (Lipschitz certiﬁcates for multi-layer neural networks) Consider an (cid:96)-layer fully con-
(cid:96)
k=1 nk be the total number of hidden neurons and suppose
nected neural network described by (2). Let n =
the activation functions are slope-restricted in the sector [α, β]. Deﬁne
Tn as in (8). Deﬁne A and B as in
(13). Consider the matrix inequality

(cid:80)

M (ρ, T ) =

A
B

(cid:20)

(cid:62)

(cid:21)

2αβT
−
(α + β)T

(cid:20)

(α + β)T
2T

−

A
B

(cid:21) (cid:20)

(cid:21)

If (14) is satisﬁed for some (ρ, T )

R+ × Tn, then

∈

f (x)

||

+ 




f (y)

−

−

ρIn0
0
...
0

0
0
...
0

. . .
. . .
. . .
. . .

0
0
...
(W (cid:96))(cid:62)W (cid:96)








||2 ≤

√ρ

x

||

−

y

||2,

∀

x, y

∈

Rn0 .

0.

(cid:22)

(14)

In a similar way to the single-layer case, we can ﬁnd the best bound on the Lipschitz constant by solving
the SDP in (11) with M (ρ, T ) deﬁned as in (14).

Remark 1 We have only considered the (cid:96)2 norm in our exposition. By using the inequality
n

1
p −

x

1
q

(cid:107)q, the (cid:96)2-Lipschitz bound implies

(cid:107)

x

(cid:107)

(cid:107)p ≤

n−

( 1

p −

1
2 )

f (y)
(cid:107)

−

f (x)

f (y)

q L2 is a Lipschitz constant of f when (cid:96)q
or, equivalently,
and (cid:96)p norms are used in the input and output spaces, respectively. We can also extend our framework to
accommodate quadratic norms

(cid:107)p ≤

−
(cid:107)P = √x(cid:62)P x, where P

f (x)

x
(cid:107)

−

∈

n

x

(cid:107)

f (y)

−

(cid:107)p ≤ (cid:107)
y
q L2(cid:107)

1
p −

1

−
1

f (x)

y
L2(cid:107)
(cid:107)2 ≤
(cid:107)q. Hence, n
Sn
++.

1
p −

x

(cid:107)2 ≤

1
2 −

n

1

y
q L2(cid:107)

x

(cid:107)q,

−

2.5 Variants of LipSDP: reconciling accuracy and eﬃciency

≤

≤

i, j

In LipSDP, there are O(n2) decision variables λij, 1
n (λij = λji), where n is the total number
of hidden neurons. For i
= j, the variable λij couples the i-th and j-th hidden neuron. For i = j,
the variable λii constrains the input-output of the i-th activation function individually. Using all these
decision variables would provide the tightest convex relaxation in our formulation. Interestingly, numerical
experiments suggest that this tightest convex relaxation yields bounds on the Lipschitz constant that almost
coincide with the naive lower bound on the Lipschitz constant. However, solving this SDP with all the
decision variables included is impractical for large networks. Nevertheless, we can consider a hierarchy of
relaxations of LipSDP by removing a subset of the decision variables. Below, we give a brief description of
the eﬃciency and accuracy of each variant. Throughout, we let n be the total number of neurons and (cid:96) the
number of hidden layers.

1. LipSDP-Network imposes constraints on all possible pairs of activation functions and has O(n2)

decision variables. It is the least scalable but the most accurate method.

2. LipSDP-Neuron ignores the cross coupling constraints among diﬀerent neurons and has O(n) decision
variables. It is more scalable and less accurate than LipSDP-Network. For this case, we have T =
diag(λ11,

, λnn).

· · ·

6

(cid:54)
n

500
1000
1500
2000
2500
3000

LipSDP-
Neuron
5.22
27.91
82.12
200.88
376.07
734.63

LipSDP-
Layer
2.85
17.88
58.61
146.09
245.94
473.25

(cid:96)

5
10
50
100
200
500

LipSDP-
Neuron
20.33
32.18
87.45
135.85
221.2
707.56

LipSDP-
Layer
3.41
7.06
25.88
40.39
64.90
216.49

Table 1: Computation time in seconds for evaluat-
ing Lipschitz bounds of one-hidden-layer neural net-
works with a varying number of hidden units. A
plot showing the Lipschitz constant for each net-
work tested in this table has been provided in the
Appendix.

Table 2: Computation time in seconds for comput-
ing Lipschitz bounds of (cid:96)-hidden-layer neural net-
works with 100 activation functions per layer. For
LipSDP-Neuron and LipSDP-Layer, we split each
network up into 5-layer sub-networks.

3. LipSDP-Layer considers only one constraint per layer, resulting in O((cid:96)) decision variables. It is the

most scalable and least accurate method. For this variant, we have T = blkdiag(λ1In1,

, λ(cid:96)In(cid:96)).

· · ·

Parallel implementation by splitting. The Lipschitz constant of the composition of two or more functions
can be bounded by the product of the Lispchtiz constants of the individual functions. By splitting a neural
network up into small sub-networks, one can ﬁrst bound the Lipschitz constant of each sub-network and
then multiply these constants together to obtain a Lipschitz constant for the entire network. Because sub-
networks do not share weights, it is possible to compute the Lipschitz constants for each sub-network in
parallel. This greatly improves the scalability of of each variant of LipSDP with respect to the total number
of activation functions in the network.

3 Experiments

In this section we describe several experiments that highlight the key aspects of this work. In particular, we
show empirically that our bounds are much tighter than any comparable method, we study the impact of
robust training on our Lipschitz bounds, and we analyze the scalability of our methods.

Experimental setup. For our experiments we used MATLAB, the CVX toolbox [16] and MOSEK [2] on a
9-core CPU with 16GB of RAM to solve the SDPs. All classiﬁers trained on MNIST used an 80-20 train-test
split.

Training procedures. Several training procedures have recently been proposed to improve the robustness of
neural network classiﬁers. Two prominent procedures are the LP-based method in [20] and projected gradient
descent (PGD) based method in [23]. We refer to these training methods as LP-Train and PGD-Train,
respectively. Both procedures take as input a parameter (cid:15) that deﬁnes the (cid:96)
perturbation of the training
data points.

∞

Baselines. Throughout the experiments, we will often show comparisons to what we call the naive lower
and upper bounds. As has been shown in several previous works [9, 36], trivial lower and upper bounds
on the Lipschitz constant of a feed-forward neural network with (cid:96) hidden-layers are given by L2, lower =
2, which we refer to them as naive lower and upper bounds,
respectively. We are aware of only two methods that bound the Lipschitz constant and can scale to fully-
(cid:12)
(cid:12)
connected networks with more than two hidden layers; these methods are [9], which we will refer to as CPLip,
(cid:12)
(cid:12)
and [36], which is called SeqLip.

2 and L2, upper =

W (cid:96)W (cid:96)

(cid:96)
i=0

W 0

W i

· · ·

(cid:81)

(cid:12)
(cid:12)

(cid:12)
(cid:12)

(cid:12)
(cid:12)

(cid:12)
(cid:12)

(cid:12)
(cid:12)

(cid:12)
(cid:12)

−

1

We compare the Lipschitz bounds obtained by LipSDP-Neuron, LipSDP-Layer, CPLip, and SeqLip in
Figure 2a. It is evident from this ﬁgure that the bounds from LipSDP-Neuron are tighter than CPLip and
SeqLip. Our results show that the true Lipschitz constants of the networks shown above are very close to
the naive lower bound.

7

(a) Comparison of Lipschitz bounds
found by various methods for ﬁve-
hidden-layer networks trained on
MNIST with the Adam optimizer.
Each network had a test accuracy
above 97%.

(b) Lipschitz bounds obtained by
splitting a 100-layer network into
sub-networks.
Each sub-network
had six layers, and the weights
were generated randomly by sam-
pling from a normal distribution.

LipSDP-Network

(c)
Lipschitz
bounds and computation time for
a one-hidden-layer network with
100 neurons. The weights for this
network were obtained by sampling
from a normal distribution.

Figure 2: Comparison of the accuracy LipSDP methods to other methods that compute the Lipschitz constant
and scalability analysis of all three SeqLip methods.

(a) Lipschitz bounds for a one-hidden-layer neural net-
works trained on the MNIST dataset with the Adam
optimizer and LP-Train and PGD-Train for two values
of the robustness parameter (cid:15). Each network reached an
accuracy of 95% or higher.

(b) Histograms showing the local robustness (in (cid:96)∞
norm) around each correctly-classiﬁed test instance from
the MNIST dataset. The neural networks had three hid-
den layers with 100, 50, 20 neurons, respectively. All
classiﬁers had a test accuracy of 97%.

Figure 3: Analysis of impact of robust training on the Lipschitz constant and the distance to misclassiﬁcation
for networks trained on MNIST

To demonstrate the scalability of the LipSDP formulations, we split a 100-hidden layer neural network
into sub-networks with six hidden layers each and computed the Lipschitz bounds using LipSDP-Neuron
and LipSDP-Layer. The results are shown in Figure 2b. Furthermore, in Tables 1 and 2, we show the
computation time for scaling the LipSDP methods in the number of hidden units per layer and in the number
of layers. In particular, the largest network we tested in Table 2 had 50,000 hidden neurons; SDPLip-Neuron
took approximately 12 minutes to ﬁnd a Lipschitz bound, and SDPLip-Layer took approximately 4 minutes.
To evaluate SDPLip-Network, we coupled random pairs of hidden neurons in a one-hidden-layer network
and plotted the computation time and Lipschitz bound found by SDPLip-Network as we increased the number
of paired neurons. Our results show that as the number of coupled neurons increases, the computation time
increases quadratically. This shows that while this method is the most accurate of the three proposed LipSDP
methods, it is intractable for even modestly large networks.

8

Figure 4: Trade-oﬀ between accuracy and Lipschitz
constant for diﬀerent values of the robustness pa-
rameter used for LP-Train and PGD-Train. All net-
works had one hidden layer with 50 hidden neurons.

Figure 5: Lipschitz constants for topologically iden-
tical three-hidden-layer networks with ReLU and
leaky ReLU activation functions. All classiﬁers were
trained until they reached 97% test accuracy.

Impact of robust training.
In Figure 3, we empirically demonstrate that the Lipschitz bound of a
neural network is directly related to the robustness of the corresponding classiﬁer. This ﬁgure shows that
LP-train and PGD-Train networks achieve lower Lipschitz bounds than standard training procedures. Figure
3a indicates that robust training procedures yield lower Lipschitz constants than networks trained with
standard training procedures such as the Adam optimizer. Figure 3b shows the utility of sharply estimating
the Lipschitz constant; a lower value of L2 guarantees that a neural network is more locally robust to input
perturbations; see Proposition 1 in the Appendix.

In the same vein, Figure 4 shows the impact of varying the robustness parameter (cid:15) used in LP-Train
and PGD-Train on the test accuracy of networks trained for a ﬁxed number of epochs and the corresponding
In essence, these results quantify how much robustness a ﬁxed classiﬁer can handle
Lipschitz constants.
before accuracy plummets. Interestingly, the drops in accuracy as (cid:15) increases coincide with corresponding
drops in the Lipschitz constant for both LP-Train and PGD-Train.

Robustness for diﬀerent activation functions. The framework proposed in this work allows us to
examine the impact of using diﬀerent activation functions on the Lipschitz constant of neural networks. We
trained two sets of neural networks on the MNIST dataset. The ﬁrst set used ReLU activation functions,
while the second set used leaky ReLU activations. Figure 5 shows empirically that the networks with the
leaky ReLU activation function have larger Lipschitz constants than networks of the same architecture with
the ReLU activation function.

4 Conclusions and future work

In this paper, we proposed a hierarchy of semideﬁnite programs to derive tight upper bounds on the Lipschitz
constant of feed-forward fully-connected neural networks. Some comments are in order. First, our framework
can be directly used to certify convolutional neural networks (CNNs) by unrolling them to a large feed-
forward neural network. A future direction is to exploit the special structure of CNNs in the resulting
SDP. Second, we only considered one application of Lipschitz bounds in depth (robustness certiﬁcation).
Having an accurate upper bound on the Lipschitz constant can be useful in domains beyond robustness
analysis, such as stability analysis of feedback systems with control policies updated by deep reinforcement
learning. Furthermore, Lipschitz bounds can be utilized during training as a heuristic to promote out-of-
sample generalization [35]. We intend to pursue these applications for future work.

9

References

[1] Beh¸cet A¸cıkme¸se and Martin Corless. Observers for systems with nonlinearities satisfying incremental

quadratic constraints. Automatica, 47(7):1339–1348, 2011.

[2] MOSEK ApS. The MOSEK optimization toolbox for MATLAB manual. Version 8.1., 2017.

[3] Anil Aswani, Humberto Gonzalez, S Shankar Sastry, and Claire Tomlin. Provably safe and robust

learning-based model predictive control. Automatica, 49(5):1216–1226, 2013.

[4] Radu Balan, Maneesh Singh, and Dongmian Zou. Lipschitz properties for deep convolutional networks.

arXiv preprint arXiv:1701.05217, 2017.

[5] Peter L Bartlett, Dylan J Foster, and Matus J Telgarsky. Spectrally-normalized margin bounds for
neural networks. In Advances in Neural Information Processing Systems, pages 6240–6249, 2017.

[6] Osbert Bastani, Yani Ioannou, Leonidas Lampropoulos, Dimitrios Vytiniotis, Aditya Nori, and Anto-
nio Criminisi. Measuring neural net robustness with constraints. In Advances in neural information
processing systems, pages 2613–2621, 2016.

[7] Felix Berkenkamp, Matteo Turchetta, Angela Schoellig, and Andreas Krause. Safe model-based rein-
In Advances in neural information processing systems,

forcement learning with stability guarantees.
pages 908–918, 2017.

[8] Stephen Boyd and Lieven Vandenberghe. Convex optimization. Cambridge university press, 2004.

[9] Patrick L. Combettes and Jean-Christophe Pesquet. Lipschitz certiﬁcates for neural network structures

driven by averaged activation operators. arXiv preprint arXiv:1903.01014, 2019.

[10] Dheeru Dua and Casey Graﬀ. UCI machine learning repository, 2017.

[11] Souradeep Dutta, Susmit Jha, Sriram Sankaranarayanan, and Ashish Tiwari. Output range analysis
for deep feedforward neural networks. In NASA Formal Methods Symposium, pages 121–138. Springer,
2018.

[12] Mahyar Fazlyab, Manfred Morari, and George J Pappas. Safety veriﬁcation and robustness anal-
arXiv preprint

ysis of neural networks via quadratic constraints and semideﬁnite programming.
arXiv:1903.01287, 2019.

[13] Mahyar Fazlyab, Alejandro Ribeiro, Manfred Morari, and Victor M Preciado. Analysis of optimiza-
tion algorithms via integral quadratic constraints: Nonstrongly convex problems. SIAM Journal on
Optimization, 28(3):2654–2689, 2018.

[14] Timon Gehr, Matthew Mirman, Dana Drachsler-Cohen, Petar Tsankov, Swarat Chaudhuri, and Martin
Vechev. Ai2: Safety and robustness certiﬁcation of neural networks with abstract interpretation. In
2018 IEEE Symposium on Security and Privacy (SP), pages 3–18. IEEE, 2018.

[15] Ian J Goodfellow, Jonathon Shlens, and Christian Szegedy. Explaining and harnessing adversarial

examples (2014). arXiv preprint arXiv:1412.6572.

[16] Michael Grant, Stephen Boyd, and Yinyu Ye. Cvx: Matlab software for disciplined convex programming,

2008.

[17] Todd Huster, Cho-Yu Jason Chiang, and Ritu Chadha. Limitations of the lipschitz constant as a defense
In Joint European Conference on Machine Learning and Knowledge

against adversarial examples.
Discovery in Databases, pages 16–29. Springer, 2018.

[18] Ming Jin and Javad Lavaei. Stability-certiﬁed reinforcement learning: A control-theoretic perspective.

arXiv preprint arXiv:1810.11505, 2018.

10

[19] Matt Jordan, Justin Lewis, and Alexandros G Dimakis. Provable certiﬁcates for adversarial examples:

Fitting a ball in the union of polytopes. arXiv preprint arXiv:1903.08778, 2019.

[20] J Zico Kolter and Eric Wong. Provable defenses against adversarial examples via the convex outer

adversarial polytope. arXiv preprint arXiv:1711.00851, 1(2):3, 2017.

[21] Alexey Kurakin, Ian Goodfellow, and Samy Bengio. Adversarial machine learning at scale. arXiv

preprint arXiv:1611.01236, 2016.

[22] Yann LeCun. The mnist database of handwritten digits. http://yann. lecun. com/exdb/mnist/, 1998.

[23] Aleksander Madry, Aleksandar Makelov, Ludwig Schmidt, Dimitris Tsipras, and Adrian Vladu. Towards

deep learning models resistant to adversarial attacks. arXiv preprint arXiv:1706.06083, 2017.

[24] Yurii Nesterov.

Introductory lectures on convex optimization: A basic course, volume 87. Springer

Science & Business Media, 2013.

[25] Nicolas Papernot, Patrick McDaniel, Xi Wu, Somesh Jha, and Ananthram Swami. Distillation as
In 2016 IEEE Symposium on

a defense to adversarial perturbations against deep neural networks.
Security and Privacy (SP), pages 582–597. IEEE, 2016.

[26] Jonathan Peck, Joris Roels, Bart Goossens, and Yvan Saeys. Lower bounds on the robustness to
adversarial perturbations. In Advances in Neural Information Processing Systems, pages 804–813, 2017.

[27] Haifeng Qian and Mark N Wegman. L2-nonexpansive neural networks. arXiv preprint arXiv:1802.07896,

2018.

[28] Aditi Raghunathan, Jacob Steinhardt, and Percy Liang. Certiﬁed defenses against adversarial examples.

arXiv preprint arXiv:1801.09344, 2018.

[29] Aditi Raghunathan, Jacob Steinhardt, and Percy S Liang. Semideﬁnite relaxations for certifying robust-
ness to adversarial examples. In Advances in Neural Information Processing Systems, pages 10900–10910,
2018.

[30] Wenjie Ruan, Xiaowei Huang, and Marta Kwiatkowska. Reachability analysis of deep neural networks

with provable guarantees. arXiv preprint arXiv:1805.02242, 2018.

[31] Ernest K Ryu and Stephen Boyd. Primer on monotone operator methods. Appl. Comput. Math,

15(1):3–43, 2016.

[32] Gagandeep Singh, Timon Gehr, Matthew Mirman, Markus P¨uschel, and Martin Vechev. Fast and
eﬀective robustness certiﬁcation. In Advances in Neural Information Processing Systems, pages 10802–
10813, 2018.

[33] Christian Szegedy, Wojciech Zaremba, Ilya Sutskever, Joan Bruna, Dumitru Erhan, Ian Goodfellow,
and Rob Fergus. Intriguing properties of neural networks. arXiv preprint arXiv:1312.6199, 2013.

[34] Vincent Tjeng, Kai Xiao, and Russ Tedrake. Evaluating robustness of neural networks with mixed

integer programming. arXiv preprint arXiv:1711.07356, 2017.

[35] Yusuke Tsuzuku, Issei Sato, and Masashi Sugiyama. Lipschitz-margin training: Scalable certiﬁcation
In Advances in Neural Information Processing

of perturbation invariance for deep neural networks.
Systems, pages 6541–6550, 2018.

[36] Aladin Virmaux and Kevin Scaman. Lipschitz regularity of deep neural networks: analysis and eﬃcient

estimation. In Advances in Neural Information Processing Systems, pages 3835–3844, 2018.

[37] Tsui-Wei Weng, Huan Zhang, Hongge Chen, Zhao Song, Cho-Jui Hsieh, Duane Boning, Inderjit S
Dhillon, and Luca Daniel. Towards fast computation of certiﬁed robustness for relu networks. arXiv
preprint arXiv:1804.09699, 2018.

11

[38] Tsui-Wei Weng, Huan Zhang, Pin-Yu Chen, Jinfeng Yi, Dong Su, Yupeng Gao, Cho-Jui Hsieh, and
Luca Daniel. Evaluating the robustness of neural networks: An extreme value theory approach. arXiv
preprint arXiv:1801.10578, 2018.

[39] Eric Wong, Frank Schmidt, Jan Hendrik Metzen, and J Zico Kolter. Scaling provable adversarial

defenses. In Advances in Neural Information Processing Systems, pages 8400–8409, 2018.

[40] Huan Zhang, Tsui-Wei Weng, Pin-Yu Chen, Cho-Jui Hsieh, and Luca Daniel. Eﬃcient neural network
robustness certiﬁcation with general activation functions. In Advances in Neural Information Processing
Systems, pages 4939–4948, 2018.

[41] Stephan Zheng, Yang Song, Thomas Leung, and Ian Goodfellow. Improving the robustness of deep
neural networks via stability training. In Proceedings of the ieee conference on computer vision and
pattern recognition, pages 4480–4488, 2016.

[42] Dongmian Zou, Radu Balan, and Maneesh Singh. On lipschitz bounds of general convolutional neural

networks. arXiv preprint arXiv:1808.01415, 2018.

12

A Appendix

A.1 Robustness certiﬁcation of DNN-based classiﬁers
Consider a classiﬁer described by a feed-forward neural network f : Rn
Rk, where n is the number of
input features and k is the number of classes. In this context, the function f takes as input an instance or
measurement x and returns a k-dimensional vector of scores – one for each class. The classiﬁcation rule is
based on assigning x to the class with the highest score. That is, we deﬁne the classiﬁcation C(x) : Rn
→
k fi(x). Now suppose that x(cid:63) is an instance that is classiﬁed correctly
1, . . . , k
{
≤
by the neural network. To evaluate the local robustness of the neural network around x(cid:63), we consider a
Rn that represents the set of all possible (cid:96)2-norm perturbations
bounded set
x
of x(cid:63). Then the classiﬁer is locally robust at x(cid:63) against
(x(cid:63)) if it assigns all the perturbed inputs to the
same class as the unperturbed input, i.e., if

to be C(x) = argmax1

(x(cid:63)) =

(cid:107)2 ≤

x
{

} ⊆

x(cid:63)

| (cid:107)

→

A

A

−

}

≤

(cid:15)

i

C(x) = C(x(cid:63))

x

∀

(x(cid:63)).

∈ A

(15)

In the following proposition, we derive a suﬃcient condition to guarantee local robustness around x(cid:63) for the
perturbation set

(x(cid:63)).

A

Proposition 1 Consider a neural-network classiﬁer f : Rn
Let (cid:15) > 0 be given and consider the inequality

→

Rk with Lipschitz constant L2 in the (cid:96)2-norm.

L2 ≤

1
(cid:15)√2

min
k, j
≤

j

1

≤

fj(x(cid:63))

=i(cid:63) |

fi(cid:63) (x(cid:63))

.
|

−

(16)

Then (16) implies C(x) := argmax1

kfi(x) = C(x(cid:63)) for all x

i
≤

≤

(x(cid:63)).

∈ A

The inequality in (16) provides us with a simple and computationally eﬃcient test for assessing the
point-wise robustness of a neural network. According to (16), a more accurate estimation of the Lipschitz
constant directly increases the maximum perturbation (cid:15) that can be certiﬁed for each test example. This
makes the framework suitable for model selection, wherein one wishes to select the model that is most robust
to adversarial perturbations from a family of proposed classiﬁers [26].

A.2 Proof of Proposition 1

Let i(cid:63) = argmax1
≤

i
≤

k fi(x(cid:63)) be the class of x(cid:63). Deﬁne the polytope in the output space of f :

Pi(cid:63) =

y
{

(ej −

|

ei(cid:63) )(cid:62)y

0

1

j

≤

≤

≤

k, j

= i(cid:63)

.

}

which is the set of all outputs whose score is the highest for class i(cid:63); and, of course, f (x(cid:63))
distance of f (x(cid:63)) to the boundary ∂
polytope:

∈ Pi(cid:63) . The
Pi of the polytope is the minimum distance of f (x(cid:63)) to all edges of the

dist(f (x(cid:63)), ∂

Pi(cid:63) ) = inf

∂

y

y
Pi(cid:63) (cid:107)

∈

f (x(cid:63))

(cid:107)2 =

−

1
√2

min
k, j
≤

1

j
≤

fj(x(cid:63))

=i(cid:63) |

fi(cid:63) (x(cid:63))
.
|

−

Note that the Lipschitz condition implies dist(f (x(cid:63)), f (x)) =
The condition in (16) then implies

f (x)
(cid:107)

−

f (x(cid:63))

(cid:107)2 ≤

L2(cid:15) for all

x(cid:63)

x
(cid:107)

−

(cid:107)2 ≤

(cid:15).

dist(f (x(cid:63)), f (x))

dist(f (x(cid:63)), ∂

Pi(cid:63) ) for all

x

(cid:107)

−

x(cid:63)

(cid:15),

(cid:107)2 ≤

≤

k, j

= i(cid:63). Therefore, the output of the classiﬁcation would not change for any

and for all 1
x(cid:63)
(cid:15).
x
(cid:107)2 ≤
(cid:107)

−

≤

j

≤

A.3 Proof of Lemma 1

We ﬁrst prove the following lemma, which is a slight variation to the lemma proved in [12].

13

(cid:54)
(cid:54)
(cid:54)
(cid:54)
Figure 6: Illustration of local robustness certiﬁcation using the Lipschitz bound.

Lemma 2 Suppose ϕ : R

→

R is slope-restricted on [α, β] and satisﬁes ϕ(0) = 0. Deﬁne the set

Then the vector-valued function φ : Rn

n

i=1
(cid:88)

Tn =

T
{

∈

Sn

|

T =

λiieie(cid:62)i +

λij(ei −

ej)(ei −

ej)(cid:62), λij ≥

0

.
}

i<j
(cid:88)1
≤
≤
Rn deﬁned by φ(x) = [ϕ(x1)

n

ϕ(xn)](cid:62) satisﬁes

· · ·

→
2αβT
−
(α + β)T

(cid:62)

x
φ(x)

(cid:20)

(cid:21)

(cid:20)

(α + β)T
2T

−

x
φ(x)

(cid:21) (cid:20)

(cid:21)

0

≥

x

∀

∈

Rn,

(17)

(18)

for all T

∈ Tn.

Proof of Lemma 2. Note that the slope restriction condition implies that for any two pairs (xi, ϕ(xi)) and
(xj, ϕ(xj)), we can write the following incremental quadratic constraint:

(cid:62)

xi −
−

xj
ϕ(xj)
(cid:21)

2αβ α + β

−
α + β

λij

ϕ(xi)
(cid:20)
0 is arbitrary. Similarly, for any two pairs (xi, ϕ(xi)) and (0, ϕ(0)) = (0, 0), we can also write

ϕ(xi)

2
−

(cid:19) (cid:20)

(cid:18)(cid:20)

≥

≤

≤

(cid:21)

(cid:21)

·

0

1

i

= j

n.

(19)

xi −
−

xj
ϕ(xj)

where λij ≥
the incremental quadratic constraint

0

xi −
ϕ(xi)
(cid:20)
−

(cid:62)

0
(cid:21)

(cid:18)(cid:20)

−
α + β

2αβ α + β

λii

0

xi −
ϕ(xi)
−

(cid:19) (cid:20)

0

i = 1,

, n.

· · ·

≥

0
(cid:21)

(20)

2

−

·

(cid:21)

where λii ≥
compact representation (18).

0 is arbitrary. By adding (19) and (20) and vectorizing the notation, we would arrive at the

Proof of Lemma 1. For a ﬁxed z

R, deﬁne the map ˜ϕ by shifting ϕ, as follows,

∈
˜ϕ(z, δ) = ϕ(z + δ)

ϕ(z)

δ

−

R.

∈

It is not hard to verify that if ϕ is slope-restricted on [α, β], the map δ
the same interval for any ﬁxed z

R. Next, for a ﬁxed x

Rn deﬁne

˜ϕ(z, δ) is also slope-restricted on

(cid:55)→

∈

˜φ(x, δ) = φ(x + δ)

−

∈
φ(x) = [ ˜ϕ(x1, δ1)

˜ϕ(xn, δn)](cid:62) δ

· · ·

Since ˜ϕ is slope-restricted on [α, β] and satisﬁes ˜ϕ(z, 0) = 0 for any ﬁxed z
that ˜φ(x, δ) satisﬁes the quadratic constraint

∈

−
By substituting the deﬁnition of ˜φ(x, δ) and setting δ = y

(cid:21)

(cid:20)

δ
˜φ(x, δ)
(cid:20)

(cid:62)

2αβT
−
(α + β)T

(α + β)T
2T

δ
˜φ(x, δ)
(cid:21)
x, we obtain

(cid:21) (cid:20)

0.

≥

−

Rn.

∈
R, it follows from Lemma 2

y
φ(y)

(cid:20)

x
φ(x)

(cid:62)

(cid:21)

−
−

2αβT
−
(α + β)T

(cid:20)

(α + β)T
2T

−

y
φ(y)

(cid:21) (cid:20)

x
φ(x)

(cid:21)

−
−

0.

≥

(21)

14

LinearLinearLinearf(x)<latexit sha1_base64="k7ufXGKAt77G/FaPA5tscWX9PSc=">AAAB7XicbVA9TwJBEJ3DL8Qv1NJmI5hgQ+6w0MKCxMYSE/lI4EL2lj1Y2du97O4ZyYX/YGOhMbb+Hzv/jQtcoeBLJnl5byYz84KYM21c99vJra1vbG7ltws7u3v7B8XDo5aWiSK0SSSXqhNgTTkTtGmY4bQTK4qjgNN2ML6Z+e1HqjST4t5MYupHeChYyAg2VmqVw8rTeblfLLlVdw60SryMlCBDo1/86g0kSSIqDOFY667nxsZPsTKMcDot9BJNY0zGeEi7lgocUe2n82un6MwqAxRKZUsYNFd/T6Q40noSBbYzwmakl72Z+J/XTUx45adMxImhgiwWhQlHRqLZ62jAFCWGTyzBRDF7KyIjrDAxNqCCDcFbfnmVtGpV76Lq3dVK9essjjycwClUwINLqMMtNKAJBB7gGV7hzZHOi/PufCxac042cwx/4Hz+ACPVjig=</latexit>f(x?)<latexit sha1_base64="j3AFkUraq/AuU1MJs5rROJBN+MY=">AAAB83icbZBNT8JAEIan+IX4hXr00ggmeCEtFz2SePGIiXwktJLtsoUN222zOzWShr/hxYPGePXPePPfuEAPCr7JJk/emcnMvkEiuEbH+bYKG5tb2zvF3dLe/sHhUfn4pKPjVFHWprGIVS8gmgkuWRs5CtZLFCNRIFg3mNzM691HpjSP5T1OE+ZHZCR5yClBY3nVsPb04Gkk6rI6KFecurOQvQ5uDhXI1RqUv7xhTNOISaSCaN13nQT9jCjkVLBZyUs1SwidkBHrG5QkYtrPFjfP7AvjDO0wVuZJtBfu74mMRFpPo8B0RgTHerU2N/+r9VMMr/2MyyRFJulyUZgKG2N7HoA95IpRFFMDhCpubrXpmChC0cRUMiG4q19eh06j7hq+a1SazTyOIpzBOdTAhStowi20oA0UEniGV3izUuvFerc+lq0FK585hT+yPn8Ay5mQ2Q==</latexit><latexit sha1_base64="j3AFkUraq/AuU1MJs5rROJBN+MY=">AAAB83icbZBNT8JAEIan+IX4hXr00ggmeCEtFz2SePGIiXwktJLtsoUN222zOzWShr/hxYPGePXPePPfuEAPCr7JJk/emcnMvkEiuEbH+bYKG5tb2zvF3dLe/sHhUfn4pKPjVFHWprGIVS8gmgkuWRs5CtZLFCNRIFg3mNzM691HpjSP5T1OE+ZHZCR5yClBY3nVsPb04Gkk6rI6KFecurOQvQ5uDhXI1RqUv7xhTNOISaSCaN13nQT9jCjkVLBZyUs1SwidkBHrG5QkYtrPFjfP7AvjDO0wVuZJtBfu74mMRFpPo8B0RgTHerU2N/+r9VMMr/2MyyRFJulyUZgKG2N7HoA95IpRFFMDhCpubrXpmChC0cRUMiG4q19eh06j7hq+a1SazTyOIpzBOdTAhStowi20oA0UEniGV3izUuvFerc+lq0FK585hT+yPn8Ay5mQ2Q==</latexit><latexit sha1_base64="j3AFkUraq/AuU1MJs5rROJBN+MY=">AAAB83icbZBNT8JAEIan+IX4hXr00ggmeCEtFz2SePGIiXwktJLtsoUN222zOzWShr/hxYPGePXPePPfuEAPCr7JJk/emcnMvkEiuEbH+bYKG5tb2zvF3dLe/sHhUfn4pKPjVFHWprGIVS8gmgkuWRs5CtZLFCNRIFg3mNzM691HpjSP5T1OE+ZHZCR5yClBY3nVsPb04Gkk6rI6KFecurOQvQ5uDhXI1RqUv7xhTNOISaSCaN13nQT9jCjkVLBZyUs1SwidkBHrG5QkYtrPFjfP7AvjDO0wVuZJtBfu74mMRFpPo8B0RgTHerU2N/+r9VMMr/2MyyRFJulyUZgKG2N7HoA95IpRFFMDhCpubrXpmChC0cRUMiG4q19eh06j7hq+a1SazTyOIpzBOdTAhStowi20oA0UEniGV3izUuvFerc+lq0FK585hT+yPn8Ay5mQ2Q==</latexit><latexit sha1_base64="j3AFkUraq/AuU1MJs5rROJBN+MY=">AAAB83icbZBNT8JAEIan+IX4hXr00ggmeCEtFz2SePGIiXwktJLtsoUN222zOzWShr/hxYPGePXPePPfuEAPCr7JJk/emcnMvkEiuEbH+bYKG5tb2zvF3dLe/sHhUfn4pKPjVFHWprGIVS8gmgkuWRs5CtZLFCNRIFg3mNzM691HpjSP5T1OE+ZHZCR5yClBY3nVsPb04Gkk6rI6KFecurOQvQ5uDhXI1RqUv7xhTNOISaSCaN13nQT9jCjkVLBZyUs1SwidkBHrG5QkYtrPFjfP7AvjDO0wVuZJtBfu74mMRFpPo8B0RgTHerU2N/+r9VMMr/2MyyRFJulyUZgKG2N7HoA95IpRFFMDhCpubrXpmChC0cRUMiG4q19eh06j7hq+a1SazTyOIpzBOdTAhStowi20oA0UEniGV3izUuvFerc+lq0FK585hT+yPn8Ay5mQ2Q==</latexit>x?<latexit sha1_base64="1HSaP89i6ZNuxL6ngZHnK0kxsVM=">AAAB8HicbZA9T8MwEIYvfJbyVWBkiWiRmKqkC4yVWBiLRD9QGyrHdVqrthPZF0QV9VewMIAQKz+HjX+D22aAlley9Oi9O/nuDRPBDXret7O2vrG5tV3YKe7u7R8clo6OWyZONWVNGotYd0JimOCKNZGjYJ1EMyJDwdrh+HpWbz8ybXis7nCSsECSoeIRpwStdV95eugZJLrSL5W9qjeXuwp+DmXI1eiXvnqDmKaSKaSCGNP1vQSDjGjkVLBpsZcalhA6JkPWtaiIZCbI5gtP3XPrDNwo1vYpdOfu74mMSGMmMrSdkuDILNdm5n+1borRVZBxlaTIFF18FKXCxdidXe8OuGYUxcQCoZrbXV06IppQtBkVbQj+8smr0KpVfcu3tXK9nsdRgFM4gwvw4RLqcAMNaAIFCc/wCm+Odl6cd+dj0brm5DMn8EfO5w8+Z5AE</latexit><latexit sha1_base64="1HSaP89i6ZNuxL6ngZHnK0kxsVM=">AAAB8HicbZA9T8MwEIYvfJbyVWBkiWiRmKqkC4yVWBiLRD9QGyrHdVqrthPZF0QV9VewMIAQKz+HjX+D22aAlley9Oi9O/nuDRPBDXret7O2vrG5tV3YKe7u7R8clo6OWyZONWVNGotYd0JimOCKNZGjYJ1EMyJDwdrh+HpWbz8ybXis7nCSsECSoeIRpwStdV95eugZJLrSL5W9qjeXuwp+DmXI1eiXvnqDmKaSKaSCGNP1vQSDjGjkVLBpsZcalhA6JkPWtaiIZCbI5gtP3XPrDNwo1vYpdOfu74mMSGMmMrSdkuDILNdm5n+1borRVZBxlaTIFF18FKXCxdidXe8OuGYUxcQCoZrbXV06IppQtBkVbQj+8smr0KpVfcu3tXK9nsdRgFM4gwvw4RLqcAMNaAIFCc/wCm+Odl6cd+dj0brm5DMn8EfO5w8+Z5AE</latexit><latexit sha1_base64="1HSaP89i6ZNuxL6ngZHnK0kxsVM=">AAAB8HicbZA9T8MwEIYvfJbyVWBkiWiRmKqkC4yVWBiLRD9QGyrHdVqrthPZF0QV9VewMIAQKz+HjX+D22aAlley9Oi9O/nuDRPBDXret7O2vrG5tV3YKe7u7R8clo6OWyZONWVNGotYd0JimOCKNZGjYJ1EMyJDwdrh+HpWbz8ybXis7nCSsECSoeIRpwStdV95eugZJLrSL5W9qjeXuwp+DmXI1eiXvnqDmKaSKaSCGNP1vQSDjGjkVLBpsZcalhA6JkPWtaiIZCbI5gtP3XPrDNwo1vYpdOfu74mMSGMmMrSdkuDILNdm5n+1borRVZBxlaTIFF18FKXCxdidXe8OuGYUxcQCoZrbXV06IppQtBkVbQj+8smr0KpVfcu3tXK9nsdRgFM4gwvw4RLqcAMNaAIFCc/wCm+Odl6cd+dj0brm5DMn8EfO5w8+Z5AE</latexit><latexit sha1_base64="1HSaP89i6ZNuxL6ngZHnK0kxsVM=">AAAB8HicbZA9T8MwEIYvfJbyVWBkiWiRmKqkC4yVWBiLRD9QGyrHdVqrthPZF0QV9VewMIAQKz+HjX+D22aAlley9Oi9O/nuDRPBDXret7O2vrG5tV3YKe7u7R8clo6OWyZONWVNGotYd0JimOCKNZGjYJ1EMyJDwdrh+HpWbz8ybXis7nCSsECSoeIRpwStdV95eugZJLrSL5W9qjeXuwp+DmXI1eiXvnqDmKaSKaSCGNP1vQSDjGjkVLBpsZcalhA6JkPWtaiIZCbI5gtP3XPrDNwo1vYpdOfu74mMSGMmMrSdkuDILNdm5n+1borRVZBxlaTIFF18FKXCxdidXe8OuGYUxcQCoZrbXV06IppQtBkVbQj+8smr0KpVfcu3tXK9nsdRgFM4gwvw4RLqcAMNaAIFCc/wCm+Odl6cd+dj0brm5DMn8EfO5w8+Z5AE</latexit>kx x?k2✏<latexit sha1_base64="DoHRRz7mM2yqCp0yK1NCedRVspQ=">AAACCHicbZC7TsMwFIadcivlFmBkwKJFYqFKusBYiYWxSPQiNSVy3NPWquME20Gt0o4svAoLAwix8ghsvA3uZYCWX7L06T/n6Pj8QcyZ0o7zbWVWVtfWN7Kbua3tnd09e/+gpqJEUqjSiEeyERAFnAmoaqY5NGIJJAw41IP+1aRefwCpWCRu9TCGVki6gnUYJdpYvn1c8EaD88GdpzSR3sgvYY/DPfYgVoxHouDbeafoTIWXwZ1DHs1V8e0vrx3RJAShKSdKNV0n1q2USM0oh3HOSxTEhPZJF5oGBQlBtdLpIWN8apw27kTSPKHx1P09kZJQqWEYmM6Q6J5arE3M/2rNRHcuWykTcaJB0NmiTsKxjvAkFdxmEqjmQwOESmb+immPSEK1yS5nQnAXT16GWqnoGr4p5cvleRxZdIRO0Bly0QUqo2tUQVVE0SN6Rq/ozXqyXqx362PWmrHmM4foj6zPH8u1mSs=</latexit><latexit sha1_base64="DoHRRz7mM2yqCp0yK1NCedRVspQ=">AAACCHicbZC7TsMwFIadcivlFmBkwKJFYqFKusBYiYWxSPQiNSVy3NPWquME20Gt0o4svAoLAwix8ghsvA3uZYCWX7L06T/n6Pj8QcyZ0o7zbWVWVtfWN7Kbua3tnd09e/+gpqJEUqjSiEeyERAFnAmoaqY5NGIJJAw41IP+1aRefwCpWCRu9TCGVki6gnUYJdpYvn1c8EaD88GdpzSR3sgvYY/DPfYgVoxHouDbeafoTIWXwZ1DHs1V8e0vrx3RJAShKSdKNV0n1q2USM0oh3HOSxTEhPZJF5oGBQlBtdLpIWN8apw27kTSPKHx1P09kZJQqWEYmM6Q6J5arE3M/2rNRHcuWykTcaJB0NmiTsKxjvAkFdxmEqjmQwOESmb+immPSEK1yS5nQnAXT16GWqnoGr4p5cvleRxZdIRO0Bly0QUqo2tUQVVE0SN6Rq/ozXqyXqx362PWmrHmM4foj6zPH8u1mSs=</latexit><latexit sha1_base64="DoHRRz7mM2yqCp0yK1NCedRVspQ=">AAACCHicbZC7TsMwFIadcivlFmBkwKJFYqFKusBYiYWxSPQiNSVy3NPWquME20Gt0o4svAoLAwix8ghsvA3uZYCWX7L06T/n6Pj8QcyZ0o7zbWVWVtfWN7Kbua3tnd09e/+gpqJEUqjSiEeyERAFnAmoaqY5NGIJJAw41IP+1aRefwCpWCRu9TCGVki6gnUYJdpYvn1c8EaD88GdpzSR3sgvYY/DPfYgVoxHouDbeafoTIWXwZ1DHs1V8e0vrx3RJAShKSdKNV0n1q2USM0oh3HOSxTEhPZJF5oGBQlBtdLpIWN8apw27kTSPKHx1P09kZJQqWEYmM6Q6J5arE3M/2rNRHcuWykTcaJB0NmiTsKxjvAkFdxmEqjmQwOESmb+immPSEK1yS5nQnAXT16GWqnoGr4p5cvleRxZdIRO0Bly0QUqo2tUQVVE0SN6Rq/ozXqyXqx362PWmrHmM4foj6zPH8u1mSs=</latexit><latexit sha1_base64="ck8pdC+ekZH4nUmSP+ZG7r8lEyk=">AAAB2XicbZDNSgMxFIXv1L86Vq1rN8EiuCozbnQpuHFZwbZCO5RM5k4bmskMyR2hDH0BF25EfC93vo3pz0JbDwQ+zknIvSculLQUBN9ebWd3b/+gfugfNfzjk9Nmo2fz0gjsilzl5jnmFpXU2CVJCp8LgzyLFfbj6f0i77+gsTLXTzQrMMr4WMtUCk7O6oyaraAdLMW2IVxDC9YaNb+GSS7KDDUJxa0dhEFBUcUNSaFw7g9LiwUXUz7GgUPNM7RRtRxzzi6dk7A0N+5oYkv394uKZ9bOstjdzDhN7Ga2MP/LBiWlt1EldVESarH6KC0Vo5wtdmaJNChIzRxwYaSblYkJN1yQa8Z3HYSbG29D77odOn4MoA7ncAFXEMIN3MEDdKALAhJ4hXdv4r15H6uuat66tDP4I+/zBzjGijg=</latexit><latexit sha1_base64="p3GAkxepYK0/hd8vlF9v81bI25M=">AAAB/XicbZBLTwIxFIXv4AsRFd26sBFM3Ehm2OjSxI1LTOSRMEg65QINnc7YdgwEWLrxr7hxoTH+DXf+G8tjoeBJmnw5p83tPUEsuDau++2k1tY3NrfS25md7O7efu4gW9VRohhWWCQiVQ+oRsElVgw3AuuxQhoGAmtB/3qa1x5RaR7JOzOMsRnSruQdzqixVit3XPDHg/PBva8NVf64VSK+wAfiY6y5iGShlcu7RXcmsgreAvKwULmV+/LbEUtClIYJqnXDc2PTHFFlOBM4yfiJxpiyPu1iw6KkIermaLbIhJxap006kbJHGjJzf78Y0VDrYRjYmyE1Pb2cTc3/skZiOpfNEZdxYlCy+aBOIoiJyLQV0uYKmRFDC5Qpbv9KWI8qyoztLmNL8JZXXoVqqehZvnUhDUdwAmfgwQVcwQ2UoQIMnuAF3uDdeXZenY95XSln0dsh/JHz+QM32Je8</latexit><latexit sha1_base64="p3GAkxepYK0/hd8vlF9v81bI25M=">AAAB/XicbZBLTwIxFIXv4AsRFd26sBFM3Ehm2OjSxI1LTOSRMEg65QINnc7YdgwEWLrxr7hxoTH+DXf+G8tjoeBJmnw5p83tPUEsuDau++2k1tY3NrfS25md7O7efu4gW9VRohhWWCQiVQ+oRsElVgw3AuuxQhoGAmtB/3qa1x5RaR7JOzOMsRnSruQdzqixVit3XPDHg/PBva8NVf64VSK+wAfiY6y5iGShlcu7RXcmsgreAvKwULmV+/LbEUtClIYJqnXDc2PTHFFlOBM4yfiJxpiyPu1iw6KkIermaLbIhJxap006kbJHGjJzf78Y0VDrYRjYmyE1Pb2cTc3/skZiOpfNEZdxYlCy+aBOIoiJyLQV0uYKmRFDC5Qpbv9KWI8qyoztLmNL8JZXXoVqqehZvnUhDUdwAmfgwQVcwQ2UoQIMnuAF3uDdeXZenY95XSln0dsh/JHz+QM32Je8</latexit><latexit sha1_base64="PsaHGvPWV2KMb4TyqF9oOMdYXqg=">AAACCHicbZC7TsMwFIadcivlFmBkwKJFYqFKusBYiYWxSPQiNSVy3NPWquME20Gt0o4svAoLAwix8ghsvA3uZYCWX7L06T/n6Pj8QcyZ0o7zbWVWVtfWN7Kbua3tnd09e/+gpqJEUqjSiEeyERAFnAmoaqY5NGIJJAw41IP+1aRefwCpWCRu9TCGVki6gnUYJdpYvn1c8EaD88GdpzSR3sgvYY/DPfYgVoxHouDbeafoTIWXwZ1DHs1V8e0vrx3RJAShKSdKNV0n1q2USM0oh3HOSxTEhPZJF5oGBQlBtdLpIWN8apw27kTSPKHx1P09kZJQqWEYmM6Q6J5arE3M/2rNRHcuWykTcaJB0NmiTsKxjvAkFdxmEqjmQwOESmb+immPSEK1yS5nQnAXT16GWqnoGr5x8uXyPI4sOkIn6Ay56AKV0TWqoCqi6BE9o1f0Zj1ZL9a79TFrzVjzmUP0R9bnD8sVmSk=</latexit><latexit sha1_base64="DoHRRz7mM2yqCp0yK1NCedRVspQ=">AAACCHicbZC7TsMwFIadcivlFmBkwKJFYqFKusBYiYWxSPQiNSVy3NPWquME20Gt0o4svAoLAwix8ghsvA3uZYCWX7L06T/n6Pj8QcyZ0o7zbWVWVtfWN7Kbua3tnd09e/+gpqJEUqjSiEeyERAFnAmoaqY5NGIJJAw41IP+1aRefwCpWCRu9TCGVki6gnUYJdpYvn1c8EaD88GdpzSR3sgvYY/DPfYgVoxHouDbeafoTIWXwZ1DHs1V8e0vrx3RJAShKSdKNV0n1q2USM0oh3HOSxTEhPZJF5oGBQlBtdLpIWN8apw27kTSPKHx1P09kZJQqWEYmM6Q6J5arE3M/2rNRHcuWykTcaJB0NmiTsKxjvAkFdxmEqjmQwOESmb+immPSEK1yS5nQnAXT16GWqnoGr4p5cvleRxZdIRO0Bly0QUqo2tUQVVE0SN6Rq/ozXqyXqx362PWmrHmM4foj6zPH8u1mSs=</latexit><latexit sha1_base64="DoHRRz7mM2yqCp0yK1NCedRVspQ=">AAACCHicbZC7TsMwFIadcivlFmBkwKJFYqFKusBYiYWxSPQiNSVy3NPWquME20Gt0o4svAoLAwix8ghsvA3uZYCWX7L06T/n6Pj8QcyZ0o7zbWVWVtfWN7Kbua3tnd09e/+gpqJEUqjSiEeyERAFnAmoaqY5NGIJJAw41IP+1aRefwCpWCRu9TCGVki6gnUYJdpYvn1c8EaD88GdpzSR3sgvYY/DPfYgVoxHouDbeafoTIWXwZ1DHs1V8e0vrx3RJAShKSdKNV0n1q2USM0oh3HOSxTEhPZJF5oGBQlBtdLpIWN8apw27kTSPKHx1P09kZJQqWEYmM6Q6J5arE3M/2rNRHcuWykTcaJB0NmiTsKxjvAkFdxmEqjmQwOESmb+immPSEK1yS5nQnAXT16GWqnoGr4p5cvleRxZdIRO0Bly0QUqo2tUQVVE0SN6Rq/ozXqyXqx362PWmrHmM4foj6zPH8u1mSs=</latexit><latexit sha1_base64="DoHRRz7mM2yqCp0yK1NCedRVspQ=">AAACCHicbZC7TsMwFIadcivlFmBkwKJFYqFKusBYiYWxSPQiNSVy3NPWquME20Gt0o4svAoLAwix8ghsvA3uZYCWX7L06T/n6Pj8QcyZ0o7zbWVWVtfWN7Kbua3tnd09e/+gpqJEUqjSiEeyERAFnAmoaqY5NGIJJAw41IP+1aRefwCpWCRu9TCGVki6gnUYJdpYvn1c8EaD88GdpzSR3sgvYY/DPfYgVoxHouDbeafoTIWXwZ1DHs1V8e0vrx3RJAShKSdKNV0n1q2USM0oh3HOSxTEhPZJF5oGBQlBtdLpIWN8apw27kTSPKHx1P09kZJQqWEYmM6Q6J5arE3M/2rNRHcuWykTcaJB0NmiTsKxjvAkFdxmEqjmQwOESmb+immPSEK1yS5nQnAXT16GWqnoGr4p5cvleRxZdIRO0Bly0QUqo2tUQVVE0SN6Rq/ozXqyXqx362PWmrHmM4foj6zPH8u1mSs=</latexit><latexit sha1_base64="DoHRRz7mM2yqCp0yK1NCedRVspQ=">AAACCHicbZC7TsMwFIadcivlFmBkwKJFYqFKusBYiYWxSPQiNSVy3NPWquME20Gt0o4svAoLAwix8ghsvA3uZYCWX7L06T/n6Pj8QcyZ0o7zbWVWVtfWN7Kbua3tnd09e/+gpqJEUqjSiEeyERAFnAmoaqY5NGIJJAw41IP+1aRefwCpWCRu9TCGVki6gnUYJdpYvn1c8EaD88GdpzSR3sgvYY/DPfYgVoxHouDbeafoTIWXwZ1DHs1V8e0vrx3RJAShKSdKNV0n1q2USM0oh3HOSxTEhPZJF5oGBQlBtdLpIWN8apw27kTSPKHx1P09kZJQqWEYmM6Q6J5arE3M/2rNRHcuWykTcaJB0NmiTsKxjvAkFdxmEqjmQwOESmb+immPSEK1yS5nQnAXT16GWqnoGr4p5cvleRxZdIRO0Bly0QUqo2tUQVVE0SN6Rq/ozXqyXqx362PWmrHmM4foj6zPH8u1mSs=</latexit><latexit sha1_base64="DoHRRz7mM2yqCp0yK1NCedRVspQ=">AAACCHicbZC7TsMwFIadcivlFmBkwKJFYqFKusBYiYWxSPQiNSVy3NPWquME20Gt0o4svAoLAwix8ghsvA3uZYCWX7L06T/n6Pj8QcyZ0o7zbWVWVtfWN7Kbua3tnd09e/+gpqJEUqjSiEeyERAFnAmoaqY5NGIJJAw41IP+1aRefwCpWCRu9TCGVki6gnUYJdpYvn1c8EaD88GdpzSR3sgvYY/DPfYgVoxHouDbeafoTIWXwZ1DHs1V8e0vrx3RJAShKSdKNV0n1q2USM0oh3HOSxTEhPZJF5oGBQlBtdLpIWN8apw27kTSPKHx1P09kZJQqWEYmM6Q6J5arE3M/2rNRHcuWykTcaJB0NmiTsKxjvAkFdxmEqjmQwOESmb+immPSEK1yS5nQnAXT16GWqnoGr4p5cvleRxZdIRO0Bly0QUqo2tUQVVE0SN6Rq/ozXqyXqx362PWmrHmM4foj6zPH8u1mSs=</latexit><latexit sha1_base64="DoHRRz7mM2yqCp0yK1NCedRVspQ=">AAACCHicbZC7TsMwFIadcivlFmBkwKJFYqFKusBYiYWxSPQiNSVy3NPWquME20Gt0o4svAoLAwix8ghsvA3uZYCWX7L06T/n6Pj8QcyZ0o7zbWVWVtfWN7Kbua3tnd09e/+gpqJEUqjSiEeyERAFnAmoaqY5NGIJJAw41IP+1aRefwCpWCRu9TCGVki6gnUYJdpYvn1c8EaD88GdpzSR3sgvYY/DPfYgVoxHouDbeafoTIWXwZ1DHs1V8e0vrx3RJAShKSdKNV0n1q2USM0oh3HOSxTEhPZJF5oGBQlBtdLpIWN8apw27kTSPKHx1P09kZJQqWEYmM6Q6J5arE3M/2rNRHcuWykTcaJB0NmiTsKxjvAkFdxmEqjmQwOESmb+immPSEK1yS5nQnAXT16GWqnoGr4p5cvleRxZdIRO0Bly0QUqo2tUQVVE0SN6Rq/ozXqyXqx362PWmrHmM4foj6zPH8u1mSs=</latexit>kf(x) f(x?)k2 <latexit sha1_base64="kcW+Rnzd5H2LQ0zdmiQW/3R8W1g=">AAACDHicbVDLTgIxFO34RHyhLt00gokuJDO40CWJG5eYyCNhkHTKHWjsPGzvGAnwAW78FTcuNMatH+DOv7HALBQ8SZuTc85Ne48XS6HRtr+thcWl5ZXVzFp2fWNzazu3s1vTUaI4VHkkI9XwmAYpQqiiQAmNWAELPAl17/Zi7NfvQWkRhdfYj6EVsG4ofMEZGqmdyxfcoX/0cHxirhtXI1PH7rBdoq6EO+p2QCIrmJRdtCeg88RJSZ6kqLRzX24n4kkAIXLJtG46doytAVMouIRR1k00xIzfsi40DQ1ZALo1mCwzoodG6VA/UuaESCfq74kBC7TuB55JBgx7etYbi/95zQT989ZAhHGCEPLpQ34iKUZ03AztCAUcZd8QxpUwf6W8xxTjaPrLmhKc2ZXnSa1UdE6LzlUpXy6ndWTIPjkgR8QhZ6RMLkmFVAknj+SZvJI368l6sd6tj2l0wUpn9sgfWJ8/PGqZ1A==</latexit>x<latexit sha1_base64="BmVm2kBoL99/m27NxkUbvUDufsI=">AAAB6nicbZA9TwJBEIbn8AvxC7W02QgmVuSORksSG0uM8pHAhewte7Bhb++yO2ckF36CjYXG2PqL7Pw3LnCFgm+yyZN3ZrIzb5BIYdB1v53CxubW9k5xt7S3f3B4VD4+aZs41Yy3WCxj3Q2o4VIo3kKBkncTzWkUSN4JJjfzeueRayNi9YDThPsRHSkRCkbRWvfVp+qgXHFr7kJkHbwcKpCrOSh/9YcxSyOukElqTM9zE/QzqlEwyWelfmp4QtmEjnjPoqIRN362WHVGLqwzJGGs7VNIFu7viYxGxkyjwHZGFMdmtTY3/6v1Ugyv/UyoJEWu2PKjMJUEYzK/mwyF5gzl1AJlWthdCRtTTRnadEo2BG/15HVo12ue5bt6pdHI4yjCGZzDJXhwBQ24hSa0gMEInuEV3hzpvDjvzseyteDkM6fwR87nD5y6jVQ=</latexit><latexit sha1_base64="BmVm2kBoL99/m27NxkUbvUDufsI=">AAAB6nicbZA9TwJBEIbn8AvxC7W02QgmVuSORksSG0uM8pHAhewte7Bhb++yO2ckF36CjYXG2PqL7Pw3LnCFgm+yyZN3ZrIzb5BIYdB1v53CxubW9k5xt7S3f3B4VD4+aZs41Yy3WCxj3Q2o4VIo3kKBkncTzWkUSN4JJjfzeueRayNi9YDThPsRHSkRCkbRWvfVp+qgXHFr7kJkHbwcKpCrOSh/9YcxSyOukElqTM9zE/QzqlEwyWelfmp4QtmEjnjPoqIRN362WHVGLqwzJGGs7VNIFu7viYxGxkyjwHZGFMdmtTY3/6v1Ugyv/UyoJEWu2PKjMJUEYzK/mwyF5gzl1AJlWthdCRtTTRnadEo2BG/15HVo12ue5bt6pdHI4yjCGZzDJXhwBQ24hSa0gMEInuEV3hzpvDjvzseyteDkM6fwR87nD5y6jVQ=</latexit><latexit sha1_base64="BmVm2kBoL99/m27NxkUbvUDufsI=">AAAB6nicbZA9TwJBEIbn8AvxC7W02QgmVuSORksSG0uM8pHAhewte7Bhb++yO2ckF36CjYXG2PqL7Pw3LnCFgm+yyZN3ZrIzb5BIYdB1v53CxubW9k5xt7S3f3B4VD4+aZs41Yy3WCxj3Q2o4VIo3kKBkncTzWkUSN4JJjfzeueRayNi9YDThPsRHSkRCkbRWvfVp+qgXHFr7kJkHbwcKpCrOSh/9YcxSyOukElqTM9zE/QzqlEwyWelfmp4QtmEjnjPoqIRN362WHVGLqwzJGGs7VNIFu7viYxGxkyjwHZGFMdmtTY3/6v1Ugyv/UyoJEWu2PKjMJUEYzK/mwyF5gzl1AJlWthdCRtTTRnadEo2BG/15HVo12ue5bt6pdHI4yjCGZzDJXhwBQ24hSa0gMEInuEV3hzpvDjvzseyteDkM6fwR87nD5y6jVQ=</latexit><latexit sha1_base64="BmVm2kBoL99/m27NxkUbvUDufsI=">AAAB6nicbZA9TwJBEIbn8AvxC7W02QgmVuSORksSG0uM8pHAhewte7Bhb++yO2ckF36CjYXG2PqL7Pw3LnCFgm+yyZN3ZrIzb5BIYdB1v53CxubW9k5xt7S3f3B4VD4+aZs41Yy3WCxj3Q2o4VIo3kKBkncTzWkUSN4JJjfzeueRayNi9YDThPsRHSkRCkbRWvfVp+qgXHFr7kJkHbwcKpCrOSh/9YcxSyOukElqTM9zE/QzqlEwyWelfmp4QtmEjnjPoqIRN362WHVGLqwzJGGs7VNIFu7viYxGxkyjwHZGFMdmtTY3/6v1Ugyv/UyoJEWu2PKjMJUEYzK/mwyF5gzl1AJlWthdCRtTTRnadEo2BG/15HVo12ue5bt6pdHI4yjCGZzDJXhwBQ24hSa0gMEInuEV3hzpvDjvzseyteDkM6fwR87nD5y6jVQ=</latexit>kf(x) f(x?)k2L2kx x?k2<latexit sha1_base64="TZXeUxHiiPOPf+dFoMuZUOCv/+8=">AAACGXicbZC7TsMwFIYdrqXcAowsFi1SO7RKygBjJRYGhiLRi9SEyHGd1qpzwXZQq7SvwcKrsDCAECNMvA1OmwFajmTr0/+fI/v8bsSokIbxra2srq1vbOa28ts7u3v7+sFhS4Qxx6SJQxbyjosEYTQgTUklI52IE+S7jLTd4WXqtx8IFzQMbuU4IraP+gH1KEZSSY5uFK2JVxqVK+q6s4REvGxNnBq0GLmH1ylMRpXMSY2ioxeMqjEruAxmBgWQVcPRP61eiGOfBBIzJETXNCJpJ4hLihmZ5q1YkAjhIeqTrsIA+UTYyWyzKTxVSg96IVcnkHCm/p5IkC/E2HdVp4/kQCx6qfif142ld2EnNIhiSQI8f8iLGZQhTGOCPcoJlmysAGFO1V8hHiCOsFRh5lUI5uLKy9CqVc2zqnlTK9TrWRw5cAxOQAmY4BzUwRVogCbA4BE8g1fwpj1pL9q79jFvXdGymSPwp7SvH4KVnr8=</latexit>activationsactivations✏<latexit sha1_base64="i4udT/drOOO6oO7noIEGU/CaHW4=">AAAB8XicbVA9TwJBEJ3DL8Qv1NJmI5hYkTsspCSxscREwAgXsrfMwYa9vcvungm58C9sLDTG1n9j579xgSsUfMkkL+/NZGZekAiujet+O4WNza3tneJuaW//4PCofHzS0XGqGLZZLGL1EFCNgktsG24EPiQKaRQI7AaTm7nffUKleSzvzTRBP6IjyUPOqLHSY7WPieYiltVBueLW3AXIOvFyUoEcrUH5qz+MWRqhNExQrXuemxg/o8pwJnBW6qcaE8omdIQ9SyWNUPvZ4uIZubDKkISxsiUNWai/JzIaaT2NAtsZUTPWq95c/M/rpSZs+BmXSWpQsuWiMBXExGT+PhlyhcyIqSWUKW5vJWxMFWXGhlSyIXirL6+TTr3mXdW8u3ql2cjjKMIZnMMleHANTbiFFrSBgYRneIU3RzsvzrvzsWwtOPnMKfyB8/kDBdqQcw==</latexit> <latexit sha1_base64="7vRx4A6LElEKTYMBacNXemCuKD8=">AAAB73icbVA9SwNBEJ3zM8avqKXNYiJYhbtYaCUBG8sI5gOSI+xt5pIle3vn7p4QjvwJGwtFbP07dv4bN8kVmvhg4PHeDDPzgkRwbVz321lb39jc2i7sFHf39g8OS0fHLR2nimGTxSJWnYBqFFxi03AjsJMopFEgsB2Mb2d++wmV5rF8MJME/YgOJQ85o8ZKnUpvgMLQSr9UdqvuHGSVeDkpQ45Gv/TVG8QsjVAaJqjWXc9NjJ9RZTgTOC32Uo0JZWM6xK6lkkao/Wx+75ScW2VAwljZkobM1d8TGY20nkSB7YyoGellbyb+53VTE177GZdJalCyxaIwFcTEZPY8GXCFzIiJJZQpbm8lbEQVZcZGVLQheMsvr5JWrepdVr37Wrl+k8dRgFM4gwvw4ArqcAcNaAIDAc/wCm/Oo/PivDsfi9Y1J585gT9wPn8ASyOPdQ==</latexit>(cid:54)
A.4 Proof of Theorem 1

Deﬁne x1 = φ(W 0x + b0)
Lemma 1, we can write the quadratic inequality

Rn and y1 = φ(W 0y + b0)

∈

Rn for two arbitrary inputs x, y

Rn0. Using

∈

∈

0

≤

(cid:20)
∈ Tn and

where T

(W 0x + b0)
x1

(W 0y + b0)
y1

−
−

(cid:62)

(cid:21)

2αβT
−
(α + β)T

(cid:20)

(α + β)T
2T

−

(W 0x + b0)
x1

(cid:21) (cid:20)

(W 0y + b0)
y1

(cid:21)

,

−
−

Tn is deﬁned as in (8). The preceding inequality can be simpliﬁed to

0

≤

x
x1
(cid:20)

−
−

y
y1

(cid:62)

(cid:21)

(cid:20)

2αβW 0(cid:62)T W 0
(α + β)T W 0

−

(α + β)W 0(cid:62)T
2T

−

x
x1

(cid:21) (cid:20)

y
y1

−
−

(cid:21)

.

(22)

By left and right multiplying M (ρ, T ) in (10) by
tively, and rearranging terms, we obtain

(x

−

(cid:2)

y)(cid:62) (x1

−

y1)(cid:62)

and

(x

y)(cid:62) (x1

−

−

y1)(cid:62)

(cid:62), respec-

(cid:2)

(cid:3)

x
x1
(cid:20)

−
−

y
y1

(cid:21)

(cid:20)

(cid:62)

2αβW 0(cid:62)T W 0
(α + β)T W 0

−

(cid:3)
(α + β)W 0(cid:62)T
2T

−

x
x1

(cid:21) (cid:20)

x
x1
(cid:20)

−
−

y
y1

≤

(cid:62)

L2
2In0
0

0
W 1(cid:62)W 1

x
x1

(cid:21) (cid:20)

−
−

(23)

(cid:21)

.

y
y1

−
−
y
y1

(cid:21)

−
By adding both sides of the preceding inequalities, we obtain

(cid:21)

(cid:20)

0

≤

x
x1
(cid:20)

−
−

y
y1

(cid:62)

(cid:21)

L2
2In0
0

(cid:20)

0
W 1(cid:62)W 1

−

x
x1

(cid:21) (cid:20)

y
y1

−
−

(cid:21)

,

or, equivalently,

(x1

−

y1)(cid:62)W 1(cid:62)W 1(x1

y1)

−

≤

L2

2(x

−

y)(cid:62)(x

y).

−

Finally, note that by deﬁnition of x1 and y1, we have f (x) = W 1x1 + b1 and f (y) = W 1y1 + b1. Therefore,
the preceding inequality implies

f (x)
(cid:107)

−

f (y)

(cid:107)2 ≤

x

L2(cid:107)

−

y

(cid:107)2

for all x, y

Rn0.

∈

(24)

A.5 Proof of Theorem 2

For two arbitrary inputs x0, y0
compact notation in (12), we can write

∈

Rn0, deﬁne x = [x0(cid:62)

x(cid:96)(cid:62)](cid:62)

and

y = [y0(cid:62)

y(cid:96)(cid:62)](cid:62) Using the

· · ·

· · ·

Bx = φ(Ax + b)

and By = φ(Ay + b).

Multiply both sides of the ﬁrst matrix in (14) by (x
identities to obtain

−

y)(cid:62) and (x

−

y), respectively and use the preceding

(cid:62)

A
B

2αβT
−
(α + β)T

(α + β)T
2T

A
B

(x

y)

−

(25)

(x

−

y)(cid:62)

(cid:20)
(cid:21)
Ay
By

Ax
Bx
(cid:20)

−
−

(cid:21)
Ax
φ(Ax + b)
(cid:20)

(cid:20)
(cid:62)

−
−

=

=

2αβT
−
(α + β)T

(cid:20)
Ay
φ(Ay + b)
(cid:21)

(cid:21) (cid:20)

−
(α + β)T
2T

(cid:21)
Ax
Bx

−

(cid:21) (cid:20)

(cid:62)

2αβT
−
(α + β)T

(cid:20)

Ay
By

−
−

(cid:21)
(α + β)T
2T

−

Ax
φ(Ax + b)

(cid:21) (cid:20)

Ay
φ(Ay + b)
(cid:21)

−
−

0,

≥

where the last inequality follows from Lemma 1. On the other hand, by multiplying both sides of the second
matrix in (14) by (x

y), respectively, we can write

y)(cid:62) and (x

−

−

(x

−

y)(cid:62)M (x

y) =

f (x)

(cid:107)

f (y)
(cid:107)

2
2 −

L2
x
2(cid:107)

−

y

2
2,
(cid:107)

−

−

(26)

15

Figure 7: Bounds on the image of an (cid:96)2 0.01-ball under a neural network trained on the Iris dataset. The
network was trained using the Adam optimizer and reached a test accuracy of 99%.

Figure 8: Lipschitz bounds for networks in Table 1.

where we have used the fact that f (x) = W (cid:96)x(cid:96) + b(cid:96) and f (y) = W (cid:96)y(cid:96) + b(cid:96). By adding both sides of (25)
and (26), we get

(x

−

y)(cid:62)

A
B

(cid:32)(cid:20)

(cid:62)

(cid:21)

2αβT
−
(α + β)T

(cid:20)

(α + β)T
2T

−

A
B

(cid:21) (cid:20)

(cid:21)

+ M

(x

(cid:33)

y)

−

f (x)

≥ (cid:107)

f (y)

2
2 −
(cid:107)

L2
2(cid:107)

x

y

(cid:107)2.

−

−

(27)

When the LMI in (14) holds, the left-hand side of (27) is non-positive, implying that the right-hand side is
non-positive.

A.6 Bounding the output set of a neural network classiﬁer

As we have shown, accurate estimation of the Lipschitz constant can provide tighter bounds on adversarial
examples drawn from a perturbation set around a nominal point ¯x. Figure 7 shows these bounds for a neural
network trained on the Iris dataset. Because this dataset has three classes, we project the output sets onto
the coordinate axes. This ﬁgure clearly shows that our bound is quite close to the naive lower bound. Indeed,
the naive upper bound and the constant computed by CPLip are considerably looser.

A.7 Further analysis of scalability

Figure 8 shows the Lipschitz bounds found for the networks used in Table 1.

16

