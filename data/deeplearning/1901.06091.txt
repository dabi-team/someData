Transfer Learning and Meta Classification Based Deep Churn 
Prediction System for Telecom Industry 

Uzair Ahmed1, Asifullah Khan1,2, Saddam Hussain Khan1, Abdul Basit3, Irfan Ul Haq1, and 

Yeon Soo Lee4* 

1Pattern Recognition Lab, Department of Computer & Information Sciences, Pakistan Institute of Engineering & 

2Center for Mathematical Sciences, Pakistan Institute of Engineering & Applied Sciences, Nilore, Islamabad 

Applied Sciences, Nilore, Islamabad 45650, Pakistan 

3TPD, Pakistan Institute of Nuclear Science and Technology, Nilore, Islamabad, Pakistan 
4Department of Biomedical Engineering, College of Medical Science, Catholic University of Daegu, South Korea  
asif@pieas.edu.pk  

45650, Pakistan 

Abstract: A churn prediction system guides telecom service providers to reduce revenue loss. However, the development of 
a churn prediction system for a telecom industry is a challenging task, mainly due to the large size of the data, high dimensional 
features,  and  imbalanced  distribution  of  the  data.  In  this  paper,  we  present  a  solution  to  the  inherent  problems  of  churn 
prediction, using the concept of Transfer Learning (TL) and Ensemble-based Meta-Classification. The proposed method ‚ÄúTL-
DeepE‚Äù  is  applied  in  two  stages.  The  first  stage  employs  TL  by  fine-tuning  multiple  pre-trained  Deep  Convolution  Neural 
Networks (CNNs). Telecom datasets are normally in vector form, which is converted into 2D images because Deep CNNs 
have high learning capacity on images. In the second stage, predictions from these Deep CNNs are appended to the original 
feature vector and thus are used to build a final feature vector for the high-level Genetic Programming (GP) and AdaBoost 
based  ensemble  classifier.  Thus,  the  experiments  are  conducted  using  various  CNNs  as  base  classifiers  and  the  GP-
AdaBoost  as  a  meta-classifier.  By  using  10-fold  cross-validation,  the  performance  of  the  proposed  TL-DeepE  system  is 
compared  with  existing  techniques,  for  two  standard  telecommunication  datasets;  Orange  and  Cell2cell.  Performing 
experiments on Orange and Cell2cell datasets, the prediction accuracy obtained was 75.4% and 68.2%, while the area under 
the curve was 0.83 and 0.74, respectively. 
Keywords:  Churn  Prediction,  Deep  Convolutional  Neural  Networks,  Telecom,  Transfer  Learning,  Meta-Classification, 
Genetic Programming, Ada-Boost, Imbalance Data, High Dimensionality, Customer Retention. 

1. 

Introduction 

‚ÄòCustomer  churn‚Äô  is  a  term  related  to  the  customer  subscription  model  and  refers  to  the  process,  whereby  a  customer 
terminates to use the services of a provider [1].  This term is frequently used in the telecommunication industries [2, 3], since 
several factors like coverage area and lucrative offers from competitors can trigger a user‚Äôs decision to switch from one service 
provider to another. Telecom corporations can benefit greatly by retaining customers that are on the verge  of churning. This 
is because it has been estimated that the cost of retention is usually 5 to 15 times less as compared with the cost of acquiring 
a new customer [4, 5]. The corporations, therefore, need an effective churn prediction system to automate the task of detecting 
the customers who might churn. The prediction is usually made on customer‚Äôs usage patterns and other factors, which directly 
or indirectly affect the user‚Äôs decisions. 

Customer Churn Prediction is a complex problem having challenges such as messy data, low churn rate, churn event 
censorship,  etc.  [6].  The  current  customer  churn  prediction  methods  are  largely  based  on  machine  learning  classification 
methods.  However,  the  performance  of  a  classifier  generally  suffers  due  to  high  dimensions  of  the  telecommunication 
datasets.  Furthermore,  the  telecommunication  data  has  an  imbalanced  nature  in  its  distribution  (with  a  limited  number  of 
examples of the minority class) that also hampers in achieving accurate churn prediction. Churner prediction is considered as 
a binary classification problem, where churners lie in the minority class and non-churners lie in the majority class. Several 
machine learning algorithms focus on improving the whole classification performance by sacrificing the accurate prediction of 
churners in the minority classes. Generally, churners  (minority class samples) are more misclassified as compared to non-
churners, due to the fundamental problem of its imbalanced nature. Due to the reason, machine learning based classification 
has  a  tendency  to  classify  all  samples  as  non-churners,  which  can  provide  good  accuracy  but  less  precision  in  terms  of 
predicting  churner‚Äôs  class.  These  complications  in  churn  prediction  show  that  there  is  still  a  margin  of  improvement  in 
performance, keeping in view the new challenges and opportunities that are arising in the domain of big data analytics. 

Previously, Support Vector Machines (SVMs) were applied to improve the performance of telecommunication churn 
prediction. Similarly, Filter and Wrapper methods were collectively used for feature selection, and their selected features were 
further provided to ensemble classifiers. These ensemble classifiers included Random Forest, RotBoost and SVMs [7, 8] that 
explore the selected feature vectors helpful for churn classification  [9]. Yabas et al. presented meta-classification methods 
[10]  for  standard  telecom  Orange  dataset.  Their  meta-classification  methods  include  bagging,  Random  Forest,  Logistic 
Regression and Decision Trees, which reported an Area Under the Curve (AUC) of 0.7230. Along similar lines, Koen et al. 
applied Rotation Forest and AdaBoost-based ensemble and reported an AUC as 0.697 [11]. Verbeke et al. have proposed 
different  methods  including  SVMs,  Na√Øve  Bayes,  k-NN,  Neural  Network  and  reported  AUC  of  0.714  on  standard 
telecommunication Orange dataset  [12]. Similarly, a work using an ensemble classifier with mRMR based processing [13]  
reported  that  RotBoost  and  Random  Forest-based  ensemble  [14]  resulted  in  an  AUC  of  0.749.  The  approach  of  GP  and 
AdaBoost  ensemble.  The  approach  of  GP  and  AdaBoost  ensemble  [15,  16]  was  employed  for  churn  prediction 
telecommunication data and reported an AUC of 0.63. 

1 

 
 
 
 
 
 
On the other hand, different strategies have been proposed to overcome class imbalance problems, which are divided 
into  two  main  categories;  Algorithm  Level  and  Data  Level.  Nowadays,  Transfer  Learning  method  has  also  been  used  for 
prediction problems with imbalanced nature of dataset.  The motivation of employing Transfer Learning  in conjunction  with 
ensemble  algorithms  for  telecommunication  churn  prediction  data  classification  is  three-fold.  Firstly,  several  ensemble 
algorithms  were  applied  by  researchers  and  mostly  did  not show  satisfactory  classification  performance,  i.e.  only  showed 
marginal improvement in the churn prediction. Secondly, Convolution Neural Networks (CNNs) are known to achieve better 
performance on images or grids like input topology. Consequently, in this work, feature vectors are converted into images, 
and deep learning methods are applied to achieve better performance. Finally, it has been observed through various research 
studies that Transfer Learning [17-20] using fine-tuning of an already trained network may give an improved performance as 
compared with training a CNN from scratch. 

The  proposed  Transfer  Learning  and  Ensemble-based  Meta-Classification  ‚ÄúTL-DeepE‚Äù  exploits  three  ideas  to  develop  an 
effective prediction system: 

1.  Firstly, it uses Deep CNNs as base classifiers for the ensemble classification. Moreover, both original and converted 
features are considered. The original numerical features are converted into 2D image data first and then, are provided 
to the base classifiers. 

2.  Secondly,  the  proposed  methodology  exploits  the  quick  learning  and  knowledge  transfer  abilities  of  ‚ÄúTransfer 

Learning‚Äù. To perform Transfer Learning in the CNNs models, TF Slim library [21] is used. 

3.  Thirdly,  we  exploit  the  learning  and  discrimination  abilities  of  the  high-level  Genetic  Programming  and  AdaBoost 
based ensemble classifier. In this regard, the predictions of the Deep CNNs are appended to the original feature 
vector and thus are used to build a final feature vector for the high-level ensemble classifier. 

The remaining sections of this manuscript are organized as follows: Section  2 provides details of the proposed TL-DeepE 
technique. It explains both the Transfer Learning idea as well as the GP-AdaBoost ensemble-based classification. Section 3 
provides details of the telecommunication dataset and experimental setup used in this work. Results and comparative analysis 
are discussed in section 4. Finally, section 5 concludes the paper. 

2.  Proposed TL-DeepE Prediction System 

The proposed TL-DeepE technique for telecom churn prediction uses the concept of meta-classification in addition to Transfer 
Learning (Figure 1). Input features are converted from one-dimensional vector to two-dimensional image format and are used 
to train multiple CNNs, which in turns give churn predictions (collectively called ‚Äúprediction space‚Äü). These predictions are then 
appended with the original feature vector, and classification is performed on this ‚Äúextended‚Äü feature set using GP-AdaBoost 
ensemble.  Experimental  evaluations  are  performed  by  using  10-fold  cross-validation  on  two  standard  telecommunication 
churn prediction datasets i.e. from Orange and Cell2cell. The exploitation of sample space by AdaBoosting and enhancement 
of the learning and generalization by GP [22] makes the proposed TL-DeepE an effective method for churn prediction and 
classification. 

2.1  Data Pre-Processing 

 Figure 1: An Overview of the proposed TL-DeepE Technique. 

Telecom datasets have many concerns to be addressed like missing values, non-numeric feature values, inconsistent feature 
scales, etc. It is, therefore, necessary to pre-process the data before applying a learning model. The datasets used, namely 
Cell2cell and Orange, have their individual characteristics. Initially, missing values from the dataset were handled by numeric 
and categorical feature vectors, respectively. Missing values from the dataset were inserted as column mean and mode for 
numeric and categorical feature vectors. This was done for the features which had not an overwhelming number of missing 
values. The features with more than 95% of missing values were removed from the dataset since it was assumed that they 
contribute little to the learning. Next, the categorical features were converted to numeric using the ‚Äòdummy variable‚Äô technique, 
where different categories are split to form their own features.  The value of 1 or 0 is assigned, depending  on whether the 

2 

 
 
 
 
 
 
 
instance had the category as its original feature or not, respectively. Once the entire dataset was converted into a numeric 
format, we normalized it on a scale of 0 to 1 in order to scale the features uniformly.  

2.2  Conversion of 1D Features to Images 

A major aspect of this research work was to explore the possibility of converting the raw feature vector-based dataset to image 
form, with the belief that enhanced performance can be achieved this way using CNNs. The assumption behind this hypothesis 
is that  CNNs are known to  perform well on image data. Every single instance present in the dataset was converted to an 
individual representative image. To achieve this, the  one-dimensional feature vector of each instance was converted into a 
two-dimensional  matrix  form.  This  conversion  was  performed  on  the  hit-and-trial  basis,  and  it  was  found  that  the  images 
obtained using the method of interpolation works well as far as the accuracy is concerned. Specifically, we constructed a 2-
dimensional matrix of normalized pixels, where each row is for each day and each column is for each type of behavior.  This 
conversion  method was used to convert all instances to image format and in turn, (both in case training and testing). It is 
hypothesized  that  one can exploit  the  pixel-based  differences  between  churner  and non-churner  instance  images  when  it 
comes to CNNs learning. 
2.3  Development of Individual Prediction Spaces using Transfer Learning 

Training a CNN on a new dataset from scratch can be a very time consuming and resource intensive practice. The concept 
of Transfer Learning offers an alternative route to avoid learning from scratch. This concept introduces that instead of training 
from  scratch,  CNNs  already  trained  on  massive  dataset  like  ImageNet  should  be  fine-tuned  (i.e.  the  weight  updatation  is 
performed) on the target dataset.  

In order to obtain the decision spaces, three CNN models were used: AlexNet, Inception-ResNet-V2, and a custom 
6-layer neural network model. These CNNs were pre-trained on ImageNet dataset and Transfer Learning was used to fine-
tune them on the Orange and Cell2cell datasets. 
2.3.1  AlexNet 
AlexNet is a famous CNN architecture proposed by Alex Krizhevsky for ImageNet classification [23-25]. The architecture is 
composed of eight layers: first five are convolution layers with 4-pixel stride and the next three are fully-connected and lead 
to the final layer, i.e. softmax. Softmax layer provides the probabilistic distributed decision over 1000 classes. The multinomial 
logistic regression-based objective is maximized by the network.  The layers have kernels smaller in size but an increased 
number of channels, such as 5x5x48 and 3x3x256. Fully-connected layers each have 4096 neurons and a final 1000-way 
softmax layer is employed at the end. The network uses data augmentation and dropout to reduce the effects of overfitting. 

2.3.2 

Inception-ResNet-V2 

Inception family is a class of deep CNN architectures developed by the Google team. Inception family architectures usually 
need more computational power as compared with AlexNet because of their more complex architecture. The concept behind 
Inception-ResNet-v2 [26] was to train deeper neural network architectures using skip connections [27, 28] and to automate 
the filter size selection. A single block in Inception ResNet architecture uses a filter bank with multiple sizes of filters (5x5, 3x3, 
1x1) On the ILSVRC image classification benchmark, this architecture demonstrated an accuracy of 95.3%, which was among 
the top 5 of its categories [29]. 

2.3.3  Custom-CNN Architecture 

For  comparison  purposes,  a  custom  deep  neural  network  [30,  31]  architecture  was  also  trained  after  hit-and-trial 
experimentation with different architectural parameters. The architecture was built in the layer patterns of conv-relu-pool-fc-
softmax. It takes 32x32 input image, has a 3x3x6 filter with no padding at first Convolution layer and then 2x3 pooling, 5x5x10 
conv filter in the second layer, 2x2 max-pooling, and the final classification layer consists of a softmax with 2 neurons. 
              These  custom-CNNs  were  originally  pre-trained  on  ImageNet  data  and  then  were  fine-tuned  on  the  Orange  and 
Cell2cell  datasets  using  Transfer  Learning.  Once  fine-tuned,  the  models  were  tested  on  the  test  set  of  images  and  their 
predictions were collected. These predictions were then appended to the original features to obtain extended feature space. 
The extended feature space was passed on for training and testing of the high-level GP-AdaBoost ensemble classifier. 

2.3.4  Exploitation of Individual Prediction Spaces using GP-AdaBoost Ensemble Based Meta Classification 

GP has been reported to be quite successful in solving problems related to classification, clustering, and regression. In this 
work,  GP  concentrates  on  developing  effective  one-class  classifiers  to  come  up  with  an  improved  prediction  space.  The 
Adaboost,  on  the  other  hand,  tries  to  provide  stringent  sample  space  to  the  one-class  classifiers  by  intensifying  the  hard 
examples.  Finally,  for  an  overall  binary  decision,  a  maximum  weighted  strategy  is  applied.  GP-AdaBoost  ensemble  [32] 
incorporates the idea of boosting in evolving different GP programs. A classifier in a subsequent step is evolved for each class 
to identify the ‚Äúhard samples‚Äù that were incorrectly classified by the preceding classifier. The method in an earlier stage, divides 
the data into two sets, i.e. training and testing.  

In Figure 2, a number of GP programs are evolved for a given fixed Elite size in each class, and then, weighted sum 
is computed for each class. A class with the highest weighted sum is then predicted by the GP-Adaboost. Final prediction is 
made using the highest weighted sum output in each class.

3 

 
 
         
 
 
The GP programs are basically one-class classifiers and therefore, evolved separately for both churners and non-
churners.  In this work, the Genetic Programming-AdaBoost [33] ensemble is used as a high-level meta-classifier on 
top of the base results achieved by the CNNs and Transfer Learning.  The proposed method extracts discriminative 
features  using  deep  autoencoder  and  concatenates  it  to  the  original  feature  vector  to  form  images.  The  extended 
feature vector uses the original feature vector concatenated with the base CNNs predicting space as input and outputs 
the final predictions.  

3.  Performance Evaluation 

Generally, results of churn prediction systems are compared by standard measures like prediction accuracy. But in 
telecommunication  datasets,  accuracy  is  not  sufficient  to  achieve  the  true  picture  of  results  due  to  an  imbalanced 
distribution of the data. Therefore, the Area Under Curve (AUC) of Receiver Operating  Characteristics (ROC) curve 
along with prediction accuracy is used as a performance measure to estimate the true functionality of the predictor. 
ROC [34] curve are generated on the basis of true positive rate (Sensitivity) and false negative rate (1-Specificity). Let 
TP denotes True Positives, FP False Positives, TN True Negatives, and FN False Negatives. The positive prediction 
(TP+FN) shows the true churners rate and negatives prediction (TN+FP) shows non-churners rate. 
3.1  Prediction Accuracy 
Prediction  accuracy  (Equation  1)  gives  a  measure  of  the  correct  predictions  against  the  total  number  of  cases 
evaluated. The metric of prediction accuracy has been considered for the Transfer Learning task since the number of 
correct predictions from the neural network models would affect the overall ensemble classifier‚Äôs result. The prediction 
accuracy is defined as below: 

  ùëÉùëüùëíùëëùëñùëêùë°ùëñùëúùëõ ùê¥ùëêùëêùë¢ùëüùëéùëêùë¶ =

TP+TN

ùëáùëÉ+ùëáùëÅ+ùêπùëÉ+ùêπùëÅ

           (1)                                                                        

          (a)                                                                     (b) 

                                                                (c) 

 Figure 2: Block diagram of GP-AdaBoost approach: (a) Obtaining preprocessed datasets, (b) Developing the final expression for both 
Churners and non-Churners using GP-AdaBoost, and (c) Final churn prediction. 

3.2  ROC Curve 
The  ROC  curve  and  other  related  evaluation  measurements  are  pertinent  in  two  ways.  Firstly,  it  provides  a 
comparatively unbiased diagnostic ability of a binary classifier as compared to prediction accuracy, which can easily 
become biased in the case of imbalanced datasets. Secondly, it is helpful while comparing the performance of different 
methods, reported in the literature. Equations (2), and (3) show Sensitivity and Specificity parameters, while AUC is 
calculated as in (Equation 4) [35].                          

ùëÜùëíùëõùë†ùëñùë°ùëñùë£ùëñùë°ùë¶ =

ùëÜùëùùëíùëêùëñùëìùëñùëêùëñùë°ùë¶ =

ùëáùëÉ

ùëáùëÉ+ùêπùëÅ
ùëáùëÅ

ùëáùëÅ+ùêπùëÉ

                                                                                                                 (2)                                                                                                                       

                                                                                                                   (3)                                                                    

4 

 
 
 
     
 
 
                                        
 
  
 
 
 
1
        ùê¥ùëàùê∂ = ‚à´
0

ùëáùëÉ

ùëáùëÉ+ùêπùëÅ

ùëë

ùêπùëÉ

ùëáùëÅ+ùêπùëÉ

          (4)       

4.  Results and Discussion 

Churn prediction system is assessed on the basis of its ability to correctly identify the churners in telecom data. At 
present, most of the reported churn prediction systems are not capable of achieving accurate predictions due to the 
serious  complications  in  telecommunication  data.  The  correct  prediction  of  churners  helps  the  telecom  industry  in 
avoiding a heavy loss. The proposed ‚ÄúTL-DeepE‚Äù method for churn prediction shows improved results for the standard 
telecom datasets.  

4.1  Datasets 

Two standard telecom datasets namely; Orange and Cell2cell are used for experimentation and analysis. The former 
one, i.e. Orange dataset from Telecom, UK, is made available to the researchers as part of KDD-Cup 2009 competition. 
The Cell2cell dataset, on the other hand, is from Duke University‚Äôs Center for Customer Relationship Management. 
The Orange dataset has eighteen features with missing values and five features have just a single value, while Cell2cell 
dataset has no missing values. The symbolic feature values existing in telecommunication dataset are converted into 
numerical forms. Attributes of these telecommunication datasets are described in Table 1. 

Dataset 

Source 

Features 

Samples 

Table 1: Characteristics of Telecom Datasets 
Cell2cell 

Orange 

Duke University 

KDD Cup 2009 

77 

40000 

230 

50000 

Features title 

Well- defined 

Undefined 

Categorical variables 

VisioBehavior 

Positive samples 

Negative samples 
Missing features value 

1 

Fair 

20000 

20000 
No 

34 

Unfair 

46328 

3672 
Yes 

4.2  Dataset Splits and Overall Working of the Prediction System 

As regards the splitting of the data, we have used the hold-out method. However, in the meta-classification phase, we 
have used 10-fold cross-validation technique. The distribution of dataset must be taken in the account before the whole 
process starts because some portion of the dataset will be used for training and the rest in evaluating the GP-AdaBoost 
ensemble.  Therefore,  both  the  datasets  are divided  into  two  subsets  A  and  B  after applying  proper  pre-processing 
techniques. Neural network models are trained on the subset A, which encompasses 60% of the entire dataset. These 
CNNs  are  used  as  the  trained  base  learner  in  the  GP-AdaBoost  ensemble.  Once  trained,  the  model  predicts  the 
outcome of their learning on the subset B, i.e. encompassing 40% of the total data. Once their test predictions on the 
subset B are completed, then these predictions are appended to the feature vector of subset B to form the extended 
feature vector and saved in subset B‚Äô. Finally, the performance of the GP-AdaBoost on subset B' is evaluated by using 
10-fold cross-validation. The training and testing of the CNN models and the GP-AdaBoost classifier is shown in Figure 
3. The final prediction is then provided by the GP-AdaBoost classifier. 

4.3  Churn Prediction Using Individual CNN Classifiers and Transfer Learning 

The performance of the base prediction models (pre-trained CNNs models that are fine-tuned) for Cell2cell and Orange 
datasets  are  summarized  in  Table  2.  Inception-ResNet-V2  neural  networks  are  pre-trained  and  then  fine-tuned  on 
telecom datasets. Transfer learning increases the prediction accuracies on Orange and Cell2cell to 63.57 and 68.01 
%, respectively. It can be observed that the performance of the base models is not that high. 

4.4  Ensemble‚Äôs Performance without Transfer Learning 

It was hypothesized that Transfer Learning can improve the overall churn prediction and classification performance. In 
order to validate this idea, it was essential to evaluate the performance of TL-DeepE without Transfer Learning. In this 
regard, Table 3 shows the GP-AdaBoost ensemble performance without Transfer Learning on Orange and Cell2cell  

5 

 
 
     
 
 
        
               
 
 
      
Figure 3: (a) Individual CNN Based Training, (b) Individual CNN Based Testing, (c) 10-fold GP-AdaBoost Based Training and Testing. 

(a) 

(b) 

  (c) 

                                        Table 2: Transfer Learning Based Results of Neural Network Models on Cell2cell and Orange Datasets. 

Runs 

Run 1 
Run 2 
Run 3 
Run 4 
Run 5 
Run 6 
Run 7 
Run 8 
Run 9 
Run 10 
Average 

Prediction Accuracy (%) 

Cell2cell 
Incep-RN-V2 
63.83 
63.65 
63.44 
63.25 
63.48 
62.98 
63.17 
64.27 
63.55 
64.05 
63.57 

Custom-NN 
62.92 
61.08 
62.57 
62.60 
62.41 
61.35 
61.41 
61.20 
62.40 
62.37 
62.03 

AlexNet 
65.48 
66.19 
64.40 
64.89 
66.05 
65.86 
66.12 
65.29 
65.75 
66.10 
65.61 

Orange 
Incep-RN-V2 
68.21 
67.74 
68.13 
67.78 
67.61 
67.94 
68.06 
68.49 
67.97 
68.24 
68.01 

Custom-NN 
63.16 
63.04 
63.27 
62.95 
63.18 
63.07 
63.20 
62.84 
62.98 
62.80 
63.05 

AlexNet 
59.97 
59.45 
60.77 
60.53 
59.80 
60.96 
61.36 
60.20 
61.37 
61.74 
60.62 

datasets. It is observed that the performance of the GP-AdaBoost ensemble without Transfer learning is not satisfactory 
and even lower than some of the individual base learners. 

4.5  Ensemble‚Äôs Performance using Transfer Learning 

Table  4  summarizes  the  results  obtained  by  the  proposed  TL-DeepE  using  Transfer  Learning.  In  this  case,  the 
performance  evaluation  of  the  high-level  meta-classifier  is  performed  on  two  standard  telecommunication  datasets 
using 10-fold cross-validation. The table depicts an average performance for 10 independent runs of the algorithm. 
Furthermore,  AUC  for  Orange  and  Cell2cell  datasets  are  0.83  and  0.74,  is  shown  in  Figure  4(a).  In  Figure  4(b), 
comparisons are provided for GP-AdaBoost ensemble (using both with and without Transfer Learning), on Orange and 
Cell2cell  datasets.  It is observed  that the performance  of  the  GP-AdaBoost ensemble in  combination  with  Transfer 
learning is boosted and becomes higher than all of the deep individual base learners. 
4.6  Comparative Analysis 
Performance  of  the  proposed  churn  prediction  technique,  i.e.  TL-DeepE,  is compared  with  previous  techniques,  on 
Cell2cell and Orange telecom datasets. Initially, the performance of the churn predictors;  AlexNet, ResNet-V2, Custom-
CNN,  and  then  GP-AdaBoost  ensemble classifier  without  Transfer  Learning  is evaluated on  the  telecommunication 
dataset.    The  poor  performance  of  Alex  Net,  ResNet-V2,  Custom-CNN  neural  networks  is  because  of  the  already 
performed pre-tuning and little fine-tuning on telecom datasets.  Hence an effective prediction system is necessary in 
order to avoid customer churn related losses. 

The proposed system  ‚ÄúTL-DeepE‚Äù in (Figure 4 (b)) shows that the  integration of information from Transfer 
Learning  with  GP-Adaboost  ensemble  improves  churn  prediction  performance  over  the  complex  datasets  and 
surpasses the performance of all the earlier reported systems. The proposed system achieved highest churn prediction 
accuracy, on Orange and Cell2cell telecommunication datasets, as 75.4% and 68.2%, while the area under the curve 
as 0.83 and 0.74, respectively. This shows its strength to perform on complicated nature of the telecom data. 

6 

 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
Runs 

Run 1 
Run 2 
Run 3 
Run 4 
Run 5 
Run 6 
Run 7 
Run 8 
Run 9 
Run 10 
Average 

     Table 3: GP-AdaBoost Ensemble Performance on Cell2Cell and Orange Datasets without Transfer Learning. 

Cell2Cell 

Orange 

Prediction Accuracy (%) 
60.30 
60.62 
61.22 
61.00 
60.33 
61.04 
60.90 
60.57 
61.18 
60.43 
60.76 

AUC 
0.64 
0.65 
0.64 
0.64 
0.62 
0.63 
0.63 
0.63 
0.64 
0.63 
0.64 

Prediction Accuracy (%) 
62.16 
61.47 
62.81 
62.04 
63.19 
63.13 
62.48 
62.94 
62.03 
62.80 
62.50 

AUC 
0.69 
0.65 
0.67 
0.66 
0.67 
0.67 
0.66 
0.66 
0.66 
0.64 
0.66 

                                         Table 4: Prediction Performance of the proposed TL-DeepE on Cell2cell and Orange Datasets. 

Runs 

Run 1 
Run 2 
Run 3 
Run 4 
Run 5 
Run 6 
Run 7 
Run 8 
Run 9 
Run 10 
Average 

Cell2Cell 

Orange 

Prediction Accuracy (%) 
68.07 
67.86 
69.30 
68.65 
68.42 
67.38 
67.74 
68.40 
67.91 
67.84 
68.16 

AUC 
0.74 
0.74 
0.75 
0.74 
0.75 
0.73 
0.73 
0.74 
0.73 
0.73 
0.74 

Prediction Accuracy (%) 
76.13 
75.90 
75.33 
75.51 
74.94 
75.11 
75.77 
75.50 
74.11 
75.51 
75.38 

AUC 
0.83 
0.83 
0.83 
0.83 
0.82 
0.83 
0.83 
0.83 
0.83 
0.83 
0.83 

Churn Prediction Accuracy 

TL-DeepE

DeepE without TL

AlexNet

Incep-RN-V2

Custom-NN

80

75

70

%

65

60

55

50

y
c
a
r
u
c
c
A
e
g
a
r
e
v
A

Cell2cell

Orange

Telecom Datasets

(a)                                                                                           (b) 

Figure 4. (a) ROC curve of GP- AdaBoost on Cell2cell and Orange dataset and (b) Comparison of Churn predictors. 

7 

 
 
 
 
 
  
 
 
 
 
 
 
 
5.  Conclusion 

A  novel TL-DeepE technique is proposed to  predict  potential  churners,  which  is  very  important for  the  competitive 
telecom industry. The size and dimensionality of the telecom data are high and require a large computational power for 
churn 
and GP-
AdaBoost ensemble with the exploitation of Transfer Learning concepts. The individual prediction of the base original 
feature  vectors  of  the  dataset  to  form  extended  feature  vectors.  Finally,  using  a  10-fold  cross-validation  technique, 
a GP-AdaBoost ensemble as meta-classifier is evaluated to obtain predictions. 

TL-DeepE  makes use 

of multiple  CNN 

prediction. The 

architectures 

proposed 

The proposed  TL-DeepE churn  prediction system  has  shown  its effectiveness in  predicting  churners using 
standard telecommunication datasets. TL-DeepE has demonstrated a churn prediction accuracy of  75.4% and 0.83 
AUC on Orange dataset. Its accuracy was 68.2% and AUC 0.74 on Cell2Cell dataset. It is observed that due to the 
features  and the  exploitation  of Deep 
challenging  nature  of 
Learning, Transfer  Learning,  and GP-AdaBoost meta-classification appears more  effective 
for  modeling  churn 
prediction  for  the  telecommunication  industry.  The  proposed  method can  be potentially  effective  in  addressing  the 
concerns and complications of the telecommunication industry. 

task, choice of  suitable 

the  churn  prediction 

Acknowledgment: 

This  research  was  supported by  HEC  NRPU  Research  Grant  (No.  20-3408/R&D/HEC/14/233)  Pakistan,  and  Basic 
Science  Research  Program  through  the  National  Research  Foundation  of  Korea  (NRF)  funded  by  the  Ministry  of 
Education (2017R1A2B2005065). 

8 

 
 
 
  
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
References: 

1. 

2. 

3. 

4. 

5. 

6. 

7. 

8. 

9. 

10. 

11. 

12. 

13. 

14. 

15. 

16. 

17. 

18. 

19. 

20. 

Kim,  M.-K.,  M.-C.  Park,  and  D.-H.  Jeong,  The  effects  of  customer  satisfaction  and  switching  barrier  on 
customer loyalty in Korean mobile telecommunication services. Telecommunications policy, 2004. 28(2): p. 
145-159. 

Verbeke, W., et al., New insights into churn prediction in the telecommunication sector: A profit-driven data 
mining approach. European Journal of Operational Research, 2012. 218(1): p. 211-229. 

Yabas,  U.  and  H.C.  Cankaya.  Churn  prediction  in  subscriber  management  for  mobile  and  wireless 
communications services. in Globecom Workshops (GC Wkshps), 2013 IEEE. 2013. IEEE. 

Ahmad, N., et al. Computer vision based room interior design. in Eighth International Conference on Machine 
Vision (ICMV 2015). 2015. International Society for Optics and Photonics. 

Reichheld, F. and C. Detrick, Loyalty: A prescription for cutting costs. Marketing Management, 2003. 12(5): 
p. 24-24. 

Khaleghi,  B.  What  makes  predicting  customer  churn  a  challenge.  2017;  Available 
https://medium.com/@b.khaleghi/what-makes-predicting-customer-churn-a-challenge-be195f35366e. 

from: 

Lessmann, S. and S. Vo√ü, A reference model for customer-centric data mining with support vector machines. 
European Journal of Operational Research, 2009. 199(2): p. 520-530. 

Suryadi,  K.  and  S.  Gumilang.  Actionable  decision  model  in  customer  churn  monitoring  based  on  support 
vector machines technique. in 9th Asia Pacific Industrial Engineering and Management Systems Conference, 
Bandung, Indonesia. 2008. 

Idris, A. and A. Khan, Churn prediction system for telecom using filter‚Äìwrapper and ensemble classification. 
The Computer Journal, 2016. 60(3): p. 410-430. 

2017; 
Raschka, 
https://rasbt.github.io/mlxtend/user_guide/classifier/StackingClassifier. 

Classifier. 

Stacking 

S. 

Available 

from 

De  Bock,  K.W.  and  D.  Van  den  Poel,  An  empirical  evaluation  of  rotation-based  ensemble  classifiers  for 
customer churn prediction. Expert Systems with Applications, 2011. 38(10): p. 12293-12301. 

Neslin, S., Cell2Cell: The churn game. Cell2Cell Case Notes. Hanover, NH: Tuck School of Business, Dartmoth 
College, 2002. 

Idris, A., A. Khan, and Y.S. Lee, Intelligent churn prediction in telecom: employing mRMR feature selection 
and RotBoost based ensemble classification. Applied intelligence, 2013. 39(3): p. 659-672. 

Xie,  Y.,  et  al.,  Customer  churn  prediction  using  improved  balanced  random  forests.  Expert  Systems  with 
Applications, 2009. 36(3): p. 5445-5449. 

Idris, A., A. Khan, and Y.S. Lee. Genetic programming and adaboosting based churn prediction for telecom. 
in Systems, Man, and Cybernetics (SMC), 2012 IEEE International Conference on. 2012. IEEE. 

Khan, A., et  al.,  A Recent Survey on the Applications of Genetic Programming in  Image Processing. arXiv 
preprint arXiv:1901.07387, 2019. 

Pan, S.J. and Q. Yang, A survey on transfer learning. IEEE Transactions on knowledge and data engineering, 
2010. 22(10): p. 1345-1359. 

Yosinski, J., et al. How transferable are featured in deep neural networks? in Advances in neural information 
processing systems. 2014. 

Qureshi, A.S. and A. Khan,  Adaptive Transfer Learning in  Deep Neural Networks: Wind Power Prediction 
using  Knowledge  Transfer  from  Region  to  Region  and  Between  Different  Task  Domains.  arXiv  preprint 
arXiv:1810.12611, 2018. 

Qureshi, A.S., et al., Wind power prediction using deep neural network based meta-regression and transfer 
learning. Applied Soft Computing, 2017. 58: p. 742-755. 

9 

 
 
 
21. 

22. 

23. 

24. 

25. 

26. 

27. 

28. 

29. 

30. 

31. 

32. 

33. 

34. 

35. 

Guadarrama,  N.S.a.S.  Tensor  Flow-Slim  Image  Classification  Model  Library.  2016;  Available  from 
https://github.com/tensorflow/models/tree/master/research/slim. 

Koza, J.R. How Genetic Programming Works. 2007; Available from http://www.genetic-programming.org. 

Russakovsky, O., et al., Imagenet large scale visual recognition challenge. International Journal of Computer 
Vision, 2015. 115(3): p. 211-252. 

Krizhevsky,  A.,  I.  Sutskever,  and  G.E.  Hinton.  Imagenet  classification  with  deep  convolutional  neural 
networks. in Advances in neural information processing systems. 2012. 

Khan, A., et al., A Survey of the Recent Architectures of Deep Convolutional Neural Networks. arXiv preprint 
arXiv:1901.06032, 2019. 

Sin, K. Transfer Learning in Tensorflow using a Pre-trained Inception-Resnet-v2 Model. 2017; Available from 
https://kwotsin.github.io/tech/2017/02/11/transfer-learning.html. 

Szegedy, C., et al. Inception-v4, inception-resnet and the impact of residual connections on learning. in AAAI. 
2017. 

He, K., et al. Deep residual learning for image recognition. in Proceedings of the IEEE conference on computer 
vision and pattern recognition. 2016. 

S.  Winner 

Tsang, 
https://towardsdatascience.com/review-trimps-soushen-winner-in-ilsvrc-2016-image-classification-
dfbc423111dd. 

Classification). 

Available 

(Image 

ILSVRC 

2018; 

2016 

in 

from 

team, 

The 
http://deeplearning.net/tutorial/lenet.html. . 

Convolutional 

T.D. 

Neural 

Networks 

(LeNet). 

2015; 

Available 

from: 

Castrounis, A. Artificial Intelligence, Deep Learning, and Neural Networks, Explained. 2016; Available from 
http://www.kdnuggets.com/2016/10/artificial-intelligence-deep-learning-neural-networks-
explained.html. 

Freund, Y. and R.E. Schapire,  A Short  Introduction to Boosting": Journal of Japanese Society for Artificial 
Intelligence, 14 (5): 771-780. 1999. 

Khan,  A.,  A.  Sohail,  and  A.  Ali,  A  New  Channel  Boosted  Convolutional  Neural  Network  using  Transfer 
Learning. arXiv preprint arXiv:1804.08528, 2018. 

Zou, K.H., A.J. O‚ÄôMalley, and L. Mauri, Receiver-operating characteristic analysis for evaluating diagnostic 
tests and predictive models. Circulation, 2007. 115(5): p. 654-657. 

Burez, J. and D. Van den Poel, Handling class imbalance in customer churn prediction. Expert Systems with 
Applications, 2009. 36(3): p. 4626-4636. 

10 

 
 
 
 
 
                                                                                                                                                          
 
