Deep reinforcement learning driven inspection and maintenance planning under 
incomplete information and constraints 

C.P. Andriotis, K.G. Papakonstantinou 

Department of Civil & Environmental Engineering, The Pennsylvania State University, University Park, PA, USA 

Abstract 

Determination of inspection and maintenance policies for minimizing long-term risks and costs in deteriorating engineering environments constitutes 
a complex optimization problem. Major computational challenges include the (i) curse of dimensionality, due to exponential scaling of state/action set 
cardinalities with the number of components; (ii) curse of history, related to exponentially growing decision-trees with the number of decision-steps; 
(iii) presence of state uncertainties, induced by inherent environment stochasticity and variability of inspection/monitoring measurements; (iv) presence 
of constraints, pertaining to stochastic long-term limitations, due to resource scarcity and other infeasible/undesirable system responses.  In this work, 
these challenges are addressed within a joint framework of constrained Partially Observable Markov Decision Processes (POMDP) and multi-agent 
Deep  Reinforcement  Learning  (DRL).  POMDPs  optimally  tackle  (ii)-(iii),  combining  stochastic  dynamic  programming  with  Bayesian  inference 
principles.  Multi-agent DRL addresses (i), through deep function parametrizations and decentralized control  assumptions. Challenge (iv) is herein 
handled through proper state augmentation and Lagrangian relaxation, with emphasis on life-cycle risk-based constraints and budget limitations. The 
underlying algorithmic steps are provided, and the proposed framework is found to outperform well-established policy baselines and facilitate adept 
prescription of inspection and intervention actions, in cases where decisions must be made in the most resource- and risk-aware manner. 

Keywords: Inspection and maintenance planning; system risk and reliability; constrained stochastic optimization; partially observable Markov decision 
processes; deep reinforcement learning; decentralized multi-agent control  

1.  Introduction 

Optimal inspection and maintenance planning  delineates a class of 
important  engineering  decision-making  problems,  aimed  at 
supporting  sustainable  and  resilient  operation  of  systems  and 
networks  over  their  life-cycle.  Optimality  refers  to  minimizing 
various societal, environmental, and economic risks, along with other 
operational costs, as these emerge due to the combined consequences 
of the selected actions of the decision-maker and their effects on the 
future  exogenous  deterioration  of  the  environment.  Within  this 
context, the goal of the decision-maker is to determine an appropriate 
policy, i.e. an optimal rule of sequential decisions over a presumed 
time frame, which is able to aptly map states and times to intervention 
and observation actions [1, 2].  

Literature  indicates  several  approaches  to  solving  this  problem, 
from  threshold-based  nonlinear  and  mixed-integer  programming 
formulations (e.g. in [3, 4, 5, 6]), to analysis of decision trees (e.g. in 
[7, 8, 9, 10]), and from renewal theory (e.g. in [11, 12, 13, 14]), to 
stochastic optimal control (e.g. in [15, 16, 17, 18]). These approaches 
are also applicable to infrastructure problems beyond inspection and 
maintenance planning, such as post-disaster recovery, e.g. in [19, 20, 
21].  Respectively,  admissible  solution  strategies  to  the  above 
approaches  span  from  exhaustive  policy  enumeration,  and  genetic 
algorithms, to gradient-based schemes, and dynamic programming. 
Besides  formulations  that  leverage  dynamic  programming  and 
stochastic  optimal  control  concepts,  a  common  characteristic 
underlying traditional inspection and maintenance planning methods 
is that the decision-making problem, despite its inherent  sequential 
and  dynamic  nature,  is  articulated  by  means  of  static  optimization 
formulations. As a result, many otherwise practical approaches tend 
to  be  more  susceptible  to  optimality  limitations,  especially  in 

problems with high-dimensional spaces and long decision horizons, 
challenges  also  known  as  the  curse  of  dimensionality  and  curse of 
history,  respectively  [22,  23].  Moreover,  many  solution  techniques 
often  lack  cohesive  and  generalizable  mathematical  capabilities 
regarding  the  consistent  integration  of  stochastic  environments 
and/or uncertain observation outcomes in the optimization process, 
as well as the incorporation of stochastic or deterministic constraints 
that need to be satisfied over multiple time steps or even the entire 
operating life of the system.   

To address the above issues, this work follows a stochastic optimal 
control approach, casting the optimization problem within the joint 
context  of  constrained  Partially  Observable  Markov  Decision 
Processes (POMDPs) and multi-agent Deep Reinforcement Learning 
(DRL). POMDPs are able to alleviate the curse of history as a result 
of  their  dynamic  programming  principles,  and  to  facilitate  optimal 
reasoning in the presence of real-time noisy observations [24]. Their 
efficiency  in  inspection  and  maintenance  planning  has  been 
thoroughly studied and exemplified in [25, 26, 27, 28], among others.  
Within the same class of applications, in the confluence of DRL and 
point-based POMDPs, the Deep Centralized Multi-agent Actor Critic 
(DCMAC) approach has been recently developed in [29, 30], an off-
policy  algorithm  with  experience  replay,  belonging  in  the  general 
family  of  actor-critic  approaches  [31,  32].  DCMAC  leverages  the 
the 
concept  of  belief-state  MDPs,  a  fundamental 
development  of  point-based  POMDP  algorithms,  thus  directly 
operating on  the  posterior probabilities of  system  states  given  past 
actions and observations [33]. In DCMAC, individual control units 
are  centralized  in  terms  of  global  state  information  and  sharing  of 
policy  network  parameters,  nonetheless,  they  are  decentralized  in 
terms  of  policy  outputs.  Hence,  based  on  classic  Markov  decision 
processes    formalism,  DCMAC  provides  Decentralized  POMDP 

idea  for 

1 

 
 
 
 
 
 
C. P. Andriotis, K. G. Papakonstantinou / Deep reinforcement learning driven inspection and maintenance planning under incomplete information and constraints 

(Dec-POMDP)  solutions  [34,  35],  for  a  setting  where  the  agents 
representing the various control units have access to the entire state 
distribution of the system, however, having the autonomy to make 
their own choices without being aware of each other’s actions. DRL 
is  extremely  efficient  in  tackling  the  curse  of  dimensionality 
stemming from high-dimensional and/or combinatoric state spaces, 
whereas  the  computational  hurdle  of  exponential  scaling  of  the 
number  of  actions  with  the  number  of  components,  is  seamlessly 
handled by the decentralized multi-agent formulation of the problem, 
given that decentralization enables linear scaling. 

Building upon the above described DRL concepts in this work, a 
modified architecture compared to the original DCMAC approach is 
implemented for the actor. We consider a sparser parametrization of 
the  actor,  without  parameter  sharing,  i.e.  each  agent  has  its  own 
this  architecture  Deep 
individual  policy  network.  We  call 
Decentralized  Multi-agent  Actor  Critic 
(DDMAC).  Similar 
approaches  exist  for  various  cooperative/competitive  multi-agent 
robotic  and  gaming  control  tasks  [36,  37].    Thorough  reviews  on 
state-of-the-art  methods  and  applications  can  be  also  found in  [38, 
39].  Despite  the  architectural  differences  with  DCMAC,  DDMAC 
solves the same Dec-POMDP problem, eliminating, however, inter-
agent interactions in the hidden layers for the sake of computational 
efficacy. Based on this numerical approach, this paper is particularly 
focused  on  investigating  the  effects  of  incorporating  resource 
constraints  and other  limitations,  especially  in  the  forms  of  budget 
and  life-cycle  risk  constraints.  Depending  on  the  nature  of  the 
modeled limitations, the constraints can be addressed through either 
state augmentation or primal-dual optimization approaches based on 
the Lagrangian function of the problem.   

Constrained  static  optimization  formulations  for  operation  and 
maintenance  policies  exist  in  the  literature,  e.g.  in  [3,  14,  40,  41], 
mainly  reflecting  short-term  risk,  reliability-based,  and  budget-
related  considerations.  In  the  case  of  POMDPs,  the  optimization 
problem  now  falls  in  the  category  of  constrained  POMDPs. 
Constrained  Markov  decision  processes  have  been  given  model-
based solutions with the aid of linear programming formulations in 
[42,  43].  Exact  POMDP  alpha-vector  value  interation  can  be 
extended  to  constrained  problems  as  well,  inheriting, however,  the 
PSPACE  complexity  of 
[44]. 
Unconstrained  point-based  POMDPs  algorithms,  which  are  well-
suited for inspection and maintenance planning of systems with up to 
thousands of states and hundreds of actions and observations [27, 18], 
have  also  been  extended  to  constrained  problems  [45].  In  multi-
component  systems,  under  the  assumption  of  component-wise 
independent  cost  functions,  states,  and  actions,  [46]  derives 
constrained  POMDP  solutions  through  a  series  of  unconstrained 
solutions  controlled  by  a  linear  master  program.  Overall,  and 
notwithstanding  their  principled  mathematical  descriptions,  the 
above  value 
linear  or  nonlinear  programming 
formulations are fundamentally hard to extend to high-dimensional 
systems that are of interest in this work.  

the  unconstrained 

iteration  and 

solution 

In DRL, constraints typically refer to either the parameters of the 
approximated functions, or the cumulative returns related to auxiliary 
functions  of  interest [47,  48, 49].  The  former  methods  restrain  the 
iterate increment of the policy parameter updates to be within a trust 
region of the Kullback-Leibler divergence between the new and the 
old policy, thus preventing abrupt policy changes and, consequently, 
training instabilities. In such cases, optimization is typically based on 

2 

surrogates of the objective and constraint functions [47]. The latter 
methods typically aim to protect the agent from unsafe or otherwise 
undesirable states and choices during training or policy deployment. 
To  this  end,  the  objective  is optimized  with  the  aid of  primal-dual 
formulations,  either  through  trust  region  concepts,  or  Lagrangian 
relaxation, or domain-based manual penalization [48, 50, 51]. Safe 
RL  formulations similarly integrate risk and policy  variance in the 
constraint  functions  of  the  problem,  or  directly  intervene  in 
exploration to guide training [52, 53]. Such “safety” constraints can 
for example pertain to the probability of failure over multiple steps 
and, as such, they reflect soft constraints, meaning that they only need 
to be satisfied in a probabilistic or expected sense. The satisfaction of 
hard constraints, such as budget constraints, are easier to account for 
in  the  optimization  process  through  state  augmentation.  Such 
constraints tend to be relevant for other resource limitations as well, 
e.g. in cases of limited availability of operating crews, inspectors, etc. 
In this work, we consider and study both types of constraints. 

In  summarizing,  in  this  paper  we  consider  and  optimize  DRL-
driven  non-periodic  inspection  and  maintenance  policies  in  the 
presence of resource limitations and risk-related constraints. First, the 
preliminaries  of  the  POMDP  formulation 
in  inspection  and 
maintenance planning are elaborated, with insights in the problem-
specific  modeling  requirements.  State  updating  equations  and 
inspection,  maintenance,  shutdown,  and  risk  cost  definitions  are 
presented. It is studied and discussed how the selected actions affect 
the  above  costs,  and  which  the  inherent  mechanisms  that  drive 
observational  strategies  in  POMDPs  are.  Theoretical  analysis 
pertaining  to  perpetual  and  instantaneous  risk  definitions  is 
presented,  along  with  their  relation  to  classical  definitions.  The 
optimization  problem  is  cast  within  the  context  of  decentralized 
multi-agent DRL control, where agents operate directly on the belief 
space,  i.e.  the  space  of  posterior  system  statistics  based  on  past 
actions  and  observations.  The  developed  and  employed  DRL 
approach,  DDMAC,  is  an  off-policy  actor-critic  method  with 
experience  replay,  modifying  the  original  architecture  presented in 
[29].  The  relevant  algorithmic  steps  for  implementing  the  above 
described decentralized DRL framework are provided, based on state 
augmentation for hard constraints and Lagrangian relaxation for soft 
constraints.  Quantitative  investigation  is  conducted  based  on  a 
stochastically  deteriorating  multi-component  system.  Numerical 
experiments  include  evaluation  of  different  baseline  policies,  and 
different budget and risk constraint scenarios. The resulting evolution 
of  various  system  metrics  pertaining  to  risk,  reliability,  inspection, 
and 
is 
parametrically studied and discussed based on the learned policies.  

intervention  choices  over 

the  system  operating 

life 

2.  POMDPs in inspection and maintenance planning 

2.1. The optimization problem  

The goal of the decision-maker (agent) in a life-cycle inspection and 
maintenance optimization problem is to determine an optimal policy 
π  =  π* that  minimizes  the  total  cumulative  future  operational  costs 
and risks in expectation: 

(1) 

0:0:0:,,0:0:1000*argmi,n~,~TCTTTtsoatttttcaoasb0argmin()cVb 
  
  
C. P. Andriotis, K. G. Papakonstantinou / Deep reinforcement learning driven inspection and maintenance planning under incomplete information and constraints 

2.2. Mapping posterior state distributions to actions 

c0 

a 
s0 

c1 

a 
s1 

c2 

a 
s2 

cT 

a 
sT-1 

s0 

s1 

s2 

o0 

o1 

o2 

sT 

oT 

a0 

a1 

a2 

aT-1 

 =s to state st+1

In a POMDP environment, transition from state st
 =s' 
is Markovian. Detaching the effect of the maintenance action from 
the environment transition (natural deterioration), we can define an 
a =sa ϵ S. This state succeeds s, with probability 
intermediate state, st
Pr(sa|s,a),  and 
immediately  after 
maintenance and before the environment transition. This distinction 
is important to help us better define and quantify the risk in the next 
section.  State  s'  succeeds  sa  with  probability  Pr(s'|sa,a),  after  the 
environment transition, i.e. s' = sa,e. Owing to the Markovian property, 
given  a  pair  (s,a),  the  probability  distribution  of  s'  can  be  fully 
defined, regardless of the prior history of actions and states as: 

the  system  state 

reflects 

Fig. 1. POMDP diagram in time, including intermediate states occurring after 
actions and before environment transitions. 

Similarly, the cost at a certain time step can be expressed as: 

(2) 

(3) 

 ϵ S to state st+1

where ct = c(st,at,st+1) is the cost incurred at time t by taking action at
 ϵ S; ot   ϵ   Ω is an 
ϵ A, and transitioning from state  st
observation outcome; γ ϵ [0,1] is the discount factor translating future 
costs to current value; b0 is an initial distribution over states (or initial 
belief); Vπ is the value function, which expresses the total discounted 
cost given a state or a belief under policy π; and T is the length of the 
planning horizon. Planning horizon T can be either finite or infinite. 
A  finite horizon problem can be solved as an infinite one, through 
proper formulation of the problem, i.e. through augmenting the state 
space with time, and considering an absorbing state at the final time 
step [54].  

Policy  π  is  a  rule  according  to  which  actions  are  taken  by  the 
decision-maker at different time steps, and it can be, at best, a map 
from histories of actions and observations to actions, π: At-1× Ω 
t
 →A. 
The  policy  function  belongs  to  a  space,  π ϵ Πc,  which  contains  all 
possible policies that are admissible under the existing constraints of 
the  problem.  Πc  is  a  subset  of  Π,  which  is  the  policy  space  of  the 
unconstrained  problem.  From  the  mapping  a  policy  function 
conducts, it can be observed that the number of possible policies can 
easily  become  immense,  even  in  problems  with  small  planning 
horizons.  Also  known  as  the  curse  of  history,  this  problem  is 
optimally  tackled  by  dynamic  programming  and  POMDPs  as 
explained in detail in the next section. Another approach to attack this 
complexity, however, often at the expense of solution efficiency, is 
to  exploit  problem-specific  characteristics  and  employ  simplified 
assumptions,  including  approaches  that  impose  action  periodicity, 
policy  uniformity  among  components,  component  prioritization, 
ranking, or clustering [12, 55, 56, 57, 58]. Particularly in inspection 
planning, periodic inspection visits or non-periodic inspections that 
exploit similarity  and/or prioritization of components is typical  for 
deteriorating structural systems [10, 57].  

Policy π can also be stochastic, in which case it is a mapping to a 
probability distribution over actions, i.e. π: At-1× Ω 
t
 →P(A). It can be 
shown under loose regularity conditions about the cost function that 
the optimal policy in a Markov decision process is deterministic [59]. 
However, in general and especially in the presence of constraints, the 
optimal policy is more broadly described by functions accounting for 
stochastic mappings [42]. 

State  augmentation  can  be  applied  if  higher  order  temporal 
dependencies exist regarding the history of states and/or actions prior 
to t, or the environment is characterized by non-stationarity [54, 25]. 
In POMDPs, at each time step, states are hidden to the agent, and are 
only perceivable through the noisy observation ot=o ϵ  Ω. Observation 
o depends on the state of the system and the respective action at the 
current  step,  and  is  defined  by  probability  Pr(o|  s,  a).  The  entire 
process described above is depicted in the network of Fig. 1. 

As a result of the structure of POMDPs, optimal policy π* can be 
defined, without any loss of information, as a function of belief bt=b 
ϵ  B: S→P(S),  which  is  a  sufficient  statistic  of  the  entire  history  of 
previous actions and observations, up to time t. Belief b is thus the 
posterior  probability  distribution  over  states,  given  the  previous 
belief, and the current transition, action and observation. Hence, the 
belief at time t+1, bt+1=b'=ba,e,o, is easily computed though Bayesian 
updating as: 

(4) 

where probabilities b(s), for all s ϵ S, form the |S|-dimensional belief 
vector  b.  The  denominator  of  Eq.  (4),  Pr(o'|b,a),  is  the  standard 
normalizing constant: 

(5) 

Similarly to sa, belief ba in Eqs. (4) and (5) is the intermediate belief, 
right  after  the  maintenance  action  and  before  the  environment 
transition and observation, defined as: 

(6) 

3 

Pr'|,Pr('|,)Pr(|,)aaasSssassassa,,'Pr|,,,,'aaasScsasssacsass,,,'''Pr'|',,Pr'|','Pr'|,aeoaebsbssoaosabsoabbPr'|',Pr'|,Pr'|,aaaasSosassabsoab'Pr'|,Pr'|',Pr'|,aaaasSsSoaosassabsbPr|,aaasSbsssabs 
 
 
 
 
  
  
  
 
  
  
  
C. P. Andriotis, K. G. Papakonstantinou / Deep reinforcement learning driven inspection and maintenance planning under incomplete information and constraints 

In the special case that the environment is fully observable, i.e. o= 
s, observation specifies exactly which one of the belief vector entries 
is 1, assigning 0 otherwise. This defines an MDP environment and, 
accordingly,  Pr(o'|b,a)  reduces  to  Pr(s'|b,a),  which  is  the  transition 
probability of MDPs given the current state distribution. Following 
this remark, it is apparent that Pr(o'|b,a) holds transition probability 
semantics for the belief space, B, hence a POMDP can be seen as a 
belief-MDP,  where  now,  however,  states  are  the  belief  vectors. 
Accordingly, the transition between beliefs is given as: 

rather as a result of the selected actions. Accounting for this as well, 
the total cost at each decision step can be generally expressed as: 

 (10) 

Using  Eq.  (10)  in Eq.  (9),  the  expected  inspection,  maintenance 

and shutdown costs, can be written as: 

(7) 

 (11) 

where δij is the Kronecker delta, i.e. δij=1 if i=j, 0 otherwise.  

This allows us to write the optimality equation, also known as the 

Bellman equation [22], in the belief space as: 

Although Eq. (11) provides a broad description of the cost function, 
it  is  often  appropriate  to  adopt  the  hypothesis  that  inspection  and 
maintenance  actions  affect  the  respective  costs  independently,  and 
are also independent of the system state (this hypothesis is stronger 
for inspections since certain maintenance actions may depend on the 
extent of damage in the system): 

(8) 

(12) 

where  V(b)=Vπ*(b)  is  the  optimal  value  function,  representing  the 
total life-cycle cost under the optimal policy π* given an initial belief 
b, H is the Bellman backup operator, Q is the optimal action-value 
function, and cb is the expected cost at belief b, defined as: 

(9) 

Operator H is a contraction with unique fixed point V(b). It has been 
shown that the POMDP cost value function described by the Bellman 
equation in Eq. (8) is piece-wise linear and concave (convex for the 
maximization  problem)  at  every  time  step,  composed  of  linear 
hyperplanes,  also  called  the  alpha-vectors  [60].  Each  alpha-vector 
corresponds to an inspection and maintenance action [26, 61]. 

Despite its analogies with MDPs, Eq. (8) is hard to solve exactly 
through  standard  MDP-based  approaches,  e.g.  through  value  or 
policy iteration. However, there are numerous efficient approximate 
solution procedures along the lines of  point-based algorithms [33]. 
Point-based algorithms sample a subset of the reachable belief space, 
starting from an initial root belief, thus making value iteration scale 
linearly with the cardinality of this subset. DRL is used for solving 
Eq.  (8)  in  this  work,  using  the  point-based  belief  MDP  concept 
combined  with  deep  function  approximations  and  actor-critic 
training [29].  

2.3. Risks and costs 

Cost at different time steps for a selected action can be decomposed 
into inspection cost, cI, maintenance cost, cM, and damage state cost, 
cD. In addition, it is often important for the decision-maker to account 
for  the  possibility  of  additional  losses  due  to  intentional  system 
shutdowns, cS, which may occur not as a consequence of damage, but 

where  aI  ϵ  AI is  the  selected  inspection  action  and  aM  ϵ  AM   is  the 
selected maintenance action. Under this distinctive consideration of 
actions, the total action can be defined as a ϵ A=AI×AM. We will refer 
here to no inspection and no maintenance actions as trivial inspection 
and trivial maintenance actions respectively. Trivial actions may also 
refer to routine maintenance and inspection actions, which are actions 
that are always taken at every time step, thus their costs do not affect 
the optimization process. Similarly to Eq. (12), it is also reasonable 
to assume in many problems of inspection and maintenance planning 
that scheduled shutdowns will be primarily triggered by maintenance 
actions only, namely: 

(13)  

Damage state cost cD translates various losses associated with the 
damage states of the system to cost units. These can be devised into 
two types of losses, which we will refer to as instantaneous losses 
and  perpetual  losses.  Instantaneous  losses  refer  to  costs  incurred 
upon arrival at a damage state and do not continue to be collected for 
as  long  as  the  system  sojourns  this  damage  state.  In  the  case  of  a 
failed  civil  engineering  structure,  for  instance,  such  costs  can  be 
related to capital-related losses, which occur at the time step at which 
the  structural  system  transitions  to  the  failure  state.  This  cost  is 
collected once over the operating life, unless the system is restored 
and  fails  again.  Perpetual  losses,  on  the  other  hand,  refer  to  costs 
collected for as long as the system sojourns a certain damage state, 
regardless  of  which  damage  state  it  transitioned  from.  In  the 
previously mentioned example of a failed civil engineering structure, 
such costs can be related to economic losses due to downtime, which 
are,  of  course,  not  instantaneous  but  accrue  over  time,  until  the 
system is restored to an operating status. Following this distinction, 
the damage cost component of Eq. (10) is written as: 

(14) 

4 

''Pr'|,Pr'|,oaoabxbxbbmin,aAVHVQabbb'minPr'|,baAocoaVbb,,',,,,'abbasssccacsassb'Pr|,Pr'|,,,,'aaaasSsSsSbsssassacsassmaintenan.costshutdown costinspectioncostdamagestatecost,,,,,',,'aaMSIDcsasscsacsacsacss,,,'',,,{,},'','bXsXXsSaebIsIIsSccsabscsaXMSccasbscas,,',,IbIIIMbMMMcsacacacsacaca,,SSMcsacsa','''aaperinstDDDsscsscsdcs 
  
  
  
  
 
  
  
  
  
  
C. P. Andriotis, K. G. Papakonstantinou / Deep reinforcement learning driven inspection and maintenance planning under incomplete information and constraints 

where [dij]i,j ϵ S is the adjacency matrix pertaining to damage states, as 
this  can  be  derived  by  state  connectivity  according  to  available 
actions. That is, if there is an action such that state j is an immediate 
successor  of  i,  then  dij=1.  For  i=j,  dij=0.  In  deteriorating 
environments, it commonly happens that states are ordered, that is, 
transitions  from  sa  to  s'  form  an  upper-triangular  transition  matrix, 
meaning that the system can only transition to a worse state, or at best 
remain at the same one, due to environment effects. In this case, the 
adjacency matrix will be strictly upper-triangular. 

As implied by Eq. (14), the cost of perpetual losses is a function of 
the next state, s', whereas the  part instantaneous losses  depends on 
the current state after the effect of the maintenance action, sa, and the 
next state.  The expected costs in Eq. (14), which is required to solve 
Eq. (8), with the aid of Eq. (9), give the step or interval risk as: 

Thus, overall, the problem of Eq. (1) consists in jointly minimizing 
the above life-cycle cumulative discounted costs. 

2.4. To observe or not? Value of information in POMDPs 

We can define the step-wise Value of Information (VoI) associated 
with a certain policy and a certain inspection action aI as [62]: 

(20) 

Observation  oe  ϵ  Ωe  describes  the  default  observation,  i.e.  an 
observation always available to the decision-maker, oI ϵ ΩI refers to 
the optional observation provided by the selected inspection action, 
and o ϵ Ω =Ωe ×Ω I is the total observation.  

Similarly, we can define the net step-wise VoI under a policy as: 

(15) 

(21) 

Using Eq. (15), risk is defined as the expected cumulative discounted 
damage state cost over the life-cycle: 

Net  step-wise  VoI  expresses  the  net  gain  as  a  result  of  additional 
information, also considering the cost to acquire this information (i.e. 
inspection cost). Elaborating on Eq. (8), and considering the fact that 
inspections do not change the state of the system, we have [62]: 

(16) 

(22) 

To  better  understand  Eq.  (16),  one  can  consider  a  case  where  the 
system may suffer only instantaneous losses due to failure with cost 
cF.  In this case, Eq. (16) reduces to: 

(17) 

where PFt is the probability of failure up to time t. The specialized 
definition  of  risk  provided  by  Eq.  (17)  follows  standard  risk  and 
reliability  assumptions  and  is  well-studied  in  inspection  and 
maintenance planning [10]. The proof that Eq.  (16) reduces to Eq. 
(17) under the above stated assumptions is presented in Appendix A. 
This work employs the risk definition of Eq. (16) instead of that of 
Eq. (17), as it facilitates a broader consideration of losses related to 
multiple system states.  

Similarly,  the  other  step  costs  of  Eq.  (10)  assume  the  following 

expected cumulative discounted values over the life-cycle: 

where  cb,I-  combines  any  costs  other  than  the  expected  inspection 
cost,  i.e.  maintenance,  shutdown,  and damage  state  costs.  Eq.  (22)
provides  an  alternative  description  of  the  Bellman  equation,  and 
shows that for any possible maintenance action, the decision-maker 
takes that inspection action which maximizes the net VoI at this step.  
Following the above, the concavity of the POMDP value function 
of Eq. (8), and the properties of the Bellman contraction operator, we 
can show that step-wise VoI, as well as VoI over the life-cycle, are 
always  non-negative  if  the  decisions  follow  the  POMDP  optimal 
policy  [62,  63].  At  the  extreme  case  that  no  inspection  means  no 
information at all, VoI reaches its highest value. This result can be 
similarly shown. 

3.  Operating under constraints 

We  consider  the  following  form  of  the  stochastic  optimization 
problem of Eq. (1): 

(18) 

(23) 

Hence,  the  optimal  POMDP  value  with  its  optimality  equation 
described in Eq. (8) is: 

where  Gh,k and Gs,m are the hard and soft constraints, respectively, 
gh,k and gs,m are their respective auxiliary costs (e.g. cM, cI, cS, cD, or 
else), and αh,k, αs,m are real-valued scalars. The form of constraints in 
Eq.  (23)  is  amenable  to  a  broad  class  of  constraint  types  that  are 
relevant to infrastructure management. For example, hard constraints 

(19) 

5 

,,,',''''Pr'|,''aaaaaperinstbDbDbDperinstsDDssssaaaperinstDDsssSsSccccsdcsbsssacsdcs0:1111,0aaTttttTtperinstoDtDtsssstcsdcs0:10:0:0:0:|,|,0TttttttTtFFoFaoFaotcPP0:0:1010,,{,},TtTtTtXosXtttTtIosItttCcsaXMSCcasminCVVbbminCMSICCC,,,,,,,VoIMeMIeIeeIaeoaaeoostepIoooaVVbb,netVoIVoIstepIstepbIac,,,*minmaxnetVoIMeeMMIIaeoobIaAstepIaAVcVabb0:0:0:,,0:0:1000,,,0*~,,~s.t.,0,1,.argmi,n.TTTTtsoatttttTthkhktthktcaoasGgsakKb0:0:0:,,,,1,0,,0,1,..,TTTTtsmsoasmtttsmtGgsasmM 
  
  
  
  
 
  
 
  
 
  
  
  
C. P. Andriotis, K. G. Papakonstantinou / Deep reinforcement learning driven inspection and maintenance planning under incomplete information and constraints 

can  model  a  great  variety  of  fixed  resource  allocation  and  control 
action  availability  problems,  such  as  problems  referring  to  strict 
budget limitations. In turn, soft constraints, of the Eq. (23) form, can 
model  a  great  variety  of  risk-based  constraints.  More details  about 
these can be found in Section 3.2. The term soft constraints, although 
not standard in stochastic optimization and optimal control literature, 
is used here to distinguish from the term hard constraints, indicating 
that the underlying constraints do not need to be strictly satisfied, but 
are rather imposed in an expected or probabilistic fashion. 

Hard  constraints  can  be  straightforwardly  taken  into  account 
through state augmentation. On an interesting remark, in one of his 
notes on  dynamic  programming  under  constraints  in 1956 [64], R. 
Bellman mentions that this approach may not be favored since “due 
to the limited memory of present-day digital computers, this method 
founders on the reef of dimensionality”. However, this restriction has 
been widely lifted today, whereas DRL has diminished the effects of 
the  curse  of  state  dimensionality  even  further.  Thus,  state 
augmentation is followed for the hard constraints here. Note that, in 
the  special  case  where  functions  gs,m  are  deterministic,  soft 
constraints become hard. However, soft constraints are not practical 
to  consider  through  state  augmentation  since  one  should  track  the 
entire  distribution  of  the  cumulative  discounted  value  of  gs,m. 
Therefore,  probabilistic  constraints  are  addressed  here  through 
Lagrangian  relaxation  [65].  Based  on  the  above,  the  optimization 
problem is restated as: 

(24) 

where  variables  ykt  track  the  discounted  cumulative  value  of  the 
function related to hard constraints, gh,k, up to time step t-1. Variables 
ykt are upper bounded by ah,k. Lagrange multipliers, λm, constitute the 
dual variables of the max-min dual problem, they are positive scalars, 
and are associated with the soft constraints. 

3.1.  Budget constraints 

for 

funding 

Depending on the operational and resource allocation strategy of the 
management  agency,  available 
inspection  and 
maintenance must comply with certain short-term or long-term goals 
related to a specific budget cycle duration, TB. Namely, in the extreme 
case  of  a  short-term  budget  cycle  duration,  budget  caps  exist  for 
every decision step (e.g. annual budget), whereas in the extreme case 
of a long-term budget cycle duration, there is a budget cap pertaining 
to  the  cumulative  inspection  and  maintenance  expenses  over  the 
entire  life-cycle  of  the  system,  i.e.  TB=T.  The  cumulative  cost  of 
inspection and maintenance actions over period TB is given for: 

where 
maintenance and inspection costs at each time step read: 

 is the integer part of  x. For a given budget cap  αh, the 

(26) 

(27) 

According  to  Eqs.  (25)-(27),  inspection  and  maintenance  costs  are 
accounted for only at the current budget cycle, and if the currently 
selected action does not violate the budget cap.  The total cost at each 
time step of Eq. (10) is accordingly rewritten as: 

(28) 

Transition  and  observation  probabilities  are  also  affected  by  the 
presence of the budget constraints as: 

 (29) 

where  ao  is  the  trivial  decision,  where  no  inspection  and  no 
maintenance  are  performed.  As  indicated  by  Eqs.  (24)-(29), 
incorporation  of  budget  constraints  can  be  accomplished  by 
accounting for new state variables y=yt. This way the agent is able to 
reason about control actions based on the available budget, αh - yt, at 
each time step of the decision process. In the case of step-wise budget 
constraints, i.e. TB=1, this state augmentation is not necessary, since 
the  agent  does  not  need  to  track  any  inspection  and  maintenance 
expenses made in the past, thus having the entire amount of αh at its 
disposal for every single step.  

As  opposed  to  state  variables  st,  new  variables  yt  are  fully 
observable. In this regard, the problem can be also seen as a mixed 
observability Markov decision process, which admits favorable state 
decompositions  and  can be  solved  by  value  iteration  algorithms  in 
settings  with  moderate  dimensions  [18].  In  this  case,  constrained 
value  iteration  based  POMDP  solution  procedures  devised  for 
constrained  problems  can  be  employed  to  drive  the  optimization 
process [44, 45, 46]. However, as for the unconstrained case, such 
formulations can manifest limitations  related to efficient scaling in 
systems with large state and action spaces, like the systems that are 
typically encountered in the class of sequential decision-making for 
infrastructure and networks.  

3.2.  Risk-based constraints 

For  notational  efficiency  of  the  present  section  we  introduce  the 
following random variables: 

                                (30) 

(25) 

where 
maintenance  actions  over  the  life-cycle,  and 

,  for  example,  accumulates  total  costs,  related  to 

6 

0:0:0:0:11,,,,...,00,,0:0:10001m,,~,,,~axmi,nTTTTMTttttsoatMmsmsmttttmcsasgasVaoybyby10,...,1,0100,0maxmin(,)s.t.,,,0,0,,1,...,MtKtktkthkkkkthkyygsayykVKyyb,thMIgsacc1/,/1tBBBBtTTtTTxhhhhMMygIIygcccc1111,,,,atMttIttSttDttccsacascsacssPr|,,Pr|,1Pr|,hhhhaaaygygossyassassa11Pr'|,,Pr'|,1Pr'|,Pr'|',,Pr'|',1Pr'|',hhhhhhhhaaaygygoygygossyassassaosyaosaosa111110,,,{,,,}TtiittttiiJcsasiMISDJJMJ0:0:0:,,[]TTTsoaMMJC 
  
  
  
  
  
  
  
 
C. P. Andriotis, K. G. Papakonstantinou / Deep reinforcement learning driven inspection and maintenance planning under incomplete information and constraints 

according to the definitions of Eq. (11). 

3.3.  Constrained control with deep reinforcement learning 

We are now interested in incorporating constraints that bound risk 
over the system life-cycle. The risk-related random variable based on  
Eqs.  (16)  and  (30)  is 
.  Thus,  the  respective  constraint  function 
obtains the following form, for gs=cD in Eq. (23): 

    (31) 

It should be noted that, although the budget constraints of focus in 
this  work  are  not  soft,  budget  constraints  can  also  be  expressed 
through  Gs  constraints,  satisfied  in  expectation,  depending  on  the 
modeling  needs of  the  problem,  as  in  Eq.  (31).  Any  other  costs  as 
introduced in Eq. (10) can be considered in the same logic as well. 

Constraints  of  the  generic  Gs  form  are  also  the  chance  or 
probabilistic  constraints,  which  bound  the  probabilities  of  certain 
quantities  or  events  [52,  53].  As  such,  if  one  wants  to  bound  the 
probability of the optimal policy exceeding a certain life-cycle cost 
threshold Jcr, one may apply the following gs function, for any 

similarly to Eq. (31): 

(32) 

where the second indicator  signifies the cumulative cost constraint 
violation, and the first one ensures that this is taken into account once, 
at  the  end  of  the  planning  horizon.  Taking  the  expectation  of 
cumulative value of the constraint function of Eq. (32), we have: 

(33) 

Considering  Eq.  (33),  if  αs  =1,  we  end  up  with  a  hard  constraint 
. It is thus obvious that hard constraints can 
requirement, i.e. 
be also seen as a limiting case of soft constraints. 

From a stricter reliability standpoint, many decision problems are 
interested in bounding the probability of failure (i.e. the probability 
reaching a failure state sF  from a  non-failure  state)  over  the  system  
,  γ=1,  and 
operating  life.  In  this  case,  we  just  need  to  set, 

 in Eq. (31): 

In recent work by the authors [29, 30], the Deep Centralized Multi-
agent Actor Critic approach has been proposed for management of 
large  engineering  systems,  which  treats  system  control  units  as 
individual  agents  making  decentralized  decisions  based  on 
shared/centralized  system  information  and  actor  hidden  layer 
parameters. Control units are defined in reference to system parts for 
which separate actions apply at each decision step, and can be either 
individual system components or greater sub-system parts comprised 
of  multiple components. As such, one control  unit has at least one 
component, and one component may belong to more than one control 
units. The system policy function is written as: 

where a is a vector of actions and 

 is a 2D matrix, such that: 

(35) 

(36) 

where a(i) is the action of control unit  i, b(j) is the belief of system 
component j, NCU is the number of control units, and NC is the number 
of system components. 

The policy functions of Eq. (35), as well as a centralized system 
Lagrangian  value  function  are  parametrized  with  the  aid  of  deep 
neural networks as: 

(37) 

Parameters
, θV are real-valued vectors, and can either vary or be 
shared among control units. In either case, each control unit’s policy  
is conditioned on the global belief and the budget-related states. Note 
that here we have a separate policy network for each agent as denoted 
by  superscript  i  in  the  policy  parameters  of  Eq.  (37),  thus  a 
completely  decentralized  actor  parametrization 
is  used.  To 
distinguish this from the original DCMAC architecture we call this 
Deep  Decentralized  Multi-agent  Actor  Critic  (DDMAC).  As 
discussed  in  Section 1,  both  provide  decentralized  POMDP  policy 

PFT is the probability of failure up to the end of the life-cycle t=T. 
Scalar αs in Eq. (33) and (34) is a valid probability designating the (1 
-  αs)-percentile  of  risk  and  probability  of  failure,  respectively,  the 
decision-maker is willing to tolerate.  

Other relevant constraint definitions in stochastic optimization and 
constrained Markov decision processes literature include constraints 
on the value-at-risk and conditional-value-at-risk [66, 53] (with the 
former coinciding with probabilistic constraints), constraints on the 
policy variance [67, 68], as well as constraints whose satisfaction is 
implicitly encouraged through reward-based penalization [69].  

components 

(34) 

… 
critic 
network 
… 

… 
… 

… 

actor 
network 

λ 

^ 
Vπ (b, y) 
λ 

^ 
Aπ (b, y, a) 

λ 

)
y

,

b
^

|
)
j
(

a
(
j
π
g
o
l

j

∑

^ 
π (a | b, y) 

sample 
at 

^ 
b 

y 

component belief 
hard constraint state 

forward evaluation 

back-propagation 

Fig. 2. Constrained Deep Decentralized Multi-agent Actor Critic (DDMAC) 
architecture. 

7 

DJ0:0:0:0:0:0:1,,,,110TTTaTTTttssoaDsTtperinstsoaDtDtssstsGJcsdcsiJicrstTJJg110:0:0:,,0PrTTTicrTssoatTsJJticrsGJJ11icrJJ0perDc1tFinstDssc0:0:0:11,,0aTTTtFttTTssoassssstFsGdP()1ˆˆ|,|,CUNiiiaabybyˆb()1()1ˆCUCNiiNjjaabb()()()ˆˆ|,|,,ˆˆ,,|iiiiiVaaVVbybyθbybyθ()iθ 
  
  
  
  
 
  
 
 
 
 
 
 
 
 
 
 
 
 
 
 
C. P. Andriotis, K. G. Papakonstantinou / Deep reinforcement learning driven inspection and maintenance planning under incomplete information and constraints 

Algorithm 1 Constrained Deep Decentralized Multi-agent Actor Critic  

The gradient of dual variables λm is easily computed as [50]: 

Initialize replay buffer 

Initialize actor, critic, and dual parameters

for number of episodes do 
    for t=1,…,T do 
           Select action 

 at random according to exploration noise 

           Otherwise select action 

           Estimate costs 

, 

 given 

and  at 

           Observe 

 for 

           Compute beliefs 

 for 

           Store tuple 

           Sample batch 

           If 

is terminal state

           Otherwise

 to replay buffer  

from replay buffer 

    Update actor parameters

according to gradient:        

    Update critic parameters 

according to gradient:   

    Update dual variables
    according to gradient:   

, m=1,…,M, based on current policy return,    

    end for 
end for 

solutions.  The  respective  architectures  are  shown  in  Fig.  2.  In  this 
figure, 4 components are depicted, and each component is a control 
unit, thus NCU=NC. 

DDMAC is trained based on off-policy experiences retrieved from  
. These experiences are in the 

the replay memory or replay buffer,

form  of 
  tuples that  are generated  
while  the  agent  interacts with  the  environment.  Thus,  the  replay  
memory is a stack of transition tuples.  

The  off-policy  gradients  of  the  policy  function  and  the  value 

function are computed by importance sampling as: 

   (38) 

(39) 

where wt is the importance sampling weight with sample distribution  
a policy μ retrieved from the experience replay and target distribution 
the  current  policy.
 is  the  advantage  function,  which  is  herein  
approximated by the temporal difference: 

(41) 

Dual  variables  are  updated  through  on-policy  samples  since  off-
policy  weighted  sampling  of  multiple  time  steps  produces  high-
variance estimators that may trigger training instabilities. Algorithm 
1 describes the aforementioned implementation steps. 

4.  Results 

4.1. Environment details 

A  stochastic,  non-stationary,  partially  observable  10-component 
deteriorating system is considered, operating over a life-cycle period 
of 50 decision steps (years), with a discount factor of γ=0.975. For 
civil engineering systems, discount factors typically range from 0.93 
to  0.98.  Higher  discount  factors  make  the  decision  problem  more 
challenging, in the sense that they increase the effective horizon of 
important  decisions.  Links  between  components  create  the  system 
shown in Fig. 3. It is assumed that link operation depends solely on 
the  operating  status  of  the  respective  components.  Overall  system 
connectivity is determined by the connectivity of nodes A and B. 

Each  component  has  independent  deterioration  dynamics.  These 
are  expressed  by  4x4x50  three-dimensional  transition  matrices, 
corresponding  to  4  damage  states  (intact,  minor  damage,  major 
damage,  severe  damage),  combined  with  50 deterioration  rates, as 
many  as  the  decision  steps  of  the  system  life-cycle.  Component 
transitions are given in Tables 1,2. Component transition parameters 
for the underlying hidden Markov models are assumed to be known 
or already learned, thus model uncertainty is not considered in this 
example. For learning of hidden Markov models based on structural 
data  the  interested  reader  is  referred  to  [70,  71].  Different  failure 
probabilities are considered based on each one of the above damage 
states, as shown in Table 3. Thus, the system behavior, as a whole, is 
described by the Bayesian network of Fig. 4. 

Further  details  on  consistently  coupling  inference  of  dynamic 
Bayesian networks with POMDPs for deteriorating structures can be 
found  in  [72,  73].  The  final  state  vector  for  each  component  is 
(i),t),  where x(i) is the damage state; τ(i) is the deterioration 
s(i)=(x(i),τ(i),f 

Type III 

9 

8 

Type I 

7 

6 

10 

3 

B 

Type II 

A 

5 

4 

2 

1 

Fig. 3. Multi-component deteriorating system. System fails when connectivity 
between nodes A and B is lost. Major costs are incurred when system fails. 
Minor costs are incurred for combinations of failed series subsystems. Types 
I-III refer to the severity of the deterioration model, from less to more severe, 
respectively. 

(40) 

8 

1,1ˆˆ,,,[],,[],','CUNMiibsmmcgbyaby()()()1ˆˆlog|,,,,CUiNiitiiVwaAθθbyθbyaˆˆ,|,,VVVtVwVAθθbyθbyaA,1ˆ,,|MVbmsmmAcgbyaθˆˆ','|,|VVVVbyθbyθ,,0mTtsmsmtVg()11,,CUNMjVmmjθθta()ˆ~|,,CUNjttjttjaμbyθ,btbcc,,smtsmggˆtb()()()11~|,,llltttttopobya1,2,...,ClN()1ltb1,2,...,ClN,,111ˆˆ,,,,,[],,Mttttbtsmtmttcgbyaμby,,111ˆˆ,,,,,[],,Miiiibismimiicgbyaμbyˆib,,,1ˆ,|MVibimsmiiimAcgVbyθ,,,111ˆˆ,|,|MVVibimsmiiiiimAcgVVbyθbyθ()jθ()()(),1ˆlog|,,CUjNjjijiiiiijVwaAθθbyθVθ,ˆ,|VVViiiiiVwVAθθbyθm,,0mTtsmtsmtVg 
 
  
 
 
  
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
   
 
  
 
  
               
      
  
       
  
        
 
  
C. P. Andriotis, K. G. Papakonstantinou / Deep reinforcement learning driven inspection and maintenance planning under incomplete information and constraints 

Table 1 
Component initial damage state transition probabilities for deterioration model 
Types I, II, and III. 

Deterioration 
model 
Type I 
Type II 
Type III 

p12 

p13 

p14 

p23 

p24 

p34 

0.0129 
0.0311 
0.0428 

0.0072 
0.0096 
0.0229 

0.0008 
0.0014 
0.0033 

0.0102 
0.0283 
0.0406 

0.0038 
0.0057 
0.0095 

0.0092 
0.0281 
0.0328 

Table 2 
Component final damage state transition probabilities for deterioration model 
Types I, II, and III. 

Transition 
probability 
Type I 
Type II 

p12 

p13 

p14 

p23 

p24 

p34 

0.0618 
0.0862 

0.0512 
0.0868 

0.0036 
0.0051 

0.0905 
0.1219 

0.0091 
0.0121 

0.0768 
0.1091 

Type III 

0.1347 

0.0669 

0.0098 

0.1665 

0.0244 

0.1462 

Table 3 
Component  failure probabilities  for different  deterioration types  and damage 
states. 

Damage 
state 
Type I 
Type II 
Type III 

intact 

0. 0019 
0. 0028 
0. 0088 

minor 

0. 0067 
0. 0076 
0. 0210 

major 

0. 0115 
0. 0163 
0. 0449 

severe 

0. 0177 
0. 0219 
0. 0564 

(i) is a binary failure indicator; and t is the decision time step (t 
rate; f 
is the same for all components). Failure is considered an absorbing 
state. Hence, we assume that when a component fails it remains failed 
at the next step, as long as no restorative action is taken. This allows 
us to augment the component state space, finally obtaining 5x5x50 
transition matrices. 

(i) 
τ0 

(i) 
x0 

(i) 
f0 

fS,0 

c0 

(i) 
τ1 

(i) 
x1 

(i) 
τ2 

(i) 
x2 

(i) 
τT 

(i) 
xT 

(i) 
o0 

(i) 
o1 

(i) 
o2 

(i) 
oT 

(i) 
a0 

(i) 
a1 

(i) 
a2 

(i) 
aT-1 

(i) 
f1 

fS,1 

c1 

(i) 
f2 

fS,2 

c2 

i=1,…,Nc 

(i) 
fT 

fS,T 

cT 

Fig. 4. Dynamic Bayesian network of multi-component deteriorating system 
in time. 

If an inspection action is taken, observation probabilities are given 

by the following observation matrices: 

(42) 

We  consider  three  types  of  available  maintenance  actions;  AM 
={no-repair,  partial-repair,  restoration/replacement}.  There  are 
also  two  types  of  available  inspection  actions;  AI ={no-inspection, 
inspection}. Accordingly, to allow for utmost diversification between 
component policies, each component, which herein defines a separate 
control  unit,  is  assigned  5  available  inspection  and  maintenance 
actions, based on the combinations of the abovementioned sets, i.e. 
a(i)  ϵAM×AI  \  (restoration/replacement,  inspection).  The  (resto-
ration/replacement,  inspection)  action  is  excluded  from  the  set  of 
available actions, as it is assumed that whenever a system component 
is replaced, thus returning to an as good as new condition, a decision 
for inspection is strictly suboptimal. No-repair costs are null, whereas 
restoration/replacement  costs  are  the  same  for  all  components. 
Partial-repair costs are (7.5,10,15)% of the component replacement 
cost, for component Types (I, III, II), respectively. Inspection costs 
are  the  same  for  all  components,  at  1.5%  of  the  component 
replacement cost. Partial-repairs send components one damage state 
back  without  changing 
restorations/ 
replacements  send  components  to  the  initial  damage  state  and 
deterioration rate, whereas no-repairs have no effect on the damage 
state  and  deterioration  rate.  Partial-repairs  have  no  effect  on  failed 
components and are considered to have been completed  before the 
next  environment  transition.    When  restorations/replacements  are 
chosen, these are completed at the end of the next time step, negating 
the  deterioration  transition  during  that  step.  Thus,  in  this  case,  the 
next state is the intact one with certainty. 

the  deterioration 

rate, 

Observation  matrices  depend  on  state  discretization  and  presumed 
measurement  noise  or  estimated  model  errors  [15].  Failure  is 
considered to be a self-announcing event, hence, component (5,5) of 
the observation matrix of Eq. (42) is 1. Accordingly, if no inspection 
is taken the observation matrix reads: 

(43) 

System failure, i.e. loss of connectivity between nodes A and B, is 
described by random variable fs. Random variable fs assumes 4 values 
associated with events E0: all links available, E1: 25% of links failed, 
E2: 50% of links failed without system failure, and Fs: system failure. 
A  link  is  failed  if  at  least  one  component  is  failed.  We  can  thus 
consider  the  series  subsystems,  controlling  the  link  failures, 
l1={1,2,3}, l2={4,5}, l3={6,7} and l4={8,9,10}. Their failure events 
are accordingly described by events Fl,1, Fl,2, Fl,3, and Fl,4. Based on 
the above, it can be derived that the system failure probability is: 

(44) 

9 

()()()()()Pr|,{}0.840.130.020.010.110.770.090.030.020.160.700.120.010.020.130.841iiiiiMosSosaAinspection()()()()()Pr|,{}11111iiiiiMosSTosaAnoinspection4,1,3,2,4,1PrPrPrPrPrPrsllllliiFFFFFF 
 
 
 
 
 
 
 
 
 
  
  
 
  
C. P. Andriotis, K. G. Papakonstantinou / Deep reinforcement learning driven inspection and maintenance planning under incomplete information and constraints 

the system rebuild cost, {5, 7.5, 10, 12.5, 15, 17.5, 20, 25, 30}% creb. 
For  the  second  set  of  analyses,  9  different  levels  of  life-cycle  risk 
constraints are considered, i.e. {1, 1.25, 1.5, 1.75, 2, 2.25, 2.5, 2.75, 
3.25} creb. In addition to the above analyses, the unconstrained policy 
is also learned. 

For training, the Keras deep learning python libraries are used with 
Tensorflow backend. For all analyses, the actor networks consist of 
two  fully  connected  hidden  layers  with  50  Rectified  Linear  Unit 
activation functions each, for all 10 components. No parameters are 
shared  among  component  actors,  and  each  control  unit  has  a  5-
dimensional  softmax  output  corresponding  to  the  cardinality  of 
AM×AI  \  (restoration/replacement,  inspection).  The  critic  network 
also  consists  of  two  fully  connected  hidden layers  with  150  ReLU 
activations  each.  The  critic  has  a  one-dimensional  linear  output, 
which approximates the POMDP  Lagrangian  value  function of the 
entire system.   

The Adam optimizer [74] is utilized for stochastic gradient descent 
on the networks parameter space, with learning rates being gradually 
adjusted  from  1E-3  and  1E-4,  to  1E-4  and  1E-5  for  the  critic  and 
actor, respectively. The learning rate of Lagrange multipliers is set to 
1E-5. The size of the experience replay is set equal to 300,000 and an 
exploration noise of linearly annealed from 100% to 1% is added at 
the first 2500 episodes of the training process.  

4.3. DRL solutions and baseline policies 

To  verify  the  quality  of  DDMAC  solutions,  we  construct  and 
optimize  various  baseline  policies,  incorporating  well-established 
condition-  risk-,  and  time-based  inspection  and  maintenance 
assumptions,  which  are  also  combined  with  periodic  action 
considerations,  as  well  as  component  prioritization  approaches. 
These baselines are: 
  Fail  Replacement  (FR)  policy.  No  inspections  are  taken.    If  a 
component  fails  it  is  immediately  replaced.  No  variable  is 
optimized.  

  Age-Periodic  Maintenance  (APM)  policy.  No  inspections  are 
taken, whereas maintenance actions are taken based on the age of 
components.  Two maintenance ages are optimized; periodic age 

The  corresponding  non-failure  events  of  interest,  E0,  E1,  E2,  are 
defined as: 

(45) 

Accordingly, the probabilities of events E1, E2 are computed as: 

(46) 

Perpetual and instantaneous losses due to failure are equivalent to 2.5  
and 50 times the system rebuild cost, respectively,  i.e. 
and
,  respectively.   Similarly,  we  consider  perpetual  
and instantaneous losses incurred when 25% and 50% of system links 
are not available (i.e. at least one of their respective components is at 
the failure state). These losses  are  incurred  if  events  E1, E2  occur,   
respectively,    and    are  quantified  in  cost  units  as 

,
.  In  case  of  transporta- 
tion networks, for example, such perpetual losses may refer to time 
delays  and/or  additional  user  costs  due  to  detours,  whereas  such 
instantaneous losses may pertain to capital loss due to asset failures 
related to those links. 

, 

,

Based  on  the  above  losses,  the  fact  that  system  events  are  fully 
observable, and following the risk definition of Eq. (16), the interval 
risk reads:  

(47) 

Apart from the above losses, additional costs are included in the 
analysis, pertaining to scheduled system shutdowns. Those come as 
a  result  of  different  action  combinations  on  different  system 
components.  That  is,  considering  that  non-trivial  maintenance 
actions  require  some  degree  of  component  non-operability  for 
completion during a time step, events Ea0, Ea1, Ea2, Fas can occur, in 
analogy to events E0, E1, E2, Fs. Those losses are only incurred if the 
system would be otherwise in an operating condition, i.e. not failed. 
Events and their probabilities are similarly defined as in Eqs. (45)-
(47), whereas respective costs  are the same  as the perpetual losses 
due to events E0, E1, E2, Fs.  

4.2. Experimental setup 

For the purposes of this numerical investigation two sets of analyses 
a conducted. The first set considers a budget cycle period of TB = 5. 
Each budget period shares the same budget cap, and 9 different levels 
of budget constraints are considered, which are given as functions of 

Fig.  5.  Comparison  of  DDMAC  life-cycle  policies  with  different  baseline 
policies.  Total  life-cycle  cost  and  life-cycle  costs  due  to  inspection, 
maintenance, shutdown, and risk. The best optimized baseline is 42% worse 
than the DDMAC policy. 

10 

40,1441,,142,,,1,:::liililjijililjsijijEFEFFEFFF40,1441,,11,201Pr1PrPr1PrPrPr1PrPrPrliiljliijjisEFEFFEEEF2.5sperFrebcc50sinstFrebcc10.05perErebcc20.25perErebcc11instErebcc25instErebcc,1,121,,1,{,,}Pr1PrststssperinstbDstfstffFEEcfcfc 
  
  
 
  
 
 
 
 
C. P. Andriotis, K. G. Papakonstantinou / Deep reinforcement learning driven inspection and maintenance planning under incomplete information and constraints 

for  component  partial-repair  and  periodic  age  for  component 
restoration/replacement.  

  Age-Periodic  Inspections  and  Condition-Based  Maintenance 
(API-CBM)  policy.  Age-based  inspections  are  taken  for  all 
components,  based  on  each  component’s  age.  At  inspection 
times,  maintenance  actions  are  taken  based  on  the  observed 
damage state of each component. Five variables are optimized; 
age interval for component inspection, and type of maintenance 
for each one of the 4 observed damage states. 

  Time-Periodic  Inspections  and  Condition-Based  Maintenance 
(TPI-CBM)  policy.  Time-based  inspections  are  taken  for  all 
components  at  fixed  intervals  of  the  planning  horizon.  At 
inspection  times,  maintenance  actions  are  taken  based  on  the 
observed  damage  state  of  each  component.  Five  variables  are 
optimized;  time  interval  for  block  component  inspection,  and 
type of maintenance for each one of the 4 observed damage states. 
  Risk-Based Inspections and Condition-Based Maintenance (RBI-
CBM) policy. Inspections are taken for all components each time 
the system exceeds a predefined failure probability threshold. At 
inspection  times,  maintenance  actions  are  taken  based  on  the 
observed  damage  state  of  each  component.  Five  variables  are 
optimized; failure probability threshold, and type of maintenance 
for each one of the 4 observed damage states. 

The  last  two  baseline  policies  are  also  optimized  with  Component 
Prioritization (CP), which produces policies RBI-CBM-CP and TPI-
CBM-CP. Components are prioritized based on their probability of 
failure. In this case, one extra decision variable regarding the number 
of components (1 to 10) to inspect and maintain is added. This policy 
adapts a heuristic presented in [10]. In all baselines, if a component 
fails, it is immediately replaced. 

In Fig. 5, a comparison of the learned DDMAC policy with the 
various  baselines  is  presented,  for  the  unconstrained  environment 
(total  costs  and  decomposed  costs  in  linear  and  log  scales, 
respectively). The best optimal baseline is the policy combining risk-
based  inspections,  condition-based  maintenance  and  component 
prioritization. It can be observed that the life-cycle cost attained by 
the best baseline is about 42% worse than the DDMAC solution. The 
optimal age-periodic maintenance and fail-replacement policies, do 
not include the possibility of inspections and achieve the worst life-  

cycle costs. It is overall observed that baselines including inspections 
achieve  consistently  better  results.  Adding  to  this  remark,  it  is 
interesting  to  note  that  the  DDMAC  policy  spends  more  for 
inspections, i.e. performs a higher number of inspections, compared 
to the 3 best optimal baselines. As discussed, these inspections are in 
principle non-periodic and, as shown in Section 2.4, are driven by the 
innate  notion  of  VoI  in  POMDPs.  This  allows  the  agents  to  make 
more informed decisions regarding proper maintenance actions that 
overall  minimize  the  total  cumulative  costs  of  Eq.  (19)  more 
efficiently. Risk is significantly reduced with the DDMAC policy, as 
also  indicated in Fig. 5, whereas scheduled system shutdown costs 
are more intelligently avoided compared to other baselines, due to the 
flexibility in intervention timings and action combinations. 

4.4. Constrained system solutions 

life-cycle 

in  case  of  a  need  for  major 

Constrained  DDMAC  results  for 
inspection  costs, 
maintenance  costs,  shutdown  costs  and  risk  for  different  5-year 
constraint  levels  are  shown  in  Fig.  6  (all  costs  in  log  scale).  As 
expected, higher budget limits result in lower  total life-cycle costs. 
Budget  limits  higher  than  25%  of  the  system  rebuild  cost,  creb, 
practically  converge  to  the  unconstrained  solution.  A  noticeable 
feature of the learned near-optimal policies is that as budget becomes 
tighter, the agents tend to reduce their inspection expenses, to save 
resources 
interventions  (e.g. 
restoration/replacement  actions).  This  means  that  they  deliberately 
choose  to  forfeit  better  system  information,  in  order  to  be  more 
effective  against  disruption.  It  is  characteristic  that  inspections  are 
overall reduced in the budget cases below 15% creb, compared to the 
cases above that budget threshold, since the component replacement 
cost  is  10%  creb.  That  is,  below  10%  creb  budget  constraints, 
restorations/replacements  are  infeasible.  In  Fig.  7,  the  respective 
results for risk constraints are shown (all costs in log scale). It can be 
observed that as the decision-making task becomes more risk averse 
the  total  life-cycle  cost  becomes  higher,  since  more  frequent 
inspection  and  maintenance  actions  need  to  be  taken.  Constrained 
solutions practically converge to the unconstrained one after the risk 
tolerance threshold of 2.75creb. It is interesting to note here that for 
lower risk constraints, i.e. for scenarios where the agents need to keep 

Fig.  6.  Comparison  of  DDMAC  life-cycle  policies  for  different  5-year 
constraints from 5% creb to infinity. Total life-cycle cost and life-cycle costs 
due to inspection, maintenance, shutdown, and risk. 

Fig. 7. Comparison of DDMAC life-cycle policies for different life-cycle risk 
constraints from 1 creb to infinity. Total life-cycle cost and life-cycle costs due 
to inspection, maintenance, shutdown, and risk. 

11 

 
 
 
 
 
 
 
 
C. P. Andriotis, K. G. Papakonstantinou / Deep reinforcement learning driven inspection and maintenance planning under incomplete information and constraints 

Budget constraint: 15% creb 

Budget constraint: 20% creb 

Risk constraint: 2.75 creb 

Risk constraint: 3.25 creb 

# maintenance actions / step 

# maintenance actions / step 

# maintenance actions / step 

# maintenance actions / step 

s
t
n
e
n
o
p
m
o
c

1- 
2- 
3- 
4- 
5- 
6- 
7- 
8- 
9- 
10- 

s
t
n
e
n
o
p
m
o
c

1- 
2- 
3- 
4- 
5- 
6- 
7- 
8- 
9- 
10- 

# inspection actions / step 

1- 
2- 
3- 
4- 
5- 
6- 
7- 
8- 
9- 
10- 

1- 
2- 
3- 
4- 
5- 
6- 
7- 
8- 
9- 
10- 

0.350 

0.175 

0.000 

1.000 

0.500 

0.000 

s
t
n
e
n
o
p
m
o
c

1- 
2- 
3- 
4- 
5- 
6- 
7- 
8- 
9- 
10- 

0.350 

0.175 

0.000 

# inspection actions / step 

# inspection actions / step 

s
t
n
e
n
o
p
m
o
c

1- 
2- 
3- 
4- 
5- 
6- 
7- 
8- 
9- 
10- 

1.000 

0.500 

0.000 

1- 
2- 
3- 
4- 
5- 
6- 
7- 
8- 
9- 
10- 

1- 
2- 
3- 
4- 
5- 
6- 
7- 
8- 
9- 
10- 

0.300 

0.175 

0.000 

1.000 

0.500 

0.000 

# inspection actions / step 

0.300 

0.175 

0.000 

1.000 

0.500 

0.000 

Fig.  8.  Components  maintenance  and  inspection  frequency  per  step  and 
respective mean costs for 5-year budget constraints of 15% and 20% creb. 

Fig.  9.  Components  maintenance  and  inspection  frequency  per  step  and 
respective mean costs for risk constraints of 2.75 and 3.25 creb. 

total risk lower over the operating life, although the maintenance cost 
increases, the inspection cost is not following the same trend, hence, 
the  inspection  per  maintenance  cost  ratio  of  the  optimal  policy 
consistently decreases. This is attributed to the fact that more frequent 
maintenance is unavoidable in a case where risks have to be kept low, 
something that, by itself, leads on average, to longer sojourn in states 
of lower damage. As such, increased frequency of inspections, which 
would solely serve better state determination, is not favored by the 
agents, and thus life-cycle inspection costs do not present important 
changes for different risk-based constraints. Accordingly, due to the 
high demand for maintenance actions, scheduled shutdown costs also 
increase in low-risk cases.  

In  Fig.  8,  action  frequencies  and  respective  cost  metrics  of 
inspection and maintenance are depicted for two budget constraints 
corresponding to a low and a high budget scenario, i.e.  to 15% and 
20% creb 5-year budget constraints, respectively. Contour plots depict 
the frequency of  maintenance and inspection actions per time unit. 
Adjacent graphs on the right show the mean step cost per component 
related to the respective action type, whereas the bottom graphs show 
the action cost per step, collectively for the all system components. 
The same features are depicted for risk constraints of 2.75 and 3.25 
creb in Fig. 9. Examining Figs. 6 and 8 together, we can observe that 
lowering  the  budget  from  20%  to  15%  creb  has  significant 
consequences  for  risk,  which  increases  disproportionally  with  the 
achieved reduction in the expected total life-cycle maintenance cost. 
What changes significantly for maintenance cost, as shown in Fig. 8, 
is its distribution per time unit and component, rather than its total 
life-cycle  value.  This  is  indicative  of  the  general  observation  that 
stricter  budgets  increase  risk,  without  necessarily  yielding  clear 
economic  budget-related  benefits,  if  any,  in  the  long  run.  Another 
interesting  feature  is  that,  in  the  presence  of  stricter  budgets,  the 
imbalance  in  the  allocation  of  maintenance  resources  among 
components increases. Inspections and their respective expenditures 
are  considerably  restricted,  as  also  mentioned  previously.  As  also 
shown in Fig. 8, for the 15% creb case, inspections are rather reserved  

mainly for component 4, as this is the most vulnerable component of 
path 6,7,4,5, which is the path securing system survival with the least 
number of components. 

For the  cases  of  risk-based  constraints,  examining  Figs.  7  and 9 
together,  we  can  observe  that  relevant  costs  are  distributed  more 
evenly in time over the planning horizon. Over the system life-cycle, 
we observe that lowering the risk tolerance considerably encumbers 
maintenance  costs  per  step  and  in  total.  Similarly  to  the  budget-
constrained  cases,  for  the  2.75  creb  versus  3.25  creb  risk  constraint 
case,  inspections  are  prominently  clustered  to  fewer  components. 
Accordingly,  it  is  observed  that  the  agents  reserve  their  inspection 
actions  exclusively  for  components  3-5,7,8,10.  This  intrinsically 
prioritized selection of components to be frequently inspected allows 
the agents to track the state of at least half of the components from 
each link, and thereby to better synchronize maintenance actions in 
order to minimize system shutdowns and costs. It was observed that 
although mathematically feasible from an optimization perspective, 
policies below 2.0 creb start becoming practically unrealistic due to 
the very frequent restorations/replacements that need to be taken in 
order for the risk constraints to be satisfied. 

To look closer into how policies change for different constraints, 
4 detailed policy realizations are shown in Figs. 10 and 11, for the 
constrained  environments  shown  in  Figs.  8  and  9,  respectively.  In 
the  realization  of  component  failure 
Fig.  10(a),  displaying 
probabilities and respective inspection and maintenance actions, for 
two cases of 5-year budget constraints, it can be readily observed that, 
in  the  low  budget  scenario,  available  budgetary  resources  are 
primarily allotted to the maintenance needs of components 3,4,8, and 
9. This is explained by the fact that these are Type III components, 
thus  being  described  by  the  most  aggressive  deterioration.  In  this 
realization  example,  only  component  4  is  inspected,  since,  as  also 
explained  earlier,  with  a  budget  limit  close  to  the  component 
replacement cost, the agents choose to inspect more rarely in order to 
save resources if major interventions are needed. In the high budget 
scenario, inspections play a  more prominent role, since the imposed  

12 

 
 
 
 
 
 
 
C. P. Andriotis, K. G. Papakonstantinou / Deep reinforcement learning driven inspection and maintenance planning under incomplete information and constraints 

(a) 

Low budget scenario (15% creb) 

Type III 

8 

7 

10 

A 
5 

Type II 

4 

3 
B 

1 

2 
Type I 

9 

6 

(b) 

(c) 

High budget scenario (20% creb) 

Type III 

8 

7 

10 

A 
5 

Type II 

4 

3 
B 

1 

2 
Type I 

9 

6 

Control Actions: 

0: Do Nothing 

1: Inspection 

2: Partial Repair 

3: Inspect. & Part. Repair 

4: Restoration/Replacement 

control action 
failure prob. 
failure sojourn 
failure event 

Fig. 10. A life-cycle realization of the DDMAC policy for 15% creb and 20% creb 5-year budget constraints. (a) Component failure probabilities and actions; (b) 
System failure with selected interventions; (c) Costs of inspection and maintenance actions, scheduled shutdowns, and risks. 

(a) 

Low risk scenario (2.75 creb) 

Type III 

8 

7 

10 

A 
5 

3 
B 

1 

9 

6 

4 

Type II 

2 
Type I 

(b) 

(c) 

High risk scenario (3.25 creb ≡ unconstrained) 

Type III 

8 

7 

10 

A 
5 

3 
B 

1 

9 

6 

4 

Type II 

2 
Type I 

Control Actions: 

0: Do Nothing 

1: Inspection 

2: Partial Repair 

3: Inspect. & Part. Repair 

4: Restoration/Replacement 

control action 
failure prob. 
failure sojourn 
failure event 

Fig. 11. A life-cycle realization of the learned DDMAC policy for  2.75 creb and 3.25 creb life-cycle risk constraints. (a) Component failure probabilities and 
actions; (b) System failure probability with selected interventions; (c) Costs of inspection and maintenance actions, scheduled shutdowns, and risks. 

13 

 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
  
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
C. P. Andriotis, K. G. Papakonstantinou / Deep reinforcement learning driven inspection and maintenance planning under incomplete information and constraints 

(a) 

(b) 

Fig.  12.  System  mean  failure  probabilities  based  on  DDMAC  life-cycle 
policies  for  different  (a)  5-year  budget  constraints  and  (b)  life-cycle  risk 
constraints.  

budget  restrictions  have  become  looser,  and  the  agents  have  the 
budgetary capacity to afford expenditure for acquiring information. 
Although  Type  III  components  continue to  receive  the  majority  of 
maintenance  actions,  intervention  resources  are  now  allotted  more 
frequently  to  all  components.  Some  of  the  most  prominent 
intervention effects changing significantly the overall system failure 
probability,  are  indicated  in  Fig.  10(b).  The  various  costs  are  also 
tracked in Fig. 10(c). For the 20% creb case, a notable feature can be 
observed for components 6 and 7, controlling the operability of the 
third link. Component 7 fails at t=38 and available resources do not 
allow for immediate replacement, which is postponed to t=40, when 
the  next  budget  cycle  begins.  In  the  meanwhile,  the  agent  of 
component  6  takes  advantage  of  the  link  shutdown  and  applies 
repeated opportunistic partial repairs which do not yield additional 
shutdown costs. Overall, it can be interestingly observed in  Figs. 8 
and 10, that the agents, despite their decentralized policies, form and 
increase collaboration as the budget becomes lower, directing their 
focus  to  components  that  are  more  vulnerable  to  deterioration,  or 
more strategically placed in terms of system connectivity. 

Similar features can be seen for the low- and high-risk constraints 
cases of Fig. 11. In the 3.25 creb case, effectively coinciding with the 
unconstrained  policy,  a  complex  and  diverse  policy  is  overall 
illustrated.  It  is  worth  noting  that,  in  the  absence  of  any  budget 
constraints, inspections are now taken frequently for all components, 
whereas  restoration/replacement  actions  start  to  also  have  more 
prominent preventive characteristics, i.e. they are not only reserved 
for failure events. This is even more apparent in the low-risk scenario, 
in which case, restorations need to be performed in a more recurrent 
fashion to ensure low probability of failure. In turn, this also causes 
more system closures and, thus, increases shutdown costs. To balance 
this  side  effect  of  frequent  restorative  actions,  the  agents  are 
interestingly shown to deploy a block-restoration/replacement logic 
in their policies. That is, as shown in the 2.75  creb scenario of Fig. 
11(a),  component  agents  of  the  same  links  synchronize  their 
restoration actions (e.g. components 2,3 at t=37; components 8-10 at 
t=20; components 4,5 at t=26), whereas they also start to extensively 
leverage  opportunistic  interventions  in  links  where  failure  events 
occur (e.g.  components 1-3 at t=12; components 4,5 at  t=39). The 
system  failure probability and the various costs along with various 
actions that affect them are shown in Figs. 11(b),(c), respectively. 

14 

The  mean  failure  interval  probability  of  the  system  over  time  is 
shown in Fig. 12, for various 5-year budget and various life-cycle risk 
constraints. It is observed that, on average, system failure probability 
reaches  its  peak  before  the  onset  of  new  budget  cycles.  For  the 
unconstrained case, mean  failure probability is allowed to increase 
over time, without abrupt escalations, since no budget limitation is 
imposed.  The  7.5%  creb  constrained  case  reflects  an  extreme  life-
cycle optimization setting where no replacement actions are feasible. 
Thus,  in  this  case  no  major  corrective  steps  are  detected  in  the 
evolution  of  the  mean  failure  probability.  In  the  case  of  risk 
constraints, the more stringent the risk constraint is, the higher is the 
reliability of the system at each time step, as anticipated. 

5.  Conclusions 

In this work a stochastic optimal control framework  for inspection 
and maintenance planning of deteriorating systems operating under 
incomplete  information  and  constraints  is  developed.  Decision-
making is cast in a multi-agent decentralized framework of DRL and 
POMDPs, where each system component, or control unit consisting 
of  multiple  components,  acts  as  an  independent  agent  given  the 
dynamically  updated  global  system  state  probabilistic  information. 
While satisfying a shared overarching objective, each agent can make 
its own inspections and maintenance choices. Operational resource-
based  restrictions  and  policy  risk  considerations  are  taken  into 
account by means of relevant stochastic soft and/or hard constraints. 
The  latter  are  incorporated  in  the  solution  scheme  through  state 
augmentation,  thus  being  rendered  as  environment  properties, 
whereas the former are appended in the life-cycle objective function 
as dual variables, to form the Lagrangian function to be optimized. 
Modeling  of  various  constraint  choices  is  discussed,  whereas  a 
thorough  numerical  investigation  is  provided  for  budget  and  risk 
constraints,  which  are  of  particular  significance  in  infrastructure 
management applications. Along these lines, a broad risk definition 
is  also  presented  and  utilized  in  the  constrained  optimization 
procedure,  accommodating  both  the  instantaneous  and  perpetual 
nature of damage-related losses. This risk definition is further shown 
to  be  reducible  to  classic  reliability-based  definitions.  Solutions  to 
the  optimization  problem  are  driven  by  the  introduced  DDMAC 
algorithm.  DDMAC  uses  both  policy  and  value 
function 
parametrizations,  experience  replay,  off-policy  network  parameter 
updating, and operates on the belief space of the underlying POMDP.  
Operation under constraints is shown to  considerably affect how 
the  agents  adapt  their  policies.  The  conducted  parametric  analysis 
shows that: 
  The need for inspections and, therefore, the value of information, 
fades  in  low-budget  environments,  where  the  agents  tend  to 
diminish expenses otherwise allotted to system updating needs, in 
order  to  secure  advanced  intervention  capabilities  through 
availability of maintenance resources.  

  Stricter  budget  constraints  reduce  inspection  and  maintenance 
costs  for 
the  respective  budget  cycle,  however,  without 
comparably reducing these costs in the long run, i.e., cumulatively, 
over the system life-cycle.  

  In  risk-averse  environments,  inspection  costs  do  not  follow  the 
notable  increase  in  maintenance  costs,  which  are  necessary  in 
order to maintain low-risk levels over the system operating life.  

 
 
 
 
C. P. Andriotis, K. G. Papakonstantinou / Deep reinforcement learning driven inspection and maintenance planning under incomplete information and constraints 

  In  such  cases,  agents  are  shown  to  increasingly  leverage  the 
structural properties of the system or incidental sub-system failure 
configurations, to develop opportunistic repair strategies, so that 
system operability is minimally disrupted.  

  Budget limitations and risk intolerance, disproportionally increase 
the  risk  and  maintenance  costs,  respectively,  compared  to  the 
reductions they achieve in the constrained quantities. 

  For  both  types  of  constraints,  multi-agent  cooperation  emerges 
more  prevalent  as  restrictions  become  stricter,  since  resource 
scarcity  and  risk  intolerance  force  the  agents  to  more  carefully 
reallocate resources and redefine management priorities, based on 
the  specific  deterioration  dynamics  and  structural  importance  of 
different  system  parts.  This  rescheduling  arises  naturally  and 
intrinsically  through  the  training  process,  without  any  explicit 
user-based enforcement or penalty-driven motivation. 

Overall,  the  derived  DRL  policies  showcase  remarkable  flexibility 
and  multi-agent  cooperation  in  various  constrained  and  un-
constrained  environments,  whereas  the  obtained  decentralized 
solutions  are  found  to  significantly  outperform  conventional  and 
state-of-the-art inspection and maintenance planning formulations.  

Acknowledgements 

This  material  is  based  upon  work  supported  by  the  U.S.  National 
Science  Foundation  under  CAREER  Grant  No.  1751941,  and  the 
Center 
for  Multimodal 
Transportation Infrastructure Systems (CIAMTIS), 2018 U.S. DOT 
Region 3 University Center. 

Integrated  Asset  Management 

for 

Appendix A: On the definition of risk 

Proposition A.1. If only failure incurs damage cost, this cost is cF, 
and it is instantaneous, then: 

  (A.2) 

Combining  both  instantaneous  and  perpetual  damage  costs,  and 
elaborating further on Eq. (15) we finally obtain: 

(A.3) 

Using Eq. (A.3), risk is defined as the cumulative damage state cost 
over the life-cycle in expectation: 

(A.4) 

Eq. (A.4) is equivalent to Eq. (15). We now consider that only failure 
incurs damage related cost, this cost is cF, and it is instantaneous. We  
denote  failure  state(s)  as  sF.  The  respective  cost  is  written  as

. In this case, Eq. (A.4) reduces to: 

and

are the probabilities of failure up to time 
where
t+1 and t, respectively, given a history of actions and observations 
a0:t, o0:t. 

Proof. By definition, the transition probability from  sa to s' can be 
written as: 

    (A.5) 

Probability ba(.), is the updated probability, as defined by Eqs. (4) and 
(6),  hence,  ba(.) =  Pr( 
 |a0:t,o0:t).  As  such,  Eq.  (A.5)  is  equivalently 
written as: 

.

    (A.1) 

  (A.6) 

where  dij  and  δij  are  the  adjacency  and  Kronecker  indicators,  as 
defined in Eqs. (14) and (7), respectively, for all i,j belonging to S. 
Thus, using Eq. (A.1), the expected cost of the instantaneous costs of 
Eq. (14) reads: 

From (A.6) follows immediately that

.   

Under the assumptions of the above proposition, one can model the 
POMDP problem with damage cost: 

Marginalizing with respect  to  s'  and  assuming  that  pair  (s,a)  leads  

                        (A.7) 

15 

0:110:10:0:0:0:11,0|,|,0aaTttttTttttttTtperinstoDtDtsssstTtFoFaoFaoFtcsdcscPP10:0:|,tttFaoP0:0:|,tttFaoP''Pr'|,Pr'|,aaaassssssadssa,,'','''''()(11)'aaaaaainstinstbDDssssinstDsssssssscdcsddcs,'''','',','''(1)()'''''aaaaaaaaainstinstDDssssssssinstinstDDssssinstinstDDsssssscsddcscscscscs'Pr'|,'Pr'|,aaaaainstDsSsSaaaainstaDsSbsssacsbssssacs,,','''''aaaperinstinstbDDDDssssssccscscs'Pr'|,''Pr'|,aaaaaperinstDDsSsSaaaainstaDsSbsssacscsbssssacs0:11111,01,aTttaattttTtperinstoDtDtsstinstDtsssscscscs1tFinstDssFcc0:111110:11,,0,0aaaTtFtFttttttaaaTtFttttFTtossFssFsssssstTtFossssssstccc0:10Pr|,TattTtaaaaFotFttFtsSsScbsssabs0:10:0:0:0:|,|,0TttttttTtFoFaoFaotcPPF''',,,'aaFFFaDssFssFFsssscsassdccc 
 
 
 
 
 
    
 
  
  
 
  
  
  
  
  
    
C. P. Andriotis, K. G. Papakonstantinou / Deep reinforcement learning driven inspection and maintenance planning under incomplete information and constraints 

deterministically to sa, a simpler expression can also be used for the 
cost function: 

                                      (A.8) 

Note  that  Eq.  (A.8)  is  a  closed  form  expression,  if  transitions  to 
failure  state  from  all  other  states,  Pr(sF  |  sa,a),  are  known,  i.e. 
according to standard offline Markov decision processes semantics.          

References 

[1]   D. M. Frangopol, M. J. Kallen and J. M. V. Noortwijk, "Probabilistic models 
for  life‐cycle  performance  of  deteriorating  structures:  review  and  future 
directions," Progress in Structural Engineering and Materials, vol. 6, no. 4, 
pp. 197-212, 2004.  

[2]   M.  Sanchez-Silva,  D.  M.  Frangopol,  J.  Padgett  and  M.  Soliman, 
"Maintenance  and  operation  of  infrastructure  systems,"  Journal  of 
Structural Engineering, vol. 142, no. 9, p. F4016004, 2016.  

[3]   P.  Bocchini  and  D.  M.  Frangopol,  "A  probabilistic  computational 
framework for bridge network optimal maintenance scheduling," Reliability 
Engineering & System Safety, vol. 96, no. 2, pp. 332-49, 2011.  

[4]   D.  Saydam  and  D.  Frangopol,  "Risk-based  maintenance  optimization  of 
deteriorating bridges," Journal of Structural Engineering, vol. 141, no. 4, p. 
04014120, 2014.  

[5]   D. Y. Yang and D. M. Frangopol, "Life-cycle management of deteriorating 
civil  infrastructure  considering  resilience  to  lifetime  hazards:  A  general 
approach  based  on  renewal-reward  processes,"  Reliability  Engineering  & 
System Safety, vol. 183, pp. 197-212, 2019.  

[6]   M. Marseguerra, E. Zio and L. Podofillini, "Condition-based maintenance 
optimization by means of genetic algorithms and Monte Carlo simulation," 
Reliability Engineering & System Safety, vol. 77, no. 2, pp. 151-65, 2002.  

[7]   D.  M.  Frangopol,  K.  Y.  Lin  and  A.  C.  Estes,  "Life-cycle  cost  design  of 
deteriorating structures,"  Journal of Structural  Engineering,  vol.  123,  no. 
10, pp. 1390-401, 1997.  

[8]   M.  H.  Faber  and  M.  G.  Stewart,  "Risk  assessment  for  civil  engineering 
facilities:  critical  overview  and  discussion,"  Reliability  Engineering  & 
System Safety, vol. 80, no. 2, pp. 173-84, 2003.  

[9]   D. Straub and M. H. Faber, "Risk based inspection planning for structural 

systems," Structural Safety, vol. 27, no. 4, pp. 335-355, 2005.  

[10]  J.  Luque  and  D.  Straub,  "Risk-based  optimal  inspection  strategies  for 
structural  systems  using  dynamic  Bayesian  networks,"  Structural  Safety, 
vol. 76, pp. 60-80, 2019.  

[11]  A.  Grall,  C.  Bérenguer  and  L.  Dieulle,  "A  condition-based  maintenance 
policy for stochastically deteriorating systems,"  Reliability Engineering & 
System Safety, vol. 76, no. 2, pp. 167-180, 2002.  

[12]  A.  Grall,  L.  Dieulle,  C.  Berenguer  and  M.  Roussignol,  "Continuous-time 
predictive-maintenance  scheduling  for  a  deteriorating  system,"  IEEE 
Transactions on Reliability, vol. 51, no. 2, pp. 141-150, 2002.  

[13]  B.  Castanier,  C.  Bérenguer  and  A.  Grall,  "A  sequential  condition‐based 
repair/replacement policy with non‐periodic inspections for a system subject 
to continuous wear," Applied Stochastic Models in Business and Industry, 
vol. 19, no. 4, pp. 327-347, 2003.  

[14]  R. Rackwitz, A. Lentz and M. H. Faber, "Socio-economically sustainable 
civil engineering infrastructures by optimization," Structural Safety, vol. 27, 
no. 3, pp. 187-229, 2005.  

[15]  S.  Madanat,  "Optimal 

infrastructure  management  decisions  under 
uncertainty," Transportation Research Part C: Emerging Technologies, vol. 
1, no. 1, pp. 77-88, 1993 .  

[16]  H. Ellis, M. Jiang and R. B. Corotis, "Inspection, maintenance, and repair 
with partial observability," Journal of Infrastructure Systems, vol. 1, no. 2, 
pp. 92-99, 1995.  

[17]  K.  G.  Papakonstantinou  and  M.  Shinozuka,  "Optimum  inspection  and 
maintenance  policies  for  corroded  structures  using  partially  observable 
Markov  decision  processes  and  stochastic,  physically  based  models," 
Probabilistic Engineering Mechanics, vol. 37, pp. 93-108, 2014.  

[18]  K. G. Papakonstantinou, C. P. Andriotis and M. Shinozuka, "POMDP and 
MOMDP solutions for structural life-cycle cost minimization under partial 
and mixed observability," Structure and Infrastructure Engineering, vol. 14, 
no. 7, pp. 869-882, 2018.  

[19]  P.  Bocchini  and  D.  M.  Frangopol,  "Optimal  resilience-and  cost-based 
postdisaster  intervention  prioritization  for  bridges  along  a  highway 
segment," Journal of Bridge Engineering, vol. 17, no. 1, pp. 117-29, 2012.  

[20]  A.  D.  González,  L.  Dueñas‐Osorio,  S.  M.  and  A.  L.  Medaglia,  "The 
interdependent  network  design  problem  for  optimal  infrastructure  system 
restoration," Computer‐Aided Civil and Infrastructure Engineering, vol. 31, 
no. 5, pp. 334-50, 2016 .  

[21]  S.  Nozhati,  Y.  Sarkale,  E.  K.  Chong  and  B.  R.  Ellingwood,  "Optimal 
stochastic  dynamic  scheduling  for  managing  community  recovery  from 
natural  hazards,"  Reliability  Engineering  &  System  Safety,  vol.  193,  p. 
106627, 2020.  

[22]  R. E. Bellman, Dynamic programming, Princeton University Press, 1957.  

[23]  J. Pineau, G. Gordon and S. Thrun, "Point-based value iteration: An anytime 
algorithm  for  POMDPs,"  in  International  Joint  Conference  on  Artificial 
Intelligence, 2003.  

[24]  L. P. Kaelbling, M. L. Littman and A. R. Cassandra, "Planning and acting 
in partially observable stochastic domains," Artificial Intelligence, vol. 101, 
no. 1, pp. 99-134, 1998.  

[25]  K. G. Papakonstantinou and M. Shinozuka, "Planning structural inspection 
and maintenance policies via dynamic programming and Markov processes. 
Part I: Theory," Reliability Engineering & System Safety, vol. 130, pp. 202-
213, 2014.  

[26]  K. G. Papakonstantinou and M. Shinozuka, "Planning structural inspection 
and maintenance policies via dynamic programming and Markov processes. 
Part II: POMDP implementation," Reliability Engineering & System Safety, 
vol. 130, pp. 214-224, 2014.  

[27]  K. G.  Papakonstantinou, C.  P.  Andriotis and  M.  Shinozuka,  "Point-based 
POMDP solvers for life-cycle cost minimization of deteriorating structures," 
in  Proceedings  of  the  5th  International  Symposium  on  Life-Cycle  Civil 
Engineering (IALCCE 2016), Delft, The Netherlands, 2016.  

[28]  M.  Memarzadeh  and  M.  Pozzi,  "Integrated  inspection  scheduling  and 
maintenance  planning  for  infrastructure  systems,"  Computer‐Aided  Civil 
and Infrastructure Engineering, vol. 31, no. 6, pp. 403-415, 2015.  

[29]  C.  P.  Andriotis  and  K.  G.  Papakonstantinou,  "Managing  engineering 
systems  with  large  state  and  action  spaces  through  deep  reinforcement 
learning,"  Reliability  Engineering  &  System  Safety,  vol.  191,  p.  106483, 
2019.  

[30]  C. P. Andriotis and K. G. Papakonstantinou, "Life-cycle policies for large 
engineering  systems  under  complete  and  partial  observability,"  in  13th 
International  Conference  on  Applications  of  Statistics  and  Probability  in 
Civil Engineering (ICASP), Seoul, South Korea, 2019.  

[31]  Z. Wang, V. Bapst, N. Heess, R. Munos, K. Kavukcuoglu and N. De Freitas, 
"Sample  efficient  actor-critic  with  experience  replay,"  arXiv  preprint 
arXiv:1611.01224, 2016.  

[32]  T.  Degris,  M.  White  and  R.  S.  Sutton,  "Off-policy  actor-critic,"  arXiv 

preprint arXiv:1205.4839, 2012.  

[33]  G.  Shani,  J.  Pineau  and  R.  Kaplow,  "A  survey  of  point-based  POMDP 
solvers," Autonomous Agents and Multi-Agent Systems, vol. 27, no. 1, pp. 1-
51, 2013.  

[34]  F.  A.  Oliehoek  and  C.  Amato,  A  concise  introduction  to  decentralized 

POMDPs, Springer, 2016.  

[35]  D. S. Bernstein, R. Givan, N. Immerman and S. Zilberstein, "The complexity 
of  decentralized  control  of  Markov  decision  processes,"  Mathematics  of 
Operations Research, vol. 27, no. 4, pp. 819-40, 2002.  

16 

,Pr(|,)aFaDFFFsscsassacc 
 
C. P. Andriotis, K. G. Papakonstantinou / Deep reinforcement learning driven inspection and maintenance planning under incomplete information and constraints 

[56]  M. Memarzadeh, M. Pozzi and J. Kolter, "Hierarchical modeling of systems 
with  similar  components:  A  framework  for  adaptive  monitoring  and 
control,"  Reliability  Engineering  &  System  Safety,  vol.  153,  pp.  159-69, 
2016.  

[57]  E.  Bismut  and  D.  Straub,  "Inspection  and  maintenance  planning  in  large 
monitored  structures,"  in  6th  International  Symposium  on  Reliability  and 
Engineering Risk Management (ISRERM), 2018.  

[58]  K. Rokneddin, J. Ghosh, L. Dueñas-Osorio and J. E. Padgett, "Bridge retrofit 
prioritisation for ageing transportation networks subject to seismic hazards," 
Structure and Infrastructure Engineering, vol. 9, no. 10, pp. 1050-66, 2013.  

[59]  M.  L.  Putterman,  Markov  Decision  process:  discrete  stochastic  dynamic 

programming, Wiley, 1994.  

[60]  E. Sondik, The optimal control of partially observable Markov processes, 

Stanford University, Stanford Electronics Labs, 1971.  

[61]  K.  G.  Papakonstantinou,  C.  P.  Andriotis  and  M.  Shinozuka,  "POMDP 
solutions  for  monitored  structures,"  in  IFIP  WG-7.5  Conference  on 
Reliability and Optimization of Structural Systems, 2016.  

[62]  C.  P.  Andriotis,  K.  G.  Papakonstantinou  and  E.  N.  Chatzi,  "Value  of 
stochastic 

in  partially  observable 

structural  health 
information 
environments," Structural Safety, Under review, 2020.  

[63]  K.  G.  Papakonstantinou,  C.  P.  Andriotis,  H.  Gao  and  E.  N.  Chatzi, 
"Quantifying the value of structural health monitoring for decision making," 
in  13th  International  Conference  on  Applications  of  Statistics  and 
Probability in Civil Engineering (ICASP13), Seoul, South Korea, 2019.  

[64]  R.  Bellman,  "Dynamic  programming  and  Lagrange  multipliers," 
Proceedings of the National  Academy of Sciences of the  United States of 
America, vol. 42, no. 10, p. 767, 1956 .  

[65]  D. Bertsekas, Nonlinear Programming, Athena Scientific, 1999.  

[66]  R. Uryasev and S. Rockafellar, "Conditional value-at-risk for general loss 
distributions,"  Journal of  Banking and Finance,  vol. 26,  no. 7, pp. 1443-
1471, 2002.  

[67]  D.  Di  Castro,  A.  Tamar  and  S.  Mannor,  "Policy  gradients  with  variance 

related risk criteria," arXiv preprint arXiv:1206.6404, 2012.  

[68]  L.  A.  Prashanth  and  M. Ghavamzadeh,  "Variance-constrained  actor-critic 
algorithms for discounted and average reward MDPs," Machine Learning, 
vol. 105, no. 3, pp. 367-417, 2016.  

[69]  A. E. Smith, D. W. Coit, T. Baeck, D. Fogel and Z. Michalewicz, "Penalty 
functions," Handbook of Evolutionary Computation, vol. 1, p. 97, 1995.  

[70]  C.  P.  Andriotis  and  K.  G.  Papakonstantinou,  "Extended  and  generalized 
fragility functions," Journal of Engineering Mechanics, vol. 144, no. 9, p. 
04018087, 2018.  

[71]  C.  P.  Andriotis  and  K.  G.  Papakonstantinou,  "Probabilistic  structural 
performance  assessment  in  hidden  damage  spaces,"  in  Proceedings  of 
Computational  Stochastic  Mechanics  Conference  (CSM),  Paros,  Greece, 
2018.  

[72]  P. G. Morato, K. G. Papakonstantinou, C. P. Andriotis, J. S. Nielsen and P. 
Rigo,  "Optimal  inspection  and  maintenance  planning  for  deteriorating 
structures  through  dynamic  Bayesian  networks  and  Markov  decision 
processes," Structural Safety, Under review, 2020.  

[73]  P.  G.  Morato,  J.  S.  Nielsen,  A.  Q.  Mai  and  P.  Rigo,  "POMDP  based 
maintenance  optimization  of  offshore  wind  substructures 
including 
monitoring," in 13th International Conference on Applications of Staitstics 
and Probability in Civil Engineering (ICASP13), Seoul, South Korea, 2019.  

[74]  D.  P.  Kingma  and  J.  Ba,  "Adam:  A  method  for  stochastic  optimization," 

arXiv preprint arXiv:1412.6980, 2014.  

[36]  J.  K.  Gupta,  M.  Egorov  and  M.  Kochenderfer,  "Cooperative  multi-agent 
control using deep reinforcement learning," in International Conference on 
Autonomous Agents and Multiagent Systems, 2017.  

[37]  B. Baker, I. Kanitscheider, T. Markov, Y. Wu, G. Powell, B. McGrew and 
I.  Mordatch,  "Emergent  tool  use  from  multi-agent  autocurricula,"  arXiv 
preprint arXiv:1909.07528, 2019.  

[38]  A. Oroojlooyjadid and D. Hajinezhad, "A review of cooperative multi-agent 
deep reinforcement learning," arXiv preprint arXiv:1908.03963, 2019.  

[39]  P. Hernandez-Leal, B. Kartal and M. E. Taylor, "A survey and critique of 
multiagent  deep  reinforcement  learning,"  Autonomous  Agents  and  Multi-
Agent Systems, vol. 33, no. 6, pp. 750-97, 2019.  

[40]  J. A. Goulet, A. Der Kiureghian and B. Li, "Pre-posterior optimization of 
sequence  of  measurement  and  intervention  actions  under  structural 
reliability constraint," Structural Safety, vol. 52, pp. 1-9, 2015.  

[41]  J.  D.  Sørensen,  "Framework  for  risk‐based  planning  of  operation  and 
maintenance  for  offshore  wind  turbines,"  Wind  Energy:  An  International 
Journal  for  Progress  and  Applications  in  Wind  Power  Conversion 
Technology, vol. 12, no. 5, pp. 493-506, 2009.  

[42]  E. Altman, Constrained Markov decision processes, CRC Press, 1999.  

[43]  P.  Poupart,  A.  Malhotra,  P.  Pei,  K.  Kim,  B.  Goh  and  M.  Bowling, 
"Approximate  linear  programming  for  constrained  partially  observable 
Markov decision processes," in 29th Conference of the Association for the 
Advancement of Artificial Intelligence (AAAI), 2015.  

[44]  J.  D.  Isom,  S.  P.  Meyn  and  R.  D.  Braatz,  "Piecewise  Linear  Dynamic 
the 

Programming  for  Constrained  POMDPs," 
in  Association 
Advancement of Artificial Intelligence (AAAI) Conference, 2008.  

for 

[45]  D. Kim, J. Lee, K. E. Kim and P. Poupart, "Point-based value iteration for 
constrained POMDPs," in 22nd International Joint Conference on Artificial 
Intelligence, 2011.  

[46]  E.  Walraven  and  M.  T.  Spaan,  "Column  generation  algorithms  for 
constrained POMDPs," Journal of Artificial Intelligence Research, vol. 62, 
pp. 489-533, 2018.  

[47]  J. Schulman, S. Levine, P. Abbeel, M. Jordan and P. Moritz, "Trust region 
policy  optimization,"  in  International  Conference  on  Machine  Learning, 
2015.  

[48]  J.  Achiam,  D.  Held,  A.  Tamar  and  P.  Abbeel,  "Constrained  policy 
optimization,"  in  34th  International  Conference  on  Machine  Learning, 
2017.  

[49]  Y.  Zhang,  Q.  Vuong and  K.  W. Ross, "First  order  optimization  in policy 
space  for  constrained  deep  reinforcement  learning,"  arXiv  preprint 
arXiv:2002.06506, 2020.  

[50]  C.  Tessler,  D.  J. Mankowitz  and  S.  Mannor,  "Reward  constrained  policy 

optimization.," arXiv preprint arXiv:1805.11074, 2018.  

[51]  X.  B.  Peng,  P.  Abbeel,  S.  Levine  and  M.  Van  de  Panne,  "Deepmimic: 
Example-guided  deep  reinforcement  learning  of  physics-based  character 
skills," ACM Transactions on Graphics, vol. 37, no. 4, pp. 1-4, 2018.  

[52]  J. Garcıa and F. Fernández, "A comprehensive survey on safe reinforcement 
learning," Journal of Machine Learning Research, vol. 16, no. 1, pp. 1437-
80, 2015.  

[53]  Y. Chow, M. Ghavamzadeh, L. Janson and M. Pavone, "Risk-constrained 
reinforcement learning with percentile risk criteria," The Journal of Machine 
Learning Research, vol. 18, no. 1, pp. 6070-120, 2017.  

[54]  D. Bertsekas, Dynamic programming and optimal control, vol. 1, Belmont, 

MA: Athena Scientific, 2005.  

[55]  R.  P.  Nicolai  and  R.  Dekker,  "Optimal  maintenance  of  multi-component 
systems: A review," in Complex System Maintenance Handbook, Springer, 
London, 2008, pp. 263-286. 

17 

 
 
 
