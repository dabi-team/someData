Beneﬁcial Perturbation Network for designing
general adaptive artiﬁcial intelligence systems

Shixian Wen∗, Amanda Rios†§, Yunhao Ge∗ § and Laurent Itti∗ † ‡
∗Department of Computer Science
University of Southern California, Los Angeles, California 90089
Email: shixianw@usc.edu
† Neuroscience Graduate Program, University of Southern California
‡Department of psychology, University of Southern California
§Contributed equally as the second authors

1
2
0
2

b
e
F
2

]

V
C
.
s
c
[

2
v
4
5
9
3
1
.
9
0
0
2
:
v
i
X
r
a

Abstract—The human brain is the gold standard of adaptive
learning. It not only can learn and beneﬁt from experience,
but also can adapt to new situations. In contrast, deep neural
networks only learn one sophisticated but ﬁxed mapping from
inputs to outputs. This limits their applicability to more dynamic
situations, where the input to output mapping may change with
different contexts. A salient example is continual
learning -
learning new independent tasks sequentially without forgetting
previous tasks. Continual learning of multiple tasks in artiﬁcial
neural networks using gradient descent leads to catastrophic
forgetting, whereby a previously learned mapping of an old task
is erased when learning new mappings for new tasks. Here,
we propose a new biologically plausible type of deep neural
network with extra, out-of-network, task-dependent biasing units
to accommodate these dynamic situations. This allows, for the
ﬁrst time, a single network to learn potentially unlimited parallel
input to output mappings, and to switch on the ﬂy between
them at runtime. Biasing units are programmed by leveraging
beneﬁcial perturbations (opposite to well-known adversarial per-
turbations) for each task. Beneﬁcial perturbations for a given
task bias the network toward that task, essentially switching the
network into a different mode to process that task. This largely
eliminates catastrophic interference between tasks. Our approach
is memory-efﬁcient and parameter-efﬁcient, can accommodate
many tasks, and achieves state-of-the-art performance across
different tasks and domains.

Keyword: Adaptive artiﬁcial intelligence system | Switch modes |
Beneﬁcial perturbations | Continual learning | Adversarial examples

I. INTRODUCTION
The human brain is the benchmark of adaptive learning. While
interacting with new environments that are not fully known to an
individual, it is able to quickly learn and adapt its behavior to achieve
goals as well as possible, in a wide range of environments, situations,
tasks, and problems. In contrast, deep neural networks only learn
one sophisticated but ﬁxed mapping between inputs and outputs,
thereby limiting their application in more complex and dynamic
situations in which the mapping rules are not kept the same but change
according to different tasks or contexts. One of the failed situations
is continual learning - learning new independent tasks sequentially
without forgetting previous tasks. In the domain of image classiﬁcation,
for example, each task may consist of learning to recognize a small
set of new objects. A standard neural network only learns a ﬁxed
mapping rule between inputs and outputs after training on each task.
Training the same neural network on a new task would destroy the
learned ﬁxed mapping of an old task. Thus, current deep learning
models based on stochastic gradient descent suffer from so-called
"catastrophic forgetting" [35, 14, 46], in that they forget all previous
tasks after training each new one.

Here, we propose a new biological plausible (Discussion) method —
Beneﬁcial Perturbation Network (BPN) — to accommodate these
dynamic situations. The key new idea is to allow one neural network
to learn potentially unlimited task-dependent mappings and to switch
between them at runtime. To achieve this, we ﬁrst leverage existing
lifelong learning methods to reduce interference between successive
tasks (Elastic Weight Consolidation, EWC [23], or parameter super-
position, PSP [4]). We then add out-of-network, task-dependent bias
units, to provide per-task correction for any remaining parameter drifts
due to the learning of a sequences of tasks. We compute the most
beneﬁcial biases — beneﬁcial perturbations — for each task in a
manner inspired by recent work on adversarial examples. The central
difference is that, instead of adding adversarial perturbations that
can force the network into misclassiﬁcation, beneﬁcial perturbations
can push the drifted representations of old tasks back to their initial
task-optimal working states (Fig. 1).

Fig. 1. With BPN, one can switch at runtime the network parameters that
are global optimal for each task. Training trajectories are illustrated in loss
and parameter space. The green curve shows loss as a function of network
parameters for a ﬁrst task A, with optimal parameters shown by the green
circle. The purple curve and circle correspond to a second task B. Training
ﬁrst task A then task B with stochastic gradient descend (SGD, without any
constraints on parameters, gray) leads to optimal parameters for task B (purple
circle), but those are destructive for task A. When, instead, learning task
B using EWC or PSP (have some constraints on parameters, yellow), the
solution is a compromise that can be sub-optimal for both tasks (black circle).
Beneﬁcial perturbations (blue curve for task A, red curve for task B) push the
representation learned by EWC or PSP back to their task-optimal states.

Traditional learning e.g, SGDContinual learning e.g, EWC or PSPBeneficial perturbations  for task A (BPA)EWC or PSPSGDBPABPBParameter ѲLoss Ltask Atask BBeneficial perturbations  for task B (BPB) 
 
 
 
 
 
Fig. 2. Concept: Type 1 - constrain the network weights while training the new task: (a) Retraining models such as elastic weight consolidation [23]: retrains
the entire network learned on previous tasks while using a regularizer to prevent drastic changes in the original model. Type 2 - expanding and retraining
methods (b-c); (b) Expanding models such as progressive neural networks [45] expand the network for new task t without any modiﬁcations to the network
weights for previous tasks. (c) Expanding model with partial retraining such as dynamically expandable networks [53] expand the network for new task t with
partial retraining on the network weights for previous tasks. Type 3 - episodic memory methods (d): Methods such as Gradient Episodic Memory [31] store a
subset of the original dataset from previous tasks into the episodic memory and replays them with new data during the training of new tasks. Type 4 - Partition
network (e): these use context or mask matrices to partition the core network into several sub-networks for different tasks [4, 54, 33, 8, 52]. Type 5 - beneﬁcial
perturbation methods (f): Beneﬁcial perturbation networks create beneﬁcial perturbations which are stored in bias units for each task. Beneﬁcial perturbations
bias the network toward that task and thus allow the network to switch into different modes to process different independent tasks. It retrains the normal weights
learned from previous tasks using elastic weight consolidation [23] or parameter superposition [4]. (g) Strengths and weaknesses for each type of method.

There are three major beneﬁts of BPN: 1) BPN is memory and
parameter efﬁcient: to demonstrate it, we validate our BPN for
continual learning on incremental tasks. We test it on multiple
public datasets (incremental MNIST [27], incremental CIFAR-10 and
incremental CIFAR-100 [25]), on which it achieves better performance
than the state-of-the-art. For each task, by adding bias units that store
beneﬁcial perturbations to every layer of a 5-layer fully connected
network, we only introduce a 0.3% increase in parameters, compared
to a 100% parameter increase for models that train a separate network,
and 11.9% - 60.3% for dynamically expandable networks [53]. Our
model does not need any episodic memory to store data from the

previous tasks and does not need to replay them during the training
of new tasks, compared to episodic memory methods [41, 31, 40, 42].
Our model does not need large context matrices, compared to partition
methods [4, 54, 52, 33, 8, 11, 48, 34]. 2) BPN achieves state-of-the-art
performance across different datasets and domains: to demonstrate
it, we consider a sequence of eight unrelated object recognition
datasets (Experiments). After training on the eight complex datasets
sequentially, the average test accuracy of BPN is better than the state-
of-the-art. 3) BPN has capacity to accommodate a large number of
tasks: to demonstrate it, we test a sequence of 100 permuted MNIST
tasks (Experiments). A variant of BPN that uses PSP to constrain the

All data from task t -1All data from task tt  - 1tTasks timelineTraining dataType 1: constrain the network weights for previous tasksAll data from task tA portion of data from task 1A portion of data from task 2A portion of data from task tTrain task t with episodic memoryType 3: store a subset of original datasetAll data from task t -1All data from task tt  - 1tTasks timelineTraining dataAll data from task t -1All data from task tt  - 1tTasks timelineTraining dataType 2: expand the network capacity for new task and constrain the network weights for previous tasksContext or maskgeneratorType 4: Using context or mask matrices to create severalsubspaces or sub-networks in a big network for different tasksRotator (context) or filter (mask)Type 5: Our Method: beneficial perturbation network create beneficial perturbations for new task (provide bias toward that task to switch modes) and constrain the network weights for previous tasksAll data from task t -1All data from task tt  - 1tTasks timelineTraining dataLegendsFixed neuronFixed weightRetrained neuronRetrained  weightMultiplierBias units for task t - 1Bias units for task t a)b)c)d)e)f)Type 1Type 2Type 3Type 4Type 5(ours)Parameter efficientMemory efficientMultiple input tooutput mappingsDoes not requirespervious task dataunlimited capacity to accomondate new tasks Strengths and weaknesses for each typeg)t  - 1tTasks timelineContext or mask matricesAll data from task t -1All data from task tt  - 1tTasks timelineTraining datanormal network achieves 30.14% better performance than the second
best, the original PSP [4], a partition method which performs well in
incremental tasks and eight object recognition tasks. Thus, BPN has
a promising future to solve continual learning compared to the other
types of methods.

To lay out the foundation of our approach we start by introducing
the following key concepts: Sec. II: Different types of methods
for enabling lifelong learning; Sec. III: Adversarial directions and
perturbations; Sec. IV: Beneﬁcial directions and perturbations, and
the effects of beneﬁcial perturbations in sequential learning scenarios;
Sec. V: Structure and updating rules for BPN.

We then present experiments (Sec. VI), results (Sec. VII) and

discussion (Sec. VIII).

II. TYPES OF METHODS FOR ENABLING LIFELONG
LEARNING

Four major types of methods have been proposed to alleviate
catastrophic forgetting. Type 1: constrain the network weights to
preserve performance on old tasks while training the new task [23,
28, 1] (Fig. 2a); A famous example of type 1 methods is EWC [23].
EWC constrains certain parameters based on how important they are
to previously seen tasks. The importance is calculated from their
task-speciﬁc Fisher information matrix. However, solely relying on
constraining the parameters of the core network eventually exhausts
the core network’s capacity to accommodate new tasks. After learning
many tasks, EWC cannot learn anymore because the parameters
become too constrained (see Results). Type 2: dynamic network
expansion [30, 28, 45, 53] creates new capacity for the new task,
which can often be combined with constrained network weights for
previous tasks (Fig. 2b-c); However, this type is not scalable because
it is not parameter efﬁcient (e.g., 11.9% - 60.3% additional parameters
per task for dynamically expandable networks [53]). Type 3: using an
episodic memory [41, 31, 40] to store a subset of the original dataset
from previous tasks, then rehearsing it while learning new tasks to
maintain accuracy on the old tasks (Fig. 2d). However, this type is
not scalable because it is neither memory nor parameter efﬁcient. All
three approaches attempt to shift the network’s single ﬁxed mapping
initially obtained by learning the ﬁrst task to a new one that satisﬁes
both old and new tasks. They create a new, but still ﬁxed mapping
from inputs to outputs across all tasks so far, combined. Type 4:
Partition Network: using task-dependent context [4, 54, 52, 34] or
mask matrices [33, 8, 8, 11, 48] to partition the original network into
several small sub-networks (Fig. 2e, ﬂow chart - Fig. S1a). Zeng et al.
[54] used context matrices to partition the network into independent
subspaces spanned by rows in the weight matrices to avoid interference
between tasks. However, context matrices introduce as many additional
parameters as training a separate neural network for each new task
(additional 100% parameters per task). To reduce parameter costs,
Cheung et al. proposed binary context matrices [4], further restricted
to diagonal matrices with -1 and 1 values. The restricted context
matrices [54] (1 and -1 values) behave similarly to mask matrices
[33] (0 and 1 values) that split the core network into several sub-
networks for different tasks. With too many tasks, the core network
would eventually run out of capacity to accommodate any new task,
because there is no vacant route or subspace left. Although type 4
methods create multiple input to output mappings for different tasks,
many of these methods are too expensive in terms of parameters, and
none of them has enough capacity to accommodate numerous tasks
because methods such as PSP run out of unrealized capacity of the
core network.

In marked contrast to the above artiﬁcial neural network methods,
here, we propose a fundamentally new ﬁfth type (Fig. 2f, ﬂow
chart - Fig. S1 b): We add out-of-network, task-dependent bias units
to neural network. Bias units enable a neural network to switch
into different modes to process different independent tasks through
beneﬁcial perturbations (the memory storage cost of these new bias

units is actually lower than the cost of adding a new mask or context
matrix). With only an additional 0.3% of parameters per mode 1, this
structure allows BPN to learn potentially unlimited task-dependent
mappings from inputs to outputs for different tasks. The strengths
and weaknesses of each type are in Fig. 2g.

III. ADVERSARIAL DIRECTIONS AND PERTURBATIONS
Three spaces of a neural network are important for this and the
following sections: The input space is the space of input data (e.g.,
pixels of an image); the parameter space is the space of all the
weights and biases of the network; the activation space is the space
of all outputs of all neurons in all layers in the network.

By adding a carefully computed “noise” (adversarial perturbations)
to the input space of a picture, without changing the neural network,
one can force the network into misclassiﬁcation. The noise is usually
computed by backpropagating the gradient in a so-called “adversarial
direction” such as by using the fast gradient sign method (FGSD) [49].
For example, consider a task of recognizing handwritten digits "1"
versus "2". Adversarial perturbations aimed at misclassifying an image
of digit 2 as digit 1 may be obtained by backpropagating from the class
digit 1 to the input space, following any of the available adversarial
directions. In Fig. 3a, adding adversarial perturbations to the input
image can be viewed as adding an adversarial direction vector (gray
arrows AD) to the clear (non-perturbated) input image of digit 2.
The resulting vector crosses the decision boundary. Thus, adversarial
perturbations can force the neural network into misclassiﬁcation, here
from digit 2 to digit 1. Because the dimensionality of adversarial
directions is around 25 for MNIST [49], when we project them
into a 2D space, we use the fan-shaped gray arrows to depict those
dimensions.

IV. BENEFICIAL DIRECTIONS AND PERTURBATIONS, & THE
EFFECTS OF BENEFICIAL PERTURBATIONS IN MULTITASK
SEQUENTIAL LEARNING SCENARIO

In this section, we ﬁrst introduce the deﬁnition of beneﬁcial direc-
tions and beneﬁcial perturbations. Then, we explain why beneﬁcial
perturbations can help a network recover from a parameter drifting of
old tasks after learning new tasks and can push task representations
back to their initial task-optimal working region.

We consider two incremental digits recognition tasks; Task A
(recognizing 1s and 2s) and Task B (recognizing 3s and 4s). Attack
and defense researchers usually view adversarial examples as a curse
of neural networks, but we view it as a gift to solve continual
learning. Instead of adding input "noise" (adversarial perturbations)
to the input space calculated from other classes to force the network
into misclassiﬁcation, we add "noise" to the activation space, using
beneﬁcial perturbations stored in bias units added to the parameter
space (Supplementary Fig. S1b) calculated by the input’s own correct
class to assist in correct classiﬁcation. To understand beneﬁcial
perturbations, we ﬁrst explain beneﬁcial directions. Beneﬁcial di-
rections are vectors that point toward the direction of high conﬁdence
classiﬁcation region for each class (Fig. 3b); BD1 (BD2) are the
beneﬁcial directions that point to the high conﬁdence classiﬁcation
region of digit 1 (digit 2). The point A1 represents the activation of
the normal neurons of each layer generated from an input image of
task A. A1 + BD1 (A1 + BD2) pushes the activation A1 across the
decision boundary of R2 (R1) and toward R1_high (R2_high). Thus,
the network would classify the A1 + BD1 (A1 + BD2) as digit 1
(2) with high conﬁdence. To overcome catastrophic forgetting, we
create some beneﬁcial perturbations for each task and store them in
task-dependent bias units (Fig. 4, Supplementary Fig. S1b). Beneﬁcial
perturbations allow a neural network to operate in different modes
by biasing the network toward that particular task, even though the

1Check supplementary discussion for more information about additional

parameter costs

Fig. 3. Deﬁning adversarial perturbations in input space vs. beneﬁcial perturbations in activation space. We consider two digits recognition tasks; Task A
(recognizing 1s and 2s) and Task B (recognizing 3s and 4s). (a) Adversarial directions (AD). Adding adversarial perturbations (calculated from digits 1) to
input digits 2 can be viewed as adding an adversarial direction vector (gray arrow) to the clear input image of digit 2 in the input space. Thus, the network
misclassiﬁes the clear input image of digit 2 as digit 1. Beneﬁcial directions are not operated as adding beneﬁcial perturbations to the clear input image of
digit 2 in the input space to assist the correct classiﬁcation (orange arrow). (b) Beneﬁcial directions (class speciﬁc) for each class of task A . R1 (R2)
is the classiﬁcation region (region of constant estimated label) of digit 1 (digit 2) from the MNIST dataset. Subregion R1_high (R1_low) is the high (low)
conﬁdence classiﬁcation region of digit 1, and likewise for R2_high (R2_low) for digit 2. The point A1 is the activations of normal neurons of each layer
from an input image of task A. It lies in the intersection of R1_low and R2_low. BD1 (BD2) are beneﬁcial directions for class digit 1 (digit 2). A1 + BD1,
blue arrows,(A+BD2, red arrows) pushes the activation A1 across the decision boundary of R2 (R1) and towards R1_high (R2_high). Thus, the network
classiﬁes A1 + BD1 (A1 + BD2) as digit 1 (digit 2) with high conﬁdence. (c) After training task B, beneﬁcial perturbations (task speciﬁc) for task A
push the drifted representation of inputs from task A back to its initial optimal working region of task A. R3 (R4 ) is the classiﬁcation region (region
of constant estimated label) of digit 3 (digit 4) from the MNIST dataset. BD1 (BD2) is a beneﬁcial direction for digit 1 (digit 2). During the training of task
A, the network has been trained on two images from digit 1 (1a and 1b) and two images from digit 2 (2a and 2b). Thus, the beneﬁcial perturbations for task
A are the vector (BDa
1 which lies inside of the
classiﬁcation regions of task B (R2 or R3). The drifted point A(cid:48)
1 alone cannot be correctly classiﬁed as digit 1 or 2 because it lies outside of the classiﬁcation
region of task A (R1 or R2). At test time, adding beneﬁcial perturbations for task A to the activations of A(cid:48)
1, can drag it back the correct classiﬁcation
regions for task A (intersection of R1 and R2). Thus, it biases the network’s outputs toward the correct classiﬁcation region and push task representations back
to their initial task-optimal working region.

1). After training task B, with gradient descent, point A1 in b) is drifted to the A(cid:48)

2 + BDa

1 + BDb

2 + BDb

2 + BDb

1 + BDb

2 + BDa

1 which lies inside the classiﬁcation regions for task B (R3

shared normal weights become contaminated by other tasks. The
beneﬁcial perturbations for each task are created by aggregating the
beneﬁcial direction vectors sequentially for each class through mini-
batch backpropagation. For example, during the training of task A,
the network has been trained on two images from digit 1 (1a and 1b)
and two images from digit 2 (2a and 2b). The beneﬁcial perturbations
for task A are the summation of the beneﬁcial directions calculated
1 in Fig. 3c, BDj
from each image (BDa
i
is the beneﬁcial direction for sample j in class i). During the training
of task B, with gradient descent, the point A1 (Fig. 3b) is drifted to
(cid:83) R4).
A(cid:48)
The drifted A(cid:48)
1 alone cannot be classiﬁed as digit 1 or 2 since it lies
(cid:83) R2). However,
outside of the classiﬁcation regions of task A (R1
during testing of task A, after training task B, adding beneﬁcial
perturbations for task A to the drifted activation (A(cid:48)
1) drags it back to
(cid:83) R2 in Fig. 3c).
the correct classiﬁcation regions for task A ( R1
Thus, beneﬁcial perturbations bias the neural network toward that task
and push task representations back to their initial task-optimal working
region. Note that in this work we focus on adding more compact
beneﬁcial perturbations to the activation space, as adding perturbations
to the input space has already been explored in adversarial attack
methods, and adding perturbations to the parameter space is unlikely
to be scalable due to the very large number of parameters in a typical
neural network.

V. BENEFICIAL PERTURBATION NETWORK

We implemented two variants of BPN: BD + EWC and BD + PSP
(Experiments). The backbone - BD (updating extra out-of-network
bias units in beneﬁcial directions to create beneﬁcial perturbations)
is the same for both methods. The only difference is BD + EWC
(BD + PSP) uses EWC (PSP) method to retrain the normal weights
while attempting to minimize disruption of old tasks. Here, we
choose BD + EWC to explain our method (for BD + PSP, see
Supplementary). We use a scenario with two tasks for illustration;
task A - recognizing MNIST digit 1s, 2s, task B - recognizing MNIST
digit 3s, 4s. BPN has task-dependent bias units (BIASi
t ∈ R1×K ,
K is the number of normal neurons in each layer, i is the layer
number, and t is the task number) in each layer to store the beneﬁcial
perturbations. The beneﬁcial perturbations are formulated as an
additive contribution to each layer’s weighted activations. Unlike most
adversarial perturbations, beneﬁcial perturbations are not speciﬁc to
each example, but are applied to all examples in each task (Fig. 3 c,
d). We deﬁne beneﬁcial perturbations as a task-dependent bias term:

V i+1 = σ(W iV i + bi + BIASi

t) ∀ i ∈ [1, n]

(1)

where V i is the activations at layer i, W i is the normal weights at
layer i, BIASi
t is the task dependent bias units at layer i for task
t, σ(·) is the nonlinear activation function at each layer, bi is the

21_low1_high2_low2_high121A1b)Beneficial directions (BD, class specific)for digits 1 and 2 in activation space21112221ADBD212a)Adversarial directions (AD)for digits 2 in input spacec)Beneficial perturbations for task A (BPA , task specific)  =BD2a  + BD1a + BD2b + BD1bA’1212BPAR1_lowR2_lowR4R1R1_highR2R2_highR3R1 - classification region for class digit 1R2 - classification region for class digit 2R3 - classification region for class digit 3 R4 - classification region for class digit 4R1 Ս R2  - work space for task A: R3 Ս R4 - Work space for task B R3_highR4_hightask A: recognizing digits 1s and 2stask B: recognizing digits 3s and 4sA1 - activations for an input image of task AA’1 - Drifted activations of A1   after training task B Legend for panel b) and c)BD1  -  beneficial direction     for class digit 1BD2 -  beneficial direction    for class digit 2BPA -  beneficial perturbations for task ABeneﬁcial perturbation network (BD + EWC or BD + PSP variant) with two tasks. (a) Structure of beneﬁcial perturbation network. (b) Train
Fig. 4.
on task A. Backpropagating through the network to bias units for tasks A in beneﬁcial direction (FGSD) using input’s own correct class (digits label 1 and 2),
normal weights (gradient descent). (c) Test on task A. Feed the input images to the network. Activating bias units for task A and adding the stored beneﬁcial
perturbations to the activations. The beneﬁcial perturbations bias the network to mode on classifying digits 1, 2 task. (d) Train on task B. Backpropagating
through the network to bias units for tasks B in beneﬁcial direction (FGSD) using input’s own correct class (digits label 3 and 4), normal weights (constrained
by EWC or PSP). (e) Test on task B. Feed the input images to the network. Activating bias units for task B and adding the stored beneﬁcial perturbations to
the activations. The beneﬁcial perturbations bias the network to mode on classifying digits 3, 4 task.

normal bias term at layer i, n is the number of layers. For a simple
fully connected network (Fig. 4 a), the forward functions are:

V 1 = σ(W 1Xt + b1 + BIAS1
t )

V 2 = σ(W 2V 1 + b2 + BIAS2
t )

y = Sof tmax(W 3V 2 + b3 + BIAS3
t )

(2)

(3)

(4)

t ∈ R1×H and W i

where y is the output logits, Xt is the input data for task t, Sof tmax
is the normalization function, other notations are the same as in Eqn. 1.
During the training of a speciﬁc task, the bias units are the product
of two terms2: M i
t ∈ RH×K (H is the hidden
dimension (a hyper-parameter), K is the number of normal neurons
in each layer, and t is the task number). After training a speciﬁc task,
we discard both M i
t , and only keep their product BIASi
t,
reducing memory and parameter costs to a negligible amount (0.3%
increase for parameters per task, and 4*K Bytes increase per layer per
task, it is just a bias term). After training on different sequential tasks,
at test time, the stored beneﬁcial perturbations from the speciﬁc bias
units can bias the neural network outputs to each task. Thus, these
allow the BPN to switch into different modes to process different
tasks. We use the forward and backward rules (Alg. 1, Alg. 2) to
update the BPN.

t and W i

For training, ﬁrst, during the training of task A, our goal is to
maximize the probability P (y = yA|XA, W i, BIASi
∀ i ∈
[1, n] by selecting the bias units corresponding to tasks A . Thus, we
set up our optimization function as:

A)

W i, BIASi

A = arg min
W i, BIASi
A
− log [ P (y = yA|XA, W i, BIASi

A) ]

(5)

∀ i ∈ [1, n]

A

L(M i

where yA is the true label for data in task A (MNIST input images
1, 2), XA is the data for task A, other notations are the same as
notations in Eqn. 1. We update M i
A in the beneﬁcial direction (FGSD)
A, yA)) to generate beneﬁcial perturbations for
as (cid:15)sign(∇M i
task A, where M i
A are the ﬁrst term of bias units for task A. We
update W i
A (the second term of bias units for task A) in the gradient
direction. The factorization allows the bias units for task A to better
learn the beneﬁcial perturbations for task A (a vector towards the
work space of task A that has non-negligible network response for
MNIST digits 1, 2, similar to Fig. 3b, c ). We use a softmax cross
entropy loss to optimize Eqn. 1. After training task A, the bias units
for task A (BIASi
A. We discard
M i
A and W i
A to reduce the memory storage and parameter costs and
freeze the BIASi
A to ensure that the beneﬁcial perturbations are not
being corrupted by other tasks (Task B). Then, we discard all of the
MNIST input images 1, 2 because all of the information is stored
inside the bias units for task A and we do not need to replay these
images when we train on the following sequential tasks.

A) are the product of M i

A and W i

2the factorization provides more degrees of freedom to better learn the

beneﬁcial perturbations [17, 7]

After training task A, during the training of task B (Fig. 4 d), our
B)

goal is to maximize the probability P (y = yB|XB, W i, BIASi

V1V2W1W2W3BIAS1BBIAS2BBIAS1ABIAS2AXyTraining on task AGround truth1212...22SGDFGSDFGSDinput imagesTesting on task Ainput imagesTesting on task BTraining on task BGround truth4334...34EWC or PSPFGSDFGSDinput imagesinput imagesBias units for task A (float tensor)Bias units for task B (float tensor)Bias units pools that are notused for the current taskAdding beneficial perturbationsfrom corresponding bias units to normal neuron for the current tasksNormal neuron unitsNormal Weight WLegendTrainingTestinga)b)c)d)e)∀ i ∈ [1, n] by selecting the bias units corresponding to tasks B. To
minimize the disruption for task A, we apply EWC or PSP constraints
on normal weights. We set up our optimization function as

W i, BIASi

B = arg min
W i, BIASi
B

− log [ P (y = yB|XB, W i, BIASi
∀ i ∈ [1, n]

B) ] + EW C(W i)

(6)

where yB is the true label for data in task B (MNIST input images 3,
4), XB is the data for task B, EW C(·) is the EWC constraint [23]
on normal weights, other notations are the same as in Eqn. 1. In the
loss function of Alg. 2, λFj(Wj − W A∗
)2 is the EWC constraint on
the normal weights, where j labels each parameter, Fj is the Fisher
information matrix for each parameter j (determine which parameters
are most important for a task [23]), λ sets how important the old task
is compared to the new one, Wj is normal weight j, and W A∗
is
the optimal normal weight j after training on task A. Apart from the
additional EWC constraint, training task B and all subsequent tasks
then simply proceeds in the same manner as for task A above.

j

j

For testing, after training task B, we test the accuracy for task A
on a test set by manually activating the bias units corresponding to
task A (Fig. 4 c, Alg. 1). Although the shared normal weights have
been contaminated by task B, the integrity of bias units for task A that
store the beneﬁcial perturbations still can bias the network outputs
to task A (set the network into a mode to process input from task
A, see Results). In another word, the task-dependent bias units can
still maintain a high probability – P (y = yA|XA, W i, BIASi
A)
for task A. During testing of task B, we test the accuracy for task
B on a test set by manually activating the bias units corresponding
to task B (Fig. 4 e, Alg. 1). The bias units for task B can bias
the network outputs to task B and maintain a high probability –
P (y = yB|XB, W i, BIASi
B) for task B, in case the shared normal
weights are further modiﬁed by later tasks. In scenarios with more
than two tasks, the forward and backward algorithms for later tasks
are the same as for task B, except that they will select and update
their own bias units.

In sum, beneﬁcial perturbations act upon the network not by
adding biases to the input data (like adversarial examples do, Fig. 3a),
but instead by dragging the drifted activations back to the correct
working region in activation space for the current task ( Fig. 1 and
Fig. 3 c). The intriguing properties of task-dependent beneﬁcial
perturbations on maintaining high probabilities for different tasks
can further be explained in two ways. The beneﬁcial perturbations
from the bias units can be viewed as features that capture how
"furry" the images are for task A (or B). Olshausen et al. [4] showed
that training a neural network only on these features is sufﬁcient
to make correct classiﬁcation on the dataset that generates these
features. They argued that these features have sufﬁcient information
for a neural network to make correct classiﬁcation. In our continual
learning scenarios, although the shared normal weights (W i) have
been contaminated after the sequential training of all tasks, by
activating corresponding bias units, the task-dependent bias units
still have sufﬁcient information to bias the network toward that
task. In other words, the task-dependent bias units can maintain
high probabilities – P (y = yA|XA, W i, BIASi
A) for task A
or P (y = yB|XB, W i, BIASi
B) for task B . Thus, bias units
can assist the network to make correct classiﬁcation. In addition,
Elsayed et al. [9] showed how a carefully computed adversarial
perturbations for each new task embedded in the input space can
repurpose machine learning models to perform a new task. Here, these
beneﬁcial perturbations can be viewed as task-dependent beneﬁcial
"programs"[9] in the parameter space. Once activated, these task-
dependent "programs" can maximize the probability for corresponding
tasks.

VI. EXPERIMENTS

A. Experimental Setup For Incremental Tasks

To demonstrate that BPN is very parameter efﬁcient and can learn
different tasks in an online and continual manner, we used a fully-
connected neural network with 5 hidden layers of 300 ReLU units. We
tested it on three public computer vision datasets with "single-head
evaluation", where the output space consists of all the classes from
all tasks learned so far.

1. Incremental MNIST. A variant of the MNIST dataset [27] of
handwritten digits with 10 classes, where each task introduces a new
set of classes. We consider 5 tasks; each new task concerns examples
from a disjoint subset of 2 classes.

2. Incremental CIFAR-10. A variant of the CIFAR object
recognition dataset [25] with 10 classes. We consider 5 tasks; each
new task has 2 classes.

3. Incremental CIFAR-100. A variant of the CIFAR object
recognition dataset [25] with 100 classes. We consider 10 tasks; each
new task has 2 classes. We use 20 classes for CIFAR-100 experiment.

B. Experimental Setup For Eight Sequential Object Recognition
Tasks

To demonstrate the superior performance of BPN across different
datasets and domains, we consider a sequence of eight object
recognition datasets: 1. Oxford Flowers [37] for ﬁne-grained ﬂower
classiﬁcation (8,189 images in 102 categories); 2. MIT Scenes [39]
for indoor scene classiﬁcation (15,620 images in 67 categories); 3.
Caltech-UCSD Birds [50] for ﬁne-grained bird classiﬁcation (11,788
images in 200 categories); 4. Stanford Cars [24] for ﬁne-grained car
classiﬁcation (16,185 images of 196 categories); 5. FGVC-Aircraft
[32] for ﬁned-grained aircraft classiﬁcation (10,200 images in 70
categories); 6. VOC actions [10], the human action classiﬁcation
subset of the VOC challenge 2012 (3,334 images in 10 categories); 7.
Letters, the Chars74K datasets [6] for character recognition in natural
images (62,992 images in 62 categories); and 8. the Google Street
View House Number SVHN dataset [36] for digit recognition (99,289
images in 10 categories). To have a fair comparison, we use the same
AlexNet [26] architecture pretrained on ImageNet [44] as Aljundi et al.
[2, 1], and tested on 8 sequential tasks with a "multi-head evaluation",
where each task has its own classiﬁcation layer (introduce same
parameter costs for every method) and output space. All methods
have a task oracle at test time to decide which classiﬁcation layer to
use. We run the different methods on the following sequence: Flower
-> Scenes -> Birds -> Cars -> Aircraft -> Action -> Letters -> SVHM.

C. Experimental Setup for 100 permuted MNIST dataset

To demonstrate that BPN has capacity to accommodate a large
number of tasks, we tested it on 100 permuted MNIST datasets
generated from randomly permuted handwritten MNIST digits. We
consider 100 tasks; each new task has 10 classes. We used a fully-
connected neural network with 4 hidden layers of 128 ReLu Units
(a core network with small capacity) to compare the performances
of different methods. The type 4 methods, such as the Parameter
Superposition (PSP [4]) would exhaust the unrealized capacity and
inevitably dilute the capacity of the core network under a large number
of tasks: in their Fig. 2, with a network that has 128 hidden units
(leftmost panel), the average task performance for all tasks trained so
far, is 95% after training one task, but decreases to 50% after training
ﬁfty tasks. While a larger network with 2048 hidden units shows much
smaller decrease from 96% to about 90% (see Fig. 2 in their paper,
rightmost panel). The reason is that this method generates a random
diagonal binary matrix for each task, which in essence is a key or
selector for that task. As more and more tasks are learned, those keys
start to overlap more, causing interference among tasks. In comparison,
BPN can counteract the dilution, hence it can accommodate a large
number of tasks.

Algorithm 1 BD + EWC : forward rules for task t

For each fully connected layer i
Select bias units (task t): BIASi
Input: BIASi

t — Bias units for task t

t) for the current task

V i−1— Activations from the last layer

// provide beneﬁcial perturbations to bias the neural network

Output: V i = σ(W i · V i−1 + bi + BIASi

t) ∀ i ∈ [1, n]
where: W i— normal neuron weights at layer i. bi— normal bias term at layer i

// activations for the next layer

n — the number of FC layers.

σ(·) — the nonlinear activation function at each layer

Algorithm 2 BD + EWC : backward rules for task t

For the ﬁrst task A (t = 1):

Minimizing loss function: L(XA, W i, BIASi

A) ∀ i ∈ [1, n]

where: XA— data for task One. W i— normal neuron weights at layer i.

A — bias units for task One from FC layers i, which is the product of (M i

A, W i
A)

BIASi
n— the number of FC layers.

For task B (t > 1):

Minimizing loss function: L(Xt, W i, BIASi

)2 ∀ i ∈ [1, n]
where: XB— data for task B. W i— normal neuron weights from FC at layers i

j λFj(Wj − W A∗

j

B) + (cid:80)

j— labels each parameter. Fj — Fisher information matrix for parameter j.
Wj— normal weight j.
BIASi
n— the number of FC layers.

B — bias units for task B at FC layers i, which is the product of (M i

W A∗

B, W i
B)

j — optimal normal weight j after training on task A.

For each fully connected layer i:

During the training of task t
Select bias units for the current task t (BIASi
t)
Input: Grad — Gradients from the next layer
output: dWi
dMi

t = Grad · ((M i
t = (cid:15) sign ((Wi

t )T ) // gradients for the second term of bias units for task t at layer i
t)T · (Grad))

dWi = Grad · ((Vi)T )
dVi = (Wi)T · (Grad)
dbi = (cid:80)
j Gradj

// gradients for the ﬁrst term of bias units for task t at layer i using FGSD method
// gradients for normal weights at layer i
// gradients for activations at layer i to last layer i -1
// gradients for normal bias at layer i, j is iterator over the ﬁrst dimension of Grad

After training of task t
Freeze the BIASi
t
t and Mi
Delete the Wi

t to reduce parameter and memory storage cost

D. Our model and baselines

We compared the proposed Beneﬁcial Perturbation Network (
Beneﬁcial Perturbation + Elastic Weight Consolidation (the eleventh
model), BD + EWC (variant 1) and Beneﬁcial Perturbation + Parameter
Superposition (the twelfth model), BD + PSP (variant 2)) to 11
alternatives to demonstrate its superior performance.

1. Single Task Learning (STL). We consider several 5-layer fully-
connected neural networks. Each network is trained for each task
separately. Thus, STL does not suffer from catastrophic forgetting at
all. It is used as an upper bound.

2. Elastic Weight Consolidation (EWC)

[23]. The loss is

regularized to avoid catastrophic forgetting.

3. Gradient Episodic Memory with task oracle (GEM (*)) [31],
GEM uses a task oracle to build a ﬁnal linear classiﬁer (FLC) per
task. The ﬁnal linear classiﬁer adapts the output distributions to the
subset of classes for each task. GEM uses an episodic memory to
store a subset of the observed examples from previous tasks, which
are interleaved with new data from the latest task to produce a new
classiﬁer for all tasks so far. We use notation GEM (*) for the rest of
the paper, where * is the size of episodic memory (number of training
images stored) for each class.

4. Incremental Moment Matching [28] (IMM) IMM incremen-
tally matches the moment of the posterior distribution of the neural

network with a L2 penalty and equally applies it to changes to the
shared parameters.

5. Learning without forgetting [30] (LwF) First, LwF freezes
the shared parameters while learning a new task. Then, LwF trains
all the parameters until convergence.

6. Encoder based lifelong learning [40] (EBLL) Based on LwF,
using an autoencoder to capture the features that are crucial for each
task.

7. Synaptic Intelligence [55] (SI) While training on new task, SI
estimates the importance weights in an online manner. Parameters
important for previous tasks are penalized during the training of new
task.

8. Memory Aware Synapses [1] (MAS) Similar to SI method,
MAS estimates the importance weights through the sensitivity of the
learned function on training data. Parameters important for previous
tasks are penalized during the training of new task.

9. Sparse coding through Local Neural Inhibition and Dis-
counting [2] (SLNID) SLNID proposed a new regularizer that
penalizes neurons that are active at the same time to create sparse
and decorrelated representations for different tasks.

10. Parameter Superposition [4] (PSP) PSP used task-speciﬁc
context matrices to map different inputs from different tasks to
different subspaces spanned by rows in the weight matrices to avoid

to demonstrate it and to visualize the decision boundary (Fig. 5). We
randomly generate 3 normal distributed clusters different locations.
We have two tasks - Task 1: separate the black cluster from the red
cluster. Task 2: separate the black cluster from the light blue cluster.
The yellower (bluer) the heatmap, the higher (lower) the conﬁdence
that the neural network classiﬁes a location into the black cluster.
After training task 2, both plain gradient descent and GD + EWC
forget task 1 (dark blue boundary around the red cluster disappeared).
However, BD + EWC not only learns how to classify task 2 (clear
decision boundary between light blue and black clusters), but also
remembers how to classify the old task 1 (clear decision boundary
between red and black clusters). Thus, the beneﬁcial perturbations are
what can bias the network outputs and maintain the decision boundary
for each task, not just adding more dimensions.

B. Quantitative analysis for incremental tasks

Our BPN achieves a comparable or better performance than PSP,
GEM, EWC, GD + EWC in "single-head" evaluations, where the
output space consists of all the classes from all tasks learned so far. In
addition, it introduces negligible parameter and memory storage costs
per task. Fig. 6 and Tab. I summarize performance for all datasets and
methods. STL has the best performance since it trained for each task
separately and did not suffer from catastrophic forgetting at all. Thus,
STL is the upper bound. BD + EWC performed slightly worse than
STL (1%,4%,1% worse for incremental MNIST, CIFAR-10, CIFAR-
100 datasets). BD + EWC achieved comparable or better performance
than GEM. On incremental CIFAR-100 (10 tasks, 2 classes per task),
BD + EWC outperformed PSP, GEM (256) and GEM (10) by 1.80%,
6.96%, and 22.4%. BD + PSP outperformed PSP, GEM (256) and
GEM (10) by 2.40%, 7.59%, and 23.1%. By comparing the memory
storage costs (Tab. I, Supplementary), to achieve similar performance,
BD + EWC only introduces an additional 4,808 Bytes memory per
task, which is only 0.1% of the memory storage cost required by
GEM (256). BD + PSP only introduces 20,776 Bytes, or 0.44% of the
memory storage cost required by GEM (256). The memory storage
costs of BD + EWC is 30% of that of PSP. The memory storage
costs of BD + PSP is of the same order of magnitude as PSP. EWC
alone rapidly decreased to 0% accuracy. This conﬁrms similar results
on EWC performance on incremental datasets [42, 21, 38, 22] in
"single-head" evaluations although EWC generally performs well in
"multi-head" tasks. GD + EWC has the same additional dimensions as
BD + EWC, but GD + EWC failed in the continual learning scenario.
This result suggests that it is not the additional dimensions of the
bias units, but the beneﬁcial perturbations, which help overcome
catastrophic forgetting.

C. Quantitative analysis for eight sequential object recognition
tasks

The eight sequential object recognition tasks demonstrate the
superior performance of BPN (BD + PSP or BD + EWC) compared
to the state-of-the-art and the ability to learn sequential tasks across
different datasets and different domains. Our BPN achieves much
better performance than IMM [28], LwF[30], EWC [23], EBLL [40],
SI [55], MAS [1], SLNID [2], PSP [4] in "multi-head" evaluations,
where each task has its own classiﬁcation layer and output space.
After training on the 8 sequential object recognition datasets, we
measured the test accuracy for each dataset and calculated their average
performance (Tab. II). On average, BD + PSP (ours) outperforms
all other methods: PSP (7.52% better), SLNID (8.02% better), MAS
(11.73% better), SI (16.60% better), EBLL (17.07% better), EWC
(17.75% better), LwF (18.96% better) and IMM (35.62% better).
Although MAS, SI and EBLL performed better than EWC alone, with
the help of our beneﬁcial perturbations (BD), BD + EWC can achieve
a better performance than these methods: MAS (0.34% better), SI
(4.71% better), EBLL (5.13% better) and EWC (5.74% better). By
including the BD (BD + PSP and BD + EWC), we can signiﬁcantly

Fig. 5. Visualization of classiﬁcation regions: classify 3 randomly generated
normal distributed clusters. Task 1: separate black from red clusters. Task
2: separate black from light blue clusters. The yellower (bluer) the heatmap,
the higher (lower) the chance the neural network classiﬁes a location as the
black cluster. After training task 2, only BD + EWC remembers task 1 by
maintaining its decision boundary between the black and red clusters. Both
plain gradient descent and GD + EWC forget task 1 entirely.

interference between tasks. We use the binary superposition model
of PSP throughout the paper, because it is not only more memory
efﬁcient, but also, in our testing, it performed better than other PSP
variants (e.g., complex superposition).

11. BD + EWC (ours): Beneﬁcial Perturbation Network (variant
1). The ﬁrst term (Mt) of the bias units is updated in the beneﬁcial
direction (BD) using FGSD method. The second term (Wt) of the
bias units is updated in the gradient direction. The normal weights
are updated with EWC constraints.

12. BD + PSP (ours): Beneﬁcial Perturbation Network (variant
2). The ﬁrst term (Mt) of the bias units is updated in the beneﬁcial
direction (BD) using FGSD method. The second term (Wt) of the
bias units is updated in the gradient direction. The normal weights
are updated using PSP (binary superposition model, Supplementary).
13. GD + EWC: The update rules and network structure are the
same as BD + EWC, except the ﬁrst term (Mt) of the bias units is
updated in the Gradient direction (GD). This method has the same
parameter costs as BD + EWC . The failure of GD + EWC suggests
that the good performance of BD + EWC is not from the additional
dimensions provided by bias units.

VII. RESULTS:

A. The beneﬁcial perturbations can bias the network and
maintain the decision boundary

To show the advantages of our method are really from the beneﬁcial
perturbations and not just from additional dimensions to the neural
network, we compare between updating the ﬁrst term of the bias units
in the beneﬁcial direction (BD + EWC which comes from beneﬁcial
perturbations) and in the gradient direction (GD + EWC, which just
comes from the additional dimensions that our bias units provide). We
use a toy example (classifying 3 groups of Normal distributed clusters)

Plain GradientDescentGD+EWC(Gradient direction)BD+EWC(Beneficial direction)After training task 1After training task 2Less likelyblack clusterMore likelyblack clusterFig. 6. Results for a fully-connected network with 5 hidden layers of 300 ReLU units. (a) Incremental MNIST tasks (5 tasks, 2 classes per task). (b)
Incremental CIFAR-10 tasks (5 tasks, 2 classes per task). For a and b, the dashed line indicates the start of a new task. The vertical axis is the accuracy for
each task. The horizontal axis is the number of epochs. (c) Incremental CIFAR-100 tasks (10 tasks, 2 classes per task). The vertical axis is the accuracy for
task 1. The horizontal axis is the number of tasks.

TABLE I
TASK 1 PERFORMANCE WITH "SINGLE-HEAD" EVALUATION AFTER TRAINING ALL SEQUENTIAL TASKS ON INCREMENTAL MNIST, CIFAR-10 AND
CIFAR-100 DATASET. WE INCLUDE ADDITIONAL MEMORY STORAGE COSTS PER TASK (EXTRA COMPONENTS THAT ARE NECESSARY TO BE STORED ONTO
THE DISKS AFTER TRAINING EACH TASK, SUPPLEMENTARY) OF GEM , BD+EWC, BD + PSP AND PSP METHOD.

DATASET

INCREMENTAL MNIST
(5 TASKS, 2 CLASSES PER TASK)

INCREMENTAL CIFAR-10
(5 TASKS, 2 CLASSES PER TASK)

INCREMENTAL CIFAR-100
(10 TASKS, 2 CLASSES PER TASK)

METHOD

GEM(10)
BD+EWC
GEM(256)
GEM(150)
BD+EWC
GEM(256)
GEM(209)
BD+PSP
PSP
BD+EWC

TASK 1 PERFORMANCE AFTER
TRAINING ALL SEQUENTIAL TASKS

ADDITIONAL MEMORY STORAGE
COSTS PER TASK (BYTES)

0.980
0.980
0.800
0.698
0.795
0.790
0.775
0.850
0.830
0.845

47,040
4,808
4,718,592
2,764,800
4,808
4,718,592
3,852,288
20,776
15,968
4,808

boost performance when compared to using PSP or EWC alone (black
arrows in the Tab. II).

D. Quantitative analysis for 100 permuted MNIST dataset

100 permuted MNIST dataset demonstrates that our BPN has
capacity to accommodate a large number of tasks. After training 100
permuted MNIST tasks, the average task performance of BD + PSP
is 30.14% better than PSP. The average task performance of BD

+ EWC is 35.47% higher than EWC (Fig. 7.a). As the number of
tasks increases (Fig. 7.a), the average task performance of BD + PSP
becomes increasingly better than PSP. The reason is that adding new
tasks signiﬁcantly dilutes the capacity of the original network in Type
4 methods (e.g., PSP) as there are limited routes or subspaces to
form sub-networks. In this case, even though the core network can no
longer fully separate each task, the Beneﬁcial perturbations (BD) can
drag the misrepresented activations back to their correct work space

BD + EWCGEM (10)EWCGD + EWC01Task 1 AccBD + EWCGEM (256)EWCGD + EWC01Task 2 Acc01Task 3 Acc01Task 4 Acc01Task 5 Acc0255101520Epochs020481216Epochs01Task 1 Acc01Task 2 Acc01Task 3 Acc01Task 4 Acc01Task 5 Acc0.980.98000.800.79500Incremental MNIST tasks (5 tasks, 2 classes per task)Incremental CIFAR-10 tasks (5 tasks, 2 classes per task)(a)(b)(c)STL=0.99STL=0.83123456789100.50.9Task 1 Accuracy0.860.8450.830.850.790.69Incremental CIFAR-100 tasks (10 tasks, 2 classes per task)TasksBD + PSPBD + EWCPSPGEM (256)GEM (10)STL=EWC0out of rangeTABLE II
TEST ACCURACY (IN PERCENT CORRECT) ACHIEVED BY EACH METHOD WITH "MULTI-HEAD" EVALUATION FOR EACH DATASET AFTER TRAINING ON THE 8
SEQUENTIAL OBJECT RECOGNITION DATASETS. (DASH (–) MEANS THAT THE RESULTS ARE NOT AVAILABLE IN THEIR PAPERS. STAR (*) MEANS THAT WE
DIDN’T REPRODUCE THE METHODS AND THE RESULTS WERE TAKEN FROM SLNID [2] AND MAS [1]. THUS, WE KEEP THE SAME PERCENTAGE TABLE
FORMAT AS THEIRS).

network learned from the new tasks to have large deviations from
the parameters trained from old tasks). Although the performance of
PSP is much better than EWC, with the help of BD, BD + EWC still
reaches a similar performance as PSP.

VIII. DISCUSSION

We proposed a fundamentally new biologically plausible type of
method - beneﬁcial perturbation network (BPN), a neural network
that can switch into different modes to process independent tasks,
allowing the network to create potentially unlimited mappings between
inputs and outputs. We successfully demonstrated this in the continual
learning scenario. Our experiments demonstrate the performance of
BPN is better than the state-of-the-art. 1) BPN is more parameter
efﬁcient (0.3% increase per task) than the various network expansion
and network partition methods. it does not need a large episodic
memory to store any data from previous tasks, compared to episodic
memory methods, or large context matrices, compared to partition
methods. 2) BPN achieves state-of-the-art performance across different
datasets and domains. 3) BPN has a larger capacity to accommodate
a higher number of tasks than the partition networks. Through
visualization of classiﬁcation regions and quantitative results, we
validate that beneﬁcial perturbations can bias the network towards
a task, allowing the network to switch into different modes. Thus,
BPN signiﬁcantly contributes to alleviating catastrophic forgetting
and achieves much better performance than other types of methods.
Elsayed et al. [9] showed how carefully computed adversarial
perturbations embedded in the input space can repurpose machine
learning models to perform a new task without changing the parameters
of the models. This attack ﬁnds a single adversarial perturbation
for each task, to cause the model to perform a task chosen by the
adversary. This adversarial perturbation can thus be considered as a
program to execute each task. Here, we leverage similar ideas. But, in
sharp contrast, instead of using malicious programs embedded in the
input space to attack a system, we embedded beneﬁcial perturbations
(’beneﬁcial programs’) into the network’s parameter space (the bias
terms), enabling the network to switch into different modes to process
different tasks. The goal of both approaches is similar - maximizing
the probability (P (current task|image input, program)) of the
current task given the image input and the corresponding program for
the current task. This can be achieved by either forcing the network
to perform an attack task in Elsayed et al., or assisting it to perform a
beneﬁcial task in our method. The addition of programs to either input
space (Elsayed et al.’s method) or the network’s activation space (our

Fig. 7. 100 permuted MNIST datasets results for a fully-connected network
with 4 hidden layers of 128 ReLU units. This network is relatively small for
these tasks and hence does not offer much available redundancy or unrealized
capacity. (a) The average task accuracy of all tasks trained so far as the number
of tasks increases. (b) After training 100 tasks, the average task accuracy for
a group 10 tasks. We use t-test to validate the results.

of each task and recover their separation (as demonstrated in Fig. 3).
Thus, the BD of BD + PSP can still increase the capacity of the
network and boost the performance. Similarly, BD components in BD
+ EWC can boost performance, increasing the capacity of the network
to accommodate more tasks than EWC alone (Fig. 7.b). In addition,
after training 100 tasks (Fig. 7. b), the accuracy of BD + EWC
for the ﬁrst 50 tasks is higher than PSP, likely because BD+EWC
did not severely dilute the core network’s capacity while PSP did.
This means BD + EWC has a larger capacity than PSP. In contrast,
the lower performance of the last 50 tasks for BD + EWC comes
from the constraints of EWC (do not allow the parameters of the

BD + PSPBD + EWCPSP0100Number of tasksAverage task accuracy0.31(a) Permuted MNIST dataset ( average task performance of all tasks trained so far)0.4660.4810.6260.344EWCBD + PSPBD + EWCPSP01Tasks 1 - 10Tasks 11 - 20Tasks 21 - 30Tasks 31 - 40Tasks 41 - 50Tasks 51 - 60Tasks 61 - 70Tasks 71 - 80Tasks 81 - 90Tasks 91 - 100Tasks ID (group of 10)(b) Permuted MNIST dataset ( average task performance for a group of 10 tasks after training 100 tasks ) Average task accuracy******p < 0.001**p < 0.01*p < 0.05n.s.p > 0.05*********************n.s.EWC*********************n.s.n.s.*method) helps the network maximize this probability for a speciﬁc
task.

We suggest that the intriguing property of the beneﬁcial perturba-
tions that can bias the network toward a task might come from the
property of adversarial subspaces. Following the adversarial direction,
such as by using the fast gradient sign method (FGSD) [16], can help
in generating adversarial examples that span a continuous subspace
of large dimensionality (adversarial subspace). Because of “excessive
linearity” in many neural networks [49] [15], due to features including
Rectiﬁed linear units and Maxout, the adversarial subspace often
takes a large portion of the total input space. Once an adversarial
input lies in the adversarial subspace, nearby inputs also tend to
lie in it. Interestingly, this corroborates recent ﬁndings by Ilyas et
al. [20] that imperceptible adversarial noise can not only be used
for adversarial attacks on an already-trained network, but also as
features during training. For instance, after training a network on
dog images perturbed with adversarial perturbation calculated from
cat images, the network can achieve a good classiﬁcation accuracy
on the test set of cat images. This result shows that those features
(adversarial perturbations) calculated from the cat training sets, contain
sufﬁcient information for a machine learning system to make correct
classiﬁcation on the test set of cat images. In our method, we calculate
those features for each task, and store them into the bias units. In this
case, although the normal weights have been modiﬁed (information
from old tasks are corrupted), the stored beneﬁcial features for each
task have sufﬁcient information to bias the network and enable the
network to make correct predictions.

BPN is loosely inspired by its counterpart in the human brain:
having task-dependent modules such as bias units in our Beneﬁcial
Perturbation Network, and long-term memories in hippocampus (HPC,
[3]) in a brain network, are crucial for a system to switch into
different modes to process different tasks. During weight consolidation,
the HPC [29, 47, 13, 18] fuses features from different tasks into
coherent memory traces. Over days to weeks, as memories mature,
the HPC progressively stores permanent abstract high-level long-term
memories to remote memory storage areas (neocortical regions). The
HPC can then maintain and mediate their retrieval independently
when a speciﬁc memory is in need. We suggest that when a speciﬁc
memory is retrieved, it helps the HPC switch into distinct modes to
process different tasks. Thus, our analogy between HPC and BPN
can be formulated as: during the training of BPN, updating the shared
normal weights using EWC or PSP in theory leads to distinct task-
dependent representations (similar to the coherent memory traces
in HPC). However, some overlap between these representations is
inevitable because model parameters become too constrained for
EWC, or PSP runs out of unrealized capacity of the core network. To
circumvent this effect, Bias units (akin to the long-term memories in
the neocortical areas) are trained independently for each task. At test
time, bias units for a given task are activated to push representations
of old tasks back to their initial task-optimal working regions in
an analogous manner to maintaining and mediating the retrieval of
Long-term memories independently in HPC.

An alternative biological explanation evokes the concept of fac-
torized codes. In biological neuronal populations, neurons can be
active for one task or, in many cases, for more than one tasks. At
the population level, different tasks are encoded by different neuronal
ensembles which can overlap. In our model, the PSP component
deploys binary keys to activate task-speciﬁc readouts in hidden
layers, in an analogy to neuronal task ensembles. When activating
a BD component for a task, we would be further disambiguating a
task-speciﬁc ensemble, particularly across neurons which are active
for more than one task. The reason for this is that adding task-
speciﬁc beneﬁcial perturbations to activations of hidden layers can
shift the distribution of the net activation (akin to a DC offset or
carrier frequency). Evidence from nonhuman primate experiments
[43, 5] and human behavioral results [12] support this factorized code
theory. Electrophysiological experiments using monkeys demonstrated

that neurons in prefrontal cortex are either representing competing
categories independently [43] or could represent multiple categories
[5]. In human behavior experiments, "humans tend to form factorized
representation that optimally segregated the tasks [12]". In addition,
recent neural network simulations [51] demonstrated that "network
developed mixed task selectivity similar to recorded prefrontal neurons
after learning multiple tasks sequentially with a continual learning
technique". Thus, having factorized representations for different tasks
is important for enabling life-long learning and designing a general
adaptive artiﬁcial intelligence system.

ACKNOWLEDGMENT
This work was supported by the National Science Foundation
(grant number CCF-1317433), C-BRIC (one of six centers in JUMP,
a Semiconductor Research Corporation (SRC) program sponsored by
DARPA), and the Intel Corporation. The authors afﬁrm that the views
expressed herein are solely their own, and do not represent the views
of the United States government or any agency thereof.

REFERENCES
[1] R. Aljundi, F. Babiloni, M. Elhoseiny, M. Rohrbach, and
T. Tuytelaars. Memory aware synapses: Learning what (not) to
forget. In Proceedings of the European Conference on Computer
Vision (ECCV), pages 139–154, 2018.

[2] R. Aljundi, M. Rohrbach, and T. Tuytelaars. Selﬂess sequential

learning. arXiv preprint arXiv:1806.05421, 2018.

[3] A. Bakker, C. B. Kirwan, M. Miller, and C. E. Stark. Pattern
separation in the human hippocampal ca3 and dentate gyrus.
Science, 319(5870):1640–1642, 2008.

[4] B. Cheung, A. Terekhov, Y. Chen, P. Agrawal, and B. Olshausen.
Superposition of many models into one. In Advances in Neural
Information Processing Systems, pages 10868–10877, 2019.
[5] J. A. Cromer, J. E. Roy, and E. K. Miller. Representation of
multiple, independent categories in the primate prefrontal cortex.
Neuron, 66(5):796–807, 2010.

[6] T. E. De Campos, B. R. Babu, M. Varma, et al. Character

recognition in natural images. VISAPP (2), 7, 2009.

[7] S. Du, J. Lee, H. Li, L. Wang, and X. Zhai. Gradient descent
ﬁnds global minima of deep neural networks. In International
Conference on Machine Learning, pages 1675–1685, 2019.
[8] X. Du, G. Charan, F. Liu, and Y. Cao. Single-net continual
learning with progressive segmented training (pst). arXiv preprint
arXiv:1905.11550, 2019.

[9] G. F. Elsayed, I. Goodfellow, and J. Sohl-Dickstein. Ad-
versarial reprogramming of neural networks. arXiv preprint
arXiv:1806.11146, 2018.

[10] M. Everingham, S. A. Eslami, L. Van Gool, C. K. Williams,
J. Winn, and A. Zisserman. The pascal visual object classes
challenge: A retrospective. International journal of computer
vision, 111(1):98–136, 2015.

[11] M. Farajtabar, N. Azizan, A. Mott, and A. Li. Orthogonal
In International
gradient descent for continual
Conference on Artiﬁcial Intelligence and Statistics, pages 3762–
3773. PMLR, 2020.

learning.

[12] T. Flesch, J. Balaguer, R. Dekker, H. Nili, and C. Summerﬁeld.
Comparing continual task learning in minds and machines. Pro-
ceedings of the National Academy of Sciences, 115(44):E10313–
E10322, 2018.

[13] P. W. Frankland and B. Bontempi. The organization of recent
and remote memories. Nature Reviews Neuroscience, 6(2):119,
2005.

[14] R. M. French. Dynamically constraining connectionist networks
to produce distributed, orthogonal representations to reduce
catastrophic interference. network, 1111:00001, 1994.

[15] Goodfellow. Adversarial examples and adversarial training.
stanford cs231n lecture16 slides. http://cs231n.stanford.edu/
slides/2017/cs231n_2017_lecture16.pdf, 2017.

[16] I. J. Goodfellow, J. Shlens, and C. Szegedy. Explaining
and harnessing adversarial examples (2014). arXiv preprint
arXiv:1412.6572, 2014.

[17] B. D. Haeffele and R. Vidal. Global optimality in neural network
training. In Proceedings of the IEEE Conference on Computer
Vision and Pattern Recognition, pages 7331–7339, 2017.
[18] R. F. Helfrich, J. D. Lendner, B. A. Mander, H. Guillen, M. Paff,
L. Mnatsakanyan, S. Vadera, M. P. Walker, J. J. Lin, and
R. T. Knight. Bidirectional prefrontal-hippocampal dynamics
organize information transfer during sleep in humans. Nature
Communications, 10(1):1–16, 2019.

[19] F. N. Iandola, S. Han, M. W. Moskewicz, K. Ashraf, W. J.
Dally, and K. Keutzer. Squeezenet: Alexnet-level accuracy with
50x fewer parameters and< 0.5 mb model size. arXiv preprint
arXiv:1602.07360, 2016.

[20] A. Ilyas, S. Santurkar, D. Tsipras, L. Engstrom, B. Tran, and
A. Madry. Adversarial examples are not bugs, they are features.
arXiv preprint arXiv:1905.02175, 2019.

[21] R. Kemker and C. Kanan. Fearnet: Brain-inspired model for
incremental learning. arXiv preprint arXiv:1711.10563, 2017.

[22] R. Kemker, M. McClure, A. Abitino, T. L. Hayes, and C. Kanan.
Measuring catastrophic forgetting in neural networks. In Thirty-
second AAAI conference on artiﬁcial intelligence, 2018.
[23] J. Kirkpatrick, R. Pascanu, N. Rabinowitz, J. Veness, G. Des-
jardins, A. A. Rusu, K. Milan, J. Quan, T. Ramalho, A. Grabska-
Barwinska, et al. Overcoming catastrophic forgetting in neural
networks. Proceedings of the national academy of sciences,
page 201611835, 2017.

[24] J. Krause, M. Stark, J. Deng, and L. Fei-Fei.

3d object
representations for ﬁne-grained categorization. In Proceedings
of the IEEE International Conference on Computer Vision
Workshops, pages 554–561, 2013.

[25] A. Krizhevsky and G. Hinton. Learning multiple layers of
features from tiny images. Technical report, Citeseer, 2009.

[26] A. Krizhevsky, I. Sutskever, and G. E. Hinton.

Imagenet
classiﬁcation with deep convolutional neural networks.
In
Advances in neural information processing systems, pages 1097–
1105, 2012.

[27] Y. LeCun, L. Bottou, Y. Bengio, and P. Haffner. Gradient-based
learning applied to document recognition. Proceedings of the
IEEE, 86(11):2278–2324, 1998.

[28] S.-W. Lee, J.-H. Kim, J. Jun, J.-W. Ha, and B.-T. Zhang.
Overcoming catastrophic forgetting by incremental moment
In Advances in Neural Information Processing
matching.
Systems, pages 4652–4662, 2017.

[29] E. Lesburguères, O. L. Gobbo, S. Alaux-Cantin, A. Hambucken,
P. Triﬁlieff, and B. Bontempi. Early tagging of cortical networks
is required for the formation of enduring associative memory.
Science, 331(6019):924–928, 2011.

[30] Z. Li and D. Hoiem. Learning without forgetting.

IEEE
transactions on pattern analysis and machine intelligence,
40(12):2935–2947, 2017.

[31] D. Lopez-Paz et al. Gradient episodic memory for continual
learning. In Advances in Neural Information Processing Systems,
pages 6467–6476, 2017.

[32] S. Maji, E. Rahtu, J. Kannala, M. Blaschko, and A. Vedaldi.
Fine-grained visual classiﬁcation of aircraft. arXiv preprint
arXiv:1306.5151, 2013.

[33] A. Mallya, D. Davis, and S. Lazebnik. Piggyback: Adapting a
single network to multiple tasks by learning to mask weights.
In Proceedings of the European Conference on Computer Vision
(ECCV), pages 67–82, 2018.

[34] N. Y. Masse, G. D. Grant, and D. J. Freedman. Alleviating
catastrophic forgetting using context-dependent gating and
synaptic stabilization. Proceedings of the National Academy of
Sciences, 115(44):E10467–E10475, 2018.

[35] M. McCloskey and N. J. Cohen. Catastrophic interference in

connectionist networks: The sequential learning problem. In
Psychology of learning and motivation, volume 24, pages 109–
165. Elsevier, 1989.

[36] Y. Netzer, T. Wang, A. Coates, A. Bissacco, B. Wu, and A. Y.
Ng. Reading digits in natural images with unsupervised feature
learning. 2011.

[37] M.-E. Nilsback and A. Zisserman. Automated ﬂower classiﬁ-
cation over a large number of classes. In 2008 Sixth Indian
Conference on Computer Vision, Graphics & Image Processing,
pages 722–729. IEEE, 2008.

[38] G. I. Parisi, R. Kemker, J. L. Part, C. Kanan, and S. Wermter.
Continual lifelong learning with neural networks: A review.
Neural Networks, 2019.

[39] A. Quattoni and A. Torralba. Recognizing indoor scenes. In 2009
IEEE Conference on Computer Vision and Pattern Recognition,
pages 413–420. IEEE, 2009.

[40] A. Rannen, R. Aljundi, M. B. Blaschko, and T. Tuytelaars.
Encoder based lifelong learning. In Proceedings of the IEEE
International Conference on Computer Vision, pages 1320–1328,
2017.

[41] S.-A. Rebufﬁ, A. Kolesnikov, G. Sperl, and C. H. Lampert.
icarl: Incremental classiﬁer and representation learning.
In
Proceedings of the IEEE Conference on Computer Vision and
Pattern Recognition, pages 2001–2010, 2017.

[42] A. Rios and L. Itti. Closed-loop gan for continual learning.

arXiv preprint arXiv:1811.01146, 2018.

[43] J. E. Roy, M. Riesenhuber, T. Poggio, and E. K. Miller.
Prefrontal cortex activity during ﬂexible categorization. Journal
of Neuroscience, 30(25):8519–8528, 2010.

[44] O. Russakovsky, J. Deng, H. Su, J. Krause, S. Satheesh, S. Ma,
Z. Huang, A. Karpathy, A. Khosla, M. Bernstein, et al. Imagenet
large scale visual recognition challenge. International journal
of computer vision, 115(3):211–252, 2015.

[45] A. A. Rusu, N. C. Rabinowitz, G. Desjardins, H. Soyer,
J. Kirkpatrick, K. Kavukcuoglu, R. Pascanu, and R. Hadsell.
Progressive neural networks. arXiv preprint arXiv:1606.04671,
2016.

[46] S. A. Sloman and D. E. Rumelhart. Episodic gating. From

learning theory to connectionist theory, 2:227, 1992.

[47] L. R. Squire and P. Alvarez. Retrograde amnesia and memory
consolidation: a neurobiological perspective. Current opinion in
neurobiology, 5(2):169–177, 1995.

[48] R. K. Srivastava, J. Masci, S. Kazerounian, F. Gomez, and
J. Schmidhuber. Compete to compute. In Advances in neural
information processing systems, pages 2310–2318, 2013.
[49] F. Tramèr, N. Papernot, I. Goodfellow, D. Boneh, and P. Mc-
Daniel. The space of transferable adversarial examples. arXiv
preprint arXiv:1704.03453, 2017.

[50] C. Wah, S. Branson, P. Welinder, P. Perona, and S. Belongie.

The caltech-ucsd birds-200-2011 dataset. 2011.

[51] G. R. Yang, M. R. Joglekar, H. F. Song, W. T. Newsome, and
X.-J. Wang. Task representations in neural networks trained to
perform many cognitive tasks. Nature neuroscience, 22(2):297–
306, 2019.

[52] J. Yoon, S. Kim, E. Yang, and S. J. Hwang. Oracle: Order robust

adaptive continual learning. 2019.

[53] J. Yoon, E. Yang, J. Lee, and S. J. Hwang. Lifelong learning

with dynamically expandable networks. 2018.

[54] G. Zeng, Y. Chen, B. Cui, and S. Yu. Continual learning
of context-dependent processing in neural networks. Nature
Machine Intelligence, 1(8):364–372, 2019.

[55] F. Zenke, B. Poole, and S. Ganguli. Continual learning through
synaptic intelligence. In Proceedings of the 34th International
Conference on Machine Learning-Volume 70, pages 3987–3995.
JMLR. org, 2017.

IX. SUPPLEMENTARY
A. Clariﬁcation of memory storage costs

For the memory storage costs, we only consider what components
are necessary to be stored on the disk after training each task, that
is, the cost of storing the model for later re-use. In other words, the
memory storage cost is deﬁned as “the number of bytes required
to store all of the parameters in the trained model" [19] (in table 2
of Iandola et al.). This is usually the metric that is reported along
with the number of operations to run a model (e.g., mobilenet web
page, darknet web page, SqueezeNet [19] and Additive Parameter
Superposition [52]). The extra memory storage costs of EWC are
zero under this deﬁnition. Indeed, even though it requires a lot of
transient memory during training, in the end the contents of this
memory are used only to constrain network weight updates, and they
are discarded once a training run is complete. At test time, a network
trained with EWC has the same number of parameters, uses the same
amount of runtime memory, and the same amount of operations as the
original model. For example, consider that after training 5 sequential
tasks, we want to train a new task 6. There are 5 steps: 1) Load
the trained model from disk; 2) EWC would make a duplication of
the parameters learned so far and just loaded from disk, and put
them into transient memory (RAM); 3) During the training of task 6,
EWC calculates the Fisher information matrix and applies the EWC
constraints, which relies on the contents of the transient memory; 4)
Delete the duplications of the parameters in the transient memory;
5) Save the parameters of latest model onto disk. In contrast, after
training each task, GEM needs to store some images from that task
onto the disk. Likewise, PSP needs to store the context matrix for
each new task to disk, and BD + EWC needs to store the bias units
to disk. Thus, the extra memory storage costs of BD + EWC is just
the memory storage costs of the bias units (BD).

B. Clariﬁcation of parameter costs

Similar to memory storage costs, we only consider what components
are necessary to be stored on disk after training each task (0.3%
increase per task), that is, the cost of storing the model for later
re-use. It should be noted that BPN needs large additional weight
matrices (called W i
t in the paper) during training. Likewise, EWC
essentially doubles the size of the network during training, to create
the Fisher information matrix used by this method. However, both our
weight matrices and EWC’s Fisher information matrix are discarded
After 5 epochs of training, the network converged in Fig. 6 as only
2 classes per task need to be trained in that ﬁgure (but see Tab. II and
Fig. 7 for the more complex 8-dataset where each task may have up
to 200+ classes; this one required more epochs per task to converge,
up to 300 epochs per dataset).

after training. So while the overall growth in the number of parameters
is negligible, the number of parameters needed during training is surely
higher than with vanilla SGD.

C. Choice of Hyperparameter

We found a large λ (2 ∗ 103) for EWC constraint in Eqn. 6 can
effectively prevent a large parameter drifting from old tasks. However,
if the λ is too large, the strict constraint would hinder the learning of
new tasks.

t and W i

We tested the hyperparameter H for M i

t from 1 to 2500.
The more complex the task, the larger H is needed for the BPN
since it provides more degree of freedom to better learn beneﬁcial
perturabtions [17, 7]. For example, The H is 25∼255 for Permuted
MNIST task and is 100 ∼ 900 for eight sequential object recognition
tasks. However, if the H is too large (e.g, 2500 for eight object
recognition tasks) that does not match the complexity of the task, the
performance of the BPN would decrease.
D. Difference between Transfer Learning and Continual Learn-
ing

Continual learning is a different idea from transfer learning.
For transfer learning, after learning the ﬁrst task and when receiving
a new task to learn, the parameters of the network are ﬁnetuned on
the new task data. Thus, transfer learning is expected to suffer from
forgetting the old tasks while being advantageous for the new task.
Though, the shared convolutional layers beneﬁts from a more general
embeddings learned from a much more difﬁcult task (pretrained on
ImageNet model).

In comparison, for continual learning, the focus is on learning
new independent tasks sequentially without forgetting previous task.
To achieve this focus, our BPN updates the shared normal weight
using EWC or PSP. In theory, this update lead to orthogonal, hence
non-overlapping and local task representation. However, in reality,
the overlapping is inevitable because the parameters become too
constrained for EWC, or PSP runs out of unrealized capacity of the
core network. Thus, we introduce bias units trained independently
for each task. At test time, bias units for a given task are activated
to push representations of old tasks back to their initial task-optimal
working regions.

One of the beneﬁts of continual learning is that learning new tasks
can be aided by the knowledge already accumulated while learning
the previous tasks.

E. Algorithms for BD + PSP

The forward and backward rules for BD + PSP are detailed in

Alg. 3 and Alg. 4.

Fig. S1. The dashed line indicates the start of a new task. (a) Flow chart of a typical Type 4 method, here illustrated using a pictorial representation similar
to that of Progressive Segmented Training (PST) [8]. PST subdivides the core network by freezing the important weight parameters for old tasks, and allowing
new tasks to update the remaining free weight parameters. (b) Flow chart of Beneﬁcial Perturbation Network (BPN; new type 5 method). BPN adds
task-dependent beneﬁcial perturbations to the activations, biasing the network toward that task, and retrains the weight parameters of the core network, with
constraints from EWC [23], PSP[4], or other similar approach.

Algorithm 3 BD + PSP : forward rules for task t

For each fully connected layer i
Select bias units (task t): BIASi
Input: BIASi

t — Bias units for task t

t) for the current task

V i−1— Activations from the last layer
kt— Binary keys for task t

// provide beneﬁcial perturbations to bias the neural network

Output: V i = σ(W i · V i−1 (cid:12) kt + bi + BIASi

t) ∀ i ∈ [1, n]

where: W i— normal neuron weights at layer i. bi— normal bias term at layer i

n —the number of layers

σ(·) — the nonlinear activation function at each layer

Algorithm 4 BD + PSP : backward rules for task t

// activations for the next layer

For the task t:

Minimizing loss function: L(Xt, W i, BIASi

t) ∀ i ∈ [1, n]

where: Xt— data for task One. W i— normal neuron weights at layer i.

t — bias units for task One from FC layers i, which is the product of (M i

t , W i
t )

BIASi
n— is the number of FC layers.

For each fully connected layer i:

During the training of task t
Select bias units for the current task t (BIASi
t)
Input: Grad — Gradients from the next layer
output: dWi

t = Grad · ((M i
t = (cid:15) sign ((Wi

t )T ) // gradients for the second term of bias units for task t at layer i
t)T · (Grad))

dMi

dWi = Grad · ((Vi)T ) (cid:12) kt
dVi = (Wi)T · (Grad) (cid:12) kt
dbi = (cid:80)
j Gradj

// gradients for the ﬁrst term of bias units for task t at layer i using FGSD method
// gradients for normal weights at layer i, kt is the key for task t
// gradients for activations at layer i to last layer i -1, kt is the key for task t
// gradients for normal bias at layer i, j is iterator over the ﬁrst dimension of Grad

After training of task t
Freeze the BIASi
t
t and Mi
Delete the Wi

t to reduce parameter and memory storage cost

a)Weight parameters of core network for Ti-1 ,Ti , Ti+1b)Fronzen important weight parameters for Ti-1, Ti, Ti+1Free weight parameters for Ti-1,Ti, Ti+1Additional out-of-network bias units for Ti-1 ,Ti , Ti+1Task TiTask Ti+1Constrain weight parameters of the core network by EWC or PSPConstrain weight parameters of the core network by EWC or PSPActivationsActivationsAdd beneficial perturbationsto bias the networkAdd beneficial perturbationsto bias the networkweight parametersTask Ti-1ActivationsAdd beneficial perturbationsto bias the networkTask TiFreeze important weight parameters&Model Segmentationweight parametersTask Ti+1Freeze important weight parameters&Model SegmentationActivationsActivationsTask Ti-1ActivationsActivations for Ti-1 ,Ti , Ti+1