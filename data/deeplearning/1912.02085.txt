Protecting Geolocation Privacy of Photo Collections

Jinghan Yang, Ayan Chakrabarti, Yevgeniy Vorobeychik
Computer Science & Engineering, Washington University in St. Louis
{jinghan.yang, ayan, yvorobeychik}@wustl.edu

9
1
0
2
c
e
D
4

]

V
C
.
s
c
[

1
v
5
8
0
2
0
.
2
1
9
1
:
v
i
X
r
a

Abstract

People increasingly share personal information, including
their photos and photo collections, on social media. This in-
formation, however, can compromise individual privacy, par-
ticularly as social media platforms use it to infer detailed
models of user behavior, including tracking their location.
We consider the speciﬁc issue of location privacy as poten-
tially revealed by posting photo collections, which facilitate
accurate geolocation with the help of deep learning methods
even in the absence of geotags. One means to limit associ-
ated inadvertent geolocation privacy disclosure is by carefully
pruning select photos from photo collections before these are
posted publicly. We study this problem formally as a combi-
natorial optimization problem in the context of geolocation
prediction facilitated by deep learning. We ﬁrst demonstrate
the complexity both by showing that a natural greedy algo-
rithm can be arbitrarily bad and by proving that the problem
is NP-Hard. We then exhibit an important tractable special
case, as well as a more general approach based on mixed-
integer linear programming. Through extensive experiments
on real photo collections, we demonstrate that our approaches
are indeed highly effective at preserving geolocation privacy.

1

Introduction

A person’s physical whereabouts—the city, state, or coun-
try they are in or have traveled to—is sensitive information
that many desire to keep private. With routine information
about everyday activities increasingly being shared online
and on social media, there is a heightened risk that such data
may unintentionally leak information about the geographic
location of individuals. For example, disclosure of a per-
son’s current location or travel history to law-enforcement
agencies, hostile governments, employers, advertising com-
panies, etc. could result in signiﬁcant real harms. In this
work, we speciﬁcally address the risk of geolocation privacy
from collections of photographs shared by users online.

The IM2GPS method (Hays and Efros 2008) was the ﬁrst
to demonstrate that, even without explicit embedded geo-
tags, image content sometimes contains enough cues to al-
low accurate estimation of the location where the image was
taken. The accuracy of image geolocation algorithms has

Copyright c(cid:13) 2020, Association for the Advancement of Artiﬁcial
Intelligence (www.aaai.org). All rights reserved.

Figure 1: Illustration of our problem and approach. This
photo collection has 16 photos, all taken in Barcelona, which
is also the correctly predicted location when all photos are
used (top left map). Greedily removing photos to ensure
that Barcelona is not among top-5 predicted locations corre-
sponds to the selection highlighted in dashed green, and the
resulting top-5 predictions are on the top right map. Photos
chosen by our approach are identiﬁed by a red box, and the
result is shown on the lower map.

 
 
 
 
 
 
since improved considerably, most recently through the use
of deep convolutional neural networks (CNNs) (Weyand,
Kostrikov, and Philbin 2016). Nevertheless, geolocation pre-
diction accuracy from individual images is still limited, and
the content of a single photograph is typically insufﬁcient to
unambiguously resolve the location of the photographer.

However, as we show, geolocation predictions can be
signiﬁcantly more accurate when aggregating information
across a collection of photographs known to have been taken
in the same geographic location (e.g., a person’s photo-
album from a speciﬁc trip, or images from different people
known to be traveling together or attending the same event).
While individual images can appear to have been taken in
any of a large set of possible locations, the intersection of
these sets for different images is often sufﬁcient to narrow
down the true location of the image collection.

We consider the problem of censoring photo collections
in order to preserve geolocation privacy: selecting a mini-
mal set of images to delete from a given collection, such that
the true location is not included in the most likely locations
predicted from the remaining images. We consider differ-
ent variants of this task: (1) defeating top-1 accuracy while
minimizing deletions, to ensure the most likely predicted lo-
cation is incorrect; (2) defeating top-k accuracy, such that
the true location is not among the k most likely predicted
locations; and (3) maximizing the rank of the true location
in the list of likely predicted locations, under a ﬁxed budget
on the number of deleted images. We consider both white-
box and black-box versions of these tasks. In the white-box
version, the privacy-preserving algorithm has access to the
true location classiﬁer. In the black-box version, in contrast,
the algorithm instead uses a proxy classiﬁer.

We show that, except for the top-1 variant, this problem
is NP-hard, because the optimal deletion set should not just
censor the images that are most indicative of the true lo-
cation, but also maintain the likelihood of plausible alter-
natives. We also speciﬁcally show that a naive greedy al-
gorithm can be arbitrarily sub-optimal. Then, we propose a
polynomial-time algorithm for the top-1 variant and present
a mixed-integer linear programming (MILP) formulation for
the other variants to allow the use of standard MILP solvers.
Finally, we present an extensive experimental evaluation
of our proposed approaches using images and albums down-
loaded from Flickr. 1 In particular, our experiments show
that our methods signiﬁcantly outperform simple greedy
deletions in all task variants, and in many cases admit op-
timal solutions with only a small number of deleted images.
Our work thus highlights the risk to geolocation privacy that
exists from photo collections commonly shared by users to-
day, while also providing an effective solution that comes at
the relatively low cost of censoring a small fraction of im-
ages. Our code could be found in the project page 2.
Related Work Sharing photographs with embedded loca-
tion information can allow images to tell a richer story of
the places and events they represent. Consequently, many
cameras (especially those on mobile phones) allow users the

1https://www.ﬂickr.com
2https://github.com/jinghanY/geoPrivacyAlbum

option of embedding GPS coordinates in image EXIF tags.
Hays and Efros (2008) were the ﬁrst to exploit such
data to train geolocation predictors, and show that geo-
graphic localization is possible from image information
alone. Since then, a number of methods have made steady
progress on improving the accuracy of image location es-
timators through the use of deep convolutional neural net-
works (CNN). Weyand, Kostrikov, and Philbin (2016) for-
mulate geolocation as a classiﬁcation problem, by dividing
the globe into a discrete number of cells and assigning a
separate label for each. Howard et al. (2017) demonstrated
versions of these classiﬁers that can run on mobile devices,
while Kim, Dunn, and Frahm (2017) considered learning
representations that exploit contextual information. Vo, Ja-
cobs, and Hays (2017) presented a study conﬁrming the ef-
ﬁcacy of Weyand, Kostrikov, and Philbin, as well as of us-
ing deep network features for localization by retrieval. Tian,
Chen, and Shah (2017) devised a matching-based geoloca-
tion approach speciﬁcally designed for street-view images.
Mura (2017) and Hal (2013) discuss the dangers of using
geolocation for commercial purposes and the importance of
geolocation privacy. In the security domain, most existing
works propose ways to anonymize location data by replacing
the associated name with an untraceable ID (Beresford and
Stajano 2003; Gruteser and Grunwald 2003). Other work
addresses geolocation privacy of user interfaces (Doty and
Wilde 2010). Andr´es et al. (2013), Xiao and Xiong (2015)
and Ho and Ruan (2011) develop mechanisms for preserv-
ing geolocation privacy using differential privacy techniques
which add noise to actual location information; as such, they
do not deal with the issue of inferring location from a photo
collection, which is the focus of our work.

However, these efforts do not address the issue of loca-
tion inference based on published images and photo collec-
tions. Our work is also broadly related to adversarial ma-
chine learning (Vorobeychik and Kantarcioglu 2018), al-
though the speciﬁc problem and techniques differ consider-
ably. In particular, our focus is not to exploit divergences be-
tween learned classiﬁers and human perception (by creating
imperceptible perturbations that fool classiﬁers (Szegedy et
al. 2013; Nguyen, Yosinski, and Clune 2015)), but rather to
censor a small number of images to remove information that
can be leveraged for geolocation.

2 Problem Formulation

Geolocation Prediction from Images
Predicting geographic location from a single image is often
naturally cast as a classiﬁcation task (Weyand, Kostrikov,
and Philbin 2016). Possible locations on the globe are dis-
cretized into a discrete set C = [c1, c2, . . . cM ] of M geospa-
tial regions, and a classiﬁer is trained to predict a probability
distribution P (c|X) from an input image X for c ∈ C. Here,
P (c|X) denotes the probability or classiﬁer’s belief that the
image X was taken in the discretized location c.

While the accuracy of per-image location classiﬁers is still
limited, we consider the effect of aggregating location infor-
mation across images. Formally, we consider the task of es-
timating location from a collection A = [X1, X2, . . . XN ]

of N images that are known to have all been taken at the
same location c. Without making any assumption on the re-
lationship between images in A (i.e., treating them as condi-
tionally independent given c), and further assuming that all
locations c ∈ C are equally likely a-priori, the probability
distribution of the collection A is simply given by the prod-
uct of the individual image distributions:

P (c|A) =

(cid:81)N

i=1 P (c|Xi)
(cid:81)N

i=1 P (c(cid:48)|Xi)

(cid:80)

c(cid:48)∈C

·

(1)

For convenience, we deﬁne M −dimensional score vec-
tors {Si} as the log probabilities of locations from individual
images, with the jth element deﬁned by Sj
i = log P (cj|Xi).
Similarly, we deﬁne S = (cid:80)N
i=1 Si as the score corre-
sponding to the image collection. This implies that Sj =
log P (cj|A) + z, where z is a scalar corresponding to the
normalization constant in (1) (i.e., it is added to all elements
of the vector S). Note that since P (cj|A) > P (cj(cid:48)|A) ⇔
Sj > Sj(cid:48)
, a location cj has the same relative rank in both
the probability and score vectors.

Thus, the location scores for an image collection are given
simply by summing score vectors, as returned by a classi-
ﬁer, from individual images. As our experiments will show,
aggregating information in this way from even a relatively
small number of photos leads to signiﬁcant gains in location
accuracy compared to single images.

Location Privacy by Selecting Images for Deletion
When using the distributions P (c|·) to make inferences
about location, one can choose to look at either the most
likely location—i.e., arg maxc P (c|·)—or the set of k most
likely locations. Speciﬁcally, let H(cj, S) deﬁne the set of
locations that have higher scores in S than a given cj:

H(cj, S) = {j(cid:48) : cj(cid:48) ∈ C, Sj(cid:48)

≥ Sj, j (cid:54)= j(cid:48)}.

(2)

If the true location for a collection is ct, then our location
estimate is exactly correct (top-1 accurate) if |H(ct, S)| = 0,
and top-k accurate if |H(ct, S)| < k.

Our goal is to develop a privacy protection method that,
given an image set A and per-image scores {Si}, selects a
small set of images D ⊂ A to delete from A to yield a cen-
sored image set A(cid:48) = A \ D, so that score vector S(cid:48) from
the remaining images, i.e., S(cid:48) = (cid:80)
i:Xi∈A(cid:48) Si, leads to in-
correct location estimates. Next, we deﬁne different variants
of this task for different objectives trading-off utility (num-
ber of deletions) and privacy.
Top-k Guarantee: The ﬁrst variant seeks to minimize the
number of deletions, while ensuring that the resulting scores
from the censored image set push the true location ct out of
the top-k location results, for a speciﬁed k, i.e.,

|D|, s.t. |H(ct, S(cid:48))| ≥ k.

min
D

(3)

A special case of this is when k = 1, when we wish to ensure
that the true location isn’t predicted to be the most likely.
Fixed Deletion Budget: Another variant works with a ﬁxed
budget d on the number of images to delete, while trying

to maximize the rank of the true label in the resulting score
vector S(cid:48), i.e.,

max
D

|H(ct, S(cid:48))|, s.t. |D| ≤ d.

(4)

Here, we call the resulting value of |H(ct, S(cid:48))| for the opti-
mized deletion set as the “protected-k”, since it defeats top-k
accuracy for all values of k up to and including |H(ct, S(cid:48))|.
Incorporating User Preference: In some cases, a user may
want to specify a set of images U ⊂ A that should not be
deleted. In this case, we add the constraint D ⊂ A \ U to the
formulations (3) and (4) above.
Black-box Protection: So far, we have assumed that we
have access to the location classiﬁer that will be used for
inference, and therefore to the corresponding scores {Si}.
However, in many real-world settings, our privacy protec-
tion algorithm will not have access to the actual classiﬁer.
In such cases, we propose using scores {Si} from a proxy
classiﬁer, and adding a scalar margin θ > 0 to the scores
St
i of the true location in each image. These modiﬁed scores
are used to solve the optimization problems (3) and (4), and
capture uncertainty about the true value of the scores Sj
i (we
can set θ such that the score advantage St
i , of the true
location over any other location in a given image from our
proxy, is likely to be within θ of the true advantage).

i − Sj

3 Computational Hardness
It is tempting to view the task of selecting images for dele-
tion as computationally easy—that it can be solved simply
with a greedy selection of images with highest probabili-
ties or scores for the true location. However, this is not the
case since optimal deletion must both reduce the cumulative
score of the true location, and maintain high scores for pos-
sible alternate locations. Deleting an image with high scores
for other locations, as well as the true one, can often lead to
no change, or even a decrease, in the relative rank of the true
location. Conversely, it may sometimes be optimal to delete
an image with a moderate true location score, when it is the
only one in the collection with a very low score for an other-
wise plausible alternate location. In this section, we discuss
the sub-optimality of greedy selection, and then prove that
optimal deletion is NP-hard.

Consider the greedy selection method that constructs a
deletion set D by selecting images with the highest values
of the score St
i for the true location ct. To defeat top-k accu-
racy in (3) for a given k, it sequentially deletes images with
decreasing values St
i , stopping when ct is no longer in the
top-k. Given a ﬁxed budget d for deletions in (4), it simply
deletes the d images with the highest scores St
i .
Observation 1. Greedy selection can be arbitrarily sub-
optimal. Speciﬁcally, in cases with just three possible dis-
cretized locations (i.e., |C| = 3), there exists a set of score
vectors corresponding to log-probabilities for N +3 images,
∀N ≥ 1, where the ﬁrst N deletions by greedy selection
leave the true location as the most likely prediction, whereas
deleting two optimally selected images is sufﬁcient to make
it the least likely prediction (i.e., defeat top-2 accuracy).

Broadly, the above set corresponds to N score vectors
i = log(1/3), ∀j),

with uniform scores for all locations (Sj

and three additional vectors with lower than uniform true-
location scores (St
i < log(1/3)). From the latter, two vectors
each feature a very low score for each of the incorrect loca-
tions, respectively. The greedy method selects the images
with uniform distributions leading to no change in rank of
the true location, while deleting the latter two images makes
the true location the least likely prediction. The supplement
provides the explicit construction of these score vectors and
a complete proof.

Beyond worst-case sub-optimality, our experiments will
show that greedy selection also yields poorer results than op-
timal deletion with our method in practice, with typical score
vectors from real images. Figure 1 shows an example set of
sixteen real images where, to achieve a top-5 location pri-
vacy guarantee, greedy selection requires twelve deletions
while the optimal solution requires only four.

We next show that the optimization problems in our pri-

vacy formulation are in fact NP-hard.
Theorem 1. The optimization problem in (3), to minimize
deletions for a top-k guarantee, is NP-hard ∀k ≥ 2.

Theorem 2. The optimization problem in (4), for maximiz-
ing protected-k under a ﬁxed deletion budget, is NP-hard.

We begin by introducing and proving a lemma that relates
to the NP-hardness of optimal deletion for making the true
location less likely than a given set of alternate locations. We
then use this lemma to prove Theorem 1, and subsequently,
prove Theorem 2.
Lemma 1. Given an image collection A with true label ct,
and a target set ¯H ⊂ C, ct /∈ | ¯H| of alternate locations, with
ﬁxed size k = | ¯H|, it is NP-hard ∀k ≥ 2 to ﬁnd the smallest
deletion set D ⊂ A , s.t. S(cid:48)t ≤ S(cid:48)j, ∀cj ∈ ¯H, where S(cid:48) are
score vectors for A(cid:48) = A \ D.
Deﬁnition 1. Deletion for Two Alternate Locations (DTAL).
Given: a collection A, pair of alternate labels ¯H =
{cp, cq}, and true label ct /∈ ¯H. Question: is there any dele-
tion set D ⊂ A , s.t. S(cid:48)t ≤ S(cid:48)j, ∀cj ∈ ¯H ?
Note that although scores correspond to log-probabilities
in our formulation, we place no restrictions on Sj
i above,
since given arbitrary score values, we can subtract an image-
speciﬁc constant log (cid:80)
i ) from the scores for all la-
bels to yield valid log-probabilities. This results in an iden-
tical decision problem, as the same constant occurs on both
sides of all inequality constraints.
Deﬁnition 2. Knapsack Problem (KP). Given: Values {vi >
0} and weights {wi > 0} for a set of objects O, with a ca-
pacity constraint W and value bound V . Question: Is there
a subset of objects Q ⊆ O with total weight at most W > 0
and total value at least V > 0 ?

j exp(Sj

Proof of Lemma 1. It is easy to see DTAL is in NP. We
prove DTAL is NP-complete by reduction from KP. Given
an instance of KP, we form an instance of DTAL by creat-
ing a collection A with |O| + 1 images. We set St
i = 0 for
all images. For 1 ≤ i ≤ |O|, we set the scores Sp
i = vi
and Sq
i = −wi. For the ﬁnal image with i = |O + 1|, we set

i > Sq

Sp
i = −V and Sq
i = W . Any deletion set returned by DTAL
can not include the last image X|O|+1, since St
i for all
other images. Given this solution, we can get a correspond-
ing solution for KP as the set of all objects for which the
corresponding image Xi was not included in the deletion
set. Similarly, a solution of the KP instance corresponds to
a solution of DTAL, where the deletion set includes images
corresponding to all objects not included in the KP solution.
Since DTAL—which asks if a deletion set, without any
constraint on its size—is NP-complete, it follows that the
task of determining if a deletion set of bounded size exists
is also NP-complete. The latter corresponds to the decision
version of the optimization problem in Lemma 1 for k = 2.
Therefore, the optimization in Lemma 1 is NP-hard for k =
2. It is then easy to show the problem is NP-hard also for
any k > 2, since if there were a polynomial-time solver for
k > 2, we could call it to solve the k = 2 version by setting
the scores for all but two locations to −∞, in all images.

Proof of Theorem 1. The optimization problem in Theorem
1 requires making a similar decision as in Lemma 1, except
that it also requires determining the best set of alternate la-
bels ¯H where | ¯H| = k, ct /∈ ¯H. It is easy to see that this is at
least as hard as the ﬁxed alternate target set variant. Speciﬁ-
cally, if we have a solver for the problem in Theorem 1, we
can use it to solve the problem in Lemma 1 by simply setting
the scores Sj

i = ∞, ∀i, ∀j : j (cid:54)= ct, j /∈ ¯H.

Proof of Theorem 2. The ﬁxed deletion budget optimization
problem in Theorem 2 is at least as hard as the top-k guaran-
tee problem in Theorem 1. If we have a solver for the ﬁxed
budget problem, we can call that solver sequentially with
deletion budget d = (1, 2, . . . N ), and stop whenever the re-
turned protected-k satisﬁes the desired top-k guarantee.

4 Proposed Approach
We now present algorithms for solving the privacy-
preserving optimization problems in our formulation. We
begin with a generic approach that maps our objectives and
constraints to a mixed integer linear program (MILP). This
formulation is naturally also NP-hard, but it allows us to use
standard MILP solvers that use various heuristics to ﬁnd a
solution efﬁciently. Finally, we had shown that the top-k
guarantee problem is NP-hard ∀k ≥ 2. For the important
special case k = 1, we show that this problem can in fact be
solved in polynomial time.t

Mixed Integer Linear Programming
Given a collection A with set of locations C, true loca-
tion ct, and score vectors {Si}, we let zi ∈ {0, 1} denote
whether a photo is deleted (zi = 0) or kept (zi = 1). We let
hj ∈ {0, 1}, ∀j (cid:54)= t denote whether location cj has a lower
(hj = 0) or higher (hj = 1) score S(cid:48)j than that of the true
location S(cid:48)t in the censored collection A(cid:48). We enforce con-
sistency of these values of hj to the assignment zi by adding
the constraints:

hj ×

(cid:32)

(cid:88)

i

(cid:16)

zi

Sj
i − St
t

(cid:33)

(cid:17)

≥ 0, ∀j (cid:54)= t.

(5)

Under these deﬁnitions, the protected-k of a given assign-
ment is given by (cid:80)
j(cid:54)=t hj, and the number of deletions by
(cid:80)
i(1 − zi). Thus, our problem variant (3) where we mini-
mize the number of deleted photos that ensure top-k location
privacy corresponds to solving

max
{zi},{hj }

(cid:88)

zi,

i

s.t

(cid:88)

j(cid:54)=t

hj ≥ k,

(6)

since minimizing the number of deleted photos is equivalent
to maximizing the number of photos remaining in the cen-
sored collection. Analogously, the problem of maximizing
protected-k with a ﬁxed deletion budget d corresponds to:

max
{zi},{hj }

(cid:88)

j(cid:54)=t

hj,

s.t

(cid:88)

i

(1 − zi) ≤ d.

(7)

Both problems above are over the binary variables zi ∈
{0, 1}, hj ∈ {0, 1}, under the additional constraints in (5).

Note that the formulations above involve bilinear con-
straints (5), as these feature products of hj and zi. However,
since these are both binary variables, it is straightforward to
linearize the constraints using McCormick inequalities (see
the supplement for details).
User Preference: User speciﬁed constraints that a set of im-
ages U not be deleted can be incorporated simply by adding
the constraints zi = 1, ∀Xi ∈ U to the MILP.
Black-Box Protection: The margin θ described in Sec. 2,
that accounts for discrepancies between the proxy and un-
known true classiﬁers, can also be incorporated in our MILP
by simply modifying the constraints in (5) above to:

(cid:32)

hj ×

(cid:88)

(cid:16)
Sj
i − St

t − θ

(cid:17)

zi

(cid:33)

≥ 0, ∀j (cid:54)= t.

(8)

i

Polynomial-Time Algorithm for top-1 Guarantee
As we’ve shown, the optimization problems in our formula-
tion are NP-hard for k ≥ 2. Conversely, the special case of
minimizing deletions for top-1 accuracy admits a solution in
polynomial time. We begin by noting that, for this case, we
must only ensure that the true location ct has a lower score
than any other single location cj, j (cid:54)= t.

First, note that for a speciﬁc choice of cj, the assign-
i ∈ {0, 1} with fewest deletions that ensures cj has a

ment ¯zj
higher score than ct is given by
zi, s.t.

i } = arg max
{zi}

{¯zj

(cid:88)

(cid:88)

zi(Sj

i − St

i ) ≥ 0,

(9)

i

i
where ¯zj
i = 0 indicates that Xi is deleted, as previously.
This can be solved by sorting the images Xi in descending
i −Sj
order of the score advantage (St
i ) of the true location ct
over cj, and deleting images (setting ¯zj
i = 0) sequentially till
the constraint above is satisﬁed. The solution to the original
problem is then given by searching over all possible cj (cid:54)= ct,
i.e., minj(cid:54)=t((cid:80)
i ). Thus, we solve (9) for each possi-
ble alternate location cj (cid:54)= ct, compare the number of dele-
tions required for each, and select the answer with the fewest
deletions. The overall algorithm runs in O(M N log N ) time
(where N = |A| is the number of images, and M = |C| the
number of locations). The full algorithm is included in the
supplement.

i 1 − ¯zj

5 Experiments
We now present an extensive evaluation of our method for
all variants of our geolocation privacy problem, measuring
their success at preserving privacy against a state-of-the-art
deep neural network-based location classiﬁer with experi-
ments on real photographs downloaded from Flickr.

Preliminaries
Datasets and Discretized Locations: We conduct exper-
iments on a set of 750k images downloaded from Flickr,
where we ensure each image has a geotag to specify ground-
truth location. We randomly split these into training and test
sets, of size 90% and 10% of total images respectively. Fol-
lowing Weyand, Kostrikov, and Philbin (2016), we cast ge-
olocation as a multi-class classiﬁcation problem over a set
of discretized locations C. This set is formed by an adap-
tive scheme to partition the globe into contiguous cells, such
that each cell contains the same number of training images.
This means that cell sizes vary depending on the density
of photos (and indirectly, of population) in the correspond-
ing region (e.g., oceans tend to inhabit large cells, while
large metropolitan areas may be partitioned among multiple
cells). We use a discretization into 512 cells (i.e., |C| = 512).
Photo Collections: Photo collections for evaluation are con-
structed by randomly forming sets of images with the same
discretized location from the test set. We consider collec-
tions of size 16, 32, 64, and 128, where collections for each
size are formed as 10 random partitions of the entire test-
set. This gives us roughly 45k collections of size 16, 21k
of size 32, 10k of size 16, and 5k of size 128. Further, we
collected 10k Flickr user designated “albums”, each with
images all from the same discretized location, and with no
image overlap with the training set. The album sizes ranged
between 16 and 250 (33 photos on average), with all 512
locations represented. In our evaluations, we treat these al-
bums as photo collections, although images in photo albums
tend to be more correlated than random collections.
Location Classiﬁer: We use a deep convolutional neural
network—with a standard VGG-16 (Vo, Jacobs, and Hays
2017) architecture—to produce the probability distribution
over locations from a single input image (the last layer of
the network is modiﬁed from the standard architecture to
have 512 channels, to match the size of C). We initialize
the weights of all but the last layer of the network from a
model trained for ImageNet (Russakovsky et al. 2015) clas-
siﬁcation, and then train the network with a standard cross-
entropy loss on the entire training set (except for the results
on the black-box setting, as described later).

Geolocation Performance of Images vs Collections
We begin with experiments that demonstrate that the threat
to location privacy is signiﬁcantly heightened when aggre-
gating information across multiple images. We evaluate per-
image location accuracy from our trained classiﬁer in Ta-
ble 1, and compare it to accuracies of predictions from dif-
ferent collections using (1) as a collection-based classiﬁer.
While performance is relatively modest with single images,
it increases signiﬁcantly when we use photo collections,
even with only 16 images, yielding a signiﬁcant privacy risk.

Collection Type

top-1

top-5

protected-k

Single Image
R-16
R-32
R-64
R-128
Albums

0.20
0.97
0.99
1.00
1.00
0.20

0.38
1.00
1.00
1.00
1.00
0.39

59.58
0.06
0.01
0.00
0.00
53.51

Table 1: Geolocation performance on single images vs im-
age collections. We show top-1 and top-5 accuracies (frac-
tion of instances when the true location is in the top 1 or
5 predictions), and average protected-k (where rank of true
location is k + 1) for all cases. Note lower protected-k rep-
resents more accurate geolocation. Here R-N refers to ran-
domly constructed collections of size N, while Albums cor-
responds to user designated photo albums.

Collection
Type

top-1

top-5

Optimal Greedy Optimal Greedy

R-16
R-32
R-64
R-128
Albums

0.32
0.31
0.31
0.31
0.21

0.37
0.43
0.49
0.53
0.33

0.45
0.47
0.48
0.49
0.31

0.53
0.59
0.64
0.68
0.39

Table 2: Average fraction of deleted photos selected to
achieve top-1 and top-5 geolocation privacy. We compute
averages only over instances where predictions on the origi-
nal whole collection was accurate in the top-1 / top-5 sense.

Note that accuracies for user designated albums are con-
siderably lower because images in these albums are often
correlated with similar content in many images, since most
albums have images all taken by a single photographer and
in similar environs. Nevertheless, some user albums may
still pose a privacy risk, and our top-k guarantee approach
can be used on these, since it would delete photos only in
albums that compromised location privacy.

Top-k Guarantee with Minimum Deletions
We now consider the effectiveness of our approach, starting
with the problem in (3), where the goal is to minimize the
number of deleted photos while ensuring that the true loca-
tion is not in the top-k most likely predictions. We present
these results in Table 2, for k = 1 and 5.

We ﬁnd that when using our approach, we typically need
to delete only a modest ∼30% of the photos on average to
achieve a top-1 guarantee, and < 50% of photos for the
stronger top-5 guarantee—note that these averages are com-
puted only over collections where the original predictions
(prior to any deletion) were already accurate in the top-1
and top-5 sense, respectively. We also see that the optimal
solution from our approach always requires fewer deletions
than greedy selection—usually by a signiﬁcant margin.

Thus, our method ensures location privacy while still al-

lowing users to share a signiﬁcant fraction of their images.

Coll.
Type

R-16

R-32

R-64

R-128

Albums

Method

Optimal
Greedy

Optimal
Greedy

Optimal
Greedy

Optimal
Greedy

Optimal
Greedy

Deletion Budget (% Photos)
12.5% 25% 37.5% 50%

0.67
0.46

0.17
0.07

0.05
0.02

0.03
0.01

3.00
1.88

1.14
0.42

0.57
0.09

0.38
0.03

77.09
70.78

103.18
86.31

10.20
6.22

4.84
1.90

2.87
0.57

2.06
0.21

-
-

29.12
17.67

17.49
7.15

10.20
2.80

8.07
1.29

-
-

Table 3: Average protected-k, such that the true label is not
in top-k, for different deletion budgets as percentages of col-
lection size. Note higher protected-k implies greater privacy.

Coll.
Type

R-16

R-32

R-64

R-128

Deletion Budgets
12.5%→25% 25%→50%

0.27

0.32

0.34

0.33

0.10

0.15

0.19

0.22

Table 4: Fraction of images that were deleted with a lower-
deletion budget, but NOT with a higher one.

Maximum protected-k under Deletion Budget
Next, we consider our approach for our second problem vari-
ant in (4) where, given a ﬁxed budget on the number of
images to delete from each collection, the goal is to max-
imize the value of k such that the true location is not in the
top-k predictions. Table 3 reports results for this setting on
our various collection types, from optimal selection with our
approach as well as from the greedy selection method (this
time, averaging over all collection instances).

We ﬁnd that again, optimal selection with our approach
yields signiﬁcantly better privacy protection than the greedy
baseline. Moreover, the same fraction of deleted images pro-
vides greater protection when the size of the collection is
small. Note that average protected-k values are higher for
the user albums, because the corresponding values for the
uncensored collections were already high (Table 1), so much
so that it is unnecessary to consider deletion budget above
25%, with k already above 100 at that point (hence the blank
entries in the table).

In addition, we check if the photos deleted for a given
collection for one deletion budget, are also deleted given a
larger budget. As our results in Table 4 indicate, this is not
the case: when more deletions are possible, the optimal so-
lution may involve keeping some of the photos that would
have been deleted under a lower budget. This indicates that
any sequential greedy strategy is likely sub-optimal.

1
-
p
o
T

5
-
p
o
T

R-16

R-32

R-64

R-128

Figure 2: Effect of user-speciﬁed constraint to retain a chosen photo, while providing top-k guarantee. Rank of the chosen photo
is with respect to its score for the true location St
i , success rate indicates fraction of collections for which a feasible deletion set
was found, and fraction of photos removed is reported averaged over successful cases.

User Preference
Next we consider the case where we incorporate user-
speciﬁed constraints to retain certain photos in each collec-
tion. Naturally, the effectiveness of privacy protection will
depend on how informative that photo is towards location:
preserving privacy may be impossible without deleting im-
ages with clearly recognizable landmarks. For a systematic
evaluation, we always choose one photo to retain per col-
lection, with different experiments where we consistently
choose the photo with the same “rank” in terms of true loca-
tion scores (rank 1 implying photo with highest St

Figure 2 presents these results for different collection
types, for the tasks of top-1 and -5 guarantees. We show the
success rate (fraction of collections where it was possible
to achieve a guarantee without deleting the speciﬁed photo)
and the average fraction of deletions required (for success-
ful cases). For comparison, we show the baseline fraction of
deletions from Table 2. We see that putting constraints does
degrade performance—requiring more deletions and some-
times leading the problem to be infeasible. This is more so
for small collections. However, the rank of the photo plays
a major role, and the performance drop is negligible when
constraining photos that have higher rank (i.e., lower scores
for the true location), especially for larger collections. Such
settings are likely to match user expectations of maintaining
privacy while retaining location neutral photographs.

i ).

Black-Box Protection
Finally, we evaluate the case when we do not have access to
true classiﬁer scores but only a proxy. For this case, we train
two VGG network classiﬁers, each on two separate halves
of the training set. We then use one classiﬁer to derive our
proxy scores and apply our approach for top-k guarantees,
and evaluate its success by determining the fraction of col-
lections for which this guarantee holds when using the other
classiﬁer. These results are shown in Figure 3, where we re-
port both success rate and the number of deletions required

top-1

top-5

Figure 3: Effectiveness of black-box attacks as a function of
the margin θ for R-128 collections.

for different values of margin θ. We see that our success at
defeating an unknown classiﬁer increases with higher mar-
gins, albeit at the cost of an increased number of deletions.

Running Time
Although our optimization problem is NP-hard, we ﬁnd that
a modern MILP solver, CPLEX, returns solutions in reason-
able time: ∼9.2s for a R-128 collection and ∼4.8s for R-64,
for the top-5 guarantee setting.

6 Conclusion
Our work demonstrated that photo collections pose a
clear risk to location privacy—simply from aggregating
the outputs of existing deep neural network-based location
classiﬁers—and proposed a framework for ameliorating this
risk, through an algorithm for selecting optimal photos for
deletion. While we considered that entire set was available
to us prior to selection, an interesting direction of future
work relates to online versions of the selection problem—
choosing to delete or keep/post a photo without knowing
what other photos may be added to the collection.

It is also worth highlighting the assumptions and limita-
tions of our formulation. Our selection method requires ac-

and Adam, H.
2017. Mobilenets: Efﬁcient convo-
lutional neural networks for mobile vision applications.
arXiv:1704.04861.
[Kim, Dunn, and Frahm 2017] Kim, H. J.; Dunn, E.; and
Frahm, J.-M. 2017. Learned contextual feature reweight-
ing for image geo-localization. In CVPR.
[Mura 2017] Mura, R. 2017. Geolocation and targeted adver-
tising: Making the case for heightened protections to address
growing privacy concerns. 9 Buff. Intell. Prop. L.J. 77.
[Nguyen, Yosinski, and Clune 2015] Nguyen, A.; Yosinski,
J.; and Clune, J. 2015. Deep neural networks are easily
fooled: High conﬁdence predictions for unrecognizable im-
ages. In Proceedings of the IEEE Conference on Computer
Vision and Pattern Recognition, 427–436.
[Russakovsky et al. 2015] Russakovsky, O.; Deng, J.; Su, H.;
Krause, J.; Satheesh, S.; Ma, S.; Huang, Z.; Karpathy, A.;
Khosla, A.; Bernstein, M.; Berg, A. C.; and Fei-Fei, L. 2015.
ImageNet Large Scale Visual Recognition Challenge. Inter-
national Journal of Computer Vision (IJCV).
[Szegedy et al. 2013] Szegedy, C.; Zaremba, W.; Sutskever,
I.; Bruna, J.; Erhan, D.; Goodfellow, I.; and Fergus, R. 2013.
Intriguing properties of neural networks. In Porceedings of
the International Conference on Learning Representations.
[Tian, Chen, and Shah 2017] Tian, Y.; Chen, C.; and Shah,
M. 2017. Cross-view image matching for geo-localization
in urban environments. 2017 IEEE Conference on Computer
Vision and Pattern Recognition (CVPR) 1998–2006.
[Vo, Jacobs, and Hays 2017] Vo, N.; Jacobs, N.; and Hays, J.
2017. Revisiting im2gps in the deep learning era. ICCV.
[Vorobeychik and Kantarcioglu 2018] Vorobeychik, Y., and
2018. Adversarial machine learning.
Kantarcioglu, M.
Synthesis Lectures on Artiﬁcial Intelligence and Machine
Learning.
T.;
[Weyand, Kostrikov, and Philbin 2016] Weyand,
PlaNet - Photo
Kostrikov, I.; and Philbin, J.
Geolocation with Convolutional Neural Networks. Cham:
Springer International Publishing. 37–55.
[Xiao and Xiong 2015] Xiao, Y., and Xiong, L. 2015. Pro-
tecting locations with differential privacy under temporal
correlations. In Proceedings of the 22Nd ACM SIGSAC Con-
ference on Computer and Communications Security, CCS
’15, 1298–1309. New York, NY, USA: ACM.

2016.

cess to at least a reasonable proxy for the model that an ad-
versary may use for location inference—this is implicit in
the choice of the margin θ in the black-box setting. We also
assume there is no source of location information other than
the images themselves, and accounting for side-channels re-
quires extensions to our framework (which could potentially
be modeled as a score offset provided by the side-channel).
We also assume conditional independence between differ-
ent images in a collection, in deriving the collection-level
location score as the sum of per-image scores. However, it
is conceivable that a more sophisticated location classiﬁer
could learn to model user behavior, and leverage higher-
order statistics to ﬁnd, for example, that the presence of cer-
tain combinations of images in a collection are more indica-
tive of certain locations.

Acknowledgments

This work was partially supported by the National Science
Foundation CAREER award (IIS-1905558) and NVIDIA.

References

M.;

Bordenabe,

[Andr´es et al. 2013] Andr´es,
N.;
Chatzikokolakis, K.; and Palamidessi, C.
2013. Geo-
indistinguishability: differential privacy for location-based
systems. In CCS 2013, 901–914. Association for Computing
Machinery.
[Beresford and Stajano 2003] Beresford, A. R., and Stajano,
F. 2003. Location privacy in pervasive computing. IEEE
Pervasive Computing 2(1):46–55.
[Doty and Wilde 2010] Doty, N., and Wilde, E. 2010. Ge-
olocation privacy and application platforms. In Proceedings
of the 3rd ACM SIGSPATIAL International Workshop on Se-
curity and Privacy in GIS and LBS, SPRINGL ’10, 65–69.
New York, NY, USA: ACM.
[Gruteser and Grunwald 2003] Gruteser, M., and Grunwald,
D. 2003. Anonymous usage of location-based services
In Proceedings of
through spatial and temporal cloaking.
the 1st International Conference on Mobile Systems, Appli-
cations and Services, MobiSys ’03, 31–42. New York, NY,
USA: ACM.
[Hal 2013] Hal, T. J. V. 2013. Taming the golden goose:
Private companies, consumer geolocation data, and the need
for a class action regime for privacy protection. 15 Vand. J.
Ent. & Tech. L. 713.
2008.
[Hays and Efros 2008] Hays, J., and Efros, A. A.
im2gps: estimating geographic information from a single
image. In Proceedings of the IEEE Conf. on Computer Vi-
sion and Pattern Recognition (CVPR).
[Ho and Ruan 2011] Ho, S.-S., and Ruan, S. 2011. Differ-
In Proceedings
ential privacy for location pattern mining.
of the 4th ACM SIGSPATIAL International Workshop on Se-
curity and Privacy in GIS and LBS, SPRINGL ’11, 17–24.
New York, NY, USA: ACM.
[Howard et al. 2017] Howard, A. G.; Zhu, M.; Chen, B.;
Kalenichenko, D.; Wang, W.; Weyand, T.; Andreetto, M.;

Supplement
A Construction of Scores for Observation 1
In Table 5, We show construct the construction of a collec-
tion with probabilities that lead greedy selection to be ar-
bitrarily sub-optimal. Note that a and (cid:15) must be such that

photos

X1
...
XN
XN +1
XN +2
XN +3

c1
1
3
...
1
3
a
2
3 + (cid:15) − a
1
3 + (cid:15)

c2
1
3
...
1
3
2
3 + (cid:15) − a
a
1
3 + (cid:15)

ct
1
3
...
1
3
1
3 − (cid:15)
1
3 − (cid:15)
1
3 − 2(cid:15)

Table 5: Probabilities p(cj|Xi) for a collection where greedy
deletion is arbitrarily sub-optimal.

3 + (cid:15) and 0 < (cid:15) < 1

0 < a < 2
6 to ensure all elements
in the matrix are valid probabilities. Further, we must ensure
that the most likely location prediction from the collection—
without deletions—is the true location ct. Accordingly, we
3 −(cid:15))2×( 1
set a = ( 1
. It is easy to verify that a > 0. Since
2( 1
3 +(cid:15))2
3 −(cid:15))2
( 1
3 +(cid:15))2 < 1 and 1
2( 1
since a > 0, we have 2

3 + (cid:15). Also,
3 + (cid:15)). Consequently,

3 + (cid:15), we have a < 2

3 + (cid:15) − a < 2( 1

3 − 2(cid:15) < 2

3 −2(cid:15))

B Algorithm for k = 1

Algorithm 1 Untargeted with k = 1
Input: ct, C−ct == C \ ct, A
Output: D
1: Initialize D = ∅, d = 1
2: while d < |A| do
3:
4:
5:

for cj ∈ C−ct do

i − St

i(Sj
return D.

6:
7:
end if
8:
end for
9:
10: end while
11: return No approach to protect.

Sort Sj − St in ascending order.
Delete the ﬁrst d elements in Sj − St and update
D with indexes of the deleted elements.
if (cid:80)
i ) > 0 then

C Linearizing Constraint (6)
Since we need all constraints and objectives to be linear to
use an MILP solver, we adopt a standard linearization tech-
nique to arrive at a modiﬁed set of constraints. Choosing T
to be a sufﬁcient large number, and wj = hjvj, the fol-
lowing optimization problem with only linear constraints is
equivalent to ours (for the top-k guarantee):

3 − 2(cid:15))

3 − 2(cid:15))

=

=

exp(S1) = exp(S2)
3 − (cid:15))2 × ( 1
( 1
2( 1
3 + (cid:15))2
( 1
3 − (cid:15))2 × ( 1
2( 1
3 + (cid:15))
2
3
1
3

1
3 + (cid:15))
1
2( 1
3 + (cid:15))
= exp(St).

× 2(

2( 1

× (

= [

< [

× (

× (

2
3

2
3

+ (cid:15) − a) × (

+ (cid:15) − a) × (

1
3

1
3

+ (cid:15)) × (

1
3

)N

)N

max

s.t.

+ (cid:15) − a)] × [(

1
3

− (cid:15))2 × (

1
3

− 2(cid:15)) × (

1
3

)N ]

+ (cid:15))] × [(

1
3

− (cid:15))2 × (

1
3

− 2(cid:15)) × (

1
3

)N ]

(10)

(cid:88)

zi

j = 1, 2, ..., M − 1

i
wj ≥ 0,
wj ≥ −T hj
wj ≤ T hj
wj ≥ vj − T (1 − hj)
wj ≤ vj + T (1 − hh)
(cid:88)

hj ≥ k

j

where

vj =

(cid:88)

(cid:16)

zi

Sj
i − St
i

(cid:17)

i

(12)

A similar approach can be used for the ﬁxed budget problem.

Note that a greedy algorithm applied to these probabilities
will ﬁrst select X1 . . . XN for deletion before the remaining
images (because they have the highest values of St
i ). But
deleting these images will make no difference to the rank of
the true location t in the corresponding score vector S(cid:48).

Conversely, optimal selection need only delete XN +1 and

XN +2 to ensure S(cid:48)t < S(cid:48)1 and S(cid:48)t < S(cid:48)2:

S(cid:48)1 = S(cid:48)2 = log(

1
3

− 2(cid:15)) + N log(

< log(

1
3

+ (cid:15)) + N log(

1
3

)

1
3
) = S(cid:48)t.

(11)

