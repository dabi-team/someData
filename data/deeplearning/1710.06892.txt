7
1
0
2

t
c
O
8
1

]
L
P
.
s
c
[

1
v
2
9
8
6
0
.
0
1
7
1
:
v
i
X
r
a

TypesafeAbstractionsforTensorOperations(ShortPaper)TongfeiChenJohnsHopkinsUniversity,USAtongfei@cs.jhu.eduAbstractWeproposeatypesafeabstractiontotensors(i.e.multidi-mensionalarrays)exploitingthetype-levelprogrammingcapabilitiesofScalathroughheterogeneouslists(HList),andshowcasetypesafeabstractionsofcommontensorop-erationsandvariousneurallayerssuchasconvolutionorrecurrentneuralnetworks.ThisabstractioncouldlaythefoundationoffuturetypesafedeeplearningframeworksthatrunsonScala/JVM.CCSConcepts•Softwareanditsengineering→Soft-warelibrariesandrepositories;KeywordsScala,tensor,heterogeneouslist,deeplearningACMReferenceFormat:TongfeiChen.2017.TypesafeAbstractionsforTensorOperations(ShortPaper).InProceedingsof8thACMSIGPLANInternationalScalaSymposium(SCALA’17).ACM,NewYork,NY,USA,6pages.h￿ps://doi.org/10.1145/3136000.31360011IntroductionRecentlythemachinelearningcommunitysawasurgeoflibrariesthathandletensors.ExamplesincludePythonli-brariessuchasNumPy[Waltetal.2011],Theano[TheanoDevelopmentTeam2016],TensorFlow[Abadietal.2015],PyTorch1,DyNet[Neubigetal.2017],orJavalibrarieslikeNd4j2).Theselibrariesprovideabstractionstotensorop-erationsonCPUsorGPUs,andsomesupportautomaticdi￿erentiationoncomputationalgraphs.Fortensorsintheselibraries(belongstojustonetypeNdArray/Tensor),speci￿cmeaningisimplicitlyassignedtoeachofthedimensions.Correctlymanipulatingthedi￿erentdimensionsoftensorscouldbedi￿cult,renderingthewholeprogrampronetoruntimeerrorsandhardtoreasonormaintain:wecouldmistakenlyaddedupan1-Dtensorand2-Dtensor,orper-formedmatrixmultiplicationbetweena2-Dtensoranda1github.com/pytorch/pytorch.2github.com/deeplearning4j/nd4j.SCALA’17,October22–23,2017,Vancouver,Canada©2017Copyrightheldbytheowner/author(s).PublicationrightslicensedtoAssociationforComputingMachinery.Thisistheauthor’sversionofthework.Itispostedhereforyourpersonaluse.Notforredistribution.Thede￿nitiveVersionofRecordwaspublishedinProceedingsof8thACMSIGPLANInternationalScalaSymposium(SCALA’17),h￿ps://doi.org/10.1145/3136000.3136001.3-Dtensor.Theseerrorsareonlydiscoveredatruntime,orinsomelibraries,evenignoredbecauseofimplicitbroad-casting.Programmersmustkeeptrackoftheaxesofthetensorsthemselves(usuallyascommentsincode),ratherthanleveragingthetypesystemtoguidetheprogrammers.Wewouldliketohaveatypingmechanismfortensorsthatnotonlyencodestherankofthetensor,butalsoen-codeswhateachaxismeans.Scala,beingastatically-checkedtype-safelanguagethatishighlycapableoftype-levelpro-grammingandDSLconstruction,asisdemonstratedinthepopularlibraryShapeless3,makesitanideallanguageforimplementingsuchtypefulandtypesafetensorabstractions.Wedescribethedesignandimplementationofsuchatypesafeabstractionthataddressesthetypesafetyproblemofothertensorlibraries,andreleaseaprototype.42TypesafeTensorsWeproposeatypesafetensorabstraction,inwhichtheaxesareencodedbyaheterogeneouslist(HList)typeparameter.traitTensor[D,A<:HList]Disthetypeoftheelementsthistensorholds(e.g.Float,Double,etc.),whereastypesintheHListtypeparameterAarephantomtypes,i.e.,theyonlyserveaslabels.Basicconstructssuchasscalars,vectorsandmatricescouldberepresentedasfollows(typesA,Betc.arelabels/namestoaxes).typeScalar=Tensor[Float,HNil]typeVector[A]=Tensor[Float,A::HNil]typeMatrix[A,B]=Tensor[Float,A::B::HNil]Lookingatmoreconcreteexamplesfromdeeplearningapplications,imagesincomputervision,orsentencesinwhicheachwordismappedtoawordembedding(i.e.vectorrepresentationofthatwordinRdspace)innaturallanguageprocessingcouldbeencodedasfollows.(SeeFig.1)typeImage=Tensor[Float,Width::Height::Channel::HNil]typeEmbeddedSentence=Tensor[Float,Word::Embedding::HNil]Byencodingthemeaningofeachaxisintothetype,oursystemguaranteesthatalloperationsontensorsareallowed3github.com/milessabin/shapeless.4github.com/ctongfei/nexus. 
 
 
 
 
 
SCALA’17,October22–23,2017,Vancouver,CanadaTongfeiChenFigure1.Exampleofencodinganimageandasentenceastypesafetensors.onlyiftheiroperands’axesmakesensemathematically.Fordi￿erentmathematicaltensoroperators,di￿erenttypeguar-anteesaremade.Forinstance:•Add(+)guaranteesthatonlytwotensorsoftheexactsameaxescouldbeadded.Vector[A]andVector[B]arenotaddable.•MatMul(matrixmultiplication)guaranteesthatonlytwomatricesintheshapeofMatrix[A,B]andMatrix[B,C]canbemultiplied,i.e.,thesecondaxisofthe￿rstoperandandthe￿rstaxisofthesecondoperandmustmatch.ThegeneraltreatmentofoperatortypesafetyisaddressedinSection5.Notethattheactualsizeofeachaxisisnotencodedinthetype.Shapeless’sChurchencodingofnaturalnumbers(Nat),whenthenumberislarge,signi￿cantlyreducescom-pilationspeed.Thus,sizemismatch(e.g.addingtwotensorsofthesameaxesbutnotthesamesize)errorswillnotbecapturedandwillbethrownasruntimeerrors.Weleavethetype-levelencodingofdimensionsizesasfuturework(possiblybyusingdependenttypes).2.1Shape/axesManipulationFunctionsIncommonlibrariesfortensoroperationsthereisusuallyasetofoperationsforaxesmanipulation,namelytranspose,expand_dims,squeeze,tileetc.Tomakethesetypesafe,weagainturntotype-levelHListoperations.Takeexpand_dimsasanexample.Thisoperationinsertsataspeci￿cpositionadimensioninthetensorwithsize1,i.e.,atensortofshape(a,b,c),whencallingexpand_dims(t,axis=1),resultsinatensorofshape(a,1,b,c).Toencodethistyperelation,wede￿nethefollowingtype-levelfunctioninthestyleofShapeless:traitInsertAt[L<:HList,I<:Nat,X]extendsDepFn2[L,X]{typeOut<:HList}objectInsertAt{typeAux[L<:HList,I<:Nat,X,Out0<:HList]=InsertAt[L,I,X]{typeOut=Out0}implicitdefat0[T<:HList,H]:Aux[T,_0,H,H::T]=//implementationimplicitdefatN[H,T<:HList,P<:Nat,X,R<:HList](implicitev:InsertAt.Aux[T,P,X,R]):Aux[H::T,Succ[P],X,H::R]=//implementation}Thistype-levelfunctionInsertAt[L,I,X]representstheresultofthetype-levelcomputationthatinsertstypeXtotheI-thindexoftype-levellistL.Followingthe“Aux”pat-tern,aninstanceofInsertAt.Aux[L,I,X,R]witnessesthattheresultinglistisRwheninsertingXtotheI-thindexofL.Giventhistypeencoding,wecouldde￿neatypesafemethodexpand_dimsfortheTensor[D,A<:HList]traitthatadmitsanewtype(label/nameforthenewaxis)andatype-levelnaturalnumber(Nat)thatspeci￿esthepositiontowhichthetypebeinserted.Ittakesthefollowingtypedeclaration:defexpandDims[X,I<:Nat,B<:HList](axis:X,i:I)(implicitd:InsertAt.Aux[A,I,X,B],n:ToInt[I]):Tensor[D,B]=//implementationThisde￿nitionofexpandDimsiscompletelytypesafe:theaxesoftheoutputtensorwillbecompletelyknownatcom-piletime.Othertensormanipulationfunctionscouldfollowthispattern.3ComputationGraphFordeeplearningframeworks,thecorealgorithmisre-verseautomaticdi￿erentiation[GriewankandWalther2008;Wengert1964].Thistechnique,giventhesymbolicexpres-sionofthelossfunctionofthemodel,couldautomaticallydi￿erentiatethroughthecomputationgraphandreturnsthegradientforeachparameterinthenetwork.Computationgraphs,beingabstractsyntaxtrees(ASTs)ofsymbolicexpressions,arenaturallyencodedasgeneralizedalgebraicdatatypes(GADTs)usingcaseclasses[KennedyandRusso2005;Xietal.2003]inScala.WeproposethefollowingGADTde￿nition:sealedtraitExpr[X]{}caseclassInput[X]()extendsExpr[X]caseclassParam[X](varvalue:X)extendsExpr[X]caseclassConst[X](valvalue:X)extendsExpr[X]caseclassApply1[X,Y](f:Op1[X,Y],x:X)extendsExpr[Y]caseclassApply2[X1,X2,Y](f:Op2[X1,X2,Y],x1:X1,x2:X2)extendsExpr[Y]//higher-aritiesfollowInthede￿nitionabove,Expr[X]isthebasetraitforallabstractexpressionsthatconceptuallyholdvaluesoftypeX.Input[X]isanyinputtoneuralnetworksthathastypeX.ItissimilartoTensorFlow’stf.placeholder,anddoesnotholdanyvalue.TypesafeAbstractionsforTensorOperationsSCALA’17,October22–23,2017,Vancouver,CanadaParam[X]representsparametersofneuralnetworks.Thevaluestheycontainaresubjecttoupdateduringeverytrain-ingiterationoftheneuralnetwork.Const[X]representsconstantsinneuralnetworks.Dur-ingbackpropagationcomputationofgradients,gradientsonConstsarenotcomputed.Apply1[X,Y](f,x)(orhigher-aritygeneralizedApply2,etc.)nodesaretheintermediateresultoftheapplica-tionofadi￿erentiableoperatorftoasymbolicexpressionx.TheOp1,Op2etc.traitsaretraitsfordi￿erentiablefunctions,andwillbeelaboratedinthesectionbelow.4Di￿erentiableTensorOperatorsReverseautomaticdi￿erentiationisessentiallytheapplica-tionofthechainruleincalculusthroughthecomputationgraph:rx=r ·@ @x.Agenericdi￿erentiableunaryoperatorf:X!Yshouldbecapableofperformingbothforwardcomputation =f(x)andreversecomputationasstatedabove.Itcouldbeencodedasfollows(forbinaryorn-aryfunctions,thede￿nitionscanbeeasilygeneralized).traitOp1[X,Y]{//Performsforwardcomputation =f(x)defforward(x:X):Y//Performsbackwardcomputationrx=r ·@f@x(x)defbackward(dy:Y,y:Y,x:X):X}Inthebackwardmethod,becauseinmachinelearningappli-cationsthelossfunctionisalwaysascalarvalue,thetypeofr wouldbethesameas .Thereasonforincludingtheparameter inthebackwardmethodisbecausesomefunction’sderivativecanbeeasilyexpressedbythevalue ratherthanx.Forexample,thecommonsigmoidactivationfunction = (x)=11+e xhasthederivative@ @x= (1  ).5OperatorTypePolymorphismHowever,anoperatorcanapplytomultipledi￿erenttypes.Forexample,Add(+)canbeappliedtoanytwotensorsofthesametype,however,iftheiraxesdonotmatch,additionisnotpossible:itmakesnosensetoaddanimageoftypeTensor[Float,Width::Height::HNil]andanothertransposedTensor[Float,Height::Width::HNil].Or,matrixmultiplicationcanonlyapplytoanytwotensorsinwhichthetypeofthesecondaxisofthe￿rstmatrixmatchesthetypeofthe￿rstaxisofthesecondmatrix.Namely,foranytypesA,B,C,wehavematrixmultiplicationMatMul:(Matrix[A,B],Matrix[B,C])!Matrix[A,C].Itisobviousthatourde￿nedtraitsOpNarenotpolymorphictoallowthis.Tocapturearbitrarytyperelationsoninputs/outputsofoperatorsliketheseabove,wede￿nethefollowingpolymor-phicunaryoperator,whichcanbeconsideredasadi￿eren-tiableversionofShapeless’spolymorphicfunctionPoly1,inwhichtheactualimplementationofthefunctionisfoundthroughimplicitresolutionofinstancesoftypeF[X,Y](akintoShapeless’sCase.Aux[X,Y]).traitPolyOp1[F[X,Y]<:Op1[X,Y]]{//Appliesthisoperatortoasymbolicexpression//requirestheactualfunctionF[X,Y]befounddefapply[X,Y](x:Expr[X])(implicitf:F[X,Y]):Expr[Y]=Apply1(f,x)}Thisde￿nitioncapturesthefollowingtypesafetyguaran-tee:anoperatoroftypePolyOp1[F]canonlybeappliedtoanexpressionoftypeExpr[X]ifanimplicitinstanceofF[X,Y]isfound.Iffound,thetypeoftheresultingexpressionisExpr[Y].Wecanarbitrarilyde￿nethedesiredtypeguaran-teesinF:foreachdi￿erenttensoroperatoradi￿erentFisde￿nedtoexpresswhatkindsofoperandsitcanbeappliedto.Thisiseasilygeneralizedintohigher-aritypolymorphicoperators.Weusetwooperatorstodescribethetypepoly-morphismde￿nedabove.5.1MatrixMultiplicationWede￿neatype-polymorphicmatrixmultiplicationoperatorMatMul:objectMatMulextendsPolyOp2[MatMulF]traitMatMulF[X1,X2,Y]extendsOp2[X1,X2,Y]objectMatMulF{implicitdefimpl[D,A,B,C]:MatMulF[Tensor[D,A::B::HNil],Tensor[D,B::C::HNil],Tensor[D,A::C::HNil]]=//implementation}Thisessentiallyexpresses:MatMulcanonlybeappliedtotwoexpressionsExpr[X1]andExpr[X2]onlyifaninstanceofMatMulF[X1,X2,Y]isfound.ThisisfoundonlyifX1andX2taketheformofTensor[D,A::B::HNil]andTensor[D,B::C::HNil].Forexample,wehavethreetensorswiththefollowingtypes.valab:Tensor[Float,A::B::HNil]valac:Tensor[Float,A::C::HNil]valbc:Tensor[Float,B::C::HNil]WhencompilingMatMul(ab,bc),accordingtothedef-initionofPolyOp2,thecompilerattemptsto￿ndtheim-plicitparameterwithtypeMatMulF[Tensor[D,A::B::HNil],Tensor[D,B::C::HNil],Y].MatMulF.implmatchesthistypeandbytypeuni￿cationwegettypeYbeingTensor[D,A::C::HNil].HenceforththeresulttypeofMatMul(ab,bc)isTensor[D,A::C::HNil].SCALA’17,October22–23,2017,Vancouver,CanadaTongfeiChenHowever,whencompilingMatMul(ab,ac),thecom-pilerattemptstoresolvetheimplicitparameterwithtypeMatMulF[Tensor[D,A::B::HNil],Tensor[D,A::C::HNil],Y].Suchimplicitcouldnotberesolved,result-ingincompilationfailure,justaswedesired.Thisexampleshowsthatusingthepolymorphicfunctiontraits(PolyOpN)andimplicits,weachievedtheaxistypesafetyformatrixmultiplicationMatMul.5.2TensorContractionTensorcontractionisalsotermedvariouslyaseinsum(Ein-steinsummation)ortensordotinvariousPythonlibraries.Apartfromcommonusageindeeplearning,thisoperatorisalsowidelyfoundinthesum-productmessagepassingprocedure[Pearl1982]inthebeliefpropagationalgorithmforinferenceinprobabilisticgraphicalmodels[KollerandFriedman2009].Mathematically,giventwotensorsA,B,andalistofaxispairslalongwhichtensorsarecontracted,wehavetheresultCthatretainsaxesfrombothAandBnotspeci￿edinl,andallotheraxesmarginalized(summed)out.Thisgeneraloperationsubsumesmanycommonlinearalgebraoperations:•Dotproduct:C=ÕiAiBi;•Matrixmultiplication:Cik=ÕjAijBjk;•Tensorproduct:Ci1···imj1···jn=Ai1···imBj1,···,jn.InpopularlibrarieslikeNumPyorTensorFlow,wewritetensordot(A,B,axes=[[1,0]])tospecifytheaxesalongwhichtensorsarecontracted.Manuallywritingtheaxescouldbedi￿cult,howeverusingthetypefulencodingoftensors,thesecouldbeexpressedsuccinctly.ConsidertwotensorsAandBwithaxes(A)={a1,···,am}andaxes(B)={b1,···,bn}5.Wede￿nethenaturaltensorcontractionA./Basthecontractionofalltheaxesthatsharethesamename/label(similartothenatu-raljoin[Codd1979]inrelationaldatabaseswherebycolumnswiththesamenamesarejoined).Forexample,thenaturaltensorcontractionofmatricesA[i,j]andB[j,k]isthenaturalmatrixmultiplicationAB,butthenaturaltensorcontractionofmatricesA[i,j]andB[k,j]isatransposedproductABTasthecontractionalignsaxeswiththesamenamej.Whatisaxes(A./B)?BecauseaxesthatoccurinbothAandBarecontracted,wecouldseethataxes(A./B)=axes(A)4axes(B),where4isthesymmetricsetdi￿erence.Wecouldde￿neatypelevelfunctiontocapturesymmetricdi￿erence:traitSymDiff[A<:HList,B<:HList]extendsDepFn2[A,B]{typeOut<:HList}Giventhis,wecouldencodenaturaltensorcontractiontype-fullyandtypesafelywithSymDiffasanimplicitevidenceas5ForatensorAwithtypeTensor[D,Axes<:HList],denoteitsaxesbyaxes(A)=Axes,whichistheHList.objectContractextendsPolyOp2[ContractF]traitContractF[X1,X2,Y]extendsOp2[X1,X2,Y]objectContractF{implicitdefimpl[D,A<:HList,B<:HList,C<:HList](implicitC:SymDiff.Aux[A,B,C]):ContractF[Tensor[D,A],Tensor[D,B],Tensor[D,C]]=//implementation}Naturaltensorcontractionalsoexhibitsanelegantprop-ertyforautomaticdi￿erentiation(proofomitted):rA=r(A./B)./B;rB=r(A./B)./A.TheequationabovetypecheckssinceA=(A4B)4B;B=(A4B)4A.6CommonNeuralLayersIndeeplearningapplications,multiplesneurallayers(func-tionofsymbolicexpressions)areoftenstackedtogether(functioncomposition).Thereareacollectionofcommonlayersthatperformscertainfunctions,whosetypesafeen-codingswedescribebelow.6.1Fully-connectedLayersOneofthemostcommonneuralnetworklayeristhefully-connectedlayer.Itisessentiallyana￿netransformationy=Wx+bontheinputvector.ItcouldbeencodedascaseclassAffine[D,A,B](W:Param[Tensor[D,B::A::HNil]],b:Param[Tensor[D,B::HNil]])extends(Expr[Tensor[D,A::HNil]]=>Expr[Tensor[D,B::HNil]])wheretheinputvectorhastypeTensor[D,A::HNil]andtheoutputhastypeTensor[D,B::HNil].6.2ConvolutionalLayersConvolutionallayersconvolvesakernelwiththelayerin-puttoproduceatensorofoutputs.Thisiswidelyusedinvision/speech/etc.foritsshiftinvariantproperties.Forthe2-dimensionalcasecommonincomputervision,itcouldbeencodedascaseclassConvolution2D[D,W,H,IC,OC](W:Param[Tensor[D,OC::IC::HNil],b:Param[Tensor[D,OC::HNil])extends(Expr[Tensor[D,W::H::IC::HNil]]=>Expr[Tensor[D,W::H::OC::HNil]])wheretheinput(animage)hasaxeswidth,heightandinputchannel(e.g.RGB),andtheoutputhasthreeaxes:width,heightandoutputchannel.6.3RecursiveLayersSequentialrecurrentneuralnetworkscanbeconsideredasasemiautomaton(S,I, )whereS=RhisthesetofhiddenTypesafeAbstractionsforTensorOperationsSCALA’17,October22–23,2017,Vancouver,Canadastates,I=Rdisthesetofinputvectors,and :(S,I)!Sisthetransitionfunction,i.e.therecurrentunit.WeencodetherecurrentunitasatypetypeRecurrentUnit[D,S,I]=(Tensor[D,S::HNil],Tensor[D,I::HNil])=>Tensor[D,S::HNil]ArecurrentunitsuchasLSTM[HochreiterandSchmidhuber1997]wouldbeencodedasasubtypeofRecurrentUnit.Givenaninputsequence(Seq[Expr[Tensor[D,I::HNil]]],anexamplewouldbeasentenceinwhicheachwordisrepresentedbyavectorrepresentation),wecouldnaturallyuseScala’sdefaultcombinatorfoldLeft/Rightonsequenceswitharecurrentunittogetthe￿nalhiddenstate(Expr[Tensor[D,H::HNil]])andscanLeft/Righttogetallthehiddenstates(Seq[Expr[Tensor[D,H::HNil]]]).Thisgeneralizestotrees(e.g.sentimentanaly-sisondependencyparsetreesofsentences[Taietal.2015])wherewecouldde￿neatreerecursionunitanduseitwithafoldoperation(catamorphism[SheardandFegaras1993])ontrees.7UsabilityExploitingcomplexlibrariessuchasShapelessmayimposesomedi￿cultytoprogrammers:slowcompilingspeed(re-cursiveimplicitresolution)andconfusingcompilererrormessages.Sincetensorsingeneralmachinelearningareusuallyatmost5dimensions(invideoprocessing,atensorcouldhave5dimensions:batch,time,height,width,colorchannel),per-formingtype-levelHListoperationsatcompiletimeorrun-timeareanegligentoverhead.Compilinganormal3-layerneuralnetworkjusttakesaninstant.WehaveimplementedasimpleXORnetwork(2,2,2neu-ronsforeachlayer)andasimpleimageclassi￿cationnetworkforMNIST(784,300,100,10neuronsforeachlayer).Futureworkincludesdeepconvolutionalnetworksforimageclassi-￿cation;recurrentnetworksfortextannotation;orsequencetosequencetransductiontaskssuchasmachinetranslation.Runtimebenchmarkwouldbeleftasfuturework,sincewedonothaveanativeCPUorGPUunderlyingimplementationasofnow.CompilererrorscouldbecustomizedbyusingtheScala@implicitNotFoundannotation.Usingthematrixmultipli-cation(MatMul)exampleabove,wecouldannotateasfollows.@implicitNotFound( CannotapplyMatMulto${X1}and${X2}. )traitMatMulF[X1,X2,Y]extendsOp2[X1,X2,Y]Whenmultiplyingtwotensorsthatshouldnotbemultiplied(e.g.,callingMatMulontwotensorswithtypeTensor[Float,A::B::HNil]andTensor[Float,C::B::HNil]),wewouldgetacompilermessage“Cannotap-plyMatMultoTensor[Float,::[A,::[B,HNil]]]andTensor[Float,::[C,::[B,HNil]]]”6locatedjustattheapplicationsiteoftheoperatorMatMul.Additionally,IDEs(e.g.ScalaplugininIn-telliJIDEA)willalsodetectthiskindoftypeerrorwhileediting.Theseerrorreportingmechanismswouldgreatlyaidprogrammerstoidentifypotentialtypingerrors.8RelatedWorkThisworkisrelatedtotheresearchareaoftypedlinearalgebra.Mostrecentresearchfocusedontypingthesizeofthemultidimensionalarraysbyusingtype-levelintegers,ortypingtheunitofmeasurementsofeachdimension,insteadofwhatthisworkispresenting:typingthemeaningofeachaxisasalabel.[Eaton2006]implementedatypedlinearalgebrasys-teminHaskellwheredimensionsizesareencodedinthetypesystem.[Gri￿oen2015]proposeddimensionedmatri-ces/tensorsinwhicheachaxisisdimensionedwithaunitofmeasurement,andbythistheunitofmeasurementsofeachdimensionoftensorscanbedeterminedatcompiletime.[MuranushiandEisenberg2014]extendsthisideatobeusedinastrophysicsresearchandtypedthelengthofeachdimensionbyusingtype-levelintegers.Therehasbeenworkthatimplementstensors/neuralnet-worksintypesafelanguagessuchasJava(DeepLearning4j7)orHaskell(Grenade8).NoneoftheseachievedtheHList-backedtypefulnessandtypesafetyasdescribedinthispaper.9ConclusionandFutureWorkAScala-basedtypesafeabstractionaroundtensorsinneuralnetworksispresentedinthispaper.Wehavedemonstratedthetypesafetyandexpressivenessofourabstractionthroughvariousexamplesthatarecommonindeeplearningtasks.Type-levelencodingofaxissizeisnotexploredthispaper.MethodsusingliteraltypesordependenttypesinScalaareoutofscopeofthispaperandleftasfuturework.Futureworkalsoincludesfurtherimplementationthisframework:addingoptimizedCPU/GPUoperations,imple-mentingautomaticbatching,varioustrainingmethods,etc.Wehopethatthistoolwouldeventuallyevolveintoafully-￿edgedlibraryfortypesafedeeplearninginScala.AcknowledgmentsTheauthorthanksthethreeanonymousreviewers,theshep-herdOlegKiselyov,andMatthewFrancis-Landau,YuhuanJiangandTimVieirawhosesuggestionsandadvicegreatlyhelpedimproveandclarifythispaper.6TorendertheHListin￿xtype::inamorehuman-readableway,onecouldusetheinfixfunctionalityintheSplainScalacompilerplugin(h￿ps://github.com/tek/splain).7github.com/deeplearning4j/deeplearning4j8github.com/HuwCampbell/grenadeSCALA’17,October22–23,2017,Vancouver,CanadaTongfeiChenReferencesMartínAbadi,AshishAgarwal,PaulBarham,EugeneBrevdo,ZhifengChen,CraigCitro,GregS.Corrado,AndyDavis,Je￿reyDean,MatthieuDevin,SanjayGhemawat,IanGoodfellow,AndrewHarp,Geo￿reyIrving,MichaelIsard,YangqingJia,RafalJozefowicz,LukaszKaiser,ManjunathKudlur,JoshLevenberg,DanMané,RajatMonga,SherryMoore,DerekMurray,ChrisOlah,MikeSchuster,JonathonShlens,BenoitSteiner,IlyaSutskever,KunalTalwar,PaulTucker,VincentVanhoucke,VijayVasudevan,FernandaViégas,OriolVinyals,PeteWarden,MartinWat-tenberg,MartinWicke,YuanYu,andXiaoqiangZheng.2015.Tensor-Flow:Large-ScaleMachineLearningonHeterogeneousSystems.(2015).h￿p://tensorflow.org/Softwareavailablefromtensor￿ow.org.EdgarFCodd.1979.Extendingthedatabaserelationalmodeltocapturemoremeaning.ACMTransactionsonDatabaseSystems(TODS)4,4(1979),397–434.FrederikEaton.2006.StaticallytypedlinearalgebrainHaskell.InProceed-ingsofthe2006ACMSIGPLANworkshoponHaskell.ACM,120–121.AndreasGriewankandAndreaWalther.2008.Evaluatingderivatives:prin-ciplesandtechniquesofalgorithmicdi￿erentiation.SIAM.P.R.Gri￿oen.2015.TypeInferenceforArrayProgrammingwithDi-mensionedVectorSpaces.InProceedingsofthe27thSymposiumontheImplementationandApplicationofFunctionalProgrammingLan-guages(IFL’15).ACM,NewYork,NY,USA,Article4,12pages.h￿ps://doi.org/10.1145/2897336.2897341SeppHochreiterandJürgenSchmidhuber.1997.Longshort-termmemory.Neuralcomputation9,8(1997),1735–1780.AndrewKennedyandClaudioVRusso.2005.Generalizedalgebraicdatatypesandobject-orientedprogramming.ACMSIGPLANNotices40,10(2005),21–40.DaphneKollerandNirFriedman.2009.Probabilisticgraphicalmodels:principlesandtechniques.MITpress.TakayukiMuranushiandRichardAEisenberg.2014.Experiencereport:Type-checkingpolymorphicunitsforastrophysicsresearchinHaskell.InACMSIGPLANNotices,Vol.49.ACM,31–38.GrahamNeubig,ChrisDyer,YoavGoldberg,AustinMatthews,WaleedAmmar,AntoniosAnastasopoulos,MiguelBallesteros,DavidChiang,DanielClothiaux,TrevorCohn,KevinDuh,ManaalFaruqui,CynthiaGan,DanGarrette,YangfengJi,LingpengKong,AdhigunaKuncoro,GauravKumar,ChaitanyaMalaviya,PaulMichel,YusukeOda,MatthewRichardson,NaomiSaphra,SwabhaSwayamdipta,andPengchengYin.2017.DyNet:TheDynamicNeuralNetworkToolkit.arXivpreprintarXiv:1701.03980(2017).JudeaPearl.1982.ReverendBayesoninferenceengines:Adistributedhierar-chicalapproach.CognitiveSystemsLaboratory,SchoolofEngineeringandAppliedScience,UniversityofCalifornia,LosAngeles.TimSheardandLeonidasFegaras.1993.Afoldforallseasons.InProceedingsoftheconferenceonFunctionalprogramminglanguagesandcomputerarchitecture.ACM,233–242.KaiShengTai,RichardSocher,andChristopherDManning.2015.Improvedsemanticrepresentationsfromtree-structuredlongshort-termmemorynetworks.arXivpreprintarXiv:1503.00075(2015).TheanoDevelopmentTeam.2016.Theano:APythonframeworkforfastcomputationofmathematicalexpressions.arXive-printsabs/1605.02688(May2016).h￿p://arxiv.org/abs/1605.02688StéfanvanderWalt,SChrisColbert,andGaelVaroquaux.2011.TheNumPyarray:astructurefore￿cientnumericalcomputation.ComputinginScience&Engineering13,2(2011),22–30.R.E.Wengert.1964.ASimpleAutomaticDerivativeEvaluationProgram.Commun.ACM7,8(Aug.1964),463–464.h￿ps://doi.org/10.1145/355586.364791HongweiXi,ChiyanChen,andGangChen.2003.Guardedrecursivedatatypeconstructors.InACMSIGPLANNotices,Vol.38.ACM,224–235.