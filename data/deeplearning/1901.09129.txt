Optimal k-Coverage Charging Problem

Xuan Li, Miao Jin
Center for Advanced Computer Studies, University of Louisiana at Lafayette, Lafayette, LA 70504

9
1
0
2

y
a
M
2
1

]
I

N
.
s
c
[

2
v
9
2
1
9
0
.
1
0
9
1
:
v
i
X
r
a

Abstract—Wireless rechargeable sensor networks, consisting
of sensor nodes with rechargeable batteries and mobile chargers
to replenish their batteries, have gradually become a promising
solution to the bottleneck of energy limitation that hinders the
wide deployment of wireless sensor networks (WSN). In this
paper, we focus on the mobile charger scheduling and path
optimization scenario in which the k-coverage ability of a network
system needs to be maintained. We formulate the optimal k-
coverage charging problem of ﬁnding a feasible path for a mobile
charger to charge a set of sensor nodes within their estimated
charging deadlines under the constraint of maintaining the k-
coverage ability of the network system, with an objective of
minimizing the energy consumption on traveling per tour. We
show the hardness of the problem that even ﬁnding a feasible
path for the trivial case of the problem is an NP-complete one.
We model the problem and apply dynamic programming to
design an algorithm that ﬁnds an exact solution to the opti-
mal k-coverage charging problem. However, the computational
complexity is still prohibitive for large size networks. We then
introduce Deep Q-learning, a reinforcement learning algorithm
to tackle the problem. Traditional heuristic or Mixed-Integer
and Constraint Programming approaches to such combinatorial
optimization problems need to identify domain-speciﬁc heuristics,
while reinforcement learning algorithms could discover domain-
speciﬁc control policies automatically. Speciﬁcally, Deep Q-
Learning applies Deep Neural Network (DNN) to approximate
the reward function that provides feedback to the control policy.
It can handle complicated problems with huge size of states
like the optimal k-coverage charging problem. It is also fast to
learn the reward function. We implement three other heuristic
algorithms for comparison and then conduct extensive simula-
tions with experimental data included. Results demonstrate that
the proposed Deep Q-Learning algorithm consistently produces
optimal solutions or closed ones and signiﬁcantly outperforms
other methods.

Index Terms—Mobile charger, k-Coverage, Reinforcement

learning, Wireless rechargeable sensor networks

I. INTRODUCTION

Wireless rechargeable sensor networks, consisting of sensor
nodes with rechargeable batteries and mobile chargers to
replenish their batteries, have gradually become a promising
solution to the bottleneck of energy limitation that hinders the
wide deployment of wireless sensor networks (WSN) [1]–[4].
The mobile charger scheduling and path optimization problem
optimizes the trajectory of a mobile charger (e.g., a mobile
robot) to maintain the operation of a WSN system. Variants
are studied by considering different optimization goals, appli-
cation scenarios, and constraints. Some research works extend
the mobile charger scheduling and path optimization problem
from one charger to multiple ones [5], [6] and static sensor
nodes to mobile ones [7], [8].

Many of the prior works focused on the problem of max-
imizing the number of nodes charged within a ﬁxed time

horizon or energy constraint with the assumption that each
sensor node contributes equally to the sensing quality of a
network. However, a full area coverage is a basic requirement
of WSN deployment
to monitor a certain area. Multiple
coverage, where each point of the ﬁeld of interest (FoI)
least k different sensors with k > 1 (k-
is covered by at
coverage) , is often applied to increase the sensing accuracy
of data fusion and enhance the fault tolerance in case of node
failures [9]–[14]. Existing approaches to achieve k-coverage
deploy a set of sensor nodes over a FoI either in a randomized
way [15], [16] or with a regular pattern [13], [17]. Regular
deployments need less sensor nodes than randomized ones, but
they require centralized coordination and a FoI with regular-
shape. A common practice is a high density of sensor nodes
randomly distributed over the monitored FoI.

In this paper, we focus on the mobile charger scheduling
scenario in which the k-coverage ability of a network system
needs to be maintained. A node sends a charging request with
its position information and a charging deadline estimated
based on its current residual energy and battery consump-
tion rate. A mobile charger seeks a path to charge sensor
nodes before their charging deadlines under the constraint of
maintaining the k-coverage ability of the monitored area, with
an objective of maximizing the energy usage efﬁciency, i.e.,
minimizing the energy consumption on traveling per tour.

We formulate the optimal k-coverage charging problem and
show its hardness. We then construct a directed graph to
model the problem and prove that it is a directed acyclic
graph (DAG). We ﬁrst apply dynamic programming to search
for an optimal charging path. However, the computational
complexity is still prohibitive for large size networks. We then
introduce Deep Q-learning, a reinforcement learning algorithm
to tackle the problem. Traditional heuristic or Mixed-Integer
and Constraint Programming approaches to such combinatorial
optimization problems need to identify domain-speciﬁc heuris-
tics, while reinforcement learning algorithms could discover
domain-speciﬁc control policies automatically. Speciﬁcally,
Deep Q-Learning applies Deep Neural Network (DNN) to
approximate the reward function that provides feedback to the
control policy. It can handle complicated problems with huge
size of states like the optimal k-coverage charging problem. It
is also fast to learn the reward function. We implement three
other heuristic algorithms for comparison and then conduct
extensive simulations with experimental data included. Results
demonstrate that the proposed Deep Q-Learning algorithm
consistently produces optimal solutions or closed ones and
signiﬁcantly outperforms other methods.

The main contributions of this work are as follows:

 
 
 
 
 
 
• We formulate the optimal k-coverage charging problem.
• We prove the NP-hardness of the optimal k-coverage

charging problem.

• We model the optimal k-coverage charging problem and
apply dynamic programming and reinforcement learning
techniques to design algorithms to tackle the problem
with extensive simulations conducted to verify their ef-
fectiveness.

The rest of the paper is organized as follows. We review
the closely related works in Section II. We formulate the
optimal k-coverage charging problem and analyze its hardness
in Section III. We present our algorithms in Sections V and VI,
respectively. Section VII presents the simulation results. Sec-
tion VIII concludes the paper.

II. RELATED WORKS

The problem we study in the paper is closely related with
the mobile charger scheduling and traveling salesman problem
with deadline. We give a brief review of the related works.

A. Mobile Charger Scheduling

The mobile charger scheduling problem optimizes the tra-
jectory of a mobile charger (e.g., a mobile robot) to maintain
the operation of a network system. There are many research
works in this area with variants of the problem. Here we only
list some of the most recent and representative works.

Shi et al. [1] consider the scenario of a wireless vehicle
charger periodically traveling inside a sensor network and
charging sensor nodes. They aim to minimize the time spent
on path in each cycle. Multi-node wireless energy transfer
technology is considered in [18]. The authors propose a
cellular structure that partitions a two-dimensional plane into
adjacent hexagonal cells such that a wireless vehicle charger
visits the center of each cell and charges several sensor nodes
at the same time. Xie et al. [19], [20] consider the scenario of
co-locating a mobile base station in a wireless charging vehicle
and investigate the optimization problems of entire system.
Liang et al. [4] seek a charging tour that maximizes the total
energy replenished to sensor nodes by a mobile charger with a
limit energy capacity. Dai et al. [21] considers the scenario that
both chargers and rechargeable devices are static. They study
the optimization problem to maximize the overall effective
charging energy of all rechargeable devices and minimize
the total charging time without violating the electromagnetic
radiation (EMR) safety.

Energy replenishment in robotic sensor networks is dis-
cussed in [7], [8]. Speciﬁcally, He et al. [7] aim to minimize
the traveling distance of a mobile charger and keep all the
mobile robots charged before their deadlines. Chen et al. [8]
seek a charging path maximizing the number of mobile robots
charged within a limited time or energy budget.

Multiple mobile chargers in a wireless rechargeable net-
work raise new challenges but can work more efﬁciently.
Collaborative mobile charging, where mobile chargers are
allowed to intentionally transfer energy between themselves, is
proposed in [22] to optimize energy usage effectiveness. Liang

et al. [5] minimize the number of mobile charging vehicles
to charge sensors in a large-scale WSN so that none of the
sensors will run out of energy. Lin et al. [6] propose a real-
time temporal and spatial-collaborative charging scheme for
multiple mobile chargers by combining temporal requirements
as well as spatial features into a single priority metric to sort
real-time charging requests.

B. Traveling Salesman Problem with Deadline

The Traveling Salesman Problem (TSP), a class of combina-
torial optimization problems, has been extensively studied with
many approximation and heuristic algorithms proposed [23].
Deadline-TSP is a relevant extensions of TSP, which contains
two type of problems. One is prize collecting TSP problem
with deadlines [24], [25], where each node has a prize and
need to be visited before their deadline, and we want to max-
imize the prizes with the limit of time or length of the walk.
log N-approximation solution exists for prize collecting TSP
with deadlines problem. The second type of the problem is
that all the nodes need to be visited before their deadlines with
a minimum traveling cost. The added time constraint seems
restrict the search space of solution, but it actually renders the
problem even more difﬁcult. Even ﬁnding a feasible path for
such problems is NP-complete [26]. Some exact algorithms
for type II TSP with deadline problem have been proposed in
[26]–[28]. Compared with ﬁrst type of TSP-deadline problem,
even ﬁnding a feasible path for the trivial case of the type II
TSP-deadline problem is an NP-complete one.

III. NETWORK MODEL AND PROBLEM FORMULATION

Before giving a formal deﬁnition of the k-coverage charging
problem studied in the paper, we ﬁrst introduce the wireless
sensor network model employed in this research.

A. Model of Wireless Sensor Network

We assume a set of stationary sensor nodes, V = {vi|1 ≤ i ≤
n}, deployed over a planar FoI with locations, P = {pi|1 ≤ i ≤
n}. For each sensor node vi, we assume a disk sensing model
with sensing range r. Speciﬁcally, denote A the FoI, if the
Euclidean distance between a point q ∈ A and node position
pi is within distance r, i.e., ||pi − q||L2 ≤ r, then the point q
is covered by the sensor vi, and we use vi(p) = 1 to represent
it, as shown in equation (1):

vi(q) =

1
0

(

||pi − q||L2 ≤ r
otherwise

(1)

Deﬁnition 1 (Full Coverage). If for any point q ∈ A, there
exists at least one sensor node covering it, i.e., ∑i vi(q) ≥ 1,
then area A is full covered.

Deﬁnition 2 (K-Coverage). If for any point q ∈ A, there exist
at least k ≥ 1 sensor nodes covering it, i.e., ∑i vi(q) ≥ k, then
area A is k-covered.

It is obvious that full coverage is a special case of k-

coverage with k = 1.

B. Problem Statement

A sensor node vi is equipped with a rechargeable battery
with capacity B. Bi(t) denotes the residual energy of sensor
node i at time t. Charger sends its departure time denoted
as t0 from service station to each sensor. When receiving the
message, vi estimates its residual energy at t0 denoted as Bi(t0).
If it is less than an energy threshold α, i.e., Bi(t0)/B ≤ α, vi
sends a charging request (id, pi, Di) to charger. The request
includes the ID, position, and energy exhausted time of sensor
vi, denoted as id, pi, and Di, respectively. The estimated energy
exhausted time, i.e., charging deadline, is estimated based on
the residual energy at t0 and an average battery consumption
rate denoted as βi. Speciﬁcally, Di = Bi(t0)/βi. Note that nodes
may have different energy consumption rates.

A mobile charger with an average moving speed s is re-
sponsible for selecting and charging sensors sending requests.
We assume that the time spent on charging path is less then
the operation time of sensors, so a sensor node only needs
to be charged once in each tour. Unless under an extremely
dense sensor deployment, we consider that a charger charges
sensor nodes one by one because the energy efﬁciency reduces
dramatically with distance, e.g., the energy efﬁciency drops to
45 percent when the charging distance is 2m (6.56 ft) [29].
The energy transfer rate of charger is denoted as rc.

We consider a charging path scheduling and optimization
problem. A mobile charger selects and charges a number of
sensor nodes before their deadlines to guarantee k-coverage
of area A, and it seeks a path with a minimum energy
consumption on traveling. Speciﬁcally, the charging time is
deﬁned as the following:

Deﬁnition 3 (Charging Time). Denote P a charging path and
tP(vi) the charging time along P at node vi. If P goes from
nodes vi to v j, the charging time begins at node v j is

s.t.

min |P|
n
∑
i=1
tP(vi) ≤ Di, ∀vi ∈ P.

vi(q) ≥ k, ∀q ∈ A.

(3a)

(3b)

(3c)

Note that a charger does not need to respond all the nodes

sending requests.

D. Problem Hardness

We prove the NP-hardness of optimal k-coverage charging

problem below.

Theorem 1. The optimal k-coverage charging problem is NP-
hard.

Proof. To prove the NP-hardness of the optimal k-coverage
charging problem, we prove that the NP-hard problem: type
II Traveling Salesman Problem with Deadline can be reduced
to the trivial case of the optimal k-coverage charging problem
in polynomial time. This type of Traveling Salesman Problem
is that all the nodes need to be visited before their deadlines
with a minimum traveling cost.

We consider a trivial case of the optimal k-coverage charg-
ing problem: we require k = 1 and assume that the initial
deployment of sensor nodes has no coverage redundancy.
A charger needs to charge all
the sensor nodes sending
requests before their deadlines. It is straightforward to see
that the solution of type II of Traveling Salesman Problem with
Deadline is also the solution of the trivial case of the optimal k-
coverage charging problem and vice versa. Since even ﬁnding
a feasible path for this type of Traveling Salesman Problem
with Deadline is NP-complete [26], the optimal k-coverage
charging problem is NP-hard.

inf






tP(vi) + B−Bi(tP(vi))

+

rc

if tP(vi) + B−Bi(tP(vi))

di j
s

rc

tP(v j) =

otherwise,

IV. PROBLEM DISCRETIZATION

+

di j
s ≤ D j

(2)

Given a sensor network with n sensor nodes randomly
deployed over a FoI, we assume the network with a reasonable
density such that the area is at least k-coveraged initially.

A. Area Segmentation

where di j the Euclidean distance between nodes vi and v j
and s is the average speed of a charger. The residual energy
Bi(t) is estimated as Bi(t) = Bi(t0) − βi ∗ (t − t0).

C. Problem Formulation

The optimal k-coverage charging problem can be formulated

as follows.

Deﬁnition 4 (Optimal k-coverage charging problem). Given
a set of sensor nodes V = {vi|1 ≤ i ≤ n}, randomly deployed
over a planar region A with locations P = {pi|1 ≤ i ≤ n} such
that every point of A has been at least k covered initially, the
optimal k-coverage charging problem is to schedule a charging
path P

The sensing range of a sensor node vi

is a disk-shape
region centered at pi with radius ri. These disk-shape sensing
regions of a network divide a planar FoI A into a set of
subregions, marked as A = {ai|1 ≤ i ≤ m}. Then ∑n
i=1 vi(ai) is
the number of sensors with ai within their sensing ranges. As
the deﬁnition of the optimal k-coverage charging problem, we
assume ∑n
i=1 vi(ai) ≥ k in the initial deployment of a network.
Denote r(ai) the number of sensor nodes sending charging
requests among the ∑n
i=1 vi(ai) ones. Three cases exist for
subregion ai:

i=1 vi(ai) − r(ai) ≥ k: a charger may ignore all

Case I: ∑n

the requests.

Case II: ∑n

i=1 vi(ai) − r(ai) < k: a
charger needs to charge all the sensor nodes sending requests

i=1 vi(ai) = k and ∑n

with sensing ranges containing area ai within their charging
windows.

Case III: ∑n

i=1 vi(ai) > k and ∑n

i=1 vi(ai) − r(ai) < k: a
charger may choose to charge only k − ∑n
i=1 vi(ai) + r(ai)
sensor nodes within their charging windows among those
sending requests with sensing ranges containing area ai .

A table denoted as T with size m, is constructed to store the
minimum number of sensors to charge for each ai. Speciﬁcally,
T [i] = k−∑n
i=1 vi(ai)+r(ai). If the value is negative, we simply
set T [i] to zero.

B. Time Discretization and Graph Construction

For a sensor node vi with a charging request sent out, we
divide its time window [t0,t0 + Di] into a set of time units
{tk
i = Di. We represent
node vi with a set of discretized nodes {vi(tk
i )|0 ≤ k ≤ Di},
where vi(tk

i ) represents Node vi at time tk
i .

i |0 ≤ k ≤ Di}, where t0

i = t0 and tDi

We then construct a directed graph denoted as G with

vertices and edges deﬁned as follows.

Vertices. The vertex set V (G) includes the discretized sen-
i )|0 ≤ k ≤ Di}.
j ) from vi(tk
i )

sor nodes sending charging requests, i.e., {vi(tk
−−−−−−−→
i )v j(tk′
vi(tk

Edges. There exists a directed edge

to v j(tk′

j ) in the edge set E(G) if and only if
B − Bi
rc
B − Bi
rc

di j
s
di j
s

> tk′−1
j

≤ tk′
j ,

tk
i +

tk
i +

+

+

,

where k′ > 0.

A directed edge ensures that a charger arrives at sensor node
v j before its deadline. The charging time is the arrival time of
the charger.

Theorem 2. G is a directed acyclic graph (DAG).

i ) and v j(tk′

Proof. Suppose there exists a cycle in G. Assume vertices
vi(tk
j ) are on the cycle. Along the directed path
from vi(tk
j . However,
along the directed path from v j(tk′
j < tk
i .
Contradiction, so G is a directed acyclic graph.

i < tk′
i ), we have tk′

j ), it is obvious that tk

i ) to v j(tk′

j ) to vi(tk

Deﬁnition 5 (Clique). A set of nodes {vi(tk
i )|0 ≤ k ≤ Di} in
G is deﬁned as a clique if they correspond to the same node
vi at different time units.

Deﬁnition 6 (Feasible Path). A path P in G is a feasible one
if it passes no more than one vertex of a clique. At the same
time, charging along P satisﬁes the k-coverage requirement of
the given network.

color coding technique introduced in [30] to assign each vertex
a color. Speciﬁcally, we generate a coloring function cv : V →
{1, ..., n} that assigns each sensor node a unique node color.
Each sensor node then passes its node color to its discretized
ones. A path in G is said to be colorful if each vertex on it is
colored by a distinct node color. It is obvious that a colorful
path in G passes no more than one discretized vertex of a
sensor node.

To take into the consideration of traveling distance from
service station to individual sensor node, we add an extra
vertex denoted as v0 and connect it with directed edges to
−−−−→
vertices in G, i.e., {vi(tk
v0vi(t0
i )
is the Euclidean distance between the service station and
sensor node vi. The table T constructed in Sec. IV-A is stored
at v0.

i )|k == 0}. The length of edge

We ﬁrst topologically sort the new graph, i.e., G + v0. Then
we start from v0 to ﬁnd colorful paths by traversing from left
to right in linearized order. Speciﬁcally, v0 checks neighbors
connected with outgoing edges and sends table T to those
contributing to the decrease of at least one table entry. Once
a vertex vi(t0
i ) checks the subregions within
its sensing range and updates the corresponding entries of T .
vi(t0
i ))} and stores
with T , which indicates a colorful path of length |C|.

i ) also generates a color set C = {c(vi(t0

i ) receives T , vi(t0

Similarly, suppose the algorithm has traversed to vertex
i ), we check each color set C stored at vi(tk
vi(tk
i ) and its
j )) 6∈ C and charging v j(tk′
j ). If c(v j(tk′
outgoing edge v j(tk′
j )
helps decrease at least one entry of T associated with C, we
add the color set C = {C + c(v j(tk′
j ))} along with the updated
T to the collection of v j(tk′

j ).

After the update of the last vertex in linearized order, we
check the stored T s in each node and identify those with all
zero entries. A color set associated with a T with all zero
entries represents a colorful path that is a feasible solution of
the k-coverage problem.

A path can be easily recovered from a color set. The basic
idea is to start from vertex vi(tk
i ) with a color set C. We
check the stored color sets of vertices connected to vi(tk
i ) with
incoming edges. Assume we identify a neighbor node v j(tk′
j )
storing a color set C − c(vi(tk
i )), then we continue to trace back
the path from v j(tk′
i )). When we
trace back to v0, we have recovered the charging path. Among
all feasible charging paths, the one with a minimal traveling
distance is the optimal one.

j ) with a color set C − c(vi(tk

Lemma 1. The algorithm returns an optimal solution of the
k-coverage charging problem, i.e., a feasible path maximizing
the energy usage efﬁciency, if it exists.

V. DYNAMIC PROGRAMMING ALGORITHM

Considering that the constructed graph G is a DAG that can
be topologically sorted, we design a dynamic programming
algorithm to ﬁnd an optimal charging path for the k-coverage
charging problem.

To make sure that the computed charging path passes no
more than one discretized vertex of a sensor node, we apply the

Proof. We ﬁrst show that the algorithm returns a feasible
path. A path returned by the algorithm is a colorful one that
guarantees the path passes a sensor node no more than once.
In the meantime, charging time at each sensor node along the
path is before its deadline, otherwise a directed edge along
the path won’t exist. Array T with all zero entries makes sure
that the k-coverage is maintained.

When the algorithm has traversed to the ith node in lin-
earized order, each colorful path passing through the node has
been stored in the node.

Note that the computational complexity of the dynamic
programming algorithm can increase exponentially in the
worst case because the stored color sets at a vertex can increase
exponentially to the size of sensor nodes n.

VI. DEEP Q-LEARNING ALGORITHM

A. Motivation

A combinatorial optimization problem searches for an op-
timal solution of an objective function under a set of con-
straints. The domain of the objective function is discrete, but
prohibitive for an exhaustive search. An optimal solution is a
feasible one satisfying the set of constraints and minimizing
the value of the objective function.

Previous heuristic or mixed-integer and constraint program-
ming approaches to combinatorial optimization problems need
to identify domain-speciﬁc heuristics, while reinforcement
learning algorithms discover domain-speciﬁc control policies
automatically by exploring different solutions and evaluating
their qualities as rewards. These rewards are then provided as
feedback to improve the future control policy [31].

The optimal k-coverage charging problem is a combinatorial
optimization one. We introduce Deep Q-learning, a reinforce-
learning algorithm to tackle the optimal k-coverage
ment
charging problem. “Q” stands for the quality or reward of an
action taken in a given state. In Q-learning, an agent maintains
a state-action pair function stored as a Q-table Q[S, A] where
S is a set of states and A is a set of actions. A Q-value of the
table estimates how good a particular action will be in a given
state, or what reward the action is expected to bring.

Q-Learning is a model-free reinforcement learning algo-
rithm, so there is no need to ﬁnd all the combinations to
check the existence of a solution. An agent will choose next
state based on current one and stored state-action rewards,
requiring less computation and storage space comapred with
model-based reinforcement learning algorithms. Q-Learning is
a also temporal-difference reinforcement learning algorithm.
An agent can learn online after every step, and even from
an incomplete sequences (a situation that leads to unfeasible
solution). In theory, Q-learning has been proven to converge
to the optimal Q-function for an arbitrary target policy given
sufﬁcient training [32].

Deep Q-Learning applies Deep Neural Network (DNN) to
approximate the Q-function, i.e., a deep neural network that
takes a state and approximates the Q-value for each state-
action pair. It can handle the situation when the number of
states of a problem is huge. It is also faster to learn the reward
value of each state-action pair than Q-learning. Therefore, deep
Q-learning can handle more complicated problems compared
with Q-Learning.

framework in [34] and combine with deep Q-learning in [35]
to tackle the optimal k-coverage charging problem.

B. Graph Construction

We ﬁrst construct a directed graph denoted as G as input
for the Deep Q-Learning algorithm. Notice that the reason
that we reconstruct graph is that sensor nodes do not need
to be discretized by using DQN because agent will know the
exact time when it comes to the node and it could decide
to go ahead or abandon the path after checking the current
residual energy Bi(t) and the deadline constraint. This graph
construction can reduce the number of action of the DQN and
reduce the training and test time. The vertices and edges of G
are deﬁned as follows.

Vertices. The vertex set V (G) includes all the sensor nodes,
i.e., {vi|1 ≤ i ≤ n} and a start vertex denoted as v0. Each vertex
vi has a deadline Di. The location of v0 is the service station
and the deadline of v0 is inf. Note that the deadline of sensor
nodes without sending any request is set as 0.

Edges. For any sensor nodes vi and v j in V (G), di j is the
euclidean distance between vi and v j. There exists an edge
−→viv j in V (G) if and only if the inequality below holds

B − Bi(t0)
rc

+

di j
s

≤ D j

(4)

where Bi(t0) denotes as the residual energy of sensor node i
at charger departure time t0, rc is the energy transfer rate of
charger and s is the average speed of a charger. In this way,
we can make sure that all the feasiable path exist in the graph,
but if the path valid or not need to be checked according the
time t when agent comes to the node. Because compared with
Def. 3, this edge construction do not consider the tP(vi) and
only consider the initial energy Bi(t0) instead of Bi(tP(vi))
which related with the path P. If a charging path P goes from
node vi to node v j, the charging time beginning at node v j is
given by Def. 3.

Deﬁnition 7 (Feasible Path). A path P in G is a feasible one
if it starts from and ends at v0, and has no repeated vertex
and the charging time at each vertex is not inf. At the same
time, charging along P satisﬁes the k-coverage requirement of
the given network.

C. Deep Q-Learning Formulation

We deﬁne the states, actions, rewards, and stop function in

the deep Q-learning framework as follows:

States: A state S is a partial solution S ⊆ V (G), an ordered

list of visited vertices. The ﬁrst vertex in S is v0.

Actions: Let ¯S contain vertices not in S and has at least
one edge from vertices in S. An action is a vertex v from ¯S
returning the maximum reward. After taking the action v, the
partial solution S is updated as

S′ := (S, v), where v = arg max
v∈ ¯S

Q(S, v)

(5)

An end-to-end deep Q-learning framework introduced
in [33] automatically learns greedy heuristics for hard com-
binatorial optimization problems on graphs. We modify the

(S, v) denotes appending v to the best position after v0 in S
that introduces the least traveling distance and maintains all
vertices in the new list a valid charging time.

Rewards: The reward function R(S, v) is deﬁned as the
change of the traveling distance when taking the action v and
transitioning from the state S to a new one S′. Assume vi, v j
are two adjacent vertex in S, v0 is the ﬁrst vertex in the S, and
vt is the last vertex in the S.

The reward function R(S, vk) is deﬁned as follows:

(

R(S, vk) =

− min(dik + dk j − di j, dtk + dk0 − dt0)
−inf

tS′(v) 6= inf
otherwise
(6)
where di j is the euclidean distance between nodes vi and v j.
tS′(v) stands for the new charging time of each node in path
formed by S′ after inserting the vk. For example, tS′ (vk), tS′(vt )
are the charging time of vk, vt by the Def. 3 along the charging
path formed by S′ after inserting the vk to S, respectively.

Stop function: : The algorithm terminates when the current
solution satisﬁes the k-coverage requirement or R(S,v) is -inf.

D. Deep Q-Learning Algorithm

Algorithm 1 summarizes the major steps of our algorithm.
Brieﬂy, Q(S, v; Θ) is parameterized by a deep network with a
parameter Θ and learned by one-step Q-learning. At each step
of an episode, Θ is updated by a stochastic gradient descent
to minimize the squared loss:

(yl − Q(Sl, vl; Θ))2

where

R(Sl, vl) + γ maxv′ Q(Sl+1, v′; Θ)

if Sl+1 non-terminal and v′ ∈ ¯Sl

(7)

yl = 

R(Sl, vl)


otherwise

We use experience replay method in [35] to update Θ, where
the agent’s experience in each step is stored into a dataset.
When we update Θ, we sample random batch from dataset
that is populated from previous episodes. The beneﬁts include
increasing data efﬁciency, reducing correlations between sam-
ples, and avoiding oscillations with the parameters.

Algorithm 1 Deep Q-learning Algorithm
1: Initialize replay memory H to capacity C
2: for each episode do
3:

Initialize state S1 = (v0)
for step m = 1 to M do

Select vm = argmaxv∈ ¯Sm Q(Sm, v; Θ) with probability
1 − ε
Otherwise select a random vertex vm ∈ ¯Sm
Add vm to partial solution: Sm+1 := (Sm, vm)
Calculate reward R(Sm, vm) by (6)
Store tuple (Sm, vm, R(Sm, vm), Sm+1) to H
Sample random batch (Sl, vl, R(Sl, vl), Sl+1) from H

Update the network parameter Θ by (7)
if Sm+1 satisfy the stop function then

Break

4:
5:

6:
7:

8:

9:
10:

11:

12:

13:
14:

end if
end for

15:
16: end for

residual battery of a sensor is a uniform random variable
Bi between (0.54, 10.8]KJ [1]. The energy exhausted time
Di = Bi/βi. We choose the energy consumption rate βi from
the historical record of real sensors in [36] where the rate is
varying according to the remaining energy and arrival charging
time to the sensor. The energy transfer rate rc is 20W [8]. The
discredited time stepsize is 1s.

We add three heuristic algorithms including Ant Colony
System (ACS) based algorithm, Random algorithm, and
Greedy algorithm for comparison. Sec. VII-B explains the
implementations of the three algorithms in detail.

Traveling energy and computing time are important metrics
to evaluate the performances of different algorithms. Network
settings may also affect the performance. Therefore, we study
the impact of the three parameters of network setting: the
coverage requirement k, the size of sensor network n, and
the remaining energy threshold α in Secs. VII-C, VII-D,
and VII-E, respectively.

VII. PERFORMANCE EVALUATION

B. Comparison Algorithms

A. Simulation Settings

We set up an Euclidean square [500, 500]m2 as a simulation
area and randomly deploy sensor nodes ranging from 32 to 80
in the square such that the area is at least k-coverage initially
where k varies from 2 to 4. The sensing range r is 135m.
The base station and service station of charger are co-located
in the center of the square. A charger with a starting point
from the service station has an average traveling speed 5m/s
and consumes energy 600J/m [4]. The battery capacity B of
each sensor is 10.8KJ [4]. The remaining energy threshold
α vary from 0.2 to 0.8. A sensor sends a charging request
before the leaving of the charger from the service station.
The sensor will include in the request the estimated energy
exhausted time based on its current residual energy and energy
consumption rate. To simulate such request, we consider the

We implement three heuristic algorithms for comparison:
Ant Colony System (ACS) based algorithm, Random algo-
rithm, and Greedy algorithm.

ACS algorithm solves the traveling salesmen problem with
an approach similar to the foraging behavior of real ants [37]–
[39]. Ants seek path from their nest to food sources and leave
a chemical substance called pheromone along the paths they
traverse. Later ants sense the pheromone left by earlier ones
and tend to follow a trail with a stronger pheromone. Over a
period of time, the shorter paths between the nest and food
sources are likely to be traveled more often than the longer
ones. Therefore, shorter paths accumulate more pheromone,
reinforcing these paths.

Similarly, ACS algorithm places agents at some vertices of
a graph. Each agent performs a series of random moves from

current vertex to a neighboring one based on the transition
probability of the connecting edge. After an agent has ﬁn-
ished its tour, the length of tour is calculated and the local
pheromone amounts of edges along the tour are updated based
on the quality of the tour. After all agents have ﬁnished their
tours, the shortest one is chosen and the global pheromone
amounts of edges along the tour are updated. The procedure
continues until certain criteria are satisﬁed. When applying
ACS algorithm to solve the the traveling salesmen problem
with deadline, two local heuristic functions are introduced
in [34] to exclude paths that violate the deadline constraints.
We modify ACS algorithm introduced in [34] for the opti-
mal k-coverage charging problem. Agents start from and end at
v0. Denote τi j(t) the amount of global pheromone deposited on
edge −→viv j and ∆τi j(t) the increased amount at the tth iteration.
∆τi j(t) is deﬁned as

∆τi j(t) =

1
L∗
0

(

if −→viv j ∈ P∗
otherwise

(8)

where L∗ is the traveling distance of the shortest feasible tour
P∗ at the tth iteration. Global pheromone τi j(t) is updated
according to the following equation:

τi j(t) = (1 − θ)τi j(t − 1) + θ∆τi j(t),

(9)

where θ is the global pheromone decay parameter. The local
pheromone is updated in a similar way, where θ is replaced
by a local pheromone decay parameter and ∆τi j(t) is set as
the initial pheromone value.

We also modify the stop criteria of one agent such that the
traveling path satisﬁes the requirement of k-coverage, or the
traveling time of current path is inf, or the agent is stuck at a
vertex based on the transition rule.

Random and Greedy algorithms work much more straight-
forward. The Random algorithm randomly chooses a next node
not in the same clique of the existing path and with an outgoing
edge from current one to charge. The Greedy algorithm always
chooses the nearest node not in the same clique of the existing
path with an outgoing edge from current one. Random and
Greedy algorithms terminate when they either ﬁnd a feasible
path or are locally stuck. Note that for ACS and Random
algorithms, we always run multiple times and choose the best
solution.

C. Coverage Requirement

We set the number of sensor nodes n = 64 and the remaining
energy threshold α = 0.45. The coverage requirement k varies
from 2 to 4. Table I gives the performances of different
algorithms. The higher the coverage requirement k is, the more
the sensor nodes need to be charged. The trend is obvious in
Table I that the traveling energy increases with the increase of
the coverage requirement k. The Random algorithm can only
ﬁnd a feasible path when k is small. The Greedy algorithm
can ﬁnd a feasible solution for different k, but with traveling
energy much higher than others. The ACS algorithm can ﬁnd a
feasible path for different k too, with traveling energy less than

TABLE I
PERFORMANCE COMPARISON UNDER DIFFERENT COVERAGE
REQUIREMENT k

Algorithm k

Dynamic
DQN
ACS
Random
Greedy
Dynamic
DQN
ACS
Random
Greedy
Dynamic
DQN
ACS
Random
Greedy

2

3

4

n = 64, α = 0.45

Computation
Time
(s)
0.102
16
5
0.0006
0.0007
455
20
19
0.0003
0.0003
–
71
73
0.0006
0.0005

Feasible
Path
Found
Yes
Yes
Yes
Yes
Yes
Yes
Yes
Yes
No
Yes
–
Yes
Yes
No
Yes

Traveling
Energy
(kJ)
249
249
249
249
288
702
702
702
–
846
–
1089
1188
–
1254

the Random and Greedy ones. The DQN algorithm can ﬁnd the
optimal path for all ks. At the same time, the computing time
of DQN including both the training and computing time, grows
slowly with the increase of k. By contrast, the computing time
of the dynamic programming algorithm increases exponen-
tially. The dynamic programming algorithm runs out of the
memory when k is large. Overall, the performance of DQN
signiﬁcantly outperforms all other comparison algorithms. It is
worth mentioning that the Random algorithm performs better
than the Greedy one in some case because the nearest node
may not be a good choice and a randomly chosen one may
lead the searching of a feasible path out of stuck.

D. Size of Sensor Network

We set the coverage requirement k = 3 and the remaining
energy threshold α = 0.45. The size of a sensor network n
varies from 48 to 80. Table II gives the performances of
different algorithms. It is obvious that the traveling energy
in Table II decreases with the increase of n because there are
more redundant sensor nodes to maintain the k-coverage of
a network. The Random algorithm fails to detect a feasible
path and the result of the Greedy algorithm is far from the
optimal one. The ACS algorithm performs better when n is
large and the network has more redundant sensor nodes to
provide coverage. Again, the DQN algorithm still performs
the best, ﬁnding the optimal paths for all ns with a stable
computing time.

E. Remaining Energy Threshold

Tables III and IV give the performances of different algo-
rithms with the remaining energy threshold α varying from 0.2

TABLE II
PERFORMANCE COMPARISON UNDER DIFFERENT SIZES OF SENSOR
NETWORK n

TABLE III
PERFORMANCE COMPARISON UNDER DIFFERENT REMAINING ENERGY
THRESHOLD α WHEN k = 2, n = 32

Algorithm

n

Dynamic
DQN
ACS
Random
Greedy
Dynamic
DQN
ACS
Random
Greedy
Dynamic
DQN
ACS
Random
Greedy
Dynamic
DQN
ACS
Random
Greedy

48

64

72

80

Computation
Time
(s)
156700
83
92
0.0004
0.0004
455
20
19
0.0003
0.0003
362
32
57
0.0003
0.0003
268
20
18
0.0003
0.0003

k = 3, α = 0.45
Feasible
Path
Found
Yes
Yes
Yes
Yes
Yes
Yes
Yes
Yes
No
Yes
Yes
Yes
Yes
Yes
Yes
Yes
Yes
Yes
No
Yes

Traveling
Energy
(kJ)
771
771
951
2085
888
702
702
702
–
846
567
567
567
1941
810
345
345
345
–
375

to 0.8 under two network settings: n = 32 and k = 2, and n = 48
and k = 3, respectively. With the increased remaining energy
threshold,
the traveling energy increases in both network
settings. The Random and Greedy algorithms fail to detect
a feasible path in many cases. The dynamic programming
algorithm runs out of the memory when α is large. Both the
ACS and DQN algorithms ﬁnd feasible paths for all cases.
However, the performance of the ACS algorithm decreases
with the increase of α. The DQN algorithm consistently
and signiﬁcantly outperforms all other comparison algorithms
including botht the traveling energy and computing time.

VIII. CONCLUSIONS

We explore the mobile charger scheduling and path op-
timization problem that the k-coverage ability of a wireless
rechargeable sensor network system needs to be maintained.
We formulate the problem and show its hardness. We model
the problem and apply both dynamic programming and rein-
forcement learning techniques to design algorithms to tackle
the optimal k-coverage charging problem. We also implement
three other heuristic algorithms for comparison. Extensive sim-
ulations with experimental data included demonstrate that the
proposed Deep Q-Learning algorithm consistently produces
optimal solutions or closed ones and signiﬁcantly outperforms
other methods.

Algorithm

α

Dynamic
DQN
ACS
Random
Greedy
Dynamic
DQN
ACS
Random
Greedy
Dynamic
DQN
ACS
Random
Greedy
Dynamic
DQN
ACS
Random
Greedy

0.2

0.4

0.6

0.8

Computation
Time
(s)
0.0012
13
3
0.0002
0.0003
9760
26
38
0.0003
0.0003
135742
116
232
0.0006
0.0004
–
136
400
0.002
0.001

k = 2, n = 32
Feasible
Path
Found
Yes
Yes
Yes
No
Yes
Yes
Yes
Yes
No
No
Yes
Yes
Yes
No
No
–
Yes
Yes
No
No

REFERENCES

Traveling
Energy
(kJ)
405
405
405
–
420
696
696
855
–
891
1071
1071
2289
–
–
–
1080
2544
–
–

[1] Y. Shi, L. Xie, Y. T. Hou, and H. D. Sherali, “On renewable sensor net-
works with wireless energy transfer,” in INFOCOM, 2011 Proceedings
IEEE, pp. 1350–1358, IEEE, 2011.

[2] Y. Yang and C. Wang, Wireless Rechargeable Sensor Networks. Springer,

2015.

[3] F. Y. C. Wang, J. Li and Y. Yang, “A mobile data gathering frame-
work for wireless rechargeable sensor networks with vehicle movement
costs and capacity constraints,” IEEE Trans. Comput., vol. 65, no. 8,
p. 2411–2427, 2016.

[4] W. Liang, Z. Xu, W. Xu, J. Shi, G. Mao, and S. K. Das, “Approximation
algorithms for charging reward maximization in rechargeable sensor
networks via a mobile charger,” IEEE/ACM Transactions on Networking,
vol. 25, no. 5, pp. 3161–3174, 2017.

[5] W. Liang, W. Xu, X. Ren, X. Jia, and X. Lin, “Maintaining large-scale
rechargeable sensor networks perpetually via multiple mobile charging
vehicles,” ACM Trans. Sen. Netw., vol. 12, no. 2, pp. 14:1–14:26, 2016.
[6] C. Lin, Z. Wang, J. Deng, L. Wang, J. Ren, and G. Wu, “mTS: Temporal-
and spatial-collaborative charging for wireless rechargeable sensor net-
works with multiple vehicles,” in INFOCOM, 2018 Proceedings IEEE,
2018.

[7] L. He, P. Cheng, Y. Gu, J. Pan, T. Zhu, and C. Liu, “Mobile-to-mobile
energy replenishment in mission-critical robotic sensor networks,” in
INFOCOM, 2014 Proceedings IEEE, pp. 1195–1203, IEEE, 2014.
[8] L. Chen, S. Lin, and H. Huang, “Charge me if you can: Charging path
optimization and scheduling in mobile networks,” in Proceedings of the
17th ACM International Symposium on Mobile Ad Hoc Networking and
Computing, pp. 101–110, ACM, 2016.

[9] S. Yang, F. Dai, M. Cardei, J. Wu, and F. Patterson, “On connected
multiple point coverage in wireless sensor networks,” Journal of Wireless
Information Networks, vol. 2006, 2006.

[10] G. Simon, M. Moln´ar, L. G¨onczy, and B. Cousin, “Dependable k-
coverage algorithms for sensor networks,” in Proc. of IMTC, 2007.

symposium on Mobile ad hoc networking and computing, pp. 109–118,
ACM, 2013.

[21] H. Dai, H. Ma, A. X. Liu, and G. Chen, “Radiation constrained
scheduling of wireless charging tasks,” IEEE/ACM Transactions on
Networking (TON), vol. 26, no. 1, pp. 314–327, 2018.

[22] S. Zhang, J. Wu, and S. Lu, “Collaborative mobile charging,” IEEE

Transactions on Computers, vol. 64, no. 3, p. 654–667, 2015.

[23] T. H. Cormen, Introduction to algorithms. MIT press, 2009.
[24] N. Bansal, A. Blum, S. Chawla, and A. Meyerson, “Approximation
algorithms for deadline-tsp and vehicle routing with time-windows,” in
Proceedings of the thirty-sixth annual ACM symposium on Theory of
computing, pp. 166–174, ACM, 2004.

[25] C. Chekuri and M. Pal, “A recursive greedy algorithm for walks in
directed graphs,” in Foundations of Computer Science, 2005. FOCS
2005. 46th Annual IEEE Symposium on, pp. 245–253, IEEE, 2005.
[26] M. W. Savelsbergh, “Local search in routing problems with time
windows,” Annals of Operations research, vol. 4, no. 1, pp. 285–305,
1985.

[27] A. Langevin, M. Desrochers, J. Desrosiers, S. G´elinas, and F. Soumis,
“A two-commodity ﬂow formulation for the traveling salesman and
the makespan problems with time windows,” Networks, vol. 23, no. 7,
pp. 631–640, 1993.

[28] Y. Dumas, J. Desrosiers, E. Gelinas, and M. M. Solomon, “An optimal
algorithm for the traveling salesman problem with time windows,”
Operations research, vol. 43, no. 2, pp. 367–371, 1995.

[29] A. Kurs, Power transfer through strongly coupled resonances. PhD

thesis, Massachusetts Institute of Technology, 2007.

[30] N. Alon, R. Yuster, and U. Zwick, “Color-coding,” Journal of the ACM

(JACM), vol. 42, no. 4, pp. 844–856, 1995.

[31] R. S. Sutton, A. G. Barto, et al., Reinforcement learning: An introduc-

tion. MIT press, 1998.

[32] F. S. Melo, “Convergence of q-learning: a simple proof.”
[33] E. Khalil, H. Dai, Y. Zhang, B. Dilkina, and L. Song, “Learning
combinatorial optimization algorithms over graphs,” in Advances in
Neural Information Processing Systems, pp. 6351–6361, 2017.

[34] C.-B. Cheng and C.-P. Mao, “A modiﬁed ant colony system for solving
the travelling salesman problem with time windows,” Mathematical and
Computer Modelling, vol. 46, no. 9-10, pp. 1225–1235, 2007.

[35] V. Mnih, K. Kavukcuoglu, D. Silver, A. Graves, I. Antonoglou, D. Wier-
stra, and M. Riedmiller, “Playing atari with deep reinforcement learn-
ing,” arXiv preprint arXiv:1312.5602, 2013.

[36] T. Zhu, Z. Zhong, Y. Gu, T. He, and Z.-L. Zhang, “Leakage-aware
energy synchronization for wireless sensor networks,” in Proceedings of
the 7th International Conference on Mobile Systems, Applications, and
Services, pp. 319–332, 2009.

[37] A. Colorni, M. Dorigo, and V. Maniezzo, “Distributed optimization by
ant colonies,” in Proceedings of ECAL91 - European Conference on
Artiﬁcial Life, p. 134–142, 1991.

[38] M. Dorigo and L. M. Gambardella, “Ant colony system: a cooperative
learning approach to the traveling salesman problem,” IEEE Transac-
tions on evolutionary computation, vol. 1, no. 1, pp. 53–66, 1997.
[39] W. J. Gutjahr, “A graph-based ant system and its convergence,” Future

Gener. Comput. Syst., vol. 16, no. 9, pp. 873–888, 2000.

TABLE IV
PERFORMANCE COMPARISON UNDER DIFFERENT REMAINING ENERGY
THRESHOLD α WHEN k = 3, n = 48

Algorithm

α

Dynamic
DQN
ACS
Random
Greedy
Dynamic
DQN
ACS
Random
Greedy
Dynamic
DQN
ACS
Random
Greedy
Dynamic
DQN
ACS
Random
Greedy

0.2

0.4

0.6

0.8

Computation
Time
(s)
0.02
10
1
0.0003
0.0002
145602
81
90
0.0004
0.0004
–
138
466
0.001
0.001
–
481
4200
0.01
0.02

k = 3, n = 48
Feasible
Path
Found
Yes
Yes
Yes
No
Yes
Yes
Yes
Yes
Yes
Yes
–
Yes
Yes
No
No
–
Yes
Yes
No
No

Traveling
Energy
(kJ)
348
348
348
–
348
750
750
930
2070
870
–
1020
1521
–
–
–
1272
4788
–
–

[11] M. Liggins, D. Hall, and J. Llinas, Handbook of Multisensor Data
Fusion: Theory and Practice, Second Edition. CRC Press, 2008.
[12] S. D. Z. Zhou and H. Gupta, “Variable radii connected sensor cover in
sensor networks,” ACM Trans. Senor Networks, vol. 5, no. 1, pp. 8:1–
8:36, 2009.

[13] X. Bai, Z. Yun, D. Xuan, B. Chen, and W. Zhao, “Optimal multiple-
coverage of sensor networks,” in INFOCOM, 2011 Proceedings IEEE,
p. 2498–2506, IEEE, 2011.

[14] F. Li, J. Luo, W. Wang, and Y. He, “Autonomous deployment for load
balancing k-surface coverage in sensor networks,” IEEE Transactions
on Wireless Communications, vol. 14, no. 1, pp. 279–293, 2015.
[15] S. Kumar, T. H. Lai, and J. Balogh, “On k-coverage in a mostly sleeping
sensor network,” in Proceedings of the 10th Annual International Con-
ference on Mobile Computing and Networking, MobiCom ’04, pp. 144–
158, 2004.

[16] M. Hefeeda and M. Bagheri, “Randomized k-coverage algorithms
for dense sensor networks,” in INFOCOM, 2007 Proceedings IEEE,
pp. 2376–2380, 2007.

[17] H. Ammari and S. Das, “Centralized and clustered k-coverage protocols
for wireless sensor networks,” IEEE Trans. on Computers, vol. 6, no. 1,
p. 118–133, 2012.

[18] L. Xie, Y. Shi, Y. T. Hou, W. Lou, H. D. Sherali, and S. F. Midkiff, “On
renewable sensor networks with wireless energy transfer: The multi-
node case,” in Sensor, mesh and ad hoc communications and networks
(SECON), 2012 9th annual IEEE communications society conference
on, pp. 10–18, IEEE, 2012.

[19] L. Xie, Y. Shi, Y. T. Hou, W. Lou, H. D. Sherali, and S. F. Midkiff,
“Bundling mobile base station and wireless energy transfer: Modeling
and optimization,” in INFOCOM, 2013 Proceedings IEEE, pp. 1636–
1644, IEEE, 2013.

[20] L. Xie, Y. Shi, Y. T. Hou, W. Lou, and H. D. Sherali, “On traveling
path and related problems for a mobile station in a rechargeable
sensor network,” in Proceedings of the fourteenth ACM international

