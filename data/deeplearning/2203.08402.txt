2
2
0
2

r
a

M
6
1

]
L
P
.
s
c
[

1
v
2
0
4
8
0
.
3
0
2
2
:
v
i
X
r
a

Gradual Tensor Shape Checking

MOMOKO HATTORI, The University of Tokyo, Japan
NAOKI KOBAYASHI, The University of Tokyo, Japan
RYOSUKE SATO, The University of Tokyo, Japan

Tensor shape mismatch is a common source of bugs in deep learning programs. Most of the studies conducted
to solve this problem use the whole-program analysis approach, which lacks compositionality. A type-based
approach is desirable in this respect, but since the problem of shape inference is undecidable in general, fully
automated shape inference is bound to be either unsound or too conservative.

We propose a type-based approach to detect tensor shape mismatches that works practically under such
limitations. One of the main features of our approach is the best-eﬀort shape inference. Because of the un-
decidability of shape inference, our procedure performs it only in a best-eﬀort manner. The inference result
may be too imprecise to statically guarantee the absence of the shape inconsistencies, and in such cases, dy-
namic checks are inserted into the program. Another main feature is gradual typing. Users can improve the
precision of the inference by adding appropriate type annotations to the program.

We formalize our approach and prove that it satisﬁes the criteria of gradual typing proposed by Siek et al.
in 2015. We implement a prototype shape checking tool based on our approach and evaluate its eﬀectiveness
by applying it to some deep neural network programs.

CCS Concepts: • Software and its engineering → Formal software veriﬁcation.

Additional Key Words and Phrases: reﬁnement types, gradual typing

1 INTRODUCTION

1.1 Tensor Shape Checking and Its Diﬀiculty
Tensor shape mismatch is one of the common sources of dynamic errors in programs that use
tensors (i.e., multi-dimensional arrays). For example, consider the reshape function of tensors. The
reshape operation takes a tensor 𝑥 and an integer list 𝑆 and returns a new tensor of the shape 𝑆
obtained by realigning the elements in 𝑥. The input tensor 𝑥 and the return value of the reshape
operation must have the same number of elements. Therefore, one can successfully reshape a
tensor of shape [2; 3; 4]1 into a shape [3; 2; 4], while trying to reshape it into [3; 4] results in a
runtime error.

One of the domains that frequently uses tensors is deep learning. Since deep learning programs
often take a considerable amount of time to train networks, it is often the case that a program
takes hours and days to compute the weights of deep neural networks only to be terminated by
one tensor shape mismatch error, throwing away the trained weights. What is even worse is that
some tensor shape mismatches are harder to notice. For example, mixing up the height and the
width of square images does not result in runtime errors but only degrades the performance of the
neural network.

Some methods have been proposed to detect such errors before the execution, and they can be
classiﬁed into two categories. One is the whole-program analysis approach [15, 26] which keeps
track of the tensor shape by partially evaluating the program in the style of abstract interpretation.
The other is the type-based approach [1, 22] which expresses the shapes of the tensors as a part of
the type information. Still, none of them is fully satisfactory: either they are too conservative and
reject valid programs or fail to detect shape mismatch errors.

1In this paper, we denote lists in the OCaml-style as in [1; 2; 3] to disambiguate it from the citations.

Authors’ addresses: Momoko Hattori, hattori@kb.is.s.u-tokyo.ac.jp, The University of Tokyo, Tokyo, Japan; Naoki
Kobayashi, , The University of Tokyo, Tokyo, Japan; Ryosuke Sato, , The University of Tokyo, Tokyo, Japan.

 
 
 
 
 
 
2

Hattori et al.

1 let model s =

2

3

let f = ... in let g = ... in
fun x -> let x1 = f x in g x1

4
5 let _ = model 1 (Tensor.rand [20])

Fig. 1. An OCaml program written with OCaml-Torch.

This paper pursuits a type-based approach as it is expected to provide modular detection of
tensor shape inconsistencies. One of the main diﬃculties in this approach is that shapes are ﬁrst-
class objects. For example, the library function Tensor.zeros of OCaml-Torch [2] (which provides
OCaml bindings for libtorch [19]) takes a list 𝑆 of integers, and returns a new tensor whose shape
is 𝑆. Thus, we have to work with dependent types: Tensor.zeros would be given the type

𝑆 : int list → {𝑟 : int | 𝑟 .shape = 𝑆 }.

It is diﬃcult to infer such dependent (reﬁnement) types fully automatically. Yet, we wish to avoid
programmers’ burden of writing too many type annotations.

Another diﬃculty is that shape constraints can be so complex that even type checking, let alone
inference, can be too costly or impossible. For instance, the reshape operation that we explained
earlier needs the proof that the shape of the input tensor 𝑥 is compatible with the given shape
𝑆 = [𝑠1; . . . ; 𝑠𝑛] (i.e., if the shape of 𝑥 is to be [𝑠 ′
𝑖=1𝑠𝑖 holds)2. Thus, type
checking requires complex reasoning about integer arithmetic and lists.

𝑚], then Π𝑚

1; . . . ; 𝑠 ′

𝑖=1𝑠 ′
𝑖

= Π𝑛

1.2 Overview of Our Approach

Based on the observations above, we propose an approach that works practically under such dif-
ﬁculties. Our approach can be characterized by three main features: best-eﬀort type inference,
hybrid type checking, and gradual typing [24]. We explain those features using our prototype tool
GraTen.

1.2.1 Best-Eﬀort Type Inference. One of the main features of our approach is that it provides best-
eﬀort type and shape inference. Although it is performed in a best-eﬀort manner, it enables the
type checker to work on programs with no type annotations and still detect some simple shape
mismatches.

As an example, we use the program presented in Figure 1 and illustrate how GraTen analyzes
it. The function model takes an integer parameter s, deﬁnes functions f and g, and returns a layer
(function that takes tensor and returns a tensor) which composes f and g. The deﬁnitions of f and
g are omitted here, but let us assume that their respective types are as below, where s in the type
of f is the argument of model.

f : 𝑥:{𝜈 : tensor | len(𝜈 .shape) = 1} → tensor

g : tensor( [10]) → tensor( [1])

nth(0, 𝑥 .shape)
s

(cid:18) (cid:20)

(cid:21) (cid:19)

These types indicate that 𝑓 takes a 1-dimensional tensor (i.e., a vector) and returns a vector whose
length is one-sth of the length of the argument, and 𝑔 expects a vector of length 10 and returns a
vector of length 1. We formally introduce the syntax of the types later in Section 2.

For this program, GraTen infers the following type for the function model with its best-eﬀort
= 10 for the shape of 𝑥 is necessary for this program not to

inference. The constraint nth(0,𝜈.shape)

𝑠

2Actually, even worse, some 𝑠𝑖 can be −1, in which case, the size of the 𝑖-th dimension is unspeciﬁed.

Gradual Tensor Shape Checking

1 let model s =

2

3

let f = ... in let g = ... in
fun x -> let x1 = if s = 1 then x else f x in g x1

3

Fig. 2. The program from Figure 1 with small modification.

1 let model s =

2

3

4

let f = ... in let g = ... in
fun x -> let x1 = if s = 1 then x else f x in

g (assert (x1.shape = [10]); x1)

Fig. 3. A program returned from GraTen after running it against the program in Figure 2.

1 let model s =

2

3

4

let f = ... in let g = ... in
fun x ->

let x1 = (if s = 1 then x else f x : tensor([nth 0 x.shape / s])) in g x1

Fig. 4. The program from Figure 2 after adding type annotations.

raise shape mismatch errors at the application of g.

𝑠:int → 𝑥:

𝜈:tensor | len(𝜈 .shape) = 1 ∧
(cid:26)

nth(0, 𝜈 .shape)
s

= 10

→ tensor( [1])

(cid:27)

The inferred type of model is used to prevent any calls to model that can crash anywhere in the
body of model. Indeed, GraTen rejects the calls in line 5 of Figure 1 where the arguments do
not satisfy the constraint nth(0,𝜈.shape)
= 10. As such, our approach can statically detect shape
mismatches if the type information of the program (obtained from the type annotations or the
best-eﬀort type inference) is precise enough.

s

1.2.2 Hybrid Type Checking. The second feature of our approach is that the type checking is hy-
brid: we combine static and dynamic checking. The type checker inserts assertions to the program
points where the type safety is not statically guaranteed, à la Knowles and Flanagan’s hybrid type
checking [14].

For example, consider the program in Figure 2, which is obtained by adding a conditional branch
to the one in Figure 1. The type of the then and else branch of the if expression is inferred to be
tensor(x.shape) and tensor( [ nth(0,x.shape)
]), respectively. In this case, the type of x1 is simply
inferred to be tensor without any information about its shape, and the inferred type for model is
as follows. The best-eﬀort inference of GraTen fails to capture the constraint nth(0,𝜈.shape)
= 10
for 𝑥 due to the imprecise type information of x1.

𝑠

𝑠

𝑠:int → 𝑥:{𝜈 : tensor | len(𝜈 .shape) = 1} → tensor( [1])

Along with the inferred types, GraTen outputs the program in Figure 3, which is the same as the
original program except for an assertion inserted at the argument of g. Since the statically inferred
type of x1 does not guarantee that the application of g to x1 does not fail, GraTen inserts an
assertion into the program to check the requirement dynamically.

4

Hattori et al.

1.2.3 Gradual Typing. The third feature of our approach is that it incorporates gradual typing [24]
so that the users can improve the precision of the best-eﬀort type inference by adding type anno-
tations.

For example, Figure 4 shows the program from Figure 2 with a type annotation to x1. With
this annotation, GraTen infers the same type for model as it did to the model in Figure 1, and
no assertions are inserted to the program produced by GraTen. As such, adding correct type
annotations improves the type checking and decreases the number of assertions inserted.

Thanks to the best-eﬀort inference, users should not have to add type annotations to everywhere
in the program but can focus on the program points where the static inference did not perform
well, which is usually indicated by the insertion of assertions.

We have proved that our type system satisﬁes the gradual guarantee [24], which ensures that
adding type annotation preserves the type-ability and the behavior of the program (with some
assertions inserted) regardless of its precision, as long as the annotation does not disagree with
the program.

Among the three features, the notion of hybrid type checking was ﬁrst proposed by Knowles
and Flanagan [14], and our gradual typing is closely related to gradual reﬁnement types by [16],
but we believe that the particular combination of three features is new. In particular, unlike the
original gradual reﬁnement types [16], we insert assertions instead of carrying around evidence
terms [11] in the reduction to guarantee type safety.

The contribution and the structure of this paper is as follows.

• The formalization of a type system consisting of the three features above. We deﬁne our type
system as the type-based transformation relation from source programs to programs with
runtime assertion checks. We prove the soundness of our type system as well (Section 2).
The shape-polymorphic extension of the type system is also brieﬂy discussed (Section 3).

• A proof that our system satisﬁes the gradual guarantee [24] (Section 4).
• Implementation of a prototype system GraTen based on the formalization (Section 5).
• Experimental evaluation of GraTen using the examples of deep learning programs bundled
in the OCaml-Torch library. We conﬁrm that GraTen can statically type-check the programs
eﬀectively with a reasonable amount of type annotations (Section 6).

2 A GRADUALLY-TYPED LANGUAGE WITH TENSORS

In this section, we formalize our type system and the translation to insert assertions. We ﬁrst
introduce the source and the target language of the translation in Subsections 2.1 and 2.2. We then
formalize the type system and the translation and prove their soundness in Subsection 2.3. The
gradual guarantee properties will be discussed later in Section 4.

2.1 Source Language
The syntax of the source language and types is presented in Figure 5. Throughout this paper, 𝑛, 𝑐, 𝑥
respectively denote integers, constants (including integers and primitive functions) and variables.
The language we consider is a basic ML-style functional language with integer lists, which is
used to express tensor shapes. Lists are restricted to those that only contain integer elements for
simplicity. The empty list [ ] and cons function are both included in the constant 𝑐.

Type annotations can be added to the function arguments 𝜆𝑥:𝜏 .𝑀, recursive functions fix(𝑓 :(𝑥:𝜏1 →

𝜏2), 𝑥, 𝑀) and to arbitrary expressions by (𝑀 : 𝜏). In the actual implementation of GraTen, users
may omit the type annotations in lambda expressions and recursive functions as they are inferred
in the best-eﬀort type inference.

Gradual Tensor Shape Checking

5

𝑀 (term) ::= 𝑐 | 𝑥 | 𝜆𝑥:𝜏 .𝑀 | 𝑀 𝑥 | (𝑀 : 𝜏) | let 𝑥 = 𝑀1 in 𝑀2
fix(𝑓 :(𝑥:𝜏1 → 𝜏2), 𝑥, 𝑀) | if 𝑥 then 𝑀1 else 𝑀2

|

𝜏 (type) ::= {𝑥 : 𝐵 | 𝜑} | 𝑥:𝜏1 → 𝜏2

𝐵 (base type) ::= bool | int | int list | tensor
𝜑 (predicate) ::= true | false | 𝑠1 = 𝑠2 | 𝑆1 = 𝑆2 | 𝑥 | ¬𝜑 | 𝜑1 ∧ 𝜑2 | 𝜑1 ∨ 𝜑2

|

broadcastable(𝑆1, 𝑆2) | reshapeable(𝑆1, 𝑆2) | · · ·

𝑆 (shape) ::= [𝑠1; . . . ; 𝑠𝑛] | 𝑥 | 𝑥 .shape | append(𝑆1, 𝑆2) | · · ·

𝑠 (size) ::= 𝑛 | 𝑥 | len(𝑆) | nth(𝑠, 𝑆) | −𝑠 | 𝑠1 + 𝑠2 | 𝑠1 × 𝑠2 |

𝑠1
𝑠2

| · · ·

𝑡 ::= 𝜑 | 𝑆 | 𝑠

Γ (type env.) ::= ∅ | Γ, 𝑥 : 𝜏
Δ (base type env.) ::= ∅ | Δ, 𝑥 : 𝐵

Fig. 5. Syntax of the source language, the types and the type enviornments.

Δ ⊢wf 𝜏
Δ, 𝑥 : 𝐵 ⊢wf 𝜑 : bool
Δ ⊢wf {𝑥 : 𝐵 | 𝜑}

Δ, BT(𝑥 : 𝜏1) ⊢wf 𝜏2
Δ ⊢wf 𝑥:𝜏1 → 𝜏2

BT(Γ)

BT(∅) = ∅

BT(Γ, 𝑥 : {𝑥 : 𝐵 | 𝜑}) = BT(Γ), 𝑥 : 𝐵
BT(Γ, 𝑥 : (𝑦:𝜏1 → 𝜏2)) = BT(Γ)

Δ ⊢BT 𝑡 : 𝐵

Δ ⊢BT 𝑥 : Δ(𝑥)

Δ(𝑥) = tensor
Δ ⊢BT 𝑥 .shape : int list

Δ ⊢BT 𝑠 : int

Δ ⊢BT 𝑆 : int list

Δ ⊢BT nth(𝑠, 𝑆) : int

Fig. 6. Selected rules of well-formedness condition of types (full definition in the appendix Figure 18).

The argument of a function application and the branching condition of an if-expression are
restricted to variables for the sake of simplicity of typing rules. Note that this restriction does not
lose generality, as a general function application 𝑀1 𝑀2 can be normalized to let 𝑓 = 𝑀1 in let 𝑥 =
𝑀2 in 𝑓 𝑥.

Types are deﬁned following the standard deﬁnition of reﬁnement types. Intuitively, the type
{𝑥 : 𝐵 | 𝜑} describes a value 𝑥 of type 𝐵 such that 𝜑 holds. For example, {𝑥 : tensor | dim(𝑥) = 2}
is the type of 2-dimenstional tensors. We may omit the reﬁnement predicates when they are true.
For example, we may write {𝑥 : int | true} as int.

The reﬁnement predicates, shapes and sizes are expressions of type bool, int list and int re-
spectively. As shown in the deﬁnition, they may use some built-in functions such as broadcastable
and append in order to express common tensor operations.

Figure 6 deﬁnes the well-formedness of types. A base type environment Δ is a mapping from
variables to base types, as deﬁned in Figure 5. The function BT(·) converts a type environment to
a base type environment. The typing relation for predicates, shapes and sizes Δ ⊢BT 𝑡 : 𝐵 is a part
of the well-formedness of types Δ ⊢wf 𝜏.

6

Hattori et al.

𝑣 (value) ::= 𝑐 | 𝑥 | [𝑣1, . . . , 𝑣𝑛] | 𝜆𝑥𝜏 .𝑁 | fix(𝑓 𝜏, 𝑥, 𝑁 )

𝑁 (cast term) ::= 𝑣 | if 𝑣 then 𝑁1 else 𝑁2 | 𝑁 𝑣 | let 𝑥𝜏 = 𝑁1 in 𝑁2 | assert(𝜑); 𝑁

Fig. 7. Syntax of the target language.

[𝑣/𝑥]𝑁

[𝑣/𝑥] (assert(𝜑); 𝑁 ) = assert( [𝑣/𝑥]𝜑); [𝑣/𝑥]𝑁
[𝑣/𝑥] (𝜆𝑦𝜏 .𝑁 ) = 𝜆𝑦 [𝑣/𝑥 ]𝜏 .[𝑣/𝑥]𝑁
[𝑣/𝑥] (fix(𝑓 𝜏, 𝑦, 𝑁 )) = fix(𝑓 [𝑣/𝑥 ]𝜏, 𝑦, [𝑣/𝑥]𝑁 )

[𝑣/𝑥] (let 𝑦𝜏 = 𝑁1 in 𝑁2) = (let 𝑦 [𝑣/𝑥 ]𝜏 = [𝑣/𝑥]𝑁1 in [𝑣/𝑥]𝑁2)
(Variables are assumed to be alpha-renamed so that variables at
diﬀerent scopes do not collide)

𝑁1 −→ 𝑁2

assert(true); 𝑁 −→ 𝑁
assert(false); 𝑁 −→ error

(𝜆𝑥𝜏 .𝑁1) 𝑣 −→ [𝑣/𝑥]𝑁1
𝑐 𝑣 −→ ev(𝑐, 𝑣)
let 𝑥𝜏 = 𝑣 in 𝑁 −→ [𝑣/𝑥]𝑁

Fig. 8. Selected rules of substitution and reduction of the target language (full definition in the appendix
Figure 19 and Figure 20).

2.2 Target Language

As explained already (in Section 1), since our type checker inference/checking procedure is incom-
plete, run-time checks are inserted into places where type-safety cannot be statically guaranteed.
Figure 7 shows the syntax of programs obtained by the insertion of assertions. The syntax of the
target language is similar to that of the source language with some diﬀerences. One of them is the
addition of assertion assert(𝜑); 𝑁 , which is used to implement the dynamic check. Like Flanagan’s
hybrid type system [14] (and unlike the blame calculus [27]), we guarantee the safety of target
programs by assertions of the form assert(𝜑); 𝑁 , which checks that 𝜑 holds and then proceeds to
evaluate 𝑁 ; if 𝜑 does not hold, the program is aborted and a run-time type error is reported. This
method is expected to be easier to implement since most of the modern programming languages
are equipped with assertions, and more eﬃcient than the blame calculus in that it avoids the accu-
mulation of dynamic casts at runtime. This implementation of the dynamic cast is possible since
our system is only “gradualized” at the predicate level of the reﬁnement type and the underlying
simple type is static.

Another diﬀerence is that the binders in let expressions are annotated with their type. This is

required when deﬁning the precision relation over the cast terms in Section 4.

The substitution and the reduction rules of the cast terms are presented in Figure 8. The eval-
uation of primitive function ev(𝑐, 𝑣) is deﬁned to be the return value of the primitive function 𝑐
applied to an argument 𝑣 if 𝑣 meets the constraint of the argument of 𝑐, and otherwise undeﬁned.
We denote 𝑁 ⇑ if there exists an inﬁnite reduction sequence from 𝑁 .

The substitution for cast terms is deﬁned in the standard manner, except that the implicitly-
annotated type information and the predicate in the assertion need to be updated as well. As
can be seen in the deﬁnition of the cast term reduction, these implicitly-annotated types are only
required for the sake of formalization and ignored at runtime.

We also introduce the type derivation rules for the cast terms Γ; 𝜑 ⊢ 𝑁 : 𝜏 in Figure 9. This
relation is used in the discussion of the soundness of the type system later in Subsubsection 2.3.2.
The 4-ary relation Γ; 𝜑 ⊢ 𝑁 : 𝜏 denotes that a cast term 𝑁 has type 𝜏 under a type environment

Gradual Tensor Shape Checking

7

Γ; 𝜑 ⊢ 𝑁 : 𝜏

Γ; 𝜑 ⊢ 𝑐 : ty(𝑐) (CT-Const)

Γ(𝑥) = 𝑦:𝜏1 → 𝜏2
Γ; 𝜑 ⊢ 𝑥 : Γ(𝑥)

Γ(𝑥) = {𝑦 : 𝐵 | 𝜑 ′}
Γ; 𝜑 ⊢ 𝑥 : {𝑦 : 𝐵 | 𝑦 = 𝑥 }

Γ, 𝑥 : 𝜏1; 𝜑 ⊢ 𝑁 : 𝜏2
Γ; 𝜑 ⊢ 𝜆𝑥𝜏1 .𝑁 : 𝑥:𝜏1 → 𝜏2

(CT-Lam)

(CT-Var-Fun)
Γ; 𝜑 ⊢ 𝑁 : 𝑥:𝜏1 → 𝜏2

(CT-Var-Base)

Γ; 𝜑 ⊢ 𝑣 : 𝜏1

(CT-App)

Γ; 𝜑 ⊢ 𝑁 𝑣 : [𝑣/𝑥]𝜏2

Γ, 𝑓 : (𝑥:𝜏1 → 𝜏2), 𝑥 : 𝜏1; 𝜑 ⊢ 𝑁 : 𝜏2
Γ; 𝜑 ⊢ fix(𝑓 𝑥:𝜏1→𝜏2, 𝑥, 𝑁 ) : 𝑥:𝜏1 → 𝜏2

(CT-Fix)

Γ; 𝜑 ∧ 𝜑 ′ ⊢ 𝑁 : 𝜏
Γ; 𝜑 ⊢ assert(𝜑 ′); 𝑁 : 𝜏

(CT-Assert)

Γ; 𝜑 ⊢ 𝑣 : {𝑥 : bool | 𝜑 ′}

Γ; 𝜑 ∧ 𝑣 ⊢ 𝑁1 : 𝜏

Γ; 𝜑 ∧ ¬𝑣 ⊢ 𝑁2 : 𝜏

Γ; 𝜑 ⊢ if 𝑣 then 𝑁1 else 𝑁2 : 𝜏

(CT-If)

Γ; 𝜑 ⊢ 𝑁1 : 𝜏2

Γ, 𝑥 : 𝜏2; 𝜑 ⊢ 𝑁2 : 𝜏

Γ; 𝜑 ⊢ 𝑁 : 𝜏 ′

Γ; 𝜑 ⊢ 𝜏 ′ <: 𝜏

Γ; 𝜑 ⊢ let 𝑥𝜏1 = 𝑁1 in 𝑁2 : 𝜏

Γ; 𝜑 ⊢ 𝑁 : 𝜏

(CT-Sub)

(CT-Let)

Fig. 9. Typing rules for the cast terms.

Φ(Γ)

Γ; 𝜑 ⊢ 𝜏1 <: 𝜏2

Φ(∅) = true

Φ(Γ, 𝑥 : {𝑦 : 𝐵 | 𝜑}) = Φ(Γ) ∧ [𝑥/𝑦]𝜑
Φ(Γ, 𝑥 : (𝑦:𝜏1 → 𝜏2)) = Φ(Γ)

(cid:15) ∀BT(Γ), 𝑥:𝐵.Φ(Γ) ∧ 𝜑 ∧ 𝜑1 ⇒ 𝜑2
Γ; 𝜑 ⊢ {𝑥 : 𝐵 | 𝜑1} <: {𝑥 : 𝐵 | 𝜑2}

(Sub-Base)

Γ; 𝜑 ⊢ 𝜏3 <: 𝜏1

Γ, 𝑥 : 𝜏3; 𝜑 ⊢ 𝜏2 <: 𝜏4

Γ; 𝜑 ⊢ 𝑥:𝜏1 → 𝜏2 <: 𝑥:𝜏3 → 𝜏4

(Sub-Fun)

Fig. 10. Subtyping rules.

Γ and a logical context 𝜑. The logical context 𝜑, sharing its syntax with the reﬁnement predicate,
holds the information of logically valid predicates at respective program points. New predicates
are added at the then branch and the else branch of (CT-If), and the post-assertion cast term in
(CT-Assert). The subsumption is allowed in (CT-Sub) by the subtyping relation Γ; 𝜑 ⊢ 𝜏1 <: 𝜏2
(Figure 10), which is deﬁned in a standard manner.

2.3 Typing Rules

2.3.1 Assertion Insertion to Source Program. Next, we discuss the typing rules for the source lan-
guage and the assertion insertion to it. Figure 11 deﬁnes the type judgement and cast insertion
relation. The 5-ary relation Γ; 𝜑 ⊢ 𝑀 { 𝑁 : 𝜏 intuitively reads: under a type environment Γ and a
logical context 𝜑, a term 𝑀 translates to a cast term 𝑁 and has type 𝜏. If we ignore the part “{ 𝑁 ”
and replace the gradual subtyping relation . with the standard subtyping relation on reﬁnement
types (Figure 10), then our type system is a standard reﬁnement type system. Thus, the main nov-
elty in the rules in Figure 11 lies in the use of the consistent subtyping relation Γ; 𝜑 ⊢ 𝜏1 . 𝜏2 { 𝑁 ,
which is explained below.

8

Γ; 𝜑 ⊢ 𝑀 { 𝑁 : 𝜏

Γ; 𝜑 ⊢ 𝑐 { 𝑐 : ty(𝑐)

(CI-Const)

Γ(𝑥) = {𝑦 : 𝐵 | 𝜑 ′}
Γ; 𝜑 ⊢ 𝑥 { 𝑥 : {𝑦 : 𝐵 | 𝑦 = 𝑥 }

(CI-Var-Base)

Hattori et al.

Γ(𝑥) = 𝑦:𝜏1 → 𝜏2
Γ; 𝜑 ⊢ 𝑥 { 𝑥 : Γ(𝑥)
Γ, 𝑥 : 𝜏1; 𝜑 ⊢ 𝑀 { 𝑁 : 𝜏2
Γ; 𝜑 ⊢ 𝜆𝑥:𝜏1.𝑀 { 𝜆𝑥𝜏1 .𝑁 : 𝑥:𝜏1 → 𝜏2

(CI-Var-Fun)

(CI-Lam)

Γ; 𝜑 ⊢ 𝑀1 { 𝑁1 : 𝑦:𝜏1 → 𝜏2

Γ(𝑥) = 𝜏3

Γ; 𝜑 ⊢ 𝜏3 . 𝜏1 { 𝑁2

Γ; 𝜑 ⊢ 𝑀1 𝑥 { (let 𝑥𝜏1 = 𝑁2 𝑥 in 𝑁1 𝑥) : [𝑥/𝑦]𝜏2

Γ′ = Γ, 𝑓 : (𝑥:𝜏1 → 𝜏2), 𝑥 : 𝜏1

Γ′; 𝜑 ⊢ 𝑀 { 𝑁1 : 𝜏3

Γ′; 𝜑 ⊢ 𝜏3 . 𝜏2 { 𝑁2

Γ; 𝜑 ⊢ fix(𝑓 :(𝑥:𝜏1 → 𝜏2), 𝑥, 𝑀) { fix(𝑓 𝑥:𝜏1→𝜏2, 𝑥, let 𝑦𝜏3 = 𝑁1 in 𝑁2 𝑦) : 𝑥:𝜏1 → 𝜏2

Γ; 𝜑 ⊢ 𝑀1 { 𝑁1 : 𝜏1

Γ, 𝑥 : 𝜏1; 𝜑 ⊢ 𝑀2 { 𝑁2 : 𝜏2

Γ, 𝑥 : 𝜏1; 𝜑 ⊢ 𝜏2 . 𝜏 { 𝑁3

BT(Γ) ⊢wf 𝜏

Γ; 𝜑 ⊢ (let 𝑥 = 𝑀1 in 𝑀2) { (let 𝑥𝜏1 = 𝑁1 in let 𝑦𝜏2 = 𝑁2 in 𝑁3 𝑦) : 𝜏

(CI-App)

(CI-Fix)

(CI-Let)

Γ; 𝜑 ⊢ 𝑥 : {𝑥 : bool | 𝜑 ′}

Γ; 𝜑 ∧ 𝑥 ⊢ 𝑀1 { 𝑁1 : 𝜏1

Γ; 𝜑 ∧ 𝑥 ⊢ 𝜏1 . 𝜏 { 𝑁3

Γ; 𝜑 ∧ ¬𝑥 ⊢ 𝑀2 { 𝑁2 : 𝜏2

Γ; 𝜑 ∧ ¬𝑥 ⊢ 𝜏2 . 𝜏 { 𝑁4

(CI-If)

(CI-Annot)

Γ; 𝜑 ⊢ if 𝑥 then 𝑀1 else 𝑀2
{ if 𝑥 then (let 𝑦𝜏1 = 𝑁1 in 𝑁3 𝑦) else (let 𝑦𝜏2 = 𝑁2 in 𝑁4 𝑦) : 𝜏

Γ; 𝜑 ⊢ 𝑀1 { 𝑁1 : 𝜏1
Γ; 𝜑 ⊢ 𝜏1 . 𝜏 { 𝑁2
Γ; 𝜑 ⊢ (𝑀1 : 𝜏) { (let 𝑥𝜏1 = 𝑁1 in 𝑁2 𝑥) : 𝜏

Fig. 11. Type derivation rules for the source language.

{𝑥 : 𝐵 | 𝜑1} ⊓ {𝑥 : 𝐵 | 𝜑2} = {𝑥 : 𝐵 | 𝜑1 ∧ 𝜑2}
(𝑥:𝜏1 → 𝜏2) ⊓ (𝑥:𝜏3 → 𝜏4) = 𝑥:(𝜏1 ⊓ 𝜏3) → (𝜏2 ⊓ 𝜏4)

𝜏1 ⊓ 𝜏2

Γ; 𝜑 ⊢ 𝜏1 . 𝜏2 { 𝑁

(cid:15) ∃BT(Γ), 𝑥:𝐵.Φ(Γ) ∧ 𝜑 ∧ 𝜑1 ∧ 𝜑2

(cid:15) ∀BT(Γ), 𝑥:𝐵.Φ(Γ) ∧ 𝜑 ∧ 𝜑1 ⇒ (𝜑 ′ ⇔ 𝜑2)

Γ; 𝜑 ⊢ {𝑥 : 𝐵 | 𝜑1} . {𝑥 : 𝐵 | 𝜑2} { 𝜆𝑥 {𝑥:𝐵 |𝜑1 }.assert(𝜑 ′); 𝑥

(Cast-Base)

Γ; 𝜑 ⊢ 𝜏3 . 𝜏1 { 𝑁1

Γ, 𝑥 : 𝜏1 ⊓ 𝜏3; 𝜑 ⊢ 𝜏2 . 𝜏4 { 𝑁2

Γ; 𝜑 ⊢ 𝑥:𝜏1 → 𝜏2 . 𝑥:𝜏3 → 𝜏4 { 𝜆𝑓 𝑥:𝜏1→𝜏2 .𝜆𝑥𝜏3 .(let 𝑦𝜏1⊓𝜏3 = 𝑁1 𝑥 in let 𝑧𝜏2 = 𝑓 𝑦 in 𝑁2 𝑧)

(Cast-Fun)

Fig. 12. Definition of the consistent subtyping relation Γ; 𝜑 ⊢ 𝜏1 . 𝜏2 { 𝑁 .

Gradual Tensor Shape Checking

9

The consistent subtyping relation Γ; 𝜑 ⊢ 𝜏1 . 𝜏2 { 𝑁 (Figure 12) is used in the cast insertion
relation to guarantee that there exists a value that has both of the types 𝜏1 and 𝜏2 under Γ and 𝜑,
and to produce an assertion term 𝑁 that checks at runtime if a value that is statically known to be
of type 𝜏1 can be used as a value of type 𝜏2.

The rule for the base case (Cast-Base) checks if there exists a value, and an assignment of the
values to the variables in the type environment, that satisﬁes both 𝜏1 and 𝜏2. This intuitively holds
if 𝜏1 is castable to 𝜏2 for some runtime values.

The rule (Cast-Base) also produces a lambda function that implements the cast with an as-
sertion. The content of the assertion 𝜑 ′ must be at least the “diﬀerence” of 𝜑1 and 𝜑2. For ex-
ample, consider the case when 𝜏1 is a tensor type whose dimension is known to be 1, namely
{𝑥 : tensor | len(𝑥 .shape) = 1}, and 𝜏2 is a tensor type whose shape is known to be [3], namely
{𝑥 : tensor | 𝑥 .shape = [3]}. The assertion generated from casting 𝜏1 to 𝜏2 is as follows. Note
how the assertion does not need to check if 𝑥 has at least one dimension because it is guaranteed
by 𝜏1. 3

∅; true ⊢ {𝑥 : tensor | len(𝑥 .shape) = 1} . {𝑥 : tensor | 𝑥 .shape = [3]}
{ 𝜆𝑥 {𝑥:tensor |len(𝑥 .shape)=1}.assert(nth(0, 𝑥 .shape) = 3); 𝑥
The rule for the function types (Cast-Fun) recursively checks the castability of the argument
types and the return types and combines the assertion terms for them. Notice how the subsumption
for the return types 𝜏2 and 𝜏4 has the meet of two argument types 𝜏1 ⊓ 𝜏3 in the type environment.
The meet of two types (Figure 12) is deﬁned as a conjunction of the reﬁnement predicates4.

The consistent subtyping relation can be seen as a gradualization of the subtyping relation
Γ; 𝜑 ⊢ 𝜏1 <: 𝜏2 (Figure 10). In fact, when a type 𝜏1 is a subtype of another type 𝜏2, it is possible
that the assertion term generated by casting 𝜏1 to 𝜏2 only contains assertions that always succeed,
which can be erased by some optimization. The following proposition states this fact. Note that
this corresponds to the blame-subtyping theorem, one of the criteria for gradual typing presented
in [24].

Proposition 1. Γ; 𝜑 ⊢ 𝜏1 <: 𝜏2 implies Γ; 𝜑 ⊢ 𝜏1 . 𝜏2 { 𝑁 for some 𝑁 where all the assertions in

𝑁 are of the form assert(true); 𝑁 ′.

2.3.2 Type Safety. We conclude this section with a note on the soundness of our type system.
The soundness is based on the fact that if the source program is well-typed, the program after the
assertion insertion is also well-typed.

The most critical part of the proof is to prove the assertion term can be assigned a function type

from the pre-assertion type to the post-assertion type.

Lemma 1. Γ; 𝜑 ⊢ 𝜏1 . 𝜏2 { 𝑁 implies Γ; 𝜑 ⊢ 𝑁 : 𝑥:𝜏1 → 𝜏2 for some variable 𝑥 that does not

occur in 𝜏2.

This is an immediate result of Lemma 6 in the appendix. With Lemma 1, we can prove that the

assertion-inserted program can be assigned the same type as that of the original program.

Lemma 2 (Assertion Insertion Preserves Types). Γ; 𝜑 ⊢ 𝑀 { 𝑁 : 𝜏 implies Γ; 𝜑 ⊢ 𝑁 : 𝜏.

We can also prove the standard progress and preservation properties under a reasonable assump-
tion that the types of the primitive functions are properly deﬁned as follows (see the appendix for
the proofs).

3We assume that the function nth(𝑛, 𝑆) returns the 𝑛-th element of the list 𝑆, and the index is 0-indexed.
4Although the meet of two function types is deﬁned, it does not make any diﬀerence in the deﬁnition of consistent sub-
typing relation since function types in the type environment is not used.

10

Hattori et al.

Assumption 1. ⊢ 𝑐 𝑣 : 𝜏 implies ev (𝑐, 𝑣) is deﬁned and ⊢ ev (𝑐, 𝑣) : 𝜏

Combining Lemma 2 with the progress and preservation properties, we obtain the type safety

as follows.

Theorem 1 (Type Safety). With Assumption 1, ∅; true ⊢ 𝑀 { 𝑁 : 𝜏 implies 𝑁 −→∗ 𝑣 for some

𝑣, 𝑁 ⇑, or 𝑁 −→∗ error.

The type safety property states that a well-typed program does not cause untrapped dynamic
errors. The only case where a cast-inserted program causes untrapped errors is when the result
of an application of a primitive function is undeﬁned (i.e., ev (𝑐, 𝑣) is undeﬁned). The type safety
property ensures that such untrapped errors do not happen for well-typed terms as long as the
ty(𝑐) is deﬁned appropriately.

Remark 1. In our system, we used reﬁnement types to express tensor shapes. An alternative design
would be to express the type of a tensor with shape 𝑆 as tensor(𝑆) (instead of {𝑥 : tensor | 𝑥 .shape =
𝑆 }) and incorporate shape polymorphism and dynamic shapes/sizes. In such a system, the type of ReLU
function would be ∀𝑆.tensor(𝑆) → tensor(𝑆) and the type of a 1-dimensional tensor with statically
unknown length would be tensor( [★]), where ★ denotes the unknown size. Although this approach
may look simpler, it introduces an unnecessary complication.

Consider a function of type ∀𝑎, 𝑏.tensor( [𝑎 × 𝑏]) → tensor( [𝑎 + 𝑏]), and applying it to a 1-
dimensional tensor with statically unknown length tensor( [★]). The polymorphic shape parameters
𝑎 and 𝑏 would both need to be instantiated with ★, since no other option would be more appropriate.
Then, we would need to consider a dynamic cast from tensor( [★]) to tensor( [★×★]), which would
require an existential quantiﬁer in the assertion.

Since we observed through the experiments that the types of most of the realistic tensor functions can
be expressed using our reﬁnement types, we did not choose this alternative design. Some exceptional
cases we have noticed are discussed in the next section.

3 EXTENSION WITH POLYMORPHISM
Using the reﬁnement types, we can express the types of most of the shape polymorphic ten-
sor functions. For example, ReLU is a function that takes a tensor and returns a tensor of the
same shape as the input. In a type system with shape polymorphism, the type of such function
is expressed as ∀𝑆:int list. tensor(𝑆) → tensor(𝑆), whereas in our system, it is expressed as
𝑥:tensor → tensor(𝑥 .shape).

There is, however, shape polymorphism that cannot be expressed in this style. Consider the

type:

∀𝑆1, 𝑆2. (tensor(𝑆1) → tensor(𝑆2)) → tensor(𝑆1) → tensor(𝑆2)

The application function 𝜆𝑓 .𝜆𝑥 .𝑓 𝑥, for example, has this type. In our system presented so far, there
is no way to present the equivalent of this type.

We therefore discuss an extension of our type system with explicit polymorphism on sizes and
shapes. We introduce type schemes and redeﬁne type environments to map from variables to type
schemes.

𝑇 (type scheme) ::= ∀𝑥1:𝐵1, . . . , 𝑥𝑛:𝐵𝑛.𝜏

Γ (type environment) ::= ∅ | Γ [𝑥 ↦→ 𝑇 ]

We deﬁne the new typing relation as a transformation relation Γ; Δ; 𝜑 ⊢ 𝑀 : 𝜏 =⇒ 𝑀 ′ which
associates a polymorphically-typed program 𝑀 with a monomorphically-typed program 𝑀 ′:

Gradual Tensor Shape Checking

Γ(𝑓 ) = ∀𝑥1:𝐵1, . . . , 𝑥𝑛:𝐵𝑛.𝜏

BT(Γ), Δ ⊢BT 𝑡𝑖 : 𝐵𝑖 for each 𝑖

Γ; Δ; 𝜑 ⊢ 𝑓 : [𝑡1/𝑥1, . . . , 𝑡𝑛/𝑥𝑛]𝜏 =⇒ 𝑓 𝑡1 · · · 𝑡𝑛

Γ; (Δ, 𝑥1 : 𝐵1, . . . , 𝑥𝑛 : 𝐵𝑛); 𝜑 ⊢ 𝑀1 : 𝜏1 =⇒ 𝑀 ′
1

𝑥1, . . . , 𝑥𝑛 do not occur in 𝑝

(Γ, 𝑓 : ∀𝑥1:𝐵1, . . . , 𝑥𝑛:𝐵𝑛.𝜏1); Δ; 𝜑 ⊢ 𝑀2 : 𝜏2 =⇒ 𝑀 ′
2

Γ; Δ; 𝜑 ⊢ (let 𝑓 = 𝑀1 in 𝑀2) : 𝜏 ′ =⇒ (let 𝑓 = 𝜆𝑥1:𝐵1. · · · 𝜆𝑥𝑛:𝐵𝑛.𝑀 ′

1 in 𝑀 ′
2)

11

(PT-Var)

(PT-Let)

Here, Δ is a type environment for polymorphic shape variables.

Thus, a type scheme ∀𝑥1:𝐵1, . . . , 𝑥𝑛:𝐵𝑛.𝜏 is mapped to a monomorphic (reﬁnement) type 𝑥1:𝐵1 →
· · · → 𝑥𝑛:𝐵𝑛 → 𝜏. The term obtained by type inference is thus just a monomorphically-typed term,
to which the theory of gradual tensor types developed in the previous subsections apply.

There is, however, a subtle conﬂict in the best-eﬀort inference of type schemes and the coercion
mechanism of gradual typing. In the rule (PT-Let), we need to infer appropriate 𝑡1, . . . , 𝑡𝑛 and
choosing wrong 𝑡1, . . . , 𝑡𝑛 may cause cast failures, even if the original program is safe. For example,
consider the following program:
let app f x = f x in
let g = ... (* complex function on tensors *) in

... app g ...

Then, the function app can be assigned the following type scheme.

∀𝑆1, 𝑆2 : int list. (tensor(𝑆1) → tensor(𝑆2)) → tensor(𝑆1) → tensor(𝑆2)

Type inference then tries to convert the caller app g 𝑥 to a term of the form app 𝑡1 𝑡2 g 𝑥. However,
if g is a complex function for which the precise inference of shapes is diﬃcult, then we will fail to
infer appropriate parameters 𝑡1, 𝑡2. In that case, we need to fall back to the safe side, and assign to
app a less precise monomorphic type:

(tensor → tensor) → tensor → tensor.

The caller app g is then just transformed to app g 𝑥.

The solution above (of falling back to monomorphic typing when type inference for callers fail)

still has the following problems.

(1) In the example above, other callers of app will also be monomorphically typed, which leads

to imprecise type inference. For example, suppose h is statically known to have type tensor( [2; 3]) →
tensor( [1]). Based on the type scheme, we could infer the type of app h to be tensor( [2; 3]) →
tensor( [1]), but due to the presence of the caller app g, the imprecise type tensor →
tensor is inferred also for app h.

(2) It is against the principle of modular type inference that whether a polymorphic type is

assigned to app depends on callers.

A remedy to the problems above is to prepare both polymorphic and monomorphic versions for
each polymorphic function. In the example above, thus app is transformed to two functions:
let app_poly s1 s2 f x = ...
let app_mono f x = ...

where:

app_poly : ∀𝑆1, 𝑆2. (tensor(𝑆1) → tensor(𝑆2)) → tensor(𝑆1) → tensor(𝑆2)
app_mono : (tensor → tensor) → tensor → tensor

Then we can transform app g and app h to app_mono g and app_poly [2; 3] [1] h.

12

𝑥 ⊢ 𝜏1 ⊑ 𝜏2

e

(cid:15) ∀

𝑦, 𝑥 .𝜑1 ⇒ 𝜑2

𝑦 ⊢ {𝑥 : 𝐵 | 𝜑1} ⊑ {𝑥 : 𝐵 | 𝜑2}

(Prec-Base)

Hattori et al.

𝑦, 𝑥 ⊢ 𝜏2 ⊑ 𝜏4
𝑦 ⊢ 𝜏1 ⊑ 𝜏3
𝑦 ⊢ 𝑥:𝜏1 → 𝜏2 ⊑ 𝑥:𝜏3 → 𝜏4
e

e

(Prec-Fun)

Γ1 ⊑ Γ2
e

e

∅ ⊑ ∅

Γ1 ⊑ Γ2

e

dom(Γ1) ⊢ 𝜏1 ⊑ 𝜏2

Γ1, 𝑥 : 𝜏1 ⊑ Γ2, 𝑥 : 𝜏2

Fig. 13. Precision relation of types and type environments.

Remark 2. The polymorphic extension we have discussed here may still not be suitable for the

inference of some programs. For example, let 𝑓 be a function of the following type:

𝑓 : {𝑥 : tensor | prod 𝑥 .shape = 128} → tensor

and consider applying 𝑓 to the above-mentioned app. Since it is impossible to infer the shape of the
argument or the return value of 𝑓 , the type of app falls back to that of app_mono. The type of app 𝑓
is thus inferred to be tensor → tensor, although we know that app 𝑓 can be assigned the same type
as 𝑓 .

Function Layer.forward of OCaml-Torch is one of the most frequently used primitive function, and
it has similar types as app. We noticed that not being able to infer the same type as 𝑓 for app 𝑓 in the
above case critically degrades the precision of inference. Therefore, in the prototype implementation,
we specially assign the following reﬁnement-polymorphic type to Layer.forward:

∀𝑏1:bool, 𝑏2:bool.
(𝑥:{𝑥:tensor | 𝑏1} → {𝑦:tensor | 𝑏2}) → 𝑥:{𝑥:tensor | 𝑏1} → {𝑦:tensor | 𝑏2}

where the parameters 𝑏1 and 𝑏2 are instantiated with temporary reﬁnement variables. We leave the
formal justiﬁcation of such reﬁnement-polymorphic types as future work.

4 GRADUAL GUARANTEE
In a standard gradual type system, programs are compared by their precision, or the amount of
information they contain in the type annotations. This notion is used to deﬁne the gradual guar-
antee [24], which is the core property of gradual typing. The gradual guarantee comes in two
parts. The ﬁrst one is called static gradual guarantee, which states that decreasing the precision of
type annotation from a well-typed program still preserves the typeability of the program at a less
precise type. The second one is called dynamic gradual guarantee, which claims that a less precise
program behaves the same as the more precise one with fewer assertion errors.

In Subsection 4.1, we ﬁrst deﬁne the precision for the language introduced in Section 2. We then

show in Subsection 4.2 that our type system satisﬁes the gradual guarantee.

4.1 Precision
𝑥 ⊢ 𝜏1 ⊑ 𝜏2 on types by using the logical implication
Figure 13 deﬁnes the precision relation
𝑥 keeps the variables that may ap-
between the reﬁnement predicates. The sequence of variables
pear in the reﬁnement predicates. For example, the following is an example of the type precision
relation for the base type.

e

e

⊢ {𝑥 : tensor | 𝑥 .shape = [3]} ⊑ {𝑥 : tensor | len(𝑥 .shape) = 1}

Gradual Tensor Shape Checking

13

𝑥 ⊢ 𝑀1 ⊑ 𝑀2

e

𝑦 ⊢ 𝜏1 ⊑ 𝜏2

𝑦, 𝑥 ⊢ 𝑀1 ⊑ 𝑀2

𝑦 ⊢ 𝜆𝑥:𝜏1.𝑀1 ⊑ 𝜆𝑥:𝜏2.𝑀2
e

e

(PM-Lam)

𝑦 ⊢ 𝑀1 ⊑ 𝑀2

𝑦 ⊢ 𝜏1 ⊑ 𝜏2

𝑦 ⊢ (𝑀1 : 𝜏1) ⊑ (𝑀2 : 𝜏2)

(PM-Annot)

e

𝑦 ⊢ 𝑥:𝜏1 → 𝜏2 ⊑ 𝑥:𝜏3 → 𝜏4

𝑦 ⊢ fix(𝑓 : (𝑥:𝜏1 → 𝜏2), 𝑥, 𝑀) ⊑ fix(𝑓 : (𝑥:𝜏3 → 𝜏4), 𝑥, 𝑀 ′)

e

e
𝑦, 𝑓 , 𝑥 ⊢ 𝑀 ⊑ 𝑀 ′
e

(PM-Fix)

e

e

Fig. 14. Selected rules for the precision relation on terms (full definition in the appendix Figure 21).

e

Γ; 𝜑 ⊢ 𝑁1 ⊑ 𝑁2

dom(Γ) ⊢ 𝜏1 ⊑ 𝜏2

Γ, 𝑥 : 𝜏1; 𝜑 ⊢ 𝑁1 ⊑ 𝑁2

Γ ⊢ 𝑁1 ⊑ 𝑁3

Γ, 𝑥 : 𝜏1; 𝜑 ⊢ 𝑁2 ⊑ 𝑁4

Γ; 𝜑 ⊢ 𝜆𝑥𝜏1 .𝑁1 ⊑ 𝜆𝑥𝜏2 .𝑁2

(PC-Lam)

Γ; 𝜑 ⊢ let 𝑥𝜏1 = 𝑁1 in 𝑁2 ⊑ let 𝑥𝜏3 = 𝑁3 in 𝑁4
(PC-Let)

∀BT(Γ).Φ(Γ) ∧ 𝜑 ∧ 𝜑1 ⇒ 𝜑2

Γ; 𝜑 ∧ 𝜑1 ⊢ 𝑁1 ⊑ 𝑁2

Γ; 𝜑 ⊢ assert(𝜑1); 𝑁1 ⊑ assert(𝜑2); 𝑁2

(PC-Assert)

Fig. 15. Selected rules for the precision relation on cast terms ((full definition in the appendix Figure 22).

Note that in the rule (Prec-Fun), the precision of the argument type and the return type are
compared independently; the type information on 𝑥 is not used in the comparison of the return
types. This is in contrast with the rule (Sub-Fun) in Figure 10 for subtyping.

Figure 13 also extends the relation to Γ ⊑ Γ′ on type environments.
The precision relation is also extended to the relation

𝑥 ⊢ 𝑀 ⊑ 𝑀 ′ on terms, by the rules in

Figure 14. Here,

𝑥 is the sequence of variables in scope.

Finally, we deﬁne the precision relation of the cast terms in Figure 15. Unlike the term precision
relation (Figure 14), the precision relation Γ; 𝜑 ⊢ 𝑁1 ⊑ 𝑁2 on cast terms requires the type environ-
ment Γ and the logical context 𝜑 in the judgement, and the reﬁnement extraction from the type
environment Φ(Γ) is used in the rule (PC-Assert).

e

e

We also assume the following property on the evaluation of the primitive functions.

Assumption 2. If ev (𝑐, 𝑣2) and ev (𝑐, 𝑣1) are both deﬁned, then 𝑣1 ⊑ 𝑣2 implies ev (𝑐, 𝑣1) ⊑ ev (𝑐, 𝑣2)

Intuitively, the precision of cast terms are designed in such a way that, when ∅; true ⊢ 𝑁1 ⊑ 𝑁2
holds, the assertions in 𝑁1 is more strict than that of 𝑁2, and therefore the dynamic checks in 𝑁1
is more likely to fail than in 𝑁2. The following two propositions state this intuition (the proofs are
in the appendix).

𝑁1 −→ 𝑁 ′

Proposition 2. Suppose ∅; true ⊢ 𝑁1 : 𝜏 and ∅; true ⊢ 𝑁2 : 𝜏 ′. Then, ∅; true ⊢ 𝑁1 ⊑ 𝑁2 and
1 ⊑ 𝑁 ′
Proposition 3. Suppose ∅; true ⊢ 𝑁1 : 𝜏 and ∅; true ⊢ 𝑁2 : 𝜏 ′. Then, ∅; true ⊢ 𝑁1 ⊑ 𝑁2 and

2 and ∅; true ⊢ 𝑁 ′

1 imply 𝑁2 −→ 𝑁 ′

2 for some 𝑁 ′
2.

𝑁2 −→ 𝑁 ′

2 imply either of the following.

• 𝑁1 −→ 𝑁 ′
• 𝑁1 −→ error

1 and 𝑁 ′

1 ⊑ 𝑁 ′

2 for some 𝑁 ′
1

14

Hattori et al.

4.2 Gradual Guarantee
In this subsection, we prove that our system satisﬁes the gradual guarantee [24]. First, we prove
that the consistent subtyping relation Γ; 𝜑 ⊢ 𝜏1 . 𝜏2 { 𝑁 is upper-closed with respect to the
precision relation

𝑥 ⊢ 𝜏1 ⊑ 𝜏3 on types.

Lemma 3. Γ; 𝜑 ⊢ 𝜏1 . 𝜏2 { 𝑁1, dom(Γ) ⊢ 𝜏1 ⊑ 𝜏3, dom(Γ) ⊢ 𝜏2 ⊑ 𝜏4 and Γ ⊑ Γ′ implies

Γ′; 𝜑 ⊢ 𝜏3 . 𝜏4 { 𝑁2 for some 𝑁2.

e

We can further prove that the cast term 𝑁2 in the statement of Lemma 3 is less precise than the

original cast term 𝑁1 as follows.

Lemma 4. Suppose Γ ⊑ Γ′, dom(Γ) ⊢ 𝜏1 ⊑ 𝜏 ′
{ 𝑁 ′ implies Γ; 𝜑 ⊢ 𝑁 ⊑ 𝑁 ′.

and Γ′; 𝜑 ⊢ 𝜏 ′
1

. 𝜏 ′
2

1 and dom(Γ) ⊢ 𝜏2 ⊑ 𝜏 ′

2. Then, Γ; 𝜑 ⊢ 𝜏1 . 𝜏2 { 𝑁

Using the above properties, we can prove the following lemma which constitutes the core part

of the proof of the gradual guarantee.

Lemma 5. Γ ⊑ Γ′, dom(Γ) ⊢ 𝑀 ⊑ 𝑀 ′ and Γ; 𝜑 ⊢ 𝑀 { 𝑁 : 𝜏 imply Γ′; 𝜑 ⊢ 𝑀 ′ { 𝑁 ′ : 𝜏 ′,

Γ; 𝜑 ⊢ 𝑁 ⊑ 𝑁 ′ and dom(Γ) ⊢ 𝜏 ⊑ 𝜏 ′ for some 𝑁 ′ and 𝜏 ′.

Finally, we can show the static and dynamic gradual guarantee as follows.
Theorem 2 (Static gradual guarantee). ∅ ⊢ 𝑀1 ⊑ 𝑀2 and ⊢ 𝑀1 : 𝜏1 imply ⊢ 𝑀2 : 𝜏2 and

∅ ⊢ 𝜏1 ⊑ 𝜏2 for some 𝜏2.

Proof. Immediate result from Lemma 5.
Theorem 3 (Dynamic gradual guarantee). Suppose ∅ ⊢ 𝑀1 ⊑ 𝑀2 and ⊢ 𝑀1 { 𝑁1 : 𝜏1. Then,

(cid:3)

there exist 𝑁2 and 𝜏2 that satisfy all of the following.

• ⊢ 𝑀2 { 𝑁2 : 𝜏2.
• 𝑁1 −→∗ 𝑣1 implies 𝑁2 −→∗ 𝑣2 and 𝑣1 ⊑ 𝑣2 for some 𝑣2.
• 𝑁1 ⇑ implies 𝑁2 ⇑.
• 𝑁2 −→∗ 𝑣2 implies 𝑁1 −→∗ 𝑣1 and 𝑣1 ⊑ 𝑣2 for some 𝑣1, or 𝑁1 −→∗ error.
• 𝑁2 ⇑ implies 𝑁1 ⇑ or 𝑁1 −→∗ error.
Proof. From Lemma 5, ⊢ 𝑀2 { 𝑁2 : 𝜏2 holds for some 𝑁2 and 𝜏2 where ⊢ 𝑁1 ⊑ 𝑁2 and ⊢ 𝜏1 ⊑ 𝜏2.
Also, from Lemma 2, we obtain ⊢ 𝑁1 : 𝜏1 and ⊢ 𝑁2 : 𝜏2.
Using Proposition 2, 𝑁1 −→∗ 𝑣1 for some 𝑣1 implies 𝑁2 −→∗ 𝑣2 for some 𝑣2 such that 𝑣1 ⊑ 𝑣2.

Also, 𝑁1 −→∞ implies 𝑁2 −→∞.

Using Proposition 3, 𝑁2 −→∗ 𝑣2 for some 𝑣2 implies 𝑁1 −→∗ 𝑣1 for some 𝑣1 such that 𝑣1 ⊑ 𝑣2, or
(cid:3)

𝑁1 −→∗ error. Also, 𝑁2 −→∞ implies 𝑁1 −→∞ or 𝑁1 −→∗ error.

5 IMPLEMENTATION
Based on the type system introduced so far, we have implemented GraTen, a prototype type
checker targeted at OCaml programs that deﬁne and run deep learning models with OCaml-Torch.
GraTen takes an OCaml program and performs best-eﬀort type inference and type checking. If
the type checking is successful, it returns the types of top-level variables deﬁned in the program,
and the source program with necessary assertions inserted. Otherwise, the type checking fails
with an error message.

The types of the library functions, including those of OCaml-Torch, is deﬁned in stub ﬁles. For

example, the type of tr (matrix transpose function) is deﬁned as follows.
val tr : x:{ v:tensor | len v.shape = 2 } -> tensor([nth 1 x.shape; nth 0 x.shape])
GraTen uses the polymorphic extension of the formalized type system which we have discussed

in Section 3.

Gradual Tensor Shape Checking

15

1 let f (x : { v:int | v > 0 }) = x in

f (random_int ())

2
1 let f (x : { v:int | v > 0 }) = x in

f (random_int () : [] |- int => { v:int | v > 0 })

2
1 let f (x : { v:int | v > 0 }) = x in

2

f ((fun v -> assert (v > 0); v) (random_int ()))

Fig. 16. Evolution of a program in each stage of the type checking. On the top is the original program, in the
middle is the program after the type inference and insertion of the subsumption marker, and at the bottom
is the program after the cast insertion.

5.1 Structure of GraTen
5.1.1 Overview. The type checking of GraTen consists of three phases: simple type inference, best-
eﬀort type inference and consistent subtyping veriﬁcation.

The ﬁrst phase performs simple type inference for the input program, annotating the AST with

the inferred simple types of each node.

The second phase infers the reﬁnement types in a best-eﬀort manner, using the simple type
information from the previous stage. This stage also inserts subsumption marker to places in the
program which may require the assertion. A subsumption marker is a 4-tuple (Γ, 𝜑, 𝜏1, 𝜏2) which
indicates that Γ; 𝜑 ⊢ 𝜏1 . 𝜏2 { 𝑁 needs to hold for some 𝑁 .

The third phase uses these subsumption markers to check the validity of the consistent subtyp-
ing. For a subsumption marker (Γ, 𝜑, 𝜏1, 𝜏2), it is checked with a SMT solver if Γ; 𝜑 ⊢ 𝜏1 . 𝜏2 { 𝑁
actually holds for some 𝑁 . The assertion term 𝑁 is inserted for the term the subsumption marker
is inserted.

An example of the evolution of a program in each stage of the type checking is shown in Fig-
ure 16. The source program (on the top) deﬁnes a function f which takes a positive integer and
applies it to a randomly generated integer. After the type inference, a subsumption marker is in-
serted at the application of f to a random integer. The subsumption marker indicates that under
an empty logical context [], the expression random_int () has type int and it is used as the type
{𝜈 : int | 𝜈 > 0}. The ﬁnal stage uses this marker to check if ∅; true ⊢ int . {𝜈 : int | 𝜈 > 0} {
𝑁 holds for any 𝑁 , and inserts the assertion 𝑁 . Since the term random_int () is not statically
guaranteed to evaluate to a positive integer, we need to check if it is positive before being applied
to the function f.

A detailed description of the second and the third phase is given in the following subsubsections.

5.1.2 Best-Eﬀort Type Inference. In the second phase of the analysis, the type inference engine
uses the information of simple types to generate templates of reﬁnement types for wherever the
reﬁnements need to be come up with. Speciﬁcally, the argument types of unannotated lambda
expressions, the types of unannotated recursive functions, and the types of if expressions and
let expressions are tentatively expressed with the template types.

Then, the inference engine collects all the subtyping constraints from the program. Only during
the inference phase, we assume that the subsumption in the type derivation uses the standard
subtyping relation Γ; 𝜑 ⊢ 𝜏1 <: 𝜏2 instead of the consistent subtyping relation Γ; 𝜑 ⊢ 𝜏1 . 𝜏2 { 𝑁
to get more desirable inference results with fewer assertions.

The subtyping constraints are further decomposed to veriﬁcation conditions, which are logic
terms of the form 𝜑1 ⇒ 𝜑2 ⇒ 𝜑3, following the deﬁnition of the subtyping relation (Figure 10).
The ﬁrst, second, and third predicate in a veriﬁcation condition respectively corresponds to the

16

Hattori et al.

predicate from the context Φ(Γ) ∧ 𝜑, the reﬁnement of the type on the left 𝜑1, and that of the type
on the right 𝜑2. For instance, a subtyping constraint

∅; true ⊢ 𝑥:{𝑥 : int | 𝜑1} → {𝑦 : int | 𝜑2} <: 𝑥:{𝑥 : int | 𝜑3} → {𝑦 : int | 𝜑4}

is transformed into two veriﬁcation conditions true ⇒ 𝜑3 ⇒ 𝜑1 and 𝜑3 ⇒ 𝜑2 ⇒ 𝜑4.

The veriﬁcation conditions may contain the tentative variables for the undetermined reﬁnement
predicates. The inference engine tries to ﬁnd a solution for these variables in such a way that 𝜑2 and
𝜑3 in a veriﬁcation condition 𝜑1 ⇒ 𝜑2 ⇒ 𝜑3 are as “close” as possible. For example, the solution
for a veriﬁcation condition true ⇒ 𝑥 = 1 ⇒ ‘𝑏 where ‘𝑏 is a tentative variable is ‘𝑏 = (𝑥 = 1),
rather than ‘𝑏 = true. Each tentative variable holds a collection of variables it can depend on to
enhance the precision of the inference.

Of course, it is diﬃcult to ﬁnd a solution in some situations, for example, when there are multiple
tentative predicate variables on the rightmost-hand side of a veriﬁcation condition. In such cases,
the inference engine does not try to do anything with the veriﬁcation condition. As such, the solver
process the constraints until they are all cleared or it reached saturation with all of the constraints
left cannot be processed further. All the undecided tentative variables are then given the solution
of true, the most general reﬁnement predicate.

5.1.3 Consistent Subtyping Verification and Assertion Insertion. After the type inference is ﬁnished,
GraTen checks the validity of the subsumption markers and inserts assertions if necessary.

For a subsumption marker (Γ, 𝜑, 𝜏1, 𝜏2), GraTen ﬁrst attempts to prove Γ; 𝜑 ⊢ 𝜏1 <: 𝜏2 with Z3 [5].
If it is veriﬁed within a predetermined time limit, the subsumption marker is resolved as valid and
no assertion is inserted to that position. This is because Γ; 𝜑 ⊢ 𝜏1 <: 𝜏2 implies Γ; 𝜑 ⊢ 𝜏1 . 𝜏2 { 𝑁
for some 𝑁 , and as we learned in Section 2 (Proposition 1), no assertions need to be inserted if 𝜏1
is a subtype of 𝜏2.

If Γ; 𝜑 ⊢ 𝜏1 <: 𝜏2 is not veriﬁed, it still does not mean that the program cannot be type-checked.
In that case, GraTen tries to disprove ∃𝑁 . Γ; 𝜑 ⊢ 𝜏1 . 𝜏2 { 𝑁 with Z3. If it is disproved within the
time limit, it implies that the consistent subtyping constraints cannot be satisﬁed for any possible
inputs, and hence program does not type-check. Otherwise, the subsumption marker is optimisti-
cally resolved as valid and an appropriate assertion is inserted.

The inserted assertion is constructed following the deﬁnition of the consistent subtyping rela-
tion (Figure 12). In the assertion generation for the base case (Cast-Base), the predicate 𝜑2 from
the type on the right-hand side is chosen as the predicate 𝜑 ′ to be checked in the assertion.

6 EXPERIMENT
This section reports on experiments to evaluate the eﬀectiveness of the best-eﬀort inference, using
the example programs bundled in the OCaml-Torch library [2]. We have also checked how type
annotations changed the inference results.

6.1 Methods
6.1.1 Test Cases. We applied GraTen to programs under examples/ directory of the repository of
OCaml-Torch5. Table 1 shows the list of programs we have tested GraTen with. Since GraTen is
yet to support some features of OCaml and OCaml-Torch, some of the programs were modiﬁed in
such a way that does not change the structure of the neural network for GraTen to successfully
analyze them. A list of the notable modiﬁcations added to the target programs is shown below.
Smaller syntactic modiﬁcations are not listed here and can be found in the supplementary materi-
als.

5https://github.com/LaurentMazare/ocaml-torch/tree/main/examples

Gradual Tensor Shape Checking

17

(M1) Replacing or removing type-polymorphic functions. Type-polymorphic functions that are
used to create loops such as List.foldl are replaced with recursive functions. Only in
cifar/densenet.ml, List.foldi is unrolled assuming that the length of block_config is 4,
since otherwise it was impossible to add precise type annotations that can make the program
statically type-check. Other type-polymorphic functions such as no_grad are replaced with
the type-instantiated versions.

(M2) Removing use of lists with non-integer elements, especially tensor lists and layer6 lists. Since
GraTen does not support reﬁnement inference for tensor lists and layer lists yet, using lists
of tensors or layers can interfere with the precise inference. As a result, two list-taking primi-
tive functions are removed. First, Tensor.cat is replaced with a variant Tensor.cat_ which
takes two tensors instead of a list of tensors. Consequently, an application of Tensor.cat
to a list of two tensors like Tensor.cat dim [xs; ys] is changed to Tensor.cat_ dim xs
ys. Second, the use of Layer.sequential is replaced with a recursive function or a chain
of layer applications.

(M3) Replacing mutable ﬂoat objects with 0-dimensional tensors.
(M4) Deconstructing record objects. Some programs use record types to put together parameters,
tensors or layers. Some of the record ﬁelds hold integer parameters on which the tensor
shape of other objects depends. Since ﬁeld access of record objects is not supported in re-
ﬁnement predicates, such parameters are separated from the record types and added to the
function arguments.

(M5) Currying some functions that take pairs of integers. For example, function max_pool2d takes
a pair of integers for the height and width of the padding, kernel size and the stride pa-
rameters. Since tuples are not available yet in the reﬁnement predicates, we replace it with
max_pool2d_ which has the following type signature.

(M6) Changing optional arguments to required arguments. OCaml has a feature called optional
arguments which allows the programmer to omit arguments. When it is omitted, the default
argument will be used if it is deﬁned; otherwise, the argument becomes the option type
(nullable type). Because the support of polymorphic types such as option is premature in
GraTen, optional arguments without default arguments are changed to required arguments.
Among the above modiﬁcations, we believe that enabling to undo the changes from (M2), (M4),
(M5) and (M6) will be a matter of engineering that can be achieved by improving the implementa-
tion. Meanwhile, (M1) and (M3) would reuire extention of the type system.

The programs that can be found under examples/ and not listed in Table 1 are excluded from

the test cases for the following reasons:

• neural_transfer uses a library function Vgg.vgg16_layers whose type cannot be de-

scribed in GraTen.

• Programs dqn.ml, dqn_atari.ml,dqn_pong.ml under reinforcement-learninguse queues

which are not supported in GraTen yet.

• env_gym_pyml.ml and venv_env_gym_pyml.ml under reinforcement-learning treats Python

objects whose veriﬁcation is not the scope of this paper.

• reinforcement-learning/policy_gradient.ml uses mutable lists which cannot be re-

placed with another datatype already supported in GraTen.

• yolo/darknet.ml and translation/lang.ml use hash tables which are not supported in

GraTen yet.

• translation/dataset.ml and translation/lang.ml are irrelevant as tensor objects do

not appear in them.

6Functions that takes a tensor and returns a tensor.

18

Hattori et al.

Location under examples/
char_rnn/char_rnn.ml
cifar/cifar_train.ml
cifar/densenet.ml
cifar/fast_resnet.ml
cifar/preact_resnet.ml
cifar/resnet.ml
gan/began.ml
gan/gan_stability.ml
gan/mnist_cgan.ml
gan/mnist_dcgan.ml
gan/mnist_gan.ml
gan/progressive_growing_gan.ml
gan/relativistic_dcgan.ml
jit/load_and_run.ml
min-gpt/mingpt.ml
mnist/conv.ml
mnist/linear.ml
mnist/nn.ml
pretrained/ﬁnetuning.ml
pretrained/predict.ml
reinforcement-learning/a2c.ml
reinforcement-learning/ppo.ml
reinforcement-learning/rollout.ml
translation/seq2seq.ml
vae/vae.ml
yolo/yolo.ml

Modiﬁcations
(M1), (M3), (M4)
(M3)
(M1), (M2), (M5)
(M2), (M5)
(M2), (M5)
(M1), (M5)
(M1), (M3), (M5)
(M1), (M5)
(M2)
None
None
(M2)
None
None
(M4)
(M5)
None
None
(M1), (M3)
None
(M4)
(M4)
(M3), (M4), (M6)
(M1), (M3)
(M3)
None

Table 1. List of programs used in the experiment. The left column shows the location of the original program
in the OCaml-Torch repository, and the right column lists the added modifications.

To successfully analyze the test cases, GraTen provides basic support for OCaml module sys-
tems: users can deﬁne and use modules within a single ﬁle, and use modules implemented in
other ﬁles as well. For instance, cifar/cifar_train.ml uses the other programs under cifar/.
In such a case, GraTen loads in advance the type signature of the other dependent modules
from the corresponding .mli ﬁles7. Therefore, we assume in this experiment that the reﬁnement
type signatures of the dependent modules are given even before type-annotating the target pro-
gram itself. For example, upon experimenting with cifar/cifar_train.ml, it is assumed that the
cifar/model.mli deﬁnes the precise types for each ﬁeld of the type Model.t as in Figure 178, and
the .mli ﬁles for the other modules under cifar/ is deﬁned similarly.

6.1.2 Evaluation. We evaluated GraTen and its best-eﬀort inference by focusing on the following
three aspects.

7In OCaml, ﬁles with extension .mli deﬁne the interface of a module. For example, foo.mli provides the signature of a
module named Foo implemented in foo.ml.
8In GraTen, ’˜’ (tilde) before filename indicates that the name filename is used as the argument label.

Gradual Tensor Shape Checking

type t =

19

{ model_name : string
; model : x:{ v:tensor | len v.shape = 4 && nth 1 v.shape = 3 &&

-> ~is_training:bool -> tensor([nth 0 x.shape; 10])

(nth 2 v.shape // 32) * (nth 3 v.shape // 32) = 1 }

; epochs : int
; lr_schedule : ~batch_idx:int -> ~batches_per_epoch:int -> ~epoch_idx:int -> float
; batch_size : int
}

Fig. 17. Definition of Model.t in cifar/mode.mli

First, we counted the number of assertions inserted into the original program when GraTen
is used for the target program. Since the assertions indicate the program points that could fail
at runtime, the user of GraTen would want to pay attention to the location and the number of
inserted assertions and try to decrease them.

Second, we counted the minimum number of type annotations required to statically type-check
the entire program to show the realistic programmers’ burden of trying to statically type-check
the program with type annotations. The annotations were added in such a way that the functions
do not lose the original shape polymorphism. The type annotations are counted by the number of
reﬁnement types with reﬁnement predicates in the provided annotations. For example, the follow-
ing type annotation of a function type counts as 3 since the reﬁnement of the input tensor and the
two output tensors are written. The type annotation bool for the second argument is not counted
because its reﬁnement is omitted as true.
tensor([h; w]) -> ~is_training:bool -> tensor([h; w]) * tensor([h; w])

Some programs have type annotations in the deﬁnition of record types in a similar manner as in
Figure 17. Such annotations are taken into account as well.

Lastly, we measured the time taken by GraTen to analyze the unannotated and annotated pro-
grams. The experiment is conducted on a Linux machine with 8-core Intel i7-7700K (4.20GHz) and
GHC version 8.8.4 is used.

6.2 Experimental Results

Table 2 presents the result of the experiment. We analyze this result by the following three aspects:
assertions, type annotations and analysis time.

Inserted Assertions. Out of 26 programs tested, 11 programs did not require type annotations
6.2.1
to statically type-check the whole program. Among the other 15 programs, 7 programs statically
type-checked after adding appropriate type annotations. The assertions remained even after an-
notated for the other 8 programs such as gan/began.ml and gan/gan_stability.ml. These remaining
assertions are caused by the type signatures of some library functions being imprecise about the
return type. For instance, Torch.Serialize.load is a function that loads a tensor from a ﬁle and
its type signature is deﬁned as follows.

val load : ~filename:string -> tensor

The return type of load is simply deﬁned as tensor since it is impossible to assume any properties
about its shape. As a result, an assertion was inserted to check if the loaded tensor satisﬁes the
requirement to run the program without uncaught errors. Even adding type annotations to the
loaded tensor does not remove the assertion.

20

Hattori et al.

Location under examples/
char_rnn/char_rnn.ml
cifar/cifar_train.ml
cifar/densenet.ml
cifar/fast_resnet.ml
cifar/preact_resnet.ml
cifar/resnet.ml
gan/began.ml
gan/gan_stability.ml
gan/mnist_cgan.ml
gan/mnist_dcgan.ml
gan/mnist_gan.ml
gan/progressive_growing_gan.ml
gan/relativistic_dcgan.ml
jit/load_and_run.ml
min-gpt/mingpt.ml
mnist/conv.ml
mnist/linear.ml
mnist/nn.ml
pretrained/ﬁnetuning.ml
pretrained/predict.ml
reinforcement-learning/a2c.ml
reinforcement-learning/ppo.ml
reinforcement-learning/rollout.ml
translation/seq2seq.ml
vae/vae.ml
yolo/yolo.ml

LOC time (s)

98
72
116
64
85
78
220
224
117
136
83
118
171
16
207
53
50
39
69
68
105
129
91
258
78
144

1.86
0.27
10.75
0.96
3.16
2.51
24.66
15.33
0.97
4.21
0.56
1.13
5.97
0.23
16.21
0.41
0.30
0.23
0.32
0.41
1.14
1.48
8.53
8.20
1.66
0.86

#assert
4
0
6
0
8
8
1
40
1
4
0
0
1
1
8
0
0
0
0
2
0
0
9
11
4
4

#annot
4
-
2
-
5
4
1
4
-
2
-
-
1
1
7
-
-
-
-
2
-
-
7
35
10
2

time (s)
0.81
-
23.97
-
1.18
1.49
24.67
16.12
-
2.10
-
-
5.83
0.23
15.94
-
-
-
-
0.32
-
-
3.83
5.77
0.59
0.85

#assert
0
-
0
-
0
0
1
2
-
0
-
-
1
1
0
-
-
-
-
2
-
-
1
3
0
3

Table 2. Result of running GraTen to the test cases. The second column shows the size of the program
after being modified following the description in Table 1. The third and fourth columns are the results for
unannotated programs. The third column shows the time taken by GraTen to type-check the program, and
the fourth column shows the number of assertions inserted after that. From the fifth to the seventh columns
are for the programs after adding some annotations. The fifth column shows the number of annotations
added to the program.

It must be noted that some other functions are given imprecise types due to the immature sup-
port of polymorphic data types. For example, the type of Tensor.stack is deﬁned as follows
because GraTen does not eﬀectively support non-integer lists yet. Reﬁning the return types of
such functions is left as a future work.
val stack : ~dim:int -> list (tensor) -> tensor

6.2.2 Patterns of Added Type Annotations. As we added type annotations to the test cases, we
observed that the program points that require type annotations have similarities. In fact, all of the
type annotations falls into one of the following patterns.
(P1) Branches i.e., if expressions and match expressions with multiple branches (e.g., Figure 4 in

Section 1).

(P2) Recursive functions. For example, loop in translation/seq2seq.ml is annotated as follows.

Gradual Tensor Shape Checking

21

(P5) Total

Location under examples/
char_rnn/char_rnn.ml
cifar/densenet.ml
cifar/preact_resnet.ml
cifar/resnet.ml
gan/began.ml
gan/gan_stability.ml
gan/mnist_dcgan.ml
gan/relativistic_dcgan.ml
jit/load_and_run.ml
pretrained/predict.ml
reinforcement-learning/rollout.ml
translation/seq2seq.ml
min-gpt/mingpt.ml
vae/vae.ml
yolo/yolo.ml

4
2
5
4
1
4
2
1
1
2
7
35
7
10
2
Table 3. Breakdown of the number of annotations required in the test cases.

(P1)
0
0
3
2
0
2
2
0
0
1
0
3
0
0
0

(P3)
2
0
0
0
0
0
0
0
0
0
3
2
4
0
0

(P2)
2
2
2
2
0
0
0
0
0
0
0
15
3
0
0

(P4)
0
0
0
0
0
0
0
0
0
0
0
13
0
10
0

0
0
0
0
1
2
0
1
1
1
4
2
0
0
2

let rec loop

: ~state:tensor([1; hidden_size]) -> ~prevs:list ({ v:tensor | prod v.shape = 1 })
-> ~max_length:int -> list ({ v:tensor | prod v.shape = 1 })

= fun ~state ~prevs ~max_length -> ...

(P3) Higher-order shape-polymorphic arguments. For example, sample in char_rnn.ml is anno-

tated as follows.
let sample ~labels ~lstm ~dataset ~device

~linear:(linear : x:{ v:tensor | last v.shape = hidden_size }

-> tensor(init x.shape @ [labels])) = ...

(P4) Deﬁnition of record types (e.g., Figure 17).
(P5) Imprecise type signatures of primitive functions, or user-deﬁned functions of dependent
modules. For example, translation/seq2seq.ml has the following type annotation since the
return type of Tensor.stack is imprecise.
let enc_outputs : tensor([1; len input_; hidden_size]) =

Tensor.stack enc_outputs ~dim:1

As stated in Subsubsection 6.2.1, these annotations do not result in removing the assertion
insertion.

The ﬁrst three patterns indicate that the best-eﬀort type inference does not eﬀectively infer pre-
cise reﬁnements for branches, recursive functions and higher-order shape-polymorphic arguments.
The fourth pattern (P4) would be inevitable when using record types. As we discussed in the pre-
vious section, it remains as future work to exempt users from having to add type annotations for
(P5). With such improvements, we believe that it will be quite predictable to point out the program
points that require type annotation for better inference.

Table 3 shows the breakdown of the annotation counts categorized by the patterns from (P1)
to (P5). While most of the annotated programs statically type-checked with no more than 10 an-
notations, translation/seq2seq.ml required comparatively many annotations as several recursive

22

Hattori et al.

functions and record types are used in it. Still, the use of record types seems to have been eﬀec-
tive in reducing the number of annotations in translation/seq2seq.ml as the record type deﬁnition
worked as type aliases and avoided repetitive annotations for the decoder, which is passed around
to functions predict and train_loss as a higher-order shape-polymorphic argument.

6.2.3 Analysis Time. The analysis time varies widely by test cases with a slight tendency that
larger programs take a longer time than smaller ones. While GraTen type-checked most of the
15 annotated programs in a shorter time than the unannotated ones, the analysis time increased
after annotations for cifar/densenet.ml and gan/gan_stability.ml. This may be because adding pre-
cise type annotations increases the complexity of the inferred types for a function that is used in
multiple places, leading to more precise yet costly analysis.

Some instances such as gan/began.ml took a considerably long time to analyze. It is left as future

work to proﬁle and speed up the analysis.

6.3 Discussions
In this subsection, we discuss the strengths, weaknesses and our perspective of the future devel-
opment of our system.

6.3.1 Performance of Best-Eﬀort Inference. As we reported in Subsubsection 6.2.2, the best-eﬀort
type inference of GraTen does not perform well for branches, recursions and higher-order shape-
polymorphic arguments – at a glance, this may seem unsatisfying. However, the objective of this
research is not to develop a perfect inference algorithm but to provide a method that can be used
for unannotated programs, and to encourage users to work interactively with the type checker to
gradually add type annotations. With this respect, we believe that GraTen has achieved desirable
results since it will be easy for the user to ﬁnd out where to add type annotations. This is because
1) the inserted assertions can tell which parts of the program are not statically type-checked, and
2) as we discussed in Subsubsection 6.2.2, the location where type annotations would be required
is predictable from the program structure.

6.3.2 Lists of Tensors and Layers. As of now, the reﬁnement inference for lists in GraTen is only
available for integer lists. Meanwhile, it is common in deep learning programs to deﬁne a list of
tensors or a list of functions. For example, library functions Tensor.cat and Tensor.stack both
take a list of tensors and return a new tensor by concatenating them. Another library function
Layer.sequential takes a list of layers (functions that take and return a tensor) and makes an-
other layer by composing them.

An idea to add support for these library functions is to add new reﬁnement predicates for ten-
sors lists or layer lists. For example, we can add a predicate composable(𝑥, 𝑆1, 𝑆2) which means
that a list of layers 𝑥 is composable and the composed function takes a tensor of shape 𝑆1 and re-
turns a tensor of shape 𝑆2. The type of Layer.sequential can then be expressed with the shape
polymorphic extension (Section 3)) as follows.
val sequential : forall S1 S2.

{ v:list(tensor -> tensor) | composable(x,S1,S2) } -> tensor(S1) -> tensor(S2)
To practically infer composable reﬁnements for layer lists, it will also be necessary to change the
type-instantiated versions of list-manipulating functions as well. For instance, the cons function
has the following type for integer lists:
val cons_int : x:int -> y:list(int) -> { v:list(int) | v = x :: y }

Meanwhile, it would need to have the following type for layer lists.
val cons_layers

Gradual Tensor Shape Checking

23

: forall S1 S2 S3. (tensor(S1) -> tensor(S2))
-> { v:list(tensor -> tensor) | composable(v, S2, S3) }
-> { v:list(tensor -> tensor) | composable(v, S1, S3) }

6.3.3 Type-Level Polymorphism. Since GraTen does not support type-level polymorphism, some
type-polymorphic functions were replaced with type-instantiated versions in the experiments.

One idea to implement type-level polymorphism would be to allow adding multiple type signa-
tures in the fashion of mypy [17]. For example, the cons function would be assigned diﬀerent type
signatures depending on the type of the list elements as follows.
val cons : 'a -> list('a) -> list('a)
@overload
val cons: x:int -> y:list(int) -> { v:list(int) | v = x :: y }
@overload
val cons : forall S1 S2 S3. x:(tensor(S1) -> tensor(S2)) -> ...

6.3.4 Reporting Incorrect Type Annotations. Since our type system sees the standard reﬁnement
types as gradual, some users might ﬁnd the behavior of GraTen unexpected in some cases. For
example, the following program deﬁnes a function f which takes a matrix and returns a matrix.
Suppose that the programmer intended the function f to return a matrix of the same shape as the
input, but mistakenly used a transpose function tr. As a result, the type annotation is incorrect in
general.
let f x = (tr x : tensor(x.shape))

Although the type annotations in this program seem precise enough, the wrong type annotation
does not make this program fail to type-check in our type system. This is because there is a pos-
sibility that these type annotations hold when the input matrix is square. GraTen accepts this
program and outputs the following program with an assertion.
let f x = (fun y -> assert(y.shape = x.shape); y) (tr x)
To make sure that a given program behaves as expected, users not only need to make GraTen
accept the program but also need to check the output program to see if no unexpected assertions
are inserted.

7 RELATED WORK
7.1 Tensor Shape Checking in Deep Learning Programs

The tensor shape checking method has been studied for decades, mainly focusing on the ﬁeld of
numeric analysis [4, 7]. The tensor shape checking with a focus on the deep learning programs,
however, is a relatively new topic, and a variety of methods have been proposed both in academia
and in industry.

Some tools statically check tensor shapes with advanced type systems. Hasktorch [1] is a Haskell
binding of libtorch [19] which provides a mode that statically checks tensor shapes. Since they use
the type-level programming feature of Haskell to implement the tensor shapes, tensor shapes are
not ﬁrst-class objects. As a result, programs such as the one in Figure 1 cannot be expressed since it
is impossible to deﬁne the function f whose type depends on the ﬁrst-class object s. Relay [21, 22]
is an IR for deep learning compilers with a rich type system for tensor shape with type inference.
Both Relay and Hasktorch support dynamic shape as a wild card in the static shape checking.

Apart from the type-based veriﬁcation methods, some tensor shape error detection tools also
take a static approach. Pythia [6, 15] statically detects shape fault for TensorFlow [3] programs by
keeping track of the tensor shapes throughout the program using value-ﬂow analysis. The tracking

24

Hattori et al.

of shape is in a best-eﬀort manner, allowing the shape inference results to be “unknown” in some
cases. The analysis crucially relies on the programming practice in TensorFlow to annotate tensor
shapes as much as possible.

Other static checking tools took an approach that uses symbolic execution to collect constraints
from the program and veriﬁes it with a solver; Tensors Fitting Perfectly [20] and PyTea [13] are
on this approach. Both methods remove loops from the program in an ad-hoc manner based on a
reasonable assumption for the program.

Lastly, some took dynamic approaches to provide lightweight shape fault detection. Shape-
Flow [26] is an abstract interpreter of TensorFlow programs; it shares the same APIs as TensorFlow
but only calculates the shape of tensors. Users can run the analysis by replacing the import of Ten-
sorFlow with ShapeFlow in the target program, which executes more eﬃciently than the original
TensorFlow program. Elichika [12] uses a similar method to ShapeFlow with a feature to display
the interpreted shapes with a symbolic expression. These dynamic approaches enable quick anal-
ysis and require no type annotations, but provide no guarantee for untested inputs.

7.2 Static and Dynamic Checking for Refinement Types
Earlier work on dependent type system focuses on decidable type checking and inference with
restricted reﬁnement logic [10, 23, 28, 29]. Meanwhile, dynamic checking with contracts [9, 18]
oﬀers expressive veriﬁcation that cannot be covered with a static type system, but at a cost of
runtime overhead. Naturally, the combination of static and dynamic checking has been actively
explored by the successors of both parties.

Hybrid type checking [14] is a similar method to ours which extends the purely-dynamic method
of using contracts by verifying speciﬁcations statically as much as possible. This method diﬀers
from ours in that it inserts a dynamic check only when the subtyping constraint is not proven to
be valid or invalid. As a result, this method statically rejects the incorrectly annotated program
that we discussed in Subsubsection 6.3.4, while our method accepts it with a dynamic check in the
hope that a more precise type annotation will remove the need for a dynamic check. Our method
can be understood as a variant of hybrid type checking with a focus on being gradual in adding
type annotations.

The application of gradual typing to dependent type systems has also been studied [8, 16].
Especially, Gradual Reﬁnement Types [16] is very similar to our type system in that it gradual-
izes only the predicate part of a reﬁnement type system and the underlying simple type is static.
One of the diﬀerences is that their system distinguishes statically-unknown reﬁnement predicates
with statically-known ones, while our system assumes that any reﬁnement predicates can have a
statically-unknown portion. For example, consider the following program:

let 𝑓 𝑥 (𝑦 : {𝜈 : int | true}) = 𝑥/𝑦

This program is rejected in their system because the type annotation of 𝑦 indicates that the pro-
grammer is conﬁdent that 𝑦 can be any integers including 0; otherwise, the type annotation should
have been {𝜈 : int | ★ }. Meanwhile, our system interprets the type annotation as not precise
enough and accepts the program by inserting a dynamic check to 𝑦.

The type inference for gradual reﬁnement types has been studied by Vazou et al. [25]. Their

work limits the reﬁnement to liquid predicates [23] to maintain the decidability.

8 CONCLUSION AND FUTURE WORK

We presented a new type-based approach to verifying tensor shapes by combining best-eﬀort type
inference with gradual typing. Since the inference and checking of tensor shapes are undecidable

Gradual Tensor Shape Checking

25

in general, the type checker inserts assertions to places where the type safety is not statically
guaranteed, and users can assist the best-eﬀort inference by annotating the program.

We formalized the type system and proved that it satisﬁes the criteria of gradual typing, espe-
cially the gradual guarantee. The key idea is to interpret the standard reﬁnement types as gradual;
the precision relation of types is deﬁned by the logical implication of the reﬁnement predicates.
Instead of the standard subtyping relation Γ; 𝜑 ⊢ 𝜏1 <: 𝜏2, our type system uses the consistent
subtyping relation Γ; 𝜑 ⊢ 𝜏1 . 𝜏2 { 𝑁 that states there exists a value that has both of types 𝜏1 and
𝜏2. The consistent subtyping relation generates an assertion to dynamically check the type safety.
The typing relation is deﬁned as a relation to insert these assertions into the source program.

Finally, we implemented a prototype type checker GraTen and applied it with some of the
example programs publicly available in OCaml-Torch repository. We observed that, thanks to the
best-eﬀort type inference, users would not be required too many type annotations to statically
type-check the whole program, and it would not be diﬃcult to ﬁnd where to add type annotations
to improve the inference.

We conclude with some ideas for future work.

• Extension with type polymorphism. As we observed in the experiments, type polymorphic
functions are frequently used in realistic programs. Extending our type system with ML-
style type polymorphism would make the type checker more practical.

• Application for imperative languages with dynamic type system, especially Python. In this
paper, we have selected OCaml as a target language of the prototype in order to ensure
that the input program is statically-typed. Since Python and Python-based libraries such as
TensorFlow and PyTorch are more popular in the machine learning community than OCaml
and OCaml-Torch, it would be more beneﬁcial for the community to apply our approach for
Python.

Acknowledgment

This work was partially supported by JSPS KAKENHI Grant Numbers JP20H05703.

REFERENCES
[1] 2020. Hasktorch. http://hasktorch.org/. [Online; accessed 15-July-2021].
[2] 2020. OCaml-Torch. https://github.com/LaurentMazare/ocaml-torch. [Online; accessed 05-July-2021].
[3] Martín Abadi, Ashish Agarwal, Paul Barham, Eugene Brevdo, Zhifeng Chen, Craig Citro, Greg S Corrado, Andy Davis,
Jeﬀrey Dean, Matthieu Devin, et al. 2016. Tensorﬂow: Large-scale machine learning on heterogeneous distributed
systems. arXiv preprint arXiv:1603.04467 (2016).

[4] Akinori Abe and Eijiro Sumii. 2015. A simple and practical linear algebra library interface with static size checking.

arXiv preprint arXiv:1512.01898 (2015).

[5] Leonardo De Moura and Nikolaj Bjørner. 2008. Z3: An eﬃcient SMT solver. In International conference on Tools and

Algorithms for the Construction and Analysis of Systems. Springer, 337–340.

[6] Julian Dolby, Avraham Shinnar, Allison Allain, and Jenna Reinen. 2018. Ariadne: analysis for machine learning pro-
grams. In Proceedings of the 2Nd ACM SIGPLAN International Workshop on Machine Learning and Programming Lan-
guages. 1–10.

[7] Frederik Eaton. 2006. Statically typed linear algebra in Haskell. In Proceedings of the 2006 ACM SIGPLAN workshop on

Haskell. 120–121.

[8] Joseph Eremondi, Éric Tanter, and Ronald Garcia. 2019. Approximate normalization for gradual dependent types.

Proceedings of the ACM on Programming Languages 3, ICFP (2019), 1–30.

[9] Robert Bruce Findler and Matthias Felleisen. 2002. Contracts for higher-order functions. In Proceedings of the seventh

ACM SIGPLAN international conference on Functional programming. 48–59.

[10] Tim Freeman and Frank Pfenning. 1991. Reﬁnement types for ML. In Proceedings of the ACM SIGPLAN 1991 conference

on Programming language design and implementation. 268–277.

[11] Ronald Garcia, Alison M Clark, and Éric Tanter. 2016. Abstracting gradual typing. In Proceedings of the 43rd Annual

ACM SIGPLAN-SIGACT Symposium on Principles of Programming Languages. 429–442.

26

Hattori et al.

[12] Momoko Hattori, Shimpei Sawada, Shinichiro Hamaji, Masahiro Sakai, and Shunsuke Shimizu. 2020. Semi-static
type, shape, and symbolic shape inference for dynamic computation graphs. In Proceedings of the 4th ACM SIGPLAN
International Workshop on Machine Learning and Programming Languages. 11–19.

[13] Ho Young Jhoo, Sehoon Kim, Woosung Song, Kyuyeon Park, DongKwon Lee, and Kwangkeun Yi. 2021. A Static
Analyzer for Detecting Tensor Shape Errors in Deep Neural Network Training Code. arXiv preprint arXiv:2112.09037
(2021).

[14] Kenneth Knowles and Cormac Flanagan. 2010. Hybrid type checking. ACM Trans. Program. Lang. Syst. 32, 2 (2010),

6:1–6:34. https://doi.org/10.1145/1667048.1667051

[15] Siﬁs Lagouvardos, Julian Dolby, Neville Grech, Anastasios Antoniadis, and Yannis Smaragdakis. 2020. Static analysis
of shape in TensorFlow programs. In 34th European Conference on Object-Oriented Programming (ECOOP 2020). Schloss
Dagstuhl-Leibniz-Zentrum für Informatik.

[16] Nico Lehmann and Éric Tanter. 2017. Gradual reﬁnement types. In Proceedings of the 44th ACM SIGPLAN Symposium

on Principles of Programming Languages. 775–788.

[17] Jukka Lehtosalo et al. 2017. Mypy: optional static typing for python. http://mypy-lang.org/.

[Online; accessed

02-March-2022].

[18] Bertrand Meyer. 1992. Eiﬀel: the language. Prentice-Hall, Inc.
[19] Adam Paszke, Sam Gross, Soumith Chintala, Gregory Chanan, Edward Yang, Zachary DeVito, Zeming Lin, Alban

Desmaison, Luca Antiga, and Adam Lerer. 2017. Automatic diﬀerentiation in pytorch. (2017).

[20] Adam Paszke and Brennan Saeta. 2021. Tensors Fitting Perfectly. arXiv preprint arXiv:2102.13254 (2021).
[21] Jared Roesch, Steven Lyubomirsky, Marisa Kirisame, Logan Weber, Josh Pollock, Luis Vega, Ziheng Jiang, Tianqi
Chen, Thierry Moreau, and Zachary Tatlock. 2019. Relay: A high-level compiler for deep learning. arXiv preprint
arXiv:1904.08368 (2019).

[22] Jared Roesch, Steven Lyubomirsky, Logan Weber, Josh Pollock, Marisa Kirisame, Tianqi Chen, and Zachary Tatlock.
2018. Relay: A new ir for machine learning frameworks. In Proceedings of the 2nd ACM SIGPLAN International Work-
shop on Machine Learning and Programming Languages. 58–68.

[23] Patrick M Rondon, Ming Kawaguci, and Ranjit Jhala. 2008. Liquid types. In Proceedings of the 29th ACM SIGPLAN

Conference on Programming Language Design and Implementation. 159–169.

[24] Jeremy G Siek, Michael M Vitousek, Matteo Cimini, and John Tang Boyland. 2015. Reﬁned criteria for gradual typ-
ing. In 1st Summit on Advances in Programming Languages (SNAPL 2015). Schloss Dagstuhl-Leibniz-Zentrum fuer
Informatik.

[25] Niki Vazou, Éric Tanter, and David Van Horn. 2018. Gradual liquid type inference. Proceedings of the ACM on Pro-

gramming Languages 2, OOPSLA (2018), 1–25.

[26] Sahil Verma and Zhendong Su. 2020.

ShapeFlow: Dynamic Shape Interpreter for TensorFlow.

arXiv preprint

arXiv:2011.13452 (2020).

[27] Philip Wadler and Robert Bruce Findler. 2009. Well-typed programs can’t be blamed. In European Symposium on

Programming. Springer, 1–16.

[28] Hongwei Xi and Frank Pfenning. 1998. Eliminating array bound checking through dependent types. In Proceedings

of the ACM SIGPLAN 1998 conference on Programming language design and implementation. 249–257.

[29] Hongwei Xi and Frank Pfenning. 1999. Dependent types in practical programming. In Proceedings of the 26th ACM

SIGPLAN-SIGACT symposium on Principles of programming languages. 214–227.

A COMPLETE DEFINITIONS AND PROOFS
A.1 Well-Formedness of Types

A.2 Semantics of Cast Terms
Figure 19 deﬁnes the full deﬁnition of the reduction of the cast terms 𝑁1 −→ 𝑁2.

A.3 Properties about Type System

Proposition 4. (Proposition 1 in paper) Γ; 𝜑 ⊢ 𝜏1 <: 𝜏2 implies Γ; 𝜑 ⊢ 𝜏1 . 𝜏2 { 𝑁 for some 𝑁

where all the assertions in 𝑁 are of the form assert(true); 𝑁 ′.

Proof. By induction on Γ; 𝜑 ⊢ 𝜏1 <: 𝜏2.

• Case (Sub-Base).

Gradual Tensor Shape Checking

27

Γ ⊢BT 𝑡 : 𝐵

Δ ⊢BT 𝑥 : Δ(𝑥)

Δ ⊢BT true : bool

Δ ⊢BT false : bool

Δ ⊢BT 𝑠𝑖 : int

(𝑖 = 1, 2)

Δ ⊢BT 𝑆𝑖 : int list

(𝑖 = 1, 2)

Δ ⊢BT 𝑠1 = 𝑠2 : bool

Δ ⊢BT 𝑆1 = 𝑆2 : bool

Δ ⊢BT 𝑛 : int
Δ ⊢BT 𝜑 : bool
Δ ⊢BT ¬𝜑 : bool

Δ ⊢BT 𝜑𝑖 : bool

(𝑖 = 1, 2)

Δ ⊢BT 𝜑𝑖 : bool

(𝑖 = 1, 2)

Δ ⊢BT ¬𝜑1 ∧ 𝜑2 : bool
(𝑖 = 1, 2)

Δ ⊢BT 𝑆𝑖 : int list

Δ ⊢BT broadcastable(𝑆1, 𝑆2) : bool

Δ ⊢BT ¬𝜑1 ∨ 𝜑2 : bool
Δ ⊢BT 𝑆𝑖 : int list
Δ ⊢BT reshapeable(𝑆1, 𝑆2) : bool

(𝑖 = 1, 2)

Δ ⊢BT 𝑠𝑖 : int(𝑖 = 1, . . . , 𝑛)
Δ ⊢BT [𝑠1; . . . ; 𝑠𝑛] : int list
Δ ⊢BT 𝑆 : int list
Δ ⊢BT len(𝑆) : int

Δ ⊢BT 𝑆𝑖 : int list
(𝑖 = 1, 2)
Δ ⊢BT append(𝑆1, 𝑆2) : int list

Δ ⊢BT 𝑠 : int

Δ ⊢BT 𝑆 : int list

Δ ⊢BT nth(𝑠, 𝑆) : int

Δ ⊢BT 𝑠𝑖 : int

(𝑖 = 1, 2)

Δ ⊢BT 𝑠𝑖 : int

(𝑖 = 1, 2)

Δ ⊢BT 𝑠1 + 𝑠2 : int

Δ ⊢BT 𝑠1 × 𝑠2 : int

Δ(𝑥) = tensor
Δ ⊢BT 𝑥 .shape : int list
Δ ⊢BT 𝑠 : int
Δ ⊢BT −𝑠 : int
(𝑖 = 1, 2)

Δ ⊢BT 𝑠𝑖 : int
𝑠1
Δ ⊢BT
𝑠2

: int

Fig. 18. Complete rules for the typing rules of predicates, shapes and sizes.

(cid:15) ∀BT(Γ), 𝑥:𝐵.Φ(Γ) ∧ 𝜑 ∧ 𝜑1 ⇒ 𝜑2
Γ; 𝜑 ⊢ {𝑥 : 𝐵 | 𝜑1} <: {𝑥 : 𝐵 | 𝜑2}

Since (cid:15) ∃BT(Γ), 𝑥:𝐵.Φ(Γ) ∧ 𝜑 ∧ 𝜑1 ∧ 𝜑2 holds, the following is obtained as expected.

Γ; 𝜑 ⊢ {𝑥 : 𝐵 | 𝜑1} . {𝑥 : 𝐵 | 𝜑2} { 𝜆𝑥 {𝑥:𝐵 |𝜑1 }.assert(true); 𝑥

• Case (Sub-Fun).

Γ; 𝜑 ⊢ 𝜏5 <: 𝜏3

Γ, 𝑥 : 𝜏5; 𝜑 ⊢ 𝜏4 <: 𝜏6

Γ; 𝜑 ⊢ 𝑥:𝜏3 → 𝜏4 <: 𝑥:𝜏5 → 𝜏6

From the induction hypothesis, there exists 𝑁1 and 𝑁2 such that Γ; 𝜑 ⊢ 𝜏5 . 𝜏3 { 𝑁1
and Γ, 𝑥 : 𝜏5; 𝜑 ⊢ 𝜏4 . 𝜏6 { 𝑁2 hold, and all the assertions in 𝑁1 and 𝑁2 are of the form
assert(true); 𝑁 ′. Since Γ, 𝑥 : 𝜏3 ⊓ 𝜏5 ⊑ Γ, 𝑥 : 𝜏5 holds, Γ, 𝑥 : 𝜏3 ⊓ 𝜏5; 𝜑 ⊢ 𝜏4 . 𝜏6 { 𝑁2
follows. Therefore, Γ; 𝜑 ⊢ 𝑥:𝜏3 → 𝜏4 . 𝑥:𝜏5 → 𝜏6 { 𝑁 holds for an 𝑁 such that 𝑁 ≡
𝜆𝑓 𝑥:𝜏3→𝜏4 .𝜆𝑥𝜏5 .let 𝑥𝜏3⊓𝜏5 = 𝑁1 𝑥 in let 𝑦𝜏4 = 𝑓 𝑥 in 𝑁2 𝑦.

(cid:3)

28

[𝑣/𝑥]𝑁

[𝑣/𝑥]𝑐 = 𝑐

Hattori et al.

[𝑣/𝑥]𝑦 =

𝑣
𝑦
(
[𝑣/𝑥] (𝑁 𝑣 ′) = ( [𝑣/𝑥]𝑁 ) ( [𝑣/𝑥]𝑣 ′)

(𝑥 = 𝑦)
(𝑥 ≠ 𝑦)

[𝑣/𝑥] (𝜆𝑦𝜏 .𝑁 ) = 𝜆𝑦 [𝑣/𝑥 ]𝜏 .[𝑣/𝑥]𝑁
[𝑣/𝑥] (fix(𝑓 𝜏, 𝑦, 𝑁 )) = fix(𝑓 [𝑣/𝑥 ]𝜏, 𝑦, [𝑣/𝑥]𝑁 )
[𝑣/𝑥] (assert(𝜑); 𝑁 ) = assert( [𝑣/𝑥]𝜑); [𝑣/𝑥]𝑁

[𝑣/𝑥] (if 𝑣1 then 𝑁1 else 𝑁2) = if [𝑣/𝑥]𝑣1 then [𝑣/𝑥]𝑁1 else [𝑣/𝑥]𝑁2
[𝑣/𝑥] (let 𝑦𝜏 = 𝑁1 in 𝑁2) = (let 𝑦 [𝑣/𝑥 ]𝜏 = [𝑣/𝑥]𝑁1 in [𝑣/𝑥]𝑁2)
(We assume variables are appropriately alpha-renamed so that variables at diﬀerent scopes do
not collide)

𝑁1 −→ 𝑁2

assert(true); 𝑁 −→ 𝑁
assert(false); 𝑁 −→ error

(𝜆𝑥𝜏 .𝑁1) 𝑣 −→ [𝑣/𝑥]𝑁1

(fix(𝑓 𝜏, 𝑥, 𝑁1)) 𝑣 −→ [𝑣/𝑥, fix(𝑓 𝜏, 𝑥, 𝑁1)/𝑓 ]𝑁1

𝑐 𝑣 −→ ev(𝑐, 𝑣)
let 𝑥𝜏 = 𝑣 in 𝑁 −→ [𝑣/𝑥]𝑁

if true then 𝑁1 else 𝑁2 −→ 𝑁1
if false then 𝑁1 else 𝑁2 −→ 𝑁2

𝐶 [𝑁1] −→

𝐶 [𝑁2]
error

(

(𝑁1 −→ 𝑁2)
(𝑁1 −→ error)

𝐶 (context) ::= (cid:3) | 𝐶 𝑁 | 𝑣 𝐶 | let 𝑥 = 𝐶 in 𝑁

Fig. 19. Substitution and reduction of the target language (full version of Figure 8 in paper).

A.4 Type Safety

Lemma 6. Let Base(Γ) and self(𝜏, 𝑥) be deﬁned as follows.

Base(∅) = ∅

Base(Γ, 𝑥 : {𝑦 : 𝐵 | 𝜑}) = Base(Γ), 𝑥 : {𝑦 : 𝐵 | 𝜑}
Base(Γ, 𝑥 : 𝑦:𝜏1 → 𝜏2) = Base(Γ)

self({𝑥 : 𝐵 | 𝜑}, 𝑦) = {𝑥 : 𝐵 | 𝜑 ∧ 𝑥 = 𝑦}
self(𝑥:𝜏1 → 𝜏2, 𝑦) = 𝑥:𝜏1 → 𝜏2

Gradual Tensor Shape Checking

29

Then, Γ; 𝜑 ⊢ 𝜏1 . 𝜏2 { 𝑁 implies Base(Γ); 𝜑 ⊢ 𝑁 : 𝑥:𝜏1 → self(𝜏2, 𝑥) for a variable 𝑥 does not occur
in 𝜏2.

Proof. By induction on the derivation of Γ; 𝜑 ⊢ 𝜏1 . 𝜏2 { 𝑁 .
• Case (Cast-Base). The derivation must be of the following form (note 𝜏1 ≡ {𝑥 : 𝐵 | 𝜑1} and

𝜏2 ≡ {𝑥 : 𝐵 | 𝜑2}).

(cid:15) ∃BT(Γ), 𝑥:𝐵.Φ(Γ) ∧ 𝜑 ∧ 𝜑1 ∧ 𝜑2
(cid:15) ∀BT(Γ), 𝑥:𝐵.Φ(Γ) ∧ 𝜑 ∧ 𝜑1 ⇒ (𝜑 ′ ⇔ 𝜑2)
Γ; 𝜑 ⊢ {𝑥 : 𝐵 | 𝜑1} . {𝑥 : 𝐵 | 𝜑2} { 𝜆𝑥 {𝑥:𝐵 |𝜑1 }.assert(𝜑 ′); 𝑥

Noting BT(Γ) = BT(Base(Γ)) and Φ(Γ) = Φ(Base(Γ)), the following holds.
(cid:15) ∀BT(Base(Γ)), 𝑥:𝐵.Φ(Base(Γ)) ∧ 𝜑 ∧ 𝜑1 ∧ 𝜑 ′ ∧ 𝑦 = 𝑥 ⇒ [𝑦/𝑥]𝜑2

Therefore, the following proof tree concludes the proof.

Base(Γ), 𝑥 : 𝜏1; 𝜑 ∧ 𝜑 ′ ⊢ 𝑥 : {𝑦 : 𝐵 | 𝑦 = 𝑥 }
Base(Γ), 𝑥 : 𝜏1; 𝜑 ∧ 𝜑 ′ ⊢ {𝑦 : 𝐵 | 𝑦 = 𝑥 } <: {𝑦 : 𝐵 | [𝑦/𝑥]𝜑2 ∧ 𝑦 = 𝑥 }
Base(Γ), 𝑥 : 𝜏1; 𝜑 ⊢ assert(𝜑 ′); 𝑥 : {𝑦 : 𝐵 | [𝑦/𝑥]𝜑2 ∧ 𝑦 = 𝑥 }
Base(Γ); 𝜑 ⊢ (𝜆𝑥 {𝑥:𝐵 |𝜑1 }.assert(𝜑 ′); 𝑥) : 𝑥:𝜏1 → {𝑦 : 𝐵 | [𝑦/𝑥]𝜑2 ∧ 𝑦 = 𝑥 }

• Case (Cast-Fun). The derivation must be of the following form (note 𝜏1 ≡ 𝑥:𝜏3 → 𝜏4 and

𝜏2 ≡ 𝑥:𝜏5 → 𝜏6).

Γ; 𝜑 ⊢ 𝜏5 . 𝜏3 { 𝑁1

Γ, 𝑥 : 𝜏3 ⊓ 𝜏5; 𝜑 ⊢ 𝜏4 . 𝜏6 { 𝑁2

Γ; 𝜑 ⊢ 𝑥:𝜏3 → 𝜏4 . 𝑥:𝜏5 → 𝜏6
{ 𝜆𝑓 𝑥:𝜏3→𝜏4 .𝜆𝑥𝜏5 .(let 𝑦𝜏3⊓𝜏5 = 𝑁1 𝑥 in let 𝑧𝜏4 = 𝑓 𝑦 in 𝑁2 𝑧)

The following holds from the induction hypothesis for some variables 𝑎 and 𝑏 that does not
occur in 𝜏3 and 𝜏6 respectively.

Base(Γ); 𝜑 ⊢ 𝑁1 : 𝑎:𝜏5 → self(𝜏3, 𝑎)
Base(Γ, 𝑥 : 𝜏3 ⊓ 𝜏5); 𝜑 ⊢ 𝑁2 : 𝑏:𝜏4 → self(𝜏6, 𝑏)
The following proof tree concludes the proof (Γ′ := Base(Γ), 𝑓 : (𝑥:𝜏3 → 𝜏4), 𝑥 : 𝜏5).

(1)

(2)

Π1
Γ′; 𝜑 ⊢ 𝑁1 𝑥 : self(𝜏3, 𝑥)

Π2
Γ′, 𝑦:self(𝜏3, 𝑥); 𝜑 ⊢ (let 𝑧𝜏4 = 𝑓 𝑦 in 𝑁2 𝑧) : 𝜏6

Γ′; 𝜑 ⊢ (let 𝑦𝜏3⊓𝜏5 = 𝑁1 𝑥 in let 𝑧𝜏4 = 𝑓 𝑦 in 𝑁2 𝑧) : 𝑥:𝜏5 → 𝜏6
Base(Γ); 𝜑 ⊢ 𝜆𝑓 𝑥:𝜏3→𝜏4 .𝜆𝑥𝜏5 .(let 𝑦𝜏3⊓𝜏5 = 𝑁1 𝑥 in let 𝑧𝜏4 = 𝑓 𝑦 in
𝑁2 𝑧) : 𝑓 :(𝑥:𝜏3 → 𝜏4) → (𝑥:𝜏5 → 𝜏6)

Π1:

Π2:

Γ′; 𝜑 ⊢ 𝑁1 : 𝑎:𝜏5 → self(𝜏3, 𝑎)

Γ′; 𝜑 ⊢ 𝑥 : 𝜏5

Γ′; 𝜑 ⊢ 𝑁1 𝑥 : self(𝜏3, 𝑥)

Π3
Γ′, 𝑦:self(𝜏3, 𝑥); 𝜑 ⊢ 𝑓 𝑦 : 𝜏4

Γ′, 𝑦:self(𝜏3, 𝑥), 𝑧:𝜏4; 𝜑 ⊢ 𝑁2 : 𝑏:𝜏4 → self(𝜏6, 𝑏)
Γ′, 𝑦:self(𝜏3, 𝑥), 𝑧:𝜏4; 𝜑 ⊢ 𝑧 : 𝜏4
Γ′, 𝑦:self(𝜏3, 𝑥), 𝑧:𝜏4; 𝜑 ⊢ 𝑁2 𝑧 : self(𝜏6, 𝑧)
Γ′, 𝑦:self(𝜏3, 𝑥), 𝑧:𝜏4; 𝜑 ⊢ 𝑁2 𝑧 : 𝜏6

Γ′, 𝑦:self(𝜏3, 𝑥); 𝜑 ⊢ (let 𝑧𝜏4 = 𝑓 𝑦 in 𝑁2 𝑧) : 𝜏6

30

Π3:

Hattori et al.

Γ′, 𝑦:self(𝜏3, 𝑥); 𝜑 ⊢ 𝑓 : 𝑥:𝜏3 → 𝜏4
Γ′, 𝑦:self(𝜏3, 𝑥); 𝜑 ⊢ 𝑦 : 𝜏3
Γ′, 𝑦:self(𝜏3, 𝑥); 𝜑 ⊢ 𝑓 𝑦 : [𝑦/𝑥]𝜏4

Γ′, 𝑦:self(𝜏3, 𝑥); 𝜑 ⊢ [𝑦/𝑥]𝜏4 <: 𝜏4

Γ′, 𝑦:self(𝜏3, 𝑥); 𝜑 ⊢ 𝑓 𝑦 : 𝜏4

In Π3, one of the leaves Γ′, 𝑦:self(𝜏3, 𝑥); 𝜑 ⊢ [𝑦/𝑥]𝜏4 <: 𝜏4 is proven as follows:
– If 𝜏3 is a function type, so is 𝜏5, and 𝑥 is a variable of function type. Therefore, 𝑥 does not

occur in 𝜏4, and [𝑦/𝑥]𝜏4 = 𝜏4 holds.

– If 𝜏3 is a base type, Φ(Γ′, 𝑦:self(𝜏3, 𝑥)) implies 𝑦 = 𝑥. Therefore,

Γ′; 𝑦:self(𝜏3, 𝑥); 𝜑 ⊢ [𝑦/𝑥]𝜏4 <: 𝜏4 holds.

Also, in Π2, one of the leaves

Γ′, 𝑦:self(𝜏3, 𝑥), 𝑧:𝜏4; 𝜑 ⊢ 𝑁2 : 𝑏:𝜏4 → self(𝜏6, 𝑏)

(3)

is proven as follows.
– If 𝜏3 is a function type, so is 𝜏3 ⊓ 𝜏5, and (2) is equivalent to Base(Γ); 𝜑 ⊢ 𝑁2 : 𝑏:𝜏4 →

self(𝜏6, 𝑏). Therefore, (3) holds trivially.

– If 𝜏3 is a base type, so is 𝜏5. Let 𝜏3 ≡ {𝑥 : 𝐵 | 𝜑3} and 𝜏5 ≡ {𝑥 : 𝐵 | 𝜑5}. Then,

Φ(Γ′, 𝑦:self(𝜏3, 𝑥)) is logically equivalent to Φ(Base(Γ, 𝑥:𝜏3 ⊓ 𝜏5)) as follows:

Φ(Γ′, 𝑦:self(𝜏3, 𝑥)) = Φ(Base(Γ)) ∧ 𝜑5 ∧ [𝑦/𝑥]𝜑3 ∧ 𝑦 = 𝑥

Φ(Base(Γ, 𝑥:𝜏3 ⊓ 𝜏5)) = Φ(Base(Γ)) ∧ 𝜑3 ∧ 𝜑5

Therefore, Γ′, 𝑦:self(𝜏3, 𝑥); 𝜑 ⊢ 𝑁2 : 𝑏:𝜏4 → self(𝜏6, 𝑏) holds, and so does (3).

(cid:3)

Lemma 7. (Lemma 2 in paper) Γ; 𝜑 ⊢ 𝑀 { 𝑁 : 𝜏 implies Γ; 𝜑 ⊢ 𝑁 : 𝜏.

Proof. By induction on the derivation of Γ; 𝜑 ⊢ 𝑀 { 𝑁 : 𝜏.
• Case (CI-App).

Γ; 𝜑 ⊢ 𝑀1 { 𝑁1 : 𝑦:𝜏1 → 𝜏2

Γ(𝑥) = 𝜏3

Γ; 𝜑 ⊢ 𝜏3 . 𝜏1 { 𝑁2

Γ; 𝜑 ⊢ 𝑀1 𝑥 { (let 𝑥𝜏1 = 𝑁2 𝑥 in 𝑁1 𝑥) : [𝑥/𝑦]𝜏2

From the induction hypothesis and Lemma 1, we obtain the following.

Γ; 𝜑 ⊢ 𝑁1 : 𝑦:𝜏1 → 𝜏2

Γ; 𝜑 ⊢ 𝑁2 : 𝜏3 → 𝜏1

Therefore, Γ; 𝜑 ⊢ 𝑁2 𝑥 : 𝜏1 holds, and Γ; 𝜑 ⊢ (let 𝑥𝜏1 = 𝑁2 𝑥 in 𝑁1 𝑥) : [𝑥/𝑦]𝜏2 is obtained as
expected.
• Case (CI-Fix).

Γ′ = Γ, 𝑓 : (𝑥:𝜏1 → 𝜏2), 𝑥 : 𝜏1

Γ′; 𝜑 ⊢ 𝑀 { 𝑁1 : 𝜏3

Γ′; 𝜑 ⊢ 𝜏3 . 𝜏2 { 𝑁2

Γ; 𝜑 ⊢ fix(𝑓 :(𝑥:𝜏1 → 𝜏2), 𝑥, 𝑀) { fix(𝑓 𝑥:𝜏1→𝜏2, 𝑥, let 𝑦𝜏3 = 𝑁1 in 𝑁2 𝑦) : 𝑥:𝜏1 → 𝜏2

From the induction hypothesis and Lemma 1, we obtain Γ′; 𝜑 ⊢ 𝑁1 : 𝜏3 and Γ′; 𝜑 ⊢ 𝑁2 :
𝑦:𝜏3 → 𝜏2 for some variable 𝑦 that does not occur in 𝜏2. Therefore, Γ′; 𝜑 ⊢ (let 𝑦𝜏3 =
𝑁1 in 𝑁2 𝑦) : 𝜏2 holds, and thus Γ; 𝜑 ⊢ fix(𝑓 𝑥:𝜏1→𝜏2, 𝑥, let 𝑦𝜏3 = 𝑁1 in 𝑁2 𝑦) : 𝑥:𝜏1 → 𝜏2
holds as expected.

Gradual Tensor Shape Checking

• Case (CI-Let).

31

Γ; 𝜑 ⊢ 𝑀1 { 𝑁1 : 𝜏1

Γ, 𝑥 : 𝜏1; 𝜑 ⊢ 𝑀2 { 𝑁2 : 𝜏2

Γ, 𝑥 : 𝜏1; 𝜑 ⊢ 𝜏2 . 𝜏 { 𝑁3

BT(Γ) ⊢wf 𝜏

Γ; 𝜑 ⊢ (let 𝑥 = 𝑀1 in 𝑀2) { (let 𝑥𝜏1 = 𝑁1 in let 𝑦𝜏2 = 𝑁2 in 𝑁3 𝑦) : 𝜏

From the induction hypothesis and Lemma 1, we obtain the following, where 𝑧 does not
appear in 𝜏.

Γ; 𝜑 ⊢ 𝑁1 : 𝜏1

Γ, 𝑥 : 𝜏1; 𝜑 ⊢ 𝑁2 : 𝜏2

Γ, 𝑥 : 𝜏1; 𝜑 ⊢ 𝑁3 : 𝑧:𝜏2 → 𝜏

Therefore, we obtain Γ; 𝜑 ⊢ (let 𝑥𝜏1 = 𝑁1 in let 𝑦𝜏2 = 𝑁2 in 𝑁3 𝑦) : [𝑦/𝑧]𝜏 and [𝑦/𝑧]𝜏 = 𝜏
concludes the proof.

• Case (CI-If).

Γ; 𝜑 ⊢ 𝑥 : {𝑥 : bool | 𝜑 ′}

Γ; 𝜑 ∧ 𝑥 ⊢ 𝑀1 { 𝑁1 : 𝜏1
Γ; 𝜑 ∧ ¬𝑥 ⊢ 𝑀2 { 𝑁2 : 𝜏2

Γ; 𝜑 ∧ 𝑥 ⊢ 𝜏1 . 𝜏 { 𝑁3
Γ; 𝜑 ∧ ¬𝑥 ⊢ 𝜏2 . 𝜏 { 𝑁4

Γ; 𝜑 ⊢ if 𝑥 then 𝑀1 else 𝑀2
{ if 𝑥 then (let 𝑦𝜏1 = 𝑁1 in 𝑁3 𝑦) else (let 𝑦𝜏2 = 𝑁2 in 𝑁4 𝑦) : 𝜏

From the induction hypothesis and Lemma 1, we obtain the following, where 𝑧 does not
appear in 𝜏.

Γ; 𝜑 ∧ 𝑥 ⊢ 𝑁1 : 𝜏1
Γ; 𝜑 ∧ ¬𝑥 ⊢ 𝑁2 : 𝜏2

Γ; 𝜑 ∧ 𝑥 ⊢ 𝑁3 : 𝑧:𝜏1 → 𝜏
Γ; 𝜑 ∧ ¬𝑥 ⊢ 𝑁4 : 𝑧:𝜏2 → 𝜏

Therefore, Γ; 𝜑 ∧ 𝑥 ⊢ let 𝑦𝜏1 = 𝑁1 in 𝑁3 𝑦 : 𝜏 and Γ; 𝜑 ∧ ¬𝑥 ⊢ let 𝑦𝜏2 = 𝑁2 in 𝑁4 𝑦 : 𝜏
holds, and Γ; 𝜑 ⊢ if 𝑥 then (let 𝑦𝜏1 = 𝑁1 in 𝑁3 𝑦) else (let 𝑦𝜏2 = 𝑁2 in 𝑁4 𝑦) : 𝜏 follows
as expected.

• Case (CI-Annot).

Γ; 𝜑 ⊢ 𝜏1 . 𝜏 { 𝑁2
Γ; 𝜑 ⊢ 𝑀1 { 𝑁1 : 𝜏1
Γ; 𝜑 ⊢ (𝑀1 : 𝜏) { (let 𝑥𝜏1 = 𝑁1 in 𝑁2 𝑥) : 𝜏

From the induction hypothesis, Γ; 𝜑 ⊢ 𝑁1 : 𝜏1 holds. Also, from Lemma 1, we obtain Γ; 𝜑 ⊢
𝑁2 : 𝑦:𝜏1 → 𝜏, where 𝑦 does not occur in 𝜏. Therefore, Γ; 𝜑 ⊢ (let 𝑥𝜏1 = 𝑁1 in 𝑁2 𝑥) : [𝑥/𝑦]𝜏
holds, and [𝑥/𝑦]𝜏 = 𝜏 concludes the proof.

• Other cases are trivial.

(cid:3)

Definition 1. Substitution of type [𝑣/𝑥]𝜏 and type environment [𝑣/𝑥]Γ is deﬁned in Figure 20.

Lemma 8. Γ; 𝜑 ⊢ 𝜏 <: 𝜏 ′ and ⊢ 𝑣 : Γ(𝑥) imply [𝑣/𝑥]Γ; [𝑣/𝑥]𝜑 ⊢ [𝑣/𝑥]𝜏 <: [𝑣/𝑥]𝜏 ′.

Proof. By induction on Γ; 𝜑 ⊢ 𝜏 <: 𝜏 ′.
• Case (Sub-Base).

(cid:15) ∀BT(Γ), 𝑦:𝐵.Φ(Γ) ∧ 𝜑 ∧ 𝜑1 ⇒ 𝜑2
Γ; 𝜑 ⊢ {𝑦 : 𝐵 | 𝜑1} <: {𝑦 : 𝐵 | 𝜑2}

Hattori et al.

32

[𝑣/𝑥]𝜏

[𝑣/𝑥]Γ

[𝑣/𝑥]{𝑦 : 𝐵 | 𝜑} = {𝑦 : 𝐵 | [𝑣/𝑥]𝜑}
[𝑣/𝑥] (𝑦:𝜏1 → 𝜏2) = 𝑦:( [𝑣/𝑥]𝜏1) → ( [𝑣/𝑥]𝜏2)

[𝑣/𝑥]∅ = ∅
[𝑣/𝑥] (Γ, 𝑥 : 𝜏) = [𝑣/𝑥]Γ
[𝑣/𝑥] (Γ, 𝑦 : 𝜏) = ( [𝑣/𝑥]Γ),𝑦 : [𝑣/𝑥]𝜏

(𝑥 ≠ 𝑦)

Fig. 20. Substitution of type and type environment.

We can assume 𝑥 ≠ 𝑦 w.l.o.g by alpha renaming. If Γ(𝑥) is a function type, 𝑥 ∉ dom(BT(Γ))
and 𝑥 does not appear in Φ(Γ), 𝜑, 𝜑1 or 𝜑2. Therefore, BT( [𝑣/𝑥]Γ) = BT(Γ) and Φ( [𝑣/𝑥]Γ) =
Φ(Γ), we obtain ∀BT( [𝑣/𝑥]Γ), 𝑦:𝐵.Φ( [𝑣/𝑥]Γ) ∧ [𝑣/𝑥]𝜑 ∧ [𝑣/𝑥]𝜑1 ⇒ [𝑣/𝑥]𝜑2. Therefore,
[𝑣/𝑥]Γ; [𝑣/𝑥]𝜑 ⊢ [𝑣/𝑥]{𝑦 : 𝐵 | 𝜑1} <: [𝑣/𝑥]{𝑦 : 𝐵 | 𝜑2} holds.
If Γ(𝑥) is a base type, 𝑥 ∈ dom(BT(Γ)) and dom(BT( [𝑣/𝑥]Γ)) = dom(BT(Γ)) \ {𝑥 }. Since 𝑣
is a closed term, we obtain ∀BT( [𝑣/𝑥]Γ),𝑦:𝐵.[𝑣/𝑥] (Φ(Γ) ∧ 𝜑 ∧ 𝜑1) ⇒ [𝑣/𝑥]𝜑2. Therefore,
[𝑣/𝑥]Γ; [𝑣/𝑥]𝜑 ⊢ {𝑦 : 𝐵 | [𝑣/𝑥]𝜑1} <: {𝑦 : 𝐵 | [𝑣/𝑥]𝜑2} holds.

• Case (Sub-Fun).

Γ; 𝜑 ⊢ 𝜏3 <: 𝜏1

Γ, 𝑦 : 𝜏3; 𝜑 ⊢ 𝜏2 <: 𝜏4

Γ; 𝜑 ⊢ 𝑦:𝜏1 → 𝜏2 <: 𝑦:𝜏3 → 𝜏4

We can assume 𝑥 ≠ 𝑦 w.l.o.g by alpha renaming. From the induction hypothesis, we obtain:

[𝑣/𝑥]Γ; [𝑣/𝑥]𝜑 ⊢ [𝑣/𝑥]𝜑3 <: [𝑣/𝑥]𝜑1
( [𝑣/𝑥]Γ), 𝑥 : [𝑣/𝑥]𝜏3; [𝑣/𝑥]𝜑 ⊢ [𝑣/𝑥]𝜏2 <: [𝑣/𝑥]𝜏4
Therefore, [𝑣/𝑥]Γ; [𝑣/𝑥]𝜑 ⊢ [𝑣/𝑥] (𝑦:𝜏1 → 𝜏2) <: [𝑣/𝑥] (𝑦:𝜏3 → 𝜏4) holds as expected.

(cid:3)

Lemma 9. Γ; 𝜑 ⊢ 𝑁 : 𝜏 and ⊢ 𝑣 : Γ(𝑥) imply [𝑣/𝑥]Γ; [𝑣/𝑥]𝜑 ⊢ [𝑣/𝑥]𝑁 : [𝑣/𝑥]𝜏.

Proof. By induction on Γ; 𝜑 ⊢ 𝑁 : 𝜏.
• Case (CT-Const), (CT-Var-Fun) and (CT-Var-Base) is trivial.
• Case (CT-Lam).

Γ, 𝑦 : 𝜏1; 𝜑 ⊢ 𝑁1 : 𝜏2
Γ; 𝜑 ⊢ 𝜆𝑦𝜏1 .𝑁1 : 𝑦:𝜏1 → 𝜏2

We can assume 𝑥 ≠ 𝑦 by alpha renaming. From the induction hypothesis,

( [𝑣/𝑥]Γ),𝑦 : [𝑣/𝑥]𝜏1; [𝑣/𝑥]𝜑 ⊢ [𝑣/𝑥]𝑁1 : [𝑣/𝑥]𝜏2

holds. Therefore, [𝑣/𝑥]Γ; [𝑣/𝑥]𝜑 ⊢ 𝜆𝑦 [𝑣/𝑥 ]𝜏1 .[𝑣/𝑥]𝑁1 : 𝑦:( [𝑣/𝑥]𝜏1) → [𝑣/𝑥]𝜏2 holds as ex-
pected.

• Case (CT-Fix) is similar to the case (CT-Lam).

Gradual Tensor Shape Checking

• Case (CT-App).

33

Γ; 𝜑 ⊢ 𝑁1 : 𝑦:𝜏1 → 𝜏2

Γ; 𝜑 ⊢ 𝑣1 : 𝜏1

Γ; 𝜑 ⊢ 𝑁1 𝑣1 : [𝑣1/𝑦]𝜏2

We assume 𝑥 ≠ 𝑦 by alpha renaming. From the induction hypothesis, we obtain the follow-
ing.

[𝑣/𝑥]Γ; [𝑣/𝑥]𝜑 ⊢ [𝑣/𝑥]𝑁1 : 𝑦:[𝑣/𝑥]𝜏1 → [𝑣/𝑥]𝜏2
[𝑣/𝑥]Γ; [𝑣/𝑥]𝜑 ⊢ [𝑣/𝑥]𝑣1 : [𝑣/𝑥]𝜏1

Therefore, [𝑣/𝑥]Γ; [𝑣/𝑥]𝜑 ⊢ ( [𝑣/𝑥]𝑁1) ( [𝑣/𝑥]𝑣1) : [[𝑣/𝑥]𝑣1/𝑦] [𝑣/𝑥]𝜏2 holds as expected.

• Case (CT-Sub).

Γ; 𝜑 ⊢ 𝑁 : 𝜏 ′

Γ; 𝜑 ⊢ 𝜏 ′ <: 𝜏

Γ; 𝜑 ⊢ 𝑁 : 𝜏

From the induction hypothesis and Lemma 8, we obtain the following.

[𝑣/𝑥]Γ; [𝑣/𝑥]𝜑 ⊢ [𝑣/𝑥]𝑁 : [𝑣/𝑥]𝜏 ′
[𝑣/𝑥]Γ; [𝑣/𝑥]𝜑 ⊢ [𝑣/𝑥]𝜏 ′ <: [𝑣/𝑥]𝜏

Therefore, [𝑣/𝑥]Γ; [𝑣/𝑥]𝜑 ⊢ [𝑣/𝑥]𝑁 : [𝑣/𝑥]𝜏 follows as expected.

• Other cases are trivial from the induction hypothesis.

From now on, we abbreviate ∅; true ⊢ 𝑁 : 𝜏 as ⊢ 𝑁 : 𝜏.

Lemma 10 (Progress). Suppose ⊢ 𝑁 : 𝜏 holds, and ev (𝑐, 𝑣) is deﬁned whenever ⊢ 𝑐 𝑣 : 𝜏 holds for

(cid:3)

some 𝜏. Then, one of the following holds.

• 𝑁 is a value.
• 𝑁 −→ 𝑁 ′ for some 𝑁 ′
• 𝑁 −→ error

Proof. By induction on the derivation of ⊢ 𝑁 : 𝜏.

• In cases (CT-Const), (CT-Var-Fun), (CT-Var-Base), (CT-Lam) and (CT-Fix), 𝑁 is a value.
• Case (CT-App).

⊢ 𝑁1 : 𝑥:𝜏1 → 𝜏2

⊢ 𝑣 : 𝜏1

⊢ 𝑁1 𝑣 : [𝑣/𝑥]𝜏2

We split cases from the induction hypothesis.
– If 𝑁1 is a value, then 𝑁1 is either a constant, a lambda term, or a recursive function. If 𝑁1 is
a constant 𝑐, then 𝑐 𝑣 −→ ev(𝑐, 𝑣) holds since ev (𝑐, 𝑣) is deﬁned whenever 𝑐 𝑣 is well-typed.
If 𝑁1 is a lambda term or a recursive function, there exists 𝑁 ′ such that 𝑁1 𝑣 −→ 𝑁 ′ holds.

– If 𝑁1 −→ 𝑁 ′
– If 𝑁1 −→ error, then 𝑁1 𝑣 −→ error.

1 holds for some 𝑁 ′

1, then 𝑁1 𝑣 −→ 𝑁 ′

1 𝑣.

34

• Case (CT-Assert).

Hattori et al.

∅; 𝜑 ⊢ 𝑁1 : 𝜏
⊢ assert(𝜑); 𝑁1 : 𝜏

Since 𝜑 is a closed predicate, 𝜑 is either true or false. If 𝜑 ≡ true, then assert(𝜑); 𝑁1 −→
𝑁1. Otherwise, 𝜑 ≡ false holds, thus assert(𝜑); 𝑁1 −→ error.

• Case (CT-If).

⊢ 𝑣 : {𝑥 : bool | 𝜑}

∅; 𝑣 ⊢ 𝑁1 : 𝜏

∅; ¬𝑣 ⊢ 𝑁2 : 𝜏

⊢ if 𝑣 then 𝑁1 else 𝑁2 : 𝜏

Since 𝑣 is a closed boolean value, 𝑣 is either true or false. If 𝑣 ≡ true, then if true then 𝑁1 else 𝑁2 −→
𝑁1. If 𝑣 ≡ false, then if false then 𝑁1 else 𝑁2 −→ 𝑁2.

• Case (CT-Let).

⊢ 𝑁1 : 𝜏2

𝑥 : 𝜏2; true ⊢ 𝑁2 : 𝜏

⊢ let 𝑥𝜏1 = 𝑁1 in 𝑁2 : 𝜏

We split cases from the induction hypothesis.
– If 𝑁1 is a value, let 𝑣1 such that 𝑁1 ≡ 𝑣1. Then, let 𝑥𝜏1 = 𝑣1 in 𝑁2 −→ [𝑣1/𝑥]𝑁2 holds.
– If 𝑁1 −→ 𝑁 ′
1 in 𝑁2 holds.
– If 𝑁1 −→ error holds, then let 𝑥𝜏1 = 𝑁1 in 𝑁2 −→ error.

1, then let 𝑥𝜏1 = 𝑁1 in 𝑁2 −→ let 𝑥𝜏1𝑁 ′

1 holds for some 𝑁 ′

• Case (CT-Sub) is immediate from the induction hypothesis.

(cid:3)

Lemma 11 (Preservation). Suppose ⊢ 𝑐 𝑣 : 𝜏 implies ⊢ ev (𝑐, 𝑣) : 𝜏 for every 𝑐, 𝑣 and 𝜏. Then,

⊢ 𝑁 : 𝜏 and 𝑁 −→ 𝑁 ′ imply ⊢ 𝑁 ′ : 𝜏.

Proof. By induction on 𝑁 −→ 𝑁 ′.
• Case assert(true); 𝑁1 −→ 𝑁1. From the inversion of ⊢ assert(true); 𝑁1 : 𝜏, it follows that

⊢ 𝑁1 : 𝜏.

• Case (𝜆𝑥𝜏1 .𝑁1) 𝑣 −→ [𝑣/𝑥]𝑁1. The derivation of ⊢ (𝜆𝑥𝜏1 .𝑁1) 𝑣 : 𝜏 must be of the following

form.

⊢ 𝑣 : 𝜏1

𝑥 : 𝜏1; true ⊢ 𝑁1 : 𝜏2
⊢ (𝜆𝑥𝜏1 .𝑁1) : 𝑥:𝜏1 → 𝜏2

⊢ (𝜆𝑥𝜏1 .𝑁1) 𝑣 : [𝑣/𝑥]𝜏2
Using Lemma 9, 𝑥 : 𝜏1; true ⊢ [𝑣/𝑥]𝑁1 : [𝑣/𝑥]𝜏2 holds. Since 𝑥 does not appear freely in
[𝑣/𝑥]𝑁1, this entails ⊢ [𝑣/𝑥]𝑁1 : [𝑣/𝑥]𝜏2 as expected.

• Case let 𝑥𝜏1 = 𝑣 in 𝑁1 −→ [𝑣/𝑥]𝑁1 is similar to the previous case.
• Case (fix(𝑓 𝜏1, 𝑥, 𝑁1)) 𝑣 −→ [𝑣/𝑥, fix(𝑓 𝜏1, 𝑥, 𝑁1)/𝑣]𝑁1. The derivation of ⊢ (fix(𝑓 𝜏1, 𝑥, 𝑁1)) 𝑣 :

𝜏 is as follows (note 𝜏1 ≡ 𝑥:𝜏2 → 𝜏3).

⊢ 𝑣 : 𝜏2

𝑓 : (𝑥:𝜏2 → 𝜏3), 𝑥 : 𝜏2; true ⊢ 𝑁1 : 𝜏3
⊢ fix(𝑓 𝑥:𝜏2→𝜏3, 𝑥, 𝑁1) : 𝑥:𝜏2 → 𝜏3

⊢ (fix(𝑓 𝑥:𝜏2→𝜏3, 𝑥, 𝑁1)) 𝑣 : [𝑣/𝑥]𝜏3
Using Lemma 9, 𝑓 : (𝑥:𝜏2 → 𝜏3), 𝑥 : 𝜏2; true ⊢ [𝑣/𝑥, fix(𝑓 𝜏1, 𝑥, 𝑁1)/𝑣]𝑁1 : 𝜏3 holds. There-
fore,
⊢ [𝑣/𝑥, fix(𝑓 𝜏1, 𝑥, 𝑁1)/𝑣]𝑁1 : 𝜏3 holds as expected.

Gradual Tensor Shape Checking

• Case 𝑐 𝑣 −→ ev (𝑐, 𝑣) follows from the assumption.
• Case if true then 𝑁1 else 𝑁2 −→ 𝑁1 is trivial.
• Case if false then 𝑁1 else 𝑁2 −→ 𝑁2 is trivial.
• Other cases are trivial.

35

(cid:3)

Theorem 4 (Type Safety). Suppose ⊢ 𝑐 𝑣 : 𝜏 implies ⊢ ev (𝑐, 𝑣) : 𝜏 for every 𝑐, 𝑣 and 𝜏. Then,

∅; true ⊢ 𝑀 { 𝑁 : 𝜏 implies either 𝑁 −→∗ 𝑣, 𝑁 ⇑ or 𝑁 −→∗ error.

Proof. From Lemma 7, ⊢ 𝑁 : 𝜏 holds. Therefore, the result is obtained from Lemma 10 and
(cid:3)

Lemma 11.

A.5 Properties about Precision

𝑥 ⊢ 𝜏1 ⊑ 𝜏2 and

Lemma 12.
Lemma 13. (Lemma 3 in paper) Γ; 𝜑 ⊢ 𝜏1 . 𝜏2, dom(Γ) ⊢ 𝜏1 ⊑ 𝜏3, dom(Γ) ⊢ 𝜏2 ⊑ 𝜏4 and Γ ⊑ Γ′

𝑥 ⊢ 𝜏1 ⊓ 𝜏3 ⊑ 𝜏2 ⊓ 𝜏4.

𝑥 ⊢ 𝜏3 ⊑ 𝜏4 imply

imply Γ′; 𝜑 ⊢ 𝜏3 . 𝜏4.

e

e

e

Proof. By induction on Γ; 𝜑 ⊢ 𝜏1 . 𝜏2.
• Case (Cast-Base). The derivation must be of the following form.

(cid:15) ∃BT(Γ), 𝑥:𝐵.Φ(Γ) ∧ 𝜑 ∧ 𝜑1 ∧ 𝜑2
(cid:15) ∀BT(Γ), 𝑥:𝐵.Φ(Γ) ∧ 𝜑 ∧ 𝜑1 ⇒ (𝜑 ′ ⇔ 𝜑2)
Γ; 𝜑 ⊢ {𝑥 : 𝐵 | 𝜑1} . {𝑥 : 𝐵 | 𝜑2} { 𝜆𝑥 {𝑥:𝐵 |𝜑1 }.assert(𝜑 ′); 𝑥

Let 𝜑3 such that 𝜏3 ≡ {𝑥 : 𝐵 | 𝜑3}, 𝜑4 such that 𝜏4 ≡ {𝑥 : 𝐵 | 𝜑4}, and Γ′ such that Γ ⊑ Γ′.
From dom(Γ) ⊢ 𝜏1 ⊑ 𝜏3, dom(Γ) ⊢ 𝜏2 ⊑ 𝜏4 and Γ ⊑ Γ′, we obtain the following, noting that
only a variable of base type appears in the predicates.
∀BT(Γ), 𝑥:𝐵.𝜑1 ⇒ 𝜑3
∀BT(Γ), 𝑥:𝐵.𝜑2 ⇒ 𝜑4
∀BT(Γ).Φ(Γ) ⇒ Φ(Γ′)

Therefore, ∃BT(Γ′), 𝑥:𝐵.Φ(Γ′) ∧ 𝜑 ∧ 𝜑3 ∧ 𝜑4 holds, noting BT(Γ) = BT(Γ′). Let 𝜑 ′′ such that
∀BT(Γ′), 𝑥:𝐵.Φ(Γ′) ∧ 𝜑 ∧ 𝜑3 ⇒ (𝜑 ′′ ⇔ 𝜑4). Thus, Γ′; 𝜑 ⊢ {𝑥 : 𝐵 | 𝜑3} . {𝑥 : 𝐵 | 𝜑4} {
𝜆𝑥 {𝑥:𝐵 |𝜑3 }.assert(𝜑 ′′); 𝑥 holds as expected.

• Case (Cast-Fun). The derivation must be of the following form.

Γ; 𝜑 ⊢ 𝜏 ′
2

. 𝜏 ′
1

{ 𝜆𝑓 𝑥:𝜏′

1→𝜏′′

1 .𝜆𝑥𝜏′

{ 𝑁1
Γ; 𝜑 ⊢ 𝑥:𝜏 ′
2 .(let 𝑥𝜏′

Γ, 𝑥 : 𝜏 ′

1 ⊓ 𝜏 ′
2; 𝜑 ⊢ 𝜏 ′′
1
. 𝑥:𝜏 ′
2 → 𝜏 ′′
2
2 = 𝑁1 𝑥 in let 𝑦𝜏′′

1 → 𝜏 ′′
1
1⊓𝜏′

. 𝜏 ′′
2

{ 𝑁2

1 = 𝑓 𝑥 in 𝑁2 𝑦)

3, 𝜏 ′′

3 , 𝜏 ′

Let 𝜏 ′
and dom(Γ) ⊢ 𝜏2 ⊑ 𝜏4, the following holds.

4 such that 𝜏3 ≡ 𝑥:𝜏 ′

4 and 𝜏 ′′

3 → 𝜏 ′′

3 and 𝜏4 ≡ 𝑥:𝜏 ′

4 → 𝜏 ′′

4 . From dom(Γ) ⊢ 𝜏1 ⊑ 𝜏3

dom(Γ) ⊢ 𝜏 ′
dom(Γ) ⊢ 𝜏 ′

1 ⊑ 𝜏 ′
3
2 ⊑ 𝜏 ′
4
2 ⊑ 𝜏 ′

dom(Γ), 𝑥 ⊢ 𝜏 ′′
dom(Γ), 𝑥 ⊢ 𝜏 ′′

1 ⊑ 𝜏 ′′
3
2 ⊑ 𝜏 ′′
4

Using Lemma 12, dom(Γ) ⊢ 𝜏 ′
𝜏 ′
1 ⊓ 𝜏 ′
Γ′, 𝑥 : 𝜏 ′

2 ⊑ Γ′, 𝑥 : 𝜏 ′
3 ⊓ 𝜏 ′

4 holds. Let Γ′ such that Γ ⊑ Γ′. Then, Γ, 𝑥 :
{ 𝑁3 and
{ 𝑁4 hold for some 𝑁3 and 𝑁4. Therefore, we obtain Γ; 𝜑 ⊢

4 holds. From the induction hypothesis, Γ′; 𝜑 ⊢ 𝜏 ′
. 𝜏 ′′
4

3 ⊓ 𝜏 ′
4; 𝜑 ⊢ 𝜏 ′′
3

1 ⊓ 𝜏 ′

3 ⊓ 𝜏 ′

. 𝜏 ′
3

4

36

Hattori et al.

𝑥:𝜏 ′
3 → 𝜏 ′′
expected.

3 . 𝑥:𝜏 ′

4 → 𝜏 ′′
4

{ 𝜆𝑓 𝑥:𝜏′

3→𝜏′′

3 .𝜆𝑥𝜏′

4 .(let 𝑥𝜏′

3⊓𝜏′

4 = 𝑁3 𝑥 in let 𝑦𝜏′′

3 = 𝑓 𝑥 in 𝑁4 𝑦) as

Lemma 14. (Lemma 4 in paper) Suppose Γ ⊑ Γ′, dom(Γ) ⊢ 𝜏1 ⊑ 𝜏 ′
{ 𝑁 ′ implies Γ; 𝜑 ⊢ 𝑁 ⊑ 𝑁 ′.
. 𝜏 ′
2

Γ; 𝜑 ⊢ 𝜏1 . 𝜏2 { 𝑁 and Γ′; 𝜑 ⊢ 𝜏 ′
1
Proof. By induction on the derivation of Γ; 𝜑 ⊢ 𝜏1 . 𝜏2 { 𝑁 .
• Case (Cast-Base).

1 and dom(Γ) ⊢ 𝜏2 ⊑ 𝜏 ′

2. Then,

(cid:3)

(cid:15) ∃BT(Γ), 𝑥:𝐵.Φ(Γ) ∧ 𝜑 ∧ 𝜑1 ∧ 𝜑2
(cid:15) ∀BT(Γ), 𝑥:𝐵.Φ(Γ) ∧ 𝜑 ∧ 𝜑1 ⇒ (𝜑 ′ ⇔ 𝜑2)
Γ; 𝜑 ⊢ {𝑥 : 𝐵 | 𝜑1} . {𝑥 : 𝐵 | 𝜑2} { 𝜆𝑥 {𝑥:𝐵 |𝜑1 }.assert(𝜑 ′); 𝑥

From the inversion of dom(Γ) ⊢ 𝜏1 ⊑ 𝜏 ′
1} and 𝜏 ′
𝜑 ′

2 ≡ {𝑥 : 𝐵 | 𝜑 ′

1 and 𝜑 ′

2} for some 𝜑 ′
∀dom(Γ), 𝑥 .𝜑1 ⇒ 𝜑 ′
1

∀dom(Γ), 𝑥 .𝜑2 ⇒ 𝜑 ′
2

1 and dom(Γ) ⊢ 𝜏2 ⊑ 𝜏 ′

2, it must be that 𝜏 ′

1 ≡ {𝑥 : 𝐵 |

2 where the following holds.

Since the variables of function types cannot be used in the predicates, we can restrict the
quantiﬁed variables to dom(BT(Γ)) as follows.

∀dom(BT(Γ)), 𝑥 .𝜑1 ⇒ 𝜑 ′
1
∀dom(BT(Γ)), 𝑥 .𝜑2 ⇒ 𝜑 ′
2

(4)

(5)

The derivation of Γ; 𝜑 ⊢ 𝜏 ′
1

. 𝜏 ′
2

{ 𝑁 ′ must be of the following form.

(cid:15) ∃BT(Γ), 𝑥:𝐵.Φ(Γ) ∧ 𝜑 ∧ 𝜑 ′

(cid:15) ∀BT(Γ), 𝑥:𝐵.Φ(Γ) ∧ 𝜑 ∧ 𝜑 ′
1} . {𝑥 : 𝐵 | 𝜑 ′

2} { 𝜆𝑥 {𝑥:𝐵 |𝜑′

Γ; 𝜑 ⊢ {𝑥 : 𝐵 | 𝜑 ′

1 ∧ 𝜑 ′
2
1 ⇒ (𝜑 ′′ ⇔ 𝜑 ′
2)
1 }.assert(𝜑 ′′); 𝑥

Therefore, we obtain ∀BT(Γ), 𝑥:𝐵.Φ(Γ)∧𝜑∧𝜑1∧𝜑 ′ ⇒ 𝜑 ′′ as follows: Assume Φ(Γ)∧𝜑∧𝜑1∧𝜑 ′.
1 follows from (4), and 𝜑2 follows from the premise of Γ; 𝜑 ⊢ 𝜏1 . 𝜏2 { 𝑁 . Therefore,
Then, 𝜑 ′
2 is obtained by (5). With the premise of Γ; 𝜑 ⊢ 𝜏 ′
𝜑 ′
Thus, we obtain Γ; 𝜑 ⊢ 𝑁 ⊑ 𝑁 ′ as follows.

{ 𝑁 ′, we obtain 𝜑 ′′ as expected.

. 𝜏 ′
2

1

∀BT(Γ), 𝑥:𝐵.Φ(Γ) ∧ 𝜑 ∧ 𝜑1 ∧ 𝜑 ′ ⇒ 𝜑 ′′
Γ, 𝑥 : {𝑥 : 𝐵 | 𝜑1}; 𝜑 ⊢ assert(𝜑 ′); 𝑥 ⊑ assert(𝜑 ′′); 𝑥

Γ; 𝜑 ⊢ 𝜆𝑥 {𝑥:𝐵 |𝜑1 }.assert(𝜑 ′); 𝑥 ⊑ 𝜆𝑥 {𝑥:𝐵 |𝜑′

1 }.assert(𝜑 ′′); 𝑥

• Case (Cast-Fun).

Γ; 𝜑 ⊢ 𝜏5 . 𝜏3 { 𝑁1

Γ, 𝑥 : 𝜏3 ⊓ 𝜏5; 𝜑 ⊢ 𝜏4 . 𝜏6 { 𝑁2

Γ; 𝜑 ⊢ 𝑥:𝜏3 → 𝜏4 . 𝑥:𝜏5 → 𝜏6
{ 𝜆𝑓 𝑥:𝜏3→𝜏4 .𝜆𝑥𝜏5 .(let 𝑥𝜏3⊓𝜏5 = 𝑁1 𝑥 in let 𝑦𝜏4 = 𝑓 𝑥 in 𝑁2 𝑦)

Gradual Tensor Shape Checking

37

𝑥 ⊢ 𝑀1 ⊑ 𝑀2

𝑥 ⊢ 𝑐 ⊑ 𝑐 (PM-Const)

𝑦 ⊢ 𝑥 ⊑ 𝑥 (PM-Var)

e

e

𝑦 ⊢ 𝑀1 ⊑ 𝑀2

𝑦 ⊢ 𝜏1 ⊑ 𝜏2
e

e
(PM-Annot)

𝑦 ⊢ (𝑀1 : 𝜏1) ⊑ (𝑀2 : 𝜏2)

𝑦 ⊢ 𝜏1 ⊑ 𝜏2

𝑦, 𝑥 ⊢ 𝑀1 ⊑ 𝑀2

𝑦 ⊢ 𝜆𝑥:𝜏1.𝑀1 ⊑ 𝜆𝑥:𝜏2.𝑀2
e
𝑦 ⊢ 𝑀1 ⊑ 𝑀2
𝑦 ⊢ 𝑀1 𝑥 ⊑ 𝑀2 𝑥

e

e

e

e
𝑦, 𝑥 ⊢ 𝑀2 ⊑ 𝑀 ′
𝑦 ⊢ 𝑀1 ⊑ 𝑀 ′
2
1
e
1 in 𝑀 ′
𝑦 ⊢ (let 𝑥 = 𝑀1 in 𝑀2) ⊑ (let 𝑥 = 𝑀 ′
2)
𝑦, 𝑓 , 𝑥 ⊢ 𝑀 ⊑ 𝑀 ′

e
𝑦 ⊢ 𝑥:𝜏1 → 𝜏2 ⊑ 𝑥:𝜏3 → 𝜏4
e

e

e

𝑦 ⊢ fix(𝑓 : (𝑥:𝜏1 → 𝜏2), 𝑥, 𝑀) ⊑ fix(𝑓 : (𝑥:𝜏3 → 𝜏4), 𝑥, 𝑀 ′)

e

e

𝑦 ⊢ 𝑀1 ⊑ 𝑀 ′
1

e
𝑦 ⊢ 𝑀2 ⊑ 𝑀 ′
2

𝑦 ⊢ (if 𝑥 then 𝑀1 else 𝑀2) ⊑ (if 𝑥 then 𝑀 ′

1 else 𝑀 ′
2)

e

e

e

Fig. 21. Precision relation over terms (full version of Figure 14 in paper).

(PM-Lam)

(PM-App)

(PM-Let)

(PM-Fix)

(PM-If)

From the inversion of dom(Γ) ⊢ 𝜏1 ⊑ 𝜏 ′
and 𝜏 ′

6 for some 𝜏 ′

2 ≡ 𝑥:𝜏 ′

5 → 𝜏 ′

3, 𝜏 ′

1 and dom(Γ) ⊢ 𝜏2 ⊑ 𝜏 ′

2, it must be that 𝜏 ′

1 ≡ 𝑥:𝜏 ′

3 → 𝜏 ′
4

6 where the following holds.

5 and 𝜏 ′
4, 𝜏 ′
dom(Γ) ⊢ 𝜏3 ⊑ 𝜏 ′
3
dom(Γ) ⊢ 𝜏5 ⊑ 𝜏 ′
5
dom(Γ), 𝑥 ⊢ 𝜏4 ⊑ 𝜏 ′
4
dom(Γ), 𝑥 ⊢ 𝜏6 ⊑ 𝜏 ′
6
{ 𝑁 ′ must be of the following form.

(6)

(7)

(8)

(9)

The derivation of Γ′; 𝜑 ⊢ 𝜏 ′
1

. 𝜏 ′
2

Γ′; 𝜑 ⊢ 𝜏 ′
5

. 𝜏 ′
3

{ 𝜆𝑓 𝑥:𝜏′

3→𝜏′

4 .𝜆𝑥𝜏′

Γ′, 𝑥 : 𝜏 ′

{ 𝑁 ′
1
Γ′; 𝜑 ⊢ 𝑥:𝜏 ′
3 → 𝜏 ′
4
3⊓𝜏′
5 .(let 𝑥𝜏′
5 = 𝑁 ′

3 ⊓ 𝜏 ′
5; 𝜑 ⊢ 𝜏 ′
4
5 → 𝜏 ′
. 𝑥:𝜏 ′
6
1 𝑥 in let 𝑦𝜏′

. 𝜏 ′
6

{ 𝑁 ′
2

With (6) and (7), we obtain Γ; 𝜑 ⊢ 𝑁1 ⊑ 𝑁 ′
𝜏3 ⊓ 𝜏5 ⊑ Γ′, 𝑥 : 𝜏 ′
with (8) and (9). Thus, Γ; 𝜑 ⊢ 𝑁 ⊑ 𝑁 ′ follows as expected.

5, we obtain Γ, 𝑥 : 𝜏3 ⊓ 𝜏5; 𝜑 ⊢ 𝑁2 ⊑ 𝑁 ′

3 ⊓ 𝜏 ′

4 = 𝑓 𝑥 in 𝑁 ′

2 𝑦)
1 from the induction hypothesis. Noting Γ, 𝑥 :
2 from the induction hypothesis

(cid:3)

A.6 Gradual Guarantee

Figure 21 and Figure 22 presents the full deﬁnition of the precision of terms
precision of cast terms Γ; 𝜑 ⊢ 𝑁1 ⊑ 𝑁2 respectively.

𝑥 ⊢ 𝑀1 ⊑ 𝑀2 and the

Remark 3. The notation ˜𝑥 \ {𝑦} denotes a sequence of variables that is created by removing 𝑦 from

e

˜𝑥.

Lemma 15. dom(Γ) ⊢ 𝜏1 ⊑ 𝜏2, 𝑣1 ⊑ 𝑣2 and ⊢ 𝑣1 : Γ(𝑥) imply (dom(Γ) \ {𝑥 }) ⊢ [𝑣1/𝑥]𝜏1 ⊑

[𝑣2/𝑥]𝜏2.

Proof. By induction on Γ ⊢ 𝜏1 ⊑ 𝜏2.

38

Γ; 𝜑 ⊢ 𝑁1 ⊑ 𝑁2

Hattori et al.

dom(Γ) ⊢ 𝜏1 ⊑ 𝜏2

Γ; 𝜑 ⊢ 𝑐 ⊑ 𝑐

(PC-Const)
Γ, 𝑥 : 𝜏1; 𝜑 ⊢ 𝑁1 ⊑ 𝑁2

Γ; 𝜑 ⊢ 𝑥 ⊑ 𝑥

Γ; 𝜑 ⊢ 𝑁1 ⊑ 𝑁2

(PC-Var)
∅; true ⊢ 𝑣1 ⊑ 𝑣2

Γ; 𝜑 ⊢ 𝜆𝑥𝜏1 .𝑁1 ⊑ 𝜆𝑥𝜏2 .𝑁2

Γ; 𝜑 ⊢ 𝑁1 𝑣1 ⊑ 𝑁2 𝑣2

(PC-Lam)

dom(Γ) ⊢ 𝑥:𝜏1 → 𝜏2 ⊑ 𝑥:𝜏 ′

1 → 𝜏 ′
2
Γ; 𝜑 ⊢ fix(𝑓 𝑥:𝜏1→𝜏2, 𝑥, 𝑁1) ⊑ fix(𝑓 𝑥:𝜏′

Γ, 𝑓 : 𝑥:𝜏1 → 𝜏2, 𝑥 : 𝜏1; 𝜑 ⊢ 𝑁1 ⊑ 𝑁2
2, 𝑥, 𝑁2)

1→𝜏′

Γ ⊢ 𝑁1 ⊑ 𝑁3

Γ, 𝑥 : 𝜏1; 𝜑 ⊢ 𝑁2 ⊑ 𝑁4

Γ; 𝜑 ⊢ let 𝑥𝜏1 = 𝑁1 in 𝑁2 ⊑ let 𝑥𝜏3 = 𝑁3 in 𝑁4

∅; true ⊢ 𝑣1 ⊑ 𝑣2

Γ; 𝜑 ∧ 𝑣1 ⊢ 𝑁1 ⊑ 𝑁3

Γ; 𝜑 ∧ ¬𝑣1 ⊢ 𝑁2 ⊑ 𝑁4

Γ; 𝜑 ⊢ if 𝑣1 then 𝑁1 else 𝑁2 ⊑ if 𝑣2 then 𝑁3 else 𝑁4

∀BT(Γ).Φ(Γ) ∧ 𝜑 ∧ 𝜑1 ⇒ 𝜑2

Γ; 𝜑 ∧ 𝜑1 ⊢ 𝑁1 ⊑ 𝑁2

Γ; 𝜑 ⊢ assert(𝜑1); 𝑁1 ⊑ assert(𝜑2); 𝑁2

Fig. 22. Precision relation of the cast terms (full version of Figure 15 in paper).

(PC-App)

(PC-Fix)

(PC-Let)

(PC-If)

(PC-Assert)

• Case (Prec-Base).

∀dom(Γ),𝑦.𝜑1 ⇒ 𝜑2
dom(Γ) ⊢ {𝑦 : 𝐵 | 𝜑1} ⊑ {𝑦 : 𝐵 | 𝜑2}

We can assume 𝑦 ≠ 𝑥 w.l.o.g by alpha renaming.
If Γ(𝑥) is a function type, 𝑥 does not appear in 𝜑1 or 𝜑2. Therefore, the result follows trivially.
If Γ(𝑥) is a base type, 𝑣1 is a constant, and thus 𝑣1 = 𝑣2 follows from 𝑣1 ⊑ 𝑣2. Therefore,
∀dom(Γ) \ {𝑥 }, 𝑦.[𝑣1/𝑥]𝜑1 ⇒ [𝑣2/𝑥]𝜑2 follows.

• Case (Prec-Fun).

dom(Γ) ⊢ 𝜏3 ⊑ 𝜏5

dom(Γ), 𝑦 ⊢ 𝜏4 ⊑ 𝜏6

dom(Γ) ⊢ 𝑦:𝜏3 → 𝜏4 ⊑ 𝑦:𝜏5 → 𝜏6

We can assume 𝑦 ≠ 𝑥 w.l.o.g by alpha renaming. From the induction hypothesis, the follow-
ing holds.

dom(Γ) \ {𝑥 } ⊢ [𝑣1/𝑥]𝜏3 ⊑ [𝑣2/𝑥]𝜏5
(dom(Γ) \ {𝑥 }), 𝑦 ⊢ [𝑣1/𝑥]𝜏3 ⊑ [𝑣2/𝑥]𝜏5

Therefore, we obtain dom(Γ) \ {𝑥 } ⊢ [𝑣1/𝑥] (𝑦:𝜏3 → 𝜏4) ⊑ [𝑣2/𝑥] (𝑦:𝜏5 → 𝜏6) as expected.

(cid:3)

Lemma 16. Γ; 𝜑 ⊢ 𝑁 ⊑ 𝑁 ′, 𝑣 ⊑ 𝑣 ′ and ⊢ 𝑣 : Γ(𝑥) imply [𝑣/𝑥]Γ; [𝑣/𝑥]𝜑 ⊢ [𝑣/𝑥]𝑁 ⊑ [𝑣 ′/𝑥]𝑁 ′.

Proof. By induction on Γ; 𝜑 ⊢ 𝑁 ⊑ 𝑁 ′.
• Case (PC-Const) is trivial.

Gradual Tensor Shape Checking

39

• Case (PC-Var). If 𝑁 = 𝑁 ′ = 𝑥, then [𝑣/𝑥]𝑁 = 𝑣 and [𝑣 ′/𝑥]𝑁 ′ = 𝑣 ′. Therefore, [𝑣/𝑥]Γ; [𝑣/𝑥]𝜑 ⊢
[𝑣/𝑥]𝑁 ⊑ [𝑣 ′/𝑥]𝑁 ′ holds from 𝑣 ⊑ 𝑣 ′. Otherwise, [𝑣/𝑥]Γ; [𝑣/𝑥]𝜑 ⊢ [𝑣/𝑥]𝑁 ⊑ [𝑣 ′/𝑥]𝑁 ′ triv-
ially holds from (PC-Var).

• Case (PC-Lam).

dom(Γ) ⊢ 𝜏1 ⊑ 𝜏2

Γ, 𝑦 : 𝜏1; 𝜑 ⊢ 𝑁1 ⊑ 𝑁2

Γ; 𝜑 ⊢ 𝜆𝑦𝜏1 .𝑁1 ⊑ 𝜆𝑦𝜏2 .𝑁2

We can assume 𝑥 ≠ 𝑦 w.l.o.g by alpha renaming. From the induction hypothesis, ( [𝑣/𝑥]Γ), 𝑦 :
[𝑣/𝑥]𝜏1; [𝑣/𝑥]𝜑 ⊢ [𝑣/𝑥]𝑁1 ⊑ [𝑣 ′/𝑥]𝑁2 holds. Also, dom(Γ) \ {𝑥 } ⊢ [𝑣/𝑥]𝜏1 ⊑ [𝑣 ′/𝑥]𝜏2
holds from Lemma 15. Noting dom( [𝑣/𝑥]Γ) = dom(Γ) \ {𝑥 }, we obtain [𝑣/𝑥]Γ; [𝑣/𝑥]𝜑 ⊢
𝜆𝑦 [𝑣/𝑥 ]𝜏1 .[𝑣/𝑥]𝑁1 ⊑ 𝜆𝑦 [𝑣′/𝑥 ]𝜏2 .[𝑣 ′/𝑥]𝑁2 as expected.

• Case (PC-Let).

Γ ⊢ 𝑁1 ⊑ 𝑁3

Γ, 𝑦 : 𝜏1; 𝜑 ⊢ 𝑁2 ⊑ 𝑁4

Γ; 𝜑 ⊢ let 𝑦𝜏1 = 𝑁1 in 𝑁2 ⊑ let 𝑦𝜏3 = 𝑁3 in 𝑁4

We can assume 𝑥 ≠ 𝑦 w.l.o.g by alpha renaming. From the induction hypothesis, followings
hold.

[𝑣/𝑥]Γ; [𝑣/𝑥]𝜑 ⊢ [𝑣/𝑥]𝑁1 ⊑ [𝑣/𝑥]𝑁3
( [𝑣/𝑥]Γ),𝑦 : [𝑣/𝑥]𝜏1; [𝑣/𝑥]𝜑 ⊢ [𝑣/𝑥]𝑁2 ⊑ [𝑣/𝑥]𝑁4

Therefore,

[𝑣/𝑥]Γ; [𝑣/𝑥]𝜑 ⊢ (let 𝑦 [𝑣/𝑥 ]𝜏1 = [𝑣/𝑥]𝑁1 in [𝑣/𝑥]𝑁2) ⊑ (let 𝑦 [𝑣/𝑥 ]𝜏3 = [𝑣/𝑥]𝑁3 in [𝑣/𝑥]𝑁4)

holds as expected.

• Case (PC-Fix) and (PC-Let) is similar to (PC-Lam).
• Case (PC-App) and (PC-If) is immediate from the induction hypothesis.
• Case (PC-Assert).

∀BT(Γ).Φ(Γ) ∧ 𝜑 ∧ 𝜑1 ⇒ 𝜑2

Γ; 𝜑 ∧ 𝜑1 ⊢ 𝑁1 ⊑ 𝑁2

Γ; 𝜑 ⊢ assert(𝜑1); 𝑁1 ⊑ assert(𝜑2); 𝑁2

From the induction hypothesis, [𝑣/𝑥]Γ; [𝑣/𝑥]𝜑 ∧ [𝑣/𝑥]𝜑1 ⊢ [𝑣/𝑥]𝑁1 ⊑ [𝑣 ′/𝑥]𝑁2 holds.
If Γ(𝑥) is a function type, then BT( [𝑣/𝑥]Γ) = BT(Γ) and Φ( [𝑣/𝑥]Γ) = Φ(Γ) holds. Also, 𝑥 does
not appear in 𝜑, 𝜑1 or 𝜑2. Therefore, ∀BT( [𝑣/𝑥]Γ).Φ( [𝑣/𝑥]Γ) ∧ [𝑣/𝑥]𝜑 ∧ [𝑣/𝑥]𝜑1 ⇒ [𝑣/𝑥]𝜑2
holds.
If Γ(𝑥) is a base type, 𝑣 = 𝑣 ′ holds from 𝑣 ⊑ 𝑣 ′ since 𝑣 must be a constant. Also, BT( [𝑣/𝑥]Γ) =
BT(Γ) \{𝑥 } and Φ( [𝑣/𝑥]Γ) = [𝑣/𝑥]Φ(Γ) holds. Therefore, ∀BT( [𝑣/𝑥]Γ).Φ( [𝑣/𝑥]Γ) ∧ [𝑣/𝑥]𝜑 ∧
[𝑣/𝑥]𝜑1 → [𝑣/𝑥]𝜑2 holds.
Therefore, in either cases,

∀BT( [𝑣/𝑥]Γ).Φ( [𝑣/𝑥]Γ) ∧ [𝑣/𝑥]𝜑 ∧ [𝑣/𝑥]𝜑1 ⇒ [𝑣 ′/𝑥]𝜑2

follows as expected.

(cid:3)

From now on, we abbreviate ∅; true ⊢ 𝑁1 ⊑ 𝑁2 as 𝑁1 ⊑ 𝑁2.

As we stated in the paper, we assume the following property for the primitive functions.

40

Hattori et al.

and 𝑁1 −→ 𝑁 ′

Assumption 3. If ev (𝑐, 𝑣2) and ev(𝑐, 𝑣1) are both deﬁned, then 𝑣1 ⊑ 𝑣2 implies ev (𝑐, 𝑣1) ⊑ ev (𝑐, 𝑣2).
Lemma 17. (Proposition 2 in paper) Suppose ⊢ 𝑁1 : 𝜏 and ⊢ 𝑁2 : 𝜏 ′ for some 𝜏 and 𝜏 ′. Then, 𝑁1 ⊑ 𝑁2

1 ⊑ 𝑁 ′
2.

1 imply 𝑁2 −→ 𝑁 ′

2 and 𝑁 ′
Proof. By induction on 𝑁1 −→ 𝑁 ′
1.
• Case assert(true); 𝑁3 −→ 𝑁3. From the inversion of 𝑁1 ⊑ 𝑁2, it must be that 𝑁2 ≡
assert(true); 𝑁4 for some 𝑁4 where 𝑁3 ⊑ 𝑁4. Therefore, assert(true); 𝑁4 −→ 𝑁4 holds.
• Case (𝜆𝑥𝜏1 .𝑁3)𝑣1 −→ [𝑣1/𝑥]𝑁3. From the inversion of 𝑁1 ⊑ 𝑁2, it must be that 𝑁2 ≡
(𝜆𝑥𝜏2 .𝑁4) 𝑣2 for some 𝜏2, 𝑁4 and 𝑣2 where 𝜏1 ⊑ 𝜏2, 𝑣1 ⊑ 𝑣2 and 𝑥 : 𝜏1; true ⊢ 𝑁3 ⊑ 𝑁4.
Also, from the inversion of ⊢ (𝜆𝑥𝜏1 .𝑁3) 𝑣1 : 𝜏, we obtain ⊢ 𝑣1 : 𝜏1. Therefore, by using
Lemma 16, [𝑣1/𝑥]𝑁3 ⊑ [𝑣2/𝑥]𝑁4 holds as expected.

• Case (fix(𝑓 𝜏1, 𝑥, 𝑁1))𝑣1 −→ [𝑣1/𝑥, fix(𝑓 𝜏1, 𝑥, 𝑁1)/𝑓 ]𝑁1. From the inversion of 𝑁1 ⊑ 𝑁2, it
must be that 𝑁2 ≡ (fix(𝑓 𝜏2, 𝑥, 𝑁2))𝑣2 where fix(𝑓 𝜏1, 𝑥, 𝑁1) ⊑ fix(𝑓 𝜏2, 𝑥, 𝑁2) and 𝑣1 ⊑ 𝑣2.
By using Lemma 16, we obtain the following as expected.

[𝑣1/𝑥, fix(𝑓 𝜏1, 𝑥, 𝑁1)/𝑓 ]𝑁1 ⊑ [𝑣2/𝑥, fix(𝑓 𝜏2, 𝑥, 𝑁2)/𝑓 ]𝑁2

• Case 𝑐 𝑣1 −→ ev (𝑐, 𝑣1). From the inversion of 𝑁1 ⊑ 𝑁2, it must be that 𝑁2 ≡ 𝑐 𝑣2 for some 𝑣2
where 𝑣1 ⊑ 𝑣2 holds. Since ⊢ 𝑐 𝑣2 : 𝜏 ′ holds, ev(𝑐, 𝑣2) is also deﬁned. Using the property of
ev (·, ·), we obtain ev (𝑐, 𝑣1) ⊑ ev (𝑐, 𝑣2) as expected.

• Case (let 𝑥𝜏1 = 𝑣1 in 𝑁3) −→ [𝑣1/𝑥]𝑁3. From the inversion of 𝑁1 ⊑ 𝑁2, it must be that
𝑁2 ≡ let 𝑥𝜏2 = 𝑣2 in 𝑁4 for some 𝜏2, 𝑣2 and 𝑁4 where 𝑣1 ⊑ 𝑣2 and 𝑥 : 𝜏1; true ⊢ 𝑁3 ⊑ 𝑁4
holds.
Also, we obtain ⊢ 𝑣1 : 𝜏3 and 𝑥 : 𝜏3; true ⊢ 𝑁3 ⊑ 𝑁4 for some 𝜏3 from the inversion of
⊢ (let 𝑥𝜏1 = 𝑣 in 𝑁3) : 𝜏. Using Lemma 16, [𝑣1/𝑥]𝑁3 ⊑ [𝑣2/𝑥]𝑁4 holds as expected.

• Case if true then 𝑁3 else 𝑁4 −→ 𝑁3. From the inversion of 𝑁1 ⊑ 𝑁2, it must be that
𝑁2 ≡ if true then 𝑁5 else 𝑁6 for some 𝑁5 and 𝑁6 where 𝑁3 ⊑ 𝑁5 and 𝑁4 ⊑ 𝑁6 holds.

• Case if false then 𝑁3 else 𝑁4 −→ 𝑁3 is similar to the previous case.
• Case let 𝑥𝜏1 = 𝑁3 in 𝑁4 −→ let 𝑥𝜏1 = 𝑁 ′

3. From the inversion of
𝑁1 ⊑ 𝑁2, it must be that 𝑁2 ≡ (let 𝑥𝜏2 = 𝑁5 in 𝑁6) where 𝑁3 ⊑ 𝑁5 and 𝑥 : 𝜏1; 𝜑 ⊢ 𝑁4 ⊑ 𝑁6.
From the induction hypothesis, 𝑁5 −→ 𝑁 ′
5. Therefore,
let 𝑥𝜏2 = 𝑁5 in 𝑁6 −→ let 𝑥𝜏2 = 𝑁 ′
5 in 𝑁6
holds as expected.

5 and 𝑁 ′
5 in 𝑁6 and let 𝑥𝜏1 = 𝑁 ′

3 in 𝑁4 where 𝑁3 −→ 𝑁 ′

3 in 𝑁4 ⊑ let 𝑥𝜏2 = 𝑁 ′

5 holds for some 𝑁 ′

3 ⊑ 𝑁 ′

• Other cases are trivial from the induction hypothesis.

(cid:3)

Lemma 18. (Proposition 3 in paper) Suppose ⊢ 𝑁1 : 𝜏 and ⊢ 𝑁2 : 𝜏 ′ for some 𝜏 and 𝜏 ′. Then, 𝑁1 ⊑ 𝑁2

and 𝑁2 −→ 𝑁 ′

2 imply one of the following.
1 ⊑ 𝑁 ′
1 and 𝑁 ′
2

• 𝑁1 −→ 𝑁 ′
• 𝑁1 −→ error.
Proof. By induction on 𝑁2 −→ 𝑁 ′
2.
• Case assert(true); 𝑁4 −→ 𝑁4. From the inversion of 𝑁1 ⊑ 𝑁2, it must be that 𝑁1 ≡
(assert(𝜑); 𝑁3) for some 𝜑 and 𝑁3 where 𝜑 ⇒ true and 𝑁3 ⊑ 𝑁4. Since 𝜑 is a closed pred-
icate, 𝜑 ≡ true or 𝜑 ≡ false holds. If 𝜑 ≡ true, then assert(𝜑); 𝑁3 −→ 𝑁3 and 𝑁3 ⊑ 𝑁4
holds as expected. Otherwise, assert(𝜑); 𝑁3 −→ error as expected.

• Case (𝜆𝑥𝜏2 .𝑁4) 𝑣2 −→ [𝑣2/𝑥]𝑁4. From the inversion of 𝑁1 ⊑ 𝑁2, it must be that 𝑁1 ≡
(𝜆𝑥𝜏1 .𝑁3 𝑣1) −→ [𝑣1/𝑥]𝑁3 for some 𝜏1, 𝑣1 and 𝑁3 where 𝜏1 ⊑ 𝜏2, 𝑣1 ⊑ 𝑣2 and 𝑥 : 𝜏1; true ⊢
𝑁3 ⊑ 𝑁4. By inverting ⊢ 𝑁1 : 𝜏, we obtain ⊢ 𝑣1 : 𝜏1. Therefore, using Lemma 16, [𝑣1/𝑥]𝑁3 ⊑
[𝑣2/𝑥]𝑁4 holds as expected.

Gradual Tensor Shape Checking

41

• Case (fix(𝑓 𝜏2, 𝑥, 𝑁4)) 𝑣2 −→ [𝑣2/𝑥, fix(𝑓 𝜏2, 𝑥, 𝑁4)/𝑓 ]𝑁4 is immediate using Lemma 16.
• Case 𝑐 𝑣2 −→ ev (𝑐, 𝑣2). From the inversion of 𝑁1 ⊑ 𝑁2, it must be that 𝑁1 ≡ 𝑐 𝑣1 for some
𝑣1 where 𝑣1 ⊑ 𝑣2 holds. Since ⊢ 𝑐 𝑣1 : 𝜏 holds, ev(𝑐, 𝑣1) is also deﬁned. Using the property of
ev (·, ·), we obtain ev (𝑐, 𝑣1) ⊑ ev (𝑐, 𝑣2) as expected.

• Case let 𝑥𝜏2 = 𝑣2 in 𝑁4 −→ [𝑣2/𝑥]𝑁4. From the inversion of 𝑁1 ⊑ 𝑁2, it must be that 𝑁1 ≡
let 𝑥𝜏1 = 𝑣1 in 𝑁3 for some 𝜏1, 𝑣1 and 𝑁3 where 𝜏1 ⊑ 𝜏2, 𝑣1 ⊑ 𝑣2 and 𝑥 : 𝜏1; true ⊢ 𝑁3 ⊑ 𝑁4.
Also, ⊢ 𝑣1 : 𝜏3 and 𝑥 : 𝜏3; true ⊢ 𝑁3 ⊑ 𝑁4 holds for some 𝜏3 from the inversion of ⊢ 𝑁1 : 𝜏.
Therefore, from Lemma 16, we obtain [𝑣1/𝑥]𝑁3 ⊑ [𝑣2/𝑥]𝑁4 as expected.

• Other cases are trivial.

(cid:3)

Lemma 19. Γ ⊑ Γ′, dom(Γ) ⊢ 𝑀 ⊑ 𝑀 ′ and Γ; 𝜑 ⊢ 𝑀 { 𝑁 : 𝜏 imply Γ′; 𝜑 ⊢ 𝑀 ′ { 𝑁 ′ : 𝜏 ′,

Γ; 𝜑 ⊢ 𝑁 ⊑ 𝑁 ′ and dom(Γ) ⊢ 𝜏 ⊑ 𝜏 ′ for some 𝑁 ′ and 𝜏 ′.

Proof. By induction on the derivation of Γ; 𝜑 ⊢ 𝑀 ⊑ 𝑀 ′.
• Case (PM-Lam).

dom(Γ) ⊢ 𝜏1 ⊑ 𝜏 ′
1

dom(Γ), 𝑥 ⊢ 𝑀1 ⊑ 𝑀 ′
1

dom(Γ) ⊢ 𝜆𝑥:𝜏1.𝑀1 ⊑ 𝜆𝑥:𝜏 ′

1.𝑀 ′
1

The derivation of Γ; 𝜑 ⊢ 𝜆𝑥:𝜏1.𝑀1 { 𝑁 : 𝜏 must be of the following form.

Γ, 𝑥 : 𝜏1; 𝜑 ⊢ 𝑀1 { 𝑁1 : 𝜏2
Γ; 𝜑 ⊢ 𝜆𝑥:𝜏1.𝑀1 { 𝜆𝑥:𝜏1.𝑁1 : 𝑥:𝜏1 → 𝜏2

Let Γ′ such that Γ ⊑ Γ′. Since Γ, 𝑥 : 𝜏1 ⊑ Γ′, 𝑥 : 𝜏 ′
hypothesis to obtain Γ′, 𝑥 : 𝜏 ′
1 : 𝜏 ′
𝜏 ′
1.𝑀 ′
2 for some 𝑁 ′
1
We conclude by noting dom(Γ) ⊢ 𝑥:𝜏1 → 𝜏2 ⊑ 𝑥:𝜏 ′

{ 𝑁 ′
1; 𝜑 ⊢ 𝑀 ′
1
2. Therefore, Γ′; 𝜑 ⊢ 𝜆𝑥:𝜏 ′

1 and 𝜏 ′

{ 𝜆𝑥:𝜏 ′
1 → 𝜏 ′
2.

2, Γ, 𝑥 : 𝜏1; 𝜑 ⊢ 𝑁1 ⊑ 𝑁 ′
1.𝑁 ′

1 holds, we can apply the induction
1 and dom(Γ), 𝑥 ⊢ 𝜏2 ⊑
2 holds as expected.

1 : 𝑥:𝜏 ′

1 → 𝜏 ′

• Case (PM-App).

dom(Γ) ⊢ 𝑀1 ⊑ 𝑀 ′
1
dom(Γ) ⊢ 𝑀1 𝑥 ⊑ 𝑀 ′
1 𝑥

The derivation of Γ; 𝜑 ⊢ 𝑀1 𝑥 { 𝑁 : 𝜏 must be of the following form.

Γ; 𝜑 ⊢ 𝑀1 { 𝑁1 : 𝑦:𝜏1 → 𝜏2

Γ(𝑥) = 𝜏3

Γ; 𝜑 ⊢ 𝜏3 . 𝜏1 { 𝑁2

Γ; 𝜑 ⊢ 𝑀1 𝑥 { (let 𝑦𝜏1 = 𝑁2 𝑥 in 𝑁1 𝑦) : [𝑥/𝑦]𝜏2

Let Γ′ such that Γ ⊑ Γ′. From the induction hypothesis, Γ′; 𝜑 ⊢ 𝑀 ′
1 → 𝜏 ′
1 : 𝑦:𝜏 ′
2,
1
Γ; 𝜑 ⊢ 𝑁1 ⊑ 𝑁 ′
1 and dom(Γ) ⊢ 𝑦:𝜏1 → 𝜏2 ⊑ 𝜏 ′ holds for some 𝑁 ′
1 and 𝜏 ′. By inverting
dom(Γ) ⊢ 𝑦:𝜏1 → 𝜏2 ⊑ 𝜏 ′, it must be that 𝜏 ′ ≡ 𝑦:𝜏 ′
2 where dom(Γ) ⊢
1 and 𝜏 ′
2. Also, Γ′(𝑥) = 𝜏 ′
𝜏1 ⊑ 𝜏 ′
3 ⊑ 𝜏3 holds for some 𝜏 ′
3
from Γ ⊑ Γ′.
From Lemma 3 and Lemma 4, we obtain Γ′; 𝜑 ⊢ 𝜏 ′
𝑁2 ⊑ 𝑁 ′

1 → 𝜏 ′
2 for some 𝜏 ′
3 and dom(Γ) ⊢ 𝜏 ′

1 and dom(Γ), 𝑦 ⊢ 𝜏2 ⊑ 𝜏 ′

2 where Γ; 𝜑 ⊢

2 for some 𝑁 ′

3 . 𝜏 ′
1

2 holds. Therefore,

{ 𝑁 ′

{ 𝑁 ′

Γ′; 𝜑 ⊢ 𝑀 ′

1 𝑥 { (let 𝑦𝜏′

1 = 𝑁 ′

2 𝑥 in 𝑁 ′

1 𝑦) : [𝑥/𝑦]𝜏 ′
2

42

Hattori et al.

follows. We conclude by noting dom(Γ) ⊢ [𝑥/𝑦]𝜏2 ⊑ [𝑥/𝑦]𝜏 ′
𝜏1 and 𝜏3 are the same, and the following.

2 since the underlying types for

Γ; 𝜑 ⊢ (let 𝑦𝜏1 = 𝑁2 𝑥 in 𝑁1 𝑦) ⊑ (let 𝑦𝜏′

1 = 𝑁 ′

2 𝑥 in 𝑁 ′

1 𝑦)

• Case (PM-Let).

dom(Γ) ⊢ 𝑀1 ⊑ 𝑀 ′
1

dom(Γ) ⊢ (let 𝑥𝜏1 = 𝑀1 in 𝑀2) ⊑ (let 𝑥𝜏′

dom(Γ), 𝑥 ⊢ 𝑀2 ⊑ 𝑀 ′
2
1 in 𝑀 ′
2)

1 = 𝑀 ′

The derivation of Γ; 𝜑 ⊢ let 𝑥𝜏1 = 𝑀1 in 𝑀2 { 𝑁 : 𝜏 must be f the following form.

Γ; 𝜑 ⊢ 𝑀1 { 𝑁1 : 𝜏1

Γ, 𝑥 : 𝜏1 ⊢ 𝜏2 . 𝜏 { 𝑁3

Γ, 𝑥 : 𝜏1 ⊢ 𝑀2 { 𝑁2 : 𝜏2
BT(Γ) ⊢wf 𝜏

Γ; 𝜑 ⊢ (let 𝑥𝜏1 = 𝑀1 in 𝑀2) { (let 𝑥𝜏1 = 𝑁1 in let 𝑦𝜏2 = 𝑁2 in 𝑁3 𝑦) : 𝜏

1, 𝑁 ′

{ 𝑁 ′

2 : 𝜏 ′

2 for some 𝑁 ′

Let Γ′ such that Γ ⊑ Γ′. From the induction hypothesis, Γ′ ⊢ 𝑀 ′
1
2, 𝜏 ′
𝑀 ′
2
1 and dom(Γ), 𝑥 ⊢ 𝜏2 ⊑ 𝜏 ′
dom(Γ) ⊢ 𝜏1 ⊑ 𝜏 ′
2 . 𝜏 { 𝑁 ′
Γ′, 𝑥 : 𝜏−1′; 𝜑 ⊢ 𝜏 ′
3 for some 𝑁 ′
Γ, 𝑥 : 𝜏1 ⊑ Γ′, 𝑥 : 𝜏 ′
1. Therefore,
Γ; 𝜑 ⊢ (let 𝑥𝜏′
1 = 𝑀 ′

2 where Γ; 𝜑 ⊢ 𝑁1 ⊑ 𝑁 ′

{ 𝑁 ′
1 and Γ′, 𝑥 : 𝜏 ′
1; 𝜑 ⊢
1, Γ, 𝑥 : 𝜏1; 𝜑 ⊢ 𝑁2 ⊑ 𝑁 ′
2,
2. From Lemma 13 and Lemma 4, we obtain
3 where Γ, 𝑥 : 𝜏1; 𝜑 ⊢ 𝑁3 ⊑ 𝑁 ′
3 holds, since

1 and 𝜏 ′

2 = 𝑁 ′

1 = 𝑁 ′

1 : 𝜏 ′

2) { (let 𝑥𝜏′

1 in let 𝑦𝜏′

1 in 𝑀 ′

2 in 𝑁 ′

3 𝑦) : 𝜏

holds as expected. We conclude by noting
Γ; 𝜑 ⊢ (let 𝑥𝜏1 = 𝑁1 in let 𝑦𝜏2 = 𝑁2 in 𝑁3 𝑦) ⊑ (let 𝑥𝜏′

1 = 𝑁 ′

1 in let 𝑦𝜏′

2 = 𝑁 ′

2 in 𝑁 ′

3 𝑦)

• Case (PM-Fix).

dom(Γ), 𝑓 , 𝑥 ⊢ 𝑀1 ⊑ 𝑀 ′
1
dom(Γ) ⊢ fix(𝑓 : (𝑥:𝜏1 → 𝜏2), 𝑥, 𝑀1) ⊑ fix(𝑓 : (𝑥:𝜏 ′

dom(Γ) ⊢ 𝑥:𝜏1 → 𝜏2 ⊑ 𝑥:𝜏 ′
1 → 𝜏 ′

1 → 𝜏 ′
2
2), 𝑥, 𝑀 ′
1)

The derivation of Γ; 𝜑 ⊢ fix(𝑓 : (𝑥:𝜏1 → 𝜏2), 𝑥, 𝑀1) { 𝑁 : 𝜏 must be of the following form.
(Note 𝜏 ≡ 𝑥:𝜏1 → 𝜏2)

Γ1 = Γ, 𝑓 : (𝑥:𝜏1 → 𝜏2), 𝑥 : 𝜏1

Γ1; 𝜑 ⊢ 𝑀1 { 𝑁1 : 𝜏3

Γ1; 𝜑 ⊢ 𝜏3 . 𝜏2 { 𝑁2

Γ; 𝜑 ⊢ fix(𝑓 : (𝑥:𝜏1 → 𝜏2), 𝑥, 𝑀1) { fix(𝑓 𝑥:𝜏1→𝜏2, 𝑥, let 𝑦𝜏3 = 𝑁1 in 𝑁2 𝑦) : 𝑥:𝜏1 → 𝜏2

= Γ, 𝑓 : (𝑥:𝜏 ′
{ 𝑁 ′
1 : 𝜏 ′

Let Γ′ such that Γ ⊑ Γ′, and Γ′
1
induction hypothesis, Γ′
1 ; 𝜑 ⊢ 𝑀 ′
1
and dom(Γ), 𝑓 , 𝑥 ⊢ 𝜏3 ⊑ 𝜏 ′
(PM-Fix). Using Lemma 13 and Lemma 4, Γ′
Γ; 𝜑 ⊢ 𝑁2 ⊑ 𝑁 ′
2 holds. Therefore,
2), 𝑥, 𝑀 ′
1 → 𝜏 ′

1) { fix(𝑓 𝑥:𝜏′

3. Also, dom(Γ), 𝑓 , 𝑥 ⊢ 𝜏2 ⊑ 𝜏 ′

1 ; 𝜑 ⊢ 𝜏 ′
3

. 𝜏 ′
2

1 → 𝜏 ′

2), 𝑥 : 𝜏 ′

3 holds for some 𝑁 ′

Γ′; 𝜑 ⊢ fix(𝑓 : (𝑥:𝜏 ′
holds as expected. We conclude by noting dom(Γ) ⊢ 𝑥:𝜏1 → 𝜏2 ⊑ 𝑥:𝜏 ′
2, 𝑥, let 𝑦𝜏′
Γ; 𝜑 ⊢ fix(𝑓 𝑥:𝜏1→𝜏2, 𝑥, let 𝑦𝜏3 = 𝑁1 in 𝑁2 𝑦) ⊑ fix(𝑓 𝑥:𝜏′

3 = 𝑁 ′

2, 𝑥, let 𝑦𝜏′

1→𝜏′

1→𝜏′

2 𝑦) : 𝑥:𝜏 ′
2 and
1 in 𝑁 ′

1 → 𝜏 ′
3 = 𝑁 ′

1 in 𝑁 ′

2 𝑦)

1 → 𝜏 ′
2

1. Then, Γ1 ⊑ Γ′

1 holds. From the
3 where Γ1; 𝜑 ⊢ 𝑁1 ⊑ 𝑁 ′
1 and 𝜏 ′
1
2 holds from the assumption of
{ 𝑁 ′
2 holds for some 𝑁 ′
2 where

Gradual Tensor Shape Checking

• Case (PM-If).

43

dom(Γ) ⊢ 𝑀1 ⊑ 𝑀 ′
1

dom(Γ) ⊢ 𝑀2 ⊑ 𝑀 ′
2
dom(Γ) ⊢ (if 𝑥 then 𝑀1 else 𝑀2) ⊑ (if 𝑥 then 𝑀 ′
1 else 𝑀 ′
2)

The derivation of Γ; 𝜑 ⊢ if 𝑥 then 𝑀1 else 𝑀2 : 𝜏 must be of the following form.

Γ; 𝜑 ⊢ 𝑥 : {𝑣 : bool | 𝜑 ′}

Γ; 𝜑 ∧ 𝑥 ⊢ 𝑀1 { 𝑁1 : 𝜏1
Γ; 𝜑 ∧ ¬𝑥 ⊢ 𝑀2 { 𝑁2 : 𝜏2

Γ; 𝜑 ∧ 𝑥 ⊢ 𝜏1 . 𝜏 { 𝑁3
Γ; 𝜑 ∧ ¬𝑥 ⊢ 𝜏2 . 𝜏 { 𝑁4

Γ; 𝜑 ⊢ if 𝑥 then 𝑀1 else 𝑀2
{ if 𝑥 then (let 𝑦𝜏1 = 𝑁1 in 𝑁3 𝑦) else (let 𝑦𝜏2 = 𝑁2 in 𝑁4 𝑦) : 𝜏

Let Γ′ such that Γ ⊑ Γ′. We obtain the following from the induction hypothesis.
1 : 𝜏 ′
1
2 : 𝜏 ′
2

Γ; 𝜑 ∧ 𝑥 ⊢ 𝑁1 ⊑ 𝑁 ′
1
Γ; 𝜑 ∧ ¬𝑥 ⊢ 𝑁2 ⊑ 𝑁 ′
2

{ 𝑁 ′
Γ′; 𝜑 ∧ 𝑥 ⊢ 𝑀 ′
1
{ 𝑁 ′
Γ′; 𝜑 ∧ ¬𝑥 ⊢ 𝑀 ′
2
dom(Γ′) ⊢ 𝜏1 ⊑ 𝜏 ′
1

Using Lemma 13, Γ′; 𝜑 ∧ 𝑥 ⊢ 𝜏 ′
1
and 𝑁 ′
obtain

4. From Lemma 4, Γ; 𝜑 ∧ 𝑥 ⊢ 𝑁3 ⊑ 𝑁 ′

. 𝜏 { 𝑁 ′

dom(Γ′) ⊢ 𝜏2 ⊑ 𝜏 ′
2
. 𝜏 { 𝑁 ′
3 and Γ′; 𝜑 ∧ ¬𝑥 ⊢ 𝜏 ′

3 and Γ; 𝜑 ∧ ¬𝑥 ⊢ 𝑁4 ⊑ 𝑁 ′

2

4 holds for some 𝑁 ′
3
4 holds. Therefore, we

{ if 𝑥 then (let 𝑦𝜏′
as expected. We conclude by noting

1 = 𝑁 ′

Γ′; 𝜑 ⊢ if 𝑥 then 𝑀 ′
1 in 𝑁 ′

1 else 𝑀 ′
2
3 𝑦) else (let 𝑦𝜏′

2 = 𝑁 ′

2 in 𝑁 ′

4 𝑦) : 𝜏

Γ; 𝜑 ⊢ if 𝑥 then (let 𝑦𝜏1 = 𝑁1 in 𝑁3 𝑦) else (let 𝑦𝜏2 = 𝑁2 in 𝑁4 𝑦)

⊑ if 𝑥 then (let 𝑦𝜏′

1 = 𝑁 ′

1 in 𝑁 ′

3 𝑦) else (let 𝑦𝜏′

2 = 𝑁 ′

2 in 𝑁 ′

4 𝑦)

• Case (PM-Annot).

dom(Γ) ⊢ 𝑀1 ⊑ 𝑀 ′
1
dom(Γ) ⊢ (𝑀1 : 𝜏) ⊑ (𝑀 ′

dom(Γ) ⊢ 𝜏 ⊑ 𝜏 ′
1 : 𝜏 ′)

The derivation of Γ; 𝜑 ⊢ (𝑀1 : 𝜏) { 𝑁 : 𝜏 must be of the following form.

Γ; 𝜑 ⊢ 𝜏1 . 𝜏 { 𝑁2
Γ; 𝜑 ⊢ 𝑀1 { 𝑁1 : 𝜏1
Γ; 𝜑 ⊢ (𝑀1 : 𝜏) { (let 𝑥𝜏1 = 𝑁1 in 𝑁2 𝑥) : 𝜏

1 where dom(Γ) ⊢ 𝜏1 ⊑ 𝜏 ′

Let Γ′ such that Γ ⊑ Γ′. From the induction hypothesis, Γ′; 𝜑 ⊢ 𝑀 ′
1
𝑁 ′
1 and 𝜏 ′
we have Γ′; 𝜑 ⊢ 𝜏 ′
2 for some 𝑁 ′
obtain Γ′; 𝜑 ⊢ (𝑀 ′
1 = 𝑁 ′
Γ; 𝜑 ⊢ (let 𝑥𝜏1 = 𝑁1 in 𝑁2 𝑥) ⊑ (let 𝑥𝜏′

{ 𝑁 ′
1 for some
1. Using Lemma 13 and Lemma 4,
2 holds. Therefore, we
2 𝑥) : 𝜏 ′ as expected. We conclude by noting

1 . 𝜏 ′ { 𝑁 ′
1 : 𝜏 ′) { (let 𝑥𝜏′

2 where Γ; 𝜑 ⊢ 𝑁2 ⊑ 𝑁 ′

1 and Γ; 𝜑 ⊢ 𝑁1 ⊑ 𝑁 ′

1 in 𝑁 ′
1 = 𝑁 ′

1 : 𝜏 ′

1 in 𝑁 ′

2 𝑥).

(cid:3)

