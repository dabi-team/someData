3

1
2
0
2

r
p
A
4
1

]
L
P
.
s
c
[

2
v
7
1
0
8
0
.
7
0
0
2
:
v
i
X
r
a

𝜆𝑆: Computable Semantics for Differentiable Programming
with Higher-Order Functions and Datatypes

BENJAMIN SHERMAN, MIT, USA
JESSE MICHEL, MIT, USA
MICHAEL CARBIN, MIT, USA
Deep learning is moving towards increasingly sophisticated optimization objectives that employ higher-order
functions, such as integration, continuous optimization, and root-finding. Since differentiable programming
frameworks such as PyTorch and TensorFlow do not have first-class representations of these functions, devel-
opers must reason about the semantics of such objectives and manually translate them to differentiable code.
We present a differentiable programming language, 𝜆𝑆 , that is the first to deliver a semantics for higher-order
functions, higher-order derivatives, and Lipschitz but nondifferentiable functions. Together, these features
enable 𝜆𝑆 to expose differentiable, higher-order functions for integration, optimization, and root-finding as
first-class functions with automatically computed derivatives. 𝜆𝑆 ’s semantics is computable, meaning that
values can be computed to arbitrary precision, and we implement 𝜆𝑆 as an embedded language in Haskell.

We use 𝜆𝑆 to construct novel differentiable libraries for representing probability distributions, implicit
surfaces, and generalized parametric surfaces – all as instances of higher-order datatypes – and present case
studies that rely on computing the derivatives of these higher-order functions and datatypes. In addition to
modeling existing differentiable algorithms, such as a differentiable ray tracer for implicit surfaces, without
requiring any user-level differentiation code, we demonstrate new differentiable algorithms, such as the
Hausdorff distance of generalized parametric surfaces.

CCS Concepts: • Mathematics of computing → Arbitrary-precision arithmetic; Continuous functions;
Point-set topology; • Theory of computation → Categorical semantics.

Additional Key Words and Phrases: Constructive Analysis, Diffeological Spaces, Automatic Differentiation

1 INTRODUCTION
Deep learning is centered on optimizing objectives ℓ : Θ → R over some parameter space Θ by
gradient descent, following the derivative of ℓ at some particular value 𝜃 ∈ Θ to move in a direction
that decreases ℓ (𝜃 ). Before deep-learning practitioners adopted frameworks such as TensorFlow and
PyTorch, creating a new model (i.e., parameter space and objective) was a laborious and error-prone
endeavor, since it involved manually determining and computing the derivative of the objective.
The advent of deep-learning frameworks that provide automatic differentiation (AD)—the automated
computation of derivatives of a function given just the definition of the function itself—has made
creating and modifying models much easier: a user simply writes the objective and its derivative is
computed automatically. As a result, progress in deep learning has rapidly accelerated – a testament
to the value of programming-language abstractions.

However, the creativity of deep-learning practitioners has exceeded the capabilities of current
AD frameworks: practitioners have devised objectives that current AD frameworks cannot handle
directly. A simple example is an objective including an expectation over a probability distribution
whose parameters may vary, like this:

ℓ (𝜃 ) = E𝑥∼N (𝜇 (𝜃 ),𝜎 2 (𝜃 )) [𝑓 (𝑥)].
If this ℓ is translated naïvely to PyTorch, by approximating the expectation with Monte Carlo
sampling, the automatically generated derivative will be incorrect. Numerous algorithms have been

Authors’ addresses: Benjamin Sherman, MIT, USA, sherman@csail.mit.edu; Jesse Michel, MIT, USA, jmmichel@mit.edu;
Michael Carbin, MIT, USA, mcarbin@csail.mit.edu.

 
 
 
 
 
 
3:2

Benjamin Sherman, Jesse Michel, and Michael Carbin

proposed to compute the derivatives of objectives that average over parameterized probability distri-
butions [Figurnov et al. 2018; Jang et al. 2017; Jankowiak and Obermeyer 2018; Naesseth et al. 2017].
How does one compute derivatives of objectives like these in general? No existing differentiable-
programming semantics has tackled the problem of differentiating through expectations such as
these.

Other objectives are sufficiently complex that they do not even beg an incorrect naïve implemen-

tation. Objectives ℓ that optimize over compact sets Δ

ℓ (𝜃 ) = max
𝛿 ∈Δ
arise in adversarial contexts, including adversarial training and generative adversarial networks
(GANs). Conceptually, optimizing this objective with gradient-based techniques requires a semantics
for a differentiable max operation over a compact set, which, to date, has not been covered in the
literature on the semantics of differentiable programs. Devising the appropriate derivative for these
kinds of objectives is an object of current study [Lorraine et al. 2019; Wang et al. 2020].

𝑓 (𝜃, 𝛿)

Sometimes, an objective involves root-finding,

ℓ (𝜃 ) = let 𝑥 be such that 𝑔(𝜃, 𝑥) = 0 in 𝑓 (𝜃, 𝑥).
This arises in learning implicit surfaces, with applications both to learning the decision boundaries
of classifiers as well as to reconstructing surfaces from point-cloud data or other visual data. How
to compute the derivative of objectives like this is a key contribution of several papers [Atzmon
et al. 2019; Bai et al. 2019; Niemeyer et al. 2020].

What do these objectives all have in common? They all involve higher-order functions: their
definitions introduce variables that are subject to integration, optimization, or root-finding. Not only
are these three operations troublesome in practice, but no semantics of differentiable programming
has yet addressed them.

Approach. We present 𝜆𝑆 , a differentiable programming language that includes higher-order
functions for integration, optimization, and root-finding. A key technical challenge is that these
functions are higher-order and our semantic approach must wed higher-order functions with
higher-order derivatives and nonsmooth functions to encompass these and other modern deep
learning objectives. As a toy example, consider computing the derivative 𝑓 ′(0.6) of the function

𝑓 (𝑐) ≜

∫ 1

0

ReLU(𝑥 − 𝑐) 𝑑𝑥,

where ReLU(𝑥) = max(0, 𝑥). We can compute 𝑓 ′(0.6) = −0.4 with the 𝜆𝑆 expression

eps=1e-2> deriv (𝜆 c ⇒ integral01 (𝜆 x ⇒ relu (x - c))) 0.6

[-0.407, -0.398]

where there is a type ℜ for real numbers, a function relu : ℜ → ℜ for ReLU, a higher-order
function integral01 : (ℜ → ℜ) → ℜ for integration over the unit interval [0,1], and a
higher-order function for differentiation of real-valued functions deriv : (ℜ → ℜ) → ℜ →
ℜ. The result can be queried to any precision, returning an interval guaranteed to include the true
answer. Here, the precision is specified in the prompt as eps=1e-2.

𝜆𝑆 is the first language that gives semantics to such an operation and moreover is the first to
support its computation to arbitrary precision. Note that, in order to determine this derivative, we
must evaluate the derivative of the ReLU function everywhere from -0.6 to 0.4, which includes 0,
where ReLU is not (classically) differentiable.

𝜆𝑆 : Computable Semantics for Differentiable Programming with Higher-Order Functions and Datatypes

3:3

Our work is unique compared to related work in supporting higher-order functions, higher-
order derivatives, and nondifferentiable functions. We combine the use of Clarke derivatives in
Di Gianantonio and Edalat [2013] to support nondifferentiable functions, the diffeological approach
of Vákár et al. [2018] to support higher-order functions, and the derivative towers of Elliott [2008] to
support higher-order derivatives. §9 covers related work in more detail. Merging these techniques
gives us a platform to accomplish the contributions described below.

Contributions. We present 𝜆𝑆 , a differentiable programming language whose types are (gen-
eralized) smooth spaces and whose functions are (generalized) smooth maps. Our contributions
are:

(1) The first semantics for a differentiable programming language that admits all of the following:
1) higher-order functions (§5), 2) higher-order derivatives (§4), and 3) Lipschitz but nonsmooth
functions, such as min, max, and ReLU (§4).

(2) The first semantics for differentiable integration, optimization, and root-finding (§5), enabled

by the features above.

(3) An implementation of this semantics, including implementations for higher-order functions
such as integration (§6). Our implementation is based directly on a constructive categorical
semantics that demonstrates how these constructs can be computed to arbitrary precision.
(4) New smooth libraries for constructing and computing on three higher-order datatypes:

probability distributions, implicit surfaces, and generalized parametric surfaces (§7).

𝜆𝑆 ’s semantics allows computation with and reasoning about the derivatives of higher-order
functions, such as integration, optimization, and root-finding. 𝜆𝑆 elucidates foundational princi-
ples for how to program with smooth values in a sound, arbitrarily precise manner, including
which operations are possible to compute soundly and which are not. While in many cases 𝜆𝑆 is
not practically efficient, in some cases, programs can serve as executable specifications to guide
programming in other frameworks, to validate separately developed systems, and to suggest new
functionality that could be added to other differentiable programming frameworks.

2 AN INTRODUCTION TO 𝜆𝑆
We demonstrate 𝜆𝑆 ’s core functionality by implementing a simple differentiable ray tracer, an
algorithm that generates an image of a scene as viewed by a camera by tracing how rays of
light emanate from a light source, bounce off the scene, and then enter the camera’s aperture.
Differentiable ray tracing is a new deep-learning technique that propagates derivatives through
image rendering algorithms, permitting the use of inverse graphics to solve computer-vision
tasks [Li et al. 2018; Niemeyer et al. 2020]. These techniques optimize the parameters of a scene
representation to make the image generated by the ray tracer more closely match a target image.
As a simple example, consider computing the brightness of a particular scene at a particular
direction, using the 𝜆𝑆 library for representing scenes and a function for performing ray tracing,
both of which we present in Fig. 1:

eps=1e-5> raytrace (circle (1, -3/4) 1) (1, 1) (1, 0)

[2.587289, 2.587299]

Fig. 1a depicts the computation at hand. The camera is located at the origin (0, 0), the circle
is centered at (1, -3/4) and has radius 1, the light source is at (1, 1), and we consider a ray
pointing horizontally to the right from the camera, in the direction (1, 0). The computation
returns an interval and the eps=1e-5 specifies the precision tolerance, such that the interval-valued
result, [2.587289, 2.587299], has a width at most 10−5. Our implementation guarantees that

3:4

Benjamin Sherman, Jesse Michel, and Michael Carbin

type Surface A = A →ℜ

firstRoot : (ℜ→ℜ) →ℜ ! language primitive

let dot (x y : ℜ2) : ℜ = x[0] * y[0] + x[1] * y[1]
let scale (c : ℜ) (x : ℜ2) : ℜ2 = (c * x[0], c * x[1])
let norm2 (x : ℜ2) : ℜ = x[0]2 + x[1]2
let normalize (x : ℜ2) : ℜ2 = scale (1 / sqrt (norm2 x)) x

(a) A ray of light from a source
above bounces off a circle before
hitting a camera. How does the
brightness change when the cir-
cle is moved up?

deriv : (ℜ→ℜ) → (ℜ→ℜ) ! library function
let gradient (f : ℜ2 →ℜ) (x : ℜ2) : ℜ2 =
(deriv (𝜆 z : ℜ⇒ f (z, x[1])) x[0],
deriv (𝜆 z : ℜ⇒ f (x[0], z)) x[1])

(b) Basic definitions used in raytrace below.

! camera assumed to be at the origin
let raytrace (s : Surface (ℜ2)) (lightPos : ℜ2) (rayDirection : ℜ2) : ℜ =

let tStar = firstRoot (𝜆 t : ℜ⇒ s (scale t rayDirection)) in
let y = scale tStar rayDirection in let normal : ℜ2 = - gradient s y in
let lightToSurf = y - lightPos in
max 0 (dot (normalize normal) (normalize lightToSurf))
/ (norm2 y * norm2 lightToSurf)

(c) A 𝜆𝑆 function for differentiable ray tracing of implicit surfaces.

Fig. 1. A library for differentiable ray tracing and scene representation.

whenever it returns a finite-width interval, the true, real-valued result is contained within that
interval.

𝜆𝑆 permits differentiation of any functions in the language, so we can compute how the brightness

would change if the circle were moved up by an infinitesimal amount:

eps=1e-3> deriv (𝜆 y : ℜ ⇒ raytrace (circle (0, y) 1) (1, 1) (1, 0)) (-3/4)

[1.3477, 1.3484]

The 𝜆𝑆 function deriv : (ℜ → ℜ) → (ℜ → ℜ) computes the derivative of a scalar-valued
real function. The result indicates that when the circle is moved up infinitesimally from its current
location, the brightness increases infinitesimally at a rate of ∼1.35 units brightness per unit distance
the circle is moved up.

Several changes occur when the circle is moved up that affect the image brightness. The point at
which the light ray bounces off the circle moves closer to the camera, decreasing the distance from
the camera to the circle (increasing brightness) but increasing the distance from the light to the
camera (decreasing brightness). Both the direction of the surface normal of the circle at the point
where the light deflects and the direction from the light source to that point change, increasing the
angle between the surface normal of the circle and the light ray (decreasing brightness). Automatic
differentiation automatically takes all of these effects into account.

Figure 1c shows the implementation of the differentiable ray tracing in 𝜆𝑆 . The function firstRoot
: (ℜ → ℜ) → ℜ in the definition of raytrace computes the distance that the light travels
from the scene to the camera. Given a function f : ℜ → ℜ, firstRoot f performs root finding,

𝜆𝑆 : Computable Semantics for Differentiable Programming with Higher-Order Functions and Datatypes

3:5

computing min{𝑥 ∈ [0, 1] | f(𝑥) = 0}. 𝜆𝑆 ’s higher-order functions for root finding are novel,
and accordingly, 𝜆𝑆 ’s ability to express differentiable ray tracing of implicit surfaces (embodied in
raytrace) without needing any custom code for specifying derivatives.

The differentiable ray tracer raytrace critically depends on 𝜆𝑆 ’s unique support for higher-order
functions, higher-order derivatives, and Lipschitz but nondifferentiable functions such as min, max,
and ReLU. We now provide a brief introduction to these three features.

2.1 Higher-Order Functions
The raytrace function must compute the distance the ray of light travels from the scene to the
camera, represented by the let-definition tStar in raytrace. When applied to the scene circle
(1, y) 1, the definition reduces to

let tStar y = firstRoot (𝜆 t : ℜ⇒ 1 - y 2 - (t - 1) 2)

The function firstRoot : (ℜ → ℜ) → ℜ is a higher-order function since it takes a
function as input. In order to admit a function like this in a differentiable programming language,
the language must be able to compute how the result of firstRoot changes when there is an
infinitesimal perturbation to its input function. In this example, we want to know how tStar
changes when y changes. To answer this, define f t y = 1 - y 2 - (t - 1) 2. Then tStar finds a
solution for the variable t to the equation f t y = 0. So whatever change is induced by changing
y must be counterbalanced by changing tStar. 𝜆𝑆 ’s semantics validate the equation (for values of
y giving well-defined roots)

deriv tStar y = - deriv (𝜆 y0 : ℜ⇒ f (tStar y) y0) y /

deriv (𝜆 t : ℜ⇒ f t y) (tStar y)

This equation for the derivative of root finding is known as the implicit function theorem. By the

rules of calculus, we can further simplify this to

deriv tStar y = - y / (tStar y - 1).

Note that the semantics of 𝜆𝑆 ensures that these equations are indeed program equivalences: one
can substitute one expression for the other within the context of a larger expression without
affecting its meaning. Indeed, taking y = -3/4, and evaluating both sides of the expression above in
𝜆𝑆 produces compatible answers, roughly −1.1, which indicates that moving the circle up decreases
the distance that the light travels from the circle to the camera.

We implement the firstRoot function as a language primitive by specifying not only how
firstRoot acts on values but also how derivatives propagate through it, via the implicit function
theorem (see §5.2 for more detail).

2.2 Higher-Order Derivatives
The brightness of the image computed by the raytrace function depends on the angle at which
the ray of light deflects as it bounces off the circle, so we need to know which direction the circle
faces where the light hits it, which is known as the surface normal. In the code for raytrace, the
surface normal is computed as

let normal : ℜ2 = - gradient s y

3:6

Benjamin Sherman, Jesse Michel, and Michael Carbin

Consider, for instance, the unit circle centered at (0, 0), i.e., circle (0, 0) 1, given by the

function 𝑓 (𝑥, 𝑦) = 1 − 𝑥 2 − 𝑦2. The surface normal is given by the negative gradient,

−∇𝑓 (𝑥, 𝑦) = −

(cid:19)

(cid:18) 𝜕𝑓
𝜕𝑥

,

𝜕𝑓
𝜕𝑦

= (2𝑥, 2𝑦)

√

√

2) on the upper-right of the circle has a surface normal that

So, for instance, the point (1/
points up and to the right, in the direction (2/

2, 1/

√

2, 2/
Note that, in the raytrace code itself, this gradient computation requires the computation of
derivatives of the implicitly defined surface in order to compute the image brightness. Accordingly,
computing the derivative of the image brightness with respect to an infinitesimal perturbation in
the scene requires computing the second derivatives of the implicitly defined surface with respect
to its arguments. Thus, higher-order differentiation is a valuable language feature.

2).

√

In 𝜆𝑆 , differentiation is a first-class programming construct, so higher-order differentiation is
naturally supported, as we can compute higher-order derivatives by applying the deriv : (ℜ →
ℜ) → ℜ → ℜ function multiple times. Note that some approaches to differentiable programming
do not support higher-order differentiation (see Table 1) and thus do not have differentiation as a first-
class construct. Higher-order derivatives are also used for numerical integration, in optimization
algorithms, and in other contexts.

The requirement to support higher-order derivatives means that language primitives, such as
firstRoot, must specify not only how they act on values but also how derivatives of all orders
propagate through them.

2.3 Nondifferentiability
Note that the raytrace code uses the built-in function max : ℜ → ℜ → ℜ in computing the
image brightness. If the light source is behind the scene, the dot product of the surface normal and
the vector from the light to the surface will be negative, but the brightness should be 0, rather than
this negative value. Hence, we clamp the value to be at least zero by applying max 0. Note that this
function is exactly the rectified linear unit (ReLU) that is common in deep learning:

let relu (x : ℜ) : ℜ = max 0 x

ReLU is not differentiable at 0. When we compute its derivative at 0 in 𝜆𝑆 , we get a nonmaximal

result. That means that, for sufficiently fine (≤ 1) precision tolerances, we get nontermination:

eps=1e-1> deriv relu 0

eps=2> deriv relu 0

(nontermination)

[0.0, 1.0]

The interval approximations never converge to intervals smaller than [0, 1]. The type ℜ contains,
in addition to the real numbers, nonmaximal elements such as this one, which we name [0, 1], e.g.,
we find that ReLU′(0) = [0, 1].

Differentiable programming frameworks such as PyTorch admit min and max operations, but
they are unsound, in the sense that one can define 𝑓 (𝑥) = max(𝑥, 0) + min(0, 𝑥), which is the
identity function, but compute in PyTorch that 𝑓 ′(0) = 2, whereas it should be 𝑓 ′(0) = 1. Because
of this issue, most differentiable programming semantics leave the derivative of max undefined at 0.
However, 𝜆𝑆 ’s interval-valued semantics for functions like max enables productive computational
functionality that the partiality approach would not permit. For instance, suppose rather than
having a point light source for ray-tracing, we instead have a line light source, so we integrate

𝜆𝑆 : Computable Semantics for Differentiable Programming with Higher-Order Functions and Datatypes

3:7

over the entire line, using the primitive higher-order function integral01 : (ℜ → ℜ) →
ℜ, where for f : ℜ → ℜ, integral01 f computes the integral of f over the unit interval,
∫ 1
0 f(𝑥) 𝑑𝑥. For simplicity, consider a camera located at (0, 1) pointing downwards at a flat surface
that stretches from (−1, 𝑦) to (1, 𝑦), with a light source stretching from (1, 0) to (1, 1). Furthermore,
let us disregard the effect of brightness decreasing when the light travels longer distances, such
that the brightness is

let brightness (y : ℜ) : ℜ =

integral01 (𝜆 y0 : ℜ⇒ max 0 ((y0 - y) / sqrt (1 + (y0 - y) 2)))

When 0 ≤ y ≤ 1, the integrand will be nondifferentiable with respect to y at the point where y0

= y. For instance, taking y0 = y = 1/2, we find that the derivative of the integrand is
deriv (𝜆 y : ℜ ⇒ max 0 ((1/2 - y) / sqrt (1 + (1/2 - y) 2)))) (1/2) = [-1, 0].
When y0 is just greater than y, the derivative will be near −1, but when y0 is just less than y, the
derivative will be near 0. Because the derivative at this point is a bounded interval, rather than a
completely undefined result, it ends up being soundly neglected when it is integrated over:

eps=1e-3> deriv brightness (1/2)

[-0.4476, -0.4469]

The expression deriv brightness (1/2) is indeed maximal, meaning that it can be evaluated
to arbitrary precision. Were the derivative of the integrand to be undefined rather than interval-
valued, deriv brightness (1/2) would necessarily need to be undefined as well, but with these
semantics, we can soundly compute the correct derivative.

This generalized notion of derivative that works for ReLU is based on Clarke’s generalized
derivative [Clarke 1990]. The basic idea can be motivated by the desire for continuity and robustness
in the numerical computation. The derivative of ReLU is 1 for numbers imperceptibly greater than
0, and the derivative is 0 for numbers imperceptibly smaller than 0, so the derivative of ReLU at 0
should be consistent with those nearby answers. The specialization relation ⊑ on ℜ formalizes this
notion of compatible behavior, where we have [0, 1] ⊑ 0 and [0, 1] ⊑ 1. We will prove a consistency
theorem for our language (Proposition 5.4) that says that derivatives are always compatible, i.e.,
related by ⊑, with the infinitesimal rates of change indicated by its value-level operation.

3 SYNTAX AND SEMANTICS OF 𝜆𝑆

0, 1, 2, ... : ℜ
(+), (-), (*), (/) : ℜ→ℜ→ℜ
max : ℜ→ℜ→ℜ
sin, exp : ℜ→ℜ

integral01 : (ℜ→ℜ) →ℜ
cutRoot : (ℜ→ℜ) →ℜ
firstRoot : (ℜ→ℜ) →ℜ
max01 : (ℜ→ℜ) →ℜ
argmax01 : (ℜ→ℜ) →ℜ

tangent A B : (A → B) → Tan A → Tan B
tangentValue A : Tan A → A

record ((cid:27)) A B = { to : A → B,

from : B → A }

tangent_R : Tan ℜ (cid:27) ℜ * ℜ
tangentProd A B : Tan (A * B) (cid:27) Tan A * Tan B
tangentTo_R A : Tan (A →ℜ) (cid:27) (A →ℜ) * (A →ℜ)

Fig. 2. 𝜆𝑆 constants and their types.

𝜆𝑆 is the simply-typed lambda calculus with the constants shown in Fig. 2. These include basic
operators, such as arithmetic and trigonometric operators, higher-order operators, and primitives

3:8

Benjamin Sherman, Jesse Michel, and Michael Carbin

to compute derivatives. The syntax permits polymorphic type signatures, but semantically we treat
polymorphism at the metatheoretic level.

Higher-Order Operators. The function integral01 gives the Riemannian integral of a function
on the interval [0, 1]. max01 maximizes a function over the interval [0, 1], and argmax01 finds its
maximizing argument. cutRoot finds the root of a function f : ℜ → ℜ, assuming that it has a
single root and is negative for smaller values and positive for larger values. firstRoot, on input f
: ℜ → ℜ, finds the first root of f on a region starting at 0.

Derivatives. tangent is a first-class function that computes derivatives, where the type function
Tan gives the space of tangent bundles over a space; conceptually, a space of pairs of values
and derivatives. The function tangentValue projects the value part of this tangent bundle. The
isomorphisms of tangent bundles – i.e., tangent_R, tangentProd, and tangentTo_R – assist with
manipulating the information that corresponds to the derivative part of the tangent bundle when
it is possible for certain spaces. To concretize the concept behind these isomorphisms, we now
present the implementation of deriv from Fig. 1, which uses tangent and these isomorphisms:

let deriv (f : ℜ→ℜ) (x : ℜ) : ℜ =

snd (tangent_R.to (tangent f (tangent_R.from (x, 1))))

This implementation calls tangent with 𝑓 and a query for the derivative of 𝑓 at 𝑥 in the direction
1. The query is a tangent bundle constructed with the isomorphism tangent_R from the pair (𝑥, 1).
deriv then projects out the derivative part of tangent’s result, using tangent_R in the opposite
direction and the standard second projection on binary products.

Semantics. Over the next sections, we develop the full syntax and semantics of 𝜆𝑆 . In §4, we
describe a first-order (i.e., no higher-order functions) differentiable language that supports Clarke
semantics and higher-order derivatives, by defining a Cartesian monoidal category AD. In §5, we
will define semantics for the higher-order language 𝜆𝑆 by taking a category of presheaves, HAD,
over AD. We defer computability concerns to §6.

4 SEMANTICS OF A FIRST-ORDER DIFFERENTIABLE LANGUAGE (AD)
In this section, we describe a first-order (i.e., no higher-order functions) differentiable language that
supports Clarke semantics and higher-order derivatives, by defining a Cartesian monoidal category
AD. Fig. 3 presents the syntax and typing rules for the language for AD. The ∗ type represents
the unit type, having a single value ! in it. Given any object 𝐾 ∈ AD, the type expression ⌊𝐾⌋
represents the type whose semantics is 𝐾. Given any arrow 𝑓 : ⟦𝜏1⟧ (cid:123) ⟦𝜏2⟧ of AD and given some
expression Γ ⊢ 𝑒 : 𝜏1 the syntax ⌊𝑓 ⌋ (𝑒) applies the map 𝑓 to the result of 𝑒. When the constants
are binary operators like + and ×, we permit syntactic sugar to write them infix, such that, e.g.,
𝑒1 + 𝑒2 is shorthand for +(𝑒1, 𝑒2). The syntax 𝜕𝑒𝑦
𝜕𝑥 |𝑥=𝑒𝑥 ·𝑒𝑑𝑥 computes the directional derivative of
𝑒𝑦 with respect to 𝑥 at 𝑥 = 𝑒𝑥 in the direction of infinitesimal perturbation 𝑒𝑑𝑥 .

Fig. 4 presents the semantics for the language for AD, which we explain in this section. Our
semantics of derivatives is phrased in terms of Clarke’s generalized derivative [Clarke 1990], which
enables capturing differentiable properties of locally Lipschitz but nonsmooth functions such as
max, min, and ReLU. We will now present the background material we use to define the semantics
of the language for AD.

𝜆𝑆 : Computable Semantics for Differentiable Programming with Higher-Order Functions and Datatypes

3:9

Syntax

variables 𝑥

types 𝜏 ::= ∗ | 𝜏1 × 𝜏2
| Γ, 𝑥 : 𝜏

contexts Γ ::= ·
functions 𝑓 ∈ Arr(AD)

| !

⌊𝑓 ⌋ (𝑒)
(𝑒, 𝑒)

expressions 𝑒 ::= 𝑥 |
|
𝜕𝑒
|𝑥=𝑒 ·𝑒
𝜕𝑥
let 𝑥 ≜ 𝑒 in 𝑒

|

|

|

⌊𝐾⌋

(𝑥 : 𝜏) ∈ Γ
Γ ⊢ 𝑥 : 𝜏

Typing rules
Γ ⊢ 𝑒 : 𝜏1

𝑓 : ⟦𝜏1⟧ (cid:123) ⟦𝜏2⟧

Γ ⊢ ⌊𝑓 ⌋ (𝑒) : 𝜏2

Γ ⊢ 𝑒1 : 𝜏1

Γ ⊢ 𝑒2 : 𝜏2

Γ ⊢ ! : ∗

Γ ⊢ (𝑒1, 𝑒2) : 𝜏1 × 𝜏2

Γ ⊢ 𝑒1 : 𝜏1

Γ, 𝑥 : 𝜏1 ⊢ 𝑒2 : 𝜏2

Γ ⊢ let 𝑥 ≜ 𝑒1 in 𝑒2 : 𝜏2

Γ, 𝑥 : 𝜏1 ⊢ 𝑒𝑦 : 𝜏2

Γ ⊢ 𝑒𝑥 : 𝜏1

Γ ⊢ 𝑒𝑑𝑥 : 𝜏1

Γ ⊢

𝜕𝑒𝑦
𝜕𝑥

|𝑥=𝑒𝑥 ·𝑒𝑑𝑥 : 𝜏2

Fig. 3. Syntax and typing rules for the language for AD.

Types
𝜏 type
⟦𝜏⟧ ∈ Ob(AD)

Contexts
Γ context
⟦Γ⟧ ∈ Ob(AD)

⟦∗⟧ ≜ 1AD
⟦𝜏1 × 𝜏2⟧ ≜ ⟦𝜏1⟧ × ⟦𝜏2⟧

⟦·⟧ ≜ 1AD
⟦Γ, 𝑥 : 𝜏⟧ ≜ ⟦Γ⟧ × ⟦𝜏⟧

⟦⌊𝐾⌋⟧ ≜ 𝐾

Terms
Γ ⊢ 𝑒 : 𝜏
⟦𝑒⟧ : ⟦Γ⟧ (cid:123) ⟦𝜏⟧

⟦⌊𝑓 ⌋ (𝑒)⟧ ≜ 𝑓 ◦ ⟦𝑒⟧
⟦!⟧ ≜ !

⟦(𝑒1, 𝑒2)⟧ ≜ ⟨⟦𝑒1⟧, ⟦𝑒2⟧⟩

⟦let 𝑥 ≜ 𝑒1 in 𝑒2⟧ ≜ ⟦𝑒2⟧ ◦ ⟨id, ⟦𝑒1⟧⟩

𝜕𝑒𝑦
𝜕𝑥

⟦

|𝑥=𝑒𝑥 ·𝑒𝑑𝑥 ⟧ ≜ ⟦𝑒𝑦⟧′ ◦ ⟨⟨id, 𝑒𝑥 ⟩, ⟨0, 𝑒𝑑𝑥 ⟩⟩

Fig. 4. Semantics of the language for AD.

4.1 Preliminaries
A domain 𝐷 is a set with a partial-order structure ⊑ that supports directed joins (cid:195)𝑑 ∈𝑆 𝑑, which
are just joins of directed subsets 𝑆 ⊆ 𝐷, which are those subsets such that if 𝑥, 𝑦 ∈ 𝐷, then there
is some 𝑧 ∈ 𝐷 such that 𝑥 ⊑ 𝑧 and 𝑦 ⊑ 𝑧. We call the partial-order relation ⊑ specialization. The
relation 𝑥 ⊑ 𝑦 intuitively means that 𝑥 behaves in a way that is compatible with how 𝑦 behaves.
An element 𝑥 ∈ 𝐷 is maximal if for any 𝑦 ∈ 𝐷, if 𝑥 ⊑ 𝑦, then 𝑦 ⊑ 𝑥.

Define

R ≜ {[𝑎, 𝑏] | 𝑎, 𝑏 ∈ R, 𝑎 ≤ 𝑏} ∪ {R}
as the domain of interval reals, partially ordered (⊑) by reverse set inclusion. Its maximal elements
are the intervals of the form [𝑎, 𝑎], which we often just write as 𝑎. Arithmetic operations can
be extended from R to R (see, e.g., Edalat and Lieutier [2004]). Note that R serves as a bottom
element, and we refer to it with the symbol ⊥. For any vector space 𝑉 (over R), let C(𝑉 ) be the set
of nonempty convex sets in 𝑉 , with an order relation ⊑ also corresponding to reverse inclusion.
Note that 𝑉 serves as a bottom element, and we refer to it with the symbol ⊥. Note that we have
the sequence of embeddings R𝑛 ↩→ R𝑛 ↩→ C(R𝑛): every vector 𝑣 ∈ R𝑛 can be treated as a tuple
of singleton intervals R𝑛, and every element 𝑥 ∈ R𝑛 can be treated as a (convex) hyperrectangle,

3:10

Benjamin Sherman, Jesse Michel, and Michael Carbin

where some dimensions of the hyperrectangle may be infinite. We use the notation 𝜄R𝑛↩→R𝑛 and
𝜄 R𝑛↩→C(R𝑛) to denote these embeddings, respectively.

The Clarke Derivative. Let 𝑓 : R𝑛 → R𝑚. If 𝑓 is locally Lipschitz on 𝑋 ⊆ 𝑈 , let 𝑍 𝑓 ⊆ 𝑋 be the
points of nondifferentiability of 𝑓 . The Bouligand subdifferential of 𝑓 at 𝑥 ∈ 𝑋 is the set of matrices

𝜕𝐵 𝑓 (𝑥) ≜

(cid:26)
𝐻 : R𝑚×𝑛 |

𝐻 = lim𝑗→∞ 𝐽 𝑓 (𝑥 𝑗 ) for some sequence (𝑥 𝑗 ) 𝑗 ∈N
where 𝑥 𝑗 ∈ 𝑋 \ 𝑍 𝑓 for all 𝑗 ∈ N and lim𝑗→∞ 𝑥 𝑗 = 𝑥

(cid:27)

,

where 𝐽 is the Jacobian operator defining the derivative of a function at a point where it is
differentiable. The Clarke Jacobian of 𝑓 at 𝑥 is given by the convex hull 𝜕𝑓 (𝑥) ≜ hull(𝜕𝐵 𝑓 (𝑥)). The
Clarke Jacobian 𝜕𝑓 (𝑥) ∈ C(R𝑚×𝑛) is always compact (since 𝑓 is locally Lipschitz).

Given 𝑓

: R𝑛 → R𝑚

⊥, let 𝑈 be the largest open set on which 𝑓 is both defined and locally

Lipschitz. We can define the partial Clarke Jacobian of 𝑓 to be

𝜕⊥𝑓 (𝑥) =

(cid:40)𝜕𝑓 (𝑥)
⊥

𝑥 ∈ 𝑈
𝑥 ∉ 𝑈

such that 𝜕⊥ : (R𝑛 → R𝑚
⊥) → R𝑛 → C(R𝑚×𝑛). We can map values of C(𝐴) to 𝐴⊥ (for any 𝐴) by
mapping maximal elements {𝑥 } ∈ C(𝐴) to 𝑥 ∈ 𝐴⊥ and everything else to ⊥. Using this conversion,
we can also give the partial Clarke Jacobian the type 𝜕⊥ : (R𝑛 → C(R𝑚)) → R𝑛 → C(R𝑚×𝑛), and
thus we can also iterate the partial Clarke Jacobian construction to get higher-order derivatives
𝜕𝑘
). Note that the 𝑘 + 1th-order Clarke Jacobian is ⊥ unless the
⊥
𝑘th-order Clarke Jacobian is maximal; thus, when defined, higher-order Clarke Jacobians are just
Clarke Jacobians of conventional higher-order derivatives.

⊥) → R𝑛 → C(R𝑚×𝑛𝑘

: (R𝑛 → R𝑚

When a function is differentiable, its partial Clarke Jacobian is a maximal element. When it is
locally Lipschitz but not differentiable, the partial Clarke Jacobian is a compact convex set. When it
is not locally Lipschitz, the partial Clarke Jacobian is the entire space, corresponding to ⊥.

4.2 Smoothish Maps
We will now define AD. The objects of AD are the natural numbers, where 𝑛 ∈ N corresponds
to 𝑛-dimensional Euclidean space. To emphasize that we are thinking of Euclidean space, we
write the object 𝑛 ∈ N as R𝑛. A morphism of AD is a smoothish map: a derivative tower that is
successively consistent. A derivative tower 𝑓 between spaces R𝑛 and R𝑚, 𝑓 : R𝑛 (cid:123) R𝑚, is a collection
of continuous maps (taking the Scott topology for R)

𝑓 (𝑘) : R𝑛 × (R𝑛)𝑘 → R𝑚

for each 𝑘 ∈ N, where 𝑓 (𝑘) represents the 𝑘th-order derivative. This defines a smoothish map
as a power series, where the first R𝑛 argument is the point where the map is evaluated, and the
remaining 𝑘 arguments represent the inputs to a multilinear map representing the derivative.1
Given vectors 𝑥 ∈ R𝑛 and 𝑦 ∈ R𝑘 , let 𝑥 ⊗ 𝑦 ∈ R𝑛×𝑘 denote the tensor product. Define Mat𝑘 :
(R𝑛 × (R𝑛)𝑘 → R𝑚) → R𝑛 → R𝑚×𝑛𝑘
⊥ at a point 𝑥 ∈ R𝑛 such that Mat𝑘 (𝑓 )(𝑥) = 𝑀 if there is a
matrix 𝑀 ∈ R𝑚×𝑛𝑘 such that for all 𝑑𝑥1, . . . , 𝑑𝑥𝑘 ∈ R𝑛, we have

𝑓 (𝑥; 𝑑𝑥1, . . . , 𝑑𝑥𝑘 ) = 𝜄R𝑚↩→R𝑚 (𝑀 · (𝑑𝑥1 ⊗ . . . ⊗ 𝑑𝑥𝑘 )) ,
and Mat𝑘 (𝑓 )(𝑥) = ⊥ if there is no such matrix (𝑀). Equation 1 requires that 𝑓 is multilinear in its
𝑑𝑥1, . . . , 𝑑𝑥𝑘 arguments, which means that 𝑓 has a form that permits differentiation.

(1)

1This representation as derivative towers is largely drawn from [Elliott 2008].

𝜆𝑆 : Computable Semantics for Differentiable Programming with Higher-Order Functions and Datatypes

3:11

Definition 4.1. We define a consistency relation Cons𝑘 (𝑔, 𝑓 ) for a function 𝑔 : R𝑛 × (R𝑛)𝑘 → R𝑚

and a function 𝑓 : R𝑛 → C(R𝑚×𝑛𝑘

) to hold if for all 𝑥 ∈ R𝑛 and for all 𝑑𝑥1, . . . , 𝑑𝑥𝑘 ∈ R𝑛,

𝜄 R𝑚↩→C(R𝑚) (𝑔(𝑥; 𝑑𝑥1, . . . , 𝑑𝑥𝑘 )) ⊑ 𝑓 (𝑥) · (𝑑𝑥1 ⊗ . . . ⊗ 𝑑𝑥𝑘 ).

A derivative tower 𝑓 is successively consistent if for all 𝑘 ∈ N, we have

Cons𝑘+1(𝑓 (𝑘+1), 𝜕⊥Mat𝑘 (𝑓 (𝑘) )),
meaning that each successive derivative 𝑓 (𝑘+1) is consistent with the value-level behavior of 𝑓 (𝑘) .

A smoothish map 𝑓 is a successively consistent derivative tower. We call a smoothish map smooth
if 𝑓 (𝑘) is maximal for all 𝑘 (which agrees with the standard definition of a smooth map). We will
later show (in §4.4) that smoothish maps form a category AD, and then by categorical semantics,
that all expressions in the first-order language map to that category.

4.3 Primitives
Any first-order primitive may be implemented by giving its power-series representation. We use the
notation 𝑓 (𝑘) (𝑥; (cid:174)𝑣) to denote the 𝑘th derivative of 𝑓 at 𝑥 in directions (cid:174)𝑣; a smoothish map 𝑓 is defined
by the collection of these functions for all 𝑘 ∈ N. These data provide power-series expansions
around any input point. There is a map 0 : Γ (cid:123) 𝐴 (for any Γ, 𝐴 ∈ AD) that always returns zero
regardless of its input. A linear map 𝑓 : 𝐴 → 𝐵 determines a smooth map linear(𝑓 ) : 𝐴 (cid:123) 𝐵 by

linear(𝑓 ) (0) (𝑥) ≜ 𝑓 (𝑥)
linear(𝑓 ) (1) (𝑥; 𝑣) ≜ 𝑓 (𝑣)

linear(𝑓 ) (𝑘+2) (𝑥; (cid:174)𝑣) ≜ 0

Derivative-Tower Construction. A derivative tower can be viewed as a stream of a function and

all of its derivatives. Streams are characterized by the isomorphism

Stream(𝐴) (cid:27) 𝐴 × Stream(𝐴)
that says that a stream 𝑠 : Stream(𝐴) is exactly composed of its head, head(𝑠) : 𝐴, and its tail,
tail(𝑠) : Stream(𝐴). To construct a derivative tower, we define the map foldDer as an analogue to
the cons operation on streams. For instance, given value-level definitions of sine and cosine, sin
and cos, it is well-founded to define their derivative towers as

⟦sin⟧AD
⟦cos⟧AD

≜ foldDer(sin, ⟦𝑥, 𝑑𝑥 ⊢ cos(𝑥) * 𝑑𝑥⟧AD)
≜ foldDer(cos, ⟦𝑥, 𝑑𝑥 ⊢ −sin(𝑥) * 𝑑𝑥⟧AD),

just as it would be to define two mutually recursive streams evens = cons(0, map(𝜆𝑥 . 𝑥 + 1, odds))
and odds = cons(1, map(𝜆𝑥 . 𝑥 + 3, evens))).

We define foldDer as follows, where 𝑓 : 𝐴 → 𝐵 and 𝑔 : 𝐴 × 𝐴 (cid:123) 𝐵, such that foldDer(𝑓 , 𝑔) :

𝐴 (cid:123) 𝐵.

foldDer(𝑓 , 𝑔) (0) (𝑥) ≜ 𝑓 (𝑥)

foldDer(𝑓 , 𝑔) (𝑘+1) (𝑥; 𝑣1, . . . , 𝑣𝑘+1) ≜ 𝑔 (𝑘) ((𝑥, 𝑣1); (𝑣2, 0), . . . , (𝑣𝑘+1, 0))

(𝑘 ∈ N)

One of the perturbations 𝑣1 is passed in as the value to 𝑔, and then that perturbation is not considered
to have any derivatives itself, hence the 0s in the second components of the perturbation passed to
𝑔. Setting the first components of the derivatives to 𝑣2, . . . , 𝑣𝑘+1 establishes these as independent
infinitesimal perturbations of the first value component, 𝑥.

3:12

Benjamin Sherman, Jesse Michel, and Michael Carbin

4.3.1 Arithmetic Operations. The binary arithmetic operations are first-order functions and so can
be represented in AD as functions with the type R × R (cid:123) R. Addition and subtraction are linear,
≜ linear(−). We define the smooth
≜ linear(+) and ⟦-⟧AD
so their semantics is simply ⟦+⟧AD
multiplication operator by

⟦*⟧AD

≜ foldDer(𝜆(𝑥, 𝑦). 𝑥 × 𝑦, ⟦(𝑥, 𝑦), (𝑑𝑥, 𝑑𝑦) ⊢ 𝑥 * 𝑑𝑦 + 𝑦 * 𝑑𝑥⟧AD),

whose derivative is the familiar product rule. Note that our definition of ⟦*⟧AD
has two recursive
references to multiplication’s own smooth map. This recursive reference is well-founded because
the result is used in a way that does not demand any further differentiation. This recursive pattern
is similar to defining the stream of natural numbers nats : Stream(N) by

nats ≜ cons(0, map (𝜆𝑥 . 𝑥 + 1) nats),

where mapping a function over nats does not demand any further calls to tail. Reciprocals (used
for division) can be defined using foldDer as well, where all 𝑘th-order derivatives will return ⊥
when the input is 0.

Lipschitz but Nonsmooth Functions. Many functions, such as max, min, and ReLU, are locally
4.3.2
Lipschitz but not smooth. These functions are used pervasively in contexts that require differentia-
tion, so their admissibility in a differential-programming semantics is paramount. Whereas most
differential-programming semantics say that derivative of max is undefined when its arguments
are equal, our use of Clarke derivatives permits a non-⊥ result.

We define max as follows, where hull computes the interval corresponding to the convex hull of

the union of a set of points.

⟦max⟧(0)

AD

⟦max⟧(1)

AD

((𝑥, 𝑦); (𝑑𝑥, 𝑑𝑦)) ≜

⟦max⟧(𝑘+2)

AD

((𝑥, 𝑦); (cid:174)𝑣) ≜

(𝑥, 𝑦) ≜ max(𝑥, 𝑦)
𝑑𝑥

𝑑𝑦

hull({𝑑𝑥, 𝑑𝑦})

(cid:40)0
𝑥 ≠ 𝑦
⊥ 𝑥 = 𝑦

𝑥 > 𝑦
𝑦 < 𝑥
𝑥 = 𝑦

4.3.3 Differentiation Operator. To give a semantics to the syntax 𝜕𝑒𝑦
𝜕𝑥 |𝑥=𝑒𝑥 ·𝑒𝑑𝑥 for differentiation,
we first define a differentiation operator, postfix ′, on smoothish maps, where 𝑓 : 𝐴 (cid:123) 𝐵 maps to
𝑓 ′ : 𝐴 × 𝐴 (cid:123) 𝐵. Defining this operator is nontrivial, because all the derivatives of 𝑓 ′ must consider
not only perturbations to the function value but also perturbations to the derivative argument,
which are not accounted for in the original derivative tower: note that the 𝑘th derivative of 𝑓 is
a multilinear map from 𝐴𝑘 , whereas the 𝑘th derivative of 𝑓 ′ is a multilinear map from 𝐴2𝑘 . We
show the value and first few derivatives; because 𝑥 will always be applied as the value argument to
derivatives of 𝑓 , we elide those arguments:

𝑓 ′ (0) (𝑥, 𝑣) = 𝑓 (1) (𝑣)

𝑓 ′ (1) ((𝑥, 𝑣); (𝑑𝑥𝑎, 𝑑𝑣𝑎)) = 𝑓 (2) (𝑣, 𝑑𝑥𝑎) + 𝑓 (1) (𝑑𝑣𝑎)

𝑓 ′ (2) ((𝑥, 𝑣); (𝑑𝑥𝑎, 𝑑𝑣𝑎), (𝑑𝑥𝑏, 𝑑𝑣𝑏)) = 𝑓 (3) (𝑣, 𝑑𝑥𝑎, 𝑑𝑥𝑏) + 𝑓 (2) (𝑑𝑣𝑎, 𝑑𝑥𝑏) + 𝑓 (2) (𝑑𝑥𝑎, 𝑑𝑣𝑏)

𝜆𝑆 : Computable Semantics for Differentiable Programming with Higher-Order Functions and Datatypes

3:13

The general formula is:

𝑓 ′ (𝑘) ((𝑥, 𝑣); (𝑑𝑥1, 𝑑𝑣1), . . . , (𝑑𝑥𝑘, 𝑑𝑣𝑘 )) ≜

𝑓 (𝑘+1) (𝑥; 𝑣, 𝑑𝑥1, . . . , 𝑑𝑥𝑘 ) +

𝑘
∑︁

𝑗=1

𝑓 (𝑘) (𝑥; 𝑑𝑥1, . . . , 𝑑𝑥 𝑗−1, 𝑑𝑣 𝑗, 𝑑𝑥 𝑗+1, . . . 𝑑𝑥𝑘 ).

4.3.4 Revisiting Derivative Tower Construction. The ′ operator is analogous to the tail operator of a
stream, in that derivative towers have the section-retraction pair

𝐴 (cid:123) 𝐵

𝜆𝑓 .(𝑓 (0) ,𝑓 ′)

foldDer

(𝐴 → 𝐵) × (𝐴 × 𝐴 (cid:123) 𝐵)

that characterizes a derivative tower 𝑓 : 𝐴 (cid:123) 𝐵 as a function 𝑓 (0) : 𝐴 → 𝐵 for the evaluation map
of 𝑓 together with a derivative tower 𝑓 ′ : 𝐴 × 𝐴 (cid:123) 𝐵 where 𝑓 ′(𝑥, 𝑣) represents the directional
derivative of 𝑓 at 𝑥 in the direction 𝑣.

Given this observation, we may for convenience in the rest of the paper define a smoothish
map 𝑓 by its value-level function 𝑓 (0) and its smoothish derivative 𝑓 ′, denoting an implicit use of
foldDer. For example, we can equivalently define the smooth multiplication operator (§4.3.1) by

⟦*⟧(0)
AD
⟦*⟧′

AD

≜ 𝜆(𝑥, 𝑦). 𝑥 × 𝑦
≜ ⟦(𝑥, 𝑦), (𝑑𝑥, 𝑑𝑦) ⊢ 𝑥 * 𝑑𝑦 + 𝑦 * 𝑑𝑥⟧AD

.

4.4 Categorical Operations
AD forms a Cartesian monoidal category. We describe the categorical operations here, and prove
that they satisfy the expected properties in Proposition 4.5. The maps id : 𝐴 → 𝐴 (for all 𝐴),
! : Γ → ∗ (for all Γ), fst : 𝐴 × 𝐵 → 𝐴 and snd : 𝐴 × 𝐵 → 𝐵 (for all 𝐴, 𝐵) are all in fact linear maps
and so can be made into smooth maps with the linear operator described above. Given 𝑓 : Γ (cid:123) 𝐴
and 𝑔 : Γ (cid:123) 𝐵, we define their product ⟨𝑓 , 𝑔⟩ : Γ (cid:123) 𝐴 × 𝐵 by

⟨𝑓 , 𝑔⟩ (𝑘) (𝑥; (cid:174)𝑣) ≜ (𝑓 (𝑘) (𝑥; (cid:174)𝑣), 𝑔 (𝑘) (𝑥; (cid:174)𝑣)),

It only remains to define composition. Composition of smooth maps is given by Faà di Bruno’s
formula. The definition is perhaps easier to understand by example for small 𝑘. The following
shows derivatives of 𝑔 ◦ 𝑓 at 𝑥; since 𝑔 is always differentiated at 𝑓 (𝑥) and 𝑓 is always differentiated
at 𝑥, we elide those arguments:

(𝑔 ◦ 𝑓 ) (0) () = 𝑔 (0) ()

(𝑔 ◦ 𝑓 ) (1) (𝑣𝑎) = 𝑔 (1) (𝑓 (1) (𝑣𝑎))

(𝑔 ◦ 𝑓 ) (2) (𝑣𝑎, 𝑣𝑏) = 𝑔 (2) (𝑓 (1) (𝑣𝑎), 𝑓 (1) (𝑣𝑏)) + 𝑔 (1) (𝑓 (2) (𝑣𝑎, 𝑣𝑏))

(𝑔 ◦ 𝑓 ) (3) (𝑣𝑎, 𝑣𝑏, 𝑣𝑐 ) = 𝑔 (3) (𝑓 (1) (𝑣𝑎), 𝑓 (1) (𝑣𝑏), 𝑓 (1) (𝑣𝑐 ))

+ 𝑔 (2) (𝑓 (2) (𝑣𝑎, 𝑣𝑏), 𝑓 (1) (𝑣𝑐 )) + 𝑔 (2) (𝑓 (2) (𝑣𝑎, 𝑣𝑐 ), 𝑓 (1) (𝑣𝑏))
+ 𝑔 (2) (𝑓 (2) (𝑣𝑏, 𝑣𝑐 ), 𝑓 (1) (𝑣𝑎)) + 𝑔 (1) (𝑓 (3) (𝑣𝑎, 𝑣𝑏, 𝑣𝑐 ))

The general formula is

(𝑔 ◦ 𝑓 ) (𝑘) (𝑥; (cid:174)𝑣) ≜ ∑︁

𝜋 ∈H ( {1,...,𝑘 })

let 𝑛 ≜ |𝜋 | in 𝑔 (𝑛) (cid:169)
(cid:173)
(cid:173)
(cid:171)

𝑓 (𝑥);

𝑓 ( |𝜋1 |) (𝑥; 𝑣𝜋11
...,
𝑓 ( |𝜋𝑛 |) (𝑥; 𝑣𝜋𝑛 1

, . . . , 𝑣𝜋1 |𝜋1 | ),

, . . . , 𝑣𝜋𝑛 |𝜋𝑛 | )

,

(cid:170)
(cid:174)
(cid:174)
(cid:172)

3:14

Benjamin Sherman, Jesse Michel, and Michael Carbin

where H (𝑆) is the set of partitions of a set 𝑆, and |𝑆 | is the cardinality of a set. Note that in the
general case, the inputs to 𝑔 (𝑛) may be elements of R𝑏 rather than R𝑏 (for some 𝑏 ∈ N). Given any
𝑛th derivative 𝑔 (𝑛) : R𝑏 ×(R𝑏)𝑘 → R𝑚, we extend it to apply to inputs 𝑥 ∈ R𝑏 and 𝑑𝑥1, . . . , 𝑑𝑥𝑘 ∈ R𝑏
by

𝑔 (𝑛) (𝑥; 𝑑𝑥1, . . . , 𝑑𝑥𝑘 ) ≜ hull (cid:110)

𝑔 (𝑛) (𝑦; 𝑑𝑦1, . . . , 𝑑𝑦𝑘 ) | 𝑦 ∈ 𝑥, 𝑑𝑦1 ∈ 𝑑𝑥1, . . . , 𝑑𝑦𝑘 ∈ 𝑑𝑥𝑘

(cid:111)

.

Faà di Bruno’s formula simplifies drastically in the case that either function is linear:

Proposition 4.2. For any 𝑔 : 𝐵 → 𝐶 and any derivative tower 𝑓 : 𝐴 (cid:123) 𝐵, for any 𝑘 ∈ N and any

𝑥 ∈ 𝐴 and 𝑣1, . . . , 𝑣𝑘 ∈ 𝐴,

(linear(𝑔) ◦ 𝑓 ) (𝑘) (𝑥; 𝑣1, . . . , 𝑣𝑘 ) = 𝑔(𝑓 (𝑘) (𝑣1, . . . , 𝑣𝑘 ))
Proof sketch. Because linear(𝑔) ( 𝑗) (𝑣1, . . . , 𝑣 𝑗 ) = 0 whenever 𝑗 > 1 by definition of linear, all
terms in the sum given by the Faà di Bruno formula where |𝜋 | > 1 will be 0. We can thus remove
□
those terms, and the only term in the sum that will remain is the one where |𝜋 | = 1.

Proposition 4.3. For any consistent derivative tower 𝑔 : 𝐵 (cid:123) 𝐶 and any 𝑓 : 𝐴 → 𝐵 that maps
maximal elements to maximal elements, for any 𝑘 ∈ N and any maximal 𝑥 ∈ 𝐴 and maximal
𝑣1, . . . , 𝑣𝑘 ∈ 𝐴,

(𝑔 ◦ linear(𝑓 )) (𝑘) (𝑥; 𝑣1, . . . , 𝑣𝑘 ) = 𝑔 (𝑘) (𝑓 (𝑣1), . . . , 𝑓 (𝑣𝑘 )).
Proof sketch. Note that the term in the sum given by the Faà di Bruno formula where |𝜋 | = 𝑘
gives the right-hand side 𝑔 (𝑘) (𝑓 (𝑣1), . . . , 𝑓 (𝑣𝑘 )). For all other terms in the sum, where |𝜋 | < 𝑘, we
have that one of the inputs to 𝑔 ( |𝜋 |) will be 0, because we have linear(𝑓 ) ( 𝑗) (𝑣1, . . . , 𝑣 𝑗 ) = 0 whenever
𝑗 > 1 by definition of linear.

We need to know that adding all these terms to the term |𝜋 | = 𝑘 makes no difference to the
sum, which can happen either if all of the terms are 0, or if already 𝑔 (𝑘) (𝑓 (𝑣1), . . . , 𝑓 (𝑣𝑘 )) = ⊥, in
which case the addition of any elements will not change the result. Thus, it suffices to prove that
if 𝑔 (𝑘) (𝑓 (𝑣1), . . . , 𝑓 (𝑣𝑘 )) ≠ ⊥, then all of those other terms in the sum are 0. A detailed technical
□
argument can show that this is the case.

The chain rule for Clarke derivatives is a specialization relation rather than an equality:
Proposition 4.4 (Chain rule for 𝜕⊥). Given 𝑓 : R𝑛 → R𝑚

⊥, and 𝑔 : R𝑚 → R𝑘

⊥ for all 𝑥 ∈ R𝑛

and all 𝑑𝑥 ∈ R𝑛,

hull ({𝐺 · 𝐹 · 𝑑𝑥 | 𝐺 ∈ (𝜕⊥𝑔) (𝑓 (𝑥)), 𝐹 ∈ 𝜕⊥𝑓 (𝑥)}) ⊑ 𝜕⊥(𝑔 ◦ 𝑓 )(𝑥) · 𝑑𝑥 .

Proof sketch. A minor variation of [Clarke 1990, Corollary on page 75].

□

For example, at the value level, max x 0 + min 0 x = x, but the derivative of the left-hand side
at 0 is [0, 2] while the derivative at the right-hand side is 1, noting [0, 2] ⊑ 1. This has important
ramifications for 𝜆𝑆 , where we construct functions as compositions of others and need composition
to be computable. Because of the specialization relation, we know that any behavior of a function
in 𝜆𝑆 (e.g., [0, 2]) will be compatible with the ideal derivative of its value-level function (e.g., 1), but
it may not return the maximal such value.

Proposition 4.5. These operations (identity, composition, pairing, projections) give AD the structure

of a Cartesian monoidal category. Therefore, AD admits the internal language described in Fig. 3.

Proof sketch. There are two main classes of properties we must confirm about these categorical
operations. First, we must verify that all of the operations preserve successive consistency, taking
consistent derivative towers to consistent derivative towers. Second, we must confirm that the
algebraic laws of a Cartesian monoidal category.

𝜆𝑆 : Computable Semantics for Differentiable Programming with Higher-Order Functions and Datatypes

3:15

1. Operations preserve consistency. Because several of the categorical operations are of the form

linear(𝑓 ) we first prove a lemma that these maps are consistent:
Lemma 4.6. Call a map 𝑓 : R𝑛 → R𝑘 linear if it always outputs values in R𝑘 and if it is linear in
the traditional sense, i.e., 𝑓 (𝑢) + 𝑓 (𝑣) = 𝑓 (𝑢 + 𝑣) for all 𝑢, 𝑣 ∈ R𝑛 and 𝑐 · 𝑓 (𝑣) = 𝑓 (𝑐 · 𝑣) for all
𝑐 ∈ R and all 𝑣 ∈ R𝑛. Whenever 𝑓 : R𝑛 → R𝑘 is linear, linear(𝑓 ) is consistent.
Proof. Since 𝑓 is linear in the above-defined sense, it is smooth, and so its derivatives will
always be maximal, and will coincide with the traditional derivatives, which is exactly what
□
linear(𝑓 ) computes.
• Identity maps are consistent. Follows from Lemma 4.6.
• Product projections are consistent. Also follows from Lemma 4.6.
• Pairing preserves consistency. Essentially reduces to the following lemma:

Proposition 4.7. Given two maps 𝑓 : R𝑛 → R𝑚

⊥ and 𝑔 : R𝑛 → R𝑘

⊥, for any 𝑥 ∈ R𝑛,

{ (cid:2)𝑢 𝑣 (cid:3) | 𝑢 ∈ 𝜕⊥ 𝑓 (𝑥), 𝑣 ∈ 𝜕⊥𝑔(𝑥)} ⊑ 𝜕⊥ (𝜆𝑧.(𝑓 (𝑧), 𝑔(𝑧)))(𝑥),

⊥ → R𝑚+𝑘

⊥ returns ⊥ if either of its arguments it ⊥,

where the pairing operation (·, ·) : R𝑚
⊥ × R𝑘
or the pair of values if both inputs are not ⊥.
Proof sketch. Note that the set defined by the set comprehension on the left-hand side of
the relation is convex, since both 𝜕⊥𝑓 (𝑥) and 𝜕⊥𝑔(𝑥) are. Suppose (cid:2)𝐻 𝐿(cid:3) is in the Bouligand
subdifferential of 𝜆𝑧.(𝑓 (𝑧), 𝑔(𝑧)) at 𝑥. Then it must be the case that 𝐻 is in the Bouligand
□
subdifferential for 𝑓 and that 𝐿 is in the Bouligand subdifferential for 𝑔.
• Composition preserves consistency. The full proof is quite detailed and technical. At its core,
the proof proceeds much like the proof of the conventional Faà di Bruno formula, which can
proceed by induction on the order of differentiation. However, whereas conventionally there is
an equality between the Faá di Bruno formula and the derivative, in our case, their is an order
relation that the Faá di Bruno formula is at most the derivative. The base case is the general
chain rule of calculus, which in our case corresponds to the chain rule for Clarke derivatives,
Proposition 4.4. The key step in the inductive case is the tensor product rule:
Proposition 4.8 (Tensor product rule for 𝜕⊥). Given 𝑔 : 𝐷 → R𝑚×𝑛 𝑗 ×...×𝑛1
𝑖 ∈ {1, . . . , 𝑗 }, 𝑓𝑖 : 𝐷 → R𝑛𝑖

⊥ and for all

⊥, for all 𝑥 ∈ 𝐷,
𝑗
∑︁

(cid:33)

+ 𝑔(𝑥) ·

𝑓𝑘 (𝑥)

𝜕⊥𝑔(𝑥) ·

(cid:32) 𝑗
(cid:204)

𝑘=1

(cid:32) 𝑖−1
(cid:204)

(cid:33)

𝑓𝑘 (𝑥)

⊗ 𝜕⊥ 𝑓𝑖 (𝑥) ⊗

(cid:33)

𝑓𝑘 (𝑥)

(cid:32)

𝑗
(cid:204)

𝑘=𝑖+1

𝑖=1

𝑘=1
⊑
𝜕⊥ (𝑔 · (𝑓1 ⊗ . . . ⊗ 𝑓𝑗 ))(𝑥).

Proof. By repeated application of the product rule for Clarke derivatives.

□

2. Algebraic laws hold.

• Composition is associative. Follows from associativity and commutativity of + and the associa-

tivity of taking partitions of partitions (in the appropriate sense).

• 𝑓 ◦ id = 𝑓 = id ◦ 𝑓 . Follows from Proposition 4.3 and Proposition 4.2, since id is linear.
• 𝛽 and 𝜂 laws for product projections. Follows from the fact that linear commutes with pairing,
□

i.e., linear(⟨𝑓 , 𝑔⟩) = ⟨linear(𝑓 ), linear(𝑔)⟩, and from Proposition 4.3 and Proposition 4.2.

4.5 Consistency
The derivatives that our semantics defines are consistent: the behaviors of 𝑘th derivative that is
computed, ⟦𝑒⟧(𝑘)
, are compatible with the derivatives that would be abstractly defined by looking
AD

3:16

Benjamin Sherman, Jesse Michel, and Michael Carbin

Syntax

variables 𝑥

constant types 𝐾 ∈ Ob(HAD)

types 𝜏 ::= ∗ | 𝜏1 × 𝜏2

| 𝜏1 → 𝜏2

|

⌊𝐾⌋

(𝑥 : 𝜏) ∈ Γ
Γ ⊢ 𝑥 : 𝜏

Typing rules
Γ ⊢ 𝑒1 : 𝜏1 → 𝜏2

Γ ⊢ 𝑒2 : 𝜏1

Γ ⊢ 𝑒1 𝑒2 : 𝜏2

Γ, 𝑥 : 𝜏1 ⊢ 𝑒 : 𝜏2
Γ ⊢ 𝜆 𝑥 : 𝜏1. 𝑒 : 𝜏1 → 𝜏2

| Γ, 𝑥 : 𝜏

contexts Γ ::= ·
constants 𝑘 ∈ Arr(HAD)
⌊𝑘⌋
(𝑒, 𝑒)
let 𝑥 ≜ 𝑒 in 𝑒

expressions 𝑒 ::= 𝑥 |
|

| !
|

| 𝑒 𝑒

| 𝜆𝑥 . 𝑒

𝑘 ∈ ⟦Γ⟧ →HAD ⟦𝜏⟧
Γ ⊢ ⌊𝑘⌋ : 𝜏

Γ ⊢ ! : ∗

Γ ⊢ 𝑒1 : 𝜏1

Γ ⊢ 𝑒2 : 𝜏2

Γ ⊢ (𝑒1, 𝑒2) : 𝜏1 × 𝜏2

Γ ⊢ 𝑒1 : 𝜏1

Γ, 𝑥 : 𝜏1 ⊢ 𝑒2 : 𝜏2

Γ ⊢ let 𝑥 ≜ 𝑒1 in 𝑒2 : 𝜏2

Fig. 5. Syntax and typing rules for 𝜆𝑆 . The constants are those listed in Fig. 2.

Types

⟦∗⟧(Γ) ≜ 1Set

⟦𝜏1 × 𝜏2⟧(Γ) ≜ ⟦𝜏1⟧(Γ) × ⟦𝜏2⟧(Γ)

⟦⌊𝐾⌋⟧(Γ) ≜ 𝐾 (Γ)

⟦𝜏1 → 𝜏2⟧(Γ) ≜

∫

Δ∈AD

(Δ (cid:123) Γ) × ⟦𝜏1⟧(Δ) → ⟦𝜏2⟧(Δ)

Terms

⟦𝑒1 𝑒2⟧(𝛾) ≜ ⟦𝑒1⟧(𝛾)(id, ⟦𝑒2⟧(𝛾))

⟦𝜆 𝑥 : 𝜏1. 𝑒⟧ ≜ abstract(⟦𝑒⟧)

⟦⌊𝑘⌋⟧(𝛾) ≜ 𝑘 (𝛾)
⟦!⟧ ≜!

⟦(𝑒1, 𝑒2)⟧(𝛾) ≜ (⟦𝑒1⟧(𝛾), ⟦𝑒2⟧(𝛾))

⟦let 𝑥 ≜ 𝑒1 in 𝑒2⟧ ≜ ⟦(𝜆𝑥 . 𝑒2) 𝑒1⟧

Fig. 6. The semantics of 𝜆𝑆 .

at its value-level behavior, 𝜕𝑘
⊥
derivative towers are successively consistent.

Mat0(⟦𝑒⟧(0)
AD

). This proposition follows by first demonstrating that

Proposition 4.9. Given any term Γ ⊢ 𝑒 : 𝜏, the derivative tower ⟦𝑒⟧AD is successively consistent.
Proof sketch. By induction on the typing derivation of 𝑒. We then see that, to know the
proposition is true, we must know that the derivative towers for all primitives are consistent
(including product projections) and that pairing and composition preserve successive consistency
□
(proof sketch in Proposition 4.5).

Proposition 4.10 (Consistency of differentiation in the first-order language). Given

any term Γ ⊢ 𝑒 : 𝜏, for all 𝑘 ∈ N, Cons𝑘+1

(cid:16)

⟦𝑒⟧(𝑘+1)
AD

, 𝜕𝑘+1
⊥

Mat0(⟦𝑒⟧(0)
AD

)

(cid:17)

.

Proof sketch. By Proposition 4.9, ⟦𝑒⟧AD

by a simple induction on 𝑘.

is successively consistent. Then the proof proceeds
□

5 HIGHER-ORDER SEMANTICS (HAD)
The category AD does not admit exponentiation (function spaces), since its objects are limited to
R𝑛. However, higher-order functions yield novel expressive power that is critical for §7. To admit
higher-order functions, 𝜆𝑆 uses a category HAD of presheaves over AD, i.e., HAD = [ADop, Set].

𝜆𝑆 : Computable Semantics for Differentiable Programming with Higher-Order Functions and Datatypes

3:17

Syntax and Semantics. The basic syntax of HAD is that of the simply-typed lambda calculus,
shown in Fig. 5, where the constants are those listed in Fig. 2. Fig. 6 presents the semantics of 𝜆𝑆
(generic to any Cartesian closed category of presheaves). However, the categorical semantics in
HAD means that 𝜆𝑆 is inherently extensible and not limited to just those constants in Fig. 2; any
object or morphism in HAD could be added to the language.

We now proceed to describe the semantics of the higher-order constants in Fig. 2.

5.1 Ground Types and First-Order Primitives
Any space 𝑋 ∈ AD can be lifted into a presheaf HAD by the Yoneda embedding, 𝑋 ∈ HAD, which
acts as 𝑋 (Γ) ≜ Γ (cid:123) 𝑋 . Because the Yoneda embedding is full and faithful and preserves products,
ground types (and their products) in HAD represent Cartesian spaces and first-order functions
represent smoothish maps. In particular, ⟦ℜ⟧ ≜ R. Note that all first-order functions from AD can
be lifted into HAD by the Yoneda embedding.

5.2 Smoothish Higher-Order Primitive Functions
Each of the smoothish higher-order primitive functions has a type (ℜ → ℜ) → ℜ in 𝜆𝑆 . To
construct primitives of this type, we can equivalently construct maps (Γ × R (cid:123) R) → (Γ (cid:123) R) for
all Γ ∈ AD in 𝜆𝐶 .2 Such a map takes as input R-valued expression in a context Γ × R and produce
an R-valued expression in the context Γ (for any Γ).

Accordingly, we defined these second-order primitives with parametrically polymorphic map-
pings of derivative towers. We must confirm that these definitions preserve successive consistency,
i.e., they must map successively consistent derivative towers to successively consistent derivative
towers. In general, this boils down to confirming that taking the derivative of the value-level
definitions of each of these primitives (when applied to any possible function 𝑓 : Γ × R (cid:123) R) yields
the definitions for the derivatives of these primitives. It is possible to confirm for each definition
that this is the case.

5.2.1

Smooth integral. The integral integral01 is defined as follows for any 𝑓 : Γ × R (cid:123) R:

⟦integral01⟧HAD (𝑓 ) (𝑘) (𝛾; 𝑑𝛾1, . . . , 𝑑𝛾𝑘 ) ≜

∫ 1

0

𝑓 (𝑘) (𝛾, 𝑥; (𝑑𝛾1, 0), . . . , (𝑑𝛾𝑘, 0)) 𝑑𝑥 .

Since integration is a linear operator, we essentially just integrate the first-order infinitesimal
perturbations arising from 𝑓 at every order of derivative. Integration is smooth in the sense that
if its input is smooth, its output will be smooth as well. Note the similarity between the above
AD tower and the result of postcomposing a linear function ℓ after a function 𝑓 arising from
Faà di Bruno’s formula described previously. The reader may wonder how a semantics invoking
integration might be computable; we discuss this in §6.

Smoothish Root Finding. The primitive cutRoot : (ℜ → ℜ) → ℜ smoothly finds the
5.2.2
root of any function with a single isolated root that is positive to its left and negative to its right.
Equivalently, cutRoot is a map (Γ × R (cid:123) R) → (Γ (cid:123) R). We will define cutRoot by using the
stream characterization of smooth maps, defining it with a function for its evaluation map and a

2In any category of presheaves [ Cop, Set], letting · denote the Yoneda embedding and letting ⇒ denote the internal hom,
then there is an equivalence between constants with the second-order type (𝐴 ⇒ 𝐵) ⇒ 𝐶 and the end ∫
Γ (Γ × 𝐴 →C 𝐵) →
(Γ →C 𝐶):

1 →[Cop,Set] (𝐴 ⇒ 𝐵) ⇒ 𝐶 (cid:27) (𝐴 ⇒ 𝐵) →[Cop,Set] 𝐶 =

∫

Γ

(𝐴 ⇒ 𝐵) (Γ) → 𝐶 (Γ) (cid:27)

∫

Γ

(Γ × 𝐴 →C 𝐵) → (Γ →C 𝐶)

3:18

Benjamin Sherman, Jesse Michel, and Michael Carbin

smooth map for its derivative:

⟦cutRoot⟧HAD (𝑓 ) (0) ≜ 𝜆𝛾 . [sup{𝑥 : R | 𝑓 (0) (𝛾, 𝑥) > 0}, inf {𝑥 : R | 𝑓 (0) (𝛾, 𝑥) < 0}]
⟦cutRoot⟧HAD (𝑓 ) ′ ≜

(cid:22)
𝛾, 𝑑𝛾 ⊢ let 𝑦 ≜ (cid:4)⟦cutRoot⟧HAD (𝑓 )(cid:5) (𝛾) in −

(cid:23)

⌊𝑓 ′⌋ ((𝛾, 𝑦), (𝑑𝛾, 0))
⌊𝑓 ′⌋ ((𝛾, 𝑦), (0, 1))

AD
The formula for the derivative is a simple application of the implicit function theorem. Note that we
have a well-founded recursive reference following the same pattern as with multiplication.

cutRoot enables root-finding only for functions that have only one root. In graphics, for ray
tracing of implicit surfaces, it is useful to be able to find for a function f : ℜ → ℜ the least root
𝑥 ∈ [0, 1] such that 𝑓 switches from positive for values just less than 𝑥 to negative for values just
greater than 𝑥. firstRoot : (ℜ → ℜ) → ℜ accomplishes this:

⟦firstRoot⟧HAD (𝑓 ) (0) ≜ 𝜆𝛾 . [sup{𝑥 ∈ [0, 1] | ∀𝑞 ∈ [0, 𝑥]. 𝑓 (0) (𝛾, 𝑞) > 0}
, inf {𝑥 ∈ [0, 1] | ∃𝑞 ∈ [0, 𝑥]. 𝑓 (0) (𝛾, 𝑥) < 0}]
(cid:25)
let 𝑦 ≜ (cid:4)⟦firstRoot⟧HAD (𝑓 )(cid:5) (𝛾) in
− ⌊𝑓 ′ ⌋ ( (𝛾,𝑦),(𝑑𝛾,0))
⌊𝑓 ′ ⌋ ( (𝛾,𝑦),(0,1))

⟦firstRoot⟧HAD (𝑓 ) ′ ≜

𝛾, 𝑑𝛾 ⊢

(cid:24)

AD

Like with cutRoot, its derivatives are determined by the implicit function theorem; the only
difference is in the definition of the value of the root.

Smoothish Optimization. 𝜆𝑆 admits primitives argmax01, max01 : (ℜ → ℜ) → ℜ that
5.2.3
find the maximizing argument and the maximum, respectively, of a function f : ℜ → ℜ over
the unit interval. Equivalently, each of argmax01 and max01 are maps (Γ × R (cid:123) R) → (Γ (cid:123) R).

We first describe argmax01, which is defined as follows:

⟦argmax01⟧HAD (𝑓 ) (0) ≜ 𝜆𝛾 . hull

(cid:26)

⟦argmax01⟧HAD (𝑓 ) ′ ≜

(cid:30)
(cid:30)
(cid:30)
(cid:30)
(cid:30)
𝛾, 𝑑𝛾 ⊢
(cid:30)
(cid:30)
(cid:30)
(cid:28)






⊥

(cid:18)

{𝑥 ∈ [0, 1] | 𝑓 (𝛾, 𝑥) = max
𝑧 ∈ [0,1]

𝑓 (𝛾, 𝑧)}

(cid:19)

let 𝑦 ≜ (cid:4)⟦argmax01⟧HAD (𝑓 )(cid:5) (𝛾) in

let 𝑓 ′

𝑦 ≜ ⌊𝑓 ′⌋ ((𝛾, 𝑦), (0, 1)) in

− ⌊𝑓 ′′ ⌋ ( ( (𝛾,𝑦),(0,1)),( (𝑑𝛾,0),(0,0)))
⌊𝑓 ′′ ⌋ ( ( (𝛾,𝑦),(0,1)),( (0,1),(0,0)))
0
0

0 < 𝑦 < 1
𝑦 = 0 ∧ 𝑓 ′
𝑦 = 1 ∧ 𝑓 ′
otherwise

𝑦 < 0
𝑦 > 0

(cid:27)

(cid:31)
(cid:31)
(cid:31)
(cid:31)
(cid:31)
(cid:31)
(cid:31)
(cid:31)
(cid:29)

AD

The input is a smooth map 𝑓 : Γ × R (cid:123) R. In general, for a 𝛾 ∈ Γ, there may be many values
of 𝑥 achieving the same maximum 𝑓 (𝛾, 𝑥), so the value-level definition takes the convex hull of
the set of those maximizing arguments. The derivative of argmax01 is not ⊥ only when its value
is maximal, i.e., there is only one maximizing argument, which we will call 𝑦. There are three
possibilities for 𝑦: either 0 < 𝑦 < 1, or 𝑦 = 0, or 𝑦 = 1. In the case that 0 < 𝑦 < 1, then if 𝑓 ′′ is
defined at (𝛾, 𝑦), then we know that 𝑓 ′
𝑦 ,
𝑦 (𝛾, 𝑦) = 0 and that this argmax is an isolated root of 𝑓 ′
𝑦 is the derivative of 𝑓 with respect to its latter argument. Any infinitesimal perturbation
where 𝑓 ′
𝑦 , so the implicit function theorem
𝑑𝛾 to 𝛾 results in an infinitesimal perturbation to the root of 𝑓 ′
defines how the root changes. If the maximizing argument 𝑦 is on the boundary, i.e., 𝑦 = 0 or 𝑦 = 1,
then if we additionally know that either 𝑓 ′
𝑦 (𝛾, 𝑦) > 0, respectively, then it must be
𝑦 (𝛾, 𝑦) < 0 or 𝑓 ′
the case that the derivative of the argmax is 0, because the argmax will be stuck at the boundary
no matter how 𝛾 might be infinitesimally perturbed.

𝜆𝑆 : Computable Semantics for Differentiable Programming with Higher-Order Functions and Datatypes

3:19

We can now proceed to describe max01, whose derivative is defined in terms of argmax01:

𝑓 (𝛾, 𝑥)

⟦max01⟧HAD (𝑓 ) (0) ≜ 𝜆𝛾 . max
𝑥 ∈ [0,1]
⟦max01⟧HAD (𝑓 ) ′ ≜ (cid:0)𝑓 ◦ ⟦argmax01⟧HAD (𝑓 )(cid:1) ′
Just as the derivative of max depends on which argument results in the max, similarly the derivative
of max01 is a function of the maximizing argument. If we can isolate a single argmax, then max01
f = f (argmax01 f), and thus all the derivatives of max01 f follow from the chain rule and the
smooth derivatives of f and argmax01 f.

5.3 Internal Derivatives of Functions at All Types
The primitive tangent A B : (A → B) → Tan A → Tan B permits the expression of the
derivative of any function in 𝜆𝑆 , with any input type A and output type B. These types are much more
general than those on which differentiation in classically defined in mathematics. In this section,
we will explain the semantics of tangent and Tan, which generalize the notion of differentiation
from AD to apply to all objects in HAD (i.e., all types in 𝜆𝑆 ).

We need to systematically generalize the derivative of AD, as expressed with the postfix ′
operator, to apply to HAD. Following Vákár et al. [2018], we can apply the categorical technique
of left Kan extensions, which extend a functor on a base category to one that acts on presheaves
over that category. Our definition of generalized tangent spaces and its properties will also be
similar to the dvs diffeology on internal tangent bundles as described by Christensen and Wu
[2017]. Accordingly, we can lift the operation of forward-mode differentiation from the first-order
language AD to the higher-order language HAD. Defining

valueWithDer : (𝐴 (cid:123) 𝐵) → (𝐴 × 𝐴 (cid:123) 𝐵 × 𝐵)
valueWithDer(𝑓 ) ≜ ⟦𝑥, 𝑑𝑥 ⊢ (⌊𝑓 ⌋ (𝑥), ⌊𝑓 ′⌋ (𝑥, 𝑑𝑥))⟧AD
we find that valueWithDer defines a functor on AD acting on objects by 𝑋 ↦→ 𝑋 × 𝑋 from a space
𝑋 to its tangent bundle 𝑋 × 𝑋 , where the tangent bundle 𝑋 × 𝑋 represents a point of 𝑋 together
with an infinitesimal perturbation of that point. The functoriality of valueWithDer follows from
the chain rule of differentiation (and that ⌊id′⌋ (𝑥, 𝑑𝑥) = 𝑑𝑥).

,

This functor can be extended to HAD via a left Kan extension to produce a functor Tan and
its functorial map tangent A B : (A → B) → Tan A → Tan B, which runs generalized
forward-mode derivatives, interpreted geometrically as a pushforward of the tangent bundles.
Concretely, we define the tangent bundle functor Tan, as a left Kan extension, corresponds to a
coend:

Tan(𝐹 )(Γ) (cid:27)

∫ Δ

(Γ (cid:123) Δ2) × 𝐹 (Δ).

Informally, the tangent bundle over the 𝜆𝑆 type 𝐹 in a context Γ is represented by a pair of a value
and infinitesimal perturbation Γ (cid:123) Δ2 for some Cartesian space Δ (i.e., Δ = R𝑛 for some 𝑛 ∈ N),
together with a map from the space Δ into the type 𝐹 . Thus, if we wish to define an infinitesimal
perturbation into a complicated type 𝐹 , we are able to do it by choosing a Cartesian space Δ to
express that infinitesimal perturbation, and then we construct a map from Δ to 𝐹 . All elements of
the tangent bundle of 𝐹 arise in that way.

We now explain how these tangent bundles work with an example. Suppose 𝐹 = R2 and we want
to represent the tangent bundle ((0, 1), (1, 0)) ∈ R2 × R2, i.e., the vector (0, 1) moving infinitesimally
in the (1, 0) direction. Since there are no variables in the context, we can define the tangent bundle

3:20

Benjamin Sherman, Jesse Michel, and Michael Carbin

at once for all Γ. The type of generalized tangent bundles is

Tan(R2)(Γ) (cid:27)

∫ Δ

(∗ (cid:123) Δ2) × (Δ (cid:123) R2).

We can represent the tangent bundle ((0, 1), (1, 0)) ∈ R2 × R2 in two equivalent ways. The straight-
forward way is to take Δ = R2 and put the point and its perturbation in the first component and
the identity map in the second,

(R2, (⟨(0, 1), (1, 0)⟩, id)).

: R → R2 defined by 𝑓 (𝑡) =
Alternatively, we can represent it with a parametric function 𝑓
(0, 1) + 𝑡 · (1, 0), describing a point that moves from (0, 1) at 𝑡 = 0 in the direction of (1, 0) as 𝑡
increases:

(R, (⟨0, 1⟩, 𝜆𝑡 . (0, 1) + 𝑡 · (1, 0))).

Two members (𝜏1, (𝑓1, 𝑔1)) and (𝜏2, (𝑓2, 𝑔2)) of Tan(R2)(Γ) are equivalent if

valueWithDer(𝑔1) ◦ 𝑓1 = valueWithDer(𝑔2) ◦ 𝑓2.
Indeed, this is the case for the two examples above, as both compositions yield ⟨(0, 1), (1, 0)⟩. This
criterion for equivalence is for representable types such as R2 but generalizes for tangent bundles
over types that are not representable. It intuitively captures the notion that the first component
of the tuple represents a tangent bundle of a representable space, whereas the second is a map
that applies to that output but is yet to be differentiated. This is the justification for applying
valueWithDer above.

The Kan extension is genuinely an extension of the underlying functor valueWithDer. That is,
we have the equivalence Tan(𝐴) (cid:27) 𝐴2, where · is the Yoneda embedding (Proposition 5.2). This
means that the generalized tangent bundle for Cartesian spaces R𝑛 is indeed R𝑛 × R𝑛: one R𝑛 for
the point and one R𝑛 for the infinitesimal perturbation.

The generalized tangent bundle functor supports other operations as well. A polymorphic
function tangentValue A : Tan A → A projects out the base point. The primitive tangentProd
A B : Tan (A * B) (cid:27) Tan A * Tan B implements the following isomorphism:

Proposition 5.1. Tangent bundles commute with products, i.e., Tan(𝐹 × 𝐺) (cid:27) Tan(𝐹 ) × Tan(𝐺).
Proof sketch. First, we construct mappings in both directions: We easily have the product
projections Tan(𝐹 × 𝐺) → Tan(𝐹 ) and Tan(𝐹 × 𝐺) → Tan(𝐺). Conversely, given (Tan(𝐹 ) ×
Tan(𝐺))(Γ), we get Δ1 and Δ2 with 𝑓 : Γ (cid:123) Δ2
2 and 𝐹 (Δ1) and 𝐺 (Δ2). Taking
Δ ≜ Δ1 × Δ2, we can define ℎ(𝛾) ≜ ((𝑥, 𝑦), (𝑑𝑥, 𝑑𝑦)) where (𝑥, 𝑑𝑥) = 𝑓 (𝛾) and (𝑦, 𝑑𝑦) = 𝑔(𝛾).
2 : 𝐺 (Δ2) → 𝐺 (Δ1 × Δ2), we can produce
Using the pullbacks 𝜋 ∗
Tan(𝐹 × 𝐺)(Γ).

1 : 𝐹 (Δ1) → 𝐹 (Δ1 × Δ2) and 𝜋 ∗

1 and 𝑔 : Γ (cid:123) Δ2

Next, it is possible to confirm that these mappings are mutually inverse, using the fact that
valueWithDer(𝜆(𝑥, 𝑦).(𝑓 (𝑥), 𝑔(𝑦)))((𝑥, 𝑦), (𝑑𝑥, 𝑑𝑦)) = ((𝑓 (𝑥), 𝑔(𝑥)), (𝑓 ′(𝑥, 𝑑𝑥), 𝑔′(𝑦, 𝑑𝑦))),

together with general properties of limits and functoriality of Tan, 𝐹 , and 𝐺.

□

The primitive tangent_R : Tan ℜ (cid:27) ℜ * ℜ that implements the following isomorphism

(for the special case of ℜ):

Proposition 5.2. We have the equivalence Tan(𝐴) (cid:27) 𝐴2, where · is the Yoneda embedding.
Proof.

Tan(𝐴)(Γ) (cid:27)

∫ Δ

(Γ (cid:123) Δ2) × (Δ (cid:123) 𝐴).

𝜆𝑆 : Computable Semantics for Differentiable Programming with Higher-Order Functions and Datatypes

3:21

Given 𝑓 : 𝐴2(Γ) = Γ (cid:123) 𝐴2, we can take Δ = 𝐴 and use (𝑓 , id). Given an element of Tan(𝐴)(Γ), i.e.,
some Δ and 𝑓 : Γ (cid:123) Δ2 and 𝑔 : Δ (cid:123) 𝐴, then valueWithDer(𝑔) ◦ 𝑓 : 𝐴2(Γ).
□

Note that we have tangentValue ◦ tangent_R = fst, i.e., the first component is the base point

and the second is the infinitesimal perturbation.

Note that the types to represent isomorphisms of tangent bundles are not necessarily iso-
morphisms in 𝜆𝑆 : the type (cid:27) just corresponds to pairs of maps back and forth. The primitive
tangentTo_R A : Tan (A → ℜ) (cid:27) (A → ℜ) * (A → ℜ), in which tangent bundles
distribute over functions into ℜ, implements mappings that are an isomorphism only when we
restrict ℜ to R (rather than all of R):

Proposition 5.3. There is an isomorphism Tan(𝐴 ⇒ R) (cid:27) 𝐴 ⇒ Tan(R) (cid:27) 𝐴 ⇒ R2.

Proof. First, we construct the mappings in each direction. Note that these types are:

Tan(𝐴 ⇒ R)(Γ) (cid:27)

∫ Δ

(Γ (cid:123) Δ2) ×

∫

(𝑋 (cid:123) Δ) → 𝐴(𝑋 ) → (𝑋 (cid:123) R)

(𝐴 ⇒ R2)(Γ) (cid:27)

∫

𝑋

𝑋
(𝑋 (cid:123) Γ) → 𝐴(𝑋 ) → (𝑋 (cid:123) R2)

Given 𝑓 : (𝐴 ⇒ R2), we take Δ = Γ × R, and use

∃Γ × R. (𝜆𝛾 . ((𝛾, 0), (0, 1)), Λ𝑋 . 𝜆(𝑒 : 𝑋 (cid:123) Γ × R). 𝜆(𝑎 : 𝐴(𝑋 )).
let (𝑔, 𝑑𝑔) = 𝑓 (𝑋, 𝜋1 ◦ 𝑒, 𝑎) in 𝜆𝑥 : 𝑋 . 𝑔(𝑥) + 𝜋2(𝑒 (𝑥)) · 𝑑𝑔(𝑥))

Conversely, given a member of Tan(𝐴 ⇒ R)(Γ), i.e., a Δ with 𝑑 : Γ (cid:123) Δ2 and 𝑓 : ∫

𝑋 (𝑋 (cid:123) Δ) →

𝐴(𝑋 ) → (𝑋 (cid:123) R), we can provide
Λ𝑋 . 𝜆(𝑒 : 𝑋 (cid:123) Γ). 𝜆(𝑎 : 𝐴(𝑋 )). valueWithDer(𝑓 (𝑋, 𝜋1⟨𝑓 (𝑋, 𝜋1 ◦ 𝑑 ◦ 𝑒, 𝑎), 𝑓 (𝑋, 𝜋2 ◦ 𝑑 ◦ 𝑒, 𝑎)⟩)).
Next, we must confirm that these mappings are mutually inverse. This boils down to the basic
□

identity 𝑓 ′(𝑥; 𝑣) =

𝜕 (𝑓 (𝑥+𝑡 ·𝑣))
𝜕𝑡

|𝑡 =0 .

Note that it is not an isomorphism for all of ℜ, because we rely on the algebraic law 𝑥 + 0 · 𝑦 = 𝑥

for all 𝑦, but if we allow 𝑦 ∈ R \ R, there is the counterexample 𝑥 + 0 · ⊥ = ⊥.

5.4 Consistency

Proposition 5.4 (Consistency of differentiation in the higher-order language). Given
any term Γ ⊢ 𝑒 : 𝜏 in 𝜆𝑆 where Γ is a context of all ground types and 𝜏 is a ground type, then ⟦𝑒⟧HAD
is equivalent to some first-order smoothish map 𝑓 , i.e., successively consistent derivative tower.

Proof. Since the Yoneda embedding is full and faithful, first-order terms in HAD correspond to
□

morphisms in AD, so this statement reduces to Proposition 4.10.

6 COMPUTABILITY AND NUMERICALLY-SOUND IMPLEMENTATION
It is not obvious that the categorical semantics of 𝜆𝑆 we present in §4-5 is actually implementable
(in a sound manner). The semantics critically uses reals and real arithmetic, rather than some
approximation like floating point (which would fail to give even the most basic equalities such as
1/5 + 2/5 = 3/5). And value-level definitions of higher-order primitives in 𝜆𝑆 are expressed in terms
of mathematical operations for integration, optimization, and root finding applied to arbitrary
continuous maps. In fact, our semantic development is computable, and we have implemented it in
a numerically sound manner as an embedded DSL in Haskell.

3:22

Benjamin Sherman, Jesse Michel, and Michael Carbin

Our semantics can be developed constructively and interpreted within the internal language
of another topos, which we call 𝜆𝐶 , in order to provide a computable interpretation. We base
𝜆𝐶 on MarshallB [Sherman et al. 2019]. Our implementation of 𝜆𝑆 more-or-less directly follows
interpreting the semantics of 𝜆𝑆 within 𝜆𝐶 and in turn implementing 𝜆𝐶 in Haskell.

𝜆𝐶 is a topos of sheaves over a Cartesian monoidal category that we call CTop. CTop is a
category of computably presented topological spaces and computable continuous maps. 𝜆𝐶 is the
topos of sheaves over CTop with the open cover topology (along the lines of [Fourman 1984]).

What results is a stack of languages: 𝜆𝑆 reducing to AD, implemented in 𝜆𝐶 , which reduces
to CTop, which carries the final executable content of ground terms. We can view it like a stack
of metaprogramming languages on top of CTop: ultimately, when a closed term of 𝜆𝑆 (or any
other language in the stack) of ground type is evaluated and displayed as a sequence of improving
approximations, it is in fact a closed term of CTop, i.e., a computable point of a topological space.

Semantics of 𝜆𝐶 and Implications for 𝜆𝑆 . 𝜆𝐶 is a language whose types are (generalized) topological
spaces with computable structure and whose functions are (generalized) computable continuous
maps. 𝜆𝐶 permits all the higher-order functions and higher-order types that we will seek to define
in 𝜆𝑆 and enables their computation to arbitrary precision. This section describes 𝜆𝐶 by example. In
𝜆𝐶 , the type ℜ in 𝜆𝐶 represents the interval reals R. One closed term, or value, of type ℜ is sqrt
2. A value of ℜ represents a point of the space R and is computationally represented by streams of
increasingly precise approximations (i.e., monotone with respect to ⊑):

> sqrt 2 : ℜ

[1.4142135619, 1.4142135624]
[1.414213562370, 1.414213562384]
[1.4142135623729, 1.4142135623733]
...

Note that these streams of increasingly precise approximations can be used to provide the
arbitrary-precision interface where one asks for a precision tolerance and gets a result. Each
interval [𝑥, 𝑥], where 𝑥 ∈ {−∞} ∪ D, 𝑥 ∈ D ∪ {∞}, has either infinite or dyadic-rational (D =
{𝑘/2𝑛 | 𝑘 ∈ Z, 𝑛 ∈ N}) endpoints and represents partial information about sqrt 2: the first
component represents a rational lower bound (with −∞ being a vacuous bound) and the second an
upper bound (with ∞ vacuous). 𝜆𝐶 is sound in the sense that these bounds are guaranteed to hold
of the true value. Two closed terms of ℜ in 𝜆𝐶 are considered equivalent if their streams always
overlap, even if the streams are not identical. For instance, (sqrt 2) 2 = 2:

> (sqrt 2)2

> 2

[1.9999999986, 2.0000000009]
[1.999999999985, 2.000000000058]
[1.9999999999991, 2.0000000000009]
...

[2.0000000000, 2.0000000000]
[2.000000000000, 2.000000000000]
[2.0000000000000, 2.0000000000000]
...

The equivalence means that one can substitute (sqrt 2)2 for 2 within any program without
affecting its meaning. In contrast, the floating-point computation for many languages and CPUs
returns 2.0000000000000004, which is not 2 and does not itself indicate a larger range of possible
values that includes 2, and would not validate the equation (sqrt 2)ˆ2 = 2.

First-order functions in 𝜆𝐶 are stream transformers of their approximations. For instance, applying

the squaring function (-) 2 : ℜ → ℜ to sqrt 2 yields the following result:

𝜆𝑆 : Computable Semantics for Differentiable Programming with Higher-Order Functions and Datatypes

3:23

> sqrt 2 : ℜ

> (sqrt 2)2 : ℜ

[1.4142135619, 1.4142135624]
[1.414213562370, 1.414213562384]
[1.4142135623729, 1.4142135623733]
...

[1.9999999986, 2.0000000009]
[1.999999999985, 2.000000000058]
[1.9999999999991, 2.0000000000009]
...

In this case, the squaring function squares each input interval to produce output intervals. The
computation is continuous in the sense that the computation of each interval result of (sqrt 2) 2
needs only an interval approximation of sqrt 2. First-order functions such as (-) 2 are continuous
maps, meaning that in order to approximate the output to any finite level of precision, it suffices to
inspect the input to only a finite level of precision.

Implementing Higher-Order Primitives. The value-level definitions of higher-order primitives in 𝜆𝑆
are expressed in terms of mathematical operations for integration, optimization, and root finding. It’s
not obvious that these are computable. However, MarshallB [Sherman et al. 2019] demonstrates how
to endow a language with computable implementations of Riemannian integration, maximization
over compact sets, as well as a Dedekind cut primitive that is essentially equivalent to the root
finding of cutRoot and can be used to implement the root finding of firstRoot. We were able to
implement these MarshallB primitives in 𝜆𝐶 and use them to implement the higher-order primitives
in 𝜆𝑆 .

Haskell Implementation. We implemented 𝜆𝑆 as an embedded language within Haskell. Because
R𝑛 and R𝑛 are representable within CTop, we actually implement AD directly using CTop within
Haskell, rather than working internally to 𝜆𝐶 . We implement CTop using an interval-arithmetic
library that in turn uses MPFR [Fousse et al. 2007], a library for multi-precision floating-point
arithmetic. We include this implementation and all the code examples as supplementary material,
and will make it publicly available. See the readme file for more information about the code.

Computability and Numerical Soundness. The semantics for 𝜆𝑆 supports a realistic machine model
for computing real-valued results to arbitrary precision. This is in contrast to semantics that permit
Boolean-valued comparison of real numbers, and computational models like Real RAM, in which
a machine can compare real numbers in constant time. When algorithms are designed based on
such models but implemented with floating-point arithmetic, those implementations may fail to be
robust to floating-point error (e.g., [Kettner et al. 2008]). In contrast, the continuity inherent in 𝜆𝑆 ’s
semantics provides a robustness guarantee: arbitrary-precision approximations of the output can
be produced by inspecting only finite-precision approximations of the input.

7 HIGHER-ORDER DATATYPES AND LIBRARIES
This section demonstrates the unique expressivity and computability of 𝜆𝑆 . We use the novel higher-
order primitives available in 𝜆𝑆 – including integration, optimization, and root-finding – to build
libraries for constructing and computing with three different higher-order datatypes: probability
distributions (and measures), implicit surfaces, and generalized parametric surfaces. Since these
libraries are implemented in 𝜆𝑆 , they are differentiable (arbitrarily many times). For each library,
we compute an example differentiation task. Fig. 7 shows a high-level overview of each example.
We now detail the implementation of each of the libraries and provide the implementations for
each of the corresponding examples.

3:24

Benjamin Sherman, Jesse Michel, and Michael Carbin

1

0

0

𝑥

1

(a) Probability distributions:
How does the mean and vari-
ance of the uniform distribution
change as you weight its mass
to tilt more towards higher
values and away from lower
values?

1

0

0

1

Implicit

(b)
surfaces: A ray
of light from a source above
bounces off a circle before hit-
ting a camera. How does the
brightness change when the cir-
cle is moved up?

(c) Generalized parametric sur-
faces: How does the Hausdorff
distance between the quarter
circle and the “L” shape change
as the quarter circle is moved
up?

Fig. 7. Three example differentiation problems we will express and compute with libraries in 𝜆𝑆 .

7.1 Probability Distributions (and Measures)
Probability is central to many machine-learning applications. Loss functions for Bayesian neural net-
works, GANs, etc. involve expectations over probability distributions. However, no previous work
on the semantics of AD supports probability distributions3. The interaction between probabilistic
choice and differentiation is nontrivial, and the lack of a semantic treatment of their interaction has
real consequences for machine-learning practitioners using AD libraries who seek to combine them.
Practitioners often use Monte Carlo sampling to approximate expectations, but because derivatives
cannot be propagated through the samplers in common frameworks such as PyTorch and Tensor-
Flow, code that looks correct and produces appropriate approximations of its value-level output can
end up producing incorrect derivatives when AD is applied (as mentioned in the introduction). This
common pitfall, which can be difficult to detect, necessitates the reparameterization trick, where
code is rewritten such that samplers do not depend on any parameters that are to be differentiated.
𝜆𝑆 can represent a monad of probability distributions P, making it the first language semantics
to support differentiation through probabilistic choice, including through distributions such as the
uniform distribution on the unit interval. Supporting probability distributions is hard because they
must involve higher-order functions: expectations are higher-order functions P (𝐴)×(𝐴 → R) → R,
as is the monadic bind operator P (𝐴) × (𝐴 → P (𝐵)) → P (𝐵) that supports compositional
construction of complex probability distributions from simple ones.

A 𝜆𝑆 Library for Probability Distributions and Measures. Probability distributions, measures, and

distributions (in the sense of generalized functions) can all be described as integrals,

type Integral A = (A → ℜ) → ℜ,

detailed in Fig. 8. Integrals are functions 𝑖 : (𝐴 → R) → R which are linear in their arguments.
Measures are those integrals 𝑖 satisfying 𝑖 (𝑓 ) ≥ 0 whenever 𝑓 (𝑥) ≥ 0 for all 𝑥 ∈ R. Probability
distributions are those measures 𝑖 satisfying 𝑖 (𝜆𝑥 . 1) = 1; the integral for a probability distribution
computes the expectation of a real-valued function under that distribution.

Example. What happens if we make an infinitesimal perturbation to the uniform distribution as

in Fig. 7a? How will its mean and variance change? Differentiation answers these questions.

3While other works can represent expectations over distributions with finite support as sums, this would not work for
distributions with infinite support. Loss functions frequently involve expectations over distributions with infinite support.

𝜆𝑆 : Computable Semantics for Differentiable Programming with Higher-Order Functions and Datatypes

3:25

type Integral A = (A →ℜ) →ℜ

let dirac A (x : A) : Integral A = 𝜆 f : A →ℜ⇒ f x
let bind A B (x : Integral A) (f : A → Integral B) : Integral B

= 𝜆 k : B →ℜ⇒ x (𝜆 a : A ⇒ f a k)

let zero A : Integral A = 𝜆 f : A → Real ⇒ 0
let add A (x y : Integral A) : Integral A = 𝜆 f : A →ℜ⇒ x f + y f
let map A B (f : A → B) (e : Integral A) : Integral B =

𝜆 k : B →ℜ⇒ e (𝜆 x : A ⇒ k (f x))

let factor (x : ℜ) : Integral unit = 𝜆 f : unit →ℜ⇒ f () * x
let measToProb A (e : Integral A) : Integral A = 𝜆 f : A →ℜ⇒ e f / e (𝜆 x : A ⇒ 1)
let bernoulli (p : ℜ) : Integral 𝔅 = 𝜆 f : 𝔅→ℜ⇒ p * f tt + (1 - p) * f ff
let uniform : Integral ℜ = integral01

let total_mass A (mu : Integral A) = mu (𝜆 x : A ⇒ 1)
let mean (mu : Integral ℜ) = mu (𝜆 x : ℜ⇒ x)
let variance (mu : Integral ℜ) = mu (𝜆 x : ℜ⇒ (x - mean mu) 2)

Fig. 8. Integrals and 𝜆𝑆 programs that manipulate them.

0

The uniform distribution over the interval [0, 1] is equivalent to the integral of [0, 1], namely
1𝑑𝑥 = 1 (as any probability distribution

uniform : Integral ℜ = integral01. It satisfies ∫ 1
must), and has mean ∫ 1
0

𝑥𝑑𝑥 = 1/2 and variance ∫ 1

0 (𝑥 − 1/2)2𝑑𝑥 = 1/12.

Next, we must craft a perturbation to consider. There is an isomorphism Tan (Integral A)
(cid:27) Integral A * Integral A, which says that a perturbation to an integral itself has the form
of an integral as well. Hence, our perturbation must also be an integral. In addition, because we
are perturbing a probability distribution, whose total mass must sum to 1, the total mass of our
perturbation must be 0: if we are to increase mass somewhere, we must decrease it elsewhere.
Given these design considerations, consider the following perturbation to the uniform distribution
that makes 1 more likely, 0 less likely, 1/2 equally likely as before, and interpolates between these:4

let change : Integral ℜ = 𝜆 f : ℜ→ℜ⇒ integral01 (𝜆 x : ℜ⇒ (x - 1/2) * f x)

The perturbation is an integral with total mass 0: ∫ 1
Returning to our question of how this perturbation changes the mean and variance of uniform, for
convenience let der : (Integral A → ℜ) → Integral A → Integral A → ℜ compute the
derivative of its argument at a point and infinitesimal perturbation, using the appropriate coercions
and projections to and from tangent spaces.5 Since mean is linear, its derivative is independent of
the current value and is just the original mean function applied to the infinitesimal perturbation:

0 (𝑥 − 1/2)𝑑𝑥 = 0.

der mean uniform change

= mean change
= integral01 (𝜆 x : ℜ⇒ (x - 1/2) * x)
= 1/12

And indeed, that’s what we compute:

4Fig. 7a shows a schematic of this perturbation.
5let der f x dx = snd (tangetTo_R.to (tangent f (tangetTo_R.from (x, dx))))

3:26

Benjamin Sherman, Jesse Michel, and Michael Carbin

type Surface A = A →ℜ

let circle (c : ℜ2) (r : ℜ) : Surface (ℜ2) =

𝜆 x : ℜ2 ⇒ r2 - (x[0] - c[0])2 - (x[1] - c[1]) 2

let halfplane A (normal : ℜ2) : Surface (ℜ2) = 𝜆 x : ℜ2 ⇒ dot normal x
let union A (s s’ : Surface A) : Surface A = 𝜆 x : A ⇒ max (s x) (s’ x)
let intersection A (s s’ : Surface A) : Surface A = 𝜆 x : A ⇒ min (s x) (s’ x)
let complement A (s : Surface A) : Surface A = 𝜆 x : A ⇒ - (s x)

Fig. 9. A 𝜆𝑆 library for implicit surfaces.

eps=1e-3> der mean uniform change

[0.0829, 0.0837]

However, variance is nonlinear, so its derivative does depend on the current point. Let’s compute
it and then reason about the answer:

eps=1e-2> der variance uniform change

[-0.005, 0.004]

We can reason about the change in the variance with the laws about derivatives, just as we would
in first-order cases:

der variance uniform change

= der (𝜆 mu : Integral ℜ⇒ mu (𝜆 x : ℜ⇒ x2) - (mean mu) 2) uniform change
= change (𝜆 x : ℜ⇒ x 2) - 2 * mean uniform * mean change
= integral01 (𝜆 x : ℜ⇒ (x-1/2)*x2) - 2 * 1/2 * 1/12
= 1/12 - 1/12
= 0

So it turns out that this infinitesimal perturbation will actually not change the variance.

7.2 Implicit Surfaces and Root-Finding
§2 and Fig. 1 presented a library for implicit surfaces and a function for performing ray tracing on
scenes represented by implicit surfaces.

Fig. 9 presents a library for constructing implicit surfaces. An implicit surface is a representation
of a surface (such as a sphere or plane) with the zero-set of a differentiable function 𝑓 : R𝑛 → R
(where usually we consider 𝑛 = 3 for 3-dimensional space). Whether 𝑓 (𝑥, 𝑦) is positive, negative,
or zero indicates whether (𝑥, 𝑦) is inside, outside, or on the border of the surface, respectively. The
angle at which a ray deflects is determined by the surface normal at the location where the ray hits
the surface, which is the vector that is orthogonal to the plane that is tangent to the surface.

In 𝜆𝑆 , we can represent implicit surfaces as type Surface A = A → ℜ. Fig. 9 presents a small
library for constructing implicit surfaces. The Boolean operations of Constructive Solid Geometry
(CSG) – union, intersection, and complement – are available for these implicit surfaces. Because
𝜆𝑆 permits nonsmooth functions, it is able to represent implicit surfaces that don’t necessarily
correspond to manifolds, such as the union of two spheres that are offset and equally sized. Where
they touch, there is a corner, and thus there is no (unique) surface normal.

𝜆𝑆 : Computable Semantics for Differentiable Programming with Higher-Order Functions and Datatypes

3:27

Our smooth ray tracer, shown in Fig. 1c, renders the image of an implicit surface with a single
light source and a Lambertian reflectance model, computing the angle at which light reflects off of
the surface using automatic differentiation. The code in Fig. 1c reflects the contributions of Niemeyer
et al. [2020], who use a differentiable ray-tracing renderer to learn implicit 3D representations of
surfaces, noting their “key insight is that depth gradients can be derived analytically using the
concept of implicit differentiation.”

We can implement a smooth (and thus differentiable) ray tracer for implicit surfaces in 𝜆𝑆 in just

a few lines of code, and the use of implicit differentiation automatically falls out.

7.3 Generalized Parametric Surfaces and Optimization
We now build a library within 𝜆𝑆 for constructing shapes and computing operations on them. For
instance, we can represent the quarter disk and unit square in Fig. 7c as shapes and compute the
√
Hausdorff distance between them, which equals

2 − 1, as:

eps=1e-3> hausdorffDist R2Dist lShape (quarterCircle 0)

[0.4138, 0.4145]

We can also compute derivatives, such as the infinitesimal perturbation in the Hausdorff distance

that would result if the quarter circle were to infinitesimally move up by a unit magnitude:

eps=1e-1> deriv (𝜆 y : ℜ ⇒ hausdorffDist R2Dist lShape (quarterCircle y))
0

[-0.752, -0.664]

This application is admittedly more speculative in its practical applications, but it demonstrates
a novel domain in which we can define and compute derivatives. We will now explain how this
library for shapes works.

We represent these generalized parametric surfaces as maximizers, represented in 𝜆𝑆 as

type Maximizer A = (A → ℜ) → ℜ.
Maximizers are functions 𝐹 : (𝐴 → R) → R that satisfy the algebraic laws 𝐹 (𝜆𝑥 : 𝐴. 𝑘) = 𝑘
for all 𝑘 ∈ R and 𝐹 (𝜆𝑥 : 𝐴. max(𝑓 (𝑥), 𝑔(𝑥))) = max(𝐹 (𝑓 ), 𝐹 (𝑔)) (analogously to how integrals
are functions that satisfy the algebraic laws of linearity). A generalized parametric surface k :
Maximizer A, when applied to a function f : A → ℜ, returns the maximum value that f attains
on the region represented by k.

Fig. 10 shows an excerpt of the library for generalized parametric surfaces. Note that general-
ized parametric surfaces shapes form a monad (representing nondeterminism), with point and
indexedUnion as return and bind, yielding a programming model for constructing shapes.

Returning to the earlier Hausdorff-distance example, note that the maximal distance on the
“L” shape occurs at the corner point, which is represented twice, as the endpoint of each line;
thus, a maximum is taken over two equal distances. In [Abadi and Plotkin 2020], because the
maximum operator is defined with a partial conditional statement, the result — not to mention the
derivative — would be undefined. Because both the values and derivatives are the same for the two
representations of this corner point, the derivative is a maximal element. Also note that we need
second derivatives to compute the derivative of the Hausdorff distance, due to the use of max01.

8 DISCUSSION
In this section, we discuss the capability of 𝜆𝑆 to represent control flow as well as the opportunity
to soundly speed up execution of higher-order primitives using derivative information.

3:28

Benjamin Sherman, Jesse Michel, and Michael Carbin

type Maximizer A = (A →ℜ) →ℜ
let point A (x : A) : Maximizer A = 𝜆 f : A →ℜ⇒ f x
let indexedUnion A B (ka : Maximizer A) (kb : A → Maximizer B) : Maximizer B =

𝜆 f : B →ℜ⇒ ka (𝜆 a : A ⇒ kb a f)

let union A (k1 k2 : Maximizer A) : Maximizer A =

𝜆 f : A →ℜ⇒ max (k1 f) (k2 f)

let map A B (g : A → B) (k : Maximizer A) : Maximizer B =

𝜆 f : B →ℜ⇒ k (𝜆 a : ℜ⇒ f (g a))

let sup A (k : Maximizer A) (f : A →ℜ) : ℜ = k f
let inf A (k : Maximizer A) (f : A →ℜ) : ℜ = - k (𝜆 x : A ⇒ - (f x))
let hausdorffDist A (d : A → A →ℜ) (k1 k2 : Maximizer A) : ℜ =

max (sup k1 (𝜆 x1 : A ⇒ inf k2 (𝜆 x2 : A ⇒ d x1 x2)))
(sup k2 (𝜆 x2 : A ⇒ inf k1 (𝜆 x1 : A ⇒ d x1 x2)))

let unitInterval : Maximizer ℜ = max01
let quarterCircle (y : ℜ) : Maximizer (ℜ2) = map

(𝜆 theta : ℜ⇒ (cos (pi / 2 * theta), sin (pi / 2 * theta) + y))
unitInterval

let lShape : Maximizer (ℜ2) =

union (map (𝜆 x : ℜ→ (x, 1)) unitInterval)
(map (𝜆 y : ℜ⇒ (1, y)) unitInterval)

let R2Dist (a b : ℜ2) : ℜ = sqrt ((a[0] - b[0])2 + (a[1] - b[1])2)

Fig. 10. Generalized parametric surfaces and 𝜆𝑆 programs that manipulate them.

8.1 Control Flow: Conditionals and Recursion
𝜆𝑆 supports discrete spaces, including in particular the Booleans B and any well-founded set
(such as the natural numbers). The recursion principles for these yield, respectively, if-then-else
expressions and well-founded recursion. These control-flow expressions must be independent of
“continuous data”: all maps from connected spaces to discrete spaces are constant. This property
defines connected spaces. Connected spaces include all vector spaces, such as R𝑛. Di Gianantonio
and Edalat [2013] explain some particular issues that demonstrate why implementing piecewise-
differentiable functions with branching is problematic.

8.2 Optimizing Higher-Order Primitives with Derivative Information
We can also use the fact that functions in 𝜆𝑆 come equipped with all their derivatives to opportunis-
tically speed up some operations. For instance, consider applying cut_root to some function 𝑓 .
Its value-level definition naturally maps to a bisection-like algorithm on the values of 𝑓 . However,
since we have access to 𝑓 (1) , we can use a variation of Newton’s method generalized to interval
arithmetic to speed up the convergence drastically, and indeed we do this in our implementation.
Note that we are guaranteed that this optimization is sound, because consistency of differentiation
ensures that 𝑓 (1) appropriately reflects 𝑓 (0) . It may be the case that 𝑓 (1) returns ⊥ at some points,
or even everywhere, in which case the algorithm falls back on bisection to ensure progress.

Similarly, the literal interpretation of the value-level definition of Riemannian integration in
§5 maps to a quadrature method that uses only the values 𝑓 (0) of 𝑓 . However, the availability of
higher derivatives of 𝑓 makes it possible to use interval-based versions of higher-order integration
methods, which can also drastically speed up the convergence. We do not use these higher-order
methods by default in our actual implementation.

𝜆𝑆 : Computable Semantics for Differentiable Programming with Higher-Order Functions and Datatypes

3:29

Table 1. Summary of other approaches to semantics of differentiable programming and their properties.
Higher-order derivatives: The differentiation operator can be iterated arbitrarily many times (when applied
to smooth functions). Higher-order functions: A concrete test: is twice(𝑓 : R → R)(𝑥 : R) : R ≜ 𝑓 (𝑓 (𝑥))
admitted? Non-differentiable functions: Some nondifferentiable functions are admitted. A concrete test: is
max : R2 → R admitted? “Clarke derivative” indicates that locally Lipschitz functions support derivatives in
the sense of Clarke derivatives or L-derivatives [Edalat and Lieutier 2004], whereas “partiality” indicates that
nondifferentiable maps are supported by considering them to be partial at their discontinuities.

higher-order
functions
✓
Vákár et al. [2018]
Di Gianantonio and Edalat [2013] ✓
✗
Elliott [2018]
✗
Abadi and Plotkin [2020]
✗
Sigal [2018]
✓
Vytiniotis et al. [2019]
✓
Huot et al. [2020]
✓
Ehrhard and Regnier [2003]
✓
𝜆𝑆 (this work)

higher-order
derivatives
✓
✗
✗
✓
✓
✗
✓
✓
✓

nondifferentiable func-
tions
✗
✓ (Clarke derivative)
✗
✓ (partiality)
✓ (partiality)
✗
✗
✗
✓ (Clarke derivative)

9 RELATED WORK
Table 1 illustrates the unique set of features that 𝜆𝑆 provides and their relationship to other
approaches to AD semantics. We note that these other approaches also have features that 𝜆𝑆 lacks.
Di Gianantonio and Edalat [2013] describe a programming language for nonexpansive (i.e.,
Lipschitz constant 1) functions on the interval [−1, 1] with a differentiation operator that applies
to functions from [−1, 1] to [−1, 1]. The semantics of this differentiation operator are that of the
L-derivative [Edalat 2008; Edalat and Lieutier 2004], which is closely related to the Clarke Jacobian
definition we use. Their domain-theoretic account ensures computability: in theory, results can be
computed to arbitrary precision. Their semantics is fundamentally limited to first-order derivatives:
their interval type denotes [−1, 1] × [−1, 1], corresponding to a dual-number representation, baking
in that limited capability. It is unclear how that representation could be generalized directly to
permit higher-order differentiation and appropriately handle nested differentiation (without the
perturbation confusion [Siskind and Pearlmutter 2005] that may arise with nested differentiation).
Elliott [2008] presents a data type for representing smooth maps, where a smooth map 𝑓 is
represented by the collection of its 𝑘th derivatives for all 𝑘. Elliott [2008] defines the derivatives
of some arithmetic functions as well as some categorical operations, though the definition of
composition of smooth maps is incorrect. We support higher-order derivatives by adapting this
representation for the Clarke derivative.

Vákár et al. [2018] presents the semantics of a differentiable programming language that supports
higher-order functions and higher-order derivatives using the quasitopos of diffeological spaces. As
a quasitopos, the semantics supports higher-order functions and quotient types. Vákár et al. [2018]
show an internal derivative operator that can be applied to any function of any type, and thus can
be applied repeatedly for higher-order derivatives. We based our internal derivative operator on
theirs. Functions such as max that are not smooth are not admissible. It is not made clear how one
could implement a differentiable programming language supporting the expressive possibilities
suggested by the semantics.

None of the works in Table 1 describe higher-order functions for root-finding, optimization, or
integration, nor do they describe datatypes for implicit surfaces, compact shapes, or probability

3:30

Benjamin Sherman, Jesse Michel, and Michael Carbin

distributions. Edalat and Lieutier [2004] describe an integration operator in a domain-theoretic
framework for differential calculus, but it does not handle higher-order derivatives. Sherman et al.
[2019] describe computable higher-order functions and libraries for root-finding, optimization, and
integration, but does not admit differentiation of any sort.

We follow Sherman et al. [2019] in our approach to computability. We are unaware of any system
that computes arbitrary-precision derivatives (given the definition of the function) in any capacity.

10 CONCLUSION
This paper demonstrates how to compute and make sense of derivatives of higher-order functions,
such as integration, optimization, and root-finding and at higher-order types, such as probability
distributions, implicit surfaces, and generalized parametric surfaces. Our libraries and case stud-
ies model existing differentiable algorithms, for instance, a differentiable ray tracer for implicit
surfaces, without requiring any user-level differentiation code, in addition to demonstrating new
differentiable algorithms, such as computing derivatives of the Hausdorff distance of generalized
parametric surfaces. Ideally, the ideas 𝜆𝑆 demonstrates may enable differentiable programming
frameworks to support the new abstractions and expressivity suggested by this paper.

ACKNOWLEDGMENTS
We thank Eric Atkinson, Tej Chajed, Alexander Lew, Alex Renda, and David Spivak, as well as the
anonymous reviewers, for their helpful feedback and discussions. This work was supported in part
by the Office of Naval Research (ONR-N00014-17-1-2699). Any opinions, findings, and conclusions
or recommendations expressed in this material are those of the author and do not necessarily reflect
the views of the Office of Naval Research.

REFERENCES
Martín Abadi and Gordon D. Plotkin. A simple differentiable programming language. In Principles of Programming Languages,

2020.

Matan Atzmon, Niv Haim, Lior Yariv, Ofer Israelov, Haggai Maron, and Yaron Lipman. Controlling neural level sets. In

Advances in Neural Information Processing Systems. 2019.

Shaojie Bai, J Zico Kolter, and Vladlen Koltun. Deep equilibrium models. In Advances in Neural Information Processing

Systems, 2019.

J Daniel Christensen and Enxin Wu. Tangent spaces and tangent bundles for diffeological spaces. American Mathematical

Society, 2017.

Frank H Clarke. Optimization and nonsmooth analysis. 1990.
Pietro Di Gianantonio and Abbas Edalat. A language for differentiable functions. In Foundations of Software Science and

Computational Structures, 2013.

Abbas Edalat. A continuous derivative for real-valued functions. In New Computational Paradigms. 2008.
Abbas Edalat and André Lieutier. Domain theory and differential calculus (functions of one variable). Mathematical

Structures in Computer Science, 14(6), 2004.

Thomas Ehrhard and Laurent Regnier. The differential lambda-calculus. Theoretical Computer Science, 309(1-3), 2003.
Conal Elliott. Higher-dimensional, higher-order derivatives, functionally. 2008. URL http://conal.net/blog/posts/higher-

dimensional-higher-order-derivatives-functionally.

Conal Elliott. The simple essence of automatic differentiation. In International Conference on Functional Programming, 2018.
In Advances in Neural
Mikhail Figurnov, Shakir Mohamed, and Andriy Mnih.

Implicit reparameterization gradients.

Information Processing Systems. 2018.

Michael P Fourman. Continuous truth I: Non-constructive objects. Studies in Logic and the Foundations of Mathematics, 112,

1984.

Laurent Fousse, Guillaume Hanrot, Vincent Lefèvre, Patrick Pélissier, and Paul Zimmermann. MPFR: A multiple-precision

binary floating-point library with correct rounding. ACM Transactions on Mathematical Software, 33(2), 2007.

Mathieu Huot, Sam Staton, and Matthijs Vákár. Correctness of automatic differentiation via diffeologies and categorical

gluing. In Foundations of Software Science and Computation Structures, 2020.

𝜆𝑆 : Computable Semantics for Differentiable Programming with Higher-Order Functions and Datatypes

3:31

Eric Jang, Shixiang Gu, and Ben Poole. Categorical reparameterization with gumbel-softmax. International Conference on

Learning Representations, 2017.

Martin Jankowiak and Fritz Obermeyer. Pathwise derivatives beyond the reparameterization trick. In International Conference

on Machine Learning, 2018.

Lutz Kettner, Kurt Mehlhorn, Sylvain Pion, Stefan Schirra, and Chee Yap. Classroom examples of robustness problems in

geometric computations. Computational Geometry, 40(1), 2008.

Tzu-Mao Li, Miika Aittala, Frédo Durand, and Jaakko Lehtinen. Differentiable Monte Carlo ray tracing through edge

sampling. In Special Interest Group on Computer Graphics and Interactive Techniques, 2018.

Jonathan Lorraine, Paul Vicol, and David Duvenaud. Optimizing millions of hyperparameters by implicit differentiation.

arXiv preprint arXiv:1911.02590, 2019.

Christian A. Naesseth, Francisco J. R. Ruiz, Scott W. Linderman, and David M. Blei. Reparameterization gradients through

acceptance-rejection sampling algorithms. In Artificial Intelligence and Statistics, 2017.

Michael Niemeyer, Lars Mescheder, Michael Oechsle, and Andreas Geiger. Differentiable volumetric rendering: Learning

implicit 3D representations without 3D supervision. In Computer Vision and Pattern Recognition, 2020.

Benjamin Sherman, Jesse Michel, and Michael Carbin. Sound and robust solid modeling via exact real arithmetic and

continuity. In International Conference on Functional Programming, 2019.

Jesse Sigal. Denotational semantics for differentiable programming with manifolds. In Student Research Competition at the

Internation Conference on Functional Programming, 2018.

Jeffrey Mark Siskind and Barak A Pearlmutter. Perturbation confusion and referential transparency: Correct functional
implementation of forward-mode ad. Workshop on Implementation and Application of Functional Languages, 2005.
Dimitrios Vytiniotis, Dan Belov, Richard Wei, Gordon Plotkin, and Martin Abadi. The differentiable curry. In Neural

Information Processing Systems Workshop Program Transformations, 2019.

Matthijs Vákár, Ohad Kammar, and Sam Staton. Diffeological spaces and semantics for differential programming. In Domains,

2018. URL https://andrejbauer.github.io/domains-floc-2018/slides/Matthijs-Kammar-Staton.pdf.

Yuanhao Wang, Guodong Zhang, and Jimmy Ba. On solving minimax optimization locally: A follow-the-ridge approach. In

International Conference on Learning Representations, 2020.

