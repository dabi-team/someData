3

1
2
0
2

r
p
A
4
1

]
L
P
.
s
c
[

2
v
7
1
0
8
0
.
7
0
0
2
:
v
i
X
r
a

ğœ†ğ‘†: Computable Semantics for Differentiable Programming
with Higher-Order Functions and Datatypes

BENJAMIN SHERMAN, MIT, USA
JESSE MICHEL, MIT, USA
MICHAEL CARBIN, MIT, USA
Deep learning is moving towards increasingly sophisticated optimization objectives that employ higher-order
functions, such as integration, continuous optimization, and root-finding. Since differentiable programming
frameworks such as PyTorch and TensorFlow do not have first-class representations of these functions, devel-
opers must reason about the semantics of such objectives and manually translate them to differentiable code.
We present a differentiable programming language, ğœ†ğ‘† , that is the first to deliver a semantics for higher-order
functions, higher-order derivatives, and Lipschitz but nondifferentiable functions. Together, these features
enable ğœ†ğ‘† to expose differentiable, higher-order functions for integration, optimization, and root-finding as
first-class functions with automatically computed derivatives. ğœ†ğ‘† â€™s semantics is computable, meaning that
values can be computed to arbitrary precision, and we implement ğœ†ğ‘† as an embedded language in Haskell.

We use ğœ†ğ‘† to construct novel differentiable libraries for representing probability distributions, implicit
surfaces, and generalized parametric surfaces â€“ all as instances of higher-order datatypes â€“ and present case
studies that rely on computing the derivatives of these higher-order functions and datatypes. In addition to
modeling existing differentiable algorithms, such as a differentiable ray tracer for implicit surfaces, without
requiring any user-level differentiation code, we demonstrate new differentiable algorithms, such as the
Hausdorff distance of generalized parametric surfaces.

CCS Concepts: â€¢ Mathematics of computing â†’ Arbitrary-precision arithmetic; Continuous functions;
Point-set topology; â€¢ Theory of computation â†’ Categorical semantics.

Additional Key Words and Phrases: Constructive Analysis, Diffeological Spaces, Automatic Differentiation

1 INTRODUCTION
Deep learning is centered on optimizing objectives â„“ : Î˜ â†’ R over some parameter space Î˜ by
gradient descent, following the derivative of â„“ at some particular value ğœƒ âˆˆ Î˜ to move in a direction
that decreases â„“ (ğœƒ ). Before deep-learning practitioners adopted frameworks such as TensorFlow and
PyTorch, creating a new model (i.e., parameter space and objective) was a laborious and error-prone
endeavor, since it involved manually determining and computing the derivative of the objective.
The advent of deep-learning frameworks that provide automatic differentiation (AD)â€”the automated
computation of derivatives of a function given just the definition of the function itselfâ€”has made
creating and modifying models much easier: a user simply writes the objective and its derivative is
computed automatically. As a result, progress in deep learning has rapidly accelerated â€“ a testament
to the value of programming-language abstractions.

However, the creativity of deep-learning practitioners has exceeded the capabilities of current
AD frameworks: practitioners have devised objectives that current AD frameworks cannot handle
directly. A simple example is an objective including an expectation over a probability distribution
whose parameters may vary, like this:

â„“ (ğœƒ ) = Eğ‘¥âˆ¼N (ğœ‡ (ğœƒ ),ğœ 2 (ğœƒ )) [ğ‘“ (ğ‘¥)].
If this â„“ is translated naÃ¯vely to PyTorch, by approximating the expectation with Monte Carlo
sampling, the automatically generated derivative will be incorrect. Numerous algorithms have been

Authorsâ€™ addresses: Benjamin Sherman, MIT, USA, sherman@csail.mit.edu; Jesse Michel, MIT, USA, jmmichel@mit.edu;
Michael Carbin, MIT, USA, mcarbin@csail.mit.edu.

 
 
 
 
 
 
3:2

Benjamin Sherman, Jesse Michel, and Michael Carbin

proposed to compute the derivatives of objectives that average over parameterized probability distri-
butions [Figurnov et al. 2018; Jang et al. 2017; Jankowiak and Obermeyer 2018; Naesseth et al. 2017].
How does one compute derivatives of objectives like these in general? No existing differentiable-
programming semantics has tackled the problem of differentiating through expectations such as
these.

Other objectives are sufficiently complex that they do not even beg an incorrect naÃ¯ve implemen-

tation. Objectives â„“ that optimize over compact sets Î”

â„“ (ğœƒ ) = max
ğ›¿ âˆˆÎ”
arise in adversarial contexts, including adversarial training and generative adversarial networks
(GANs). Conceptually, optimizing this objective with gradient-based techniques requires a semantics
for a differentiable max operation over a compact set, which, to date, has not been covered in the
literature on the semantics of differentiable programs. Devising the appropriate derivative for these
kinds of objectives is an object of current study [Lorraine et al. 2019; Wang et al. 2020].

ğ‘“ (ğœƒ, ğ›¿)

Sometimes, an objective involves root-finding,

â„“ (ğœƒ ) = let ğ‘¥ be such that ğ‘”(ğœƒ, ğ‘¥) = 0 in ğ‘“ (ğœƒ, ğ‘¥).
This arises in learning implicit surfaces, with applications both to learning the decision boundaries
of classifiers as well as to reconstructing surfaces from point-cloud data or other visual data. How
to compute the derivative of objectives like this is a key contribution of several papers [Atzmon
et al. 2019; Bai et al. 2019; Niemeyer et al. 2020].

What do these objectives all have in common? They all involve higher-order functions: their
definitions introduce variables that are subject to integration, optimization, or root-finding. Not only
are these three operations troublesome in practice, but no semantics of differentiable programming
has yet addressed them.

Approach. We present ğœ†ğ‘† , a differentiable programming language that includes higher-order
functions for integration, optimization, and root-finding. A key technical challenge is that these
functions are higher-order and our semantic approach must wed higher-order functions with
higher-order derivatives and nonsmooth functions to encompass these and other modern deep
learning objectives. As a toy example, consider computing the derivative ğ‘“ â€²(0.6) of the function

ğ‘“ (ğ‘) â‰œ

âˆ« 1

0

ReLU(ğ‘¥ âˆ’ ğ‘) ğ‘‘ğ‘¥,

where ReLU(ğ‘¥) = max(0, ğ‘¥). We can compute ğ‘“ â€²(0.6) = âˆ’0.4 with the ğœ†ğ‘† expression

eps=1e-2> deriv (ğœ† c â‡’ integral01 (ğœ† x â‡’ relu (x - c))) 0.6

[-0.407, -0.398]

where there is a type â„œ for real numbers, a function relu : â„œ â†’ â„œ for ReLU, a higher-order
function integral01 : (â„œ â†’ â„œ) â†’ â„œ for integration over the unit interval [0,1], and a
higher-order function for differentiation of real-valued functions deriv : (â„œ â†’ â„œ) â†’ â„œ â†’
â„œ. The result can be queried to any precision, returning an interval guaranteed to include the true
answer. Here, the precision is specified in the prompt as eps=1e-2.

ğœ†ğ‘† is the first language that gives semantics to such an operation and moreover is the first to
support its computation to arbitrary precision. Note that, in order to determine this derivative, we
must evaluate the derivative of the ReLU function everywhere from -0.6 to 0.4, which includes 0,
where ReLU is not (classically) differentiable.

ğœ†ğ‘† : Computable Semantics for Differentiable Programming with Higher-Order Functions and Datatypes

3:3

Our work is unique compared to related work in supporting higher-order functions, higher-
order derivatives, and nondifferentiable functions. We combine the use of Clarke derivatives in
Di Gianantonio and Edalat [2013] to support nondifferentiable functions, the diffeological approach
of VÃ¡kÃ¡r et al. [2018] to support higher-order functions, and the derivative towers of Elliott [2008] to
support higher-order derivatives. Â§9 covers related work in more detail. Merging these techniques
gives us a platform to accomplish the contributions described below.

Contributions. We present ğœ†ğ‘† , a differentiable programming language whose types are (gen-
eralized) smooth spaces and whose functions are (generalized) smooth maps. Our contributions
are:

(1) The first semantics for a differentiable programming language that admits all of the following:
1) higher-order functions (Â§5), 2) higher-order derivatives (Â§4), and 3) Lipschitz but nonsmooth
functions, such as min, max, and ReLU (Â§4).

(2) The first semantics for differentiable integration, optimization, and root-finding (Â§5), enabled

by the features above.

(3) An implementation of this semantics, including implementations for higher-order functions
such as integration (Â§6). Our implementation is based directly on a constructive categorical
semantics that demonstrates how these constructs can be computed to arbitrary precision.
(4) New smooth libraries for constructing and computing on three higher-order datatypes:

probability distributions, implicit surfaces, and generalized parametric surfaces (Â§7).

ğœ†ğ‘† â€™s semantics allows computation with and reasoning about the derivatives of higher-order
functions, such as integration, optimization, and root-finding. ğœ†ğ‘† elucidates foundational princi-
ples for how to program with smooth values in a sound, arbitrarily precise manner, including
which operations are possible to compute soundly and which are not. While in many cases ğœ†ğ‘† is
not practically efficient, in some cases, programs can serve as executable specifications to guide
programming in other frameworks, to validate separately developed systems, and to suggest new
functionality that could be added to other differentiable programming frameworks.

2 AN INTRODUCTION TO ğœ†ğ‘†
We demonstrate ğœ†ğ‘† â€™s core functionality by implementing a simple differentiable ray tracer, an
algorithm that generates an image of a scene as viewed by a camera by tracing how rays of
light emanate from a light source, bounce off the scene, and then enter the cameraâ€™s aperture.
Differentiable ray tracing is a new deep-learning technique that propagates derivatives through
image rendering algorithms, permitting the use of inverse graphics to solve computer-vision
tasks [Li et al. 2018; Niemeyer et al. 2020]. These techniques optimize the parameters of a scene
representation to make the image generated by the ray tracer more closely match a target image.
As a simple example, consider computing the brightness of a particular scene at a particular
direction, using the ğœ†ğ‘† library for representing scenes and a function for performing ray tracing,
both of which we present in Fig. 1:

eps=1e-5> raytrace (circle (1, -3/4) 1) (1, 1) (1, 0)

[2.587289, 2.587299]

Fig. 1a depicts the computation at hand. The camera is located at the origin (0, 0), the circle
is centered at (1, -3/4) and has radius 1, the light source is at (1, 1), and we consider a ray
pointing horizontally to the right from the camera, in the direction (1, 0). The computation
returns an interval and the eps=1e-5 specifies the precision tolerance, such that the interval-valued
result, [2.587289, 2.587299], has a width at most 10âˆ’5. Our implementation guarantees that

3:4

Benjamin Sherman, Jesse Michel, and Michael Carbin

type Surface A = A â†’â„œ

firstRoot : (â„œâ†’â„œ) â†’â„œ ! language primitive

let dot (x y : â„œ2) : â„œ = x[0] * y[0] + x[1] * y[1]
let scale (c : â„œ) (x : â„œ2) : â„œ2 = (c * x[0], c * x[1])
let norm2 (x : â„œ2) : â„œ = x[0]2 + x[1]2
let normalize (x : â„œ2) : â„œ2 = scale (1 / sqrt (norm2 x)) x

(a) A ray of light from a source
above bounces off a circle before
hitting a camera. How does the
brightness change when the cir-
cle is moved up?

deriv : (â„œâ†’â„œ) â†’ (â„œâ†’â„œ) ! library function
let gradient (f : â„œ2 â†’â„œ) (x : â„œ2) : â„œ2 =
(deriv (ğœ† z : â„œâ‡’ f (z, x[1])) x[0],
deriv (ğœ† z : â„œâ‡’ f (x[0], z)) x[1])

(b) Basic definitions used in raytrace below.

! camera assumed to be at the origin
let raytrace (s : Surface (â„œ2)) (lightPos : â„œ2) (rayDirection : â„œ2) : â„œ =

let tStar = firstRoot (ğœ† t : â„œâ‡’ s (scale t rayDirection)) in
let y = scale tStar rayDirection in let normal : â„œ2 = - gradient s y in
let lightToSurf = y - lightPos in
max 0 (dot (normalize normal) (normalize lightToSurf))
/ (norm2 y * norm2 lightToSurf)

(c) A ğœ†ğ‘† function for differentiable ray tracing of implicit surfaces.

Fig. 1. A library for differentiable ray tracing and scene representation.

whenever it returns a finite-width interval, the true, real-valued result is contained within that
interval.

ğœ†ğ‘† permits differentiation of any functions in the language, so we can compute how the brightness

would change if the circle were moved up by an infinitesimal amount:

eps=1e-3> deriv (ğœ† y : â„œ â‡’ raytrace (circle (0, y) 1) (1, 1) (1, 0)) (-3/4)

[1.3477, 1.3484]

The ğœ†ğ‘† function deriv : (â„œ â†’ â„œ) â†’ (â„œ â†’ â„œ) computes the derivative of a scalar-valued
real function. The result indicates that when the circle is moved up infinitesimally from its current
location, the brightness increases infinitesimally at a rate of âˆ¼1.35 units brightness per unit distance
the circle is moved up.

Several changes occur when the circle is moved up that affect the image brightness. The point at
which the light ray bounces off the circle moves closer to the camera, decreasing the distance from
the camera to the circle (increasing brightness) but increasing the distance from the light to the
camera (decreasing brightness). Both the direction of the surface normal of the circle at the point
where the light deflects and the direction from the light source to that point change, increasing the
angle between the surface normal of the circle and the light ray (decreasing brightness). Automatic
differentiation automatically takes all of these effects into account.

Figure 1c shows the implementation of the differentiable ray tracing in ğœ†ğ‘† . The function firstRoot
: (â„œ â†’ â„œ) â†’ â„œ in the definition of raytrace computes the distance that the light travels
from the scene to the camera. Given a function f : â„œ â†’ â„œ, firstRoot f performs root finding,

ğœ†ğ‘† : Computable Semantics for Differentiable Programming with Higher-Order Functions and Datatypes

3:5

computing min{ğ‘¥ âˆˆ [0, 1] | f(ğ‘¥) = 0}. ğœ†ğ‘† â€™s higher-order functions for root finding are novel,
and accordingly, ğœ†ğ‘† â€™s ability to express differentiable ray tracing of implicit surfaces (embodied in
raytrace) without needing any custom code for specifying derivatives.

The differentiable ray tracer raytrace critically depends on ğœ†ğ‘† â€™s unique support for higher-order
functions, higher-order derivatives, and Lipschitz but nondifferentiable functions such as min, max,
and ReLU. We now provide a brief introduction to these three features.

2.1 Higher-Order Functions
The raytrace function must compute the distance the ray of light travels from the scene to the
camera, represented by the let-definition tStar in raytrace. When applied to the scene circle
(1, y) 1, the definition reduces to

let tStar y = firstRoot (ğœ† t : â„œâ‡’ 1 - y 2 - (t - 1) 2)

The function firstRoot : (â„œ â†’ â„œ) â†’ â„œ is a higher-order function since it takes a
function as input. In order to admit a function like this in a differentiable programming language,
the language must be able to compute how the result of firstRoot changes when there is an
infinitesimal perturbation to its input function. In this example, we want to know how tStar
changes when y changes. To answer this, define f t y = 1 - y 2 - (t - 1) 2. Then tStar finds a
solution for the variable t to the equation f t y = 0. So whatever change is induced by changing
y must be counterbalanced by changing tStar. ğœ†ğ‘† â€™s semantics validate the equation (for values of
y giving well-defined roots)

deriv tStar y = - deriv (ğœ† y0 : â„œâ‡’ f (tStar y) y0) y /

deriv (ğœ† t : â„œâ‡’ f t y) (tStar y)

This equation for the derivative of root finding is known as the implicit function theorem. By the

rules of calculus, we can further simplify this to

deriv tStar y = - y / (tStar y - 1).

Note that the semantics of ğœ†ğ‘† ensures that these equations are indeed program equivalences: one
can substitute one expression for the other within the context of a larger expression without
affecting its meaning. Indeed, taking y = -3/4, and evaluating both sides of the expression above in
ğœ†ğ‘† produces compatible answers, roughly âˆ’1.1, which indicates that moving the circle up decreases
the distance that the light travels from the circle to the camera.

We implement the firstRoot function as a language primitive by specifying not only how
firstRoot acts on values but also how derivatives propagate through it, via the implicit function
theorem (see Â§5.2 for more detail).

2.2 Higher-Order Derivatives
The brightness of the image computed by the raytrace function depends on the angle at which
the ray of light deflects as it bounces off the circle, so we need to know which direction the circle
faces where the light hits it, which is known as the surface normal. In the code for raytrace, the
surface normal is computed as

let normal : â„œ2 = - gradient s y

3:6

Benjamin Sherman, Jesse Michel, and Michael Carbin

Consider, for instance, the unit circle centered at (0, 0), i.e., circle (0, 0) 1, given by the

function ğ‘“ (ğ‘¥, ğ‘¦) = 1 âˆ’ ğ‘¥ 2 âˆ’ ğ‘¦2. The surface normal is given by the negative gradient,

âˆ’âˆ‡ğ‘“ (ğ‘¥, ğ‘¦) = âˆ’

(cid:19)

(cid:18) ğœ•ğ‘“
ğœ•ğ‘¥

,

ğœ•ğ‘“
ğœ•ğ‘¦

= (2ğ‘¥, 2ğ‘¦)

âˆš

âˆš

2) on the upper-right of the circle has a surface normal that

So, for instance, the point (1/
points up and to the right, in the direction (2/

2, 1/

âˆš

2, 2/
Note that, in the raytrace code itself, this gradient computation requires the computation of
derivatives of the implicitly defined surface in order to compute the image brightness. Accordingly,
computing the derivative of the image brightness with respect to an infinitesimal perturbation in
the scene requires computing the second derivatives of the implicitly defined surface with respect
to its arguments. Thus, higher-order differentiation is a valuable language feature.

2).

âˆš

In ğœ†ğ‘† , differentiation is a first-class programming construct, so higher-order differentiation is
naturally supported, as we can compute higher-order derivatives by applying the deriv : (â„œ â†’
â„œ) â†’ â„œ â†’ â„œ function multiple times. Note that some approaches to differentiable programming
do not support higher-order differentiation (see Table 1) and thus do not have differentiation as a first-
class construct. Higher-order derivatives are also used for numerical integration, in optimization
algorithms, and in other contexts.

The requirement to support higher-order derivatives means that language primitives, such as
firstRoot, must specify not only how they act on values but also how derivatives of all orders
propagate through them.

2.3 Nondifferentiability
Note that the raytrace code uses the built-in function max : â„œ â†’ â„œ â†’ â„œ in computing the
image brightness. If the light source is behind the scene, the dot product of the surface normal and
the vector from the light to the surface will be negative, but the brightness should be 0, rather than
this negative value. Hence, we clamp the value to be at least zero by applying max 0. Note that this
function is exactly the rectified linear unit (ReLU) that is common in deep learning:

let relu (x : â„œ) : â„œ = max 0 x

ReLU is not differentiable at 0. When we compute its derivative at 0 in ğœ†ğ‘† , we get a nonmaximal

result. That means that, for sufficiently fine (â‰¤ 1) precision tolerances, we get nontermination:

eps=1e-1> deriv relu 0

eps=2> deriv relu 0

(nontermination)

[0.0, 1.0]

The interval approximations never converge to intervals smaller than [0, 1]. The type â„œ contains,
in addition to the real numbers, nonmaximal elements such as this one, which we name [0, 1], e.g.,
we find that ReLUâ€²(0) = [0, 1].

Differentiable programming frameworks such as PyTorch admit min and max operations, but
they are unsound, in the sense that one can define ğ‘“ (ğ‘¥) = max(ğ‘¥, 0) + min(0, ğ‘¥), which is the
identity function, but compute in PyTorch that ğ‘“ â€²(0) = 2, whereas it should be ğ‘“ â€²(0) = 1. Because
of this issue, most differentiable programming semantics leave the derivative of max undefined at 0.
However, ğœ†ğ‘† â€™s interval-valued semantics for functions like max enables productive computational
functionality that the partiality approach would not permit. For instance, suppose rather than
having a point light source for ray-tracing, we instead have a line light source, so we integrate

ğœ†ğ‘† : Computable Semantics for Differentiable Programming with Higher-Order Functions and Datatypes

3:7

over the entire line, using the primitive higher-order function integral01 : (â„œ â†’ â„œ) â†’
â„œ, where for f : â„œ â†’ â„œ, integral01 f computes the integral of f over the unit interval,
âˆ« 1
0 f(ğ‘¥) ğ‘‘ğ‘¥. For simplicity, consider a camera located at (0, 1) pointing downwards at a flat surface
that stretches from (âˆ’1, ğ‘¦) to (1, ğ‘¦), with a light source stretching from (1, 0) to (1, 1). Furthermore,
let us disregard the effect of brightness decreasing when the light travels longer distances, such
that the brightness is

let brightness (y : â„œ) : â„œ =

integral01 (ğœ† y0 : â„œâ‡’ max 0 ((y0 - y) / sqrt (1 + (y0 - y) 2)))

When 0 â‰¤ y â‰¤ 1, the integrand will be nondifferentiable with respect to y at the point where y0

= y. For instance, taking y0 = y = 1/2, we find that the derivative of the integrand is
deriv (ğœ† y : â„œ â‡’ max 0 ((1/2 - y) / sqrt (1 + (1/2 - y) 2)))) (1/2) = [-1, 0].
When y0 is just greater than y, the derivative will be near âˆ’1, but when y0 is just less than y, the
derivative will be near 0. Because the derivative at this point is a bounded interval, rather than a
completely undefined result, it ends up being soundly neglected when it is integrated over:

eps=1e-3> deriv brightness (1/2)

[-0.4476, -0.4469]

The expression deriv brightness (1/2) is indeed maximal, meaning that it can be evaluated
to arbitrary precision. Were the derivative of the integrand to be undefined rather than interval-
valued, deriv brightness (1/2) would necessarily need to be undefined as well, but with these
semantics, we can soundly compute the correct derivative.

This generalized notion of derivative that works for ReLU is based on Clarkeâ€™s generalized
derivative [Clarke 1990]. The basic idea can be motivated by the desire for continuity and robustness
in the numerical computation. The derivative of ReLU is 1 for numbers imperceptibly greater than
0, and the derivative is 0 for numbers imperceptibly smaller than 0, so the derivative of ReLU at 0
should be consistent with those nearby answers. The specialization relation âŠ‘ on â„œ formalizes this
notion of compatible behavior, where we have [0, 1] âŠ‘ 0 and [0, 1] âŠ‘ 1. We will prove a consistency
theorem for our language (Proposition 5.4) that says that derivatives are always compatible, i.e.,
related by âŠ‘, with the infinitesimal rates of change indicated by its value-level operation.

3 SYNTAX AND SEMANTICS OF ğœ†ğ‘†

0, 1, 2, ... : â„œ
(+), (-), (*), (/) : â„œâ†’â„œâ†’â„œ
max : â„œâ†’â„œâ†’â„œ
sin, exp : â„œâ†’â„œ

integral01 : (â„œâ†’â„œ) â†’â„œ
cutRoot : (â„œâ†’â„œ) â†’â„œ
firstRoot : (â„œâ†’â„œ) â†’â„œ
max01 : (â„œâ†’â„œ) â†’â„œ
argmax01 : (â„œâ†’â„œ) â†’â„œ

tangent A B : (A â†’ B) â†’ Tan A â†’ Tan B
tangentValue A : Tan A â†’ A

record ((cid:27)) A B = { to : A â†’ B,

from : B â†’ A }

tangent_R : Tan â„œ (cid:27) â„œ * â„œ
tangentProd A B : Tan (A * B) (cid:27) Tan A * Tan B
tangentTo_R A : Tan (A â†’â„œ) (cid:27) (A â†’â„œ) * (A â†’â„œ)

Fig. 2. ğœ†ğ‘† constants and their types.

ğœ†ğ‘† is the simply-typed lambda calculus with the constants shown in Fig. 2. These include basic
operators, such as arithmetic and trigonometric operators, higher-order operators, and primitives

3:8

Benjamin Sherman, Jesse Michel, and Michael Carbin

to compute derivatives. The syntax permits polymorphic type signatures, but semantically we treat
polymorphism at the metatheoretic level.

Higher-Order Operators. The function integral01 gives the Riemannian integral of a function
on the interval [0, 1]. max01 maximizes a function over the interval [0, 1], and argmax01 finds its
maximizing argument. cutRoot finds the root of a function f : â„œ â†’ â„œ, assuming that it has a
single root and is negative for smaller values and positive for larger values. firstRoot, on input f
: â„œ â†’ â„œ, finds the first root of f on a region starting at 0.

Derivatives. tangent is a first-class function that computes derivatives, where the type function
Tan gives the space of tangent bundles over a space; conceptually, a space of pairs of values
and derivatives. The function tangentValue projects the value part of this tangent bundle. The
isomorphisms of tangent bundles â€“ i.e., tangent_R, tangentProd, and tangentTo_R â€“ assist with
manipulating the information that corresponds to the derivative part of the tangent bundle when
it is possible for certain spaces. To concretize the concept behind these isomorphisms, we now
present the implementation of deriv from Fig. 1, which uses tangent and these isomorphisms:

let deriv (f : â„œâ†’â„œ) (x : â„œ) : â„œ =

snd (tangent_R.to (tangent f (tangent_R.from (x, 1))))

This implementation calls tangent with ğ‘“ and a query for the derivative of ğ‘“ at ğ‘¥ in the direction
1. The query is a tangent bundle constructed with the isomorphism tangent_R from the pair (ğ‘¥, 1).
deriv then projects out the derivative part of tangentâ€™s result, using tangent_R in the opposite
direction and the standard second projection on binary products.

Semantics. Over the next sections, we develop the full syntax and semantics of ğœ†ğ‘† . In Â§4, we
describe a first-order (i.e., no higher-order functions) differentiable language that supports Clarke
semantics and higher-order derivatives, by defining a Cartesian monoidal category AD. In Â§5, we
will define semantics for the higher-order language ğœ†ğ‘† by taking a category of presheaves, HAD,
over AD. We defer computability concerns to Â§6.

4 SEMANTICS OF A FIRST-ORDER DIFFERENTIABLE LANGUAGE (AD)
In this section, we describe a first-order (i.e., no higher-order functions) differentiable language that
supports Clarke semantics and higher-order derivatives, by defining a Cartesian monoidal category
AD. Fig. 3 presents the syntax and typing rules for the language for AD. The âˆ— type represents
the unit type, having a single value ! in it. Given any object ğ¾ âˆˆ AD, the type expression âŒŠğ¾âŒ‹
represents the type whose semantics is ğ¾. Given any arrow ğ‘“ : âŸ¦ğœ1âŸ§ (cid:123) âŸ¦ğœ2âŸ§ of AD and given some
expression Î“ âŠ¢ ğ‘’ : ğœ1 the syntax âŒŠğ‘“ âŒ‹ (ğ‘’) applies the map ğ‘“ to the result of ğ‘’. When the constants
are binary operators like + and Ã—, we permit syntactic sugar to write them infix, such that, e.g.,
ğ‘’1 + ğ‘’2 is shorthand for +(ğ‘’1, ğ‘’2). The syntax ğœ•ğ‘’ğ‘¦
ğœ•ğ‘¥ |ğ‘¥=ğ‘’ğ‘¥ Â·ğ‘’ğ‘‘ğ‘¥ computes the directional derivative of
ğ‘’ğ‘¦ with respect to ğ‘¥ at ğ‘¥ = ğ‘’ğ‘¥ in the direction of infinitesimal perturbation ğ‘’ğ‘‘ğ‘¥ .

Fig. 4 presents the semantics for the language for AD, which we explain in this section. Our
semantics of derivatives is phrased in terms of Clarkeâ€™s generalized derivative [Clarke 1990], which
enables capturing differentiable properties of locally Lipschitz but nonsmooth functions such as
max, min, and ReLU. We will now present the background material we use to define the semantics
of the language for AD.

ğœ†ğ‘† : Computable Semantics for Differentiable Programming with Higher-Order Functions and Datatypes

3:9

Syntax

variables ğ‘¥

types ğœ ::= âˆ— | ğœ1 Ã— ğœ2
| Î“, ğ‘¥ : ğœ

contexts Î“ ::= Â·
functions ğ‘“ âˆˆ Arr(AD)

| !

âŒŠğ‘“ âŒ‹ (ğ‘’)
(ğ‘’, ğ‘’)

expressions ğ‘’ ::= ğ‘¥ |
|
ğœ•ğ‘’
|ğ‘¥=ğ‘’ Â·ğ‘’
ğœ•ğ‘¥
let ğ‘¥ â‰œ ğ‘’ in ğ‘’

|

|

|

âŒŠğ¾âŒ‹

(ğ‘¥ : ğœ) âˆˆ Î“
Î“ âŠ¢ ğ‘¥ : ğœ

Typing rules
Î“ âŠ¢ ğ‘’ : ğœ1

ğ‘“ : âŸ¦ğœ1âŸ§ (cid:123) âŸ¦ğœ2âŸ§

Î“ âŠ¢ âŒŠğ‘“ âŒ‹ (ğ‘’) : ğœ2

Î“ âŠ¢ ğ‘’1 : ğœ1

Î“ âŠ¢ ğ‘’2 : ğœ2

Î“ âŠ¢ ! : âˆ—

Î“ âŠ¢ (ğ‘’1, ğ‘’2) : ğœ1 Ã— ğœ2

Î“ âŠ¢ ğ‘’1 : ğœ1

Î“, ğ‘¥ : ğœ1 âŠ¢ ğ‘’2 : ğœ2

Î“ âŠ¢ let ğ‘¥ â‰œ ğ‘’1 in ğ‘’2 : ğœ2

Î“, ğ‘¥ : ğœ1 âŠ¢ ğ‘’ğ‘¦ : ğœ2

Î“ âŠ¢ ğ‘’ğ‘¥ : ğœ1

Î“ âŠ¢ ğ‘’ğ‘‘ğ‘¥ : ğœ1

Î“ âŠ¢

ğœ•ğ‘’ğ‘¦
ğœ•ğ‘¥

|ğ‘¥=ğ‘’ğ‘¥ Â·ğ‘’ğ‘‘ğ‘¥ : ğœ2

Fig. 3. Syntax and typing rules for the language for AD.

Types
ğœ type
âŸ¦ğœâŸ§ âˆˆ Ob(AD)

Contexts
Î“ context
âŸ¦Î“âŸ§ âˆˆ Ob(AD)

âŸ¦âˆ—âŸ§ â‰œ 1AD
âŸ¦ğœ1 Ã— ğœ2âŸ§ â‰œ âŸ¦ğœ1âŸ§ Ã— âŸ¦ğœ2âŸ§

âŸ¦Â·âŸ§ â‰œ 1AD
âŸ¦Î“, ğ‘¥ : ğœâŸ§ â‰œ âŸ¦Î“âŸ§ Ã— âŸ¦ğœâŸ§

âŸ¦âŒŠğ¾âŒ‹âŸ§ â‰œ ğ¾

Terms
Î“ âŠ¢ ğ‘’ : ğœ
âŸ¦ğ‘’âŸ§ : âŸ¦Î“âŸ§ (cid:123) âŸ¦ğœâŸ§

âŸ¦âŒŠğ‘“ âŒ‹ (ğ‘’)âŸ§ â‰œ ğ‘“ â—¦ âŸ¦ğ‘’âŸ§
âŸ¦!âŸ§ â‰œ !

âŸ¦(ğ‘’1, ğ‘’2)âŸ§ â‰œ âŸ¨âŸ¦ğ‘’1âŸ§, âŸ¦ğ‘’2âŸ§âŸ©

âŸ¦let ğ‘¥ â‰œ ğ‘’1 in ğ‘’2âŸ§ â‰œ âŸ¦ğ‘’2âŸ§ â—¦ âŸ¨id, âŸ¦ğ‘’1âŸ§âŸ©

ğœ•ğ‘’ğ‘¦
ğœ•ğ‘¥

âŸ¦

|ğ‘¥=ğ‘’ğ‘¥ Â·ğ‘’ğ‘‘ğ‘¥ âŸ§ â‰œ âŸ¦ğ‘’ğ‘¦âŸ§â€² â—¦ âŸ¨âŸ¨id, ğ‘’ğ‘¥ âŸ©, âŸ¨0, ğ‘’ğ‘‘ğ‘¥ âŸ©âŸ©

Fig. 4. Semantics of the language for AD.

4.1 Preliminaries
A domain ğ· is a set with a partial-order structure âŠ‘ that supports directed joins (cid:195)ğ‘‘ âˆˆğ‘† ğ‘‘, which
are just joins of directed subsets ğ‘† âŠ† ğ·, which are those subsets such that if ğ‘¥, ğ‘¦ âˆˆ ğ·, then there
is some ğ‘§ âˆˆ ğ· such that ğ‘¥ âŠ‘ ğ‘§ and ğ‘¦ âŠ‘ ğ‘§. We call the partial-order relation âŠ‘ specialization. The
relation ğ‘¥ âŠ‘ ğ‘¦ intuitively means that ğ‘¥ behaves in a way that is compatible with how ğ‘¦ behaves.
An element ğ‘¥ âˆˆ ğ· is maximal if for any ğ‘¦ âˆˆ ğ·, if ğ‘¥ âŠ‘ ğ‘¦, then ğ‘¦ âŠ‘ ğ‘¥.

Define

R â‰œ {[ğ‘, ğ‘] | ğ‘, ğ‘ âˆˆ R, ğ‘ â‰¤ ğ‘} âˆª {R}
as the domain of interval reals, partially ordered (âŠ‘) by reverse set inclusion. Its maximal elements
are the intervals of the form [ğ‘, ğ‘], which we often just write as ğ‘. Arithmetic operations can
be extended from R to R (see, e.g., Edalat and Lieutier [2004]). Note that R serves as a bottom
element, and we refer to it with the symbol âŠ¥. For any vector space ğ‘‰ (over R), let C(ğ‘‰ ) be the set
of nonempty convex sets in ğ‘‰ , with an order relation âŠ‘ also corresponding to reverse inclusion.
Note that ğ‘‰ serves as a bottom element, and we refer to it with the symbol âŠ¥. Note that we have
the sequence of embeddings Rğ‘› â†©â†’ Rğ‘› â†©â†’ C(Rğ‘›): every vector ğ‘£ âˆˆ Rğ‘› can be treated as a tuple
of singleton intervals Rğ‘›, and every element ğ‘¥ âˆˆ Rğ‘› can be treated as a (convex) hyperrectangle,

3:10

Benjamin Sherman, Jesse Michel, and Michael Carbin

where some dimensions of the hyperrectangle may be infinite. We use the notation ğœ„Rğ‘›â†©â†’Rğ‘› and
ğœ„ Rğ‘›â†©â†’C(Rğ‘›) to denote these embeddings, respectively.

The Clarke Derivative. Let ğ‘“ : Rğ‘› â†’ Rğ‘š. If ğ‘“ is locally Lipschitz on ğ‘‹ âŠ† ğ‘ˆ , let ğ‘ ğ‘“ âŠ† ğ‘‹ be the
points of nondifferentiability of ğ‘“ . The Bouligand subdifferential of ğ‘“ at ğ‘¥ âˆˆ ğ‘‹ is the set of matrices

ğœ•ğµ ğ‘“ (ğ‘¥) â‰œ

(cid:26)
ğ» : Rğ‘šÃ—ğ‘› |

ğ» = limğ‘—â†’âˆ ğ½ ğ‘“ (ğ‘¥ ğ‘— ) for some sequence (ğ‘¥ ğ‘— ) ğ‘— âˆˆN
where ğ‘¥ ğ‘— âˆˆ ğ‘‹ \ ğ‘ ğ‘“ for all ğ‘— âˆˆ N and limğ‘—â†’âˆ ğ‘¥ ğ‘— = ğ‘¥

(cid:27)

,

where ğ½ is the Jacobian operator defining the derivative of a function at a point where it is
differentiable. The Clarke Jacobian of ğ‘“ at ğ‘¥ is given by the convex hull ğœ•ğ‘“ (ğ‘¥) â‰œ hull(ğœ•ğµ ğ‘“ (ğ‘¥)). The
Clarke Jacobian ğœ•ğ‘“ (ğ‘¥) âˆˆ C(Rğ‘šÃ—ğ‘›) is always compact (since ğ‘“ is locally Lipschitz).

Given ğ‘“

: Rğ‘› â†’ Rğ‘š

âŠ¥, let ğ‘ˆ be the largest open set on which ğ‘“ is both defined and locally

Lipschitz. We can define the partial Clarke Jacobian of ğ‘“ to be

ğœ•âŠ¥ğ‘“ (ğ‘¥) =

(cid:40)ğœ•ğ‘“ (ğ‘¥)
âŠ¥

ğ‘¥ âˆˆ ğ‘ˆ
ğ‘¥ âˆ‰ ğ‘ˆ

such that ğœ•âŠ¥ : (Rğ‘› â†’ Rğ‘š
âŠ¥) â†’ Rğ‘› â†’ C(Rğ‘šÃ—ğ‘›). We can map values of C(ğ´) to ğ´âŠ¥ (for any ğ´) by
mapping maximal elements {ğ‘¥ } âˆˆ C(ğ´) to ğ‘¥ âˆˆ ğ´âŠ¥ and everything else to âŠ¥. Using this conversion,
we can also give the partial Clarke Jacobian the type ğœ•âŠ¥ : (Rğ‘› â†’ C(Rğ‘š)) â†’ Rğ‘› â†’ C(Rğ‘šÃ—ğ‘›), and
thus we can also iterate the partial Clarke Jacobian construction to get higher-order derivatives
ğœ•ğ‘˜
). Note that the ğ‘˜ + 1th-order Clarke Jacobian is âŠ¥ unless the
âŠ¥
ğ‘˜th-order Clarke Jacobian is maximal; thus, when defined, higher-order Clarke Jacobians are just
Clarke Jacobians of conventional higher-order derivatives.

âŠ¥) â†’ Rğ‘› â†’ C(Rğ‘šÃ—ğ‘›ğ‘˜

: (Rğ‘› â†’ Rğ‘š

When a function is differentiable, its partial Clarke Jacobian is a maximal element. When it is
locally Lipschitz but not differentiable, the partial Clarke Jacobian is a compact convex set. When it
is not locally Lipschitz, the partial Clarke Jacobian is the entire space, corresponding to âŠ¥.

4.2 Smoothish Maps
We will now define AD. The objects of AD are the natural numbers, where ğ‘› âˆˆ N corresponds
to ğ‘›-dimensional Euclidean space. To emphasize that we are thinking of Euclidean space, we
write the object ğ‘› âˆˆ N as Rğ‘›. A morphism of AD is a smoothish map: a derivative tower that is
successively consistent. A derivative tower ğ‘“ between spaces Rğ‘› and Rğ‘š, ğ‘“ : Rğ‘› (cid:123) Rğ‘š, is a collection
of continuous maps (taking the Scott topology for R)

ğ‘“ (ğ‘˜) : Rğ‘› Ã— (Rğ‘›)ğ‘˜ â†’ Rğ‘š

for each ğ‘˜ âˆˆ N, where ğ‘“ (ğ‘˜) represents the ğ‘˜th-order derivative. This defines a smoothish map
as a power series, where the first Rğ‘› argument is the point where the map is evaluated, and the
remaining ğ‘˜ arguments represent the inputs to a multilinear map representing the derivative.1
Given vectors ğ‘¥ âˆˆ Rğ‘› and ğ‘¦ âˆˆ Rğ‘˜ , let ğ‘¥ âŠ— ğ‘¦ âˆˆ Rğ‘›Ã—ğ‘˜ denote the tensor product. Define Matğ‘˜ :
(Rğ‘› Ã— (Rğ‘›)ğ‘˜ â†’ Rğ‘š) â†’ Rğ‘› â†’ Rğ‘šÃ—ğ‘›ğ‘˜
âŠ¥ at a point ğ‘¥ âˆˆ Rğ‘› such that Matğ‘˜ (ğ‘“ )(ğ‘¥) = ğ‘€ if there is a
matrix ğ‘€ âˆˆ Rğ‘šÃ—ğ‘›ğ‘˜ such that for all ğ‘‘ğ‘¥1, . . . , ğ‘‘ğ‘¥ğ‘˜ âˆˆ Rğ‘›, we have

ğ‘“ (ğ‘¥; ğ‘‘ğ‘¥1, . . . , ğ‘‘ğ‘¥ğ‘˜ ) = ğœ„Rğ‘šâ†©â†’Rğ‘š (ğ‘€ Â· (ğ‘‘ğ‘¥1 âŠ— . . . âŠ— ğ‘‘ğ‘¥ğ‘˜ )) ,
and Matğ‘˜ (ğ‘“ )(ğ‘¥) = âŠ¥ if there is no such matrix (ğ‘€). Equation 1 requires that ğ‘“ is multilinear in its
ğ‘‘ğ‘¥1, . . . , ğ‘‘ğ‘¥ğ‘˜ arguments, which means that ğ‘“ has a form that permits differentiation.

(1)

1This representation as derivative towers is largely drawn from [Elliott 2008].

ğœ†ğ‘† : Computable Semantics for Differentiable Programming with Higher-Order Functions and Datatypes

3:11

Definition 4.1. We define a consistency relation Consğ‘˜ (ğ‘”, ğ‘“ ) for a function ğ‘” : Rğ‘› Ã— (Rğ‘›)ğ‘˜ â†’ Rğ‘š

and a function ğ‘“ : Rğ‘› â†’ C(Rğ‘šÃ—ğ‘›ğ‘˜

) to hold if for all ğ‘¥ âˆˆ Rğ‘› and for all ğ‘‘ğ‘¥1, . . . , ğ‘‘ğ‘¥ğ‘˜ âˆˆ Rğ‘›,

ğœ„ Rğ‘šâ†©â†’C(Rğ‘š) (ğ‘”(ğ‘¥; ğ‘‘ğ‘¥1, . . . , ğ‘‘ğ‘¥ğ‘˜ )) âŠ‘ ğ‘“ (ğ‘¥) Â· (ğ‘‘ğ‘¥1 âŠ— . . . âŠ— ğ‘‘ğ‘¥ğ‘˜ ).

A derivative tower ğ‘“ is successively consistent if for all ğ‘˜ âˆˆ N, we have

Consğ‘˜+1(ğ‘“ (ğ‘˜+1), ğœ•âŠ¥Matğ‘˜ (ğ‘“ (ğ‘˜) )),
meaning that each successive derivative ğ‘“ (ğ‘˜+1) is consistent with the value-level behavior of ğ‘“ (ğ‘˜) .

A smoothish map ğ‘“ is a successively consistent derivative tower. We call a smoothish map smooth
if ğ‘“ (ğ‘˜) is maximal for all ğ‘˜ (which agrees with the standard definition of a smooth map). We will
later show (in Â§4.4) that smoothish maps form a category AD, and then by categorical semantics,
that all expressions in the first-order language map to that category.

4.3 Primitives
Any first-order primitive may be implemented by giving its power-series representation. We use the
notation ğ‘“ (ğ‘˜) (ğ‘¥; (cid:174)ğ‘£) to denote the ğ‘˜th derivative of ğ‘“ at ğ‘¥ in directions (cid:174)ğ‘£; a smoothish map ğ‘“ is defined
by the collection of these functions for all ğ‘˜ âˆˆ N. These data provide power-series expansions
around any input point. There is a map 0 : Î“ (cid:123) ğ´ (for any Î“, ğ´ âˆˆ AD) that always returns zero
regardless of its input. A linear map ğ‘“ : ğ´ â†’ ğµ determines a smooth map linear(ğ‘“ ) : ğ´ (cid:123) ğµ by

linear(ğ‘“ ) (0) (ğ‘¥) â‰œ ğ‘“ (ğ‘¥)
linear(ğ‘“ ) (1) (ğ‘¥; ğ‘£) â‰œ ğ‘“ (ğ‘£)

linear(ğ‘“ ) (ğ‘˜+2) (ğ‘¥; (cid:174)ğ‘£) â‰œ 0

Derivative-Tower Construction. A derivative tower can be viewed as a stream of a function and

all of its derivatives. Streams are characterized by the isomorphism

Stream(ğ´) (cid:27) ğ´ Ã— Stream(ğ´)
that says that a stream ğ‘  : Stream(ğ´) is exactly composed of its head, head(ğ‘ ) : ğ´, and its tail,
tail(ğ‘ ) : Stream(ğ´). To construct a derivative tower, we define the map foldDer as an analogue to
the cons operation on streams. For instance, given value-level definitions of sine and cosine, sin
and cos, it is well-founded to define their derivative towers as

âŸ¦sinâŸ§AD
âŸ¦cosâŸ§AD

â‰œ foldDer(sin, âŸ¦ğ‘¥, ğ‘‘ğ‘¥ âŠ¢ cos(ğ‘¥) * ğ‘‘ğ‘¥âŸ§AD)
â‰œ foldDer(cos, âŸ¦ğ‘¥, ğ‘‘ğ‘¥ âŠ¢ âˆ’sin(ğ‘¥) * ğ‘‘ğ‘¥âŸ§AD),

just as it would be to define two mutually recursive streams evens = cons(0, map(ğœ†ğ‘¥ . ğ‘¥ + 1, odds))
and odds = cons(1, map(ğœ†ğ‘¥ . ğ‘¥ + 3, evens))).

We define foldDer as follows, where ğ‘“ : ğ´ â†’ ğµ and ğ‘” : ğ´ Ã— ğ´ (cid:123) ğµ, such that foldDer(ğ‘“ , ğ‘”) :

ğ´ (cid:123) ğµ.

foldDer(ğ‘“ , ğ‘”) (0) (ğ‘¥) â‰œ ğ‘“ (ğ‘¥)

foldDer(ğ‘“ , ğ‘”) (ğ‘˜+1) (ğ‘¥; ğ‘£1, . . . , ğ‘£ğ‘˜+1) â‰œ ğ‘” (ğ‘˜) ((ğ‘¥, ğ‘£1); (ğ‘£2, 0), . . . , (ğ‘£ğ‘˜+1, 0))

(ğ‘˜ âˆˆ N)

One of the perturbations ğ‘£1 is passed in as the value to ğ‘”, and then that perturbation is not considered
to have any derivatives itself, hence the 0s in the second components of the perturbation passed to
ğ‘”. Setting the first components of the derivatives to ğ‘£2, . . . , ğ‘£ğ‘˜+1 establishes these as independent
infinitesimal perturbations of the first value component, ğ‘¥.

3:12

Benjamin Sherman, Jesse Michel, and Michael Carbin

4.3.1 Arithmetic Operations. The binary arithmetic operations are first-order functions and so can
be represented in AD as functions with the type R Ã— R (cid:123) R. Addition and subtraction are linear,
â‰œ linear(âˆ’). We define the smooth
â‰œ linear(+) and âŸ¦-âŸ§AD
so their semantics is simply âŸ¦+âŸ§AD
multiplication operator by

âŸ¦*âŸ§AD

â‰œ foldDer(ğœ†(ğ‘¥, ğ‘¦). ğ‘¥ Ã— ğ‘¦, âŸ¦(ğ‘¥, ğ‘¦), (ğ‘‘ğ‘¥, ğ‘‘ğ‘¦) âŠ¢ ğ‘¥ * ğ‘‘ğ‘¦ + ğ‘¦ * ğ‘‘ğ‘¥âŸ§AD),

whose derivative is the familiar product rule. Note that our definition of âŸ¦*âŸ§AD
has two recursive
references to multiplicationâ€™s own smooth map. This recursive reference is well-founded because
the result is used in a way that does not demand any further differentiation. This recursive pattern
is similar to defining the stream of natural numbers nats : Stream(N) by

nats â‰œ cons(0, map (ğœ†ğ‘¥ . ğ‘¥ + 1) nats),

where mapping a function over nats does not demand any further calls to tail. Reciprocals (used
for division) can be defined using foldDer as well, where all ğ‘˜th-order derivatives will return âŠ¥
when the input is 0.

Lipschitz but Nonsmooth Functions. Many functions, such as max, min, and ReLU, are locally
4.3.2
Lipschitz but not smooth. These functions are used pervasively in contexts that require differentia-
tion, so their admissibility in a differential-programming semantics is paramount. Whereas most
differential-programming semantics say that derivative of max is undefined when its arguments
are equal, our use of Clarke derivatives permits a non-âŠ¥ result.

We define max as follows, where hull computes the interval corresponding to the convex hull of

the union of a set of points.

âŸ¦maxâŸ§(0)

AD

âŸ¦maxâŸ§(1)

AD

((ğ‘¥, ğ‘¦); (ğ‘‘ğ‘¥, ğ‘‘ğ‘¦)) â‰œ

âŸ¦maxâŸ§(ğ‘˜+2)

AD

((ğ‘¥, ğ‘¦); (cid:174)ğ‘£) â‰œ

(ğ‘¥, ğ‘¦) â‰œ max(ğ‘¥, ğ‘¦)
ğ‘‘ğ‘¥
ï£±ï£´ï£´ï£´ï£²
ğ‘‘ğ‘¦
ï£´ï£´ï£´
hull({ğ‘‘ğ‘¥, ğ‘‘ğ‘¦})
ï£³
(cid:40)0
ğ‘¥ â‰  ğ‘¦
âŠ¥ ğ‘¥ = ğ‘¦

ğ‘¥ > ğ‘¦
ğ‘¦ < ğ‘¥
ğ‘¥ = ğ‘¦

4.3.3 Differentiation Operator. To give a semantics to the syntax ğœ•ğ‘’ğ‘¦
ğœ•ğ‘¥ |ğ‘¥=ğ‘’ğ‘¥ Â·ğ‘’ğ‘‘ğ‘¥ for differentiation,
we first define a differentiation operator, postfix â€², on smoothish maps, where ğ‘“ : ğ´ (cid:123) ğµ maps to
ğ‘“ â€² : ğ´ Ã— ğ´ (cid:123) ğµ. Defining this operator is nontrivial, because all the derivatives of ğ‘“ â€² must consider
not only perturbations to the function value but also perturbations to the derivative argument,
which are not accounted for in the original derivative tower: note that the ğ‘˜th derivative of ğ‘“ is
a multilinear map from ğ´ğ‘˜ , whereas the ğ‘˜th derivative of ğ‘“ â€² is a multilinear map from ğ´2ğ‘˜ . We
show the value and first few derivatives; because ğ‘¥ will always be applied as the value argument to
derivatives of ğ‘“ , we elide those arguments:

ğ‘“ â€² (0) (ğ‘¥, ğ‘£) = ğ‘“ (1) (ğ‘£)

ğ‘“ â€² (1) ((ğ‘¥, ğ‘£); (ğ‘‘ğ‘¥ğ‘, ğ‘‘ğ‘£ğ‘)) = ğ‘“ (2) (ğ‘£, ğ‘‘ğ‘¥ğ‘) + ğ‘“ (1) (ğ‘‘ğ‘£ğ‘)

ğ‘“ â€² (2) ((ğ‘¥, ğ‘£); (ğ‘‘ğ‘¥ğ‘, ğ‘‘ğ‘£ğ‘), (ğ‘‘ğ‘¥ğ‘, ğ‘‘ğ‘£ğ‘)) = ğ‘“ (3) (ğ‘£, ğ‘‘ğ‘¥ğ‘, ğ‘‘ğ‘¥ğ‘) + ğ‘“ (2) (ğ‘‘ğ‘£ğ‘, ğ‘‘ğ‘¥ğ‘) + ğ‘“ (2) (ğ‘‘ğ‘¥ğ‘, ğ‘‘ğ‘£ğ‘)

ğœ†ğ‘† : Computable Semantics for Differentiable Programming with Higher-Order Functions and Datatypes

3:13

The general formula is:

ğ‘“ â€² (ğ‘˜) ((ğ‘¥, ğ‘£); (ğ‘‘ğ‘¥1, ğ‘‘ğ‘£1), . . . , (ğ‘‘ğ‘¥ğ‘˜, ğ‘‘ğ‘£ğ‘˜ )) â‰œ

ğ‘“ (ğ‘˜+1) (ğ‘¥; ğ‘£, ğ‘‘ğ‘¥1, . . . , ğ‘‘ğ‘¥ğ‘˜ ) +

ğ‘˜
âˆ‘ï¸

ğ‘—=1

ğ‘“ (ğ‘˜) (ğ‘¥; ğ‘‘ğ‘¥1, . . . , ğ‘‘ğ‘¥ ğ‘—âˆ’1, ğ‘‘ğ‘£ ğ‘—, ğ‘‘ğ‘¥ ğ‘—+1, . . . ğ‘‘ğ‘¥ğ‘˜ ).

4.3.4 Revisiting Derivative Tower Construction. The â€² operator is analogous to the tail operator of a
stream, in that derivative towers have the section-retraction pair

ğ´ (cid:123) ğµ

ğœ†ğ‘“ .(ğ‘“ (0) ,ğ‘“ â€²)

foldDer

(ğ´ â†’ ğµ) Ã— (ğ´ Ã— ğ´ (cid:123) ğµ)

that characterizes a derivative tower ğ‘“ : ğ´ (cid:123) ğµ as a function ğ‘“ (0) : ğ´ â†’ ğµ for the evaluation map
of ğ‘“ together with a derivative tower ğ‘“ â€² : ğ´ Ã— ğ´ (cid:123) ğµ where ğ‘“ â€²(ğ‘¥, ğ‘£) represents the directional
derivative of ğ‘“ at ğ‘¥ in the direction ğ‘£.

Given this observation, we may for convenience in the rest of the paper define a smoothish
map ğ‘“ by its value-level function ğ‘“ (0) and its smoothish derivative ğ‘“ â€², denoting an implicit use of
foldDer. For example, we can equivalently define the smooth multiplication operator (Â§4.3.1) by

âŸ¦*âŸ§(0)
AD
âŸ¦*âŸ§â€²

AD

â‰œ ğœ†(ğ‘¥, ğ‘¦). ğ‘¥ Ã— ğ‘¦
â‰œ âŸ¦(ğ‘¥, ğ‘¦), (ğ‘‘ğ‘¥, ğ‘‘ğ‘¦) âŠ¢ ğ‘¥ * ğ‘‘ğ‘¦ + ğ‘¦ * ğ‘‘ğ‘¥âŸ§AD

.

4.4 Categorical Operations
AD forms a Cartesian monoidal category. We describe the categorical operations here, and prove
that they satisfy the expected properties in Proposition 4.5. The maps id : ğ´ â†’ ğ´ (for all ğ´),
! : Î“ â†’ âˆ— (for all Î“), fst : ğ´ Ã— ğµ â†’ ğ´ and snd : ğ´ Ã— ğµ â†’ ğµ (for all ğ´, ğµ) are all in fact linear maps
and so can be made into smooth maps with the linear operator described above. Given ğ‘“ : Î“ (cid:123) ğ´
and ğ‘” : Î“ (cid:123) ğµ, we define their product âŸ¨ğ‘“ , ğ‘”âŸ© : Î“ (cid:123) ğ´ Ã— ğµ by

âŸ¨ğ‘“ , ğ‘”âŸ© (ğ‘˜) (ğ‘¥; (cid:174)ğ‘£) â‰œ (ğ‘“ (ğ‘˜) (ğ‘¥; (cid:174)ğ‘£), ğ‘” (ğ‘˜) (ğ‘¥; (cid:174)ğ‘£)),

It only remains to define composition. Composition of smooth maps is given by FaÃ  di Brunoâ€™s
formula. The definition is perhaps easier to understand by example for small ğ‘˜. The following
shows derivatives of ğ‘” â—¦ ğ‘“ at ğ‘¥; since ğ‘” is always differentiated at ğ‘“ (ğ‘¥) and ğ‘“ is always differentiated
at ğ‘¥, we elide those arguments:

(ğ‘” â—¦ ğ‘“ ) (0) () = ğ‘” (0) ()

(ğ‘” â—¦ ğ‘“ ) (1) (ğ‘£ğ‘) = ğ‘” (1) (ğ‘“ (1) (ğ‘£ğ‘))

(ğ‘” â—¦ ğ‘“ ) (2) (ğ‘£ğ‘, ğ‘£ğ‘) = ğ‘” (2) (ğ‘“ (1) (ğ‘£ğ‘), ğ‘“ (1) (ğ‘£ğ‘)) + ğ‘” (1) (ğ‘“ (2) (ğ‘£ğ‘, ğ‘£ğ‘))

(ğ‘” â—¦ ğ‘“ ) (3) (ğ‘£ğ‘, ğ‘£ğ‘, ğ‘£ğ‘ ) = ğ‘” (3) (ğ‘“ (1) (ğ‘£ğ‘), ğ‘“ (1) (ğ‘£ğ‘), ğ‘“ (1) (ğ‘£ğ‘ ))

+ ğ‘” (2) (ğ‘“ (2) (ğ‘£ğ‘, ğ‘£ğ‘), ğ‘“ (1) (ğ‘£ğ‘ )) + ğ‘” (2) (ğ‘“ (2) (ğ‘£ğ‘, ğ‘£ğ‘ ), ğ‘“ (1) (ğ‘£ğ‘))
+ ğ‘” (2) (ğ‘“ (2) (ğ‘£ğ‘, ğ‘£ğ‘ ), ğ‘“ (1) (ğ‘£ğ‘)) + ğ‘” (1) (ğ‘“ (3) (ğ‘£ğ‘, ğ‘£ğ‘, ğ‘£ğ‘ ))

The general formula is

(ğ‘” â—¦ ğ‘“ ) (ğ‘˜) (ğ‘¥; (cid:174)ğ‘£) â‰œ âˆ‘ï¸

ğœ‹ âˆˆH ( {1,...,ğ‘˜ })

let ğ‘› â‰œ |ğœ‹ | in ğ‘” (ğ‘›) (cid:169)
(cid:173)
(cid:173)
(cid:171)

ğ‘“ (ğ‘¥);

ğ‘“ ( |ğœ‹1 |) (ğ‘¥; ğ‘£ğœ‹11
...,
ğ‘“ ( |ğœ‹ğ‘› |) (ğ‘¥; ğ‘£ğœ‹ğ‘› 1

, . . . , ğ‘£ğœ‹1 |ğœ‹1 | ),

, . . . , ğ‘£ğœ‹ğ‘› |ğœ‹ğ‘› | )

,

(cid:170)
(cid:174)
(cid:174)
(cid:172)

3:14

Benjamin Sherman, Jesse Michel, and Michael Carbin

where H (ğ‘†) is the set of partitions of a set ğ‘†, and |ğ‘† | is the cardinality of a set. Note that in the
general case, the inputs to ğ‘” (ğ‘›) may be elements of Rğ‘ rather than Rğ‘ (for some ğ‘ âˆˆ N). Given any
ğ‘›th derivative ğ‘” (ğ‘›) : Rğ‘ Ã—(Rğ‘)ğ‘˜ â†’ Rğ‘š, we extend it to apply to inputs ğ‘¥ âˆˆ Rğ‘ and ğ‘‘ğ‘¥1, . . . , ğ‘‘ğ‘¥ğ‘˜ âˆˆ Rğ‘
by

ğ‘” (ğ‘›) (ğ‘¥; ğ‘‘ğ‘¥1, . . . , ğ‘‘ğ‘¥ğ‘˜ ) â‰œ hull (cid:110)

ğ‘” (ğ‘›) (ğ‘¦; ğ‘‘ğ‘¦1, . . . , ğ‘‘ğ‘¦ğ‘˜ ) | ğ‘¦ âˆˆ ğ‘¥, ğ‘‘ğ‘¦1 âˆˆ ğ‘‘ğ‘¥1, . . . , ğ‘‘ğ‘¦ğ‘˜ âˆˆ ğ‘‘ğ‘¥ğ‘˜

(cid:111)

.

FaÃ  di Brunoâ€™s formula simplifies drastically in the case that either function is linear:

Proposition 4.2. For any ğ‘” : ğµ â†’ ğ¶ and any derivative tower ğ‘“ : ğ´ (cid:123) ğµ, for any ğ‘˜ âˆˆ N and any

ğ‘¥ âˆˆ ğ´ and ğ‘£1, . . . , ğ‘£ğ‘˜ âˆˆ ğ´,

(linear(ğ‘”) â—¦ ğ‘“ ) (ğ‘˜) (ğ‘¥; ğ‘£1, . . . , ğ‘£ğ‘˜ ) = ğ‘”(ğ‘“ (ğ‘˜) (ğ‘£1, . . . , ğ‘£ğ‘˜ ))
Proof sketch. Because linear(ğ‘”) ( ğ‘—) (ğ‘£1, . . . , ğ‘£ ğ‘— ) = 0 whenever ğ‘— > 1 by definition of linear, all
terms in the sum given by the FaÃ  di Bruno formula where |ğœ‹ | > 1 will be 0. We can thus remove
â–¡
those terms, and the only term in the sum that will remain is the one where |ğœ‹ | = 1.

Proposition 4.3. For any consistent derivative tower ğ‘” : ğµ (cid:123) ğ¶ and any ğ‘“ : ğ´ â†’ ğµ that maps
maximal elements to maximal elements, for any ğ‘˜ âˆˆ N and any maximal ğ‘¥ âˆˆ ğ´ and maximal
ğ‘£1, . . . , ğ‘£ğ‘˜ âˆˆ ğ´,

(ğ‘” â—¦ linear(ğ‘“ )) (ğ‘˜) (ğ‘¥; ğ‘£1, . . . , ğ‘£ğ‘˜ ) = ğ‘” (ğ‘˜) (ğ‘“ (ğ‘£1), . . . , ğ‘“ (ğ‘£ğ‘˜ )).
Proof sketch. Note that the term in the sum given by the FaÃ  di Bruno formula where |ğœ‹ | = ğ‘˜
gives the right-hand side ğ‘” (ğ‘˜) (ğ‘“ (ğ‘£1), . . . , ğ‘“ (ğ‘£ğ‘˜ )). For all other terms in the sum, where |ğœ‹ | < ğ‘˜, we
have that one of the inputs to ğ‘” ( |ğœ‹ |) will be 0, because we have linear(ğ‘“ ) ( ğ‘—) (ğ‘£1, . . . , ğ‘£ ğ‘— ) = 0 whenever
ğ‘— > 1 by definition of linear.

We need to know that adding all these terms to the term |ğœ‹ | = ğ‘˜ makes no difference to the
sum, which can happen either if all of the terms are 0, or if already ğ‘” (ğ‘˜) (ğ‘“ (ğ‘£1), . . . , ğ‘“ (ğ‘£ğ‘˜ )) = âŠ¥, in
which case the addition of any elements will not change the result. Thus, it suffices to prove that
if ğ‘” (ğ‘˜) (ğ‘“ (ğ‘£1), . . . , ğ‘“ (ğ‘£ğ‘˜ )) â‰  âŠ¥, then all of those other terms in the sum are 0. A detailed technical
â–¡
argument can show that this is the case.

The chain rule for Clarke derivatives is a specialization relation rather than an equality:
Proposition 4.4 (Chain rule for ğœ•âŠ¥). Given ğ‘“ : Rğ‘› â†’ Rğ‘š

âŠ¥, and ğ‘” : Rğ‘š â†’ Rğ‘˜

âŠ¥ for all ğ‘¥ âˆˆ Rğ‘›

and all ğ‘‘ğ‘¥ âˆˆ Rğ‘›,

hull ({ğº Â· ğ¹ Â· ğ‘‘ğ‘¥ | ğº âˆˆ (ğœ•âŠ¥ğ‘”) (ğ‘“ (ğ‘¥)), ğ¹ âˆˆ ğœ•âŠ¥ğ‘“ (ğ‘¥)}) âŠ‘ ğœ•âŠ¥(ğ‘” â—¦ ğ‘“ )(ğ‘¥) Â· ğ‘‘ğ‘¥ .

Proof sketch. A minor variation of [Clarke 1990, Corollary on page 75].

â–¡

For example, at the value level, max x 0 + min 0 x = x, but the derivative of the left-hand side
at 0 is [0, 2] while the derivative at the right-hand side is 1, noting [0, 2] âŠ‘ 1. This has important
ramifications for ğœ†ğ‘† , where we construct functions as compositions of others and need composition
to be computable. Because of the specialization relation, we know that any behavior of a function
in ğœ†ğ‘† (e.g., [0, 2]) will be compatible with the ideal derivative of its value-level function (e.g., 1), but
it may not return the maximal such value.

Proposition 4.5. These operations (identity, composition, pairing, projections) give AD the structure

of a Cartesian monoidal category. Therefore, AD admits the internal language described in Fig. 3.

Proof sketch. There are two main classes of properties we must confirm about these categorical
operations. First, we must verify that all of the operations preserve successive consistency, taking
consistent derivative towers to consistent derivative towers. Second, we must confirm that the
algebraic laws of a Cartesian monoidal category.

ğœ†ğ‘† : Computable Semantics for Differentiable Programming with Higher-Order Functions and Datatypes

3:15

1. Operations preserve consistency. Because several of the categorical operations are of the form

linear(ğ‘“ ) we first prove a lemma that these maps are consistent:
Lemma 4.6. Call a map ğ‘“ : Rğ‘› â†’ Rğ‘˜ linear if it always outputs values in Rğ‘˜ and if it is linear in
the traditional sense, i.e., ğ‘“ (ğ‘¢) + ğ‘“ (ğ‘£) = ğ‘“ (ğ‘¢ + ğ‘£) for all ğ‘¢, ğ‘£ âˆˆ Rğ‘› and ğ‘ Â· ğ‘“ (ğ‘£) = ğ‘“ (ğ‘ Â· ğ‘£) for all
ğ‘ âˆˆ R and all ğ‘£ âˆˆ Rğ‘›. Whenever ğ‘“ : Rğ‘› â†’ Rğ‘˜ is linear, linear(ğ‘“ ) is consistent.
Proof. Since ğ‘“ is linear in the above-defined sense, it is smooth, and so its derivatives will
always be maximal, and will coincide with the traditional derivatives, which is exactly what
â–¡
linear(ğ‘“ ) computes.
â€¢ Identity maps are consistent. Follows from Lemma 4.6.
â€¢ Product projections are consistent. Also follows from Lemma 4.6.
â€¢ Pairing preserves consistency. Essentially reduces to the following lemma:

Proposition 4.7. Given two maps ğ‘“ : Rğ‘› â†’ Rğ‘š

âŠ¥ and ğ‘” : Rğ‘› â†’ Rğ‘˜

âŠ¥, for any ğ‘¥ âˆˆ Rğ‘›,

{ (cid:2)ğ‘¢ ğ‘£ (cid:3) | ğ‘¢ âˆˆ ğœ•âŠ¥ ğ‘“ (ğ‘¥), ğ‘£ âˆˆ ğœ•âŠ¥ğ‘”(ğ‘¥)} âŠ‘ ğœ•âŠ¥ (ğœ†ğ‘§.(ğ‘“ (ğ‘§), ğ‘”(ğ‘§)))(ğ‘¥),

âŠ¥ â†’ Rğ‘š+ğ‘˜

âŠ¥ returns âŠ¥ if either of its arguments it âŠ¥,

where the pairing operation (Â·, Â·) : Rğ‘š
âŠ¥ Ã— Rğ‘˜
or the pair of values if both inputs are not âŠ¥.
Proof sketch. Note that the set defined by the set comprehension on the left-hand side of
the relation is convex, since both ğœ•âŠ¥ğ‘“ (ğ‘¥) and ğœ•âŠ¥ğ‘”(ğ‘¥) are. Suppose (cid:2)ğ» ğ¿(cid:3) is in the Bouligand
subdifferential of ğœ†ğ‘§.(ğ‘“ (ğ‘§), ğ‘”(ğ‘§)) at ğ‘¥. Then it must be the case that ğ» is in the Bouligand
â–¡
subdifferential for ğ‘“ and that ğ¿ is in the Bouligand subdifferential for ğ‘”.
â€¢ Composition preserves consistency. The full proof is quite detailed and technical. At its core,
the proof proceeds much like the proof of the conventional FaÃ  di Bruno formula, which can
proceed by induction on the order of differentiation. However, whereas conventionally there is
an equality between the FaÃ¡ di Bruno formula and the derivative, in our case, their is an order
relation that the FaÃ¡ di Bruno formula is at most the derivative. The base case is the general
chain rule of calculus, which in our case corresponds to the chain rule for Clarke derivatives,
Proposition 4.4. The key step in the inductive case is the tensor product rule:
Proposition 4.8 (Tensor product rule for ğœ•âŠ¥). Given ğ‘” : ğ· â†’ Rğ‘šÃ—ğ‘› ğ‘— Ã—...Ã—ğ‘›1
ğ‘– âˆˆ {1, . . . , ğ‘— }, ğ‘“ğ‘– : ğ· â†’ Rğ‘›ğ‘–

âŠ¥ and for all

âŠ¥, for all ğ‘¥ âˆˆ ğ·,
ğ‘—
âˆ‘ï¸

(cid:33)

+ ğ‘”(ğ‘¥) Â·

ğ‘“ğ‘˜ (ğ‘¥)

ğœ•âŠ¥ğ‘”(ğ‘¥) Â·

(cid:32) ğ‘—
(cid:204)

ğ‘˜=1

(cid:32) ğ‘–âˆ’1
(cid:204)

(cid:33)

ğ‘“ğ‘˜ (ğ‘¥)

âŠ— ğœ•âŠ¥ ğ‘“ğ‘– (ğ‘¥) âŠ—

(cid:33)

ğ‘“ğ‘˜ (ğ‘¥)

(cid:32)

ğ‘—
(cid:204)

ğ‘˜=ğ‘–+1

ğ‘–=1

ğ‘˜=1
âŠ‘
ğœ•âŠ¥ (ğ‘” Â· (ğ‘“1 âŠ— . . . âŠ— ğ‘“ğ‘— ))(ğ‘¥).

Proof. By repeated application of the product rule for Clarke derivatives.

â–¡

2. Algebraic laws hold.

â€¢ Composition is associative. Follows from associativity and commutativity of + and the associa-

tivity of taking partitions of partitions (in the appropriate sense).

â€¢ ğ‘“ â—¦ id = ğ‘“ = id â—¦ ğ‘“ . Follows from Proposition 4.3 and Proposition 4.2, since id is linear.
â€¢ ğ›½ and ğœ‚ laws for product projections. Follows from the fact that linear commutes with pairing,
â–¡

i.e., linear(âŸ¨ğ‘“ , ğ‘”âŸ©) = âŸ¨linear(ğ‘“ ), linear(ğ‘”)âŸ©, and from Proposition 4.3 and Proposition 4.2.

4.5 Consistency
The derivatives that our semantics defines are consistent: the behaviors of ğ‘˜th derivative that is
computed, âŸ¦ğ‘’âŸ§(ğ‘˜)
, are compatible with the derivatives that would be abstractly defined by looking
AD

3:16

Benjamin Sherman, Jesse Michel, and Michael Carbin

Syntax

variables ğ‘¥

constant types ğ¾ âˆˆ Ob(HAD)

types ğœ ::= âˆ— | ğœ1 Ã— ğœ2

| ğœ1 â†’ ğœ2

|

âŒŠğ¾âŒ‹

(ğ‘¥ : ğœ) âˆˆ Î“
Î“ âŠ¢ ğ‘¥ : ğœ

Typing rules
Î“ âŠ¢ ğ‘’1 : ğœ1 â†’ ğœ2

Î“ âŠ¢ ğ‘’2 : ğœ1

Î“ âŠ¢ ğ‘’1 ğ‘’2 : ğœ2

Î“, ğ‘¥ : ğœ1 âŠ¢ ğ‘’ : ğœ2
Î“ âŠ¢ ğœ† ğ‘¥ : ğœ1. ğ‘’ : ğœ1 â†’ ğœ2

| Î“, ğ‘¥ : ğœ

contexts Î“ ::= Â·
constants ğ‘˜ âˆˆ Arr(HAD)
âŒŠğ‘˜âŒ‹
(ğ‘’, ğ‘’)
let ğ‘¥ â‰œ ğ‘’ in ğ‘’

expressions ğ‘’ ::= ğ‘¥ |
|

| !
|

| ğ‘’ ğ‘’

| ğœ†ğ‘¥ . ğ‘’

ğ‘˜ âˆˆ âŸ¦Î“âŸ§ â†’HAD âŸ¦ğœâŸ§
Î“ âŠ¢ âŒŠğ‘˜âŒ‹ : ğœ

Î“ âŠ¢ ! : âˆ—

Î“ âŠ¢ ğ‘’1 : ğœ1

Î“ âŠ¢ ğ‘’2 : ğœ2

Î“ âŠ¢ (ğ‘’1, ğ‘’2) : ğœ1 Ã— ğœ2

Î“ âŠ¢ ğ‘’1 : ğœ1

Î“, ğ‘¥ : ğœ1 âŠ¢ ğ‘’2 : ğœ2

Î“ âŠ¢ let ğ‘¥ â‰œ ğ‘’1 in ğ‘’2 : ğœ2

Fig. 5. Syntax and typing rules for ğœ†ğ‘† . The constants are those listed in Fig. 2.

Types

âŸ¦âˆ—âŸ§(Î“) â‰œ 1Set

âŸ¦ğœ1 Ã— ğœ2âŸ§(Î“) â‰œ âŸ¦ğœ1âŸ§(Î“) Ã— âŸ¦ğœ2âŸ§(Î“)

âŸ¦âŒŠğ¾âŒ‹âŸ§(Î“) â‰œ ğ¾ (Î“)

âŸ¦ğœ1 â†’ ğœ2âŸ§(Î“) â‰œ

âˆ«

Î”âˆˆAD

(Î” (cid:123) Î“) Ã— âŸ¦ğœ1âŸ§(Î”) â†’ âŸ¦ğœ2âŸ§(Î”)

Terms

âŸ¦ğ‘’1 ğ‘’2âŸ§(ğ›¾) â‰œ âŸ¦ğ‘’1âŸ§(ğ›¾)(id, âŸ¦ğ‘’2âŸ§(ğ›¾))

âŸ¦ğœ† ğ‘¥ : ğœ1. ğ‘’âŸ§ â‰œ abstract(âŸ¦ğ‘’âŸ§)

âŸ¦âŒŠğ‘˜âŒ‹âŸ§(ğ›¾) â‰œ ğ‘˜ (ğ›¾)
âŸ¦!âŸ§ â‰œ!

âŸ¦(ğ‘’1, ğ‘’2)âŸ§(ğ›¾) â‰œ (âŸ¦ğ‘’1âŸ§(ğ›¾), âŸ¦ğ‘’2âŸ§(ğ›¾))

âŸ¦let ğ‘¥ â‰œ ğ‘’1 in ğ‘’2âŸ§ â‰œ âŸ¦(ğœ†ğ‘¥ . ğ‘’2) ğ‘’1âŸ§

Fig. 6. The semantics of ğœ†ğ‘† .

at its value-level behavior, ğœ•ğ‘˜
âŠ¥
derivative towers are successively consistent.

Mat0(âŸ¦ğ‘’âŸ§(0)
AD

). This proposition follows by first demonstrating that

Proposition 4.9. Given any term Î“ âŠ¢ ğ‘’ : ğœ, the derivative tower âŸ¦ğ‘’âŸ§AD is successively consistent.
Proof sketch. By induction on the typing derivation of ğ‘’. We then see that, to know the
proposition is true, we must know that the derivative towers for all primitives are consistent
(including product projections) and that pairing and composition preserve successive consistency
â–¡
(proof sketch in Proposition 4.5).

Proposition 4.10 (Consistency of differentiation in the first-order language). Given

any term Î“ âŠ¢ ğ‘’ : ğœ, for all ğ‘˜ âˆˆ N, Consğ‘˜+1

(cid:16)

âŸ¦ğ‘’âŸ§(ğ‘˜+1)
AD

, ğœ•ğ‘˜+1
âŠ¥

Mat0(âŸ¦ğ‘’âŸ§(0)
AD

)

(cid:17)

.

Proof sketch. By Proposition 4.9, âŸ¦ğ‘’âŸ§AD

by a simple induction on ğ‘˜.

is successively consistent. Then the proof proceeds
â–¡

5 HIGHER-ORDER SEMANTICS (HAD)
The category AD does not admit exponentiation (function spaces), since its objects are limited to
Rğ‘›. However, higher-order functions yield novel expressive power that is critical for Â§7. To admit
higher-order functions, ğœ†ğ‘† uses a category HAD of presheaves over AD, i.e., HAD = [ADop, Set].

ğœ†ğ‘† : Computable Semantics for Differentiable Programming with Higher-Order Functions and Datatypes

3:17

Syntax and Semantics. The basic syntax of HAD is that of the simply-typed lambda calculus,
shown in Fig. 5, where the constants are those listed in Fig. 2. Fig. 6 presents the semantics of ğœ†ğ‘†
(generic to any Cartesian closed category of presheaves). However, the categorical semantics in
HAD means that ğœ†ğ‘† is inherently extensible and not limited to just those constants in Fig. 2; any
object or morphism in HAD could be added to the language.

We now proceed to describe the semantics of the higher-order constants in Fig. 2.

5.1 Ground Types and First-Order Primitives
Any space ğ‘‹ âˆˆ AD can be lifted into a presheaf HAD by the Yoneda embedding, ğ‘‹ âˆˆ HAD, which
acts as ğ‘‹ (Î“) â‰œ Î“ (cid:123) ğ‘‹ . Because the Yoneda embedding is full and faithful and preserves products,
ground types (and their products) in HAD represent Cartesian spaces and first-order functions
represent smoothish maps. In particular, âŸ¦â„œâŸ§ â‰œ R. Note that all first-order functions from AD can
be lifted into HAD by the Yoneda embedding.

5.2 Smoothish Higher-Order Primitive Functions
Each of the smoothish higher-order primitive functions has a type (â„œ â†’ â„œ) â†’ â„œ in ğœ†ğ‘† . To
construct primitives of this type, we can equivalently construct maps (Î“ Ã— R (cid:123) R) â†’ (Î“ (cid:123) R) for
all Î“ âˆˆ AD in ğœ†ğ¶ .2 Such a map takes as input R-valued expression in a context Î“ Ã— R and produce
an R-valued expression in the context Î“ (for any Î“).

Accordingly, we defined these second-order primitives with parametrically polymorphic map-
pings of derivative towers. We must confirm that these definitions preserve successive consistency,
i.e., they must map successively consistent derivative towers to successively consistent derivative
towers. In general, this boils down to confirming that taking the derivative of the value-level
definitions of each of these primitives (when applied to any possible function ğ‘“ : Î“ Ã— R (cid:123) R) yields
the definitions for the derivatives of these primitives. It is possible to confirm for each definition
that this is the case.

5.2.1

Smooth integral. The integral integral01 is defined as follows for any ğ‘“ : Î“ Ã— R (cid:123) R:

âŸ¦integral01âŸ§HAD (ğ‘“ ) (ğ‘˜) (ğ›¾; ğ‘‘ğ›¾1, . . . , ğ‘‘ğ›¾ğ‘˜ ) â‰œ

âˆ« 1

0

ğ‘“ (ğ‘˜) (ğ›¾, ğ‘¥; (ğ‘‘ğ›¾1, 0), . . . , (ğ‘‘ğ›¾ğ‘˜, 0)) ğ‘‘ğ‘¥ .

Since integration is a linear operator, we essentially just integrate the first-order infinitesimal
perturbations arising from ğ‘“ at every order of derivative. Integration is smooth in the sense that
if its input is smooth, its output will be smooth as well. Note the similarity between the above
AD tower and the result of postcomposing a linear function â„“ after a function ğ‘“ arising from
FaÃ  di Brunoâ€™s formula described previously. The reader may wonder how a semantics invoking
integration might be computable; we discuss this in Â§6.

Smoothish Root Finding. The primitive cutRoot : (â„œ â†’ â„œ) â†’ â„œ smoothly finds the
5.2.2
root of any function with a single isolated root that is positive to its left and negative to its right.
Equivalently, cutRoot is a map (Î“ Ã— R (cid:123) R) â†’ (Î“ (cid:123) R). We will define cutRoot by using the
stream characterization of smooth maps, defining it with a function for its evaluation map and a

2In any category of presheaves [ Cop, Set], letting Â· denote the Yoneda embedding and letting â‡’ denote the internal hom,
then there is an equivalence between constants with the second-order type (ğ´ â‡’ ğµ) â‡’ ğ¶ and the end âˆ«
Î“ (Î“ Ã— ğ´ â†’C ğµ) â†’
(Î“ â†’C ğ¶):

1 â†’[Cop,Set] (ğ´ â‡’ ğµ) â‡’ ğ¶ (cid:27) (ğ´ â‡’ ğµ) â†’[Cop,Set] ğ¶ =

âˆ«

Î“

(ğ´ â‡’ ğµ) (Î“) â†’ ğ¶ (Î“) (cid:27)

âˆ«

Î“

(Î“ Ã— ğ´ â†’C ğµ) â†’ (Î“ â†’C ğ¶)

3:18

Benjamin Sherman, Jesse Michel, and Michael Carbin

smooth map for its derivative:

âŸ¦cutRootâŸ§HAD (ğ‘“ ) (0) â‰œ ğœ†ğ›¾ . [sup{ğ‘¥ : R | ğ‘“ (0) (ğ›¾, ğ‘¥) > 0}, inf {ğ‘¥ : R | ğ‘“ (0) (ğ›¾, ğ‘¥) < 0}]
âŸ¦cutRootâŸ§HAD (ğ‘“ ) â€² â‰œ

(cid:22)
ğ›¾, ğ‘‘ğ›¾ âŠ¢ let ğ‘¦ â‰œ (cid:4)âŸ¦cutRootâŸ§HAD (ğ‘“ )(cid:5) (ğ›¾) in âˆ’

(cid:23)

âŒŠğ‘“ â€²âŒ‹ ((ğ›¾, ğ‘¦), (ğ‘‘ğ›¾, 0))
âŒŠğ‘“ â€²âŒ‹ ((ğ›¾, ğ‘¦), (0, 1))

AD
The formula for the derivative is a simple application of the implicit function theorem. Note that we
have a well-founded recursive reference following the same pattern as with multiplication.

cutRoot enables root-finding only for functions that have only one root. In graphics, for ray
tracing of implicit surfaces, it is useful to be able to find for a function f : â„œ â†’ â„œ the least root
ğ‘¥ âˆˆ [0, 1] such that ğ‘“ switches from positive for values just less than ğ‘¥ to negative for values just
greater than ğ‘¥. firstRoot : (â„œ â†’ â„œ) â†’ â„œ accomplishes this:

âŸ¦firstRootâŸ§HAD (ğ‘“ ) (0) â‰œ ğœ†ğ›¾ . [sup{ğ‘¥ âˆˆ [0, 1] | âˆ€ğ‘ âˆˆ [0, ğ‘¥]. ğ‘“ (0) (ğ›¾, ğ‘) > 0}
, inf {ğ‘¥ âˆˆ [0, 1] | âˆƒğ‘ âˆˆ [0, ğ‘¥]. ğ‘“ (0) (ğ›¾, ğ‘¥) < 0}]
(cid:25)
let ğ‘¦ â‰œ (cid:4)âŸ¦firstRootâŸ§HAD (ğ‘“ )(cid:5) (ğ›¾) in
âˆ’ âŒŠğ‘“ â€² âŒ‹ ( (ğ›¾,ğ‘¦),(ğ‘‘ğ›¾,0))
âŒŠğ‘“ â€² âŒ‹ ( (ğ›¾,ğ‘¦),(0,1))

âŸ¦firstRootâŸ§HAD (ğ‘“ ) â€² â‰œ

ğ›¾, ğ‘‘ğ›¾ âŠ¢

(cid:24)

AD

Like with cutRoot, its derivatives are determined by the implicit function theorem; the only
difference is in the definition of the value of the root.

Smoothish Optimization. ğœ†ğ‘† admits primitives argmax01, max01 : (â„œ â†’ â„œ) â†’ â„œ that
5.2.3
find the maximizing argument and the maximum, respectively, of a function f : â„œ â†’ â„œ over
the unit interval. Equivalently, each of argmax01 and max01 are maps (Î“ Ã— R (cid:123) R) â†’ (Î“ (cid:123) R).

We first describe argmax01, which is defined as follows:

âŸ¦argmax01âŸ§HAD (ğ‘“ ) (0) â‰œ ğœ†ğ›¾ . hull

(cid:26)

âŸ¦argmax01âŸ§HAD (ğ‘“ ) â€² â‰œ

(cid:30)
(cid:30)
(cid:30)
(cid:30)
(cid:30)
ğ›¾, ğ‘‘ğ›¾ âŠ¢
(cid:30)
(cid:30)
(cid:30)
(cid:28)

ï£±ï£´ï£´ï£´ï£´ï£´ï£²
ï£´ï£´ï£´ï£´ï£´

ï£³

âŠ¥

(cid:18)

{ğ‘¥ âˆˆ [0, 1] | ğ‘“ (ğ›¾, ğ‘¥) = max
ğ‘§ âˆˆ [0,1]

ğ‘“ (ğ›¾, ğ‘§)}

(cid:19)

let ğ‘¦ â‰œ (cid:4)âŸ¦argmax01âŸ§HAD (ğ‘“ )(cid:5) (ğ›¾) in

let ğ‘“ â€²

ğ‘¦ â‰œ âŒŠğ‘“ â€²âŒ‹ ((ğ›¾, ğ‘¦), (0, 1)) in

âˆ’ âŒŠğ‘“ â€²â€² âŒ‹ ( ( (ğ›¾,ğ‘¦),(0,1)),( (ğ‘‘ğ›¾,0),(0,0)))
âŒŠğ‘“ â€²â€² âŒ‹ ( ( (ğ›¾,ğ‘¦),(0,1)),( (0,1),(0,0)))
0
0

0 < ğ‘¦ < 1
ğ‘¦ = 0 âˆ§ ğ‘“ â€²
ğ‘¦ = 1 âˆ§ ğ‘“ â€²
otherwise

ğ‘¦ < 0
ğ‘¦ > 0

(cid:27)

(cid:31)
(cid:31)
(cid:31)
(cid:31)
(cid:31)
(cid:31)
(cid:31)
(cid:31)
(cid:29)

AD

The input is a smooth map ğ‘“ : Î“ Ã— R (cid:123) R. In general, for a ğ›¾ âˆˆ Î“, there may be many values
of ğ‘¥ achieving the same maximum ğ‘“ (ğ›¾, ğ‘¥), so the value-level definition takes the convex hull of
the set of those maximizing arguments. The derivative of argmax01 is not âŠ¥ only when its value
is maximal, i.e., there is only one maximizing argument, which we will call ğ‘¦. There are three
possibilities for ğ‘¦: either 0 < ğ‘¦ < 1, or ğ‘¦ = 0, or ğ‘¦ = 1. In the case that 0 < ğ‘¦ < 1, then if ğ‘“ â€²â€² is
defined at (ğ›¾, ğ‘¦), then we know that ğ‘“ â€²
ğ‘¦ ,
ğ‘¦ (ğ›¾, ğ‘¦) = 0 and that this argmax is an isolated root of ğ‘“ â€²
ğ‘¦ is the derivative of ğ‘“ with respect to its latter argument. Any infinitesimal perturbation
where ğ‘“ â€²
ğ‘¦ , so the implicit function theorem
ğ‘‘ğ›¾ to ğ›¾ results in an infinitesimal perturbation to the root of ğ‘“ â€²
defines how the root changes. If the maximizing argument ğ‘¦ is on the boundary, i.e., ğ‘¦ = 0 or ğ‘¦ = 1,
then if we additionally know that either ğ‘“ â€²
ğ‘¦ (ğ›¾, ğ‘¦) > 0, respectively, then it must be
ğ‘¦ (ğ›¾, ğ‘¦) < 0 or ğ‘“ â€²
the case that the derivative of the argmax is 0, because the argmax will be stuck at the boundary
no matter how ğ›¾ might be infinitesimally perturbed.

ğœ†ğ‘† : Computable Semantics for Differentiable Programming with Higher-Order Functions and Datatypes

3:19

We can now proceed to describe max01, whose derivative is defined in terms of argmax01:

ğ‘“ (ğ›¾, ğ‘¥)

âŸ¦max01âŸ§HAD (ğ‘“ ) (0) â‰œ ğœ†ğ›¾ . max
ğ‘¥ âˆˆ [0,1]
âŸ¦max01âŸ§HAD (ğ‘“ ) â€² â‰œ (cid:0)ğ‘“ â—¦ âŸ¦argmax01âŸ§HAD (ğ‘“ )(cid:1) â€²
Just as the derivative of max depends on which argument results in the max, similarly the derivative
of max01 is a function of the maximizing argument. If we can isolate a single argmax, then max01
f = f (argmax01 f), and thus all the derivatives of max01 f follow from the chain rule and the
smooth derivatives of f and argmax01 f.

5.3 Internal Derivatives of Functions at All Types
The primitive tangent A B : (A â†’ B) â†’ Tan A â†’ Tan B permits the expression of the
derivative of any function in ğœ†ğ‘† , with any input type A and output type B. These types are much more
general than those on which differentiation in classically defined in mathematics. In this section,
we will explain the semantics of tangent and Tan, which generalize the notion of differentiation
from AD to apply to all objects in HAD (i.e., all types in ğœ†ğ‘† ).

We need to systematically generalize the derivative of AD, as expressed with the postfix â€²
operator, to apply to HAD. Following VÃ¡kÃ¡r et al. [2018], we can apply the categorical technique
of left Kan extensions, which extend a functor on a base category to one that acts on presheaves
over that category. Our definition of generalized tangent spaces and its properties will also be
similar to the dvs diffeology on internal tangent bundles as described by Christensen and Wu
[2017]. Accordingly, we can lift the operation of forward-mode differentiation from the first-order
language AD to the higher-order language HAD. Defining

valueWithDer : (ğ´ (cid:123) ğµ) â†’ (ğ´ Ã— ğ´ (cid:123) ğµ Ã— ğµ)
valueWithDer(ğ‘“ ) â‰œ âŸ¦ğ‘¥, ğ‘‘ğ‘¥ âŠ¢ (âŒŠğ‘“ âŒ‹ (ğ‘¥), âŒŠğ‘“ â€²âŒ‹ (ğ‘¥, ğ‘‘ğ‘¥))âŸ§AD
we find that valueWithDer defines a functor on AD acting on objects by ğ‘‹ â†¦â†’ ğ‘‹ Ã— ğ‘‹ from a space
ğ‘‹ to its tangent bundle ğ‘‹ Ã— ğ‘‹ , where the tangent bundle ğ‘‹ Ã— ğ‘‹ represents a point of ğ‘‹ together
with an infinitesimal perturbation of that point. The functoriality of valueWithDer follows from
the chain rule of differentiation (and that âŒŠidâ€²âŒ‹ (ğ‘¥, ğ‘‘ğ‘¥) = ğ‘‘ğ‘¥).

,

This functor can be extended to HAD via a left Kan extension to produce a functor Tan and
its functorial map tangent A B : (A â†’ B) â†’ Tan A â†’ Tan B, which runs generalized
forward-mode derivatives, interpreted geometrically as a pushforward of the tangent bundles.
Concretely, we define the tangent bundle functor Tan, as a left Kan extension, corresponds to a
coend:

Tan(ğ¹ )(Î“) (cid:27)

âˆ« Î”

(Î“ (cid:123) Î”2) Ã— ğ¹ (Î”).

Informally, the tangent bundle over the ğœ†ğ‘† type ğ¹ in a context Î“ is represented by a pair of a value
and infinitesimal perturbation Î“ (cid:123) Î”2 for some Cartesian space Î” (i.e., Î” = Rğ‘› for some ğ‘› âˆˆ N),
together with a map from the space Î” into the type ğ¹ . Thus, if we wish to define an infinitesimal
perturbation into a complicated type ğ¹ , we are able to do it by choosing a Cartesian space Î” to
express that infinitesimal perturbation, and then we construct a map from Î” to ğ¹ . All elements of
the tangent bundle of ğ¹ arise in that way.

We now explain how these tangent bundles work with an example. Suppose ğ¹ = R2 and we want
to represent the tangent bundle ((0, 1), (1, 0)) âˆˆ R2 Ã— R2, i.e., the vector (0, 1) moving infinitesimally
in the (1, 0) direction. Since there are no variables in the context, we can define the tangent bundle

3:20

Benjamin Sherman, Jesse Michel, and Michael Carbin

at once for all Î“. The type of generalized tangent bundles is

Tan(R2)(Î“) (cid:27)

âˆ« Î”

(âˆ— (cid:123) Î”2) Ã— (Î” (cid:123) R2).

We can represent the tangent bundle ((0, 1), (1, 0)) âˆˆ R2 Ã— R2 in two equivalent ways. The straight-
forward way is to take Î” = R2 and put the point and its perturbation in the first component and
the identity map in the second,

(R2, (âŸ¨(0, 1), (1, 0)âŸ©, id)).

: R â†’ R2 defined by ğ‘“ (ğ‘¡) =
Alternatively, we can represent it with a parametric function ğ‘“
(0, 1) + ğ‘¡ Â· (1, 0), describing a point that moves from (0, 1) at ğ‘¡ = 0 in the direction of (1, 0) as ğ‘¡
increases:

(R, (âŸ¨0, 1âŸ©, ğœ†ğ‘¡ . (0, 1) + ğ‘¡ Â· (1, 0))).

Two members (ğœ1, (ğ‘“1, ğ‘”1)) and (ğœ2, (ğ‘“2, ğ‘”2)) of Tan(R2)(Î“) are equivalent if

valueWithDer(ğ‘”1) â—¦ ğ‘“1 = valueWithDer(ğ‘”2) â—¦ ğ‘“2.
Indeed, this is the case for the two examples above, as both compositions yield âŸ¨(0, 1), (1, 0)âŸ©. This
criterion for equivalence is for representable types such as R2 but generalizes for tangent bundles
over types that are not representable. It intuitively captures the notion that the first component
of the tuple represents a tangent bundle of a representable space, whereas the second is a map
that applies to that output but is yet to be differentiated. This is the justification for applying
valueWithDer above.

The Kan extension is genuinely an extension of the underlying functor valueWithDer. That is,
we have the equivalence Tan(ğ´) (cid:27) ğ´2, where Â· is the Yoneda embedding (Proposition 5.2). This
means that the generalized tangent bundle for Cartesian spaces Rğ‘› is indeed Rğ‘› Ã— Rğ‘›: one Rğ‘› for
the point and one Rğ‘› for the infinitesimal perturbation.

The generalized tangent bundle functor supports other operations as well. A polymorphic
function tangentValue A : Tan A â†’ A projects out the base point. The primitive tangentProd
A B : Tan (A * B) (cid:27) Tan A * Tan B implements the following isomorphism:

Proposition 5.1. Tangent bundles commute with products, i.e., Tan(ğ¹ Ã— ğº) (cid:27) Tan(ğ¹ ) Ã— Tan(ğº).
Proof sketch. First, we construct mappings in both directions: We easily have the product
projections Tan(ğ¹ Ã— ğº) â†’ Tan(ğ¹ ) and Tan(ğ¹ Ã— ğº) â†’ Tan(ğº). Conversely, given (Tan(ğ¹ ) Ã—
Tan(ğº))(Î“), we get Î”1 and Î”2 with ğ‘“ : Î“ (cid:123) Î”2
2 and ğ¹ (Î”1) and ğº (Î”2). Taking
Î” â‰œ Î”1 Ã— Î”2, we can define â„(ğ›¾) â‰œ ((ğ‘¥, ğ‘¦), (ğ‘‘ğ‘¥, ğ‘‘ğ‘¦)) where (ğ‘¥, ğ‘‘ğ‘¥) = ğ‘“ (ğ›¾) and (ğ‘¦, ğ‘‘ğ‘¦) = ğ‘”(ğ›¾).
2 : ğº (Î”2) â†’ ğº (Î”1 Ã— Î”2), we can produce
Using the pullbacks ğœ‹ âˆ—
Tan(ğ¹ Ã— ğº)(Î“).

1 : ğ¹ (Î”1) â†’ ğ¹ (Î”1 Ã— Î”2) and ğœ‹ âˆ—

1 and ğ‘” : Î“ (cid:123) Î”2

Next, it is possible to confirm that these mappings are mutually inverse, using the fact that
valueWithDer(ğœ†(ğ‘¥, ğ‘¦).(ğ‘“ (ğ‘¥), ğ‘”(ğ‘¦)))((ğ‘¥, ğ‘¦), (ğ‘‘ğ‘¥, ğ‘‘ğ‘¦)) = ((ğ‘“ (ğ‘¥), ğ‘”(ğ‘¥)), (ğ‘“ â€²(ğ‘¥, ğ‘‘ğ‘¥), ğ‘”â€²(ğ‘¦, ğ‘‘ğ‘¦))),

together with general properties of limits and functoriality of Tan, ğ¹ , and ğº.

â–¡

The primitive tangent_R : Tan â„œ (cid:27) â„œ * â„œ that implements the following isomorphism

(for the special case of â„œ):

Proposition 5.2. We have the equivalence Tan(ğ´) (cid:27) ğ´2, where Â· is the Yoneda embedding.
Proof.

Tan(ğ´)(Î“) (cid:27)

âˆ« Î”

(Î“ (cid:123) Î”2) Ã— (Î” (cid:123) ğ´).

ğœ†ğ‘† : Computable Semantics for Differentiable Programming with Higher-Order Functions and Datatypes

3:21

Given ğ‘“ : ğ´2(Î“) = Î“ (cid:123) ğ´2, we can take Î” = ğ´ and use (ğ‘“ , id). Given an element of Tan(ğ´)(Î“), i.e.,
some Î” and ğ‘“ : Î“ (cid:123) Î”2 and ğ‘” : Î” (cid:123) ğ´, then valueWithDer(ğ‘”) â—¦ ğ‘“ : ğ´2(Î“).
â–¡

Note that we have tangentValue â—¦ tangent_R = fst, i.e., the first component is the base point

and the second is the infinitesimal perturbation.

Note that the types to represent isomorphisms of tangent bundles are not necessarily iso-
morphisms in ğœ†ğ‘† : the type (cid:27) just corresponds to pairs of maps back and forth. The primitive
tangentTo_R A : Tan (A â†’ â„œ) (cid:27) (A â†’ â„œ) * (A â†’ â„œ), in which tangent bundles
distribute over functions into â„œ, implements mappings that are an isomorphism only when we
restrict â„œ to R (rather than all of R):

Proposition 5.3. There is an isomorphism Tan(ğ´ â‡’ R) (cid:27) ğ´ â‡’ Tan(R) (cid:27) ğ´ â‡’ R2.

Proof. First, we construct the mappings in each direction. Note that these types are:

Tan(ğ´ â‡’ R)(Î“) (cid:27)

âˆ« Î”

(Î“ (cid:123) Î”2) Ã—

âˆ«

(ğ‘‹ (cid:123) Î”) â†’ ğ´(ğ‘‹ ) â†’ (ğ‘‹ (cid:123) R)

(ğ´ â‡’ R2)(Î“) (cid:27)

âˆ«

ğ‘‹

ğ‘‹
(ğ‘‹ (cid:123) Î“) â†’ ğ´(ğ‘‹ ) â†’ (ğ‘‹ (cid:123) R2)

Given ğ‘“ : (ğ´ â‡’ R2), we take Î” = Î“ Ã— R, and use

âˆƒÎ“ Ã— R. (ğœ†ğ›¾ . ((ğ›¾, 0), (0, 1)), Î›ğ‘‹ . ğœ†(ğ‘’ : ğ‘‹ (cid:123) Î“ Ã— R). ğœ†(ğ‘ : ğ´(ğ‘‹ )).
let (ğ‘”, ğ‘‘ğ‘”) = ğ‘“ (ğ‘‹, ğœ‹1 â—¦ ğ‘’, ğ‘) in ğœ†ğ‘¥ : ğ‘‹ . ğ‘”(ğ‘¥) + ğœ‹2(ğ‘’ (ğ‘¥)) Â· ğ‘‘ğ‘”(ğ‘¥))

Conversely, given a member of Tan(ğ´ â‡’ R)(Î“), i.e., a Î” with ğ‘‘ : Î“ (cid:123) Î”2 and ğ‘“ : âˆ«

ğ‘‹ (ğ‘‹ (cid:123) Î”) â†’

ğ´(ğ‘‹ ) â†’ (ğ‘‹ (cid:123) R), we can provide
Î›ğ‘‹ . ğœ†(ğ‘’ : ğ‘‹ (cid:123) Î“). ğœ†(ğ‘ : ğ´(ğ‘‹ )). valueWithDer(ğ‘“ (ğ‘‹, ğœ‹1âŸ¨ğ‘“ (ğ‘‹, ğœ‹1 â—¦ ğ‘‘ â—¦ ğ‘’, ğ‘), ğ‘“ (ğ‘‹, ğœ‹2 â—¦ ğ‘‘ â—¦ ğ‘’, ğ‘)âŸ©)).
Next, we must confirm that these mappings are mutually inverse. This boils down to the basic
â–¡

identity ğ‘“ â€²(ğ‘¥; ğ‘£) =

ğœ• (ğ‘“ (ğ‘¥+ğ‘¡ Â·ğ‘£))
ğœ•ğ‘¡

|ğ‘¡ =0 .

Note that it is not an isomorphism for all of â„œ, because we rely on the algebraic law ğ‘¥ + 0 Â· ğ‘¦ = ğ‘¥

for all ğ‘¦, but if we allow ğ‘¦ âˆˆ R \ R, there is the counterexample ğ‘¥ + 0 Â· âŠ¥ = âŠ¥.

5.4 Consistency

Proposition 5.4 (Consistency of differentiation in the higher-order language). Given
any term Î“ âŠ¢ ğ‘’ : ğœ in ğœ†ğ‘† where Î“ is a context of all ground types and ğœ is a ground type, then âŸ¦ğ‘’âŸ§HAD
is equivalent to some first-order smoothish map ğ‘“ , i.e., successively consistent derivative tower.

Proof. Since the Yoneda embedding is full and faithful, first-order terms in HAD correspond to
â–¡

morphisms in AD, so this statement reduces to Proposition 4.10.

6 COMPUTABILITY AND NUMERICALLY-SOUND IMPLEMENTATION
It is not obvious that the categorical semantics of ğœ†ğ‘† we present in Â§4-5 is actually implementable
(in a sound manner). The semantics critically uses reals and real arithmetic, rather than some
approximation like floating point (which would fail to give even the most basic equalities such as
1/5 + 2/5 = 3/5). And value-level definitions of higher-order primitives in ğœ†ğ‘† are expressed in terms
of mathematical operations for integration, optimization, and root finding applied to arbitrary
continuous maps. In fact, our semantic development is computable, and we have implemented it in
a numerically sound manner as an embedded DSL in Haskell.

3:22

Benjamin Sherman, Jesse Michel, and Michael Carbin

Our semantics can be developed constructively and interpreted within the internal language
of another topos, which we call ğœ†ğ¶ , in order to provide a computable interpretation. We base
ğœ†ğ¶ on MarshallB [Sherman et al. 2019]. Our implementation of ğœ†ğ‘† more-or-less directly follows
interpreting the semantics of ğœ†ğ‘† within ğœ†ğ¶ and in turn implementing ğœ†ğ¶ in Haskell.

ğœ†ğ¶ is a topos of sheaves over a Cartesian monoidal category that we call CTop. CTop is a
category of computably presented topological spaces and computable continuous maps. ğœ†ğ¶ is the
topos of sheaves over CTop with the open cover topology (along the lines of [Fourman 1984]).

What results is a stack of languages: ğœ†ğ‘† reducing to AD, implemented in ğœ†ğ¶ , which reduces
to CTop, which carries the final executable content of ground terms. We can view it like a stack
of metaprogramming languages on top of CTop: ultimately, when a closed term of ğœ†ğ‘† (or any
other language in the stack) of ground type is evaluated and displayed as a sequence of improving
approximations, it is in fact a closed term of CTop, i.e., a computable point of a topological space.

Semantics of ğœ†ğ¶ and Implications for ğœ†ğ‘† . ğœ†ğ¶ is a language whose types are (generalized) topological
spaces with computable structure and whose functions are (generalized) computable continuous
maps. ğœ†ğ¶ permits all the higher-order functions and higher-order types that we will seek to define
in ğœ†ğ‘† and enables their computation to arbitrary precision. This section describes ğœ†ğ¶ by example. In
ğœ†ğ¶ , the type â„œ in ğœ†ğ¶ represents the interval reals R. One closed term, or value, of type â„œ is sqrt
2. A value of â„œ represents a point of the space R and is computationally represented by streams of
increasingly precise approximations (i.e., monotone with respect to âŠ‘):

> sqrt 2 : â„œ

[1.4142135619, 1.4142135624]
[1.414213562370, 1.414213562384]
[1.4142135623729, 1.4142135623733]
...

Note that these streams of increasingly precise approximations can be used to provide the
arbitrary-precision interface where one asks for a precision tolerance and gets a result. Each
interval [ğ‘¥, ğ‘¥], where ğ‘¥ âˆˆ {âˆ’âˆ} âˆª D, ğ‘¥ âˆˆ D âˆª {âˆ}, has either infinite or dyadic-rational (D =
{ğ‘˜/2ğ‘› | ğ‘˜ âˆˆ Z, ğ‘› âˆˆ N}) endpoints and represents partial information about sqrt 2: the first
component represents a rational lower bound (with âˆ’âˆ being a vacuous bound) and the second an
upper bound (with âˆ vacuous). ğœ†ğ¶ is sound in the sense that these bounds are guaranteed to hold
of the true value. Two closed terms of â„œ in ğœ†ğ¶ are considered equivalent if their streams always
overlap, even if the streams are not identical. For instance, (sqrt 2) 2 = 2:

> (sqrt 2)2

> 2

[1.9999999986, 2.0000000009]
[1.999999999985, 2.000000000058]
[1.9999999999991, 2.0000000000009]
...

[2.0000000000, 2.0000000000]
[2.000000000000, 2.000000000000]
[2.0000000000000, 2.0000000000000]
...

The equivalence means that one can substitute (sqrt 2)2 for 2 within any program without
affecting its meaning. In contrast, the floating-point computation for many languages and CPUs
returns 2.0000000000000004, which is not 2 and does not itself indicate a larger range of possible
values that includes 2, and would not validate the equation (sqrt 2)Ë†2 = 2.

First-order functions in ğœ†ğ¶ are stream transformers of their approximations. For instance, applying

the squaring function (-) 2 : â„œ â†’ â„œ to sqrt 2 yields the following result:

ğœ†ğ‘† : Computable Semantics for Differentiable Programming with Higher-Order Functions and Datatypes

3:23

> sqrt 2 : â„œ

> (sqrt 2)2 : â„œ

[1.4142135619, 1.4142135624]
[1.414213562370, 1.414213562384]
[1.4142135623729, 1.4142135623733]
...

[1.9999999986, 2.0000000009]
[1.999999999985, 2.000000000058]
[1.9999999999991, 2.0000000000009]
...

In this case, the squaring function squares each input interval to produce output intervals. The
computation is continuous in the sense that the computation of each interval result of (sqrt 2) 2
needs only an interval approximation of sqrt 2. First-order functions such as (-) 2 are continuous
maps, meaning that in order to approximate the output to any finite level of precision, it suffices to
inspect the input to only a finite level of precision.

Implementing Higher-Order Primitives. The value-level definitions of higher-order primitives in ğœ†ğ‘†
are expressed in terms of mathematical operations for integration, optimization, and root finding. Itâ€™s
not obvious that these are computable. However, MarshallB [Sherman et al. 2019] demonstrates how
to endow a language with computable implementations of Riemannian integration, maximization
over compact sets, as well as a Dedekind cut primitive that is essentially equivalent to the root
finding of cutRoot and can be used to implement the root finding of firstRoot. We were able to
implement these MarshallB primitives in ğœ†ğ¶ and use them to implement the higher-order primitives
in ğœ†ğ‘† .

Haskell Implementation. We implemented ğœ†ğ‘† as an embedded language within Haskell. Because
Rğ‘› and Rğ‘› are representable within CTop, we actually implement AD directly using CTop within
Haskell, rather than working internally to ğœ†ğ¶ . We implement CTop using an interval-arithmetic
library that in turn uses MPFR [Fousse et al. 2007], a library for multi-precision floating-point
arithmetic. We include this implementation and all the code examples as supplementary material,
and will make it publicly available. See the readme file for more information about the code.

Computability and Numerical Soundness. The semantics for ğœ†ğ‘† supports a realistic machine model
for computing real-valued results to arbitrary precision. This is in contrast to semantics that permit
Boolean-valued comparison of real numbers, and computational models like Real RAM, in which
a machine can compare real numbers in constant time. When algorithms are designed based on
such models but implemented with floating-point arithmetic, those implementations may fail to be
robust to floating-point error (e.g., [Kettner et al. 2008]). In contrast, the continuity inherent in ğœ†ğ‘† â€™s
semantics provides a robustness guarantee: arbitrary-precision approximations of the output can
be produced by inspecting only finite-precision approximations of the input.

7 HIGHER-ORDER DATATYPES AND LIBRARIES
This section demonstrates the unique expressivity and computability of ğœ†ğ‘† . We use the novel higher-
order primitives available in ğœ†ğ‘† â€“ including integration, optimization, and root-finding â€“ to build
libraries for constructing and computing with three different higher-order datatypes: probability
distributions (and measures), implicit surfaces, and generalized parametric surfaces. Since these
libraries are implemented in ğœ†ğ‘† , they are differentiable (arbitrarily many times). For each library,
we compute an example differentiation task. Fig. 7 shows a high-level overview of each example.
We now detail the implementation of each of the libraries and provide the implementations for
each of the corresponding examples.

3:24

Benjamin Sherman, Jesse Michel, and Michael Carbin

1

0

0

ğ‘¥

1

(a) Probability distributions:
How does the mean and vari-
ance of the uniform distribution
change as you weight its mass
to tilt more towards higher
values and away from lower
values?

1

0

0

1

Implicit

(b)
surfaces: A ray
of light from a source above
bounces off a circle before hit-
ting a camera. How does the
brightness change when the cir-
cle is moved up?

(c) Generalized parametric sur-
faces: How does the Hausdorff
distance between the quarter
circle and the â€œLâ€ shape change
as the quarter circle is moved
up?

Fig. 7. Three example differentiation problems we will express and compute with libraries in ğœ†ğ‘† .

7.1 Probability Distributions (and Measures)
Probability is central to many machine-learning applications. Loss functions for Bayesian neural net-
works, GANs, etc. involve expectations over probability distributions. However, no previous work
on the semantics of AD supports probability distributions3. The interaction between probabilistic
choice and differentiation is nontrivial, and the lack of a semantic treatment of their interaction has
real consequences for machine-learning practitioners using AD libraries who seek to combine them.
Practitioners often use Monte Carlo sampling to approximate expectations, but because derivatives
cannot be propagated through the samplers in common frameworks such as PyTorch and Tensor-
Flow, code that looks correct and produces appropriate approximations of its value-level output can
end up producing incorrect derivatives when AD is applied (as mentioned in the introduction). This
common pitfall, which can be difficult to detect, necessitates the reparameterization trick, where
code is rewritten such that samplers do not depend on any parameters that are to be differentiated.
ğœ†ğ‘† can represent a monad of probability distributions P, making it the first language semantics
to support differentiation through probabilistic choice, including through distributions such as the
uniform distribution on the unit interval. Supporting probability distributions is hard because they
must involve higher-order functions: expectations are higher-order functions P (ğ´)Ã—(ğ´ â†’ R) â†’ R,
as is the monadic bind operator P (ğ´) Ã— (ğ´ â†’ P (ğµ)) â†’ P (ğµ) that supports compositional
construction of complex probability distributions from simple ones.

A ğœ†ğ‘† Library for Probability Distributions and Measures. Probability distributions, measures, and

distributions (in the sense of generalized functions) can all be described as integrals,

type Integral A = (A â†’ â„œ) â†’ â„œ,

detailed in Fig. 8. Integrals are functions ğ‘– : (ğ´ â†’ R) â†’ R which are linear in their arguments.
Measures are those integrals ğ‘– satisfying ğ‘– (ğ‘“ ) â‰¥ 0 whenever ğ‘“ (ğ‘¥) â‰¥ 0 for all ğ‘¥ âˆˆ R. Probability
distributions are those measures ğ‘– satisfying ğ‘– (ğœ†ğ‘¥ . 1) = 1; the integral for a probability distribution
computes the expectation of a real-valued function under that distribution.

Example. What happens if we make an infinitesimal perturbation to the uniform distribution as

in Fig. 7a? How will its mean and variance change? Differentiation answers these questions.

3While other works can represent expectations over distributions with finite support as sums, this would not work for
distributions with infinite support. Loss functions frequently involve expectations over distributions with infinite support.

ğœ†ğ‘† : Computable Semantics for Differentiable Programming with Higher-Order Functions and Datatypes

3:25

type Integral A = (A â†’â„œ) â†’â„œ

let dirac A (x : A) : Integral A = ğœ† f : A â†’â„œâ‡’ f x
let bind A B (x : Integral A) (f : A â†’ Integral B) : Integral B

= ğœ† k : B â†’â„œâ‡’ x (ğœ† a : A â‡’ f a k)

let zero A : Integral A = ğœ† f : A â†’ Real â‡’ 0
let add A (x y : Integral A) : Integral A = ğœ† f : A â†’â„œâ‡’ x f + y f
let map A B (f : A â†’ B) (e : Integral A) : Integral B =

ğœ† k : B â†’â„œâ‡’ e (ğœ† x : A â‡’ k (f x))

let factor (x : â„œ) : Integral unit = ğœ† f : unit â†’â„œâ‡’ f () * x
let measToProb A (e : Integral A) : Integral A = ğœ† f : A â†’â„œâ‡’ e f / e (ğœ† x : A â‡’ 1)
let bernoulli (p : â„œ) : Integral ğ”… = ğœ† f : ğ”…â†’â„œâ‡’ p * f tt + (1 - p) * f ff
let uniform : Integral â„œ = integral01

let total_mass A (mu : Integral A) = mu (ğœ† x : A â‡’ 1)
let mean (mu : Integral â„œ) = mu (ğœ† x : â„œâ‡’ x)
let variance (mu : Integral â„œ) = mu (ğœ† x : â„œâ‡’ (x - mean mu) 2)

Fig. 8. Integrals and ğœ†ğ‘† programs that manipulate them.

0

The uniform distribution over the interval [0, 1] is equivalent to the integral of [0, 1], namely
1ğ‘‘ğ‘¥ = 1 (as any probability distribution

uniform : Integral â„œ = integral01. It satisfies âˆ« 1
must), and has mean âˆ« 1
0

ğ‘¥ğ‘‘ğ‘¥ = 1/2 and variance âˆ« 1

0 (ğ‘¥ âˆ’ 1/2)2ğ‘‘ğ‘¥ = 1/12.

Next, we must craft a perturbation to consider. There is an isomorphism Tan (Integral A)
(cid:27) Integral A * Integral A, which says that a perturbation to an integral itself has the form
of an integral as well. Hence, our perturbation must also be an integral. In addition, because we
are perturbing a probability distribution, whose total mass must sum to 1, the total mass of our
perturbation must be 0: if we are to increase mass somewhere, we must decrease it elsewhere.
Given these design considerations, consider the following perturbation to the uniform distribution
that makes 1 more likely, 0 less likely, 1/2 equally likely as before, and interpolates between these:4

let change : Integral â„œ = ğœ† f : â„œâ†’â„œâ‡’ integral01 (ğœ† x : â„œâ‡’ (x - 1/2) * f x)

The perturbation is an integral with total mass 0: âˆ« 1
Returning to our question of how this perturbation changes the mean and variance of uniform, for
convenience let der : (Integral A â†’ â„œ) â†’ Integral A â†’ Integral A â†’ â„œ compute the
derivative of its argument at a point and infinitesimal perturbation, using the appropriate coercions
and projections to and from tangent spaces.5 Since mean is linear, its derivative is independent of
the current value and is just the original mean function applied to the infinitesimal perturbation:

0 (ğ‘¥ âˆ’ 1/2)ğ‘‘ğ‘¥ = 0.

der mean uniform change

= mean change
= integral01 (ğœ† x : â„œâ‡’ (x - 1/2) * x)
= 1/12

And indeed, thatâ€™s what we compute:

4Fig. 7a shows a schematic of this perturbation.
5let der f x dx = snd (tangetTo_R.to (tangent f (tangetTo_R.from (x, dx))))

3:26

Benjamin Sherman, Jesse Michel, and Michael Carbin

type Surface A = A â†’â„œ

let circle (c : â„œ2) (r : â„œ) : Surface (â„œ2) =

ğœ† x : â„œ2 â‡’ r2 - (x[0] - c[0])2 - (x[1] - c[1]) 2

let halfplane A (normal : â„œ2) : Surface (â„œ2) = ğœ† x : â„œ2 â‡’ dot normal x
let union A (s sâ€™ : Surface A) : Surface A = ğœ† x : A â‡’ max (s x) (sâ€™ x)
let intersection A (s sâ€™ : Surface A) : Surface A = ğœ† x : A â‡’ min (s x) (sâ€™ x)
let complement A (s : Surface A) : Surface A = ğœ† x : A â‡’ - (s x)

Fig. 9. A ğœ†ğ‘† library for implicit surfaces.

eps=1e-3> der mean uniform change

[0.0829, 0.0837]

However, variance is nonlinear, so its derivative does depend on the current point. Letâ€™s compute
it and then reason about the answer:

eps=1e-2> der variance uniform change

[-0.005, 0.004]

We can reason about the change in the variance with the laws about derivatives, just as we would
in first-order cases:

der variance uniform change

= der (ğœ† mu : Integral â„œâ‡’ mu (ğœ† x : â„œâ‡’ x2) - (mean mu) 2) uniform change
= change (ğœ† x : â„œâ‡’ x 2) - 2 * mean uniform * mean change
= integral01 (ğœ† x : â„œâ‡’ (x-1/2)*x2) - 2 * 1/2 * 1/12
= 1/12 - 1/12
= 0

So it turns out that this infinitesimal perturbation will actually not change the variance.

7.2 Implicit Surfaces and Root-Finding
Â§2 and Fig. 1 presented a library for implicit surfaces and a function for performing ray tracing on
scenes represented by implicit surfaces.

Fig. 9 presents a library for constructing implicit surfaces. An implicit surface is a representation
of a surface (such as a sphere or plane) with the zero-set of a differentiable function ğ‘“ : Rğ‘› â†’ R
(where usually we consider ğ‘› = 3 for 3-dimensional space). Whether ğ‘“ (ğ‘¥, ğ‘¦) is positive, negative,
or zero indicates whether (ğ‘¥, ğ‘¦) is inside, outside, or on the border of the surface, respectively. The
angle at which a ray deflects is determined by the surface normal at the location where the ray hits
the surface, which is the vector that is orthogonal to the plane that is tangent to the surface.

In ğœ†ğ‘† , we can represent implicit surfaces as type Surface A = A â†’ â„œ. Fig. 9 presents a small
library for constructing implicit surfaces. The Boolean operations of Constructive Solid Geometry
(CSG) â€“ union, intersection, and complement â€“ are available for these implicit surfaces. Because
ğœ†ğ‘† permits nonsmooth functions, it is able to represent implicit surfaces that donâ€™t necessarily
correspond to manifolds, such as the union of two spheres that are offset and equally sized. Where
they touch, there is a corner, and thus there is no (unique) surface normal.

ğœ†ğ‘† : Computable Semantics for Differentiable Programming with Higher-Order Functions and Datatypes

3:27

Our smooth ray tracer, shown in Fig. 1c, renders the image of an implicit surface with a single
light source and a Lambertian reflectance model, computing the angle at which light reflects off of
the surface using automatic differentiation. The code in Fig. 1c reflects the contributions of Niemeyer
et al. [2020], who use a differentiable ray-tracing renderer to learn implicit 3D representations of
surfaces, noting their â€œkey insight is that depth gradients can be derived analytically using the
concept of implicit differentiation.â€

We can implement a smooth (and thus differentiable) ray tracer for implicit surfaces in ğœ†ğ‘† in just

a few lines of code, and the use of implicit differentiation automatically falls out.

7.3 Generalized Parametric Surfaces and Optimization
We now build a library within ğœ†ğ‘† for constructing shapes and computing operations on them. For
instance, we can represent the quarter disk and unit square in Fig. 7c as shapes and compute the
âˆš
Hausdorff distance between them, which equals

2 âˆ’ 1, as:

eps=1e-3> hausdorffDist R2Dist lShape (quarterCircle 0)

[0.4138, 0.4145]

We can also compute derivatives, such as the infinitesimal perturbation in the Hausdorff distance

that would result if the quarter circle were to infinitesimally move up by a unit magnitude:

eps=1e-1> deriv (ğœ† y : â„œ â‡’ hausdorffDist R2Dist lShape (quarterCircle y))
0

[-0.752, -0.664]

This application is admittedly more speculative in its practical applications, but it demonstrates
a novel domain in which we can define and compute derivatives. We will now explain how this
library for shapes works.

We represent these generalized parametric surfaces as maximizers, represented in ğœ†ğ‘† as

type Maximizer A = (A â†’ â„œ) â†’ â„œ.
Maximizers are functions ğ¹ : (ğ´ â†’ R) â†’ R that satisfy the algebraic laws ğ¹ (ğœ†ğ‘¥ : ğ´. ğ‘˜) = ğ‘˜
for all ğ‘˜ âˆˆ R and ğ¹ (ğœ†ğ‘¥ : ğ´. max(ğ‘“ (ğ‘¥), ğ‘”(ğ‘¥))) = max(ğ¹ (ğ‘“ ), ğ¹ (ğ‘”)) (analogously to how integrals
are functions that satisfy the algebraic laws of linearity). A generalized parametric surface k :
Maximizer A, when applied to a function f : A â†’ â„œ, returns the maximum value that f attains
on the region represented by k.

Fig. 10 shows an excerpt of the library for generalized parametric surfaces. Note that general-
ized parametric surfaces shapes form a monad (representing nondeterminism), with point and
indexedUnion as return and bind, yielding a programming model for constructing shapes.

Returning to the earlier Hausdorff-distance example, note that the maximal distance on the
â€œLâ€ shape occurs at the corner point, which is represented twice, as the endpoint of each line;
thus, a maximum is taken over two equal distances. In [Abadi and Plotkin 2020], because the
maximum operator is defined with a partial conditional statement, the result â€” not to mention the
derivative â€” would be undefined. Because both the values and derivatives are the same for the two
representations of this corner point, the derivative is a maximal element. Also note that we need
second derivatives to compute the derivative of the Hausdorff distance, due to the use of max01.

8 DISCUSSION
In this section, we discuss the capability of ğœ†ğ‘† to represent control flow as well as the opportunity
to soundly speed up execution of higher-order primitives using derivative information.

3:28

Benjamin Sherman, Jesse Michel, and Michael Carbin

type Maximizer A = (A â†’â„œ) â†’â„œ
let point A (x : A) : Maximizer A = ğœ† f : A â†’â„œâ‡’ f x
let indexedUnion A B (ka : Maximizer A) (kb : A â†’ Maximizer B) : Maximizer B =

ğœ† f : B â†’â„œâ‡’ ka (ğœ† a : A â‡’ kb a f)

let union A (k1 k2 : Maximizer A) : Maximizer A =

ğœ† f : A â†’â„œâ‡’ max (k1 f) (k2 f)

let map A B (g : A â†’ B) (k : Maximizer A) : Maximizer B =

ğœ† f : B â†’â„œâ‡’ k (ğœ† a : â„œâ‡’ f (g a))

let sup A (k : Maximizer A) (f : A â†’â„œ) : â„œ = k f
let inf A (k : Maximizer A) (f : A â†’â„œ) : â„œ = - k (ğœ† x : A â‡’ - (f x))
let hausdorffDist A (d : A â†’ A â†’â„œ) (k1 k2 : Maximizer A) : â„œ =

max (sup k1 (ğœ† x1 : A â‡’ inf k2 (ğœ† x2 : A â‡’ d x1 x2)))
(sup k2 (ğœ† x2 : A â‡’ inf k1 (ğœ† x1 : A â‡’ d x1 x2)))

let unitInterval : Maximizer â„œ = max01
let quarterCircle (y : â„œ) : Maximizer (â„œ2) = map

(ğœ† theta : â„œâ‡’ (cos (pi / 2 * theta), sin (pi / 2 * theta) + y))
unitInterval

let lShape : Maximizer (â„œ2) =

union (map (ğœ† x : â„œâ†’ (x, 1)) unitInterval)
(map (ğœ† y : â„œâ‡’ (1, y)) unitInterval)

let R2Dist (a b : â„œ2) : â„œ = sqrt ((a[0] - b[0])2 + (a[1] - b[1])2)

Fig. 10. Generalized parametric surfaces and ğœ†ğ‘† programs that manipulate them.

8.1 Control Flow: Conditionals and Recursion
ğœ†ğ‘† supports discrete spaces, including in particular the Booleans B and any well-founded set
(such as the natural numbers). The recursion principles for these yield, respectively, if-then-else
expressions and well-founded recursion. These control-flow expressions must be independent of
â€œcontinuous dataâ€: all maps from connected spaces to discrete spaces are constant. This property
defines connected spaces. Connected spaces include all vector spaces, such as Rğ‘›. Di Gianantonio
and Edalat [2013] explain some particular issues that demonstrate why implementing piecewise-
differentiable functions with branching is problematic.

8.2 Optimizing Higher-Order Primitives with Derivative Information
We can also use the fact that functions in ğœ†ğ‘† come equipped with all their derivatives to opportunis-
tically speed up some operations. For instance, consider applying cut_root to some function ğ‘“ .
Its value-level definition naturally maps to a bisection-like algorithm on the values of ğ‘“ . However,
since we have access to ğ‘“ (1) , we can use a variation of Newtonâ€™s method generalized to interval
arithmetic to speed up the convergence drastically, and indeed we do this in our implementation.
Note that we are guaranteed that this optimization is sound, because consistency of differentiation
ensures that ğ‘“ (1) appropriately reflects ğ‘“ (0) . It may be the case that ğ‘“ (1) returns âŠ¥ at some points,
or even everywhere, in which case the algorithm falls back on bisection to ensure progress.

Similarly, the literal interpretation of the value-level definition of Riemannian integration in
Â§5 maps to a quadrature method that uses only the values ğ‘“ (0) of ğ‘“ . However, the availability of
higher derivatives of ğ‘“ makes it possible to use interval-based versions of higher-order integration
methods, which can also drastically speed up the convergence. We do not use these higher-order
methods by default in our actual implementation.

ğœ†ğ‘† : Computable Semantics for Differentiable Programming with Higher-Order Functions and Datatypes

3:29

Table 1. Summary of other approaches to semantics of differentiable programming and their properties.
Higher-order derivatives: The differentiation operator can be iterated arbitrarily many times (when applied
to smooth functions). Higher-order functions: A concrete test: is twice(ğ‘“ : R â†’ R)(ğ‘¥ : R) : R â‰œ ğ‘“ (ğ‘“ (ğ‘¥))
admitted? Non-differentiable functions: Some nondifferentiable functions are admitted. A concrete test: is
max : R2 â†’ R admitted? â€œClarke derivativeâ€ indicates that locally Lipschitz functions support derivatives in
the sense of Clarke derivatives or L-derivatives [Edalat and Lieutier 2004], whereas â€œpartialityâ€ indicates that
nondifferentiable maps are supported by considering them to be partial at their discontinuities.

higher-order
functions
âœ“
VÃ¡kÃ¡r et al. [2018]
Di Gianantonio and Edalat [2013] âœ“
âœ—
Elliott [2018]
âœ—
Abadi and Plotkin [2020]
âœ—
Sigal [2018]
âœ“
Vytiniotis et al. [2019]
âœ“
Huot et al. [2020]
âœ“
Ehrhard and Regnier [2003]
âœ“
ğœ†ğ‘† (this work)

higher-order
derivatives
âœ“
âœ—
âœ—
âœ“
âœ“
âœ—
âœ“
âœ“
âœ“

nondifferentiable func-
tions
âœ—
âœ“ (Clarke derivative)
âœ—
âœ“ (partiality)
âœ“ (partiality)
âœ—
âœ—
âœ—
âœ“ (Clarke derivative)

9 RELATED WORK
Table 1 illustrates the unique set of features that ğœ†ğ‘† provides and their relationship to other
approaches to AD semantics. We note that these other approaches also have features that ğœ†ğ‘† lacks.
Di Gianantonio and Edalat [2013] describe a programming language for nonexpansive (i.e.,
Lipschitz constant 1) functions on the interval [âˆ’1, 1] with a differentiation operator that applies
to functions from [âˆ’1, 1] to [âˆ’1, 1]. The semantics of this differentiation operator are that of the
L-derivative [Edalat 2008; Edalat and Lieutier 2004], which is closely related to the Clarke Jacobian
definition we use. Their domain-theoretic account ensures computability: in theory, results can be
computed to arbitrary precision. Their semantics is fundamentally limited to first-order derivatives:
their interval type denotes [âˆ’1, 1] Ã— [âˆ’1, 1], corresponding to a dual-number representation, baking
in that limited capability. It is unclear how that representation could be generalized directly to
permit higher-order differentiation and appropriately handle nested differentiation (without the
perturbation confusion [Siskind and Pearlmutter 2005] that may arise with nested differentiation).
Elliott [2008] presents a data type for representing smooth maps, where a smooth map ğ‘“ is
represented by the collection of its ğ‘˜th derivatives for all ğ‘˜. Elliott [2008] defines the derivatives
of some arithmetic functions as well as some categorical operations, though the definition of
composition of smooth maps is incorrect. We support higher-order derivatives by adapting this
representation for the Clarke derivative.

VÃ¡kÃ¡r et al. [2018] presents the semantics of a differentiable programming language that supports
higher-order functions and higher-order derivatives using the quasitopos of diffeological spaces. As
a quasitopos, the semantics supports higher-order functions and quotient types. VÃ¡kÃ¡r et al. [2018]
show an internal derivative operator that can be applied to any function of any type, and thus can
be applied repeatedly for higher-order derivatives. We based our internal derivative operator on
theirs. Functions such as max that are not smooth are not admissible. It is not made clear how one
could implement a differentiable programming language supporting the expressive possibilities
suggested by the semantics.

None of the works in Table 1 describe higher-order functions for root-finding, optimization, or
integration, nor do they describe datatypes for implicit surfaces, compact shapes, or probability

3:30

Benjamin Sherman, Jesse Michel, and Michael Carbin

distributions. Edalat and Lieutier [2004] describe an integration operator in a domain-theoretic
framework for differential calculus, but it does not handle higher-order derivatives. Sherman et al.
[2019] describe computable higher-order functions and libraries for root-finding, optimization, and
integration, but does not admit differentiation of any sort.

We follow Sherman et al. [2019] in our approach to computability. We are unaware of any system
that computes arbitrary-precision derivatives (given the definition of the function) in any capacity.

10 CONCLUSION
This paper demonstrates how to compute and make sense of derivatives of higher-order functions,
such as integration, optimization, and root-finding and at higher-order types, such as probability
distributions, implicit surfaces, and generalized parametric surfaces. Our libraries and case stud-
ies model existing differentiable algorithms, for instance, a differentiable ray tracer for implicit
surfaces, without requiring any user-level differentiation code, in addition to demonstrating new
differentiable algorithms, such as computing derivatives of the Hausdorff distance of generalized
parametric surfaces. Ideally, the ideas ğœ†ğ‘† demonstrates may enable differentiable programming
frameworks to support the new abstractions and expressivity suggested by this paper.

ACKNOWLEDGMENTS
We thank Eric Atkinson, Tej Chajed, Alexander Lew, Alex Renda, and David Spivak, as well as the
anonymous reviewers, for their helpful feedback and discussions. This work was supported in part
by the Office of Naval Research (ONR-N00014-17-1-2699). Any opinions, findings, and conclusions
or recommendations expressed in this material are those of the author and do not necessarily reflect
the views of the Office of Naval Research.

REFERENCES
MartÃ­n Abadi and Gordon D. Plotkin. A simple differentiable programming language. In Principles of Programming Languages,

2020.

Matan Atzmon, Niv Haim, Lior Yariv, Ofer Israelov, Haggai Maron, and Yaron Lipman. Controlling neural level sets. In

Advances in Neural Information Processing Systems. 2019.

Shaojie Bai, J Zico Kolter, and Vladlen Koltun. Deep equilibrium models. In Advances in Neural Information Processing

Systems, 2019.

J Daniel Christensen and Enxin Wu. Tangent spaces and tangent bundles for diffeological spaces. American Mathematical

Society, 2017.

Frank H Clarke. Optimization and nonsmooth analysis. 1990.
Pietro Di Gianantonio and Abbas Edalat. A language for differentiable functions. In Foundations of Software Science and

Computational Structures, 2013.

Abbas Edalat. A continuous derivative for real-valued functions. In New Computational Paradigms. 2008.
Abbas Edalat and AndrÃ© Lieutier. Domain theory and differential calculus (functions of one variable). Mathematical

Structures in Computer Science, 14(6), 2004.

Thomas Ehrhard and Laurent Regnier. The differential lambda-calculus. Theoretical Computer Science, 309(1-3), 2003.
Conal Elliott. Higher-dimensional, higher-order derivatives, functionally. 2008. URL http://conal.net/blog/posts/higher-

dimensional-higher-order-derivatives-functionally.

Conal Elliott. The simple essence of automatic differentiation. In International Conference on Functional Programming, 2018.
In Advances in Neural
Mikhail Figurnov, Shakir Mohamed, and Andriy Mnih.

Implicit reparameterization gradients.

Information Processing Systems. 2018.

Michael P Fourman. Continuous truth I: Non-constructive objects. Studies in Logic and the Foundations of Mathematics, 112,

1984.

Laurent Fousse, Guillaume Hanrot, Vincent LefÃ¨vre, Patrick PÃ©lissier, and Paul Zimmermann. MPFR: A multiple-precision

binary floating-point library with correct rounding. ACM Transactions on Mathematical Software, 33(2), 2007.

Mathieu Huot, Sam Staton, and Matthijs VÃ¡kÃ¡r. Correctness of automatic differentiation via diffeologies and categorical

gluing. In Foundations of Software Science and Computation Structures, 2020.

ğœ†ğ‘† : Computable Semantics for Differentiable Programming with Higher-Order Functions and Datatypes

3:31

Eric Jang, Shixiang Gu, and Ben Poole. Categorical reparameterization with gumbel-softmax. International Conference on

Learning Representations, 2017.

Martin Jankowiak and Fritz Obermeyer. Pathwise derivatives beyond the reparameterization trick. In International Conference

on Machine Learning, 2018.

Lutz Kettner, Kurt Mehlhorn, Sylvain Pion, Stefan Schirra, and Chee Yap. Classroom examples of robustness problems in

geometric computations. Computational Geometry, 40(1), 2008.

Tzu-Mao Li, Miika Aittala, FrÃ©do Durand, and Jaakko Lehtinen. Differentiable Monte Carlo ray tracing through edge

sampling. In Special Interest Group on Computer Graphics and Interactive Techniques, 2018.

Jonathan Lorraine, Paul Vicol, and David Duvenaud. Optimizing millions of hyperparameters by implicit differentiation.

arXiv preprint arXiv:1911.02590, 2019.

Christian A. Naesseth, Francisco J. R. Ruiz, Scott W. Linderman, and David M. Blei. Reparameterization gradients through

acceptance-rejection sampling algorithms. In Artificial Intelligence and Statistics, 2017.

Michael Niemeyer, Lars Mescheder, Michael Oechsle, and Andreas Geiger. Differentiable volumetric rendering: Learning

implicit 3D representations without 3D supervision. In Computer Vision and Pattern Recognition, 2020.

Benjamin Sherman, Jesse Michel, and Michael Carbin. Sound and robust solid modeling via exact real arithmetic and

continuity. In International Conference on Functional Programming, 2019.

Jesse Sigal. Denotational semantics for differentiable programming with manifolds. In Student Research Competition at the

Internation Conference on Functional Programming, 2018.

Jeffrey Mark Siskind and Barak A Pearlmutter. Perturbation confusion and referential transparency: Correct functional
implementation of forward-mode ad. Workshop on Implementation and Application of Functional Languages, 2005.
Dimitrios Vytiniotis, Dan Belov, Richard Wei, Gordon Plotkin, and Martin Abadi. The differentiable curry. In Neural

Information Processing Systems Workshop Program Transformations, 2019.

Matthijs VÃ¡kÃ¡r, Ohad Kammar, and Sam Staton. Diffeological spaces and semantics for differential programming. In Domains,

2018. URL https://andrejbauer.github.io/domains-floc-2018/slides/Matthijs-Kammar-Staton.pdf.

Yuanhao Wang, Guodong Zhang, and Jimmy Ba. On solving minimax optimization locally: A follow-the-ridge approach. In

International Conference on Learning Representations, 2020.

