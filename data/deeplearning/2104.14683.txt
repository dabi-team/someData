2
2
0
2

n
a
J

8
1

]

M
P
.
n
i
f
-
q
[

2
v
3
8
6
4
1
.
4
0
1
2
:
v
i
X
r
a

Springer Nature 2021 LATEX template

Deep Reinforcement Trading with
Predictable Returns

Alessio Brini1* and Daniele Tantari2

1*Scuola Normale Superiore, Pisa, Italy.
2University of Bologna, Bologna, Italy.

*Corresponding author(s). E-mail(s): alessio.brini@sns.it;
Contributing authors: daniele.tantari@unibo.it;

Abstract

Classical portfolio optimization often requires forecasting asset returns
and their corresponding variances in spite of the low signal-to-noise
ratio provided in the ﬁnancial markets. Modern deep reinforcement
learning (DRL) oﬀers a framework for optimizing sequential trader deci-
sions but lacks theoretical guarantees of convergence. On the other
hand the performances on real ﬁnancial trading problems are strongly
aﬀected by the goodness of the signal used to predict returns. To dis-
entangle the eﬀects coming from return unpredictability from those
coming from algorithm un-trainability, we investigate the performance
of model-free DRL traders in a market environment with diﬀerent
known mean-reverting factors driving the dynamics. When the frame-
work admits an exact dynamic programming solution, we can assess
limits and capabilities of diﬀerent value-based algorithms to retrieve
meaningful trading signals in a data-driven manner. We consider DRL
agents that leverage on classical strategies to increase their performances
and we show that this approach guarantees ﬂexibility, outperform-
ing the benchmark strategy when the price dynamics is misspeciﬁed
and some original assumptions on the market environment are vio-
lated with the presence of extreme events and volatility clustering.

Keywords: machine learning, reinforcement learning, ﬁnancial trading,
portfolio optimization

1

 
 
 
 
 
 
Springer Nature 2021 LATEX template

2

Deep Reinforcement Trading with Predictable Returns

1 Introduction

The important milestone represented by modern portfolio theory of Markowitz
(1952) has set the basis for the beginning of ﬁnancial portfolio optimization
as an active ﬁeld of research. Its original formulation suﬀers several drawbacks
(Kolm et al., 2014) and has been extended from a single-period to a multi-
period framework to capture intertemporal eﬀects and to allow dynamical
portfolio rebalancing (Grinold, 2006; Engle and Ferstenberg, 2007; Tutuncu,
2011; Kolm and Maclin, 2012; Kolm and Ritter, 2014). However, the addi-
tion of the time dimension makes even more complicated the estimation of an
optimal strategy, which requires to forecast ﬁnancial quantities such as risks
and returns for several periods in the future. Single-period models are often
still adopted because their dynamic counterpart is not practical and the fore-
casting step may lead to systematic errors due to the uncertainty about the
chosen model or the inherent presence of a low signal-to-noise ratio in the
ﬁnancial data. Even when a multi-period model is eﬀective in capturing the
market impact or alpha decay, classical optimal control techniques lay over a
set of restricting assumptions which cannot properly represent the real ﬁnancial
world.

In this work we use reinforcement learning (RL) (Sutton and Barto, 2018;
Szepesv´ari, 2010) as a convenient framework to model sequential decision prob-
lems of a ﬁnancial nature without the need of directly modeling the underlying
asset dynamics. RL ﬁnds its roots in the optimal control theory along with
the dynamic programming literature (Bertsekas, 2005) and has gained a huge
revival after the last decade improvement of deep learning (DL) as a ﬁeld of
research. This gave rise to the so-called deep reinforcement learning (DRL)
that has already obtained relevant results in application domains such as gam-
ing (Silver et al., 2016; Mnih et al., 2015) and robotics (Levine et al., 2016).
For a comprehensive overview of DRL methods and its ﬁelds of application,
see Arulkumaran et al. (2017).

The RL approach is not new to the ﬁnancial domain and there are exam-
ples of practical applications for trading and portfolio management (Zhang et
al., 2020; Jiang et al., 2017). However, recent DRL algorithms are very often
home-made recipes without theoretical control. For this reason the study of
their performances in real ﬁnancial trading problems is always an intricate
combination of diﬀerent eﬀects, some of them related to the goodness of the
dataset and the signals used to predict returns, some others related to the
speciﬁc algorithm and its trainability issues.

To the best of our knowledge, there is a lack of research works that investi-
gate DRL performances in ﬁnancial trading problems besides the issues coming
from market eﬃciency: the search for a good signal to predict returns or the
possible lack of any signals in the dataset. For this reason we consider a con-
trolled environment in which a signal is known to exist and study the capability
of DRL agents to discover proﬁtable opportunities in the market.

Similarly to Kolm and Ritter (2019); Chaouki et al. (2020), we simulate
ﬁnancial asset returns which contain predictable factors and we let the agent

Springer Nature 2021 LATEX template

Deep Reinforcement Trading with Predictable Returns

3

trade in an environment whose associated optimization problem admits an
exact solution (Gˆarleanu and Pedersen, 2013). The optimal benchmark strat-
egy allows us to evaluate strengths and ﬂaws of a DRL approach, both in terms
of accuracy and eﬃciency.

As a main novelty of our work, we exploit a data-driven setting of DRL
in which the agents not only compete against classical strategies but can also
leverage on their experience to optimize the state-action space and increase
the learning speed. We test diﬀerent DRL approaches on a variety of ﬁnancial
data with diﬀerent properties to investigate their ﬂexibility when the simulated
dynamics is misspeciﬁed with respect to the assumptions of the benchmark
model. We show that DRL algorithms can reach the performance of the bench-
mark strategy, when it is optimal, and also outperform it in the case of model
misspeciﬁcations like the presence of extreme events and volatility clustering.
We also show that classical strategies can help DRL agents by giving them
information about the typical scale of a good strategy to start and adjust.

2 Financial Market Environment

The agent operates in a ﬁnancial market where at each time t ∈ Z it can trade
N securities whose excess returns yt+1 = pt+1 − (1 + rf )pt are given by

yt+1 = Bft + ut+1,

(1)

where ft is a K × 1 vector of return-predicting factors, B is a matrix of factor
loadings and ut+1 is a noise term with E[ut+1] = 0 and Var[ut+1] = Σ.

The factors can be either value factors, which describes the proﬁtability
of the asset relatively to some fundamental measure, or momentum factors,
which rely on past price movements to predict the future (Moskowitz et al.,
2012). We assume they evolve according to a discretization of a mean-reverting
process (Uhlenbeck and Ornstein, 1930)

∆ft+1 = −φft + (cid:15)t+1,

(2)

where φ is a K × K matrix of mean-reversion coeﬃcients and (cid:15)t+1 represents
a stochastic shock component with E[(cid:15)t+1] = 0 and V ar[(cid:15)t+1] = Ω.

Trading in this environment produces transaction costs which we assume

to be a quadratic function of the traded amount ∆ht = ht − ht−1, i.e.

C(∆ht) =

1
2

∆hT

t Λ∆ht,

(3)

where Λ is a symmetric positive deﬁnite matrix ensuring transaction costs con-
vexity as generally required by empirical literature (Lillo et al., 2003; Garleanu
et al., 2008) and is consistent with the assumption of a linear price impact. In
the following we also assume that Λ = λΣ, i.e. that trading costs are actually

Springer Nature 2021 LATEX template

4

Deep Reinforcement Trading with Predictable Returns

the compensation for the dealer’s risk that takes the other part of the trans-
action. In this context, λ can be interpreted as the dealer’s risk aversion and
controls the degree of liquidity of the asset.

The agent’s goal is to ﬁnd a dynamic portfolio strategy (h0, h1, . . .) by
maximizing the present value of all future returns, penalized for risk and net
of transaction costs, i.e.

max
(h0,h1,...)

E0

(cid:104) (cid:88)

t

ρt+1(hT

t yt+1 −

γ
2

hT
t Σht) − +

∆hT

t Λ∆ht

(cid:105)

,

ρt
2

(4)

where ρ ∈ (0, 1) is a discount rate and γ is the risk aversion coeﬃcient.

When the noise terms ut and (cid:15)t are assumed to be distributed as a Gaussian,
the model coincides with Gˆarleanu and Pedersen (2013) that has a closed-form
solution as

ht =

1 −

ht−1 +

haim
t

.

(5)

(cid:16)

(cid:17)

a
λ

a
λ

The optimal strategy is then a convex combination of holding the previous
portfolio and trading towards the objective portfolio haim
with trading rate
a/λ. Such trading rate a
λ < 1, where

t

a =

−(γ(1 − rf ) + λrf ) + (cid:112)(γ(1 − rf ) + λrf )2 + 4γλ(1 − rf )2
2(1 − rf )

(6)

is a decreasing function of the transaction costs by the eﬀect of λ and increasing
in the risk aversion γ. The objective portfolio haim

is deﬁned by

t

haim
t = (γΣ)−1B(I +

a
γ

Φ)−1ft,

(7)

being a generalization of the well-known Markovitz portfolio (Markowitz, 1952)

t = (γΣ)−1 Bft,
hM

(8)

which is optimal only in the static case and in absence of transaction costs.
Instead, the aim portfolio in eq. (5) represents a dynamic strategy and can be
shown to be a weighted average of all future Markovitz portfolios.

If the matrix Φ is diagonal, the aim portfolio become

t = (γΣ)−1B
haim

(cid:32)

f 1
t
1 + Φ1 a
γ

, . . . ,

f K
t
1 + ΦK a
γ

(cid:33)T

,

(9)

where the K factors are scaled down by their speed mean-reversion Φ. A factor
i with slower speed of mean-reversion is scaled less than a faster factor j and
the relative weight of f i with respect to the weight of f j,
increases
with the transaction cost λ. In fact the cost friction leads the investor to slow

1+Φj a
γ
1+Φi a
γ

Springer Nature 2021 LATEX template

Deep Reinforcement Trading with Predictable Returns

5

down the rate of portfolio rebalancing and faster factors require to close out
the position in a shorter time frame.

In the following we will use the optimal strategy (5) of the Gaussian model
as a benchmark for the DRL performance in solving the problem (4) but we
also consider other possible model speciﬁcations for which an explicit opti-
mal solution is not available. In particular we introduce fat-tailed distributed
shocks and heteroskedastic volatility as interesting model misspeciﬁcations
that reﬂect general properties of empirical asset returns. (Cont, 2001).

A riskier environment with many extreme events is constructed by assum-
ing the asset noise to depart from a Gaussian distribution. In particular we
consider ut and (cid:15)t distributed as a Student’s T distribution with ν degrees
of freedom. On the other hand, heteroskedasticity is introduced according
to a generalized autoregressive conditional heteroskedastic (GARCH) pro-
cess (Bollerslev, 1987) for the variance of asset returns to model volatility
clustering. In the case of a single asset it means that ut = σtzt where

σ2
t = ω +

p
(cid:88)

j=1

αj | uj−i |2 +

q
(cid:88)

k=1

βkσ2

t−k

(10)

and zt is a noise term that can be either a standard Gaussian or a Student’s
T with ν degrees of freedom.

3 Deep Reinforcement Learning Methods

The aim of RL is to solve a decision making problem in which the timing
of costs and beneﬁts is relevant. Financial portfolio optimization comprises a
set of problems where current actions can inﬂuence the future, even at a very
distant point in time. RL approaches the resolution of this problem by trial
and error, learning by obtaining a feedback after each sequential decision.

A RL problem can be formulated in the context of a Markov Decision
Process (MDP), which is deﬁned by a set of possible states St ∈ S, a set of
possible actions At ∈ A and a transition probability P a
ss(cid:48) = P [St+1 = s(cid:48) | St =
s, At = a]. Therefore, it is the (stochastic) control problem of ﬁnding

E

max
{π}

(cid:34) ∞
(cid:88)

t=0

ρtRt+1(St, At, St+1)

(cid:35)

(11)

where π deﬁnes the agent’s strategy that associates a probability π(a | s) to
the action At = a given the state St = s. A RL agent aims at maximizing
the expected sum of (discounted) rewards by ﬁnding the best action given
the current state. We consider the model-free context in which the agent has
no knowledge of the internal dynamics of the environment, i.e. the transition
probability is not known, and the only source of information is the sequence
of states, actions and rewards.

Springer Nature 2021 LATEX template

6

Deep Reinforcement Trading with Predictable Returns

Value based methods are deﬁned by introducing the action-value function

Qπ(s, a) ≡ E

(cid:34) ∞
(cid:88)

k=0

ρkRt+1+k | St = s, At = a, π

,

(12)

(cid:35)

which reﬂects the long-term reward associated with the action a taken in the
state s if the strategy π is followed hereafter. The estimation of (12) allows
to derive a deterministic optimal policy as the highest valued action in each
state. Depending on how the agent estimates the action-value function (12),
diﬀerent classes of value-based algorithms can be introduced. Conversely, direct
policy search approaches are alternative methods that try to explore directly
the policy space (or some subset of it), being the problem a particular case of
stochastic optimization.

3.1 Tabular Reinforcement Learning

Tabular RL methods are practical when the possible states and actions are
few enough to be represented in a table, which has an entry for every (s, a)
pair. In this case the agent can explore many possible state-action pairs within
a reasonable amount of computational time and obtain a good approximation
of the value function.

Q-learning (Watkins and Dayan, 1992) is a tabular method in which at
each timestep the agent tries an action At, receives a reward Rt+1 and updates
the current estimate of the action-value function Q(St, At) as

Q(St, At) ← Q(St, At) + α(T Q

t − Q(St, At)),

where α is a learning rate and the target

T Q
t = Rt+1 + ρ max

a

Q(St+1, a)

(13)

(14)

is a decomposition of the value function in terms of the current reward and the
current estimate of the future value discounted by ρ. At the end of the learning
process, the optimal policy is the greedy strategy At = argmaxa Q(St, a) but
Q-learning is trained oﬀ-policy because the agent chooses the action At follow-
ing an (cid:15)-greedy policy that ensures adequate exploration of the state-action
space.

When states and actions are continuous, as it is for a realistic ﬁnancial
environment, Q-learning barely obtains good estimates of the value function in
a feasible computational time. Moreover, the discretization of the state space
itself may cause loss of relevant information depending on its granularity. In
this context the DRL framework becomes particularly necessary.

Springer Nature 2021 LATEX template

Deep Reinforcement Trading with Predictable Returns

7

3.2 Approximate Reinforcement Learning

DRL algorithms tackle previously intractable problems by approximating
eq. (12) through a neural network that allows a continuous state space
representation.

Deep Q-Network (DQN) (Mnih et al., 2015) is an extension of Q-learning
and allows to learn a parametrized value function Q∗(s, a) ≈ Q(s, a; θ).
Q(s, a; θ) is a multi-layer neural network that for a given input state s returns
a vector of action values. The standard update of eq. (13) therefore becomes

θt+1 = θt + α(T DQN

t

− Q(St, At; θt))∇θtQ(St, At; θt)

(15)

which resembles a standard gradient descent toward the target

T DQN
t

= Rt+1 + ρ max

a

Q(St+1, a; θt).

(16)

Even if tabular methods converge to the optimal function (Watkins and Dayan,
1992), they fail to generalize over previously unseen states. Instead, DRL has
good generalization capabilities, but produces unstable behaviors during the
training whenever function approximation is combined with an oﬀ-policy algo-
rithm and learning by estimates (Sutton and Barto, 2018). The issue of training
instability is partially solved by adding two ingredients: an experience replay
buﬀer and a ﬁxed target. An experience buﬀer is a ﬁnite set D = {e1, . . . , eN }
of ﬁxed cardinality N , where at each time t the agent’s stream of experience
et = (St, At, Rt+1, St+1) is stored replacing one of the old ones. The replay
buﬀer is then used to perform a batch update of the network parameters. A
ﬁxed target is exactly as the online target except that its parameters θ− are
updated (θ−
t = θt) and then kept ﬁxed for τ iterations. Combining the two
ingredients the gradient step of eq. (15) becomes

Ee[(r + ρ max
a(cid:48)

Q (cid:0)s(cid:48), a(cid:48); θ−

t

(cid:1) − Q (s, a; θt))∇θtQ(s, a; θt)]

(17)

where e = (s, a, r, s(cid:48)) is uniformly sampled from D.

In what follows we adopt a variant of the algorithm called double DQN
(DDQN) (Van Hasselt et al., 2016) which prevents some overestimation issues
of the value function. For convenience, we still refer to the chosen value based
algorithm as DQN, even if the implementation follows the variant of DDQN.
In the appendix we recap the technical details of the value based algorithms
used in the numerical experiments.

The optimization problem in eq. (11) can be equivalently solved using
a policy gradient algorithm like the Proximal Policy Optimization (PPO)
(Schulman et al., 2017). A policy gradient algorithm directly parametrizes the
optimal strategy within a given policy class πθ = π(At | St; θ), for example
a multilayer neural network with the set of parameters θ. The optimization
problem is approximately solved by computing the gradient of the performance

Springer Nature 2021 LATEX template

8

Deep Reinforcement Trading with Predictable Returns

measure J(θ) = (cid:80)∞
ascent updates according to

t=0 ρtRt+1(St, At, St+1; πθ) and then carrying out gradient

θt+1 = θt + α∇θJ(θt),

(18)

where α is still a scalar learning rate. The policy gradient theorem (Sutton et
al., 2000; Marbach and Tsitsiklis, 2001) provides an analytical expression for
the gradient of J(θ) as

∇θJ(θ) = Eπθ

(cid:20) ∇θπ (At | St; θ)
π (At | St; θ)

(cid:21)
Qπθ (St, At)

= Eπθ [∇θ log π (At | St; θ) Qπθ (St, At)] ,

(19)

where the expectation, w.r.t. (St, At), is taken along a trajectory (episode) that
occurs adopting the policy πθ. It can be proven that it is possible to modify
the action value function Qπ(s, a) in (19) by subtracting a baseline Vπ(s) that
reduces the variance of the empirical average along the episode, while keeping
the mean unchanged. A popular baseline choice is the state-value function

Vπ(s) ≡ E

(cid:34) ∞
(cid:88)

k=0

ρkRt+1+k | St = s, π

,

(20)

(cid:35)

which reﬂects the long-term reward starting from the state s if the strategy π
is adopted onwards. The gradient thus can be rewritten as

∇θJ(θ) = Eπθ [∇θ log π (At | St; θt) Aπθ(St, At)]

(21)

where

Aπ(s, a) ≡ Qπ(s, a) − Vπ(s),
(22)
is called advantage function and quantiﬁes the gain obtained by choosing a
speciﬁc action in a given state with respect to its average value for the policy
π.

Diﬀerent policy gradient algorithms depend on how the advantage function
is estimated. In PPO the advantage estimator A (s, a; ψ) is parametrized by
another neural network with parameters ψ. This approach is known as actor-
critic: the actor is represented by the policy estimator π(a | s; θ) that outputs
the mean and the standard deviation of a Gaussian distribution which the
agent uses to sample actions, the critic is the advantage function estimator
A (s, a; ψ) whose output is a single scalar value. The two neural networks
interact during the learning process: the critic drives the updates of the actor
which successively collects new sample sequences that will be used to update
the critic and again evaluated by it for new updates. The PPO algorithm can
therefore be described by the extended objective function

J PPO(θ, ψ) = J(θ) − c1LAF(ψ) + c2H (π (a | s; θ)) .

(23)

Springer Nature 2021 LATEX template

Deep Reinforcement Trading with Predictable Returns

9

The second term is a loss between the advantage function estimator A (s, a; ψ)
and a target Atarg, represented by the cumulative sum of discounted reward,
needed to train the critic neural network. The last term represents an entropy
bonus to guarantee an adequate level of exploration. Details about the spe-
ciﬁc choice of the losses, the target and the neural network parametrization,
together with additional information about the general algorithm implemen-
tation are given in the appendix.

4 Numerical Experiments

In this section we conduct synthetic experiments in the controlled ﬁnancial
environment outlined in Section 2. We present two diﬀerent groups of experi-
ments where the agents observe ﬁnancial time series that come from diﬀerent
data generating processes. The ﬁrst group is related to the case where the
return dynamics is driven by Gaussian mean reverting factors as in eq. (2) and
the optimal strategy is known to be eq. (5). The second group includes a set of
cases where the model that generates the dynamics allows for a bigger amount
of extreme events and heteroskedastic volatility. In this case eq. (5) is still used
as a representative classical strategy of dynamic portfolio optimization.

In all experiments the agents trade a single asset, but the framework is gen-
eral enough to allow for multi asset trading. We test Q-learning and DQN in
parallel in the same environment, while PPO training is slightly diﬀerent. The
ﬁrst two algorithms are trained in-sample for a number of updates equal to the
length Tin of the simulated series, while the learned policy is evaluated out-of-
sample at several intermediate training moments on diﬀerent series of length
Tout. The same logic operates for PPO, which however works in an episodic
way: the algorithm is trained in-sample and evaluated out-of-sample respec-
tively for Ein and Eout number of episodes of length equal to 2000 timesteps.
Each agent operates in a model-free context so that no prior information about
the data generating process is provided.

In order to bring the RL formalism to the portfolio optimization problem
of eq. (4) we choose the actions as the amount of shares traded At = ∆ht,
while the state is deﬁned as the pair return-holding St = (yt, ht−1). We include
the asset return in the state representation instead of the predicting factors
because it is our interest to assess DRL as a pure data-driven approach. The
choice of ﬁnancial factors is known to be a non-trivial task and it can be highly
discretionary. For every experiment we also adapt the boundaries of the action
space A according to the magnitude of the action performed by the benchmark.
More speciﬁc details about this heuristic are provided in the appendix.

After taking an action and causing a change in portfolio position, the agent

observes the next price movement and the reward signal that is

Rt+1(yt+1, ht−1, ∆ht) = hT

t yt+1 −

γ
2

hT
t Σht −

1
2

∆hT

t Λ∆ht.

(24)

Springer Nature 2021 LATEX template

10

Deep Reinforcement Trading with Predictable Returns

Fig. 1 Results for DQN and Q-learning in the case of Gaussian dynamics driven by one
(ﬁrst row) and two (second row) mean-reverting factors. Cumulative net P nL (ﬁrst column)
and SR (second column) are displayed as the size of the training time series increases up
to Tin = 300000 on the x-axis. Every dot represents the average over 10 out-of-sample tests
of length Tout = 5000 for a speciﬁc agent out of the 20 tested in total. The horizontal
dashed line represents the optimal benchmark, while the solid lines represent the average
performance of all the agents in relative percentage to the benchmark.

Note that we decided to allow the benchmark agent to be perfectly informed,
so that it knows exactly the predicting factors of the price dynamics. On the
contrary RL agents can just gather information from the observed return which
is aﬀected by an additional source of noise. This choice allows the RL agent
to be completely agnostic with respect to the price dynamics. This represents
a clear disadvantage for the RL agent, but it is a step towards a more ﬂexible
approach when the dynamics is not known and the performance is strongly
dependent on the factors selection. Alternatively one can assume that the RL
agents are completely informed by replacing yt with ft in the deﬁnition of the
state variables. For the purpose of comparison, we highlight that the value-
based algorithms in this study perform discrete control, while the benchmark
solution can adopt a continuous strategy according to eq. (5). Although PPO
can express both discrete and continuous policies, we test the continuous ver-
sion to allow for a more expressive policy and compare the diﬀerences of the
two settings.

The details about the parameters used to simulate the ﬁnancial data and
the hyperparameters setting for training the neural networks are provided in
the appendix. The experiments are run in parallel on a 64-cores Linux server
which has an Intel Xeon CPU E5-2683 v4 @ 2.10GHz. The training runtime for
a single value-based method experiment of length Tin = 300000 ranges from
two to four hours when the neural network architecture is not deeper than
two hidden layers. Approximately the same runtime is needed for the PPO
algorithm when Ein = 300. The source code written in Python is available on
GitHub1. The following subsections discuss the results of the two groups of
experiments.

1https://github.com/Alessiobrini/Deep-Reinforcement-Trading-with-Predictable-

Returns

         1 H W  3 Q / 6 K D U S H  5 D W L R ' 4 1 4 E H Q F K P D U N                                               7 L Q   E H Q F K P D U NSpringer Nature 2021 LATEX template

Deep Reinforcement Trading with Predictable Returns

11

Fig. 2 Learned action-value function for a DQN agent when the asset return varies and
the holding is ﬁxed at 0. Diﬀerent colors represent diﬀerent actions.
4.1 Tracking the Benchmark

From Figure 1 we observe the evolution of the out-of-sample performance of
several Q and DQN agents in the case of a return dynamics driven by one or
two Gaussian factors. After about half of the training runtime, DQN reaches
on average a close-to-optimal cumulative net P nL, which is expressed as the
gross return of the portfolio deducted from the transaction costs, i.e.

P nLnet

t+1(yt+1, ht, ∆ht) = htyt+1 −

1
2

∆htΛ∆ht.

(25)

The trained DQN agents are then able to retrieve the mean reverting signals
in the data and to control the amount of transaction costs without knowing
the data generating process of the underlying dynamics. On the other hand,
Q-learning agents hardly reach half of the cumulative net P nL of the optimal
benchmark in the same training time.

The performances of the tabular algorithm are strictly dependent on the
granularity of the state discretization. Q-learning can reach the benchmark
performance only when Tin → ∞ and S is dense enough to closely represent
the continuous trading environment. However, even if we keep the Q-table
relatively small in size, usually below 100000, it can still be very sparse for this
range of Tin. This is particularly evident in the case of two Gaussian factors,
where many agents have a negligible cumulative net P nL simply because they
do not perform any buy or sell actions. Increasing the size of the Q-table for
experiments of ﬁxed length Tin leads to even worse results.

DQN avoids the ineﬃcient tabular parametrization of the action-value func-
tion by using less parameters with respect to the amount of entries in the
Q-table. The use of a neural network as an action-value function approximator
is crucial in this ﬁnancial environment because the agent learns faster when
the state space is entirely observable and the parameters can be updated by
batches of experience.

                     \                     D F W L R Q  Y D O X H  I X Q F W L R Q  H                        Springer Nature 2021 LATEX template

12

Deep Reinforcement Trading with Predictable Returns

Fig. 3 Results for PPO in the case of Gaussian dynamics driven by one (ﬁrst row) and two
(second row) mean-reverting factors. Cumulative net P nL (ﬁrst column) and SR (second
column) are displayed as the training episodes increases up to Ein = 300 on the x-axis.
Every dot represents the average over 10 out-of-sample tests of length Tout = 2000 for a
speciﬁc agent out of the 20 tested in total. The horizontal dashed line represents the optimal
benchmark, while the solid line represents the average performance of all the agents in
relative percentage to the benchmark.

In order to compare the risk of diﬀerent strategies we use the annualized

Sharpe ratio (SR) (Sharpe, 1994) which is computed as

SR =

E[P nLnet]
(cid:112)Var[P nLnet]

√

∗

252,

(26)

deﬁning the expected return of the portfolio per unit of risk on a yearly basis.
It is a common metric to evaluate the trade-oﬀ between risk and return of
ﬁnancial strategies especially in a mean-variance optimization framework.

The second column of Figure 1 showcases the evolution of the SR of
the agents and highlights that DQN obtains on average the same level of
benchmark proﬁt adjusted for risk since the beginning of the training. The
performances of Q-learning in this case are strongly biased, since often the
tabular agents choose not to trade and avoid increasing the risk of its portfolio
position. The DQN agents ﬁrst learn how to obtain low-risk portfolios, then
they start making higher proﬁts, as it is shown from the faster convergence of
the SR with respect to the cumulative net P nL.

Figure 2 provides insights about the learned behavior of the DRL agents
showing the learned action-value function of the best performing DQN agent at
the end of the period of training represented in Figure 1. The agent is trained
over a return dynamics driven by one predicting factor, but the ﬁndings are
valid also in the case of multiple factors. The estimated action-value function
Q((y, h), a; θ) is displayed for all the actions in the discrete space A and
implicitly represents the behavior of the agent when diﬀerent levels of returns
are experienced. If the agent acts greedily and chooses the highest Q-value for
every level of y, positive actions appear prevalent when returns are positive,
while the opposite holds for negative actions.

            1 H W  3 Q / 6 K D U S H  5 D W L R 3 3 2                                ( L Q   E H Q F K P D U NSpringer Nature 2021 LATEX template

Deep Reinforcement Trading with Predictable Returns

13

Fig. 4 Greedy policy function for a DQN and a PPO agent together with the policy of
the benchmark when the asset return varies and the holding is ﬁxed at 0. Diﬀerent colors
represent diﬀerent algorithms. The curves present a conﬁdence interval because the average
maximum action is obtained from the results of 20 trained agents for each algorithm.

Figure 3 shows analogous experiment results for PPO trained with ﬁnancial
returns driven by mean-reverting Gaussian dynamics. PPO retrieves the signal
in the data and converges to the benchmark, but exhibits higher variance in
the Net PnL measure compared to DQN. This is motivated by the diﬀerent
type of policy that the algorithm is describing. Working in a continuous action
space allows the possibility to trade any fraction of the synthetic asset, but
also complicates the exact convergence to the benchmark because the sampling
space is large. In practice there is no theoretical guarantee to ﬁnd the proper
way to sample actions from A in a ﬁnite time.

Figure 4 represents the average greedy policy learned by the agents for
DQN and PPO presented in Figures 1 and 3. Both algorithms can discover the
inherent arbitrage introduced in the market, since the average policy follows
the sign of the returns by buying low and selling high. The learned policies
appear to be monotonic as the one of the benchmark.

4.2 Outperforming the Benchmark

In order to show the ﬂexibility of the DRL approach, we study its performances
with respect to the benchmark when the return dynamics depart from the
original model speciﬁcation. In particular we introduce two types of model
misspeciﬁcations: the presence of extreme events and noise heteroskedasticity.
In both cases the strategy in eq. (5) is no more optimal, but since it performs
well and it is often used in practice, it can be considered as a benchmark
representing a broader class of factor trading strategies. It is therefore natural
to investigate whether DQN and PPO are able to outperform the benchmark
other than just reaching it.

We consider two diﬀerent reference strategies that are respectively referred
to as fully informed when the benchmark is provided with the simulated factors
and partially informed when instead it needs to extract them from the observed
returns. These diﬀerent settings should not aﬀect the DRL performance, except

                     \      E H V W  $ W  H  ' 4 1 3 3 2 E H Q F K P D U NSpringer Nature 2021 LATEX template

14

Deep Reinforcement Trading with Predictable Returns

Fig. 5 Results for Student’s T dynamics in the case of one mean-reverting factor. DQN is
tested over Student’s T distributed returns with ν = 6 (ﬁrst row) and ν = 8 (second row)
respectively. The ﬁgure should be read with the same logic of Figure 1 since the number of
agents and the length of in-sample and out-of-sample experiments are equal. The solid lines
represent diﬀerent PPO performances with respect to diﬀerent benchmark strategies, which
in this context are not optimal anymore.

Fig. 6 Results for Student’s T dynamics in the case of one mean-reverting factor. PPO is
tested over Student’s T distributed returns with ν = 6 (ﬁrst row) and ν = 8 (second row)
respectively. The ﬁgure should be read with the same logic of Figure 3 since the number of
agents and the length of in-sample and out-of-sample experiments are equal. The solid lines
represent diﬀerent PPO performances with respect to diﬀerent benchmark strategies, which
in this context are not optimal anymore.

for the boundaries of the action space A that we decided to adapt to the
benchmark for a better comparison (see the appendix).

The fully informed benchmark agent can directly use eq. (5) to trade just by
estimating the speeds of mean reversion and factor loadings from the observed
return predicting factors. Instead, in the partially informed case the benchmark
agent does not know exactly which are the best predicting factors and needs
to guess or extract them from what is observed in the state space. One of the
typical choices in ﬁnancial literature is to use lagged past returns (Asness et

       1 H W  3 Q / 6 K D U S H  5 D W L R I X O O \  L Q I R U P H G S D U W L D O O \  L Q I R U P H G                                             7 L Q   E H Q F K P D U N            1 H W  3 Q /           6 K D U S H  5 D W L R I X O O \  L Q I R U P H G S D U W L D O O \  L Q I R U P H G                                          ( L Q   E H Q F K P D U NSpringer Nature 2021 LATEX template

Deep Reinforcement Trading with Predictable Returns

15

Fig. 7 Results for AR-GARCH return dynamics when the noise is distributed as a standard
Normal or a Student’s T distribution (ν = 8). The performance of DQN is compared with
the dashed line benchmark. On the y-axis of the left plot there is the cumulative net Pnl
diﬀerence between DQN and the benchmark, so that the dashed line is set at 0. Then, the
ﬁgure should be read with the same logic of Figure 1 since the number of agents and the
length of in-sample and out-of-sample tests are equal.

al., 2013) as factors to predict future returns. We resort to a simple heuristic
to select the best possible lagged variables by ﬁtting the eq. (1) for a set of
candidate lags. Then, we select the best one by minimizing the average squared
residuals.

From Figure 5 we note that in presence of extreme events (T-student noise)
the DQN agents are able to control the trading costs and obtain equal or supe-
rior cumulative net P nL with respect to the two benchmark agents, especially
for lower degrees of freedom where the misspeciﬁcation has a greater impact
and extreme events are more frequent. The SR fairly outperforms the bench-
mark towards the end of the training process in both the cases. The RL agents
learn how to control the higher amount of risk introduced in the environment,
while model based strategies like the benchmark should have considered this
in advance. Figure 6 shows the same misspeciﬁed case for PPO agent which is
able to consistently manage the transaction costs and obtain higher net PnL
than the benchmark, still exhibiting greater variance than DQN. We note that
PPO performs better with respect to the benchmark when the latter is pro-
vided with partial information and needs to discover the persistence of the
signal on its own.

The second proposed misspeciﬁcation introduces heteroskedasticity in the
asset returns by considering a GARCH process with p = 1 and q = 1 for the
asset variance. For simplicity, we assume the predictable component of the
returns to be an autoregressive model with lag of order 1.

Figure 7 showcases that DQN obtains on average a higher cumulative net
P nL with respect to the benchmark. We compare the diﬀerence, instead of
the ratio, between the two cumulative net P nLs because in some cases the
net P nL obtained by the benchmark agent is negative. Diﬀerently from the
previous set of experiments, the increment of performance in the presence
of heteroskedasticity regards the control of the amount of transaction cost.

                                                         H  1 H W  3 Q /                                   6 K D U S H  5 D W L R Q R U P D O V W X G H Q W 
 V  W 7 L Q   E H Q F K P D U N  E H Q F K P D U NSpringer Nature 2021 LATEX template

16

Deep Reinforcement Trading with Predictable Returns

Fig. 8 Results for AR-GARCH return dynamics when the noise is distributed as a standard
Normal or a Student’s T distribution (ν = 8). The performance of PPO is compared with
the dashed line benchmark. On the y-axis of both subﬁgures there is the diﬀerence between
PPO and the benchmark respectively for cumulative net PnL and the Sharpe ratio. Then,
the ﬁgure should be read with the same logic of Figure 3 since the number of agents and
the length of in-sample and out-of-sample tests are equal.

Looking at the SR, DQN mostly tracks the benchmark and outperforms it only
in the case of Gaussian noises. The increased amount of extreme events in the
Student’s T case causes a worsening in the DQN performance relative to the
benchmark. It has to be noted that we use the same set of hyperparameters for
all these experiments. This is a signal that the performance in the fat-tailed
case could be improved by tuning a more eﬀective conﬁguration. Figure 8
further conﬁrms that PPO deals more eﬀectively with model misspeciﬁcations
with respect to DQN. When trained over GARCH dynamics, PPO achieves
better than the benchmark performance controlling associated risks and costs.
Figure 9 shows the realized out-of-sample holdings for some DQN agents.
When the underlying dynamics can be predicted by mean reverting factors, as
for the Gaussian and Student’s T cases, the inversion of the factor sign causes
the inversion of the sign of the portfolio itself. The oscillation between short
and long positions conﬁrms that the DRL algorithm has learned to follow
the signal present in the data. In particular, when compared with a partially
informed benchmark agent, as in the bottom left plot of Figure 9, the DQN
algorithm obtains a higher cumulative net P nL by anticipating the reverting
movement of the returns. Figure 10 presents out-of-sample holdings for PPO
in the same cases already shown for DQN. When the returns are driven by a
Gaussian or a Student’s T dynamics, PPO tracks the portfolio benchmark well
and in the partially informed case seems to anticipate the mean-reversion of
the signal as DQN does. In the case of GARCH dynamics both the algorithmic
approaches show a portfolio holding which greatly diﬀers from the benchmark.
In both ﬁgures we have two diﬀerent y-axes for the bottom right panel. In
order to visualize portfolio holdings that are on diﬀerent scales, we let the left
y-axis be associated with the algorithm and the right y-axis be associated with
the benchmark. The latter does not adapt well to heteroskedastic peaks in the
time-series of simulated returns and it happens that the benchmark portfolio

                                         1 H W  3 Q /                                   6 K D U S H  5 D W L R Q R U P D O V W X G H Q W 
 V  W ( L Q  E H Q F K P D U NSpringer Nature 2021 LATEX template

Deep Reinforcement Trading with Predictable Returns

17

is more sensitive to extreme events. On the other hand, RL is able to limit the
trading even in the presence of heteroskedasticity and obtain a lower portfolio
size that produces less transaction costs when it needs to be rebalanced.

Fig. 9 Portfolio Holdings for a snapshot of length 500 for some of the out-of-sample tests
performed. The DQN agents selected are the best performing for each group in terms of
cumulative net P nL: the Gaussian case with one factor, the Student’s T case with 6 degrees
of freedom (fully and partially informed) and the GARCH(1,1) with normal noise.

Fig. 10 Portfolio Holdings for a snapshot of length 500 for some of the out-of-sample tests
performed. The PPO agents selected are the best performing for each group in terms of
cumulative net P nL: the Gaussian case with one factor, the Student’s T case with 6 degrees
of freedom (fully and partially informed) and the GARCH(1,1) with normal noise.

It is important to check the robustness of the RL performance with respect
to the choice of the dynamics parameters, or conversely its sensitivity w.r.t.
some of them. Figures 11 and 12 show the level of SR for DQN and PPO agent
as a function of dynamics half life, factor loading and fat-tailed return distri-
bution parameters. The result is an average over 10 diﬀerent agents for each

    H  * D X V V L D Q     H  6 W X G  I X O O \  L Q I ' 4 1 E H Q F K P D U N           H  6 W X G  S D U W L D O O \  L Q I           H  * D U F K           H  R X W  R I  V D P S O H  L W H U D W L R Q V K R O G L Q J    H  * D X V V L D Q    H  6 W X G  I X O O \  L Q I 3 3 2 E H Q F K P D U N           H  6 W X G  S D U W L D O O \  L Q I           H  * D U F K    H  R X W  R I  V D P S O H  L W H U D W L R Q V K R O G L Q JSpringer Nature 2021 LATEX template

18

Deep Reinforcement Trading with Predictable Returns

parameter conﬁguration, this also allows to create conﬁdence intervals around
the average performance. In the top row of both ﬁgures the eﬀect of variation
in half-life of mean-reversion and in the factor loading b of a one-factor Gaus-
sian dynamics is similar for DQN and PPO. They both tend to obtain the
same SR of the benchmark strategy, which is in both cases an increasing func-
tion. This is due by one side from the fact that a higher half life produces a
more persistent return sign that agents can more easily exploit to make proﬁt.
From the other side when the factor loading is small then the return dynamics
contains no meaningful signal and the eq. (1) is driven purely by noise. The
left panel in the bottom row of 11 and 12 proposes the same sensitivity analy-
sis when we simulate one factor Student’s T dynamics with increasing degrees
of freedom ν. All the strategies get worse as the percentage of extreme events
increases but both PPO and DQN deal with riskier events in an eﬀective way
and consistently achieve a greater SR than the benchmark strategy. The right
panel in the bottom row of both ﬁgures presents the variation of the perfor-
mance when the kurtosis of the GARCH(1,1) process distribution increases.
The fourth standardized moment for the stochastic volatility process in eq (10)
is computed as

1 − (α1 + β1)2(cid:105)
3
1 − (α1 + β1)2 − 2α2
1

E (cid:0)σ4
t
t )]2 =
[E (σ2
1 − (α1 + β1)2 > 0, the tails in the distribution of
knowing that when 1 − 2α2
the GARCH(1,1) are heavier than a Gaussian. A GARCH(1,1) model with
heavy tails represents a consistent misspeciﬁcation of the original conditions
and only PPO consistently outperforms the benchmark while the tails of the
simulated return distribution become thicker.

(27)

(cid:104)

(cid:1)

Fig. 11 DQN performances measured by percentage or diﬀerential SR with respect to
the benchmark when some relevant parameter of the simulated dynamics varies. The top
row shows the one Gaussian mean-reverting factor dynamics when the level of half-life of
mean-reversion and factor loadings increases. The bottom row shows the one mean-reverting
Student’s T factor dynamics and the GARCH dynamics when respectively the degrees of
freedom (dof) and the kurtosis of the returns distribution increase.

       + D O I O L I H          H              ) D F W R U  / R D G L Q J    H  ' 4 1 E H Q F K P D U N       6 W X G H Q W 
 V  7  G R I          H           * $ 5 & +  N X U W R V L V    H  6 5Springer Nature 2021 LATEX template

Deep Reinforcement Trading with Predictable Returns

19

Fig. 12 PPO performances measured by percentage or diﬀerential SR with respect to
the benchmark when some relevant parameter of the simulated dynamics varies. The top
row shows the one Gaussian mean-reverting factor dynamics when the level of half-life of
mean-reversion and factor loadings increases. The bottom row shows the one mean-reverting
Student’s T factor dynamics and the GARCH dynamics when respectively the degrees of
freedom (dof) and the kurtosis of the returns distribution increase.
5 Conclusions

In this work we have used diﬀerent RL algorithms to solve a trading prob-
lem in a ﬁnancial environment where trading is costly. When the optimization
problem is known to have an exact solution, DQN and PPO are able to track
this benchmark, but they can also adapt to variations of the original environ-
ment setting and ﬁnd a way to control portfolio risks and costs in a data-driven
manner. While value-based DRL results to be accurate in following the trad-
ing signals and controlling the market frictions, policy-based DRL appears to
be more robust to extreme events and heteroskedastic volatility.

Although DQN is able to learn the direction of the trades, the discretization
of the action space still represents a major concern because the traded size is
a multiple of a ﬁxed quantity chosen in advance. Instead of moving towards
actor-critic frameworks, which usually shares the instability issues with DQN,
PPO helps in solving this issue by expressing the policy in a continuous action
space.

RL algorithms are demanding in terms of training data that can be quite
scarce especially at low frequencies. We believe that the use of a ﬁnancial
model with a known optimal solution can oﬀer a workaround to this problem
by allowing to pretrain DRL agents on synthetic data and then to ﬁne tune
them on real time series. Moreover, classical strategies can facilitate the train-
ing of DRL agents by providing information about good (even if suboptimal)
strategies. This can help in terms of a rationalisation of the state-action space,
which results in the lightening of the training process itself.

Acknowledgments

Daniele Tantari acknowledges GNFM-Indam for ﬁnancial support.

                + D O I O L I H          H                           ) D F W R U  / R D G L Q J    H  3 3 2 E H Q F K P D U N       6 W X G H Q W 
 V  7  G R I        H           * $ 5 & +  N X U W R V L V        H  6 5Springer Nature 2021 LATEX template

20

Deep Reinforcement Trading with Predictable Returns

References

Marcin Andrychowicz, Anton Raichuk, Piotr Sta´nczyk, Manu Orsini, Ser-
tan Girgin, Raphael Marinier, L´eonard Hussenot, Matthieu Geist, Olivier
Pietquin, Marcin Michalski, Sylvain Gelly, and Olivier Bachem. What
matters in on-policy reinforcement learning? a large-scale empirical study,
2020.

Kai Arulkumaran, Marc Peter Deisenroth, Miles Brundage, and Anil Anthony
Bharath. A brief survey of deep reinforcement learning. arXiv preprint
arXiv:1708.05866, 2017.

Cliﬀord S. Asness, Tobias J. Moskowitz, and Lasse Pedersen. Value and

momentum everywhere. Journal of Finance, 68(3):929–985, 2013.

Dimitri P. Bertsekas. Dynamic Programming and Optimal Control, volume I.

Athena Scientiﬁc, Belmont, MA, USA, 3rd edition, 2005.

Tim Bollerslev. A conditionally heteroskedastic time series model for specula-
tive prices and rates of return. The review of economics and statistics, pages
542–547, 1987.

Ayman Chaouki, Stephen Hardiman, Christian Schmidt, Emmanuel S´eri´e, and
Joachim De Lataillade. Deep deterministic portfolio optimization. The
Journal of Finance and Data Science, 6:16–30, 2020.

Djork-Arn´e Clevert, Thomas Unterthiner, and Sepp Hochreiter. Fast and accu-
rate deep network learning by exponential linear units (elus). arXiv preprint
arXiv:1511.07289, 2015.

R. Cont. Empirical properties of asset returns: stylized facts and statistical

issues. Quantitative Finance, 1(2):223–236, 2001.

Robert F Engle and Robert Ferstenberg. Execution risk. The Journal of

Trading, 2(2):10–20, 2007.

Logan Engstrom, Andrew Ilyas, Shibani Santurkar, Dimitris Tsipras, Firdaus
Janoos, Larry Rudolph, and Aleksander Madry. Implementation matters in
deep policy gradients: A case study on ppo and trpo, 2020.

Nicolae Gˆarleanu and Lasse Pedersen. Dynamic trading with predictable
returns and transaction costs. Journal of Finance, 68(6):2309–2340, 12 2013.

Nicolae Garleanu, Lasse Heje Pedersen, and Allen M Poteshman. Demand-
based option pricing. The Review of Financial Studies, 22(10):4259–4299,
2008.

Springer Nature 2021 LATEX template

Deep Reinforcement Trading with Predictable Returns

21

Richard Grinold. A dynamic model of portfolio management. Journal of

Investment Management, 4:5–22, 01 2006.

Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Delving deep into
rectiﬁers: Surpassing human-level performance on imagenet classiﬁcation. In
Proceedings of the IEEE international conference on computer vision, pages
1026–1034, 2015.

Sergey Ioﬀe and Christian Szegedy. Batch normalization: Accelerating deep
arXiv preprint

network training by reducing internal covariate shift.
arXiv:1502.03167, 2015.

Zhengyao Jiang, Dixing Xu, and Jinjun Liang. A deep reinforcement learning
framework for the ﬁnancial portfolio management problem. arXiv preprint
arXiv:1706.10059, 2017.

Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic

optimization. arXiv preprint arXiv:1412.6980, 2014.

Petter N Kolm and Lee Maclin. Algorithmic trading, optimal execution,
In The Oxford Handbook of Quantitative Asset

and dynamic portfolios.
Management. 2012.

Petter N Kolm and Gordon Ritter. Multiperiod portfolio selection and

bayesian dynamic models. Risk, 28(3):50–54, 2014.

Petter Kolm and Gordon Ritter. Modern perspectives on reinforcement

learning in ﬁnance. SSRN Electronic Journal, 2019.

Petter Kolm, Reha Tutuncu, and Frank Fabozzi. 60 years of portfolio opti-
mization: Practical challenges and current trends. European Journal of
Operational Research, 234:356–371, 04 2014.

Sergey Levine, Chelsea Finn, Trevor Darrell, and Pieter Abbeel. End-to-end
training of deep visuomotor policies. The Journal of Machine Learning
Research, 17(1):1334–1373, 2016.

Timothy P. Lillicrap, Jonathan J. Hunt, Alexander Pritzel, Nicolas Heess,
Tom Erez, Yuval Tassa, David Silver, and Daan Wierstra. Continuous con-
trol with deep reinforcement learning. In Yoshua Bengio and Yann LeCun,
editors, ICLR, 2016.

Fabrizio Lillo, J Doyne Farmer, and Rosario N Mantegna. Master curve for

price-impact function. Nature, 421(6919):129–130, 2003.

P. Marbach and J.N. Tsitsiklis. Simulation-based optimization of markov
reward processes. IEEE Transactions on Automatic Control, 46(2):191–209,
2001.

Springer Nature 2021 LATEX template

22

Deep Reinforcement Trading with Predictable Returns

Harry Markowitz. Portfolio selection. The Journal of Finance, 7(1):77–91,

March 1952.

Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei A Rusu, Joel
Veness, Marc G Bellemare, Alex Graves, Martin Riedmiller, Andreas K
Fidjeland, Georg Ostrovski, et al. Human-level control through deep
reinforcement learning. nature, 518(7540):529–533, 2015.

Volodymyr Mnih, Adri`a Puigdom`enech Badia, Mehdi Mirza, Alex Graves,
Timothy P. Lillicrap, Tim Harley, David Silver, and Koray Kavukcuoglu.
Asynchronous methods for deep reinforcement learning, 2016.

Tobias J. Moskowitz, Yao Hua Ooi, and Lasse Heje Pedersen. Time series

momentum. Journal of Financial Economics, 104(2):228–250, 2012.

John Schulman, Sergey Levine, Pieter Abbeel, Michael Jordan, and Philipp
Moritz. Trust region policy optimization.
In Francis Bach and David
Blei, editors, Proceedings of the 32nd International Conference on Machine
Learning, volume 37 of Proceedings of Machine Learning Research, pages
1889–1897, Lille, France, 07–09 Jul 2015. PMLR.

John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg
Klimov. Proximal policy optimization algorithms. CoRR, abs/1707.06347,
2017.

William F Sharpe. The sharpe ratio.

Journal of portfolio management,

21(1):49–58, 1994.

David Silver, Aja Huang, Chris J Maddison, Arthur Guez, Laurent Sifre,
George Van Den Driessche, Julian Schrittwieser, Ioannis Antonoglou, Veda
Panneershelvam, Marc Lanctot, et al. Mastering the game of go with deep
neural networks and tree search. nature, 529(7587):484–489, 2016.

Richard S. Sutton and Andrew G. Barto. Reinforcement Learning: An

Introduction. The MIT Press, 2018.

Richard Sutton, David Mcallester, Satinder Singh, and Yishay Mansour. Policy
gradient methods for reinforcement learning with function approximation.
Adv. Neural Inf. Process. Syst, 12, 02 2000.

Csaba Szepesv´ari. Algorithms for Reinforcement Learning. Synthesis Lec-
tures on Artiﬁcial Intelligence and Machine Learning. Morgan & Claypool
Publishers, 2010.

R.H. Tutuncu. Recent advances in portfolio optimization. The Oxford

Handbook of Quantitative Asset Management, 01 2011.

Springer Nature 2021 LATEX template

Deep Reinforcement Trading with Predictable Returns

23

G. E. Uhlenbeck and L. S. Ornstein. On the theory of the brownian motion.

Phys. Rev., 36:823–841, Sep 1930.

Hado Van Hasselt, Arthur Guez, and David Silver. Deep reinforcement learning
with double q-learning. In Proceedings of the AAAI conference on artiﬁcial
intelligence, volume 30, 2016.

Hado van Hasselt. Double q-learning. In John D. Laﬀerty, Christopher K. I.
Williams, John Shawe-Taylor, Richard S. Zemel, and Aron Culotta, editors,
NIPS, pages 2613–2621. Curran Associates, Inc., 2010.

Christopher J. C. H. Watkins and Peter Dayan. Q-learning.

In Machine

Learning, 1992.

Bing Xu, Naiyan Wang, Tianqi Chen, and Mu Li. Empirical evaluation of recti-
ﬁed activations in convolutional network. arXiv preprint arXiv:1505.00853,
2015.

Zihao Zhang, Stefan Zohren, and Stephen Roberts. Deep reinforcement
learning for trading. The Journal of Financial Data Science, 2(2):25–40,
2020.

Appendix A Algorithms and

Hyperparameters

In this section we provide some details regarding the implementation of Q-
learning and DQN algorithms used in the numerical experiments. Then we
outline the choice of the parameters for simulating the ﬁnancial data and of
the relevant hyperparameters to set up the training of the algorithms.

A.1 Q-learning

Q-learning requires the discretization of the state space S and the action space
A, which aﬀects the dimensionality of the Q-table that produces estimates
Q(St, At) of the optimal action-value function. For every possible state and
action variable we need to choose a proper discrete range that we believe is
adequately large to capture the relevant information and solve the problem.

Since real traders usually operate by trading quantities of assets that are
multiples of a ﬁxed size called lot, the dimensions of the Q-table are bounded
by setting the traded quantity ∆ht to be at most K round lots and the portfolio
holding ht to a maximum of M round lots. The discrete set of returns is
represented by an upper and lower bounded set of values that are linearly
spaced by the size of a basis point denoted as bp. The bounded sets and their
dimensionality are respectively:

A = {−K, −K + 1, . . . , K} ,

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12) = 2K + 1
(cid:12)A

(A1)

Springer Nature 2021 LATEX template

24

Deep Reinforcement Trading with Predictable Returns

H = {−M, −M + 1, . . . , M } ,

R = bp · {−T, −T + 1, . . . , T } ,

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)H
(cid:12) = 2M + 1
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12) = 2T + 1
(cid:12)R

(A2)

(A3)

In our ﬁnancial environment, the basis point and the lot are respectively the
size of minimum return movement and the minimum tradable quantity of the
asset at each discrete time. The sizes of the three sets are deﬁned respectively
by the hyperparameters K, M and T , which are a crucial choice to deﬁne the
magnitude of the synthetic ﬁnancial problem.
(cid:12) × (cid:12)
(cid:12)R(cid:12)
Denoting the size of the table as d = (cid:12)

(cid:12), our simulated exper-
iments show that Q-learning is not even able to reach a positive proﬁt when d
approaches the length of the simulated series Tin. The more the dimensional-
ity of the table increases, the worse are the cumulative net PnLs and rewards
obtained, when the Tin is ﬁxed.

(cid:12) × (cid:12)

(cid:12)H(cid:12)

(cid:12)A(cid:12)

If Tin is not suﬃciently long to allow the agent to visit the entire state space
and update the Q-table in each corresponding entry, the algorithm approxi-
mates the action-value function with a sparse Q-table. Thus, it is not able to
represent the eﬀect of slight changes in the state space variables. Such a bot-
tleneck becomes even worse if we increase the number of actions that the agent
can perform.

In principle we could let Q-learning experience longer simulated series to
partially avoid the exploration issue, but this would not be of any practical use
for two reasons: (i) it requires an increasingly long training runtime to match
the benchmark performance and still this result would be obtained under a
discretized state-space of a more complex ﬁnancial environment; (ii) training
for an high number of iteration the experiment would not even resemble a real
ﬁnancial application since there is no way to retrieve such a massive amount
of ﬁnancial data, especially at a daily frequency.

To ensure proper exploration of the state space, the agent acts according to
an (cid:15)-greedy policy, such that at each time a greedy action a = argmaxa Q(St, a)
is selected with probability 1−(cid:15), while occasionally with probability (cid:15) a random
action is sampled from the set A. As a common approach in the RL literature,
the value of (cid:15) decays linearly during training until it reaches a small value that
is kept ﬁxed until the end.

A.2 DQN

DQN requires a discretization of the action space A, which is approached as
for the tabular case. This discretization could represent an issue when one
wants to represent the choice of the agent at a more granular level. The more
one increases the size of A, the more the computational cost of the algo-
rithm increases and its eﬃciency in solving the ﬁnancial problem decreases.
However, we believe the discrete control can still be adequate for a set of ﬁnan-
cial problems since usually market orders are executed in multiples of a ﬁxed
quantity.

Springer Nature 2021 LATEX template

Deep Reinforcement Trading with Predictable Returns

25

Since the agent learns oﬄine by choosing past batches of experience from
a buﬀer with ﬁxed size, we set this dimension as a percentage of the total
updates in-sample Tin. We have found that letting the buﬀer size to increase
can improve the performance, therefore we do not discard any sequence. The
exploration-exploitation trade-oﬀ is balanced as in Q-learning, using an (cid:15)-
greedy policy where the (cid:15) decreases linearly to a low value towards the end
of the training. Despite the original DQN implementation (Mnih et al., 2015)
suggests to update the target network parameters at every ﬁxed discrete step,
we choose to continuously update the target parameter so that they slowly
track the learned networks as follows:

t ←− τ θ−
θ−

t + (1 − τ )θt

(A4)

where τ is the chosen step size for the update, θ−
t are the target network
parameters and θ are the parameters for the current action-value function
estimator.

A problem of the overestimation of the action-value function is known to
arise in the classical DQN algorithm (van Hasselt, 2010; Van Hasselt et al.,
2016). Thus we adopt the double DQN (DDQN) variant suggested in Van Has-
selt et al. (2016). Recalling that the target of a DQN update is computed
as

T DQN
t

= Rt+1 + ρ max

a

Q(St+1, a; θ),

(A5)

where we can write

max
a(cid:48)

Q (s(cid:48), a(cid:48); θ) = Q (s(cid:48), argmaxa(cid:48) Q (s(cid:48), a(cid:48); θ) ; θ) ,

it happens that the same noise aﬀects both the maximization over the action
space and the value function estimates. Removing the correlation between the
sources of noise coming into these two operations is beneﬁcial to avoid an
overestimation of the value function.

Double Q-learning decouples the selection of the action from the evaluation

as

T DDQN
t

= Rt+1 + ρQ(St+1, argmaxa(cid:48) Q (St+1, a(cid:48); θ1) ; θ2),

(A6)

i.e., DDQN uses two neural networks: one computes the target and the other
computes the current action-value function. The computation of the target is
split between the current neural network that greedily selects the action and
the target neural network that evaluates such action. Therefore, the selection
of the action in eq. (A6) is due to the current weights θ1 = θt, while the target
network is used to evaluate the value function for that action θ2 = θ−
t .

We have found that DDQN outperforms DQN in all the tests we carried
out, meaning that also in this speciﬁc ﬁnancial application it is a proﬁtable
procedure.

Regarding the shape of the loss function, a common choice for the DQN
family of algorithms is the Huber loss rather than the mean squared error

Springer Nature 2021 LATEX template

26

Deep Reinforcement Trading with Predictable Returns

(MSE), which is typical for regression tasks. Huber loss is less sensitive to the
presence of outliers and it is expressed as

Lδ(y, ˆy) =

(cid:40) 1
N
δ 1
N

(cid:80)N
i=1 (yi − ˆyi)2
(cid:80)N
i=1 | yi − ˆyi | − 1

for | yi − ˆyi |≤ δ

2 δ2 otherwise

(A7)

The Huber loss is quadratic for small values of the squared diﬀerence and
linear for larger values. This kind of loss function adds a lower penalty to large
errors and it is better than MSE for this kind of problems because learning by
estimates as in DQN could produce unexpectedly high errors. Even if in the
main we showed the case of a MSE loss, in practice our implementation utilizes
a Huber loss. This choice does not change the update rule of the presented
algorithms, but the computation of the gradient will diﬀer in the presence of
large quadratic errors.

The neural networks used to approximate the action-value function are
2-layer fully connected networks with ELU (Clevert et al., 2015) activation
and uniform weight initialization as in He et al. (2015). We have tried with
diﬀerent types of rectiﬁed nonlinear activation, but ELU outperforms a more
usual choice as ReLU. The sizes of the hidden layers are 256 and 128 for the
ﬁrst and the second respectively, but also smaller hidden layers have proven
to be eﬀective (Xu et al., 2015).

The gradient descent optimizer is Adam (Kingma and Ba, 2014) which
performs a batch update of size 256. The original implementation proposes
default values for β1, β2, and (cid:15)adam, which are respectively the exponential
decay rates for the ﬁrst and the second moment estimates of the true gradient
and a small constant for numerical stability. Those parameters required some
tuning for improving performances, so that we set them as β1 = 0.5, β2 = 0.75
and (cid:15)adam = 0.1 for all experiments. The learning rate α usually starts around
0.005 and then decays exponentially towards the end of the training.

Since in a RL setting the data are not all available at the beginning of the
training, we can not normalize our input variables as usual in the preprocessing
step of a supervised learning context. Hence, we add a Batch Normalization
layer (Ioﬀe and Szegedy, 2015) before the ﬁrst hidden layer to normalize the
inputs batch by batch and obtain the same eﬀect.

A.3 PPO

PPO allows expression continuous policies through an algorithm which is easier
to implement than a trust-region method (Schulman et al., 2015) and easier to
tune with respect to the continuous counterpart of DQN (Lillicrap et al., 2016).
In principle continuous policies are more expressive than discrete policies, but
are also harder to learn. Our implementation of PPO follows Andrychowicz et
al. (2020) which performs a large empirical study of the eﬀect of implementa-
tion and parameters choices on the PPO performances. Even if our ﬁnancial
problem is diﬀerent from their testbed, we also follow the direction of their

Springer Nature 2021 LATEX template

Deep Reinforcement Trading with Predictable Returns

27

results in order to tune our hyperparameters since we have limited computa-
tional resources to do this search from scratch. Another relevant source for an
eﬀective implementation is Engstrom et al. (2020).

As described in the main, we implement PPO in an actor-critic setting
without shared architectures. Diﬀerently from the standard implementaton,
the actor outputs a single scalar value, which is the mean of a Gaussian dis-
tribution, while the standard deviation is then learned as a global parameter
in the optimization process and updated using the same gradient optimizer.
Learning a global standard deviation for all the state representation has proven
to be as much as eﬀective aslearning a state dependent parameter, with the
beneﬁt of being slightly less computational expensive (Andrychowicz et al.,
2020). Policy gradient methods like PPO allow to insert some prior knowledge
on the form of the policy with respect to value-based methods. The real-valued
output of the actor usually passes through an hyperbolic tangent function in
order to bound the action in the interval [−1, 1]. Then is rescaled to directly
express a range of possible trading actions whose extreme values are selected
according to an heuristic described in the next subsection. Exploration during
training is guaranteed by the learned standard deviation parameter and by
the entropy bonus in the objective function. When doing out-of-sample tests,
the PPO policy is tested as if it were deterministic by just picking the mean
of the Gaussian instead of sampling from it. We take this choice because we
do not want the test results to be aﬀected by stochasticity.

The on-policy feature of PPO makes the training process episodic, so that
experience is collected by interacting with the environment and then discarded
immediately once the policy has been updated. The on-policy learning appears
in principle a more obvious setup for learning even if it comes with some caveats
because it makes the training less sample eﬃcient and more computationally
expensive since a new sequence of experiences need to be collected after each
update step. In this process, the advantage function is computed before per-
forming the optimization steps, when the discounted sum of returns over the
episode can be computed. In order to increase the training eﬃciency, after one
sweep through the collected samples, we compute again the advantage estima-
tor and perform another sweep through the same experience. This trick reduces
the computational expense of recollecting experiences and increases the sam-
ple eﬃciency of the training process. Usually we do at most 3 sweeps (epochs)
over a set of collected experiences before moving on and collecting a new set.
The optimizer and the normalization of the inputs through a Batch Nor-
malization layer are the same used for DQN, with the only exception that in
the PPO case we do not tune the hyperparameters of the Adam optimizer.

Maximizing the objective function that returns the gradient in eq. (21) is
known to be unstable since updates are not bounded and can move the policy
too far from the local optimum. Similarly to TRPO (Schulman et al., 2015),

Springer Nature 2021 LATEX template

28

Deep Reinforcement Trading with Predictable Returns

PPO optimizes an alternative objective to mitigate the instability

J CLIP(θ, ψ) = Eπθ

(cid:104)

min

(cid:16)

r(θ) ˆA (s, a; ψ) , clip (r(θ), 1 − (cid:15), 1 + (cid:15)) ˆA (s, a; ψ)

(cid:17)(cid:105)

(A8)
where r(θ) = π(At|St; θ)
π(At|St; θold) is a ratio indicating the relative probability of an
action under the current policy with respect to the old one. Instead of intro-
ducing a hard constraint as in TRPO, the ratio is bounded according to a
tolerance level (cid:15) to limit the magnitude of the updates. The combined objective
function in eq. (23) can be easily optimized by PyTorch’s automatic diﬀeren-
tiation engine, which quickly computes the gradients with respect to the two
sets of parameters θ and ψ. The implemented advantage estimator depends
on the parametrized value function Vψ and is a truncated version of the one
introduced by Mnih et al. (2016) for a rollout trajectory (episode) of length T :

ˆAt = δt + (γτ )δt+1 + · · · + · · · + (γτ )T −t+1δT −1

(A9)

where δt = rt + γVψ (st+1) − Vψ (st), γ is a discount rate with the same role
of ρ in DQN and τ is the exponential weight discount which controls the bias
variance trade-oﬀ in the advantage estimation. The generalized advantage esti-
mator (GAE) uses a discounted sum of temporal diﬀerence residuals similarly
to the one step target value of DQN in eq. (16).

(cid:12)A(cid:12)

A.4 Environment choices
In all the simulated experiments we set (cid:12)
(cid:12) = 5 for both Q-learning and DQN,
so that every agent can perform two buy actions, two sell actions and a zero
action. In order to make the results comparable with those of the benchmark
solution, we adopt a systematic way to choose the size of the action space
A from which we obtain the possible actions. Basically, we let the dynamic
programming solution run for some iterations before starting the training of
the RL algorithms and we look at the distribution of the continuous action
performed. Then we select the lower and upper boundary of A as respectively
the quantiles located at the 0.1% and the 99,9% of that distribution. In doing
so, we avoid extreme actions of the benchmark agent and allow the RL agents
to operate in the most possible similar setting to be able to compare the
performances. We could adopt the same approach for the discretization of the
variable in S as required by Q-learning, but this would produce very large Q-
tables ending up to be very sparse after the training process. Therefore, for
Q-learning we choose T = 0.05 and M = 100000.

For what concerns the simulated environment, the cost multiplier λ is cho-
sen equal to 0.001 for all experiments. Then we assume zero discount rate and
all the starting positions for the holding are zero. For any experiment involving
mean-reverting factors, we compute the speed of mean reversion φ as φ = log(2)
log(h)
where h is known as the half-life of mean-reversion and represents the time
it is expected to take for half of the trading signal to disappear. This allows

Springer Nature 2021 LATEX template

Deep Reinforcement Trading with Predictable Returns

29

to simulate the predicting factors and aggregate their eﬀect to compute the
asset returns. We tried many diﬀerent set up for the half-lives, factor loadings
and volatility of simulated assets and the ﬁndings are quite robust. The half-
lives of the mean-reverting factors are 350 for the case of a single factor and
(170,350) for the case of two factors. Factor loadings are chosen of the order of
magnitude of 10−3, speciﬁcally as 0.00535 and 0.005775 in the proposed cases.
The volatilities of the factor are respectively 0.2 and 0.1, while the volatility
of the unpredictable part of the asset return is always set to 0.01. Suitable
ranges for these hyperparameters is [0.5, 0.05] for the former and [0.05, 0.005]
for the latter. In general, DQN is also able to retrieve the underlying dynam-
ics in the case of two concurrent factors with diﬀerent speeds, as long as those
factors do not include one which is really fast (e.g. half-life of mean reversion
lower than 10 days) and also highly noisy with a volatility above 0.2. This
is acceptable because the signal-to-noise ratio would be very low and it may
require more sophisticated layers for feature extraction, other than a feedfor-
ward network structure. The parameters for the AR-GARCH simulation are
ω = 0.01, α1 = 0.05 and β1 = 0.94 which are common GARCH parameters
to simulate a stable ﬁnancial market. The autoregressive parameter is set to
φLr = 0.9 and the degrees of freedom in the Student’s T case are ν = 10.

