2
2
0
2

r
p
A
0
3

]
E
S
.
s
c
[

2
v
1
0
1
0
0
.
3
0
2
2
:
v
i
X
r
a

ApacheJIT: A Large Dataset for Just-In-Time Defect Prediction

Hossein Keshavarz
David R. Cheriton School of Computer Science
University of Waterloo
Waterloo, Ontario, Canada
hossein.keshavarz@uwaterloo.ca

Meiyappan Nagappan
David R. Cheriton School of Computer Science
University of Waterloo
Waterloo, Ontario, Canada
mei.nagappan@uwaterloo.ca

ABSTRACT
In this paper, we present ApacheJIT, a large dataset for Just-In-
Time (JIT) defect prediction. ApacheJIT consists of clean and bug-
inducing software changes in 14 popular Apache projects. ApacheJIT
has a total of 106,674 commits (28,239 bug-inducing and 78,435
clean commits). Having a large number of commits makes ApacheJIT
a suitable dataset for machine learning JIT models, especially deep
learning models that require large training sets to eﬀectively gen-
eralize the patterns present in the historical data to future data.

CCS CONCEPTS
• Software and its engineering → Software maintenance tools;
Software version control; Maintaining software.

KEYWORDS
Defect Prediction, Software Engineering, Dataset

ACM Reference Format:
Hossein Keshavarz and Meiyappan Nagappan. 2022. ApacheJIT: A Large
Dataset for Just-In-Time Defect Prediction. In 19th International Conference
on Mining Software Repositories (MSR ’22), May 23–24, 2022, Pittsburgh, PA,
USA. ACM, New York, NY, USA, 5 pages. https://doi.org/10.1145/3524842.3527996

1 INTRODUCTION
Change-level defect prediction, known as Just-In-Time (JIT) de-
fect prediction, has attracted researchers’ attention in recent years
[12, 19]. JIT defect prediction models are machine learning models
relying on historical data. They require a set of past change revi-
sions with each revision being identiﬁed whether or not it intro-
duced a bug to the software (bug-inducing). In addition to change
revisions and change labels, JIT defect prediction datasets often
come with change metrics that have proved to be helpful in analy-
sis and prediction [12, 13, 23].

Over the past few years, deep learning models found their way
to JIT defect prediction [8, 9]. Although deep learning models have
demonstrated solid performances in other areas of computing [1,
4, 17], DeepJIT [9] and and CC2Vec [8] do not outperform simple
methods like logistic regression [13, 18]. This can be attributed to
two main reasons. First, there are not many JIT datasets publicly

Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for proﬁt or commercial advantage and that copies bear this notice and the full cita-
tion on the ﬁrst page. Copyrights for components of this work owned by others than
ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or re-
publish, to post on servers or to redistribute to lists, requires prior speciﬁc permission
and/or a fee. Request permissions from permissions@acm.org.
MSR 2022, May 23–24, 2022, Pittsburgh, PA, USA
© 2022 Association for Computing Machinery.
ACM ISBN 978-1-4503-9303-4/22/05. . . $15.00
https://doi.org/10.1145/3524842.3527996

available and most of the existing ones are small; while deep learn-
ing models are more eﬀective when the size of the training data is
large [2, 24, 25]. Secondly, the number of bug-inducing changes in
the lifetime of a software system is often smaller than the number
of clean changes. This leads to the class imbalance problem in JIT
datasets. Undersampling the majority class makes the dataset even
smaller and oversampling the bug-inducing class introduces bias.
Deep learning models are more sensitive to both [11].

For example, one of the most widely used datasets for JIT defect
prediction is presented by McIntosh and Kamei [13]. Although this
dataset consists of carefully curated change revisions in QT 1 and
OpenStack2 projects, it has 25,150 QT changes and 12,374 Open-
Stack changes, and the ratios of bug-inducing changes to total changes
are 8% and 13% for QT and OpenStack respectively.

In this work, we present ApacheJIT, a large dataset for JIT de-
fect prediction. ApacheJIT consists of software changes in popular
Apache projects. These changes have been selected carefully af-
ter applying ﬁltering steps recommended in the literature [3, 13].
ApacheJIT has 106,674 commits (28,239 bug-inducing, 78,435 clean).
ApacheJIT is one of the largest available JIT defect prediction datasets
and it is suitable for JIT models that require a large number of soft-
ware changes with many bug-inducing changes.

2 RELATED WORK
We found four major JIT datasets in the literature that are large or
used in multiple studies.

Kamei et al. [12] performed a large-scale study of change-level
defect prediction and coined the term Just-In-Time Quality Assur-
ance, which evolved into Just-In-Time Defect Prediction. They in-
vestigated the eﬀectiveness of logistic regression on detecting bug-
inducing changes in 6 open-source and 5 commercial software projects.
They extracted the changes from CVS and linked the ﬁxing changes
to the issues in the issue tracking systems. They used the basic SZZ
algorithm [26] to label bug-inducing changes (except for two open-
source projects that did not have issue keys in their change com-
ments and they used Approximate SZZ). The dataset is not publicly
available.

Jiang et al. [10] attempted to separate the prediction for diﬀer-
ent developers and called this problem Personalized Defect Predic-
tion. They built a dataset of Java and C/C++ source codes from
6 open-source projects. The bug-ﬁxing changes of two projects
were previously manually found and for the rest of the projects,
they applied keyword search. They labeled bug-inducing changes
using SZZ without applying any ﬁltering. Although their dataset
has been used in [21] and [22], these works are done by the same
team and the data is not publicly available.

1https://www.qt.io/
2https://www.openstack.org/

 
 
 
 
 
 
MSR 2022, May 23–24, 2022, Pittsburgh, PA, USA

Keshavarz and Nagappan

McIntosh and Kamei [13] conducted a longitudinal study on JIT
bug prediction models to see how the performance of JIT models
changes over time. They built a dataset of 37,524 commits from
OpenStack (12,374 commits) and QT (25,150 commits). They ex-
tended their work in [3] and applied a set of ﬁltering steps on SZZ
to remove the false positive bug-inducing changes. The replication
package of the study and the datasets are available. This dataset has
been widely used to evaluate JIT models.

Fan et al. [6] investigated the impact of mislabeled changes la-
beled by four SZZ variants (Basic SZZ, AG-SZZ, MA-SZZ, RA-SZZ).
They claim that RA-SZZ generates the cleanest labels and used this
variant as the baseline. They did not include McIntosh and Kamei
[13]’s variant in the study because it does not address false nega-
tives (due to code indentation). They built a dataset of 10 Apache
projects with 126,526 commits. RA-SZZ (the baseline) identiﬁes
13,078 bug-inducing commits in this data. Although the authors
have made the data available, the link between revision IDs and
bug-inducing labels is missing. To the best of our knowledge, this
data is not used in any JIT model evaluation.

In this work, we adopt the McIntosh and Kamei [13]’s approach
to identify bug-inducing commits because the dataset has been
widely used to evaluate JIT models [7–9, 16].

3 APACHEJIT DATASET & USAGE
The current work presents the ApacheJIT dataset. ApacheJIT is
one of the largest available datasets for JIT defect prediction. This
dataset is a collection of carefully selected and ﬁltered software
changes in a set of popular Apache projects. ApacheJIT includes
106,674 software revisions from 2003 to 2019. These change revi-
sions are derived from the issue reports from January 1, 2010, to
December 31, 2019. 28,239 of these revisions are labeled as bug-
inducing through the process explained in Section 4. ApacheJIT
is suitable for defect prediction models that require a large set of
historical data to learn prediction models.

In particular, ApacheJIT can be used to train deep learning mod-
els that require large datasets for eﬀectively capturing the patterns
in the historical data and using them to accurately predict future
observations. Currently, the performances of deep learning mod-
els on JIT defect prediction datasets are not as promising as their
performances in other areas of computing. One reason is that avail-
able JIT defect prediction datasets do not contain many samples
and consequently, the number of bug-inducing changes models see
during the training is very small.

In addition to identifying whether or not each change revision
has introduced bugs into systems, the data presented in this work
also includes some of the change metrics that are commonly used
for JIT defect prediction. The following is the list of these metrics
(columns of the datasets):

change date, # of lines added, # lines deleted, # ﬁles touched, # di-
rectories touched, # of subsystems touched, change entropy, # of dis-
tinct developers touched ﬁles, the average time from last change, #
of unique changes in ﬁles, change author experience, change author
recent experience, change author subsystem experience.

The explanation of each metric is presented in Kamei et al. [12].
We used the same approach to obtain the change metrics in this

work. Table 2 shows the statistics of ApacheJIT. The ApacheJIT
dataset and the related scripts are publicly available3.

4 DATA CONSTRUCTION
The major part of constructing the ApacheJIT dataset is ﬁnding
bug-inducing commits. This part was done based on the SZZ algo-
rithm [26]. The SZZ algorithm has been widely used to detect bug-
inducing commits, and in this work, we used it with some modiﬁ-
cations. The SZZ algorithm starts with collecting the issue reports
that are marked as ﬁxed. Then these ﬁxed issue reports are linked
to their corresponding ﬁxing commits. And ﬁnally, from the lines
changed in the ﬁxing commits, potential bug-inducing commits
are detected.

Initially, we selected 15 popular Apache projects that had many
bug reports (we used the data of 14 project in the end). Our measure
of popularity in this selection was the number of stars each project
has on GitHub. Table 1 shows the selected projects.

Table 1: Selected Apache projects in this study

ActiveMQ
Zeppelin
Zookeeper

Camel
Groovy
Ignite

Cassandra
Hadoop HDFS

Spark
Hive
Hadoop MapReduce Mesos* Kafka

Flink
HBase

* removed after collecting ﬁxing commits (Section 4.2)

4.1 Bug Report Collection
As explained above, SZZ starts with collecting issue reports. All the
Apache projects we selected keep their issue reports on Apache’s
JIRA Issue Tracker4. On JIRA, after selecting the aforementioned
projects, we applied further ﬁltering. First, we narrowed down our
study focus to the issue reported from January 1, 2010, to December
31, 2019. Next, we ﬁltered out the issues that were not identiﬁed
as bugs. And ﬁnally, we picked the issues that are ﬁxed. On JIRA,
these issues are the issues with Status set to Closed or Resolved and
with Resolution set to Fixed. Finally, after applying the ﬁltering
steps mentioned above, we had 56,929 bug reports. Table 2 shows
the number of bug reports (issues) in each project.

4.2 Fixing Commit Collection
After obtaining the issue reports that have been ﬁxed, we looked
for the commits that ﬁxed these issue reports in the version control
system (VCS). All these projects use Git as their VCS. We followed
the approach in [12] and [13]. Each issue is identiﬁed uniquely
with an issue key on JIRA. With the help of this identiﬁer, for each
project, we searched through all the commits on the main branch
of the project Git repository and looked for commits whose com-
mit messages indicate the change is ﬁxing one of the issue keys
we collected. This approach works because conventionally, devel-
opers add the issue key of the bugs they ﬁx to the commit message.
In previous work, the search for these commits is done man-
ually by looking for keywords in the result of the git log com-
mand [12, 13, 26]. In this work, however, we utilized GitHub search.
This was feasible because all the projects we selected are stored on

3https://doi.org/10.5281/zenodo.5907001
4https://issues.apache.org/jira/

ApacheJIT: A Large Dataset for Just-In-Time Defect Prediction

MSR 2022, May 23–24, 2022, Pittsburgh, PA, USA

GitHub. The reason we preferred GitHub search to manual pat-
tern matching was that the GitHub search engine returns the best
match if it exists. This is especially useful when a commit message
of a ﬁxing commit does not include the issue key in the expected
format. We compared the result of the GitHub search with the re-
sult of string matching on git log outputs and found out that over-
all, the commits returned by GitHub search are more relevant. To
search on GitHub, we used the GitHub REST API5.

In this process, if there is no commit with a commit message
that contains an issue key, GitHub returns no commit. If there are
commits with commit messages containing an issue key, GitHub
returns one or several commits. We could identify the following
reasons for the latter case:

(1) Developers make several attempts to ﬁx an issue before clos-
ing the bug report because the ﬁrst attempts are not enough.
(2) Developers make several attempts to ﬁx an issue before clos-
ing the bug report because the ﬁrst attempts are incorrect.

The scenarios mentioned above make ﬁnding true ﬁxing com-
mits challenging in both pattern matching and GitHub search ap-
proaches. On one hand, one may decide to include all the com-
mits returned for one issue key because they are all related. On
the other hand, one may only consider the most relevant of the
multiple commits as the ﬁxing commit. In this work, we chose the
latter direction and picked the latest commit as the ﬁxing commit.
Our justiﬁcation was that by picking all the aforementioned com-
mits, later SZZ will consider many clean commits as bug-inducing
(case 2 above). Therefore, the ultimate dataset will have high false
positive bug-inducing commits.

Among all the commits returned for one bug report, we found
the latest one to be the most relevant. By picking the latest com-
mit as the ﬁx commit, we are almost certain that the commit we
have picked is truly a ﬁxing commit and can be used to ﬁnd bug-
inducing commits based on SZZ. This approach, however, leads
to missing some bug-inducing commits (case 1 above), and conse-
quently, our ultimate dataset will have higher false negative com-
mits (bug-inducing commits that are labeled as clean). Essentially,
this is a trade-oﬀ between more false positives and more false neg-
atives, and in this study, we chose the latter.

After collecting ﬁxing commits as described above for all 15
projects we noticed that the commit messages in Apache Mesos do
not comply with the conventional format and even GitHub search
was not able to ﬁnd ﬁxing commits. Therefore, we eliminated all
Apache Mesos data and continued with the remaining 14 projects.

4.3 Finding Bug-inducing Commits
At the end of the previous step, we linked 44,202 commits in the 14
projects to issue keys. These commits represent the ﬁxing commits
for the collected issues. The next step is to use these ﬁxing commits
to ﬁnd bug-inducing commits.

4.3.1 Git Annotate. In this step, each ﬁxing commit is traced using
the git annotate command. This command annotates all lines of a
given ﬁle showing the last revisions that touched each line. For
each ﬁxing commit, we ran git annotate on the ﬁles modiﬁed in
the commit and obtained the last revisions that touched the deleted

Table 2: The number of collected issues and the statistics of
ApacheJIT. Percentages under the Bug-inducing column in-
dicate the ratio of bug-inducing commits to total commits.

Project
ActiveMQ
Camel
Cassandra
Flink
Groovy
HDFS
HBase
Hive
Ignite
MapReduce
Mesos
Kafka
Spark
Zeppelin
Zookeeper
Total

Issues Bug-inducing Clean
4,722
1,404 (23%)
1,967
19,622
3,078 (14%)
3,276
5,042
3,117 (38%)
5,358
8,880
2,811 (24%)
4,166
6,445
1,614 (20%)
2,549
8,137
2,222 (21%)
3,672
4,948
3,782 (43%)
7,085
2,619
4,223 (61%)
7,931
9,597
2,439 (20%)
3,256
4,995
838 (15%)
2,080
-
-
2,955
1,269
1,115 (46%)
3,038
833
632 (43%)
7,648
829
622 (42%)
1,089
497
342 (40%)
859
78,435
28,239 (26%)
56,929

Total
6,126
22,700
8,159
11,691
8,059
10,359
8,730
6,842
12,036
5,833
-
2,384
1,465
1,451
839
106,674

lines before the ﬁxing commit. To implement this process, we used
the SZZ tool in the PyDriller framework6 [20]. The SZZ tool in
PyDriller gets a commit and returns the commits that last touched
the deleted lines of the ﬁles modiﬁed in the given commit.

4.3.2
Filtering. The basic version of SZZ has limitations. The SZZ
algorithm tends to label many clean commits as bug-inducing. Ac-
cordingly, we performed some heuristics to reduce false positives.
We followed the ﬁltering discussed in [3] and [13] to ﬁlter out
linked bug-inducing commits that are likely to be clean.

(1) We made ﬁxing commit - bug-inducing commit pairs and
associated each with the issue key corresponding to the ﬁx-
ing commit. We removed the pairs where the bug-inducing
commit date was after the issue report date (the date the is-
sue was created on JIRA). Note that we removed the pairs
and not the bug-inducing commits or the ﬁxing commits
separately as they may show up in other pairs and end up
as valid bug-inducing and valid ﬁxing commits respectively.
This step ﬁltered 5,048 bug-inducing commit candidates.
(2) At the end of the git annotate process each ﬁxing commit
may be linked to several bug-inducing commits (a ﬁxing
commit may ﬁx several bugs). We call the number of bug-
inducing commits each ﬁxing commit is linked to ﬁxcount.
da Costa et al. [3] and McIntosh and Kamei [13] ﬁlter out
ﬁxing commits whose ﬁxcounts are more than a threshold.
They refer to these commits as suspicious ﬁxing commits. In
their works, the threshold is set to upper Median Absolute
Deviation (MAD) of ﬁxcounts.

𝑢𝑝𝑝𝑒𝑟𝑀𝐴𝐷 = 𝑀 + 𝑚𝑒𝑑𝑖𝑎𝑛(|𝑀 − 𝑋𝑖 |),

(1)

where 𝑀 is the median of ﬁxcounts and 𝑋𝑖 is the ﬁxcount
of commit 𝑖.

5https://docs.github.com/en/rest

6https://pydriller.readthedocs.io/

MSR 2022, May 23–24, 2022, Pittsburgh, PA, USA

Keshavarz and Nagappan

In the present work, however, the upper MAD was too small,
and choosing it as the threshold would ﬁlter too many ﬁx-
ing commits. As an alternative, we chose the sum of the
mean and the standard deviation of ﬁxcounts as our thresh-
old. Note that again, this is a trade-oﬀ between high false
positive and high false negative. Filtering out too many ﬁx-
ing commits will cause more bug-inducing commits to be
labeled as clean commits in later steps. This step ﬁltered
12,165 commits.

(3) Similarly, we can deﬁne bugcount as the number of ﬁxing
commits each bug-inducing commit is linked to. This means
that one commit has introduced multiple bugs into the sys-
tem (multiple bug reports) and each has been ﬁxed by a ﬁx-
ing commit. Again, to ﬁlter out the suspicious bug-inducing
commits, we set a threshold of 𝑚𝑒𝑎𝑛+𝑠𝑡𝑑 of bugcounts. This
step removed 1,257 bug-inducing commit candidates.

(4) Following McIntosh and Kamei [13], we removed large com-
mits. By large commits, here, we mean the commits that
modify more than 100 ﬁles or have more than 10,000 lines of
changed code. Lines of changed code is the total number of
lines that were removed or added through the commit. This
step removed 890 bug-inducing commit candidates.

(5) In this work, we focused on Java programming language
and built a uniform Java dataset. Language constraint makes
it feasible to do static analysis on the source codes (our static
analysis is explained in the next part). Among the selected
projects, Java is the language in which most of the reposi-
tory ﬁles are written. Therefore, we picked Java and ﬁltered
out the commits that do not modify any Java source code.
10,251 commits were ﬁltered.

(6) To avoid trivial changes, we performed a static analysis on
the abstract syntax trees (ASTs) of the changed source code.
In this process, we compared the AST of each Java program
that was changed in a commit before and after the change. If
there was at least one node in either of the two ASTs without
a match in the other AST, we marked the change as non-
trivial and kept the Java source code; otherwise, the change
is trivial. We ﬁltered the commit if all the Java source codes
in it were changed trivially. To conduct this analysis, we
used GumTreeDiﬀ [5]. GumTreeDiﬀ is a tool that ﬁnds the
diﬀerences between two source codes written in the same
language using their ASTs. Examples of trivial changes are
changes that modify comments, white spaces, and string or
numeric literals. This step removed 274 commits.

Before applying the ﬁltering steps, there were 58,124 bug-inducing

commit candidates. 29,885 commits were ﬁltered and the remain-
ing 28,239 commits were labeled as bug-inducing commits. Table
2 shows the number of bug-inducing commits in each project. It
is worth mentioning that the number of bug-inducing commits in
Apache Spark is signiﬁcantly low with respect to the number of is-
sue reports in this project. The reason is that Java is not the major
programming language in Apache Spark.

4.4 Finding Clean Commits
In addition to the commits that introduce bugs into the system,
we also would like to know what changes are safe. We call these

changes clean commits. Usually in projects, the clean commits out-
number bug-inducing commits because most changes are reviewed
before they are integrated into the system.

We collected all the commits in the selected project repositories
from the date of the earliest bug-inducing commit (Sep 11, 2003)
to the date of the latest one (Dec 26, 2019). We removed the com-
mits we already had labeled as bug-inducing and the ﬁxing com-
mits that had at least one corresponding bug-inducing commit to
avoid bias (they change the same set of lines as their corresponding
bug-inducing commits). The number of remaining commits was
149,962. We also applied ﬁltering steps (4) and (5) in Section 4.3.2 to
make the ﬁltering process similar to bug-inducing commits (steps
(1)-(3) are not applicable). Finally, we had 78,435 clean commits.

4.5 Commit Metrics
In the end, we also added the set of common change metrics defect
prediction datasets have. The metrics we collected were the same
as the ones discussed in [12]. We followed the same steps.

5 LIMITATIONS
Like previous work, we used the SZZ algorithm to ﬁnd bug-inducing
changes in this work. Prior studies have discussed the limitations
of SZZ [3, 14, 15]. However, SZZ is still the most commonly used al-
gorithm for ﬁnding bug-inducing commits. We followed the ﬁlter-
ing steps that are shown to be eﬀective in removing safe changes
that are identiﬁed as bug-inducing by SZZ [3, 13]. Moreover, in
this work we utilized GumTreeDiﬀ [5] to remove trivial changes.
GumTreeDiﬀ is a powerful tool that uses abstract syntax trees (ASTs)
of programs to ﬁnd the structural diﬀerences between two codes;
however, it does not support all programming languages7.

Finally, in this work, the focus is on intrinsic bugs. Rodriguez-
Perez et al. [18] study another class of software bugs, called extrin-
sic bugs, that are not easily detected. Extrinsic bugs directly aﬀect
the performance of defect prediction models. As discussed in that
paper, the SZZ algorithm is unable to identify extrinsic bugs.

6 CONCLUSION
Among diﬀerent forms of software defect prediction, Just-In-Time
(JIT) defect prediction has gained popularity in recent years. Mod-
els attempting to predict whether a change is likely to introduce
bugs into the system are machine learning models trained on his-
torical data. Currently, most of the available datasets are small and
it is diﬃcult for JIT models to eﬀectively learn from historical data
and generalize to future data. In this work, we present ApacheJIT,
a large JIT defect prediction dataset with 106,674 software changes.
ApacheJIT has 28,239 bug-inducing changes. The dataset is avail-
able here: https://doi.org/10.5281/zenodo.5907001

ACKNOWLEDGMENTS
The authors would like to thank Dr. Gema Rodríguez-Pérez, Dr.
Shane McIntosh, and Dr. Yasutaka Kamei for their invaluable help
in collecting this dataset. The authors also acknowledge that our
work takes place on the traditional territory of the Neutral, Anishi-
naabeg and Haudenosaunee peoples.

7https://github.com/GumTreeDiﬀ/gumtree/wiki/Languages

ApacheJIT: A Large Dataset for Just-In-Time Defect Prediction

MSR 2022, May 23–24, 2022, Pittsburgh, PA, USA

REFERENCES
[1] Md Zahangir Alom, Tarek M. Taha, Chris Yakopcic, Stefan Westberg, Pa-
heding Sidike, Mst Shamima Nasrin, Mahmudul Hasan, Brian C. Van Essen,
Abdul A. S. Awwal, and Vijayan K. Asari. 2019. A State-of-the-Art Sur-
vey on Deep Learning Theory and Architectures.
Electronics 8, 3 (2019).
https://doi.org/10.3390/electronics8030292

[2] Jayme Garcia Arnal Barbedo. 2018.

Impact of dataset size and variety on
the eﬀectiveness of deep learning and transfer learning for plant disease
classiﬁcation.
Computers and Electronics in Agriculture 153 (2018), 46–53.
https://doi.org/10.1016/j.compag.2018.08.013

[3] Daniel Alencar da Costa, Shane McIntosh, Weiyi Shang, Uirá Kulesza,
A Framework for Eval-
Roberta Coelho, and Ahmed E. Hassan. 2017.
uating the Results of the SZZ Approach for Identifying Bug-Introducing
Changes.
IEEE Transactions on Software Engineering 43, 7 (2017), 641–657.
https://doi.org/10.1109/TSE.2016.2616306

[4] Shi Dong, Ping Wang, and Khushnood Abbas. 2021. A survey on deep
Computer Science Review 40 (2021), 100379.

learning and its applications.
https://doi.org/10.1016/j.cosrev.2021.100379

[5] Jean-Rémy Falleri, Floréal Morandat, Xavier Blanc, Matias Martinez, and
Martin Monperrus. 2014.
Fine-grained and accurate source code diﬀer-
encing. In ACM/IEEE International Conference on Automated Software En-
gineering, ASE ’14, Vasteras, Sweden - September 15 - 19, 2014. 313–324.
https://doi.org/10.1145/2642937.2642982

[6] Yuanrui Fan, Xin Xia, Daniel Alencar da Costa, David Lo, Ahmed E. Hassan, and
Shanping Li. 2021. The Impact of Mislabeled Changes by SZZ on Just-in-Time
Defect Prediction. IEEE Transactions on Software Engineering 47, 8 (2021), 1559–
1586. https://doi.org/10.1109/TSE.2019.2929761

[7] Jiri Gesi, Jiawei Li, and Iftekhar Ahmed. 2021. An Empirical Examination of the
Impact of Bias on Just-in-Time Defect Prediction. Association for Computing Ma-
chinery, New York, NY, USA. https://doi.org/10.1145/3475716.3475791

[8] Thong Hoang, Hong Jin Kang, David Lo, and Julia Lawall. 2020. CC2Vec: Dis-
tributed Representations of Code Changes. In 2020 IEEE/ACM 42nd International
Conference on Software Engineering (ICSE). 518–529.

[9] Thong Hoang, Hoa Khanh Dam, Yasutaka Kamei, David Lo, and Naoyasu
Ubayashi. 2019. DeepJIT: An End-to-End Deep Learning Framework for Just-in-
Time Defect Prediction. In 2019 IEEE/ACM 16th International Conference on Min-
ing Software Repositories (MSR). 34–45. https://doi.org/10.1109/MSR.2019.00016
[10] Tian Jiang, Lin Tan, and Sunghun Kim. 2013. Personalized defect prediction. In
2013 28th IEEE/ACM International Conference on Automated Software Engineering
(ASE). 279–289. https://doi.org/10.1109/ASE.2013.6693087

[11] Justin M. Johnson and Taghi M. Khoshgoftaar. 2019. Survey on deep learning

with class imbalance. Journal of Big Data 6 (2019), 1–54.

[12] Yasutaka Kamei, Emad Shihab, Bram Adams, Ahmed E. Hassan, Audris Mockus,
Anand Sinha, and Naoyasu Ubayashi. 2013. A large-scale empirical study of
just-in-time quality assurance. IEEE Transactions on Software Engineering 39, 6
(2013), 757–773. https://doi.org/10.1109/TSE.2012.70

[13] Shane McIntosh and Yasutaka Kamei. 2018.

Are Fix-Inducing Changes
a Moving Target? A Longitudinal Case Study of Just-In-Time Defect Pre-
diction.
IEEE Transactions on Software Engineering 44, 5 (2018), 412–428.
https://doi.org/10.1109/TSE.2017.2693980

[14] Edmilson Campos Neto, Daniel Alencar da Costa, and Uirá Kulesza. 2019. Re-
visiting and Improving SZZ Implementations. In 2019 ACM/IEEE International

Symposium on Empirical Software Engineering and Measurement (ESEM). 1–12.
https://doi.org/10.1109/ESEM.2019.8870178

[15] Edmilson Campos Neto, Daniel Alencar da Costa, and Uirá Kulesza. 2018. The
impact of refactoring changes on the SZZ algorithm: An empirical study. In 2018
IEEE 25th International Conference on Software Analysis, Evolution and Reengi-
neering (SANER). 380–390. https://doi.org/10.1109/SANER.2018.8330225
[16] Chanathip Pornprasit and Chakkrit Tantithamthavorn. 2021. JITLine: A Simpler,
Better, Faster, Finer-grained Just-In-Time Defect Prediction. In Proceedings of the
International Conference on Mining Software Repositories (MSR). 369–379.
[17] Samira Pouyanfar, Saad Sadiq, Yilin Yan, Haiman Tian, Yudong Tao, Maria Presa
Reyes, Mei-Ling Shyu, Shu-Ching Chen, and S. S. Iyengar. 2018. A Survey on
Deep Learning: Algorithms, Techniques, and Applications. ACM Comput. Surv.
51, 5, Article 92 (sep 2018), 36 pages. https://doi.org/10.1145/3234150

[18] Gema Rodriguez-Perez, Meiyappan Nagappan, and Gregorio Robles. 2020.
Watch out for Extrinsic Bugs! A Case Study of their Impact in Just-In-Time Bug
Prediction Models on the OpenStack project. IEEE Transactions on Software En-
gineering (2020), 1–1. https://doi.org/10.1109/TSE.2020.3021380

[19] Emad Shihab, Ahmed E. Hassan, Bram Adams, and Zhen Ming Jiang. 2012. An
Industrial Study on the Risk of Software Changes. In Proceedings of the ACM SIG-
SOFT 20th International Symposium on the Foundations of Software Engineering
(Cary, North Carolina) (FSE ’12). Association for Computing Machinery, New
York, NY, USA, Article 62, 11 pages. https://doi.org/10.1145/2393596.2393670

[20] Davide Spadini, Maurício Aniche, and Alberto Bacchelli. 2018. PyDriller: Python
Framework for Mining Software Repositories. In Proceedings of the 2018 26th
ACM Joint Meeting on European Software Engineering Conference and Sympo-
sium on the Foundations of Software Engineering (Lake Buena Vista, FL, USA)
(ESEC/FSE 2018). Association for Computing Machinery, New York, NY, USA,
908–911. https://doi.org/10.1145/3236024.3264598

[21] Ming Tan, Lin Tan, Sashank Dara, and Caleb Mayeux. 2015. Online Defect Predic-
tion for Imbalanced Data. In 2015 IEEE/ACM 37th IEEE International Conference
on Software Engineering, Vol. 2. 99–108. https://doi.org/10.1109/ICSE.2015.139

[22] Song Wang, Taiyue Liu, Jaechang Nam, and Lin Tan. 2020. Deep Semantic Fea-
ture Learning for Software Defect Prediction.
IEEE Transactions on Software
Engineering 46, 12 (2020), 1267–1293. https://doi.org/10.1109/TSE.2018.2877612
[23] Yibiao Yang, Yuming Zhou, Jinping Liu, Yangyang Zhao, Hongmin Lu, Lei
Xu, Baowen Xu, and Hareton Leung. 2016.
Eﬀort-Aware Just-in-Time De-
fect Prediction: Simple Unsupervised Models Could Be Better than Super-
vised Models. In Proceedings of the 2016 24th ACM SIGSOFT International
Symposium on Foundations of Software Engineering (Seattle, WA, USA) (FSE
2016). Association for Computing Machinery, New York, NY, USA, 157–168.
https://doi.org/10.1145/2950290.2950353

[24] Xiangxin Zhu, Carl Vondrick, Charless C Fowlkes, and Deva Ramanan. 2016.
Do we need more training data? International Journal of Computer Vision 119, 1
(2016), 76–92.

[25] Ayşe Nur Çayır and Tuğba Selcen Navruz. 2021. Eﬀect of Dataset Size on
Deep Learning in Voice Recognition. In 2021 3rd International Congress on
Human-Computer Interaction, Optimization and Robotic Applications (HORA). 1–
5. https://doi.org/10.1109/HORA52670.2021.9461395

[26] Jacek Śliwerski, Thomas Zimmermann, and Andreas Zeller. 2005. When
the 2005 International
Induce Fixes?.
Do Changes
Workshop on Mining Software Repositories
(MSR
’05). Association for Computing Machinery, New York, NY, USA, 1–5.
https://doi.org/10.1145/1083142.1083147

(St. Louis, Missouri)

In Proceedings of

