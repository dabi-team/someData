Reinforcement Learning for Integer Programming: Learning to Cut

Yunhao Tang 1 Shipra Agrawal 1 Yuri Faenza 1

0
2
0
2

l
u
J

1
2

]

G
L
.
s
c
[

3
v
9
5
8
4
0
.
6
0
9
1
:
v
i
X
r
a

Abstract

Integer programming is a general optimization
framework with a wide variety of applications,
e.g., in scheduling, production planning, and
graph optimization. As Integer Programs (IPs)
model many provably hard to solve problems,
modern IP solvers rely on heuristics. These heuris-
tics are often human-designed, and tuned over
time using experience and data. The goal of this
work is to show that the performance of those
heuristics can be greatly enhanced using reinforce-
ment learning (RL). In particular, we investigate
a speciﬁc methodology for solving IPs, known
as the cutting plane method. This method is em-
ployed as a subroutine by all modern IP solvers.
We present a deep RL formulation, network archi-
tecture, and algorithms for intelligent adaptive se-
lection of cutting planes (aka cuts). Across a wide
range of IP tasks, we show that our trained RL
agent signiﬁcantly outperforms human-designed
heuristics. Further, our experiments show that the
RL agent adds meaningful cuts (e.g. resembling
cover inequalities when applied to the knapsack
problem), and has generalization properties across
instance sizes and problem classes. The trained
agent is also demonstrated to beneﬁt the popular
downstream application of cutting plane methods
in Branch-and-Cut algorithm, which is the back-
bone of state-of-the-art commercial IP solvers.

1. Introduction

Integer Programming is a very versatile modeling tool for
discrete and combinatorial optimization problems, with ap-
plications in scheduling and production planning, among
others. In its most general form, an Integer Program (IP)
minimizes a linear objective function over a set of integer
points that satisfy a ﬁnite family of linear constraints. Clas-

1Columbia University, New York, USA. Correspondence to:

yt2541@columbia.edu <Yunhao>.

Proceedings of the 37 th International Conference on Machine
Learning, Vienna, Austria, PMLR 119, 2020. Copyright 2020 by
the author(s).

sical results in polyhedral theory (see e.g. Conforti et al.
(2014)) imply that any combinatorial optimization problem
with ﬁnite feasible region can be formulated as an IP. Hence,
IP is a natural model for many graph optimization problems,
such as the celebrated Traveling Salesman Problem (TSP).

Due to the their generality, IPs can be very hard to solve in
theory (NP-hard) and in practice. There is no polynomial
time algorithm with guaranteed solutions for all IPs. It is
therefore crucial to develop efﬁcient heuristics for solving
speciﬁc classes of IPs. Machine learning (ML) arises as a
natural tool for tuning those heuristics. Indeed, the appli-
cation of ML to discrete optimization has been a topic of
signiﬁcant interest in recent years, with a range of different
approaches in the literature (Bengio et al., 2018)

One set of approaches focus on directly learning the map-
ping from an IP instance to an approximate optimal solu-
tion (Vinyals et al., 2015; Bello et al., 2017; Nowak et al.,
2017; Kool and Welling, 2018). These methods implicitly
learn a solution procedure for a problem instance as a func-
tion prediction. These approaches are attractive for their
blackbox nature and wide applicability. At the other end
of the spectrum are approaches which embed ML agents
as subroutines in a problem-speciﬁc, human-designed al-
gorithms (Dai et al., 2017; Li et al., 2018). ML is used
to improve some heuristic parts of that algorithm. These
approaches can beneﬁt from algorithm design know-how
for many important and difﬁcult classes of problems, but
their applicability is limited by the speciﬁc (e.g. greedy)
algorithmic techniques.

In this paper, we take an approach with the potential to
combine the beneﬁts of both lines of research described
above. We design a reinforcement learning (RL) agent to be
used as a subroutine in a popular algorithmic framework for
IP called the cutting plane method, thus building upon and
beneﬁting from decades of research and understanding of
this fundamental approach for solving IPs. The speciﬁc cut-
ting plane algorithm that we focus on is Gomory’s method
(Gomory, 1960). Gomory’s cutting plane method is guaran-
teed to solve any IP in ﬁnite time, thus our approach enjoys
wide applicability. In fact, we demonstrate that our trained
RL agent can even be used, in an almost blackbox man-
ner, as a subroutine in another powerful IP method called
Branch-and-Cut (B&C), to obtain signiﬁcant improvements.

 
 
 
 
 
 
Reinforcement Learning for Integer Programming: Learning to Cut

A recent line of work closely related to our approach in-
cludes (Khalil et al., 2016; 2017; Balcan et al., 2018), where
supervised learning is used to improve branching heuris-
tics in the Branch-and-Bound (B&B) framework for IP. To
the best of our knowledge, no work on focusing on pure
selection of (Gomory) cuts has appeared in the literature.

Cutting plane and B&C methods rely on the idea that every
IP can be relaxed to a Linear program (LP) by dropping the
integrality constraints, and efﬁcient algorithms for solving
LPs are available. Cutting plane methods iteratively add
cuts to the LPs, which are linear constraints that can tighten
the LP relaxation by eliminating some part of the feasible
region, while preserving the IP optimal solution. B&C
methods are based on combining B&B with cutting plane
methods and other heuristics; see Section 2 for details.

Cutting plane methods have had a tremendous impact on
the development of algorithms for IPs, e.g., these meth-
ods were employed to solve the ﬁrst non-trivial instance of
TSP (Dantzig et al., 1954). The systematic use of cutting
planes has moreover been responsible for the huge speedups
of IP solvers in the 90s (Balas et al., 1993; Bixby, 2017).
Gomory cuts and other cutting plane methods are today
widely employed in modern solvers, most commonly as
a subroutine of the B&C methods that are the backbone
of state-of-the-art commercial IP solvers like Gurobi and
Cplex (Gurobi Optimization, 2015). However, despite the
amount of research on the subject, deciding which cutting
plane to add remains a non-trivial task. As reported in (Dey
and Molinaro, 2018), “several issues need to be considered
in the selection process [...] unfortunately the traditional
analyses of strength of cuts offer only limited help in under-
standing and addressing these issues”. We believe ML/RL
not only can be utilized to achieve improvements towards
solving IPs in real applications, but may also aid researchers
in understanding effective selection of cutting planes. While
modern solvers use broader classes of cuts than just Go-
mory’s, we decided to focus on Gomory’s approach because
it has the nice theoretical properties seen above, it requires
no further input (e.g. other human-designed cuts) and, as
we will see, it leads to a well-deﬁned and compact action
space, and to clean evaluation criteria for the impact of RL.

Our contributions. We develop an RL based method for
intelligent adaptive selection of cutting planes, and use it
in conjunction with Branch-and-Cut methods for efﬁciently
solving IPs. Our main contributions are the following:

ple, directly formulating the B&C method as an MDP
would lead to a very large state space containing all open
branches. Another example is the use of Gomory’s cuts
(vs. other cutting plane methods), which helped limit the
number of actions (available cuts) in every round to the
number of variables. For some other classes of cuts, the
number of available choices can be exponentially large.

• Deep RL solution architecture design. We build upon
state-of-the-art deep learning techniques to design an ef-
ﬁcient and scalable deep RL architecture for learning to
cut. Our design choices aim to address several unique
challenges in this problem. These include slow state-
transition machine (due to the complexity of solving LPs)
and the resulting need for an architecture that is easy to
generalize, order and size independent representation, re-
ward shaping to handle frequent cases where the optimal
solution is not reached, and handling numerical errors
arising from the inherent nature of cutting plane methods.

• Empirical evaluation. We evaluate our approach over a
range of classes of IP problems (namely, packing, binary
packing, planning, and maximum cut). Our experiments
demonstrate signiﬁcant improvements in solution accu-
racy as compared to popular human designed heuristics
for adding Gomory’s cuts. Using our trained RL policy
for adding cuts in conjunction with B&C methods leads
to further signiﬁcant improvements, thus illustrating the
promise of our approach for improving state-of-the-art IP
solvers. Further, we demonstrate the RL agent’s potential
to learn meaningful and effective cutting plane strategies
through experiments on the well-studied knapsack prob-
lem. In particular, we show that for this problem, the
RL agent adds many more cuts resembling lifted cover
inequalities when compared to other heuristics. Those
inequalities are well-studied in theory and known to work
well for packing problems in practice. Moreover, the
RL agent is also shown to have generalization properties
across instance sizes and problem classes, in the sense
that the RL agent trained on instances of one size or from
one problem class is shown to perform competitively for
instances of a different size and/or problem class.

2. Background on Integer Programming

Integer programming.
It is well known that any Integer
Program (IP) can be written in the following canonical form

min{cT x : Ax ≤ b, x ≥ 0, x ∈ Zn}

(1)

• Efﬁcient MDP formulation. We introduce an efﬁcient
Markov decision process (MDP) formulation for the prob-
lem of sequentially selecting cutting planes for an IP.
Several trade-offs between the size of state-space/action-
space vs. generality of the method were navigated in
order to arrive at the proposed formulation. For exam-

where x is the set of n decision variables, Ax ≤ b, x ≥ 0
with A ∈ Qm×n, b ∈ Qm formulates the set of constraints,
and the linear objective function is cT x for some c ∈ Qn.
x ∈ Zn implies we are only interested in integer solutions.
Let x∗
IP denote the optimal solution to the IP in (1), and z∗
IP
the corresponding objective value.

Reinforcement Learning for Integer Programming: Learning to Cut

LP(0) ≤ z∗

IP. Let us assume x∗

LP(0) its optimal solution, and z∗

The cutting plane method for integer programming.
The cutting plane method starts with solving the LP obtained
from (1) by dropping the integrality constraints x ∈ Zn.
This LP is called the Linear Relaxation (LR) of (1). Let
C(0) = {x|Ax ≤ b, x ≥ 0} be the feasible region of this
LP, x∗
LP(0) its objective
value. Since C(0) contains the feasible region of (1), we
LP(0) /∈ Zn. The cut-
have z∗
ting plane method then ﬁnds an inequality aT x ≤ β (a cut)
that is satisﬁed by all integer feasible solutions of (1), but
not by x∗
LP(0) (one can prove that such an inequality always
exists). The new constraint aT x ≤ β is added to C(0), to
obtain feasible region C(1) ⊆ C(0); and then the new LP is
solved, to obtain x∗
LP(1). This procedure is iterated until
LP(t) ∈ Zn. Since C(t) contains the feasible region of (1),
x∗
x∗
LP(t) is an optimal solution to the integer program (1). In
fact, x∗
LP(t) is the only feasible solution to (1) produced
throughout the algorithm.

IP − z∗

A typical way to compare cutting plane methods is by the
number of cuts added throughout the algorithm: a better
method is the one that terminates after adding a smaller num-
ber of cuts. However, even for methods that are guaranteed
to terminate in theory, in practice often numerical errors
will prevent convergence to a feasible (optimal) solution.
In this case, a typical way to evaluate the performance is
the following. For an iteration t of the method, the value
gt := z∗
LP(t) ≥ 0 is called the (additive) integrality
gap of C(t). Since C(t+1) ⊆ C(t), we have that gt ≥ gt+1.
Hence, the integrality gap decreases during the execution of
the cutting plane method. A common way to measure the
performance of a cutting plane method is therefore given
by computing the factor of integrality gap closed between
the ﬁrst LR, and the iteration τ when we decide to halt the
method (possibly without reaching an integer optimal solu-
tion), see e.g. Wesselmann and Stuhl (2012). Speciﬁcally,
we deﬁne the Integrality Gap Closure (IGC) as the ratio

g0 − gτ
g0

∈ [0, 1].

(2)

i ∈ It, we can generate a Gomory cut (Gomory, 1960)

(− ˜A(i) + (cid:98) ˜A(i)(cid:99))T x ≤ −˜bi + (cid:98)˜bi(cid:99),

(3)

where ˜A(i) is the ith row of matrix ˜A and (cid:98)·(cid:99) means
component-wise rounding down. Gomory cuts can therefore
be generated for any IP and, as required, are valid for all in-
teger points from (1) but not for x∗
LP(t). Denote the set of all
candidate cuts in round t as D(t), so that It := |D(t)| = |It|.

It is shown in Gomory (1960) that a cutting plane method
which at each step adds an appropriate Gomory’s cut termi-
nates in a ﬁnite number of iteration. At each iteration t, we
have as many as It ∈ [n] cuts to choose from. As a result,
the efﬁciency and quality of the solutions depend highly on
the sequence of generated cutting planes, which are usually
chosen by heuristics (Wesselmann and Stuhl, 2012). We
aim to show that the choice of Gomory’s cuts, hence the
quality of the solution, can be signiﬁcantly improved with
RL.

LP(t)]1(cid:99) and x1 ≥ (cid:100)[x∗

Branch and cut. In state-of-the-art solvers, the addition of
cutting planes is alternated with a branching phase, which
can be described as follows. Let x∗
LP(t) be the solution to
the current LR of (1), and assume that some component
of x∗
LP(t), say wlog the ﬁrst, is not integer (else, x∗
LP(t) is
the optimal solution to (1)). Then (1) can be split into two
subproblems, whose LRs are obtained from C(t) by adding
constraints x1 ≤ (cid:98)[x∗
LP(t)]1(cid:101), respec-
tively. Note that the set of feasible integer points for (1) is
the union of the set of feasible integer points for the two new
subproblems. Hence, the integer solution with minimum
value (for a minimization IP) among those subproblems
gives the optimal solution to (1). Several heuristics are
used to select which subproblem to solve next, in attempt
to minimize the number of suproblems (also called child
nodes) created. An algorithm that alternates between the cut-
ting plane method and branching is called Branch-and-Cut
(B&C). When all the other parameters (e.g., the number of
cuts added to a subproblem) are kept constant, a typical way
to evaluate a B&C method is by the number of subproblems
explored before the optimal solution is found.

In order to measure the IGC achieved by RL agent on test
instances, we need to know the optimal value z∗
IP for those
instances, which we compute with a commercial IP solver.
Importantly, note that we do not use this measure, or the op-
timal objective value, for training, but only for evaluation.

Gomory’s Integer Cuts. Cutting plane algorithms differ in
how cutting planes are constructed at each iteration. Assume
that the LR of (1) with feasible region C(t) has been solved
via the simplex algorithm. At convergence, the simplex
algorithm returns a so-called tableau, which consists of a
constraint matrix ˜A and a constraint vector ˜b. Let It be the
set of components [x∗
LP(t)]i that are fractional. For each

3. Deep RL Formulation and Solution

Architecture

Here we present our formulation of the cutting plane selec-
tion problem as an RL problem, and our deep RL based
solution architecture.

3.1. Formulating Cutting Plane selection as RL

The standard RL formulation starts with an MDP: at time
step t ≥ 0, an agent is in a state st ∈ S, takes an action
at ∈ A, receives an instant reward rt ∈ R and transitions to
the next state st+1 ∼ p(·|st, at). A policy π : S (cid:55)→ P(A)

Reinforcement Learning for Integer Programming: Learning to Cut

gives a mapping from any state to a distribution over actions
π(·|st). The objective of RL is to search for a policy that
maximizes the expected cumulative rewards over a horizon
T , i.e., maxπ J(π) := Eπ[(cid:80)T −1
t=0 γtrt], where γ ∈ (0, 1]
is a discount factor and the expectation is w.r.t. randomness
in the policy π as well as the environment (e.g. the transi-
tion dynamics p(·|st, at)). In practice, we consider param-
eterized policies πθ and aim to ﬁnd θ∗ = arg maxθ J(πθ).
Next, we formulate the procedure of selecting cutting planes
into an MDP. We specify state space S, action space A, re-
ward rt and the transition st+1 ∼ p(·|st, at).

i x ≤ bi}Nt

State Space S. At iteration t, the new LP is deﬁned by
the feasible region C(t) = {aT
i=1 where Nt is
the total number of constraints including the original linear
constraints (other than non-negativity) in the IP and the cuts
added so far. Solving the resulting LP produces an optimal
solution x∗
LP(t) along with the set of candidate Gomory’s
cuts D(t). We set the numerical representation of the state
to be st = {C(t), c, x∗
LP(t), D(t)}. When all components of
x∗
LP(t) are integer-valued, st is a terminal state and D(t) is
an empty set.

Action Space A. At iteration t, the available actions are
given by D(t), consisting of all possible Gomory’s cutting
planes that can be added to the LP in the next iteration.
The action space is discrete because each action is a dis-
crete choice of the cutting plane. However, each action is
represented as an inequality eT
i x ≤ di, and therefore is
parameterized by ei ∈ Rn, di ∈ R. This is different from
conventional discrete action space which can be an arbitrary
unrelated set of actions.

Reward rt. To encourage adding cutting plane aggressively,
we set the instant reward in iteration t to be the gap between
objective values of consecutive LP solutions, that is, rt =
cT x∗
LP(t) ≥ 0. With a discount factor γ < 1,
this encourages the agent to reduce the integrality gap and
approach the integer optimal solution as fast as possible.

LP(t + 1) − cT x∗

Transition. Given state st = {C(t), c, x∗
LP(t), D(t)}, on
taking action at (i.e., on adding a chosen cutting plane
eT
i x ≤ di), the new state st+1 is determined as follows.
Consider the new constraint set C(t+1) = C(t) ∪{eT
i x ≤ di}.
The augmented set of constraints C(t+1) form a new LP,
which can be efﬁciently solved using the simplex method
to get x∗
LP(t + 1). The new set of Gomory’s cutting planes
D(t+1) can then be computed from the simplex tableau.
LP(t + 1), D(t+1)}.
Then, the new state st+1 = {C(t+1), c, x∗

3.2. Policy Network Architecture

We now describe the policy network architecture for
πθ(at|st). Recall from the last section we have in the state
st a set of inequalities C(t) = {aT
i=1, and as avail-
able actions, another set D(t) = {eT
i=1. Given

i x ≤ di}It

i x ≤ bi}Nt

state st, a policy πθ speciﬁes a distribution over D(t), via
the following architecture.

Attention network for order-agnostic cut selection.
Given current LP constraints in C(t), when computing dis-
tributions over the It candidate constraints in D(t), it is
desirable that the architecture is agnostic to the ordering
among the constraints (both in C(t) and D(t)), because
the ordering does not reﬂect the geometry of the feasi-
ble set. To achieve this, we adopt ideas from the atten-
tion network (Vaswani et al., 2017). We use a parametric
function Fθ : Rn+1 (cid:55)→ Rk for some given k (encoded
by a network with parameter θ). This function is used
to compute projections hi = Fθ([ai, bi]), i ∈ [Nt] and
gj = Fθ([ej, dj]), j ∈ [It] for each inequality in C(t) and
D(t), respectively. Here [·, ·] denotes concatenation. The
score Sj for every candidate cut j ∈ [It] is computed as

Sj = 1
Nt

(cid:80)Nt

i=1 gT

j hi

(4)

Intuitively, when assigning these scores to the candidate
cuts, (4) accounts for each candidate’s interaction with all
the constraints already in the LP through the inner products
gT
j hi. We then deﬁne probabilities p1, . . . , pIt by a softmax
function softmax(S1, . . . , SIt). The resulting It-way cate-
gorical distribution is the distribution over actions given by
policy πθ(·|st) in the current state st.

LSTM network for variable sized inputs. We want our
RL agent to be able to handle IP instances of different sizes
(number of decision variables and constraints). Note that
the number of constraints can vary over different iterations
of a cutting plane method even for a ﬁxed IP instance. But
this variation is not a concern since the attention network
described above handles that variability in a natural way. To
be able to use the same policy network for instances with
different number of variables , we embed each constraint us-
ing a LSTM network LSTMθ (Hochreiter and Schmidhuber,
1997) with hidden state of size n + 1 for a ﬁxed n. In partic-
i ˜x ≤ ˜bi with ˜ai ∈ R˜n with
ular, for a general constraint ˜aT
˜n (cid:54)= n, we carry out the embedding ˜hi = LSTMθ([˜ai, ˜bi])
where ˜hi ∈ Rn+1 is the last hidden state of the LSTM net-
work. This hidden state ˜hi can be used in place of [˜ai, ˜bi] in
the attention network. The idea is that the hidden state ˜hi
can properly encode all information in the original inequali-
ties [˜ai, ˜bi] if the LSTM network is powerful enough.

Policy rollout. To put everything together, in Algorithm 1,
we lay out the steps involved in rolling out a policy, i.e.,
executing a policy on a given IP instance.

3.3. Training: Evolutionary Strategies

We train the RL agent using evolution strategies (ES) (Sal-
imans et al., 2017). The core idea is to ﬂatten the RL
problem into a blackbox optimization problem where the
input is a policy parameter θ and the output is a noisy esti-

Reinforcement Learning for Integer Programming: Learning to Cut

Algorithm 1 Rollout of the Policy

1: Input: policy network parameter θ, IP instance parame-

terized by c, A, b, number of iterations T .

2: Initialize iteration counter t = 0.
3: Initialize minimization LP with constraints C(0) =
{Ax ≤ b} and cost vector c. Solve to obtain x∗
LP(0).
Generate set of candidate cuts D(0).

4: while x∗
5:
6:

LP(t) not all integer-valued and t ≤ T do

LP(t), D(t)}.

Construct state st = {C(t), c, x∗
Sample an action using the distribution over candi-
date cuts given by policy πθ, as at ∼ πθ(·|st). Here
the action at corresponds to a cut {eT x ≤ d} ∈ D(t).
Append the cut to the constraint set, C(t+1) = C(t) ∪
LP(t + 1). Generate D(t+1).
{eT x ≤ d}. Solve for x∗
Compute reward rt.
t ← t + 1.

7:

8:
9:
10: end while

mate of the agent’s performance under the corresponding
policy. ES apply random sensing to approximate the policy
gradient ˆgθ ≈ ∇θJ(πθ) and then carry out the iteratively
update θ ← θ + ˆgθ for some α > 0. The gradient estimator
takes the following form

some problems! To remedy this, we have added a simple
stopping criterion at test time. The idea is to maintain a
running statistics that measures the relative progress made
by newly added cuts during execution. When a certain num-
ber of consecutive cuts have little effect on the LP objective
value, we simply terminate the episode. This prevents the
agent from adding cuts that are likely to induce numerical
errors. Indeed, our experiments show this modiﬁcation
is enough to completely remove the generation of invalid
cutting planes. We postpone the details to the appendix.

4. Experiments

We evaluate our approach with a variety of experiments, de-
signed to examine the quality of the cutting planes selected
by RL. Speciﬁcally, we conduct ﬁve sets of experiments to
evaluate our approach from the different aspects:

1. Efﬁciency of cuts. Can the RL agent solve an IP prob-

lem using fewer number of Gomory cuts?

2. Integrality gap closed. In cases where cutting planes
alone are unlikely to solve the problem to optimality, can
the RL agent close the integrality gap effectively?

ˆgθ = 1
N

(cid:80)N

i=1 J(πθ(cid:48)

i

) (cid:15)i
σ ,

(5)

3. Generalization properties.

where (cid:15)i ∼ N (0, I) is a sample from a multivariate Gaus-
sian, θ(cid:48)
i = θ + σ(cid:15)i and σ > 0 is a ﬁxed constant. Here the
return J(πθ(cid:48)) can be estimated as (cid:80)T −1
t=0 rtγt using a single
trajectory (or average over multiple trajectories) generated
on executing the policy πθ(cid:48), as in Algorithm 1. To train the
policy on M distinct IP instances, we average the ES gra-
dient estimators over all instances. Optimizing the policy
with ES comes with several advantages, e.g., simplicity of
communication protocol between workers when compared
to some other actor-learner based distributed algorithms (Es-
peholt et al., 2018; Kapturowski et al., 2018), and simple
parameter updates. Further discussions are in the appendix.

3.4. Testing

We test the performance of a trained policy πθ by rolling out
(as in Algorithm 1) on a set of test instances, and measuring
the IGC. One important design consideration is that a cut-
ting plane method can potentially cut off the optimal integer
solution due to the LP solver’s numerical errors. Invalid cut-
ting planes generated by numerical errors is a well-known
phenomenon in integer programming (Cornuéjols et al.,
2013). Further, learning can amplify this problem. This
is because an RL policy trained to decrease the cost of the
LP solution might learn to aggressively add cuts in order
to tighten the LP constraints. When no countermeasures
were taken, we observed that the RL agent could cut the
optimal solution in as many as 20% of the instances for

• (size) Can an RL agent trained on smaller instances
be applied to 10X larger instances to yield perfor-
mance comparable to an agent trained on the larger
instances?

• (structure) Can an RL agent trained on instances
from one class of IPs be applied to a very different
class of IPs to yield performance comparable to an
agent trained on the latter class?

4. Impact on the efﬁciency of B&C. Will the RL agent
trained as a cutting plane method be effective as a sub-
routine within a B&C method?

5. Interpretability of cuts: the knapsack problem. Does
RL have the potential to provide insights into effective
and meaningful cutting plane strategies for speciﬁc prob-
lems? Speciﬁcally, for the knapsack problem, do the cuts
learned by RL resemble lifted cover inequalities?

IP instances used for training and testing. We consider
four classes of IPs: Packing, Production Planning, Binary
Packing and Max-Cut. These represent a wide collection of
well-studied IPs ranging from resource allocation to graph
optimization. The IP formulations of these problems are
provided in the appendix. Let n, m denote the number of
variables and constraints (other than nonnegativity) in the
IP formulation, so that n × m denotes the size of the IP
instances (see tables below). The mapping from speciﬁc
problem parameters (like number of nodes and edges in

Reinforcement Learning for Integer Programming: Learning to Cut

Table 1: Number of cuts it takes to reach optimality. We show
mean ± std across all test instances.

Table 2: IGC for test instances of size roughly 1000. We show
mean ± std of IGC achieved on adding T = 50 cuts.

Tasks

Packing

Planning

Binary

Max Cut

Tasks Packing

Planning

Binary

Max Cut

Size

10 × 5

13 × 20

10 × 20

10 × 22

Size

30 × 30

61 × 84

33 × 66

27 × 67

RANDOM 48 ± 36
62 ± 40
53 ± 39
34 ± 17
14 ± 11

MV
MNV
LE
RL

44 ± 37
48 ± 29
60 ± 34
310 ± 60
10 ± 12

81 ± 32
87 ± 27
85 ± 29
89 ± 26
22 ± 27

69 ± 34
64 ± 36
47 ± 34
59 ± 35
13 ± 4

maximum-cut) to n, m depends on the IP formulation used
for each problem. We use randomly generated problem
instances for training and testing the RL agent for each IP
problem class. For the small (n × m ≈ 200) and medium
(n × m ≈ 1000) sized problems we used 30 training in-
stances and 20 test instances. These numbers were doubled
for larger problems (n × m ≈ 5000). Importantly, note that
we do not need “solved" (aka labeled) instances for training.
RL only requires repeated rollouts on training instances.

Baselines. We compare the performance of the RL agent
with the following commonly used human-designed heuris-
tics for choosing (Gomory) cuts (Wesselmann and Stuhl,
2012): Random, Max Violation (MV), Max Normalized
Violation (MNV) and Lexicographical Rule (LE), with LE
being the original rule used in Gomory’s method, for which
a theoretical convergence in ﬁnite time is guaranteed. Pre-
cise descriptions of these heuristics are in the appendix.

Implementation details. We implement the MDP simula-
tion environment for RL using Gurobi (Gurobi Optimization,
2015) as the LP solver. The C interface of Gurobi entails
efﬁcient addition of new constraints (i.e., the cut chosen
by RL agent) to the current LP and solve the modiﬁed LP.
The number of cuts added (i.e., the horizon T in rollout of
a policy) depend on the problem size. We sample actions
from the categorical distribution {pi} during training; but
during testing, we take actions greedily as i∗ = arg maxi pi.
Further implementation details, along with hyper-parameter
settings for the RL method are provided in the appendix.

Experiment #1: Efﬁciency of cuts (small-sized in-
stances). For small-sized IP instances, cutting planes
alone can potentially solve an IP problem to optimality. For
such instances, we compare different cutting plane methods
on the total number of cuts it takes to ﬁnd an optimal integer
solution. Table 1 shows that the RL agent achieves close
to several factors of improvement in the number of cuts
required, when compared to the baselines. Here, for each
class of IP problems, the second row of the table gives the
size of the IP formulation of the instances used for training
and testing.

RAND 0.18±0.17
MV 0.14±0.08
MNV 0.19±0.23
0.20±0.22
LE
0.55 ± 0.32 0.88 ± 0.12 0.95 ± 0.14 0.86 ± 0.14
RL

0.39±0.21
0.32±0.18
0.32±0.24
0.41±0.27

0.56±0.16
0.18±0.08
0.31±0.09
0.01±0.01

0.56±0.09
0.55±0.10
0.62±0.12
0.54±0.15

(a) Packing

(b) Planning

(c) Binary Packing

(d) Max Cut

Figure 2: Percentile plots of IGC for test instances of size roughly
1000. X-axis shows the percentile of instances and y-axis shows
the IGC achieved on adding T = 50 cuts. Across all test instances,
RL achieves signiﬁcantly higher IGC than the baselines.

Experiment #2: Integrality gap closure for large-sized
instances. Next, we train and test the RL agent on signif-
icantly larger problem instances compared to the previous
experiment. In the ﬁrst set of experiments (Table 2 and Fig-
ure 2), we consider instances of size (n × m) close to 1000.
In Table 3 and Figure 3, we report results for even larger
scale problems, with instances of size close to 5000. We
add T = 50 cuts for the ﬁrst set of instances, and T = 250
cuts for the second set of instances. However, for these
instances, the cutting plane methods is unable to reach op-
timality. Therefore, we compare different cutting plane
methods on integrality gap closed using the IGC metric de-
ﬁned in (2), Section 2. Table 2, 3 show that on average RL
agent was able to close a signiﬁcantly higher fraction of gap
compared to the other methods. Figure 2, 3 provide a more
detailed comparison, by showing a percentile plot – here the
instances are sorted in the ascending order of IGC and then

Reinforcement Learning for Integer Programming: Learning to Cut

Table 3: IGC for test instances of size roughly 5000. We show
mean ± std of IGC achieved on adding T = 250 cuts.

Tasks

Packing

Planning

Binary

Max Cut

Size

60 × 60

121 × 168

66 × 132

54 × 134

RANDOM 0.05±0.03
0.04±0.02
0.05±0.03
0.04±0.02
0.11 ± 0.05 0.68 ± 0.10 0.61 ± 0.35 0.57 ± 0.10

0.17±0.12
0.19±0.18
0.19±0.18
0.23±0.20

0.50±0.10
0.50±0.06
0.56±0.11
0.45±0.08

0.38±0.08
0.07±0.03
0.17±0.10
0.01±0.01

MV
MNV
LE
RL

(a) Packing

(b) Planning

Figure 4: Percentile plots of Integrality Gap Closure. ‘RL/10X
packing’ trained on instances of a completely different IP problem
(packing) performs competitively on the maximum-cut instances.

Table 4: IGC in B&C. We show mean ± std across test instances.

Tasks

Packing

Planning

Binary

Max Cut

Size

NO
CUT

30 × 30

61 × 84

33 × 66

27 × 67

0.57±0.34

0.35±0.08

0.60±0.24

1.0 ± 0.0

RANDOM 0.79±0.25
0.88±0.16
0.67±0.38
0.64±0.27
0.83±0.23
0.74±0.22
0.35±0.08
0.80±0.26
0.88 ± 0.23 1.0 ± 0.0

MV
MNV
LE
RL

0.97±0.09
0.97±0.09
1.0 ± 0.0
0.97±0.08
1.0 ± 0.0

1.0 ± 0.0
0.97±0.18
1.0 ± 0.0
1.0 ± 0.0
1.0 ± 0.0

(c) Binary Packing

(d) Max Cut

Figure 3: Percentile plots of IGC for test instances of size roughly
5000, T = 250 cuts. Same set up as Figure 2 but on even larger-
size instances.

plotted in order; the y-axis shows the IGC and the x-axis
shows the percentile of instances achieving that IGC. The
blue curve with square markers shows the performance of
our RL agent. In Figure 2, very close to the blue curve is the
yellow curve (also with square marker). This yellow curve
is for RL/10X, which is an RL agent trained on 10X smaller
instances in order to evaluate generalization properties, as
we describe next.

Experiment #3: Generalization.
In Figure 2, we also
demonstrate the ability of the RL agent to generalize across
different sizes of the IP instances. This is illustrated through
the extremely competitive performance of the RL/10X agent,
which is trained on 10X smaller size instances than the test
instances. (Exact sizes used in the training of RL/10X agent
were were 10 × 10, 32 × 22, 10 × 20, 20 × 10, respectively,
for the four types of IP problems.) Furthermore, we test
generalizability across IP classes by training an RL agent on

(a) Packing (1000 nodes)

(b) Planning (200 node)

(c) Binary (200 nodes)

(d) Max Cut (200 nodes)

Figure 5: Percentile plots of number of B&C nodes expanded. X-
axis shows the percentile of instances and y-axis shows the number
of expanded nodes to close 95% of the integrality gap.

small sized instances of the packing problem, and applying
it to add cuts to 10X larger instances of the maximum-cut
problem. The latter, being a graph optimization problem,
has intuitively a very different structure from the former.
Figure 4 shows that the RL/10X agent trained on packing
(yellow curve) achieve a performance on larger maximum-
cut instances that is comparable to the performance of agent

Reinforcement Learning for Integer Programming: Learning to Cut

(a) Criterion 1

(b) Criterion 2

(c) Criterion 3

(d) Number of cuts

Figure 6: Percentage of cuts meeting the designed criteria and number of cuts on Knapsack problems. We train the RL agent on 80
instances. All baselines are tested on 20 instances. As seen above, RL produces consistently more ’high-quality’ cuts.

trained on the latter class (blue curve).

Experiment #4: Impact on the efﬁciency of B&C.
In
practice, cutting planes alone are not sufﬁcient to solve large
problems. In state-of-the-art solvers, the iterative addition
of cutting planes is alternated with a branching procedure,
leading to Branch-and-Cut (B&C). To demonstrate the full
potential of RL, we implement a comprehensive B&C pro-
cedure but without all the additional heuristics that appear
in the standard solvers. Our B&C procedure has two hyper-
parameters: number of child nodes (suproblems) to expand
Nexp and number of cutting planes added to each node Ncuts.
In addition, B&C is determined by the implementation of
the Branching Rule, Priority Queue and Termination Condi-
tion. Further details are in the appendix.

Figure 5 gives percentile plots for the number of child
nodes (suproblems) Nexp until termination of B&C. Here,
Ncuts = 10 cuts were added to each node, using either RL or
one of the baseline heuristics. We also include as a compara-
tor, the B&C method without any cuts, i.e., the branch and
bound method. The trained RL agent and the test instances
used here are same as those in Table 2 and Figure 2. Perhaps
surprisingly, the RL agent, though not designed to be used
in combination with branching, shows substantial improve-
ments in the efﬁciency of B&C. In the appendix, we have
also included experimental results showing improvements
for the instances used in Table 3 and Figure 3.

Experiment #5: Interpretability of cuts. A knapsack
problem is a binary packing problem with only one con-
straint. Although simple to state, these problems are NP-
Hard, and have been a testbed for many algorithmic tech-
niques, see e.g. the books (Kellerer et al., 2003; Martello and
Toth, 1990). A prominent class of valid inequalities for knap-
sack is that of cover inequalities, that can be strengthened
through the classical lifting operation (see the appendix for
deﬁnitions). Those inequalities are well-studied in theory
and also known to be effective in practice, see e.g. (Crow-
der et al., 1983; Conforti et al., 2014; Kellerer et al., 2003).

Our last set of experiments gives a “reinforcement learning
validation” of those cuts. We show in fact that RL, with the
same reward scheme as in Experiment #2, produces many
more cuts that “almost look like” lifted cover inequalities
than the baselines. More precisely, we deﬁne three increas-
ingly looser criteria for deciding when a cut is “close” to
a lifted cover inequality (the plurality of criteria is due to
the fact that lifted cover inequalities can be strengthened
in many ways). We then check which percentage of the
inequalities produced by the RL (resp. the other baselines)
satisfy each of these criteria. This is reported in the ﬁrst
three ﬁgures in Figure 6, together with the number of cuts
added before the optimal solution is reached (rightmost ﬁg-
ure in Figure 6). More details on the experiments and a
description of the three criteria are reported in the appendix.
These experiments suggest that our approach could be use-
ful to aid researchers in the discovery of strong family of
cuts for IPs, and provide yet another empirical evaluation of
known ones.

Runtime. A legitimate question is whether the improve-
ment provided by the RL agent in terms of solution accuracy
comes at the cost of a large runtime. The training time for
RL can indeed be signiﬁcant, especially when trained on
large instances. However, there is no way to compare that
with the human-designed heuristics. In testing, we observe
no signiﬁcant differences in time required by an RL policy
to choose a cut vs. time taken to execute a heuristic rule. We
detail the runtime comparison in the appendix.

5. Conclusions

We presented a deep RL approach to automatically learn an
effective cutting plane strategy for solving IP instances. The
RL algorithm learns by trying to solve a pool of (randomly
generated) training instances again and again, without hav-
ing access to any solved instances. The variety of tasks
across which the RL agent is demonstrated to generalize
without being trained for, provides evidence that it is able

Reinforcement Learning for Integer Programming: Learning to Cut

to learn an intelligent algorithm for selecting cutting planes.
We believe our empirical results are a convincing step for-
ward towards the integration of ML techniques in IP solvers.
This may lead to a functional answer to the “Hamletic ques-
tion Branch-and-cut designers often have to face: to cut or
not to cut?” (Dey and Molinaro, 2018).

Acknowledgements. Author Shipra Agrawal acknowl-
edges support from an Amazon Faculty Research Award.

References

Balas, E., Ceria, S., and Cornuéjols, G. (1993). A lift-and-
project cutting plane algorithm for mixed 0–1 programs.
Mathematical programming, 58(1-3):295–324.

Balcan, M.-F., Dick, T., Sandholm, T., and Vitercik,
arXiv preprint
Learning to branch.

E. (2018).
arXiv:1803.10150.

Bello, I., Pham, H., Le, Q. V., Norouzi, M., and Bengio, S.

(2017). Neural combinatorial optimization.

Bengio, Y., Lodi, A., and Prouvost, A. (2018). Machine
learning for combinatorial optimization: a methodologi-
cal tour d’horizon. arXiv preprint arXiv:1811.06128.

Bixby, B. (2017). Optimization: past, present, future. Ple-

nary talk at INFORMS Annual Meeting.

Conforti, M., Cornuéjols, G., and Zambelli, G. (2014). Inte-

ger programming, volume 271. Springer.

Cornuéjols, G., Margot, F., and Nannicini, G. (2013). On
the safety of gomory cut generators. Math. Program.
Comput., 5(4):345–395.

Crowder, H., Johnson, E. L., and Padberg, M. (1983). Solv-
ing large-scale zero-one linear programming problems.
Operations Research, 31(5):803–834.

Dai, H., Khalil, E. B., Zhang, Y., Dilkina, B., and Song, L.
(2017). Learning combinatorial optimization algorithms
over graphs. arXiv preprint arXiv:1704.01665.

Dantzig, G., Fulkerson, R., and Johnson, S. (1954). Solution
of a large-scale traveling-salesman problem. Journal of
the operations research society of America, 2(4):393–
410.

Dey, S. S. and Molinaro, M. (2018). Theoretical challenges
towards cutting-plane selection. Mathematical Program-
ming, 170(1):237–266.

Espeholt, L., Soyer, H., Munos, R., Simonyan, K., Mnih, V.,
Ward, T., Doron, Y., Firoiu, V., Harley, T., Dunning, I.,
et al. (2018). Impala: Scalable distributed deep-rl with
importance weighted actor-learner architectures. arXiv
preprint arXiv:1802.01561.

Gomory, R. (1960). An algorithm for the mixed integer
problem. Technical report, RAND CORP SANTA MON-
ICA CA.

Gurobi Optimization, I. (2015). Gurobi optimizer reference

manual. URL http://www. gurobi. com.

Hochreiter, S. and Schmidhuber, J. (1997). Long short-term

memory. Neural computation, 9(8):1735–1780.

Kapturowski, S., Ostrovski, G., Quan, J., Munos, R., and
Dabney, W. (2018). Recurrent experience replay in dis-
tributed reinforcement learning.

Kellerer, H., Pferschy, U., and Pisinger, D. (2003). Knap-

sack problems. 2004.

Khalil, E., Dai, H., Zhang, Y., Dilkina, B., and Song, L.
(2017). Learning combinatorial optimization algorithms
over graphs. In Advances in Neural Information Process-
ing Systems, pages 6348–6358.

Khalil, E. B., Le Bodic, P., Song, L., Nemhauser, G. L.,
and Dilkina, B. N. (2016). Learning to branch in mixed
integer programming. In AAAI, pages 724–731.

Kingma, D. P. and Ba, J. (2014). Adam: A method for
stochastic optimization. arXiv preprint arXiv:1412.6980.

Kool, W. and Welling, M. (2018). Attention solves your tsp.

arXiv preprint arXiv:1803.08475.

Li, Z., Chen, Q., and Koltun, V. (2018). Combinatorial opti-
mization with graph convolutional networks and guided
tree search. In Advances in Neural Information Process-
ing Systems, pages 539–548.

Martello, S. and Toth, P. (1990). Knapsack problems: algo-
rithms and computer implementations. Wiley-Interscience
series in discrete mathematics and optimiza tion.

Mnih, V., Badia, A. P., Mirza, M., Graves, A., Lillicrap,
T., Harley, T., Silver, D., and Kavukcuoglu, K. (2016).
Asynchronous methods for deep reinforcement learning.
In International conference on machine learning, pages
1928–1937.

Nowak, A., Villar, S., Bandeira, A. S., and Bruna, J.
(2017). A note on learning algorithms for quadratic as-
signment with graph neural networks. arXiv preprint
arXiv:1706.07450.

Pochet, Y. and Wolsey, L. A. (2006). Production planning by
mixed integer programming. Springer Science & Business
Media.

Rothvoß, T. and Sanità, L. (2017). 0/1 polytopes with
quadratic chvátal rank. Operations Research, 65(1):212–
220.

Reinforcement Learning for Integer Programming: Learning to Cut

Salimans, T., Ho, J., Chen, X., Sidor, S., and Sutskever, I.
(2017). Evolution strategies as a scalable alternative to re-
inforcement learning. arXiv preprint arXiv:1703.03864.

Sutskever, I., Vinyals, O., and Le, Q. V. (2014). Sequence to
sequence learning with neural networks. In Advances in
neural information processing systems, pages 3104–3112.

Tokui, S., Oono, K., Hido, S., and Clayton, J. (2015).
Chainer: a next-generation open source framework for
deep learning. In Proceedings of workshop on machine
learning systems (LearningSys) in the twenty-ninth an-
nual conference on neural information processing sys-
tems (NIPS), volume 5, pages 1–6.

Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones,
L., Gomez, A. N., Kaiser, Ł., and Polosukhin, I. (2017).
Attention is all you need. In Advances in neural informa-
tion processing systems, pages 5998–6008.

Vinyals, O., Fortunato, M., and Jaitly, N. (2015). Pointer
networks. In Advances in Neural Information Processing
Systems, pages 2692–2700.

Wesselmann, F. and Stuhl, U. (2012). Implementing cutting
plane management and selection techniques. Technical
report, Tech. rep., University of Paderborn.

Reinforcement Learning for Integer Programming: Learning to Cut

A. Experiment Details

A.1. Projection into the original variable space

In the following we look at only the ﬁrst iteration of the cutting plane procedure, and we drop the iteration index t. Recall
the LP relaxation of the original IP problem (1), where A ∈ Qm×n, b ∈ Qm:






min cT x
Ax ≤ b
x ≥ 0.

When a simplex algorithm solves the LP, the original LP is ﬁrst converted to a standard form where all inequalities are
transformed into equalities by introducing slack variables.






min cT x
Ax + Is = b
x ≥ 0, s ≥ 0,

(6)

where I is an identity matrix and s is the set of slack variables. The simplex method carries out iteratively operations on
the tableau formed by [A, I], b and c. At convergence, the simplex method returns a ﬁnal optimal tableau. We generate a
Gomory’s cut using the row of the tableau that corresponds to a fractional variable of the optimal solution x∗
LP. This will in
general create a cutting plane of the following form

where e, x ∈ Rn, r, s ∈ Rm and d ∈ R. Though this cutting plane involves slack variables, we can get rid of the slack
variables by multiplying both sides of the linear constraints in (6) by r

eT x + rT s ≤ d

(7)

and subtract the new cutting plane (7) by the above. This leads to an equivalent cutting plane

(eT − rT A)x ≤ d − rT b.

rT Ax + rT s = rT b

(8)

(9)

Note that this cutting plane only contains variables in the original variable space. For a downstream neural network that
takes in the parameters of the cutting planes as inputs, we ﬁnd it helpful to remove such slack variables. Slack variables do
not contribute to new information regarding the polytope and we can also parameterize a network with a smaller number of
parameters.

A.2. Integer programming formulations of benchmark problems

A wide range of benchmark instances can be cast into special cases of IP problems. We provide their speciﬁc formulations
below. For simplicity, we only provide their general IP formulations (with ≤, ≥, = constraints). It is always possible
to convert original formulations into the standard formulation (1) with properly chosen A, b, c, x. Some problems are
formulated within a graph G = (V, E) with nodes v ∈ V and edges (v, u) ∈ E.

Their formulations are as follows:

Max Cut. We have one variable per edge yu,v, (u, v) ∈ E and one variable per node xu, u ∈ V . Let wu,v ≥ 0 be a set of
non-negative weights per edge.






max (cid:80)
(u,v)∈E wuvyuv
yuv ≤ xu + xv, ∀(u, v) ∈ E
yuv ≤ 2 − xu − xv, ∀(u, v) ∈ E
0 ≤ x, y ≤ 1
xu, yuv ∈ Z ∀u ∈ V, (u, v) ∈ E.

(10)

In our experiments the graphs are randomly generated. To be speciﬁc, we specify a vertex size |V | and edge size |E|. We
then sample |E| edges from all the possible |V | · (|V | − 1)/2 edges to form the ﬁnal graph. The weights wuv are uniformly
sampled as an integer from 0 to 10. When generating the instances, we sample graphs such that |V |, |E| are of a particular
size. For example, for middle size problem we set |V | = 7, |E| = 20.

Reinforcement Learning for Integer Programming: Learning to Cut

Packing. The packing problem takes the generic form of (1) while requiring that all the coefﬁcients of A, b, c be non-
negative, in order to enforce proper resource constraints.

Here the constraint coefﬁcients aij for the jth variable and ith constraint is sampled as an integer uniformly from 0 and 5.
Then the RHS coefﬁcient bi is sampled from 9n to 10n uniformly as an integer where n is the number of variables. Each
component of cj is uniformly sampled as an integer from 1 to 10.

Binary Packing. Binary packing augments the original packing problem by a set of binary constraints on each variable
xi ≤ 1.

Here the constraint coefﬁcients aij for the jth variable and ith constraint is sampled as an integer uniformly from 5 and 30.
Then the RHS coefﬁcient bi is sampled from 10n to 20n uniformly as an integer where n is the number of variables. Each
component of cj is uniformly sampled as an integer from 1 to 10.

Production Planning. Consider a production planning problem (Pochet and Wolsey, 2006) with time horizon T . The
decision variables are production xi, 1 ≤ i ≤ T , along with by produce / not produce variables yi, 1 ≤ i ≤ T and storage
variables si, 0 ≤ i ≤ T . Costs p(cid:48)
i, qi and demands di are given as problem parameters. The LP formulation is as follows

i, h(cid:48)






isi + (cid:80)T

i=0 qiyi

i=1 p(cid:48)

i=0 h(cid:48)

min (cid:80)T
ixi + (cid:80)T
si−1 + xi = di + si, ∀1 ≤ i ≤ T
xi ≤ M yi, ∀1 ≤ i ≤ T
s ≥ 0, x ≥ 0, 0 ≤ y ≤ 1
s0 = s∗
0, sT = s∗
T
x, s, y ∈ ZT ,

(11)

where M is a positive large number and s∗

0, s∗

T are also given.

The instance parameters are the initial storage s∗
i, h(cid:48)
p(cid:48)

i, qi are generated uniformly random as integers from 1 to 10.

0 = 0, ﬁnal storage s∗

T = 20 and big M = 100. The revenue parameter

In our results, we describe the sizes of the IP instances as n × m where n is the number of
Size of IP formulations.
columns and m is the number of rows of the constraint matrix A from the LR of (1). For a packing problem with n items
and m resource constraints, the IP formulation has n variables and m constraints; for planning with period K, n = 3K + 1,
m = 4K + 1; for binary packing, there are n extra binary constraints compared to the packing problem; for max-cut, the
problem is deﬁned on a graph with a vertex set V and an edge set V , and its IP formulation consists of n = |V | + |E|
variables and m = 3|E| + |V | constraints.

A.3. Criteria for selecting Gomory cuts

Recall that It is the number of candidate Gomory cuts available in round t, and it denotes the index of cut chosen by a given
baseline. The baseline heuristics we use are the following:

• Random. One cut it ∼ Uniform{1, 2...It} is chosen uniformly at random from all the candidate cuts.

• Max Violation (MV). Let x∗

B(t) be the basic feasible solution of the curent LP relaxation. MV selects the cut that

corresponds to the most fractional component, i.e. it = arg max{|[x∗

B(t)]i − round([x∗

B(t)]i)|}.

• Max Normalized Violation (MNV). Recall that ˜A denotes the optimal tableau obtained by the simplex algorithm upon
B(t)]i)|/(cid:107) ˜Ai(cid:107)}.

convergence. Let ˜Ai be the ith row of ˜A. Then, MNV selects cut it = arg max{|[x∗

B(t)]i − round([x∗

• Lexicographic (LE): Add the cutting plane with the least index, i.e. it = arg min{i, [x∗

B(t)]i is fractional}.

The ﬁrst three rules are common in the IP literature, see e.g. (Wesselmann and Stuhl, 2012), while the fourth is the original
rule used by Gomory to prove the convergence of his method (Gomory, 1960).

Reinforcement Learning for Integer Programming: Learning to Cut

A.4. Hyper-parameters

Policy architecture. The policy network is implemented with Chainer (Tokui et al., 2015). The attention embedding Fθ
is a 2-layer neural network with 64 units per layer and tanh activation. The LSTM network encodes variable sized inputs
into hidden vector with dimension 10.

During a forward pass, a LSTM + Attention policy will take the instance, carry out embedding into a n-d vector and then
apply attention. Such architecture allows for generalization to variable sized instances (different number of variables). We
apply such architecture in the generalization part of the experiments.

On the other hand, a policy network can also consist of a single attention network. This policy can only process IP instances
of a ﬁxed size (ﬁxed number of varibles) and cannot generalize to other sizes. We apply such architecture in the IGC part of
the experiments.

ES optimization. Across all experiments, we apply Adam optimizer (Kingma and Ba, 2014) with learning rate α = 0.01
to optimize the policy network. The perturbation standard deviation σ is selected from {0.002, 0.02, 0.2}. By default, we
apply N = 10 perturbations to construct the policy gradient for each iteration, though we ﬁnd that N = 1 could also work
as well. For all problem types except planning, we ﬁnd that σ = 0.2 generally works properly except for planning, where
we apply σ = 0.02 and generate N = 5 trajectory per instance per iteration. Empirically, we observe that the training is
stable for both policy architectures and the training performance converges in ≤ 500 weight updates.

Distributed setup. For training, we use a Linux machine with 60 virtual CPUs. To fully utilize the compute power of the
machine, the trajectory collection is distributed across multiple workers, which run in parallel.

B. Branch-and-Cut Details

As mentioned in the introduction, Branch-and-Cut (B&C) is an algorithmic procedure used for solving IP problems. The
choice of which variable to branch on, as well as which node of the branching tree to explore next, is the subject of much
research. In our experiments, we implemented a B&C with very simple rules, as explained below. This is motivated by the
fact that our goal is to evaluate the quality of the cutting planes added by the RL rather than obtaining a fast B&C method.
Hence, sophisticated and computationally expensive branching rules could have overshadowed the impact of cutting planes.
Instead, simple rules (applied both to the RL and to the other techniques) highlight the impact of cutting planes for this
important downstream application.

We list next several critical elements of our implementation of B&C.

Branching rule. At each node, we branch on the most fractional variable of the corresponding LP optimal solution (0.5
being the most fractional).

Priority queue. We adopt a FIFO queue (Breath ﬁrst search). FIFO queue allows the B&C procedure to improve the
lower bound.

Termination condition. Let z0 = cT x∗
ﬁnds an increasing set of feasible integer solutions XF , and an upper bound on the optimal objective z∗ = cT x∗
zupper = minx∈XF cT x. Hence, zupper monotonically decreases.
Along with B&C, cutting planes can iteratively improve the lower bound zlower of the optimal objective z∗. Let zi be the
objective of the LP solution at node i and denote N as the set of unpruned nodes with unexpanded child nodes. The lower
bound is computed as zlower = mini∈N zi and monotonically increases as the B&C procedure proceeds.

LP(0) be the objective of the initial LP relaxation. As B&C proceeds, the procedure
IP is

This produces a ratio statistic

r =

zupper − zlower
zupper − z∗
LP

> 0

Note that since zlower ≥ z∗
The B&C terminates when r is below some threshold which we set to be 0.0001.

LP, zlower monotonically increases, and zupper monotonically decreases, r monotonically decreases.

Reinforcement Learning for Integer Programming: Learning to Cut

C. Test Time Considerations

Stopping criterion. Though at training time we guide the agent to generate aggressive cuts that tighten the LP relaxation
as much as possible, the agent can exploit the defects in the simulation environment - numerical errors, and generate invalid
cuts which cut off the optimal solution.

This is undesirable in practice. In certain cases at test time, when we execute the trained policy, we adopt a stopping criterion
which automatically determines if the agent should stop adding cuts, in order to prevent from invalid cuts. In particular, at
each iteration let rt = |cT xLP∗(t) − cT xLP∗(t+1)| be the objective gap achieved by adding the most recent cut. We maintain
a cumulative ratio statistics such that

st =

rt
t(cid:48)≤t rt

(cid:80)

.

We terminate the cutting plane procedure once the average st over a ﬁxed window of size H is lower than certain threshold
η. In practice, we set H = 5, η = 0.001 and ﬁnd this work effectively for all problems, eliminating all the numerical errors
observed in reported tasks. Intuitively, this approach dictates that we terminate the cutting plane procedure once the newly
added cuts do not generate signiﬁcant improvements for a period of H steps.

To analyze the effect of η and H, we note that when H is too small or η is too large, we have very conservative cutting plane
procedure. On the other hand when H is large while η is small, the cutting plane procedure becomes more aggressive.

Greedy action. The policy network deﬁnes a stochastic policy, i.e. a categorical distribution over candidate cuts. At test
time, we ﬁnd taking the greedy action i∗ = arg max pi to be more effective in certain cases, where pi is the categorical
distribution over candidate cuts. The justiﬁcation for this practice is that: the ES optimization procedure can be interpreted
as searching for a parameter θ such that the induced distribution over trajectories has large concentration on those high
return trajectories. Given a trained model, to decode the most likely trajectory of horizon T generated by the policy, we need
to run a full tree search of depth T , which is infeasible in practice. Taking the greedy action is equivalent to applying a
greedy strategy in decoding the most likely trajectory.

This approach is highly related to beam search in sequence modeling (Sutskever et al., 2014) where the goal is to decode the
prediction that the model assigns the most likelihood to. The greedy action selection above corresponds to a beam search
with 1-step lookahead.

D. Details on the Interpretation of Cutts

One interesting aspect of studying the RL approach to generating cuts, is to investigate if we can interpret cuts generated by
RL. For a particular class of IP problems, certain cuts might be considered as generally ’better’ than other cuts. For example,
these cuts might be more effective in terms of closing the objective gap, according to domain knowledge studied in prior
literature. Ideally, we would like to ﬁnd out what RL has learned, whether it has learned to select these more ’effective’ cuts
with features identiﬁed by prior works. Here, we focus on Knapsack problems.

Problem instances. Consider the knapsack problems
max (cid:80)n
(cid:80)n
xi ∈ {0, 1},






i cixi

i=1 aixi ≤ β := (cid:80)n

i=1 ai/2

(12)

where ai are generated independently and uniformly in [1, 30] as integers, and the ci are generated independently and
uniformly in [1, 10]. We consider n = 10 in our experiments. Knapsack problems are fundamental in IP, see e.g. (Kellerer
et al., 2003). The intuition of the problem is that we attempt to pack as many items as possible into the knapsack, as to
maximize the proﬁt of the selected items. Polytopes as (12) are also used to prove strong (i.e., quadratic) lower bounds on
the Chvátal-Gomory rank of polytopes with 0/1 vertices (Rothvoß and Sanità, 2017).

Evaluation scores. For knapsack problems, one effective class of cuts is given by cover inequalities, and their strengthen-
ing through lifting (Conforti et al., 2014; Kellerer et al., 2003). The cover inequality associated to a set S ⊆ {1, . . . , n} with
(cid:80)

i∈S ai > β and |S| = k is given by

(cid:88)

i∈S

xi ≤ k − 1.

Reinforcement Learning for Integer Programming: Learning to Cut

Note that cover inequalities are valid for (12). The inequality can be strengthened (while maintaining validity) by replacing
the 0 coefﬁcients of variables xi for i ∈ {1, . . . , n} \ S with appropriate positive coefﬁcients, leading to the lifted cover
inequality below:

(cid:88)

(cid:88)

xi +

αixi ≤ k − 1.

(13)

with all αi ≥ 0. There are in general exponentially many ways to generate lifted cover inequalities from a single cover
inequality. In practice, further strengthenings are possible, for instance, by perturbing the right-hand side or the coefﬁcients
of xi for i ∈ S. We provide three criteria for identifying (strengthening of) lifted cover inequalities, each capturing certain
features of the inequalities (below, RHS denotes the right-hand side of a given inequality).

i∈S

/∈S

1. There exists an integer p such that (1) the RHS is an integer multiple of p and (2) p times (number of variable with

coefﬁcient exactly p) > RHS.

Criterion 1 is satisﬁed by all lifted cover inequalities as in (13). The scaling by p is due to the fact that an inequality may be
scaled by a positive factor, without changing the set of points satisfying it.

2. There exists an integer p such that (1) holds and (2’) p times (number of variables with coefﬁcients between p and

p + 2) > RHS.

3. There exists an integer p such that (1) holds and (2”) p times (number of variables with coefﬁcients at least p) > RHS.

A lifted cover inequality can often by strengthened by increasing the coefﬁcients of variables in S, after the lifting has been
performed. We capture this by criteria 2 and 3 above, where 2 is a stricter criterion, as we only allow those variables to have
their coefﬁcients increased by a small amount.

For each cut cj generated by the baseline (e.g. RL), we evaluate if this cut satisﬁes the aforementioned conditions. For one
particular condition, if satisﬁed, the cut is given a score s(cj) = 1 or else s(cj) = 0. On any particular instance, the overall
score is computed as an average across the m cuts that are generated to solve the problem with the cutting plane method:

1
m

m
(cid:88)

j=1

s(cj) ∈ [0, 1].

Evaluation setup. We train a RL agent on 100 knapsack instances and evaluate the scores on another independently
generated set of 20 instances. Please see the main text for the evaluation results.

E. Additional results on Large-scale Instances

We provide additional results on large-scale instances in Figure 7, in the context of B&C. Experimental setups and details
are similar to those of the main text: we set the threshold limit to be 1000 nodes for all problem classes. The results show
the percentile plots of the number of nodes required to achieve a certain level of IGC during the B&C with the use of cutting
plane heuristics, where the percentile is calculated across instances. Baseline results for each baseline are shown via curves
in different colors. When certain curves do not show up in the plot, this implies that these heuristics do not achieve the
speciﬁed level of IGC within the node budgets. The IGC level is set to be 95% as in the main text, except for the random
packing problem where it is set to be 25%.

The IGC of the random packing is set at a relatively low level because random packing problems are signiﬁcantly more
difﬁcult to solve when instances are large-scaled. This is consistent with the observations in the main text.

Overall, we ﬁnd that the performance of RL agent signiﬁcantly exceeds that of the other baseline heuristics. For example,
on the planning problem, other heuristics barely achieve the IGC within the node budgets. There are also cases where RL
does similarly to certain heuristics, such as to MNV on the Max Cut problems.

F. Comparison of Distributed Agent Interface

To scale RL training to powerful computational architecture, it is imperative that the agent becomes distributed. Indeed,
recent years have witnessed an increasing attention on the design and implementation of distributed algorithms (Mnih et al.,

Reinforcement Learning for Integer Programming: Learning to Cut

(a) Packing (25% IGC, 1000 nodes)

(b) Planning (95% IGC, 1000 nodes)

(c) Binary Packing (95% IGC, 1000 nodes)

(d) Max Cut (95% IGC, 1000 nodes)

Figure 7: Percentile plots of number of B&C nodes expanded for large-scale instances. The same setup as Figure 5 but for even larger
instances.

2016; Salimans et al., 2017; Espeholt et al., 2018; Kapturowski et al., 2018).

General distributed algorithms adopt a learner-actor architecture, i.e. one central learner and multiple distributed actors.
Actors collect data and send partial trajectories to the learner. The learner takes data from all actors and generates updates to
the central parameter. The general interface requires a function πθ(a|s) parameterized by θ, which takes a state s and outputs
an action a (or a distribution over actions). In a gradient-based algorithm (e.g. (Espeholt et al., 2018)), the actor executes
such an interface with the forward mode and generates trajectory tuple (s, a); the learner executes the interface with the
backward mode to compute gradients and update θ. Below we list several practical considerations why ES is a potentially
better distributed alternative to such gradient-based distributed algorithms in this speciﬁc context, where state/action spaces
are irregular.

• Communication. The data communication between learner-actor is more complex for general gradient-based algo-
rithms. Indeed, actors need to send partial trajectories {(si, ai, ri)}τ
i=1 to the learner, which requires careful adaptations
to cases where the state/action space are irregular. On the other hand, ES only require sending returns over trajectories
(cid:80)T

i=0 ri, which greatly simpliﬁes the interface from an engineering perspective.

• Updates. Gradient-based updates require both forward/backward mode of the agent interface. Further, the backward
mode function needs to be updated such that batched processing is efﬁcient to allow for fast updates. For irregular
state/action space, this requires heavier engineering because of e.g. arrays of variable sizes are not straightforward to be

Reinforcement Learning for Integer Programming: Learning to Cut

Table 5: IGC in B&C with large-scale instances. We adopt the same setup as Table 2

Tasks

Size

NO CUT
RANDOM
MV
MNV
LE
RL

Packing

60 × 60

0.26 ± 0.09
0.31 ± 0.08
0.23 ± 0.08
0.27 ± 0.08
0.28 ± 0.08
0.36 ± 0.10

Planning

122 × 168

0.25 ± 0.04
0.65 ± 0.10
0.27 ± 0.07
0.33 ± 0.15
0.25 ± 0.04
0.99 ± 0.02

Binary

66 × 132

0.74 ± 0.24
0.94 ± 0.10
0.92 ± 0.12
0.93 ± 0.10
0.95 ± 0.08
0.96 ± 0.08

Max Cut

54 × 134

0.95 ± 0.09
0.99 ± 0.04
0.98 ± 0.06
1.0 ± 0.0
0.95 ± 0.09
1.0 ± 0.0

batched. On the other hand, ES only requires forward mode computations required by CPU actors.

G. Considerations on CPU Runtime

In practice, instead of the number of cuts, a more meaningful budget constraint on solvers is the CPU runtime, i.e.
practitioners typically set a runtime constraint on the solver and expect the solver to return the best possible solution within
this constraint. Below, we report runtime results for training/test time. We will show that even under runtime constraints, the
RL policy achieves signiﬁcant performance gains.

Training time. During training time, it is not straightforward to explicitly maintain a constraint on the runtime, because it
is very sensitive to hardware conditions (e.g. number of available processors). Indeed, prior works (Khalil et al., 2016; Dai
et al., 2017) do not apply runtime constraint during training time, though runtime constraint is an important measure at test
time.

The absolute training time depends on speciﬁc hardware architecture. In our experiments we train with a single server with
64 virtual CPUs. Recall that each update consists in collecting trajectories across training instances and generating one
single gradient update. We observe that typically the convergence takes place in ≤ 500 weight updates (iterations).

Test time. To account for the practical effect of runtime, we need to account for the following trade-off: though RL based
policy produces higher-quality cutting planes in general, running the policy at test time could be costly. To characterize the
trade-offs, we address the following question: (1) When adding a ﬁxed number of cuts, does RL lead to higher runtime? (2)
When solving a particular problem, does RL lead to performance gains in terms of runtime?

To address (1), we reuse the experiments in Experiment #2, i.e. adding a ﬁxed number of cuts T = 50 on middle sized
problems. The runtime results are presented in Table 6, where we show that RL cutting plane selection does not increase the
runtime signiﬁcantly compared to other ’fast’ heuristics. Indeed, RL increases the average runtime in some cases while
decreases in others. Intuitively, we expect the runtime gains to come from the fact that RL requires a smaller number of
cuts - leading to fewer iterations of the algorithm. However, this is rare in Experiment #2, where for most instances optimal
solution is not reached in maximum number of cuts, so all heuristics and RL add same number of cuts (T = 50). We expect
such advantages to become more signiﬁcant with the increase of the size of the problem, as the computational gain of adding
good cuts becomes more relevant. We conﬁrm such intuitions from the following.

To address (2), we reuse the results from Experiment #4, where we solve more difﬁcult instances with B&C, we report
the runtime results in Table 7. In these cases, the beneﬁts of high-quality cuts are magniﬁed by a decreased number of
iterations (i.e. expanded nodes) - indeed, for RL policy, the advantages resulting from decreased iterations signiﬁcantly
overweight the potentially slight drawbacks of per-iteration runtime. In Table 7, we see that RL generally requires much
smaller runtime than other heuristics, mainly due to a much smaller number of B&C iterations. Note that these results are
consistent with Figure 5. Again, for large-scale problems, this is an important advantage in terms of usage of memory and
overall performance of the system.

Reinforcement Learning for Integer Programming: Learning to Cut

Table 6: CPU runtime for adding cutting planes (units are seconds). Here we present the results from Experiment #2 from
the main text, where we ﬁx the number of added cuts T = 50. Note that though RL might increase runtime in certain cases,
it achieves much larger IGC within the cut budgets. Note that these results are consistent with Table 2.

Tasks

Size

RANDOM
MV
MNV
RL

Packing

30 × 30

0.06 ± 0.01
0.9 ± 0.01
0.10 ± 0.02
0.10 ± 0.02

Planning

61 × 84

0.09 ± 0.01
0.100 ± 0.004
0.100 ± 0.004
0.14 ± 0.03

Binary

33 × 66

0.088 ± 0.003
0.10 ± 0.01
0.12 ± 0.02
0.07 ± 0.04

Max Cut

27 × 67

0.08 ± 0.01
0.11 ± 0.01
0.12 ± 0.01%
0.08 ± 0.02

Table 7: CPU runtime in B&C with large-scale instances. The measures are normalized with respect to RL so that the RL
runtime is always measured as 100%. Here, we measure the runtime as the time it takes to reach a certain level of IGC. We
only measure the runtime on test instances where the IGC level is reached within the node budgets. When the IGC is not
reached for most test instances (as in the case of the planning problem for most baselines), the runtime measure is ’N/A’.
Note that the results here are consistent with Table 3 and Figure 7.

Tasks

Size

RANDOM
MV
MNV
LE
RL

Packing

60 × 60

146%
256%
238%
120%
100%

Planning

122 × 168

N/A
N/A
N/A
N/A
100%

Binary

66 × 132

190%
340%
370%
370%
100%

Max Cut

54 × 134

250%
210%
95%
120%
100%

