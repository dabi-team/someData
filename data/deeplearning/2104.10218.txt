Episodic Memory Model for Learning Robotic
Manipulation Tasks

Sanaz B. Behbahani, Siddharth Chhatpar, Said Zahrai, Vishakh Duggal and Mohak Sukhwani
ABB Discrete Automation and Robotics
sanaz.b.behbahani@us.abb.com, siddharth.chhatpar@us.abb.com, said.zahrai@de.abb.com,
vishakh.duggal@in.abb.com, mohak.sukhwani@in.abb.com

1
2
0
2

r
p
A
0
2

]

O
R
.
s
c
[

1
v
8
1
2
0
1
.
4
0
1
2
:
v
i
X
r
a

Abstract—Machine learning, artiﬁcial intelligence and espe-
cially deep learning based approaches are often used to simplify
or eliminate the burden of programming industrial robots. Using
these approaches robots inherently learn a skill instead of being
programmed using strict and tedious programming instructions.
While deep learning is effective in making robots learn skills,
it does not offer a practical route for teaching a complete
task, such as assembly or machine tending, where a complex
logic must be understood and related sub-tasks need to be
performed. We present a model similar to an episodic memory
that allows robots to comprehend sequences of actions using
single demonstration and perform them properly and accurately.
The algorithm identiﬁes and recognizes the changes in the states
of the system and memorizes how to execute the necessary
tasks in order to make those changes. This allows the robot to
decompose the tasks into smaller sub-tasks, retain the essential
steps, and remember how they have been performed.

Index Terms—Cognitive robot, Artiﬁcial Intelligence, Robots,

Co-bots, Episodic Memory

I. INTRODUCTION

A typical automation system includes, in addition to the
robot itself, various elements that collectively make the in-
tended process feasible. Examples of such elements are grip-
pers, ﬁxtures, cameras, sensors, and simple passive elements
such as trays and containers. The system composed of such
elements needs to be programmed so that action sequences
take place in an expected and controlled manner to achieve
the desired goal. The conventional programming process for
industrial robots includes representing all the elements that are
present in the process and programming how and in what order
the interactions between them should occur. Each program is
case-speciﬁc and typically needs weeks or months of imple-
mentation and veriﬁcation before the production starts [1], [2].
To enhance the re-usability of such systems, programs are
usually divided into more diminutive and reusable modules
or routines, which can be utilized in other applications. To
further simplify the work, one can provide a framework with
all essential parts prepared and introduced [3], [4]. As long
as the template explicitly covers the case of interest, it can
minimize the programming time considerably [5], [6], [7].
Graphical tools and visual programming languages are also
often utilized to help the programming process [8], [9], [10],
[11].

All of the above mentioned methods can be recognized as
standard techniques to reduce the burden of programming and
make it more effective. Despite all the attempts to simplify the

programming of robots, a single and comprehensive method
that eliminates or reduces the programming effort for all
kind of applications does not exist and unless templates are
prepared for a speciﬁc domain, an expert programmer with
knowledge from a speciﬁc robot is needed to develop an
efﬁcient and reliable robot application for industrial use.

Recent advances in machine learning, computer vision, and
artiﬁcial intelligence, together with availability of affordable
high-performance computing power have led to the initiation
and development of new ideas for making robots more acces-
sible to inexperienced and ﬁrst time robot users. Innovative
strategies are proposed and developed to make the robots
trainable rather than programmable.

Learning from Demonstration (LfD) or Programming by
Demonstration (PbD) denote methods, which have received
wide attention from the robotic community. The fundamental
idea is to enable the robot to learn and perform tasks without
programming in a similar way as humans [12], [13], [14]. In
a nutshell, a human tutor will demonstrate speciﬁc tasks to a
robot, and the robot will use the demonstrated knowledge to
execute the instructed task. The LfD process usually has an
off-line learning phase, from which the data collected from
successfully performed tasks are used for training. Typically,
Gaussian Mixture Models (GMM), Bayesian networks, or
dynamical system control are used to learn the trajectory or
the action sequence. During operation, the input from a set of
sensors is used to control the robot. As long as the input is
consistent with the training cases, with some level of robust-
ness to environmental noise and perturbations, the robot will
behave properly. If the system faces an unknown condition,
incompatible with the training data, the system does its best
to perform, but the outcome will not be predictable [15], [16],
[17], [18]. In general, these methods require a large amount
of data or prior knowledge for each new skill.

A more recent, forward-looking approach to the LfD, is
the one-shot learning or learning from a single demonstration.
In some approaches, the concept of learning from demonstra-
tion is joined with some hierarchical task networks and is
accomplished via the bi-directional communication between
the tutor and robot, where both are committed to the shared
goal of the interaction and both actively contribute towards
achieving a single goal [19]. More advanced strategies to one-
shot learning combine meta-learning with imitation, enabling a
robot to reuse previous experience and learn new skills from a

 
 
 
 
 
 
was used as the core of the semantic memory. It enables the
robot to learn a new task, by shadowing a tutor, learning, and
generalizing from only one demonstration. Episodic memory
is a long-term memory of experiences and memories of events.
Episodic memory provides means to store various experiences
and combine them to a more abstract construct and recall
consciously when needed [30], [31], [32].

While all of above methods are contributing to ease of
working with robots, none of them have reached a degree
of reliability and generality to enter manufacturing industry,
where the expectations on predictability, reliability and ac-
curacy of outcomes are extremely high. In this work, our
goal is to build a model for episodic memory that enables
the user to teach the robot to perform necessary instructions
in an industrial manufacturing application through one single
demonstration, the same way as teaching an apprentice. Unlike
typical LfD methods, our single shot teaching provides an
economic, efﬁcient, and convenient way of providing the robot
with the required information to complete the desired task
without programming [33], [34].

II. RELATED WORK

Industrial robots have evolved from being pure engineering
systems to become collaborative over time. Collaborative
robots are designed to augment humans’ abilities and act as
an extension in work in complex industrial environments. To
enhance the interactions, collaborative robotic architectures
to recreate human cognition and intend to reach
attempt
a higher degree of autonomy by hierarchic control system
design, environment sensing, and information fusion [29],
[35].

Recent researches have successfully demonstrated task-
based dialogues with collaborative robots [29]. The proposed
cognitive architecture can acquire a new understanding of
objects,
their properties, and details of the (manipulative)
actions performed on the object. The authors have deﬁned a
graph-based memory design that sets the methodology apart
from present-day data-hungry statistics-based approaches. The
described architecture is robust for knowledge sharing among
multiple industrial robots. This system allows users to teach
the robot new knowledge about objects, activities, and tasks in
one shot [36]. One can transfer the learning and experiences
from one robot to another in a simple manner. To be optimally
useful and easy to use, robots need to effectively remember
repetitive manipulative experiences. Episodic memory has an
effective structure to store explicit past experiences and plays
an essential role in diverse cognitive processes. It helps to put
knowledge in order and structure it for future use.

Various strategies for managing computational models of
episodic memory are available to memorize experience se-
quences [37], [38], [39]. Hierarchical emotional episodic mem-
ory, using deep adaptive resonance theory network [40] has
also been proposed to learn emotions correlated with past ex-
periences. Generalized fusion adaptive resonance theory-based
EM-ART has been introduced to make the episodic memory
suitable to handle intricate relations of past events [41], [42].

Fig. 1: Structure of human memory.

single demonstration using deep learning methodologies [20].
In deep learning (DL) approaches, high capacity, deep
network models are used to generalize to the open world. The
major downside to the DL approaches is that they require large
amounts of diverse training data to be effective, therefore, it
may not be suitable for novel robot users [21]. Reinforcement
learning (RL) is another novel method of robot learning. In this
approach, the focus is on the manipulated object at any time,
where the demonstrations are generated automatically through
RL rather than the human demonstration. In this approach, the
goal is to solve sequential decision-making problems by policy
learning [22], [23]. Deep Q-networks are the most famous
methods for deep reinforcement learning, where the action
value function is approximated by a neural network and the
value functions get updated by the Q-network [24].

Learning is an ability possessed by humans, animals, and
certain plants, and concerns acquiring understanding, knowl-
edge, behaviors, skills, values, and preferences [25]. Acquired
knowledge and information are collected in the memory and
prepared for later use. In general, memory can be short-term or
long-term. As far as the learning concerns, long-term memory
should be considered. According to neuroscientists, long-term
memory can be divided into procedural and declarative. Pro-
cedural or implicit memory is the unconscious memory that is
used for learning skills, such as balancing, biking, and playing
musical instruments. Declarative memory is where facts and
events are stored and can be recalled consciously. Declarative
memory is further divided into episodic and semantic memory,
as illustrated in ﬁgure 1. Episodic memory stores knowledge
about events, while semantic memory accumulates facts and
data together with logical relations [26].

Procedural memory is typically acquired through repetition
and practice and is composed of unconsciously relating sen-
sory inputs to motoric behaviors. Artiﬁcial neural networks are
used to model procedural memory and allow a fast recognition
or reaction to sensory data after a complete training [27].
Semantic memory is developed through observations, training,
and consciously acquiring information. For example, we learn
the laws of physics and apply them to solve problems and
perform our daily tasks. Semantic memory can for example be
modeled by expert systems [28]. In [29] a knowledge graph

Temporal event sequences are efﬁciently learned and recalled
from stored episodes. Such elements are required to make
intelligent robots for drastically simpler automation.

The proposed method presented in this paper considers
remembering essential events in the right order from software
architecture and information management perspective. The
method enables the robot to learn a new task, by shadowing
a tutor, without the need for programming knowledge. We
deﬁne a uniﬁed approach using graph-based ﬁnite-state au-
tomata. The behavior of the elements used in the application
is modeled by state machines. Each element and its state
machine can be utilized in any other application without the
necessity for adaptation. A software architecture is introduced
that simpliﬁes building large robotic systems.

III. LEARNING WORK INSTRUCTIONS

A. Apprentice at work

In the approach we present here, we aim to provide an
industrial robot the ability to memorize the steps in a process
and repeat them when requested, similar to what an apprentice
would learn working in an industrial environment. Before
going into the details of the methodology, it is worth imagining
a learning scenario for our apprentice on the very ﬁrst working
day.

On the ﬁrst day of work, the apprentice is introduced to
the working environment. Tools and procedures are described,
and the apprentice observes how each device works and if
needed, receives necessary training. Next,
the instructions
are described by the tutor. The apprentice observes what is
happening and how objects change in the working space. If our
apprentice has a perfect memory, all steps will be memorized
and repeated in the right order when needed. Our apprentice
learns by observing changes in the environment and processes
the information, but what is learned will depend on personal
judgments and prior experiences as well as the attention paid
to the details of the process. An apprentice that does not pay
signiﬁcant attention will not be able to memorize the steps
properly and if there is a lack of experience to understand the
observations, the learning process will not be complete. As
an example, an adolescent will have complications learning
to perform a complex assembly task, even if mechanical
capabilities are sufﬁcient.

To give our robots the ability to learn instructions and
reproduce them, it must ﬁrst be able to make observations
on the objects it is working with. Secondly, the robot needs
to understand the variations in the object and the surrounding
world, record and retrieve both the changes and the process
that has caused them. We accomplish this by decomposing
the world around the robot
into a collection of elements
and describe their behavior, as well as interactions between
them, in an observable and measurable manner. Details of the
proposed system are described in the next section through an
example of machine tending.

Fig. 2: A simpliﬁed machine tending application work-cell.

B. Machine tending application

A typical machine tending application has the following
workﬂow. There is an infeed tray that contains parts that
should be placed in the machine. The task starts by loading
the machine with parts from this tray. After safely loading the
machine, the door is closed and the machine is instructed to
start the process. Once the process is complete, the machine
stops, the door opens and the system is ready to remove the
part from the machine and put it in the outfeed tray. In our
example, we assume that the machine performs a test with a
result that could be good or bad. Depending on the result, the
parts are sorted in the respective trays.

Learning this process for an apprentice will not be chal-
lenging and it will be likely adequate to see the process only
once. The apprentice will understand that the object should be
picked from the in-feed tray and be placed in the machine, or
be picked from the machine and placed in the proper outfeed
trays, depending on the result of the test. Not picking from
empty slots and not placing in full slots are explicit rules for
an apprentice. We intend to build a robotic system that is
capable of learning the process similarly. We use the case of
the testing machine as an example, but the method is by no
means restricted to any particular applications.

C. Solution

A simplistic structure of a machine tending cell is shown
in Figure 2 and consists of a robotic arm, grippers, trays, and

Fig. 3: Architecture for learning and executing a Machine
Tending Application.

a test machine. Although it is conventional to use machine
vision to recognize and localize objects, here, for simplicity,
we overlook that and instead assume that the objects are known
and the locations are deﬁned. Each component of this system is
represented by a ﬁnite state machine, which closely describes
the characteristic behavior of that element, regardless of the
application it is being used in. Note that in a production
system, the objects in the work cell are recognized by the
vision system, their position and their conﬁgurations will be
distinguished automatically.

Figure 3 shows the construction of the system for observ-
ing, learning, and executing the application. First, the data
concerning all observable elements is collected and analyzed.
The data can be augmented to advance the understanding
and categorizing ability of the system. The data is ﬁrst used
to generate the path for moving elements of the system.
Correlations between signals are used for discovering the
essential events that change the state of one or several elements
in the system.

How the environment is changing is a combination of the
changes in the elements of the system. After the correlations
are found and the state of each element is identiﬁed, the
information is integrated into a transition to a new state of the
environment, with the conditions required for the transition to
happen. If the new state is the same as the ones previously
visited, a closed loop is created and can be executed repeatedly.
The series of state transitions form the application, for which
a code can be generated and executed later. The proposed
solution and the procedure of learning is described as follows:

• A scene is separated into elements whose interactions
lead to the execution of the desired task. These elements
are identiﬁed either with a vision system or the infor-
mation is fed to the system during commissioning as
conﬁguration.

• A state machine is connected to each element, which
includes the states of the element and the conditions
that would lead to a transition from one state to another.
These state machines are not application dependent but
are constructed assuming that the element is used in a
robotic system.

• All the existing elements are attached to the system and
their data will become observable for the data collector.
• The robot and tools connected to it are the players
in the system. The demonstration concerns leading the
players so that the task can be accomplished. During the
demonstration, the following steps are taken:

1) The data collector accumulates all data at each time

step.

5) A loop is identiﬁed if one of the previous states is

revisited.

6) The synthesized state machine, together with the

corresponding path, are saved as a program.

• The recorded program can be executed on command.
With the above method, our robot learns the steps necessary
to perform a task, very similar to an apprentice who pays full
attention during the demonstration and memorizes all details.
In the next section, we present the details of implementation
for our speciﬁc case of machine tending with an ABB robot.

IV. IMPLEMENTATION

In this section, we explain the system developed for machine
tending with ABB’s industrial robot IRB 14000, YuMi. YuMi
is a two-armed robot with integrated grippers and lead-through
functionality that allows the demonstration of tasks to be
convenient. The tutor simply moves the robot arm in lead-
through mode and demonstrates the desired task. During this
operation, data are collected by the data collection module, and
the raw data are analyzed by the data augmentation module
to extract element states.

This state vector is the basis for extracting the logic of
the process. In the case of machine tending, the elements
whose actions lead to changes in the environment are the robot
and the gripper. Therefore, we pay special attention to the
following system states: robot motion (RM), indicating the
robot is moving, gripper motion (GM), occurring when the
gripper is in motion, and stand still (SS), which is when the
robot and gripper are not moving. A sample state trajectory is
presented in Table I. At each instance of time, the elements
of the system are in a speciﬁc state. We monitor the element
states and record when a state transition occurs.
Three properties are used to identify a state:
• Type of state (e.g., Robot Motion, Gripper Motion, Stand-

still)

• Condition for transitioning into state
• Commands given in the state

Using the state identity, we can recognize when a state is
repeated and whether there is a loop in the system. The
task application program is being generated on-the-ﬂy, which
enables the robot to perform the same task without the need
for programming and based on what the system has observed
from the tutor. Machine tending is only an example of what
the system is capable of doing. This learning framework can
handle different variations, for example, several trays and test
machines, process task sequence, etc without any changes.

A. State machines

2) The data is ﬁltered and correlations, as well as the

state changes for each element, are found.

3) The state changes, as well as conditions leading to
them, are synthesized in a state machine describing
the changes in the scene.

4) The recorded data is used to deﬁne the path for

movable elements, like the robot itself.

The element state machines encode essential information
about the operation of the individual elements and how they
interact with each other. This is equivalent
to the mental
models built by the apprentice for the elements involved in
the process s/he is observing. The information encoded in
an element state machine includes the discrete physical and
operational states that the element can be in, the conditions

Time
Robot position
Command

t0
SS0
M oveL

t1
SS0
−

t2
SS0
−

t4

t3
t5
SS0 RM0 RM0 RM0 RM0
−
−

t7

t6

−

−

−

t8
SS1
Grip

TABLE I: A sample state trajectory.

that lead it to transition between states, the labeling of certain
states as active to indicate interaction with other elements, and
the commands that can be given to the element.

Consider the example of the Test Machine State Machine
(TMSM) (Figure 4d). It includes operational states that are
common to all element state machines: Start, Init, System
Conﬁg, and Idle. Machine Full and Machine Empty represent
physical states where an object has been placed in the machine
and when there is no object in the machine, respectively. A
physical property common to both these states is that the
machine door is closed. When the door is opened in the
Machine Empty state, the state machine makes the transition
to the Ready to Get state, where an object can be placed
into the machine. The door opening is achieved via an “open
door” command sent to the Test Machine Element. The TMSM
encodes this information into the state machine for the beneﬁt
of the learning system by issuing an output with the command
name when there is a transition from Machine Empty to Ready
to Get. In general, every state transition has an output; this
output is set to “None” if there is no command triggering
that state change. In the Ready to Get state, the test machine
is empty with its door open and ready to receive a test
object. When a loaded robot enters the test machine in this
state, the state transitions to a special state, called an “active
state”, indicating that the element is in interaction with another
element. There are two active states in the TMSM: Getting
Object and Giving Object. The tagging of states as active
allows the learning system to identify the interaction between
elements. Other appropriate information can be included in
the element’s state variables, e.g.,
the result generated by
the test machine. This causes the Test Running state to be
split into Result Positive and Result Negative, depending on
the result generated, enabling the learning system to utilize
this information to branch process control based on the result
without any special means for encoding the decision making.

Once the element state machines have been constructed in
this manner, it is easy to see that a superstate machine can
capture all the relevant information for the discrete process.
The superstate machine aggregates the element states and
has states representing seen element state combinations. Any
information that is not captured in this process is a result of an
insufﬁcient design of one or more of the incorporated element
state machines. Any state/event that is important to the process
for selecting an “action” needs to be captured in an overall
state change of the system so that the learning machine can
recognize the need for taking action.

B. Active elements

In a robotic process, robot motions are almost always
targeted towards interaction with other elements of the process,
e.g., pick or put objects from/to tray, pick/put objects from/into
test machine. Identifying and capturing these interactions are
essential for learning the process. Also, the robot motions
between interactions need to be understood as a single action
with the target of getting into position to interact; meaning
that the exact trajectory/motion proﬁle is not important, but
getting into position to interact is. To highlight this interaction,
the concept of active elements is introduced. Elements are
considered active when they enter into states that are labeled
as active states. For example, the test machine is in interaction
with the robot when the robot enters the machine to take or
leave an object; therefore, the states Getting Object and Giving
Object are active states in the TMSM.

C. Single shot learning of task

During learning,

the system builds an application state
machine (ASM), which can later execute the learned process.
The ASM build process is started with the inclusion of states
that are common to all element state machines: Start, Init,
System Conﬁg, and Idle, as well as an Exception state. As
the process is demonstrated, the learning system adds states
to the ASM. Because our implementation targets a robotic
application, the types of states in our ASM were selected to
be Robot Motion, Gripper Motion and Standstill; however,
these can be arbitrarily chosen for different implementations.
In fact, in a generic implementation new states can be added
to the ASM whenever any of the underlying elements changes
state without any correspondence to the state of the robot or
gripper. With each new state, two transitions are added: one
from the previous state to the new state and a second from the
new state to the Exception state.

The learning system also captures any commands produced
by the element state machines and adds them to the appropriate
state in the ASM. Commands can take arguments, which
can be simple, e.g., the binary open/close commands for the
gripper; or complex, e.g., location, in the case of robot motion
command. Location information for robot motion commands
is recorded in semantic form by the ASM, e.g., {“Move to”,
“Tray”}. The semantic location is inferred by the correlation
module based on what element is active when a robot com-
pletes its move. During execution, the semantic location is
replaced by the known metric location for the Tray.

In this manner, the learning system builds an initial ap-
plication state machine as a string of states with each state
having a transition to the next state in the process as well
as the Exception state. This state machine is later ﬁltered to

(a) State machine describing the behavior of the robot.

(b) State machine describing the behavior of the gripper.

(c) State machine describing the behavior of the tray.

(d) State machine describing the behavior of the test machine.

Fig. 4: State machine description for different elements of the system.

detect loops and branches, resulting in a more complex state
machine.

When executing, the ASM transitions along the state chain
as the underlying elements change state in accordance with
the observed process. If a state change occurs that does not
follow this process, i.e., the ASM sees a state combination
that is not in its current state or the immediate next states that
it can transition to, it transitions to the Exception state. The
Exception state is a special state where the system interacts
with a user describing why it is in this state and looks for
direction on how to resolve the situation. The user can direct
the system to continue, which is interpreted by the system to
mean that the element state combination seen is an allowed
combination in the state it was in. Or, if the user decides
that special steps need to be taken to resolve the exceptional
the system to learn and then
condition, s/he can request
proceed to demonstrate what to do in that condition. In this
case, the system goes back to learning and adds new states to
the ASM based on the user demonstration.

D. Task state pruning

The ASM is constructed as a linear chain of states. After
the demonstration is complete, this linear chain is examined to
detect any duplicate states. States are compared for duplexity
based on their properties: type of state (Robot motion, Gripper
motion, Standstill); transition condition (element state vector);
any commands given in the state. If all the properties match for
two states, then they are considered duplicates. In this case,

the state that comes later in the chain is removed and the
transitions to and from that state are instead pointed to the state
that comes earlier in the chain. This is how loops get formed
in the ASM. Loops can be genuine or erroneous. For example,
when the robot is being moved to interface with a tray during
a demonstration, jitter in successive position measurements
at the boundary can indicate robot moving into and out of
interfacing with the tray. This will result in duplicate states
and loop formation. This loop is identiﬁed as an erroneous
loop and discarded. Genuine loops can form if a process
demonstration is repeated to show a variation. In case of
the machine tending example, the user will demonstrate the
process for the two possible results of the test. In this case, all
the states leading up to the test machine producing a result are
duplicates for the repeated demonstration. Therefore, during
pruning, the duplicate states get replaced resulting in a branch
formation in the ASM where the two branches represent the
test machine producing a positive and a negative result, resp.

E. Execution of learned task

it

the system stores it

Once an application is learned,

in
its library. Any time the system is on,
is continuously
monitoring the elements in its workcell. Based on the identiﬁed
elements, it is able to load all the applications possible with
those elements. When a particular application is selected, the
system executes the corresponding ASM. The ASM moves to
Init state and waits on a trigger – in the case of the machine
tending example: introduction of a full input tray. When a full

Fig. 5: Platform architecture

input tray is introduced, the change in the tray element results
in the ASM transitioning to a state where a command is given
to the robot to move into interaction with the tray. As the robot
starts moving, the change in the robot state machine results
in the ASM transitioning to the corresponding robot motion
state. In this manner, as long as the combined element state
transitions follow the demonstrated process, the ASM executes
normally. If a previously unseen condition is encountered, the
ASM enters Exception state. In this state, the ASM interacts
with the user to determine how to proceed.

Fig. 6: Experimental setup for machine tending.

• Thread 1 and 2 execute motion commands for left and

right arms of YuMi.

• Thread 3 and 4 execute gripper commands for left and

right grippers.

• Thread 5 and 6 provide access to real time data generated

by robot.

• Thread 7 and 8 allow stopping/re-starting of commands

V. SOFTWARE PLATFORM

in execution.

Figure 5 presents the software architecture used for this
implementation. The core of the platform is a message broker,
RabbitMQ, to allow a ﬂexible communication between differ-
ent components. Any component in the system which either
generates or consumes data, e.g. robot, tray, test machine,
needs to be connected to the message broker. Messages are
in JSON format to ensure cross language compatibility. Each
component
is wrapped by a driver, following a standard
template. In what follows, we describe the robot driver, as an
example that illustrates the structure of a driver and especially
explain how the communication with the robot is implemented.
All drivers consist of three major sub modules: Fac¸ade,
Decorator and Bridge. Fac¸ade acts as the interface between
the driver and the message broker and is responsible to
interpret JSON messages. Decorator implement the logic and
the desired representation of the element. Bridge provides the
communication link to the element.

In the case of the robot, two arms and their respective
grippers are packed in one driver, which communicates with
the robot controller, running a hard real time application. The
representation should be such that they can be considered as
four different components, but can be operated synchronously
if needed. Consequently, the driver needs to provide following
functionalities:

1) Ability to control robot arms and their respective grip-

pers independently.

2) Lead through programming support.
3) Access to data generated by robot in real time.
4) Synchronous execution of commands.

To ensure that the system responds properly to the robot,
the bridge is divided into 8 modules, each of which running
in a separate thread, as follows:

The robot provides a TCP/IP server, implemented in ABB’s
programming language, RAPID, and the platform is a client
consuming services offered by the server.

VI. EXPERIMENT

The method described above was examined by applying to
the case of machine tending explained earlier. The experiment
was performed with the dual arm, collaborative IRB14000
(YuMi), equipped with smart gripper from ABB, with simple
wooden blocks and trays as shown in Figure 6. The test
machine is simulated with a given location on the table, the
black square, and the results was randomly set to resemble a
realistic scenario.

When the system starts for the ﬁrst time, it is expected
to scan its environment and collect information about the
environment. For the object in surrounding, this will typically
mean that a vision sensor scans the scene and identiﬁes the
objects present in the scene. The system will also scan the
interfaces, such as ﬁeld busses, Ethernet ports, USB ports
etc. to identify devices that are available. Although this is
a standard procedure, in our test, we explicitly provide the
information to simplify the implementation.

Each recognized element is expected to be represented by
a state machine as described above. That is all information
which will be available to the system before training. The
information is generic and does not depend on the application
to be trained. Consequently, before training the robot system
does not have any information about what the task will be and
how it should be performed. However, after recognizing the
elements in the system and in the environment, the robot is
capable to record any changes and can learn to react to them
according to the instructions from the tutor.

(a) Initial state observed and understood
by the system.

(b) Tutor leads the robot to the infeed tray
and closes the gripper to grasp the object.

(c) The robot with the closed gripper is
moved to the test machine and the gripper
is opened to release the object.

(d) The robot is moved to the home posi-
tion waiting for test result.

(e) Post test result, the robot is moved to
the test machine, gripper is closed to grasp
the object.

(f) The robot is moved to the tray corre-
sponding to the test result, the gripper is
opened and the object is released.

(g) The robot is moved to the infeed tray
to pick the next object.

(h) The object is moved to the test machine
and the gripper is opened to release object.

(i) The robot is moved to the home posi-
tion waiting for test results.

(j) Once the result is knows, the object is
picked from the test machine.

(k) The object is placed in the tray corre-
sponding to the test result.

(l) The robot is moved to the infeed tray
to pick the next object.

Fig. 7: Training process.

We assume that the process will start with placing a full
infeed tray in the scene. This change of environment state will
be detected and will trig execution of the process that has been
learnt. In what follows, we describe the training process ﬁrst
and then show how the robot will repeat the process accurately.
Once again, it should be noted that the robot does not learn
a path, but understands the semantic of the actions, including
all logic.

Figure 7 presents the steps that the tutor needs to take to
teach the process to the robot system. The tray to the left is
the infeed and the middle one and the right one are outfeed
trays associated to the binary result of the test, say ’good’ and
’defected’. As described in the captions of the sub-ﬁgures, the

tutor simply needs to guide the robot to perform the task at
least once for each possible branch, here a good object and a
bad object. Obviously, during the training time, it would be
practical to have one good and one bad at the start and let the
robot continue by itself.

Actions that are taken by the tutor, or autonomously by
the elements themselves, are observed and used for dividing
the process into meaningful steps that should be executed
when certain well-deﬁned conditions are fulﬁlled. These steps
are assembled as the application state machine that can be
executed in continuation.

Figure 8 shows how the robot repeats the steps that it
has learnt after a complete training session. Obviously, to be

(a) The robot follows the learnt process
and moves to the ﬁrst available object and
grasps it by closing the gripper.

(b) The object is placed in the dedicated
location in the test machine and is released
as the gripper is opened.

(c) The robot exits the test machine and
goes to the home position awaiting the test
to complete.

the test

(d) After that
is complete and
result is known, the robot moves to the
test machine and picks the object.

(e) The target tray is selected depending
on the result of the test and the object is
placed in the ﬁrst available position.

(f) The robot returns to the infeed tray and
picks the next available object.

(g) The robot is moved to the test machine
and the object is left to be tested.

(h) The robot moves to the home position
waiting for the test to be completed.

(i) The robot is moved the test machine to
pick the object.

(j) The object is placed in the tray corre-
sponding to the test result.

(k) The robot moves to the infeed tray to
pick the next object.

(l) The process is continued until an un-
foreseen state occurs. Here,
the middle
tray is full, and therefore the robot stops
and waits for human intervention.

Fig. 8: Execution process.

able to perform the work, the robot needs to understand the
semantics and generalize the learnt step. For example, at the
pick, the robot picks next available object, whose coordinate
will be delivered by the tray, which tracks its content with
the help of the camera. Equally, at the placement, the next
available empty slot is taken, without the tutor having taught
that speciﬁc position.

request human interventions. The operator will take action, for
example instruct the other hand to remove the full tray and
replace it with an empty one. This will bring the system to a
recognized state and the procedure can continue. By doing so,
how to handle exceptions can also be learned after that each
exception has happened only once. This reduces the burden of
programming enormously.

The last sub-ﬁgure in ﬁgure 8, ﬁgure 8l, shows that the
robot has observed a yet unknown state: The middle tray is
full, an event that has not occurred before. In such a case,
the system lacks information to proceed and will stop and

VII. SUMMARY AND CONCLUSION

We propose an approach that allows machines to record
and follow instructions in a reliable manner. Each element

[20] C. Finn, T. Yu, T. Zhang, P. Abbeel, and S. Levine, “Oneshot visual
imitation learning via meta-learning,” arXiv preprint arXiv:1709.04905,
2017.

[21] S. Dasari, F. Ebert, S. Tian, S. Nair, B. Bucher, K. Schmeckpeper,
S. Singh, S. Levine, and C. Finn, “Robonet: Large-scale multi-robot
learning,” arXiv:1910.11215, 2019.

[22] O. Kilinc, Y. Hu, and G. Montana, “Reinforcement

learning for
robotic manipulation using simulated locomotion demonstrations,” arXiv
preprint arXiv:1910.07294, 2019.

[23] S. Gu, E. Holly, T. P. Lillicrap, and S. Levine, “Deep reinforcement

learning for robotic manipulation,” CoRR, 2016.

[24] H. Sasaki, T. Horiuchi, and S. Kato, “A study on vision-based mobile
robot learning by deep q-network,” in Conference of the Society of
Instrument and Control Engineers of Japan (SICE).

IEEE, 2017.

[25] Learning. [Online]. Available: https://en.wikipedia.org/wiki/Learning
[26] Human Memory. [Online]. Available: http://www.human-memory.net
[27] D. Vernon, M. Beetz, and G. Sandini, “Prospection in cognition:
The case for joint episodic-procedural memory in cognitive robotics,”
Frontiers in Robotics and AI, 2015.

[28] G. Sarthou, A. Clodic, and R. Alami, “Ontologenius : A long-term
semantic memory for robotic agents,” IEEE International Conference
on Robot & Human Interactive Communication (IEEE Ro-MAN), 2019.
[29] M. Sukhwani, V. Duggal, and S. Zahrai, “Dynamic knowledge graphs as
semantic memory model for industrial robots,” arXiv:2101.01099, 2021.
[30] D. Stachowicz and G. M. Kruijff, “Episodic-like memory for cognitive
robots,” IEEE Transactions on Autonomous Mental Development, 2012.
[31] J. Rothfuss, F. Ferreira, E. E. Aksoy, Y. Zhou, and T. Asfour, “Deep
episodic memory: Encoding, recalling, and predicting episodic experi-
ences for robot action execution,” CoRR, 2018.

[32] E. Castro and R. Gudwin, “An episodic memory for a simulated

autonomous robot,” Robocontrol, 2010.

[33] T. M. Mitchell, R. M. Keller, and S. T. Kedar-Cabelli, “Explanation-
based generalization: A unifying view,” Machine learning, 1986.
[34] R. Dillmann, “Teaching and learning of robot tasks via observation of
human performance,” Robotics and Autonomous Systems, 2004.
[35] A. Mandal, D. Sharma, M. Sukhwani, R. Jetley, and S. Sarkar, “Improv-

ing safety in collaborative robot tasks,” in INDIN, 2019.
[36] Thinking Robots. [Online]. Available: https://thinkingrobots.ai
[37] G. J. Rinkus, “A neural model of episodic and semantic spatiotemporal

memory,” in COGSCI, 2004.

[38] A. M. Nuxoll and J. E. Laird, “Enhancing intelligent agents with

episodic memory,” Cognitive Systems Research, 2012.

[39] J. A. Starzyk and H. He, “Spatio–temporal memories for machine
learning: A long-term memory organization,” Transactions on Neural
Networks, 2009.

[40] W.-H. Lee and J.-H. Kim, “Hierarchical emotional episodic memory for

social human robot collaboration,” Autonomous Robots, 2018.

[41] W. Wang, B. Subagdja, A.-H. Tan, and J. A. Starzyk, “Neural modeling
of episodic memory: Encoding, retrieval, and forgetting,” Transactions
on neural networks and learning systems, 2012.

[42] B. Subagdja and A.-H. Tan, “Neural modeling of sequential inferences

and learning over episodic memory,” Neurocomputing, 2015.

of the system is modeled by a state machine that describes
the behavior of the element and its interaction with other
elements in the system. The element model is independent of
the application, allowing its reuse across multiple applications.
The training process consists of a single demonstration of the
tasks, allowing the system to comprehend the steps and the
underlying inherent logic.

This procedure minimizes programming efforts and auto-
matically generates the execution logic of the tasks performed.
Occurrence of untrained circumstances leads to the system
prompting for unexpected state and requesting human inter-
vention for complementary training. Episodic memory based
design leads to continuous learning for robots by exploiting
effective memory structures that utilizes past experiences.

REFERENCES

[1] S. Mitsi, K.-D. Bouzakis, G. Mansour, D. Sagris, and G. Maliaris,
“Off-line programming of an industrial robot for manufacturing,” The
International Journal of Advanced Manufacturing Technology, 2005.
[2] Z. Pan, J. Polden, N. Larkin, S. V. Duin, and J. Norrish, “Recent progress
on programming methods for industrial robots,” Robotics and Computer-
Integrated Manufacturing, 2012.

[3] M. A. Potter, K. A. D. Jong, and J. J. Grefenstette, “A coevolutionary

approach to learning sequential decision rules,” ICGA, 1995.

[4] S. Niekum, S. Osentoski, G. Konidaris, and A. G. Barto, “Learning
and generalization of complex tasks from unstructured demonstrations,”
IROS, 2012.

[5] A. M. Howard, C. H. Park, and S. Remy, “Using haptic and auditory
interaction tools to engage students with visual impairments in robot
programming activities,” IEEE transactions on Learning Technologies,
2012.

[6] U. Thomas, G. Hirzinger, B. Rumpe, C. Schulze, and A. Wortmann, “A
new skill based robot programming language using uml/p statecharts,”
ICRA, 2013.

[7] X. Long and T. Padır, “Template-based human supervised robot task

programming,” IROS, 2016.

[8] P. T. Cox and T. J. Smedley, “Visual programming for robot control,”

IEEE Symposium on Visual Languages., 1998.

[9] S. H. Kim and J. W. Jeon, “Programming lego mindstorms nxt with

visual programming,” ICCAS, 2007.

[10] C. Datta, C. Jayawardena, I. H. Kuo, and B. A. MacDonald, “Ro-
bostudio: A visual programming environment for rapid authoring and
customization of complex services on a personal service robot,” IROS,
2012.

[11] J. Trower and J. Gray, “Blockly language creation and applications:
Visual programming for media computation and bluetooth robotics
control,” ACM Technical Symposium on Computer Science Education.,
2015.

[12] B. D. Argall, S. Chernova, M. Veloso, and B. Browning, “A survey of

robot learning from demonstration,” RAS, 2009.

[13] J. Lee, “A survey of robot learning from demonstrations for human-robot

collaboration,” arXiv preprint arXiv:1710.08789, 2017.

[14] A. Billard, S. Calinon, R. Dillmann, and S. Schaal, “Robot programming

by demonstration,” Springer Handbook of Robotics., 2008.

[15] H. Veeraraghavan and M. Veloso, “Teaching sequential tasks with repe-
tition through demonstration,” International Foundation for Autonomous
Agents and Multiagent Systems, 2008.

[16] M. Hersch, F. Guenter, S. Calinon, and A. Billard, “Dynamical system
modulation for robot learning via kinesthetic demonstrations,” Transac-
tions on Robotics, 2008.

[17] C. Kohrt, R. Stamp, A. Pipe, J. Kiely, and G. Schiedermeier, “An online
robot trajectory planning and programming support system for industrial
use,” Robotics and Computer-Integrated Manufacturing, 2013.

[18] M. Kyrarini, M. A. Haseeb, D. Ristic-Durrant, and A. G. aser, “Robot
learning of object manipulation task actions from human demonstra-
tions,” Facta Universitatis, Series: Mechanical Engineering, 2017.
[19] A. Mohseni-Kabir, C. Rich, S. Chernova, C. L. Sidner, and D. Miller,
“Interactive hierarchical task learning from a single demonstration,”
International Conference on Human-Robot Interaction., 2009.

