9
1
0
2

t
c
O
4

]

G
L
.
s
c
[

1
v
0
4
8
2
0
.
0
1
9
1
:
v
i
X
r
a

FARKAS LAYERS: DON’T SHIFT THE DATA, FIX THE GE-
OMETRY

Aram-Alexandre Pooladian∗, Chris Finlay∗ & Adam M. Oberman
Department of Mathematics and Statistics
McGill University
Montreal, QC, Canada

ABSTRACT

Successfully training deep neural networks often requires either batch normaliza-
tion, appropriate weight initialization, both of which come with their own chal-
lenges. We propose an alternative, geometrically motivated method for training.
Using elementary results from linear programming, we introduce Farkas layers:
a method that ensures at least one neuron is active at a given layer. Focusing on
residual networks with ReLU activation, we empirically demonstrate a signiﬁcant
improvement in training capacity in the absence of batch normalization or meth-
ods of initialization across a broad range of network sizes on benchmark datasets.

1

INTRODUCTION

The training process of deep neural networks has gone through signiﬁcant shifts in recent years,
primarily due to revolutionary network architectures, such as Residual neural networks (He et al.,
2016) and normalization techniques, initially proposed as batch normalization (Ioffe & Szegedy,
2015). The former is the backbone for current “state-of-the-art” results for image-based tasks such
as classiﬁcation. Normalization can be found in a variety of ﬂavors and applications; (Ba et al.,
2016; Ulyanov et al., 2016; Wu & He, 2018; Vaswani et al., 2017; Zhu et al., 2017). For reasons
that are not fully understood, normalization gives rise to many desirable traits in network training,
e.g. a fast convergence rate, but also comes with a cost by increasing vulnerability to adversarial
attacks (Galloway et al., 2019). Apart from normalization, network weight initialization has been a
driving force of improved performance. Throughout the years, various initialization schemes have
been proposed, in particular Xavier initialization (Glorot & Bengio, 2010), FixUp (Zhang et al.,
2019), and a recent asymmetric initialization (Lu et al., 2019). These approaches are often
connected to probabilistic notions or balancing learning rates while training.

Modern network architectures catered for image-classiﬁcation tasks incorporate various one-
sided activation functions,
the canonical example being the Rectiﬁed Linear Unit (ReLU)
(Nair & Hinton, 2010). Other one-sided activations include the Exponential Linear Unit (ELU)
(Clevert et al., 2015), Scaled ELU (SELU) (Klambauer et al., 2017), and LeakyReLU (Maas et al.,
2013). ELU and SELU were constructed with the purpose of eliminating batch normalization
(BN); these activation functions minimize the internal covariate shift (ICS), which is what BN
was intended to accomplish. Recent developments have shown that BN has little impact on ICS
but affects the smoothness of the loss landscape, allowing for easier (and faster) training regimes
(Santurkar et al., 2018).

A geometric interpretation of batch normalization can be readily seen in the context of the
dying ReLU problem. This phenomenon occurs when the gradient of the neuron is zero (when the
neuron outputs only negative values), in which case the neuron is “dead”. This effectively freezes
the neuron during training. If too many neurons are dead, the network learns slowly. In fact, it is
entirely possible for a network to be “born dead”, where it does not allow learning at all (Lu et al.,
2019). To motivate this, consider a simple binary classiﬁcation problem: suppose two sets of
points in R2 are sufﬁciently separated and we wish to classify them using a simple 1-layer ReLU
network (with standard Cross Entropy loss). If the network is initialized such that one cloud of
points is not “observed”, as in Figure 1a, the network will not learn to classify those points. Batch

1

 
 
 
 
 
 
normalization will shift the points to behave like 1b; the network is no longer dead. There is a far
simpler geometric solution in R2: given one hyperplane, construct another to face the missing data,
as shown in Figure 1c by the green hyperplane, which is now a non-dead component of the network.

(a) Example of dead layer

(b) Effect of normalization

(c) Effect of Farkas layers

Figure 1: A motivating geometric interpretation

OUR CONTRIBUTIONS

We present a geometrically motivated method for network training in the absence of initialization
and normalization. Our novel layer structure, called Farkas layers, ensures that at least one neuron
is active in every layer. Using off-the-shelf networks, Farkas layers can recover a signiﬁcant part
of the explanatory power of DNNs when batch normalization is removed, sometimes up to 10%,
as demonstrated by empirical results on benchmark image classiﬁcation tasks, such as CIFAR10
and CIFAR100. When used in conjunction with batch normalization on ImageNet-1k networks,
we empirically show an approximate 20% improvement on the ﬁrst epoch over only using batch
normalization. Finally, we claim that input data normalization is a critical component for using
large learning rates in FixUp, which is not the case for Farkas layer-based networks. All in all,
this work provides a new research direction for architecture design beyond initialization methods,
normalization layers, or new activation functions.

2 BACKGROUND

2.1 NOTATION

We deﬁne neural networks as iterative compositions of activation functions with linear functions.
More speciﬁcally, given an input x(k) ∈ Rn to the k-th layer, the output x(k+1) is typically written

x(k+1) = ϕ(W (k)x(k) + b(k))

where W (k) ∈ Rm×n is a weight matrix, the rows are written as wT
i (i = 1, . . . , m), b(k) ∈ Rm is
the layer’s bias, and ϕ is the activation function of the layer. For the remainder of our analysis, we
consider the ReLU activation function, written

which acts component-wise in the case of vector arguments.

ϕ(x) = max{x, 0},

2.2 RELATED WORK

Previous work on the dying ReLU problem, or vanishing gradient problem, in the case of using
sigmoid-like activation functions, has heavily revolved around novel weight initializations. To ad-
dress the vanishing gradient problem, (Glorot & Bengio, 2010) proposed an initialization that is still
often used with ReLU activations (Shang et al., 2017). (He et al., 2015) proposed a scaled version
of a normal distribution, which improved training for pure convolutional neural networks. More
recently, (Lu et al., 2019) studied the dying ReLU problem from a probabilistic perspective and ad-
dressed networks that are “born dead”, i.e. not capable of learning. Their analysis is based on the

2

reasonable assumption that all weights and biases are initialized with a non-zero probability of being
dead:

i x + bi < 0

≥ p > 0 (i = 1, . . . , m),

P

wT
(cid:0)

(cid:1)

where m is the number of neurons in a given layer, and p is a ﬁxed positive constant. They show
that, as the number of layers approaches inﬁnity, the probability of having a network be born dead
approaches one. They also ﬁnd that initializing weights with a symmetric distribution is more
likely to cause a network to be born dead and thus propose initializing weights using asymmetric
distributions to mitigate this problem. Their analysis is relevant in the case that the network is not
very wide, which is distinct from the case we study in this paper.

Normalization has been one of the driving forces of recent state-of-the-art results;
the gen-
eral idea is to manipulate the neuron activation with various statistics, such as subtracting the
mean and dividing by the variance. Popular normalization techniques include batch normal-
ization (Ioffe & Szegedy, 2015), Layer normalization Ba et al. (2016), Instance normalization
(Ulyanov et al., 2016), and Group normalization (Wu & He, 2018). However, there is a desire to
eliminate normalization from training. A recent work that does this is FixUp (Zhang et al., 2019),
a novel initialization technique that works by scaling the weights as a function of the network
structure, allowing the network to take large learning rate steps during training. Reportedly, FixUp
provides the same level of accuracy as that of a BN-enhanced network and scales to networks
that train on ImageNet-1k. As stated in FixUp, the inherent structure of residual networks already
prevents some level of gradient vanishing, since the variance of the layer output grows with depth.
In the case of positively-homogeneous blocks (e.g. no bias in a Linear or Convolutional layer), this
type of variance increase can lead to gradient explosion (Hanin & Rolnick, 2018), which makes
training more difﬁcult.

3 FARKAS LAYERS

3.1 AUGMENTING RELU ACTIVATED LAYERS

We present a novel approach to understanding how weight matrices and bias vectors contribute to
neural network learning, called Farkas layers. As the name suggests, our method is inﬂuenced by
Farkas’ lemma. In particular, we use the following representation of Farkas’ lemma; the proof is an
exercise in linear programming and is left in the appendix.
Lemma 3.1 (Representation of Farkas’ lemma). Let W ∈ Rm×n, b ∈ Rm with x ∈ Rn. Then
the set {x | W x + b ≤ 0} is empty if and only if there exists a λ ∈ Rm such that λi ≥ 0 (for
i = 1, . . . , m), with W Tλ = 0 and λTb > 0.

We present the construction of W and b and prove using Lagrangian duality (Boyd & Vandenberghe,
2004) that such weights and bias vectors will satisfy Farkas’ lemma, resulting in at least one positive
component. In essence, this solves the dying ReLU problem while training.

Theorem 3.2. Let L be a layer of a neural network with ReLU activation function, with weights
W ∈ Rm×n (with rows wT
i ) and bias vector b ∈ Rm, and let λ ∈ Rm, with λi ≥ 0 (with at least
one λi > 0, without loss of generality λm > 0). Further, suppose W has the property that

wm =

−1
λm

m−1

Xj=1

λj wj

and that bm >

m−1
j=1 λj bj. Then this layer has a non-zero gradient.

−1
λm P
Proof. Let W ∈ Rm×n (with rows wT
i ) and bias b ∈ Rm, with the aforementioned assumptions,
and arbitrary input x. The output of this layer is ϕ(W x + b), and has a non-zero gradient if and only
if there exists at least one component such that

p∗ := max

i

wT

i x + bi > 0.

(1)

3

The left-hand side of (1) (i.e. omitting the strict inequality constraint) can be written as the following
minimization problem:

p∗ := min
x,s

s

subject to wT

i x + bi ≤ s (i = 1, . . . , m),

or equivalently,

p∗ = min
x,s

s

subject to W x + b ≤ s1,

where 1 is the vector of all ones. The Lagrangian for this problem is
(W x + b − s1)

L(x, s, λ) = s + λT

where λ ≥ 0. We compute the dual problem,

d(λ) = inf
x,s

L(x, s, λ)

s(1 − λT1) + λTb + (W Tλ)

Tx

= inf
x,s

=

if λT1 = 1, W Tλ = 0, (λ ≥ 0)

λTb
−∞ otherwise.

(cid:26)

The conditions on W are met by assumption, and there are many ways to ensure that λ is on the
simplex and thus has at least one strictly positive component. Both the primal and dual problems
are linear optimization problems: the objective and constraints are linear. By weak duality, p∗ ≥
supλ d(λ). Thus to ensure that p∗ > 0 (as in (1)), it sufﬁces to show that λTb > 0. This is guaranteed
by the assumptions on the bias vectors, and so we are done.

It is highly unlikely that a given layer in a deep neural network has all dead neurons. However,
we believe that guaranteeing the explicit activity of one neuron is beneﬁcial in that it improves
training. Figure 1 motivates the use of Farkas layers as a potential substitute for (BN) in the context
of learning: the arrows of the corresponding hyperplane indicate the “non-zero” part of the ReLU.
In the case of 1a, we see that all data points end up on the “zero” side, which would typically lead
to no learning. Heuristically, the effect of BN mimics Figure 1b, where the data points are centered
and scaled by a diagonal matrix (which geometrically acts like an ellipse) allowing for learning to
occur. Farkas layers will instead append another hyperplane that will be guaranteed to see all the
points (in a “third” dimension).

3.2 EXTENSION TO GENERAL ONE-SIDED ACTIVATIONS

The use of Farkas layers can be extended to other one-sided activation functions, such as a smooth-
ReLU (i.e. replacing the kink with a polynomial), ELU, and several others. In all these cases, one
can impose a scalar cutoff value c such that “beyond” this cutoff, we have very small (or zero)
gradients. This is shown in the following corollary, which has the same proof as Theorem 3.2.
Corollary 3.3. Let L be a layer of a neural network with an arbitrary one-sided activation function
ψ with cutoff c ∈ R, i.e. for all x < c, we have ψ(x) ≃ 0 (or exactly zero). Denote the weights by
W ∈ Rm×n (with rows wi) and bias vector b ∈ Rm, and let λ ∈ Rm, with λi ≥ 0, with at least
one positive component. To ensure that ψ(W x + b) has at least one component greater than c, we
require that

wm =

−1
λm

m−1

Xj=1

λj wj

and that bm > c − 1
λm

m−1
j=1 λjbj.

P

3.3 ALGORITHM AND LIMITATIONS

We denote a Farkas layer by F := F (·; W, b) : Din → Dout, where the weights and biases
satisfy Theorem 3.2. The construction of W and b imposes certain restrictions on the existence
of Farkas layers: namely we require that dim(Dout) ≥ 2. Finally, some network architectures
claim to perform better without bias vectors present; we require bias vectors by construction.

4

We focus on replacing Convolutional and Linear layers in (deep) neural networks, with efﬁcient
implementations in PyTorch. The general method of incorporating Farkas layers is provided in
Algorithm 1. To minimize computational cost, we consider λ to be the evenly spaced simplex
element, i.e. λi = dim(Dout)−1 (for i = 1, . . . , dim(Dout)), but for smaller problems (such as the
motivating 1-layer example) λ can be learnable. Farkas layers can be made into Farkas blocks in
conjunction with residual networks as well, as outlined in Algorithm 2, which we leave in the Ap-
pendix. In the case of multiple convolutional layers in a given residual block, the extension is natural.

In both Algorithm 1 and 2 we consider two choices for the aggregating function (AggFunc):
either the sum or the mean of the previous outputs and biases. Indeed, let dim(Dout) = m, and thus
λj = m−1. Then we have the following two aggregation methods:

m−1

wm = −

wj

(sum),

wm =

Xj=1

−1
m − 1

m−1

Xj=1

wj

(mean),

and similarly for the bias vectors. Theorem 3.2 is constructed using “sum”, but the result holds in
the case of using “mean” since ReLUs are 1-positive homogenous. Furthermore, by using “mean”,
we induce stability with respect to the ℓ∞-induced matrix norm for a matrix A ∈ Rm×n, deﬁned as

kAk∞ = max
1≤j≤m

kaT

j k1,

j are the rows of A. Let ˜A ∈ R(m−1)×n and aT be the mean of the rows of ˜A. Deﬁne

where aT
A := ( ˜A; aT) ∈ Rm×n, then

kAk∞ = max

k ˜Ak∞, kaT
n

k1

o

≤ max 


k ˜Ak∞, (m − 1)−1

m−1

Xj=1

kaT

j k1


≤ k ˜Ak∞.


Thus, in the context of network weights, the burden of stability lies on the trainable weights and not
the aggregated component. A similar analysis can be done for the bias vectors. Thus, for networks
that train on ImageNet-1k, or to potentially use larger learning rates, we believe that using “mean”
as the aggregation function is necessary.



Algorithm 1 FarkasLayer with ReLU activation; F (·; W, b) with AggFunc = “sum” or “mean”

Initialize W ∈ R(m−1)×n and b ∈ Rm
Forward pass: x
y = W x
Compute the Aggregated Components:
y := −AggFunc(y)
b := max{−AggFunc(b[0 : −1]), bm}
y ← concatenate(y, y)
b ← concatenate(b[0 : −1], b)
y ← y + b
y ← ReLU(y)
Optional: apply batch normalization to y
Return y

⊲ e.g. Linear and/or Conv with no bias

4 EXPERIMENTS — IMAGE CLASSIFICATION

Image classiﬁcation is a standard benchmark for measuring new advancements in deep learning.
Our goal is to show how training with Farkas layers, henceforth referred to as FLs, impacts learning
in these settings relative to other training techniques such as normalization and initialization (e.g.
FixUp).

We consider CIFAR10, CIFAR100, and ImageNet-1k datasets for the purposes of testing FL-
based residual networks on image classiﬁcation tasks. For CIFAR10, we consider 18, 34, 50,
and 101 layer residual networks. The latter two of these use a BottleNeck block structure. On

5

CIFAR100, we only consider 34, 50, and 101 layers. All models are trained using the same SGD
schedule for 200 epochs. We make the distinction between a “large” learning rate, meaning 0.1,
and a “small” learning rate, which is 0.01. We use standard data augmentation (RandomCrop and
RandomHorizontalFlip) but do not normalize the inputs with respect to the mean and standard
deviation of the dataset in the case of CIFAR10/CIFAR100. ] The weights are left at the default
PyTorch initialization. Additionally, we use cutout to improve generalization (DeVries & Taylor,
2017) and use weight-decay. For ImageNet-1k, we use modiﬁed code from the DAWNBench
competition (Coleman et al., 2018; Shaw et al., 2018). In this set of experiments, we augment the
data via RandomCrop, RandomHorizontalFlip, and normalize the input data. We only consider
the case of networks using BN and see how FLs can improve performance, and we train with 30
epochs.

Henceforth, we call a residual network a FarkasNet if is it comprised of only FLs, and we
always use the “sum” aggregation function unless otherwise speciﬁed. We make the distinction of
networks that use or do not use BN in the tables. When omitting BN, we use the small learning
rate. For CIFAR10 and CIFAR100, the results presented are averaged across three runs; if one of
the runs failed1 to train, we omit it from the average but place an asterisk. Full training curves are
in the Appendix.

INTERPRETING DEPENDENCE OF BATCH NORMALIZATION

Table 1: Percent misclassiﬁcation comparison for CIFAR10 (lower is better)

18 Layers

34 Layers

50 Layers

101 Layers

ResNet (with BN, large LR)
FarkasNet (with BN, large LR)

ResNet (no BN, small LR)
FarkasNet (no BN, small LR)

7.01
6.87

12.51
10.83

6.79
6.50

9.90
9.45

5.04
5.03

24.53
19.84

4.95
4.98

25.25
15.38

Table 2: Percent misclassiﬁcation comparison for CIFAR100 (lower is better)

34 Layers

50 Layers

101 Layers

ResNet (with BN)
FarkasNet (with BN)

ResNet (no BN)
FarkasNet (no BN)

30.95
30.34

40.10
34.70

23.43
23.88
55.05∗
43.93

22.90
23.22

54.02
42.94

Tables 1 and 2 both show a strong disparity in the learning capacity of a DNN when batch nor-
malization is removed, which is unsurprising. On CIFAR10, we primarily observe similar, if not
better, test errors when batch normalization is used. Without normalization, our test error is always
better; the same can be said for the CIFAR100 dataset. Thus, while FarkasResNets also diminish
in quality, we note that adding a guaranteed undead neuron to all layers heavily impacts learning in
the case of a non-normalized network. This is not at all to say that training a deep neural network
is only possible using FLs or that we achieve state-of-the-art performance; however, our implemen-
tation demonstrates the strong dependency of getting low test accuracy with normalization and is
geometrically motivated.

1Failed implies that the network did not make any progress in the ﬁrst ﬁve epochs.

6

IMPROVEMENT AT FIRST EPOCH FOR IMAGENET-1K

The ﬁnal table shows similar behaviour on ImageNet-1k, where FLs2 neither dramatically improve
nor inhibit the learning capacity towards the end of training except in the case of 50-layers, where
it performs signiﬁcantly better. Figure 2 shows that FarkasNets have a dramatic advantage over
standard ResNets at the start of training. Evidently, this advantage gradually diminishes; nonethe-
less, its presence alludes to a bigger problem in network initialization. We notice a roughly 20%
improvement on the ﬁrst epoch, simply due to the use of FLs.

Table 3: Top1/Top5 percent misclassiﬁcation comparison for ImageNet-1k (lower is better)

50 Layers

101 Layers

152 Layers

ResNet (with BN)
FarkasNet (with BN)

25.64/7.68
23.70/6.65

23.23/6.49
23.74/6.67

22.26/5.95
22.17/5.86

ResNet50
FarkasNet50
ResNet101
FarkasNet101
ResNet152
FarkasNet152

100

95

90

85

80

75

70

65

60

0.0

0.5

1.0

1.5

2.0

2.5

3.0

3.5

4.0

Figure 2: Illustration of improved performance at starting epochs with Farkas layers

IMPACT OF DATA NORMALIZATION AND BEST PRACTICE COMPARISON

We brieﬂy compare against other best practice learning regimes on CIFAR10. Table 4 highlights
the various differences between methods. At the core, we compare FarkasNets to FixUp, a recently
proposed initialization scheme that allows the use of larger learning rates in the absence of batch
normalization, and thus faster convergence. We consider a 34-layer ResNet with BasicBlock
structure using the ofﬁcial implementation of FixUp3 for the network architecture. We compare
using a 34-layer FarkasNet, where the last layer in a block is initialized to zero (as in FixUp, which
is motivated by several other works (Srivastava et al., 2015; Hardt & Ma, 2016; Goyal et al., 2017))
but use standard initialization otherwise, as well as “mean” AggregationFunction. We maintain the
setup of the previous experiments.

Data normalization is a standard “trick” for training deep neural networks, and acts like an

2For ImageNet-1k, we use “mean” aggregation function.
3From one of the authors’ Github pages

7

initial batch normalization step, where the incoming data is scaled to have zero mean and unit
variance. In the case of CIFAR10 and CIFAR100, we strove to study the problem of training without
any attempts at normalization, which is why we have omitted this from our training methodology.
In the absence of data normalization, we observed that FixUp requires a small learning rate for
training. Thus, while (Zhang et al., 2019) claims to present a theory for addressing training in the
absence of batch normalization, we ﬁnd that it still heavily hinges on normalization principles,
namely on the ﬁrst layer pass. This observation is not to diminish the beneﬁts of FixUp, as faster
convergence is observed (see Figure 3), but the learning rate does need adjustment. On the other
hand, our 34-layer FarkasNet was able to use a medium learning rate of 0.05, allowing for improved
performance relative to Table 1.

Table 4: Best practice comparison on CIFAR10 (lower test error is better)

Batch Normalization LR capacity Test Error

ResNet34
ResNet34
FixUp34
FarkasNet34

✓
✗
✗
✗

Large
Small
Small
Medium

6.77
9.18
7.49
7.12

With BN
Without BN
FixUp
Farkas

80

60

40

20

0

25

50

75

100

125

150

175

200

Figure 3: Training curves for best practice comparison on CIFAR10

Finally, we also address the notion that training a very deep neural network is challenging using max-
imal learning rate without normalization, which has been experimentally observed in FixUp and
Layer-Sequential Unit-Variance (LSUV) orthogonal initialization (Mishkin & Matas, 2015). De-
spite the shortcomings of previous work, by incorporating the “mean” aggegration function and
default initialization, we are able to successfully train a 101-layer FarkasNet with a large learning
rate on CIFAR10. We present the averaged training curve in Figure 5, left in the Appendix, where
we repeated the experiment three times. We remark that using default initialization alone results in
complete failure to train.

5 CONCLUSION

To date, successful training of a deep neural networks requires either some form of normalization,
weight initialization, and/or choice of activation function, all of which come with their own chal-
In this work, we provide a fourth option, Farkas layers, which can be used alone or in
lenges.

8

conjunction with the aforementioned techniques. The Farkas layer is based on the interaction of the
geometry of the weights with the data: by simply adding one linearly dependent row to the weight
matrix, we ensure that no neurons are dead. Using our method, we have shown that training with
larger learning rates is possible even in the absence of batch normalization and with default initial-
ization. In this work, we only touched on image classiﬁcation, but Farkas layers could be used in
many deep learning applications, which is left for future work.

REFERENCES

Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton. Layer normalization. arXiv preprint

arXiv:1607.06450, 2016.

Stephen Boyd and Lieven Vandenberghe. Convex optimization. Cambridge university press, 2004.

Djork-Arn´e Clevert, Thomas Unterthiner, and Sepp Hochreiter. Fast and accurate deep network

learning by exponential linear units (elus). arXiv preprint arXiv:1511.07289, 2015.

Cody Coleman, Daniel Kang, Deepak Narayanan, Luigi Nardi, Tian Zhao, Jian Zhang, Peter
Bailis, Kunle Olukotun, Christopher R´e, and Matei Zaharia. Analysis of dawnbench, a time-
to-accuracy machine learning performance benchmark. CoRR, abs/1806.01427, 2018. URL
http://arxiv.org/abs/1806.01427.

Terrance DeVries and Graham W Taylor. Improved regularization of convolutional neural networks

with cutout. arXiv preprint arXiv:1708.04552, 2017.

Angus Galloway, Anna Golubeva, Thomas Tanay, Medhat Moussa, and Graham W. Taylor. Batch
normalization is a cause of adversarial vulnerability. CoRR, abs/1905.02161, 2019. URL
http://arxiv.org/abs/1905.02161.

Xavier Glorot and Yoshua Bengio. Understanding the difﬁculty of training deep feedforward neural
networks. In Proceedings of the thirteenth international conference on artiﬁcial intelligence and
statistics, pp. 249–256, 2010.

Priya Goyal, Piotr Doll´ar, Ross Girshick, Pieter Noordhuis, Lukasz Wesolowski, Aapo Kyrola, An-
drew Tulloch, Yangqing Jia, and Kaiming He. Accurate, large minibatch sgd: Training imagenet
in 1 hour. arXiv preprint arXiv:1706.02677, 2017.

Boris Hanin and David Rolnick. How to start training: The effect of initialization and architecture.

In Advances in Neural Information Processing Systems, pp. 571–581, 2018.

Moritz Hardt and Tengyu Ma. Identity matters in deep learning. arXiv preprint arXiv:1611.04231,

2016.

Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Delving deep into rectiﬁers: Surpassing
human-level performance on imagenet classiﬁcation. arXiv preprint arXiv:1502.01852, 2015.
URL http://arxiv.org/abs/1502.01852.

Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.

Identity mappings in deep residual
networks. In Bastian Leibe, Jiri Matas, Nicu Sebe, and Max Welling (eds.), Computer Vision –
ECCV 2016, pp. 630–645, Cham, 2016. Springer International Publishing.

Sergey Ioffe and Christian Szegedy. Batch normalization: Accelerating deep network training by

reducing internal covariate shift. arXiv preprint arXiv:1502.03167, 2015.

G¨unter Klambauer, Thomas Unterthiner, Andreas Mayr, and Sepp Hochreiter. Self-normalizing
neural networks. In Advances in neural information processing systems, pp. 971–980, 2017.

Lu Lu, Yeonjong Shin, Yanhui Su, and George Em Karniadakis. Dying relu and initialization:

Theory and numerical examples. arXiv preprint arXiv:1903.06733, 2019.

Andrew L Maas, Awni Y Hannun, and Andrew Y Ng. Rectiﬁer nonlinearities improve neural net-
work acoustic models. In Proceedings of the 30th International Conference on Machine Learning-
Volume 28, 2013.

9

Dmytro Mishkin and Jiri Matas. All you need is a good init. arXiv preprint arXiv:1511.06422,

2015.

Vinod Nair and Geoffrey E Hinton. Rectiﬁed linear units improve restricted boltzmann machines. In
Proceedings of the 27th international conference on machine learning (ICML-10), pp. 807–814,
2010.

Shibani Santurkar, Dimitris Tsipras, Andrew Ilyas, and Aleksander Madry. How does batch nor-
malization help optimization? In Advances in Neural Information Processing Systems, pp. 2483–
2493, 2018.

Wenling Shang, Justin Chiu, and Kihyuk Sohn. Exploring normalization in deep residual networks
with concatenated rectiﬁed linear units. In Thirty-First AAAI Conference on Artiﬁcial Intelligence,
2017.

Andrew Shaw, Yaroslav Bulatov, and Jeremy Howard.

Imagenet in 18 minutes. 2018. URL

https://github.com/diux-dev/imagenet18.

Rupesh Kumar Srivastava, Klaus Greff, and J¨urgen Schmidhuber. Highway networks. arXiv preprint

arXiv:1505.00387, 2015.

Dmitry Ulyanov, Andrea Vedaldi, and Victor Lempitsky. Instance normalization: The missing in-

gredient for fast stylization. arXiv preprint arXiv:1607.08022, 2016.

Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez,
Łukasz Kaiser, and Illia Polosukhin. Attention is all you need. In Advances in neural information
processing systems, pp. 5998–6008, 2017.

Yuxin Wu and Kaiming He. Group normalization. In Proceedings of the European Conference on

Computer Vision (ECCV), pp. 3–19, 2018.

Hongyi Zhang, Yann N Dauphin, and Tengyu Ma. Fixup initialization: Residual learning without

normalization. arXiv preprint arXiv:1901.09321, 2019.

Jun-Yan Zhu, Taesung Park, Phillip Isola, and Alexei A Efros. Unpaired image-to-image translation
using cycle-consistent adversarial networks. In Proceedings of the IEEE international conference
on computer vision, pp. 2223–2232, 2017.

A APPENDIX

A.1 ELEMENTARY LINEAR PROGRAMMING

Proposition A.1 (Farkas’ Lemma). Let W ∈ Rm×n and b ∈ Rm.

{x | W x + b = 0, x ≥ 0} 6= ∅ ⇐⇒ ∄ λ s.t. λTW ≥ 0, λTb > 0.

Corollary A.2. Let W ∈ Rm×n and b ∈ Rm. Then

{x | W x + b ≤ 0} = ∅ ⇐⇒ ∃ λ s.t λTW = 0, λTb > 0

(λ ≥ 0).

Proof. Suppose by contradiction there exists an x such that W x + b ≤ 0. Let x+ := max{x, 0}
and x− := max{−x, 0}, and so x = x+ − x−. Then:

∃x s.t W x + b ≤ 0
⇐⇒ ∃x+, x− s.t W (x+ − x−) + b ≤ 0
⇐⇒ ∃x+, x−, λ s.t W (x+ − x−) + λ + b = 0 (λ ≥ 0)
T
⇐⇒ ¯x := (x+, x−, λ)

≥ 0, ¯W := [W | − W | I] s.t ¯W ¯x + b = 0,

which follows from Farkas’ Lemma.

10

A.2 ALGORITHM FOR RESIDUAL FARKAS LAYER

Algorithm 2 Residual Farkas layer with ReLU activation

Initialize W (1), W (2), b(1), b(2)
Forward pass: x
y = F (x; W (1), b(1))
y ← W (2)y
Compute aggregated components:
y := −AggFunc(x + y)
b := max{−AggFunc(b(2)[0 : −1]), b(2)
m }
y ← concatenate(y, y)
b ← concatenate(b(2)[0 : −1], b)
y ← y + b
y ← ReLU(y)
Optional: apply batch normalization to y
Return y

A.3 TRAINING CURVES FOR CIFAR10

⊲ e.g. Linear and/or Conv with no bias

⊲ residual component

Vanilla, with BN
Vanilla, without BN
Farkas, with BN
Farkas, without BN 

Vanilla, with BN
Vanilla, without BN
Farkas, with BN
Farkas, without BN 

100

90

80

70

60

50

40

30

0

25

50

75

100

125

150

175

200

0

50

100

150

200

250

300

(a) 18 layers

(b) 34 layers

Vanilla, with BN
Vanilla, without BN
Farkas, with BN
Farkas, without BN 

Vanilla, with BN
Vanilla, without BN
Farkas, with BN
Farkas, without BN 

100

90

80

70

60

50

40

30

20

80

70

60

50

40

30

20

10

100

90

80

70

60

50

40

30

20

0

50

100

150

200

250

300

0

50

100

150

200

250

300

(c) 50 layers

(d) 101 layers

Figure 4: Average training curves for CIFAR10; “Vanilla” refers to standard ResNet. Dotted lines
omit batch normalization, solid lines incorporate it.

11

80

70

60

50

40

30

20

10

0

25

50

75

100

125

150

175

200

Figure 5: Average training curve for 101-layer FarkasNet, with “mean” aggregation function, maxi-
mal learning rate, and without batch normalization.

12

A.4 TRAINING CURVES FOR CIFAR100

For 50-layers, one of the networks (standard ResNet without BN) failed to train, and we omit it from
the average.

Vanilla, with BN
Vanilla, without BN
Farkas, with BN
Farkas, without BN 

100

90

80

70

60

50

40

30

Vanilla, with BN
Vanilla, without BN
Farkas, with BN
Farkas, without BN 

100

90

80

70

60

50

40

30

20

0

50

100

150

200

250

300

0

50

100

150

200

250

300

(a) 34 layers

(b) 50 layers

Vanilla, with BN
Vanilla, without BN
Farkas, with BN
Farkas, without BN 

100

90

80

70

60

50

40

30

20

0

50

100

150

200

250

300

(c) 101 layers

Figure 6: Average training curves for CIFAR100; “Vanilla” refers to standard ResNet

A.5 TRAINING CURVES FOR IMAGENET-1K

Here, dotted line means Farkas layer-based network.

100

90

80

70

60

50

40

30

20

ResNet50
FarkasNet50

100

90

80

70

60

50

40

30

20

ResNet101
FarkasNet101

90

80

70

60

50

40

30

20

ResNet152
FarkasNet152

0

5

10

15

20

25

0

5

10

15

20

25

0

5

10

15

20

25

(a) 50 layers

(b) 101 layers

(c) 152 layers

Figure 7: Training curves for ImageNet

13

100

95

90

85

80

75

70

65

60

ResNet150
FarkasNet50
ResNet101
FarkasNet101
ResNet152
FarkasResNet152

0.0

0.5

1.0

1.5

2.0

2.5

3.0

3.5

4.0

