1
2
0
2
c
e
D
3

]
E
S
.
s
c
[

1
v
1
7
7
1
0
.
2
1
1
2
:
v
i
X
r
a

Characterizing Performance Bugs in Deep Learning Systems

Junming Cao

Bihuan Chen∗

Chao Sun

School of Computer Science and
Shanghai Key Laboratory of Data
Science

School of Computer Science and
Shanghai Key Laboratory of Data
Science

School of Computer Science and
Shanghai Key Laboratory of Data
Science

Fudan University

Shanghai, China

Fudan University

Shanghai, China

Fudan University

Shanghai, China

Longjie Hu

Xin Peng

School of Computer Science and
Shanghai Key Laboratory of Data
Science

School of Computer Science and
Shanghai Key Laboratory of Data
Science

Fudan University

Shanghai, China

Fudan University

Shanghai, China

ABSTRACT

Deep learning (DL) has been increasingly applied to a variety of do-
mains. The programming paradigm shift from traditional systems to
DL systems poses unique challenges in engineering DL systems. Per-
formance is one of the challenges, and performance bugs (PBs) in DL
systems can cause severe consequences such as excessive resource con-
sumption and financial loss. While bugs in DL systems have been ex-
tensively investigated, PBs in DL systems have hardly been explored.
To bridge this gap, we present the first comprehensive study to char-
acterize symptoms, root causes, and introducing and exposing stages
of PBs in DL systems developed in TensorFLow and Keras, with a
total of 238 PBs collected from 225 StackOverflow posts. Our find-
ings shed light on the implications on developing high-performance
DL systems, and detecting and localizing PBs in DL systems. We also
build the first benchmark of 56 PBs in DL systems, and assess the
capability of existing approaches in tackling them. Moreover, we
develop a static checker DeepPerf to detect three types of PBs, and
identify 488 new PBs in 130 GitHub projects. 62 and 18 of them have
been respectively confirmed and fixed by developers.

1 INTRODUCTION

The advances in deep learning (DL) have attracted an increasing in-
terest in applying DL to various applications in both industry and
academia, e.g., image processing, machine translation, speech recog-
nition, medical diagnosis, self-driving cars, and robotics. These DL sys-
tems adopt a data-driven programming paradigm, where developers
define a desired neural network that learns the decision logic from a
large amount of training data. Differently, traditional systems follow
a logic-based programming paradigm, where developers directly en-
code the decision logic in the source code. This paradigm shift poses
unique challenges to engineering DL systems [1, 7, 21, 83].

∗Bihuan Chen is the corresponding author.

In particular, performance, as an important quality requirement, is
one of the challenges in engineering DL systems [83]. It has a signifi-
cant impact on the time and resources (e.g., GPU memory and power)
required during the process pipeline (e.g., training and inference) of
DL systems [44]. For example, the language model GPT-3 costs mil-
lions of dollars for a single training run1. Thus, performance bugs (PBs)
can slow down DL systems, consume excessive resources, hurt user ex-
perience, cause financial loss, or threaten human lives. For example,
many users suffered a significant slowdown of their DL systems af-
ter upgrading TensorFlow 1.x to TensorFlow 2.x, and hence de-
cided to switch to PyTorch2. Moreover, performance questions of
DL systems are recognized as the most difficult to answer among all
questions of DL systems on StackOverflow [83]. Therefore, it is nec-
essary to study the characteristics of PBs in DL systems.

A lot of efforts have been recently made to extensively investigate
the characteristics (e.g., symptoms, root causes, fixes and taxonomy)
of general bugs [26–28, 86] and specific bugs [8, 68, 82, 85] in DL sys-
tems. However, these studies are not specifically designed for PBs,
and thus only capture some partial characteristics of PBs in DL sys-
tems. In contrast, PBs have been widely studied for traditional sys-
tems, e.g., desktop or server applications [30, 48, 62, 79], highly con-
figurable systems [24, 25], mobile applications [38, 40], database-
backed web applications [77, 78], and JavaScript systems [59]. How-
ever, PBs in DL systems could be different due to the programming
paradigm shift from traditional systems to DL systems. In summary,
the characteristics of PBs in DL systems are under-investigated.

To bridge this knowledge gap, we present the first comprehensive
study to characterize PBs in DL systems developed in TensorFlow
and Keras. We collect 238 PBs from 225 StackOverflow posts, and
manually analyze these PBs to answer three research questions.

• RQ1 Symptom: what are the symptoms of PBs?

1https://lambdalabs.com/blog/demystifying-gpt-3/
2https://stackoverflow.com/questions/58441514/why-is-tensorflow-2-much-slower-
than-tensorflow-1

 
 
 
 
 
 
• RQ2 Root Cause: what are the root causes of PBs?
• RQ3 Stage: what are the stages of introducing and exposing PBs?

with TensorFlow 2. We do not distinguish between TensorFlow
and Keras in our analysis because they are often used together.

Through these research question analysis, we aim to provide useful
findings for developers and researchers. For example, more than half of
the PBs slow down DL systems, and nearly one-third of the PBs con-
sume either extremely low or high resources. About half of the PBs
are introduced by API misuses, and root causes related to model, data
and hardware introduce more than one-third of the PBs. The most bug-
prone stages are data preparation, environment setting, model build-
ing and training. The most bug-affecting stages are training and data
preparation. 40% of the PBs are not exposed in the introducing stage.
Our findings provide implications for developers and researchers on
developing high-performance DL systems and detecting and local-
izing PBs in DL systems, e.g., performance-aware techniques to rec-
ommend DL library APIs and DL models, static techniques to model
and estimate time cost and resource consumption of DL systems,
and rule-based techniques to detect and localize PBs in DL systems.

Based on those 238 PBs, we build a benchmark of 56 PBs that cover
most symptoms and root causes. For each PB, we report its environ-
ment configuration, input data, buggy version, fixed version, the per-
formance change after fixing, and reproduction steps. Moreover, us-
ing our benchmark, we quantitatively assess the capability of a pro-
filer in detecting PBs, the capability of a compiler in optimizing PBs,
and the capability of documentation in hinting PBs. The results have
indicated that all these approaches have a very limited capability.

To demonstrate the usefulness of our findings, we develop a static
checker, named DeepPerf, that supports rule-based detection of three
types of PBs derived from our study. We run DeepPerf against 1,108
GitHub projects with more than 100 stars. DeepPerf detects 488
new PBs in 130 of these projects with 15 false positives. 62 of these
PBs have already been confirmed by the developers, and 18 of them
have already been fixed. Others are still waiting for confirmation.

In summary, this paper makes the following contributions.

• We present the first comprehensive study to characterize 238 PBs

in DL systems developed in TensorFlow and Keras.

• We build the first benchmark of 56 PBs in DL systems and assess

the capability of existing approaches in tackling them.

• We develop a static checker, named DeepPerf, to detect three types

of PBs, and detect 488 new PBs in 130 GitHub projects.

2 EMPIRICAL STUDY METHODOLOGY

We first introduce the design of our empirical study, and then present
our data collection and labeling process.

2.1 Study Design

Our goal is to characterize PBs in DL systems. As DL systems can be
built on top of various DL libraries, we limit our scope to DL systems
developed in TensorFlow and Keras. We select TensorFlow as it
is the most popular DL library on GitHub. Specifically, TensorFlow
is used in 110,641 GitHub repositories, while the second most pop-
ular DL library PyTorch is used in 69,947 GitHub repositories. We
also include Keras because it is built on top of and tightly integrated

To achieve this goal, we propose the three research questions as in-
troduced in Sec. 1. Our symptom analysis in RQ1 aims to understand
the observable consequences of PBs. Our findings from RQ1 can char-
acterize the significance of PBs, and provide insights for developing
PB detection approaches. Our root cause analysis in RQ2 aims to char-
acterize the fundamental reasons for the occurrence of PBs. Our find-
ings from RQ2 can provide insights for designing PB localization ap-
proaches. Our stage analysis in RQ3 aims to locate DL pipeline stages
where PBs are introduced and exposed, and measure the distance be-
tween exposing stage and introducing stage. Our findings from RQ3
can locate the bug-prone and bug-affecting stages that should be con-
cerned, and reflect the difficulty of PB localization. Our findings can
also provide hints to develop high-performance DL systems.

2.2 Data Collection

We collected PBs from a well-known Q&A site StackOverflow, where
world-wide developers can discuss software development problems.
Our PB collection process consists of the following three steps.

Step 1: DL Post Selection. We first selected posts related to DL
libraries TensorFlow and Keras by checking whether the tags of a
post contain the keywords “tensorflow” and “keras”. We also filtered
posts that were created before 2018-01-01 to avoid usage discussions
about old versions of DL libraries that are usually no longer used. At
the time of selection (i.e., 2021-03-01), we obtained 61,169 DL posts.
Then, we excluded posts that did not contain any source code in ques-
tion descriptions for the ease of our manual analysis. To focus on high-
quality posts, we also excluded posts that did not have an accepted
answer or any answer whose votes were greater than two because
questioners often commented that the problems had been solved, but
forgot to accept the answer. After this step, we had 18,730 DL posts.

Step 2: PB Post Selection. Instead of directly using performance-
related keywords from the existing studies on PBs in traditional sys-
tems (e.g., [30, 48, 62, 79]), we derived a keyword set in the following
way to achieve a wide and comprehensive coverage of PB posts. We
first randomly sampled 100 posts with a tag of “performance” from the
18,730 posts in Step 1. Then, we manually analyzed these posts to ex-
tract performance-related keywords, and added them to the set of key-
words from existing studies. We continued this procedure of random
sample and manual analysis for two rounds until no new performance-
related keyword is found. Finally, we used the derived keyword set to
search the question descriptions of the 18,730 posts in Stage 1, which
resulted in a total of 742 candidate PB posts.

Step 3: PB Identification. We manually verified the 742 candi-
date PB posts to reduce noise that was not about PBs in DL systems.
For example, some posts might happen to have performance-related
keywords, but did not discuss PBs; some posts actually discussed the
accuracy of DL models (because accuracy is often interchangeable
with performance in the DL community, and we align with the SE com-
munity where performance is usually referred to as efficiency); and
some posts indeed discussed performance, but did not have a correct
answer, which could not be used to understand the characteristics of

100 (42.0%) of the PBs manifest Slow Execution Time during the exe-
cution of DL systems, including data preparation, model building,
training, evaluation, hyper parameter tuning, or prediction. Further,
16 (6.7%) of the PBs exhibit Increasing Time Over Time; e.g., the pre-
diction time became longer and longer as the model ran3. Moreover,
8 (3.4%) of the PBs manifest Slow Initialization Time when DL sys-
tems are initialized before execution; e.g., it spent more than 80 sec-
onds to import TensorFlow4. DL systems can still work but slowly
when exhibiting the above symptoms. Differently, 8 (3.4%) of the PBs
result in Program Hang that makes DL systems cease to respond to
inputs, which is the most severe symptom.

Memory. This category includes PBs consuming RAM/GPU mem-
ory abnormally, accounting for 58 (24.4%) of the PBs. Specifically, Out
of Memory is the most common as well as the most severe symptom,
covering 42 (17.6%) of the PBs. Memory Leak, manifested in 16 (6.7%)
of the PBs, occurs when the memory usage keeps increasing, and may
finally lead to out of memory errors. Moreover, Abnormal GPU Mem-
ory Usage, i.e., either unexpectedly high or low GPU memory usage,
is exhibited in 5 (2.1%) of the PBs.

Processor. This category consists of PBs with abnormal CPU/GPU
utilization, which accounts for 16 (6.7%) of the PBs. In particular, Ab-
normal GPU Utilization, i.e., either unexpectedly high or low GPU uti-
lization, is manifested in 8 (3.4%) of the PBs. For example, the GPU uti-
lization was only around 15%, while the training time was slow (each
epoch took 40 to 50 seconds)5. Moreover, DL systems may Not Use
GPU, leading to no speedup than when running on CPU, which oc-
curs in 4 (1.7%) of the PBs. In addition, Abnormal CPU Utilization is
also exhibited in 4 (1.7%) of the PBs.

Summary. More than half of the PBs slow down DL systems, and
nearly one-third of the PBs consume either extremely low or high re-
sources like memory and processor. Such severe consequences of PBs
motivate the significance of PBs. Moreover, only four of the ten symp-
toms, as highlighted in dotted rectangles in Fig. 1, are shared with the
existing symptom taxonomies for general DL bugs [27, 86]. In other
words, symptoms of PBs are quite different from those of general DL
bugs, and the existing studies on general DL bugs only capture a par-
tial set of PBs, and thus PBs deserve a comprehensive investigation.

3.2 Root Cause Analysis (RQ2)

The taxonomy of PB root causes is reported in Fig. 2. It is grouped into
five high-level categories (i.e., API, Model, Library, Data and Environ-
ment) and 15 inner categories, which cause 218 (91.6%) of the 238 PBs.
The other 20 (8.4%) PBs have unclear or infrequent (which occur
once) root causes, and thus are included in the Others category.

API. This category covers PBs caused by library API misuses, which

is the most common category and accounts for 110 (46.2%) of the PBs.
Specifically, TensorFlow and Keras provide efficient APIs for achiev-
ing high performance, e.g., the tf.data API for building efficient in-
put pipelines, and various operation APIs for efficient computation.
However, developers often write their own implementation which is

Figure 1: Taxonomy of PB Symptoms

PBs. In particular, two of the authors separately inspected each can-
didate PB post to identify PBs. After completing every 100 candidate
PB posts, they investigated inconsistent cases together to reach con-
sensus. Finally, we identified 238 PBs from 225 PB posts, of which 13
PB posts contained two PBs. The scale of our data set is comparable
to previous studies on PBs, e.g., 109 PBs in desktop or server appli-
cations [30] and 70 PBs in mobile applications [40].

2.3 Data Labeling

To answer the three research questions, two of the authors labeled
each of the 238 PBs with respect to three dimensions, i.e., symptom,
root cause, and introducing and exposing stages, following an open
coding procedure [58]. We started with the classification schema,
used for labeling, from the existing general DL bug studies [26–
28, 86] and adapted it during our manual analysis by appending
new ones and excluding non-applicable ones. The differences of our
final taxonomy from the existing ones will be discussed in Sec. 3.

In particular, two of the authors separately read all post contents,
including the title, question description, comments, answers, and ref-
erence links mentioned during discussion, to carefully understand the
characteristics of PBs. When disagreement was found, they investi-
gated the inconsistent cases together to reach consensus. It is worth
mentioning that the manual effort, involved in our data collection
and labeling procedure, required six person-months.

3 EMPIRICAL STUDY RESULTS

We first present the results of our study, and then present the chal-
lenges derived from our study.

3.1 Symptom Analysis (RQ1)

The taxonomy of PB symptoms is shown in Fig. 1. It is organized into
three high-level categories (i.e., Time, Memory and Processor) and 10
inner categories, which are exhibited by 187 (78.6%) of the 238 PBs. The
remaining 51 (21.4%) PBs have no clear indication about their symp-
toms in the posts, and hence are included in the Unknown category.
Notice that a PB can exhibit multiple symptoms.

Time. This category covers PBs exhibiting high time cost, which
accounts for the largest portion of PBs, i.e., 129 (54.2%). In particular,

3https://stackoverflow.com/questions/60267911/
4https://stackoverflow.com/questions/49053434/
5https://stackoverflow.com/questions/56795642/

PerformanceBugs(238)Time(129)Memory(58)Processor (16)Unknown (51)SlowExecutionTime (100)SlowInitializationTime (8)ProgramHang (8)IncreasingTimeOverTime (16)MemoryLeak (16)AbnormalGPUMemoryUsage (5)NotUsingGPU (4)AbnormalGPUUtilization (8)AbnormalCPUUtilization (4)Out of Memory (42)Figure 2: Root Causes of PBs in DL Systems

often less efficient, but do Not Use the corresponding Efficient API di-
rectly, potentially due to the unfamiliarity with APIs. This causes 50
(21.0%) of the PBs. For example, a developer wrote a for loop to per-
form concatenation on a set of images, which could be efficiently
achieved by the map API from tf.data.Dataset6. Moreover, Ten-
sorFlow and Keras provide various batch processing APIs for high
performance, e.g., data loading, training, evaluation or prediction in
a batch mode. However, developers might Not Use a Batch API, and
some even implement batch processing by themselves, which causes
16 (6.7%) of the PBs. For example, a developer loaded a large data set
into GPU memory all at once, causing an out of memory error7. The
flow_from_directory API in Keras can solve this PB by dynami-
cally loading a batch of data from the specified directory. In the pre-
vious two root causes, developers are mostly unaware of the effi-
cient or batch APIs. However, even when developers are aware of
some APIs, they might not fully understand their performance char-
acteristics, and write Inefficient API Usage, which causes 44 (18.5%)
of the PBs. Fig. 3 shows an example of inefficient API usage, where a
developer called the map API before the batch API, and did not pass
the num_parallel_calls argument to map8, leading to a long train-
ing time. To speed up, map should be called after batch to reduce
the number of times the mapped function _batch_parser is called,
and num_parallel_calls should be passed to enable parallelism.

Model. This category consists of PBs that are related to DL mod-
els, which is the second most common category, accounting for 50
(21.0%) of the PBs. In particular, developers may have Confusion with
Computation Graph because of the unfamiliarity with the program-
ming model in TensorFlow and Keras, which causes 27 (11.3%) of the
PBs. A typical confusion is with the programming model of Tensor-
Flow 1.x, which is to first build a dataflow computation graph and then
run it repeatedly with inputs being fed to and outputs being fetched
from the graph. Developers often mix the graph construction into the
graph execution. As a result, nodes are repeatedly added to the graph,
and the graph execution becomes slower and slower. An example9
is shown in Fig. 4, where Line 14–16 builds the graph and should be
moved out of the execution loop to Line 6–8. Another common con-
fusion is with the usage of session, which owns resources like queues
and variables. However, developers repeatedly create a session in the

6https://stackoverflow.com/questions/63002205/
7https://stackoverflow.com/questions/59456128/
8https://stackoverflow.com/questions/53424152/
9https://stackoverflow.com/questions/53137115/

1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26

- def _parser ( record ) :][3]
+ def _batch_parser ( record_batch ):
-
+

parsed = tf . parse_single_example ( record , _keys_to_map )
parsed = tf . parse_example ( record_batch , _keys_to_map )
return parsed [ 'd '], parsed [ 's ']

def init_tfrecord_dataset () :

files_train = glob . glob ( DIR_TFRECORDS + '*. tfrecord ')
random . shuffle ( files_train )

with tf . name_scope ( ' tfr_iterator '):

# define data from randomly ordered files
ds = tf . data . TFRecordDataset ( files_train )
# select elements randomly from the buffer
ds = ds . shuffle ( buffer_size =10000)
# map them based on tfrecord format
ds = ds . map ( _parser )
# group elements in batch
ds = ds . batch ( BATCH_SIZE , drop_remainder = True )
# map batches based on tfrecord format
ds = ds . map ( _batch_parser , num_parallel_calls =4)
# iterate infinitely
ds = ds . repeat ()

-
-

+
+

# initialize the iterator
return ds . make_initializable_iterator ()

Figure 3: Inefficient API Usage Before and After Fix

graph execution loop without reusing, or forget to close the session.
The example in Fig. 4 also forgets to close the session, and the fix is
to use the session as a context manger at Line 11 that will automati-
cally close the session. A typical confusion in TensorFlow 2.x is with
the @tf.function decorator, which accelerates the decorated func-
tion by running it in graph mode instead of in eager mode. However,
developers often do not know where to add the decorator and how to
design the decorated function to get real speedup. Further, develop-
ers design an Inefficient Model Structure (e.g., missing convolution
and pooling layers before the flatten layer to have too many weights)
or set Improper Model Parameter (e.g., a large kernel size in a convo-
lution layer to cause a long training time). These two categories re-
spectively cause 6 (2.5%) and 12 (5.0%) of the PBs. Moreover, develop-
ers also set Improper Hyper Parameter, e.g., a large batch size to cause
an out of memory error or a small batch size to cause a long training
time. This category causes 5 (2.1%) of the PBs.

Library. This category refers to PBs caused by problems of DL li-
braries, accounting for 24 (10.1%) of the PBs. Specifically, 15 (6.3%) of
the PBs are caused by Library Bug; i.e., DL systems themselves are
correctly written, but trigger the PBs in DL libraries. For example,

API(110)Library(24)Data(21)Model(50)Hardware(13)Not UsingEfficientAPI (50)Not UsingBatchAPI (16)InefficientAPIUsage (44)LibraryBug (15)Wrong LibraryVersion (9)InefficientDataPreprocessing (3)InefficientDataTransmission (12)ImproperDataInput (6)ConfusionwithComputationGraph (27)InefficientModelStructure (6)ImproperModelParameter (5)ImproperHyperParameter (12)HardwareandLibrary Mismatch (4)ImproperConfiguration (2)HardwareandCode Mismatch (7)Others(20)PerformanceBugs(238)1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17

inp = tf . constant ([[1. ,1.]])
out = tf . constant ([[1. ,0.]])
weight = tf . Variable ([[1. ,1.] , [1. ,1.]])

optimizer = tf . train . GradientDescentOptimizer (0.1)

+ y = tf . matmul ( inp , weight )
+ loss = ( out [0][0] - y [0][0]) **2 + ( out [0][1] - y [0][1]) **2
+ train = optimizer . minimize ( loss )

- sess = tf . Session ()
+ with tf . Session () as sess :

sess . run ( tf . global_variables_initializer () )
for epoch in range (1000) :

y = tf . matmul ( inp , weight )
loss = ( out [0][0] - y [0][0]) **2 + ( out [0][1] - y [0][1]) **2
sess . run ( optimizer . minimize ( loss ))
sess . run ( train )

-
-
-
+

Summary. About half of the PBs are introduced by API misuses.
Model, data and hardware, i.e., the enabling characteristics of DL sys-
tems, introduce more than one-third of the PBs. DL libraries also in-
troduce one-tenth of the PBs. These diverse sources of root causes in-
crease the complexity of PB localization. Moreover, only seven of the
15 root causes, as shown in dotted rectangles in Fig. 2, are the same to
the previous root cause taxonomies for general DL bugs [26, 27, 86].
These differences owe to the fact that our study is focused on the per-
formance of DL systems, while the previous studies are mainly con-
centrated on the functionality of DL systems.

Figure 4: Graph Confusion Before and After Fix

3.3 Stage Analysis (RQ3)

repeated calls to model.predict (e.g., in a loop) resulted in a mem-
ory leak10, due to a memory leak persisting across multiple versions
of TensorFlow11. Moreover, Wrong Library Version causes 9 (3.8%)
of the PBs, as version restrictions have to be satisfied for full GPU us-
age. For example, a CPU version of TensorFlow is used on a GPU ma-
chine, leading to no GPU usage12; and TensorFlow 1.x is not fully sup-
ported on CUDA 11.1, resulting in a long time to start the training13.

Data. This category covers PBs related to data processing, which
accounts for 21 (8.8%) of the PBs. In particular, developers may write In-
efficient Data Transmission, e.g., loading input data over the network
during training but not directly copying them to the local storage, or
storing weight data in CPU which causes the weights copied to GPU
and the gradients copied back to CPU in each training iteration. This
category accounts for 12 (5.0%) of the PBs. Further, developers may
implement Inefficient Data Preprocessing (e.g., lack of image normal-
ization before changing an image to a tensor), which causes 3 (1.3%)
of the PBs. Moreover, Improper Input Data (e.g., improper data for-
mat or size that consumes excessive resources) causes 6 (2.5%) of the
PBs. For example, images with unnecessarily high resolution were
loaded, resulting in an out of memory error14.

Hardware. This category covers PBs related to hardware issues,
accounting for 13 (5.5%) of the PBs. Specifically, hardware may only
support part of the DL library versions, and hence Hardware and Li-
brary Mismatch causes 4 (1.7%) of the PBs. For example, a GPU with
compute capability 6.1 is not supported in TensorFlow 2.3 which
requires a GPU with compute capability 7.015. Further, to utilize the
full acceleration capability of TPU, DL systems often need specific
code design. Thus, Hardware and Code Mismatch causes 7 (2.9%) of
the PBs. For example, to use Colab TPU, the DL model need to be ex-
plicitly converted to a TPU compatible version; otherwise the train-
ing becomes extremely slow16. Moreover, hardware need proper
configuration to achieve full utilization, especially for distributed
training. Thus, Improper Configuration causes 2 (0.8%) of the PBs.
For example, the tf.distribute.Strategy API should be used to
properly configure and allocate multiple GPUs17.

10https://stackoverflow.com/questions/60267911/
11https://github.com/tensorflow/tensorflow/issues/34579/
12https://stackoverflow.com/questions/59040128/
13https://stackoverflow.com/questions/64462347/
14https://stackoverflow.com/questions/50742757/
15https://stackoverflow.com/questions/63602858/
16https://stackoverflow.com/questions/58670563/
17https://stackoverflow.com/questions/59074659/

Islam et al. [27] classify the pipeline of DL systems into six stages, i.e.,
Data Preparation, Model Building, Training, Evaluation, Hyper Para-
meter Tuning and Prediction, in their study on general DL bugs. We con-
sider these stages as the execution stages of DL systems, and further
add two new stages, identified in our data labeling, before them. The
first newly added stage is Environment Setting, where the DL envi-
ronment like libraries and hardware are properly installed and con-
figured. The second one is Initialization, where the DL system is ini-
tialized (e.g., importing libraries and initializing parameters) before
starting the execution stages. For each PB, we determine the stage that
introduces the PB by analyzing where its root cause is located, and de-
cide the stage that exposes the PB by analyzing where its symptom is
exhibited. We categorize the introducing stage or exposing stage of
a PB as Unknown when there is no clear indication in the post.

Fig. 5a reports the number of PBs introduced and exposed in each
stage, where the stage name on the 𝑥-axis is simplified to the initial
letters. Data preparation is the most bug-prone stage, which is blamed
in 92 (38.7%) of the PBs. Environment setting, model building and train-
ing are the second most bug-prone stages, respectively causing about
10% of the PBs. Hence, developers should pay more attention to these
stages to avoid the introduction of PBs, while automated PB local-
ization approaches should be specifically developed for these stages.
The other stages are less bug-prone, respectively introducing at most
5% of the PBs. On the other hand, training and data preparation are the
two most bug-affecting stages, where 99 (41.6%) and 45 (18.9%) of the
PBs are respectively exposed. Thus, developers should focus more ef-
forts on these two stages to optimize their performance, while auto-
mated PB detection approaches should be specifically developed for
these two stages. Around 7% of the PBs are respectively exposed un-
til the evaluation and prediction stages. The other stages are less bug-
affecting, which respectively expose at most 3% of the PBs.

Further, data preparation introduces more PBs than exposed. This
difference is even more severe in the other two earlier pipeline stages,
i.e., environment setting and model building. About 61% of the PBs are
introduced in the earlier four pipeline stages, around 60% of which are
exposed in the later four pipeline stages. The other way around, train-
ing exposes more PBs than introduced. This difference also holds in the
other two later pipeline stages, i.e., evaluation and prediction. Nearly
60% of the PBs are exposed in the later four pipeline stages. Thus, PBs
should be proactively detected and localized before severe conse-
quences occur so as to reduce time cost and resource consumption.
Besides, for each PB, we measure the distance between its exposing

(a) Introduced and Exposed PBs in Each Stage

(b) Distance between Exposing Stage and Introducing Stage

Figure 5: The Exposing Stage and Introducing Stage of PBs and their Distance

(a) Symptoms of the PBs Exposed in Each Stage

(b) Root Causes of the PBs Introduced in Each Stage

Figure 6: Correlation between Symptoms and Exposing Stages and between Root causes and Introducing Stages

stage and introducing stage, which is used as an indicator of the diffi-
culty of PB localization. Intuitively, the larger the distance, the more
difficult to localize a PB from its symptom to root cause. As shown in
Fig. 5b, 105 (44.1%) of the PBs are exposed and introduced in the same
stage, while 93 (39.1%) of the PBs cannot be exposed in the introduc-
ing stage. Specifically, 68 (28.6%) of the PBs are exposed two stages later.
Extremely, 4 (1.7%) of the PBs are exposed seven stages later; i.e., they
are introduced in the first stage but exposed in the last stage. Hence,
PB localization is quite challenging for a considerable amount of PBs.

Moreover, we investigate the symptom distribution of the PBs ex-
posed in each stage, which is shown in Fig. 6a. This distribution helps
pinpoint the potentially useful performance indicators for detecting
PBs exposed in different stages. For example, time-related indicators
can be valuable to detect PBs exposed in initialization, data prepara-
tion and prediction, because the most common symptom of the PBs
exposed in these stages is under the category of Time. Similarly, we
report the root cause distribution of the PBs introduced in each stage
in Fig. 6b. This distribution helps hint the potential technical solu-
tions to localize PBs introduced in different stages. For example, the
most frequent root cause of the PBs introduced in most stages is
under the category of API, and hence API misuse detection could be
developed to localize PBs introduced in these stages.

Summary. The most bug-prone stages are data preparation, envi-
ronment setting, model building and training, which introduce nearly
70% of the PBs. The most bug-affecting stages are training and data
preparation, which expose around 60% of the PBs. Nearly 40% of the

PBs cannot be exposed in the introducing stage. Moreover, we intro-
duce two new stages that are not covered in the previous stage anal-
ysis for general DL bugs [27], and investigate the introducing and ex-
posing stages that are not distinguished in the previous study [27].

3.4 Implications

We discuss the implications of our findings for developers of DL sys-
tems as well as researchers.

Developers. Our study reveals the common symptoms of PBs that
developers could pay attention to when testing and running their DL
systems for detecting potential PBs. Our study also identifies the com-
mon root causes of PBs that can be useful for developers to diagnose,
debug or fix PBs. Our study also captures the most bug-prone or bug-
affecting stages where developers could focus more efforts on to pro-
vide the most benefit for PB introduction avoidance or performance
optimization. Furthermore, our findings provide some development
suggestions. Developers should carefully read the release note and
API documentation of DL libraries to get familiar with the rich set of
library APIs and their performance characteristics. In this way, PBs
caused by the most common root cause (i.e., API misuses) might be
reduced. Developers should also be systematically trained to have a
comprehensive understanding of computation graph to build effi-
cient DL models. In this way, PBs caused by the second most com-
mon root cause (i.e., model construction) might be reduced.

Researchers. Our findings provide several implications on fu-
ture research in three directions. First, intelligent techniques for high-
performance DL system development are needed. As developers are of-
ten unaware of library APIs that are specifically designed for high per-
formance or unaware of the performance characteristics of library APIs,
DL library API recommendation techniques should be developed. To
achieve performance-aware API recommendation, a knowledge graph
of DL library APIs should be constructed based on release note, API
documentation and StackOverflow discussions with a specific focus
on modeling performance characteristics of APIs and performance
differences across library versions. To locate and replace inefficient
code snippets written from scratch by developers, semantic analysis
techniques should be developed to determine their semantic similar-
ity to existing library APIs. Apart from such intelligent techniques at
the code level, recommendation techniques should be developed to
automatically suggest DL library versions, efficient DL models and
their parameters, and environment configurations.

Second, PB detection techniques are needed. Half of the symptoms
(i.e., Increasing Time Over Time, Program Hang, Out of Memory, Mem-
ory Leak, and Not Using GPU ) can be regarded as a credible oracle for
detecting PBs in DL systems. Therefore, proactive monitoring and
prediction techniques should be developed to detect PBs as early as
possible before these severe symptoms occur. DL systems exhibiting
the other symptoms are not guaranteed to contain PBs as it is often
not clear how much time or resources a DL system should consume
to run without a PB. To solve this performance oracle problem, one
potential way is to design differential testing techniques to compare
the performance of DL systems running with different DL libraries,
different DL models, or different hardware configurations. However,
it may incur too much overhead. Hence, another potential way is to
design static techniques to model and estimate time cost or resource
consumption of DL systems so that performance bottlenecks can be
identified in advance before execution. During our manual analysis,
we find that TensorFlow has some built-in mechanism in detecting
PBs and recommending fixes by throwing a warning message, e.g.,
“WARNING: tensorflow: multiprocessing can interact badly with Ten-
sorFlow, causing nondeterministic deadlocks. For high performance
data pipelines tf.data is recommended”. However, such warning mes-
sages are only raised in 3 of the PBs, indicating the preliminary sup-
port in PB detection due to symptom and root cause diversity. Hence,
built-in mechanisms in DL libraries should be further enhanced.

Third, PB localization techniques are needed. Our study reveals that
the exposing stage of a PB is often not the introducing stage. For ex-
ample, the location that throws the error message of an out of mem-
ory error is usually not the location of the root cause. Therefore, it is
often challenging to localize PBs. During our manual analysis, we
find that developers often use logs as the clue to locate PBs. Hence,
automated log analysis techniques should be developed to smartly
insert log statements into DL systems and locate potential PBs using
log traces. Further, as API misuse is the most common root cause of
PBs, mining techniques should be designed to learn frequent API us-
age sequences and localize potential violations in DL systems. API
usage mining has been widely explored in traditional systems [57],
but it is interesting to investigate how they are applicable to PBs in
DL systems. Last but not the least, rule-based techniques should be
developed to detect and localize PBs, considering the potentially

large amount of PBs on StackOverflow or GitHub. The challenge is
to automatically derive but not manually specify the rules.

4 BENCHMARK AND ASSESSMENT

We reproduce and build a benchmark of 56 PBs from the 238 PBs in
our empirical study with four person-months effort, which can be
used to facilitate the future research on PBs in DL systems. We also
assess the capability of existing approaches in addressing them.

4.1 Benchmark Construction

We reproduce PBs on the machine with a 16-core Intel i7-7820X CPU
(3.60GHz), NVIDIA TITAN Xp GPU, 128GB RAM and 1TB SSD. Dif-
ferent PBs require different TensorFlow versions which further re-
quire different CUDA Toolkit versions to support GPU. As it is tricky
to install different CUDA versions in the same physical machine, we
use TensorFlow Docker images18. As a result, only NVIDIA GPU Dri-
ver19 is installed in the physical machine, and each docker container
has its own CUDA Toolkit version. Finally, TensorFlow Docker im-
ages ranging from version 1.12 to 1.15, and version 2.0 to 2.5 with GPU
support are covered to build our benchmark.

We first sample some PBs from the 238 PBs in our empirical study
instead of trying to reproduce all the 238 PBs due to the large manual
effort involved in reproducing PBs from StackOverflow posts. To have
a good coverage of symptoms and root causes, we sample 50% PBs from
each set of PBs that are caused by each inner category of root causes
while exhibiting each high-level category of symptoms. Then, for each
sampled PB, we reproduce it with the following three steps.

Step 1: Decide TensorFlow Version. If the TensorFlow ver-
sion is shown in the post, we use it. If not, we check whether APIs spe-
cific to TensorFlow 1.x (e.g., tf.Session) or 2.x (e.g., @tf.function)
exist in the post. If yes, we use the latest TensorFlow version of 1.x
(i.e., 1.15) or 2.x (i.e., 2.5). If not, we use TensorFlow 2.5.

Step 2: Complete Code Snippets. As developers tend to only in-
clude code fragments that are directly related to questions, code snip-
pets in the post are often incomplete. Specifically, if the buggy (or fixed)
version is executable, we complete the fixed (or buggy) version based
on it. Otherwise, we manually write missing code fragments for both
buggy and fixed versions based on question description and answer.

Step 3: Reproduce Symptoms. We execute the buggy and fixed
version to reproduce symptoms described in the post. We may change
input data size, model parameters, etc. to reproduce described symp-
toms as our hardware environment might be different from the post.
For PBs with out of memory errors, we set the maximum GPU mem-
ory limit with tf.GPUOptions such that the out of memory errors
can be reproduced even on GPUs with a larger memory.

We successfully reproduce 56 PBs. The first six columns of Table 1
present the number of reproduced PBs across root causes and symp-
toms, where the number in parentheses is the number of PBs used in
our empirical study. Overall, they cover all the root causes except for
Wrong Library Version and the three hardware relevant root causes.
18https://tensorflow.org/install/docker
19https://github.com/NVIDIA/nvidia-docker

Table 1: Benchmark PBs across Root Causes and Symptoms and Assessment Results

Profiler

XLA

App.

Par.

App.

Par.

Doc.
App.

Root Cause

API
Not Using Efficient API
Not Using Batch API
Inefficient API Usage

Model
Confusion with Computation Graph
Inefficient Model Structure
Improper Model Parameter
Improper Hyper Parameter

Library
Library Bug
Wrong Library Version

Data
Inefficient Data Transmission
Inefficient Data Preprocessing
Improper Data Input

Hardware

Others

Total

Time

13 (51)
7 (18)
1 (5)
5 (28)

9 (28)
6 (20)
0 (2)
2 (2)
1 (4)

3 (14)
3 (7)
0 (7)

2 (14)
1 (10)
0 (1)
1 (3)

0 (12)

1 (10)

Symptom

Memory

Processor Unknown

4 (23)
0 (1)
2 (9)
2 (13)

7 (19)
2 (4)
1 (2)
3 (4)
1 (9)

4 (9)
4 (8)
0 (1)

0 (4)
0 (1)
0 (1)
0 (2)

0 (1)

0 (2)

0 (10)
0 (5)
0 (1)
0 (4)

0 (0)
0 (0)
0 (0)
0 (0)
0 (0)

0 (3)
0 (0)
0 (3)

1 (3)
1 (2)
0 (0)
0 (1)

0 (0)

0 (0)

10 (36)
10 (30)
0 (1)
0 (5)

2 (5)
1 (3)
1 (2)
0 (0)
0 (0)

0 (1)
0 (1)
0 (0)

1 (1)
0 (0)
1 (1)
0 (0)

0 (0)

2 (8)

Total

27 (110)
17 (50)
3 (16)
7 (44)

17 (50)
9 (27)
2 (6)
4 (5)
2 (12)

6 (24)
6 (15)
0 (9)

3 (21)
1 (12)
1 (3)
1 (6)

0 (13)

3 (20)

6
1
2
3

8
2
1
4
1

0
0
–

1
0
1
0

–

0

28 (129)

15 (58)

1 (16)

15 (51)

56 (238)

15

1
1
0
0

1
0
0
1
0

0
0
–

0
0
0
0

–

0

2

14
13
0
1

7
6
1
0
0

0
0
–

0
0
0
0

–

2

23

3
3
0
0

1
1
0
0
0

0
0
–

0
0
0
0

–

0

4

6
2
0
4

2
0
0
0
2

0
0
–

1
1
0
0

–

0

9

They also cover all high-level symptoms, but achieve a relatively low
coverage of processor relevant symptoms. The main reasons for failed
reproduction are two-fold: i) developers provide very incomplete code
snippets in the posts, and hence it is difficult to complete the buggy or
fixed version, and ii) some PBs require specific hardware environ-
ments that are different from our machine. To foster future research
on PBs in DL systems, we record for each of the PB in our benchmark
its environment configuration, input data, buggy version, fixed ver-
sion, performance change after fixing, and reproduction steps.

4.2 Approach Assessment

A potential application of our PB benchmark is to assess the capabil-
ity of existing approaches in detecting and localizing PBs. To the best
of our knowledge, there is no PB detection and localization approach
for DL systems. Hence, we select and assess the following three typi-
cal performance analysis techniques, which can be used by develop-
ers to improve the performance of DL systems, with our benchmark.

• TensorFlow Profiler20: It is built on top of NVIDIA CUDA Profil-
ing Interface to track the performance of TensorFlow models. It
visualizes the time cost and resource consumption of various Ten-
sorFlow operations in the model, finds performance bottlenecks,
and recommends best practices to improve performance.

• XLA (Accelerated Linear Algebra)21: It is a domain-specific com-
piler that can accelerate TensorFlow models. Each TensorFlow
operation is executed by a precompiled GPU kernel implementa-
tion. XLA can compile the TensorFlow graph into a sequence of
computation kernels generated specifically for the given model,
and fuse the kernels to avoid memory operations between the ex-
ecution of different kernels to improve the performance [37].

• TensorFlow Documentation: It includes all TensorFlow API doc-
umentation22 and performance guide23 where developers can find
hints about performance problems and optimization solutions.

Generally, we assess each technique in two dimensions: i) whether
a technique is applicable to a PB (or whether a PB is in the capability
scope of a technique), and ii) whether a technique can solve a PB. The
assessment results are shown in the last five columns in Table 1.

As shown in the seventh column of Table 1, TensorFlow Profiler
is only applicable to 15 (26.8%) PBs, but is not applicable to the others
for two reasons. First, TensorFlow Profiler requires a TensorFlow
version of at least 1.14. However, some PBs are reproduced with a
lower version. Second, TensorFlow Profiler requires a full training
or evaluation process to track the performance, which is not always
available for the PBs in our benchmark. Moreover, of these 15 PBs,
TensorFlow Profiler fails to finish profiling because of out of mem-
ory errors for 9 PBs, and does not raise any warning or raises a false
warning for 4 PBs. Hence, we consider these 13 PBs as not solved by
TensorFlow Profiler. For the remaining 2 PBs, TensorFlow Pro-
filer either raises a warning but suggests a fix that achieves a smaller
performance improvement than our fixed version in the benchmark,
or helps detect the PB by reporting the most time-consuming opera-
tion but fails to raise a warning and suggest a fix. Thus, we consider
these 2 PBs as partially solved by TensorFlow Profiler, as reported
in the eighth column of Table 1. These results demonstrate that Ten-
sorFlow Profiler has limited capability in tackling PBs.

As presented in the ninth column of Table 1, XLA is applicable to
23 (41.1%) PBs. There are two reasons that XLA is not applicable to
the others. First, XLA uses just-in-time (JIT) compilation. However,
compilation errors might occur for some PBs in our benchmark. Sec-
ond, XLA is designed for optimizing the performance of Tensor-
Flow models. Thus, it is not applicable to PBs whose root causes are

20https://tensorflow.org/guide/profiler
21https://tensorflow.org/xla

22https://www.tensorflow.org/versions/r2.5/api_docs
23https://www.tensorflow.org/guide

not related to TensorFlow operations or computation graphs. Fur-
thermore, of these 23 PBs, XLA only improves the performance for 4
PBs but still achieves a smaller performance improvement than our
fixed version in the benchmark. This is reasonable because XLA is
actually not aware of the PBs, but optimizes performance by fusing
nodes in computation graphs, while our fixed version reduces the
number of nodes in computation graphs. Hence, we consider these 4
PBs as partially solved by XLA, as reported in the tenth column of
Table 1. For the other 19 PBs, XLA does not have any performance
improvement because of the small number of nodes in computation
graphs. Thus, we consider these 19 PBs as not solved by XLA. These
results indicate that PBs in DL systems often cannot be eliminated
by the compilation optimization techniques in XLA.

As shown in the last column of Table 1, TensorFlow documen-
tation is only applicable to 9 (16.1%) PBs. We consider TensorFlow
documentation as applicable as long as the documentation mentions
the optimization solution of a PB. There are two main reasons that
TensorFlow documentation is applicable to a small portion of PBs.
The first is that performance characteristics, especially non-time
characteristics, are hardly described in API documentation. The sec-
ond is that many PBs are caused by inefficient usages of multiple
APIs, but API documentation is often focused on individual API us-
ages. Although performance guide covers usages of multiple APIs,
they only cover limited APIs such as tf.data. We consider these 9
PBs as solved by TensorFlow documentation. These results show
that TensorFlow documentation provides limited support for PBs.

Summary. Efforts like profiling, compilation optimization and doc-

umentation have been devoted to optimizing the performance of DL
systems from different perspectives. However, they provide limited
capability in tackling PBs, potentially due to the lack of a compre-
hensive understanding of PBs in DL systems.

5 DETECTION

To demonstrate the usefulness of our findings, we implement a rule-
based static checker, named DeepPerf, to detect PBs in DL systems.
DeepPerf is implemented with two static analysis tools, AST24 and
Jedi25. It currently supports three types of PBs whose detection rules
are manually derived from our empirical study (Sec. 3).

5.1 Approach

We design static checkers for the following three types of PBs.

Checker 1: Repeated Node Creation. Creating the same nodes
repeatedly to a computation graph is one of the common types of PBs
under the root cause category of Confusion with Computation Graph.
DeepPerf is designed to detect node creation APIs that are called in
loops with the same argument values; e.g., the two APIs tf.matmul
and optimizer.minimize in Fig. 4. Actually, it is similar to Loop In-
variant Computation and Code Motion (LICM) optimization, which
has been well studied in classic compilers [3]. However, Grappler26,

24https://docs.python.org/3/library/ast.html
25https://github.com/davidhalter/jedi/
26https://tensorflow.org/guide/graph_optimization

Table 2: PB Detection Results of DeepPerf

Checker

Checker 1
Checker 2
Checker 3

Total

PB

77
195
216

488

Detected
Proj.

49
68
66

130

FP

15
0
0

15

Confirmed
Proj.
PB

Fixed

PB

Proj.

20
16
26

62

14
9
15

35

6
0
12

18

3
0
6

9

the default graph optimizer in TensorFlow runtime, cannot elimi-
nate this type of PBs although it has the loop optimizer.

To implement the checker, we first extract TensorFlow APIs that
may add computation graph nodes by parsing the @tf_export dec-
orators in the source code of TensorFlow Python APIs27. Then, we
manually review these APIs to exclude APIs that actually do not add
nodes (e.g., tf.assign) or APIs that produce different values given the
same inputs (e.g., tf.random.uniform). Finally, we obtain 356 APIs.

Our checker determines whether these 356 APIs are called with the
same argument values among loop iterations. To this end, it tracks vari-
ables that are changed among loop iterations, including the loop con-
trol variable, variables that are assigned in the loop body but are de-
fined outside the loop, and any variables that depend on them. It iden-
tifies APIs called without using changed variables as arguments as PBs.
Our analysis is inter-procedural. If there are functions called in the
loop, it passes changed variables to callee functions, analyzes changed
variables in callee functions, and identifies APIs called without us-
ing changed variables as arguments in callee functions.

Checker 2: Inefficient Order of batch and map. As showed in
Fig. 3, calling map before batch is not efficient, and hence batch is
suggested to be called before map to reduce the number of times the
mapped function is called. To detect such API misuse of batch and map,
our checker first identifies tf.Dataset object, and then analyzes the
call sites to check whether batch is called after map.

Checker 3: Disabled Parallelism of map and interleave. As
showed in Fig. 3, calling map without setting its num_parallel_calls
argument disables parallelism. This also holds for interleave. To de-
tect such API misuse of map and interleave, our checker identifies
tf.Dataset object, and then analyzes the call sites to check whether
map and interleave are called without setting num_parallel_calls.

5.2 Evaluation

To evaluate the effectiveness of the three checkers in DeepPerf, we
use PyGitHub28 to crawl 1,108 GitHub repositories that use Tensor-
Flow and Python and have at least 100 stars, and run DeepPerf on
these repositories. We also report detected PBs as issues to develop-
ers. The results are shown in Table 2, where the statistics about de-
tected, confirmed and fixed PBs are reported for each checker.

Specifically, Checker 1 detected 77 PBs in 49 projects. It detected
15 false positives (i.e., the fourth column in Table 2). The reason is
that we use lightweight heuristics to decide loop invariants based on
AST and Jedi, but do not use heavyweight data/control flow analysis,
for the scalability of our checker. 20 PBs in 14 projects have been
confirmed by developers, and 6 of them in 3 projects have been fixed.

27https://github.com/tensorflow/tensorflow/tree/r1.15/tensorflow/python/ops
28https://github.com/PyGithub/PyGithub

Checker 2 detected 195 PBs in 68 projects with no false positive.
16 PBs in 9 projects have been confirmed by developers, but none of
them has been fixed. The reason is that the fix requires extra effort in
vectorizing the mapped function (e.g., the _batch_parser function
in Fig. 3), which is non-trivial. In that sense, automated vectorization
is required in TensorFlow, like auto-vectorization in LLVM29.

Checker 3 detected 216 PBs in 66 projects with no false positive.
26 PBs in 15 projects have been confirmed by developers, while 12 of
them in 6 projects have already been fixed.

These results indicate that PB is a widespread problem in DL sys-
tems, and rule-based PB detection is promising. The projects that have
confirmed or fixed our detected PBs include popular ones like Keras,
TensorFlow Agents, TensorFlow Hub and Tensorforce.

Summary. The three checkers in DeepPerf detected 488 PBs in
130 projects with 15 false positives. 62 PBs in 35 projects have been con-
firmed by developers, while 18 of them in 9 projects have been fixed.

6 THREATS

We discuss the threats to our empirical study, PB benchmark, and de-
tection approach. Our study investigates PBs in DL systems written
with TensorFlow and Keras. Thus, it is not clear whether our find-
ings can generalize to DL systems developed with other DL libraries
like PyTorch. We believe it deserves a separate study to investigate
differences across DL libraries. Further, our study analyzes PBs from
StackOverflow posts. However, GitHub is another valuable source of
PBs. It is interesting to further explore PBs from GitHub to strength
our findings, which in fact requires large manual efforts as we spent
six person-months to analyze 238 PBs. Our PB detection results on
GitHub projects also indicate the potential applicability of our find-
ings. Moreover, our study involves manual analysis on PBs, which
may incur biases. To reduce them, two of the authors separately an-
alyzed the PBs and discussed inconsistent cases to reach consensus.

Our benchmark consists of 56 PBs, whose size, to be honest, is not
very large. However, considering the large human efforts involved in
constructing the benchmark, we believe it is acceptable. We are still
continuously enlarging our benchmark via reproducing those non-
sampled PBs from the 238 PBs and collecting PBs from GitHub.

Our rule-based static checker, DeepPerf, currently only supports
three types of PBs. Here, DeepPerf is not designed to cover all type
of PBs, but to demonstrate the potential of rule-based PB detection
as well as the usefulness of our findings. We plan to manually enrich
the detection rules in DeepPerf to support more PB types. In the
long run, we hope to automatically learn the detection rules.

7 RELATED WORK

We discuss the closely related work in understanding and analyzing
deep learning bugs and performance bugs.

29https://www.llvm.org/docs/Vectorizers.html#slp-vectorizer

7.1 Deep Learning Bugs

The recent success in applying deep learning techniques to a variety
of domains has gained increasing interest in understanding charac-
teristics of bugs in deep learning systems. Zhang et al. [86] collected
175 bugs in deep learning systems developed in TensorFlow from
StackOverflow posts and GitHub commits. They analyzed the symp-
toms and root causes of these bugs, and explored the challenges and
strategies in bug detection and localization. Islam et al. [27] and Hum-
batova et al. [26] expanded the scope of Zhang et al.’s study to in-
clude more deep learning libraries. Islam et al. [27] analyzed the types,
root causes, impacts and pipeline stages of 970 bugs in deep learning
systems written in Caffe, Keras, TensorFlow, Theano and Torch,
while Humbatova et al. [26] constructed a taxonomy of bugs in deep
learning systems that use TensorFlow, Keras and PyTorch based
on manual analysis of 375 bugs and interviews with 20 developers.
In their follow-up work, Islam et al. [28] analyzed bug fix patterns.
Kim et al. [33] built a benchmark of 4,577 bugs from 193 deep learn-
ing systems. Differently, Jia et al. [29] explored the symptoms, root
causes and locations of 202 bugs in the TensorFlow library.

Apart from these studies that are focused on a general scope of bugs

in deep learning systems, several recent studies have targeted more
specific bugs. Zhang et al. [82] studied failures of deep learning jobs
that are running on a remote, shared platform in Microsoft. Chen et
al. [8] investigated faults related to the deployment of deep learning
models to mobile devices. Zhang et al. [85] summarized five com-
mon training problems in deep learning systems, and developed a
tool to automatically detect and repair training problems. Wan et
al. [68] studied API misuses when deep learning systems use cloud
AI services, summarized eight misuse patterns, and developed static
checkers to automatically detect some of the misuse patterns.

Some of these existing studies reveal some partial characteristics
of performance bugs in deep learning systems. For example, Zhang
et al. [86] and Islam et al. [27] respectively recognized low efficiency
and hang as a symptom of deep learning bugs. Zhang et al. [82] iden-
tified GPU out of memory as a failure category of deep learning jobs.
Chen et al. [8] recognized memory and speed issues as two types of
faults in the inference stage of deployment process. Wan et al. [68] de-
rived four performance-related API misuse patterns of cloud AI ser-
vices. Despite these efforts, there still lacks a comprehensive study to
understand the characteristics of performance bugs in deep learning
systems, and thus our study aims to bridge this knowledge gap and
raise the awareness of PBs in DL systems.

Besides, some studies have investigated general problems and chal-
lenges in developing and deploying deep learning systems. For ex-
ample, Guo et al. [19] measured the accuracy and and performance
differences across four deep learning libraries. Zhang et al. [83] iden-
tified seven kinds of frequently asked deep learning questions in Stack-
Overflow, and analyzed their resolution difficulty and root causes.
Han et al. [21] investigated the topics that developers discuss when de-
veloping deep learning systems. Chen et al. [7] built a taxonomy of
challenges in deploying deep learning systems to different platforms
through manual analysis of StackOverflow posts. Pham et al. [54]
measured accuracy variance in training deep learning systems. Cum-
maudo et al. [11] studied the pain-points that developers face when us-
ing cloud services of computer vision by mining StackOverflow posts.

Although these studies are not designed for deep learning bugs, they

shed light on debugging and bug detection in deep learning systems.
Specifically, Guo et al. [19] reported performance differences in terms
of time cost and memory consumption when trained deep learning
models are migrated or quantized to different mobile devices and web
browsers, and called for performance optimization and testing tech-
niques. Zhang et al. [83] summarized performance as a category of
frequently asked deep learning questions in StackOverflow, and rec-
ognized that performance questions are the most difficult to answer.
Our study is inspired by these studies to systematically characterize
performance bugs in deep learning systems.

Moreover, some advances have been made to detect deep learning
bugs. For example, Zhang et al. [87] developed a static analysis ap-
proach to detect numerical bugs in neural architectures based on ab-
stract interpretation. Lagouvardos et al. [34] proposed a static analy-
sis to detect shape incompatibility errors in TensorFlow programs,
while Verma and Su [67] proposed a dynamic abstract interpreter to
catch such errors. Wardat et al. [72] developed a dynamic analysis
approach to locate faults in deep neural networks. In addition, great
efforts have been devoted to testing deep learning systems (e.g., [32,
42, 50, 51, 64, 65, 74]) and deep learning libraries (e.g., [20, 45, 53, 70,
71, 84]) for quality assurance. Zhang et al. [81] presented a compre-
hensive survey of work in this direction. However, little attention
has been received to detecting and testing performance bugs in deep
learning systems, and our study sheds light on this area.

7.2 Performance Bugs

A lot of empirical studies have characterized performance bugs from
different perspectives (e.g., root causes, discovery, diagnosis, fixing
and reporting) for desktop or server applications [30, 48, 62, 79, 88],
highly configurable systems [24, 25], mobile applications [38, 40],
database-backed web applications [77, 78], and JavaScript systems
[59]. They shed light on potential directions on performance analy-
sis (e.g., detection, profiling and testing). Our study is the first to
understand performance bugs in deep learning systems, which dif-
fers from traditional systems on the programming paradigm.

Several advances (e.g., [2, 9, 12, 22]) have been made to detect gen-
eral performance bugs using dynamic profiles from production runs.
A large body of work has proposed pattern-based techniques to de-
tect specific performance bugs such as reusable/cacheable data (e.g.,
[5, 13, 46]), inefficient/redundant loops (e.g., [14, 47, 49, 63]), and in-
efficient collections (e.g., [31, 60, 75]). Besides, a lot of techniques have
been proposed for performance testing, i.e., generating test cases to
trigger worst-case performance (e.g., [6, 36, 41, 52, 73]) and find per-
formance bugs (e.g., [18, 61, 66]). Another line of work is focused on
performance profiling techniques to identify hot paths (e.g., [4, 15,
35]) and fit a performance model to the input size (e.g., [10, 17, 80]).
These performance analysis approaches are designed for traditional
systems, and cannot be directly applied to deep learning systems.

Recently, some performance analysis approaches have been pro-
posed for deep learning systems. For example, Qi et al. [55] modeled
and estimated time cost of training deep neural networks, while Gao
et al. [16] estimated GPU memory consumption. These estimation

techniques are useful to identify potential performance bugs in ad-
vance. Liu et al. [39] measured the performance of training deep learn-
ing models on mobile devices, while Ma et al. [43] compared time
cost of JavaScript-based deep learning libraries when running deep
learning tasks in browsers. These studies empirically demonstrate
the performance differences. To reduce the memory usage of deep neu-
ral networks, Rhu et al. [56] developed a dynamic memory manager
to virtualize memory usage, while Wang et al. [69] proposed a dy-
namic GPU memory scheduler. To make deep learning models effi-
cient, Han et al. [23] used pruning and quantization to compress
models, Yan et al. [76] used a performance model to estimate the
time of distributed model training and find the optimal distributed
configuration, and Menghani [44] presented a survey in this area.
These approaches are system-level performance optimization tech-
niques, while our detection approach is at the source code level. De-
spite these efforts, the characteristics of performance bugs in deep
learning systems are still unclear, and thus our study motivates new
directions on performance analysis.

8 CONCLUSIONS

In this paper, we present the first comprehensive study to character-
ize PBs in DL systems developed in TensorFLow and Keras. More-
over, we build the first benchmark of PBs in DL systems, and assess
the capability of existing approaches in tackling them. Further, we
develop a static checker DeepPerf to detect three types of PBs, and
detect many new PBs in GitHub projects. The significance of our
study, benchmark, and detection approach is that we raise the aware-
ness of PBs in DL systems and shed light on the implications for
both developers and researchers on developing high-performance
DL systems as well as detecting and localizing PBs in DL systems.
All the study data and the source code of DeepPerf is available at
https://dlperf.github.io to foster future research.

REFERENCES

[1] Saleema Amershi, Andrew Begel, Christian Bird, Robert DeLine, Harald Gall,
Ece Kamar, Nachiappan Nagappan, Besmira Nushi, and Thomas Zimmermann.
2019. Software engineering for machine learning: A case study. In Proceedings
of the IEEE/ACM 41st International Conference on Software Engineering: Software
Engineering in Practice. 291–300.

[2] Glenn Ammons, Jong-Deok Choi, Manish Gupta, and Nikhil Swamy. 2004. Finding
and removing performance bottlenecks in large systems. In Proceedings of the
European Conference on Object-Oriented Programming. 172–196.

[3] David F. Bacon, Susan L. Graham, and Oliver J. Sharp. 1994. Compiler Trans-
formations for High-Performance Computing. ACM Comput. Surv. 26, 4 (1994),
345–420.

[4] Thomas Ball and James R Larus. 1996. Efficient path profiling. In Proceedings of
the 29th Annual IEEE/ACM International Symposium on Microarchitecture. 46–57.
[5] Suparna Bhattacharya, Mangala Gowri Nanda, Kanchi Gopinath, and Manish
Gupta. 2011. Reuse, recycle to de-bloat software. In Proceedings of the European
Conference on Object-Oriented Programming. 408–432.

[6] Jacob Burnim, Sudeep Juvekar, and Koushik Sen. 2009. WISE: Automated test
generation for worst-case complexity. In Proceedings of the IEEE 31st International
Conference on Software Engineering. 463–473.

[7] Zhenpeng Chen, Yanbin Cao, Yuanqiang Liu, Haoyu Wang, Tao Xie, and Xuanzhe
Liu. 2020. A comprehensive study on challenges in deploying deep learning based
software. In Proceedings of the 28th ACM Joint Meeting on European Software
Engineering Conference and Symposium on the Foundations of Software Engineering.
750–762.

[8] Zhenpeng Chen, Huihan Yao, Yiling Lou, Yanbin Cao, Yuanqiang Liu, Haoyu
Wang, and Xuanzhe Liu. 2021. An empirical study on deployment faults of
deep learning based mobile applications. In Proceedings of the IEEE/ACM 43rd
International Conference on Software Engineering. 674–685.

[9] Jürgen Cito, Philipp Leitner, Martin Rinard, and Harald C Gall. 2019. Interactive
production performance feedback in the IDE. In Proceedings of the IEEE/ACM
41st International Conference on Software Engineering. 971–981.
[10] Emilio Coppa, Camil Demetrescu, and Irene Finocchi. 2012.

Input-Sensitive
Profiling. In Proceedings of the 33rd ACM SIGPLAN Conference on Programming
Language Design and Implementation. 89–98.

[11] Alex Cummaudo, Rajesh Vasa, Scott Barnett, John Grundy, and Mohamed Abdel-
razek. 2020. Interpreting Cloud Computer Vision Pain-Points: A Mining Study
of Stack Overflow. In Proceedings of the ACM/IEEE 42nd International Conference
on Software Engineering. 1584–1596.

[12] Charlie Curtsinger and Emery D Berger. 2015. Coz: Finding code that counts
with causal profiling. In Proceedings of the 25th Symposium on Operating Systems
Principles. 184–197.

[13] Luca Della Toffola, Michael Pradel, and Thomas R. Gross. 2015. Performance
Problems You Can Fix: A Dynamic Analysis of Memoization Opportunities. In
Proceedings of the ACM SIGPLAN International Conference on Object-Oriented
Programming, Systems, Languages, and Applications. 607–622.

[14] Monika Dhok and Murali Krishna Ramanathan. 2016. Directed test generation to
detect loop inefficiencies. In Proceedings of the 24th ACM SIGSOFT International
Symposium on Foundations of Software Engineering. 895–907.

[15] Evelyn Duesterwald and Vasanth Bala. 2000. Software Profiling for Hot Path
Prediction: Less is More. In Proceedings of the Ninth International Conference
on Architectural Support for Programming Languages and Operating Systems.
202–211.

[16] Yanjie Gao, Yu Liu, Hongyu Zhang, Zhengxian Li, Yonghao Zhu, Haoxiang Lin,
and Mao Yang. 2020. Estimating gpu memory consumption of deep learning
models. In Proceedings of the 28th ACM Joint Meeting on European Software
Engineering Conference and Symposium on the Foundations of Software Engineering.
1342–1352.

[17] Simon F Goldsmith, Alex S Aiken, and Daniel S Wilkerson. 2007. Measuring
empirical computational complexity. In Proceedings of the the 6th joint meeting of
the European software engineering conference and the ACM SIGSOFT symposium
on The foundations of software engineering. 395–404.

[18] Mark Grechanik, Chen Fu, and Qing Xie. 2012. Automatically finding performance
problems with feedback-directed learning software testing. In Proceedings of the
34th International Conference on Software Engineering. 156–166.

[19] Qianyu Guo, Sen Chen, Xiaofei Xie, Lei Ma, Qiang Hu, Hongtao Liu, Yang Liu,
Jianjun Zhao, and Xiaohong Li. 2019. An empirical study towards character-
izing deep learning development and deployment across different frameworks
and platforms. In Proceedings of the 34th IEEE/ACM International Conference on
Automated Software Engineering. 810–822.

[20] Qianyu Guo, Xiaofei Xie, Yi Li, Xiaoyu Zhang, Yang Liu, Xiaohong Li, and Chao
Shen. 2020. Audee: Automated testing for deep learning frameworks. In Pro-
ceedings of the 35th IEEE/ACM International Conference on Automated Software
Engineering. 486–498.

[21] Junxiao Han, Emad Shihab, Zhiyuan Wan, Shuiguang Deng, and Xin Xia. 2020.
What do programmers discuss about deep learning frameworks. Empirical Soft-
ware Engineering 25, 4 (2020), 2694–2747.

[22] Shi Han, Yingnong Dang, Song Ge, Dongmei Zhang, and Tao Xie. 2012. Perfor-
mance debugging in the large via mining millions of stack traces. In Proceedings
of the 34th International Conference on Software Engineering. 145–155.

[23] Song Han, Huizi Mao, and William J. Dally. 2016. Deep Compression: Compress-
ing Deep Neural Network with Pruning, Trained Quantization and Huffman
Coding. In Proceedings of the 4th International Conference on Learning Representa-
tions.

[24] Xue Han and Tingting Yu. 2016. An empirical study on performance bugs
for highly configurable software systems. In Proceedings of the 10th ACM/IEEE
International Symposium on Empirical Software Engineering and Measurement.
1–10.

[25] Haochen He, Zhouyang Jia, Shanshan Li, Erci Xu, Tingting Yu, Yue Yu, Ji Wang,
and Xiangke Liao. 2020. CP-detector: using configuration-related performance
properties to expose performance bugs. In Proceedings of 35th IEEE/ACM Interna-
tional Conference on Automated Software Engineering. 623–634.

[26] Nargiz Humbatova, Gunel Jahangirova, Gabriele Bavota, Vincenzo Riccio, Andrea
Stocco, and Paolo Tonella. 2020. Taxonomy of real faults in deep learning sys-
tems. In Proceedings of the ACM/IEEE 42nd International Conference on Software
Engineering. 1110–1121.

[27] Md Johirul Islam, Giang Nguyen, Rangeet Pan, and Hridesh Rajan. 2019. A
Comprehensive Study on Deep Learning Bug Characteristics. In Proceedings of
the 2019 27th ACM Joint Meeting on European Software Engineering Conference
and Symposium on the Foundations of Software Engineering. 510–520.

[28] Md Johirul Islam, Rangeet Pan, Giang Nguyen, and Hridesh Rajan. 2020. Repairing
deep neural networks: Fix patterns and challenges. In Proceedings of the IEEE/ACM
42nd International Conference on Software Engineering. 1135–1146.

[29] Li Jia, Hao Zhong, Xiaoyin Wang, Linpeng Huang, and Xuansheng Lu. 2020. An
Empirical Study on Bugs Inside TensorFlow. In Proceedings of the International
Conference on Database Systems for Advanced Applications. 604–620.

[30] Guoliang Jin, Linhai Song, Xiaoming Shi, Joel Scherpelz, and Shan Lu. 2012. Un-
derstanding and Detecting Real-World Performance Bugs. In Proceedings of the
33rd ACM SIGPLAN Conference on Programming Language Design and Implemen-
tation. 77–88.

[31] Changhee Jung, Silvius Rus, Brian P. Railing, Nathan Clark, and Santosh Pande.
2011. Brainy: Effective Selection of Data Structures. In Proceedings of the 32nd
ACM SIGPLAN Conference on Programming Language Design and Implementation.
86–97.

[32] Jinhan Kim, Robert Feldt, and Shin Yoo. 2019. Guiding deep learning system
testing using surprise adequacy. In Proceedings of the IEEE/ACM 41st International
Conference on Software Engineering. 1039–1049.

[33] Misoo Kim, Youngkyoung Kim, and Eunseok Lee. 2021. Denchmark: A Bug
Benchmark of Deep Learning-related Software. In Proceedings of the IEEE/ACM
18th International Conference on Mining Software Repositories. 540–544.

[34] Sifis Lagouvardos, Julian Dolby, Neville Grech, Anastasios Antoniadis, and Yan-
nis Smaragdakis. 2020. Static analysis of shape in TensorFlow programs. In
Proceedings of the 34th European Conference on Object-Oriented Programming.
1–29.

[35] James R. Larus. 1999. Whole Program Paths. In Proceedings of the ACM SIGPLAN
1999 Conference on Programming Language Design and Implementation. 259–269.
[36] Caroline Lemieux, Rohan Padhye, Koushik Sen, and Dawn Song. 2018. Perffuzz:
Automatically generating pathological inputs. In Proceedings of the 27th ACM
SIGSOFT International Symposium on Software Testing and Analysis. 254–265.
[37] Mingzhen Li, Yi Liu, Xiaoyan Liu, Qingxiao Sun, Xin You, Hailong Yang, Zhongzhi
Luan, Lin Gan, Guangwen Yang, and Depei Qian. 2021. The Deep Learning
Compiler: A Comprehensive Survey. IEEE Transactions on Parallel and Distributed
Systems 32, 3 (2021), 708–727.

[38] Mario Linares-Vasquez, Christopher Vendome, Qi Luo, and Denys Poshyvanyk.
2015. How developers detect and fix performance bottlenecks in android apps.
In Proceedings of the IEEE international conference on software maintenance and
evolution. 352–361.

[39] Jie Liu, Jiawen Liu, Wan Du, and Dong Li. 2019. Performance analysis and
characterization of training deep learning models on mobile device. In Proceedings
of the IEEE 25th International Conference on Parallel and Distributed Systems. 506–
515.

[40] Yepang Liu, Chang Xu, and Shing-Chi Cheung. 2014. Characterizing and detect-
ing performance bugs for smartphone applications. In Proceedings of the 36th
international conference on software engineering. 1013–1024.

[41] Kasper Luckow, Rody Kersten, and Corina Păsăreanu. 2017. Symbolic com-
plexity analysis using context-preserving histories. In Proceedings of the IEEE
International Conference on Software Testing, Verification and Validation. 58–68.
[42] Lei Ma, Felix Juefei-Xu, Fuyuan Zhang, Jiyuan Sun, Minhui Xue, Bo Li, Chun-
yang Chen, Ting Su, Li Li, Yang Liu, et al. 2018. Deepgauge: Multi-granularity
testing criteria for deep learning systems. In Proceedings of the 33rd ACM/IEEE
International Conference on Automated Software Engineering. 120–131.

[43] Yun Ma, Dongwei Xiang, Shuyu Zheng, Deyu Tian, and Xuanzhe Liu. 2019.
Moving deep learning into web browser: How far can we go?. In Proceedings of
the World Wide Web Conference. 1234–1244.

[44] Gaurav Menghani. 2021. Efficient Deep Learning: A Survey on Making Deep
Learning Models Smaller, Faster, and Better. CoRR abs/2106.08962 (2021).
[45] Mahdi Nejadgholi and Jinqiu Yang. 2019. A study of oracle approximations in
testing deep learning libraries. In Proceedings of the 34th IEEE/ACM International
Conference on Automated Software Engineering. 785–796.

[46] Khanh Nguyen and Guoqing Xu. 2013. Cachetor: Detecting cacheable data to
remove bloat. In Proceedings of the 9th Joint Meeting on Foundations of Software
Engineering. 268–278.

[47] Adrian Nistor, Po-Chun Chang, Cosmin Radoi, and Shan Lu. 2015. Caramel:
Detecting and fixing performance problems that have non-intrusive fixes. In
Proceedings of the IEEE/ACM 37th IEEE International Conference on Software
Engineering. 902–912.

[48] Adrian Nistor, Tian Jiang, and Lin Tan. 2013. Discovering, reporting, and fixing
performance bugs. In Proceedings of the 10th working conference on mining software
repositories. 237–246.

[49] Adrian Nistor, Linhai Song, Darko Marinov, and Shan Lu. 2013. Toddler: Detecting
performance problems via similar memory-access patterns. In Proceedings of the
35th International Conference on Software Engineering. 562–571.

[50] Augustus Odena, Catherine Olsson, David Andersen, and Ian Goodfellow. 2019.
Tensorfuzz: Debugging neural networks with coverage-guided fuzzing. In Pro-
ceedings of the International Conference on Machine Learning. 4901–4911.
[51] Kexin Pei, Yinzhi Cao, Junfeng Yang, and Suman Jana. 2017. Deepxplore: Au-
tomated whitebox testing of deep learning systems. In proceedings of the 26th
Symposium on Operating Systems Principles. 1–18.

[52] Theofilos Petsios, Jason Zhao, Angelos D Keromytis, and Suman Jana. 2017.
Slowfuzz: Automated domain-independent detection of algorithmic complexity
vulnerabilities. In Proceedings of the 2017 ACM SIGSAC Conference on Computer
and Communications Security. 2155–2168.

[53] Hung Viet Pham, Thibaud Lutellier, Weizhen Qi, and Lin Tan. 2019. CRADLE:
cross-backend validation to detect and localize bugs in deep learning libraries. In

[76] Feng Yan, Olatunji Ruwase, Yuxiong He, and Trishul Chilimbi. 2015. Performance
modeling and scalability optimization of distributed deep learning systems. In
Proceedings of the 21th ACM SIGKDD International Conference on Knowledge
Discovery and Data Mining. 1355–1364.

[77] Junwen Yang, Cong Yan, Pranav Subramaniam, Shan Lu, and Alvin Cheung.
2018. How not to structure your database-backed web applications: a study of
performance bugs in the wild. In Proceedings of the IEEE/ACM 40th International
Conference on Software Engineering. 800–810.

[78] Junwen Yang, Cong Yan, Chengcheng Wan, Shan Lu, and Alvin Cheung. 2019.
View-centric performance optimization for database-backed web applications. In
Proceedings of the IEEE/ACM 41st International Conference on Software Engineering.
994–1004.

[79] Shahed Zaman, Bram Adams, and Ahmed E Hassan. 2012. A qualitative study on
performance bugs. In Proceedings of the 9th IEEE working conference on mining
software repositories. 199–208.

[80] Dmitrijs Zaparanuks and Matthias Hauswirth. 2012. Algorithmic Profiling. In
Proceedings of the 33rd ACM SIGPLAN Conference on Programming Language
Design and Implementation. 67–76.

[81] Jie M Zhang, Mark Harman, Lei Ma, and Yang Liu. 2020. Machine learning testing:
IEEE Transactions on Software Engineering

Survey, landscapes and horizons.
(2020).

[82] Ru Zhang, Wencong Xiao, Hongyu Zhang, Yu Liu, Haoxiang Lin, and Mao Yang.
2020. An empirical study on program failures of deep learning jobs. In Proceedings
of the IEEE/ACM 42nd International Conference on Software Engineering. 1159–
1170.

[83] Tianyi Zhang, Cuiyun Gao, Lei Ma, Michael Lyu, and Miryung Kim. 2019. An
empirical study of common challenges in developing deep learning applications.
In Proceedings of the IEEE 30th International Symposium on Software Reliability
Engineering. 104–115.

[84] Xufan Zhang, Ning Sun, Chunrong Fang, Jiawei Liu, Jia Liu, Dong Chai, Jiang
Wang, and Zhenyu Chen. 2021. Predoo: precision testing of deep learning
operators. In Proceedings of the 30th ACM SIGSOFT International Symposium
on Software Testing and Analysis. 400–412.

[85] Xiaoyu Zhang, Juan Zhai, Shiqing Ma, and Chao Shen. 2021. AUTOTRAINER: An
Automatic DNN Training Problem Detection and Repair System. In Proceedings
of the IEEE/ACM 43rd International Conference on Software Engineering. 359–371.
[86] Yuhao Zhang, Yifan Chen, Shing-Chi Cheung, Yingfei Xiong, and Lu Zhang. 2018.
An empirical study on TensorFlow program bugs. In Proceedings of the 27th ACM
SIGSOFT International Symposium on Software Testing and Analysis. 129–140.
[87] Yuhao Zhang, Luyao Ren, Liqian Chen, Yingfei Xiong, Shing-Chi Cheung, and
Tao Xie. 2020. Detecting numerical bugs in neural network architectures. In
Proceedings of the 28th ACM Joint Meeting on European Software Engineering
Conference and Symposium on the Foundations of Software Engineering. 826–837.
[88] Yutong Zhao, Lu Xiao, Xiao Wang, Lei Sun, Bihuan Chen, Yang Liu, and Andre B
Bondi. 2020. How Are Performance Issues Caused and Resolved?-An Empirical
Study from a Design Perspective. In Proceedings of the ACM/SPEC International
Conference on Performance Engineering. 181–192.

Proceedings of the IEEE/ACM 41st International Conference on Software Engineering.
1027–1038.

[54] Hung Viet Pham, Shangshu Qian, Jiannan Wang, Thibaud Lutellier, Jonathan
Rosenthal, Lin Tan, Yaoliang Yu, and Nachiappan Nagappan. 2020. Problems and
opportunities in training deep learning software systems: an analysis of vari-
ance. In Proceedings of the 35th IEEE/ACM International Conference on Automated
Software Engineering. 771–783.

[55] Hang Qi, Evan R Sparks, and Ameet Talwalkar. 2016. Paleo: A performance
model for deep neural networks. In Proceedings of the 5th International Conference
on Learning Representations.

[56] Minsoo Rhu, Natalia Gimelshein, Jason Clemons, Arslan Zulfiqar, and Stephen W
Keckler. 2016. vDNN: Virtualized deep neural networks for scalable, memory-
efficient neural network design. In Proceedings of the 49th Annual IEEE/ACM
International Symposium on Microarchitecture. 1–13.

[57] Martin P Robillard, Eric Bodden, David Kawrykow, Mira Mezini, and Tristan
Ratchford. 2012. Automated API property inference techniques. IEEE Transactions
on Software Engineering 39, 5 (2012), 613–637.

[58] Carolyn B. Seaman. 1999. Qualitative methods in empirical studies of software

engineering. IEEE Transactions on Software Engineering 25, 4 (1999), 557–572.

[59] Marija Selakovic and Michael Pradel. 2016. Performance issues and optimizations
in javascript: an empirical study. In Proceedings of the 38th International Conference
on Software Engineering. 61–72.

[60] Ohad Shacham, Martin Vechev, and Eran Yahav. 2009. Chameleon: Adaptive
Selection of Collections. In Proceedings of the 30th ACM SIGPLAN Conference on
Programming Language Design and Implementation. 408–418.

[61] Du Shen, Qi Luo, Denys Poshyvanyk, and Mark Grechanik. 2015. Automating
performance bottleneck detection using search-based application profiling. In
Proceedings of the 2015 International Symposium on Software Testing and Analysis.
270–281.

[62] Linhai Song and Shan Lu. 2014. Statistical Debugging for Real-World Performance
Problems. In Proceedings of the 2014 ACM International Conference on Object
Oriented Programming Systems Languages & Applications. 561–578.

[63] Linhai Song and Shan Lu. 2017. Performance diagnosis for inefficient loops. In
Proceedings of the IEEE/ACM 39th International Conference on Software Engineering.
370–380.

[64] Youcheng Sun, Min Wu, Wenjie Ruan, Xiaowei Huang, Marta Kwiatkowska, and
Daniel Kroening. 2018. Concolic testing for deep neural networks. In Proceedings
of the 33rd ACM/IEEE International Conference on Automated Software Engineering.
109–119.

[65] Yuchi Tian, Kexin Pei, Suman Jana, and Baishakhi Ray. 2018. Deeptest: Automated
testing of deep-neural-network-driven autonomous cars. In Proceedings of the
40th international conference on software engineering. 303–314.

[66] Saeid Tizpaz-Niari, Pavol Čern`y, and Ashutosh Trivedi. 2020. Detecting and
understanding real-world differential performance bugs in machine learning
libraries. In Proceedings of the 29th ACM SIGSOFT International Symposium on
Software Testing and Analysis. 189–199.

[67] Sahil Verma and Zhendong Su. 2020. ShapeFlow: Dynamic Shape Interpreter for

TensorFlow. CoRR abs/2011.13452 (2020).

[68] Chengcheng Wan, Shicheng Liu, Henry Hoffmann, Michael Maire, and Shan Lu.
2021. Are Machine Learning Cloud APIs Used Correctly?. In Proceedings of the
IEEE/ACM 43rd International Conference on Software Engineering. 125–137.
[69] Linnan Wang, Jinmian Ye, Yiyang Zhao, Wei Wu, Ang Li, Shuaiwen Leon Song,
Zenglin Xu, and Tim Kraska. 2018. Superneurons: Dynamic GPU memory man-
agement for training deep neural networks. In Proceedings of the 23rd ACM
SIGPLAN symposium on principles and practice of parallel programming. 41–53.
[70] Song Wang, Nishtha Shrestha, Abarna Kucheri Subburaman, Junjie Wang, Moshi
Wei, and Nachiappan Nagappan. 2021. Automatic Unit Test Generation for
Machine Learning Libraries: How Far Are We?. In Proceedings of the IEEE/ACM
43rd International Conference on Software Engineering. 1548–1560.

[71] Zan Wang, Ming Yan, Junjie Chen, Shuang Liu, and Dongdi Zhang. 2020. Deep
learning library testing via effective model generation. In Proceedings of the 28th
ACM Joint Meeting on European Software Engineering Conference and Symposium
on the Foundations of Software Engineering. 788–799.

[72] Mohammad Wardat, Wei Le, and Hridesh Rajan. 2021. DeepLocalize: Fault
Localization for Deep Neural Networks. In Proceedings of the IEEE/ACM 43rd
International Conference on Software Engineering. 251–262.

[73] Jiayi Wei, Jia Chen, Yu Feng, Kostas Ferles, and Isil Dillig. 2018. Singularity:
Pattern fuzzing for worst case complexity. In Proceedings of the 2018 26th ACM
Joint Meeting on European Software Engineering Conference and Symposium on
the Foundations of Software Engineering. 213–223.

[74] Xiaofei Xie, Lei Ma, Felix Juefei-Xu, Minhui Xue, Hongxu Chen, Yang Liu, Jianjun
Zhao, Bo Li, Jianxiong Yin, and Simon See. 2019. Deephunter: a coverage-guided
fuzz testing framework for deep neural networks. In Proceedings of the 28th ACM
SIGSOFT International Symposium on Software Testing and Analysis. 146–157.
[75] Guoqing Xu and Atanas Rountev. 2010. Detecting Inefficiently-Used Containers to
Avoid Bloat. In Proceedings of the 31st ACM SIGPLAN Conference on Programming
Language Design and Implementation. 160–173.

