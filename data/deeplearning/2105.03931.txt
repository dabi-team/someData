1
2
0
2

y
a
M
9

]

G
L
.
s
c
[

1
v
1
3
9
3
0
.
5
0
1
2
:
v
i
X
r
a

Automated Decision-based Adversarial Attacks

Qi-An Fu, Yinpeng Dong, Hang Su, Jun Zhu
Dept. of Comp. Sci. & Tech., Institute for AI, BNRist Center
{fqa19,dyp17}@mails.tsinghua.edu.cn, {suhangss,dcszj}@mail.tsinghua.edu.cn

Abstract

Deep learning models are vulnerable to adversarial examples, which can fool a
target classiﬁer by imposing imperceptible perturbations onto natural examples.
In this work, we consider the practical and challenging decision-based black-box
adversarial setting, where the attacker can only acquire the ﬁnal classiﬁcation
labels by querying the target model without access to the model’s details. Under
this setting, existing works often rely on heuristics and exhibit unsatisfactory
performance. To better understand the rationality of these heuristics and the
limitations of existing methods, we propose to automatically discover decision-
based adversarial attack algorithms. In our approach, we construct a search space
using basic mathematical operations as building blocks and develop a random
search algorithm to efﬁciently explore this space by incorporating several pruning
techniques and intuitive priors inspired by program synthesis works. Although
we use a small and fast model to efﬁciently evaluate attack algorithms during
the search, extensive experiments demonstrate that the discovered algorithms are
simple yet query-efﬁcient when transferred to larger normal and defensive models
on the CIFAR-10 and ImageNet datasets. They achieve comparable or better
performance than the state-of-the-art decision-based attack methods consistently.

1

Introduction

Deep learning models have demonstrated impressive performance on various pattern recognition
tasks [29, 26, 16, 22]. However, these models are vulnerable to adversarial examples [38, 21], which
are maliciously crafted by adding small adversarial perturbations to natural images but can fool the
target model. A number of adversarial attack methods have been developed [21, 33, 27, 5] to generate
adversarial perturbations under various threat models, which help to identify the vulnerabilities of
deep learning models and serve as a surrogate to evaluate adversarial robustness [7, 18].

With the rapid development of adversarial attack and defense methods, it is of great importance
to evaluate the existing methods correctly and reliably [8, 18, 14]. It sometimes needs carefully
designed adaptive attacks to evaluate the worst-case robustness of a particular defense in case of
gradient obfuscation [3, 41]. Those attack methods were manually designed by experts case by case,
which requires considerable trial-and-error efforts. One may hope to automatically discover attack
methods to reduce this burden, which can not only serve as a reasonable baseline for measuring the
strength of human designed attack methods but also examine the rationality of assumptions made in
these attack methods.

Such a desire to automate the attacks becomes even urgent when we consider the practical yet
challenging setting of decision-based black-box attacks, where the attacker can only query the target
model for the ﬁnal classiﬁcation labels. Although various decision-based attack methods have
been proposed [5, 10, 17, 6, 11], many of them are much more heuristic and exhibit unsatisfactory
performance, as compared to gradient-based white-box attack methods which could be optimal in
some sense [32]. Such a gap urges the need for automated attack methods more than other threat

Preprint. Under review.

 
 
 
 
 
 
models, to understand the rationality of these heuristics, and even to discover new methods with
better performance.

The problem of automatically discovering adversarial attack algorithms falls into the general direction
of program synthesis, which aims to automatically discover a program satisfying a user intent
speciﬁcation [25]. Many generic methodologies and techniques have been developed for program
synthesis, with the majority focusing on software problems [36, 24, 4]. One the other side, the task
of automating the process of solving machine learning problems is known as automated machine
learning (AutoML) [20]. One most attractive direction of AutoML is neural architecture search
(NAS), which aims to automatically discover good architectures of deep networks [45], while existing
methods often start with expert designed layers. AutoML-Zero [35] moves one-step further and shows
the promise to search for a complete classiﬁcation algorithm (e.g., two-layer neural networks) from
scratch using basic mathematical operations as building blocks with minimal human participation.
However, the discovered classiﬁcation algorithms are still far behind the current practice.

In this work, we propose to solve a practical yet challenging problem of decision-based adversarial
attack with competitive performance by automatically searching for attack algorithms. We call
our approach Automated Decision-based Attacks (AutoDA). Technically, we design a search space
constructed from basic mathematical operations, which provides sufﬁcient expressiveness for the
decision-based attack problem with affordable complexity. Similar search spaces are used by many
program synthesis works aiming to automatically solve software problems [24]. Thus we adapt useful
methodologies and techniques from program synthesis to AutoDA. For example, we use an algorithm
template [37] to alleviate the difﬁculty of the search problem and use pruning techniques based on
logical constraints [25] imposed by the algorithm template and the adversarial attack problem. The
design choice of constructing search space from basic mathematical operations is also similar to the
previous mentioned AutoML-Zero [35]. However, due to the theoretical and practical differences
between our problem and AutoML-Zero’s, AutoDA settles on quite different design choices and
implementations, e.g., we use the static single assignment (SSA) form instead of the three-address
code (TAC) form used in AutoML-Zero to deﬁne the search space for better sample efﬁciency and
computational performance in our use case of generating random programs, as detailed in Section 3.2.

To explore this search space efﬁciently, we develop a random search algorithm combined with several
pruning techniques and intuitive priors. To further reduce computational cost, we utilize a small and
fast model for evaluating attack algorithms during the search. Despite the simplicity of the discovered
top performing algorithms on this small model, they are also query-efﬁcient when transferred to larger
models and share common operation sequence with existing attack methods, which illustrates the
rationality of some heuristics in existing works. Our discovered algorithms consistently demonstrate
comparable or better performance than the state-of-the-art decision-based methods when attacking
normal and defensive models on the CIFAR-10 [28] and ImageNet [15] datasets.

2 Related Work

Adversarial attacks and defenses. After deep learning models have been found to be vulnerable
to adversarial examples [38, 21], lots of attack methods under different threat models have been
developed recently. In general, existing attacks can be categorized into the white-box and black-box
attacks. Under the white-box setting, the attacker has full knowledge about the target model, and thus
various gradient-based attack methods can be applied, such as the fast gradient sign method (FGSM)
[21], the projected gradient descent (PGD) method [32], and the C&W method [7]. In contrast, under
the black-box threat model, the attacker has limited access to the target model. For example, under
the score-based black-box threat model, the attacker can only acquire the output probabilities of the
black-box model with a limited number of queries [27, 42, 12]. The decision-based black-box setting
is more challenging because the attacker can only obtain the ﬁnal classiﬁcation labels by querying the
target model [5, 17, 6, 10, 11]. This setting is yet more practical in real-world scenarios [5]. Due to
the security threat, various defense methods have been proposed to defend against adversarial attacks
[32]. However, many of them cause obfuscated gradients and can be broken by adaptive attacks [3].
Currently, the most effective defense methods are based on adversarial training [18, 32].

Program synthesis. Our approach also relates to program synthesis, whose core problem is to
generate a program that meets an intent speciﬁcation [25]. Many program synthesis works use
traditional techniques, e.g., SKETCH [36] solves the programming by sketching problem using SAT

2

solver and Brahma [24] can efﬁciently discover highly nontrivial up to 20 lines loop-free bitvector
programs from basic bitvector operations using SMT solver. Recent works may use machine learning
techniques, e.g., DeepCoder [4] solves the programming by example problem using neural-guided
search. These works mainly focus on software problems instead of machine learning problems. Static
analysis techniques are essential in these works and they are also useful for our AutoDA, as detailed
below.

3 Methods

In this section, we present AutoDA in detail. For simplicity, we particularly focus on untargeted
attacks in this work, where the attacker aims to cause misclassiﬁcation on the victim classiﬁer.
Nevertheless, our approach can be extended to targeted attacks straightforwardly.

3.1 Overview

Discovering an algorithm that satisﬁes an intent speciﬁcation is an undecidable problem in general
[25], and thus is extremely hard. One approach to reduce the difﬁculty of this problem is to provide a
template for the algorithm, which reduces the problem down to searching for missing components in
this template [37]. Inspired by this approach, we choose the random walk framework for decision-
based attacks under the (cid:96)2 norm as our algorithm template. This framework is ﬁrst proposed by the
Boundary attack [5] and used by many later decision-based attacks [17, 6].

As outlined in Alg. 1, the random walk process starts at an adversarial starting point x1, which
could be obtained by keeping adding different large random noises to the original example x0 until
ﬁnding one that causes misclassiﬁcation. In each iteration of the random walk, the attacker executes
the generate() function to generate the next random point x(cid:48) to walk to based on the original
example x0 and the best adversarial example x already found. x(cid:48) is usually generated by applying
transformations to a Gaussian noise. If x(cid:48) is adversarial and is closer to x0 than the old adversarial
example x, we update the adversarial example x to x(cid:48) since we found a better adversarial example
with a smaller perturbation. There are also some hyperparameters inside the generate() function
controlling the step size of the random walk process. After each iteration, the framework would
collect the success rate of whether x(cid:48) is adversarial and adjust the hyperparameters according to the
success rate of several past trials.

There are two missing components in this template — the generate() function and the hyper-
parameter adjustment strategy. The main difference between existing attack methods lies in their
generate() functions, while their strategies for hyperparameter adjustment are similar, i.e., they all
adjust their hyperparameters to make the step size smaller when the success rate is too low and vice
versa. Without loss of generality, we only search for the generate() function to make our problem
easier, and settle on a predeﬁned negative feedback strategy for adjusting hyperparameters similar to
existing works, as detailed in the supplementary material.

To solve our problem, we follow the generic methodology from program synthesis: deﬁne a search
space for the generate() function, design a search method, and search for programs with top
performance under some designed evaluation metrics. Before diving into the details, we provide
an overview of AutoDA ﬁrst: (1) We choose a generic search space constructed from basic scalar
and vector mathematical operations which provides sufﬁcient expressiveness for our problem; (2)
We use random search combined with several pruning techniques and intuitive priors to efﬁciently
explore the search space; (3) We evaluate programs with a small and fast model on a small number of
samples to reduce computational cost. The whole system of AutoDA is complex. We will elaborate
design choices as well as important implementation details for the rest of this section and include
more implementation details in the supplementary material.

3.2 Search Space

Designing a search space is the art of trading off between expressiveness and complexity [25]. On one
hand, the search space should be expressive enough to include useful programs for the target problem.
On the other hand, great expressiveness does not come for free — it usually leads to high complexity.
Searching over a complex space is both time-consuming and hard to implement. Instead of using a
full-featured programming language like Python which provides more-than-needed expressiveness

3

Algorithm 1 Random walk framework for decision-based attacks under the (cid:96)2 norm.

−

x
x1; dmin ← (cid:107)

Data: original example x0, adversarial starting point x1;
Result: adversarial example x such that the distortion
x
(cid:107)
x
x0(cid:107)2;
←
while query budget is not reached do
x(cid:48)
generate(x, x0);
if x(cid:48) is adversarial and
x(cid:48); dmin ← (cid:107)
x

←
x(cid:48)
(cid:107)
−
x
x0(cid:107)2;
−
end if
update the success rate of whether x(cid:48) is adversarial;
adjust hyperparameters according to the success rate;

x0(cid:107)2 < dmin then

←

x0(cid:107)2 is minimized;

−

end while

with high complexity, we choose to design a domain speciﬁc language (DSL) specialized for our
problem that provides sufﬁcient expressiveness with relative low complexity.

We list all the available operations in our AutoDA DSL in Table 1. These operations are basic
mathematical operations for scalars and vectors, and all vector operations have geometric meaning
in the Euclidean space. Then we construct our search space for the generate() function as all
valid static single assignment (SSA) form programs in this DSL with a given length and a given
number of hyperparameters. In our use case of generating random programs, we choose the SSA
form widely used in modern compilers [30] instead of the three-address code (TAC) form used in
AutoML-Zero [35] for better sample efﬁciency and computational performance. Although the SSA
and TAC forms are equivalent in the sense that they can be converted to each other, when generating
random programs in the SSA form, we can enforce many wanted properties of these programs
explicitly and straightforwardly, e.g., limiting the number of hyperparameters and avoiding unused
inputs and operations. In contrast, for the TAC form, we need to generate programs ﬁrst, then check
their properties and reject the failed ones. Moreover, checking a TAC form program requires almost
as much work as converting it into an equivalent SSA form program. Consequently, this generate-
then-check process hurts sample efﬁciency and computational performance. It is worth noting that
the AutoDA DSL only requires all vector variables to have the same dimension but does not restrict
them to a speciﬁc dimension. This property of our DSL preserves the possibility for transferring
the discovered programs to other datasets with different image shapes without modiﬁcation, though
hyperparameter’s initial values might need extra tuning after changing the input dimension.

We design the program to accept three parameters: x and x0 as in the generate(x, x0) function
(0, I).
from Alg. 1, as well as a random noise n sampled from the standard Gaussian distribution
Instead of providing operations for generating random noise in the AutoDA DSL, we provide the
random noise as a parameter n. Combining with the SSA form, the program itself would be pure and
more handy to do property testing efﬁciently.

N

The above designed AutoDA DSL has sufﬁcient expressiveness for the decision-based adversarial at-
tack under the (cid:96)2 norm problem. For example, we can implement the Boundary attack’s generate()
function with our AutoDA DSL. We provide one possible implementation of it in the supplementary
material. On the other hand, the AutoDA DSL does not have high complexity since it has no control
ﬂow and has only ten unary and binary operations. However, this search space is still huge, because
its size grows at least exponentially with the length of the program. As a result, we need to design
and implement an efﬁcient search method.

3.3 Search Method

Searching for programs is a combinatorial optimization problem, because the search space is ﬁnite
when ignoring the initial values of hyperparameters. In this work, we develop a random search based
method combined with several pruning techniques and intuitive priors. We choose random search due
to several reasons. First, from a theoretical perspective, the no free lunch theorems for optimization
[43] imply that random search is on average a good baseline method for combinatorial optimization.
For example, random search based methods are shown to be competitive baselines in NAS [31].
Second, from a practical perspective, random search is much simpler to implement efﬁciently and
correctly than other methods, e.g., evolutionary search, and it can be surprisingly effective [4] when

4

Table 1: List of available operations in the AutoDA DSL. The sufﬁx of each operation’s notation
indicates the parameters’ type of the operation, where S denotes scalar type, and V denotes vector type.
For example, the VS sufﬁx means the operation’s ﬁrst parameter is a scalar and second parameter is a
vector. Detailed deﬁnitions are provided in the supplementary material.

ID

1
2
3
4
5
6
7
8
9
10

Notation

Description

ADD.SS
SUB.SS
MUL.SS
DIV.SS
ADD.VV
SUB.VV
MUL.VS
DIV.VS
DOT.VV
NORM.V

scalar-scalar addition
scalar-scalar subtraction
scalar-scalar multiplication
scalar-scalar division
vector-vector element-wise addition
vector-vector element-wise subtraction
vector-scalar broadcast multiplication
vector-scalar broadcast division
vector-vector dot product
vector (cid:96)2 norm

combined with other techniques, e.g., pruning techniques. Finally, random search can run in parallel
by its nature, which helps us easily distribute tasks to multiple machines. For the hyperparameters,
since the framework would adjust them automatically during the random walk process, we initialize
them to a given ﬁxed value to reduce implementation complexity and computational cost.

Unlike NAS works, in which the search spaces are usually constructed from expert-designed layers
such that good architectures are dense in them, AutoDA’s search space is constructed from a
generic DSL such that good programs should be quite sparse. Naive random search would waste
most computation on meaningless programs. We mitigate this issue by introducing four techniques
specialized for the decision-based attack problem from two aspects — the random program generating
process and the search process. We will conduct an ablation study on these four techniques to show
their effectiveness in Section 4.3.

For the random program generating process, we apply two intuitive priors to improve the quality of
the generated programs: (1) Compact program: We use a program generating algorithm that prefers
to generate programs with less unused operations. It is noted that our algorithm could still generate
programs with many unused operations, but with a lower probability. (2) Predeﬁned operations: We
(cid:107)2, and u = v/d to the program before randomly
v
add three predeﬁned operations v = x0 −
(cid:107)
generating the remaining operations. These predeﬁned operations are common for decision-based
attacks under the (cid:96)2 norm, because the program needs to minimize the distance between x0 and x.
Thus the distance d between x and x0 and the direction u from x to x0 should be useful. These
operations all appear at the very beginning of many existing methods, including the Boundary attack
[5], the Evolutionary attack [17], and the state-of-the-art Sign-OPT attack [11]. Again, programs left
these predeﬁned operations unused could still be generated, but with a lower probability. Without
reducing the size of our search space, both techniques just add priors to the generating process and
increase the probability of generating better programs.

x, d =

For the search process, we apply two pruning techniques to ﬁlter out trivially meaningless programs
based on constraints imposed by the decision-based attack problem and the random walk algorithm
template, including: (1) Inputs check: We ﬁlter out programs that do not make use of all inputs,
because they would be meaningless for the decision-based attack problem. This property is checked
formally with some basic static analysis techniques [2]. (2) Distance test: We ﬁlter out programs
that generate x(cid:48) violating the inequality
x0(cid:107)2 < dmin required by the framework in Alg. 1.
However, formally checking this property is extremely hard. Instead, we test this property on ten
different inputs and ﬁlter out programs that fail in any of these tests. This informal test does not
guarantee the inequality to hold for all inputs, but it works well in practice. The inputs check and
the distance test are both done on CPU cores. By ﬁltering out bad programs before they reach GPU,
much less programs need to be evaluated on GPU. We will show that they save lots of expensive
GPU computational cost for us in Section 4.1.

x(cid:48)

−

(cid:107)

5

3.4 Evaluation Method

The last step is to deﬁne evaluation metrics such that we can distinguish good programs from bad
ones. When evaluating the performance of decision-based attacks, we usually run them against many
large deep models on different datasets to generate adversarial example for each sample in the test
set. However, as running large models and attacking all samples in the test set are computationally
expensive, this kind of evaluation is time-consuming and impractical for our problem with a huge
search space. To address this issue, we leverage two strategies to make the evaluation fast and
cheap. First, we adopt a shrunk by a factor of 0.5 version of EfﬁcientNet-B0 [40] for evaluation.
EfﬁcientNets are small and fast deep models that achieve high accuracies on various benchmarks. We
train different binary classiﬁers for each pair of labels on the CIFAR-10 dataset. These classiﬁers can
process more than 60,000 images per second on a single GTX 1080 Ti GPU. Second, we evaluate
programs on a handful of examples and take an average over the evaluation metrics to save GPU time.
Instead of using an absolute (cid:96)2 distance between the original example x0 and the best adversarial
x0(cid:107)2 as the metric where
example x the program found, we use a relative distance
x1 is the adversarial starting point as in Alg. 1. We call it (cid:96)2 distortion ratio. A lower (cid:96)2 distortion
ratio means a better program.

x0(cid:107)2/

x1 −
(cid:107)

x
(cid:107)

−

Even with the small and fast classiﬁer, running programs for tremendous random walk iterations
is still unbearable computationally expensive. However, adopting lots of iterations is necessary for
hyperparameters adjustment strategies to take effect in existing methods [5, 17]. To mitigate this
issue, we ﬁrst evaluate programs for a small number of iterations and select several top performing
programs according to the evaluation metric. Then we perform a second round of evaluation of
these programs for a larger number of iterations. This evaluation strategy would also prefer choosing
programs that achieve relatively high query-efﬁciency within few iterations. At the initial stage of
the random walk process, the success rate is usually high, and thus the hyperparameters adjustment
strategy tends to overshoot and harm the performance. So we disable hyperparameters adjustment in
the small evaluation.

We generate random programs in the SSA form as described in Section 3.2 and Section 3.3. Though
SSA form programs are easy to analyze, they are slow and memory-inefﬁcient to run. To make our
SSA form programs run faster and occupy less memory, we compile them into their equivalent TAC
form programs. During the compilation, we discard unused operations and allocate memory slots
efﬁciently, such that the output TAC form programs are usually shorter and thus run faster with less
memory usage than the original SSA form programs.

4 Experiments

In this section, we ﬁrst run AutoDA to search for top performing programs under the evaluation metric
on the small classiﬁers as described in Section 3.4. We then compare the discovered algorithms with
human designed attacks against different models on CIFAR-10 and ImageNet. Finally, we conduct an
ablation study for the four techniques used in the search method of AutoDA proposed in Section 3.3
to show their effectiveness.

4.1 Searching for Programs

We ﬁrst introduce the detailed settings. For the search space, we limit the maximum length of the
program to 20 (i.e., the length of the Boundary attack’s generate() function in AutoDA DSL). We
allow one scalar hyperparameter and set it to 0.01 initially. We use the binary classiﬁer for class 0
(airplane) and class 1 (automobile) of the CIFAR-10 dataset described in Section 3.4. For the search
method and the evaluation method, we ﬁrst generate programs randomly with all the four techniques
introduced in Section 3.3, then evaluate these programs in batches for 100 iterations to calculate their
(cid:96)2 distortion ratios. Each batch of programs is evaluated on one randomly selected example from
the CIFAR-10 test set such that the (cid:96)2 distortion ratios of these programs in the same batch can be
compared with each other. The batch size here is 150, which achieves optimal performance on our
hardware. Second, we select the best program with lowest (cid:96)2 distortion ratio from each batch of
programs and evaluate it for 10,000 iterations on ten ﬁxed examples from the CIFAR-10 test set to
obtain their ﬁnal (cid:96)2 distortion ratios. Since the ten examples are ﬁxed, these ﬁnal (cid:96)2 distortion ratios
can be compared with each other.

6

Figure 2: Distribution of the lowest (cid:96)2 dis-
tortion ratio found in each of the 50 runs of
searching for programs in our experiment.

Figure 1: The SSA form programs of AutoDA 1st
and 2nd, where s0 is the hyperparameter, v1 is the
original example x0, v2 is the adversarial example x
the random walk process already found, and v3 is the
standard Gaussian noise n. The return value of these
programs is the next random point to walk to. The
s-preﬁx in variable’s name means the variable is a
scalar, and v-preﬁx for vector. Unused operations are
discarded for clarity. The original programs as well
as the compiled TAC form programs are provided in
the supplementary material.

Figure 3: Search method ablation study ex-
periment results. Each column shows the top
200 lowest (cid:96)2 distortion ratios found by each
search method. From left to right, each col-
umn adds a new technique.

We run this experiment for 50 times in parallel. Each run allows a maximum number of 500 million
queries to the classiﬁer, which takes about two hours on one GTX 1080 Ti GPU. In all 50 runs, we
generate about 125 billion random programs. 45.475% of these programs failed in the inputs check,
54.497% of them failed in the distance test, and only 0.028% of them survived both and continued
to be evaluated against the classiﬁer on GPU. These results show that the inputs check and distance
test techniques save a lot of expensive GPU computational cost for us. As a result, we achieve a
throughput of 294k programs per second per GTX 1080 Ti GPU.

We plot the lowest (cid:96)2 distortion ratio found on the ten ﬁxed examples in each run in Figure 2. They
average at 0.01797 with a standard deviation of 0.00043. The top one (cid:96)2 distortion ratio is 0.01699,
the second place is 0.01705, while the third place is 0.01723. The ﬁrst place and the second place
programs perform quite similarly and we choose both of them to compare with human designed
attacks. We call them AutoDA 1st and AutoDA 2nd, respectively. We show the SSA form programs of
AutoDA 1st and 2nd in Figure 1. We are surprised that they are quite short after discarding unused
operations — AutoDA 1st only uses 10 operations and AutoDA 2nd uses 13 operations, while the
Boundary attack’s generate() function uses 20 operations when expressed in the AutoDA DSL.
We also observe that AutoDA 1st includes an operation sequence of v8 = MUL(v3,s0); s18 =
DOT(v17,v8), and AutoDA 2nd includes v7 = MUL(v3,s0); s11 = DOT(v10,v7), where s0 is
the scalar hyperparameter and v3 is the Gaussian noise. They share a pattern of DOT(*,MUL(v3,s0)).
A similar pattern is also observed in the Boundary attack [5]. Moreover, both discovered attacks use
the three predeﬁned operations which are also used in existing works. These similarities qualitatively
suggest the rationality of some heuristics used in existing attack methods.

4.2 Results on CIFAR-10 and ImageNet

We benchmark the AutoDA 1st and 2nd programs we found for attacking various models under the
(cid:96)2 norm untargeted decision-based threat model on the CIFAR-10 [28] and ImageNet [15] datasets,
and compare them with existing methods. We follow Dong et al.’s benchmark methodology: We
consider one attack to be successful after it ﬁnds adversarial example whose l2 distance w.r.t. the
original example is smaller than (cid:15) = 1.0 on CIFAR-10, and whose normalized (cid:96)2 distance w.r.t. the
original example is smaller than (cid:15) = √0.001 on ImageNet (normalized (cid:96)2 distance is deﬁned as
(cid:107) · (cid:107)2/√d where d is the dimension of the input to the classiﬁer). Then we use the attack success
rate vs. queries curve to show the effectiveness and efﬁciency of these attack algorithms, as well as
the (cid:96)2 distortion vs. queries curve widely used in previous decision-based attack works [5, 11].

7

defAutoDA_1st(s0,v1,v2,v3):v4=SUB.VV(v1,v2)s5=NORM.V(v4)v6=DIV.VS(v4,s5)v8=MUL.VS(v3,s0)v11=MUL.VS(v8,s5)v17=ADD.VV(v8,v6)s18=DOT.VV(v17,v8)v21=MUL.VS(v4,s18)v22=ADD.VV(v21,v2)v23=SUB.VV(v22,v11)returnv23defAutoDA_2nd(s0,v1,v2,v3):v4=SUB.VV(v1,v2)s5=NORM.V(v4)v6=DIV.VS(v4,s5)v7=MUL.VS(v3,s0)v8=ADD.VV(v7,v6)s9=NORM.V(v2)v10=MUL.VS(v8,s5)s11=DOT.VV(v10,v7)v12=DIV.VS(v6,s9)v17=ADD.VV(v6,v12)v20=MUL.VS(v17,s11)v21=ADD.VV(v1,v20)v23=SUB.VV(v21,v10)returnv230.017000.017250.017500.017750.018000.018250.018500.018750.01900‘2DistortionRatioBase+PredeﬁnedOperations+InputsCheck+DistanceTest+CompactProgram0.20.30.40.50.6‘2DistortionRatioFigure 4: The (cid:96)2 distortion vs. queries and attack success rate vs. queries curves on the three models
on the CIFAR-10 dataset and the Inception-v3 model on the ImageNet dataset.

We compare the following untargeted decision-based attack methods with our AutoDA 1st and 2nd:
(1) The Boundary attack [5], (2) the Evolutionary attack [17], and (3) the Sign-OPT attack [11].
The ﬁrst two attacks are both based on the random walk framework. They are included in Dong
et al.’s benchmark, so we adapt their implementations. However, we disable the dimension reduction
trick for these two attacks on ImageNet because we want to know the original attacks’ strength.
The third one is a recently proposed query-efﬁcient attack based on zeroth-order optimization. We
adapt the implementation from its ofﬁcial repository and leave all hyperparameters unmodiﬁed.
Together with our AutoDA 1st and 2nd attacks, we have ﬁve attacks to run. As for the initialization
of hyperparameters in AutoDA 1st and 2nd, we adopt the original value 0.01 used in the search
to attack the CIFAR-10 models. However, when transferred to ImageNet, they would fail to pass
the distance test with their hyperparameters initialized to 0.01. To overcome this issue caused by
changing dimension, we decrease their hyperparameters’ initial value to 0.001 on ImageNet. The
Sign-OPT attack might spend up to several hundreds of queries for ﬁnding the starting points, while
other attacks do not. Thus we include these queries for ﬁnding the starting points in the total queries
to make the comparison more fair. More details on how we run the ﬁve attacks are provided in the
supplementary material.

We select the ﬁrst 1,000 images from the CIFAR-10 test set and the ﬁrst 1,000 images from the
ImageNet test set to run our benchmark. We choose the normally trained ResNet50 [26] model on
the CIFAR-10 dataset and normally trained Inception-v3 [39] model on the ImageNet dataset both
provided by torchvision [34] for the ﬁve methods to attack. Besides, we also aim to understand the
strength of our attacks on stronger models, and thus we include (cid:96)2 adversarially trained ((cid:15) = 1.0)
ResNet50 model provided by Engstrom et al. [19] and (cid:96)∞ adversarially trained ((cid:15) = 8/255) WRN
model [44] provided by Madry et al. [32]. The clean accuracies for these models on our benchmark
examples are: 94.4% for the normally trained ResNet50 model, 78.1% for the normally trained
Inception-v3 model, 82.4% for the (cid:96)2 adversarially trained ResNet50 model, and 87.3% for the (cid:96)∞
adversarially trained WRN model.

We plot the (cid:96)2 distortion vs. queries and attack success rate vs. queries curves for the ﬁve attacks on
the three models on the CIFAR-10 dataset and the Inception-v3 model on the ImageNet dataset in
Figure 4. We also provide attack success rate at different number of queries in Table 2 for numerical
comparisons.

From these curves and the table, we can observe that AutoDA 1st performs slightly better than
AutoDA 2nd, which is consistent with their quite-close (cid:96)2 distortion ratios shown in Section 4.1.
This fact suggests the rationality of using the (cid:96)2 distortion ratio as the evaluation metric. Moreover,
both the AutoDA 1st and 2st outperform the other three human designed baselines by a lot when the
number of queries is smaller than 5,000. When the number of queries grows larger than 5,000, our
AutoDA 1st attack method still outperforms the Boundary attack and the Evolutionary attack, while

8

ResNet50(normaltraining)ResNet50(‘2adversarialtraining)WRN(‘∞adversarialtraining)Inception-v3(normaltraining)05k10k15k20kQueries048121‘2Distortion05k10k15k20kQueries048121‘2Distortion05k10k15k20kQueries048121‘2Distortion05k10k15k20kQueries0.00.10.20.3√0.001Normalized‘2Distortion05k10k15k20kQueries0.000.250.500.751.00AttackSuccessRate05k10k15k20kQueries0.000.250.500.751.00AttackSuccessRate05k10k15k20kQueries0.000.250.500.751.00AttackSuccessRate05k10k15k20kQueries0.000.250.500.751.00AttackSuccessRateBoundaryEvolutionarySign-OPTAutoDA1stAutoDA2ndTable 2: The attack success rate given different number of queries on the three models on the
CIFAR-10 dataset and the Inception-v3 model on the ImageNet dataset.

Model

ResNet50
(normal training)

ResNet50
((cid:96)2 adv. training)

WRN
((cid:96)∞ adv. training)

Inception-v3
(normal training)

Queries

2,000 4,000 20,000 2,000 4,000 20,000 2,000 4,000 20,000 2,000 4,000 20,000

Boundary

10.7% 28.4% 100.0% 0.6% 1.6% 21.6% 1.1% 2.7% 43.1% 7.0% 15.0% 86.8%
Evolutionary 64.9% 96.3% 100.0% 4.4% 8.9% 25.4% 7.7% 18.2% 49.4% 33.4% 59.2% 98.1%
Sign-OPT 76.1% 98.8% 100.0% 6.6% 12.6% 29.5% 11.3% 24.1% 60.3% 41.4% 69.1% 99.2%

AutoDA 1st 95.9% 99.7% 100.0% 9.7% 14.9% 27.8% 19.2% 28.3% 57.4% 57.1% 74.6% 98.7%
AutoDA 2nd 95.6% 99.5% 100.0% 10.0% 14.8% 27.7% 18.9% 27.1% 57.0% 56.5% 73.4% 97.8%

becomes slightly behind the Sign-OPT attack method, and only for defensive models and ImageNet
model, this small gap is noticeable. These behaviors are consistent on all models and datasets we
run our experiments on, demonstrating the great query-efﬁciency of our discovered attack methods
especially under low number of queries, which is important in real-world scenarios for black-box
attacks [5, 27].

4.3 Ablation Study on Search Method

As described in Section 3.3, we apply four techniques to our search method, which are (1) Prede-
ﬁned operations, (2) Inputs check, (3) Distance test, and (4) Compact program. To illustrate their
effectiveness, we conduct the following ablation study on search method. Starting from the base
search method using only naive random search, we add the four techniques one by one, so we get
ﬁve different random search methods including the base one. For each of these ﬁve random search
methods, we run it to evaluate 100,000 programs against the classiﬁer for 100 iterations on ﬁve
ﬁxed examples and calculate the (cid:96)2 distortion ratio for each program. We plot the top 200 lowest (cid:96)2
distortion ratios that each search method found in Figure 3.

From the ﬁgure, we can observe that with more techniques added, the top 200 lowest (cid:96)2 distortion
ratios overall show a decreasing trend. For example, the lowest (cid:96)2 distortion ratio in each column
becomes lower and lower. These results demonstrate the effectiveness of the four techniques we
applied to our search method. These results are qualitative, because these techniques might interfere
with each other so that multiple techniques combined might bring improvement larger than the sum
of improvements brought by applying each of them. As a result, the absolute improvement shown in
the ﬁgure does not imply the effectiveness of each technique.

5 Conclusion and Discussion

In this work, we propose to automate the process of discovering decision-based attack algorithms.
Starting from the random walk framework as the algorithm template, we construct our generic
search space from the AutoDA DSL, explore this search space using random search integrated with
several pruning techniques and intuitive priors, and evaluate programs in the search space using
a small and fast model. The discovered attack algorithms are simple, while consistently achieve
high query-efﬁciency when transferred to both normal and defensive models on the CIFAR-10 and
ImageNet datasets.

Many future extensions can be done to this work. First, we particularly focus on the untargeted
decision-based threat model under the (cid:96)2 norm in this work. Extending our approach to targeted
attack should be straightforward, while extending to the (cid:96)∞ norm might need more efforts, because
designing another search space specialized for the (cid:96)∞ norm is necessary. Second, we limit the search
space to be relatively small and use a random search based method to explore it. More advanced
search methods like evolutionary search and more computational resources could explore larger and
more powerful search space, which should lead to better algorithms. Finally, advanced static analysis
tools can help us simplify the discovered attack algorithms and identify important operations in these
algorithms.

9

References

[1] Martín Abadi, Ashish Agarwal, Paul Barham, Eugene Brevdo, Zhifeng Chen, Craig Citro,
Greg S. Corrado, Andy Davis, Jeffrey Dean, Matthieu Devin, Sanjay Ghemawat, Ian Goodfellow,
Andrew Harp, Geoffrey Irving, Michael Isard, Yangqing Jia, Rafal Jozefowicz, Lukasz Kaiser,
Manjunath Kudlur, Josh Levenberg, Dandelion Mané, Rajat Monga, Sherry Moore, Derek
Murray, Chris Olah, Mike Schuster, Jonathon Shlens, Benoit Steiner, Ilya Sutskever, Kunal
Talwar, Paul Tucker, Vincent Vanhoucke, Vijay Vasudevan, Fernanda Viégas, Oriol Vinyals, Pete
Warden, Martin Wattenberg, Martin Wicke, Yuan Yu, and Xiaoqiang Zheng. TensorFlow: Large-
scale machine learning on heterogeneous systems, 2015. URL https://www.tensorflow.
org/.

[2] Alfred V. Aho, Ravi Sethi, and Jeffrey D. Ullman. Compilers: Principles, Techniques, and Tools.
Addison-Wesley series in computer science / World student series edition. Addison-Wesley,
1986. ISBN 0-201-10088-6.

[3] Anish Athalye, Nicholas Carlini, and David A. Wagner. Obfuscated gradients give a false sense
of security: Circumventing defenses to adversarial examples. In ICML, pages 274–283, 2018.

[4] Matej Balog, Alexander L. Gaunt, Marc Brockschmidt, Sebastian Nowozin, and Daniel Tarlow.

DeepCoder: Learning to write programs. In ICLR, 2017.

[5] Wieland Brendel, Jonas Rauber, and Matthias Bethge. Decision-based adversarial attacks:

Reliable attacks against black-box machine learning models. In ICLR, 2018.

[6] Thomas Brunner, Frederik Diehl, Michael Truong-Le, and Alois C. Knoll. Guessing smart:
Biased sampling for efﬁcient black-box adversarial attacks. In ICCV, pages 4957–4965, 2019.

[7] Nicholas Carlini and David A. Wagner. Towards evaluating the robustness of neural networks.

In IEEE Symposium on Security and Privacy, pages 39–57, 2017.

[8] Nicholas Carlini, Anish Athalye, Nicolas Papernot, Wieland Brendel, Jonas Rauber, Dimitris
Tsipras, Ian J. Goodfellow, Aleksander Madry, and Alexey Kurakin. On evaluating adversarial
robustness. arXiv.org, 2019.

[9] Pin-Yu Chen, Huan Zhang, Yash Sharma, Jinfeng Yi, and Cho-Jui Hsieh. ZOO: Zeroth order
optimization based black-box attacks to deep neural networks without training substitute models.
In Proceedings of the 10th ACM workshop on artiﬁcial intelligence and security, pages 15–26,
2017.

[10] Minhao Cheng, Thong Le, Pin-Yu Chen, Huan Zhang, Jinfeng Yi, and Cho-Jui Hsieh. Query-
efﬁcient hard-label black-box attack: An optimization-based approach. In ICLR, 2019.

[11] Minhao Cheng, Simranjit Singh, Patrick H. Chen, Pin-Yu Chen, Sijia Liu, and Cho-Jui Hsieh.

Sign-OPT: A query-efﬁcient hard-label adversarial attack. In ICLR, 2020.

[12] Shuyu Cheng, Yinpeng Dong, Tianyu Pang, Hang Su, and Jun Zhu. Improving black-box
adversarial attacks with a transfer-based prior. In NeurIPS, pages 10934–10944, 2019.

[13] François Chollet. Keras. https://github.com/fchollet/keras, 2015.

[14] Francesco Croce, Maksym Andriushchenko, Vikash Sehwag, Nicolas Flammarion, Mung
Chiang, Prateek Mittal, and Matthias Hein. RobustBench: a standardized adversarial robustness
benchmark. arXiv.org, 2020.

[15] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Fei-Fei Li. ImageNet: A large-scale

hierarchical image database. In CVPR, pages 248–255, 2009.

[16] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. BERT: pre-training of
deep bidirectional transformers for language understanding. In NAACL-HLT, pages 4171–4186,
2019.

[17] Yinpeng Dong, Hang Su, Baoyuan Wu, Zhifeng Li, Wei Liu, Tong Zhang, and Jun Zhu. Efﬁcient
decision-based black-box adversarial attacks on face recognition. In CVPR, pages 7714–7722,
2019.

10

[18] Yinpeng Dong, Qi-An Fu, Xiao Yang, Tianyu Pang, Hang Su, Zihao Xiao, and Jun Zhu.
Benchmarking adversarial robustness on image classiﬁcation. In CVPR, pages 318–328, 2020.

[19] Logan Engstrom, Andrew Ilyas, Hadi Salman, Shibani Santurkar, and Dimitris Tsipras. Robust-

ness (Python library), 2019. URL https://github.com/MadryLab/robustness.

[20] Matthias Feurer, Aaron Klein, Katharina Eggensperger, Jost Tobias Springenberg, Manuel
Blum, and Frank Hutter. Efﬁcient and robust automated machine learning. In NIPS, pages
2962–2970, 2015.

[21] Ian J. Goodfellow, Jonathon Shlens, and Christian Szegedy. Explaining and harnessing adver-

sarial examples. In ICLR, 2015.

[22] Alex Graves, Abdel-rahman Mohamed, and Geoffrey E. Hinton. Speech recognition with deep

recurrent neural networks. In ICASSP, pages 6645–6649, 2013.

[23] Gaël Guennebaud, Benoît Jacob, et al. Eigen v3. http://eigen.tuxfamily.org, 2010.

[24] Sumit Gulwani, Susmit Jha, Ashish Tiwari, and Ramarathnam Venkatesan. Synthesis of

loop-free programs. In PLDI, pages 62–73, 2011.

[25] Sumit Gulwani, Oleksandr Polozov, Rishabh Singh, et al. Program synthesis. Foundations and

Trends® in Programming Languages, 4(1-2):1–119, 2017.

[26] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image

recognition. In CVPR, pages 770–778, 2016.

[27] Andrew Ilyas, Logan Engstrom, Anish Athalye, and Jessy Lin. Black-box adversarial attacks

with limited queries and information. In ICML, pages 2142–2151, 2018.

[28] Alex Krizhevsky and Geoffrey Hinton. Learning multiple layers of features from tiny images.

Technical report, University of Toronto, 2009.

[29] Alex Krizhevsky, Ilya Sutskever, and Geoffrey E. Hinton. ImageNet classiﬁcation with deep

convolutional neural networks. In NIPS, pages 1106–1114, 2012.

[30] Chris Lattner and Vikram S. Adve. LLVM: A compilation framework for lifelong program

analysis & transformation. In CGO, pages 75–88, 2004.

[31] Liam Li and Ameet Talwalkar. Random search and reproducibility for neural architecture search.

In UAI, pages 367–377, 2019.

[32] Aleksander Madry, Aleksandar Makelov, Ludwig Schmidt, Dimitris Tsipras, and Adrian Vladu.

Towards deep learning models resistant to adversarial attacks. In ICLR, 2018.

[33] Nicolas Papernot, Patrick D. McDaniel, Ian J. Goodfellow, Somesh Jha, Z. Berkay Celik, and
Ananthram Swami. Practical black-box attacks against deep learning systems using adversarial
examples. arXiv.org, 2016.

[34] Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan,
Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, Alban Desmaison, Andreas
Köpf, Edward Yang, Zachary DeVito, Martin Raison, Alykhan Tejani, Sasank Chilamkurthy,
Benoit Steiner, Lu Fang, Junjie Bai, and Soumith Chintala. PyTorch: An imperative style,
high-performance deep learning library. In NeurIPS, pages 8024–8035, 2019.

[35] Esteban Real, Chen Liang, David R. So, and Quoc V. Le. AutoML-Zero: Evolving machine

learning algorithms from scratch. In ICML, pages 8007–8019, 2020.

[36] Armando Solar-Lezama, Liviu Tancau, Rastislav Bodík, Sanjit A. Seshia, and Vijay A. Saraswat.

Combinatorial sketching for ﬁnite programs. In ASPLOS, pages 404–415, 2006.

[37] Saurabh Srivastava, Sumit Gulwani, and Jeffrey S. Foster. Template-based program veriﬁcation

and program synthesis. Int. J. Softw. Tools Technol. Transf., pages 497–518, 2013.

11

[38] Christian Szegedy, Wojciech Zaremba, Ilya Sutskever, Joan Bruna, Dumitru Erhan, Ian J.
Goodfellow, and Rob Fergus. Intriguing properties of neural networks. In ICLR, 2014.

[39] Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jonathon Shlens, and Zbigniew Wojna.
Rethinking the Inception architecture for computer vision. In CVPR, pages 2818–2826, 2016.

[40] Mingxing Tan and Quoc V. Le. EfﬁcientNet: Rethinking model scaling for convolutional neural

networks. In ICML, pages 6105–6114, 2019.

[41] Florian Tramèr, Nicholas Carlini, Wieland Brendel, and Aleksander Madry. On adaptive attacks

to adversarial example defenses. In NeurIPS, 2020.

[42] Jonathan Uesato, Brendan O’Donoghue, Pushmeet Kohli, and Aäron van den Oord. Adversarial
risk and the dangers of evaluating against weak attacks. In ICML, pages 5032–5041, 2018.

[43] David H. Wolpert and William G. Macready. No free lunch theorems for optimization. IEEE

Trans. Evol. Comput., pages 67–82, 1997.

[44] Sergey Zagoruyko and Nikos Komodakis. Wide residual networks. In BMVC, 2016.

[45] Barret Zoph and Quoc V. Le. Neural architecture search with reinforcement learning. In ICLR,

2017.

12

A Hyperparameters Adjustment Strategies

In this section, we will introduce the hyperparameters adjustment strategies used in the Boundary
attack [5], the Evolutionary attack [17], and our AutoDA.

Boundary attack. The Boundary attack [5] has two hyperparameters — the total perturbation δ and
the length of the step (cid:15) towards the original input. The original implementation would adjust both
hyperparameters during the random walk process. If the success rate for the past several trails is too
low, (cid:15) would be decreased and vise versa. The attack is converged when (cid:15) reaches zero. The δ would
be adjusted according to the so called orthogonal perturbation’s success rate similar to adjusting (cid:15).
However, using a ﬁxed δ during the random walk process has negligible performance impact with a
proper initial value. Under this case, we only need to adjust (cid:15) during the random walk process.

Evolutionary attack. The Evolutionary attack [17] has several hyperparameters, while only the µ is
adjusted during the random walk process. The µ is adjusted once for every T iterations as follows:

µ

µ

·

←

exp (p

¯p)

−

(1)

where p is the success rate of past T trails, ¯p is a predeﬁned target success rate. This is a negative
feedback strategy keeping the success rate around ¯p. The original implementation set T to 30 and ¯p
to 0.2.

AutoDA. Our AutoDA uses a negative feedback strategy similar to the ones used in the above two
attack methods especially the Evolutionary attack. Instead of using the exp(p
¯p) function in
the Evolutionary attack, we use a piecewise linear function f that satisﬁes f (0) = l, f (1) = h,
and f (¯p) = 1, where l, h are both predeﬁned constants satisfying 0 < l < 1 < h. The ¯p is the
predeﬁned target success rate same as in the Evolutionary attack. For implementation simplicity,
instead of adjusting the hyperparameter every T iterations as in the Evolutionary attack, we adjust
the hyperparameter in each iteration according to a decayed success rate p deﬁned as follows:

−

p

α

p + (1

α)

k

(2)

←

·

−

·

where k = 1 if x(cid:48) is adversarial, otherwise k = 0, α is the decay rate. We adjust the hyperparameter
s as follows:

s

(3)
1/10 is to stabilize the hyperparameter adjustment, so that when p is quite close to
·
s, and
·
·
s instead of a
s. This negative feedback strategy would keep the success rate around ¯p, too. We

where the extra
0.0 for ten iterations, the s would be decreased to at most l
when p is quite close to 1.0 for ten iterations, the s would be increased to at most h
much larger h10
set the decay rate α to 0.95, l to 0.5, h to 1.5, and ¯p to 0.25 in AutoDA.

s instead of a much smaller l10

←

s

·

·

·

[f (p)]1/10

B The AutoDA 1st and AutoDA 2nd

For the AutoDA 1st and AutoDA 2nd, we show their original SSA form programs, their SSA form
programs after discarding unused operations, and their compiled TAC form programs in Figure 5.

C Additional Details on the AutoDA DSL

We provide the detailed deﬁnitions of available operations in the AutoDA domain speciﬁc language
(DSL) in Table 3. To show the expressiveness of our DSL, we provide one possible implementation
of the Boundary attack [5] using our AutoDA DSL in Figure 6.

D Additional Implementation Details

We need to implement the AutoDA system efﬁciently, because our task is computational intensive
even with the computational cost reducing techniques described in Section 3. As a result, we
implement the AutoDA system in the C++ programming language. We train our binary classiﬁers
in Python using a keras [13] implementation 1 of EfﬁcientNet [40], then export them to a proper

1https://github.com/qubvel/efficientnet

13

discard unused operations

discard unused operations

allocate memory slots

allocate memory slots

(a)

(b)

Figure 5: (a) and (b) show the original SSA form programs, the SSA form programs after discarding
unused operations, and the compiled TAC form programs after allocating memory slots for the SSA
form programs, for the AutoDA 1st and AutoDA 2nd respectively. In the original SSA form programs
and the SSA form programs after discarding unused operations, s0 is the hyperparameter, v1 is the
original adversarial example x0, v2 is the adversarial example x the random walk process already
found, and v3 is the standard Gaussian noise n. In the TAC form programs, s0, v0, v1 and v2 is
compiled from the s0, v1, v2, v3 in the SSA form programs respectively.

format, so that we can run these classiﬁers using TensorFlow [1] for C. These classiﬁers require large
batch size (> 1, 000) to achieve their full speed, so that we have to run a large number of programs in
parallel to fulﬁll this requirement. As a result, we divide these large number of programs into smaller
batches (e.g., with batch size of 150 as mentioned in Section 4.1), and execute these smaller batches
in parallel on multiple threads to utilize multiple CPU cores. We also use the Eigen C++ template
library for linear algebra [23] to execute programs on CPU for better performance. with batch size of
150 as mentioned in Section 4.1), and execute these smaller batches in parallel on multiple threads to
utilize multiple CPU cores. We also use the Eigen C++ template library for linear algebra [23] to
execute programs on CPU for better performance. We will provide additional implementation details
on random program generating and the SSA form to TAC form compiler for the rest of this section.

Random program generating. We describe in detail how we generate a random program starting
from an empty program in this paragraph: (1) Add the three predeﬁned operations described in
Section 3.3 to the empty program. (2) Keep randomly and uniformly selecting one operation from all
available operations in the AutoDA DSL and append it to the program. We keep a record of unused
operations, so that when randomly choosing the parameter(s) of a new operation, we choose unused
operation’s outputs with higher probability. This is the compact program technique mentioned in
Section 3.3. (3) For the last operation before the program reaching maximum length, we consider it
as the output of the program, so that this operation must output a vector. As a result, we randomly
select a vector output operation as the last operation. After generating each random program, we run
the inputs check and the distance test as described in Section 3.3. The whole process of generating
random programs then running the inputs check and the distance test is CPU intensive. To mitigate
this issue, we do random program generating, checking and testing in parallel on multiple threads.

SSA form to TAC form compiler. As mentioned in Section 3.4, we compile SSA form programs
to their equivalent TAC form programs before executing them for better performance and smaller
memory usage. This simple SSA form to TAC form compiler ﬁrst discards unused operations then
allocates memory slots to get the equivalent TAC form program. Discarding unused operations can be
solved with some simple compiler techniques, while allocating memory slots with optimal memory
usage is NP-complete [2]. Instead, we use a non-optimal linear complexity algorithm to allocate
memory slots, which is fast and produces TAC form programs with reasonable quality. For example,

14

defAutoDA_1st(s0,v1,v2,v3):v4=SUB.VV(v1,v2)s5=NORM.V(v4)v6=DIV.VS(v4,s5)v7=MUL.VS(v4,s0)v8=MUL.VS(v3,s0)s9=DIV.SS(s0,s0)s10=SUB.SS(s9,s5)v11=MUL.VS(v8,s5)s12=ADD.SS(s10,s9)s13=MUL.SS(s12,s0)s14=ADD.SS(s9,s12)v15=DIV.VS(v11,s13)s16=DOT.VV(v11,v11)v17=ADD.VV(v8,v6)s18=DOT.VV(v17,v8)v19=SUB.VV(v7,v15)s20=DOT.VV(v19,v4)v21=MUL.VS(v4,s18)v22=ADD.VV(v21,v2)v23=SUB.VV(v22,v11)returnv23defAutoDA_1st(s0,v1,v2,v3):v4=SUB.VV(v1,v2)s5=NORM.V(v4)v6=DIV.VS(v4,s5)v8=MUL.VS(v3,s0)v11=MUL.VS(v8,s5)v17=ADD.VV(v8,v6)s18=DOT.VV(v17,v8)v21=MUL.VS(v4,s18)v22=ADD.VV(v21,v2)v23=SUB.VV(v22,v11)returnv23defAutoDA_1st(s0,v0,v1,v2):v3=SUB.VV(v0,v1)s1=NORM.V(v3)v4=DIV.VS(v3,s1)v5=MUL.VS(v2,s0)v6=MUL.VS(v5,s1)v4=ADD.VV(v5,v4)s1=DOT.VV(v4,v5)v3=MUL.VS(v3,s1)v3=ADD.VV(v3,v1)v3=SUB.VV(v3,v6)returnv3defAutoDA_2nd(s0,v1,v2,v3):v4=SUB.VV(v1,v2)s5=NORM.V(v4)v6=DIV.VS(v4,s5)v7=MUL.VS(v3,s0)v8=ADD.VV(v7,v6)s9=NORM.V(v2)v10=MUL.VS(v8,s5)s11=DOT.VV(v10,v7)v12=DIV.VS(v6,s9)s13=DOT.VV(v12,v8)s14=MUL.SS(s13,s13)s15=MUL.SS(s14,s14)s16=MUL.SS(s11,s15)v17=ADD.VV(v6,v12)s18=SUB.SS(s9,s11)s19=SUB.SS(s9,s18)v20=MUL.VS(v17,s11)v21=ADD.VV(v1,v20)s22=MUL.SS(s16,s19)v23=SUB.VV(v21,v10)returnv23defAutoDA_2nd(s0,v1,v2,v3):v4=SUB.VV(v1,v2)s5=NORM.V(v4)v6=DIV.VS(v4,s5)v7=MUL.VS(v3,s0)v8=ADD.VV(v7,v6)s9=NORM.V(v2)v10=MUL.VS(v8,s5)s11=DOT.VV(v10,v7)v12=DIV.VS(v6,s9)v17=ADD.VV(v6,v12)v20=MUL.VS(v17,s11)v21=ADD.VV(v1,v20)v23=SUB.VV(v21,v10)returnv23defAutoDA_2nd(s0,v0,v1,v2):v3=SUB.VV(v0,v1)s1=NORM.V(v3)v3=DIV.VS(v3,s1)v4=MUL.VS(v2,s0)v5=ADD.VV(v4,v3)s2=NORM.V(v1)v5=MUL.VS(v5,s1)s1=DOT.VV(v5,v4)v4=DIV.VS(v3,s2)v3=ADD.VV(v3,v4)v3=MUL.VS(v3,s1)v3=ADD.VV(v0,v3)v3=SUB.VV(v3,v5)returnv3Table 3: List of available operations in the AutoDA DSL. The sufﬁx of each operation’s notation
indicates the parameters’ type of the operation, where S denotes scalar type, and V denotes vector type.
For example, the VS sufﬁx means the operation’s ﬁrst parameter is a scalar and second parameter is a
vector. In the parameter(s) column, a or (cid:126)a stands for the ﬁrst parameter, a for scalar and (cid:126)a for vector;
b or (cid:126)b stands for the second parameter, b for scalar and (cid:126)b for vector. In the output column, r for scalar
output, and (cid:126)r for vector output. In the mathematical expression column, the subscript
i on vector
variable means the i-th component of the vector,

·
i means for all dimension of the vector.

∀

ID Notation Description

Parameter(s) Output Mathematical Expression

1
2
3
4
5
6
7
8
9
10

ADD.SS
SUB.SS
MUL.SS
DIV.SS
ADD.VV
SUB.VV
MUL.VS
DIV.VS
DOT.VV
NORM.V

scalar-scalar addition
scalar-scalar subtraction
scalar-scalar multiplication
scalar-scalar division
vector-vector element-wise addition
vector-vector element-wise subtraction
vector-scalar broadcast multiplication
vector-scalar broadcast division
vector-vector dot product
vector (cid:96)2 norm

a, b
a, b
a, b
a, b
(cid:126)a,(cid:126)b
(cid:126)a,(cid:126)b
(cid:126)a, b
(cid:126)a, b
(cid:126)a,(cid:126)b
(cid:126)a

r
r
r
r
(cid:126)r
(cid:126)r
(cid:126)r
(cid:126)r
r
r

r = a + b
r = a − b
r = ab
r = a/b
(cid:126)r = (cid:126)a + (cid:126)b
(cid:126)r = (cid:126)a − (cid:126)b
ri = aib, ∀i
ri = ai/b, ∀i
r = (cid:126)a · (cid:126)b
r = (cid:107)(cid:126)a(cid:107)2

for the AutoDA 1st shown in Figure 5(a), the SSA form program after discarding unused operations
still use three scalar memory slots and eleven vector memory slots, while the compiled TAC form
program use two scalar memory slots and seven vector memory slots. For the AutoDA 2nd shown
in Figure 5(b) and the Boundary attack’s generate() function shown in Figure 6, we can observe
similar memory usage reduction.

E Detailed Experiment Setups

We describe in detail how we run our AutoDA 1st, AutoDA 2nd, the Boundary attack, the Evolutionary
attack, and the Sign-OPT attack in this paragraph. We adapt the implementations of the Boundary
attack and the Evolutionary attack from Dong et al.’s benchmark 2. We adapt the implementation of
the Sign-OPT attack from its ofﬁcial repository 3. We remove the dimension reduction technique in
the Boundary attack and the Evolutionary attack’s implementations, which is a useful technique for
accelerating black-box attacks [9] when attacking models on ImageNet, because we want to know the
original attacks’ strength. Though this technique can be applied to our AutoDA 1st and 2nd easily,
the Sign-OPT attack’s implementation does not support it, so disabling it in all attacks also makes the
comparison more fair. For all attacks, we clip the inputs into [0, 1] to make them valid images before
running the classiﬁer for prediction labels. The Sign-OPT attack’s original implementation would fail
to ﬁnd starting points for some inputs. We modify its implementation to fallback to starting points
selected from the test set after 100 failures. For our AutoDA 1st and 2nd as well as the Boundary
attack and the Evolutionary attack, we ﬁnd starting points by keeping adding different standard
Gaussian noises to the original examples until ﬁnding one that causes misclassiﬁcation, and fallback
to starting points selected from the test set after 100 failures similar to the Sign-OPT attack.

2https://github.com/thu-ml/ares
3https://github.com/cmhcbb/attackbox

15

(a)

(b)

Figure 6: (a) One possible implementation of the Boundary attack’s generate() function as a SSA
form program in the AutoDA DSL. s0 and s1 are the (cid:15) and δ hyperparameters in the Boundary attack
respectively, s2 is derived from the δ as √1 + δ2. The Boundary attack would adjust both the (cid:15) and
the δ during the random walk process. However, a ﬁxed δ has negligible performance impact as
mentioned in Section A. For simplicity, we ﬁx the δ so that both s1 and s2 can be considered as
ﬁxed hyperparameters. As a result, we do not need to add the extra √
operation to our DSL for
implementing the Boundary attack. v3 is the original example x0, v4 is the adversarial example x
the random walk process already found, and v5 is the standard Gaussian noise n. (b) The compiled
TAC form version of the same program. The s0, s1, s2, v0, v1 and v2 are compiled from the s0, s1,
s2, v3, v4 and v5 in the SSA form program respectively.

·

16

defBoundary_generate(s0,s1,s2,v3,v4,v5):v6=SUB.VV(v3,v4)s7=NORM.V(v6)v8=DIV.VS(v6,s7)s9=DOT.VV(v5,v8)v10=MUL.VS(v8,s9)v11=SUB.VV(v5,v10)s12=NORM.V(v11)s13=MUL.SS(s1,s7)s14=DIV.SS(s13,s12)v15=MUL.VS(v11,s14)v16=SUB.VV(v6,v15)v17=DIV.VS(v16,s2)s18=NORM.V(v17)v19=SUB.VV(v3,v17)s20=MUL.SS(s0,s7)s21=SUB.SS(s18,s7)s22=ADD.SS(s20,s21)s23=DIV.SS(s22,s18)v24=MUL.VS(v17,s23)v25=ADD.VV(v19,v24)returnv25defBoundary_generate(s0,s1,s2,v0,v1,v2):v3=SUB.VV(v0,v1)s3=NORM.V(v3)v4=DIV.VS(v3,s3)s4=DOT.VV(v2,v4)v4=MUL.VS(v4,s4)v4=SUB.VV(v2,v4)s4=NORM.V(v4)s5=MUL.SS(s1,s3)s4=DIV.SS(s5,s4)v4=MUL.VS(v4,s4)v3=SUB.VV(v3,v4)v3=DIV.VS(v3,s2)s4=NORM.V(v3)v4=SUB.VV(v0,v3)s5=MUL.SS(s0,s3)s3=SUB.SS(s4,s3)s3=ADD.SS(s5,s3)s3=DIV.SS(s3,s4)v3=MUL.VS(v3,s3)v3=ADD.VV(v4,v3)returnv3