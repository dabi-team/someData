Multi-Task Deep Learning with Dynamic
Programming for Embryo Early Development Stage
Classiﬁcation from Time-Lapse Videos

Zihan Liu, Bo Huang, Yuqi Cui, Yifan Xu, Bo Zhang, Lixia Zhu, Yang Wang, Lei Jin and Dongrui Wu

1

9
1
0
2

g
u
A
2
2

]

V

I
.
s
s
e
e
[

1
v
7
3
6
9
0
.
8
0
9
1
:
v
i
X
r
a

Abstract—Time-lapse is a technology used to record the devel-
opment of embryos during in-vitro fertilization (IVF). Accurate
classiﬁcation of embryo early development stages can provide
embryologists valuable information for assessing the embryo
quality, and hence is critical to the success of IVF. This paper
proposes a multi-task deep learning with dynamic programming
(MTDL-DP) approach for this purpose. It ﬁrst uses MTDL to
pre-classify each frame in the time-lapse video to an embryo
development stage, and then DP to optimize the stage sequence
so that the stage number is monotonically non-decreasing, which
usually holds in practice. Different MTDL frameworks, e.g., one-
to-many, many-to-one, and many-to-many, are investigated. It
is shown that the one-to-many MTDL framework achieved the
best compromise between performance and computational cost.
To our knowledge, this is the ﬁrst study that applies MTDL to
embryo early development stage classiﬁcation from time-lapse
videos.

Index Terms—Multi-task learning, in-vitro fertilization, convo-
lutional neural networks, dynamic programming, image classiﬁ-
cation

I. INTRODUCTION

In-vitro fertilization (IVF) [1]–[3] is a frequently used
technology for treating infertility. The process involves the
collection of multiple follicles for fertilization and in-vitro
culture. Cultivation, selection and transplantation of embryo
are the key steps in determining a successful implantation
during IVF [4], [5]. During the development of embryos, the
morphological characteristics [6] and kinetic characteristics [7]
are highly correlated with the outcome of transplantation.

Time-lapse videos have been widely used in various repro-
ductive medicine centers during the cultivation of embryos [8]
to monitor them. A time-lapse video records the embryonic
development process in real time by taking photos of the
embryos at short time intervals [9]. Thus, a large amount of
time series image data for each embryo are produced in this
process. At the ﬁnal stage of embryo selection, an embryolo-
gist reviews the entire embryo development process to score

Z. Liu, Y. Cui, Y. Xu, Y. Wang and D. Wu are with the
Image Processing
the Ministry of Education for
Key Laboratory of
and Intelligent Control, School of Artiﬁcial
Intelligence and Automa-
tion, Huazhong University of Science and Technology, Wuhan 430074,
China. Email: zhliu95@hust.edu.cn, yqcui@hust.edu.cn, yfxu@hust.edu.cn,
wangyang sky@hust.edu.cn, drwu@hust.edu.cn.

B. Huang, B. Zhang, L. Zhu and L. Jin are with the Reproductive Medicine
Center, Tongji Hospital, Tongji Medical College, Huazhong University of
Science and Technology, Wuhan 430074, China. Email: hb2627@gmail.com,
dramanda@126.com, zhulixia027@foxmail.com, jinleirepro@yahoo.com.

Zihan Liu and Bo Huang contributed equally to this work.
Lei Jin and Dongrui Wu are the corresponding authors.

and sort them. Studies with different time-lapse equipment
reported improved prediction accuracy of embryo implantation
potential by analyzing the morphokinetics of human embryos
at early cleavage stages [8]–[12]. These features have been
shown to be statistically signiﬁcant to the ﬁnal outcome of
the transplantation [7].

There have been only a few approaches to analyze time-
lapse image data [9], [13]–[18]. Due to the limitations of the
time-lapse technology, stereoscopic cells of different heights
overlap in the images when photographed. It is difﬁcult for
even an experienced embryologist to accurately count the num-
ber of cells in a single time-lapse image when there are more
than eight cells. Therefore, most research focused on the early
development stages of embryos. Wong et al. [9] identiﬁed
several key parameters that can predict blastocyst formation
at the 4-cell stage from time-lapse images, and employed
sequential Monte Carlo based probabilistic model estimation
to monitor these parameters and track the cells. Wang et al.
[13] presented a multi-level embryo stage classiﬁcation ap-
proach, by using both hand-crafted and automatically learned
embryo features to identify the number of cells in a time-lapse
video. Conaghan et al. [14] used an automated and proprietary
image analysis software EEVATM (Early Embryo Viability
Assessment), which exhibited high image contrast through the
use of darkﬁeld illumination, to track cell divisions from one-
cell stage to four-cell stage. Their experiments veriﬁed that the
EEVA Test can signiﬁcantly improve embryologists’ ability to
identify embryos that would develop into usable blastocysts.
There are also several other studies on embryo selection by
using EEVATM [19]–[22], but they did not provide the details
of the used EEVA Test. Jonaitis et al. [15] compared the
performance of neural network, support vector machine and
nearest neighbor classiﬁer in detecting cell division time. Khan
et al. [18] used a deep convolutional neural network (CNN) to
classify the number of cells, and also semantic segmentation
to extract the cell regions in a time-lapse image [16]. Ng
et al. [17] combined late fusion networks with dynamic
programming (DP) to predict different cell development stages
and obtained better results than a single-frame model.

Multi-task learning has been successfully used in many
applications, such as natural language processing [23], speech
recognition [24], and computer vision [25]. Its basic idea is
to share representations among related tasks, so that each
trained model may have better generalization ability [26].
This paper proposes a multi-task deep learning with dynamic
programming (MTDL-DP) approach, which ﬁrst uses MTDL

 
 
 
 
 
 
II. CLASSIFICATION FRAMEWORKS

layer name

to pre-classify each frame in the time-lapse video to an embryo
development stage, and then DP to optimize the stage sequence
so that the stage number is monotonically non-decreasing,
which usually holds in practice. To our knowledge, this is the
ﬁrst study that applies MTDL to embryo early development
stage classiﬁcation from time-lapse videos.

The remainder of this paper is organized as follows: Sec-
tion II introduces four classiﬁcation frameworks for time-
lapse video analysis. Section III proposes our MTDL-DP
approach. Section IV presents the experimental results. Finally,
Section V draws conclusion.

This section introduces four frameworks for embryo early
development stage classiﬁcation from time-lapse videos. We
ﬁrst describe our dataset and the baseline network architecture,
and then extend it to many-to-one, one-to-many and many-to-
many MTDL frameworks.

A. Dataset

The time-lapse video dataset used in our experiments came
from the Reproductive Medicine Center of Tongji Hospital,
Huazhong University of Science and Technology, Wuhan,
China. It consisted of 170 time-lapse videos extracted from
incubators, using an EmbryoScope+ time-lapse microscope
system1 at 10-minute sampling interval. Each frame in the
video is a grayscale 800 × 800 image with a well number
in the lower left corner and a time marker after fertilization
in the lower right corner, as shown in Fig. 1. The embryo is
surrounded by some granulosa cells in the microscope ﬁeld.
The scale bar in the upper right corner indicates the size of the
cells. Each video began about 0-2 hours after fertilization, and
ended about 140 hours after fertilization. We only used the ﬁrst
N = 350 frames in each video, which were manually labeled
for the embryo development stages. Therefore, we had a total
of 170 × 350 = 59, 500 labeled frames in the experiment.

As in [17], we focused on the ﬁrst six embryo development
stages, which included initialization (tStart), the appearance
and breakdown of the male and female pronucleus (tPNf),
and the appearance of 2 through 4+ cells (t2, t3, t4, t4+). We
counted the number of images in different embryo develop-
ment stages in the dataset, and show the summary in Fig. 2.
Note that t3 was rarely observed in our dataset.

B. The Baseline One-to-One Classiﬁcation Framework

Let xn be the nth frame in a time-lapse video. For image
classiﬁcation, a standard one-to-one classiﬁcation framework
learns a mapping:

f0 : xn 7→ yn ∈ L

(1)

where yn is the stage label of xn, and L the label set of the
embryo development stages.

When information of the previous and future frames is
used, the standard one-to-one classiﬁcation framework can

2

be extended to many-to-one, one-to-many and many-to-many
MTDL frameworks, as illustrated in Fig. 3.

We used ResNet [27], which won the 2015 ImageNet
classiﬁcation competition, to process individual video frames.
Table I shows our baseline ResNet50 model. The input image
had three channels (RGB), each with 224×224 pixels (the
800×800 images were resized). The model was initialized
from the ResNet weights pre-trained on ImageNet [28], which
can help reduce overﬁtting on small datasets.

conv1

res2

res3

res4

res5

TABLE I
THE BASELINE RESNET50 MODEL.

50-layer
7 × 7, 64, stride 2
3 × 3 max pool, stride 2

output size
112 × 112 × 64











1 × 1, 64
3 × 3, 64
1 × 1, 256
1 × 1, 128
3 × 3, 128
1 × 1, 512
1 × 1, 256
3 × 3, 256
1 × 1, 1024
1 × 1, 512
3 × 3, 512
1 × 1, 2048
global average pool











fc

softmax

56 × 56 × 256

× 3

× 4

28 × 28 × 512











× 6

14 × 14 × 1024

× 3

7 × 7 × 2048











1 × 1 × 2048
1 × 1 × 1024
1 × 1 × 6

C. The Many-to-One MTDL Framework

The many-to-one MTDL framework, shown in Fig. 3(b),
is frequently used in video understanding [29]–[31] because
multiple frames in the same video usually have the same
label, and hence they can be considered together to predict the
ﬁnal label. Many-to-one can better make use of input context
information than one-to-one.

Many-to-one performs the following mapping:

f1 : (xn−τ , . . . , xn, . . . , xn+τ ) 7→ yn ∈ L,

(2)

where τ is the number of neighboring frames before and after
the current frame (the input context window size is hence 2τ +
1).

There are two common approaches to fuse time domain
information from the 2τ + 1 frames: Conv Pooling [32] and
Late Fusion [30].

1) Conv Pooling: This is a convolutional temporal feature
pooling architecture, which has been extensively used for
video classiﬁcation, especially for bag-of-words representa-
tions [33]. Image features are computed for each frame and
then max pooled. The pooling features can then be sent to fully
connected layers for ﬁnal classiﬁcation. A major advantage of

1https://www.vitrolife.com/products/time-lapse-systems/embryoscopeplus-

time-lapse-system/

50 μμ

50 μμ

3

(a)

(c)

50 μμ

50 μμ

(b)

(d)

Fig. 1. Sample frames from a time-lapse video. (a) 1-cell stage; (b) 2-cell stage; (c) 4-cell stage; (d) 4+-cell stage.

frame AlexNet model [34].

2) Late Fusion: In Late Fusion, all frames in the input
context window are encoded via identical ConvNets. The
ﬁnal representations after all convolutional layers are concate-
nated and passed through a fully-connected layer to generate
classiﬁcations. The concatenation can happen to a subset of
frames in the input context window [30], or to all frames
in that window [17]. Previous research [17] demonstrated
that Late Fusion ConvNets using 15 frames and a DP-based
decoder outperformed Early Fusion for predicting embryo
morphokinetics in time-lapse videos.

D. The One-to-Many MTDL Framework

One-to-many, shown in Fig. 3(c), means each input
is
mapped to multiple outputs, which is also called multi-task
nets [35] in deep learning. This paper uses hard parameter
sharing of hidden layers [26], as illustrated in Fig. 4. The
parameters of the convolution layers are shared among differ-
ent tasks, but those of the fully connected layers are trained
separately.

In one-to-many, each xn is used in classifying 2τ + 1
stages centered at n, i.e., it learns the following one-to-many

Fig. 2. Percentage of frames in different embryo development stages.

this approach is that spatial information in multiple frames,
output by the convolutional layers, is preserved through a
max pooling operation in the time domain. Experiments [32]
veriﬁed that Conv Pooling outperformed all other feature
pooling approaches on the Sports-1M dataset, using a 120-

4

(a)

(b)

Fig. 3. Different classiﬁcation frameworks. (a) one-to-one; (b) many-to-one; (c) one-to-many; (d) many-to-many. The convolutional layers are denoted by ‘C’.
Blue and red rectangles denote the ﬂatten layer and the max-pooling layer, respectively. Orange rectangles denote the fully connected and softmax layers.

(c)

(d)

(cid:28605)(cid:28642)(cid:28644)(cid:28649)(cid:28648)

the loss on all involved outputs:

(cid:28646)(cid:28667)(cid:28660)(cid:28677)(cid:28664)(cid:28663)(cid:28595)(cid:28671)(cid:28660)(cid:28684)(cid:28664)(cid:28677)(cid:28678)

(cid:28647)(cid:28660)(cid:28678)(cid:28670)(cid:28608)(cid:28678)(cid:28675)(cid:28664)(cid:28662)(cid:28668)(cid:28665)(cid:28668)(cid:28662)(cid:28595)(cid:28671)(cid:28660)(cid:28684)(cid:28664)(cid:28677)(cid:28678)

(cid:28616)(cid:28629)(cid:28647)(cid:28639)(cid:28564)(cid:28597)

(cid:1710)

(cid:28616)(cid:28629)(cid:28647)(cid:28639)(cid:28564)(cid:28598)

(cid:1710)

(cid:28616)(cid:28629)(cid:28647)(cid:28639)(cid:28564)(cid:28599)

Fig. 4. Hard parameter sharing for MTDL.

mapping:

f2 : xn 7→ (yn−τ , . . . , yn+τ ) ∈ L2τ +1.
xn’s classiﬁcation for the stage at time index t ∈ [n− τ, n+ τ ]
is a probability vector ˆpt(xn) ∈ R|L|×1.

(3)

At each Frame Index n, the corresponding label is estimated
by 2τ + 1 neighboring xt, t ∈ [n − τ, n + τ ]. We need to
aggregate them to obtain the ﬁnal classiﬁcation. This can be
done by an ensemble approach.

Because each frame xn is involved in 2τ + 1 outputs, the
total loss on a training frame xn is computed as the sum of

ℓ(xn) =

n+τ

X
t=n−τ

wt · ℓ(yt, ˆpt(xn)),

(4)

where wt is the weight for the t-th output, and yt is the true
label for Frame t. wt = 1 and the cross-entropy loss were
used in this paper. The cross-entropy loss on the t-th output
can be written as follows:

ℓ(yt, ˆpt(xn)) = − log (ˆpt,yt(xn)),

(5)

where ˆpt,yt(xn) is the yt-th element of ˆpt(xn).

E. The Many-to-Many MTDL Framework

Many-to-many can be viewed as a combination of one-to-
many and many-to-one. Each input frame is processed by a
separate CNN. Late Fusion was used, and the parameters of the
fully connected layers were also trained separately, as shown
in Fig. 3(d).

III. MULTI-TASK DEEP LEARNING WITH DYNAMIC
PROGRAMMING (MTDL-DP)

This section introduces our proposed MTDL-DP approach.

5

(

)

(cid:2198)(cid:3041)(cid:2879)(cid:2870)(cid:3099)

(cid:1824)(cid:3041)(cid:2879)(cid:3099)

(cid:1710)

(cid:1710)

(

)

(cid:2198)(cid:3041)(cid:2879)(cid:3099)

(cid:1824)(cid:3041)(cid:2879)(cid:3099)

(

)

(cid:2198)(cid:3041)(cid:2879)(cid:3099)

(cid:1824)(cid:3041)(cid:2879)(cid:2869)

(

)

(cid:2198)(cid:3041)(cid:2879)(cid:3099)

(cid:1824)(cid:3041)

(cid:1710)

(cid:1710)

(cid:1710)

(cid:1710)

(

)

(cid:2198)(cid:2196)

(cid:1824)(cid:3041)(cid:2879)(cid:3099)

(

)

(cid:2198)(cid:2196)

(cid:1824)(cid:3041)(cid:2879)(cid:2869)

(

)

(cid:2198)(cid:2196)

(cid:1824)(cid:3041)

(

)

(cid:2198)(cid:2196)

(cid:1824)(cid:3041)(cid:2878)(cid:2869)

(

)

(cid:2198)(cid:2196)

(cid:1824)(cid:3041)(cid:2878)(cid:3099)

(cid:1710)

(cid:1710)

(cid:1710)

(cid:1710)

(

)

(cid:2198)(cid:3041)(cid:2878)(cid:3099)

(cid:1824)(cid:3041)

(

)

(cid:2198)(cid:3041)(cid:2878)(cid:3099)

(cid:1824)(cid:3041)(cid:2878)(cid:2869)

(

)

(cid:2198)(cid:3041)(cid:2878)(cid:3099)

(cid:1824)(cid:3041)(cid:2878)(cid:3099)

(cid:1710)

(cid:1710)

(

)

(cid:2198)(cid:3041)(cid:2878)(cid:2870)(cid:3099)

(cid:1824)(cid:3041)(cid:2878)(cid:3099)

Fig. 5. Ensemble of the multi-task net’s predictions at Frame Index n, made by neighboring frames xt, t ∈ [n − τ, n + τ ].

A. Ensemble Learning for MTDL

As mentioned in Section II-D, a multi-task net has multiple
outputs. The easiest approach to get the ﬁnal classiﬁcation
corresponding to a speciﬁc frame is to choose the middle
output of the network. A more sophisticated approach is
ensemble learning [36]. We consider two common probabilis-
tic aggregation approaches in this paper: additive mean and
multiplicative mean.

Let ˆpn(xt) be the predicted probability vector at Frame
Index n, given Frame xt, t ∈ [n − τ, n + τ ], as illustrated
in Fig. 5. The ensemble probability ˆpn at Frame Index n,
aggregated by the additive mean, is:

ˆpn =

1
2τ + 1

n+τ

X
t=n−τ

ˆpn(xt),

If the multiplicative mean is used,

ˆpn =

1
2τ + 1

n+τ

Y
t=n−τ

ˆpn(xt).

(6)

(7)

Since each ˆpn(xt) is a vector, the summation in (6) and
multiplication in (7) are element-wise operations.

The ﬁnal classiﬁcation label ˆyn for Frame xn is obtained

by probability maximization:

ˆyn = arg max
1≤l≤|L|

ˆpn,l,

(8)

where ˆpn,l is the l-th element of ˆpn.

B. Post-processing with DP

The number of cells in the development of an embryo
is almost always non-decreasing [37]. However, this is not
guaranteed in the classiﬁcation outputs of MTDL. We use DP
to adjust the classiﬁcations so that this constraint is satisﬁed.
n=1 form
a sequence. MTDL outputs a probability vector ˆpn =
[pn,1, ..., pn,|L|]T before likelihood maximization at Frame n,
where ˆpn,l is the estimated probability that Frame n is at

the groundtruth stages {yn}N

For each video,

Stage l. We deﬁne E(ˆy, ˆP) as the total loss for an estimated
prediction ˆy = {ˆyn}N
n=1, given the model output probability
matrix ˆP = [ ˆp1, ..., ˆpN ]. The total loss is the sum of the
N
n=1 e(ˆyn, ˆpn), which must be optimized
per-frame losses P
subject to the monotonicity constraint: ˆyn+1 ≥ ˆyn, ∀n.

Two common per-frame losses [17] were used. The ﬁrst is

negative label likelihood (LL), deﬁned as:

eLL(ˆyn, ˆpn) = − log (ˆpn,yn).

(9)

The second is earth mover (EM) distance, deﬁned as:

eEM (ˆyn, ˆpn) = −

|L|

X
l=1

ˆpn,l|ˆyn − l|,

(10)

The ﬁnal classiﬁcation stage sequence ˆy∗ = {ˆyn}N

n=1 can

be obtained as:

N

ˆy∗ = arg min
ˆy={ˆyn}N
ˆyn+1 ≥ ˆyn, ∀n.

X
n=1

s.t.

n=1

e(ˆyn, ˆpn)

(11)

which can be easily solved by DP, as shown in Algorithm 1.

C. MTDL-DP

Our proposed MTDL-DP consists of three steps: 1) con-
struct a multi-task net with the one-to-many or many-to-many
MTDL framework; 2) use multiplicative mean to aggregate
the prediction of the multi-task net; and, 3) post-process with
DP using the EM distance per-frame loss. Its pseudocode is
given in Algorithm 2.

The one-to-many MTDL framework can also be replaced

by the many-to-many MTDL framework.

IV. EXPERIMENTAL RESULTS

This section investigates the performance of our proposed

MTDL-DP.

Algorithm 1: Pseudocode of dynamic programming (DP).

Input: N , the number of frames in a time-lapse video;
L, label set of embryo development stages;
ˆP = [ ˆp1, ..., ˆpN ] ∈ R|L|×N , the MTDL model output
probability matrix for the N frames.
Output: ˆy∗, the optimized stage sequence.
Set e(l, ˆpn) = 0 and E(l, ˆpn) = 0, ∀l ∈ [1, |L|],
∀n ∈ [1, N ];
for n = 1, ..., N do

for ˆy = 1, ..., |L| do

Compute e(ˆy, ˆpn) in (10);

end

end
for n = 2, ..., N do

for ˆy = 1, ..., |L| do

E(ˆy, ˆpn) = e(ˆy, ˆpn) + min
1≤l≤ˆy

E(l, ˆpn−1);

end

end
k = |L|;
for n = N, ..., 1 do
ˆyn = arg min
16l6k

E(l, ˆpn);

if ˆyn < k then
k = ˆyn;

end

end
ˆy∗ = {ˆyn}N
Return The optimized stage sequence ˆy∗.

n=1;

A. Experimental Setup

We created training/validation/test data partitions by ran-
domly selecting 70%/10%/20% videos from the dataset, i.e.,
41,650/5,950/11,900 frames, respectively. We resized each
frame to 224×224 so that it can be used by ResNet50, our
baseline model. Random rotation and ﬂip data augmentation
was used. All MTDL frameworks were initialized by the
weights trained by one-to-one (ResNet50). Then, the convo-
lution layer parameters were frozen, and the fully connected
layers were further tuned.

We used the cross-entropy loss function and Adam op-
timizer [38], and early stopping to reduce overﬁtting,
in
all experiments. Multiplicative mean and EM distance per-
frame loss were used in the MTDL-DP. All experiments were
repeated ﬁve times, and the mean results were reported.

B. Classiﬁcation Accuracy

First, we considered MTDL only, without using DP. The
classiﬁcation accuracies are shown in the left panel of Table II,
with τ = {1, 4, 7} (the output context window size was
2τ + 1). All MTDL frameworks outperformed the one-to-
one framework, suggesting using neighboring input or label
information in multi-task learning was indeed beneﬁcial.

For the many-to-one MTDL framework, when τ increased,
the performance of Late Fusion also increased, whereas the
performance of Conv Pooling decreased. This is intuitive,

6

Algorithm 2: MTDL-DP
Input: N , the number of frames in a time-lapse video;
D, set of labeled time-lapse videos;
{xn}N
n=1, frames to be labeled;
τ , the number of left and right neighboring frames in the
context window.
Output: ˆy∗, the labeled stage sequence.
Use the one-to-one framework to train a baseline model
f0 from D;
Initialize an MTDL model, whose convolution layer
parameters are identical to f0;
Fine-tune the fully connected layer parameters of the
MTDL model on D;
for n = 1, ..., N do

Use the MTDL model to compute ˆpt(xn),
t = n − τ, ..., n + τ ;

end
for n = 1, ..., N do

Compute ˆpn by (7);
Compute the per-frame loss eEM (ˆyn, ˆpn) in (10);

end
Solve for ˆy∗ in (11) by Algorithm 1;
Return The optimized stage sequence ˆy∗.

because more input information was ignored in Conv Pooling
when τ increased.

The classiﬁcation accuracies with DP post-processing are
shown in the right panel of Table II. Post-processing increased
the classiﬁcation accuracies for all classiﬁers and different τ ,
e.g., the ﬁve classiﬁers achieved 2.3%, 1.2%, 2.1%, 1.5%,
and 2.0% performance improvement when τ = 1, respec-
tively. However, as τ increased, the classiﬁcation performance
improvements became less obvious. After post-processing,
the many-to-many and one-to-many frameworks had higher
accuracies than the many-to-one framework, and only many-
to-many consistently outperformed one-to-one for all τ , sug-
gesting post-processing may be more beneﬁcial when more
input and output information was utilized.

C. Root Mean Squared Error (RMSE)

We also computed the root mean squared error (RMSE)
between the true video label sequences and the classiﬁcations.
The RMSEs without DP post-processing are shown in the left
panel of Table III. All MTDL frameworks had lower RMSEs
than the one-to-one framework, suggesting again that using
neighboring input or label information in multi-task learning
was beneﬁcial.

The results after DP post-processing are shown in the right
panel of Table III. DP post-processing reduced the RMSE
for all MTDL frameworks and different τ , suggesting that
DP was indeed beneﬁcial. Though all MTDL frameworks
outperformed the one-to-one framework only at τ = 1, the
many-to-many framework consistently outperformed one-to-
one for all different τ .

TABLE II
CLASSIFICATION ACCURACIES FOR DIFFERENT CLASSIFICATION FRAMEWORKS AND τ , BEFORE AND AFTER DP POST-PROCESSING.

7

Framework

Method

Accuracy without DP
τ = 4

τ = 7

τ = 1

Accuracy with DP
τ = 4

τ = 1

τ = 7

One-to-one

Many-to-one

One-to-many

Many-to-many

ResNet50

Conv Pooling

Late Fusion

Multi-Task Nets (ours)

83.8%

84.7%

83.9%

83.8%

84.4%

84.6%

85.0% 85.4%

83.8%

83.8%

85.1%

85.3%

86.1%

85.9%

86.0%

86.5%

86.1%

85.1%

85.2%

85.8%

86.1%

84.5%

85.2%

85.7%

84.6% 85.7% 85.8% 86.6% 86.5% 86.9%

TABLE III
RMSES FOR DIFFERENT CLASSIFICATION FRAMEWORKS AND τ , BEFORE AND AFTER DP POST-PROCESSING.

Framework

Method

RMSE without DP
τ = 4

τ = 1

τ = 7

RMSE with DP
τ = 4

τ = 1

τ = 7

One-to-one

ResNet50

0.4840

0.4840

0.4840

0.4199

0.4199

0.4199

Many-to-one

One-to-many

Many-to-many

Conv Pooling

Late Fusion

Multi-Task Nets (ours)

0.4728

0.4761

0.4638

0.4752

0.4690

0.4531

0.4695

0.4640

0.4795

0.4740

0.4480

0.4360

0.4066

0.4036

0.3964

0.4085

0.4432

0.4254

0.4155

0.4077

0.4419

0.4214

0.4260

0.4083

D. Training Time

The training time of different models, averaged over ﬁve
runs, is shown in Table IV. The training time of the many-
to-one and many-to-many MTDL frameworks increased about
linearly with the input context size; however, the training time
of the one-to-many MTDL framework was insensitive to τ ,
which is an advantage.

TABLE IV
TRAINING TIME FOR DIFFERENT CLASSIFICATION FRAMEWORKS AND τ .

Framework

Method

One-to-one

Many-to-one

One-to-many

Many-to-many

ResNet50

Conv Pooling

Late Fusion

Multi-Task Nets (ours)

Training time (s)
τ = 4
2231

τ = 7
2231

τ = 1
2231

5318

4892

2246

5759

15378

17390

2265

29139

27534

2542

16182

27808

0.860

One-to-Many

Without ensemble
Add mean
Mul mean

0.855

0.850

y
c
a
r
u
c
c
A

0.845

1

0.865

0.860

0.855

0.850

y
c
a
r
u
c
c
A

4
τ

(a)

7

Many-to-Many

Without ensemble
Add mean
Mul mean

E. Comparison of Different Ensemble Approaches

We also compared the performances of different ensemble
approaches introduced in Section III-A, without considering
DP post-processing. The CNN models were constructed using
the one-to-many and many-to-many MTDL frameworks. The
results are shown in Figs. 6 and 7. Both additive mean
and multiplicative mean achieved performance improvements.
Multiplicative mean also slightly outperformed additive mean.
As τ increased, the performance of the many-to-many MTDL
framework was improved. The one-to-many MTDL framework
had the best performance when τ = 4.

F. Comparison of Different Losses in DP Post-Processing

Next, we studied the effect of different per-frame losses in
DP post-processing. The RMSEs for different τ and different

0.845

0.840

1

4
τ

(b)

7

Fig. 6. Classiﬁcation accuracies with and without ensemble learning. (a)
One-to-many; (b) Many-to-many.

MTDL frameworks are shown in Fig. 8. The EM loss always
gave smaller RMSEs than the LL loss.

The true stage labels, and the classiﬁed labels before and
after DP in two time-lapse videos, are shown in Fig. 9. Clearly,
DP smoothed the classiﬁcations, and its outputs were closer
to the groundtruth labels.

0.49

0.48

E
S
M
R

0.47

0.46

0.45

0.48

0.47

E
S
M
R

0.46

0.45

0.44

1

1

One-to-Many

Without ensemble
Add mean
Mul mean

4
τ

(a)

Many-to-Many

Without ensemble
Add mean
Mul mean

4
τ

(b)

7

7

Fig. 7. RMSEs with and without ensemble learning. (a) One-to-many; (b)
Many-to-many.

The confusion matrix for the one-to-many MTDL frame-
work, using the multiplicative mean and τ = 1, is shown
in Fig. 10(a) before DP post-processing, and in Fig. 10(b)
after DP post-processing. The diagonal shows the classiﬁcation
accuracy of each individual cell stage. Post-processing im-
proved the accuracy of all embryonic stages except t3, whose
classiﬁcation accuracy before DP (16%) was much lower than
others. There may be two reasons for this: 1) Stage t3 had
much fewer training examples in our dataset (see Fig. 2), and
hence it was not trained adequately; and, 2) the low accuracy
of t3 may also be due to multipolar cleavages from the zygote
stage, which occurs in 12.2% of human embryos [39].

V. CONCLUSION

Accurate classiﬁcation of embryo early development stages
can provide embryologists valuable information for assessing
the embryo quality, and hence is critical to the success of
IVF. This paper has proposed an MTDL-DP approach for
automatic embryo development stage classiﬁcation from time-
lapse videos. Particularly, the one-to-many and many-to-many
MTDL frameworks performed the best. Considering the trade-
off between training time and classiﬁcation accuracy, we
recommend the one-to-many MTDL framework in MTDL-DP,
because it can achieve comparable performance with the many-
to-many MTDL framework, with much lower computational
cost.

To our knowledge, this is the ﬁrst study that applies MTDL
to embryo early development stage classiﬁcation from time-
lapse videos.

8

τ = 1

LL loss
EM loss

1

2

3
Approaches

4

5

(a)

τ = 4

LL loss
EM loss

1

2

3
Approaches

4

5

(b)

τ = 7

LL loss
EM loss

0.43

E
S
M
R

0.42

0.41

0.47

0.46

0.45

E
S
M
R

0.44

0.43

0.42

0.41

0.46

E
S
M
R

0.44

0.42

1

2

3
Approaches

4

5

(c)

Fig. 8. RMSEs of different per-frame losses in DP. (a) τ = 1; (b) τ =
4; (b) τ = 7. The numbers on the horizontal axis denote different MTDL
frameworks: 1–One-to-one, 2–Many-to-one (Conv Pooling), 3–Many-to-one
(Late Fusion), 4–One-to-many, 5–Many-to-many.

REFERENCES

[1] B. Huang, X. Ren, L. Wu, L. Zhu, B. Xu, Y. Li, J. Ai, and L. Jin,
“Elevated progesterone levels on the day of oocyte maturation may affect
top quality embryo IVF cycles,” PLoS One, vol. 11, no. 1, p. e0145895,
2016.

[2] B. Huang, D. Hu, K. Qian, J. Ai, Y. Li, L. Jin, G. Zhu, and H. Zhang,
“Is frozen embryo transfer cycle associated with a signiﬁcantly lower
incidence of ectopic pregnancy? an analysis of more than 30,000 cycles,”
Fertility Sterility, vol. 102, no. 5, pp. 1345–1349, 2014.

[3] B. Huang, K. Qian, Z. Li, J. Yue, W. Yang, G. Zhu, and H. Zhang,
“Neonatal outcomes after early rescue intracytoplasmic sperm injection:
an analysis of a 5-year period,” Fertility Sterility, vol. 103, no. 6, pp.
1432–1437, 2015.

[4] A. S. in Reproductive Medicine and E. S. I. G. of Embryology, “The
istanbul consensus workshop on embryo assessment: Proceedings of an
expert meeting,” Human Reproduction, vol. 26, no. 6, pp. 1270–1283,
2011.

[5] B. Tomasz, K. Rafal, and G. Wojciech, “Methods of embryo scoring

t4+

t4

t3

t2

e
g
a
t
s

l
l

e
C

tPNf

tStart

t4+

t4

t3

t2

e
g
a
t
s

l
l

e
C

tPNf

tStart

True
Before DP
After DP

0

100

200

300

Video frame

(a)

True
Before DP
After DP

0

100

200

300

Video frame

(b)

Fig. 9. True stage labels, and classiﬁcations before and after DP, in two
time-lapse videos. One-to-many and τ = 1 were used.

in in vitro fertilization,” Reproductive Biology, vol. 4, no. 1, pp. 5–22,
2004.

[6] J. Holte, L. Berglund, K. Milton, C. Garello, G. Gennarelli, A. Revelli,
and T. Bergh, “Construction of an evidence-based integrated morphology
cleavage embryo score for implantation potential of embryos scored
and transferred on day 2 after oocyte retrieval,” Human Reproduction,
vol. 22, no. 2, pp. 548–557, 2006.

[7] J. Lemmen, I. Agerholm, and S. Ziebe, “Kinetic markers of human em-
bryo quality using time-lapse recordings of IVF/ICSI-fertilized oocytes,”
Reproductive Biomedicine Online, vol. 17, no. 3, pp. 385–391, 2008.
[8] K. Kirkegaard, I. E. Agerholm, and H. J. Ingerslev, “Time-lapse moni-
toring as a tool for clinical embryo assessment,” Human Reproduction,
vol. 27, no. 5, pp. 1277–1285, 2012.

[9] C. C. Wong, K. E. Loewke, N. L. Bossert, B. Behr, C. J. De Jonge,
T. M. Baer, and R. A. R. Pera, “Non-invasive imaging of human
embryos before embryonic genome activation predicts development to
the blastocyst stage,” Nature Biotechnology, vol. 28, no. 10, pp. 1115–
1121, 2010.

[10] J. Herrero, A. Tejera, C. Albert, C. Vidal, M. J. De Los Santos,
and M. Meseguer, “A time to look back: Analysis of morphokinetic
characteristics of human embryo development,” Fertility Sterility, vol.
100, no. 6, pp. 1602–1609, 2013.

[11] A. A. Chen, L. Tan, V. Suraj, R. R. Pera, and S. Shen, “Biomarkers
identiﬁed with time-lapse imaging: Discovery, validation, and practical
application,” Fertility Sterility, vol. 99, no. 4, pp. 1035–1043, 2013.
[12] M. Meseguer, J. Herrero, A. Tejera, K. M. Hilligsoe, N. B. Ramsing,
and J. Remohi, “The use of morphokinetics as a predictor of embryo
implantation,” Human Reproduction, vol. 26, no. 10, pp. 2658–2671,
2011.

[13] Y. Wang, F. Moussavi, and P. Lorenzen, “Automated embryo stage
classiﬁcation in time-lapse microscopy video of early human embryo
development,” in Proc. 16th Int’l Conf. on Medical Image Computing
and Computer-Assisted Intervention (ICMICCAI), Nagoya, Japan, Sep.
2013, pp. 460–467.

[14] J. Conaghan, A. A. Chen, S. P. Willman, K.

Ivani, P. E. Ch-
enette, R. Boostanfar, V. L. Baker, G. D. Adamson, M. E. Abusief,
M. Gvakharia et al., “Improving embryo selection using a computer-
automated time-lapse image analysis test plus day 3 morphology: results

9

0.8

0.6

0.4

0.2

0.0

0.8

0.6

0.4

0.2

0.0

Confusion matrix

0.79

0.21

0.00

0.00

0.00

0.00

0.06

0.92

0.02

0.00

0.00

0.00

0.00

0.00

0.97

0.01

0.01

0.00

0.00

0.00

0.24

0.16

0.50

0.10

0.00

0.00

0.02

0.01

0.89

0.09

tStart

tPNf

l

e
b
a

l

e
u
r
T

t2

t3

t4

t4+

0.00

0.00

0.01

0.01

0.11

0.87

tStart

tP Nf

t2

t3

t4

t4 +

Predicted label
(a)

Confusion matrix

tStart

tPNf

l

e
b
a

l

e
u
r
T

t2

t3

t4

0.80

0.20

0.00

0.00

0.00

0.00

0.05

0.92

0.03

0.00

0.00

0.00

0.00

0.00

0.98

0.00

0.01

0.00

0.00

0.00

0.17

0.05

0.77

0.01

0.00

0.00

0.01

0.00

0.92

0.08

t4+

0.00

0.00

0.00

0.00

0.11

0.89

tStart

tP Nf

t2

t3

t4

t4 +

Predicted label
(b)

Fig. 10. Confusion matrices (a) before and (b) after DP post-processing.

from a prospective multicenter trial,” Fertility and Sterility, vol. 100,
no. 2, pp. 412–419, 2013.

[15] D. Jonaitis, V. Raudonis, and A. Lipnickas, “Application of numerical
intelligence methods for the automatic quality grading of an embryo
development,” International Journal of Computing, vol. 15, no. 3, pp.
177–183, 2016.

[16] A. Khan, S. Gould, and M. Salzmann, “Segmentation of developing
human embryo in time-lapse microscopy,” in Proc. 13th Int’l Symposium
on Biomedical Imaging (ISBI), Prague, Czech Republic, April 2016, pp.
930–934.

[17] N. H. Ng, J. McAuley,

J. A. Gingold, N. Desai, and Z. C.
Lipton, “Predicting embryo morphokinetics
in videos with late
fusion nets & dynamic decoders,” May 2018. [Online]. Available:
https://openreview.net/forum?id=By1QAYkvz

[18] A. Khan, S. Gould, and M. Salzmann, “Deep convolutional neural
networks for human embryonic cell counting,” in Proc. 14th European
Conf. on Computer Vision (ECCV), Amsterdam, The Netherlands, Oc-
tober 2016, pp. 339–348.

[19] M. D. VerMilyea, L. Tan, J. T. Anthony, J. Conaghan, K. Ivani,
M. Gvakharia, R. Boostanfar, V. L. Baker, V. Suraj, A. A. Chen et al.,
“Computer-automated time-lapse analysis results correlate with embryo
implantation and clinical pregnancy: a blinded, multi-centre study,”
Reproductive Biomedicine Online, vol. 29, no. 6, pp. 729–736, Dec.
2014.

[20] M. P. Diamond, V. Suraj, E. J. Behnke, X. Yang, M. J. Angle, J. C.
Lambe-Steinmiller, R. Watterson, K. A. Wirka, A. A. Chen, and S. Shen,
“Using the Eeva Test adjunctively to traditional day 3 morphology is

 
 
 
 
10

informative for consistent embryo assessment within a panel of embry-
ologists with diverse experience,” Journal of Assisted Reproduction and
Genetics, vol. 32, no. 1, pp. 61–68, Jan. 2015.

[21] B. Aparicio-Ruiz, N. Basile, S. P. Albal´a, F. Bronet, J. Remoh´ı, and
M. Meseguer, “Automatic time-lapse instrument is superior to single-
point morphology observation for selecting viable embryos: retrospective
study in oocyte donation,” Fertility and Sterility, vol. 106, no. 6, pp.
1379–1385, Nov. 2016.

[22] D. C. Kieslinger, S. De Gheselle, C. B. Lambalk, P. De Sutter, E. H.
Kostelijk, J. W. R. Twisk, J. van Rijswijk, E. Van den Abbeel, and
C. G. Vergouw, “Embryo selection using time-lapse analysis (Early Em-
bryo Viability Assessment) in conjunction with standard morphology:
a prospective two-center pilot study,” Human Reproduction, vol. 31,
no. 11, pp. 2450–2457, Nov. 2016.

[23] R. Collobert and J. Weston, “A uniﬁed architecture for natural language
processing: Deep neural networks with multitask learning,” in Proc. 25th
Int’l Conf. on Machine Learning (ICML), Helsinki, Finland, July 2008,
pp. 160–167.

[24] L. Deng, G. Hinton, and B. Kingsbury, “New types of deep neural
network learning for speech recognition and related applications: an
overview,” in Proc. 38th Int’l Conf. on Acoustics, Speech, and Signal
Processing (ICASSP), Vancouver, Canada, May 2013, pp. 8599–8603.
[25] R. Girshick, “Fast R-CNN,” in Proc. Int’l Conf. on Computer Vision

(ICCV), Santiago, Chile, December 2015.

[26] S. Ruder, “An overview of multi-task learning in deep neural
[Online]. Available:

networks,” CoRR, vol. abs/1706.05098, 2017.
http://arxiv.org/abs/1706.05098

[27] K. He, X. Zhang, S. Ren, and J. Sun, “Deep residual learning for
image recognition,” in Proc. IEEE Conf. on Computer Vision and Pattern
Recognition (CVPR), Las Vegas, NV, June 2016, pp. 770–778.

[28] J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-Fei, “ImageNet:
image database,” in Proc. IEEE Conf. on
A large-scale hierarchical
Computer Vision and Pattern Recognition (CVPR), Miami Beach, FL,
June 2009, pp. 248–255.

[29] K. Soomro, A. R. Zamir, and M. Shah, “UCF101: A dataset of
101 human actions classes from videos in the wild,” CoRR, vol.
abs/1212.0402, 2012. [Online]. Available: http://arxiv.org/abs/1212.0402
[30] A. Karpathy, G. Toderici, S. Shetty, T. Leung, R. Sukthankar, and
L. Fei-Fei, “Large-scale video classiﬁcation with convolutional neural
networks,” in Proc. IEEE Conf. on Computer Vision and Pattern
Recognition (CVPR).
Columbus, OH: IEEE, June 2014, pp. 1725–
1732.

[31] F. Caba Heilbron, V. Escorcia, B. Ghanem, and J. Carlos Niebles,
“Activitynet: A large-scale video benchmark for human activity un-
derstanding,” in Proc. IEEE Conf. on Computer Vision and Pattern
Recognition (CVPR). Boston, MA: IEEE, June 2015, pp. 961–970.

[32] J. Y.-H. Ng, M. Hausknecht, S. Vijayanarasimhan, O. Vinyals, R. Monga,
and G. Toderici, “Beyond short snippets: Deep networks for video
classiﬁcation,” in Proc. IEEE Conf. on Computer Vision and Pattern
Recognition (CVPR). Boston, MA: IEEE, June 2015, pp. 4694–4702.
[33] L. Fei-Fei and P. Perona, “A bayesian hierarchical model for learning
natural scene categories,” in Proc. IEEE Conf. on Computer Vision and
Pattern Recognition (CVPR), vol. 2, San Diego, CA, June 2005, pp.
524–531.

[34] A. Krizhevsky, I. Sutskever, and G. E. Hinton, “ImageNet classiﬁcation
with deep convolutional neural networks,” in Proc. Advances in Neural
Information Processing Systems, Lake Tahoe, NV, December 2012, pp.
1097–1105.

[35] G. E. Dahl, N. Jaitly, and R. Salakhutdinov, “Multi-task neural networks

for qsar predictions,” arXiv preprint arXiv:1406.1231, 2014.

[36] Z.-H. Zhou, Ensemble methods: foundations and algorithms.

Boca

Raton, FL: CRC press, 2012.

[37] Y. Liu, V. Chapple, P. Roberts, and P. Matson, “Prevalence, consequence,
and signiﬁcance of reverse cleavage by human embryos viewed with the
use of the embryoscope time-lapse video system,” Fertility Sterility, vol.
102, no. 5, pp. 1295–1300, 2014.

[38] D. P. Kingma

stochastic
optimization,” CoRR, vol. abs/1412.6980, 2014. [Online]. Available:
https://arxiv.org/abs/1412.6980

“Adam: A method for

and J. Ba,

[39] B. Kalatova, R. Jesenska, D. Hlinka, and M. Dudas, “Tripolar mitosis
in human cells and embryos: occurrence, pathophysiology and medical
implications,” Acta Histochemica, vol. 117, no. 1, pp. 111–125, 2015.

