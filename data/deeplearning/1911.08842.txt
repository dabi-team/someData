Neural Approximate Dynamic Programming for On-Demand Ride-Pooling

Sanket Shah, Meghna Lowalekar, Pradeep Varakantham
School of Information Systems, Singapore Management University
sankets@smu.edu.sg, meghnal.2015@phdcs.smu.edu.sg, pradeepv@smu.edu.sg

9
1
0
2

v
o
N
0
2

]

Y
C
.
s
c
[

1
v
2
4
8
8
0
.
1
1
9
1
:
v
i
X
r
a

Abstract

On-demand ride-pooling (e.g., UberPool, LyftLine, Grab-
Share) has recently become popular because of its ability to
lower costs for passengers while simultaneously increasing
revenue for drivers and aggregation companies (e.g., Uber).
Unlike in Taxi on Demand (ToD) services – where a vehi-
cle is assigned one passenger at a time – in on-demand ride-
pooling, each (possibly partially ﬁlled) vehicle must be as-
signed a group of passenger requests with multiple different
origin and destination pairs in such a way that quality con-
straints are not violated. To ensure near real-time response,
existing solutions to the real-time ride-pooling problem are
myopic in that they optimise the objective (e.g., maximise the
number of passengers served) for the current time step with-
out considering the effect such an assignment could have on
feasible assignments in future time steps. However, consid-
ering the future effects of an assignment that already has to
consider what combinations of passenger requests can be as-
signed to vehicles adds an extra layer of combinatorial com-
plexity on top of the already challenging problem of consid-
ering future effects in the ToD case.
A popular approach that addresses the limitations of my-
opic assignments in ToD problems is Approximate Dynamic
Programming (ADP). Existing ADP methods for ToD can
only handle Linear Program (LP) based assignments as the
value update relies on dual values from the LP. The as-
signment problem in ride pooling requires an Integer Lin-
ear Program (ILP) that has bad LP relaxations. Therefore,
our key technical contribution is in providing a general ADP
method that can learn from the ILP based assignment found
in ride-pooling. Additionally, we handle the extra combina-
torial complexity from combinations of passenger requests
by using a Neural Network based approximate value function
and show a connection to Deep Reinforcement Learning that
allows us to learn this value-function with increased stability
and sample-efﬁciency. We show that our approach easily out-
performs leading approaches for on-demand ride-pooling on
a real-world dataset by up to 16%, a signiﬁcant improvement
in city-scale transportation problems.

1

Introduction

On-demand ride-pooling, exempliﬁed by UberPool, Lyft-
Line, GrabShare, etc., has become hugely popular in major
cities with 20% of all Uber trips coming from their ride-
pool offering UberPool (Heath 2016; Gessner 2019). Apart
from reducing emissions and trafﬁc congestion compared to

Taxi/car on-Demand (ToD) services (e.g., UberX, Lyft), it
beneﬁts all the stakeholders involved: (a) Individual passen-
gers have reduced costs as these are shared by overlapping
passengers; (b) Vehicles make more money per trip as mul-
tiple passengers (or passenger groups) are present; (c) For
the centralized entity (like Uber, Lyft etc.) more customer
requests can be satisﬁed with the same number of vehicles.
Underlying these on-demand ride-pooling services is the
Ride-Pool Matching Problem (RMP) (Alonso-Mora et al.
2017; Bei and Zhang 2018; Lowalekar, Varakantham, and
Jaillet 2019). The objective in the RMP is to assign groups of
user requests to vehicles that can serve them, online, subject
to predeﬁned quality constraints (e.g., the detour delay for
a customer cannot be more than 10 minutes) in such a way
that a quality metric is maximised (e.g., revenue). The RMP
reduces to the taxi/car-on-demand (ToD) problem when the
capacity, i.e., the maximum number of simultaneous passen-
gers (with different origin and destination pairs) that can be
served by a vehicle, is 1. In this paper, we consider the most
general version of the RMP, in which batches of requests are
assigned to vehicles of arbitrary capacity, and present a solu-
tion that can scale to practical use-cases involving thousands
of locations and vehicles.

Past research in solving RMP problems can be catego-
rized along four threads. Past work along the ﬁrst thread
employs traditional planning approaches to model the RMP
as an optimisation problem (Ropke and Cordeau 2009;
Ritzinger, Puchinger, and Hartl 2016; Parragh, Doerner, and
Hartl 2008). The problem with this class of approaches
is that they don’t scale to on-demand city-scale scenarios.
The second thread consists of approaches that make the
best greedy assignments (Ma, Zheng, and Wolfson 2013;
Tong et al. 2018; Huang et al. 2014; Lowalekar, Varakan-
tham, and Jaillet 2019; Alonso-Mora et al. 2017). While
these scale well, they are myopic and, as a result, do not
consider the impact of a given assignment on future assign-
ments. The third thread, consists of approaches that use Re-
inforcement Learning (RL) to address the myopia associ-
ated with approaches from the second category for the ToD
problem (Xu et al. 2018; Lin et al. 2018; Li et al. 2019;
Wang et al. 2018; Verma et al. 2017). Past RL work for the
ToD problem cannot be extended to solve the RMP, however,
because it relies heavily on the assumption that vehicles can
only serve one passenger at a time.

 
 
 
 
 
 
Lastly, there has been work in operations research that
uses the Approximate Dynamic Programming (ADP) frame-
work to solve the ToD problem (Powell 2007) and a spe-
cial case of a capacity-2 RMP (Yu and Shen 2019). In ADP,
matching is performed using a learned value function that
estimates the future value of performing a certain matching.
There are two major reasons for why past ADP approaches
for solving the ToD problem cannot immediately be applied
to the RMP, however. Firstly, the value function approxima-
tion in past work is linear and its update relies heavily on
the assumption that matching can be modelled as a Linear
Program (LP); this does not hold for the RMP with arbitrary
vehicle capacities. Secondly, in the RMP, passengers may be
assigned to a partially ﬁlled vehicle at each time step. This
results in a complex state space for each vehicle that is com-
binatorial (combinations of requests already assigned) in the
vehicle capacity (more details in Section 4).

We make three key contributions in this paper. First,
we formulate the arbitrary capacity RMP problem as an
Approximate Dynamic Programming problem. Second, we
propose Neural ADP (NeurADP), a general ADP method
that can learn value functions (approximated using Neu-
ral Networks) from ILP based assignment problems. Fi-
nally, we bring together techniques from Deep Q-Networks
(DQN) (Mnih et al. 2015) to improve the stability and scal-
ability of NeurADP.

In the experiments, we compare our approach to two lead-
ing approaches for the RMP on a real-world dataset (NYYel-
lowTaxi 2016). Compared to a baseline approach proposed
by (Alonso-Mora et al. 2017), we show that our approach
serves up to 16% more seen requests across different pa-
rameter settings. This translates to a relative improvement
of 40% over the baseline.

2 Background: Approximate Dynamic

Programming (ADP)

ADP is a framework based on the Markov Decision Prob-
lem (MDP) model for tackling large multi-period stochastic
ﬂeet optimisation problems (Powell 2007) such as ToD. The
problem is formulated using the tuple (cid:104)S, A, ξ, T, O(cid:105):

S: denotes the system state with st denoting the state of sys-

tem at decision epoch t.

A: denotes the set of all possible actions 1 (which satisfy the
constraints on the action space) with At denoting the set
of possible actions at decision epoch t. at ∈ At is used to
denote an action at decision epoch t.

ξ: denotes the exogenous information – the source of ran-
domness in the system. For instance, this would corre-
spond to demand in ToD problems. ξt denotes the exoge-
nous information (e.g., demand) at time t.

T : denotes the transition function which describes how the

system state evolves over time.

O: denotes the objective function with ot(st, at) denoting the

value obtained on applying action at on state st.

1We use action and decision interchangeably in the paper.

as

In

happens

an MDP,

(s0, a0, sa

system evolution

as
(s0, a0, s1, a1, s2, ....). However, in an ADP, the evolution
0, ξ1, s1, a1, sa
t , · · · ),
happens
where st denotes the pre-decision state at decision epoch t
and sa
2 denotes the post-decision state (Powell 2007). The
t
transition from state st to st+1 depends on the action vector
at and the exogenous information ξt+1. Therefore,

1, · · · , st, at, sa

st+1 = T (st, at, ξt+1)

Using post-decision state, this transition can be written as

t = T a(st, at); st+1 = T ξ(sa
sa

t , ξt+1)

Let V (st) denotes the value of being in state st at decision

epoch t, then using Bellman equation we get

V (st) = max
at∈At

(O(st, at) + γE[V (st+1)|st, at, ξt+1])

where γ is the discount factor. Using post-decision state, this
expression can be broken down into two parts:

V (st) = max
at∈At
t ) = E[V (st+1)|sa

V a(sa

t , ξt+1]

(O(st, at) + γV a(sa

t ))

(1)

(2)

The advantage of this decomposition is that Equation 1 can
be solved using an LP in ﬂeet optimisation problems. The
basic idea in any ADP algorithm is to deﬁne a value func-
tion approximation around post-decision state, V a(sa
t )) and
to update it by stepping forward through time using sample
realizations of exogenous information (i.e. demand in ﬂeet
optimisation that is typically observed in data). Please refer
to Powell (Powell 2007) for more details.

3 Ride-pool Matching Problem (RMP)
In this problem, we consider a ﬂeet of vehicles/resources R
with random initial locations, travelling on a predeﬁned road
network G. Passengers that want to travel from one location
to another send requests to a central entity that collects these
requests over a time-window called the decision epoch ∆.
The goal of the RMP is to match these collected requests
U t to empty or partially ﬁlled vehicles that can serve them
such that an objective O is maximised subject to constraints
on the delay D. These delay constraints D are important be-
cause customers are only willing to trade being delayed in
exchange for a reduced fare up to a point. In this paper, we
consider the objective O to be the number of requests served.
We provide a formal deﬁnition for the RMP using the tu-

ple (cid:104)G, U, R, D, ∆, O(cid:105):
G: (L, E) is a weighted graph representing the road network.
Along the lines of (Alonso-Mora et al. 2017), L denotes
the set of street intersections and E deﬁnes the adjacency
of these intersections. Here, the weights correspond to the
travel time for a road segment. We assume that vehicles
only pick up and drop people off at intersections.

U: denotes the set of user requests. U = ×t Ut is the combi-
nation of requests that we observe at each decision epoch

2Here a is just used to indicate that it is post decision state and

it does not correspond to any speciﬁc action.

Figure 1: Schematic outlining our overall approach. We start with a hypothetical G, U and R in (A). The grid represents a
road network. The blue people and circles correspond to user requests and the nearest street intersection that they’re mapped
to respectively. The blue dotted lines represent the shortest path between the pick-up and drop-off points of a request. The red
and green triangles correspond to existing pick-up/drop-off points for the red and green vehicles respectively. The dotted lines
describe their current trajectory. In (B) we map the requests and their combinations to vehicles that can serve them under the
constraints deﬁned by D to create feasible actions using the approach presented in (Alonso-Mora et al. 2017). In (C), we score
each of these feasible actions using our Neural Network Value Function. In (D), we create a mapping of requests to vehicles
that maximises the sum of scores generated in (C) using the Integer Linear Program (ILP) in Table 1. In (E), we use this ﬁnal
mapping to update the score function (Section 4). In (F), we simulate the motion of vehicles until the next epoch either based
on their current trajectories or a re-balancing strategy. This process then repeats for the next decision epoch.

t , t

t ∈ Ut is represented by the tuple:
t , ej
t ∈ L denote the origin and desti-

t. Each request uj
(cid:69)
(cid:68)
, where oj
t , ej
oj
nation and t denotes the arrival epoch of the request.
R: denotes the set of resources/vehicles. Each element i ∈ R
is represented by the tuple (cid:10)ci, pi, Li(cid:11). ci denotes the ca-
pacity of the vehicle, i.e., the maximum number of pas-
sengers it can carry simultaneously, pi its current position
and Li the ordered list of locations that the vehicle should
visit next to satisfy the requests currently assigned to it.
D: {τ, λ} denotes the set of constraints on delay. τ denotes
the maximum allowed pick-up delay which is the differ-
ence between the arrival time of a request and the time
at which a vehicle picks the user up. λ denotes the max-
imum allowed detour delay which is the difference be-
tween the time at which the user arrived at their destina-
tion in a shared cab and the time at which they would have
arrived if they had taken a single-passenger cab.

∆: denotes the decision epoch duration.
O: represents the objective, with Oi

t denoting the value ob-
tained by serving request i at decision epoch t. The goal of

the online assignment problem is to maximize the overall
objective over a given time horizon, T .

4 NeurADP: Neural Approximate Dynamic

Programming

Figure 1 represents the overall framework used for solv-
ing the RMP. As shown in the ﬁgure, the framework exe-
cutes 6 steps at each decision epoch to assign incoming user
requests to available vehicles. Existing myopic approaches
only execute steps (A), (B), (D) and (F). The crucial steps
(C) and (E) help in maximising the expected long-term value
of serving a request rather than its immediate value. To learn
this long-term value, we model the RMP problem using
ADP and use deep neural networks to learn the value func-
tions of post-decision states.

In this section, we ﬁrst indicate key challenges that pre-
clude direct application of existing ADP methods. We next
provide the ADP model for the RMP problem and describe
our contributions in using neural function approximations
for scalable and effective policies in RMP.

Departure From Past Work
Approximate Dynamic Programming has been used to
model many different
transportation problems such as
ﬂeet management (Simao et al. 2009), ambulance alloca-
tion (Maxwell et al. 2010) etc. While we also model our
RMP problem using ADP, we cannot use the solutions from
past work for the following reasons:

1. Non-trivial generation of feasible actions: In using ADP
to solve the ToD problem, the action for a single empty
vehicle is to match a single request. Computing the fea-
sible set of requests for a vehicle is a straightforward and
the best action for all vehicles together can then be com-
puted by solving a Linear Program (LP). In the case of the
RMP, multiple requests can be assigned to a single empty
or partially ﬁlled vehicle. Generating the set of feasible
actions, in this case, is complex and real-time solutions to
this problem have been the key challenge in literature on
myopic solutions to ride-pooling. In this paper, we use the
approach proposed by (Alonso-Mora et al. 2017) to gen-
erate feasible actions for a single vehicle (Section 4) and
then use an Integer Linear Program (ILP) to choose the
best action (Table 1) over all vehicles.

2. Inability to use LP-duals to update the value function:
Past work in ADP for ToD (Simao et al. 2009) uses the
dual values of the matching LP to update the parameters
of their value function approximation. However, choos-
ing the best action in the RMP requires solving an Inte-
ger Linear Program (ILP) that has bad LP-relaxations. As
a result, we cannot use duals to update our value func-
tion. Instead, we show the connection between ADP and
Reinforcement Learning (RL), and use the more general
Bellman update used in RL to update the value function
(Section 4).

3. Curse of Dimensionality: Past work in ADP for trans-
portation problems addresses the curse of dimensionality
by considering the value function to be dependent on a
small set of hand-crafted attributes (e.g., aggregated num-
ber of vehicles in each location) rather than on the states
of a large number of vehicles. Hand-crafting of state at-
tributes is domain-speciﬁc and is incredibly challenging
for a complex problem like RMP, where aggregation of
vehicles is not a feasible attribute (as each vehicle can
have different number of passengers going to multiple dif-
ferent locations). Instead, we use a Neural Network based
value function to automatically learn a compact low di-
mensional representation of the large state space.

4. Incorporating Neural Network value functions into the
optimisation problem: Past work in ADP for ToD uses
linear or piece-wise linear value function approximations
that allow for the value function to be easily integrated
into the matching LP. Non-linear value functions (such as
neural networks) cannot be integrated in this way, how-
ever, as they would make the overall optimisation pro-
gram non-linear. In Section 4), we address this issue by
using a two-step decomposition of the value function that
allows it to be efﬁciently integrated into the ILP as con-
stants.

5. Challenges of learning a Neural Network value func-
tion: In Deep Reinforcement Learning literature (Mnih et
al. 2015), it has been shown that naive approaches to ap-
proximating Neural Network value functions are unstable.
Additionally, training them requires millions of samples.
To address this, we propose a combination of method-
ological and practical solutions in Section 4.

The combination of using a Neural Network value function
(instead of linear approximations) and updating it with a
more general Bellman update (instead of LP-duals) repre-
sents a general alternative to past ADP approaches that we
term Neural ADP (NeurADP).

Approximate Dynamic Programming Model for
the RMP

We model the RMP by instantiating the tuple in Section 2.

S: The state of the system is represented as st = (rt, ut)
where rt is the state of all vehicles and ut is contains all
the requests waiting to be served. A vehicle r ∈ R at
t = (pi, t, Li)
decision epoch t is described by a vector ri
which represents its current trajectory. Speciﬁcally, it
captures the current location (pi), time(t) and an ordered
list of future locations (along with the cut-off time by
which each must be visited) that the vehicle has to visit
(Li) to satisfy the currently assigned requests. Each user
request j at decision epoch t is represented using vector
uj
t = (oj, ej) which captures its origin and destination.

A: For each vehicle, the action is to assign a group of users

from the set Ut to it. These actions should satisfy:

1. Constraints at the vehicle level - satisfying delay con-

straints D and vehicle capacity constraints

2. Constraints at the system level - Each request is as-

signed to at most one vehicle.

Handling exponential action space: To reduce the com-
plexity, feasible actions are generated in two steps. In the
ﬁrst step, we handle vehicle-level constraints by generat-
ing a set of feasible actions (groups of users) for each ve-
hicle. To do this efﬁciently, we ﬁrst generate an RTV (Re-
quest, Trip, Vehicle) graph using the algorithm by Alonso
et.al. (Alonso-Mora et al. 2017). Along with feasible ac-
tions, this generation process also provides the routes that
the vehicle should take to complete each action. We use
F i
t to denote the set of feasible actions generated for ve-
hicle i at decision epoch t.

t = {f i | f i ∈ ∪ci
F i

c(cid:48)=1 [U]c(cid:48)

, P ickU pDelay(f i, i) ≤ τ,

DetourDelay(f i, i) ≤ λ}

To ensure that system-level constraints are satisﬁed, we
solve an ILP that considers all vehicles and requests to-
gether. Let ai,f
t denote that vehicle i takes action f at de-
cision epoch t. Then, the decision variables ai,f
need to
t

AssignmentILP(t):

max

(cid:88)

(cid:88)

i

f ∈F i
t

t ∗ ai,f
oi,f

t + V i(T i,a(ri,a

t

, f )) ∗ ai,f

t

(6)

subject to Constraints (3) - (5)

Table 1: Optimization Formulation for assignment of vehi-
cles to feasible actions

satisfy following constraints:

(cid:88)

f ∈F i
t
(cid:88)

ai,f
t = 1 ::: ∀i ∈ R

(cid:88)

ai,f
t ≤ 1 ::: ∀j ∈ Ut

i∈R

f ∈F i

t ;j∈f

ai,f
t ∈ {0, 1} ::: ∀i, f

(3)

(4)

(5)

Constraint (3) ensures that each vehicle is assigned a sin-
gle action and constraint (4) ensures that each request is
a part of, at most, one action. Together, they ensure that a
request can be mapped to at most one vehicle.
We use At denote the set of all actions that satisfy both
individual and system-level constraints at time t and
at ∈ At to denote a feasible action in this set3.

ξ: As in previous work, exogenous information ξt represents
the user demand that arrives between time t − 1 and t.
T : The transition function T a deﬁnes how the vehicle state
changes after taking an action. In the case of the RMP, all
user requests that are not assigned are lost (Alonso-Mora
et al. 2017). Therefore, the user demand component of
post-decision state will be empty, i.e., ua

t = φ.

T a(st, at) = ra
t

(7)

t

t, ai

t) = ri,a

Here, ra
t denotes the post decision state of the vehicles.
We use T i,a(si
to denote the transition of indi-
vidual vehicles. At each decision epoch, based on the ac-
tions taken, (pi, t, Li) ∀i are updated and are captured in
ri,a
. Each vehicle has a ﬁxed path corresponding to each
t
action and as a result the transition above is deterministic.
O: When vehicle i takes a feasible action f at decision epoch
t, its contribution to the objective is oi,f
. For the objective
t
of maximizing the number of requests served, oi,f
is the
t
number of requests that are part of a feasible action f (0
for the null action aφ). The objective function at time t is
as follows:

ot(st, at) =

(cid:88)

(cid:88)

i∈R

f ∈F i
t

t ∗ ai,f
oi,f

t

3At every time step, we augment At with a null action aφ. This
allows a vehicle to continue along its trajectory without being as-
signed a passenger.

Algorithm 1: NeurADP (N, T )

1: Initialize: replay memory M , Neural value function V

(with random weights θ)

0 by randomly positioning vehicles.

2: for each episode 1 ≤ n < N do
Initialize the state sn
3:
Choose a sample path ξn
4:
for each step 0 ≤ t ≤ T do
5:
6:
7:

Compute the feasible action set Ft based on sn
t .
Solve the ILP in Table 1 to get best action an
Gaussian noise for exploration.
Store (rn
if t % updateFrequency == 0 then

t , Ft) as an experience in M .

t . Add the

Sample a random mini-batch of experiences
from M
for each experience e do

Solve the ILP in Table 1 with the information
from experience e to get the objective value ye
for each vehicle i do

Perform a gradient descent step on
(ye,i − V (ri,n
))2 with respect to
t
the network parameters θ
t ), sn
t = T a(sn

t , an

t+1 = T ξ(sa,n

t

Update: sa,n

, ξn

t+1)

8:
9:
10:

11:
12:

13:
14:

15:

Value Function Decomposition
Non-linear value functions, unlike their linear counterparts,
cannot be directly integrated into the matching ILP. One way
to incorporate them is to evaluate the value function for all
possible post-decision states and then add these values as
constants. However, the number of post-decision states is
exponential in the number of resources/vehicles.

To address this, we propose a two-step decomposition
of our overall value function that converts it into a linear
combination over individual value functions associated with
each vehicle4:
• Decomposing joint value function based on individual
vehicles’ value functions: We use the fact that we have re-
wards associated with each vehicle to decompose the joint
value function for all vehicles’ rewards into a sum over
the value function for each vehicle’s rewards. The proof
for this is straightforward and follows along the lines of
(Russell and Zimdars 2003).

V (ra

t ) =

V i(ra
t )

(cid:88)

i

• Approximation of individual vehicles’ value functions:
We make the assumption that the long-term reward of
a given vehicle is not signiﬁcantly affected by the spe-
ciﬁc actions another vehicle makes in the current decision
epoch. This makes sense because the long-term reward of
a given vehicle is affected by the interaction between its
trajectory and that of the other vehicles and, at a macro
level, these do not change signiﬁcantly in a single epoch.
This assumption allows us to use the pre-decision, rather
than post-decision, state of other vehicles.
(cid:68)
(cid:68)
ri,a
t ) = V i(
t

) ≈ V i(

, r-i,a
t

V i(ra

ri,a
t

, r-i
t

(cid:69)
)

(cid:69)

4As mentioned in equation (7), the post-decision state only de-

pends on the vehicle state. Therefore, V (sa

t ) = V (ra

t ).

Here, -i refers to all vehicles that are not vehicle i. This
step is crucial because the second term in the equation
above r-i
t can now be seen as a constant that does not de-
pend on the exponential post-decision state of all vehicles.

us and choosing the best action, in our case, involves solv-
ing an ILP. In addition to off-policy updates, we use standard
approaches in DRL like using a target network and Double
Q-Learning (Van Hasselt, Guez, and Silver 2016).

Therefore, the overall value function can be rewritten as:

V (ra

t ) =

(cid:88)

V i(

(cid:68)
ri,a
t

, r-i
t

(cid:69)
)

i

We evaluate these individual V i values for all possible
ri,a
and then integrate the overall value function into the
t
ILP in Table 1 as a linear function over these individual val-
ues. This reduces the number of evaluations of the non-linear
value function from exponential to linear in the number of
vehicles.

Value Function Estimation for NeurADP
To estimate the value function V over the post-decision
state, we use the Bellman equation (decomposed in the ADP
as equation (1) and (2)) to iteratively update the parameters
of the function approximation. In past work (Simao et al.
2009), the parameters of a linear (or piece-wise linear) value
function were updated in the direction of the gradient pro-
vided by the dual values at every step. Hence, the LP-duals
removed the need to explicitly calculate the gradients in the
case of a linear function approximation.

Given that we use a neural network function approxima-
tion and require an ILP (rather than an LP), we cannot use
this approach. Instead, we use standard symbolic differenti-
ation libraries (Abadi et al. 2015) to explicitly calculate the
gradients associated with individual parameters. We then up-
date these parameters by trying to minimise the L2 distance
between a one-step estimate of the return (from the Bell-
man equation) and the current estimate of the value func-
tion (Mnih et al. 2015), as shown in Algorithm 1.

Overcoming challenges in Neural Network Value
Function Estimation
In this section, we describe how we mitigate the stability
and scalability challenges associated with learning neural
network value functions through a combination of method-
ological and practical methods.

Improving stability of Bellman updates:
It has been
shown in Deep Reinforcement Learning (DRL) literature
that using standard on-policy methods to update Neural Net-
work (NN) based value function approximations can lead to
instability (Mnih et al. 2015). This is because the NN expects
the input samples to be independently distributed while con-
secutive states in RL and ADP are highly correlated. To ad-
dress these challenges, we propose using off-policy updates.
To do this, we save the current state and feasible action set
for each vehicle ∀i (si
t ) during sample collection. Then,
ofﬂine, we score the feasible actions using the value func-
tion and use the ILP create the best matching. Finally, we
update the value function of the saved post-decision state
with that of the generated next post-decision state. This is
different from experience replay in standard Q-Learning be-
cause the state and transition functions are partly known to

t, F i

Addressing the data scarcity: Neural Networks typically
require millions of data points to be effective, even on sim-
ple arcade games (Mnih et al. 2015). In our approach, we
address this challenge in 3 ways:
• Practically, we see that in the RMP, the biggest bottleneck
in speed is in generating feasible actions. To address this,
as noted above, we directly store the set of feasible actions
instead of recomputing them for each update.

• Secondly, we use the same Neural Network for the value
function associated with each of the individual vehicles.
This means that a single experience leads to multiple up-
dates, one for each vehicle.

• Finally, we use Prioritised Experience Replay (Schaul et
al. 2015) to reuse existing experiences more effectively.

Practical simpliﬁcations: Finally, based on our domain
knowledge, we introduce a set of practical simpliﬁcations
that makes learning tractable:
• Instead of using one-hot representations for discrete lo-
cations, we create a low-dimensional embedding for each
location by solving the proxy-problem of trying to esti-
mate the travel times between these locations.

• During training, we perform exploration by adding Gaus-
sian noise to the predicted Vi values (Plappert et al. 2017).
This allows us to more delicately control the amount of
randomness introduced into the training process than the
standard (cid:15)-greedy strategy.

• We don’t use the pre-decision state of all the other vehi-
cles to calculate the value function for a given vehicle (as
suggested in Section 4). Instead, we aggregate this infor-
mation into the count of the number of nearby vehicles
and provide this to the network, instead.

The speciﬁcs of the neural network architecture and training
can be found in the supplementary (Section 8).

5 Experiments
The goal of the experiments is to compare the performance
of our NeurADP approach to leading approaches for solv-
ing the RMP on a real-world dataset(NYYellowTaxi 2016)
across different RMP parameter values. The metric we use
to compare them is the service rate, i.e., the percentage of
total requests served. Similar to (Alonso-Mora et al. 2017;
Lowalekar, Varakantham, and Jaillet 2019), we vary the fol-
lowing parameters: the maximum allowed waiting time τ
from 120 seconds to 420 seconds, the number of vehicles
|R| from 1000 to 3000 and the capacity ci from 2 to 10. The
value of maximum allowable detour delay λ is taken as 2∗τ .
The decision epoch duration ∆ is taken as 60 seconds.

We compare NeurADP against

the following algo-

rithms:
• ZAC – ZAC algorithm by (Lowalekar, Varakantham, and

Jaillet 2019).

Figure 2: The three graphs benchmark our performance across 3 sets of parameter values - ci, τ and |R| respectively (from left
to right). In each case, we start with the prototypical conﬁguration of τ = 300 seconds, ci = 4 and |R| = 1000 and vary the
chosen parameter.

low Taxi Dataset (NYYellowTaxi 2016). The experimental
setup is similar to the setup used by (Alonso-Mora et al.
2017; Lowalekar, Varakantham, and Jaillet 2019). Street in-
tersections are used as the set of locations L. They are iden-
tiﬁed by taking the street network of the city from open-
streetmap using osmnx with ’drive’ network type (Boe-
ing 2017). Nodes that do not have outgoing edges are re-
moved, i.e., we take the largest strongly connected compo-
nent of the network. The resulting network has 4373 loca-
tions (street intersections) and 9540 edges. Similar to ear-
lier work (Alonso-Mora et al. 2017), we only consider the
street network of Manhattan as a majority (∼75%) of re-
quests have both pickup and drop-off locations within it.

The real-world dataset contains data about past customer
requests for taxis at different times of the day and different
days of the week. From this dataset, we take the following
ﬁelds: (1) Pickup and drop-off locations (latitude and longi-
tude coordinates) - These locations are mapped to the nearest
street intersection. (2) Pickup time - This time is converted
to appropriate decision epoch based on the value of ∆. The
travel time on each road segment of the street network is
taken as the daily mean travel time estimate computed using
the method proposed in (Santi et al. 2014). The dataset con-
tains on an average 322714 requests in a day (on weekdays)
and 19820 requests during peak hour.

We evaluate the approaches over 24 hours on different
days starting at midnight and take the average value over
5 weekdays (4 - 8 April 2016) by running them with a single
instance of initial random location of taxis 6. NeurADP is
trained using the data for 8 weekdays (23 March - 1 April
2016) and it is validated on 22 March 2016. For the experi-
mental analysis, we consider that all vehicles have identical
capacities.
Results: We now compare the results of our approach, Neu-
rADP, against past approaches. Figure 2 shows the com-
parison of service rate between NeurADP and existing ap-

6All experiments are run on 24 core - 2.4GHz Intel Xeon
E5-2650 processor and 256GB RAM. The algorithms are
implemented in python and optimisation models are solved
using CPLEX 12.8. The setup and code are available at
https://github.com/sanketkshah/NeurADP-for-Ride-Pooling.

Figure 3: The graph compares the number of requests served
as a function of time. The bold lines represent a moving av-
erage of the actual values (represented by the lighter lines).
This graph corresponds to the conﬁguration λ = 300sec, ci
= 10 and |R| = 1000 on 4 April 2016

• TBF-Complete – Implementation of (Alonso-Mora et al.
2017) taken from (Lowalekar, Varakantham, and Jaillet
2019).

• TBF-Heuristic (Baseline) – This is our implementation
of Alonso et.al.’s (Alonso-Mora et al. 2017) approach 5.

To disentangle the source of improvement in our ap-
proach, we introduce TBF-Heuristic which we refer to as the
baseline. This uses a fast insertion method, that mirrors the
implementation in NeurADP, to generate feasible actions.
This is important because training requires a lot of samples
and the key bottleneck in generating samples is the gener-
ation of feasible actions. While this process is completely
parallelisable in theory, our limited academic computing re-
sources do not allow us to leverage this. Therefore, com-
paring against this baseline allows us to measure the impact
using future information has on solution quality.
Setup: The experiments are conducted by taking the de-
mand distribution from the publicly available New York Yel-

5Please refer to the supplementary (Section 8) for a complete

list of differences in the implementation.

proaches. As shown in the ﬁgure, NeurADP consistently
beats all existing approaches across different parameters.
Here are the key observations:
• Effect of changing the tolerance to delay, τ : Neu-
rADP obtains a 16.07% improvement over the baseline
approach for τ = 120 seconds. The difference between
the baseline and NeurADP decreases as τ increases. The
lower value of τ makes it difﬁcult for vehicles to accept
new requests while satisfying the constraints for already
accepted requests. Therefore, it is more important to con-
sider future requests while making current assignments
when τ is lower, leading to a larger improvement.

• Effect of changing the capacity, ci: NeurADP obtains a
14.03% gain over baseline for capacity 10. The difference
between the baseline and NeurADP increases as the ca-
pacity increases. This is because, for higher capacity ve-
hicles, there is a larger scope for improvement if the future
impact of making an assignment is taken into account.
• Effect of changing the number of vehicles, |R|: The dif-
ference between the baseline and NeurADP decreases as
the number of vehicles increase. This is because, in the
presence of a large number of vehicles, there will always
be a vehicle that can serve the request. As a result, the
quality of assignments plays a smaller role.

For the speciﬁc case of τ = 120, NeurADP does not out-
perform ZAC and TBF-Complete because they use a more
complex feasible action generation which allows them to
leverage complex combinations of requests and their order-
ing. This becomes important as the delay constraints be-
come stricter. If NeurADP is implemented with the complete
search for feasible action generation, we expect it to outper-
form ZAC and TBF in this case as well.

We further analyse the improvements obtained by Neu-
rADP over baseline by comparing the number of requests
served by both approaches at each decision epoch. Figure 3
shows the total number of requests available and the num-
ber of requests served by the baseline and our approach
NeurADP at different decision epochs. As shown in the ﬁg-
ure, initially at night time when the demand is low both ap-
proaches serve all available demand. During the transition
period from low demand to high demand period, the base-
line algorithm starts to greedily serve the available requests
without considering future requests. On the other hand, Neu-
rADP ignores some requests during this time to serve more
requests in future. This allows NeurADP to serve more re-
quests during peak time.

The approach can be executed in real-time settings. The
average time taken to compute each batch assignment using
NeurADP is less than 60 seconds (for all cases) 7.

These results indicate that using our approach can help

ride-pooling platforms to better meet customer demand.

6 Conclusion
On-demand ride-pooling has become quite popular in trans-
portation (through services like UberPool, LyftLine, etc.),

food delivery (through services like FoodPanda, Deliveroo,
etc.) and in logistics. This is a challenging problem as we
have to assign each (empty or partially ﬁlled) vehicle to a
group of requests. Due to the difﬁculty of making such as-
signments online, most existing work has focussed on my-
opic assignments that do not consider the future impact of
assignments. Through a novel combination of approaches
from ADP, ride-sharing and Deep Reinforcement Learn-
ing, we provide an ofﬂine-online approach that trains of-
ﬂine on past data and provides online assignments in real-
time. Our approach, NeurADP improves the state of art by
up to 16% on a real dataset. To put this result in perspec-
tive, typically, an improvement of 1% is considered a signif-
icant improvement on ToD for an entire city (Xu et al. 2018;
Lowalekar, Varakantham, and Jaillet 2019).

7 Acknowledgements
This research was supported by the Singapore Ministry of
Education Academic Research Fund (AcRF) Tier 2 grant
under research grant MOE2016-T2-1-174. This work was
also partially supported by the Singapore National Research
Foundation through the Singapore-MIT Alliance for Re-
search and Technology (SMART) Centre for Future Urban
Mobility (FM).

8 Supplementary

Neural Network and Training Speciﬁcs
As inputs to this NN, we take the current location of the ve-
hicle along with information about the remaining delay8 and
locations for the current requests that have been accepted.
We order these according to the trajectory associated with
them and feed them as inputs to an LSTM after an embed-
ding layer. The embeddings for the locations are calculated
separately and are the byproduct of a two-layer neural net-
work that attempts to estimate the travel times between two
locations.

Additionally, we add information about the current deci-
sion epoch, the number of vehicles in the vicinity of vehi-
cle i and the total number of requests that arrived in the
epoch. Due to the constraint that a single request can only
be assigned to a single vehicle, multiple agents compete for
the same request. As a consequence, the value of being in
a given state is dependent on the competition it faces from
other agents when it is in that state. Adding the informa-
tion about other taxis and the number of current requests sta-
bilises learning signiﬁcantly. These inputs are concatenated
with the output of the LSTM from the previous paragraph
and after 2 dense layers, used to predict the V-value. The loss
considered is the mean squared error and it is minimised us-
ing the Adam optimiser using default initial parameters. We
need to explore despite having determinstic transition and
reward functions because the action space, in our model, is
stochastic.

This value function over individual vehicles is learned
ofﬂine. When the approach is running online, we compute

760 seconds is the decision epoch duration considered in the

8The deadline to reach the location, according to quality con-

experiments

straints C, minus the expected arrival time

Alonso approach

Baseline

Generation of Feasible Trips

1. Generate RV graph by checking feasibility of each
request with each vehicle. Keep only 30 closest vehicles for
each request.

1. Same

2. Perform exhaustive search for up to 4 requests (in the
taxi currently and in the proposed trip). For more requests,
check if the request can be inserted into the current vehicle
path.

3. The exploration of feasible trips for a vehicle is stopped
when a time limit of 0.2 seconds is reached.

2. Insert request into current vehicle path, irrespective of
number of requests, for faster computation.

3. Exploration is stopped when feasibility constraints are
evaluated 150 times. This is done to make the performance
independent of the processing speed.

Number of vehicles rebalanced is min(unassigned vehicles,
unassigned requests)

All unassigned vehicles are rebalanced.

Rebalancing Strategy

Table 2: Differences between Baseline and Alonso Approach

the assignment (of customer requests to vehicles) that max-
imizes the value function computed in the ofﬂine phase

Details of Baseline algorithm
There are some differences in our implementation of the ap-
proach by (Alonso-Mora et al. 2017). We refer to our im-
plementation as Baseline and the approach by Alonso et.al.
as Alonso approach. The differences are highlighted in ta-
ble 2. The difference in generation of feasible trips is due to
following practical considerations

1. Training time: To effectively train RL algorithms, we need
a large number of experiences. To generate samples to
train from, we must run our approach for some training
days. Using (Alonso-Mora et al. 2017)’s strategy for gen-
erating feasible trips takes signiﬁcantly longer than the
modiﬁcation we propose. Given that our training time al-
ready takes multiple days, this is not viable. During test
time, we maintain the same strategy to ensure coherence
with what our value function is trained on.

2. Limitation due to academic computational resources: Our
problem is completely parallelisable across different ve-
hicles and so, in commercial set-ups, the consideration
above would not stay relevant. In our case, however, we
are bound by academic infrastructure.

This is not a limitation for our approach. We expect the
results of both the baseline and our approach improve pro-
portionally if the feasible trips are generated as proposed in
the paper by (Alonso-Mora et al. 2017).

Rebalancing Rebalancing empty vehicles has a signiﬁ-
cant impact on the number of requests served(Wallar et al.
2018). Similar to (Alonso-Mora et al. 2017), we perform a
re-balancing of unassigned vehicles to high demand areas
after each batch assignment. But unlike them we do not per-
form rebalancing by using only current unserved requests.
This is because by using only current unserved requests for

rebalancing, number of vehicles rebalanced will be mini-
mum of unassigned vehicles and requests leaving majority
of vehicles not being rebalanced in case of low demand sce-
narios. This means that vehicles that could be stuck in ar-
eas where requests are infrequent. Our approach differs from
(Wallar et al. 2018) as we do not use the concept of ’regions’
which are disjoint sets of locations. We work with individual
locations, instead.

Therefore, we sample min(500,|V|) requests from the
number of requests seen so far and rebalance all vehicles
to move to the areas of these sampled request by performing
the optimization provided in table 4.

Let V t

u denotes the set of unassigned vehicles at decision
epoch t and Dt denotes the set of sampled customer requests
(as described above). mt
ij is a binary variable indicating that
vehicle i is moving towards customer request j. The objec-
tive of the linear optimization program is to minimize the
sum of travel times. We use T (pi, oj) to denote the time
taken to travel from initial location pi of vehicle i to the
origin oj of request j. Constraint 9 ensures that each vehi-
cle is assigned to exactly one request. Constraint 10 ensures
that each customer request is assigned to exactly nt
j vehicles
500 (cid:99) or (cid:100) |V t
j = (cid:98) |V t
500 (cid:101) such that (cid:80)
u|
u|
j = |V t
j∈Dt nt
where nt
u|.

Detailed Results
Table 3 presents the average number of requests served by
baseline and our algorithm over different days across differ-
ent parameters.

ADP for ToD Problems
In order to provide a clear distinction between our contribu-
tions and the ADP methods employed in ﬂeet optimization,
we describe ADP for ToD problems (Simao et al. 2009).
Each vehicle is represented using an attribute vector which
captures the location and other information related to vehi-
cle. The value function approximation used is linear

Capacity

Pickup Delay

Number of Vehicles

Number of
Vehicle
1000
1000
1000
1000
1000
1000
1000
1000
2000
3000

Parameters
Pickup
Delay
300
300
300
300
120
300
420
300
300
300

Capacity

1
2
4
10
4
4
4
4
4
4

Baseline

Requests
Served
98581.4 ± 1588.674
139497 ± 3438.244
192430 ± 2687.394
220737.4 ± 5334.318
109436 ± 3309.862
192430 ± 2687.394
207396.6 ± 5154.906
192430 ± 2687.394
280715.8 ± 16320.16
300122.6 ± 16428.76

Our Approach

Requests
Served
121302.6 ± 3008.226
177625.8 ± 4791.103
237534.2 ± 7682.915
266310 ± 13789.16
161257.8 ± 2704.993
237534.2 ± 7682.915
243899.8 ± 9279.68
237534.2 ± 7682.915
307306.6 ± 15183.41
318786.4 ± 19384.14

Percentage
Improvement
23.04816 ± 2.719923
27.33306 ± 2.312733
23.43928 ± 2.80972
20.64562 ± 3.910312
47.35352 ± 4.301165
23.43928 ± 2.80972
17.60067 ± 1.821488
23.43928 ± 2.80972
9.472499 ± 1.198605
6.218725 ± 1.300478

Table 3: Detailed Results

RebalanceVehicles(t):

(cid:88)

(cid:88)

min

T (pi, oj) ∗ mt
ij

subject to

j∈Dt
(cid:88)

i∈V t
u
(cid:88)

i∈V t
u
mt

ij ≤ nt

j ::: ∀j ∈ Dt

mt

ij = 1 ::: ∀i ∈ V t
u

j∈Dt
0 ≤ mt

ij ≤ 1 ::: ∀i, j

(8)

(9)

(10)

(11)

Table 4: Optimization Formulation for Rebalancing unas-
signed vehicles

9:
10:

Algorithm 2: ADP(N, T )

1: Initialization

2: Initialize V 0 = 0,
3: Set n = 1

4: for n < N do
5:
6:
7:
8:

Initialize the state sn
Choose a sample realization ξn of exogenous information
for 0 ≤ t ≤ T do

0 randomly.

Solve the optimisation problem
vn = maxat∈At O(st, at) + γV n−1(T a(st, at)) to get
actions an
t
Update the value function V n using vn
Update states sa,n

t = T a(st, at), sn

t+1 = T ξ(sa

t , ξn

t+1)

V (sat

t ) =

(cid:88)

vt,b · Rat
t,b

(12)

b∈B
where B is the set of all possible attribute vectors and Ra
t,b
denotes the number of vehicles having attribute vector b in
post decision state. vb is the value of having an additional
vehicle with attribute vector b at time t.

The value function approximation is improved by improv-
ing the estimate of vb values iteratively. Estimate of the pa-
rameter value at iteration n is given by vn
b :

b = (1 − αn−1) · vn−1
vn

b + αn−1 ∗ ˆvb

(13)

where, vn−1

represents the values at iteration n − 1, ˆvb
represents the dual values corresponding to the ﬂow preser-
vation constraint in the LP and αn−1 is the step size.

b

The pseudocode for ADP algorithm is shown in algorithm

2.

References
Abadi, M.; Agarwal, A.; Barham, P.; Brevdo, E.; Chen, Z.;
Citro, C.; Corrado, G. S.; Davis, A.; Dean, J.; Devin, M.;
Ghemawat, S.; Goodfellow, I.; Harp, A.; Irving, G.; Isard,
M.; Jia, Y.; Jozefowicz, R.; Kaiser, L.; Kudlur, M.; Leven-
berg, J.; Man´e, D.; Monga, R.; Moore, S.; Murray, D.; Olah,
C.; Schuster, M.; Shlens, J.; Steiner, B.; Sutskever, I.; Tal-
war, K.; Tucker, P.; Vanhoucke, V.; Vasudevan, V.; Vi´egas,

F.; Vinyals, O.; Warden, P.; Wattenberg, M.; Wicke, M.; Yu,
Y.; and Zheng, X. 2015. TensorFlow: Large-scale machine
learning on heterogeneous systems. Software available from
tensorﬂow.org.
Alonso-Mora, J.; Samaranayake, S.; Wallar, A.; Frazzoli, E.;
and Rus, D. 2017. On-demand high-capacity ride-sharing
via dynamic trip-vehicle assignment. Proceedings of the Na-
tional Academy of Sciences 201611675.
Bei, X., and Zhang, S. 2018. Algorithms for trip-vehicle
assignment in ride-sharing.
Boeing, G.
2017. Osmnx: New methods for acquir-
ing, constructing, analyzing, and visualizing complex street
networks. Computers, Environment and Urban Systems
65:126–139.
Gessner, K. 2019. Uber vs. lyft: Who’s tops in the bat-
tle of u.s. rideshare companies. https://www.uber.com/en-
GB/newsroom/company-info/.
Heath, A. 2016. Inside uber’s quest to get more people in
fewer cars. https://www.businessinsider.com/uberpool-ride-
sharing-could-be-the-future-of-uber-2016-6/.
Huang, Y.; Bastani, F.; Jin, R.; and Wang, X. S. 2014.
Large scale real-time ridesharing with service guarantee
on road networks. Proceedings of the VLDB Endowment
7(14):2017–2028.
Li, M.; Jiao, Y.; Yang, Y.; Gong, Z.; Wang, J.; Wang, C.; Wu,
G.; Ye, J.; et al. 2019. Efﬁcient ridesharing order dispatching

Simao, H. P.; Day, J.; George, A. P.; Gifford, T.; Nienow, J.;
and Powell, W. B. 2009. An approximate dynamic program-
ming algorithm for large-scale ﬂeet management: A case ap-
plication. Transportation Science 43(2):178–197.
Tong, Y.; Zeng, Y.; Zhou, Z.; Chen, L.; Ye, J.; and Xu, K.
2018. A uniﬁed approach to route planning for shared mo-
bility. Proceedings of the VLDB Endowment 11(11):1633–
1646.
Van Hasselt, H.; Guez, A.; and Silver, D. 2016. Deep re-
In Thirtieth
inforcement learning with double q-learning.
AAAI conference on artiﬁcial intelligence.
Verma, T.; Varakantham, P.; Kraus, S.; and Lau, H. C. 2017.
Augmenting decisions of taxi drivers through reinforcement
learning for improving revenues. In Twenty-Seventh Interna-
tional Conference on Automated Planning and Scheduling.
Wallar, A.; Van Der Zee, M.; Alonso-Mora, J.; and Rus, D.
2018. Vehicle rebalancing for mobility-on-demand systems
with ride-sharing. In 2018 IEEE/RSJ International Confer-
ence on Intelligent Robots and Systems (IROS), 4539–4546.
IEEE.
Wang, Z.; Qin, Z.; Tang, X.; Ye, J.; and Zhu, H. 2018. Deep
reinforcement learning with knowledge transfer for online
rides order dispatching. In 2018 IEEE International Confer-
ence on Data Mining (ICDM), 617–626. IEEE.
Xu, Z.; Li, Z.; Guan, Q.; Zhang, D.; Li, Q.; Nan, J.; Liu,
C.; Bian, W.; and Ye, J. 2018. Large-scale order dispatch
in on-demand ride-hailing platforms: A learning and plan-
ning approach. In Proceedings of the 24th ACM SIGKDD
International Conference on Knowledge Discovery & Data
Mining, 905–913. ACM.
Yu, X., and Shen, S.
2019. An integrated decomposi-
tion and approximate dynamic programming approach for
on-demand ride pooling. IEEE Transactions on Intelligent
Transportation Systems.

with mean ﬁeld multi-agent reinforcement learning. arXiv
preprint arXiv:1901.11454.
Lin, K.; Zhao, R.; Xu, Z.; and Zhou, J. 2018. Efﬁcient large-
scale ﬂeet management via multi-agent deep reinforcement
learning. In Proceedings of the 24th ACM SIGKDD Inter-
national Conference on Knowledge Discovery &#38; Data
Mining, 1774–1783. ACM.
Lowalekar, M.; Varakantham, P.; and Jaillet, P. 2019. ZAC:
A zone path construction approach for effective real-time
In Proceedings of the Twenty-Ninth Interna-
ridesharing.
tional Conference on Automated Planning and Scheduling,
ICAPS 2018, Berkeley, CA, USA, July 11-15, 2019., 528–
538.
Ma, S.; Zheng, Y.; and Wolfson, O. 2013. T-share: A large-
scale dynamic taxi ridesharing service. In Data Engineering
(ICDE), 2013 IEEE 29th International Conference on, 410–
421. IEEE.
Maxwell, M. S.; Restrepo, M.; Henderson, S. G.; and
Topaloglu, H. 2010. Approximate dynamic programming
for ambulance redeployment. INFORMS Journal on Com-
puting 22(2):266–281.
Mnih, V.; Kavukcuoglu, K.; Silver, D.; Rusu, A. A.; Ve-
ness, J.; Bellemare, M. G.; Graves, A.; Riedmiller, M.;
2015. Human-
Fidjeland, A. K.; Ostrovski, G.; et al.
level control through deep reinforcement learning. Nature
518(7540):529.
NYYellowTaxi. 2016. New york yellow taxi dataset. http:
//www.nyc.gov/html/tlc/html/about/trip record data.shtml.
Parragh, S. N.; Doerner, K. F.; and Hartl, R. F. 2008. A
survey on pickup and delivery problems. Journal f¨ur Be-
triebswirtschaft 58(1):21–51.
Plappert, M.; Houthooft, R.; Dhariwal, P.; Sidor, S.; Chen,
R. Y.; Chen, X.; Asfour, T.; Abbeel, P.; and Andrychowicz,
M. 2017. Parameter space noise for exploration. arXiv
preprint arXiv:1706.01905.
Powell, W. B. 2007. Approximate Dynamic Programming:
Solving the curses of dimensionality, volume 703. John Wi-
ley & Sons.
Ritzinger, U.; Puchinger, J.; and Hartl, R. F. 2016. A survey
on dynamic and stochastic vehicle routing problems. Inter-
national Journal of Production Research 54(1):215–231.
Ropke, S., and Cordeau, J.-F. 2009. Branch and cut and
price for the pickup and delivery problem with time win-
dows. Transportation Science 43(3):267–286.
Russell, S. J., and Zimdars, A. 2003. Q-decomposition for
reinforcement learning agents. In Proceedings of the 20th
International Conference on Machine Learning (ICML-03),
656–663.
Santi, P.; Resta, G.; Szell, M.; Sobolevsky, S.; Strogatz,
S. H.; and Ratti, C. 2014. Quantifying the beneﬁts of ve-
hicle pooling with shareability networks. Proceedings of the
National Academy of Sciences 111(37):13290–13294.
Schaul, T.; Quan, J.; Antonoglou,
2015.
arXiv:1511.05952.

I.; and Silver, D.
arXiv preprint

Prioritized experience replay.

