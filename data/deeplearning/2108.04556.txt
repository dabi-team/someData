1
2
0
2

p
e
S
9

]
L
C
.
s
c
[

3
v
6
5
5
4
0
.
8
0
1
2
:
v
i
X
r
a

SYNCOBERT: Syntax-Guided Multi-Modal Contrastive Pre-Training for
Code Representation

Xin Wang1, Yasheng Wang2, Fei Mi2, Pingyi Zhou2
Yao Wan3, Xiao Liu4, Li Li5, Hao Wu6, Jin Liu1, Xin Jiang2
1Wuhan University, 2Noah’s Ark Lab, Huawei
3Huazhong University of Science and Technology
4Deakin University, 5Monash University, 6Yunnan University
{xinwang0920, jinliu}@whu.edu.cn, {wangyasheng, mifei2, zhoupingyi, Jiang.Xin}@huawei.com,
wanyao@hust.edu.cn, xiao.liu@deakin.edu.au, Li.Li@monash.edu, haowu@ynu.edu.cn

Abstract

Code representation learning, which aims to encode the se-
mantics of source code into distributed vectors, plays an im-
portant role in recent deep-learning-based models for code
intelligence. Recently, many pre-trained language models for
source code (e.g., CuBERT and CodeBERT) have been pro-
posed to model the context of code and serve as a basis for
downstream code intelligence tasks such as code search, code
clone detection, and program translation. Current approaches
typically consider the source code as a plain sequence of to-
kens, or inject the structure information (e.g., AST and data-
ﬂow) into the sequential model pre-training. To further ex-
plore the properties of programming languages, this paper
proposes SYNCOBERT, a Syntax-guided multi-modal con-
trastive pre-training approach for better Code representations.
Specially, we design two novel pre-training objectives origi-
nating from the symbolic and syntactic properties of source
code, i.e., Identiﬁer Prediction (IP) and AST Edge Prediction
(TEP), which are designed to predict identiﬁers, and edges
between two nodes of AST, respectively. Meanwhile, to ex-
ploit the complementary information in semantically equiv-
alent modalities (i.e., code, comment, AST) of the code, we
propose a multi-modal contrastive learning strategy to maxi-
mize the mutual information among different modalities. Ex-
tensive experiments on four downstream tasks related to code
intelligence show that SYNCOBERT advances the state-of-
the-art with the same pre-training corpus and model size.

1

Introduction

Code intelligence that utilizes machine learning techniques
to promote the productivity of software developers, has at-
tracted increasing interest in both communities of software
engineering and artiﬁcial intelligence (Lu et al. 2021). To
achieve code intelligence, one fundamental task is code
representation learning (also known as code embedding),
which can support many downstream tasks, including code
search (Gu, Zhang, and Kim 2018), code clone detec-
tion (White et al. 2016), code defect detection (Li et al.
2017), and program translation (Chen, Liu, and Song 2018).
Beneﬁting from the strong power of pre-training tech-
niques in natural language processing (NLP), such as BERT

Copyright © 2022, Association for the Advancement of Artiﬁcial
Intelligence (www.aaai.org). All rights reserved.

Figure 1: A Python code snippet with its AST.

(Devlin et al. 2018), RoBERTa (Liu et al. 2019), AL-
BERT (Lan et al. 2020) and GPT (Radford et al. 2018), there
are several attempts to pre-train a language model on the
corpus of source code for code understanding and genera-
tion. CodeBERT (Feng et al. 2020) is a bimodal pre-trained
model on the combination of source code and natural-
language descriptions. PLBART (Ahmad et al. 2021) is a
uniﬁed pre-training framework for code understanding and
generation. To incorporate the syntax structure of code, Guo
et al. (2021) further propose GraphCodeBERT to preserve
the syntax structure of source code by introducing an edge
masking technique over data-ﬂow graphs. Jiang et al. (2021)
propose TreeBERT, which incorporates the abstract syntax
tree (AST) of the code into model pre-training by designing
two objectives of tree-based masked language modeling and
node order prediction.

Despite much progress having been made towards pre-
training language models for source code, several character-
istics of programming languages are still not sufﬁciently ex-
plored. To better represent the syntax structure of code, we
consider two crucial but overlooked characteristics of source
code. (1). Code identiﬁer contains symbolic and syntactic
information. Identiﬁer or variable is a basic component for
programming language. It should not be simply considered
as regular textual code tokens. For example, given an expres-
sion x = len("x"), the previous x is a identiﬁer, which

function_definitionCode snippetAbstract Syntax Tree1 def add_func(x,y):2     result = x + y3     return resultmoduleparametersadd_funcdef:blockexpression_statementreturn_statementassignment(x,y)result=binary_operatorreturnresultx+yComment# Return the sum of two numbersleaf nodesnon-leaf nodesParser 
 
 
 
 
 
differentiates the later string x. identiﬁers play an important
role in understanding the logic of code, because they contain
crucial symbolic and syntactic information. (2). The syntax
information along AST edges is ignored. To obtain the syn-
tax structure of code, AST has been widely adopted (Zugner
et al. 2021; Jiang et al. 2021). Figure 1 shows a Python
code snippet with its AST. In this AST, a binary operator
statement x + y can be represented by a non-leaf node
binary operator points to three leaf nodes (x, y, and
an operational character +). We argue that the edges con-
necting non-leaf nodes and leaf nodes contain rich syntactic
structure information that should be considered.

In addition, a program is often composed of both code
snippets and corresponding comments. Furthermore, a code
snippet can be parsed into one or more syntactic structures
(e.g., AST or control-/data- ﬂow graph). In this paper, we
call these code features from different perspectives as mul-
tiple modalities of code. We argue that these semantically
equivalent modalities provide complementary information
to learn more comprehensive code representations. How-
ever, previous works do not further explore the potential mu-
tual information among different modalities of the code.

Motivated by the aforementioned limitations, this paper
proposes SYNCOBERT, a Syntax-guided multi-modal con-
trastive pre-training approach for better Code representa-
tions. We design two novel pre-training objectives originat-
ing from the symbolic and syntactic properties of source
code, i.e., Identiﬁer Prediction (IP) and AST Edge Predic-
tion (TEP), which are designed to predict identiﬁers, and
edges between two nodes of AST, respectively. We propose a
multi-modal contrastive learning (MCL) objective to obtain
more comprehensive representations by learning from three
modalities (code, comment, and AST) through contrastive
learning. Overall, the key contributions of this paper are as
follows:
• We propose SYNCOBERT, a syntax-guided multi-modal
contrastive pre-training framework for code representa-
tions. We design two new pre-training objectives to en-
code the symbolic and syntactic information of program-
ming languages. The ﬁrst IP objective predicts whether the
code token is an identiﬁer or not. The second TEP objec-
tive predicts the edges between two nodes of AST.

• We propose a multi-modal contrastive pre-training strat-
egy that maximizes mutual information among different
modalities (code, comment, and AST) through contrastive
learning to learn more comprehensive representations.
• Comprehensive experiments conducted on four code in-
telligence tasks (code search, code clone detection, code
defect detection, program translation) demonstrate that
SYNCOBERT advances the state-of-the-art with the same
pre-training corpus and model size.

2 SYNCOBERT
This section ﬁrst gives preliminaries about input representa-
tions and model architecture of SYNCOBERT. Then, we in-
troduce our novel pre-training framework covering four ob-
jectives, including Multi-Modal Masked Language Model-
ing (MMLM), Identiﬁer Prediction (IP), AST Edge Predic-

Figure 2: A part of the AST sequence obtained from the
AST in Figure 1, blue arrows denote edges between nodes.

tion (TEP), and Multi-Modal Contrastive Learning (MCL).

2.1 Preliminary
Input Representations We take the AST of the code as
part of the input to the model, which provides an AST token
sequence with a depth-ﬁrst traversal. We adopt tree-sitter1
to convert the code into an AST. Figure 2 shows an example
of partial AST sequence obtained from the AST in Figure
1, and the blue arrows denote edges between nodes. Given
a natural language comment w = {w1, w2, . . . , w|w|}, the
corresponding source code c = {c1, c2, . . . , c|c|}, and the
AST sequence a = {a1, a2, . . . , a|a|}, SYNCOBERT takes
the concatenation of multiple modalities (NL, PL, AST) as
input, i.e.,

x = {[CLS], w, [SEP], c, [SEP], a, [SEP]},

(1)

where [CLS] (Devlin et al. 2018) is a special token for
“classiﬁcation tasks”, appearing at the beginning of the input
sequence, and [SEP] is a special token to split two kinds of
sub-sequences. For unimodal code data (without NL com-
ments), the input is x = {[CLS], c, [SEP], a, [SEP]}.

Model Architecture SYNCOBERT is built on a multi-
layer Transformer (Vaswani et al. 2017) encoder as in BERT,
which we will not review in detail. The embedding of each
token in x is the sum of corresponding token and position
embeddings. SYNCOBERT adopts 12-layer Transformer
with 768 hidden sizes and 12 attention heads to encode the
input vectors into contextual representations.

2.2 Multi-Modal Masked Language Modeling

(MMLM)

Our approach jointly models NL, PL, and AST, providing
complementary information contained in multiple modali-
ties. We extend the NLP task know as Masked Language
Model (MLM) (Devlin et al. 2018) to multiple modalities.
Given a data point of NL-PL-AST triplet {w, c, a} as input,
we randomly select 15% of tokens from the concatenation
of NL, PL, and AST. Following the same settings in (De-
vlin et al. 2018), we replace 80% of them with [MASK]
tokens, 10% with random tokens, and the remaining 10%
unchanged. The MMLM objective is a cross-entropy loss on
predicting the masked tokens as:

LMMLM = −

V
(cid:88)

M
(cid:88)

yMMLM
i

lnpMMLM
i

,

(2)

i
where M = wm∪cm∪am is the set of masked tokens for NL
(wm), PL (cm) and AST (am). V represents the vocabulary

1https://github.com/tree-sitter/tree-sitter

function_definitiondefadd_funcparameters(x,y)Figure 3: Different scenes of SYNCOBERT pre-training. (a) SYNCOBERT takes source code paired with comment and the
corresponding AST as the input, and is pre-trained with MMLM, IP, TEP objectives. (b) Positive sampling for NL-PL paired
data, (left) NL vs PL-AST, (right) NL-PL-AST vs NL-AST-PL. (c) An illustration about positive and negative pairs, including
in-batch and cross-batch negative sampling.

size, yMMLM
i
pMMLM
i

denotes the label of the masked token i, and

is the predicted probability of token i.

Identiﬁer Prediction (IP)

2.3
Previous works often overlook the symbolic property of pro-
gramming languages. As a typical symbol, the identiﬁer
plays an important role in source codes. It can be replaced
by another string without affecting the logic of the source
code. As it is prohibitive the exhaustively predict a large
number of code token types in source code, we only divide
the code token types into identiﬁer or non-identiﬁer, con-
sidering the importance and large proportion of identiﬁers.
Different from MMLM (predicting 15% code tokens), we
pose the identiﬁer prediction objective over all code tokens.
For each token in the source code, a label 1 is applied if it
is an identiﬁer, and a label 0 is applied otherwise (c.f. Fig-
ure 3(a)). Therefore, the IP loss function is a binary classiﬁ-
cation loss deﬁned as:

LIP = −

(cid:88)

[yIP
i

i∈c

lnpIP

i +(1 − yIP

i )ln(1 − pIP

i )] ,

(3)

where pIP
i
code token, and yIP

i

is the predicted identiﬁer probability of the i-th

is the label of the i-th code token.

2.4 AST Edge Prediction (TEP)
When converting an AST tree into a sequence, some crucial
structural information might get lost. Some existing stud-
ies, such as Tree Transformer (Wang, Lee, and Chen 2019)
and Tree LSTM (Tai, Socher, and Manning 2015), put trees
into models by introducing additional modules. Inspired by
the edge masking technique over data-ﬂow graphs proposed
in GraphCodeBERT (Guo et al. 2021), we design an AST

edge prediction objective to encode the tree structure in-
formation into the model simply and directly without in-
troducing additional modules. Taking the token “result”
as an example in Figure 3(a), there is an edge between to-
kens (“assignment”, “result”), and there is no edge
between (“result”, “=”). To incorporate such tree struc-
tural information, we mask edges in the AST and ask the
model to predict these edges. Formally, the loss function of
this TEP objective is deﬁned as:

LTEP = −

(cid:88)

(i,j)∈Na

[yTEP

(i,j) lnpTEP

(i,j) +(1−yTEP

(i,j) )ln(1−pTEP

(i,j) )] , (4)

where Na represents the set of all AST node pairs. yTEP
(i,j) is 1
if there is an edge between the i-th and j-th nodes, otherwise
yTEP
(i,j) = 0. pTEP
(i,j) is the probability of whether there is an
edge between the i-th and j-th nodes, which is calculated
by dot product using representations of these two nodes. A
sigmoid activation function is utilized to normalize the value
of pTEP

(i,j) within the range of 0 to 1.

2.5 Multi-Modal Contrastive Learning (MCL)
Previous works (Li et al. 2020; Reimers and Gurevych
2019) have shown that native sentence representations de-
rived from BERT are dominated by high-frequency tokens.
This token imbalance issue is even more serious in codes.
Taking the Python language as an example, the “def” to-
ken appears in almost all functions. Contrastive learning en-
courages the representation of the original sequence to be
closer to the representation of the “positive” augmented se-
quence, while staying away from representations of “nega-
tive” sequences, making the model learn a more even deci-
sion boundary across different data points to reconcile the

Code sequenceText SequenceAST sequencereturn the  sum  ...[CLS] return  the [MASK]  ...[SEP]...  result =     x      +  y  ...  ...  result = [MASK] +  y  ... [SEP]...  assignment  result        =      ...   ...  assignment  result   [MASK]  ...   sum=xMMLMIP✔✔✖TEP[SEP]CommentReturn the sum of two numbersSource CodeAbstract Syntax Tree......(a) SYNCOBERT pre-training over MMLM, IP and TEP objectivesNL-PL-AST sample x2(b) Multi-modal contrastive pre-training on NL-PL paired databatch b1(c) Positive and negative sampling for xi   positivenegative...function_definitionblockexpression_statementreturn_statementassignmentresult=binary_operatorreturnresultx+yl=12l=2l=1SYNCOBERT12312310101Multi-Modal Contrastive TrainingPL-AST sample  x1wca[CLS][SEP][SEP]1. MASK( x2, seed1 )2. Swap   PL  AST 1. MASK( x2, seed2 )SYNCOBERT[SEP]SYNCOBERTMLPMLPv2+v2[CLS]c[SEP][SEP]aw[CLS][SEP]SYNCOBERTSYNCOBERTMLPMLPv1+ v1NL sample  x1+ wca[CLS][SEP][SEP][SEP]wca[CLS][SEP][SEP][SEP](left)(right)xixi+ def add_func(x,y):    result = x + y    return resultx2x2+...batch b2..................representation bias caused by token imbalance (Yan et al.
2021). Several recent works (Jain et al. 2021; Bui, Yu, and
Jiang 2021) attempt to compare similar and dissimilar code
snippets. However, they only handle the single modality
of code, and ignore the multi-modal characteristic of pro-
gramming languages. These semantically equivalent modal-
ities can provide complementary information to learn more
comprehensive code representations.To this end, we pro-
pose a Multi-Modal Contrastive Learning (MCL) objective
to explore the potential for maximizing mutual information
among different modalities, encouraging the model to learn
useful connections and semantic equivalences.

To be speciﬁc, we train SYNCOBERT with paired data
and unpaired data. Paired data refers to codes (PL) with
paired natural language comments (NL), while unpaired
data stands for standalone codes without paired natural lan-
guage comments. Next, we explain how we construct posi-
tive and negative samples for these two cases.

Positive Samples For paired data, we design two simple
and effective ways to construct positive samples for MCL:
• NL vs PL-AST. To bridge the gap between a natural lan-
guage comment with its code snippet, we consider a com-
ment (NL) as a positive sample w.r.t the corresponding
code and AST. That it, NL & PL-AST forms a positive
pair, and an illustration example can be found at x1 and
x+
1 of Figure 3(b)(left).

• NL-PL-AST vs NL-AST-PL. To better learn the semantic
equivalence between PL and AST conditioned on the same
NL comment, we propose to construct another group of
positive samples by reversing the order of PL (c) and AST
(a) in the input triplet {w, c, a} to be {w, a, c}. Before
this order swapping operation, the original input triplet is
ﬁrstly masked by different random seeds. This step aims
to increase differences between positive pairs. A concrete
example of these steps is illustrated in Figure 3(b)(right).
For unpaired data, we construct positive sample pairs con-
sidering PL-AST vs AST-PL. This scheme works the same
as the setting of NL-PL-AST vs NL-AST-PL introduced
above without considering the NL comment.

Negative Samples To obtain negative samples for MCL,
we adopt in-batch and cross-batch sampling during training.
For a batch of training data b1 = [x1 . . . xN ] with size N , we
1 . . . x+
can ﬁrst obtain another positive data batch b2 = [x+
N ]
with size N using schemes described before where {xi, x+
i }
is a positive pair. For xi, the in-batch and the cross-batch
negative samples are {xj}, ∀j (cid:54)= i, In this way, we can ob-
tain a set X− of 2N − 2 negative samples for each xi, and
an illustrative example is given in Figure 3(c).

Overall MCL Pipeline For an input xi in paired data, the
following steps are executed (unpaired data is similar):
• First, we construct a positive sample x+
i

for xi, and the
two ways to construct positive samples for paired data in-
troduced before are illustrated in Figure 3(b).

• Second, we take xi and x+

i as inputs of SYNCOBERT.
Then we can obtain the vector representations of them
hi = SYNCOBERT(xi) and h+
i = SYNCOBERT(x+
i ).

i = f (h+

i with representation v+

• Finally, we adopt a two-layer MLP f (·) that maps repre-
sentations to the space vi = f (hi), v+
i ) where
contrastive loss is applied. Through the nonlinear transfor-
mation, more information can be maintained in h (Chen
et al. 2020).
For an input xi with representation vi, it has one pos-
itive sample x+
i , and it also has a
set of negative samples X− with size 2N − 2. We denote
the set of representations for samples in X− as V− =
{v−
2N −2}. The objective of contrastive learning is
to maximize the representation similarity between positive
samples, while minimizing the representation similarity be-
tween negative samples. Therefore, we deﬁne the loss func-
tion for a positive pair (xi, x+
i ) as:
exp(vi · v+
i )
i ) + (cid:80)2N −2

1 , . . . , v−

exp(vi · v−
k )

exp(vi · v+

i ) = −ln

l(xi, x+

(5)

k=1

,

where the similarity of a pair of samples is deﬁned by the
dot product of their representations as: vi · vj. This loss has
been used in previous works (Chen et al. 2020; Wu et al.
2020). We calculate the loss for the same pair twice with
order switched, i.e., (xi, x+
i ) changes to (x+
i , xi) as the dot
product with negative samples for xi and x+
i are different.
Finally, the overall MCL loss is deﬁned as follows:

LMCL =

N
(cid:88)

i

(cid:2)l(xi, x+

i ) + l(x+

i , xi)(cid:3) ,

(6)

2.6 Training Objective
The overall loss function in SYNCOBERT is the integration
of the several components we deﬁned before as:

L = LMMLM + LIP + LTEP + LMCL + λ(cid:107)Θ(cid:107)2 ,

(7)

where Θ contains all trainable parameters of the model. λ is
the L2 regularization coefﬁcient used to prevent overﬁtting.

3 Experiment Setup

3.1 Pre-Training Dataset and Settings
For fair comparisons, we pre-train SYNCOBERT on the
CodeSearchNet dataset (Husain et al. 2019), which is the
same as that used by CodeBERT and GraphCodeBERT.
CodeSearchNet dataset contains 2.1M bimodal data points
(code functions paired with natural language comments) and
6.4M unimodal codes across six programming languages.
More details are presented in the supplementary materials.

We train SYNCOBERT using Transformer with 12 lay-
ers, 768 hidden sizes, and 12 attention heads. SYNCOBERT
is trained on 8 NVIDIA Tesla V100 with 32GB memory.
The lengths of sequences containing special tokens in NL,
PL, and AST are set to 96, 160, and 256, respectively. The
batch size is set to 128. The learning rate is set to 1e-4. We
use an Adam optimizer to optimize the parameters of the
model. We employ a Byte-Pair Encoding (BPE) tokenizer
(Sennrich, Haddow, and Birch 2016). To accelerate the pro-
cess of training, we adopt the parameters of CodeBERT to
initialize the model same as GraphCodeBERT. Finally, the
model is trained with 110K steps and costs about 80 hours.

Table 1: Results on the natural language code search task evaluating with MRR, using the AdvTest and CodeSearch datasets.

Model

NBow
CNN
BiRNN
Transformer
RoBERTa
RoBERTa (code)
CodeBERT
GraphCodeBERT
SYNCOBERT

AdvTest
Python
-
-
-
-
18.3
-
27.2
35.2
38.1

Ruby
16.2
27.6
21.3
27.5
58.7
62.8
67.9
70.3
72.2

Javascript
15.7
22.4
19.3
28.7
51.7
56.2
62.0
64.4
67.7

CodeSearch
Python
16.1
24.2
29.0
39.8
58.7
61.0
67.2
69.2
72.4

Go
33.0
68.0
68.8
72.3
85.0
85.9
88.2
89.7
91.3

Java
17.1
26.3
30.4
40.4
59.9
62.0
67.6
69.1
72.3

PHP Average
15.2
26.0
33.8
42.6
56.0
57.9
62.8
64.9
67.8

18.9
32.4
33.8
41.9
61.7
64.3
69.3
71.3
74.0

3.2 Evaluation Tasks, Datasets, and Metrics
Since SYNCOBERT belongs to the BERT-like code pre-
trained models, it is more suitable for programming lan-
guage understanding (PLU) tasks, so we choose all three
PLU tasks in CodeXGLUE (Lu et al. 2021), including nat-
ural language code search, code clone detection, and code
defect detection. To reﬂect the generality of the model, we
also select a programming language generation (PLG) task,
such as program translation.

Natural Language Code Search is to match the most se-
mantically relevant code functions through natural language
queries. We use the AdvTest dataset (Lu et al. 2021) to con-
duct the experiment on Python language. In order to evaluate
other programming languages, we also adopt CodeSearch
dataset (Guo et al. 2021), including six programming lan-
guages (Ruby, Javascript, Go, Python, Java, PHP). We adopt
Mean Reciprocal Rank (MRR) to evaluate the performances
of all code search methods.

Code Clone Detection is to identify the existence of
code clone issues by measuring the similarity between two
code snippets. We ﬁne-tune SYNCOBERT on the Big-
CloneBench (Svajlenko et al. 2014) and POJ-104 (Mou et al.
2016) datasets. In the BigCloneBench dataset (Java), given
two codes, the task is to judge whether they are semantically
similar, evaluating by Precision, Recall, and F1-score. In the
POJ-104 dataset (C/C++), given a code, the task retrieves
499 codes evaluating by Mean Average Precision (MAP).

Code Defect Detection is to identify whether it is an in-
secure code that may attack software systems.It can be re-
garded as a binary classiﬁcation task.We evaluate all models
on Defects4J dataset (C language) (Zhou et al. 2019), using
the Accuracy score.

Program Translation is to translate the code of one
programming language into another. We adopt CodeTrans
dataset (Chen, Liu, and Song 2018), which contains the mu-
tual translation of C# and Java. All methods are evaluated by
BLEU-4, Exact Match, and CodeBLEU (Lu et al. 2021).

All datasets

except CodeSearch are provided in
CodeXGLUE (Lu et al. 2021), and the default
train-
ing/validation/testing splits are used. All evaluation task
datasets and ﬁne-tuning details are presented in the
supplementary materials.

3.3 Baseline Methods

We compare SYNCOBERT with various state-of-the-art
methods in two categories. In the ﬁrst category, models are
directly trained on the evaluation task from scratch. In the
second category, models are pre-trained on unlabeled cor-
pus ﬁrst and then ﬁne-tuned on the evaluation task.

Training from Scratch
• NBow is short for bag-of-words, which ignores the word
order, grammar, syntax, and other elements of the se-
quence. It selects candidates based on the number of
shared works for natural language code search tasks.

• Naive copy, PBSMT are used in program translation
tasks. Naive copy means copying the source code as
the translation result. PBSMT (Koehn, Och, and Marcu
2003) is a statistical machine translation method based on
phrases.

• TextCNN (Kim 2014) is a CNN-based model to capture

the features of NL or PL at the word level.

• BiLSTM (Cho et al. 2014) is a Seq2Seq model based on
bidirectional LSTM with an attention mechanism (Luong,
Pham, and Manning 2015).

• Transformer (Vaswani et al. 2017) is the base architec-
ture of SYNCOBERT and other pre-trained models. We
use the same number of layers and hidden size as pre-
trained models.

Pre-Trained Models
• RoBERTa (Liu et al. 2019) is pre-trained on natural lan-

guages.

• RoBERTa(code) is a varient of RoBERTa, and is pre-

trained on source code from CodeSearchNet corpus.

• code2vec (Alon et al. 2019) uses a soft-attention mecha-
nism on AST paths of the code, and aggregate all of their
vector representations into a single vector. Coimbra et al.
(2021) pre-trained the code2vec on open-source C lan-
guage corpus for code defect detection task.

• CodeBERT (Feng et al. 2020) is pre-trained on PL-NL

pairs with MLM and RTD pre-training objectives.

• GraphCodeBERT (Guo et al. 2021) is pre-trained on the

basis of CodeBERT, integrating data ﬂow of codes.

Table 2: Results on the clone detection task, using the Big-
CloneBench (Java) and POJ-104 (C/C++) datasets.

Table 3: Results on the code defect detection task, using the
Defects4J dataset (C language).

Model

BigCloneBench

POJ-104
Precision Recall F1-score MAP@R

RoBERTa
CodeBERT
GraphCodeBERT
SYNCOBERT

93.5
96.0
97.3
97.1

96.5
96.9
96.8
97.7

94.9
96.5
97.1
97.4

76.67
82.67
85.16
88.24

4 Results and Analysis

4.1 Natural Language Code Search
Table 1 summarizes the results of natural language code
search on different datasets. The baseline results on the Ad-
vTest dataset are reported in (Lu et al. 2021). We lever-
age the checkpoint of GraphCodeBERT to obtain the result
on the AdvTest dataset. Baseline results on the CodeSearch
dataset are all reported in (Guo et al. 2021).

We can observe that SYNCOBERT outperforms all meth-
ods on two datasets. On average, it outperforms CodeBERT
by 10.9 points on the AdvTest dataset and 4.7 points on
the CodeSearch dataset. Compared to GraphCodeBERT, it
scores 2.9 points higher on the AdvTest dataset, and 2.7
points averagely higher on the CodeSearch dataset. SYN-
COBERT is pre-trained on the same pre-training corpus as
CodeBERT and GraphCodeBERT. The signiﬁcant perfor-
mance improvement indicates that SYNCOBERT can learn
better code representations. We credit this improvement to
the introduction of multi-modal contrastive learning. More-
over, compared to the Transformer baseline, SYNCOBERT
achieves a 76.6% improvement on the CodeSearch dataset,
and we credit this improvement to the pre-training step.

4.2 Code Clone Detection
Table 2 shows experimental results on the code clone detec-
tion task. The baseline results are reported in (Lu et al. 2021)
and (Guo et al. 2021). We can observe that SYNCOBERT
achieves the best overall performance on the two datasets.
On the BigCloneBench dataset, SYNCOBERT outperforms
all other methods w.r.t. Recall and F1-score although it is
slightly worse than GraphCodeBERT w.r.t. Precision. On
the POJ-104 dataset, SYNCOBERT consistently outper-
forms all methods. Compared to CodeBERT and Graph-
CodeBERT, it achieves 5.57 and 3.08 points higher respec-
tively. As SYNCOBERT is not pre-trained on C/C++ lan-
guages, the superior performance on the POJ-104 dataset
indicates that SYNCOBERT learns better generic program
semantics. These results indicate that SYNCOBERT indeed
learns better code representations for code clone detection.

4.3 Code Defect Detection
The experimental results on code defect detection task are
shown in Table 3. Several representative baseline results are
reported in (Lu et al. 2021). We can observe that SYN-
COBERT outperforms all representative models. Speciﬁ-
cally, it outperforms CodeBERT and GraphCodeBERT by
2.42 and 1.29, respectively. The performance improvement

Models
BiLSTM
TextCNN
RoBERTa
CodeBERT
code2vec
GraphCodeBERT
SYNCOBERT

Accuracy
59.37
60.69
61.05
62.08
62.48
63.21
64.50

indicates that SYNCOBERT learns more comprehensive
program semantics for defect detection. It is also noteworthy
that SYNCOBERT is not pre-trained on C language, while
outperforms the code2vec (pre-trained on C language). This
result indicates that SYNCOBERT can learn general syntax
information, beneﬁting other programming languages.

4.4 Program Translation
Table 4 shows the experimental results on the program trans-
lation task. The baseline results are reported in (Lu et al.
2021) and (Guo et al. 2021). For BERT-like pre-trained
models, we leverage them to initialize the encoder, and
randomly initialize the parameters of the decoder (6-layer
Transformer) and the source-to-target attention weights for
the translation task. We can observe that SYNCOBERT still
surpasses all models and achieve the best overall perfor-
mance. In the C#→Java task, SYNCOBERT outperforms
CodeBERT by 4.38 BLEU points, 2.5 exact match points,
and 2.81 CodeBLEU points. We contend that the reason for
the above improvements is the introduction of code syn-
tax knowledge. Although SYNCOBERT has not been pre-
trained on the C#, there is signiﬁcant syntactic and seman-
tic similarities between C# and Java. The syntax knowledge
integrated into the pre-training phase of SYNCOBERT can
effectively generalize to other programming languages.

4.5 Ablation Study
In this experiment, we compare several simpliﬁed versions
of SYNCOBERT to understand the effect of different com-
ponents, including AST Edge Prediction (TEP) objective,
Identiﬁer Prediction (IP) objective and Multi-modal Con-
trastive Learning (MCL). As a case study, we take the
natural language code search task using the CodeSearch
dataset, and present the results in Table 5. The setting of
w/o (TEP, IP, MCL) indicates that SYNCOBERT progres-
sively removes these components. We can observe that: (1)
the two pre-training objectives (TEP and IP) that exploit
the symbolic and syntactic information are effective. Drop-
ping these two components slightly hurts the performance
of SYNCOBERT. (2) MCL plays a more important role for
SYNCOBERT as further cutting this component degrades
the performance a lot. MCL may allow the model to max-
imize the mutual information between different modalities
to learn more comprehensive sentence-level representations.
This ablation result shows that it contributes a lot to the nat-
ural language code search task.

Table 4: Results on the code translation task with BLEU, Accuracy and CodeBLEU score, using the CodeTrans dataset.

Methods

Naive copy
PBSMT
Transformer
RoBERTa (code)
CodeBERT
GraphCodeBERT
SYNCOBERT

C#→Java
BLEU Exact Match
18.69
40.06
50.47
71.99
72.14
72.64
76.52

0.0
16.1
37.90
57.90
58.80
58.80
61.30

Table 5: Ablation study on the natural language code search
task evaluating with MRR, using the CodeSearch dataset.

Ruby JS Go PY Java PHP Avg.
Models
SYNCOBERT 72.2 67.7 91.3 72.4 72.3 67.8 74.0
72.0 67.5 91.1 72.2 71.9 67.6 73.7
w/o TEP
w/o IP
71.4 66.7 90.5 71.6 71.2 66.9 73.1
70.6 64.2 89.3 68.6 68.7 64.6 71.0
w/o MCL

5 Related Work

Pre-Trained Models on Programming Language With
the success of pre-trained models in NLP, some recent works
attempt to extend the pre-training technologies to codes. The
pre-trained models on codes promote the development of
code intelligence. Kanade et al. (2020) developed CuBERT,
which is pre-trained on the Python language. They adopted
the masked language modeling objective of BERT to ob-
tain general code representations. Feng et al. (2020) pro-
posed CodeBERT, which is pre-trained on NL-PL pairs in
six programming languages, adding a replaced token de-
tection objective (Clark et al. 2020). Guo et al. (2021) de-
veloped GraphCodeBERT based on CodeBERT consider-
ing the data ﬂow of codes. Besides the BERT-like models,
Svyatkovskiy et al. (2020) and Liu et al. (2020) respec-
tively proposed CodeGPT and CugLM for code comple-
tion task based on the transformer (Vaswani et al. 2017) de-
coder. Ahmad et al. (2021) proposed PLBART based on the
BART (Lewis et al. 2020) architecture, which is pre-trained
on large-scale Java and Python functions paired with nat-
ural language comments via denoising autoencoding. Phan
et al. (2021) proposed CoTexT follows the architecture of T5
(Raffel et al. 2019), which employs denoising sequence-to-
sequence pre-training on multiple programming languages.
Jiang et al. (2021) proposed TreeBERT, an encoder-decoder
architecture, incorporating the AST into the model. It is pre-
trained by tree masked language modeling (TMLM) and
node order prediction (NOP) objectives. TreeBERT does not
consider the edges in AST, which is not sufﬁcient to exploit
rich syntactic structure information within edges. Neither of
them takes into account the symbolic property of program-
ming languages. Further exploration of the multi-modal po-
tential of programming languages is still insufﬁcient.

Contrastive Learning Contrastive learning has become
an emerging ﬁeld due to its great success in computer vi-

Java→C#
CodeBLEU BLEU Exact Match
18.54
43.53
55.84
77.46
79.92
80.58
80.75

-
43.48
61.59
80.18
79.41
-
82.22

0.0
12.50
33.00
56.10
59.00
59.40
60.40

CodeBLEU
-
42.71
63.74
83.07
85.10
-
84.85

sion (Chen et al. 2020; Misra and van der Maaten 2020;
Tschannen et al. 2020). Some works use different data aug-
mentations (spatial/geometric and appearance transforma-
tions) to make an image agree with each other, improving
the quality of visual representations. Inspired by these, sev-
eral works try to use contrastive learning on NL and PL.
Fang and Xie (2020) proposed CERT model, treating back-
translated sentence and original sentence as a positive pair.
Giorgi et al. (2020) presented DeCLUTR model, consider-
ing that different spans in the same document are similar to
each other. Bui, Yu, and Jiang (2021) and Jain et al. (2021)
exploited the contrastive learning on codes. they trained the
neural network over a contrastive learning objective to com-
pare similar and dissimilar code snippets. However, they
only handle the single modality of code, and ignore the
multi-modal characteristic of programming languages.

Multiple modalities contain complementary information
that offers the potential for drawing useful connections
across different modalities. Some recent works attempt to
adopt multi-modal contrastive learning to mine more com-
prehensive representations (Yuan et al. 2021; Xu et al. 2021;
Hassani and Ahmadi 2020). Yuan et al. (2021) proposed a
method to improve visual representations embracing multi-
modal data. They exploited intrinsic data properties within
each modality and semantic information from cross-modal
correlation simultaneously. Xu et al. (2021) proposed a con-
trastive multi-modal clustering framework to mine high-
level semantic information, considering both multi-modal
consistency and diversity. Hassani and Ahmadi (2020) pro-
posed a multi-view method for learning node and graph level
representations by contrasting structural views of graphs.

6 Conclusion
In this paper, we have proposed SYNCOBERT, a syntax-
guided multi-modal contrastive pre-training framework for
code representation. Considering the symbolic and syntactic
property of source code, we design two new pre-training ob-
jectives to predict identiﬁers, and edges between two nodes
of AST, respectively. Meanwhile, to exploit the complemen-
tary information in semantically equivalent modalities (i.e.,
code, comment, AST) of code, We propose a multi-modal
contrastive learning strategy to maximize the mutual infor-
mation among different modalities. Comprehensive experi-
ments conducted on four code intelligence tasks demonstrate
that SYNCOBERT achieves state-of-the-art with the same
pre-training corpus and model size.

References
Ahmad, W. U.; Chakraborty, S.; Ray, B.; and Chang, K.-
W. 2021. Uniﬁed Pre-training for Program Understanding
and Generation. In Proceedings of the 2021 Conference of
the North American Chapter of the Association for Compu-
tational Linguistics: Human Language Technologies, 2655–
2668.
Alon, U.; Zilberstein, M.; Levy, O.; and Yahav, E. 2019.
code2vec: learning distributed representations of code. Pro-
ceedings of the ACM on Programming Languages, 3: 40.
Bui, N. D. Q.; Yu, Y.; and Jiang, L. 2021. Self-Supervised
Contrastive Learning for Code Retrieval and Summarization
via Semantic-Preserving Transformations. In Proceedings of
the 44th International ACM SIGIR Conference on Research
and Development in Information Retrieval, 511–521.
Chen, T.; Kornblith, S.; Norouzi, M.; and Hinton, G. 2020. A
Simple Framework for Contrastive Learning of Visual Rep-
resentations. In ICML 2020: 37th International Conference
on Machine Learning, volume 1, 1597–1607.
Chen, X.; Liu, C.; and Song, D. 2018. Tree-to-tree neural
networks for program translation. In NIPS’18 Proceedings
of the 32nd International Conference on Neural Information
Processing Systems, volume 31, 2552–2562.
Cho, K.; van Merrienboer, B.; Bahdanau, D.; and Bengio,
Y. 2014. On the Properties of Neural Machine Translation:
In Proceedings of SSST-8,
Encoder–Decoder Approaches.
Eighth Workshop on Syntax, Semantics and Structure in Sta-
tistical Translation, 103–111.
Clark, K.; Luong, M.-T.; Le, Q. V.; and Manning, C. D.
2020. ELECTRA: Pre-training Text Encoders as Discrim-
inators Rather Than Generators. In ICLR 2020 : Eighth In-
ternational Conference on Learning Representations.
Coimbra, D.; Reis, S.; Abreu, R.; Pasareanu, C. S.; and Er-
dogmus, H. 2021. On using distributed representations of
source code for the detection of C security vulnerabilities.
arXiv: Cryptography and Security.
Devlin, J.; Chang, M.-W.; Lee, K.; and Toutanova, K. N.
2018. BERT: Pre-training of Deep Bidirectional Transform-
ers for Language Understanding. In Proceedings of the 2019
Conference of the North American Chapter of the Associa-
tion for Computational Linguistics: Human Language Tech-
nologies, Volume 1 (Long and Short Papers), 4171–4186.
Fang, H.; and Xie, P. 2020. CERT: Contrastive Self-
supervised Learning for Language Understanding. arXiv
preprint arXiv:2005.12766.
Feng, Z.; Guo, D.; Tang, D.; Duan, N.; Feng, X.; Gong,
M.; Shou, L.; Qin, B.; Liu, T.; Jiang, D.; and Zhou, M.
2020. CodeBERT: A Pre-Trained Model for Programming
and Natural Languages. In Findings of the Association for
Computational Linguistics: EMNLP 2020, 1536–1547.
Giorgi, J. M.; Nitski, O.; Bader, G. D.; and Wang, B. 2020.
DeCLUTR: Deep Contrastive Learning for Unsupervised
Textual Representations. arXiv preprint arXiv:2006.03659.
Gu, X.; Zhang, H.; and Kim, S. 2018. Deep code search. In
2018 IEEE/ACM 40th International Conference on Software
Engineering (ICSE), 933–944.

Guo, D.; Ren, S.; Lu, S.; Feng, Z.; Tang, D.; Liu, S.; Zhou,
L.; Duan, N.; Svyatkovskiy, A.; Fu, S.; Tufano, M.; Deng,
S. K.; Clement, C.; Drain, D.; Sundaresan, N.; Yin, J.; Jiang,
D.; and Zhou, M. 2021. GraphCodeBERT: Pre-training
Code Representations with Data Flow. In ICLR 2021: The
Ninth International Conference on Learning Representa-
tions.
Hassani, K.; and Ahmadi, A. H. K. 2020. Contrastive
Multi-View Representation Learning on Graphs. ArXiv,
abs/2006.05582.
Husain, H.; Wu, H.-H.; Gazit, T.; Allamanis, M.; and
Brockschmidt, M. 2019. CodeSearchNet Challenge: Eval-
uating the State of Semantic Code Search. arXiv preprint
arXiv:1909.09436.
Jain, P.; Jain, A.; Zhang, T.; Abbeel, P.; Gonzalez, J. E.; and
Stoica, I. 2021. Contrastive Code Representation Learning.
In arXiv e-prints.
Jiang, X.; Zheng, Z.; Lyu, C.; Li, L.; and Lyu, L. 2021. Tree-
BERT: A Tree-Based Pre-Trained Model for Programming
In UAI 2021: Uncertainty in Artiﬁcial Intelli-
Language.
gence.
Kanade, A.; Maniatis, P.; Balakrishnan, G.; and Shi, K.
2020. Learning and Evaluating Contextual Embedding of
Source Code. In ICML 2020: 37th International Conference
on Machine Learning, volume 1, 5110–5121.
Kim, Y. 2014. Convolutional Neural Networks for Sen-
In Proceedings of the 2014 Confer-
tence Classiﬁcation.
ence on Empirical Methods in Natural Language Processing
(EMNLP), 1746–1751.
Koehn, P.; Och, F. J.; and Marcu, D. 2003. Statistical phrase-
based translation. In NAACL ’03 Proceedings of the 2003
Conference of the North American Chapter of the Associ-
ation for Computational Linguistics on Human Language
Technology - Volume 1, 48–54.
Lan, Z.; Chen, M.; Goodman, S.; Gimpel, K.; Sharma, P.;
and Soricut, R. 2020. ALBERT: A Lite BERT for Self-
supervised Learning of Language Representations. In ICLR
2020 : Eighth International Conference on Learning Repre-
sentations.
Lewis, M.; Liu, Y.; Goyal, N.; Ghazvininejad, M.; Mo-
hamed, A.; Levy, O.; Stoyanov, V.; and Zettlemoyer, L.
2020. BART: Denoising Sequence-to-Sequence Pre-training
for Natural Language Generation, Translation, and Compre-
hension. In Proceedings of the 58th Annual Meeting of the
Association for Computational Linguistics, 7871–7880.
Li, B.; Zhou, H.; He, J.; Wang, M.; Yang, Y.; and Li, L. 2020.
On the Sentence Embeddings from Pre-trained Language
In Proceedings of the 2020 Conference on Em-
Models.
pirical Methods in Natural Language Processing (EMNLP),
9119–9130.
Li, J.; He, P.; Zhu, J.; and Lyu, M. R. 2017. Software De-
fect Prediction via Convolutional Neural Network. In 2017
IEEE International Conference on Software Quality, Relia-
bility and Security (QRS), 318–328.
Liu, F.; Li, G.; Zhao, Y.; and Jin, Z. 2020. Multi-task learn-
ing based pre-trained language model for code completion.

In Proceedings of the 35th IEEE/ACM International Confer-
ence on Automated Software Engineering, 473–485.
Liu, Y.; Ott, M.; Goyal, N.; Du, J.; Joshi, M.; Chen, D.;
Levy, O.; Lewis, M.; Zettlemoyer, L.; and Stoyanov, V.
2019. RoBERTa: A Robustly Optimized BERT Pretraining
Approach. arXiv preprint arXiv:1907.11692.
Lu, S.; Guo, D.; Ren, S.; Huang, J.; Svyatkovskiy, A.;
Blanco, A.; Clement, C. B.; Drain, D.; Jiang, D.; Tang, D.;
Li, G.; Zhou, L.; Shou, L.; Zhou, L.; Tufano, M.; Gong,
M.; Zhou, M.; Duan, N.; Sundaresan, N.; Deng, S. K.; Fu,
S.; and Liu, S. 2021. CodeXGLUE: A Machine Learning
Benchmark Dataset for Code Understanding and Genera-
tion. arXiv preprint arXiv:2102.04664.
Luong, M.-T.; Pham, H.; and Manning, C. D. 2015. Effec-
tive Approaches to Attention-based Neural Machine Trans-
lation. In Proceedings of the 2015 Conference on Empirical
Methods in Natural Language Processing, 1412–1421.
Misra, I.; and van der Maaten, L. 2020. Self-Supervised
In 2020
Learning of Pretext-Invariant Representations.
IEEE/CVF Conference on Computer Vision and Pattern
Recognition (CVPR), 6707–6717.
Mou, L.; Li, G.; Zhang, L.; Wang, T.; and Jin, Z. 2016. Con-
volutional neural networks over tree structures for program-
ming language processing. In AAAI’16 Proceedings of the
Thirtieth AAAI Conference on Artiﬁcial Intelligence, 1287–
1293.
Phan, L.; Tran, H.; Le, D.; Nguyen, H.; Anibal, J.; Peltekian,
A.; and Ye, Y. 2021. CoTexT: Multi-task Learning with
Code-Text Transformer. arXiv: Artiﬁcial Intelligence.
Radford, A.; Narasimhan, K.; Salimans, T.; and Sutskever,
I. 2018.
Improving language understanding by generative
pre-training.
Raffel, C.; Shazeer, N.; Roberts, A.; Lee, K.; Narang, S.;
Matena, M.; Zhou, Y.; Li, W.; and Liu, P. J. 2019. Explor-
ing the Limits of Transfer Learning with a Uniﬁed Text-to-
Text Transformer. Journal of Machine Learning Research,
21(140): 1–67.
Sentence-BERT:
Reimers, N.; and Gurevych, I. 2019.
Sentence Embeddings using Siamese BERT-Networks.
In
Proceedings of the 2019 Conference on Empirical Meth-
ods in Natural Language Processing and the 9th Interna-
tional Joint Conference on Natural Language Processing
(EMNLP-IJCNLP), 3980–3990.
Sennrich, R.; Haddow, B.; and Birch, A. 2016. Neural Ma-
chine Translation of Rare Words with Subword Units.
In
Proceedings of the 54th Annual Meeting of the Associa-
tion for Computational Linguistics (Volume 1: Long Papers),
volume 1, 1715–1725.
Svajlenko, J.; Islam, J. F.; Keivanloo, I.; Roy, C. K.; and
Mia, M. M. 2014. Towards a Big Data Curated Benchmark
of Inter-project Code Clones. In ICSME ’14 Proceedings of
the 2014 IEEE International Conference on Software Main-
tenance and Evolution, 476–480.
Svyatkovskiy, A.; Deng, S. K.; Fu, S.; and Sundaresan, N.
Intellicode compose: Code generation using trans-
2020.
In Proceedings of the 28th ACM Joint Meeting
former.

on European Software Engineering Conference and Sym-
posium on the Foundations of Software Engineering, 1433–
1443.
Tai, K. S.; Socher, R.; and Manning, C. D. 2015.
Im-
proved Semantic Representations From Tree-Structured
Long Short-Term Memory Networks. In Proceedings of the
53rd Annual Meeting of the Association for Computational
Linguistics and the 7th International Joint Conference on
Natural Language Processing (Volume 1: Long Papers), vol-
ume 1, 1556–1566.
Tschannen, M.; Djolonga, J.; Rubenstein, P. K.; Gelly, S.;
and Lucic, M. 2020. On Mutual Information Maximization
for Representation Learning. In ICLR 2020 : Eighth Inter-
national Conference on Learning Representations.
Vaswani, A.; Shazeer, N.; Parmar, N.; Uszkoreit, J.; Jones,
L.; Gomez, A. N.; Kaiser, L.; and Polosukhin, I. 2017. At-
tention is All You Need. In Proceedings of the 31st Inter-
national Conference on Neural Information Processing Sys-
tems, volume 30, 5998–6008.
Wang, Y.; Lee, H.-Y.; and Chen, Y.-N. 2019. Tree trans-
former: Integrating tree structures into self-attention.
In
Proceedings of the 2019 Conference on Empirical Meth-
ods in Natural Language Processing and the 9th Interna-
tional Joint Conference on Natural Language Processing
(EMNLP-IJCNLP), 1061–1070.
White, M.; Tufano, M.; Vendome, C.; and Poshyvanyk, D.
2016. Deep learning code fragments for code clone detec-
In Proceedings of the 31st IEEE/ACM International
tion.
Conference on Automated Software Engineering, 87–98.
Wu, Z.; Wang, S.; Gu, J.; Khabsa, M.; Sun, F.; and Ma, H.
2020. CLEAR: Contrastive Learning for Sentence Repre-
sentation. arXiv preprint arXiv:2012.15466.
Xu, J.; Tang, H.; Ren, Y.; Zhu, X.; and He, L. 2021.
arXiv preprint
Contrastive Multi-Modal Clustering.
arXiv:2106.11193.
Yan, Y.; Li, R.; Wang, S.; Zhang, F.; Wu, W.; and Xu,
W. 2021. ConSERT: A Contrastive Framework for Self-
Supervised Sentence Representation Transfer. In ACL 2021:
59th annual meeting of the Association for Computational
Linguistics, 5065–5075.
Yuan, X.; Lin, Z.; Kuen, J.; Zhang, J.; Wang, Y.; Maire,
M.; Kale, A.; and Faieta, B. 2021. Multimodal Contrastive
In Proceed-
Training for Visual Representation Learning.
ings of the IEEE/CVF Conference on Computer Vision and
Pattern Recognition, 6995–7004.
Zhou, Y.; Liu, S.; Siow, J. K.; Du, X.; and Liu, Y. 2019.
Devign: Effective Vulnerability Identiﬁcation by Learning
Comprehensive Program Semantics via Graph Neural Net-
works. In Advances in Neural Information Processing Sys-
tems 2019, volume 32, 10197–10207.
Zugner, D.; Kirschstein, T.; Catasta, M.; Leskovec, J.; and
G¨unnemann, S. 2021. Language-Agnostic Representation
Learning of Source Code from Structure and Context.
In
ICLR 2021: The Ninth International Conference on Learn-
ing Representations.

