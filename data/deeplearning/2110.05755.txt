Draft version November 25, 2021
Typeset using LATEX twocolumn style in AASTeX63

1
2
0
2

v
o
N
4
2

]

A
G
.
h
p
-
o
r
t
s
a
[

3
v
5
5
7
5
0
.
0
1
1
2
:
v
i
X
r
a

Deep learning reconstruction of three-dimensional galaxy distributions with intensity mapping
observations
Kana Moriwaki1 and Naoki Yoshida1, 2, 3, 4

1Department of Physics, The University of Tokyo, 7-3-1 Hongo, Bunkyo, Tokyo 113-0033, Japan
2Kavli Institute for the Physics and Mathematics of the Universe (WPI), UT Institutes for Advanced Study, The University of Tokyo,
5-1-5 Kashiwanoha, Kashiwa, Chiba 277-8583, Japan
3Research Center for the Early Universe, School of Science, The University of Tokyo, 7-3-1 Hongo, Bunkyo, Tokyo 113-0033, Japan
4Institute for Physics of Intelligence, School of Science, The University of Tokyo, 7-3-1 Hongo, Bunkyo, Tokyo 113-0033, Japan

Submitted to ApJL

ABSTRACT

Line intensity mapping is emerging as a novel method that can measure the collective intensity
ﬂuctuations of atomic/molecular line emission from distant galaxies. Several observational programs
with various wavelengths are ongoing and planned, but there remains a critical problem of line confu-
sion; emission lines originating from galaxies at diﬀerent redshifts are confused at the same observed
wavelength. We devise a generative adversarial network that extracts designated emission line signals
from noisy three-dimensional data. Our novel network architecture allows two input data, in which
the same underlying large-scale structure is traced by two emission lines of Hα and [Oiii], so that the
network learns the relative contributions at each wavelength and is trained to decompose the respec-
tive signals. After being trained with a large number of realistic mock catalogs, the network is able
to reconstruct the three-dimensional distribution of emission-line galaxies at z = 1.3 − 2.4. Bright
galaxies are identiﬁed with a precision of 84%, and the cross-correlation coeﬃcients between the true
and reconstructed intensity maps are as high as 0.8. Our deep-learning method can be readily applied
to data from planned space-borne and ground-based experiments.

Keywords: high-redshift galaxies — large-scale structure of the universe — observational cosmology

1. INTRODUCTION

The large-scale distribution of galaxies carries rich in-
formation on the structure and the evolution of the Uni-
verse and on how galaxies are formed from early through
to the present day. Line intensity mapping (LIM) is
aimed at measuring large-scale intensity ﬂuctuations of
line emissions from galaxies and intergalactic gas. Com-
plementary to traditional galaxy surveys, LIM covers a
broad spectral range and detects signals from essentially
all emission sources residing in a large cosmological vol-
ume (Kovetz et al. 2017). It is thus possible to make a
structural ”map” of the Universe by a single observation.
There have already been a few successful experiments
that detect hydrogen 21-cm line (Chang et al. 2010; Ali

Corresponding author: Kana Moriwaki
kana.moriwaki@phys.s.u-tokyo.ac.jp

et al. 2015), and observations targeting other emission
lines such as CO/[Cii] and Lyα/Hα/[Oiii] are ongoing
(e.g., Keating et al. 2020; Concerto Collaboration et al.
2020; Cleary et al. 2021) or planned (e.g., Dor´e et al.
2014, 2018). LIM can eﬃciently survey a large observa-
tional volume, and the data from LIM are well suited, for
instance, to study the formation and evolution of galax-
ies (Breysse et al. 2016; Keating et al. 2016) as well as
geometry and the matter content of the Universe (see,
e.g., Dor´e et al. 2014). LIM can also be used to study
the reionization by combining with 21 cm observations
of the inter-galactic medium (Dumitru et al. 2019; Mori-
waki et al. 2019).

A key process in the analysis of LIM data is to separate
the contributions from diﬀerent emission lines originat-
ing from sources at diﬀerent redshifts. Let us consider
two emission lines with rest-frame wavelengths λ1 and
λ2. If they are emitted at redshifts z1 and z2 that sat-
isfy λ1(1 + z1) = λ2(1 + z2) = λo, they are observed at

 
 
 
 
 
 
2

Moriwaki et al.

the same wavelength λo, appearing as ”interlopers” to
each other. Cross-correlation analyses are proposed to
solve this line confusion problem (e.g., Visbal & Loeb
2010), and there are several other statistical methods
(e.g., Gong et al. 2014; Cheng et al. 2016). It is tech-
nically challenging to isolate the contribution of a par-
ticular emission line and to infer the intensity distribu-
tion, but a successful direct reconstruction of the three-
dimensional distribution of the emission sources would
enhance the constraining power in cosmological studies
as well as studies on the galaxy formation and evolu-
tion.
If the contamination of interloper lines can be
removed, we are able to analyze the large-scale struc-
ture accurately (see e.g., Fonseca et al. 2017) and also
constrain the galaxy population by using methods such
as the voxel intensity distribution (Breysse et al. 2017).
Customized convolutional neural networks (CNNs)
have been developed and applied to separate diﬀerent
emission line signals and to eﬀectively de-noise a map
(Moriwaki et al. 2020, 2021), but such applications are
limited to two-dimensional images without spectral in-
formation. Cheng et al. (2020) devise a reconstruction
method that makes use of spectral analysis. Their algo-
rithm eﬀectively extracts the source galaxies with mul-
tiple emission lines brighter than a few times the noise
level, but fainter signals still remain diﬃcult to be de-
tected. In this Letter, we propose to utilize the spectral
information in an eﬃcient manner so that a ”machine”
can learn the correlation of multiple emission lines at
It is possible to perform a full
diﬀerent wavelengths.
three-dimensional reconstruction by using a LIM obser-
vation with a broad wavelength coverage. This ﬁnally
enables the reconstruction of the three-dimensional cos-
mic structure with LIM.

2. METHOD
We primarily consider NASA’s SPHEREx1 mission to
be launched in 2024 and identify the two brightest emis-
sion lines Hα 6563 ˚A and [Oiii] 5007 ˚A as our target
to be detected by SPHEREx. We do not consider the
other interlopers such as [Oii] and Hβ. While the other
lines’ intensities are likely to be subdominant, they can
also carry additional information in the spectral domain.
Our method can easily be adjusted to deal with more
than two emission lines, although the time needed for
training may increase.

2.1. Training data

To generate mock observation catalogs for training
and test, we use a publicly available code pinocchio

1 https://spherex.caltech.edu

(Monaco et al. 2013) that populates a large cosmologi-
cal volume with dark matter halos2. We conﬁgure past
light-cones of a hypothetical observer by arranging sev-
eral simulation outputs to ﬁll the volume (Figure 1). We
set the simulation box size to 690h−1 comoving Mpc and
the aperture of the light cone to 1.5 deg. The minimum
halo mass considered is 2 × 1011h−1 M(cid:12). We have con-
ﬁrmed that the presence of smaller haloes does not aﬀect
the total intensity signiﬁcantly nor the intensity distri-
bution. We carefully choose the line-of-sight direction of
the light cone so that any galaxy does not appear more
than once in the redshift range of our interest.

To assign line luminosities to the galaxies (haloes), we
use halo mass-to-line luminosity relations computed in
our previous study (Moriwaki et al. 2020) based on the
results of cosmological hydrodynamics simulation Illus-
trisTNG (Nelson et al. 2019). We assign the luminosities
by assuming that the line luminosities of haloes in a halo
mass bin Mi follow an asymmetric normal distribution
with diﬀerent variances on the larger and smaller side
than the most frequent luminosity value Li. This assign-
ing process produces similar scatter in the halo mass-to-
line luminosity relations as that of IllustrisTNG. Both
the Hα and [Oiii] line luminosities are approximately
proportional to the star formation rate, but the derived
Hα /[Oiii] ratio varies over a factor of ten because the
[Oiii] luminosity depends also on the properties of the
interstellar medium such as metallicity and ionization
parameter. We ﬁnd that the line ratios of our catalog
haloes are also scattered in a similar way as that com-
puted with IllustrisTNG.

The middle and bottom panels of Figure 1 show the
intensity distributions of Hα and [Oiii] on a past light-
cone. We adopt the spatial and spectral resolutions and
the noise levels of SPHEREx 3. The angular resolution is
0.1 arcmin and the spectral resolution is approximately
constant (R ∼ 40) over the wavelength range of our in-
terest.4 Note that the corresponding physical length to
the angular resolution is much smaller than that of the
spectral resolution. At z = 1.5, for instance, 0.1 ar-
cmin corresponds to 52 kpc, while R ∼ 40 corresponds
to 47.2 Mpc. We add Gaussian noise to make realistic
mock catalogs. The noise level is about two orders of
magnitude larger than the mean intensities of line emis-
sions (top panel of Figure 1), and thus detecting diﬀuse

2 We adopt Ωm = 0.316, ΩΛ = 0.684, and h = 0.673 Planck Col-

laboration VI (2018).

3 We use data in https://github.com/SPHEREx/Public-products
4 The spectral resolution (binning) is not always constant. For
example, there are wider bins at around 1.1 µm. We do not use
such irregular bins.

reconstruction of 3D intensity maps with ML

3

Figure 1. The intensity distribution on a past light-cone of a hypothetical observer having a 0.85 deg ﬁeld-of-view. The
observed intensity (top), Hα (middle) and [Oiii] (bottom) contributions in units of erg s−1 cm−2 sr−1. The black lines show the
spectral binning of the SPHEREx detector. The yellow and orange boxes indicate the redshift ranges of Hα and [Oiii] emitters,
whose signals originates from galaxies from z = 1.3 to 2.4. The data cube within an angular size of 6.4(cid:48) (the size of the yellow
boxes) are used for training. Note that we have adopted larger angular resolution for visibility in this ﬁgure than the actual
resolution of our training data.

sources that are distributed over the entire intensity ﬁeld
is diﬃcult even with our machine learning method.

For training, we generate 500 independent light-cones
with 1.5 deg aperture over λobs = 1.0µm − 2.5 µm us-
ing pinocchio. The wavelength range corresponds to
32 spectral bins of SPHEREx as shown in Figure 1. To
reduce computational cost, we generate input data with
64 × 64 angular pixels. This corresponds to a ﬁeld-of-
view of 6.4(cid:48) × 6.4(cid:48) with an angular resolution of 0.1 ar-
cmin. From each light cone, we randomly extract 100
such small volumes. Then a total of 50,000 mock ob-
servational data cubes are generated. As discussed in
the following section, we use two portions of the mock
observational data with diﬀerent wavelength ranges (in-
dicated by orange and yellow boxes in Figure 1) as input
to the neural networks.

2.2. Network

We use a conditional generative adversarial net-
work (cGAN; Isola et al. 2016) to perform the three-
dimensional reconstruction.
In particular, we adopt
conditional Wasserstein GAN (WGAN; Arjovsky et al.
2017). WGAN is known to increase training stability
and the diversity of generated data (Foster 2019). We
have four 3D convolutional neural networks: two gener-
ators, G1 and G2, that reconstruct Hα and [Oiii] signals

from observed data and corresponding two critics5 , D1
and D2, that distinguish true and reconstructed images.
Each generator consists of four convolution layers fol-
lowed by four de-convolution layers (Figure 2), whereas
the critic consists of four de-convolution layers. The net-
works also include skip connections (Isola et al. 2016),
dropout (Srivastava et al. 2014), and batch normaliza-
tion (Ioﬀe & Szegedy 2015).

The most important information to be learned by the
generators is the co-existence of multiple emission lines
at diﬀerent wavelengths. To make it easier for the gen-
erators to learn that the two emission lines are always
observed with a separation of ∆λobs = (λHα − λ[OIII]) ×
(1 + z), we arrange the architecture so that the genera-
tors receive a pair of observed data cubes as an input.
The cubes are covered by sixteen SPHEREx wavelengths
ﬁlters from 1.48 µm to 2.19 µm, and 1.14 µm to 1.68 µm,
which correspond to 1.25 (cid:46) z (cid:46) 2.4 of Hα and [Oiii]
lines, respectively. The input cubes, denoted by x1 and
x2, are indicated by the orange and yellow boxes in Fig-
ure 1. By giving the two data cubes arranged such that
the two emission lines from the same source appear at
the same pixel, we let the generators learn the consistent
co-existence of the two lines.

5 In WGAN, a network that works as a discriminator in vanilla

GANs is called critic.

4

Moriwaki et al.

force the constraint, we adopt the same approach as in
the original proposal by Arjovsky et al. (2017) in which
they clip the weights of the critic to lie within a small
range of [−0.01, 0.01].

We build our network using Tensorﬂow. We use Adam
optimizer (Kingma & Ba 2014) with a learning rate of
0.0002 for training, set the batch size to be 50, and run
50 epochs on a single Nvidia Titan RTX GPU.

3. RESULT

To measure the performance of our WGAN, we gen-
erate an additional set of 1000 light-cones that are inde-
pendent of the training data. We randomly choose an
area of 0.85 deg × 0.85 deg from each light-cone and di-
vide it into 8×8 cubes with the same size as the training
data. The prepared test data are given to the generator
of our WGAN. Finally, we reconstruct intensity cubes
by combining 8 × 8 outputs.

In Figure 3, we show an example of the true and
reconstructed Hα intensity distributions from z = 1.3
to 2.4. The large-scale galaxy distribution is repro-
duced accurately in 3D, despite the large noise level
(see Figure 1). Pixel-by-pixel comparison shows remark-
ably good agreement between the true and reconstructed
maps (Figure 4). Our network reconstructs the brightest
sources accurately, and thus the underlying large-scale
distribution is also well reproduced. Diﬀuse sources
are not well reproduced because of the large observa-
tional noise considered in our study. This can also be
seen in the point distribution function (Figure 5). The
bright ends are reproduced, but the WGAN appears
to have learned that it is optimal to regard faint pix-
els just as noise-dominated. The vertical lines are the
noise level of SPHEREx averaged over 16 wavelength
bins of the input data cubes, σn = 2.25 × 10−6 (upper),
3.06 × 10−6 erg/s/cm2/sr (bottom). Figure 5 indicates
that the eﬀective limit of our machine learning recon-
struction is a few-σn. This is similar to the result of
Cheng et al. (2020), who show that the CO line signals
from similar redshifts are reconstructed down to a few-
σn level. Detecting diﬀuse ”clouds” would be extremely
diﬃcult unless the observational noise is signiﬁcantly re-
duced in future experiments.
It should be noted here
that the weaker [Oiii] signals are also accurately recon-
structed, even though the bright end of the observed
PDF is dominated by foreground Hα intensities.

We count the numbers of the pixels with intensities
larger than 3-σn in true (Ntrue) and reconstructed (Nrec)
maps. We then compute the recall, NX/Ntrue, and the
precision, NX/Nrec, where NX is the number of pixels
that are detected and matched in both the true and
reconstructed maps. The recall and the precision are

Figure 2. The architecture of the generator that takes two
feature maps (data cubes) as an input and consists of four
shared convolution layers, followed by four deconvolution lay-
ers.

The critics also receive two data cubes as an input:
either a pair of the observed and reconstructed data,
(xi, Gi(x1, x2)), or a pair of observed and true data,
(xi, yi), where yi is the true data cubes of Hα (i = 1) or
[Oiii] (i = 2) that cover the same wavelength range as
xi.

The networks are trained to optimize two loss func-

tions deﬁned by

Li = Di(xi, yi) − Di(xi, Gi(x1, x2)) + λi|yi − Gi(x1, x2)|,
(1)

where the indices i = 1, 2 correspond to Hα and [Oiii],
and λi is a hyperparameter which we set λ1 = λ2 = 100
after some experiments. The objective of the genera-
tors (critics) is to decrease (increase) the loss functions.
Another important building block of WGAN is the Lip-
schitz constraint imposed on the critics, which prevents
the outputs of the critics from changing abruptly. To en-

reconstruction of 3D intensity maps with ML

5

Figure 4. Pixel-by-pixel correspondence between the true
and reconstructed intensities of Hα (top) and [Oiii] (bot-
tom). Intensities are normalized by 10−5 erg/s/cm2/sr.

Figure 3. The true (top) and reconstructed (bottom) inten-
sities of Hα line emission from z = 1.3 to 2.4. The angular
size is 0.43 deg × 0.43 deg. The intensities are smoothed for
visibility with 6 and 0.5 times the pixel size for angular and
spectral domain, respectively.

eﬃcient

r(k) =

PX (k)
(cid:112)Ptrue(k)Prec(k)

,

(2)

0.67 and 0.84 for Hα, and the corresponding values for
[Oiii] are 0.78 and 0.68. We estimate that the typical
intensities of [Oiii] are roughly half of Hα at the same
observed wavelength, and our previous study shows that
the detection performance degrades for such weaker lines
when only two-dimensional data is used for the machine
learning analysis (Moriwaki et al. 2020). The impressive
reproducibility of the [Oiii] distribution in the present
study can be attributed to the inclusion of the spectral
information, as we discuss in the following.

To quantify the reconstruction accuracy of the large-
scale distribution, we compute the cross-correlation co-

where PX is the cross-power spectrum and Ptrue and Prec
are the auto-power spectra of the true and reconstructed
maps. We ﬁnd that a high reconstruction performance
with r ∼ 0.8 at k = 0.3 arcmin−1 for both Hα and [Oiii]
has been achieved over the wide redshift range. This
is consistent with the point source detection accuracy
discussed above.

The high reproducibility of weaker [Oiii] signals sug-
gests that the [Oiii] generator refers to the Hα intensities
that are more easily reconstructed from the two inputs.
This is exactly what we expect the machine to learn, and
it is important to understand how much it depends on
the Hα intensity. To investigate the learning process fur-
ther, we generate test data with diﬀerent, uncorrelated
realizations for Hα and [Oiii] and feed to the [Oiii] gen-

6

Moriwaki et al.

To examine if the spatial clustering information is used
along with the spectral information, we perform an ad-
ditional test. We randomly shuﬄe the pixels of the test
data and get rid of the angular correlation in the sig-
nals while preserving the spectral correlation. We then
input the shuﬄed data into our network. The test re-
sult shows that the network still achieves high repro-
ducibility; the bright pixels (> 10−5 erg/s/cm2/sr) are
reproduced with similar precision of ∼ 0.6 − 0.8 for both
the lines. This implies that our network emphasizes the
spectral information (emission line features) more than
the spatial correlation information. We note that we
consider a small area of 6.4 × 6.4 arcmin2 for the re-
construction in this study. With the ﬁnest resolution
achievable for our available computational resources, we
are able to represent point sources but the particular
conﬁguration does not allow incorporating large-scale
In our previous study (Moriwaki
clustering features.
et al. 2021), we showed that the information on the
large-scale clustering is more properly used when the
training data are generated with a suﬃciently large area.
Clearly, there is room for improvement in our method.
In our future work, we will use data set with larger di-
mensions so that a machine can learn both the spectral
information and the large-scale clustering of galaxies.

4. SUMMARY

We have developed, for the ﬁrst time, neural networks
that extract signals of two emission lines from noisy data
obtained in LIM observations. Our 3D WGAN makes
use of the information on the co-existence of two emis-
sion lines in a given pair of data cubes.
It is able to
reconstruct the bright sources when trained with a large
number of mock observational maps that are closely con-
ﬁgured for the SPHEREx experiment. Our method can
be extended and applied to LIM observations at any
other wavelengths. Once we can extract the individual
signals, the reconstructed data can be used for cosmolog-
ical/astrophysical parameter estimate, cross-correlation
analysis, and planning follow-up observations.

ACKNOWLEDGEMENTS

We thank the anonymous referee for helpful sugges-
tions and constructive remarks on our manuscript. We
thank Masato Shirasaki for helping to develop the net-
works. KM is supported by JSPS KAKENHI Grant
Number 19J21379 and by JSR Fellowship. NY acknowl-
edges ﬁnancial support from JST AIP Acceleration Re-
search Grant Number JP20317829.

Figure 5. The one-point distribution function (PDF) of the
true and reconstructed maps of Hα (left) and [Oiii] (right).
The 1-σ variation of the observed (light shades), true (dark
shades) and reconstructed (error bars) PDFs over 1000 test
data are shown. The dashed vertical lines are the noise level
of SPHEREx, σn, averaged over 16 wavelength bins of the
input data cubes.

erator. The result shows that the reconstructed [Oiii]
map is biased toward the true Hα map, indicating that
the [Oiii] generator strongly relies on the input x1 that
includes the Hα signals rather than the input x2. How-
ever, the test case yields the cross-correlation coeﬃcients
between the reconstructed Hα and [Oiii] maps that are
smaller than the real case with actual Hα - [Oiii] pairs
by 0.2. This indicates that the information on the weak
[Oiii] line in the observed maps is still used to recon-
struct accurately [Oiii] intensity distributions.

Ali, Z. S., Parsons, A. R., Zheng, H., et al. 2015, ApJ, 809,

Arjovsky, M., Chintala, S., & Bottou, L. 2017, arXiv

61, doi: 10.1088/0004-637X/809/1/61

e-prints, arXiv:1701.07875.
https://arxiv.org/abs/1701.07875

REFERENCES

106105104intensity[erg/s/cm2/sr]102100102104PDFobservedtrue Hrec H106105104202106105104intensity[erg/s/cm2/sr]101101103105PDFobservedtrue OIIIrec OIII106105104202reconstruction of 3D intensity maps with ML

7

Breysse, P. C., Kovetz, E. D., Behroozi, P. S., Dai, L., &

Kamionkowski, M. 2017, MNRAS, 467, 2996,
doi: 10.1093/mnras/stx203

Breysse, P. C., Kovetz, E. D., & Kamionkowski, M. 2016,

MNRAS, 457, L127, doi: 10.1093/mnrasl/slw005

Chang, T.-C., Pen, U.-L., Bandura, K., & Peterson, J. B.

2010, Nature, 466, 463, doi: 10.1038/nature09187

Cheng, Y.-T., Chang, T.-C., Bock, J., Bradford, C. M., &

Cooray, A. 2016, ApJ, 832, 165,
doi: 10.3847/0004-637X/832/2/165

Cheng, Y.-T., Chang, T.-C., & Bock, J. J. 2020, arXiv

e-prints, arXiv:2005.05341.
https://arxiv.org/abs/2005.05341

Cleary, K. A., Borowska, J., Breysse, P. C., et al. 2021,

arXiv e-prints, arXiv:2111.05927.
https://arxiv.org/abs/2111.05927

Concerto Collaboration, Ade, P., Aravena, M., et al. 2020,

A&A, 642, A60, doi: 10.1051/0004-6361/202038456
Dor´e, O., Bock, J., Ashby, M., et al. 2014, arXiv e-prints,

arXiv:1412.4872. https://arxiv.org/abs/1412.4872
Dor´e, O., Werner, M. W., Ashby, M. L. N., et al. 2018,

arXiv e-prints, arXiv:1805.05489.
https://arxiv.org/abs/1805.05489

Dumitru, S., Kulkarni, G., Lagache, G., & Haehnelt, M. G.
2019, MNRAS, 485, 3486, doi: 10.1093/mnras/stz617
Fonseca, J., Silva, M. B., Santos, M. G., & Cooray, A.

2017, MNRAS, 464, 1948, doi: 10.1093/mnras/stw2470

Foster, D. 2019, Generative Deep Learning (O’Reilly

Media, Inc.)

Gong, Y., Silva, M., Cooray, A., & Santos, M. G. 2014,

ApJ, 785, 72, doi: 10.1088/0004-637X/785/1/72

Ioﬀe, S., & Szegedy, C. 2015, arXiv e-prints,

arXiv:1502.03167. https://arxiv.org/abs/1502.03167

Isola, P., Zhu, J., Zhou, T., & Efros, A. A. 2016, CoRR,
abs/1611.07004. https://arxiv.org/abs/1611.07004

Keating, G. K., Marrone, D. P., Bower, G. C., & Keenan,

R. P. 2020, ApJ, 901, 141,
doi: 10.3847/1538-4357/abb08e

Keating, G. K., Marrone, D. P., Bower, G. C., et al. 2016,

ApJ, 830, 34, doi: 10.3847/0004-637X/830/1/34

Kingma, D. P., & Ba, J. 2014, arXiv e-prints,

arXiv:1412.6980. https://arxiv.org/abs/1412.6980

Kovetz, E. D., Viero, M. P., Lidz, A., et al. 2017, arXiv

e-prints, arXiv:1709.09066.
https://arxiv.org/abs/1709.09066

Monaco, P., Sefusatti, E., Borgani, S., et al. 2013, MNRAS,

433, 2389, doi: 10.1093/mnras/stt907

Moriwaki, K., Filippova, N., Shirasaki, M., & Yoshida, N.

2020, MNRAS, doi: 10.1093/mnrasl/slaa088

Moriwaki, K., Shirasaki, M., & Yoshida, N. 2021, ApJL,

906, L1, doi: 10.3847/2041-8213/abd17f

Moriwaki, K., Yoshida, N., Eide, M. B., & Ciardi, B. 2019,

MNRAS, 489, 2471, doi: 10.1093/mnras/stz2308

Nelson, D., Springel, V., Pillepich, A., et al. 2019,

Computational Astrophysics and Cosmology, 6, 2,
doi: 10.1186/s40668-019-0028-x

Planck Collaboration VI. 2018, arXiv e-prints,

arXiv:1807.06209. https://arxiv.org/abs/1807.06209

Srivastava, N., Hinton, G., Krizhevsky, A., Sutskever, I., &
Salakhutdinov, R. 2014, Journal of Machine Learning
Research, 15, 1929.
http://jmlr.org/papers/v15/srivastava14a.html

Visbal, E., & Loeb, A. 2010, JCAP, 11, 016,

doi: 10.1088/1475-7516/2010/11/016

