4
1
0
2

c
e
D
4

]
L
M

.
t
a
t
s
[

1
v
6
7
5
1
.
2
1
4
1
:
v
i
X
r
a

LightLDA:BigTopicModelsonModestComputeClustersJinhuiYuan1,FeiGao1,2,QirongHo3,WeiDai4,JinliangWei4,XunZheng4,EricP.Xing4,Tie-YanLiu1,Wei-YingMa1AbstractWhenbuildinglarge-scalemachinelearning(ML)programs,suchasmassivetopicsmodelsordeepnetworkswithuptotrillionsofparametersandtrainingexamples,oneusuallyassumesthatsuchmassivetaskscanonlybeattemptedwithindustrial-sizedclusterswiththousandsofnodes,whichareoutofreachformostpractitionersoracademicresearchers.Weconsiderthischallengeinthecontextoftopicmodelingonweb-scalecorpora,andshowthatwithamodestclusterofasfewas8machines,wecantrainatopicmodelwith1milliontopicsanda1-million-wordvocabulary(foratotalof1trillionparameters),onadocumentcollectionwith200billiontokens—ascalenotyetreportedevenwiththousandsofmachines.Ourmajorcontributionsinclude:1)anew,highly-efﬁcientO(1)Metropolis-Hastingssamplingalgorithm,whoserunningcostis(surprisingly)agnosticofmodelsize,andempiricallyconvergesnearlyanorderofmagnitudemorequicklythancurrentstate-of-the-artGibbssamplers;2)astructure-awaremodel-parallelscheme,whichleveragesdependencieswithinthetopicmodel,yieldingasamplingstrategythatisfrugalonmachinememoryandnetworkcommunication;3)adifferentialdata-structureformodelstorage,whichusesseparatedatastructuresforhigh-andlow-frequencywordstoallowextremelylargemodelstoﬁtinmemory,whilemaintaininghighinferencespeed;and4)aboundedasynchronousdata-parallelscheme,whichallowsefﬁcientdistributedprocessingofmassivedataviaaparameterserver.Ourdistributionstrategyisaninstanceofthemodel-and-data-parallelprogrammingmodelunderlyingthePetuumframeworkforgeneraldistributedML,andwasimplementedontopofthePetuumopen-sourcesystem.Weprovideexperimentalevidenceshowinghowthisdevelopmentputsmassivemodelswithinreachonasmallclusterwhilestillenjoyingproportionaltimecostreductionswithincreasingclustersize,incomparisonwithalternativeoptions.Keywords:Large-scaleMachineLearning,DistributedSystems,Model-Parallelism,Data-Parallelism,TopicMod-els,Metropolis-Hastings,ParameterServer,Petuum1IntroductionTopicmodels(TM)areapopularandimportantmodernmachinelearningtechnologythathasbeenwidelyusedintextmining,networkanalysisandgenetics,andmoreotherdomains[17,4,23,25,2].Theirimpactonthetech-nologysectorandtheInternethasbeentremendous—numerouscompanieshavingdevelopedtheirlarge-scaleTMimplementations[12,1,21],withapplicationstoadvertisingandrecommendersystems.AkeygoalofcontemporaryresearchistoscaleTMs,particularlytheLatentDirichletAllocation(LDA)model[4],toweb-scalecorpora(BigData).Crucially,internet-scalecorporaaresigniﬁcantlymorecomplexthansmaller,well-curateddocumentcollec-tions,andthusrequirehigh-capacitytopicparameterspacesfeaturinguptomillionsoftopicsandvocabularywords(andhencetrillionsofparameters,i.e.BigModels),inordertocapturelong-tailsemanticinformationthatwouldotherwisebelostwhenlearningonlyafewthousandtopics[21].Toachievesuchmassivedataandmodelscales,oneapproachwouldbetoengineeradistributedsystemthatcanefﬁcientlyexecutewell-establisheddata-parallelstrategies(i.e.,splitdocumentsoverworkerswhichhaveshared1MicrosoftResearch.Email:{jiyuan,tie-yan.liu,wyma}@microsoft.com2DepartmentofComputerScience&Engineering,BeihangUniversity.Email:gf0109@gmail.com3InstituteforInfocommResearch,A*STAR,Singapore.Email:hoqirong@gmail.com4SchoolofComputerScience,CarnegieMellonUniversity.Email:{wdai,jinliangw,xunzheng,epxing}@cs.cmu.edu1 
 
 
 
 
 
accesstoalltopicparameters)forLDA[15],whileapplyingalgorithmicspeedupssuchastheSparseLDA[22]orAliasLDA[10]samplerstofurtherdecreaserunningtimes.SucheffortshaveenabledLDAmodelswith10sofbillionsofparameterstobeinferredfrombillionsofdocuments,usinguptothousandsofmachines[12,1,11,21].Whilesuchachievementsareimpressive,theyareunfortunatelycostlytorun:forinstance,aclusterof1000machineswillcostmillionsofdollarstosetup(nottomentionthefuturecostsofpowerandregularmaintenance).Alternatively,onemightrentequivalentcomputecapacityfromacloudprovider,butgiventhecurrenttypicalpriceof≥$1perhourperresearch-grademachine,asinglemonthofoperationswouldcost≥$700,000.Neitheroptionisfeasibleforthemajorityofresearchersandpractitioners,whoarelikelytohavemoremodestbudgets.RatherthaninsistingthatbigcomputingistheonlywaytosolvelargeMLproblems,whatifwecouldbringlargetopicmodelswithtrillionsofparameterstoamorecost-effectivelevel,byprovidinganimplementationthatisefﬁcientenoughformodestclusterswithatmost10sofmachines?Weapproachthisproblematthreelevels:(1)wedistributeLDAinferenceinadata-andmodel-parallelfashion:bothdataandmodelarepartitionedandthenstreamedacrossmachines,soastomakeefﬁcientuseofmemoryandnetworkresourceswithinadistributedcluster;(2)wedevelopanMetropolis-HastingssamplerwithcarefullyconstructedproposalsthatallowsforO(1)amortizedsamplingtimeperword/token,resultinginahighconvergencerate(intermsofrealtime)thatbeatsexistingstate-of-the-artsamplersbyasigniﬁcantmargin[22,10];(3)weemployadifferentialdatastructuretoleveragethefactthatthatweb-scalecorporaexhibitbothhigh-frequencypower-lawwordsaswellaslow-frequencylong-tailwords,whichcanbetreateddifferentlyinstorage,resultinginhighmemoryefﬁciencywithoutaperformancepenaltyfromahomogeneousdatastructure.Byrealizingtheseideasusingtheopen-sourcePetuumframework(www.petuum.org)[8,9,5],wehavepro-ducedacompute-and-memoryefﬁcientdistributedLDAimplementation,LightLDA,thatcanlearnanLDAmodelwith1trillionmodelparameters(1mtopicsby1mvocabularywords)frombillionsofdocuments(200billiontokens),onacomputeclusterwithasfewas8standardmachines(whoseconﬁgurationisroughlysimilartoatypicalcomputeinstancefromacloudprovider)in180hours,whichproportionallydropsto60hourson24machines.Intermsofparametersize,ourresultistwoordersofmagnitudelargerthanrecently-setLDArecordsintheliterature,whichinvolvedmodelswith10sofbillionsofparametersandtypicallyusedmassiveindustrial-scaleclusters[11,21,1,12];ourdatasizeisalsoatleastcomparableor1orderofmagnitudelargerthanthosesameworks1.Asforthroughput,oursystemisabletosample50milliondocuments(ofaveragelength200tokens)worthoflatenttopicindicatorsperhourper20-coremachine;whichcomparesfavorablytopreviouslyreportedresults:PLDA+withroughly1200documentsperhourpermachineusingthestandardcollaspedGibbssampler,andYahooLDAwithroughly2milliondocumentsperhourper8-coremachine.Overall,LightLDAbeneﬁtsbothfromahighlyefﬁcientMCMCsamplerbuiltonanewproposalscheme,andahighlyefﬁcientdistributedarchitectureandimplementationbuiltonPetuum.Itrepresentsatrulylightweightrealiza-tion(henceitsnameLightLDA)ofamassiveMLprogram,whichwehopewillbeeasilyaccessibletoordinaryusersandresearcherswithmodestresources.ComparedtousingalternativeplatformslikeSparkandGraphlabthatalsoofferhighlysophisticateddata-ormodel-parallelsystems,ordesigningbespokeground-upsolutionslikePLDAandYahooLDA,wesuggestthatourintermediateapproachthatleveragesbothsimple-but-criticalalgorithmicinnovationandlightweightML-friendlysystemplatformsstandsasahighlycost-effectivesolutiontoBigML.2ChallengesandRelatedWorkLatentDirichletAllocation(LDA)[4]isusedtorecoversemanticallycoherenttopicsfromdocumentcorpora,aswellastodiscoverthetopicproportionswithineachdocument.LDAisaprobabilisticgenerativemodelofdocu-ments,whereeachdocumentisrepresentedasamixtureoverlatenttopics,andwhereeachtopicischaracterizedbyadistributionoverwords.Toﬁndthemostplausibletopicsanddocument-topicassignments,onemustinfertheposteriordistributionoftheLDAparameters(word-topicdistributionsanddoc-topicdistributions),byusingeitheravariational-orsampling-basedinferencealgorithm.Wefocusonsampling-basedalgorithms,becausetheyyieldverysparseupdatesthatmakethemwell-suitedtosettingswithamassivenumberoftopicsK.MuchresearchhasbeeninvestedintoscalingLatentDirichletAllocation(LDA)toever-largerdataandmodel1[21]used4.5billiontokens,while[11]used5billionshortdocumentsofunspeciﬁedlength.2sizes;existingpapersusuallyshowanalgorithmicfocus(i.e.betterLDAinferencealgorithmspeed)orasystemsfocus(i.e.bettersoftwaretoexecuteLDAinferenceonadistributedcluster)—orevenbothfociatonce.Recentlarge-scaleimplementationsofLDA[12,1,21,11]demonstratethattrainingisfeasibleonbigdocumentcorpora(uptobillionsofdocs)usinglarge,industrial-scaleclusterswiththousandstotensofthousandsofCPUcores.OnereasonwhytheseimplementationsrequirelargeclustersisthattheyemployeithertheSparseLDAinferencealgorithm[22]orthe(slower)originalcollapsedGibbssamplerinferencealgorithm[6];ineithercasetheinferencealgorithmisalimitingfactortoefﬁciency.WedirectlyaddressthisbottleneckbydevelopinganewO(1)-per-tokenMetropolis-HastingssamplerthatisnearlyanorderofmagnitudefasterthanSparseLDA—whichallowsustoprocessbigcorporaonasmallerclusterinareasonableamountoftime.WenotethattherecentlydevelopedAliasLDAalgorithm[10]providesanalternativesolutiontotheSparseLDAbottleneck.However,ontheonehand,AliasLDA’scomputationalcomplexityisO(Kd),soitisnotgoodatprocessinglongerdocumentssuchaswebpages(particularlybecausethedoc-topictablesaredenseintheinitialiterations,soKdislarge);ontheotherhand,theAliasLDApaperonlydescribesasingle-machineimplementation,soitisunclearifitscaleswelltothedistributedsetting(especiallyconsideringAliasLDA’shighspacecomplexity,O(K)foreachword’saliastable).Inthispaper,wedemonstratethatourMetropolis-HastingssamplerinallaspectsconvergesfasterthanAliasLDAinthesingle-machinesetting,whichledustonotuseAliasLDA.Theabovementionedlarge-scaleLDApapersdiffersubstantiallyinthedegreetowhichtheyemploydata-parallelism(splittingdocumentsovermachines)versusmodel-parallelism(splittingtheword-topicdistributionsovermachines).Whileafulltreatmentofeachpaper’scontributionisnotpossiblewithintheconﬁnesofthisrelatedworksection,atahighlevel,YahooLDA[1]andparameter-server-basedimplementations[11]treattheword-topicdistributionsasglobally-shared,inthattheinferencealgorithmisagnostictohowtheword-topicdistributionsarephysicallylaidoutacrossmachines.Moreimportantly,theyscheduleinferencecomputationsontokentopicindicatorszdiinadocument-centricmanner,andwewouldthereforeclassifythemasdata-parallel-onlyimplementations.Asaconsequence,wedonotexpecteither[1]or[11]tohandleverylargetopicmodelswithover1trillionparameters(thelargestreportedresultwas10billionparametersin[11]).Inparticular,[11]assumesthatoncetheentirecorpushasbeendistributedtosufﬁcientlymanymachines,thelocaldocumentsoneachmachinewillonlyactivateasmallportionoftheLDAmodel,andthereforethememoryrequiredbyeachmachinewillnotbetoolarge.Consequently,theirdesigncannothandlelargetopicmodelswithoutalargecomputecluster.Ontheotherhand,PLDA+[12]andPeacock[21]additionallygrouptokentopicindicatorszdiaccordingtotheirwordwdi;thisisbeneﬁcialbecauseitreducestheproportionoftheword-topicdistributionsthatmustbeheldateachworkermachine—effectively,model-parallelismontopofdata-parallelism.Inparticular,[21]adoptedagrid-likemodel-parallelpartitioningstrategy,thatrequirescommunicatingboththetrainingdataandLDAmodeltoworkermachines(thoughthisrequiresadditionalnetworkoverheadcomparedtoourdesign).Alsonotableisthepipelineddesignin[12],whichonlyrequiresworkerstoholdasmallportionofthemodelinmemory;however,theirsystemusedanoutdated,slowGibbssampler,whileaspectsoftheirdataplacementandschedulerareunsuitableforextremelylargedataandmodels.Speciﬁcally,theirword-bundlingstrategyreliesonaninvertedindexrepresentationoftrainingdata,whichdoublesthememoryusedbydocuments(andwhichwecannotaffordsincememoryisalwaysatapremiuminlarge-scaleLDA).OurownLightLDAadoptsadifferentdata-and-model-parallelstrategytomaximizememoryandCPUefﬁciency:weslicetheword-topicdistributions(theLDAmodel)inastructure-awaremodel-parallelmanner[9,24],andweﬁxblocksofdocumentstoworkerswhiletransferringneededmodelparameterstothemviaabounded-asynchronousdata-parallelscheme[8].Thisallowedustotraina1-trillion-parameterLDAmodelover1billiondocumentsusingasfewas8machines,whilestillenjoyingproportionalspeedupswhenadditionalmachinesareavailable.3Structure-AwareModelParallelismforLDAWhentrainingLDAonweb-scalecorporawithbillionsofdocuments,using100softhousandsoftopicscansignif-icantlyimprovethelearnedmodel—onereasonbeingthatverylargecorporamaycontainnumeroussmall,nichetopics(the“longtail”),whichwouldgoundetectedwhenthemodelonlycontainsafewthousandtopics[21].How-ever,foratopicmodelwithuptoamilliontopics,theresultingword-topicdistributions(themodel)maycontaintrillionsofparameters,becauseweb-scalecorporacaneasilycontainmillionsofuniquewords.EventhoughtheLDAmodelissparseinpractice(i.e.manyoftheparametersarezero),atrillion-parametermodelisneverthelessnearlytwo3(cid:1)(cid:2)(cid:3)(cid:2)(cid:4)(cid:5)(cid:6)(cid:7)(cid:8)(cid:9)(cid:4)(cid:10)(cid:1)(cid:2)(cid:3)(cid:2)(cid:4)(cid:5)(cid:6)(cid:7)(cid:8)(cid:9)(cid:4)(cid:11)(cid:1)(cid:2)(cid:3)(cid:2)(cid:4)(cid:5)(cid:6)(cid:7)(cid:8)(cid:9)(cid:4)(cid:12)(cid:1)(cid:2)(cid:3)(cid:4)(cid:5)(cid:6)(cid:7)(cid:1)(cid:2)(cid:3)(cid:4)(cid:5)(cid:1)(cid:1)(cid:2)(cid:3)(cid:4)(cid:5)(cid:6)(cid:7)(cid:2)(cid:5)(cid:8)(cid:2)(cid:9)(cid:5)(cid:6)(cid:10)(cid:11)(cid:12)(cid:13)(cid:14)(cid:15)(cid:2)(cid:16)(cid:13)(cid:2)(cid:17)(cid:1)(cid:18)(cid:1)(cid:2)(cid:3)(cid:4)(cid:3)(cid:5)(cid:6)(cid:7)(cid:8)(cid:9)(cid:7)(cid:10)(cid:8)(cid:4)(cid:11)(cid:2)(cid:7)(cid:12)(cid:2)(cid:3)(cid:13)(cid:11)(cid:12)(cid:1)(cid:14)(cid:15)(cid:16)(cid:16)(cid:3)(cid:5)(cid:6)(cid:7)(cid:8)(cid:9)(cid:7)(cid:4)(cid:15)(cid:17)(cid:15)(cid:7)(cid:18)(cid:2)(cid:8)(cid:13)(cid:19)(cid:12)(cid:1)(cid:2)(cid:3)(cid:2)(cid:4)(cid:5)(cid:6)(cid:7)(cid:8)(cid:9)(cid:4)(cid:13)(cid:8)(cid:2)(cid:9)(cid:4)(cid:10)(cid:6)(cid:7)(cid:8)(cid:2)(cid:9)(cid:4)(cid:10)Figure1:Structure-awaremodelparallelisminLDA.Thearrowsacrossdocumentsd1,d2,...indicatethetokensamplingorder—observethatwesampletokenszassociatedwithwordsvintheword-topictable(model)sliceV1,beforemovingontotokenscorrespondingtoV2,andsoforth.Oncealldocumenttokensinthedatablockhavebeensampled,thesystemloadsthenextdatablockfromdisk.Eachdocumentissparse(withrespecttothevocabulary):shadedcellsindicatethatthedocumenthasoneormoretokenszcorrespondingtowordv,whereaswhiteindicatethatthedocumenthasnotokenscorrespondingtowordv(andarehenceskippedover).ordersofmagnitudelargerthanrecentlyreportedresults[12,1,21,11];infact,thereisevidencethatsomeexistingdistributedimplementationsmaynotscaletosuchlargemodels,duetoissueswiththewaythemodelispartitionedoverworkers—forexample,thesystemdesignmaynaivelyassumethataworker’sdocumentswillnevertouchmorethanasmallfractionofthemodel,butthishasbeenshowntobeuntrueonsomecorpora[9].Thus,inadditiontopartitioningdocumentsoverworkermachines(whichallrecentdistributedLDAimplementationsalreadyperforminsomeway[12,1,21,11]),wemustsimultaneouslypartitionthemodelinaconservativefashion,inordertoensurethattheworkersneverrunoutofmemory.Werefertothisprocessasstructure-awaremodelparallelism[9,24].Beforediscussingourmodelparallelstrategy,webrieﬂyreviewtheLDAmodeltoestablishnomenclature.LDAassumesthefollowinggenerativeprocessforeachdocumentinacorpus:•ϕk∼Dirichlet(β):Drawworddistributionsϕkforeachtopick.•θd∼Dirichlet(α):Drawtopicdistributionsθdforeachdocumentd.•nd∼Poisson(γ):Foreachdocumentd,drawitslengthnd(i.e.,thenumberoftokensitcontains).•Foreachtokeni∈{1,2,...,nd}indocumentd:–zdi∼Multinomial(θdi):Drawthetoken’stopic.–wdi∼Multinomial(ϕzdi):Drawthetoken’sword.ThestandardcollapsedGibbssamplerforLDA[6]worksasfollows:allvariablesexceptthetoken’stopicindicatorzdiareanalyticallyintegratedout,andweonlyneedtoGibbssamplezdiaccordingtop(zdi=k|rest)∝(n−dikd+αk)(n−dikw+βw)n−dik+¯β,(1)wherewisshortforwdi,¯β:=Pwβw,n−dikdisthenumberoftokensindocumentdthatareassignedtotopick(excludingtokenzdi),n−dikwisthenumberoftokenswithwordw(acrossalldocs)thatareassignedtotopick(excludingzdi),andn−dikisthenumberoftokens(acrossalldocs)assignedtotopick(excludingzdi).Toavoidcostlyrecalculation,thesecounts(alsocalled“sufﬁcientstatistics”)arecachedastables,andupdatedwheneveratokentopicindicatorzdichanges.Inparticular,thesetofallcountsnkdiscolloquiallyreferredtoasthedocument-topictable(andservesasthesufﬁcientstatisticsforθd),whilethesetofallcountsnkwisknownastheword-topictable(andformsthesufﬁcientstatisticsforϕk).Atminimum,anydistributedLDAimplementationmustpartition(1)thetokentopicindicatorszdianddoc-topictablenkd(collectivelyreferredtoasthedata),aswellas(2)theword-topictablenkw(themodel).WhenanLDA4samplerissamplingatokentopicindicatorzdi,itneedstoseerownkwdiintheword-topictable(aswellasallofdocumentd).However,naivepartitioningcanleadtosituationswheresomemachinestouchalargefractionoftheword-topictable:supposewesampledeverydocument’stokensinsequence,thentheworkerwouldneedtoseeallrowsinword-topictablecorrespondingtowordsinthedocument.UsingourfastMetropolis-Hastingssampler(describedinthefollowingsection),eachworkermachinecansamplethousandsofdocumentspersecond(assuminghundredsoftokensperdocument);furthermore,wehaveempiricallyobservedthatafewmilliondocuments(outofbillionsinourweb-scalecorpus)issufﬁcienttoactivatealmosttheentireword-topictable.Thus,thenaivesequencejustdescribedwouldrapidlyswappartsoftheword-topictable(whichcouldbeterabytesinsize)inandoutofeachworker,generatingaprohibitiveamountofnetworkcommunication.Ourstructure-awaremodelparallelapproachismeanttoresolvetheconﬂictbetweenfastLDAsamplingandlimitedlocalmemoryateachworker;itissimilarinspirittoblockcoordinatedescentalgorithms.DatablocksaregeneratedpriortorunningtheLDAsampler,andwenotethatitischeaptodeterminewhichvocabularywordsareinstantiatedbyeachblock.Thisinformationisattachedasmeta-datatotheblock.AsshowninFigure1,whenweloadadatablock(anditsmeta-data)intolocalmemory(denotedbytheredrectangle),wechooseasmallsetofwords(sayV1intheﬁgure)fromtheblock’slocalwords.Thesetofwordsissmallenoughthatthecorrespondingrowsn·,wdiinword-topictablecanbeheldintheworkermachine’slocalmemory–wecallthissetofrowsa“modelslice”.Oursystemfetchesthemodelsliceoverthenetwork,andthesampleronlysamplesthosetokensintheblockthatarecoveredbythefetchedslice;allothertokensarenottouched.Inthismanner,thesystemonlymaintainsathinmodelsliceinlocalmemory,andre-usesitforalldocumentsinthecurrentdatablock.Oncealltokenscoveredbytheslicehavebeensampled,thesystemfetchesthenextmodelsliceoverthenetwork(sayV2),andproceedstosamplethetokenscoveredbyit.Inthismanner(similartoslidingwindowsinimageprocessingortheTCP/IPprotocol),thesystemprocessesalltokensinadatablock,onesliceatatime,beforeﬁnallyloadingthenextblockfromdisk.Thisswappingofblockstoandfromdiskisessentiallyout-of-coreexecution.Inadditiontokeepingworkermemoryrequirementslow,thisstructure-awaremodelparallelismalsomitigatesnetworkcommunicationbottlenecks,inthefollowingways:(1)workersdonotmoveontothenextmodelsliceuntilalltokensassociatedwiththecurrentslicehavebeensampled,hencewedonotneedtoapplycachingandevictionstrategiestothemodel(whichcouldincuradditionalcommunication,asmodelslicesarerepeatedlyswappedinandout);(2)sincethedata-modelslicesarestaticandunchanging,wepipelinetheirloading(datablocksfromdisk,modelslicesfromacentralparameterserver)tohidenetworkcommunicationslatency.Onaﬁnalnote,wepointoutthatourstructure-awaremodelparallelstrategy“sendsthemodeltothedata”,ratherthantheconverse.Thisismotivatedbytwofactors:(1)thedata(includingtokenswdiandcorrespondingtopicindicatorszdi)ismuchlargerthanthemodel(evenwhenthemodelhas1trillionparameters);(2)asthesamplerconverges,themodelgetsincreasinglysparse(thusloweringcommunication),whilethedatasizeremainsconstant.WeobservethatotherdistributedLDAdesignshaveadopteda“senddatatomodel”strategy[21],whichiscostlyinouropinion.4FastSamplingAlgorithmforLDAAsjustdiscussed,thepurposeofstructure-awaremodel-parallelismistoenableverylarge,trillion-parameterLDAmodelstobelearnedfrombillionsofdocumentsevenonsmallclusters;furthermore,bounded-asynchronousdata-parallelschemesusingparameterserverscanalsoreducethecostofnetworksynchronizationandcommunication(asnotedinotherdistributedLDApapers;see[12,1,21,11]).However,thesecontributionsalonedonotallowhugeLDAmodelstobetrainedquickly,andthismotivatesourbiggestcontribution:anovelsamplingalgorithmforLDA,whichconvergessigniﬁcantlyfasterthanrecentalgorithmssuchasSparseLDA[22]andAliasLDA[10].Inordertoexplainouralgorithm,weﬁrstreviewthemechanicsofSparseLDAandAliasLDA.SparseLDASparseLDA[22]exploitstheobservationthat(1)mostdocumentsexhibitasmallnumberoftopics,and(2)mostwordsonlyparticipateinafewtopics.Thismanifestsassparsityinboththedoc-topicandword-topictables,whichSparseLDAexploitsbydecomposingthecollapsedGibbssamplerconditionalprobability(Eq.1)into5threeterms:p(zdi=k|rest)∝αkβwn−dik+¯β|{z}r+n−dikdβwn−dik+¯β|{z}s+n−dikw(n−dikd+αk)n−dik+¯β|{z}t.(2)WhentheGibbssamplerisclosetoconvergence,boththesecondtermsandthethirdtermtwillbecomeverysparse(becausedocumentsandwordssettleintoafewtopics).SparseLDAﬁrstsamplesoneofthethreetermsr,sort,accordingtotheirprobabilitymassessummedoverallkoutcomesThen,SparseLDAsamplesthetopickconditioneduponwhichtermr,sortwaschosen.Ifsortwaschosen,thensamplingthetopicktakesO(Kd)orO(Kw)timerespectively,whereKdisthenumberoftopicsdocumentdcontains,andKwisthenumberoftopicswordwbelongsto.TheamortizedsamplingcomplexityofSparseLDAisO(Kd+Kw),asopposedtoO(K)forthestandardcollapsedGibbssampler.AliasLDAAliasLDA[10]proposesanalternativedecompositiontotheGibbssamplingprobability:p(zdi=k|rest)∝n−dikd(n−dikw+βw)n−dik+¯β|{z}u+αk(nkw+βw)nk+¯β|{z}v.(3)AliasLDApre-computesanaliastable[20]forthesecondterm,whichallowsittobesampledinO(1)timeviaMetropolis-Hastings.Byre-usingthetableovermanytokens,theO(K)costofbuildingthetableisalsoamortizedtoO(1)pertoken.Theﬁrsttermuissparse(linearinKd,thecurrentnumberoftopicsindocumentd),andcanbecomputedinO(Kd)time.4.1Metropolis-HastingssamplingWehavejustseenthatSparseLDAandAliasLDAachieveO(Kd+Kw)andO(Kd)amortizedsamplingtimepertoken,respectively.Suchacceleratedsamplingisimportant,becausewesimplycannotaffordtosampletokentopicindicatorszdinaively;theoriginalcollapsedGibbssampler(Eq.1)requiresO(K)computationpertoken,whichisclearlyintractableatK=1milliontopics.SparseLDAreducesthesamplingcomplexitybyexploitingthesparsityofproblem,whileAliasLDAharnessesthealiasapproachtogetherwiththeMetropolis-Hastingsalgorithm[14,7,19,3].OurLightLDAsampleralsoturnstoMetropolis-Hastings,butwithnewinsightsintoproposaldistributiondesign,whichismostcrucialforhighperformance.Weshowthatthesamplingprocesscanbeacceleratedevenfurtherwithawell-designedproposaldistributionq(·)tothetrueLDAposteriorp(·).Awell-designedproposalq(·)shouldspeedupthesamplingprocessintwoways:(1)drawingsamplesfromq(·)willbemuchcheaperthandrawingsamplesfromp(·);(2)theMarkovchainshouldmixquickly(i.e.requiresonlyafewsteps).Whatarethetrade-offsinvolvedinconstructingagoodproposaldistributionq(·)forp(·)?Ifq(·)isverysimilartop(·),thentheconstructedMarkovchainwillmixquickly—however,thecostofsamplingfromq(·)mightendupasexpensiveassamplingfromp(·)itself.Onthecontrary,ifq(·)isverydifferentfromp(·),wemightbeabletosamplefromitcheaply—buttheconstructedMarkovchainmaymixtooslowly,andrequiremanystepsforconvergence.Tounderstandthistrade-off,considerthefollowingextremes:UniformDistributionProposalSupposewechooseq(·)tobetheuniformdistribution.TheMHalgorithmwillproposethenextstatet∼Unif(1,...,K),andaccepttheproposedstatewithprobabilitymin{1,p(t)p(s)}.Obviously,samplingfromauniformdistributionisverycheapandcanbedoneinO(1)time;however,theuniformdistributionisnon-sparseandthereforeextremelyfarfromp(·),thusitneedsmanyMHstepstomix.FullConditionalDistributionProposalWecouldinsteadchoosep(·)itselfastheproposaldistributionq(·).TheMHalgorithmproposesthenextsteptwithprobabilityp(t),andacceptsitwithprobabilitymin{1,p(t)p(s)p(s)p(t)}=1;i.e.thealgorithmacceptsallproposals.Samplingfromq(·)obviouslycostsasmuchasp(·),butmixingisveryfastbecauseallproposalsareaccepted.6(cid:1)(cid:2)(cid:3)(cid:4)(cid:0)(cid:5)(cid:6)(cid:0)(cid:7)(cid:8)(cid:6)(cid:1)(cid:2)(cid:1)(cid:3)(cid:4)(cid:0)(cid:5)(cid:6)(cid:0)(cid:7)(cid:8)(cid:6)(cid:2)(cid:1)(cid:2)(cid:1)(cid:3)(cid:4)(cid:0)(cid:5)(cid:6)(cid:0)(cid:7)(cid:8)(cid:6)(cid:2)(cid:2)(cid:1)(cid:1)(cid:2)(cid:4)(cid:3)(cid:0)(cid:5)(cid:6)(cid:0)(cid:7)(cid:8)(cid:6)Figure2:Anexampleshowinghowtobuildanaliastable.Thisproceduretransformsanon-uniformsamplingproblemintoauniformsamplingone.Thealiastablemaintainsthemassofeachbinandcanbere-usedonceconstructed.Moredetailsaboutthealiasmethodcanbefoundin[13].(cid:1)(cid:2)(cid:1)(cid:3)(cid:2)(cid:2)(cid:3)(cid:2)(cid:3)(cid:4)(cid:5)(cid:6)(cid:7)(cid:4)(cid:8)(cid:4)(cid:8)(cid:9)(cid:5)(cid:10)(cid:11)(cid:12)(cid:9)(cid:10)(cid:7)(cid:8)(cid:5)(cid:11)(cid:9)(cid:10)(cid:12)(cid:13)(cid:6)(cid:1)(cid:8)(cid:2)(cid:11)(cid:3)(cid:5)(cid:9)(cid:10)(cid:11)(cid:12)(cid:13)(cid:10)(cid:14)(cid:15)(cid:11)(cid:13)(cid:16)(cid:17)(cid:18)(cid:19)(cid:9)(cid:10)(cid:11)(cid:20)(cid:21)(cid:19)(cid:22)(cid:13)(cid:19)(cid:23)(cid:20)(cid:15)(cid:24)(cid:16)(cid:18)(cid:19)(cid:22)(cid:13)Figure3:Illustrationofhowwesamplethedoc-proposalinO(1)time,withouthavingtoconstructanaliastable.4.2CheapProposalsbyFactorizationTodesignanMHalgorithmthatischeaptodrawfrom,yethashighmixingrate,weadoptafactorizedstrategy:insteadofasingleproposal,weshallconstructasetofO(1)proposals,andalternatebetweenthem.Toconstructtheseproposals,letusbeginfromthetrueconditionalprobabilityoftokentopicindicatorzdi:p(k)=p(zdi=k|rest)∝(n−dikd+αk)(n−dikw+βw)n−dik+¯β.(4)Observethatitcanbedecomposedintotwoterms:q(zdi=k|rest)∝(nkd+αk)|{z}doc−proposal×nkw+βwnk+¯β|{z}word−proposal.(5)Evenifweexploitsparsityinbothterms,samplingfromthisconditionalprobabilitycostsatleastO(min(Kd,Kw))—canwedobetter?Weobservethattheﬁrsttermisdocument-dependentbutword-independent,whilethesecondtermisdoc-independentbutword-dependent.Furthermore,itisintuitivetoseethatthemostprobabletopicsarethosewithhighprobabilitymassfromboththedoc-dependenttermandtheword-dependentterm;hence,eithertermalonecanserveasagoodproposalq—becauseifphashighprobabilitymassontopick,theneithertermwillalsohavehighprobabilitymassonk(thoughtheconverseisnottrue).Justasimportantly,thealiasmethod[13](alsousedinAliasLDA[10])canbeappliedtobothterms,reducingthecostofsamplingfromeitherproposaltojustO(1)amortizedtimepertoken(wherethecostofconstructingthealiastableisgettingamortized).Wenowdiscusstheproposalsindividually.Word-ProposalforMetropolis-HastingsDeﬁnepwastheword-proposaldistributionpw(k)∝nkw+βwnk+¯β.(6)7Theacceptanceprobabilityofstatetransitions→tismin{1,p(t)pw(s)p(s)pw(t)}.(7)Letπw:=p(t)pw(s)p(s)pw(t),wecanshowthatπw=(n−ditd+αt)(n−ditw+βw)(n−dis+¯β)(nsw+βw)(nt+¯β)(n−disd+αs)(n−disw+βw)(n−dit+¯β)(ntw+βw)(ns+¯β).(8)Oncet∼pw(t)issampled,theacceptanceprobabilitycanbecomputedinO(1)time,aslongaswekeeptrackofallsufﬁcientstatisticsn·duringsampling.Intuitively,πwishigh(relativetotopics)whenevertheproposedtopictiseither(1)popularwithindocumentd,or(2)popularforthewordw.Sincetheword-proposaltendstoproposetopicstwhicharepopularforwordw,usingtheword-proposalwillhaveexplorethestatespaceofp(k).TosamplefrompwinO(1),weusealiastablesimilarto[10].AsillustratedbyFigure2,thebasicideaofthealiasapproachistotransformanon-uniformdistributionintoauniformdistribution(i.e.,aliastable).Sincethealiastablewillbere-usedinMHsampling,thetransformationcostgetsamortizedtoO(1)2.AlthoughthealiasapproachhaslowO(1)amortizedtimecomplexity,itsspacecomplexityisstillveryhigh,becausethealiastableforeachword’sproposaldistributionstores2Kvalues:thesplittingpointofeachbinandthealiasvalueabovethatsplittingpoint;thisbecomesprohibitiveifweneedtostorealotofwords’aliastables.Ourinsighthereisthatthealiastablecanbesparsiﬁed;speciﬁcally,webeginbydecomposingpw=nkwnk+β+βwnk+β.Wethendrawoneofthetwoterms,withprobabilitygivenbytheirmasses(thisisknownasamixtureapproach3).Ifwedrawtheﬁrstterm,weuseapre-constructedaliastable(createdfromnkw,speciﬁctowordw)topickatopic,whichissparse.Ifwedrawthesecondterm,wealsouseapre-constructedaliastable(createdfromnk,commontoallwordswandthusamortizedoverallVwords)topickatopic,whichisdense.Inthisway,wereduceboththetimeandspacecomplexityofbuildingwordw’saliastabletoO(Kw)(thenumberoftopicswordwparticipatesin).Doc-ProposalforMetropolisHastingsDeﬁnepdasthedoc-proposaldistributionpd(k)∝nkd+αk.(9)Theacceptanceprobabilityofstatetransitions→tismin{1,p(t)pd(s)p(s)pd(t)}.(10)Letπd:=p(t)pd(s)p(s)pd(t),wecanshowthatπd=(n−ditd+αt)(n−ditw+βw)(n−dis+¯β)(nsd+αs)(n−disd+αs)(n−disw+βw)(n−dit+¯β)(ntd+αt).(11)Aswiththeword-proposal,weseethatthedoc-proposalacceptswhenevertopictispopular(relativetotopics)withindocumentd,orpopularforwordw.Wedecomposepd(k)∝nkdnd+α+αknd+αjustliketheword-proposal,exceptthatwhenwepicktheﬁrstterm,wedonotevenneedtoexplicitlybuildthealiastable—thisisbecausethedocumenttokentopicindicatorszdiserveasanaliastable.Speciﬁcally,theﬁrsttermnkdcountsthenumberoftimestopickoccursindocumentd,inotherwordsnkd=ndXi=1[zdi=k],(12)2ThisstrategykeepstheMetropolis-Hastingsproposalpwﬁxedovermultipledocuments,ratherthanchangingitaftereverytoken.Thisiswell-justiﬁed,sinceMetropolis-Hastingsallowsanyproposal(uptosomeconditions)providedtheacceptanceprobabilitycanbecorrectlycomputed.WehavealreadyarguedinSection4.2thattheacceptanceprobabilitycanbecomputedinO(1)timebysimplykeepingtrackofafewsufﬁcientstatisticsn·.3ThismixturestrategywasalsousedbySparseLDA[22],butonthetrueGibbssamplingprobabilityratherthananMHproposal.8(cid:25)(cid:26)(cid:27)(cid:28)(cid:29)(cid:30)(cid:31) !!!(cid:25)(cid:26)(cid:27)(cid:28)(cid:29)(cid:30)(cid:31) (cid:25)(cid:26)(cid:27)(cid:28)(cid:29)(cid:30)(cid:31) (cid:1)(cid:2)(cid:3)(cid:4)(cid:5)(cid:6)(cid:7)(cid:8)(cid:9)(cid:10)(cid:1)(cid:10)(cid:7)(cid:8)(cid:11)(cid:12)(cid:5)(cid:9)(cid:10)(cid:13)(cid:1)(cid:2)(cid:10)(cid:14)(cid:3)(cid:1)(cid:10)(cid:7)(cid:8)(cid:15)(cid:7)(cid:2)(cid:9)(cid:16)(cid:17)(cid:2)(cid:7)(cid:17)(cid:7)(cid:13)(cid:11)(cid:12)(cid:5)(cid:9)(cid:10)(cid:13)(cid:1)(cid:2)(cid:10)(cid:14)(cid:3)(cid:1)(cid:10)(cid:7)(cid:8)(cid:9)(cid:7)(cid:6)(cid:16)(cid:17)(cid:2)(cid:7)(cid:17)(cid:7)(cid:13)(cid:11)(cid:12)(cid:5)(cid:9)(cid:10)(cid:13)(cid:1)(cid:2)(cid:10)(cid:14)(cid:3)(cid:1)(cid:10)(cid:7)(cid:8)Figure4:Anexamplethatexplainswhycyclingtwoproposalshelpsmixingrate.The2ndbinisamodeofp(k),andpd(k)isobviouslynotagoodproposalforthismode—butpw(k)isgoodatexploringit.where[·]istheindicatorfunction.Thisimpliesthatthearrayzdiisanaliastablefortheunnormalizedprobabilitydistributionnkd,andthereforewecansamplefromnkdbysimplydrawinganintegerjuniformlyfrom{1,2,...,nd},andsettingzdi=zdj.Figure3usesatoyexampletoillustratethisprocedure.Hence,weconcludethatthedoc-proposalcanbesampledinO(1)non-amortizedtime(becausewedonotneedtoconstructanaliastable)4.4.3CombiningProposalstoImproveMixingWhileeitherthedoc-orword-proposalalonecanbeusedasanasefﬁcientMHalgorithmforLDA,inpracticemanyMH-steps(repeatedlysamplingeachtoken)arerequiredtoproducepropermixing.WithasmallnumberofMH-steps,usingtheword-proposalaloneencouragessparsityinword-topicdistribution(i.e.eachwordbelongstofewtopics)butcauseslowsparsityindocument-topicdistributions(i.e.eachdocumentcontainsmanytopics).Conversely,usingthedoc-proposalalonewithfewMH-stepsleadstosparsityindocument-topicdistributionbutnon-sparseword-topicdistributions.Therefore,whileeitherproposalcansampletokensveryquickly,theyneedmanyMH-stepstomixwell.ThekeytofastMetropolis-Hastingsmixingisaproposaldistributionthatcanquicklyexplorethestatespace,andreachallstateswithhighprobability(themodes).Theword-proposalpw(k)isgoodatproposingonlyitsownmodes(resultinginconcentrationofwordsinafewtopics),andlikewiseforthedoc-proposalpd(k)(resultinginconcentrationofdocsontoafewtopics).AsFigure4shows,withtheword-proposalordoc-proposalalone,somemodeswillneverbeexploredquickly.Howcanweachieveabettermixingratewhilestillmaintaininghighsamplingefﬁciency?Ifwelookatp(k)∝pw(k)×pd(k),weseethatforp(k)tobehigh(i.e.amode),weneedeitherpw(k)orpd(k)tobesufﬁcientlylarge—butnotnecessarilybothatthesametime.Hence,oursolutionistocombinethedoc-proposalandword-proposalintoa“cycleproposal”pc(k)∝pd(k)pw(k),(13)whereweconstructanMHsequenceforeachtokenbyalternatingbetweendoc-andword-proposal.Theresultsof[19]showthatsuchcycleMHproposalsaretheoreticallyguaranteedtoconverge.Bycombiningthetwoproposalsinthismanner,allmodesinp(k)willbeproposed,withsufﬁcientlyhighprobability,byatleastoneoftheproposals.Anotherpotentialbeneﬁtofcyclingdifferentproposalsisthatithelpstoreducetheauto-correlationamongsampledstates,thusexploringthestatespacemorequickly.5HybridDataStructuresforPower-LawWordsEvenwithcarefuldata-modelpartitioning,memorysizeremainsacriticalobstaclewhenscalingLDAtoverylargenumbersoftopics.TheLDAmodel,orword-topictablenkw,isaV×Kmatrix,andanaivedenserepresentationwouldrequireprohibitiveamountsofmemory—forexample,forV=K=1millionusedinthispaper’sexperiments,themodelwouldbe4terabytesinsizeassuming32-bitintegerentries.Evenwithreasonablywell-equippedmachines4Unliketheword-proposal,pdchangesaftereverytokentoreﬂectthecurrentstateofzdi.Again,thisisﬁneunderMetropolis-Hastingstheory.9100101102103104105106107word id103104105106107108109101010111012term frequencystop wordshot wordslong-tail wordsPower Law Phenomenon of Term FrequencyFigure5:Wordfrequenciesintopicmodelingfollowapower-lawphenomenon(log-logplot).Thegreatdifferenceinthefrequencyofhotwordsversuslong-tailedwordsmakesselectingtherightdatastructuredifﬁcult,asdiscussedinthetext.Thisplotisobtainedfrom15billionwebpages,withoverthan3000billiontokens.with128gigabytesofRAM,juststoringthematrixinmemorywouldrequire32machines—andinpractice,theactualusageisoftenmuchhigherduetoothersystemoverheads(e.g.cache,aliastables,buffers,parameterserver).Acommonsolutionistoturntosparsedatastructuressuchashashmaps.Therationalebehindsparsestorageisthatdocumentwordsfollowapowerlawdistribution(Figure5).Therearetwoimplications:(1)afterremovingstopwords,thetermfrequencyofalmostallmeaningfulwordswillnotexceedtheupperrangeofa32-bitinteger(2,147,483,647);thiswasmeasuredonaweb-scalecorpuswith15billionwebpagesandover3000billiontokens,andonly300words’termfrequenciesexceedthe32-bitlimit.Forthisreason,wechoosetouse32-bitintegersratherthan64-bitones.(2)Evenwithseveralbilliondocuments,itturnsoutthemajorityofwordsoccurfewerthanKtimes(whereKisthenumberoftopics,upto1millioninourexperiments).Thisnecessarilymeansthatmostrowsnk,·intheword-topictableareextremelysparse,soasparserowrepresentation(hashmaps)willsigniﬁcantlyreducethememoryfootprint.However,comparedtodensearrays,sparsedatastructuresexhibitpoorrandomaccessperformance,whichhurtsMCMCalgorithmslikeSparseLDA,AliasLDAandourMetropolis-Hastingsalgorithmbecausetheyallrelyheavilyonrandommemoryreferences.Inourexperiments,usingpurehashmapsresultedinaseveral-foldperformancelosscomparedtodensearrays.Howcanweenjoylowmemoryusagewhilstmaintaininghighsamplingthroughput?Oursolutionisahybriddatastructure,inwhichword-topictablerowscorrespondingtofrequenthotwordsarestoredasdensearrays,whileuncommon,long-tailwordsarestoredasopen-addressing/quadratic-probinghashtables.Inourweb-scalecorpuswithseveralbilliondocuments,wefoundthat10%ofthevocabularywordsare“hot”andcoveralmost95%ofalltokensinthecorpus,whiletheremaining90%ofvocabularywordsarelong-tailwordsthatcoveronly5%ofthetokens.Thisimpliesthat(1)mostaccessestoourhybridword-topictablegotodensearrays,whichkeepsthroughputhigh;(2)mostrowsoftheword-topictablearestillsparsehashtables5,whichkeepsmemoryusagereasonablylow.InourV=K=1millionexperiments,ourhybridword-topictableused0.7TB,downfrom4TBifwehadusedpurelydensearrays.Whenthistableisdistributedacross24machines,only30GBpermachineisrequired,freeingupvaluablememoryforothersystemcomponents.6SystemImplementationDistributedimplementationsareclearlydesirableforweb-scaledata:theyreducetrainingtimetorealisticlevels,andmostpractitionershaveaccesstoatleastasmalldistributedcluster.However,existingdistributedLDAimplementa-tionshaveonlybeenshowntoworkatmuchsmallerproblemscales(particularlymodelsize),orsuggesttheuseofextremelylargecomputeclusters(sometimesnumberinginthethousandsofmachines)toﬁnishtraininginacceptabletime.WhatarethechallengesinvolvedinsolvingbigLDAproblemsonjusttensofmachines?Ifwewanttotrainonacorpuswithbillionsofdocuments(eachwithatleasthundredsoftokens)occupyingterabytesofspace,thenonthedata-parallelside,simplycopyingthedatafromdisktomemorywilltaketensofhours,whiletransferringthe5Inordertofurtherimprovethethroughputofthelongtailwords,wesetthecapacityofeachhashtabletoatleasttwotimesthetermfrequencyofalong-tailword.Thisguaranteesaloadfactorthatis≤0.5,thuskeepingrandomaccessperformancehigh.10"#$%&’()*+,-()*(cid:1)(cid:2)(cid:3)(cid:4)(cid:5)(cid:6)(cid:7)(cid:8)(cid:9)(cid:10)(cid:6)(cid:5)(cid:11)(cid:12)(cid:4)(cid:13)(cid:14)(cid:15)(cid:16)(cid:17)(cid:9)(cid:18)(cid:9).#/0%&1(cid:13)(cid:8)(cid:19)(cid:6)(cid:20)(cid:8)(cid:4)(cid:13)(cid:14)(cid:15)(cid:4)(cid:6)(cid:21)(cid:4)(cid:2)(cid:11)(cid:7)(cid:11)(cid:19)(cid:8)(cid:20)(cid:8)(cid:7)(cid:4)(cid:22)(cid:8)(cid:7)(cid:23)(cid:8)(cid:7)(cid:9)(cid:1)(cid:2)(cid:3)(cid:4)(cid:5)(cid:6)(cid:7)(cid:8)(cid:9)(cid:10)(cid:6)(cid:5)(cid:11)(cid:12)(cid:4)(cid:13)(cid:14)(cid:15)(cid:16)(cid:17)(cid:9)(cid:18)(cid:9)2%&1#&342%&1#&3567/7"%8#9:%78;#&*<*/#=/67/7>7?@<=AB/%&#CD87/#867/7B7$D9#&EE;)997$%8#9*9<?#;)*@$%8#9)D87/#*Figure6:Systemarchitecture,data/modelplacementandlogicalﬂow.dataoverthenetworkalsotakesasimilaramountoftime.Onthemodel-parallelside,storing1trillionparameters(1millionwordsby1milliontopics)cantakeuptoterabytesofmemory—necessitatingdistributedstorage,whichinturnrequiresinter-machineparametersynchronization,andthushighnetworkcommunicationcost.Inlightoftheseconsiderations,ourgoalistodesignanarchitectureforLightLDAthatreducesthesedatatransmissionandparametercommunicationcostsasmuchaspossible,thusmakingexecutiononsmallclustersrealistic.SystemOverviewWebuildLightLDAontopofanopen-sourcedistributedmachinelearningsystemPetuum(www.petuum.org),whichprovidesageneralframeworkforstructure-awaremodelparallelismandbounded-asynchronousdata-parallelisminlarge-scalemachinelearning.Intermsofcode,wespeciﬁcallymakeuseofitsparameterserver[18,8,11]forbounded-asynchronousdata-parallelism.Aparameterserverhidesthedistributednetworkingdetails(suchascommunicationandconcurrencycontrol)fromusers,andprovideselegantAPIsforthedevelopmentofdistributedmachinelearningprograms—theideabeingtoletMachineLearningexpertsfocusonalgorithmiclogicratherthanthedeepsystemdetails.Weﬁrstintroducethegeneralparameterserveridea,andthendescribeoursubstantialenhancementstomakebigLDAmodelspossibleonsmallclusters.ParameterServerandDataPlacementAtthebasiclevel,aparameterserver(PS)presentsadistributedsharedmemoryinterface[16],whereprogrammerscanaccessanyparameterfromanymachine,agnostictothephysicallocationoftheparameter.Essentially,thePSextendsthememoryhierarchyofasinglemachine(Figure6);storagemediaclosertotheCPUcoreshasloweraccesslatencyandhighertransmissionbandwidth,buthasmuchsmallercapacity.InthePSarchitecture,eachmachine’sRAMissplitintotwoparts:localRAMforclientusageandremoteRAMforcentralizedparameterstorage(referredtoasthe“server”part).Thesehardwarelimitations,togetherwiththerequirementsimposedbybigtopicmodeldatamodel,stronglyinﬂuencethemannerinwhichwerunourMetropolis-Hastingsalgorithm.WeusethePStostoretwotypesofLDAmodelparameters:theword-topictable{nkv}K,Vk=1,v=1,whichcountsthenumberoftokenswithwordvassignedtotopick,andalength-K“summaryrow”{nk}Kk=1whichcountsthetotalnumberoftokensassignedtotopick(regardlessofword).32-bitintegersareusedfortheword-topictable(usingacombinationofdensearraysandsparsehashmaps;seeSection5),anda64-bitintegerarrayforthesummaryrow.WeobservethatastheLDAsamplerprogresses,theword-topictablebecomesincreasinglysparse,leadingtolowernetworkcommunicationcostsastimepasses.FurthermorePetuumPSsupportsabounded-asynchronousconsistencymodel[8],whichreducesinter-iterationparametersynchronizationtimesthroughastalenessparameters—forLightLDA,whichisalreadyaheavily-pipelineddesign,wefoundtheoptimalvaluetobes=1.Giventhattheinputdataaremuchlargerthanthemodel(andremainunchangedthroughoutLDAinference),itisunwisetoexchangedataoverthenetwork.Instead,weshufﬂeandshardthecorpusacrossthedisksofallworkermachines,andeachworkermachineonlyeveraccessesthedatainitslocaldisk.InFigure6,{wdi,zdi}Dn,ndd=1,i=1indicatesashardoftrainingdatainthen-thworkermachine,whereDnrepresentsthenumberofdocumentsinthen-thworker,ndindicatesthenumberoftokensindocumentd.Eachworker’slocalmemoryholds(1)theactiveworking11(cid:1)(cid:2)(cid:3)(cid:4)(cid:5)(cid:6)(cid:7)(cid:8)(cid:9)(cid:10)(cid:6)(cid:11)(cid:12)(cid:13)(cid:11)(cid:7)(cid:14)(cid:5)(cid:9)(cid:15)(cid:10)(cid:16)(cid:17)(cid:13)(cid:3)(cid:6)(cid:1)(cid:2)(cid:3)(cid:3)(cid:4)(cid:5)(cid:3)(cid:1)(cid:2)(cid:3)(cid:4)(cid:5)(cid:6)(cid:7)(cid:2)(cid:8)(cid:9)(cid:5)(cid:10)(cid:1)(cid:2)(cid:3)(cid:4)(cid:5)(cid:6)(cid:7)(cid:8)(cid:9)(cid:10)(cid:6)(cid:11)(cid:12)(cid:13)(cid:11)(cid:7)(cid:14)(cid:5)(cid:9)(cid:15)(cid:10)(cid:18)(cid:6)(cid:2)(cid:7)(cid:8)(cid:9)(cid:5)(cid:3)(cid:10)(cid:11)(cid:12)(cid:3)(cid:5)(cid:2)(cid:13)(cid:14)(cid:15)(cid:2)(cid:16)(cid:2)(cid:10)(cid:11)(cid:12)(cid:3)(cid:5)(cid:2)(cid:13)(cid:11)(cid:12)(cid:2)(cid:13)(cid:14)(cid:5)(cid:6)(cid:7)(cid:2)(cid:8)(cid:9)(cid:5)(cid:15)(cid:1)(cid:2)(cid:3)(cid:4)(cid:5)(cid:6)(cid:7)(cid:2)(cid:8)(cid:9)(cid:5)(cid:16)(cid:1)(cid:2)(cid:3)(cid:3)(cid:4)(cid:5)(cid:3)(a)Datablockpipeline(cid:1)(cid:2)(cid:3)(cid:4)(cid:5)(cid:6)(cid:7)(cid:8)(cid:9)(cid:10)(cid:6)(cid:11)(cid:12)(cid:13)(cid:11)(cid:7)(cid:12)(cid:5)(cid:13)(cid:14)(cid:6)(cid:7)(cid:15)(cid:16)(cid:13)(cid:3)(cid:6)(cid:1)(cid:2)(cid:3)(cid:3)(cid:4)(cid:5)(cid:3)(cid:1)(cid:2)(cid:3)(cid:3)(cid:4)(cid:5)(cid:3)(cid:1)(cid:2)(cid:3)(cid:3)(cid:4)(cid:5)(cid:3)(cid:6)(cid:7)(cid:8)(cid:4)(cid:9)(cid:1)(cid:2)(cid:3)(cid:4)(cid:5)(cid:6)(cid:7)(cid:8)(cid:9)(cid:10)(cid:6)(cid:11)(cid:12)(cid:13)(cid:11)(cid:7)(cid:12)(cid:5)(cid:13)(cid:14)(cid:6)(cid:7)(cid:17)(cid:1)(cid:2)(cid:3)(cid:3)(cid:4)(cid:5)(cid:3)(cid:6)(cid:7)(cid:8)(cid:4)(cid:10)(cid:1)(cid:2)(cid:5)(cid:11)(cid:4)(cid:2)(cid:12)(cid:13)(cid:14)(cid:15)(cid:8)(cid:5)(cid:4)(cid:16)(cid:17)(cid:4)(cid:5)(cid:3)(cid:6)(cid:7)(cid:8)(cid:4)(cid:18)(cid:6)(cid:2)(cid:7)(cid:8)(cid:9)(cid:5)(cid:3)(cid:10)(cid:11)(cid:12)(cid:3)(cid:5)(cid:2)(cid:13)(cid:14)(cid:15)(cid:16)(cid:13)(cid:5)(cid:9)(cid:10)(cid:11)(cid:12)(cid:3)(cid:5)(cid:2)(cid:13)(cid:17)(cid:8)(cid:13)(cid:2)(cid:18)(cid:5)(cid:11)(cid:12)(cid:3)(cid:5)(cid:2)(cid:13)(b)ModelslicepipelineFigure7:Pipelinestooverlapcomputation,diskI/Oandnetwork.setofdata{wdi,zdi}DSn,ndd=1,i=1,and(2)themodel{nkv}K,VSk=1,v=1requiredtosamplethecurrentsetoftokens(usingtheMetropolis-Hastingssampler).Duringsampling,weupdatethetokentopicindicatorszdi,andtheword-topictable.Thetoken-topicpairs((wdi,zdi)arelocaltotheworkermachineandincurnonetworkcommunication,whiletheword-topictableisstoredinthePSthereforerequiresabackgroundthreadforefﬁcientcommunication.TokenandTopicIndicatorStorageAspartofdata-parallelexecution,eachworkermachinestoresashardofthecorpusonitslocaldisk.Forweb-scalecorpora,eachshardmaystillbeverylarge—hundredsofgigabytes,ifnotseveralterabytes—whichprohibitsloadingtheentireshardintomemory.Thus,wefurtherspliteachdatashardintodatablocks,andstreamtheblocksoneatatimeintomemory(Figure1left).Intermsofdatastructures,wedeliberatelyplacetokenswdiandtheirtopicindicatorszdiside-by-side,asavectorof(wdi,zdi)pairsratherthantwoseparatevectorsfortokensandtopicindicators(whichwasdonein[1]).WedothistoimprovedatalocalityandCPUcacheefﬁciency:wheneverweaccessatokenwdi,wealwaysneedtoaccessitstopicindicatorzdi,andthevector-of-pairsdesigndirectlyimproveslocality.OnedrawbacktothisdesignisextradiskI/O,fromreading/writingthe(read-only)tokenswditodiskeverytimeadatashardgetsswappedout.However,diskI/Ocanalwaysbemaskedviapipelinedreads/writes,doneinthebackgroundwhilethesamplerisprocessingthecurrentshard.Wepointoutthatourstreaminganddisk-swapping(out-of-core)designnaturallyfacilitatesfaulttoleranceinthefollowingway:ifweperformdataswappingtodiskviaatomicﬁleoverwrites,thenwheneverthesystemfails,itcansimplyresumetrainingviawarm-start:readtheswapped-to-diskmodel,re-initializetheword-topicanddoc-topictables,andcarryon.Incontrast,forLDAsystemslikePLDA+[12]andYahooLDA[1]tohavefaultrecovery,theywouldrequireperiodicdumpsofthedataand/ormodel—butthisincursanontrivialcostinthebigdata/modelscenariosthatwearetryingtoaddress.TuningtheStructure-AwareModelParallelizationInSection3,weintroducedthehigh-levelideaofstructure-awaremodelparallelizationasappliedtoLDA,andtherearestillanumberofimprovementsthatcanbeemployedtoimproveitsefﬁciency.Wepresentthemostnotableones:1.Aftercompletingadatablockoramodelslice,aworkermachine’sCPUcoresneedtowaitforthenextdatablock/modelslicetobeloadedfromdisk/networkrespectively.WeeliminatethisI/Olatencyviapipelining(Figure7),thoughwecautionthatperfectpipeliningrequirescarefulparameterconﬁguration(takingintocon-siderationthethroughputofsamplers,sizeofdatablocks,sizeofmodelslices).2.Topreventdataloadimbalancesacrossmodelslices,wegeneratemodelslicesbysortingthevocabularybywordfrequencies,andthenshufﬂingthewords.Inthismanner,eachslicewillcontainbothhotwordsandlongtailwords,improvingloadbalance.3.Toeliminateunnecessarydatatraversal,whengeneratingdatablocks,wesorttoken-topicpairs(wdi,zdi)ac-cordingtowdi’spositionintheshufﬂedvocabulary,ensuringthatalltokensbelongingtothesamemodelsliceareactuallycontiguousinthedatablock(seeFigure1).Thissortingonlyneedstobeperformedonce,andisveryfastondataprocessingplatformslikeHadoop(comparedtotheLDAsamplingtime).Wearguethatthisismoreefﬁcientthanthe“wordbundle”approachinPLDA+[12],whichusesaninvertedindextoavoiddatatraversal,butatthecostofdoublingdatamemoryrequirements.12DATASETVLDL/VL/DNYTIMES10163699542125299752979332PUBMED1410437378690838200000523190BINGWEBC1000000200B1.2B200000167Table1:Experimentaldatasetsandtheirstatistics.Vdenotesvocabularysize,Ldenotesthenumberoftrainingtokens,Ddenotesthenumberofdocuments,L/Vindicatestheaveragenumberofoccurrencesofaword,L/Dindicatestheaveragelengthofadocument.FortheBingwebchunkdata,200Bdenotes200billion.(cid:1)(cid:2)(cid:1)(cid:3)(cid:4)(cid:1)(cid:1)(cid:5)(cid:2)(cid:1)(cid:3)(cid:4)(cid:1)(cid:6)(cid:7)(cid:2)(cid:1)(cid:3)(cid:4)(cid:1)(cid:6)(cid:8)(cid:2)(cid:1)(cid:3)(cid:4)(cid:1)(cid:6)(cid:9)(cid:2)(cid:1)(cid:3)(cid:4)(cid:1)(cid:6)(cid:10)(cid:2)(cid:1)(cid:3)(cid:4)(cid:1)(cid:6)(cid:1)(cid:10)(cid:5)(cid:1)(cid:5)(cid:10)(cid:7)(cid:1)(cid:7)(cid:10)(cid:8)(cid:1)(cid:8)(cid:10)(cid:1)(cid:2)(cid:1)(cid:3)(cid:4)(cid:5)(cid:6)(cid:3)(cid:7)(cid:8)(cid:9)(cid:10)(cid:11)(cid:7)(cid:12)(cid:10)(cid:3)(cid:6)(cid:13)(cid:3)(cid:9)(cid:14)(cid:4)(cid:15)(cid:1)(cid:16)(cid:1)(cid:4)(cid:17)(cid:18)(cid:1)(cid:2)(cid:3)(cid:4)(cid:5)(cid:6)(cid:7)(cid:8)(cid:9)(cid:7)(cid:10)(cid:11)(cid:6)(cid:5)(cid:12)(cid:13)(cid:14)(cid:15)(cid:5)(cid:6)(cid:9)(cid:5)(cid:16)(cid:10)(cid:17)(cid:18)(cid:19)(cid:11)(cid:10)(cid:17)(cid:20)(cid:21)Figure8:Intra-nodescalabilityofLightLDA,using1,16,32threadsonasinglemachine(nonetworkcommunicationcosts).4.Weuseaboundedasynchronousdataparallelscheme[8]toremovethenetworkwaitingtimeoccurringattheboundaryofadjacentiterations.Notethat,topre-fetchtheﬁrstsliceofmodelforthenextdatablock,wedonotneedtowaitforthecompletionofthesamplingonthecurrentdatablockwiththelastsliceofthemodel.WecallthisStaleSynchronousParallel(SSP),whichisshowntooutperformthewellknownBulkSynchronousParallel(BSP)programmingmodel.Multi-threadEfﬁciencyOursamplerisembarrassinglyparallelwithinasingleworkermachine.Thisisachievedbysplittingthein-memorydatablockintodisjointpartitions(tobesampledbyindividualthreads),andsharingthein-memorymodelsliceamongstthethreads.Furthermore,wemakethesharedmodelsliceimmutable,anddelayallmodelupdateslocallybeforesendingthemtobeaggregatedattheparameterserver.Bykeepingthemodelsliceimmutable,weavoidconcurrencyissuessuchasraceconditionsandlocking,thusachievingnear-linearintra-nodescalability.Whiledelayingmodelupdatestheoreticallyslowsdownthemodelconvergencerate,inpractice,itelimi-natesconcurrencyissuesandthusincreasessamplerthroughput,easilyoutweighingtheslowerconvergencerate.Modernserver-grademachinescontainseveralCPUsockets(eachCPUhousesmanyphysicalcores)whichareconnectedtoseparatememorybanks.WhilethesebankscanbeaddressedbyallCPUs,memorylatenciesaremuchlongerwhenaccessingremotebanksattachedtoanothersocket—inotherwords,Non-UniformMemoryAccess(NUMA).Inourexperiments,wehavefoundNUMAeffectstobefairlysigniﬁcant,andwepartiallyaddressthemthroughtuningsamplingparameterssuchasthenumberofMetropolis-Hastingssteps(whichinﬂuencesCPUcachehitrates,andmitigatesNUMAeffects).Thatsaid,webelieveproperNUMA-awareprogrammingisabetterlong-termsolutiontothisproblem.Finally,wenotethatsettingcoreafﬁnitiesforeachthreadandenablinghardwarehyper-threadingonIntelprocessorscanbebeneﬁcial;weobserveda30%performancegainwhenemployingboth.7ExperimentalResultsWedemonstratethatLightLDAisabletotrainmuchlargerLDAmodelsonsimilarorlargerdatasizesthanpreviousLDAimplementations,usingmuchfewermachines—duetocarefuldata-modelslicing,andespeciallyournewMetropolis-HastingssamplerthatisnearlyanorderofmagnitudefasterthanSparseLDAandAliasLDA.Weuseseveraldatasets(Table7),notablyaBing“webchunk”datasetwith1.2billionwebpages(about200billiontokens13(cid:1)(cid:2)(cid:1)(cid:3)(cid:4)(cid:1)(cid:1)(cid:5)(cid:2)(cid:1)(cid:3)(cid:4)(cid:1)(cid:6)(cid:7)(cid:2)(cid:1)(cid:3)(cid:4)(cid:1)(cid:6)(cid:8)(cid:2)(cid:1)(cid:3)(cid:4)(cid:1)(cid:6)(cid:9)(cid:2)(cid:1)(cid:3)(cid:4)(cid:1)(cid:6)(cid:1)(cid:10)(cid:11)(cid:1)(cid:11)(cid:10)(cid:5)(cid:1)(cid:5)(cid:10)(cid:1)(cid:2)(cid:1)(cid:3)(cid:4)(cid:5)(cid:6)(cid:3)(cid:7)(cid:8)(cid:9)(cid:10)(cid:11)(cid:7)(cid:12)(cid:10)(cid:3)(cid:6)(cid:13)(cid:3)(cid:9)(cid:14)(cid:4)(cid:15)(cid:1)(cid:16)(cid:1)(cid:4)(cid:17)(cid:18)(cid:1)(cid:2)(cid:3)(cid:4)(cid:5)(cid:6)(cid:7)(cid:8)(cid:9)(cid:7)(cid:10)(cid:8)(cid:11)(cid:5)(cid:12)(cid:12)(cid:13)(cid:14)(cid:15)(cid:13)(cid:16)(cid:17)(cid:18)(cid:19)(cid:20)(cid:21)(cid:17)(cid:18)(cid:22)(cid:23)(cid:24)(cid:25)(cid:11)(cid:1)(cid:1)(cid:25)(cid:19)(cid:17)(cid:13)(cid:14)(cid:26)(cid:17)(cid:19)(cid:27)(cid:28)(cid:29)(cid:18)(cid:19)(cid:20)(cid:21)(cid:17)(cid:18)(cid:22)(cid:23)(cid:24)(cid:25)(cid:15)(cid:19)(cid:14)(cid:29)(cid:17)(cid:25)(cid:11)(cid:1)(cid:25)(cid:19)(cid:17)(cid:13)(cid:14)(cid:26)(cid:17)(cid:19)(cid:27)(cid:28)(cid:29)(cid:18)(cid:19)(cid:20)(cid:21)(cid:17)(cid:18)(cid:22)(cid:23)(cid:24)(cid:25)(cid:30)(cid:26)(cid:29)(cid:17)(cid:25)(cid:11)(cid:1)(cid:25)(cid:19)(cid:17)(cid:13)(cid:14)(cid:26)(cid:17)(cid:19)(cid:27)(cid:28)(cid:29)Figure9:Inter-nodescalabilityofLightLDA,using1,8,24machines.intotal).Ourexperimentsshowthat(1)ourdistributedimplementationofLightLDAhasnear-linearscalabilityinthenumberofcoresandmachines;(2)theLightLDAMetropolis-Hastingssamplerconvergessigniﬁcantlyfasterthanthestate-of-the-artSparseLDAandAliasLDAsamplers(measuredinasingle-threadedsetting);(3)mostimportantly,LightLDAenablesverylargedataandmodelsizestobetrainedonasfewas8machines.7.1ScalabilityofDistributedLightLDAIntheseexperiments,weestablishthatourLightLDAimplementationscalesalmostlinearlyincomputationalre-sources.Webeginwithintra-node,single-machinemulti-threadedscalability:todothis,werestrictthewebchunkdatasettothe50,000mostfrequentwords,sothatthemodelcanbeheldintheRAMofasinglemachine.WethenrunLightLDAwith1,16,32threads(ona32-coremachine),trainingamodelwith1milliontopicsonthewebchunkdataset.Werecordthenumberoftokenssampledbythealgorithmover2iterations,andplotitinFigure8.TheﬁgureshowsthatLightLDAexhibitsnearlylinearscalinginsideasinglemachine.Next,wedemonstratethatLightLDAscalesinthedistributedsetting,withmultiplemachines.UsingthesamewebchunkdatasetwithV=50,000vocabularywordsandK=1milliontopics,werunLightLDAon1,8,24machines(using20threadspermachine,and256GBRAMpermachine;suchconﬁgurationscanbereadilypurchasedfromcloudcomputeproviders),andsetthePetuumparameterservertouseastalenessvalueof16.Thistime,werecordthenumberoftokenssampledover100iterations,andcomputetheaveragetokenthroughputovertheﬁrst10iterations,thelast10iterations,andall100iterations.TheseresultsareplottedinFigure9,andwenotethatscalabilityispoorintheﬁrst10iterations,butclosetoperfectinthelast10iterations.Thisisbecauseduringtheinitialiterations,theLDAmodelisstillverydense(wordsanddocumentsareassignedtomanytopics,resultinginlargemodelsize;seeFigure10(c)),whichmakesthesystemincurhighnetworkcommunicationcosts(FigureFigure10(b))—enoughtosaturateourcluster’s1GbpsEthernet.Inthissituation,ourpipelining(explainedinFigure7)doesnotmasktheextracommunicationtime,resultinginpoorperformance.However,aftertheﬁrst10iterationsorso,themodelbecomessparseenoughforpipeliningtomaskthecommunicationtime,leadingtonear-perfectinter-nodescalability.Weconjecturethatupgradingto10GbpsEthernet(frequentlyseeninindustrialplatformsandcloudcomputingservices)orbetterwilleliminatetheinitialbottleneckduringtheﬁrstfewiterations.Finally,wedemonstratethatLightLDAisabletohandleverylargemodelsizes:werestorethefullV=1millionvocabularyofthewebchunkdataandK=1milliontopics,yieldingatotalof1trillionmodelparameterson200billiontokens.WethenrunLightLDAonthislargedatasetandmodelusing8and24machines,andplotthelog-likelihoodcurvesin(Figure10(a)).Convergenceisobservedwithin2dayson24machines(or5dayson8machines),anditisinthissensethatweclaimbigtopicmodelsarenowpossibleonmodestcomputeclusters.Onemightaskifoverﬁttinghappensonsuchlargemodels,giventhatthenumberofparameters(1trillion)islargerthanthenumberoftokens(200billion).Wepointoutthat(1)thereisevidencetoshowthatlargeLDAmodelscanimproveadpredictiontasks[21],and(2)theconvergedLDAmodelissparse,withfarfewerthan200billionnonzeroelements.Asevidence,wemonitoredthenumberofnon-zeroentriesinword-topictableduringthewholetraining6ThisstalenesssettingispartofStaleSynchronousParalleldata-parallelism[8].ApositivestalenessvalueletsLightLDAmaskthedelaybetweeniterationsduetotransferringmodelparametersoverthenetwork,yieldinghigherthroughputatalmostnocosttoquality.14(cid:1)(cid:2)(cid:3)(cid:4)(cid:5)(cid:6)(cid:7)(cid:8)(cid:1)(cid:9)(cid:3)(cid:10)(cid:5)(cid:6)(cid:7)(cid:8)(cid:1)(cid:9)(cid:3)(cid:4)(cid:5)(cid:6)(cid:7)(cid:8)(cid:1)(cid:8)(cid:3)(cid:10)(cid:5)(cid:6)(cid:7)(cid:8)(cid:1)(cid:8)(cid:3)(cid:4)(cid:5)(cid:6)(cid:7)(cid:8)(cid:4)(cid:11)(cid:4)(cid:7)(cid:8)(cid:4)(cid:7)(cid:12)(cid:4)(cid:1)(cid:2)(cid:3)(cid:4)(cid:5)(cid:6)(cid:7)(cid:8)(cid:5)(cid:6)(cid:9)(cid:2)(cid:2)(cid:10)(cid:1)(cid:2)(cid:3)(cid:4)(cid:5)(cid:1)(cid:2)(cid:3)(cid:4)(cid:5)(cid:6)(cid:7)(cid:8)(cid:9)(cid:10)(cid:2)(cid:11)(cid:10)(cid:2)(cid:12)(cid:13)(cid:2)(cid:3)(cid:4)(cid:5)(cid:6)(cid:7)(cid:8)(cid:9)(cid:10)(cid:6)(cid:7)(cid:8)(cid:9)(cid:10)(cid:11)(cid:12)(cid:13)(cid:14)(cid:15)(cid:5)(cid:16)(cid:8)(cid:8)(cid:8)(cid:9)(cid:10)(cid:11)(cid:12)(cid:13)(cid:14)(cid:15)(cid:5)(a)(cid:1)(cid:2)(cid:1)(cid:1)(cid:1)(cid:1)(cid:3)(cid:1)(cid:1)(cid:1)(cid:1)(cid:4)(cid:1)(cid:1)(cid:1)(cid:1)(cid:5)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:2)(cid:1)(cid:3)(cid:1)(cid:1)(cid:2)(cid:4)(cid:3)(cid:4)(cid:1)(cid:2)(cid:5)(cid:3)(cid:5)(cid:1)(cid:2)(cid:6)(cid:3)(cid:6)(cid:1)(cid:2)(cid:7)(cid:3)(cid:7)(cid:1)(cid:2)(cid:8)(cid:3)(cid:8)(cid:1)(cid:2)(cid:9)(cid:3)(cid:9)(cid:1)(cid:2)(cid:10)(cid:3)(cid:10)(cid:1)(cid:2)(cid:11)(cid:3)(cid:11)(cid:1)(cid:2)(cid:1)(cid:3)(cid:3)(cid:1)(cid:2)(cid:3)(cid:4)(cid:5)(cid:6)(cid:4)(cid:7)(cid:8)(cid:9)(cid:10)(cid:6)(cid:11)(cid:1)(cid:2)(cid:3)(cid:4)(cid:5)(cid:2)(cid:6)(cid:7)(cid:8)(cid:9)(cid:1)(cid:2)(cid:3)(cid:4)(cid:5)(cid:6)(cid:7)(cid:4)(cid:8)(cid:9)(cid:10)(cid:11)(cid:12)(cid:13)(cid:14)(cid:5)(cid:15)(cid:11)(cid:3)(cid:16)(cid:17)(cid:18)(cid:4)(cid:5)(cid:19)(cid:20)(cid:5)(cid:21)(cid:4)(cid:18)(cid:12)(cid:11)(cid:7)(cid:9)(cid:10)(cid:3)(cid:2)(cid:11)(cid:7)(cid:4)(cid:12)(cid:13)(cid:11)(cid:5)(cid:6)(cid:2)(cid:6)(cid:8)(cid:14)(cid:13)(cid:2)(cid:6)(cid:15)(cid:3)(cid:16)(cid:7)(cid:15)(cid:17)(cid:18)(cid:2)(cid:3)(cid:13)(cid:2)(cid:6)(cid:15)(cid:3)(b)(cid:1)(cid:2)(cid:1)(cid:3)(cid:1)(cid:1)(cid:3)(cid:2)(cid:1)(cid:4)(cid:1)(cid:1)(cid:4)(cid:2)(cid:1)(cid:5)(cid:1)(cid:1)(cid:1)(cid:4)(cid:1)(cid:6)(cid:1)(cid:7)(cid:1)(cid:8)(cid:1)(cid:3)(cid:1)(cid:1)(cid:1)(cid:2)(cid:3)(cid:4)(cid:5)(cid:6)(cid:7)(cid:8)(cid:9)(cid:4)(cid:6)(cid:10)(cid:11)(cid:12)(cid:13)(cid:1)(cid:2)(cid:3)(cid:4)(cid:5)(cid:2)(cid:6)(cid:7)(cid:8)(cid:9)(cid:1)(cid:2)(cid:3)(cid:4)(cid:5)(cid:6)(cid:7)(cid:8)(cid:9)(cid:4)(cid:6)(cid:2)(cid:10)(cid:4)(cid:11)(cid:6)(cid:8)(cid:12)(cid:4)(cid:11)(cid:13)(cid:12)(cid:8)(cid:2)(cid:14)(c)Figure10:(a)Log-likelihoodoverrunningtimewith8and24machines,onBingwebchunkdataset,usingV=K=1million(1trillionparameters).(b)Computetimev.s.Networkwaitingtime,asafunctionofiterationnumber.Observethatcommunicationcostsaresigniﬁcantlyhigherduringtheﬁrst10iterations,whenthemodelisstilldense.(c)Modelsize(non-zeroentriesinword-topictable)versusiterationnumber.FGHIJKLMFNHIJKLMFIHIJKLMFOHIJKLMFPHIJKLMFQHIJKLMFRHIJKLMLRLQLPLOLILNLGLMLSLRLLTUVWUXYWZ[\WZ]UUT^_‘abcdefgbchgfdijklmnopqropslltuvwFxyvxvz{|}vy~Fxyvxvz{|(cid:127)(cid:128)w|(cid:129)Fxyvxvz{|(cid:130)(cid:131)(cid:132)(cid:133)(cid:134)(cid:135)(cid:131)u(cid:136)(cid:137)(cid:138)x{yz(cid:129)(cid:131)u(cid:136)(cid:139)(cid:140)(cid:141)(cid:142)(cid:143)(cid:144)(cid:142)(cid:145)(cid:139)(cid:145)(cid:141)(cid:146)(cid:143)(cid:144)(cid:142)(cid:145)(cid:139)(cid:145)(cid:141)(cid:142)(cid:143)(cid:144)(cid:142)(cid:145)(cid:139)(cid:147)(cid:141)(cid:146)(cid:143)(cid:144)(cid:142)(cid:145)(cid:139)(cid:147)(cid:141)(cid:142)(cid:143)(cid:144)(cid:142)(cid:145)(cid:139)(cid:148)(cid:141)(cid:146)(cid:143)(cid:144)(cid:142)(cid:145)(cid:139)(cid:148)(cid:141)(cid:142)(cid:143)(cid:144)(cid:142)(cid:145)(cid:142)(cid:149)(cid:142)(cid:150)(cid:142)(cid:151)(cid:142)(cid:152)(cid:142)(cid:146)(cid:142)(cid:148)(cid:142)(cid:147)(cid:142)(cid:145)(cid:142)(cid:140)(cid:142)(cid:149)(cid:142)(cid:142)(cid:153)(cid:154)(cid:155)(cid:156)(cid:157)(cid:154)(cid:158)(cid:159)(cid:157)(cid:160)¡¢(cid:157)(cid:160)£(cid:154)(cid:154)(cid:156)⁄¥ƒ§¤'“«‹›¤'ﬁ›‹“ﬂ(cid:176)–†‡·(cid:181)¶•‚„¶•”††·»…‰(cid:139)(cid:190)¿…(cid:190)…(cid:192)`´ˆ…¿˜(cid:139)(cid:190)¿…(cid:190)…(cid:192)`´¯˘‰´˙(cid:139)(cid:190)¿…(cid:190)…(cid:192)`´¨(cid:201)˚¸(cid:204)˝(cid:201)»˛ˇ—(cid:190)`¿(cid:192)˙(cid:201)»˛(cid:139)(cid:149)(cid:141)(cid:148)(cid:143)(cid:144)(cid:142)(cid:140)(cid:139)(cid:149)(cid:141)(cid:146)(cid:143)(cid:144)(cid:142)(cid:140)(cid:139)(cid:149)(cid:141)(cid:152)(cid:143)(cid:144)(cid:142)(cid:140)(cid:139)(cid:149)(cid:141)(cid:151)(cid:143)(cid:144)(cid:142)(cid:140)(cid:139)(cid:149)(cid:141)(cid:150)(cid:143)(cid:144)(cid:142)(cid:140)(cid:139)(cid:149)(cid:141)(cid:149)(cid:143)(cid:144)(cid:142)(cid:140)(cid:139)(cid:149)(cid:141)(cid:142)(cid:143)(cid:144)(cid:142)(cid:140)(cid:139)(cid:140)(cid:141)(cid:142)(cid:143)(cid:144)(cid:142)(cid:145)(cid:139)(cid:145)(cid:141)(cid:142)(cid:143)(cid:144)(cid:142)(cid:145)(cid:142)(cid:149)(cid:142)(cid:150)(cid:142)(cid:151)(cid:142)(cid:152)(cid:142)(cid:146)(cid:142)(cid:148)(cid:142)(cid:147)(cid:142)(cid:145)(cid:142)(cid:140)(cid:142)(cid:149)(cid:142)(cid:142)(cid:209)(cid:154)(cid:209)(cid:210)(cid:157)(cid:157)(cid:154)(cid:158)(cid:159)(cid:157)(cid:160)¡¢(cid:157)(cid:160)£(cid:154)(cid:154)(cid:156)⁄¥ƒ§¤'“«‹›¤'ﬁ›‹“ﬂ(cid:176)(cid:211)†(cid:212)(cid:213)¶(cid:181)¶•‚„¶•”††·»…‰(cid:139)(cid:190)¿…(cid:190)…(cid:192)`´ˆ…¿˜(cid:139)(cid:190)¿…(cid:190)…(cid:192)`´¯˘‰´˙(cid:139)(cid:190)¿…(cid:190)…(cid:192)`´¨(cid:201)˚¸(cid:204)˝(cid:201)»˛ˇ—(cid:190)`¿(cid:192)˙(cid:201)»˛Figure11:PerformanceofdifferentLightLDAMetropolis-Hastingsproposals,ontheNYTimesdatasetwithK=1000topics.process,andobservedthatafter100iterations,themodelhadonly2billionnon-zeroentries(whichis1%of200billiontokens).7.2LightLDAAlgorithmversusBaselinesWealsowanttoestablishthatourMetropolis-Hastingsalgorithmconvergesfasterthanexistingsamplers(SparseLDAandAliasLDA)toahighqualityanswer.Usingasinglecomputationalthread,weranLightLDA,SparseLDAandAliasLDAontwosmallerdatasets(NYTimes,PubMed)usingK=1,000or10,000topics,andplottedthelog-likelihoodversusrunningtimeinFigure12.WenotethatAliasLDAconsistentlyperformsbetterthanSparseLDA(asreportedin[10]),andthatLightLDAisaround3to5timesasfastasAliasLDA.Tobetterunderstandtheperfor-mancedifferences,wealsoplotthetimetakenbytheﬁrst100iterationsofeachalgorithminFigure13.Ingeneral,LightLDAhasaconsistentlylowiterationtime7.AliasLDAissigniﬁcantlyfasterthanSparseLDAondatasetswithshortdocuments(PubMed8),butisonlymarginallyfasteronlongerdocuments(NYTimes).AlthoughLightLDAiscertainlyfasterper-iterationthanSparseLDAandAliasLDA,toobtainacompletepictureofconvergence,wemustplotthelog-likelihoodversuseachiteration(Figure14).WeobservethatSparseLDAmakesthebestprogressperiteration(becauseitusestheoriginalconditionalGibbsprobability),whileLightLDAisusuallyclosebehind(becauseitusessimpleMetropolis-Hastingsproposals).AliasLDA(anotherMetropolis-Hastingsalgo-rithm)iseithercomparabletoLightLDA(onNYTimes)orstrictlyworse(onPubmed).BecausethetimetakenforeachLightLDAandAliasLDAalgorithmisveryshort(Figure13),intermsofconvergencepertime,LightLDAandAliasLDAaresigniﬁcantlyfasterthanSparseLDA(withLightLDAbeingthefasterofthetwo).7Unlikethewebchunkexperiments,thereisnointer-machinecommunicationbottleneck,andthereforeLightLDAdoesnotexperienceslowinitialiterations.8NotethatweusethewholePubMeddatasetinthisexperiment,whereastheAliasLDApaper[10]onlyconsidered1%ofthetotalPubMeddata.15(cid:214)(cid:215)(cid:216)(cid:217)(cid:218)(cid:219)(cid:220)(cid:221)(cid:214)(cid:215)(cid:216)(cid:222)(cid:218)(cid:219)(cid:220)(cid:221)(cid:214)(cid:215)(cid:216)(cid:223)(cid:218)(cid:219)(cid:220)(cid:221)(cid:214)(cid:215)(cid:216)(cid:224)(cid:218)(cid:219)(cid:220)(cid:221)(cid:214)(cid:215)(cid:216)Æ(cid:218)(cid:219)(cid:220)(cid:221)(cid:214)(cid:215)(cid:216)(cid:226)(cid:218)(cid:219)(cid:220)(cid:221)(cid:214)(cid:215)(cid:216)ª(cid:218)(cid:219)(cid:220)(cid:221)(cid:214)(cid:215)(cid:216)(cid:215)(cid:218)(cid:219)(cid:220)(cid:221)(cid:214)(cid:215)(cid:216)(cid:220)(cid:218)(cid:219)(cid:220)(cid:221)(cid:220)(cid:224)(cid:220)(cid:220)(cid:220)(cid:215)(cid:220)(cid:220)(cid:220)(cid:220)(cid:215)(cid:224)(cid:220)(cid:220)(cid:220)ª(cid:220)(cid:220)(cid:220)(cid:220)ª(cid:224)(cid:220)(cid:220)(cid:220)(cid:226)(cid:220)(cid:220)(cid:220)(cid:220)(cid:228)(cid:229)(cid:230)(cid:231)ŁØŒºŁØ(cid:236)(cid:229)(cid:229)(cid:237)(cid:238)(cid:239)(cid:240)æ(cid:242)(cid:243)(cid:244)(cid:1)(cid:2)(cid:3)(cid:4)(cid:5)(cid:6)(cid:7)(cid:8)(cid:9)(cid:10)(cid:10)(cid:10)(cid:10)(cid:8)(cid:11)(cid:12)(cid:13)(cid:4)(cid:14)(cid:7)(cid:238)ı(cid:246)(cid:247)(cid:244)(cid:239)łøœœß(cid:252)(cid:246)(cid:244)łøœł(cid:252)(cid:253)(cid:254)(cid:255)łøœ(cid:214)(cid:215)(cid:216)Æ(cid:218)(cid:219)(cid:215)(cid:220)(cid:214)(cid:215)(cid:216)(cid:226)(cid:218)(cid:219)(cid:215)(cid:220)(cid:214)(cid:215)(cid:216)ª(cid:218)(cid:219)(cid:215)(cid:220)(cid:214)(cid:215)(cid:216)(cid:215)(cid:218)(cid:219)(cid:215)(cid:220)(cid:214)(cid:215)(cid:216)(cid:220)(cid:218)(cid:219)(cid:215)(cid:220)(cid:214)(cid:221)(cid:216)(cid:220)(cid:218)(cid:219)(cid:220)(cid:221)(cid:214)(cid:217)(cid:216)(cid:220)(cid:218)(cid:219)(cid:220)(cid:221)(cid:214)(cid:222)(cid:216)(cid:220)(cid:218)(cid:219)(cid:220)(cid:221)(cid:220)ª(cid:220)(cid:220)(cid:220)(cid:220)Æ(cid:220)(cid:220)(cid:220)(cid:220)(cid:223)(cid:220)(cid:220)(cid:220)(cid:220)(cid:217)(cid:220)(cid:220)(cid:220)(cid:220)(cid:215)(cid:220)(cid:220)(cid:220)(cid:220)(cid:220)(cid:228)(cid:229)(cid:230)(cid:231)ŁØŒºŁØ(cid:236)(cid:229)(cid:229)(cid:237)(cid:238)(cid:239)(cid:240)æ(cid:242)(cid:243)(cid:244)(cid:15)(cid:16)(cid:17)(cid:18)(cid:6)(cid:19)(cid:8)(cid:9)(cid:10)(cid:10)(cid:10)(cid:10)(cid:8)(cid:11)(cid:12)(cid:13)(cid:4)(cid:14)(cid:7)(cid:238)ı(cid:246)(cid:247)(cid:244)(cid:239)łøœœß(cid:252)(cid:246)(cid:244)łøœł(cid:252)(cid:253)(cid:254)(cid:255)łøœ(cid:214)(cid:215)(cid:216)(cid:226)(cid:218)(cid:219)(cid:215)(cid:220)(cid:214)(cid:215)(cid:216)ª(cid:218)(cid:219)(cid:215)(cid:220)(cid:214)(cid:215)(cid:216)(cid:215)(cid:218)(cid:219)(cid:215)(cid:220)(cid:214)(cid:215)(cid:216)(cid:220)(cid:218)(cid:219)(cid:215)(cid:220)(cid:214)(cid:221)(cid:216)(cid:220)(cid:218)(cid:219)(cid:220)(cid:221)(cid:214)(cid:217)(cid:216)(cid:220)(cid:218)(cid:219)(cid:220)(cid:221)(cid:214)(cid:222)(cid:216)(cid:220)(cid:218)(cid:219)(cid:220)(cid:221)(cid:214)(cid:223)(cid:216)(cid:220)(cid:218)(cid:219)(cid:220)(cid:221)(cid:220)(cid:215)(cid:220)(cid:220)(cid:220)(cid:220)ª(cid:220)(cid:220)(cid:220)(cid:220)(cid:226)(cid:220)(cid:220)(cid:220)(cid:220)Æ(cid:220)(cid:220)(cid:220)(cid:220)(cid:224)(cid:220)(cid:220)(cid:220)(cid:220)(cid:223)(cid:220)(cid:220)(cid:220)(cid:220)(cid:228)(cid:229)(cid:230)(cid:231)ŁØŒºŁØ(cid:236)(cid:229)(cid:229)(cid:237)(cid:238)(cid:239)(cid:240)æ(cid:242)(cid:243)(cid:244)(cid:15)(cid:16)(cid:17)(cid:18)(cid:6)(cid:19)(cid:8)(cid:9)(cid:10)(cid:10)(cid:10)(cid:8)(cid:11)(cid:12)(cid:13)(cid:4)(cid:14)(cid:7)(cid:238)ı(cid:246)(cid:247)(cid:244)(cid:239)łøœœß(cid:252)(cid:246)(cid:244)łøœł(cid:252)(cid:253)(cid:254)(cid:255)łøœ(cid:214)(cid:215)(cid:216)(cid:223)(cid:218)(cid:219)(cid:220)(cid:221)(cid:214)(cid:215)(cid:216)(cid:224)(cid:218)(cid:219)(cid:220)(cid:221)(cid:214)(cid:215)(cid:216)Æ(cid:218)(cid:219)(cid:220)(cid:221)(cid:214)(cid:215)(cid:216)(cid:226)(cid:218)(cid:219)(cid:220)(cid:221)(cid:214)(cid:215)(cid:216)ª(cid:218)(cid:219)(cid:220)(cid:221)(cid:214)(cid:215)(cid:216)(cid:215)(cid:218)(cid:219)(cid:220)(cid:221)(cid:214)(cid:221)(cid:216)(cid:224)(cid:218)(cid:219)(cid:220)(cid:217)(cid:214)(cid:217)(cid:216)(cid:224)(cid:218)(cid:219)(cid:220)(cid:217)(cid:220)ª(cid:220)(cid:220)(cid:220)Æ(cid:220)(cid:220)(cid:220)(cid:223)(cid:220)(cid:220)(cid:220)(cid:217)(cid:220)(cid:220)(cid:220)(cid:228)(cid:229)(cid:230)(cid:231)ŁØŒºŁØ(cid:236)(cid:229)(cid:229)(cid:237)(cid:238)(cid:239)(cid:240)æ(cid:242)(cid:243)(cid:244)(cid:1)(cid:2)(cid:3)(cid:4)(cid:5)(cid:6)(cid:7)(cid:8)(cid:9)(cid:10)(cid:10)(cid:10)(cid:8)(cid:11)(cid:12)(cid:13)(cid:4)(cid:14)(cid:7)(cid:238)ı(cid:246)(cid:247)(cid:244)(cid:239)łøœœß(cid:252)(cid:246)(cid:244)łøœł(cid:252)(cid:253)(cid:254)(cid:255)łøœFigure12:Log-likelihoodversusrunningtimeforSparseLDA,AliasLDA,LightLDA.Finally,wearguedthattheLightLDA“cycleproposal”,whichalternatesbetweenthedoc-proposalandword-proposal,wouldbehighlyeffectiveatexploringthemodelspace,andthusabetteroptionthaneitherproposalin-dividually.Todemonstratethis,inFigure11,weplottheperformanceofthefullLightLDAcycleproposalversusthedoc-andword-proposalsalone,aswellasSparseLDA(whichrepresentsthegoldstandardforquality,beinganon-Metropolis-HastingsGibbssampler).Theplotsaresubdividedinto(1)fulllikelihood,(2)documentlikelihood,and(3)wordlikelihood([18]explainsthisdecompositioninmoredetail).Intuitively,ahighdoc-likelihoodshowsthatthelearnedtopicscompactlyrepresentalldocuments,whileahighword-likelihoodindicatesthatthetopicscompactlyrepresentthevocabulary.Althoughthedoc-andword-proposalsappeartodowellontotallikelihood,inactualfact,theword-proposalfailstomaximizethedoc-likelihood,whilethedoc-proposalfailstomaximizetheword-likelihood.Incontrast,thefullLightLDAcycleproposalrepresentsbothdocumentsandvocabularycompactly,andisnearlyasgoodinqualityasSparseLDA.8ConclusionsWehaveimplementedadistributedLDAsampler,LightLDA,thatenablesverylargedatasizesandmodelstobepro-cessedonasmallcomputecluster.LightLDAfeaturessigniﬁcantlyimprovedsamplingthroughputandconvergencerateviaa(surprisingly)fastO(1)Metropolis-Hastingsalgorithm,andallowsevensmallclusterstotackleverylargedataandmodelsizesthankstocarefulstructure-awaremodel-parallelismandbounded-asynchronousdata-parallelismonthePetuumframework.Ahybriddatastructureisusedtosimultaneouslymaintaingoodperformanceandmem-oryefﬁciency,providingabalancedtrade-off.Onafuturenote,webelievetheMetropolis-Hastingsdecompositioninoursamplercanbesuccessfullyappliedtoinferenceofothergraphicalmodels,alongsidethegeneralconceptsofstructure-awaremodel-parallelismandbounded-asynchronousdata-parallelism.ItisourhopethatmoreMLappli-cationscanberunwithbigdataandmodelsizesonsmall,widely-availableclusters,andthatthisworkwillinspirecurrentandfuturedevelopmentoflarge-scaleMLsystems.References[1]A.Ahmed,M.Aly,J.Gonzalez,S.Narayanamurthy,andA.J.Smola.Scalableinferenceinlatentvariablemodels.InWSDM,pages123–132,2012.16(cid:1)(cid:2)(cid:1)(cid:1)(cid:3)(cid:4)(cid:1)(cid:1)(cid:1)(cid:3)(cid:4)(cid:2)(cid:1)(cid:1)(cid:5)(cid:4)(cid:1)(cid:1)(cid:1)(cid:5)(cid:4)(cid:2)(cid:1)(cid:1)(cid:6)(cid:4)(cid:1)(cid:1)(cid:1)(cid:6)(cid:4)(cid:2)(cid:1)(cid:1)(cid:1)(cid:3)(cid:1)(cid:5)(cid:1)(cid:6)(cid:1)(cid:7)(cid:1)(cid:2)(cid:1)(cid:8)(cid:1)(cid:9)(cid:1)(cid:10)(cid:1)(cid:11)(cid:1)(cid:3)(cid:1)(cid:1)(cid:0)(cid:12)(cid:13)(cid:14)(cid:15)(cid:16)(cid:17)(cid:18)(cid:14)(cid:19)(cid:14)(cid:15)(cid:12)(cid:20)(cid:21)(cid:12)(cid:19)(cid:22)(cid:21)(cid:20)(cid:14)(cid:15)(cid:23)(cid:24)(cid:25)(cid:26)(cid:27)(cid:28)(cid:29)(cid:30)(cid:31) (cid:27)(cid:28)! (cid:31)(cid:29)"#(cid:1)(cid:2)(cid:3)(cid:4)(cid:5)(cid:6)(cid:7)(cid:8)(cid:9)(cid:10)(cid:10)(cid:10)(cid:10)(cid:8)(cid:11)(cid:12)(cid:13)(cid:4)(cid:14)(cid:7)$%!(cid:28)#(cid:27)&’(()(cid:31)!#&’(&(cid:31)*+ &’((cid:1)(cid:3)(cid:1)(cid:1)(cid:5)(cid:1)(cid:1)(cid:6)(cid:1)(cid:1)(cid:7)(cid:1)(cid:1)(cid:2)(cid:1)(cid:1)(cid:8)(cid:1)(cid:1)(cid:1)(cid:3)(cid:1)(cid:5)(cid:1)(cid:6)(cid:1)(cid:7)(cid:1)(cid:2)(cid:1)(cid:8)(cid:1)(cid:9)(cid:1)(cid:10)(cid:1)(cid:11)(cid:1)(cid:3)(cid:1)(cid:1)(cid:0)(cid:12)(cid:13)(cid:14)(cid:15)(cid:16)(cid:17)(cid:18)(cid:14)(cid:19)(cid:14)(cid:15)(cid:12)(cid:20)(cid:21)(cid:12)(cid:19)(cid:22)(cid:21)(cid:20)(cid:14)(cid:15)(cid:23)(cid:24)(cid:25)(cid:26)(cid:27)(cid:28)(cid:29)(cid:30)(cid:31) (cid:27)(cid:28)! (cid:31)(cid:29)"#(cid:1)(cid:2)(cid:3)(cid:4)(cid:5)(cid:6)(cid:7)(cid:8)(cid:9)(cid:10)(cid:10)(cid:10)(cid:8)(cid:11)(cid:12)(cid:13)(cid:4)(cid:14)(cid:7)$%!(cid:28)#(cid:27)&’(()(cid:31)!#&’(&(cid:31)*+ &’(,-,,./,,,./-,,0/,,,,.,0,1,2,-,3,4,5,6,.,,789:;<=>:?:;8@A8?BA@:;CDEFGHIJKLGHMLKINO(cid:1)(cid:2)(cid:3)(cid:4)(cid:5)(cid:6)(cid:7)(cid:8)(cid:9)(cid:9)(cid:9)(cid:9)(cid:7)(cid:10)(cid:11)(cid:12)(cid:13)(cid:14)(cid:15)PQKMORSPRKTULRSP,./,,,0/,,,1/,,,2/,,,-/,,,3/,,,,.,0,1,2,-,3,4,5,6,.,,789:;<=>:?:;8@A8?BA@:;CDEFGHIJKLGHMLKINO(cid:1)(cid:2)(cid:3)(cid:4)(cid:5)(cid:6)(cid:7)(cid:8)(cid:9)(cid:9)(cid:9)(cid:7)(cid:10)(cid:11)(cid:12)(cid:13)(cid:14)(cid:15)VWMHOGRSPPQKMORSPRKTULRSPFigure13:Runningtimeforeachoftheﬁrst100iterationsofSparseLDA,AliasLDA,LightLDA.ThecurveforSparseLDAwasomittedfromtheK=10,000-topicPubMedexperiment,asitwastooslowtoshowupintheplottedrange.XYZ[\]^_XYZ‘\]^_XYZa\]^_XYZb\]^_XYZc\]^_XYZd\]^_XYZe\]^_XYZY\]^_XYZ^\]^_^Y^e^d^c^b^a^‘^[^_^Y^^fghijklmjknggopqrstuvwxytuzyxv{|(cid:1)(cid:2)(cid:3)(cid:4)(cid:5)(cid:6)(cid:7)(cid:8)(cid:9)(cid:10)(cid:10)(cid:10)(cid:10)(cid:8)(cid:11)(cid:12)(cid:13)(cid:4)(cid:14)(cid:7)}~zu|t(cid:127)(cid:128)(cid:129)(cid:129)(cid:130)xz|(cid:127)(cid:128)(cid:129)(cid:127)x(cid:131)(cid:132)y(cid:127)(cid:128)(cid:129)(cid:133)(cid:134)(cid:135)(cid:136)(cid:137)(cid:138)(cid:139)(cid:140)(cid:133)(cid:134)(cid:135)(cid:141)(cid:137)(cid:138)(cid:139)(cid:140)(cid:133)(cid:134)(cid:135)(cid:142)(cid:137)(cid:138)(cid:139)(cid:140)(cid:133)(cid:134)(cid:135)(cid:143)(cid:137)(cid:138)(cid:139)(cid:140)(cid:133)(cid:134)(cid:135)(cid:144)(cid:137)(cid:138)(cid:139)(cid:140)(cid:133)(cid:134)(cid:135)(cid:134)(cid:137)(cid:138)(cid:139)(cid:140)(cid:133)(cid:140)(cid:135)(cid:141)(cid:137)(cid:138)(cid:139)(cid:145)(cid:133)(cid:145)(cid:135)(cid:141)(cid:137)(cid:138)(cid:139)(cid:145)(cid:139)(cid:134)(cid:139)(cid:144)(cid:139)(cid:143)(cid:139)(cid:142)(cid:139)(cid:141)(cid:139)(cid:136)(cid:139)(cid:146)(cid:139)(cid:145)(cid:139)(cid:140)(cid:139)(cid:134)(cid:139)(cid:139)(cid:147)(cid:148)(cid:149)(cid:150)(cid:151)(cid:152)(cid:153)(cid:154)(cid:151)(cid:152)(cid:155)(cid:148)(cid:148)(cid:156)(cid:157)(cid:158)(cid:159)(cid:160)¡¢£⁄¥ƒ¡¢§ƒ¥£¤'(cid:1)(cid:2)(cid:3)(cid:4)(cid:5)(cid:6)(cid:7)(cid:8)(cid:9)(cid:10)(cid:10)(cid:10)(cid:8)(cid:11)(cid:12)(cid:13)(cid:4)(cid:14)(cid:7)“«§¢'¡‹›ﬁﬁﬂ¥§'‹›ﬁ‹¥(cid:176)–ƒ‹›ﬁ†‡·(cid:181)¶•‡‚†‡·„¶•‡‚†‡·‚¶•‡‚†”·‚¶•‚»‚‡‚„‚…‚(cid:181)‚‰‚(cid:190)‚¿‚”‚»‚‡‚‚(cid:192)`´ˆ˜¯˘˙˜¯¨``(cid:201)˚¸(cid:204)˝˛ˇ—(cid:209)(cid:210)(cid:211)˛ˇ(cid:212)(cid:211)(cid:210)—(cid:213)(cid:214)(cid:1)(cid:2)(cid:3)(cid:4)(cid:5)(cid:6)(cid:7)(cid:8)(cid:9)(cid:9)(cid:9)(cid:9)(cid:7)(cid:10)(cid:11)(cid:12)(cid:13)(cid:14)(cid:15)(cid:215)(cid:216)(cid:210)(cid:212)(cid:214)(cid:217)(cid:218)(cid:215)(cid:217)(cid:210)(cid:219)(cid:220)(cid:211)(cid:217)(cid:218)(cid:215)(cid:221)(cid:222)(cid:223)(cid:224)Æ(cid:226)(cid:222)ª(cid:221)(cid:222)(cid:223)(cid:228)Æ(cid:226)(cid:222)ª(cid:221)(cid:222)(cid:223)(cid:222)Æ(cid:226)(cid:222)ª(cid:221)(cid:222)(cid:223)ªÆ(cid:226)(cid:222)ª(cid:221)(cid:229)(cid:223)ªÆ(cid:226)ª(cid:229)(cid:221)(cid:230)(cid:223)ªÆ(cid:226)ª(cid:229)(cid:221)(cid:231)(cid:223)ªÆ(cid:226)ª(cid:229)(cid:221)Ł(cid:223)ªÆ(cid:226)ª(cid:229)ª(cid:222)ª(cid:228)ª(cid:224)ªØªŒªŁª(cid:231)ª(cid:230)ª(cid:229)ª(cid:222)ªªº(cid:236)(cid:237)(cid:238)(cid:239)(cid:240)æ(cid:242)(cid:239)(cid:240)(cid:243)(cid:236)(cid:236)(cid:244)ı(cid:246)(cid:247)łøœß(cid:252)(cid:253)(cid:254)øœ(cid:255)(cid:254)(cid:253)ß(cid:13)(cid:14)(cid:1)(cid:2)(cid:3)(cid:4)(cid:5)(cid:6)(cid:7)(cid:8)(cid:9)(cid:9)(cid:9)(cid:7)(cid:10)(cid:11)(cid:12)(cid:13)(cid:14)(cid:15)(cid:15)(cid:16)(cid:255)œ(cid:14)ø(cid:17)(cid:18)(cid:19)(cid:19)(cid:20)(cid:253)(cid:255)(cid:14)(cid:17)(cid:18)(cid:19)(cid:17)(cid:253)(cid:21)(cid:22)(cid:254)(cid:17)(cid:18)(cid:19)Figure14:Log-likelihoodversusiterationnumberforSparseLDA,AliasLDA,andLightLDA.17[2]E.Airoldi,D.Blei,S.Fienberg,andE.Xing.Mixedmembershipstochasticblockmodels.J.Mach.Learn.Res.,9:1981–2014,2008.[3]C.Andrieu,N.D.Freitas,A.Doucet,andM.I.Jordan.AnintroductiontoMCMCformachinelearning.Machinelearning,50(1):5–43,2003.[4]D.M.Blei,A.Y.Ng,andM.I.Jordan.Latentdirichletallocation.J.Mach.Learn.Res.,3(30):993–1022,2003.[5]W.Dai,A.Kumar,J.Wei,Q.Ho,G.Gibson,andE.P.Xing.High-performancedistributedmlatscalethroughparameterserverconsistencymodels.InAAAI,2015.[6]T.L.GrifﬁthsandM.Steyvers.Findingscientiﬁctopics.PNAS,101(1):5228–5235,2004.[7]W.K.Hastings.MonteCarlosamplingmethodsusingMarkovchainsandtheirapplications.Biometrika,57(1):97–109,1970.[8]Q.Ho,J.Cipar,H.Cui,S.Lee,J.K.Kim,P.B.Gibbons,G.A.Gibson,G.Ganger,andE.Xing.Moreeffectivedistributedmlviaastalesynchronousparallelparameterserver.InNIPS,2013.[9]S.Lee,J.K.Kim,X.Zheng,Q.Ho,G.A.Gibson,andE.P.Xing.Onmodelparallelizationandschedulingstrategiesfordistributedmachinelearning.InNIPS,2014.[10]A.Li,A.Ahmed,S.Ravi,andA.J.Smola.Reducingthesamplingcomplexityoftopicmodels.InKDD,2014.[11]M.Li,D.G.Andersen,J.W.Park,A.Ahmed,V.Josifovski,J.Long,E.J.Shekita,andB.-Y.Su.Scalingdistributedmachinelearningwiththeparameterserver.InOSDI,pages583–598,2014.[12]Z.Liu,Y.Zhang,E.Y.Chang,andM.Sun.Plda+:Parallellatentdirichletallocationwithdataplacementandpipelineprocessing.ACMTransactionsonIntelligentSystemsandTechnology(TIST),2(3):26,2011.[13]G.Marsaglia,W.W.Tsang,andJ.Wang.Fastgenerationofdiscreterandomvariables.JournalofStatisticalSoftware,11(3):1–11,2004.[14]N.Metropolis,A.Rosenbluth,M.Rosenbluth,A.Teller,andE.Teller.Equationsofstatecalculationsbyfastcomputingmachines.J.Chem.Phys.,21:1087–1091,1953.[15]D.Newman,A.Asuncion,P.Smyth,andM.Welling.Distributedalgorithmsfortopicmodels.TheJournalofMachineLearningResearch,10:1801–1828,2009.[16]R.PowerandJ.Li.Piccolo:Buildingfast,distributedprogramswithpartitionedtables.InOSDI,pages1–14,2010.[17]S.ShringarpureandE.P.Xing.mstruct:inferenceofpopulationstructureinlightofbothgeneticadmixingandallelemutations.Genetics,182(2):575–593,2009.[18]A.SmolaandS.Narayanamurthy.Anarchitectureforparalleltopicmodels.Proc.VLDBEndow.,3(1-2),2010.[19]L.Tierney.Markovchainsforexploringposteriordistributions.AnnalsofStatistics,22:1701–1762,1994.[20]A.J.Walker.Anefﬁcientmethodforgeneratingdiscreterandomvariableswithgeneraldistributions.ACMTrans.Math.Softw.,3(3):253–256,1977.[21]Y.Wang,X.Zhao,Z.Sun,H.Yan,L.Wang,Z.Jin,L.Wang,Y.Gao,J.Zeng,Q.Yang,etal.Towardstopicmodelingforbigdata.arXivpreprintarXiv:1405.4402,2014.[22]L.Yao,D.Mimno,andA.McCallum.Efﬁcientmethodsfortopicmodelinferenceonstreamingdocumentcollections.InKDD,2009.18[23]J.Yin,Q.Ho,andE.P.Xing.Ascalableapproachtoprobabilisticlatentspaceinferenceoflarge-scalenetworks.NIPS,2013.[24]X.Zheng,J.K.Kim,Q.Ho,andE.P.Xing.Model-parallelinferenceforbigtopicmodels.arXivpreprintarXiv:1411.2305,2014.[25]J.Zhu,A.Ahmed,andE.P.Xing.Medlda:maximummarginsupervisedtopicmodelsforregressionandclassiﬁcation.InICML,pages1257–1264,2009.19