2
2
0
2

b
e
F
4
1

]

V
C
.
s
c
[

1
v
1
5
8
6
0
.
2
0
2
2
:
v
i
X
r
a

HAKE: A Knowledge Engine Foundation for
Human Activity Understanding

Yong-Lu Li, Xinpeng Liu, Xiaoqian Wu, Yizhuo Li, Zuoyu Qiu, Liang Xu, Yue Xu, Hao-Shu Fang, Cewu Lu

1

Abstract—Human activity understanding is of widespread interest in artiﬁcial intelligence and spans diverse applications like health
care and behavior analysis. Although there have been advances with deep learning, it remains challenging. The object recognition-like
solutions usually try to map pixels to semantics directly, but activity patterns are much different from object patterns, thus hindering
another success. In this work, we propose a novel paradigm to reformulate this task in two-stage: ﬁrst mapping pixels to an
intermediate space spanned by atomic activity primitives, then programming detected primitives with interpretable logic rules to infer
semantics. To afford a representative primitive space, we build a knowledge base including 26+ M primitive labels and logic rules from
human priors or automatic discovering. Our framework, Human Activity Knowledge Engine (HAKE), exhibits superior generalization
ability and performance upon canonical methods on challenging benchmarks. Code and data are available at http://hake-mvig.cn/.

Index Terms—Human Activity Understanding, Knowledge Engine, Activity Primitive, Neural-Symbolic Reasoning, Logic Rule.

(cid:70)

1 INTRODUCTION

impacts,

problem with great

V ISUAL activity understanding is a fundamental AI

e.g., helping health
care [3], advancing robot skill learning from humans [4],
etc. All these require accurate activity recognition. However,
current systems are far from qualiﬁed, even in this booming
deep learning era. This is mainly because the conventional
pattern recognition route, which attempts to learn a direct
mapping from the image space to the activity semantic
space, may not be suitable for activity recognition.

The underlying reason is that, taking the most rep-
resentative object recognition as a comparison case, the
activity pattern is much different from the object pattern.
Thus, similar success cannot be achieved by direct map-
ping. To prove this, a novel measurement depicting im-
age semantic “density” is introduced: Semantic Coverage
Rate (SCR). In a bounding box containing an entity (per-
son/object/interactive person-object pair, Fig. 1a), the area
proportion of key regions carrying the indispensable semantics
for recognition is SCR. The principle for measuring SCR is
to ask human participants to mask the smallest box regions
to make other human participants unable to distinguish the
entity (Appendix A). For object recognition (Fig. 1a), pizza’s
SCR is 93.7%. In contrast, more concentrated masks are
usually made to activity images, leading to smaller SCRs: eat
pizza has an SCR of 15.3% (unmasked regions
“clutter”).
By testing on 1,500 images, the average SCRs of object and
activity images are 61.0% and 12.2%, respectively. Hence,
we conclude that activity recognition is greatly different
from object recognition. A lower SCR requires us to mine

≈

• Yong-Lu Li, Xinpeng Liu, Xiaoqian Wu, Yizhuo Li, Zuoyu Qiu, Liang
Xu, Yue Xu, and Hao-Shu Fang are with the Department of Electrical
and Computer Engineering, Shanghai Jiao Tong University, Shanghai,
200240, China. E-mail: {yonglu li, enlighten, liyizhuo, 17803091056,
liangxu, silicxuyue}@sjtu.edu.cn, xinpengliu, fhaoshu0907@gmail.com.

• Cewu Lu is the corresponding author, member of Qing Yuan Research In-
stitute and MoE Key Lab of AI, AI Institute, Shanghai Jiao Tong Univer-
sity, China, and Shanghai Qi Zhi institute. E-mail: lucewu@sjtu.edu.cn.

sparse semantics from “cluttered” scenes, thus hindering
direct mapping (Fig. 1c) from achieving efﬁcacy in object
recognition, even with large-scale data. In Fig. 1b, given a
quantity of data, it can only achieve limited performance
(detailed in Sec. 3), which is far from successful object
recognition [2].

To ﬁnd a better solution, a natural method is to observe
how humans recognize activities. An empirical study is con-
ducted to ask participants to explain how they differentiate
activities. First, local visual cues are frequently mentioned.
For activities run and walk, the claimed differences mainly
lie in the moving state of legs, but the upper body affords
limited cues, which is in line with the discovery of a smaller
SCR. For drink, the main criterion is often the existence of
hand-hold-sth, head-contact with-sth, cup. These atomic and
shareable cues can be seen as primitives (Fig. 2a). We be-
lieve that humans can unconsciously discovering primitives
(like System 1 [6]). Moreover, cognitive neuroscience studies
also found that certain brain regions in the ventral stream
(EBA) process visual information about body parts and
objects [7]. Second, primitives can be seen as the causes of
activities: participants can logically combine hand-hold-sth,
head-contact with-sth, cup to infer drink (like System 2 [6]).
This is in accordance with the cognition study that humans
combine basic components to perceive concepts [8], [9], [10].
We argue that activity perception rests on the ability to dis-
cover primitives, while the efﬁcacy of human inference arises
from logical reasoning to program primitives into semantics
with compositional generalization (Fig. 2b). Thus, we pro-
pose Human Activity Knowledge Engine (HAKE), which is
an end-to-end differentiable two-stage system (Fig. 1d):

(1) Intermediate primitive space embeds activity infor-
mation in images with limited and representative primi-
tives. We build a comprehensive knowledge base by crowd-
sourcing. As primitive space is sparse [5], we can cover most
primitives from daily activities, i.e., one-time labeling and
transferability. HAKE trained on our knowledge base can
detect primitives well and focus only on primitives instead

 
 
 
 
 
 
2

Fig. 1: Object Recognition vs. Activity Recognition. a. SCR test reveals the difference between activity and object visual patterns. b. Bottleneck of
direct mapping. Given the same order of magnitude of training images, it rapidly saturates on HICO-DET [1] test set and performs much worse
than object recognition on MS-COCO [2] (dotted line, 55+ mAP (mean Average Precision, %)). c-d. Direct mapping and our two-stage paradigm.
We introduce an intermediate primitive space to embed activity information and infer semantics via primitive programming.

Existing datasets [11], [12], [13], [19] often have a large
difference in deﬁnition, thus transferring knowledge from
one dataset to another is ineffective. Plenty of works based
on CNN, 3D-CNN, GNN, or GCN has been proposed to
address activity understanding [18], [20], [21], [22], [23], [24],
[25], [26]. But compared with object detection [27] or pose
estimation [28], its performance is still limited.

Human-Object Interaction (HOI) Learning. HOI consists
of the most important part of daily activities. As a sub-
ﬁled of visual relationship learning, HOI understanding has
attracted a lot of attention and several large datasets [1],
[12] have been released. Meanwhile, to prompt this ﬁeld,
many deep learning based methods [29], [30], [31], [32], [33],
[34] were proposed. Wu et. al. [1] proposed multi-stream
framework followed by subsequent works [30], [31], [34].
GPNN [32] used graphical model to address HOI detec-
tion. iCAN [30] and PMFNet [35] adopted self-attention to
correlate the human, object, and context. TIN [31] modeled
interactiveness to suppress non-interactive pairs. VCL [34]
exploited the compositional characteristic of HOI. Also,
some works [33], [36], [37] dug into the relationship among
HOIs, while IDN [38] focused on how HOIs are constructed
via integration and decomposition. Moreover, there also
appeared one-stage methods [39] that directly detected HOI
triplets. Besides the works based on convolutional neural
networks (CNN), recently Transformer-based methods [40],
[41] are proposed and achieved decent improvements.

Body Part based Activity Understanding. Usually, previous
works used the instance-level patterns as the cue. However,
some approaches were studied to utilize ﬁner-grained hu-
man body part pattern [18], [42], [43], [44] for better under-
standing. Khan et. al. [45] combines the features of instance
and parts to operate the recognition. Gkioxari et. al. [21]
detects both the instance and parts and input them all
into a classiﬁer. Yao et. al. [46] builds a graphical model

Fig. 2: Nature of activity perception: atomic primitive and logical
reasoning. a. Primitives exist such as body part states [5] and objects.
Here, we show some common hand states. b. Activities can be inferred
by programming primitives following logical rules.

of the whole image, thus achieving an equivalent higher SCR.
(2) A reasoning engine programs detected primitives
into semantics with explicit logic rules and updates the
rules during reasoning. That is, diverse activities can be
composed of a ﬁnite set of primitives via logical reasoning
with compositional generalization.

2 RELATED WORKS

Activity Understanding. Beneﬁted by deep learning and
large-scale datasets, activity understanding has achieved
huge improvements recently. There are mainly image-
based [11],
[16] and
skeleton-based [17], [18] methods. Human activities have
a hierarchical structure and include diverse verbs, so it is
hard to deﬁne an explicit organization for their categories.

[12], video-based [13],

[15],

[14],

ad<feedcat><jumpsnowboard><eatpizza>To eliminate the activitysemantics?To eliminate the objectsemantics?<pizza><snowboard><cat>cbThrow FrisbeeImage Space Semantic Space DNN<hand-throw-sth><arm-swing><head-look_at-sth><foot-stand><leg-bend><frisbee>DNNIntuitive (System 1)Logical (System 2)∧∨¬∧Throw FrisbeeImage Space Semantic Space PrimitiveSpace ReasoningTouchTwistWearPressHoldCarrySqueezePinchPushHand Stateshead-eat-sthhand-hold-sthapple<eatapple>∧→∧<rideski_board>leg-bendarm-swingfoot-step_on-sthski_board∧∧∧→<ridemotorcycle>hand-hold-sthmotorcyclefoot-step_on-sthhip-sit_on-sth∧∧∧→Primitive Programming with Logical Rulesaband embed parts appearance as nodes, and uses them with
object feature and pose to predict the HOIs. Previous works
mainly utilized the part appearance/location, but few studies
tried to divide the instance actions into discrete part-level se-
mantic tokens, and refer to them as the basic components of
activity concepts. In comparison, we aim at building human
part semantics as the reusable and transferable primitives.
Furthermore, a neural-symbolic reasoning system is built to
program primitives into activity concepts.
Neural-Symbolic Reasoning. The integration of neural
models with logic-based symbolic models provides an AI
system capable of bridging lower-level information process-
ing (for perception and pattern recognition) and higher-level
abstract knowledge (for reasoning and explanation) [47]. In
neural-symbolic computing, knowledge is represented in
symbolic form, whereas learning and reasoning are com-
puted by a neural network. Recently, researchers combined
neural-symbolic reasoning with visual question answer-
ing (VQA) task, and proposed methods including neural
module networks [48], neural state machine [49], symbolic
program execution on structural scene representation [50],
neuro-symbolic concept learner [51]. Besides, [52] proposed
neural logic machines for relational reasoning and decision-
making tasks. Inspired by this line of methods, we use Deep
Neural Networks (DNN) to represent the activity primitives
as symbols meanwhile borrow the power of symbolic rea-
soning to use logic rules to program these primitives.

3 BOTTLENECK OF CANONICAL DIRECT MAPPING
Firstly, we conduct an experiment to demonstrate that the
canonical direct mapping method suffers from severe per-
formance bottleneck problems. We choose a canonical action
detection method TIN [31] as a representation of the direct
mapping methods. Then, we train it with a different number
of images from HAKE data and evaluate its performance
on HICO-DET [1] test set. For each run, the TIN model
is trained for 30 epochs using an SGD optimizer with a
learning rate (LR) of 0.001 and a momentum of 0.9. The
ratio of positive and negative samples is 1:4. A late fusion
strategy is adopted. For HICO-DET, we adopt the Default
mAP metric as proposed in HICO-DET [1].

The result is illustrated in Fig. 1b. It shows that, in
the beginning, the increasing amount of data indeed helps
boost the performance of the direct mapping method in
a nearly linear manner. However, as more and more data
get involved, the performance gain gets less signiﬁcant and
rapidly saturates. This indicates that it is hard for canonical
direct mapping methods to make the best use of the in-
creasing data without the help of knowledge and reasoning.
Even providing more labeled data, the direct mapping may
not achieve the success of object detection on MS-COCO [2]
(CNN-based, 55+ mAP) given the same magnitude (105)
of training images. Besides, activity images are also much
more difﬁcult to annotate than object images, considering
the complex patterns and ambiguities of human activities.

4 METHOD
4.1 Overview

As depicted in Fig. 3, HAKE casts activity understanding
into two sub-problems: (a) knowledge base building, (b)

3

neuro-symbolic reasoning.

4.1.1 Building Human Activity Knowledge Base

Firstly, HAKE should detect primitives “unconsciously” like
System 1 [6]. Thus, we built a knowledge base including
abundant activity-primitive labels. To discover and deﬁne
primitives, we conduct a beforehand user study: given
activity images/videos, participants should give decisive
primitives for activities from an initial primitive dictio-
nary including 200 human body Part States (PaSta [5]), 80
common objects [2], and 400 scenes [54]. Each primitive is
depicted as a phrase token, e.g., hand-touch-sth, chair, class-
room. This dictionary grows as the annotation progresses,
and participants can supplement primitives to better explain
their decisions. After annotating 122 K+ images and 234 K+
video frames [13] covering 156 activities, we found that: 1)
most participants believed body parts were the foremost ac-
tivity semantic carriers. 2) PaSta classes were limited. After
exhaustively checking 26.2 M+ manually labeled PaSta, only
approximately 100 classes were prominent. 3) Object some-
times makes a small difference, e.g., in inspect-sth, touch-sth.
4) Few participants believed that scene always matters, e.g.,
though rarely, we can play football in living room.

In light of these, our dictionary would contain as many
cues as possible and leaves the choice of use to reasoning
policy. Though scalable, it already contains enough primi-
tives to compose common activities, so we usually do not
need to supplement primitives for new activities. In total,
HAKE includes 357 K+ images/frames, 673 K+ persons,
220 K+ object primitives, and 26.4 M+ PaSta primitives.
With abundant annotations, a simple CNN-based detector
detects primitives well and achieves 42.2 mAP (Sec. 5.3.1).

4.1.2 Constructing Logic Rule Base and Reasoning Engine

{

{

}

Pi

Am

N
i=1 and activities Act =
}

Second, to determine activities, a causal protocol consisting
of a logic rule base and a reasoning engine is proposed.
After detecting primitives, we use a DNN to extract vi-
sual and linguistic representations to represent primitives
M
P ri =
m=1. As inter-
pretable symbolic reasoning can capture causal primitive-
activity relations, we leverage it to program primitives
following logic rules. A logic rule base is initialized to
import common sense: participants are asked to describe
the causes (primitives) of effects (activities). Each activity
has initial multi-rule from different participants to ensure
diversity. For example (Fig. 3b), Pi, Pj, Pk represent head-
read-sth, hand-hold-sth, newspaper and Am indicates read news-
paper, a rule is expressed as Pi
:
∧
: implication). Pi, Pj, Pk, Am are seen as events that
AND,
are occurring/True or not/False. When Pi, Pj, Pk are True
simultaneously, Am is True. For simplicity, we turn
∧
y.
are implemented as
into
∨
→
¬
functions N OT (
) with Multi-Layer Perceptrons
·
·
(MLPs) that are reusable for all events. We set logic laws
(idempotence, complementation, etc.) as optimized objec-
tives [55] imposed on all events to attain logical operations
via backpropagation. Then, the expression output is fed into
a discriminator to estimate the true probability of an event.
Given a sample, multi-rule predictions of all activities
are generated concurrently. We use voting to combine multi-
decision into the ﬁnal prediction via multi-head atten-

y
x
⇔ ¬
), OR(

Am (

via x

,
→

,
∨

and

Pk

Pj

→

→

,
·

¬

∧

∧

∨

4

Fig. 3: HAKE Overview. a. We cast activity understanding into: a(1) Knowledge base construction: annotating large-scale activity-primitive
labels to afford accurate primitive detection. a(2) Reasoning: Given detected primitives, adopting neuro-symbolic reasoning to program them
into semantics. b. Detailed pipeline. b(1) Primitive detection and Activity2Vec. Given an image, we utilize the detectors [27], [28] to locate the
human/object and human body parts. Then we use a simple CNN model together with Bert [53] to extract the visual and linguistic features of
primitives via primitive detection. b(2) Primitive-based logical reasoning. With the two kinds of representations from Activity2Vec, we operate
logical reasoning in neuro-symbolic paradigm following the prior and auto-discovered logic rules. Here, N OT (·) and OR(·, ·) modules are shared
by all events but drawn separately here for clarity.

tion [56]. Besides, to better capture the causal relations, we
propose an inductive-deductive policy to make the rule
base scalable instead of using rule templates or enumerating
possible rules [57]. Although annotating rules is much eas-
ier than annotating images/videos, we can discover novel
rules with the initial prior rules (Sec. 4.4): 1) Inductively
concluding rules from observations. As activity-primitive
annotations can be seen as rule instantiations, we randomly
select them as rule candidates or heuristically generate
candidates according to the human prior rule distribution. 2)
Deductively evaluating rule candidates in practical training.
Good candidates inducing minor losses are selected via
backpropagation since they are more compatible and gen-
eral with various contexts. They are updated into the rule
base and in turn boost the performance. The bidirectional
process can robustly discover rules and is less insensitive to
the lack of activity annotation shortage than direct activity
classiﬁcation. Finally, we discover 4,090 rules for 156 activi-
ties. From the experiments, we ﬁnd that HAKE requires only
several rules for each activity to perform well.

Next, we detail our framework in Sec. 4.2-4.5.

4.2 Primitive Detection

Given an image, detected body part boxes, and object boxes
, bp, bo (with object classiﬁcation), we operate primitive
I
detection following Fig. 3b. We assume the number of
body parts is m. In detail, we ﬁrst get the part features
f (i)
and object feature fo via ROI pooling from a feature
p

F

extractor
(Faster R-CNN [27] pre-trained on COCO [2]
for image and 3D convolution backbone [14] pre-trained on
Kinetics [14] for video). For body-only motions [13] (e.g.,
walk, run) that are not involved with objects, bo is replaced
with the whole image coordinates. That is, we input the
whole image feature fc as fo. Besides, fc can also represent
the scene primitive. Then, we use these features to predict
the part relevance indicating the contribution of a body part
for recognizing an activity. For example, feet usually have
weak correlations with drink with cup. And in eat apple, only
hands and head are essential. For each part, we concatenate
the part feature f (i)
p and object feature fo from bo
p
as the inputs. All features will be input to a part relevance
), which contains FC layers and Sigmoids,
pa(
predictor
·
P
m
i=1 (ai
ai
getting a =
}
{

[0, 1]) following

from b(i)

∈

f (i)
p }

m
i=1, fo).

P

(1)

a =

pa(
{
The relevance/attention labels can be converted from prim-
itive labels directly, i.e. the attention label will be one unless
is no activity, which means this part
its primitive label
contributes nothing to the inference. Next, we perform
primitive classiﬁcation. For each part, we concatenate f (i)
p
with fo and input them into a max pooling and FC layers,
thus obtain the primitive score

(i)
P ri for the i-th part:
S
(i)
P ri(f (i)

p , fo).

(2)

P
Because a part can have multiple states, e.g. head performs
eat and watch simultaneously, we use multiple Sigmoids

(i)
P ri =
S

Human PriorLogic Rule Base𝑃𝑖∧𝑃𝑗∧𝑃𝑘→𝐴𝑚𝑃𝑖=<hand-hold-sth>𝑃𝑗=<head-eat-sth>𝑃𝑖=<apple>𝐴𝑚=<eatapple>Example:𝑃𝑖∧𝑃𝑗∧𝑃𝑘→𝐴𝑚𝑁𝑂𝑇(⋅)𝑃𝑖¬𝑃𝑖𝑂𝑅(⋅,⋅)¬𝑃𝑖¬𝑃𝑗¬𝑃𝑖)∨(¬𝑃𝑗Neuro-Symbolic Reasoning EngineBinaryClassifierTrueProbabilityFalseProbability(¬𝑃𝑖)∨(¬𝑃𝑗)∨(¬𝑃𝑘)∨𝐴𝑚=𝑇𝑟𝑢𝑒(¬𝑃𝑖)∨(¬𝑃𝑗)∨(¬𝑃𝑘)∨𝐴𝑚Activity PredictionLogic RulesRawDataParticipantsActivity-Primitive Knowledge BaseDeductionInductionPrimitive DictionaryaRaw Activity Data122K+ Images & 299 VideosPrimitive DictionaryBody Part States (WordNet)Objects (COCO)ParticipantsActivity-Primitive Knowledge BasebCNNsPart FeaturePart State Detectionhead-inspect-newspaperarm-close_to-newspaperhand-hold-newspaper…BertHeadArmsHandsHipLegsFeetObjectFeature𝑓𝑝(𝑖)𝑓𝑜Part AttentionPrimitive Detector𝑓𝑃𝑟𝑖𝐿𝑓𝑃𝑟𝑖𝑉𝑓𝑝𝑓𝑝∗Tokens𝑃𝑃𝑟𝑖Logic Rule BaseActivity PredictionCombine multiple rules𝑃𝑖∧𝑃𝑗∧𝑃𝑘→𝐴𝑚𝑃𝑖=<hand-hold-sth>𝑃𝑗=<head-inspect-sth>𝑃𝑖=<newspaper>𝐴𝑚=<readnewspaper>Example:BinaryClassifierTrueProbabilityFalseProbability𝑁𝑂𝑇(⋅)𝑃𝑗¬𝑃𝑗𝑁𝑂𝑇(⋅)𝑃𝑘¬𝑃𝑘𝑁𝑂𝑇(⋅)¬𝑃𝑖𝑃𝑖𝑂𝑅(⋅,⋅)𝑂𝑅(⋅,⋅)𝐴𝑚𝑂𝑅(⋅,⋅)Transforming𝑃𝑖∧𝑃𝑗∧𝑃𝑘→𝐴𝑚to(¬𝑃𝑖)∨(¬𝑃𝑗)∨(¬𝑃𝑘)∨𝐴𝑚=𝑇𝑟𝑢𝑒Primitive-based Logical ReasoningHeadArmsHandsHipLegsFeetto operate the multi-label classiﬁcation. For part relevance
prediction and primitive classiﬁcation, we construct cross
entropy losses L(i)
P ri for each part. Formally, for a
pair (HOI) consists of a person and an interacted object or a
single person (body-only motion), the ﬁnal loss

att and L(i)

P ri is

P ri =

L

m
(cid:88)

i

(i)
P ri +

(
L

L

(i)
att).

4.3 Activity2Vec

L

(3)

As mentioned before, we deﬁne the primitives according
to the most common activities via crowdsourcing. That is,
choosing the part-level verbs which are most often used to
compose and describe the activities by a large number of an-
notators. Therefore, primitives can be seen as the fundamen-
tal components of activities. Meanwhile, primitive detection
performs well. Thus, we can utilize primitive detection
based on HAKE to learn primitive representation with good
transferability. They can be used as “symbols” to reason out
the ongoing activities in both supervised and transfer learn-
ing. Here, the primitive representation extractor is called
as Activity2Vec. Under such circumstances, HAKE works
like ImageNet [58] as the knowledge base. And the HAKE
pre-trained Activity2Vec functions as a knowledge engine to
transform low-level activity patterns to the primitive space
for subsequent primitive programming.
Visual Primitive Representation. First, we extract visual
primitive representations from primitive detection. We ex-
(i)
m
)
P ri(
tract the feature from the last FC layer in
i=1 as
}
·
m
the raw primitive visual representation
i=1 for each
part. The predicted part relevance a provides cue on how
important a part is to the activity. To utilize this cue, we
further use a to re-weight the raw primitive visual represen-
tation to generate the primitive visual representation f V
f V ∗(i)
P ri

{P
f V ∗(i)
P ri }

f V
P ri =

P ri:

(4)

ai

{

{

m
i=1.
}

∗

Linguistic Primitive Representation. To further enhance
the representation ability, we utilize the uncased BERT-
Base pre-trained model [53] to represent primitives as
event vectors. Language priors are useful in visual con-
cept understanding [59], [60]. Thus the combination of
visual and language knowledge is a good choice for es-
tablishing this mapping. Bert [53] is a language under-
standing model that considers the context of words and
uses a deep bidirectional transformer to extract contex-
tual representations. It is trained with large-scale corpus
databases such as Wikipedia, hence the generated embed-
ding contains helpful implicit semantic knowledge about
the activity and primitive. For example, the description of
the entry basketball in Wikipedia: “drag one’s foot with-
out dribbling the ball,
to carry it, or to hold the ball
with both hands...placing his hand on the bottom of the
ball;..known as carrying the ball”. In speciﬁc, if there are
n primitives in total, we reform each primitive into tokens
n
v, tj
p, tj
tj
part class, verb class, object class
j=1, e.g.,
,
o}
(cid:105)
{
(cid:104)
object class
comes from object detection. Then we
where
(cid:105)
(cid:104)
get primitive linguistic representation f L
v, tk
o)

f L
P ri =

Bert(tk

p, tk

(5)

P ri as:
n
k=1.
}

{

Also each activity is converted to f k
fashion, where “part class” is replaced by “human” as:

Act following the above

5

fAct =

Bert((cid:48)human(cid:48), tk
{

v, tk
o)

n
k=1.
}

For body-only motions, we use
can also add

scene class
(cid:104)
(cid:105)

part class, verb class
(cid:105)
(cid:104)

if necessary.

(6)

. We

(

y

x

∨

∨

∨

∨

∧

→

→

Pi)

and

∧
∨

¬
¬

→
Pk

⇔ ¬

P ri, eL

Pj)
and

Primitive-Based Logical Reasoning. We utilize the prim-
itive representations from the Activity2Vec as symbols to
implement logical reasoning. Both the linguistic and vi-
sual representations are used. Firstly, all representations
P ri, f V
f L
P ri, fAct are mapped into the a lower-dimensional
event space eV
P ri, eAct with MLPs. The dimension re-
duction can accelerate the learning process without too
many losses on performance. As mentioned before, we
via x
y. Hence,
turn all
into
¬
Am is converted
Pj
logical expression like Pi
to (
Am. Then we just need logical
Pk)
(
¬
∨
¬
to construct all logic rules to reduce
operations
the learning difﬁculty of logic operation. Each time OR(
)
,
·
·
takes two compressed event vectors as the inputs and gen-
erates one output event vector representing the conjunction
of two input events. This output vector can then be input to
) again together with another event vector to operate
OR(
·
) takes one compressed
the subsequent conjunction. N OT (
·
event vector each time and outputs its opposite event vector.
In practice, we use probability to measure the existence of
primitive events. (
Am tends to be
Pj)
¬
true if and only if Pi, Pj, Pk all have high probabilities
of occurrence. As eV
P ri is naturally probabilistic extracted
from DNN, we directly use it in logic reasoning. As for
eL
P ri transformed from the (happening) event truths like
hand, hold, cup
that is deterministic, we cannot directly
(cid:105)
(cid:104)
use it. Naturally, the expectation of linguistic representation
is appropriate. For a primitive event Pi represented as eL
with probability
event is
1

P ri
P ri (from the Activity2Vec), its opposite
P ri) with probability

Pi represented as N OT (eL

P ri. Thus the expectation is

(
¬

(
¬

Pk)

Pi)

,
·

∨

∨

∨

¬

S

P riS

eL(cid:48)
P ri = eL

P ri + N OT (eL
− S
Besides, for visual representation, eV (cid:48)
P ri. Hereinafter,
P ri or eV (cid:48)
P ri to refer to eL(cid:48)
we use e
P ri for clarity. As for the
target activity Am, we use the linguistic representation truth
fAct (like
) from Eq. 6 in logic reasoning.
(cid:105)

human, eat, apple
(cid:104)

P ri)(1
P ri = eV

P ri).

(7)

(cid:48)

− S

}

∈

ri
{

Rm, assume its expression as: Pa

For the m-th action category, we get its rule set as
lm
i=1, where lm is the rule number. For the i-th rule
Rm =
ri
Am, where
Pa, Pb is the a-th, b-th primitive, and Am is the m-th activity.
human-play-ball. Then
For example, hand-catch-sth
ball
Am, and
we transform the expression into (
calculate the vote vector via
(cid:48)
P ria

= OR(OR(N OT (e

)), eActm).

), N OT (e

(cid:48)
P rib

→
¬

e(i)
clsm

Pa)

Pb)

Pb

(8)

→

∧

¬

∨

∨

∧

(

The vector e(i)
on the i-th rule. All OR(
·
but written separately for clarity.

clsm implies the result of logical reasoning based
) are shared functions
), N OT (
·
·

,

We explore two different ways to synthesize the rules:
early combination and late combination. For early combina-

tion, we get a new embedding e(i)(cid:48)

clsm for each rule via

e(i)(cid:48)
clsm

=

lm(cid:88)

j=1

att(i)

j ∗

e(j)(cid:48)
clsm

,

logic rule base is ﬁrst initialized with the human prior logic
rules. Additionally, some rule candidates are selected to
further enrich our rule base. They mainly have two sources:
1) some of the annotated primitive-activity sets from the
HAKE train set, 2) automatically generated rules.

(9)

6

S

(i)

clsm from e(i)
S

where the attention scores are obtained by Multi-Head
Attention strategy [56]. Then we concatenate all e(i)(cid:48)
clsm and
map it into an aggregated event vector eclsm via MLP, and
get action inference score
clsm with a binary discriminator
transforming eclsm to probability. For the late combina-
tion, the process is the opposite. We ﬁrst get the action
clsm . Since different persons may
prediction
have different understanding to activities, we synthesize the
(i)
inference from different rules and average
clsm to get the
ﬁnal result
clsm . These two policies are used differently. In
rule discovery and updating (Sec. 4.4), we utilize the late
combination. Then, we freeze the updated rule base and
ﬁnetune the whole reasoning module via early combination.
)
To perform logical operations, logical modules N OT (
·
) should satisfy the basic laws of logic: 1) NOT
TRUE = FALSE, 2) NOT double negation:
x = x, 4) OR comple-
refers to the
is the event vector space. We utilize the

¬¬
mentation: x
(
¬
event vector, and
X
laws as the regularization objective via

x = x, 3) OR idempotence: x

x) = TRUE, where x

and OR(
·
negation:

∈ X

·
¬

∨

∨

S

S

,

reg =

L

4
(cid:88)

(cid:88)

i=1

x∈X

(
S

ia

− S

ib)2,

(10)

where i indicates the i-th logic law. In detail,

ia,

S

J

(N OT (x)),
1a =
S
(N OT (N OT (x))),
J
(OR(x, x)),
3a =
S
(OR(x, N OT (x))),
J

J

2a =

S

4a =

S

− J
(x);
(x);

1b = 1
2b =
J
3b =
J
4b = 1,

S
S
S
S

ib are

S
(x);

(11)

Since the logic laws are universal despite input and out-
put, we impose regularizations on all event representations
x
related to logical modules, including primitive,
intermediate, and ﬁnal event vectors.

∈ X

Therefore, the loss of reasoning is constructed by two
parts. First, the modules must follow the logical laws, so
reg for all event vectors.
we use the logical regularizer loss
Second, the modules should be able to perform accurate
reasoning, so the classiﬁcation loss
cls is essential. The ﬁnal
L
loss for this module is constructed as

L

LR = α

L

reg +

cls,

L

∗ L

(12)

where α is a hyperparameter adjusting the ratio.

4.4 Updating and Evaluating Logic Rules

As illustrated above, the collected initial rules are utilized
as the seeds to activate our reasoning engine. Nevertheless,
they are concept-level and have a limited amount. Thus,
simply using them to construct the rule base may lack
ﬂexibility and generalization. Hence, more qualiﬁed logic
rules are essential for practical reasoning.

To this end, we propose an inductive-deductive policy to
maintain and update our logic rule base. During the reason-
ing, our rule base is not ﬁxed but scalable and renewable. The

count
Rm
{
R∗
{

Algorithm 1 Evaluating and Updating logic rules
Input: Train set T, model parameter Θ, maximum rule
max(lm)), selected rule set R =
l0(l0
≥
lm
m=1, candidate rule set R∗ =
A
A
ri
m=1 =
i=1}
}
{{
}
l∗
r∗
A
A
m=1 =
m=1 (A: activity category count)
m
m}
i }
i=1}
l0
A
Rm
Output: New rule set R=
m=1
i=1}
{
[1, A] do
1: for each m
R∗
2:
i ∈
candidate L(i)
for each rj

∈
for each r∗

A
m=1=
}

(i)
clsm

3:
4:

{{

{{

ri

}

m do
m =
L
Rm do
m =

∈

5:
6:
7:

8:
9:
10:
11:

12:
13:
14:

m < selected L(k)

m then

selected L(j)

(j)
clsm

L

end for
k = argmaxj(selected L(j)
m )
if candidate L(i)
if lm < l0 then
lm = lm + 1
rlm = r∗
i
selected L(lm)

m =candidate L(i)
m

else

rk = r∗
i
selected L(k)

m = candidate L(i)
m

end if

15:
16:
17:
18:
19: end for
20: return R

end for

end if

First, as the annotated activity-primitive sets of im-
ages/videos are instantiated rules (i.e., abstract rules and
happened cases), some of them may be general and ﬁt other
scenarios. Hence, we may ﬁnd out good rules from the
annotations. In practice, we randomly sample the annotated
activity-primitive sets as the candidates and convert them
into the standard format of the logic rule.

Second, we can also automatically generate rule can-
didates with our primitive dictionary. As we deﬁne about
200 PaSta primitives before, thus the possible rules for one
activity can be exaggerated 2200. Fortunately, each activity
has its own primitive bias and just very few possible prim-
itive compositions are suitable. In our implementation, for
the m-th activity, we ﬁrst count the co-occurrence between
p
all primitives and it and get Cm =
i=1, where p is
}
the number of primitives and i is the primitive index.
Then, we impose min-max normalization on Cm and get
C
i indicates the occurring probability of
i}
the i-th primitive. C
m is the prior knowledge for primitives
activity relations and can be regarded as a rule generator.
i). Here,
{
) is the Bernoulli distribution. To generate diverse rules,
(
·

B
we adopt a different generator based on the prior as

→
It can generate rules as rm =

p
i=1, rmi
}

p
i=1, where c

m =

ci
{

∼ B

c
{

rmi

(c

(cid:48)

(cid:48)

(cid:48)

(cid:48)

(cid:48)

and we randomly make c

(cid:48)(cid:48)

C

m(β) =

(cid:48)(cid:48)

{

i (β)
c
}
i (β) = (1

(cid:48)(cid:48)

p
i=1

(13)

(cid:48)(cid:48)

(cid:48)
i or c
c

i (β) = c

i +

(cid:48)

β)

∗

−

(cid:48)

c

(1

∗
∈

i) to decrease or increase the primitive probabilities.
β
−
β
[0, 1] is a parameter controlling the “distance” between
the original and transformed generators, since Kullback-
(cid:48)(cid:48)
Leibler divergence KL(
(c
i)
i (β))) increases as β
increases. In practice, we set β = 0.1
[0, 10] and
get 11 different generators. We generate 5 rules from each
generator and obtain 55 rules for each activity category.

θ for θ

(cid:107) B

(c

∈

B

·

(cid:48)

After extending the rule base with the candidates, we
operate logic rules evaluation and update the base after each
training epoch. The procedure is illustrated in Algorithm 1.
The performances of the selected and candidate rules are
evaluated separately. The rule with a smaller classiﬁcation
loss is assumed to perform better since it ﬁts more activities
and contexts better. For each activity, the candidate rules
that outperform the selected rules will be added to the rule
base. Once the number of all selected rules lm reached a
given upper limit l0, the rules with the worst performances
in the selected set will be removed (Line 14 of Algorithm 1).
Through the evaluation and updating, we maintain a scal-
able rule base to better ﬁt the practical scenarios.

4.5 Uniﬁed Activity Inference

Though we leverage a different mechanism to infer activities
based on primitive representations, our method is not ex-
clusive to previous activity understanding methods [5], [13],
[30], [31], [38], [59] adopting instance-level representations of
visual entities (human instances). Thus, as a plug-and-play,
we can ﬂexibly integrate our primitive-level representation
with the instance-level representation. We use Eq. (14) to
indicate the result from the instance-based method:

inst =

S

inst(

I

F

, bh, bo),

(14)

F

inst(

) can be any instance-based methods [13], [30],
where
·
[31], [38]. The loss of instance-level result is denoted as
inst. To obtain the ﬁnal result, we adopt the late fusion
L
strategy, i.e., compactly fusing the predictions of two levels
following

LR. The total loss is constructed as

inst

=

S

∗ S
S
total =

L

LR +

L

L

inst +

P ri,

L

(15)

where primitive detection can be simultaneously ﬁne-tuned
with our uniﬁed activity inference.

5 EXPERIMENTS
5.1 Datasets and Metrics

We conduct experiments on several large-scale benchmarks
including HICO-DET [1], AVA [13], V-COCO [12], and
Ambiguous-HOI [61]. HICO-DET [1] is a benchmark built
on HICO [11] and provides human-object boxes, which
contains 47,776 images and 600 interactions. AVA [13] con-
tains 430 videos with spatio-temporal labels. It includes
80 atomic activities consisting of body motions and HOIs.
V-COCO [12] contains 10,346 images with instance box
annotations. It has COCO 80 objects [2] and 29 activity
categories. Ambiguous-HOI [61] contains 8,996 images with
25,188 annotated human-object pairs in 87 HOIs (consist-
ing of 48 verbs and 40 objects from HICO-DET [1]). It is
designed to specially examine the ability to process 2D
pose and appearance ambiguities. For HICO-DET [1], V-
COCO [12], and Ambiguous-HOI [61], we follow the mAP

7

metric: a true positive contains accurate human and object
boxes (IoU > 0.5 regarding GT) and accurate interaction
prediction. For a fair comparison, we use ResNet-50 [62]
as the backbone for HAKE and all methods together with
their object detectors. For AVA [13], we follow its frame-
mAP metric: a true positive contains an accurate human box
(IoU > 0.5 regarding GT) and accurate activity prediction.
Similarly, for fairness, we use object detection from LFB [63]
and SlowFast [64] for all methods. Besides, for PaSta primi-
tive detection, we adopt the same metric with [1].

5.2 Implementation Details

For enhancing experiments in Sec. 5.3.2, we use all HAKE
image data for HICO-DET and all video data for AVA (the
data in the HICO-DET test set and AVA validation set
together with the corresponding labels are all excluded to
avoid data pollution). We use instance-level primitive labels:
each annotated person with the corresponding primitive
labels, to train the primitive detector and Activity2Vec
and then ﬁne-tune the primitive detector, Activity2Vec,
and primitive-based logical reasoning module together on
HICO-DET [1] or AVA [13] train set respectively. When
training the reasoning module, the primitive detector is
frozen. The pre-training and ﬁne-tuning take 1 M and 2 M
iterations respectively. The learning rate (LR) is 1e-3 and
the ratio of positive and negative samples is 1:4. We use
an SGD with momentum (0.9) and cosine decay restart (the
ﬁrst decay step is 80 K). The reasoning module is trained for
5 epochs using Adam with a LR of 1e-3. And a late fusion
strategy is adopted. In Eq. 12, α = 0.2.

S

=

We detail the settings of transfer learning as follows.
On V-COCO [12] in Sec. 5.3.3, we exclude the images of
V-COCO and the corresponding primitive labels in HAKE
and use the remaining data (109 K images) for pre-training.
We use an SGD with 0.9 momenta and cosine decay restarts
(the ﬁrst decay is 80 K). The pre-training costs 300 K
iterations with the LR as 1e-3. The ﬁne-tuning costs 80
K iterations with the LR as 7e-4. To enhance the transfer
effect, besides the primitive logical reasoning result and
the instance-level result (from instance-level methods), we
also include a perceptual reasoning result. That is, with the
f V
P ri extracted with Activity2Vec, we feed it to an MLP
and perform the activity classiﬁcation, getting the perceptual
P R. Thus, the ﬁnal result is formulated as
reasoning result
P R. On Ambiguous-HOI [61], we use the
LR
inst
S
same model and setting used in the enhancing experiment
on HICO-DET [1]. On image-based AVA [13], as HAKE is
built upon still images, we use the frames per second as still
images for image-based activity detection. We adopt ResNet-
50 [62] as the backbone and an SGD with momentum of 0.9.
The initial LR is 1e-2 and the ﬁrst decay of cosine decay
restarts is 350 K. For a fair comparison, we use the human
boxes from LFB [63]. The pre-training costs 1.1 M iterations
and ﬁne-tuning costs 710 K iterations. Besides, we also
adopt an image-based baseline: Faster R-CNN detector [27]
with ResNet-101 [62] provided by the AVA website [65]. Due
to the huge domain gap, to enhance the transfer effect, we
used a similar strategy to that for V-COCO [12]: besides the
logical reasoning result and instance-level result, we include
P R, getting the ﬁnal result
the perceptual reasoning result
following

S
×S

S
P R.

×S

inst

LR

=

S

S

× S

× S

All
45.52
62.65
71.69
17.03

Rare
47.60
71.03
82.25
13.42

8

23.22 (

72.6%)
36.3%) 23.16 (
↑
17.21

↑
23.63

28.30 (

45.5%)
19.8%) 25.04 (
↑
21.85

↑
29.07

Dataset Method

QPIC [41]+GT-HAKE (detection [41])
GT-HAKE (GT H-O boxes)
GT-HAKE+rule search (GT H-O boxes)
TIN [31]
TIN [31]+HAKE
VCL [34]
VCL [34]+HAKE
QPIC [41]
QPIC [41]+HAKE
SlowFast [64]+GT-HAKE (detection [64])
GT-HAKE (GT H boxes)
LFB-Res-50-max [63]
LFB-Res-50-max [63]+HAKE
SlowFast [64]
SlowFast [64]+HAKE

HICO-DET

AVA

32.10 (

26.8 (

↑
42.23
47.27
23.9

26.2%)
10.4%) 27.57 (
↑
30.86
34.47
5.2
48.08%)
12.13%) 7.7 (
↑
9.6

↑
28.2

29.3 (

8.33%)
3.90%) 10.4 (
↑

↑

TABLE 1: Results (mAP, %) of enhancing experiments on HICO-DET [1]
and AVA [13]. The relative improvements are shown in the brackets.
“GT-HAKE” means inputting GT primitive to reasoning module. “de-
tection” means using the human (-object) boxes from the detector. “GT
H-O/H boxes” means inputting the GT human (-object) boxes.

Dataset

V-COCO [12]

Ambiguous-HOI [61]

AVA [13]

59.7 (

Method
TIN [31]
TIN [31]+HAKE
TIN [31]
TIN [31]+HAKE
DJ-RN [61]
DJ-RN [61]+HAKE 12.68 (
AVA-TF [13]
AVA-TF [13]+HAKE 15.6 (

10.56 (

mAP
54.2

10.1%)
↑
8.22

28.5%)
↑
10.37

22.3%)
↑
11.4

36.8%)
↑

TABLE 2: Transfer learning results on V-COCO [12], Ambiguous-
HOI [61], and image-based AVA [13] (v2.1).

5.3 Results and Comparisons

5.3.1 Primitive Detection

With extensive primitive annotations, a concise CNN-based
primitive detector (ResNet-50) can perform very well. On
HICO-DET [1], given the same detected human-object
boxes [30], our primitive detector achieves 30.51 mAP (28.37
(head), 38.18 (arms), 19.48 (hands), 42.06 (hip), 23.84 (legs),
31.12 (feet)) and greatly outperforms the instance-level ac-
tivity detection performance of state-of-the-art [34] with
ResNet-50 (19.43 mAP). If inputting GT human-object boxes,
our primitive detector achieves impressive 42.22 mAP. This
veriﬁes that primitives can be well learned with the help
of our activity-primitive knowledge base. Similarly, on the
image-level HICO [11], the performance gap between prim-
itive and activity recognition is also large. HAKE achieves
55 mAP on primitive recognition and an activity recognition
state-of-the-art [44] achieves 40 mAP.

5.3.2 HAKE-Enhanced Activity Detection

We conduct the challenging instance-level activity detection
on HICO-DET [1] and AVA [13] which needs to locate active
humans/objects and classify activities simultaneously, to
show the enhancing effect of HAKE. State-of-the-arts are
chosen to compare and cooperate with HAKE.
HICO-DET. Results are shown in Tab. 1. On HICO-DET [1],
HAKE signiﬁcantly boosts previous instance-level methods
especially on the Rare sets, bringing 9.74 mAP improve-
ment upon TIN [31] and strongly proving the efﬁcacy of
learned primitive information. Moreover, GT-HAKE (the
upper bound of HAKE with perfect primitive detection)
outperforms the state-of-the-art signiﬁcantly, indicating the
potential of our logical reasoning. Besides, on the image-
level activity recognition benchmark HICO [11]), we achieve
higher than 50 mAP on 206 image-level HOIs (206/600).

Fig. 4: Positive correlation between the primitive detection quality
and activity detection performance (w/o uniﬁed inference) on HICO-
DET [1]. Detected boxes means the detection from iCAN [30].
AVA. Meanwhile, on the AVA [13] validation set, HAKE also
brings signiﬁcant improvements over previous methods. It
boosts the detection performance of a considerable number
of activities, especially the 20 most rare activities.

More detailed results are provided in Tab. 6, 7 (ap-

pendix).

5.3.3 Evaluating the Transfer Ability of HAKE

In three challenging transfer learning experiments, HAKE
also shows efﬁcacy. Results are shown in Tab. 2.
V-COCO. We select state-of-the-arts as baselines and adopt
the metric AProle [12] (requires accurate human-object
boxes and activity prediction). With domain gap, HAKE still
brings 5.5 mAP (10.1%) improvement upon TIN [31].
Ambiguous-HOI. On more difﬁcult Ambiguous-HOI,
HAKE also improves the performance by 2.34 and 2.31 mAP
(28.5% and 22.3% relative improvements) upon [31], [61].
Image-based AVA. Both image- and video-based methods
cooperating with HAKE achieve impressive improvements,
even when our model is trained without temporal informa-
tion. Considering the huge domain gap (movies in AVA [13])
and unseen activities, these results strongly prove the gener-
alization ability of HAKE again.

More detailed results are provided in Tab. 8, 9, 10 (ap-
pendix). We also visualize some activity cases to exhibit the
generalization of HAKE in Fig. 13 (appendix).

5.3.4 Analyzing the Upper Bound of HAKE

Next, we reveal the potential of HAKE:

∗

1) Ground Truth (GT) Primitive. We input GT primi-
tives with random noise to the reasoning engine, i.e., less
noise indicates better primitive detection. Given the ra-
[0, 1] and totally n pairs in the test set, we
tio mr
∈
randomly choose n
mr pairs and replace the GT prim-
itives with evenly distributed noise. In practice, we set
mr = 0, 0.005, 0.01, 0.05, 0.1, 0.2, 0.5 and calculate primi-
tive/activity mAP. As shown in Fig. 4, less noise (better
primitive detection) results in better activity detection. In
Tab. 1, on the challenging HICO-DET [1], the upper bounds
are 45.52 (+QPIC [41], detection [41]) and 62.65 (GT human-
object boxes) mAP, which are signiﬁcantly superior to the
state-of-the-arts (about 29 mAP [41] and 44 mAP [38]). Here,
detection [41] indicates using the detected human-object
boxes from [41]. On AVA [65], the upper bound of HAKE
is also impressive, i.e., 42.23 (+SlowFast [64], detection [64])
and 47.27 (GT human boxes) mAP, which also largely out-
perform the SlowFast [64] (about 28 and 34 mAP).

020000400006000080000100000#Training samples363840424446Accuracy (%)Baseline-1Baseline-2HAKEGT-HAKE0.00.20.40.60.8Accuracy (%)0.40.50.60.9Human20304050607080Primitive mAP (%)102030405060Activity mAP (%)with detected boxeswith GT boxes0100101102103104105106Scales of searched rules (million)505560657075808590Performance (%)mAPAccuracy0.00.20.40.60.8SCR010203040506070NumberHAKEHuman9488907391897793854065702376297955596062697857924664758187483043838037Activity category0.00.10.20.30.40.50.60.7Accuracy (%)HumanHAKEridecarryholdeatwalkwashHAKEHAKEHAKEHAKEHAKEHAKEHAKEHAKEHAKEHAKEHumanHumanHumanHumanHumanHumanHumanHumanHumanHumanHumanHumanHAKEHAKEHAKEHAKEHAKEHAKEHumanHumanHumanHumanabdfec9

Fig. 6: The correlation between PaSta and logic rules before and after
the updating for activities hug horse and carry potted plant.

Fig. 7: Annotated rules and generated rules for ride bicycle class and
their performances (AP) on the HICO-DET [1] test set.

It should also be noticed that the logical modules are not
ideal and restricted in our deﬁned task space. The modules
tend to judge the embedding as T rue and lose the ability
of inference with multiple layer logic expressions. This is
expected since the backpropagation-driven DNN modules
tend to take a shortcut and the inputs of logical modules
are not balanced. Despite this, the results of experiments
still demonstrate the good logical reasoning ability of our
logical reasoning engine in the activity event space.

5.3.6 Analyzing Logic Rules
Fig. 6 shows the correlation between PaSta and rules before
and after the updating for hug horse and carry potted plant.
Two endpoints are more related if they are connected with
a wider stream. After updating, HAKE has learned more
diverse and suitable PaSta related to the activity.

For most activities, the farther away from the generated
rule distribution from the human prior distribution, the
worse the performance. However, there are a few cases
indeed work, e.g., ride bicycle (green line). This is in detail
demonstrated in Fig. 7. The four pictures present various
situations for activity ride bicycle. The 1st image from the
left shows a common case for ride bicycle. However, people
may also ride a bicycle without hands holding the handles
(the 2nd), without the hip sitting on the seat (the 3rd), or
with feet jumping from the bicycle (the 4th). Such various
situations are very difﬁcult and costly to be fully annotated
case by case. However, our rule generator can discover these
corner cases by evaluating the generated logic rules. As
shown in Fig. 7, besides the annotated 1st rule, the other
three rules are all generated and adopted by our inductive-
deductive reasoning policy. We further measure their per-
formances on HICO-DET [1] and ﬁnd that these “corner”
rules also ﬁt the data well, achieving 63.60, 63.83, 63.35 AP
respectively compared with the annotated one (64.84 AP).

Due to the pape limit, we provide more evaluations and
analyses in Appendix C, e.g., the evaluations of logic rules,

Fig. 5: Veriﬁcation of
the
T rue/F alse in sampled embeddings, which shows the well distin-
guishable ability of HAKE for two logical states.

the logic operations. Distribution of

Expression
x (cid:54)= ¬x
x ∨ x = x
T ∨ T = T
F ∨ T = T
T ∨ T ∨ T = T
T ∨ F ∨ F = T

Accuracy
0.9411
0.8854
0.9870
0.9990
0.9870
0.9790

Expression
x = ¬¬x
x ∨ ¬x = T
T ∨ F = T
F ∨ F = F
T ∨ T ∨ F = T
T ∨ F ∨ F = F

Accuracy
0.9426
0.9425
0.9960
0.7950
0.9940
0.7180

TABLE 3: Accuracy of different logic expressions. x stands for random
embedding, and T, F stand for T rue, F alse respectively.

2) GT Primitives + Million Rules. To verify the complete-
ness of primitive-semantic space bridging, we input GT
primitives and search for the best rules (Appendix C.1.2).
This operation further boosts the performance. After search-
ing 1 M rules for each possible activity of each sample,
HAKE achieves impressive 71.69 (GT H-O boxes) mAP on
HICO-DET [1], which vastly outperforms the above results
and veriﬁes the ﬁrm guarantee and potential of our logical
reasoning.

5.3.5 Evaluating the effectiveness of logical modules

To verify the logical reasoning ability of our logical modules,
we examine them with the logic expressions on the test set.
Speciﬁcally, we randomly sample 20 K event embed-
dings in the test set from eP ri, eAct, and eclsm as the inputs
of logic expressions. We ﬁrst feed them into the binary
) to obtain their T rue/F alse probabili-
discriminator
ties. As shown in Fig. 5, the sampled event embeddings
show obvious logical characteristics, as most embeddings
have logical probabilities close to either 0 or 1, which
means they represent T rue/F alse without ambiguity. If the
embeddings are sampled randomly from the whole vector
space rather than the test set event space, the probabilities
distribution will be Gaussian instead of bipolar.

(
·

J

J

To further demonstrate the effectiveness of logical mod-
0.5 as the criterion to select
ules, we choose a threshold tl
≥
embeddings to represent T rue/F alse. We select embed-
dings e such that
(e) < tl as F alse
(e) > tl as T rue and
then feed them into the logical modules to evaluate the
ability of logical reasoning. It should be noted the threshold
tl actually sets a stricter standard since any probabilities
higher than 0.5 should be regarded as T rue. The accuracy of
every logic expression is shown in Tab. 3. It is evident that
in the given speciﬁc event space, our logic modules have the
ability of logical reasoning since the accuracy of all binary
and ternary logic expressions exceeded 50% signiﬁcantly.

J

0.00.20.40.60.81.002000400060008000e¬eInitial rulesUpdated rulesride bicycleannotatedgeneratedgeneratedgeneratedfoot: tread on hip: sit on hand: holdAP=64.84foot: tread on hip: sit onAP=63.60foot: stand on leg: is close with hand: holdAP=63.83leg: jump with hand: holdAP=63.35logic reasoning, completeness of primitive space, etc.

5.4 Ablation Study

We design ablation studies on Ambiguous-HOI [61] under
transfer learning and GT-HAKE (GT H-O boxes) setting to
verify the components and avoid the inﬂuence of primitive
detection.
Language Feature. To verify the ability of linguistic repre-
sentation, we replace the primitive Bert feature in Activ-
ity2Vec with Gaussian noise, Word2Vec, and GloVe on GT-
HAKE mode (64.21 mAP, without rule update). The results
are all worse (60.55, 62.17, 62.43 mAP). Especially, the worst
performance of Gaussian noise shows the effectiveness of
linguistic event features.
Logic Rules Complexity. We change the upper limit of
the adopted logic rule number to test its inﬂuence on the
ﬁnal performance. In the beginning, the inference ability of
HAKE improves as human prior knowledge extends. How-
ever, as the rule count further increases, the performance
falls. The possible reason is that the rule base is “overﬁtted”
and more rules would hurt the generalization ability of
HAKE. For more details, please refer to Appendix C.1.
Rule Updating Policy. 1) Without the evaluation and up-
dating of logic rules, the performance degrades from 68.20
mAP to 64.21 mAP, verifying the efﬁcacy of the inductive-
deductive policy. 2) When evaluating the rules, we can
select better rules according to either lower train loss or
higher performance (mAP) on the train set data. We ﬁnd
that they perform basically the same (68.20 and 68.15 mAP
respectively). 3) During updating, the candidate rules have
two sources: the annotated primitive-activity pairs from the
HAKE train set and automatically generated rules. With
annotated or generated only, the performance falls to 66.89
and 66.96 mAP respectively.

5.5 SCR Test

Human intelligence can create things with few samples, but
evaluating activity understanding systems via creation is
difﬁcult. For another, SCR measures the area ratio of key se-
mantic regions that humans can quickly discover. Similarly,
HAKE can detect primitives, so we can utilize it to discover
key semantics. Hence, to verify that HAKE has gained
the knowledge to recognize the logic relations between
primitives and activities, we propose an SCR test based
on the SCR principle, i.e., understanding-via-elimination
which compares the semantics elimination effect of humans
and HAKE. That said, given an image, humans and HAKE
would do their best to mask the minimum and equivalent
pixels to eliminate the action semantics, i.e., making other
human participants cannot recognize the activities.

We conduct a test on 1,000 images of 105 activities,
human participants, and HAKE attempt to mask the small-
est and equivalent box regions to eliminate key semantics.
Finally, we calculate the recognition performance of other
participants on the unmasked and two masked image sets
from humans and HAKE. For the unmasked images, the
human recognition performance is 90+%. Besides, for the
two masked sets, the performances are close: 35.6% (hu-
man masking) and 39.5% (HAKE masking), indicating that

10

Fig. 8: Masking results from HAKE and humans in the SCR Test. The
verbs are given at the top. The source of masking is marked below
the sub-image. Two sets of maskings are very similar and difﬁcult to
distinguish even by human participants (59.55% accuracy).

HAKE can eliminate key semantics well with a similar im-
age masking ratio to humans. Some examples are visualized
in Fig. 8. The texts under the images are the sources of the
maskings. We can ﬁnd out that HAKE achieves good key
regions localization and recognition abilities.

test

Moreover, an additional

to determine whether
HAKE’s masking is indistinguishable from humans’. Through
a user study, the accuracy of masking source discrimination
(humans or HAKE) is only 59.55%. Thus, HAKE can create
masking very similar to humans’, thus can robustly trans-
form the activity information of image space to primitive
space. For more details, please refer to Appendix F.

6 CONCLUSION

HAKE explores a new insight to advance activity under-
standing. We worked with 702 participants to release a
large-scale and ﬁne-grained knowledge base as the fuel for
reasoning. HAKE is, to the best of our knowledge, the ﬁrst
and largest publicly available activity dataset with conceptual
and logical descriptions, which yields a great performance
boost, particularly for few-shot learning. Furthermore, our
primitive dictionary and logic rule base are scalable and
can be easily adapted to novel scenarios. We will open the
website upload port for users to upload their primitives and
rules to continually enrich HAKE.

From the SCR test, HAKE shows its signiﬁcant ability
to locate key semantics via primitive detection and reason-
ing, ensuring that primitive space can faithfully embed the
visual activity information. And the reasoning engine can
bridge primitive semantic space well. All of these factors
give us the hope that HAKE can approach the success of
object recognition. To prove this, we analyzed its upper
bound on challenging tasks, where it shows signiﬁcant
superiority. With the enrichment of primitive dictionaries

ridecarryholdeatwalkwashHAKEHAKEHumanHumanHumanHAKEHAKEHAKEHAKEHumanHumanHumanHAKEHAKEHumanHAKEHumanHumanHumanHumanHAKEHAKEHumanHAKEHAKEHAKEHAKEHumanHumanHAKEHumanHumanHAKEHumanHAKEHumanHumanHumanHumanHumanHAKEHAKEHAKEHAKEHAKEHAKEHumanHumanHumanHAKEHAKEHAKEHAKEHumanHumanHumanand rule bases, we believe HAKE will lay the foundation
for an interpretable and applicable system for areas related
to human survival and development, e.g., ambient intelli-
gence [3]. For example, detecting patient activities in the
ICU to monitor accidents such as falls, planning diet and
treatment according to the amount of exercise of the patient,
and monitoring sleep quality by detecting turnover, getting
up, drinking, etc.

HAKE also harnesses concept learning with compositional
generalization. First, as the compositions of primitives, activ-
ities usually differ locally, just as molecules are composed of
atoms and differ in the type, number, and compound mode
of atoms. We can manipulate primitives and transform global
activities into novel categories. This elegant characteristic
will advance zero-shot learning. Second, inductive reason-
ing extracting rules from raw data and deductive reasoning
verifying rules are practically promising for combining deep
learning and symbolic reasoning. Considering the practi-
cability of activity understanding, we hope HAKE will be
a good platform based on real-world data for cognition
analysis and causal inference [66].

7 ACKNOWLEDGMENT

Supported in part by National Key R&D Program of
China, No. 2017YFA0700800, Shanghai Municipal Science
and Technology Major Project (2021SHZDZX0102), SHEITC
(2018-RGZN-02046), Shanghai Qi Zhi Institute, and Baidu
Scholarship.

REFERENCES

[1] Y.-W. Chao, Y. Liu, X. Liu, H. Zeng, and J. Deng, “Learning to

detect human-object interactions,” in WACV, 2018.

[2] T. Y. Lin, M. Maire, S. Belongie, J. Hays, P. Perona, D. Ramanan,
P. Doll´ar, and C. L. Zitnick, “Microsoft coco: Common objects in
context,” in ECCV, 2014.

[3] A. Haque, A. Milstein, and L. Fei-Fei, “Illuminating the dark
spaces of healthcare with ambient intelligence.” Nature, 2020.
[4] L. Smith, N. Dhawan, M. Zhang, P. Abbeel, and S. Levine, “Avid:
Learning multi-stage tasks via pixel-level translation of human
videos,” arXiv preprint arXiv:1912.04443, 2019.

[5] Y.-L. Li, L. Xu, X. Liu, X. Huang, Y. Xu, S. Wang, H.-S. Fang, Z. Ma,
M. Chen, and C. Lu, “Pastanet: Toward human activity knowledge
engine,” in CVPR, 2020.

[6] Y. Bengio, “From system 1 deep learning to system 2 deep learn-
ing,” in Posner lecture at NeurIPS’2019, 2019, Vancouver, BC.
[7] M. Zimmermann, R. B. Mars, F. P. De Lange, I. Toni, and L. Ver-
hagen, “Is the extrastriate body area part of the dorsal visuomotor
stream?” Brain Structure and Function, 2018.

[8] D. D. Hoffman and W. Richards, “Parts of recognition,” Cognition,

[9]

1983.
I. Biederman, “Recognition-by-components: a theory of human
image understanding.” Psychological review, 1987.

[10] B. M. Lake, T. D. Ullman, J. B. Tenenbaum, and S. J. Gershman,
“Building machines that learn and think like people,” Behavioral
and brain sciences, 2017.

[11] Y. W. Chao, Z. Wang, Y. He, J. Wang, and J. Deng, “Hico: A
benchmark for recognizing human-object interactions in images,”
in ICCV, 2015.

[12] S. Gupta and J. Malik, “Visual semantic role labeling,” arXiv

preprint arXiv:1505.04474, 2015.

[13] C. Gu, C. Sun, D. A. Ross, C. Vondrick, C. Pantofaru, Y. Li,
S. Vijayanarasimhan, G. Toderici, S. Ricco, R. Sukthankar et al.,
“Ava: A video dataset of spatio-temporally localized atomic visual
actions,” in CVPR, 2018.

[14] J. Carreira and A. Zisserman, “Quo vadis, action recognition? a

new model and the kinetics dataset,” in CVPR, 2017.

11

[15] K. Soomro, A. R. Zamir, and M. Shah, “Ucf101: A dataset of 101
human actions classes from videos in the wild,” arXiv preprint
arXiv:1212.0402, 2012.

[16] H. Jhuang, J. Gall, S. Zufﬁ, C. Schmid, and M. J. Black, “Towards

understanding action recognition,” in ICCV, 2013.

[17] R. Vemulapalli, F. Arrate, and R. Chellappa, “Human action recog-
nition by representing 3d skeletons as points in a lie group,” in
CVPR, 2014.

[18] Y. Du, W. Wang, and L. Wang, “Hierarchical recurrent neural
network for skeleton based action recognition,” in CVPR, 2015.
[19] B. G. Fabian Caba Heilbron, Victor Escorcia and J. C. Niebles,
“Activitynet: A large-scale video benchmark for human activity
understanding,” in CVPR, 2015.

[20] V. Delaitre, J. Sivic, and I. Laptev, “Learning person-object interac-

tions for action recognition in still images,” in NIPS, 2011.

[21] G. Gkioxari, R. Girshick, and J. Malik, “Actions and attributes from

wholes and parts,” in ICCV, 2015.

[22] K. Simonyan and A. Zisserman, “Two-stream convolutional net-

works for action recognition in videos,” in NIPS, 2014.

[23] C. Feichtenhofer, A. Pinz, and A. Zisserman, “Convolutional two-
stream network fusion for video action recognition,” in CVPR,
2016.

[24] S. Ji, W. Xu, M. Yang, and K. Yu, “3d convolutional neural

networks for human action recognition,” TPAMI, 2012.

[25] D. Tran, L. Bourdev, R. Fergus, L. Torresani, and M. Paluri, “Learn-
ing spatiotemporal features with 3d convolutional networks,” in
ICCV, 2015.

[26] L. Sun, K. Jia, D.-Y. Yeung, and B. E. Shi, “Human action recogni-
tion using factorized spatio-temporal convolutional networks,” in
ICCV, 2015.

[27] S. Ren, K. He, R. Girshick, and J. Sun, “Faster r-cnn: Towards real-
time object detection with region proposal networks,” in NIPS,
2015.

[28] H.-S. Fang, S. Xie, Y.-W. Tai, and C. Lu, “RMPE: Regional multi-

person pose estimation,” in ICCV, 2017.

[29] G. Gkioxari, R. Girshick, P. Doll´ar, and K. He, “Detecting and

recognizing human-object interactions,” in CVPR, 2018.

[30] C. Gao, Y. Zou, and J.-B. Huang, “ican: Instance-centric attention
network for human-object interaction detection,” in BMVC, 2018.
[31] Y.-L. Li, S. Zhou, X. Huang, L. Xu, Z. Ma, H.-S. Fang, Y. Wang, and
C. Lu, “Transferable interactiveness knowledge for human-object
interaction detection,” in CVPR, 2019.

[32] S. Qi, W. Wang, B. Jia, J. Shen, and S.-C. Zhu, “Learning human-
object interactions by graph parsing neural networks,” in ECCV,
2018.

[33] J. Peyre, I. Laptev, C. Schmid, and J. Sivic, “Detecting rare visual

relations using analogies,” in ICCV, 2019.

[34] Z. Hou, X. Peng, Y. Qiao, and D. Tao, “Visual compositional

learning for human-object interaction detection,” in ECCV, 2020.

[35] B. Wan, D. Zhou, Y. Liu, R. Li, and X. He, “Pose-aware multi-level
feature network for human object interaction detection,” in ICCV,
2019.

[36] D.-J. Kim, X. Sun, J. Choi, S. Lin, and I. S. Kweon, “Detect-
ing human-object interactions with action co-occurrence priors,”
ECCV, 2020.

[37] X. Zhong, C. Ding, X. Qu, and D. Tao, “Polysemy deciphering

network for human-object interaction detection,” in ECCV, 2020.

[38] Y.-L. Li, X. Liu, X. Wu, Y. Li, and C. Lu, “Hoi analysis: Integrating
and decomposing human-object interaction,” in NeurIPS, 2020.
[39] Y. Liao, S. Liu, F. Wang, Y. Chen, and J. Feng, “Ppdm: Parallel point
detection and matching for real-time human-object interaction
detection,” in CVPR, 2020.

[40] B. Kim, J. Lee, J. Kang, E.-S. Kim, and H. J. Kim, “Hotr: End-to-end
human-object interaction detection with transformers,” in CVPR,
2021.

[41] M. Tamura, H. Ohashi, and T. Yoshinaga, “QPIC: Query-based
pairwise human-object interaction detection with image-wide con-
textual information,” in CVPR, 2021.

[42] S. Maji, L. Bourdev, and J. Malik, “Action recognition from a
distributed representation of pose and appearance,” in CVPR,
2011.

[43] Z. Zhao, H. Ma, and S. You, “Single image action recognition using

semantic body part actions,” in ICCV, 2017.

[44] H. S. Fang, J. Cao, Y. W. Tai, and C. Lu, “Pairwise body-part
attention for recognizing human-object interactions,” in ECCV,
2018.

[45] F. S. Khan, J. Van De Weijer, R. M. Anwer, M. Felsberg, and
C. Gatta, “Semantic pyramids for gender and action recognition,”
TIP, 2014.

[74] S. Qi, W. Wang, B. Jia, J. Shen, and S.-C. Zhu, “Learning human-
object interactions by graph parsing neural networks,” in ECCV,
2018.

12

[46] B. Yao and F. F. Li, “Modeling mutual context of object and human
pose in human-object interaction activities,” in CVPR, 2010.
[47] A. d. Garcez, M. Gori, L. C. Lamb, L. Seraﬁni, M. Spranger, and
S. N. Tran, “Neural-symbolic computing: An effective methodol-
ogy for principled integration of machine learning and reasoning,”
arXiv preprint arXiv:1905.06088, 2019.

[48] J. Andreas, M. Rohrbach, T. Darrell, and D. Klein, “Neural module

networks,” in CVPR, 2016.

[49] D. A. Hudson and C. D. Manning, “Learning by abstraction: The
neural state machine,” arXiv preprint arXiv:1907.03950, 2019.
[50] K. Yi, J. Wu, C. Gan, A. Torralba, P. Kohli, and J. B. Tenenbaum,
“Neural-symbolic vqa: Disentangling reasoning from vision and
language understanding,” arXiv preprint arXiv:1810.02338, 2018.

[51] J. Mao, C. Gan, P. Kohli, J. B. Tenenbaum, and J. Wu, “The
neuro-symbolic concept learner: Interpreting scenes, words, and
sentences from natural supervision,” in ICLR, 2019.

[52] H. Dong, J. Mao, T. Lin, C. Wang, L. Li, and D. Zhou, “Neural logic

machines,” arXiv preprint arXiv:1904.11694, 2019.

[53] J. Devlin, M.-W. Chang, K. Lee, and K. Toutanova, “Bert: Pre-
training of deep bidirectional transformers for language under-
standing,” arXiv preprint arXiv:1810.04805, 2018.

[54] B. Zhou, A. Lapedriza, A. Khosla, A. Oliva, and A. Torralba,
“Places: A 10 million image database for scene recognition,”
TPAMI, 2017.

[55] S. Shi, H. Chen, W. Ma, J. Mao, M. Zhang, and Y. Zhang, “Neural

logic reasoning,” in CIKM, 2020.

[56] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N.
Gomez, L. u. Kaiser, and I. Polosukhin, “Attention is all you need,”
in NIPS, 2017.

[57] T. Rockt¨aschel and S. Riedel, “End-to-end differentiable proving,”

in NIPS, 2017.

[58] J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-Fei, “Ima-
genet: A large-scale hierarchical image database,” in CVPR, 2009.
[59] C. Lu, R. Krishna, M. Bernstein, and F. F. Li, “Visual relationship

detection with language priors,” in ECCV, 2016.

[60] O. Vinyals, A. Toshev, S. Bengio, and D. Erhan, “Show and tell: A

neural image caption generator,” in CVPR, 2015.

[61] Y.-L. Li, X. Liu, H. Lu, S. Wang, J. Liu, J. Li, and C. Lu, “Detailed
2d-3d joint representation for human-object interaction,” in CVPR,
2020.

[62] K. He, X. Zhang, S. Ren, and J. Sun, “Deep residual learning for

image recognition,” in CVPR, 2016.

[63] C.-Y. Wu, C. Feichtenhofer, H. Fan, K. He, P. Krahenbuhl, and
R. Girshick, “Long-term feature banks for detailed video under-
standing,” in CVPR, 2019.

[64] C. Feichtenhofer, H. Fan, J. Malik, and K. He, “Slowfast networks

for video recognition,” CoRR, 2018.

[65] C. Gu, C. Sun, D. A. Ross, C. Vondrick, C. Pantofaru, Y. Li,
S. Vijayanarasimhan, G. Toderici, S. Ricco, R. Sukthankar et al.,
“Ava,” https://research.google.com/ava/download.html, 2018.

[66] J. Pearl, M. Glymour, and N. P. Jewell, Causal inference in statistics:

A primer.

John Wiley & Sons, 2016.

[67] I. Krasin, T. Duerig, N. Alldrin, V. Ferrari, S. Abu-El-Haija,
A. Kuznetsova, H. Rom, J. Uijlings, S. Popov, A. Veit et al., “Open-
images: A public dataset for large-scale multi-label and multi-
class image classiﬁcation,” Dataset available from https://github.
com/openimages, vol. 2, p. 3, 2017.

[68] B. Zhuang, Q. Wu, C. Shen, I. Reid, and A. v. d. Hengel, “Care
about you: towards large-scale human-centric visual relationship
detection,” arXiv preprint arXiv:1705.09892, 2017.

[69] S. Liu, L. Ren, Y. Liao, G. Ren, H. Xiang, and G. Li, “Pic-person in

context,” http://picdataset.com/challenge/index/, 2018.

[70] M. Andriluka, L. Pishchulin, P. Gehler, and B. Schiele, “2d human
pose estimation: New benchmark and state of the art analysis,” in
CVPR, 2014.

[71] C. Lu, H. Su, Y. Li, Y. Lu, L. Yi, C.-K. Tang, and L. J. Guibas, “Be-
yond holistic object recognition: Enriching image understanding
with part states,” in CVPR, 2018.

[72] G. A. Miller, “Wordnet: a lexical database for english,” Communi-

cations of the ACM, 1995.

[73] K. W. Church and P. Hanks, “Word association norms, mutual
information, and lexicography,” Computational linguistics, 1990.

APPENDIX A
DETAILS OF THE BOTTLENECK ANALYSIS OF DI-
RECT MAPPING

To investigate the difference between object patterns and
activity patterns, we invite participants and divide them
into two groups: the ﬁrst group would mask the key ar-
eas of object or activity images and take efforts to make
other participants cannot recognize the objects or activities
again. Meanwhile, the second group would be given the
masked images from the ﬁrst group and recognize them.
The masked images that cannot be recognized would be
noted as successful cases. The object and activity image
sets have the same number of images. In detail, we collect
742 images for object recognition covering 58 objects from
COCO [2] and 745 images for activity recognition covering
92 activities from HICO-DET [1] to ensure diversity. To
avoid biases, we balance the images of object and activity
classes: about 13 images for each object category and 10
images for each activity category.

As shown in Fig. 9a, the image number of activity classes
varies more dramatically than object classes. This is due to
more severe long-tailed distribution [1] on activity classes.
Comparatively, the long-tailed distribution of object classes
is relatively mild because the dataset [2] we adopt has been
manually modulated to balance the object classes. All masks
are added within the tight bounding boxes containing all
the activity/object entities, i.e., whole human body (human-
object pair) or object. Moreover, all the lengths and widths
of these boxes are greater than 200 pixels to provide clear
visual information. After the masking by humans, SCR is
calculated as the area ratio between the mask and tight
box in successful cases (Fig. 9b). From the results shown
in Fig. 9c, we can ﬁnd that the activity images have much
smaller SCRs than the object images, which strongly veriﬁes
the huge difference between activity and object patterns.
Speciﬁcally, the activity images have the average SCR of
0.122, while the object images have the average SCR of 0.610.
To make the results more vivid, we visualized the SCRs of
each activity/object class in Fig. 9d,e.

It can be easily discovered that the activity classes have
much lower SCRs (0.0-0.3). And the SCRs of object classes
are between 0.4-0.9. As a result, accurately capturing the key
activity semantic regions is much more difﬁcult than object
recognition. Directly mapping thus requires more training
data to be able to locate the concentrated activity semantics
and thus hinders the performance. Hence, we need different
paradigms for these two different tasks. To this end, we
propose HAKE for activity understanding.

APPENDIX B
KNOWLEDGE BASE CONSTRUCTING DETAILS

This section details the construction processes of the HAKE
knowledge base and the logic rule base.

B.1 Primitive Collection

We ﬁrst introduce the collection of activity primitives.
HAKE seeks to explore the common knowledge of activity
primitives as the atomic elements to infer activities.

13

Primitive Deﬁnition. For part states (PaSta), we decom-
pose human body into ten parts, namely head, two arms, two
hands, hip, two legs, two feet. Then, PaSta will be assigned
to describe these body parts. For example, the primitive of
hand can be hold-something (sth) or push-sth, the primitive of
head can be watch-sth, eat-sth. After exhaustively reviewing
the collected 122 K+ images and 74.7 hours (299) of videos,
we ﬁnd that the descriptions of any human parts can be
concluded into limited classes (about 100). Notably, a person
may have more than one activity simultaneously. Thus each
part can have multiple primitives too. Furthermore, body
part can also conveys pose information such as hand-wave
and waist-bend. As for objects, we focus on the most common
80 objects from the COCO [2] dataset. As for scenes, we
adopt the 400 scenes from [54].

Data Collection. For generalization, we collect human-
centered activity images by crowd-sourcing (with rough
activity label) as well as existing well-designed datasets [1],
[11], [12], [67], [68], [69] which are structured around a rich
semantic ontology, diversity, and variability of activities. All
their annotated persons and objects are extracted for our
construction. For AVA [13], we collect the active persons’
annotations from its available 299 15-min videos (train and
validation sets of AVA 2.1). Finally, we collect more than 350
K+ images/frames of diverse activity categories.

Activity Labeling. Activity categories of HAKE are cho-
sen according to the most common daily human activities,
interactions with object/person. Referred to the hierarchical
structure [19] of activity, common activities in existing
datasets [11], [12], [13], [19], [67], [68], [69], [70] and crowd-
sourcing labels, we select 156 activities including human-
object interactions (also including human-human interac-
tions) and body only motions. We detail the 156 activities
in Tab. 4. We ﬁrst clean and reorganize the annotated per-
sons and objects from existing datasets and crowd-sourcing.
Then, we annotate the bounding boxes of active persons
and interacted COCO 80 objects in the rest images. As for
AVA [13] videos, since its diverse and moving objects, we
follow previous works [13], [63], [64] and focus on the active
persons only.

Body Part Box for PaSta. To locate the human parts, we
use pose estimation [28] to obtain the joints of all annotated
persons. Then we follow Fang et.al. [44] and generate ten
body part boxes. Estimation errors are addressed manually
to ensure high-quality annotation. Each part box is centered
with a joint, and the box size is pre-deﬁned by scaling
the distance between the joints of the neck and pelvis. A
joint with conﬁdence higher than 0.7 will be seen as visible.
When not all joints can be detected, we use body knowledge-
based rules. That said, if the neck or pelvis is invisible, we
conﬁgure the part boxes according to other visible joint
groups (head, main body, arms, legs), e.g., if only the upper
body is visible, we set the size of the hand box to twice the
pupil distance.

Primitive Annotation. We carry out the annotation by
crowd-sourcing. As for object primitives, we direct annotate
the interacted objects in human-object interactions with their
80 classes [2] and tight boxes. As for scenes, here, we use
a model pre-trained on Place365 [54] to classify the scenes
in images/frames and manually ﬁx the wrong predictions.
We would not always use scene information in reasoning

14

Fig. 9: Details of the bottleneck analysis based on SCR. a. Image numbers of different activity/object categories in a SCR descending order. b.
The deﬁnition and illustration of SCR measurement. c. Some examples of SCR test results on object/activity images. d. Speciﬁc SCRs of different
object categories. e. Speciﬁc SCRs of different activity categories.

since the user study concludes that scenes usually cannot
decide the activities and may in turn import noise/bias in
inference. For example, the spurious correlation between
play football and grassland would bias the reasoning. For
the more complex human body PaSta, the process is as
follows: 1) First, we choose the PaSta categories considering
the generalization. For example, about 200 human body part
states (PaSta [5], [71] from WordNet [72] are chosen to
form a primitive dictionary, e.g., foot-kick-sth. If a part does
not have any active states, we depict it as no activity). 2)
Second, to ﬁnd the most common PaSta that can work as

the transferable activity knowledge, we invite 200 annotators
from different backgrounds to annotate 10 K images and 50
videos of 156 activities with our primitive dictionary. For
example, given an activity ride bicycle, they may describe it
as hip-sit on-sth, hand-hold-sth, foot-step on-sth, etc. When the
participants cannot explain their decisions with the current
dictionary, they should propose new PaSta. If a proposed
PaSta is frequently used by other participants, it will be
formally included. 3) Based on their annotations, we use the
Normalized Point-wise Mutual Information (NPMI) [73] to
calculate the co-occurrence between activities and PaSta in

abcdeActivityObject15

HOIs

Body Motions

Head

Arm
Hand

Hip
Thigh

Foot

Objects

adjust, assemble, block, blow, board, break, brush with, board gaming, buy, carry, catch, chase,
check, chop, clean, clink glass, close, control, cook, cut, cut with, dig, direct, drag, dribble, drink
with, drive, dry, eat, eat at, enter, exit, extract, feed, ﬁll, ﬂip, ﬂush, ﬂy, ﬁght, ﬁshing, give sth to sb,
grab, greet, grind, groom, hand shake, herd, hit, hold, hop on, hose, hug, hunt, inspect, install,
jump, kick, kiss, lasso, launch, lick, lie on, lift, light, listen to sth, listen to a person, load, lose,
make, milk, move, open, operate, pack, paint, park, pay, peel, pet, play musical instrument, play
with sb, play with pets, pick, pick up, point, pour, press, pull, push, put down, put on, race, read,
release, repair, ride, row, run, sail, scratch, serve, set, shear, shoot, shovel, sign, sing to sb, sip,
sit at, sit on, slide, smell, smoke, spin, squeeze, stab, stand on, stand under, stick, stir, stop at,
straddle, swing, tag, take a photo, take sth from sb, talk on, talk to, teach, text on, throw, tie, toast,
touch, train, turn, type on, walk, wash, watch, wave, wear, wield, work on laptop, write, zip
bow, clap, climb, crawl, dance, fall, get up, kneel, physical exercise, swim

TABLE 4: Activities in HAKE. HOIs indicate the interactions between person and object/person.

eat, inspect, talk with, talk to, close with, kiss, raise up, lick, blow, drink with, smell, wear, listen
to, no activity
carry, close to, hug, swing, crawl, dance, material art, no activity
hold, carry, reach for, touch, put on, twist, wear, throw, throw out, write on, point with, point to,
use sth to point to, press, squeeze, scratch, pinch, gesture to, push, pull, pull with, wash, wash
with, hold in both hands, lift, raise, feed, cut with, catch with, pour into, crawl, dance, martial art,
no activity
sit on, sit in, sit beside, close with, bend, no activity
walk with, walk to, run with, run to, jump with, close with, straddle, jump down, walk away,
bend, kneel, crawl, dance, material art, no activity
stand on, step on, walk with, walk to, run with, run to, dribble, kick, jump down, jump with,
walk away, crawl, dance, fall down, martial art, no activity
airplane, apple, backpack, banana, baseball bat, baseball glove, bear, bed, bench, bicycle, bird,
boat, book, bottle, bowl, broccoli, bus, cake, car, carrot, cat, cell phone, chair, clock, couch, cow,
cup, dining table, dog, donut, elephant, ﬁre hydrant, fork, frisbee, giraffe, hair drier, handbag,
horse, hot dog, keyboard, kite, knife, laptop, microwave, motorcycle, mouse, orange, oven,
parking meter, person, pizza, potted plant, refrigerator, remote, sandwich, scissors, sheep, sink,
skateboard, skis, snowboard, spoon, sports ball, stop sign, suitcase, surfboard, teddy bear, tennis
racket, tie, toaster, toilet, toothbrush, trafﬁc light, train, truck, tv, umbrella, vase, wine glass, zebra

TABLE 5: Activity primitives consist of human body part states (PaSta) and objects.

dictionary. Finally, 93 PaSta with the highest NPMI values
are chosen. 4) Using the annotations of 10 K images and
50 videos as seeds, we automatically generate the initial
PaSta labels for all of the rest data. Thus the other 408
annotators are asked to revise the annotations according to
the speciﬁc human states in images. 5) Considering that a
person may have multiple activities, for each activity, we
annotate its corresponding ten PaSta respectively. Then we
combine all sets of PaSta from all activities. Thus, a part
can also have multiple states, e.g., in activity eating while
talking, the head has PaSta head-eat-sth, head-talk to-sth and
head-look at-sth simultaneously. 6) To ensure quality, each
image or video clip will be annotated twice and checked by
automatic procedures and supervisors. We cluster all labels
and discard the outliers to obtain robust agreements. We
details the Pasta classes in Tab. 5.

Finally, HAKE includes 122 K+ images (247 K+ persons,
345 K instance activities, 220 K+ object primitives, and 7.4
M+ PaSta primitives) and 299 15-min videos (74.7 hours; one
second per frame, 234 K+ frames in total; 426 K+ persons,
1.0 M+ instance activities, and 18.8 M+ PaSta primitives).

With this large-scale activity-primitive knowledge base,
we ﬁnd that primitives can be well learned. A shallow and
concise CNN trained with a part of HAKE data can easily

achieve about 55 mAP in image-level primitive recognition on
HICO [11] and 30 mAP in instance-level primitive detection on
HICO-DET [1]. Meanwhile, deeper and sophisticate state-
of-the-arts [34], [44] can only achieve about 40 and 20
mAP on image-level activity recognition and instance-level ac-
tivity detection. Moreover, primitives can be well transferred.
To verify this, we conduct transfer learning experiments
on Ambiguous-HOI [61], V-COCO [12], and image-based
AVA [13], i.e., ﬁrst train a backbone model to learn the
knowledge of HAKE and then use it to infer the activities of
unseen data, even unseen activities. Results show that prim-
itives can be well transferred and boost the performance
greatly (main test Tab. 2).

B.2 Annotation Analysis

There were 702 participants involved in the construction
of HAKE. They have various backgrounds. Thus we can
ensure annotation diversity and reduce biases. The basic
background information is shown in Fig. 10a. We also
list the selected 156 activities which cover both person-
object/person interactions, body-only motions, and the se-
lected primitives in our dictionary (93 PaSta and 80 objects)
in Appendix D.

16

Fig. 10: HAKE construction details. a. Participant statistics of HAKE construction and experiments. Basic participant background information
including education and age. b. The primitive-activity initial rule annotation. Activity-Primitive annotation to generate the causal graph (logic
rules), i.e., which primitives occurring simultaneously would cause an activity.

B.3 Initial Logic Rule Collection

To initialize our logic rule base, we invite 5 participants
to annotate the human prior logic rules according to their
understanding of human activities. Given the activity con-
cepts only (activity class names), they are asked to de-
scribe these concepts with our primitive concepts from the
dictionary. Hence, the collected logic rules would be the
abstract concept-level and learned from the participants’ daily
life experience without the bias from images/videos. As
illustrated in Fig. 10b, participants are asked to annotate
activity causal graphs, i.e., which primi-
the primitives
tives existing simultaneously can cause the effect (speciﬁc
activities). To deduce one activity, the causes (primitives)
are annotated as one, and the primitive nodes in the graph
would have arrows pointing to this activity node. On the
contrary, the unrelated primitive nodes are annotated as
zero. For example, for activity clean oven, the possible cause-
effect results are hand-wash-sth
clean oven, or hand-
∧
put on-sth
clean oven. After the an-
oven
notation, for the m-th activity, we obtain its initial logic
p, ri is a binary
0, 1
rule set Rm =
}

∧
lm
i=1, where ri
}

hand-wash-sth

ri
{

oven

∈ {

→

→

→

∧

sequence, p indicates the number of primitives, and lm is
the rule count. This initial logic rule base covers most human
daily activities and thus carries the convincing human prior
knowledge of activities. It would act as a good starting or
seed for the subsequent automatic logic rule discovery and
neuro-symbolic reasoning.

(cid:48)

{

0, 1

Logic Rule Aggregate. After annotation, we get m0
rules for each activity. For the i-th activity, its rule set
Ri =
i is the j-th rule for
the i-th activity, q is the number of primitives. Based on the
annotation, we aggregate the logic rules and summarize the
activity expertise base.

q, where rj
}

j=1, rj
m0

i ∈ {

rj
i }

= (cid:80)m

j=1. Next, we calculate rmean
m

First, we exclude the repetitive logic rules rj

i in Ri and
j=1 rj
rj
i =
get R
i and
i }
{
rj
m
rank
j=1 according to the Euclidean/Cosine Distance
i }
{
between rmean
i . Then, we sample n rules with equal
intervals in the rank list. Finally, for the i-th activity, we
aggregate n logic rules as its expertise base. The sampling
strategy increases the diversity of cognition and provides
a better generalization. On the basis of prior logic rule
collection, we can construct a convincing and professional

and rj

i

i

22abhandput_onhandholdhandwashheadinspectcleanovenarm swing………Person_1 AnnoPerson_2 AnnoPerson_k AnnoPrimitive→Activity Casual Graph<hand-wash-sth><hand-put_on-sth><oven><clean oven><arm-swing-sth><hand-wash-sth><oven><clean oven><hand-wash-sth><hand-hold-sth><oven><clean oven><head-inspect-sth>Primitive→Activity Annotationexpertise base.

APPENDIX C
MORE ANALYSES

In this section, we provide more analyses and details about
HAKE and the experiments.

C.1 Analysis of Logic Rules

C.1.1 Rule Complexity

17

C.1.4 Generated Rules.
When increasing the β in the rule generator, the KL diver-
gence between the human prior distribution and the distri-
bution of generated rules would be larger. We also analyze
to study how this affects performance. Fig. 11c illustrates the
performance on HICO-DET [1] with β changing (β
[0, 1]).
For most activities, the farther away from the generated rule
distribution from the human prior distribution, the worse
the performance. However, there are a few cases indeed
work, e.g., ride bicycle (green line).

∈

C.2 Completeness of Primitive Space

For each activity category, we ﬁrst exclude the repetitive
rules and count the collected ones. Interestingly, differ-
ent activities usually have different rule complexities in
annotation: activities with fewer ambiguities such as hold
baseball bat usually have fewer different initial rules, but the
rules of ambiguous activities such as pet cat differ greatly,
pet cat, hand-scratch-
e.g., arm-hug-sth
pet cat. Activity
cat
sth
hold baseball bat ﬁnally gets 13 rules out of 945 samples
(ratio=1.4%) while pet cat gets 27 rules out of 345 samples
(ratio=7.8%). This is following our common sense that the
latter activity is more complex than the former one.

∧
hand-hold-sth

head-inspect-sth

head-kiss-sth

→
∧

cat

→

∧

∧

∧

C.1.2 Activity logical complexity

In addition to the collected rules, we also analyze the
learned rules from our reasoning engine. As illustrated in
Fig. 11a, when trained with a different number of rules,
HAKE achieves different performances. The performances
on HICO-DET [1] Full Set and Rare Set both reach the
plateaus when the rule count is about 10-15, which proves
the validity of our primitive-based logical reasoning. HAKE
only needs a small quantity of human prior knowledge
to achieve state-of-the-art, while previous methods require
hundreds of instance annotations per activity category. If
we continue to increase the rule count, the performances of
some activity categories may still be boosted but most of
them start to fall, the reason might be that their rule com-
plexities are “overﬁtted” and more rules are redundancy
and even in turn obstruct the reasoning. We also explore
the relationships between performance and rule count for
several activity categories in Fig. 11b, which further veriﬁes
the “saturation” timings are different for different activities.

C.1.3 Evaluating and Updating

After evaluating and updating, we ﬁnally get 4,090 logic
rules for 156 activity categories. On HICO-DET [1], we get
3,746 rules for all activity categories. Among them, 80.8%
of rules come from the activity-primitive annotation and
19.2% of rules come from the generated candidates. Fig. 11d
shows the initial and updated logic rules for activity kiss
cat and direct bus. After updating, more reasonable rules
are learned and absorbed by our rule base (orange text),
while the “worse” rules which cannot perform qualiﬁed
are ﬁltered out (purple text, e.g., head-inspect-sth for activity
direct bus). These strongly verify the effectiveness of our
inductive-deductive strategy.

∈

Rq, Rq =

An important advantage of reasoning activity from primi-
tives is: the limited set of primitives can powerfully describe
complex and numerous activities. Then a natural question
follows: how to evaluate the completeness of the primitive
set? Given suitable rules, how accurate can an activity be
inferred based on the existing primitives? To answer these
questions, we search and optimize logic rules to estimate the
upper bound of the neuro-symbolic reasoning.
Given q primitives, one logic rule rk

q
0, 1
}
{
totally has 2q kinds of possible permutations. The model
would achieve its upper bound when the most suitable
rules are selected for each activity inference of each human-
object pair. As demonstrated before, during training, the
model evaluates and updates the logic rules to select more
qualiﬁed rules for practical reasoning. However, this is
limited, because all human-object pairs share the same rules
for reasoning. Besides, the rules are selected from the anno-
tated primitive-activity sets or are automatically generated
based on some principles, which are quantitatively limited
q. Thus,
compared to the huge solution space Rq =
}
the potential of our model is not fully exploited and this is
a trade-off question between performance and computation
complexity. Under the setting of q = 76 (only using the
q is too computa-
HOI-related PaSta primitives), Rq =
tionally expensive to exhaust all possible rules. Speciﬁcally,
1014 years are needed to exhaust all the
approximately 2
276 logic rules for each pair in HICO-DET. For AVA, as it
contains much more video frames, the process would be
unaffordable.

0, 1
{

0, 1
}

×

{

∈

In practice, given the i-th human-object pair and the m-
th activity, each time a rule rk is randomly selected, we
record its corresponding output prediction pimk
[0, 1]
for the m-th activity based on the pre-trained model. The
process is incremental. Since enumerating all the 276 rules is
computationally unreachable, we make effort to search and
sample about 1 M rules for each possible activity of each
human-object pair (all testing pairs in the HICO-DET test
set). Then, we select the best rule from all 1 M rules based on
lim)2 between
the recorded predictions. The distance (pimk
the prediction pimk and the binary label lim = 0/1 is calcu-
lim)2 is
lated and the rule rk∗
selected as the ﬁnal rule. Finally, we use pimk∗
im as the ﬁnal
prediction.

im with k∗ = argmink(pimk

−

−

Fig. 11e show the results with the above rule search pol-
icy. With more searched rules, our model gains an increase
of performance under all settings. Speciﬁcally, with GT
primitives and human-object boxes both, rule search makes
HAKE achieve 71.69 mAP on HICO-DET [1]. This is much

18

Fig. 11: Analysis of Logic Rules. a. Performance on Full Set and Rare Set of HICO-DET [1] with changing activity logical complexity. Blue indicates
the Full set performance and orange indicates the Rare set performance. b. Performances of different activity categories on HICO-DET [1] with
changing activity logical complexity. c. Performance of different activities on HICO-DET [1] with changing β. d. The initial and updated logic rules
for activities kiss cat and direct bus. e. Results of rule search. Performance on HICO-DET [1] after/before (a/b) giving the larger rule search range.
“GT-HAKE” indicates that inputting GT primitives to the reasoning engine. Relatively, “HAKE” indicates that we use the detected primitives
from our primitive detector in reasoning. “GT boxes” means inputting ground truth human-object boxes. Meanwhile, “detected boxes” means
that using the detected boxes from the object detector [27], [30] in reasoning.

rule search can also ﬁnd enough good rules given a part of
detected existing primitives and boost the performance. But
that is the result of an expensive rule search. In practice, the
future work would be practical to focus on the improvement
of primitive detection and reasoning implementation to tap
the potential of HAKE. Moreover, as illustrated in Fig. 12,
the performance on HICO-DET [1] improves with increas-
ing searched rules. Meanwhile, the growth also gradually
slows down. Thus, the results in Fig. 11e based on 1 M
rules are a sound upper bound estimation of our neuro-
symbolic reasoning. All of these strongly prove the validity
of inferring activities via primitives and the potential of
HAKE. Once suitable rules are selected, HAKE reasoning
engine can accurately reason out the activities given deﬁned
primitives.

Fig. 12: GT primitives and extensive rule search result in better perfor-
mance (mAP, %) on HICO-DET [1].

higher than the same upper bound setting w/o rule search
(62.65 mAP). Interestingly, when conducting a large-scale
rule search and selecting the best rules, the performance gap
between using the GT or detected primitives would be much
smaller (71.69 and 71.26 mAP). The possible reason may be
that even the detected primitives are not always accurate,

APPENDIX D
ACTIVITIES AND PRIMITIVES IN HAKE

In this section, we detail the classes of activities and primi-
tives in HAKE.

 abdckiss catinitial rulesupdated rules['head: kiss']['head: kiss']['hand: hbh' 'head: kiss']['hand: hbh' 'head: kiss']['hand: touch' 'head: kiss']['hand: touch' 'head: kiss']['arm: hug' 'head: kiss']['hand: hold' 'arm: hug' 'head: kiss']['foot: tre' 'foot: wal' 'hand: hold'] ['hand: hbh' 'hand: raise' 'arm: hug']direct businitial rulesupdated rules['hand: po' 'hand: upo' 'hand: g']['hand: po' 'hand: upo' 'hand: g'][ 'head: ins'][ 'head: ins']['hand: ges' 'head: ins']['hand: ges' 'head: ins']['hand: pull' 'head: ins']['hand: pull' 'head: ins']['hand: ges']['hand: ges']['hand: push']['hand: push']['foot: st' 'hand: ges' 'hand: pull'['arm: be close to' 'head: ins']foot: run' 'hand: hold' 'hand: push' 'head: ins']['leg: run to' 'hand: rea' 'hand: ges' 'hand: push']po: point toupo: use something to point toges: gesture tost: stand onins: inspectraise: raise (over head)hbh: hold in both handstre: tread onwal: walk withrun: run witheabdceridecarryholdeatwalkwashHAKEHAKEHAKEHAKEHAKEHAKEHAKEHAKEHAKEHAKEHumanHumanHumanHumanHumanHumanHumanHumanHumanHumanHumanHumanHAKEHAKEHAKEHAKEHAKEHAKEHumanHumanHumanHumanActivities in HAKE HAKE includes 156 different ev-
eryday human activities, including human-object/human
interactions (HOIs) and body-only motions. The included
categories are: adjust, assemble, block, blow, board, break,
brush with, board gaming, buy, carry,catch, chase, check,
chop, clean, clink glass, close, control, cook, cut, cut with,
dig,direct, drag, dribble, drink with, drive, dry, eat, eat at,
enter, exit, extract, feed, ﬁll, ﬂip, ﬂush, ﬂy, ﬁght, ﬁshing, give
something to somebody, grab, greet, grind, groom, hand
shake, herd, hit ,hold, hop on, hose, hug, hunt, inspect,
install, jump, kick, kiss, lasso, launch, lick,lie on, lift, light,
listen to something, listen to a person, load, lose, make, milk,
move, open, operate, pack, paint, park, pay, peel, pet, play
musical instrument, play with somebody, play with pets,
pick, pick up, point, pour, press, pull, push, put down, put
on, race, read, release, repair, ride, row, run, sail, scratch,
serve, set, shear, shoot, shovel, sign, sing to somebody, sip,
sit at, sit on, slide, smell, smoke, spin, squeeze, stab, stand
on, stand under, stick, stir, stop at, straddle, swing, tag,
take a photo, take something from something, talk on, talk
to,teach, text on, throw, tie, toast, touch, train, turn, type
on, walk, wash, watch, wave, wear, wield, work on laptop,
write, zip, bow, clap, climb, crawl, dance, fall, get up, kneel,
physical exercise, swim.

Primitives in HAKE HAKE primitives contain 93 hu-

man body part states (PaSta) and 80 common objects.
(1) 14 primitives are deﬁned for the head. They are: eat,
inspect, talk with, talk to, close with, kiss, raise up, lick,
blow, drink with, smell, wear, listen to, no activity.
(2) 8 primitives are deﬁned for the arm. They are: carry, close
to, hug, swing, crawl, dance, material art, no activity.
(3) 34 primitives are deﬁned for hand. They are: hold, carry,
reach for, touch, put on, twist, wear, throw, throw out, write
on, point with, point to, use sth to point to, press, squeeze,
scratch, pinch, gesture to, push, pull, pull with, wash, wash
with, hold in both hands, lift, raise, feed, cut with, catch
with, pour into, crawl, dance, martial art, no activity.
(4) 6 primitives are deﬁned for the head. They are: sit on, sit
in, sit beside, close with, bend, no activity.
(5) 15 primitives are deﬁned for the leg. They are: walk with,
walk to, run with, run to, jump with, close with, straddle,
jump down, walk away, bend, kneel, crawl, dance, material
art, no activity.
(6) 16 primitives are deﬁned for the foot. They are: stand on,
step on, walk with, walk to, run with, run to, dribble, kick,
jump down, jump with, walk away, crawl, dance, fall down,
martial art, no activity.
(7) 80 objects are included in HAKE, which are: airplane,
apple, backpack, banana, baseball bat, baseball glove, bear,
bed, bench, bicycle, bird, boat, book, bottle, bowl, broc-
coli, bus, cake, car, carrot, cat, cell phone, chair, clock,
couch, cow, cup, dining table, dog, donut, elephant, ﬁre
hydrant, fork, frisbee, giraffe, hair drier, handbag, horse, hot
dog, keyboard, kite, knife, laptop, microwave, motorcycle,
mouse, orange, oven, parking meter, person, pizza, potted
plant, refrigerator, remote, sandwich, scissors, sheep, sink,
skateboard, skis, snowboard, spoon, sports ball, stop sign,
suitcase, surfboard, teddy bear, tennis racket, tie, toaster,
toilet, toothbrush, trafﬁc light, train, truck, tv, umbrella,
vase, wine glass, zebra.

Method
InteractNet [29]
GPNN [74]
iCAN [30]
TIN [31]
VCL [34]
QPIC [41]
HAKE
TIN [31]+HAKE
VCL [34]+HAKE
QPIC [41]+HAKE
GT-HAKE (detection [30])
GT-HAKE (detection [41])
QPIC [41]+GT-HAKE (detection [41])
GT-HAKE (GT boxes)
GT-HAKE+rule search (GT boxes)

Full
9.94
13.11
14.84
17.03
23.63
29.07
19.52

Default
Rare
7.16
9.34
10.45
13.42
17.21
21.85
17.29

23.22 (
28.30 (
32.10 (

36.3%)
↑
19.8%)
↑
10.4%)
↑
38.74
41.63
45.52
62.65
71.69

72.6%)
23.16 (
↑
45.5%)
25.04 (
↑
27.57 (
26.2%)
↑
47.11
47.04
47.60
71.03
82.25

19

Known Object

Rare Non-Rare

-
-
11.33
15.51
19.12
24.14
20.47
26.54
27.41
29.55
47.44
48.79
49.00
-
-

-
-
17.73
20.26
28.03
33.93
22.45
26.04
32.05
35.85
37.16
41.79
46.37
-
-

Non-Rare
10.77
14.23
16.15
18.11
25.55
31.23
20.19
23.24
29.28
33.46
36.24
40.02
44.90
60.15
68.54

Full
-
-
16.26
19.17
25.98
31.68
21.99
26.16
30.99
34.40
39.52
43.40
46.98
-
-

TABLE 6: Results of enhancing experiment on HICO-DET [1].

Method
LFB-Res-50-max [63]
LFB-Res-101-nl-3l [63]
SlowFast [64]
LFB-Res-50-max [63]-HAKE
LFB-Res-101-nl-3l [63]-HAKE
SlowFast-Res-101 [64]-HAKE
GT-HAKE (detection [64])
SlowFast [64]+GT-HAKE (detection [64])
GT-HAKE (GT boxes)

Full
23.9
26.9
28.2

12.13%)
26.8 (
↑
3.72%)
27.9 (
↑
29.3 (
3.90%)
↑
39.55
42.23
47.27

Rare-20
5.2
7.8
9.6
48.08%)
7.7 (
↑
10.26%)
8.6 (
↑
10.4 (
8.33%)
↑
25.73
30.86
34.47

TABLE 7: Results of enhancing experiment on AVA [13]. As the rule
search on AVA is too expensive considering the enormous frames of
AVA, we did not conduct the +search rule (GT primitive) test.

APPENDIX E
DETAILED RESULTS AND ANALYSIS OF EXPERI-
MENTS

E.1 HAKE-based Enhancing

We conduct the challenging instance-based activity detec-
tion experiments on HICO-DET [1] and AVA [13] which
need to locate the active humans/objects and classify the
activities simultaneously, to show the enhancing effect of
HAKE. More detailed results are reorganized and shown in
Tab. 6 and Tab. 7.

E.2 HAKE-based Transfer Learning

To verify the transferability of HAKE, we design trans-
fer learning experiments on large-scale benchmarks: V-
COCO [12]
(10,346 images, 29 activities), Ambiguous-
HOI [61] (8,996 images, 87 activities), and AVA [13] (seen
as 234,630 images, 80 activities). We ﬁrst exclude the data
from three datasets in our database and then use the rest to
pre-train Activity2Vec and logical reasoning module with
156 activities and primitive labels. Then we change the
classiﬁcation layer (FC) size in the logical reasoning module
to ﬁt the activity categories of the target benchmark. Finally,
we freeze the primitive detector, Activity2Vec, and ﬁne-tune
the logical reasoning module only on the train set of the
target dataset if it exists (Ambiguous-HOI [61] just contains
testing data). Here, HAKE works like the ImageNet [58] and
Activity2Vec is used as a pre-trained knowledge engine to
promote downstream tasks. Detailed results are reorganized
and shown in Tab. 8-10. And we visualize some cases to
show the generalization ability of HAKE in Fig. 13.

APPENDIX F
IMPLEMENTING THE SCR TEST

We introduce the SCR test details in our main text as follows.

Method
Gupta et al. [12]
InteractNet [29]
GPNN [74]
iCAN [30]
TIN [31]
iCAN [30]+HAKE
TIN [31]+HAKE

AProle(Scenario1) AProle(Scenario2)

31.8
40.0
44.0
45.3
47.8

-
-
-
52.4
54.2

49.2 (
↑
51.3 (
↑

8.6%)
7.3%)

55.6 (
6.1%)
↑
59.7 (
10.1%)
↑

TABLE 8: Transfer learning results on V-COCO [12]. The relative
improvements are shown in the bracklets.

Method
iCAN [30]
TIN [31]
DJ-RN [61]
TIN [31]+HAKE
DJ-RN [61]+HAKE

mAP
8.14
8.22
10.37

10.56 (
28.5%)
↑
12.68 (
22.3%)
↑

TABLE 9: Results comparison on Ambiguous-HOI [61]. The relative
improvements are shown in the bracklets.

Method
AVA-TF [65]
LFB-Res-50-baseline [63]
LFB-Res-101-baseline [63]
AVA-TF [65]-HAKE
LFB-Res-50-baseline [63]-HAKE
LFB-Res-101-baseline [63]-HAKE

mAP
11.4
22.2
23.3

15.6 (
↑
23.4 (
↑
24.3 (
↑

36.8%)
5.4%)
4.3%)

TABLE 10: Transfer learning results on image-based AVA [13]. The
relative improvements are shown in the bracklets.

Fig. 13: Visualized cases to show the generalization ability of HAKE.
The ﬁrst row shows the training activities, while the second row shows
the testing activities that need compositional generalization ability
to infer the unseen activities, i.e., unseen verb-object compositions.
The prediction probabilities from HAKE are shown below the unseen
compositions.

Motivation. To verify that HAKE has gained the knowl-
edge to recognize the causal relations between primitives
and activities. We design the SCR test. We select 1,000
images from HICO-DET [1] and let HAKE and human
participants mask the critical regions of the activities at the
same time, then we mix the images masked by human and
HAKE and allocate them to some other participants. Their
task is to recognize the ongoing activities in these masked
images. In this way, we can evaluate HAKE by comparing
its human recognition performance degradation ability. If HAKE
degrades the human recognition performance signiﬁcantly
and even approaches the degradation effect of humans’

20

masking, we can verify that HAKE can ﬁnd out the critical
primitives according to the accurate primitive detection and
activity reasoning.

Data Preparation. Before the experiment, we ﬁrst pre-
process the images. We select 2,968 images from the HICO-
DET [1] test set to build an alternative pool. Several factors
are considered to select these images: 1) All the images we
select have objects that interact with a person. 2) For clarity,
the images’ width and height are all greater than 200 pixels.
3) Many other factors also affect the recognition, e.g., a
speciﬁc body part of a person occupies all the image, crowd
persons in one image, minimal persons or only a few human
body parts are seen (e.g., only a ﬁnger pointed to something
is seen, human detector and pose estimator usually fail
under this kind of situation). So we ﬁrst exclude these low-
quality images to ensure the image quality. 4) To avoid bias,
we try our best to make the selected images cover as many
activity categories as possible. Finally, 1,000 images are
manually selected. The image number of different activity
categories is about 105, as shown in Fig. 14b.

HAKE Masking Process. We introduce the masking pro-
cess of HAKE as follows. For humans/objects in the images,
HAKE ﬁrst detects them via an object detector [27] and a
pose estimator [28] to locate the whole body and body parts,
including feet, legs, hands, arms, hip, and head. During the
primitive detection and activity reasoning, HAKE would
give conﬁdences (0-1) to all primitives (human body parts
and the interacted objects), indicating their contributions to
the activity inference. More important primitives will have
larger probabilities to be masked ﬁrst. Thus, we rank the
primitives in a conﬁdence descending order and mask them
according to the thresholds.

Notably, as different body parts usually have different
characteristics, e.g., in daily activities, hands are usually
more critical than feet. A shared threshold for all parts is
unfair and would make all the masked PaSta bias to some
body parts. Thus, we calculate the conﬁdence means and
variances of different body parts of 2,968 images in Fig. 15a
with a box plot. Then we generate the conﬁdence threshold
of each body part according to their conﬁdence distribution,
respectively. Another factor is the human masking area
ratio, which is a constant in large-scale masking tests. On
our selected images, the human masking generates an area
ratio of 0.156. Thus, the above conﬁdence thresholds should
make the area ratio of HAKE masking close to 0.156 to keep
the comparison fair. Usually, more masking regions would
degrade the human recognition performance more (when
the masking area ratio is 1, the accuracy would be 0 be-
cause all the information of an image would be eliminated).
Accordingly, we ﬁnally set the thresholds of different body
parts as: 0.868 (head), 0.755 (arms), 0.763 (hands), 0.743 (hip),
0.824 (legs), 0.948 (feet). All the selected images contain
human-object interactions. Thus all the objects in them have
contributions to the activity inference. However, we still
have to decide when to mask these interacted objects, i.e.,
before or after some body parts. To this end, we set our
policy as to when all body parts have conﬁdences lower
than their thresholds, and the object occupies a relatively
small area, we only mask the object; when IoUs between the
human bounding box and all body parts are all greater than
0.9, we only mask the object (even if it occupies an extensive

0.92350.8473seen verb-unseen object unseen verb-seen object0.889236 <carry book><drink with cup> <sip spoon><carry frisbee> <drink with wine glass>  <eat with spoon>Testing with unseen compositionsTrainingabDatasetFew-shotmAPmAPMethodHICO-DET1TIN2713.4217.03TIN27+HAKE23.22(↑36.3%)23.16(↑72.6%)VCL3117.2123.63VCL31+HAKEGT-HAKE(detectedboxes)47.1138.74GT-HAKE(GTboxes)71.0362.65AVA20LFB-Res-50-max325.223.9LFB-Res-50-max32-HAKE26.8(↑12.13%)7.7(↑48.08%)SlowFast339.628.2SlowFast33-HAKE29.3(↑3.90%)10.4(↑8.33%)DatasetmAPMethodV-COCO24TIN2754.2TIN27+HAKE59.7(↑10.1%)Ambiguous-HOI25TIN278.22TIN27+HAKE10.56(↑28.5%)DJ-RN2510.37DJ-RN25+HAKE12.68(↑22.3%)AVA20AVA-TF3511.4AVA-TF35-HAKE15.6(↑36.8%)28.30 (↑19.8%) 25.04 (↑45.5%)21

Fig. 14: Activity recognition process sketch and statistics of the SCR test. a. In the SCR test, to test the semantic elimination effect, each masked
human-object pair (from humans/HAKE) would be given to human participants to recognize the existing activities. b. The image numbers of
different activities for the SCR test.

area). The ﬁnal mean masking area ratio of HAKE masking
is 0.166, which is very close to the human effect (0.156).

Human Activity Recognition on Masked Images. After
the above steps, we obtain 1,000 images masked by humans
and 1,000 images masked by HAKE. Next, we describe the
setting of human activity recognition on masked images. For
each image, we use bounding boxes to indicate the human-
object pairs. Each image can be seen as a choice question.
As one person can perform multiple actions with one object
simultaneously, thus each image can have multiple correct
answers. To be close to reality, i.e., humans may recognize a
part of the ongoing activities instead of all, we adopt the one-
choice question and set the metric as: if anyone of the ongoing
activities is chosen, this question/image is recognized right.
To control the difﬁculty, we provide 60 options for each
human-object pair including one of the right right options
and the other 59 wrong options. In our test, more or fewer
options would result in too low or high human perfor-
mances that affect the masking effect comparison between
HAKE and humans.

Next, we ask the other participants to recognize the

activities from these two masked image sets and record the
results. Participants need to choose the most-likely activity
for each human-object pair, as shown in Fig. 14a.

To be fair and avoid bias, each participant will be al-
located 500 randomly selected HAKE/human-masked im-
ages. During human recognition, 1,000 images are also
randomly disorganized. The ﬁnal results of masked activity
recognition are 35.6% (human masking) and 39.5% (HAKE
masking). From the result, to some extent, we can ﬁnd
out that HAKE achieves similar key regions localization and
recognition abilities to humans. But HAKE still has a rela-
tively lower degradation based on a slightly higher masking
area ratio. More detailed SCRs and the human recognition
accuracy of human- and HAKE- masked images are shown
in Fig. 16.

HAKE/Human Masking Difference Analysis. To fur-
ther study the masking difference between humans and
HAKE, we conduct an additional participant recognition
test. That is, we mixed the 2,000 masked images from human
and HAKE and ask the participants to make the binary
classiﬁcation: determine whether humans or HAKE masked

ab22

Fig. 15: Part level analysis. a. PaSta conﬁdences. The box plot of the estimated conﬁdences of different body parts. The triangle represents the
average conﬁdence of each part. b. Comparison between the human and HAKE masking results. b(1). Activities including body parts have clear
semantics and directly contacting with objects. b(2). Activities including body parts indirectly contacting with objects. b(3). Activities including
the persons interacting with an object and performing many different activities at the same time.

words, HAKE can create the critical semantic region masking
very well. Although having similar masking abilities, the
masking results of humans and HAKE still have some
differences (Fig. 15b):

1) In Fig. 15b(1), we ﬁnd that both humans and HAKE
can recognize the body parts directly contacting with objects
well, which usually have clear semantics. For example, for
activity hold frisbee, both HAKE and human can quickly lo-
cate hands and mask them. This shows that HAKE performs
well on simple activity recognition.

2) Meanwhile, Fig. 15b(2) shows the “abstract” ability
of HAKE. Human participants usually tend to mask the
body parts directly contacting with the object and ignore
the other body parts. However, HAKE usually ﬁnds out
the relationship between activity and the indirect contacted
body parts and provides prominent conﬁdence, bringing a
new perspective for us to understand the nature of activities.
For instance, for activity jump skateboard, human participants
are prone to mask the feet directly, but HAKE takes more
attention on arms, head, and hip, which are also crucial for
the judgment of jump but are often easier to be ignored.

3) Finally, HAKE has a certain “analytical” ability. As

Fig. 16: Activity recognition results of human- and HAKE- masked
images. With similar masking area ratios (top), HAKE can discover
key regions similar to humans and degrade the human recognition
performance well (bottom).

each masked image. In total, 12 participants were involved in
this test, and the classiﬁcation accuracy is 59.55%. In other

abbd23

shown in Fig. 15b(3), when a person is interacting with
an object and performing many different activities at the
same time, human participants tend to mask the object,
which effectively eliminates the interaction semantics on the
objectives (bicycle, horse) but leaves much semantics of the
verb. For example, in the case of ride bicycle, even we may
not recognize what does this person rides with (e.g., bicycle,
motorcycle, etc.), but we can still easily identify the ride after
the human masking. Meanwhile, HAKE is quite different.
It tends to mask the most related primitive it thought of. In
two cases, HAKE masked the hip and saved the semantics
of other activities like hold (hands) and straddle (legs, feet),
which may be because it believes ride is usually related to
hip most.

Discussion. Though HAKE has shown some decent
abilities, it still faces a noticeable gap with human intelli-
gence, e.g., it cannot always robustly perform well on all
activity images. However, its unique characteristics pro-
vide us with a new vision and show promising potentials.
In general, human masking is more concise and precise
visually because humans can locate the semantic entities
(human/object/part) more precisely. Nevertheless, HAKE
can only mask the detected object and human body parts
which are usually not as accurate as humans’ localization
and contain redundant pixels. This is also why the masking
area ratio of HAKE is tough to be lower than humans’ effect
and simultaneously keep the essential precision. However,
we believe that, with the progress of object detection, HAKE
would be more precise on activity primitive discovery.

