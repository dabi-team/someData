Analyzing Learned Representations of a Deep ASR Performance
Prediction Model

Zied Elloumi1,2

Laurent Besacier2

Olivier Galibert1

Benjamin Lecouteux2

1Laboratoire national de m´etrologie et d’essais (LNE) , France
2Univ. Grenoble Alpes, CNRS, Grenoble INP, LIG, F-38000 Grenoble, France
firstname.name@lne.fr
firstname.name@univ-grenoble-alpes.fr

Abstract

This paper addresses a relatively new task:
prediction of ASR performance on unseen
broadcast programs. In a previous paper, we
presented an ASR performance prediction sys-
tem using CNNs that encode both text (ASR
transcript) and speech, in order to predict word
error rate. This work is dedicated to the anal-
ysis of speech signal embeddings and text em-
beddings learnt by the CNN while training our
prediction model. We try to better understand
which information is captured by the deep
model and its relation with different condition-
ing factors. It is shown that hidden layers con-
vey a clear signal about speech style, accent
and broadcast type. We then try to leverage
these 3 types of information at training time
through multi-task learning. Our experiments
show that this allows to train slightly more ef-
ﬁcient ASR performance prediction systems
that - in addition - simultaneously tag the an-
alyzed utterances according to their speech
style, accent and broadcast program origin.

1

Introduction

Predicting automatic speech recognition (ASR)
performance on unseen speech recordings is an
In a previ-
important Grail of speech research.
ous paper (Elloumi et al., 2018), we presented
a framework for modeling and evaluating ASR
performance prediction on unseen broadcast pro-
grams. CNNs were very efﬁcient encoding both
text (ASR transcript) and speech to predict ASR
word error rate (WER). However, while achiev-
ing state-of-the-art performance prediction results,
our CNN approach is more difﬁcult to understand
compared to conventional approaches based on
engineered features such as TransRater1 for in-
stance. This lack of interpretability of the repre-
sentations learned by deep neural networks is a

1https://github.com/hlt-mt/TranscRater

general problem in AI. Recent papers started to
address this issue and analyzed hidden represen-
tations learned during training of different natu-
ral language processing models (Mohamed et al.,
2012; Wu and King, 2016; Belinkov and Glass,
2017; Shi et al., 2016; Belinkov et al., 2017; Wang
et al., 2017).

Contribution. This work is dedicated to the
analysis of speech signal embeddings and text em-
beddings learnt by the CNN during training of
our ASR performance prediction model. Our goal
is to better understand which information is cap-
tured by the deep model and its relation with con-
ditioning factors such as speech style, accent or
broadcast program type. For this, we use a data
set presented in (Elloumi et al., 2018) which con-
tains a large amount of speech utterances taken
from various collections of French broadcast pro-
grams. Following a methodology similar to (Be-
linkov and Glass, 2017), our deep performance
prediction model is used to generate utterance
level features that are given to a shallow classiﬁer
trained to solve secondary classiﬁcation tasks. It
is shown that hidden layers convey a clear signal
about speech style, accent and show. We then try
to leverage these 3 types of information at training
time through multi-task learning. Our experiments
show that this allows to train slightly more efﬁ-
cient ASR performance prediction systems that -
in addition - simultaneously tag the analyzed utter-
ances according to their speech style, accent and
broadcast program origin.

Outline. The paper is organized as follows. In
section 2, we present a brief overview of related
works and present our ASR performance predic-
tion system in section 3. Then, we detail our
methodology to evaluate learned representations
in section 4. Our multi-task learning experiments
for ASR performance prediction are presented in
section 5. Finally, section 6 concludes this work.

8
1
0
2

g
u
A
8
2

]
L
C
.
s
c
[

2
v
3
7
5
8
0
.
8
0
8
1
:
v
i
X
r
a

 
 
 
 
 
 
2 Related works

Several works tried to understand learned rep-
resentations for NLP tasks such as Automatic
Speech Recognition (ASR) and Neural Machine
Translation (NMT).

(Shi et al., 2016) and (Belinkov et al., 2017)
tried to better understand the hidden represen-
tations of NMT models which were given to a
shallow classiﬁer in order to predict syntactic la-
bels (Shi et al., 2016), part-of-speech labels or
semantic ones (Belinkov et al., 2017).
It was
shown that lower layers are better at POS tag-
ging, while higher layers are better at learning
semantics.
(Mohamed et al., 2012) and (Be-
linkov and Glass, 2017) analyzed the feature rep-
resentations from a deep ASR model using t-
SNE visualization (Maaten and Hinton, 2008) and
tried to understand which layers better capture
the phonemic information by training a shallow
phone classiﬁer. Also relevant is the work of
(Wang et al., 2017) who proposed an in-depth in-
vestigation on three kinds of speaker embeddings
learned for a speaker recognition task, i.e. i-vector,
d-vector and RNN/LSTM based sequence-vector
(s-vector). Classiﬁcation tasks were designed
to facilitate better understanding of the encoded
speaker representations. Multi-task learning was
also proposed to integrate different speaker em-
beddings and improve speaker veriﬁcation perfor-
mance.

3 ASR performance prediction system

In (Elloumi et al., 2018), we proposed a new ap-
proach using convolution neural networks (CNNs)
to predict ASR performance from a collection of
heterogeneous broadcast programs (both radio and
TV). We particularly focused on the combina-
tion of text (ASR transcription) and signal (raw
speech) inputs which both proved useful for CNN
prediction. We also observed that our system re-
markably predicts WER distribution on a collec-
tion of speech recordings.

To obtain speech transcripts (ASR outputs) for
the prediction model, we built our own French
ASR system based on the KALDI toolkit (Povey
et al., 2011). A hybrid HMM-DNN system was
trained using 100 hours of broadcast news from
Quaero2, ETAPE (Gravier et al., 2012), ESTER 1
& ESTER 2 (Galliano et al., 2005) and REPERE

2http://www.quaero.org

(Kahn et al., 2012) collections. ASR performance
was evaluated on the held out corpora presented
in table 2 (used to train and evaluate ASR predic-
tion) and its averaged value was 22.29% on the
TRAIN set, 22.35% on the DEV set and 31.20%
on the TEST set (which contains more challenging
broadcast programs).

Figure 1 shows our network architecture. The
in-
network input can be either a pure text
put, a pure signal input (raw signal) or a dual
(text+speech) input. To avoid memory issues, sig-
nals are downsampled to 8khz and models are
trained on six-second speech turns (shorter speech
turns are padded with zeros). For text input, the
architecture is inspired from (Kim, 2014) (green
the input is a matrix of dimen-
in Figure 1):
sions 296x100 (296 is the longest ASR hypothe-
sis length in our corpus ; 100 is the dimension of
pre-trained word embeddings on a large held out
text corpus of 3.3G words). For speech input, we
use the best architecture (m18) proposed in (Dai
et al., 2017) (colored in red in Figure 1) of dimen-
sions 48000 x 1 (48000 samples correspond to 6s
of speech).

For WER prediction, our best approach (called
CNNSof tmax) used sof tmax probabilities and an
external ﬁxed WERV ector which corresponds to
a discretization of the WER output space (see
(Elloumi et al., 2018) for more details). The
best performance obtained is 19.24% MAE3 using
text+speech input. Our ASR prediction system is
built using both Keras (Chollet et al., 2015) and
Tensorﬂow4.

In the next section, we analyze the represen-
tations learnt in the higher layers (3 blocks col-
ored in yellow and dotted in Figure 1) for pure
text (TXT), pure speech (RAW-SIG) and both
(TXT+RAW-SIG).

4 Evaluating learned representations

4.1 Methodology

In this section, we attempt to understand what our
best ASR performance prediction system (Elloumi
et al., 2018) learned. We analyze the text and
speech representations obtained by our architec-
ture. Alike (Belinkov and Glass, 2017), the joint
text+speech model is used to generate utterance

3Mean Absolute Error (MAE) is a common metric to eval-
uate WER prediction ; it computes the absolute deviation be-
tween the true and predicted WERs, averaged over the num-
ber of utterances in the test set.
4https://www.tensorﬂow.org

Figure 1: Architecture of our CNN with text (green) and signal (red) inputs for WER prediction

level features (hidden representations of speech
turns colored in yellow in Figure 1) that are given
to a shallow classiﬁer trained to solve secondary
classiﬁcation tasks such as:

• STYLE: classify the utterances between
(spontaneous and non spontaneous) styles
(see table 1),

• ACCENT: classify the utterances between
native and non native speech (see also table
1, we used the speaker annotations provided
with our datasets in order to label our utter-
ances in native/non native speech),

• SHOW: classify the utterances in different
broadcast programs (as described in table 2,
each utterance of our corpus is labeled with a
broadcast program name).

As a more visual analysis, we also plot an ex-
ample of hidden representations projected to a 2-D
space using t-distributed Stochastic Neighbor Em-
bedding (t-SNE) (Maaten and Hinton, 2008).5

4.2 Shallow classiﬁers

three

shallow classiﬁers

We built
(SHOW,
STYLE, ACCENT) with a similar architecture.
The classiﬁer is a feed-forward neural network
with one hidden layer (size of the hidden layer
is set to 128) followed by dropout (rate of 0.5)
and a ReLU non-linearity. Finally, a sof tmax
layer is used for mapping onto the label set size.
We chose this simple formulation as we are inter-
ested in evaluating the quality of the representa-
tions learned by our ASR prediction model, rather
than optimizing the secondary classiﬁcation tasks.

5https://lvdmaaten.github.io/tsne/

code/tsne_python.zip

The network input size depends on which layer
to analyze (see ﬁgure 1). Training is performed
using Adam (Kingma and Ba, 2014) (using de-
fault parameters) over shufﬂed mini-batches in or-
der to minimize the cross-entropy loss. The mod-
els are trained for 30 epochs with a batch size of
16 speech utterances. After training, we keep the
model with the best performance on DEV set and
report its performance on the TEST set. The clas-
siﬁer outputs are evaluated in terms of accuracy.

4.3 Data

A data set from (Elloumi et al., 2018) was em-
ployed in our experiments, divided into three sub-
sets: training (TRAIN), development (DEV) and
test (TEST). Speech utterances come from vari-
ous French broadcast collections gathered during
projects or shared tasks: Quaero, ETAPE, ESTER
1 & ESTER 2 and REPERE.

The TEST set contains unseen broadcast pro-
grams that are different from those present in
TRAIN and DEV (Elloumi et al., 2018).

Category

TRAIN DEV TEST

Non Spontaneous
Spontaneous
Native
Non Native

54250
13277
44487
23040

6101
1403
4945
2559

3109
3728
5298
1539

Table 1: Distribution of our utterances between
non spontaneous and spontaneous styles, native
and non native accents

Tables 1 and 2 show the whole data set in terms
of speech turns available for each classiﬁcation
task. We clearly see that the data is unbalanced for
the three categories (STYLE, ACCENT, SHOW).
Since we are interested in evaluating the discrim-
inative power of our learned representations for

Show

TRAIN DEV

FINTER-DEBATE
FRANCE3-DEBATE
LCP-PileEtFace
RFI
RTM
TELSONNE
Total

7632
928
4487
25565
24198
4717
67527

833
77
525
2831
2745
493
7504

Table 2: Number of utterances for each broadcast
program

these 3 tasks, we extracted a balanced version of
our TRAIN/DEV/TEST sets by ﬁltering among
over-represented labels (ﬁnal number of kept ut-
terances corresponds to bold numbers in table 1
and 2 ). Table 3 shows the distribution of our ﬁnal
balanced TRAIN/DEV/TEST sets as well as the
number of categories for each task.6

#Catg

SHOW
STYLE
ACCENT

5
2
2

Turns of speech per category
TEST
DEV
TRAIN
-
493×5
4487×5
3109×2
1403×2
13277×2
1539×2
2559×2
23040×2

Table 3: Description of our balanced data set for
each category

4.4 Results

For each classiﬁcation task, we build a shallow
classiﬁer using the hidden representations of TXT,
RAW-SIG and TXT+RAW-SIG blocks as input.
The experimental results are presented in table 4
for both DEV and TEST sets separated by two ver-
tical bars (||).

Classiﬁcation performance is all above a ran-
dom baseline accuracy (>50% for STYLE and
ACCENT and >20% for SHOW). This shows
that training a deep WER prediction system gives
representation layers that contain a meaningful
amount of information about speech style, speech
accent and broadcast program label. Predicting
utterance style (spontaneous/non spontaneous) is
slightly easier than predicting accent (native/non
native) especially from text input. One expla-
nation might be that speech utterances are short
(< 6s) while accent identiﬁcation needs proba-
bly longer sequences. We also observe that us-
ing both text and speech improves the learned
representations for the STYLE task while it is

6For

the SHOW classiﬁcation task,

the FRANCE3-
DEBATE shows were ﬁnally removed since they represent a
too small amount of speech turns.

less clear for the ACCENT task (for which im-
provement seen on DEV is not conﬁrmed on
TEST). Finally, text input is signiﬁcantly better
than speech input whereas we could have expected
better performance from speech for the SHOW
task (speech signals convey information about the
audio characteristics of a broadcast program). It
means that text input contains correlated infor-
mation with broadcast-program type, speech style
and speaker’s accent.
In case of SHOW task,
our performance prediction system is able to cap-
ture information (vocabulary, topic, syntax, etc.)
about a speciﬁc broadcast program type, based on
textual features and to differ it from others (ra-
dio programs, TV debate programs, phone calls,
broadcast news programs, etc.). Likewise, the
textual information captured is very different be-
tween spontaneous/non-spontaneous speech styles
and native/non-native speaker’s accents.

Among the representations analyzed, the out-
puts of the CNNs (A1,B1) lead to the best classiﬁ-
cation results, in line with previous ﬁndings about
convolutions as feature extractors. Performance
then drops using the higher (fully connected) lay-
ers that do not generate better representations for
detecting style, accent or show.

Layer

Dim.

SHOW

STYLE

ACCENT

A1
A2
A3

B1
B2
B3
B4

C1 (A3+B4)
C2
Random

TXT

1280
256
128

57.12||-
54.89||-
51.04||-

80.72||68.99
80.01||69.56
79.23||68.27

70.75||66.54
69.30||69.43
68.25||70.89

RAW-SIG

512
512
256
128

256
128
-

72.92||58.64
72.20||58.41
72.38||58.44
72.38||58.52

42.35||-
41.22||-
41.22||-
40.77||-
TXT + RAW-SIG
57.04||-
53.06||-
20.00

81.29||70.36
79.62||70.55
50.00

64.60||55.85
64.44||54.84
64.50||54.65
64.74||54.87

71.41||65.98
70.01||65.20
50.00

Table 4: Show/Style/Accent classiﬁcation accu-
racies using representations from different layers
learned during the training of our ASR WER pre-
diction system.

We visualize an example of utterance represen-
tations from C2(TXT+RAW-SIG) layer in ﬁgure
2 using the t-SNE. For a ﬁxed utterance dura-
tion 4s≤D<5s (716 speech turns) and 5s≤D<6s
(489 speech turns), non spontaneous utterances are
plotted in blue while spontaneous ones are in pink.
The C2 layer produces clusters which shows that
spontaneous utterances are in the upper-left part

edge of these labels (style, accent, show) at train-
ing time on prediction systems qualities. For this,
we perform multi-task learning providing the ad-
ditional information about broadcast type, speech
style and speaker’s accent during training. The ar-
chitecture of the multi-task model is similar to the
single-task WER prediction model of Figure 1 but
we add additional outputs: a sof tmax function
is added for each new classiﬁcation task after the
last fully connected layer (C2). The output dimen-
sion depends on the task: 6 for SHOW and 2 for
STYLE and ACCENT tasks.

We use the full (unbalanced) data set described
in tables 1 and 2. Training of the multitask model
uses Adadelta update rule and all parameters are
initialized from scratch (8.70M). Models are per-
formed for 50 epochs with batch size of 32. MAE
is used as the loss function for WER prediction
task while cross-entropy loss is used for the clas-
siﬁcation tasks.

In the composite (multitask) loss, we assign a
weight of 1 for MAE loss (main task) and a smaller
weight of 0.3 (tuned using a grid search on DEV
dataset) for cross-entropy (secondary classiﬁca-
tion task) loss(es).

After training, we take the model that lead to the
best MAE on DEV set and report its performance
on TEST. We build several models that simulta-
neously address 1, 2, 3 and 4 tasks. The mod-
els are evaluated with a speciﬁc metric for each
task: MAE & Kendall7 for WER prediction task
and Accuracy for classiﬁcation tasks.

Table 5 summarizes the experimental results on
DEV and TEST sets, separated by two vertical
bars (||). We considered the mono-task model de-
scribed in (Elloumi et al., 2018) (and summarized
in section 3) as a baseline system.

We recall that we evaluated the SHOW classiﬁ-
cation task only on the DEV set (TEST broadcast
programs are new and were unseen in the TRAIN).
First of all, we notice that performance of classi-
ﬁcation tasks in muti-task scenarios are very good:
we are able to train efﬁcient ASR performance
prediction systems that simultaneously tag the an-
alyzed utterances according to their speech style,
accent and broadcast program origin. Such multi-
task systems might be useful diagnostic tools to
analyze and predict ASR on large speech collec-
tions. Moreover, our best multi-task systems dis-

7Correlation between true ASR values and predicted ASR

values

(a) 4s≤D<5s

(b) 5s≤D<6s

Figure 2: Visualization of utterance representations
from C2 layer for different speech styles (S spon-
taneous - NS non spontaneous) - (a) utt. length is
4s≤D<5s and (b) 5s≤D<6s

of the 2D space. This suggests that C2 hidden rep-
resentation captures a weak signal about speaking
style.

Finally, ﬁgure 3 is the confusion matrix pro-
duced using C2(TXT+RAW-SIG) layer. The clas-
siﬁers very well predicted TELSONNE category
(Accuracy of 82%), which contains many phone
calls from the radio listeners. This show is rather
different from the 4 other shows in DEV (broad-
cast debates and news).

Figure 3: Confusion matrix for SHOW classiﬁ-
cation using C2(TXT+RAW-SIG) layer as input,
evaluated on DEV

5 Multi-task learning

We have seen in the previous section that, while
training an ASR performance prediction system,
hidden layers convey a clear signal about speech
style, accent and show. This suggests that these
3 types of information might be useful to struc-
ture the deep ASR performance prediction models.
In this section, we investigate the effect of knowl-

Models

Performance prediction task

Classiﬁcation tasks

MAE

Kendall

SHOW

STYLE

ACCENT

WER (Elloumi et al., 2018)

15.24||19.24

45.00||46.83

-

-

-

Baseline: Mono-task

WER SHOW
WER STYLE
WER ACCENT

WER STYLE ACCENT
WER SHOW ACCENT
WER SHOW STYLE

2-task

14.83||19.15
15.07||19.66
15.05||19.60

47.25||47.05
45.92||45.49
46.17||45.60

3-task

15.12||20.23
14.94||19.76
14.90||19.14

45.75||44.09
46.19||43.61
45.87||47.28

4-task

99.29||-
-
-

-
98.38||-
99.12||-

-
99.01||65.24
-

-
-
91.72|| 75.30

98.63||69.07
-
99.47||81.98

88.99|| 77.46
89.87||71.44
-

WER SHOW STYLE ACCENT
WER ALL COMBINED OUTPUTS

15.15||19.64
14.50 ||18.87

45.59||45.42
48.16||48.63

99.04||-
-

99.29||81.55
-

91.92||73.60
-

Table 5: Evaluation of ASR performance prediction with multi-tasks models (DEV ||T EST ) computed
with MAE and Kendall - secondary classiﬁcation tasks accuracy is also reported

play a better performance (MAE, Kendall) than
the baseline system, which means that the implicit
information given about style, accent and broad-
cast program type can be helpful to structure the
system’s predictions. For example, in 2-task case,
the best model is obtained on WER+SHOW tasks
with a difference of +0.41%, +2.25% for MAE and
Kendall respectively (on DEV) compared to the
baseline on WER prediction task. However, it is
also important to mention that the impact of multi-
task learning on the main task (ASR performance
prediction) is limited: only slight improvements
on the test set are observed for MAE and Kendall
metrics. Anyway, the systems trained seem com-
plementary since their combination (averaging,
over all multi-task systems, predicted WERs at ut-
terance level) leads to signiﬁcant performance im-
provement (MAE and Kendall).

6 Conclusion

This paper presented an analysis of learned repre-
sentations of our deep ASR performance predic-
tion system. Experiments show that hidden layers
convey a clear signal about speech style, accent,
and broadcast type. We also proposed a multi-task
learning approach to simultaneously predict WER
and classify utterances according to style, accent
and broadcast program origin.

References

Yonatan Belinkov and James Glass. 2017. Analyz-
ing hidden representations in end-to-end automatic
speech recognition systems. In Advances in Neural
Information Processing Systems, pages 2438–2448.

Yonatan Belinkov, Llu´ıs M`arquez, Hassan Sajjad,

Nadir Durrani, Fahim Dalvi, and James Glass. 2017.
Evaluating layers of representation in neural ma-
chine translation on part-of-speech and semantic
tagging tasks. In Proceedings of the Eighth Interna-
tional Joint Conference on Natural Language Pro-
cessing (Volume 1: Long Papers), volume 1, pages
1–10.

Franc¸ois Chollet et al. 2015. Keras. https://

github.com/fchollet/keras.

Wei Dai, Chia Dai, Shuhui Qu, Juncheng Li, and
Samarjit Das. 2017. Very deep convolutional neural
networks for raw waveforms. In Acoustics, Speech
and Signal Processing (ICASSP), 2017 IEEE Inter-
national Conference on, pages 421–425. IEEE.

Zied Elloumi, Laurent Besacier, Olivier Galibert, Juli-
ette Kahn, and Benjamin Lecouteux. 2018. Asr per-
formance prediction on unseen broadcast programs
using convolutional neural networks. In IEEE Inter-
national Conference on Acoustics, Speech and Sig-
nal Processing (ICASSP).

Sylvain Galliano, Edouard Geoffrois, Djamel Mostefa,
Khalid Choukri, Jean-Franc¸ois Bonastre, and Guil-
laume Gravier. 2005. The ester phase ii evaluation
campaign for the rich transcription of french broad-
cast news. In Interspeech, pages 1149–1152.

Guillaume Gravier, Gilles Adda, Niklas Paulson,
Matthieu Carr´e, Aude Giraudel, and Olivier Galib-
ert. 2012. The etape corpus for the evaluation of
speech-based tv content processing in the french lan-
guage. In LREC-Eighth international conference on
Language Resources and Evaluation, page na.

Juliette Kahn, Olivier Galibert, Ludovic Quintard,
Matthieu Carr´e, Aude Giraudel, and Philippe Joly.
2012. A presentation of the repere challenge.
In
Content-Based Multimedia Indexing (CBMI), 2012
10th International Workshop on, pages 1–6. IEEE.

Yoon Kim. 2014.

works for sentence classiﬁcation.
arXiv:1408.5882.

Convolutional neural net-
arXiv preprint

Diederik P. Kingma and Jimmy Ba. 2014. Adam:
CoRR,

A method for stochastic optimization.
abs/1412.6980.

Laurens van der Maaten and Geoffrey Hinton. 2008.
Journal of machine

Visualizing data using t-sne.
learning research, 9(Nov):2579–2605.

Abdel-rahman Mohamed, Geoffrey Hinton, and Ger-
ald Penn. 2012. Understanding how deep belief
In Acous-
networks perform acoustic modelling.
tics, Speech and Signal Processing (ICASSP), 2012
IEEE International Conference on, pages 4273–
4276. IEEE.

Daniel Povey, Arnab Ghoshal, Gilles Boulianne, Lukas
Burget, Ondrej Glembek, Nagendra Goel, Mirko
Hannemann, Petr Motlicek, Yanmin Qian, Petr
Schwarz, et al. 2011. The kaldi speech recog-
In IEEE 2011 workshop on auto-
nition toolkit.
matic speech recognition and understanding, EPFL-
CONF-192584. IEEE Signal Processing Society.

Xing Shi, Inkit Padhi, and Kevin Knight. 2016. Does
string-based neural mt learn source syntax? In Pro-
ceedings of the 2016 Conference on Empirical Meth-
ods in Natural Language Processing, pages 1526–
1534.

Shuai Wang, Yanmin Qian, and Kai Yu. 2017. What
In Inter-

does the speaker embedding encode?
speech, volume 2017, pages 1497–1501.

Zhizheng Wu and Simon King. 2016.

Investigating
gated recurrent neural networks for speech synthe-
sis. CoRR, abs/1601.02539.

