2
2
0
2

r
a

M
3

]

G
L
.
s
c
[

1
v
5
0
5
1
0
.
3
0
2
2
:
v
i
X
r
a

Large-scale Optimization of Partial AUC in a Range of False
Positive Rates

Yao Yao
Department of Mathematics
The University of Iowa
Iowa City, IA 52242, USA

Qihang Lin
Tippie College of Business
The University of Iowa
Iowa City, IA 52242, USA

Tianbao Yang
Department of Computer Science
The University of Iowa
Iowa City, IA 52242, USA

yao-yao-2@uiowa.edu

qihang-lin@uiowa.edu

tianbao-yang@uiowa.edu

Abstract
The area under the ROC curve (AUC) is one of the most widely used performance measures
for classiﬁcation models in machine learning. However, it summarizes the true positive rates
(TPRs) over all false positive rates (FPRs) in the ROC space, which may include the FPRs
with no practical relevance in some applications. The partial AUC, as a generalization of
the AUC, summarizes only the TPRs over a speciﬁc range of the FPRs and is thus a more
suitable performance measure in many real-world situations. Although partial AUC opti-
mization in a range of FPRs had been studied, existing algorithms are not scalable to big
data and not applicable to deep learning. To address this challenge, we cast the problem
into a non-smooth diﬀerence-of-convex (DC) program for any smooth predictive functions
(e.g., deep neural networks), which allowed us to develop an eﬃcient approximated gradient
descent method based on the Moreau envelope smoothing technique, inspired by recent ad-
vances in non-smooth DC optimization. To increase the eﬃciency of large data processing,
we used an eﬃcient stochastic block coordinate update in our algorithm. Our proposed
algorithm can also be used to minimize the sum of ranked range loss, which also lacks eﬃ-
cient solvers. We established a complexity of ˜O(1/ǫ6) for ﬁnding a nearly ǫ-critical solution.
Finally, we numerically demonstrated the eﬀectiveness of our proposed algorithms for both
partial AUC maximization and sum of ranked range loss minimization.

1. Introduction

The area under the receiver operating characteristic (ROC) curve (AUC) is one of the most
widely used performance measures for classiﬁers in machine learning, especially when the
data is imbalanced between the classes (Hanley and McNeil, 1982; Bradley, 1997). Typically,
the classiﬁer produces a score for each data point. Then a data point is classiﬁed as positive
if its score is above a chosen threshold; otherwise, it is classiﬁed as negative. Varying the
threshold will change the true positive rate (TPR) and the false positive rate (FPR) of the
classiﬁer. The ROC curve shows the TPR as a function of the FPR that corresponds to the
same threshold. Hence, maximizing the AUC of a classiﬁer is essentially maximizing the

©2022 Yao, Lin, Yang.

License: CC-BY 4.0, see https://creativecommons.org/licenses/by/4.0/.

 
 
 
 
 
 
Yao, Lin, Yang

classiﬁer’s average TPR over all FPRs from zero to one. However, for some applications,
some FPR regions have no practical relevance. So does the TPR over those regions. For
example, in clinical practice, a high FPR in diagnostic tests often results in a high monetary
cost, so people may only need to maximize the TPR when the FPR is low (Dodd and Pepe,
2003; Ma et al., 2013; Yang et al., 2019). Moreover, since two models with the same AUC
can still have diﬀerent ROCs, the AUC does not always reﬂect the true performance of a
model that is needed in a particular production environment (Bradley, 2014).

As a generalization of the AUC, the partial AUC (pAUC) only measures the area under
the ROC curve that is restricted between two FPRs. A probabilistic interpretation of the
pAUC can be found in Dodd and Pepe (2003). In contrast to the AUC, the pAUC represents
the average TPR only over a relevant range of FPRs and provides a performance measure
that is more aligned with the practical needs in some applications.

In literature, the existing algorithms for training a classiﬁer by maximizing the pAUC in-

clude the boosting method (Komori and Eguchi, 2010) and the cutting plane algorithm (Narasimhan and Agarwal,
2013b,a, 2017). However, the former has no theoretical guarantee, and the latter applies
only to linear models. More importantly, both methods require processing all the data in
each iteration and thus, become computationally ineﬃcient for large datasets.

In this paper, we proposed an approximate gradient method for maximizing the pAUC
that works for nonlinear models (e.g., deep neural networks) and only needs to process
randomly sampled positive and negative data points of any size in each iteration. In par-
ticular, we formulated the maximization of the pAUC as a non-smooth diﬀerence-of-convex
(DC) program (Tao and An, 1997; Le Thi and Dinh, 2018). Due to non-smoothness, most
existing DC optimization algorithms cannot be applied to our formulation. Motivated by
Sun and Sun (2021), we approximate the two non-smooth convex components in the DC
program by their Moreau envelopes and obtained a smooth approximation of the problem,
which will be solved using the gradient descent method. Since the gradient of the smooth
problem cannot be calculated explicitly, we approximated the gradient by solving the two
proximal-point subproblems deﬁned by each convex component using the stochastic block
coordinate descent (SBCD) method. Our method, besides its low per-iteration cost, has
a rigorous theoretical guarantee, unlike the existing methods. In fact, we show that our
method ﬁnds a nearly ǫ-critical point of the pAUC optimization problem in ˜O(ǫ−
6) itera-
tions with only small samples of positive and negative data points processed per iteration.1
This is the main contribution of this paper.

Note that, for non-convex non-smooth optimization, the existing stochastic methods (Davis and Grimmer,

4) iterations un-
2019; Davis and Drusvyatskiy, 2018) ﬁnd an nearly ǫ-critical point in O(ǫ−
6) iterations because our problem
der a weak convexity assumption. Our method needs O(ǫ−
is a DC problem with both convex components non-smooth which is much more challeng-
ing than a weakly non-convex minimization problem.
In addition, our iteration number
matches the known best iteration complexity for non-smooth non-convex min-max opti-
mization (Raﬁque et al., 2021; Liu et al., 2021) and non-smooth non-convex constrained
optimization (Ma et al., 2020).

In addition to pAUC optimization, our method can be also used to minimize the sum
of ranked range (SoRR) loss, which can be viewed as a special case of pAUC optimization.

1. Throughout the paper, ˜O(·) suppresses all logarithmic factors.

2

Large-scale Optimization of Partial AUC

Many machine learning models are trained by minimizing an objective function, which is
deﬁned as the sum of losses over all training samples (Vapnik, 1992). Since the sum of losses
weights all samples equally, it is insensitive to samples from minority groups. Hence, the
sum of top-k losses (Shalev-Shwartz and Wexler, 2016; Fan et al., 2017) is often used as an
alternative objective function because it provides the model with robustness to non-typical
instances. However, the sum of top-k losses can be very sensitive to outliers, especially
when k is small. To address this issue, Hu et al. (2020) proposed the SoRR loss as a new
learning objective, which is deﬁned as the sum of a consecutive sequence of losses from any
range after the losses are sorted. Compared to the sum of all losses and the sum of top-k
losses, the SoRR loss maintains a model’s robustness to a minority group but also reduces
the model’s sensitivity to outliers. See Fig.1 in Hu et al. (2020) for an illustration of the
beneﬁt of using the SoRR loss over other ways of aggregating individual losses.

To minimize the SoRR loss, Hu et al. (2020) applied a diﬀerence-of-convex algorithm
(DCA) (Tao and An, 1997; An and Tao, 2005), which linearizes the second convex compo-
nent and solves the resulting subproblem using the stochastic subgradient method. DCA
has been well studied in literature; but when the both components are non-smooth, as in
our problem, only asymptotic convergence results are available. To establish the total num-
ber of iterations needed to ﬁnd an ǫ-critical point in a non-asymptotic sense, most existing
studies had to assume that at least one of the components is diﬀerentiable, which is not the
case in this paper. Using the approximate gradient method presented in this paper, one can
ﬁnd a nearly ǫ-critical point of the SoRR loss optimization problem in ˜O(ǫ−

6) iterations.

2. Related Works

The pAUC has been studied for decades (McClish, 1989; Thompson and Zucchini, 1989;
Jiang et al., 1996). However, most studies focused on its estimation (Dodd and Pepe, 2003)
and application as a performance measure, while only a few studies were devoted to numeri-
cal algorithms for optimizing the pAUC. Besides the boosting method (Komori and Eguchi,
2010) and the cutting plane algorithm (Narasimhan and Agarwal, 2013b,a, 2017) mentioned
in the previous section, Ueda and Fujino (2018) and Yang et al. (2021) developed surrogate
optimization techniques that directly maximize a smooth approximation of the pAUC or the
two-way pAUC (Yang et al., 2019). However, their approaches can only be applied when the
FPR starts from exactly zero. On the contrary, our method allows the FPR to start from
any value between zero and one. Wang and Chang (2011) and Ricamato and Tortorella
(2011) developed algorithms that use the pAUC as a criterion for creating a linear combi-
nation of multiple existing classiﬁers while we consider directly train a classiﬁer using the
pAUC.

DC optimization has been studied since the 1950s (Alexandroﬀ, 1950; Hartman, 1959).
We refer interested readers to Tuy (1995); Tao and An (1998, 1997); Pang et al. (2017);
Le Thi and Dinh (2018), and the references therein. The actively studied numerical meth-
ods for solving a DC program include DCA (Tao and An, 1998, 1997; An and Tao, 2005;
Souza et al., 2016), which is also known as the concave-convex procedure (Yuille and Rangarajan,
2003; Sriperumbudur and Lanckriet, 2009; Lipp and Boyd, 2016), the proximal DCA (Sun et al.,
2003; Moudaﬁ and Maing´e, 2006; Moudaﬁ, 2008; An and Nam, 2017), and the direct gra-
dient methods (Khamaru and Wainwright, 2018). However, when the two convex compo-

3

Yao, Lin, Yang

nents are both non-smooth, the existing methods have only asymptotic convergence re-
sults except the method by Abbaszadehpeivasti et al. (2021), who considered a stopping
criterion diﬀerent from ours. When at least one component is smooth, non-asymptotic
convergence rates have been established with and without the Kurdyka- Lojasiewicz (KL)
condition (Souza et al., 2016; Artacho et al., 2018; Wen et al., 2018; An and Nam, 2017;
Khamaru and Wainwright, 2018).

The algorithms mentioned above are deterministic and require processing the entire
dataset per iteration. Stochastic algorithms that process only a small data sample per
iteration have been studied (Mairal, 2013; Nitanda and Suzuki, 2017; Le Thi et al., 2017;
Deng and Lan, 2020; He et al., 2021). However, they all assumed smoothness on at least
one of the two convex components in the DC program. The stochastic methods of Xu et al.
(2019); Thi et al. (2021); An et al. (2019) can be applied when both components are non-
smooth but their methods require an unbiased stochastic estimation of the gradient and/or
value of the two components, which is not available in the DC formulation of the pAUC
maximization problem in this paper.

The technique most related to our work is the smoothing method based on the Moreau
envelope (Ellaia, 1984; Gabay, 1982; Hiriart-Urruty, 1985, 1991; Sun and Sun, 2021; Moudaﬁ,
2021). Our work was motivated by Sun and Sun (2021); Moudaﬁ (2021), but the important
diﬀerence is that they studied deterministic methods and assumed either that one func-
tion is smooth or that the proximal-point subproblems can be solved exactly, which we do
not assume. However, Sun and Sun (2021) developed an algorithm when there were linear
equality constraints, which we do not consider in this paper.

Finally, it was brought to our attention that a recent work (Zhu et al., 2022) also consid-
ered partial AUC maximization with a non-convex objective. The diﬀerence between this
work and (Zhu et al., 2022) is that: (i) their algorithms can only optimize one-way pAUC
with FPR in a range (0, β); in contrast our algorithm is more general that can be applicable
to FRP in any range (α, β) with α

0, which is more challenging for α > 0.

≥

3. Preliminary

1,

→

∈ {
R be the predictive model parameterized by a vector w

We consider a classical binary classiﬁcation problem, where the goal is to build a predictive
Rp. Let
model that predicts a binary label y
∈
Rd, which produces
hw : Rp
a score hw(x) for x. Then x is classiﬁed as positive (y = 1) if hw(x) is above a chosen
threshold and classiﬁed as negative (y =
N+
x+
x−i }
i
i }
{
{

be the sets of feature vectors of positive and
negative training data, respectively. The problem of learning hw through maximizing its
empirical AUC on the training data can be formulated as

based on a feature vector x

1), otherwise.

X+ =

−
N−
i

1
}

and

Let

X−

−

=

∈

max
w

1
N+N

N+

N−

−

Xi=1

Xi=1

1(hw(x+

i ) > hw(x−j )),

where 1(
) is the indicator function which equals one if the inequality inside the parentheses
·
holds and equals zero, otherwise. According to the introduction, pAUC can be a better
1.
performance measure of hw than AUC. Consider two FPRs α and β with 0

α < β

≤

≤

4

Large-scale Optimization of Partial AUC

For simplicity of exposition, we assume N
and n = N
−
β can be formulated as

α
β. The problem of maximizing the empirical pAUC with FPR between α and

β are both integers. Let m = N

α and N

−

−

−

max
w

1
N+(n

m)

N+

n

1(hw(x+

i ) > hw(x−[j])),

(1)

−

Xj=m+1

Xi=1
where [j] denotes the index of the jth largest coordinate in vector (hw(x−j ))N−
j=1 with ties
m) in (1) is a normalizer that makes the objective
broken arbitrarily. Note that N+(n
R
value between zero and one. Solving (1) is challenging due to discontinuity. Let ℓ : R
be a diﬀerential non-increasing loss function. Problem (1) can be approximated by the
following loss minimization problem

→

−

min
w

1
N+(n

−

m)

N+

n

Xi=1

Xj=m+1

ℓ(hw(x+
i )

hw(x−[j])).

−

(2)

To facilitate the discussion, we ﬁrst introduce a few notations. Given a vector S =

(si)N

i=1 ∈

RN and an integer l with 0

l

≤

≤

N , the sum of the top-l values in S is

φl(S) :=

l
j=1 s[j],

(3)

P
where [j] denotes the index of the jth largest coordinate in S with ties broken arbitrarily.
For integers l1 and l2 with 0

N , we deﬁne

≤

l1 < l2 ≤
ψl1,l2(S) := φl2(S)

φl1(S),

−

(4)

which is the sum from the (l1 + 1)th to the l2th (inclusive) largest coordinates of S, also
called a sum of ranked range (SoRR). In addition, we deﬁne vectors

Si(w) := (sij(w))Ni
j=1

for i = 1, . . . , N+, where sij(w) := ℓ(hw(x+
i )
Since ℓ is non-increasing, the jth largest coordinate of Si(w) is ℓ(hw(x+
a result, for any i, we have

−

i ))

hw(x−j )) for i = 1, . . . , N+ and j = 1, . . . , N

.
−
hw(x−[j])). As

−

n
j=m+1 ℓ(hw(x+

i ))

−

hw(x−[j])) = ψm,n(Si(w)).

Hence, by (3) and (4), after dropping the normalizer, (2) can be equivalently written as

P

F ∗ = min
w

= min

w {

N+
i=1 ψm,n(Si(w))
F (w) := f n(w)
P

f m(w)
}

,

−

where

f l(w) =

N+
i=1 φl(Si(w))

for l = m, n.

P

5

(5)

(6)

Yao, Lin, Yang

Next, we introduce an interesting special case of (5), namely, the problem of minimizing
R does
SoRR loss. We still consider a supervised learning problem but the target y
∈
Rp using
not need to be binary. We want to predict y based on a feature vector x
function hw(x). With a little abuse of notation, we measure the discrepancy between hw(x)
R+ is a loss function. We consider learning the
and y by ℓ(hw(x), y), where ℓ : R2
→
R for
N
model’s parameter w from a training set
j=1, where xj ∈
(xj, yj)
}
{
j = 1, . . . , N , by minimizing the SoRR loss. More speciﬁcally, we deﬁne vector

Rp and yj ∈

=

D

∈

S(w) = (sj(w))N

j=1,

where sj(w) := ℓ(hw(xj), yj), j = 1, . . . , N . Recall (3) and (4). For any integers m and n
with 0
N , the problem of minimizing the SoRR loss with a range from m + 1
to n is formulated as minw ψm,n(S(w)), which can be also reformulated as (5) with

m < n

≤

≤

f l = φl(S(w)) for l = m, n.

(7)

If we view Si(w) and S(w) only as vector-value functions of w but ignore how they are
formulated using data, (7) is a special case of (6) with N+ = 1 and N

= N .

−

4. Nearly Critical Point and Moreau Envelope Smoothing

We ﬁrst develop a stochastic algorithm for (5) with f l deﬁned in (6). To do so, we make
the following assumptions, which are satisﬁed by many smooth hw’s and ℓ’s.

Assumption 1 (a) sij(w) is smooth and there exists L
v
w
L
k
such that

≥
k∇
Rd, i = 1, . . . , N+ and j = 1, . . . , N

Rd, i = 1, . . . , N+ and j = 1, . . . , N

sij(w)
. (b) There exists B
. (c) F ∗ >

for any w, v

B for any w

sij(w)

sij(v)
k ≤
0
.

− ∇

0 such that2

−

∈

−

−

≥
−∞

∈

k
k∇
Given f : Rd

k ≤
R

→

+

, the subdiﬀerential of f is

∞}

∪ {

Rd

f (v)

(cid:12)
(cid:12)
(cid:12)
(cid:12)
f (w) +

∂f (w) =

ξ

(cid:26)

∈

h(w) + ξ⊤(v

≥

w) + o(
k

v

w

k2), v

−

−

→

w

,

(cid:27)

0 if f (v)

where each element in ∂f (w) is called a subgradient of f at w. We say f is ρ-weakly convex
∂f (w)
for some ρ
2
w
and say f is ρ-strongly convex for some ρ
k
2
for any v and w and ξ
w
k
is a (µ−

ρ
2 for any v and w and ξ
w
2 k
∈
k
−
+ ρ
v
ξ, v
0 if f (v)
2 k
h
≥
∂f (w). It is known that, if f is ρ-weakly convex, f (w) + 1

ρ)-strongly convex function when µ−

i −
≥

ξ, v
h

f (w) +

−
2µ k

1 > ρ.

w

w

−

−

≥

≥

∈

v

i

1

Under Assumption 1, φl(Si(w)) is a composite of the closed convex function φl and
the smooth map Si(w). According to Lemma 4.2 in Drusvyatskiy and Paquette (2019), we
have the following lemma.

−

Lemma 1 Under Assumption 1, f m(w) and f n(w) in (6) are ρ-weakly convex with ρ :=
N+N

L.

−

To solve (5) numerically, we need to overcome the following challenges.

(i) F (w) is
In fact, F (w) is a DC function because, by

non-convex even if each sij(w) is convex.

2. In this paper, k · k represents Euclidean norm.

6

Large-scale Optimization of Partial AUC

Lemma 1, we can represent F (w) as the diﬀerence of the convex functions f n(w) + 1
2
k
and f m(w) + 1
1 > ρ. (ii) F (w) is non-smooth due to the non-smoothness
of φl so that ﬁnding an approximate critical point (deﬁned below) of F (w) is diﬃcult. (iii)
Computing the exact subgradient of f l(w) for l = m, n requires processing N+N
data
pairs, which is computationally expensive for a large data set.

2 with µ−
k

2µ k

2µ k

w

w

−

∈

∈

Rd is a critical point of (5) if 0

∈
Rd is an ǫ-critical point of (5) if there exists ξ

Because of challenges (i) and (ii), we have to consider a reasonable goal when solving
∂f m(w). Given ǫ > 0, we say
(5). We say w
w
ǫ.
A critical point can only be achieved asymptotically in general.3 Within ﬁnitely many
iterations, there also exists no algorithm that can ﬁnd an ǫ-critical point unless at least one
of f m and f n is smooth, e.g., Xu et al. (2019). Since f m and f n are both non-smooth, we
have to consider a weaker but achievable target, which is a nearly ǫ-critical point deﬁned
below.

∂f m(w) such that

−
∂f n(w)

∂f n(w)

ξ
k

k ≤

−

∈

Deﬁnition 2 Given ǫ > 0, we say w
w′, and w′′ ∈
ǫ.

Rd such that ξ

∈
∂f n(w′)

∈

Rd is a nearly ǫ-critical point of (5) if there exist ξ,
w′′k} ≤
,
k

∂f m(w′′) and max

w′k
,

w
k

w
k

{k

−

−

ξ

−

Deﬁnition 2 is reduced to the ǫ-stationary point deﬁned by Sun and Sun (2021); Moudaﬁ
(2021) when w equals w′ or w′′. However, obtaining their ǫ-stationary point requires exactly
solving the proximal mapping of f m or f n while ﬁnding a nearly ǫ-critical point requires only
solving the proximal mapping inexactly. When w is generated by a stochastic algorithm,
we also call w a nearly ǫ-critical point if it satisﬁes Deﬁnition 2 with each
replaced by
E

k · k

Motivated by Sun and Sun (2021) and Moudaﬁ (2021), we approximate non-smooth
F (w) by a smooth function using the Moreau envelopes. Given a proper, ρ-weakly convex
and closed function f on Rd, the Moreau envelope of f with the smoothing parameter
µ

1) is deﬁned as

(0, ρ−

.
k · k

∈

fµ(w) := min

v

f (v) +

(cid:26)

1
2µ k

v

w

2
k

−

(cid:27)

and the proximal mapping of f is deﬁned as

vµf (w) := arg min

v

f (v) +

(cid:26)

1
2µ k

v

w

2
k

−

.

(cid:27)

(8)

(9)

Note that the vµf (w) is unique because the minimization above is strongly convex. Stan-
dard results show that fµ(w) is smooth with
vµf (w)) and vµf (w) is
1-Lipschitz continuous. See Proposition 13.37 in Rockafellar and Wets (2009) and
(1
Proposition 1 in Sun and Sun (2021). Hence, using the Moreau envelope, we can construct
a smooth approximation of (5) as follows

fµ(w) = µ−

µρ)−

1(w

∇

−

−

min
w

F µ := f n

µ (w)

f m
µ (w)

.

−

(10)

(cid:9)
Function F µ has the following properties. The ﬁrst property is shown in Sun and Sun
(2021). We give the proof for the second in Appendix A.

(cid:8)

3. A stronger notion than criticality is (directional) stationarity, which can also be achieved asymptoti-

cally (Pang et al., 2017).

7

Yao, Lin, Yang

Lemma 3 Suppose Assumption 1 holds and µ > ρ−
following claims hold
F µ(w) = µ−

vµf n(w)) and

1(vµf m (w)

1.

−

∇

∇
Lµ = 2
−

µ

µ2ρ .

1 with ρ deﬁned in Lemma 1. The

F µ(w) is Lµ-Lipschitz continuous with

2. If ¯v and w are two random vectors such that E

ǫ2/4 and
}
ǫ2/4 for l equals either m or n, then ¯v is a nearly ǫ-critical points

2
F µ(w)
k

1, µ−
{

min

k∇

≤

2

E
¯v
k
−
of (5).

2
vµf l(w)
k

≤

Since F µ is smooth, we can directly apply a ﬁrst-order method for smooth non-convex
F µ(w), which requires computing
optimization to (10). To do so, we need to evaluate
vµf m (w) and vµf n(w), i.e., exactly solving (9) with f = f m and f = f n, respectively.
Computing the subgradients of f m and f n require processing N+N
data pairs which is
costly. Unfortunately, the standard approach of sampling over data pairs does not produce
unbiased stochastic subgradients of f m and f n due to the composite structure φl(Si(w)).
In the next section, we will discuss a solution to overcome this challenge and approximate
vµf m (w) and vµf n(w), which leads to an eﬃcient algorithm for (10).

∇

−

5. Algorithm for pAUC Optimization

Consider (10) with f l deﬁned in (6) for l = m and n. To avoid of processing N+N
−
points, one method is to introduce dual variables pi = (pij)N−
formulate f l as

data
j=1 for i = 1, . . . , N+ and

f l(w) =

max
l,i=1,...,N+

pi

∈P

N+
i=1

N−
j=1 pijsij(w)

,

(11)

P

N−

l =

RN−

p
{

nP
where
. Then (10) can be reformulated as a min-
[0, 1]
}
max problem and solved by a primal-dual stochastic gradient method (e.g. Raﬁque et al.
(2021)). However, the maximization in (11) involves N+N
decision variables and equality
) even after using stochastic gradients.
constraints, so the per-iteration cost is still O(N+N
To further reduce the per-iteration cost, we take the dual form of the maximization in

j=1 pj = l, pj ∈

P

P

∈

o

−

−

|

(11) (see Lemma 8 in Appendix A) and formulate f l as

f l(w) = min

gl(w, λ) := l1⊤λ +

λ 


N+

N−

i=1
X

j=1
X

[sij(w)

−

,

λi]+


(12)

where λ = (λ1, . . . , λN+). Hence, (9) with f = f l for l = m and n can be equivalently
formulated as





min
v,λ

gl(v, λ) +

1
2µ k

v

w

2
k

−

.

(13)

(cid:26)

(cid:27)
Note that gl(v, λ) is jointly convex in v and λ when µ−
L (see Lemma 7
in Appendix A). Thanks to formulation (13), we can construct stochastic subgradient of
gl and apply coordinate update to λ by sampling indexes i’s and j’s, which signiﬁcantly
are both large. We present this standard
reduce the computational cost when N+ and N
stochastic block coordinate descent (SBCD) method for solving (13) in Algorithm 1 and
present its convergence property as follows.

1 > ρ = N+N
−

−

8

Large-scale Optimization of Partial AUC

Algorithm 1 Stochastic Block Coordinate Descent for (13): (¯v, ¯λ) =SBCD(w, λ, T, µ, l)
1: Input: Initial solution (w, λ), the number of iterations T , µ > 0, an integer l > 0 and

sample sizes I and J.

2: Set (v(0), λ(0)) = (w, λ) and choose (ηt, θt)T
1 do
3: for t = 0 to T
−
1, . . . , N+}
Sample
4:
It ⊂ {
1, . . . , N
Sample
Jt ⊂ {
−}
Compute stochastic subgradient w.r.t. v:

= I.
= J.

with
with

|It|
|Jt|

5:
6:

1
t=0 .
−

G(t)

v =

N+N−
IJ

sij (v(t))1

∇

sij (v(t)) > λ(t)
i
(cid:16)

(cid:17)

i∈It X
j∈Jt
X

7:

Proximal stochastic subgradient update on v:

v(t+1) = arg min

(G(t)

v )⊤v + k

v

v

w

−
2µ

2

k

+ k

v

v(t)

−
2ηt

2

k

8:

Compute stochastic subgradient w.r.t. λi for i

G(t)

λi = l

N−
J

−

1

j∈Jt
X

(cid:16)

∈ It:
sij(v(t)) > λ(t)
i

for i

t
∈ I

(cid:17)

9:

Stochastic block subgradient update on λi for i

λ(t+1)
i

=

(

θtG(t)
λi

λ(t)
i −
λ(t)
i

∈ It:
i
i /

t,
t.

∈ I
∈ I

(14)

(15)

10: end for
11: Output: (¯v, ¯λ) = 1
T

1

T
t=0 (v(t), λ(t)).
−

P
Proposition 4 Suppose Assumption 1 holds and µ−

ηt = k

vµf l (w)
w
N+N−B√T

−

k

for any t in Algorithm 1. It holds that

1 > ρ = N+N

L, θt =

−

dist(λ(0),Λ∗)
√IT N−

and

1
2

1
µ −

ρ

¯v

E
k

−

(cid:18)

(cid:19)
where Λ∗ = arg min

2

N+N−
vµf l (w))
√IT
≤
k
gl(vµf l(w), λ).

λ

dist(λ(0), Λ∗) +

N+N−B

2√T k

vµf l (w)

w

k

−

+ k

vµf l (w)
2µT

−

w

2

k

,

Using Algorithm 1 to compute an approximation of vµf l (w) for l = m and n and thus, an
F µ(w), we can apply an approximate gradient descent (AGD) method
approximation of
to (10) and ﬁnd a nearly ǫ-critical point of (5) according to Lemma 3. We present the AGD
method in Algorithm 2 and its convergence property as follows.

∇

Theorem 5 Suppose Assumption 1 holds and µ−
vµf l (w(k))
k

w(k)

for any t, and

−

k

N+N−B√Tk

1 > ρ = N+N

L, θt =

−

dist(¯λ(k),Λ∗
k)
√ITkN−

, ηt =

Tk = max

n

144N 2

l (k+1)2

+N 2
−D2
I(µ−1−ρ)2

,

4N 2

+N 2

−µ2l2B2(k+1)2
(µ−1−ρ)2

, 6µl2B2(k+1)
2(µ−1−ρ)2

o

9

Yao, Lin, Yang

n ), the number of iterations K, µ > ρ−

1, γ > 0, m =

Algorithm 2 Approximate Gradient Descent for (10)
1: Input: Solutions (w(0), ¯λ(0)
and n = βN

m , ¯λ(0)

αN

.
−
1 do

−

) =SBMD(w(k), ¯λ(k)
m , Tk, µ, m)
) =SBMD(w(k), ¯λ(k)
n , Tk, µ, n)
¯v(k)
n )

1(¯v(k)
m

γµ−

−

−

−
2: for k = 0 to K
m , ¯λ(k+1)
(¯v(k)
3:
m
n , ¯λ(k+1)
(¯v(k)
n

4:
5: w(k+1) = w(k)
6: end for
7: Output: ¯v(¯k)

n with ¯k sampled from

0, . . . , K
{

.
1
}

−

when Algorithm 1 is called in the kth iteration of Algorithm 2, where Λ∗k = arg min
and

λ

gl(vµf l(w(k)), λ)

Dl := max

dist(¯λ(0)

l

, Λ∗

0),

+ µl2B2

2 + N+B + N+B

1−µρ

1
µ −

ρ

1
2

(cid:16)

(cid:17)

2γ
µ + γnB + γmB

.

(16)

(cid:16)

(cid:17) o

is an nearly ǫ-critical point of (5) with f l deﬁned in (6) with K no more than

n

Then ¯v(¯k)

n

K = max

16µ2
γ min{1,µ2}ǫ2

F (vµf n (w(0)))

F ∗

,

96

min{1,µ2}ǫ2 log

96
min{1,µ2}ǫ2

.

(17)

n

(cid:0)

−

(cid:1)

(cid:16)

(cid:17) o

According to Theorem 5. To ﬁnd a nearly ǫ-critical point of (5), we need K = ˜O(ǫ−
iterations in Algorithm 2 and
total across all calls.

2)
6) iterations of Algorithm 1 in

k=0 Tk = O(K 3) = ˜O(ǫ−

K

−

1

P

6. Algorithm for Sum of Range Optimization

The technique in the previous sections can be directly applied to minimize the SoRR loss,
which is formulated as (5) but with f l deﬁned in (7). Since (7) is a special case of (6)
= N , we can again formulate subproblem (9) with f = f l as (13)
with N+ = 1 and N
with λ = λ being a scalar. Since λ is a scalar, when solving (13), we no longer use block
coordinate update but only need to sample over indexes j = 1, . . . , N to construct stochastic
subgradients. We present the stochastic subgradient (SGD) method for (13) in Algorithm 3.
Next, we apply Algorithm 2 with SBCD in lines 3 and 4 replaced by SGD. The convergence
result in this case is directly from Theorem 5.

−

= N and sij(w) = sj(w) and
Corollary 6 Suppose Assumption 1 holds with N+ = 1, N
SBCD in Algorithm 2 are replaced by SGD. Suppose θt, ηt, and Tk are set the same as in
Theorem 5 when Algorithm 3 is called in the kth iteration of Algorithm 2. Then ¯v(¯k)
is an
nearly ǫ-critical point of (5) with f l deﬁned in (7) with K no more than (17).

−

n

7. Numerical Experiments

In this section, we demonstrate the eﬀectiveness of our AGD algorithm for pAUC maxi-
mization problem and SoRR loss minimization problem in comparisons with two baselines,
DCA (Hu et al., 2020) and SVMpAU C-tight (Narasimhan and Agarwal, 2013b). Since AGD

10

Large-scale Optimization of Partial AUC

Algorithm 3 Stochastic Subgradient Descent for SoRR: (¯v, ¯λ) =SGD(w, λ, T, µ, l)
1: Input: Initial solution (w, λ), the number of iterations T , µ > ρ−

1, an integer l > 0

and sample size J.

2: Set (v(0), λ(0)) = (w, λ) and choose (ηt, θt)T
1 do
3: for t = 0 to T
Sample
1, . . . , N
4:
Compute stochastic subgradient w.r.t. v:
5:

−
Jt ⊂ {

|Jt|

= J.

with

}

1
t=0 .
−

G(t)

v =

N
J

sj(v(t))1

∇

sj(v(t)) > λ(t)
i
(cid:16)

(cid:17)

j∈Jt
X

6:

Proximal stochastic subgradient update on v:

v(t+1) = arg min

(G(t)

v )⊤v + k

v

v

w

−
2µ

2

k

+ k

v

v(t)

−
2ηt

2

k

7:

Compute stochastic subgradient w.r.t. λ:

G(t)

λ = l

N
J

−

1

sj(v(t)) > λ(t)
(cid:16)

(cid:17)

j∈Jt
X

8:

Stochastic subgradient update on λ:

λ(t+1) = λ(t)

ηtG(t)
λ

−

9: end for
10: Output: (¯v, ¯λ) = 1
T

1

T
t=0 (v(t), λ(t)).
−

P

and DCA are stochastic, we perform ﬁve independent trials with diﬀerent random seeds
and report the standard deviations of their performances. We focus on a linear model
hw(x) = wT x and conduct all experiments in Python and Matlab on a computer with the
CPU 2GHz Quad-Core Intel Core i5 and the GPU NVIDIA GeForce RTX 2080 Ti.

Although DCA is originally only studied for SoRR loss minimization in Hu et al. (2020),
it can be also applied to pAUC maximization. Hence, we only describe DCA for pAUC
maximization which cover SoRR loss minimization as a special case. At the kth iteration
N+
of DCA, it computes a deterministic subgradient of f m(w) =
i=1 φm(Si(w(k))) at iterate
w(k), denoted by ξ(k). Then DCA updates w(k) by approximately solving the following
subproblem using a SBCD method similar to Algorithm 1 by sampling indexes i and j (see
Algorithm 4 in Appendix D for details).

P

(w(k+1), λ(k+1))

arg min
w,λ

≈

n1⊤λ +

N+

N−

[sij(w)

Xi=1

Xj=1

λi]+ −

−

w⊤ξ(k)

(18)

In AGD and DCA, each subproblem is initialized by the solution from the previous
subproblem. For all methods, we use the model from the last iterate as the ﬁnal output to
create the performance measures and ﬁgures in comparisons.

11

Yao, Lin, Yang

a9a

AGD-SGD
DCA.Constant.lr
DCA.Changing.lr

40

60

80

100

0.41

0.405

0.4

20

102

100

10-2

s
s
o
L
g
n
n
a
r
T

i

i

0

20

60
40
# of epoch
ijcnn1

80

100

10-4

0

20

40

s
s
o
L
g
n
n
a
r

i

i

100T

w8a

AGD-SGD
DCA.Constant.lr
DCA.Changing.lr

80

100

120

60
# of epoch
covtype

AGD-SGD
DCA.Constant.lr
DCA.Changing.lr

1.8
1.6
1.4
1.2

1

0.8

0.6

0.4

s
s
o
L
g
n
n
i
a
r
T

i

AGD-SGD
DCA.Constant.lr
DCA.Changing.lr

5

4

3

2

1

s
s
o
L
g
n
n
i
a
r
T

i

0

20

40
60
# of epoch

80

100

0

0.74

0.72

0.7

5

5

10

15

10

15
# of epoch

20

25

30

Figure 1: Results for SoRR Loss Minimization

7.1 SoRR Loss Minimization

−

−

−

×

(1

∈ {

y) log(1

y log(hw(x))

−
For AGD, we ﬁx the J = 100, m = 103 and n = 2

We use four benchmark datasets from the UCI (Dua and Graﬀ, 2017) data repository pre-
processed by Libsvm (Chang and Lin, 2011): a9a, w8a, ijcnn1 and covtype. The statis-
tics of these datasets are summarized in Table 4 in Appendix D.1. We use logistic loss
l(hw(x), y) =

hw(x)) where label y

.
0, 1
}
104. In the spirit of Corollary 6,
(k+1)N for
and Tk is set to C(k + 1)2 with C
/N and γ is tuned
}

0.1, 2, 5, 10, 1
}
{

within Algorithm 3 called in the kth main iteration, ηt and θt are both set to
any t with c tuned in the range of
selected from
2
from
{

. Parameter µ is chosen from
30, 50, 100
}
{
102, 4
102, 8
102, 5
×

/N .
}
According to the experiments in Hu et al. (2020), when solving (18), we ﬁrst use the
same step size and the same number of iterations for all k’s. We choose the step size from
. However, we ﬁnd that the
2000, 3000
0.01, 0.1, 0.5, 1
{
}
{
}
performance of DCA improves if we let the step size and the number of iterations vary with
k. Hence, we apply the same process as in AGD to select θt, ηt, and T in Algorithm 4 in
the kth main iteration of DCA. We report the performances of DCA under both settings
(named DCA.Constant.lr and DCA.Changing.lr, respectively). The changes of the SoRR
loss with the number of epochs are shown in Figure 1. From the curves, we notice that our
algorithm reduce SoRR loss faster than DCAs for all of these four datasets.

and the iteration number from

102, 103

103, 2

2
{

104

×

×

×

×

×

c

12

 
 
 
 
Large-scale Optimization of Partial AUC

Table 1: The pAUCs returned by SVMpAU C-tight on the CheXpert training data and
the runtime (in seconds) it takes, compared with the time AGD uses to exceed
SVMpAU C-tight’s pAUCs.

pAUC

CPU Time

GPU

Methods

SVM SVM AGD-SBCD AGD-SBCD

D1
D2
D3
D4
D5

0.6259
0.5860
0.3745
0.3895
0.7267

95.14
90.83
90.56
89.64
90.86

2.91
3.36
3.26
10.09
3.97

1.85
1.93
1.84
8.38
1.89

7.2 Partial AUC Maximization

For maximizing pAUC, we focus on large-scale imbalanced medical dataset CheXpert (Irvin et al.,
2019) with the logistic loss ℓ(z) = log(1 + exp(
z)), which has 224,316 images. We use a
pre-trained deep neural network to extract a ﬁxed dimensional feature vectors of 1024. The
deep neural network was trained by optimizing the cross-entropy loss following the same
setting as in Yuan et al. (2020). We construct ﬁve binary classiﬁcation tasks for predict-
ing ﬁve popular diseases, Cardiomegaly (D1), Edema (D2), Consolidation (D3), Atelectasis
(D4), and P. Eﬀusion (D5).

−

c

2
{

103, 5

For AGD, we ﬁx the I = J = 100, m = 104 and n = 105.

In the spirit of Theo-
c
(k+1)N +N − and
rem 5, within Algorithm 3 called in the kth main iteration, ηt is set to
θt is set to
(k+1)N − for any t with c tuned in the range of
and Tk is
set to 50(k + 1)2 according to Theorem 5. Parameter µ is chosen from
)
/(N+N
}
−
). For DCA, we only implement the set-
and γ is tuned from
ting of DCA.Changing.lr. In Algorithm 4 called in the kth main iteration, ηt and θt are
selected following the same process as in AGD and T is set to C(k + 1)2 with C cho-
sen from
. For the SVMpAU C-tight method, we use their MATLAB code
100, 200, 500
}
{
1, 100
in Narasimhan and Agarwal (2013b) and tune hyper-parameter C from
In all tables, SVM is used to represent SVMpAU C-tight. In Figure 2, we show how the train-
ing loss (the objective value of (2)) and normalized partial AUC on the training data change
with the number of epochs. We observe that for all of these ﬁve diseases, our algorithm
converges much faster than DCA and we get a better partial AUC than DCA.

0.1, 1, 5, 10, 20, 30
}
{
102, 103
{

/(N+N
}

10−
{

3, 10−

2, 10−

103

×

×

−

.
}

The comparison between our AGD and SVMpAU C-tight on training data are shown in
Tables 1 and 2. Table 1 shows that our algorithm needs only a few seconds to exceed the
pAUCs that SVMpAU C-tight takes more than one minute to return. Table 2 shows that our
algorithm eventually improves the pAUC by at least 12% compared with SVMpAU C-tight.
DCA is not included in the tables because it computes deterministic subgradients, which
leads to a runtime signiﬁcantly longer than the other two methods.

We also conduct a separated set of experiments to compare the testing performances of
the three methods. The CheXpert validation dataset, which has 234 samples, is used as

13

Yao, Lin, Yang

Table 2: The pAUCs returned by AGD on the CheXpert training data and the runtime (in

seconds) it takes.

pAUC

CPU

GPU

D1
D2
D3
D4
D5

0.7005
0.7214
0.4910
0.4616
0.8272

±
±
±
±
±

0.0003
0.0024
0.0006
0.0006
0.0001

118.32
415.66
181.70
187.36
238.10

82.13
247.29
104.55
158.14
142.91

the testing set. From the CheXpert training data, we randomly select 90% for training and
use the remaining 10% for validation. The range of FPRs in pAUC is [0.05, 0.5]. Diﬀerent
from the previous experiments, for each method, we select the model with the best pAUC
on the validation set across all iterations as the method’s ﬁnal output. We then tune the
parameters in the algorithms (e.g., c and C) based the pAUC on the validation set of the
ﬁnal output. The random splitting is repeated ﬁve times. The mean and standard deviation
of the pAUCs on the testing set are reported in Table 3, which shows that our algorithm
performs the best for all diseases except D4. The complete ROC curves on the testing set
are shown in Figure 3 in Appendix D.2.

Table 3: The pAUCs with FPRs between 0.05 and 0.5 on the testing sets from the CheXpert

data.

SVM

DCA

AGD-SBCD

D1
0.6538
D2
0.6038
0.6946
D3
D4 0.6521
0.7994
D5

0.0042
0.0009
0.0020
0.0006
0.0004

±
±
±
±
±

0.6636
0.8078
0.7427
0.6169
0.8371

±
±
±
±
±

0.0093 0.6721
0.0030 0.8257
0.0257 0.8016
0.0208
0.6340
0.0022 0.8500

0.0081
0.0025
0.0075
0.0165
0.0017

±
±
±
±
±

8. Conclusion

Most existing methods for optimizing pAUC are deterministic and only have an asymptotic
convergence property. We formulate pAUC optimization as a non-smooth DC program
and develop a stochastic subgradient method based on the Moreau envelope smoothing
technique. We show that our method ﬁnds a nearly ǫ-critical point in ˜O(ǫ−
6) iterations and
demonstrate its performance numerically.

14

Large-scale Optimization of Partial AUC

References

Hadi Abbaszadehpeivasti, Etienne de Klerk, and Moslem Zamani. On the rate of con-
vergence of the diﬀerence-of-convex algorithm (dca). arXiv preprint arXiv:2109.13566,
2021.

AD Alexandroﬀ. Surfaces represented by the diﬀerence of convex functions. In Doklady

Akademii Nauk SSSR (NS), volume 72, pages 613–616, 1950.

Le Thi Hoai An and Pham Dinh Tao. The dc (diﬀerence of convex functions) programming
and dca revisited with dc models of real world nonconvex optimization problems. Annals
of operations research, 133(1):23–46, 2005.

Le Thi Hoai An, Huynh Van Ngai, Pham Dinh Tao, and Luu Hoang Phuc Hau. Stochas-
tic diﬀerence-of-convex algorithms for solving nonconvex optimization problems. arXiv
preprint arXiv:1911.04334, 2019.

Nguyen Thai An and Nguyen Mau Nam. Convergence analysis of a proximal point algorithm

for minimizing diﬀerences of functions. Optimization, 66(1):129–147, 2017.

Francisco J Arag´on Artacho, Ronan MT Fleming, and Phan T Vuong. Accelerating the dc

algorithm for smooth functions. Mathematical Programming, 169(1):95–118, 2018.

Andrew P Bradley. The use of the area under the roc curve in the evaluation of machine

learning algorithms. Pattern recognition, 30(7):1145–1159, 1997.

Andrew P Bradley. Half-auc for the evaluation of sensitive or speciﬁc classiﬁers. Pattern

Recognition Letters, 38:93–98, 2014.

Chih-Chung Chang and Chih-Jen Lin. Libsvm: A library for support vector machines.

ACM transactions on intelligent systems and technology (TIST), 2(3):1–27, 2011.

Damek Davis and Dmitriy Drusvyatskiy. Stochastic subgradient method converges at the

rate o(k−

1/4) on weakly convex functions. arXiv preprint arXiv:1802.02988, 2018.

Damek Davis and Benjamin Grimmer. Proximally guided stochastic subgradient method
for nonsmooth, nonconvex problems. SIAM Journal on Optimization, 29(3):1908–1930,
2019.

Qi Deng and Chenghao Lan. Eﬃciency of coordinate descent methods for structured non-
convex optimization. In Joint European Conference on Machine Learning and Knowledge
Discovery in Databases, pages 74–89. Springer, 2020.

Lori E Dodd and Margaret S Pepe. Partial auc estimation and regression. Biometrics, 59

(3):614–623, 2003.

Dmitriy Drusvyatskiy and Courtney Paquette. Eﬃciency of minimizing compositions of
convex functions and smooth maps. Mathematical Programming, 178(1):503–558, 2019.

Dheeru Dua and Casey Graﬀ.

UCI machine learning repository, 2017.

URL

http://archive.ics.uci.edu/ml.

15

Yao, Lin, Yang

Rachid Ellaia. Contribution `a l’analyse et l’optimisation de diﬀ´erence de fonctions convexes.

PhD thesis, Universit´e Paul Sabatier, 1984.

Yanbo Fan, Siwei Lyu, Yiming Ying, and Bao-Gang Hu. Learning with average top-k loss.

arXiv preprint arXiv:1705.08826, 2017.

D. Gabay. Minimizing the diﬀerence of two convex functions. I. Algorithms based on exact

regularization. 1982.

James A Hanley and Barbara J McNeil. The meaning and use of the area under a receiver

operating characteristic (roc) curve. Radiology, 143(1):29–36, 1982.

Philip Hartman. On functions representable as a diﬀerence of convex functions. Paciﬁc

Journal of Mathematics, 9(3):707–713, 1959.

Lulu He, Jimin Ye, et al. Accelerated proximal stochastic variance reduction for dc opti-

mization. Neural Computing and Applications, 33(20):13163–13181, 2021.

J-B Hiriart-Urruty. Generalized diﬀerentiability/duality and optimization for problems deal-
ing with diﬀerences of convex functions. In Convexity and duality in optimization, pages
37–70. Springer, 1985.

J-B Hiriart-Urruty. How to regularize a diﬀerence of convex functions. Journal of mathe-

matical analysis and applications, 162(1):196–209, 1991.

Shu Hu, Yiming Ying, Siwei Lyu, et al. Learning by minimizing the sum of ranked range.

Advances in Neural Information Processing Systems, 33, 2020.

Jeremy Irvin, Pranav Rajpurkar, Michael Ko, Yifan Yu, Silviana Ciurea-Ilcus, Chris Chute,
Henrik Marklund, Behzad Haghgoo, Robyn Ball, Katie Shpanskaya, Jayne Seekins,
David A. Mong, Safwan S. Halabi, Jesse K. Sandberg, Ricky Jones, David B. Larson,
Curtis P. Langlotz, Bhavik N. Patel, Matthew P. Lungren, and Andrew Y. Ng. Chexpert:
A large chest radiograph dataset with uncertainty labels and expert comparison, 2019.

Yulei Jiang, Charles E Metz, and Robert M Nishikawa. A receiver operating characteristic
partial area index for highly sensitive diagnostic tests. Radiology, 201(3):745–750, 1996.

Koulik Khamaru and Martin Wainwright. Convergence guarantees for a class of non-convex
and non-smooth optimization problems. In International Conference on Machine Learn-
ing, pages 2601–2610. PMLR, 2018.

Osamu Komori and Shinto Eguchi. A boosting method for maximizing the partial area

under the roc curve. BMC bioinformatics, 11(1):1–17, 2010.

Hoai An Le Thi and Tao Pham Dinh. Dc programming and dca: thirty years of develop-

ments. Mathematical Programming, 169(1):5–68, 2018.

Hoai An Le Thi, Hoai Minh Le, Duy Nhat Phan, and Bach Tran. Stochastic dca for the
large-sum of non-convex functions problem and its application to group variable selection
in classiﬁcation.
In International Conference on Machine Learning, pages 3394–3403.
PMLR, 2017.

16

Large-scale Optimization of Partial AUC

Thomas Lipp and Stephen Boyd. Variations and extension of the convex–concave procedure.

Optimization and Engineering, 17(2):263–287, 2016.

Mingrui Liu, Hassan Raﬁque, Qihang Lin, and Tianbao Yang. First-order convergence the-
ory for weakly-convex-weakly-concave min-max problems. Journal of Machine Learning
Research, 22(169):1–34, 2021.

Hua Ma, Andriy I Bandos, Howard E Rockette, and David Gur. On use of partial area
under the roc curve for evaluation of diagnostic performance. Statistics in medicine, 32
(20):3449–3458, 2013.

Runchao Ma, Qihang Lin, and Tianbao Yang. Quadratically regularized subgradient meth-
In International

ods for weakly convex optimization with weakly convex constraints.
Conference on Machine Learning, pages 6554–6564. PMLR, 2020.

Julien Mairal. Stochastic majorization-minimization algorithms for large-scale optimization.

arXiv preprint arXiv:1306.4650, 2013.

Donna Katzman McClish. Analyzing a portion of the roc curve. Medical decision making,

9(3):190–195, 1989.

Abdellatif Moudaﬁ. On the diﬀerence of two maximal monotone operators: Regulariza-
tion and algorithmic approaches. Applied mathematics and computation, 202(2):446–452,
2008.

Abdellatif Moudaﬁ. A complete smooth regularization of dc optimization problems. 2021.

Abdellatif Moudaﬁ and Paul-Emile Maing´e. On the convergence of an approximate proximal
method for dc functions. Journal of computational Mathematics, pages 475–480, 2006.

Harikrishna Narasimhan and Shivani Agarwal. A structural svm based approach for op-
timizing partial auc. In International Conference on Machine Learning, pages 516–524.
PMLR, 2013a.

Harikrishna Narasimhan and Shivani Agarwal. SVMtight

pAUC: a new support vector method
for optimizing partial auc based on a tight convex upper bound. In Proceedings of the
19th ACM SIGKDD international conference on Knowledge discovery and data mining,
pages 167–175, 2013b.

Harikrishna Narasimhan and Shivani Agarwal. Support vector algorithms for optimizing

the partial area under the roc curve. Neural computation, 29(7):1919–1963, 2017.

Atsushi Nitanda and Taiji Suzuki. Stochastic diﬀerence of convex algorithm and its applica-
tion to training deep boltzmann machines. In Artiﬁcial intelligence and statistics, pages
470–478. PMLR, 2017.

Jong-Shi Pang, Meisam Razaviyayn, and Alberth Alvarado. Computing b-stationary points
of nonsmooth dc programs. Mathematics of Operations Research, 42(1):95–118, 2017.

17

Yao, Lin, Yang

Hassan Raﬁque, Mingrui Liu, Qihang Lin, and Tianbao Yang. Weakly-convex–concave
min–max optimization: provable algorithms and applications in machine learning. Opti-
mization Methods and Software, pages 1–35, 2021.

Maria Teresa Ricamato and Francesco Tortorella. Partial auc maximization in a linear

combination of dichotomizers. Pattern Recognition, 44(10-11):2669–2677, 2011.

R Tyrrell Rockafellar and Roger J-B Wets. Variational analysis, volume 317. Springer

Science & Business Media, 2009.

Shai Shalev-Shwartz and Yonatan Wexler. Minimizing the maximal loss: How and why. In

International Conference on Machine Learning, pages 793–801. PMLR, 2016.

Jo˜ao Carlos O Souza, Paulo Roberto Oliveira, and Antoine Soubeyran. Global convergence
of a proximal linearized algorithm for diﬀerence of convex functions. Optimization Letters,
10(7):1529–1539, 2016.

Bharath K Sriperumbudur and Gert RG Lanckriet. On the convergence of the concave-

convex procedure. In Nips, volume 9, pages 1759–1767. Citeseer, 2009.

Kaizhao Sun and Xu Andy Sun. Algorithms for diﬀerence-of-convex (dc) programs based
on diﬀerence-of-moreau-envelopes smoothing. arXiv preprint arXiv:2104.01470, 2021.

Wen-yu Sun, Raimundo JB Sampaio, and MAB Candido. Proximal point algorithm for
minimization of dc function. Journal of computational Mathematics, pages 451–462, 2003.

Pham Dinh Tao and Le Thi Hoai An. Convex analysis approach to dc programming: theory,

algorithms and applications. Acta mathematica vietnamica, 22(1):289–355, 1997.

Pham Dinh Tao and Le Thi Hoai An. A dc optimization algorithm for solving the trust-

region subproblem. SIAM Journal on Optimization, 8(2):476–505, 1998.

Hoai An Le Thi, Hoang Phuc Hau Luu, and Tao Pham Dinh. Online stochastic dca with
applications to principal component analysis. arXiv preprint arXiv:2108.02300, 2021.

Mary Lou Thompson and Walter Zucchini. On the statistical analysis of roc curves. Statis-

tics in medicine, 8(10):1277–1290, 1989.

Hoang Tuy. Dc optimization: theory, methods and algorithms.

In Handbook of global

optimization, pages 149–216. Springer, 1995.

Naonori Ueda and Akinori Fujino. Partial auc maximization via nonlinear scoring functions.

arXiv preprint arXiv:1806.04838, 2018.

Vladimir Vapnik. Principles of risk minimization for learning theory. In Advances in neural

information processing systems, pages 831–838, 1992.

Zhanfeng Wang and Yuan-Chin Ivan Chang. Marker selection via maximizing the partial

area under the roc curve of linear risk scores. Biostatistics, 12(2):369–385, 2011.

18

Large-scale Optimization of Partial AUC

Bo Wen, Xiaojun Chen, and Ting Kei Pong. A proximal diﬀerence-of-convex algorithm
with extrapolation. Computational optimization and applications, 69(2):297–324, 2018.

Yi Xu, Qi Qi, Qihang Lin, Rong Jin, and Tianbao Yang. Stochastic optimization for dc
functions and non-smooth non-convex regularizers with non-asymptotic convergence. In
International Conference on Machine Learning, pages 6942–6951. PMLR, 2019.

Hanfang Yang, Kun Lu, Xiang Lyu, and Feifang Hu. Two-way partial auc and its properties.

Statistical methods in medical research, 28(1):184–195, 2019.

Zhiyong Yang, Qianqian Xu, Shilong Bao, Yuan He, Xiaochun Cao, and Qingming Huang.
When all we need is a piece of the pie: A generic framework for optimizing two-way partial
auc. In International Conference on Machine Learning, pages 11820–11829. PMLR, 2021.

Zhuoning Yuan, Yan Yan, Milan Sonka, and Tianbao Yang. Robust deep auc maximization:
A new surrogate loss and empirical studies on medical image classiﬁcation. arXiv preprint
arXiv:2012.03173, 2020.

Alan L Yuille and Anand Rangarajan. The concave-convex procedure. Neural computation,

15(4):915–936, 2003.

Dixian Zhu, Gang Li, Bokun Wang, Xiaodong Wu, and Tianbao Yang. When auc meets
dro: Optimizing partial auc for deep learning with non-convex convergence guarantee.
arXiv preprint, 2022.

19

Yao, Lin, Yang

Appendix

A. Proofs of Lemmas

Proof.[of Lemma 3] We will only prove the second conclusion in Lemma 3 since the ﬁrst
conclusion has been shown in Proposition 1 in Sun and Sun (2021).

2

2

Suppose E
2E

k∇
vµf m (w)
k

2
F µ(w)
ǫ2/4. By the ﬁrst conclusion in Lemma 3, we must
1, µ−
min
≤
k
}
{
ǫ2/4. By the optimality conditions satisﬁed
2
vµf n (w)
1, µ−
min
have µ−
}
{
≤
k
−
∂f n(vµf n (w)) such
∂f m(vµf m (w)) and ξn ∈
by vµf m (w) and vµf n (w), there exist ξm ∈
that
ξm + µ−

1(vµf n (w)
−
∂f n(vµf n (w))
which implies ξ = ξn−
∈
ǫ2/4. We have E
E
E
2
ξ
ξ
k
k
k
and E
vµf m (w)
vµf m (w)
Deﬁnition 2 with w′ = vµf n (w) and w′′ = vµf m(w). Suppose E
The conclusion can be also proved similarly.

ξm = µ−
ǫ/2. Suppose E
¯v
k

−
1(vµf m (w)
¯v
−
k
vµf n(w)
k

w) = 0 = ξn + µ−
vµf n(w))

−
2
vµf n(w)
k
+ E

≤
vµf n(w)
k

w),
∂f m(vµf m (w)) and
−
¯v
ǫ/2
k
ǫ. Hence, ¯v satisﬁes
k ≤
ǫ2/4.
2
vµf m(w)
¯v
k
k
(cid:3)

k ≤
¯v
p
−
k

1(vµf m (w)

vµf n(w)

k ≤

k ≤

−

≤

−

−

≤

−

E

We ﬁrst present the following lemma which is similar to Lemma 4.2 in Drusvyatskiy and Paquette

(2019).

Lemma 7 Suppose Assumption 1 holds. For any v, λ, v′, λ′, and (ξv, ξλ)
we have

∂gl(v′, λ′),

∈

gl(v, λ)

gl(v′, λ′) + ξ⊤v (v

≥

−
L. Moreover, gl(v, λ) + 1

v′) + ξ⊤λ (λ

λ′)

−

−

ρ
2 k

v

v′

2,
k

−

v

w

2 is jointly convex in λ and v for any
k

−

2µ k

where ρ = N+N
w and any µ−

−
1 > ρ.

Proof. By Assumption 1, we have

sij(v)

−

sij(v′)

≥ ∇

sij(v′)⊤(v

v′)

−

−

L
2 k

v

v′

2.
k

−

(19)

Let ξij ∈

∂[sij(v′)

−

λ′i]+. We have

gl(v, λ)

N+

N−

= l1⊤λ +

[sij(v)

Xi=1

Xj=1

λi]+

−

N+

N−

λ′i]+ +

−

N+

N−

Xi=1

Xj=1

l1⊤λ′ + l1⊤(λ

λ′) +

−

[sij(v′)

Xi=1

Xj=1
N+

N−

gl(v′, λ′) + ξ⊤λ (λ

gl(v′, λ′) + ξ⊤v (v

−

−

λ′) +

Xi=1

Xj=1

sij(v′)⊤(v

ξij∇

v′)

−

−

v′) + ξ⊤λ (λ

λ′)

−

−

L

N+N
2

−

v
k

−

v′

N+

N−

Xj=1

Xi=1
2,
k

≥

≥

≥

ξij(sij(v)

sij(v′)

−

−

λi + λ′i)

ξij

L
2 k

v

v′

2
k

−

where the ﬁrst inequality is by the convexity of [
]+, the second inequality is from (19) and
·
the last inequality is by the deﬁnitions of (ξv, ξλ) and the fact that ξij ∈

[0, 1].

20

Large-scale Optimization of Partial AUC

Combining the inequality from the ﬁrst conclusion with the equality 1
1
2 and using the fact that µ−
v
2µ k
k
−
obtain

v′)⊤(v′ −

w) + 1

v′ −

2 + 1

v′k

µ (v

2µ k

w

−

2µ k

v

2 =
w
k
1 > ρ, we can

−

gl(v, λ)+

1
2µ k

v

w

2
k

−

≥

gl(v′, λ′)+

1
2µ k

v′

w

2 + (µ−
k

−

1(v′

−

w)+ ξv)⊤(v

v′)+ ξ⊤λ (λ

−

which proves the second conclusion.

λ′),

−

(cid:3)

Lemma 8 The dual problem of the maximization problem in (11) is the minimization prob-
lem in (12).

Proof. For i = 1, . . . , N+, we introduce a Lagrangian multiplier λi for the constraint
z]+. Then, for each i, we

which equals

N−
j=1 pj = l in (11). Let [z]
−

= min

z, 0
}
{

[
−

−

have
P

N−

max
pi
∈P

l 


Xj=1

pijsij(w)








=

=

=

−

pij

min
[0,1]
∈

j
∀

max

λi 


−

max
λi

−

pij

min
[0,1]
∈

−


j 
∀

N−


N−

Xj=1

N−

pijsij(w) + λi(

Xj=1

pj −

lλi +

N−

Xj=1

pij[λi −

sij(w)]




l)






−

max

−

λi 


lλi +

Xj=1

[λi −

sij(w)]

−




= min

λi 


N−


lλi +

[sij(w)

Xj=1



−

.

λi]+


The conclusion is thus proved by summing up the equality above for i = 1, . . . , N+.





(cid:3)

B. Proof of Proposition 4

Let v∗ = vµf l(w) be the unique optimal solution of (8) and let si[j](v∗) be the jth largest
coordinate of Si(v∗). It is easy to show that the set of optimal solutions of (13) is
Λ∗
where

v∗} ×
{

Λ∗ = arg min

λ

gl(v∗, λ) +

1
2µ k

v∗

w

2 =
k

−

N+

Λ∗i :=

si[l](v∗), si[l+1](v∗)

.

(20)

(cid:0)
RN+, we denote its projection onto Λ∗ as ProjΛ∗(λ). By the structure of
Given any λ
Λ∗, the ith coordinate of ProjΛ∗(λ) is just the projection of λi onto Λ∗i , which we denote by
ProjΛ∗

(λi). Moreover, we denote the distance from λ to Λ∗ as dist(λ, Λ∗) and it satisﬁes

(cid:3)(cid:1)

∈

(cid:2)

i

Yi=1

dist2(λ, Λ∗) =

dist2(λi, Λ∗i ) =

N+

Xi=1

N+

Xi=1

(λi −

ProjΛ∗

i

(λi))2

(21)

With these deﬁnitions, we can present the following lemma.

21

Yao, Lin, Yang

Lemma 9 Suppose Assumption 1 holds and µ−
we have

1 > ρ. For any w, v, λ and v∗ = vµf l(w),

N+B

v
k

−

v∗

k

+ gl(v, λ)

−

gl(v∗, ProjΛ∗(λ))

N+

≥

Xi=1

dist(λi, Λ∗i )

≥

dist(λ, Λ∗).

Proof. It is easy to observe that gl(v, λ) :=

N+
i=1 gl

i(v, λi), where

P

N−

gl
i(v, λi) := lλi +

[sij(v)

Xj=1

λi]+,

−

and Λ∗i = arg minλi gl
slope of at least one at either end of the interval Λ∗i , we must have

i(v∗, λi). Since gl

i(v∗, λi) is a piecewise linear in λi with an outward

gl
i(v∗, λi)

−

gl
i(v∗, ProjΛ∗

i

(λi))

λi −

≥ |

ProjΛ∗

i

,
(λi)
|

N+

N+

which implies

gl(v∗, λ)

−

gl(v∗, ProjΛ∗(λ))

≥

λi −
|

ProjΛ∗

i

(λi)
|

=

dist(λi, Λ∗i )

dist(λ, Λ∗).

≥

Moreover, by Assumption 1, gl
Lipschitz continuous in v. We then have N+B
v∗k
the conclusion together with the previous inequality.

Xi=1
i(v, λi) is B-Lipschitz continuous in v so gl(v, λ) is N+B-
gl(v∗, λ), which implies
(cid:3)

+ gl(v, λ)

Xi=1

v
k

−

≥

We present the proof of Proposition 4 below.

Proof.[of Proposition 4] Let us denote
be the expectation conditioning on

{I0, . . . ,
I[t] =
1].
1] and
J[t
−
v and G(t)
By Assumption 1 and the deﬁnitions of G(t)
λi

I[t
−

It}

and

J[t] =

{J0, . . . ,

. Let Et

Jt}

in Algorithm 1, we have

G(t)
v
k

k ≤

N+N

−

B and

G(t)
|

λi | ≤

N

−

for t = 0, . . . , T

−

1 and i = 1, . . . , N+.

(22)

By the optimality condition satisﬁed by v(t+1) and

1

µ + 1
ηt

-strong convexity of the

objective function in (14), we have

(cid:16)

(cid:17)

v(t+1)

w

2 +
k

−

1
2ηt k

v(t+1)

v(t)

−

2 +
k

v∗
k

−

v(t+1)

2
k

1
2µ k
(G(t)

v )⊤

≤
= (G(t)

v )⊤

≤

(G(t)

v )⊤

v∗

(cid:16)

v(t+1)

+

1
2µ k

v∗

v

2 +
k

−

v∗

(cid:16)

v∗

(cid:16)

−

−

−

v(t)

v(t)

(cid:17)

(cid:17)

(cid:17)
+ (G(t)

v )⊤

(cid:16)
ηt(G(t)
v )2
2

+

v(t)

v(t+1)

+

−

(cid:17)

−

+

1
2ηt k

v(t)

22

1
µ

(cid:18)
v∗

1
2
1
2ηt k

+

1
ηt (cid:19)
v(t)

2
k

−
1
2µ k

v∗

w

2 +
k

−

1
2ηt k

v∗

v(t)

2
k

−

v(t+1)

2 +
k

1
2µ k

v∗

w

2 +
k

−

1
2ηt k

v∗

v(t)

2,
k

−

Large-scale Optimization of Partial AUC

where the last inequality is by Young’s inequality. Since v∗ −
ing on
J[t
I[t
−
−
inequality above yield

v(t) is deterministic condition-
1], applying (22) and taking expectation Et on the both sides of the

1] and

v(t+1)

w

2 +
k

−

1
2

1
2µ

Etk
EtG(t)
v

≤

⊤

v∗

v(t)

−

(cid:16)

(cid:17)

(cid:16)

(cid:17)

v∗

v(t+1)

2
k

+

1
µ
(cid:18)
ηtN 2

1
ηt (cid:19)
+N 2
−
2

+

Etk
B2

+

−
1
2µ k

By the updating equation (15) for λ(t+1), we have

v∗

w

2 +
k

−

1
2ηt k

v∗

v(t)

2.
k

−

(23)

=

≤

=

=

dist2(λ(t+1), Λ∗)
(λ(t+1)
i

ProjΛ∗

i

−

c
t

Xi
∈I

c
t

Xi
∈I

c
t

Xi
∈I

c
t

Xi
∈I
2θt

−

(λ(t)

i −

(λ(t)

i −

(λ(t)

i −

ProjΛ∗

i

(λ(t)

i ))2 +

ProjΛ∗

i

(λ(t)

i ))2 +

ProjΛ∗

i

(λ(t)

i ))2 +

Xi
∈I

t

Xi
∈I

t

t

Xi
∈I
ProjΛ∗

i

(G(t)
λi

)⊤(λ(t)

i −

Xi
∈I

t

(λ(t+1)
i

))2 +

(λ(t+1)
i

ProjΛ∗

i

(λ(t+1)
i

))2

−

t

Xi
∈I
(λ(t+1)
i

ProjΛ∗

i

(λ(t)

i ))2

−

(λ(t)

i −

θtG(t)

λi −

ProjΛ∗

i

(λ(t)

i ))2

(λ(t)

i −

ProjΛ∗

i

(λ(t)

i ))2

t

Xi
∈I
1] and

(λ(t)

i )) + θ2
t

(G(t)
λi

)2.

(24)

Since λ(t)
J[t
−
taking expectation Et on the both sides of the inequality above yield

i ) is deterministic conditioning on

ProjΛ∗

(λ(t)

i −

I[t
−

i

1], applying (22) and

Etdist2(λ(t+1), Λ∗)

dist2(λ(t), Λ∗)

≤

2θtI
N+

−

N+

Xi=1

EtG(t)
λi

(λ(t)

i −

ProjΛ∗

i

(λ(t)

i )) + θ2

t IN 2
(25)
−

Multiplying both sides of (25) by N+
2Iθt

and adding it with (23), we have

N+
2Iθt
N+
2Iθt

Etdist2(λ(t+1), Λ∗) +

1
2µ

Etk

v(t+1)

w

2 +
k

1
2

1
µ

+

−
1
2ηt k

(cid:18)
v(t)

2 +
k

v∗

−

1
ηt (cid:19)
ηtN 2

Etk
+N 2
−
2

v∗

−
B2

v(t+1)

2
k
θtN+N 2
−
2

+

dist2(λ(t), Λ∗) +

1
2µ k

v∗

−
N+

w

2 +
k

+

EtG(t)
v

⊤

v∗

v(t)

+

−

EtG(t)
λi

(ProjΛ∗

i

(λ(t)
i )

λ(t)
i )

−

i

(cid:16)
dist2(λ(t), Λ∗) +

h
N+
2Iθt
+gl(v∗, ProjΛ∗(λ(t)))

Xi=1
v∗

(cid:17)
1
2µ k

w

2 +
k

−
gl(v(t), λ(t)),

−

1
2

ρ +

(cid:18)

1
ηt (cid:19)

v∗
k

−

v(t)

2 +
k

B2

ηtN 2

+N 2
−
2

+

θtN+N 2
−
2

(26)

≤

≤

where the second inequality is because (EtG(t)
allows us to apply Lemma 7 with (v, λ) = (v∗, ProjΛ∗(λ(t))) and (v′, λ′) = (v(t), λ(t)).

, . . . , EtG(t)
λN+

v , EtG(t)
λ1

∂gl(v(t), λ(t)), which

∈

)

23

Yao, Lin, Yang

Notice that θt and ηt do not change with t that ρ < µ−

1. Summing up (26) for t =

1, taking full expectation, and organizing terms give us

1

−

E

gl(v(t), λ(t)) +

1
2µ k

v(t)

w

2
k

−

−

gl(v∗, ProjΛ∗(λ(t)))

1
2µ k

v∗

−

w

2
k

−

(cid:19)

0, . . . , T

−
T

t=0
X
+

(cid:18)
N+
2IθT

−

N+
2Iθ0
T

1

−

+

t=0 (cid:18)
X

Edist2(λ(T ), Λ∗) +

1

1
2µ

E

v(T )
k

dist2(λ(0), Λ∗) +

≤

B2

ηtN 2

+N 2
−
2

+

v(0)

1
2µ k
−
θtN+N 2
−
2

w

.

(cid:19)

−
1
2

2 +
k

w

2 +
k
1
µ

+

(cid:18)

1
µ

(cid:18)

1
2
1
η0 (cid:19)

+

1
ηT

1 (cid:19)
−
v(0)

2
k

v∗
k

−

E

v∗
k

−

v(T )

2
k

Because f l(¯v) + 1
2 is (µ−
k
and that f l(v∗) = gl(v∗, λ∗) for any optimal λ∗, we have that

2µ k

w

−

−

¯v

1

ρ)-strongly convex and the facts that f l(¯v)

(27)

gl(¯v, ¯λ)

≤

w

2
k

−

f l(v∗)

−

−

1
2µ k

v∗

gl(¯v, ¯λ) +

¯v

w

2
k

−

−

gl(v∗, λ∗)

−

1
2

E

E

1
T

≤

≤

≤

(cid:18)

(cid:18)
T

1
µ −

ρ

(cid:18)

(cid:19)
f l(¯v) +

v∗

2
k

E

¯v

−

¯v
k
1
2µ k
1
2µ k

1

−

E

gl(v(t), λ(t)) +

t=0
X

(cid:18)

w

2
k

(cid:19)

−
1
2µ k

v∗

w

2
k

−

(cid:19)

1
2µ k

v(t)

w

2
k

−

−

gl(v∗, ProjΛ∗(λ(t)))

1
2µ k

v∗

−

w

2
k

−

,(28)
(cid:19)

where the last inequality is because gl(v, λ) + 1
2 is jointly convex in v and λ.
k
Recall that v(0) = w. Applying (27) to the left-hand side of the inequality above, we obtain

2µ k

w

−

v

1
2

1
µ −

(cid:18)
N+
2Iθ0T
N+N
−
√IT

E

¯v
k

v∗

2
k

−

ρ

(cid:19)

dist2(λ(0), Λ∗) +

dist(λ(0), Λ∗) +

≤

≤

v(0)

w

2 +
k

−

1
2µT k
N+N

B

−
2√T k

v∗

w

k

−

+

1
2T

1
µ

+

(cid:18)
1
2µT k

v∗

1
η0 (cid:19)
w

−

v∗
k

−

v(0)

2 +
k

B2

η0N 2

+N 2
−
2

+

θ0N+N 2
−
2

2.
k

The conclusion is then proved given that vµf l(w) = v∗.

(cid:3)

C. Proof of Theorem 4

We ﬁrst present an technical lemma.

Lemma 10 Given two intervals [a, b] and [a′, b′] with max

dist(z, [a, b])

≤

dist(z, [a′, b′]) + δ,

24

b
|

b′|} ≤

−

δ. We have

a

{|

−

z

∀

∈

,
a′|
R.

Large-scale Optimization of Partial AUC

Proof. We will prove the result in three cases, z < a′, z > b′ and a′ ≤
z
b′. Suppose
z
a′|
z < a′ so that dist(z, [a′, b′]) =
+
a′| ≤
| ≤ |
b′ so
dist(z, [a′, b′]) + δ. The result when z > b′ can be proved similarly. Suppose a′ ≤
a′
a′. We deﬁne z′ = z
that dist(z, [a′, b′]) = 0. Note that z = z
a
−
b′
a′
b′
a′
−
z
[a, b]. Then dist(z, [a, b])
a′
b′

≤
a
a′|
−
|
z
≤
b + b′
z
−
a′
b′
∈
−
dist(z, [a′, b′]) + δ. (cid:3)

. We have dist(z, [a, b])

z
≤ |

−
a′
a′

z
|

−
−

z
a′

−

−

a

a

·

·

z
≤ |

z′| ≤

−

b′ + b′
b′
·
a′| ≤
b′|
b
−
· |
· |
−
B for any v so that
sij(v)

z
−
a′
·
+ b′
−
−
b′
−

−

−

Under Assumption 1, we have

∂f l(v) and any v. By the deﬁnition of vµf l(w), we have µ−
which implies

1(w

−

k∇

k ≤

ξ
k
k ≤
vµf l(w))

lB for any ξ
∈
∂f l(vµf l(w)),

∈

(29)

w
k

−

vµf l(w)

k ≤

µlB for any w.

We then provide the following proposition as an additional conclusion from the proof of

Proposition 4.

Proposition 11 Suppose Assumption 1 holds. Algorithm 1 guarantees that

Edist(¯λ, Λ∗)

Edist(¯λi, Λ∗i )

N+

≤

Xi=1

N+N
−
√IT

≤

dist(λ(0), Λ∗) +

N+N

B

−
2√T k

v∗

+

w

k

−

1
2µT k

v∗

w

2 +
k

−

1
2µ k

v∗

w

2 + N+BE
k

¯v
k

v∗

.
k

−

−

Proof. By (27), the last inequality in (28), and Lemma 9, we have

N+

dist(¯λi, Λ∗i )

E

E





Xi=1
gl(¯v, ¯λ)

N+B

¯v
k

−

v∗

k −

−

1
2µ k

v∗

w

2
k



−

≤

≤

(cid:18)
N+N
−
√IT

gl(v∗, λ∗) +

−

w

2
k

−

−

1
2µ k

v∗

−


w

2
k

dist(λ(0), Λ∗) +

N+N

−
2√T k

v∗

w

k

−

+

1
2µT k

v∗

−

¯v

1
2µ k
B

(cid:19)
w

2,
k

which implies the conclusion by reorganizing terms.

(cid:3)

Next we are ready to present the proof of of Theorem 4.

Proof.[of Theorem 4] Let ǫk = 1
Algorithm 1 is applied to the subproblem

√k+1

for k = 1, . . . . In the kth iteration of Algorithm 2,

min
v,λ

gl(v, λ) +

1
2µ k

v

w(k)

2
k

−

.

(30)

(cid:26)
with initial solution (w(k), ¯λ(k)
for (30). We will prove by induction the following two inequalities for k = 0, 1, . . . .

) and runs for Tk iterations. Let Λ∗k be the set of optimal λ

(cid:27)

l

E

¯v(k)
l −
k

2
vµf l (w(k))
k
, Λ∗k)

Edist(¯λ(k)

l

ǫ2
k/4

Dl,

≤

≤

(31)

(32)

where Dl is deﬁned in (16) for l = m and n.

25

Yao, Lin, Yang

Applying Proposition 4 to (30) and using (29), we have, for k = 0, 1, . . . ,

1
µ −

1
2
(cid:18)
N+N
−
√ITk
N+N
−
√ITk

≤

≤

E

¯v(k)
l −
k

ρ

(cid:19)

2
vµf l (w(k))
k
B

N+N

Edist(¯λ(k)

l

, Λ∗k) +

Edist(¯λ(k)

l

, Λ∗k) +

−
2√Tk

N+N

−
2√Tk

E

vµf l(w(k))
k
µlB2

µ2l2B2
2µTk

+

w(k)

+

k

1
2µTk

E

vµf l(w(k))
k

−

w(k)

2
k

(33)

−

.

Moreover, applying Proposition 11 to (30) and using (29), we have

N+

Edist(¯λ(k+1)

l,i

, Λ∗i )

Xi=1
N+N
−
√ITk

≤

Edist(¯λ(k)

l

, Λ∗k,i) +

N+N

µlB2

−
2√Tk

+

µ2l2B2
2µTk

+

µl2B2
2

+ N+BE

¯v(k)
l −
k

vµf l(w(k))
.
k
(34)

Suppose k = 0. By (33) and the choice of T0, we have E

ǫ2
0/4, so
(31) holds for k = 0. In addition, (32) holds trivially for k = 0. Next, we assume (31) and
(32) both hold for k and prove they also hold for k + 1.

2
vµf l(w(0))
k

¯v(0)
l −
k

≤

Since (31) and (32) hold for k, by (34) and the choice of Tk, we have

Edist(¯λ(k+1)

l,i

, Λ∗k,i)

N+

Xi=1

1
2

1
2
1
2

1
µ −

1
µ −
1
µ −

(cid:18)

(cid:18)

(cid:18)

ρ

ρ

ρ

(cid:19)

(cid:19)

(cid:19)

≤

≤

≤

ǫ2
k/4 +

ǫ2
k/4 +

µl2B2
2

µl2B2
2

+

µl2B2
2

+ N+BE

¯v(k)
l −
k

vµf l(w(k))
k

+ N+B

q

E

¯v(k)
l −
k

2
vµf l(w(k))
k

+ N+B.

(35)

From the updating equation w(k+1) = w(k)

that

γµ−

1(¯v(k)
m

−

−

¯v(k)
n ) in Algorithm 2, we know

w(k)

−
¯v(k)
m −
k
E
¯v(k)
m −
k
E
¯v(k)
n −
k

k
¯v(k)
n k
vµf m(w(k))
k
vµf n (w(k))
k

E

w(k+1)
k
1E
γµ−

1

γµ−

(cid:16)
1
+γµ−

(cid:16)

2γǫk
µ

≤

≤

≤

+ γnB + γmB,

+ E

vµf m (w(k))
k
+ E

−
vµf n(w(k))
k

−

w(k)

k
(cid:17)
w(k)
k
(cid:17)

(36)

where the second inequality is by triangle inequality and the last inequality is by (29) and
the fact that (31) holds for k.

Let ¯λ(k+1)
l,i

for i = 1, . . . , N+ and l = m and n.
Recall that si[j](v) be the jth largest coordinate of Si(v). By Assumption 1 and elementary

denote the ith coordinate of ¯λ(k+1)

l

26

Large-scale Optimization of Partial AUC

argument, we can show that si[j](v) is B-Lipschitz continuous for any i and j. Since vµf l(w)
is

1
µρ -Lipschitz continuous, we have

1

−

E

si[j](vµf l (w(k+1)))
(cid:12)
(cid:12)
(cid:12)

−

si[j](vµf l(w(k)))
(cid:12)
(cid:12)
(cid:12)

≤

≤

≤

BE

vµf l(w(k+1))
k
B

E

w(k+1)
k
2γǫk
µ

1

1

µρ

−
B

µρ

−

(cid:18)

vµf l (w(k))
k

−

w(k)

k

−

+ γnB + γmB

(37)

(cid:19)

for j = l and j = l + 1, where the last inequality is by (36). According to (20), (21), (37),
and Lemma 10, we can prove that, for i = 1, . . . , N+,

Edist

¯λ(k+1)
l,i
(cid:16)

, Λ∗k+1,i

≤

(cid:17)

Edist

¯λ(k+1)
l,i

, Λ∗k,i

+

(cid:16)

(cid:17)

B

µρ

1

−

(cid:18)

2γ
µ

+ γnB + γmB

.

(cid:19)

Combining this inequality with (35) yields (32) for case k + 1.

Stating (33) for case k + 1 gives

1
µ −

ρ

1
2

(cid:18)

(cid:19)

E

¯v(k+1)
l
k

2
vµf l(w(k+1))
k

−

≤

N+N

−
ITk+1

p

Edist(¯λ(k+1)

l

, Λ∗k+1) +

N+N
2

µlB2

−
Tk+1

+

µ2l2B2
2µTk+1

.(38)

p

By (38), (32) for case k +1, and the choice of Tk+1, we have E

≤
ǫ2
k+1/4, which means (31) holds for case k + 1. By induction, we have proved that (31) and
(32) holds for k = 0, 1, . . . .
F µ(w) = µ−

vµf n (w)) is Lµ-Lipschitz continuous, we have

1(vµf m (w)

Because

−

2
vµf l(w(k+1))
k

¯v(k+1)
l
k

∇

−

F µ(w(k))

−

F µ(w(k+1))

F µ(w(k)), w(k+1)

≥h−∇

µ−

1(vµf m (w(k))

=

=

≥

D
γ
µ2

−
γ
µ2

+

−

−

vµf m(w(k))

D
γ2Lµ
¯v(k)
m −
2µ2
(cid:13)
(cid:13)
vµf m(w(k))
(cid:13)

D

(cid:18)

γ
µ2 −

γ2Lµ
µ2

w(k)

Lµ
2 k
−
i −
vµf n(w(k))), γµ−

−
vµf n(w(k)), ¯v(k)

w(k+1)

w(k)

2
k

−

1(¯v(k)

m −

¯v(k)
n )

γ2Lµ
2µ2

−

¯v(k)
m −

¯v(k)
n

m −

¯v(k)
n −

vµf m (w(k)) + vµf n (w(k)) + vµf m (w(k))

E

(cid:13)
(cid:13)
(cid:13)

2

(cid:13)
(cid:13)
(cid:13)

vµf m (w(k)) + vµf n(w(k)) + vµf m(w(k))

¯v(k)
n −
vµf n(w(k)), ¯v(k)

m −

¯v(k)
n −

2

−

vµf n(w(k))
(cid:13)
(cid:13)
(cid:13)

vµf m (w(k))
k

−

(cid:19)

2
vµf n (w(k))
k

vµf m (w(k)) + vµf n (w(k))
E
vµf m (w(k)) + vµf n (w(k))
(cid:13)
(cid:13)
(cid:13)

γ2Lµ
µ2

¯v(k)
m −

¯v(k)
n −

(cid:13)
(cid:13)
(cid:13)

−

2

.

27

−

vµf n(w(k))
E

Yao, Lin, Yang

Applying Young’s inequality to the ﬁrst term on the right-hand side of the last inequality
above gives

≥ −

+

E

F µ(w(k))
h

−
¯v(k)
m −
k
γ2Lµ
µ2

¯v(k)
n −

F µ(w(k+1))
i
2 +
vµf m (w(k)) + vµf n (w(k))
k
γ2Lµ
µ2
2γ2Lµ
µ2

2
vµf n (w(k))
k
γ
µ2 +

vµf m (w(k))
k

vµf m (w(k))
k

1
2

−

−

−

(cid:19)

E

E

E

E

1
2

γ
µ2
(cid:18)
γ
µ2 −
2γ2Lµ
2µ2

(cid:18)

γ

−

≥

≥

γ
4µ2

E

vµf m (w(k))
k

−

2
vµf n (w(k))
k
−
3γ
2µ2 ǫ2
k,

−

2
vµf n (w(k))
k

(cid:18)

E

vµf m (w(k))
k

−

2
vµf n (w(k))
k

(cid:19)

¯v(k)
n −

vµf m(w(k)) + vµf n(w(k))
(cid:13)
(cid:13)
(cid:13)

¯v(k)
m −
(cid:13)
(cid:13)
(cid:13)
(cid:19)

ǫ2
k

2

where the second inequality is because of (32) for l = m and n and the last because of
γ

.

1
Lµ
Summing the above inequality over k = 0,

≤

, K

−

· · ·

1, we have

1
K

K

1

−

Xk=0

E

vµf m (w(k))
k

2
vµf n (w(k))
k

−

≤

≤

4µ2
γK

4µ2
γK

E(F µ(w(0))

−

F µ(w(K))) +

F (vµf n (w(0)))
(cid:16)

−

F ∗

+

(cid:17)

K

1

−

ǫ2
k

6
K

Xk=0
6 log K
,
K

where the second inequality is because F ∗ ≤
≤
F (vµf m (w(0))) (see Lemma 1 in Sun and Sun (2021)). Let ¯k be randomly sampled from
, then it holds
1
0, . . . , K
−
}
{
F µ(w(¯k))
2 = µ−
k

ǫ2 = min
1, µ−
{
}

vµf n(w(¯k))
2
k

vµf m(w(¯k))
k

1, µ2
2 min
{

F µ(w(K)) and F µ(w(0))

F (vµf n (w(K)))

ǫ2
}

k∇

µ−

2E

≤

≤

−

E

2

by the deﬁnition of K. On the other hands, by (31), we have for l = m and n that

E

¯v(¯k)
l −
k

vµf l (w(¯k))
2
k

≤

1
K

K

1

−

Xk=0

ǫ2
k ≤

log K

min

K ≤

1, µ2
{
48

ǫ2
}

ǫ2
4

,

≤

where the last inequality is because of the value of K. Hence, by the second claim in
Lemma 3 (with w = w(¯k) and ¯v = ¯v(¯k)
is an nearly ǫ-critical point of (5) with f l
(cid:3)
deﬁned in (6). This completes the proof.

n ), ¯v(¯k)

n

D. Additional Materials for Numerical Experiments

In this section, we present some additional details of our numerical experiments in Section 7
which we are not able to show due to the limit of space.

28

Large-scale Optimization of Partial AUC

D.1 Statistics on Data

In this section, we present some summary statistics on the datasets we use in our exper-
iments in this paper. For SoRR loss minimization, we use four benchmark datasets from
the UCI (Dua and Graﬀ, 2017) data repository preprocessed by Libsvm (Chang and Lin,
2011): a9a, w8a, ijcnn1 and covtype. The statistics of these datasets are summarized in
Table 4.

Table 4: Statistics of the UCI datasets.

Datasets # Samples # Features

a9a
w8a
ijcnn1
covtype

32,561
49,749
49,990
581,012

123
300
22
54

D.2 ROC Curves of CheXpert Testing Set

In this subsection, we plot the ROC curves of three algorithms on the CheXpert testing set
with random seed set as 2 (Figure 3). The range of false positive rate for the pAUC is set
as [0.05, 0.5].

D.3 Algorithm for DCA Subproblem

Algorithm 4 is used to solve the subproblem (18) in the kth main iteration of DCA. It is
similar to the SBCD method in Algorithm 1.

29

Yao, Lin, Yang

Algorithm 4 Stochastic Block Coordinate Descent for (18): ( ¯w, ¯λ) =SBCD(w, λ, T, ξ(k))
1: Input: Initial solution (w, λ), the number of iterations T , sample sizes I and J and a

deterministic subgradient ξ(k).

2: Set (w(0), λ(0)) = (w, λ) and choose (ηt, θt)T
1 do
3: for t = 0 to T
−
Sample
1, . . . , N+}
4:
It ⊂ {
Sample
1, . . . , N
Jt ⊂ {
−}
Compute stochastic subgradient w.r.t. w:

= I.
= J.

with
with

|It|
|Jt|

5:
6:

1
t=0 .
−

G(t)

w =

N+N−
IJ

sij (w(t))1

∇

sij(w(t)) > λ(t)
i
(cid:16)

(cid:17)

−

ξ(k)

i∈It X
j∈Jt
X

7:

Stochastic subgradient update on w:

−
Compute stochastic subgradient w.r.t. λi for i

w(t+1) = w(t)

8:

ηtG(t)
w

∈ It:

G(t)

λi = n

N−
J

−

1

sij(w(t)) > λ(t)
i
(cid:16)

(cid:17)

j∈Jt
X

for i

t
∈ I

9:

Stochastic block subgradient update on λi for i

λ(t+1)
i

=

(

10: end for
11: Output: ( ¯w, ¯λ) = (w(T ), λ(T )).

θtG(t)
λi

λ(t)
i −
λ(t)
i

∈ It:
i
i /

t,
t.

∈ I
∈ I

30

Large-scale Optimization of Partial AUC

CheXpert_D1

CheXpert_D1

3

2.5

2

0.65

1.5

1

0.6

0.55

s
s
o
L
g
n
n
i
a
r
T

i

AGD-SBCD
DCA

20

40

60

80

0

50

100

150

# of epoch
CheXpert_D2

AGD-SBCD
DCA

3

2.5

2

0.58

1.5

1

3.5

3

2.5

2

0.56

0.54

50

100

150

0

50

100

150

# of epoch
CheXpert_D3

AGD-SBCD
DCA

0.76

20

40

60

80

50

100

150

# of epoch
CheXpert_D4

AGD-SBCD
DCA

1.5

0.74

0.72

0.8

1

0

3.5

3

2.5

2

1.5

0.75

1

0.7

s
s
o
L
g
n
n
i
a
r
T

i

s
s
o
L
g
n
n
a
r
T

i

i

s
s
o
L
g
n
n
a
r
T

i

i

50

100

150

0

50

100

150

3.5
3
2.5

2

1.5

1

s
s
o
L
g
n
n
i
a
r
T

i

0.5

0

# of epoch
CheXpert_D5

AGD-SBCD
DCA

0.42

0.41

0.4

20

40

60

80

100

120

140

50

100

150

# of epoch

0.7

0.6

0.5

0.4

0.3

i

t
e
s
g
n
n
i
a
r
T
n
o
C
U
A
p

0.2

0

0.8

0.7

0.6

0.5

0.4

0.3

i

t
e
s
g
n
n
i
a
r
T
n
o
C
U
A
p

0.2

0

0.5

0.45

0.4

0.35

0.3

0.25

0.2

0

0.5

0.45

0.4

0.35

0.3

0.25

0.2

0

i

i

t
e
s
g
n
n
a
r
T
n
o
C
U
A
p

i

i

t
e
s
g
n
n
a
r
T
n
o
C
U
A
p

0.9

0.8

0.7

0.6

0.5

0.4

0.3

i

t
e
s
g
n
n
i
a
r
T
n
o
C
U
A
p

0.2

0

AGD-SBCD
DCA
Random Classifier

50

100

150

# of epoch
CheXpert_D2

AGD-SBCD
DCA
Random Classifier

50

100

150

# of epoch
CheXpert_D3

AGD-SBCD
DCA
Random Classifier

50

100

150

# of epoch
CheXpert_D4

AGD-SBCD
DCA
Random Classifier

50

100

150

# of epoch
CheXpert_D5

AGD-SBCD
DCA
Random Classifier

50

100

150

# of epoch

Figure 2: Results for Patial AUC Maximization.

31

 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
Yao, Lin, Yang

R
P
T

1

0.8

0.6

0.4

0.2

0

1

0.8

R
P
T

0.6

0.4

0.2

0

AUC of CheXpert_D1

AGD-SBCD
SVM

-tight

pAUC

DCA

0.05

0.2

0.4 0.5 0.6
FPR
AUC of CheXpert_D3

0.8

1

AGD-SBCD
SVM

-tight

pAUC

DCA

0.05

0.2

0.4 0.5 0.6
FPR

0.8

1

R
P
T

R
P
T

1

0.8

0.6

0.4

0.2

0

1

0.8

0.6

0.4

0.2

0

AUC of CheXpert_D2

AGD-SBCD
SVM

-tight

pAUC

DCA

0.05

0.2

0.4 0.5 0.6
FPR
AUC of CheXpert_D4

0.8

1

AGD-SBCD
SVM

-tight

pAUC

DCA

0.05

0.2

0.4 0.5 0.6
FPR

0.8

1

R
P
T

1

0.8

0.6

0.4

0.2

0

AUC of CheXpert_D5

AGD-SBCD
SVM

-tight

pAUC

DCA

0.05

0.2

0.4 0.5 0.6
FPR

0.8

1

Figure 3: ROC Curves of CheXpert Testing Set.

32

