1
2
0
2

r
p
A
8
1

]
I

A
.
s
c
[

1
v
5
0
8
8
0
.
4
0
1
2
:
v
i
X
r
a

Low-rank State-action Value-function Approximation

Sergio Rozada, Victor Tenorio, and Antonio G. Marques∗

Abstract—Value functions are central to Dynamic Program-
ming and Reinforcement Learning but their exact estimation
suffers from the curse of dimensionality, challenging the devel-
opment of practical value-function (VF) estimation algorithms.
Several approaches have been proposed to overcome this issue,
from non-parametric schemes that aggregate states or actions
to parametric approximations of state and action VFs via, e.g.,
linear estimators or deep neural networks. Relevantly, several
high-dimensional state problems can be well-approximated by
an intrinsic low-rank structure. Motivated by this and leveraging
results from low-rank optimization, this paper proposes different
stochastic algorithms to estimate a low-rank factorization of
the Q(s, a) matrix. This is a non-parametric alternative to VF
approximation that dramatically reduces the computational and
sample complexities relative to classical Q-learning methods that
estimate Q(s, a) separately for each state-action pair.

Index Terms—Reinforcement Learning, Value Iteration, Q-
Learning, Low-Rank Approximation, Stochastic Dynamic Pro-
gramming.

I. INTRODUCTION AND MOTIVATION
As complex interconnected systems and big data become
pervasive, the engineering goal is to design intelligent algo-
rithms that leverage the data and learn how to interact with
the world autonomously. Reinforcement Learning (RL) tries to
embed into algorithms how humans interact with the world,
learning in real-time by trial and error [1]–[3]. Technically
speaking, RL solves sequential optimization problems, like
Dynamic Programming (DP) [2], in a model-free fashion.
When the environment is “simple”, solid theoretical results
exist, and a range of algorithms address how to learn the
mapping from the state observed at every time instant to
the action to be executed. However, RL is algorithmically
challenging when the actions and states describing the en-
vironment are numerous, high-dimensional, or deﬁned in
continuous domains. Such a curse of dimensionality calls
for approximated algorithms that, e.g., discretize continuous
domains while simultaneously limiting the complexity of the
problem to be solved [1]. Indeed, different (parametric and
nonparametric) approaches to reduce the number of degrees
of freedom (keeping computational complexity under control
and facilitating learning) at the cost of sacriﬁcing optimality
exist [2]. Motivated by its practical relevance and the described
computational challenges, this paper puts forth nonparametric,
stochastic, model-free algorithms that leverage results from
low-rank optimization and matrix completion [4], [5]. The
rest of this section provides details on the notation and
fundamentals of RL and, once those have been presented,
describes the contribution in a more rigorous manner.

This work was supported by the Spanish Federal Grants SPGRAPH
(PID2019-105032GB) and by the Young Researchers R&D Project Grants
F661-MAPPING-UCI and F663-AAGNCS both funded by the Community of
Madrid (CAM) and the King Juan Carlos University (URJC). All the authors
are with the Dept. of Signal Theory and Comms., URJC, Madrid, Spain.
Sergio Rozada is also with Clients Solutions Advanced Analytics, BBVA,
Madrid, Spain. ∗Contact author: antonio.garcia.marques (AT) urjc.es.

Fundamentals of RL and notation. We deal with closed-loop
setups where agents interact sequentially with an environment.
The three key elements to deﬁne the RL problem are the states
describing the environment, the actions agent(s) can take, and
their associated rewards. Mathematically, the environment is
represented by a (discretized) set of states S. In each state,
the agent takes actions from a (discretized) set of actions,
or action space, A. We assume a time-slotted scenario and,
with t = 1, ..., T representing the time index. After taking an
action at in a particular state st, the agent receives a numerical
signal, or reward rt, that quantiﬁes the instantaneous value
of that state-action pair. Two key aspects in RL are: (i) the
dependence of rt on st and at is oftentimes not deterministic,
and (ii) the action at has an impact not only in rt but also
in st(cid:48) for t(cid:48) > t, coupling the optimization across time. In
other words, when deciding the value of at one must take
into account not only its impact on rt but also on rt(cid:48) for
t > t(cid:48). A common approach to deal with these two issues is
to rely on Markovianity, recast ﬁrst the optimization within
the framework of Markov Decision Process (MDP) and, then,
view RL as an stochastic approach for the MDP.

In this context, suppose that we have a policy π : S (cid:55)→ A
that maps states into actions and deﬁne the expected aggre-
gated reward as E[(cid:80)T
τ =t γτ −trτ |st, at], where γ ∈ (0, 1) is a
discount factor that places more focus on near-future values,
and the time-horizon is oftentimes set to T = ∞ [1], [2].
Given the policy π, the so-called value function (VF) quantiﬁes
the expected reward associated with that particular (alterna-
tively, the optimal) policy. Since the expectation depends on
both the state and the action, two versions of the VF can be
deﬁned. For the one used in this paper, the so-called state-
S × A → R, the
action value function (SA-VF) Qπ :
dependence on the state and the action is kept. Since each
(s, a) pair has one Q-value associated with it, when the states
and actions are discrete, all the Q-values are arranged in the
form of a matrix Qπ ∈ RDS ×DA , with DS being the number
of states (the cardinality of S) and DA the cardinality of A [1].
When the state (action) is dS-dimensional and continuous, the
classical approach is to discretize each dimension using NS
intervals so that DS = N dS
S . This exponential dependence
affects computational and learning performance, calling for
algorithms to keep the complexity of the model under control.
The distinctive feature in RL is that the statistical depen-
dence across states and actions is unknown (or intractable),
and one must resort to model-free algorithms that learn on-
the-ﬂy by using realizations (trajectories) of the MDP [1],
[2]. As in MDP and DP, there are RL methods focused on
learning the policy π –typically under a parametric approach
followed by stochastic gradient descent (SGD)–, while others
focus on learning the VF (using parametric or nonparametric
approaches). Stochastic optimization of neural networks (NN)
and linear-based models are widely-used parametric alterna-

 
 
 
 
 
 
tives [1], [2], while the classical Q-learning algorithm is the
most celebrated example of the non-parametric class [6].
Contribution and related work. This manuscript proposes
different low-rank stochastic RL algorithms to estimate the
Q-function in a model-free and online setup. Most of the
proposed algorithms are designed using SGD and can be
interpreted as temporal differences (TD)-based schemes [2].
Once the estimation concludes, the policies are obtained as
those that, for the current state, select the action maximizing
the estimated SA-VF. The key aspect of the design is to
regularize the estimation problem by promoting an SA-VF
with a low-rank structure via matrix factorization. Those
techniques have been successfully utilized in the context of
low-rank optimization and matrix completion [4], [5], [7], [8],
but their use in DP/MDP (and, especially, in RL and TD) has
been limited. For example, [9], [10] used matrix-factorization
approaches to approximate the transition matrix of an MDP
and, then, leverage those to obtain the associated VF and
optimal policy. In [11], the author proposes obtaining ﬁrst
the Q-function of an MDP and, then, approximate it using a
low-rank plus sparse decomposition. Similarly, in the context
of DPs associated with energy storage, [12], [13] proposed a
low-rank (rank-one) approximation for the estimation of the
state VF. Recently, low-rank optimization has been proposed to
approximate the SA-VF using model-based and ofﬂine setups
[14], [15]. Another work that uses factorization techniques in
the context of RL is [16], where the focus is on deep factorized
architectures amenable to be implemented distributedly in
multi-agent setups. Also within RL setups, linear models have
been used to approximate the SA-VF on-the-ﬂy [17]. In those
works a set of features is deﬁned (based on prior knowledge,
collected data, or spectral properties of the transition matrices
[18], [19]) and then each entry of the SA-VF is modeled as a
weighted sum of the features associated with the (s, a) pair.
The weights are assumed to be the same across Q and the
focus of the RL algorithm is on their estimation using sample
trajectories. As discussed in detail at the end of Sec. 2, a
distinctive feature of the low-rank RL approaches we put forth
in this paper is that, here, the features are assumed to follow a
factorized (bilinear) model and both features and weights are
learned jointly, in real-time, from the observed data.

II. APPROXIMATE LOW-RANK STATE-ACTION VALUE
FUNCTION ESTIMATION

Before presenting our algorithms, we need to provide a
succinct description of classical nonparametric RL estimation
of the Q-function, which tries to optimize:

ˆQ = argminQ

1
2

(cid:80)

(s,a)∈M(qa

s − [Q]s,a)2

(1)

where M is the set of state-action tuples (s, a) that the agent
has sampled from the environment, and qa
s is the target signal
when being in state s and taking action a. In contrast to other
learning paradigms, in RL this target signal qa
s is not known
in advance and should be estimated on the ﬂy. To estimate
the target signal, we can use the available reward to deﬁne
s + γ[ ˆQ]s(cid:48),a(cid:48), where the tuple (s(cid:48), a(cid:48)) represents the
qa
s = ra
state-action pair sampled in the subsequent time instant.

Suppose now that: i) the action a(cid:48) is selected as the one
maximizing the current estimate of the SA-VF and, hence,
[ ˆQ]s(cid:48),a(cid:48) = maxa[ ˆQ]s(cid:48),a; and ii) (1) is solved via an SGD
method that, at each time t, updates the current estimates based
on (st, at, rt, st+1). This yields the following update rule
[ ˆQt−1]st+1,a

[ ˆQt]st,at =[ ˆQt−1]st,at + αt

(cid:0)rt + γ max

a

(cid:1)

−[ ˆQt−1]st,at

(2)
with αt being the learning rate (stepsize), ˆQt being the
estimate of the SA-VF at time t, and where we remark that
[ ˆQt]s,a = [ ˆQt−1]s,a for all (s, a) (cid:54)= (st, at). The scheme
in (2) corresponds to the celebrated (TD-based) Q-learning
algorithm, which enjoys convergence guarantees, provided that
all (s, a) pairs are visited inﬁnitely often [6]. To facilitate
this latter point, an exploration module is added to the al-
gorithm so that with probability (1 − εt) the action a(cid:48) =
argmaxa[ ˆQt−1]st+1,a is selected (and, hence, the expression
in (2) applies) and, with probability εt, the action a(cid:48) is chosen
uniformly at random. Unfortunately, Q-learning can take a
long time to converge, the reason being that the number of
parameters DSDA can be very large and that, when a pair
(s, a) is observed at time t, only one of the entries of the
matrix ˆQt ∈ RDS ×DA is updated.

Low-rank via matrix factorization. Our contribution to
facilitate and accelerate the learning of the SA-VF is to
limit its degrees of freedom by promoting low-rank solutions.
More speciﬁcally, we modify the optimization (estimation)
of Q to either force or promote low-rank estimates. Since
adding the constraint rank(Q) ≤ M to the optimization in
(1) –or, alternatively, adding rank(Q) as a regularizer– is
computationally intractable, one can replace rank(Q) with the
nuclear norm (cid:107)Q(cid:107)∗ , its convex surrogate, or implement a non-
convex matrix factorization approach [5]. The latter entails
considering the matrices L ∈ RDS ×M and R ∈ RM ×DA ,
write the VF as Q = LR, and solving

{ˆL, ˆR} = argminL,R

1
2

(cid:80)

(s,a)∈M(qa

s − [LR]s,a)2.

(3)

The approximated SA-VF is then obtained as ˆQ = ˆL ˆR,
which is guaranteed to have a rank no larger than M . While
the factorized problem in (3) is non-convex, computationally
efﬁcient alternating-minimization schemes that, in the con-
text of matrix completion, are guaranteed to converge to a
local minimum can be implemented [5], [7]. While different
stochastic algorithms can be developed to handle (3), since the
cost is quadratic and the problem is bilinear in L and R, a
stochastic alternating least-squares approach is well motivated.
Let ˆLt and ˆRt denote the stochastic estimates of L and R
at the end of the slot t; deﬁne the matrix Qt ∈ RDS ×DA
as [Qt]s,a = [ˆLt−1( ˆRt−1)]s,a for all (s, a) (cid:54)= (st, at) and
s = rt + γ max[ˆLt−1( ˆRt−1)]s(cid:48),a; and let
[Qt]st,at = qa
k = 1, ..., K be an iteration index. With this notation at hand,
initialize ˆR[0] = ˆRt−1, set ¯Q = Qt, and iterate as

ˆL[k] = ¯Q ˆRT
ˆR[k] = (ˆLT
[k]

[k−1]( ˆR[k−1]
ˆL[k])−1 ˆLT
[k]

ˆRT
¯Q

[k−1])−1 and

(4)

(5)

for k = 1, ..., K and, ﬁnally, set the stochastic estimates to
ˆLt = ˆL[K] and ˆRt = ˆR[K]. A simpler alternative to optimize
(3) is to run just a single iteration of a stochastic gradient
descent. To that end, let [ˆLt]st ∈ RM be a vector collecting
the M entries of ˆLt associated with state st and, likewise,
let [ ˆRt]at ∈ RM be a vector collecting the M entries of ˆRt
associated with action at. With these conventions, at time t, we
use (st, at, st+1) to update the stochastic estimates as follows

[ˆLt]st =[ˆLt−1]st + αt

−[ˆLt−1 ˆRt−1]st,at

[ ˆRt]at =[ ˆRt−1]at + αt

a

(cid:0)rt + γ max
(cid:1)[ ˆRt]at
(cid:0)rt + γ max
(cid:1)[ˆLt]st,

a

− [ˆLt−1 ˆRt−1]st,at

[ˆLt−1 ˆRt−1]st+1,a

[ˆLt−1 ˆRt−1]st+1,a

(6)

(7)

and [ˆLt]s = [ˆLt−1]s and [ ˆRt]a = [ ˆRt−1]a for all (s, a) (cid:54)=
(st, at). The updates in (6)-(7) resemble those of the Q-
learning algorithm in (2), are less computationally demanding
than those in (4), and for sufﬁciently small αt converge to a
local optimum. In contrast, the convergence of the alternating
scheme (which is not always guaranteed) happens at a faster
pace. If the speed of convergence is an issue, the stepsize in
(6)-(7) can incorporate a gradient normalization to circumvent
the slower convergence due to plateaus and saddle points
oftentimes present in non-convex optimization [20]. Finally,
all the methods proposed here consider an exploration module
to guarantee that all (s, a) pairs are visited.

F + (cid:107)R(cid:107)2

Compared to Q-learning,

the algorithms just described
exhibit two main advantages: a) the number of parameters to
estimate is smaller –(DS + DA)M vs. DSDA– and b) at each
t, the entire row [ˆLt]st and column [ ˆRt]at (with M values
each) are updated. This contrasts with Q-learning, where only
a single parameter, the entry [ ˆQt]st,at, is updated at time t.
Matrix factorization meets the nuclear norm. Leveraging
the fact that the nuclear norm can be written as (cid:107)Q(cid:107)∗ =
1
2 minLR=Q (cid:107)L(cid:107)2
F , the cost in (3) can be augmented
F and η(cid:107)R(cid:107)2
with the Frobenius regularizers η(cid:107)L(cid:107)2
F . The reg-
ularizers are quadratic; hence, the structure and computational
complexity of the updated stochastic iterates is roughly the
same. On the other hand, since the nuclear norm is convex,
the regularizers help to stabilize the behavior of our stochastic
algorithms. In fact, conditions under which the Frobenius-
regularized iterates converge to the same solution than the
nuclear-norm minimization can be rigorously obtained [8],
[21]. Lastly, the regularized formulation promotes solutions
whose rank is typically smaller than M . This is important
because it bypasses the problem of selecting the value of M .
Comparison with parametric linear estimation of the SA-
VF. Linear parametric schemes for SA-VF estimation consider
the approximation [ ˆQ]s,a = Qθ(s, a) = φ(s, a)T θ, where
φ(s, a) ∈ RM is the set of M features that deﬁnes a particular
state-action pair and θ ∈ RM are the parameters/weights to
be estimated online [18]. The key issue in these approaches
is how to pre-design the feature vectors φ(s, a). Differently,
we learn the feature vectors and the weights jointly. To be
speciﬁc, suppose that rank( ˆQ) = M and let vm (um) be
the mth left (right) singular vector and σm the mth singular

value. Then, we have that [ ˆQ]s,a = (cid:80)M
m=1 σm[vm]s[um]a,
illustrating that in our approach the features are implicitly
deﬁned as φ(s, a) = [[v1]s[u1]a, ..., [vM ]s[uM ]a]T and the
linear weights as θ = [σ1, ..., σM ]T , with the difference being
that here both φ(s, a) and θ are learned online.
Reshaping the Q-matrix. In many applications, the states
and, oftentimes, the actions involve multiple variables. To
be mathematically precise, suppose that we have dS state
variables (dimensions) and dA action variables. If the ith
state variable takes values from the set Si, the state space
is deﬁned as S = S1 × ... × SdS . Analogously, we have
that A = A1 × ... × AdA. Even in those cases, the RL
literature refers to Q(s, a) as a two-dimensional function,
arranging its values in the form of a matrix Q whose rows
represent
then tuples of states and its columns tuples of
actions. Throughout this paper, we kept such a convention
and developed algorithms to learn a low-rank approximation
of Q ∈ R|S|×|A| , which, because since |A| is typically much
smaller than |S|, is a (sometimes extremely) tall matrix.

We propose here an alternative low-rank approximation
scheme that ﬁrst reshapes Q ∈ R|S|×|A| as ¯Q ∈ RNR×NC ,
with NRNC = |S||A|, and then implements a low-rank
decomposition on the reshaped matrix ¯Q. The idea in the ﬁrst
step is to have NR ≈ NC ≈ (cid:112)|S||A|, so that the reshaped
matrix ¯Q is approximately square, reducing the number of
parameters to be estimated in the low-rank approximation
run in the second step from PQ = M (|S| + |A|) to P ¯Q =
M (NC + NR) ≈ 2M (cid:112)|S||A|. For the practical setups where
|S| (cid:29) |A| we can split the dS state variables into two sets
and consider a ¯Q matrix whose rows correspond to an element
of S1 × ... × SdS(cid:48) , whose columns correspond to an element
of SdS(cid:48) +1 × ... × SdS × A1 × ... × AdA , and whose number
of columns and rows is approximately the same. Unless prior
knowledge exists, the ordering of the state and action variables
is arbitrary and, hence, the assignment of state variable to
columns and rows is arbitrary as well, potentially leading to
different approximation performances.

the numerical

While conceptually simple,

results will
demonstrate the beneﬁts and robustness of the proposed ap-
proach in standard RL test-cases. Arranging the values of the
Q-matrix in the form of a tensor with dS +dA dimensions and
postulating low-rank tensor decomposition algorithms emerge
as a natural follow-up research direction.

III. NUMERICAL EXPERIMENTS
This section tests the proposed algorithms in three of the
standard RL problems of the toolkit OpenAI Gym (see [22]
and Fig. 1.a). Our goal is to illustrate i) the convergence
properties of our schemes; ii) the advantages of considering a
low-rank Q(s, a) matrix; and iii) the success of our method in
an environment too large for classical non-parametric methods.
The code and full details can be found in [23].
Convergence properties. We test the rate of convergence of
(3) in the FrozenLake-v0 environment [22]. This is a simple
deterministic ﬁnite-state ﬁnite-action grid-like environment,
where a reward is given to the agent for reaching a goal
state. Although it can be solved using Q-learning, our algo-
rithm can exploit the low-rank structure of the environment

to accelerate the rate of convergence using almost half of
the parameters. Convergence is analyzed via i) the number
of episodes required to obtain the optimal solution; and ii)
measuring the evolution of Squared Frobenius Error (SFE)
between the estimated ˆQ and the true Q as SFE = || ˆQ−Q||2
F .
We run 100 simulations for different values of (cid:15)t = (cid:15).
Q-learning stores a 16 × 4 state-action table, so that
the
total number of parameters is 64. When testing our low-rank
alternative we impose a maximum rank of M = 2, so that the
total number of parameters is (16 + 4)M = 40. The two main
observations that can be obtained from Fig. 1.b and 1.c are
that the low-rank alternatives: 1) can converge to the optimal
solution, both in terms of the number of steps and SFE; and
2) converge faster in all possible scenarios, that is, for a ﬁxed
value of (cid:15), the low-rank approach proposed in (3) requires
fewer episodes than Q-learning to converge.
Parametrization efﬁciency. In the Pendulum-v0 environ-
ment [22], an agent tries to maintain a pendulum upright.
This problem has two continuous states (angle and angular
velocity), one continuous action, and the reward is multi-
objective (keeping the pendulum vertical while minimizing
the action exerted). Tackling the continuous-deﬁned problem
requires discretizing the state-action space. However, the ﬁner
the discretization, the larger the size of the Q(s, a)-matrix.
In particular, the Cartesian product of the regularly-sampled
states deﬁnes a discrete state space of size DS = 2121 (see
[23] for details). Five RL schemes have been compared with
different discretizations of the action space. Two of them are
Q-learning with the action space discretized at low and high
resolutions, with size DA = 5 and DA = 41 respectively. Two
are low-rank schemes that use high-resolution discretization
(DA = 41 actions), keep the shape of the Q matrix, and
consider a (low) rank of M = 3 and M = 5, respectively.
The last scheme uses the same resolution, but reshapes the Q
matrix (with 295 rows and 295 columns) and sets the rank to
M = 10. Note that imposing low rank brings down the number
of parameters signiﬁcantly. A roughly-discretized version of
Q-learning uses 10, 605 parameters while the ﬁner version
needs 86, 961 parameters. The low-rank alternative with rank
M = 3 only needs 6, 486 parameters while the larger version
with M = 5 needs 10, 810 parameters. Finally, the reshaped
alternative, which is closer to a square matrix, requires 5, 900
parameters for M = 10. Variants of Q-learning undersampling
the state space did not converge. Notice also that the roughly-
discretized version of the Q-learning with DA = 5 actions
and our low-rank scheme with M = 5 entail basically the
same number of parameters. The low-rank scheme with rank
M = 3 and the reshaped one with M = 10 were trained with
the stochastic algorithm deﬁned in (6) and (7) while the low-
rank scheme with the larger rank M = 5 was trained with
the regularized alternative. A certain level of exploration is
required, thus the exploratory probability was set to (cid:15)t = 0.2.
Each algorithm was trained 10 times, and the results ob-
tained can be found in Figs. 1.d and 1.e. These results are
obtained running a greedy test episode every several training
episodes. As expected, the smaller version of Q-learning con-
verges faster to a solution able to keep the pendulum upright,
but fails at selecting low-cost actions (the poor discretization

of the action space forces the agent to select sub-optimal
actions). In contrast, the proposed low-rank schemes converge
faster than the Q-learning version with the largest number
of parameters. For the simulated number of episodes, they
also outperform both versions of Q-learning in terms of cost.
Moreover, an SVD decomposition of the Q-matrix estimated
by the Q-learning algorithm reveals that the ﬁrst three singular
values account for more than 80 % of the total variance. On the
other hand, the reshaped scheme is the best performing agent.
A more prudently designed parametrization of the model
leads to better results in terms of both, cumulative rewards
and speed of convergence. These results demonstrate that the
(unconstrained) optimal solutions are inherently low-rank and,
hence, our algorithms fruitfully exploit this structure.
Comparison against the state-of-the-art. The Acrobot-v1 is
a double-pendulum environment that mimics a gymnast trying
to swing up [22]. There are six state variables: the sine and
cosine of the angle and the angular velocity of the upper and
lower joints. Only one action variable exists, which can take
three different values. The number of discrete states produced
by the Cartesian product of regularly-sampled states grows
exponentially on the number of states. Even discretizing each
of the six states at low-resolution results in roughly 7 · 106
states, leading to a Q(s, a)-matrix with more than 20 · 106
entries (see [23] for details). This dimensionality explosion can
be addressed using an intelligent reshaping of the Q(s, a) into
a 4520 × 4519 matrix and applying the LR scheme imposing
low rank M = 2. The total number of parameters drops
dramatically to 18, 078. Two versions of the LR algorithm
are compared in this experiment, one that normalizes the
stochastic gradient update and another one that does not.

The VF and SA-VF-based approaches to handle high-
dimensional environments are limited to parametric estimators.
Here we will compare our performance with that of Deep
Q-learning (DQN) [3], an ofﬂine NN state-of-the-art repre-
sentative of parametric value-based methods. Linear methods
were also tested but their performance is not shown because
their value was worse and required many more episodes (more
than one additional order of magnitude) to converge. NN-
based RL algorithms suffer from convergence problems due to
the temporal correlation. This is typically overcome by using
experience replay (ER) [3], which requires the training to be
ofﬂine. A one-layer Multi-Layer Perceptron (MLP) with a
comparable number of parameters is used to implement DQN.
One simulation implements a more efﬁcient version of DQN
using mini-batches of size S = 12 obtained from the ER buffer
to perform each training update. A lighter version of DQN is
trained using mini-batches of size S = 1.

The results of 10 trials are shown in Fig. 1.f. As expected,
the parametric NN-based schemes converge to a better solu-
tion. However, our normalized non-parametric LR alternative
successfully handles the problem and, in fact, converges faster
than the light version of DQN. In contrast to DQN, the LR
algorithm is fully online and does not need ancillary ofﬂine
twists to stabilize the training. It is also important to note that
the DQN trained with mini-batches of size S = 12 took 2.5
more training time: the use of larger batches introduces a trade-
off between the training time and efﬁcient use of previous

Fig. 1: The upper left picture (a) exempliﬁes the three tested RL environments, from left to right: FrozenLake-v0, Pendulum-v0
and Acrobot-v1. The two upper right pictures (b, c) show the performance of Q-learning and our low-rank (LR) alternative in
FrozenLake-v0 for different values of (cid:15). The two bottom left pictures (d, e) show the performance of Q-learning and the LR
alternative in Pendulum-v0. The bottom right picture (f) depicts the performance of DQN and the LR alternative in Acrobot-v1.

experience. The effect of the normalization of the gradient
can be observed as well. Both LR variants converge to the
same solution, but the normalized one does it faster.

IV. CONCLUDING REMARKS

A collection of RL algorithms that exploit the intrinsic low-
rank structure of the SA-VF has been proposed. The schemes
leverage sounded matrix completion results to estimate the
Q-function in a model-free and online fashion. Moreover, a
reshaping of the Q-matrix has been introduced to parametrize
the model in a more efﬁcient way. The numerical experiments
show the advantages of the low-rank models both in terms of
speed of convergence and the achieved cumulative reward.

REFERENCES

[1] R. S. Sutton and A. G. Barto, “Reinforcement learning: An introduction,”

2011.

[2] D. P. Bertsekas, Reinforcement learning and optimal control. Athena

Scientiﬁc, 2019.

[3] V. Mnih, K. Kavukcuoglu, D. Silver, A. Graves, I. Antonoglou, D. Wier-
stra, and M. Riedmiller, “Playing Atari with deep reinforcement learn-
ing,” arXiv preprint arXiv:1312.5602, 2013.

[4] C. Eckart and G. Young, “The approximation of one matrix by another
of lower rank,” Psychometrika, vol. 1, no. 3, pp. 211–218, 1936.

[5] I. Markovsky, Low rank approximation. Springer, 2019.
[6] C. J. Watkins and P. Dayan, “Q-learning,” Machine learning, vol. 8, no.

3-4, pp. 279–292, 1992.

[7] M. Udell, C. Horn, R. Zadeh, S. Boyd et al., “Generalized low rank
models,” Foundations and Trends® in Machine Learning, vol. 9, no. 1,
pp. 1–118, 2016.

[8] M. Mardani, G. Mateos, and G. B. Giannakis, “Decentralized sparsity-
regularized rank minimization: Algorithms and applications,” IEEE
Trans. Signal Processing, vol. 61, no. 21, pp. 5374–5388, 2013.

[9] A. M. Barreto, R. L. Beirigo, J. Pineau, and D. Precup, “Incremental
stochastic factorization for online reinforcement learning,” in Proc. AAAI
Conf. Artiﬁcial Intelligence, 2016.

[10] N. Jiang, A. Krishnamurthy, A. Agarwal, J. Langford, and R. E.
Schapire, “Contextual decision processes with low bellman rank are pac-
learnable,” in Proc. Intl. Conf. Machine Learning-Volume 70.
JMLR.
org, 2017, pp. 1704–1713.

[11] H. Y. Ong, “Value function approximation via low-rank models,” arXiv

preprint arXiv:1509.00061, 2015.

[12] B. Cheng and W. B. Powell, “Co-optimizing battery storage for the
frequency regulation and energy arbitrage using multi-scale dynamic
programming,” IEEE Trans. Smart Grid, vol. 9.3, pp. 1997–2005, 2016.
[13] B. Cheng, T. Asamov, and W. B. Powell, “Low-rank value function
approximation for co-optimization of battery storage,” IEEE Trans.
Smart Grid, vol. 9.6, pp. 6590–6598, 2017.

[14] Y. Yang, G. Zhang, Z. Xu, and D. Katabi, “Harnessing structures
for value-based planning and reinforcement learning,” arXiv preprint
arXiv:1909.12255, 2019.

[15] D. Shah, D. Song, Z. Xu, and Y. Yang, “Sample efﬁcient rein-
learning via low-rank matrix estimation,” arXiv preprint

forcement
arXiv:2006.06135, 2020.

[16] Y. Chen, M. Zhou, Y. Wen, Y. Yang, Y. Su, W. Zhang, D. Zhang,
J. Wang, and H. Liu, “Factorized Q-learning for large-scale multi-agent
systems,” arXiv preprint arXiv:1809.03738, 2018.

[17] F. S. Melo and M. I. Ribeiro, “Q-learning with linear function approx-
Springer, 2007, pp.

imation,” in Intl. Conf. Comp. Learning Theory.
308–322.

[18] B. Behzadian, S. Gharatappeh, and M. Petrik, “Fast feature selection
for linear value function approximation,” in Proc. Intl. Conf. Automated
Planning and Scheduling, vol. 29, no. 1, 2019, pp. 601–609.

[19] B. Behzadian and M. Petrik, “Low-rank feature selection for reinforce-

ment learning.” in ISAIM, 2018.

[20] R. Murray, B. Swenson, and S. Kar, “Revisiting normalized gradient
descent: Fast evasion of saddle points,” IEEE Trans. Automatic Control,
vol. 64, no. 11, pp. 4818–4824, 2019.

[21] S. Burer and R. D. Monteiro, “Local minima and convergence in low-
rank semideﬁnite programming,” Mathematical Programming, vol. 103,
no. 3, pp. 427–444, 2005.

[22] G. Brockman, V. Cheung, L. Pettersson, J. Schneider, J. Schul-
man, J. Tang, and W. Zaremba, “OpenAI Gym,” arXiv preprint
arXiv:1606.01540, 2016.

[23] V.

T.

S. Rozada
repository: Low-rank
https://github.com/sergiorozada12/low-rank-rl.

and A. G. Marques,
value-function
state-action

code
“Online
approximation,”

