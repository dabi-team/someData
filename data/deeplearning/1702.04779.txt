7
1
0
2

b
e
F
5
1

]

C
C
.
s
c
[

1
v
9
7
7
4
0
.
2
0
7
1
:
v
i
X
r
a

Compression Complexity

Stephen Fenner
University of South Carolina

Lance Fortnow
Georgia Institute of Technology

February 17, 2017

Abstract

The Kolmogorov complexity of x, denoted C(x), is the length of
the shortest program that generates x. For such a simple deﬁnition,
Kolmogorov complexity has a rich and deep theory, as well as applica-
tions to a wide variety of topics including learning theory, complexity
lower bounds and SAT algorithms.

Kolmogorov complexity typically focuses on decompression, going
from the compressed program to the original string. This paper de-
velops a dual notion of compression, the mapping from a string to its
compressed version. Typical lossless compression algorithms such as
Lempel-Ziv or Huﬀman Encoding always produce a string that will
decompress to the original. We deﬁne a general compression concept
based on this observation.

For every m, we exhibit a single compression algorithm q of length
about m which for n and strings x of length n ≥ m, the output of q
will have length within n − m + O(1) bits of C(x). We also show this
bound is tight in a strong way, for every n ≥ m there is an x of length
n with C(x) ≈ m such that no compression program of size slightly
less than m can compress x at all.

We also consider a polynomial time-bounded version of compres-
sion complexity and show that similar results for this version would
rule out cryptographic one-way functions.

1

 
 
 
 
 
 
1

Introduction

Kolmogorov complexity has a rich history, with many applications to areas
such as computability, machine learning, number theory, and computational
complexity. The book of Li & Vit´anyi [LV08] gives an excellent background
on the subject.

Kolmogorov complexity measures the information content inherent in a
string x by the length of the shortest program that computably produces x;
this length is denoted C(x). We think of the program p as a compressed
version of x and the algorithm producing x a decompression procedure. Kol-
mogorov complexity focuses on decompression procedures. In this paper we
turn our attention to the task of producing p given x, i.e., compression.

How hard is it to compute the shortest program p from x? It’s equivalent
to the halting problem since even the set R of random strings (x for which
C(x) ≥ |x|) is hard for the halting problem [Kol65, Cha66, Sol64a, Sol64b].
If we are given the value C(x) then we can compute a program p of that
length by dovetailing through all programs of that length until we ﬁnd one
the produces x. While this method works on a single string, it does not work
in general and won’t even halt if we are given an underestimate of the length.
Consider the lossless compression algorithms in use today. Among the
general-purpose compression algorithms are Huﬀman coding, run-length en-
coding, Burrows-Wheeler, and Lempel-Ziv (and variants thereof) [Wik17].
All of these algorithms have a common property, for any input x they will
produce an output that will decompress back to x.

To this end we deﬁne compression functions as having this single property.
A compression function is a computable function q such that U(q(x)) = x
for all strings x, for some ﬁxed universal Turing machine U. The trivial
compression function q(x) simply outputs the program “Print x”.

A compression algorithm may beneﬁt from having information embed-
ded in it; for example, a Huﬀman encoding tree (of letters, digrams, or
even trigrams) to Huﬀman compress English text. Therefore, the size of a
compression algorithm itself may improve its performance—at least on some
strings.

We investigate a basic trade-oﬀ between the size of a general-purpose

compression algorithm and how well it compresses.

Our main result shows roughly that for any value of m, there is a com-
pression function q of size roughly m that will fully compress any string of
Kolmogorov complexity at most m. We need only m bits for q even though

2

there are exponentially many (in m) strings that q can fully compress.

We also show an essentially tight strong inverse: roughly, that for any n
and m with m < n there is a single string x of length n and Kolmogorov
complexity m such that every compression function q of size less than m fails
to compress x at all.

We further consider polynomial-time bounded versions of compression
complexity whose results depend on unproven assumptions in computational
complexity. If P = NP, then one can have perfect eﬃcient polynomial-time
compression functions; on the other hand, even a small compression would
break one-way functions.

Finally we explore some future directions for research into compression

complexity.

1.1 Main Results

Our ﬁrst theorem says that for any m there is a compression function of size
about m that optimally compresses all strings x with C(x) ≤ m and does
not signiﬁcantly expand other strings.

In what follows, k is a suﬃciently large ﬁxed constant independent of m

or x. See the next section for detailed deﬁnitions.

Theorem 1 For every m, there exists a compression function q with |q| ≤
m + k such that, for all strings x,

1. |q(x)| = C(x) if C(x) ≤ m, and

2. |q(x)| ≤ |x| + k otherwise.

Corollary 2 For every m, there exists a compression function q with |q| ≤
m + k such that, for all x with |x| ≥ m,

|q(x)| − C(x) ≤ |x| − m + k.

The next theorem implies that Theorem 1 and is corollary are essentially
It says for any n ≥ m that if a compression function q for length
tight.
n (i.e., one that only needs to work on strings of length n) is signiﬁcantly
shorter than m, then it behaves poorly on at least one string: there is an x
of length n—independent of q—such that C(x) is about m but q does not
compress x at all.

3

Theorem 3 For all m, n with 0 ≤ m ≤ n there exists an x such that

1. |x| = n,

2. C(x) ≤ m + k log n,

3. For all compression functions q for length n, with |q| ≤ m − k log n,

|q(x)| ≥ n, and

4. For all compression functions q for length n, with |q| ≤ m − k log n,

|q(x)| − C(x) ≥ n − m − k log n.

Note that (4) follows from (2) and (3). Compare (4) with the result of
Corollary 2.

We prove Theorems 1 and 3 in Section 3. In Section 4 we give deﬁnitions

and results for time-bounded compression.

2 Preliminaries

Our notation and basic deﬁnitions are standard. Here we brieﬂy outline the
basic deﬁnitions of Kolmogorov complexity. For an in-depth treatment, see
the standard textbook on the subject by Li & Vit´anyi [LV08].

We assume all strings are over the binary alphabet {0, 1}. We deﬁne a
standard pairing function h·, ·i injectively mapping pairs of strings to strings
as

hx, yi := 1|x|0xy

for all strings x and y.

Fix a universal machine U suitable for deﬁning Kolmogorov complexity.
It suﬃces to choose U such that, for any Turing machine M, there exists
a string p such that M(x) = U(hp, xi) for any string x such that M(x) is
deﬁned. Abusing notation, we also write p(x) for U(hp, xi) and call p a
“program.”

Deﬁne C(x) as the length of the shortest p such that U(p) = x.
Both our main results freely refer to a constant k. We can (and do) choose

k large enough (depending only on U) such that our results hold.

Deﬁnition 4 Let n be a natural number. A compression function for length
n is a program q such that U(q(z)) = z for all strings z of length n. A
compression function is a program q that is a compression function for all
lengths.

4

Note that any compression function q is total, and for all x, q(x) ≥ C(x).

Let BB(m) (“Busy Beaver of m”) be the maximum time for U(p) to
halt over all halting programs p of length at most m. Let pm be the lexico-
graphically least program of length at most m that achieves BB(m) running
time.

3 Proof of the Main Results

In this section we give proofs of Theorems 1 and 3.

Proof of Theorem 1:
Given m, we deﬁne q(z) as follows:

• Let t be the number of steps used by U(pm) before halting. Note that

t = BB(m).

• Look for the lexicographically shortest program p of length at most |z|

such that U(p) = z within t steps.

– If p is found, then output p.

– Otherwise, output “Print z.”

We hardwire the value of pm into the code for q (e.g., let q = hr, pmi for some
program r independent of m), so |q| ≤ m + k.

Fix a string x.
Case 1: C(x) ≤ m.
Let p be the lexicographically ﬁrst program of length C(x) such that
U(p) = x. By the deﬁnition of BB(m), U(p) will halt in at most t steps. So
we’ll have q(x) = p and |q(x)| = C(x).

Case 2: C(x) > m.
Either q(x) outputs “Print x” or a program of length at most n. Either
(cid:3)

way |q(x)| ≤ n + k.

Proof of Theorem 3:
Let Aℓ

s be the set of strings y of length ℓ such that there is no program
p with |p| < ℓ such that U(p) outputs y within s steps. One can compute
a canonical index for Aℓ
s given s and ℓ as input. Aℓ
s contains the random
strings of length ℓ, so in particularly Aℓ
s is never empty. If s ≥ BB(ℓ), then
Aℓ

s is exactly the set of random strings of length ℓ.

5

Given m ≤ n, let t = BB(m) and x be the lexicographically ﬁrst string
in An
t .
Note that C(x) ≤ m + k log n, since we can describe x by pm, m, and n,

using pm to ﬁnd t.

Suppose there is a compression function q for length n with |q| ≤ m −

k log n such that |q(x)| < n.

Let z be the lexicographically ﬁrst random string of length m. We show
how to use q, n, and m to ﬁnd z. This will yield a contradiction to the fact
that z is random.

Let t′ be the maximum over all y, |y| = n, of the number of steps required
for U to halt on input q(y). Here we use the fact that U(q(y)) = y for all y
of length n.

Since |q(x)| < n and U(q(x)) = x, by the deﬁnition of x the number of
steps ˆt required for U to halt on input q(x) must be greater than t = BB(m).
So we have t′ ≥ ˆt > t = BB(m). We can’t necessarily compute t or ˆt from
just q, n, and m, but we can compute t′.

Now compute Am

t′ , which will be exactly the random strings of length m,

and z will be the lexicographically least string in that set.

(cid:3)

4 Time-Bounded Compression Complexity

The proofs in Section 3 create machines that run in time based on the busy-
beaver function, which grows faster than any computable function. Practical
compression and decompression algorithms need to be far more eﬃcient. We
explore a polynomial time-bounded version of compression complexity in this
section, though, not too surprisingly, the results we get will depend on open
questions in computational complexity.

Time bounds can play into both the compression and decompression pro-

cedures.

Deﬁnition 5 An (f, g)-time bounded compression function is a function q
such that for all strings x,

1. q(x) halts within f (|x|) steps.

2. U(q(x)) halts and outputs x within g(|x|) steps.

6

Let C t(x) be the length of the shortest program p such that U(p) outputs
|q(x)| ≥

x within t(|x|) steps. For every (f, g)-compression function q,
C g(x) ≥ C(x).

Typically we consider f and g as polynomials. For a ﬁxed polynomial p
we can easily compute the smallest p-time bounded program for x in FPNP
.
As a corollary we get

Proposition 6 If P = NP then for every polynomial p there is a polynomial
p′ and a (p′, p)-compression function q such that |q(x)| = C p(x).

So in particular, to show that we don’t have perfect eﬃcient compression
would require settling the P versus NP problem.

On the other hand, we show that if we can have short compression pro-
grams that more than trivially compress all strings that have signiﬁcant
compressions, `a la Theorem 1 and Corollary 2, then we can break one-way
functions.

First we need to deﬁne a family of compression functions.

Deﬁnition 7 A family of (f, g)-time bounded compression functions is an
enumeration of programs q0, q1, . . . such that for all n and all strings x of
length n,

1. qn(x) halts within f (n) steps.

2. U(qn(x)) halts and outputs x within g(n) steps.

We say a compression family has polynomial-size if for some constant c,
|qn| ≤ nc for all n ≥ 2. A compression family is uniform if there is a
polynomial-time algorithm Q such that Q(1n) = qn for all n.

Theorem 8 Fix a constant δ with 0 < δ < 1. Suppose that for any poly-
nomial p′ there is a polynomial p and a polynomial-size family of (p, p′)-
compression functions q0, q1, . . . such that for all n and x ∈ Σn with C p′
(x) ≤
nδ, we have |qn(x)| < n. Then one-way functions do not exist relative to
polynomial-size circuits.

In particular, we could factor numbers on average with polynomial-size
circuits. Under the additional assumption that the family is uniform we can
factor numbers on average with a polynomial-time algorithm.

7

Proof: Let’s assume we have a one-way function f . H˚astad, Impagliazzo,
Levin and Luby [HILL99] show how to convert this one-way function into
a polynomial-time pseudorandom generator G : Σnǫ
→ Σn (for any ﬁxed
ǫ > 0) so that no polynomial-size circuit can distinguish the output of G on
a random seed from a truly uniformly chosen string of length n.

Choose an ǫ so that 0 < ǫ < δ. Pick p′(n) larger than the running time
of G and let q0, q1, . . . be the family of (p, p′)-compression functions given in
the assumptions of Theorem 8.

Consider the following test T (x) that can be expressed as a polynomial-

sized circuit: Output 0 if |qn(x)| < n, and output 1 otherwise.

Buhrman, Jiang, Li and Vit´anyi [BJLV00] show that a constant fraction
of the strings of length n have C(x) ≥ n. Since |qn(x)| ≥ C(x), if we choose
a string x at random, then T (x) will output 1 with probability at least some
constant α > 0.

Suppose x is the output of G(r) for some r. We can describe x by the
code for G and r, or nǫ + O(1) ≤ nδ bits for suﬃciently large n. Since the
running time of G is less than p′, we have |C p′
(x)| ≤ nδ. By the assumptions
of Theorem 8 we have |q(x)| < n and T (x) = 0.

T (x) will output 1 with probability at least α when x is chosen at random
and with probability 0 when x is the output of G on a randomly chosen seed.
This contradicts the fact that G is a pseudorandom generator and f is a
one-way function.

In particular, the function that maps two primes to their product is not
one-way, so we can factor randomly chosen numbers using polynomial-size
circuits.

If the family of compression functions is uniform, then T (x) above can be
expressed as a polynomial-time algorithm. H˚astad, Impagliazzo, Levin and
Luby [HILL99] show how to take any one-way function against polynomial-
time algorithms and convert it to a pseudorandom generator that has no
uniform tests. Putting this together there must be a polynomial-time algo-
(cid:3)
rithm that factors randomly chosen numbers.

5 Future Directions

This work is just the start of compression complexity.

Is there a compression analogue of conditional Kolmogorov complexity
C(x|y)? The results of Section 1.1 should go through if we allow q to have

8

access to y, but it is less clear what happens if q does not have access to y.
Symmetry of Information shows that for any strings x and y, C(x, y) ≤
C(x)+C(y|x)+O(log(|x|+|y|)). Suppose we consider compression functions
q(x, y) to produce programs p1 and p2 such that U(p1) = x and U(hp2, xi) =
y. How short can we get |p1| + |p2| compared to C(x, y)?

There are several other notions of time-bounded Kolmogorov complexity
(see [LV08]) such as distinguishing complexity, where we need only distin-
guish a string x from other strings. Do the results of Section 4 still apply?

One could also consider lossy compression, perhaps building on Kolmo-

gorov complexity with errors [FLV06].

Acknowledgments

We would like to thank Eric Allender, Saurabh Sawlani, and Jason Teutsch
for helpful discussions.

References

[BJLV00] Harry Buhrman, Tao Jiang, Ming Li, and Paul Vit´anyi. New
applications of the incompressibility method: Part II. Theoretical
Computer Science, 235(1):59 – 70, 2000.

[Cha66] Gregory J. Chaitin. On the length of programs for computing

ﬁnite binary sequences. J. ACM, 13(4):547–569, October 1966.

[FLV06] Lance Fortnow, Troy Lee, and Nikolai Vereshchagin. Kolmogorov
Complexity with Error, pages 137–148. Springer Berlin Heidelberg,
Berlin, Heidelberg, 2006.

[HILL99] Johan H˚astad, Russell Impagliazzo, Leonid A. Levin, and Michael
Luby. A pseudorandom generator from any one-way function.
SIAM Journal on Computing, 28(4):1364–1396, 1999.

[Kol65] Andrei Kolmogorov. Three approaches for deﬁning the concept
of information quantity. Problems of Information Transmission,
1:1–7, 1965.

[LV08] Ming Li and Paul Vit´anyi. An Introduction to Kolmogorov Com-

plexity and Its Applications. New York, third edition, 2008.

9

[Sol64a] R.J. Solomonoﬀ. A formal theory of inductive inference. Part I.

Information and Control, 7(1):1–22, 1964.

[Sol64b] R.J. Solomonoﬀ. A formal theory of inductive inference. Part II.

Information and Control, 7(2):224–254, 1964.

[Wik17] Wikipedia. Lossless compression — Wikipedia, the free encyclo-

pedia, 2017. [Online; accessed 09-February-2017].

10

