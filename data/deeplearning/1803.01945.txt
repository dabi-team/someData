M 3F usion: A Deep Learning Architecture for
Multi-{Scale/Modal/Temporal} satellite data fusion

P. Benedetti, D. Ienco, R. Gaetano, K. Os´e, R. Pensa and S. Dupuy

Abstract

Modern Earth Observation systems provide sensing data at different temporal and spatial resolutions. Among optical sensors,
today the Sentinel-2 program supplies high-resolution temporal (every 5 days) and high spatial resolution (10m) images that
can be useful to monitor land cover dynamics. On the other hand, Very High Spatial Resolution images (VHSR) are still an
essential tool to ﬁgure out land cover mapping characterized by ﬁne spatial patterns. Understand how to efﬁciently leverage these
complementary sources of information together to deal with land cover mapping is still challenging.

With the aim to tackle land cover mapping through the fusion of multi-temporal High Spatial Resolution and Very High
Spatial Resolution satellite images, we propose an End-to-End Deep Learning framework, named M 3F usion, able to leverage
simultaneously the temporal knowledge contained in time series data as well as the ﬁne spatial information available in VHSR
information. Experiments carried out on the Reunion Island study area asses the quality of our proposal considering both
quantitative and qualitative aspects.

Land Cover Mapping, Data Fusion, Deep Learning, Satellite Image Time series, Very High Spatial Resolution, Sentinel-2.

Index Terms

I. INTRODUCTION

Modern Earth Observation systems produce huge volumes of data every day. This information can be organized into time

series of high-resolution satellite imagery (SITS) (i. e. Sentinel) that are useful for area monitoring over time.

In addition to this high temporal frequency information, we can also obtain Very High Spatial Resolution (VHSR) information,

such as Spot6/7 or Pleiades imaging, with a more limited temporal frequency [1] (e. g. once a year).

The analysis of time series and its coupling/fusion with punctual VHSR data remains an important challenge in the ﬁeld of

remote sensing. [2], [3].

In the context of land use classiﬁcation, employing high spatial resolution (HSR) time series, instead of a single image of
the same resolution, can be useful to distinguish classes according to their temporal proﬁles [4]. On the other hand, the use of
ﬁne spatial information helps to differentiate other kind of classes that need spatial context information at higher scale [3].

Typically, the approaches that use these two types of information [5], [6], perform data fusion at descriptor level [3]. This
type of fusion involves extracting a set of independent features for each data source (time series, VHSR image) and then
stacking these features together to feed a traditional supervised learning method (i. e., Random Forest).

Recently, the deep learning revolution [7] has shown that neural network models are well adapted tools for automatically
managing and classifying remote sensing data [7]. The main characteristic of this type of model is the ability to simultaneously
extract features optimized to image classiﬁcation and the associated classiﬁer. This advantage is fundamental in a data fusion
process such as the one involving high resolution time series (i. e. Sentinel-2) and VHSR data (i. e. Spot6/7 and/or Pleiades).
Considering deep learning methods, we can ﬁnd two main families of approaches: convolutional neural networks [7] (CNN)
and recurrent neural networks [8] (RNN). CNN are well suited to model the spatial autocorrelation available in an image,
while RNN networks are especially tailored to manage time dependencies [9] from multidimensional time series.

In this article, we propose to leverage both CNN and RNN to address the fusion problem between an HSR time series of
Sentinel-2 images and a VHSR image on the same study area with the goal to perform land use mapping. The method we
propose, named M 3F usion (Multi-Scale/Modal/Temporal Fusion), consists in a deep learning architecture that integrates both
a CNN component (to manage VHSR information) and an RNN component (to analyze HSR time series information) in an
end-to-end learning process. Each information source is integrated through its dedicated module and the extracted descriptors
are then concatenated to perform the ﬁnal classiﬁcation. Setting up such a process, which takes both data sources into account
at the same time, ensures that we can extract complementary and useful features for land use mapping.

To validate our approach, we conducted experiments on a data set involving the Reunion Island study site. This site is a
French Overseas Department located in the Indian Ocean (east of Madagascar) and it will be described in Section II. The rest
of the article is organized as follows: Section III introduces the M 3F usionDeep Learning Architecture for the multi-source
classiﬁcation process. The experimental setting and the ﬁndings are discussed in Section IV and conclusions are drawn in
Section V.

8
1
0
2

r
a

M
5

]

V
C
.
s
c
[

1
v
5
4
9
1
0
.
3
0
8
1
:
v
i
X
r
a

 
 
 
 
 
 
Figure 1: Visual representation of M 3F usion.

II. DATA

The study was carried out on Reunion Island, a French overseas department located in the Indian Ocean. The dataset
consists of a time series of 34 Sentinel-2 (S2) images acquired between April 2016 and May 2017, as well as a very high
spatial resolution image (VHSR) SPOT6/7 acquired in April 2016 and covering the whole island. The S2 images used are
those provided at level 2A by the Continental Surfaces pole THEIA1, where the bands at 20 m resolution were resampled to
10 m. A preprocessing was performed to ﬁll cloudy observations through a linear multi-temporal interpolation over each band
(cfr. Temporal Gapﬁlling, [5]), and six radiometric indices were calculated for each date (NDVI, NDWI, brightness index -
BI, NDVI and NDWI of infrared means - MNDVI and MNDWI, and vegetation index Red-Edge - RNDVI) [5], [6]). A total
of 16 variables (10 surface reﬂectances plus 6 indices) are considered for each pixel of each image in the time series.

The SPOT6/7 image, originally consisting of a 1.5 m panchromatic band and 4 multispectral bands (blue, green, red and
near infrared) at 6 m resolution, was merged to produce a single multispectral image at 1.5 m resolution and then resampled at
2 m because of the network architecture learning requirements.2. Its ﬁnal size is 33280 × 29565 pixels on 5 bands (4 reﬂectors
Top of Atmosphere plus the NDVI). This image was also used as a reference to realign the different images in the time series
by a searching and mapping anchor points, in order to improve the spatial coherence between the different sources.

The ﬁeld database was built from various sources: (i) the graphical parcel register (RPG) data set of 2014, (ii) GPS records
from June 2017 and (iii) photo interpretation of the VHSR image conducted by an expert, with knowledge of the territory, for
natural and urban spaces . All polygon contours have been resumed using the VHSR image as a reference. The ﬁnal dataset
includes a total of 322 748 pixels (2 656 objects) distributed over 13 classes, as indicated in the Table I.

Class
0
1
2
3
4
5
6
7
8
9
10
11
12

Label
Crop Cultivations
Sugar cane
Orchards
Forest plantations
Meadow
Forest
Shrubby savannah
Herbaceous savannah
Bare rocks
Urbanized areas
Greenhouse crops
Water Surfaces
Shadows

# Objects
380
496
299
67
257
292
371
78
107
125
50
96
38

# Pixels
12090
84136
15477
9783
50596
55108
20287
5978
18659
36178
1877
7349
5230

Table I: Characteristics of the Reunion Dataset

RNNCNNFeature FusionFusion ClassiﬁerRNN Auxiliary ClassiﬁerCNN Auxiliary ClassiﬁerSentinel-2 Time Series at High Spatial Resolution25 x 25  patch extracted from Spot 6/7 VHSR imageT2T1T3TnA. M 3F usion model description

III. CONTRIBUTIONS

Figure 3 visually describes the M 3F usion approach proposed in this article. First of all, we deﬁne the input data for our
deep learning model. M 3F usion takes as input a data {(xi, yi)}M
i=1 where each example is associated with a class value
yi ∈ 1, ..., C. An example xi is deﬁned as a pair xi = (tsi, patchi) such that tsi is the (multidimensional) time series of a
Sentinel-2 pixel (10m resolution) and patchi is a subset of the image Spot6/7 (2m resolution) centered around the corresponding
pixel Sentinel-2. Every example has two representation: a temporal HSR modality (provided by the Sentinel-2 time series) and
a VHSR modality (provided by the Spot6/7 image).

For every patchi, we ﬁxed the window size to 25×25 pixels on the Spot6/7 (which correspond to a window size 5x5 on a

Sentinel-2 image) centered around a Sentinel-2 pixel described by the corresponding tsi.

In order to merge the temporal information with the VHSR one contained in the Spot 6/7 image, we designed a network
which has two parallel branches, one for each of the two modes (spatial/temporal). For the time series concerning the Sentinel-2
pixel we use a Recurrent Neural Network (RNN) architecture. In particular, we used a Gated Recurrent Unit (GRU) introduced
in [11] which has already demonstrated its effectiveness in the remote sensing ﬁeld [12], [13]. On the other hand, the spatial
information, with a scale of 2m, introduced through VHSR image is integrated through the use of a Convolutional Neural
Network [1] which allows to extract spatial context knowledge around the Sentinel-2 pixel.

Via the two streams of analysis we learn two complementary groups of features that we successively leverage for the
classiﬁcation that is performed at the scale of the Sentinel-2 pixel. According to the philosophy introduced in [14], the proposed
architecture aims to learn two sets of complementary features (thanks to the different spatial and temporal modalities) that are
as much as possible discriminative when used alone. To ensure this last point, the strategy provides in [14] introduces two
additional auxiliary classiﬁers, working independently on each group of features, as shown in the Figure 3. A third classiﬁer,
working on the fusion (by concatenation) of the two sets of features, perform the ﬁnal land use mapping.

Each of the above mentioned classiﬁers is realized by directly connecting the associated features to the output neurons
on which SoftMax activation function is successively applied [7]. The model weights are learned by back-propagation. The
cost-function associated to the model is derived by a linear combination of the individual cost function of each of the classiﬁers.

B. Integration of information from the HSR time series

Recently, recurrent neural network (RNN) approaches demonstrated their quality in the remote sensing ﬁeld to produce
land use mapping using time series of optical images [9] and recognize vegetation cover status using Sentinel-1 radar time
series [13]. Motivated by these recent works, we decided to introduce an RNN module to integrate information from the
Sentinel-2 time series into our fusion process. In our model we chose the GRU unit (Gated Recurrent Unit) introduced by [11],
coupled with an attention mechanism [15]. Attention mechanisms are widely used in automatic signal processing (language
or 1D signal) and they allow to combine together the information extracted by the GRU model at the different timestamps.
The input of a GRU unit is a sequence (xt1,..., xtN ) where a generic element xti is a multidimensional vector and ti refers
to the corresponding date in the time series. The output returned by the GRU model is a sequence of feature vectors learned
for each date: (ht1,..., htN ) where each hti has the same dimension d. Their matrix representation H ∈ RN,d is obtained
vertically stacking the set of vectors. The attention mechanism allows to combine together these different vectors hti, in a
single one rnnf eat, to better combine the information returned by the GRU unit at each of the different timestamps. The
attention formulation we used, from a vector sequence of the learned descriptors (ht1 ,..., htN ), is the following one:

va = tanh(H · Wa + ba)
λ = Sof tM ax(va · ua)

rnnf eat =

N
(cid:88)

i=1

λi · hti

(1)

(2)

(3)

Matrix Wa ∈ Rd,d and vectors ba, ua ∈ Rd are parameters learned during the process. These parameters allow to combine
the vectors contained in matrix H. The purpose of this procedure is to learn a set of weights (λt1,..., λtN ) that allows the
contribution of each timestamp to be weighted hti through a linear combination. The Sof tM ax(·) [9] function is used to
normalize weights λ so that their sum is equal to 1. The output of the RNN module is the feature vector rnnf eat, these features
encode temporal information related to tsi for the pixel i.

1Donnes disponibles via http://theia.cnes.fr, pre-treated in surface reﬂectance via the MACCS-ATCOR Joint Algorithm [10] developed by the National Centre

for Space Studies (CNES).

2This was done to ensure a direct and non-overlapping correspondence between the time series pixels (10 m) and a block of VHSR pixels (5 × 5).

C.

Integration of VHSR information

The VHSR information is integrated in M 3F usion through a CNN module. Computer vision literature offers several
convolutional architectures [16], [17]. Most of these networks are designed to process RGB images (three channels) having
size higher then 200x200. Such networks are composed by multiple layers ( tens or hundreds ). In our scenario, the image
patches to analyze have a size of 25x25 and they are described by ﬁve channels. In order to propose a CNN module that well
ﬁt our scenario and remains computational affordable parameters-wise, we design the CNN module depicted in Figure 2.

Figure 2: Convolutional Neural Network Structure

Our CNN network applies a ﬁrst kernel 7×7 to the ﬁve-channel patch to produce 256 feature maps. Then, a max pooling
layer is used to reduce the size and the number of parameters. Two successive convolution operations with a kernel 3×3 extract
512 features maps each, which are then concatenated and reduced again by a convolution kernel 1×1 (ﬁnal size 512×7×7).
Finally, a Global Average Pooling operation allows to produce a vector of features with size equals to 512.

Each convolution is associated with a linear ﬁlter, followed by a Rectiﬁer Linear Unit activation function [18] to introduce
non-linearity and a Batch Normalization step [19]. The key points of our proposal are twofold: a) a higher number of ﬁlters in
the ﬁrst step and b) the concatenation of features maps at different resolutions. The ﬁrst point is related to the higher amount
of spectral input information (ﬁve channels) compared to RGB images. To better exploit the high spectral richness of this data,
we have increased the number of feature maps generated at this stage. The second point concerns the concatenation of features
maps. With the goal to exploit information at different resolutions we adopt a philosophy similar to [17]. The output of this
module is a vector of dimensions 512 (cnnf eat) which summarizes the spatial context (patchi) associated to the Sentinel-2
pixel i.

D. Merging descriptors through an End-To-End process

One of the advantages of deep learning, compared to standard machine learning methods, is the ability to link, in a single
pipeline, the feature extraction step and the associated classiﬁer [7]. This quality is particularly important in a multi-source,
multi-scale and multi-temporal fusion process. M 3F usion leverages this asset to be able to extract complementary descriptors
from two sources of information that describe the same pixel from different viewpoints. In order to further strengthen the
complementarity and then, the discriminative power of the learned features for each information stream, we adapted the
technique proposedin [14] to our problem. In [14], the authors proposed to learn two complementary representations (using
two convolutional networks) of the same image. The discriminative power is enhanced by two auxiliary classiﬁers, linked to
each group of features, in addition to the classiﬁer that uses the merged information through a sum operation. In our case, we
have two complementary sources of information (sentinel-2 time series and VHSR data) to which two auxiliary classiﬁers are
attached in order to independently increase their ability to recognize land cover classes. Regarding the classiﬁer that exploits
the full set of features, we feed it concatenating the output features of both CNN (cnnf eat) and RNN (rnnf eat) module
together. The learning process will involve optimizing three classiﬁers at the same time, one speciﬁc to rnnf eat, a second one
related to cnnf eat and the third one that consider [rnnf eat, cnnf eat].

5 @25x25256 @19x19256 @9x9512 @7x7512 @7x71024 @7x7512 @7x7512@1GlobalAvg PoolingConv(512, 1x1)Conv(512, 3x3)Conv(512, 3x3)MaxPooling( 2x2,s=2)Conv(256, 7x7)ConcatThe cost function associated to our model is :

Ltotal = α1 ∗ L1(rnnf eat, W1, b1)+
= α2 ∗ L2(cnnf eat, W2, b2)+
= Lf us([cnnf eat, rnnf eat], W3, b3)

where

Li(f eat, Wi, bi) = Li(Y, Sof tM ax(f eat · Wi + bi))

(1)

(2)

(3)

(4)

Y is the true value of the class variable. L1(rnnf eat, W1, b1) (resp. L2(cnnf eat, W2, b2)) is the cost function of the ﬁrst
(resp. the second) auxiliary classiﬁer which takes as input the set of descriptors returned by a speciﬁc module (CNN or RNN)
and the parameters to make the prediction (W1, b1 or W2, b2). Lf us(cnnf eat, rnnf eat, W3, b3) is the cost function of the
classiﬁer that uses the total set of features ([cnnf eat, rnnf eat]). This last cost function is parameterized through W3 et b3.
Each of the cost function is modeled through categorical cross entropy, a typical choice for multi-class supervised classiﬁcation
tasks [9].

Ltotal is optimized End-To-End. Once the network has been trained, the prediction is carried out using only the classiﬁer
involving W3 and b3 which uses all the features learned by the two branches. The cost functions L1 et L2, as highlighted in [14],
operate a kind of regularization that forces, within the network, the extracted features to be discriminative independently.

In this section, we present the experimental setting we used and we discuss the results obtained on the data introduced in

IV. EXPERIMENTS

Section II.

A. Experimental Setting

We compare the performances of the M 3F usion approach w.r.t the Random Forest classiﬁer (RF), which is commonly

used for supervised classiﬁcation in the ﬁeld of remote sensing [6].

For the RF model, we ﬁxed the number of generated random trees at 200 with no depth limits imposed. For the Random
Forest method, we used the python implementation supplied by the scikit-learn [20] library. In order to compare these two
methods, we supplied the same input data set both to RF and M 3F usion model. Each example of the data set for this
competitor has a size of 3 669 which corresponds to 25 × 25 × 5 (patchi) plus 34 × 16 (tsi).

In our model we choose the value d (number of hidden units in the recurrent unit GRU ) equals to 1 024. We empirically
ﬁxed α1 and α2 to 0.3. During the learning phase, we used the Adam method [21] to learn the model parameters with a
learning rate equal to 2 · 10−4. The training process is conducted over 400 epochs. The best model regarding the cost function’s
value is used in the test phase.

We implemented M 3F usion using the python Tensorﬂow library. The learning phase takes about 15 hours while the
classiﬁcation on the test data takes about one minute on a workstation with an Intel (R) Xeon (R) CPU CPU E5-2667 CPU
v4@3.20Ghz with 256 GB of RAM and TITAN X GPU.

In terms of data, we divided the set into two parts, one for learning and the other to test the performances of the supervised
classiﬁcation methods. We used 30% of the objects for the training phase (meaning 97 110 pixels) while the remaining 70%
are used for the test phase (meaning 225 638 pixels). We impose that pixels of the same object belong exclusively to the train
or to the test set. [5]. The values were normalized in the interval [0, 1] by spectral band.

The assessment of classiﬁcation performance is done by global precision (Accuracy) and F-Measure metrics [9].

B. Quantitative Results

Figure 3 shows the results from both classiﬁcation models in terms of F-Measure per class. We can observe that M 3F usionreaches

average better performances compared to the RF. The only exception is supplied by class (12) where performance are more
than comparable and RF obtains slightly better results. Considering classes (1),(3),(4),(5),(7),(8) and (9), which we can consider
more difﬁcult to manage since absolute performances are low, the behavior of the deep learning method is always superior to
the one exhibited by RF.

A class where we can notice a sensible enhancement is the class (10) Greenhouse crops. For an F-Measure of 0.25 given
by RF, M 3F usion achieves an F-Measure of 0.58. Indeed, both the appearance and dynamics of the elements in this class
are very similar to those of the built-up class. Probably, the spatial information extracted by the deep approach makes possible
to derive more discriminative characteristics.

Regarding the Accuracy results, M 3F usion (resp. RF) reach a score rate of 90.67% (resp. 87.39%). For a more detailed
analysis, we show in Figure 4a (resp. Figure 4b) the heat map associated with the method’s confusion matrix RF (resp.
M 3F usion). We can observe that the heat map gives a good overview on the behavior of the two methods. First of all, we can
observe that the Random Forest is noisier, especially on the diagonal. This noise locates the classiﬁer’s errors in its decision.

Figure 3: F-Measure by class of the two classiﬁcation methods.

This behavior is particularly evident in class (10) where the majority of the elements of this class are categorized as class (9).
About the M 3F usion method, we can observe a more coherent structure along the diagonal of the heat map, which indicate
less noise. Talking about class (10), also our method tends to make some confusion between classes (9) and (10), but this
phenomenon is less evident in relation to the Random Forest method since most of the elements are better classiﬁed.

The results presented so far are linked to a single split 30%/70% of our data set. It is known that, depending on the split of
the data, the performances of the different methods may vary because simpler or more difﬁcult examples can be involved in
the train or test set. With the objective of a ﬁrst understanding of the robustness of our method, regarding this phenomenon,
we produced four other splits of the dataset, using the same protocol. The results of the ﬁve splits are shown in the Table II.
We can observe that the behavior of the two methods, in relation to the different splits, is similar: both methods obtain the
best performance on the second split and the worst results on the third split. This behavior is related to the phenomenon we
mentioned earlier. On the other hand, we can see that M 3F usion always gets the best results on all splits with an Accuracy
gain (resp. en F-Measure) varying between 2.28 and 4.04 (resp. 2.65 and 4.53). We can highlight two more important facts,
M 3F usion seems to be more stable than the Random Forest method. We observe this behavior on split number 3 where the
performance of the propositional classiﬁer decreases by more than 3 points compared to its best result. Considering M 3F usion,
the difference between the best and worst score is around 2 points. Finally, we can note that for the results relating to the deep
learning method, the difference between the Accuracy value and the F-Measure value is minimal, the two values are always
rather similar and aligned. On the contrary, for the Random Forest method we can see some discrepancy between the two
measures. Accuracy measurement is always about half a point higher. Looking closely to the results, we found that RF seems
to be more inﬂuenced by the class imbalance giving more chance in its decision to the majority classes.

C. Qualitative Results

In addition to the numerical evaluations reported in the previous section, we also propose a ﬁrst qualitative evaluation of
the map produced by M 3F usion. The map obtained by the M 3F usion is also shown in Figure 5 for a qualitative overview.
The recognition of the majority classes, i. e. the areas cultivated with sugar cane on the coast, as well as the various degraded

(a)

(b)

Figure 4: Confusion matrix of Random Forest (a) and M 3F usion (b).

Essai

RF

M 3F usion

Gain

Acc.
87.39
88.47
85.21
88.33
87.29

F-Meas.
87.11
88.05
84.62
88.05
86.88

Acc.
90.67
91.52
89.25
90.61
90.09

F-Meas.
90.67
91.39
89.15
90.7
89.96

Acc.
+3.28
+3.05
+4.04
+2.28
+2.8

F-Meas.
+3.56
+3.34
+4.53
+2.65
+3.08

1
2
3
4
5

Table II: Accuracy and F-Measure of the two methods on ﬁve different random splits of the data set

natural areas (grasslands, savannas and forests) and the urban fabric, seem to be well localized and regular, with a less important
presence of noise than on the map obtained by Random Forest (not reported for brevity).

Some comparisons between the two maps are provided at the scale of some remarkable details in Figure 6: in the line above,
a fragment of urban areas is displayed, where the presence of noise is particularly marked for the RF map (in the middle),
as shown by the transition zones between buildings that are often interpreted as market gardening, an effect that does not
occur on the M 3F usionmap (right). A particular interesting effect concerns the artifacts of the RF map due to the presence
of clouds or shadows (detail on the bottom line) on the VHSR image, which are not produced with the proposed method:
this is probably due to an RF bias in favor of information from the VHSR, a situation that does not occur with the proposed
approach.

V. CONCLUSIONS

In this article, we proposed a new deep learning architecture for the fusion of satellite data with high temporal/spatial
resolution and very high spatial resolution to perform land use mapping. Experiments carried out on a real study site validate
the quality and the results of our approach compared to the ones obtained by a common machine learning methods usually
employed in the ﬁeld of remote sensing for the same task. In the future, we plan to study the extension of our architecture to
take into account other complementary data sources.

This work was supported by the French National Research Agency under the Investments for the Future Program, referred as
ANR-16-CONV-0004 and the GEOSUD project with reference ANR-10-EQPX-20, as well as from the ﬁnancial contribution
from the Ministry of Agriculture’s ”Agricultural and Rural Development” trust account. This work also used an image acquired
under the CNES Kalideos scheme (La R´eunion site).

VI. ACKNOWLEDGEMENTS

Figure 5: Map produced by M 3F usion

REFERENCES

[1] E. Maggiori, Y. Tarabalka, G. Charpiat, and P. Alliez, “Convolutional neural networks for large-scale remote-sensing image classiﬁcation,” IEEE TGRS,

vol. 55, no. 2, pp. 645–657, 2017.

[2] A. Karpatne, Z. Jiang, R. R. Vatsavai, S. Shekhar, and V. Kumar, “Monitoring land-cover changes: A machine-learning perspective,” IEEE Geoscience

and Remote Sensing Magazine, vol. 4, pp. 8–21, 2016.

[3] M. Schmitt and X. X. Zhu, “Data fusion and remote sensing: An ever-growing relationship,” IEEE Geoscience and Remote Sensing Magazine, vol. 4,

no. 4, pp. 6–23, 2016.

[4] N. A. Abade, O. A. d. C. Jnior, R. F. Guimares, and S. N. de Oliveira, “Comparative analysis of modis time-series classiﬁcation using support vector
machines and methods based upon distance and similarity measures in the brazilian cerrado-caatinga boundary,” Remote Sensing, vol. 7, no. 9, pp.
12 160–12 191, 2015.

[5] J. Inglada, A. Vincent, M. Arias, B. Tardy, D. Morin, and I. Rodes, “Operational high resolution land cover map production at the country scale using

satellite image time series,” Remote Sensing, vol. 9, no. 1, p. 95, 2017.

[6] V. Lebourgeois, S. Dupuy, E. Vintrou, M. Ameline, S. Butler, and A. B´egu´e, “A combined random forest and OBIA classiﬁcation scheme for mapping
smallholder agriculture at different nomenclature levels using multisource data (simulated sentinel-2 time series, VHRS and DEM),” Remote Sensing,
vol. 9, no. 3, p. 259, 2017.

[7] L. Zhang and B. Du, “Deep learning for remote sensing data: A technical tutorial on the state of the art,” IEEE Geoscience and Remote Sensing

Magazine, vol. 4, pp. 22–40, 2016.

[8] Y. Bengio, A. C. Courville, and P. Vincent, “Representation learning: A review and new perspectives,” IEEE TPAMI, vol. 35, no. 8, pp. 1798–1828,

2013.

[9] D. Ienco, R. Gaetano, C. Dupaquier, and P. Maurel, “Land cover classiﬁcation via multitemporal spatial data by deep recurrent neural networks,” IEEE

GRSL, vol. 14, no. 10, pp. 1685–1689, 2017.

[10] O. Hagolle, M. Huc, D. Villa Pascual, and G. Dedieu, “A Multi-Temporal and Multi-Spectral Method to Estimate Aerosol Optical Thickness over Land,
for the Atmospheric Correction of FormoSat-2, LandSat, VENµS and Sentinel-2 Images,” Remote Sensing, vol. 7, no. 3, pp. 2668–2691, 2015.
[11] K. Cho, B. van Merrienboer, C¸ . G¨ulc¸ehre, D. Bahdanau, F. Bougares, H. Schwenk, and Y. Bengio, “Learning phrase representations using RNN

encoder-decoder for statistical machine translation,” in EMNLP, 2014, pp. 1724–1734.

[12] L. Mou, P. Ghamisi, and X. X. Zhu, “Deep recurrent neural networks for hyperspectral image classiﬁcation,” IEEE TGRS, vol. 55, no. 7, pp. 3639–3655,

2017.

[13] D. H. T. Minh, D. Ienco, R. Gaetano, N. Lalande, E. Ndikumana, F. Osman, and P. Maurel, “Deep recurrent neural networks for winter vegetation

quality mapping via multitemporal sar sentinel-1,” IEEE GRSL, vol. Preprint, no. -, pp. –, 2018.

[14] S. Hou, X. Liu, and Z. Wang, “Dualnet: Learn complementary features for image recognition,” in IEEE ICCV, 2017, pp. 502–510.
[15] D. Britz, M. Y. Guan, and M. Luong, “Efﬁcient attention using a ﬁxed-size memory representation,” in EMNLP, 2017, pp. 392–400.

Figure 6: Classiﬁcation results obtained with Random Forest and M 3F usion: right to left, excerpt from SPOT6/ image7,
classiﬁcation by RF, classiﬁcation by M 3F usion.

[16] K. He, X. Zhang, S. Ren, and J. Sun, “Deep residual learning for image recognition,” in CVPR, 2016, pp. 770–778.
[17] G. Huang, Z. Liu, L. van der Maaten, and K. Q. Weinberger, “Densely connected convolutional networks,” in CVPR, 2017, pp. 2261–2269.
[18] V. Nair and G. E. Hinton, “Rectiﬁed linear units improve restricted boltzmann machines,” in ICML10, 2010, pp. 807–814.
[19] S. Ioffe and C. Szegedy, “Batch normalization: Accelerating deep network training by reducing internal covariate shift,” in ICML, 2015, pp. 448–456.
[20] F. Pedregosa, G. Varoquaux, A. Gramfort, V. Michel, B. Thirion, O. Grisel, M. Blondel, P. Prettenhofer, R. Weiss, V. Dubourg, J. Vanderplas, A. Passos,
D. Cournapeau, M. Brucher, M. Perrot, and E. Duchesnay, “Scikit-learn: Machine learning in Python,” Journal of Machine Learning Research, vol. 12,
pp. 2825–2830, 2011.

[21] D. P. Kingma and J. Ba, “Adam: A method for stochastic optimization,” CoRR, vol. abs/1412.6980, 2014.

