Deep Movement Primitives: toward Breast Cancer Examination Robot

Oluwatoyin Sanni,1,* Giorgio Bonvicini,2 Muhammad Arshad Khan,1 Pablo C. L´opez-Custodio,1
Kiyanoush Nazari,1 Amir M. Ghalamzan E.1,*
1 University of Lincoln, UK, 2 Polytechnic University of Milan, Italy
aghalamzanesfahani@lincon.ac.uk

2
2
0
2

b
e
F
4
1

]

O
R
.
s
c
[

1
v
5
6
2
9
0
.
2
0
2
2
:
v
i
X
r
a

Abstract

Breast cancer is the most common type of cancer world-
wide. A robotic system performing autonomous breast pal-
pation can make a signiﬁcant impact on the related health
sector worldwide. However, robot programming for breast
palpating with different geometries is very complex and
unsolved. Robot learning from demonstrations (LfD) re-
duces the programming time and cost. However, the avail-
able LfD are lacking the modelling of the manipulation
path/trajectory as an explicit function of the visual sensory
information. This paper presents a novel approach to manip-
ulation path/trajectory planning called deep Movement Prim-
itives that successfully generates the movements of a manip-
ulator to reach a breast phantom and perform the palpation.
We show the effectiveness of our approach by a series of real-
robot experiments of reaching and palpating a breast phan-
tom. The experimental results indicate our approach outper-
forms the state-of-the-art method.

Breast cancer is the most common cancer worldwide with
a signiﬁcant impact on the life of patients and society (Fer-
lay et al. 2021). 2.3 million females were diagnosed with
breast cancer in 2020 with 685,000 related deaths world-
wide (Sung et al. 2021). Less invasive and cheaper cancer
treatments and higher quality of patients’ lives post-cancer
detection are among the beneﬁts of early breast cancer de-
tection (K¨osters and Gøtzsche 2003). Breast Palpation (BP)
is the easiest, most effective and most widely used early can-
cer detection method. BP – both self and clinical exami-
nations – seeks to detect palpable anomalies in the breast
tissue (Provencher et al. 2016; Saslow et al. 2004). BP in-
volves human tactile and visual inspection during palpating
the breast and lymph nodes. Because of the lack of patients’
expertise in palpation, self-examination is ineffective across
societies. Moreover, subjects are reluctant to be examined
by human experts (Yang et al. 2010), detection precision de-
pends on the examiner’s expertise, and the availability of
experts everywhere may be limited. Therefore, our survey
shows autonomous robotic BP (ARBP) is of great interest
to both clinicians and patients (Houghton et al. 2021)

ARBP reduces the burden on the general healthcare sys-
tem for clinical BP, improves the accessibility of the ser-

*Authors contributed equally.

Copyright © 2022, Association for the Advancement of Artiﬁcial
Intelligence (www.aaai.org). All rights reserved.

vice and precision of early breast cancer detection. Al-
though there are many studies for robotic tissue palpation
for tumour detection (Scimeca et al. 2020; Herzig et al.
2018; Kobayashi et al. 2009; Nichols and Okamura 2015;
Keshavarz and Mojra 2015), most of them focus on tis-
sue stiffness classiﬁcation and developing a suitable robot
ﬁnger/hand for palpation. Hence, the problem of efﬁcient
path/motion planning for autonomous robotic palpation re-
mains vastly unexplored. Geometrical variability across dif-
ferent palpation paths and the subjects’ breasts leads to the
complexity of motion/path planning for breast palpation.

Human experts suggest benchmark patterns for a more
effective way of performing breast palpation (see Fig. 1a),
such as circular, wedges or linear (Murali and Crabtree
1992). An autonomous robotic palpation system needs to
encode the expert knowledge into the palpation movements
and adapt the planned motions according to the breast ge-
ometry. Movement Primitives as a tool for compact repre-
sentation of robot’s control policy are used for generating
robot motions (Ude et al. 2010; Matsubara, Hyon, and Mori-
moto 2011; Ghalamzan E. and Ragaglia 2018). Probabilistic
Movement Primitives (ProMP) (Paraschos et al. 2013) can
be used for planning and control purposes where it can ex-
press a distribution of trajectories by the corresponding vari-
ance and mean. Nonetheless, these conventional Movement
Primitives methods (Paraschos et al. 2018, 2013; Schaal
2006; Ude et al. 2010) cannot capture the correlation be-
tween the visual information and the generated movements.
We propose a novel Learning from Demonstration (LfD)
approach called deep movement primitives (deep-MP)1 di-
rectly mapping the visual sensory information into the
learned trajectory. Other contributions of this paper include:
(i) the effectiveness of deep-MP variations for different tasks
complexities are extensively studied; (ii) a series of real-
robot breast palpation experiments that show the effective-
ness of our proposed approach to complex trajectory/path
planning tasks.

Related Works
In trajectory planning, LfD methods use demonstrations to
build a task model and execute it. Probabilistic approaches

1Code, data and an extended appendix are available here:

https://github.com/imanlab/deep movement primitives

 
 
 
 
 
 
(a) Palpation patterns (Deepthi 2017)

(b) Panda arm at home conﬁguration

(c) Breast phantom

(d) Reach-to-palpate

Figure 1: Breast palpation is an efﬁcient and the ﬁrst approach to
evaluate glandular tissue and nipple (Mahoney and Csima 1982):
(a) well-known breast palpation paths: Spiral (left), Wedges (mid-
dle) and Lines (right); (b) 7-DOF Panda arm at Home Conﬁgura-
tion; (c) the breast phantom used use for study in this paper and
the demonstrated palpation paths. The phantom is made of silicon
with 2 lumps implanted on path 4 and 7; (d) reach-to-palpate (RTP)
action of Panda arm; RGB-D images taken at home conﬁguration
shown in Fig. 1b.

such as Gaussian Mixture models (GMM) (Jaquier, Gins-
bourger, and Calinon 2019; Girgin et al. 2020), Gaussian
Process (Schneider and Ertel 2010) are applied in LfD set-
tings for expressing the distributions of trajectories by the
corresponding mean and covariance. However, these mod-
els do not encode the relation between the visual information
of robot’s workspace and the demonstrated behaviour (Rana
et al. 2018).

End-to-end learning-based control approaches such as In-
verse Reinforcement Learning (Levine, Popovic, and Koltun
2011), vision-based Model Predictive Control (Finn and
Levine 2017), or Behaviour cloning (Rahmatizadeh et al.
2018) usually suffer from lack of generalisation and they
are only applied to a class of tasks involved complicated
motion control but not complex motion planning. End-to-
end LfD enables generating control commands directly from

raw image data, e.g., (Rahmatizadeh et al. 2018) proposed
behaviour cloning probabilistic generative model. However,
high dimensional observation space, e.g. raw images, make
the combined motion/path planning and motion control in-
tractable in such settings (Akgun et al. 2012; Nagahama
and Yamazaki 2019). In a line of research, deep-time-series
are used to learn the control policy of a robot to perform
a speciﬁc task. For instance, (Levine et al. 2018, 2016) ap-
plied Deep Neural Networks (NNs) combined with Recur-
rent NNs to image data to learn robots’ control policy. In
these frameworks, deep NNs ﬁnds a mapping between the
raw image and desired control signals. These approaches are
proved to be effective only for limited classes of tasks with
challenging motion control. Moreover, such models are hard
to interpret.

Dynamic Movement Primitives (DMP) model (Schaal
2006) is a well-known LfD approach useful for imitating a
single demonstrated trajectory. Recent works on deep DMP
(d-DMP) show a Deep NN learning to generate the param-
eters of a DMP model from an image (Ridge et al. 2020;
Pervez, Mao, and Lee 2017). For instance, (Ridge et al.
2020) propose a neural network that is trained to output
the parameters of DMP (Schaal 2006). (Pervez, Mao, and
Lee 2017) also use deep NNs to learn the forcing terms of
the DMP model for visual servoing. A distribution of a set
of demonstrations expresses variability of the task execu-
tions which is captured by Prbabilistic Movement Primitives
(ProMP) (Paraschos et al. 2018, 2013). However, it fails to
capture the relation between visual information and trajec-
tory variations, hence, it misses the generalisation capabil-
ity (Rueckert et al. 2015).

Breast palpation movements is hard to program and is
needed for ARBP (Scimeca et al. 2020; Herzig et al. 2018).
Our proposed method can address the existing challenges in
motion planning for Breast Palpation, such as (1) variations
across different palpation executions; (2) variability across
different palpation paths; (3) relation between the visual in-
formation and demonstrated trajectories.

In contrast to d-DMP (Pervez, Mao, and Lee 2017; Ridge
et al. 2020) being deterministic – i.e. they cannot capture
the distribution of trajectories and are designed only to gen-
eralise to initial and goal points – and ProMP, which only
generalise to initial, goal and via points, our approach suc-
cessfully maps images of breast phantom in robot workspace
to robot joint space trajectories. Deep movement primitives
utilises deep Neural Networks (NNs) for feature extraction
and subsequent generation of the weights deﬁning a ProMP
trajectory. Our customised loss function is deﬁned as the
distance between the Ground Truth (GT) and the trajec-
tory generated by the predicted weights, rather than being
the distance between GT and the predicted weights them-
selves. We implemented several different network and LfD
architectures. The results obtained by our framework show
its effectiveness for breast palpation tasks in a practical and
real-world use case. We consider two sub-tasks for breast
palpation. Reach to Palpate (RTP): RGB-D images of the
breast phantom are used to generate trajectory weights that
control the robot joints to reach the starting point of palpa-
tion on the breast phantom. Wedges Palpation Path (WPP):

deep-MP is used to generate WPP shown in Fig. 1c) from a
dataset of demonstrations. This framework will be integrated
into a motion control in future works.

like to have a predictive distribution of qt which does not
depend on Θ, but on ρ := (µΘ, ΣΘ). This is done by
marginalising Θ out in the distribution as follows:

Problem Formulation
Consider a set of Ntr demonstrations, which is deﬁned
as T := {{Q1, I1}, . . . , {QNtr, INtr}} where Qn, n =
1, . . . , Ntr, are the joint space trajectories and In is RGB-
D images taken from the corresponding robot’s workspace.
For a single joint, we deﬁne a trajectory as the ordered set
q := {qt}t=1,...,T , where qt ∈ R is the joint position at sam-
ple t, and Q := {q1, ..., qNjoint } where Njoint is the number
of the joints of the manipulator.

Probabilistic Movement Primitives (ProMP)
In order to
deﬁne a distribution over trajectories, We ﬁrst model a tra-
jectory with an observation uncertainty added to the follow-
ing deterministic model (Paraschos et al. 2013):

q =

Nbas(cid:88)

i=1

θiψi(z(t)) + (cid:15)q

(1)

where ψi are basis functions (usually Gaussian (Bishop
2006)) evaluated at z(t). z is a phase function that allows
time modulation. If no modulation is required, then z(t) =
t/f , where f is the sampling frequency. θi ∈ R are weights,
and (cid:15)q adds zero-mean Gaussian observation noise with vari-
ance Σq.

For stroke-like movements,
Gaussian basis functions are used:

the following normalised

ψi(t) :=

bi(z(t))
j=1 bj(z(t))

(cid:80)Nbas

where

bi(z(t)) := exp

−

(cid:18)

(cid:19)

(z(t) − ci)2
2h

We can also write eq. (1) in a matrix form, as follows:

qt = ΨT

t Θ + (cid:15)q

(2)

(3)

(4)

:= (ψ1(z(t), . . . , ψNbas(z(t)) ∈ RNbas×1,
where Ψt
Θ := (θ1, . . . , θNbas) ∈ RNbas×1, and we also deﬁne
Ω := (Θ1, . . . , ΘNjoint ) ∈ RNbasNjoint×1 and Φ :=
[Ψ1, . . . , ΨT ]T ∈ RT ×Nbas.

From eq. (1), it follows that the probability of observing

qt is given by:

p(qt|Θ) = N (cid:0)qt

(cid:12)
(cid:12) ΨT

t Θ, Σq

(cid:1)

(5)

Since Σq is the same for every time step, the values qt
are taken from independent and identical distributions, i.i.d.
Hence, the probability of observing a trajectory q is given
by:

p(q|Θ) :=

T
(cid:89)

p(qt|Θ)

(6)

t=1
However, since parameters Θ are to be learnt from data,
we also assume such parameters are taken from a distribu-
tion Θ ∼ p(Θ|ρ) = N (Θ|µΘ, ΣΘ). We therefore would

(cid:90)

p(qt|ρ) =

(cid:12)
(cid:12) ΨT

t Θ, Σq

(cid:1) N (cid:0)Θ (cid:12)

(cid:12) µΘ, ΣΘ

(cid:1) dΘ

= N (cid:0)qt

t Θ, Σq + ΨT

t ΣΘΨt

(cid:1)

(7)

N (cid:0)qt
(cid:12)
(cid:12) ΨT

deep-MP weights learning: The weights of ProMP mod-
els are conventionally learned from demonstrations where
they can be later adapted according to different trajectory
reproduction needs, e.g. (1) the initial/goal point of the
desired trajectory are set, or (2) some via points are de-
termined based on the problem constrains. Computing the
via/start/goal points based on visual sensory information of
robot’s workspace needs hand-designed and task- or robot’s
workspace-speciﬁc features. This is effective and handy in
some robotic tasks like simple pick-and-place, but it is too
complex for breast palpation, e.g., in which the task trajec-
tory and the geometry of the breast are related. Two follow-
ing deep-MP models learn the relation between visual sen-
sory information and joint trajectories.

(deep-MP):
Instead of learning the weights of the ProMP,
a deep model– that can be a CNN, FC or PointNet model–
captures the correlation between the visual sensory infor-
mation and ProMP weights as per eq. (8). The algorithm is
described in Alg. 1. Moreover, a schematic of the algorithm
is shown in Fig. 2 in the green block at top right).

For a neural network similar to Fig. 2, network’s be-

haviour can be described by the following:
Θk = hk(Wk, Ik, σk) + vk
Equation (8), known as observation equation, shows that
the network’s target vector Θk is equivalent to a nonlinear

(8)

Algorithm 1: Deep MP

Input: NN architecture h, ProMP basis functions Φ,
image I, training set trajectories q, activation function σ
Output: NN weights W, predicted trajectory ˆq
Note: This pseudo code is for single joint trajectory ˆq.
Generalising it for all joints ˆQ is straightforward.
———————————
1: Dataset: T ← {q, I}1,...,Ntr
2: InitialiseDeepModel : ˆΘ ← h(W, I, σ) eq. (8)
as per Fig. 4: either CNN (2-D) or FCN (1-D);

for all {q, I} ∈ T do

3: InitialiseProMP : ˆq ← ΦT ˆΘ (eq. (4))
4: RMSE ← e = (cid:107)ˆq − q(cid:107)
5: while (e > (cid:15)) do
6:
7:
8:
9:
10:
11:
12: end while
13: deep-MP: ˆq = h(Φ, W, I, σ)
14: end

FORWARDPROPAGATION: ˆΘk = h(Wk, Ik, σ)
FORWARDPROMP: ˆqk = ΦT ˆΘk (eq. (4))
RMSEJOINTLOSS: ek = ek−1 + (cid:107)ˆqk − qk(cid:107)

end for
BACKPROPAGATION: Wk+1 ← {Wk, ∂ek
∂Wk

}

Figure 2: The blue box contains three NN models learning the weight of ProMP. RGB data is passed through an autoencoder, producing
a bottleneck representation of the input Z, which is then fed into a CNN (a) or ﬂattened and fed into a Fully Connected network (b).
Alternatively, the depth data from the image, in the form of vector of (x, y, z) coordinates of the pointcloud, is fed to a PointNet network (c)
and the resulting feature vector is passed through a dense network. All models are used to predict the full ProMP weights (green box - Alg.
1) or just the residual with respect to the mean (yellow box - Alg. 2). Fig. A.4 in the Appendix is an higher resolution version of this image.

function hk of the input image Ik, the weight parameter Wk,
the node activation σk and the observation/measurement
noise vk. We consider the measurement noise to be a zero-
mean white noise with covariance given by E[vkvT
l ] = Rk.

where hk is a nonlinear deep model mapping the image Ik
taken by robot’s camera at a home position (see Fig. 1b) to
the ProMP weights. The weights then generate the corre-
sponding trajectories using eq. (4).

Algorithm 2: Residual deep MP

Input: NN architecture h, ProMP basis functions Φ,
image I, training set trajectories q, activation function σ
Output: NN weights W, predicted trajectory ˆq
Note: This pseudo code is for single joint trajectory ˆq.
Generalising it for all joints ˆQ is straightforward.
——————————–
1: Dataset: T ← {q, I}1,...,Ntr
2: InitWeights: {Θ1, ..., ΘNtr} ← Φ · {q1, ..., qNtr}
3: InitAverages : ¯Θ ← mean(Θ1, ..., ΘNtr) (eq. (9))
4: InitDeepModel : ˆΘres ← h(W, I, σ) eq. (8)

as per Fig. 4: either CNN (2-D) or FCN (1-D);

for all {q, I} ∈ T do

5: InitFullWeights : ˆΘ = ˆΘres + ¯Θ (eq. (10))
6: InitProMP : ˆq ← Φ ˆΘ (eq. (4))
7: RMSE ← e = (cid:107)ˆq − q(cid:107)
8: while (e > (cid:15)) do
9:
10:
11:
12:
13:
14:
15:
16: end while
17: deep-MP residual: ˆq = h(Φ, W, I, σ)
18: end

FORWARDPROPAGATION: ˆΘres
FULLWEIGHTS: ˆΘ = ˆΘres + ¯Θ (eq. (10))
FORWARDPROMP: ˆqk = ΦT ˆΘk (eq. (4))
RMSEJOINTLOSS: ek = ek−1 + (cid:107)ˆqk − qk(cid:107)

end for
BACKPROPAGATION:Wk+1 ← {Wk, ∂ek
∂Wk

}

k = h(Wk, Ik, σ)

(Residual deep-MP):
usually a set of demonstrated tra-
jectories convey information about the presented behaviour
regardless of the scene. GMM, GP and ProMP have been
used to encode such information in a probabilistic model
that can be expressed as a mean and distribution (see Fig. 3).
In order to improve the performance of the deep-MP, we
propose a deep model which learns the correlation between
the input image and residual trajectories, i.e. difference be-
tween the mean and demonstrated trajectories. Hence, the
deep model is required to learn a much simpler mapping
since part of the complexities are captured by the mean tra-
jectory (see the yellow block at bottom right in Fig. 2). First,
we learn to ﬁt a ProMP trajectory ˆQ to the demonstrated
trajectories using least squares optimisation.

Hence, we can compute the mean weights ¯Θ and mean
joint space trajectories q (see Fig. 3) by maximising the like-
lihood as per eq. (9).

Θn = (λI + ΦT Φ)−1ΦT qn, ∀n = 1, . . . , Ntr
¯Θ = E([Θ1, ..., ΘNtr])

(9)

where qn := (q1, . . . , qT ) ∈ RT ×1 is the vectorized form
of single-joint values in trajectory n, and λ is a regularising
term used to avoid over-ﬁtting in the original optimisation
objective. Then the deep-MP model learns the correlation
between the residuals of the trajectories and the visual sen-
sory information.

Θn = Θres,n + ¯Θ

(10)

(a) Joint Space trajectory

Figure 4: RTP-RGB Datasets: CNN–deep-MP Model tested on
unseen Breast Phantom conﬁgurations. (a) shows precise task ex-
ecution whereas (b) and (c) have a larger errors as they belong to
regions with different sample densities (a, b and c belong to region
1, 2 and 4, respectively in Fig. A.3 of Appendix). The distance
between the Robot EE and the corner of the breast phantom–the
desired touching point demonstrated– is because of difference in
sample density of the corresponding regions.

(b) Reproduction mean trajectory and corresponding variance

Figure 3: (a) Samples of RTP-RGBD demonstrated trajecto-
ries from left/joint 1 to right/joint 4 – trajectories of joint 5,
6 and 7 are not shown here; (b) the computed mean (solid
blue lines)– used for the residual deep-MP implementation–
and representative variations of the distribution (shaded grey
areas).

where Θn and Θres,n are, respectively, the full and residual
weights of the ProMP model for Ntr demonstrated trajec-
tories. We can use the deep-MP model presented in eq. (8)
to learn Θres,n which will be added to the ¯Θ to form the
ProMP corresponding with {Qn, In}, as per eq. (4).

Hardware setup and data collection
Our experimental setup consists of a 7-DoF Panda robotic
arm manufactured by Franka Emika. An Intel RealSense
D435i RGB-D camera is mounted on the wrist of the arm.
We also use a tactile ﬁnger consisting of a 6x4 uSkin Xela
magnetic-based tactile sensor for RTP-RGBD and WPP data
collection. This sensor is ﬁrmly connected to the left ﬁnger
link of the gripper using a 3D printed mount. Although the
reading of the tactile sensing is not used in this study, we
will use it for future study of palpation motion control.

We have obtained three data sets: (1) we collected reach-
to-palpate (RTP) dataset, called RTP-RGB, in a mock study;
(2) Reach-to-palpate dataset 2 called RTP-RGBD, and (3)
Wedged-palpation-path dataset called WPP.

Reach-to-palpate (RTP)
(i) RTP-RGB: Our mock study
includes the RTP-RGB data collection. For each sample in
this dataset, the robotic arm starts from a ﬁxed home pose
as shown in Fig. 1b. At this home pose, the camera takes
an RGB image of the breast phantom; and then the robot
is manually moved to the corner of the breast phantom in
kinesthetic teaching mode as shown in Fig. 4. This dataset
contains 500 samples, i.e. the robot at home conﬁguration
takes RGB images of breast phantom at a random position
on the table. We have trained the CNN and FC deep-MP
model, where they yield 0.0108 and 0.0118 [Radian2] errors

Conﬁg. I

Conﬁg. II

Conﬁg. III

Conﬁg. IV

Figure 5: Samples of WPP data set: the brown disk on im-
ages show the desired end-points of palpation path 5, 1, 4
and 3 at conﬁguration I, II, III and IV, respectively. The start-
ing point for each demonstrated palpation is the nipple.

in joint space and 39.7 mm and 46.8 mm errors in task space.
Full details of the RTP-RGB dataset and the obtained re-
sults are described in Appendix. The results obtained by this
dataset suggest (1) we need a more structured dataset to bet-
ter understand the impact of samples density on the results;
(2) the depth data may be relevant; (3) a more challenging
task is needed to showcase the effectiveness of the approach.
(ii) RTP-RGBD: the setup for the following data collec-
tion is the same as the one in RTP-RGB dataset. Nonethe-
less, we collected RGB and depth data for each sample and
the robot is moved by joint space motion planning to the
nipple of the breast phantom. We consider 4 regions for data
collection as shown in Fig. 6: Region A, B, C and D. Af-
ter each sample collection, the breast phantom was moved
to a new location within the region boundary to create uni-
form distribution for each region with different densities as
shown in Fig. 6. A total of 545 samples were collected with
292, 128, 73 and 52 samples in region A, B, C, and D re-
spectively.

Wedges Palpation Path (WPP) After moving the robot
tactile ﬁnger to the nipple of the breast phantom, the robot
needs to follow the palpation path. The robot is moved us-
ing kinesthetic-teaching-mode from nipple along to the edge
of the phantom similar to WPP shown in Fig. 1c. Synchro-
nised robot full state, tactile sensor readings, and joint tra-
jectory are recorded. 31 palpation trials for every 7 WPPs
(Fig. 1c) and four different phantom conﬁgurations (Fig. 5)
are recorded. A total of 868 palpation samples were col-
lected in WPP dataset.

for training with Adam optimiser using a learning rate of
0.001 and a batch size of 32. Our models were trained using
150 epochs with a callback function to terminate training
when over-ﬁtting.

Loss function We have implemented our custom loss
function using the deep-MP model. We denote the ground
truth of our joints weights as Θgt ∈ RNbas×1 and the corre-
sponding predicted ones as Θps ∈ RNbas×1. Our loss func-
tion is the root mean squared error between the trajectories
generated by Θgt and Θps. The loss of a predicted vector
Θps is then given by:

Figure 6: XY coordinates of the end-effector when the robot
reaches the start point of the palpation in RTP-RGBD dataset.

L(Θps, Θgt) :=

(cid:118)
(cid:117)
(cid:117)
(cid:116)

(cid:18) 1
T

(cid:19) T

(cid:88)

t=1

(qgt,t − qps,t)2

(11)

Experiments and Results

To validate our hypothesis, i.e. deep-MP can produce accu-
rate trajectory/path for breast palpation, we performed dif-
ferent experiments using the collected dataset RTP-RGBD
and WPP in this proof-of-concept study. Although we only
have one breast model and we cannot claim the generali-
sation across different breast geometry, the palpation paths
varies across different palpation experiments in WPP exper-
iments due to variations in the palpation terminal points.

Autoencoder as feature extractor: Using the RGB images,
we trained an autoencoder on the 500 images for feature ex-
traction, by using the bottleneck layer of the trained autoen-
coder. We then trained two models that use the learned fea-
tures from the bottleneck layer to predict the joint weights
that takes the robot to the palpation starting point (see
Fig. 2).

Convolutional Neural Network (CNN) for mapping from
features to ProMP weights Model: The bottleneck layer was
an image of 32x32x3 which was passed through a CNN ar-
chitecture to predict the joint weights. We implemented two
versions of this model, one using Alg. 1 that predicts the full
ProMP weights and one using Alg. 2 that predicts the resid-
ual ProMP weights with respect to the mean trajectory for
any given region.

Fully Connected (FC) Model for mapping from features
to ProMP weights: The bottleneck layer was ﬂattened to a
1-D vector and then fed into stack of dense layers to predict
the joint weights at the last layer.

PointNet for mapping the pointcloud coordinates to
ProMP weights: In addition to the CNN and FC models
described above, we also trained deep models using the
pointcloud coordinates. The vector of Cartesian coordinates
was fed into a pre-trained PointNet network (Qi et al. 2017),
from which the global feature vector was extracted and fed
into a smaller Fully Connected network to predict the joint
weights (Blue block in Fig. 2).

For all models, the dataset is split into 85% and 15% for
training and testing, respectively, and 25% of training data
are used for cross-validation. We use Tensorﬂow framework

where qgt,t = ΦT
t Θgt and qps,t = ΦT
For our experiments, the number of samples was T =

t Θps.

150.

Metrics To measure the performance of the approach,
ps ∈ RNbas×1 be
Ntest test samples are evaluated. Let Θn
the predicted weights of test sample n. We consider the fol-
lowing two metrics.
(i) Average Mean Squared Error (AveMSE) between the tra-
jectories generated from the predicted weights and ground
truth.

AveMSE :=

1
Ntest

Ntest(cid:88)

n=1

(cid:2)L(Θn

ps, Θn

gt)(cid:3)2

(12)

where Ntest is the number of samples in the test set.

(ii) Average Euclidean Distance (AveED) between the po-
sition of the end-effector at the last conﬁguration, t = T
generated by deep-MP model and the ground truth.

AveED :=

1
Ntest

Ntest(cid:88)

n=1

(cid:12)
(cid:12)rn

ee,ps − rn

ee,gt

(cid:12)
(cid:12)

(13)

ee,gt, rn

ee,ps ∈ R3×1 are, respectively, the ground
where, rn
truth and predicted position vectors of the end-effector for
sample n, at t = T .

The CNN network architectures and the loss function for
the WPP and RTP are the same, except the last units of the
dense layer in CNN-RTP is replaced with 70 units in CNN-
WPP. Moreover, the images input to autoencoder are marked
with the target point as shown in Fig. 5. The training param-
eters for WPP and RTP experiments are the same. However,
200 epochs was used in WPP.

d-DMP (Ridge et al. 2020) recently proposed deep DMP.
We also implemented d-DMP both for RTP and WPP tasks
with our best CNN (see Appendix for implementation de-
tails). The results obtained by d-DMP and deep-MP are
shown in Table 2 for WPP experiments illustrating our deep-
MP models performance is much better than the d-DMP
ones in all the cases.

While the CNN deep-MP outperforms the FC, d-DMP
and PointNet deep-MP model in RTP-RGBD dataset in RTP

MSE∗

Model

Reg. A (292 samples) B (128) C (73) D (52)

CNN residual deep-MP
CNN deep-MP
FC deep-MP
PointNet deep-MP
d-DMP

0.0009
0.0016
0.0047
0.0095
0.0117

0.0030
0.0031
0.0051
0.0169
0.0109

0.0051
0.0037
0.0058
0.0233
0.0172

0.0047
0.0041
0.0082
0.0191
0.0226

∗ MSE in rad2; ∗∗ Absolute error in mm;

Absolute Error∗∗
A (292) B (128) C (73) D (52)

24.6
41.1
47.2
75.8
84.8

41.7
48.0
63.3
192.3
66.5

72.1
55.2
69.5
226.8
113.8

72.5
58.5
83.8
167.7
132.6

Table 1: Evaluation of RTP models with Average MSE (AveMSE) of the joint trajectory in each region of RTP-RGBD data
set (columns at left); Evaluation of RTP models with Average Euclidean Distance (AveED) in cartesian coordinates of the end
effector position at the end of the trajectory in each region of RTP-RGBD data set (columns at right).

Model

Conﬁg.I Conﬁg.II Conﬁg.III Conﬁg.IV

MSE∗

Absolute Error∗∗
Conﬁg.I Conﬁg.II Conﬁg.III Conﬁg.IV

CNN deep-MP
CNN residual deep-MP
d-DMP

WPP4 CNN deep-MP

0.0373
0.0380
1.0541

0.0340

0.0245
0.0763
1.4307

0.0617

0.0072
0.0662
1.7214

0.0441

0.0420
0.0465
1.7013

0.0756

67.4
61.3
487.0

63.8

57.6
58.2
499.9

112.9

53.9
89.3
399.8

113.5

63.1
83.6
407.9

61.4

∗ MSE in rad2; ∗∗ Absolute error in mm.

Table 2: Evaluation of CNN deep-MP and CNN residual deep-MP Model with AveMSE and Absolute Error at each conﬁgura-
tion for WPP9 data set (middle row). Evaluation of CNN deep-MP for WPP4 dataset (bottom row).

experiments (Table 1), the performance of d-DMP is better
than just some cases of PointNet deep-MP. CNN can cap-
ture more complex mapping than FC model and PointNet
deep-MP; hence, it is by far better than others (the trajecto-
ries generated by the CNN models are shown in Fig. A.5 and
A.6 of Appendix).

We considered different conﬁgurations of training and
test set for WPP task conducting a series of experiments –
{WPP1, . . . , WPP10} (see Appendix for details). Two ex-
periments with interesting results are presented in Table 2.
Our results (see Table A.4 in Appendix) show the best model
performance is obtained when including all the palpation
path, as it was done in WPP9 experiment. The results of
WPP9 are also presented in the middle row of Table 2 show-
ing all deep-MP models outperforming d-DMP by a large
margin. Another interesting observation is presented in the
bottom row of Table 2 for WPP4 where – similarly to WPP1-
3 – only palpation 1,4,5 are seen in training and 2,3 used for
testing. The results indicate that our model is able to gen-
eralise to unseen geometry of breast palpation. Other eight
ablation studies also suggest the effectiveness of our deep-
MP model for complex palpation tasks.

The results in Table 1 and 2 show that residual version
of deep-MP outperforms the standard one in RTP but not
quite well in WPP. We learned that RTP trajectories are more
uniform than WPP ones. Moreover, the RTP has travelling
distance longer than WPP with much more non-linearity.
Hence, the mean trajectory across different RTP demonstra-
tions is more informative than the WPP. i.e. the mean trajec-
tory captures large non-linearity across the RTP trajectories,
hence the CNN model requires to learn much simpler map-

ping.

In spite of WPP results (in Table 2), the MSE and Abso-
lute Error of d-DMP and the variations of deep-MP reported
in RTP cases (in Table 1) are very close. Hence, the com-
bined results in Table 1 and Table 2 suggest that d-DMP can
perform relatively well in simple tasks such as RTP– where,
in contrast to WPP, RTP tasks depend only on a target point,
i.e. the nipple– whereas it yields a very poor performance
in complex tasks such as WPP. The video attachment shows
deep-MP successfully outputting the joints weights for pal-
pation paths from the nipple to a terminal point that is set
manually – demonstrating the generalisation of our model –
or autonomously.

Conclusion

Autonomous robots for breast palpation can have signiﬁcant
impact on the societies health sector. One of the challenges
for such technologies is robot programming which forms
a very interesting scientiﬁc problem. We proposed a novel
learning from demonstration method, called deep movement
primitives, that maps visual information to palpation trajec-
tories and is useful for programming a robot for autonomous
palpation. We show the effectiveness of our approach in a
series of real-robot breast phantom palpation experiments.
While state-of-the-art approach fails to learn palpation due
to the variations in the start, terminal points and the interme-
diate points, our results shows deep-MP is a suitable method
for learning complex tasks, such as palpation. Our future
works include use of the sensor data and creating demon-
strations across different breast models that helps deep-MP
generalise even better.

Ethical statement
Cancer Research UK has funded a project in 2020 to shape
new technology for breast health called ARTEMIS 2. This
proof of concept project aims to develop a breast cancer ex-
amination robot in collaboration with clinicians and with Pa-
tient and Public Involvement (PPI) (Houghton et al. 2021).
The consortium consists of University of Lincoln, Univer-
sity of Bristol and Imperial College London. Our partners
are developing a soft robot tool and soft sensors for safe,
non-invasive and comfortable use, building Breast phantoms
based on real subjects’ breasts that can give clinicians a feel
of real breasts during palpation, collecting data of clinicians
performing palpation, with active PPI in design and devel-
opment of the technology.

The autonomous Breast Palpation Robot (ABPR) will
have a chair where patients can incline/bend forward 45 de-
grees and lie on the top of the robot housing and breasts go
into designated holes allowing the soft sensors to touch the
breast and perform the palpation. A survey of 155 women in
the United Kingdom was conducted, showing them schemat-
ics of different designs of ABPR. Results indicated enthu-
siasm for ABPR with 92% of respondents indicating they
would use ABPR. 83% would be willing to be examined for
up to 15 minutes. GP surgery is the most popular location
for ABPR. Thematic analysis of free-text responses identi-
ﬁed the following: a) Subjects perceived ABPR has the po-
tential to address limitations in current screening services;
b) ABPR facilitates increased user choice and autonomy; c)
there are ethical motivations for supporting ABPR develop-
ment; d) accuracy is essential; e) integration with health ser-
vices is important. The above results are included in a paper
(Houghton et al. 2021) under revision for ﬁnal publication.
ABPR collects data for clinician references and poten-
tially reduces the number of (true negative) visits to hospitals
for breast cancer examination. It is no replacement for hos-
pital examinations. We aim to develop a cheap device safe,
comfortable, reliable and accessible–especially in poor com-
munities/countries that lack hospitals or expert clinicians.

ABPR allows recording the history of the palpation data
for individuals helping patients and clinicians with precise
information about any changes observed during palpation to
be judged by clinicians or AI whereas judgement based on
human palpation can be subjective.

Acknowledgements
This work was partially supported by Centre for Doctoral
Training, United Kingdom (CDT) in Agri-Food Robotics
(AgriFoRwArdS) Grant reference: EP/S023917/1; Lincoln
Agri-Robotics (LAR) funded by Research England; and
by ARTEMIS project funded by Cancer Research UK
C24524/A300038.

References
Akgun, B.; Cakmak, M.; Jiang, K.; and Thomaz, A. L. 2012.
Keyframe-based learning from demonstration. International
Journal of Social Robotics, 4(4): 343–355.

2https://bit.ly/3GFIsC3

https://www.

Bishop, C. M. 2006. Linear Models for Regression.
In
Jordan, M.; Kleinberg, J.; and Sch¨olkopf, B., eds., Pattern
recognition and machine learning, chapter 3, 137–147. New
York: springer.
Deepthi, G. L. 2017. Breast screening.
slideshare.net/LAKSHMIDEEPTHIGEDELA/breast-
screening-81998153. Accessed: 2021-08-28.
Ferlay, J.; Ervik, M.; Lam, F.; Colombet, M.; Mery, L.;
and Pi˜neros, M. 2021. Global Cancer Observatory: Cancer
Today. https://www.who.int/news-room/fact-sheets/detail/
cancer. Accessed: 2021-08-30.
Finn, C.; and Levine, S. 2017. Deep visual foresight for
In 2017 IEEE International Con-
planning robot motion.
ference on Robotics and Automation (ICRA), 2786–2793.
IEEE.
Ghalamzan E., A.; and Ragaglia, M. 2018. Robot learning
from demonstrations: Emulation learning in environments
with moving obstacles. Robotics and autonomous systems,
101: 45–56.
Girgin, H.; Pignat, E.; Jaquier, N.; and Calinon, S. 2020. Ac-
tive improvement of control policies with Bayesian Gaus-
sian mixture model. In 2020 IEEE/RSJ International Con-
ference on Intelligent Robots and Systems (IROS), 5395–
5401. IEEE.
Herzig, N.; Maiolino, P.; Iida, F.; and Nanayakkara, T. 2018.
A variable stiffness robotic probe for soft tissue palpation.
IEEE Robotics and Automation Letters, 3(2): 1168–1175.
Houghton, N.; Jenkinson, G. P.; van Zalk, N.; Waller, J.;
Bello, F.; and Tzemanaki, A. 2021. Acceptability of Auto-
mated Robotic Clinical Breast Examination. Forthcoming.
Jaquier, N.; Ginsbourger, D.; and Calinon, S. 2019. Learn-
ing from demonstration with model-based Gaussian process.
arXiv:1910.05005.
Keshavarz, M.; and Mojra, A. 2015. Dynamic modelling
of breast tissue with application of model reference adap-
tive system identiﬁcation technique based on clinical robot-
assisted palpation. Journal of the mechanical behavior of
biomedical materials, 51: 269–278.
Kobayashi, Y.; Suzuki, M.; Kato, A.; Konishi, K.;
Hashizume, M.; and Fujie, M. G. 2009. A robotic palpation-
based needle insertion method for diagnostic biopsy and
treatment of breast cancer. In 2009 IEEE/RSJ International
Conference on Intelligent Robots and Systems, 5534–5539.
IEEE.
K¨osters, J.; and Gøtzsche, P. 2003. Regular self-examination
or clinical examination for early detection of breast can-
cer. Cochrane database of systematic reviews (Online), 2:
CD003373.
Levine, S.; Finn, C.; Darrell, T.; and Abbeel, P. 2016. End-
to-end training of deep visuomotor policies. The Journal of
Machine Learning Research, 17(1): 1334–1373.
Levine, S.; Pastor, P.; Krizhevsky, A.; Ibarz, J.; and Quillen,
D. 2018. Learning hand-eye coordination for robotic grasp-
ing with deep learning and large-scale data collection. The
International Journal of Robotics Research, 37(4-5): 421–
436.

Conference on Robotics and Automation (ICRA), 1511–
1518. IEEE.
Saslow, D.; Hannan, J.; Osuch, J.; Alciati, M. H.; Baines, C.;
Barton, M.; Bobo, J. K.; Coleman, C.; Dolan, M.; Gaumer,
G.; et al. 2004. Clinical breast examination: practical recom-
mendations for optimizing performance and reporting. CA:
a cancer journal for clinicians, 54(6): 327–344.
Schaal, S. 2006. Dynamic movement primitives-a frame-
work for motor control in humans and humanoid robotics.
In Adaptive motion of animals and machines, 261–280.
Springer.
Schneider, M.; and Ertel, W. 2010. Robot learning by
demonstration with local gaussian process regression.
In
2010 IEEE/RSJ International Conference on Intelligent
Robots and Systems, 255–260. IEEE.
Scimeca, L.; Maiolino, P.; Bray, E.; and Iida, F. 2020. Struc-
turing of tactile sensory information for category formation
in robotics palpation. Autonomous Robots, 44(8): 1377–
1393.
Sung, H.; Ferlay, J.; Siegel, R. L.; Laversanne, M.; Soer-
jomataram, I.; Jemal, A.; and Bray, F. 2021. Global can-
cer statistics 2020: GLOBOCAN estimates of incidence and
mortality worldwide for 36 cancers in 185 countries. CA: a
cancer journal for clinicians, 71(3): 209–249.
Ude, A.; Gams, A.; Asfour, T.; and Morimoto, J. 2010. Task-
speciﬁc generalization of discrete and periodic dynamic
IEEE Transactions on Robotics,
movement primitives.
26(5): 800–815.
Yang, R.-J.; Huang, L.-H.; Hsieh, Y.-S.; Chung, U.-L.;
Huang, C.-S.; and Bih, H.-D. 2010. Motivations and rea-
sons for women attending a breast self-examination training
program: a qualitative study. BMC women’s health, 10(1):
1–11.

Levine, S.; Popovic, Z.; and Koltun, V. 2011. Nonlinear in-
verse reinforcement learning with gaussian processes. Ad-
vances in neural information processing systems, 24: 19–27.
Mahoney, L.; and Csima, A. 1982. Efﬁciency of palpation
in clinical detection of breast cancer. Canadian Medical As-
sociation Journal, 127(8): 729.
Matsubara, T.; Hyon, S.-H.; and Morimoto, J. 2011. Learn-
ing parametric dynamic movement primitives from multiple
demonstrations. Neural networks, 24(5): 493–500.
Murali, M. E.; and Crabtree, K. 1992. Comparison of two
breast self-examination palpation techniques. Cancer nurs-
ing, 15(4): 276–282.
Nagahama, K.; and Yamazaki, K. 2019. Learning from
Demonstration Based on a Mechanism to Utilize an Object’s
Invisibility. In 2019 IEEE/RSJ International Conference on
Intelligent Robots and Systems (IROS), 6120–6127. IEEE.
Nichols, K. A.; and Okamura, A. M. 2015. Methods to
segment hard inclusions in soft tissue during autonomous
IEEE Transactions on Robotics, 31(2):
robotic palpation.
344–354.
Paraschos, A.; Daniel, C.; Peters, J.; and Neumann, G. 2018.
Using probabilistic movement primitives in robotics. Au-
tonomous Robots, 42(3): 529–551.
Paraschos, A.; Daniel, C.; Peters, J. R.; and Neumann, G.
2013. Probabilistic Movement Primitives.
In Burges, C.
J. C.; Bottou, L.; Welling, M.; Ghahramani, Z.; and Wein-
berger, K. Q., eds., Advances in Neural Information Process-
ing Systems, volume 26. Curran Associates, Inc.
Pervez, A.; Mao, Y.; and Lee, D. 2017. Learning deep move-
ment primitives using convolutional neural networks.
In
2017 IEEE-RAS 17th international conference on humanoid
robotics (Humanoids), 191–197. IEEE.
Provencher, L.; Hogue, J.; Desbiens, C.; Poirier, B.; Poirier,
E.; Boudreau, D.; Joyal, M.; Diorio, C.; Duchesne, N.; and
Chiquette, J. 2016. Is clinical breast examination important
for breast cancer detection? Current Oncology, 23(4): 332–
339.
Qi, C. R.; Su, H.; Mo, K.; and Guibas, L. J. 2017. Point-
Net: Deep Learning on Point Sets for 3D Classiﬁcation and
Segmentation. arXiv:1612.00593.
Rahmatizadeh, R.; Abolghasemi, P.; B¨ol¨oni, L.; and Levine,
S. 2018. Vision-based multi-task manipulation for inexpen-
sive robots using end-to-end learning from demonstration.
In 2018 IEEE international conference on robotics and au-
tomation (ICRA), 3758–3765. IEEE.
Rana, M.; Mukadam, M.; Ahmadzadeh, S. R.; Chernova, S.;
and Boots, B. 2018. Towards robust skill generalization:
Unifying learning from demonstration and motion planning.
In Intelligent robots and systems.
Ridge, B.; Gams, A.; Morimoto, J.; Ude, A.; et al. 2020.
Training of deep neural networks for the generation of dy-
namic movement primitives. Neural Networks, 127: 121–
131.
Rueckert, E.; Mundo, J.; Paraschos, A.; Peters, J.; and Neu-
mann, G. 2015. Extracting low-dimensional control vari-
ables for movement primitives. In 2015 IEEE International

