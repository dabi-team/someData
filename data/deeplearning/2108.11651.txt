1
2
0
2

g
u
A
1
3

]

G
L
.
s
c
[

2
v
1
5
6
1
1
.
8
0
1
2
:
v
i
X
r
a

Scalable and Modular Robustness Analysis of
Deep Neural Networks

Yuyi Zhong1 Quang-Trung Ta1 Tianzuo Luo1

Fanlong Zhang2

Siau-Cheng Khoo1

{yuyizhong, taqt, tianzuoluo}@comp.nus.edu.sg
izhangfanlong@gmail.com khoosc@nus.edu.sg

1 School of Computing, National University of Singapore, Singapore
2 School of Computer, Guangdong University of Technology, China

Abstract. As neural networks are trained to be deeper and larger, the
scalability of neural network analyzer is urgently required. The main
technical insight of our method is modularly analyzing neural networks
by segmenting a network into blocks and conduct the analysis for each
block. In particular, we propose the network block summarization tech-
nique to capture the behaviors within a network block using a block
summary and leverage the summary to speed up the analysis process.
We instantiate our method in the context of a CPU-version of the state-
of-the-art analyzer DeepPoly and name our system as Bounded-Block
Poly (BBPoly). We evaluate BBPoly extensively on various experiment
settings. The experimental result indicates that our method yields com-
parable precision as DeepPoly but runs faster and requires less compu-
tational resources. Especially, BBPoly can analyze really large neural
networks like SkipNet or ResNet that contain up to one million neurons
in less than around 1 hour per input image, while DeepPoly needs to
spend even 40 hours to analyze one image.
Keywords: Abstract Interpretation · Formal Veriﬁcation · Neural Nets.

1

Introduction

Deep neural networks are one of the most well-established techniques and have
been applied in a wide range of research and engineering domains such as image
classiﬁcation, autonomous driving etc. However, researchers have found out that
neural nets can sometimes be brittle and show unsafe behaviors. For instance, a
well-trained network may have high accuracy in classifying the testing image
dataset. But, if the testing image is perturbed subtly without changing the
context of the image, it could fool the network into classifying the perturbed
image as something else; this perturbation is known as adversarial attack [1,
2]. To tackle the issue, robustness veriﬁcation is used to guarantee that unsafe
states will not be reached within a certain perturbation size. Several veriﬁcation
techniques have been proposed to verify the robustness of neural networks.

 
 
 
 
 
 
2

Yuyi Zhong et al.

In general, these techniques can be categorized into incomplete methods (e.g.
abstract interpretation [3–5]) and complete methods (e.g. constraint solving [6,
7]). Complete methods reason over exact result, but also require long execution
time and heavy computational power. On the contrary, incomplete methods run
much faster but will lose precision along the way.

One of the most state-of-the-art neural network veriﬁcation methods pro-
posed in recent years is DeepPoly [5]. It is an incomplete but eﬃcient method
that uses abstract interpretation technique to over-approximate operations in
neural network. In particular, DeepPoly designs the abstract domain to contain
symbolic lower and upper constraints, together with concrete lower and upper
bounds of a neuron’s value. The symbolic constraints of a neuron are deﬁned over
neurons in the previous layer; during analysis, they will be revised repeatedly
into constraints deﬁned over neurons of even earlier layers. This computation
is named as back-substitution and is aimed to obtain more precise analysis re-
sults [5].

Considering a network with n aﬃne layers and each layer has at most N
neurons, the time complexity of this back-substitution operation is O(n2·N 3) [8].
When the neural network has many layers (n is large), this computation is heavy
and it also demands extensive memory space. This is the main bottleneck of the
abstract-interpretation-based analysis used by DeepPoly.

Motivation. As deep neural networks are trained to be larger and deeper to
achieve higher accuracy or handle more complicated tasks, the veriﬁcation tools
will inevitably need to scale up so as to analyze more advanced neural networks.
To mitigate the requirement for high computational power of DeepPoly, we
propose a network block summarization technique to enhance the scalability of
the veriﬁcation tool. Our key insight is to deﬁne a method that enables trade-
oﬀ between precision requirement, time-eﬃciency requirement and computing-
resource limitation. Our method, specially tailored to handle very deep networks,
leads to faster analysis and requires less computational resources with reasonable
sacriﬁce of analysis precision. We instantiate our method in the context of a
CPU-version of DeepPoly, but it can also be implemented for the GPU version
of DeepPoly (named as GPUPoly [8]) which can lead to even more gain in speed.

Contribution. We summarize our contributions below:

– We propose block summarization technique supplemented with bounded back-
substitution heuristic to scale up the veriﬁcation process to handle large
networks like ResNet34 [9] with around one million neurons.

– We design two types of block summaries that allow us to take “shortcuts” in
the back-substitution process for the purpose of reducing the time complexity
and memory requirement during the analysis process.

– We implement our proposal into a prototype analyzer called BBPoly, which
is built on top of the CPU-version of DeepPoly, and conduct extensive exper-
iments on fully-connected, convolutional and residual networks. The experi-
mental results show that BBPoly is faster and requires less memory allocation
compared to the original DeepPoly, while achieves comparable precision.

Scalable and Modular Robustness Analysis of Deep Neural Networks

3

2 Overview

We present an overview of the whole analysis process with an illustrative exam-
ple. Our analyzer is built on top of DeepPoly system, leveraging their design of
abstract domains and abstract transformers. But we will analyze the network
in blocks and generate block summarization to speed up the analysis process.
Formal details of our proposed method will be provided in Section 3.

The illustrative example is a fully-connected network with ReLU activation
function as shown in Figure 1. The network has 4 layers with 2 neurons in each
layer and the two input neurons i1, i2 can independently take any real number
between [−1, 1]. The weights of the connections between any two neurons from
two adjacent layers are displayed at their corresponding edges, the bias of each
neuron is indicated either above or below the neuron. Computation for a neuron
in a hidden layer undergoes two steps: (i) an aﬃne transformation based on
the inputs, weights and biases related to this neuron, which generates a value
v, followed by (ii) a ReLU activation which outputs v if v > 0, or 0 if v ≤ 0.
For the output layer, only aﬃne transformation is applied to generate the ﬁnal
output of the entire network.

Input layer

i1 ∈ [−1, 1]

i2 ∈ [−1, 1]

1

1

1

-1

Hidden layer 1
0

Hidden layer 2
0

Output layer
1

1

1

1

-1

0

0

1

0

1

1

0

Fig. 1: Example fully-connected network with ReLU activation (cf. [5])

To analyze a neural network, we follow the approach taken by DeepPoly
where each hidden layer is perceived as a combination of an aﬃne layer and a
ReLU layer. Therefore, network in Figure 1 will be represented by the network
depicted in Figure 2 for analysis purpose, where a neuron in a hidden layer is
expanded into two nodes: (i) one aﬃne node for the related aﬃne transformation
(such as x3, x4, x7, x8), and (ii) one ReLU node which is the output of ReLU
function (such as x5.x6, x9, x10).
2.1 Preliminary Description on Abstract Domain

We use the abstract domain designed from DeepPoly system [5] to verify neural
networks. For each neuron xi, its abstract value is comprised of four elements:
a symbolic upper constraint us
i , a symbolic lower constraint ls
i , a concrete lower
bound li and a concrete upper bound ui. And we have ls
i ≤ xi ≤ us
i , xi ∈
[li, ui]. All the symbolic constraints associated with xi can be formulated as
bi + (cid:80)
j wj · xj, where wj ∈ R, bi ∈ R, j < i. Here, the constraint j < i asserts
that the constraints for xi only refer to variables “before” xi, since the value of

4

Yuyi Zhong et al.

[−1, 1]

x1

1

1

1

[−1, 1]

x2

-1

0
x3

x4

0

max(0, x3)

x5

1

1

1

max(0, x4)

x6

-1

0
x7

x8

0

max(0, x7)

x9

x10

max(0, x8)

1

0

1

1

1
x11

x12

0

Fig. 2: The transformed network from Figure 1 to perform analysis (cf. [5])

one neuron (at a layer) only depends on the values of the neurons at preceding
layers. For the concrete bounds of xi, we have li ∈ R, ui ∈ R, li ≤ ui and the
interval [li, ui] over-approximates all the values that xi can possibly take.

2.2 Abstract Interpretation on the Example Network

We now illustrate how to apply abstract interpretation on the example network
in order to get the output range of the network, given an abstract input [−1, 1]
for both the input neurons.

The analysis starts at the input layer and processes layer by layer until output
layer. The abstract values of the inputs x1, x2 are respectively (cid:104)ls
1 =
1, l1 = −1, u1 = 1(cid:105) and (cid:104)ls
2 = −1, us
2 = 1, l2 = −1, u2 = 1(cid:105). Next, the aﬃne
abstract transformer (designed by DeepPoly [5]) for x3 and x4 generates the
following symbolic constraints, where the coeﬃcients (and the constant terms,
if any) in constraints are the weights (and bias) in the fully connected layer:

1 = −1, us

x1 + x2 ≤ x3 ≤ x1 + x2;

x1 − x2 ≤ x4 ≤ x1 − x2

(1)

The concrete bounds are computed using concrete intervals of x1, x2 and
symbolic constraints in Equation (1), thus l3 = l4 = −2 and u3 = u4 = 2 (the
process of computing concrete bound is formally described in Appendix A).

The activation transformer (designed by DeepPoly [5]) is then applied to get
the abstract elements for x5, x6 from x3, x4 respectively. In general, given that
xi = ReLU(xj), if uj ≤ 0, xi is always 0, therefore we have 0 ≤ xi ≤ 0, li =
0, ui = 0. If lj ≥ 0, then xi = xj and we get xj ≤ xi ≤ xj, li = lj, ui = uj. For
the case where lj < 0 and uj > 0, an over-approximation error will be introduced
and we set the abstract element as followed for xi:

xi ≥ ci · xj,

xi ≤

uj(xj − lj)
uj − lj

,

li = 0,

ui = uj,

(2)

where ci = 0 if |lj| > |uj| and ci = 1 otherwise. For example, x5 = ReLU(x3)
and since l3 < 0, u3 > 0, it belongs to the last case described in Equation (2).
|l3| = |u3| = 2 therefore c5 = 1. Finally, we get the abstract value for x5:
5 = x3, us
l5 = 0, u5 = 2, ls
5 = 0.5 · x3 + 1. Similar computation can be done for x6
6 = x4, us
to yield l6 = 0, u6 = 2, ls

6 = 0.5 · x4 + 1.

Scalable and Modular Robustness Analysis of Deep Neural Networks

5

Next, we work on the symbolic bounds for x7, x8, beginning with:

x5 + x6 ≤ x7 ≤ x5 + x6;

x5 − x6 ≤ x8 ≤ x5 − x6

(3)

From the symbolic constraints in Equation (3), we recursively substitute the
symbolic constraints backward layer by layer until the constraints are expressed
in terms of the input variables. Upon reaching back to an earlier layer, constraints
deﬁned over neurons in that layer are constructed and concrete bound values are
evaluated and recorded (refer to Appendix A). Finally the most precise bound
among all these layers will be selected as the actual concrete bound for x7 and
x8 respectively. This process is called back-substitution and is the key technique
proposed in DeepPoly to achieve tighter bounds. We follow the back-substitution
procedure in DeepPoly and construct constraints for x7, x8 deﬁned over x3, x4:

x3 + x4 ≤ x7 ≤ 0.5 · x3 + 0.5 · x4 + 2
x3 − (0.5 · x4 + 1) ≤ x8 ≤ 0.5 · x3 + 1 − x4,

And we further back-substitute to have them deﬁned over x1, x2:

2x1 ≤ x7 ≤ x1 + 2
0.5 · x1 + 1.5 · x2 − 1 ≤ x8 ≤ −0.5 · x1 + 1.5 · x2 + 1

(4)

(5)

Finally, we determine the best bound for x7 to be l7 = 0, u7 = 3 and that
for x8 to be l8 = −2, u8 = 2. Note that we have additionally drawn a dashed
orange box in Figure 2 to represent a network block. Here, we propose a block
summarization method which captures the relations between the input (leftmost)
layer and output (rightmost) layer of the block. Thus Equation (5) can function
as the block summarization for the dashed block in Figure 2; we leverage on this
block summarization to make “jumps” during back-substitution process so as to
save both running time and memory (details in Section 3).
To continue with our analysis process, we obtain next:

l9 = 0,

l10 = 0,

u10 = 2,

us
9 = x7

ls
u9 = 3,
9 = x7,
us
ls
10 = x8,
10 = 0.5 · x8 + 1
ls
11 = x9 + x10 + 1,
ls
12 = x10,
u12 = 2,

us
11 = x9 + x10 + 1
us
12 = x10,

l11 = 1,

u11 = 6,

l12 = 0,

(6)

Here, we can quickly construct the constraints of x11 deﬁned over x1, x2 by
using the block summarization derived in Equation (5); yielding 2.5·x1+1.5·x2 ≤
x11 ≤ 0.75 · x1 + 0.75 · x2 + 4.5. By doing so, our analysis will return x11 ∈ [1, 6]
and x12 ∈ [0, 2]. Note that we lose some precision when making “jumps” through
block summarization; the interval for x11 would be [1, 5.5] if we were to stick to
layer-by-layer back-substitution as originally designed in DeepPoly.

2.3 Scaling up with block summarization

As illustrated in Equation (4) and Equation (5), we conduct back-substitution
to construct symbolic constraints deﬁned over neurons at earlier layer in order to

6

Yuyi Zhong et al.

(a) Intuitive segmentation

(b) Designed segmentation

Fig. 3: Example on Segmenting Network Into Blocks

obtain a tighter concrete bound. In DeepPoly, every aﬃne layer initiates layer-by-
layer back-substitution until the input layer. Speciﬁcally, we assume a network
with n aﬃne layers, maximum N neurons per layer and consider the kth aﬃne
layer (where the input layer is indexed as 0). Every step of back-substitution
for layer k through a preceding aﬃne layer requires O(N 3) time complexity
and every back-substitution through a preceding ReLU layer requires O(N 2), it
takes O(k ·N 3) for the kth aﬃne layer to complete the back-substitution process.
Overall, DeepPoly analysis requires O(n2 · N 3) time complexity. This can take a
toll on DeepPoly when handling large networks. For example, in our evaluation
platform, DeepPoly takes around 40 hours to analyze one image on ResNet18 [9]
with 18 layers. Therefore, we propose to divide the neural networks into blocks,
and compute the summarization for each block. This summarization enables
us to charge up the back-substitution operation by speeding across blocks, as
demonstrated in Equation (5) where constraints of neuron x7 are directly deﬁned
over input neurons.

3 Network Block Summarization

3.1 Network analysis with modularization

For better scalability, we propose a modularization methodology to decrease the
computational cost as well as the memory usage, where we segment the network
into blocks and analyze each block in sequence. Speciﬁcally, we propose the
following two techniques to reduce computation steps:

1. Generate summarization between the input and output neurons for each
block, and leverage block summarization to make “jumps” during back-
substitution instead of doing it layer by layer.

2. Leverage block summarization by bounding back-substitution operation to

terminate early.
As illustrated by the simpliﬁed network representation in Figure 3, we seg-
ment a network into two blocks. We then show (1) how to generate summariza-
tion given the network fragment and (2) how to leverage the summarization to
perform back-substitution. The details are as follows.

Network segmentation. We parameterize network segmentation with a
parameter σ, which is the number of aﬃne layers required to form a block.

Scalable and Modular Robustness Analysis of Deep Neural Networks

7

I
n
p
u
t

l
a
y
e
r

7
*
7
c
o
n
v
6
4
/
2

3
*
3
p
o
o
l
/
2

3
*
3
c
o
n
v
6
4

3
*
3
c
o
n
v
6
4

3
*
3
c
o
n
v
6
4

3
*
3
c
o
n
v
6
4

3
*
3
c
o
n
v
1
2
8
/
2

3
*
3
c
o
n
v
1
2
8

3
*
3
c
o
n
v
1
2
8

3
*
3
c
o
n
v
1
2
8

3
*
3
c
o
n
v
2
5
6
/
2

3
*
3
c
o
n
v
2
5
6

3
*
3
c
o
n
v
2
5
6

3
*
3
c
o
n
v
2
5
6

3
*
3
c
o
n
v
5
1
2
/
2

3
*
3
c
o
n
v
5
1
2

3
*
3
c
o
n
v
5
1
2

3
*
3
c
o
n
v
5
1
2

a
v
e
r
a
g
e

p
o
o
l

f
u
l
l
y

c
o
n
n
e
c
t
e
d

O
u
t
p
u
t

l
a
y
e
r

block 1

block 2 block 3 block 4 block 5 block 6 block 7 block 8 block 9

block 10

Fig. 4: ResNet18 [9] and the corresponding block segmentation
For example, σ is set to 3 in Figure 3. Since each layer in the neural network
ends with the ReLU function, an intuitive segmentation solution is to divide the
network so that each block always ends at a ReLU layer, as depicted in Figure 3a.
However, doing so requires more computation during the back-substitution but
does not gain more accuracy as compared to the segmentation method in which
each block ends by an aﬃne layer.3 Therefore, we choose the later segmentation
option, as shown in Figure 3b.

Moreover, special care is required to segment a residual network. As illus-
trated in Figure 4, the most important feature of residual network is the skip
connection that enables a layer to take “shortcut connections” with a much
earlier layer [9] (displayed by the curved line). Thus, a set of layers residing in
between a layer and its skip-connected layer forms an “intrinsic” residual block,
to be used to segment the network (eg., blocks #2 to #9 in Figure 4). A more
dynamic choice of block size or block number could potentially lead to better
trade-oﬀ between speed and precision; we leave it as future work.

Back-substitution with block summarization. We present the analysis
procedure which implements our block summarization method (Section 3.2) and
bounded back-substitution heuristic in Algorithm 1. Given an input neural net-
work, it will be ﬁrst segmented (line 1) using the method described in previous
subsection. For a block consisting of layers γa, . . . , γk, the start layer and the
end layer of the block will be remembered by the markers GetStartLayer and
IsEndLayer respectively. The analysis of ReLU layers (line 3) only depends on
the preceding aﬃne layer it connects to (line 4). The computation of ReLU layer
(line 5) follows the process described in Section 2.2 and Equation (2).

To handle aﬃne layer with back-substitution, we ﬁrstly assign γpre to be the
preceding layer of γk (line 7). Then, we initialize the constraint set of γk to be the
symbolic lower and upper constraints for neurons in γk (line 8). Constraints Υk

3 An explanation of our choice to end blocks at an aﬃne layer instead of a ReLU layer

can be found in Appendix B

8

Yuyi Zhong et al.

are deﬁned over neurons in layer γpre and directly record the aﬃne transforma-
tion between layer γk and γpre. Thus, we could use Υk and the concrete bounds
of neurons in γpre to compute the initial concrete bounds for neurons in layer
γk (line 9), using the constraint evaluation mechanism described in Appendix
A. As such, we conduct back-substitution to compute the concrete bounds for
neurons in aﬃne layer γk (lines 11-27).

Algorithm 1: Overall analysis procedure in BBPoly

Input: M is the network (eg. Figure 2); τ is the back-substitution threshold;

σ is the network segmentation parameter

Annotatation: input layer of M as γin; constraint set of aﬃne layer γk as Υk;
the set of concrete bounds for neurons in layer γk ∈ M as Ck; the segmented
network model as M

else

if IsReluLayer(γk) then

if IsEndLayer(γpre) then

γpre ← PredecessorLayer(γk)
Ck ← ComputeReluLayer(γpre)

γpre ← PredecessorLayer(γk)
Υk ← GetSymbolicConstraints(γk)
Ck ← EvaluateConcreteBounds(Υk, γpre)
counterk = 0
while γpre (cid:54)= γin do

Assumption: the analysis is conducted in ascending order of the layer index
Output: tightest concrete bounds Ck computed for all layer γk ∈ M
1: M ← SegmentNetwork(M, σ)
2: for all layer γk ∈ M do
3:
4:
5:
6:
7:
8:
9:
10:
11:
12:
13:
14:
15:
16:
17:
18:
19:
20:
21:
22:
23:
24:
25:
26:
27:
28: return all Ck for all layer γk ∈ M

sym cons ← GetSymbolicConstraints(γpre)
Υk ← BacksubWithSymbolicConstraints(Υk, sym cons)
counterk ← counterk + 1
γpre ← PredecessorLayer(γpre)

sum ← ReadSummary(γpre)
Υk ← BacksubWithBlockSummary(Υk, sum)
counterk ← counterk + 1
γpre ← GetStartLayer(γpre)

temp ck ← EvaluateConcreteBounds(Υk, γpre)
Ck ← UpdateBounds(Ck, temp ck)
if counterk ≥ τ and ¬(IsEndLayer(γk) and LackSummary(γk)) then

if IsEndLayer(γk) and γpre = GetStartLayer(γk) then

StoreSummary(γk, Υk)

break

else

We have two types of back-substitution and executing either one of the two
will be considered as one step of back-substitution which leads to an increment
of the counter for layer γk (lines 15, 20):

Scalable and Modular Robustness Analysis of Deep Neural Networks

9

– If γpre is the end layer of a block, we ﬁrst read the block summary of γpre
(lines 12-13), and then call BacksubWithBlockSummary(Υk, sum) to perform
back-substituion over a block (line 14). After execution, Υk will be updated
to be deﬁned over the start layer of the block. Lastly, in preparation for next
iteration of execution, γpre is set to the start layer of the block (line 16).
– Otherwise, we conduct layer-by-layer back-substitution (lines 18-21) similarly
to DeepPoly. We obtain sym cons, the symbolic constraints built for γpre, and
call BacksubWithSymbolicConstraints(Υk, sym cons) (line 19). Then, Υk will be
updated to be deﬁned over the predecessor layer of γpre. Pertaining to block
summarization construction, if γpre and γk are the start and the end layer of
the same block, Υk will be recorded as the block summary (lines 22-23).

After generating a new set of constraints (lines 14, 19), we can compute a set
of potential concrete bounds temp ck using the new constraints Υk deﬁned over
the new γpre (line 24). Then we update Ck by the most precise bounds between
temp ck and the previous Ck (line 25) as proposed in DeepPoly, where the most
precise means the smallest upper bound and biggest lower bound.

Bounded Back-substitution. Normally, we continue new constraint con-
struction, constraint evaluation and concrete bound update for γk until the input
layer (line 11). The goal here is to explore the opportunity for cancellation of
variables in the constraints deﬁned over a particular layer. Such opportunity
may lead to attaining tighter bounds for abstract values of neurons at layer k.
Nevertheless, it is possible to terminate such back-substitution operation earlier
to save computational cost, at the risk of yielding less precise results.4 This idea
is similar in spirit to our introduction of block summarization. We term such
earlier termination as bounded back-substitution. It may appear similar to the
“limiting back-substitution” suggested in DeepPoly [5] or GPUPoly [8]. How-
ever, we note that one step in back-substitution in our approach can either be a
back-substitution over one layer or over a block summary. Please note the diﬀer-
ence between our bounded back-substitution and the limiting back-substitution
in DeepPoly [5] or GPUPoly [8]. We count either a layer back-substitution or a
back-substitution over block summary as one step of back-substitution. There-
fore, even though we bound the same number of steps of back-substitution, our
method allows us to obtain constraints deﬁned over more preceding layers com-
pared to limiting back-substitution in DeepPoly or GPUPoly.

Bounded back-substitution is incorporated in Algorithm 1, by accepting an
input τ , which is a threshold for the maximum number of steps to be taken during
back-substitution. More speciﬁcally, we initialize a counter when processing layer
γk (line 10), and increment the counter accordingly during the analysis (lines
15, 20). Finally, we end the back-substitution iteration for layer γk once the
threshold is reached (line 26).

4 As a matter of fact, our empirical evaluation (detailed in Appendix C) shows that
the degree of improvement in accuracy degrades as we explore further back into
earlier layers during back-substitution.

10

Yuyi Zhong et al.

During the construction of block summarization, we suspend this threshold
checking when γk is the end layer of a block (second test in line 26). This ensures
that the algorithm can generate its block summarization without being forced
to terminate early. In summary, suppose each block has at most (cid:96) layers, under
bounded back-substitution, the layer γk will back-substitute either (cid:96) layers (if
γk is the end layer of a block) or τ steps (if γk is not the end layer of a block).

3.2 Summarization within block

Block Summarization. The summarization captures the relationship between
the output neurons and input neurons within a block. Given a block with k
aﬃne layers inside, we formally deﬁne it as Γ = {γin, γ1, γ(cid:48)
1, . . . , γk} (e.g. block1
in Figure 3b) or Γ = {γ(cid:48)
0, γ1, γ(cid:48)
1, . . . , γk} (like block2 in Figure 3b), where γi
refers to an aﬃne layer, γin refers to the input layer and γ(cid:48)
i refers to the ReLU
layer with ReLU function applied on γi, for i ∈ {0, 1, 2, · · · , k}.

xkj

xkj

xk1

xk1

xkN

, φU

, φU

xkN
, φU

(cid:105), · · · , (cid:104)φL

Suppose the last layer γk = {xk1, · · · , xkN } contains N neurons in total. The
block summarization ΦΓ = {(cid:104)φL
(cid:105)} is deﬁned as a set of
constraint-pairs. For j ∈ {1, 2, · · · , N }, each pair (cid:104)φL
(cid:105) corresponds to the
lower and upper constraints of neuron xkj deﬁned over the neurons in the ﬁrst
layer of the block (be it an aﬃne layer γin or a ReLU layer γ(cid:48)
0). As these lower
and upper constraints encode the relationship between output neurons and input
neurons with respect to the block Γ , they function as the block summarization.
Back-substitution with Block Summarization. To explain our idea, we
present the overall back-substitution process as the matrix multiplication (cf. [8])
depicted in Figure 5. Matrix M k encodes the current constraints for neurons in
layer l deﬁned over neurons in previous layer k, where 1 ≤ k < l. The cell indexed
by the pair (xl
hm
in layer l and neuron xk
ic in layer k. The same notation also applies for matrix F k
and M k−1, where F k denotes next-step back-substitution and M k−1 represents
a newly generated constraint for neurons in layer l deﬁned over neurons in the
preceding layer k − 1. As we always over-approximate ReLU function to a linear
function, without loss of generality, we therefore discuss further by considering
a network as a composition of aﬃne layers.

ic) in the matrix records the coeﬃcient between neuron xl

hm, xk

xk
i1

xk
i2

∗

∗

∗

∗

· · ·

· · ·

· · ·

xk
is

∗

∗

· · ·

· · ·

· · ·

· · ·

∗

∗

· · ·

∗

xl

h1

xl

h2

· · ·
xl
ht

xk−1
j1

xk−1
j2

· · ·

xk−1
jr

xk−1
j1

xk−1
j2

· · ·

xk−1
jr

◦

xk
i1

xk
i2

· · ·
xk
is

∗

∗

∗

∗

· · ·

· · ·

∗

∗

· · ·

· · ·

· · ·

· · ·

∗

∗

· · ·

∗

=

xl

h1

xl

h2

· · ·
xl
ht

∗

∗

∗

∗

· · ·

· · ·

∗

∗

· · ·

· · ·

· · ·

· · ·

∗

∗

· · ·

∗

M k

F k

M k−1

Fig. 5: Back-substitution process can be represented as matrix multiplication
with constant terms (e.g. biases) being omitted, cf. [8]

Scalable and Modular Robustness Analysis of Deep Neural Networks

11

Next, we describe how to perform back-substitution with the generated block
summarization. After completing the layer-by-layer back-substitution process
within a given block (take block 2 in Figure 3b as example), we obtain constraints
of neurons in the aﬃne layer 6 (γ6) deﬁned over neurons in the ReLU layer 3 (γ(cid:48)
3),
which corresponds to M k. This matrix is then multiplied with matrix F k1 which
captures the aﬃne relationship between neurons in layer γ(cid:48)
3 and γ3 (this aﬃne
relationship is actually an over-approximation since γ(cid:48)
3 = ReLU(γ3)), followed by
another multiplication with matrix F k2 constructed from block summarization
for block 1 (in Figure 3b), denoted here by ΦΓ1. ΦΓ1 is a set of constraints for
neurons in the aﬃne layer 3 (γ3) deﬁned over neurons in the input layer (γin)
and is computed already during the analysis of block 1. Hence, the resulting
matrix M k ◦ F k1 ◦ F k2 encodes the coeﬃcients of neurons in layer γ6 deﬁned
over neurons in layer γin. Through this process, we achieve back-substitution of
the constraints of layer γ6 to the input layer.

Memory Usage and Time Complexity. In the original method, the mem-
ory usage of DeepPoly is high since it associates all neurons with symbolic con-
straints and maintains all symbolic constraints throughout the analysis process
for the sake of layer-by-layer back-substitution. In work of [10] and [11], they
all faced with out-of-memory problem when running DeepPoly on their eval-
uation platform. In our block summarization approach, a block captures only
the relationship between its end and start layers. Consequently, all the symbolic
constraints for intermediate layers within the block can be released early once
we complete the block summarization computation (illustrated by a delete icon
in the layer in Figure 3). Thus our method requires less memory consumption
when analyzing the same network, and the memory usage can also be controlled
using the network segmentation parameter σ.

For time complexity, consider a network with n aﬃne layers and each layer
has at most N neurons, DeepPoly’s complexity is O(n2·N 3). In our method, with
bounded back-substitution (detail in Section 3.1), we can bound the number of
steps for back-substitution to a constant for each layer. Thus the time complex-
ity can be reduced to O(n · N 3). Without bounded back-substitution, we have
constant-factor reduction in time complexity, yielding the same O(n2 · N 3).

3.3 Summarization deﬁned over input layer

Previously, Section 3.2 describes a back-substitution mechanism on “block-by-
block” basis. To further simplify the back-substitution process and save even
more on the execution time and memory, we also design a variation of block
summarization that is deﬁned over the input layer. As the overall procedure of
back-substitution with summarization deﬁned over input layer is similar to the
block summarization described in Algorithm 1, we provide the algorithm for this
new summary in Appendix G.

Summary over Input. Just as in Section 3.2, the summary-over-input is
, φU
still formulated as ΦΓ . However, (cid:104)φL
(cid:105) corresponds to constraints of neuron
xjk
xjk which are now deﬁned over the input neurons. To generate summary for block
Γi, we ﬁrstly do layer-by-layer analysis within the block, then we back-substitute

xjk

12

Yuyi Zhong et al.

further with the summary for block Γi−1 which is deﬁned over input neurons,
thus we get ΦΓi deﬁned over the input neurons.

Back-substitution with Summary over Input. The back-substitution
process also follows the formulation described in Section 3.2. The resulting ma-
trix M k ◦ F k1 ◦ F k2 will directly be deﬁned over input neurons since F k2 is the
summary of preceding block directly deﬁned over input neurons.

Memory Requirement and Time Complexity. Once the summary gen-
eration for block Γi has completed, all symbolic constraints and summaries from
previous i − 1 blocks could be released, only the input layer needs to be kept. For
time complexity, each layer back-substitutes at most l + 1 steps (if each block
has maximum l layers), the time complexity will be O(n · N 3).

4 Experiment

We have implemented our proposed method in a prototype analyzer called
BBPoly, which is built on top of the state-of-the-art veriﬁer DeepPoly. Then,
we conducted extensive experiments to evaluate the performance of both our
tool and DeepPoly, in terms of precision, memory usage and runtime. In the
following subsections, we will describe the details of our experiment.

4.1 Experiment Setup

We propose two types of block summary in our BBPoly system:

– Block summarization as described in Section 3.2. It can be supplemented
with bounded back-substitution heuristic in Section 3.1 to facilitate the anal-
ysis even more for extremely large network;

– Block summary deﬁned over input layer that is introduced in Section 3.3

We compare our methods with the state-of-the-art system DeepPoly [5], on
top of which our prototype tool is built. DeepPoly is publicly available at the
GitHub repository of the ERAN system [12]. On the other hand, we do not
compare with existing approach using MILP solving [13] since the latter can
only handle small networks, such as MNIST/CIFAR10 networks with 2 or 3
hidden layers while our BBPoly can analyze large networks of up to 34 hidden
layers.

Evaluation datasets. We choose the popular MNIST [14] and CIFAR10
[15] image datasets that are commonly used for robustness veriﬁcation. MNIST
contains gray-scale images with 28 × 28 pixels and CIFAR10 consists of RGB
3-channel images of size 32 × 32. Our test images are provided from DeepPoly
paper where they select out the ﬁrst 100 images of the test set of each dataset.
The test images are also publicly available at [12].

Evaluation platform and networks. The evaluation machine is equipped
with a 2600 MHz 24 core GenuineIntel CPU with 64 GB of RAM. We con-
ducted experiments on networks of various sizes as itemized in Table 1; these
include fully-connected, convolutional and (large sized) residual networks where
the number of hidden neurons is up to 967K. All networks use ReLU activa-
tion, and we list the layer number and number of hidden neurons in the table.

Scalable and Modular Robustness Analysis of Deep Neural Networks

13

Speciﬁcally, the networks whose names suﬃxed by “DiﬀAI” were trained with
adversarial training DiﬀAI [16]. These benchmarks are also collected from [12].
Veriﬁed robustness property. We verify robustness property against the
L∞ norm attack [17] which is paramterized by a constant (cid:15) of perturbation.
Originally, each pixel in an input image has a value pi indicating its color in-
tensity. After applying the L∞ norm attack with a certain value of (cid:15), each pixel
now corresponds to an intensity interval [pi − (cid:15), pi + (cid:15)]. Therefore we constructed
n
an adversarial region deﬁned as
i=1[pi − (cid:15), pi + (cid:15)]. Our job is to verify that
whether a given neural network can classify all perturbed images within the
given adversarial region as the same label as of the original input image. If so,
we conclude that robustness is veriﬁed for this input image, the given perturba-
tion (cid:15) and the tested network. For images that fail the veriﬁcation, due to the
over-approximation error, we fail to know if the robustness actually holds or not,
thus we report that the results are inconclusive. We set a 3-hour timeout for the
analysis of each image, if the veriﬁer fails to return the result within 3 hours, we
also state that the result is inconclusive.

(cid:16)

Table 1: Experimental Networks
Dataset #Layer #Neurons
Neural Network
MNIST 9 200
MNIST
ﬀcnRELU Point 6 500 MNIST
MNIST
convBigRELU
convSuperRELU
MNIST
ﬀcnRELU Point 6 500 CIFAR10
CIFAR10
convBigRELU
CIFAR10
SkipNet18 DiﬀAI
CIFAR10
ResNet18 DiﬀAI
CIFAR10
ResNet34 DiﬀAI

1,610
3,000
48,064
88,544
3,000
62,464
558K
558K
967K

Type
fully connected
fully connected
convolutional
convolutional
fully connected
convolutional
residual
residual
residual

9
6
6
6
6
6
18
18
34

Candidates
97
100
95
99
56
60
41
46
39

4.2 Experiments on fully-connected and convolutional networks

We ﬁrstly present the experiment results on fully-connected and convolutional
networks for both the MNIST and CIFAR10 datasets. We set the block segmen-
tation parameter to be 3;5 this means there will be 3 aﬃne layers contained in
a block. We conduct experiments on both block-summarization and summary-
over-input methods. And the bounded back-substitution heuristic is disabled for
this part of experiments. We set six diﬀerent values of perturbation (cid:15) for diﬀerent
networks according to the settings in DeepPoly (details in Appendix D).

The veriﬁed precision is computed as follows:

Number of veriﬁed images
Number of candidate images

(7)

5 We have conducted preliminary experiments with the eﬀectiveness of having diﬀerent
block sizes; the results are available in Appendix F. A more thorough investigation
on the eﬀectiveness is left as future work.

14

Yuyi Zhong et al.

where candidate images are those which have been correctly classiﬁed by a net-
work. The numbers of candidate images for each network are presented in Ta-
ble 1. Figure 6 shows the precision comparison among diﬀerent methods on
MNIST networks, and Figure 7 shows the precision on CIFAR10 networks. Due
to page constraint, full details of the precision and average execution time per
image for the experiments are recorded in Table 5 of Appendix E. As expected,
DeepPoly ≥ BBPoly (using block summary) ≥ BBPoly (using input summary)
with respect to precision and execution time. Apart from MNIST 9 200 network,
our methods actually achieve comparable precision with DeepPoly.

With regard to the execution time, for larger networks such as the three con-
volutional networks experimented, our block-summarization method can save
around half of the execution time in comparison with that by DeepPoly. In-
terested reader may wish to refer to Table 5 in Appendix E for detail. The
execution time can be signiﬁcantly reduced for even larger network, such as the
deep residual networks, as demonstrated in Section 4.3.

(a)

(b)

(c)

(d)

Fig. 6: Veriﬁed robustness precision comparison between our BBPoly system and
DeepPoly for MNIST fully-connected and convolutional networks

Scalable and Modular Robustness Analysis of Deep Neural Networks

15

(a)

(b)

Fig. 7: Veriﬁed robustness precision comparison between our BBPoly system and
DeepPoly for CIFAR10 fully-connected and convolutional networks.

4.3 Experiments on residual networks

Network description. We select three residual networks that have 18 or 34
layers and contain up to almost one million neurons as displayed in Table 1. The
SkipNet18, ResNet18 and ResNet34 are all trained with DiﬀAI defence.

Perturbation size. DeepPoly is not originally designed to handle such large
networks and is inconclusive within our timeout. However, an eﬃcient GPU
implementation of DeepPoly (called GPUPoly) [8] is proposed for much larger
networks. GPUPoly achieves the same precision as DeepPoly and it selects (cid:15) =
8/255 for our experimental residual networks. Thus we follow the same setting
as in GPUPoly. Unfortunately, GPUPoly does not run in one-CPU environment,
and thus not comparable with our experimental setting.

Precision comparison with DeepPoly. We only conduct robustness ver-
iﬁcation on candidate images as in Section 4.2. We set our baselines to be
block-summarization method with bounded back-substitution in four steps, and
summary-over-input method. The number of candidate images, veriﬁed images
and the average execution time per image for our experiment are listed in Ta-
ble 2, where column “BlkSum 4bound” refers to block-summarization method
together with bounded back-substitution in four steps and “Input Sum” refers to
our summary-over-input method. As illustrated in Table 2, the summary-over-
input method veriﬁes more or at least the same number of images compare to the
block-summarization method with bounded back-substitution but requires less
execution time, which demonstrates the competitive advantage of our summary-
over-input method.

Veriﬁed precision is computed using formula 7 with data from Table 2; the
results are displayed in Table 3 for residual networks. DeepPoly fails to verify any
image within the timeout of 3 hours in our evaluation platform (indicated by ‘-’)

16

Yuyi Zhong et al.

Table 2: The number of veriﬁed images and average execution time per image
for CIFAR10 residual networks

Neural Net

(cid:15)

Cand-
idates

BBPoly
BBPoly
(BlkSum 4bound)
(Input Sum)
Veriﬁed Time(s) Veriﬁed Time(s) Veriﬁed Time(s)

DeepPoly

SkipNet18 DiﬀAI 8/255

ResNet18 DiﬀAI 8/255

ResNet34 DiﬀAI 8/255

41

46

39

35

29

21

4027.08

3212.26

2504.89

36

29

22

1742.93

984.43

1296.78

-

-

-

-

-

-

Table 3: Veriﬁed precision comparison computed from Table 2

Neural Net

(cid:15)

SkipNet18 DiﬀAI 8/255

ResNet18 DiﬀAI 8/255

ResNet34 DiﬀAI 8/255

BBPoly
(BlkSum 4bound)
85.3%

BBPoly
(Input Sum)
87.8%

63.0%

53.8%

63.0%

56.4%

DeepPoly

-

-

-

whereas our method yields reasonable veriﬁed precision within this time limit,
supporting our hypothesis that BBPoly can scale up to analyze large networks
with fair execution time and competitive precision.

Time comparison with DeepPoly. To the best of our knowledge, there is
no public experimental result of using DeepPoly to analyze ResNets. We initially
used DeepPoly to analyze input images in our dataset with a smaller (cid:15) = 0.002
for ResNet18 DiﬀAI. Since DeepPoly took around 29 hours to complete the
veriﬁcation of an image, we could not aﬀord to run DeepPoly for all 100 test
images. In contrast, our summary-over-input method takes only 1319.66 seconds
(≈ 22 minutes) for the same image. We also try to analyze ResNet18 DiﬀAI with
(cid:15) = 8/255 according to the original perturbation setting, and DeepPoly takes
around 41 hours to complete the veriﬁcation of one image. On the other hand,
our block-summarization with bounded back-substitution in 4 steps uses average
3212.26 seconds (≈ 54 minutes) for one image.

Memory comparison with DeepPoly. We mention earlier that our meth-
ods utilize less memory. To empirically testify this, we compare the peak mem-
ory usage between DeepPoly and summary-over-input method with respect to
ResNet18 DiﬀAI, on the ﬁrst input image in our dataset and (cid:15) = 8/255. We use
the following command to check the peak memory usage of our analysis process:
$ grep VmPeak /proc/$PID/status

According to the result, DeepPoly takes up to 20.6 GB of memory while our
summary-over-input method needs much less memory. It takes only 11.4 GB of
memory, which is 55% of the memory usage of DeepPoly.

5 Discussion

We now discuss the limitation of our work. There are two limitations as follows.
Firstly, although the experimental results in Section 4.2 demonstrate that our

Scalable and Modular Robustness Analysis of Deep Neural Networks

17

tool yields comparable precision with DeepPoly for majority of the tested net-
works, it still signiﬁcantly less precise than DeepPoly in certain benchmarks,
such as the MNIST 9 200 network. We have explained earlier that this loss of
precision is due to our current block summarization technique which cannot cap-
ture a precise enough relationship between neurons in the start and the end layer
of a block. In the future, we aim to generate a more tightened summarization to
reduce the over-approximation error and increase the precision of our analyzer.
Secondly, our current construction of a block is simple and straightforward. We
currently ﬁx the block size to be a constant (eg. 3), and have not considered the
intricate information related to the connectivity between layers when choosing
the block size. For future work, we will investigate how to utilize such informa-
tion to assign the block size dynamically. This could potentially help the analysis
to ﬁnd a better trade-oﬀ between speed and precision.

Our proposed method on block summarization could potentially be applied
to other neural network veriﬁcation techniques to enhance their scalability. For
instance, in constraint-based veriﬁcation, the network is formulated by the con-
junction of the encoding of all neurons and all connections between neurons [18].
This heavy encoding is exact but lays a huge burden on the constraint solver.
Following our block summary method, we could generate over-approximate en-
coding of the network block to summarize the relationship between the start
layer and end layer of the block. This could potentially lead to a signiﬁcant de-
crease in the number of constraints and make such analyzer more amenable to
handle larger networks.

6 Related Work

Existing works on analyzing and verifying the robustness of neural networks can
be broadly categorized as complete or incomplete methods: given suﬃcient time
and computational resource, a complete veriﬁer can always provide a deﬁnite
answer (yes or no) indicating whether a neural network model is robust or not,
while an incomplete veriﬁer might return an unknown answer.

Typical complete methods include the works in [6,7,19,20], which encode the
veriﬁcation problems into arithmetic constraints, and utilize the corresponding
sound and complete solvers to solve them. In particular, the techniques in [6,19]
are based on MILP (mixed integer liner program) solvers, while the veriﬁers
in [7, 20] use SMT (satisﬁability modulo theory) solvers in the theory of linear
real arithmetic with ReLU constraints. Although these solvers can give precise
answers, they are also costly when handling a large set of constraints with many
variables. Hence, it is diﬃcult for complete veriﬁers to scale up.

In a diﬀerent approach, the works [3–5] introduce incomplete methods which
over-approximate the behavior of neural networks using techniques like abstrac-
tion interpretation, reachability analysis etc. Even though they might lose preci-
sion in certain situations, they are more scalable than those complete methods.
The abstract domain devised for abstract interpretation is the essential part of
the analysis. There has been progress in the design of abstract domains, from in-
terval domains in [3] to zonotope domains in [4] and ﬁnally to polyhedral domains

18

Yuyi Zhong et al.

in [5]. These domains allow the veriﬁers to prove more expressive speciﬁcations,
such as the robustness of neural networks, and handle more complex networks,
like the deep convolutional networks. Especially, the polyhedral domain in [5]
can scale up the performance of the veriﬁer DeepPoly to handle large networks.
Recently, there have been also eﬀorts on combining both incomplete method
(such as abstraction) and complete method (MILP encoding and solving), such
as the works [19] and [21].

All above-mentioned veriﬁcation methods are actually doing qualitative veri-
ﬁcation by considering only two cases: whether the network satisﬁes the property,
or not. In most recent years, researchers have been looking into quantitative ver-
iﬁcation to check how often a property is satisﬁed by a given network under a
given input distribution. For instance, the work [10] examines if majority portion
of the input space still satisﬁes the property with a high probability.

7 Conclusion

We have proposed the block summarization and bounded back-substitution to
reduce the computational steps during back-substitution process, making it more
amenable for analyzer to handle larger network with limited computational re-
sources, such as having only CPU setup. We instantiated our idea on top of Deep-
Poly and implement a system called BBPoly. Experiment shows that BBPoly
can achieve the veriﬁed precision comparable to DeepPoly but save both running
time and memory allocation. Furthermore, our system is capable of analyzing
large networks with up to one million neurons while DeepPoly cannot conclude
within a decent timeout. We believe that our proposal can assist with eﬃcient
analysis and be applied to other methods for better scalability.

8 Acknowledgement

We are grateful to Gagandeep Singh and Mark Niklas M¨uller for their prompt
and patient answer to our queries on DeepPoly/GPUPoly. This research is sup-
ported by a Singapore Ministry of Education Academic Research Fund Tier 1
T1-251RES2103. The second author is supported by both a Singapore Ministry
of Education Academic Research Fund Tier 3 MOE2017-T3-1-007 and a Sin-
gapore National Research Foundation Grant R-252-000-B90-279 for the project
Singapore Blockchain Innovation Programme.

References

1. Kui Ren, Tianhang Zheng, Zhan Qin, and Xue Liu. Adversarial attacks and de-

fenses in deep learning. Engineering, 6(3):346–360, 2020.

2. Xiaoyong Yuan, Pan He, Qile Zhu, and Xiaolin Li. Adversarial examples: At-
tacks and defenses for deep learning. IEEE Transactions on Neural Networks and
Learning Systems, 30(9):2805–2824, 2019.

3. Luca Pulina and Armando Tacchella. An abstraction-reﬁnement approach to ver-
iﬁcation of artiﬁcial neural networks. In International Conference on Computer
Aided Veriﬁcation (CAV), pages 243–257. Springer, 2010.

Scalable and Modular Robustness Analysis of Deep Neural Networks

19

4. Timon Gehr, Matthew Mirman, Dana Drachsler-Cohen, Petar Tsankov, Swarat
Chaudhuri, and Martin T. Vechev. AI2: safety and robustness certiﬁcation of
neural networks with abstract interpretation.
In IEEE Symposium on Security
and Privacy (SP), pages 3–18. IEEE Computer Society, 2018.

5. Gagandeep Singh, Timon Gehr, Markus P¨uschel, and Martin T. Vechev. An ab-
stract domain for certifying neural networks. Proceedings of the ACM on Program-
ming Languages, 3(POPL):41:1–41:30, 2019.

6. Vincent Tjeng, Kai Y. Xiao, and Russ Tedrake. Evaluating robustness of neu-
In International Conference on

ral networks with mixed integer programming.
Learning Representations (ICLR). OpenReview.net, 2019.

7. Guy Katz, Clark W. Barrett, David L. Dill, Kyle Julian, and Mykel J. Kochen-
derfer. Reluplex: An eﬃcient SMT solver for verifying deep neural networks. In
International Conference on Computer Aided Veriﬁcation (CAV), pages 97–117.
Springer, 2017.

8. Christoph M¨uller, Gagandeep Singh, Markus P¨uschel, and Martin T. Vechev. Neu-

ral network robustness veriﬁcation on gpus. CoRR, abs/2007.10868, 2020.

9. Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learn-
ing for image recognition. In IEEE Conference on Computer Vision and Pattern
Recognition (CVPR), pages 770–778, 2016.

10. Teodora Baluta, Zheng Leong Chua, Kuldeep S. Meel, and Prateek Saxena. Scal-
able quantitative veriﬁcation for deep neural networks. pages 312–323. IEEE, 2021.
11. Hoang-Dung Tran, Stanley Bak, Weiming Xiang, and Taylor T. Johnson. Veriﬁ-
cation of deep convolutional neural networks using imagestars. In International
Conference on Computer Aided Veriﬁcation (CAV), pages 18–42. Springer, 2020.
12. ETH. ETH Robustness Analyzer for Neural Networks (ERAN), 2021. https:

//github.com/eth-sri/eran. Retrieved on June 18th, 2021.

13. Elena Botoeva, Panagiotis Kouvaros, Jan Kronqvist, Alessio Lomuscio, and Ruth
Misener. Eﬃcient veriﬁcation of relu-based neural networks via dependency analy-
sis. In The Thirty-Fourth AAAI Conference on Artiﬁcial Intelligence, AAAI 2020,
The Thirty-Second Innovative Applications of Artiﬁcial Intelligence Conference,
IAAI 2020, The Tenth AAAI Symposium on Educational Advances in Artiﬁcial
Intelligence, EAAI 2020, New York, NY, USA, February 7-12, 2020, pages 3291–
3299. AAAI Press, 2020.

14. Yann LeCun and Corinna Cortes. MNIST handwritten digit database. 2010.
15. Alex Krizhevsky, Vinod Nair, and Geoﬀrey Hinton. Cifar-10 (canadian institute

for advanced research).

16. Matthew Mirman, Timon Gehr, and Martin T. Vechev. Diﬀerentiable abstract
interpretation for provably robust neural networks. In International Conference
on Machine Learning (ICML), pages 3575–3583, 2018.

17. Nicholas Carlini and David A. Wagner. Towards evaluating the robustness of
neural networks. In IEEE Symposium on Security and Privacy (SP), pages 39–57,
2017.

18. Aws Albarghouthi. Introduction to Neural Network Veriﬁcation. veriﬁeddeeplearn-

ing.com, 2021. http://verifieddeeplearning.com.

19. Elena Botoeva, Panagiotis Kouvaros, Jan Kronqvist, Alessio Lomuscio, and Ruth
Misener. Eﬃcient veriﬁcation of relu-based neural networks via dependency anal-
ysis. pages 3291–3299. AAAI Press, 2020.

20. Guy Katz, Derek A. Huang, Duligur Ibeling, Kyle Julian, Christopher Lazarus,
Rachel Lim, Parth Shah, Shantanu Thakoor, Haoze Wu, Aleksandar Zeljic,

20

Yuyi Zhong et al.

David L. Dill, Mykel J. Kochenderfer, and Clark W. Barrett. The marabou frame-
work for veriﬁcation and analysis of deep neural networks. In International Con-
ference on Computer Aided Veriﬁcation (CAV), pages 443–452. Springer, 2019.
21. Gagandeep Singh, Timon Gehr, Markus P¨uschel, and Martin T. Vechev. Boost-
In International Conference on

ing robustness certiﬁcation of neural networks.
Learning Representations (ICLR). OpenReview.net, 2019.

Scalable and Modular Robustness Analysis of Deep Neural Networks

21

A Concrete Bound Computation from Symbolic

Constraints

As can be seen from the illustrative example in Section 2.2, the symbolic con-
straints are used to compute the concrete bounds of aﬃne neurons and this is the
stratagem proposed in DeepPoly to achieve tighter intervals. To do so, it requires
us to evaluate the minimum or maximum value for the symbolic constraints.

For symbolic lower constraint of neuron m, suppose we have m ≥ w1 · x1 +
· · · + wn · xn. Note that x1, . . . , xn represent the neurons from some preceding
layer. Since the analysis of the preceding neurons has completed, we know the
concrete bounds for each xi ∈ {x1, . . . , xn} and have xi ∈ [li, ui].

To evaluate the expression w1 ·x1 +· · ·+wn ·xn for lower bound computation,
we will calculate the minimum value β of w1 · x1 + · · · + wn · xn. The minimum
value is computed by the following rules:

– For positive coeﬃcient wi, replace xi with the concrete lower bound li.
– For negative coeﬃcient wi, replace xi with the concrete upper bound ui.

Therefore, we have m ≥ w1 · x1 + · · · + wn · xn ≥ β, and β functions as the
concrete lower bound of neuron m. Similarly for the upper bound computation
of m, we have symbolic constraint m ≤ w1 · x1 + · · · + wn · xn and calculate the
maximum value δ of w1 · x1 + · · · + wn · xn by:

– For positive coeﬃcient wi, replace xi with the concrete upper bound ui
– For negative coeﬃcient wi, replace xi with the concrete lower bound li

So we have m ≤ w1 · x1 + · · · + wn · xn ≤ δ and δ is the concrete upper bound

of neuron m.

B Explanation why a block is ended by an aﬃne layer

Without loss of generality, suppose the symbolic lower constraint for a neuron
m at a layer is expressed as a linear combination of neurons (cid:126)xi at a preceding
layer: m ≥ w1 · x1 + · · · + wn · xn. In a back-substitute step, (cid:126)xi will be replaced
by its own symbolic lower constraint expressed in terms of its preceding layer:
xi ≥ ui1 ·y1 +· · ·+uin ·yn. Expressing the symbolic lower constraint of m in terms
of (cid:126)yj would require n2 computation for combining coeﬃcients (cid:126)wi and (cid:126)uij. On the
other hand, some of these combining computations can be eliminated if the ReLU
neurons connecting neuron m and layer (cid:126)xi can be set to 0. As it is not infrequent
for ReLU neuron values to be set to 0 (for its symbolic lower constraint), by
letting a block to end at an aﬃne layer and begin at an ReLU layer, we increase
the opportunity to leverage on the symbolic lower constraint of the ReLU neurons
being 0, which will lead to elimination of n2 coeﬃcient computation when back
propagating from one block to another. Thus, to preserve the back-substitution
approach in DeepPoly and speed up our back-substitution over block summary,
we elect to end summary block with an aﬃne layer.

22

Yuyi Zhong et al.

C Empirical proof of bounded back-substitution heuristic

To empirically test the idea of bounded back-substitution, we experiment on the
networks enumerated in Table 4, where the classiﬁed dataset, the layer num-
ber and the number of hidden neurons are listed as well. Please be noted that
one actual layer in the network corresponds to one aﬃne layer and one ReLU
layer in the analysis. Therefore networks with 9 layers totally have 18 layers in
DeepPoly’s representation, so setting max backsub steps as 18 meaning that we
will NOT bound the back-substitution process and it will be equal to DeepPoly
method.

Table 4: Networks for early termination experiment

Dataset #Layer #hidden neurons

Type
Network
fully connected MNIST
mnist 9 200
9
mnist 9 100
fully connected MNIST
9
mnist ﬀcnRELU Point 6 500 fully connected MNIST
6
fully connected MNIST
mnist 6 100
6
convolutional MNIST
convSuperRELU mnist
6
6
convolutional MNIST
convBigRELU mnist
fully connected CIFAR10 6
cifar 6 100
CIFAR10 6
convolutional
convBigRELU cifar

1,610
810
3,000
510
88,544
48,064
610
62,464

Figure 8 demonstrates the tendency of interval length of output neuron with
respect to diﬀerent setting of max backsub steps. The interval length is averaged
over diﬀerent input images, diﬀerent perturbations applied and diﬀerent output
neurons. From the outline, we can see that as we allow deeper back-substitution,
the beneﬁt will become less and less signiﬁcant.

D Perturbation size for fully-connected and convolutional

networks

We set six diﬀerent values for perturbation size (cid:15) according to the following
settings made in DeepPoly paper:

– (cid:15) ∈ {0.005, 0.01, 0.015, 0.02, 0.025, 0.03} for MNIST fully-connected net-

works;

– (cid:15) ∈ {0.02, 0.04, 0.06, 0.08, 0.1, 0.12} for MNIST convolutional networks;
– (cid:15) ∈ {0.0002, 0.0004, 0.0006, 0.0008, 0.001, 0.0012} for CIFAR10 fully-

connected networks;

– (cid:15) ∈ {0.002, 0.004, 0.006, 0.008, 0.01, 0.012} for CIFAR10 convolutional

networks.

Note that, since the speciﬁed (cid:15) set couldn’t really diﬀerentiate between various (cid:15)
and methods for MNIST convolutional big network as demonstrated in Figure 6c,
so we choose a larger set (cid:15) ∈ {0.12, 0.14, 0.16, 0.18, 0.2, 0.22} for MNIST
convolutional super network.

Scalable and Modular Robustness Analysis of Deep Neural Networks

23

(a)

(b)

(c)

(d)

Fig. 8: Empirical result of bounded back-substitution

E Precision and time comparison in table

We also demonstrate the precision and average execution time per image in
tables. The experimental result of fully-connected and convolutional networks
for MNIST/CIFAR10 is given in Table 5. In the table, we record the number of
veriﬁed images out of 100 test images together with the average execution time
per image (which is recorded within the square brackets) for diﬀerent experiment
settings and three diﬀerent methods. The three methods include

– DeepPoly;
– Our method with block summarization, which is denoted as “Block sum”;
– Our summary-over-input method, denoted as “Input sum”.

F Additional experimental result on the neural network
MNIST 9x200 with diﬀerent block sizes of 3, 4, 5

In Table 6, the record “83[2.658]” indicates that BBPoly veriﬁes robustness for
83 images, with average execution time 2.658 seconds for each image.

24

Yuyi Zhong et al.

Table 5: Precision for convolutional and fully-connected networks

Neural
Network

Perturbation
Size

MNIST
9 200

MNIST ﬀcn
Point 6 500

MNIST
convBig
RELU

MNIST
convSuper
RELU

CIFAR10
ﬀcn Point
6 500

CIFAR10
convBig
RELU

0.005
0.01
0.015
0.02
0.025
0.03
0.005
0.01
0.015
0.02
0.025
0.03
0.02
0.04
0.06
0.08
0.1
0.12
0.12
0.14
0.16
0.18
0.2
0.22
0.0002
0.0004
0.0006
0.0008
0.001
0.0012
0.002
0.004
0.006
0.008
0.01
0.012

DeepPoly

#Veriﬁed Images out of 100 Images and Time(s)
BBPoly
(Block sum)
74[2.773]
18[3.170]
3[3.544]
1[3.838]
0[4.026]
0[4.135]
99[5.494]
97[5.452]
94[5.634]
82[6.065]
58[6.668]
38[7.489]
95[18.87]
95[18.74]
95[18.85]
95[18.92]
93[18.59]
92[19.38]
96[75.22]
77[78.12]
10[84.84]
0[93.19]
0[102.6]
0[114.4]
55[14.43]
53[14.53]
52[14.76]
46[14.74]]
43[14.88]
43[15.07]
51[42.45]
42[41.38]
31[41.88]
21[41.87]
12[41.87]
9[41.79]

BBPoly
(Input sum)
66[2.644]
12[3.068]
2[3.368]
0[3.566]
0[3.751]
0[3.851]
99[5.336]
97[5.467]
94[5.622]
82[5.966]
55[6.717]
37[7.390]
95[12.40]
95[17.76]
95[18.31]
95[17.37]
93[17.96]
91[18.21]
95[73.94]
74[78.12]
6[82.90]
0[90.12]
0[98.21]
0[109.8]
55[15.18]
53[15.18]
49[15.08]
46[15.16]
43[15.36]
43[15.35]
51[37.46]
42[37.49]
31[37.78]
21[37.94]
12[38.32]
9[38.87]

92[3.299]
67[3.598]
29[4.011]
8[4.466]
3[4.720]
2[4.897]
99[7.244]
98[7.357]
95[7.673]
89[8.039]
71[8.754]
49[9.545]
95[33.96]
95[33.12]
95[33.68]
95[34.21]
94[33.55]
93[34.06]
96[133.5]
81[138.7]
28[148.7]
0[159.5]
0[179.8]
0[197.9]
55[22.32]
53[22.63]
53[22.75]
52[22.71]
48[22.76]
46[22.70]
51[89.07]
46[89.69]
31[91.04]
24[91.26]
15[91.58]
11[92.56]

G Algorithm of analysis with summary-over-input

method

We present the overall procedure of analysis with summary-over-input in Algo-
rithm 2. The algorithm is very similar to Algorithm 1, the diﬀerences between
the two algorithms are:

Scalable and Modular Robustness Analysis of Deep Neural Networks

25

Table 6: Precision and execution time for diﬀerent block sizes

Neural
Network

Perturbation
Size

MNIST
9 200
blk size=3

MNIST
9 200
blk size=4

MNIST
9 200
blk size=5

0.005
0.01
0.015
0.02
0.025
0.03
0.005
0.01
0.015
0.02
0.025
0.03
0.005
0.01
0.015
0.02
0.025
0.03

DeepPoly

#Veriﬁed Images out of 100 Images and Time(s)
BBPoly
(Block sum)
74[2.773]
18[3.170]
3[3.544]
1[3.838]
0[4.026]
0[4.135]
83[2.658]
29[3.174]
6[3.561]
2[3.725]
0[3.910]
0[3.960]
88[2.617]
37[3.041]
9[3.504]
2[3.712]
0[3.831]
0[3.930]

BBPoly
(Input sum)
66[2.644]
12[3.068]
2[3.368]
0[3.566]
0[3.751]
0[3.851]
82[2.683]
29[3.118]
6[3.530]
2[3.767]
0[3.900]
0[3.968]
88[2.643]
37[3.047]
9[3.473]
2[3.729]
0[3.854]
0[3.940]

92[3.299]
67[3.598]
29[4.011]
8[4.466]
3[4.720]
2[4.897]
92[3.299]
67[3.598]
29[4.011]
8[4.466]
3[4.720]
2[4.897]
92[3.299]
67[3.598]
29[4.011]
8[4.466]
3[4.720]
2[4.897]

– The bounded back-substitution heuristic is not involved in summary-over-

input method;

– In the summary-over-input method, the summary is directly deﬁned over
the actual input layer of the network in stead of the start layer of the given
block. Therefore, after back-substitution with summary, the new constraint
will directly be deﬁned over input layer (line 13-14);

– If γk is the end layer of a block and the current preceding layer γpre is
the input layer of the network, we then store the current Υk as the block
summary of layer γk (line 19-20).

26

Yuyi Zhong et al.

Algorithm 2: Overall analysis procedure with summary-over-input

else

γpre ← PredecessorLayer(γk)
Ck ← ComputeReluLayer(γpre)

Input: M is the network (eg. Figure 2);
σ is the network segmentation parameter
Annotatation: input layer of M as γin;
constraint set of aﬃne layer γk as Υk;
the set of concrete bounds for neurons in layer γk ∈ M as Ck;
the segmented network model as M
Assumption: the analysis is conducted in ascending order of the layer index
Output: tightest concrete bounds Ck computed for all layer γk ∈ M
1: M ← SegmentNetwork(M, σ)
2: for all layer γk ∈ M do
3:
if IsReluLayer(γk) then
4:
5:
6:
7:
8:
9:
10:
11:
12:
13:
14:
15:
16:
17:
18:
19:
20:
21:
22:
23: return all Ck for all layer γk ∈ M

γpre ← PredecessorLayer(γk)
Υk ← GetSymbolicConstraints(γk)
Ck ← EvaluateConcreteBounds(Υk, γpre)
while γpre (cid:54)= γin do

sym cons ← GetSymbolicConstraints(γpre)
Υk ← BacksubWithSymbolicConstraints(Υk, sym cons)
γpre ← PredecessorLayer(γpre)

sum ← ReadSummary(γpre)
Υk ← BacksubWithBlockSummary(Υk, sum)
γpre ← γin

temp ck ← EvaluateConcreteBounds(Υk, γpre)
Ck ← UpdateBounds(Ck, temp ck)

if IsEndLayer(γpre) {γpre is the end layer of a block} then

if IsEndLayer(γk) and γpre = γin then

StoreSummary(γk, Υk)

else

