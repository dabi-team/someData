An Actor-Critic-Based UAV-BSs Deployment
Method for Dynamic Environments

Zhiwei Chen†, Yi Zhong†, Xiaohu Ge†∗
†School of Electronic Information and Communications
Huazhong University of Science and Technology, Wuhan, China,
{zhiwei chen, yzhong, xhge}@hust.edu.cn

Yi Ma‡
‡Institute for Communication Systems (ICS)
University of Surrey, Guildford, England,
y.ma@surrey.ac.uk

0
2
0
2

b
e
F
3

]
I

N
.
s
c
[

1
v
1
3
8
0
0
.
2
0
0
2
:
v
i
X
r
a

Abstract—In this paper,

the real-time deployment of un-
manned aerial vehicles (UAVs) as ﬂying base stations (BSs) for
optimizing the throughput of mobile users is investigated for
UAV networks. This problem is formulated as a time-varying
mixed-integer non-convex programming (MINP) problem, which
is challenging to ﬁnd an optimal solution in a short time
with conventional optimization techniques. Hence, we propose
an actor-critic-based (AC-based) deep reinforcement learning
(DRL) method to ﬁnd near-optimal UAV positions at every
moment. In the proposed method, the process searching for
the solution iteratively at a particular moment is modeled as
a Markov decision process (MDP). To handle inﬁnite state
and action spaces and improve the robustness of the decision
process, two powerful neural networks (NNs) are conﬁgured
to evaluate the UAV position adjustments and make decisions,
respectively. Compared with the heuristic algorithm, sequential
least-squares programming and ﬁxed UAVs methods, simulation
results have shown that the proposed method outperforms these
three benchmarks in terms of the throughput at every moment
in UAV networks.

Index Terms—UAV deployment, deep reinforcement learning,

throughput maximization, dynamic user, actor-critic.

I. INTRODUCTION

Unmanned aerial vehicles have been proposed in future
wireless networks to act as mobile base stations (UAV-BSs) in
order to offer emergency communication or provide wireless
services to underserved areas [1], [2]. Compared with ground
base stations (GBSs), UAV-BSs can be deployed ﬂexibly,
which will increase the probability of establishing line-of-
sight (LOS) links to moving users. Hence, the deployment
of UAV-BSs is one of the key design considerations in future
heterogeneous wireless networks for capacity maximization,
smart city, mobile edge computing and autonomous vehicular
networks [3].

As a result,

the deployment of UAV-BSs has received
signiﬁcant attentions. For instance, the optimal deployment
and mobility of multiple UAVs for data collection from IoT
sensors was researched [4]. Furthermore, the optimal altitude,
which enables one UAV to achieve maximum coverage, was
investigated by [5]. The authors maximized the minimum
throughput over all ground users by optimizing the UAV
trajectory and power control joint with user communication
scheduling and association [6], where the non-convex opti-
mization problem was solved via applying block coordinate
descent and successive convex optimization techniques. The

article [7] utilized the deep deterministic policy gradient
algorithm (DDPG) [8] to maximize the UAV power with
consideration for fairness, communication coverage and con-
nectivity. The evolutionary algorithm was adopted to ﬁnd the
optimal deployment of UAV-BSs for disaster relief scenarios
in [9].

However, only stationary scenarios are conﬁgured for users
in the above studies for UAV-BS deployment optimizations. In
real-life scenarios users often randomly move, which results
in users being at different positions in different time slots
and difﬁculties in the evaluation of the random network
performance [10], [11]. To improve the performance of UAV
networks, UAV-BSs have to quickly adjust their positions
considering different locations of users. In this case, a rapid
UAV-BS deployment method considering moving users needs
to be investigated for dynamic environments.

UAV-BS deployment in dynamic environments is usually
formulated as a time-varying mixed-integer non-convex pro-
gramming (MINP) problem, which is a type of NP-hard
problem. For this NP-hard problem, heuristic algorithms are
applied to ﬁnd the near-optimal solution with computation
power. Joint optimization and evaluation of the computation
and communication power in cellular networks was investi-
gated in [12], [13]. Meanwhile the heuristic algorithms is often
fall into the optimal local solution. Hence, an actor-critic (AC)
based UAV-BS deployment method based on the deep rein-
forcement learning (DRL) framework is proposed to improve
the real-time network throughput. The main contributions and
innovation of this paper are summarized as follows:

1) To avoid solving the MINP problem directly, the pro-
cess of ﬁnding the optimal position of the UAV-BS is
formulated as a Markov decision process (MDP). To
search for the best policy function of the MDP where
the state and action space are continuous position values
generated by the UAV-BSs and users, an AC-based DRL
method is adopted in dynamic environments. Compared
with the deep Q learning method, the AC-based method
can avoid to fall into the optimal local solutions which is
caused by the Bellman optimal equation of state-action
value function.

2) Instead of simply setting the reward function empir-
ically, a mathematical expression of the immediate
reward function is derived considering the MINP op-

 
 
 
 
 
 
timization objective. The mathematical expression of
reward function ensures the convergence of the actor
and critic neural networks (NNs).

3) Based on the proposed AC-based method, the UAV-
BS agent ﬁnally learn the cooperation among UAV-BSs
and effectively reduce mutual interference. Simulation
results indicate the proposed method has a good gener-
alization when users are located in different probability
distributions.

The rest of this paper is organized as follows. In Section
II, the system model and problem formulation are introduced.
The AC-based method is depicted for UAV-BS deployment in
Section III. Simulation results are presented and analyzed in
Section IV. Finally, the conclusions are given in Section V.

with the altitude value H, xk(t) is the user x axis value
and yk(t) is the user y axis value at time slot t. All UAV-
BSs are assumed to ﬂy at a ﬁxed altitude H above the
ground and the horizontal coordinates of each user and the
horizontal coordinates of UAV-BS are changed with time.
The reason why the UAV ﬂy at a ﬁxed altitude is that
searching the best UAV-BSs 3-D positions could be divided
into two stages. This work focuses on the ﬁrst stage, which
is letting the UAV-BSs in the right horizontal position at a
short time. The Euclidean distance between a UAV-BS p and
a user k at a time slot t can be expressed as rp,k(t) =
(cid:112)(xk(t) − xp(t))2 + (yk(t) − yp(t))2 + H 2, k ∈ K, p ∈ P.

A. Channel Model

The channel gain gk,p between a UAV-BS p and a user k is
and a non-line-
and gN LOS
k,p

composed of a line-of-sight component gLOS
of-sight (NLOS) component gN LoS
can be given as follows [14]

k,p
. the gLOS

k,p

k,p





gLOS
k,p =

gNLOS
k,p =

(cid:19)2

(cid:19)2

(cid:18) 4πf
v
(cid:18) 4πf
v

µLOSr−αLOS
k,p

,

µNLOSr−αNLOS

k,p

,

(1)

Where f is the carrier frequency, v is the light speed,
αLOS and αN LOS is the path loss exponent in the LOS
and NLOS transmission conditions, respectively. Compared
with the inﬂuence induced by the NLOS transmission, the
impact of multi-path fading can be neglected [14]. µφ(φ ∈
{LOS, NLOS}) is the attenuation factor. The user received
power Prec(r) can be written as
(cid:26) PV gLOS
k,p , LOS link,
PV gNLOS
k,p

, N LOS link,

Prec(r) =

(2)

where PV is the UAV-BS transmit power. Assuming that all
UAV-BSs have the same PV . Here, the probability of LOS
connection depends on the different environments, density,
height of buildings and the elevation angle between users and
UAV-BSs. The LOS probability can be expressed as follow
[5]

PLOS =

1
1 + C exp(−B[θ − C])

,

(3)

where B and C are constants which depend on the environ-
ments. θ is the elevation angle. For the user k and UAV-BS
p, θp,k is

θp,k =

180
π

× sin−1

(cid:18) H
rp,k

(cid:19)

.

(4)

The probability of NLOS link is PN LOS = 1 − PLOS.
Furthermore, the average received power of the user k served
by the UAV-BS p can be expressed as

Pk,p = PLOSPrec(r) + PN LOSPrec(r).

(5)

Fig. 1. UAV networks.

II. SYSTEM MODEL

Two important reasons for adopting UAV to assist current
communication systems are listed as follows: one reason is
that UAV-BSs can be used for emergency communications;
the other reason is that UAV-BSs ﬂy to area without ground
BSs coverage to provide wireless services. A UAV network
is conﬁgured in Fig. 1, where a set G includes a number of
G ground BSs, a set K includes a number of K users, a
set P includes a number of P UAV-BSs. The downlink (DL)
is denoted as the wireless link from a UAV-BS p to a user
k. We consider the sub-6 GHz band for the UAV-BS data
links and all the UAV-BSs share the same frequency band
for wireless communications. The total bandwidth is equally
divided among the associated users. All users are randomly
located and equipped with the single antenna. The time
division multiplexing scheme is adopted in UAV networks.
Each user is allowed to access only one UAV-BS in one time
slot. For ease of exposition, we assume that the UAV-BSs use a
different frequency from the ground BSs to avoid interference
between UAV-BSs and ground BSs.

Without loss of generality, the 3D cartesian coordinate of
each UAV-BS p ∈ P and user k ∈ K are (xp(t), yp(t), H)
and (xk(t), yk(t), 0), where xp(t) is the UAV-BS x axis value
and yp(t) is the UAV-BS y axis value at the time slot t

B. Problem Formulation

Considering that all UAV-BSs share the same frequency
the interference among UAV-BSs can’t be ignored
band,
in UAV networks. The signal-to-interference-plus-noise ratio
(SINR) at a user equipment (UE) k from UAV-BS p at time
t is

SIN Rk,p,t =

Pk,p,t
j∈P\p Pk,j,t + σ2 ,

(cid:80)

(6)

where σ2 is the noise power at
the user side which is
considered to be the additive white Gaussian noise (AWGN).
Pk,j,t is the received power of the user k served by the UAV-
BS j at time t. The transmission rate of a UE k at time t can
be expressed as:

Rk,p,t = ak,p,t log2 (1 + SIN Rk,p,t) , k ∈ K, p ∈ P,

(7)

where ak,p,t is the indicator factor. ak,p,t = 1 means a user k
is served by a UAV-BS p at time t, otherwise, ak,p,t = 0.

In this paper, a period of time T is discretized into multiple
time slots and users in each time slot are assumed to be in
same positions. The UAV-BSs deployment aims to maximize
the real-time throughput by moving to the proper positions
at every time slot. Mathematically,
this problem can be
formulated as

(cid:88)

(cid:88)

max

k∈K

p∈P

ak,tRk,p,t, ∀t ∈ T,

s.t. ak,p,t = 1[0,∞][Rk,p,t − Γ] × 1[0,∞][δ − rk,p,t],

∀(k, p) ∈ K × P, ∀t ∈ T,

1[0,∞][n] =

(cid:26) 0, n < 0
1, n ≥ 0

xk,t, xp,t ∈ [0, AX ], ∀t ∈ T,

yk,t, yp,t ∈ [0, AY ], ∀t ∈ T,

(8)

(9)

(10)

(11)

(12)

where ak,p,t = 1 when the distance between UAV-BS p and
user k is shorter than the UAV-BS p communication distance
δ and the rate of user k is larger than the communication
rate threshold Γ, otherwise, ak,p,t = 0. (11) and (12) imply
that UAV-BSs and users can only move in a given Ax × Ay
rectangular area.

Since the problem at time slot t is a mixed-integer non-
convex programming (MINP) problem which is challenging to
solve due to these two reasons as follow: ﬁrst, Rk,p,t is a non-
convex function respect to the distance between UAV-BSs and
users rk,p,t; Second, the variable ak,p,t is binary. Conventional
optimization technique like the convex optimization can not
used directly for this problem due to the high complexity.
In general, heuristic algorithms are adopted to ﬁnd a near-
optimal solution. However, the heuristic algorithm has to be
re-run as long as the user locations have been changed, which
causes a high computational overhead. Different from heuristic
the DRL agent can learn the policy of UAV-
algorithms,
BS position adjustments based on the continuous interactions
with the environment and then save the policy as the deep
neural network weights. Considering the continuous position

values generated by the UAV-BSs and users, AC-based method
is adopted to solve this UAV-BS deployment optimization
problem.

III. AC-BASED DEPLOYMENT OPTIMIZATION OF
UAV-BSS

In this section, we propose an AC-based method combining
the advantages of deep Q learning and policy gradient [15],
where each UAV-BS can ﬁnd intelligently and quickly the
target regions in every time slot.

A. Preliminaries

Based on a standard reinforcement learning setting, an agent
interacts with a system environment in discrete epochs. The
agent observes a state si, executes a action ai and obtains
a reward ri at a epoch i. As usual, the episode is written
as (s0, a0, s1, a1, ..., si, ai, si+1) after several multiple actions
and state transitions. The reinforcement learning objective is
to ﬁnd a good policy function π(ai) that maps a state to an
action for maximizing the discounted cumulative reward of a
MDP, which is also called the value function. The discounted
cumulative reward can be denoted as

V π(s) = E

(cid:34) ∞
(cid:88)

(cid:35)
γir(si, ai)|s0 = s

,

(13)

i=0

where r(si, ai) is the immediate reward, γi ∈ [0, 1] is the
discount factor at the epoch i and E[·] is the expectation
operation. The state si and action ai is usually combined as
a state-action pair (si, ai). Hence, the value function can be
replaced as a Q-value function Q(si, ai). The Q-value function
can be expressed as

Q(si, ai) = r(si, ai)

(cid:88)

(cid:88)

+ γ

si+1∈S

ai+1∈A

Psisi+1(ai)Q(si+1, ai+1), (14)

where Psisi+1 is the transition probability from the state si
to the next state si+1. S and A are the state space and
action space, respectively. Considering that the state transition
probability is unknown in dynamic environments, the deep
Q learning algorithm is proposed by adopting a deep neural
network (DNN) to approximate the Q-value function. The
deep Q network (DQN) is trained by minimizing the following
loss function

L(θQ) = E[(yi − Q(si, ai|θQ))2],

(15)

where θQ is the weight vector of the DQN. yi is the target
value, which can be estimated by a temporal difference
approach [16]. Hence, yi is denoted by

yi = r(si, ai) + γiQ(si+1, ai+1)|θQ).

(16)

However, the deep Q learning algorithm only works for
problems with a discrete action space. To apply the DRL for
the high dimensional continuous problem, the AC method has
been proposed. In the AC method, the action is generated by
the policy function which can be approximated by a DNN.

The policy DNN are updated by gradient ascent and the loss
of the policy DNN is the expectation of discounted cumulative
reward of multiple episodes, which is written as follows:

Rθπ = Eτ ∼pθπ (τ )[R(τ )] ≈

1
N

N
(cid:88)

n=1

R(τ n),

(17)

where the pθ(τ ) is the probability of the episode τ showing
up at the policy DNN with the weight θπ. In the AC method,
R(τ ) can be estimated by the Q-value produced by the DQN.
Therefore, the actor DNN gradient updating expression is

θπ ← θπ + η∇Rθπ ,

where the ∇Rθπ can be calculated by:

∇ ¯Rθπ ≈ E[∇θπ Q(s, a|θQ)|s=si,a=π(si|θπ)]
= E[∇aQ(s, a|θQ)|s=si,a=π(si)
· ∇θπ π(s|θπ)|s = si].

(18)

(19)

Different from the policy gradient method, the AC method
adopts the output value of DQN as a loss function to improve
the robustness of policy function. Compared with both the
deep Q learning and policy gradient methods, the AC method
is difﬁcult to train and converge.

B. AC-based UAV-BS Deployment Method

Considering the time-varying MINP problem in maximizing
the real-time throughput of UAV networks, we transfer the
MINP problem into an MDP problem at every time slot, which
can be solved by the DRL algorithm rather than solving the
MINP problem directly and violently. At every time slot t,
the DRL agent can output the change of position of UAV-
BS in multiple epochs iteratively. To begin with, the state
space, action space and the reward function are denoted in
the following:

1) State space: si(t) is the state at an epoch i when the
time slot is t. The state information includes the position of
each user, position of each UAV-BS and each user association
with UAV-BSs. There are K × 3 + P × 2 elements in the
state. The ﬁrst group is [ui,k(t), ∀k ∈ K] representing the
user horizontal coordinate positions. The second group is
[qi,p(t), ∀p ∈ P] which denotes the UAV-BS positions. The
third part is [ci,k(t), ∀k ∈ U] showing the user association
with UAV-BSs, where the value is the number of UAV-BSs.
Hence, the si(t) is expressed as

si(t) = [ui,0(t), ..., ui,K−1(t);
qi,0(t), ..., qi,P −1(t);
ci,0(t), ..., ci,K−1(t); ].

(20)

2) Action space: ai(t) is the action at a epoch i when the
time slot is t. ai(t) = [ui+1,p(t) − ui,p(t)] is the change of
horizontal coordinates of the UAV-BS positions between the
epoch i and epoch i + 1. Hence, ai(t) can be expressed as

ai(t) = [∆xi,0(t), ..., ∆xi,P (t); ∆yi,0(t), ...∆yi,P (t)]. (21)

3)

reward: The MINP objective
at

reward of

episode

a

should

be
every time

the
slot.

cumulative

(cid:80)

(cid:80)

k∈K

p∈P ak,tRk,p,t,I is the throughput of UAV network

at the last epoch I when the time slot is t, i.e.

(cid:88)

(cid:88)

k∈K

p∈P

ak,tRk,p,t,I =

I
(cid:88)

i=1

ri(si, ai).

(22)

The throughput of UAV network at
the initial epoch is
considered to be zero at every time slot. Hence, (22) can be
rewritten as

I
(cid:88)

i=1

ri(si, ai) =

(cid:88)

(cid:88)

k∈K

p∈P

ak,tRk,p,t,I −

(cid:88)

(cid:88)

k∈K

p∈P

ak,tRk,p,t,0

I
(cid:88)
(

=

(cid:88)

(cid:88)

ak,tRk,p,t,i

,

p∈P

i=1
(cid:88)

−

k∈K
(cid:88)

k∈K

p∈P

ak,tRk,p,t,i−1)

p∈P ak,tRk,p,t,0 = 0. From (23),

(23)
the immediate

where (cid:80)
reward is derived by
(cid:88)

(cid:88)

ri(si, ai) =

k∈K

p∈P

ak,t,iRk,p,t,i −

(cid:88)

(cid:88)

k∈K

p∈P

ak,tRk,p,t,i−1.

(24)
Based on (24), the change of throughput between the epoch i
and i − 1 is considered as the immediate reward at epoch i.
A centralized ground base station is responsible for
training process and the inference process is executed by
each UAV-BS independently. The UAV-BSs are also con-
nected to ground base stations to get core network ser-
vice. The centralized trainer collects experiences in dy-
namic environments and trains the actor and critic DNNs.
Each UAV-BS as an agent equipped with the same ac-
tor DNN adjusts their position independently. The Markov
chain (s0(t), a0(t), ..., si(t), ai(t), si+1(t)) denotes a process
searching for the optimal position iteratively at a time slot t.
Given the initial positions of the UAV-BSs and users, the UAV-
BS as an agent outputs a position change ai(t) step by step and
arrives at the target position in the end. The MINP problem
constraints are included in the environment conﬁguration.

The main steps of training process and decision process are
presented in Algorithm 1. Without loss of generality, all users
and UAV-BSs are randomly located and the weights of both
actor and critic DNN are random. Each UAV-BS with state
si(t) executes the action ai(t) to move to a new position
and obtains an immediate reward ri(si(t), ai(t)) as well as
a new state si+1(t) in the epoch i + 1. After executing above
steps, the new experience (si(t); ai(t); ri(t); si+1(t)) has been
collected into the replay buffer B (i.e. the step 8-10). A mini-
batch of experiences with size N is randomly sampled from
B. The DQN target value is calculated by (15), i.e. the step
11-12. The step 14 is used to update the weights θQ of the
critic DNN considering the loss function (14) and the weights
of actor DNN are updated via (17), where the gradient is
calculated by (18). Besides, the target actor DNN and target
critic DNN are updated in every L epoch, i.e., the step 13-
14. In the decision process, the UAV-BS agent interacts with

7:
8:
9:

10:

11:

12:
13:

14:

15:

the dynamic environment as long as the user locations are
changed. At every time slot t, a Markov chain is formed. In
the last epochs of the entire Markov chain, the action value
converges to 0 which means UAV-BS no longer change its
position. And then, the state with the maximum throughput is
selected for each episode.

Algorithm 1 AC-based UAV deployment

1: Training Process
2: Randomly initializes the critic DNN Q(s, a|θQ) and actor

DNN π(s|θπ) with weights θQ and θπ;
(s, a|θQ

3: Initializes target critic DNN Q

(cid:48)

(cid:48)

actor DNN π(s|θπ

) with weights θQ

(cid:48)

(cid:48)

) and the target
= θπ;

= θQ ,θπ

(cid:48)

4: Initializes the replay buffer B;
5: for time slot :=1, ... , N do
6:

Initializes the environment and receives an initial state
s1;
for epoch:=1, ... , I do
ai = π (si) + N ;
Executes ai and obtains the new state si+1, reward
ri;
Stores the transition sample (si, ai, ri, si+1) into B;

Samples a mini-batch of H samples (sj, aj, rj, sj+1)
from B;
Calculates the target value yj by (15);
Updates the weights of critic DNN θQ by minimizing
the loss by (14):
Updates the weights θπ of actor DNN by θ ← θ +
η∇Rθ , where the gradient is calculated by (18):
In every L epoch, update the corresponding target
DNNs:

end for

16:
17: end for
18: Decision Process
19: while the initial state s0(t) updates do
20:
21:

for epoch := 1, ... ,I do

UAV as a DRL agent executes a action ai, obtains
new state si+1, reward ri.

22:

23:

end for
Find the si with maximum throughput in one episode
[s1, a1, ..., si, ai]

24: end while

IV. SIMULATION AND RESULTS

A. Simulation Conﬁguration and Training Tricks

Without loss of generality, 24 users and 2 UAV-BSs are
conﬁgured in the UAV network. In the simulation, the number
of users served by the UAV-BSs can be changed ﬂexibly to
depict the departure and arrival of users. In particular, users
are uniformly distributed in a square area with a size of 800m
800m at every time slot. Other parameters are list in Table I.
The actor DNN consist of an input layer with K × 3 + P ×
2 = 76 neurons, 4 fully connected hidden layers and an output

layer with 4 neurons. In particular, the 4 fully connected
layers have 256, 128, 64 and 16 neurons, respectively. The
activation function of the output layer of the actor DNN is
Tanh function, and the activation functions of the hidden layers
and output layers of the actor DNN are Relu function. The
Adam optimization algorithm is used with the learning rate
0.0001. The critic DNN is composed of an input layer with
K ×3+P ×4 = 81 neurons, the 4 fully connected layers with
256, 128, 64 and 16 neurons, respectively, and the output layer
with 1 neuron. The target actor and critic DNN are updated
every L = 200 epoch. One episode has 800 epochs. 5000
episodes are used to train the actor and critic DNNs.

To help actor and critic DNNs converge faster, we consider
the change of system throughput without interference as the
immediate reward to pretrain the actor and critic DNNs in
the early stage of the training process. The pretraining makes
UAV-BSs to move to places with high user density and get
many samples in which immediate rewards are diverse. Batch
normalization technique is used to prevent actor and critic
DNNs from overﬁtting.

TABLE I
SIMULATION CONFIGURATION

parameters
Height of UAV-BS H
Noise power density σ2
µLOS
Γ
Carrier frequency f
B
Critic learn rate
Batch size
Epoch

values
100m
-174dBm
1dB
2.5bps/Hz
2GHz
0.136
0.0001
64
800

parameters
UAV-BS transmit power PV
Discounted factor γ
ηN LOS
δ
Total bandwidth
C
Actor learn rate
Buffer capacity
Episode

values
1W
0.9
0dB
250m
20MHz
11.95
0.0001
1 × 107
5000

B. Result and Performance Evaluating

In this part, the simulation results are presented to evaluate
the performance of the proposed AC-based UAV-BS deploy-
ment method.

The throughput performances of the proposed AC-based
UAV-BS deployment method, the sequential least-squares pro-
gramming (SLSQP), heuristic and ﬁxed UAV-BSs methods at
100 time slots are shown in Fig.2. The classical annealing
algorithm in heuristic algorithms is implemented for compar-
ison. Compared with the throughput of heuristic algorithm,
the proposed Algorithm 1 achieves a better throughput in the
78% time slots of simulation period for UAV networks. The
MINP problem is solved by the SLSQP method when the
MINP problem is considered simply as a quadratic nonlinear
programming problem. Compared with the throughput of the
SLSQP method, the proposed Algorithm 1 achieves a better
throughput in the 84% time slots of simulation period for UAV
networks. For the ﬁxed method, the throughput is lower than
the proposed method at every time slot. Besides, under the
same hardware and operating system condition, the solution
time of the proposed method is 1439.93 seconds, signiﬁcantly
less than the 1916.90 seconds of the heuristic algorithm.

The long-term average throughput performances with users
in the Gaussian and Uniform distribution is shown in Fig.3.

problem. To solve this problem, the process of ﬁnding the
optimal UAV-BS positions is modeled as a MDP. The AC-
based DRL method is used to search for the ideal UAV-BS
deployment policy function. The simulation results show that
the proposed method achieves 27% increase in the long-term
average throughput for UAV networks and 24% decrease in
solution time as compared with the heuristic algorithm in UAV
deployments with dynamic environments.

VI. ACKNOWLEDGMENT

The authors would like to acknowledge the support from

National Key R&D Program of China (2017YFE0121600).

REFERENCES

[1] Y. Zeng, R. Zhang, and T. J. Lim, “Wireless Communications with
Unmanned Aerial Vehicles: Opportunities and Challenges,” IEEE Com-
munications Magazine, vol. 54, no. 5, pp. 36–42, May 2016.

[2] M. Mozaffari, W. Saad, M. Bennis, Y.-H. Nam, and M. Debbah, “A
Tutorial on UAVs for Wireless Networks: Applications, Challenges, and
Open Problems,” IEEE Communications Surveys & Tutorials, vol. 21,
no. 3, pp. 2334–2360, 2019.

[3] Y. Zhong, T. Q. S. Quek, and X. Ge, “Heterogeneous cellular networks
trafﬁc: Delay analysis and scheduling,” IEEE
with spatio-temporal
Journal on Selected Areas in Communications, vol. 35, no. 6, pp. 1373–
1386, June 2017.

[4] M. Mozaffari, W. Saad, M. Bennis, and M. Debbah, “Mobile unmanned
aerial vehicles (uavs) for energy-efﬁcient internet of things communica-
tions,” IEEE Transactions on Wireless Communications, vol. 16, no. 11,
pp. 7574–7589, Nov 2017.

[5] A. Al-Hourani, S. Kandeepan, and S. Lardner, “Optimal LAP Alti-
tude for Maximum Coverage,” IEEE Wireless Communications Letters,
vol. 3, no. 6, pp. 569–572, Dec. 2014.

[6] Q. Wu, Y. Zeng, and R. Zhang, “Joint Trajectory and Communication
Design for Multi-UAV Enabled Wireless Networks,” IEEE Transactions
on Wireless Communications, vol. 17, no. 3, pp. 2109–2121, Mar. 2018.
[7] C. H. Liu, Z. Chen, J. Tang, J. Xu, and C. Piao, “Energy-Efﬁcient
UAV Control for Effective and Fair Communication Coverage: A Deep
Reinforcement Learning Approach,” IEEE Journal on Selected Areas in
Communications, vol. 36, no. 9, pp. 2059–2070, Nov. 2018.

[8] T. P. Lillicrap, J. J. Hunt, A. Pritzel, N. Heess, T. Erez, Y. Tassa,
D. Silver, and D. Wierstra, “Continuous control with deep reinforcement
learning,” arXiv preprint arXiv:1509.02971, 2015.

[9] J. Komerl and A. Vilhar, “Base stations placement optimization in
wireless networks for emergency communications,” in 2014 IEEE
International Conference on Communications Workshops (ICC), June
2014, pp. 200–205.

[10] X. Ge, J. Ye, Y. Yang, and Q. Li, “User mobility evaluation for 5g
small cell networks based on individual mobility model,” IEEE Journal
on Selected Areas in Communications, vol. 34, no. 3, pp. 528–541,
March 2016.

[11] X. Ge, B. Yang, J. Ye, G. Mao, C. Wang, and T. Han, “Spatial spectrum
and energy efﬁciency of random cellular networks,” IEEE Transactions
on Communications, vol. 63, no. 3, pp. 1019–1030, March 2015.
[12] X. Ge, S. Tu, T. Han, Q. Li, and G. Mao, “Energy efﬁciency of small
cell backhaul networks based on gaussmarkov mobile models,” IET
Networks, vol. 4, no. 2, pp. 158–167, 2015.

[13] L. Xiang, X. Ge, C. Wang, F. Y. Li, and F. Reichert, “Energy efﬁciency
evaluation of cellular networks based on spatial distributions of trafﬁc
load and power consumption,” IEEE Transactions on Wireless Commu-
nications, vol. 12, no. 3, pp. 961–973, March 2013.

[14] A. Al-Hourani, S. Kandeepan, and A. Jamalipour, “Modeling air-to-
ground path loss for low altitude platforms in urban environments,” in
2014 IEEE Global Communications Conference, Dec 2014, pp. 2898–
2904.

[15] V. Mnih, K. Kavukcuoglu, D. Silver, A. A. Rusu, J. Veness, M. G.
Bellemare, A. Graves, M. Riedmiller, A. K. Fidjeland, G. Ostrovski
et al., “Human-level control
learning,”
Nature, vol. 518, no. 7540, p. 529, 2015.

through deep reinforcement

[16] R. S. Sutton, A. G. Barto et al., Introduction to reinforcement learning.

MIT press Cambridge, 1998, vol. 2, no. 4.

Fig. 2. Throughput of UAV networks.

Compared with the heuristic algorithm, the proposed method
can improve the long-term average throughput by 7% and 27%
when the user locations are in 2-D uniform distribution and 2-
D Gaussian distribution, respectively. Under the condition that
the user locations are in the gaussian distribution, the UAV-BS
agents do not just tend to be in places with dense crowds, and
also have learned to cooperate with each other to avoid the
interference by ﬁne-tuning their positions. Fig.3. indicates the
proposed method has good generalization. As shown in Fig.4,
the throughput performance of the proposed method increases
as the user density increase. Compared with the heuristic
algorithm, the proposed method can maximally improve the
throughput by 43.4% when the user density varies.

Fig. 3. Long-term average throughput of UAV networks with users in uniform
(left) and Gaussian (right) distribution.

Fig. 4. Throughput of UAV networks with respect to the user density

V. CONCLUSION

In this article, an AC-based DRL method for UAV-BS
deployment has been proposed to improve the throughput
of UAV networks in dynamic environments. The UAV de-
ployment optimization is formulated as a time-varying MINP

020406080100Time slots (t)020406080100120Throughput (bps/Hz)ACFixedSLSQPHeuristic246810Time slots (t)1020304050607080Throughput (bps/Hz)ACFixedSLSQPHeuristic246810Time slot (t)1020304050607080Throughput (bps/Hz)ACFixedSLSQPHeuristic406080100120140Density (users/km2)020406080100120Throughput (bps/Hz)ACSLSQPHeuristicFixed43.4%