0
2
0
2

p
e
S
7
1

]

R
G
.
s
c
[

1
v
6
2
0
8
0
.
9
0
0
2
:
v
i
X
r
a

ShapeAssembly: Learning to Generate Programs for
3D Shape Structure Synthesis

R. KENNY JONES, Brown University
THERESA BARTON, Brown University
XIANGHAO XU, Brown University
KAI WANG, Brown University
ELLEN JIANG, Brown University
PAUL GUERRERO, Adobe Research
NILOY J. MITRA, University College London, Adobe Research
DANIEL RITCHIE, Brown University

Fig. 1. We present a deep generative model which learns to write novel programs in ShapeAssembly, a domain-specific language for modeling 3D shape
structures. Executing a ShapeAssembly program produces a shape composed of a hierarchical connected assembly of part proxies cuboids. Our method
develops a well-formed latent space that supports interpolations between programs. Above, we show one such interpolation, and also visualize the geometry
these programs produce when executed. In the last column, we manually edit the continuous parameters of a generated program, in order to produce a
variant geometric structure with new topology.

Manually authoring 3D shapes is difficult and time consuming; generative
models of 3D shapes offer compelling alternatives. Procedural representa-
tions are one such possibility: they offer high-quality and editable results
but are difficult to author and often produce outputs with limited diversity.
On the other extreme are deep generative models: given enough data, they

Authors‚Äô addresses: R. Kenny Jones, Brown University; Theresa Barton, Brown Univer-
sity; Xianghao Xu, Brown University; Kai Wang, Brown University; Ellen Jiang, Brown
University; Paul Guerrero, Adobe Research; Niloy J. Mitra, University College London,
Adobe Research; Daniel Ritchie, Brown University.

¬© 2020 Association for Computing Machinery.
This is the author‚Äôs version of the work. It is posted here for your personal use. Not for
redistribution. The definitive Version of Record was published in ACM Transactions on
Graphics, https://doi.org/10.1145/3414685.3417812.

can learn to generate any class of shape but their outputs have artifacts and
the representation is not editable.

In this paper, we take a step towards achieving the best of both worlds
for novel 3D shape synthesis. First, we propose ShapeAssembly, a domain-
specific ‚Äúassembly-language‚Äù for 3D shape structures. ShapeAssembly pro-
grams construct shape structures by declaring cuboid part proxies and at-
taching them to one another, in a hierarchical and symmetrical fashion. Sha-
peAssembly functions are parameterized with continuous free variables,
so that one program structure is able to capture a family of related shapes.
We show how to extract ShapeAssembly programs from existing shape
structures in the PartNet dataset. Then, we train a deep generative model, a
hierarchical sequence VAE, that learns to write novel ShapeAssembly pro-
grams. Our approach leverages the strengths of each representation: the
program captures the subset of shape variability that is interpretable and

ACM Trans. Graph., Vol. 39, No. 6, Article 234. Publication date: December 2020.

def Chair():  bbox = Cuboid(1.2, 1.4, 1, T)  base = Base(.9, .5, .8, T)  seat = Seat(1.1, .1, .9, T)  back = Back(1.1, .9, .2, F)  arm = Cuboid(.1, .4, .7, F)  attach(base, bbox, .5, 0, .5, .5, 0, .5)  squeeze(back, bbox, base, top, .5, .1)  attach(seat, base, .5, 0, .5, .5, 1, .5)  attach(arm, back, .5, .5, 0, .1, .3, .5)  attach(arm, seat, .5, 0, .5, .1, .7, .5)  reflect(arm, X). . .def Back(l, w, h, aligned):  bbox = Cuboid(l, w, h, aligned)  surface = Cuboid(1.16, .64, .13, T)  slat = Cuboid(.04, .76, .1, F)  attach(surface, bbox, .5, 1, .5, .5, 1, .7)  attach(slat, bbox, .5, 0, .5, .2, 0, .45)  attach(slat, surface, .5, .6, .8, .2, .3, .2)  reflect(slat, X) EDITdef Chair():. . .def Back(l, w, h, aligned):  bbox = Cuboid(l, w, h, aligned)  surface = Cuboid(1.08, .58, .11, T)  slat = Cuboid(.04, .73, .1, F)  attach(surface, bbox, .5, 1, .5, .5, 1, .6)  attach(slat, bbox, .5, 0, .5, .15, 0, .3)  attach(slat, surface, .6, .5, .6, .1, .1, .1)  reflect(slat, X)def Chair():. . .def Back(l, w, h, aligned):  bbox = Cuboid(l, w, h, aligned)  surface = Cuboid(.9, .51, .08, T)  slat = Cuboid(.05 .6, .07, F)  attach(surface, bbox, .5, 1, .5, .5, 1, .5)  attach(slat, bbox, .5, 0, .5, .1, 0, .3)  attach(slat, surface, .5, .8, .5, .1, .1, .3)  reflect(slat, X)def Chair():  bbox = Cuboid(.82, 1.6, .85, T)  base = Base(.75, .66, .66, T)  seat = Seat(.8, .13, .85, T)  back = Back(.8, .9, .1, T)  attach(base, bbox, .5, 0, .5, .5, 0, .5)  attach(back, bbox, .5, 1, .5, .5, 1, .05)  attach(seat, base, .5, .0, .5, .5, 1, .5)  attach(back, seat, .5, .0, .5, .5, .75, .05). . .def Back(l, w, h, aligned):  bbox = Cuboid(l, w, h, aligned)  surface = Cuboid(.8, .4, .1, T)  slat = Cuboid(.05, .5, .05, T)  attach(surface, bbox, .5, 1, .5, .5, 1, .5)  squeeze(slat, bbox, surface, bot, .1, .5)  translate(slat, X, 3, 0.8)def Chair():  bbox = Cuboid(0.5, 2, 0.7, T)  base = Base(0.5, .95, 0.7, T)  seat = Seat(0.5, .05, 0.7, T)  back = Back(0.5, 1, 0.05, T)  attach(base, bbox, .5, 0, .5, .5, 0, .5)  attach(back, bbox, .5, 1, .5, .5, 1, .05)  attach(seat, base, .5, .0, .5, .5, 1, .5)  attach(Back, seat, .5, .0, .5, .5, .75, .05). . .def Back(l, w, h, aligned):  bbox = Cuboid(l, w, h, aligned)  surface = Cuboid(0.5, 0.1, 0.05, T)  slat = Cuboid(.09, .9, .05, T)  attach(surface, bbox, .5, 1, .5, .5, 1, .5)  squeeze(slat, bbox, surface, bot, .1, .5)  translate(slat, X, 2, 0.8)executeexecuteexecuteexecuteexecuteInterpolation in ShapeAssembly Program Space 
 
 
 
 
 
234:2

‚Ä¢ R. Kenny Jones, Theresa Barton, Xianghao Xu, Kai Wang, Ellen Jiang, Paul Guerrero, Niloy J. Mitra, and Daniel Ritchie

editable, and the deep generative model captures variability and correlations
across shape collections that is hard to express procedurally.

We evaluate our approach by comparing the shapes output by our gener-
ated programs to those from other recent shape structure synthesis models.
We find that our generated shapes are more plausible and physically-valid
than those of other methods. Additionally, we assess the latent spaces of
these models, and find that ours is better structured and produces smoother
interpolations. As an application, we use our generative model and differen-
tiable program interpreter to infer and fit shape programs to unstructured
geometry, such as point clouds.

CCS Concepts: ‚Ä¢ Computing methodologies ‚Üí Neural networks; La-
tent variable models; Shape analysis.

Additional Key Words and Phrases: Shape analysis, shape synthesis, genera-
tive models, deep learning, procedural modeling, neurosymbolic models

ACM Reference Format:
R. Kenny Jones, Theresa Barton, Xianghao Xu, Kai Wang, Ellen Jiang, Paul
Guerrero, Niloy J. Mitra, and Daniel Ritchie. 2020. ShapeAssembly: Learning
to Generate Programs for 3D Shape Structure Synthesis. ACM Trans. Graph.
39, 6, Article 234 (December 2020), 20 pages. https://doi.org/10.1145/3414685.
3417812

1

INTRODUCTION

3D models of human-made objects are more in-demand than ever. In
addition to the traditional drivers of demand in computer graphics
(visual effects, animation, games), new applications in artificial in-
telligence increasingly benefit from or even require high-quality 3D
objects, such as producing synthetic training imagery for computer
vision systems [Kar et al. 2019; Richter et al. 2016; Zhang et al. 2017]
or training robots to perform tasks in virtual environments [Ab-
batematteo et al. 2019; Kolve et al. 2017; Savva et al. 2019; Xia et al.
2018]. Despite the growing demand, the craft of 3D modeling largely
remains as difficult and time-consuming as it has ever been. The
time and expertise required to create 3D content by hand will not
scale to these demands.

One promising way out of this conundrum is the development of
generative models of 3D shapes, i.e. procedures which can be exe-
cuted to generate novel shapes within some class [M√ºller et al. 2006;
Parish and M√ºller 2001; Prusinkiewicz and Lindenmayer 1996]. An
ideal generative model would produce plausible output geometry,
capture a wide range of shape variations, and use an interpretable
representation which a user could subsequently manipulate and
edit. Unfortunately, no existing shape generative model achieves all
of these properties. On the one hand are procedural models: struc-
tured computer programs which produce geometry when executed.
Procedural models can produce high-quality geometry, and their
program-based representation makes them interpretable and ed-
itable to users with some programming background. However, au-
thoring a good procedural model from scratch is difficult (arguably
at least as difficult as modeling an object by hand), and the amount
of shape variation captured by a single procedural model is limited
(e.g., it is difficult to write one program that can model all types of
cars). On the other hand are data-driven generative models, particu-
larly deep generative models: neural networks which learn how to
generate 3D shapes from data [Fan et al. 2017; Groueix et al. 2018; Li
et al. 2017; Mo et al. 2019a; Wu et al. 2016]. Deep generative models
capture variability with little human effort: given enough training

ACM Trans. Graph., Vol. 39, No. 6, Article 234. Publication date: December 2020.

data, they can in theory learn to generate any class of shape. Since
they lack the strict semantics of programs, however, their outputs
often exhibit ‚Äúnoise‚Äù artifacts such as incomplete geometry and float-
ing parts. Additionally, the representations they learn are typically
inscrutable to people, making them hard to edit or manipulate in
predictable ways.

Our insight, in this work, is that these two approaches have com-
plementary strengths: deep generative models are efficient to create
and excel at broad-scale variability, and procedural models produce
high-quality geometry by construction and better facilitate editing
for fine-scale variability. We take a first step toward achieving the
best of both worlds by integrating these two approaches into a sin-
gle pipeline: a deep generative model that learns to write programs,
which, when executed, themselves output 3D geometry. We hypoth-
esize that going through this intermediate program representation
produces a generative model with a smoother latent space, whose
outputs are more likely to be physically valid, compact, and editable.
As the motivating applications mentioned earlier demand 3D
models of human-made objects, we focus on generating novel part-
based shape structures in this paper. We introduce ShapeAssembly,
an ‚Äúassembly language‚Äù for 3D shape structures. In ShapeAssem-
bly, shape structures are represented by hierarchical assemblies
of connected parts, where leaf-level parts are approximated by a
bounding cuboid (a similar representation as the ones used by Part-
Net [Mo et al. 2019b] and StructureNet [Mo et al. 2019a]); these
hierarchical cuboid structures can then be used to condition the
generation of shape surface geometry in the form of e.g. point
clouds. A ShapeAssembly program constructs a shape by declaring
cuboids, iteratively attaching them to one another, and specifying
symmetric repetitions of connected cuboid assemblies. The dimen-
sions of these cuboids and the positions of these attachments are a
program‚Äôs parameters; manipulating them allows for exploring a
family of related shapes. Furthermore, our interpreter for execut-
ing ShapeAssembly programs is fully differentiable, meaning it is
possible to compute gradients of a program‚Äôs output geometry with
respect to its continuous parameters. Figure 1 shows some example
hierarchical ShapeAssembly programs and the output shapes they
produce.

While ShapeAssembly programs produce valid geometry under a
range of parameter values, they do not exhibit structural variability,
and authoring them from scratch still takes time. Thus, we train
a neural network to write a variety of ShapeAssembly programs
for us. Using programs we extract from a shape dataset, we train a
hierarchical sequence VAE which outputs hierarchical ShapeAssem-
bly programs. Each node in the hierarchy uses a recurrent language
model to generate the program text at that level, and to decide
which cuboids should be expanded into subroutine calls. Further-
more, the well-defined semantics of ShapeAssembly allow us to
identify semantically-invalid programs and modify the generator
such that it never produces them. The programs shown in Figure 1
were written by our generative model, by decoding code vectors
along a straight line in its latent space. We show that this generative
model indeed learns to generate plausible, novel shape programs
that were never seen its the training set.

We evaluate our approach by comparing it to other recently-
proposed generative models of 3D shape structure along several

ShapeAssembly: Learning to Generate Programs for 3D Shape Structure Synthesis

‚Ä¢

234:3

axes including plausibility, diversity, complexity, and physical valid-
ity. We find that our generated shapes are both more plausible and
more physically-valid than those of other methods. Additionally, we
assess the latent spaces of these models, and find that ours is better
structured and produces smoother interpolations, both in terms
of geometric and structural continuity. As a bonus, we also show
that ShapeAssembly‚Äôs decoder does a better job of fitting programs
to unstructured point clouds while also maintaining physical valid-
ity, and that this performance difference is magnified by optimizing
the program fit via our differentiable interpreter.

In summary, our contributions are:
(i) The ShapeAssembly language and its differentiable interpreter,
allowing the procedural specification of shape structures rep-
resented as connected part assemblies.

(ii) A deep generative model for ShapeAssembly programs, cou-
pling the ease-of-training and variability of neural networks
with the precision and editability of procedural representa-
tions.

Code and data used for all of our experiments can be found at

https://github.com/rkjones4/ShapeAssembly .

2 RELATED WORK

Deep Generative Models of 3D Shapes. Recent years have seen an
explosion of activity in applying deep generative models to 3D
shape generation. Some of the earliest approaches generated shapes
as 3D occupancy grids [Wu et al. 2016; Z. Wu 2015]; later work
has explored generative representations of point clouds [Fan et al.
2017], 2D surface patches [Groueix et al. 2018], and implicit sur-
faces [Chen et al. 2019; Chen and Zhang 2019; Michalkiewicz et al.
2019; Park et al. 2019]. Our approach is more closely related to gen-
erative models of part-based shapes, wherein a complete object is
synthesized by generating and assembling multiple subparts. These
include approaches for iteratively adding parts to partially-complete
shapes [Sung et al. 2017], generating symmetry hierarchies [Li et al.
2017], composing parts from two different shapes [Zhu et al. 2018],
and generating hierarchical connectivity graphs [Mo et al. 2019a].
Our method is different in that we do not aim to learn a generative
model that outputs a single shape; rather, ours outputs a procedural
program which then itself generates a related family of shapes.

Procedural and Inverse Procedural Modeling. There is a rich history
of methods for procedural modeling in computer graphics: especially
noteworthy examples include its use in modeling plants [Prusinkiewicz
and Lindenmayer 1996] and urban environments [M√ºller et al. 2006;
Parish and M√ºller 2001]. Most procedural modeling systems use
some form of (context-free) grammar, i.e., a recursive string re-
writing system (which may be interpreted as e.g., recursively split-
ting a spatial domain, in the case of shape grammars). Additionally,
attachment-based grammars of part assemblies have been used to aid
in the structural analysis of shapes [Lau et al. 2011]. Our procedural
representation is fundamentally different: we use an imperative
language which iteratively constructs shapes via declaring and then
connecting parts represented as simple proxy geometry. Also related
to our work is the line of research on inverse procedural modeling,
i.e., inferring a procedural model from a set of examples [Hwang
et al. 2011; Martinovic and Van Gool 2013; Nishida et al. 2018, 2016;

Ritchie et al. 2018; Talton et al. 2012]. These methods all strive to
infer an interpretable, stochastic program which generates multiple
output shapes. In contrast, we represent shapes via deterministic
programs, and then we use a stochastic neural network to generate
those programs.

Visual Program Induction. Another related line of work to ours is
visual program induction (VPI): the practice of inferring a program
which describes a single visual entity, such as a 3D shape. We address
a fundamentally different problem: training a generative model to
generate novel 3D shape programs from scratch. We do use a VPI-like
process as a subroutine, to convert every shape in a large dataset into
training programs for our generative model. Prior work in this area
can be roughly divided into two categories: methods that assume
that clean, segmented geometry is available and then use geometric
heuristics to infer a program [Demir et al. 2016; Stava et al. 2010],
and methods which use learning or optimization to operate directly
on ‚Äúraw‚Äù visual inputs such as images and occupancy grids [Du
et al. 2018; Ellis et al. 2019, 2018; Liu et al. 2019; Sharma et al. 2018;
Tian et al. 2019; Zhou et al. 2019; Zou et al. 2017]. Our approach to
extracting programs from shapes to formulate training data is more
similar to the former.

One could consider solving our problem of novel shape program
generation by first generating novel 3D shapes with an existing
shape generative model and then using a VPI-like system to infer
a program describing that shape. However, as we will later show,
the programs produced by such a process are less clean and ed-
itable than ones generated by our model; furthermore, training to
generate programs rather than shapes directly actually produces a
better-structured latent space.

Our complete pipeline of using a neural network to generate a
program and then using that program to generate the ultimate out-
put is also related to work in visual question answering which uses
neural networks to generate a ‚Äúquery program‚Äù for each question
which then analyzes the input image and produces an answer [John-
son et al. 2017]. It is also related to work that tries to combine the
advantages of neural guidance with symbolic search for performing
inference over structured domains [Lu et al. 2019].

Our work is the first to train a deep generative model to pro-
duce novel shape programs from scratch, each of which outputs a
parametric family of related 3D shapes. This pipeline combines the
advantages of neural and procedural shape modeling.

3 APPROACH

Our approach (Figure 2) is divided into the following stages:

Input. Our pipeline takes as input a large dataset of hierarchical 3D
part graphs [Mo et al. 2019a,b]. This is a shape representation in
which each node represents a part in a shape consisting of an as-
sembly of parts. Nodes are connected via edges that denote physical
part attachments. They can also be connected via parent-child edges
that denote hierarchy relationships (i.e., that one part is composed
of several other smaller parts). At the leaf level of this hierarchy,
atomic parts are represented by cuboid proxy geometry (typically
minimum-volume bounding boxes of more detailed part meshes).

ACM Trans. Graph., Vol. 39, No. 6, Article 234. Publication date: December 2020.

234:4

‚Ä¢ R. Kenny Jones, Theresa Barton, Xianghao Xu, Kai Wang, Ellen Jiang, Paul Guerrero, Niloy J. Mitra, and Daniel Ritchie

Fig. 2. Our pipeline for generating 3D shape structure programs. We first define a DSL language for 3D shapes, ShapeAssembly . Then, given a dataset of
hierarchical part graphs, we extract ShapeAssembly programs from them. Finally, we use these programs as training data for a deep generative model. Our
method learns to generate novel program instances that can be executed to produce complex and interesting 3D shape structures.

Defining a DSL for connected, hierarchical shapes. To represent
shapes as programs, we introduce a domain-specific language (DSL).
Since our input shapes are characterized by graphs of parts, where
graph edges denote physical part connections, we introduce a DSL
based around declaring parts and then attaching them to one an-
other. We call this language ShapeAssembly (as in, an ‚Äúassembly
language‚Äù for shapes). Section 4 describes the language.

Creating a dataset of shape-program pairs. Given the language de-
scribed above, we present a method for finding programs that rep-
resent the shapes in our dataset.

In our procedure, we first extract the program content based on
a combination of data cleaning and geometric analysis. Then, we
create canonical programs through a series of ordering and filter
steps. Section 5 describes this procedure in more detail.

Learning to generate programs. Finally, we treat the programs ex-
tracted from each shape as training data for a generative model.
Section 6 describes our deep generative model‚Äôs architecture, the
procedure we use to train it, and how we sample from it to syn-
thesize new programs, which when executed produce novel shape
structures.

ACM Trans. Graph., Vol. 39, No. 6, Article 234. Publication date: December 2020.

Fig. 3. An example ShapeAssembly program and the shape that it generates.
Parts are colored according to the line of the program which instantiates
them, and attachment points are numbered accordingly. In the top shape, we
show the executed Chair program without hierarchy. In the bottom shape,
we show the Chair program executed hierarchically with its sub-programs
(Base and Back). For instance, the light grey back part is expanded into the
purple back surface and gold slats.

Input Hierarchical Part GraphsShapeAssembly DSL(Section 4)Shapes to Training Programs(Section 5)Learning to Generate Programs(Section 6)def Chair():  bbox = Cuboid(.7, 1.7, .5, True)  prog1 = Program1(.7, .6, .5, True)  prog2 = Program2(.7, .9, .05, True)  cube2 = Cuboid(.7, .15, .5, True)  attach(prog1, bbox, .5, 0, .5, .5, 0, .5)  attach(cube2, prog1, .5, 0, .5, .5, 1, .5)  squeeze(Prog2, bbox, cube2, top, .5, .1)def Program1(l, w, h, aligned):  bbox = Cuboid(.7, .6, .5, True)  prog3 = Program3(.05, .6, .5, True)  squeeze(prog3, bbox, bbox, top, 0, .5)  reflect(prog3, X)...def Chair():...def Chair():...def Chair():...def Chair():...executeexecuteexecuteexecute21222313141615768Root ProgramExpand Sub-Programs1.  def Chair():2.bbox= Cuboid(1, 1.5, .8, True)3.    base = Base(.8, .5, .8, True)4.cube1 = Cuboid(.8, .1, .8, True)5.    back = Back(.9, .8, .07, True)6.attach(base, bbox, .5, 0, .5, .5, 0, .5)7.attach(cube1, base, .5, 0, .5, .5, 1, .5)8.squeeze(back, bbox, cube1, top, .5, .05)9.  def Base(l, w, h, aligned):10.bbox= Cuboid(l, w, h, aligned)11.cube0 = Cuboid(.2, .5, .2, True)12.cube1 = Cuboid(.2, .5, .2, True)13.squeeze(cube0, bbox, bbox, top, .1, .1)14.squeeze(cube1, bbox, bbox, top, .1, .8)15.reflect(cube0, X)16.reflect(cube1, X)17. def Back(l, w, h, aligned):18.bbox= Cuboid(l, w, h, aligned)19.cube0 = Cuboid(.9, .4, .07, True)20.cube1 = Cuboid(.1, .4, .05, True)21.attach(cube0, bbox, .5, 1, .5, .5, 1, .5)22.squeeze(cube1, bbox, cube0, bot, .3, .5)23.translate(cube1, X, 2, .5)ShapeAssembly: Learning to Generate Programs for 3D Shape Structure Synthesis

‚Ä¢

234:5

‚Ä¢ CBlock: Declares all the cuboid part proxies that will be used
by the remainder of the program. The Cuboid command takes
in l, w, h parameters that control the starting dimensions of
the part, and an aligned flag a that specifies if the part has
the same orientation as its bounding volume.

‚Ä¢ ABlock: Connects cuboids by iteratively attaching them to
one another. The attach command takes in two cuboids, cn1,
cn2, and attaches the point (x1, y1, z1) in the local coordinate
frame of cn1 with the point (x2, y2, z2) in the local coordinate
frame of cn2. The squeeze macro expands into two attach
statements, such that cn1 is placed in-between cn2 and cn3
along the specified face f at the face-coordinate position
(u, v).

‚Ä¢ SBlock: Generates symmetry groups by instantiating addi-
tional Cuboid and attach commands. The reflect macro
reflects cuboid cn over axis axis of the bounding volume. The
translate macro creates a translational symmetry group
starting at cn with m additional members along axis a of the
bounding volume that ends distance d away.

Semantics. ShapeAssembly has imperative semantics: every line of
the program immediately takes effect and alters the state of the shape
being constructed. Figure 4 shows an example of a simple shape
being imperatively constructed. Declaring a cuboid instantiates
a new piece of cuboid geometry with the requested dimensions,
centered at the origin. Invoking the attach command alters the
cuboid, potentially translating, rotating, or resizing it in order to
satisfy the attachment (see Appendix A for details). Higher-level
macros expand into two or more Cuboid or attach lines, which are
then immediately executed (see Appendix B for details).

One distinct advantage of this imperative semantics, as opposed
to an alternative formulation in which the program specifies con-
straints which are jointly optimized, is that the entire process of

Start ‚àí‚Üí BBlock; CBlock; ABlock; SBlock;
BBlock ‚àí‚Üí bbox = Cuboid(l, h, w, True)
CBlock ‚àí‚Üí cn = Cuboid(l, w, h, a) ; CBlock | None
ABlock ‚àí‚Üí Attach ; ABlock | Squeeze ; ABlock | None
SBlock ‚àí‚Üí Reflect ; SBlock | Translate ; SBlock | None
Attach ‚àí‚Üí attach(cn1, cn2, x1, y1, z1, x2, y2, z2)
Squeeze ‚àí‚Üí squeeze(cn1, cn2, cn3, f , u, v)
Reflect ‚àí‚Üí reflect(cn, axis)
Translate ‚àí‚Üí translate(cn, axis, m, d )
f ‚àí‚Üí right | left | top | bot | front | back
axis ‚àí‚Üí X | Y | Z
l, h, w ‚àà R+
x, y, z, u, v, d ‚àà [0, 1]2
a ‚àà [True, False]
n, m ‚àà Z+

Table 1. The grammar for ShapeAssembly, our low-level domain-specific
‚Äúassembly language‚Äù for shape structure. A program consists of Cuboid
statements which instantiate new geometry and attach statements which
connect these geometries together at specified points on their surfaces.
Macro functions (reflect, translate, squeeze) form complex spatial rela-
tionships by expanding into multiple Cuboid and attach statements.

ACM Trans. Graph., Vol. 39, No. 6, Article 234. Publication date: December 2020.

Fig. 4. An illustration of how the ShapeAssembly interpreter incrementally
constructs shapes by imperatively executing program commands. Cuboids
are instantiated at the origin and are moved through attachment. Notice
how the reflect command in line 6 acts as a macro function, creating a
new cuboid and two new attachments.

4 AN ASSEMBLY LANGUAGE FOR SHAPES

Our goal in this section is to define a domain-specific language for
shapes which are specified as connected assemblies of parts. As we
focus on the problem of shape structure synthesis, cuboids, serving
as part proxy geometry, are the only data type in our language. In
Section 7, we show how to use other existing techniques to convert
these proxies into surface geometry.

The primary operation in the language is attaching these cuboids
together. Attachment turns out to be a very powerful and flexi-
ble operation. In fact, our language does not include any opera-
tions for explicitly positioning or orienting cuboids: all of this is
accomplished via attachment operations. Additionally, the language
includes higher-level macros that capture more complex spatial
relationships, such as symmetry. At execution time, each macro
is expanded into a series of cuboid declarations and attachment
operations.

We call this DSL ShapeAssembly, because it is an ‚Äúassembly
language for shapes‚Äù: a low-level language for creating shapes, in
which shapes are created by assembling parts. Table 1 shows the
grammar for ShapeAssembly, and Figure 3 shows an annotated
hierarchical program along with its executed 3D shape.

A ShapeAssembly program consists of four main blocks:

‚Ä¢ BBlock: Declares a non-visible bounding volume of the over-
all shape. This bounding volume is treated as a physical entity
to which other parts can be connected.

    (1) attach(cube0, bbox, .5, 0, .5, .5, 0, .5)(2) attach(cube1, cube0, .5, 0, .5, .5, 1, .5)(3) squeeze(cube2, bbox, cube1, top, .5, .18)(4) attach(cube3, cube2, .5, .5, 0, .1, .1, 1)(5) attach(cube3, cube1, .5, 0, .5, .1, 1, .7)123456bbox = Cuboid(.7, 1.8, .6, True)cube0 = Cuboid(.6, .6, .6, True)cube1 = Cuboid(.6, .2, .6, True)cube2 = Cuboid(.6, .9, .2, True)cube3 = Cuboid(.2, .2, .4, True)(6) reflect(cube3, X)234:6

‚Ä¢ R. Kenny Jones, Theresa Barton, Xianghao Xu, Kai Wang, Ellen Jiang, Paul Guerrero, Niloy J. Mitra, and Daniel Ritchie

Fig. 5. The steps of our program extraction pipeline. (a) Fragment of an input hierarchical part graph showing chair back (parent node), chair back frame (blue
child), and chair back surface (orange child). (b) Locally flattening the hierarchy so that physically interacting leaf parts become siblings. (c) Shortening leaf
parts that intersect other leaf parts. (d) Locating attachment points between parts. (e) Forming leaf parts into symmetry groups.

executing a program is end-to-end differentiable. That is, it is possi-
ble to compute the gradient of the program‚Äôs output geometry with
respect to the continuous parameters in the text of the program
(e.g., cuboid dimensions, attachment point locations). We make use
of this feature in results shown later in this paper.

Handling hierarchy. Thus far, we have described a language that
can generate flat assemblies of parts, but not hierarchical ones. The
extension to hierarchical shapes is straightforward: we represent hi-
erarchical shapes by treating select non-leaf cuboids as the bounding
box of another program (e.g., the contents of its ‚ÄúBBlock‚Äù). Figure 3
shows an example of a program in which cuboids expand into sub-
programs.

5 TURNING SHAPES INTO TRAINING PROGRAMS

ShapeAssembly allows us to write programs that generate new
shapes. However, we are interested in using the language to repre-
sent existing shapes in a dataset, so that we can learn to generate
novel instances from the same underlying shape distribution. In
this section, we describe how we accomplish this goal. Given an
input shape, represented as a hierarchical part graph, the process
divides into three steps: extracting program information, creating
candidate programs, and checking program validity.

5.1

Extracting Program Information

To convert hierarchical part graphs into ShapeAssembly programs,
we perform a series of data regularizations, record cuboid parame-
ters, locate cuboid-to-cuboid attachments, and identify symmetry
groups (Figure 5). We provide a high-level overview of the steps
involved here, and a detailed description in Appendix C.

Regularization. Before we parse program attributes, we attempt
to create more regularized part graphs through a series of data-
cleaning steps. For instance, in the flattening phase, we restructure
the part graph hierarchy so that leaf parts with spatial relationships
are more often siblings. In the shortening phase, we decrease the
dimensions of leaf cuboids that interpenetrate other leaf cuboids (to
create more surface-to-surface part connections).

ACM Trans. Graph., Vol. 39, No. 6, Article 234. Publication date: December 2020.

Cuboids. Ground truth cuboid dimensions are provided in the input
part graphs. A cuboid is marked as aligned if its orientation matches
its parent cuboid (with an allowable error of 5-degrees).

Attachment. To locate cuboid-to-cuboid attachments, we sample a
uniform, dense point cloud on each cuboid in the scene. For each
pair of cuboids, we compute the intersection of the point clouds.
If the intersection set is non-zero, we record an attachment point
within the volume formed by the intersection, with preference for
locations on the centers of faces. For every cuboid, we then check
if any of its parsed attachments could be represented as a squeeze
relationship, and replace any that can.

Symmetry. To find symmetry groups, we identify collections of
cuboids that share a reflectional or translational relationship about
either the X, Y, or Z axis of their parent cuboid. For each collection, if
all of the member cuboids have the same connectivity relationships,
we form them into a symmetry group. Each symmetry group is
represented by a transform applied to a single cuboid, and all other
members are removed from the graph.

5.2 Creating Candidate Programs

Given the extracted program information, we know the content
of the program, but not how the lines should be ordered. To make
the task of learning a generative model of programs easier, we aim
to extract only a single, ‚Äúcanonical‚Äù program for each shape. As
the ordering of cuboid and symmetry lines doesn√¢ƒÇ≈πt change the
executed geometry, this consistency is enforced by ordering these
lines according to the semantic label of each part involved in the line.
Ties in this ordering between same part-type cuboids are broken by
sorting on centroid position.

Deciding on a single ordering of the attach and squeeze statements
is more challenging. Since ShapeAssembly has an imperative exe-
cution semantics, the order in which these commands are executed
is significant: different orderings can potentially create different
output geometries. To reduce the space of possible orderings, we
only consider programs which follow a grounded attachment order,
which we define as follows:

‚Ä¢ Initially, only the shape bounding box is grounded.

abcdeShapeAssembly: Learning to Generate Programs for 3D Shape Structure Synthesis

‚Ä¢

234:7

‚Ä¢ The only valid attachments to perform are those which con-

nect a cuboid to a grounded cuboid.

‚Ä¢ After executing an attachment, the newly-attached cuboid

becomes grounded.

If there are multiple valid grounding orders, we first discard any
orderings that produce worse geometric fits to the target shape.
If ambiguities in the attachment ordering still remain, we break
ties using (1) the semantic ordering of the cuboids involved in the
attachment (2) preferring attachments from non-aligned to aligned
cuboids and finally (3) preferring attachments from cuboid face-
centers.

Gated Recurrent Unit (GRU), a recurrent language model which is
responsible for constructing a representation of the program state.
The output of the GRU cell is sent to the line decoder sub-routine,
which predicts a line in the ShapeAssembly grammar, that is then
passed as input back to the GRU cell at the next time step.

The purple callout in Figure 6 gives a detailed depiction of the
line decoder sub-routine. The line decoder receives the hidden state
of the GRU cell, along with conditioning information about the size
of the current bounding volume, and uses a collection of multilayer
perceptrons (MLPs) to predict a 63-dimensional vector representing
a single line in ShapeAssembly . The sub-networks it uses are:

5.3 Validating Programs

Once we extract a canonical ShapeAssembly program, we perform
a series of checks to verify the results of our procedure. Programs
must pass the following validation steps in order to be added to our
training data:

Reconstruction. Executed programs should recreate the geometry of
their respective ground truth part graph. To verify this, we sample
point clouds from the surfaces of the ground truth shape and the
geometry generated by executing the canonical program. These
point clouds are compared using the F-score [Knapitsch et al. 2017]
metric; a program is filtered out if it produces an F-score lower than
75.

Semantics. Programs must respect the semantics of ShapeAssem-
bly. For instance, within each program, the connectivity graph of
all parts should have only one component. Likewise, executed pro-
grams should not create geometry that extends beyond the bounding
volumes they define.

Complexity. Programs that are overly complex (more than 12 leaf
cuboid instantiations) are discarded. Note that, when executed, pro-
grams can still produce more than 12 leaf cuboids through expanding
symmetry macros.

6 LEARNING TO GENERATE PROGRAMS

Given the programs extracted from our dataset, we now have the
data we need to train a neural network to write novel hierarchi-
cal ShapeAssembly programs for us. In this section, we describe
the generative model architecture we use, our learning procedure,
and how we sample new shapes from the learned model.

6.1 Model Architecture

Figure 6 shows our generative model architecture. It is a hierarchi-
cal sequence VAE. The encoder branch embeds a hierarchical Sha-
peAssembly program into a latent space. The decoder branch con-
verts a point in this latent space into a hierarchical ShapeAssem-
bly program. The bottleneck of our network is parameterized by
separate ¬µ and œÉ vectors in the standard variational autoencoder
(VAE) setup.

The dark grey callout in Figure 6 illustrates the operation of
our decoder within a single node of the program hierarchy. The
decoder receives as input the latent code zpar of its parent node (or
the root latent code from the encoder, if it is the root node of the
hierarchy). This latent code is used to initialize the hidden state of a

‚Ä¢ fcmd: (7): Predicts the type of command to execute. This is a
one-hot vector whose seven entries correspond to <start>
(the special program start token), <stop> (the special pro-
gram stop token), Cuboid, attach, squeeze, translate and
reflect.

‚Ä¢ fcube: (4): Predicts the length, width, height, and aligned
flag for cuboid lines, conditioned on the bounding volume
dimensions.

‚Ä¢ fidx: (11 √ó 3): Predicts the indices of the cuboids involved in
the line represented as 3 one-hot vectors, conditioned on the
predicted command. We limit each node in the hierarchy to
contain at most 10 children parts, so there are 11 choices (10
cuboids and the bounding volume).

‚Ä¢ fatt: (3 √ó 2): Predicts the (x, y, z) coordinates involved in
an attach line, conditioned on the cuboids involved in the
attach.

‚Ä¢ fsqz: (8): Predicts the the face involved in a squeeze line as a
one-hot vector in the first 6 indices. The last 2 indices predict
the (u, v) coordinates. Both predictions are conditioned on
the cuboids involved in the squeeze operation.

‚Ä¢ fsym: (5): Predicts the axis involved in a symmetry line as a
one-hot vector in the first 3 indices. For translate lines, the
4th index is the number of cuboids involved in the symmetry
group, and the 5th index is the scale of the symmetry. All
predictions are conditioned on the cuboid involved in the
symmetry and the bounding volume dimensions.

Hierarchical decoding. To generate a hierarchical program, our de-
coder also includes a submodule fchild which is executed after ev-
ery predicted Cuboid command to determine whether that cuboid
should be recursively expanded. This is another MLP which takes
as input both the current hidden state of the GRU as well as zpar,
the overall latent code for this hierarchy node. fchild produces two
outputs: a Boolean flag for whether the current cuboid should be
expanded into a child program, and a new latent code zchild which
is used to initialize the decoder for this child program.

6.2 Learning Procedure

We implement our models in PyTorch[Paszke et al. 2017]. All train-
ing is done with the Adam optimizer [Kingma and Ba 2014], with a
learning rate of 0.0001 without batching.

All multilayer perceptrons have 3 layers and use leaky ReLU [Maas

2013] with Œ± = 0.2.

We train our model in a seq2seq fashion, where the ground truth
input sequence is teacher forced to the model, and our model is

ACM Trans. Graph., Vol. 39, No. 6, Article 234. Publication date: December 2020.

234:8

‚Ä¢ R. Kenny Jones, Theresa Barton, Xianghao Xu, Kai Wang, Ellen Jiang, Paul Guerrero, Niloy J. Mitra, and Daniel Ritchie

Fig. 6. Architecture of our hierarchical sequence VAE for ShapeAssembly programs. Given a ShapeAssembly program, the encoder ascends the hierarchy from the
leaves to the root, encoding each sub-program into a latent z vector. Given a latent code, the decoder recursively decodes a hierarchical ShapeAssembly program.
Within each hierarchy node, a recurrent neural network decodes each line of the program.

tasked with predicting each subsequent line. During training, we
use a program reconstruction loss that only considers entries of
the predicted 63 dimensional vector that are relevant to the target
line. For instance, when predicting a Cuboid line, no part of the
reconstruction loss comes from the indices in the tensor associated
with symmetry. The program reconstruction loss is comprised of a
cross-entropy component for each one-hot prediction (with weight
1) and an l1 loss for each continuous component (with weight 50).
Additionally we use a KL loss in the standard VAE setup with weight
0.1 [Kingma and Welling 2014].

Enforcing semantically-valid output. As our model generates shape
programs, rather than raw shape geometry, we can use the semantics
of the ShapeAssembly language to detect outputs that would be
invalid, and prevent them from happening. For instance, attaches
must be made in a grounded order. If a predicted attach line vio-
lates such a constraint, we use a backtracking procedure to find new
‚Äòvalid‚Äô parameter values whenever possible. During unconditional
generation, if we cannot fix the line through backtracking, we reject
the sample. During interpolation, if we cannot fix the line through
backtracking we don‚Äôt add the predicted line to the program. Ap-
pendix D describes the complete semantic validity procedure we
enforce. We also note that this approach to forbidding the genera-
tion of invalid outputs is similar to that of the Grammar Variational
Autoencoder [Kusner et al. 2017]. However, that model only uses
grammar syntax to determine whether an output is valid, whereas
as we use program semantics.

7 RESULTS AND EVALUATION

In this section, we demonstrate our learned generative model‚Äôs
ability to synthesize high-quality hierarchical ShapeAssembly pro-
grams, and we compare it to alternative generative models of 3D
shape structure. All of the experiments described were run on a
GeForce RTX 2080 Ti GPU with an Intel i9-9900K CPU, and con-
sumed 3GB of GPU memory.

We use objects from the PartNet dataset [Mo et al. 2019b] as our
training data. It contains 3D shapes in multiple categories, each with
a hierarchical part segmentation and labeling. For the experiments

ACM Trans. Graph., Vol. 39, No. 6, Article 234. Publication date: December 2020.

in this paper, we use the Chairs, Tables, and Storage categories. After
running the extraction procedure described in Section 5, we obtain
3835 Chair, 6536 Table, and 1551 Storage ground truth programs.

7.1 Novel Shape Synthesis

In this section, we present both qualitative and quantitative eval-
uations of our method‚Äôs ability to produce novel shape structures.
Figure 7 includes some unconditionally generated samples from our
learned generative model for each of the three shape categories.
Above each sample we show its nearest neighbor in the training
data based on Chamfer distance. Additionally, below each sample
we visualize its nearest neighbor in the training data based on pro-
gram distance, the string edit distance of a tokenized version of our
hierarchical programs. As shown, our method is able to generate
complex and interesting structural variation without copying either
the geometry or program structure of its training data.

As our model directly generates programs, its outputs can be
easily edited to produce variants. In Figure 8 we demonstrate how
by changing just the continuous parameters of programs generated
by our model, we are able to create a wide variety of output geometry,
all the while maintaining part-to-part attachment relationships.

We compare the generated results of our method against two

baselines:

‚Ä¢ StructureNet is a variational autoencoder that generates
hierarchical part graphs with cuboids at each node [Mo et al.
2019a].

‚Ä¢ 3D-PRNN is a recurrent neural network that generates a se-
quence of cuboids [Zou et al. 2017]. It enforces global bilateral
symmetry by only generating cuboids with some part of their
geometry on the negative side of the x = 0 plane, and then
reflecting generated cuboids which fall entirely on that side
of the plane.

We compare against the StructureNet models released by the
authors. These were trained on the subset of PartNet that they were
able to represent within the constraints of their problem formulation.
This is a heavily overlapping set, but not identical, with the shapes
we were able to find valid ShapeAssembly programs for. In direct

ùúáùúéùí©(ùúá,ùúé)ùëßGRUùëß!"#Line 1GRUùëì!"#$GRU......ùëì%&"!‚Äôùëì%&"!‚Äôùëß$%&‚Äô(DecoderEncoderLine 1ùëì!"#$Line 2Line 3Line ùëÅùëì!"#$ùëì!"#$GRUGRUGRUGRU......ùëß$%&‚Äô(Line DecoderLine 2Line DecoderLine ùëÅLine Decoder.........Line DecoderGRUBBoxdimsCuboid( ùëô, ùë§, ‚Ñé, ùëé)attach( ùëê(, ùëê), ùë•ùë¶ùëß(, ùë•ùë¶ùëß))squeeze( ùëê(, ùëê), ùëê*, ùëì, ùë¢ùë£)reflect( ùëê(, axis )translate( ùëê(, axis, ùëö, ùëë)ENDùëì+,-ùëì.//ùëì+01ùëì%23$ùëì+,-ùëì"‚Äô4ùëì"‚Äô4ùëì%-‚Äôùëì"‚Äô4ùëì"‚Äô4ShapeAssembly: Learning to Generate Programs for 3D Shape Structure Synthesis

‚Ä¢

234:9

N
N
m
o
e
G

e
l
p
m
a
S

N
N
g
o
r
P

Fig. 7. In the middle row, we show samples from our generative model of ShapeAssembly programs. In the top row, we show the nearest neighbor shape
in the training set by Chamfer distance. In the bottom row, we show the nearest neighbor shape in the training set by program edit distance. Our method
synthesizes interesting and high-quality structures that go beyond direct structural or geometric memorization. We quantitatively examine ShapeAssembly‚Äôs
generalization in Table 4. Refer to the supplemental material for the corresponding program text.

comparisons with StructureNet for reconstruction tasks, we only
consider shapes that appear in the validation splits of both methods.
We compare against a version of 3D-PRNN that was re-trained on
the data we use for our generative model. Figure 9 shows a qualita-
tive comparison of unconditionally generated samples from each
method. Our method is capable of generating diverse, structurally
complex, 3D shape structures across multiple categories. Attach-
ment as a primary operation provides a strong inductive bias for
generating physically plausible shapes that maintain realistic part-
to-part relationships. In contrast, both comparison methods that
directly predict part placements in 3D space are prone to producing
floating cuboids or jumbled collections of spatially colocated parts.

e
l
p
m
a
S

t
n
a
i
r
a
V

Fig. 8. Programs, by way of representational form, allow for easy semantic
editing of generated output. Each column shows a sample from our model in
the top row. In the bottom row we create a variant with the same structure,
but different geometry, by editing only the continuous parameters of the
program. Program text can be found in the supplemental material.

7.1.1 Analysis of Shape Quality. We also quantitatively compare
the quality of the shape structures generated by different methods.
Our desiderata for generated shape structures is that they should
be physically plausible and come from the same distribution that
the model was trained on. In order to asses the quality of generated
output, we use the following metrics:

‚Ä¢ Rootedness ‚áë (% rooted): The percentage of shapes for
which a connected path exists between the ground and all
leaf parts.

‚Ä¢ Stability ‚áë (% stable): The percentage of shapes which re-
main upright under gravity and small forces in a physical
simulation.

‚Ä¢ Realism ‚áë (% fool): The percentage of test set shapes classi-
fied as ‚Äúgenerated‚Äù by a PointNet classifier trained to distin-
guish between generated shapes and shapes from the training
dataset.

‚Ä¢ Frechet Distance ‚áì (FD): Measurement of distributional sim-
ilarity between generated shapes and the training dataset us-
ing the feature space of a pre-trained PointNet model [Heusel
et al. 2017]

Further details about these metrics are provided in Appendix E.

We show results for these metrics on 1000 unconditional gener-
ated shapes in Table 2. Our method largely outperforms 3D-PRNN
and StructureNet across these metrics for three categories of shapes.
While StructureNet achieves good rootedness scores, especially
for the Storage category, our method performs better in the other
three metrics along all categories. The samples from 3D-PRNN,
achieve similar FD and % fool scores with StructureNet, but perform
markedly worse on the rootedness and stability metrics.

Additionally in this experiment we compare our model with a

series of ablated versions:

ACM Trans. Graph., Vol. 39, No. 6, Article 234. Publication date: December 2020.

234:10

‚Ä¢ R. Kenny Jones, Theresa Barton, Xianghao Xu, Kai Wang, Ellen Jiang, Paul Guerrero, Niloy J. Mitra, and Daniel Ritchie

‚Ä¢ Flat: Training on programs with no hierarchies, only leaf

Category Method

% rooted ‚áë % stable ‚áë % fool ‚áë FD ‚áì

parts.

‚Ä¢ No Order: Training on programs without canonical ordering

as described in Section 5.

‚Ä¢ No Align: Training on programs without an aligned flag for

cuboids.

‚Ä¢ No Macros: Training on programs without squeeze, translate,

or reflect commands.

‚Ä¢ No Reject: At generation time, discard unfixable, invalid
program line predictions instead of rejecting the entire sample.

Training without hierarchy (Flat) slightly improves rootedness,
but drastically lowers the quality of output as seen in the % fool
and FD columns. Training on programs without a canonical or-
dering (No Order) performs worse on every metric. Removing the
alignment flag (No Align) actually improves performance on the
Chair category for % rooted and % fool, but drastically worsens the
physical validity of generations for Tables and Storage, categories
where parts are much more often aligned with their parent cuboid.
Training without macros (No Macros) once again decreases the per-
formance of all metrics, but not by a substantial margin. Finally, we
see that while the rejection sampling step does improve the quality
of our generated samples, without it we still outperform 3D-PRNN
and StructureNet by a wide margin.

7.1.2 Analysis of Editability. In this section, we quantitatively ana-
lyze our previous claim that directly predicting programs improves
editability. We claim that a program is more editable if it is both com-
pact and compromised of higher level functions. That is, a shorter
program that uses higher-level constructs will be easier to under-
stand and make changes to.

As a strong baseline, we evaluate the editability of our programs
against the generated outputs of 3D-PRNN and StructureNet. As
3D-PRNN and StructureNet do not directly produce ShapeAssem-
bly programs, we use our extraction procedure described in Section
5 in order to convert their generations into programs. As Struc-
tureNet predicts part graph hierarchies, the representational form
our extraction procedure takes as input, we use our procedure with-
out any of the data cleaning steps. As 3D-PRNN has no notion of
hierarchy, we create single node part graphs out of their output
samples, which are then run through our program extraction logic.
Table 3 shows results from an experiment where we compare
the ShapeAssembly programs of each method‚Äôs generations (di-
rectly predicted by our method, parsed programs from comparisons).
The metrics we use are the number of lines in each program (as a
coarse measure of compactness) and the percentage of lines which
are macros (split by macro type).

Compared with programs parsed from StructureNet, the pro-
grams generated by our model are much more compact and have
higher rates of macro usage across all categories of shapes. While
our method also has higher macro rate usage compared with 3D-
PRNN, 3D-PRNN programs are more compact in the Chair and
Table categories. Based on 3D-PRNN‚Äôs poor performance within our
shape quality experiments (Table 2), and its significant deviation
from the number of lines found in the ground truth programs (the
cleanest set of ShapeAssembly programs we have access to), there

ACM Trans. Graph., Vol. 39, No. 6, Article 234. Publication date: December 2020.

Chair

Table

3D-PRNN
StructureNet
Ours (Flat)
Ours (No Order)
Ours (No Align)
Ours (No Macros)
Ours (No Reject)
Ours
Ground Truth

3D-PRNN
StructureNet
Ours (Flat)
Ours (No Order)
Ours (No Align)
Ours (No Macros)
Ours (No Reject)
Ours
Ground Truth

73.1
89.7
95.0
82.4
94.6
92.0
92.9
94.5
100

71.2
94.4
87.0
84.5
92.2
95.9
94.1
96.2
100

50.9
74.9
60.0
58.4
84.6
77.9
79.7
84.7
88.0

29.4
76.8
66.0
56.0
61.5
85.0
76.4
85.9
93.1

12.60
4.04
11.58
12.36
28.68
19.56
23.36
25.06
‚Äî

2.12
3.94
29.84
27.38
23.64
33.16
29.20
33.21
‚Äî

39.30
64.79
77.45
64.17
29.32
36.78
20.63
22.34
‚Äî

140.07
173.35
148.63
114.10
46.64
53.21
52.78
49.07
‚Äî

Storage

3D-PRNN
StructureNet
Ours (Flat)
Ours (No Order)
Ours (No Align)
Ours (No Macros)
Ours (No Reject)
Ours
Ground Truth

44.8
96.2
95.9
87.9
89.7
87.5
94.3
95.3
100
Table 2. Comparing the quality of generated samples. Our method outper-
forms other generative methods for 3D shape structure in terms of realism
and physical validity. Through a series of ablation baselines, we validate
various design decisions of our method.

94.08
92.85
81.17
107.42
30.15
72.80
31.69
31.72
‚Äî

4.62
5.04
7.44
8.70
11.04
5.92
11.66
13.50
‚Äî

20.8
75.0
74.0
63.4
49.3
69.9
80.9
83.7
87

is reason to believe that the compactness of its parsed programs
more likely reflects shape simplicity rather than useful editability.

7.1.3 Analysis of Variability. Beyond quality and editability, we also
consider the variability of outputs of each method. Specifically, for
generated shapes, we care about their novelty with respect to the
training data, their complexity, and their variety. We present results
of an experiment using Chamfer distance to quantify performance
across these areas in Table 4.

The Generalization metric measures the average distance of each
generated sample to its nearest neighbor in the training set. As all
methods have higher generalization scores than the validation set,
we can conclude that none of the methods appear to be overfitting.
For our method specifically, this re-enforces the qualitative nearest
neighbor results presented in Figure 7.

The Coverage metric measures the average distance of each vali-
dation shape to its nearest neighbor in the set of generated shapes.
Across all categories our method achieves the best results, and
by a wide-margin for tables, which indicates that our generations
have enough complexity to match the distribution of the validation
shapes.

The Variety metric measures the average distance of each gen-
erated shape to its nearest neighbor in the set of generated shapes

3D-PRNN

Ours

StructureNet

ShapeAssembly: Learning to Generate Programs for 3D Shape Structure Synthesis

‚Ä¢

234:11

3D-PRNN

Ours

StructureNet

3D-PRNN

Ours

StructureNet

r
i
a
h
C

e
l
b
a
T

e
g
a
r
o
t
S

Fig. 9. Qualitative comparison between generated samples from our method, StructureNet, and 3D-PRNN. Across different categories, our method creates
novel ShapeAssembly programs that, when executed, produce shape structures that maintain realistic and physically valid part-to-part relationships.
Comparison methods that directly predict 3D shape geometry exhibit failure cases where parts become disconnected or intersect in an implausible manner.

ACM Trans. Graph., Vol. 39, No. 6, Article 234. Publication date: December 2020.

234:12

‚Ä¢ R. Kenny Jones, Theresa Barton, Xianghao Xu, Kai Wang, Ellen Jiang, Paul Guerrero, Niloy J. Mitra, and Daniel Ritchie

Category Method

Lines ‚áì Refl ‚áë Trans ‚áë Squeeze ‚áë Total ‚áë

Macros Per Line

Chair

Table

Storage

3D-PRNN
StructureNet
Ours
Ground Truth

3D-PRNN
StructureNet
Ours
Ground Truth

3D-PRNN
StructureNet
Ours
Ground Truth

15.7
27.1
20.4
24.4

13.1
24.8
19.0
20.0

22.6
30.7
19.8
24.7

0.1100 0.0020
0.0600
0.0004
0.0880 0.0054
0.0090
0.0800

0.1300 0.0010
0.0006
0.0270
0.0002
0.0990
0.0050
0.0950

0.0060
0.0170
0.0390
0.0040
0.0820 0.0080
0.0147
0.0650

0.0240
0.0700
0.0920
0.1130

0.0680
0.0620
0.1440
0.1450

0.0530
0.0770
0.1440
0.1510

0.1430
0.1330
0.1860
0.2070

0.1990
0.0900
0.2440
0.2460

0.0770
0.1200
0.2340
0.2320

Table 3. Markers of program editability for ShapeAssembly programs pre-
dicted by our generative model compared with ShapeAssembly programs
parsed from outputs of other generative methods. Training our model in
the space of programs allows us to represent geometry more compactly. We
find higher rates of macro functions per program line in our method‚Äôs gen-
erations compared with extracting programs from other generative models‚Äô
predictions.

besides itself. Once again, across all categories our method achieves
top, or tied for top performance.

Additionally, we look at average number of leaf parts as a coarse
proxy for the complexity of a shape‚Äôs structure, which is shown in
Table 5. While our method has a similar number of leaf parts to
the comparison methods for the Chair and Table categories, we do
have fewer leaf parts on average for Storage. Qualitatively, these
additional parts in the comparison methods often manifest as col-
lections of spatially colocated cuboids, and not necessarily more
complex shape structures.

In terms of the variability of programs generated by our method,
we note that 65% of Chair programs, 85% of Table programs, and 53%
of Storage programs contained ShapeAssembly program structures
not present in the training data. Thus our method not only exhibits
novelty in the geometric domain, but also in the structural domain.

7.1.4 Program Clustering. Our approach is predicated on the as-
sumption that a single program can represent a parametric family of
multiple shapes, allowing for this shape space to be explored via ma-
nipulation of interpretable program parameters. To verify whether
this is true, we cluster shapes that are represented by structurally-
equivalent programs (i.e. programs that are the same up to contin-
uous parameter variations). Figure 10 shows program clustering
results for the ground truth programs we parse from PartNet. These
results demonstrate how the structure of a single ShapeAssem-
bly program is able to represent related shapes through different
parameterizations. The marked improvement in clustering when
splitting by intermediate part programs compared with clustering
on entire shape programs, provides additional support for our hier-
archical approach; shape programs are more likely to share structure
within a node of the hierarchy than they are to match entire hierar-
chies exactly.

ACM Trans. Graph., Vol. 39, No. 6, Article 234. Publication date: December 2020.

Category Method

Chair

Table

Storage

3D-PRNN
StructureNet
Ours
Validation

3D-PRNN
StructureNet
Ours
Validation

3D-PRNN
StructureNet
Ours
Validation

Coverage
Generalization
NND to Train ‚áë NND from Val ‚áì NND to Self ‚áë
CD

Variety

CD

CD

0.111
0.104
0.108
0.105

0.095
0.129
0.101
0.09

0.134
0.129
0.125
0.11

0.123
0.119
0.118
‚Äî

0.130
0.141
0.108
‚Äî

0.132
0.135
0.129
‚Äî

0.104
0.087
0.104
0.114

0.086
0.0925
0.102
0.099

0.119
0.107
0.119
0.125

Table 4. We compare the geometric variability of generated shapes from
different methods. In the first column, we measure generalization as the av-
erage nearest neighbor distance (NND) from generated samples to shapes in
the training set. In the second column we measure coverage as the average
NND from shapes in the validation set to generated samples. In the last col-
umn, we measure variety as the average NND from shapes in the generated
samples to other generated shapes in the same set. Across three categories
of shapes, our method performs the best on the coverage and variety met-
rics, while outperforming validation on generalization (demonstrating we
are not overfitting).

Category Method

Avg # Leaf Parts

Chair

Table

Storage

3D-PRNN
StructureNet
Ours
Ground Truth

3D-PRNN
StructureNet
Ours
Ground Truth

3D-PRNN
StructureNet
Ours
Ground Truth

8.6
8.7
7.9
9.7

7.07
8.16
7.84
8.4

10.6
12.3
8.4
10.8

Table 5. We compare the average number of leaf parts in generated shapes,
as a coarse proxy for complexity of shape structure. Our method generates
similar numbers of leaf parts compared with other methods for Chairs
and Tables, but fewer leaf parts for Storage. Qualitatively, the additional
leaf parts measured in comparison methods often manifests as spurious
overlapping cuboids, rather than more complex structural variety.

Synthesizing Surface Geometry. While collections of part
7.1.5
proxies are a useful modeling representation for 3D shape struc-
tures, they do not directly attempt to capture the wide range of
intra-part variability present in man-made objects. We demonstrate
how ShapeAssembly programs can additionally be used to model
parts at finer levels of detail by turning ShapeAssembly programs
into dense point clouds. As a proof of concept, we augment our
generative model with a point cloud encoder that consumes dense
point cloud samples of ground truth leaf parts, and a point cloud
decoder that generates dense point clouds for every leaf part within

ShapeAssembly: Learning to Generate Programs for 3D Shape Structure Synthesis

‚Ä¢

234:13

Category Method

Avg. Step Size ‚áì
Prog
Geo

Chair

Table

Storage

StructureNet 0.0384
0.0384
Ours

StructureNet 0.0474
0.0389
Ours

StructureNet 0.0512
0.0482
Ours

3.90
1.33

4.75
2.48

4.29
2.6

Table 6. We measure smoothness along random high-frequency interpo-
lation sequences in each method‚Äôs latent space. The Geo column mea-
sures smoothness with Chamfer distance, while the Prog column measures
smoothness with program edit distance. Note that 3D-PRNN is missing
because it is not a latent variable model and thus does not support interpo-
lation.

Fig. 10. Clustering results that demonstrate how the structure of a sin-
gle ShapeAssembly program is capable of capturing a family of related
shapes. Using ground truth programs found with our program extraction
procedure, in the left graph we plot the percentage of shapes captured as
we consider more program structures extracted from the data. In the right
graph we show the same plot but with parts (nodes) instead of shapes (full
hierarchy).

its predicted bounding volume. Figure 11 shows some qualitative
results of our method, trained on point clouds sampled from the
dense geometry of Chairs found in PartNet. These generated sur-
faces provide additional detail over the geometry specified by their
cuboid part proxies, as evidenced by both the rounding in the legs
and back slats, and also in the curvature of the chair back surfaces.

7.2 Latent Space Interpolation

Beyond novel shape generation, we evaluate the ability of our
method to interpolate between two points in our latent space. The
presence of smooth, semantic transitions between end-points in-
dicates a well-formed latent space. In Figure 12 we qualitatively
compare our method with StructureNet on the task of interpolating
between shapes in the validation sets of both models. Our interpola-
tions demonstrate both geometrically smooth and semantically con-
sistent transitions. For instance, in the top interpolation sequence,
the surface of the chair back in the source shape gradually shrinks
vertically until in the target shape it is just a horizontal bar. At the
same time, the number of vertical slats in the chair back gradually
increases from 2, to 4, to 5.

s
d
i
o
b
u
C

t
r
a
P

s
t
n

i
o
P
e
c
a
f
r
u
S

Fig. 11. Converting generated ShapeAssemblyprograms into dense point
clouds. We use a point cloud decoder to predict the surface geometry of
each leaf part proxy in our 3D shape structure. In this process, geometric
details begin to take form, at the cost of some artifacts. We discuss a method
for improving this procedure in section 8.

In Table 6, we attempt to quantify the smoothness along random
interpolation sequences within the latent space of each generative
model. In this experiment, 100 interpolation sequences were com-
puted from sources to targets that were randomly sampled in each
model‚Äôs latent space, with 100 interpolation steps per sequence.
Each method‚Äôs geometric smoothness is computed by taking the
average Chamfer distance (normalized by shape scale) between
each interpolation step. The lower geometric smoothness of our
method, compared to StructureNet in the Table and Storage cate-
gories, demonstrates the quality of the latent space learned by our
method. Moreover, using our procedure to turn StructureNet out-
puts into ShapeAssembly programs, we can measure the program
smoothness along these interpolation paths. Each method‚Äôs program
smoothness is computed by taking the average tokenized program
edit distance between each interpolation step. As a measure for
structural change throughout the transitions of an interpolation
sequence, our lower program smoothness metric again shows how
our method benefits by operating within the space of 3D shape
programs.

7.3 Synthesis from Unstructured Geometry

Another way to inspect the structure of a generative model‚Äôs latent
space is through performing ‚Äúsynthesis from X", by projecting X
into the latent space of the generative model. As an application for
3D reconstruction, we are able perform such a projection with point
clouds, demonstrating how our generative model‚Äôs latent space can
synthesize ShapeAssembly programs from unstructured geometry.
Specifically, we train a PointNet++ encoder [Qi et al. 2017] to
map point clouds sampled on dense mesh geometry to the latent
space learned by our generative model. These latent codes are then
converted into programs by our trained decoder.

In Table 7, we show an experiment comparing our method against
StructureNet on the task of reconstructing point cloud samplings
of dense geometry on the intersection of each method‚Äôs validation
set for Chairs in Partnet (463 shapes total). We evaluate reconstruc-
tion accuracy with F-score [Knapitsch et al. 2017], and the physical

ACM Trans. Graph., Vol. 39, No. 6, Article 234. Publication date: December 2020.

234:14

‚Ä¢ R. Kenny Jones, Theresa Barton, Xianghao Xu, Kai Wang, Ellen Jiang, Paul Guerrero, Niloy J. Mitra, and Daniel Ritchie

Source Shape

Interpolations

Target Shape

t
e
N
e
r
u
t
c
u
r
t
S

s
r
u
O

t
e
N
e
r
u
t
c
u
r
t
S

s
r
u
O

t
e
N
e
r
u
t
c
u
r
t
S

s
r
u
O

t
e
N
e
r
u
t
c
u
r
t
S

s
r
u
O

Fig. 12. A qualitative comparison of latent space interpolation between our method and StructureNet on shapes from the validation set. Our method‚Äôs
interpolations within program space produce sequences that combine smooth continuous variation with discrete structural transitions.

ACM Trans. Graph., Vol. 39, No. 6, Article 234. Publication date: December 2020.

ShapeAssembly: Learning to Generate Programs for 3D Shape Structure Synthesis

‚Ä¢

234:15

F1 ‚áë % rooted ‚áë % stable ‚áë

Method

StructureNet
Ours

24.3
31.1

80.0
SN + Opt Cuboids
SN + Opt Program 77.4
Ours + Opt Cuboids 77.6
Ours + Opt Program 75.8

95.1
95.5

92.9
90.0
93.1
95.3

78.4
84.4

72.7
71.9
72.9
80.2

Table 7. Results from our point cloud reconstruction experiment. Our
model‚Äôs well-formed latent space allows for more accurate and physically
valid reconstructions without further optimization. With additional op-
timization, using the reconstructed program from our method and our
differentiable interpreter finds the best trade-off between reconstruction
accuracy and maintaining physical validity.

validity of reconstructions with the rootedness and stability met-
rics. When projecting point clouds into the latent space of each
method (top two rows), our method outperforms StructureNet on
both reconstruction accuracy and maintaining physical validity.
This demonstrates, once again, the well-structured nature of our
method‚Äôs latent space.

Moreover, as the ShapeAssembly interpreter is differentiable,
we can further refine the continuous parameters of a program by
minimizing the Chamfer distance between executed geometry and a
target point cloud with a gradient-based optimizer. We compare this
procedure (Ours + Opt Program) against the following conditions:
‚Ä¢ SN + Opt Cuboids: Starting with StructureNet‚Äôs reconstruc-
tion, then directly optimizing predicted cuboids to minimize
Chamfer distance to the target point cloud.

‚Ä¢ SN + Opt Program: Parsing StructureNet‚Äôs reconstruction
into a ShapeAssembly program, then optimizing the program
to minimize Chamfer distance to the target point cloud.
‚Ä¢ Ours + Opt Cuboids: Starting with our reconstruction, di-
rectly optimizing predicted cuboids to minimize Chamfer
distance to the target point cloud.

We show results for this experiment in the last four rows of Ta-
ble 7. All of the optimization procedures improve reconstruction
accuracy at the cost of physical validity. However, Ours + Opt Pro-
gram is the only condition that achieves a desirable trade-off in this
exchange, gaining much more reconstruction accuracy improve-
ment than it loses in physical validity.

We show some qualitative results of this experiment in Figure 13.
Through latent space projection, our model is able to output the
rough 3D shape structure (column 1) of an input unstructured point
cloud (column 0). Through our differentiable interpreter, we are
able to find continuous parameters for the predicted program struc-
ture that ultimately lead to better reconstruction fits (column 3).
Shape programs place a strong structural regularization prior over
unstructured 3D data, and thus our presented method is less prone
to ‚Äúlosing‚Äù semantic parts, such as small legs, in comparison to the
other conditions.

8 CONCLUSION AND FUTURE WORK

In this paper, we took a first step toward marrying the complemen-
tary strengths of neural and procedural 3D shape generative models

by introducing a hybrid neural-procedural approach for synthesiz-
ing novel 3D shape structures. We introduced ShapeAssembly, a
low-level ‚Äúassembly language‚Äù for shape structures, in which shapes
are constructed by declaring cuboidal parts and attaching them to
one another. We also introduced a differentiable interpreter for Sha-
peAssembly, allowing the optimization of program parameters to
produce desired output geometry. After describing how to extract
consistent programs from existing shape structures in the PartNet
dataset, we then defined a deep generative model for ShapeAssem-
bly programs, effectively training a neural network to write novel
shape programs for us. We evaluated the quality of the generative
model along several axes, showing that it produces more plausi-
ble and physically-valid shapes, and that its latent space is better-
structured than that of other generative models of shape structure.
We also found that directly generating shape programs leads to
more compact, editable programs than extracting programs from
shapes generated by methods that directly output 3D geometry.

Limitations. As mention in Section 5, we do not successfully extract
training programs from every shape in our dataset.

For instance, our program extraction procedure assumes that
the orientation of all parts can be specified through solely part-to-
part attachments, yet as demonstrated in Figure 14, this does not
hold for all shapes. While it is possible to reconstruct these shapes
with ShapeAssembly programs (through attaching parts to ‚Äúfloating‚Äù
points in space via the bounding volume) such programs will never
be added to our training data, and thus our generative model won‚Äôt
learn to produce such constructs. Our design decision to discard
training programs with more than 12 total Cuboid declarations has
a similar effect: it limits our generative model from synthesizing
the most complex of shape structures that exist in our dataset. We
impose such strict criteria in order to make our training programs
exhibit more regularity, simplifying the learning task for our neural
network at the expense of its potential expressivity.

This highlights a central tradeoff: higher variability in the train-
ing programs may result in lower quality shapes synthesized by a
generative model. This phenomenon is not unique to our setting:
it is well-known that e.g., image generative models perform better
on very-regularly-structured domains, such as human faces. The
question, looking forward, is how to capture more data variability
while keeping a high-degree of regularity in the input data repre-
sentation? We believe that using programs as a data representation
is the best avenue of attack, here. As we have shown in our work, a
single program can capture a wide range of parametrically related
shapes. One program, many shapes; strong regularity, but also high
variability. We are excited to investigate extensions of ShapeAssem-
bly, as well as other shape-generating languages, which can capture
even more shape variability with highly-regular structures.

While ShapeAssembly has a strong inductive bias for generating
physically-connected shapes, it is not guaranteed to do so. Hierar-
chical part structures which are locally connected everywhere may
occasionally still exhibit disconnected leaf cuboids. This is more
likely to happen with very non-axis-aligned structures that result in
loose bounding cuboids at the intermediate levels of the hierarchy. It
is worth investigating mechanisms to guarantee that ShapeAssem-
bly programs maintain leaf-to-leaf connectivity.

ACM Trans. Graph., Vol. 39, No. 6, Article 234. Publication date: December 2020.

234:16

‚Ä¢ R. Kenny Jones, Theresa Barton, Xianghao Xu, Kai Wang, Ellen Jiang, Paul Guerrero, Niloy J. Mitra, and Daniel Ritchie

Input Points

Ours

Ours + Opt Cuboids Ours + Opt Program

SN

SN + Opt Cuboids

SN + Opt Program

Fig. 13. Qualitative comparison of synthesis from point clouds of our method against StructureNet (SN). Our method is able to infer good program structures
that match well with the unstructured geometry. The continuous parameters of this program structure can be further refined through an optimization
procedure in order to better fit the target point cloud without creating artifacts.

an independent, part-by-part fashion. What would it look like to
swap out the ‚Äúsurface style‚Äù code for a shape while retaining its
‚Äústructure‚Äù code? Our procedural representation may confer distinct
advantages here, as the attachments explicitly specify where and
how part geometries must connect.

It would also be interesting to move beyond cuboids as the proxy
geometry used for atomic parts, as not all atomic parts are well-
approximated by rectilinear geometry. In some cases, spherical,
cylindrical, or more general curvilinear geometry would be a better
choice. Pursuing this direction would help push more shape vari-
ability into the procedural representation, so that we do not lean so
heavily on the neural network to capture it.

Another way to push knowledge from the learned latent space
into the programs would be to make the programs include con-
straints on their parameters: either independent bounds, or correla-
tions between parameters. For instance, it is non-semantic to make
a chair leg too thin, or to make a chair back much narrower than
the seat to which it is attached. It should be possible to mine shape
datasets for this information, and to include it in the data used to
train the generative model.

There are also more opportunities to apply generative models
of shape programs to ‚Äúsynthesis from X‚Äù applications. While we
showed translation from point clouds to shape programs, there are
many more exciting possibilities in terms of linking 3D geometry,
2D images, and shape programs, and seamlessly using the three
modalities to author different forms of shape manipulations.

Finally, if we aim for our generated shapes to be useful in embod-
ied AI applications, they should also be equipped with information
about kinematics and/or dynamics. For example, a program which

Fig. 14. Examples of PartNet shapes that contain parts whose orientations
cannot be inferred from part-to-part attachments alone. While these shapes
can be represented with ShapeAssembly programs that attach parts to
‚Äúfloating‚Äù points within the bounding volume, such programs are not added
to our training data during our program extraction phase. As a result, our
generative model never learns to produce shapes that require this type of
attachment pattern.

Future work. In Section 7, we showed an example of refining our
generated hierarchical cuboid structures with point cloud surface
geometry. This is not a new idea; other recent related work on
part-based shape generation takes a similar approach to refining
high-level part structures [Gao et al. 2019; Li et al. 2017; Mo et al.
2019a]. However, there is more work to be done at the intersection of
structure generation and surface generation. These two paradigms
could be much more closely married than they have been thus far,
as existing part-surface generation has been explored largely in

ACM Trans. Graph., Vol. 39, No. 6, Article 234. Publication date: December 2020.

ShapeAssembly: Learning to Generate Programs for 3D Shape Structure Synthesis

‚Ä¢

234:17

specifies a cabinet could also specify the type of hinge with which
the door attaches to the body, and how far that hinge opens. Ul-
timately, we believe that shape programs, and generative models
which produce them, are the right fundamental representation for
both human creative tasks and AI analysis tasks involving part-
based 3D shapes.

ACKNOWLEDGMENTS

We would like to thank the anonymous reviewers for their help-
ful suggestions. Renderings of part cuboids and point clouds were
produced using the Blender Cycles renderer. This research was sup-
ported by the National Science Foundation (#1753684, #1941808), a
Brown University Presidential Fellowship, gifts from the University
of College London AI Center and Adobe Research, and by GPU
donations from NVIDIA. Daniel Ritchie is an advisor to Geopipe,
Inc. and owns equity in the company. Geopipe is a start-up that is
developing 3D technology to build immersive virtual copies of the
real world with applications in various fields, including games and
architecture.

REFERENCES

Ben Abbatematteo, Stefanie Tellex, and George Konidaris. 2019. Learning to Generalize
Kinematic Models to Novel Objects. In Proceedings of the Third Conference on Robot
Learning.

Zhiqin Chen, Andrea Tagliasacchi, and Hao Zhang. 2019. BSP-Net: Generating Compact

Meshes via Binary Space Partitioning. (2019). arXiv:cs.CV/1911.06971

Zhiqin Chen and Hao Zhang. 2019. Learning Implicit Fields for Generative Shape
Modeling. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR).
√Ñ≈ô. Demir, D. G. Aliaga, and B. Benes. 2016. Proceduralization for Editing 3D Architec-

tural Models. In 2016 Fourth International Conference on 3D Vision (3DV).

Tao Du, Jeevana Priya Inala, Yewen Pu, Andrew Spielberg, Adriana Schulz, Daniela
Rus, Armando Solar-Lezama, and Wojciech Matusik. 2018. InverseCSG: Automatic
Conversion of 3D Models to CSG Trees. ACM Trans. Graph. 37, 6 (Dec. 2018).
Kevin Ellis, Maxwell Nye, Yewen Pu, Felix Sosa, Josh Tenenbaum, and Armando Solar-
Lezama. 2019. Write, Execute, Assess: Program Synthesis with a REPL. In Advances
in Neural Information Processing Systems (NeurIPS).

Kevin Ellis, Daniel Ritchie, Armando Solar-Lezama, and Josh Tenenbaum. 2018. Learn-
ing to Infer Graphics Programs from Hand-Drawn Images. In Advances in Neural
Information Processing Systems (NeurIPS).

Haoqiang Fan, Hao Su, and Leonidas J Guibas. 2017. A point set generation network for
3D object reconstruction from a single image. In Proceedings of the IEEE conference
on computer vision and pattern recognition. 605‚Äì613.

Lin Gao, Jie Yang, Tong Wu, Yu-Jie Yuan, Hongbo Fu, Yu-Kun Lai, and Hao (Richard)
Zhang. 2019. SDM-NET: Deep Generative Network for Structured Deformable Mesh.
In SIGGRAPH Asia.

Thibault Groueix, Matthew Fisher, Vladimir G. Kim, Bryan C. Russell, and Mathieu
Aubry. 2018. AtlasNet: A Papier-M√¢ch√© Approach to Learning 3D Surface Generation.
In IEEE Conference on Computer Vision and Pattern Recognition (CVPR).

Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp
Hochreiter. 2017. GANs Trained by a Two Time-Scale Update Rule Converge to a
Local Nash Equilibrium. In NeurIPS.

Irvin Hwang, Andreas Stuhlm√ºller, and Noah D. Goodman. 2011. Inducing Probabilistic

Programs by Bayesian Program Merging. CoRR arXiv:1110.5667 (2011).

Justin Johnson, Bharath Hariharan, Laurens van der Maaten, Judy Hoffman, Li Fei-Fei,
C Lawrence Zitnick, and Ross Girshick. 2017. Inferring and Executing Programs for
Visual Reasoning. In ICCV.

Amlan Kar, Aayush Prakash, Ming-Yu Liu, Eric Cameracci, Justin Yuan, Matt Rusiniak,
David Acuna, Antonio Torralba, and Sanja Fidler. 2019. Meta-Sim: Learning to
Generate Synthetic Datasets. (2019). arXiv:cs.CV/1904.11621

Diederik P. Kingma and Jimmy Ba. 2014. Adam: A Method for Stochastic Optimization.

CoRR abs/1412.6980 (2014).

Diederik P. Kingma and Max Welling. 2014. Auto-Encoding Variational Bayes. In

International Conference on Learning Representations (ICLR).

Arno Knapitsch, Jaesik Park, Qian-Yi Zhou, and Vladlen Koltun. 2017. Tanks and
Temples: Benchmarking Large-Scale Scene Reconstruction. ACM Transactions on
Graphics 36, 4 (2017).

Eric Kolve, Roozbeh Mottaghi, Daniel Gordon, Yuke Zhu, Abhinav Gupta, and Ali
Farhadi. 2017. AI2-THOR: An Interactive 3D Environment for Visual AI. CoRR

arXiv:1712.05474 (2017).

Matt J. Kusner, Brooks Paige, and Jos√© Miguel Hern√°ndez-Lobato. 2017. Grammar
Variational Autoencoder. In Proceedings of the 34th International Conference on
Machine Learning - Volume 70 (ICML√¢ƒÇ≈π17). JMLR.org, 1945√¢ƒÇ≈û1954.

Manfred Lau, Akira Ohgawara, Jun Mitani, and Takeo Igarashi. 2011. Converting 3D
Furniture Models to Fabricatable Parts and Connectors. ACM Trans. Graph. 30, 4,
Article 85 (July 2011), 6 pages. https://doi.org/10.1145/2010324.1964980

Jun Li, Kai Xu, Siddhartha Chaudhuri, Ersin Yumer, Hao Zhang, and Leonidas Guibas.
2017. GRASS: Generative recursive autoencoders for shape structures. ACM Trans-
actions on Graphics (TOG) 36, 4 (2017), 52.

Yunchao Liu, Zheng Wu, Daniel Ritchie, William T. Freeman, Joshua B. Tenenbaum,
and Jiajun Wu. 2019. Learning to Describe Scenes with Programs. In International
Conference on Learning Representations (ICLR).

Sidi Lu, Jiayuan Mao, Joshua B. Tenenbaum, and Jiajun Wu. 2019. Neurally-Guided
Structure Inference. In International Conference on Machine Learning (ICML).
Andrew L. Maas. 2013. Rectifier Nonlinearities Improve Neural Network Acoustic

Models.

A. Martinovic and L. Van Gool. 2013. Bayesian Grammar Learning for Inverse Procedural

Modeling. In CVPR.

Mateusz Michalkiewicz, Jhony K. Pontes, Dominic Jack, Mahsa Baktashmotlagh, and
Anders P. Eriksson. 2019. Deep Level Sets: Implicit Surface Representations for 3D
Shape Inference. CoRR abs/1901.06802 (2019).

Kaichun Mo, Paul Guerrero, Li Yi, Hao Su, Peter Wonka, Niloy Mitra, and Leonidas
Guibas. 2019a. StructureNet: Hierarchical Graph Networks for 3D Shape Generation.
In SIGGRAPH Asia.

Kaichun Mo, Shilin Zhu, Angel X. Chang, Li Yi, Subarna Tripathi, Leonidas J. Guibas, and
Hao Su. 2019b. PartNet: A Large-Scale Benchmark for Fine-Grained and Hierarchical
Part-Level 3D Object Understanding. In The IEEE Conference on Computer Vision
and Pattern Recognition (CVPR).

Pascal M√ºller, Peter Wonka, Simon Haegler, Andreas Ulmer, and Luc Van Gool. 2006.

Procedural Modeling of Buildings. In SIGGRAPH.

Gen Nishida, Adrien Bousseau, and Daniel G. Aliaga. 2018. Procedural Modeling of a
Building from a Single Image. Computer Graphics Forum (Eurographics) 37, 2 (2018).
Gen Nishida, Ignacio Garcia-Dorado, Daniel G Aliaga, Bedrich Benes, and Adrien
Bousseau. 2016. Interactive Sketching of Urban Procedural Models. ACM Transac-
tions on Graphics (TOG) 35, 4 (2016), 130.

Yoav I. H. Parish and Pascal M√ºller. 2001. Procedural Modeling of Cities. In SIGGRAPH.
Jeong Joon Park, Peter Florence, Julian Straub, Richard Newcombe, and Steven Love-
grove. 2019. DeepSDF: Learning Continuous Signed Distance Functions for Shape
Representation. In The IEEE Conference on Computer Vision and Pattern Recognition
(CVPR).

Adam Paszke, Sam Gross, Soumith Chintala, Gregory Chanan, Edward Yang, Zachary
DeVito, Zeming Lin, Alban Desmaison, Luca Antiga, and Adam Lerer. 2017. Auto-
matic differentiation in PyTorch. (2017).

Przemyslaw Prusinkiewicz and Aristid Lindenmayer. 1996. The Algorithmic Beauty of

Plants. Springer-Verlag, Berlin, Heidelberg.

Charles Ruizhongtai Qi, Li Yi, Hao Su, and Leonidas J Guibas. 2017. Pointnet++: Deep
hierarchical feature learning on point sets in a metric space. In Advances in neural
information processing systems. 5099‚Äì5108.

Stephan R Richter, Vibhav Vineet, Stefan Roth, and Vladlen Koltun. 2016. Playing for
data: Ground truth from computer games. In European conference on computer vision.
Springer, 102‚Äì118.

Daniel Ritchie, Sarah Jobalia, and Anna Thomas. 2018. Example-based Authoring
of Procedural Modeling Programs with Structural and Continuous Variability. In
EUROGRAPHICS.

Manolis Savva, Abhishek Kadian, Oleksandr Maksymets, Yili Zhao, Erik Wijmans,
Bhavana Jain, Julian Straub, Jia Liu, Vladlen Koltun, Jitendra Malik, Devi Parikh,
and Dhruv Batra. 2019. Habitat: A Platform for Embodied AI Research. In The IEEE
International Conference on Computer Vision (ICCV).

Gopal Sharma, Rishabh Goyal, Difan Liu, Evangelos Kalogerakis, and Subhransu Maji.
2018. CSGNet: Neural Shape Parser for Constructive Solid Geometry. In IEEE
Conference on Computer Vision and Pattern Recognition (CVPR).

Ondrej Stava, Bedrich Benes, Radom√≠r Mech, Daniel G. Aliaga, and Peter Kristof. 2010.
Inverse Procedural Modeling by Automatic Generation of L-systems. Comput. Graph.
Forum 29 (2010), 665‚Äì674.

Minhyuk Sung, Hao Su, Vladimir G. Kim, Siddhartha Chaudhuri, and Leonidas Guibas.
2017. ComplementMe: Weakly-Supervised Component Suggestions for 3D Modeling.
ACM Transactions on Graphics (Proc. of SIGGRAPH Asia) (2017).

Jerry O. Talton, Lingfeng Yang, Ranjitha Kumar, Maxine Lim, Noah D. Goodman, and
Radom√≠r Mech. 2012. Learning design patterns with Bayesian grammar induction.
In UIST.

Yonglong Tian, Andrew Luo, Xingyuan Sun, Kevin Ellis, William T. Freeman, Joshua B.
Tenenbaum, and Jiajun Wu. 2019. Learning to Infer and Execute 3D Shape Programs.
In International Conference on Learning Representations (ICLR).

Jiajun Wu, Chengkai Zhang, Tianfan Xue, William T. Freeman, and Joshua B. Tenen-
baum. 2016. Learning a Probabilistic Latent Space of Object Shapes via 3D

ACM Trans. Graph., Vol. 39, No. 6, Article 234. Publication date: December 2020.

234:18

‚Ä¢ R. Kenny Jones, Theresa Barton, Xianghao Xu, Kai Wang, Ellen Jiang, Paul Guerrero, Niloy J. Mitra, and Daniel Ritchie

could resolve a globally-optimal configuration of cuboids given
the attachment constraints. Instead, the interpreter immediately
executes each attachment as it is declared, i.e. it greedily solves for
attachments. To make the behavior of this procedure as predictable
as possible, the greedy attachment procedure should induce the
fewest changes possible to the current cuboid shapes.

With these desiderata in mind, we designed the following proce-
dure for attaching cuboid c1 to cuboid c2 (see Figure 15). The logic
that executes depends upon how many prior attachments c1 has
and the aligned flag of c1:

No prior attachments. In this case, cuboid c1 can connect to cuboid
c2 by simply translating until the attach points are colocated.

One prior attachment. Here, the interpreter scales cuboid c1 along
one of its axes and then rotates it such that the attachment is sat-
isfied. To choose the axis along which to scale c1, the interpreter
checks how quickly scaling each of its three dimensions would re-
duce the ratio n/k, where n is the distance between c1‚Äôs existing
attachment point and the new target attachment point, and k is the
distance between c1‚Äôs existing attachment point and the new source
attachment point. The interpreter then scales c1 by n/k along this
dimension, which gives it the correct length. Finally, c1 is rotated
such that the source and target attachment points are colinear (and
thus colocated).

Two or more prior attachments. In this case, it is not always possible
to satisfy the attachment, as three point constraints on a cube may
be overconstrained. If a solution exists, however, our interpreter
will find it. And in the case where no solution exists, it attempts to
approximately satisfy the attachment (which we decided to be more
user-friendly behavior than throwing an error).

First, the interpreter checks if c1‚Äôs existing attachment points are
all colinear. If they are, then it rotates c1 about this axis of colinearity
to make the source attachment point face the target attachment
point. The final step is to scale c1 along the normal of the face
containing the source attachment point. If the existing attachment
points were not colinear, and this face was not rotated to point
toward the target attachment point, then this may not be a useful
operation (i.e. it may introduce undesirable change to the cuboid
shape while doing little to bring the source point closer to the target
point). Thus, the interpreter only executes this scale if the angle
between the source face normal and the vector to the target point
is smaller than a threshold œÑ (25 degrees in our implementation).

Aligned Cuboids. Cuboids that are marked as aligned in ShapeAssem-
bly programs cannot have their orientations changed through at-
tachment. In fact, with correct cuboid dimension parameterization,
a single attachment is enough to properly position and orient an
aligned cuboid. However, in order to ensure that aligned Cuboids
remain connected through edits and predictions of our generative
model, we minimally grow aligned cuboid dimensions to satisfy the
part-to-part connectivity specified through attachments. That is, for
aligned cuboids we do not guarantee attachment point colocation
after the first attachment, as this is often impossible to exactly fulfill
without changing a cuboid‚Äôs orientation. Rather, we guarantee that
aligned cuboids will fulfill attachment relationships with cuboids
they are attached to at some attachment point.

Fig. 15. Illustrating how the attach command executes, depending on the
number of existing attachments (left column) to the cuboid in question.
Cuboids with no existing attachments can simply be translated into place
(top). Cuboids with one existing attachment can be scaled along one axis
and then rotated (middle). Cuboids with two or more existing attachments
are more complicated, and the attachment may not always be satisfiable.
Our interpreter attempts to rotate and scale the cuboid to get as close as
possible to valid solution.

Generative-Adversarial Modeling. In Advances in Neural Information Processing
Systems (NeurIPS).

Fei Xia, Amir R. Zamir, Zhi-Yang He, Alexander Sax, Jitendra Malik, and Silvio Savarese.

2018. Gibson env: real-world perception for embodied agents. In CVPR.

A. Khosla F. Yu L. Zhang X. Tang J. Xiao Z. Wu, S. Song. 2015. 3D ShapeNets: A Deep
Representation for Volumetric Shapes. In Computer Vision and Pattern Recognition.
Yinda Zhang, Shuran Song, Ersin Yumer, Manolis Savva, Joon-Young Lee, Hailin Jin,
and Thomas Funkhouser. 2017. Physically-Based Rendering for Indoor Scene Under-
standing Using Convolutional Neural Networks. The IEEE Conference on Computer
Vision and Pattern Recognition (CVPR) (2017).

Chenghui Zhou, Chun-liang Li, and Barnabas Poczos. 2019. Program Synthesis for

Images using Tree-Structured LSTM. In PGR Workshop at NeurIPS.

Chenyang Zhu, Kai Xu, Siddhartha Chaudhuri, Renjiao Yi, and Hao Zhang. 2018.
SCORES: Shape Composition with Recursive Substructure Priors. ACM Transactions
on Graphics (TOG) 37, 6 (2018), 211:1‚Äì211:14.

Chuhang Zou, Ersin Yumer, Jimei Yang, Duygu Ceylan, and Derek Hoiem. 2017. 3D-
PRNN: Generating Shape Primitives with Recurrent Neural Networks. In IEEE
International Conference on Computer Vision (ICCV).

A SEMANTICS OF THE ATTACH COMMAND
In designing the ShapeAssembly interpreter, our goal is to ensure
that its internal operations stay limited to simple fixed-function,
differentiable operations. Thus, implementing the attach command,
we opt not to use any constrained optimization routines which

ACM Trans. Graph., Vol. 39, No. 6, Article 234. Publication date: December 2020.

01ùëõùëòùëõ2+If existingattachmentsarecolinearSideviewùúÉ<ùúè?ShapeAssembly: Learning to Generate Programs for 3D Shape Structure Synthesis

‚Ä¢

234:19

B SEMANTICS OF SHAPEASSEMBLY MACRO

FUNCTIONS

We provide an account of the logic for macro function expansion
in ShapeAssembly :

Squeeze. The squeeze macro is parameterized by three cuboids
(cn1, cn2, cn3) a face f and a (u, v) position on f ‚Äôs 2D coordinate
system. A squeeze command expands into two attach functions.
The first attach function attaches the center of cn1‚Äôs f face to the
(u, v) position on the opposite face of f on cn2. The second attach
function attaches the center of cn1‚Äôs opposite face of f to the (u, v)
position on the face of f on cn3. For example, the line squeeze
(cn1, cn2, cn3, left, .1, .4). It expands into attach(cn1, cn2, 0.0, .5, .5,
1.0, .1, .4) and attach(cn1, cn3, 1.0, .5, .5, 0.0, .1, .4).

Reflect. The reflect macro is parameterized by a cuboid cn and an
axis a. A reflect command first expands into one Cuboid function,
that creates a new cuboid cn‚Ä≤ with the same parameters as cn . Then
for every previous attachment line pair that had moved cn , of the
form attach(cn, cm, x1, y1, z1, x2, y2, z2), the reflect command cre-
ates a new attachment line: attach(cn‚Ä≤, cm , x1, y1, z1, R(x1, y1, z1, cn, cm, a)).
R is a function that applies a reflection of the global point specified
by (x1, y1, z1) in the local coordinate frame of cn about the axis a,
and then returns the local coordinates of that point within cm .

Translate. The translate macro is parameterized by a cuboid cn , an
axis a, a number of members m, and a distance d. A translate com-
mand first expands into m Cuboid functions, that each creates a new
cuboid cni with the same parameters as cn . Then for every previous
attachment line pair that had moved cn , of the form attach(cn, cm ,
x1, y1, z1, x2, y2, z2), the translate command creates a new attach-
ment line attach(cni , cm , x1, y1, z1, T (x1, y1, z1, cn, cm, a, d)). T is a
function that applies a translation of the global point specified by
(x1, y1, z1) in the local coordinate frame of cn along the axis a (of
the bounding volume) for for a distance of d (where d is normalized
by the size of the bounding volume), and then returns the local
coordinates of that point within cm .

C PROGRAM EXTRACTION PROCEDURE

Here, we provide an account of our program extraction procedure
in greater detail:

Part Shortening. Before any hierarchical processing, we first attempt
to regularize any artifacts in the input data. Specifically, for each
leaf cuboid part proxy, we check if any of its faces are completely
contained within any other leaf cuboid. If we find that we can
shorten a leaf cuboid without changing the visible, non-intersecting,
geometry of the part graph, we do so.

Semantic Hierarchy Arrangement. During our data preprocessing
stage when converting PartNet part graphs into ShapeAssembly pro-
grams, we locally flatten part graph hierarchies based on semantic
rules as depicted in Figure 5. For chairs we flatten the following
nodes: back, arm, base, seat, footrest and head. For tables we flatten
the following nodes: top and base. For storage we flatten the follow-
ing nodes: cabinet frame, cabinet base. For storage, we move the
following nodes into the cabinet frame sub-program: countertop,
shelf, drawer, cabinet door and mirror. We also perform a semantic

collapsing step where the intermediate nodes containing detailed ge-
ometry are converted into leaf nodes and their children are discarded.
For chairs we collapse the following nodes: caster and mechanical
control. For tables we collapse the following nodes: caster, cabinet
door, drawer, keyboard tray. For storage we collapse the follow-
ing nodes: drawer, cabinet door, mirror and caster. Empirically we
observed that this method of hierarchy re-arrangements produces
cleaner and more regularized training data for our generative model.

Attachment Point Detection. In order to identify which cuboids con-
nect, and where they connect, we use a point cloud intersection
procedure. We sample a uniform 20x20x20 point cloud within the
volume defined by each cuboid. To check if two cuboids are attached,
we find the set of points in the pairwise point cloud comparison
that have a minimum distance to any point in the other point cloud
within a distance threshold determined by the scale of the larger
cuboid. For cuboids that attach (i.e. this intersection set is non-zero)
we sample a denser 50x50x50 point cloud within the bounds of the
detected intersection volume, forming a set of candidate attach-
ment points. From this set we first filter all attachment points that
are outside of either cuboid. If any remaining attachment points
form face-to-face connections between cuboids we choose them,
otherwise we define the attachment as taking place at the mean
of the remaining attachment points. With the same procedure, we
also record if cuboids connect to the top or bottom of the bounding
volume. Sampled points with bounding volume local y-coordinates
in the ranges of [0, 0.05] and [.95, 1.0] are assigned to the bottom
and top respectively.

Symmetry Detection. We enforce that all members of a symmetry
group share the same connectivity structure in the input part graph.
Cuboids are grouped together by symmetry if they: (i) connect to
the same cuboids, (ii) share a reflectional or translational symmetry
about the X, Y or Z axis of their parent bounding volume, and (iii)
each attachment point involved in their outgoing connections also
shares this same symmetrical relationship. Two cuboids, or two at-
tachment points, are considered to share a symmetrical relationship
if applying the symmetry transformation matrix to one member
produces a parameterization close to that of the other member.

Notice that this procedure can disqualify symmetry formation
about groups of interconnected cuboids that share a symmetrical re-
lationship. As such, before forming symmetry groups about individ-
ual cuboids, we attempt to form symmetry groups about connected
components of multiple cuboids. Whenever such a component is
found, we locally abstract its structure with a bounding volume, and
create a symmetry group sub-program. In this manner, we capture
additional spatial symmetries while continuing to enforce the re-
lationship between symmetry and part connectivity. The "H-leg"
program (Program3) in Figure 2 shows an example of where such a
symmetry sub-program was formed.

In total, our parsing procedure finds valid ShapeAssembly pro-
grams for 46% of Chairs, 65% of Tables and 58% of Storage shapes
in PartNet.

ACM Trans. Graph., Vol. 39, No. 6, Article 234. Publication date: December 2020.

234:20

‚Ä¢ R. Kenny Jones, Theresa Barton, Xianghao Xu, Kai Wang, Ellen Jiang, Paul Guerrero, Niloy J. Mitra, and Daniel Ritchie

of shapes from the test set, and measure the percentage of
them incorrectly classified as ‚Äúfake". To reduce fluctuation,
the percentage is averaged over the last 50 epochs.

D DECODER SEMANTIC VALIDITY CHECKS

During the process of decoding a latent code, our generative network
enforces the following semantic validity conditions on its outputs:
‚Ä¢ XYZ attachment coordinates are clamped between 0 and 1.0.
Additionally, attachments to the bounding box can only be at
the top or bottom faces with an allowable error of .05.

‚Ä¢ Cuboid dimensions are clamped between 0.01 and the corre-

sponding bounding box dimension

‚Ä¢ Bounding box cuboids can have no sub-programs
‚Ä¢ Cuboids only attach at a single location. As an exception,
cuboids are allowed to attach to both the top and bottom
faces of the bounding volume.

‚Ä¢ The bounding box cannot be moved by an attach command
‚Ä¢ Attachment orderings must be grounded. Upon terminating,

any ungrounded cuboids instantiations are discarded.

‚Ä¢ Symmetries can only operate on grounded cuboids
‚Ä¢ The ordering of Cuboid, attach, squeeze, reflect, and translate
lines must be consistent with the ShapeAssembly grammar.
‚Ä¢ Commands must keep cuboids within the bounds of the de-

fined bounding volume with an allowable error of 10%.
During generation, if our model predicts a non-semantic program
line, we attempt to back-track until we are able to find a semantically
valid solution. For instance, if we predict a new line to be a reflect
command, but no cuboids have been grounded, we pick a new
command type for the line by zeroing out the logits for the reflect
command index.

In some cases, a combination of bad continuous parameters and
program structure predictions produce a violating line that cannot
be easily fixed. During unconditional generation, we reject the sam-
ple if we encounter this behavior (this happens for 10% - 20% of
our random samples across the categories we consider). We run an
ablation on this rejection sampling in Table 2. During interpolation,
we never reject a sample. Instead, we simply do not add lines to the
predicted program for which we could not find a fix.

E SHAPE QUALITY METRICS

We provide additional details about the metrics used in Table 2:

‚Ä¢ Rootedness : We check if a connected path exists between
the ground and all parts in the shape. We judge two parts to
be connected if they are separated by a distance no larger
than 2% of the overall shape‚Äôs bounding box diagonal length.
‚Ä¢ Stability : We convert generated 3D shape structures into
rigid bodies and place them in a physical simulation with
gravity. A vertical force is applied to each shape proportional
to its mass, along with some other small random forces and
torques. If the resting height of any connected component of
the shape changes by more than 10% after these perturbations
we declare it unstable. Note that this is by definition less than
or equal to the percentage of rooted shapes, as a shape must
be rooted in order to be stable.

‚Ä¢ Realism: The percentage of test set shapes classified as ‚Äúgen-
erated‚Äù by a binary PointNet classifier trained to distinguish
between generated shapes and shapes from the training dataset.
The classifier is trained on an equal amount of positive and
negative examples for 300 epochs. We hold out a portion

ACM Trans. Graph., Vol. 39, No. 6, Article 234. Publication date: December 2020.

