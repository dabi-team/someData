Supervised learning of random quantum circuits via scalable
neural networks

Simone Cantori1, David Vitali1,2,3, and Sebastiano Pilati1,2

1School of Science and Technology, Physics Division, Università di Camerino, 62032 Camerino (MC), Italy
2INFN-Sezione di Perugia, 06123 Perugia, Italy
3CNR-INO, 50125 Firenze, Italy

Predicting the output of quantum circuits is
a hard computational task that plays a piv-
otal role in the development of universal quan-
tum computers. Here we investigate the super-
vised learning of output expectation values of
random quantum circuits. Deep convolutional
neural networks (CNNs) are trained to predict
single-qubit and two-qubit expectation values us-
ing databases of classically simulated circuits.
These circuits are represented via an appro-
priately designed one-hot encoding of the con-
stituent gates. The prediction accuracy for pre-
viously unseen circuits is analyzed, also making
comparisons with small-scale quantum comput-
ers available from the free IBM Quantum pro-
gram. The CNNs often outperform the quan-
tum devices, depending on the circuit depth, on
the network depth, and on the training set size.
Notably, our CNNs are designed to be scalable.
This allows us exploiting transfer learning and
performing extrapolations to circuits larger than
those included in the training set. These CNNs
also demonstrate remarkable resilience against
noise, namely, they remain accurate even when
trained on (simulated) expectation values aver-
aged over very few measurements.

1 Introduction

Universal quantum computers promise to solve
some relevant computational problems which are
intractable for classical computers [1, 2].
In
fact, the claim of quantum speed-up is justi-
ﬁed only when the targeted computational task
cannot be completed by any classical algorithm
in a comparable computation time [3].
In this
context, machine learning techniques represent
a relevant benchmark. They are alternative to
direct classical simulations of quantum-circuits,

which are plagued by an exponentially scal-
ing computational cost.
In particular, super-
vised learning from classically simulated datasets
has emerged as a promising and computation-
ally feasible strategy to predict the ground-state
properties of complex quantum systems, includ-
ing, e.g., small molecules [4, 5], solid-state sys-
tems [6, 7], atomic gases [8, 9], and protein-
Interestingly, it has
ligand complexes [10, 11].
recently been proven that data-based algorithms
can solve otherwise classically intractable compu-
tational tasks [12], including predicting ground-
state properties of quantum systems, and rigor-
ous guarantees on the accuracy and on the scaling
of the required training set size have been demon-
strated [13]. Still, producing training sets for su-
pervised learning via classical computers quickly
becomes unfeasible as the system size increases.
In the context of ground-state simulations, this
problem has been addressed via scalable neural
networks [14, 15]. These allow performing trans-
fer learning from small to large systems [9, 16],
and even to extrapolate to sizes larger than those
included in the training set. This paves the way
to addressing classically intractable quantum sys-
tems.

The above considerations led us to investigate
the supervised learning of gate-based quantum
computers. Our goal is to demonstrate that deep
convolutional neural networks (CNNs) can be
trained to predict relevant output properties of
quantum circuits, both from exact classical sim-
ulations of expectation values, as well as from
noisy measurements. Remarkably, we show that
the CNNs trained on random circuits are able
to emulate a broad category of quantum circuits,
including, e.g., the Berstein-Vazirani (BV) algo-
rithm. Furthermore, thanks to an appropriately
designed scalable structure, they provide accu-
rate extrapolations for circuits larger than those

1

2
2
0
2

n
u
J

4
2

]
h
p
-
t
n
a
u
q
[

2
v
8
4
3
0
1
.
6
0
2
2
:
v
i
X
r
a

 
 
 
 
 
 
included in the training set. These ﬁndings moti-
vate the use of quantum devices as platforms for
training CNNs to emulate classically intractable
circuits.

In detail, in this article we consider large en-
sembles on quantum circuits built with gates ran-
domly selected from an approximate universal
set.
It is worth mentioning that the sampling
from random circuits is the computational task
considered in the recent demonstrations of quan-
tum supremacy [17, 18]. The target of the super-
vised learning is a partial description of the cir-
cuit output, namely, single-qubit and two-qubit
expectation values. As we demonstrate, this lim-
ited information is still suﬃcient to emulate the
category of quantum circuits with only one or
two possible output bit strings.
Interestingly,
this category includes relevant circuits such as
the BV algorithm [19]. An appropriate one-hot
encoding of the constituent gates is designed to
unequivocally describe the circuits. Their out-
put is classically computed using the Qiskit li-
brary [20], considering numbers of qubits up to
N ’ 10 and circuit depths (number of gates per
qubit) up to P ’ 10. Considerably larger cir-
cuits are also considered in the testing processes
performed via extrapolation procedures. Deep
CNNs are trained to map the circuit descriptors
to the output expectation values. The CNNs are
tested on previously unseen circuits, and we anal-
yse how the predictions accuracy varies as a func-
tions of the circuit size, of the network depth,
and of the training set size. We also compare
the accuracy of the trained CNNs against the
one of various small quantum computers avail-
able via IBM Quantum Experience [21]. Gener-
ally, the CNNs outperform the freely available
noisy intermediate-scale quantum (NISQ) pro-
cessors, unless the circuit’s depth is increased at
ﬁxed CNN parameters. Notably, our CNNs are
designed to be scalable. This allows us inves-
tigating transfer-learning and extrapolation pro-
tocols. Speciﬁcally, we show that the learning of
large circuits can be accelerated via a pretraining
performed on smaller circuits. Furthermore, we
employ CNNs trained on small circuits to predict
the output of circuits with up to twice as many
qubits. Interestingly, CNNs also learn (from ran-
dom circuits) to emulate the BV algorithm, even
when the number of qubits is increased by sev-
eral orders of magnitude. Finally, we consider the

training on noisy expectation values, obtained as
averages over a variable number of (simulated)
measurements. We ﬁnd that the CNNs are able
to ﬁlter this noise, providing remarkably accurate
estimates of the exact expectation values even
when very few measurements are considered in
the training data.

The rest of the article is organized as follows:
in Section 2 we describe the random circuits, the
one-hot encoding we design for supervised learn-
ing, the CNNs we adopt, and the target expecta-
tion values. Furthermore, we discuss the class
of random circuits that can be emulated from
the target values we address. The predictions of
the trained CNNs are analyzed in Section 3. In
the same Section these predictions are also com-
pared with small quantum computers available
from IBM Quantum Experience. Then, trans-
fer learning and extrapolations protocols are ana-
lyzed, and we ﬁnally discuss the training on noisy
expectation values. Section 4 reports our conclu-
sions and an outlook on future perspectives.

2 Methods

2.1 Representation of random circuits

Our goal is to train deep CNNs to map univo-
cal representations of random circuits to their
output expectation values. Speciﬁcally, we con-
sider circuits built with two single-qubit gates,
namely, the T-gate (T ) and the Hadamard gate
(H), together with one two-qubit gate, namely,
the controlled-not gate (CX). Notably, the set
S = {T, H, CX} constitutes an approximately
universal set [22, 23], meaning that any unitary
operator can be implemented using these three
gates.
It is worth noticing that the identity
"

#

can be represented as I = H 2. Be-

I =

1 0
0 1

low, an extended set explicitly including the gate
I will be discussed. We adopt the standard com-
putational basis corresponding to the eigenstates

of the Pauli matrix Z =

. In this basis,

#

"
0
1
0 −1

the three considered gates are represented by the

2

following matrices:

#

H =

"

#

1
1
1 −1

1
√
2

T =

CX =

"

1
0
0 ei π


4

1 0 0 0
0 1 0 0
0 0 0 1
0 0 1 0













.

(1)

In the following, we consider circuits with N
qubits and P layers of gates. Therefore, the in-
teger P corresponds to the number of gates per
qubit; this parameter will be referred to also as
the circuit depth.
In every layer, each qubit is
processed by a gate randomly selected from the
set S. Notice that the two-qubit gate CX acts
on a control and on a target qubit. To simplify
the circuit representation, we allow only one CX
gate at every layer. Circuits with more CX gates
per layer can be emulated by deeper circuits sat-
isfying the constraint. This constraint allows us
adopting a relatively simple univocal circuit rep-
resentation. It is based on the following one-hot
encoding of the gate acting on each qubit: the
T -gate corresponds to the vector (1, 0, 0, 0), the
H-gate to (0, 1, 0, 0), the control qubit of the CX-
gate corresponds to (0, 0, 1, 0), while the target
qubit corresponds to (0, 0, 0, 1). This map is also
Therefore, the feature
represented in Fig. 1.

T

H

[1, 0, 0, 0]

[0, 1, 0, 0]

[0, 0, 1, 0]

[0, 0, 0, 1]

Figure 1: Visualization of the one-hot encoding that
represents the quantum gates in the set S. The full circle
and the empty circle with plus sign represent the control
and the target qubit of the CX gate, respectively.

vector representing a random circuit is a four-
channel two-dimensional binary matrix with di-
mensions N × P × 4. Analogous gate-based de-
scriptions of quantum circuits have been adopted
in Refs. [24, 25]. Despite the constraint on the
number of CX gates per layer, the number Q
of possible circuits rapidly grows with N and P .

3

I

T

H

[1, 0, 0, 0, 0]

[0, 1, 0, 0, 0]

[0, 0, 1, 0, 0]

[0, 0, 0, 1, 0]

[0, 0, 0, 0, 1]

Figure 2: Visualization of the one-hot encoding that
represents the quantum gates in the set S ∗.

This number can be computed as:

Q =

P
X

m=0

2N ·P −2m

!

P
m

2m

!m

N
2

,

(2)

(cid:1)m

where m is the number of CX-gates in the cir-
cuit. The ﬁrst term, namely, 2N ·P −2m, represents
the possible combinations considering only the T -
gates and the H-gates. The second term, namely,
(cid:1), corresponds to the possible combinations
(cid:0)P
m
of the CX-gates in P layers. The third term,
namely, 2m, corresponds to the choice of the con-
trol and of the target qubit for each CX gate.
Finally, the term (cid:0)N
corresponds to the avail-
2
able pairs for each CX-gate. For example, for
the smallest circuit size considered in this article,
namely, N = 3 and P = 5, one has Q = 3.2 × 106
possible random circuits. For the largest size,
namely, N = 20 and P = 6, one has the astro-
nomic number Q ’ 1048. This means that it is
virtually impossible to create a dataset exhaust-
ing the whole ensemble of possible circuits. We
instead resort to the generalization capability of
deep CNNs. These are expected to provide accu-
rate predictions for previously unseen instances,
even when trained on (largely non-exhaustive)
training sets including a number Ntrain (cid:28) Q of
training instances.

While the set S is, in principle, universal, the
choice of operating on all qubits in every layer
implies that some unitary operators cannot be
represented. Therefore, we also consider the ex-
tended set S ∗ = S S{I}, where the identity is
explicitly included. For this set, a ﬁve channel
one-hot encoding is needed. We use the map rep-
resented in Fig. 2. Most of the results reported in
this article are based on the set S, unless stated
otherwise. Notably, this set is ﬂexible enough to

 
 
represent the BV algorithm, which we use as a
relevant test bed. For a few representative test
cases, we also consider the extended gate set S ∗,
ﬁnding very similar performances.

2.2 Target values

The output state of a quantum circuit can be
written as

|ψi = U |0i⊗N ,
(3)
where the tensor product |0i⊗N ≡ |0i1 ⊗ . . . |0iN
is the input state and the unitary operator U rep-
resents the sequence of quantum gates that con-
stitute the circuit. Here and in the following, we
indicate with |0ii and |1ii the eigenvectors of the
Pauli operator Zi acting on qubit i = 1, . . . , N ,
such that Zi |0ii = |0ii and Zi |1ii = − |1ii. With
this notation, each state |xi of the many-qubit
computational basis corresponds to a bit string
x = x1 . . . xN , where xi = 0, 1 for i = 1, . . . , N .
Our goal is to perform supervised learning of out-
put expectation values. First, we focus on the
single-qubit expectation values

hZii ≡ hψ|Zi|ψi .

(4)

These expectation values can be computed as

hZii = |hψ|0ii|2 − |hψ|1ii|2 .

(5)

It is convenient to perform the following rescal-
ing:

zi = 1 −

hZii + 1
2

,

(6)

so that zi ∈ [0, 1], and zi = 0 corresponds to |0ii,
while zi = 1 corresponds to |1ii. The rescaled
variable zi is the ﬁrst target value we address for
supervised learning. It is worth anticipating that
we will consider both CNNs designed to predict
only one expectation value, say, z1, and CNNs
that
single-qubit
expectation values zi, for i = 1, . . . , N . This is
discussed with more details in subsection 2.4.

simultaneously predict all

For illustrative purposes, we show in Fig. 3 the
distribution of the target value z1 over an ensem-
ble of random circuits. Four representative cir-
cuit sizes are considered. One notices rather pro-
nounced peaks around z1 = 1/2 for more qubits
and/or deeper circuits.

The second target values we consider are the
two-qubit expectation values hZiZji, where, in

Figure 3: Normalized histograms of the rescaled single-
qubit expectation values z1 = 1− hZ1i+1
over ensembles
of random circuits. Diﬀerent number of qubits N and
circuit depths P are considered in the four panels: N =
3 and P = 5 (a); N = 3 and P = 20 (b); N = 5 and
P = 10 (c); N = 20 and P = 6 (d). The ensembles in
(a), (b), and (c), and the one in (d) include 5000 and
2000 circuits, respectively.

2

general, i, j = 1, 2, ..., N . Speciﬁcally, we focus
on the case i = 1 and j = 2, and the target value
is the rescaled variable:

z12 = 1 −

hZ1Z2i + 1
2

.

(7)

Clearly, single-qubit and two-qubit Pauli-Z ex-
pectation values represent a limited description
of the circuit output. However, this information
is suﬃcient to unambiguously identify the out-
put of a signiﬁcant category of quantum circuits.
This category is described in the following sub-
section, where we also discuss some relevant ex-
amples belonging to the category.

2.3 Emulable quantum algorithms

For certain quantum algorithms, only a small
subset of the 2N output bit strings have non-
zero measurement probability. In fact, some rel-
evant circuits have only one possible outcome
(in the absence of noise and errors). These are
discussed below. First, we consider the more
generic category for which only two output bit
strings, which we indicate as a and b, have non-
zero measurement probabilities p(a) = |ha|ψi|2

4

0.00.10.20.30.40.50.6(a)0.00.10.20.3(b)0.00.20.40.60.81.0z10.00.10.20.30.40.5(c)0.00.20.40.60.81.0z10.00.10.20.30.40.50.6(d)and p(b) = |hb|ψi|2. With this notation, one has
p(a) + p(b) = 1. For this category of circuits, the
expectation values hZii and hZiZji provide all
the information required to unambiguously iden-
tify the bit strings a and b. This statement is
proven here by providing an explicit algorithm.
It is assumed that p(x) = 0 for x 6= a, b and
that the expectation values mentioned above are
known. It is worth emphasising that, in fact, if
p(a) 6= p(b), knowledge of the single-qubit expec-
tation values suﬃces. Indeed, the values hZiZji
are only used when p(a) = p(b) — see case iv)
below — and such case is easily identiﬁed since
one must have hZii = 0 for at least one qubit i.

2

and p(b) = 1+hZii

Proof. The qubit are analyzed in the order i =
1, . . . , N . Four possible cases have to be sepa-
rately treated:
i) If hZii = 1, the corresponding bits are set to
ai = bi = 0.
ii) If hZii = −1, one sets ai = bi = 1.
iii) If hZii ∈ (−1, 1) and either i = 1 or i > 1
with aj = bj for j = 1, . . . , i − 1 we arbitrarily
set, without loss of generality, ai = 1 and bi = 0.
Notice that we can also infer the two probabili-
ties: p(a) = 1−hZii
iv) Otherwise, when p(a) 6= p(b) (this is known
from case iii), two expectation values are pos-
sible. One is hZii = hZji, where the integer
j ∈ [1, i − 1] is the ﬁrst index such that hZii ∈
(−1, 1) (at least one exists); in this case one sets
If, instead, hZii 6= hZji,
ai = aj and bi = bj.
one sets ai = 1 − aj and bi = 1 − bj. When
p(a) = p(b) = 1/2, one must have hZii = 0,
and we also know that an integer j ∈ [1, i − 1]
such that hZji = 0 exists.
In this situation,
hZiZji = ±1.
If hZiZji = 1, one sets ai = aj
and bi = bj. If, instead, hZiZji = −1, one sets
ai = 1 − aj and bi = 1 − bj.

2

.

As discussed in the previous paragraph, the
single-qubit expectation values hZii, eventually
combined with hZiZji, are suﬃcient to identify
the output bit strings when only two of them
have non-zero probability. Clearly, the values
hZii are suﬃcient when only one bit string is
possible. Interestingly, some paradigmatic quan-
tum algorithm belong to this group. A rele-
vant example is the Deutsch-Jozsa algorithm [26].
This allows assessing whether a boolean function
f : {0, 1}(N −1) → {0, 1} is either constant or
balanced. The algorithm involves measuring the

5

ﬁrst N − 1 qubits. If the outcome corresponds to
the bit string 00...0, the function is constant, oth-
erwise the function is balanced. Predicting N − 1
single-qubit expectation values allows reaching
the same result. Practically,
if hZii = 1 for
i = 1, 2, ..., N − 1, the only possible bit string is
the bit string 00...0, corresponding to a constant
function. Otherwise, the function is balanced.
Also in the BV algorithm [19] there is only one
possible outcome. This algorithm is designed to
identify an unknown bit string w ∈ {0, 1}N −1,
assuming we are given an oracle that implements
the boolean function f : {0, 1}N −1 → {0, 1} de-
ﬁned as f (x) = x (cid:125) w, where the symbol (cid:125)
represents the dot product modulo 2. Notably,
the BV algorithm provides the answer with one
function query, outperforming classical comput-
ers which require N − 1 queries. Single-qubit ex-
pectation values allow identifying w: hZii = 1
corresponds to wi = 0, while hZii = −1 corre-
sponds to wi = 1. The Grover algorithm with
two searched items [27] and the quantum count-
ing algorithm [28] have two output bit strings
with much higher probabilities than the other
strings. Therefore one could use the predictions
for hZii and hZiZji to emulate also these latter
algorithms.

2.4 Convolutional neural networks and training
protocol

The CNNs considered in this article have the
overall architecture described in Fig. 4. Rele-
vant variations occur in the last layer. Therein,
the number of neurons corresponds the desired
number of outputs No. Speciﬁcally, we consider
CNNs designed to predict only one expectation
value at a time (No = 1), as well as N expec-
tation values simultaneously (No = N ). Clearly,
in the ﬁrst case, the last layer includes only one
neuron. In the second case, it includes N neu-
rons. The overall network structure we adopt is
standard in ﬁelds such as, e.g., image classiﬁca-
tion or object detection. The ﬁrst part includes
Nc multi-channel convolutional layers, which cre-
ate ﬁltered maps of their input. As in standard
CNNs, the output of the convolutional part is
connected to a few dense layers (four in our case)
with all-to-all interlayer connectivity. However,
in standard CNNs, the connection is commonly
performed through a ﬂatten layer. This choice
would force to scale the width of the map passed

to the ﬁrst dense layer with the size of the net-
work input. Therefore, the whole network would
be applicable only to one circuit size.
Instead,
we perform the connection using a global pool-
ing layer. This extracts the maximum values
of each (whole) map in the last convolutional
layer. This feature was adopted in Ref. [15] for
the supervised learning of ground-state energies
of quantum systems.
It allowed implementing
scalable CNNs, i.e., networks that can be trained
on heterogeneous datasets including diﬀerent sys-
tem sizes and that can predict properties for sizes
larger than those included in the training set.
Here, we exploit the global pooling layer to allow
a single CNN addressing diﬀerent circuit sizes.
Notice, however, that full scalability is obtained
only when the CNN predicts only one expecta-
tion value (with a single neuron in the output
layer) at a time. More expectation values corre-
sponding to diﬀerent qubits can, in fact, be pre-
dicted even by the single output network. How-
ever, these predictions have to be performed in a
sequential manner, by feeding the network with
an appropriate swapping of the features. Specif-
ically, when the goal is to predict, say, zj, for
any j ∈ [2, N ], one can employ a CNN trained
to predict z1, swapping the rows 1 and j of the
If, instead, the goal is to si-
network’s input.
multaneously predict the expectation values cor-
responding to all qubits, obviously the number
of neurons in the last layer has to be adapted
to the targeted qubit number. In this case, full
scalability is lost.

The training of the CNN is performed by min-
imizing the loss function. We adopt the binary
cross-entropy:

L = −

1
Ntrain
+ (1 − y(k)

i

NtrainX

NoX

h

y(k)
i

log(ˆy(k)

i

)

i=1
k=1
) log(1 − ˆy(k)

i

i

)

,

(8)

i

where Ntrain is the number of instances in the
training set, No is the number of outputs (corre-
sponding to the number of nodes in the last dense
layer), and ˆy(k)
is the network prediction corre-
sponding to the (ground-truth) target value y(k)
.
As discussed above, we consider the cases No =
N and No = 1. In the ﬁrst case, the N target
values correspond to all rescaled single-qubit ex-
pectation values: yi = zi, for i = 1, . . . , N . In the
latter case, we consider only one (rescaled) single-
qubit expectation value, namely, y1 = z1, or one

i

(rescaled) two-qubit expectation value, namely,
y1 = z12. The optimization method we adopt
is a successful variant of the stochastic gradient-
descent algorithm, named Adam [29]. No beneﬁt
is found by introducing a regularization term in
Instead, batch normalization
the loss function.
layers are included after every layer, before the
application of the activation function. The cho-
sen mini-batch size is in the range Nb ∈ [128, 512],
depending on the training set size. The CNNs
are trained on large ensembles of random quan-
tum circuits. These are implemented and sim-
ulated using the Qiskit library. These simula-
tions provide numerically exact predictions for
the considered expectation values. We also con-
sider noisy estimates of the expectation values
obtained from ﬁnite samples of simulated mea-
surements. These estimates are employed in sub-
section 3.3 to inspect the impact of errors in the
training set on the prediction accuracy. After
training, the CNNs are tested on previously un-
seen random circuits. It is also worth mentioning
that we remove possible circuit replicas, both in
In fact, only
the training and in the test sets.
for the smallest circuits we consider, correspond-
ing to N = 3 and P = 5, one can ﬁnd within
Ntrain ’ 106 training circuits a non-negligible
number of identical replicas.

3 Results

3.1 Single-qubit expectation values

The CNNs described in subsection 2.4 are trained
to map the circuit descriptors (see subsection 2.1)
to various outputs. We ﬁrst focus on a CNN
designed to simultaneously predict the rescaled
single-qubit expectation values zi, with i =
1, . . . , N . Fig. 5 displays the scatter plots of pre-
dicted versus ground-truth expectation values,
for four representative circuit sizes. The tests
are performed on random circuits distinct from
those included in the training set. One notices
a close correspondence in all four examples. In
Fig. 6, we analyse how the prediction accuracy
varies with the circuit depth P . Three datasets
correspond to random circuits generated with the
gates from the set S, but with diﬀerent qubit
numbers N . For the fourth dataset the gates are
sampled from the extended set S ∗ (see subsec-
tion 2.1). To quantify the accuracy, we consider

6

Figure 5: Rescaled single-qubit expectation values ˆy
predicted by the CNN versus the ground-truth results
y ≡ zi, with i = 1, . . . , N . The latter results are simu-
lated via Qiskit. Diﬀerent number of qubits N and cir-
cuit depths P are considered in the four panels: N = 3
and P = 5 (a); N = 3 and P = 7 (b); N = 5 and
P = 5 (c); N = 5 and P = 7 (d). These test sets
include 100 random circuits. The color scale (blue to
yellow) represents the absolute discrepancy |ˆy − y|. The
(red) line represents the bisector ˆy = y.

Figure 4: Representation of the CNN for the illustra-
tive case of N = 3 qubits and circuit depth P = 5.
Boxes report the input and the output shapes of each
layer. This example includes Nc = 10 convolutional
layers (Conv2D). Their input and output shapes have
dimensions: (B, L1, L2, F ), where B = None is the
mini-batch size (not speciﬁed), L1 = N and L2 = P
denote the size of the two-dimensional feature maps,
while F is the number of ﬁlters. For the dense layers
(Dense), we only have the mini-batch size and the num-
ber of nodes. The ﬁgure omits the batch normalization
layers, included in every layer before the application of
the activation function. The latter corresponds to the
Mish function [30], except for the last node where it
corresponds to the sigmoid function.
It is worth high-
lighting the global maximum pooling layer (GlobalMax-
layer to
Pooling2D) connecting the last convolutional
the ﬁrst dense layer.

7

0.00.20.40.60.81.0ˆy(a)(b)0.00.20.40.60.81.0y0.00.20.40.60.81.0ˆy(c)0.00.20.40.60.81.0y(d)the coeﬃcient of determination:

R2 = 1 −

PNtest
k=1
PNtest
k=1

PNo

i=1 (y(k)
i=1 (y(k)
PNo

i − ˆy(k)
)2
i
i − ¯y)2

(9)

i

i

where ˆy(k)
is the prediction associated to the
ground-truth target value y(k)
, ¯y is the average
of the target values, Ntest is the number of ran-
dom circuits in the test set, and No is the num-
ber of outputs.
In this case, we have No = N
outputs. It is worth stressing that R2 quantiﬁes
the accuracy in relation to the intrinsic variance
of the test data. It is therefore suitable for fair
comparisons among diﬀerent circuits sizes, which
generally correspond to diﬀerent variances of ex-
pectation values. For small P one observes re-
markably high scores R2 ’ 1, corresponding to
essentially exact predictions. However, the accu-
racy signiﬁcantly decreases for deeper circuits. It
is worth pointing out that, in this analysis, the
depth of the CNN is not varied, and the training
set size is also ﬁxed at Ntrain ’ 106.

Figure 6: Coeﬃcient of determination R2 for rescaled
single-qubit expectation values zi as a function of the
circuits depth P . Three datasets correspond to ran-
dom circuits built with gates from the set S, but having
diﬀerent qubit numbers N (see legend). For the fourth
dataset, the extended gate set S ∗ is used. For P = 5 the
neural network is trained from scratch on Ntrain ’ 106
instances, while for P > 5 the training starts with the
optimized weights and biases for P − 1.

The prediction accuracy can be improved by
enlarging the training set, or by increasing the
depth of the CNN. The ﬁrst approach is ana-
lyzed in Fig. 7, for three representative circuit
sizes. One notices the typical power-law sup-
pression (see, e.g., [4, 31]) of the prediction error
1 − R2 ∝ N −α
train, where α > 0 is a non-universal

8

Figure 7: Normalized prediction error 1 − R2 for
(rescaled) single-qubit expectation values zi as a func-
tion of the training set size Ntrain. Three datasets corre-
spond to random circuits built with gates from the set S,
but having diﬀerent qubit numbers N and circuit depths
P (see legend). For the fourth dataset, the extended
gate set S ∗ is used. The adopted CNN is described in
Fig. 4.

exponent. The second approach is analyzed in
Fig. 8. We ﬁnd that the R2 score systematically
increases with the number of convolutional lay-
ers Nc. It is worth pointing out that the deep-
est CNNs adopted in this article include around
2 × 106 parameters. This number does not rep-
resent a noteworthy computational burden for
modern high-performance computers, in particu-
lar if equipped with state-of-the-art graphic pro-
cessing units. In fact, signiﬁcantly deeper neural
networks are routinely trained in the ﬁeld of com-
puter vision. Relevant examples are VGG-16,
VGG-19 [32] and ConvNeXt [33]. Instead, creat-
ing copious training sets for circuits with N > 10
qubits becomes computationally expensive.
In
fact, simulating circuits with N (cid:29) 10 qubits is
virtually impossible. Two strategies to circum-
vent this problem are discussed in the subsection
3.2.

3.2 Transfer learning and extrapolation

We exploit the scalability of the CNNs featur-
ing the global pooling layer to exploit two strate-
gies, namely, transfer learning and extrapolation.
The ﬁrst strategy is rather common in ﬁelds such
as, e.g., computer vision [34].
It involves per-
forming an extensive training of a deep CNN
on a large generic database. Then, the pre-
trained network is used as starting point in a
second training performed on a more speciﬁc,

567891011P0.8500.8750.9000.9250.9500.9751.000R2N=3N=4N=5N=4,S∗1052×1053×105Ntrain10−210−11−R2N=3,P=7N=4,P=7N=3,P=8N=4,P=7,S∗Figure 8: Coeﬃcient of determination R2 for rescaled
single-qubit expectation values zi as a function of the the
number of convolutional layers Nc. The qubit number is
N = 3 and the circuit depth is P = 7 . The training set
includes Ntrain ’ 106 random circuits. The employed
CNN is similar to the one depicted in Fig. 4, but with
two fully-connected layers including 100 and 50 neurons.
The Nc convolutional layers include F = 32 ﬁlters. The
batch normalization layers and the output layer act as
described in Fig. 4.

typically smaller, database. At this stage the
CNN learns to solve the targeted task. This
approach has already been adopted for the su-
pervised learning of ground-state properties of
quantum systems [9, 14, 15, 16, 35]. Here, we
use it to accelerate the learning of deep quantum
circuits, exploiting a pre-training performed on
computationally cheaper circuits with fewer gates
per qubit. Speciﬁcally, we compare the learning
speed of a CNN trained from scratch on circuits
of depth P = 8, with the one of a CNN pre-
trained on circuits of depth P = 7. The results
are analyzed in Fig. 9. We ﬁnd that the pre-
trained CNN needs a signiﬁcantly smaller train-
ing set to reach high R2 scores.

The extrapolation strategy aims at predicting
properties of circuits including more qubits than
those included in the training set. As discussed
in subsection 2.4, to allow ﬂexibility in the num-
ber of qubits N , we adopt the CNN with one out-
put neuron. This, in combination with the global
pooling layer, provides the network full scalabil-
ity, allowing the same network parameters to be
applied to diﬀerent circuit sizes. The results are
analyzed in Fig. 10. Remarkably, we ﬁnd that
a CNN trained on (computationally aﬀordable)
circuits with ˜N = 10 qubits accurately predicts
the (single qubit) expectation values of signiﬁ-

Figure 9: Coeﬃcient of determination R2 for the
rescaled single-qubit expectation values z1 as a func-
tion of the training set size Ntrain. The size of the test
circuits is: N = 3 and P = 8. (Blue) squares corre-
spond to training from scratch on the same circuit size.
The (violet) circles correspond to transfer learning from
P = 7 to P = 8 (same qubit number). The pretraining
on P = 7 is performed with Ntrain ∼ 106 circuits. The
CNN is as described in Fig. 4.

cantly larger circuits (with N = 20 qubits). In-
stead, when the training is performed on smaller
circuits ( ˜N ’ 5), the R2 score rapidly drops as
the test-circuit size increases. This suggests that
a minimum training circuit-size is needed to al-
low the CNN learning how to perform accurate
extrapolations.

Figure 10: Coeﬃcient of determination R2 for rescaled
single-qubit expectation values z1 as a function of the
number of qubits N in the test circuits. Both training
and test circuits have depth P = 6. Diﬀerent datasets
correspond to diﬀerent number of qubits ˜N in the train-
ing circuits (see legend). The employed CNN is as shown
in Fig. 4 except for the last layer, which has only one
neuron.

9

1234Nc0.600.650.700.750.800.850.900.95R210000100000200000300000Ntrain0.840.860.880.900.920.940.960.98R2P=7→P=8P=82468101214161820N−0.20.00.20.40.60.81.0R2˜N=3˜N=5˜N=7˜N=103.3 Real quantum computers and noisy simu-
lators

Supervised learning with scalable CNNs is be-
ing discussed as a potentially useful benchmark
for quantum computers. Therefore, it is inter-
esting to compare the predictions provided by
trained CNNs with those of actual physical de-
vices. For this purpose, we execute random cir-
cuits on ﬁve devices freely available through IBM
Quantum Experience [21]. In Fig. 11, the predic-
tion accuracy of a CNN trained on Ntrain ’ 107
classically simulated circuits is compared with
the corresponding scores reached by the IBM
devices. The quantum circuits include N = 5
qubits and P = 10 gates per qubit. In this case,
the neural network outperforms the chosen phys-
ical quantum devices. Another comparison be-

Fig. 12 also a CNN trained on Ntrain ’ 107
circuits with N = 10 qubits, and used to ex-
trapolate predictions for N = 11. One observes
that this CNN outperforms all of the considered
physical devices. We recall that, in Fig. 10 (see
also Fig. 18 below), accurate extrapolations to
even more challenging qubit numbers N = 20
are demonstrated. These ﬁndings indicate that
scalable CNNs trained via supervised learning on
classically simulated quantum circuits represent
a potentially useful benchmark for the develop-
ment of quantum devices.

Figure 12: Coeﬃcient of determination R2 for (rescaled)
single-qubit expectation values as a function of the cir-
cuit depth P . The ﬁve datasets correspond to three
IBM quantum computers with N = 3 qubits, and to
two CNNs, with N = 3 and with N = 11 qubits, re-
spectively. The quantum computers are tested on 100
test circuits, estimating the three expectation values via
Nmeasure = 2048 measurements. The CNN for N = 3
is trained on Ntrain ’ 106 random circuits with the
same number of qubits, and it simultaneously predicts
the three (rescaled) single-qubit expectation values z1,
z2, and z3. For N = 11, the CNN has one output neu-
ron. It is trained on Ntrain ’ 107 circuits with ˜N = 10
qubits, and it extrapolates to N = 11 performing a sin-
gle prediction for z1. The training of both CNNs starts
with the optimized weights and biases for P − 1, ex-
cept for the smallest P , where the training starts from
scratch.

One can envision the use of data produced
by physical quantum devices to train scalable
CNNs. This could allow them learning how to
emulate classically intractable quantum circuits.
However, physical devices only allow estimating
output expectation values via ﬁnite number of
In the era of noisy intermedi-
measurements.
ate quantum computers [36], one should expect
this number to be quite limited, leading to noisy

Figure 11: Coeﬃcient of determination R2 for single-
qubit expectation values measured on ﬁve IBM quan-
tum computers and predicted by a trained CNN. The
circuit size is: N = 5 and P = 10. The ﬁve
quantum computers, namely, QC-0 ≡ ibmq_lima,
QC-1 ≡ ibmq_bogota, QC-2 ≡ ibmq_belem, QC-3 ≡
ibmq_quito, and QC-4 ≡ ibmq_manila, are ordered
for increasing R2 score. For each quantum circuit, the
expectation values are estimated using Nmeasure = 2048
measurements. The training set size is Ntrain ’ 107.
The CNN is as described in Fig. 4.

tween CNNs and quantum computers is shown
in Fig. 12. Here, only three IBM devices are con-
sidered, and the accuracy scores R2 are plotted
as a function of the circuit depth P , for a ﬁxed
number of qubits N = 3. Notably, for P > 9,
two out of the three quantum computers outper-
form the CNN. Notice that the latter is trained
on a (ﬁxed) training set with Ntrain ’ 106 in-
In fact, it is quite feasible to improve
stances.
the CNN’s accuracy, even for larger qubit num-
bers. As a term of comparison, we consider in

10

QC-0QC-1QC-2QC-3QC-4CNN0.750.800.850.900.951.00R2567891011P0.8500.8750.9000.9250.9500.9751.000R2˜N=10→N=11,z1N=3ibmqlimaibmqmanilaibmqbelemestimates aﬀected by signiﬁcant statistical ﬂuc-
tuations. Therefore, it is important to analyse
the impact of this noise on the supervised train-
ing of CNNs. For this, we consider as train-
ing target values the noisy estimates obtained
by simulating via Qiskit ﬁnite numbers of mea-
surements Nmeasure.
In the testing phase, the
CNN’s predictions are compared against exact
expectation values. This comparison is shown
in the scatter plot of Fig. 13, for the case of
Nmeasure = 32. One notices that, while the noisy
estimates display large random ﬂuctuations, the
CNN’s predictions accurately approximate the
exact expectation value. This eﬀect is quanti-
tatively analyzed via the R2 score in Fig. 14.
Notably, the CNN reaches remarkably accuracies
R2 (cid:38) 0.99 for numbers of measurements as small
as Nmeasure ∼ 32, despite the fact that the esti-
mated expectation values are signiﬁcantly inac-
curate, corresponding to R2 ’ 0.9. An analo-
gous resilience to noise in training data was ﬁrst
observed in applications of CNNs to image clas-
siﬁcation tasks [37].
It was also demonstrated
in the supervised learning of ground-state ener-
gies of disordered atomic quantum gases [8]. It is
quite relevant to recover this property in the case
of quantum computing, where noise represents
a major obstacle to be overcome. Chieﬂy, this
resilience paves the way to the use of quantum
computing devices for the production of train-
ing datasets. Deep CNNs could be trained to
solve classically intractable circuits, and then dis-
tributed to practitioners more easily than a phys-
ical device.

3.4 Emulation of the Bernstein-Vazirani algo-
rithm

As discussed in subsection 2.3, the BV algorithm
can be emulated by predicting the single-qubit
expectation values hZii, with i = 1, . . . , N − 1.
This means that these expectation values allow
one unequivocally identifying the sought-for bit
string w ∈ {0, 1}N −1. Here we analyse how accu-
rately our scalable CNNs emulate this algorithm.
Notably, we challenge the CNN in the extrapo-
lation task, i.e., we use it to emulate BV circuits
with (many) more qubits than those included in
the training set. Conventionally, the BV algo-
rithm is implemented using the following gates:
I, Z, H, and CX. However, it can also be real-
ized using only gates from the set S. An example

Figure 13: Predictions of (rescaled) single-qubit expec-
tation values versus ground-truth results y ≡ zi (see
Eq. (6)). The CNN predictions ˆy (red circles) are com-
pared against the noisy estimates ˜zi (black squares) ob-
tained by averaging Nmeasure = 32 (simulated) mea-
surements. The circuits size is N = 3 and P = 7. The
CNN is trained on the noisy estimates corresponding to
Ntrain ’ 106 random circuits.

Figure 14: Coeﬃcient of determination R2 as a function
of the number of simulated measurements Nmeasure. In
the main panel, the R2 score of the noisy estimates ˜zi
with respect to the ground-truth (rescaled) single-qubit
expectation values zi (black squares) is compared with
the corresponding score of the CNN predictions (red cir-
cles). The circuits size is N = 3 and P = 7. The inset
displays the CNN data on a narrower scale.

of this alternative implementation is visualized in
Fig. 15. Notice that a dangling T gate, acting
on the N -th qubit in the last layer, needs to be
inserted. However, this does not aﬀect the rel-
evant output expectation values. The tests we
perform are limited to sought-for bit strings w
where all bits except one, two, or three, have
zero value; that is, only one, two, or three indices
iα ∈ [1, N − 1] exist such that wiα = 1, where

11

0.00.20.40.60.81.0zi0.00.20.40.60.81.0ˆy;˜zNoisyest.CNN48163264128256512Nmeasure0.00.20.40.60.81.0R2CNNNoisyest.481632641282565120.960.970.980.991.0α ∈ {1, 2, 3} spans the group of (up to three)
non-zero bits. These indices are randomly se-
lected. Speciﬁcally, a CNN is trained on circuits
with ˜N = 10 qubits and depth P = 7, 8, or 9, for
one, two, or three non-zero bits, respectively. It is
then invoked to predict the single qubit expecta-
tion values of BV circuits with larger N . To visu-
alize the prediction accuracy, we show in Fig. 16
the expectation values zi, for i ∈ [1, N ], for a
BV circuit with as many as N = 5 × 105 qubits.
Notice that also the N -th expectation value, cor-
responding to the ancilla qubit, is shown. One
notices that, beyond the N -th qubit, all but one,
two, or three expectation values are small, mean-
ing that the CNN is able to identify the sought-
for indices iα corresponding to wiα = 1.
It is
remarkable that a CNN trained on random cir-
cuits learns to emulate a rather peculiar algo-
rithm such as the BV circuit, even for larger qubit
numbers.

(a)

(b)

Figure 15: Representation of the Bernstein-Vazirani
(BV) algorithm for a sought-for string w ∈ {0, 1}3, cor-
responding to a circuit with N = 4 qubits. Panel (a)
displays the conventional implementation using the gates
I, Z, H, and CX, for the bit string w = 010. Panel (b)
displays an alternative implementation using only gates
from the set S. The dangling T -gate on the last qubit
is required for shape consistency, and it does not aﬀect
the relevant output probabilities.

3.5 Two-qubit expectation values

The scalable CNN can be trained to predict also
two-qubit expectation values. We consider only
the ﬁrst two qubits, i.e., the CNN predicts the
rescaled expectation value z12 deﬁned in Eq. (7).
Henceforth, only one neuron is included in the
output layer (see discussion in subsection 2.4).
Again, it is worth pointing out that the same
CNN could predict also other two-qubit expec-
tation values, corresponding to any pair (i, j).
These predictions are obtained by performing the
double exchange of row indices (1, 2) ←→ (i, j) in
the circuit descriptor matrix. In Fig. 17, the pre-
diction accuracy is analysed as a function of the

Figure 16: Rescaled output expectation values zi (see
Eq. (6)) as a function of the qubit index i = 1, . . . , N ,
for a BV algorithm with N = 5 × 105 qubits. The CNN
predictions (blue circles) are compared to the expected
values (red squares). The three panels correspond to
sought-for bit strings w with one non-zero bit i1 =
132121 [panel (a)], with two non-zero bits (i1, i2) =
(341924, 141725) [panel (b)], and with three non-zero
bits (i1, i2, i3) = (64793, 212973, 485883) [panel (c)].
The CNNs are trained on Ntrain ’ 106 [panel (a)] and
Ntrain ’ 107 [panels (b) and (c)] random circuits with
N = 10 qubits.

circuit depth P . One observes remarkably high
scores R2 ’ 1 for small and intermediate circuits

12

R2 is plotted in Fig. 18. The three datasets
corresponds to diﬀerent training qubit numbers
˜N = 5, 7, and 10, and the extrapolation is ex-
tended up to N = 20. Notably, if the train-
ing qubit number is suﬃciently large, the pre-
dictions remain remarkably accurate for signiﬁ-
cantly larger circuits.

4 Conclusions

We explored the supervised learning of ran-
dom quantum circuits. Deep CNNs have been
trained to map a properly designed one-hot en-
coding of the circuit quantum gates to the corre-
sponding single-qubit and two-qubit output ex-
pectation values. After training on suﬃciently
large datasets of classically simulated circuits,
the CNN provided remarkably accurate predic-
tions, even superior than those provided by small
quantum computers available for free from the
IBM quantum program. Notably, we imple-
mented scalable CNNs. This allows them pre-
dicting the properties of (computationally chal-
lenging) circuit sizes larger than those included
in the training set. This represents a promis-
ing strategy to produce benchmark data for clas-
sically intractable qubit numbers. The consid-
ered expectation values represent an admittedly
limited description of the circuits’ output. How-
ever, we demonstrated that circuits with only one
or two possible output bit strings can be em-
ulated using these targeted expectation values.
Notably, the BV algorithm belongs to this cate-
gory, and we use it as a relevant benchmark for
the CNN’s predictions. In fact, we veriﬁed that
the scalable CNN can emulate it even for qubit
numbers much larger than those included in the
training set. The supervised learning turned out
to be remarkably robust against random errors
in the training set. Speciﬁcally, the CNN pro-
vided accurate predictions for expectation val-
ues even when trained on noisy averages per-
formed over few simulated measurements. This
ﬁnding suggests that CNNs could be trained on
data produced by noisy intermediate-scale quan-
tum computers [36]. To produce the results re-
ported in this article, several training and test
datasets have been produced, and various CNNs
have been trained. To facilitate future compar-
ative studies, avoiding cluttering the repository,
we provide the datasets and the code used for

Figure 17: Coeﬃcient of determination R2 for the
rescaled two-qubit expectation value z12 (see Eq. (7))
as a function of the circuit depth P . The three datasets
correspond to diﬀerent qubit numbers N . The training
set size is Ntrain ’ 106. For P = 5 the CNN is trained
from scratch, while for P > 5 the weights and biases are
initialized at the optimized values for P − 1. This allows
a signiﬁcant reduction in computation time.

depths, and a moderate accuracy degradation for
deeper circuits. As already shown for single-qubit
expectation values (see subsection 3.1), we stress
that also in this case the prediction accuracy can
be further improved by increasing the training
set size or deepening the CNN (data not shown).

Figure 18: Coeﬃcient of determination R2 for the
rescaled two-qubit expectation value z12 as a function of
the number of qubits N of the test circuits. The three
datasets correspond to diﬀerent qubit numbers ˜N of the
training circuits. The training set includes Ntrain ’ 106
random circuits. Both training and test circuits have
depth P = 6.

The scalable CNN is also tested in the extrap-
olation task, i.e., in predicting the two-qubit ex-
pectation value for circuits larger than those in-
cluded in the training set. The accuracy score

13

567891011P0.8500.8750.9000.9250.9500.9751.000R2N=3N=4N=55791113151719N−0.20.00.20.40.60.81.0R2˜N=5˜N=7˜N=10one emblematic test, namely, the one analysed in
Fig. 10, through Ref. [38].

Classical simulations of quantum algorithms
play a pivotal role in the development of quan-
tum computing devices. On the one hand, they
provide benchmark data for validation. On
the other hand, they represent an indispensable
term of comparison to justify claims of quan-
tum speed-up in the solution of computational
problems [3]. For adiabatic quantum computers,
quantum Monte Carlo algorithms have emerged
as the standard benchmark [39, 40, 41, 42]. This
stems from their ability of simulating the tun-
neling dynamics of quantum annealers based on
sign-problem free Hamiltonians [43, 44, 45, 46,
47]. Simulating universal gate-based quantum
computers is more challenging. Direct simu-
lation methods, such as those based on ten-
sor networks [48], are being continuously im-
proved [49, 50, 51, 52, 53], but they anyway suf-
fer from an exponentially-scaling computational
cost.
Supervised machine-learning algorithms
were recently proven to be able of solving com-
putational tasks which are intractable for algo-
rithms that did not learn from data [13]. In this
article, we investigated their eﬃciency in simulat-
ing a limited description of quantum circuits’ out-
put. It is worth mentioning that the combination
of classical machine learning and quantum com-
puters has already been discussed in various con-
texts [25, 54, 55, 56]. For example, in Ref. [57],
generative neural networks trained via unsuper-
vised learning were used to accelerate the conver-
gence of expectation-value estimation. One can
envision the use of stochastic generative neural
network to predict a more complete description
of the circuits’ outputs such as, e.g., the classi-
cal shadow [13, 58]. We leave this endeavour to
future investigations.

Acknowledgments

This work was supported by the Italian Ministry
of University and Research under the PRIN2017
project CEnTraL 20172H2SC4, and by the Eu-
ropean Union Horizon 2020 Programme for Re-
search and Innovation through the Project No.
862644 (FET Open QUARTET). S.P. acknowl-
edges PRACE for awarding access to the Fenix
Infrastructure resources at Cineca, which are par-
tially funded by the European Union’s Horizon

2020 research and innovation program through
the ICEI project under the Grant Agreement No.
800858. S. Cantori acknowledges partial support
from the B-GREEN project of the italian MiSE
- Bando 2018 Industria Sostenibile. The authors
acknowledge the use of IBM Quantum services
for this work. The views expressed are those of
the authors, and do not reﬂect the oﬃcial policy
or position of IBM or the IBM Quantum team.

References

[1] A. Steane. Quantum computing. Rep. Prog.
doi:10.

Phys., 61(2):117–173, feb 1998.
1088/0034-4885/61/2/002.

[2] D. Aharonov.

Quantum computa-
In Annual Reviews of Compu-
259–346.

tion.
tational Physics VI,
doi:10.1142/9789812815569_0007.

pages

[3] T. F. Rønnow, Z. Wang, J. Job, S. Boixo,
S. V. Isakov, D. Wecker, J. M. Marti-
nis, D. A. Lidar, and M. Troyer. Deﬁn-
ing and detecting quantum speedup. Sci-
ence, 345(6195):420–424, 2014.
doi:10.
1126/science.1252319.

[4] F. A. Faber, L. Hutchison, B. Huang,
J. Gilmer, S. S. Schoenholz, G. E. Dahl,
O. Vinyals, S. Kearnes, P. F. Riley, and
O. A. Von Lilienfeld. Prediction errors of
molecular machine learning models lower
than hybrid DFT error. J. of Chem. The-
ory Comput., 13(11):5255–5264, 2017. doi:
10.1021/acs.jctc.7b00577.

[5] K. Hansen, F. Biegler, R. Ramakrishnan,
W. Pronobis, O. A. von Lilienfeld, K.-R.
M¨uller, and A. Tkatchenko. Machine learn-
ing predictions of molecular properties: Ac-
curate many-body potentials and nonlocal-
ity in chemical space. J. Phys. Chem. Lett.,
6(12):2326–2331, 2015. PMID: 26113956.
doi:10.1021/acs.jpclett.5b00831.

[6] K. T. Sch¨utt, H. Glawe, F. Brockherde,
A. Sanna, K. R. M¨uller, and E. K. U.
Gross. How to represent crystal structures
for machine learning: Towards fast predic-
tion of electronic properties. Phys. Rev.
B, 89:205118, May 2014.
doi:10.1103/
PhysRevB.89.205118.

14

[7] K. Ryczko, K. Mills, I. Luchak, C. Home-
Convolutional
systems.
149:134–142,
doi:https://doi.org/10.1016/j.

nick, and I. Tamblyn.
neural networks
Comput. Mater.
2018.
commatsci.2018.03.005.

for atomistic

Sci.,

[8] S. Pilati and P. Pieri. Supervised machine
learning of ultracold atoms with speckle dis-
order. Sci. Rep., 9(1), Apr 2019.
doi:
10.1038/s41598-019-42125-w.

[9] P. Mujal, `Alex Mart´ınez Miguel, A. Polls,
B. Juli´a-D´ıaz, and S. Pilati.
Supervised
learning of few dirty bosons with variable
particle number. SciPost Phys., 10:73, 2021.
doi:10.21468/SciPostPhys.10.3.073.

[10] P. J. Ballester and J. B. Mitchell.

A
machine learning approach to predicting
protein–ligand binding aﬃnity with appli-
cations to molecular docking. Bioinformat-
ics, 26(9):1169–1175, 2010. doi:10.1093/
bioinformatics/btq112.

[11] M. A. Khamis, W. Gomaa, and W. F.
Ahmed. Machine learning in computational
docking. Artif. Intell. Med., 63(3):135–
152, 2015. doi:10.1016/j.artmed.2015.
02.002.

[12] H.-Y. Huang, M. Broughton, M. Mohseni,
R. Babbush, S. Boixo, H. Neven, and J. R.
McClean. Power of data in quantum ma-
chine learning. Nat. Commun., 12(1):1–
9, 2021. doi:https://doi.org/10.1038/
s41467-021-22539-9.

[13] H.-Y. Huang, R. Kueng, G. Torlai, V. V.
Albert, and J. Preskill. Provably eﬃcient
machine learning for quantum many-body
problems, 2021. arXiv:2106.12627.

[14] K. Mills, K. Ryczko, I. Luchak, A. Domu-
rad, C. Beeler, and I. Tamblyn. Exten-
sive deep neural networks for transferring
small scale learning to large scale systems.
Chem. Sci., 10(15):4129–4140, 2019. doi:
10.1039/C8SC04578J.

[15] N. Saraceni, S. Cantori, and S. Pilati. Scal-
able neural networks for the eﬃcient learn-
ing of disordered quantum systems. Phys.
Rev. E, 102(3), Sep 2020. doi:10.1103/
physreve.102.033301.

[16] H. Jung, S. Stocker, C. Kunkel, H. Ober-
hofer, B. Han, K. Reuter, and J. T. Margraf.

15

Size-extensive molecular machine learning
with global representations. Chem. Systems
Chem., 2:e1900052, 2020.
doi:10.1002/
syst.201900052.

[17] F. Arute, K. Arya, R. Babbush, D. Bacon,
J. Bardin, R. Barends, R. Biswas, S. Boixo,
F. Brandao, D. Buell, B. Burkett, Y. Chen,
J. Chen, B. Chiaro, R. Collins, W. Court-
ney, A. Dunsworth, E. Farhi, B. Foxen,
A. Fowler, C. M. Gidney, M. Giustina,
R. Graﬀ, K. Guerin, S. Habegger, M. Har-
rigan, M. Hartmann, A. Ho, M. R. Hoﬀ-
mann, T. Huang, T. Humble, S. Isakov,
E. Jeﬀrey, Z. Jiang, D. Kafri, K. Kechedzhi,
J. Kelly, P. Klimov, S. Knysh, A. Ko-
rotkov, F. Kostritsa, D. Landhuis, M. Lind-
mark, E. Lucero, D. Lyakh, S. Mandr`a,
J. R. McClean, M. McEwen, A. Megrant,
X. Mi, K. Michielsen, M. Mohseni, J. Mutus,
O. Naaman, M. Neeley, C. Neill, M. Y. Niu,
E. Ostby, A. Petukhov, J. Platt, C. Quin-
tana, E. G. Rieﬀel, P. Roushan, N. Rubin,
D. Sank, K. J. Satzinger, V. Smelyanskiy,
K. J. Sung, M. Trevithick, A. Vainsencher,
B. Villalonga, T. White, Z. J. Yao, P. Yeh,
A. Zalcman, H. Neven, and J. Marti-
nis. Quantum supremacy using a pro-
grammable superconducting processor. Na-
ture, 574:505–510, 2019. doi:https://doi.
org/10.1038/s41586-019-1666-5.

[18] Y. Wu, W.-S. Bao, S. Cao, F. Chen, M.-
C. Chen, X. Chen, T.-H. Chung, H. Deng,
Y. Du, D. Fan, M. Gong, C. Guo, C. Guo,
S. Guo, L. Han, L. Hong, H.-L. Huang, Y.-
H. Huo, L. Li, N. Li, S. Li, Y. Li, F. Liang,
C. Lin, J. Lin, H. Qian, D. Qiao, H. Rong,
H. Su, L. Sun, L. Wang, S. Wang, D. Wu,
Y. Xu, K. Yan, W. Yang, Y. Yang, Y. Ye,
J. Yin, C. Ying, J. Yu, C. Zha, C. Zhang,
H. Zhang, K. Zhang, Y. Zhang, H. Zhao,
Y. Zhao, L. Zhou, Q. Zhu, C.-Y. Lu, C.-
Z. Peng, X. Zhu, and J.-W. Pan. Strong
quantum computational advantage using a
superconducting quantum processor. Phys.
Rev. Lett., 127:180501, Oct 2021. doi:10.
1103/PhysRevLett.127.180501.

[19] E. Bernstein and U. Vazirani. Quantum
SIAM J. Comput.,
complexity theory.
26(5):1411–1473, 1997. doi:https://doi.
org/10.1137/S0097539796300921.

[20] M. Treinish, J. Gambetta, P. Nation, qiskit
bot, P. Kassebaum, D. M. Rodr´ıguez,
S. de la Puente Gonz´alez, S. Hu, K. Kr-
sulich, J. Garrison, L. Zdanski, J. Lishman,
J. Yu, J. Gacon, D. McKay, J. Gomez,
L. Capelluto, Travis-S-IBM, M. Marques,
A. Panigrahi,
I. Rahman,
S. Wood, L. Bello, T. Itoko, C. J. Wood,
D. Singh, Drew, E. Arbel, and J. Schwarm.
Qiskit/qiskit: Qiskit 0.36.2, May 2022. doi:
10.5281/zenodo.6560959.

lerongil, R.

[21] IBM Quantum,

2021.

doi:https://

quantum-computing.ibm.com/.

[22] P. Boykin, T. Mor, M. Pulver, V. Roy-
chowdhury,
A new
and F. Vatan.
universal and fault-tolerant quantum ba-
Inf. Process. Lett., 75(3):101–107,
sis.
2000.
doi:https://doi.org/10.1016/
S0020-0190(00)00084-3.

[23] C. Toﬀalori, F. Corradini, S. Leonesi, and
S. Mancini. Teoria della computabilit`a e
della complessit`a. Collana di istruzione sci-
entiﬁca. McGraw-Hill, 2005.

[24] A. Zlokapa and A. Gheorghiu. A deep learn-
ing model for noise prediction on near-term
quantum devices, 2020. arXiv:2005.10811.

[25] S.-X. Zhang, C.-Y. Hsieh, S. Zhang, and
H. Yao. Neural predictor based quantum ar-
chitecture search. Mach. Learn.: Sci. Tech-
nol., 2(4):045027, oct 2021. doi:10.1088/
2632-2153/ac28dd.

[26] D. Deutsch and R. Jozsa. Rapid solu-
tion of problems by quantum computation.
Proc. R. Soc. Lond. A Math. Phys. Sci.,
439(1907):553–558, 1992.
doi:10.1098/
rspa.1992.0167.

[27] L. K. Grover. A fast quantum mechani-
cal algorithm for database search. In Proc.
Annu. ACM Symp. Theory Comput., STOC
’96, page 212–219, New York, NY, USA,
1996. Association for Computing Machinery.
doi:10.1145/237814.237866.

[28] M. A. Nielsen and I. L. Chuang. Quan-
tum Computation and Quantum Informa-
Cam-
10th Anniversary Edition.
tion:
bridge University Press, 2010.
doi:10.
1017/CBO9780511976667.

[29] D. P. Kingma and J. Ba.

Adam: A
method for stochastic optimization. 2014.
arXiv:1412.6980.

[30] D. Misra.

regularized
Mish: A self
non-monotonic neural activation function.
CoRR, abs/1908.08681, 2019. arXiv:1908.
08681.

[31] J. Hestness, S. Narang, N. Ardalani, G. Di-
amos, H. Jun, H. Kianinejad, M. M. A. Pat-
wary, Y. Yang, and Y. Zhou. Deep learn-
ing scaling is predictable, empirically, 2017.
arXiv:1712.00409.

[32] K. Simonyan and A. Zisserman. Very deep
convolutional networks for large-scale image
recognition, 2014. arXiv:1409.1556.

[33] Z. Liu, H. Mao, C.-Y. Wu, C. Feichtenhofer,
T. Darrell, and S. Xie. A convnet for the
2020s, 2022. arXiv:2201.03545.

[34] R. Caruana. Multitask learning. Mach.
Learn., 28(1):41–75, 1997. doi:10.1023/A:
1007379606734.

[35] M. Scherbela, R. Reisenhofer, L. Gerard,
P. Marquetand, and P. Grohs. Solving the
electronic Schr¨odinger equation for multi-
ple nuclear geometries with weight-sharing
deep neural networks. Nat. Comput. Sci.,
pages 1–11, 2022. doi:https://doi.org/
10.1038/s43588-022-00228-x.

[36] J. Preskill.

Quantum Computing in
Quan-
doi:10.22331/

the NISQ era and beyond.
tum, 2:79, August 2018.
q-2018-08-06-79.

[37] D. Rolnick, A. Veit, S. Belongie, and
N. Shavit. Deep learning is robust to mas-
sive label noise, 2017. arXiv:1705.10694.

[38] S. Cantori, D. Vitali, and S. Pilati. Super-
vised learning of quantum circuits. URL:
https://github.com/simonecantori/
Supervised-learning-of-quantum-circuits.

[39] G. E. Santoro, R. Martoˇn´ak, E. Tosatti,
Theory of quantum an-
and R. Car.
nealing of an Ising spin glass.
Science,
295(5564):2427–2430, 2002. doi:10.1126/
science.1068774.

[40] S. Boixo, T. Albash, F. M. Spedalieri,
N. Chancellor, and D. A. Lidar. Exper-
imental signature of programmable quan-
tum annealing. Nat. Commun., 4(1):1–

16

8, 2013. doi:https://doi.org/10.1038/
ncomms3067.

9(1):1–11, 2019.
10.1038/s41598-019-47174-9.

doi:https://doi.org/

[41] S. Boixo, T. F. Rønnow, S. V. Isakov,
Z. Wang, D. Wecker, D. A. Lidar, J. M.
Martinis, and M. Troyer.
Evidence for
quantum annealing with more than one
hundred qubits. Nat. Phys., 10(3):218–
224, 2014.
doi:https://doi.org/10.
1038/nphys2900.

[42] B. Heim, T. F. Rønnow, S. V. Isakov,
and M. Troyer. Quantum versus classi-
cal annealing of Ising spin glasses.
Sci-
ence, 348(6231):215–217, 2015.
doi:10.
1126/science.aaa4170.

[43] S. V. Isakov, G. Mazzola, V. N. Smelyanskiy,
Z. Jiang, S. Boixo, H. Neven, and M. Troyer.
Understanding quantum tunneling through
quantum Monte Carlo simulations. Phys.
Rev. Lett., 117:180402, Oct 2016. doi:10.
1103/PhysRevLett.117.180402.

[44] G. Mazzola, V. N. Smelyanskiy,

and
M. Troyer. Quantum Monte Carlo tunnel-
ing from quantum chemistry to quantum an-
nealing. Phys. Rev. B, 96:134305, Oct 2017.
doi:10.1103/PhysRevB.96.134305.

[45] L. T. Brady and W. van Dam. Quan-
tum Monte Carlo simulations of tunneling
in quantum adiabatic optimization. Phys.
Rev. A, 93(3):032304, 2016. doi:10.1103/
PhysRevA.93.032304.

[50] G. G. Guerreschi, J. Hogaboam, F. Baruﬀa,
and N. P. Sawaya. Intel Quantum Simula-
tor: A cloud-ready high-performance sim-
ulator of quantum circuits. Quantum Sci.
Technol., 5(3):034007, 2020. doi:10.1088/
2058-9565/ab8505.

[51] D. S. Steiger, T. H¨aner, and M. Troyer.
ProjectQ: an open source software frame-
work for quantum computing.
Quan-
tum, 2:49, January 2018. doi:10.22331/
q-2018-01-31-49.

[52] B. Villalonga, D. Lyakh, S. Boixo, H. Neven,
T. S. Humble, R. Biswas, E. G. Rief-
fel, A. Ho, and S. Mandr`a.
Establish-
ing the quantum supremacy frontier with
a 281 pﬂop/s simulation. Quantum Sci.
Technol., 5(3):034003, 2020. doi:10.1088/
2058-9565/ab7eeb.

[53] F. Pan and P. Zhang. Simulation of quan-
tum circuits using the big-batch tensor net-
work method. Phys. Rev. Lett., 128:030501,
Jan 2022. doi:10.1103/PhysRevLett.128.
030501.

[54] M. Schuld, I. Sinayskiy, and F. Petruccione.
An introduction to quantum machine learn-
ing. Contemp. Phys., 56(2):172–185, 2015.
doi:10.1080/00107514.2014.964942.

[46] E. M. Inack, G. Giudici, T. Parolini, G. San-
toro, and S. Pilati. Understanding quantum
tunneling using diﬀusion Monte Carlo simu-
lations. Phys. Rev. A, 97:032307, Mar 2018.
doi:10.1103/PhysRevA.97.032307.

[55] A. Seif, K. A. Landsman, N. M. Linke,
C. Figgatt, C. Monroe, and M. Hafezi. Ma-
chine learning assisted readout of trapped-
ion qubits. J. Phys. B, 51(17):174006, 2018.
doi:10.1088/1361-6455/aad62b.

[47] T. Parolini, E. M. Inack, G. Giudici, and
S. Pilati. Tunneling in projective quantum
Monte Carlo simulations with guiding wave
functions. Phys. Rev. B, 100:214303, Dec
2019. doi:10.1103/PhysRevB.100.214303.

[48] T. Felser, S. Notarnicola, and S. Mon-
tangero. Eﬃcient tensor network ansatz
for high-dimensional quantum many-body
problems. Phys. Rev. Lett., 126:170603,
Apr 2021.
doi:10.1103/PhysRevLett.
126.170603.

[49] T. Jones, A. Brown, I. Bush, and S. C. Ben-
jamin. QuEST and high performance sim-
ulation of quantum computers. Sci. Rep.,

[56] G. Torlai, B. Timar, E. P. L. van Nieuwen-
burg, H. Levine, A. Omran, A. Keesling,
H. Bernien, M. Greiner, V. Vuleti´c, M. D.
Lukin, R. G. Melko, and M. Endres.
In-
tegrating neural networks with a quantum
simulator for state reconstruction. Phys.
Rev. Lett., 123:230504, Dec 2019.
doi:
10.1103/PhysRevLett.123.230504.

[57] G. Torlai, G. Mazzola, G. Carleo, and
A. Mezzacapo.
Precise measurement of
quantum observables with neural-network
estimators. Phys. Rev. Res., 2:022060, Jun
2020.
doi:10.1103/PhysRevResearch.2.
022060.

17

[58] H.-Y. Huang, R. Kueng, and J. Preskill.
Predicting many properties of a quantum
system from very few measurements. Nat.
Phys., 16(10):1050–1057, 2020. doi:https:
//doi.org/10.1038/s41567-020-0932-7.

18

