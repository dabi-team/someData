1
2
0
2

y
a
M
6
1

]

G
L
.
s
c
[

1
v
1
7
3
7
0
.
5
0
1
2
:
v
i
X
r
a

Expressive Explanations of DNNs by Combining
Concept Analysis with ILP

Johannes Rabold (cid:0)1[0000−0003−0656−5881], Gesina
Schwalbe1,2[0000−0003−2690−2478], and Ute Schmid1[0000−0002−1301−0326]

1 Cognitive Systems, University of Bamberg, Germany
{forename.lastname}@uni-bamberg.de
2 Holistic Engineering and Technologies, Artiﬁcial Intelligence
Continental AG, Regensburg, Germany
{forename.lastname}@continental-corporation.com

Abstract. Explainable AI has emerged to be a key component for black-
box machine learning approaches in domains with a high demand for
reliability or transparency. Examples are medical assistant systems, and
applications concerned with the General Data Protection Regulation of
the European Union, which features transparency as a cornerstone. Such
demands require the ability to audit the rationale behind a classiﬁer’s
decision. While visualizations are the de facto standard of explanations,
they come short in terms of expressiveness in many ways: They cannot
distinguish between diﬀerent attribute manifestations of visual features
(e.g. eye open vs. closed), and they cannot accurately describe the inﬂu-
ence of absence of, and relations between features. An alternative would
be more expressive symbolic surrogate models. However, these require
symbolic inputs, which are not readily available in most computer vi-
sion tasks. In this paper we investigate how to overcome this: We use
inherent features learned by the network to build a global, expressive,
verbal explanation of the rationale of a feed-forward convolutional deep
neural network (DNN). The semantics of the features are mined by a con-
cept analysis approach trained on a set of human understandable visual
concepts. The explanation is found by an Inductive Logic Programming
(ILP) method and presented as ﬁrst-order rules. We show that our ex-
planation is faithful to the original black-box model3.

Keywords: Explainable AI · Concept Analysis · Concept Embeddings
· Inductive Logic Programming

1

Introduction

Machine learning went through several changes of research perspective since its
beginnings more than ﬁfty years ago. Initially, machine learning algorithms were
inspired by human learning [14]. Inductive Logic Programming (ILP) [17] and

3 The code for our experiments is available at

https://github.com/mc-lovin-mlem/concept-embeddings-and-ilp/tree/ki2020

 
 
 
 
 
 
2

J. Rabold et al.

explanation-based generalization [16] were introduced as integrated approaches
which combine reasoning in ﬁrst-order logic and inductive learning.

With the rise of statistical approaches to machine learning, focus shifted
from human-like learning to optimizing learning for high predictive accuracy.
Deep learning architectures [7] resulted in data-intensive, black-box approaches
with impressive performances in domains such as object recognition, machine
translation, and game playing. However, since machine learning more and more is
moving from the lab to the real world, researchers and practitioners alike realize
that interpretable, human-like approaches to machine learning are necessary
to allow developers as well as end-users to evaluate and understand classiﬁer
decisions or possibly also the learned models themselves.

Consequently there is a growing number of approaches to support explain-
ability of black-box machine learning [1]. Explainable AI (XAI) approaches are
proposed to support developers to recognize oversampling and problems with
data quality such as number of available data, class imbalance, expensive label-
ing, and sampling biases [13,2]. For many application domains, it is a legal as
well as an ethical obligation to make classiﬁer decisions transparent and com-
prehensible to end-users who need to make sense of complex information, for
instance in medical diagnosis, automotive safety, or quality control.

A main focus of research on explanations for image classiﬁcations is on visual
explanations, that is, highlighting of relevant pixels such as LRP [23] or showing
relevant areas in the image such as LIME [21]. However, visual explanations can
only show which conjunction of information in an image is relevant. In many
domains, more sophisticated information needs to be taken into account [24]:

– Feature values: highlighting the area of the eye in an image is not helpful to
understand that it is important for the class decision that the lids are tight-
ened (indicating pain) in contrast to eyes which are wide open (indicating
startle, [29]);

– Quantiﬁcation: highlighting all blowholes on the supporting parts of a rim
does not make clear that the rim is not a reject because all blowholes are
smaller than 0,5 mm;

– Negation: highlighting the ﬂower in the hand of a person does not transport
the information that this person is not a terrorist because he or she does not
hold a weapon;

– Relations: highlighting all windows in a building cannot help to discrimi-
nate between a tower, where windows are above each other and a bungalow,
where windows are beside each other [19];

– Recursion: highlighting all stones within a circle of stones cannot transport
the information that there must be a sequence of an arbitrary number of
stones with increasing size [20].

Such information can only be expressed in an expressive language, for in-
stance some subset of ﬁrst-order logic [18]. In previous work, it has been shown
how ILP can be applied to replace the simple linear model agnostic explanations
of LIME [3,20,19,25]. Alternatively, it is investigated how knowledge can be in-
corporated into deep networks. For example, capsule networks [22] are proposed

Expressive Explanations of DNNs by Combining Concept Analysis with ILP

3

to model hierarchical relationships and embeddings of knowledge graphs allow
to grasp relationships between entities [8].

In this paper, we investigate how symbolic knowledge can be extracted from
the inner layers of a deep convolutional neural network to uncover and extract
relational information to build an expressive global explanation for the network.
In the following, we ﬁrst introduce concept embedding analysis (to extract visual
concepts) and ILP (to build the explanation). In section 3, the proposed approach
to model-inherent generation of symbolic relational explanations is presented.
We present a variety of experiments on a new “Picasso” data set of faces with
permuted positions of sub-parts such as eyes, mouth, and nose. We conclude
with an outlook to extend this ﬁrst, preliminary investigation in the future.

2 Theoretical Background

2.1 Concept Embedding Analysis

To understand the process ﬂow of an algorithm, it is of great value to have access
to interpretable intermediate outputs. The goal of concept embedding analysis
is to answer whether, how well, how, and with what contribution to the reasoning
information about semantic concepts is embedded into the latent spaces (inter-
mediate outputs) of DNNs, and to provide the result in an explainable way.
Focus currently lies on ﬁnding embeddings in either the complete output of a
layer (image-level concepts), or single pixels of an activation map of a convo-
lutional DNN (concept segmentation). To answer the whether, one can try to
ﬁnd a decoder for the information about the concept of interest, the concept
embedding. This means, one is looking for a classiﬁer on the latent space that
can predict the presence of the concept. The performance of the classiﬁer pro-
vides a measure of how well the concept is embedded. For an explainable answer
of how a concept is embedded, the decoder should be easily interpretable. One
constraint to this is introduced by the rich vector space structure of the space of
semantic concepts respectively word vector spaces [15]: The decoder map from
latent to semantic space should preserve at least a similarity measure. For ex-
ample, the encodings of “cat” and “dog” should be quite similar, whereas that of
a “car” should be relatively distant from the two. The methods in literature can
essentially be grouped by their choice of distance measure (cid:104)−, −(cid:105) used on the
latent vector space. A concept embedding classiﬁer Ec predicting the presence
of concept c in the latent space L then is of the form Ec(v) = (cid:104)vc, v(cid:105) > tc for
v ∈ L, where vc ∈ L is the concept vector of the embedding, and tc ∈ R.

Automated concept explanations [6] uses L2 distance as similarity measure.
They discover concepts in an unsupervised fashion by k-means clustering of the
latent space representations of input samples. The concept vectors of the dis-
covered concepts are the cluster centers. In TCAV [11] it is claimed that the
mapping from semantic to latent space should be linear for best interpretability.
To achieve this, they suggest to use linear classiﬁers as concept embeddings. This
means they try to ﬁnd a separation hyperplane between the latent space repre-
sentations of positive and negative samples of the concept. A normal vector of

4

J. Rabold et al.

the hyperplane then is their concept vector, and the distance to the hyperplane is
used as distance measure. As method to obtain the embedding they use support
vector machines (SVMs). TCAV further investigated the contribution of con-
cepts to given output classes by sensitivity analysis. A very similar approach to
TCAV, only instead relying on logistic regression, is followed by Net2Vec [5]. As
a regularization, they add a ﬁlter-speciﬁc cut-oﬀ before the concept embedding
analysis to remove noisy small activations. The advantage of Net2Vec over the
SVMs in TCAV is that they can more easily be used in a convolutional setting:
They used a 1×1-convolution to do a prediction of the concept for each acti-
vation map pixel, providing a segmentation of the concept. This was extended
by [26], who suggested to allow larger convolution windows to ensure that the
receptive ﬁeld of the window can cover the complete concept. This avoids a focus
on local patterns. A measure that can be applied to concept vectors of the same
layer regardless of the analysis method, is that of completeness suggested in [31].
They try to measure, how much of the information relevant to the ﬁnal output
of the DNN is covered by a chosen set of concepts vectors. They also suggested
a metric to compare the attribution of each concept to the completeness score
of a set of concepts.

2.2

Inductive Logic Programming

Inductive Logic Programming (ILP) [17] is a machine learning technique that
builds a logic theory over positive and negative examples (E+, E−). The exam-
ples consist of symbolic background knowledge (BK) in the form of ﬁrst-order
logic predicates, e.g. contains(Example, Part), isa(Part, nose). Here the up-
per case symbols are variables and the lower case symbol is a constant. The
given BK describes that example Example contains a part Part which is a nose.
Based on the examples, a logic theory can be learned. The hypothesis language
of this theory consists of logic Horn clauses that contain predicates from the BK.
We write the Horn clauses as implication rules, e.g.

face(Example) :- contains(Example, Part), isa(Part, nose) .

For this work we obey the syntactic rules of the Prolog programming language.
The :- denotes the logic implication (←). We call the part before the implication
the head of a rule and the part after it the body or preconditions of a rule.

We use the framework Aleph [28] for this work since it is a ﬂexible and
adaptive general purpose ILP toolbox. Aleph’s built in algorithm attempts to
induce a logic theory from the given BK to cover as many positive examples
E+ as possible while avoiding covering the negative examples E−. The general
algorithm of Aleph can be summarized as follows [28]:

1. As long as positive examples exist, select one. Otherwise halt.
2. Construct the most-speciﬁc clause that entails the selected example and is

within the language constraints.

3. Find a more general clause which is a subset of the current literals in the

clause.

4. Remove examples covered by the current clause.
5. Repeat from step 1.

Expressive Explanations of DNNs by Combining Concept Analysis with ILP

5

3 Explaining a DNN with Concept Localization and ILP

When building explanations for a DNN via approximate rule sets, the underly-
ing logic and predicates of the rules should reﬂect the capabilities of the model.
For example, spatial relations like top_of or right_of should be covered, as e.g.
dense layers of a DNN are capable of encoding these. Spatial relations cannot
be represented by current visualization methods for explainable AI, which only
feature predicates of the form contains(Example, Part) and at_position(Part,
xy). Rule-based methods like ILP are able to incorporate richer predicates into
the output. However, their input must be symbolic background knowledge about
the training and inference samples which is formulated using these predicates
explicitly. For computer vision tasks with pixel-level input, this encoding of the
background knowledge about samples is not available. To remedy this, we pro-
pose to use existing concept mining techniques for extraction of the required
background knowledge:

1. Associate pre-deﬁned visual semantic concepts with intermediate output of
the DNN. Concepts can be local, like parts and textures, or image-level.
2. Automatically infer the background knowledge about a sample Ex given the
additional concept output, which deﬁnes predicates isa(C, concept), and
isa(Ex, C) (image-level) or contains(Ex, C) with at_position(C, xy). From
this, spatial relations and negations can be extracted.

3. Given background knowledge for a set of training samples, apply an inductive
logic programming approach to learn an expressive set of rules for the DNN.

The approach presented in this paper diﬀers from the previous work outlined

in [19] by the following main aspects:

– We will ﬁnd a global verbal explanation for a black-box decision in contrast

to a local explanation.

– We directly make use of information stored in the building blocks of the
DNN instead of relying on the linear surrogate model generated by LIME.

3.1 Enrich DNN Output via Concept Embedding Analysis

We directly built upon the concept detection approach from [5,26], suggesting
some further improvements. Net2Vec bilinearly upscaled the predicted masks
before applying the sigmoid for logistic regression. This overrates the contribu-
tion to the loss by pixels at the edges from positive to negative predicted pixels.
We instead apply upscaling after applying the sigmoid. Instead of the suggested
IoU penalty from [26], we propose a more stable Dice loss to ﬁt the overlap
objective, supported by a small summand of the balanced binary cross-entropy
(bBCE) suggested in Net2Vec to ensure pixel-wise accuracy.

One major disadvantage of the linear model approaches over the clustering
ones is their instability, i.e. several runs for the same concept yield diﬀerent
concept vectors. Reasons may be dependence on the outliers of the concept
cluster (SVM); non-unique solutions due to a margin between the clusters; and

6

J. Rabold et al.

inherent variance of the used optimization methods. To decrease dependence on
the training set selection and ordering, and the initialization values, we for now
simply use ensembling. For this we deﬁne a hyperplane H as the zero set of the
distance function dH (v) = (v − bH · vH ) ◦ vH for the normal vector vH and the
support vector bH vH , bH ∈ R. Then, the zero set of the mean 1
i=1 dHi of
N
the distance functions of hyperplanes Hi again deﬁnes a hyperplane with

(cid:80)N

vH = 1
N

(cid:80)N

i=1 vHi

and

bH = 1

(cid:107)vH (cid:107)2

1
N

(cid:80)N

i=1(bHi(cid:107)vHi(cid:107)2) .

Note, that hyperplanes with longer normal vectors (i.e. higher conﬁdence values)
are overrated in this calculation. To remedy this, concept vectors are normalized
before ensembling, using the property (w − b · v) ◦ v = (cid:107)v(cid:107) · (w − (b(cid:107)v(cid:107)) v
(cid:107)v(cid:107) ) · v
of the distance function for scalar b and vectors v, w.

(cid:107)v(cid:107)

3.2 Automatic Generation of Symbolic Background Knowledge

The output of the concept analysis step (binary masks indicating the spatial lo-
cation of semantic concepts) can be used to build a symbolic global explanation
for the behavior of the original black-box model. We obtain the explanation by
ﬁnding a ﬁrst-order logic theory with the ILP approach Aleph (see Section 2.2).
Since Aleph needs a set of positive and negative examples (E+, E−), the ﬁrst
step is to obtain these examples along with their corresponding symbolic back-
ground knowledge (BK). In order to obtain a good approximation of the behavior
of the model, we sample N + binary masks from positively predicted images and
N − binary masks from negatively predicted images that lie close to the decision
boundary of the original black-box model using the concept analysis model de-
scribed above. Let M +, M − be the set of positive and negative binary masks.
Let m+ ∈ M +, m− ∈ M − be single masks. Each mask (e.g. m+) consists of mul-
tiple mask layers (e.g. lc ∈ m+, c ∈ C) for the diﬀerent human understandable
concepts from the pool of concepts C. These mask layers are sparse matrices up-
sampled to the same size as the original images they are masking. The matrices
have the value 1 at all the positions where the concept analysis model detected
the respective concept and 0 at all other positions.

The symbolic explanation of the original model should consist of logic rules
that establish the prototypical constellation of visual parts of an image that
resembles the positive class as seen by the DNN. We therefore need not only the
information about occurrence of certain visual parts in the sampled examples
but also the diﬀerent relations that hold between the parts. In the next sections
we adhere to the following general workﬂow:

1. Find positions of visual parts in the examples and name them.
2. Find relations between parts.
3. Build BK with the information from step 1 and 2.
4. Induce a logic theory with Aleph.

Expressive Explanations of DNNs by Combining Concept Analysis with ILP

7

Fig. 1. Potential positions of an object to be only top of, right of, bottom of, or left of
a reference object located in the origin. Also the four overlapping regions are indicated.

Find Visual Parts One mask layer lc contains possibly multiple contiguous
clusters of concept propositions. Therefore, as a denoising step, we only take
the cluster with the largest area into account. As a proposition for the position
of the concept c in the picture, in this cluster we take the mean point of the
area Amax of 1’s in lc. We therefore ﬁnd the position pc = (x, y) with x =
(minx(Amax) + maxx(Amax))/2 and y = (miny(Amax) + maxy(Amax))/2. This
procedure can be followed for all masks lc that are contained in all m+ ∈ M +
and m− ∈ M −.

Find Relations Between Parts By taking relationships between the parts
into account, we strive for more expressive explanations. For this work we limit
ourselves to spatial relationships that hold between the parts that were found
in the previous steps. We assume that pairs of two parts can be in the following
four relationships to each other: left_of, right_of, top_of, bottom_of. We declare
part A to be top_of part B if the vertical component yA of the position pA is above
yB and the value for the horizontal oﬀset ∆x = xA − xB does not diverge from
the value for the vertical oﬀset ∆y = yA − yB by more than double. The other
spatial relations can be formalized in an analogous manner. Thus, the relations
that can hold between two parts A and B can be visualized as in Fig. 1.

Inferring Global Symbolic Explanations After the inference of visual parts
and the relationships that hold between them, we can build the BK needed for
Aleph. Part aﬃliation to an example can be declared by the contains predicate.
We give all parts a unique name over all examples. Suppose part A is part of a
particular example E and describes the human understandable concept c ∈ C.
Then the example aﬃliation can be stated by the predicates contains(E, A),
isa(A, c). Likewise for the relations we can use 2-ary predicates that state the
constellation that holds between the parts. When part A is left of part B we
incorporate the predicate left_of(A, B) in the BK and likewise for the other
relations. When the BK for the positive and negative examples E+ and E− is
found, we can use Aleph’s induction mechanism to ﬁnd the set of rules that best
ﬁt the examples. The complete algorithm for the process is stated in Algorithm 1.

8

J. Rabold et al.

Algorithm 1 Verbal Explanation Generation for DNNs
1: Require: Positive and negative binary masks M +, M −
2: Require: Pool of human understandable concepts C
3: E+ ← {}
4: E− ← {}
5: for each (cid:12) ∈ {+, −} do
6:
for each m(cid:12) ∈ M (cid:12) do
7:
8:
9:
10:
11:
12:
13: T ← Aleph(E+, E−)
14: return T

P ← {}
for each lc ∈ m(cid:12) where c ∈ C do
pc ← calculateP artP osition(lc)
P ← P ∪ {(cid:104)c, pc(cid:105)}

R ← calculateRelations(P )
E(cid:12) ← E(cid:12) ∪ (cid:104)P, R(cid:105)

4 Experiments and Results

We conducted a variety of experiments to audit our previously described ap-
proach. As a running example we used a DNN which we trained on images from
a generated dataset we dubbed Picasso Dataset. The foundation are images of
human faces we took from the FASSEG dataset [9,10]. The Picasso Dataset con-
tains collage images of faces with the facial features (eyes, mouth, nose) either in
the correct constellation (positive class) or in a mixed-up constellation (negative
class). See Fig. 2 for examples. No distinction is made between originally left
and right eyes.

Fig. 2. Examples from the Picasso Dataset (left: positive class, right: negative).

In order to not establish a divergence in the image space of the two classes,
the positive and negative classes contain facial features that were cut out of a set
of images from the original FASSEG dataset. As a canvas to include the features,
we took a set of original faces and got rid of the facial features by giving the
complete facial area a similar skin-like texture. Then we included the cut out
facial features onto the original positions of the original features in the faces.

The face images in Fig. 2 show that the resulting dataset is rather con-
structed. This however will suﬃce for a proof of concept to show that our ap-

Expressive Explanations of DNNs by Combining Concept Analysis with ILP

9

proach in fact exploits object parts and their relations. In the future we plan on
moving towards more natural datasets.

4.1 Analyzed DNNs

We evaluated our method on three diﬀerent architectures from the pytorch
model-zoo4: AlexNet [12], VGG16 [27], and ResNeXt-50 [30]. The convolutional
parts of the networks were initialized with weights pre-trained on the ImageNet
dataset. For ﬁne-tuning the DNNs for the Picasso Dataset task, the output di-
mension was reduced to one and the in- and output dimension of the second to
last hidden dense layer was reduced to 512 for AlexNet and VGG16. Then the
dense layers and the last two convolutional layers (AlexNet, VGG16) respectively
bottleneck blocks (ResNeXt) were ﬁne-tuned. The ﬁne-tuning was conducted in
one epoch on a training set of 18,002 generated, 224 × 224-sized picasso sam-
ples with equal distribution of positive and negative class. All models achieved
accuracy greater than 99% on a test set of 999 positive and 999 negative samples.

4.2 Training the Concept Models

In our example we determined the best ensembled detection concept vectors for
the concepts eyes, mouth and nose amongst the considered layers. We ex-
cluded layers with low receptive ﬁeld, as they are assumed to hold only very
local features (for the layers used see Fig. 3). Convolutional output was only
considered after the activation. For each concept, 452 training/validation, and

Fig. 3. The layer-wise mean set IoU results of the concept analysis runs.

48 test picasso samples with segmentation masks were used. The training ob-
jective was: Predict at each activation map pixel whether the kernel window
centered there lies “over” an instance of the concept. Over meant that the fuzzy
intersection of the concept segmentation and the kernel window area exceeds a
threshold (intersection encoding). This fuzzy deﬁnition of a box center tackles
the problem of sub-optimal intersections in later layers due to low resolution.
Too high values may lead to elimination of an instance, and thresholds were

4 https://pytorch.org/docs/stable/torchvision/models.html

234conv layer / block index0.00.10.20.3set IoUAlexNetNOSEMOUTHEYES45678910111213141516conv layer / block indexResNeXtNOSEMOUTHEYES5678910111213conv layer / block indexVGG16NOSEMOUTHEYES10

J. Rabold et al.

chosen to avoid such issues with values 0.5/0.8/0.7 for nose/mouth/eye. We im-
plemented the encoding via a convolution. As evaluation metric we use set IoU
(sIoU) between the detection masks and the intersection encoded masks as in
Net2Vec. On each dataset and each layer, 15 concept models were trained in
three 5-fold-cross-validation runs with the following settings: Adam optimiza-
tion with mean best learning rate of 0.001, a weighting of 5:1 of Dice to bBCE
loss, batch size of 8, and two epochs (all layers showed quick convergence).

Results Our normalized ensembling approach proved valuable as it yielded mean
or slightly better performance compared to the single runs. For the considered
models, meaningful embeddings of all concepts could be found (see Tab. 1): The
layers all reached sIoU values greater than 0.22 despite of the still seemingly high
inﬂuence of sub-optimal resolutions of the activation maps. Fig. 4 shows some
exemplary outputs. The concepts were best embedded in earlier layers, while
diﬀerent concepts did not necessarily share the same layer.

Table 1. Results for ensemble embeddings with set IoU (sIoU), mean cosine distance
to the runs (Cos.d.), and index of conv layer or block (L) (cf. Fig. 3).

t
e
N
x
e
l
A

L sIoU Cos.d.

nose
2 0.228 0.040
mouth 2 0.239 0.040
2 0.272 0.058
eyes

6
1
G
G
V

L sIoU Cos.d.

nose
7 0.332 0.104
mouth 6 0.296 0.154
6 0.350 0.197
eyes

t
X
e
N
s
e
R

L sIoU Cos.d.

nose
6 0.264 0.017
mouth 5 0.237 0.020
7 0.302 0.020
eyes

Fig. 4. Ensemble embedding outputs of nose (green), mouth (blue), eyes (red).

4.3 Example Selection for ILP Training

The goal of the ILP model is to approximate the behavior of the main DNN, i.e.
its decision boundary. For this, few but meaningful training samples and their

Expressive Explanations of DNNs by Combining Concept Analysis with ILP

11

DNN output are needed: class-prototypes as well as ones that tightly frame the
DNN decision boundary. From the 1,998 samples in the picasso test set, in total
100 samples were chosen from the DNN test set to train the ILP model. The
DNN conﬁdence score here was used to estimate the proximity of a data point
to the decision boundary. For each class, we selected the 50 samples predicted
to be in this class and with conﬁdence closest to the class boundary of 0.5. In
our setup this provided a wide range of conﬁdence values (including 0 and 1).

4.4 Finding the Symbolic Explanation

In order to ﬁnd the background knowledge needed for Aleph to generate the
explanation, we need to extract the information about the facial features and
their constellations from the masks of the samples drawn in the previous step.
Abiding the procedure described in Sec. 3.2, we ﬁrst ﬁnd contiguous clusters
in the mask layers to then infer the positional information for them. This is
straight-forward for the nose and the mouth but imposes a problem for the eyes,
since we do not want to have a single position proposal for them in the eye that
produces the biggest cluster in the mask layer. Thus, we allow for the top two
biggest clusters to infer a position. Although we give them unique constants in
the BK, we both give them the type eye ∈ C.

The next step consists of the extraction of the spatial features between
the found parts. Since the relation pair left_of / right_of as well as top_of /
bottom_of can be seen as the inverses of the respective other relation, we omit the
relations right_of and bottom_of in the BK. This is possible, because the Closed
World Assumption holds (Everything that is not stated explicitly is false).

Once the BK is found for all examples, we can let Aleph induce a theory of

logic rules. Consider the induced theory for the trained VGG16 network:

face(F) :- contains(F, A), isa(A, nose), contains(F, B), isa(B, mouth),

top_of(A, B), contains(F, C), top_of(C, A).

The rule explicitly names the required facial concepts nose and mouth and the
fact that the nose has to be above the mouth in order for an image to be a face.
Further there is another unnamed component C required which has to be placed
above the nose. By construction this has to be one of the eyes. The rule makes
sense intuitively as it describes a subset of correct constellations of the features
of a human face.

To further test the ﬁdelity of the generated explanations to the original black-
box network, we calculated several performance metrics for a test set of 1998
test images (999 positive and 999 negative examples). We handled the learned
explanation rules as binary classiﬁcation model for the test images in BK rep-
resentation. If an image representation is covered by the explanation rules, it
is predicted to be positive, otherwise negative. We now can handle the binary
output of the black-box model as ground truth to our explanation predictions.
The performance metrics together with the induced explanation rules for several
DNN architectures are listed in Tab. 2. It can be seen that the explanations stay
true to the original black-box model.

12

J. Rabold et al.

Table 2. Learned rules for diﬀerent architectures and their ﬁdelity scores (accuracy
and F1 score wrt. to the original model predictions). Learned rules are of common form

face(F) :- contains(F, A), isa(A, nose), contains(F, B), isa(B, mouth), distinctPart

Arch. Accuracy F1

Distinct rule part

VGG16
AlexNet
ResNext

99.60% 99.60%
99.05% 99.04%
99.75% 99.75%

top_of(A, B), contains(F, C), top_of(C, A)

contains(F, C), left_of(C, A), top_of(C, B), top_of(C, A)

top_of(A, B), contains(F, C), top_of(C, A)

5 Conclusion and Future Work

Within the described simple experiment we showed that expressive, verbal sur-
rogate models with high ﬁdelity can be found for DNNs using the developed
methodology. We suggest that the approach is promising and worth future re-
search and optimization.

The proposed concept detection approach requires a concept to have little
variance in its size. It should easily extend to a concept with several size cate-
gories (e.g. close by and far away faces) by merging the result for each category.
A next step for the background knowledge extraction would be to extend it to an
arbitrary number of concept occurences per image, where currently the algorithm
assumes a ﬁxed amount (exactly one mouth, one nose, two eyes). This could e.g.
be achieved by allowing a maximum number per sliding window rather than an
exact amount per image. In cases, where the predicates cannot be pre-deﬁned,
one can learn the relations as functions on the DNN output from examples as
demonstrated in [4].

We further did not consider completeness (cf. Sec. 2.1) of the chosen concepts:
They may not be well aligned with the decision relevant features used by the
DNN, infringing ﬁdelity of the surrogate model. We suggest two ways to remedy
this: One could rely on (possibly less interpretable) concepts found via concept
mining [6]. Or, since ILP is good at rejecting irrelevant information, one can
start with a much larger set of pre-deﬁned, domain related concepts. We further
assume that best ﬁdelity can only be achieved with the minimal complete sub-
set of most decision-relevant concepts, which fosters uniqueness of the solution.
For a decision relevance measure see e.g. [6].

It may be noted that the presented concept analysis approach is not tied
to image classiﬁcation: As long as the ground truth for concepts in the form of
masks or classiﬁcation values is available, the method can be applied to any DNN
latent space (imagine e.g. audio, text, or video classiﬁcation). However, spatial
or temporal positions and relations are currently inferred using the receptive
ﬁeld information of convolutional DNNs. This restriction may again be resolved
by learning of relations.

Lastly, in order to examine the understandability of the induced explanation
in a real world scenario, we need to let explanations be evaluated in a human
user study. For this matter, subjective evaluation measures have to be speciﬁcally
designed for verbal explanations.

Expressive Explanations of DNNs by Combining Concept Analysis with ILP

13

References

1. Adadi, A., Berrada, M.: Peeking inside the black-box: A survey on explainable

artiﬁcial intelligence (XAI). IEEE Access 6, 52138–52160 (2018)

2. Arya, V., Bellamy, R.K., Chen, P.Y., Dhurandhar, A., Hind, M., Hoﬀman, S.C.,
Houde, S., Liao, Q.V., Luss, R., Mojsilović, A., et al.: One explanation does not
ﬁt all: A toolkit and taxonomy of ai explainability techniques. CoRR (2019), http:
//arxiv.org/abs/1909.03012

3. Dai, W.Z., Xu, Q., Yu, Y., Zhou, Z.H.: Bridging machine learning and logical
reasoning by abductive learning. In: Advances in Neural Information Processing
Systems. pp. 2811–2822 (2019)

4. Donadello, I., Seraﬁni, L., d’Avila Garcez, A.S.: Logic Tensor Networks for Seman-
tic Image Interpretation. In: Proc. 26th Int. Joint Conf. Artiﬁcial Intelligence. pp.
1596–1602. ijcai.org (2017). https://doi.org/10.24963/ijcai.2017/221

5. Fong, R., Vedaldi, A.: Net2Vec: Quantifying and explaining how concepts
are encoded by ﬁlters
In: Proc. 2018 IEEE
in deep neural networks.
Conf. Comput. Vision and Pattern Recognition. pp. 8730–8738. IEEE (2018).
https://doi.org/10.1109/CVPR.2018.00910

6. Ghorbani, A., Wexler,

J., Zou,

concept-based explanations.
cessing Systems
9126-towards-automatic-concept-based-explanations

In: Advances

9273–9282

32. pp.

J.Y., Kim, B.: Towards
in Neural

automatic
Information Pro-
(2019), http://papers.nips.cc/paper/

7. Goodfellow, I., Bengio, Y., Courville, A.: Deep learning. MIT press (2016)
8. Ji, G., He, S., Xu, L., Liu, K., Zhao, J.: Knowledge graph embedding via dynamic
mapping matrix. In: Proc. 53rd Ann. Meeting Association for Computational Lin-
guistics and 7th Int. Joint Conf. Natural Language Processing (Vol. 1: Long Pa-
pers). pp. 687–696 (2015)

9. Khan, K., Mauro, M., Leonardi, R.: Multi-class semantic segmentation of faces. In:
Proc. 2015 IEEE Int. Conf. Image Processing (ICIP). pp. 827–831. IEEE (2015)

10. Khan, K., Mauro, M., Migliorati, P., Leonardi, R.: Head pose estimation through
multi-class face segmentation. In: Proc. 2017 IEEE Int. Conf. Multimedia and Expo
(ICME). pp. 175–180. IEEE (2017)

11. Kim, B., Wattenberg, M., Gilmer, J., Cai, C., Wexler, J., Viegas, F., Sayres,
R.: Interpretability beyond feature attribution: Quantitative testing with con-
cept activation vectors (TCAV). In: Proc. 35th Int. Conf. Machine Learning. Pro-
ceedings of Machine Learning Research, vol. 80, pp. 2668–2677. PMLR (2018),
http://proceedings.mlr.press/v80/kim18d.html

12. Krizhevsky, A.: One weird trick for parallelizing convolutional neural networks.

CoRR (2014), http://arxiv.org/abs/1404.5997

13. Lapuschkin, S., Wäldchen, S., Binder, A., Montavon, G., Samek, W., Müller, K.R.:
Unmasking clever hans predictors and assessing what machines really learn. Nature
communications 10(1), 1–8 (2019)

14. Michalski, R.S., Carbonell, J.G., Mitchell, T.M. (eds.): Machine Learning – An

Artiﬁcial Intelligence Approach. Tioga (1983)

15. Mikolov, T., Yih, W.t., Zweig, G.: Linguistic regularities in continuous space word
representations. In: Proc. 2013 Conf. North American Chapter Association for
Computational Linguistics: Human Language Technologies. pp. 746–751. Associ-
ation for Computational Linguistics (2013), https://www.aclweb.org/anthology/
N13-1090

14

J. Rabold et al.

16. Mitchell, T.M., Keller, R.M., Kedar-Cabelli, S.T.: Explanation-based generaliza-

tion: A unifying view. Machine learning 1(1), 47–80 (1986)

17. Muggleton, S.: Inductive logic programming. New generation computing 8(4), 295–

318 (1991)

18. Muggleton, S., Schmid, U., Zeller, C., Tamaddoni-Nezhad, A., Besold, T.: Ultra-
strong machine learning: Comprehensibility of programs learned with ilp. Machine
Learning 107(7), 1119–1140 (2018)

19. Rabold, J., Deininger, H., Siebers, M., Schmid, U.: Enriching visual with verbal
explanations for relational concepts–combining lime with aleph. arXiv preprint
arXiv:1910.01837 (2019)

20. Rabold, J., Siebers, M., Schmid, U.: Explaining black-box classiﬁers with ilp: Em-
powering lime with aleph to approximate non-linear decisions with relational rules.
In: 2018 Int. Conf. Inductive Logic Programming. pp. 105–117. Springer (2018)
21. Ribeiro, M.T., Singh, S., Guestrin, C.: Why should i trust you?: Explaining the
predictions of any classiﬁer. In: Proc. 22nd ACM SIGKDD Int. Conf. Knowledge
Discovery and Data Mining. pp. 1135–1144. ACM (2016)

22. Sabour, S., Frosst, N., Hinton, G.E.: Dynamic routing between capsules. In: Ad-

vances in neural information processing systems. pp. 3856–3866 (2017)

23. Samek, W., Wiegand, T., Müller, K.R.: Explainable artiﬁcial intelligence: Un-
derstanding, visualizing and interpreting deep learning models. CoRR (2017),
http://arxiv.org/abs/1708.08296

24. Schmid, U.: Inductive programming as approach to comprehensible machine learn-
ing. In: Proc. 6th Workshop KI & Kognition (KIK-2018), co-located with KI 2018
(2018), http://ceur-ws.org/Vol-2194/schmid.pdf

25. Schmid, U., Finzel, B.: Mutual explanations for cooperative decision making in
medicine. KI – Künstliche Intelligenz, Special Issue Challenges in Interactive Ma-
chine Learning 34 (2020)

26. Schwalbe, G., Schels, M.: Concept enforcement and modularization as meth-
ods for the ISO 26262 safety argumentation of neural networks. In: Proc. 10th
European Congress Embedded Real Time Software and Systems (2020), https:
//hal.archives-ouvertes.fr/hal-02442796

27. Simonyan, K., Zisserman, A.: Very deep convolutional networks for large-scale
image recognition. In: Proc. 3rd Int. Conf. Learning Representations (2015), http:
//arxiv.org/abs/1409.1556

28. Srinivasan, A.: The Aleph manual (2004), https://www.cs.ox.ac.uk/activities/

programinduction/Aleph

29. Weitz, K., Hassan, T., Schmid, U., Garbas, J.U.: Deep-learned faces of pain and
emotions: Elucidating the diﬀerences of facial expressions with the help of explain-
able ai methods. tm-Technisches Messen 86(7-8), 404–412 (2019)

30. Xie, S., Girshick, R.B., Dollár, P., Tu, Z., He, K.: Aggregated resid-
ual transformations for deep neural networks. In: Proc. 2017 IEEE Conf.
Comput. Vision and Pattern Recognition. pp. 5987–5995.
IEEE (2017).
https://doi.org/10.1109/CVPR.2017.634

31. Yeh, C.K., Kim, B., Arik, S.O., Li, C.L., Pﬁster, T., Ravikumar, P.: On
completeness-aware concept-based explanations in deep neural networks. CoRR
(2020), http://arxiv.org/abs/1910.07969

