Empirical Bounds on Linear Regions of Deep Rectiﬁer Networks

Thiago Serra,1 Srikumar Ramalingam2
1Bucknell University, USA
2The University of Utah, USA
thiago.serra@bucknell.edu, srikumar@cs.utah.edu

9
1
0
2

c
e
D
4
1

]

G
L
.
s
c
[

3
v
0
7
3
3
0
.
0
1
8
1
:
v
i
X
r
a

Abstract

We can compare the expressiveness of neural networks that
use rectiﬁed linear units (ReLUs) by the number of linear re-
gions, which reﬂect the number of pieces of the piecewise
linear functions modeled by such networks. However, enu-
merating these regions is prohibitive and the known analyti-
cal bounds are identical for networks with same dimensions.
In this work, we approximate the number of linear regions
through empirical bounds based on features of the trained
network and probabilistic inference. Our ﬁrst contribution is
a method to sample the activation patterns deﬁned by Re-
LUs using universal hash functions. This method is based
on a Mixed-Integer Linear Programming (MILP) formula-
tion of the network and an algorithm for probabilistic lower
bounds of MILP solution sets that we call MIPBound, which
is considerably faster than exact counting and reaches values
in similar orders of magnitude. Our second contribution is
a tighter activation-based bound for the maximum number of
linear regions, which is particularly stronger in networks with
narrow layers. Combined, these bounds yield a fast proxy for
the number of linear regions of a deep neural network.

1

Introduction

Neural networks with piecewise linear activations have be-
come increasingly more common along the past decade,
in particular since (Nair and Hinton 2010; Glorot, Bor-
des, and Bengio 2011). The simplest and most commonly
used among such forms of activation is the Rectiﬁer Linear
Unit (ReLU), which outputs the maximum between 0 and its
input argument (Hahnloser et al. 2000; LeCun, Bengio, and
Hinton 2015). In the functions modeled by these networks
with ReLUs, we can associate each part of the domain in
which the network corresponds to an afﬁne function with a
particular set of units having positive outputs. We say that
those are the active units for that part of the domain. Con-
sequently, over its entire input domain, the network mod-
els a piecewise-linear function (Arora et al. 2018). Count-
ing these “pieces” into which the domain is split, which are
often denoted as linear regions, is one way to compare the
expressiveness of models deﬁned by networks with differ-
ent conﬁgurations or coefﬁcients. The theoretical analysis of
the number of input regions in deep learning dates back to

at least (Bengio 2009), and recent experiments have shown
that the number of regions relates to the accuracy of similar
networks (Serra, Tjandraatmadja, and Ramalingam 2018).

The study of linear regions in different network conﬁgu-
rations has led to some interesting observations. For exam-
ple, in a rectiﬁer network with n ReLUs, we learned that not
all conﬁgurations – and in some cases none – can reach the
ceiling of 2n regions (the number of possible sets of active
units). On the one hand, we can construct networks where
the number of regions is exponential on network depth (Pas-
canu, Mont´ufar, and Bengio 2014; Mont´ufar et al. 2014). On
the other hand, there is a bottleneck effect by which the num-
ber of active units on each layer affects how the regions are
partitioned by subsequent layers due to the dimension of the
space containing the image of the function, up to the point
that even shallow networks deﬁne more linear regions than
their deeper counterparts as the input dimension approaches
n (Serra, Tjandraatmadja, and Ramalingam 2018). Due to
the linear local behavior, the size and shape of linear regions
have been explored for provable robustness. In that case,
one wants to identify large stable regions and move certain
points away from their boundaries (Wong and Kolter 2018;
Elsayed et al. 2018; Croce, Andriushchenko, and Hein 2019;
Guang-He Lee 2019). To some extent, we may regard the
number and the geometry of linear regions as complemen-
tary. We can also use linear regions of a network to obtain
smaller networks that are equivalent or a global approxima-
tion (Kumar, Serra, and Ramalingam 2019). However, we
need faster methods to count or reasonably approximate the
number of linear regions to make such metric practical.

Linear Regions and Network Expressiveness

The literature on counting linear regions has mainly fo-
cused on bounding their maximum number. Lower bounds
are obtained by constructing networks deﬁning increasingly
larger number of linear regions (Pascanu, Mont´ufar, and
Bengio 2014; Mont´ufar et al. 2014; Arora et al. 2018; Serra,
Tjandraatmadja, and Ramalingam 2018). Upper bounds are
proven using the theory of hyperplane arrangements (Za-
slavsky 1975) along with other analytical insights (Raghu
et al. 2017; Mont´ufar 2017; Serra, Tjandraatmadja, and Ra-
malingam 2018). So far, these bounds are only identical –

 
 
 
 
 
 
and thus tight – in the case of one-dimensional inputs (Serra,
Tjandraatmadja, and Ramalingam 2018). Both of these lines
have explored deepening connections with polyhedral the-
ory, but some of these results have also been recently revis-
ited using tropical algebra (Zhang, Naitzat, and Lim 2018;
Charisopoulos and Maragos 2018). The linear regions of a
trained network can be enumerated as the projection of a
Mixed-Integer Linear Program (MILP) on the binary vari-
ables deﬁning if each unit is active (Serra, Tjandraatmadja,
and Ramalingam 2018). Another recent line of work focuses
on analytical results for the average number of linear regions
in practice (Hanin and Rolnick 2019a; Hanin and Rolnick
2019b).

Other methods to study neural network expressiveness in-
clude universal approximation theory (Cybenko 1989), VC
dimension (Bartlett, Maiorov, and Meir 1998), and trajec-
tory length (Raghu et al. 2017). A network architecture can
be studied and analyzed based on the largest class of func-
tions that it can approximate. For example, it has been shown
that any continuous function can be modeled using a sin-
gle hidden layer of sigmoid activation functions (Cybenko
1989). The popular ResNet architecture (He et al. 2016)
with a single ReLU in every hidden layer can be a universal
approximator (Lin and Jegelka 2018). Furthermore, a rec-
tiﬁer network with a single hidden layer can be trained to
global optimality in polynomial time on the data size, but
exponential on the input dimension (Arora et al. 2018). The
use of trajectory length for expressiveness is related to lin-
ear regions, i.e., by changing the input along a one dimen-
sional path we study the transition across linear regions. Cer-
tain critical network architectures using leaky ReLUs are
identiﬁed to produce connected decision regions (Nguyen,
Mukkamala, and Hein 2018). To avoid such degenerate
cases, one needs to use sufﬁciently wide hidden layers. How-
ever, this result is mainly applicable to leaky ReLUs and not
to standard ReLUs (Beise, Cruz, and Schr¨oder 2018).

Counting and Probabilistic Inference
Approximating the size of a set of binary vectors, such as
those units that are active on each linear region of a neu-
ral network, has been extensively studied in the context of
propositional satisﬁability (SAT). A SAT formula on a set V
of Boolean variables has to satisfy a set of predicates. Count-
ing solutions of SAT formulas is #P-complete (Toda 1989),
but one can approximate the number of solutions by making
a relatively small number of solver calls to restricted for-
mulas. This line of work relies on hash functions with good
statistical properties to partition the set of solutions S into
subsets having approximately half of the solutions each. Af-
ter restricting a given formula r times to either subset, one
can test if the subset is empty. Intuitively, |S| ≥ 2r with
some probability if these subsets are more often nonempty,
or else |S| < 2r. SAT formulas are often restricted with
predicates that encode XOR constraints, which can be inter-
preted in terms of 0–1 variables as restricting the sum of a
subset U ⊂ V of the variables to be either even or odd. XOR
constraints are universal hash functions (Carter and Wegman
1979), which enable approximate counting in polynomial
time with an NP-oracle (Sipser 1983; Stockmeyer 1985). In-

terestingly, formulas with a unique solution are as hard as
those with multiple solutions (Valiant and Vazirani 1986).
From a theoretical standpoint, such approximations are thus
not much harder than obtaining a feasible solution.

In the seminal MBound algorithm (Gomes, Sabharwal,
and Selman 2006a), XOR constraints on sets of variables
with a ﬁxed size k yield the probability that 2r is either a
lower or an upper bound. The probabilistic lower bounds
are always valid but get better as k increases, whereas the
probabilistic upper bounds are only valid if k = |V |/2.
In practice, these lower bounds can be good for small
k (Gomes et al. 2007b). These ideas were later extended
to constraint satisfaction problems (Gomes et al. 2007a).
Some of the subsequent work has been inﬂuenced by uni-
form sampling results from (Gomes, Sabharwal, and Sel-
man 2006b), where the ﬁxed size k is replaced with an
independent probability p of including each variable in
each XOR constraint. That includes the ApproxMC and
the WISH algorithms (Chakraborty, Meel, and Vardi 2013;
Ermon et al. 2013b), which rely on ﬁnding more solu-
tions of the restricted formulas but generate (σ, (cid:15)) certiﬁ-
cates by which, with probability 1 − σ, the result is within
(1 ± (cid:15))|S|. Later work has provided upper bound guarantees
when p < 1/2, showing that the size of those sets can be
Θ(cid:0)log(|V |)(cid:1) (Ermon et al. 2014; Zhao et al. 2016). Others
have tackled this issue differently. One approach limited the
counting to any set of variables I for which any assignment
leads to at most one solution in V , denoting those as minimal
independent supports (Chakraborty, Meel, and Vardi 2014;
Ivrii et al. 2016). Another approach broke with the indepen-
dent probability p by using each variable the same number of
times across r XOR constraints (Achlioptas and Jiang 2015;
Achlioptas, Hammoudeh, and Theodoropoulos 2018). Re-
lated work on MILP has only focused on upper bounds
based on relaxations (Jain, Kadioglu, and Sellmann 2010).

Contributions of This Paper
We propose empirical bounds based on the weight and bias
coefﬁcients of trained networks, which are the ﬁrst able to
compare networks with same conﬁguration of layers. We
also suggest replacing the potential number of linear regions
N of an architecture with the Minimum Activation Pattern
Size (MAPS) η = log2 N . This value can be interpreted as
the number of units that any network should have in order
to deﬁne as many linear regions as another network when
adjacent linear regions map to distinct afﬁne functions.
Our main technical contributions are the following:

(i) We introduce a probabilistic lower bound based on
sampling the activation patterns of the trained net-
work. More generally, we can approximate solutions of
MILP formulations more efﬁciently than if directly ex-
tending SAT-based methods with the MIPBound algo-
rithm introduced in Section 4. See results in Figure 2.

(ii) We reﬁne the best known upper bound by further ex-
ploring how units partition the input space. With the
theory in Section 5, we ﬁnd that unit activity further
contributes to the bottleneck effect caused by narrow
layers (those with few units). See results in Table 1.

2 Preliminaries and Notations
We consider feedforward Deep Neural Networks (DNNs)
with ReLU activations. Each network has n0 input variables
given by x = [x1 x2 . . . xn0 ]T with a bounded domain X
and m output variables given by y = [y1 y2 . . . ym]T . Each
hidden layer l ∈ L = {1, 2, . . . , L} has nl hidden neurons
indexed by i ∈ Nl = {1, 2, . . . , nl} with outputs given by
hl = [hl
]T . For notation simplicity, we may use
h0 for x and hL+1 for y. Let W l be the nl × nl−1 matrix
where each row corresponds to the weights of a neuron of
layer l. Let bl be the bias vector used to obtain the activation
functions of neurons in layer l. The output of unit i in layer
l consists of an afﬁne transformation gl
i to
which we apply the ReLU activation hl

i = W l
i = max{0, gl

i hl−1 + bl

2 . . . hl
nl

1 hl

i}.

We regard the DNN as a piecewise linear function F :
Rn0 → Rm that maps the input x ∈ X ⊂ Rn0 to y ∈ Rm.
Hence, the domain is partitioned into regions within which
F corresponds to an afﬁne function, which we denote as lin-
ear regions. Following the literature convention (Raghu et
al. 2017; Mont´ufar 2017; Serra, Tjandraatmadja, and Rama-
lingam 2018), we characterize each linear region by the set
of units that are active in that domain. For each layer l, let
Sl ⊆ {1, . . . , nl} be the activation set in which i ∈ Sl if
i > 0. Let S = (S1, . . . , Sl) be the activation
and only if hl
pattern aggregating those activation sets. Consequently, the
number of linear regions deﬁned by the DNN is the number
of nonempty sets in x among all possible activation patterns.

3 Counting and MILP Formulations
We can represent each linear region deﬁned by a rectiﬁer
network with n hidden units on domain X by a distinct vec-
tor in {0, 1}n, where each element denotes if a unit is active
or not. Such vector can be embedded into an MILP formu-
lation mapping network inputs to outputs (Serra, Tjandraat-
madja, and Ramalingam 2018). For a neuron i in layer l, we
use such binary variable zi, vector hl−1 of inputs coming
from layer l − 1, variable gl
i for the value of the afﬁne trans-
i = max{0, gl
i hl−1 +bl
formation W l
i} denoting
the output of the unit, and a variable ¯hl
i denoting the output
of a complementary ﬁctitious unit ¯hl

i, variable hl

(cid:9):

W l

i

i = max (cid:8)0, −gl
i hl−1 + bl
i = gl
i
i − ¯hl
gl
i = hl
i
¯hl
i ≤ ¯H l
i (1 − zl
i)
zl
i ∈ {0, 1}

(1)

(2)

(3)

(4)

i ≤ H l
hl

i zl
i,
¯hl
i ≥ 0,

hl
i ≥ 0,

Figure 1: (a) ReLU mapping hl
vex outer approximation on (gl

i = max{0, gl
i, hl

i).

i}; and (b) Con-

i and ¯H l

i and ¯hl

For correctness, constants H l

i should be positive
i can be. In such case, the value of gl
and as large as hl
i
determines if the unit or its ﬁctitious complement is active.
However, constraints (1)–(4) allow zl
i = 0. To
count the number of linear regions, we consider the set of
binary variables in the solutions where all active units have
i > 0 if zl
positive outputs, i.e., hl
i = 1 (Serra, Tjandraat-
madja, and Ramalingam 2018), thereby counting the posi-
tive solutions with respect to f on the binary variables of

i = 1 when gl

max f

s.t. (1) − (4), f ≤ hl

i + (1 − zl

i)H l
i

x ∈ X

(5)
l ∈ L; i ∈ Nl (6)
(7)

The solutions on z can be enumerated using the one-tree al-
gorithm (Danna et al. 2007), in which the branch-and-bound
tree used to obtain the optimal solution of the formulation
above is further expanded to collect near-optimal solutions
up to a given limit. In general, ﬁnding a feasible solution to a
MILP is NP-complete (Cook 1971) and thus optimization is
NP-hard. However, a feasible solution in our case can be ob-
tained from any valid input (Fischetti and Jo 2018). While
that does not directly imply that optimization over neural
networks is easy, it hints at good properties to explore.

MILP formulations have been used for network veriﬁca-
tion (Lomuscio and Maganti 2017; Dutta et al. 2018) and
evaluation of adversarial perturbations (Cheng, N¨uhrenberg,
and Ruess 2017; Fischetti and Jo 2018; Tjeng, Xiao, and
Tedrake 2019; Xiao et al. 2018; Anderson et al. 2019). Other
applications relax the binary variables as continuous vari-
ables in [0, 1] or use the Linear Programming (LP) for-
mulation of a particular linear region (Bastani et al. 2016;
Ehlers 2017; Wong and Kolter 2018), which is deﬁned us-
ing W l
i ≥ 0 for active units and the comple-
ment for inactive units. Although equivalent, these MILP
formulations may differ in strength (Fischetti and Jo 2018;
Tjeng, Xiao, and Tedrake 2019; Huchette 2018; Anderson et
al. 2019). When the binary variables are relaxed, their linear
relaxation may be different. We say that an MILP formula-
tion A is stronger than a formulation B if, when projected
on common sets of variables, the linear relaxation of A is a
subset of the linear relaxation of B. Formulation strength is
often used as a proxy for solver performance.

i hl−1 + bl

i and ¯H l

Differences in strength may be due to smaller values for
constants such as H l
i , additional valid inequalities
to remove fractional values of z, or an extended formulation
with more variables. Let us consider the strength of the for-
mulation for each ReLU activation hl
i}. Ide-
ally, we want the projection on gl
i to be the convex
outer approximation of all possible combined values of those
variables (Wong and Kolter 2018), as illustrated in Figure 1.
Lemma 1. If H l
i = arg maxgl−1{gl
i =
arg maxgl−1 {−gl
i} ≥ 0, then the linear relaxation of (2)–
(4) deﬁnes the convex outer approximation on (gl

i} ≥ 0 and ¯H l

i = max{0, gl

i and hl

i, hl

i).

Lemma 1 shows that the smallest possible values of H l
i
and ¯H l
i are necessary to obtain a stronger formulation. The
proof can be found in Appendix A. A similar claim without
proof is found in (Huchette 2018).

𝑔𝑖𝑙ℎ𝑖𝑙𝐻𝑖𝑙−ഥ𝐻𝑖𝑙𝐻𝑖𝑙(𝑎)𝑔𝑖𝑙ℎ𝑖𝑙(𝑏)i and ¯H l

i and ¯H 1

When X is deﬁned by a box, and thus the domain of each
input variable xi is an independent continuous interval, then
the smallest possible values for H 1
i can be com-
puted with interval arithmetic by taking element-wise max-
ima (Cheng, N¨uhrenberg, and Ruess 2017; Serra, Tjandraat-
madja, and Ramalingam 2018). When extended to subse-
quent layers, this approach may overestimate H l
i as
the maximum value of the outputs are not necessarily inde-
pendent. More generally, if X is polyhedral, we can obtain
the smallest values for these constants by solving a sequence
of MILPs (Fischetti and Jo 2018; Tjeng, Xiao, and Tedrake
2019). We can use H l(cid:48)
i = max {gl(cid:48)
: (1) − (4) ∀l ∈
{1, . . . , l(cid:48) − 1}, i ∈ Nl, x ∈ X}. If H l(cid:48)
i ≤ 0, the unit is
always inactive, denoted as stably inactive, and we can re-
move such units from the formulation. Similarly, we can use
¯H l(cid:48)
i = − min {gl(cid:48)
: (1) − (4) ∀l ∈ {1, . . . , l(cid:48) − 1}, i ∈
i
Nl, x ∈ X}. If ¯H l(cid:48)
i < 0, the unit is always active, denoted as
stably active, and we can simply replace constraints (1)-(4)
with hl
i. In certain large networks, many units are sta-
ble (Tjeng, Xiao, and Tedrake 2019). The remaining units,
where activity depends on the input, are denoted unstable.

i = gl

i

We propose some valid inequalities involving consecutive
layers of the network. For unit i to be active when bl
i ≤ 0,
there must be a positive contribution, and thus some unit j
in layer l − 1 such that W l
ij > 0 is also active. Hence, for
each layer l ∈ {2, . . . , L} and unit i ∈ Nl such that bl
i ≤ 0,

zl
i ≤

(cid:88)

zl−1
j

.

(8)

j∈{1,...,nl−1}:W l

ij >0

Similarly, unit i is only inactive when bl
in layer l − 1 such that W l
layer l ∈ {2, . . . , L} and unit i ∈ Nl such that bl

i > 0 if some unit j
ij < 0 is active. Likewise, for each

i > 0,

(1 − zl

i) ≤

(cid:88)

zl−1
j

.

(9)

j∈{1,...,nl−1}:W l

ij <0
Let us denote unstable units in which bl

i ≤ 0, and thus
(8) applies, as inactive leaning; and those in which bl
i > 0,
and thus (9) applies, as active leaning. Within linear regions
where none among the units of the previous layer in the cor-
responding inequalities is active, these units can be regarded
as stably inactive and stably active, respectively. We will use
the same ideas to obtain better bounds in Section 5.

4 Approximate Lower Bound
We can use approximate model counting to estimate the
size of the set of binary vectors corresponding to the acti-
vation patterns of a neural network. Essentially, we restrict
the set of solutions by iteratively adding constraints with
good sampling properties, such as XOR, until the problem
becomes infeasible. Based on how many constraints it takes
to make the formulation infeasible across many runs, we ob-
tain bounds on the number of solutions with a certain prob-
ability. We describe in Section 1 how this type of approach
has been extensively studied in the SAT literature.

The same ideas have not yet been extended to MILP for-
mulations, where we exploit the speciﬁcity of MILP solvers

to devise a more efﬁcient algorithm. The assumption in SAT-
based approaches is that each restricted formula entails a
new call to the solver. Hence, obtaining a data point for each
number of restrictions takes a linear number of calls. That
has been improved to a logarithmic number of calls by or-
derly applying the same sequence of constraints up to each
number of r of XOR constraints, with which one can apply
binary search to ﬁnd the smallest r that makes the formula
unsatisﬁable (Chakraborty, Meel, and Vardi 2016). In MILP
solvers, we can test for all values of r with a single call to the
solver by generating parity constraints as lazy cuts, which
can be implemented through callbacks. When a new solution
is found, a callback is invoked to generate parity constraints.
Each constraint may or may not remove the solution just
found, since we preserve the independence between the so-
lutions found and the constraints generated, and thus we may
need to generate multiple parity constraints before yielding
the control back to the solver. Algorithm 1 does that based
on MBound (Gomes, Sabharwal, and Selman 2006a).

F (cid:48) ← F
i ← i + 1
r ← 0
while F (cid:48) has some solution s do

Algorithm 1 MIPBound computes the probability of some
lower bounds on the solutions of a formulation F with n
binary variables by adding parity constraints of size k
1: i ← 0
2: for j ← 0 → n do
f [j] ← 0
3:
4: end for
5: while Termination criterion not satisﬁed do
6:
7:
8:
9:
10:
11:
12:
13:
14:
15:
16:
17:
end for
18:
19: end while
20: for j ← 0 → n − 1 do
21:
22:

Generate parity constraint C with k of the variables
F (cid:48) ← F (cid:48) ∩ C
r ← r + 1
until C removes s

end while
for j ← 0 → r − 1 do
f [j] ← f [j] + 1

repeat

δ ← f [j + 1]/i − 1/2
if δ > 0 then
Pj ← 1 −

(cid:16)

e2.δ
(1+2.δ)1+2.δ

(cid:17)i/2

else

23:
24:
25:
26:
27: end for
28: return Probabilities P

break

end if

We denote Algorithm 1 as MIPBound. For each repeti-
tion of the outer while loop, parity constraints are added
to a copy F (cid:48) of the formulation until it becomes infeasible.
Appendix B discusses how to represent parity constraints in
MILP using unit hypercube cuts from (Balas and Jeroslow
1972). The inner while loop corresponds to the solver call
and the block between repeat and until is implemented
as a lazy cut callback, which is invoked when an incumbent

solution s is found. After each solver call, the number of
constraints r to make F (cid:48) infeasible is used to increase f [j]
for all j < r, which counts the number of times that F (cid:48)
remained feasible after adding j constraints. If F often re-
mained feasible with j constraints, we compute the probabil-
ity Pj−1 that |S| > 2j−1, which is explained in Appendix C.

5 Analytical Upper Bound
In order to bound the number of linear regions, we use ac-
tivation hyperplanes and the theory of hyperplane arrange-
i hl−1 +
ments. For each unit, the activation hyperplane W l
i = 0 splits the input space hl−1 into the regions where the
bl
unit is active (W l
i hl−1 +
bl
i ≤ 0). The number of full-dimensional regions deﬁned by
the arrangement of nl hyperplanes in an nl−1-dimensional
(cid:1) (Zaslavsky 1975). See Ap-
space is at most (cid:80)nl−1
j=0
pendix D for related bounds (Raghu et al. 2017; Mont´ufar
2017; Serra, Tjandraatmadja, and Ramalingam 2018).

i > 0) or inactive (W l

i hl−1 + bl

(cid:0)nl
j

We can improve on these previous bounds by leveraging
that some units of the trained network are stable for some or
all possible inputs. First, note that only units in layer l that
can be active in a given linear region produced by layers 1
to l − 1 affect the dimension of the space in which the linear
region can be further partitioned by layers l + 1 to L. Sec-
ond, only the subset of these units that can also be inactive
within that region, i.e., the unstable ones, counts toward the
number of hyperplanes partitioning the linear region at layer
l. Every linear region is contained in the same side of the hy-
perplane deﬁned by each stable unit. Hence, let Al(k) be the
maximum number of units that can be active in layer l if k
units are active in layer l−1; and Il(k) be the corresponding
maximum number of units that are unstable.
Theorem 2. Consider a deep rectiﬁer network with L layers
with input dimension n0 and at most Al(k) active units and
Il(k) unstable units in layer l for every linear region deﬁned
by layers 1 to l − 1 when k units are active in layer l − 1.
Then the maximum number of linear regions is at most

(cid:88)

L
(cid:89)

(j1,...,jL)∈J

l=1

(cid:19)

(cid:18)Il(kl−1)
jl

where J = {(j1, . . . , jL) ∈ ZL : 0 ≤ jl ≤ ζl, ζl =
min{n0, k1, . . . , kl−1, Il(kl−1)} } with k0 = n0 and kl =
Al(kl−1) − jl−1 for l ∈ L.

Proof. We deﬁne a recursive recurrence to bound the num-
ber of subregions within a region. Let R(l, k, d) bound the
maximum number of regions attainable from partitioning a
region with dimension at most d among those deﬁned by
layers 1 to l − 1 in which at most k units are active in
layer l − 1. For the base case l = L, we have R(L, k, d) =
(cid:1) since Il(k) ≤ Al(k). The recurrence
(cid:80)min{IL(k),d}
groups regions with same number of active units in layer l
as R(l, k, d) = (cid:80)Al(k)
j=0 N l
Il(k),d,jR(l + 1, j, min{j, d}) for
l = 1 to L − 1, where N l
p,d,j is the maximum number of re-
gions with j active units in layer l from partitioning a space
of dimension d using p hyperplanes.

(cid:0)IL(k)
j

j=0

j

Note that there are at most (cid:0)Il(k)

(cid:1) regions deﬁned by
layer l when j unstable units are active and there are k
active units in layer l − 1, which can be regarded as the
subsets of Il(k) units of size j. Since layer l deﬁnes at
(cid:1) regions with an input dimension
most (cid:80)min{Il(k),d}
j=0
d and k active units in the layer above, by assuming the
largest number of active hyperplanes among the unstable
units and also using (cid:0) Il(k)
(cid:1), then we deﬁne
Il(k)−j
R(l, k, d) for 1 ≤ l ≤ L − 1 as the following expression:
min{Il(k),d}
(cid:88)

(cid:1) = (cid:0)Il(k)

(cid:0)Il(k)
j

R(l+1, Al(k)−j, min{Al(k)−j, d}).

(cid:19)

j

(cid:18)Il(k)
j

j=0

Without loss of generality, we assume that the input is
generated by n0 active units feeding the network, hence
implying that the bound can be evaluated as R(1, n0, n0):
min{I1(k0),d1}
(cid:19)
(cid:88)

min{IL(kL−1),dL}
(cid:88)

(cid:19)

(cid:18)IL(kL−1)
jL

,

(cid:18)Il(k0)
j1

· · ·

j1=0

jL=0

where k0 = n0 and kl = Al(kl−1)−jl−1 for l ∈ L, whereas
dl = min{n0, k1, . . . , kl−1}. We obtain the ﬁnal expression
by nesting the values of j1, . . . , jL.

l ∪ U −

l and U +

Theorem 2 improves the result in (Serra, Tjandraatmadja,
and Ramalingam 2018) when not all hyperplanes partition
every region from previous layers (Il(kl−1) < nl) or not all
units can be active (smaller intervals for jl due to Al(kl−1)).
Now we discuss how the parameters that we introduced in
this section can be computed exactly, or else approximated.
We ﬁrst bound the value of Il(k). Let U −
l denote
the sets of inactive leaning and active leaning units in layer l,
and Ul = U +
, we can deﬁne
a set J −(l, i) of units from layer l − 1 that, if active, can
potentially make i active. In fact, we can use the set in the
summation of inequality (8), and therefore let J −(l, i) :=
{j : 1 ≤ j ≤ nl−1, W l
, we
can similarly use the set in inequality (9), and let J +(l, i) :=
{j : 1 ≤ j ≤ nl−1, W l
ij < 0}. Conversely, let I(l, j) :=
{i : i ∈ U +
l+1, j ∈
J −(l + 1, i)} be the units in layer l + 1 that may be locally
unstable if unit j in layer l is active.

l+1, j ∈ J +(l + 1, i)} ∪ {i : i ∈ U −

ij > 0}. For a given unit i ∈ U +
l

. For a given unit i ∈ U −
l

l

Proposition 3. Il(k) ≤ max

(cid:12)
(cid:12)
(cid:12)
I(l − 1, j)
(cid:12)
(cid:12)
where Sk = {S : S ⊆ {1, . . . , nl−1}, |S| ≤ k}.

(cid:40)(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:83)
j∈S

S

: S ∈ Sk

(cid:41)
,

Next we bound the value of Al(k) by considering a larger
subset of the units in layer l that only excludes locally inac-
tive units. Let n+
l denote the number of stably active units in
layer l, which is such that n+
l ≤ nl−|Ul|, and let I −(l, j) :=
{i : i ∈ U −
l+1, j ∈ J −(l + 1, i)} be the inactive leaning units
in layer l +1 that can be activated if unit j in layer l is active.
Proposition 4. For Sk deﬁned as before, Al(k) ≤ n+
l +

(cid:12)
(cid:12)
(cid:12)
I −(l − 1, j)
(cid:12)
(cid:12)
In practice, however, we may only need to inspect a small
number of such subsets. In the average-case analysis pre-

l |+ max

: S ∈ Sk

(cid:40)(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:83)
j∈S

|U +

(cid:41)

S

.

sented in Appendix E, only O(nl−1) subsets are needed. We
also observed in the experiments that the minimum value of
k to maximize Il(k) and Al(k) is rather small. If not, we can
approximate Il(k) and Al(k) with strong optimality guaran-
tees (1 − 1
e ) using simple greedy algorithms for submodu-
lar function maximization (Nemhauser, Wolsey, and Fisher
1978). We discuss that possibility in Appendix F.

6 Experiments
We tested on rectiﬁer networks trained on the MNIST
benchmark dataset
(LeCun et al. 1998), consisting of 22
units distributed in two hidden layers and 10 output units,
with 10 distinct networks for each distribution of units be-
tween the hidden layers. See Appendix G for more details
about the networks and the implementation. For each size of
parity constraints k, which we denote as XOR-k, we mea-
i and ¯H l
sure the time to ﬁnd the smallest coefﬁcients H l
i
for each unit along with the subsequent time of MIPBound
(Algorithm 1). We let MIPBound run for enough steps to
obtain a probability of 99.5% in case all tested constraints
of a given size preserve the formulation feasible, and we re-
port the largest lower bound with probability at least 95%.
We also use the approach in (Serra, Tjandraatmadja, and Ra-
malingam 2018) to count the exact number of regions for
benchmarking. Since counting can be faster than sampling
for smaller sets, we deﬁne a DNN with η < 12 as small and
large otherwise. We use Conﬁguration Upper Bound (Con-
ﬁguration UB) for the bound in (Serra, Tjandraatmadja,
and Ramalingam 2018). The upper bound from Theorem 2,
which we denote as Empirical Upper Bound (Empirical
UB), is computed at a small fraction of the time to obtain
coefﬁcients H l
i for the lower bound. We denote as
APP-k the average between the XOR-k Lower Bound (LB)
and Empirical UB.

i and ¯H l

Table 1 shows the gap closed by Empirical UB. Fig-
ure 2 (top) compares the bounds with the number of lin-
ear regions. Figure 2 (bottom) compares the time for exact
counting and approximation. Figure 3 compares APP-k with
the accuracy of networks not having particularly narrow lay-
ers, in which case the number of regions relates to network
accuracy (Serra, Tjandraatmadja, and Ramalingam 2018).

7 Conclusion
This paper introduced methods to obtain tighter bounds on
the number of linear regions. These methods are consid-

Table 1: Gap (%) closed by Empirical UB between Conﬁg-
uration UB and the number of regions for widths n1; n2; n3.

Widths Gap
73.1
1;21;10
17.8
2;20;10
10.4
3;19;10
3.1
4;18;10
3.9
5;17;10
2
6;16;10
1.1
7;15;10

Widths
8;14;10
9;13;10
10;12;10
11;11;10
12;10;10
13;9;10
14;8;10

Gap
0
0
1
0
0
0.1
0.2

Widths Gap
0.5
15;7;10
1
16;6;10
1.8
17;5;10
3.4
18;4;10
9.5
19;3;10
44.5
20;2;10
98.3
21;1;10

30

20

10

η
S
P
A
M

0
1;21;10

103

102

101

)
s
(
n
o
i
t
a
m
i
x
o
r
p
p
a

r
o
f

e
m
T

i

10−1

6;16;10

11;11;10
Neurons in each layer

16;6;10

21;1;10

Conﬁguration UB
Empirical UB
Exact values
XOR-5 LB
XOR-4 LB
XOR-3 LB
XOR-2 LB

XOR-5; Large
XOR-5; Small
XOR-4; Large
XOR-4; Small
XOR-3; Large
XOR-3; Small
XOR-2; Large
XOR-2; Small

100

101

102

105
Time for exact counting (s)

103

104

106

Figure 2: Top: Averages of the Empirical UB and XOR-k
LBs with probability 95% compared with the Conﬁguration
UB and the exact number of regions for 10 networks of each
type. Bottom: Comparison of approximation vs. exact count-
ing times by XOR size and number of regions.

erably faster than direct enumeration, entail a probabilistic
lower bound algorithm to count MILP solutions, and help
understanding how ReLUs partition the input space.

Prior work on bounding the number of linear regions has
ﬁrst focused on the beneﬁt of depth (Pascanu, Mont´ufar, and
Bengio 2014; Mont´ufar et al. 2014), and then on the bottle-
neck effect that is caused by a hidden layer that is too nar-
row (Mont´ufar 2017) and more generally by small activation
sets (Serra, Tjandraatmadja, and Ramalingam 2018). In our
work, we looked further into how many units can possibly
be active or not by taking into account the weights and the
bias of each ReLU, which allows us to identify stable units.
Stable units do not contribute as signiﬁcantly to the number
of linear regions. Consequently, we found that the bottleneck
effect in the upper bound is even stronger in narrow layers,
as evidenced in both extremes of Table 1.

The probabilistic lower bound is based on universal hash-
ing functions to sample activation patterns, and more gen-
erally allows us to estimate the number of solutions on bi-
nary variables of MILP formulations. By exploiting call-
backs that are typical of MILP solvers, the number of solver
calls in the proposed algorithm MIPBound does not depend
on the number of sizes for which we evaluate the proba-
bilistic bounds like in related work. The algorithm is orders
of magnitude faster than exact counting on networks with a
large number of linear regions. These bounds can be param-
eterized for a balance between precision and speed. Never-
theless, we noted that lower bounds from XOR constraints

)

E
C

(

r
o
r
r
e

g
n
i
n
i
a
r
T

)

E
C

(

r
o
r
r
e

g
n
i
n
i
a
r
T

)

E
C

(

r
o
r
r
e

g
n
i
n
i
a
r
T

)

E
C

(

r
o
r
r
e

g
n
i
n
i
a
r
T

0.6

0.4

0.6

0.4

0.6

0.4

0.6

0.4

15

20
MAPS η

25

15

20
MAPS η

25

15

20

25

MAPS η

14

12

10

8

6

14

12

10

8

6

14

12

10

8

6

14

12

10

8

6

)

R
M
%

(

r
o
r
r
e

t
s
e
T

)

R
M
%

(

r
o
r
r
e

t
s
e
T

)

R
M
%

(

r
o
r
r
e

t
s
e
T

)

R
M
%

(

r
o
r
r
e

t
s
e
T

15

20
MAPS η

25

15

20
MAPS η

25

15

20

25

MAPS η

Exact
APP-2

Exact
APP-3

Exact
APP-4

Exact
APP-5

15

20
MAPS η

25

15

20
MAPS η

25

Figure 3: Regression on network accuracy for exact and ap-
proximated count in black and color, respectively. The ap-
proximated count averages lower and upper bounds. As the
XOR size increases, these regressions become more parallel,
and thus the approximate count regression is more accurate.

of size 2, which are faster to compute but not as accurate, can
be used to compare relative expressiveness since the curves
from XOR-2 to XOR-5 have a very similar shape.

When upper and lower bounds are combined, the weakest
approximation still preserves a negative correlation with ac-
curacy, hence indicating that it may sufﬁce to compare net-
works for relative expressiveness. It is important to note that
we do not need the exact count to compare two networks in
terms of expressiveness or performance. Nevertheless, the
stronger approximations produce more precise correlations,
which is evidenced by the more parallel regressions and thus
more stable gaps across network sizes.

A Convex Outer Approximation of a ReLU

i = arg maxhl−1{W l

i = arg maxhl−1 {−W l

Lemma 1. If H l
i} ≥ 0 and
¯H l
i hl−1 − bl
i} ≥ 0, then the linear
relaxation of (1)–(4) deﬁnes the convex outer approximation
on (gl

i hl−1 + bl

i, hl

i).

Proof. We begin with the linear relaxation of the formula-

tion deﬁned by constraints (2)–(4):

i = hl
gl

i − ¯hl

i

i ≤ H l
hl

i zl
i

¯hl
i ≤ ¯H l

i (1 − zl
i)

hl
i ≥ 0
¯hl
i ≥ 0
0 ≤ zl
i ≤ 1

⇔

⇔

⇔

¯hl
i − gl
i = hl
i
hl
i
H l
i

zl
i ≥

zl
i ≤ 1 −

¯hl
i
¯H l
i

(10)

(11)

(12)

(13)

(14)

(15)

We ﬁrst project zl
i out by isolating that variable on one side
of each inequality, and then combining every lower bound
with every upper bound. Hence, we replace (11), (12), and
(15) with:

hl
i
H l
i

≤ 1 −

hl
i
H l
i

¯hl
i
¯H l
i

≤ 1

0 ≤ 1 −

¯hl
i
¯H l
i
0 ≤ 1

(cid:18)

1 −

(cid:19)

hl
i
H l
i

⇔

⇔

⇔

¯hl
i ≤ ¯H l

i

i ≤ H l
hl
i

¯hl
i ≤ ¯H l

i

(16)

(17)

(18)

(19)

Next, we project ¯hl
i through the same steps, also combining
the equality with the lower and upper bounds on the variable.
Hence, we replace (10), (14), (16), and (18) with:

i − gl
hl

i ≤ ¯H l

i

hl
i − gl
(cid:18)

1 −

i ≥ 0
(cid:19)
hl
i
H l
i
i ≤ ¯H l
(cid:19)

i

≥ 0

i ≥ 0

i − gl
hl
hl
i
H l
i
¯H l

1 −

(cid:18)

¯H l
i

(20)

(21)

(22)

(23)

(24)

We drop (19) as a tautology and (24) as implicit on our as-
sumptions. Similarly, for ¯H l
i > 0, inequality (23) is equiva-
lent to (17). Therefore, we are left with (13), (17), (20), (21),
and (22). We show in Figure 4 that the ﬁrst four inequalities
deﬁne the convex outer approximation on (gl
i), whereas
(22) is active at (− ¯H l
i ) and thus dom-
inated by (21) in that region.

i , 0) and (H l

i + ¯H l

i , H l

i, hl

Figure 4: The projected inequalities (13), (17), (20), and (21)
deﬁne the convex outer approximation on (gl

i, hl

i).

𝑔𝑖𝑙ℎ𝑖𝑙(21)(28)(29)(25)B Parity Constraints on 0–1 Variables
Similarly to the case of SAT formulas, we need to ﬁnd a
suitable way of translating a XOR constraint to a MILP for-
mulation. This is also discussed in (Ermon et al. 2013a) for
a related application of probabilistic inference. Let w be the
set of binary variables and U ⊆ V the set of indices of w
variables of a XOR constraint. To remove all assignments
with an even sum in U , we use a family of canonical cuts on
the unit hypercube (Balas and Jeroslow 1972):

(cid:88)

i∈U (cid:48)

(cid:88)

wi −

i∈U \U (cid:48)

wi ≤ |U (cid:48)| − 1 ∀U (cid:48) ⊆ U : |U (cid:48)| even, (25)

which is effectively separating each such assignment with
one constraint. Although exponential in k, each of those con-
straints – and only those – are necessary to deﬁne a convex
hull of the feasible assignments in the absence of other con-
straints (Jeroslow 1975). However, we note that we can do
better when k = 2 by using
wi + wj = 1

if U = { i, j }.

(26)

C Deriving the Lower Bound Probabilities of

Algorithm 1

The probabilities given to the lower bounds by Algorithm 1
are due to the main result for MBound in (Gomes, Sabhar-
wal, and Selman 2006a), which is based on the following
parameters: XOR size k; number of restrictions r; loop rep-
etitions i; number of repetitions that remain feasible after j
restrictions f [j]; deviation δ ∈ (0, 1/2]; and precision slack
α ≥ 1. We choose the values for the latter two.

A strict lower bound of 2r−α can be deﬁned if

f [j] ≥ i.(1/2 + δ),
(27)
and for δ ∈ (0, 1/2) it holds with probability 1 −
(cid:16)

for β = 2α.(1/2+δ)−1. We choose α = 1,
hence making β = 2.δ, and then set δ to the largest value
satisfying condition (27).

eβ
(1+β)1+β

(cid:17)i/2α

D Previous Bounds on the Maximum

Number of Linear Regions

l=1

(cid:0)nl
j

(cid:80)nl−1
j=0

We can obtain a bound for deep networks by recursively
combining the bounds obtained on each layer. By assum-
ing that every linear region deﬁned by the ﬁrst l − 1 lay-
ers is then subdivided into the maximum possible number
of linear regions deﬁned by the activation hyperplanes of
(cid:1) in
layer l, we obtain the implicit bound of (cid:81)L
(Raghu et al. 2017). By observing that the dimension of the
input of layer l on each linear region is also constrained by
the smallest input dimension among layers 1 to l − 1, we
(cid:1),
obtain the bound in (Mont´ufar 2017) of (cid:81)L
where dl = min{n0, n1, . . . , nl}. If we reﬁne the effect on
the input dimension by also considering that the number of
units that are active on each layer varies across the linear
regions, we obtain the tighter bound in (Serra, Tjandraat-
(cid:1),
madja, and Ramalingam 2018) of (cid:80)
where J = {(j1, . . . , jL) ∈ ZL : 0 ≤ jl ≤ min{n0, n1 −
j1, . . . , nl−1 − jl−1, nl} ∀l ∈ L}.

(j1,...,jL)∈J

(cid:0)nl
jl

(cid:0)nl
j

(cid:80)dl

(cid:81)L

j=0

l=1

l=1

E Average-Case Complexity of Computing

Al(k) and Il(k) for Any k

If we assume that each row of W l and vector bl have about
the same number of positive and negative elements, then we
can expect that each set I(l − 1, j) contains half of the units
in Ul. If these positive and negative elements are distributed
randomly for each unit, then a logarithmic number of the
units in layer l − 1 being active may sufﬁce to cover Ul. In
such case, we would explore O(nl−1) subsets on average.

F Approximation Algorithms for Computing

Al(k) and Il(k)

In Section 5, the assume these bounds for Il(k) and Al(k):






(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:91)

j∈S

(cid:12)
(cid:12)
(cid:12)
I(l − 1, j)
(cid:12)
(cid:12)
(cid:12)

Il(k) ≤ max

S

Al(k) ≤n+

: S ⊆ {1, . . . , nl−1}, |S| ≤ k






l |+

l + |U +



(cid:91)

max
S



j∈S

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)
(cid:12)
I −(l − 1, j)
(cid:12)
(cid:12)
(cid:12)

: S ⊆ {1, . . . , nl−1}, |S| ≤ k






The maximization terms on the right hand side of the in-
equalities for Il(k) and Al(k) can be seen as ﬁnding a set
of k subsets of the form I(l − 1, j) or I −(l − 1, j), re-
spectively, and whose union achieves the largest cardinal-
ity. This can be shown to be directly related to the maxi-
mum k-coverage problem with (1 − 1
e )−approximation us-
ing an efﬁcient greedy algorithm (Feige 1998). Note that the
maximum k-coverage problem is actually a special case of
the maximization of submodular functions, which are dis-
crete analogue of convex functions (Nemhauser, Wolsey,
and Fisher 1978). For large networks, the use of greedy al-
gorithms can be beneﬁcial to get good approximations for
Il(k) and Al(k) efﬁciently.

G Additional Information on Networks and

Experiments

To compare with exact counting, the networks used in the
experiments are those originally described in (Serra, Tjan-
draatmadja, and Ramalingam 2018). They consist of rectiﬁer
networks trained on the MNIST benchmakrk dataset (LeCun
et al. 1998), each with two hidden layer summing to 22 units
and an output layer having another 10 units. For each possi-
ble conﬁgurations of units in the hidden layers, there are 10
distinct networks that were trained for 20 epochs. The linear
regions considered are in the [0, 1]n0 box of the input x. In
the cases where no layer has 3 or less units, the number of
linear regions was shown to relate to the network accuracy.
In our experiments, the code is written in C++ (gcc 4.8.4)
using CPLEX Studio 12.8 as a solver and ran in Ubuntu
14.04.4 on a machine with 40 Intel(R) Xeon(R) CPU E5-
2640 v4 @ 2.40GHz processors and 132 GB of RAM.

Acknowledgements We would like to thank Christian
Tjandraatmadja, Michele Lombardi, and Tiago F. Tavares
for their helpful suggestions and comments on this work.

References
[Achlioptas and Jiang 2015] Achlioptas, D., and Jiang, P.
2015. Stochastic integration via error-correcting codes. In
UAI.
[Achlioptas, Hammoudeh, and Theodoropoulos 2018]
Achlioptas, D.; Hammoudeh, Z.; and Theodoropoulos, P.
In
2018. Fast and ﬂexible probabilistic model counting.
SAT.
[Anderson et al. 2019] Anderson, R.; Huchette, J.; Tjan-
draatmadja, C.; and Vielma, J. P. 2019. Strong mixed-integer
programming formulations for trained neural networks. In
IPCO.
[Arora et al. 2018] Arora, R.; Basu, A.; Mianjy, P.; and
Mukherjee, A. 2018. Understanding deep neural networks
with rectiﬁed linear units. In ICLR.
[Balas and Jeroslow 1972] Balas, E., and Jeroslow, R. G.
1972. Canonical cuts on the unit hypercube. SIAM J. Appl.
Math. 23:61–69.
[Bartlett, Maiorov, and Meir 1998] Bartlett, P.; Maiorov, V.;
and Meir, R. 1998. Almost linear VC-dimension bounds for
piecewise polynomial networks. Neural computation.
[Bastani et al. 2016] Bastani, O.; Ioannou, Y.; Lampropou-
los, L.; Vytiniotis, D.; Nori, A.; and Criminisi, A. 2016.
Measuring neural net robustness with constraints.
In
NeurIPS.
[Beise, Cruz, and Schr¨oder 2018] Beise, H.-P.; Cruz, S.
D. D.; and Schr¨oder, U. 2018. On decision regions of
narrow deep neural networks. CoRR abs/1807.01194.
[Bengio 2009] Bengio, Y. 2009. Learning deep architec-
tures for AI. Foundations and Trends R(cid:13)in Machine Learning
2(1):1–127.
[Carter and Wegman 1979] Carter, J. L., and Wegman, M. N.
1979. Universal classes of hash functions. J. Comput. Syst.
Sci. 18:143–154.
[Chakraborty, Meel, and Vardi 2013] Chakraborty, S.; Meel,
K. S.; and Vardi, M. Y. 2013. A scalable approximate model
counter. In CP.
[Chakraborty, Meel, and Vardi 2014] Chakraborty, S.; Meel,
K. S.; and Vardi, M. Y. 2014. Balancing scalability and
uniformity in SAT witness generator. In DAC.
[Chakraborty, Meel, and Vardi 2016] Chakraborty, S.; Meel,
K. S.; and Vardi, M. Y. 2016. Algorithmic improvements
in approximate counting for probabilistic inference: From
linear to logarithmic SAT calls. In IJCAI.
[Charisopoulos and Maragos 2018] Charisopoulos, V., and
Maragos, P. 2018. A tropical approach to neural networks
with piecewise linear activations. CoRR abs/1805.08749.
[Cheng, N¨uhrenberg, and Ruess 2017] Cheng,
C.-H.;
N¨uhrenberg, G.; and Ruess, H. 2017. Maximum resilience
of artiﬁcial neural networks. In ATVA.
[Cook 1971] Cook, S. A. 1971. The complexity of theorem-
proving procedures. In STOC.
[Croce, Andriushchenko, and Hein 2019] Croce, F.; An-
driushchenko, M.; and Hein, M. 2019. Provable robustness

In

of ReLU networks via maximization of linear regions.
UAI.
[Cybenko 1989] Cybenko, G. 1989. Approximation by su-
perpositions of a sigmoidal function. Math. Control Signals
Syst. 2(4):303–314.
[Danna et al. 2007] Danna, E.; Fenelon, M.; Gu, Z.; and
Wunderling, R. 2007. Generating multiple solutions for
mixed integer programming problems. In IPCO.
[Dutta et al. 2018] Dutta, S.; Jha, S.; Sankaranarayanan, S.;
and Tiwari, A. 2018. Output range analysis for deep feed-
forward networks. In NFM.
[Ehlers 2017] Ehlers, R. 2017. Formal veriﬁcation of piece-
wise linear feed-forward neural networks. In ATVA.
[Elsayed et al. 2018] Elsayed, G. F.; Krishnan, D.; Mobahi,
H.; Regan, K.; and Bengio, S. 2018. Large margin deep
networks for classiﬁcation. In NeurIPS.
[Ermon et al. 2013a] Ermon, S.; Gomes, C. P.; Sabharwal,
A.; and Selman, B. 2013a. Optimization with parity con-
straints: From binary codes to discrete integration. In UAI.
[Ermon et al. 2013b] Ermon, S.; Gomes, C. P.; Sabharwal,
A.; and Selman, B. 2013b. Taming the curse of dimen-
sionality:discrete integration by hashing and optimization.
In ICML.
[Ermon et al. 2014] Ermon, S.; Gomes, C. P.; Sabharwal, A.;
and Selman, B. 2014. Low-density parity constraints for
hashing-based discrete integration. In ICML.
[Feige 1998] Feige, U. 1998. A threshold of ln n for approx-
imating set cover. J. ACM.
[Fischetti and Jo 2018] Fischetti, M., and Jo, J. 2018. Deep
neural networks as 0-1 mixed integer linear programs: A fea-
sibility study. In CPAIOR.
[Glorot, Bordes, and Bengio 2011] Glorot, X.; Bordes, A.;
and Bengio, Y. 2011. Deep sparse rectiﬁer neural networks.
In AISTATS.
[Gomes et al. 2007a] Gomes, C. P.; Hoeve, W.-J. V.; Sabhar-
wal, A.; and Selman, B. 2007a. Counting CSP solutions
using generalized XOR constraints. In AAAI.
[Gomes et al. 2007b] Gomes, C. P.; Hoffmann, J.; Sabhar-
wal, A.; and Selman, B. 2007b. Short XORs for model
counting: From theory to practice. In SAT.
[Gomes, Sabharwal, and Selman 2006a] Gomes, C. P.; Sab-
harwal, A.; and Selman, B. 2006a. Model counting: A new
strategy for obtaining good bounds. In AAAI.
[Gomes, Sabharwal, and Selman 2006b] Gomes, C. P.; Sab-
harwal, A.; and Selman, B. 2006b. Near-uniform sampling
of combinatorial spaces using XOR constraints. In NeurIPS.
[Guang-He Lee 2019] Guang-He Lee, David Alvarez-Melis,
T. S. J. 2019. Towards robust, locally linear deep networks.
In ICLR.
[Hahnloser et al. 2000] Hahnloser, R. H. R.; Sarpeshkar, R.;
Mahowald, M. A.; Douglas, R. J.; and Seung, H. S. 2000.
Digital selection and analogue ampliﬁcation coexist in a
cortex-inspired silicon circuit. Nature 405.

of response regions of deep feed forward networks with
piece-wise linear activations. In ICLR.
[Raghu et al. 2017] Raghu, M.; Poole, B.; Kleinberg, J.;
Ganguli, S.; and Sohl-Dickstein, J. 2017. On the expres-
sive power of deep neural networks. In ICML.
[Serra, Tjandraatmadja, and Ramalingam 2018] Serra,
T.;
Tjandraatmadja, C.; and Ramalingam, S. 2018. Bounding
and counting linear regions of deep neural networks.
In
ICML.
[Sipser 1983] Sipser, M. 1983. A complexity theoretic ap-
proach to randomness. In STOC.
[Stockmeyer 1985] Stockmeyer, L. 1985. On approximation
algorithms for #P. SIAM Journal on Computing 14(4):849–
861.
[Tjeng, Xiao, and Tedrake 2019] Tjeng, V.; Xiao, K.; and
Tedrake, R. 2019. Evaluating robustness of neural networks
with mixed integer programming. In ICRL.
[Toda 1989] Toda, S. 1989. On the computational power of
PP and (+)P. In FOCS.
[Valiant and Vazirani 1986] Valiant, L., and Vazirani, V.
1986. NP is as easy as detecting unique solutions. Theo-
retical Computer Science 47:85–93.
[Wong and Kolter 2018] Wong, E., and Kolter, J. Z. 2018.
Provable defenses against adversarial examples via the con-
vex outer adversarial polytope. In ICML.
[Xiao et al. 2018] Xiao, K. Y.; Tjeng, V.; Shaﬁullah, N. M.;
and Madry, A. 2018. Training for faster adversarial ro-
bustness veriﬁcation via inducing ReLU stability. CoRR
abs/1809.03008.
[Zaslavsky 1975] Zaslavsky, T. 1975. Facing up to Arrange-
ments: Face-Count Formulas for Partitions of Space by Hy-
perplanes. AMS.
[Zhang, Naitzat, and Lim 2018] Zhang, L.; Naitzat, G.; and
Lim, L.-H. 2018. Tropical geometry of deep neural net-
works. In ICML.
[Zhao et al. 2016] Zhao, S.; Chaturapruek, S.; Sabharwal,
A.; and Ermon, S. 2016. Closing the gap between short
and long XORs for model counting. In AAAI.

2019.

[Hanin and Rolnick 2019a] Hanin, B., and Rolnick, D.
2019a. Complexity of Linear Regions in Deep Networks.
In ICML.
[Hanin and Rolnick 2019b] Hanin, B., and Rolnick, D.
2019b. Deep ReLU Networks Have Surprisingly Few Ac-
tivation Patterns. In NeurIPS.
[He et al. 2016] He, K.; Zhang, X.; Ren, S.; and Sun, J. 2016.
Deep residual learning for image recognition. In CVPR.
[Huchette 2018] Huchette, J. 2018. Advanced mixed-integer
programming formulations: Methodology, computation, and
application. Ph.D. Dissertation, Massachusetts Institute of
Technology.
[Ivrii et al. 2016] Ivrii, A.; Malik, S.; Meel, K. S.; and Vardi,
M. Y. 2016. On computing minimal independent support
and its applications to sampling and counting. Constraints
21:41–58.
[Jain, Kadioglu, and Sellmann 2010] Jain, S.; Kadioglu, S.;
and Sellmann, M. 2010. Upper bounds on the number of
solutions of binary integer programs. In CPAIOR.
[Jeroslow 1975] Jeroslow, R. G. 1975. On deﬁning sets of
vertices of the hypercube by linear inequalities. Discrete
Math 11:119–124.
[Kumar, Serra, and Ramalingam 2019] Kumar, A.; Serra, T.;
Equivalent and approxi-
and Ramalingam, S.
CoRR
mate transformations of deep neural networks.
abs/1905.11428.
[LeCun, Bengio, and Hinton 2015] LeCun, Y.; Bengio, Y.;
and Hinton, G. 2015. Deep learning. Nature 521.
[LeCun et al. 1998] LeCun, Y.; Bottou, L.; Bengio, Y.; and
Haffner, P. 1998. Gradient-based learning applied to docu-
ment recognition. Proceedings of the IEEE.
2018.
[Lin and Jegelka 2018] Lin, H., and Jegelka, S.
ResNet with one-neuron hidden layers is a universal approx-
imator. In NeurIPS.
[Lomuscio and Maganti 2017] Lomuscio, A., and Maganti,
L. 2017. An approach to reachability analysis for feed-
forward ReLU neural networks. CoRR abs/1706.07351.
[Mont´ufar et al. 2014] Mont´ufar, G.; Pascanu, R.; Cho, K.;
and Bengio, Y. 2014. On the number of linear regions of
deep neural networks. In NeurIPS.
[Mont´ufar 2017] Mont´ufar, G. 2017. Notes on the number
of linear regions of deep neural networks. In SampTA.
[Nair and Hinton 2010] Nair, V., and Hinton, G. E. 2010.
Rectiﬁed linear units improve restricted Boltzmann ma-
chines. In ICML.
[Nemhauser, Wolsey, and Fisher 1978] Nemhauser, G. L.;
Wolsey, L. A.; and Fisher, M. L. 1978. An analysis of the
approximations for maximizing submodular set functions.
Mathematical Programming 14:265–294.
[Nguyen, Mukkamala, and Hein 2018] Nguyen, Q.; Mukka-
mala, M. C.; and Hein, M. 2018. Neural networks should be
wide enough to learn disconnected decision regions. CoRR
abs/1803.00094.
[Pascanu, Mont´ufar, and Bengio 2014] Pascanu,
Mont´ufar, G.; and Bengio, Y.

R.;
2014. On the number

