Compilable Neural Code Generation with Compiler Feedback
Xin Wang1(cid:63)(cid:5), Yasheng Wang2(cid:63), Yao Wan4, Fei Mi2, Yitong Li2,3,
Pingyi Zhou2, Jin Liu1(cid:0), Hao Wu5, Xin Jiang2, Qun Liu2
1School of Computer Science, Wuhan University, China

2Huawei Noah’s Ark Lab,

3Huawei Technologies Co., Ltd.

4School of Computer Sci. & Tech., Huazhong University of Science and Technology, China
5School of Information Science and Engineering, Yunnan University, China
{xinwang0920, jinliu}@whu.edu.cn, wanyao@hust.edu.cn, haowu@ynu.edu.cn
{wangyasheng, feimi2, liyitong3, zhoupingyi, Jiang.Xin, qun.liu}@huawei.com

2
2
0
2

r
a

M
0
1

]
L
C
.
s
c
[

1
v
2
3
1
5
0
.
3
0
2
2
:
v
i
X
r
a

Abstract

Automatically generating compilable pro-
grams with (or without) natural language de-
scriptions has always been a touchstone prob-
lem for computational linguistics and auto-
mated software engineering. Existing deep-
learning approaches model code generation as
text generation, either constrained by grammar
structures in decoder, or driven by pre-trained
language models on large-scale code corpus
(e.g., CodeGPT, PLBART, and CodeT5). How-
ever, few of them account for compilability of
the generated programs. To improve compil-
ability of the generated programs, this paper
proposes COMPCODER, a three-stage pipeline
utilizing compiler feedback for compilable
code generation,
including language model
ﬁne-tuning, compilability reinforcement, and
compilability discrimination. Comprehensive
experiments on two code generation tasks
demonstrate the effectiveness of our proposed
approach, improving the success rate of compi-
lation from 44.18 to 89.18 in code completion
on average and from 70.3 to 96.2 in text-to-
code generation, respectively, when compar-
ing with the state-of-the-art CodeGPT.

1

Introduction

Automated code generation (or program synthe-
sis) has attracted much attention over the past few
years (Lu et al., 2021), because of its potential to
improve the productivity of developers, as well
as to speed up the software development (Parvez
et al., 2021; Wang et al., 2021). In the life cycle
of software development, different types of code
generation tasks are desired, including code com-
pletion (Liu et al., 2020b,a), text-to-code gener-
ation (Hashimoto et al., 2018), program transla-
tion (Chen et al., 2018), and program repair (Ya-
sunaga and Liang, 2021).

(cid:63) Equal contribution.
(cid:5) Work done while this author was an intern at Huawei

Noah’s Ark Lab.

(cid:0) Correspondence author.

Figure 1: An illustration of Python code completion
by COMPCODER, utilizing the compiler feedback with
three stages.

Recently, much effort has been made to ad-
vance the development of code generation (Li et al.,
2018), using different logical forms of code, such
as the abstract syntax tree (AST) (Kim et al., 2021;
Yin and Neubig, 2017; Rabinovich et al., 2017),
sketch (Nye et al., 2019) and graph (Yasunaga and
Liang, 2020). Beneﬁting from the strong power
of pre-training techniques (Devlin et al., 2019;
Wang et al., 2021a) in natural language process-
ing, several attempts have been made towards pre-
training a language model on large-scale code cor-
pus for code generation, such as CodeGPT (Lu
et al., 2021), PLBART (Ahmad et al., 2021), and
CodeT5 (Wang et al., 2021b).

However, to the best of our knowledge, most
deep-learning approaches for code generation are
still difﬁcult to guarantee the compilability of the
generated code, resulting in non-compilable code.
For example, Chen et al. (2021) found that up
to 67%-97% of patches generated by the most
advanced deep-learning-based models are non-

LM(a) Language Model Fine-TuningLMCompiler Feedback(b) Compilability ReinforcementLM(c) Compilability Discrimination...LMDiscrimminatorCandidatesCompiler FeedbackCompiler Compiler Code Snippet1 def func(x,y):2     out = Code Snippet1 def func(x,y):2     out = x * y3     return outCode Snippet1 def func(x,y):2     out = CompletingCode Snippet1     def func(x,y):2         out = x * y3         return outCode Snippet1  def func(x,y):2      out = x * y3      return out<BOS><EOS> 
 
 
 
 
 
compilable. We think this is because they generally
do not directly optimize the compilability for code
generation. The generation of non-compilable code
will waste the time of programmers, as well as seri-
ously reduce the trust and satisfaction of developers
with the model. To improve the compilability of the
generated code, some works attempt to repair the
synthesized program which fails to compile (Ku-
lal et al., 2019; Yasunaga and Liang, 2020, 2021).
Recently, Korbak et al. (2021) attempt to directly
generate compilable code using an energy model
with compilability constraints.

This paper focuses on the task of compilable
neural code generation. Different from previous
works, we use compilability signals in two ways
and design a novel method to jointly train the dis-
criminator and generator for compilable code gen-
eration. Concretely, we propose COMPCODER, a
novel three-stage pipeline utilizing compiler feed-
back for compilable code generation, including lan-
guage model ﬁne-tuning, compilability reinforce-
ment, and compilability discrimination. Figure 1
shows an example of Python code completion by
COMPCODER, which utilizes the compiler feed-
back in two ways. In Figure 1(b), we use the com-
piler feedback to optimize the generator. In Fig-
ure 1(c), we use the discriminator to check if the
results generated by the generator can be success-
fully compiled. The joint training of the generator
and discriminator signiﬁcantly improves the com-
pilability of the generated code.

Overall, the key contributions of this paper are

as follows:
• We use compilability signals in two ways and de-
sign a novel method to jointly train the generator
and discriminator for compilable code generation,
called COMPCODER. We reﬁne a pre-trained
code generator using reinforcement learning and
jointly learn a discriminator to enforce the gener-
ator to correct its own mistakes.

• Comprehensive experiments on two code gen-
eration tasks demonstrate the effectiveness of
COMPCODER. It boosts the average compila-
tion rate of CodeGPT from 44.18 to 89.18 in the
code completion task and from 70.3 to 96.2 in
the text-to-code generation task.

2 Preliminary

In this section, we set out notations for task formu-
lation, as well as some preliminaries of compiler
feedback. Let s ∈ S denote a given input, which

can be a piece of partial code, natural-language
description, or buggy program. Let t ∈ T denote
the generated source code. Formally, the problem
of code generation can be formulated as learning a
mapping f between the input space and target code
space, i.e. f : S → T . In this paper, we investigate
two speciﬁc code generation tasks, code comple-
tion and text-to-code generation, conditioned on
different inputs.

Code Completion Let c = {c1, c2, . . . , c|c|} de-
note a sequence of code tokens for program c,
where |c| denotes the length of the code. We
use notation c1 : m ∈ S to refer to the previ-
ous code snippet {c1, c2, . . . , cm} and notation
cm+1 : |c| ∈ T to represent the subsequent code
snippet {cm+1, . . . , c|c|}. The code completion
task can be deﬁned as generating the subsequent (t)
code token sequence cm+1 : |c|, given the previous
(s) code sequence c1 : m.

Text-to-Code Generation Different from code
completion, text-to-code generation aims to gen-
erate a whole program based on natural language
description. Let d = {d1, d2, . . . , d|d|} refer to a
sequence of natural-language tokens. The text-to-
code generation task can be deﬁned as generating
source code c = t ∈ T , given the corresponding
natural language description d = s ∈ S.

Compiler Feedback As the whole program c is
generated, no matter from partial code snippets
or natural-language descriptions, we feed it into a
compiler to test whether it can be compiled suc-
cessfully. Formally, we deﬁne the the compiler
feedback as:

feedback = 1Compiler(c) ,

(1)

where the compiler feedback is a binary value (com-
pilable or non-compilable), and c denotes the code
snippet fed into the compiler. As for the task of
text-to-code generation, we simply feed the gener-
ated code t into the compiler, i.e., c = t. As for the
task of code completion, we concatenate the partial
code with generated code as a whole program, i.e.,
c = [s; t], where ; is the concatenation operation.

3 COMPCODER

Figure 2 shows the overall architecture of COMP-
CODER on the code completion task, which covers
three stages, i.e., language model ﬁne-tuning (Stage
1), compilability reinforcement (Stage 2) and com-
pilability discrimination (Stage 3). In the following

Figure 2: An illustration of our proposed three-stage pipeline for Python code completion. (a) We ﬁrst ﬁne-tune
the generator based on pre-trained language models. (b) We take the compiler feedback into account as a reward
via reinforcement learning. (c) We design a compilability discriminator which is jointly trained with the generator,
to enforce the generator to correct its own mistakes. Stages 2 and 3 are performed alternately.

subsections, we will elaborate on each stage one
by one. We alternately perform Stages 2 and 3, as
described in Section 3.4.

3.1 Stage 1: Language Model Fine-Tuning

As shown in Figure 2(a), we adopt CodeGPT as the
generator, which uses GPT-2 (Radford et al., 2019)
as the starting point and is continually pre-trained
on the large-scale code corpus. Our generator is
then ﬁne-tuned on the target task to minimize the
cross-entropy loss:

from the real environment (Sutton and Barto, 1998;
Wan et al., 2018). As shown in Figure 2(b), we
use the ﬁne-tuned generator ρ (after Stage 1) as
the reference model. Then we initialize a policy
π = ρ. Given an input sequence s ∈ S, our goal
is to ﬁnd a policy π that generates an output se-
quence t ∈ T with the objective of maximizing the
compilability-based reward. We use RL (speciﬁ-
cally PPO2 version of Proximal Policy Optimiza-
tion (Schulman et al., 2017)) to directly optimize
the expected reward as:

LG = −

1
|M|

|M|
(cid:88)

|V|
(cid:88)

i

j

Yij log Pij ,

(2)

where M denotes the set of the generated code
tokens, V represents the vocabulary, Yij denotes
the label of the code token i in class j, and Pij is
the predicted probability of token i in class j.

During training,

the generator takes x =
{<BOS>, c, <EOS>} as the input in the code com-
pletion task, and x = {d, <BOS>, c, <EOS>} as in-
put in the text-to-code generation task, correspond-
ingly. Special tokens <BOS> and <EOS> indicate
the start and end symbols of code sequences. Af-
ter several epochs of supervised ﬁne-tuning on the
target task dataset, we save the trained generator,
which will be used in the next stage.

3.2 Stage 2: Compilability Reinforcement

Reinforcement Learning (RL) is a method of learn-
ing the optimal policy by obtaining reward signals

Eπ [r] = Es∼S,t∼π(.|s) [r(s, t)] ,

(3)

where the policy π is rewarded by the compiler
(Eq. 1), r is the reward function. We deﬁne
r(s, t) = 1.0 iff the code can be compiled by the
program compiler and r(s, t) = −1.0 otherwise.

It is worth mentioning that code compilability
constraints can be strong or weak. Strong con-
straint is deﬁned that a long piece of code snippet
may not be correctly compiled if a certain token
is changed. And weak constraint means a blank
string consisting of whitespace characters can be
correctly compiled by the compiler. Concretely, in
the text-to-code generation task, if the generator
generates a string composed of whitespace charac-
ters, the compiler will consider it as a good case.
In the code completion task, if the previous code
snippet is compilable, the generator can fool the
compiler easily. The RL is good at making use of
this, resulting in the generated code can be com-

GeneratorTraining Procedure(a) LM Fine-tuningGeneratorGenerator(b)  Compilability Reinforcement  GeneratorPolicy TrainingDiscriminating LossActive Model  π Reference Model   ρ (c)  Compilability Discrimination--(a)       (b)        (c)GeneratorDiscriminatorJoint  LearningP(.|t,s)✖/✔CompilerFeedback...Generating CandidatesCompilerGenerating Loss+FeedbackRewardKL-divergenceCode Snippet1     def func(x,y):2         out = x * y3         return outCode Snippet1    def func(x,y):2        out = x * y3        return outCode Snippet1 def func(x,y):2     out = Code Snippet1 def func(x,y):2     out = x * y3     return outCode Snippet1 def func(x,y):2     out = Code Snippet1     def func(x,y):2         out = x * y3         return out<BOS><EOS><BOS><EOS>Figure 3: An example of code completion. We mask the last ﬁve tokens of the code and let the generator complete
them. Some minor mistakes prevent four candidates from being correctly compiled by the program compiler.

piled, but seriously deviating from the generation
likelihood objective.

To avoid active model π being too far away from
reference model ρ, we add a Kullback-Leibler (KL)
penalty with expectation, e.g., βKL(π, ρ) (Ziegler
et al., 2019). Therefore, the modiﬁed reward will
be reformulated as follows:

r(s, t) = r(s, t) − β log

π(t|s)
ρ(t|s)

,

(4)

where β is a constant, which plays the role of an
entropy bonus, preventing the policy from moving
too far from the range where r is valid.

To alleviate the imbalance between the reward
term and the KL penalty term and improve the sta-
bility of training, we use autoregressive ﬁne-tuning
(Causal Language Modeling) (Radford et al., 2019)
to make the KL penalty term ﬂuctuate within a
small range after RL training. This ﬁne-tuning pro-
cess incorporates a compilability-aware discrimi-
nator that will be introduced in the next stage.

3.3 Stage 3: Compilability Discrimination

Figure 3 shows an example of code completion. We
mask the last ﬁve tokens of a Python function and
ask the generator to complete them. The generator
generates ﬁve candidates with high probabilities.
Some minor mistakes prevent four of them from
being successfully compiled. We hope the gener-
ator can have more perception power to explicitly
distinguish compilable and non-compilable code
generated by itself. Therefore, at this stage, we
design a compilability-aware discriminator to deal
with this issue.

Concretely, we add a discriminator (a two-layer
MLP equipped with the tanh activation function
between layers) after the ﬁnal hidden layer of the

generator. As shown in Figure 2(c), given the input
sequence (s), we perform beam search on the gen-
erator to generate top-k candidates (t). Each entire
code c ∈ Q (c = [s; t] in the code completion task)
is labeled by the program compiler as positive (1)
or negative (0), depending on whether it can be
successfully compiled (see Eq. 1).

We use the hidden representation of the last to-
ken (<EOS>) as the ﬁnal representation of the en-
tire code c. Finally, the hidden representation of
the last token (<EOS>) is fed into the discriminator
for prediction:

h<EOS> = CodeGPT(s, t) ,
(cid:48)
<EOS> = Discriminator(h<EOS>) ,
h

(cid:48)
P (.|t, s) = softmax(h

<EOS>) ,

(5)

(6)

(7)

where h<EOS> denotes the representation of the last
token <EOS>. The training loss of the discrimina-
tion process can be deﬁned as:

LD = −

1
|Q+ ∪ Q−|





(cid:88)

c∈Q+


log P (1|t, s)

(8)

(cid:88)

+

c∈Q−

log P (0|t, s)

 ,

where Q+ and Q− represent positive and negative
sets respectively. The parameters of the generator
and discriminator will be jointly updated.

At this stage, we jointly train the generator and
discriminator, including a generating objective (to
learn the generator only) and a discriminating ob-
jective (to learn the generator and discriminator
together), as shown in Figure 2(c). The joint train-
ing loss is deﬁned as follows:

L = LG + LD .

(9)

1  def add_sparql_line_nums(sparql):2      lines = sparql.split("\n")3      return "\n".join(["%s %s" % (i + 1, line) for i, line in enumerate(lines)]).]).1  def add_sparql_line_nums(sparql):2      lines = sparql.split("\n")3      return "\n".join(["%s %s" % (i + 1, line) for i, line in enumerate(lines)]]1  def add_sparql_line_nums(sparql):2      lines = sparql.split("\n")3      return "\n".join(["%s %s" % (i + 1, line) for i, line in enumerate(lines) if if1  def add_sparql_line_nums(sparql):2      lines = sparql.split("\n")3      return "\n".join(["%s %s" % (i + 1, line) for i, line in enumerate(lines)] +] +1  def add_sparql_line_nums(sparql):2      lines = sparql.split("\n")3      return "\n".join(["%s %s" % (i + 1, line) for i, line in enumerate(lines)])])Candidate1Candidate2Candidate3Candidate4Candidate53.4 Overall Pipeline

Training Procedure We perform an interactive
training procedure. Concretely, except that the ﬁrst
epoch contains Stages 1, 2, and 3, each subsequent
epoch only consists of Stages 2 and 3. We update
the reference model (at Stage 2), and candidates in
Stage 3 is generated on the training dataset, which
is time consuming, so we update the candidates in
a preset frequency.

For better understanding, Stage 2 improves the
compilability of generated code, Stage 3 distin-
guishes the compilable and non-compilable code
generated by itself. Stage 2 and 3 reﬁne each other
and improve the performance iteratively, which is
a basic idea of this training procedure. We think
that the generator with high compilability (after
Stage 2) facilitates the learning of the discriminator
(discriminating objective at Stage 3). The autore-
gressive ﬁne-tuning (generating objective at Stage
3) helps the KL penalty term (at Stage 2) ﬂuctu-
ate in a small range, improving the stability of RL
training. At Stage 3, the discriminating objective
is optimized by learning the generator and discrim-
inator together, which makes the generator have
more perception power to distinguish compilable
and non-compilable code.

Inference Procedure The model inference con-
sists of two stages. Given an input sequence (s),
we perform the beam search on the generator to
generate top-k candidates. The code (c in Eq. 1)
with the highest compilability probability evalu-
ated by the discriminator will be selected. Then the
output (t) can be obtained as the ﬁnal result.

4 Experiment Setup

4.1 Evaluation Tasks and Datasets

We conduct experiments on two tasks: code com-
pletion and text-to-code generation. To investigate
the compilability of the generated code, we need
to preserve the indentation and newline operations
in code. We also need to make sure that the code
and its version belong to the scope of the compiler.
Existing datasets on both of the two tasks usually
do not serve these considerations. For convenience,
we choose Python for experiments, as it is very
popular and used in many projects. We conduct all
experiments based on Python 3 environment and
adopt the codeop1 module to simulate the pro-

1https://docs.python.org/3.6/library/

codeop.html

gram compiler. We remove code that could not be
compiled correctly by the compiler.

Code Completion For the code completion task,
we use the Python corpus in CodeSearchNet (Hu-
sain et al., 2019). We want to study the compilabil-
ity of long enough code, while longer code means
higher computational overhead. Therefore, we ex-
tract 50k compilable Python methods (Python 3
version) with eclectic token lengths ranging from
64 to 96. We randomly select 45k samples for train-
ing and the remaining 5k samples for testing. We
mask a different number of tokens at the tail of the
source code and let the model complete.

Text-to-Code Generation For the text-to-code
generation task, we adopt the AdvTest dataset (Lu
et al., 2021), which contains 251,820 text and
Python code pairs. We only need code in Python
3 version. We expect code token lengths to range
from 128 to 170, a moderate length, and text to-
ken lengths to be at least more than 5, containing
sufﬁcient semantics. Finally, we extract about 41k
text-code pairs. We randomly select 40k text-code
pairs for training, and the remaining 1k text-code
pairs for testing.

4.2 Evaluation Metrics

To evaluate the quality of the generated code, we
adopt two widely-used evaluation metrics: Leven-
shtein Edit Similarity (ES) (Svyatkovskiy et al.,
2020; Lu et al., 2021) and Compilation Rate
(CR) (Kulal et al., 2019). Levenshtein Edit Similar-
ity measures the number of single-character edits
required to transform one string into another. It is
a critical evaluation metric for the code generation
scenario, as it measures the effort required for the
developer to correct the code. Compilation Rate
measures how many code can be correctly com-
piled by the program compiler. For both of these
metrics, bigger values indicate better performance.

4.3 Baseline Methods

We compare our approach with various state-of-
the-art models in the code completion task and the
text-to-code generation task:
• BiLSTM is a Seq2Seq model based on Bidirec-
tional LSTM with an attention mechanism (Lu-
ong et al., 2015).

• Transformer (Vaswani et al., 2017) is the base
architecture of CodeGPT. We use 6-layer Trans-
former decoder to conduct experiments.

(a)

(b)

(c)

(d)

Figure 4:
Edit Similarity (ES) and Compilation Rate (CR) metrics, using the CodeSearchNet-Python dataset.

: Results in the code completion task (completing 30, 35, 40, 45 tokens respectively) evaluating with

• GPT-2 (Radford et al., 2019)

is an auto-
regressive pre-trained model trained on large-
scale text corpus.

• CodeGPT (Lu et al., 2021) is pre-trained with
source code corpus on the basis of GPT-2 vis
causal language modeling objective (Radford
et al., 2019).

• PLBART (Ahmad et al., 2021) is based on the
BART (Lewis et al., 2020) architecture, which is
pre-trained on large-scale Java and Python cor-
pora via denoising autoencoding.

• CodeT5 (Wang et al., 2021b) is based on the
T5 (Raffel et al., 2020) architecture, which
employs denoising sequence-to-sequence pre-
training on multiple programming languages.

4.4

Implementation Details

In the code completion task, we set the learning
rate as 1.5e-5, the batch size as 32, the maximum
ﬁne-tuning epoch as 20, the maximum code se-
quence length as 96. We mask different numbers
of code tokens (25, 30, 35, 40, and 45) and ask
the model to complete them. We set the minimum
generation length as 25, 30, 35, 40, and 45 accord-
ingly. In the text-to-code generation task, we set
the learning rate as 1.5e-5, the batch size as 16, the
maximum ﬁne-tuning epoch as 20, the maximum
text and code sequence length as 32 and 170. We
set the minimum generation length as 96 (the gen-
erated code is slightly shorter than the ground-truth
is allowed). In these two tasks, the generated se-
quence consisting of whitespace characters will be
considered as a bad case.

We use the Adam optimizer to update model
parameters. We train our model on the basis of
CodeGPT checkpoint2. Our model is trained on 2

2https://huggingface.co/microsoft/

CodeGPT-small-py-adaptedGPT2

NVIDIA Tesla V100 with 32GB memory. We em-
ploy the same tokenizer as CodeGPT. To train the
policy π, we use the PPO2 version of Proximal Pol-
icy Optimization (Schulman et al., 2017). In each
epoch, we only randomly select 5% training data
for the stability of RL training (Stage 2). In other
stages (Stages 1 and 3), we use the full training
data. To generate candidates (at Stage 3), we set
the beam size as 5 in beam search. For efﬁciency,
we update the candidates every 5 epochs.

Models

BiLSTM
Transformer
GPT-2
CodeGPT
COMPCODER

ES

55.32
61.47
63.02
64.47
64.53

CR

36.34
40.22
43.26
46.84
94.48

Table 1: Results in the code completion task (com-
pleting 25 tokens) evaluating with Edit Similarity
(ES) and Compilation Rate (CR) metrics, using the
CodeSearchNet-Python dataset.

5 Results and Analysis

5.1 Code Completion

Table 1 shows the results of the code completion
task. We mask 25 tokens at the tail of code func-
tions and ask the generation model to complete.
We can observe that: (1) The code generated by
existing autoregressive models has a low Compi-
lation Rate. CodeGPT and GPT-2 only achieve
46.84 and 43.26 scores respectively on the Compi-
lation Rate, which means that more than half of the
code generated by them cannot be correctly com-
piled by the program compiler. (2) COMPCODER
signiﬁcantly improves the Compilation Rate. It ob-
tains 94.48 scores on the Compilation Rate, which
is 47.64 points higher than the closest competitor

ESCR20406080100PerformanceToken Num = 30BiLSTMTransformerGPT-2CodeGPTCompCoderESCR20406080100PerformanceToken Num = 35BiLSTMTransformerGPT-2CodeGPTCompCoderESCR20406080100PerformanceToken Num = 40BiLSTMTransformerGPT-2CodeGPTCompCoderESCR20406080100PerformanceToken Num = 45BiLSTMTransformerGPT-2CodeGPTCompCoder(CodeGPT). (3) When our approach signiﬁcantly
improves the Compilation Rate, it does not sacriﬁce
the ﬂuency of the generated code. COMPCODER
obtains a comparable and even slightly better Edit
Similarity score than other baselines, indicating
that it effectively preserves the code ﬂuency.

Figure 4 presents more results of the code com-
pletion task in the setting of completing 30, 35,
40, and 45 tokens. COMPCODER still effectively
improves the Compilation Rate when generating
longer code. As the completion length increases,
our approach outperforms CodeGPT by 49.66,
47.68, 46.64, and 33.36 points in the setting of
completing 30, 35, 40, and 45 tokens, respectively.
On average, our approach outperforms CodeGPT
by 45 points across a different number of tokens
for the task of code completion.

Models

BiLSTM
Transformer
GPT-2
CodeGPT
PLBART
CodeT5
COMPCODER

ES

54.86
57.47
60.54
61.82
62.43
62.58
62.74

CR

48.7
55.6
63.3
70.3
71.9
73.1
96.2

Table 2: Results in the text-to-code generation task
evaluating with Edit Similarity (ES) and Compilation
Rate (CR), using the AdvTest dataset.

5.2 Text-to-Code Generation

Table 2 presents the results of the text-to-code gen-
eration task. We could see that: (1) COMPCODER
signiﬁcantly outperforms all other models w.r.t. the
Compilation Rate. E.g., COMPCODER achieves
23.1 points and 24.3 points improvements when
compared with PLBART and CodeT5 respectively.
(2) Compared to code completion task (Table 1),
all models in the text-to-code generation task have
relatively higher Compilation Rate. One of the
main reasons we think may be: code completion re-
quires the generated code to be constrained by the
existing (previous) code, which is a much stronger
restriction than the text-to-code generation.

5.3 Ablation Study

We compare several simpliﬁed versions of our
model to understand contributions of different com-
ponents, including the Reinforcement Learning
(RL) component and the discriminator’s effect for

Models

(1) CodeGPT
(2) w/ Dtrain
(3) w/ RL
(4) w/ RL+Dtrain
(5) w/ Dtrain+Dtest
(6) w/ RL+Dtrain+Dtest (Ours)

ES

CR

64.47
65.46
64.71
64.43
65.24
64.53

46.84
64.88
76.48
83.14
81.96
94.48

Table 3: Ablation study in the code completion task in
the setting of completing 25 code tokens.

both model training (Dtrain) and model inference
(Dtest). As a case study, we take the code comple-
tion task as an example in the setting of completing
25 tokens and present the results in Table 3.

Several meaningful observations can be drawn:
First, both RL (Row 2) and Dtrain (Row 3) effec-
tively increase the code Compilation Rate of the
generation model (CodeGPT in Row 1), which con-
ﬁrms that the two components we designed can
indeed improve the ability of the generator for com-
pilable code generation. Second, applying RL and
Dtrain together (Row 4) further improves the Com-
pilation Rate over their individual contributions.
Third, using the discriminator to select the output
during model inference stage (Dtest) is beneﬁcial.
It further boosts the Compilation Rate of vanilla
“Dtrain” by 17.08% (Row 5 v.s. Row 2) and boosts
“RL+Dtrain” by 11.34% (Row 6 v.s. Row 4). Forth,
these three components (RL, Dtrain, Dtest) that effec-
tively improve the Compilation Rate do not com-
promise the generation capability measured by the
Edit Similarity.

5.4 Case Study

To better understand the effectiveness of our pro-
posed approach, we present two cases for code com-
pletion and text-to-code generation tasks respec-
tively. For both CodeGPT and COMPCODER, we
present top-1 result in Figure 5. For code comple-
tion, we observe that CodeGPT can not complete
code with high quality (non-compilable), while
COMPCODER can complete the code well, and it
is exactly the same for the reference solution. For
text-to-code generation, we observe that although
both models can not generate exactly the same code
as the reference solution, COMPCODER generates a
compilable code at the function level. These results
reveal the effectiveness of our proposed approach
for compilable code generation.

top to model the reasoning process. Many unla-
beled programs are used for program repair with
self-supervised learning.

Beneﬁting from the strong power of pre-training
techniques (Devlin et al., 2019; Wang et al., 2021a)
in natural language processing, such as GPT (Rad-
ford and Narasimhan, 2018), BART (Lewis et al.,
2020), and T5 (Raffel et al., 2020), some recent
works attempt to pre-train language models on the
corpus of source code for code generation. Lu et al.
(2021) proposed CodeGPT follows the architec-
ture of GPT-2 (Radford et al., 2019), which is pre-
trained with a causal language modeling (CLM)
objective on large-scale source code. Ahmad et al.
(2021) proposed PLBART follows the architecture
of BART (Lewis et al., 2020), which is pre-trained
on Java and Python functions paired with code
comments via denoising autoencoding. Wang et al.
(2021b) proposed CodeT5 based on the T5 (Raffel
et al., 2020) architecture, which employs denois-
ing sequence-to-sequence pre-training on multiple
programming languages.

Reinforced Text Generation Reinforcement
learning (Sutton and Barto, 1998) has shown great
success in various tasks. It focuses on how agents
ought to take actions in an environment to max-
imize the cumulative reward, is well suited for
decision-making tasks. Ranzato et al. (2016)
were among the ﬁrst to apply REINFORCE algo-
rithm (Williams, 1992) to train recurrent neural
networks on sequence generation tasks, suggesting
that directly optimizing the metric used at the test
phase can lead to better results. Chen and Bansal
(2018) proposed a hybrid extractive-abstractive ar-
chitecture with policy-based reinforcement learn-
ing. They used an extractor agent to select salient
sentences and then employed an abstractor network
to rewrite these extracted sentences. Wan et al.
(2018); Wang et al. (2022) incorporated the tree
structure and sequential content of code snippets
and designed a deep reinforcement learning frame-
work optimized by the metric of BLEU to improve
the performance of the code summarization task.
Yao et al. (2019) proposed a reinforcement learning
framework, which encourages the code annotation
model to generate annotations that can be used for
code retrieval tasks. Korbak et al. (2021) proposed
an energy-based model with an imposed constraint
of generating only compilable sequences to im-
prove compilation rates of generated code.

Figure 5: Case study for code completion and text-to-
code generation tasks.

6 Related Work

Neural Code Generation With the rapid devel-
opment of Deep Learning (DL), some researchers
attempt to use DL for code generation tasks. Liu
et al. (2020a) proposed a neural architecture for
code completion task with multi-task learning
based on the architecture of Transformer-XL Dai
et al. (2019) and BiLSTM (Schuster and Paliwal,
1997). Kim et al. (2021) presented several ways of
feeding the code structure to Transformer (Vaswani
et al., 2017) and further improved the accuracy of
the code prediction (next token prediction) task.
Wei et al. (2019) adopted an encoder-decoder ar-
chitecture and utilized the relations between code
generation and code summarization to improve the
performance of both tasks. Yasunaga and Liang
(2021) proposed a new training approach for pro-
gram repair. They used the critic to check a ﬁxer’s
output on real bad inputs and add good outputs to
the training data, and trains a breaker to generate
realistic bad code from good code. Yasunaga and
Liang (2020) used compiler error messages to re-
pair programs. They designed a program-feedback
graph and then applied a graph neural network on

1 def extract_arguments(arguments, long_keys, key_prefix='--'):2     long_arguments = extract(arguments, long_keys)3     return dict(4         [(key.replace(key_prefix, ''), value) for key, value in long_arguments.items()]5     )Reference Code1 def extract_arguments(arguments, long_keys, key_prefix='--'):2     long_arguments = extract(arguments, long_keys)3     return dict(4         [(key.replace(key_prefix, ''), value)5                  CodeGPT.  .  .  ..  .  .  ..  .  .  .  .  .  .  ..  .  .  ..   .   .   ..   .   .   ..   .   .   .   .   .   .   ..   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .1 def extract_arguments(arguments, long_keys, key_prefix='--'):2     long_arguments = extract(arguments, long_keys)3     return dict(4         [(key.replace(key_prefix, ''), value) for key, value in long_arguments.items()]5     )COMPCODER.  .  .  ..  .  .  ..  .  .  .  .  .  .  ..  .  .  .Encode a Password    :param password: Password    :param algorithm    :param salt: Salt    :param iterations: iterations    :return: PBKDF2 hashed PasswordNatural Language Comment1 def encode(password, algorithm, salt, iterations):2     hash = hashlib.pbkdf2_hmac(digest().name, password.encode(), salt.encode(), iterations)3     encoded = base64.b64encode(hash).decode('ascii').strip()4     return "%s$%d$%s$%s" % (algorithm, iterations, salt, encoded)Reference Code1 def _encode_password(password, algorithm):2     salt = salt.encode('utf-8')3     iterations = int(iterations)4     pbkdf2 = PBKDF2(algorithm, salt,CodeGPT1 def encode_password(password, algorithm, salt, iterations):2     hasher = PBKDF2HMAC(3         algorithm=algorithm,4         salt=salt,5         iterations=iterations)6     hasher.update(password)7     return hasher.finalize()COMPCODERCode CompletionText-to-Code Generation.   .   .   ..   .   .   ..   .   .   ..   .   .   ..   .   .   ..   .   .   ..   .   .   ..   .   .   .   .   .   .   .✔✖✔✖.   .   .   ..   .   .   ..   .   .   .   .   .   .   ..   .   .   .   .   .   .   .7 Conclusion and Future Work

In this paper, we use the compilability signals in
two ways and design a novel method to jointly
train the generator and discriminator for compilable
code generation, called COMPCODER. Compre-
hensive experiments on two code generation tasks
demonstrate the effectiveness of COMPCODER, im-
proving the average compilation rate of state-of-
the-art CodeGPT from 44.18 to 89.18 in the code
completion task and from 70.3 to 96.2 in the text-
to-code generation task.

This work presents our preliminary attempt to
generate compilable code. Yet, considering the
compilation rate is not the whole story as it still
cannot guarantee the correctness of generated code.
As a future work, we would like to utilize unit tests
to evaluate the code correctness towards building
more useful code generation models.

Acknowledgements

This work is supported by National Natural Science
Foundation of China under Grant No. 61972290.
Yao Wan is partially supported by National Natu-
ral Science Foundation of China under Grant No.
62102157. Hao Wu is supported by National Natu-
ral Science Foundation of China under Grant No.
61962061.

References

Wasi Uddin Ahmad, Saikat Chakraborty, Baishakhi
Ray, and Kai-Wei Chang. 2021. Uniﬁed pre-training
In
for program understanding and generation.
Proceedings of the 2021 Conference of the North
American Chapter of the Association for Computa-
tional Linguistics: Human Language Technologies,
NAACL-HLT 2021, Online, June 6-11, 2021, pages
2655–2668. Association for Computational Linguis-
tics.

Xinyun Chen, Chang Liu, and Dawn Song. 2018. Tree-
to-tree neural networks for program translation. In
Advances in Neural Information Processing Systems
31: Annual Conference on Neural Information Pro-
cessing Systems 2018, NeurIPS 2018, December 3-8,
2018, Montréal, Canada, pages 2552–2562.

Yen-Chun Chen and Mohit Bansal. 2018. Fast abstrac-
tive summarization with reinforce-selected sentence
rewriting. In Proceedings of the 56th Annual Meet-
ing of the Association for Computational Linguistics,
ACL 2018, Melbourne, Australia, July 15-20, 2018,
Volume 1: Long Papers, pages 675–686. Association
for Computational Linguistics.

Zimin Chen, Steve Kommrusch, Michele Tufano,
Louis-Noël Pouchet, Denys Poshyvanyk, and Mar-
tin Monperrus. 2021.
Sequencer: Sequence-to-
sequence learning for end-to-end program repair.
IEEE Trans. Software Eng., 47(9):1943–1959.

Zihang Dai, Zhilin Yang, Yiming Yang, Jaime G. Car-
bonell, Quoc Viet Le, and Ruslan Salakhutdinov.
2019. Transformer-xl: Attentive language models
In Proceedings of
beyond a ﬁxed-length context.
the 57th Conference of the Association for Compu-
tational Linguistics, ACL 2019, Florence, Italy, July
28- August 2, 2019, Volume 1: Long Papers, pages
2978–2988. Association for Computational Linguis-
tics.

Jacob Devlin, Ming-Wei Chang, Kenton Lee, and
Kristina Toutanova. 2019. BERT: pre-training of
deep bidirectional transformers for language under-
In Proceedings of the 2019 Conference
standing.
of the North American Chapter of the Association
for Computational Linguistics: Human Language
Technologies, NAACL-HLT 2019, Minneapolis, MN,
USA, June 2-7, 2019, Volume 1 (Long and Short Pa-
pers), pages 4171–4186. Association for Computa-
tional Linguistics.

Tatsunori B. Hashimoto, Kelvin Guu, Yonatan Oren,
and Percy Liang. 2018. A retrieve-and-edit frame-
work for predicting structured outputs. In Advances
in Neural Information Processing Systems 31: An-
nual Conference on Neural Information Processing
Systems 2018, NeurIPS 2018, December 3-8, 2018,
Montréal, Canada, pages 10073–10083.

Hamel Husain, Ho-Hsiang Wu, Tiferet Gazit, Miltiadis
Allamanis, and Marc Brockschmidt. 2019. Code-
searchnet challenge: Evaluating the state of seman-
tic code search. CoRR, abs/1909.09436.

Seohyun Kim, Jinman Zhao, Yuchi Tian, and Satish
Chandra. 2021. Code prediction by feeding trees
In 43rd IEEE/ACM International
to transformers.
Conference on Software Engineering, ICSE 2021,
Madrid, Spain, 22-30 May 2021, pages 150–162.
IEEE.

Tomasz Korbak, Hady ElSahar, Marc Dymetman, and
Germán Kruszewski. 2021. Energy-based models
for code generation under compilability constraints.
arXiv preprint arXiv: 2106.04985.

Sumith Kulal, Panupong Pasupat, Kartik Chandra,
Mina Lee, Oded Padon, Alex Aiken, and Percy
Liang. 2019. Spoc: Search-based pseudocode to
In Advances in Neural Information Process-
code.
ing Systems 32: Annual Conference on Neural Infor-
mation Processing Systems 2019, NeurIPS 2019, De-
cember 8-14, 2019, Vancouver, BC, Canada, pages
11883–11894.

Mike Lewis, Yinhan Liu, Naman Goyal, Mar-
jan Ghazvininejad, Abdelrahman Mohamed, Omer
Levy, Veselin Stoyanov, and Luke Zettlemoyer.
2020. BART: denoising sequence-to-sequence pre-
training for natural language generation, translation,

and comprehension. In Proceedings of the 58th An-
nual Meeting of the Association for Computational
Linguistics, ACL 2020, Online, July 5-10, 2020,
pages 7871–7880. Association for Computational
Linguistics.

Jian Li, Yue Wang, Michael R. Lyu, and Irwin King.
2018. Code completion with neural attention and
In Proceedings of the Twenty-
pointer networks.
Seventh International Joint Conference on Artiﬁcial
Intelligence, IJCAI 2018, July 13-19, 2018, Stock-
holm, Sweden, pages 4159–4165. ijcai.org.

Fang Liu, Ge Li, Bolin Wei, Xin Xia, Zhiyi Fu, and
Zhi Jin. 2020a. A self-attentional neural architec-
ture for code completion with multi-task learning.
In ICPC ’20: 28th International Conference on
Program Comprehension, Seoul, Republic of Korea,
July 13-15, 2020, pages 37–47. ACM.

Fang Liu, Ge Li, Yunfei Zhao, and Zhi Jin. 2020b.
Multi-task learning based pre-trained language
model for code completion. In 35th IEEE/ACM In-
ternational Conference on Automated Software En-
gineering, ASE 2020, Melbourne, Australia, Septem-
ber 21-25, 2020, pages 473–485. IEEE.

Shuai Lu, Daya Guo, Shuo Ren, Junjie Huang, Alexey
Svyatkovskiy, Ambrosio Blanco, Colin B. Clement,
Dawn Drain, Daxin Jiang, Duyu Tang, Ge Li, Li-
dong Zhou, Linjun Shou, Long Zhou, Michele Tu-
fano, Ming Gong, Ming Zhou, Nan Duan, Neel Sun-
daresan, Shao Kun Deng, Shengyu Fu, and Shujie
Liu. 2021. Codexglue: A machine learning bench-
mark dataset for code understanding and generation.
CoRR, abs/2102.04664.

Thang Luong, Hieu Pham, and Christopher D. Man-
ning. 2015. Effective approaches to attention-based
In Proceedings of the
neural machine translation.
2015 Conference on Empirical Methods in Natural
Language Processing, EMNLP 2015, Lisbon, Portu-
gal, September 17-21, 2015, pages 1412–1421. The
Association for Computational Linguistics.

Maxwell I. Nye, Luke B. Hewitt, Joshua B. Tenen-
baum, and Armando Solar-Lezama. 2019. Learning
In Proceedings of the
to infer program sketches.
36th International Conference on Machine Learn-
ing, ICML 2019, 9-15 June 2019, Long Beach, Cali-
fornia, USA, volume 97 of Proceedings of Machine
Learning Research, pages 4861–4870. PMLR.

Md. Rizwan Parvez, Wasi Uddin Ahmad, Saikat
Chakraborty, Baishakhi Ray, and Kai-Wei Chang.
2021. Retrieval augmented code generation and
In Findings of the Association for
summarization.
Computational Linguistics: EMNLP 2021, Virtual
Event / Punta Cana, Dominican Republic, 16-20
November, 2021, pages 2719–2734. Association for
Computational Linguistics.

Maxim Rabinovich, Mitchell Stern, and Dan Klein.
2017. Abstract syntax networks for code generation
and semantic parsing. In Proceedings of the 55th An-
nual Meeting of the Association for Computational

Linguistics, ACL 2017, Vancouver, Canada, July 30 -
August 4, Volume 1: Long Papers, pages 1139–1149.
Association for Computational Linguistics.

Alec Radford and Karthik Narasimhan. 2018.

Im-
proving language understanding by generative pre-
training.

Alec Radford, Jeff Wu, Rewon Child, David Luan,
Dario Amodei, and Ilya Sutskever. 2019. Language
models are unsupervised multitask learners.

Colin Raffel, Noam Shazeer, Adam Roberts, Katherine
Lee, Sharan Narang, Michael Matena, Yanqi Zhou,
Wei Li, and Peter J. Liu. 2020. Exploring the limits
of transfer learning with a uniﬁed text-to-text trans-
former. J. Mach. Learn. Res., 21:140:1–140:67.

Marc’Aurelio Ranzato, Sumit Chopra, Michael Auli,
and Wojciech Zaremba. 2016. Sequence level train-
In 4th Inter-
ing with recurrent neural networks.
national Conference on Learning Representations,
ICLR 2016, San Juan, Puerto Rico, May 2-4, 2016,
Conference Track Proceedings.

John Schulman, Filip Wolski, Prafulla Dhariwal, Alec
Radford, and Oleg Klimov. 2017. Proximal pol-
icy optimization algorithms. arXiv preprint arXiv:
1707.06347.

Mike Schuster and Kuldip K. Paliwal. 1997. Bidirec-
IEEE Trans. Sig-

tional recurrent neural networks.
nal Process., 45(11):2673–2681.

Richard S. Sutton and Andrew G. Barto. 1998. Intro-

duction to reinforcement learning.

Alexey Svyatkovskiy, Shao Kun Deng, Shengyu Fu,
Intellicode compose:
and Neel Sundaresan. 2020.
In ESEC/FSE
code generation using transformer.
’20: 28th ACM Joint European Software Engineer-
ing Conference and Symposium on the Founda-
tions of Software Engineering, Virtual Event, USA,
November 8-13, 2020, pages 1433–1443. ACM.

Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob
Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz
Kaiser, and Illia Polosukhin. 2017. Attention is all
you need. In Advances in Neural Information Pro-
cessing Systems 30: Annual Conference on Neural
Information Processing Systems 2017, December 4-
9, 2017, Long Beach, CA, USA, pages 5998–6008.

Yao Wan, Zhou Zhao, Min Yang, Guandong Xu,
Haochao Ying, Jian Wu, and Philip S. Yu. 2018. Im-
proving automatic source code summarization via
deep reinforcement learning. In Proceedings of the
33rd ACM/IEEE International Conference on Auto-
mated Software Engineering, ASE 2018, Montpellier,
France, September 3-7, 2018, pages 397–407. ACM.

Wenhua Wang, Yuqun Zhang, Yulei Sui, Yao Wan,
Zhou Zhao, Jian Wu, Philip S. Yu, and Guan-
Reinforcement-learning-guided
dong Xu. 2022.
source code summarization using hierarchical atten-
IEEE Transactions on Software Engineering,
tion.
48(1):102–119.

Long Papers, pages 440–450. Association for Com-
putational Linguistics.

Daniel M. Ziegler, Nisan Stiennon, Jeffrey Wu, Tom B.
Brown, Alec Radford, Dario Amodei, Paul F. Chris-
tiano, and Geoffrey Irving. 2019. Fine-tuning lan-
guage models from human preferences. CoRR,
abs/1909.08593.

Xin Wang, Yasheng Wang, Fei Mi, Pingyi Zhou, Yao
Wan, Xiao Liu, Li Li, Hao Wu, Jin Liu, and
Xin Jiang. 2021. Syncobert: Syntax-guided multi-
modal contrastive pre-training for code representa-
tion. arXiv preprint arXiv: 2108.04556.

Xin Wang, Pingyi Zhou, Yasheng Wang, Xiao Liu, Jin
Liu, and Hao Wu. 2021a. Servicebert: A pre-trained
model for web service tagging and recommenda-
In Service-Oriented Computing - 19th Inter-
tion.
national Conference, ICSOC 2021, Virtual Event,
November 22-25, 2021, Proceedings, volume 13121
of Lecture Notes in Computer Science, pages 464–
478. Springer.

Yue Wang, Weishi Wang, Shaﬁq R. Joty, and Steven
C. H. Hoi. 2021b. Codet5:
Identiﬁer-aware uni-
ﬁed pre-trained encoder-decoder models for code un-
In Proceedings of the
derstanding and generation.
2021 Conference on Empirical Methods in Natural
Language Processing, EMNLP 2021, Virtual Event
/ Punta Cana, Dominican Republic, 7-11 November,
2021, pages 8696–8708. Association for Computa-
tional Linguistics.

Bolin Wei, Ge Li, Xin Xia, Zhiyi Fu, and Zhi Jin. 2019.
Code generation as a dual task of code summariza-
tion. In Advances in Neural Information Processing
Systems 32: Annual Conference on Neural Informa-
tion Processing Systems 2019, NeurIPS 2019, De-
cember 8-14, 2019, Vancouver, BC, Canada, pages
6559–6569.

Ronald J. Williams. 1992. Simple statistical gradient-
following algorithms for connectionist reinforce-
ment learning. Mach. Learn., 8:229–256.

Ziyu Yao, Jayavardhan Reddy Peddamail, and Huan
Sun. 2019. Coacor: Code annotation for code re-
In The World
trieval with reinforcement learning.
Wide Web Conference, WWW 2019, San Francisco,
CA, USA, May 13-17, 2019, pages 2203–2214.
ACM.

Michihiro Yasunaga and Percy Liang. 2020. Graph-
based, self-supervised program repair from diagnos-
In Proceedings of the 37th Inter-
tic feedback.
national Conference on Machine Learning, ICML
2020, 13-18 July 2020, Virtual Event, volume 119 of
Proceedings of Machine Learning Research, pages
10799–10808. PMLR.

Michihiro Yasunaga and Percy Liang. 2021. Break-
it-ﬁx-it: Unsupervised learning for program repair.
In Proceedings of the 38th International Confer-
ence on Machine Learning, ICML 2021, 18-24 July
2021, Virtual Event, volume 139 of Proceedings of
Machine Learning Research, pages 11941–11952.
PMLR.

Pengcheng Yin and Graham Neubig. 2017. A syntactic
neural model for general-purpose code generation.
In Proceedings of the 55th Annual Meeting of the As-
sociation for Computational Linguistics, ACL 2017,
Vancouver, Canada, July 30 - August 4, Volume 1:

