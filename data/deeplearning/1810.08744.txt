9
1
0
2

n
u
J

1
2

]

G
L
.
s
c
[

2
v
4
4
7
8
0
.
0
1
8
1
:
v
i
X
r
a

MMLSpark: Unifying Machine Learning Ecosystems at Massive Scales

Mark Hamilton 1 Sudarshan Raghunathan 2 Ilya Matiach 3 Andrew Schonhoffer 3 Anand Raman 2
Eli Barzilay 1 Karthik Rajendran 4 5 Dalitso Banda 4 5 Casey Jisoo Hong 4 5 Manon Knoertzer 4 5 Ben Brodsky 2
Minsoo Thigpen 4 Janhavi Suresh Mahajan 4 Courtney Cochrane 4 Abhiram Eswaran 4 Ari Green 4

Abstract

We introduce Microsoft Machine Learning for
Apache Spark (MMLSpark), an open-source li-
brary that expands the Apache Spark distributed
computing library to tackle problems in deep
learning, micro-service orchestration, gradient
boosting, model interpretability, and other areas
of modern machine learning. We also present a
novel machine learning deployment system called
Spark Serving that can deploy Apache Spark pro-
grams as distributed, sub-millisecond latency web
services with signiﬁcantly greater ﬂexibility and
lower latencies than existing frameworks. Spark
Serving generalizes beyond just map-style compu-
tations and allows distributed aggregations, joins,
and shufﬂes and allows users to leverage the
same cluster for both training and deployment.
Our contributions allow easy composition across
machine learning frameworks, compute modes
(batch, streaming, and RESTful web serving) and
cluster types (static, elastic, and serverless). We
demonstrate the value of MMLSpark by creating a
method for deep object detection capable of learn-
ing without human labeled data and demonstrate
its effectiveness for Snow Leopard conservation.
We also demonstrate its ability to create large-
scale image search engines.

1. Introduction

As the ﬁeld of machine learning has advanced, frameworks
for using, authoring, and training machine learning systems

*Equal contribution 1Microsoft Applied AI, Cambridge, Mas-
sachusetts, USA 2Microsoft Applied AI, Redmond, Washing-
ton, USA 3Microsoft Azure Machine Learning, Cambridge, Mas-
sachusetts, USA 4Microsoft AI Development Acceleration Pro-
gram, Cambridge, Massachusetts, USA 5AUTHORERR: Miss-
ing \icmlafﬁliation.
. Correspondence to: Mark Hamilton
<marhamil@microsoft.com>.

Proceedings of the 36 th International Conference on Machine
Learning, Long Beach, California, PMLR 97, 2019. Copyright
2019 by the author(s).

have proliferated. These different frameworks often have
dramatically different APIs, data models, usage patterns,
and scalability considerations. This heterogeneity makes
it difﬁcult to combine systems and complicates production
deployments. In this work, we present Microsoft Machine
Learning for Apache Spark (MMLSpark), an ecosystem that
aims to unify major machine learning workloads into a sin-
gle API for execution in a variety of distributed production
grade environments and languages. We describe the tech-
niques and principles used to unify a representative sample
of machine learning technologies, each with its own soft-
ware stack, communication requirements, and paradigms.
We also introduce tools for deploying these technologies
as distributed real-time web services. Code and documen-
tation for MMLSpark can be found through our website,
https://aka.ms/spark.

2. Background

Throughout this work we build upon the distributed comput-
ing framework Apache Spark (Zaharia et al., 2016). Spark
is capable of a broad range of workloads and applications
such as fault-tolerant and distributed map, reduce, ﬁlter, and
aggregation style programs. Spark improves on its prede-
cessors MapReduce and Hadoop by reducing disk IO with
in memory computing, and whole program optimization
(Dean & Ghemawat, 2008; Shvachko et al., 2010). Spark
clusters can adaptively resize to compute a workload efﬁ-
ciently (elasticity) and can run on resource managers such
as Yarn, Mesos, Kubernetes, or manually created clusters.
Furthermore, Spark has language bindings in several popu-
lar languages like Scala, Java, Python, R, Julia, C# and F#,
making it usable from almost any project.

In recent years, Spark has expanded its scope to support
SQL, streaming, machine learning, and graph style compu-
tations (Armbrust et al., 2015; Meng et al., 2016; Xin et al.,
2013). This broad set of APIs allows a rich space of compu-
tations that we can leverage for our work. More speciﬁcally,
we build upon the SparkML API, which is similar to the
popular Python machine learning library, scikit-learn (Buit-
inck et al., 2013). Like scikit-learn, all SparkML models
have the same API, which makes it easy to create, substitute,

 
 
 
 
 
 
MMLSpark: Unifying Machine Learning Ecosystems at Massive Scales

and compose machine learning algorithms into “pipelines”.
However, SparkML has several key advantages such as
limitless scalability, streaming compatibility, support for
structured datasets, broad language support, a ﬂuent API
for initializing complex algorithms, and a type system that
differentiates computations based on whether they extract
state (learn) from data. In addition, Spark clusters can use a
wide variety of hardware SKUs making it possible to lever-
age modern advances in GPU accelerated frameworks like
Tensorﬂow, CNTK, and PyTorch (Abadi et al., 2016; Seide
& Agarwal, 2016; Paszke et al., 2017). These properties
make the SparkML API a natural and principled choice to
unify the APIs of other machine learning frameworks.

Across the broader computing literature, many have turned
to intermediate languages to “unify” and integrate disparate
forms of computation. One of the most popular of these lan-
guages is the Hypertext Transfer Protocol (HTTP) used
widely throughout internet communications. To enable
broad adoption and integration of code, one simply needs to
create a web-hosted HTTP endpoint or “service”. Putting
compute behind an intermediate language allows different
system components to scale independently to minimize bot-
tlenecks. If services reside on the same machine, one can
use local networking capabilities to bypass internet data
transfer costs and come closer to the latency of normal func-
tion dispatch. This pattern is referred to as a “micro-service”
architecture, and powers many of today’s large-scale appli-
cations (Sill, 2016).

Many machine learning workﬂows rely on deploying
learned models as web endpoints for use in front-end appli-
cations. In the Spark ecosystem, there are several ways to
deploy applications as web services such as Azure Machine
Learning Services (AML), Clipper, and MLeap. However,
these frameworks all compromise on the breadth of models
they export, or the latency of their deployed services. AML
deploys PySpark code in a dockerized Flask application that
uses Spark’s Batch API on a single node standalone cluster
(Microsoft, a; Grinberg, 2018). Clipper uses on an interme-
diate RPC communication service to invoke a Spark batch
job for each request (Crankshaw et al., 2017). Both method
use Spark’s Batch API which adds large overheads. Further-
more, if back-end containers are isolated, this precludes ser-
vices with inter-node communication like shufﬂes and joins.
MLeap achieves millisecond latencies by re-implementing
SparkML models in single threaded Scala, and exporting
SparkML pipelines to this alternate implementation (Com-
bust). This incurs a twofold development cost, a lag behind
the SparkML library, and a export limitation to models in
Spark’s core, which is a small subset of the ecosystem. In
section 3.3, we present a novel method, Spark Serving, that
achieves millisecond latencies without compromising the
breadth or latency of models and does not rely on model
export. This makes the transition from distributed training

to distributed serving seamless and instant.

Many companies such as Microsoft, Amazon, IBM, and
Google have embraced model deployment with web ser-
vices to provide pre-built intelligent algorithms for a wide
range of applications (Jackson et al., 2010; Microsoft, c;
High, 2012). This standardization enables easy use of cloud
intelligence and abstracts away implementation details, en-
vironment setup, and compute requirements. Furthermore,
intelligent services allow application developers to quickly
use existing state of the art models to prototype ideas. In the
Azure ecosystem, the Cognitive Services provide intelligent
services in domains such as text, vision, speech, search,
time series, and geospatial workloads.

3. Contributions

In this section we describe our contributions in three key ar-
eas: 1) Unifying several Machine Learning ecosystems with
Spark. 2) Integrating Spark with the networking protocol
HTTP and several intelligent web services. 3) Deploying
Spark computations as distributed web services with Spark
Serving.

These contributions allow users to create scalable machine
learning systems that draw from a wide variety of libraries
and expose these contributions as web services for others
to use. All of these contributions carry the common theme
of building a single distributed API that can easily and
elegantly create a variety of different intelligent applications.
In Section 4 we show how to combine these contributions
to solve problems in unsupervised object detection, wildlife
ecology, and visual search engine creation.

3.1. Algorithms and Frameworks Uniﬁed in

MMLSpark

3.1.1. DEEP LEARNING

To enable GPU accelerated deep learning on Spark, we have
previously parallelized Microsoft’s deep learning frame-
work, the Cognitive Toolkit (CNTK) (Seide & Agarwal,
2016; Hamilton et al., 2018). This framework powers
roughly 80% of Microsoft’s internal deep learning work-
loads and is ﬂexible enough to create most models described
in the deep learning literature. CNTK is similar to other
automatic differentiation systems like Tensorﬂow, PyTorch,
and MxNet as they all create symbolic computation graphs
that automatically differentiate and compile to machine code.
These tools liberate developers and researchers from the dif-
ﬁcult task of deriving training algorithms and writing GPU
accelerated code.

CNTK’s core functionality is written in C++ but exposed to
C# and Python through bindings. To integrate this frame-
work into Spark, we used the Simple Wrapper and Interface

MMLSpark: Unifying Machine Learning Ecosystems at Massive Scales

Generator (SWIG) to contribute a set of Java Bindings to
CNTK (Beazley, 1996). These bindings allow users to
call and train CNTK models from Java, Scala and other
JVM based languages. We used these bindings to create
a SparkML transformer to distribute CNTK in Scala. Ad-
ditionally we automatically generate PySpark and Spark-
lyR bindings for all of MMLSpark’s Scala transformers,
so all contributions in this work are usable across differ-
ent languages. To improve our implementation’s perfor-
mance we broadcast the model to each worker using Bit-
Torrent broadcasting, re-use C++ objects to reduce garbage
collection overhead, asynchronously mini-batch data, and
share weights between local threads to reduce memory over-
head. With CNTK on Spark, users can embed any deep
network into parallel maps, SQL queries, and streaming
pipelines. We also contribute and host a large cloud reposi-
tory of trained models and tools to perform image classiﬁ-
cation with transfer learning. We have utilized this work for
wildlife recognition, bio-medical entity extraction, and gas
station ﬁre detection (Hamilton et al., 2018).

We released our
implementation concurrently with
Databrick’s “Deep Learning Pipelines” that provides an
analogous integration of Spark and Tensorﬂow (Databricks).
Our two integrations share the same API making it easy to
use CNTK and/or Tensorﬂow inside of SparkML pipelines
without code changes.

3.1.2. GRADIENT BOOSTING AND DECISION TREES

Though Tensorﬂow and CNTK provide GPU enabled deep
networks, these frameworks are optimized for differentiable
models. To efﬁciently learn tree/forest-based models, many
turn to GPU enabled gradient boosting libraries such as
XGBoost or LightGBM (Chen & Guestrin, 2016; Ke et al.,
2017). We contribute an integration of LightGBM into
Spark to enable large scale optimized gradient boosting
within SparkML pipelines. LightGBM is one of the most
performant decision tree frameworks and can use socket or
Message Passing Interface (MPI) communication schemes
that communicate much more efﬁciently than SparkML’s
Gradient Boosted Tree implementation. As a result, Light-
GBM trains up to 30% faster than SparkML’s gradient
boosted tree implementation and exposes many more fea-
tures, optimization metrics, and growing/pruning parame-
ters. Like CNTK, LightGBM is written in C++ and pub-
lishes bindings for use in other languages. For this work,
we used SWIG to contribute a set of Java bindings to Light-
GBM for use in Spark. Unlike our work with CNTK on
Spark, LightGBM training involves nontrivial MPI com-
munication between workers. To unify Spark’s API with
LightGBM’s communication scheme, we transfer control to
LightGBM with a Spark “MapPartitions” operation. More
speciﬁcally, we communicate the hostnames of all workers
to the driver node of the Spark cluster and use this informa-

tion to launch an MPI ring. The Spark worker processes
use the Java Native Interface to transfer control and data to
the LightGBM processes. This integration allows users to
create performant models for classiﬁcation, quantile regres-
sion, and other applications that excel in discrete feature
domains.

3.1.3. MODEL INTERPRETABILITY

In addition to integrating frameworks into Spark through
transfers of control, we have also expanded SparkML’s na-
tive library of algorithms. One example is our distributed
implementation of Local Interpretable Model Agnostic Ex-
planations (LIME) (Ribeiro et al., 2016). LIME provides
a way to “interpret” the predictions of any model without
reference to that model’s functional form. More concretely,
LIME interprets black box functions though a locally lin-
ear approximation constructed from a sampling procedure.
LIME, and its generalization SHAP, rely solely on function
evaluations and can and apply to any black box algorithm
(Lundberg & Lee, 2017). We consider other methods such
as DeepLIFT and feature gradients outside the scope of this
work because SparkML models are not necessarily differen-
tiable (Shrikumar et al., 2017).

Intuitively speaking, if “turning off” a part of the model
input dramatically changes a model’s output, that part is
“important”. More concretely, LIME for image classiﬁers
creates thousands of perturbed images by setting random
chunks or “superpixels” of the image to a neutral color. Next,
it feeds each of these perturbed images through the model to
see how the perturbations affect the model’s output. Finally,
it uses a locally weighted Lasso model to learn a linear
mapping between a Boolean vector representing the “states”
of the superpixels to the model’s outputs. Text and tabular
LIME employ similar featurization schemes, and we refer
readers to (Ribeiro et al., 2016) for detailed descriptions.

To interpret a classiﬁer’s predictions for an image, one must
evaluate the classiﬁer on thousands of perturbed images to
sufﬁciently sample the superpixel state space. Practically
speaking, if it takes 1 hour to score a model on your dataset,
it would take ≈ 50 days to interpret this dataset with LIME.
We have created a distributed implementation to reduce this
massive computation time. LIME affords several possible
distributed implementations, and we have chosen a paral-
lelization scheme that speeds each individual interpretation.
More speciﬁcally, we parallelize the superpixel decompo-
sitions over the input images. Next, we iterate through the
superpixel decompositions and create a new parallel col-
lection of “state samples” for each input image. We then
perturb these images and apply the model in parallel. Fi-
nally, we ﬁt a distributed linear model to the inner collection
and add its weights to the original parallel collection. Be-
cause of this nontrivial parallelization scheme, this kind of

MMLSpark: Unifying Machine Learning Ecosystems at Massive Scales

integration beneﬁted from a complete re-write in fast com-
piled Scala and Spark SQL, as opposed to using a tool like
Py4J to integrate the existing LIME repository into Spark.

3.2. Unifying Microservices with Spark

In Section 3.1 we explored three contributions that unify
Spark with other Machine Learning tools using the Java
Native Interface (JNI) and function dispatch. These meth-
ods are efﬁcient, but require re-implementing code in Scala
or auto-generating wrappers from existing code. For many
frameworks, these dispatch-based integrations are impos-
sible due to differences in language, operating system, or
computational architecture. For these cases, we can utilize
inter-process communication protocols like HTTP to bridge
the gap between systems.

3.2.1. HTTP ON SPARK

We present HTTP on Spark, an integration between the en-
tire HTTP communication protocol and Spark SQL. HTTP
on Spark allows Spark users to leverage the parallel network-
ing capabilities of their cluster to integrate any local, docker,
or web service. At a high level, HTTP on Spark provides a
simple and principled way to integrate any framework into
the Spark ecosystem. The contribution adds HTTP Request
and Response types to the Spark SQL schema so that users
can create and manipulate their requests and responses using
SQL operations, maps, reduces, and ﬁlters. When combined
with SparkML, users can chain services together, allowing
Spark to function as a distributed micro-service orchestrator.
HTTP on Spark also automatically provides asynchronous
parallelism, batching, throttling, and exponential back-offs
for failed requests.

3.2.2. THE COGNITIVE SERVICES ON SPARK

We have built on HTTP on Spark to create a simple and
powerful integration between the Microsoft Cognitive Ser-
vices and Spark. The Cognitive Services on Spark allows
users to embed general purpose and continuously improv-
ing intelligent models directly into their Spark and SQL
computations. This contribution aims to liberate users from
low level networking details, so they can focus on creating
intelligent distributed applications. Each Cognitive Service
is a SparkML transformer, so users can add services to ex-
isting SparkML pipelines. We introduce a new class of
model parameters to the SparkML framework that allow
users to parameterize models by either a single scalar value
or vectorize the requests with columns of the DataFrame.
This syntax yields a succinct yet powerful ﬂuent query lan-
guage that offers a full distributed parameterization without
clutter. For example, by vectorizing the “subscription key”
parameter, users can distribute requests across several ac-
counts, regions, or deployments to maximize throughput

and resiliency to error.

Once can combine HTTP on Spark with Kubernetes or
other container orchestrators to deploy services directly onto
Spark worker machines (Rensin, 2015). This enables near
native integration speeds as requests do not have to travel
across machines. The cognitive services on Spark can also
call the newly released containerized cognitive services,
which dramatically reduces the latency of cognitive service
pipelines. We have contributed a helm chart for deploying
a Spark based microservice architecture with containerized
cognitive services, load balancers for Spark Serving, and
integration with the newly released second generation of
Azure Storage with a single command (Norm Estabrook;
Sayfan, 2017). Figure 1 shows a diagram of the aforemen-
tioned architecture.

Figure 1. Architecture diagram of our integration of cloud and
containerized Cognitive Services. Architecture depicted on Kuber-
netes, but any container orchestrator could deploy the same. Load
balancers expose deployed Spark Serving models, job submission
endpoints, and monitoring frontends. This helm chart can also
leverage our integration between Azure Search and Spark. Note
that we omit Kubernetes and Spark head nodes for simplicity.

3.2.3. AZURE SEARCH SINK FOR SPARK

We demonstrate the ﬂexibility and robustness of the HTTP
on Spark framework by contributing an integration between
Spark and Azure Search. Azure Search is a cloud database
that supports rapid information retrieval and query execution
on heterogeneous, unstructured data (Microsoft, b). Azure
Search leverages elastic search to index documents and pro-
vide REST APIs for document search on linguistic similarity
and a variety of other ﬁlters and logical constraints. With
this integration, users can leverage the frameworks men-
tioned in 3.1 to enrich their documents in parallel prior to
indexing.

3.3. Spark Serving: Scalable Real-Time Web Serving

Through HTTP on Spark, we have enabled Spark as a dis-
tributed web client. In this work we also contribute Spark

MMLSpark: Unifying Machine Learning Ecosystems at Massive Scales

Serving, a framework that allows Spark clusters to oper-
ate as distributed web servers. Spark Serving builds upon
Spark’s Structured Streaming library that transforms ex-
isting Spark SQL computations into continuously running
streaming queries. Structured Streaming supports a large
majority of Spark primitives including maps, ﬁlters, aggre-
gations, and joins. To convert a batch query to a streaming
query, users only need to change a single line of dataset
reading/writing code and can keep all other computational
logic in place. We extend this easy to use API to web serv-
ing by creating novel paired sources and sinks that manage a
service layer. Intuitively speaking, a web service is a stream-
ing pipeline where the data source and the data sink are the
same HTTP request.

Spark Serving can deploy any Spark computation as a web
service including all of our contributions (CNTK, Light-
GBM, SparkML, Cognitive Services, HTTP Services), arbi-
trary Python, R, Scala, Java, and all compositions and com-
binations therein. Through other open source contributions
in the Spark ecosystem, frameworks such as Tensorﬂow,
XGBoost, and Scikit-learn models join this list.

3.3.1. IMPLEMENTATION AND ARCHITECTURE

Under the hood, each Spark worker/executor manages a web
service that en-queues incoming data in an efﬁcient parallel
data structure that supports constant time routing, addition,
deletion, and load balancing across the multiple threads.
Each worker node manages a public service for accepting
incoming requests, and an internal routing service to send
response data to the originating request (on a potentially
different node after a shufﬂe). The worker services commu-
nicate their locations and statuses to a monitor service on
the driver node. This lets future developers create hooks for
their own load balancers, and lets users understand the state
and locations of their servers.

If one uses an external load balancer, each request is routed
directly to a worker, skipping the costly hop from head
node to worker node that is common in other frameworks.
The worker converts each request to our Spark SQL type
for an HTTP Request (The same types used in HTTP on
Spark), and a unique routing address to ensure the reply
can route to the originating request. Once the data is con-
verted to Spark’s SQL representation it ﬂows through the
computational pipeline like any other Spark data.

To reply to incoming requests, Spark Serving leverages a
new data sink that uses routing IDs and SQL objects for
HTTP Responses to reply to the stored request. If the re-
sponse ends up on a different machine then the originating
request, a routing service sends the request to the appropri-
ate machine through an internal network. If the request and
response are on the same machine, Spark Serving uses faster
function dispatch to reply. In the future, we hope to explore

whether the routing service could leverage Spark’s under-
lying shufﬂe capabilities to speed data transfer throughput.
Our choice of HTTP Request and Response Data types en-
ables users to work with the entire breadth of the HTTP
protocol for full generality and customizability. Figure 2
depicts a schematic overview of the architecture of Spark
Serving, and the hotpath during request and response time.

Figure 2. Overview of Spark Serving architecture during request
time (a) and response time (b). Requests are routed to each worker
through a load balancer where they can ﬂow through the pipeline
when partitions are formed. At reply time, workers use paired
ID rows to route information to the correct machines that can
reply to the open request.
If the request resides on the same
machine, Spark Serving uses function dispatch instead of inter-
process communication

As of MMLSpark v0.14, we have integrated Spark Serving
with Spark’s new Continuous Processing feature. Continu-
ous Processing dramatically reduces the latency of stream-
ing pipelines from ≈ 100ms to 1ms. This acceleration
enables real-time web services and machine learning appli-
cations. Minibatch processing can still be used to maximize
throughput, and MMLSpark also provides a batching API
for use in continuous processing.

To the authors’ knowledge, Spark Serving is the only serving
framework that leverages an existing Spark cluster’s work-
ers to serve models. As a result, developers do not need to
re-implement and export their models into other languages,
such as MLeap, to create web services (Combust). Fur-
thermore, frameworks that require additional run-times add

MMLSpark: Unifying Machine Learning Ecosystems at Massive Scales

Table 1. Latencies of various Spark deployment methods in mil-
liseconds. We compare Azure Machine Learning Services (AML),
Clipper, and MLeap against Spark Serving with local services on
an azure virtual machine for reliability. We explore the latency
with a pipeline of string indexing, one hot encoding and logistic
regression (LR), and a SQLTransformer select statement (SQL).
Note that MLeap cannot export SQLTransformer SparkML mod-
els.

4. Applications

We have used MMLSpark to power engagements in a wide
variety of machine learning domains, such as text, image,
and speech domains.
In this section, we highlight the
aforementioned contributions in our ongoing work to use
MMLSpark for wildlife conservation and custom search
engine creation.

METHOD

LR

SELECT

4.1. Snow Leopard Conservation

AML
CLIPPER
MLEAP
SPARK SERVING

530.3 ± 32.1
626.8 ± 332.1
3.18 ± 1.84
2.13 ± .96

179.5 ± 19.5
403.6 ± 280.0
N/A
1.81 ± .73

Table 2. Comparison of features across different Spark Deploy-
ment systems

FEATURE

AML CLIPPER MLEAP

√

√

ANY
SPARKML
ANY
SPARK
(NARROW)
×
JOINS
AGGREGATES ×
×
SHUFFLES
×
MILLISECOND
LATENCY

√

√

×
×
×
×

×

×

×
×
×
√

SPARK
SERVING
√

√

√
√
√
√

Snow leopards are dwindling due to poaching, mining, and
retribution killing yet we know little about how to best
protect them. Currently, researchers estimate that there
are only about four thousand to seven thousand individual
animals within a potential 2 million square kilometer range
of rugged central Asian mountains (Taubmann et al., 2016).
Our collaborators, the Snow Leopard Trust, have deployed
hundreds of motion sensitive cameras across large areas
of Snow Leopard territory to help monitor this population
(Taubmann et al., 2016). Over the years, these cameras have
produced over 1 million images, but most of these images
are of goats, sheep, foxes, or other moving objects and
manual sorting takes thousands of hours. Using tools from
the MMLSpark ecosystem, we have created an automated
system to classify and localize Snow Leopards in camera
trap images without human labeled data. This method saves
the Trust hours of labor and provides data to help identify
individual leopards by their spot patterns.

complexity and incur costs from additional deployment sys-
tems. Other serving frameworks, such as Clipper and Azure
Machine Learning Services (AML), rely on using Spark’s
batch processing API to evaluate data. This approach results
in large latencies, as each call re-builds the computation
graph on the head node, generates code for the workers on
the head node, sends this generated code (and all data in the
closures) to the workers, sends all the request data to the
workers, and collects the response data from the workers.
Because we leverage Structured Streaming and continuous
processing, all code generation and graph building occurs
only in initialization, and the hotpath of the request stays
clear of unnecessary computations. Table 1 demonstrates
our improvements in latency over other systems in the liter-
ature. Furthermore Spark Serving inherits desire-able fault
tolerance behavior, management APIs, and ability to handle
shufﬂes and joins from Spark Streaming. This ﬂexibility
makes Spark Serving signiﬁcantly more general than other
frameworks whose workers cannot communicate with each
other and do not have access to continuously updated dis-
tributed tables. Table 2 shows a comparison of functionality
across different serving frameworks.

Figure 3. End to end architecture of unsupervised object detection
on Spark

4.1.1. UNSUPERVISED CLASSIFICATION

In our previous work, we used deep transfer learning with
CNTK on Spark to create a system that could classify Snow

MMLSpark: Unifying Machine Learning Ecosystems at Massive Scales

Table 3. Object detector performance (mAP@[.5:.95]) evaluated
on human curated test images

TRAINING METHOD

UNSUPERVISED + PRE-TRAINING
HUMAN
HUMAN + PRE-TRAINING

MAP

49.8
30.9
79.3

Table 4. Deep classiﬁer performance on synthetic data, human
labelled data, and a logistic regression model on human data

ALGORITHM

ACCURACY

UNSUPERVISED
HUMAN
HUMAN + LR

77.6%
86.8%
65.6%

download the images to the cluster in only a few minutes.
Next, we used CNTK on Spark to train a deep classiﬁcation
network using transfer learning on our automatically gener-
ated dataset. Though we illustrated this process with Snow
Leopard classiﬁcation, the method applies to any domain
indexed by Bing Images.

4.1.2. UNSUPERVISED OBJECT DETECTION

Many animal identiﬁcation systems, such as HotSpotter,
require more than just classiﬁcation probabilities to identify
individual animals by their patterns (Crall et al., 2013). In
this work, we introduce a reﬁnement method capable of
extracting a deep object detector from any image classiﬁer.
When combined with our unsupervised dataset generation
technique in Section 4.1.1, we can create a custom object
detector for any object found on Bing Image Search. This
method leverages our LIME on Spark contribution to “inter-
pret” our trained leopard classiﬁer. These classiﬁer interpre-
tations often directly highlight leopard pixels, allowing us to
reﬁne our input dataset with localization information. How-
ever, this reﬁnement operation incurs the 1000x computa-
tion cost associated with LIME, making even the distributed
version untenable for real-time applications. However, we
can use this localized dataset to train a real-time object de-
tection algorithm like Faster-RCNN (Girshick, 2015). We
train Faster-RCNN to quickly reproduce the computation-
ally expensive LIME outputs, and this network serves as
a fast leopard localization algorithm that does not require
human labels at any step of the training process. Because
LIME is a model agnostic interpretation engine, this reﬁne-
ment technique can apply to any image classiﬁcation from
any domain. Figure 3 shows a diagram of the end-to-end
architecture.

4.1.3. RESULTS FOR UNSUPERVISED LEOPARD

DETECTION

We discovered that our completely unsupervised object de-
tector closely matched human drawn bounding boxes on

Figure 4. Human labeled images (top left) and unsupervised Faster-
RCNN outputs (top right). Difﬁcult human labeled images (bottom
left) and unsupervised Faster-RCNN outputs (bottom right).

Leopards in Camera trap images (Hamilton et al., 2018).
This work leveraged a large dataset of manually labeled
images accumulated through years of intensive labelling
by the Snow Leopard Trust. In this work, we show that
we can avoid all dependence on human labels by using
Bing Image Search to automatically curate a labeled Snow
Leopard dataset. More speciﬁcally, we used our SparkML
bindings for Bing Image Search to make this process easy
and scalable. To create a binary classiﬁcation dataset, we
ﬁrst create a dataset of leopards by pulling the ﬁrst 80 pages
of the results for the “Snow Leopard” query. To create
a dataset of negative images, we drew inspiration from
Noise Contrastive Estimation, a mathematical technique
used frequently in the Word Embedding literature (Gut-
mann & Hyv¨arinen, 2010). More speciﬁcally, we generated
a large and diverse dataset of random images, by using ran-
dom queries as a surrogate for random image sampling. We
used an existing online random word generator to create a
dataframe of thousands of random queries. We used Bing
Images on Spark to pull the ﬁrst 10 images for each ran-
dom query. After generating two datasets, we used Spark
SQL to add labels, stitch them together, drop duplicates, and

MMLSpark: Unifying Machine Learning Ecosystems at Massive Scales

most images. Table 3 and 4 show that our method can ap-
proach that of a classiﬁers and object detectors trained on
human labelled images. However, certain types of images
posed problems for our method. Our network tended to
only highlight the visually dominant leopards in images
with more than one leopard, such as those in Figure 4. We
hypothesize that this arises from our simple method of con-
verting LIME outputs to bounding boxes. Because we only
draw a single box around highlighted pixels, our algorithm
has only seen examples with a single bounding box. In the
future, we plan to cluster LIME pixels to identify images
with bi-modal interpretations. Furthermore, the method also
missed several camouﬂaged leopards, as in Figure 4. We
hypothesize that this is an anthropic effect, as Bing only re-
turns clear images of leopards. We plan to explore this effect
by combining this Bing generated data with active learn-
ing on a “real” dataset to help humans target the toughest
examples quickly.

Figure 5. Overview of distributed image analysis pipeline that
leverages containerized microservices on Spark, deep-learning
computation graphs, nontrivial joins for KNN and locally sensitive
hashing, and distributed index creation with Azure Search.

4.2. Visual Search Engine Creation

When many different frameworks unify within the same API
and ecosystem, it becomes possible to create high-quality
distributed applications with very few lines of code. We
demonstrate this by using MMLSpark and its surrounding
ecosystem to create a visual search engine. As is shown
in Figure 5, we use MMLSpark’s binary reading extension

to ingest raw ﬁles with high throughput using all nodes
of the cluster. We can then pump these images through
a variety of Computer Vision services with MMLSpark’s
cognitive service integration. These services add image
content, descriptions, faces, celebrities, and other useful in-
telligence to the dataframe of images. We can then featurize
images with headless networks such as ResNet50 or Incep-
tionV3 with either CNTK on Spark (ours), or Tensorﬂow
on Spark (Databrick’s). We can then pass these high-level
features through SparkML’s locally sensitive hashing imple-
mentation, K-means clustering, or a third-party K-nearest
neighbor SparkML package (Liu et al., 2007). We can then
create and write the dataframe to an Azure Search index
in a distributed fashion in a single line with MMLSpark’s
Azure Search integration. The resulting index can be quickly
queried for fuzzy matches on image information, content,
and visually similarity to other images.

5. Conclusion

In this work we have introduced Microsoft Machine Learn-
ing for Apache Spark, a framework that integrates a wide
variety of computing technologies into a single distributed
API. We have contributed CNTK, LightGBM, and LIME on
Spark, and have added a foundational integration between
Spark and the HTTP Protocol. We built on this to integrate
the Microsoft Cognitive Services with Spark and create a
novel real-time serving framework for Spark models. We
have also shown that through combining these technologies,
one can quickly create and deploy intelligent applications.
Our ﬁrst application used Bing, CNTK, LIME, and Spark
Serving to create a deep Snow Leopard detector that did not
rely on costly human labeled data. This application made
no assumptions regarding the application domain and could
extract custom object detectors for anything searchable on
Bing Images. Our second application leveraged container-
ized cognitive services, HTTP on Spark, CNTK, Tensorﬂow,
and other tools from the SparkML ecosystem to create an
intelligent image search engine. The contributions we have
provided in MMLSpark allow users to draw from and com-
bine a wide variety machine learning frameworks in only a
few lines of Spark code. This work dramatically expands
the Spark framework into several new areas of modern com-
puting and has helped us rapidly create several distributed
machine learning applications.

6. Future Work

We hope to continue adding computation frameworks and
aim to extend the techniques of HTTP on Spark to other
protocols like GRPC. We are continuously expanding our
collection of HTTP-based Spark models, and aim to add ser-
vices from Google, IBM, and AWS in addition to Microsoft.
We also hope to explore Spark Clusters of accelerated hard-

MMLSpark: Unifying Machine Learning Ecosystems at Massive Scales

ware SKUs like cloud TPUs and FPGAs to accelerate com-
putations. Additionally, we aim to leverage advancements in
the Spark Ecosystem such as Barrier Execution to improve
the fault tolerance of LightGBM (Xin). Additionally, we
hope to integrate Spark Serving as a deployment ﬂavor for
the popular machine learning management tool, MLFlow
(Zaharia et al., 2018).

Acknowledgements

We would like to acknowledge the generous support from
our collaborators, Dr. Koustubh Sharma, Rhetick Sengupta,
Michael Despines, and the rest of the Snow Leopard Trust.
We would also like to acknowledge those at Microsoft who
helped fund and share this work: the Microsoft AI for Earth
Program, Lucas Joppa, Joseph Sirosh, Pablo Castro, Brian
Smith, Arvind Krishnaa Jagannathan, and Wee Hyong Tok.

References

Abadi, M., Barham, P., Chen, J., Chen, Z., Davis, A., Dean,
J., Devin, M., Ghemawat, S., Irving, G., Isard, M., et al.
Tensorﬂow: a system for large-scale machine learning.
In OSDI, volume 16, pp. 265–283, 2016.

Armbrust, M., Xin, R. S., Lian, C., Huai, Y., Liu, D.,
Bradley, J. K., Meng, X., Kaftan, T., Franklin, M. J.,
Ghodsi, A., and Zaharia, M. Spark sql: Relational
In Proceedings of the 2015
data processing in spark.
ACM SIGMOD International Conference on Manage-
ment of Data, SIGMOD ’15, pp. 1383–1394, New York,
NY, USA, 2015. ACM. ISBN 978-1-4503-2758-9. doi:
10.1145/2723372.2742797. URL http://doi.acm.
org/10.1145/2723372.2742797.

Beazley, D. M. Swig: An easy to use tool for integrating
scripting languages with c and c++. In Proceedings of
the 4th Conference on USENIX Tcl/Tk Workshop, 1996
- Volume 4, TCLTK’96, pp. 15–15, Berkeley, CA, USA,
1996. USENIX Association. URL http://dl.acm.
org/citation.cfm?id=1267498.1267513.

Buitinck, L., Louppe, G., Blondel, M., Pedregosa, F.,
Mueller, A., Grisel, O., Niculae, V., Prettenhofer, P.,
Gramfort, A., Grobler, J., Layton, R., VanderPlas, J.,
Joly, A., Holt, B., and Varoquaux, G. API design for ma-
chine learning software: experiences from the scikit-learn
project. In ECML PKDD Workshop: Languages for Data
Mining and Machine Learning, pp. 108–122, 2013.

Chen, T. and Guestrin, C. Xgboost: A scalable tree boosting
system. In Proceedings of the 22nd acm sigkdd inter-
national conference on knowledge discovery and data
mining, pp. 785–794. ACM, 2016.

Combust, I. MLeap. http://mleap-docs.combust.

ml/.

Crall, J. P., Stewart, C. V., Berger-Wolf, T. Y., Rubenstein,
D. I., and Sundaresan, S. R. Hotspotterpatterned species
instance recognition. In Applications of Computer Vision
(WACV), 2013 IEEE Workshop on, pp. 230–237. IEEE,
2013.

Crankshaw, D., Wang, X., Zhou, G., Franklin, M. J., Gon-
zalez, J. E., and Stoica, I. Clipper: A low-latency online
prediction serving system. In NSDI, pp. 613–627, 2017.

Deep learning pipelines

Databricks.
spark.
spark-deep-learning. Accessed: 2019-01-20.

for apache
https://github.com/databricks/

Dean, J. and Ghemawat, S. Mapreduce: simpliﬁed data
processing on large clusters. Communications of the
ACM, 51(1):107–113, 2008.

Girshick, R. Fast r-cnn. In Proceedings of the IEEE inter-
national conference on computer vision, pp. 1440–1448,
2015.

Grinberg, M. Flask web development: developing web
applications with python. ” O’Reilly Media, Inc.”, 2018.

Gutmann, M. and Hyv¨arinen, A. Noise-contrastive esti-
mation: A new estimation principle for unnormalized
statistical models. In Proceedings of the Thirteenth Inter-
national Conference on Artiﬁcial Intelligence and Statis-
tics, pp. 297–304, 2010.

Hamilton, M., Raghunathan, S., Annavajhala, A., Kirsanov,
D., Leon, E., Barzilay, E., Matiach, I., Davison, J.,
Busch, M., Oprescu, M., Sur, R., Astala, R., Wen, T.,
and Park, C. Flexible and scalable deep learning with
mmlspark. In Hardgrove, C., Dorard, L., and Thomp-
son, K. (eds.), Proceedings of The 4th International
Conference on Predictive Applications and APIs, vol-
ume 82 of Proceedings of Machine Learning Research,
pp. 11–22, Microsoft NERD, Boston, USA, 24–25 Oct
2018. PMLR. URL http://proceedings.mlr.
press/v82/hamilton18a.html.

High, R. The era of cognitive systems: An inside look at ibm
watson and how it works. IBM Corporation, Redbooks,
2012.

Jackson, K. R., Ramakrishnan, L., Muriki, K., Canon, S.,
Cholia, S., Shalf, J., Wasserman, H. J., and Wright, N. J.
Performance analysis of high performance computing ap-
plications on the amazon web services cloud. In 2nd IEEE
international conference on cloud computing technology
and science, pp. 159–168. IEEE, 2010.

MMLSpark: Unifying Machine Learning Ecosystems at Massive Scales

Sayfan, G. Mastering Kubernetes. Packt Publishing Ltd,

2017.

Seide, F. and Agarwal, A. Cntk: Microsoft’s open-source
deep-learning toolkit. In Proceedings of the 22nd ACM
SIGKDD International Conference on Knowledge Dis-
covery and Data Mining, pp. 2135–2135. ACM, 2016.

Shrikumar, A., Greenside, P., and Kundaje, A. Learning
important features through propagating activation differ-
ences, 2017.

Shvachko, K., Kuang, H., Radia, S., and Chansler, R. The
hadoop distributed ﬁle system. In Mass storage systems
and technologies (MSST), 2010 IEEE 26th symposium
on, pp. 1–10. Ieee, 2010.

Sill, A. The design and architecture of microservices. IEEE

Cloud Computing, 3(5):76–80, 2016.

Taubmann, J., Sharma, K., Uulu, K. Z., Hines, J. E., and
Mishra, C. Status assessment of the endangered snow
leopard panthera uncia and other large mammals in the
kyrgyz alay, using community knowledge corrected for
imperfect detection. Oryx, 50(2):220230, 2016. doi:
10.1017/S0030605315000502.

Xin, R. Project hydrogen: Unifying state-of-the-art ai and
big data in apache spark. https://databricks.
com/session/databricks-keynote-2.
Ac-
cessed: 2019-01-20.

Xin, R. S., Gonzalez, J. E., Franklin, M. J., and Stoica, I.
Graphx: A resilient distributed graph system on spark.
In First International Workshop on Graph Data Manage-
ment Experiences and Systems, pp. 2. ACM, 2013.

Zaharia, M., Xin, R. S., Wendell, P., Das, T., Armbrust,
M., Dave, A., Meng, X., Rosen, J., Venkataraman, S.,
Franklin, M. J., Ghodsi, A., Gonzalez, J., Shenker, S.,
and Stoica, I. Apache spark: A uniﬁed engine for big
data processing. Commun. ACM, 59(11):56–65, October
2016. ISSN 0001-0782. doi: 10.1145/2934664. URL
http://doi.acm.org/10.1145/2934664.

Zaharia, M., Chen, A., Davidson, A., Ghodsi, A., Hong,
S. A., Konwinski, A., Murching, S., Nykodym, T.,
Ogilvie, P., Parkhe, M., et al. Accelerating the machine
learning lifecycle with mlﬂow. Data Engineering, pp. 39,
2018.

Ke, G., Meng, Q., Finely, T., Wang, T., Chen, W., Ma,
W., Ye, Q., and Liu, T.-Y. Lightgbm: A highly efﬁcient
gradient boosting decision tree. In Advances in Neural
Information Processing Systems 30, December 2017.

Liu, T., Rosenberg, C., and Rowley, H. A. Clustering bil-
lions of images with large scale nearest neighbor search.
In null, pp. 28. IEEE, 2007.

Lundberg, S. M. and Lee, S.-I. A uniﬁed approach to in-
In Advances in Neural
terpreting model predictions.
Information Processing Systems, pp. 4765–4774, 2017.

Meng, X., Bradley, J., Yavuz, B., Sparks, E., Venkataraman,
S., Liu, D., Freeman, J., Tsai, D., Amde, M., Owen,
S., et al. Mllib: Machine learning in apache spark. The
Journal of Machine Learning Research, 17(1):1235–1241,
2016.

Microsoft. Azure machine learning service documenta-
tion. https://docs.microsoft.com/en-us/
azure/machine-learning/service/, a. Ac-
cessed: 2019-01-20.

Microsoft.

Azure

search.
microsoft.com/en-us/services/search/,
b. Accessed: 2019-01-20.

https://azure.

Microsoft.

Cognitive

services.

https://

azure.microsoft.com/en-us/services/
cognitive-services/, c. Accessed: 2019-01-20.

Norm Estabrook,

James Baker,

T. M. T. W.
Introduction to azure data lake storage
https://docs.microsoft.

N. S.
gen2 preview.
com/en-us/azure/storage/blobs/
data-lake-storage-introduction.
cessed: 2019-01-20.

Ac-

Paszke, A., Gross, S., Chintala, S., Chanan, G., Yang, E.,
DeVito, Z., Lin, Z., Desmaison, A., Antiga, L., and Lerer,
A. Automatic differentiation in pytorch. 2017.

Rensin, D. K. Kubernetes - Scheduling the Future at Cloud
Scale. 1005 Gravenstein Highway North Sebastopol, CA
95472, 2015. URL http://www.oreilly.com/
webops-perf/free/kubernetes.csp.

Ribeiro, M. T., Singh, S., and Guestrin, C. ”why should i
trust you?”: Explaining the predictions of any classiﬁer.
In Proceedings of the 22Nd ACM SIGKDD International
Conference on Knowledge Discovery and Data Mining,
KDD ’16, pp. 1135–1144, New York, NY, USA, 2016.
ACM. ISBN 978-1-4503-4232-2. doi: 10.1145/2939672.
2939778. URL http://doi.acm.org/10.1145/
2939672.2939778.

