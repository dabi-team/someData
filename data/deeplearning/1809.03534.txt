1

Energy Disaggregation via  
Deep Temporal Dictionary Learning  

Mahdi Khodayar, Student Member, IEEE, Jianhui Wang, Senior Member, IEEE, 
and Zhaoyu Wang, Member, IEEE 

 

Abstract—  This  paper  addresses  the  energy  disaggregation 
problem, i.e. decomposing the electricity signal of a whole home to 
its  operating  devices.  First,  we  cast  the  problem  as  a  dictionary 
learning  (DL)  problem  where  the  key  electricity  patterns 
representing consumption behaviors are extracted for each device 
and  stored  in  a  dictionary  matrix.  The  electricity  signal  of  each 
device is then  modeled  by a linear combination of such  patterns 
with  sparse  coefficients  that  determine  the  contribution  of  each 
device  in  the  total  electricity.  Although  popular,  the  classic  DL 
approach  is  prone  to  high  error  in  real-world  applications 
including  energy  disaggregation,  as  it  merely  finds  linear 
dictionaries.  Moreover,  this  method  lacks  a  recurrent  structure; 
thus,  it  is  unable  to  leverage  the  temporal  structure  of  energy 
signals.  Motivated  by  such  shortcomings,  we  propose  a  novel 
optimization  program  where  the  dictionary  and  its  sparse 
coefficients  are  optimized  simultaneously  with  a  deep  neural 
model  extracting  powerful  nonlinear  features  from  the  energy 
signals.  A  long  short-term  memory  auto-encoder  (LSTM-AE)  is 
proposed  with  tunable  time  dependent  states  to  capture  the 
temporal behavior of energy signals for each device. We learn the 
dictionary  in  the  space  of  temporal  features  captured  by  the 
LSTM-AE  rather  than  the  original  space  of  the  energy  signals; 
hence,  in  contrast  to  the  traditional  DL,  here,  a  nonlinear 
dictionary is learned using powerful temporal features extracted 
from our deep model. Real experiments on the publicly available 
Reference  Energy  Disaggregation  Dataset 
show 
significant 
the  state-of-the-art 
methodologies in terms of the disaggregation accuracy and F-score 
metrics.  

improvement  compared 

(REDD) 

to 

Index  Terms—  Energy  Disaggregation,  Dictionary  Learning, 

Long Short-term Memory Auto-encoder, Deep Learning 

I.  INTRODUCTION 

E

NERGY  disaggregation  also  known  as  non-intrusive  load 
monitoring,  is  the  problem  of  decomposing  the  whole 
electricity consumption signal of a residential, commercial, or 
industrial  building  into  the  signals  of  its  appliances.  The 
disaggregation models can inform the service customers of their 
consumption patterns and recognize malfunctions in electricity 
appliances  [1].  Furthermore,  finding  the  detailed  electricity 
consumption  patterns of the customers  helps energy  suppliers 
to efficiently plan and operate power systems.     

Motivated by such beneficial applications, the energy society 
this  problem.  Energy 
has  been  recently 
disaggregation studies are categorized into two groups. The first 

interested 

in 

Mahdi Khodayar and Jianhui Wang are with the Department of Electrical 
(email: 

Engineering,  Southern  Methodist  University,  TX,  USA 
MahdiK@smu.edu and Jianhui@smu.edu).  

Zhaoyu  Wang  is  with  the  Department  of  Electrical  and  Computer 

Engineering, Iowa State University, IA, USA. (wzy@iastate.edu). 

class  of  approaches  classify  electricity  events  rather  than 
decomposing the energy signals. [2] considers each device as a 
finite  state  machine  and  identifies  sharp  edges  of  the 
active/reactive  power.  The  appliances  are  further  clustered 
based on their electricity changes. The devices with low energy 
consumption  are  likely  to  be  assigned  to  the  same  cluster; 
hence,  degrading  the  accuracy  of  this  method.  Later  works 
including  [3]  leveraged  transient  and  harmonic  information 
with  very  high  sampling  rates;  however,  such  data  requires 
costly  hardware  and  monitoring  devices.  Also,  in  this  line  of 
research,  several  load  classification  methods  using  different 
factors such as load control [4] or power usage [5] have been 
presented.  

The  second  class  of  algorithms  decompose  the  total 
electricity  signal  into its component  devices [1, 6]. While the 
unsupervised models [7] do not require individual appliances’ 
data  for  the  disaggregation  task,  the  supervised  approaches 
make  use  of  such  data collected  from  the  target  building.  [8] 
proposes  a  supervised  discriminative  sparse  coding  model 
based  on  structured  prediction,  to  maximize  disaggregation 
performance.  The  whole  energy  signal  of  each  device  is 
modeled  as  a  sparse  linear  combination  of  an  unknown 
dictionary; hence, this model requires lots of data to be trained. 
[9]  introduces  the  factorial  hidden  Markov  model  (FHMM) 
using  block Gibbs sampling  for energy  signal decomposition. 
[10]  further  enhances  FHMM  by  an  approximate  algorithm 
based  on  convex  programming  in  order  to  address  the 
unmodeled devices. Although FHMM is flexible to be applied 
in  both  supervised/unsupervised  settings,  the  EM  training 
procedure of this model highly depends on initialization; hence, 
degrading the accuracy.         

In this paper, the deep temporal dictionary learning (DTDL) 
is presented as a novel supervised algorithm for the problem of 
energy  disaggregation.  Given  an  aggregate  signal  of  a  whole 
building  and  a  set  of  electricity  signals  for  each  device,  we 
automatically capture powerful temporal consumption patterns 
for  each  device.  To  learn  such  patterns,  a  long  short-term 
memory  auto-encoder  (LSTM-AE)  is  designed  to capture  the 
nonlinear  manifold  of  energy  signals  for  each  device  inside 
small time windows. Using a recurrent structure, we are able to 
extract  useful  temporal  features  for  the  underlying  signals. 
Furthermore, an optimization program is proposed to tune the 
LSTM-AE’s  parameters while  learning a dictionary of energy 
features  that  best  represent  the  temporal  features  obtained  by 

 
 
 
our auto-encoder. Learning the deep temporal features as well 
as  the  signal  dictionary  simultaneously,  our  model  is  able  to 
capture a nonlinear dictionary that can leverage time dependent 
features  of  a  deep  model.  Computing  the  dictionary,  we 
disaggregate the building’s consumption signal using our sparse 
coding model; hence, finding the contribution of each device to 
the aggregate electricity.  

The contributions of the presented work are the following:  
1)  Previous  dictionary  learning  works  merely  optimize  for  a 
linear dictionary, which makes them error-prone for real-world 
applications  where  most 
inputs/features  have  nonlinear 
characteristics  and  relationships.  In  this  study,  a  nonlinear 
dictionary  is  computed  for  the  electricity  signals;  hence, 
improving the accuracy of energy disaggregation solutions.  
2) In contrast to previous sparse modeling works, DTDL finds 
the optimal dictionary and sparse codes in the space of a deep 
model’s  latent  features  rather  than  the  original  space  of  the 
energy  signals;  hence,  our  algorithm  is  able  to  leverage 
powerful  deep learning  features  extracted from  the  electricity 
signals of various devices. 
3) This is the first work that captures the temporal manifold of 
energy  signals  for  each  device;  hence,  learning  power 
unsupervised  features  from  energy  signals  while  employing 
such  features  in  the  disaggregation  task.  Using  the  recurrent 
structure of our presented LSTM-AE,  our approach is able to 
model the temporal structures of electricity patterns.  

The rest of the paper is organized as the following:  Section 
II defines the energy disaggregation problem. In Section III, the 
classic  DL  approach  is  explained;  furthermore,  the  proposed 
DTDL model using LSTM-AE for temporal feature extraction 
is  discussed.  Section  IV  presents  our  novel  optimization  for 
nonlinear  deep  dictionary  learning.  Section  V  shows  real 
experiments on a publicly available electricity dataset. Finally, 
Section VI presents the conclusion of this research.  

II.  PROBLEM FORMULATION 

each  device 

Let us assume there are 

 electric devices in a building and 
  at  each  time 
.  The  aggregate  consumption  signal  observed 

  consumes  an  energy  signal 
(cid:1)

(cid:3)(cid:4)((cid:6))

(cid:2)

(recorded) by the smart meter is computed by: 
1 ≤ (cid:6) ≤ (cid:10)

(cid:2)

(cid:19)

(cid:19)

(cid:6)

(cid:3)((cid:6))

 and 

{(cid:3)(cid:4)((cid:6))}(cid:18)(cid:14)(cid:15)

for each valid 

(cid:3)((cid:6)) = (cid:12) (cid:3)(cid:4)((cid:6))
 is the total power consumed at time 
(cid:4)(cid:14)(cid:15)

where 
aggregated  signals 
consumption signal of the individual appliances 
{(cid:3)((cid:6))}(cid:18)(cid:14)(cid:15)
.  
the estimation of 

. Observing the 
  ,  the  goal  is  to  recover  the 
(cid:6)
, i.e. 

1 ≤ (cid:2) ≤ (cid:1)
Let  us  consider  an  energy  consumption  dataset 
corresponding to a building, that contains the energy signals of 
(cid:20)
).  We 
different  devices  through  time  (from 
break the consumption  signals into  windows of  length 
(cid:6) = (cid:10)
for all devices and denote the consumption electricity of each 
(cid:21) ≪ (cid:10)
device 
, called 
an energy snippet, for all 
 . The corresponding 
(cid:29)(cid:4)((cid:24))
 is defined by 
aggregate signal is denoted by 
 is a tensor of 
(cid:20)
  .  The  goal  is  to  build  a 
(cid:20)$
 such that a solution of the following 

(cid:24) = 1,2, … ,   =
 in which each data sample 
(cid:29)((cid:24))

the  form 
(cid:20) = {(cid:20)(cid:15), (cid:20)", … , (cid:20)#}
dictionary matrix 

(cid:19)
. As a result, 
!

 in the time interval 

[((cid:24) − 1)(cid:21) + 1, (cid:24)(cid:21)]

〈(cid:29)(cid:15)((cid:24)), (cid:29)"((cid:24)), … , (cid:29)(cid:13)((cid:24)), (cid:29)((cid:24))〉

  up  to 

(cid:6) = 1

 by 

(cid:2)

(cid:21)×+

' ∈ ℝ

problem reveals the disaggregation of 

: 

0

(cid:29)((cid:24))

(cid:29)((cid:24)) = ' -((cid:24)) = (cid:12) -.((cid:24))'.,.

.(cid:14)(cid:15)

2

(2) 

!×0

!×01

2

; 

'

ℝ

(cid:29)(cid:4)((cid:24))

     '(cid:4) ∈ ℝ

. Each column 

' = ['(cid:15) '" … '(cid:13)] ∈ ℝ

 is a dictionary matrix of size 

'.,.
; that is, every signal 

Here, 
the dictionary, i.e. 
atom)  for  the  device  consumption  signals 

 of 
!×0
 , is a representative signal (also called an 
, 
 can be written as a linear 
(cid:29)(cid:4)((cid:24))  (cid:2) ∈ [1, (cid:1)]
 is a sparse 
combination of several columns (atoms) in 
(cid:24) ∈ [1,  ]
coefficient  vector  that  determines  the  coefficients  of  such  a 
 decides the contribution 
linear combination. Each element 
of  each  column 
.  As 
  sub-dictionaries 
shown  in  (1),  we  decompose 
, each corresponding to a device; hence, each signal 
  can  be  written  as  a  linear  combination  of 
  is  the  number  of 
. Therefore, the 
+(cid:4)
  is  a  linear  combination  of  the  columns 
 each associated with a sparse coefficient vector 

'(cid:4) ∈ ℝ
columns  of  the  sub-dictionary 
(cid:29)(cid:4)((cid:24))
these columns (atoms) defined for each device 
aggregate  signal 
(atoms) in 

  in  the  total  consumption  signal 
-.((cid:24))

(cid:24) ∈ [1,  ]

,  while 

  into 

(cid:29)((cid:24))

-((cid:24))

!×01

'.,.

'(cid:4)

'

'

(cid:1)

(cid:2)

(cid:29)((cid:24))

 written by: 
'(cid:4)

((cid:24))

(cid:4)

-

(cid:29)((cid:24)) = ' -((cid:24))

!×0

(3) 

(cid:4)

(cid:13)

(cid:15)

"

0

01

   -

((cid:24))  -

((cid:24)) … -

((cid:24)) ∈ ℝ

-((cid:24)) = [-

' = ['(cid:15)  '" … '(cid:13)] ∈ ℝ
Since  each  device  has  multiple  consumption  patterns 
((cid:24))] ∈ ℝ
corresponding to different operation modes, the objective is to 
extract  useful  consumption  signatures  (temporal  patterns) 
 , as a 
through time to build sub-dictionaries 
matrix  whose  columns  (atoms)  can  best  represent  the  energy 
.  Moreover,  we  need  to  find  the 
snippets 
optimal  sparse  coefficients 
 for  all  devices  in  order  to 
reveal the contribution of each device in the total consumption 
((cid:24))
 are 
.  Notice  that  the  devices  with  a  non-zero 
signal 
(cid:4)
detected to be operating/on at the time interval 
, while others 
are considered to be off.  

 for each device 

 (cid:24) ∈ [1,  ]

(cid:29)(cid:4)((cid:24))

(cid:29)((cid:24))

((cid:24))

'(cid:4)

-

-

(cid:2)

(cid:4)

(cid:24)

III.  DICTIONARY LEARNING FOR ENERGY DISAGGREGATION  
In this section, first, we discuss a classic dictionary learning 
, then, 
(DL) method to solve (3) to find the optimal 
the  drawbacks  of  this  approach  are  explained  and  a  novel 
nonlinear deep dictionary learning methodology is proposed for 
the energy disaggregation problem.  
A.  Classic Dictionary Learning 

 and 

-((cid:24))

'

  One can find the optimal sparse coefficients 
 by solving the sparse coding problem with 

 for each 
 regularization as 

∗

-

((cid:24))

formulated by: 
(cid:24)

4(cid:15)

(4a) 

(cid:4)

∗

-

-

(4b) 

"
||(cid:29)((cid:24)) − '-((cid:24))||"
(cid:4)

((cid:24)) ∈ {0,1}

((cid:24)) = argmin

;($)
(cid:19)
s. t.     1

+ =(cid:15)||-((cid:24))||(cid:15)
01
 is the reconstruction error that computes 
Here, 
((cid:24)) ≤ 1    -
2
the  distance  between 
, 
  with  the  true  total  energy 
||(cid:29)((cid:24)) − '-((cid:24))||2
 is the sparsity error that ensures the sparsity of 
while 
 provides a trade-off between the 
. The 
reconstruction accuracy and sparsity of the solution 
-((cid:24))
∗
condition in (4b)  makes sure  that  for each device 
, only one 
-
column  (atom)  is  found  as  the  signature  of  that  device  in  the 
(cid:2)
; that is, for each device, only one column 
aggregate signal 

'-((cid:24))
. A sparsity coefficient 

||-((cid:24))||(cid:15)

(cid:29)((cid:24))

((cid:24))

=(cid:15)

(cid:29)((cid:24))

(cid:13)

(1)                                    

 
 
 
 
 
 
     
 
 
 
 
 
   
 
 
 
 
 is selected to have non-zero coefficient inside 

of 
the device 
 with a non-zero element in 
its contribution to the total consumption 

; thus, 
 is operating and 
((cid:24))
 is determined by: 

'(cid:4)

-

∗

(cid:4)

(cid:2)

(5) 

 is the 

-th 

where 
entry of 

 is the 
('(cid:4)).,. (-
.  

(cid:4)

(cid:4)
-th column of 
((cid:24))).     A. (cid:6).   (-

2

((cid:24))

('(cid:4)).,.
(cid:4)
-

(-
Furthermore, to find the optimal dictionary 

the  dataset 
following  empirical  cost  function  over  the  dictionary 
(cid:20)
sparse coefficient matrix 

((cid:24))).
 with respect to 
  data  samples,  one  can  minimize  the 
  and 
: 
'
0×#
(6a) 

C = [-(1)  -(2) …   -( )] ∈ ℝ

  with 

'

2

((cid:24))
-
(cid:29)((cid:24))
 while 
((cid:24))). ≠ 0
'(cid:4)

(cid:4)

(6b) 

(6c) 

#

1

minD,E

"
(cid:12) (||(cid:29)((cid:24)) − '-((cid:24))||"

$(cid:14)(cid:15)

+ =(cid:15)||-((cid:24))||(cid:15))

"
A. (cid:6).  ||'.,.||"
(cid:19)
1

-

(cid:4)

≤ 1     2 = 1,2, … , + 
((cid:24)) ∈ {0,1}

01

(cid:4)

((cid:24)) ≤ 1  -

here, the constraint in (6b) prevents the dictionary from being 
arbitrarily large, since it can cause very small coefficient values 
in 

, which makes the solution less informative.  

   (cid:2) = 1,2, … , (cid:1)  (cid:24) = 1,2, … ,  

C

yɶ

1d

y

3d

2d

Original Space

S

(a)       

1d

yɶ
y

)
y
(
h

3d

2d

yɶ

Transformed Space S
              (b) 

,

F(cid:15)

F"

Fig.  1.  (a)  Classic  DL:  estimating  the  true  consumption  signal 
dictionary  atoms
and 
(b)  DTDL:  Transformation  of 
 by  a  mapping function 
FG
atoms inside the transformed space
H

 by 
.  
 to  learn  dictionary 
. The mapping provides a better estimation 
ℎ

 inside  the  original  nonlinear  space 
(cid:29)((cid:24))

. 
(cid:29)K ((cid:24))
H
B.   Deep Temporal Dictionary Learning: A Novel  Paradigm 
Towards Dictionary Learning for Energy Disaggregation  
1)  Challenges and Contributions  

J
 when mapped back to 

(cid:29)((cid:24))

 for 

H

H

'

(cid:29)((cid:24))

(cid:29)K((cid:24))

,  making  the  estimation 
(cid:29)K((cid:24))

The  classic  optimization  in  (4a)-(6c)  has  two  major 
drawbacks  that  motivate  the  need  for  a  more  powerful 
the  problem  of  energy  disaggregation:  
framework  for 
1- Linearity of solution: the classic dictionary learning learns a 
linear 
. As shown in Fig. 1, the minimization in the classic DL 
 for the true value 
 by 
model finds an approximation 
 that are inside the original space of 
finding dictionary atoms 
 ; however, if such a space is nonlinear 
'.,.
(as in the case of most real-world applications including energy 
(cid:29)(cid:4)((cid:24))  (cid:2) ∈ [1, (cid:1)]
(cid:24) ∈ [1,  ]
 might not be in the 
disaggregation), the estimation value 
  useless  for 
original  space 
. This motivates us to devise a novel non-
modeling the true 
H
linear dictionary learning based on deep learning to provide a 
 to an appropriate transformed space 
nonlinear mapping from 
 can be well written as a combination of atoms 
H
. 
 and the columns 
of 
H′
the 
Learning  such  a  nonlinear  mapping, 
H′
transformed  space 
,  is  a  crucial  challenge  solved  in  this 
Section. 2- Classic DL algorithms lack a recurrent structure to 
model  the  temporal  behavior  of  the  underlying  energy 
consumption dataset; thus, the need for a recurrent optimization 
model  that  can  capture  useful  temporal  patterns  from  the 
, is raised. 
underlying data, i.e. signals 
As  the  energy  consumption  signals  are  all  time  series,  we 
(cid:29)(cid:4)((cid:24))

 in which 
 as both 

 lie on the same space 

 inside the dataset 

(cid:29)K((cid:24))
(cid:29)((cid:24))

learning 

'-((cid:24))

(cid:29)((cid:24))

i.e. 

'.,.

H′

'

(cid:20)

propose a recurrent optimization model to address this issue.  
2)  Proposed Deep Recurrent Model 

3

is 

To  tackle  the  presented  challenges  in  Section  III-B-1,  we 
propose  a  novel  deep  learning  based  optimization  for  energy 
disaggregation.  Our  method  is  a  DL  approach  with  a  deep 
recurrent  formulation  to  capture  nonlinear  temporal  features 
that can help our model to better understand the behavior of the 
 signals. Our main 
energy consumption temporal data, i.e. 
idea 
to 
in  order 
corresponding to each device 
  number  of  optimal 
learn 

, we 
'(cid:4)
dimensional  energy  snippets 
C
, such that 
 for 
 can be 
(cid:29)(cid:4)((cid:24))
 ; hence, one 
written as a linear combination of elements of 
(cid:29)(cid:4)((cid:24))
(cid:24) ∈ [1,  ]
can  conclude  that  the  optimal  sub-dictionary  is  found  by 

+(cid:4)
the elements of 
(cid:29)M(cid:4) = 〈(cid:29)M(cid:4)(1), (cid:29)M(cid:4)(2), … , (cid:29)M(cid:4)(+(cid:4))〉 ∈ ℝ

learn  each  sub-dictionary 
 and the sparse code matrix 

 best represent the energy snippets 
(cid:2)

. In other words, for each device 

 for each device 

(cid:2)
(cid:21) −

, every 

(cid:29)(cid:4)((cid:24))

that 

!×01

(cid:29)M(cid:4)

(cid:2)

(cid:29)M(cid:4)

.  

!×01

U

U

!

(cid:29)(cid:4)((cid:24)) ∈ ℝ

.  The  feature 

  by  a 
OPQR: ℝ
F

Assuming that the consumption snippets 
, in order to find 

'(cid:4) = (cid:29)M(cid:4) ∈ ℝ
on a nonlinear manifold 
nonlinear  transformation 
N
energy  snippet 

 lie 
 , we learn a 
(cid:29)(cid:4)((cid:24))  (cid:24) ∈ [1,  ]
that  encodes  each 
'(cid:4) = (cid:29)M(cid:4)
!
-dimensional  feature  vector 
→   ℝ
 that captures the fundamental nonlinear temporal 
  is 
features  of  the  energy  snippet 
ℎ((cid:29)(cid:4)((cid:24))) ∈ ℝ
  that 
further  decoded  by  a  nonlinear  mapping 
(cid:29)(cid:4)((cid:24))
 in the nonlinear (transformed) 
maps the learned feature 
OUPR: ℝ
  in  the  original  space;  hence, 
space  to  the  observed 
learning  a  powerful  nonlinear  mapping 
  that  is  capable  of 
reconstructing the original consumption signal. Such nonlinear 
  and  mapped  back  to  the 
mapping 
original  space  of  energy  snippets  by 
  is 
 in the dataset 
 and 
learned (i.e. 
ℎ((cid:29)(cid:4)((cid:24)))
  inside  the  transformed  space 
(cid:29)(cid:4)((cid:24))
 ; hence learning the 
snippets  

corresponding to the nonlinear mapping 
(cid:20)
nonlinear  dictionary 

OPQR
 are found) for all 
OUPR

,  our  model  learns 

  is  implemented  by 

ℎ((cid:29)(cid:4)((cid:24)))
→   ℝ

ℎ((cid:29)(cid:4)((cid:24)))
(cid:29)(cid:4)((cid:24))

the  energy 

.  While 

'(cid:4) = (cid:29)Y(cid:2)

OUPR

for 

OVWX

ℎ

ℎ

(cid:21)

F

ℎ

.  

 for each device 
'(cid:4) = (cid:29)M(cid:4)
Since we are working with temporal data 

, we devise a 
(cid:29)(cid:4)((cid:24))  (cid:24) ∈ [1,  ]
(cid:2)
long  short-term  memory  auto-encoder  (LSTM-AE)  neural 
network using a deep learning based recurrent formulation. As 
shown in Fig. 2, our LSTM-AE is an LSTM network with 
temporal  states 
model 
iteration 
LSTM unit and the temporal state 
following recurrent formulation:  

  states  serve  to 
2(cid:21)
 iterations. At each 
(cid:21)
  is  observed  by  the 
(cid:21)
 using the 
(cid:29)(cid:4)((cid:24))Z ∈ ℝ

H(cid:4)  (cid:2) = 1,2, … ,2(cid:21)
,  an  element 
(cid:29)(cid:4)((cid:24))

 is updated to 

.  The  first 

 that maps 

ℎ((cid:29)(cid:4)((cid:24)))

1 ≤ 4 ≤ (cid:21)

(cid:29)(cid:4)((cid:24))

OPQR

 in 

 to 

HZ[(cid:15)

HZ

(cid:3)Z = (cid:29)(cid:4)((cid:24))Z
-Z = (cid:6)-Wℎ(\;(cid:3)Z + ];ℎZ[(cid:15) + ^;)
(cid:2)Z = H(cid:2)_`(\(cid:4)(cid:3)Z + ](cid:4)ℎZ[(cid:15) + ^(cid:4))
aZ = H(cid:2)_`b\c(cid:3)Z + ]cℎZ[(cid:15) + ^cd
eZ = H(cid:2)_`(\f(cid:3)Z + ]fℎZ[(cid:15) + ^f)
HZ = aZ ∘ HZ[(cid:15) + (cid:2)Z ∘ -Z
ℎZ = eZ ∘ (cid:6)-Wℎ(HZ)

(7) 

, 

(cid:3)Z

,  and 

where 

  is  the  input  vector,  while 

  are  the  
-dimensional input gate, forget gate, and output gate decision 
(cid:2)Z
 is  the  input  activation 
variables  at  iteration 
`
with bias 
 is the output vector of the LSTM at iteration 
,  which  stores  the  temporal  features  obtained  from  the  input 
, 
 up to 
sequence from the iterations 
4

. The parameters 

,  respectively. 

, and 

^;

-Z

ℎZ

eZ

aZ

, 

4

,

1

4

^(cid:4)

 ^c

^f

 
 
 
 
 
 
 
 
                 
 
 
 
 
 
 
 
 
 
 
 
 
i

`

ℝ
i×i

^h
\h
]h

, 
, 
\f
]f

, 
, 
\c
]c

- dimensional bias vectors  while 

 are the 
 are weight parameters inside 
 are the weight  matrices in 

and 
, 
and 
; Moreover, 
, 
\(cid:4)
. All bias and  weight 
and 
](cid:4)
parameters are tunable parameters that are learned to find the 
optimal  state 
  at 
each iteration 
 signal has been 
 is obtained by  the  LSTM as 
observed and 
the  temporal  features  of  the  whole  consumption  signal 
; 
thus, the first 
 mapping 
(cid:29)(cid:4)((cid:24))
each  energy  snippet 
  to  the  corresponding  temporal 
feature. 

HZ
4
ℎZ = ℎ! = ℎ((cid:29)(cid:4)((cid:24)))
 iterations of the LSTM implement 

ℝ
  as  well  as  the  optimal  temporal  feature 
. When 

, the whole 

(cid:29)(cid:4)((cid:24))

4 = (cid:21)

OPQR

ℎZ

(cid:21)

(cid:29)(cid:4)((cid:24))

As shown in Fig. 2, the iterations 

; At each 

, an output feature 

 reconstruct 
 is generated by 
(cid:21) + 1 ≤ 4 ≤ 2(cid:21)
  that  maps  the  resulting 
,  to  the  original 
. This leads to learning the nonlinear 

ℎZ = (cid:29)(cid:4)((cid:24))Z[!
OFVX

4

the  LSTM  in  order  to  model 
(cid:29)(cid:4)((cid:24))
temporal  features  of 
consumption snippet 
ℎ((cid:29)(cid:4)((cid:24)))
temporal  manifold  of  the  energy  snippets 

,  i.e. 

OPQR

(cid:29)(cid:4)((cid:24))

  as  the  LSTM  learns  powerful  temporal  features 
(cid:2) = 1,2, … , (cid:1)
 iteration, that are so powerful that can 

 in its 

(cid:29)(cid:4)((cid:24))

(cid:24) = 1,2, … ,  
reconstruct the original energy snippets 
ℎ! = ℎ((cid:29)(cid:4)((cid:24)))

4 = (cid:21)

. 

3)  Proposed Optimization of Dictionary Learning for Energy 
Disaggregation  

(cid:29)(cid:4)((cid:24))

matrix 
mapped in the 
C

We  learn  the  sub-dictionaries 
 in  the  space  of 

  and  the  sparse  coefficient 
  is 
-th iteration of the LSTM to the feature vector 

(cid:29)(cid:4)((cid:24))
ℎ! = ℎ((cid:29)(cid:4)((cid:24)))
, the columns (atoms) of the corresponding sub-dictionary 

;  that  is,  when 

'(cid:4)

are learned to represent 
ℎ!
the  atoms  (columns)  of  each 
feature vectors 
combinations are determined by the sparse code matrix 

 so that the linear combinations of 
'(cid:4)
  would  be  able  to  yield  the 
. Such linear 

 for all 
'(cid:4)

ℎ!

.  

ℎ! = ℎ((cid:29)(cid:4)((cid:24)))

(cid:24) = 1,2, … ,  

We  define  the  following  optimization  program  for  the 
,  while  finding  the 

  and 

C

problem  of  learning  the  optimal 
optimal mappings 

 and 

: 

'

C

(cid:21)

OPQR
q

min
jklm,jnkm,{D1}1op
(cid:13)

#

r(cid:15) =

(cid:12)

(cid:4)(cid:14)(cid:15)

1
(cid:1)

1

OUPR

,E  r = r(cid:15) + ="r" + =GrG + =srs

(cid:12) b||OPQRb(cid:29)(cid:4)((cid:24))d − '(cid:4)-
(cid:4)

$(cid:14)(cid:15)

(cid:4)

"
((cid:24))||"

+ =(cid:15)||-
(cid:13)

((cid:24))||(cid:15)d
(cid:19)
"
'.||j
||'(cid:4)

r" = (cid:12)
#

.(cid:14)(cid:15),.t(cid:4)

rG =

(cid:13)

(cid:12)

(cid:4)(cid:14)(cid:15)

1

1
(cid:1)

"
(cid:12) u||OUPR uOPQRb(cid:29)(cid:4)((cid:24))dv − (cid:29)(cid:4)((cid:24))||"

$(cid:14)(cid:15)

(8) 

v

rs

1

-

-

x

r(cid:15)

= w

A. (cid:6).    

= 0     ∀(cid:2)

((cid:24) + 1)y

"
||\c||j

"
+ ||]f||j

"
+ ||\(cid:4)||j

≤ 1   (cid:2) = 1,2, … , (cid:1)    2 = 1,2, … , +(cid:4)

"
+ ||](cid:4)||j
"
+ ||^f||"
(cid:4)

"
+ ||]c||j
"
+ ||^(cid:4)||"
(cid:19)
((cid:24)) − 1

Here, 
difference between 
combination of the atoms in the sub-dictionary 

"
+ ||\f||j
"
+||^c||"
#
(cid:4)
(cid:19)
(cid:12) y1
$(cid:14)(cid:15)
"
 is the dictionary learning cost function to compute the 
     {('(cid:4)).,.{"
 and the linear 
 computed by 
 and all 
 ensures sparsity for 
time intervals 
'(cid:4)-
the solution of 
 is our cross-dictionary incoherence term; 
(cid:24)
minimizing this error promotes incoherence between two sub-
;  that  is,  this  error  term  is  in  favor  of 
dictionaries 
 is 
having distinct sub-dictionary atoms for different devices. 

ℎZ(cid:14)! = ℎ((cid:29)(cid:4)((cid:24))) = OPQRb(cid:29)(cid:4)((cid:24))d
'(cid:4)

.  Such difference is considered for all devices 

 in the dataset. 

r"
C
  and 

((cid:24))||(cid:15)

=(cid:15)||-

((cid:24))

. 

(cid:2)

(cid:4)

(cid:4)

'(cid:4)

'.t(cid:4)

rG

4

(cid:2)

(cid:24)

. 

rs

,  i.e. 

OUPR uOPQRb(cid:29)(cid:4)((cid:24))dv

  for  all  devices 

  and  all  time  intervals 

the LSTM-AE’s reconstruction error term, which is the distance 
between  the  output  of  LSTM-AE  generated  at  iterations 
,  and  the  desired  output 
  is  the 
(cid:21) + 1 ≤ 4 ≤ 2(cid:21)
regularization  error  defined  to  control  the  magnitude  of  the 
(cid:29)(cid:4)((cid:24))
LSTM-AE’s  parameters.  Large  parameters  might  lead  to  the 
 is defined over LSTM parameters of (7) 
overfitting problem; 
 is an error term to satisfy the 
in order to avoid such problem. 
temporal  smoothness  prior.  Notice  that,  for  any  device 
,  the 
term 
 is zero except at intervals when it 
turns on/off. Given the fact that such switching happens in very 
small periods of time compared to the whole time period, we 
 for all devices. Finally, 
minimize 

(cid:15)
#
(cid:19)
# ∑ y1
((cid:24)) − 1
-
$(cid:14)(cid:15)
  is  assumed  to  avoid  each  sub-
the  constraint 
"
dictionary  from  obtaining  arbitrary  large  entries  as  it  would 
{('(cid:4)).,.{"
cause  very  small  coefficient  matrix 
,  making  the  solution 
trivial. 

((cid:24) + 1)y

((cid:24) + 1)y

((cid:24)) − 1

≤ 1

y1

rs

r|

-

-

-

(cid:2)

(cid:19)

(cid:19)

(cid:19)

(cid:4)

(cid:4)

(cid:4)

(cid:4)

C

Encoding function 

encF

1S

1h

2S

1lS 

2h

1lh 

*



li
la
  tanh

lf

tanh
lo
 



x
1

y k
( )
1
i



x
l

y k
( )
i

l

2S 

S 
2
1

2S

1S

lS

1S

S

lh

1h

h

x
l







y k
( )
i

l





e
r
u
t
a
e
f

l
a
r
o
p
m
e
T

)
)
k
(

i

y
(
c
n
e

F


h

S

h

2h 

1h 
2

2h

1h



h

2

y k
( )
i



 
h

2

y k
( )
i

2

 
h

1

y k
( )
1
i

Decoding function 

decF

Fig. 2. Structure of the proposed long short-term memory auto-encoder 

, 

, 

IV.  OPTIMIZATION SOLUTION FOR ENERGY DISAGGREGATION  
The optimization program in  (8) is not jointly convex  with 
respect to 
. As a result, we present an 
, and 
iterative  algorithm  that  alternates  between  optimizing  the 
. 
functions 
(cid:13)
Our  proposed  algorithm  addresses 
three  sub-problems 
{'(cid:4)}(cid:4)(cid:14)(cid:15)
OUPR
alternately as explained in the following. 
A.  Optimize the LSTM-AE mappings 

, as well as the variables 

OUPR
 and 

(cid:13)
{'(cid:4)}(cid:4)(cid:14)(cid:15)

 and 

OPQR

OPQR

C

C

Having a fixed 

in 
our objective (8), we need to solve the following optimization: 

 and 

OUPR

OPQR

OUPR

'

C

 and 
, in order to optimize 

OPQR

:  
 and 

jklm,jnkm, r~ = (cid:127)

min

(cid:13)

(cid:12)

(cid:4)(cid:14)(cid:15)

1
(cid:1)

#

1

(cid:4)

$(cid:14)(cid:15)

(cid:12) €(cid:129)‚yOPQRb(cid:29)(cid:4)((cid:24))d − '(cid:4)-

"
((cid:24))y‚"
"
+ w„‚OUPR uOPQRb(cid:29)(cid:4)((cid:24))dv − (cid:29)(cid:4)((cid:24))‚„"
"
+ ||\(cid:4)||j

"
+ ||\f||j
"
+||^c||"

"
+ ||]c||j
"
+ ||^(cid:4)||"

x…†
"
+ ||](cid:4)||j
"
+ ||^f||"

ƒ

"
||\c||j

-th iteration of the LSTM-AE denoted by 

OPQRb(cid:29)(cid:4)((cid:24))d

                +=s w
As explained in Section 3-B-2, the term 
output of the 
hence, we need to train our LSTM unit to output 
(cid:21)
at this iteration. Moreover, 
LSTM-AE 

OUPR uOPQRb(cid:29)(cid:4)((cid:24))dv

iterations 

in 

i.e. 
; Therefore,  we  need  to 
 2(cid:21)
(cid:21) + 1
 in (9). Let 
train LSTM-AE to satisfy 
OUPR uOPQRb(cid:29)(cid:4)((cid:24))dv = [ℎ!‡(cid:15) ℎ!‡" … ℎ"!]
us define the following notations for our LSTM parameters at 

[ℎ!‡(cid:15) ℎ!‡" … ℎ"!] = (cid:29)(cid:4)((cid:24))

through 

, 

ℎZ(cid:14)!
(cid:4)
 is the output of the 
((cid:24))
ℎZ(cid:14)! = '(cid:4)-

(9) 

"
+ ||]f||j
 in (9) is the 
; 

x

 
 
  
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
iteration 

: 

in 

the 

following 

form  using  Lagrangian  multipliers 

5

(12) 

 with 
r~

(13) 

(14) 

4

‰

‰

‰

, 

, 

, 

^ =

] = ˆ

(10) 

 \ = ˆ

-Z
(cid:2)Z
aZ
eZ

_-(cid:6)VAZ = ˆ

];
](cid:4)
]c
]f

Œ
‹
‹
Š
(cid:29)(cid:4)((cid:24))

^;
\;
(cid:143) 
\(cid:4)
^(cid:4)
Ž
\c
^c
Ž
, we compute the 
To minimize 
 in (9) after observing each 
\f
^f(cid:141)
 using:  
 with respect to the LSTM’s output 
gradient of 
r~
r~

ℎZ
‘r~
4 = (cid:21)
∆Z=
‘ℎZ ((cid:29)(cid:4)((cid:24))) = (cid:127)
hence, for each LSTM iteration 
, we compute the 
4 ≥ (cid:21) + 1
 with respect to various LSTM’s gates by:  
partial derivatives of 
r~
•ℎZ = ∆Z + ∆ℎZ

2’[ℎ!‡(cid:15) ℎ!‡" … ℎ"!] − (cid:29)(cid:4)((cid:24))“       
4 = 1,2, … , (cid:21)

2’OPQRb(cid:29)(cid:4)((cid:24))d − '(cid:4)-

(11) 

((cid:24))“

(cid:4)

"

•HZ = •ℎZ ∘   eZ   ∘ b1 − (cid:6)-Wℎ

(HZ)d + •HZ‡(cid:15) ∘ aZ‡(cid:15)

"
•-Z = •HZ ∘   (cid:2)Z   ∘ (1 − -Z
•(cid:2)Z = •HZ ∘   -Z   ∘   (cid:2)Z   ∘ (1 − (cid:2)Z)
•aZ = •HZ ∘   HZ[(cid:15)   ∘   aZ   ∘ (1 − aZ)
•eZ = •ℎZ ∘   tanh (HZ)  ∘   eZ   ∘ (1 − eZ)
(cid:19)

)

. •_-(cid:6)VAZ
(cid:19)
Having (9)-(12),  we compute the partial derivatives of 
. •_-(cid:6)VAZ
respect to LSTM’s parameters 

•(cid:3)Z = \
∆ℎZ[(cid:15) = ]

, and 

:  

, 

"!

\

]

^

•\ = (cid:12) •_-(cid:6)VAZ⨂(cid:3)Z
Z(cid:14)˜
"![(cid:15)

+ =s\

•] = (cid:12) •_-(cid:6)VAZ‡(cid:15)⨂ℎZ
"!

Z(cid:14)˜

+ =s]

Considering (12), we update our LSTM-AE model (which is an 
implementation  of 
)  using  the  following  update 
rule based on the gradient descent method:  
OPQR

•^ = (cid:12) •_-(cid:6)VAZ‡(cid:15)
Z(cid:14)˜
  and 

OUPR

+ =s^

QP™

QP™

\
]
QP™
, and 
^

← \ − ƞ•\
← ] − ƞ•]
← ^ − ƞ•^
QP™
^

, 

]

 , 

QP™

QP™

 are the updated parameters 

, 
 using the gradient descent update rule (13) after observing 
\
\
 is the 

Here, 
and 
each 
^
learning rate that determines how strong each update can be.  
(cid:29)(cid:4)((cid:24))  (cid:2) = 1,2, … , (cid:1)
(cid:24) = 1,2, … ,  
B.  Optimize the Dictionary 
Given the fixed mappings 

, as well as some fixed 
,  our  optimization  (8)  will  have  the 

sparse  code  matrix 
following form by which we seek to optimize 
C

 iterations. 

 and 

OUPR

OPQR

 in 

2(cid:21)

:  

'

]

Ƞ

: 

 D(cid:14)[Dp  D(cid:157)…  Dq] r̅ =

min

(cid:13)

(cid:12)

(cid:4)(cid:14)(cid:15)

1

1
(cid:1)

#

$(cid:14)(cid:15)
(cid:13)

(cid:12) (cid:129)‚yOPQRb(cid:29)(cid:4)((cid:24))d − '(cid:4)-

'
(cid:4)

"
((cid:24))y‚"

ƒ

(15) 

(cid:19)
+ =" (cid:12) ||'(cid:4)
"
the  cross  sub-dictionary 
A. (cid:6).     {('(cid:4)).,.{"

.(cid:14)(cid:15),.t(cid:4)

"
'.||j

the 

(cid:19)
||'(cid:4)

Here, 

(cid:13)
.(cid:14)(cid:15),.t(cid:4)

incoherence  error 

≤ 1   (cid:2) = 1,2, … , (cid:1)    2 = 1,2, … , +(cid:4)
to  enforce 
tries 

term 
resulting  sub-
"
 to have distinct dictionary 
dictionaries of different devices 
=" ∑
'.||j
atoms. In order to investigate the effect of such error term on 
the accuracy of our solution, we assume two different settings 
 to a zero or non-zero 
to solve (15) by setting the coefficient 
value: 
="
1)  No sub-dictionary incoherence error (

) in (15): 

(cid:2) ≠ 2

In this setting, there is no incoherence error; hence, each two 
devices might contain similar atoms in their corresponding sub-
dictionaries.  This  changes  the  optimization  of  (15)  to  a  least 
squares  problem  with  quadratic  constraints;  thus,  we  solve  it 
using Lagrangian multipliers. First, let us define the Lagrangian 

=" = 0

: 

Ÿ = ’Ÿ(cid:4),. ≥ 0“ (cid:2) = 1,2, … , (cid:1)  2 = 1,2, … , +(cid:4)
1

#

(cid:13)

ℒ(', Ÿ) =

(cid:12)

(cid:4)(cid:14)(cid:15)

1
(cid:1)

(cid:12) (cid:129)‚yOPQRb(cid:29)(cid:4)((cid:24))d − '(cid:4)-

"
((cid:24))y‚"
"
+ (cid:12) (cid:12) Ÿ(cid:4),. u{('(cid:4)).,.{"

$(cid:14)(cid:15)
(cid:13)

01

(cid:4)

− 1v   

Considering  the Lagrangian  multipliers  as 
can rewrite (16) using:  

.(cid:14)(cid:15)

(cid:4)(cid:14)(cid:15)

ŸK = [ŸK. ≥ 0]2=1

+

ƒ

(16) 

, one 

ℒ(', Ÿ) =

1
(cid:1)

(cid:13)

(cid:12)

(cid:4)(cid:14)(cid:15)

1

#

$(cid:14)(cid:15)
0

(cid:12) (cid:129)‚yOPQRb(cid:29)(cid:4)((cid:24))d − '(cid:4)-

(cid:4)

"
((cid:24))y‚"

ƒ

(17) 

"
+ (cid:12) ŸK. u{'.,.{"

− 1v
, we find the following analytical solution: 

.(cid:14)(cid:15)

Having 

¡ℒ(D,¢)

¡D = 0

(cid:19)f(cid:18);Z

OPQR
where 

= 〈

(cid:19)f(cid:18);Z

(cid:19)

(cid:19)

[(cid:15)

(CC

+ Σ)

' = OPQR

C
OPQRb(cid:29)(cid:4)(cid:14)(cid:15)(1)d … OPQRb(cid:29)(cid:4)(cid:14)(cid:15)( )d
…
OPQRb(cid:29)(cid:4)(cid:14)(cid:13)(1)d … OPQRb(cid:29)(cid:4)(cid:14)(cid:13)( )d
 and 
∈ ℝ

¤×((cid:13)∗#)

(cid:19)f(cid:18);Z

OPQR

 is  a  vector  of  all 
. Notice that 
dimension  of  the  temporal  feature  vector 
 (cid:2) = 1,2, … , (cid:1)
also,  
Lagrangian dual function is written as: 

 for  all 
 is the 
; 
. Therefore, the corresponding 

OPQRb(cid:29)(cid:4)(cid:14)Z((cid:24))d
F = dim(ℎZ(cid:14)!)

ℎZ(cid:14)! = OPQRb(cid:29)(cid:4)((cid:24))d

(cid:24) = 1,2, … ,  

Σ = ((cid:1) ∗  )F(cid:2)-_(Ÿ) ∈ ℝ

¦×0

(18) 

¤×((cid:13)∗#)

〉   ∈ ℝ

ℒU§;Z(Ÿ) = minD ℒ(', Ÿ)
1

#

(cid:13)

=

(cid:12)

(cid:4)(cid:14)(cid:15)

1
(cid:1)

(cid:12) {OPQR((cid:29)(cid:4)((cid:24))) − OPQR
0

$(cid:14)(cid:15)

(cid:19)f(cid:18);Z

(cid:19)
C

(cid:19)
(CC

+ Σ)

[(cid:15)

(cid:4)

"
((cid:24)){"

-

(19) 

+ (cid:12) ŸK. (‖OPQR

(cid:19)f(cid:18);Z
[(cid:15)
-th  unit  vector  denoted  by 

with  the 
.  Leveraging  the 
gradient  descent  method,  we  maximize  the  dual  Lagrangian 
. The gradient 

(cid:2)
 in  (19) with  respect to 

− 1)
0

©(cid:4) ∈ ℝ

"
©(cid:4)‖"

+ Σ)

(cid:19)
C

(CC

.(cid:14)(cid:15)

(cid:19)

 is computed by:  

ŸK = [ŸK. ≥ 0].(cid:14)(cid:15)

0

= ‖OPQR

(cid:19)

[(cid:15)

(cid:19)f(cid:18);Z

(CC

(cid:19)
C

"
©(cid:4)‖"
 is  computed  using  gradient  descent,  the 
 is  computed  by  (18)  using  the  optimal 

+ Σ)

− 1

(20) 

of the dual for any 
ℒU§;Z(Ÿ)

ŸK.

‘ℒU§;Z(Ÿ)
‘ŸK.

when  the  optimal 
optimal  dictionary 
ŸK
.  
'

:  
(cid:29)(cid:4)((cid:24))
'(cid:4)

(21) 

2)  Consider sub-dictionary incoherence error (
Σ = ((cid:1) ∗  )F(cid:2)-_(Ÿ)

When 

the  sub-dictionary 

incoherence  error 

considered in our optimization, i.e. 
(15).  Applying  gradient  descent,  we  minimize 
respect to each sub-dictionary for each training data 
the following gradient value for each sub-dictionary 

=" ≠ 0

r̅

=" ≠ 0

) in (15): 
is 
term 
, we need to optimize 
 in  (15)  with 
 using 

‘r̿
‘'(cid:4) b(cid:29)(cid:4)((cid:24))d = '(cid:4)-

(cid:4)

(cid:19)

(cid:4)

− OPQR,(cid:4) u-

(cid:19)

(cid:4)

((cid:24))v

((cid:24))v

((cid:24)) u-
(cid:13)
(cid:19)
+ =" (cid:12) b'(cid:4)

.(cid:14)(cid:15),.t(cid:4)

'.d'(cid:4)

OPQR,(cid:4) = 〈OPQRb(cid:29)(cid:4)(1)d … OPQRb(cid:29)(cid:4)( )d〉 ∈ ℝ
 with  respect  to  every  sub-dictionary 

Using  gradient  descent,  one  can  minimize  the  optimization 
 ;  hence, 
error 
optimizing the whole dictionary 
C.  Optimize the Sparse Code Matrix 

:  

'(cid:4)

.  

'

r̅

¤×#

  , 

,  and 

When 

C
 while observing each signal 

  are  fixed,  one  can  optimize 
. Let 
the coefficient matrix 
OUPR
OPQR
us  write  our  main  optimization  in  (8)  in  the  following  form 
where the main objective 
 in (8) is optimized with respect to 
each 

(cid:29)(cid:4)((cid:24))

 in 

:   

'

C

r

(cid:4)

-

((cid:24))

C

min;
1($)

  r̿ = ‚yOPQRb(cid:29)(cid:4)((cid:24))d − '(cid:4) -

(cid:4)

"
((cid:24))y‚"

+ =(cid:15)||-

(cid:4)

((cid:24))||(cid:15)

(22) 

(cid:19)
y1

(cid:4)

(cid:19)
((cid:24)) − 1

-

-

(cid:4)

A. (cid:6).

((cid:24) + 1)y = 0    ∀(cid:2), (cid:24)

 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
  
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
Here, to satisfy the constraint 
 and 

(cid:19)
, let us rewrite this condition using a square matrix 
-

((cid:24) + 1)y = 0

((cid:24)) − 1

y1

-

(cid:19)

(cid:4)

(cid:4)

 for all 
: 

(cid:2)

(cid:24)

C = u-

0×(#∗(cid:13))

(#∗(cid:13))×(#∗(cid:13))

«

(cid:4)(cid:14)(cid:15)

C « = 0      
(cid:4)(cid:14)"
(1) -

C ∈ ℝ
(cid:4)(cid:14)(cid:13)

(1) … -

(1) … -
( ) … -
= bC.,(cid:15)  C.," … C.,. … C.,(#∗(cid:13))d       2 = ((cid:24) − 1)(cid:1) + (cid:2)

( ) -

   « ∈ ℝ
(cid:4)(cid:14)(cid:15)

(cid:4)(cid:14)"

(cid:4)(cid:14)(cid:13)

( )v

(23) 

1
«(cid:4),. = ¬
−1
Having  the  constraint 
0
the  optimization  (22)  to  solve  for  each  column 

(cid:2) = 2
(cid:2) = 2 + (cid:1)
,  i.e. 
e(cid:6)ℎV ®(cid:2)AV
«

= 0

C

(cid:19)

(cid:19)

C « = 0
 for all energy snippets 

,  one  can  rewrite 

 by:  

(cid:4)

-

((cid:24)) = C.,. 

2 = ((cid:24) − 1)(cid:1) + (cid:2)
(cid:13)

(cid:29)(cid:4)((cid:24))
"
E.,¯o(°±p)q²1  (cid:12) (cid:12) ‚yOPQRb(cid:29)(cid:4)((cid:24))d − 'C.,.y‚"
#∗(cid:13)

min

$(cid:14)(cid:15)

#∗(cid:13)

(cid:4)(cid:14)(cid:15)

#

+ =(cid:15)||C.,.||(cid:15)

(24) 

³

³

,(cid:4)

C.,. = 0

A. (cid:6).   (cid:12) (cid:12) «.
(cid:14)(cid:15)

³

³

(cid:4)

.

(cid:14)(cid:15)

Solving  (24)  by  Proximal  Jacobian  ADMM  [11],  the  optimal 
sparse  coefficient  matrix  is  computed.  Notice  that,  the 
dimension of 
 does not add much computational burden to our 
optimization program as a large portion of 
’s entries are zero.  
D.  Energy  Disaggregation  Algorithm  Using  the  Proposed 
Optimization   

«

«

Algorithm 1 shows the structure of our energy disaggregation 
algorithm  using  the  proposed  dictionary  learning  model.  We 
 using  an  iterative  algorithm 
optimize 
alternating  among  the  optimizations  (9),  (15)  and  (24).  The 
optimizations  are  repeated  until  the  average  change  in  the 
dictionary entries is less than a small 

(cid:13)
{'(cid:4)}(cid:4)(cid:14)(cid:15)

,  and 

OUPR

OPQR

.  

C

, 

, 

∗

∗

¶

'

 of a building. Having the optimal dictionary 

In the test time, the optimal dictionary 
 is used in (25) to 
´ > 0
 for  some  test  aggregate 
, 
 show the contribution of each device 
. One can simply compute 

obtain  the  optimal  coefficients 
energy signal 
the optimal coefficients 
in the total electricity consumption 
such contributions using (5).  
Algorithm 1 Deep Temporal Dictionary Learning for Energy 
Disaggregation  
Inputs: 

all  devices 

consumption 

signals  of 

'

¶

-

-

∗

∗

∗

(cid:29)(cid:4)((cid:24)) 
 for 
∗
 is  a  test 
- = -
¶

¶ = '-

 and  solution 
Outputs:  Optimal  Dictionary 
(cid:24) ∈ [1,  ]  (cid:2) ∈ [1, (cid:1)]
the  energy  disaggregation  problem 
 where 
' = '
aggregate consumption signal that we decompose 
1: Repeat:  
2:      Optimize (9) to update 
3:      Optimize (15) to update the dictionary 
4:      Optimize (24) to update the sparse coefficients 
5: Until Convergence (changes in dictionary entries are less             
    than 
6: Test the model: Given  the optimal dictionary 
optimal  coefficient  vector 
consumption signal 

, compute 
 for  an  aggregate  energy 

´ > 0

OUPR

OPQR

)  

 : 

 , 

'

'

C

∗

∗

-

                (25) 

¶

= min; ‖OPQR(¶) − '

∗

-

∗

"
-‖"

+ =(cid:15)||-||(cid:15)

V.  SIMULATION RESULTS 

A.  Dataset 

Our  proposed  energy  disaggregation  model,  DTDL,  is 
evaluated on the real-world REDD dataset [12], a large publicly 
available  dataset  for  electricity  disaggregation  problems.  The 
dataset contains power consumption signals of five houses with 

6

around  20  different  appliances  at  each  house.  The  electricity 
signals  of  each  device  as  well  as  the  total  consumption  are 
available for two weeks with a high frequency sampling rate of 
15 kHz.  

Knowing  that  the  low  frequency  sampling  leads  to  a  more 
practical  energy  measurement  that  is  less  costly  and  more 
challenging  for  energy  disaggregation,  we  train  and  evaluate 
our method on low frequency data. In this study, a sampling rate 
of  1Hz  is  applied  to  collect  the  energy  signals.  We  train  and 
validate DTDL using the data corresponding to the first week; 
 of these samples are used to train and the rest are used to 
validate  the  model  to  find  the  optimal  hyperparameters.  The 
80%
samples of the second week are applied to test the model.  
B.  Disaggregation Accuracy Metrics 

Let  us  assume  the  test  aggregate  consumption  signal 

. Signal 

 time intervals (windows) of length 

contains 
by 
signals (energy snippets) 
¶((cid:24)) (cid:24) = 1,2, … ,  
  consumes 

, each denoted 
¶
 is the summation of energy 
(cid:21)
; that is, each device 
¶((cid:24))
.  Also,  let  us 
  at  the  time  interval 
¶(cid:4)((cid:24))
(cid:2) = 1,2, … , (cid:1) 
 obtained by 
 by 
denote the estimation of 
¶(cid:4)((cid:24))
1 ≤ (cid:2) ≤ (cid:1)
our  disaggregation  method.  The  disaggregation  accuracy  is 
computed by:  

¶¹(cid:4)((cid:24)) = '(cid:4) -

(cid:24)
((cid:24))

¶(cid:4)((cid:24))

(cid:4)

∑

-XX = º1 −

∑ {¶¹(cid:4)((cid:24)) − ¶(cid:4)((cid:24)){(cid:15)
Here, the 2 factor in the denominator is due to the fact that the 
2 ∑ ‖¶((cid:24))‖(cid:15)
absolute value leads to double counting errors. 

» × 100%

#
$(cid:14)(cid:15)

(cid:13)
(cid:4)(cid:14)(cid:15)
#
$(cid:14)(cid:15)

(26) 

In  order  to  have  a  comprehensive  comparison,  we  also 
compute  precision,  recall,  and  the  F-score  at  device  level.  At 
, a binary “on/off” value indicates whether 
each time period 
each device 
 is non-zero) or not (
 is zero). 
  determines  what  portion  of  the  estimated  on/off 
Precision 
(cid:2)
decisions for a device truly belongs to that device, while recall 
 measures what portion of the on/off value for one device is 

 is operating (
(cid:24)

((cid:24))

((cid:24))

¼

-

-

(cid:4)

(cid:4)

correctly estimated. F-score is the harmonic mean of 
½
that combines these two metrics by: 

 and 

¼

½

(27) 

C.  Experimental Settings and Model Validation 

O¾Rf¿P =

2 × ¼ × ½
¼ + ½

ƞ

´

`

  and 

The learning rate 

  and  the  dictionary  convergence  threshold 

 of the LSTM-AE’s update rule (14) is set 
  in 
.  LSTM-AE’s  hidden  layer 
 are determined by model 
  on  the  validation  dataset 

to  be 
Algorithm  1  is  set  to  be 
0.01
dimension 
 and window length 
0.05
validation;  that  is,  we  compute 
(cid:21)
using  different  configurations  of 
-XX
.  The  configuration  with  the  highest 

  is 
(cid:21) ∈
,  the  number  of 
chosen  to  test  the  model.  For  each  device 
{8,10,12,14,16}
 is set to be 20; hence, for a house with 20 
dictionary atoms 
  columns  is 
appliances,  a  dictionary  with 
+(cid:4)
learned.  Fig.  3  shows  the  validation 
  of  DTDL  averaged 
over all houses; As shown in this plot, the optimal configuration 
. Increasing 
has 
  to  larger  values  would  grow  the  generalization  capability 
(nonlinearity) of LSTM unit; however, it would also make the 
`
LSTM  prone  to  overfitting;  therefore,  the  moderate  value  of 
 is the optimal choice. It is also shown that the window 

+ = 20 × 20 = 400
-XX

 with a disaggregation accuracy of 

` ∈ {5,7,9,11,13}

83.56%

` = 7

-XX

(cid:2)

` = 7

 
 
  
 
 
 
  
 
 
 
  
 
 
                    
 
 
 
 
 
 
(cid:21) = 14

size 
 leads to the highest validation accuracy. Notice that 
smaller windows would degrade the accuracy as the transients 
; Moreover, larger 
would be overemphasized when learning 
time  windows  would 
different 
lead 
dynamics/operation modes in just a single time window, hence 
decreasing the disaggregation accuracy.   

observing 

to 

'

,

rs

 r"

 rG

, and 

In order to analyze the contribution of different error terms 
(
) defined in the optimization (8) to the quality 
,
and accuracy of our energy disaggregation solution, we show 
r(cid:15)
the  validation  accuracy  using  different  combinations  of  error 
coefficients 
averaged  over  all  houses  for  such  configurations  of  our 
=G
-XX
. In this plot, each error coefficient is chosen 
objective function 
.  As  shown  in  Fig.  4,  the 
from 
.  
optimal configuration is 

r
{0, 0.2, 0.4, 0.6, 0.8, 1, 1.2,1.4}

.  Fig.  4  shows  the  validation 

,  and 

="

=s

, 

〈=", =G, =s〉 = 〈0.4,1.2,0.6〉

Fig. 3. Validation accuracy of  DTDL  with different configurations of LSTM 
dimension and window length 

Fig.  4.  Validation  accuracy  of  DTDL  with  different  configurations  of  error 
. Contribution of various error terms to the accuracy 
coefficients 
of the disaggregation model is shown in terms of the accuracy matric in (26).  

, and 

, 

="

=G

=s

r"

in 

As 

proposed 

discussed 

optimization 

Fig.  5.  The  consumption  pattern  of  wahser/dryer,  refrigerator,  and  lighting 
device during their operation time for house 3 in the REDD dataset. 
the 

in  
Section III-B-3, 
 is the cross dictionary incoherence error that 
enforces the devices to have  dissimilar consumption patterns. 
The  optimal  coefficient 
  shows  that  this  error  term 
should have a relatively low (but more than zero) value, which 
means  that,  for  some  devices  the  assumption  of  having 
dissimilar energy snippets is true (e.g. refrigerator and lighting 
devices) while for other devices, the energy snippets might have 
similar behaviors. For instance, as shown in Fig. 5, the devices 
with  rotary  components  (i.e.  motors)  such  as  refrigerator  and 
washer/dryer  have  similar  consumption  patterns  that  are 
different from lighting appliances. 

=" = 0.4

rG

  is  the  reconstruction  error  term  that  makes  sure  that  the 
LSTM-AE has learned good temporal features that are strong 
enough to make (reconstruct) the original consumption signals. 
The optimal coefficient of this error term is 
  which is 
relatively  high.  This  shows  that  learning  powerful  temporal 

rG = 1.2

7

features  help  the  model  to  find  more  accurate  disaggregation 
 justifies our proposed 
solutions. Therefore, the high value of 
  using  the 
approach  for  learning  the  transformed  space 
proposed  LSTM-based  model.  Moreover,  the  regularization 
coefficient 
  that  shows  the 
magnitude of the parameters of the LSTM-AE that need to be 
constrained in order to avoid overfitting.  
D.  Numerical Results 

  has  a  moderate  value  of 

0.6

rs

rG

H

J

We  compare  the  proposed  model,  DTDL,  with  several 
energy  disaggregation  benchmarks  including  Simple  Mean 
Prediction  (SMP)  [6],  Factorial  Hidden  Markov  Model 
(FHMM)  [12],  Approximate  MAP  Inference  (AMAPI)  [13],  
Hierarchical  FHMM  (HieFHMM)  [14]  and  Powerlet-based 
Energy  Disaggregation  (PED)  [6].  Moreover,  the  Classic 
Dictionary Learning (CDL) model discussed in Section III-A is 
considered  as  a  baseline  to  better  show  the  merit  of  deep 
learning in the area of sparse coding and dictionary learning. 

Table I shows the energy disaggregation accuracy (26) of all 
benchmarks  in  the  REDD  dataset.  As  shown  Table  I,  the 
dictionary  learning  models  PED  and  DTDL  have  generally 
better performance compared to other methodologies. FHMM 
and its variants, AMAPI and HieFHMM, are outperformed by 
PED and DTDL since HMM-based models are limited by their 
first-order  Markov  property  which  makes  them  unable  to 
capture  high  order  correlation  among  various  devices’ 
consumption patterns. DTDL obtains the highest accuracy with 
20.2%, 19.8% and 16.9% improvement over FHMM, AMAPI, 
and HieFHMM, respectively. The superiority of DTDL over the 
benchmarks  is  due  to  learning  useful  nonlinear  patterns  from 
electricity signals while incorporating the learned deep features 
in  its  dictionary  learning  process.  Moreover,  the  recurrent 
structure of DTDL makes it a more powerful temporal pattern 
recognition model for the time dependent energy data. 

TABLE I 
DISAGGREGATION ACCURACY OF VARIOUS BENCHMARKS 
House 

Methods 

SMP 
FHMM 

1 

41.4 
71.5 

2 

39.0 
59.6 

3 

46.7 
59.6 

4 

52.7 
69.0 

5 

33.7 
62.9 

Average 

42.7 

64.5 

64.7 
66.3 
66.7 
72.0 
77.5 

60.1 
51.2 
59.2 
58.5 
73.2 

73.2 
75.6 
74.0 
81.6 
83.1 

66.3 
71.0 
78.4 
79.1 
83.9 

62.3 
60.5 
60.0 
61.8 
62.8 

61.4 
73.4 
61.8 
79.0 
84.5 

AMAPI 
HieFHMM 
CDL 
PED 
DTDL 
Table  II  shows  the  precision,  recall,  and  F-score  of  all 
benchmarks.  Let  us  compare  the  dictionary  learning-based 
models:  CDL,  PED,  and  DTDL.  On  average,  DTDL  has 
13.61% and 7.10% better F-score compared to CDL and PED, 
respectively. As explained in Section III-A, CDL learns a linear 
dictionary  using  the  consumption  signals  of  all  devices. 
However,  PED  runs  a  dissimilarity-based  subset  selection 
model  on  the  temporal  windows  of  each  device  to  find  the 
windows  that  are  most  representative  (windows  that  can  best 
represent  the  whole  set  of  windows).  The  representative 
windows  of  each  device  are  used  as  the  columns  of  the  sub-
dictionary corresponding to that device. In contrast to both CDL 

 
 
 
 
 
 
 
 
 
 
 
and PED, our DTDL approach learns a nonlinear dictionary that 
takes into account the temporal state transitions of the devices 
inside each  window. DTDL shows better precision and recall 
compared  to  PED  and  CDL  due  to  modeling  the  temporal 
behavior  of  consumption  signal  and 
learning  powerful 
nonlinear features to boost the disaggregation accuracy.  

Fig.  6  shows  the  actual/estimated  power  consumption 
obtained  by  DTDL  for  two  devices  in  House  1  on  day  14. 
Notice that the model accurately understands the transients and 
various steady states in the appliances. Moreover, Fig. 7 depicts 
the pie charts of the actual/estimated energy consumption of our 
model and PED, for House 3 during the test time. Notice that 
our  estimated  consumption  values  closely  follow  the  actual 
values, achieving better accuracy compared to PED in the 7-day 
test period. This shows better reliability of our proposed model 
for  real-world  energy  disaggregation  purposes  in  long  time 
horizons.   

TABLE II 
PRECISION(%), RECALL(%), AND F-SCORE(%) COMPARISONS 

Method  Metric 

SMP 

FHMM 

AMAPI 

HieFHMM 

CDL 

PED 

DTDL 

P 
R 
F-score 
P 
R 
F-score 
P 
R 
F-score 
P 
R 
F-score 
P 
R 
F-score 
P 
R 
F-score 
P 
R 
F-score 

1 
37.78 
35.51 
36.60 
77.12 
53.45 
63.13 
80.56 
57.83 
67.32 
80.81 
58.19 
67.65 
78.02 
56.79 
65.73 
86.03 
62.29 
72.26 
90.87 
63.09 
74.47 

2 
36.52 
37.64 
37.07 
68.81 
50.02 
57.92 
73.57 
52.81 
61.48 
77.02 
54.85 
64.07 
71.27 
53.02 
60.81 
78.89 
56.70 
65.98 
85.12 
60.37 
70.63 

House 

3 
35.42 
39.71 
37.44 
67.63 
51.54 
58.49 
75.82 
55.23 
63.90 
74.09 
55.12 
63.21 
73.58 
52.11 
61.01 
74.05 
51.23 
60.56 
75.22 
52.80 
62.04 

4 
36.69 
42.41 
39.34 
71.83 
54.59 
62.03 
70.27 
53.29 
60.61 
67.68 
52.92 
59.39 
77.90 
55.54 
64.84 
77.12 
55.51 
64.55 
93.81 
68.02 
78.85 

5 
36.73 
40.75 
38.63 
70.09 
52.88 
60.28 
76.79 
55.63 
64.51 
83.01 
59.91 
69.59 
89.82 
56.05 
69.02 
90.02 
68.22 
77.61 
94.71 
68.01 
79.17 

Average 
36.62 
39.20 
37.82 
71.09 
52.49 
60.37 
75.40 
54.95 
63.56 
76.52 
56.20 
64.78 
78.11 
54.70 
64.28 
81.22 
58.79 
68.19 
87.94 
62.49 
73.03 

       Time(s)  
  (a) Washer/Dryer 

         Time(s)  
 (b) Refrigerator 

Fig. 6. Estimated energy consumption signals of washer/dryer and refrigerator 
in House 1 on day 14. 

Fig. 7. Pie charts of actual/estimated consumption signals for House 3. 

8

VI.  CONCLUSIONS 
In  this  paper,  the  problem  of  energy  disaggregation  is 
addressed as a supervised DL problem; A dictionary matrix is 
learned  to  capture  the  representative  consumption  patterns  of 
each device; Furthermore, a set of coefficients are optimized to 
find  the  most  accurate  sparse  linear  combination  of  these 
patterns to construct the aggregate electricity signal. To extract 
informative  time-dependent  electricity  patterns,  we  propose 
DTDL  that  learns  deep  temporal  features  from  the  energy 
signals  of  each  device  using  an  LSTM-AE.  An  optimization 
program is devised to learn our LSTM states/parameters while 
tuning the dictionary atoms and their sparse coefficients using 
our  nonlinear  temporal  states.  Real  energy  disaggregation 
experiments on a publicly available dataset show the superiority 
of  our  DTDL  over  HMM-based  approaches  and  dictionary 
learning  models.  Compared  to  the  state-of-the-art  PED,  our 
DTDL obtains 7.63% and 7.10% better disaggregation accuracy 
and F-score, respectively. This outperformance is mainly due to 
extracting  nonlinear  dictionaries  as  well  as  learning  temporal 
structure  of the underlying  electricity  signals.  Future research 
seeks to design a new LSTM-AE whose states can be retrieved 
by an analytical optimizer such as ADMM-based optimization 
methods in order to find the global optima temporal parameters.  

REFERENCES 

[1] A. Rahimpour, H. Qi, D. Fugate and T. Kuruganti, "Non-Intrusive Energy 
Disaggregation  Using  Non-Negative  Matrix  Factorization  With  Sum-to-k 
Constraint", IEEE Transactions on Power Systems, vol.  32, no. 6, pp. 4430-
4441, 2017. 
[2] G.W. Hart, “Nonintrusive appliance load monitoring,” Proc. IEEE, vol. 80, 
no. 12, pp. 1870–1891, Dec. 1992. 
[3] S. Gupta, M. Reynolds and S. Patel, "ElectriSense", Proceedings of the 12th 
ACM international conference on Ubiquitous computing - Ubicomp '10, 2010. 
[4] M. Mahmoudi and K. Tomsovic, “A distributed control design methodology 
for damping critical modes in power systems,” in Proc. Power Energy Conf. 
Illinois, 2016, pp. 1–6. 
[5] A. Asadinejad andK. Tomsovic, “Optimal use of incentive and price based 
demand response to reduce costs and price volatility,” Electr. Power Syst. Res., 
vol. 144, pp. 215–223, 2017. 
[6] E. Elhamifar and S. Sastry, “Energy disaggregation via learning powerlets 
and  sparse  coding,”  in  Twenty-Ninth  AAAI  Conference  on  Artificial 
Intelligence, 2015. 
[7] M. Mengistu, A. Girmay, C. Camarda, A. Acquaviva and E. Patti, "A Cloud-
based On-line  Disaggregation Algorithm  for  Home  Appliance  Loads",  IEEE 
Transactions on Smart Grid, pp. 1-1, 2018. 
[8]  J.  Z.  Kolter,  S.  Batra,  and  A.  Y.  Ng,  “Energy  disaggregation  via 
discriminative sparse coding,” in Proc. 24th Annu. Conf. Neural Inf. Proc. Syst. 
(NIPS), Vancouver, BC, Canada, 2010, pp. 1153–1161. 
[9]  H.  Kim,  M.  Marwah,  M.  Arlitt,  G.  Lyon,  and  J.  Han,  “Unsupervised 
disaggregation of low frequency power measurements,” in 11th International 
Conference on Data Mining, 2010, pp. 747–758. 
[10] F. Facchinei, G. Scutari and S. Sagratella, "Parallel Selective Algorithms 
for  Nonconvex  Big  Data  Optimization",  IEEE  Transactions  on  Signal 
Processing, vol. 63, no. 7, pp. 1874-1889, 2015. 
[11] W. Deng, M.-J. Lai, Z. Peng, and W. Yin, “Parallel multi-block ADMM 
with  O(1/k) convergence,”  Mar. 2014, arXiv:1312.3040 [Online]. Available: 
http://arxiv.org/abs/1312.3040. 
[12] J.  Z.  Kolter  and M. J.  Johnson, “REDD:  A  Public Data Set  for  Energy 
Disaggregation Research,” in Proceedings of the SustKDD Workshop on Data 
Mining Applications in Sustainability, 2011. 
[13] J.  Kolter  and  T.  Jaakkola, “Approximate  inference  in  additive factorial 
hmms with application to energy disaggregation,” Journal of Machine Learning 
Research - Proceedings Track, vol. 22, pp. 1472–1482, 2012. 
[14] J. Huang, Z. Zhang, Y. Li, Z. Peng, J. H. Son, “Energy disaggregation via 
hierarchical factorial hmm,” in Proc. of The Second International Workshop on 
NILM, 2014, pp. 1-4. 

 
 
 
 
 
 
 
 
 
 
 
 
