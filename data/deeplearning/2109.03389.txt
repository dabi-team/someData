1
2
0
2

p
e
S
8

]

Y
S
.
s
s
e
e
[

1
v
9
8
3
3
0
.
9
0
1
2
:
v
i
X
r
a

An Optimal Resource Allocator of Elastic Training for Deep
Learning Jobs on Cloud

Liang Hu1, Jiangcheng Zhu2, Zirui Zhou1, Ruiqing Cheng2, Xiaolong Bai2 and Yong Zhang1

1Huawei Technologies Canada

2Huawei Cloud

E-mail: lianghu814@gmail.com, {zhujiangcheng, zirui.zhou, chengruiqing, baixiaolong1, yong.zhang3}@huawei.com

Abstract

Cloud training platforms, such as Amazon Web Services
and Huawei Cloud provide users with computational re-
sources to train their deep learning jobs. Elastic train-
ing is a service embedded in cloud training platforms
that dynamically scales up or down the resources allo-
cated to a job. The core technique of an elastic training
system is to best allocate limited resources among het-
erogeneous jobs in terms of shorter queueing delay and
higher training efﬁciency. This paper presents an op-
timal resource allocator for elastic training system that
leverages a mixed-integer programming (MIP) model to
maximize the training progress of deep learning jobs.
We take advantage of the real-world job data obtained
from ModelArts, the deep learning training platform of
Huawei Cloud and conduct simulation experiments to
compare the optimal resource allocator with a greedy
one as benchmark. Numerical results show that the pro-
posed allocator can reduce queuing time by up to 32%
and accelerate training efﬁciency by up to 24% relative
to the greedy resource allocator, thereby greatly improv-
ing user experience with Huawei ModelArts and poten-
tially enabling the realization of higher proﬁts for the
product. Also, the optimal resource allocator is fast in
decision-making, taking merely 0.4 seconds on average.
Keywords: elastic training, resource allocation, opti-

mization, deep learning, cloud computing, ETA

1 Introduction

Cloud training platforms, such as Amazon Web Services
and Microsoft Azure, provide abundant computational
resources for training deep learning (DL) models and
charge users by usage. Currently when users submit a
deep learning job to the cloud, it is required to spec-
ify the desired computational resources, e.g., number of
GPUs or nodes [1–3]. Cloud training platforms without
elasticity will use the speciﬁed, also ﬁxed, resources to
train the job. However, There are at least two problems

with using a ﬁxed amount of resources for performance
of the training job.

First, a system using a ﬁxed number of nodes or GPUs
for any given training job may use its computational re-
sources inefﬁciently. If the system is performing a small
number of training jobs at a given time, the system will
leave many of its computational resources idle. In other
words, the training jobs could have each utilized more
nodes or GPUs in order to complete sooner, instead of
wasting the computational capacity of the idle resources.
For example, a system with 100 nodes performing only
a single training job, wherein the ﬁxed number of nodes
assigned to the training job is 8 nodes, is wasting 92 of
its nodes.

Second, the system’s computational resources are al-
ways limited by the size of the system’s resource pool,
e.g., the number of nodes available for allocation to
training jobs.
It is common for a system to receive
multiple job proﬁles from users while a small number
of computationally-intensive training jobs are monop-
olizing the system’s resource pool, requiring the sys-
tem to maintain the later job proﬁles in a job queue
for a signiﬁcant period of time while waiting for the
computationally-intensive training jobs to complete.
This introduces signiﬁcant delays, even for small train-
ing jobs that could be completed quickly if any nodes
were available. These delays are arguably inefﬁcient in
terms of meeting the needs of the user base, and tend
to generate dissatisfaction in users who experience such
delays.

Accordingly, cloud systems have been developed that
perform elastic training of deep learning models (re-
ferred to herein as “elastic training systems”) to address
the limitations of systems using a ﬁxed amount of re-
sources for a given training job [4,5]. An elastic training
system dynamically allocates computational resources
(e.g., nodes) to training jobs based on the status of the
system (e.g., how many nodes are in use, how many
jobs are in the job queue) and job attributes (e.g., how
computationally intensive is a given training job) to ad-
dress the two problems described above. If the system

1

 
 
 
 
 
 
has abundant computational resources available (e.g., a
large number of idle nodes), an elastic training system
may scale up one or more ongoing training jobs, i.e.,
allocate more nodes or other computational resources
to the one or more ongoing training jobs. If an elastic
training system is busy (e.g., all nodes are being used
for ongoing training jobs), the elastic training system
scales down one or more of the ongoing jobs, i.e., re-
leases some nodes or other computational resources so
that new training jobs can use the released nodes or other
computational resources instead of waiting in the job
queue. Fig. 1 is an example wherein different jobs are
training on different number of nodes. If systems have
no elastic training service, the resource pool may gener-
ate fragments indicating some nodes are being wasted.
With elastic training service, job 5 and 7 may scale up to
utilize more nodes, and job 6 may scale down to release
part of its resources to job 8 that just enters the pool.

Figure 1: Resource pool and job queue of an elastic
training system.

The core of an elastic training system is its resource
allocator. A resource allocator should optimally de-
cide on the nodes or other computational resources as-
signed to each training job so that the elastic training
system can (1) improve efﬁcient utilization of computa-
tional resources, (2) speed up the overall training time
required to complete a given set of training jobs, (3) re-
duce queueing delay, and (4) improve the user experi-
ence when submitting a job proﬁle to the system. By
achieving one or more of these objectives, and providing
the beneﬁts thereof to users, an elastic training system
may also be able to realize higher proﬁts in providing a
paid deep learning product-as-a-service (PaaS) to users,
through a combination of higher revenue from users due
to improved service, and/or lower overhead costs due to
more efﬁcient use of resources.

Existing works include the mechanisms of elastic
training and the resource management in an elastic train-
ing system. To achieve elasticity while training deep
learning jobs, the industry has proposed software frame-
works such as Google Kubernetes Engine (GKE) [6],

2

Amazon Elastic Container Service (ECS) [7], and Red
Hat OpenShift Container Platform [8]. Shen et al. [9]
presented an automatic elastic resource scaling system,
called CloudScale, for multi-tenant cloud computing in-
frastructures, though the infrastructures are not for the
purpose of deep learning training speciﬁcally. Other
than elasticity, resource management also plays a crit-
ical role in cloud system operations. One research
[10] formulated the resource management problem for
MapReduce jobs on the cloud as a constraint program-
ming model. Its objective is to reduce energy consump-
tion of MapReduce jobs. Liu and Xu [11] proposed
an executor scheduler that is able to dynamically allo-
cate and size resources to Spark jobs in order to min-
imize resource fragmentation. Another work by Javadi
et al. [12] also present a workload resource scheduler for
Spark jobs that can signiﬁcantly increase cloud resource
usage.

In recent years studies on resource allocation for
elastic training speciﬁcally are drawing more attention.
Chen et al. [13] found that the parameter server for deep
neural networks training could become performance
bottlenecks due to imbalanced workload distribution
among the parameter servers. They designed a dynamic
workload distribution scheme using an exploitation-
exploration method in order to accelerate distributed
model training. A similar study [14] also tried to solve
the imbalanced workload distribution problem using a
semi-dynamic load balancing approach, which acceler-
ates distributed training by up to 54%. Saxena et al. [15]
proposed a GPU-level resource allocator. The core idea
is to ﬁnd the best combination of batch size and number
of GPUs to elastically train deep learning jobs. Their ap-
proach took efforts on maximizing total throughput of
all jobs and searches for optimal combination through
dynamic programming. However, this approach only
works for GPU-level resource allocation, but not trans-
ferable to node-level. GPU-level resource allocation is a
form of process management that manages the execution
of individual software processes by individual processor
cores. Resource allocation at the node level, however,
implements a form of cluster management or container
management, both of which refers to the management
of containers (i.e., a bundle of a software program and
its data dependencies) and their execution by virtualized
clusters of processing resources. In addition, the allo-
cate decisions in this paper do not make sure that the
number of GPUs of a job is powers of two, which may
lower training accuracy [16]. Parallel computing typi-
cally requires that computational operations to be split
recursively by powers of two to avoid accuracy prob-
lems. Another practice in industry [16] tried to greedily
utilize all computational resources at all times. Section
3 thoroughly reviews this greedy resource allocator and

we will use it as benchmark for comparison with our
work.

This paper presents a novel resource allocator for
elastic training systems that takes advantage of the ETA
(estimated time of arrival), also known as estimated run-
time, of deep learning jobs as input. We ﬁrst formu-
late the resource allocation problem as a mixed-integer
programming (MIP) model that maximizes the training
progress of all jobs over a planning time horizon. The
model also uses a innovative method to make sure that
the allocated number of node to each job is 2m (m ∈
N). Obtaining the optimal resource allocate decisions is
very fast. The decisions can signiﬁcantly reduce queue-
ing delay and improve training efﬁciency. In addition,
the proposed allocator can better handle heterogeneous-
ETA jobs and perform robustly on ETA disturbance and
unexpected situations, such as jobs containing bugs and
users terminating jobs by themselves.

This paper is organized as follows. Section 2 in-
troduces the architecture of elastic training system of
Huawei ModelArts. Section 3 reviews a previous prac-
tice of Huawei ModelArts, i.e., the greedy resource allo-
cator and its limitations. Section 4 explains the method-
ology behind the optimal resource allocator and how we
linearize and simplify the MIP model to make it solvable
and efﬁcient. Section 5 takes advantages of real-world
job data from Huawei ModelArts and conducts simu-
lation experiments to compare the above two resource
allocators. In the end Section 6 concludes the paper.

2 Architecture of Elastic Training
System of Huawei ModelArts

Huawei ModelArts is a one-stop artiﬁcial intelligence
development platform of Huawei Cloud. Thousands of
deep learning jobs are training on this platform every
day. Elastic training is a service embedded in Mode-
lArts that provides the capability to dynamically scale
up or scale down distributive training jobs. Elastic train-
ing is critical to ModelArts since it holds the potential to
improve computational resource utilization and acceler-
ate user’s training jobs, and as a result, realize higher
proﬁts from users. Fig. 2 shows a screenshot of Mode-
lArts elastic training service. When a user submits a job
proﬁle to the platform, they need to specify the desired
number of nodes required for performing the training
job and the number of elastic nodes.

The architecture of elastic training system of Huawei
ModelArts consists of deep learning jobs, job queue, re-
source pool, resource allocator and other modules, as
illustrated in Fig. 3. Resource allocator is the core mod-
ule and it acts as the “brain” of the system to make the
best resource allocate decisions.

The job is deﬁned as the workload to be run on cer-
tain computational resource, i.e., GPUs, and 1 node is
made up of 8 GPUs. Jobs are trained distributively on
different number of nodes. The resource allocator can
make decision to scale-up or scale-down the computa-
tional resource of a certain job, i.e., number of nodes.
The jobs adapt to the changes of its resources and keep
on training without interruption.

To initiate a training job, ﬁrstly users should publish
an algorithm to the Algorithm Management Module, in-
cluding the algorithm code and the algorithm mirror.
The code and mirror will be pulled by the resource pool
after the job is allocated to certain nodes by the resource
allocator.

Once users have submitted a training job to the elastic
training system, the training system transfers the appli-
cation programming interface (API) body of the elastic
training job into an API body that the cluster manage-
ment module can address. The basic job proﬁle infor-
mation, including create time, starting time, ﬂavor, etc.
is stored in the database. The cluster management mod-
ule monitors the status of each workload and each node,
and makes real-time job allocation to the idle nodes.
The computing node mounts the cloud storage, pulls the
docker image from the hub, download the code, and then
start the script.

The resource allocator is the core module of the sys-
tem. It has access to all active jobs (queueing or train-
ing), job’s remaining ETA, and total computational re-
sources. Based on elastic training frequency f , e.g., 5
minutes, the resource allocator activates and makes de-
cisions on scaling-up or scaling-down an active training
job and how the nodes in the resource pool should be
re-assigned. At the end, the decisions are sent back to
job queue and resource pool for execution.

If some nodes are idle, the ﬁrst job in the queue will
start training using as many nodes as possible under a
cap (e.g., 16 nodes). If idle nodes still exist, follow the
same procedure to initiate the next queueing job until no
queuing jobs or no idle nodes exist.

Figure 2: The elastic training service of Huawei Mode-
lArts.

3

The greedy resource allocator ﬁnds the training job with
the longest training time, scales down the training job
through reducing its node count by half, and then allo-
cates the released nodes to the training job at the front
of the job queue.

In the fourth scenario, the system has no idle nodes
and no training jobs in the job queue. This is the sim-
plest scenario. All nodes are occupied and no training
jobs are waiting. In this case, the elastic training system
changes nothing about the current node allocation.

Fig. 4 gives two examples that help better understand
the rules of greedy resource allocator. Assume the range
of scaling is 1∼16 nodes. For the ﬁrst example on the
top, the elastic training system has 2 idle nodes but no
queueing jobs. There are four jobs training currently and
job 4 has the shortest training time, so the 2 idle nodes
are allocated to it. For the second example at the bottom,
job 8 is waiting but the system has no idle nodes. Job
5 has the longest training time, so the allocator scales
down its node size from 4 to 2 and assigns the released
2 nodes to job 8.

Figure 4: Two examples for greedy resource allocator.

The resource allocator is called “greedy” because it
always tries to utilize the system’s computational re-
sources to their fullest extent, i.e., leave no nodes idle.
The rules governing its behavior tend to be simple and
computationally fast. However, the simplicity of its be-
havior results in several limitations.

First, while the greedy resource allocator keeps as
many nodes working as possible, the allocation of nodes
to training jobs may not be efﬁcient or fair. For example,
a greedy resource allocator may inefﬁciently allocate 99

4

Figure 3: Architecture of elastic training system for
deep learning jobs on cloud.

3 Greedy Resource Allocator

One previous work [16] of Huawei Cloud proposed a
greedy resource allocator for elastic training. This is a
rule-based allocator that tries to greedily utilize as many
nodes as possible. We use this allocator as benchmark
for comparison with our work. Its rules are explained as
follows.

The greedy resource allocator allocates the resource
pool of the elastic training system based on four differ-
ent scenarios, in which every training job is allocated a
node count within a range, such as 1 to 16 nodes.

In the ﬁrst scenario, the system has at least one idle
node and at least one training job in the job queue. The
greedy allocator allocates as many nodes as possible to
the training job at the front of the job queue. If there are
still idle nodes and training jobs in the job queue, this
procedure is repeated until all nodes are occupied or all
training jobs have exited the job queue and are being
performed.

In the second scenario, the system has at least one
idle node and no training jobs in the job queue. The
greedy resource allocator ﬁnds the training job with the
shortest training time, and then scales up this training
job by increasing its node count as large as possible. If
there are still idle nodes, this procedure is repeated until
all nodes have been occupied or all training jobs have
scaled up.

In the third scenario, the system has no idle nodes
and at least one training job in the job queue. Thus, the
computation resources of the system have reached their
limit. Some training jobs might be occupying all the
nodes while many others have to wait in the job queue.

nodes to job 1 and 1 node to job 2, instead of allocat-
ing 50 nodes to each job (job 1 and job 2). Although
both allocations utilize all 100 nodes, the second one is
obviously more equal and may result in more overall ef-
ﬁciency.

Second, training time may not be a good metric to
use in deciding which job should scale up or down.
The greedy resource allocator scales up the job with
the shortest training time, but if this job has a very
small workload, one node may be sufﬁcient; the addi-
tional nodes might be more effectively deployed to a
larger training job. Similarly, the greedy resource allo-
cator scales down the job with the longest training time,
but this may result in computationally intensive training
jobs having their node count reduced repeatedly, thereby
resulting in unnecessarily long training times.

Third, the greedy allocate decisions are short-sighted.
The allocator only deals with what is currently happen-
ing in the elastic training system, but has no considera-
tion for the future. Because the system will face differ-
ent computational demands in the future, it is necessary
to look ahead and plan computational resource alloca-
tion accordingly.

4 Optimal Resource Allocator

This paper proposes an optimal resource allocator that
can overcome the above limitations of the greedy one.
This section presents the methodology behind it. Please
refer to Table 1 for all notations. We adopt a rolling-
horizon approach to plan resource allocation for the fu-
ture. The optimization problem is ﬁrst formulated as a
mixed-integer non-linear programming model. Then we
linearize and simplify the model into a mixed-integer
linear one to make it solvable.

4.1 A Rolling-horizon Approach

Sets
I
T

K

Parameters
di

ni,min
ni,max

N
f
p
M
Decision
variables
nt
i

ni

st
i

yt
i

δt,−
i,k ,
δt,+
i,k

Table 1: Notations

set of deep learning jobs i
set of time steps t in a planning time
horizon, i.e., {1, 2, 3, · · · }
set of legal number of nodes k,
{1, 2, 4, 8, 16, 32, · · · }

i.e.,

remaining ETA or computation demand
of job i ∈ I
minimum number of nodes for job i ∈ I
maximum number of nodes for job i ∈
I
number of nodes in resource pool
elastic training frequency (unit: minute)
time step adjustment parameter
the big-M , a sufﬁciently large number

allocated number of nodes for job i ∈ I
at time step t ∈ T , integer
implemented allocate decision for job
i ∈ I, integer
served computation demand for job i ∈
I at time step t ∈ T
whether job i ∈ I has ﬁnished training
at time step t ∈ T , binary
indicators for selecting k ∈ K nodes for
job i ∈ I at time step t ∈ T , binary

4 nodes for the next three time steps, respectively, so job
1 might be done in only 2 hours.

The optimal resource allocator adopts a rolling-horizon
approach. This approach discretizes a planning time
horizon T into multiple time steps t ∈ T = {1, 2, 3, ...},
and the length of each time step equals the elastic train-
ing frequency f , e.g., 5 minutes. Therefore, this ap-
proach can look ahead and plan decisions for the future
accordingly. Consider a job’s remaining ETA as its com-
putation demand. For each job i ∈ I at time step t ∈ T ,
use nt

i out of job i’s demand di.

i nodes to serve st

Fig. 5 is an example in which the optimal resource
allocator looks 4 hours ahead at 0:00. If elastic training
frequency is 30 minutes, there will be 8 time steps. Job
1 is submitted at 0:00 and its ETA is 3 hours on a single
node. We may use n1
1 node·hr
demand out of the total 3 node·hr. We may use 2, 1, and

1 = 1 node to serve s1

5

Figure 5: The rolling-horizon approach.

4.2 Non-linear Formulation

The objective of optimal resource allocator is to maxi-
mize the training progress of all the jobs I over the look-
ahead time horizon T , as shown in (1). A job’s training
progress is deﬁned as its total served demand during T ,
i.e., (cid:80)

i, over its demand di.

t st

(cid:88)

(cid:88)

max

i

t

st
i
di

subject to

st
i ≤ di ∀i ∈ I, ∀t ∈ T

i ≤ p · nt=1
st=1

i

· 0.8log2 nt=1

i

∀i ∈ I

(1)

(2)

(3)

i·0.8log2 nt

i

∀i ∈ I, ∀t ∈ T, t ≥ 2 (4)

i ≤ st−1
st

i +p·nt
nt

i ≤ ni,max ∀i ∈ I, ∀t ∈ T

nt

i ≤ N ∀t ∈ T

(cid:88)

i

yt
i ≤

st
i
di

∀i ∈ I, ∀t ∈ T

yt
i ≥ 1 − M (1 −

st
i
di

) ∀i ∈ I, ∀t ∈ T

nt=1

i ≥ ni,min ∀i ∈ I

(5)

(6)

(7)

(8)

(9)

i ≥ ni,min − M · yt−1
nt

i

∀i ∈ I, ∀t ∈ T, t ≥ 2 (10)

nt

i ≤ M (1 −

st−1
i
di

) ∀i ∈ I, ∀t ∈ T, t ≥ 2

(11)

nt

(14)

(13)

(12)

i ≥ 0, integer ∀i ∈ I, ∀t ∈ T
st
i ≥ 0 ∀i ∈ I, ∀t ∈ T
yt
i ∈ {0, 1} ∀i ∈ I, ∀t ∈ T
Constraint (2) makes sure that served demand st
i at
time step t cannot overtake the job’s demand. Con-
straint (3) and (4) are the training process in which p
is the time step adjustment parameter, for example, p =
30/60 = 0.5, if each time step is 30 minutes. Given nt
i
nodes to train job i at time step t, the served demand
i · 0.8log2 nt
will accumulate by p · nt
i . This non-linear re-
lationship between number of nodes and training speed,
as shown in Fig. 6 is found from the historical data of
Huawei ModelArts [3]. The ideal training speed is lin-
early related to the number of nodes, however, the actual
training speed decreases by 20% on average when the
number of nodes doubles (i.e., multiply linear training
speed by 0.8 every time the number of nodes doubles).
While a job can be trained faster using more nodes, how-
ever, the speed attenuation becomes larger as node count
increases. System-level training efﬁciency is likely to
downgrade if only a few jobs are monopolizing all the
resources.

Constraint (5) and (6) are straight-forward. Every job
should have an upper bound of node size, denoted as
ni,max. The allocated nodes must not overtake total re-
sources, i.e., N nodes. However, the lower bound condi-
tions are quite tricky. On the one hand, we must allocate
a minimum number of nodes ni,min (usually 1 node)

6

Figure 6: Relationship between number of nodes and
training speed.

when job i is training. On the other hand, if job i has
ﬁnished training at time step t, then its node size should
return back to zero starting from the next time step t+1.
To satisfy the two requirements, we have to introduce an
extra binary variable yt
i to indicate whether job i has ﬁn-
ished training at time step t and a sufﬁciently-large num-
ber M . If job i has not ﬁnished yet, yt
i = 0; otherwise,
yt
i = 1, as shown in Constraint (7) and (8). Constraint
(9) and (10) ensure that minimum resource is allocated if
a job is still training. Constraint (11) let node size return
back to zero after a job has done. Constraints (12)∼(14)
are the bounds of decision variables.

However, the above formulation is a mixed-integer
non-linear programming model and hard to solve. The
training process constraints (3) and (4) are non-linear
and may be not easy to satisfy. Constraints (7)-(11) con-
tain too many integer decision variables and constraints,
which may make the model very slow to ﬁnd optimal so-
lutions. Note that decision-making for our resource allo-
cation problem should be real-time or near real-time. In
addition, the allocate number of nodes for each job must
be 2m (m ∈ N) out of training accuracy reasons, i.e.,
nt
i ∈ K = {1, 2, 4, 8, 16, 32, ...}, but this formulation
cannot meet this requirement.

4.3 Linearized and Simpliﬁed Formula-

tion

We must linearize the non-linear constraints and sim-
plify the above model formulation to make it solvable
and fast. First, we drop the binary variables yt
i and re-
place constraints (7)-(11) with a new constraint (15).
This constraint allows nodes not return back to zero af-
ter a job has ﬁnished training. The impact is minimal be-
cause the optimal resource allocator adopts the rolling-
horizon approach in which only the decision in the ﬁrst
time step, i.e., ni = nt=1
, will be implemented. Taking
Fig. 7 as an example, after the job has ﬁnished training

i

at the second time step, we allow the minimum number
of nodes (1 node) in the optimal solution and allocate
2 nodes (the decision of the ﬁrst time step) to train this
job. The problem size decreases signiﬁcantly by simpli-
fying model formulation. For a problem with 100 jobs
and 5 time steps, we can save 500 integer variables and
1800 constraints.

nt

i ≥ ni,min ∀i ∈ I, ∀t ∈ T

(15)

Figure 7: An example for nodes not returning back to
zero after a job has ﬁnished.

i,k = δt,+

i,k and δt,+

The powers-of-two requirement can be met by intro-
ducing two binary indicators, δt,−
i,k , and con-
straints (16)-(18). Allocate nt
i = k ∈ K nodes to job
i at time step t if and only if δt,−
i,k = 1. Con-
straints (16) and (17) add lower and upper bounds to the
difference between nt
i and k. Constraint (18) ensures
that exactly |K| + 1 inequalities in (16) and (17) must
hold, thus the value of k will be a selection from the set
K. For example, if the range of elastic training is from
1 node to 16 nodes, i.e., k ∈ K = {1, 2, 4, 8, 16}, we
may allocate n1
1 = k = 4 nodes to job 1 at time step
1 if and only if δ1,−
1,4 = 1. In this case, exactly
1 ≥ 1, n1
|K|+1 = 6 inequalities must hold, i.e., n1
1 ≥ 2,
1 ≥ 4, n1
1 ≤ 4, n1
n1
1 ≤ 16, while the re-
maining 4 inequalities, i.e., n1
1 ≤ 2, n1
1 ≥ 8,
and n1
1 ≥ 16, must not hold. By this method, the opti-
i must be 2m (m ∈ N).
mal value of each nt

1 ≤ 8, and n1

1,4 = δ1,+

1 ≤ 1, n1

− M · δt,−

i,k ≤ nt

i − k ≤ M · (1 − δt,−
i,k )

∀i ∈ I, ∀t ∈ T, ∀k ∈ K

− M · δt,+

i,k ≤ k − nt

i ≤ M · (1 − δt,+
i,k )

∀i ∈ I, ∀t ∈ T, ∀k ∈ K

(17)

(cid:88)

δt,−
i,k +

(cid:88)

δt,+
i,k = |K| + 1 ∀i ∈ I, ∀t ∈ T (18)

k
k
Since binary variables δt,−

i,k indicate whether
the allocated number of nodes is a selection from the

i,k and δt,+

1 − δt,−
i,k
M

1 − δt,+
i,k
M

set K, we can replace constraints (3)-(4) with two linear
constraints (19)-(20). If a k ∈ K is selected, the training
speed will exactly be k · 0.8log2 k, as shown by the actual
training speed curve in Fig. 6.

st=1
i ≤ p ·

(cid:88)

k

k · 0.8log2 k · (δt=1,−

i,k + δt=1,+

i,k − 1)

∀i ∈ I

(19)

i ≤ st−1
st

i + p ·

(cid:88)

k

k · 0.8log2 k · (δt,−

i,k + δt,+

i,k − 1)

∀i ∈ I, ∀t ∈ T, t ≥ 2

(20)

i,k and δt,+

Proposition. The binary variables δt,−

i,k do
not introduce estimation errors to actual training speeds.
Proof . If k∗ ∈ K is selected as the optimal solution
for job i at time step t, then we have δt,−
i,k∗ = 1.
By (18), it is easy to know that for k ∈ K and k (cid:54)= k∗,
i,k +δt,+
δt,−
i,k = 1 must hold. Therefore, the training speed
is (cid:80)
k k · 0.8log2 k · (δt,−
i,k − 1) = 1 · 0.8log2 1 · (1 −
1) + · · · + k∗ · 0.8log2 k∗
· (2 − 1) + · · · = k∗ · 0.8log2 k∗
,
which is exactly the actual speed as shown in Fig. 6.

i,k∗ = δt,+

i,k + δt,+

4.4 Final Formulation

In summary, we re-formulate the optimal resource al-
locator as a mixed-integer linear program (MILP) as
follows. We use the open-source optimization solver
CBC [17] to search for optimal solutions.

subject to

max
n

(cid:88)

(cid:88)

i

t

st
i
di

st
i ≤ di ∀i ∈ I, ∀t ∈ T

nt

i ≤ ni,max ∀i ∈ I, ∀t ∈ T

nt

i ≤ N ∀t ∈ T

(cid:88)

i

(16)

nt

i ≥ ni,min ∀i ∈ I, ∀t ∈ T

1 − δt,−
i,k
M

1 − δt,+
i,k
M

− M · δt,−

i,k ≤ nt

i − k ≤ M · (1 − δt,−
i,k )

∀i ∈ I, ∀t ∈ T, ∀k ∈ K

− M · δt,+

i,k ≤ k − nt

i ≤ M · (1 − δt,+
i,k )

∀i ∈ I, ∀t ∈ T, ∀k ∈ K

δt,−
i,k +

(cid:88)

k

(cid:88)

k

δt,+
i,k = |K| + 1 ∀i ∈ I, ∀t ∈ T

7

st=1
i ≤ p ·

(cid:88)

k

k · 0.8log2 k · (δt=1,−

i,k + δt=1,+

i,k − 1)

∀i ∈ I

i ≤ st−1
st

i + p ·

(cid:88)

k

k · 0.8log2 k · (δt,−

i,k + δt,+

i,k − 1)

∀i ∈ I, ∀t ∈ T, t ≥ 2

st
i ≥ 0 ∀i ∈ I, ∀t ∈ T

nt

i ≥ 0, integer ∀i ∈ I, ∀t ∈ T

i,k , δt,+
δt,−

i,k ∈ {0, 1} ∀i ∈ I, ∀t ∈ T, ∀k ∈ K

5 Simulation

To compare the proposed optimal resource allocator
with the greedy one that Huawei Cloud previously im-
plemented, this section designs a system that simulates
the training process of deep learning jobs on the cloud.
Several experiments were conducted to examine each
resource allocator’s queueing delay, training efﬁciency,
performance on heterogeneous-ETA jobs, robustness to
ETA disturbance, and performance under scaling delay.
In addition, this section shows efﬁciency of the optimal
resource allocator.

5.1 Simulation Setup

We set up a simulation framework to conduct experi-
ments for the elastic training system. The system’s al-
locate decisions are made either by the optimal resource
allocator or the greedy resource allocator. Simulation
moves forward second by second. For each second, the
system could keep training some jobs, initiate queueing
jobs, or ﬁnish training some others. Simulation updates
the status of each job each second. Resource allocator
makes decisions every 5 minutes, for example, 9:00:00,
9:05:00 and 9:10:00. The look-ahead time horizon is 25
minutes and the length of each time step equals the elas-
tic training frequency, so there are 5 time steps. When
simulation arrives at scaling moments, resource alloca-
tor activates and tells resource pool how to scale up or
down jobs. During regular times, simulation allocates
maximum resources to queueing jobs, if any. For ex-
ample, in Fig. 8 job 1 ﬁnishes training at 9:02:09 and
releases 8 nodes. Job 2 joins queue since 9:01:02, so 1
second later the system allocates the 8 nodes to it.

Data used for simulation come from Huawei Mod-
elArts for deep learning jobs. There were 1252 jobs
submitted from January 24, 2021 14:25:41 to January
26, 2021 16:07:19, spanning around 2 days and 2 hours.
On average, a job spent 2.8 minutes in the queue, 84.3
minutes for training, and 87.1 minutes in total. Among

these jobs 447 of them spent training time of ≥5 min-
utes. The average queueing time is 7.2 minutes, training
time is 232.6 minutes, and total time is 239.9 minutes.
We select the 447 jobs as simulation baseline.

Figure 8: Example for resource allocation at regular
times in simulation.

5.2 Queueing Delay and Training Efﬁ-

ciency

We assume that total computational resources are rang-
ing from 70 to 190 nodes with an interval of 20 for the
baseline data and examine the average queueing time
per job. In Fig. 9, queueing delay shows a downward
trend as more nodes are available in resource pool for
both greedy and optimal resource allocators. The opti-
mal one always reduces queueing time given the same
resources. The largest decrease is 32% at 150 nodes.

Figure 9: Comparison of queueing time.

Total time is queueing time plus training time, and it
indicates how long users can get training results after
they have submitted a job. It is seen from Fig. 10 that
total time is declining as more available nodes train jobs
for both resource allocators. The optimal one can al-
ways accelerate training given the same resources. The
improvement is shown in Fig. 11. When the greedy re-
source allocator has trained 100 jobs, the optimal one
can train more and the additional trained jobs are illus-
trated by the blue bars. The improvement is up to 17.4
jobs given 110 nodes.

8

Note that efﬁciency improvement

is bell-shaped.
When computational resources are relatively limited,
jobs tend to occupy as fewer nodes as possible, no matter
what resource allocator is. An extreme example could
be that the number of training jobs equals node size, so
the optimal decision would be to let each job occupy
only one node. On the contrary, when computational re-
sources are abundant the best decision would be to allo-
cate as many nodes as possible to all jobs. Thus, the gap
between the two resource allocators becomes narrower.

job ETA becomes more heterogeneous than the base-
line scenario. Fig. 12 shows that up to additional 24.1
jobs can be completed by optimal resource allocator,
compared to the baseline 17.4 jobs in Fig. 11. There-
fore, the optimal resource allocator can better handle
heterogeneous-ETA jobs than the greedy one.

Figure 12: Additional trained jobs by optimal resource
allocator when ETA becomes more heterogeneous.

Figure 10: Comparison of total time.

5.4 Robustness

It is almost impossible to predict job ETA 100% accu-
rately [18–20]. We add ±10% disturbance to ETA for the
baseline scenario in simulation. For example, if a job’s
actual runtime is 10 node·hr, its ETA will be a random
sample from the range of 9∼11 node·hr. Fig. 13 shows
the additional trained jobs by optimal resource allocator
with the disturbance. Compared to the no-disturbance
baseline in Fig. 11, the additional trained jobs are only
0.6∼2.4 fewer.

Figure 11: Additional jobs trained by optimal resource
allocator.

5.3

Impact of Heterogeneous-ETA Jobs

Small-ETA jobs with training time of less than 5 min-
utes account for 64% in our simulation dataset. Elasti-
cally training small-ETA jobs or not remains a question.
On the one hand, these jobs might take a little time to
ﬁnish training even if the allocated nodes are little. On
the other hand, small-ETA jobs take a signiﬁcant portion
and may slow down the entire training efﬁciency if their
resources are insufﬁcient.

For this simulation, we let elastic training system
scales all 1252 jobs whatever their ETA is. Therefore,

Figure 13: Robustness under ±10% ETA disturbance.

It is also common that users submit jobs that contain
bugs. Bug jobs usually hang up within a few minutes
once starting training. Their ETA and actual training
duration could be signiﬁcantly different because bugs

9

are almost unpredictable. Users may also terminate jobs
that are training at any time out of many reasons, e.g.,
losing patience. We conduct a simulation based on the
baseline data in which 75% jobs have ±10% ETA dis-
turbance, 15% jobs contain bugs and hang up randomly
within 5 minutes, and 10% jobs could be terminated by
users at any time. Fig. 14 shows that there are still up
to 15.0 additional jobs completed by optimal resource
allocator under such harsh conditions.

Figure 14: Robustness under ETA disturbance, bug jobs
and user-terminated jobs.

The reasons optimal resource allocator is robust are
that it adopts a rolling-horizon approach and makes al-
locate decisions every a few minutes. Even if estimation
errors, bug jobs and user-terminated jobs enlarge the dif-
ference between ETA and actual runtime, the negative
impact on system training efﬁciency is small.

5.5

Impact of Scaling Delay

The above simulations assume no delay when a job initi-
ates or scales. The time of scaling-down is usually min-
imal while initiation or scaling-up takes 10∼20 seconds
on average. For this simulation, we take 15-second de-
lay into consideration. When a job just starts training
or resource allocator decides to scale up a job, it will
stay on its current nodes for 15 seconds before executing
the new decision. Fig. 15 shows the additional trained
jobs by optimal resource allocator with 15-second delay.
The differences compared to the baseline scenario are
merely -0.9∼1.8 additional jobs. The reason for the dif-
ferences could be that during the 15-second delay some
jobs are still executing the last allocate decisions that
are not optimal at this moment, so the additional trained
jobs become slightly fewer.

5.6 Speed of Optimal Resource Allocator

Resource allocation for elastic training should be real-
time or near real-time. We have recorded the time spent

Figure 15: Additional trained jobs by optimal resource
allocator with 15-sec scaling delay.

for every decision-making by optimal resource allocator
in simulation. The baseline scenario is provided with
70 nodes. The simulation runs on a Linux server with
Intel i7-8700K CPU and 64 GB RAM. Fig. 16 is the
histogram of solution time and shows the optimal re-
source allocation is very fast in decision-making, spend-
ing merely 0.4 seconds on average. The median time is
0.24 seconds. For 95% cases, solution time is under 1.49
seconds (95pth), and the maximum time is not over 2.48
seconds.

Figure 16: Histogram of solution time by optimal re-
source allocator.

6 Conclusion

This paper proposes an optimal resource allocator of
elastic training for deep learning jobs on cloud. The
allocator adopts a rolling-horizon approach and maxi-
mizes training progress of all jobs over a planning time
horizon. The original model formulation contains non-
linear constraints and many integer variables, which are
hard to optimize. We simplify and linearize the model
formulation into a MILP that is smaller-scaled, more

10

solvable and faster. We also introduce an innovative
method to make sure that allocated number of nodes
meet the powers-of-two requirements.

We design a simulation framework to conduct exper-
iments to elastic training systems with the optimal re-
source allocator and a greedy one as benchmark. For the
baseline scenario, the optimal resource allocator can re-
duce queueing time by up to 32% and accelerate training
efﬁciency by up to 17.4%, which greatly improves user
experience. Also, the optimal resource allocator can bet-
ter handle heterogeneous-ETA jobs than the greedy one,
training up to 24.1 additional jobs. Simulations that test
robustness show that the optimal allocator is very robust
to ETA disturbance, bug jobs and user-terminated jobs.
The impact of 15-second scaling delay is also examined
and the additional trained jobs are merely different from
the no-delay baseline. Also, searching for optimal solu-
tions is very fast, taking only 0.4 seconds on average.

References

[1] Amazon Web Services. Getting Started with AWS.
Retrieved from https://aws.amazon.com/getting-
started/.

[2] Microsoft Azure. Get to know Azure. Retrieved
from https://azure.microsoft.com/en-ca/overview/.

[3] Huawei
from
us/product/modelarts.html.

Cloud.
Retrieved
ModelArts.
https://www.huaweicloud.com/intl/en-

[4] Alibaba Cloud. Overview of Auto Scaling. Re-

trieved from https://bit.ly/3aBlIWi.

[5] Lin, H., Zhang, H., Ma, Y., He, T., Zhang, Z., Zha,
S. and Li, M., 2019. Dynamic mini-batch SGD for
elastic distributed training: learning in the limbo
of resources. arXiv preprint arXiv:1904.12043.

[6] Kubernetes. Production-Grade Container Orches-
tration. Retrieved from https://kubernetes.io/.

[7] Amazon Web
Container

Services.
Service.
tic
https://aws.amazon.com/ecs/.

Amazon
Retrieved

Elas-
from

Red
Platform.

[8] Red

Hat.

Hat

OpenShift

tainer
https://www.openshift.com/products/container-
platform.

Retrieved

Con-
from

[9] Shen, Z., Subbiah, S., Gu, X. and Wilkes, J., 2011,
October. Cloudscale: elastic resource scaling for
multi-tenant cloud systems. In Proceedings of the
2nd ACM Symposium on Cloud Computing (pp.
1-14).

11

[10] Gregory, A. and Majumdar, S., 2016, March. A
constraint programming based energy aware re-
source management middleware for clouds pro-
cessing mapreduce jobs with deadlines. In Com-
panion Publication for ACM/SPEC on Interna-
tional Conference on Performance Engineering
(pp. 15-20).

[11] Liu, L. and Xu, H., 2018, October. Elasecutor:
Elastic executor scheduling in data analytics sys-
tems. In Proceedings of the ACM Symposium on
Cloud Computing (pp. 107-120).

[12] Javadi, S.A., Suresh, A., Wajahat, M. and Gandhi,
A., 2019, November. Scavenger: A black-box
batch workload resource manager for improving
utilization in cloud environments. In Proceedings
of the ACM Symposium on Cloud Computing (pp.
272-285).

[13] Chen, Y., Peng, Y., Bao, Y., Wu, C., Zhu, Y. and
Guo, C., 2020, October. Elastic parameter server
load distribution in deep learning clusters. In Pro-
ceedings of the 11th ACM Symposium on Cloud
Computing (pp. 507-521).

[14] Chen, C., Weng, Q., Wang, W., Li, B. and Li, B.,
2020, October. Semi-dynamic load balancing: ef-
ﬁcient distributed learning in non-dedicated envi-
ronments. In Proceedings of the 11th ACM Sym-
posium on Cloud Computing (pp. 431-446).

[15] Saxena, V., Jayaram, K.R., Basu, S., Sabharwal, Y.
and Verma, A., 2020, November. Effective elastic
scaling of deep learning workloads. In 2020 28th
International Symposium on Modeling, Analysis,
and Simulation of Computer and Telecommunica-
tion Systems (MASCOTS) (pp. 1-8). IEEE.

[16] Zhu, J., Huang, Z., Wu, R., Bai, X., Yang, B., Li,
Y., Zheng, H. and Dai, Z. 2020. A Design and Im-
plementation Method for Elastic Distributed Train-
ing Systems. Chinese patent, 87068967CN02.

[17] Forrest, J. and Lougee-Heimer, R. CBC User
from https://www.coin-

Guide.
Retrieved
or.org/Cbc/cbcuserguide.html.

[18] Pham, T.P., Durillo, J.J. and Fahringer, T., 2017.
Predicting workﬂow task execution time in the
cloud using a two-stage machine learning ap-
proach. IEEE Transactions on Cloud Computing,
8(1), pp.256-268.

[19] Sidhanta, S., Golab, W. and Mukhopadhyay,
S., 2016, May. Optex: A deadline-aware cost
In 2016 16th
optimization model

for spark.

IEEE/ACM International Symposium on Cluster,
Cloud and Grid Computing (CCGrid) (pp. 193-
202). IEEE.

[20] Mustafa, S., Elghandour, I. and Ismail, M.A.,
2018. A machine learning approach for predicting
execution time of spark jobs. Alexandria engineer-
ing journal, 57(4), pp.3767-3778.

12

