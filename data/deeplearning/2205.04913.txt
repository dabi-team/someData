2
2
0
2

y
a
M
0
1

]
E
S
.
s
c
[

1
v
3
1
9
4
0
.
5
0
2
2
:
v
i
X
r
a

CROSS-LANGUAGE SOURCE CODE CLONE DETECTION USING
DEEP LEARNING WITH INFERCODE

A PREPRINT

Mohammad A. Yahya

Department of Computer Science
Oakland University
Rochester, Michigan
yahya@oakland.edu

Dae-Kyoo Kim
Department of Computer Science
Oakland University
Rochester, Michigan
kim2@oakland.edu

May 11, 2022

ABSTRACT

Software clones are beneﬁcial to detect security gaps and software maintenance in one program-
ming language or across multiple languages. The existing work on source clone detection performs
well but in a single programming language. However, if a piece of code with the same functional-
ity is written in different programming languages, detecting it is harder as different programming
languages have a different lexical structure. Moreover, most existing work rely on manual feature
engineering. In this paper, we propose a deep neural network model based on source code AST
embeddings to detect cross-language clones in an end-to-end fashion of the source code without the
need of the manual process to pinpoint similar features across different programming languages.
To overcome data shortage and reduce overﬁtting, a Siamese architecture is employed. The design
methodology of our model is twofold – (a) it accepts AST embeddings as input for two different
programming languages, and (b) it uses a deep neural network to learn abstract features from these
embeddings to improve the accuracy of cross-language clone detection. The early evaluation of the
model observes an average precision, recall and F-measure score of 0.99, 0.59 and 0.80 respectively,
which indicates that our model outperforms all available models in cross-language clone detection.

Keywords deep neural networks · cross language code clone detection · abstract syntax trees

1

Introduction

Code clone (CL) detection is the process of detecting similar code fragments. Some code fragments are copied online
without realizing potential negative effects (e.g., security threats, increasing complexity). Detecting such clones is
easier if the code snippets are in the same programming language [1, 2, 3, 4, 5, 6]. Duplicate functionalities are
common in a large software system where the software is often written in multiple programming languages. If one
functionality is to be updated or removed, this has to be reﬂected on all clones. On one hand, the latest approach by
Perez and Chiba [4] on cross-language clones detection relies on skip-gram models over AST. However, it ignores
the morphology of tokens, which impairs the accuracy of detection. Furthermore, it is not clear how their model can
capture the trees that greatly differ syntactically. For example, although the ASTs of the code snippets in Fig. 2 look
completely different as they are written in different programming languages, they have the same functionality. So
it becomes difﬁcult for an AST-based model to recognize such a clone as reported in their evaluation. This shows
that a careful selection of source code embedding technique is critical for accuracy as we will discuss how different
embeddings affect F1 score.

Detecting code clones has long been an active area of research [7] with various approaches proposed including token,
AST, metrics, binary and graph-based approaches [8, 3, 9, 10, 11, 12]. Most existing work focuses on clone detection
in the same programming language. However, common APIs (e.g., Apache Spark) for big data processing have
similar naming and call patterns written for different programming languages [5, 4]. Not all code bases use similar

 
 
 
 
 
 
Cross-Language Source Code Clone Detection Using Deep Learning with InferCode

A PREPRINT

APIs. To date, the latest approaches are not end-to-end, meaning that exhaustive feature engineering has to take
place to prepare data. Our preliminary study [13] shows that about 90% of code clone detection using deep learning
was performed on top of a single language and only %10 was about cross-language clone detection. The latest
approaches on cross-language code clone detection use low quality features for training, which makes it difﬁcult to
train a Siamese architecture which we use in our work to improve accuracy. Code clone ﬁts into four main categories–
Type I which is textual clones (i.e., identical code), Type II is lexical clones (e.g., different identiﬁers), Type III which
is syntactic clones (e.g., additional statements), and Type IV which is semantic clones or purely semantic (i.e., the
same logic) [14, 15]. In this work, we focus on Type III. It has been reported that Type III clones are more frequent
than other types of clones [16, 17, 18].

In this paper, we present an end-to-end cross-language clone detection approach based on embeddings of ASTs using
InferCode [19]. Inspired by TBCNN [20, 21], InferCode captures the syntactical essence of a program as a vector.
Our model takes the pre-trained embeddings from InferCode as input to learn abstract features by grouping related
embeddings through the contrastive loss function which guarantees to have similar embeddings together, but dissimilar
embeddings far apart. We make the following contributions.

• We present a cross-language clone detection across any two languages. In this work, we demonstrate ﬁnding

clones between Java and Python source code.

• We created a dataset of pairs containing around 40,000 code snippets that contain code that has similar and

dissimilar pairs.

• We improved the state-of-the-art work for cross-language code clone detection by over %10.

The remainder of the paper is organized as follows. Section 2 covers up-to-date approaches in clone detection with their
pros and cons. Section 3 discusses InferCode in comparison with other embedding techniques. Section 4 describes the
proposed model. Section 5 presents the results of evaluation. Section 6 concludes the work with discussion on future
work.

2 Related Work

There exist several comprehensive literature reviews on code clone detection which provide a great insight on the trend
in the area. Ratten et al. [22] reviewed the papers published until 2012 on Type I-IV clones and approaches to detect
them. Azeem et al. [23] conducted a survey of the papers published from 2000 to 2017 that covers how machine
learning is used to detect code smells including duplicate code that can be similar to code clone. Maggie et al. [13]
added to the previous by including up-to-date deep learning approaches on both single-language and cross-language
clone detection where they found only a little work existing on the latter and emphasized the need for more research
effort in the area. Sobrinho et al. [24] covered types of sub-optimal code or bad smells that may lead to undesirable
effects on code maintenance, where bad smells can be any of duplicate code (clone) and large class.

Wei and Li [25] used LSTM to learn representations of code fragments in clone detection in a single language. LSTM
uses seq2seq to learn output sequence from input sequence. LSTM autoencoder [26, 27] reconstructs output from
decoded input one step at a time, which makes it suitable for sequence generation. The main disadvantage of seq2seq-
based LSTM is their bottleneck issues as information tends to get lost, which affects the prediction capability of the
model [28, 29]. LSTM may be applied to cross-language clone detection where it can produce next AST tokens in
one language for given AST tokens encoded in another language. We followed the same approach in our LSTM
experiment but over InferCode embeddings.

Perez et al. [4] used a 45k dataset of Java and Python clones. A vocabulary map is generated from a set of AST
trees of programs, containing a set of tokens together with their types and values. Then, a skip-gram model is used
to generate a token-level node vector representation where nodes belong to an AST of source code in Python or Java.
These token-level vectors of nodes are then fed as input to a supervised neural model that learns the embeddings of the
vectors so that it becomes easier to compare two probable clones based on their embedding. They take the advantage
of AST structures to deﬁne the context of a token in AST instead of windows used in natural language processing.
A node context is customizable in the skip-gram model to include as many children as possible. The context also
includes one parent, which establishes a child-parent relationship in the context. The context of a node is deﬁned by
the ancestors, descendants, and siblings of the node within a given window which is amenable. The skip-gram model
used in their work is very similar to Tree-based Convolutional Neural Network (TBCNN) [20, 21]. The later, however,
uses a context window of size 2. They also used three additional algorithms to ﬁnd the context of a node and produced
a list of node indices along with node contexts. Once data generation is done, the data is used to train the skip-gram
model using the negative sampling objective loss function. Finally, the vectors learned through the skip-gram model
are passed to a Siamese network to ﬁnd similarity score between two code snippets written in Java and Python. They

2

Cross-Language Source Code Clone Detection Using Deep Learning with InferCode

A PREPRINT

found that increasing the sibling window decreases clone detection, while keeping the window size short for ancestors
and descendants increases clone detection accuracy, which is conﬁrmed with TBCNN results.

Sajnani et al. [2] developed a SourcererCC that is token-based large-scale clone detection tool based on the lexical
approach [30], which has the ability to achieve high precision and recall as compared to other tools and works best
with near-miss Type III clone (not Type IV though), where Type III differ at statement level and near miss to mean that
minor to signiﬁcant editing changes occur in the copy/pasted fragments. They included 25k projects with 250 millions
of code and 3 million examples. The tool works by comparing bag-of-tokens of code fragments that uses an optimized
partial index of sub-blocks of a code block to quickly query the potential clones to achieve high scalability. The code
block is reduced to (token, f requency) pairs, where f requency denotes the number of times token appeared in a
code block. A token could be a keyword, literal, and identiﬁer from a given source code. They use overlap similarity
function between two pairs of source code. Overlap has to achieve certain threshold α to be considered as a clone.

Saini et al. [1] put a method-level, named Oreo, to detect clones in the twilight zone that is between Type III and Type
IV, which is very difﬁcult to detect. Oreo combines machine learning, information retrieval, and lexical-based software
metrics, semantic scanner on methods to ﬁnd clones. BigCloneBench dataset has a variety of clone types between Type
III and Type IV including Very Strongly Type-3, Strongly Type-3, Moderately Type-3, and Weakly Type-3 [31]. Each
method in a pair has to go through a combination of size-based and semantic-based ﬁlters in order to survive and
reaches machine learning classiﬁer for clone detection. Sized-based ﬁlter mandates that a pair of methods sizes should
be close. Semantic-based ﬁlter based on action tokens extracted from methods calls in the code body and ﬁelds, so
for two methods they ﬁnd the overlap between tokens (cid:104)t, f req(cid:105) to ﬁnd if they are semantically similar, where t is the
action token and f req is the frequency of action token [32]. Then the pair is passed to another a supervised machine
learning based on Siamese architecture to assist further in detecting if the pair is clone. Out of 24 software metrics
they used to ﬁlter out non-probable clones, 5 are proposed by Saini et al. which are type literals as they noticed that
methods with similar functionality have similar literal types. They also reported that action tokens not only good for
semantic comparison but also in detecting Type III clone [2]. Software metrics are resilient to changes in identiﬁers
and literals, which make them very popular not only to detect Type I and Type II clones [33, 34, 35] but also up to
Type III clones. where for a given pair of methods, they should have close software metrics to be considered as clones
based on a predeﬁned threshold λ decided by author. After collecting metrics from data, a vector of size 48 is fed to
Siamese architecture with dropout, relative entropy loss function and stochastic gradient descent optimizer.

Naﬁ et al. [5] proposed the CLCDSA model to detect cross-language code clones using both semantic features and
syntactic features of pairs of code fragments in Java, Python, and C#. After heuristic and manual study, they elicited
9 features out of 24 total features from Saini et al.’s work [1] as based on heuritsitc study, they found these 9 features
are more frequent in cross-language setup. Moreover, these 9 features yield similar metrics between different but
functionally similar programming languages. While their model achieved higher scores compared to Perez et al.’s
work [4], there are many preconditions imposed. For example, in addition to exhaustive feature selection, they included
API documentation as part of ﬁnding clones, which adds an extra burden on the CL model. Also, they demand that
pairs have to go through the API call similarity ﬁlter based on the skip-gram model to measure the possibility of being
clones before clone detection starts. To conclude, this is not an end-to-end approach and extensive feature processing
has to take place ahead.

3 Source Code Embeddings and InferCode

In this section, we give an overview of embedding techniques and InferCode [19] which is used as the main source
code embeddings in this work. There are many source code embedding techniques published recently including
code2seq [36] and code2vec [37] which are denoted as code2*. These approaches enumerate over a set of AST k
paths between terminals. As a result, a pair of terminals is selected each time different from all other pairs. It then
uses an attention mechanism to select the path that will most likely contribute to the prediction. Strictly speaking,
their embeddings are meant only for a speciﬁc task in mind, which can threat the validity of our work if we are to use
them for cross-language clone detection. Subsequent attempts improved code2* by obfuscating code which increased
accuracy for many downstream tasks including method name prediction [38]. code2* take AST paths as input and fed
them to a recurrent neural network (RNN) powered with attention to recognize which subtree contributes the most to
predict the label. code2* encode all AST paths and then determine which path contributes the most to the ﬁnal output
during decoding. At the decoding step, code2* run over each path from AST and decode it to the target sequence one
token at a time using Long Short Term Memory (LSTM) [39]. The decoder also uses an attention mechanism with
decoder hidden state to produce each target token one at a time.

InferCode is an end-to-end self-supervised model that does not need a human to label data, which boosters the model’s
generalizability. The model also learns labels from the code by itself. InferCode uses TBCNN to loop over AST to

3

Cross-Language Source Code Clone Detection Using Deep Learning with InferCode

A PREPRINT

embed the whole subtree into a vector that describes the whole code snippet, so that it becomes easier to compare
the given code to other source code snippets in different languages. Fig. 1 shows looping over an subtree. Each node
in the subtree vi in AST tree T is associated with D-dimensional vector xv ∈ RD as input feature that represents
xv = xtype + xtoken (token and text of node vi) and then hidden representation hv is extracted. Embeddings for types
and texts of nodes are also learnable as W type, W text. Also, 3 additional weight matrices are introduced for each node
W t, W l, W r representing top, left and right.

InferCode tweaked TBCNN to include the textual information of code besides the type information provided by
the original implementation of TBCNN. InferCode also replaces the dynamic max pooling layer with an attention
mechanism to reduce information loss in the aggregation of all information from all nodes into a single vector where
each parent accumulates the information of its children nodes at one level. InferCode utilizes a special ﬁlter that
is designed to loop over subtrees and embed their information to output a feature map that replaces the traditional
convolution ﬁlter used in convolutional neural networks (CNNs). Based on the subtree of source code AST, InferCode
predicts the possibility of a subtree belonging to an AST.

Figure 1: TBCNN loops over a subtree for embedding. Each parent node accumulates the information of its descen-
dants.

4 Model Architecture

In this section, we describe how we used Siamese architecture to take pairs of code snippets embeddings to ﬁnd out if
they are clones or not. Unlike Perez et al. work, we did the opposite by using shared weights between the 2 branches
of the network because we think that existing embeddding for both are based on statement level as we explained how
TBCNN work, so we consider both to be very close to each other and in the same domain.

4.1 Subtrees Comparison

It is a good start to loop over subtrees of AST, but this leads to an exponential running algorithm. For two binary
trees, which is not the case mostly for AST of code, of levels i, j respectively, we have to loop over 2i × 2j in order
to ﬁnd out if the two ASTs for two source codes written in Python and Java are similar. We can add a margin on how
many subtrees has to match before we consider two ASTs to be similar or clones. What is more interesting is if can
aggregate all information obtained from all nodes before the a speciﬁc node in AST that will yield the context around
that node. We found that InferCode built on similar reasoning [19]. This not only helps to detect exact clones but also
near miss clones.

To reduce number of subtrees comparison for cross CL, we can use hashing trick by hashing all possible subtrees of
source codes in Python and Java and then compare those that ﬁts in the same bucket [8]. This can be challenging
if there is a slight variations among subtrees as slight change in subtree will yield different bucket in the hash table,
so mainly depending on exact subtrees matching into our approach is not wise as Python and Java ASTs does not
share much similarity at syntax level compared to code clones written in the same language. Consider the following
clones in Python and Java that are supposed to take substrings of input string and append all substrings together Fig.2.
Both subtrees and tokens names taken would help in previous scenario scenario as if we take token names or subtrees
individually to detect clones would be difﬁcult. If the programs were written in the same language, we would have
both semantic and syntactic match [40, 41], also it can be the case for cross language clones.

4

Cross-Language Source Code Clone Detection Using Deep Learning with InferCode

A PREPRINT

We should also do the same across all other languages in case the same functionality is replicated. Fig. 2 is one simple
example where clone detection can prove helpful, although it can be more intricate in production software as fragments
pieces can have complex functionalities.

Figure 2: Python and Java code that iterates over substrings and add a backslash ‘\’ between all substrings before
appending all substrings back together. This might better ﬁt Type IV clone category.

In Fig.2, although the two programs are functionally the same, yet their API call similarity is quite different. Java API
calls made are IN(), SUBSTRING() and NEXTINT() and for Python API calls are INPUT(), SPLIT(), STRIP() and
INSERT(). Only INPUT and IN are similar both at syntactic level and API documentation level, however the others
are different. So, API similarity action ﬁlter proposed by CLCDSA will drop our example provided above.

4.2 Siamese Architecture

Simple cosine similarity metrics or Euclidean distance metrics can be used to ﬁnd how similar two embeddings are.
However, we decided to use Siamese architecture for this purpose as we are looking to train the network to learn
similarity of embeddings pairs and also dissimilar pairs. Siamese architecture is an architecture built exclusively for
similarity computation. It is composed of two or more equivalent parallel networks that have the same architecture
and trainable parameters [42]. Parallel networks are updated during backpropagation with the same value. In addition,
Siamese architecture needs less data to be trained, which makes it suitable in this work as we deal with more than 40k
pairs of similar and dissimilar embeddings of Java and Python code. Siamese architecture has been widely used since
it was developed for various tasks such as images similarity, text similarity, plagiarism detection, and many others
(e.g., [43, 44, 45]).

We provide our Siamese architecture with pairs of similar embeddings followed by dissimilar embeddings. We anno-
tate the dataset with similarity score in label ∈ {0, 1} or a label that explicitly tells that a pair is similar or not. We also
use the contrastive loss function to calculate the distance of every pair and ﬁnd how far they are based on the preset
threshold [42]. Fig 3 shows two sub models to process the pairs which can be similar and dissimilar. Then, we measure
the similarity between the output of the two networks using the Euclidean distance. During the backpropagation of the
model per the output of the loss function, the layers are updated based on the similarity or dissimilarity of pairs.

Although Euclidean distance is suitable for measuring the distance between two points, in a large dimensional space,
all points tend to be far apart by the Euclidean measure. In higher dimensions, the angle between vectors is a more
effective measure. The cosine distance measures the cosine of the angle between the vectors. The cosine of identical
vectors is 1 while orthogonal and opposite vectors are 0 and -1 respectively.

The contrastive loss function is proven to be effective in unsupervised learning. This loss will learn from neighbour-
hood embeddings of source code by pushing similar pairs together and dissimilar pairs apart, which makes it suitable
in our use case. Contrastive loss looks similar to the softmax function with the addition of the vector similarity and
a normalization factor. The similarity function is just the cosine distance discussed earlier. The other difference is
that values in the denominator are the cosine distance from the positive samples to the negative samples; not very
different from CrossEntropyLoss [46]. The intuition here is that we want our similar vectors to be as close to 1 as

5

Cross-Language Source Code Clone Detection Using Deep Learning with InferCode

A PREPRINT

Figure 3: Siamese model we used along with 2 parallel networks one for each input of a pair whether it is clone or not.

possible, since that is the optimal loss. We want the negative samples to be close to 0, since any non-zero values
will reduce the value of similar vectors. Given embeddings of Python and Java pairs (clones or not clones) deﬁned as
∀pyemb, javaemb ∈ RN , where N is the dimension of the embedding, the task is the ﬁnd the distance between the
output OW (emb) of Siamese network parameterized by weight matrix W :

DW (pyemb, javaemb) =(cid:107) OW (pyemb)
− OW (javaemb) (cid:107)2

(1)

Once that is found, the output is passed to the loss function for one pair:

L(Y, W, pyemb, jvemb) = (1 − Y )

1
2

(DW )2+

where Y ∈ {0, 1}, where 1 indicates the pair is clone and 0 otherwise. To ﬁnd the total loss of all pairs 2K, as we
have K clone pairs and K dissimilar pairs:

(Y )

1
2

{max(0, m − DW )}2

(2)

L(W ) =

2k
(cid:88)

i=1

L(W, Y, pyi

emb, jvi

emb)

(3)

For similar pairs, we have (1 − Y ) 1
when the pairs
2
are dissimilar, where xi is embedding i. To penalize the loss function, we need to have a margin m when our inputs
are dissimilar. If distance goes above this margin m < DW (xi), it will yield a negative number and thus, 0 will be
chosen. In that case, the whole equation will equate to 0 if we exceed the margin and the network is not updated.

+ 0 and 0 + (Y ) 1
2

(cid:16)

max {0, m − DW (xi)}2(cid:17)

(cid:16)

DW (xi)2(cid:17)

6

Cross-Language Source Code Clone Detection Using Deep Learning with InferCode

A PREPRINT

5 Evaluation

In this section, we evaluate two models on the dataset of clones and dissimilar pairs. The ﬁrst model is LSTM with
teacher forcing and Bahdanau Attention over AST tokens. The second is our model over the same dataset. The trained
model used in this work and its manual are available online 1.

5.1 LSTM with Bahdanau Attention

We use LSTM with the Bahdanau Attention formulas [47] to ﬁnd attention. We have formulated the cross language
clone detection task in our early experiments as a sequence to sequence (seq2seq) problem where we take ASTs of
code snippets as input in one programming language and feed it to the LSTM autoencoder. The encoder of LSTM
takes the parsed ASTM tokens one by one at each timestep. The decoder then will decode output one step at a time.
Then we challenged LSTM based seq2seq model to map input in of input AST to output AST. The model achieved
0.5 F1 score, but with poor generalization in Fig. below. While passing hidden states between LSTM unit cells as
time goes increases accuracy as results below show, not passing states between layers does not perform poorly. The
goal is to encode Python script and then decode to get the clone in Java. We will use as well in this experiment teacher
forcing to accelerate training of LSTM Autoencoder. We have trained the LSTM model with 16 unit cells, train data
14280, test size 3569, 500 epochs, 20 patience degree, adaptable learning rate, rmsprop optimizer and mse loss. We
tried different testing scenarios by changing number unit cells and optimization function, but the model stops between
22-27 epochs with negligible increase in F1 score. F1 score for this model 0.53 F1 and 0.31 validation loss.

In teacher forcing, we use the ground truth output in the current time step (available in the training data) to compute
the system state in the next time steps. Teacher Forcing is a method to train encoder-decoder models in Seq2Seq model
to accelerate training. Teacher Forcing can only be used at Training as experiments show we get bad predictions. Even
though Teacher Forcing improves the training process by fast converging, the model has generate low accuracy even
with the training data. Nevertheless, LSTM model with TF gives better F1 score than LSTM with BL despite that
LSTM with BL shows stabilized training as Fig. 4 shows.

Problem with vanilla LSTM Autoencoder model is the use of a ﬁxed-length context vector. Dzmitry et al. conjectured
that this limitation may make the basic encoder–decoder approach to underperform with long sequences and as a result
information will be lost. To verify this conjecture, we conducted experiments on different time steps of LSTM model:
1)When the sequence size or number of time steps is 4, encoder-decoder model terminates at Epoch 31 with %99
accuracy score, and 2)When the number of time steps is 16, encoder-decoder model runs all the 40 epochs and ﬁnishes
with only %36 accuracy score. That is, as argued, Encoder Decoder model underperforms with long sequences. We
use not only the last hidden and cell states but also the decoder’s hidden states generated at all the time steps. Also,
we use all the decoder’s hidden states at all consecutive time steps. We start by initializing the decoder states by using
the last states of the encoder as usual. Then at each decoding time step: 1) We use encoder’s all hidden states and
the previous decoder’s output to calculate a global context vector by applying an Attention Mechanism, and 2) Lastly,
we concatenate the Context Vector with the previous Decoder’s output to create the input to the decoder. We used
in LSTM+BL (LSTM with Bahdanau Additive style) experiment vT
a tan h (W1ht + W2hs) score as deﬁned by BL
Style [47],

score (ht, hs)

=

(cid:26)hT
t W hs
vT
a tan h (W1ht + W2hs)

(4)

where hs represents all the hidden states of the encoder, ht is the previous hidden states of the decoder (previous time
step output), and ﬁnally W is the weight matrix for parameterizing the calculations. To ﬁnd attention (which AST
tokens are most likely to inﬂuence mapping Python code to Java code), we calculate a score to relate the Encoder’s all
hidden states and the previous Decoder’s output. So we will compare encoders all hidden states and previous decoder’s
output to create a score. Then calculate attention weights is very similar a softmax of the values we calculated in
context vector step,

αts =

exp (score (ht, hs))
S
(cid:80)
s(cid:48)=1

exp (score (ht, hs))

(5)

Finally, to calculate the context vector ct below by applying the attention weights onto decoder hidden states hs. Thus,
we will have weighted decoder hidden states at the end. This will weaken features from encoder hidden states that

1https://bitbucket.org/MohammadAbrahiam/crosslanguagecloneyahyaetal/src/master/

7

Cross-Language Source Code Clone Detection Using Deep Learning with InferCode

A PREPRINT

Table 1: LSTM Model Results on Cross Language Clone detection between Java and Python embeddings. LSTM
AE stands for LSTM Autoencoder. LSTM + TF stands for LSTM with teacher forcing. LSTM with BL stands for
Bahdanau and Luong Attention.

Models
Vanilla LSTM AE
LSTM+TF
LSTM+BL

Precesion Recall
0.60
0.62
0.59

0.45
0.53
0.45

F1-Measure
0.53
0.56
0.54

Table 2: Siamese Model Results on Cross Language Clone detection between Java and Python embeddings compared
to other models.

Models
Our Model
CLDSA
Daniel’s Siamese Model

Precision Recall
0.99
0.67
0.66

0.59
0.65
0.83

F1-Measure
0.80
0.66
0.66

attribute less to current decoder output hidden state,

ct =

(cid:88)

s

αtshs

(6)

Figure 4: Training loss (blue) and validation loss (orange) of LSTM+TF on the right and LSTM+BL on the left. The
training loss of LSTM+BL converges to validation loss better than LSTM+TF.

5.2 Siamese Model Network

Siamese network accepts pairs from the same domain. Unlike Perez et al.’s work, they did not share the weights across
base network of Siamese network for the pair as they considered them to be in different domains as one source code
is in Java while the other is in Python. Instead, we did the opposite and we share the weights across the two branches
of base network that accept pair.

Based on experiments, we noticed that Siamese networks perform better in terms of F1 score than other loss functions
when it’s used with contrastive loss. In our network, SGD optimizer with patience level set to 20 for early stoppings
to avoid overﬁtting. There are many version of contrastive loss function, but the one that we used as stated in 4.2 is
quadratic contrastive loss function. For the layers, a set of dense and dropout layers used alternately.

6 Conclusion

While same language clone detection achieves very high accuracy, cross language clone detection is still ongoing to
improve tools accuracy to deal with multiple types of clones across many languages. The difﬁculty in cross language
clone detection task lies in the fact that both have different structure and statements In this work, we demonstrated
how Siamese architecture can be used to learn more abstract embedding from source code embeddings obtained over
AST across different programming languages. It’s good to notice that no labeling is need need for our similar and
dissimilar pairs. We added to the dataset over 20k pairs of dissimilar clones besides 20k pairs of clones, which is very
important for Siamese architecture to learn similarity between pairs. So, total we have 80k rows in our dataset. We
showed that our model improves the state-of-the-art work by over %10. It’s true that our model works best with Type
III clones, but we are looking to further test the model on Type IV clones and increase the accuracy of the model on
Type III clone as well across different languages.

8

Cross-Language Source Code Clone Detection Using Deep Learning with InferCode

A PREPRINT

References

[1] V. Saini, F. Farmahinifarahani, Y. Lu, P. Baldi, and C. V. Lopes, “Oreo: Detection of clones in the twilight
zone,” in Proceedings of the 2018 26th ACM Joint Meeting on European Software Engineering Conference and
Symposium on the Foundations of Software Engineering, 2018, pp. 354–365.

[2] H. Sajnani, V. Saini, J. Svajlenko, C. K. Roy, and C. V. Lopes, “Sourcerercc: Scaling code clone detection to
big-code,” in Proceedings of the 38th International Conference on Software Engineering, 2016, pp. 1157–1168.

[3] T. Kamiya, S. Kusumoto, and K. Inoue, “Ccﬁnder: A multilinguistic token-based code clone detection system
for large scale source code,” IEEE Transactions on Software Engineering, vol. 28, no. 7, pp. 654–670, 2002.

[4] D. Perez and S. Chiba, “Cross-language clone detection by learning over abstract syntax trees,” in 2019
IEEE, 2019, pp. 518–528.

IEEE/ACM 16th International Conference on Mining Software Repositories (MSR).

[5] K. W. Naﬁ, T. S. Kar, B. Roy, C. K. Roy, and K. A. Schneider, “Clcdsa: cross language code clone detection using
syntactical features and api documentation,” in 2019 34th IEEE/ACM International Conference on Automated
Software Engineering (ASE).

IEEE, 2019, pp. 1026–1037.

[6] X. Wu, L. Qin, B. Yu, X. Xie, L. Ma, Y. Xue, Y. Liu, and J. Zhao, “How are deep learning models similar? an
empirical study on clone analysis of deep learning software,” in Proceedings of the 28th International Conference
on Program Comprehension, 2020, pp. 172–183.

[7] B. S. Baker, “A program for identifying duplicated code,” Computing Science and Statistics, pp. 49–49, 1993.

[8] I. D. Baxter, A. Yahin, L. Moura, M. Sant’Anna, and L. Bier, “Clone detection using abstract syntax trees,”
IEEE, 1998, pp.

in Proceedings. International Conference on Software Maintenance (Cat. No. 98CB36272).
368–377.

[9] G. A. Di Lucca, M. Di Penta, and A. R. Fasolino, “An approach to identify duplicated web pages,” in Proceedings

26th Annual International Computer Software and Applications.

IEEE, 2002, pp. 481–486.

[10] A. Sæbjørnsen, J. Willcock, T. Panas, D. Quinlan, and Z. Su, “Detecting code clones in binary executables,” in
Proceedings of the eighteenth international symposium on Software testing and analysis, 2009, pp. 117–128.

[11] J. Krinke, “Identifying similar code with program dependence graphs,” in Proceedings Eighth Working Confer-

ence on Reverse Engineering.

IEEE, 2001, pp. 301–309.

[12] C. K. Roy and J. R. Cordy, “Benchmarks for software clone detection: A ten-year retrospective,” in 2018 IEEE
IEEE, 2018, pp.

25th International Conference on Software Analysis, Evolution and Reengineering (SANER).
26–37.

[13] M. Lei, H. Li, J. Li, N. Aundhkar, and D.-K. Kim, “Deep learning application on code clone detection: A review

of current knowledge,” Journal of Systems and Software, vol. 184, p. 111141, 2022.

[14] S. Bellon, R. Koschke, G. Antoniol, J. Krinke, and E. Merlo, “Comparison and evaluation of clone detection

tools,” IEEE Transactions on software engineering, vol. 33, no. 9, pp. 577–591, 2007.

[15] C. K. Roy and J. R. Cordy, “A survey on software clone detection research,” Queen’s School of Computing TR,

vol. 541, no. 115, pp. 64–68, 2007.

[16] J. Svajlenko, J. F. Islam, I. Keivanloo, C. K. Roy, and M. M. Mia, “Towards a big data curated benchmark
of inter-project code clones,” in 2014 IEEE International Conference on Software Maintenance and Evolution.
IEEE, 2014, pp. 476–480.

[17] C. K. Roy and J. R. Cordy, “Near-miss function clones in open source software: an empirical study,” Journal of

Software Maintenance and Evolution: Research and Practice, vol. 22, no. 3, pp. 165–189, 2010.

[18] C. K. Roy, M. F. Zibran, and R. Koschke, “The vision of software clone management: Past, present, and future
(keynote paper),” in 2014 Software Evolution Week-IEEE Conference on Software Maintenance, Reengineering,
and Reverse Engineering (CSMR-WCRE).
IEEE, 2014, pp. 18–33.

[19] N. D. Bui, Y. Yu, and L. Jiang, “Infercode: Self-supervised learning of code representations by predicting sub-
IEEE, 2021, pp.

trees,” in 2021 IEEE/ACM 43rd International Conference on Software Engineering (ICSE).
1186–1197.

[20] L. Mou, G. Li, L. Zhang, T. Wang, and Z. Jin, “Convolutional neural networks over tree structures for program-

ming language processing,” in Thirtieth AAAI Conference on Artiﬁcial Intelligence, 2016.

[21] L. Mou, H. Peng, G. Li, Y. Xu, L. Zhang, and Z. Jin, “Discriminative neural sentence modeling by tree-based

convolution,” arXiv preprint arXiv:1504.01106, 2015.

9

Cross-Language Source Code Clone Detection Using Deep Learning with InferCode

A PREPRINT

[22] D. Rattan, R. Bhatia, and M. Singh, “Software clone detection: A systematic review,” Information and Software

Technology, vol. 55, no. 7, pp. 1165–1199, 2013.

[23] M. I. Azeem, F. Palomba, L. Shi, and Q. Wang, “Machine learning techniques for code smell detection: A
systematic literature review and meta-analysis,” Information and Software Technology, vol. 108, pp. 115–138,
2019.

[24] E. V. de Paulo Sobrinho, A. De Lucia, and M. de Almeida Maia, “A systematic literature review on bad smells–5
w’s: which, when, what, who, where,” IEEE Transactions on Software Engineering, vol. 47, no. 1, pp. 17–66,
2018.

[25] W. Wang, G. Li, B. Ma, X. Xia, and Z. Jin, “Detecting code clones with graph neural network and ﬂow-
augmented abstract syntax tree,” in 2020 IEEE 27th International Conference on Software Analysis, Evolution
and Reengineering (SANER).

IEEE, 2020, pp. 261–271.

[26] M. Said Elsayed, N.-A. Le-Khac, S. Dev, and A. D. Jurcut, “Network anomaly detection using lstm based au-
toencoder,” in Proceedings of the 16th ACM Symposium on QoS and Security for Wireless and Mobile Networks,
2020, pp. 37–45.

[27] I. Sutskever, O. Vinyals, and Q. V. Le, “Sequence to sequence learning with neural networks,” Advances in neural

information processing systems, vol. 27, 2014.

[28] E. Raff, J. Barker, J. Sylvester, R. Brandon, B. Catanzaro, and C. K. Nicholas, “Malware detection by eating a

whole exe,” in Workshops at the Thirty-Second AAAI Conference on Artiﬁcial Intelligence, 2018.

[29] G. Soyalp, A. Alar, K. Ozkanli, and B. Yildiz, “Improving text classiﬁcation with transformer,” in 2021 6th

International Conference on Computer Science and Engineering (UBMK).

IEEE, 2021, pp. 707–712.

[30] M. A. Nishi and K. Damevski, “Scalable code clone detection and search based on adaptive preﬁx ﬁltering,”

Journal of Systems and Software, vol. 137, pp. 130–142, 2018.

[31] J. Svajlenko and C. K. Roy, “Bigcloneeval: A clone detection tool evaluation framework with bigclonebench,”
IEEE, 2016, pp.

in 2016 IEEE international conference on software maintenance and evolution (ICSME).
596–600.

[32] A. Gofﬁ, A. Gorla, A. Mattavelli, M. Pezz`e, and P. Tonella, “Search-based synthesis of equivalent method se-
quences,” in Proceedings of the 22nd ACM SIGSOFT International Symposium on Foundations of Software
Engineering, 2014, pp. 366–376.

[33] K. Kontogiannis, “Evaluation experiments on the detection of programming patterns using software metrics,” in

Proceedings of the Fourth Working Conference on Reverse Engineering.

IEEE, 1997, pp. 44–54.

[34] J. Mayrand, C. Leblanc, and E. Merlo, “Experiment on the automatic detection of function clones in a software

system using metrics.” in icsm, vol. 96, 1996, p. 244.

[35] J.-F. Patenaude, E. Merlo, M. Dagenais, and B. Lagu¨e, “Extending software quality assessment techniques to
IEEE, 1999, pp.

java systems,” in Proceedings Seventh International Workshop on Program Comprehension.
49–56.

[36] U. Alon, S. Brody, O. Levy, and E. Yahav, “code2seq: Generating sequences from structured representations of

code,” arXiv preprint arXiv:1808.01400, 2018.

[37] U. Alon, M. Zilberstein, O. Levy, and E. Yahav, “code2vec: Learning distributed representations of code,”

Proceedings of the ACM on Programming Languages, vol. 3, no. POPL, pp. 1–29, 2019.

[38] R. Compton, E. Frank, P. Patros, and A. Koay, “Embedding java classes with code2vec: Improvements from
variable obfuscation,” in Proceedings of the 17th International Conference on Mining Software Repositories,
2020, pp. 243–253.

[39] S. Hochreiter and J. Schmidhuber, “Long short-term memory,” Neural computation, vol. 9, no. 8, pp. 1735–1780,

1997.

[40] P. Bulychev and M. Minea, “Duplicate code detection using anti-uniﬁcation,” in Proceedings of

the

Spring/Summer Young Researchers’ Colloquium on Software Engineering, no. 2, 2008.

[41] L. Jiang, G. Misherghi, Z. Su, and S. Glondu, “Deckard: Scalable and accurate tree-based detection of code

clones,” in 29th International Conference on Software Engineering (ICSE’07).

IEEE, 2007, pp. 96–105.

[42] J. Bromley, I. Guyon, Y. LeCun, E. S¨ackinger, and R. Shah, “Signature veriﬁcation using a” siamese” time delay

neural network,” Advances in neural information processing systems, vol. 6, 1993.

10

Cross-Language Source Code Clone Detection Using Deep Learning with InferCode

A PREPRINT

[43] S. Chopra, R. Hadsell, and Y. LeCun, “Learning a similarity metric discriminatively, with application to face ver-
iﬁcation,” in 2005 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR’05),
vol. 1.

IEEE, 2005, pp. 539–546.

[44] A. Y. Ichida, F. Meneguzzi, and D. D. Ruiz, “Measuring semantic similarity between sentences using a siamese

neural network,” in 2018 International Joint Conference on Neural Networks (IJCNN).

IEEE, 2018, pp. 1–7.

[45] T. Ranasinghe, C. Orˇasan, and R. Mitkov, “Semantic textual similarity with siamese neural networks,” in Pro-
ceedings of the International Conference on Recent Advances in Natural Language Processing (RANLP 2019),
2019, pp. 1004–1011.

[46] Z. Zhang and M. Sabuncu, “Generalized cross entropy loss for training deep neural networks with noisy labels,”

Advances in neural information processing systems, vol. 31, 2018.

[47] J. K. Chorowski, D. Bahdanau, D. Serdyuk, K. Cho, and Y. Bengio, “Attention-based models for speech recog-

nition,” Advances in neural information processing systems, vol. 28, 2015.

11

