1
2
0
2

p
e
S
6
1

]
E
S
.
s
c
[

2
v
3
7
1
7
0
.
9
0
1
2
:
v
i
X
r
a

A Comparison of Code Embeddings and Beyond

SIQI HAN, DONGXIA WANG, WANTING LI, and XUESONG LU, School of Data Science and
Engineering, East China Normal University, China

Program representation learning is a fundamental task in software engineering applications. With the availability of “big
code” and the development of deep learning techniques, various program representation learning models have been proposed
to understand the semantic properties of programs and applied on different software engineering tasks. However, no previous
study has comprehensively assessed the generalizability of these deep models on different tasks, so that the pros and cons of
the models are unclear. In this experience paper, we try to bridge this gap by systemically evaluating the performance of eight
program representation learning models on three common tasks, where six models are based on abstract syntax trees and two
models are based on plain text of source code. We kindly explain the criteria for selecting the models and tasks, as well as
the method for enabling end-to-end learning in each task. The results of performance evaluation show that they perform
diversely in each task and the performance of the AST-based models is generally unstable over different tasks. In order to
further explain the results, we apply a prediction attribution technique to find what elements are captured by the models and
responsible for the predictions in each task. Based on the findings, we discuss some general principles for better capturing the
information in the source code, and hope to inspire researchers to improve program representation learning methods for
software engineering tasks.

Additional Key Words and Phrases: Program Representation Learning, Deep learning, Software Engineer Tasks, Attribution
Networks

INTRODUCTION

1
The task of program representation learning is to learn continuous vectors for representing code snippets, such
that semantically-similar snippets are mapped to close vectors in the continuous space. The learnt representations,
or the program embeddings, can be further used for downstream tasks in programming language processing
and software engineering [10, 13, 24, 26, 58, 63, 67]. In the past few years, the availability of massive source code
from public repositories has brought a boost to the use of deep learning techniques for program representation
learning [1, 5, 12, 40, 47, 73, 74, 76]. A large fraction of such studies leverage the abstract syntax trees (ASTs) of
programs and use deep learning models to learn programs’ representations from the ASTs [5, 6, 40, 47, 73, 74, 76].
The assumption is that compared to the plain text of source code, an AST reflects better the structural information
of a program so that the learnt representation can capture more precise semantic property of the program. The
AST-based deep models have shown the state-of-the-art performance on tasks such as code classification and
code clone detection [65, 76].

Despite the great success, we observe that there is a lack of comprehensive comparison of these models and
in particular the evaluation of the generalizability of the program embeddings. While each model shows the
advantage on one or two selected tasks in the original paper, it is unclear how they generalize to other tasks.
Moreover, no previous work has investigated what information these embeddings capture from the source code
W.R.T a particular task. Therefore it is hard to make suggestions on the use and improvement of these embedding
models on subsequent tasks. To bridge this gap, first, we systemically evaluate the performance of six AST-based
program embedding models on three common tasks in programming language processing (see Section 4). We
kindly select the evaluated models based on four criteria to ensure that they represent the state of the art in the
field (see Section 2.2), and select the representative evaluation tasks that require the models to use as input the
embedding of an entire program (see Section 3). Second, we attempt to explain the performance generated by
these models using a prediction attribution technique (see Section 5). With the attribution technique, we may
visualize what elements in the original source code are responsible for the predictions, which helps to reveal why
the models perform diversely on the tasks. As a baseline, we additionally train two classic neural networks in

 
 
 
 
 
 
2

• Siqi Han, DongXia Wang, Wanting Li, and Xuesong Lu

natural language processing to learn directly from the plain text of source code and evaluate the corresponding
performance as well.

A related but different study is conducted by Kang et al. [31], which assesses the generalizability of code2vec
token embeddings [6]. They use code2vec as an algorithm (like word2vec in NLP) to embed the tokens in the
source code and use the token embeddings as input to the models for downstream tasks. In other words, they
assess whether code2vec produces robust word embeddings for downstream tasks. In contrast, we want to
evaluate the generalizability of the embeddings that represent an entire program (like doc2vec in NLP). The
reason is twofold. First, many software engineering models require the embedding of an entire program as
input [2, 15, 65, 76]. As such it is worth investigating which program embedding models are suitable for what
kind of tasks. Second and more importantly, program embedding models focus on how to handle the structural
information in the program rather than training the embeddings of individual tokens. Therefore the resulted
token embeddings may be sensitive to the structural context they are trained in. Yet all program embedding
models attempt to capture the semantic property of source code in general, therefore it is fairer to directly
compare the generalizability of program embeddings.

To summarize, we mainly ask two questions: how do the representative AST-based program embedding models
perform on different tasks and why do they perform well or not on each task? We attempt to answer the questions
by the following contribution:

• We select six representative AST-based program embedding models and evaluate their performance on three
common software engineering tasks. We also implement two classic NLP models as baselines. We explain
the performance discrepancy of the models by analyzing how they extract the structural information from
the ASTs. To the best of our knowledge, this is the first effort in literature to systemically evaluate these
program embedding models.

• We propose to use the prediction attribution technique to explain the model performance. The technique
finds what features in the input data are responsible for the prediction by comparing with the prediction of
the baseline input. We design a proper baseline input for each task so that the prediction can be attributed
to the input tokens in the original programs. We visualize the attribution results and discuss what features
in the source code are important for each task.

• We provide a discussion on the key lessons learned from the experimental study and draw some general
principles for better capturing the information in the source code. The insights may motivate further research
on program representation learning models that can either generalize better to various downstream tasks
or achieve the new state-of-the-art performance on specific tasks.

The rest of the paper is organized as follows. In Section 2, we describe the evaluated program embedding models
and explain our implementations to the models for fair comparison. In Section 3, we describe the three selected
tasks that are suitable to evaluate the generalizability of program embeddings and the network architecture
for each task. We report and analyze the performance of the embedding models in Section 4, followed by the
prediction attribution analysis in Section 5. We discuss the lesson learned from the study in Section 6 and threats
to validity in Section 7. Finally, we conclude in Section 8.

2 THE MODELS FOR PROGRAM EMBEDDING
In this section, we briefly review the models evaluated in the experiments. We mainly investigate the representative
models that learn program representations from abstract syntax trees. We refer to this type of models as the
AST-based models. In comparison, we also apply two classic models in natural language processing, namely,
LSTM and Transformer (only the encoder), to learn from the source code text. We refer to the two models as the
token-based models. We describe the modifications in our implementation for a fair comparison of the models.

A Comparison of Code Embeddings and Beyond

•

3

2.1 The Token-based Models
2.1.1 LSTM. LSTM can capture the token dependencies in a sequence and is suitable for encoding the source
code text. At each step, LSTM computes a hidden state that represents the information in the source code until
the current input token. We follow previous work [18, 36] and simply use the last hidden state to represent the
entire program.

2.1.2 Transformer. The Transformer model encodes an input sequence using self-attentions, where each input
token is encoded by attending to all the tokens in the same sequence. The Transformer encoder is typically
constructed using several self-attention layers, and the encoding at the first position of the last layer is used to
represent the entire program.

2.2 The AST-based Models
The AST-based models evaluated in this work are selected based on four criteria. First, the model should learn a
single embedding which represents the entire program. Second, for fair comparison, it should only use the static
structural information of an AST and not incorporate any dynamic information such as concrete execution traces.
Third, the underlying work is published on a top-ranked conference or journal, so that the model represents the
state of the art in the field. Finally, the authors should have made the stable source code publicly available so
that we may refer to their implementation details and re-implement the models correctly using a unified deep
learning framework (PyTorch).

Based on the criteria, we finally identify six representative deep learning models with high citations. According
to the methods of manipulating the structure of an AST, we roughly divide them into three categories. The first
category recursively aggregates the structural information from the leaves to the root in a bottom-up approach.
The representative models are TBCNN [45] and AutoenCODE [68], where the former proposes tree-based
convolutional neural networks and the latter adopts recursive neural networks, respectively, for the bottom-up
aggregation. The second category of models aggregate the structural information from the selected paths in the
ASTs. The representative models are code2vec [6] and code2seq [4]. Both of them embed the AST paths and
aggregate the path embeddings using the attention mechanism. The last category considers the control/data flow
in the original source code while extracting the structural information in the ASTs. The representative models
are GGNN [3] and ASTNN [76]. GGNN constructs a graph by adding semantic edges into the AST and then
adopts gated graph neural networks to learn from the graph, whereas ASTNN splits an AST into a sequence of
statement subtrees and adopts a bidirectional GRU network to learn from the sequence.

2.2.1 TBCNN. TBCNN [45] applies tree-based convolution kernels on an AST to gather the information in
each subtree. The subtree features are aggregated using dynamic pooling to obtain the program embedding. In
particular, TBCNN pretrains the embeddings of AST nodes and encodes each node as a linear combination of
its own embedding and the aggregated embedding of its children. Then it uses tree-based convolution kernels
to slide over the entire tree, and obtains a “feature tree”. To cope with arbitrary sizes of feature trees, TBCNN
applies dynamic pooling to aggregate all the features into one vector representing the entire program.

In their original implementation1, the leaf nodes in the AST, which correspond to the tokens in the source code,
are discarded before the convolutional layer. However, we found that incorporating the features of the leaf nodes
improved the performance on the downstream tasks. Therefore, we keep the leaf nodes in our implementation.

2.2.2 AutoenCODE. AutoenCODE [68] transforms an AST into a binary tree and trains an autoencoder to learn
the program embedding from it. In particular, AutoenCODE recursively combines the embeddings of every pair
of children in a bottom-up approach, and then reconstructs the embeddings of the two children. The training loss

1https://sites.google.com/site/treebasedcnn/home

4

• Siqi Han, DongXia Wang, Wanting Li, and Xuesong Lu

is computed as a weighted sum of all the reconstruction losses over the entire binary tree. The trained embedding
of the root represents the entire program.

In practice, the authors [68] found rather than combining two sibling nodes in each step, the greedy algorithm
that minimizes each reconstruction loss yields better performance. Therefore in our implementation, we also
adopt the greedy variant.

2.2.3 Code2vec. Code2vec [6] learns to represent a program with its AST paths. An AST path connects two leaf
nodes in the AST. For each AST path, code2vec concatenates the representations of the two leaf nodes and the
representation of the internal path, and forms a context vector. A fixed number of sampled context vectors are
transformed and aggregated using attention to form the program embedding.

Code2vec maintains two vocabularies for the leaf nodes and the paths, respectively. The internal nodes on

paths are inseparable in the path representation.

2.2.4 Code2seq. Code2seq [4] follows the general idea of code2vec, and extracts more fine-grained information
from the AST paths. To incorporate the information of internal nodes, code2seq feeds the corresponding node
embeddings sequentially into an LSTM network and obtains the encoding for the entire path.

In the original paper, the program embedding is the average of the vectors of the selected AST paths. However,
we notice that using attentions as code2vec to aggregate the AST paths produces better results. Therefore we
adopt the attention mechanism in our implementation.

2.2.5 GGNN. GGNN [3] adds semantic edges into an AST and learns from the resulted graph. The edges represent
the flow of control and data in the program. For instance, a NextToken edge connects a leaf node to its successor
in the source code. Once the program graph is obtained, GGNN uses the gated graph neural networks to learn
the node representations. Following [65], we use a global attention layer to aggregate all node representations
and obtain the program embedding.

2.2.6 ASTNN. ASTNN [76] splits an AST into a set of statement subtrees corresponding to the statements in the
source code. The subtrees are organized into a sequence in accordance with the order of the statements in the
source code. Therefore the flow of control in the program is preserved. The sequence of the encoded subtrees is
fed into a bidirectional GRU network. Finally, ASTNN uses max pooling to aggregate all hidden states and forms
the program embedding.

2.3 The Software Used
For fair comparison, we re-implement all the models using PyTorch. For the token-based models, we split each
code snippet into a sequence of subtokens according to camel rules2. For the AST-based models, following
previous work [45, 76], we use pycparser3 and javalang4 to convert C and Java programs into ASTs, respectively.
For code2vec and code2seq, we use astminer5 [34] to directly obtain the AST paths, which is recommended in its
source code repository. To construct the graphs used by GGNN, we follow the work [3] and use graph-ast6 to add
semantic edges between AST nodes.

2.4 The Model Detail
The efficiency of a model is often an useful aspect when assessing it. For the selected eight models, we calculated
the model parameters based on our re-implementation with the PyTorch framework. Combined with the default

2https://en.wikipedia.org/wiki/camelcase.
3https://github.com/eliben/pycparser
4https://github.com/c2nes/javalang
5https://github.com/JetBrains-Research/astminer
6https://github.com/bdqnghi/graph-ast

settings of each model, We set the embedding size to 128, and the vocab size is assumed to be fixed at 1000. The
result is shown in Tab. 1.

A Comparison of Code Embeddings and Beyond

•

5

Table 1. The Parameter Size of Models.

Group
Token-based

Model
LSTM
Transformer

Parameters(k)
656
1924

AST-based

TBCNN
AutoenCODE

code2vec
code2seq

GGNN
ASTNN

231
66

404
938

294
329

Among the eight models, LSTM and Transformer have larger number of parameters owing to it not only
considers each token but the model structure is also complicated. Both TBCNN and AutoenCODE recursively learn
the program representation on ASTs. TBCNN has a small number of parameters due to the shared parameters in
convolution on different kernels, and AutoenCODE is a recursive auto-encoder which only need two encoding-
decoding matrices so that it contains the smallest parameters. Except for these two models, other AST-based
models transform the information on the AST via focusing on a path or a statement, so there will produce more
parameters. Between code2vec and code2seq, the number of code2seq’s parameters is larger due to the richer
input information and the more complicated model structure. However, in the real situation, code2seq is lighter
than code2vec because it splits the token of leaf nodes so that the vocab size is reduced. For example, in the code
clone detection task, the number of tokens included in code2seq is reduced from 43343 to 20659, and the number
of paths is reduced from 28763 to 178 in contrast to code2vec. GGNN abstracts AST as a hypergraph, which
combines some nodes and adds edges to the AST. Therefore, the number of parameters of this model with graph
neural network is relatively small. For ASTNN, it extracts the statement sequences to simplify the representation
of the AST.

Above all, the token-based models have more parameters because they pay attention to every token in the
programs, containing redundant information. The parameters of the AST-based models are less because the
structural representation can refine input features and filter redundant information.

3 THE SELECTED TASKS
In this section, we describe the selected tasks for evaluation. For each task, we also describe the subsequent
component to connect the embedding model and the output. For the pretrained model AutoenCODE, we continue
to fine-tune the parameters during the subsequent training.

Since the purpose of the current work is to evaluate the generalizability of program embedding, we decide to
select the tasks that directly utilize the embedding of an entire program. We first select the code classification
task, which is a very common task in literature [21, 45, 76]. It aims at classifying programs by their functionality.
We also refer to the CodeXGlue [41] benchmark tasks for programming language, and select two other tasks,
namely, code clone detection and code search. The former task detects whether two programs are similar with
respect to a clone type [49], and the latter retrieves the most relevant code given a natural language query.

6

• Siqi Han, DongXia Wang, Wanting Li, and Xuesong Lu

3.1 Code Classification
Classifying programs by functionalities is important for program understanding and maintenance [11, 16, 32, 37].
For example, in a large software repository, automatically tagging programs with their functionality benefits the
reuse and maintenance of the source code during the development process. Among the models under evaluation,
TBCNN, code2vec and ASTNN also apply their program embeddings on code classification.

Fig. 1. The model structure for code classification.

Fig. 1 displays the framework of the models for code classification, which follows the settings in [76]. On the
left-hand side, the program encoder can be implemented using any of the aforementioned embedding models.
The resulted program embedding is directly input to a multi-classifier, which predicts the class of the program.
We implement the classifier using a fully-connected layer with the Softmax activation. We use the cross-entropy
loss function and Adamax for optimization.

3.2 Code Clone Detection
Code clone detection is also widely studied in software engineering research [8, 30, 65, 67, 76]. During software
development, programmers usually copy and paste common code. It may cause inefficiency in developing and
affect the stability of systems if duplicated code snippets are not properly used. In programming education,
detecting similar code can help teachers group student solutions or find code plagiarism during programming tests.
Among the models, AutoenCODE and ASTNN also apply their program embeddings on code clone detection.

Fig. 2. The model structure for code clone detection.

Fig. 2 displays the framework of the models for code clone detection, following the settings in [76]. On the
left-hand side, two programs are separately encoded using the embedding models. Then the two embeddings are
input into a classifier, which predicts whether the two programs are clones. We implement the clone classifier by
subtracting the two embeddings and transforming the resulted embedding using a fully-connected layer. The
output is activated using the Sigmoid function. During training, we use the cross-entropy loss function and
Adamax to optimize the parameters.

ProgramProgram EncoderMulti-ClassiferProgram Embedding labelProgram 2Program Embedding 2Program 1Program EncoderClone ClassiferProgram Embedding 1ProbabilityA Comparison of Code Embeddings and Beyond

•

7

3.3 Code Search
Programmers often search code fragments with natural language queries. The code search task automatically
returns the most relevant code snippets from a prepared code collection according to a query [13, 22, 39, 54, 55, 70].
None of the aforementioned models has been evaluated on this task in the original paper.

Fig. 3. The model structure for code search.

Fig. 3 displays the framework of the models for code search, which follows the approach in [22, 55]. During
training, we first select a pair of query and code snippet < 𝑄 +, 𝐶+ >, where 𝑄 + denotes a query and 𝐶+ denotes a
relevant code. We also select a random query 𝑄 − as the negative sample. The queries and the code are encoded
using the query encoder and the program encoder, respectively. With the query and program embeddings v+
𝑄 ,
𝐶 and
𝐶 , the training objective is to simultaneously maximize the cosine similarity between v+
𝑄 and v+
v−
minimize the cosine similarity between v−
𝐶 . We use Adam for optimization. To boost the performance, we
split the composed identifiers and words into subwords. For instance, "fastPathOrderedEmit" is split into "fast
path ordered emit" and then encoded separately. We build a shared vocabulary for the tokens in the source code
and the ASTs.

𝑄 and v+

𝑄 and v+

4 PERFORMANCE EVALUATION
For each task, we describe the dataset used in the experiments and the evaluation metric, and report the
performance.

4.1 The statistics of the used dataset
Table 2 is the specific statistics of the dataset we employed under different input structures on three tasks. Since
AST is an abstract structure of the program, it extracts effective syntactical structures so that it can shorten the
input size from redundant token sequences.

Query-Query EncoderProgramProgram EmbeddingQuery+Query+ Embedding SimilarityProgram EncoderQuery EncoderSimilarityQuery- Embedding 8

• Siqi Han, DongXia Wang, Wanting Li, and Xuesong Lu

Table 2. The Statistics of Three Dataset.

POJ-104 BigCloneBench CodeSearchNet

Max code tokens 24395
Token Avg. code tokens 569.25
Max doc tokens
Avg. doc tokens

-
-

76
Max AST depth
13.32
AST Avg. AST depth
7027
Max AST nodes
Avg. AST nodes 189.57

65091
1115.68
-
-

62
10.57
9602
241.46

346763
449.25
813
47.36

470
8.81
79940
103.77

Table 3. The Results of Code Classification.

Groups

Model

Accuracy

Token-based

AST-based

LSTM
Transformer

TBCNN
AutoenCODE

code2vec
code2seq

GGNN
ASTNN

94.31%
90.74%

94.00%
80.83%

91.32%
84.85%

96.06%
98.09%

4.2 Code Classification
Dataset: We use POJ-1047 for the task, which is a widely used public dataset in literature [9, 38, 45, 75, 76].
The dataset is collected from an online judge system and contains 104 programming problems. Each problem
contains 500 C programs written by the students, which are considered to belong to the same class. The total
52, 000 programs are randomly divided into training, validation and testing set with proportion 3:1:1. The detailed
dataset information is displayed in the second column in Fig. 2 with statistics of tokens, ASTs, and graphs. Since
AST is an abstract structure of the program, it extracts effective syntactical structures so that it can shorten the
input size from redundant token sequences.

Metric: We select the class with the largest score as the predicted class. The evaluation metric is the multi-class

accuracy, which is calculated as the proportion of correctly classified programs to all programs.

Results: The results on the testing set are shown in Table 3. We observe that all models except AutoenCODE
and code2seq have the test accuracy above 90%, indicating that current program embedding models can effectively
predict the functionality of a program. Among them, ASTNN achieves the highest accuracy 98.09%.

According to the taxonomy in Section 2, first we observe that GGNN and ASTNN have the best performance.
This may be because the two models capture the flow of control and data in a program in addition to the structural
information in the corresponding AST. Second, the two token-based models perform overall better than the

7https://sites.google.com/site/treebasedcnn/

A Comparison of Code Embeddings and Beyond

•

9

two categories of AST-based models that solely extract the structural information from a program’s AST (i.e.,
TBCNN, AutoenCODE, code2vec and code2seq). Given that the token-based models learn from the plain text of
source code, the results may indicate that for functionality prediction the information of control and data flow
in a program is more important than the syntactic structure information in the AST. Third, for the two models
that aggregate the node information in an AST using a bottom-up approach, TBCNN performs much better than
AutoenCODE. Apart from using the different ways for bottom-up aggregation, TBCNN preserves the original
structure of the AST, whereas AutoenCODE ignores the original structure and aggregates the nodes using a
greedy algorithm. The results indicate that the greedy algorithm may destroy the AST’s structure, leading to the
failure of capturing the semantic property of a code. Finally, the two models leveraging AST paths perform worse
than TBCNN and better than AutoenCODE. This may justify the previous conjecture because using AST paths
partially ignores the original tree structure.

In short, when predicting the functionality of a program, both the information of control and data flow and
the structural information are important for an accurate prediction. Between the two types of information, the
former might be a stronger signal to reflect the program’s functionality. Nevertheless, learning jointly the two
types of information can achieve the best prediction performance.

4.3 Code Clone Detection
Dataset: We use the BigCloneBench8 dataset for this task, which is also used in [50, 65, 67]. The dataset is
collected from a Java repository [57]. Each program in BigCloneBench is a single method. According to the
functionality of a method, BigCloneBench constructs in total 6, 000, 000 true clone pairs and 260, 000 false clone
pairs. Following [65, 67], we select 9,134 programs to generate the pairs and randomly divide the dataset into
training, validation and testing set with proportion 3:1:1.The detailed dataset information is displayed in the third
column in Fig. 2 with statistics of tokens, ASTs, and graphs.

In [49], four types of code clones are defined based on the degree of similarity between two programs. Type-1:
The two programs are identical except for variations in comments and layout. Type-2: The two programs are
identical except for variations in identifier names and literal values in addition to Type-1 differences. Type-3: The
two programs are syntactically similar and have statements added, modified, or removed with respect to each
other, in addition to Type-2 differences. Type-4: The two programs are syntactically dissimilar but implement the
similar functionality. In BigCloneBench, more than 98% clone pairs belong to Type-4. Therefore in this task we
mainly evaluate whether the program embeddings can capture functional similarity between programs.

We select another common C-language dataset on code clone detection task for test better generalizability.
OJClone is a dataset for detecting clones extracted from the POJ-104 [45]. Code files submitted to the same
programming problem can be regarded as similar code snippets. Consistent with the previous work [76], we
choose the first 15 problems out of 104 program problems to construct the clone dataset. Fifty thousand clone
pairs are randomly selected into OJClone for time-saving reasons. These code pairs are mainly among the clone
types of Type-3 or Type-4.

Metric: At testing time, the clone classifier predicts the probability that the two programs are clones. We set
probability 0.5 as the threshold to determine whether they are clones. The evaluation metric is thereby precision,
recall and F1-score. We do not distinguish among the clone types and report the overall results.

Results: The results are shown in Table 4. We observe that all the models except AutoenCODE have similar

performance. GGNN, TBCNN and Transformer perform better than other models.

The interesting observation is that the two token-based models perform very comparative to the AST-based
models, and Transformer performs overall the best. The rationale is that in practice most clone programs are
generated when programmers copy and reuse code snippets, therefore even the Type-4 clone programs are likely

8https://jeffsvajlenko.weebly.com/bigcloneeval.html

10

• Siqi Han, DongXia Wang, Wanting Li, and Xuesong Lu

Table 4. The Results of Code Clone Detection.

Groups

Models

BigCloneBench

Precision Recall

F1

OJClone
Precision Recall

Token-based

AST-based

LSTM
Transformer

TBCNN
AutoenCODE

Code2Vec
Code2Seq

GGNN
ASTNN

0.882
0.914

0.891
0.154

0.833
0.858

0.935
0.947

0.86
0.968
0.950
0.759

0.911
0.938

0.879
0.930

0.872
0.940
0.920
0.256

0.870
0.896

0.906
0.939

0.892
0.945

0.665
0.382

0.721
0.852

0.801
0.974

F1

0.857
0.410

0.496
0.274

0.668
0.249

0.825
0.262

0.395
0.241

0.623
0.146

0.981
0.900

0.880
0.935

to have very similar text and statements, which are easily captured by the token-based models. To verify this,
we obtain the mean of word2vec [43, 44] embeddings of each program and use it to calculate the average cosine
similarity between clone programs [29, 33]. The result is as high as 0.866. We therefore argue that on the currently
used benchmark dataset for code clone detection, token-based models are sufficient for detecting most clones.
However, this does not mean the AST-based models are not necessary for code clone detection. Indeed, TBCNN,
GGNN and ASTNN can reach very high values of precision or recall. As such, one may choose different models
to cater for different requirements for clone detection in real applications. Also, we expect the contribution from
the relevant area to the construction of the dataset that contains more functionally similar but textually different
clone pairs, which may better examine the advantage of learning program embeddings with the ASTs.

The performance of code2vec and code2seq is slightly lower than other models, indicating that the AST paths
convey less useful feature for clone detection. Finally, AutoenCODE performs bad in the current experimental
setting. Although the model is proposed for the clone detection task, the clones are detected via clustering using
coarse-grained distance metrics in the original paper [68]. Therefore it is worth thinking what is the proper way
to determine clone pairs based on the program embeddings. We leave this for future work.

We then discuss the performance of eight program representation models on OJClone shown in the right
part of Table 4, which can be concluded into two situations. The first case is that the models have comparable
precision and recall. ASTNN and GGNN achieve the best, with ASTNN reaching 0.974 in precision and GGNN
having a peak recall of 0.981. In general, these two models take advantage of control/data flow information
and extract more sophisticated structural features. LSTM performs moderately on detecting clones, indicating
that functionally-similar programs are very likely to have very similar statements, which are easily captured by
the token-based models. What’s more, code2vec and AutoenCODE aren’t as effective as the previous models
for insufficient syntactical information. Code2vec embeds each path as a whole separately, thus hiding a lot of
structural information in the path. AutoenCODE reconstructs the tree in a recursive way so that only leaf nodes
in ASTs are utilized. The second case is that there is a high precision from the models’ results, but the recall is
pretty low. The high precision indicates that most of the code pairs that are predicted to be positive are true
positive samples. However, a large proportion of positive samples are incorrectly predicted as non-cloned pairs,
resulting in a low recall. We inspect that Transformer, TBCNN and code2seq are more complex than other models
from the model construction. In addition, OJClone is unbalanced that positive samples in OJClone only account
for 6%, so that the difficulty of training rises, and the requirements for the model to detect clones in OJClone
also increase. Their low recall illustrates that they are very cautious in the unbalanced training process because

A Comparison of Code Embeddings and Beyond

•

11

Table 5. The Results of Code Search

Groups

Model

SuccessRate@10 MRR

Token-based

AST-based

LSTM
Transformer

TBCNN
AutoenCODE

code2vec
code2seq

GGNN
ASTNN

60.27%
53.99%

0.75%
1.32%

39.48%
72.73%
44.37%
40.72%

0.3946
0.3376

0.0088
0.0088

0.2170
0.5391
0.2583
0.2284

they incline to classify uncertain samples to negative class. We inspect that they are less capable of detecting
programs with complicated array calculations and multiple functions. Thus, they misjudge more positive samples
to negative ones.

In short, for the programs collected from a real code repository, the token-based embedding models may be
the sufficiently good choice for code clone detection. If precision or recall is the metric of interest, one may also
apply AST-based models such as ASTNN or TBCNN.

4.4 Code Search
Dataset: We use the CodeSearchNet [27] dataset for this task. The dataset is collected via the Bing search
engine, where each query is paired with a returned code snippet. The queries and programs are combined with
intent rewrites in StaQC [71]. Following the work in [55], we use the Java code for the experiments, and obtain a
training set of 330, 000 query-code pairs and a testing set of 19, 000 query-code pairs.

Metric: At testing time, for a pair of query and program, the model computes a similarity score, indicating
how much the program matches the query. Based on the similarity scores, we calculate two commonly used
metrics for code search [35, 42, 48, 72], namely SuccessRate@k and Mean Reciprocal Rank (MRR). For a given
query, both the metrics evaluate whether the paired program is among the top candidates returned by the model.
The set of candidate programs are formed by the paired program plus 998 randomly selected programs from the
dataset. Then the model computes the similarity scores between the query and each candidate program, and
sorts the scores in descending order. The metric SuccessRate@k considers a search to be successful if the score of
the paired program is among the top-k in the sorted list. Then SuccessRate@k is calculated as the proportion
of successful searches to all searches. We set 𝑘 = 10 in the experiments [22, 55]. MRR takes a step forward and
considers the rank of the paired program in the list. It is calculated as the average of the inverse of ranks for all
the paired programs. For both metrics, a higher value indicates better performance.

Results: The results for code search are shown in the Table 5. We observe that code2seq has much better
performance than all the other models on both metrics, where SuccessRate@10 is 72.73% and MRR is 0.5391, and
the two token-based models perform better than the remaining AST-based models except code2seq.

For the two models extracting features from the AST paths, code2seq has a drastic performance improvement
over code2vec. Since the difference between them is that code2seq uses more fine-grained information of the
internal nodes, we may conclude that the internal nodes of the AST paths convey vital features to match the
program with the corresponding query and therefore cannot be neglected. The moderate performance of the two
token-based models again indicates that the plain text of the source code already contains essential features for

12

• Siqi Han, DongXia Wang, Wanting Li, and Xuesong Lu

code search. To see this, we apply the method described in Section 4.3 and calculate the average cosine similarities
between a program and the corresponding query and between a program and the negative sample queries. The
results are 0.784 and 0.677, respectively. Therefore, the token-based models can benefit from the textual similarity
between a program and the corresponding query. TBCNN and AutoenCODE perform very bad on this task, which
may indicate that the key features for matching with a query are not captured with the bottom-up aggregation
approach. GGNN and ASTNN have similar performance with code2vec, although they integrate the information
of internal nodes into the program embedding. Therefore we may conclude that with the precise information of
the internal nodes, the structural information of the AST paths is the strongest signal for matching a program
with the corresponding natural language query.

In short, for tasks such as code search that need to match the content of natural language (NL) and programming
language (PL), using the token-based models to embed the programs yields overall better results than using
the AST-based models. However, the AST-paths may contain the key structural features that can drastically
boost the performance of such tasks. Indeed, code2seq outperforms all baseline models on another NL-PL task,
code summarization, in the original paper [4], which aims to automatically generate a piece of natural language
comment for a program. This is another evidence for our conjecture.

5 EXPLANATORY STUDY
In this section, we study why the models demonstrate the performance reported in Section 4. We attempt to
answer the question by investigating what elements in the source code are mostly relevant to the model prediction,
which is a widely studied problem in the field of explainable deep learning in recent years [28, 51, 69]. Note that
although the method does not directly manipulate the structure of an AST, it tells what each AST-based model
captures from the source code via the corresponding structural information. As such, the results could be viewed
as a proxy of how well each particular AST structure captures the semantic information.

To this end, a practical solution is to calculate the attention weights in the models and see which part of the
source code is mostly attended [14, 17, 59–61]. However, not all the models under evaluation take advantage of
the attention mechanism, e.g., LSTM, TBCNN, GGNN, which makes the approach infeasible for our experiments.

5.1 Explaining with Integrated Gradients
An alternative solution is to attribute the predictions of the models to their input features, i.e., to investigate
what elements in the source code are responsible for the predictions. The underlying techniques are referred
to as prediction attribution techniques [7, 23, 52, 53]. A simple yet effective prediction attribution technique is
proposed by Sundararajan et al. [56] and called integrated gradients. The intuition of the technique is that, when
attributing a prediction to some features in the input, one also needs to know the “neutral” output of the model
given the absence of the features in the input. Such an input is called the baseline input. For example, when
attributing a sentiment prediction of a sentence to the individual words, the sequence of all-zero vectors could be
used as a baseline input [19, 25, 46, 56, 62]. Then the integrated gradients technique distributes the difference
between the prediction of the original input and the prediction of the baseline input to the individual input
features.

For the models under evaluation, the individual input features are the tokens in the source code (LSTM and
Transformer), the tokens of the leaf/internal nodes (all AST-based models), and the tokens of the paths (code2vec
and code2seq). Given a proper baseline input for each model on each task, integrated gradients technique could
attribute the prediction to the tokens in the source code. Then we visualize the attribution results by highlighting
the elements in the source code with high attribution scores. In particular, we rank the input tokens by their
attribution score in the descending order and pick the top 60% tokens. We further equally divide the picked tokens
into three groups, where the group with the highest scores are highlighted using the red color, the group with

A Comparison of Code Embeddings and Beyond

•

13

Table 6. The neutral characteristic of the all-zero baseline.

Classification Task

Clone Task

Search Task

p-value of chisquare test Mean Standard Deviation Mean Standard Deviation

LSTM
Transformer

TBCNN
AutoenCODE
code2vec
code2seq
GGNN
ASTNN

1.0000
0.9999

1.0000
1.0000
1.0000
1.0000
1.0000
1.0000

0.2466
0.0200

0.0000
0.1065
0.1679
0.0143
0.0004
0.0272

3.77e-01
5.86e-02

3.30e-09
2.02e-02
1.99e-01
1.05e-01
1.70e-02
1.32e-01

0.3596
0.1727

0.0489
0.3149
0.0937
-0.0235
0.0000
0.0742

2.03e-01
1.58e-01

5.02E-02
1.61E-02
1.65e-01
1.96e-01
0.00e+00
7.59e-02

the medium scores are highlighted using the orange color, and the group with the lowest scores are highlighted
using the yellow color. For each task, we choose three programs for visualization and render the programs for
each individual model. Due to the page limit, we demonstrate the results of one program for each task and leave
the results of the other two programs in the supplementary material. In the end, we also try to systematically
interpret the attribution results by grouping different types of input tokens in all code.

5.2 The Baseline Input
A baseline input should produce “neutral” prediction for each task [56]. For code classification, neutral prediction
means the model cannot identify which class a given input belongs to, i.e., the model should output a uniform
distribution over all classes for the input. For code clone detection, neutral prediction means the output should
be close to zero when the input pair consists of a reference code and a baseline code. Then once we replace the
baseline with a code to be detected, we could blame the clone probability to the input features of the detected
code. Similarly, for code search, neutral prediction means the output cosine distance is close to zero given a query
and a baseline code. Then once we replace the baseline with a candidate code, we could blame the cosine distance
to the candidate code, no matter the distance (i.e., correlation between the query and the code) is positive or
negative.

Borrowing from the idea in NLP, we use all-zero embedding vectors to constitute the baseline code and study
the corresponding output for each task. For code classification, neutral prediction means the model cannot identify
which class a given input belongs to, that is, the output distribution is uniform over all classes. To verify this, we
calculate the average output of all baseline code in each class and then compute the mean of the output for all
classes. We assume the mean output is a uniform distribution over all classes, and conduct a goodness-of-fit test
with chi-square statistics. From the second column in Table 6, with the default setting on confidence (0.95) and
degree of freedom (𝑛 − 1, 𝑛 = 104), we cannot reject the uniformity hypothesis for every model since the p-value
of chi-square test is greater than the confidence. For code clone detection, neutral prediction means the model
output should be close to zero when the input pair consists of a reference code and a baseline code. We calculate
the mean and standard deviation of the clone probabilities between all reference code and the baseline code,
and demonstrate that most of the program embedding models get a close-to-zero output, which are displayed in
the third column of Table 6. Similarly, for code search, neutral prediction means the output cosine distance is
close to zero when the input pair consists of a query and a baseline code. From the last column in Table 6, we
calculated the mean and standard deviation of the cosine distances between all queries and the baseline code,
and demonstrate that the baseline input can produce neutral prediction for all models. The results show that

14

• Siqi Han, DongXia Wang, Wanting Li, and Xuesong Lu

(a) Attribution score on LSTM

(b) Attribution score on Transformer

(c) Attribution score on TBCNN

(d) Attribution score on AutoenCODE

(e) Attribution score on code2vec

(f) Attribution score on code2seq

(g) Attribution score on GGNN

(h) Attribution score on ASTNN

Fig. 4. Attribution analysis on code classification. The function reverses the order of the digits in an integer.

the all-zero input generally satisfies the requirement of neutral prediction in all three tasks for all the program
embedding models.

5.3 Attribution Analysis
5.3.1 Code Classification. We first perform attribution analysis on the code classification task and explore
what elements in the source code are responsible for correct or incorrect functionality prediction. The difference
between the output of a program and the output of a baseline input is calculated as the sum of the difference
between corresponding values in the two output vectors. We select a simple program that reverses the order of

void main() {int num, i, num2= 0, b;scanf("%d", &num);for(i = 1; i <=6; i++){b = num % 10;  num = num /10;if(b != 0)  num2 = num2 * 10 + b;} printf("%d", num2);}Predict:  1  void main() {int num, i, num2 =0, b;scanf("%d", &num);for (i= 1; i <= 6; i++) {b =num % 10;  num =num / 10;if(b!= 0)  num2 =num2 *10+ b;} printf("%d", num2);}Predict:  0  voidmain() {int num, i, num2 =0, b;scanf("%d", &num);for(i =1; i <=6; i++){b =num %10;  num =num / 10;if(b != 0)  num2 =num2 *10 +b;} printf("%d", num2);}Predict:  1  voidmain() {intnum, i, num2 = 0, b;scanf("%d", &num);for (i = 1; i <= 6; i++){b = num % 10;  num= num/ 10;if(b!= 0)  num2= num2* 10+ b;} printf("%d", num2);}Predict:  0  void main() {intnum, i, num2 = 0, b;scanf("%d", &num);for (i = 1; i <=6; i++){b = num% 10;  num =num/ 10;if(b != 0)  num2 = num2 * 10 + b;} printf("%d", num2);}Predict:  0  void main() {intnum, i, num2 = 0, b;scanf("%d", &num);for(i =1; i <=6;i++){b = num % 10;  num =num /10;if(b !=0)  num2 =num2*10 +b;} printf("%d", num2);}Predict:  0  void main() {int num, i, num2=0, b;scanf("%d", &num);for (i =1; i <= 6; i++) {b=num%10;  num=num/10;if(b!=0)  num2=num2*10+b;} printf("%d", num2);}Predict:  1  void main(){int num, i, num2 = 0, b;scanf("%d",&num);for (i = 1;i <= 6;i++){b = num % 10;  num = num / 10;if(b != 0)  num2 = num2 * 10 + b;} printf("%d",num2);}Predict:  1  A Comparison of Code Embeddings and Beyond

•

15

the digits in an integer for visualization. The attribution results for all the models are shown in Fig. 4, where a 1
prediction indicates a correct classification and a 0 prediction indicates a wrong classification. More results can
be read in the supplementary material.

We can observe that two parts of elements are vital for the models to correctly classify this program. The first
part is the tokens in the for loop statement (line 4), which iterates over the digits in the input integer. Among the
four models that correctly predict the functionality, LSTM, TBCNN and ASTNN give high credits to the tokens
in the statement. In contrast, 3 out of the 4 models that fail to predict the functionality have not captured the
keyword for and give next-to-zero credits to the tokens in the statement. The second part are the tokens in the
body of the for loop (line 5-6), which conduct the actual reversal process. Both ASTNN and GGNN give high
credits to the tokens in the two statements. The three failed models, Transformer, code2vec and code2seq, have
not captured much information of these tokens. For the latter two models, the prediction is first distributed to
the AST paths, which is in turn distributed to the AST nodes. Thus if a key path is not given a high credit, all the
tokens in the path might be down-weighted. On the other hand, although AutoenCODE gives high credits to
the reversal statements, it fails to predict the functionality. We guess this is because it uses a greedy bottom-up
approach to aggregate the token information and neglects the flow of control and data as well as the AST’s
structure, as explained in Section 4.2.

The results indicate that to correctly predict the functionality of a program, the embedding models should
capture the key features pertaining to the control statements and the statements that actually realize the
functionality. Moreover, neglecting the AST’s structure and the flow of control and data hinders the correct
prediction, although some key features are captured.

5.3.2 Code Clone Detection. To visualize the prediction attribution for code clone detection, we pick a clone
pair of interest and replace one program in the pair with all-zero vectors to form the baseline pair. The difference
of the two outputs is calculated as the difference of the clone probability predicted by the models. Then we
attribute the difference to the tokens in the program that is replaced, which could tell why the program is (or not)
predicted as a clone of the other one. In the main paper, we demonstrate the prediction attribution results of a
Type-4 clone pair, where the two programs have different statements but similar functionality. The functionality
is to read the content from input streams (files) and write it to output streams (files).

The results are shown in Figure 5, where Figure 5(a) shows the fixed program in the pair and the other subfigures
show the prediction attribution on the other program for the eight models. All the models except AutoenCODE
successfully predict the pair is cloned. From the visualization, we can observe that the most important features to
capture are the tokens regarding the input and output stream, which correspond to the tokens regarding the input
and output files in Figure 5(a). AutoenCODE gives no credit to any token in line 2, which initializes an output
stream, therefore it fails to predict the clone. All other models have attributed to at least one token pertaining to
the input and output stream. Although LSTM attributes few credits to the tokens conducting the actual read and
write, the successful capturing of the input and output stream leads to the correct prediction.

5.3.3 Code Search. To visualize the prediction attribution for code search, we pick a pair of query and program
and run the model to output a similarity score between them. Then we replace the program with a baseline
program of all-zero vectors and obtain another similarity between the query and the baseline. The difference of
the two outputs is thereby the difference of the similarity scores. We attribute the difference to the input tokens
in the original program and visualize the results. The selected natural language query is “count number of chars
that match in two strings starting from front”. We may observe that the corresponding program returns the
length of the longest common substring in the two strings from the beginning.

The attribution results are shown in Figure 6. A search result of 1 indicates the similarity score between the
program and the query is among the top-10 of the candidate program list (i.e., a successful search), and a search
result of 0 indicates otherwise. LSTM, Transformer, code2seq and ASTNN make the correct prediction. We believe

16

• Siqi Han, DongXia Wang, Wanting Li, and Xuesong Lu

(a) Program-1 in the clone pair

(b) Attribution score on LSTM

(c) Attribution score on Transformer

(d) Attribution score on TBCNN

(e) Attribution score on AutoenCODE

(f) Attribution score on code2vec

(g) Attribution score on code2seq

(h) Attribution score on GGNN

(i) Attribution score on ASTNN

Fig. 5. Attribution analysis on clone pairs. The functions read from input streams/files and write to output streams/files.

this is because they capture the important tokens in line 1, 3 and 4, such as “match”, “string”, “length” and “char”.
These tokens and the corresponding statements are not only relevant to the functionality of the program, but

public static void main(String[] args) throws FileNotFoundException{if (args.length< 2) throw new IllegalArgumentException();String fnOut= args[args.length-1];PrintWriterwriter = new PrintWriter(fnOut);for (int i= 0; i< args.length-1; i++) {File fInput= new File(args[i]);Scanner in = new Scanner(fInput);while (in.hasNext()) {  writer.println(in.nextLine()); }} writer.close();}private void streamContains(String in, InputStreamstream) throwsIOException{ByteArrayOutputStreambaos=newByteArrayOutputStream();IOUtils.copy(stream, baos);byte[] bytes = baos.toByteArray();String cmp= new String(bytes, "UTF-8");assertTrue(cmp.contains(in));baos.close();}Predict:  1  privatevoidstreamContains(String in, InputStreamstream) throwsIOException{ByteArrayOutputStreambaos=new ByteArrayOutputStream();IOUtils.copy(stream, baos);byte[] bytes = baos.toByteArray();String cmp=new String(bytes, "UTF-8");assertTrue(cmp.contains(in));baos.close();}Predict:  1  private void streamContains(String in, InputStreamstream) throws IOException{ByteArrayOutputStreambaos= new ByteArrayOutputStream();IOUtils.copy(stream, baos);byte[] bytes = baos.toByteArray();String cmp= new String(bytes, "UTF-8");assertTrue(cmp.contains(in));baos.close();}Predict:  1  privatevoid streamContains(Stringin, InputStreamstream) throws IOException{ByteArrayOutputStreambaos= new ByteArrayOutputStream();IOUtils.copy(stream, baos);byte[] bytes = baos.toByteArray();Stringcmp= new String(bytes, "UTF-8");assertTrue(cmp.contains(in));baos.close();}Predict:  0  private void streamContains(String in, InputStreamstream) throws IOException{ByteArrayOutputStreambaos=new ByteArrayOutputStream();IOUtils.copy(stream, baos);byte[] bytes=baos.toByteArray();Stringcmp=new String(bytes, "UTF-8");assertTrue(cmp.contains(in));baos.close();}Predict:  1  private void streamContains(Stringin, InputStreamstream) throws IOException{ByteArrayOutputStreambaos= new ByteArrayOutputStream();IOUtils.copy(stream, baos);byte[] bytes=baos.toByteArray();Stringcmp=newString(bytes, "UTF-8");assertTrue(cmp.contains(in));baos.close();}Predict:  1  private void streamContains(String in, InputStreamstream) throws IOException{ByteArrayOutputStreambaos=new ByteArrayOutputStream();IOUtils.copy(stream, baos);byte[]bytes=baos.toByteArray();Stringcmp=new String(bytes, "UTF-8");assertTrue(cmp.contains(in));baos.close();}Predict:  1  private void streamContains(String in,InputStreamstream)throws IOException{ByteArrayOutputStreambaos= new ByteArrayOutputStream();IOUtils.copy(stream, baos);byte[] bytes = baos.toByteArray();String cmp= new String(bytes, "UTF-8");assertTrue(cmp.contains(in));baos.close();}Predict:  1  A Comparison of Code Embeddings and Beyond

•

17

(a) Attribution score on LSTM

(b) Attribution score on Transformer

(c) Attribution score on TBCNN

(d) Attribution score on AutoenCODE

(e) Attribution score on code2vec

(f) Attribution score on code2seq

(g) Attribution score on GGNN

(h) Attribution score on ASTNN

Fig. 6. The example of code search. The query is “count number of chars that match in two strings starting from front”.

also highly correlated with the tokens in the natural language query. The other four models that fail to search the
program have not attributed the similarity score to all of the important tokens. For example, all the four models
fail to capture the token “length”. For TBCNN, we notice that it gives most credits to the internal nodes, therefore
most tokens in the source code (corresponding to the leaf nodes) are not highlighted.

In short, for code search, both the program tokens correlated with the query tokens and the semantic property
of the program are vital for a successful search. Therefore an embedding model that extracts fine-grained token
features might be preferred.

static public int match(Strings1, Strings2) {inti= 0;while ((i<s1.length()) && (i< s2.length())) {if(s1.charAt(i) != s2.charAt(i)) {break;  }i++; } returni;}SearchResult:  1  staticpublic int match(Strings1, Strings2) {inti=0;while((i<s1.length()) &&(i< s2.length())) {if(s1.charAt(i) != s2.charAt(i)) {break;  }i++; } return i;}SearchResult:  1  staticpublicintmatch(Strings1, Strings2) {inti= 0;while((i<s1.length()) && (i<s2.length())) {if (s1.charAt(i) !=s2.charAt(i)) {break;  }i++; } returni;}SearchResult:  0  staticpublicintmatch(Strings1, String s2) {int i= 0;while ((i< s1.length()) && (i< s2.length())) {if (s1.charAt(i) != s2.charAt(i)) {break;  }i++; } return i;}SearchResult:  0  static public int match(Strings1, Strings2) {inti= 0;while ((i< s1.length()) && (i< s2.length())) {if (s1.charAt(i) != s2.charAt(i)) {break;  }i++; } return i;}SearchResult:  0  static public int match(Strings1, Strings2) {inti=0;while((i< s1.length()) && (i< s2.length())) {if (s1.charAt(i) != s2.charAt(i)) {break;  }i++; } return i;}SearchResult:  1  static public int match(String s1, String s2) {int i= 0;while ((i< s1.length()) &&(i< s2.length())) {if (s1.charAt(i) != s2.charAt(i)) {break;  }i++; } returni;}SearchResult:  0  static public int match(String s1,String s2){int i= 0;while ((i< s1.length()) && (i< s2.length())){if (s1.charAt(i)!= s2.charAt(i)) {break;  }i++; } return i;}SearchResult:  1  18

• Siqi Han, DongXia Wang, Wanting Li, and Xuesong Lu

5.3.4 Systemic Visualization. Like most explanation techniques, the integrated gradients method is tailored
towards explaining black-box models using individual data points, i.e., the individual programs in this study. For
better understanding the overall performance, we need to know whether the findings on a single program can be
applied to most other programs. As such, we try to systemically visualize the attribution results by aggregating
the attribution scores in individual programs.

It is not possible to directly aggregate the attribution scores of the elements in programs, since different programs
use different data types, variable names, functions and other elements. Fortunately, each program element is
encapsulated in an AST node and associated with a node type. For example, the types of IdentifierType, While,
ID represent the type of an identifier, a while statement and a variable, respectively. We thus take advantage of
the AST node types and visualize the average attribution scores (normalized) of the elements of each node type.
The results are plotted in Figure 7, 8 and 9 for the three tasks, where a deeper color indicates a higher score. Note
that we exclude LSTM and Transformer, since they do not deal with ASTs. We roughly divide all types into three
categories and pick frequent types in each category for visualization. In the figures, the types from FuncDef to
Assignment are related to the elements during initialization, the types from Return to Break are related to the
control statements, and the remaining types are related to the elements in the expressions. The UnaryOp type in
C language (Figure 7) is merged into the ID type in Java (Figure 8 and 9).

Fig. 7. Overall attribution scores of token types in category-56 for code classification.

For code classification, we pick all programs in category 56 for clear visualization. All programs perform the
same function as the code in Figure 4. We observe in Figure 7 that the two best models GGNN and ASTNN give
high scores to both the elements in the control statements and the elements in the expressions, which coincides
with the observations in the example code of Figure 4. TBCNN, code2vec and code2seq also focus on either the
control statements or the expressions, and thus have moderate performance in accordance with the results in
Table 3. AutoenCODE gives the least scores to these elements and hence has the worst performance among the
models.

For code clone, in Figure 5 we observe the important elements are the tokens pertaining to the descriptions and
actual operations of functionalities. These elements often fall into the initialization and expression categories. We
observe in Figure 8 that all models except AutoenCODE give high scores to the elements in these two categories,
which is also in accordance with the performance reported in Table 4.

For code search, in Figure 6 we observe the important elements are the source code tokens correlated with
the query tokens and those relevant to the corresponding functionality. These elements are usually the function

A Comparison of Code Embeddings and Beyond

•

19

Fig. 8. Overall attribution scores of token types for code clone detection.

Fig. 9. Overall attribution scores of token types for code search.

names (e.g., “match”, “charAt”, “length”), data types (e.g., “string”) and variables, which belong to the initialization
and expression categories. We observe in Figure 9 that code2seq gives the highest scores to the two categories.
As such it has the best performance on the task, as reported in Table 5. Code2vec, ASTNN and GGNN focus less
on these elements and hence perform worse than code2seq.

6 DISCUSSION
We discuss three key points learned from the experimental study.

First, no single existing AST-based model can beat the simple token-based models in all tasks. This
may indicate that the plain text of source code has revealed strong naturalness of programming language so that
applying classic NLP models directly on source code could already bring pretty good performance on most tasks.
Indeed, recently a programming language model CodeBert [20] was trained with the plain text of source code
(and natural language sentences) and showed very good performance on many tasks. The high textual similarity
in the current benchmark datasets may also facilitate the training of the token-based models. Nevertheless, for a

20

• Siqi Han, DongXia Wang, Wanting Li, and Xuesong Lu

particular task, we can always derive an AST-based model that performs better than the token-based models. This
can be verified by the performance of ASTNN and GGNN for code classification, TBCNN and ASTNN for code
clone detection, and code2seq for code search. Therefore one may always try to explore the abundant structural
information in ASTs to achieve the best performance for a specific task.

Second, the original structure of an AST and the flow of control and data in the source code are both
important features for an AST-based model to achieve good performance. Because they capture both the
information, ASTNN and GGNN have the overall best performance on the three tasks among the AST-based
models. ASTNN preserves the structure of each statement subtree and feed the subtrees into the model in the
order complying with the statement order in the source code. GGNN augments the original AST’s structure with
semantic edges that represent the control and data flow in the source code. In contrast, AutoenCODE ignores the
positions of the leaf nodes in the AST and uses a greedy algorithm to aggregate the node information, which
destroys the structure of the original AST. Therefore it has the overall worst performance. TBCNN, code2vec and
code2seq extract only the structural information from the ASTs, and therefore have moderate overall performance.
Finally, the prediction attribution study tells that despite the difference in the ways of manipulating
the AST structure, the models with good performance all capture the key tokens to a task in the source
code. These key tokens can be the reserved words for control statements (e.g., for, if), the key variables and
operators in the expressions, or the tokens that are correlated with the words in natural language. For the
token-based models, all the tokens are presented in the plain text so that the models could naturally learn from
the tokens. However, tokens like reserved words and function calls are often organized as the internal nodes in
an AST, therefore the AST-based models may miss such information (e.g., AutoenCODE and code2vec) and result
in sub-optimal performance. As such, we suggest to always fully utilize the information of the internal nodes
when designing an AST-based embedding model.

7 THREATS TO VALIDITY
1) Selection of datasets. The experimental results are related to the datasets used. A different distribution of
data points or a different type of programming language of the code may bring different results for the same task.
As such the conclusion drawn from the datasets experimented may not be able to generalize beyond. To mitigate
the bias introduced by the datasets, we adopt widely used public datasets for all tasks, as they are recognized as
the benchmark of the corresponding task. Furthermore, we evaluate on more than one dataset for a task if all the
datasets are commonly used. For example, in addition to BigCloneBench in the main article, we also report
the results on the OJClone dataset in the supplementary material. The similar overall results may validate the
conclusions for the clone detection task.

2) Network architecture for downstream tasks. The performance of a model on each task is affected by
both the program embedding and the network architecture connecting the embedding and the output for the
downstream task. Therefore we may not be able to completely attribute the experimental results to the program
embeddings. To mitigate the influence of the network architecture, we use the structure as simple as possible for
each selected task. For code classification and code clone, we only add one fully-connected layer on top of the
program embedding and the difference of two embeddings, respectively. For code search, we directly calculate the
cosine distance between positive and negative pairs of program embeddings, respectively. These simple strategies
are also employed by the representative models for each task [22, 27, 45, 55, 76].

3) Explaining with individual examples. Most explanation techniques including the integrated gradients
method are tailored towards explaining black-box models using individual data points [7, 28, 51, 56, 69]. The
findings on individual programs may not be convincing for representing the whole picture. To mitigate this
problem, we take the advantage of the AST node types and systemically visualize the attribution results by

A Comparison of Code Embeddings and Beyond

•

21

aggregating the scores of input tokens of the same node type. The results are largely in accordance with the
attribution findings in individual programs as well as the model performance reported in Section 4.

8 CONCLUSIONS
In this experience paper, we systemically evaluate eight program embedding models on three common program-
ming language tasks. Six of the eight models are based on abstract syntax trees, which are the focus of this paper,
and the other two models are based on the plain text of source code, which are used as baseline models.

The results of performance evaluation have revealed that while the token-based models are robust to different
tasks, a carefully-designed AST-based model may still achieve the best performance given a specific task. This
further indicates that both the naturalness of a programming language such as data and control flow, and the
structural information in the ASTs are important information for learning a good program embedding.

The explanatory study tells that whether capturing the key tokens in the source code highly influences the
correctness of prediction. Given that many tokens may correspond to the internal nodes in an AST, we suggest
that an AST-based model should fully make use of the internal nodes.

REFERENCES
[1] Wasi Uddin Ahmad, Saikat Chakraborty, Baishakhi Ray, and Kai-Wei Chang. 2020. A transformer-based approach for source code

summarization. arXiv preprint arXiv:2005.00653 (2020).

[2] Miltiadis Allamanis, Earl T Barr, Premkumar Devanbu, and Charles Sutton. 2018. A survey of machine learning for big code and

naturalness. ACM Computing Surveys (CSUR) 51, 4 (2018), 1–37.

[3] Miltiadis Allamanis, Marc Brockschmidt, and Mahmoud Khademi. 2018. Learning to Represent Programs with Graphs. In International

Conference on Learning Representations.

[4] Uri Alon, Shaked Brody, Omer Levy, and Eran Yahav. 2018. code2seq: Generating Sequences from Structured Representations of Code.

In International Conference on Learning Representations.

[5] Uri Alon, Roy Sadaka, Omer Levy, and Eran Yahav. 2020. Structural language models of code. In International Conference on Machine

Learning. PMLR, 245–256.

[6] Uri Alon, Meital Zilberstein, Omer Levy, and Eran Yahav. 2019. code2vec: Learning distributed representations of code. Proceedings of

the ACM on Programming Languages 3, POPL (2019), 1–29.

[7] David Baehrens, Timon Schroeter, Stefan Harmeling, Motoaki Kawanabe, Katja Hansen, and Klaus-Robert Müller. 2010. How to explain

individual classification decisions. The Journal of Machine Learning Research 11 (2010), 1803–1831.

[8] Ira D Baxter, Andrew Yahin, Leonardo Moura, Marcelo Sant’Anna, and Lorraine Bier. 1998. Clone detection using abstract syntax trees.

In Proceedings. International Conference on Software Maintenance (Cat. No. 98CB36272). IEEE, 368–377.

[9] Tal Ben-Nun, Alice Shoshana Jakobovits, and Torsten Hoefler. 2018. Neural Code Comprehension: A Learnable Representation of Code

Semantics. Advances in Neural Information Processing Systems 31 (2018), 3585–3597.

[10] Shaked Brody, Uri Alon, and Eran Yahav. 2020. A structural model for contextual code changes. Proceedings of the ACM on Programming

Languages 4, OOPSLA (2020), 1–28.

[11] Nghi DQ Bui, Lingxiao Jiang, and Yijun Yu. 2018. Cross-Language Learning for Program Classification using Bilateral Tree-Based

Convolutional Neural Networks. In The Thirty-Second AAAI Conference on Artificial Intelligence (AAAI-18).

[12] Nghi DQ Bui, Yijun Yu, and Lingxiao Jiang. 2021. InferCode: Self-Supervised Learning of Code Representations by Predicting Subtrees.

In 2021 IEEE/ACM 43rd International Conference on Software Engineering (ICSE). IEEE, 1186–1197.

[13] Jose Cambronero, Hongyu Li, Seohyun Kim, Koushik Sen, and Satish Chandra. 2019. When deep learning met code search. In Proceedings
of the 2019 27th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software
Engineering. 964–974.

[14] Jindong Chen, Yizhou Hu, Jingping Liu, Yanghua Xiao, and Haiyun Jiang. 2019. Deep short text classification with knowledge powered

attention. In Proceedings of the AAAI Conference on Artificial Intelligence, Vol. 33. 6252–6259.

[15] Zimin Chen and Martin Monperrus. 2019. A literature study of embeddings on source code. arXiv preprint arXiv:1904.03061 (2019).
[16] Abdullah Chihada, Saeed Jalili, Seyed Mohammad Hossein Hasheminejad, and Mohammad Hossein Zangooei. 2015. Source code and
design conformance, design pattern detection from source code by classification approach. Applied Soft Computing 26 (2015), 357–367.
[17] Kevin Clark, Urvashi Khandelwal, Omer Levy, and Christopher D Manning. 2019. What does BERT look at? An Analysis of BERT’s

Attention. ACL 2019 (2019), 276.

[18] Hoa Khanh Dam, Truyen Tran, and Trang Pham. 2016. A deep language model for software code. arXiv preprint arXiv:1608.02715 (2016).

22

• Siqi Han, DongXia Wang, Wanting Li, and Xuesong Lu

[19] Mengnan Du, Ninghao Liu, Fan Yang, Shuiwang Ji, and Xia Hu. 2019. On attribution of recurrent neural network predictions via additive

decomposition. In The World Wide Web Conference. 383–393.

[20] Zhangyin Feng, Daya Guo, Duyu Tang, Nan Duan, Xiaocheng Feng, Ming Gong, Linjun Shou, Bing Qin, Ting Liu, Daxin Jiang, et al.

2020. Codebert: A pre-trained model for programming and natural languages. arXiv preprint arXiv:2002.08155 (2020).

[21] Georgia Frantzeskou, Stephen MacDonell, Efstathios Stamatatos, and Stefanos Gritzalis. 2008. Examining the significance of high-level

programming features in source code author classification. Journal of Systems and Software 81, 3 (2008), 447–460.

[22] Xiaodong Gu, Hongyu Zhang, and Sunghun Kim. 2018. Deep code search. In 2018 IEEE/ACM 40th International Conference on Software

Engineering (ICSE). IEEE, 933–944.

[23] Rahul Gupta, Aditya Kanade, and Shirish Shevade. 2019. Neural attribution for semantic bug-localization in student programs. Network

1, P2 (2019), P2.

[24] Rahul Gupta, Soham Pal, Aditya Kanade, and Shirish Shevade. 2017. Deepfix: Fixing common c language errors by deep learning. In

Proceedings of the aaai conference on artificial intelligence, Vol. 31.

[25] Shilin He, Zhaopeng Tu, Xing Wang, Longyue Wang, Michael Lyu, and Shuming Shi. 2019. Towards Understanding Neural Machine
Translation with Word Importance. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the
9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP). 952–961.

[26] Xuan Huo, Ming Li, and Zhi-Hua Zhou. 2020. Control Flow Graph Embedding Based on Multi-Instance Decomposition for Bug

Localization. In Proceedings of the AAAI Conference on Artificial Intelligence, Vol. 34. 4223–4230.

[27] Hamel Husain, Ho-Hsiang Wu, Tiferet Gazit, Miltiadis Allamanis, and Marc Brockschmidt. 2019. Codesearchnet challenge: Evaluating

the state of semantic code search. arXiv preprint arXiv:1909.09436 (2019).

[28] Alon Jacovi and Yoav Goldberg. 2020. Towards Faithfully Interpretable NLP Systems: How Should We Define and Evaluate Faithfulness?.

In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics. 4198–4205.

[29] Derry Jatnika, Moch Arif Bijaksana, and Arie Ardiyanti Suryani. 2019. Word2vec model analysis for semantic similarities in english

words. Procedia Computer Science 157 (2019), 160–167.

[30] Toshihiro Kamiya, Shinji Kusumoto, and Katsuro Inoue. 2002. CCFinder: A multilinguistic token-based code clone detection system for

large scale source code. IEEE Transactions on Software Engineering 28, 7 (2002), 654–670.

[31] Hong Jin Kang, Tegawendé F Bissyandé, and David Lo. 2019. Assessing the generalizability of code2vec token embeddings. In 2019 34th

IEEE/ACM International Conference on Automated Software Engineering (ASE). IEEE, 1–12.

[32] Shinji Kawaguchi, Pankaj K Garg, Makoto Matsushita, and Katsuro Inoue. 2006. Mudablue: An automatic categorization system for

open source repositories. Journal of Systems and Software 79, 7 (2006), 939–953.

[33] Tom Kenter and Maarten De Rijke. 2015. Short text similarity with word embeddings. In Proceedings of the 24th ACM international on

conference on information and knowledge management. 1411–1420.

[34] Vladimir Kovalenko, Egor Bogomolov, Timofey Bryksin, and Alberto Bacchelli. 2019. PathMiner: a library for mining of path-based
representations of code. In Proceedings of the 16th International Conference on Mining Software Repositories. IEEE Press, 13–17.
[35] Xuan Li, Zerui Wang, Qianxiang Wang, Shoumeng Yan, Tao Xie, and Hong Mei. 2016. Relationship-aware code search for JavaScript
frameworks. In Proceedings of the 2016 24th ACM SIGSOFT International Symposium on Foundations of Software Engineering. 690–701.
[36] Hongliang Liang, Yue Yu, Lin Jiang, and Zhuosi Xie. 2019. Seml: A semantic lstm model for software defect prediction. IEEE Access 7

(2019), 83812–83824.

[37] Mario Linares-Vásquez, Collin McMillan, Denys Poshyvanyk, and Mark Grechanik. 2014. On using machine learning to automatically

classify software applications into domain categories. Empirical Software Engineering 19, 3 (2014), 582–618.

[38] Fang Liu, Lu Zhang, and Zhi Jin. 2020. Modeling programs hierarchically with stack-augmented LSTM. Journal of Systems and Software

164 (2020), 110547.

[39] Jason Liu, Seohyun Kim, Vijayaraghavan Murali, Swarat Chaudhuri, and Satish Chandra. 2019. Neural query expansion for code search.

In Proceedings of the 3rd acm sigplan international workshop on machine learning and programming languages. 29–37.

[40] Shangqing Liu, Cuiyun Gao, Sen Chen, Nie Lun Yiu, and Yang Liu. 2020. ATOM: Commit message generation based on abstract syntax

tree and hybrid ranking. IEEE Transactions on Software Engineering (2020).

[41] Shuai Lu, Daya Guo, Shuo Ren, Junjie Huang, Alexey Svyatkovskiy, Ambrosio Blanco, Colin Clement, Dawn Drain, Daxin Jiang, Duyu
Tang, et al. 2021. CodeXGLUE: A Machine Learning Benchmark Dataset for Code Understanding and Generation. arXiv preprint
arXiv:2102.04664 (2021).

[42] Fei Lv, Hongyu Zhang, Jian-guang Lou, Shaowei Wang, Dongmei Zhang, and Jianjun Zhao. 2015. Codehow: Effective code search
based on api understanding and extended boolean model (e). In 2015 30th IEEE/ACM International Conference on Automated Software
Engineering (ASE). IEEE, 260–270.

[43] Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey Dean. 2013. Efficient estimation of word representations in vector space. arXiv

preprint arXiv:1301.3781 (2013).

[44] Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Corrado, and Jeff Dean. 2013. Distributed representations of words and phrases and

their compositionality. In Advances in neural information processing systems. 3111–3119.

A Comparison of Code Embeddings and Beyond

•

23

[45] Lili Mou, Ge Li, Lu Zhang, Tao Wang, and Zhi Jin. 2016. Convolutional neural networks over tree structures for programming language

processing. In Proceedings of the AAAI Conference on Artificial Intelligence, Vol. 30.

[46] W James Murdoch, Peter J Liu, and Bin Yu. 2018. Beyond Word Importance: Contextual Decomposition to Extract Interactions from

LSTMs. In International Conference on Learning Representations.

[47] Maxim Rabinovich, Mitchell Stern, and Dan Klein. 2017. Abstract Syntax Networks for Code Generation and Semantic Parsing. In

Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers). 1139–1149.

[48] Mukund Raghothaman, Yi Wei, and Youssef Hamadi. 2016. Swim: Synthesizing what i mean-code search and idiomatic snippet synthesis.

In 2016 IEEE/ACM 38th International Conference on Software Engineering (ICSE). IEEE, 357–367.

[49] Chanchal Kumar Roy and James R Cordy. 2007. A survey on software clone detection research. Queen’s School of Computing TR 541, 115

(2007), 64–68.

[50] Hitesh Sajnani, Vaibhav Saini, Jeffrey Svajlenko, Chanchal K Roy, and Cristina V Lopes. 2016. Sourcerercc: Scaling code clone detection

to big-code. In Proceedings of the 38th International Conference on Software Engineering. 1157–1168.

[51] Sofia Serrano and Noah A Smith. 2019. Is Attention Interpretable?. In Proceedings of the 57th Annual Meeting of the Association for

Computational Linguistics. 2931–2951.

[52] Avanti Shrikumar, Peyton Greenside, and Anshul Kundaje. 2017. Learning important features through propagating activation differences.

In International Conference on Machine Learning. PMLR, 3145–3153.

[53] Karen Simonyan, Andrea Vedaldi, and Andrew Zisserman. 2014. Deep inside convolutional networks: Visualising image classification

models and saliency maps. (2014).

[54] Aishwarya Sivaraman, Tianyi Zhang, Guy Van den Broeck, and Miryung Kim. 2019. Active inductive logic programming for code

search. In 2019 IEEE/ACM 41st International Conference on Software Engineering (ICSE). IEEE, 292–303.

[55] Zhensu Sun, Yan Liu, Chen Yang, and Yu Qian. 2020. PSCS: A Path-based Neural Model for SemanticCode Search. arXiv preprint

arXiv:2008.03042 (2020).

[56] Mukund Sundararajan, Ankur Taly, and Qiqi Yan. 2017. Axiomatic attribution for deep networks. In International Conference on Machine

Learning. PMLR, 3319–3328.

[57] Jeffrey Svajlenko, Judith F Islam, Iman Keivanloo, Chanchal K Roy, and Mohammad Mamun Mia. 2014. Towards a big data curated
benchmark of inter-project code clones. In 2014 IEEE International Conference on Software Maintenance and Evolution. IEEE, 476–480.
[58] Alexey Svyatkovskiy, Shao Kun Deng, Shengyu Fu, and Neel Sundaresan. 2020. Intellicode compose: Code generation using transformer.
In Proceedings of the 28th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software
Engineering. 1433–1443.

[59] Gongbo Tang, Rico Sennrich, and Joakim Nivre. 2018. An Analysis of Attention Mechanisms: The Case of Word Sense Disambiguation

in Neural Machine Translation. In Proceedings of the Third Conference on Machine Translation: Research Papers. 26–35.

[60] Chongyang Tao, Shen Gao, Mingyue Shang, Wei Wu, Dongyan Zhao, and Rui Yan. 2018. Get The Point of My Utterance! Learning

Towards Effective Responses with Multi-Head Attention Mechanism.. In IJCAI. 4418–4424.

[61] Jesse Vig. 2019. A Multiscale Visualization of Attention in the Transformer Model. In Proceedings of the 57th Annual Meeting of the

Association for Computational Linguistics: System Demonstrations. 37–42.

[62] Eric Wallace, Jens Tuyls, Junlin Wang, Sanjay Subramanian, Matt Gardner, and Sameer Singh. 2019. AllenNLP Interpret: A Framework
for Explaining Predictions of NLP Models. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing
and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP): System Demonstrations. 7–12.

[63] Yao Wan, Zhou Zhao, Min Yang, Guandong Xu, Haochao Ying, Jian Wu, and Philip S Yu. 2018. Improving automatic source code
summarization via deep reinforcement learning. In Proceedings of the 33rd ACM/IEEE International Conference on Automated Software
Engineering. 397–407.

[64] Ke Wang and Zhendong Su. 2020. Blended, precise semantic program embeddings. In Proceedings of the 41st ACM SIGPLAN Conference

on Programming Language Design and Implementation. 121–134.

[65] Wenhan Wang, Ge Li, Bo Ma, Xin Xia, and Zhi Jin. 2020. Detecting code clones with graph neural network and flow-augmented abstract
syntax tree. In 2020 IEEE 27th International Conference on Software Analysis, Evolution and Reengineering (SANER). IEEE, 261–271.
[66] Wenhan Wang, Ge Li, Sijie Shen, Xin Xia, and Zhi Jin. 2020. Modular tree network for source code representation learning. ACM

Transactions on Software Engineering and Methodology (TOSEM) 29, 4 (2020), 1–23.

[67] Huihui Wei and Ming Li. 2017. Supervised Deep Features for Software Functional Clone Detection by Exploiting Lexical and Syntactical

Information in Source Code.. In IJCAI. 3034–3040.

[68] Martin White, Michele Tufano, Christopher Vendome, and Denys Poshyvanyk. 2016. Deep learning code fragments for code clone

detection. In 2016 31st IEEE/ACM International Conference on Automated Software Engineering (ASE). IEEE, 87–98.

[69] Sarah Wiegreffe and Yuval Pinter. 2019. Attention is not not Explanation. In Proceedings of the 2019 Conference on Empirical Methods in
Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP). 11–20.
[70] Shuhan Yan, Hang Yu, Yuting Chen, Beijun Shen, and Lingxiao Jiang. 2020. Are the code snippets what we are searching for? a
benchmark and an empirical study on code search with natural-language queries. In 2020 IEEE 27th International Conference on Software

24

• Siqi Han, DongXia Wang, Wanting Li, and Xuesong Lu

Analysis, Evolution and Reengineering (SANER). IEEE, 344–354.

[71] Ziyu Yao, Daniel S Weld, Wei-Peng Chen, and Huan Sun. 2018. Staqc: A systematically mined question-code dataset from stack overflow.

In Proceedings of the 2018 World Wide Web Conference. 1693–1703.

[72] Xin Ye, Razvan Bunescu, and Chang Liu. 2014. Learning to rank relevant files for bug reports using domain knowledge. In Proceedings of

the 22nd ACM SIGSOFT International Symposium on Foundations of Software Engineering. 689–699.

[73] Pengcheng Yin and Graham Neubig. 2018. TRANX: A Transition-based Neural Abstract Syntax Parser for Semantic Parsing and Code
Generation. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing: System Demonstrations. 7–12.
[74] Hao Yu, Wing Lam, Long Chen, Ge Li, Tao Xie, and Qianxiang Wang. 2019. Neural detection of semantic code clones via tree-based

convolution. In 2019 IEEE/ACM 27th International Conference on Program Comprehension (ICPC). IEEE, 70–80.

[75] Huangzhao Zhang, Zhuo Li, Ge Li, Lei Ma, Yang Liu, and Zhi Jin. 2020. Generating adversarial examples for holding robustness of

source code processing models. In Proceedings of the AAAI Conference on Artificial Intelligence, Vol. 34. 1169–1176.

[76] Jian Zhang, Xu Wang, Hongyu Zhang, Hailong Sun, Kaixuan Wang, and Xudong Liu. 2019. A novel neural source code representation

based on abstract syntax tree. In 2019 IEEE/ACM 41st International Conference on Software Engineering (ICSE). IEEE, 783–794.

A ATTRIBUTION ANALYSIS ON TWO MORE INSTANCES
A.1 Code Classification
We select two other attribution instances visualized in Fig. 10 and Fig. 11 to support the insights in our main
paper. They preserve the same functionality of reversing the order of the digits in an integer. In Fig. 10, it can
be found that the three models with correct prediction give high credits on the for loop statement whereas
none of the models with incorrect prediction get a highlighted attribution score on for or while. Another
decisive part is to conduct the actual reversal process in the instance. Among the five models with incorrect
prediction, AutoenCODE perform bad on capturing both of the arithmetic operators and meaningful constants,
and the other models neglect the flow of control and disperse its attribution to more useless tokens. In the second
instance, as Fig. 11 is shown, models with correct prediction emphasize more on decisive computation consisting
of operators and constants while AutoenCODE obtains higher attribution on variable declaration statements.
With the attribution analysis on code classification task, we conclude that the embedding models should capture
the key features pertaining to the control statements and the statements that actually realize the functionality.
Moreover, neglecting the flow of control and data might hinder the correct prediction, although some key features
are captured.

A.1.1 Code Clone Detection. We analyze the attribution results of two other Type-4 examples in Fig. 12 and
Fig. 13 most of the disparity of these models appear in type-4 cloned pairs. Same as our paper, the functionality
of the first snippet in the program pair is to read the content from input streams (files) and write it to output
streams (files), which can serve as a copy procedure. The key point of our comparison is whether the model
can capture the tokens regarding the input and output stream and the copy semantics. We observe from the
Fig. 12 that all models except AutoenCODE have attributed the prediction to at least one token pertaining to the
copy semantics and the input and output stream. Similarly, in Fig. 13, although vert little tokens are related to
the input/output streams, most of the models can capture tokens in the statement of line 6, which conducts the
process of copy. Only AutoenCODE gives no credit to any meaningful token in these two instances, therefore it
fails to predict the clone.

A.1.2 Code Search. We present another two instances of code search task with different queries. In Fig. 14, the
selected natural language query aims to copy an array. Only code2seq successfully ranks the matching program
at the top 10. Although most program representation models can capture the correlated token in the shared vocab
like “array”, “copyInt” and “result”, few of them understand the functionality of the program that both of tokens,
i.e. “indexIterator”, and “next” are important to the copy process. Similarly in Fig. 15, almost all models obtain
relatively high attribution scores on “compare” and “SearchNode” under the query “compares two search nodes
by their path cost”. However, only two token-based models and code2seq learn the subtoken of “Path” or the

function name of “getPathCost”, which is closely related to the purpose of the query. We can conclude that the
token-based models benefit more from the splitting procedure to find similar subtokens and code2seq are more
effective in learning semantics from natural language query.

A Comparison of Code Embeddings and Beyond

•

25

26

• Siqi Han, DongXia Wang, Wanting Li, and Xuesong Lu

(a) Attribution score on LSTM

(b) Attribution score on Transformer

(c) Attribution score on TBCNN

(d) Attribution score on AutoenCODE

(e) Attribution score on code2vec

(f) Attribution score on code2seq

(g) Attribution score on GGNN

(h) Attribution score on ASTNN

Fig. 10. Attribution analysis on code classification. The functionality is to reverse a integer.

intmain(){int n, i = 1, a[5] = {-1,-1,-1,-1,-1}, m =0;scanf("%d",&n);while (i <n){a[m] = (int)((n%(i*10))/i);i *=10;m++;}  for(i=0;i<5;i++){if(a[i] !=-1)printf("%d",a[i]);}return 0;}Predict:1intmain(){int n, i = 1, a[5] = {-1,-1,-1,-1,-1}, m =0;scanf("%d",&n);while(i<n){a[m] =(int)((n%(i*10))/i);i *= 10;m++;}  for(i=0;i<5;i++){if (a[i] !=-1)printf("%d",a[i]);}return 0;}Predict:1intmain(){int n, i =1, a[5] = {-1,-1,-1,-1,-1}, m =0;scanf("%d",&n);while(i <n){a[m] =(int)((n%(i*10))/i);i *= 10;m++;} for(i=0;i<5;i++){if(a[i] !=-1)printf("%d",a[i]);}return 0;}Predict: 0intmain(){intn, i = 1, a[5] = {-1,-1,-1,-1,-1}, m =0;scanf("%d",&n);while (i <n){a[m] = (int)((n%(i*10))/i);i *= 10;m++;}  for(i=0;i<5;i++){if (a[i] !=-1)printf("%d",a[i]);}return 0;}Predict:0intmain(){intn, i = 1, a[5] = {-1,-1,-1,-1,-1}, m =0;scanf("%d",&n);while (i <n){a[m] =(int)((n%(i*10))/i);i*= 10;m++;}for(i=0;i<5;i++){if(a[i] !=-1)printf("%d",a[i]);}return 0;}Predict:0intmain(){intn, i= 1, a[5] = {-1,-1,-1,-1,-1}, m =0;scanf("%d",&n);while (i <n){a[m] =(int)((n%(i*10))/i);i *=10;m++;}  for(i=0;i<5;i++){if (a[i]!=-1)printf("%d",a[i]);}return 0;}Predict:0intmain(){int n, i=1, a[5] ={-1,-1,-1,-1,-1}, m =0;scanf("%d",&n);while (i<n){a[m] =(int)((n%(i*10))/i);i*=10;m++;}  for(i=0;i<5;i++){if (a[i] !=-1)printf("%d",a[i]);}return 0;}Predict:0intmain(){intn, i = 1, a[5] = {-1,-1,-1,-1,-1},m =0;scanf("%d",&n);while (i <n){a[m] = (int)((n%(i*10))/i);i *= 10;m++;}  for(i=0;i<5;i++){if (a[i] !=-1)printf("%d",a[i]);}return 0;}Predict:1A Comparison of Code Embeddings and Beyond

•

27

(a) Attribution score on LSTM

(b) Attribution score on Transformer

(c) Attribution score on TBCNN

(d) Attribution score on AutoenCODE

(e) Attribution score on code2vec

(f) Attribution score on code2seq

(g) Attribution score on GGNN

(h) Attribution score on ASTNN

Fig. 11. Attribution analysis on code classification. The functionality is to reverse a integer.

void main(){unsigned int a,b,c,d,e,x;scanf("%d",&x);a=x/10000;  b=x/1000-x/10000*10;c=x/100-x/1000*10;  d=x/10-x/100*10;  e=x%10;if(a!=0)  printf("%d\n",a+10*b+100*c+1000*d+10000*e);else if(b!=0)  printf("%d\n",b+10*c+100*d+1000*e);else if(c!=0)  printf("%d\n",c+10*d+100*e);else if(d!=0)  printf("%d\n",d+10*e);else if(e!=0)  printf("%d\n",e);}Predict: 1void main(){unsigned int a,b,c,d,e,x;scanf("%d",&x);a=x/10000;  b=x/1000-x/10000*10;c=x/100-x/1000*10;  d=x/10-x/100*10;  e=x%10;if(a!=0)  printf("%d\n",a+10*b+100*c+1000*d+10000*e);elseif(b!=0)  printf("%d\n",b+10*c+100*d+1000*e);elseif(c!=0)  printf("%d\n",c+10*d+100*e);elseif(d!=0)  printf("%d\n",d+10*e);elseif(e!=0)  printf("%d\n",e);}Predict: 1voidmain(){unsigned int a,b,c,d,e,x;scanf("%d",&x);a=x/10000;  b=x/1000-x/10000*10;c=x/100-x/1000*10;  d=x/10-x/100*10;  e=x%10;if(a!=0)  printf("%d\n",a+10*b+100*c+1000*d+10000*e);else if(b!=0)  printf("%d\n",b+10*c+100*d+1000*e);else if(c!=0)  printf("%d\n",c+10*d+100*e);else if(d!=0)  printf("%d\n",d+10*e);else if(e!=0)  printf("%d\n",e);}Predict:1voidmain(){unsignedint a,b,c,d,e,x;scanf("%d",&x);a=x/10000;  b=x/1000-x/10000*10;c=x/100-x/1000*10;  d=x/10-x/100*10;  e=x%10;if(a!=0)  printf("%d\n",a+10*b+100*c+1000*d+10000*e);else if(b!=0)  printf("%d\n",b+10*c+100*d+1000*e);else if(c!=0)  printf("%d\n",c+10*d+100*e);else if(d!=0)  printf("%d\n",d+10*e);else if(e!=0)  printf("%d\n",e);}Predict:0void main(){unsignedint a,b,c,d,e,x;scanf("%d",&x);a=x/10000;  b=x/1000-x/10000*10;c=x/100-x/1000*10;  d=x/10-x/100*10;  e=x%10;if(a!=0)  printf("%d\n",a+10*b+100*c+1000*d+10000*e);else if(b!=0)  printf("%d\n",b+10*c+100*d+1000*e);else if(c!=0)  printf("%d\n",c+10*d+100*e);else if(d!=0)  printf("%d\n",d+10*e);else if(e!=0)  printf("%d\n",e);}Predict:1void main(){unsignedinta,b,c,d,e,x;scanf("%d",&x);a=x/10000;  b=x/1000-x/10000*10;c=x/100-x/1000*10;  d=x/10-x/100*10;  e=x%10;if(a!=0)  printf("%d\n",a+10*b+100*c+1000*d+10000*e);else if(b!=0)  printf("%d\n",b+10*c+100*d+1000*e);else if(c!=0)  printf("%d\n",c+10*d+100*e);else if(d!=0)  printf("%d\n",d+10*e);else if(e!=0)  printf("%d\n",e);}Predict:1voidmain(){unsigned int a,b,c,d,e,x;scanf("%d",&x);a=x/10000;  b=x/1000-x/10000*10;c=x/100-x/1000*10;  d=x/10-x/100*10;  e=x%10;if(a!=0)  printf("%d\n",a+10*b+100*c+1000*d+10000*e);elseif(b!=0)  printf("%d\n",b+10*c+100*d+1000*e);elseif(c!=0)  printf("%d\n",c+10*d+100*e);elseif(d!=0)  printf("%d\n",d+10*e);elseif(e!=0)  printf("%d\n",e);}Predict: 1void main(){unsigned int a,b,c,d,e,x;scanf("%d",&x);a=x/10000;  b=x/1000-x/10000*10;c=x/100-x/1000*10;  d=x/10-x/100*10;  e=x%10;if(a!=0)  printf("%d\n",a+10*b+100*c+1000*d+10000*e);else if(b!=0)  printf("%d\n",b+10*c+100*d+1000*e);else if(c!=0)  printf("%d\n",c+10*d+100*e);else if(d!=0)  printf("%d\n",d+10*e);else if(e!=0)  printf("%d\n",e);}Predict:128

• Siqi Han, DongXia Wang, Wanting Li, and Xuesong Lu

(a) Code-1 in clone pair

(b) Attribution score on LSTM

(c) Attribution score on Transformer

(d) Attribution score on TBCNN

(e) Attribution score on AutoenCODE

(f) Attribution score on code2vec

(g) Attribution score on code2seq

(h) Attribution score on GGNN

(i) Attribution score on ASTNN

Fig. 12. Attribution analysis on code-2 in Type-4 clone pairs.

public static void main(String[] args) throws FileNotFoundException{if (args.length< 2) throw new IllegalArgumentException();String fnOut= args[args.length-1];PrintWriterwriter = new PrintWriter(fnOut);for (int i= 0; i< args.length-1; i++) {File fInput= new File(args[i]);Scanner in = new Scanner(fInput);while (in.hasNext()) {  writer.println(in.nextLine()); }} writer.close();}privatevoidcheckInputStream(InputStreamin, byte[] cmp, booleanall) throws IOException{ByteArrayOutputStreamstream = new ByteArrayOutputStream();IOUtils.copy(in, stream);byte[] out = stream.toByteArray();if (all) assertEquals(cmp.length, out.length);for (int i= 0; i< cmp.length; i++) assertEquals(cmp[i], out[i]);}Predict:  1  privatevoid checkInputStream(InputStreamin, byte[] cmp, booleanall) throwsIOException{ByteArrayOutputStreamstream=new ByteArrayOutputStream();IOUtils.copy(in, stream);byte[] out = stream.toByteArray();if (all) assertEquals(cmp.length, out.length);for (int i= 0; i< cmp.length; i++) assertEquals(cmp[i], out[i]);}Predict:  1  private void checkInputStream(InputStreamin, byte[] cmp, booleanall) throws IOException{ByteArrayOutputStreamstream = new ByteArrayOutputStream();IOUtils.copy(in, stream);byte[] out = stream.toByteArray();if (all) assertEquals(cmp.length, out.length);for (inti= 0; i< cmp.length; i++) assertEquals(cmp[i], out[i]);}Predict:  1 privatevoid checkInputStream(InputStreamin, byte[] cmp, booleanall) throws IOException{ByteArrayOutputStreamstream = new ByteArrayOutputStream();IOUtils.copy(in, stream);byte[] out = stream.toByteArray();if (all) assertEquals(cmp.length, out.length);for (int i= 0; i< cmp.length; i++) assertEquals(cmp[i], out[i]);}Predict:  0 private void checkInputStream(InputStreamin, byte[] cmp, booleanall) throws IOException{ByteArrayOutputStreamstream = new ByteArrayOutputStream();IOUtils.copy(in, stream);byte[] out=stream.toByteArray();if (all) assertEquals(cmp.length, out.length);for (int i= 0; i< cmp.length; i++) assertEquals(cmp[i], out[i]);}Predict:  1 private void checkInputStream(InputStreamin, byte[] cmp, booleanall) throws IOException{ByteArrayOutputStreamstream= new ByteArrayOutputStream();IOUtils.copy(in, stream);byte[] out = stream.toByteArray();if(all) assertEquals(cmp.length, out.length);for (int i= 0; i< cmp.length; i++) assertEquals(cmp[i], out[i]);}Predict:  1 private void checkInputStream(InputStreamin, byte[] cmp, booleanall) throws IOException{ByteArrayOutputStreamstream =new ByteArrayOutputStream();IOUtils.copy(in, stream);byte[] out =stream.toByteArray();if (all) assertEquals(cmp.length, out.length);for(int i=0; i<cmp.length; i++) assertEquals(cmp[i], out[i]);}Predict:  1 private void checkInputStream(InputStreamin, byte[] cmp, booleanall) throws IOException{ByteArrayOutputStreamstream = new ByteArrayOutputStream();IOUtils.copy(in, stream);byte[] out = stream.toByteArray();if (all) assertEquals(cmp.length, out.length);for (int i= 0; i< cmp.length; i++) assertEquals(cmp[i], out[i]);}Predict:  1 A Comparison of Code Embeddings and Beyond

•

29

(a) Code-1 in clone pair

(b) Attribution score on LSTM

(c) Attribution score on Transformer

(d) Attribution score on TBCNN

(e) Attribution score on AutoenCODE

(f) Attribution score on code2vec

(g) Attribution score on code2seq

(h) Attribution score on GGNN

(i) Attribution score on ASTNN

Fig. 13. Attribution analysis on code-2 in Type-4 clone pairs.

public static void main(String[] args) throws FileNotFoundException{if (args.length< 2) throw new IllegalArgumentException();String fnOut= args[args.length-1];PrintWriterwriter = new PrintWriter(fnOut);for (int i= 0; i< args.length-1; i++) {File fInput= new File(args[i]);Scanner in = new Scanner(fInput);while (in.hasNext()) {  writer.println(in.nextLine()); }} writer.close();}@OverridepublicStringreadFixString(finalintlen) {if(len<1) {  returnStringUtils.EMPTY;  }finalStringWritersw= new StringWriter();try {IOUtils.copy(createLimitedInputStream(len), sw, null);} catch (IOExceptione) {  throw createRuntimeException(e); }return sw.toString();}Predict:  1  @OverridepublicString readFixString(final intlen) {if (len< 1) {  return StringUtils.EMPTY;  }final StringWritersw= newStringWriter();try{IOUtils.copy(createLimitedInputStream(len), sw, null);} catch (IOExceptione) {  throw createRuntimeException(e); }return sw.toString();}Predict:  1  @Overridepublic StringreadFixString(final intlen) {if (len< 1) {  return StringUtils.EMPTY;  }final StringWritersw= new StringWriter();try {IOUtils.copy(createLimitedInputStream(len), sw, null);} catch (IOExceptione) {  throw createRuntimeException(e); }return sw.toString();}Predict:  1  @OverridepublicStringreadFixString(final int len) {if (len< 1) {  return StringUtils.EMPTY;  }finalStringWritersw= new StringWriter();try {IOUtils.copy(createLimitedInputStream(len), sw, null);} catch (IOExceptione) {  throw createRuntimeException(e); }return sw.toString();}Predict:  0  @Overridepublic String readFixString(final int len) {if (len< 1) {  return StringUtils.EMPTY;  }finalStringWritersw=new StringWriter();try{IOUtils.copy(createLimitedInputStream(len), sw, null);} catch(IOExceptione) {  throw createRuntimeException(e); }return sw.toString();}Predict:  1  @Overridepublic StringreadFixString(finalintlen) {if(len< 1) {  return StringUtils.EMPTY;  }finalStringWritersw= newStringWriter();try {IOUtils.copy(createLimitedInputStream(len), sw, null);} catch (IOExceptione) {  throw createRuntimeException(e); }return sw.toString();}Predict:  1  @Overridepublic StringreadFixString(final int len) {if (len<1) {  return StringUtils.EMPTY;  }final StringWritersw=newStringWriter();try {IOUtils.copy(createLimitedInputStream(len), sw, null);} catch (IOExceptione) {  throwcreateRuntimeException(e); }return sw.toString();}Predict:  1 @Overridepublic String readFixString(final int len){if (len< 1) {  return StringUtils.EMPTY;  }final StringWritersw= new StringWriter();try {IOUtils.copy(createLimitedInputStream(len),sw, null);} catch (IOExceptione) {  throw createRuntimeException(e); }return sw.toString();}Predict:  1  30

• Siqi Han, DongXia Wang, Wanting Li, and Xuesong Lu

(a) Attribution score on LSTM

(b) Attribution score on Transformer

(c) Attribution score on TBCNN

(d) Attribution score on AutoenCODE

(e) Attribution score on code2vec

(f) Attribution score on code2seq

(g) Attribution score on GGNN

(h) Attribution score on ASTNN

Fig. 14. The example of code search. The query is "copy array a to array result as integers, the values from the arrays a are
converted to integer if needed and then converted to the type of result if needed".

public static void copyInt(Arrayresult, Arraya) throwsIllegalArgumentException{if(!conformable(a, result))throw new IllegalArgumentException("copy arrays are not conformable");IndexIteratoriterA= a.getIndexIterator();IndexIteratoriterR=result.getIndexIterator();while (iterA.hasNext())iterR.setIntNext(iterA.getIntNext());}Search Result: 0 public static void copyInt(Array result, Array a) throws IllegalArgumentException{if(!conformable(a, result))throw new IllegalArgumentException("copy arraysare not conformable");IndexIteratoriterA= a.getIndexIterator();IndexIteratoriterR=result.getIndexIterator();while (iterA.hasNext())iterR.setIntNext(iterA.getIntNext());}Search Result: 0 publicstaticvoid copyInt(Array result, Array a) throws IllegalArgumentException{if(!conformable(a, result))thrownew IllegalArgumentException("copy arrays are not conformable");IndexIteratoriterA= a.getIndexIterator();IndexIteratoriterR= result.getIndexIterator();while(iterA.hasNext())iterR.setIntNext(iterA.getIntNext());}Search Result: 0  public staticvoid copyInt(Arrayresult, Arraya) throws IllegalArgumentException{if (!conformable(a, result))throw new IllegalArgumentException("copy arrays are not conformable");IndexIteratoriterA= a.getIndexIterator();IndexIteratoriterR= result.getIndexIterator();while (iterA.hasNext())iterR.setIntNext(iterA.getIntNext());}Search Result:  0public staticvoidcopyInt(Arrayresult, Arraya) throws IllegalArgumentException{if (!conformable(a, result))throw new IllegalArgumentException("copy arrays are not conformable");IndexIteratoriterA= a.getIndexIterator();IndexIteratoriterR= result.getIndexIterator();while(iterA.hasNext())iterR.setIntNext(iterA.getIntNext());}Search Result:  0  public staticvoidcopyInt(Arrayresult, Arraya) throws IllegalArgumentException{if (!conformable(a, result))throw new IllegalArgumentException("copy arrays are not conformable");IndexIteratoriterA= a.getIndexIterator();IndexIteratoriterR=result.getIndexIterator();while (iterA.hasNext())iterR.setIntNext(iterA.getIntNext());}Search Result:  1  public static voidcopyInt(Arrayresult, Arraya) throws IllegalArgumentException{if(!conformable(a, result))thrownewIllegalArgumentException("copy arrays are not conformable");IndexIteratoriterA= a.getIndexIterator();IndexIteratoriterR= result.getIndexIterator();while (iterA.hasNext())iterR.setIntNext(iterA.getIntNext());}Search Result: 0 public static void copyInt(Array result, Array a)throws IllegalArgumentException{if (!conformable(a, result))throw new IllegalArgumentException("copy arrays are not conformable");IndexIteratoriterA= a.getIndexIterator();IndexIteratoriterR= result.getIndexIterator();while (iterA.hasNext())iterR.setIntNext(iterA.getIntNext());}Search Result:  0  A Comparison of Code Embeddings and Beyond

•

31

(a) Attribution score on LSTM

(b) Attribution score on Transformer

(c) Attribution score on TBCNN

(d) Attribution score on AutoenCODE

(e) Attribution score on code2vec

(f) Attribution score on code2seq

(g) Attribution score on GGNN

(h) Attribution score on ASTNN

Fig. 15. The example of code search. The query is "compares two search nodes by their path cost".

public int compare(SearchNodeobject1, SearchNodeobject2){float cost1 =object1.getPathCost();float cost2 = object2.getPathCost();return(cost1 > cost2) ? 1: ((cost1< cost2) ? -1: 0);}Search Result:  1  publicintcompare(SearchNodeobject1, SearchNodeobject2){floatcost1=object1.getPathCost();floatcost2=object2.getPathCost();return (cost1 > cost2) ? 1: ((cost1 < cost2) ? -1: 0);}Search Result:  1  publicintcompare(SearchNodeobject1, SearchNodeobject2){floatcost1 = object1.getPathCost();floatcost2 = object2.getPathCost();return(cost1 >cost2) ?1:((cost1 <cost2) ?-1:0);}Search Result: 0publicintcompare(SearchNodeobject1, SearchNodeobject2){float cost1 = object1.getPathCost();float cost2 = object2.getPathCost();return (cost1 > cost2) ? 1: ((cost1<cost2) ? -1: 0);}Search Result:  0  public int compare(SearchNodeobject1, SearchNodeobject2){floatcost1= object1.getPathCost();floatcost2= object2.getPathCost();return(cost1 > cost2) ?1 : ((cost1 < cost2) ?-1 : 0);}Search Result:  0  public int compare(SearchNodeobject1, SearchNodeobject2){float cost1 =object1.getPathCost();float cost2 =object2.getPathCost();return (cost1>cost2) ?1 : ((cost1<cost2) ?-1 : 0);}Search Result:  1  public int compare(SearchNodeobject1, SearchNodeobject2){floatcost1 = object1.getPathCost();floatcost2= object2.getPathCost();return(cost1 > cost2) ?1 :((cost1 < cost2) ?-1 :0);}Search Result:  0  public int compare(SearchNodeobject1,SearchNodeobject2){float cost1 = object1.getPathCost();float cost2 = object2.getPathCost();return (cost1 > cost2)? 1 : ((cost1 < cost2)? -1 : 0);}Search Result:  0  