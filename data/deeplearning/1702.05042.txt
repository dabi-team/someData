7
1
0
2

b
e
F
6
1

]

R

I
.
s
c
[

1
v
2
4
0
5
0
.
2
0
7
1
:
v
i
X
r
a

Luandri: a Clean Lua Interface to the Indri Search Engine

Bhaskar Mitra∗
Microsoft, University College London
Cambridge, UK
bmitra@microsoft.com

Fernando Diaz
Microsoft
New York, USA
fdiaz@microsoft.com

Nick Craswell
Microsoft
Bellevue, USA
nickcr@microsoft.com

ABSTRACT
In recent years, the information retrieval (IR) community has wit-
nessed the ﬁrst successful applications of deep neural network mod-
els to short-text matching and ad-hoc retrieval. It is exciting to see
the research on deep neural networks and IR converge on these
tasks of shared interest. However, the two communities have less
in common when it comes to the choice of programming languages.
Indri, an indexing framework popularly used by the IR community,
is written in C++, while Torch, a popular machine learning library
for deep learning, is written in the light-weight scripting language
Lua. To bridge this gap, we introduce Luandri (pronounced “laun-
dry”), a simple interface for exposing the search capabilities of In-
dri to Torch models implemented in Lua.

CCS CONCEPTS
•Information systems → Information retrieval; Web search-
ing and information discovery; •Computing methodologies
→ Neural networks;

KEYWORDS
Information retrieval, application programming interface, neural
networks

ACM Reference format:
Bhaskar Mitra, Fernando Diaz, and Nick Craswell. 2017. Luandri: a Clean
Lua Interface to the Indri Search Engine. In Proceedings of ACM SIGIR con-
ference, Tokyo, Japan, August 2017 (Under review for SIGIR’17), 3 pages.
DOI: 10.475/123 4

1 INTRODUCTION
In recent years, deep neural networks (DNNs) have demonstrated
early positive results on a variety of standard information retrieval
(IR) tasks, including on short-text matching [10, 11, 14, 19, 21, 22]
and ad-hoc retrieval [9, 17], and shown promising performances
on exciting novel retrieval tasks such as multi-modal retrieval [15]
and conversational IR [28, 30]. However, the two research commu-
nities focused on neural networks and on IR may have less in com-
mon when it comes to the choice of programming languages for
implementing their respective toolsets. Popular neural network

∗The author is a part-time PhD student at UCL.

Permission to make digital or hard copies of part or all of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for proﬁt or commercial advantage and that copies bear this notice and the full cita-
tion on the ﬁrst page. Copyrights for third-party components of this work must be
honored. For all other uses, contact the owner/author(s).
Under review for SIGIR’17, Tokyo, Japan
© 2017 Copyright held by the owner/author(s). 123-4567-24-567/08/06. . . $15.00
DOI: 10.475/123 4

toolkits are often implemented in (or have bindings for) script-
ing languages, such as Python1 (e.g., TensorFlow [1], Theano [2],
CNTK [29], Caﬀe [13], MXNet [3], Chainer [25], and PyTorch2) or
Lua [12] (e.g., Torch [4]) because of their rapid prototyping capa-
bilities. In contrast, many of the popular indexing frameworks for
IR are implemented in C++ (e.g., Indri [24]) or Java (e.g., Terrier
[18] and Apache Lucene [16]) putting more emphasis on the speed
of execution at runtime. The open-source community has devel-
oped Python wrappers over the Indri [26] and the Apache Lucene
[20] programming interfaces to expose the functionalities of these
rich IR libraries to the programming language. However, there is
still a gap that remains to be bridged for non-Python based deep
learning toolkits, such as Torch.

Torch3 is a numeric computing framework popular among the
deep neural network community. It has been shown to be signif-
icantly faster compared to other toolkits such as TensorFlow on
convolutional neural networks in multi-GPU environment [23]. It
is implemented using the light-weight scripting language Lua.4 In
this paper, we introduce Luandri (pronounced “laundry”) – a Lua
In particular, Luandri ex-
wrapper over the Indri search engine.
poses parts of the Indri query environment application program-
ming interface (API) for document retrieval including support for
the rich Indri query language.

2 MOTIVATION
There are a variety of scenarios in which a DNN model can beneﬁt
from having access to a search engine during training and/or eval-
uation. Existing DNN models for ad-hoc retrieval [9, 17], for ex-
ample, operate on query-document pairs to predict relevance. Run-
ning these models on the full corpus is prohibitively costly – there-
fore the evaluation of these models is often limited to re-ranking
top-N candidate documents retrieved by a traditional IR model or a
search engine. Typically, these candidate sets are retrieved oﬄine
in a process separate from the one in which the DNN is evaluated.
However, if the search engine is accessible in the same language as
the one in which the DNN is implemented, then the candidate gen-
eration step and the DNN-based re-ranking step can follow each
other within the same process – removing the requirement to store
large quantity of intermediate datasets containing the candidates
to be ranked.

DNN models train on labelled data, although in some cases la-
bels can be inferred rather than explicit. For example, many DNN
models for IR [9–11, 14, 22] use negative training examples that
are sampled uniformly from the corpus. Recently Mitra et al. [17]
reported that training with judged negative documents can yield

1https://www.python.org/
2https://github.com/pytorch/pytorch
3https://github.com/torch/torch7
4https://www.lua.org

 
 
 
 
 
 
Under review for SIGIR’17, August 2017, Tokyo, Japan

Bhaskar Mitra, Fernando Diaz, and Nick Craswell

Code snippet 1: Sample Lua code for searching an Indri index using the Luandri API. The Indri index builder application
is used for generating the index beforehand. The search query is written using the popular INQUERY structured operators
that are supported natively by Indri for specifying matching constraints. The runQuery method in the Luandri API accepts
the request as a Lua table and automatically converts it into the appropriate C++ request object that Indri natively expects.
Similarly, the result object returned by Indri in C++ is automatically converted to a Lua table.

1
2
3
4
5
6
7
8
9
10
11
12
13

local luandri = paths.dofile ( ' luandri.lua ')
local query_environme nt = QueryEnvironment ()
query_environme nt : addIndex ( " path_to_index_fi le " )

local request = {

query = '# syn ( # od1 ( neural networks ) # od1 ( deep learning )) # greater ( year 2009) ' ,
resultsRequested = 10
}

local results = query_environment : runQuery ( request ) .results

for k , v in pairs ( results ) do

print ( v.docid .. '\ n ' .. v.documentName .. '\ n ' .. v.snippet .. '\ n ')

end

better NDCG performance than training with uniform random neg-
atives. Having access to a search engine during training could
enable additional methods for generating negative samples, such
as using documents that are retrieved by the engine but at lower
ranks.

The lack of adequate labelled data available for training DNN
models for ad-hoc retrieval has been a focus for the neural IR com-
munity [5]. It is possible that alternate strategies for supervision
may be considered for training these deep models – including rein-
forcement learning [27] and training under adversarial settings [8]
– which could also make use of retrieval from a full corpus during
the model training.

Diaz et al. [6] demonstrated a diﬀerent application of the tra-
ditional retrieval step in the neural IR model. Given a query, they
retrieve a set of documents using Indri and use that to train a brand
new distributed representation of words speciﬁc to that query at
run time. Such models, with query-speciﬁc representation learn-
ing, can be implemented and deployed more easily if the machine
learning framework has access to a search engine.

Finally, Ghazvininejad et al. [7] proposed to “lookup” external
repositories of facts as part of solving larger tasks using neural
network models. Empowering DNN models with access to a search
engine may be an exciting area for future exploration.

In all these scenarios, it is useful for a search engine, such as
Indri, to be accessible from the same programming language used
to implement the DNN. Therefore, we are optimistic that by pub-
licly releasing the Luandri API we will stimulate novel explorations
from IR researchers already familiar with Torch.

3 QUERYING INDRI FROM LUA
Indri is an open-source search engine available with the Lemur
toolkit.5 Indri consists of two primary components – an applica-
tion that builds an index from a raw document collection and an-
other application that can perform searches using this index. The
Indri index builder can deal with several diﬀerent document for-
mats for indexing. This includes TREC (text and Web), HTML,
XML, PDF, and plain text among many others.

Searching using Indri involves specifying one or more indices
and querying them by either interactively calling the API or by run-
ning an application in batch-mode. The Indri query language sup-
ports a rich set of operators for specifying phrasal matching con-
ditions, synonymy relationships, document ﬁltering criteria, and
other complex constraints. The full query language grammar is
available online for reference.6

Invoking a search on an Indri index using the Luandri API is
very similar to how one may use the native C++ Indri API. Code
snippet 1 shows a minimal example of a typical Indri-based search
using the Luandri API. We observe that the search is performed by
invoking very few lines of Lua code.

The example also demonstrates the use of Indri structured queries.

A search is performed using a structured query that constraints the
matching to either of the two ordered phrases – “neural networks”
or “deep learning”. The query directs Indri to treat both phrases as
synonyms. In addition, a numeric ﬁlter is speciﬁed to limit matches
to only documents whose value corresponding to the year ﬁeld is
greater than 2009.

This example shows searching on the full document index. How-
ever, Luandri also allows users to specify a list of document iden-
tiﬁers in the request object to limit the search to only those set

5http://www.lemurproject.org/indri/
6https://www.lemurproject.org/lemur/IndriQueryLanguage.php

Luandri: a Clean Lua Interface to the Indri Search Engine

Under review for SIGIR’17, August 2017, Tokyo, Japan

of documents. A ﬁxed list of stop words can also be speciﬁed for
retrieval using the Luandri API.

The full Luandri implementation is available on GitHub7 under
the MIT license. We direct interested readers to the source code
for exact API speciﬁcations.

4 UNDER THE HOOD
The implementation of Lua as a programming language puts a
strong emphasis on extensibility [12]. Lua is an extension language
because any Lua code can be relatively easily embedded as libraries
into code written in other languages. It is also an extensible lan-
guage because of its ability to call functions written in other lan-
guages, such as C. The implementation of the Luandri API beneﬁts
from the latter property of the language.

Lua comes with a fast Just In Time (JIT) compiler called LuaJIT.8
LuaJIT exposes a foreign-function interface9 (FFI) that makes it
easy to call external C functions and manipulate C data structures
from Lua. The Luandri API is written using the LuaJIT FFI library.
Luandri API wraps Indri’s query environment data types and
methods by extern C functions. Then using the LuaJIT’s FFI library
these C methods are exposed to any code written in Lua. Luandri
automatically handles any conversions necessary between Lua ta-
bles and Indri’s C++ objects, and vice versa. The “Luandri.cpp” and
“luandri.lua” ﬁles contain all the wrapper logic on the C++ and the
Lua side of our API code, respectively.

The current Luandri API exposes only some of the data struc-
tures and methods from Indri’s query environment. In future, we
hope to expose more of Indri’s retrieval functionalities prioritizing
based on the need of the broader research community.

5 CONCLUSIONS
We introduced Luandri, a Lua API to the Indri search engine. Lu-
andri brings to DNN models, implemented on Torch, the retrieval
capabilities of Indri, including its powerful query language gram-
mar. We posit that the capabilities of a search engine may be use-
ful for training future DNN models for IR – for sampling negative
examples, or for training under reinforcement or adversarial set-
tings. We hope that the release of Luandri will not only help re-
searchers working on Torch models for IR, but also stimulate new
research in novel DNN models that incorporate retrieval from an
external knowledge base as an intermediate step towards solving
larger tasks.

REFERENCES
[1] Mart´ın Abadi, Ashish Agarwal, Paul Barham, Eugene Brevdo, Zhifeng Chen,
Craig Citro, Greg S Corrado, Andy Davis, Jeﬀrey Dean, Matthieu Devin, and
others. 2016. Tensorﬂow: Large-scale machine learning on heterogeneous dis-
tributed systems. arXiv preprint arXiv:1603.04467 (2016).

[2] James Bergstra, Olivier Breuleux, Fr´ed´eric Bastien, Pascal Lamblin, Razvan Pas-
canu, Guillaume Desjardins, Joseph Turian, David Warde-Farley, and Yoshua
Bengio. 2010. Theano: A CPU and GPU math compiler in Python. In Proc. 9th
Python in Science Conf. 1–7.

[3] Tianqi Chen, Mu Li, Yutian Li, Min Lin, Naiyan Wang, Minjie Wang, Tianjun
Xiao, Bing Xu, Chiyuan Zhang, and Zheng Zhang. 2015. Mxnet: A ﬂexible and
eﬃcient machine learning library for heterogeneous distributed systems. arXiv
preprint arXiv:1512.01274 (2015).

7https://github.com/bmitra-msft/Luandri
8http://luajit.org
9http://luajit.org/ext ﬃ.html

[4] Ronan Collobert, Koray Kavukcuoglu, and Cl´ement Farabet. 2011. Torch7: A
matlab-like environment for machine learning. In BigLearn, NIPS Workshop.
[5] Nick Craswell, W Bruce Croft, Jiafeng Guo, Bhaskar Mitra, and Maarten de Rijke.
2016. Report on the SIGIR 2016 Workshop on Neural Information Retrieval (Neu-
IR). 50, 2 (2016), 96–103.

[6] Fernando Diaz, Bhaskar Mitra, and Nick Craswell. 2016. Query Expansion with
Locally-Trained Word Embeddings. arXiv preprint arXiv:1605.07891 (2016).
[7] Marjan Ghazvininejad, Chris Brockett, Ming-Wei Chang, Bill Dolan, Jianfeng
Gao, Wen-tau Yih, and Michel Galley. 2017. A Knowledge-Grounded Neural
Conversation Model. arXiv preprint arXiv:1702.01932 (2017).

[8] Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-
Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio. 2014. Generative ad-
versarial nets. In Advances in neural information processing systems. 2672–2680.
[9] Jiafeng Guo, Yixing Fan, Qingyao Ai, and W Bruce Croft. 2016. A Deep Rele-
vance Matching Model for Ad-hoc Retrieval. In Proc. CIKM. ACM, 55–64.
[10] Baotian Hu, Zhengdong Lu, Hang Li, and Qingcai Chen. 2014. Convolutional
neural network architectures for matching natural language sentences. In Proc.
NIPS. 2042–2050.

[11] Po-Sen Huang, Xiaodong He, Jianfeng Gao, Li Deng, Alex Acero, and Larry
Heck. 2013. Learning deep structured semantic models for web search using
clickthrough data. In Proc. CIKM. ACM, 2333–2338.

[12] Roberto Ierusalimschy, Luiz Henrique De Figueiredo, and Waldemar Celes Filho.
1996. Lua-an extensible extension language. Softw., Pract. Exper. 26, 6 (1996),
635–652.

[13] Yangqing Jia, Evan Shelhamer, Jeﬀ Donahue, Sergey Karayev, Jonathan Long,
Ross Girshick, Sergio Guadarrama, and Trevor Darrell. 2014. Caﬀe: Convolu-
tional architecture for fast feature embedding. In Proceedings of the 22nd ACM
international conference on Multimedia. ACM, 675–678.

[14] Zhengdong Lu and Hang Li. 2013. A deep architecture for matching short texts.

In Advances in Neural Information Processing Systems. 1367–1375.

[15] Lin Ma, Zhengdong Lu, Lifeng Shang, and Hang Li. 2015. Multimodal convolu-
tional neural networks for matching image and sentence. In Proceedings of the
IEEE International Conference on Computer Vision. 2623–2631.

[16] Michael McCandless, Erik Hatcher, and Otis Gospodnetic. 2010. Lucene in Ac-

tion: Covers Apache Lucene 3.0. Manning Publications Co.

[17] Bhaskar Mitra, Fernando Diaz, and Nick Craswell. 2016. Learning to Match
Using Local and Distributed Representations of Text for Web Search. arXiv
preprint arXiv:1610.08136 (2016).

[18] Iadh Ounis, Gianni Amati, Vassilis Plachouras, Ben He, Craig Macdonald, and
Christina Lioma. 2006. Terrier: A high performance and scalable information
retrieval platform. In Proceedings of the OSIR Workshop. 18–25.

[19] Liang Pang, Yanyan Lan, Jiafeng Guo, Jun Xu, Shengxian Wan, and Xueqi Cheng.

2016. Text Matching as Image Recognition. In Proc. AAAI.
[20] Andreas Schreiber. 2009. Mixing Python and Java. (2009).
[21] Aliaksei Severyn and Alessandro Moschitti. 2015. Learning to rank short text
pairs with convolutional deep neural networks. In Proc. SIGIR. ACM, 373–382.

[22] Yelong Shen, Xiaodong He, Jianfeng Gao, Li Deng, and Gregoire Mesnil. 2014.
A latent semantic model with convolutional-pooling structure for information
retrieval. In Proc. CIKM. ACM, 101–110.

[23] Shaohuai Shi, Qiang Wang, Pengfei Xu, and Xiaowen Chu. 2016. Benchmarking
State-of-the-Art Deep Learning Software Tools. arXiv preprint arXiv:1608.07249
(2016).

[24] Trevor Strohman, Donald Metzler, Howard Turtle, and W Bruce Croft. 2005. In-
dri: A language model-based search engine for complex queries. In Proceedings
of the International Conference on Intelligent Analysis, Vol. 2. Citeseer, 2–6.
[25] Seiya Tokui, Kenta Oono, Shohei Hido, and Justin Clayton. 2015. Chainer: a
next-generation open source framework for deep learning. In Proceedings of
workshop on machine learning systems (LearningSys) in the twenty-ninth annual
conference on neural information processing systems (NIPS).

[26] Christophe Van Gysel, Evangelos Kanoulas, and Maarten de Rijke. 2017. Pyndri:
a Python Interface to the Indri Search Engine. arXiv preprint arXiv:1701.00749
(2017).

[27] Ronald J Williams. 1992. Simple statistical gradient-following algorithms for
connectionist reinforcement learning. Machine learning 8, 3-4 (1992), 229–256.
[28] Rui Yan, Yiping Song, and Hua Wu. 2016. Learning to respond with deep neural
networks for retrieval-based human-computer conversation system. In Proceed-
ings of the 39th International ACM SIGIR conference on Research and Development
in Information Retrieval. ACM, 55–64.

[29] Dong Yu, Adam Eversole, Mike Seltzer, Kaisheng Yao, Zhiheng Huang, Brian
Guenter, Oleksii Kuchaiev, Yu Zhang, Frank Seide, Huaming Wang, and oth-
ers. 2014. An introduction to computational networks and the computational
network toolkit. Technical Report. Tech. Rep. MSR, Microsoft Research, 2014,
http://codebox/cntk.

[30] Xiangyang Zhou, Daxiang Dong, Hua Wu, Shiqi Zhao, R Yan, D Yu, Xuan Liu,
and H Tian. 2016. Multi-view response selection for human-computer conver-
sation. EMNLPﬁ16 (2016).

