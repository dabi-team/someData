How Robust are Reconstruction Thresholds for Community
Detection?

Ankur Moitra∗

1,2, William Perry†

1, and Alexander S. Wein‡

1

1Massachusetts Institute of Technology, Department of Mathematics
2Massachusetts Institute of Technology, Computer Science and Artiﬁcial Intelligence Lab

March 23, 2016

Abstract

The stochastic block model is one of the oldest and most ubiquitous models for studying
clustering and community detection. In an exciting sequence of developments, motivated by
deep but non-rigorous ideas from statistical physics, Decelle et al. [DKMZ11] conjectured a
sharp threshold for when community detection is possible in the sparse regime. Mossel, Neeman
and Sly [MNS14b] and Massouli´e [Mas14] proved the conjecture and gave matching algorithms
and lower bounds.

Here we revisit the stochastic block model from the perspective of semirandom models where
we allow an adversary to make ‘helpful’ changes that strengthen ties within each community
and break ties between them. We show a surprising result that these ‘helpful’ changes can shift
the information-theoretic threshold, making the community detection problem strictly harder.
We complement this by showing that an algorithm based on semideﬁnite programming (which
was known to get close to the threshold) continues to work in the semirandom model (even for
partial recovery). This suggests that algorithms based on semideﬁnite programming are robust
in ways that any algorithm meeting the information-theoretic threshold cannot be.

These results point to an interesting new direction: Can we ﬁnd robust, semirandom ana-
logues to some of the classical, average-case thresholds in statistics? We also explore this question
in the broadcast tree model, and we show that the viewpoint of semirandom models can help
explain why some algorithms are preferred to others in practice, in spite of the gaps in their
statistical performance on random models.

6
1
0
2

r
a

M
1
2

]
S
D
.
s
c
[

2
v
3
7
4
1
0
.
1
1
5
1
:
v
i
X
r
a

∗Email: moitra@mit.edu. This work is supported in part by NSF CAREER Award CCF-1453261, a grant from

the MIT NEC Corporation and a Google Faculty Research Award.

†Email: wperry@mit.edu.
‡Email: awein@mit.edu. This work is supported in part by an NDSEG graduate fellowship.

 
 
 
 
 
 
1

Introduction

1.1 Background

The stochastic block model is one of the oldest and most ubiquitous models for studying clustering
and community detection.
It was ﬁrst introduced by Holland et al. [HLL83] more than thirty
years ago and since then it has received considerable attention within statistics, computer science,
statistical physics and information theory. The model deﬁnes a procedure to generate a random
graph: ﬁrst, each of the n nodes is independently assigned to one of r communities, where pi is
the probability of being assigned to community i. Next, edges are sampled independently based on
the community assignments: if nodes u and v belong to communities i and j respectively, the edge
(u, v) occurs with probability Qij independent of all other edges, where Q is an r
r symmetric
matrix. The goal is to recover the community structure — either exactly or approximately — from
the graph.

×

Since its introduction, the stochastic block model has served as a testbed for the diverse range
of algorithms that have been developed for clustering and community detection, including combina-
torial methods [BCLS87], spectral methods [McS01], MCMC [JS98], semideﬁnite programs [Bop87]
and belief propagation [MNS14a]. Recently, the stochastic block model has been thrust back into
the spotlight, the goal being to establish tight thresholds for when community detection is possible,
and to ﬁnd algorithms that achieve them. Towards this end, Decelle et al. [DKMZ11] made some
fascinating conjectures (which have since been resolved) in the case of two equal-sized communities
with constant average degree, that we describe next.

Throughout this paper, we will focus on the two-community case and use a/n and b/n to denote
the within-community and between-community connection probabilities respectively. We will be
interested in the sparse setting where a, b = Θ(1). Moreover we set p1 = p2 = 1/2, so that the two
communities are roughly equal-sized, and we assume a > b (although we discuss the case a < b
in Appendix D). We will use G(n, a/n, b/n) to denote the corresponding random graph model.
The setting of parameters above is particularly well-motivated in practice, where a wide variety of
networks have been observed [LLDM08] to have average degree that is bounded by a small constant.
It is important to point out that when the average degree is constant, it is impossible to recover
the community structure exactly in the stochastic block model because a constant fraction of nodes
will be isolated. Instead, our goal is to recover a partition of the nodes into two communities that
has non-trivial agreement (better than random guessing) with the true communities as n
,
and we refer to this as partial recovery.

→ ∞

Conjecture 1.1. [DKMZ11] If (a
and if (a

−

−

b)2 < 2(a + b) then partial recovery is information-theoretically impossible.

b)2 > 2(a+b) then partial recovery in G(n, a/n, b/n) is possible,

This conjecture was based on deep but non-rigorous ideas originating from statistical physics, and
was ﬁrst derived heuristically as a stability criterion for belief propagation. This threshold also
bears a close connection to known thresholds in the broadcast tree model, which is another setting
in which to study partial recovery. We formally deﬁne the broadcast tree model in Section 2.1, but
it is a stochastic process described by two parameters a and b. Kesten and Stigum [KS66] showed
b)2 > 2(a + b), and much later Evans et al. [EKPS00] showed
that partial recovery is possible if (a
that it is impossible if (a
2(a + b). The connection between the two models is that in the
stochastic block model, the local neighborhood of a node resembles the broadcast tree model, and
this was another compelling motivation for the conjecture.

b)2

−

−

≤

In an exciting sequence of developments, Mossel, Neeman and Sly [MNS14b] proved a lower
bound that even distinguishing G(n, a/n, b/n) from an Erd˝os–R´enyi graph G(n, (a + b)/2n) is

1

2(a + b), by a careful coupling to the broadcast
information-theoretically impossible if (a
≤
tree model. Subsequently Mossel, Neeman and Sly [MNS13] and Massouli´e [Mas14] independently
gave matching algorithms that achieve partial recovery up to this threshold, thus resolving the
conjecture! Mossel, Neeman and Sly [MNS14a] later showed that for some constant C > 0, if
b)2 > C(a + b) then belief propagation works and moreover the agreement of the clustering it
(a
ﬁnds is the best possible.

−

−

b)2

In fact, many other sorts of threshold phenomena have been found in diﬀerent parameter
regimes. Abbe, Bandeira and Hall [ABH15] studied exact recovery in the logarithmic degree setting
G(n, a log n/n, b log n/n) and showed that it is eﬃciently possible to recover the two communities
√b)2 < 2. Abbe and
exactly if (√a
Sandon [AS15] gave a precise characterization of when exact recovery is possible in the general
stochastic block model for more than two communities with arbitrary relative sizes and arbitrary
connection probabilities.

√b)2 > 2 and information-theoretically impossible if (√a

−

−

1.2 Semirandom Models

This abundance of sharp thresholds begs a natural question: how robust are these reconstruction
thresholds? It is clear that if one substantially changes the distributional model, the thresholds
themselves are likely to change. However there is a subtler issue. The algorithms that achieve these
thresholds may in principle be over-ﬁtting to a particular distributional model. Random graphs
are well-known [AS00] to have rigid properties, such as sharp laws for the distribution of subgraph
counts and a predictable distribution of eigenvalues. Real-world graphs do not have such properties.
In a remarkable paper, Blum and Spencer [BS95] introduced the semirandom model as an
intermediary between average-case and worst-case analysis, to address such issues. The details
of the model vary depending on the particular optimization problem, and since we will focus on
clustering we will be most interested in the variant used by Feige and Kilian [FK01].

Semirandom Model for Community Detection: [FK01]

Sample a ‘precursor’ graph Gpre from G(n, a/n, b/n).

A monotone ‘adversary’ observes Gpre along with the hidden community structure,
and can delete any number of edges crossing between the two communities, and
add any number of edges with each community.

Output the resulting graph G.

•

•

•

The adversary above is called ‘monotone’ because it is restricted to making changes that seem
to be helpful. It can only strengthen ties within each community, and break ties between them.1
The key is that a monotone adversary can break the sorts of rigid structures that arise in random
graphs, such as predictable degree distributions and subgraph counts. An algorithm that works in
a semirandom model can no longer rely on these properties. Instead of a graph containing only
random edges, all we are assured of is that it contains some random edges.

In this paper, we use semirandom models as our notion of ‘robustness.’ Many forms of robust-
ness exist in the literature – for example, the independent work [MMV15b] gives algorithms that
are robust to o(n) non-monotone changes in addition to monotone changes. With most robustness

1This is the reason we require a > b. If we had b > a we could deﬁne the adversary in the opposite way (which

we analyze in Appendix D).

2

models, any algorithm will break down after enough errors, and one can compare algorithms based
on how many errors they can tolerate. In contrast, semirandom models distinguish between algo-
rithms qualitatively: as we will see, entire classes of algorithms continue to work under any number
of monotone changes, while others do not.

Feige and Kilian [FK01] showed that semideﬁnite programs for exact recovery in the stochastic
block model continue to work in the semirandom model — in fact they succeed up to the threshold
for the random model [HWX15]. Since then, there have been many further developments including
algorithms that work in semirandom models for planted clique [FK00], unique games [KMM11],
various graph partitioning problems [MMV12], correlation clustering [MS10, MMV15a] and the
general planted partition model2 [PW15] (and in some cases for even more powerful adversaries).
A common theme is that if you have a semideﬁnite program that works in some stochastic setting,
then it often extends almost automatically to the semirandom setting. So are there semideﬁnite
programs that achieve the same sharp partial recovery results as Mossel, Neeman and Sly [MNS13]
and Massouli´e [Mas14], and that extend to the semirandom model too? Or is there a genuine gap
between what is achievable in the random vs. the semirandom setting?

1.3 Our Results

Recall that in the semirandom block model, a monotone adversary observes a sample from the
stochastic block model and is then allowed to add edges within each community and delete edges
that cross between communities. We will design a particularly simple adversary to prevent an
algorithm from utilizing the paths of length two that go from a ‘+’ node to a ‘
’ node and back to
a ‘+’ node, where the middle node has degree two. Our adversary will delete any such path it ﬁnds
(with some additional technical conditions to locally coordinate these modiﬁcations), and our main
result is that, surprisingly, this simple modiﬁcation strictly changes the partial recovery threshold.
2 and ‘noise’ ε = b
We will state our bounds in terms of the average degree k = a+b
2 ), in
2ε)2 > 1. Note that this threshold
which case the threshold (a
requires k > 1. Then:

b)2 > 2(a + b) becomes k(1

a+b ∈

[0, 1

−

−

−

Theorem 1.2 (main). For any k > 1, there exists 0 < ε < 1
2ε)2 > 1 and hence
partial recovery in the stochastic block model G(n, a/n, b/n) is possible, and yet there is a monotone
adversary so that partial recovery in the semirandom model is information-theoretically impossible.

2 so that k(1

−

A common tool in the algorithms of Mossel, Neeman and Sly [MNS13] and of Massouli´e [Mas14] is
the use of non-backtracking walks and spectral bounds for their transition matrices. Our adversary
explicitly deletes a signiﬁcant number of these walks. This simple modiﬁcation not only defeats
these particular algorithms, but we can show that no algorithm can achieve partial recovery up to
2ε)2 > 1. To the best of our knowledge, this is the ﬁrst explicit separation
the threshold k(1
between what is possible in the random model vs. the semirandom model, for any problem with a
monotone adversary.

−

We show a complementary result, that no monotone adversary can make the problem too much
harder. Various semideﬁnite programs have been designed for partial recovery. These algorithms
work in the constant-degree regime, but are not known to work all the way down to the information-
theoretic threshold. In particular, we follow Gu´edon and Vershynin [GV15] and show that their
analysis (with a simple modiﬁcation) works as is for the semirandom model. This shows that
semideﬁnite programs not only give rise to algorithms that work for the semirandom model in the
exact recovery setting, but also for partial recovery too under some fairly general conditions.

2In the general planted partition model, we have Qii = a/n for all i and Qij = b/n for all i 6= j, with a > b, but

the number of communities and their relative sizes are arbitrary constants.

3

2 . There is a constant C > 1 so that if a > 20 and
2ε)2 > C then partial recovery is possible in the semirandom block model. Moreover it can be

2 and ε = b

a+b < 1

Theorem 1.3. Let k = a+b
k(1
solved in polynomial time.

−

Our robustness proof only applies to a particular form of SDP analysis. Given a diﬀerent proof
that the SDP succeeds in the random model for a larger range of parameters k, ε than above, there
would be no guarantee that the SDP also succeeds in the semirandom model for that range of
parameters. Hence we cannot formally conclude that it is impossible for the SDP to reach the
information-theoretic threshold in the random model, though our results are suggestive in this
direction.

We remark that each possible monotone adversary yields a new distribution on planted com-
munity detection problems. Hence, we can think of the algorithm in the theorem above as one that
performs almost as well as information-theoretically possible across an entire family of distribu-
tions, simultaneously. This is a major advantage to algorithms based on semideﬁnite programming,
and points to an interesting new direction in statistics. Can we move beyond average-case analy-
sis? Can we ﬁnd robust, semirandom analogues to some of the classical, average-case thresholds in
statistics? The above two theorems establish upper and lower bounds for this semirandom thresh-
old, that show that it is genuinely diﬀerent from the average-case threshold. The usual notion of
a threshold makes sense when we have exact knowledge of the process generating the instances we
wish to solve. But when we lack this knowledge, semirandom models oﬀer an avenue for exploration
that can lead to new algorithmic and statistical questions.

Along the way to proving our main theorem, we show a random vs. semirandom separation
for the broadcast tree model too. We deﬁne this model in Section 2.1. In short, it is a stochastic
process in which each node is given one of two labels and gives birth to Pois(a/2) nodes of the same
label and Pois(b/2) nodes of the diﬀerent label, with the goal being to guess the label of the root
given the labels of the leaves. There are many ways we could deﬁne a monotone adversary, and
we focus on a particularly weak one that is only allowed to cut edges between nodes with diﬀerent
labels. We call this the cutting adversary, and we prove:

Theorem 1.4. For any k > 1, there exists 0 < ε < 1
2ε)2 > 1 and hence partial recov-
ery in the broadcast tree model is possible, and yet there is a monotone cutting adversary for which
partial recovery in the semirandom broadcast tree model is information-theoretically impossible.

2 so that k(1

−

Furthermore we analyze the recursive majority algorithm and show that it is robust to an even
more powerful class of monotone adversaries, which are allowed to entirely control the subtree at a
node whose label is diﬀerent than its parent. We call this a strong adversary, and we prove:

Theorem 1.5. If
is possible, even with respect to a strong monotone adversary, where ‘o(1)’ is taken as k

2 then partial recovery in the broadcast tree model

2ε)2 > 1+o(1) and ε < 1

k
log k (1

−

.

→ ∞

These results highlight another well-studied model where the introduction of a monotone adversary
strictly changes what is possible. Nevertheless there is an algorithm that succeeds across the
entire range of distributions that arise from the action of a monotone adversary, simultaneously.
Interestingly, our robustness results can also be seen as a justiﬁcation for why practitioners use
recursive majority at all. It has been known for some time that recursive majority does not achieve
the Kesten–Stigum bound [Mos98] — the threshold for reconstruction in the broadcast tree model
— although taking the majority vote of the leaves does. The advantage of recursive majority is
that it is robust to very powerful adversaries while majority is not, and this only becomes clear
when studying these algorithms through semirandom models!

4

2 Models and Adversaries

2.1 Preliminaries

Here we formally deﬁne the models we will be interested in, as well as the notion of partial recovery.
Recall that G(n, a/n, b/n) denotes the stochastic block model on two communities with p1 = p2 =
1/2 so that the communities are roughly equal sized. We will encode community membership as a
on each node v. We will also refer to this as a spin, following the convention
label σv ∈ {
in statistical physics. This numeric representation has the advantage that we can ‘add’ spins in
order to take the majority vote, and ‘multiply’ them to compute the relative spin between a pair of
nodes. We will be interested in the sparse regime a, b = Θ(1) where the graph has constant average
degree, and we will assume a > b.

1
}

+1,

−

→ ∞

Next, we formally deﬁne partial recovery in the stochastic block model. Throughout this paper
. We say
o(1) as
. Similarly, we say that an event happens for a.a.e. (asymptotically almost every) x if it

will will be interested in how our algorithms perform as n (number of nodes) goes to
∞
that an event holds a.a.s. (asymptotically almost surely) if it holds with probability 1
n
holds with probability 1

o(1) over a random choice of x.

−
Deﬁnition 2.1. We say that an assignment of
spins to the nodes achieves η-partial
+1,
{
recovery if at least ηn of these spins match the true spins, or at least ηn match after a global ﬂip
n (indexed
of all the spins. Moreover, an algorithm that outputs a vector of spins
1
+1,
}
by nodes) is said to achieve partial recovery and there exists η > 1
σ achieves η-partial
.
recovery a.a.s. in the limit n

2 such that

1
}

∈ {

−

−

−

σ

→ ∞

−

+1,

1
}

Next, we deﬁne the broadcast tree model (which we introduced informally earlier). The broad-
cast tree model is a stochastic process that starts with a single root node ρ at level 0 whose spin
is chosen uniformly at random. Each node in turn gives birth to Pois(a/2) same-
σρ ∈ {
spin children and Pois(b/2) opposite-spin children, where Pois(c) is the Poisson distribution with
expectation c. This process continues until level R at which point it stops, and the nodes at level R
are called the leaves. (The nodes on level
1 that by chance do not give birth to any children
are not considered leaves, even though they are leaves in the graph-theoretic sense.) An algorithm
observes the spins at the leaf nodes and the topology of the tree, and the goal is to recover the root
spin:

≤

−

R

b

b

Deﬁnition 2.2. An algorithm that outputs a spin
if there exists η > 1
2 such that

σρ = σρ with probability at least η, as R
b
The reparameterization in terms of (k, ε) becomes particularly convenient here: each node gives
is the average branching factor. Moreover, each child has

→ ∞

b

.

σρ is said to achieve partial recovery on the tree

birth to Pois(k) children, so k = a+b
2
probability ε = b

a+b of having spin opposite to that of its parent.

−

2ε)2 > 1 [KS66] and that for k(1

It is known that taking the majority vote of the leaves is optimal in theory, in the sense that
it achieves partial recovery for k(1
1, partial recovery
is information-theoretically impossible [EKPS00]. This is called the Kesten–Stigum bound, and it
can also be interpreted as a condition on the second-largest eigenvalue of an appropriately deﬁned
transition matrix. There are many other natural variants of the broadcast tree model, that are
more general instances of multi-type branching processes. However, even for simple extensions,
In the Potts model, where nodes
the precise information-theoretic threshold is still unknown.
are labeled with one of q labels, Sly [Sly11] showed that the Kesten–Stigum bound is not tight, as
predicted by M´ezard and Montanari [MM05]. And for an asymmetric extension of the binary model
above, Borgs et al. [BCMR06] showed that the Kesten–Stigum bound is tight for some settings of

2ε)2

−

≤

5

the parameters. In our setting, this historical context presents a substantial complication because
if we apply a monotone adversary to a broadcast tree model and it results in a complex propagation
rule, there may not be good tools to prove that partial recovery is impossible.

2.2 Discussion

There is a close connection between the stochastic block model and the broadcast tree model, since
the local neighborhood of any vertex in the graph is locally tree-like. Hence, if our goal is to prove a
random vs. semirandom gap for community detection, a natural starting point is to establish such
a gap for the broadcast tree model. It turns out that there is more than one natural way to deﬁne a
monotone adversary for the broadcast tree model, but it will not be too diﬃcult to establish lower
bounds for either of them. The more diﬃcult task is in ﬁnding an adversary that can plausibly be
coupled to a corresponding adversary in the stochastic block model, and this will require us to put
many sorts of constraints on the type of adversary that we should use to obtain a separation for
the broadcast tree model.

In the broadcast tree model, we will work with two notions of a monotone adversary. One is

weak and will be used to show our separation results:

Cutting Semirandom Model for Broadcast Tree:

Sample a ‘precursor’ broadcast tree Tpre from the broadcast tree model.

The monotone cutting ‘adversary’ can delete any number of edges between nodes
of diﬀerent labels, thus removing subtrees.

Output the resulting tree T . (The removed subtrees are not revealed.)

•

•

•

Our other adversary is stronger, and we will establish upper bounds against this adversary (with
the recursive majority algorithm) to give a strong recoverability guarantee:

Strong Semirandom Model for Broadcast Tree:

Sample a ‘precursor’ broadcast tree Tpre from the broadcast tree model.

Whenever a child has the opposite label from its parent, the strong monotone
‘adversary’ can replace the entire subtree, starting from the child, with a diﬀerent
subtree (topology and labels) of its choice.

Output the resulting tree T .

•

•

•

An upper bound against this latter model amounts to a recovery guarantee without any assump-
tions as to what happens after a ‘mutation’ of labels — for example, a genetic mutation might
aﬀect reproductive ﬁtness and change the birth rule for the topology. Other variants of monotone
adversaries could also be justiﬁed.

Majority Fails
It is helpful to ﬁrst see how adversaries in these models might break existing
algorithms. Recall that in the broadcast tree model, taking the majority vote of the leaves yields
an algorithm that works up to the Kesten–Stigum bound, and this is optimal since reconstruction
ε)) nodes
is impossible beyond this. In the language of k and ε, each node gives birth to Pois(k(1

−

6

of the same label and Pois(kε) nodes of the opposite label. Hence in a depth R tree we expect kR
leaves, but the total bias of the spins can be recursively computed as kR(1
2ε)R. The fact that
majority works can be proven by applying the second moment method and comparing the bias to
its variance.

−

→ ∞

However, an overwhelming number of the leaves are on some path that has a ﬂip in the label
ε)R nodes with all-root-spin ancestry, a vanishing proportion
at some point: we only expect kR(1
as R
. The strong monotone adversary has control over all the rest, and can easily break
the majority vote. Even the monotone cutting adversary can control the majority vote, by cutting
leaves whose spin matches the root but whose parents have the opposite label. This happens for
a constant fraction of the leaf nodes, and this change overwhelms the majority vote. So majority
vote fails against the semirandom model, for all nontrivial k and ε.

−

This is an instructive example, but we emphasize that breaking one algorithm does not yield a
lower bound. For example, if the algorithm knew what the adversary were doing, it could potentially
infer information about where the adversary has cut edges based on the degree proﬁle, and could
use this to guess the label of the root.

The Problem of Orientation Many ﬁrst attempts at a separation in the broadcast tree model
(which work!) rely on knowing the label of the root. However, such adversaries present a major
diﬃculty in coupling them to a corresponding adversary in the graph. Each graph gives rise to
many overlapping broadcast trees (the local neighborhood of each vertex) and a graph adversary
needs to simultaneously make all of these tree reconstruction problems harder. This means a graph
adversary cannot focus on trying to hide the spin of a speciﬁc tree root; rather, it should act in a
local, orientation-free way that inhibits the propagation of information in all directions.

A promising approach is to look for nodes v whose neighbors in Gpre all have the opposite label,
and cut all of these edges. Such nodes serve only to further inter-connect each community, and
cutting their edges would seem to make community detection strictly harder. In the corresponding
broadcast tree model, these nodes v represent ﬂips in the label that are entirely corrected back, and
deleting them penalizes any sort of over-reliance on distributional ﬂukes in how errors propagate.
For example, majority reconstruction in the tree fully relies on predictable tail events whereby nodes
with label diﬀerent from the root may lead to subtrees voting in the correct direction nonetheless.

The Problem of Dependence Now, however, a diﬀerent sort of problem arises: if we were to
naively apply the adversary described above to a broadcast tree, this would introduce complicated
distance-2 dependencies in the distribution of observed spins, as certain diameter-2 spin patterns are
banned in the observed tree (as they would have been cut). In particular, the resulting distribution
is no longer a Markov random ﬁeld. This is not inherently a problem, in that we could still hope
to prove stronger lower bounds for such models beyond the Kesten–Stigum bound. However, the
diﬃculty is that even for quite simple models on a tree (e.g. the Potts model [Sly11], asymmetric
binary channels [BCMR06]) the threshold is not known, and the lower bound techniques that
establish the Kesten–Stigum bound seem to break down.

An alternative is to speciﬁcally look for degree-2 nodes v whose neighbors in Gpre have the
opposite label, and cut both incident edges. Although there are still issues about making this a
Markov random ﬁeld, we can alleviate them by adding a 3-clique potential on each degree-2 node
and its two neighbors. Then after we marginalize out the label of the degree-2 node, the 3-clique
potential becomes a 2-clique potential, and we return to a Markov random ﬁeld over a tree! In
other words, if we ignore the spin on a degree-2 node and treat its two incident edges like a single
edge, we return to a well-behaved spin propagation rule.

7

2.3 Our Adversary

We are now ready to describe the adversary that we will use to prove Theorems 1.2 and 1.4.
We only need two additional adjustments beyond the discussion above. Instead of making every
possible degree-2 cutting move as described earlier, we will only cut with probability δ. We will
tune δ to ensure that the monotone changes we make do not overshoot and accidentally reveal more
information about the underlying communities by cutting in too predictable a fashion. Finally, our
adversary adds local restrictions to where and how it cuts, to ensure that the changes it makes do
not overlap or interfere with each other (e.g. chains of degree-2 nodes). These details will not be
especially relevant until Section 5, where they simplify the combinatorics of guessing the original
precursor graph from the observed graph.

Distribution 2.3. Let (a, b, n) be given. Write k = a+b
graph Gpre ∼

G(n, a/n, b/n), and apply the following adversary:

2 and ε = b

a+b . We sample a ‘precursor’

Adversary:

•

•

•

If at least 3 neighbors of a vertex v have degree not equal to 2, mark v good.

If a degree-2 vertex has both of its neighbors marked good, mark it tagged.

For each tagged node v whose two neighbors w1, w2 both have opposite label to v:
with probability δ, delete both edges (v, w1) and (v, w2) (otherwise keep both edges)
where

δ ,

1
(1

(

2ε)2
−
ε2

if ε

if ε

≤

≥

1
3 ,
1
3 .

(1)

We can now outline the proof of our main theorem. In order to show that partial recovery is
impossible, it suﬃces to show that it is impossible to reconstruct the relative spin (same or diﬀerent)
of two random nodes u and v, better than random guessing. Before applying the adversary, an
O(log n)-radius neighborhood U around u resembles the broadcast tree model rooted at u. After
the adversary is applied to the graph, U resembles a broadcast tree model with a corresponding
cutting adversary applied. This resemblance will be made precise by the coupling argument of
Section 3; there will be some complications in this, and our tree will not be uniformly the same
depth but will have a serrated boundary.

We show in Section 4 that when the cutting adversary is applied to the tree, the tree reconstruc-
tion problem (guess the spin of u from the spins on the boundary of U ) becomes strictly harder: the
average branching factor becomes lower due to suﬃcient cutting, while the new spin propagation
rule resembles classical noisy propagation with at least as much noise (so long as we marginalize
out the spins of tagged nodes). We then apply the proof technique of Evans et al. [EKPS00] to
complete the tree lower bound.

The ﬁnal step in the proof is to show that reconstructing the relative spin of u and v is at least
as hard as reconstructing the spin of u given the spins on the boundary of U (which separates u
and v with high probability). This is one of the most technically involved steps in the proof. In
the lower bound for the random model [MNS14b], this step — called “no long-range correlations”
— was already an involved calculation on a closed-form expression for P[G
σ]. In our setting,
in order to get a closed-form expression for the conditional probability, we will sum over all the
possible precursor graphs Gpre that could have yielded the observed graph G. We characterize these

|

8

precursors in Lemma 5.7, and the main reason for the good and tagged nodes in our adversary
construction is to simplify this process.

A natural open question is whether one can ﬁnd the optimal monotone adversary. For instance,
we have not even used the power to add edges within communities. Note however, that our current
It is not
adversary is delicately constructed in order to make each step of the proof tractable.
too hard to propose alternative adversaries that seem stronger, but it is likely that one of the
major steps in the proof (tree lower bound or no long-range correlations) will become prohibitively
complicated. Recent predictions on the performance of SDP methods [JMR15] could be suggestive
of the true semirandom threshold.

3 Coupling of Graph and Tree Models

It is well-known that sparse, random graphs are locally tree-like with very few short cycles. This
is the basis of Mossel–Neeman–Sly’s approach in coupling between a local neighborhood of the
stochastic block model and the broadcast tree model [MNS14b]. Hence, our ﬁrst order of business
will be to couple a typical neighborhood from our graph distribution (Distribution 2.3) to the
following tree model, against which we can hope to show lower bounds.

Distribution 3.1. Given the parameters (a, b, R), generate a tree T with spins σ as follows:

Start with a root vertex ρ, with spin σρ chosen uniformly at random from

1.

±

Until a uniform depth R + 3 (where the root is considered depth 0), let each vertex v give
birth to Pois(a/2) children of the same spin and Pois(b/2) children of opposite spin.

Apply the graph adversary (from Distribution 2.3) to this tree; this involves assigning mark-
ings good and tagged, and cutting some tagged nodes. Keep only the connected component
of the root.

Remove all nodes of depth greater than R (the bottom 3 levels).

•

•

•

•

Remove any tagged node at depth R along with its siblings, exposing the parent as a leaf.

•
The reason we trim 3 levels at the bottom is to ensure that the markings and cuttings in T match
those in G, since these depend on the radius-3 neighborhood of each node and edge, respectively.
Removing tagged nodes at level R will ensure that the more complicated spin interactions of tagged
nodes and their neighbors do not span across the boundary of a tree neighborhood in the graph
— we want to cleanly separate out a tree recovery problem. We use a slightly non-conventional
deﬁnition of ‘leaves’:

Deﬁnition 3.2. When we refer to the leaves of a tree sampled from Distribution 3.1 we mean
the nodes at depth R plus any nodes at depth R
1 that are revealed during the last step. Nodes
at depth
1 that happen to give birth to no children are not considered leaves; if the root
is tagged and gets cut by the adversary so that the tree is a single node, this single node is not
considered a leaf.

−

≤

−

R

We can couple the above tree model to neighborhoods of the graph:

Proposition 3.3. Let (a, b, n) be given, and let ρG be any vertex of the graph. Let

R =

(cid:22)

log n
10 log(2(a + b))

3.

−

(cid:23)

9

(2)

There exists a joint distribution (G, σG, T, σT ) such that the marginals on (G, σG) and (T, σT ) match
the graph and tree models, respectively, while with probability 1

o(1):

−

there exists an isomorphism (preserving edges, spins, and the markings good and tagged)
between the tree T and a vertex-subset U of G,

the tree root corresponds to the vertex ρG in the graph, and the leaves correspond to a vertex
set B that separates the interior U r B from the rest of the graph G r U .

we have

U
|

|

= O(n1/8).

•

•

•

Proof. As proven in [MNS14b], there exists a coupling between the precursor graph (Gpre, σG) and
the precursor tree (Tpre, σT ), such that Tpre matches the radius R + 3 neighborhood U of ρG in
Gpre, which has size O(n1/8); this fails with probability o(1).

Next the adversary assigns vertex markings (good and tagged) to both models. The marking of
each vertex is deterministic and only depends on the topology of the radius-3 neighborhood of the
vertex. Thus the markings in Gpre and Tpre match up to radius R. Some tagged nodes are cuttable,
i.e. both their neighbors have opposite spin; the cuttable nodes in Gpre and Tpre also match up to
radius R. The adversary now cuts the edges incident to a random subset of cuttable vertices; we
can trivially couple the random choices made on Gpre with those on Tpre up to radius R. We keep
only the connected component of the root in T ; likewise let us keep only the corresponding vertices
in U , i.e. only those still connected to ρG by a path in U .

After removing nodes of depth greater than R, we have removed the subset of T for which the
markings and the action of the adversary diﬀer from those in G. Thus at this stage, the tree exactly
matches the radius R neighborhood of ρG in G, along the isomorphism given by the coupling from
[MNS14b]. Any boundary vertex of this neighborhood must have distance exactly R from ρG, thus
its corresponding vertex in the tree has depth R and is a leaf. Passing to the ﬁnal step of removing
tagged leaves in T and their siblings, we remove their corresponding nodes from U ; this does not
change the boundary-to-leaves correspondence.

4 Random vs. Semirandom Separation in the Tree Model

In this section we show that our tree distribution (Distribution 3.1) evidences a random vs. semi-
random separation in the broadcast tree model. Recall that the goal is to recover the spin of the
root from the spins on the leaves, given full knowledge of the tree topology and the node markings
(good and tagged). Recall the non-conventional deﬁnition of ‘leaves’ (Deﬁnition 3.2).

Let ∆ = ∆(a, b, R) be the advantage over random guessing, deﬁned such that 1+∆

is the
probability that the optimal estimator (maximum likelihood) succeeds at the above task. We will
) for a strictly larger range
0 as R
show that our tree model is asymptotically infeasible (∆
of parameters (a, b) than that of the corresponding random model.

→ ∞

→

2

Proposition 4.1. For every real number k > 1, there exists 0 < ε < 1
yet for a tree sampled from Distribution 3.1 with parameters a = 2k(1
.
we have ∆(a, b, R)

0 as R

−

2 such that k(1

2ε)2 > 1,
ε), b = 2kε and depth R,

−

→

→ ∞

Recall that the condition k(1

2ε)2 > 1 is the classical Kesten–Stigum bound, which is suﬃcient
to beat random guessing in the random model [KS66]. Several decades later, this bound was found
to be tight for the random model [EKPS00]. There remain many open questions regarding the
hardness of tree models, and some care was required in crafting an adversary for this problem that
keeps the proof of this lower bound tractable.

−

10

Broadly, the Kesten–Stigum bound asserts that recoverability depends on the average branching
factor (a contribution from the tree topology) and the amount of noise (a contribution from the
spin propagation rule). The ﬁrst step of our proof is to distinguish these in our distribution: we can
ﬁrst generate a tree topology from the appropriate marginal distribution, and then sample spins
from the conditional distribution given this tree. We will show how to view this distribution on
spins within the lower bound framework of [EKPS00]. Moreover, the new spin propagation rule
is at least as noisy as the original, while our cutting adversary has strictly decreased the average
branching factor.

Proof of Proposition 4.1. We ﬁrst re-state our tree model in terms of topology generation followed
by spin propagation. Instead of letting each node give birth to Pois(a/2) same-spin children and
Pois(b/2) opposite-spin children, we can equivalently let each node v give birth to Pois(k) children
and then independently choose the spin of each child to be the same as v with probability 1
ε and
opposite to v otherwise. (The equivalence of these steps is often known as ‘Poissonization’.) Here
the correspondence between (a, b) and (k, ε) is as usual: k = a+b
a+b . This allows us to
ﬁrst sample the entire tree topology without spins. Then we can add markings (good and tagged),
since these depend only on the topology. Next, we sample the spins as above, by generating an
appropriate independent
1 value fe on each edge, indicating whether or not a sign ﬂip occurs
across that edge. Finally, we cut edges according to the adversary’s rule.

2 and ε = b

−

±

Distribution 4.2. Given the parameters (a, b, R), generate a tree T with spins σ as follows:

•

•

•

•

•

Start with a root vertex ρ. Until a uniform depth R + 3, let each vertex v give birth to Pois(k)
nodes.

Mark nodes as good and tagged according to the rules of the graph adversary.

For each edge e, generate an independent ﬂip value fe which is +1 with probability 1

ε and

−

1 otherwise.

−
Choose the root spin σρ uniformly from
where e is the edge connecting v to its parent u.

±

1. Propagate spins down the tree, letting σv = σufe

Cut edges according to the adversary’s rule, keeping only the connected component of the
root.

Trim the bottom of the tree (according to the last two steps of Distribution 3.1).

•
It is clear that this tree distribution (Distribution 4.2) is identical to the original tree distribution
(Distribution 3.1). Our next step will be to re-state this model in yet another equivalent way. The
goal now is to sample the ﬁnal tree topology (including which edges get cut) before sampling the
spins. Consider a tagged node v, its parent u, and its single child w. Instead of writing the spin
propagation as independent ﬂips f(u,v) and f(v,w), we will write it as random variables cv and fv.
Here cv is equal to 1 if the adversary decides to cut v (and 0 otherwise), and fv is equal to 1 if
1 otherwise). This means if f(u,v) = f(v,w) = 1 then cv is 1 with probability δ (and
σu = σw (and
0 otherwise); and if we do not have f(u,v) = f(v,w) = 1 then cv = 0. Hence cv = 1 with probability
ε2δ. If cv = 1 then fv is irrelevant because the adversary will cut v from the tree. Conditioned on
cv = 0, fv takes the value +1 with probability (1
1 otherwise. This means that
−
for a tagged (but not cut) node v, the joint distribution of (σu, σw) obeys a propagation rule that

ε)2+ε2(1

and

ε2δ

−

−

δ)

−

−

1

11

is equivalent to putting noise ε′ (instead of ε) on edges (u, v) and (v, w), where (using the deﬁnition
of δ)

ε′ =

(

1
2

1
2 −
1
2
(0, 1

1
3ε
−
1+ε

q

if ε

if ε

1
3 ,
1
3 .

≤

≥

1
2 for all ε

∈

2 ). For most tagged nodes in the tree, we are simply
One can verify that ε < ε′ ≤
going to replace ε by ε′ on the incident edges, which gives the correct joint distribution of (σu, σw)
but not the correct joint distribution of (σu, σv, σw). This is acceptable because the distribution of
leaf spins (given the root spin) is still correct; for this reason we have made sure that none of the
leaves are tagged nodes. The only time when we actually care about the spin of a tagged node
is in the case where the root ρ is tagged. In this case, the root might be cut by the adversary,
yielding a 1-node tree (with no revealed leaves). Otherwise, if the root is tagged but not cut, our
spin propagation rule needs to sample the spins of the root’s two children (a tagged node that is
+ denote this
2; let
not cut must have degree 2) from the appropriate joint distribution over
1
}
+ explicitly (although
distribution, conditioned on σρ = +1. It will not be important to compute
it is straightforward to do so). We are now ready to state the next tree model. This model is
equivalent to the previous ones in that the joint distribution of the root spin, topology, markings,
and leaf spins is the same. The spins on the tagged nodes (other than the root) do not have the
same distribution as before, but this is irrelevant to the question of recovering the root from the
leaves.

{±
D

D

Distribution 4.3. Given the parameters (a, b, R), generate a tree T with spins σ as follows:

•

•

•

•

•

•

Start with a root vertex ρ. Until a uniform depth R + 3, let each vertex v give birth to Pois(k)
nodes.

Mark nodes as good and tagged according to the rules of the graph adversary.

Decide which nodes the adversary should cut: cut each tagged node independently with
probability ε2δ.

Let εe = ε′ for edges e that are incident to a tagged node, and let εe = ε for all other edges.

For each edge e, generate an independent ﬂip value fe which is +1 with probability 1
and

1 otherwise.

−

εe

−

1. If the root is tagged but not isolated: let u1, u2
Choose the root spin σρ uniformly from
+, and let σu1 = d1σρ, σu2 = d2σρ. For all other nodes,
be its children, draw (d1, d2)
propagate spins down the tree as usual, letting σv = σufe where e is the edge connecting v
to its parent u.

∼ D

±

Trim the bottom of the tree (according to the last two steps of Distribution 3.1).

•
Our next step is to further modify this tree model in ways that only make it easier, in the sense
that the advantage ∆(a, b, R) can only increase. First, we will address the issue of the complicated
+ in the case that the root is tagged. Suppose the root is tagged (but not
propagation rule
cut) and consider deterministically setting σu1 = σu2 = σρ where u1, u2 are the root’s two children.
From there, spin propagation continues as usual. We claim that this new model can only be easier
than the original one. To see this, note that the new model can ‘simulate’ the original one: upon
observing leaf spins drawn from the new model, drawn (d1, d2)
1, 2
}
and for each leaf w descended from ui, replace σw by diσw. For convenience we will also replace

+ and then, for each i

∼ D

∈ {

D

12

the ﬁrst level of the tree by deterministic zero-noise propagation in the case where the root is not
tagged. We can similarly argue that the model only gets easier, since one can simulate the old
model using the new model by sampling the noise on the ﬁrst level.

Note that we now have a tree model such that once the topology is chosen, the spin-propagation
rule is very simple: at each edge e, a sign ﬂip occurs independently with some probability εe ∈
. Hardness results for such a tree model were studied by Evans et al., who established the
ε, ε′, 0
{
}
following bound on the advantage ∆T for a ﬁxed tree topology T ([EKPS00] Theorem 1.3’):

∆2

T ≤

2

Xleaves v

Θ2

v where Θv ,

θe,

path(v)

Ye
∈

and where θe , 1
In our
case, the tree T (including both the tree topology and markings) is random, so the advantage is
∆ = ET [∆T ], by the law of total probability. By Jensen’s inequality,

2εe, and path(v) denotes the unique path from the root to leaf v.

−

∆2 = (ET [∆T ])2

ET [∆2
T ]

≤

ET

≤

2

"

, W.

Θ2
v

#

Xleaves v

≥

0 (as R

1. Furthermore we have εe = 0 for edges incident to the root and ε

Our goal is to show ∆
0. Any leaf in our tree is at
→
1
2 elsewhere.
height
R
εe ≤
2. Note that we have not actually used
(1
Therefore, for any leaf v, we can bound Θv ≤
the fact that ε′ strictly exceeds ε; we will in fact be able to prove our result by leveraging only the
decrease in branching factor and not the noise increase. Now we have

), so it is suﬃcient to show W

→ ∞

2ε)R

→

−

≤

−

−

W = ET

2

"

Θ2
v

# ≤

2 ET [# leaves](1

2ε)2(R
−

2)

−

Xleaves v

and so we need only to bound the expected number of leaves. For i = 0, . . . , R, let level i denote the
set of vertices at distance i from the root (so the root is on level 0). Call levels 1, 7, . . . , 6j + 1, . . .
the base levels and call levels 4, 10, . . . , 6j
2, . . . the cutting levels. Imagine growing the tree using
Pois(k) births as usual, and marking nodes as good and tagged as usual. Now allow the adversary
to cut vertices as usual, except refuse to cut any tagged node that is not on a cutting level. Note
that this can only result in more leaves, and so this new process will give an upper bound on the
expected number of leaves in our actual model.

−

To analyze the new process, suppose we have a node v on a base level. The number of de-
scendants nv of v on the next base level is a random variable whose distribution does not depend
on the ancestors of v; this independence is the reason for deﬁning base levels and cutting levels.
Deﬁne k′ by (k′)6 , E[nv]; then (k′)6 < k6, since there is some nonzero probability that one of
v’s descendants will be cut at the subsequent cutting level. The expected number of leaves in the
6, where the factor k6 accounts for the ﬁrst level and the last
entire tree is now at most k6(k′)R
−
2), which goes to

5 levels that are not followed by a base level. Now W

2ε)2(R

2k6(k′)R
−

6(1

−

−

≤
0 as R

provided that

→ ∞

k′(1

≤
2ε)2 < 1.

−

Hence this inequality suﬃces for impossibility of recovery.

However, k′ depends on the topology of the ﬁnal tree model, which depends on k, ε, and δ, so
this inequality is slightly more complicated than it looks. We write k′(k, ε) to make this dependence
explicit; recall that δ is a (continuous) function of ε. We argued above that k′(k, ε) < k for all
0

2 . In particular, at the critical value εcrit = 1

) for the random model, we have

2 (1

ε

1

≤

≤

−

1
√k
2εcrit)2 = 1.

k′(k, εcrit)(1

−

2εcrit)2 < k(1

−

13

But k′(k, ε)(1
must still have k′(k, ε)(1

−

2ε)2 < 1, while k(1

2ε)2 > 1, as desired.

−

−

2ε)2 is a continuous function of ε, so if we take ε to be slightly less than εcrit, we

This completes the proof of Theorem 1.4 (semirandom vs. random separation on trees).

In

Appendix A, we explicitly compute a lower bound on the separation k6

k′(k, ε)6.

−

5 No Long-Range Correlations

The lower bound of the previous section will form the core of a lower bound for the graph: we
have already established that, for a large range of parameters, it is impossible to learn the spin of
a node u purely from the spins at the boundary B of its tree-like local neighborhood. We will now
see why this makes recovery in the graph impossible: there is almost nothing else to learn about
σu from beyond its local neighborhood once we know the spins on B.

Lemma 5.1 (No long-range correlations). Let a graph G (including markings good and tagged)
and spins σ be drawn from Distribution 2.3. Let A = A(G), B = B(G), C = C(G) be a vertex-
partition of G such that

B separates A from C in G (no edges between A and C),

•

A

• |

B

|

∪

= O(n1/8),

B contains no tagged nodes.

•
Then P[σA |
Here σA denotes the spins on A and σBC denotes the spins on B

G, σBC ] = (1 + o(1)) P[σA |

C.

∪

G, σB], for asymptotically almost every (a.a.e.) G and σ.

To clarify, when we refer to good or tagged nodes in G we are referring to the original markings
that they were assigned in Gpre. For instance, if a tagged node is cut by the adversary, it is still
considered tagged in G even though it no longer has degree 2. Recall that the markings (good and
tagged) in G are revealed to the reconstruction algorithm.

In Lemma 5.1, we do crucially use that B does not contain tagged nodes:
if a node in B
were tagged with spin +1, and we then revealed that its neighbor in C has spin
1, this would
strengthen our belief that the neighbor in A has spin +1, as otherwise the tagged node would
have some substantial probability of having been cut, which is observed not to be the case. So
Lemma 5.1 would be false if we allowed tagged nodes in B.

−

5.1 Structure of Possible Precursors Gpre

The proof of Lemma 5.1 will require a thorough understanding of the distribution of spins given G,
which we can only obtain by understanding the possible precursors of G under the adversary. The
reason for the good and tagged nodes in our adversary construction is to make these precursors
well-behaved. We start with some simple observations. Let Gpre be a graph that yields G (with
nonzero probability) by application of the adversary.

Observation 5.2. A good node has degree at least 3 in G.

Proof. A good node has at least three non-degree-2 neighbors in Gpre and the adversary will not
remove these.

Observation 5.3. The adversary does not create any new degree-2 nodes. In other words, if a
node has degree 2 in G then it has degree 2 in Gpre (with the same two neighbors).

14

Proof. In order to create a degree-2 node v, the adversary must cut at least one edge incident
to v. This can only happen if v is either tagged or good. It cannot be tagged because it does
not have degree 2 in Gpre. But it also cannot be good or else it has degree at least 3 in G (by
Observation 5.2).

The key property of the good/tagged construction is that the adversary cannot change whether or
not a node has the following ‘goodness’ property.

Deﬁnition 5.4. Say that a node has the goodness property with respect to a graph if at least
three of its neighbors do not have degree 2.

Lemma 5.5. The good nodes are precisely the nodes that have the goodness property with respect
to G.

Note that this is not tautological because the good nodes are deﬁned as the nodes that have the
goodness property with respect to Gpre, not G.

Proof. We need to show that the goodness property is invariant under the adversary’s action. First
we assume v has goodness in Gpre and show that it also has goodness in G. Since v has the
goodness property in Gpre, it has three neighbors u1, u2, u3 in Gpre that do not have degree 2. Each
ui remains connected to v in G since the adversary only cuts edges that are incident to a degree-2
node. Furthermore, each ui does not have degree 2 in G because degree-2 nodes cannot be created
(Observation 5.3).

Now we show the converse: assume v does not have goodness in Gpre and show that it still does
not have goodness in G. The only way that v could obtain goodness is if at least one of its degree-2
neighbors u becomes non-degree-2 (while remaining connected to v). But this is impossible because
whenever the adversary cuts an edge incident to a degree-2 vertex u, it causes u to become isolated
in G.

Next we state an easy fact about the structure of tagged nodes in G.

Lemma 5.6. The tagged nodes in G are precisely the degree-2 nodes with two good neighbors,
plus some isolated nodes.

Proof. Every tagged node in Gpre has degree-2 with two good neighbors. If the node is cut, it
becomes isolated in G; otherwise it remains degree-2 with two good neighbors. Conversely, let v
be degree-2 in G with two good neighbors; we will show v is tagged. Since degree-2 nodes cannot
be created (Observation 5.3), v has degree-2 in Gpre with the same two good neighbors, and is
therefore tagged.

Now we are ready to characterize the possible precursors Gpre of a given G.

Lemma 5.7. Suppose we have a graph G (including node markings) and spins σ, drawn from
Distribution 2.3. The probability that a graph Gpre yields G under the action of the adversary is
zero unless Gpre can be obtained from G by connecting each isolated tagged node v of G to exactly
two good nodes of opposite spin to v. In this case, if G has m = m(G) isolated tagged nodes and
w = w(σ, G) tagged nodes with two opposite-spin neighbors, then

where we deﬁne 00 = 1 in the case δ = 1, w = 0.

P[G

|

Gpre, σ] = (1

δ)wδm

−

15

Recall that δ is the probability with which the adversary cuts each tagged node that has two
opposite-spin neighbors.

Proof. First suppose Gpre can yield G via the adversary. We will show that Gpre takes the desired
form. The nodes cut by the adversary are precisely the isolated tagged nodes of G. Every such
node was originally connected to two opposite-spin good nodes in Gpre. Therefore Gpre can be
obtained from G by connecting each isolated tagged node to exactly two opposite-spin good nodes.
Conversely, let Gpre be obtained from G by connecting each isolated tagged node to exactly two
opposite-spin good nodes. The good nodes in G are (by Lemma 5.5) precisely the nodes that have
the goodness property in G. These are also precisely the nodes that have the goodness property in
Gpre, since the process of connecting each isolated tagged node to two good nodes does not change
goodness. Consider running the adversary on Gpre. The nodes that it marks good will be precisely
the good nodes in G. Also, the nodes that it marks tagged will be precisely the tagged nodes in
G; this is clear from Lemma 5.6. This means the adversary will output G iﬀ it chooses to cut the
m tagged nodes that are isolated in G and chooses not to cut the w tagged nodes that have two
opposite-spin neighbors in G. This happens with probability (1

δ)wδm.

−

5.2 Proof of No Long-Range Correlations

In the ordinary stochastic block model, given an
Proof of Lemma 5.1. We proceed as follows.
observed graph, the probability of any set of spins σ factorizes as a product of pairwise interactions.
In our model, by summing over all possible precursors Gpre that could have lead to G via the
adversary, we ﬁnd the same pairwise interactions together with a further global, combinatorial
interaction. We show that the neighborhood A is too small to make a signiﬁcant impact on this
global interaction, while the pairwise interactions between A and C are weak, so that the only
factors relevant to A are the pairwise interactions within A
B, which are independent of the spins
in C.

∪

Let Gpre denote the graph before the action of the adversary. Then P[Gpre |

following potentials on unordered pairs: P[Gpre |
ranging over unordered pairs of distinct vertices, where

σ] = Φ(σ, Gpre) ,

σ] factors into the
u,v φ(σu, σv, Gpre) with u, v

Q

φ(σu, σv, H) =

if u
if u
a/n if u
b/n if u

a/n
b/n
1
1

−
−

v and σu = σv,
= σv,
v and σu 6
v and σu = σv,
= σv
v and σu 6

∼
∼
6∼
6∼






where

denotes adjacency in the graph H.

∼

To leverage this description, let us sum over all possible precursors Gpre of G under the adversary.
Let L(σ, G) denote the set of possible precursors Gpre of G, as described by Lemma 5.7: Gpre is
obtained from G by connecting each of the m isolated tagged nodes to exactly two good nodes of
the opposite spin.

P[σ

|

G] =

XGpre
∈

L(σ,G)

∝

∝

XGpre
∈

L(σ,G)

XGpre
∈

L(σ,G)

P[Gpre |

σ] P[σ] P[G

|

σ, Gpre]/ P[G]

Φ(σ, Gpre) P[G

σ, Gpre]

|

Φ(σ, Gpre)(1

δ)wδm

−

16

The proportionality constant hidden by ‘
by replacing 2m opposite-spin non-edges by opposite-spin edges, we have for every Gpre ∈

’ depends on G but not on σ. As Gpre is obtained from G
L(σ, G),

∝

Φ(σ, Gpre) = Φ(σ, G)

2m

.

b/n

1

(cid:18)

−

b/n

(cid:19)

Thus none of the terms depend on the precise choice of Gpre:

P[σ

G]

|

L(σ, G)
|
L(σ, G)
|

∝ |

∝ |

Φ(σ, G)

(cid:18)
Φ(σ, G)(1

b/n

2m

1

b/n
−
δ)w.

−

(cid:19)

δ)wδm

(1

−

where we have dropped the constants that only depend on G (and not σ).

Now we compute

α of negative spin. Suppose there are g/2 + β good nodes of positive spin, and g/2

. Suppose there are m/2 + α isolated tagged nodes of positive spin
L(σ, G)
|
|
β

and m/2
of negative spin. Then the number of possible Gpre is

−

−

L(σ, G)
|
|

=

g/2 + β
2

(cid:18)

(cid:19)

m/2

−

α

g/2
−
2

(cid:18)

m/2+α

β

.

(cid:19)

We can establish that this global L factor only barely depends on the spins of A:

Lemma 5.8. For a.a.e. G, σB, σC, it holds for all σA, σ′A that

L(σA, σB, σC, G)
|
|
L(σ′A, σB, σC, G)
|
|

= 1 + o(1).

The proof proceeds via concentration of measure and Taylor expansion, and is deferred to Ap-
pendix B.

Paraphrasing this lemma, a.a.s. over (σB, G), there exists a ‘good’ subset of a.a.e. σC such that
is independent of σA up to a 1 + o(1) factor. Let us also require of ‘good’ σC that the
L(σ, G)
|
|
census (sum of spins) is O(√n log n); as C consists of all but O(n1/8) nodes of G, it is equivalent
to ask for the same concentration over all spins of G, and this occurs a.a.s. by Hoeﬀding. Let Ω
Ω a.a.s. we have for any set D = D(G) (which will be
be this set of ‘good’ σC values. Since σC ∈
taken as either A or A

B below),

∪
P[σD, G] = (1 + o(1)) P[σD, σC ∈

Ω, G] a.a.s. over σ, G.

(3)

We include a rigorous proof of this statement in Appendix C.

It will be useful to factor the product Φ of classical pairwise interactions into subsets of these

interactions as

Φ(σ, G) = QAB,AB(σAB, G)QBC,C (σBC , G)QA,C (σAC, G).

Here, for instance, QBC,C denotes the product of φ(σu, σv, G) over unordered pairs u, v consisting
C and one from C. The corresponding “no long-range correlations” proof in
of one vertex from B
Ω, QA,C(σAC, G) = (1+ o(1))K(G) for a quantity
[MNS14b] established that for ‘good’ values σC ∈
. Their proof of this fact holds verbatim in our setting: it only
K(G) depending only on
A
|
|
|
2 ).
requires that

= o(n) and that the number of +1 spins in the graph is distributed as Binom(n, 1

C
|

and

∪

A
|
|

Similarly, we can factor the (1

δ)w term as

−
δ)w(σ,G) = (1

(1

−

δ)wA(σAB ,G)(1

δ)wC (σBC ,G)

−

−

17

where, for instance, wA counts the number of tagged nodes in A with two opposite-spin neighbors.
This factorization holds as there are no A–C edges and no tagged nodes in B. We can now absorb
these terms into the Q terms: deﬁne

Q′AB,AB(σAB, G) = (1
and similarly Q′BC,C, and Φ′ = Q′AB,ABQ′BC,CQA,C, so that for σC ∈

−

δ)wA(σAB ,G)QAB,AB(σAB, G),

Ω,

P[σ

G]

|

∝

∝

Φ′(σ, G)
(1 + o(1))Φ′(σ, G)K(G)Q′AB,AB (σAB, G)Q′BC,C (σBC , G).

L(σ, G)
|

|

At this point we roughly adapt the proof of conditional independence from factorization in a
Markov random ﬁeld, following the corresponding proof in [MNS14b]. We compute that for a.a.e.
(σ, G),

P[σA |

σB, G] =

P[σAB, G]
P[σB, G]

= (1 + o(1))

= (1 + o(1))

= (1 + o(1))

= (1 + o(1))

P[σAB, σC ∈
Ω
P[σB, σC ∈
Ω

using (3),

G]
|
G]
|
Ω Φ′(σAB, σ′C , G)
L(σAB, σ′C , G)
|
|
L(σ′AC , σB, G)
Ω Φ′(σ′AC, σB, G)
|
|
L(σAB, σ′C , G)
Ω Q′BC,C (σB, σ′C , G)
|
|

C ∈

σ′
C ∈
P
A,σ′
σ′
Q′AB,AB(σAB, G)
σ′
C ∈
Q′AB,AB(σ′A, σB, G)
P
Q′AB,AB(σAB, G)
Q′AB,AB(σ′A, σB, G)

σ′
A

P

σ′
A

P

P

σ′
C ∈

L(σ′AC , σB, G)
Ω Q′BC,C (σB, σ′C, G)
|
|

,

using Lemma 5.8.

:
L(σ, G)
Multiplying the top and bottom by K(G)Q′BC,C (σBC , G)
|
|

P

= (1 + o(1))

K(G)Q′AB,AB(σAB, G)Q′BC,C (σBC , G)
L(σ, G)
|
|
L(σ, G)
K(G)Q′AB,AB(σ′A, σB, G)Q′BC,C (σBC , G)
|
|

σ′
A

= (1 + o(1))

P

σ′
A

= (1 + o(1)) P[σA |

P

L(σ, G)
Φ′(σ, G)
|
|
L(σ′A, σBC , G)
Φ′(σ′A, σBC , G)
|
|

σBC , G],

using Lemma 5.8 again,

as desired.

6 Random vs. Semirandom Separation in the Block Model

We can now assemble all of the pieces to prove our main result. We ﬁrst prove that it is impossible
to estimate the relative spin of any ﬁxed pair of nodes, in a strictly larger parameter range than
for the random model. The impossibility of partial recovery will then easily follow.
Proposition 6.1. For all k > 1, there exists 0 < ε < 1
graph G (including markings good/tagged) from Distribution 2.3 with parameters a = 2k(1
b = 2kε, we have that for any ﬁxed vertices u, v,

2ε)2 > 1, yet given a
ε),

2 such that k(1

−

−

for a.a.e. G as n

.

→ ∞

P[σu = +1

|

σv = +1, G]

1
2

→

18

By ‘ﬁxed’ vertices u, v we mean that the vertices are ﬁxed before σ and G are chosen; so by
symmetry it doesn’t matter which u, v pair we ﬁx.

Proof. Given k, choose ε as in Proposition 4.1 (tree separation). With probability 1
o(1), the
tree coupling of Proposition 3.3 centered at vertex u succeeds. In this case, the neighborhood U
of u coupled to the tree is of size O(n1/8), and so with probability 1
o(1), v lies outside this
neighborhood. Let B be the boundary vertices of U , i.e. those vertices in U corresponding to the
leaves of the tree. Let A be the interior U r B, and let C be the complement of U in G.

−

−

By the law of total variance,

Var(σu |

σv, G)

≥

EσBC

σv,G[Var(σu |

|

σv, σBC , G)].

(4)

With further probability 1

so that

o(1), Lemma 5.1 (no long-range correlations) succeeds, and we have
−
P[σA |

σBC , G] = (1 + o(1)) P[σA |

σB, G],

But it follows from the coupling in Proposition 3.3 (which includes spins and markings) that

Var(σu |

σBC , G) = (1 + o(1)) Var(σu |

σB, G).

as n

Since R
latter variance converges to 1: the variance of a
o(1) is 1

→ ∞

→ ∞

o(1).

σB, G) = (1 + o(1)) Var(σρ |

Var(σu |
, non-reconstruction in the tree model (Proposition 4.1) implies that this
-valued random variable with expectation
1
}

σleaves, T ).

{±

So we know that, with probability 1
o(1) proportion of σBC contribute a value
o(1) to the expectation in (4). The remaining o(1) proportion of σBC contribute a value bounded

o(1), a 1

−

−

−

1
in magnitude by 1, as the variance of a

−

-valued variable must be. It follows that, a.a.s.,
1
}

and the only way that this is possible in the

σv, G is if

Var(σu |

σv, G)

≥

σBC , G)] = 1

{±
EσBC [Var(σu |
-valued distribution σu |
1
}
+ o(1),

σv = +1, G] =

{±

−

o(1),

1
2

P[σu = +1

|

as desired.

It will follow immediately from Proposition 6.1 that it is hard to ﬁnd a partition that is correlated
(better than random guessing) with the true one. For if it was possible to ﬁnd such a partition
then one could also guess the relative spins of u and v. We now complete the proof of our main
theorem, restating it slightly more precisely.

Theorem 6.2 (restatement of Theorem 1.2). For any k > 1, there exists 0 < ε < 1
k(1
and yet against the monotone adversary given in Section 2, for any η > 1
spins achieves η-partial recovery with probability greater than o(1) as n

2 so that
2ε)2 > 1 and hence partial recovery in the stochastic block model G(n, a/n, b/n) is possible,
2 , no estimator of the
.

−

→ ∞

σ be some assignment of spins to vertices that achieves η-partial recovery:

Proof. Let
σ agrees with
the true spins on at least ηn vertices, possibly after a global spin ﬂip. Consider the relative spins
σu

σv, where u and v are distinct vertices; these match the true relative spins on at least

b

b

b

b

ηn(ηn

1) + (1

η)n((1

η)n

−

−

−

−

1) = (1 + o(1))(η2 + (1

η)2)n2

−

19

ordered pairs of distinct vertices.
correctly estimating their relative spin from

σ is at least

If we choose two distinct vertices at random, the chance of

(1 + o(1))(η2 + (1
b

η)2).

−

(5)

→ ∞

η)2 > 1

Suppose for a contradiction that some estimator achieves η-partial recovery, for some η > 1

2 , with
. When η-partial recovery succeeds, the process above
probability p not converging to 0 as n
η)2),
recovers the relative spin of two random vertices with probability at least (1 + o(1))(η2 + (1
and note η2 + (1
2 . When partial recovery does not succeed, the process still recovers
the relative spin of two random vertices with probability at least (1 + o(1)) 1
2 , as can be seen by
plugging in η = 1
2 to (5). It follows that we can recover the relative spin of two random vertices
2 ] which remains bounded above 1
η)2) + (1
with probability at least (1 + o(1))[p(η2 + (1
2
as n
, contradicting Proposition 6.1. (Proposition 6.1 assumes the markings on G are known,
but clearly the problem is only harder when they are unknown, since an estimator can choose to
ignore them.)

→ ∞

p) 1

−

−

−

−

7 Robustness of SDPs for Partial Recovery

We now turn to giving algorithms for partial recovery in the semirandom setting. In the existing
literature on exact recovery, it has been observed that algorithms obtained through semideﬁnite pro-
gramming extend almost automatically to semirandom models [FK01, AL14, PW15], and moreover
many of these results match the information-theoretic thresholds for exact recovery. In contrast,
semideﬁnite programs for partial recovery, such as those of Gu´edon and Vershynin [GV15], come
within a constant of the threshold but have been unable to close this gap.

In fact Gu´edon and Vershynin [GV15] developed a general framework for using SDPs to solve
problems on sparse graphs, including partial recovery for the stochastic block model. We deﬁne
a notion of semirandom model for any problem in this framework, generalizing the semirandom
model for community detection. We show that any analysis that follows their framework carries
over automatically to this semirandom model. In particular, after minor modiﬁcations, the SDP
analyzed in [GV15] achieves partial recovery in the semirandom block model, up to within a constant
factor of the classical threshold.

Semirandom vs. random gaps oﬀer an explanation for why it seems hard to ﬁnd semideﬁnite
programs for partial recovery that reach the information-theoretic threshold: the analysis often
extends equally well to the semirandom model, where we know the threshold is strictly harder!

7.1 The Gu´edon–Vershynin Framework and Partial Recovery

−

λJ, where J is the all-ones matrix, λ = a+b

In the framework of [GV15], there are several key terms. The ﬁrst is a matrix B which is usually
computed in a simple manner from the adjacency matrix of the observed graph. In our case, we
2n is a regularization constant, and
compute B = A
A is the observed adjacency matrix (albeit with the non-standard convention that there are ones
along the diagonal instead of zeros). The goal is to show that B — which is a random matrix —
is near some ﬁxed reference matrix R. In our case this will be a
b
2n σσ⊤, which is nearly equal to
−
E[B] except on the diagonal. Then, one solves an SDP to maximize
Ω, where Ω is
B, Z
h
∈
I (i.e. the diagonal entries of Z
the space of symmetric PSD n
are at most 1).3 The goal of the analysis is to show that the SDP outputs some
Z that is close to

n matrices Z satisfying diag(Z)

over Z

≤

×

i

3The framework also allows Ω to have additional constraints, but we will not need this.

b

20

a “ground truth” ZR, which in our case is the
{±
following result outlines the steps of the analysis.

-valued matrix σσ⊤ of true relative spins. The
1
}

Proposition 7.1. Suppose we have (B, R, ZR) such that the following three conditions hold for
some value α, some function F (β), and some matrix norm

(1) ZR is a maximizer of the reference objective
(the reference SDP recovers the truth),

R,
h

−i

:
k · k
over Ω

(2)

R

B
k
(the observed objective is close to the reference one in cut norm),

k∞→

1 ≤

−

α

(3) if Z

Ω and

∈

R, ZR −
h

Z

i ≤

β, then

ZR −
k

Z

2
k

≤

F (β)

(good solutions to the reference SDP are close to the ground truth).

Z

ZR −
k

Then
≤
and KG is the Grothendieck constant.
b

F (2KGα) where

b

2
k

Here, the cut norm (or

-to-1 norm) of a matrix is deﬁned as

∞

Z is any maximizer of the empirical objective

B,
h

−i

over Ω,

M
k

k∞→

1 , max
x
k

∞=1 k
k

M x

k1 = max

x

1

∈{±

n k
}

M x

k1.

The proof of Proposition 7.1 follows from Lemma 3.3 in [GV15], and uses Grothendieck’s inequality.
The partial recovery results of [GV15] proceed by verifying the three conditions of Proposi-

tion 7.1 for a particular choice of parameters:

Proposition 7.2. Assume a > 20. Let B = A
Conditions (1–3) of Proposition 7.1 hold a.a.s. with α = O(n√a + b), F (β) = β

2n σσ⊤, and ZR = σσ⊤.
−
b)), and

O(n/(a

λJ with λ = a+b

2n , R = a

−

b

·

−

as the Frobenius norm

k · k2.

k · k
Concretely, this means that we solve the following SDP:

A
h

SDP 7.3. Maximize

λJ, Z

subject to Z

0 and diag(Z)

i

−

(cid:23)
Note that the regularization constant λ is necessary because if we simply take B = A then condition
1 of Proposition 7.1 fails. In [GV15] they estimate λ from the empirical average degree, but their
arguments also apply to the case where we deterministically take λ = a+b
2n . (This requires knowledge
of the parameters a, b but we will address this issue later.)

≤

I, where λ = a+b
2n .

In [GV15], condition 1 is shown in Lemma 5.1, and conditions 2 and 3 are implicit in the proof
of Lemma 5.2. These require a technical condition: max
20, which
for large n simply amounts to a > 20. By Propositions 7.1 and 7.2, we now attain the result
O(n2√a + b/(a
It is also shown in [GV15] how to translate this to a
ZR −
k
precise partial recovery result:

2
2 ≤
k

b)) a.a.s.

a/n), b(1

a(1
{

b/n)

} ≥

−

−

−

Z

b

Proposition 7.4. Let ZR = σσ⊤. For any 1
2 < η
2
(1
stochastic block model, provided that
2 ≤
k
of

ZR−
k

Z.

Z

−

1, η-partial recovery succeeds a.a.s. in the
≤
η)n2, by taking the signs in the top eigenvector

Corollary 7.5. There exists a constant C such that η-partial recovery succeeds a.a.s. in the stochas-
tic block model as n

whenever a > 20 and (a

2(a + b).

C(1

b)2

b

−

≥

η)−

−

→ ∞

b

21

−

[GV15] sets C = 104, although no attempt is made to optimize
The constant C is quite large:
this constant. Nevertheless this result is only oﬀ by a constant from the threshold, which is
b)2 > 2(a + b). Our random vs. semirandom separation becomes small as k grows, so it remains
(a
2ε)2 > 1 + o(1) as
plausible that semideﬁnite programs can achieve partial recovery when k(1
. In fact, SDPs are known to distinguish random block model graphs from Erd˝os–R´enyi
k
graphs in such a range [MS15], and it would be interesting to determine whether this carries through
to partial recovery in the semirandom model.

→ ∞

−

7.2 SDPs and Semirandom Models

We now turn to a semirandom view of the general Gu´edon–Vershynin framework. In the semiran-
dom block model, a monotone adversary is allowed to make changes aligned with the ground truth
ZR; more formally, it can add a matrix S to the observed adjacency A, where S is symmetric and
1’s in some opposite-spin entries where A is
has 1’s in some same-spin entries where A is 0, and
1. It is easily veriﬁed that ZR maximizes
1, 1],
and ZR has

1 entries that match the sign pattern of S.

over Ω: every matrix in Ω has entries in [

S,
h

−i

−

−

Following this observation, we propose a precise deﬁnition of semirandom models for Gu´edon–

±

Vershynin problems:

Deﬁnition 7.6. A matrix S is a monotone change if ZR is a maximizer for
over Ω. In the
semirandom model, a preliminary objective B is generated from the random model, and then an
adversary modiﬁes the objective by any monotone change S, yielding an observed matrix B + S.

S,
h

−i

Note that the set of such monotone changes S forms a cone, which matches the intuition that
semirandom models can make unbounded changes, but only in certain directions aligned with the
ground truth.

Any analysis following the framework of Proposition 7.1 generalizes in a formal sense to the

semirandom model:

Proposition 7.7. Suppose that (B, R, ZR) satisfy conditions (1–3) of Proposition 7.1 for some
. Then it remains the case that
ZS is any maximizer of
α, F (β),
k · k
over Ω, and S is any monotone change (in the sense above), which is allowed to depend
B + S,
h
−i
adversarially on B.

F (2KGα), where

ZSk
b

ZR −
k

≤

b

2

Proof. It suﬃces to establish conditions (1–3) of Proposition 7.1 with R + S playing the role of R
and B + S playing the role of B:

(1) As ZR maximizes both

(2) We have

(B + S)
k
probability.

−

R,
h
(R + S)

−i

and

S,
h
1 =

−i
B
k

−

k∞→

R

k∞→

over Ω, it certainly maximizes their sum

.
−i
1, which by hypothesis is at most α with high

R + S,
h

(3) Suppose that Z

β. Then

∈

Z

i ≤

Ω and

R + S, ZR −
h
Z
R, ZR −
h
over Ω.
S,
h
F (β) in the appropriate norm.

i ≤

−i

since ZR is a maximizer of
ZR −
k

2
k

≤

Z

β

S, ZR −

− h

Z

i ≤

β,

It follows from the original condition (3) that

22

We have shown that any analysis of an SDP following the Gu´edon–Vershynin framework trans-
fers to the semirandom model. A recovery guarantee for the same SDP by diﬀerent means would
not automatically yield such a guarantee in the semirandom model. This diﬀers from exact recov-
ery problems, where semirandom guarantees can take the success of the SDP as a black box, and
certify that any successful instance remains successful under monotone changes [FK01].

Corollary 7.8. Let A be sampled from the semirandom block model with parameters a, b, n. Suppose
that we know these parameters, so that we can run SDP 7.3 with the deterministic choice λ = a+b
2n .
2(a + b) then this SDP achieves η-partial recovery a.a.s. (after
104(1
If a > 20 and (a
taking the sign of the top eigenvector of the SDP output

η)−

Z).

b)2

−

≥

−

b

This dependence on parameter knowledge is slightly unwieldy; on the other hand, estimating
the correct regularization parameter λ from A + S is not immediately easy, since model-speciﬁc
estimators are precisely the sort of techniques that semirandom models aim to penalize. Instead,
we show that deterministically taking log n/n in place of λ avoids this dependence. Indeed, the
same SDP appears in the exact recovery literature [ABH15] with deterministic regularization pa-
rameter 1
2 , which is comparatively very large, but comes at the cost of requiring precisely balanced
communities. Our approach here is a middle ground that allows for natural √n-scale deviations in
the balance of the two communities. There is nothing special about the value log n/n and it can
in fact be taken to be anything of order strictly between 1/n and 1/√n.

Proposition 7.9. The Gu´edon–Vershynin SDP (SDP 7.3) with regularization parameter λ′ =
log n/n achieves η-partial recovery against the semi-random model a.a.s. so long as a > 20 and
(a

2(a + b).

104(1

b)2

−

≥

η)−

−

Proof. In light of Proposition 7.7, it is suﬃcient to show that this SDP works in the random
model. We check the three conditions for (B′, R′, ZR) where B′ = A
λ′)J,
−
R′ = a
n . We will
2n σσ⊤ + (λ
−
use the fact that the three conditions are satisﬁed for (B, R, ZR), and we will achieve nearly the
same parameters α, F (β) as in that case.

λ′J = B + (λ
−
2n and λ′ = log n
λ′)J, and ZR = σσ⊤ where λ = a+b

λ′)J = R + (λ

−

−

b

(1) We certify that ZR maximizes

minimize

R′,
h

−i
γv

by SDP duality. The dual SDP reads

subject to Λ , diag(γ)

R′

0,

(cid:23)

−

v
X

where v runs over all vertices in the graph. To certify that ZR is optimal, it suﬃces by
= 0. Since Λ and ZR are
complementary slackness to ﬁnd γ such that Λ
PSD, the second condition is equivalent to ΛZR = 0. As the columns of ZR are spanned by
σ, this second condition reads Λσ = 0; expanding,

Λ, ZRi
h

0 and

(cid:23)

where Cv is the set of nodes with the same spin as node v, and Cv is the complement of Cv.
By Hoeﬀding we have

√n log n a.a.s. and so

γv =

b

+

a

−
2

a + b

n −

log n
n

(cid:19)

(cid:18)

Cv| − |
|
(cid:0)

C v|
(cid:1)

v

∀

C v|
Cv| − |
|
b
a
(cid:12)
(cid:12)
(cid:12)
(cid:12)
γv =
−
2 −

≤

O(log2 n/√n) =

a

b
−
2 −

o(1).

In particular, γv ≥
that Λ
(cid:23)

0 for all v a.a.s. This choice of γ guarantees Λσ = 0 so it remains to show

0. Since Λσ = 0, it suﬃces to show that v⊤Λv

0 for all v

≥

⊥

σ. But note that

Λ =

(cid:18)

log n

n −

a + b
2n

b

a

−
2n

J

−

(cid:19)

23

σσ⊤ + diag(γ),

v⊤Λv = v⊤

log n

n −

a + b
2n

J + diag(γ)

v

0,

≥

(cid:19)
0 a.a.s. so that both J and diag(γ) are PSD. This establishes optimality of ZR.

(cid:20)(cid:18)

(cid:21)

since γ

≥

(2) We have B′ −
(3) Suppose that Z
and R′ = R
1
C
C+| − |
2 (
|

−
)2
−|

≤

R′ = B

−

R and so this step follows from the corresponding step for (B, R, ZR).
β. Letting ν = λ′ −
are the community sizes. Thus,

i ≤
0 since both matrices are PSD, while
n log2 n a.a.s. where C+, C

R′, ZR −
h
Z, J
h

ν
≤
≤
ZR, J
h

Ω satisﬁes
∈
νJ. Notice that

λ we have 0

log n
n
=

i ≥

Z

i

−

R, ZR −
h

Z

i

=

≤

≤

Z
i
n log2 n

R′, ZR −
h
β + ν
·
β + log3 n.

+ ν

J, ZR −
h
J, Z
ν
i
h

−

Z

i

Applying condition 3 for (B, R, ZR), we obtain

ZR −
k

Z

2
2 ≤
k

(β + log3 n)

O(n/(a

b)).

−

·

By Proposition 7.4, we obtain partial recovery by rounding the leading eigenvector, under the

same conditions as in Corollary 7.5, and with the same constant C.

This analysis completes the proof of Theorem 1.3 from the introduction.

Independently, [MMV15b] proved a result on SDP robustness that matches ours. Additionally,
they consider robustness to a diﬀerent adversary that can only add or remove o(n) edges but is not
required to be monotone. Our methods also extend to this case (and match their results) because
o(n) changes can only change the cut norm by o(n).

8 Robustness of Recursive Majority in the Broadcast Tree Model

Here we establish that recursive majority achieves reconstruction in the semirandom broadcast tree
model, even with respect to the strong adversary that has total control over entire subtrees. As one
would imagine, in recursive majority the root spin is estimated as the majority vote of its children’s
estimated spins, which in turn are estimated as the majority vote of their children, and so on down
to the known leaf spins. In the random model, this algorithm falls short of the Kesten–Stigum
[Mos98]. Nevertheless, it remains the algorithm of choice
bound by a factor of
in practice for tree reconstruction problems, often by the name of ‘parsimony’ [Mos04]. It seems
that the advantages of recursive majority over majority only become clear when studying these
algorithms through semirandom models!

2/π as k

→ ∞

p

To keep things simple, we will ﬁrst consider trees in which each non-leaf node has exactly k
children, matching the setting of [Mos98] and much of the tree reconstruction literature. At the
end, we show how to adapt our proof in a straightforward manner to the case where the number
of children of each non-leaf node is a Poisson random variable.

Proposition 8.1. Let Mk(p) be the probability of a majority ‘yes’ vote of k voters each voting ‘yes’
with probability p:

Mk(p) , P

Binom(k, p) >

+

P

Binom(k, p) =

.

k
2

1
2

k
2

(cid:21)
Let (T, σ) be drawn from the strong semirandom model on the regular tree with braching factor k,
height R, and noise ε. Let ε∗k be deﬁned by
1

(cid:21)

(cid:20)

(cid:20)

1

−

ε∗k

= max
(0,1]
q
∈

Mk(q)
q

,

24

ε∗k, the recursive majority algorithm correctly recovers
and let q∗k be the maximizer. So long as ε
≤
ε∗k) > 1
the root spin from the topology and leaf spins, with probability at least p∗k = q∗k/(1
2 .
Conversely, if ε > ε∗k, there exists an adversary forcing recursive majority to fail with probability
1

o(1) as R

−

.

−

→ ∞

Proof. Let (T, σ) be drawn from the strong semirandom model, starting with a k-regular tree of
height R. Let pR denote the success probability on such a tree. As a base case, we have p0 = 1.
For each child of the root, the spin matches that of the root with probability (1
ε), in which case
1, by induction. The
the child will be correctly labeled by recursive majority with probability pR
other children are under adversary control. Then the number of recovered child labels agreeing
with the root label stochastically dominates Binom(k, pR
ε)), for any choice of adversary.
The recovered root label is the majority vote of the recovered child labels, so we obtain that

1(1

−

−

−

−

Moreover this inequality is exactly tight for a certain adversary, which replaces each replaceable
subtree by a path down to a single leaf of spin opposite to the root.

pR ≥

Mk(pR

−

1(1

−

ε)).

We are interested in the limiting recovery probability p

ﬁxed point in [0, 1] of the function p
solution in [0, 1
intersection point of the graph of Mk with the line of slope 1/(1

−
ε] of the ﬁxed-point equation q/(1

Mk(p(1

7→

−

−

= limR

ε)). Equivalently, q

pR; this is the greatest
ε) is the greatest
∞
ε) = Mk(q). Geometrically, this is the greatest

→∞
= p

(1

−

∞

∞

2 ) = 1

Before proceeding, it is worth noting the geometry of the graph of Mk. From the deﬁnition, Mk
is monotone on [0, 1], with Mk(0) = 0, Mk( 1
2 , and Mk(1) = 1. By expanding the probability
mass function of the binomial distribution, Mk is a degree k polynomial, and thus smooth. Less
obviously, Mk is strictly convex on [0, 1

2 ] and strictly concave on [ 1

The line of slope 2 (ε = 1
1 on [ 1

1
2 ) does not intersect the graph of Mk nontrivially, since Mk(q)
2
on [0, 1
2 , 1]. The line of slope 1 (ε = 0) certainly intersects the graph of Mk
at 1
ε∗k)
intersects the graph of Mk tangentially at some q∗k. Equivalently, we can characterize this slope as
the maximum slope of the line deﬁned by the origin and any point on the graph of Mk:

2 and 1. Hence there exists some maximal 0 < ε∗k < 1

2 , at which the line of slope 1/(1

2 ] and Mk(q)

2 , 1] [Mos98].

≤

−

≤

ε) through the origin.

−

1

1

−

ε∗k

= max
(0,1]
q
∈

Mk(q)
q

.

By the concavity and convexity properties of Mk, the graph of Mk lies below the line of slope 1 on
(0, 1
2 , 1). It follows that q∗k > 1
2 .

2 ), and above on ( 1
As we sweep q from q∗k to 1, the slope R(q)/q passes continuously from 1/(1

an intersection q
theorem. Thus, whenever 0

≥

q∗k satisfying R(q)/q = 1/(1

ε) for every ε

−

∈

ε∗k, recursive majority achieves recovery with probability

ε∗k ) to 1, granting
[0, ε∗k], by the intermediate value

−

ε

≤

≤

=

p

∞

q
1

∞
−

q

∞ ≥

q∗k >

ε ≥

1
2

.

Whenever ε > ε∗k, the two curves do not intersect on [0, 1] except at 0, so the limiting success
probability p

is 0.

∞

It is interesting that, as the noise ε varies, we see the success probability jump discontinuously
2 to zero. This contrasts with the behavior of recursive majority in the ordinary
2 at a threshold

from p∗k > 1
broadcast tree model, in which the error probability transitions continuously to 1
[Mos98].

25

1

0

k

0.8

The curves R(q) and q/(1
R(q)/(1

iterating q

ε) for k = 11, ε = 0.25. A cobweb plot shows the eﬀect of
ε); here there is no nontrivial ﬁxed point, and recursive

majority fails. A small decrease in ε will cause a ﬁxed point to occur near q = 0.683.

−
−

7→

1

]
s
s
e
c
c
u
s
[
P
P

0

0.5

ε

impossible

n o n - r o b u s t ?

robust

ε

0.5

0

k

40

Left: success probability in the 11-regular tree. Right: the Kesten–Stigum threshold and the
semirandom recursive majority threshold in Pois(k)-birth trees.

For any ﬁxed k, computing the critical value ε∗k amounts to maximizing the polynomial Mk(q)
,
or equivalently ﬁnding the unique root of its derivative in (0, 1]. For example it is easily computed
that ε∗3 = 1
9 . However, we would like some asymptotic understanding of the values ε∗k as k
Proposition 8.2. The following asymptotic expression holds for ε∗k as k

→ ∞

:

.

q

→ ∞

ε∗k =

1
2 −

1
2

(cid:18)

+ o(1)

log k
k

.

(cid:19) r

Proof. By the maximality property deﬁning ε∗k and q∗k, we must have
Mk(q)
q

= (q∗k)−

M ′k(q∗k)

2(q∗k ·

d
dq

q=q∗
k

0 =

−

Mk(q∗k)),

(cid:12)
(cid:12)

26

from which it follows that M ′k(q∗k) = Mk(q∗
k)

q∗
k

= 1
1
−

ε∗
k

1 =

1

−

1

M ′k(q∗k) =

0 ≤

. Thus, as ε∗k ∈
Mk(q∗k)
1

q∗k ≤

1

1
2

−

[0, 1

2 ], we know that

= 2.

In analyzing recursive majority in the random model, Mossel [Mos98] computed this derivative
using Russo’s formula. Let us assume for the moment that k is odd; the even case is similar. Then

M ′k(q) = (q(1

−

q))(k

1)/2k

−

(cid:18)

If we evaluate this at q = 1

2 + 1

2

α log k/k, we obtain:

k
(k

1
−
1)/2

−

.

(cid:19)

p
M ′k(q) =

1
4 −

1
4

α log k
k

(cid:18)

= (1 + o(1))

1

(cid:18)

−

= (1 + o(1))

1

−

1)/2

(k

−

k

(cid:19)
α log k
k

(cid:19)
1
2 α log k
(k

−

1)/2 !

1
−
1)/2

k
(k
−
1)/2

(cid:19)

(cid:18)
(k
−

r
1)/2

(k

−

k
π

k
π

r

= (1 + o(1)) exp(

−

1
2

α log k)

k
π

r

= (1 + o(1))

r

1
2 −

1

2 α.

k

1
π

In the second-to-last step, we have used the asymptotic identity (1
as k
derivative M ′k(q) tends to 0 as k
q∗k = 1

x)
, which still holds when x depends on k so long as x = o(√k). Now for α > 1, the
. Thus, writing
, while for α < 1, this tends to
α∗k log k/k, we must have α∗k = 1 + o(1). We immediately obtain a lower bound on ε∗k:

→ ∞
2 + 1

x/k)k = (1 + o(1)) exp(

→ ∞

→ ∞

as k

∞

−

−

2

p

ε∗k = 1

q∗k
Mk(q∗k) ≥

q∗k
1

=

1
2

+

−

1
2

(cid:18)

+ o(1)

(cid:19) r

log k
k

.

For a matching upper bound, apply Hoeﬀding’s inequality for a binomial tail bound, to ﬁnd

≥
and use the maximality of q∗k to ﬁnd

Mk(q)

1

−

exp(

−

α log k/2) = 1

k−

α/2,

−

1

−

q
Mk(q)

ε∗k = 1

−

1

−

≤

q∗k
Mk(q∗k) ≤
2 + 1
1
2
1

α log k/k
α/2
k−

α log k

k !

+

p
−
1
2 r
α log k

O(k−

k −

Xj=0
α/2),

= 1

1
2

−  

=

1
2 −

1
2 r

∞

(k−

α/2)j

so that 1
proof.

2 −

( 1
2 + β)

log k
k

q

is an asymptotic upper bound for every β > 0, which completes the

27

 
Note that p∗k = q∗k/(1

ε) is the probability of success at the threshold ε∗k, so this proof also

−
provides some sense of the critical success probability:

p∗k =

1

−

1

2 + ( 1
2 + o(1)
( 1
2 + o(1))
p

1
2 −

(cid:16)

log k/k

log k/k

p

(cid:17)

= 1

o

−

log k

 r

k !

.

So we observe a very strong threshold: as we vary ε, there is a discrete jump from very likely
success of recursive majority to almost-sure failure!

This result does not change if we pass back to the Poisson-birth tree used throughout the rest
of this paper, rather than the k-regular tree. Here RPois(k)(q) = Eℓ
Pois(k)[Rℓ(q)], which leads to a
Poisson-averaged derivative, and the same threshold in q for when the derivative passes from large
to small. By Poissonization, we can also write

∼

RPois(k)(q) = P [Pois(kq) > Pois(k(1

q))] +

−

1
2

P [Pois(kq) = Pois(k(1

q))] ,

−

and the standard Chernoﬀ bound for “Poisson races” yields, for q = 1

2 + ν,

1

−

RPois(k)(q)

exp

k



−

≤

1
2

+ ν

 r

− r

2

1
2 −

ν

!

= exp(

= exp(


−

−

4ν2))
k(1
1
2k(ν2 + O(ν4))),
p

−

−





which is as strong as the Hoeﬀding bound used above.

The results above constitute a proof for Theorem 1.5: recursive majority achieves recovery

against the strong semirandom model, so long as

ε <

1
2 −

1
2

(cid:18)

+ o(1)

(cid:19) r

log k
k

,

or, rearranging,

with asymptotics holding as k

2ε)2 > 1 + o(1),

k
log k
.

(1

−

→ ∞

It is interesting to see how the semirandom model can yield recovery results across an entire
family of previously studied distributions. The asymmetric broadcast trees studied in [BCMR06]
may be described as Markov chains on the tree with transition matrix

1

−
ε

ε + δ
δ

−

(cid:18)

1

ε + δ
ε

−

−

,

δ

(cid:19)

where the ‘asymmetry’ δ may be positive or negative. The strong semirandom adversary can
simulate these models: starting from the broadcast tree model with noise ε +
, an adversary
|
1-to-+1 transitions, depending on the sign of δ,
searches the tree top-down for either +1-to-
and ﬂips the entire resulting subtree with probability 2
), recursing into subtrees. Thus, the
δ
|
|
semirandom recovery results above guarantee for free that recursive majority is competitive against
all of these models simultaneously, as are all algorithms robust to the strong semirandom model.
By contrast, these models each admit a simple but brittle “weighted majority” reconstruction
algorithm, where the weighting must be perfectly tuned to the speciﬁc model — a tiny deviation
in the parameters (ε, δ) will cause those census algorithms to fail as R

/(ε+
|

1 or

δ
|

δ
|

−

−

.

→ ∞

28

9 Conclusion

In revisiting the stochastic block model from the perspective of semirandom models, we showed
that there is a tension between establishing sharp thresholds and obtaining algorithms with natural
robustness guarantees. There are many more classical problems in statistics and machine learning
where semirandom models could oﬀer a promising way to move beyond average-case analysis and
explore issues related to robustness. In particular, belief propagation is one of the most far-reaching
heuristics, but at high ‘temperature’ it can result in algorithms such as majority that we have shown
are not robust. At low ‘temperature’ it leads to more robust algorithms, like recursive majority,
that are connected to convex optimization. Can exploring ‘temperature’ further lead to interesting,
provable tradeoﬀs between robustness and statistical power that result in a richer understanding
of how belief propagation performs in practice?

Acknowledgements

The authors would like to thank Philippe Rigollet and the MIT learning theory group for helpful
discussion, and Roxane Sayde for reading a draft of this document.

References

[ABH15]

E. Abbe, A. S. Bandeira, and G. Hall. Exact recovery in the stochastic block model.
IEEE Transactions on Information Theory, 2015.

[AL14]

[AS00]

[AS15]

A. A. Amini and E. Levina. On semideﬁnite relaxations for the block model.
arXiv:1406.5647, June 2014.

N. Alon and J. H. Spencer. The probabilistic method. Wiley-Interscience series in
discrete mathematics and optimization. Wiley, New York, Chichester, Weinheim, 2000.

E. Abbe and C. Sandon. Community detection in general stochastic block models:
fundamental limits and eﬃcient recovery algorithms. In 56th Annual IEEE Symposium
on Foundations of Computer Science (FOCS), 2015.

[BCLS87] T. N. Bui, S. Chaudhuri, F. T. Leighton, and M. Sipser. Graph bisection algorithms

with good average case behavior. Combinatorica, 7(2):171–191, 1987.

[BCMR06] C. Borgs, J. Chayes, E. Mossel, and S. Roch. The Kesten-Stigum Reconstruction Bound
Is Tight for Roughly Symmetric Binary Channels. In 47th Annual IEEE Symposium
on Foundations of Computer Science (FOCS), pages 518–530, 2006.

[Bop87]

[BS95]

R. B. Boppana. Eigenvalues and graph bisection: An average-case analysis. In 28th
Annual Symposium on Foundations of Computer Science (FOCS), pages 280–285, 1987.

A. Blum and J. Spencer. Coloring random and semi-random k-colorable graphs. J.
Algorithms, 19(2):204–234, 1995.

[DKMZ11] A. Decelle, F. Krzakala, C. Moore, and L. Zdeborov´a. Asymptotic analysis of the
stochastic block model for modular networks and its algorithmic applications. Physical
Review E, 84(6):066106, December 2011.

29

[EKPS00] W. Evans, C. Kenyon, Y. Peres, and L. J. Schulman. Broadcasting on trees and the
Ising model. The Annals of Applied Probability, 10(2):410–433, May 2000.

[FK00]

[FK01]

[GV15]

U. Feige and R. Krauthgamer. Finding and certifying a large hidden clique in a semi-
random graph. Random Struct. Algorithms, 16(2):195–208, 2000.

U. Feige and J. Kilian. Heuristics for semirandom graph problems. Journal of Com-
puting and System Sciences, 63:639–671, 2001.

O. Gu´edon and R. Vershynin.
Grothendieck’s inequality. Probability Theory and Related Fields, 2015.

Community detection in sparse networks via

[HLL83]

P. W. Holland, K. B. Laskey, and S. Leinhardt. Stochastic blockmodels: First steps.
Social networks, 5(2):109–137, 1983.

[HWX15] B. Hajek, Y. Wu, and J. Xu. Achieving exact cluster recovery threshold via semideﬁnite
programming. In Information Theory (ISIT), 2015 IEEE International Symposium on,
pages 1442–1446, June 2015.

[JMR15] A. Javanmard, A. Montanari, and F. Ricci-Tersenghi. Phase Transitions in Semideﬁnite

Relaxations. arXiv:1511.08769, November 2015.

[JS98]

M. Jerrum and G. B. Sorkin. The metropolis algorithm for graph bisection. Discrete
Applied Mathematics, 82(1-3):155–175, 1998.

[KMM11] A. Kolla, K. Makarychev, and Y. Makarychev. How to play unique games against a
semi-random adversary: Study of semi-random models of unique games. In IEEE 52nd
Annual Symposium on Foundations of Computer Science (FOCS), pages 443–452, 2011.

[KS66]

H. Kesten and B. P. Stigum. A Limit Theorem for Multidimensional Galton-Watson
Processes. The Annals of Mathematical Statistics, 37(5):1211–1223, October 1966.

[LLDM08] J. Leskovec, K. J. Lang, A. Dasgupta, and M. W. Mahoney. Statistical properties of
community structure in large social and information networks. In Proceedings of the
17th International Conference on World Wide Web, WWW ’08, pages 695–704. ACM,
2008.

[Mas14]

L. Massouli´e. Community detection thresholds and the weak Ramanujan property.
Proceedings of the 46th Annual ACM Symposium on Theory of Computing (STOC
’14), pages 694–703, 2014.

[McS01]

F. McSherry. Spectral partitioning of random graphs. In 42nd Annual Symposium on
Foundations of Computer Science, FOCS 2001, pages 529–537, 2001.

[MM05] M. M´ezard and A. Montanari. Reconstruction on Trees and Spin Glass Transition.

Journal of Statistical Physics, 124(6), 2005.

[MMV12] K. Makarychev, Y. Makarychev, and A. Vijayaraghavan. Approximation algorithms for
semi-random partitioning problems. In Proceedings of the 44th Symposium on Theory
of Computing Conference (STOC), pages 367–384, 2012.

[MMV15a] K. Makarychev, Y. Makarychev, and A. Vijayaraghavan. Correlation clustering with
noisy partial information. In Proceedings of The 28th Conference on Learning Theory
(COLT), pages 1321–1342, 2015.

30

[MMV15b] K. Makarychev, Y. Makarychev, and A. Vijayaraghavan. Learning communities in the

presence of errors. arXiv:1511.03229, 2015.

[MNS13]

E. Mossel, J. Neeman, and A. Sly. A Proof Of The Block Model Threshold Conjecture.
arXiv:1311.4115, November 2013.

[MNS14a] E. Mossel, J. Neeman, and A. Sly. Belief propagation, robust reconstruction and op-
timal recovery of block models. In Proceedings of The 27th Conference on Learning
Theory, COLT 2014, pages 356–370, 2014.

[MNS14b] E. Mossel, J. Neeman, and A. Sly. Reconstruction and estimation in the planted
partition model. Probability Theory and Related Fields, pages 1–31, 2014.

[Mos98]

E. Mossel. Recursive reconstruction on periodic trees. Random Struct. Algorithms,
1998.

[Mos04]

E. Mossel. Survey: Information ﬂow on trees. arXiv:math/0406446, June 2004.

[MS10]

[MS15]

C. Mathieu and W. Schudy. Correlation clustering with noisy input. In Proceedings
of the Twenty-First Annual ACM-SIAM Symposium on Discrete Algorithms (SODA),
pages 712–728, 2010.

A. Montanari and S. Sen.
arXiv:1504.05910, November 2015.

Semideﬁnite Programs on Sparse Random Graphs.

[PW15] W. Perry and A. S. Wein. A semideﬁnite program for unbalanced multisection in the

stochastic block model. arXiv:1507.05605, July 2015.

[Sly11]

A. Sly. Reconstruction for the Potts model. The Annals of Probability, 39(4):1365–1406,
July 2011.

A Explicit Computation of the Lower Bound

Here we compute an explicit lower bound on the separation k6
k′(k, ε)6 between the average
branching factor in the random model and that of the six-level-periodic branching rule described
at the end of Section 4. We will need to assume k

≥
Fix k, ε, δ. We will further modify the six-level-periodic rule by considering a node to be good
if it has three children of degree not equal to 2 and we disregard the contribution of the parent to
this rule, to simplify the computation; this only further reduces cutting.

−

9.

In this one-period tree, the ‘base’ levels are 0 (the root) and 6 (the leaves), and the ‘marking’
level is level 3. In expectation, there are k2 nodes on level 2. Let us focus on the subtree descending
from one of these nodes. Let p = P[Pois(k) = 1]; then by Poissonization, before cutting, each node
v on level 2 gives birth to Pois(kp) degree-2 children and Pois(k(1
p)) degree-not-2 children. In
particular, v is good with probability P[Pois(k(1
3]. Supposing that this is the case, let
us see how many leaves get cut. In expectation, v has kp degree-2 children on level 3, each with
exactly one child on level 4, which is good with probability P[Pois(k(1
3]. In that case,
p))
it gives rise to kp + E[Pois(k(1
= 1] leaves, in expectation; but the level-3
p))
degree-2 node and its subtree gets cut with probability δε2.

3] E[Pois(k)

| ≥

p))

−

≥

≥

−

−

−

| 6

31

The expected number of leaves cut in the subtree descending from a level-0 node is:

k6

−

(k′)6

k2
≥
·
= k3pδε2

P[Pois(k(1

p))

−

P[Pois(k(1

3]2

≥
p))

·
Whenever k
9 and k(1
deﬁnition (1) of δ). In this range we have

≥

−

−

≥

·

kp
3]2

·

·

2ε)2 > 1, we must have ε

(kp + E[Pois(k(1

δε2
(kp + E[Pois(k(1

−
p))

·

p))

3] E[Pois(k)

= 1])

| ≥
3] E[Pois(k)

| 6
= 1]).

−

| ≥

1
3 and δε2 = (1

2ε)2

−

≥

≥

| 6
1
k (from the

k6

−

(k′)6

k2p

·

≥

P[Pois(k(1

p))

3]2

·

≥

−

(kp + E[Pois(k(1

p))

−

| ≥

3] E[Pois(k)

= 1]) ,
| 6

K

(k).

Now, choosing any ε such that

(k6

− K

(k))1/6 <

(1

1
2ε)2 < k,

−

the semirandom model will be impossible while the random model is possible. This concludes our
explicit computation of a separation. We have not made any attempt to optimize it, and that
remains an interesting open question for future work.

B Proof of Lemma 5.8

Recall that m is the number of isolated tagged nodes in G, and g is the number of good nodes.
Let α be the number of excess +1 spins among the isolated tagged nodes, i.e. there are m/2 + α
tagged isolated nodes of spin +1 and m/2
1. Similarly let β be the number of excess
α of spin
+1 spins among the good nodes. Write α = αA + αBC and β = βA + βBC , where αA denotes the
number of excess +1 spins among the isolated tagged nodes of A, and likewise for the others. We
will need a number of results on the sizes of these values.

−

−

Lemma B.1. We have the following results a.a.s.

= O(n1/8),

= O(n1/2 log5 n),

• |

• |

βA|
αA|
,
|
βBC |
,
αBC |
|
m, g = Θ(n).

•

, which is O(n1/8) by assump-
Proof. The ﬁrst result is easy: αA and βA are bounded above by
A
|
|
tion. We will establish the remaining concentration results through the bounded diﬀerences method;
however, this bounded diﬀerence property will require controlling the maximum degree of a node.
Note that a.a.s. every node of Gpre (and thus G) has degree at most log n, by a Chernoﬀ-and-union
argument. Let trunc(Gpre) denote the graph obtained from Gpre by simultaneously removing all
edges incident to nodes of degree larger than log n. Note now that the radius-4 neighborhood of
any node in trunc(Gpre) has size at most O(log4 n).

C

Let

(σ, Gpre) denote the census (number of +1 spins minus number of

1 spins) among those
nodes that are ‘cuttable’ in trunc(Gpre), i.e. nodes v that are degree-2 in trunc(Gpre), with two
opposite-sign (to v) neighbors that each have at least 3 non-degree-2 neighbors. This property
of v depends only on the radius-3 neighborhood in trunc(Gpre). Given any vertex u in Gpre, if
we add or remove any number of edges incident to u in Gpre, we can only change
(σ, Gpre) by
at most O(log4 n), as we can only change the ‘cuttable’ status of the previous and new radius-4
neighborhoods of v in trunc(Gpre). (Here we need radius 4 instead of 3 because by changing edges
incident to v we can push the neighbors of v over the (log n)-degree cutoﬀ.) This constitutes a
bounded diﬀerences property for the function

−

C

.

We now apply the following concentration inequality:

C

32

Lemma B.2. Let F be a function of a random graph of size n with independent edges (not neces-
sarily identically distributed). Suppose that, when we add or remove any number of edges incident
to any given vertex v, the value of F changes by at most c. Then

P[
F
|

−

E[F ]

|≥

λc√n]

2 exp(

−

≤

λ2/2).

|C| ≤

This result is classical, and is based on applying Azuma–Hoeﬀding to a vertex exposure mar-
tingale; see for example Chapter 7 of [AS00]. Note that E[
] = 0 due to spin-reversal symmetry. It
C
√n log5 n a.a.s. Moreover, trunc(Gpre) = Gpre a.a.s., so that the true census of
follows that
cuttable nodes in Gpre is at most √n log5 n a.a.s. Our adversary turns a subset of these cuttable
nodes into isolated tagged nodes, choosing each independently with probability δ; applying Ho-
1 cuttable nodes, we see that the census of tagged
eﬀding’s inequality separately to the +1 and
isolated nodes in G is O(√n log5 n) a.a.s. But this census equals αA + αBC , and
O(n1/8), so
= O(√n log5 n) a.a.s. The same argument applies (with a strict subset of the tech-
we have
αBC |
|
nical mess) to show that βBC = O(√n log5 n), and that m and g concentrate within O(√n log5 n)
of their expectations.

αA| ≤
|

−

It is also straightforward to see that E[m] and E[g] are Θ(n); this amounts to showing that
the probability of any vertex being tagged isolated in G, or good in G, is bounded above zero as
n
. But these two properties are local, depending only on a neighborhood in the graph, so
this is clear. This completes the proof of the concentration results (Lemma B.1).

→ ∞

Now we proceed to the proof of Lemma 5.8. We need to show that for a.a.e. (σBC , G), it holds

for all σA, σ′A that

g/2+βA+βBC
2
g/2+β′
A+βBC
2

(cid:0)

m/2

−

αA

−

αBC

g/2

−

m/2
(cid:1)

−

α′

A−

αBC

g/2

−

(cid:0)

−

βA
2
β′
A−
2

βBC

m/2+αA+αBC

m/2+α′

A+αBC

βBC

(cid:1)

= 1 + o(1).

(cid:0)
We will need suﬃciently tight asymptotics for powers of binomials in this regime:

(cid:1)

(cid:1)

(cid:0)

m/2

−

αA

−

αBC

g/2 + βA + βBC
2
αA −
αA −

αBC

αBC

log

(cid:18)
m
2 −
m
2 −
m
2 −

(cid:16)

(cid:16)

=

=

=

(cid:19)

log

(cid:17) (cid:16)

(cid:16)
2 log

(cid:17) (cid:18)

αA −

αBC

2 log

+

(cid:16)

(cid:17) (cid:18)
from the Taylor series for the logarithm,

g
2
g
2
g
2

+ βA + βBC

+ log

1 +

(cid:18)
βA + βBC
g/2

+ log

g
2
(cid:17)
(cid:16)
βA + βBC
g/2
(cid:19)
βA + βBC −
g/2

+

+ βA + βBC −
+ log

1 +

1

log 2
−
(cid:17)
(cid:17)
1
βA + βBC −
g/2

(cid:18)

1

−

4

β2
BC
g2 −

log 2

(cid:19)

−

(cid:19)

log 2

+ O(n−

3/8 log5 n),

(cid:19)

m
2 −

=

(cid:16)

αA −

αBC

2 log

(cid:17) (cid:16)

g
2 −

log 2

+

(cid:17)

m
2

(cid:18)

2βA + 2βBC −
g/2

1

4

β2
BC
g2

−

2

αBC βBC
g/2

−

(cid:19)

+ O(n−

3/8 log5 n).

33

We now apply these asymptotics to the task at hand:

g/2+βA+βBC
2
g/2+β′
A+βBC
2

(cid:0)

(cid:1)

m/2

−

αA

−

αBC

m/2

−

α′

A−

αBC

g/2

−

g/2

−

(cid:0)

−

βA
2
β′
A−
2

βBC

m/2+αA+αBC

m/2+α′

A+αBC

βBC

(cid:1)

log

=

m
(cid:0)
2 −
m
2
m
2 −
m
2

(cid:16)

(cid:16)

(cid:16)
+

−

−

(cid:16)
+ O(n−

αA −

(cid:1)
αBC

2 log

(cid:17) (cid:16)

+ αA + αBC

2 log

(cid:17) (cid:16)

(cid:17) (cid:16)

(cid:17) (cid:16)

α′A −

αBC

2 log

+ α′A + αBC

2 log

3/8 log5 n)

log 2

+

g
(cid:0)
2 −
g
2 −
g
2 −
g
2 −

(cid:17)
log 2

(cid:17)

log 2

(cid:17)

log 2

(cid:17)

m
(cid:1)
2

+

−

−

(cid:18)
m
2
m
2
m
2

1

−

2βA + 2βBC −
g/2
2βA −

−
2βBC −
g/2
2β′A + 2βBC −
g/2
2β′A −

2βBC −
g/2

−

1

(cid:18)

(cid:18)

(cid:18)

β2
BC
g2

4

1

−

(cid:19)
β2
BC
g2

4

β2
BC
g2
(cid:19)
β2
BC
g2

4

−

4

−

−
1

2

αBC βBC
g/2

2

αBC βBC
g/2
αBC βBC
g/2
αBC βBC
g/2

−

(cid:19)

+ 2

+ 2

(cid:19)

= O(n−

3/8 log5 n) = o(1),

as every non-error term cancels. The result now follows: if the logarithm of an expression is o(1)
then the expression itself is 1 + o(1).

C Proof of Equation (3)

i xi = 1 and

Enumerate all possible values E1, . . . , Em for (σD, G). Let xi = P[Ei] and let yi = P[Ei and σ
i yi = P[σ
We know
some ǫ2 = o(1) we have yi
1
xi ≥
−
−
P
drawn proportional to xi. If I is the set of i for which yi/xi ≥
means
yi
xi ≤

Ω].
ǫ1 for some ǫ1 = o(1). We want to show that for
o(1); here the probability is over i
I xi. This
1
∈

−
ǫ2 with probability p = 1

ǫ2, we have p =

Ω] = 1

p + (1

ǫ1 =

yi
xi

yi
xi

yi =

p)(1

ǫ2)

P

P

xi

xi

xi

−

+

=

−

−

−

∈

∈

1

i

Xi
. We need p = 1

Xi

and so p
and ǫ1

1
≥
0, i.e. ǫ2 goes to 0 slower than ǫ1 does.
ǫ2 →

−

−

ǫ1
ǫ2

I
Xi
∈

I
Xi /
∈

o(1) and ǫ2 = o(1) so it suﬃces to take any ǫ2 such that ǫ2 →

0

D Extensions to Dissortative Models

Throughout this paper, we have assumed a > b; such block models are often called ‘assortative’.
However, everything does carry through equally in the ‘dissortative’ case b > a. Here we brieﬂy
sketch the relevant changes.

Our semirandom models are certainly designed for an assortative model, and are entirely too
powerful in the dissortative case — for example, by randomly adding and removing edges appro-
priately, the current semirandom block model can simulate the Erd˝os–R´enyi distribution G(n, k)
when starting from any dissortative model, which clearly reveals no community structure. Instead,
the semirandom model in the dissortative case should add edges between communities, and remove
edges within communities, so that these monotone changes are aligned with the latent structure
to be recovered. Similarly, the semirandom tree models are able to cut or replace any subtree that
follows a same-spin edge.

This leads to many sign changes throughout the paper. We can still couple the resulting graph
neighborhoods to a tree distribution; although this tree distribution might look quite diﬀerent at a

34

ε, by ﬂipping
glance, it couples perfectly with an assortative tree model, corresponding via ε
all spins at odd levels of the tree. Thus we obtain a random vs. semirandom separation for the
dissortative tree model. Our recovery results for recursive majority in Section 8 carry through
this coupling also, guaranteeing robust recovery by recursive anti-majority in the dissortative tree
model.

7→

−

1

Much of the “no long-range correlations” argument of Section 5 carries through unchanged, but
precursors Gpre of G now reconnect tagged isolated nodes with two same-spin good nodes. Hence
the new formula for

is

L(σ, G)
|
|

L(σ, G)
|
|

=

g/2 + β
2

(cid:18)

(cid:19)

m/2+α

g/2
−
2

(cid:18)

β

m/2

−

α

,

(cid:19)

which still satisﬁes Lemma 5.8, by negating each β variable everywhere in the proof in Appendix B.
So we also obtain a random vs. semirandom separation in the dissortative block model.

The SDP upper bounds in Section 7 carry through with very few changes of sign. One veriﬁes
that the unchanged reference objective a
where S is the matrix form
2n σσ⊤ does maximize
−
of any semirandom change as redeﬁned above. A sign does ﬂip in the proof of Proposition 7.9: we
now have

S,
h

−i

b

γv =

b

a
−
2 −

O(log2 n/√n).

Overall we obtain the same guarantees on semirandom partial recovery as in the assortative model,
requiring b > 20 rather than a > 20.

35

