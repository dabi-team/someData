2
2
0
2

r
p
A
1
1

]

Y
S
.
s
s
e
e
[

3
v
7
4
3
1
0
.
1
0
2
2
:
v
i
X
r
a

Learning Differentiable Safety-Critical Control using Control Barrier
Functions for Generalization to Novel Environments

Hengbo Ma*, Bike Zhang*, Masayoshi Tomizuka, and Koushil Sreenath

Abstract— Control barrier functions (CBFs) have become
a popular tool to enforce safety of a control system. CBFs
are commonly utilized in a quadratic program formulation
(CBF-QP) as safety-critical constraints. A class K function
in CBFs usually needs to be tuned manually in order to
balance the trade-off between performance and safety for each
environment. However, this process is often heuristic and can
become intractable for high relative-degree systems. Moreover,
it prevents the CBF-QP from generalizing to different envi-
ronments in the real world. By embedding the optimization
procedure of the exponential control barrier function based
quadratic program (ECBF-QP) as a differentiable layer within
a deep learning architecture, we propose a differentiable safety-
critical control framework that enables generalization to new
environments for high relative-degree systems with forward
invariance guarantees. Finally, we validate the proposed control
design with 2D double and quadruple integrator systems in
various environments.

I. INTRODUCTION

Safety plays a critical role in autonomous systems that in-
teract with people, such as autonomous driving and robotics.
There are several approaches to developing a safe control
strategy, e.g., Hamilton-Jacobi reachability analysis [7] and
model predictive control [18]. However, such methods may
have high computational costs in real-time applications. Con-
trol barrier functions (CBFs) [4] have gained more attention
recently since these methods only depend on the current state
and do not require heavy computation. CBFs are usually
encoded as constraints in a quadratic program (CBF-QP)
for safety-critical tasks [3]. With a properly chosen class
function in CBFs, a system can avoid unsafe sets. Mean-
K
while, it does not reduce the stabilizing performance from
a high-level controller [4]. However, the performance of the
overall controller, which consists of a high-level controller
and CBF-QP, can be easily undermined if the environment
changes. In other words, each safe set in CBFs necessitates
function that maximizes the overall control
a unique class
performance for a speciﬁc environment. In the real world, the
environment information for safety-critical tasks is usually
not fully known a priori, and a system might also face
different environments during its deployment. Thus, it is hard
function for each environment beforehand
to tune a class
to reconcile performance and safety. Moreover, the tuning
process for choosing a class
function becomes tedious
when there are multiple control barrier function constraints

K

K

K

*The authors contributed equally to this work and names are in alpha-

betical order.

H. Ma, B. Zhang, M. Tomizuka and K. Sreenath are with University
of California, Berkeley, CA 94720, USA (e-mail:hengbo ma, bikezhang,
tomizuka, koushils@berkeley.edu).

in the CBF-QP [31], or some are with high relative-degree
in the ECBF-QP [23]. This challenge impedes the progress
towards deploying CBF-based safety-critical controllers in
the real world.

To address this challenge, we investigate how to model
the relation between environment information and safety-
critical control. We propose a learning safety-critical control
framework using an environment-dependent neural network
which satisﬁes the forward invariance condition. Thanks to
the development of differentiable convex optimization [1],
we can enable the learning procedure in an end-to-end style.
After ofﬂine training, we can directly deploy the proposed
safety-critical control framework in different environments
without any adaption.

A. Related Work

1) Safe environment generalization: Cluttered environ-
ments have been considered in the safe control literature. A
provably approximately correct-bayes framework is proposed
to synthesize controllers that provably generalize to novel
environments in [21]. A control Lyapunov function and
control barrier function based quadratic program (CLF-CBF-
QP) is utilized with a high-level path plan to navigate through
obstacle-scattered environments in [8]. Moreover, for hostile
environments with adversarial agents, a probabilistic tree
logic method is proposed in [11] to assure safety. Safe gener-
alization problem with control barrier functions is considered
with a weighted mixture of existing controllers in [28]. Yet
the generalization ability is largely limited by the number of
existing controllers. With the help of reinforcement learning,
safe environment adaptation is studied through a risk-averse
approach in [36]. However,
this approach can only pro-
vide relative safety instead of safety guarantee. For robotic
applications, bipedal robot walking on stepping stones is
addressed in [22] using a robust control barrier function
method, where the distances between adjacent stones are
different at each step. Predictive control with CBFs tackles
the safe car overtaking problems in [35], where different
leading cars serve as novel environments. Our approach
function to generalize
adopts a different way using class
a controller to enforce safety under different environments.
2) Safe learning control: A safe reinforcement learning
(RL) framework under constrained Markov decision process
is proposed in [10] using a Lyapunov based method. A
learning-based control barrier function from expert demon-
stration is proposed in [25] to ensure safety. In [27], a CBF is
created using RL for risk mitigation in adversarial environ-
ments. In [9] and [29], they address the model uncertainty

K

 
 
 
 
 
 
problem by learning CBF constraints. In [12], the authors
design a learning robust control Lyapunov barrier function
that can generalize despite model uncertainty. A model-
free safe reinforcement learning is studied by synthesizing a
barrier certiﬁcate and querying a black-box dynamic function
in [37]. A game theoretic approach is adopted in [30] to
reduce conservatism while maintaining robustness during
human robot interaction. Differentiable optimization layers
have emerged as a new approach for safe learning control
recently. In [24], a differentiable layer is applied to control
barrier function based quadratic program in order to enhance
the recursive feasibility, where the parameters are adapted
online. In [15], safety is framed as a differentiable robust
CBF layer in model-based RL. We also utilize the differ-
entiable optimization layer as a tool. However, we focus on
generalizing the safety-critical control to novel environments.

B. Contributions

The contribution of this paper is as follows:

•

•

•

•

We present an approach to generalizing safety-critical
control to novel environments by integrating control
barrier functions and differentiable optimization.
We introduce a neural network based ECBF-QP and
formulate the safety-critical control as a differentiable
optimization layer.
the proposed neural network module
We show that
based on the exponential control barrier function assures
the forward invariance of a safe set.
We numerically validate the proposed learning control
design using systems with different relative-degrees and
novel environments with randomly generated obstacles.

C. Organization

This paper is organized as follows: in Sec. II, we introduce
the background of control barrier functions and differen-
tiable optimization. The problem formulation is illustrated
in Sec. III, where we motivate the formulation with a simple
case study. Then, in Sec. IV, we present the methodology
of learning differentiable safety-critical control using control
barrier functions. In Sec. V, we test the proposed control
logic on 2D double and quadruple integrator systems with
different environment settings. Secs. VI and VII provides
discussion and concluding remarks.

II. BACKGROUND

Throughout this paper, we will consider a nonlinear con-

trol afﬁne system:

˙x = f (x) + g(x)u,

(1)

where x
Rm is the control input, and f :
are locally Lipschitz continuous.

∈ X ⊂

Rn represents the state of the system, u

Rn and g :

∈
Rm

X →

X →

A. Control Barrier Functions

→

[0,

), a > 0 is said to belong to class

Deﬁnition 1. [19] A Lipschitz continuous function α :
if it
[0, a)
is strictly increasing and α(0) = 0. Moreover, α is said to
belong to class
, and
α(r) =
limr

if it belongs to class

, a =

∞

∞

K

K

→∞

K∞
.
∞

Deﬁnition 2. [2, Def. 2] Consider a continuously differen-
deﬁned as
tiable function h :
C
→
the superlevel set of h,
, then h is
0
x
}
{
a control barrier function (CBF) if there exists an extended
class

function α such that for the control system (1):

R and a set
: h(x)

X ⊂
C

Rn
=

∈ X

≥

K∞

sup
Rm
u

∈

[Lf h(x) + Lgh(x)u]

α(h(x)).

≥ −

(2)

∂

= 0 for
If h is a control barrier function on
, any Lipschitz continuous controller satisfy-
all x
C
ing (2) renders the set
forward invariant [2, Thm.2].
By incorporating (2) as a constraint, a quadratic pro-
gram based safety-critical controller is proposed in [3]:

∂x (cid:54)

X

∈

C

and ∂h

CBF-QP:

u∗(x) =

s.t.

u
(cid:107)

−

uperf

2
(cid:107)

arg min
Rm

∈

u
Lf h(x) + Lgh(x)u

α(h(x)),

≥ −

(3a)

(3b)

where uperf is the reference control input that can be from
a high-level performance controller, which is expected to
achieve the control objective. For instance, model predictive
control
is a popular choice as a high-level performance
controller [26]. In the context of safety-critical control, a
control Lyapunov function is often used in a quadratic
program formulation (CLF-QP) to realize stability.

K

K∞

Remark 1. In Deﬁnition 2, an extended class
function is
required for CBF. Here, we restrict ourselves to a subclass:
function, which can facilitate our learning algorithm.
class
Typically, α(x) is simpliﬁed as αx, with α being a positive
constant, which we term as a linear class
function. Pre-
vious work [24], [33], [34] have investigated how to adjust
function in order to improve the feasibility. In
the class
this work, we focus on learning a neural network based class
function to safely generalize to different environments.

K

K

K

The CBF constraint in (3b) has been so far assumed to be
relative-degree one, which typically does not held for most
safety-critical constraints in robotic systems [16]. A special
type of CBFs called exponential control barrier functions
(ECBFs) has been introduced to enforce arbitrarily high
relative-degree CBF constraints in [23].

Deﬁnition 3. [23, Def. 1] Consider a r-times continuously
deﬁned
differentiable function h :
C
X ⊂
, then
as the superlevel set of h,
0
=
C
}
≥
h is an exponential control barrier function (ECBF) if there
Rr such that for the system (1):
exists a row vector Kα ∈
[Lr

Rn
→
x
∈ X
{

R and a set
: h(x)

f h(x) + LgLr

Kαηb(x),

f h(x)u]
−

(4)

1

≥ −

sup
Rm
u

∈

for

x

∀

x

∈ {

∈

Rn

h(x)
|


, with
0
}

≥
h(x)
˙h(x)
¨h(x)
...
1)(x)

−










h(r

=










h(x)
Lf h(x)
L2
f h(x)
...
1
f h(x)
−

Lr

ηb(x) =

















.

(5)

f h(x) + LgLr
We deﬁne µ = Lr
dynamics of h(x) can be written as the linear system

f h(x)u, then the above
−

1

˙ηb(x) = F ηb(x) + Gµ,
h(x) = Cηb(x),

where

F =










0 1 0
1
0 0
...
...
...
0
0 0
0
0 0

C = (cid:2)1

0

. . .

, G =











0
0


...



0

1

,


0
0


...



1

0

. . .
. . .
. . .
. . .
. . .
0(cid:3) .

(6)

(7)

≥ −

Kαηb(x), with (F

If µ
total negative, then we can guarantee that h(x0)
h(x(t))
Let

≥
pi be the negative real eigenvalues of (F

0 where x0 is the initial condition.

t
∀

0,

≥

≥

−

GKα) being Hurwitz and

0 =

⇒

−

We can then deﬁne a family of functions vi :
with corresponding superlevel sets

GKα).
R

−
Rn

X ⊂

→

v0(x) = h(x),
v1(x) = ˙v0 + p1v0(x),

...
vr(x) = ˙vr

x : v0(x)
{
x : v1(x)
{

≥

≥

,
0
}
,
0
}

(8)

Ci,
C0 =
C1 =
...
Cr =

1 + prvr

,
}
where
as deﬁned in
Deﬁnition 2 for a relative-degree one CBF. Then, we have:

C0 plays the role of the safe set

x : vr(x)
{

1(x),

≥

C

0

−

−

Theorem 1. [23, Thm.2] A valid exponential CBF should
satisfy two conditions: suppose Kα is chosen such that pi >
˙vi−1(x0)
0 and the eigenvalues
vi−1(x0) , then (9b)
guarantees h(x) is an exponential control barrier function.

pi satisfy pi ≥ −

−

Given an ECBF, we can extend the CBF-QP in (3)
to enforce high relative-degree safety-critical constraints:

ECBF-QP:

u∗(x) = arg min

u

uperf

2

Rm

−
(cid:107)
u
∈
f h(x) + LgLr
s.t. Lr
f h(x)u
−

(cid:107)

1

(9a)

(9b)

Kαηb(x),

≥ −

where uperf
is the reference control input. Note that the
control barrier function constraint (9b) can be extended to
multiple constraints in order to account for different safety
criteria. Furthermore, a general formulation of high order
control barrier functions can be seen in [32].

Remark 2. In ECBF-QP (9b), due to the relation between
pi, Kαηb can
the coeeﬁcients in Kα and the eigenvalues

−

(a) Environment 1

(b) Environment 2

Fig. 1: Motivating example for safety-critical control for
generalization to novel environments using a 2D double
integrator. A hand-tuned Kα for Environment 1 in (a) is used
in the novel Environment 2 in (b). As can be seen, this results
in a trajectory with a larger deviation from the obstacle in
Environment 2. Thus, a well-tuned Kα for one environment
does not necessarily generalize to a different environment.

h(x)

i=1[Lf + pi]

be reformulated as Πr
f h(x). This will
be used to develop our differentiable safety-critical control
formulation. Note that Lf here is the Lie derivative operator
s.t. Lf ◦
B. Differentiable Optimization

h(x) = Lf h(x) = ∂

∂x h(x)f (x).

Lr

−

◦

A differentiable optimization problem is a class of opti-
mization problems whose solutions can be backpropagated
through. This functionality enables an optimization prob-
lem to serve as layers within deep learning architectures,
which can encode constraints and complex dependencies
through optimization that traditional convolutional and fully-
connected layers usually cannot capture [6]. Some successful
differentiable layer examples include differentiable model
predictive control [5], [14], Pontryagin’s maximum principle
[17], robust control [13], and meta learning [20], etc. In
this work, we utilize the differentiable optimization layer
presented in [1] for the ECBF-QP.

Remark 3. While CBFs are continuously differentiable
functions [2], here a differentiable safety-critical control
using CBF does not mean the CBF itself is differentiable but
rather that the backpropagation can go through the CBF-QP
differentiable optimization layer.

III. PROBLEM STATEMENT

Having established the background of CBFs and differen-
tiable optimizations, we now present our problem formula-
tion for generalizing to novel environments.

A. Motivating Example

Using a 2D double integrator as an illustrative example,
we design a linear quadratic regulator (LQR) to drive the
system to a goal location while avoiding different obstacles
using the ECBF-QP in (9). The LQR controller serves as
the reference performance controller uperf. The simulation
results are demonstrated in Fig. 1. Note that the Kα for
ECBF-QP is tuned manually in Fig. 1(a), which leads to a
short and smooth trajectory, i.e., a smooth trajectory that goes
around the obstacle with minimal detour from a straight-line

trajectory from start to goal. However, the ECBF-QP with
the same Kα results in a large detour in Fig. 1 (b) in a
different environment. While larger detours are conservative,
they potentially require more control effort, and result in
energy inefﬁcient motions. The motion in Environment 2
could be shorter by getting closer to the obstacle. This
example demonstrates that the Kα plays an important role in
generating desirable trajectories in different environments.

A. Differentiable Safety-Critical Control using CBFs

We formulate our differentiable safety-critical control
based on exponential control barrier functions in (9). Dif-
ferentiable CBFs have been used in [24] and [15]. How-
ever, they used systems with relative-degree one or solved
a relative-degree two system using a cascaded approach.
to a general formulation as follows:
Here, we extend it

Remark 4. In order to get a trajectory with desired proper-
ties, e.g., smoothness and minimum distance, it is necessary
to choose a proper Kα using ECBF-QP. Moreover, certain
ﬁxed Kα that works in a particular environment could
actually fail in a different environment, resulting in violation
of the safety constraint h(x)

0.

≥

B. Problem Formulation

Building upon the motivating example, we are motivated
function in CBF-QP or Kα in ECBF-
to optimize the class
QP with respect to different environments, which can result
in a safe trajectory satisfying a user-deﬁned metric.

K

To this end, we represent the Kαηb(x) in (4) with a neural
network parameterized with θ. Πθ(uperf, e, x0) represents the
solution of ECBF-QP mentioned in (9b). Such an ECBF-QP
can be embedded as a layer in a deep learning pipeline by
using differentiable convex optimization technique. Then we
in an episodic setting. The
minimize a performance cost
L
formulation is given as follows:

arg min
θ

Ex0

P0,e

∼

∼

Pe [

L

(τ, e, x0)]

s.t. u = Πθ(uperf(x), e, x0),
˙x = f (x) + g(x)u,

(10)

where e is an environment sampled from a distribution of
environments Pe, e.g., e consists of center and radius of the
obstacle. x is the state where we evaluate cost at each time
step, and x0 is the initial state which is sampled from a dis-
tribution P0. Note that
is the cost along a trajectory instead
of the cost at each time step, and τ represents the trajectory
with time horizon T . uperf is the performance control input
provided by a high-level performance controller. Once the
training procedure is done ofﬂine, we can deploy the neural
network based controller Πθ to novel environments.

L

IV. METHODOLOGY

Having seen the problem formulation, we will next intro-
duce how to enable the generalization to novel environments
via learning differentiable ECBF-QP.

The overall control architecture is shown in Fig. 2, which
basically includes two parts: a performance controller and
a differentiable ECBF-QP safety ﬁlter. The performance
controller is mainly responsible for achieving the control
objective, and the differentiable ECBF-QP serves as a safety
ﬁlter, which will be explained in detail in this section.

Differentiable ECBF-QP:

Πθ(uperf, e, x0) = arg min
∈

Rm,δ
u
∈
1
f h(x)u
−

s.t. Lr

f h(x) + LgLr

u

uperf

2 + ζδ2
(cid:107)

−

R (cid:107)

αθ(e, x0, ηb(x))

≥ −

δ2,
(11)

−

where, Πθ(uperf, e, x0) is the safe policy ﬁltered by the
ECBF-QP and conditioned on the high-level performance
control
information e.
αθ(e, x0, ηb(x)) is denoted by α-net, where θ represents
parameters of the network.

input uperf and the environment

As we will see next, the α function is a linear function (of
ηb) that encodes an ECBF constraint within the differentiable
ECBF-QP so as to deal with high relative-degree safety
constraints, which are common in many robotic applications.
We include a slack variable δ which guarantees that such an
optimization is feasible during the training procedure, and ζ
is a hyperparameter. Note that we do not use δ as in (11)
during the test time.

B. The Structure of α-net

Exponential control barrier function provides a formal
structure to guarantee safety with a vector parameter Kα.
In general, it is not easy and probably time-consuming to
ﬁnd the best Kα directly in order to generalize to novel
environments. We thus encode the structure of exponential
CBF into our neural network. As noted in Remark 2, our
formulation is shown as follows

αθ(e, x0, ηb(x)) =

r
(cid:89)

[Lf + pi(e, x0; θ)]

h(x)

◦

−

Lr

f h(x),

i=1

(12)
where Lf is the lie derivative operator as mentioned in Re-
mark 2. Notice that for the ECBF, the right side of (9b) only
includes the lie derivative with respect to f . The function
Rr outputs [p1, p2, . . . , pr], and pi(e, x0; θ)
p(e, x0; θ)
represents pi as deﬁned in (8). We use the following neural
network structure:

∈

p(e, x0; θ) = ReLU(

m
(cid:89)

k=0

σ(Wklk)

−

b(x0)) + b(x0),

bi(x0) = ReLU(

−

l0 = [e, x0],

˙vi
vi

1(x0)
1(x0) −

−

−

(cid:15)) + (cid:15), i = 1, . . . , r,

(13)

Rr, and it
where bi(x0) is the i-th element of b(x0)
represents the bounds of pi=1...r in Thm.1. The parameter
θ represents the weights
. lk represents
the outputs of the k-th layer of the neural network. We
concatenate the environment information e and initial state

W0, W1, . . . , Wm}
{

∈

Fig. 2: The overall framework of the proposed approach, which includes two main components: a performance controller
and a differentiable CBF-QP. The novel environment information, e, is an input to the performance controller and α net.
The performance loss computed along a trajectory will be backpropagated through the α net, then the α net outputs the
parameters to construct the class

function.

K

x0 together as the input l0 and choose the ReLU function as
) being any activation function. Then,
the activation, with σ(
·
we randomly initialize the neural network parameters with
positive numbers.

Theorem 2. Given the function p(e, x0; θ) deﬁned in (13),
pi satisﬁes the conditions in Thm.1. Thus, h(x) is an expo-
nential CBF and

C0 in (8) is forward invariant.
˙vi−1(x0)
vi−1(x0) , (cid:15)
}
max

Proof. Since bi(x) = max
bi(x0), we have pi(e, x0, θ)
{−
that pi(e, x0, θ) satisﬁes i) pi(e, x0, θ)
pi(e, x0, θ)
≥
From [23, Thm.1], the set
is a valid exponential CBF.

≥
˙vi−1(x0)
. It follows
vi−1(x0) , (cid:15)
}
˙vi−1(x0)
vi−1(x0) and ii)
≥ −
(cid:15) > 0, which are the conditions in Thm.1.
C0 is forward invariant given h(x)

and pi(e, x0, θ)

{−

≥

C. Loss Function

In general, the loss function

(τ, e, x0) can be designed
perf. In our work,
with any performance evaluation metric
we propose a loss function which includes two components:

L

L

(τ, e, x0) =

L

L

perf(τ, e, x0) + λδ

T
(cid:88)

t=1

δ2
t ,

(14)

where T is the number of simulation time steps with a
ﬁxed simulation interval ∆t, τ is the simulated trajectory
represented by [x0, x1, . . . , xT ]. The coefﬁcient λδ is for
slack variable penalty. Notice that we use a slack variable
δ in (11) to make sure that the optimization program will
not be interrupted by the infeasibility issue of solving the
ECBF-QP. Here, δt represents the value of the slack variable
δ at each time step. However, the gradient descent of the
neural network αθ may lead to a solution such that δt is large.
Hence, we include the penalty of δt in the loss function. The
ideal situation is that δt is zero.

D. Algorithm

The training algorithm is shown in Algorithm 1. The input
is a distribution of environments Pe, and the output is the
network weights θ of the α-net. For each iteration, we sample

n environments and rollout trajectories τ , then the weights
of the neural network are updated after each iteration.

Algorithm 1: Training algorithm
1 ] Input: Environment distribution Pe, initial state

distribution P0, simulation time interval ∆t,
simulation time horizon T .

Output: The network weights θ.
number of iteration do

2 while t
3

for i=1:n do

≤
Sample ei ∼
Collect the trajectory τi by using the designed
controller.
Update θt →
θt

ei Lθ(τi, ei, x0,i)

Pe, x0,i ∼

n ∇θ

1 −

λ 1

P0

(cid:80)

−

4

5

6

When the task is obstacle avoidance, we can iteratively use
the learned policy for m obstacles during the test time. The
differentiable ECBF-QP in (11) becomes

Πθ(uperf, e, x0) = arg min

u

Rm (cid:107)
u
∈
1
f hj(x)u
−

uperf

2

(cid:107)

−

αθ(ej, x0, ηb,j(x)),

≥ −

s.t. Lr

f hj(x) + LgLr
j = 1, . . . , m,

(15)
where hj represents the j-th exponential CBF. The environ-
ment e can have multiple obstacles and each of them ej can
be captured by an ECBF constraint.

V. RESULTS

After developing our methodology for learning differen-
tiable safety-critical control using CBFs, we now present
the simulation results of our proposed framework using 2D
double and quadruple integrator systems.

A. Simulation Setup

We focus on the collision avoidance problem. We set up
different environments with different obstacles, which are

Performance ControllerCBF ControllerDynamic SystemPerformanceLossBackpropagationLQR/CLF-QP˙x=f(x)+g(x)u<latexit sha1_base64="oSz8YAcHdbNzrzAiDydF5AYwMBM=">AAACaHicbVDLbtNAFJ2YVwmvFBaA2IyaVipCiuwKRDeVKth0waJITRvJsaLr8XU66jysmevQyDJfwxb+h1/gKxinWZSWK43m6Nx77uPklZKe4vh3L7pz9979BxsP+48eP3n6bLD5/NTb2gkcC6usm+TgUUmDY5KkcFI5BJ0rPMsvPnf5swU6L605oWWFmYa5kaUUQIGaDV5tTwtLzWXLD3i5e/mWv+Pz8NXbs8EwHsWr4LdBsgZDto7j2WZvP7QStUZDQoH3aRJXlDXgSAqFbX9ae6xAXMAc0wANaPRZszqh5TuBKXhpXXiG+Iq9rmhAe7/UeajUQOf+Zq4j/5dLayr3s0aaqiY04mpQWStOlnd+8EI6FKSWAYBwMuzKxTk4EBRc6+9cH9Nt5isUbaA9kgZpOir9IudAtUN/cIKTrDlCtcDQB8LJDg1+E1ZrMEUzXQRtmmTNNLeq6K6xqhkmbRu8Tm46exuc7o2S96MPX/eGh5/Wrm+wN2yL7bKEfWSH7IgdszET7Dv7wX6yX70/0SB6Gb2+Ko16a80L9k9EW38BBva6SQ==</latexit>···<latexit sha1_base64="I4M1CeAeV+VimsljCCP1o/1iROo=">AAACVnicbVBNT9tAEF27UCBAG9ojF4uAxCmyUatyqYTaCwcOVCIQybbQeD0JK/bD2h2niiz/iV7bH1b+DGIdcuBrpJWe3szbmfeKSgpHcXwXhO9WVt+vrW/0Nre2P3zs73y6dKa2HEfcSGPHBTiUQuOIBEkcVxZBFRKvitufXf9qhtYJoy9oXmGuYKrFRHAgT433M14acvvX/UE8jBcVvQbJEgzYss6vd4LjrDS8VqiJS3AuTeKK8gYsCS6x7WW1wwr4LUwx9VCDQpc3i4Pb6MAzZTQx1j9N0YJ9qmhAOTdXhZ9UQDfuZa8j3+qlNU2O80boqibU/HHRpJYRmahzH5XCIic59wC4Ff7WiN+ABU4+o97B0zXdZa5C3nraISkQuqPSMzEFqi267xc4zptTlDP0/4C3bFHjb26UAl022cxr0yRvssLIsnNjZDNI2tZnnbxM9jW4PBomX4Zffx0NTn4sU19nu2yPHbKEfWMn7JSdsxHjTLI/7C/7F/wP7sPVcO1xNAyWms/sWYX9B69stiE=</latexit>k1<latexit sha1_base64="VPhPc08TPx1lplv4JV7oeY0LDHg=">AAACU3icbVBNT9tAEF0bKJBC+eiRi9WA1FNkIxBckFB74dADSAQiOVY0Xk/CKvth7Y6DIsu/gWv7w3rob+ml65ADHx1ppac383bmvbyUwlEc/wnCldW1D+sbm52PW9ufdnb39u+cqSzHPjfS2EEODqXQ2CdBEgelRVC5xPt8+r3t38/QOmH0Lc1LzBRMtBgLDuSp/uF0lByOdrtxL15U9B4kS9Bly7oe7QXnw8LwSqEmLsG5NIlLymqwJLjEpjOsHJbApzDB1EMNCl1WL65toiPPFNHYWP80RQv2paIG5dxc5X5SAT24t72W/F8vrWh8ntVClxWh5s+LxpWMyESt9agQFjnJuQfArfC3RvwBLHDyAXWOXq5pL3Ml8sbTDkmB0C2V/hAToMqiu7jFQVZfoZyh/we8ZYsaH7lRCnRRD2demyZZPcyNLFo3RtbdpGl81snbZN+Du+NectI7vTnuXn5bpr7BDtgX9pUl7Ixdsit2zfqMM8Ge2E/2K/gd/A3DcPV5NAyWms/sVYXb/wBcC7SF</latexit>k2<latexit sha1_base64="lflROQD6+ICRQPZ9tEMKLyvp9yc=">AAACU3icbVBNT9tAEF2bj9IUKB9HLhYBqafIjlqVSyXUXjhwAIlAJMeKxutJWGU/rN1xqsjyb+i1/WEc+C29dB1yoMBIKz29mbcz7+WlFI7i+DEI19Y3Nt9tve982N7Z/bi3f3DrTGU5DriRxg5zcCiFxgEJkjgsLYLKJd7lsx9t/26O1gmjb2hRYqZgqsVEcCBPDU5m4/7JeK8b9+JlRa9BsgJdtqqr8X5wNioMrxRq4hKcS5O4pKwGS4JLbDqjymEJfAZTTD3UoNBl9fLaJjr1TBFNjPVPU7RknytqUM4tVO4nFdC9e9lrybd6aUWTs6wWuqwINX9aNKlkRCZqrUeFsMhJLjwAboW/NeL3YIGTD6hz+nxNe5krkTeedkgKhG6p9FJMgSqL7tsNDrP6AuUc/T/gLVvU+JMbpUAX9WjutWmS1aPcyKJ1Y2TdTZrGZ528TPY1uO33ks+9L9f97vn3Vepb7Igds08sYV/ZObtgV2zAOBPsF/vN/gQPwd8wDNefRsNgpTlk/1W48w9d97SG</latexit>Lrfh(x)+LgLr 1fh(x)u  ↵✓(e,x0,⌘b(x))<latexit sha1_base64="wL6KbA9+PzaUFHGHA61u5CIK9v4=">AAACyHicbVFBb9MwFHbCgBHY6ODIxaLd1ImtSiYQuyBNcJlQD0Nat0p1iBznNbXmOMF2ulZWLvxLLvwWnLYSY+NJlj5/731+731OK8G1CcNfnv9o6/GTp9vPgucvdnZfdvZeXemyVgxGrBSlGqdUg+ASRoYbAeNKAS1SAdfpzZc2fz0HpXkpL82ygrigueRTzqhxVNJZ9HoBSSHn0lLBcwlZExwMk+l3hWf9xSF+h4dJjlvCquOoWZM1JiQ4IDn8wMeYUFHNaGKJmYGhTR+O8CIJjzBxt8SmjRMcunICMvvbotdLOt1wEK4CPwTRBnTRJi6SPe+UZCWrC5CGCar1JAorE1uqDGcCmoDUGirKbmgOEwclLUDHdmVRg/cdk+FpqdyRBq/YuwpLC62XReoqC2pm+n6uJf+Xm9RmehpbLqvagGTrRtNaYFPi1m+ccQXMiKUDlCnuZsVsRhVlxv1KsH+3TTuZroA1jtZgCsplS02GPKemVqA/XcI4tucg5uDeoW5lBRJuWVkU1LlL5k47iWJL0lJk7TalsN2oaZzX0X1nH4Krk0H0fvDh20n37PPG9W30Br1FfRShj+gMnaMLNEIM/fa2vB1v1//qV/6tv1yX+t5G8xr9E/7PP8qC2ao=</latexit>EnvironmentsGoalInitialstate↵✓<latexit sha1_base64="6GCI1JBUyOluRwahamkHJh8slsM=">AAACYXicbVBNa9tAEF2rX4n7ZafHXESdQE9GCinNJRDaSw49pBAnBkmY0WpsL9kPsTtyMIt+Sq7tb+q5f6Qrx4c06cDC4715OzOvrKVwlCS/e9Gz5y9evtrZ7b9+8/bd+8Fw78qZxnKccCONnZbgUAqNExIkcVpbBFVKvC5vvnX69QqtE0Zf0rrGQsFCi7ngQIGaDYYHOch6CTOf0xIJ2oPZYJSMk03FT0G6BSO2rYvZsHeSV4Y3CjVxCc5laVJT4cGS4BLbft44rIHfwAKzADUodIXf7N7Gh4Gp4rmx4WmKN+xDhwfl3FqVoVMBLd1jrSP/p2UNzU8KL3TdEGp+P2jeyJhM3AURV8IiJ7kOALgVYdeYL8ECpxBX//DhmG4zVyNvA+2QFAjdUdl3sQBqLLrTS5wW/hzlCsM/EE62qPGWG6VAVz5fBW+WFj4vjay6a4z0o7RtQ9bp42SfgqujcXo8/vzjaHT2dZv6DttnH9knlrIv7Iydsws2YZzdsjv2k/3q/Yl2o0G0d98a9baeD+yfivb/Ap+buV8=</latexit>e<latexit sha1_base64="+HGhWiQsPc1AE14TO9isSQzQj04=">AAACUXicbVBNT9tAEB2bfkDoB7THXqwGpJ4iG7Uql0qovXDogQoCkRwLjdeTsGI/rN1xqsjyT+i1/LCe+lN66zrkQKEjrfT0Zt7OvFfWSnpO099RvPHo8ZOnm1uD7WfPX7zc2X117m3jBI2FVdZNSvSkpKExS1Y0qR2hLhVdlNdf+v7FgpyX1pzxsqZC49zImRTIgTrdo73LnWE6SleVPATZGgxhXSeXu9HhtLKi0WRYKPQ+z9KaixYdS6GoG0wbTzWKa5xTHqBBTb5oV7d2yX5gqmRmXXiGkxV7V9Gi9n6pyzCpka/8/V5P/q+XNzw7LFpp6obJiNtFs0YlbJPeeFJJR4LVMgAUToZbE3GFDgWHeAb7d9f0l/maRBdoT6xRmp7Kv8o5cuPIfzqjSdEek1pQ+AeDZUeGvgurNZqqnS6CNs+KdlpaVfVurGqHWdeFrLP7yT4E5wej7P3ow7eD4dHndeqb8AbewjvI4CMcwTGcwBgEzOEH/ISb6Ff0J4Y4vh2No7XmNfxT8fZf74iz2w==</latexit>x<latexit sha1_base64="maCpslwCmQt8HUyZ7rgLsmDDVbI=">AAACUXicbVBNb9NAEB2br5Ly0cKRi0VaiVNkV0X0UqmCSw8cimjaSI5VjdeTdNX9sHbHKZHln8AVfhgnfgo31mkOpWWklZ7ezNuZ98paSc9p+juKHzx89PjJxtPB5rPnL15ubb8687ZxgsbCKusmJXpS0tCYJSua1I5Ql4rOy6tPff98Qc5La055WVOhcW7kTArkQH3d+bZzsTVMR+mqkvsgW4MhrOvkYjs6mFZWNJoMC4Xe51lac9GiYykUdYNp46lGcYVzygM0qMkX7erWLtkNTJXMrAvPcLJibyta1N4vdRkmNfKlv9vryf/18oZnB0UrTd0wGXGzaNaohG3SG08q6UiwWgaAwslwayIu0aHgEM9g9/aa/jJfk+gC7Yk1StNT+Wc5R24c+cNTmhTtMakFhX8wWHZk6FpYrdFU7XQRtHlWtNPSqqp3Y1U7zLouZJ3dTfY+ONsbZfuj91/2hkcf16lvwBt4C+8ggw9wBMdwAmMQMIfv8AN+Rr+iPzHE8c1oHK01r+Gfijf/AhQbs+4=</latexit>↵(cid:64)(cid:77)(cid:50)(cid:105)<latexit sha1_base64="mAATLfB/LD7XE/aNzmP2IaMec3w=">AAACZXicbVBNb9NAEN2YrxIopHxdOGCRVOJCZFcgekGq4NIDhyI1bSTbisbrSbLqfli740C08o/hCr+IX8DfYJ3mUFpGWunpvXk7M6+spXCUJL970a3bd+7e27nff/Bw99Hjwd6TM2cay3HCjTR2WoJDKTROSJDEaW0RVCnxvLz43OnnK7ROGH1K6xoLBQst5oIDBWo2eD7KQdZLGL0d5YTfyWukdjQbDJNxsqn4Jki3YMi2dTLb6x3mleGNQk1cgnNZmtRUeLAkuMS2nzcOa+AXsMAsQA0KXeE3+7fxfmCqeG5seJriDXvV4UE5t1Zl6FRAS3dd68j/aVlD88PCC103hJpfDpo3MiYTd2HElbDISa4DAG5F2DXmS7DAKUTW3786ptvM1cjbQDskBUJ3VPZFLIAai+7jKU4Lf4xyheEfCCdb1PiNG6VAVz5fBW+WFj4vjay6a4z0w7RtQ9bp9WRvgrODcfpu/P7rwfDo0zb1HfaSvWZvWMo+sCN2zE7YhHHm2Q/2k/3q/Yl2o2fRi8vWqLf1PGX/VPTqL0x5upM=</latexit><latexit sha1_base64="aeqcTleArWB57de/VqRcTMDZQt8=">AAAB+nicbVBNS8NAEN34WetXqkcvwVbwVJIi6rHoxWMF+wFtCJvtpF26+WB3opbYn+LFgyJe/SXe/Ddu2xy09cHA470ZZub5ieAKbfvbWFldW9/YLGwVt3d29/bN0kFLxalk0GSxiGXHpwoEj6CJHAV0Egk09AW0/dH11G/fg1Q8ju5wnIAb0kHEA84oaskzS5XUy3oIj5glIIPJpOKZZbtqz2AtEycnZZKj4ZlfvX7M0hAiZIIq1XXsBN2MSuRMwKTYSxUklI3oALqaRjQE5Waz0yfWiVb6VhBLXRFaM/X3REZDpcahrztDikO16E3F/7xuisGlm/EoSREiNl8UpMLC2JrmYPW5BIZirAllkutbLTakkjLUaRV1CM7iy8ukVas659Wz21q5fpXHUSBH5JicEodckDq5IQ3SJIw8kGfySt6MJ+PFeDc+5q0rRj5zSP7A+PwBkv6UNg==</latexit>uperf(a) Environment 1: Trajectory (b) Environment 1: Control input

(a) Experiment 1: Trajectory

(b) Experiment 1: Control input

(c) Environment 2: Trajectory (d) Environment 2: Control input

(c) Experiment 2: Trajectory

(d) Experiment 2: Control input

Fig. 3: 2D double integrator (n=4, r=2) avoids a randomly
generated obstacle in two different environments. The blue
trajectory uses the proposed method, and the orange trajec-
tory is the optimal performance reference that was generated
by learning speciﬁcally for that environment. The corre-
sponding control inputs are shown on the right side.

Fig. 4: 2D quadruple integrator (n=8, r=4) avoids a ran-
domly generated obstacle in two different environments. The
blue trajectory uses the proposed method, and the orange
trajectory is the optimal performance reference that was
generated by learning speciﬁcally for that environment. The
corresponding control inputs are shown on the right side.

represented by ellipses:

h(y) = (y

yc)
y = Cx, Q = R(θ)(cid:62)ΛR(θ),

yc)(cid:62)Q(y

−

−

1,

−

(16)

where y is the measurement variable, i.e., the position in
Cartesian space. R(θ) is the rotation matrix deﬁned by
the orientation θ of the ellipse. yc is the center of the
obstacle. Λ is a diagonal matrix which represents the size
of the obstacle. We deﬁne the environment information as
R5. For different environments,
e = [yc, diag(Λ)(cid:62), θ](cid:62)
∈
we randomly sample e from a Gaussian distribution

We use a linear quadratic regulator as the performance
controller and a differentiable ECBF-QP as the safety ﬁlter.
For the α-net
in the differentiable ECBF-QP, we use a
feedforward neural network with two hidden layers. Each
hidden layer size is 100. Based on Algorithm 1, we train each
system with 100 iterations. In each iteration, we sample 30
environments, and for each rollout, the simulation time is 8s.
Futhermore, the initial condition of each system is selected
randomly within a predeﬁned region. We use the same loss
function for both systems, which is the sum of the distance
between each point and the goal location.

Pe.

perf(τ, e, x0) =

L

T
(cid:88)

t=0

xt −
(cid:107)

xgoal

2.
(cid:107)

(17)

The numerical values of this loss will serve as means to
compare performance of different controllers. Moreover, we
train a α-net only conditioned on a speciﬁc environment to
serve as the optimal solution for that speciﬁc environment.

B. Double Integrator Experiment

Two representative validation results for 2D double in-
tegrator avoiding an obstacle with the proposed approach
are shown in Fig. 3, including trajectory and control input.
The start point is chosen randomly, the goal location is at
(1.0, 0.0), and the obstacle is colored as blue. Moreover, the
proposed method is compared with the optimal performance
solution, which is obtained by ﬁnding the best Kα based on
the current environment, i.e., the environment in Fig. 3. In
Environment 1, the losses deﬁned in (17) for our method
and optimal performance solution are 26.88 and 26.41. In
Environment 2, the losses are 25.88 and 25.86 for ours and
optimal performance solution, respectively. The simulation
result shows that the performance of our proposed method
is close to the optimal performance solution in terms of the
loss function in (17). Also, our control inputs (solid lines) is
similar to the optimal ones (dashed lines) as shown in Fig. 3
(b) and (d).

C. Quadruple Integrator Experiment

In Fig. 4, we show that our approach can cope with
a system with relative-degree four for generalization to
novel environments. In both environments, the losses for the
optimal performance solution is 26.71 and 35.03, whereas
the losses for our method is 28.35 and 35.36, respectively.
We also observe that the control inputs of the proposed
method is similar to the optimal performance solution as
shown in Fig. 4 (b) and (d). The results imply that our
approach can determine a proper Kα given the environment
information without any manually tuning process for high
relative-degree systems.

(a) Experiment 1

(b) Experiment 2

(a) Ablation study 1

(b) Ablation study 2

Fig. 5: 2D double integrator is able to generalize to novel
environments with two randomly generated obstacles, which
are not experienced during training. The blue trajectory
utilizes the proposed framework, and the orange trajectory
uses a random valid Kα.

D. Multiple Obstacles Experiment

To further validate the generalization ability, we extend the
simulation setup of 2D double integrator from one obstacle to
multiple obstacles. We randomly generate two obstacles and
formulate one ECBF constraint for each object accordingly
in (15). For each constraint, we use the same learned α-
net. In this scenario, the proposed method needs to be able
to generalize to more complex environments. The simulation
results of two examples are shown in Fig. 5. In Experiment 1,
the losses for our method and random valid Kα are 28.11 and
32.50, respectively, and in Experiment 2, the corresponding
losses are 23.96 and 35.51. It shows that our approach
successfully generalizes to multiple obstacle scenarios and
outperforms the baseline by a large margin.

E. Ablation Study

We conduct two ablation studies using the 2D quadruple
integrator to validate that our proposed design is necessary
for generalizing to novel environments.

1) Is the obstacle information indeed useful?: The ﬁrst
ablation study is to evaluate whether the obstacle information
is necessary. We train an α-net with only one ﬁxed obstacle
during training as a baseline and then test it with novel envi-
ronments. The resulting trajectories are shown in Fig. 6(a).
Our proposed method has a loss of 30.26, while the loss for
baseline is 36.79. This indicates that the obstacle information
is necessary as an input to our neural network.

2) Is a larger or smaller valid Kα better?: In Fig. 6(b),
we investigate whether a larger or smaller valid Kα can
achieve a better performance with respect to our loss func-
tion. We scale Kα by multiplying the learned eigenvalues
pi}i=1,...,r with coefﬁcients 3.0 and 0.5. The losses for our
{
method, 3.0 scale, and 0.5 scale are 33.90, 65.90, and 37.85,
respectively. The resulting trajectories demonstrate that the
proposed method outperforms the cases with the scales.

VI. DISCUSSION

We next provide the analysis of the proposed framework
and a discussion on the limits and thoughts on future work.
We carry out a performance benchmark in three different
scenarios: 2D double integrator with one obstacle (Dou-
ble Integrator), 2D quadruple integrator with one obstacle

Fig. 6: Ablation study: (a) obstacle information is necessary
for environment generation. The blue curve uses the pro-
posed method, and the orange curve is the baseline with ﬁxed
obstacle information; (b) a larger or smaller valid Kα does
not lead to a better performance. Different colors represent
different scales.

Scenario
Double Integrator
Quadruple Integrator
Multiple Obstacles

Random
26.8832±0.7259
34.2979±1.1136
62.2640±2.2653

Ours
26.6774±0.5998
32.2187±0.8840
40.6258±3.6791

TABLE I: Benchmark of our proposed framework in three
different scenarios: double integrator, quadruple integrator,
and double integrator with two obstacles.

(Quadruple Integrator), and 2D double integrator with two
obstacles (Multiple Obstacles). We compare our method with
random valid Kα using 200 experiments for each scenario.
The mean and standard deviation of the losses are summa-
rized in Tab. I. We ﬁrst conduct 4 subtasks, and each of them
consists of 50 experiments. The mean and standard deviation
are computed based on these subtasks. The benchmark for
Double Integrator and Quadruple Integrator further validates
that the proposed framework is useful for generalization to
novel environments. Moreover, when we extend the learned
α-net to multiple obstacles, our method shows better results.
In terms of the overall controller design, we assume that
a high-level controller is given and ﬁxed in this work, which
could be a valid assumption in many applications. Note that
the overall performance is determined by both the high-level
performance controller and the CBF-QP safety ﬁlter.

VII. CONCLUSION

In this paper, we presented a learning differentiable safety-
critical-control framework using control barrier functions for
generalization to novel environments, which uses a learning-
based method to choose an environment-dependent Kα in
exponential control barrier function. Moreover, based on the
ECBF formulation, the proposed method ensures the forward
invariance of the safe set. We numerically veriﬁed the
proposed method with 2D double and quadruple integrator
systems in novel environments. Our framework can be easily
generalized to different shapes of obstacles and nonlinear
dynamics. Also, different representation of environment in-
formation such as images and point cloud can be used.
There are several interesting future directions. For instance,
an integrated end-to-end framework can be designed for
training the performance controller and ECBF-QP. Another

promising future direction is to test our approach in more
general scenarios.

ACKNOWLEDGEMENT
We would like to thank Ayush Agrawal for his helpful

discussions.

REFERENCES

[1] A. Agrawal, B. Amos, S. Barratt, S. Boyd, S. Diamond, and J. Z.
Kolter, “Differentiable convex optimization layers,” Advances in Neu-
ral Information Processing Systems, vol. 32, pp. 9562–9574, 2019.
[2] A. D. Ames, S. Coogan, M. Egerstedt, G. Notomista, K. Sreenath,
and P. Tabuada, “Control barrier functions: Theory and applications,”
in 2019 18th European Control Conference (ECC).
IEEE, 2019, pp.
3420–3431.

[3] A. D. Ames, J. W. Grizzle, and P. Tabuada, “Control barrier function
based quadratic programs with application to adaptive cruise control,”
in 53rd IEEE Conference on Decision and Control.

IEEE, 2014.

[4] A. D. Ames, X. Xu, J. W. Grizzle, and P. Tabuada, “Control barrier
function based quadratic programs for safety critical systems,” IEEE
Transactions on Automatic Control, vol. 62, pp. 3861–3876, 2016.
[5] B. Amos, I. Jimenez, J. Sacks, B. Boots, and J. Z. Kolter, “Differen-
tiable mpc for end-to-end planning and control,” Advances in Neural
Information Processing Systems, vol. 31, pp. 8289–8300, 2018.
[6] B. Amos and J. Z. Kolter, “Optnet: Differentiable optimization as a
layer in neural networks,” in International Conference on Machine
Learning. PMLR, 2017, pp. 136–145.

[7] S. Bansal, M. Chen, S. Herbert, and C. J. Tomlin, “Hamilton-jacobi
reachability: A brief overview and recent advances,” in 2017 IEEE
56th Annual Conference on Decision and Control (CDC).
IEEE,
2017, pp. 2242–2253.

[8] F. S. Barbosa, L. Lindemann, D. V. Dimarogonas, and J. Tumova,
“Provably safe control of lagrangian systems in obstacle-scattered
environments,” in 2020 59th IEEE Conference on Decision and
Control (CDC), 2020, pp. 2056–2061.

[9] J. Choi, F. Casta˜neda, C. Tomlin, and K. Sreenath, “Reinforcement
Learning for Safety-Critical Control under Model Uncertainty, using
Control Lyapunov Functions and Control Barrier Functions,” in Pro-
ceedings of Robotics: Science and Systems, Corvalis, Oregon, USA,
July 2020.

[10] Y. Chow, O. Nachum, E. Duenez-Guzman, and M. Ghavamzadeh,
“A lyapunov-based approach to safe reinforcement learning,” in Pro-
ceedings of the 32nd International Conference on Neural Information
Processing Systems, 2018, pp. 8103–8112.

[11] I. Cizelj, X. C. D. Ding, M. Lahijanian, A. Pinto, and C. Belta,
“Probabilistically safe vehicle control in a hostile environment,” IFAC
Proceedings Volumes, vol. 44, no. 1, pp. 11 803–11 808, 2011.
[12] C. Dawson, Z. Qin, S. Gao, and C. Fan, “Safe nonlinear control using
robust neural lyapunov-barrier functions,” in 5th Annual Conference
on Robot Learning, 2021.

[13] P. L. Donti, M. Roderick, M. Fazlyab, and J. Z. Kolter, “Enforcing
robust control guarantees within neural network policies,” in Interna-
tional Conference on Learning Representations, 2020.

[14] J. Drgona, A. Tuor, and D. Vrabie, “Learning constrained adaptive dif-
ferentiable predictive control policies with guarantees,” arXiv preprint
arXiv:2004.11184, 2020.

[15] Y. Emam, P. Glotfelter, Z. Kira, and M. Egerstedt, “Safe model-based
reinforcement learning using robust control barrier functions,” arXiv
preprint arXiv:2110.05415, 2021.

[16] S.-C. Hsu, X. Xu, and A. D. Ames, “Control barrier function based
quadratic programs with application to bipedal robotic walking,” in
2015 American Control Conference (ACC).
IEEE, 2015, pp. 4542–
4548.

[17] W. Jin, S. Mou, and G. Pappas, “Safe pontryagin differentiable
programming,” Advances in Neural Information Processing Systems,
vol. 34, 2021.

[18] E. C. Kerrigan and J. M. Maciejowski, “Invariant sets for constrained
nonlinear discrete-time systems with application to feasibility in model
predictive control,” in Proceedings of the 39th IEEE Conference on
Decision and Control (Cat. No. 00CH37187), vol. 5.
IEEE, 2000,
pp. 4951–4956.

[19] H. K. Khalil, “Nonlinear systems,” 2002.
[20] K. Lee, S. Maji, A. Ravichandran, and S. Soatto, “Meta-learning with
differentiable convex optimization,” in Proceedings of the IEEE/CVF
Conference on Computer Vision and Pattern Recognition, 2019, pp.
10 657–10 665.

[21] A. Majumdar, A. Farid, and A. Sonar, “Pac-bayes control: learning
policies that provably generalize to novel environments,” The Interna-
tional Journal of Robotics Research, vol. 40, no. 2-3, pp. 574–593,
2021.

[22] Q. Nguyen and K. Sreenath, “Optimal robust control for bipedal
robots through control lyapunov function based quadratic programs.”
in Robotics: Science and Systems. Rome, Italy, 2015, pp. 1–9.

[23] ——, “Exponential control barrier

functions for enforcing high
relative-degree safety-critical constraints,” in 2016 American Control
Conference (ACC).
IEEE, 2016, pp. 322–328.

[24] H. Parwana and D. Panagou, “Recursive feasibility guided optimal
parameter adaptation of differential convex optimization policies for
safety-critical systems,” arXiv preprint arXiv:2109.10949, 2021.
[25] A. Robey, H. Hu, L. Lindemann, H. Zhang, D. V. Dimarogonas,
S. Tu, and N. Matni, “Learning control barrier functions from expert
demonstrations,” in 2020 59th IEEE Conference on Decision and
Control (CDC).

IEEE, 2020, pp. 3717–3724.
[26] U. Rosolia and A. D. Ames, “Multi-rate control design leveraging
control barrier functions and model predictive control policies,” IEEE
Control Systems Letters, vol. 5, no. 3, pp. 1007–1012, 2020.

[27] E. Scukins and P.

¨Ogren, “Using reinforcement learning to create
control barrier functions for explicit risk mitigation in adversarial
environments,” in IEEE International Conference on Robotics and
Automation (ICRA).
IEEE Robotics and Automation Society, 2021.
[28] L. Song, N. Wan, A. Gahlawat, C. Tao, N. Hovakimyan, and E. A.
Theodorou, “Generalization of safe optimal control actions on net-
worked multi-agent systems,” arXiv preprint arXiv:2109.09909, 2021.
[29] A. Taylor, A. Singletary, Y. Yue, and A. Ames, “Learning for safety-
critical control with control barrier functions,” in Learning for Dy-
namics and Control. PMLR, 2020, pp. 708–717.

[30] R. Tian, L. Sun, A. Bajcsy, M. Tomizuka, and A. D. Dragan, “Safety
assurances for human-robot interaction via conﬁdence-aware game-
theoretic human models,” arXiv preprint arXiv:2109.14700, 2021.
[31] L. Wang, A. D. Ames, and M. Egerstedt, “Multi-objective composi-
tions for collision-free connectivity maintenance in teams of mobile
robots,” in 2016 IEEE 55th Conference on Decision and Control
(CDC).

IEEE, 2016, pp. 2659–2664.

[32] W. Xiao and C. Belta, “High order control barrier functions,” IEEE

Transactions on Automatic Control, 2021.

[33] W. Xiao, C. A. Belta, and C. G. Cassandras, “Feasibility-guided
learning for constrained optimal control problems,” in 2020 59th IEEE
Conference on Decision and Control (CDC), 2020, pp. 1896–1901.

[34] J. Zeng, B. Zhang, Z. Li, and K. Sreenath, “Safety-critical control
using optimal-decay control barrier function with guaranteed point-
wise feasibility,” in 2021 American Control Conference (ACC), 2021,
pp. 3856–3863.

[35] J. Zeng, B. Zhang, and K. Sreenath, “Safety-critical model predictive
control with discrete-time control barrier function,” in 2021 American
Control Conference (ACC).

IEEE, 2021, pp. 3882–3889.

[36] J. Zhang, B. Cheung, C. Finn, S. Levine, and D. Jayaraman, “Cautious
adaptation for reinforcement learning in safety-critical settings,” in
International Conference on Machine Learning.
PMLR, 2020, pp.
11 055–11 065.

[37] W. Zhao, T. He, and C. Liu, “Model-free safe control for zero-violation
reinforcement learning,” in 5th Annual Conference on Robot Learning,
2021.

