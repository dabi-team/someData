SEN12MS – A CURATED DATASET OF GEOREFERENCED MULTI-SPECTRAL
SENTINEL-1/2 IMAGERY FOR DEEP LEARNING AND DATA FUSION

M. Schmitt1, L. H. Hughes1, C. Qiu1, X. X. Zhu1,2

1 Signal Processing in Earth Observation, Technical University of Munich, Munich, Germany
2 Remote Sensing Technology Institute, German Aerospace Center (DLR), Oberpfaffenhofen, Wessling

KEY WORDS: Data Fusion, Dataset, Machine Learning, Remote Sensing, Multi-Spectral Imagery, Synthetic Aperture Radar (SAR),
Optical Remote Sensing, Sentinel-1, Sentinel-2, Deep Learning

ABSTRACT:

This is a pre-print of a paper accepted for publication in the ISPRS Annals of the Photogrammetry, Remote Sensing and Spatial Infor-
mation Sciences. Please refer to the original (open access) publication from September 2019.
The availability of curated large-scale training data is a crucial factor for the development of well-generalizing deep learning methods
for the extraction of geoinformation from multi-sensor remote sensing imagery. While quite some datasets have already been published
by the community, most of them suffer from rather strong limitations, e.g. regarding spatial coverage, diversity or simply number
of available samples. Exploiting the freely available data acquired by the Sentinel satellites of the Copernicus program implemented
by the European Space Agency, as well as the cloud computing facilities of Google Earth Engine, we provide a dataset consisting of
180,662 triplets of dual-pol synthetic aperture radar (SAR) image patches, multi-spectral Sentinel-2 image patches, and MODIS land
cover maps. With all patches being fully georeferenced at a 10 m ground sampling distance and covering all inhabited continents during
all meteorological seasons, we expect the dataset to support the community in developing sophisticated deep learning-based approaches
for common tasks such as scene classiﬁcation or semantic segmentation for land cover mapping.

1.

INTRODUCTION

The availability of curated annotated datasets is of crucial impor-
tance for the development of machine learning models for infor-
mation retrieval from remote sensing data. While classic shallow
learning approaches could easily be trained on comparably small
datasets such as, e.g., the famous Indian Pines scene (Baum-
gardner et al., 2015), modern deep learning requires large-scale
data to reach the desired generalization performance (Zhu et al.,
2017). However, computer vision usually deals with conventional
photographs of everyday objects, whereas remote sensing data is
more versatile and much more difﬁcult to interpret. Therefore,
massive databases of labeled imagery such as ImageNet (Deng
et al., 2009) do not yet exist in the remote sensing domain, al-
though there have been ﬁrst steps into that direction; a certainly
non-exhaustive overview of existing scientiﬁc datasets of anno-
tated remote sensing imagery can be found in Tab. 1. Additional
datasets, which were mostly provided in the frame of machine
learning competitions and are not described and discussed in sci-
entiﬁc papers, can be found in a the private link list of (Rieke,
2019).

In order to support deep learning-related research in the ﬁeld of
remote sensing, we have published the SEN1-2 dataset in 2018,
which is comprised of about 280,000 pairs of corresponding
Sentinel-1 SAR and Sentinel-2 optical images (Schmitt et al.,
2018). Since SEN1-2 was mainly intended for bridging the gap
between classical computer vision problems and remote sensing,
e.g.
image-to-image translation tasks, the provided data were
strongly simpliﬁed: For Sentinel-1, we just provided vertically
polarized (VV) imagery in dB scale, and for Sentinel-2 we re-
duced the original multi-spectral data tensors to RGB images
with adjusted histograms, whereas none of the images came with
any form of geolocation information. Based on feedback from the
community, with this paper, we now publish a follow-on version

of the dataset, which is designed to suit the needs of the remote
sensing community. SEN12MS contains full multi-spectral infor-
mation in geocoded imagery and is described in details through-
out the remainder of this paper.

2. THE DATA BASIS

We exploit freely available satellite(-derived) data to form the ba-
sis of the dataset. On the one hand, we make use of SAR and
multi-spectral imagery provided by Sentinel-1 and Sentinel-2, re-
spectively. On the other hand, we add land cover information de-
rived from observations acquired by the MODIS system. Details
of the three basic data sources are provided in the following.

2.1 Sentinel-1

The Sentinel-1 mission (Torres et al., 2012) consists of currently
two polar-orbiting satellites, equipped with C-band SAR sensors,
which enables them to acquire imagery regardless of the weather.
Sentinel-1 works in a pre-programmed operation mode to avoid
conﬂicts and to produce a consistent long-term data archive built
for applications based on long time series. Depending on which
SAR imaging mode is used, resolutions down to 5 m with a
wide coverage of up to 400 km can be achieved. Furthermore,
Sentinel-1 provides dual polarization capabilities and very short
revisit times of about 1 week at the equator. Since highly precise
spacecraft positions and attitudes are combined with the high ac-
curacy of the range-based SAR imaging principle, Sentinel-1 im-
ages come with high out-of-the-box geolocation accuracy (Schu-
bert et al., 2015).

For the Sentinel-1 images in the SEN12MS dataset, again ground-
range-detected (GRD) products acquired in the most frequently
available interferometric wide swath (IW) mode were used.
These images contain the σ0 backscatter coefﬁcient in dB scale

9
1
0
2

n
u
J

8
1

]

V
C
.
s
c
[

1
v
9
8
7
7
0
.
6
0
9
1
:
v
i
X
r
a

 
 
 
 
 
 
Image Size

Data Source

Description

Reference

Dataset

UC Merced Land Use
Dataset

Nr. of
Images

2,100

256 × 256

SAT-4

SAT-6

500,000

28 × 28

405,000

28 × 28

Brazilian Coffee
Scenes Dataset

51,004

64 × 64

SPOT

Color aerial
images

Color aerial
imagery (R, G,
B, NIR)

Color aerial
imagery (R, G,
B, NIR)

21 balanced land use classes

(Yang and Newsam, 2010)

4 agricultural classes over
continental USA

(Basu et al., 2015)

6 land cover classes over
continental USA

(Basu et al., 2015)

2 imbalanced classes
describing non-coffee and
coffee

(Penatti et al., 2015)

USGS SIRI-WHU

1

10,000 × 9,000

Color aerial
image

4 classes over Montgomery
County, Ohio, USA

(Zhong et al., 2015)

1,311

64 × 64

RapidEye (G, R,
NIR)

4 imbalanced classes
describing Cerrado-Savanna
vegetation

12 classes describing urban
areas in China

2 classes (building / not
building) for 10 cities in
Austria and USA

(Nogueira et al., 2016)

(Zhao et al., 2016)

(Maggiori et al., 2017)

17 local climate zone classes

(Yokoya et al., 2018)

1 target class: roads over India
and Thailand

(Demir et al., 2018)

1 target class: buildings in Las
Vegas, Shanghai, Paris,
Karthoum

(Demir et al., 2018)

7 land cover classes

(Demir et al., 2018)

Brazilian
Cerrado-Savanna
Dataset

SIRI-WHU

Inria Aerial Image
Labeling Dataset

2017 IEEE GRSS
Data Fusion Contest

200

360

57

DeepGlobe – Road
Extraction

8,570

200 × 200

Google Earth

1,500 × 1,500

Color aerial
imagery

from
447 × 377 to
1,461 × 1,222

Sentinel-2,
Landsat,
OpenStreetMap

1,024 × 1,024 Worldview-2/-3,
GeoEye-1 (R, G,
B)

DeepGlobe –
Building Detection

24,586

650 × 650

Worldview-3

DeepGlobe
– Land Cover
Classiﬁcation

DOTA

1,146

2,806

2,448 × 2,448 Worldview-2/-3,
GeoEye-1 (R, G,
B)

EuroSAT

SEN1-2

27,000

564,768

64 × 64

256 × 256

38-Cloud

17,601

384 × 384

BigEarthNet

590,326

up to
120 × 120

SEN12MS

541,986

256 × 256

Sentinel-1 and
Sentinel-2

Landsat 8 (R, G,
B, NIR)

Sentinel-2

Sentinel-1,
Sentinel-2,
MODIS Land
Cover

800 × 800
to
4,000 × 4,000

Color aerial
images

188,282 instances of 15 object
classes, each labeled by a
quadriliteral

(Xia et al., 2018)

Sentinel-2

10 classes

(Helber et al., 2018)

(Schmitt et al., 2018)

corresponding pairs of SAR
(single-pol intensity) and
optical (RGB) image pairs
without annotations

single target class: clouds

(Mohajerani and Saeedi, 2019)

43 Corine Land Cover classes
over Europe

globally distributed; MODIS
Land Cover maps can either be
used as labels or auxiliary data

(Sumbul et al., 2019)

this paper

Table 1. Non-exhaustive list of curated datasets for deep learning in remote sensing.

for every pixel at a pixel spacing of 5 m in azimuth and 20 m in
range.
In order to exploit the full potential of Sentinel-1 data,
SEN12MS contains both VV and VH polarized images.

data any further.

2.2 Sentinel-2

For precise ortho-rectiﬁcation, restituted orbit information was
combined with the 30 m-SRTM-DEM or the ASTER DEM for
high latitude regions where SRTM is not available. As for the
SEN1-2 dataset, we intend to leave any further pre-processing,
e.g. speckle ﬁltering, to the end user and do not manipulate the

The Sentinel-2 mission (Drusch et al., 2012) currently comprises
two identical polar-orbiting satellites in the same orbit, phased at
180◦ to each other. The mission is meant to provide continuity for
multi-spectral imagery of the SPOT and LANDSAT kind, which
have provided information about the land surfaces of our Earth

for many decades. With its wide swath width of up to 290 km
and its high revisit time of 5 days at the equator (based on two
satellites) under cloud-free conditions, the Sentinel-2 mission is
speciﬁcally well-suited to vegetation monitoring within the grow-
ing season.

For the SEN12MS dataset, we provide the full multi-spectral im-
age cubes as extracted from the original, precisely georeferenced
Sentinel-2 granules. The only manipulation we carried out was
to implement a sophisticated mosaicking workﬂow to avoid the
download of cloud-affected images (cf. Section 3.1).

2.3 MODIS Land Cover

MODIS (the Moderate Resolution Imaging Spectroradiometer)
is the main instrument on board of the Terra and Aqua satellites.
Terra’s orbit around the Earth is timed so that it passes from north
to south across the equator in the morning, while Aqua passes
south to north over the equator in the afternoon. Terra MODIS
and Aqua MODIS acquisitions cover the whole Earth with an ap-
proximately daily revisit frequency – at a band-dependent resolu-
tion of 250 m to 1000 m. Based on calibrated MODIS reﬂectance
data, hierarchical classiﬁcation following the land cover classiﬁ-
cation system (LCCS) scheme, and sophisticated post-processing
for class-speciﬁc reﬁnement incorporating prior knowledge, aux-
iliary information and temporal regularization based on a Markov
random ﬁeld, annually updated global land cover maps for the
years 2001–2016 are provided as MCD12Q1 V6 dataset at a
ground sampling distance of 500 m (Sulla-Menashe et al., 2019).
To add land cover information to the Sentinel-1/Sentinel-2 patch-
pairs constituting the core of the SEN12MS dataset, we add four-
band MODIS land cover patches created from 2016 data at an
upsampled pixel spacing of 10 m. The ﬁrst of the provided
bands contains land cover following the International Geosphere-
Biosphere Programme (IGBP) classiﬁcation scheme (Loveland
and Belward, 1997), while the remaining bands contain the LCCS
land cover layer, the LCCS land use layer, and the LCCS sur-
face hydrology layer (Di Gregorio, 2005). The schemes’ classes
are listed in Tab. 2. According to (Sulla-Menashe et al., 2019),
the overall accuracies of the layers are about 67% (IGBP), 74%
(LCCS land cover), 81% (LCCS land use), and 87% (LCCS sur-
face hydrology), respectively. This should be kept in mind when
using the land cover data as labels for training scene classiﬁcation
or semantic segmentation models, as these accuracies will con-
stitute the upper bound of actually achievable predictive power
– even if validation accuracies of 100% are reached. If the land
cover information is not utilized as annotation, but as auxiliary
data source, similar caution should be had.

3. GOOGLE EARTH ENGINE FOR DATA

PREPARATION

As for the SEN1-2 dataset, we have again utilized Google Earth
Engine (Gorelick et al., 2017) to generate a large-scale dataset of
corresponding multi-sensor remote sensing image patches. While
we basically used the same pipeline as described in (Schmitt et
al., 2018), including the random sampling of ROIs for the mete-
orological seasons of the northern hemisphere, we added a more
sophisticated mosaicking workﬂow for the generation of cloud-
free short-term Sentinel-2 mosaics.

3.1 Mosaicking of Cloud-Free Sentinel-2 Images

The general mosaicking workﬂow to produce cloud-free
Sentinel-2 images for a given region of interest (ROI) and a spec-
iﬁed time period is depicted in Fig. 1. In essence, it consists of

Figure 1. Workﬂow of the GEE-based procedure for cloud-free
Sentinel-2 image generation presented in (Schmitt et al., 2019).

three main modules, which are carried out for every ROI. These
ROIs result from two uniform random samplings over the land-
masses of the Earth and the urban areas across the globe, respec-
tively.

While the procedure is described in detail in (Schmitt et al.,
2019), a short summary of the three modules is as follows:

(1) The Query Module for loading images from the catalogue.
In this module, for the speciﬁed ROI all Sentinel-2 images
available for a speciﬁed time period are selected.

(2) The Quality Score Module for the calculation of a quality
score for each image. In this module, every pixel of each
Sentinel-2 image is assigned a score that considers the like-
lihood it is affected either by clouds or by shadow.

(3) The Image Merging Module for mosaicking of the selected
images based on the meta-information generated in the pre-
ceding modules. First, the quality scores are thresholded to
determine cloud and shadow masks for each image. After-
wards, the images are sorted by their amount of poor pixels.
The best images are ﬁnally merged into a cloud-free mosaic.

Since the Sentinel-1 images and the MODIS land cover data are
not affected by clouds, in these cases no complicated mosaick-
ing processes are required and the data are simply exported in a
straight-forward manner later on.

3.2 Data Export

For SEN12MS we have utilized the same random ROIs as for
SEN1-2. The same holds for the meteorological seasons as
deﬁned for the northern hemisphere. After preparation of the
Sentinel-1 images and the cloud-free Sentinel-2 mosaics for ev-
ery ROI and season, we export them together with the land cover

ROISeasonQuery ModuleFetch Images ee.ImageCollection() Clip Images to ROI ee.Image.clip() Quality Score ModuleCloud Score Submodule Shadow Score Submodule Threshold Quality Score toget Cloud Mask Sort Images by Share ofPoor Pixels Image Merging ModuleCloud-free  ImageClass

IGBP value

LCCS LC value

LCCS LU value

LCCS SH value

Evergreen Needleleaf Forests
Evergreen Broadleaf Forests
Deciduous Needleleaf Forests
Deciduous Broadleaf Forests
Mixed Broadleaf/Needleleaf Forests
Mixed Broadleaf Evergreen/Deciduous Forests
Mixed Forests
Dense Forests
Open Forests
Sparse Forests
Natural Herbaceous
Dense Herbaceous
Sparse Herbaceous
Shrublands
Closed (Dense) Shrublands
Open (Sparse) Shrublands
Shrubland/Grassland Mosaics
Woody Savannas
Savannas
Grasslands
Permanent Wetlands
Woody Wetlands
Herbaceous Wetlands
Herbaceous Croplands
Croplands
Urban and Built-Up Lands
Cropland/Natural Vegetation Mosaics
Forest/Cropland Mosaics
Natural Herbaceous/Croplands Mosaics
Tundra
Permanent Snow and Ice
Barren
Water Bodies

1
2
3
4
-
-
5
-
-
-
-
-
-
-
6
7
-
8
9
10
11
-
-
-
12
13
14
-
-
-
15
16
17

11
12
13
14
15
16
-
-
21
22
-
31
32
-
41
43
42
-
-
-
-
-
-
-
-
-
-
-
-
-
2
1
3

-
-
-
-
-
-
-
10
20
-
30
-
-
40
-
-
-
-
-
-
-
-
-
36
-
9
-
25
35
-
2
1
3

-
-
-
-
-
-
-
10
20
-
-
-
-
40
-
-
-
-
-
30
-
27
50
-
-
-
-
-
-
51
2
1
3

Table 2. MODIS land cover classes as represented by four different schemes: IGBP, LCCS land cover, LCCS land use, LCCS surface hydrology. The
NoData value is set to be 255 (Sulla-Menashe and Friedl, 2018).

data at a scale of 10 m in the form of GeoTiffs. In this context,
it has to be noted that the 10 m scale is deﬁned at the equator
by Google Earth Engine, which corresponds to an angular reso-
lution of 0.0001◦. This angular resolution leads to signiﬁcantly
smaller pixel widths for regions with latitudes deviating from 0◦.
We therefore used GDAL (Warmerdam, 2008) to transform all
exported data from the WGS84 lat/lon georeference to their lo-
cal UTM coordinate representation, while resampling to actually
square pixels of 10 m×10 m.

3.3 Data Curation

After the export of the data from the GEE servers to local storage,
we followed an inspection protocol similar to the one proposed
in our previous work (Schmitt et al., 2018): First, each triplet of
full scene images was converted to a visually perceivable format
(gray-scale images for Sentinel-1 and MODIS Land Cover, RGB
images for Sentinel-2) and displayed to a remote sensing expert.
If either of the three images contained very large no-data areas,
large non-detected clouds, or strong artifacts resulting from the
cloud-adaptive mosaicking, the triplet was discarded. After this
ﬁrst inspection, only 252 out of the originally downloaded 600
scenes were kept in the dataset. These remaining scenes were
then tiled into patches of 256 × 256 pixels in size. Again, we
have implemented a stride of 128 pixels, resulting in an overlap
between adjacent patches of 50%. We think, a 50% overlap is the
ideal trade-off between patch independence and maximization of
the number of samples. After the tiling, 216,596 patch triplets
were available for a second inspection. In this second inspection,
all patches were again visually inspected by remote sensing ex-
perts in order to avoid patches containing artefacts or distortions,

e.g. no data areas, clouds, or jet streams. Some examples for
patch types that were discarded in this step are displayed in Fig. 2.
After this ﬁnal inspection step, a total of 180,662 patch triplets
remained, which comprise the ﬁnal SEN12MS dataset. The lo-
cations of the ﬁnal ROI scene locations are displayed in Fig. 3.

Figure 2. Examples for manually removed SAR (top row) and optical
patches (bottom) row).

4. THE SEN12MS DATASET

The ﬁnal SEN12MS dataset contains 180,662 patch triplets
(Sentinel-1 dual-pol SAR, Sentinel-2 multi-spectral, MODIS
land cover) in the form of multi-channel GeoTiff images and re-
quires 421.3 GiB of storage. Some patch triplet examples are

Figure 3. Final distribution of the ROIs over all inhabited land masses of the Earth.

shown in Fig. 4 to give an impression of the rich and versatile
information contained in the dataset.

4.1 Structure of the Final Dataset

As mentioned in Section 3.1, the dataset is based on randomly
sampled regions of interest, resulting from four different seed
values: 1158, 1868, 1970, and 2017. These four different
seed values are related to the four meteorological seasons de-
ﬁned for the northern hemisphere: winter (1 December 2016
to 28 February 2017), spring (1 March 2017 to 30 May 2017),
summer (1 June 2017 to 31 August 2017), and fall (1 Septem-
ber 2017 to 30 November 2017). This leads to a tree-like
structure of the dataset into four branches: ROIs1158 spring,
ROIs1868 summer, ROIs1970 fall, and ROIs2017 winter.
Each of those branches again is divided into several sub-branches
corresponding to the individual ROIs (or scenes, respectively)
derived from the corresponding random number seed. The full
dataset structure is depicted in Fig. 5.

The Sentinel-1 data can be recognized by the abbreviation s1,
the Sentinel-2 data by s2, and the MODIS land cover data
by lc;
the individual patches can be identiﬁed by the token
pXXX where XXX denotes a unique identiﬁer number per patch.
Thus, the ﬁle naming convention follows the following scheme:
ROIsSSSS SEASON DD pXXX.tif, where SSSS denotes the seed
value, SEASON denotes the meteorological season as deﬁned for
the northern hemisphere, DD denotes the data identiﬁer, and XXX
denotes the patch identiﬁer.

Of course, we are aware that the seasonal structuring of the
dataset is only of little semantic worth since we have taken the
seasons of the northern hemisphere as a reference. To allow end-
users a sub-structuring of the dataset taking semantically mean-
ingful seasons into account, we provide the ﬁle seasons.csv
with the metadata of the dataset. It declares, which scenes ac-
tually were acquired in spring, summer, winter, and fall from a
climatic point of view.

While we forgo to deﬁne a ﬁxed train/test split, we think this
can easily be achieved by end-users considering their individual
needs: Deterministic splits into disjunct training and test sets can

Figure 4. Images extracted from 3 example patch triplets. Each column
shows (from top to bottom): False color Sentinel-1 SAR (R: VV, G: VH,
B: VV/VH), Sentinel-2 RGB, Sentinel-2 SWIR, IGBP Land cover,
LCCS Land cover. Note that while the GSD of all patches is upsampled
to 10 m, the actual resolution varies from 10 m (Sentinel-2 RGB) to
500 m (land cover).

Figure 5. Tree structure of the ﬁnal dataset.

be achieved via the meteorological seasons, or via the individual
ROIs.

4.2 Dataset Availability

The SEN12MS dataset is shared under the open access license
CC-BY and available for download at a persistent link provided
by the library of the Technical University of Munich (TUM):
https://mediatum.ub.tum.de/1474000. This paper must be
cited when the dataset is used for research purposes.

5. APPLICATION TO LAND COVER MAPPING

In order to provide an example for the usefulness of the dataset
with regard to the development of land cover classiﬁcation
solutions, we have trained two state-of-the-art deep convolu-
tional neural network architectures for predicting LCCS land use
classes (cf. Tab. 3). The ﬁrst network, ResNet-110 (He et al.,

Upsampling type
Input size
Batch size
Loss
Optimizer
Initial LR
LR schedule

ResNet-110

DenseNet

n/a
64 × 64 × 10
16

Deconvolution
256 × 256 × 10
4

0.0005

Categorical cross-entropy
Nesterov Adam
0.0001
ReduceOnPlateau

Table 3. Baseline network training conﬁgurations.

2016), was designed for image classiﬁcation, i.e. for assigning a
single class label to the input image. For the presented baseline
experiment, we cut images of 64 × 64 pixels from the Sentinel-
2 samples from the summer subset, and used the following ten
bands as channel information: B2 (Blue), B3 (Green), B4 (Red),
B8 (Near-infrared), B5 (Red Edge 1), B6 (Red Edge 2), B7 (Red
Edge 3), B8a (Red Edge 4), B11 (Short-wavelength infrared 1),
and B12 (Short-wavelength infrared 2). We then used the major-
ity LCCS land use class from each of the 64×64 patches as scene
label for that patch. Due to the unconventional multi-channel
conﬁguration of the data, we trained the network from scratch
rather than relying on any pre-trained weights. In order to test
the predictive power of this network, we applied it to the area of
the city of Munich, with the test image being pre-processed with
the same GEE-based procedure as described in Section 3.1. The
resulting land use map is shown in Fig. 6 (top row), and was cre-
ated by sliding the ResNet-110 on an input Sentinel-2 image with

a stride of 10 (leading to an output pixel spacing of 100 m). The
overall accuracy (OA), average accuracy (AA) and Kappa coef-
ﬁcient calculated based on a grid of manually annotated control
points can be seen in Tab. 4. It has to be noted that for sake of this
evaluation, we combined the classes Open Forests (LCCS 20) and
Forest/Cropland Mosaic (LCCS 25) to a joint Open Forests class,
and Natural Herbaceous (LCCS 30) with Herbacoeus Croplands
(LCCS 36) and Natural Herbaceous/Croplands Mosaic (LCCS
35) to a simple Herbaceous class, as it is very difﬁcult for hu-
man annotators to distinguish those classes by their only subtle
differences.

The second network we used was the fully convolutional
DenseNet for semantic segmentation (J´egou et al., 2017), aim-
ing at assigning a class label to every pixel of the input im-
age. Here, we used full-sized Sentinel-2 10-band patches (i.e.
256 × 256 pixels) as input and processed the city of Rome for test
purposes. The result is also depicted in Fig. 6, and the accuracy
metrics, calculated analogue to the Munich case, are described in
Tab. 5.

OA

Kappa

AA

MODIS
Predicted

53.0%
65.1%

0.38
0.51

37.5%
43.6%

Table 4. Accuracy metrics of the LCCS maps for the Munich scene. The
predicted result was achieved by patch classiﬁcation based on a
ResNet-110 CNN. The evaluation is based on manually labeled
reference points.

OA

Kappa

AA

MODIS
Predicted

56.0%
63.3%

0.37
0.44

33.7%
39.4%

Table 5. Accuracy metrics of the LCCS maps for the Rome scene. The
predicted result was achieved by semantic segmentation based on a
DenseNet CNN. The evaluation is based on manually labeled reference
points.

By comparing the resulting maps to the original MODIS-derived
LCCS land use map, as well as an image extracted from Google
Earth, it can be seen that in both cases the resolution of the
map was successfully enhanced so that more details can be re-
trieved, while an overall agreement between the low-resolution
MODIS map and the corresponding predicted high-resolution re-
sult is preserved. It has to be highlighed that both test areas are
not contained in the SEN12MS dataset, so that the results can
serve as a ﬁrst indicator of the strong generalization capability

(a)

(b)

(c)

(d)

(e)

(f)

(g)

(h)

Barren (LCCS 1)
Dense Forests (LCCS 10)
Natural Herbaceous/Croplands Mosaics (LCCS 35)

Permanent Snow and Ice (LCCS 2)
Open Forests (LCCS 20)
Herbaceous Croplands (LCCS 36)

Water Bodies (LCCS 3)
Forest/Cropland Mosaics (LCCS 25)
Shrublands (LCCS 40)

Urban and Built-Up Lands (LCCS 9)
Natural Herbaceous (LCCS 30)

Figure 6. Examples from Munich (top) and Rome (bottom) for the predictive power of the dataset: (a) and (e) Optical images extracted from Google
Earth, (b) and (f) MODIS-derived LCCS land use map with 500m resolution, (c) predicted LCCS land use map with 100m resolution using
classiﬁcation network, (g) predicted LCCS land use map with 10m resolution using semantic segmentation network, (d) and (h) zoom-in of the red
rectangles in the respective predicted LCCS land use maps. While the area in the red rectangle is completely deﬁned as urban in the original
MODIS-derived product, more details are visible in the predicted results.

provided by the versatility of the dataset. This holds even more
so since only a small subset of the dataset (namely the patches
of the summer season) has been used for training the networks
used in the experiments. This impression is also conﬁrmed by
the independently evaluated accuracy metrics. We expect that by
exploiting the whole dataset, i.e. both sensor modalities, all ROIs
and all seasons, powerful classiﬁers for large-scale mapping ap-
plications can be developed. Besides that, it has to be mentioned
that training on low-resolution labels of course creates a form of
label noise. Using speciﬁcally adapted strategies aiming at so-
called label super-resolution (Malkin et al., 2019) might be able
to provide even better results.

6. DISCUSSION

As can be seen from Tab. 1, SEN12MS is among the ﬁve largest
datasets when only the sheer number of image patches is consid-
ered. However, in the end, SEN12MS contains a lot more data
and is consequently much bigger than its competitors: First and
foremost, its patches are of size 256 × 256 pixels instead of just
28×28 (SAT-4/6) or 120×120 (BigEarthNet) pixels. Besides, the
spectral information content is also much higher, as SEN12MS
contains full multi-spectral Sentinel-2 imagery and, in addition,
dual-polarimetric Sentinel-1 SAR data, while most other datasets
– besides EuroSAT, SEN1-2, and BigEarthNet – only contain
color imagery with 3 or 4 bands. Last, but not least, it has to
be mentioned that SEN12MS is the most versatile dataset regard-
ing scene distribution, as it covers all regions of the Earth over
all meteorological seasons, while most of the other datasets are
restricted to fairly small areas (e.g. Brazilian Coffee Scenes or

USGS SIRI-WHU), individual countries (e.g. SAT-4/5 or Deep-
Globe - Road Extraction), or a single continent (e.g. BigEarth-
Net).

Wile this versatility is the major strength of SEN12MS, it also
poses the greatest challenge: In comparison to, e.g., ImageNet,
which also provides a lot of versatility and thus tries to model the
whole world, the number of samples in SEN12MS is still fairly
small. As can be seen from the example results provided in Sec-
tion 5, the dataset nevertheless seems to hold the potential to train
powerful, well-generalizing models even under data-scarce train-
ing situations. Besides, we believe that further beneﬁt will arise
from combining the existing datasets, e.g. BigEarthNet, with
SEN12MS in the frame of transfer learning to enlarge the amount
of data a model can learn patterns from.

7. SUMMARY AND CONCLUSION

With this paper, we publish the SEN12MS dataset, which con-
tains 180,662 triplets of Sentinel-1 dual-polarimetric SAR data,
Sentinel-2 multi-spectral images, and MODIS-derived land cover
maps. With its large patch size, its global scene distribution, and
its wealth of versatile remote sensing information, it can be con-
sidered to be the largest remote sensing dataset available to date.
We hope it will foster the development of well-generalizing ma-
chine learning models for a more sophisticated automatic analysis
of Sentinel satellite data.

ACKNOWLEDGEMENTS

This work is jointly supported by the Helmholtz Association
under the framework of the Young Investigators Group SiPEO
(VH-NG-1018), the German Research Foundation (DFG, grant
SCHM 3322/1-1), and the European Research Council (ERC) un-
der the European Unions Horizon 2020 research and innovation
programme (grant agreement ERC-2016-StG-714087, Acronym:
So2Sat).

REFERENCES

Basu, S., Ganguly, S., Mukhopadhyay, S., DiBiano, R., Karki,
M. and Nemani, R., 2015. DeepSat: a learning framework for
satellite imagery. In: Proc. ACM SIGSPATIAL. Art. No. 37.

Baumgardner, M. F., Biehl,
D. A.,
2015.
age data set:
https://purr.purdue.edu/publications/1947/1.

and Landgrebe,
L. L.
220 band AVIRIS hyperspectral
im-
June 12, 1992 Indian Pine Test Site 3.

Demir, I., Koperski, K., Lindenbaum, D., Pang, G., Huang, J.,
Basu, S., Hughes, F., Tuia, D. and Raska, R., 2018. DeepGlobe
2018: A challenge to parse the earth through satellite images. In:
Proc. CVPR Workshops, pp. 172–181.

Deng, J., Dong, W., Socher, R., Li, L., Li, K. and Fei-Fei, L.,
2009. ImageNet: A large-scale hierarchical image database. In:
Proc. CVPR, pp. 248–255.

Penatti, O. A. B., Nogueira, K. and dos Santos, J. A., 2015. Do
deep features generalize from everyday objects to remote sensing
and aerial scenes domains? In: Proc. CVPR Workshops, pp. 44–
51.

C.,

2019.

Rieke,
datasets.
awesome-satellite-imagery-datasets.
cessed 15-March-2019].

imagery
https://github.com/chrieke/
ac-

Awesome

[Online;

satellite

Schmitt, M., Hughes, L. H. and Zhu, X. X., 2018. The SEN1-
2 dataset for deep learning in SAR-optical data fusion. In: IS-
PRS Ann. Photogramm. Remote Sens. Spatial Inf. Sci., Vol. IV-1,
pp. 141–146.

Schmitt, M., Hughes, L. H., Qiu, C. and Zhu, X. X., 2019. Aggre-
gating cloud-free Sentinel-2 images with Google Earth Engine.
In: ISPRS Annals of the Photogrammetry, Remote Sensing and
Spatial Information Sciences.

Schubert, A., Small, D., Miranda, N., Geudtner, D. and Meier, E.,
2015. Sentinel-1A product geolocation accuracy: Commission-
ing phase results. Remote Sensing 7(7), pp. 9431–9449.

Sulla-Menashe, D. and Friedl, M. A., 2018. User guide to Collec-
tion 6 MODIS land cover (MCD12Q1 and MCD12C1) product.
USGS: Reston, VA, USA.

Sulla-Menashe, D., Gray, J. M., Abercrombie, S. P. and Friedl,
M. A., 2019. Hierarchical mapping of annual global land cover
2001 to present: The MODIS Collection 6 land cover product.
Remote Sens. Environ. 222, pp. 183 – 194.

Di Gregorio, A., 2005. Land cover classiﬁcation system: classi-
ﬁcation concepts and user manual: LCCS. Food & Agriculture
Org.

Sumbul, G., Charfuelan, M., Demir, B. and Markl, V., 2019.
BigEarthNet: A large-scale benchmark archive for remote sens-
ing image understanding. arXiv:1902.06148.

Drusch, M., Del Bello, U., Carlier, S., Colin, O., Fernandez, V.,
Gascon, F., Hoersch, B., Isola, C., Laberinti, P., Martimort, P.
et al., 2012. Sentinel-2: ESA’s optical high-resolution mission for
GMES operational services. Remote Sens. Environ. 120, pp. 25–
36.

Gorelick, N., Hancher, M., Dixon, M., Ilyushchenko, S., Thau,
D. and Moore, R., 2017. Google earth engine: Planetary-scale
geospatial analysis for everyone. Remote Sens. Environ. 202,
pp. 18–27.

He, K., Zhang, X., Ren, S. and Sun, J., 2016. Deep residual
learning for image recognition. In: Proc. CVPR, pp. 770–778.

Helber, P., Bischke, B., Dengel, A. and Borth, D., 2018. Intro-
ducing EuroSAT: A novel dataset and deep learning benchmark
In: Proc. IGARSS,
for land use and land cover classiﬁcation.
pp. 204–207.

J´egou, S., Drozdzal, M., Vazquez, D., Romero, A. and Bengio,
Y., 2017. The one hundred layers tiramisu: Fully convolutional
In: Proc. CVPR Work-
densenets for semantic segmentation.
shops, pp. 11–19.

Loveland, T. and Belward, A., 1997. The international geosphere
biosphere programme data and information system global land
cover data set (DISCover). Acta Astronautica 41(4-10), pp. 681–
689.

Maggiori, E., Tarabalka, Y., Charpiat, G. and Alliez, P., 2017.
Can semantic labeling methods generalize to any city? The Inria
aerial image labeling benchmark. In: Proc. IGARSS, pp. 3226–
3229.

Malkin, K., Robinson, C., Hou, L., Soobitsky, R., Samaras, D.,
Saltz, J., Joppa, L. and Jojic, N., 2019. Label super-resolution
networks. In: Proc. ICLR, pp. 1–9.

Mohajerani, S. and Saeedi, P., 2019.
Cloud-Net: An
end-to-end cloud detection algorithm for Landsat 8 imagery.
arXiv:1901.10077.

Nogueira, K., Dos Santos, J. A., Fornazari, T., Silva, T. S. F.,
Morellato, L. P. and Torres, R., 2016. Towards vegetation species
discrimination by using data-driven descriptors. In: Proc. PRRS.

Torres, R., Snoeij, P., Geudtner, D., Bibby, D., Davidson, M.,
Attema, E., Potin, P., Rommen, B., Floury, N., Brown, M. et al.,
2012. GMES Sentinel-1 mission. Remote Sens. Environ. 120,
pp. 9–24.

Warmerdam, F., 2008. The Geospatial Data Abstraction Library.
Springer, Berlin, pp. 87–104.

Xia, G., Bai, X., Ding, J., Zhu, Z., Belongie, S., Luo, J., Datcu,
M., Pelillo, M. and Zhang, L., 2018. Dota: A large-scale dataset
for object detection in aerial images. In: Proc. CVPR, pp. 3974–
3983.

Yang, Y. and Newsam, S., 2010. Bag-of-visual-words and spatial
extensions for land-use classiﬁcation. In: Proc. ACM SIGSPA-
TIAL, pp. 270–279.

Yokoya, N., Ghamisi, P., Xia, J., Sukhanov, S., Heremans, R.,
Tankoyeu, I., Bechtel, B., Saux, B. L., Moser, G. and Tuia, D.,
2018. Open data for global multimodal land use classiﬁcation:
Outcome of the 2017 IEEE GRSS data fusion contest. IEEE J.
Sel. Topics Appl. Earth Observ. 11(5), pp. 1363–1377.

Zhao, B., Zhong, Y., Xia, G. and Zhang, L., 2016. Dirichlet-
derived multiple topic scene classiﬁcation model for high spatial
resolution remote sensing imagery. IEEE Trans. Geosci. Remote
Sens. 54(4), pp. 2108–2123.

Zhong, Y., Zhu, Q. and Zhang, L., 2015. Scene classiﬁcation
based on the multifeature fusion probabilistic topic model for
IEEE Trans.
high spatial resolution remote sensing imagery.
Geosci. Remote Sens. 53(11), pp. 6207–6222.

Zhu, X. X., Tuia, D., Mou, L., Xia, G., Zhang, L., Xu, F. and
Fraundorfer, F., 2017. Deep learning in remote sensing: A com-
IEEE Geosci. Remote
prehensive review and list of resources.
Sens. Mag. 5(4), pp. 8–36.

