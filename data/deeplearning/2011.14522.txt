2
2
0
2

l
u
J

5
1

]

G
L
.
s
c
[

3
v
2
2
5
4
1
.
1
1
0
2
:
v
i
X
r
a

Feature Learning in Inï¬nite-Width Neural Networks

Greg Yang
Microsoft Research AI
gregyang@microsoft.com

Edward J. Huâˆ—
Microsoft Azure AI
edwardhu@microsoft.com

Abstract

As its width tends to inï¬nity, a deep neural networkâ€™s behavior under gradient
descent can become simpliï¬ed and predictable (e.g. given by the Neural Tangent
Kernel (NTK)), if it is parametrized appropriately (e.g. the NTK parametrization).
However, we show that the standard and NTK parametrizations of a neural network
do not admit inï¬nite-width limits that can learn features, which is crucial for pre-
training and transfer learning such as with BERT. We propose simple modiï¬cations
to the standard parametrization to allow for feature learning in the limit. Using
the Tensor Programs technique, we derive explicit formulas for such limits. On
Word2Vec and few-shot learning on Omniglot via MAML, two canonical tasks
that rely crucially on feature learning, we compute these limits exactly. We ï¬nd
that they outperform both NTK baselines and ï¬nite-width networks, with the latter
approaching the inï¬nite-width feature learning performance as width increases.
More generally, we classify a natural space of neural network parametrizations
that generalizes standard, NTK, and Mean Field parametrizations. We show 1) any
parametrization in this space either admits feature learning or has an inï¬nite-width
training dynamics given by kernel gradient descent, but not both; 2) any such
inï¬nite-width limit can be computed using the Tensor Programs technique. Code
for our experiments can be found at github.com/edwardjhu/TP4.

Figure 1: PCA of Word2Vec embeddings of top US cities and states, for NTK, width-64, and width-âˆ
feature learning networks (Deï¬nition 5.1). NTK embeddings are essentially random, while cities and
states get naturally separated in embedding space as width increases in the feature learning regime.

1

Introduction

The study of inï¬nite-width limits of neural networks, in particular the Neural Tangent Kernel
(NTK), has recently solved many longstanding open problems on the optimization and generaliza-
tion of overparametrized neural networks [26]. However, in the NTK limit, (last layer) features
learned during pretraining are essentially the same as those from random initialization (Corollary 3.9
and Theorem H.13); this is veriï¬ed empirically in Word2Vec in Fig. 1. As feature learning (e.g.
Imagenet and BERT) lies at the core of deep learningâ€™s far-ranging impact so far [7, 13, 23], this
insight amounts to a fatal weakness of the NTK theory as a model of neural networks in practice.

We seek to capture feature learning in overparametrized networks by considering other parametriza-
tions and their inï¬nite-width limits. By slightly modifying the standard parametrization (SP), in fact,
we can enable feature learning that is maximal in a sense to be explained shortly. We describe how to
compute this limit exactly (and rigorously) via the Tensor Programs technique developed in [49â€“52].

âˆ—Work done partly during the Microsoft AI Residency Program

NTKtypestatecityWidth 64Width  (Feature Learning) 
 
 
 
 
 
Feature Learning Inï¬nite-Width Networks on Real Tasks
We explicitly calculate this limit for the tasks of Word2Vec
[32, 33] and few-shot learning on Omniglot via MAML [16],2
two standard tasks relying crucially on feature learning. In
Word2Vec, an important early instance of large-scale language
pretraining, we must learn, in an unsupervised manner, word
embeddings so that similar words have close embeddings. Then
we test the learned embeddings on the word analogy task, which
asks questions of the kind â€œwhat to a queen is as a man to a
woman?â€ In few-shot learning, the model is asked to make
predictions given only a handful (e.g. 5) of labeled examples.
Metalearning/MAML makes this possible by having the model learn good representations of typical
examples that can adapt quickly, via a small number of SGD steps, to new few-shot learning tasks. On
both tasks, we ï¬nd our feature learning inï¬nite-width networks outperform both NTK baselines and
ï¬nite-width networks, with the latter approaching the inï¬nite-width performance as width increases.
Figure right shows this for one of our Word2Vec results. See Section 9 for our other experiments.

abc-Parametrizations This paper studies a natural class of parametrizations, which we call the
abc-Parametrization and describe here. Consider an L-hidden-layer perceptron: For weight matrices
W 1 âˆˆ RnÃ—d and W 2, . . . , W L âˆˆ RnÃ—n, and nonlinearity Ï† : R â†’ R, such a neural network on
input Î¾ âˆˆ Rd is given by h1(Î¾) = W 1Î¾ âˆˆ Rn, and

xl(Î¾) = Ï†(hl(Î¾)) âˆˆ Rn,

(1)
and the network output (also called the logit(s)) is f (Î¾) = W L+1xL(Î¾) for W L+1 âˆˆ R1Ã—n. An
abc-parametrization is speciï¬ed by a set of numbers {al, bl}l âˆª {c} such that

for l = 1, . . . , L âˆ’ 1,

hl+1(Î¾) = W l+1xl(Î¾) âˆˆ Rn,

(a) We parametrize each weight as W l = nâˆ’al wl for actual trainable parameter wl
Î±Î² âˆ¼ N (0, nâˆ’2bl), and
(b) We initialize each wl
(c) The SGD learning rate is Î·nâˆ’c for some width-independent Î·.3 4

Examples: The NTK parametrization (NTP) [26] has a1 = 0 and al = 1/2 for l â‰¥ 2; bl = 0 for all
l; c = 0. When depth L = 1, the Mean Field parametrization (MFP) [11, 30, 43, 45] has a1 = 0,
a2 = 1; bl = 0 for all l; c = âˆ’1. The standard parametrization (SP) available as the default setting in
PyTorch [39]5 has al = 0 for all l; b1 = 0 and bl = 1/2 for l â‰¥ 2; c = 0. However, we shall see that
c is too small (learning rate too large) in SP. We can deï¬ne abc-parametrization and generalize our
results to arbitrary neural architectures (Appendix C), but we shall focus on MLPs in the main text.

Dynamical Dichotomy For any abc-parametrization, if c is too small (i.e. learning rate too large),
SGD can lead to blowup of preactivation and/or logits; we say this parametrization is unstable. In
practice this translates to numerical issues. If c is too large (i.e. learning rate too small), then the
function computed by the network does not change in ï¬nite time; we say this parametrization is
trivial. We prove what we call the Dynamical Dichotomy theorem (Corollary 3.9):

Any nontrivial stable abc-parametrization yields a (discrete-time) inï¬nite-width limit.
This limit either 1) allows the embedding xL(Î¾) to evolve nontrivially (Deï¬nition 3.5) or
2) is described by kernel gradient descent in function space (Deï¬nition 3.7), but not both.

We call the former kind a feature learning limit and the latter a kernel limit. For 1-hidden-layer MLPs,
the former is exempliï¬ed by MFP, and the latter, NTP. This dichotomy implies that certain functional
dynamics, such as higher order generalizations of the NTK dynamics, are not valid inï¬nite-width
limits (see Remark 3.12). In addition, the neural network function f (deï¬ned in Eq. (1)) in any feature
learning limit must be identically 0 at initialization (see Corollary 3.10).6

2Short for Model Agnostic Meta-Learning
3Observe that by changing al, bl while holding al + bl ï¬xed, we effectively give layer l its own learning rate.
4One can further include a set of constants in front of nâˆ’al and nâˆ’bl , for example powers of input dimension

d, but we shall keep it simple here as we are only concerned with scaling behavior with n.

5This is also known as the â€œfaninâ€ or â€œLecunâ€ initialization; â€œKaimingâ€ initialization is the same up to
multiplicative constants. The default in Tensorï¬‚ow [1] uses Glorot initialization, where the variance of an entry
scales like 1/(f anin + f anout). This causes the ï¬rst layer preactivation to converge to 0 as n â†’ âˆ, and thus
yields pathological behavior in the limit.

6We stress this is in the n â†’ âˆ limit, so does not contradict the feature learning seen in ï¬nite-width SP NN.

2

51015epoch010203040word analogy accword2vec pretrained on text8log2(width)6.08.010.0NTK/GPStandard Param. Does Not Learn Features We
show that
the SP (resp. NTP) can only allow
O(1/width) (resp. O(1)) learning rate (i.e. c = 1,
resp. c = 0), so as to avoid blowup, and yield kernel
limits (Section 4). Instead, we propose a parametriza-
tion that has Î˜(1) max learning rate and admits fea-
ture learning maximally: it allows every parameter
to be updated maximally (in terms of scaling with
width) without leading to blowup (Section 5). We
thus call it the Maximal Update Parametrization (ab-
breviated MUP or ÂµP). It is given by a1 = âˆ’1/2,
aL+1 = 1/2, and al = 0 for all 2 â‰¤ l â‰¤ L; bl = 1/2
for all l; c = 0. In a 1-hidden-layer MLP, this spe-
cializes to MFP, up to symmetry (see Eq. (5)). The
â€œfeature learning limitsâ€ mentioned above in our main
experiments are ÂµP limits. Figure to the right: We
empirically verify our max learning rate predictions
on relu MLP with 2 hidden layers, trained with square loss on CIFAR10. We plot learning rate vs
accuracy in each subplot. Each curve represents MLP with a speciï¬c width. The right edge of each
curve indicates the max learning rate. The diagonal subplots scale the x-axes (log learning rate) in
the correct width-scaling for the corresponding parametrizations. We see, indeed, max learning rate
for SP scales like 1/width but is constant in ÂµP.

Key Theoretical Idea: Tensor Programs
In
Section 7 and Appendix H.4, we describe the
Tensor Programs technique for deriving (rigor-
ously) the inï¬nite-width training dynamics of
any abc-parametrization. The main insight of
this approach is:

When width is large, every activation vec-
tor has roughly iid coordinates, at any time
during training. Using Tensor Programs,
we can recursively calculate such coordi-
nate distributions, and consequently un-
derstand how the neural network function
evolves.

The Tensor Programs technique was developed
in a series of papers [49â€“52] that proved the architectural universality of the Neural Network-Gaussian
Process (NNGP) Correspondence and the Neural Tangent Kernel (NTK) limits and showed how to
compute the corresponding inï¬nite-width kernels. In the Figure above, the NNGP kernel can be
thought of as the â€œlimitâ€ of the ï¬rst forward pass of a randomly initialized model; the NTK can be
similarly thought of as the â€œlimitâ€ of its ï¬rst backward pass. The mechanics of calculating such limits
is 1) to write down the relevant neural network computation (e.g. the ï¬rst forward pass in the NNGP
case) as a principled composition of matrix multiplication and coordinatewise nonlinearities, called
a Tensor Program, and 2) to recursively calculate the distribution of coordinates of each vector via
whatâ€™s called the Master Theorem. In this paper, we follow the exact same recipe, where in 1) we just
write down the entire SGD training instead of only the ï¬rst step. More generally,

To derive the inï¬nite-width limit of any neural computation (e.g. SGD training),
1) express it as a Tensor Program, and 2) mechanically apply the Master Theorem.

For example, we easily recover the (discrete-time) 1-hidden-layer mean ï¬eld limit (Theorem 6.1).
It readily applies to practically any neural architecture (e.g. ResNet and Transformers)7 as well as
many common variants of SGD; however, in this paper, for pedagogical clarity, we only focus on
multilayer perceptrons. The generality of our approach allows us to easily adapt to settings outside the
traditional (CIFAR10-style) supervised classiï¬cation, such as the Word2Vec and few-shot learning
tasks in this paper, or reinforcement learning and image generation outside of our scope.

7e.g. by extending the example programs of [49, 51], which express only the ï¬rst forward and backward

passes, into the entire training computation.

3

10500.10.20.30.40.5Maximal Update Paramvalid accwidth512102420484096819205100.10.20.30.40.5max lr shifts1050log2(lr)0.10.20.30.40.5Standard Paramvalid accmax lr shifts0510log2(lr*width)0.10.20.30.40.5Verifying Max Learning Rate for P and SP1stforward pass1stbackward pass2ndforward pass2ndbackward passTthforward passTthbackward passSGD Training ProgressNNNNGPNTKFeature Learning LimitTake ğ‘¤ğ‘–ğ‘‘ğ‘¡â„â†’âˆLimit via Tensor ProgramsPrior worksThis workOur Contributions

1. Formulate a natural space of NN parametrizations (abc-parametrizations).

2. Prove Dynamical Dichotomy: Any nontrivial stable abc-parametrization yields either feature

learning or kernel limits, but not both.

3. Show both NTK and standard parametrizations yield kernel limits and propose the Maximal
Update Parametrization (ÂµP) , which admits maximal feature learning in a suitable sense.

4. Use Tensor Programs to derive the inï¬nite-width limit of ÂµP and, more generally, the limit

of any abc-parametrization. We verify our theory using extensive experiments.

5. Show the ÂµP limit outperforms both NNGP/NTK baselines and ï¬nite networks on 1)

Word2Vec and 2) Omniglot few-shot learning, trained via ï¬rst-order MAML.

Tensor Programs Series While this work is self-contained, it is positioned as the 4th paper in the
series, following Yang [49, 51, 52]. We do not extend the Tensor Programs machinery further here,
but instead extract the ï¬rst major payoff of the foundation laid in the earlier works. In fact, this paper
is the original motivation for this series; for a short history, see Appendix A.

2 Related Works

Comparison with Mean Field Limits For 1-hidden-layer MLP, the mean ï¬eld limit [11, 30, 43,
45] is equivalent to the ÂµP limit modulo the symmetry of Eq. (5) (see Section 3.1). Several works also
proposed different versions of mean ï¬eld frameworks for deeper MLPs [5, 15, 34, 35, 46]. However,
they did not consider the typical Gaussian N (0, 1/n) random initialization (or the appropriately
rescaled version in their respective parametrizations)8, which has a Central-Limit effect as opposed
to a Law-of-Large-Numbers effect. For example, [5, 35] can cover the case of N (0, 1/n2), instead
of N (0, 1/n), initialization, which in fact causes the function to be stuck at initialization. See
Appendix E for more explanations. Of these works, the mean ï¬eld limit of [15] has the form
most similar to what we derive here. There, as we do here, the coordinate distribution of each
(pre)activation vector is tracked recursively. The main difference is, while [15] has an atypical
initialization involving (cid:96)2 regression, we consider the usual Gaussian N (0, 1/n) scheme. Such a
(size nÃ—n) Gaussian matrix in the middle of the network has a distinctly different effect, more similar
to that of a Gaussian matrix in the usual NNGP/NTK calculation,9 than the â€œmean ï¬eldâ€ matrices
considered in [15] and previous works [5, 34, 35, 46], which has an â€œintegral kernelâ€ effect that is the
straightforward generalization of matrices to function spaces. Nevertheless, discrete time versions of
the 1-hidden-layer mean ï¬eld limit and of many of the multilayer limits (such as [15, 35]) can be
derived directly by writing the corresponding initialization and training inside a Tensor Program and
applying the Master Theorem (Theorem 7.4).

Discrete- vs Continuous-Time Gradient Descent At a high level, there are two natural limits
of neural networks training dynamics: large-width and continuous-time. Most prior works on
inï¬nite-width limits of neural networks also took the continuous-time limit simultaneously, e.g.
[11, 26, 30, 43, 45]. In contrast, here we only take the large width limit, so that gradient descent stays
discrete-time. Then the results of these prior works can be recovered by taking another continuous-
time limit. From a practical perspective, the continuous-time limit is often unnatural, e.g. 1) because
the step size is usually as large as possible to speed up training, 2) because of the task (such as
reinforcement learning), or 3) because of the importance of hyperparameters like batch size that are
hidden away in such limits. On the theory side, taking the continuous-time limit can create issues
with 1) well-posedness and 2) existence and uniqueness of the resulting ODE/PDE. While they can
sometimes be proved to hold, they are artifacts of the continuous-time limit, as the corresponding
questions for the discrete time evolution are trivial, and thus not relevant to the behavior of real
networks.

8In fact, empirically we observe such Gaussian random initialization to be crucial to performance compared

to the mean-ï¬eld-style initialization in this literature.

9Actually, it is more similar to the Gaussian matrix in asymmetric message passing [6] in that care must be

taken to keep track of correlation between W and W (cid:62).

4

Technical Assumptions Earlier works on neural tangent or mean ï¬eld limits (e.g. [11, 15, 26,
30, 35, 43, 45]) assume various forms of regularity conditions, such as 1) 0th, 1st, and/or 2nd
order smoothness on the nonlinearity or other related functions, and 2) the support boundedness,
subgaussianity, and/or PDF smoothness of initialization distributions. These are often either unnatural
or difï¬cult to check. In our work, the only assumption needed to rigorously obtain the inï¬nite-width
limit is that the nonlinearity Ï† has a polynomially bounded weak 2nd derivative and that the loss
function has a continuous derivative w.r.t. the prediction (Assumption H.22). In particular, when
we specialize to the 1-hidden-layer case and derive the discrete time version of the mean ï¬eld limit,
we cover the standard Gaussian initialization; in fact, we can allow any heavy-tailed initialization
that can be written as the image of a Gaussian under a pseudo-Lipschitz function, which include
nonsmooth PDFs and singular distributions.10 This generosity of technical assumptions is due to that
of the Tensor Programs Master Theorems proven in [49, 51, 52].

Training Time Many prior works (e.g. [4, 25, 30]) derived explicit time dependence of the con-
vergence to inï¬nite-width limit, so that a larger width can allow the network to stay close to the
limit for longer. In this paper, our results only concern training time independent of width, since
our primary objective is to investigate the limit itself and its feature learning capabilities. Moreover,
recent evidence suggests that, given a ï¬xed computational budget, itâ€™s always better to train a larger
model for a shorter amount of time [29], which validates the practical relevance of our limit mode.
Nevertheless, it is possible to prove a quantitative version of the Tensor Programs Master Theorem,
by which one can straightforwardly allow training time to increase with width.

Classiï¬cation of Parametrizations
[10] pointed out that the weights move very little in the NTK
limit, so that linearization approximately holds around the initial parameters, in contrast to the mean
ï¬eld limit (for 1-hidden-layer networks) where the weights move substantially. For this reason, they
called the former â€œlazy trainingâ€ and the latter â€œactive training,â€ which are classiï¬ed nonrigorously by
a multiplicative scaling factor of the logit (similar to nâˆ’aL+1 in this paper). While these terms are not
formally deï¬ned, they intuitively correspond to the kernel and feature learning regimes in our paper.
From a different perspective, [31] observed that the NTK and mean ï¬eld limit can be thought of as
short and long time-scale regimes of the mean ï¬eld evolution equations. Neither of the above works
attempted to formally classify natural parametrizations of neural networks. In contrast, [48] studied a
toy class of neural networks in the context of implicit regularization due to the scale Î± of initialization
(which is closely related to logit multiplier of [10] noted above). They identiï¬ed the Î± â†’ âˆ limit (of
the scale Î±, not of width) with the â€œkernel regimeâ€ and the Î± â†’ 0 limit with what they call the â€œrich
regimeâ€. They showed that the former is implicitly minimizing an (cid:96)2 risk while the latter, an (cid:96)1 risk.
They claim width allows the toy model to enter the kernel regime more naturally, but as we see in this
work, both kernel and feature learning regimes are admissible in the large width limit of a standard
MLP. Closer to our approach, [19] studied what amounts to a 2-dimensional subspace of the space of
stable abc-parametrizations for L = 1. They proposed a notion of stability which is similar to the
combination of stability and nontriviality in this paper. They characterized when the Neural Tangent
Kernel, suitably generalized to any parametrization and playing a role similar to the feature kernel
in this paper, evolves over time. However, to simplify the proofs, they assumed that the gradients
for the different weight matrices are estimated using different inputs, a very unnatural condition.
In contrast, here our results are for the usual SGD algorithm applied to MLPs of arbitrary depth.
In all of the above works and most of existing literature, not much attention is paid to the feature
learning capabilities of neural networks in the right parametrization, as opposed to our focus here. A
notable exception is [12], which showed that the mean ï¬eld limit, but not the NTK limit, can learn
low dimension linear structure of the input distribution resulting in ambient-dimension-independent
generalization bounds.

Other Related Works
[27] proposed a toy model to study how large learning rate can induce
a neural network to move out of the kernel regime in â„¦(log(width)) time. Since our dichotomy
result only concerns training for O(1) time (which, as we argue above, is more practically relevant),
there is no contradiction. [47] also noted that standard parametrization leads to unstable training
dynamics. They then injected constants in the NTK parametrization, such as Î±/
n
and tuned Î± in the resulting kernel. [2, 3] also observed the lack of feature learning in NNGP and
NTK limits but, in contrast to taking the exact limit of SGD training as we do here, they proposed a
deep kernel process as a way of loosely mimicking feature learning in ï¬nite-width networks. [17]

n instead of 1/

âˆš

âˆš

10We wonâ€™t expand further here, but it can be derived straightforwardly from the Master Theorem

(Theorem 7.4).

5

empirically observed that wider networks achieve better downstream performance with linear transfer
learning, even though on the original pretraining task there can be little difference. We ï¬x the input
dimension d in this work, but one can also consider varying d with width n, e.g. [36, 38]. [28] proved
a complexity separation between NTK and ï¬nite-width networks by showing the latter approximates
a sort of inï¬nite-width feature learning network. In the literature surrounding NTK, often there are
subtle differences in parametrization leading to subtle differences in conclusion (e.g. [4, 14, 57]).
Our abc framework encapsulates all such parametrizations, and can easily tell when two ostensibly
different parametrizations (e.g. [14, 57]) are actually equivalent or when they are really different (e.g.
[4, 14]) via Eq. (5).

3 Feature Learning vs Kernel Behavior

In this section, we give a characterization of training procedures that induce feature learning vs kernel
behavior; we will elaborate on what we mean by these two kinds of behavior below. We ï¬rst motivate
this discussion by reviewing the well-known tangent kernel and mean ï¬eld limits of a shallow neural
network.

3.1 Motivating Examples: Neural Tangent Kernel and Mean Field Limits

For simplicity, deï¬ne a shallow network f (Î¾) with input/output dimension 1 by

f (Î¾) = V x(Î¾) âˆˆ R,

x(Î¾) = Ï†(h(Î¾)) âˆˆ Rn,

h(Î¾) = U Î¾ âˆˆ Rn.

(2)

As a specialization of Eq. (1), we parametrize weights V = nâˆ’av v âˆˆ R1Ã—n and U = nâˆ’au u âˆˆ RnÃ—1,
where the width n should be thought of as tending to âˆ, and v, u should be thought of as the actual
trainable parameters. We will sample vÎ± âˆ¼ N (0, nâˆ’2bv ), uÎ± âˆ¼ N (0, nâˆ’2bu ) for Î± âˆˆ [n]. The
learning rate is Î·nâˆ’c for some Î· independent of n.

For example, in the Neural Tangent Parametrization (abbreviated NTP) [26], au = bv = bu = 0,
av = 1/2, c = 0. The Mean Field Parametrization (abbreviated MFP) corresponds to av = 1,
au = bu = bv = 0, c = âˆ’1; however, as will be explained shortly, we will use the equivalent
formulation au = âˆ’1/2, av = bu = bv = 1/2, c = 0 in this section so c = 0 for both NTP and MFP.
We remark that the GP limit, i.e. training only the last layer of a inï¬nite-wide, randomly initialized
network, is a special case of the NTK limit where the ï¬rst layer is not trained. Everything we discuss
below about the NTK limit specializes to the GP limit appropriately.

Given an input Î¾, the gradient of f can be calculated as

dx(Î¾) = V,

dh(Î¾) = dx(Î¾) (cid:12) Ï†(cid:48)(h(Î¾)),

dv(Î¾) = nâˆ’av x(Î¾),

du(Î¾) = nâˆ’audh(Î¾)Î¾

where d â€¢ (Î¾) is shorthand for âˆ‡â€¢f (Î¾) (however, note that later in Section 6, d â€¢ (Î¾) will stand for
nâˆ‡â€¢f (Î¾)). For loss function L : R Ã— R â†’ R, the loss gradient on a pair (Î¾, y) is then given by
L(cid:48)(f (Î¾), y)[dv(Î¾), du(Î¾)] (where L(cid:48) denotes derivative in ï¬rst argument).

Note that one can keep the function f invariant while changing the magnitude of the gradient dv by
changing av, bv, holding av + bv constant; likewise for du. Thus, the trajectory of f stays ï¬xed if,
for any Î¸ âˆˆ R, we set au â† au + Î¸, av â† av + Î¸, bu â† bu âˆ’ Î¸, bv â† bv âˆ’ Î¸, c â† c âˆ’ 2Î¸ (also see
Eq. (5)). With Î¸ = âˆ’1/2, this explains why the two formulations of MFP above are equivalent. Then,
for both NTP and MFP, we will consider the dynamics of f trained under stochastic gradient descent
with learning rate Î· = 1 and batch size 1, where the network is fed the pair (Î¾t, yt) at time t, starting
with t = 0. This simplicity is intended to intuitively illustrate our points below, but we shall state
formal results regarding more common settings in Section 3.2.

Notation and Setup Below, when we say a (random) vector v âˆˆ Rn has coordinate size O(na)
(written v = O(na)),11 we mean (cid:112)(cid:107)v(cid:107)2/n = O(na) with high probability for large n. Intuitively,
this means that each coordinate has a typical ï¬‚uctuation of O(na). Likewise if O(na) is replaced
with Î˜(na) or â„¦(na). See Deï¬nition H.2 for a formal deï¬nition.

Let ft, ht, xt, Ut, Vt, dxt, dht, dvt, dut denote the corresponding objects at time t, with t = 0 corre-
sponding to random initialization. We also abuse notation and write xt = xt(Î¾t), i.e. applying the
function xt speciï¬cally to tth input Î¾t; similarly for ft, ht, dxt, dht, dvt, dut. These symbols will

11Contrast this with a common semantics of v = O(na) as (cid:107)v(cid:107) = O(na).

6

never appear by themselves to denote the corresponding function, so this should cause no confusion.
Then SGD effectively updates U and V by

Ut+1 = Ut âˆ’ Ï‡tnâˆ’au dut,

Vt+1 = Vt âˆ’ Ï‡tnâˆ’av dvt.

where Ï‡t
example, after 1 SGD update, we have, for any Î¾ âˆˆ R,

def= L(cid:48)(ft, yt). Finally, let âˆ†â€¢t

def= â€¢t âˆ’ â€¢0, for all â€¢ âˆˆ {f, h, x, U, V, dx, dh, dv, du}. For

âˆ†h1(Î¾) = h1(Î¾) âˆ’ h0(Î¾) = âˆ’nâˆ’au Ï‡0Î¾du0 = âˆ’nâˆ’2au Ï‡0Î¾0Î¾dh0

= âˆ’nâˆ’2au Ï‡0Î¾0Î¾dx0 (cid:12) Ï†(cid:48)(h0)

âˆ†f1(Î¾) = V0âˆ†x1(Î¾) + âˆ†V1x1(Î¾) = V0âˆ†x1(Î¾) âˆ’ nâˆ’av dv(cid:62)

0 x1(Î¾)

= V0âˆ†x1(Î¾) âˆ’ nâˆ’2av x(cid:62)

0 x1(Î¾)

(3)

(4)

3.1.1 Key Observations

Letâ€™s list a few characteristics of the NTK and MF limits in the context of the shallow network in
Eq. (2), and then discuss them in the general setting of deep MLP. We will keep our discussion
intuitive to carry across the key ideas.

Feature Evolution For a generic Î¾ âˆˆ R, its embedding vector x0(Î¾) has coordinates of Î˜(1) size
in both NTP and MFP. However, for any t â‰¥ 1 independent of n, âˆ†xt(Î¾) generically has coordinate
size Î˜(1/

n) in NTP but Î˜(1) in MFP.

âˆš

(in NTP)

(in NTP)

Example for t = 1: By Eq. (3), we have

âˆ†h1(Î¾) = nâˆ’2au Ï‡0Î¾0Î¾dx0 (cid:12) Ï†(cid:48)(h0).

Plug in au = 0 for NTP. Observe that Î¾0, Î¾, Ï‡0 = Î˜(1),12 so

âˆ†h1(Î¾) = Î˜(1) Â· dx0 (cid:12) Ï†(cid:48)(h0).

In addition, Ï†(cid:48)(h0) = Î˜(1) because h0 = Î˜(1), so

Finally, dx0 = V0 = Î˜(1/

âˆš

âˆ†h1(Î¾) = Î˜(1) Â· dx0 (cid:12) Î˜(1).

n) in NTP. Altogether, this implies

âˆ†h1(Î¾) = Î˜(1/
=â‡’ âˆ†x1(Î¾) â‰ˆ Ï†(cid:48)(h0(Î¾)) (cid:12) âˆ†h1(Î¾) = Î˜(1/

âˆš

n)

âˆš

n) â†’ 0,

as n â†’ âˆ.

(in NTP)

On the other hand, in MFP, the only thing different is au = âˆ’1/2 and dx0 = Î˜(1/n), which implies
(in MFP)

âˆ†h1(Î¾) = Î˜(n) Â· Î˜(1/n) (cid:12) Î˜(1) = Î˜(1) =â‡’ âˆ†x1(Î¾) = Î˜(1).

Feature Kernel Evolution Therefore the feature kernel Ft(Î¾, Î¶) def= xt(Î¾)(cid:62)xt(Î¶)/n does not
change in the NTK limit but it does in the MF limit, i.e. for any ï¬xed t â‰¥ 1,13

lim
nâ†’âˆ
lim
nâ†’âˆ

Ft(Î¾, Î¶) = lim
nâ†’âˆ
Ft(Î¾, Î¶) (cid:54)= lim
nâ†’âˆ

F0(Î¾, Î¶),

in NTP, but

F0(Î¾, Î¶),

in MFP, in general.

Indeed, regardless of parametrization, we have

(cid:2)x0(Î¾)(cid:62)x0(Î¶) + âˆ†xt(Î¾)(cid:62)x0(Î¶) + x0(Î¾)(cid:62)âˆ†xt(Î¶) + âˆ†xt(Î¾)(cid:62)âˆ†xt(Î¶)(cid:3) .

Ft(Î¾, Î¶) =

1
n

In NTP, because âˆ†xt(Î¾) = Î˜(1/

âˆš

n) as noted above,
n
(cid:88)

âˆ†xt(Î¾)Î±x0(Î¶)Î± =

1
n

âˆ†xt(Î¾)(cid:62)x0(Î¶) =

1
n

Î±=1

1
n

n
(cid:88)

Î±=1

O(nâˆ’1/2) = O(nâˆ’1/2),

and likewise the other terms involving âˆ†xt will vanish as n â†’ âˆ. But in MFP, âˆ†xt(Î¾) = Î˜(1) will
in general be correlated with x0(Î¶) such that 1
It may seem somewhat puzzling how the NTK limit induces change in f without feature or feature
kernel evolution. We give some intuition in Appendix B.

n âˆ†xt(Î¾)(cid:62)x0(Î¶) = 1

Î±=1 Î˜(1) = Î˜(1).

(cid:80)n

n

12Ï‡0 = L(cid:48)(f0, y0) = Î˜(1) because f0 has variance Î˜(1).
13here the limit should be construed as almost sure limits; see Theorem 7.4.

7

Pretraining and Transfer Learning The simple fact above about the feature kernel K implies
that the NTK limit is unable to perform linear transfer learning. By linear transfer learning, we mean
the popular style of transfer learning where one discards the pretrained linear classiï¬er layer and
train a new one on top of the features (e.g. x in our example), which are ï¬xed. Indeed, this is a linear
problem and thus only depends on the kernel of the features. If this kernel is the same as the kernel at
initialization, then the pretraining phase has had no effect on the outcome of this â€œtransferâ€ learning.

In fact, a more sophisticated reasoning shows pretraining in the NTK limit is no better than random
initialization for transfer learning even if ï¬netuning is performed to the whole network, not just
the classiï¬er layer. This remains true if we replace the linear classiï¬er layer by a new deep neural
network. See Remark H.16 and Theorem H.17. The Word2Vec experiment we do in this paper is a
linear transfer task.

In some other settings, such as some settings of metalearning, like the few-shot learning task in
this paper, the last layer of the pretrained network is not discarded. This is called adaptation. Then
the NTK limit does not automatically trivialize transfer learning. However, as will be seen in our
experiments, the NTK limit still vastly underperforms the feature learning limit, which is exempliï¬ed
by the MF limit here.

In NTP, as n â†’ âˆ, (cid:104)âˆ‡U,V f0(Î¾), âˆ‡U,V f0(Î¶)(cid:105)
Kernel Gradient Descent in Function Space
converges to some deterministic value K(Î¾, Î¶) such that K forms a kernel (the NTK). Then, in
this limit, if the learning rate is Î·, the function f evolves according to kernel gradient descent
ft+1(Î¾) = ft(Î¾) âˆ’ Î·K(Î¾, Î¾t)Ï‡t. However, this shouldnâ€™t be the case for the MF limit. For example,
if Ï† is identity, then intuitively ft+1(Î¾) âˆ’ ft(Î¾) should be quadratic in Î·, not linear, because two
layers are updated at the same time.

3.2

abc-Parametrizations and Dynamical Dichotomy

In this section, we broaden our scope to the abc-parametrizations of deeper MLPs, deï¬ned by Eq. (1),
and their inï¬nite-width limits. In Table 1, we summarize the {al, bl}l âˆª {c} values of various
abc-parametrizations in the literature.
Assumption 3.1. Our main results in this section (and this section only) will assume Ï† is either tanh
or a smooth version of relu called Ïƒ-gelu (see Deï¬nition H.1), for sufï¬ciently small Ïƒ > 0 (which
means Ïƒ-gelu approximates relu arbitrarily well).

Note this assumption is only needed for the classiï¬cation of abc-parametrizations. For deriving the
inï¬nite-width limits, the much weaker Assumption H.22 sufï¬ces. We believe our results here will
hold for generic nonlinearities, but making this precise is outside our scope. (See Remark H.15 for an
overview on how Assumption 3.1 is used).

Symmetries of abc-Parametrizations As above, we can scale the parameter gradients âˆ‡wl f
arbitrarily while keeping f ï¬xed, if we vary al, bl while ï¬xing al + bl: âˆ‡wl f is scaled by nâˆ’Î¸ if
al â† al + Î¸, bl â† bl âˆ’ Î¸. In other words, changing al, bl this way effectively gives wl a per-layer
learning rate. If we apply this gradient with learning rate Î·nâˆ’c, then the change in W l is scaled by
Î·nâˆ’câˆ’2Î¸. Consequently, if c â† c âˆ’ 2Î¸, then W l is not affected by the change in al, bl. In summary,
âˆ€Î¸ âˆˆ R : ft(Î¾) stays ï¬xed for all t and Î¾ if we set al â† al + Î¸, bl â† bl âˆ’ Î¸, c â† c âˆ’ 2Î¸.

(5)

Stable abc-Parametrizations We will only consider abc-parametrizations such that, as n â†’ âˆ,
1) the preactivations {hl}l and activations {xl}l have Î˜(1) coordinates at initialization, and 2) their
coordinates and the logit f (Î¾) all stay O(1) throughout the course of SGD.14 Otherwise, they tend
to âˆ with n, eventually going out of ï¬‚oating point range. Indeed, this is an acute and real problem
common in modern deep learning, where ï¬‚oat16 is necessary to train large models. We call any such
parametrization stable (see Deï¬nition H.4 for a formal deï¬nition). Thus unstable parametrizations
are of no practical interest.

It turns out stable abc-parametrizations can be characterized by a set of inequalities on {al, bl}l âˆª {c}
(so that the stable ones form a polyhedron). To present these inequalities succinctly, itâ€™s useful to
deï¬ne

14but they may depend on training time and Î·; in particular, itâ€™s possible that they diverge with time.

8

Table 1: We summarize the abc values of SP (standard), NTP (Neural Tangent), MFP (Mean Field,
for 1-hidden-layer nets), ÂµP (Maximal Update, ours). We show the minimal value of c such that the
parametrization is stable (Deï¬nition H.4). We also list the quantities r, 2aL+1 + c, aL+1 + bL+1 + r
involved in stability, feature learning, and kernel regime properties of the parametrizations. Here we
only focus on scaling with n and ignore dependence on input dimension. Recall the MLP deï¬nition:
h1 = W 1Î¾ âˆˆ Rn, xl = Ï†(hl) âˆˆ Rn, hl+1 = W l+1xl âˆˆ Rn, f (Î¾) = W L+1xL

Deï¬nition

SP (w/ LR 1
n )

NTP

MFP (L = 1)

ÂµP (ours)

al

W l = nâˆ’al wl

0

bl wl

Î±Î² âˆ¼ N (0, nâˆ’2bl )
LR = Î·nâˆ’c
Deï¬nition 3.2

c
r

2aL+1 + c
aL+1 + bL+1 + r
Nontrivial?
Stable?
Feature Learning?
Kernel Regime?

(cid:26)0
1/2

l = 1
l â‰¥ 2

1
1/2

1
1
(cid:88)
(cid:88)

(cid:88)

(cid:26)0
1/2

l = 1
l â‰¥ 2

(cid:26)0
1

l = 1
l = 2

ï£±
âˆ’1/2
ï£²
0
ï£³
1/2

l = 1
2 â‰¤ l â‰¤ L
l = L + 1

0

0
1/2

1
1
(cid:88)
(cid:88)

(cid:88)

0

âˆ’1
0

1
1
(cid:88)
(cid:88)
(cid:88)

1/2

0
0

1
1
(cid:88)
(cid:88)
(cid:88)

Deï¬nition 3.2. For any abc-parametrization, we write r for the quantity

r def= min(aL+1 + bL+1, 2aL+1 + c) + c âˆ’ 1 +

L
min
l=1

[2al + I(l = 1)] .

For example, in NTP, r = 1/2, while in MFP (when L = 1), r = 0. Intuitively, r is the exponent
t (Î¾) = Î˜(nâˆ’r). Thus, to avoid activation blowup, we want r â‰¥ 0; to perform feature
such that âˆ†xL
learning, we want r = 0.

Theorem 3.3 (Stability Characterization, c.f. Theorem H.6). An abc-parametrization is stable iff all
of the following are true (with intuitions in parentheses):

1. ((pre)activations xl

0, hl

0 at initialization are Î˜(1) and logits f0 are O(1))

a1 + b1 = 0;

al + bl = 1/2, âˆ€l âˆˆ [2, L];

aL+1 + bL+1 â‰¥ 1/2.

(6)

2. (features donâ€™t blowup, i.e. âˆ†xl

t = O(1) for all l)

r â‰¥ 0.

3. (logits donâ€™t blow up during training, i.e. âˆ†W L+1

t

t , W L+1
xL

0 âˆ†xL

t = O(1))

2aL+1 + c â‰¥ 1;

aL+1 + bL+1 + r â‰¥ 1.

(7)

(8)

Nontrivial abc-Parametrizations Among stable abc-parametrizations, there are also those where
f does not change throughout training in the inï¬nite-width limit. We say such parametrizations are
trivial. Our dichotomy result will only apply to nontrivial stable abc-parametrizations.15

Nontrivial abc-parametrizations can also be described by a disjunction of equations on {al, bl}l âˆª
{c} (geometrically, they correspond to the union of two faces on the polyhedron of stable abc-
parametrizations).
Theorem 3.4. A stable abc-parametrization is nontrivial iff aL+1 + bL+1 + r = 1 or 2aL+1 + c = 1.

15In particular, itâ€™s possible for the function f to stay ï¬xed with time, but for the features to change.

9

Feature Learning Below, for brevity, we say training routine to mean the package of learning
rate Î·nâˆ’c, training sequence {(Î¾t, yt)}tâ‰¥0,16 and a loss function L(f (Î¾), y) that is continuously
differentiable in the prediction of the model f (Î¾). As above, we use â€¢t to denote the object â€¢ after t
steps of SGD.
Deï¬nition 3.5 (c.f. Deï¬nitions H.9 and H.11). We say an abc-parametrization admits feature
learning (resp. evolves the feature kernel) if, as n â†’ âˆ, âˆ†xL
t (Î¾) has â„¦(1) coordinates (resp.
1
n (xL
0 (Î¶)) = â„¦(1)) for some training routine, time t â‰¥ 1, and input Î¾ (resp.
Î¾, Î¶).17

t (Î¶) âˆ’ xL

0 (Î¾)(cid:62)xL

t (Î¾)(cid:62)xL

MFP, in the 1-hidden-layer case, is an example of feature learning parametrization.

Intuitively, feature kernel evolution implies feature learning, but a priori it seems possible that the
latter can occur without the former (akin to some kind of rotation of features). If so, then, e.g. in
terms of linear transfer learning, the pretraining ultimately had no beneï¬t. But, in fact,
Theorem 3.6. A nontrivial stable abc-parametrization admits feature learning iff it evolves the
feature kernel iff r = 0.

Kernel Regime While feature learning here is deï¬ned by looking at the embedding of an input Î¾,
we can also look at the dynamics of the function represented by the neural network.
Deï¬nition 3.7 (c.f. Deï¬nition H.12). We say an abc-parametrization is in kernel regime if there
exists a positive semideï¬nite kernel K such that, for any training routine, time t â‰¥ 0, and input Î¾, in
the n â†’ âˆ limit,

ft+1(Î¾) = ft(Î¾) âˆ’ Î·K(Î¾, Î¾t)L(cid:48)(ft(Î¾t), yt),

âˆ€t â‰¥ 0.

(9)

In other words, SGD reduces to kernel gradient descent in the large n limit.
Theorem 3.8. A nontrivial stable abc-parametrization is in kernel regime iff r > 0.

NTP is a typical example of this, where r = 1/2 and K is given by the NTK.

Dynamical Dichotomy Since a stable abc-parametrization
has either r = 0 or r > 0 by Eq. (7):
Corollary 3.9. A nontrivial stable abc-parametrization either
admits feature learning or is in kernel regime, but not both.

Note that kernel regime (Deï¬nition 3.7) is not deï¬ned as lack
of feature learning, so Corollary 3.9 is not a trivial statement.
In addition, Assumption 3.1 is necessary. For example, if Ï† is
linear, then this dichotomy doesnâ€™t hold, as a 1-hidden-layer
linear network where only the ï¬rst layer is trained would both
admit feature learning and is in kernel regime.

An interesting consequence of Dynamical Dichotomy is
Corollary 3.10. Any nontrivial stable feature learning abc-
parametrization must have limnâ†’âˆ f0(Î¾) = 0 for all Î¾, where
the limit is almost sure.

Theorems 3.6 and 3.8 and Corollary 3.10 are consequences of
the more general classiï¬cation theorem Theorem H.13, which
in addition shows: 1) feature learning in layer l would imply the
same for layers l, . . . , L; 2) in any feature learning parametriza-
tion, ft in the large n limit becomes deterministic, and thus is
incompatible with any Bayesian perspective (in contrast to the
NNGP limit).

Figure 2: A Caricature of abc-
Parametrizations. The nontrivial
stable parametrizations form a high
dimensional polyhedron. Those on
a part of its boundary admit feature
learning, while all others are in ker-
nel regime. ÂµP is a vertex in the for-
mer, while NTP, latter. See Fig. 5
for a more geometrically accurate
depiction.

Dynamical Dichotomy in the shallow perceptron case is illustrated by the NTK and MF limits, as
presented in Section 3.1, which shows the NTK limit exempliï¬es Theorem 3.8 while the MF limit

16For simplicity, we only consider batch size 1; itâ€™s straightforward to generalize to larger batch sizes.
17For the sake of streamlining the main text presentation, we deï¬ned feature learning and feature kernel
evolution slightly differently than in Deï¬nition H.9, but ultimately they are equivalent as a result of our theorems.

10

KernelRegimeNeuralTangentStandardğ¿ğ‘…=Î˜(1/ğ‘¤ğ‘–ğ‘‘ğ‘¡â„)MaximalUpdate(ours)UnstableorTrivialSpace of abc-ParametrizationsMean Fieldwhen depth=1Standardğ¿ğ‘…=Î˜(1)exempliï¬es Theorem 3.6. We present a simpliï¬ed picture of abc-parametrizations in Fig. 2, but see
Fig. 5 for a more geometrically accurate depiction.

The paragraph above Appendix H.2 gives a quick outline of the proof of Dynamical Dichotomy, and
the beginning of each succeeding section outlines the logic of that section.
Remark 3.11 (Function Space Picture). A kernel regime limit resides solely in the function space
picture, i.e. the evolution of f at any time being solely determined by the function values {lim ft(Î¶)}Î¶
themselves (as opposed to the internal activations of f as well) along with Î·, L, and (Î¾t, yt). Intuitively,
this cannot be true for the feature learning limit, and therefore, at least informally, Dynamical
Dichotomy is also a dichotomy over the sufï¬ciency of the function space picture for determining
the training evolution: We can construct two settings where {lim ft(Î¶)}Î¶, Î·, L, and (Î¾t, yt) are the
same but ft+1 are different. 1) The ï¬rst setting is at t = 0, where lim ft(Î¶) = 0 for all input Î¶ by
Corollary 3.10. Here a typical SGD will change f . 2) In the second setting, suppose Ï† is relu. Design
a sequence of inputs such that training the MLP on them with very large learning rate will make all
relu neurons saturated in the 0 region. Then f is everywhere 0, and an SGD step will not change that.
Remark 3.12 (Not All Dynamics are Inï¬nite-Width Limits). Accordingly, a nonlinear function space
dynamics cannot be a valid inï¬nite-width limit of some abc-parametrization. By nonlinear, we mean
ft+1(Î¾) âˆ’ ft(Î¾) is nonlinear in L(cid:48)(ft(Î¾t), yt). For example, any natural higher-order generalization
of Eq. (9) (perhaps derived from a Taylor expansion at initialization) is not a valid limit.18

Pretraining and Transfer Learning As in the shallow examples, Corollary 3.9 says that any
kernel regime parametrization (including NTP) trivializes pretraining and transfer learning19 in the
inï¬nite-width limit.

By calculating r for the standard parametrization (SP), we can easily see that it cannot admit feature
learning in the sense here without becoming unstable. However, in the next section, we will manually
analyze the training dynamics in an SP MLP to give an intuition why this is the case. In turn, we then
propose a simple modiï¬cation of SP, the Maximal Update Parametrization (MUP or ÂµP), which does
admit feature learning and, in fact, does so maximally in a suitable sense. In the pedagogical spirit,
we will focus on the key insights and stress the right heuristics without dwelling on formal aspects.

4 Standard Parametrization

In this section, we give intuition for why gradient descent of neural network in standard parametriza-
tion (SP) will lead to logits blowup after 1 step, if the learning rate is Ï‰(1/n), where n is the width.
In addition, we will see why, with learning rate O(1/n), SP is in kernel regime. We ï¬rst consider the
simplest example and then state the general result at the end of the section.

To demonstrate the general principle in deep networks, it is necessary to consider the behavior of an
n Ã— n matrix in the middle of the network. Thus, the simplest case is a 2-hidden-layer linear MLP,
i.e. Eq. (1) with L = 2 and Ï† = id. The standard parametrization is given by

al = 0 âˆ€l,

b1 = 0,

bl = 1/2 âˆ€l â‰¥ 2.

(SP)

We consider 1 step of SGD with learning rate nâˆ’c on a single data pair (Î¾, y). Then we can without
ambiguity suppress explicit dependence on Î¾ and write
Â¯h = W h,

f = V Â¯h,

h = U Î¾,

(10)

where UÎ±Î² âˆ¼ N (0, 1) and WÎ±Î², VÎ±Î² âˆ¼ N (0, 1/n) are the trainable parameters (simplifying the
notation in Section 3). As in Section 3, we use â€¢t to denote the quantity â€¢ after t step of SGD.
Because we only focus on the 1st step of SGD, we lighten notation and write â€¢ = â€¢0.

Initialization Since U, W, V are independently sampled, a standard Central Limit argument would
show that h, Â¯h, f all have roughly iid Gaussian coordinates of variance Î˜(1).

18It may seem that Neural Tangent Hierarchy [25], which allow some kind of higher order dynamics in the
function space, violates our observation. But their inï¬nite-width limit is identical to NTK in the constant time
t = O(1) regime, which is what Remark 3.12 (and this paper) concerns. Moreover, here we are talking about
functional dynamics that doesnâ€™t depend on n (because we are already at the n â†’ âˆ limit) whereas their
functional dynamics does.

19linear and nonlinear; see Theorem H.17.

11

First Gradient Now letâ€™s consider the gradients of f on the data pair (Î¾, y), which are given by

dÂ¯h = V (cid:62),
dV = Â¯h,

dh = W (cid:62)dÂ¯h,
dW = dÂ¯h h(cid:62) = V (cid:62)h(cid:62),

dU = dh Î¾(cid:62).

(11)

For simplicity, suppose we only update W by learning rate nâˆ’c (and leave U, V unchanged); our
conclusion will not change in the general case where we train all layers. Then with Ï‡ denoting the
loss derivative L(cid:48)(f, y), we can write

W1 = W âˆ’ nâˆ’cÏ‡ dW.

We shall show now that c â‰¥ 1 or else f1 blows up with the width n after this SGD step.

After First SGD Step At t = 1, h1 = h since we did not update U , but

Â¯h1 = W1h = Â¯h âˆ’ nâˆ’cÏ‡ dW h = Â¯h âˆ’ nâˆ’cÏ‡ Â· V (cid:62)h(cid:62)h
f1 = V Â¯h1 = f âˆ’ nâˆ’cÏ‡ V V (cid:62)h(cid:62)h.

(12)

(13)

Now, as noted above, h has iid Î˜(1) coordinates, so h(cid:62)h = Î˜(n) âˆˆ R. Similarly, V âˆˆ R1Ã—n has
Gaussian coordinates of variance Î˜(1/n), so V V (cid:62) = Î˜(1) âˆˆ R. Finally, for typical loss function
L like MSE or cross entropy, Ï‡ = L(cid:48)(f, y) is of order Î˜(1) because f ï¬‚uctuates on the order Î˜(1).
Altogether,

f1 = f âˆ’ Î˜(n1âˆ’c).

Therefore, for f1 to remain O(1), we must have c â‰¥ 1, i.e. the learning rate is O(1/n).

Kernel Regime and Lack of Feature Learning Consequently, the network cannot learn features
in the large width limit if we would like the logits to not blow up. Indeed, this version of SGD where
only W is updated can be seen to correspond to the limit where

a1 = Î¸,

b1 = âˆ’Î¸,

a2 = 0,

b2 = 1/2,

a3 = Î¸,

b3 = âˆ’Î¸ + 1/2,

Î¸ â†’ âˆ.

With c = 1 as derived above, the parametrization is stable and nontrivial, as can be checked from
Theorems 3.3 and 3.4. Then we get r = 1/2 > 0, so by Corollary 3.9, this parametrization is in
kernel regime and does not admit feature learning. We can also see this directly from Eq. (12): from
our calculations above,

Â¯h1 âˆ’ Â¯h = O(n1âˆ’c) V (cid:62) = O(1) V (cid:62)
whose coordinates have size O(nâˆ’1/2) since V â€™s coordinates do, so thereâ€™s no feature learning (at
least in the ï¬rst step). Finally, from Eq. (13), because V V (cid:62) â†’ 1 and nâˆ’ch(cid:62)h = nâˆ’1h(cid:62)h â†’ (cid:107)Î¾(cid:107)2,
we get20

f1 âˆ’ f â†’ âˆ’Ï‡K(Î¾, Î¾) def= âˆ’Ï‡(cid:107)Î¾(cid:107)2,
i.e. f evolves by kernel gradient descent with the linear kernel. Our derivations here only illustrate
the ï¬rst SGD step, but we can get the same conclusion from all steps of SGD similarly.

We summarize the general case below, which follows trivially from Theorem 3.3 and Corollary 3.9.
Theorem 4.1. An L-hidden-layer MLP in standard parametrization (see Eq. (SP) and Table 1) can
only allow SGD learning rate of order O(1/n) if we require limnâ†’âˆ E ft(Î¾)2 < âˆ for all training
routine, time t, and input Î¾. In this case, it is in kernel regime and does not admit feature learning.

5 Maximal Update Parametrization

As shown in the last section, the standard parametrization does not admit a feature learning inï¬nite-
width limit without blowing up logits. Here we propose simple modiï¬cations of the standard
parametrization to make this possible while maintaining stability: 1) To enable feature learning,
n and use Î˜(1) learning rate, i.e. set aL+1 = 1/2, c = 0 on
it sufï¬ces to divide the logits by
top of Eq. (SP); 2) to allow every layer to perform feature learning, we should furthermore set
a1 = âˆ’1/2, b1 = 1/2. We will see that this essentially means we update each weight matrix as
much as possible without blowing up the logits or activations, so we call this the Maximal Update
Parametrization (abbreviated MUP or ÂµP).

âˆš

20Formally, these are almost sure convergences, but we suppress these details to emphasize on intuition.

12

5.1 Dividing Logits by

âˆš

n

For example, in the 2-hidden-layer linear MLP example above, the network would compute

f (Î¾) =

1
âˆš
n

vÂ¯h(Î¾),

Â¯h(Î¾) = W h(Î¾),

h(Î¾) = U Î¾,

(14)

where UÎ±Î² âˆ¼ N (0, 1) and WÎ±Î², vÎ±Î² âˆ¼ N (0, 1/n) are the trainable parameters. Compared to SP
(Eq. (10)), h(Î¾), Â¯h(Î¾) stays the same; only the logit f (Î¾) is scaled down. Again, to simplify notation,
we abbreviate â€¢ = â€¢0 and suppress explicit dependence on Î¾. This has two consequences

Logits at Initialization Converge to 0
MLP in SP at initialization).

since f has variance Î˜(1/n) (compare to the GP limit of

Î˜(1) Learning Rate and Feature Learning Even though f â†’ 0, the loss derivative Ï‡ = L(cid:48)(f, y)
stays Î˜(1) if y (cid:54)= 0. When we redo the calculation in Eq. (12), we see

Â¯h1 = Â¯h âˆ’ nâˆ’câˆ’1/2Ï‡ v(cid:62)h(cid:62)h = Â¯h âˆ’ Î˜(nâˆ’c+1/2)v(cid:62)
f1 = f âˆ’ nâˆ’câˆ’1Ï‡ vv(cid:62)h(cid:62)h = f âˆ’ Î˜(nâˆ’c).

(15)

Because v has coordinates of size Î˜(nâˆ’1/2), we see that Â¯h and f both change by Î˜(1) coordinatewise
if c = 0 (i.e. learning rate is Î˜(1)). This directly illustrates feature learning after just 1 step of SGD.
For general MLPs, we can also check aL+1 = 1/2, c = 0 on top of Eq. (SP) implies r = 0 and thus
admits feature learning by Theorem 3.6.

Kernel Behavior or Lack Thereof The example we have here, where we only train the middle
layer in a linear MLP, actually is in kernel regime. This does not violate Corollary 3.9, however,
which assumes Assumption 3.1. If, for example, we have tanh nonlinearity, then it is easy to see the
ÂµP SGD dynamics does not have a kernel limit: If so, then f1 âˆ’ f is linear in the learning rate Î·. But
note Â¯h1 âˆ’ Â¯h is Î˜(1) as n â†’ âˆ and linear in Î·, as can be derived similarly to Eq. (15). Because tanh
is bounded, this cannot happen. Contrast this with SP or NTP, where Â¯h1 âˆ’ Â¯h is Î˜(1/
n) and thus
â€œresides in the linear regime of tanhâ€, allowing perfect scaling with Î·.

âˆš

In addition, even in an linear MLP, if we train the middle layer and the last layer, then the dynamics
intuitively will become quadratic in the weights, so will not have a kernel limit. Contrast this with SP
or NTP, which suppress these higher order interactions because the learning rate is small, and a ï¬rst
order Taylor expansion heuristic holds.

âˆš

âˆš

n) after 1 step of SGD with learning rate Î˜(1/

n? As shown
How is this different from standard parametrization with learning rate 1/
above, the logit f blows up like Î˜(
n) in the
standard parametrization, but remains Î˜(1) in our parametrization here. The reason these two
parametrizations seem similar is because in the 1st step, the weights receive the same updates modulo
the loss derivative Ï‡ = L(cid:48)(f, y). Consequently, xL
1 âˆ’ hL are Î˜(1) coordinatewise
in both cases. However, this update makes xL
xL
1 (and f1)
1
1
scales like Î˜(n1âˆ’aL+1âˆ’bL+1) due to Law of Large Numbers. Thus only in our parametrization here
(aL+1 = bL+1 = 1/2) is it Î˜(1), while in standard parametrization (aL+1 = 0, bL+1 = 1/2) it
n). Contrast this with the behavior at initialization, where W L+1 and xL are
blows up like Î˜(
independent and zero-mean, so W L+1xL scales like Î˜(n1/2âˆ’aL+1âˆ’bL+1) by Central Limit Theorem.

1 âˆ’ xL and hL
1 correlated with W L+1

, so that W L+1

âˆš

âˆš

5.2 First Layer Parametrization

While this now enables feature learning, the ï¬rst layer preactivation h effectively stays ï¬xed through-
out training even if we were to train U . For example, if we update U in the linear MLP example
Eq. (14), then by Eq. (11),

U1 = U âˆ’ nâˆ’cÏ‡ dU = U âˆ’ nâˆ’cÏ‡ dhÎ¾(cid:62)
h1 = U1Î¾ = h âˆ’ nâˆ’cÏ‡ dhÎ¾(cid:62)Î¾ = h âˆ’ Î˜(nâˆ’c)dh

since Î¾(cid:62)Î¾, Ï‡ = Î˜(1). Now dh = W (cid:62)dÂ¯h = W (cid:62) 1âˆš
of size Î˜(1/n), since 1âˆš

n v(cid:62) has roughly iid Gaussian coordinates, each
n v(cid:62) has coordinates of the same size. Therefore, even with c = 0, h changes

13

by at most O(1/n) coordinatewise, which is dominated by its value at initialization. This O(1/n)
change also induces a O(1/n) change in f , which would be dominated by the Î˜(1) change due to
W â€™s evolution, as seen in Eq. (15).

We therefore propose to set a1 = âˆ’1/2, b1 = 1/2 on top of Section 5.1â€™s parametrization. This
implies the forward pass of f remains the same but U â€™s gradient is scaled up by n, so that h now
changes by Î˜(1) coordinatewise. In summary, we deï¬ne
Deï¬nition 5.1. The Maximal Update Parametrization (abbreviated MUP, or ÂµP), in the context of
an L-hidden-layer MLP (Eq. (1)), is given by

c = 0,

bl = 1/2 âˆ€l,

al =

ï£±
ï£²

ï£³

âˆ’1/2
0
1/2

l = 1
2 â‰¤ l â‰¤ L
l = L + 1.

Notice that ÂµP for a 1-hidden-layer perceptron is equivalent to the mean ï¬eld parametrization by
Eq. (5). We also describe ÂµP for any architecture in Appendix C.1.

5.3 What is ÂµP Maximal In?

For technical reasons, we adopt Assumption 3.1 again for the formal results of this section.

t

, h = hl

t for any l â‰¥ 2 due to learning rate nâˆ’c
In an abc-parametrization, the change in weight W = W l
is Î´W def= âˆ’nâˆ’c Â· nâˆ’2adh x(cid:62) where we abbreviated x = xlâˆ’1
t, a = al. (We will use Î´ to
denote 1-step change, but âˆ† to denote lifetime change). In the next forward pass, Î´W contributes
Î´W Â¯x = âˆ’n1âˆ’câˆ’2a(x(cid:62) Â¯x/n)dh, where Â¯x is the new activation due to change in previous layersâ€™
weights. In general, x and Â¯x are strongly correlated. Then x(cid:62) Â¯x/n â†’ R for some R (cid:54)= 0 by Law of
Large Numbers (as they both have Î˜(1) coordinates in a stable parametrization). One can heuristically
see that dh has the same size as the last layer weights, which is Î˜(nâˆ’(aL+1+bL+1) + nâˆ’(2aL+1+c))
(where the ï¬rst summand is from W L+1
). Thus, Î´W Â¯x is a vector
with Î˜(nâˆ’rl ) def= Î˜((nâˆ’(aL+1+bL+1) + nâˆ’(2aL+1+c))n1âˆ’câˆ’2a) coordinates. If rl > 0, then Î´W Â¯x
contributes vanishingly; if rl < 0, then Î´W Â¯x blows up. For l = 1, we get similar insights after
accounting for the ï¬nite dimensionality of Î¾.
Deï¬nition 5.2. For l âˆˆ [L], we say W l is updated maximally if âˆ†W l
for some training routine21, time t â‰¥ 1, and input Î¾.
Proposition 5.3. In a stable abc-parametrization, for any l âˆˆ [L], W l is updated maximally iff

and the other from âˆ†W L+1

(Î¾) has Î˜(1) coordinates

t xlâˆ’1
t

0

t

rl

def= min(aL+1 + bL+1, 2aL+1 + c) + c âˆ’ 1 + 2al + I(l = 1) = 0.

Note that r (Deï¬nition 3.2) is the minimum of rl over all l. In ÂµP, we can calculate that rl = 0 for all
l âˆˆ [L], so all W l, l âˆˆ [L], are updated maximally. Put another way, the ï¬nal embedding xL(Î¾) will
have nonvanishing (nonlinear) contributions from âˆ†W l of all l. These contributions cause the logit
and âˆ†W L+1
f (Î¾) to change via interactions with W L+1
are too small,
then the logit is ï¬xed to its initial value, so all of the feature learning would have been useless.22 Itâ€™s
also possible for one to contribute vanishingly but not the other.23 But both contribute in ÂµP.
Deï¬nition 5.4. We say W L+1 is updated maximally (resp. initialized maximally) if âˆ†W L+1
xL
t (Î¾) =
Î˜(1) (resp. W L+1

t
t (Î¾) = Î˜(1)) for some training routine, time t â‰¥ 1, and input Î¾.

. If both W L+1

and âˆ†W L+1

0 âˆ†xL

0

0

t

t

(Î¾) âˆˆ Rn.
Note Deï¬nition 5.4 is similar to Deï¬nition 5.2 except âˆ†W L+1
Proposition 5.5. In a stable abc-parametrization, W L+1 is 1) updated maximally iff 2aL+1 + c = 1,
and 2) initialized maximally iff aL+1 + bL+1 + r = 1.

t (Î¾) âˆˆ R but âˆ†W l
xL

t xlâˆ’1
t

t

We remark that, by Theorem 3.4, a parametrization is nontrivial iff W L+1 is maximally updated or
initialized. Using Propositions 5.3 and 5.5 and Theorem 3.3, we can now easily conclude

21Recall that training routine means a package of learning rate Î·nâˆ’c, training sequence {(Î¾t, yt)}tâ‰¥0, and a

loss function L(f (Î¾), y) that is continuously differentiable in the prediction of the model f (Î¾).

22It is indeed possible to perform feature learning in a trivial parametrization, e.g. bl = 1/2 âˆ€l, a1 =

âˆ’1/2, a2 = 100 + 1/2, c = âˆ’100 in a 2-hidden-layer MLP.

23e.g. take aL+1 = 100 + 1/2, bL+1 = âˆ’100 + 1/2, then âˆ†W L+1 is negligible.

14

Theorem 5.6. In ÂµP, W l is updated maximally for every l âˆˆ [L + 1], and W L+1 is also initialized
maximally. ÂµP is the unique stable abc-parametrization with this property.

6 Deriving Feature Learning Inï¬nite-Width Limit: Intuition and Examples

We propose the Tensor Programs technique for deriving the inï¬nite-width limit of any abc-
parametrization. This ultimately just requires the researcher to mechanically apply a set of rules to
the computation graph underlying SGD. However, while operationally simple, this procedure would
seem â€œtoo magicalâ€ at ï¬rst. In this section, through a series of examples, we seek to build intuition
for what is being automated by this procedure. Then, in the next section, we formally describe the
Tensor Programs framework.

Setup and Notation For pedagogical simplicity, we only consider input dimension d = 1 and
learning rate Î· = 1 here, but generalization to d > 1, Î· (cid:54)= 1 is straightforward. We consider SGD
with a singleton minibatch {(Î¾t, yt)} at time t = 0, 1, 2, . . ., where Î¾t is the network input and yt
t for the matrix W l after t steps of such training. For any network input
is the label. We write W l
Î¾ âˆˆ R, we write xl
t(Î¾), ft(Î¾)) for the activation xl (resp. preactivation hl, logits f ) of
ft(Î¾) (resp. nâˆ‡hl
ft(Î¾)) by
the network after t steps of SGD. We denote the scaled gradient nâˆ‡xl
t(Î¾) (resp. dhl
dxl
t (without being applied to Î¾) to also
denote the vector xl
t, ft. We will not use xl
t, dhl
t
on its own to denote the function Î¾ (cid:55)â†’ xl
t(Î¾) so this should not cause confusion. The loss function is
def= L(cid:48)(ft, yt).
denoted L and the loss derivative L(cid:48)(logit, target) is in the ï¬rst argument. We write Ï‡t

t(Î¾t) (applied speciï¬cally to Î¾t); likewise for hl

t(Î¾)). For brevity, we abuse notation and use xl

t(Î¾) (resp. hl

t, dxl

t

t

6.1

1-Hidden-Layer MLP

As mentioned above, for 1 hidden layer, the inï¬nite-width ÂµP limit is the same as the mean ï¬eld
limit of [11, 30, 43, 45]. Nevertheless, we present a slightly different derivation of this that is more
consistent with the philosophy of Tensor Programs. Such a network on input Î¾ âˆˆ R is given by

f (Î¾) = V x(Î¾),

x(Î¾) = Ï†(h(Î¾)),
âˆš

h(Î¾) = U Î¾,
(16)
for U âˆˆ RnÃ—1, V âˆˆ R1Ã—n parametrized like U =
n v and with initialization uÎ±Î², vÎ±Î² âˆ¼
N (0, 1/n).24 Then U0 (the initial value of U ) has iid N (0, 1) coordinates. It will turn out to be
convenient to represent each such coordinate distribution as a random variable Z U0 def= N (0, 1).
Likewise, let Z nV0 def= N (0, 1), independent from Z U0 , represent the coordinate distribution of nV0
(we do nV0 instead of V0 so that the Z random variable is always independent of n). We derive
the ÂµP limits of the ï¬rst forward and backward passes manually before stating the general case. To
lighten notation, we suppress the t = 0 subscript (e.g. U = U0, h = h0, f = f0, etc), as we will
spend some time on the ï¬rst SGD step.

nu, V = 1âˆš

First Forward Pass After randomly initialization, the preactivation h = h(Î¾) (where Î¾ = Î¾0 âˆˆ R
is the ï¬rst input) has iid coordinates, each a sample from Z h def= Î¾Z U âˆˆ R. Naturally, x = x(Î¾) has
iid coordinates as well, each a sample from Z x def= Ï†(Z h). Finally, f = V x = 1
Î±=1(nV )Î±xÎ± â†’
n
Ëšf def= E Z nV Z x by Law of Large Numbers as n â†’ âˆ.25 In particular, f becomes deterministically
0 in this limit because V and U are independent. For a typical loss function L, the loss derivative
Ï‡ def= L(cid:48)(f, y) then also become deterministic, Ï‡ â†’ ËšÏ‡ def= L(cid:48)(Ëšf , y).

(cid:80)n

First Backward Pass Similarly, dx = nV (cid:62) (recall dxt
def= nâˆ‡xtft) has coordinates distributed like
Z dx def= Z nV and dh = dx(cid:12)Ï†(cid:48)(h) has coordinates distributed like Z dh def= Z dxÏ†(cid:48)(Z h) = Z nV Ï†(cid:48)(Z h).
Then SGD with learning rate 1 makes the following updates:
âˆš

v1 = v âˆ’ Ï‡x/
u1 = u âˆ’ Ï‡Î¾ dh/

n
âˆš

=â‡’

n

=â‡’

V1 = V âˆ’ Ï‡x/n
U1 = U âˆ’ Ï‡Î¾ dh.

24Again, more generally, we can insert constants in this parametrization, like U =

here for simplicity.

âˆš
âˆš

n
d

u, but we omit them

25All convergence in this section will be almost sure, but to focus on the intuition here and less on the

formalities, we do not explicitly write this down.

15

Since Ï‡ converges to a deterministic limit ËšÏ‡, the coordinates of these updates are roughly iid,
corresponding to an update of Z random variables:

Z nV1 = Z nV âˆ’ ËšÏ‡Z x, Z U1 = Z U âˆ’ ËšÏ‡Î¾Z dh.

Second Forward Pass Thus V1 and U1 still have roughly iid coordinates after 1 SGD step. Then,
in the second forward pass, h1 has coordinates

Z h1 def= Î¾1Z U1 = Î¾1Z U âˆ’ Î¾1ËšÏ‡Î¾Z dh = Î¾1Z U âˆ’ Î¾1ËšÏ‡Î¾Z nV Ï†(cid:48)(Z h),

x1 has coordinates Z x1 def= Ï†(Z h1), and the output is
n
(cid:88)

f1 =

1
n

Î±=1

(nV1)Î±xÎ± â†’ Ëšf1

def= E Z nV1Z x1 = E(Z nV âˆ’ ËšÏ‡Z x)Z x1

(17)

as n â†’ âˆ. Then Ï‡1
have roughly iid coordinates by a similar logic.

def= L(cid:48)(f1, y1) â†’ ËšÏ‡1

def= L(cid:48)(Ëšf1, y1) becomes deterministic. The gradient vectors

tth Iteration Repeating the above reasoning shows that at any time t (independent of n), we obtain

Theorem 6.1. Consider a 1-hidden-layer MLP in ÂµP (Eq. (16)) and any training routine with
learning rate 1. Suppose Ï†(cid:48) is pseudo-Lipschitz.26 As n â†’ âˆ, for every input Î¾, ft(Î¾) converges
almost surely to Ëšft(Î¾) deï¬ned as follows:

ft(Î¾) a.s.âˆ’âˆ’â†’ Ëšft(Î¾) def= E Z nVtZ xt(Î¾), Z xt(Î¾) def= Ï†(Z ht(Î¾)), Z ht(Î¾) def= Î¾Z Ut,
def= L(cid:48)(Ëšft, yt), Z nVt+1 def= Z nVt âˆ’ ËšÏ‡tZ xt, Z Ut+1 def= Z Ut âˆ’ ËšÏ‡tÎ¾tZ nVtÏ†(cid:48)(Z ht),

ËšÏ‡t

(18)

(19)

with, as initial conditions, Z U0 and Z nV0 being independent standard Gaussians, where in Eq. (19)
we abbreviated Ëšft = Ëšft(Î¾t), xt = xt(Î¾t), ht = ht(Î¾t).

As aforementioned, this is a discrete time, minibatched version of the mean ï¬eld limit of [11, 30,
43, 45].27 When Ï† is identity, itâ€™s easy to see that Z nVt and Z Ut are always (deterministic) linear
combinations of Z nV0 and Z U0 , say Z nVt = AtZ nV0 + BtZ U0 and Z Ut = CtZ nV0 + DtZ U0. Then
the limit Ëšft depends solely on At, Bt, Ct, Dt. By tracking their evolution, we get the following
greatly simpliï¬ed formula for an inï¬nite-width ÂµP linear network.
Corollary 6.2. Consider a 1-hidden-layer linear MLP in ÂµP (Eq. (16)) and any training routine
with learning rate 1. As n â†’ âˆ, for every input Î¾, ft(Î¾) converges almost surely to Ëšft(Î¾) deï¬ned as
follows:

Ëšft(Î¾) = (AtCt + BtDt)Î¾, ËšÏ‡t = L(cid:48)(Ëšft, yt),

(At+1, Bt+1) = (At, Bt) âˆ’ ËšÏ‡tÎ¾t(Ct, Dt),
(Ct+1, Dt+1) = (Ct, Dt) âˆ’ ËšÏ‡tÎ¾t(At, Bt),

with initial condition A0 = D0 = 1, B0 = C0 = 0.

This can be easily generalized to larger input and output dimenions (see Appendix D.1). In a gist, such
an inï¬nite-width ÂµP linear network with input dimension d and output dimension do is equivalent to
a width-(d + do) linear network with the same input/output dimensions but an â€œdiagonalâ€, instead of
random, initialization. Our Word2Vec and MAML experiments will crucially rely on this simplifying
observation. We remark that, in contrast to our approach, such an observation would be obscured by
the PDE perspective of prior works [11, 30, 43, 45].

26This roughly means that Ï†(cid:48) has a polynomially bounded weak derivative; see Deï¬nition F.3.
27[11, 30, 43, 45] present the equations in terms of the PDF of Z random variables. Formally, the PDF limit
can be obtained by taking the continous-time limit of Eqs. (18) and (19) and then applying Focker-Planck. Note
our derivation, when formalized using the Tensor Programs framework below, does not require smoothness
and support assumptions on the initialization of U, V in those works: The initialization distribution here can be
replaced with any image of Gaussians under pseudo-Lipschitz functions, which includes nonsmooth and singular
distributions.

16

6.2

2-Hidden-Layer MLP: SGD with Partially Decoupled Backpropagation

A 2-hidden-layer MLP is given by

f (Î¾) = V Â¯x(Î¾),

Â¯h(Î¾) = W x(Î¾),

Â¯x(Î¾) = Ï†(Â¯h(Î¾)),
for U âˆˆ RnÃ—1, W âˆˆ RnÃ—n, V âˆˆ R1Ã—n parametrized like U =
n v and with initial-
ization uÎ±Î², WÎ±Î², vÎ±Î² âˆ¼ N (0, 1/n). The presence of the n Ã— n Gaussian matrix W (â€œâˆ Ã— âˆâ€
as opposed to â€œâˆÃ— ï¬niteâ€ like U or â€œï¬nite Ã—âˆâ€ like V ) is new and has two major effects on the
inï¬nite-width training dynamics: 1) A Central Limit effect from the random Gaussian nature of
W and 2) a correlation effect between W and its transpose W (cid:62). We isolate the ï¬rst effect here by
analyzing a slightly different version of backpropagation (which has a different limit than normal
backpropagation), and then discuss the second effect in the next section. We abuse notation and
abbreviate W = W0.

x(Î¾) = Ï†(h(Î¾)),
nu, V = 1âˆš

h(Î¾) = U Î¾,

âˆš

Partially Decoupled Backpropagation In this section, we analyze a version of SGD where the
backpropagation weights are partially decoupled from the forward propagation weights. Here, we
think of âˆ†Wt as the trainable weights, initialized at 0, and think of the Gaussian W as untrainable
â€œconstantsâ€. The forward pass proceeds normally28 with Wt = W + âˆ†Wt. But we sample and ï¬x an
iid copy (cid:102)W of W (cid:62) before training, and in the backward pass compute

t )dÂ¯ht

dxt = ((cid:102)W + âˆ†W (cid:62)

instead of dxt = (W (cid:62) + âˆ†W (cid:62)

(20)
In particular, at initialization, we would have dx0 = (cid:102)W dÂ¯h0 instead of dx0 = W (cid:62)dÂ¯h0. Everything
else stays the same in the backward pass29. Finally, each weight is still updated by SGD via the usual
outer products: with Ï‡t
vt+1 = vt âˆ’ Ï‡t Â¯x(cid:62)
t /

def= L(cid:48)(ft, yt),
âˆš
n, âˆ†wt+1 = âˆ†wt âˆ’ Ï‡tdÂ¯htx(cid:62)

ut+1 = ut âˆ’ Ï‡tÎ¾tdh(cid:62)
t /
nu per ÂµP, this causes the following changes in W s:

n, W = w, U =

Since V = v/

t )dÂ¯ht = W (cid:62)

t dÂ¯ht.

t /n,

(21)

n.

âˆš

âˆš

âˆš

Vt+1 = Vt âˆ’ Ï‡t Â¯x(cid:62)

t /n, âˆ†Wt+1 = âˆ†Wt âˆ’ Ï‡tdÂ¯htx(cid:62)

t /n, Ut+1 = Ut âˆ’ Ï‡tÎ¾tdh(cid:62)
t

(22)

Note here we update âˆ†w and âˆ†W instead of w and W .

Why This Decoupled SGD? The reasons we talk about this version of SGD is that it isolates the
effect of having a Gaussian n Ã— n matrix (cid:102)W in the backward pass, and we can derive its inï¬nite-width
limit relatively easily using Central Limit heuristics. In the normal version of SGD, (cid:102)W would equal
W (cid:62), and its correlation with W creates additional terms in the inï¬nite-width dynamics, that are
better explained on their own.

Again, we walk through the ï¬rst few forward and backward passes to gain some intuition for the
inï¬nite-width limit, before stating the general case.

First Forward Pass
deriving the NNGP30.

is similar to that in Section 6.1 and follows the usual calculations involved in

First Backward Pass
is similar to that in Section 6.1 and to calculations involved in deriving
Neural Tangent Kernel, except swapping W (cid:62) with (cid:102)W (which at this point has no visible effect,
because of the Gradient Independence Phenomenon [51]; but the effect will become clear in the
second forward pass)31. We end up with âˆ†W1 = âˆ’Ï‡0dÂ¯h0x(cid:62)

0 , as usual.
28i.e. ft = Vt Â¯xt, Â¯xt = Ï†(Â¯ht), Â¯ht = (W + âˆ†Wt)xt, xt = Ï†(ht), ht = U Î¾t.
t , dÂ¯ht = Ï†(cid:48)(Â¯ht) (cid:12) dÂ¯xt, dht = Ï†(cid:48)(ht) (cid:12) dxt
29i.e. dÂ¯xt = nV (cid:62)
301) h0 is iid Gaussian with coordinates drawn from Z h0 = Î¾0Z U0 ; 2) x0 has coordinates Z x0 = Ï†(Z h0 );
3) Â¯h0 = W x0 has roughly iid coordinates drawn from a zero-mean Gaussian Z Â¯h0 by a Central Limit heuristic,
where Z Â¯h0 is correlated with Z Â¯h0(Î¾) for any Î¾ (including Î¾ = Î¾0) with covariance Cov(Z Â¯h0 , Z Â¯h0(Î¾)) =
Î±=1(nV0)Î± Â¯x0Î± â†’
limnâ†’âˆ
Ëšf0

0 x0(Î¾) = E Z x0 Z x0(Î¾); 4) Â¯x0 has coordinates Z Â¯x0 = Ï†(Z Â¯h0 ); 5) f0 = 1

n x(cid:62)

(cid:80)n

n

1

def= E Z nV0 Z Â¯x0 by a Law of Large Number heuristic.
311) dÂ¯x0 = nV (cid:62)

0 so Z dÂ¯x0 = Z nV0 ; 2) Z dÂ¯h0 = Ï†(cid:48)(Z Â¯h0 ) (cid:12) Z dÂ¯x0 ; 3) Z dx0 = Z (cid:102)W dÂ¯h0 is Gaussian with
0 dÂ¯h0(Î¾) = E Z dÂ¯h0 Z dÂ¯h0(Î¾) for any input Î¾; 4) Z dh0 =
def=

covariance Cov(Z dx0 , Z dx0(Î¾)) = limnâ†’âˆ
Ï†(cid:48)(Z h0 ) (cid:12) Z dx0 . Since f converges to a deterministic number Ëšf0, we also generically have L(cid:48)(f, y0) â†’ ËšÏ‡0
L(cid:48)(Ëšf0, y0). Finally, the weights are updated like Eq. (22).

n dÂ¯h(cid:62)

1

17

Second Forward Pass As usual, we have Z h1 = Î¾1Z U1 = Î¾1Z U0 âˆ’ ËšÏ‡0Î¾1Î¾0Z dh0 and Z x1 =
Ï†(Z h1 ), reï¬‚ecting the coordinate distributions of h1 and x1

32. Next,

Â¯h1 = W x1 + âˆ†W1x1 = W x1 âˆ’ Ï‡0dÂ¯h0

x(cid:62)
0 x1
n

.

(23)

On one hand, 1) x(cid:62)
0 x1
n â†’ E Z x1 Z x0 by a Law of Large Numbers heuristic. On the other hand, 2)
by a Central Limit heuristic, W x1 should roughly have Gaussian coordinates ZW x1 correlated with
Z Â¯h0 = ZW x0 with Cov(ZW x1 , ZW x0) = lim x(cid:62)
0 x1
n = E Z x1 Z x0. However, very importantly, this
Central Limit heuristic is correct only because we used (cid:102)W in backprop instead of W (cid:62); otherwise,
h1 has a strong correlation with W through dh0 = Ï†(cid:48)(h0) (cid:12) (W (cid:62)dÂ¯h0), and thus so does x1, so
that W x1 no longer has Gaussian coordinates. This is the â€œsecond major effectâ€ referred to in the
beginning of this section. See Section 6.3 for how to handle this correlation.

In any case, in our scenario here,

Z

Â¯h1 def= ZW x1 âˆ’ cZ dÂ¯h0 , where

c = ËšÏ‡0 E Z x1Z x0 ,

is a linear combination of a Gaussian variable and the gradient dÂ¯h0â€™s coordinate random vari-
able. Finally, Z Â¯x1 = Ï†(Z Â¯h1) and the logit is f1 = 1
def= E Z nV1Z Â¯x1 =
n
E Z nV0 Z Â¯x1 âˆ’ ËšÏ‡0 E Z Â¯x0 Z Â¯x1.

Î±=1(nV1)Î± Â¯x1Î± â†’ Ëšf1

(cid:80)n

Second Backward Pass Everything proceeds just like in the 1-hidden-layer case33 except for the
computation of

1 dÂ¯h1 = (cid:102)W dÂ¯h1 âˆ’ Ï‡0x0

0 dÂ¯h1
dÂ¯h(cid:62)
dx1 = (cid:102)W dÂ¯h1 âˆ’ âˆ†W (cid:62)
n
Like in the computation of Â¯h1 in Eq. (23), dÂ¯h(cid:62)
0 dÂ¯h1
n â†’ E Z dÂ¯h0Z dÂ¯h1 and (cid:102)W dÂ¯h1 is roughly Gaussian
(and correlated with (cid:102)W dÂ¯h0 in the natural way). But again, for this Gaussian intuition to be correct, it
is crucial that we use (cid:102)W here instead of W (cid:62), or else dÂ¯x1 (and thus dÂ¯h1) is strongly correlated with
W (cid:62) (through Â¯x0 = Ï†(W x0) inside nâˆ†V1 = âˆ’Ï‡0 Â¯x(cid:62)
In any case, we have

0 ).

.

Z dx1 = Z (cid:102)W dÂ¯h1 âˆ’ cZ x0, where

c = ËšÏ‡0 E Z dÂ¯h0 Z dÂ¯h1,

is a sum of Gaussian Z (cid:102)W dÂ¯h1 and a multiple of Z x0 . Then weights are updated according to Eq. (22).

tth Iteration For general t, we always have (true in normal SGD as well)

so that in the forward pass

âˆ†Wt = âˆ’

1
n

tâˆ’1
(cid:88)

s=0

Ï‡sdÂ¯hsx(cid:62)
s

Â¯ht = W xt + âˆ†Wtxt = W xt âˆ’

tâˆ’1
(cid:88)

s=0

Ï‡sdÂ¯hs

x(cid:62)
s xt
n

Z

Â¯ht def= ZW xt âˆ’

tâˆ’1
(cid:88)

s=0

ËšÏ‡sZ dÂ¯hs E Z xsZ xt.

(24)

Here ZW xt is Gaussian with covariance Cov(ZW xt, ZW xs) = E Z xt Z xs for any s. This means that
Z Â¯ht and Z Â¯hs are correlated through ZW xt, ZW xs (but also through Z dÂ¯hr , r â‰¤ min(t, s)). Likewise,
in the backward pass,

dxt = (cid:102)W dÂ¯ht âˆ’ âˆ†W (cid:62)dÂ¯ht = (cid:102)W dÂ¯ht âˆ’

tâˆ’1
(cid:88)

s=0

Ï‡sxs

s dÂ¯ht
dÂ¯h(cid:62)
n

Z dxt def= Z (cid:102)W dÂ¯ht âˆ’

tâˆ’1
(cid:88)

s=0

ËšÏ‡sZ xs E Z dÂ¯hs Z dÂ¯ht

32Recall they abbreviate h1(Î¾1) and x1(Î¾1)
33dÂ¯x1 = nV (cid:62)

1 , dÂ¯h1 = dÂ¯x1 (cid:12) Ï†(cid:48)(Â¯h1), dh1 = dx1 (cid:12) Ï†(cid:48)(h1)

18

Here, Z (cid:102)W dÂ¯ht is Gaussian with covariance Cov(Z (cid:102)W dÂ¯ht, Z (cid:102)W dÂ¯hs) = E Z dÂ¯htZ dÂ¯hs for any s. Thus,
Z dxt and Z dxs are correlated through Z (cid:102)W dÂ¯ht, Z (cid:102)W dÂ¯hs (but also through Z xr , r â‰¤ min(t, s)). Again,
the Gaussianity of ZW xt and Z (cid:102)W dÂ¯ht depend crucially on the fact that we use (cid:102)W instead of W (cid:62) in
backpropagation.

Other parts of the forward and backward propagations are similar to before. Our reasoning can be
formalized via Tensor Programs to prove the following
Theorem 6.3. Consider a 2-hidden-layer MLP in ÂµP with partially decoupled backpropagation as in
Eq. (20) and any training routine with learning rate 1. Suppose Ï†(cid:48) is pseudo-Lipschitz.34 As n â†’ âˆ,
for every input Î¾,

ft(Î¾) a.s.âˆ’âˆ’â†’ Ëšft(Î¾), where Ëšft(Î¾) is deï¬ned as follows:

(forward pass)

Ëšft(Î¾) def= E Z nVtZ Â¯xt(Î¾), Z Â¯xt(Î¾) def= Ï†(Z

Â¯ht(Î¾)), Z xt(Î¾) def= Ï†(Z ht(Î¾)), Z ht(Î¾) def= Î¾Z Ut

Z

Â¯ht(Î¾) def= ZW xt(Î¾) âˆ’

tâˆ’1
(cid:88)

ËšÏ‡sZ dÂ¯hs E Z xs Z xt(Î¾)

s=0
{ZW xt(Î¾)}Î¾,t centered, jointly Gaussian with Cov(ZW xt(Î¾), ZW xs(Î¶)) = E Z xt(Î¾)Z xs(Î¶)

(backward pass)

Ï‡t

def= L(cid:48)(Ëšft, yt), Z dÂ¯xt def= Z nVt, Z dÂ¯ht def= Ï†(cid:48)(Z

Â¯ht)Z dÂ¯xt Z dht def= Ï†(cid:48)(Z ht)Z dxt

Z dxt def= Z (cid:102)W dÂ¯ht âˆ’

tâˆ’1
(cid:88)

s=0

ËšÏ‡sZ xs E Z dÂ¯hsZ dÂ¯ht

(25)

(26)

{Z (cid:102)W dÂ¯ht}t centered, jointly Gaussian with Cov(Z (cid:102)W dÂ¯ht, Z (cid:102)W dÂ¯hs ) = E Z dÂ¯htZ dÂ¯hs

(U, V updates)

Z nVt+1 def= Z nVt âˆ’ ËšÏ‡tZ Â¯xt Z Ut+1 def= Z Ut âˆ’ ËšÏ‡tÎ¾tZ dht
with Z U0 and Z nV0 being independent standard Gaussians as initial conditions, and by deï¬nition,
{ZW xt(Î¾)}Î¾,t, {Z (cid:102)W dÂ¯ht}t, Z U0, and Z nV0 are mutually independent sets of random variables. Here,
if ht appears without argument, it means ht(Î¾t); likewise for Â¯ht, xt, Â¯xt, dht, dÂ¯ht, dxt, dÂ¯xt, Ëšft.

6.3

2-Hidden-Layer MLP: Normal SGD

Finally, we dicuss normal SGD for 2-hidden-layer MLP, i.e. in backprop we compute

dxt = W (cid:62)

t dÂ¯ht = (W (cid:62) + âˆ†W (cid:62))dÂ¯ht.

The ï¬rst forward and backward passes are essentially the same as in the last section. However, as
mentioned there, in the second forward pass, W x1 (a part of Â¯h1 = W x1 + âˆ†W1x1) will no longer be
approximately Gaussian because of the correlation between x1 and W . Letâ€™s ï¬rst get some intuition
for why this is before stating the inï¬nite-width limit formally.

Warmup: Ï† = id First, as warmup, suppose Ï† = id. In this case, W x1 will actually still be
Gaussian, but its variance will be different than whatâ€™s predicted in the previous section. To lighten
notation, we write x = x1 in this section. Then unwinding the deï¬nition of x, we have

where we abbreviated h = Î¾1U0, z = dÂ¯h0, a = âˆ’Ï‡0Î¾0Î¾1. Then W x has coordinates

x = h + aW (cid:62)z

(W x)Î± = (W h)Î± + a(W W (cid:62)z)Î±.
As derived in the ï¬rst forward pass in Section 6.2, (W h)Î± is approximately Gaussian (particularly
because W, U0 are independent). This is true for (W W (cid:62)z)Î± as well here because we assumed
Ï† = id, but not true generally. Indeed,
(W W (cid:62)z)Î± =

WÎ±Î²WÎ³Î²zÎ³ = zÎ±

WÎ±Î²WÎ³Î²zÎ³.

(WÎ±Î²)2 +

(cid:88)

(cid:88)

(cid:88)

(cid:88)

34This roughly means that Ï†(cid:48) has a polynomially bounded weak derivative; see Deï¬nition F.3.

Î²,Î³

Î²

Î²

Î³(cid:54)=Î±

19

We will soon see the derivations of Section 6.2 correspond to ignoring the ï¬rst term: In the second
term, there are n summands of the form (cid:80)
Î³(cid:54)=Î± WÎ±Î²WÎ³Î²zÎ³ that are approximately iid with vari-
ance â‰ˆ (cid:107)z(cid:107)2/n2. Thus, the second term itself, by a Central Limit heuristic, should converge to
N (0, limnâ†’âˆ (cid:107)z(cid:107)2/n). On the other hand, the ï¬rst term zÎ±
Î²(WÎ±Î²)2 â†’ zÎ± by Law of Large
Numbers. Tying it all together, (W x)Î± is a linear combination of two Gaussian terms (W h)Î± and
(cid:80)
Î³(cid:54)=Î± WÎ±Î²WÎ³Î²zÎ³, as well as as zÎ± (which is Gaussian in the case of Ï† = id, but not generally).
Î²

(cid:80)

(cid:80)

Note that, if we did (W (cid:102)W z)Î± instead of (W W (cid:62)z)Î±, as in the last section, then the same analysis
would show the ï¬rst term is zÎ±
Î² WÎ±Î² (cid:102)WÎ²Î± â†’ 0, while the second term converge in distribution to
the same Gaussian. Thus, the effect of decoupling in Section 6.2 is killing the copy of z in (W x)Î±.
We can summarize our derivation here in terms of Z:

(cid:80)

For Ï† = id: ZW x def= ZW h + aZW W (cid:62)z = ZW h + a( Ë†ZW W (cid:62)z + Z z),

(27)

where

Ë†ZW W (cid:62)z def= N (0, E(Z z)2).

Note the Central Limit heuristic in the derivation of Ë†ZW W (cid:62)z also shows Ë†ZW W (cid:62)z is jointly Gaussian
with ZW h with Cov( Ë†ZW W (cid:62)z, ZW h) = E ZW (cid:62)zZ h. So, to put Eq. (27) in a form more suggestive
of the general case, we will write

ZW x = Ë†ZW x + aZ z, where

Ë†ZW x = ZW h + a Ë†ZW W (cid:62)z d= N (0, E(Z x)2).

(28)

General Ï† Unwinding the deï¬nition of x, we have

By Taylor-expanding Ï†, we can apply a similar (though more tedious) argument as above to derive

x = Ï†(h + aW (cid:62)z (cid:12) Ï†(cid:48)(h0)).

(29)

ZW x = Ë†ZW x + cZ z

(30)

where c = a E Ï†(cid:48)(Z h1)Ï†(cid:48)(Z h0) and Ë†ZW x d= N (0, E(Z x)2). In the case of Ï† = id, c reduces to a
as above, recovering Eq. (28). For general Ï†, we can immediately see that ZW x is not Gaussian
because Z z = Z dÂ¯x0Ï†(cid:48)(Z Â¯h0) is not. In the Tensor Programs framework formalized in Section 7, cZ z
is denoted Ë™ZW x.
Similarly, coordinates distribution of dx1 = W (cid:62)

1 dÂ¯h1 will also change in the backward pass.

General t For general t, we obtain dynamical equations in Z identical to those in Theorem 6.3
except that Eq. (25) and Eq. (26) need to be modiï¬ed. We state the general result below.
Theorem 6.4. Consider a 2-hidden-layer MLP in ÂµP and any training routine with learning rate
1. Suppose Ï†(cid:48) is pseudo-Lipschitz.35 As n â†’ âˆ, for every input Î¾, ft(Î¾) a.s.âˆ’âˆ’â†’ Ëšft(Î¾) where Ëšft(Î¾) is
deï¬ned the same way as in Theorem 6.3 except that Eq. (25) should be replaced with

Z

Â¯ht(Î¾) def= Ë†ZW xt(Î¾) + Ë™ZW xt(Î¾) âˆ’

tâˆ’1
(cid:88)

s=0

ËšÏ‡sZ dÂ¯hs E Z xsZ xt(Î¾)

{ Ë†ZW xt(Î¾)}Î¾,t centered, jointly Gaussian with Cov( Ë†ZW xt(Î¾), Ë†ZW xs(Î¶)) = E Z xt(Î¾)Z xs(Î¶)

and Eq. (26) should be replaced with

Z dxt def= Ë†ZW (cid:62)dÂ¯ht + Ë™ZW (cid:62)dÂ¯ht âˆ’

tâˆ’1
(cid:88)

s=0

ËšÏ‡sZ xs E Z dÂ¯hsZ dÂ¯ht

{ Ë†ZW (cid:62)dÂ¯ht}t centered, jointly Gaussian with Cov( Ë†ZW (cid:62)dÂ¯ht, Ë†ZW (cid:62)dÂ¯hs ) = E Z dÂ¯htZ dÂ¯hs.

Like in Theorem 6.3, by deï¬nition, { Ë†ZW xt(Î¾)}Î¾,t, { Ë†ZW (cid:62)dÂ¯ht}t, Z U0, and Z nV0 are mutually indepen-
dent sets of random variables.

35This roughly means that Ï†(cid:48) has a polynomially bounded weak derivative; see Deï¬nition F.3.

20

Here,

Ë™ZW xt(Î¾) def= (cid:80)tâˆ’1

r=0 Î¸rZ dÂ¯hr where Î¸r is calculated like so: Z xt(Î¾) by deï¬nition is constructed as

for some function36 Î¦ : Rt+1 â†’ R. Then

Z xt(Î¾) = Î¦( Ë†ZW (cid:62)dÂ¯h0 , . . . , Ë†ZW (cid:62)dÂ¯htâˆ’1, Z U0)

Î¸r

def= E âˆ‚Î¦( Ë†ZW (cid:62)dÂ¯h0, . . . , Ë†ZW (cid:62)dÂ¯htâˆ’1 , Z U0 )/âˆ‚ Ë†ZW (cid:62)dÂ¯hr .

Likewise,
structed as

Ë™ZW (cid:62)dÂ¯ht def= (cid:80)tâˆ’1

r=0 Î¸rZ xr where Î¸r is calculated as follows: Z dÂ¯ht by deï¬nition is con-

Z dÂ¯ht = Î¨( Ë†ZW x0 , . . . , Ë†ZW xtâˆ’1, Z V0)

for some function36 Î¨ : Rt+1 â†’ R. Then

Î¸r

def= E âˆ‚Î¨( Ë†ZW x0 , . . . , Ë†ZW xtâˆ’1 , Z V0)/âˆ‚ Ë†ZW xr .

For example, generalizing Eq. (29), for any input Î¾, we have

Z x1(Î¾) = Î¦(ZW (cid:62)dÂ¯h0, Z U0), where Î¦(z, u) def= Ï†(Î¾u âˆ’ ËšÏ‡0Î¾0Î¾Ï†(cid:48)(Î¾0u)z).
Then Î¸0 = E âˆ‚zÎ¦(ZW (cid:62)dÂ¯h0, Z U0) = âˆ’ËšÏ‡0Î¾0Î¾ E Ï†(cid:48)(Z h1(Î¾))Ï†(cid:48)(Z h0 ), which specializes to c in
Eq. (30). Altogether,
Note that Ë†ZW xt here does not equal ZW xt
Cov( Ë†ZW xt, Ë†ZW xs ) = E Z xtZ xs is affected by the presence of Ë™ZW xr for all r â‰¤ max(s, t).

Ë™ZW x1(Î¾) = âˆ’ËšÏ‡0Î¾0Î¾Z dÂ¯h0 E Ï†(cid:48)(Z h1(Î¾))Ï†(cid:48)(Z h0).

in Eq. (25) in general, because the covariance

6.4 MLP of Arbitrary Depth

The ÂµP limit of deeper MLPs can be derived along similar logic; see Appendices H.3 to H.5 for
a rigorous treatment within the Tensor Programs framework, which also covers all stable abc-
parametrizations.

What happens in other feature learning parametrizations
If we are in the feature learning
regime, then any W l that is not maximally updated (Deï¬nition 5.2) will be effectively ï¬xed (to its
initialized value) in the inï¬nite-width limit (i.e. no learning occurs).

6.5 Summary of Main Intuitions for Deriving the ÂµP Limit
Law of Large Numbers Any vector z has roughly iid coordinates given by Z z. For any two vectors
.

(cid:80)n

Î± â†’ E Z zZ z(cid:48)

z, z(cid:48) âˆˆ Rn, 1
n

Î±=1 zÎ±z(cid:48)

1. This is all we needed to derive the 1-hidden-layer dynamics of Section 6.1, since all

the matrices there are size-n vectors.

2. In Sections 6.2 and 6.3, this is also used in calculating the limit of âˆ†Wtxt.
Central Limit If the underlying computation graph never involves the transpose W (cid:62) of a n Ã— n
Gaussian matrix W in a matrix multiplication, then W z is roughly iid Gaussian with
coordinate ZW z d= N (0, E(Z z)2) (if WÎ±Î² âˆ¼ N (0, 1/n))

1. This along with the last intuition are all we used to derive the 2-hidden-layer decoupled

dynamics of Section 6.2, where W is the middle layer weight matrix.
(W , W (cid:62)) Correlation If W (cid:62) is involved, then W z has coordinates distributed like random variable
Ë†ZW z + Ë™ZW z where Ë†ZW z is the Gaussian obtained by pretending W is independent from
W (cid:62), and Ë™ZW z results from the correlation between W and W (cid:62).
Ë™ZW z is purely a linear
combination of Z z(cid:48)

for previously deï¬ned vectors z(cid:48) such that z depends on W (cid:62)z(cid:48).

1. All three intuitions above are needed to derive the 2-hidden-layer dynamics of normal

SGD (Section 6.3), where W (cid:62) is used in backpropagation.

2. The calculation of Ë™ZW x is quite intricate, which is why we ï¬rst discussed decoupled
SGD in Section 6.2, which doesnâ€™t need Ë™ZW x calculation, before discussing normal
SGD in Section 6.3.

36that may depend on various scalars such as ËšÏ‡s, E Z xs Z xs(cid:48) (Î¾), and E Z dÂ¯hs Z dÂ¯hs(cid:48)

21

Figure 3: Graphical overview of the Tensor Programs framework. For the Master Theorem, we
illustrate Theorem 7.4(2) since Theorem 7.4(1) is a corollary of Theorem 7.4(2) for a larger program.

7 Tensor Programs Framework

While the previous section demonstrates the intuition of how to derive the ÂµP limit, it also lays
bare 1) the increasing complexity of a manual derivation as the training goes on, as well as 2) the
mounting uncertainty for whether the intuition still holds after many steps of SGD. This is a perfect
call for the Tensor Programs framework, which automates (and makes rigorous) the limit derivation
for any â€œcomputation graphâ€ â€” including the computation graph underlying SGD. Here we review
this framework (developed in Yang [49, 50, 51, 52]) in the context of ÂµP limit. Fig. 3 graphically
overviews the content of this section.

As seen abundantly in Section 6, the computation underlying SGD can be expressed purely via three
instructions: matrix multiplication (by a Gaussian matrix, e.g. W0x0), coordinatewise nonlinearities
(e.g. Ï†), and taking coordinatewise average (e.g. 1
Î±=1(nV1)Î±x1Î±). In deriving the ÂµP SGD limit,
n
we focused mostly on keeping track of Rn vectors (e.g. Â¯xt or dht), but importantly we also computed
scalars ft and Ï‡t by (what amounts to) taking coordinatewise average (e.g. f1 = 1
Î±=1(nV1)Î±x1Î±).
n
We implicitly compute scalars as well inside âˆ†Wtxt. This motivates the following notion of a
program, which can be thought of as a low-level symbolic representation of a computation graph
common in deep learning (e.g. underlying Tensorflow and Pytorch).

(cid:80)n

(cid:80)n

22

ğ‘.ğ‘ .ğ‘¥ğ‘ŠMatMulğ‘ğ‘Šğ‘¥=áˆ¶ğ‘ğ‘Šğ‘¥+áˆ˜ğ‘ğ‘Šğ‘¥ğ‘Šiidğ’©0,ğœğ‘Š2/ğ‘›entriesğ’²=Setupğ‘£1ğ‘£2ğ‘£3ğ‘£ğ‘—ğ‘ğ’±ğ‘ğ‘£1ğ‘ğ‘£2ğ‘ğ‘£3ğ‘ğ‘£ğ‘—()=ğ’±=ğ’©0,ğœğ‘Š2ğ”¼ğ‘ğ‘¥2Correction due to (ğ‘Š,ğ‘¥)correlationğ‘¥1ğ‘¥2ğ‘¥3ğ‘¥ğ‘˜ğœ™(((((())))))ğœ™ğœ™ğœ™ğœ™ğœ™Nonlinğ‘ğ‘¥1ğ‘ğ‘¥2ğ‘ğ‘¥3ğ‘ğ‘¥ğ‘˜()ğœ™ğ‘ğœ™(ğ‘¥1,â€¦,ğ‘¥ğ‘˜)=;;;;;;;áˆğœƒ1áˆğœƒ2áˆğœƒâ„“ğœƒ1ğœƒ2ğœƒâ„“Momentáˆğœ—=ğ‘¥1ğ‘¥2ğ‘¥3ğ‘¥ğ‘˜ğœ™(((((())))))ğœ™ğœ™ğœ™ğœ™ğœ™ğ‘ğ‘¥1ğ‘ğ‘¥2ğ‘ğ‘¥3ğ‘ğ‘¥ğ‘˜()ğœ™;;;;;;;áˆğœƒ1áˆğœƒ2áˆğœƒâ„“ğœƒ1ğœƒ2ğœƒâ„“ğ”¼Average1ğ‘›à·1ğ‘›ğœ—ğ’=áˆğœ—as ğ‘›â†’âˆMaster Theoremğ‘›â†’âˆğ‘›â†’âˆDeï¬nition 7.1. A Tensor Program37 is a sequence of Rn-vectors and R-scalars inductively generated
via one of the following ways from an initial set C of random scalars, V of random Rn vectors, and a
set W of random RnÃ—n matrices (which will be sampled with iid Gaussian entries in Setup 7.2)

MatMul Given W âˆˆ RnÃ—n and x âˆˆ Rn, we can generate W x âˆˆ Rn or W (cid:62)x âˆˆ Rn

Nonlin Given Ï† : Rk Ã— Rl â†’ R, previous scalars Î¸1, . . . , Î¸l âˆˆ R and vectors x1, . . . , xk âˆˆ Rn, we

can generate a new vector

Ï†(x1, . . . , xk; Î¸1, . . . , Î¸l) âˆˆ Rn

where Ï†(âˆ’; Î¸1, . . . , Î¸l) applies coordinatewise to each â€œÎ±-sliceâ€ (x1

Î±, . . . , xk

Î±).

Moment Given same setup as above, we can also generate a new scalar

1
n

n
(cid:88)

Î±=1

Ï†(x1

Î±, . . . , xk

Î±; Î¸1, . . . , Î¸l) âˆˆ R.

Explanation of Deï¬nition 7.1 The vectors mentioned in Deï¬nition 7.1 are exempliï¬ed by
ht, xt, dht, dxt in Section 6. The scalars mentioned are exempliï¬ed by ft, Ï‡t as well as e.g. x(cid:62)
s xt/n
inside the calculating of ht (Eq. (24)). The Î¸is in Nonlin and Moment rules may appear cryptic at ï¬rst.
These scalars are not needed in the ï¬rst forward and backward passes. But in the second forward pass,
for example for the 1-hidden-layer MLP (Section 6.1), x1 = Ï†(h1) = Ï†(Î¾1U0 âˆ’ Ï‡0Î¾1Î¾0nV0Ï†(cid:48)(h0))
depends on the scalar Ï‡0, Î¾0, Î¾1, and can be written in the form of Nonlin as Â¯Ï†(U0, nV0, h0; Ï‡0) for
some Â¯Ï† appropriately deï¬ned.
The initial set of scalars C is the training sequence {Î¾t, yt}t for all three examples of Section 6. In
our 2-hidden-layer MLP examples, the initial set of matrices W is {W } (Section 6.3) or {W, (cid:102)W }
(Section 6.2), i.e. the random RnÃ—n Gaussian matrices. On the other hand, in the 1-hidden-layer
MLP example (Section 6.1), W is empty. The initial set of vectors V in all three examples are
V = {U0, nV0}.3839 Notice how the vectors of these V are sampled with iid standard Gaussian
coordinates. We formalize a more general setup for arbitrary Tensor Programs:
Setup 7.2. 1) For each initial W âˆˆ W, we sample iid WÎ±Î² âˆ¼ N (0, Ïƒ2
W /n) for some variance
W associated to W , independent of other W (cid:48) âˆˆ W; 2) for some multivariate Gaussian Z V =
Ïƒ2
(cid:8)Z h : h âˆˆ V(cid:9) âˆˆ RV , we sample the initial set of vectors V like {hÎ± : h âˆˆ V} âˆ¼ Z V iid for each
Î± âˆˆ [n]. 3) For each initial scalar Î¸ âˆˆ C, we require Î¸ a.s.âˆ’âˆ’â†’ ËšÎ¸ for some deterministic ËšÎ¸ âˆˆ R.

In all of our examples, we took Ïƒ2
W = 1 for simplicity, but Setup 7.2 allows for other initializations
(e.g. a typical initialization for relu networks is Ïƒ2
W = 2); additionally, Z h, h âˆˆ V, are all standard
Gaussians, independent from one another, since U0, nV0 are sampled this way; and our initial scalars
{Î¾t, yt}t are ï¬xed with n, so they are their own limits.40

What Does a Tensor Program Vector Look Like? Recall that we represented the coordinate
distribution of each vector h with a random variable Z h in Section 6 and kept track of how different
Zs are correlated with each other. We also calculated scalar limits like ft â†’ Ëšft, Ï‡t â†’ ËšÏ‡t. These
calculations led to a set of formulas for the ÂµP limit (e.g. Theorems 6.1, 6.3 and 6.4). We can also
construct such Z h and ËšÎ¸ for vectors h and scalars Î¸ in any Tensor Program. They intuitively capture
the coordinate distribution of vector h and the deterministic limit of Î¸. The following deï¬nition
formally deï¬nes Z h and ËšÎ¸, but the connection between Z h (resp. ËšÎ¸) and the coordinates of h (resp.
Î¸) is not made rigorously until Theorem 7.4 later. The ZMatMul rule below perhaps asks for some
discussion, and we shall do so after the deï¬nition.

37What we refer to as Tensor Program is the same as NETSOR(cid:62)+ in Yang [52]; we will not talk about other

languages (like NETSOR(cid:62)) so this should not cause any confusion

38Here we write nV0 instead of V0 because we want all vectors to have Î˜(1) coordinates; see Setup 7.2.
39In Section 6 we assumed input dimension is 1. In general, each column of U0 would be a separate initial
vector. Likewise, if the output dimension is greater than 1, then each row of V0 would be a separate initial vector.
40Since {Î¾t, yt}t are ï¬xed with n, we can WLOG absorb them into any nonlinearities in Nonlin that they are
involved in, and set C = âˆ…. But, in kernel regime or nonmaximal feature learning parametrization, we usually
have initial scalars, such as nâˆ’2aL+1âˆ’c, that tend to 0 with n; see Appendix H.4.

23

Deï¬nition 7.3 (Z h and ËšÎ¸). Given a Tensor Program, we recursively deï¬ne Z h for each vector h and
ËšÎ¸ for each scalar Î¸ as follows.

ZInit If h âˆˆ V, then Z h is deï¬ned as in Setup 7.2. We also set Ë†Z h def= Z h and Ë™Z h def= 0.
ZNonlin+ Given Ï† : Rk Ã— Rl â†’ R, previous scalars Î¸1, . . . , Î¸l âˆˆ R and vectors x1, . . . , xk âˆˆ Rn,

we have

Z Ï†(x1,...,xk;Î¸1,...,Î¸l) def= Ï†(Z x1

, . . . , Z xk

; ËšÎ¸1, . . . , ËšÎ¸l).

ZMoment Given same setup as above and scalar Î¸ = 1
n

(cid:80)n

Î±=1 Ï†(x1

Î±, . . . , xk

Î±; Î¸1, . . . , Î¸l), then

ËšÎ¸ def= E Ï†(Z x1

, . . . , Z xk

; ËšÎ¸1, . . . , ËšÎ¸l).

Here ËšÎ¸1, . . . , ËšÎ¸l are deterministic, so the expectation is taken over Z x1

, . . . , Z xk

.

ZMatMul ZW x def= Ë†ZW x + Ë™ZW x for every matrix W (with N (0, Ïƒ2

W /n) entries) and vector x, where
ZHat Ë†ZW x is a Gaussian variable with zero mean. Let VW denote the set of all vectors in
the program of the form W y for some y. Then { Ë†ZW y : W y âˆˆ VW } is deï¬ned to be
jointly Gaussian with zero mean and covariance
(cid:16) Ë†ZW x, Ë†ZW y(cid:17) def= Ïƒ2

for any W x, W y âˆˆ VW .

E Z xZ y,

Cov

W

Furthermore, { Ë†ZW y : W y âˆˆ VW } is mutually independent from { Ë†Z v : v âˆˆ V âˆª
(cid:83)

Â¯W (cid:54)=W V Â¯W }, where Â¯W ranges over W âˆª {A(cid:62) : A âˆˆ W}.

ZDot We can always unwind Z x = Î¦(Â· Â· Â· ),
i=1; {ËšÎ¸i}l
}j

for some arguments (Â· Â· Â· ) =
({ Ë†ZW (cid:62)yi
i=1), zi (cid:54)âˆˆ VW (cid:62) (where VW (cid:62) is deï¬ned in ZHat), and
deterministic function Î¦ : Rk+j+l â†’ R. Deï¬ne âˆ‚Z x/âˆ‚ Ë†ZW (cid:62)yi def= âˆ‚iÎ¦(Â· Â· Â· ). Then we
set

i=1, { Ë†Z zi
}k

Ë™ZW x def= Ïƒ2
W

k
(cid:88)

i=1

Z yi E âˆ‚Z x
âˆ‚ Ë†ZW (cid:62)yi

,

(31)

There is some nuance in this deï¬nition, so see Remark F.1 and F.2.

Explanation of Deï¬nition 7.3 Nonlin and Moment should appear only natural. However, we
pause to digest the meaning of ZMatMul by relating back to our examples in Section 6. First notice
that Ë™ZW x = 0 if W (cid:62) is not used in the program, so that ZW x = Ë†ZW x. This is the case in Section 6.2,
where (cid:102)W is used in backprop instead of W (cid:62). There (in Eq. (25)), ZW xt is Gaussian with covariance
Cov(ZW xt, ZW xs ) = E Z xtZ xs for any s, consistent with ZHat. In Section 6.3, however, Ë™ZW x (cid:54)= 0
in general. The ZDot rule is a direct generalization of the calculation of Ë™Z in Theorem 6.4.

Ë™ZW xt and Ë™ZW (cid:62)dÂ¯ht of Section 6.3 for general t will all be nonzero but have no easy expression.
Here we seek to convey the complexity of computing them; this is optional reading for the ï¬rst time
reader. To calculate Ë™ZW xt ( Ë™ZW (cid:62)dÂ¯ht is similar), we need to express Z xt as a function of purely
Ë†ZW (cid:62)dÂ¯hs, s < t, and Z U0 = Ë†Z U0 . Then we symbolically differentiate Z xt by Ë†ZW (cid:62)dÂ¯hs and take
expectation to obtain the coefï¬cient of Z dÂ¯hs in Ë™ZW xt. For t = 1 as in the examples in Section 6.3,
this task is easy because Ë†ZW (cid:62)dÂ¯h0 = Ë†Z dx0 = Z dx0. But in general, the calculation can balloon
quickly. Indeed, note Z xt = Ï†(Z ht) and

Z ht = Î¾tZ Ut = Î¾tZ U0 âˆ’ Î¾t

tâˆ’1
(cid:88)

s=0

ËšÏ‡sÎ¾sZ dhs = Î¾tZ U0 âˆ’ Î¾t

tâˆ’1
(cid:88)

s=0

ËšÏ‡sÎ¾sÏ†(cid:48)(Z hs)Z dxs .

However, each Z dxs is a linear combination of ZW (cid:62)dÂ¯hs = Ë†ZW (cid:62)dÂ¯hs + Ë™ZW (cid:62)dÂ¯hs and Z xr , r < s
t dÂ¯hs). Each of Ë™ZW (cid:62)dÂ¯hs and Z xr then needs to be recursively expanded in terms
(coming from âˆ†W (cid:62)
of Ë†Z before we can calculate the symbolic partial derivative âˆ‚Z xt/âˆ‚ Ë†ZW (cid:62)dÂ¯hs .

24

Algorithm 1 Compute the inï¬nite-width limit of an NN in any abc-parametrization and any task

1: Write the computation graph underlying training and inference in a Tensor Program (akin to

writing low level PyTorch or Tensorï¬‚ow code).

2: Calculate Z h for each vector h and ËšÎ¸ for each scalar Î¸ in the program, according to Deï¬nition 7.3.
3: The logits ft(Î¾) of the neural network at any time t should be written as a collection of scalars,
so Ëšft(Î¾) is calculated in the previous step. For t being inference time, Ëšft(Î¾) is the output of the
inï¬nite-width network after training.

Master Theorem Finally, we relate the symbolic nature of a Tensor Program given in Deï¬nition 7.3
to the analytic limit of its computation, in the following Master Theorem. Pseudo-Lipschitz functions
are, roughly speaking, functions whose (weak) derivatives are polynomially bounded. We state the
theorem assuming mild regularity conditions (Assumption F.4) that roughly says most nonlinearities
in the program should be pseudo-Lipschitz.

Theorem 7.4 (Tensor Program Master Theorem, c.f. Theorem E.15 of [52]). Fix a Tensor Program
initialized accordingly to Setup 7.2. Adopt Assumption F.4. Then

1. For any ï¬xed k and any pseudo-Lipschitz Ïˆ : Rk â†’ R, as n â†’ âˆ,

1
n

n
(cid:88)

Î±=1

Ïˆ(h1

Î±, . . . , hk

Î±) a.s.âˆ’âˆ’â†’ E Ïˆ(Z h1

, . . . , Z hk

),

(32)

for any vectors h1, . . . , hk in the program, where Z hi

are as deï¬ned in Deï¬nition 7.3.

2. Any scalar Î¸ in the program tends to ËšÎ¸ almost surely, where ËšÎ¸ is as deï¬ned in Deï¬nition 7.3.

, . . . , Z hk

Î±, . . . , hk

Intuitively, Theorem 7.4(1) says that each â€œcoordinate sliceâ€ (h1
Î±) can be thought of as an iid
copy of (Z h1
).41 This intuition is consistent with our heuristic derivation in Section 6, and
Theorem 7.4 underlies the proof of Theorems 6.1, 6.3 and 6.4. Theorem 7.4(2) allows us to directly
obtain the function learned at the end of training: For example, for a 1-hidden-layer MLP, it shows
that the networkâ€™s output on any input Î¾ at time t converges to Ëšft(Î¾) given in Theorem 6.1.
Algorithm 1 summarizes how to compute the inï¬nite-width limit of any network in any abc-
parametrization and for any task, using the Tensor Programs framework laid out in this section.
It generalizes the manual derivations of Section 6. We carry out Algorithm 1 for MLPs in all of our
experiments.

Architectural and algorithmic universality Given that Tensor Programs can express the ï¬rst
forward and backward computation of practically any architecture [49, 51], it should perhaps come
as no surprise that they can also express practically any training and inference procedure â€” or just
any computation â€” involving any such architecture. This includes both feature learning and kernel
limits. We leverage this ï¬‚exibility to derive and compute the ÂµP and kernel limits for metalearning
and Word2Vec; see Section 9.

Extensions We focused on programs whose vectors all have the same dimension n here. But itâ€™s
easy to generalize to the case where vectors have different dimensions, which corresponds to e.g.
when a networkâ€™s widths are non-uniform. See [52].

8 Computational Considerations

While the TP framework is very general, computing the feature learning limits analytically is
inherently computationally intensive aside from special cases like the linear 1-hidden-layer MLP
(Corollary 6.2). Here we explain why, so as to motivate our experimental choices below.

41This implies an explicit convergence in distribution (see [52]), but this convergence in distribution is strictly

weaker than the formulation in Theorem 7.4, which is in general much more useful.

25

No closed-form formula for evaluating the expectations (e.g. in Eq. (32)) involving general
nonlinearities except in special cases For example, for a 1-hidden-layer MLP (Section 6.1), after
1 step of SGD, the logit is of the form E(Z1 + bÏ†(Z2))Ï†(Z3 + cZ1Ï†(cid:48)(Z2)) where Zis denote different
(correlated) Gaussians (Eq. (17)). While one can still evaluate this via Monte-Carlo, the error will
compound quickly with training time. On the other hand, because of the nesting of Ï†(cid:48) inside Ï†, there
is no closed-form formula for this expectation in general.

Notable Exception: If the nonlinearity Ï† is polynomial, then the expectation is a polynomial moment
of a multivariate Gaussian and can be evaluated analytically, e.g. using Isserlisâ€™ theorem from the
covariance matrix.

Even with nonlinear polynomial Ï†, there is exponential computational bottleneck As training
time t increases, due to the nesting of Ï† and Ï†(cid:48) in the preactivations, the integrand of the expectation,
e.g. E Z Â¯xtZ nVt, will turn out to be a polynomial in â„¦(1) Gaussian variables with degree â„¦(2t). The
covariance matrix of the Gaussian variables will in general be nontrivial, so evaluating the expectation,
e.g. using Isserlisâ€™ theorem, requires super-exponential time. This is because we would need to
expand the polynomial integrand into monomials, and there would be â„¦(2t) monomials, each of
which require â„¦(2t) time to evaluate using Isserlisâ€™ theorem.

0xlâˆ’1

n Ã— n Gaussian matrices Both points above apply to 1-hidden-layer MLPs. Additional difï¬culties
with deeper networks is caused by the n Ã— n initial Gaussian matrix W l
0, 2 â‰¤ l â‰¤ L, in the middle of
the network. 1) In general, due to the nonlinearities, xlâˆ’1
t would be linearly independent from xlâˆ’1
0xlâˆ’1
t xlâˆ’1
t + âˆ†W l
for all s < t. Therefore, in calculating W l
, we create a new Gaussian
t
0xlâˆ’1
variable Ë†ZW l
, s < t. This then requires us
to compute and store the covariance between them. Thus, t steps of SGD costs â„¦(t2) space and
time (not mentioning that the computation of each covariance entry can require exponential time,
as discussed above). 2) In addition, due to the interaction between W l
t in the forward pass and
in the backward pass, there is nonzero Ë™Z, as demonstrated in Eq. (30). This Ë™Z is generally a
W l(cid:62)
t
linear combination of â„¦(t) terms, and the coefï¬cients of this combination require evaluation of some
expectations that typically run into the exponential bottleneck discussed above.

linearly independent from all previous Ë†ZW l

t = W l

t xlâˆ’1

s

s

t

Summary From easiest to hardest in terms of ÂµP limitâ€™s computational cost, we have 1) 1-hidden-
layer linear networks; 2) L-hidden-layer linear MLP, L â‰¥ 2; 3) nonlinear MLP with polynomial
activations; 4) nonlinear MLP with nonpolynomial activations. Nevertheless, 1-hidden-layer linear
networks are more than sufï¬cient to demonstrate feature learning in Word2Vec and few-shot learning
with MAML, as we show below.

9 Experiments

In light of the computational difï¬culties discussed above, we divide our experiments into two
groups: 1) Verifying our theory; 2) Scaling up to realistic datasets to demonstrate feature learning.
The experiments in group 1 focus on stress-testing our theory in many scenarios to show that it
describes empirical phenomena accurately. They will run into the discussed computational difï¬culties
(Section 8), so we cannot train the inï¬nite-width ÂµP networks for very long, but nevertheless long
enough to verify the theory. Those in group 2 focus on real datasets (metalearning and Word2Vec)
where feature learning is critical, and demonstrate that the GP and NTK limits are inadequate for
those tasks. Necessarily, we adopt simpler neural architectures for this purpose so we can scale up.

9.1 Verifying the Theory

In Fig. 4, we analytically computed the ÂµP limits derived in Section 6 for quadratic and linear
activations, and veriï¬ed them against ï¬nite width networks.

9.2 Few-Shot Learning on Omniglot via First Order MAML

In few-shot learning, the model is given only a small number of labeled examples before asking to
make predictions on unseen data. Therefore, this tests whether a model contains a good prior that
can adapt quickly to the small amount of data at hand.

26

Figure 4: Empirical Simulation Agrees with Theory. We analytically compute the inï¬nite-width
ÂµP limit for the three kinds of networks (depth 1, depth 2 decoupled, depth 2) described in Section 6,
with either quadratic Ï†(x) = x2 or linear Ï†(x) = x activation. The training set is random Î¾t âˆˆ
{Â±1}, yt âˆˆ {Â±1}, so that the deviation of ï¬nite width from inï¬nite width losses are accentuated. We
compare against ï¬nite width ÂµP networks with width 1024 or 4096. For each width, we randomly
initialize with 100 different seeds and aggregate the loss curves. The mean across these seeds is
plotted as solid curves, and the standard deviation represented by the shade. As discussed in Section 8,
nonlinear activation functions and higher depth face computational difï¬culties exponential with
training time. Thus here we only train for a few steps. We observe that the quadratic network
converges slower to the limit with width. This is expected since the tail of Z xt is fatter for a quadratic
activation than a linear activation.

MAML In Model Agnostic Meta-Learning (MAML), the model performs few-shot learning by
one or more SGD steps on the given training data; this is called adaptation. In a pretraining (also
called meta-training) phase, MAML learns a good initialization of the model parameters for this
adaptation. The training objective is to minimize the loss on a random taskâ€™s test set after the model
has adapted to its training set. More precisely, the basic First Order MAML at training time goes as
follows: With fÎ¸ denoting the model with parameters Î¸, and with step sizes (cid:15), Î·, we do

1. At each time point, sample a few-shot task T

2. From T , sample a training set D

3. Adapt Î¸(cid:48) â† Î¸ âˆ’ (cid:15)âˆ‡Î¸LD(fÎ¸), where LD(fÎ¸) is the loss of fÎ¸ over D
4. Sample a test set D(cid:48) from T

5. Update Î¸ â† Î¸ âˆ’ Î·âˆ‡Î¸(cid:48)LD(cid:48)(fÎ¸(cid:48)), where LD(cid:48)(fÎ¸(cid:48)) is the loss of fÎ¸(cid:48) over D(cid:48)

6. Repeat

In practice, we batch the tasks, just like batches in SGD, so that we accumulate all the gradients from
Step 5 and update Î¸ only at the end of the batch.

During meta-test time, we are tested on random unseen few-shot tasks, where each task T provides a
training set D and a test set D(cid:48) as during meta-training. We adapt to D as in Step 3 above (or more
generally we can take multiple gradient steps to adapt better) to obtain adapted parameters Î¸(cid:48). Finally,
we calculate the accuracy of Î¸(cid:48) on the test set D. We average this accuracy over many tasks T , which
we report as the meta-test accuracy.

First Order vs Second Order MAML Notice in Step 5, we take the gradient of LD(cid:48)(fÎ¸(cid:48)) with
respect to the adapted parameters Î¸(cid:48). In Second Order MAML, we would instead take the gradient
against the unadapted parameters Î¸, which would involve the Hessian âˆ‡Î¸âˆ‡Î¸LD(fÎ¸). Second Order
MAML generally achieves performance slightly better than First Order MAML, but at the cost of
signiï¬cantly slower updates [37]. In order to scale up, we will focus on First Order MAML, hereafter
referred to as just MAML.

27

01230.51.01.5(x)=x2lossdepth 2width10244096inf01230.51.01.5depth 2, decoupled0123450.40.60.8depth 10123456789iter0.40.60.81.0(x)=xloss0123456789iter0.40.60.81.00369121518iter0.40.50.60.7Table 2: Omniglot Meta-Test Accuracies after Pretraining with First Order MAML.
Ï† = relu

Ï† = identity ; number = log2 width
13
9
5

11

7

ÂµP

GP/NTK

GP

NTK

1

3

66.42
47.60
Â±.02 Â±.04 Â±1.24 Â±0.70 Â±.15 Â±.16 Â±.23 Â±.22 Â±.18 Â±.19

55.34

64.54

66.43

66.31

47.82

66.36

66.21

66.41

41.68
Â±.09

Few-Shot Learning Terminologies An N -way classiï¬cation task asks the model to predict a class
from N possiblities. A K-shot classiï¬cation task provides K input/output pairs per class, for a total
of N K training points for N -way classiï¬cation.

Omniglot Omniglot is a standard few-shot learning benchmark. It consists of 20 instances of 1623
characters from 50 different alphabets, each handwritten by a different person. We test our models on
1-shot 5-way classiï¬cation: We draw 5 random characters, along with 1 training instance and 1 test
instance for each character. After the model adapts to the training instances, itâ€™s asked to predict the
character of the test instances (choosing among the 5 characters).

Models Our main model is the ÂµP limit of a 1-hidden-layer linear MLP. We compare against: 1)
ï¬nite width versions of the same;42 2) the NNGP and NTK limits of the same; 3) the NNGP and
NTK limits of a 1-hidden-layer relu MLP. Note 2) is equivalent to a 0-hidden-layer perceptron,
because the NNGP and NTK there are both linear kernels. In addition, the inï¬nite-width SP limit of a
1-hidden-layer network is the same as the NNGP limit. Both 2) and 3) are equivalent to linear models
with ï¬xed (not learned) features, so MAMLâ€™s adaptation only applies to the linear weights. On the
other hand, the ÂµP limit and the ï¬nite ÂµP networks will learn new representations of the data over
time that can quickly adapt to new tasks.43

Hyperparameters We use (task) batch size 32 and adaptation step size 0.4 ((cid:15) in Step 3). We also
clip the gradient in Step 5 if the gradient has norm â‰¥ 0.5.44 For each model, we tune its weight
initializaton variances and the meta learning rate (Î· in Step 5). During meta-test time, we take 20
gradient steps during adaptation (i.e. we loop Step 3 above 20 times to obtain Î¸(cid:48)). See Appendix D.1
for more details.

Findings Our results are summarized in the Figure to the
right and Table 2, where curves indicate means and shades
indicate standard deviations. There are three key takeaways:
1) The feature learning ÂµP limit signiï¬cantly outperforms the
kernel limits. 2) The beneï¬t of feature learning dominates
the beneï¬t of having nonlinearities. 3) As width increases,
the ï¬nite ÂµP networks approach the performance of the ÂµP
limit from below.

9.3 Word2Vec

Word2Vec [32, 33] is an early example of large-scale pretraining and transfer learning in natural
language processing, where one learns a feature vector h(Î¾) for every word Î¾ based on the principle
of distributional semantics. For simplicity, we focus on a speciï¬c scheme of Word2Vec using context
as a bag-of-word (CBOW), negative example sampling, and Sigmoid loss function.

Word2Vec Pretraining Consider training on a corpus with vocabulary V. At each time step, we
sample a sentence for the corpus and choose a word i âˆˆ V. This wordâ€™s context J âŠ† V is a window
of words around it in the sentence, thought of as a bag of words. Let Î¾i âˆˆ R|V| be the one-hot vector

42Because we will tune initialization variances, our results also represent ï¬nite-width SP networks.
43Note that the transfer learning comment in Section 3.1 does not apply directly to the few-shot setting here,
because the readout weights of the network carry over from the pretraining phase. Nevertheless, we will see a
large performance gap between the kernel limits (2,3) and the ÂµP limit.

44One can write down gradient clipping easily in a Tensor Program, so the its inï¬nite-width limit can be

computed straightforwardly via Theorem 7.4; see Appendix D.

28

135791113log2(width)0.450.500.550.600.65test set accOmniglot, 1-Shot 5-Waylin finitelin Plin ntk/gprelu gprelu ntkcorresponding to word i. We pass the averaged context Î¾J def= 1
|J|
MLP with hidden size n and identity activation:

(cid:80)n

jâˆˆJ Î¾j through a 1-hidden-layer

f (Î¾J ) = V h(Î¾J ) âˆˆ R|V|,

h(Î¾J ) = U Î¾J âˆˆ Rn,

(33)

where V âˆˆ R|V|Ã—n, U âˆˆ RnÃ—|V| factor as V = nâˆ’av v, U = nâˆ’au u with initialization vÎ± âˆ¼
N (0, nâˆ’2bv ), uÎ± âˆ¼ N (0, nâˆ’2bu ), where {av, bv, au, bu} specify the parametrization of the network.
After each forward pass, we sample a target word Ï„ from V: with probability p, we take Ï„ = i; with
probability 1 âˆ’ p, we sample Ï„ uniformly from V \ {i}. Following [32, 33], we take p = 1/21 â‰ˆ
4.76%. The loss is then calculated with the Sigmoid function Ïƒ(Â·) :

L(f (Î¾J ), Î¾Ï„ ) =

(cid:26)log(1 âˆ’ Ïƒ(f (Î¾J )(cid:62)Î¾Ï„ ))

log Ïƒ(f (Î¾J )(cid:62)Î¾Ï„ )

Ï„ = i
Ï„ (cid:54)= i

(34)

Then v and u are updated via SGD as usual (causing V and U to update). Conventionally, h(Î¾) âˆˆ Rn
is taken as the Word2Vec embedding for a word Î¾ after many iterations of forward-backward updates.

Word Analogy Evaluation We evaluate the word embeddings h(Î¾) with the word analogy task.
This task asks the question of the kind: What to a â€˜queenâ€™ is as a â€˜manâ€™ to a â€˜womanâ€™? (answer is
â€˜kingâ€™). The Word2Vec model answers this question by computing

argmax
i

h(Î¾i)(cid:62)(h(Î¾â€˜manâ€™) âˆ’ h(Î¾â€˜womanâ€™) + h(Î¾â€˜queenâ€™))

(35)

where i ranges over V \ {â€˜manâ€™, â€˜womanâ€™, â€˜queenâ€™}. If the argmax here is i = â€˜kingâ€™, then the model
answers correctly; otherwise, itâ€™s incorrect. The accuracy score is the percentage of such questions
answered correctly.

Dataset We train the models on text8,45 a clean dataset consisting of the ï¬rst 100 million charac-
ters of a 2006 Wikipedia dump. The dataset has been featured in the original Word2Vec codebase and
the Hutter Prize. text8 contains the ï¬rst 100 million characters of fil9, a larger dataset obtained by
ï¬ltering the ï¬rst 1 billion characters in the aforementioned Wikipedia dump. We space-separate the
datasets into tokens and keep ones that appear no less than 5 times in the entire dataset for text8 and
10 times for fil9. The resulting datasets have 71,291 and 142,276 unique vocabulary items.

Models Our main model is the ÂµP limit of Eq. (33). We compare against the baselines of 1) ï¬nite-
width versions of the same, and 2) the NTK and GP limits of Eq. (33). As shown in Corollary 3.9, the
features of the NTK limit are ï¬xed at initialization as n â†’ âˆ (and so are those of the GP limit, by
deï¬nition), so its answer to Eq. (35) is uniformly selected from the whole vocabulary.46 Its accuracy
|V|âˆ’3 . Since |V| is 71,291 for text8 and 142,276 for fil9, this number is practically 0.
is thus
We compute the ÂµP limit according to Algorithm 1, but we relate more implementation details in
Appendix D.2.

1

Findings We show our re-
sults in Table 3 and Figure
to the right. As expected,
the inï¬nite-width and ï¬nite-
width ÂµP networks signiï¬-
cantly outperform the NTK
limit.
In addition, we ob-
serve the ï¬nite width ÂµP
networks converge to the
performance of the ÂµP limit
from below, as width in-
creases.

45http://mattmahoney.net/dc/textdata.html
46There is some nuance here because h(Î¾)(cid:62)h( Â¯Î¾) is actually Î˜(

âˆš

n) instead of Î˜(n) because Î¾, Â¯Î¾ are one-hot,

but the conclusion is the same; see Appendix D.2.

29

2.55.07.510.012.515.0epoch1015202530354045word analogy accword2vec pretrained on text8log2(width)6.08.010.012345epoch303540455055word2vec pretrained on fil9log2(width)6.08.010.0Table 3: Test Accuracies on Word Analogy after Pretraining with CBOW Word2Vec.

Dataset

6

number = log2 width
8

ÂµP

10

GP/NTK

text8
fil9

33.35
44.39

41.58
54.24

42.56
55.69

43.31
56.45

0.0
0.0

10 Conclusion

In this paper, we presented a framework, based on the notion of abc-parametrizations and Tensor Pro-
grams technique, that uniï¬es the Neural Tangent Kernel (NTK) and Mean Field limits of large width
neural networks (NNs). In the Dynamical Dichotomy theorem, we classiï¬ed the abc-parametrizations
into feature learning and kernel regimes. We identiï¬ed the lack of feature learning as a fatal weakness
of NTK as a model for real NN. In fact, we showed the standard parametrization suffers from the
same problem. As a solution, we proposed the Maximal Update Parametrization (ÂµP) and derived its
inï¬nite-width limit, which admits feature learning. Through experiments on Word2Vec and few-shot
learning, we demonstrated that ÂµP is a good model for feature learning behavior in neural networks.

More generally, this paper showcased the power of the Tensor Programs technique: Any computation
expressable in a Tensor Program has a â€œinï¬nite-widthâ€ limit we can derive. Because of the universality
of Tensor Programs for expressing deep learning computation [49, 51], this technique systematically
solves the mathematical problem of taking inï¬nite-width limits which has been dealt with haphazardly
in prior literature. Its immense ï¬‚exibility means that the theory of reinforcement learning, self-
supervised learning, deep generative models, etc with overparametrized neural networks in the feature
learning regime are now ripe for the picking.

Acknowledgements

In alphabetical order, we thank Sina Alemohammad, Zeyuan Allen-Zhu, Francis Bach, Yasaman
Bahri, Lenaic Chizat, Jeremy Cohen, Yarin Gal, Quanquan Gu, Bobby He, Di He, Jiaoyang Huang,
Arthur Jacot, Jaehoon Lee, Jason Lee, Zhiyuan Li, Etai Littwin, Yiping Lu, Song Mei, Roman Novak,
Vinay Rao, Michael Santacroce, Sam Schoenholz, Lisa Schut, Jascha Sohl-Dickstein, Alessandro
Sordoni, Denny Wu, Huishuai Zhang, and Pengchuan Zhang for discusson and feedback.

References

[1] MartÃ­n Abadi, Paul Barham, Jianmin Chen, Zhifeng Chen, Andy Davis, Jeffrey Dean, Matthieu
Devin, Sanjay Ghemawat, Geoffrey Irving, Michael Isard, et al. Tensorï¬‚ow: A system for
large-scale machine learning. In 12th {USENIX} Symposium on Operating Systems Design and
Implementation ({OSDI} 16), pages 265â€“283, 2016.

[2] Laurence Aitchison. Why bigger is not always better: on ï¬nite and inï¬nite neural networks.
arXiv:1910.08013 [cs, stat], June 2020. URL http://arxiv.org/abs/1910.08013.

[3] Laurence Aitchison, Adam X. Yang, and Sebastian W. Ober. Deep kernel processes.
arXiv:2010.01590 [cs, stat], October 2020. URL http://arxiv.org/abs/2010.01590.

[4] Zeyuan Allen-Zhu, Yuanzhi Li, and Zhao Song. A Convergence Theory for Deep Learning
via Over-Parameterization. arXiv:1811.03962 [cs, math, stat], November 2018. URL http:
//arxiv.org/abs/1811.03962.

[5] Dyego AraÃºjo, Roberto I. Oliveira, and Daniel Yukimura. A mean-ï¬eld limit for certain deep
neural networks. arXiv:1906.00193 [cond-mat, stat], June 2019. URL http://arxiv.org/
abs/1906.00193.

[6] Mohsen Bayati and Andrea Montanari. The dynamics of message passing on dense graphs,
with applications to compressed sensing. IEEE Transactions on Information Theory, 57(2):
764â€“785, February 2011. ISSN 0018-9448, 1557-9654. doi: 10.1109/TIT.2010.2094817. URL
http://arxiv.org/abs/1001.3448.

30

[7] Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhari-
wal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agar-
wal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh,
Daniel M. Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler,
Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish,
Alec Radford, Ilya Sutskever, and Dario Amodei. Language Models are Few-Shot Learners.
arXiv:2005.14165 [cs], July 2020. URL http://arxiv.org/abs/2005.14165.

[8] Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhari-
wal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agar-
wal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh,
Daniel M. Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler,
Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish,
Alec Radford, Ilya Sutskever, and Dario Amodei. Language Models are Few-Shot Learners.
arXiv:2005.14165 [cs], July 2020. URL http://arxiv.org/abs/2005.14165.

[9] Minmin Chen, Jeffrey Pennington, and Samuel Schoenholz. Dynamical Isometry and a Mean
Field Theory of RNNs: Gating Enables Signal Propagation in Recurrent Neural Networks.
In Proceedings of the 35th International Conference on Machine Learning, volume 80 of
Proceedings of Machine Learning Research, pages 873â€“882, StockholmsmÃ¤ssan, Stockholm
Sweden, July 2018. PMLR. URL http://proceedings.mlr.press/v80/chen18i.html.

[10] Lenaic Chizat and Francis Bach. A Note on Lazy Training in Supervised Differentiable

Programming. page 19.

[11] Lenaic Chizat and Francis Bach. On the Global Convergence of Gradient Descent for Over-
parameterized Models using Optimal Transport. arXiv:1805.09545 [cs, math, stat], May 2018.
URL http://arxiv.org/abs/1805.09545.

[12] Lenaic Chizat and Francis Bach. Implicit Bias of Gradient Descent for Wide Two-layer Neural
Networks Trained with the Logistic Loss. arXiv:2002.04486 [cs, math, stat], June 2020. URL
http://arxiv.org/abs/2002.04486.

[13] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. BERT: Pre-training of
Deep Bidirectional Transformers for Language Understanding. arXiv:1810.04805 [cs], May
2019. URL http://arxiv.org/abs/1810.04805. arXiv: 1810.04805 version: 2.

[14] Simon S. Du, Xiyu Zhai, Barnabas Poczos, and Aarti Singh. Gradient Descent Provably
Optimizes Over-parameterized Neural Networks. arXiv:1810.02054 [cs, math, stat], October
2018. URL http://arxiv.org/abs/1810.02054.

[15] Cong Fang, Jason D. Lee, Pengkun Yang, and Tong Zhang. Modeling from Features: a Mean-
ï¬eld Framework for Over-parameterized Deep Neural Networks. arXiv:2007.01452 [cs, math,
stat], July 2020. URL http://arxiv.org/abs/2007.01452. arXiv: 2007.01452.

[16] Chelsea Finn, Pieter Abbeel, and Sergey Levine. Model-Agnostic Meta-Learning for Fast
Adaptation of Deep Networks. arXiv:1703.03400 [cs], July 2017. URL http://arxiv.org/
abs/1703.03400.

[17] Dar Gilboa and Guy Gur-Ari. Wider Networks Learn Better Features. September 2019. URL

https://arxiv.org/abs/1909.11572v1.

[18] Dar Gilboa, Bo Chang, Minmin Chen, Greg Yang, Samuel S. Schoenholz, Ed H. Chi, and
Jeffrey Pennington. Dynamical Isometry and a Mean Field Theory of LSTMs and GRUs.
arXiv:1901.08987 [cs, stat], January 2019. URL http://arxiv.org/abs/1901.08987.

[19] Eugene A. Golikov. Dynamically Stable Inï¬nite-Width Limits of Neural Classiï¬ers.
arXiv:2006.06574 [cs, stat], October 2020. URL http://arxiv.org/abs/2006.06574.

[20] Boris Hanin. Which Neural Net Architectures Give Rise To Exploding and Vanishing Gradients?

January 2018. URL https://arxiv.org/abs/1801.03744.

[21] Boris Hanin and David Rolnick. How to Start Training: The Effect of Initialization and
Architecture. arXiv:1803.01719 [cs, stat], March 2018. URL http://arxiv.org/abs/1803.
01719.

31

[22] Souï¬ane Hayou, Arnaud Doucet, and Judith Rousseau. On the Selection of Initialization and
Activation Function for Deep Neural Networks. arXiv:1805.08266 [cs, stat], May 2018. URL
http://arxiv.org/abs/1805.08266.

[23] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep Residual Learn-
URL https://www.cv-

ing for
foundation.org/openaccess/content_cvpr_2016/html/He_Deep_Residual_
Learning_CVPR_2016_paper.html.

pages 770â€“778, 2016.

Image Recognition.

[24] Dan Hendrycks and Kevin Gimpel. Gaussian Error Linear Units (GELUs). arXiv:1606.08415

[cs], July 2020. URL http://arxiv.org/abs/1606.08415.

[25] Jiaoyang Huang and Horng-Tzer Yau. Dynamics of deep neural networks and neural tangent

hierarchy, 2019.

[26] Arthur Jacot, Franck Gabriel, and ClÃ©ment Hongler. Neural Tangent Kernel: Convergence
and Generalization in Neural Networks. arXiv:1806.07572 [cs, math, stat], June 2018. URL
http://arxiv.org/abs/1806.07572.

[27] Aitor Lewkowycz, Yasaman Bahri, Ethan Dyer, Jascha Sohl-Dickstein, and Guy Gur-Ari. The
large learning rate phase of deep learning: the catapult mechanism. arXiv:2003.02218 [cs, stat],
March 2020. URL http://arxiv.org/abs/2003.02218.

[28] Yuanzhi Li, Tengyu Ma, and Hongyang R. Zhang. Learning over-parametrized two-layer
neural networks beyond ntk. In Jacob Abernethy and Shivani Agarwal, editors, Proceedings
of Thirty Third Conference on Learning Theory, volume 125 of Proceedings of Machine
Learning Research, pages 2613â€“2682. PMLR, 09â€“12 Jul 2020. URL http://proceedings.
mlr.press/v125/li20a.html.

[29] Zhuohan Li, Eric Wallace, Sheng Shen, Kevin Lin, Kurt Keutzer, Dan Klein, and Joseph E.
Gonzalez. Train Large, Then Compress: Rethinking Model Size for Efï¬cient Training and
Inference of Transformers. arXiv:2002.11794 [cs], June 2020. URL http://arxiv.org/
abs/2002.11794.

[30] Song Mei, Andrea Montanari, and Phan-Minh Nguyen. A mean ï¬eld view of the landscape
of two-layer neural networks. Proceedings of the National Academy of Sciences, 115(33):
E7665â€“E7671, August 2018. ISSN 0027-8424, 1091-6490. doi: 10.1073/pnas.1806579115.
URL https://www.pnas.org/content/115/33/E7665.

[31] Song Mei, Theodor Misiakiewicz, and Andrea Montanari. Mean-ï¬eld theory of two-layers
neural networks: dimension-free bounds and kernel limit. arXiv:1902.06015 [cond-mat, stat],
February 2019. URL http://arxiv.org/abs/1902.06015.

[32] Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey Dean. Efï¬cient Estimation of Word
Representations in Vector Space. arXiv:1301.3781 [cs], September 2013. URL http://arxiv.
org/abs/1301.3781.

[33] Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg Corrado, and Jeffrey Dean. Distributed
Representations of Words and Phrases and their Compositionality. arXiv:1310.4546 [cs, stat],
October 2013. URL http://arxiv.org/abs/1310.4546.

[34] Phan-Minh Nguyen. Mean Field Limit of the Learning Dynamics of Multilayer Neural Networks.
arXiv:1902.02880 [cond-mat, stat], February 2019. URL http://arxiv.org/abs/1902.
02880.

[35] Phan-Minh Nguyen and Huy Tuan Pham. A Rigorous Framework for the Mean Field Limit
of Multilayer Neural Networks. arXiv:2001.11443 [cond-mat, stat], January 2020. URL
http://arxiv.org/abs/2001.11443.

[36] Quynh Nguyen and Marco Mondelli. Global convergence of deep networks with one wide layer

followed by pyramidal topology, 2020.

[37] Alex Nichol, Joshua Achiam, and John Schulman. On First-Order Meta-Learning Algorithms.

March 2018. URL https://arxiv.org/abs/1803.02999v3.

32

[38] Samet Oymak and Mahdi Soltanolkotabi. Toward moderate overparameterization: Global
convergence guarantees for training shallow neural networks. IEEE Journal on Selected Areas
in Information Theory, 1(1):84â€“105, May 2020. ISSN 2641-8770. doi: 10.1109/jsait.2020.
2991332. URL http://dx.doi.org/10.1109/JSAIT.2020.2991332.

[39] Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory
Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, Alban Desmai-
son, Andreas Kopf, Edward Yang, Zachary DeVito, Martin Raison, Alykhan Tejani,
Sasank Chilamkurthy, Benoit Steiner, Lu Fang, Junjie Bai, and Soumith Chintala. Py-
torch: An imperative style, high-performance deep learning library.
In H. Wallach,
H. Larochelle, A. Beygelzimer, F. d'AlchÃ©-Buc, E. Fox, and R. Garnett, editors, Ad-
vances in Neural Information Processing Systems 32, pages 8024â€“8035. Curran Associates,
Inc., 2019. URL http://papers.neurips.cc/paper/9015-pytorch-an-imperative-
style-high-performance-deep-learning-library.pdf.

[40] Jeffrey Pennington, Samuel Schoenholz, and Surya Ganguli. Resurrecting the sigmoid in deep
learning through dynamical isometry:
In I. Guyon, U. V. Luxburg,
S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett, editors, Advances
in Neural Information Processing Systems 30, pages 4788â€“4798. Curran Associates, Inc.,
2017. URL http://papers.nips.cc/paper/7064-resurrecting-the-sigmoid-in-
deep-learning-through-dynamical-isometry-theory-and-practice.pdf.

theory and practice.

[41] George Philipp and Jaime G. Carbonell. The Nonlinearity Coefï¬cient - Predicting Overï¬tting
in Deep Neural Networks. arXiv:1806.00179 [cs, stat], May 2018. URL http://arxiv.org/
abs/1806.00179.

[42] Ben Poole, Subhaneil Lahiri, Maithreyi Raghu, Jascha Sohl-Dickstein, and Surya Ganguli.
Exponential expressivity in deep neural networks through transient chaos. In Advances In
Neural Information Processing Systems, pages 3360â€“3368, 2016.

[43] Grant M. Rotskoff and Eric Vanden-Eijnden. Neural Networks as Interacting Particle Systems:
Asymptotic Convexity of the Loss Landscape and Universal Scaling of the Approximation Error.
arXiv:1805.00915 [cond-mat, stat], May 2018. URL http://arxiv.org/abs/1805.00915.

[44] Samuel S. Schoenholz, Justin Gilmer, Surya Ganguli, and Jascha Sohl-Dickstein. Deep Infor-

mation Propagation. 2017. URL https://openreview.net/pdf?id=H1W1UN9gg.

[45] Justin Sirignano and Konstantinos Spiliopoulos. Mean Field Analysis of Neural Networks.

arXiv:1805.01053 [math], May 2018. URL http://arxiv.org/abs/1805.01053.

[46] Justin Sirignano and Konstantinos Spiliopoulos. Mean Field Analysis of Deep Neural Networks.
arXiv:1903.04440 [math, stat], February 2020. URL http://arxiv.org/abs/1903.04440.

[47] Jascha Sohl-Dickstein, Roman Novak, Samuel S. Schoenholz, and Jaehoon Lee. On the inï¬nite
width limit of neural networks with a standard parameterization. arXiv:2001.07301 [cs, stat],
January 2020. URL http://arxiv.org/abs/2001.07301.

[48] Blake Woodworth, Suriya Gunasekar, Jason D. Lee, Edward Moroshko, Pedro Savarese, Itay
Golan, Daniel Soudry, and Nathan Srebro. Kernel and Rich Regimes in Overparametrized
Models. arXiv:2002.09277 [cs, stat], July 2020. URL http://arxiv.org/abs/2002.09277.

[49] Greg Yang. Tensor Programs I: Wide Feedforward or Recurrent Neural Networks of Any Archi-
tecture are Gaussian Processes. arXiv:1910.12478 [cond-mat, physics:math-ph], December
2019. URL http://arxiv.org/abs/1910.12478.

[50] Greg Yang. Scaling Limits of Wide Neural Networks with Weight Sharing: Gaussian Process
Behavior, Gradient Independence, and Neural Tangent Kernel Derivation. arXiv:1902.04760
[cond-mat, physics:math-ph, stat], February 2019. URL http://arxiv.org/abs/1902.
04760.

[51] Greg Yang. Tensor Programs II: Neural Tangent Kernel for Any Architecture. arXiv:2006.14548

[cond-mat, stat], August 2020. URL http://arxiv.org/abs/2006.14548.

[52] Greg Yang. Tensor Programs III: Neural Matrix Laws. arXiv:2009.10685 [cs, math], September

2020. URL http://arxiv.org/abs/2009.10685.

33

[53] Greg Yang and Hadi Salman. A ï¬ne-grained spectral perspective on neural networks, 2019.

[54] Greg Yang and Sam S. Schoenholz. Deep mean ï¬eld theory: Layerwise variance and width
variation as methods to control gradient explosion, 2018. URL https://openreview.net/
forum?id=rJGY8GbR-.

[55] Greg Yang and Samuel S. Schoenholz. Mean Field Residual Network: On the Edge of Chaos.

In Advances in neural information processing systems, 2017.

[56] Greg Yang, Jeffrey Pennington, Vinay Rao, Jascha Sohl-Dickstein, and Samuel S. Schoenholz.
A Mean Field Theory of Batch Normalization. arXiv:1902.08129 [cond-mat], February 2019.
URL http://arxiv.org/abs/1902.08129.

[57] Difan Zou, Yuan Cao, Dongruo Zhou, and Quanquan Gu. Stochastic Gradient Descent Optimizes
Over-parameterized Deep ReLU Networks. arXiv:1811.08888 [cs, math, stat], November 2018.
URL http://arxiv.org/abs/1811.08888.

34

A A Short Origin Story of the Tensor Programs Paper Series

The Tensor Programs framework was initially proposed in [50] in February 2019, and was mainly
applied to extend the NNGP and NTK limits to arbitrary architectures (and to make rigorous the
signal propagation literature [9, 18, 20â€“22, 40â€“42, 44, 53â€“56]). While NNGP and NTK amount to
taking limits of neural networks at initialization, it was soon, in April 2019, realized that Tensor
Programs could 1) also trivially take limits of the entire training procedure of neural networks (which
is the main theoretical idea of this paper), and 2) calculate the feature learning limit. However, at
that point, it also became clear that [50] was not written accessibly, and its formulation of Tensor
Programs was cumbersome to use. A question had to be asked: Should the feature learning paper
be written immediately on such an unwieldy foundation, or should signiï¬cant effort be devoted
to ï¬xing this foundation ï¬rst? Eventually, a decision was made in favor of the latter. The Tensor
Programs series was created as way to re-organize and re-present the Tensor Programs machinery in
a user-friendly way to the machine learning audience (the ï¬rst 3 papers [49, 51, 52] of the series),
before extracting payoffs from this foundation (starting from this paper).

B Further Discussions on the Shallow NTK and MF Examples

How does the Function Change? If the NTK limit does not allow features to evolve, then how
does learning occur? To answer this question, note

âˆ†ft(Î¾) = V0âˆ†xt(Î¾) + âˆ†Vtx0(Î¾) + âˆ†Vtâˆ†xt(Î¾).

In short, then, the evolution of ft(Î¾) in the NTK limit is predominantly due to V0âˆ†xt(Î¾) and
âˆ†Vtx0(Î¾) only, while in the MF limit, âˆ†Vtâˆ†xt(Î¾) also contributes nontrivially.
Example: For t = 1, âˆ†f1(Î¾) = V0âˆ†x1(Î¾) + nâˆ’2av x(cid:62)
so the term nâˆ’2av x(cid:62)
O(1/

0 x0(Î¾) = Î˜(1) for generic Î¾, Î¾0. On the other hand, nâˆ’2av x(cid:62)

0 âˆ†x1(Î¾). In NTP, av = 1/2,
0 âˆ†x1(Î¾) =

0 x0(Î¾) + nâˆ’2av x(cid:62)

n) because âˆ†x1(Î¾) = O(1/
V0âˆ†x1(Î¾) â‰ˆ V0[Ï†(cid:48)(h0(Î¾)) (cid:12) âˆ†h1(Î¾)] = V0[Ï†(cid:48)(h0(Î¾)) (cid:12) âˆ†h1(Î¾)]

n) as noted above. Likewise,

âˆš

âˆš

= C

n
(cid:88)

Î±=1

V0Î±Ï†(cid:48)(h0(Î¾)Î±)V0Î±Ï†(cid:48)(h0Î±) = C

n
(cid:88)

(V0Î±)2Ï†(cid:48)(h0(Î¾)Î±)Ï†(cid:48)(h0Î±),

Î±=1

where C = Ï‡0Î¾0Î¾ = Î˜(1). Now (V0Î±)2 = Î˜(1/n) and is almost surely positive. On the other hand,
Ï†(cid:48)(h0(Î¾)Î±)Ï†(cid:48)(h0Î±) = Î˜(1) and should have a nonzero expectation over random initialization (for
example, if Ï† is relu then this is obvious). Therefore, the sum above should amount to V0âˆ†x1(Î¾) â‰ˆ
Î˜(1). In summary, in the NTK limit, âˆ†f1(Î¾) = Î˜(1) due to the interactions between V0 and âˆ†x1(Î¾)
and between âˆ†V1 and x0(Î¾), but there is only vanishing interaction between âˆ†V1 and âˆ†x1(Î¾).
The case for general t, again, can be derived easily using Tensor Programs.

C abc-Parametrization for General Neural Architectures

We can straightforwardly generalize abc-parametrizations to an arbitrary neural architecture. Each
parameter tensor W would get its own aW and bW , such that W = nâˆ’aW w and w is the actual
trainable parameter with initialization wÎ±Î² âˆ¼ N (0, nâˆ’2bW ). The learning rate is still Î·nâˆ’c for some
ï¬xed Î·.

C.1 Maximal Update Parametrization

MLP with Biases Suppose in Eq. (1), for each l âˆˆ [L], we have hl(Î¾) = W lxlâˆ’1(Î¾) + bl instead,
for bias bl âˆˆ Rn. Then in ÂµP, the bias bl should have abl = âˆ’1/2 and bbl = 1/2. We can also have
bias bL+1 in the logits f (Î¾) = W L+1xL(Î¾) + bL+1. Then we set abL+1 = bbL+1 = 0.

General Neural Architectures More generally, ÂµP can be deï¬ned easily for any neural architecture
whose forward pass can be written down as a Tensor Program (e.g. ResNet or Transformer; see
[49] for explicit programs). The learning rate is always independent of width, i.e. c = 0. For any
parameter tensor W , bW is always 1/2, and aW can be deï¬ned as follows: If W is not an output
weight matrix, then aW should be set to âˆ’1 + 1
2 pW , where pW = limnâ†’âˆ logn #(W ) is a) 0 if both

35

sides of W are ï¬xed w.r.t. n; b) 1 if W is a vector (e.g. bias) or with one side being ï¬xed dimensional
(e.g. W 1); and c) 2 if W is a matrix with both sides scaling like n (e.g. weights in the middle of an
MLP). If W is an output weight matrix (and thus the output dimension is ï¬xed w.r.t. n), then aW
should be 1

2 . If W is an output bias, then aW should be 0.

Optimality Properties One can formalize, in this general context, the notion of stability and the
notions of a parameter tensor being updated maximally and (a set of readout weights) being initialized
maximally. Then one can show that ÂµP is the unique stable abc-parametrization such that all of its
parameter tensors are updated maximally and all of its readout weights are initialized maximally.

D Experimental Details

The main models in our experiments are all 1-hidden-layer linear MLPs with input dimension d and
output dimension do. In our experiments, we will consider more advanced forms, but, as warmup, a
basic version of such a network is given by

f (Î¾) = V h(Î¾),

h(Î¾) = U Î¾,

(36)

for U âˆˆ RnÃ—d, V âˆˆ RdoÃ—n parametrized like U =
uÎ±Î², vÎ±Î² âˆ¼ N (0, 1/n). In this case, Corollary 6.2 generalizes to
Theorem D.1. Consider a 1-hidden-layer linear MLP in ÂµP (Eq. (36)) and any training routine with
learning rate Î·. As n â†’ âˆ, for every input Î¾ âˆˆ Rd, ft(Î¾) âˆˆ Rdo converges almost surely to Ëšft(Î¾)
deï¬ned as follows:

n v and with initialization

nu, V = 1âˆš

âˆš

Ëšft(Î¾) = (AtCt + BtDt)Î¾ âˆˆ Rdo,

ËšÏ‡t = L(cid:48)(Ëšft, yt) âˆˆ Rdo ,
(At+1, Bt+1) = (At, Bt) âˆ’ Î·ËšÏ‡t âŠ— (CtÎ¾t, DtÎ¾t),
(Ct+1, Dt+1) = (Ct, Dt) âˆ’ Î·(A(cid:62)

t ËšÏ‡t, B(cid:62)

t ËšÏ‡t) âŠ— Î¾t,

where âŠ— denotes outer product (u âŠ— v = uv(cid:62)), with initial condition

A0 = Ido âˆˆ RdoÃ—do , D0 = Id âˆˆ RdÃ—d, B0 = 0 âˆˆ RdoÃ—d, C0 = 0 âˆˆ RdÃ—do .

While we will not use this theorem, we intend it to give an idea of the mathematical process underneath
our implementations, which we discuss now.

D.1 Few-Shot Learning on Omniglot via MAML

D.1.1 Linear 1-Hidden-Layer ÂµP Network

We consider a linear 1-hidden-layer MLP with bias, input dimension d, output dimension do, given
by

f (Î¾) = V h(Î¾) âˆˆ Rdo ,
âˆš

h(Î¾) = U Î¾ + B âˆˆ Rn,

where Î¾ âˆˆ Rd. Following ÂµP, we factor U =
where u, v, Î² are the trainable parameters. We initialize uÎ±Î² âˆ¼ N (0, Ïƒ2
Î² = 0 âˆˆ Rn. We can cancel the factors of

nu âˆˆ RnÃ—d, V = 1âˆš

n and rewrite

âˆš

n v âˆˆ RdoÃ—n, B = Î±

âˆš

nÎ² âˆˆ Rn,
v/n),

u/n), vÎ±Î² âˆ¼ N (0, Ïƒ2

f (Î¾) = vh(Î¾) âˆˆ Rdo,

h(Î¾) = uÎ¾ + b âˆˆ Rn,

where b = Î±Î². We will also consider gradient clipping with threshold g and weight decay with
coefï¬cient Î³. So in summary, the hyperparameters are

Ïƒu, Ïƒv (init. std.), Î± (bias multiplier),

Î· (LR),

g (grad. clip),

Î³ (weight decay).

As in Corollary 6.2, itâ€™s easy to see that each column of ut at any time t is always a linear combination
of the columns of u0 and the rows of v0 such that the coefï¬cients of these linear combinations converge
deterministically in the n â†’ âˆ limit; likewise for bt and the rows of vt. To track the evolution of f ,
it sufï¬ces to track these coefï¬cients. Therefore, for implementation, we reparametrize as follows:

36

Coefï¬cient matrix and vector Let Âµ1, . . . , Âµd, Î½1, . . . , Î½do âˆˆ Rn be standard Gaussian vectors
such that the columns of u0 will be initialized as ÏƒuÂµ1/
n and the rows of V0 will
n. Write Âµ = (Âµ1, . . . , Âµd) âˆˆ RnÃ—d, Î½ = (Î½1, . . . , Î½do ) âˆˆ
be initialized as ÏƒvÎ½1/
RnÃ—do. Deï¬ne coefï¬cient matrices

n, . . . , ÏƒvÎ½do/

n, . . . , ÏƒuÂµd/

âˆš

âˆš

âˆš

âˆš

u(cid:62) âˆˆ RdÃ—(d+do), v âˆˆ RdoÃ—(d+do),

such that at any time, (u, v(cid:62)) âˆˆ RnÃ—(d+do) is
initialize

1âˆš
n (Âµ, Î½)(u, v(cid:62)) in the inï¬nite-width limit. We

(cid:19)

(cid:18)u(cid:62)
v

(cid:18)ÏƒuI
0

â†

(cid:19)

,

0
ÏƒvI

i.e. a â€œdiagonalâ€ initialization. Likewise, deï¬ne coefï¬cient vector b âˆˆ Rd+do, initialized at 0,
such that, at any time, b is approximately distributed as 1âˆš
n (Âµ, Î½)b. To track the evolution of the
inï¬nite-width network, we will track the evolution of u, v, b.

In general, we use bold to denote the coefï¬cients (in Âµ, Î½) of a tensor (e.g. b for coefï¬cients of b). We
also use capital letters to denote the batched version (e.g. H for batched version of h). Algorithms 2
and 3 below summarize the SGD training of the ï¬nite- and the inï¬nite-width networks. Note that
aside from initialization and the hidden size (n vs d + do), the algorithms are essentially identical.

Algorithm 2 SGD Training of Finite-Width Lin-
ear ÂµP 1-Hidden-Layer Network
Input: Hyperparameters n, Ïƒu, Ïƒv, Î±, Î·, g, Î³.
1: Initialize uÎ±Î² âˆ¼ N (0, Ïƒ2
2: Initialize vÎ±Î² âˆ¼ N (0, Ïƒ2
3: Initialize b â† 0
4: for each batch of inputs Î âˆˆ RBÃ—d and la-

u/n)
v/n)

Algorithm 3 SGD Training of Inï¬nite-Width Lin-
ear ÂµP 1-Hidden-Layer Network
Input: Hyperparameters Ïƒu, Ïƒv, Î±, Î·, g, Î³.
1: Initialize u(cid:62) â† (ÏƒuI, 0)
2: Initialize v â† (0, ÏƒvI)
3: Initialize b â† 0
4: for each batch of inputs Î âˆˆ RBÃ—d and la-

bels Y âˆˆ RBÃ—do do
// Forward Pass
H â† Îu(cid:62) + b âˆˆ RBÃ—n
f (Î) â† Hv(cid:62) âˆˆ RBÃ—do
// Backward Pass
Ï‡ â† L(cid:48)(f (Î), Y ) âˆˆ RBÃ—do
du â† âˆ’v(cid:62)Ï‡(cid:62)Î âˆˆ RnÃ—d
dv â† âˆ’Ï‡(cid:62)H âˆˆ RdoÃ—n
db â† âˆ’Î±21(cid:62)Ï‡v âˆˆ Rn
// Gradient Clipping
(cid:107)du(cid:107)2

(cid:113)

5:
6:
7:
8:
9:
10:
11:
12:
13:

14:

Î± (cid:107)2

F + (cid:107) db

F + (cid:107)dv(cid:107)2

G â†
Ï â† min(1, g/G)
du â† Ïdu
dv â† Ïdv
db â† Ïdb
// Gradient Step w/ Weight Decay
u += Î·du âˆ’ Î·Î³u âˆˆ RdÃ—n
v += Î·dv âˆ’ Î·Î³v âˆˆ RdoÃ—n
b += Î·db âˆ’ Î·Î³b âˆˆ Rn

15:
16:
17:
18:
19:
20:
21:
22:
23: end for

bels Y âˆˆ RBÃ—do do
// Forward Pass
H â† Îu(cid:62) + b âˆˆ RBÃ—(d+do)
f (Î) â† Hv(cid:62) âˆˆ RBÃ—do
// Backward Pass
Ï‡ â† L(cid:48)(f (Î), Y ) âˆˆ RBÃ—do
du â† âˆ’v(cid:62)Ï‡(cid:62)Î âˆˆ R(d+do)Ã—d
dv â† âˆ’Ï‡(cid:62)H âˆˆ RdoÃ—(d+do)
db â† âˆ’Î±21(cid:62)Ï‡v âˆˆ Rd+do
// Gradient Clipping

5:
6:
7:
8:
9:
10:
11:
12:
13:

14:

(cid:113)

Î± (cid:107)2

(cid:107)du(cid:107)2

F + (cid:107) db

F + (cid:107)dv(cid:107)2

G â†
Ï â† min(1, g/G)
du â† Ïdu
dv â† Ïdv
db â† Ïdb
// Gradient Step w/ Weight Decay
u += Î·du âˆ’ Î·Î³u âˆˆ R(d+do)Ã—d
v += Î·dv âˆ’ Î·Î³v âˆˆ RdoÃ—(d+do)
b += Î·db âˆ’ Î·Î³b âˆˆ Rd+do

15:
16:
17:
18:
19:
20:
21:
22:
23: end for

During inference, we just run the Forward Pass section with Î substituted with test data.

The algorithms for MAML can then be obtained by a straightforward modiï¬cation of these algorithms.
(Note that in MAML, we do not clip gradients during adaptation, but rather clip the gradient against
the validation loss of task; we also disable weight decay by setting the coefï¬cient Î³ to 0).

Hyperparameter Sweep We sweep Ïƒu, Ïƒv, Î· and Î± with the following grid for ï¬nite width and
ÂµP networks.

â€¢ Ïƒu : [0.5, 1, 2, 4, 8],

37

Algorithm 4 MAML Training of Kernel Model with Kernel K

Ï‡i â† L(cid:48)(fQ(Î¾i), yi)

Draw a batch of tasks
for each task in batch do

end for
for each input/label pair (Î¾i, yi) âˆˆ D do

// Adaptation
Sample training set D
for each input/label pair (Î¾i, yi) âˆˆ D do

Input: Kernel K, adaptation step size (cid:15), meta learning rate Î·, batch size B, gradient clip g
1: Initialize Q = {}
2: while True do
3:
4:
5:
6:
7:
8:
9:
10:
11:
12:
13:
14:
15:
16:
17:
18:
19:
20:
21:

end for
// Calculate Test Set Gradient
Sample test set Ë†D
for each input/label pair ( Ë†Î¾i, Ë†yi) âˆˆ Ë†D do

end for
for each input/label pair (Î¾i, yi) âˆˆ D do

Ë†Ï‡i â† L(cid:48)(fQ( Ë†Î¾i), Ë†yi)

Q.push((Î¾i, âˆ’(cid:15)Ï‡i))

Q.pop((Î¾i, âˆ’(cid:15)Ï‡i))

end for
// Gradient Clip
(cid:113)(cid:80)

(cid:80)

( Ë†Î¾j ,Ë†yj )âˆˆ Ë†D Ë†Ï‡i Ë†Ï‡jK( Ë†Î¾i, Ë†Î¾j)

( Ë†Î¾i,Ë†yi)âˆˆ Ë†D

G â†
Ï â† min(1, g/G)
// Gradient Update
for each input/label pair ( Ë†Î¾i, Ë†yi) âˆˆ Ë†D do

Q.push(( Ë†Î¾i, âˆ’ÏÎ· Ë†Ï‡i))

end for

22:

23:
24:
25:
26:
27:
end for
28:
29: end while

â€¢ Ïƒv : [2âˆ’5, 2âˆ’4, 2âˆ’3, 2âˆ’2, 2âˆ’1],

â€¢ Î· : [0.025, 0.05, 0.1, 0.2, 0.4],

â€¢ Î± : [0.25, 0.5, 1, 2, 4]

We are interested in 1-shot, 5-way learning with Omniglot. This means that each task provides
5 training samples, each corresponding to one of the 5 labels of the task. Each hyperparameter
combination above is used to train for 100 epochs over 3 random seeds, where each epoch consists of
100 batches of 32 tasks. We average the validation accuracy across the last 10 epochs and document
the best hyperparameters in Table 4, along with the test accuracy from a 15-seed rerun47 for better
benchmarking. For NTK and GP, we additionally tune the initialization Ïƒb for biases, which is set to
0 for both ï¬nite and ÂµP networks for simplicity.

D.1.2 NNGP and NTK for Relu Networks

Consider a kernel K, which in our case will be the NNGP or NTK of a 1-hidden-layer relu network.
WLOG, it is induced by an embedding Î¦ such that K(Î¾, Î¶) = (cid:104)Î¦(Î¾), Î¦(Î¶)(cid:105) where (cid:104), (cid:105) is the inner
product in the embedding space; we do not care about the details of Î¦ or (cid:104), (cid:105) as eventually our
algorithm only depends on K.

In our setting, we will train a linear layer W on top of Î¦ via MAML, f (Î¾) def= (cid:104)W, Î¦(Î¾)(cid:105). One can see
easily that W is always a linear combination of Î¦(Î¶) for various Î¶ from the training set weâ€™ve seen
so far. Thus, to track W , it sufï¬ces to keep an array Q of pairs (Î¶, q) such that W = (cid:80)
(Î¶,q)âˆˆQ qÎ¦(Î¶)

47After excluding outliers at least one standard deviation away from the mean.

38

Table 4: Best hyperparameters for the MAML experiment.

log2Width/Limit
1
3
5
7
9
11
13
ÂµP

Ïƒu
0.5
0.5
1
1
1
1
1
1

Ïƒv
0.5
0.25
0.125
0.125
0.03125
0.03125
0.03125
0.03125

NTK
GP

0.25
1

1
0.25

Ïƒb
-
-
-
-
-
-
-
-

1
1

Î·

0.05
0.1
0.4
0.1
0.1
0.1
0.1
0.1

0.05
0.05

Î±

2
1
0.5
1
1
1
1
1

1
1

Val. Acc. (%) Test Acc. (%)

46.72 Â± 4.30
65.30 Â± .27
68.74 Â± .18
69.03 Â± .04
69.32 Â± .07
69.27 Â± .11
69.27 Â± .14
69.26 Â± .13

55.34 Â± 1.24
64.54 Â± .70
66.21 Â± .15
66.31 Â± .16
66.43 Â± .23
66.36 Â± .22
66.41 Â± .18
66.42 Â± .19

47.47 Â± .13
38.92 Â± .15

47.82 Â± .04
47.60 Â± .02

at all times. Let fQ be the function with W given by Q. Then

fQ(Î¾) =

(cid:88)

qÎ¶K(Î¶, Î¾).

(Î¶,qÎ¶ )âˆˆQ

In our case, the number of possible inputs is too large to instantiate a value q for every Î¶, so we
gradually grow a dynamic array Q, which we model as a stack. Then MAML can be implemented as
in Algorithm 4.

Hyperparameter Sweep We sweep Ïƒu, Ïƒv, Ïƒb and Î· with the following grid for GP and NTK.

â€¢ Ïƒu : [0.25, 0.5, 1, 2, 4],
â€¢ Ïƒv : [0.25, 0.5, 1, 2, 4],
â€¢ Ïƒb : [0.25, 0.5, 1, 2, 4],
â€¢ Î· : [0.05, 0.1, 0.2, 0.4, 0.8]

Each hyperparameter combination above is used to train for 5 epochs (the ï¬rst epoch is almost always
the best) over 3 random seeds, where each epoch consists of 100 batches of 32 tasks. We take the
validation accuracy among all epochs and document the best hyperparameters in Table 4, along with
the test accuracy from a 15-seed rerun.

D.2 Word2Vec Experimental Details

D.2.1 ÂµP Limit

We shall derive the training algorithm for ÂµP Word2Vec. First, we introduce the notation for word
embeddings. We denote Î¦i def= h(Î¾i). If Î¾i is a one-hot vector with the ith element set to 1, Î¦i is
essentially the ith column of the weight matrix U . We also deï¬ne the following short-hands for the
context embedding: Î¦J def= EjâˆˆJ Î¦j = h(Î¾J ). Similarly, V (cid:62)Î¾Ï„ describes a row in V ; we can deï¬ne
Î¦Ë†Ï„ def= Ë†h(Î¾Ï„ ) def= V (cid:62)Î¾Ï„ and rewrite the loss function.

L(f (Î¾J ), Î¾Ï„ ) =

(cid:26)log(1 âˆ’ Ïƒ(Î¦J (cid:62)Î¦Ë†Ï„ ))

log Ïƒ(Î¦J (cid:62)Î¦Ë†Ï„ )

Ï„ = i
Ï„ (cid:54)= i.

Consequently, the backward pass becomes:

âˆ†Î¦j =

1
|J|

âˆ†Î¦J =

Î·
|J|

âˆ‚L
âˆ‚Î¦J =

(cid:40) Î·

|J| Î¦Ë†Ï„ (1 âˆ’ Ïƒ(Î¦J (cid:62)Î¦Ë†Ï„ ))
âˆ’ Î·
|J| Î¦Ë†Ï„ Ïƒ(Î¦J (cid:62)Î¦Ë†Ï„ )

Ï„ = i
Ï„ (cid:54)= i.

(37)

(38)

Following ÂµP, we initialize UÎ±Î² âˆ¼ N (0, Ïƒunâˆ’1) and VÎ±Î² âˆ¼ N (0, Ïƒvnâˆ’1), where n is the width of
n in V cancel out because the
the ï¬nite network. (Here the explicit multipliers of
network is linear). The tunable hyperparameters are the initialization std Ïƒuand Ïƒv, learning rate Î·

n in U and 1/

âˆš

âˆš

39

and weight decay ratio Î³. Rather than tuning the hyperparameters extensively for each width, we pick
some reasonable values and use them for all of our experiments. Speciï¬cally, we have Ïƒu = Ïƒv = 1,
Î· = 0.05 and Î³ = 0.001.
Again, using Corollary 6.2, we can train the ÂµP limit in the coefï¬cient space of u(cid:62) âˆˆ R|V|Ã—2|V|, v âˆˆ
R|V|Ã—2|V|, with the same â€œdiagonalâ€ initialization:

(cid:19)

(cid:18)u(cid:62)
v

(cid:18)ÏƒuI
0

â†

(cid:19)

,

0
ÏƒvI

We can adopt the embedding notation and represent a row of u with the embedding coefï¬cient vector
Î¦â€¢ and a column of v with Î¦Ë†â€¢. This is computationally equivalent to training with a hidden size of
2|V| and with embeddings initialized as rows (or columns) of one-hot vectors. The full algorithm is
described in Algorithm 2 and Algorithm 3; in this case, we remove biases and use weight decay with
coefï¬cient Î³ = 0.001. After training, rows of the weight matrix u (resp. coefï¬cient matrix u), i.e.
Î¦â€¢ (resp. Î¦â€¢), are taken as the word vectors.

D.2.2 NTK Limit

In the NTK parametrization, V and U in Eq. (33) factor as V = 1âˆš
n v and U = u, and the learning
rate is Î˜(1). Each column Uâ€¢i of U is equal to h(Î¾i). At any ï¬xed time t, it is easy to see via Tensor
Programs that

ht(Î¾i) = h0(Î¾i) +

O(1/

n)vj + Ocoord(1/n)

(cid:88)

âˆš

jâˆˆV

where vj denotes the jth row of v at initialization, and where Ocoord(1/n) means a vector that is
O(1/n) coordinatewise. Recall that U = u and v are initialized with iid standard Gaussian entries.
Because Î¾i is one-hot, this in particular implies h0(Î¾i) has standard Gaussian entries, and h0(Î¾i) is
independent from h0(Î¾j) for i (cid:54)= j. Then for any i (cid:54)= j,

1
âˆš
n

ht(Î¾i)(cid:62)ht(Î¾j) âˆ’

1
âˆš
n

h0(Î¾i)(cid:62)h0(Î¾j) a.s.âˆ’âˆ’â†’ 0,

1
âˆš
n

h0(Î¾i)(cid:62)h0(Î¾j) dâˆ’â†’ N (0, 1)

by Law of Large Numbers (or more formally, Theorem 7.4) and Central Limit Theorem. In other
words, 1âˆš
n h0(Î¾i)(cid:62)h0(Î¾j) is distributed completely randomly, with no regard to the semantic simi-
larities of i and j. Likewise, the inner product in Eq. (35) is random, and the argmax is a uniform
sample.48 Therefore, in the NTK limit, Word2Vec gives random answers and achieves an accuracy of
1
|V|âˆ’3 .

E More Detailed Comparison with Deep Mean Field Limits

The key idea of previous works [5, 15, 34, 35, 46] proposing multilayer mean ï¬eld limits of MLPs
is to initialize each n Ã— n matrix W like WÎ±Î² â† F (uÎ±, vÎ²)/n for some function F and uÎ± âˆ¼
Z u, vÎ² âˆ¼ Z v sampled iid for each Î±, Î² âˆˆ [n], where Z u and Z v are some ï¬xed (wrt n) random
variables. If x is an activation with approximately iid coordinates distributed like random variable Z x,
then W x looks like (W x)Î± â‰ˆ EZv,Zx F (uÎ±, Z v)Z x by Law of Large Numbers (LLN), roughly iid
across Î±. This logic will in fact hold throughout training. For well-chosen (F, Z u, Z v), this does not
get stuck at initialization but this form of initialization is very unnatural. In Nguyen & Pham (2020),
this is adapted to iid initialization straightforwardly. For example, this includes WÎ±Î² â† N (0, 1)/n.
If x is as above, then (W x)Î± â†’ 0 by LLN because W is sampled independently from x at init and
has 0 mean. This means that preactivations every layer will vanish coordinatewise to 0, from which
itâ€™s easy to see that the gradients vanish where there are more than 2 hidden layers. Hence we say
that the function gets stuck at initialization. Contrast this 1/n scaling with the more typical 1/
n
scaling, i.e. WÎ±Î² â† N (0, 1)/
n, which is what we deal with here. On a technical level, their limit
calculation purely goes through LLN, whereas we need to wrestle with Central Limit effects (from
the 1/

n scaling) as well.

âˆš

âˆš

âˆš

48Here the randomness comes from initialization: the argmax is different for different random initializations,

but it is ï¬xed throughout training in the large width limit.

40

F Nuances of the Master Theorem

Remark F.1 (Partial derivative). The partial derivative in ZDot should be interpreted as follows. By a
simple inductive argument, Z x for every vector x in the program is deï¬ned uniquely as a deterministic
function Ï•( Ë†Z x1
) of some x1, . . . , xk in V or introduced by MatMul (notationally, we are
suppressing the possible dependence on limit scalars ËšÎ¸1, . . . , ËšÎ¸l). For instance, if in a program we
have A âˆˆ W, v âˆˆ V, y = Av, x = A(cid:62)y, then Z x = Ë†Z x + Ë†Z v, so Ï• is given by Ï•(a, b) = a + b.
Then

, . . . , Ë†Z xk

âˆ‚Z x/âˆ‚ Ë†Z xi def= âˆ‚iÏ•( Ë†Z x1

, . . . , Ë†Z xk

),

and âˆ‚Z x/âˆ‚ Ë†Z z def= 0 for any z (cid:54)âˆˆ {x1, . . . , xk}.

Note this deï¬nition depends on the precise way the program is written, not just on the underlying
mathematics. For example, if y, z âˆˆ V and x = Ï†(W (y + z)), then Z x = Ï†( Ë†ZW (y+z)) so that
âˆ‚Z x/âˆ‚ Ë†ZW y = âˆ‚Z x/âˆ‚ Ë†ZW z = 0. If instead, we have x = Ï†(W y+W z), then Z x = Ï†( Ë†ZW y + Ë†ZW z)
so that âˆ‚Z x/âˆ‚ Ë†ZW (x+y) = 0. However, in both cases,
Remark F.2 (Partial derivative expectation). The quantity E âˆ‚Zx
âˆ‚ Ë†ZW (cid:62) y is well deï¬ned if Z x is differen-
tiable in Ë†ZW (cid:62)y. However, even if this is not the case, e.g. if x = Î¸(W (cid:62)y) where Î¸ is the Heavyside
step function, we can still deï¬ne this expectation by leveraging Steinâ€™s lemma:
In ZDot, suppose {W (cid:62)yi}k
def= E Z yi
C âˆˆ RkÃ—k by Cij
and deï¬ne the vector b âˆˆ Rk by bi
(where C + denotes the pseudoinverse of C), then in ZDot we may set

i=1 are all elements of VW (cid:62) introduced before x. Deï¬ne the matrix
Z yj
Z x. If a = C +b

Ë™ZW (cid:62)x = (Z y + Z z) E Ï†(cid:48)( Ë†ZW (y+z)).

def= E Ë†ZW (cid:62)yi

âˆ‚ Ë†ZW (cid:62)yi
This deï¬nition agrees with the partial derivative expectation by Steinâ€™s lemma when the latter is well
deï¬ned. Theorem 7.4 holds with this broader deï¬nition of partial derivative expectation.

(39)

E âˆ‚Z x

Ïƒ2
W

= ai.

are, roughly speaking, functions whose weak derivatives are polyno-

Pseudo-Lipschitz functions
mially bounded.
Deï¬nition F.3. A function f : Rk â†’ R is called pseudo-Lipschitz of degree d if |f (x) âˆ’ f (y)| â‰¤
C(cid:107)x âˆ’ y(cid:107)(1 + (cid:80)k
i=1 |xi|d + |yi|d) for some C. We say f is pseudo-Lipschitz if it is so for any
degree.

Here are some basic properties of pseudo-Lipschitz functions:

â€¢ The norm (cid:107) Â· (cid:107) in Deï¬nition F.3 can be any norm equivalent to the (cid:96)2 norm, e.g. (cid:96)p, p â‰¥ 1,

norms. Similarly, (cid:80)k

i=1 |xi|d + |yi|d can be replaced by (cid:107)x(cid:107)d

p + (cid:107)y(cid:107)d

p, for any p â‰¥ 1.

â€¢ A pseudo-Lipschitz function is polynomially bounded.
â€¢ A composition of pseudo-Lipschitz functions of degrees d1 and d2 is pseudo-Lipschitz of

degree d1 + d2.

â€¢ A pseudo-Lipschitz function is Lipschitz on any compact set.

We adopt the following assumption for the Master Theorem Theorem 7.4.
Assumption F.4. Suppose

1. If a function Ï†(; âˆ’) : R0+l â†’ R with only parameter arguments is used in Moment, then Ï†

is continuous in those arguments.

2. Any other function Ï†(âˆ’; âˆ’) : Rk+l â†’ R with parameters (where k > 0) used in Nonlin or

Moment is pseudo-Lipschitz in all of its arguments (both inputs and parameters).

Statement 1 in Assumption F.4 essentially says that if we have scalars Î¸1, . . . , Î¸l in the program,
then we can produce a new scalar by applying a continuous function (a weaker restriction than a
pseudo-Lipschitz function) to them. Indeed, if Î¸1, . . . , Î¸l converge almost surely, then this new scalar
does too. In our setting, statement 1 is used to allow any loss function whose derivative is continuous.

Other versions of the Master Theorem can be found in [52], for example, versions where the we do
not assume any smoothness condition at all on the nonlinearities beyond that they be polynomially

41

bounded, in exchange for assuming whatâ€™s called a rank stability condition. This rank stability should
be generically true, but checking it rigorously is subtle, so we are content with the pseudo-Lipschitz
condition in this paper.

G A Rough Sketch of the Geometry of abc-Parametrizations

By the results of Section 3.2, the stable abc-parametrizations form a polyhedron deï¬ned by the
inequalities of Theorem 3.3. We call the polyhedron obtained by quotienting Eq. (5) the stable
polyhedron. In this section, we remark on some geometric properties of this polyhedron.

First, observe that the stable polyhedron is unbounded (thus, we say polyhedron instead of polytope).
Indeed, given any stable parametrization, for any l, we can set al â† al + Î¸, bl â† bl âˆ’ Î¸ for any
Î¸ â‰¥ 0 to obtain another stable parametrization. This corresponds decreasing the layer l learning rate,
so that as Î¸ â†’ âˆ, W l is not trained.

Second, by Theorem 3.4, the nontrivial parametrizations reside in two facets of the stable polyhedron.
These facets are unbounded for the same reason as above.

Next, we show that NTP (as well as ÂµP) is a vertex on the intersection of these two facets, and NTP
and ÂµP are connected by an edge.
Deï¬nition G.1. Consider a stable abc-parametrization of the MLP in Eq. (1). We say the body of the
t(Î¾) = Î˜(nâˆ’r)
MLP is uniformly updated if, for some training routine, time t â‰¥ 1, and input Î¾, âˆ†W l
for all l simultaneously, where r is as deï¬ned in Deï¬nition 3.2.

t xl

In the results of this section below, we assume Assumption H.22.
Proposition G.2. In a stable abc-parametrization, the MLP body is uniformly updated iff rl = r for
all l âˆˆ [L], where rl is as deï¬ned in Proposition 5.3.
Theorem G.3. In NTP, the MLP body is updated uniformly and W L+1 is both initialized and updated
maximally. Furthermore, at initialization, f0 converges in distribution49 to a Gaussian Process with
nonzero kernel. NTP is the unique (modulo Eq. (5)) stable abc-parametrization with both of these
properties.
Theorem G.4. For any r âˆˆ [0, 1/2], there is a unique (modulo Eq. (5)) stable abc-parametrization
with 1) that value of r and the property that 2) the MLP body is updated uniformly and W L+1 is both
initialized and updated maximally. We call this parametrization the Uniform Parametrization with
r-value r, denoted UPr. Its abc values are
1
2

I(l = 1) + r âˆ€l âˆˆ [L], aL+1 = 1/2;

bl = 1/2 âˆ’ r;

al = âˆ’

c = 0.

In particular, UP0 is ÂµP and UP1/2 is NTP. For r > 1/2, such a uniform parametrization is not
stable because W0 would need to be Î˜(nrâˆ’1), which would cause the initial GP to blow up. Thus,
geometrically, UPr, r âˆˆ [0, 1/2], form an edge of the stable polyhedron.
We can deï¬ne the uniform stable polyhedron to be the subset of the stable polyhedron corresponding to
parametrizations which update the MLP body uniformly. This is isomorphic to the stable polyhedron
when L = 1. Since stable abc-parametrizations with L = 1 has only 3 degrees of freedom, say
a1, a2, b2 while we ï¬x c = 0 (via Eq. (5)) and b1 = âˆ’a1, we can visualize the corresponding stable
polyhedron in 3D. However, the nontrivial parametrizations only reside in the boundary of this
polyhedron. Because of its unbounded nature, we can project its boundary in 2D and visualize it.
This is done in Fig. 5.

H Proofs of Main Results

H.1 Rigorous Statements of Main Results

Applicable Nonlinearities For technical reasons, in our main results we restrict our attention to
the canonical examples of nonlinearities: tanh and relu â€” or rather, a smooth version of relu called
gelu [24] common in transformer models [8]. More precisely,

49as is conventional in the machine learning literature, the convergence in distribution we mean here is really
over ï¬nite dimensional marginals, i.e. (f0(Î¾1), . . . , f0(Î¾k)) dâˆ’â†’ (Ëšf0(Î¾1), . . . , Ëšf0(Î¾k)) where Ëšf0 is the limit GP.

42

Figure 5: 2D Projection of the Boundary of the Uniform Stable Polyhedron (Equivalently, the
Boundary of the Stable Polyhedron for L = 1). Here, we label each facet and edge of the graph
with orange text to indicate the corresponding deï¬ning algebraic condition in the L = 1 case (as part
of the stable polyhedron, assuming c = 0 and b1 = âˆ’a1), and with black text to indicate the verbal
interpretation valid for all L (as part of the uniform stable polyhedron). We obtain the caricature in
Fig. 2 by taking the nontrivial subspace of the graph here and quotienting the two facets by their
respective points at inï¬nity. Explanation of some captions: GP limit means the training dynamics
amounts to training only the last layer in the inï¬nite-width limit, starting from a nonzero initial GP.
Body NTK limit means NTK dynamics except the last layer does not contribute to the NT kernel.

Deï¬nition H.1. Deï¬ne Ïƒ-gelu to be the function x (cid:55)â†’ 1

2 xerf(Ïƒâˆ’1x) + Ïƒ eâˆ’Ïƒâˆ’2x2

Ï€ + x
2 .

âˆš

2

Ïƒ-gelu is a smooth approximation of relu and is the integral of 1
2 (erf(Ïƒâˆ’1x) + 1) that is 0 at âˆ’âˆ. The
large Ïƒ is, the smoother Ïƒ-gelu is. As Ïƒ â†’ 0, Ïƒ-gelu converges to relu. We believe our results will
hold for generic nonlinearities, but making this precise is outside our scope here. (See Remark H.15
for some discussion).

Notations and Terminologies
Deï¬nition H.2 (Big-O Notation). Given a sequence of scalar random variables c = {cn âˆˆ R}âˆ
n=1,
we write c = Î˜(nâˆ’a) if there exist constants A, B such that Anâˆ’a â‰¤ |c| â‰¤ Bnâˆ’a for sufï¬ciently
large n, almost surely50. Given a sequence of random vectors x = {xn âˆˆ Rn}âˆ
n=1, we say x has
coordinates of size Î˜(nâˆ’a) and write x = Î˜(nâˆ’a) to mean the scalar random variable sequence
{(cid:112)(cid:107)xn(cid:107)2/n}n is Î˜(nâˆ’a). Similarly for the notations O(nâˆ’a), â„¦(nâˆ’a). We use the notations
Î˜Î¾(nâˆ’a), OÎ¾(nâˆ’a), â„¦Î¾(nâˆ’a) if the hidden constants A, B are allowed to depend on some object Î¾.
For brevity, we will often abuse notation and say c itself is a random variable or x itself is a random
vector.

Most often, the vector x will have â€œapproximately iidâ€ coordinates, so the notation x = Î˜(nâˆ’a) can
be interpreted intuitively to say x has coordinates of â€œstandard deviationâ€ Î˜(nâˆ’a), which justiï¬es
the name.
Deï¬nition H.3. An abc-parametrization is a joint parametrization of an MLP and the learning rate
speciï¬ed by the numbers {al, bl}l âˆª {c} as in Eq. (1). Below we will often say abc-parametrization
of an MLP for short, even though the parametrization affects the learning rate as well. A training
routine is a combination of learning rate Î·nâˆ’c, training sequence {(Î¾t, yt)}tâ‰¥0, and a loss function
L(f (Î¾), y) that is continuously differentiable in the prediction of the model f (Î¾).

Main Results We will mainly focus on stable parametrizations, deï¬ned below, which intuitively
means 1) the preactivations {hl}l and activations {xl}l have Î˜(1) coordinates at initialization, and
2) their coordinates and the logit f (Î¾) all stay O(1) (i.e. bounded independent of n) throughout the
course of SGD.51 Otherwise, they tend to âˆ with n, eventually going out of ï¬‚oating point range.

50Here almost surely means for almost every instantiation of c1, c2, . . ., i.e. it is with regard to the product
probability space generated by all of {cn}âˆ
n=1. In this paper, this probability space will be generated by random
initializations of a neural network at every width n. Very importantly, note the order of the qualiï¬ers: we are
saying for almost every instantiation of c1, c2, . . ., for large enough n, Anâˆ’a â‰¤ |c| â‰¤ Bnâˆ’a.

51but they may depend on training time and Î·; in particular, itâ€™s possible that they diverge with time

43

ğ‘1=âˆ’12ğ‘1=0ğ‘2+ğ‘2=1/2ğ‘2=0âˆ’ğ‘1=ğ‘2=ğ‘2ğ‘“0is a nonzero GPğœ‡PNTPğ‘2+ğ‘2+2ğ‘1=0ğ‘2+ğ‘1=0ğ‘2=1/2ğ‘2+ğ‘2+ğ‘1=1/2ğ‘Šğ¿+1updatedmaximally ğ‘Šğ¿+1init. maximallyğ‘Ÿ=0Feature Learningğ‘Ÿ>0Kernel RegimeNontrivialfeature learningÎ”ğ‘Šğ¿+1dominatesğ‘Š0ğ¿+1ğ‘Š0ğ¿+1dominatesÎ”ğ‘Šğ¿+1Boundary of Uniform Stable PolyhedronTrivialNontrivialLegendBody NTK limitIndeed, this is an acute and real problem common in modern deep learning, where ï¬‚oat16 is necessary
to train large models.
Deï¬nition H.4 (Stability). We say an abc-parametrization of an L-hidden layer MLP is stable if

1. For every nonzero input Î¾ âˆˆ X ,

0(Î¾), xl
hl

0(Î¾) = Î˜Î¾(1), âˆ€l âˆˆ [L],

and E f0(Î¾)2 = OÎ¾(1),

(40)

where the expectation is taken over the random initialization.

2. For any training routine, any time t â‰¥ 0, l âˆˆ [L], Î¾ âˆˆ X , we have

âˆ†hl

t(Î¾), âˆ†xl

t(Î¾) = Oâˆ—(1), âˆ€l âˆˆ [L],

and

ft(Î¾) = Oâˆ—(1),

where the hidden constant inside O can depend on the training routine, t, Î¾, and the initial
function values f0(X ).52

Recall from the main text,
Deï¬nition H.5. For any abc-parametrization, we write r for the quantity

r def= min(aL+1 + bL+1, 2aL+1 + c) + c âˆ’ 1 +

L
min
l=1

[2al + I(l = 1)] .

Intuitively, r is the exponent such that
t (Î¾) = Î˜Î¾(nâˆ’r). Thus, to avoid activation blowup, we want r â‰¥ 0; to perform feature learning,

For example, in NTP, r = 1/2, while in ÂµP, r = 0.
âˆ†xL
we want r = 0.
Theorem H.6 (Stability Characterization). Suppose Ï† is tanh or Ïƒ-gelu for sufï¬ciently small Ïƒ. An
abc-parametrization is stable iff all of the following are true (with intuitions in parentheses):

1. ((pre)activations at initialization are Î˜(1) and logits are O(1))

a1 + b1 = 0;

al + bl = 1/2, âˆ€l âˆˆ [2, L];

aL+1 + bL+1 â‰¥ 1/2.

(41)

2. (features donâ€™t blowup, i.e. âˆ†xl

t = O(1) for all l)
r â‰¥ 0.

3. (logits donâ€™t blow up during training, i.e. âˆ†W L+1

2aL+1 + c â‰¥ 1;

Here, r is as deï¬ned in Deï¬nition H.5.

t

t , W L+1
xL

0 âˆ†xL
aL+1 + bL+1 + r â‰¥ 1.

t = O(1))

(42)

(43)

t

turns out to be Î˜(nâˆ’(2aL+1+c)) and is correlated with xL

In Eq. (43), âˆ†W L+1
t = Î˜(1) such that
their product behaves according to Law of Large Numbers; the ï¬rst inequality says this should not
blow up. Similarly, W L+1
t = Î˜(nâˆ’r) and they will
= Î˜(nâˆ’(aL+1+bL+1)) and it turns out âˆ†xL
interact via Law of Large Numbers, so the second inequality says their product shouldnâ€™t blow up.

0

Our main results concern nontrivial parametrizations:
Deï¬nition H.7 (Nontriviality). We say an abc-parametrization of an L-hidden layer MLP is trivial if
for every training routine, ft(Î¾) âˆ’ f0(Î¾) a.s.âˆ’âˆ’â†’ 0 for any time t â‰¥ 1 and input Î¾ âˆˆ X (i.e. the function
does not evolve in the inï¬nite-width limit). We say the parametrization is nontrivial otherwise.

Theorem H.8 (Nontriviality Characterization). Suppose Ï† is tanh or Ïƒ-gelu for sufï¬ciently small Ïƒ.
A stable abc-parametrization is nontrivial iff aL+1 + bL+1 + r = 1 or 2aL+1 + c = 1.
Deï¬nition H.9 (Feature Learning). We say an abc-parametrization of an L-hidden layer MLP admits
feature learning in the lth layer if there exists some training routine such that

âˆ†xl

t(Î¾) = â„¦âˆ—(1)

(44)

52For e.g. the NTK limit, f0 is a GP, so that we should expect the bounds on âˆ†hl

t(Î¾), âˆ†xl

t(Î¾) to depend on

f0.

44

for some t â‰¥ 0, Î¾ âˆˆ X , where the hidden constant inside â„¦ can depend on the training routine, t, Î¾,
and the initial function values f0(X ). We say the parametrization admits feature learning if it does
so in any layer.

We say the parametrization ï¬xes the lth layer features if for all training routine,

(cid:107)âˆ†xl

t(Î¾)(cid:107)2/n a.s.âˆ’âˆ’â†’ 0

for all t â‰¥ 0, Î¾ âˆˆ X . We say the parametrization ï¬xes all features if it does so in every layer.

We make similar deï¬nitions as above replacing feature with prefeature and xl with hl.

Note that the probabilistic nature of â„¦âˆ—(1) means that no feature learning does not imply ï¬xing all
features (because âˆ†xl
t(Î¾) can just ï¬‚uctuate wildly between 0 and inï¬nity), but we will see that in the
context of nontrivial stable abc-parametrizations, this is true.
Remark H.10. We note that this is a rather weak notion of â€œfeature learningâ€, as we only require that
the embedding xL
t (Î¾) changes from its initialization for some scenario, rather than, say for generic
scenarios; nor do we speak at all about the â€œqualityâ€ of feature learning, e.g. how it helps downstream
tasks. But our proofs (see Appendix H.7) will show that â€œsome scenarioâ€ in fact implies much more
general scenarios. In addition, we argue that such formal weakness is more than compensated by
our experiments, which show that inï¬nite-width limits of feature learning (in the sense deï¬ned here)
abc-parametrized MLPs outperform ï¬nite MLPs and their NTK limits on tasks (namely, Word2Vec
and few-shot learning) where feature learning, in the colloquial notion of the phrase, is crucial.

A somewhat stronger notion of feature learning is that the feature kernel evolves. This is, for example,
essential for linear transfer learning such as in self-supervised learning of image data.
Deï¬nition H.11 (Feature Kernel Evolution). We say an abc-parametrization of an L-hidden layer
MLP evolves the lth layer feature kernel if there exists some training routine such that
t(Î¶)/n âˆ’ xl

0(Î¶)/n = â„¦âˆ—(1)

t(Î¾)(cid:62)xl
xl

0(Î¾)(cid:62)xl

for some t â‰¥ 0, Î¾, Î¶ âˆˆ X , where the hidden constant inside â„¦ can depend on the training routine, t,
Î¾, Î¶, and the initial function values f0(X ). We say the parametrization evolves feature kernels if it
does so in any layer.

We say the parametrization ï¬xes the lth layer feature kernel if for all training routine,

t(Î¾)(cid:62)xl
xl

t(Î¶)/n âˆ’ xl

0(Î¾)(cid:62)xl

0(Î¶)/n a.s.âˆ’âˆ’â†’ 0,

as n â†’ âˆ,

for all t â‰¥ 0, Î¾, Î¶ âˆˆ X . We say the parametrization ï¬xes all feature kernels if it does so in every layer.

We make similar deï¬nitions as above replacing feature with prefeature and xl with hl.

Intuitively, for a stable parametrization, feature kernel evolution should imply feature learning (one
can see the contrapositive easily). In fact, we shall see below they are equivalent notions.

On the other hand, from the NTK example, we know certain limits can be described entirely through
kernel gradient descent with some kernel. Appropriately, we make the following deï¬nition.
Deï¬nition H.12 (Kernel Regime). We say an abc-parametrization of an L-hidden layer MLP is in
kernel regime if there exists a positive semideï¬nite kernel K : X 2 â†’ R such that for every training
routine, the MLP function evolves under kernel gradient descent, i.e. there exist random variables
Ëšft(Î¾) for each time t â‰¥ 0 and input Î¾ âˆˆ X such that, as n â†’ âˆ,53

{ft(Î¾)}tâ‰¤T,Î¾âˆˆX

dâˆ’â†’ {Ëšft(Î¾)}tâ‰¤T,Î¾âˆˆX ,

âˆ€T â‰¥ 1,

where dâˆ’â†’ denotes convergence in distribution, and

Ëšft+1(Î¾) = Ëšft(Î¾) âˆ’ Î·K(Î¾, Î¾t)L(cid:48)(Ëšft(Î¾t), yt),

âˆ€t â‰¥ 0.

(45)

Observe that, in kernel regime, Ëšft(Î¾) is deterministic conditioned on Ëšf0(Î¾), as evident inductively
from Eq. (45). For example, in the NTK limit, {Ëšf0(Î¾) : Î¾ âˆˆ X } is a nontrivial Gaussian Process (GP),
but the function evolution conditioned on this GP is deterministic.

All of the concepts deï¬ned above are related to each other by the following theorem.

53Here because we want to avoid topological issues arising for convergence in distribution of inï¬nite sequences,
we only require convergence in distribution jointly in all Î¾ âˆˆ X and time t below some cutoff T for every ï¬nite
T .

45

Theorem H.13 (Classiï¬cation of abc-Parametrizations). Suppose Ï† is tanh or Ïƒ-gelu for sufï¬ciently
small Ïƒ. Consider a nontrivial stable abc-parametrization of an L-hidden layer MLP. Then

1. The following are equivalent to r = 0

(a) feature learning
(b) feature learning in the Lth layer
(c) feature kernels evolution
(d) feature kernel evolution in the Lth layer
(e) prefeature learning
(f) prefeature learning in the Lth layer
(g) prefeature kernels evolution
(h) prefeature kernel evolution in the Lth layer

2. The following are equivalent to r > 0

(a) kernel regime
(b) ï¬xes all features
(c) ï¬xes features in the Lth layer
(d) ï¬xes all feature kernels
(e) ï¬xes feature kernel in the Lth layer
(f) ï¬xes all prefeatures
(g) ï¬xes prefeatures in the Lth layer
(h) ï¬xes all prefeature kernels
(i) ï¬xes prefeature kernel in the Lth layer

3. If there is feature learning or feature kernel evolution or prefeature learning or prefeature
kernel evolution in layer l, then there is feature learning and feature kernel evolution and
prefeature learning and prefeature kernel evolution in layers l, . . . , L.

4. If r = 0, then for all Î¾ âˆˆ X , f0(Î¾) a.s.âˆ’âˆ’â†’ 0 and ft(Î¾) a.s.âˆ’âˆ’â†’ Ëšft(Î¾) for some deterministic Ëšft(Î¾).

However, the converse is not true.

5. If r > 0, aL+1 + bL+1 + r > 1 and 2aL+1 + c = 1, then we have the Neural Network-

Gaussian Process limit.

In particular, Statement 4 implies that feature learning, at least in our context, is incompatible with
Bayesian, distributional perspectives of neural network limits, such as the NNGP limit.

The characterization above then trivially implies the following dichotomy.
Corollary H.14 (Dynamical Dichotomy). For Ï† being tanh or Ïƒ-gelu for sufï¬ciently small Ïƒ, a
nontrivial stable parametrization of an L-hidden layer MLP either admits feature learning or is in
kernel regime, but not both.
Remark H.15 (The Role of the Ï† Assumption). The dependence on Ï† being tanh or Ïƒ-gelu for
sufï¬ciently small Ïƒ is only needed to explicitly construct a training routine that leads to feature
learning for r = 0. We expect this should be true for generic Ï†, but we leave this for future work. We
expand more on how the Ï† assumption is used below.

To calculate the inï¬nite width limit of any abc-parametrization rigorously, we only need the nonlin-
earity to have a polynomially bounded 2nd derivative (or more generally pseudo-Lipschitz, so as to
apply the Master Theorem). The speciï¬c choice of tanh or gelu is needed to prove the part of the
Dynamical Dichotomy that says a limit cannot be simultaneously in kernel regime and in feature
learning regime (which, e.g. is not true for linear activation). To do so, we use Properties H.44 and
H.47 of tanh and gelu, expanded below. This is really for a more convenient proof, but we believe a
more general approach should work for general nonlinearities. Our argument is as follows (this is
also overviewed in the start of Appendix H.7): If r = 0, we show that a sufï¬ciently small nonzero
learning rate (scaled with width in the corresponding parametrization) in 1 SGD step 1) induces a
change in the features but 2) the resulting change in the NN output is not linear in the loss derivative
Ï‡. 1) means itâ€™s feature learning, and 2) means itâ€™s not in kernel regime. This argument involves
showing certain derivatives of certain expectations with respect to learning rate is positive. In the
case of tanh and gelu, this is checked explicitly using Properties H.44 and H.47.

46

Remark H.16. The equivalence between kernel regime and ï¬xed feature kernel implies that linear
transfer learning is trivialized in any kernel regime limit. This is where the classiï¬er layer of the
pretrained network is discarded and a new one (potentially outputting to a new output space) is trained
on top of the body of the pretrained network. But we can in fact say more: any nonlinear transfer
learning, where we replace the classiï¬er layer with a neural network instead of a linear layer, is
trivialized as well. In addition, linear or nonlinear transfer learning has no effect even if we ï¬netune
the entire network, instead of just the new classiï¬cation network. The intuitive reason for this is
that, as discussed in Appendix B, the effect of âˆ†xL(Î¾) on the output of the MLP is solely through
the interaction with W L+1
. If W L+1, W L+2, . . . , are sampled anew, then this effect vanishes. We
formalize this below.

0

Theorem H.17 (Kernel Regime Limit Trivializes Transfer Learning). Suppose f is an L-hidden-layer
MLP54 in a stable kernel regime parametrization. Let A and B be two training routines.55
For any T, t â‰¥ 0,56 we deï¬ne a network57 gT ;t as follows. Train f on A for T steps to obtain fT . Then
discard W L+1 in fT and extend the body of fT into an M -hidden-layer MLP g, where M â‰¥ L.58
Parametrize and initialize the new weights of g according to any stable abc-parametrization that
extends the parametrization of f . Train g on B for t steps to obtain gT ;t.
Then

1. (Finetuning the whole network) As n â†’ âˆ, for any Î¾ âˆˆ X and T, t â‰¥ 0,

gT ;t(Î¾) âˆ’ g0;t(Î¾) a.s.âˆ’âˆ’â†’ 0.

2. (Training only the classiï¬er) The above is true even if we deï¬ne gT ;t by only training the

new weights W L+1, . . . , W M in g.

The Organization for the Proof of Our Main Results Above
Deï¬nition H.18. Below, we will abbreviate abc-parametrization of an L-layer MLP to just
parametrization. We will call parametrizations satisfying the conditions of Theorem H.6 pseudostable
while we try to prove Theorem H.6 (which, in this terminology, says stability and pseudostability are
equivalent).

We ï¬rst characterize stability at initialization and prove Eq. (40) holds iff Eq. (41) (Appendix H.2).
Then, we describe the Tensor Program encoding the SGD of an MLP, assuming its parametrization
is pseudostable. The Master Theorem then naturally lets us calculate its inï¬nite-width limit. We
then divide into the case of r > 0 and r = 0. In the former case, we show the inï¬nite-width limit is
described by kernel gradient descent as in Eq. (45). In the latter case, we construct a training routine
where feature learning occurs and where the limit is not given by kernel gradient descent for any
kernel. Finally, in Appendix H.8, we combine all of our analyses to prove the main results in this
section.

H.2 Stability at Initialization

In this section, we characterize stability at initialization, which will form a foundation for our later
results.
Theorem H.19. Assume Ï† is not zero almost everywhere. For any parametrization, Eq. (40) holds iff
Eq. (41) holds, i.e. the following are equivalent

1. For every nonzero input Î¾ âˆˆ X ,

0(Î¾), xl
hl

0(Î¾) = Î˜Î¾(1), âˆ€l âˆˆ [L],

and E f0(Î¾)2 = OÎ¾(1),

where the expectation is taken over the random initialization.

54the â€œpretrained networkâ€
55the â€œpretraining datasetâ€ and the â€œï¬netuning datasetâ€
56the â€œpretraining timeâ€ and â€œï¬netuning timeâ€
57the â€œï¬netuned networkâ€
58If M = L, then this is linear transfer learning where we replace just the last layer of f ; otherwise, itâ€™s

nonlinear transfer learning.

47

2. a1 + b1 = 0;

al + bl = 1/2, âˆ€l âˆˆ [2, L];

aL+1 + bL+1 â‰¥ 1/2.

Proof. Fix an input Î¾ (cid:54)= 0. Here, because we focus on initialization, we will suppress the time 0
subscript and Î¾ dependence of hl, xl to mean t = 0, applied to Î¾.

Obviously, h1 = W 1Î¾ is a Gaussian vector with N (0, nâˆ’(a1+b1)(cid:107)Î¾(cid:107)2) coordinates, so h1 = Î˜Î¾(1)
iff a1 + b1 = 0. Assume a1 + b1 = 0. By Law of Large Numbers, 1
)2 where
Z h1
= N (0, (cid:107)Î¾(cid:107)2). Since Ï† is not almost everywhere zero and Î¾ (cid:54)= 0, this expectation is nonzero so
that x1 = Î˜Î¾(1).
We construct the following Tensor Program: the lone initial vector is h1, the initial matrices are
(cid:99)W l, 2 â‰¤ l â‰¤ L, and initial scalars Î¸l
Î±Î² âˆ¼
N (0, 1/n). Mathematically, we will represent W l = Î¸l(cid:99)W l. The program is then given by

def= n1/2âˆ’(al+bl). We sample h1

n (cid:107)x1(cid:107)2 a.s.âˆ’âˆ’â†’ E Ï†(Z h1

Î± âˆ¼ N (0, (cid:107)Î¾(cid:107)2) and (cid:99)W l

xl = Ï†(hl), âˆ€l âˆˆ [L],

Ë†hl = (cid:99)W lxlâˆ’1, hl = Î¸l

Ë†hl, âˆ€l âˆˆ [2, L],

= Z Ë†hl

= N (0, E Ï†(Z hlâˆ’1

)2 > 0) for all l â‰¤ L. By the Master Theorem, 1
)2 so this implies hl, xl = Î˜Î¾(1) for all l â‰¤ L as desired.

where we used Nonlin, MatMul, and Nonlin (with parameter Î¸l).
Suppose al + bl = 1/2 (i.e. Î¸l = 1) for all 2 â‰¤ l â‰¤ L. Then, Z hl
for each l â‰¤ L. Because Ï† is not everywhere zero, this inductively implies E(Z hl
also E(Z xl
E(Z xl
Conversely, suppose m is the smallest l â‰¥ 2 such that al + bl (cid:54)= 1/2. Then by the above reasoning,
Ë†hm = Î˜Î¾(1) so hm = Î˜Î¾(n1/2âˆ’(al+bl)) is either blowing up to âˆ or shrinking to 0 with n. This
shows that hl, xl = Î˜Î¾(1) for all l â‰¤ L iff a1 + b1 = 0 and al + bl = 1/2 for all 2 â‰¤ l â‰¤ L.
Finally, if a1 + b1 = 0 and al + bl = 1/2 for all 2 â‰¤ l â‰¤ L, then we see E f0(Î¾)2 =
(n1/2âˆ’(aL+1+bL+1))2 E (cid:107)Z xL
(cid:107)2/n. For large n, this is Î˜Î¾((n1/2âˆ’(aL+1+bL+1))2) and is OÎ¾(1) iff
aL+1 + bL+1 â‰¥ 1/2.

)2)
)2 > 0 (and so
n (cid:107)xl(cid:107)2 a.s.âˆ’âˆ’â†’

n (cid:107)hl(cid:107)2 a.s.âˆ’âˆ’â†’ E(Z hl

)2 and 1

Deï¬nition H.20. We say a parametrization is initialization-stable if it satisï¬es Eq. (40) (or equiva-
lently, Eq. (41)).

H.3 Program Setup

In the next section, we construct the Tensor Program that encodes the training of an L-hidden layer
MLP under an abc-parametrization. Here we ï¬rst describe the initial matrices, vectors, and scalars of
the program, along with necessary notations.

We ï¬rst remark on a simpliï¬cation we will make to streamline the proof.

t

t

0

0

0

vs âˆ†W L+1

By construction, W L+1

is at least as large as âˆ†W L+1

The Size of W L+1
= Î˜(nâˆ’(aL+1+bL+1)). If xL
t (Î¾) = Î˜(1)
as in a stable parametrization, then âˆ†W L+1
= Î˜(nâˆ’(2aL+1+c)). Therefore, if aL+1 + bL+1 â‰¤
2aL+1 + c, then W L+1
will stay the same order (in
t
terms of n) for all t. If the reverse inequality is true, then W L+1
for t â‰¥ 1.
This in particular implies that the gradients at time 0 is smaller than gradients at subsequent times.
For example, we can take aL+1 + bL+1 â†’ âˆ while ï¬xing 2aL+1 + c, in which case W L+1
= 0 and
the weight gradients at initialization are all 0 except for that of W L+1. One can thus think of this as a
â€œlagâ€ in the training dynamics for 1 step.
Assumption H.21. For clarity of the proof, we will assume aL+1 + bL+1 â‰¤ 2aL+1 + c, i.e. W L+1
stays the same order for all t. The case of aL+1 + bL+1 > 2aL+1 + c, corresponding to a 1-step â€œlagâ€
as explained above, can be dealt with similarly. We will remark whenever this requires some subtlety.

is smaller than W L+1

, so that W L+1

0

0

t

t

t

For the construction of the program and the application of the Master Theorem, we will also assume
the following for the rest of this paper.
Assumption H.22. Ï†(cid:48) is pseudo-Lipschitz and not almost everywhere zero.

48

Initial Matrices, Vectors, Scalars We will assume the parametrization is initialization-stable. For
ease of presentation, we also assume the input dimension d = 1.

1. Initial matrices: W 2
2. Initial vectors: input layer matrix W 1

0 , . . . , W L

0 , sampled like (W l

0)Î±Î² âˆ¼ N (0, 1/n).

0 âˆˆ RnÃ—1 and normalized output layer matrix (cid:99)W L+1

0

def=

W L+1
0

naL+1+bL+1 âˆˆ R1Ã—n, sampled like (W 1

0 )Î±, ((cid:99)W L+1

0

)Î± âˆ¼ N (0, 1).

3. Initial scalars: We deï¬ne the following scalars (where we explain the intuition in parenthesis).

The reader can skip this part on a ï¬rst read but come back when referred to.
(a) (n times the scale of coordinates of âˆ†W l

t ) For l â‰¥ 2, deï¬ne
def= nâˆ’(aL+1+bL+1+câˆ’1+2al)

Î¸W l
(b) (scale of coordinates of âˆ†W 1

t and âˆ†h1

t ) Deï¬ne

(c) (scale of coordinates of âˆ†W L+1

Î¸1 = Î¸W 1
)

def= nâˆ’(aL+1+bL+1+c+2a1)

(d) (scale of âˆ†hl

t
Î¸L+1 = Î¸W L+1
t) For l âˆˆ [L], deï¬ne
def= max
mâ‰¤l

t and âˆ†xl
Î¸hl = Î¸xl = Î¸l

def= nâˆ’2aL+1âˆ’c

Î¸W m = max(Î¸W l , Î¸lâˆ’1)

(46)

= nâˆ’(aL+1+bL+1+câˆ’1+minl

m=1(2am+I(m=1)))

Note that Î¸L = nâˆ’r with r deï¬ned in Deï¬nition H.5.

(e) (scale of W L+1

t

)

Î¸f

def= nâˆ’(aL+1+bL+1)

(f) (convenience scalars)

Î¸xlâˆ’1/hl = Î¸xlâˆ’1/Î¸hl
Î¸W l/hl = Î¸W l /Î¸hl
Î¸W lxlâˆ’1/hl = Î¸W l Î¸xlâˆ’1/Î¸hl
Î¸L+1/f = Î¸L+1/Î¸f

L+1 = nÎ¸L+1 = n1âˆ’2aL+1âˆ’c
Î¸(cid:48)
Lf = nÎ¸LÎ¸f = n1âˆ’(r+aL+1+bL+1)
Î¸(cid:48)

(g) Depending on the the value of aL+1 + bL+1, we will also construct the values of f at

initialization as initial scalars. See Appendix H.4.1 for an explanation.

By our assumption that aL+1 + bL+1 â‰¤ 2aL+1 + c, the pseudostability inequalities of Theorem H.6
imply all of these Î¸s either converge to 0 or stay constant at 1. This means that, assuming appropriate
regularity conditions on the nonlinearities and rank stability, we can apply the Master Theorem (if Î¸
blows up to âˆ then we canâ€™t do that).

Notations We use := to more clearly denote assignment happening in the program, as opposed to
mathematical equality. To clearly demonstrate the application of Nonlin, we will also freely introduce
function symbols Î¨ to put things into Nonlin form.

In the program, for each z âˆˆ {xl, hl}l, we will construct vectors
Preview of Names for Vectors
Î´zt(Î¾) to mathematically represent Î¸âˆ’1
z (zt(Î¾) âˆ’ ztâˆ’1(Î¾)) (intuition: change in z scaled to have Î˜(1)
coordinates). Similarly, for w âˆˆ {W L+1, W 1}, we will construct Î´wt to mathematically represent
Î¸âˆ’1
w (wt âˆ’ wtâˆ’1) (intuition: change in w scaled to have Î˜(1) coordinates). Then, mathematically,
zt(Î¾) = ztâˆ’1(Î¾) + Î¸zÎ´zt(Î¾), wt = wtâˆ’1 + Î¸wÎ´wt.
We will also construct dz to mathematically represent Î¸âˆ’1
have Î˜(1) coordinates). For weight changes, we have the following identity

f âˆ‡zf (intuition: gradient âˆ‡zf scaled to

tâˆ’1 = âˆ’Î·nâˆ’cÏ‡tâˆ’1nâˆ’2al Î¸f dhl

tâˆ’1xlâˆ’1(cid:62)

tâˆ’1 = âˆ’Î·Ï‡tâˆ’1Î¸W l

W l

t âˆ’ W l
and for l = 1,

1
n

tâˆ’1xlâˆ’1(cid:62)
hl
tâˆ’1 ,

âˆ€l âˆˆ [2, L], (47)

W l

t âˆ’ W l

tâˆ’1 = âˆ’Î·nâˆ’cÏ‡tâˆ’1nâˆ’2al Î¸f dhl

tâˆ’1Î¾(cid:62)

tâˆ’1 = âˆ’Î·Ï‡tâˆ’1Î¸W l hl

tâˆ’1Î¾(cid:62)

tâˆ’1.

(48)

49

H.4 Program Construction

Here we construct the Tensor Program encoding the SGD of an MLP. We separately describe the ï¬rst
forward and backward passes followed by the later forward and backward passes.

H.4.1 First Forward Pass

0 Î¾ âˆˆ Rn via Nonlin (as Î¨(W 1
For every Î¾ âˆˆ X , we compute h1
0(Î¾) := W 1
tion by Î¾), and we construct the following vectors via Nonlin and MatMul

0 ; Î¾), where Î¨ is multiplica-

0(Î¾) := Ï†(hl
xl

0(Î¾)) âˆˆ Rn,

hl+1
0

(Î¾) := W l+1

0 xl

0(Î¾) âˆˆ Rn,

for l = 1, . . . , L âˆ’ 1,

(49)

Function Output The ï¬rst output is f0(Î¾) = W L+1
slightly differently.

0

xL
0 (Î¾), but we will deï¬ne f0(Î¾) in the program

Case when aL+1 + bL+1 > 1/2 Then f0(Î¾) a.s.âˆ’âˆ’â†’ 0 for all Î¾ âˆˆ X . In the program, we will
construct f0(Î¾) as an initial scalar mathematically deï¬ned by W L+1

xL
0 (Î¾).5960

0

If aL+1 + bL+1 = 1/2, then f0(Î¾) converges to a nontrival
Case when aL+1 + bL+1 = 1/2
Gaussian via CLT [49], so we will condition on f0(Î¾) for all Î¾ âˆˆ X . Given values g(Î¾) âˆˆ R for all
Î¾ âˆˆ X , let E be the event that f0(Î¾) = 1âˆš
xL
0 (Î¾) equals g(Î¾) for all Î¾ âˆˆ X . The distribution
of (cid:99)W L+1
0

conditioned on E is given by

n (cid:99)W L+1

0

(cid:99)W L+1
0
is an iid copy of (cid:99)W L+1

âˆš

d=E

nX +g + Î (cid:102)W L+1

0

0

0

0 (Î¾) : Î¾ âˆˆ X }. Here X + denotes the pseudo-inverse of X.

, g âˆˆ RX is the vector of {g(Î¾) : Î¾ âˆˆ X }, X âˆˆ RX Ã—n has
where (cid:102)W L+1
xL
0 (Î¾) as rows, and Î  is the orthogonal projection into the orthogonal complement of the space
spanned by {xL
By standard formulas for pseudo-inverse and orthogonal projection, we can write X + =
n X (cid:62)(XX (cid:62)/n)+, Î  = I âˆ’ 1
1
Let Î£ def= XX (cid:62)/n and Î³ def= (X (cid:102)W L+1
1âˆš
n X (cid:62)Î£+g.

n X (cid:62)(XX (cid:62)/n)+X.

/n). Then Î (cid:102)W L+1

âˆ’ X (cid:62)Î£+Î³, and

= (cid:102)W L+1
0

nX +g =

âˆš

0

0

By the Master Theorem, Î³ a.s.âˆ’âˆ’â†’ 0 because (cid:102)W L+1
is independent from X, and Î£ a.s.âˆ’âˆ’â†’ ËšÎ£ for some
PSD matrix ËšÎ£. At this point in the program, all scalars we used (like Î¾) are constant with n and
can be absorbed into nonlinearities. By the rank stability property of any program without scalars
[52], the rank of Î£ is ï¬xed for large enough n, almost surely, so Î£+ a.s.âˆ’âˆ’â†’ ËšÎ£+ by the continuity of
pseudo-inverse on ï¬xed rank matrices.
We will now replace (cid:99)W L+1

0

0

(cid:99)W L+1
E

(cid:19)

in the program with
(cid:18)
Î£+ g
âˆš
n
(cid:17)

def= X (cid:62)

(cid:16)
Î£+ gâˆš
n

+ (cid:102)W L+1
0

âˆ’ X (cid:62) (cid:0)Î£+Î³(cid:1)

n , Î£+Î³ a.s.âˆ’âˆ’â†’ 0, we have Z (cid:99)W L+1

and (Î£+Î³) are ï¬nite dimensional and formally consid-
constructed using Nonlin, where
ered (collections of) scalars involved as coefï¬cients for linear combination of rows of X. Since
E = Z (cid:102)W L+1
Î£+ gâˆš
. Intuitively, this means that, even after conditioning
on f0 = g, the conditional distribution of (cid:102)W L+1
We can then proceed exactly as in the case when aL+1 + bL+1 > 1/2, with (cid:99)W L+1
(cid:102)W L+1
0

is practically the same as the original distribution.
taking the role of

. The program then encodes the evolution of f conditioned on f0(Î¾) = g(Î¾), âˆ€Î¾ âˆˆ X .61

E

0

0

59It is completely OK to deï¬ne an initial scalar using randomness from other parts of the program, as long as

this scalar converges almost surely to a deterministic limit

60We cannot deï¬ne it using a Moment instruction because, intuitively, the mechanism of this convergence is

through CLT, not Law of Large Numbers.

61Formally, we can also have {g(Î¾) : Î¾ âˆˆ X } as initial scalars, but since they are ï¬xed with n, they can be

absorbed into the Nonlin that deï¬nes (cid:99)W L+1

E

.

50

Assumption H.23. For the above reason, we will assume aL+1 + bL+1 > 1/2, and remark whenever
the case aL+1 + bL+1 = 1/2 involves subtleties.

H.4.2 First Backward Pass

Next, we write the backward pass

dxL
dhl
dxlâˆ’1
0

0 (Î¾) := (cid:99)W L+1
0
0(Î¾) (cid:12) Ï†(cid:48)(hl
0(Î¾) := dxl
0 dhl
(Î¾) := W l(cid:62)
0(Î¾)

0(Î¾))

where, recall, dz mathematically equals Î¸âˆ’1

f âˆ‡zf .

For Î¾ = Î¾0 and its label y0, we deï¬ne the ï¬rst loss derivative as

where the convergence is because L(cid:48) is continuous by assumption.

Ï‡0 := L(cid:48)(f0(Î¾0), y0) a.s.âˆ’âˆ’â†’ ËšÏ‡0(Î¾) = L(cid:48)(0, y0)

We also deï¬ne

Î´W L+1
1

:= âˆ’Î·Ï‡0xL

0 (Î¾0)

to represent the (normalized) change in W L+1 due to the ï¬rst gradient step.

H.4.3 tth Forward Pass, t â‰¥ 1

Overview We iteratively deï¬ne Î´zt(Î¾) to mathematically represent Î¸âˆ’1
z âˆˆ {xl, hl}l. Then we eventually set

z (zt(Î¾) âˆ’ ztâˆ’1(Î¾)), for

zt(Î¾) := z0(Î¾) + Î¸zÎ´z1(Î¾) + Â· Â· Â· + Î¸zÎ´zt(Î¾).

Likewise, we will deï¬ne Î´W L+1
t
the program, we will not directly use W L+1

so that W L+1

= Î¸f (cid:99)W L+1
but instead use

0

t

(cid:99)W L+1
t

:= (cid:99)W L+1
0

t
+ Î¸L+1/f (Î´W L+1

1

+ Î¸L+1(Î´W L+1

1

+ Â· Â· Â· + Î´W L+1

t

). In

+ Â· Â· Â· + Î´W L+1

t

)

(50)

where Î¸L+1/f = Î¸L+1/Î¸f . Mathematically, (cid:99)W L+1
Recall we shorthand zt = zt(Î¾t) for all z âˆˆ {xl, hl, dxl, dhl}l âˆª {f, Ï‡}.

f W L+1

= Î¸âˆ’1

.

t

t

The Construction of (Pre)Activations We start with h = h1: By Eq. (48), we have

Î´ht(Î¾) := âˆ’Î·Ï‡tâˆ’1Î¾(cid:62)

tâˆ’1Î¾dhtâˆ’1 = Î¨(dhtâˆ’1; Î¾(cid:62)

tâˆ’1Î¾, Î·Ï‡tâˆ’1).

(Notationally, recall we freely introduce function symbols Î¨ to clarify the way we apply Nonlin).
For higher layers, if h = hl, x = xlâˆ’1, and W = W l, then h = W x. By Eq. (47), we have,
mathematically,

Î¸hÎ´ht(Î¾) = Î¸xWtâˆ’1Î´xt(Î¾) + (Wt âˆ’ Wtâˆ’1)xt(Î¾)

(cid:32)

= Î¸x

W0Î´xt(Î¾) +

tâˆ’1
(cid:88)

(Ws âˆ’ Wsâˆ’1)Î´xt(Î¾)

(cid:33)

+ (Wt âˆ’ Wtâˆ’1)xt(Î¾)

(cid:32)

s=1

= Î¸x

W0Î´xt(Î¾) âˆ’ Î·Î¸W

tâˆ’1
(cid:88)

s=1

Ï‡sâˆ’1

x(cid:62)
sâˆ’1Î´xt(Î¾)
n

(cid:33)

dhsâˆ’1

âˆ’ Î·Ï‡tâˆ’1Î¸W

x(cid:62)
tâˆ’1xt(Î¾)
n

dhtâˆ’1

Recall Î¸x/h = Î¸âˆ’1
struct

h Î¸x, Î¸W/h = Î¸âˆ’1

h Î¸W , Î¸W x/h = Î¸âˆ’1

h Î¸W Î¸x. With cs denoting x(cid:62)

s Î´xt(Î¾)
n

, we con-

Î´ht(Î¾) := Î¸x/hW0Î´xt(Î¾) âˆ’ Î·Î¸W x/h

tâˆ’1
(cid:88)

s=1

Ï‡sâˆ’1csâˆ’1dhsâˆ’1 âˆ’ Î·Ï‡tâˆ’1Î¸W/hctâˆ’1dhtâˆ’1

= Î¨(W0Î´xt(Î¾), dh0, . . . , dhtâˆ’1; Î·, Î¸x/h, Î¸W x/h, Î¸W/h, {cs, Ï‡s}tâˆ’1
s=0)

51

If x = xl, h = hl, then x = Ï†(h), and (using Î¸x = Î¸h (Eq. (46))),

Î´xt(Î¾) := Î¸âˆ’1

h (Ï†(htâˆ’1(Î¾) + Î¸hÎ´ht(Î¾)) âˆ’ Ï†(htâˆ’1(Î¾)))

= Î¨(htâˆ’1(Î¾), Î´ht(Î¾); Î¸h)

(51)

where Î¨ is precisely the difference quotient for the function Ï†.62

The Function Outputs We do not construct ft(Î¾) directly, but rather through scalars Î´ft(Î¾) =
ft(Î¾) âˆ’ ftâˆ’1(Î¾), so that

ft(Î¾) := f0(Î¾) + Î´f1(Î¾) + Â· Â· Â· + Î´ft(Î¾).

Mathematically, Î´ft(Î¾) = Î¸L+1Î´W L+1
differently in the program:

t

t (Î¾) + W L+1
xL

tâˆ’1 Î¸LÎ´xL

t (Î¾), but we shall write it slightly

Î´ft(Î¾) := Î¸(cid:48)

Î´W L+1
t
n
Lf = nÎ¸LÎ¸f and (cid:99)W L+1

L+1

xL
t (Î¾)

+ Î¸(cid:48)

Lf

(cid:99)W L+1
tâˆ’1 Î´xL
n

t (Î¾)

where Î¸(cid:48)

L+1 = nÎ¸L+1, Î¸(cid:48)

tâˆ’1 is constructed in Eq. (50).

H.4.4 tth Backward Pass, t â‰¥ 1

In the last layer, we construct

For each l = L, . . . , 1 for dhl and l = L, . . . , 2 for dxlâˆ’1, we also calculate

dxL

t (Î¾) := (cid:99)W L+1

t

.

dhl

t(Î¾) := dxl

t(Î¾) (cid:12) Ï†(cid:48)(hl

t(Î¾))

dxlâˆ’1
t

(Î¾) := W l(cid:62)

0 dhl

t(Î¾) âˆ’ Î·Î¸W l

tâˆ’1
(cid:88)

Ï‡scsxlâˆ’1

s

= Î¨(W l(cid:62)

0 dhl

t(Î¾), xlâˆ’1

0

s=0
, . . . , xlâˆ’1

tâˆ’1; Î·Î¸W l , {Ï‡s, cs}tâˆ’1
s=0)

where cs = dhl(cid:62)

s dhl
n

t(Î¾)

. For Î¾ = Î¾t and its label yt, we deï¬ne63

Ï‡t := L(cid:48)(ft(Î¾t), yt).

Finally, we compute the (normalized) change in W L+1 after this SGD update.

Î´W L+1

t+1 := âˆ’Î·Ï‡txL

t (Î¾t).

H.5 The Inï¬nite-Width Limit

In this section, we describe the Z random variables (Deï¬nition 7.3) corresponding to the vectors
of the program constructed above. According to the Master Theorem, each such vector z will have
roughly iid coordinates distributed like Z z in the large n limit.
Let ËšÎ¸â€¢ denote the limit of any Î¸â€¢ in Appendix H.3. If pseudostability holds, then ËšÎ¸â€¢ is either 0 or 1,
as one can easily verify. We can construct the Z random variables for each vector in the program, as
follows.

1. For the ï¬rst forward and backward passes, we have,

Z h1
Z dxL

0(Î¾) = Î¾ZW 1
0 ,
0 (Î¾) = Z (cid:99)W L+1

0

,

Z xl
Z dhl

0(Î¾) = Ï†(Z hl
0(Î¾) = Z dxl

0(Î¾)),
0(Î¾)Ï†(cid:48)(Z hl

0(Î¾)),

0

Z hl+1
Z dxlâˆ’1

0

0

(Î¾) = ZW l+1
(Î¾) = ZW l(cid:62)

0 dhl

0(Î¾)

xl
0(Î¾),

62The pseudo-Lipschitzness of Ï†(cid:48) assumed in Assumption H.22 implies that Î¨ here is pseudo-Lipschitz, so

that we can ultimately apply our Master Theorem.

63Here we use Moment with the function Ï†(; ft(Î¾t)) = L(cid:48)(ft(Î¾t), yt) with no input and one parameter
(we absorb yt into Ï† since it does not change with n). The continuity of L(cid:48) in its ï¬rst argument satisï¬es
Assumption F.4(1), so the Master Theorem can apply.

52

2. For z âˆˆ {xl, hl}l, we have

Z zt(Î¾) = Z z0(Î¾) + ËšÎ¸zZ Î´z1(Î¾) + Â· Â· Â· + ËšÎ¸zZ Î´zt(Î¾)

(52)

3. For l âˆˆ [L], x = xl, h = hl, we have Z Î´xt(Î¾) = Î¨(Z htâˆ’1(Î¾), Z Î´ht(Î¾); ËšÎ¸h) where Î¨ is as in

Eq. (51). If ËšÎ¸h = 0 (e.g. if r > 0), then

Otherwise, ËšÎ¸h = 1, and

4. For h = h1, we have

Z Î´xt(Î¾) = Ï†(cid:48)(Z htâˆ’1(Î¾))Z Î´ht(Î¾).

Z Î´xt(Î¾) = Ï†(Z ht(Î¾)) âˆ’ Ï†(Z htâˆ’1(Î¾)).

Z Î´ht(Î¾) = âˆ’Î·ËšÏ‡tâˆ’1Î¾(cid:62)

tâˆ’1Î¾Z dhtâˆ’1.

5. For l â‰¥ 2, h = hl, x = xlâˆ’1, W = W l, we have

(53)

(54)

Z Î´ht(Î¾) = ËšÎ¸x/hZW0Î´xt(Î¾) âˆ’ Î·ËšÎ¸W x/h

tâˆ’2
(cid:88)

s=0

ËšÏ‡sZ dhs E Z xsZ xt(Î¾)

âˆ’ Î·ËšÏ‡tâˆ’1

ËšÎ¸W/hZ dhtâˆ’1 E Z xtâˆ’1Z xt(Î¾)

(55)

where at least one of ËšÎ¸x/h and ËšÎ¸W/h equals 1. As usual, here we have the ZHat-ZDot
decomposition of ZW0Î´xt(Î¾).

ZW0Î´xt(Î¾) = Ë†ZW0Î´xt(Î¾) + Ë™ZW0Î´xt(Î¾)

= Ë†ZW0Î´xt(Î¾) +

tâˆ’1
(cid:88)

s=0

Z dhs E âˆ‚Z Î´xt(Î¾)

âˆ‚ Ë†ZW (cid:62)

0 dhs

.

6. For last layer weight

and

Z Î´W L+1

t = âˆ’Î·ËšÏ‡tâˆ’1Z xL

tâˆ’1

Z (cid:99)W L+1

t = Z (cid:99)W L+1

0 + ËšÎ¸L+1/f (Z Î´W L+1

1 + Â· Â· Â· + Z Î´W L+1

t

)

7. The output deltas have limits

Î´ Ëšft(Î¾) = ËšÎ¸(cid:48)

L+1

E Z Î´W L+1

t Z xL

t (Î¾) + ËšÎ¸(cid:48)

Lf

E Z (cid:99)W L+1

tâˆ’1 Z Î´xL

t (Î¾)

and

8. For gradients:

Ëšft(Î¾) = Î´ Ëšf1(Î¾) + Â· Â· Â· + Î´ Ëšft(Î¾).

(56)

(57)

(58)

Z dxL
Z dhl

t (Î¾) = Z (cid:99)W L+1
t(Î¾) = Z dxl

t

t(Î¾)Ï†(cid:48)(Z hl

t(Î¾))

Z dxlâˆ’1

t

9. Loss derivative

(Î¾) = ZW l(cid:62)

0 dhl

t(Î¾) âˆ’ Î·ËšÎ¸W l

tâˆ’1
(cid:88)

s=0

ËšÏ‡sZ xlâˆ’1

s E Z dhl

s Z dhl

t(Î¾)

ËšÏ‡t = L(cid:48)(Ëšft, y0).

The following fact follows from the results of [51] (or can be veriï¬ed by straightforward calculation)
and will be useful for us.
Ë™Z dxl

0(Î¾) = 0 and Z dxl

0(Î¾) for any Î¾ âˆˆ X .

Proposition H.24.

0(Î¾) = Ë†Z dxl

53

If the parametrization is pseudostable, then all the Î¸â€¢ converge to 0 or 1 so Setup 7.2 is satisï¬ed.
Therefore, the Master Theorem applies and says that, for any collection of vectors v1, . . . , vk such
that Z vi

is deï¬ned above, we have

1
n

n
(cid:88)

Î±=1

Ïˆ(v1

Î±, . . . , vk

Î±) a.s.âˆ’âˆ’â†’ E Ïˆ(Z v1

, . . . , Z vk

)

for any pseudo-Lipschitz Ïˆ. In addition,64

Î´ft(Î¾) a.s.âˆ’âˆ’â†’ Î´ Ëšft(Î¾),

ft(Î¾) a.s.âˆ’âˆ’â†’ Ëšft(Î¾), Ï‡t

a.s.âˆ’âˆ’â†’ ËšÏ‡t,

âˆ€Î¾ âˆˆ X , t â‰¥ 1.

We now describe some immediate consequences of this.

H.5.1 Some Immediate Results

Proposition H.25. A pseudostable parametrization is trivial if

2aL+1 + c > 1

and aL+1 + bL+1 + r > 1.

Proof. In this case, Î¸(cid:48)

L+1, Î¸(cid:48)

Lf , Î¸(cid:48)

L,L+1 â†’ 0, and Î´ Ëšft(Î¾) = 0 for all t and Î¾ âˆˆ X by Eq. (58).

Proposition H.26. A pseudostable parametrization is stable.

Proof. For a pseudostable parametrization, all of Î¸s converge to 1 or 0, and all of the Z Î´hl
have well deï¬ned (ï¬nite) limits, which implies âˆ†hl
Oâˆ—(1).

t(Î¾) = Oâˆ—(1), âˆ€l âˆˆ [L],

t(Î¾), âˆ†xl

t(Î¾), Z Î´xl
t(Î¾)
and ft(Î¾) =

Proposition H.27. Consider a pseudostable parametrization. If r > 0, then it ï¬xes all (pre)features
and all (pre)feature kernels. In addition, âˆ†W L+1
t âˆ†xL

t (Î¾) a.s.âˆ’âˆ’â†’ 0.

Proof. If r > 0, then Î¸l â†’ 0 for all l âˆˆ [L], so that for all z âˆˆ {xl, hl}l, âˆ†zt(Î¾) = zt(Î¾) âˆ’ z0(Î¾) =
Î¸zÎ´z1(Î¾) + Â· Â· Â· + Î¸zÎ´zt(Î¾) has (cid:107)âˆ†zt(Î¾)(cid:107)2/n a.s.âˆ’âˆ’â†’ 0 by Eq. (52) and the Master Theorem, i.e. all
features are ï¬xed. Similarly, for any pair Î¾, Â¯Î¾ âˆˆ X , zt(Î¾)(cid:62)zt( Â¯Î¾)/n âˆ’ z0(Î¾)(cid:62)z0( Â¯Î¾)/n a.s.âˆ’âˆ’â†’ 0, so all
t (Î¾) a.s.âˆ’âˆ’â†’ 0
feature kernels are ï¬xed. Finally, r > 0 implies Î¸(cid:48)
by the Master Theorem.

L,L+1 â†’ 0, which means âˆ†W L+1

t âˆ†xL

Proposition H.28. An initialization-stable parametrization with r < 0 is not stable.

Proof. If r < 0, then there is some (cid:96) âˆˆ [L] such that Î¸L â‰¥ Â· Â· Â· â‰¥ Î¸(cid:96) > 1 â‰¥ Î¸(cid:96)âˆ’1 â‰¥ Â· Â· Â· â‰¥ Î¸1.
For h = h(cid:96), x = x(cid:96)âˆ’1, W = W (cid:96), we would have Î¸x/h = Î¸(cid:96)âˆ’1/Î¸(cid:96) â†’ 0, Î¸W/h = 1, and Î¸W x/h =
Î¸W/hÎ¸(cid:96)âˆ’1 â†’ 0. The Tensor Program up to the deï¬nition of Î´h1(Î¾0) satisï¬es the conditions of the
Master Theorem. Therefore, (cid:107)Î´h1(Î¾0)(cid:107)2/2 a.s.âˆ’âˆ’â†’ E(Z Î´h1(Î¾0))2 = E(Î·ËšÏ‡tâˆ’1Z dh0 E Z x0Z x1(Î¾0))2. If
Î¾0 (cid:54)= 0, then E(Z dh0 )2 > 0. If Î· is in addition sufï¬ciently small but nonzero, then E Z x0Z x1(Î¾0) â‰ˆ
E(Z x0 )2 > 0. Therefore, under these conditions, and with a training sequence that has ËšÏ‡0 (cid:54)= 0,
we have E(Î·ËšÏ‡tâˆ’1Z dh0 E Z x0Z x1(Î¾0))2 > 0, so that Î´h1(Î¾0) = Î˜Î¾0(1). However, âˆ†h1(Î¾0) =
Î¸hÎ´h1(Î¾0) and Î¸h = Î¸(cid:96) â†’ âˆ. Hence âˆ†h1(Î¾0) (cid:54)= OÎ¾0(1), as desired.

H.6 r > 0 Implies Kernel Regime

In this section, we analyze the case when r > 0. Our main result is deriving the corresponding
inï¬nite-width kernel gradient descent dynamics (Theorem H.32). Nothing here depends on Ï† being
tanh or Ïƒ-gelu.

64Again, if aL+1 + bL+1 = 1/2, remember we are conditioning on f0(Î¾), Î¾ âˆˆ X .

54

Preliminary Derivations

If r > 0, then ËšÎ¸l = ËšÎ¸W l = 0 for all l âˆˆ [L], so that we have
0(Î¾), Z dxl

0(Î¾), Z (cid:99)W L+1

t(Î¾) = Z dxl

0(Î¾), Z dhl

Z hl

0(Î¾), Z xl

t(Î¾) = Z xl

t(Î¾) = Z hl

t(Î¾) = Z dhl
for all t and Î¾ âˆˆ X . Let (cid:96) âˆˆ [L] be the unique (cid:96) such that 1 = Î¸L/Î¸L = Â· Â· Â· = Î¸(cid:96)/Î¸L > Î¸(cid:96)âˆ’1/Î¸L â‰¥
Â· Â· Â· â‰¥ Î¸1/Î¸L. Then for l â‰¥ (cid:96) + 1 and shorthand h = hl, x = xlâˆ’1, W = W l, we have ËšÎ¸x/h = 1,
ËšÎ¸W x/h = 0 and, by Eq. (55),

t = Z (cid:99)W L+1

0

Z Î´ht(Î¾) = ZW0Î´xt(Î¾) âˆ’ Î·ËšÏ‡tâˆ’1
= ZW0Î´xt(Î¾) âˆ’ Î·ËšÏ‡tâˆ’1

ËšÎ¸W/hZ dhtâˆ’1 E Z xtâˆ’1 Z xt(Î¾),
ËšÎ¸W/hZ dh0(Î¾tâˆ’1) E Z x0(Î¾tâˆ’1)Z x0(Î¾)

(59)

where ËšÎ¸W/h can be either 0 or 1. For l = (cid:96), because Î¸h = Î¸l = maxmâ‰¤l Î¸W m = max(Î¸W l , Î¸lâˆ’1) =
max(Î¸W l , Î¸x) so ËšÎ¸x/h = ËšÎ¸W x/h = 0 and ËšÎ¸W/h = 1, we also have
Z Î´ht(Î¾) = âˆ’Î·ËšÏ‡tâˆ’1Z dhtâˆ’1 E Z xtâˆ’1Z xt(Î¾)

= âˆ’Î·ËšÏ‡tâˆ’1Z dh0(Î¾tâˆ’1) E Z x0(Î¾tâˆ’1)Z x0(Î¾).

(60)

Finally, for all l âˆˆ [L], we have, by Eq. (53),

Z Î´xt(Î¾) = Ï†(cid:48)(Z htâˆ’1(Î¾))Z Î´ht(Î¾) = Ï†(cid:48)(Z h0(Î¾))Z Î´ht(Î¾).

Deï¬nition H.29. For 1 â‰¤ m â‰¤ l and Î¾, Î¶ âˆˆ X , deï¬ne
Î£ml(Î¾, Î¶) def= E Z xm
We also deï¬ne

0 (Î¶) Ã— E Ï†(cid:48)(Z hm+1

0 (Î¾)Z xm

0

(Î¾))Ï†(cid:48)(Z hm+1

0

(Î¶)) Ã— Â· Â· Â· Ã— E Ï†(cid:48)(Z hl

0(Î¾))Ï†(cid:48)(Z hl

0(Î¶)).

Î£0l(Î¾, Î¶) def= Î¾(cid:62)Î¶ Ã— E Ï†(cid:48)(Z hm+1

0

(Î¾))Ï†(cid:48)(Z hm+1

0

(Î¶)) Ã— Â· Â· Â· Ã— E Ï†(cid:48)(Z hl

0(Î¾))Ï†(cid:48)(Z hl

0(Î¶))

For example,

and so on.

Î£ll(Î¾, Î¶) = E Z xl
Î£l,l+1(Î¾, Î¶) = E Z xl

0(Î¾)Z xl
0(Î¾)Z xl

0(Î¶)

0(Î¶) E Ï†(cid:48)(Z hl+1

0

(Î¾))Ï†(cid:48)(Z hl+1

0

(Î¶)),

Notation For brevity, below we will shorthand Ï‘m = Î¸W m/hm. We write Z x â‰¡ Z y mod Ë†ZW â€¢ if
Z x âˆ’ Z y is a linear combination of Ë†ZW u for various vectors u.
Lemma H.30. For any input Î¾, any l â‰¥ (cid:96), at any time t,

Z Î´hl

t(Î¾) â‰¡ âˆ’Î·ËšÏ‡tâˆ’1Z dhl

0(Î¾tâˆ’1)

lâˆ’1
(cid:88)

m=(cid:96)âˆ’1

ËšÏ‘m+1Î£m,lâˆ’1(Î¾tâˆ’1, Î¾) mod Ë†ZW l

0â€¢.

(61)

Proof. We proceed by induction.

Base Case l = (cid:96): this is given by Eq. (60).

t

, Â¯x = xlâˆ’1

tâˆ’1, x0 = xlâˆ’1

, Â¯Î¾ = Î¾tâˆ’1, W = W l

tâˆ’1, h0 =
0, i.e. we use Â¯â€¢ to denote time t âˆ’ 1 in contrast to â€¢ for time t, and we

Induction: Assume Eq. (61) holds for l âˆ’ 1, and we shall prove it for l.
To alleviate notation, we write x = xlâˆ’1
hlâˆ’1
0
0, hl
suppress layer index. In contrast, we will write hl
First, note that Z Î´x(Î¾) = Ï†(cid:48)(Z Â¯h(Î¾))Z Î´h(Î¾) by Eq. (53). Because Z Â¯h(Î¾) = Z h0(Î¾), and, by induction
hypothesis, Z Î´h(Î¾) is a scalar multiple of Z dh0( Â¯Î¾) = Z dx0( Â¯Î¾)Ï†(cid:48)(Z h0( Â¯Î¾)), Z Î´x(Î¾) is symbolically
solely a function of Z h0(Î¾), Z h0( Â¯Î¾), Z dx0( Â¯Î¾),all of which are equal to their Ë†Z versions (with the last
0( Â¯Î¾) is constructed from matrix
due to Proposition H.24). Among these, only Z dx0( Â¯Î¾) = ZW (cid:62)dhl
multiplication with W (cid:62)

t, and Î¾ for their usual meanings.

, Â¯h = hlâˆ’1

, h = hlâˆ’1

0

t

0 . Thus,

Ë™ZW0Î´x(Î¾) = Z dhl

0( Â¯Î¾) E âˆ‚Z Î´x(Î¾)
âˆ‚Z dx0( Â¯Î¾)

= Z dhl

0( Â¯Î¾) E Ï†(cid:48)(Z h0(Î¾))

âˆ‚Z Î´h(Î¾)
âˆ‚Z dx0( Â¯Î¾)

.

(62)

55

By induction hypothesis,

âˆ‚Z Î´h(Î¾)
âˆ‚Z dx0( Â¯Î¾)

= âˆ’Î·ËšÏ‡tâˆ’1Ï†(cid:48)(Z h0( Â¯Î¾))

lâˆ’2
(cid:88)

m=(cid:96)âˆ’1

ËšÏ‘m+1Î£m,lâˆ’2( Â¯Î¾, Î¾).

Therefore,

E Ï†(cid:48)(Z h0(Î¾))

âˆ‚Z Î´h(Î¾)
âˆ‚Z dx0( Â¯Î¾)

= âˆ’Î·ËšÏ‡tâˆ’1 E

(cid:104)

Ï†(cid:48)(Z h0(Î¾))Ï†(cid:48)(Z h0( Â¯Î¾))

(cid:105) lâˆ’2
(cid:88)

m=(cid:96)âˆ’1

ËšÏ‘m+1Î£m,lâˆ’2( Â¯Î¾, Î¾).

By deï¬nition of Î£ml, this equals

E Ï†(cid:48)(Z h0(Î¾))

âˆ‚Z Î´h(Î¾)
âˆ‚Z dx0( Â¯Î¾)

= âˆ’Î·ËšÏ‡tâˆ’1

lâˆ’2
(cid:88)

m=(cid:96)âˆ’1

ËšÏ‘m+1Î£m,lâˆ’1( Â¯Î¾, Î¾).

Plugging this back into Eq. (62), we get

Ë™ZW0Î´x(Î¾) = âˆ’Î·ËšÏ‡tâˆ’1Z dhl

0( Â¯Î¾)

lâˆ’2
(cid:88)

m=(cid:96)âˆ’1

ËšÏ‘m+1Î£m,lâˆ’1( Â¯Î¾, Î¾).

(63)

Finally, by Eq. (59),

Z Î´hl

t(Î¾) = Ë™ZW0Î´x(Î¾) âˆ’ Î·ËšÏ‡tâˆ’1
= Ë™ZW0Î´x(Î¾) âˆ’ Î·ËšÏ‡tâˆ’1

ËšÏ‘lZ dhl
ËšÏ‘lZ dhl

0( Â¯Î¾) E Z x0( Â¯Î¾)Z x0(Î¾)
0( Â¯Î¾)Î£lâˆ’1,lâˆ’1( Â¯Î¾, Î¾).

Together with Eq. (63), this completes the induction.

Lemma H.31. Assume pseudostability, r > 0, and aL+1 + bL+1 â‰¤ 2aL+1 + c. If ËšÎ¸L+1/f = 1 then
ËšÎ¸(cid:48)
Lf = 0.

Proof. aL+1 + bL+1 â‰¤ 2aL+1 + c iff Î¸L+1 â‰¤ Î¸f . So ËšÎ¸L+1/f = 1 implies Î¸L+1 = Î¸f . By
pseudostability, nÎ¸L+1 â‰¤ 1. Since Î¸L = nâˆ’r, we have Î¸(cid:48)
Lf = n Â· nâˆ’r Â· Î¸f = nâˆ’r Â· nÎ¸L+1 < 0 since
r > 0. Therefore ËšÎ¸(cid:48)

Lf = 0.

Theorem H.32. Consider a pseudostable parametrization. At any time t, for any input Î¾ âˆˆ X , we
have

Î´ Ëšft(Î¾) = âˆ’Î·ËšÏ‡tâˆ’1Î£(Î¾tâˆ’1, Î¾),

where the kernel Î£ is deï¬ned for any Î¾, Î¶ âˆˆ X by

Î£(Î¶, Î¾) def= ËšÎ¸(cid:48)

L+1Î£LL(Î¶, Î¾) + ËšÎ¸(cid:48)

Lf

Lâˆ’1
(cid:88)

m=(cid:96)âˆ’1

ËšÏ‘m+1Î£mL(Î¶, Î¾).

Observe that in the NTK parametrization, (cid:96) = 1, and ËšÎ¸(cid:48)
Î£ = (cid:80)L

m=0 Î£mL is precisely the NTK (for MLP without biases).

L+1 = ËšÎ¸(cid:48)

Lf = ËšÏ‘m+1 = 1 for all m, so

Proof. By Eqs. (57) and (58),

Î´ Ëšft(Î¾) = ËšÎ¸(cid:48)
Z (cid:99)W L+1

t = Z (cid:99)W L+1

E Z Î´W L+1

t Z xL
t (Î¾) + ËšÎ¸(cid:48)
0 + ËšÎ¸L+1/f (Z Î´W L+1

E Z (cid:99)W L+1
tâˆ’1 Z Î´xL
1 + Â· Â· Â· + Z Î´W L+1

L+1

Lf

t

).

t (Î¾)

Now by Lemma H.31, either ËšÎ¸L+1/f = 0 or ËšÎ¸(cid:48)
contributes 0 to Î´ Ëšft(Î¾). So we can replace Z (cid:99)W L+1

Lf = 0. In both cases, (Z Î´W L+1
tâˆ’1 with Z (cid:99)W L+1
above, and write

0

1 + Â· Â· Â· + Z Î´W L+1

t

)

Î´ Ëšft(Î¾) = ËšÎ¸(cid:48)

L+1

E Z Î´W L+1

t Z xL

t (Î¾) + ËšÎ¸(cid:48)

Lf

E Z (cid:99)W L+1

0 Z Î´xL

t (Î¾).

56

If Eq. (61) is true for l = L, then

E Z (cid:99)W L+1

0 Z Î´xL

t (Î¾) = âˆ’Î·ËšÏ‡tâˆ’1 E Z (cid:99)W L+1

0 Z dhL

0 (Î¾tâˆ’1)Ï†(cid:48)(Z hL

0 (Î¾))

Lâˆ’1
(cid:88)

m=(cid:96)âˆ’1

ËšÏ‘m+1Î£m,Lâˆ’1(Î¾tâˆ’1, Î¾)

where the contributions from Ë†ZW L
Z dhL

0 â€¢ in Z Î´xL
0 (Î¾)), we continue

0 (Î¾) = Z (cid:99)W L+1

Ï†(cid:48)(Z hL

0

t (Î¾) vanish as they are independent from Z (cid:99)W L+1

0

. Since

E Z (cid:99)W L+1

0 Z Î´xL

t (Î¾) = âˆ’Î·ËšÏ‡tâˆ’1 E

(cid:16)

Z (cid:99)W L+1

0

(cid:17)2

Ï†(cid:48)(Z hL

0 (Î¾tâˆ’1))Ï†(cid:48)(Z hL

0 (Î¾))

Lâˆ’1
(cid:88)

m=(cid:96)âˆ’1

ËšÏ‘m+1Î£m,Lâˆ’1(Î¾tâˆ’1, Î¾)

= âˆ’Î·ËšÏ‡tâˆ’1

Lâˆ’1
(cid:88)

m=(cid:96)âˆ’1

ËšÏ‘m+1Î£mL(Î¾tâˆ’1, Î¾).

Similarly, by Eq. (56),

E Z Î´W L+1

t Z xL

t (Î¾) = âˆ’Î·ËšÏ‡tâˆ’1 E Z xL
= âˆ’Î·ËšÏ‡tâˆ’1 E Z xL

tâˆ’1(Î¾tâˆ’1)Z xL
0 (Î¾tâˆ’1)Z xL

t (Î¾)

0 (Î¾) = âˆ’Î·ËšÏ‡tâˆ’1Î£LL(Î¾tâˆ’1, Î¾).

Altogether, these prove the desired claim.

Corollary H.33. A pseudostable parametrization with r > 0 is nontrivial iff aL+1 + bL+1 + r = 1
or 2aL+1 + c = 1.

Proof. The kernel Î£ in Theorem H.32 is nonzero iff ËšÎ¸(cid:48)
aL+1 + bL+1 + r = 1 or 2aL+1 + c = 1.

L+1 or ËšÎ¸(cid:48)

Lf is 1, which is equivalent to saying

Corollary H.34. An initialization-stable parametrization with r > 0 but aL+1 + bL+1 + r < 1 or
2aL+1 + c < 1 is not stable.

Proof. If aL+1 + bL+1 + r < 1 or 2aL+1 + c < 1, then Î¸(cid:48)
Lf â†’ âˆ. Clearly, from
the deï¬nition, Î£mL(Î¾, Î¾) > 0 for any Î¾ (cid:54)= 0 and m âˆˆ [0, L]. All of our reasoning leading up to
Theorem H.32 applied at t = 1 holds, so Theorem H.32 (along with the Master Theorem) implies
|Î´ft(Î¾)| a.s.âˆ’âˆ’â†’ âˆ.
Corollary H.35. If aL+1 + bL+1 + r > 1 and 2aL+1 + c = 1, then for all Î¾ âˆˆ X , Ëšft(Î¾) a.s.âˆ’âˆ’â†’ 0 and
Î´ Ëšft(Î¾) = âˆ’Î·ËšÏ‡tâˆ’1Î£LL(Î¾tâˆ’1, Î¾), i.e. we have the Neural Network-Gaussian Process (NNGP) limit.

L+1 â†’ âˆ or Î¸(cid:48)

Conventionally, the NNGP limit is associated with only training the last layer and nothing else. This
result says that the same limit can be achieved if we train the body of the network slightly, so that
âˆ†xL
enough (embodied in the inequality aL+1 + bL+1 + r > 1) to
cause changes in ft.

t does not interact with W L+1

0

Proof. The premise implies ËšÎ¸(cid:48)

L+1 = 1 and ËšÎ¸(cid:48)

Lf = 0, and the rest follows from Theorem H.32.

Remark H.36. We have assumed for simplicity of the proof that aL+1 + bL+1 â‰¤ 2aL+1 + c. If this
is not the case, then we can easily see Corollary H.35 applies anyway.

H.7 r = 0 Implies Feature Learning

In this section, we assume r = 0 and show any such pseudostable parametrization 1) admits
(pre)feature learning and (pre)feature kernel evolution, and 2) is not in kernel regime (Theorem H.51).
The overarching logic goes like this.

1. The Master Theorem shows that the speciï¬c entry 1
1 (Î¾0))2.

If the learning rate Î· = 0, then xL

n (cid:107)xL

nel converges to E(Z xL
E(Z xL

1 (Î¾0))2 = E(Z xL

0 )2. We hope to say that as Î· increases, E(Z xL

1 (Î¾0)(cid:107)2 of the feature ker-
1 (Î¾0) = xL
0 and
1 (Î¾0))2 moves away

57

from E(Z xL
E(Z xL
pute âˆ‚2
Î·
the next best thing is âˆ‚2
results for prefeatures and for other layers can be derived similarly.

0 )2, which would imply feature kernel evolution in layer L. To do so, we com-
1 (Î¾0))2 evaluated at Î· = 0 and show it is nonzero (it turns out âˆ‚Î· vanishes, so
Î·). This then also implies feature learning in layer L. Analogous

2. If the parametrization is in the kernel regime with kernel K, the ï¬rst step of SGD in the large
width limit would look like Ëšf1(Î¾) âˆ’ Ëšf0(Î¾) = âˆ’Î·ËšÏ‡0K(Î¾, Î¾0); in particular, Ëšf1(Î¾) âˆ’ Ëšf0(Î¾)
is linear in Î·. To show that a pseudostable parametrization with r = 0 is not in the kernel
regime, we will show âˆ‚3
Î· vanishes,
so the next best thing is âˆ‚3

Î·(Ëšf1(Î¾) âˆ’ Ëšf0(Î¾)) = âˆ‚3
Î·).

Ëšf1(Î¾) is nonzero. (It turns out âˆ‚2

Î·

To calculate these Î· derivatives, we will derive recurrence relations involving quantities deï¬ned below
(see Lemma H.38 and Theorem H.41).

Setup and Notation First, write

def= Ë†ZW lxlâˆ’1
def= Z hl
0 is a centered Gaussian independent from Ë†Z l

t(Î¾0), Ë†Z l
t

Z l
t

t

Note that ËœZ l

(Î¾0), ËœZ l
0

def= Z dxl
0.

0 = Z l

0. Then we deï¬ne

Î³l(Î·) def= E Ï†(Z l

0)Ï†(Z l

1),

11(Î·) def= E Ï†(cid:48)(Z l
Î³l

0)Ï†(cid:48)(Z l

1),

02(Î·) def= E Ï†(Z l
Î³l

0)Ï†(cid:48)(cid:48)(Z l
1)

20(Î·) def= E Ï†(cid:48)(cid:48)(Z l
Î³l

0)Ï†(Z l

1),

Î»l(Î·) def= E Ï†(Z l

1)2

where the dependence on Î· is from Z l
we have Î³l(0), Î»l(0), Î³l
Observe that ( Ë†Z l

1, Ë†Z l

11(0) > 0. Note at Î· = 0, we have Z l

1. Naturally, since Ï† and Ï†(cid:48) are not almost everywhere zero,
0)2.

0, so Î³l(0) = Î»l(0) = E Ï†(Z l

1 = Z l

0) is jointly Gaussian with mean zero and covariance

Î“l(Î·) def=

(cid:18)Î»l(Î·) Î³l(Î·)
Î³l(Î·) Î»l(0)

(cid:19)

.

(64)

WLOG, for simplicity of notation, we assume we choose a training routine such that ËšÏ‡0 = 1. We
assume Î¾0 (cid:54)= 0.
Since r = 0, WLOG we can suppose for some (cid:96) âˆˆ [L], we have Î¸L = Â· Â· Â· = Î¸(cid:96) = 1 > Î¸(cid:96)âˆ’1 â‰¥ Â· Â· Â· â‰¥
Î¸1.
Lemma H.37. With the setup above, we have

0 = Z (cid:96)âˆ’1
Z (cid:96)âˆ’1

1

, . . . , Z 1

0 = Z 1
1 ,

and

1 = Ë†Z l
Z l

1 + Î·Î²l ËœZ l

0Ï†(cid:48)(Z l

0),

âˆ€l âˆˆ [(cid:96), L],

where Î²l is deï¬ned recursively by

Î²l = Î²l(Î·) def= âˆ’Î³lâˆ’1(Î·) + Î²lâˆ’1(Î·)Î³lâˆ’1

11 (Î·)

Î²(cid:96)âˆ’1(Î·) def= 0.

Additionally, Î²l(0) < 0 for all l â‰¥ (cid:96).

Proof. Straightforward calculation using Moment and Zdot. Here, âˆ’Î³lâˆ’1(Î·) comes from
11 (Î·) comes from Ë™Z hl
âˆ†W l
11 (0) > 0 for all l, the
recurrence on Î²l implies that Î²l(0) < 0 for all l â‰¥ (cid:96).

1(Î¾0) and Î²lâˆ’1(Î·)Î³lâˆ’1

1(Î¾0). Since Î³l(0), Î³lâˆ’1

1x1

H.7.1 Deriving Recurrence Relations on âˆ‚Î·Î»l, âˆ‚Î·Î³l, âˆ‚2

Î·Î»l, âˆ‚2

Î·Î³l

Below, we derive the recurrence relations required for our main result. They depend on the following
constants.

Îºl
1

def= E (cid:2)(Ï†2)(cid:48)(cid:48)(Z l

0)(cid:3) ,

Îºl
2

def= E (cid:2)(Ï†2)(cid:48)(cid:48)(Z l

0)Ï†(cid:48)(Z l

0)2(cid:3) ,

Îºl
3

58

def= E (cid:2)Ï†(Z l

0)Ï†(cid:48)(cid:48)(Z l

0)Ï†(cid:48)(Z l

0)2(cid:3) .

Lemma H.38. With the setup above, we have, for all l âˆˆ [L],

âˆ‚Î·Î»l(0) =

âˆ‚Î·Î³l(0) =

1
2
1
2

1âˆ‚Î·Î»lâˆ’1(0)
Îºl

(65)

02âˆ‚Î·Î»lâˆ’1(0) + Î³l
Î³l

11âˆ‚Î·Î³lâˆ’1(0).

Proof. We ï¬rst derive the recurrence on âˆ‚Î·Î»l. By Lemma H.39 below, we have

âˆ‚Î·Î»l = 2 E Ï†(Z l

1)âˆ‚Î·Ï†(Z l

1) +

1
2

E(Ï†2)(cid:48)(cid:48)(Z l

1)âˆ‚Î·Î»lâˆ’1.

Note the second item in the sum evaluated at Î· = 0 is exactly the RHS of Eq. (65). For the ï¬rst item,
since

âˆ‚Î·Ï†(Z l

1) = Ï†(cid:48)(Z l

1)(Î²l ËœZ l

0Ï†(cid:48)(Z l

0) + Î· ËœZ l

0Ï†(cid:48)(Z l

0)âˆ‚Î·Î²l),

(66)

we compute

1) = E Ï†(Z l

1)Ï†(cid:48)(Z l

1)(Î²l ËœZ l

0Ï†(cid:48)(Z l

0) + Î· ËœZ l

0Ï†(cid:48)(Z l

0)âˆ‚Î·Î²l).

E Ï†(Z l

1)âˆ‚Î·Ï†(Z l
0, so that ËœZ l

0 is independent from everything else in the expectation. This implies

1 = Z l

At Î· = 0, Z l
the expectation is 0 and yields the recurrence for âˆ‚Î·Î»l(0).
(cid:18)Î³l
02
Î³l
11

For âˆ‚Î·Î³l, let Î£ = Î£(Î·) def=

Î³l
11
Î³l
20

(cid:19)

. With Î“lâˆ’1 as in Eq. (64), we have

âˆ‚Î·Î³l = E Ï†(Z l

0)âˆ‚Î·Ï†(Z l

1) +

1
2

(cid:104)Î£, âˆ‚Î·Î“lâˆ’1(cid:105)

By same reasoning as in Eq. (65), the ï¬rst term of this sum is zero when evaluated at Î· = 0. Since
âˆ‚Î·Î“lâˆ’1(Î·) def=

(cid:18)âˆ‚Î·Î»lâˆ’1(Î·) âˆ‚Î·Î³lâˆ’1(Î·)

, we have

(cid:19)

âˆ‚Î·Î³l(Î·)

0

âˆ‚Î·Î³l =

1
2

(cid:104)Î£, âˆ‚Î·Î“lâˆ’1(cid:105) =

1
2

02âˆ‚Î·Î»lâˆ’1 + Î³l
Î³l

11âˆ‚Î·Î³lâˆ’1.

Lemma H.39. Consider a twice continuously differentiable f and Gaussian vector Z âˆ¼ N (0, Î£)
such that f and Î£ both depend on a parameter Î·. Then

âˆ‚Î· E f (Z) = E âˆ‚Î·f (Z) +

1
2

(cid:104)E âˆ‡2f (z), âˆ‚Î·Î£(cid:105),

where âˆ‡2 denotes Hessian wrt z, and (cid:104), (cid:105) denotes trace inner product of matrices.

Proof. Let p(z) denote the PDF of Z. We have

âˆ‚Î· E f (Z) = âˆ‚Î·

(cid:90)

f (z)p(z) dz =

(cid:90)

âˆ‚Î·f (z)p(z) dz +

(cid:90)

f (z)âˆ‚Î·p(z) dz

The ï¬rst integral is E âˆ‚Î·f (Z). The second integral can be rewritten using integration-by-parts as
(cid:104)E âˆ‡2f (z), âˆ‚Î·Î£(cid:105). (e.g. see Lemma F.18 of [56])

We then easily have
Theorem H.40. For all l âˆˆ [L],

âˆ‚Î·Î³l(0) = âˆ‚Î·Î»l(0) = 0.

Proof. For l < (cid:96), we obviously have âˆ‚Î·Î³l(Î·) = âˆ‚Î·Î»l(0) = 0 for all Î·. Then this follows from
Lemma H.38 and a simple induction.

Unfortunately, this means that the ï¬rst Î· derivative doesnâ€™t give us what we need. So we try the
second derivative, which will turn out to work.

59

Theorem H.41. For all l < (cid:96),âˆ‚2

Î·Î»l(0) = âˆ‚2

Î·Î³l(0) = 0, and for all l â‰¥ (cid:96),

Î·Î»l(0) = CÎºl
âˆ‚2

2 +

Î·Î³l(0) = CÎºl
âˆ‚2

3 +

1
2
1
2

where C = 2(Î²l(0))2 E( ËœZ l

0)2 > 0.

1âˆ‚2
Îºl

Î·Î»lâˆ’1(0)

02(0)âˆ‚2
Î³l

Î·Î»lâˆ’1(0) + Î³l

11(0)âˆ‚2

Î·Î³lâˆ’1(0),

Proof. We start with the âˆ‚2
Î·Î»l is a sum of 3 terms, representing 1) 2
derivatives in the integrand, 2) 2 derivatives in the Gaussian variance, and 3) 1 derivative each. When
evaluated at Î· = 0, only the ï¬rst two terms survive because âˆ‚Î·Î»lâˆ’1(0) = 0 by Theorem H.40:

Î·Î»l(0) recurrence. For l â‰¥ (cid:96), âˆ‚2

Î·Î»l(0) = E âˆ‚2
âˆ‚2

Î·Ï†2(Z l

1)|Î·=0 +

1
2

E(Ï†2)(cid:48)(cid:48)(Z l

0)âˆ‚2

Î·Î»lâˆ’1(0).

Now

E âˆ‚2

Î·Ï†2(Z l

1) = 2âˆ‚Î·(E Ï†(Z l
= 2 E(Ï†2)(cid:48)(cid:48)(Z l

1)Ï†(cid:48)(Z l
1)(Î²l ËœZ l

1)(Î²l ËœZ l
0Ï†(cid:48)(Z l

0Ï†(cid:48)(Z l
0) + Î· ËœZ l

0) + Î· ËœZ l
0Ï†(cid:48)(Z l

0Ï†(cid:48)(Z l
0)âˆ‚Î·Î²l))
0)âˆ‚Î·Î²l)2 + Â· Â· Â·

where other terms appear in this sum but they vanish at Î· = 0 because ËœZ l
expectation. Thus,

0 appears unpaired in the

Plugging this back in, we get the recurrence on âˆ‚2

E âˆ‚2

Î·Ï†2(Z l

1)|Î·=0 = 2(Î²l(0))2 E( ËœZ l
Î·Î»l(0).

0)2 E(Ï†2)(cid:48)(cid:48)(Z l

0)Ï†(cid:48)(Z l

0)2.

The âˆ‚2

Î·Î³l(0) recurrence is derived similarly.

The following result will be useful for showing âˆ‚3
Î·
Theorem H.42. Deï¬ne

Ëšf1(Î¾0) (cid:54)= 0.

Ë™Îºl
3

def= E (cid:2)Ï†(cid:48)(cid:48)(cid:48)(Z l

0)Ï†(cid:48)(Z l

0)3(cid:3) ,

Î³l
13

def= E Ï†(cid:48)(Z l

0)Ï†(cid:48)(cid:48)(cid:48)(Z l

0),

Î³l
22

def= E Ï†(cid:48)(cid:48)(Z l

0)2.

Then for all l â‰¥ (cid:96),

Î·Î³l
âˆ‚2

11(0) = C Ë™Îºl

3 +

1
2

13âˆ‚2
Î³l

Î·Î»lâˆ’1(0) + Î³l

22âˆ‚2

Î·Î³lâˆ’1(0),

where C = 2(Î²l(0))2 E( ËœZ l

0)2 > 0.

Proof. Similar to the proof of Theorem H.41.

The following result will be useful for showing prefeature kernel evolution.
Theorem H.43. For all l â‰¥ (cid:96),
âˆ‚2
Î·
0)2 > 0.

where C = 2(Î²l(0))2 E( ËœZ l

1)2|Î·=0 = 2C + Î³l

Î·Î»lâˆ’1(0),

11(0)âˆ‚2

E(Z l

Proof. Similar to the proof of Theorem H.41.

H.7.2 Applications to Ïƒ-Gelu

The following proposition regarding Ïƒ-gelu is easy to verify.
Proposition H.44. Let Ï† be Ïƒ-gelu. For any centered Gaussian Z âˆˆ R with nonzero variance,

E(Ï†2)(cid:48)(cid:48)(Z), E(Ï†2)(cid:48)(cid:48)(Z)Ï†(cid:48)(Z)2, E Ï†(Z)Ï†(cid:48)(cid:48)(Z)Ï†(cid:48)(Z)2, E Ï†(Z)Ï†(cid:48)(cid:48)(Z), E Ï†(cid:48)(cid:48)(Z)2 > 0,

and they converge to 0 as Ïƒ â†’ 0. Also,

E Ï†(cid:48)(cid:48)(cid:48)(Z)Ï†(cid:48)(Z)3, E Ï†(cid:48)(Z)Ï†(cid:48)(cid:48)(cid:48)(Z) < 0,

and they converge to âˆ’âˆ as Ïƒ â†’ 0.

60

13 < 0 and diverges to âˆ’âˆ with small Ïƒ.

This particularly implies that Îºl
Ë™Îºl
3, Î³l
Theorem H.45. Consider a pseudostable parametrization with r = 0. If Ï† is Ïƒ-gelu, then for all
l â‰¥ (cid:96),

22 > 0 and converges to 0 with small Ïƒ, but

02(0), Î³l

1, Îºl

2, Îºl

3, Î³l

Î·Î³l(0), âˆ‚2
âˆ‚2

Î·Î»l(0) > 0

and they converge to 0 as Ïƒ â†’ 0.

Proof. We always have (Î²l(0))2, E( ËœZ l
Theorem H.41, âˆ‚2
Î·Î³l(0) > 0 for all l â‰¥ (cid:96) as well. As Ïƒ â†’ 0, Îºl
âˆ‚2

Î·Î»l(0) > 0 for all l â‰¥ (cid:96). By Proposition H.44, Îºl

0)2 > 0. By Proposition H.44, Îºl

3, Î³l
02(0) â†’ 0, so âˆ‚2

1, Îºl

2, Îºl

3, Î³l

1, Îºl

2 > 0 as well. Thus, by
02(0) > 0, so by Theorem H.41,

Î·Î»l(0), âˆ‚2

Î·Î³L(0) â†’ 0.

Theorem H.46. Consider a pseudostable parametrization with r = 0. Suppose aL+1 + bL+1 + r = 1
or 2aL+1 + c = 1. If Ï† is Ïƒ-gelu for sufï¬ciently small Ïƒ, then

âˆ‚3
Î·

Ëšf1(Î¾0) (cid:54)= 0.

L+1

E Z Î´W L+1

Proof. We have Ëšf1(Î¾0) = ËšÎ¸(cid:48)
Lf and ËšÎ¸(cid:48)
ËšÎ¸(cid:48)

1 Z xL
L+1 is 1 because aL+1 + bL+1 + r = 1 or 2aL+1 + c = 1. We have
E Z Î´W L+1
E Z (cid:99)W L+1

1 (Î¾0) = âˆ’Î· E Z xL
1 (Î¾0) = E Z (cid:99)W L+1

1 Z xL
0 Z xL

0 Z xL
Ï†(Z hL

0 âˆ’ Î·Z (cid:99)W L+1

1 (Î¾0) + ËšÎ¸(cid:48)

E Z (cid:99)W L+1

0 Z Î´xL

1 (Î¾0)

Lf

0

0

= âˆ’Î· E Ï†(cid:48)(Z hL

1 (Î¾0))Ï†(cid:48)(Z hL

Ï†(cid:48)(Z hL
0 ) E Z xLâˆ’1

0 Z xLâˆ’1
0 ) E Z xLâˆ’1
(Î¾0)

1

0 Z xLâˆ’1

1

(Î¾0))

1 (Î¾0), where at least one of

where we used Steinâ€™s Lemma for the last equality. Thus

âˆ‚3
Î·

Ëšf1(Î¾0) = âˆ’

L+1âˆ‚2

Î·Î³L(0) + ËšÎ¸(cid:48)

Lf âˆ‚2

Î·(Î³L

(cid:16)ËšÎ¸(cid:48)

(cid:17)
11Î³Lâˆ’1)(0)

.

Below we will show that for small Ïƒ, âˆ‚2
negative, so âˆ‚3
Î·

Ëšf1(Î¾0) cannot be 0 no matter the values of ËšÎ¸(cid:48)

Î·Î³L(0) is small and positive and âˆ‚2
Lf .

L+1 and ËšÎ¸(cid:48)

Î·(Î³L

11Î³Lâˆ’1)(0) is large and

Î·Î³L
11(0) = C Ë™Îºl

Claim: For sufï¬ciently small Ïƒ, âˆ‚2

11(0) < 0. It converges to âˆ’âˆ as Ïƒ â†’ 0.

Î·Î³l

Proof: By Theorem H.42, âˆ‚2
2 Î³l
by Theorem H.45. Also, by Proposition H.44, Ë™Îºl
âˆ’âˆ, Î³l
converges to a positive constant as Ïƒ â†’ 0 as well. Therefore, for small enough Ïƒ, âˆ‚2
as Ïƒ â†’ 0, âˆ‚2

Î·Î»lâˆ’1(0) â‰¥ 0
3, Î³l
13 â†’
Î·Î»l(0) â†’ 0 by Theorem H.45). One can see that C
11(0) < 0, and

Î·Î³lâˆ’1(0). Note âˆ‚2
22âˆ‚2
22 > 0, and as Ïƒ â†’ 0, Ë™Îºl

Î·Î»lâˆ’1(0) + Î³l
13 < 0, Î³l

22 â†’ 0 (as well as âˆ‚2

Î·Î³Lâˆ’1(0), âˆ‚2

13âˆ‚2
3, Î³l

Î·Î³l

3 + 1

11(0) â†’ âˆ’âˆ.

Î·Î³L

Claim: For sufï¬ciently small Ïƒ, âˆ‚2

Î·(Î³L
11Î³Lâˆ’1)(0) = âˆ‚2

11Î³Lâˆ’1)(0) < 0. It converges to âˆ’âˆ as Ïƒ â†’ 0.
11(0)âˆ‚2
Î·Î³L

11(0)Î³Lâˆ’1(0) + Î³L

Î·(Î³L

Proof: Observe âˆ‚2
by Theorem H.40. So the above claim and Theorem H.45 yield the desired results.
Finishing the main proof: Therefore, if ËšÎ¸(cid:48)
Î·Î³L(0) > 0; if ËšÎ¸(cid:48)
âˆ‚2
0; if ËšÎ¸(cid:48)
L+1 = ËšÎ¸(cid:48)
Î·Î³L(0) â†’ 0 as Ïƒ â†’ 0.
âˆ‚2

L+1 = 0 but ËšÎ¸(cid:48)
Lf = 1, then âˆ’âˆ‚3
Î·

Ëšf1(Î¾0) < 0 for small Ïƒ because âˆ‚2

Ëšf1(Î¾0) < 0 for small Ïƒ because âˆ‚2

Lf = 0, then âˆ’âˆ‚3
Î·

L+1 = 1 but ËšÎ¸(cid:48)

Lf = 1, then âˆ’âˆ‚3
Î·

Î·(Î³L

Ëšf1(Î¾0) > 0 because
11Î³Lâˆ’1)(0) <
11Î³Lâˆ’1)(0) â†’ âˆ’âˆ while

Î·(Î³L

Î·Î³Lâˆ’1(0) because âˆ‚Î·Î³Lâˆ’1(0) = 0

H.7.3 Applications to Tanh

The following property of tanh is easy to verify.
Proposition H.47. Let Ï† = tanh. For any centered Gaussian Z âˆˆ R with nonzero variance,

E(Ï†2)(cid:48)(cid:48)(Z), E(Ï†2)(cid:48)(cid:48)(Z)Ï†(cid:48)(Z)2, E Ï†(cid:48)(cid:48)(Z)2 > 0,

and

E Ï†(Z)Ï†(cid:48)(cid:48)(Z)Ï†(cid:48)(Z)2, E Ï†(Z)Ï†(cid:48)(cid:48)(Z), E Ï†(cid:48)(cid:48)(cid:48)(Z)Ï†(cid:48)(Z)3, E Ï†(cid:48)(Z)Ï†(cid:48)(cid:48)(cid:48)(Z) < 0.

61

In particular, this means

3, Î³l
Îºl
Theorem H.48. Consider a pseudostable parametrization with r = 0. If Ï† is tanh, then for all l â‰¥ (cid:96),

02(0), Ë™Îºl

13 < 0.

22 > 0,

1, Îºl
Îºl

2, Î³l

3, Î³l

Î·Î³l(0) < 0,
âˆ‚2

Î·Î»l(0) > 0.
âˆ‚2

Proof. Similar to the proof of Theorem H.45, except that here Îºl
0.

3, Î³l

02(0) < 0, making âˆ‚2

Î·Î³l(0) <

Theorem H.49. Consider a pseudostable parametrization with r = 0. Suppose aL+1 + bL+1 + r = 1
or 2aL+1 + c = 1. If Ï† is tanh, then

âˆ‚3
Î·

Ëšf1(Î¾0) (cid:54)= 0.

Proof. Similar to the proof of Theorem H.46, except in the expression

âˆ‚3
Î·

Ëšf1(Î¾0) = âˆ’

L+1âˆ‚2

Î·Î³L(0) + ËšÎ¸(cid:48)

Lf âˆ‚2

Î·(Î³L

(cid:16)ËšÎ¸(cid:48)

(cid:17)
11Î³Lâˆ’1)(0)

,

Î·Î³L(0) and âˆ‚2
âˆ‚2
is because âˆ‚2
Proposition H.47.

Î·(Î³L

11Î³Lâˆ’1)(0) are both negative. The former is because of Theorem H.48. The latter
Î·Î³L
22 > 0 by

Î·Î³Lâˆ’1(0) â‰¤ 0 for the same reason, and âˆ‚2

11(0) < 0 since Ë™Îºl

13 < 0, Î³l

3, Î³l

H.7.4 Main Results

Proposition H.50. Suppose Ï† is tanh or Ïƒ-gelu for sufï¬ciently small Ïƒ. A pseudostable parametriza-
tion with r = 0 is nontrivial iff aL+1 + bL+1 = 1 or 2aL+1 + c = 1.

Proof. If aL+1 + bL+1 + r = 1 or 2aL+1 + c = 1, then Theorem H.46 and Theorem H.49 show that
the parametrization is nontrivial. Otherwise, it is trivial by Proposition H.25.

Theorem H.51. Suppose Ï† is tanh or Ïƒ-gelu for sufï¬ciently small Ïƒ. For any nontrivial pseudostable
parametrization with r = 0, the following are true of the parametrization:

1. not in kernel regime

2. feature learning

3. feature learning in the Lth layer

4. feature kernels evolution

5. feature kernel evolution in the Lth layer

6. prefeature learning

7. prefeature learning in the Lth layer

8. prefeature kernels evolution

9. prefeature kernel evolution in the Lth layer

10. if there is feature learning or feature kernel evolution or prefeature learning or prefeature
kernel evolution in layer l, then there is feature learning and feature kernel evolution and
prefeature learning and prefeature kernel evolution in layers l, . . . , L.

Ëšf1(Î¾0) (cid:54)= 0 by Theorem H.49 or
Proof. The parametrization cannot be in kernel regime since âˆ‚3
Î·
Theorem H.46. By Theorem H.45 or Theorem H.48, âˆ‚2
Î·Î»l(0) > 0 for all l â‰¥ (cid:96), so the feature kernel
evolves in layer (cid:96), . . . , L, for some normalized learning rate Î· > 0. This implies feature learning in
layer (cid:96), . . . , L, since Z xL
0 (cid:54)= 0, so we
have prefeature learning in layer (cid:96), . . . , L. Prefeature kernel evolution in layer (cid:96), . . . , L is implied by
Theorem H.43. Finally, the last statement follows clearly from our logic above.

0 (cid:54)= 0 in this case. This then implies Z hL

1 (Î¾0) âˆ’ Z hL

1 (Î¾0) âˆ’ Z xL

62

Corollary H.52. Suppose Ï† is tanh or Ïƒ-gelu for sufï¬ciently small Ïƒ. Consider any initialization-
stable parametrization with r = 0. If aL+1 + bL+1 < 1 or 2aL+1 + c < 1, then the parametrization
is not stable.

0 âˆ†xL

Lf = n1âˆ’(aL+1+bL+1) â†’ âˆ but
Proof. First suppose aL+1 + bL+1 < 1 and 2aL+1 + c â‰¥ 1. Then Î¸(cid:48)
L+1 â‰¤ 1. As in the proof of Theorem H.46, there is some Î· (cid:54)= 0 such that E Z (cid:99)W L+1
ËšÎ¸(cid:48)
0 Z Î´xL
1 (Î¾0) =
a.s.âˆ’âˆ’â†’ R =â‡’
R for some R (cid:54)= 0. Therefore, by the Master Theorem, 1
Î´xL
1 (Î¾0)
1 (Î¾0)| = Î˜(n1âˆ’(aL+1+bL+1)) â†’ âˆ. This dominates âˆ†W L+1
|W L+1
xL
1 (Î¾0), which by sim-
ilar reasoning is O(1). So f1(Î¾0) diverges and the parametrization is not stable.
Now suppose aL+1 + bL+1 â‰¥ 1 and 2aL+1 + c < 1. This violates our simplifying assumption
1 (Î¾0) a.s.âˆ’âˆ’â†’ âˆ’Î·ËšÏ‡0 E Z xL
0 Z xL
that aL+1 + bL+1 â‰¤ 2aL+1 + c, but itâ€™s easy to see that 1
xL
1 (Î¾0).
For Î· small enough, this is close to âˆ’Î·ËšÏ‡0 E(Z xL
xL
1 (Î¾0)| =
Î˜(n1âˆ’(2aL+1+c)) â†’ âˆ. This dominates W L+1
1 (Î¾0) = O(1), so f1(Î¾0) diverges. Therefore,
the parametrization is not stable.

0 )2 and thus is nonzero. Then |âˆ†W L+1

n Î´W L+1

0 âˆ†xL

n (cid:99)W L+1

1

1

1

0

Finally, suppose both aL+1 + bL+1, 2aL+1 + c < 1. If aL+1 + bL+1 (cid:54)= 2aL+1 + c, then we
have one of âˆ†W L+1
1 (Î¾0) dominate the other like the above, leading to
divergence. If aL+1 + bL+1 = 2aL+1 + c, then in the case of Ïƒ-gelu with small Ïƒ, W L+1
1 (Î¾0)
will dominate âˆ†W L+1
xL
1 (Î¾0), as in Theorem H.46; and in the case of tanh, both have the same sign,
as in Theorem H.49. In either case, f1(Î¾0) diverges, so the parametrization is not stable.

1 (Î¾0) and W L+1
xL

0 âˆ†xL

0 âˆ†xL

1

1

H.8 Putting Everything Together

Finally, in this section we tie all of our insights above to prove our main theorems.
Theorem H.53. Suppose Ï† is tanh or Ïƒ-gelu for sufï¬ciently small Ïƒ. A parametrization is stable iff
it is pseudostable.

Proof. The â€œifâ€ direction is given by Proposition H.26. We now show that when any (in)equality of
pseudostability is violated, the parametrization is not stable.

First, if Eq. (41) is not satisï¬ed, then Theorem H.19 shows lack of stability.

Second, if Eq. (41) is satisï¬ed but r < 0, then Proposition H.28 shows lack of stability.

Finally, if Eq. (41) is satisï¬ed and r â‰¥ 0 but aL+1 +bL+1 < 1 or 2aL+1 +c < 1, then Corollary H.52
or Corollary H.34 shows lack of stability.

Given this result, we will now just say â€œstableâ€ instead of â€œpseudostableâ€ from here on.
Theorem H.8 (Nontriviality Characterization). Suppose Ï† is tanh or Ïƒ-gelu for sufï¬ciently small Ïƒ.
A stable abc-parametrization is nontrivial iff aL+1 + bL+1 + r = 1 or 2aL+1 + c = 1.

Proof. The case of r = 0 and the case of r > 0 are resp. given by Proposition H.50 and
Corollary H.33.

Theorem H.13 (Classiï¬cation of abc-Parametrizations). Suppose Ï† is tanh or Ïƒ-gelu for sufï¬ciently
small Ïƒ. Consider a nontrivial stable abc-parametrization of an L-hidden layer MLP. Then

1. The following are equivalent to r = 0

(a) feature learning
(b) feature learning in the Lth layer
(c) feature kernels evolution
(d) feature kernel evolution in the Lth layer
(e) prefeature learning
(f) prefeature learning in the Lth layer
(g) prefeature kernels evolution
(h) prefeature kernel evolution in the Lth layer

63

2. The following are equivalent to r > 0

(a) kernel regime
(b) ï¬xes all features
(c) ï¬xes features in the Lth layer
(d) ï¬xes all feature kernels
(e) ï¬xes feature kernel in the Lth layer
(f) ï¬xes all prefeatures
(g) ï¬xes prefeatures in the Lth layer
(h) ï¬xes all prefeature kernels
(i) ï¬xes prefeature kernel in the Lth layer

3. If there is feature learning or feature kernel evolution or prefeature learning or prefeature
kernel evolution in layer l, then there is feature learning and feature kernel evolution and
prefeature learning and prefeature kernel evolution in layers l, . . . , L.

4. If r = 0, then for all Î¾ âˆˆ X , f0(Î¾) a.s.âˆ’âˆ’â†’ 0 and ft(Î¾) a.s.âˆ’âˆ’â†’ Ëšft(Î¾) for some deterministic Ëšft(Î¾).

However, the converse is not true.

5. If r > 0, aL+1 + bL+1 + r > 1 and 2aL+1 + c = 1, then we have the Neural Network-

Gaussian Process limit.

Proof. A nontrivial stable parametrization has either r = 0 or r > 0. By Theorem H.51,
Proposition H.27, and Theorem H.32, r = 0 implies all of the statements in (1) and r > 0 im-
plies all of the statements in (2). Consequently, if feature learning happens, then clearly r cannot be
positive, so r must be 0. Likewise, all of the statements in (1) imply r = 0. Symmetrically, all of the
statements in (2) about ï¬xing features imply r > 0. Finally, if the parametrization is in kernel regime,
then by Theorem H.51(1), r cannot be 0, so r > 0. This proves (1) and (2).

If the premise of (3) holds, then by the above, r = 0, so the conclusion follows from Theorem H.51.
This proves (3).
If r = 0, then nontriviality means aL+1 + bL+1 â‰¥ 1. This implies f0(Î¾) a.s.âˆ’âˆ’â†’ 0 for all Î¾ âˆˆ X (more
precisely, f0(Î¾) has standard deviation Î˜(n1/2âˆ’(aL+1+bL+1)) â†’ 0 by Central Limit Theorem). The
program describes the unconditional SGD trajectory of f (as opposed to the case when aL+1+bL+1 =
1/2), so ft(Î¾) a.s.âˆ’âˆ’â†’ Ëšft(Î¾) does not depend on f0. The converse is not true, for example because of
Corollary H.35. This prove (4).

(5) follows from Corollary H.35 (which actually allows much more general Ï†).

Proofs of Theorems 6.1, 6.3 and 6.4 For any ï¬nite subset X of the input space Rd (where
d = 1 here), we can write out the SGD computation as a Tensor Program like in Appendix H.4.
Then the Master Theorem implies the convergence of ft(Î¾) a.s.âˆ’âˆ’â†’ Ëšft(Î¾) for every Î¾ âˆˆ X . Let
X1 âŠ† Â· Â· Â· âŠ† Xk âŠ† Â· Â· Â· be an inï¬nite chain of ï¬nite subsets of Rd such that (cid:83)
k Xk is a dense subset of
Rd. Then the convergence of ft(Î¾) a.s.âˆ’âˆ’â†’ Ëšft(Î¾) holds for every Î¾ âˆˆ (cid:83)
k Xk (because we have almost
sure convergence). Finally, we apply a continuity argument to get this convergence for all of Rd:
Because Ï†(cid:48) and thus Ï† are pseudo-Lipschitz, they are locally Lipschitz (i.e. Lipschitz on any compact
set). In addition, the operator norms of W L are almost surely bounded from standard matrix operator
bounds. Thus one can see that the Tensor Program is locally Lipschitz in Î¾. Consequently, Ëšft(Î¾) is
continuous in Î¾. This allows to pass from (cid:83)
k Xk to Rd.

Proofs of Propositions 5.3, 5.5 and G.2 and Theorems G.3 and G.4 follow by dividing into
cases of r > 0 and r = 0 and easy modiï¬cation of the reasoning in Appendices H.6 and H.7.

Proof of Theorem H.17 follows from straightforward calculations. The basic outline of the
calculations is: 1) During pretraining, f â€™s change is purely due to a) the interaction betwen âˆ†W l, l â‰¤
L, and W L+1
, and b) the interaction between xL and âˆ†W L+1. 2) When W L+1 is re-initialized in
g, these interactions are killed. The pretrained âˆ†W l, l â‰¤ L, will cause xM to differ by Î˜(1/
n)
coordinatewise compared to if âˆ†W l, l â‰¤ L, are all reset to 0, but this difference is uncorrelated with

âˆš

0

64

the last layer weights W M +1 of g, so their interaction is subleading in n, i.e. in the inï¬nite-width
limit,

gT ;t(Î¾) âˆ’ g0;t(Î¾) a.s.âˆ’âˆ’â†’ 0,

whether all of g or just the new weights are trained during ï¬ntetuning.

65

