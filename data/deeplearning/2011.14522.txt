2
2
0
2

l
u
J

5
1

]

G
L
.
s
c
[

3
v
2
2
5
4
1
.
1
1
0
2
:
v
i
X
r
a

Feature Learning in Inﬁnite-Width Neural Networks

Greg Yang
Microsoft Research AI
gregyang@microsoft.com

Edward J. Hu∗
Microsoft Azure AI
edwardhu@microsoft.com

Abstract

As its width tends to inﬁnity, a deep neural network’s behavior under gradient
descent can become simpliﬁed and predictable (e.g. given by the Neural Tangent
Kernel (NTK)), if it is parametrized appropriately (e.g. the NTK parametrization).
However, we show that the standard and NTK parametrizations of a neural network
do not admit inﬁnite-width limits that can learn features, which is crucial for pre-
training and transfer learning such as with BERT. We propose simple modiﬁcations
to the standard parametrization to allow for feature learning in the limit. Using
the Tensor Programs technique, we derive explicit formulas for such limits. On
Word2Vec and few-shot learning on Omniglot via MAML, two canonical tasks
that rely crucially on feature learning, we compute these limits exactly. We ﬁnd
that they outperform both NTK baselines and ﬁnite-width networks, with the latter
approaching the inﬁnite-width feature learning performance as width increases.
More generally, we classify a natural space of neural network parametrizations
that generalizes standard, NTK, and Mean Field parametrizations. We show 1) any
parametrization in this space either admits feature learning or has an inﬁnite-width
training dynamics given by kernel gradient descent, but not both; 2) any such
inﬁnite-width limit can be computed using the Tensor Programs technique. Code
for our experiments can be found at github.com/edwardjhu/TP4.

Figure 1: PCA of Word2Vec embeddings of top US cities and states, for NTK, width-64, and width-∞
feature learning networks (Deﬁnition 5.1). NTK embeddings are essentially random, while cities and
states get naturally separated in embedding space as width increases in the feature learning regime.

1

Introduction

The study of inﬁnite-width limits of neural networks, in particular the Neural Tangent Kernel
(NTK), has recently solved many longstanding open problems on the optimization and generaliza-
tion of overparametrized neural networks [26]. However, in the NTK limit, (last layer) features
learned during pretraining are essentially the same as those from random initialization (Corollary 3.9
and Theorem H.13); this is veriﬁed empirically in Word2Vec in Fig. 1. As feature learning (e.g.
Imagenet and BERT) lies at the core of deep learning’s far-ranging impact so far [7, 13, 23], this
insight amounts to a fatal weakness of the NTK theory as a model of neural networks in practice.

We seek to capture feature learning in overparametrized networks by considering other parametriza-
tions and their inﬁnite-width limits. By slightly modifying the standard parametrization (SP), in fact,
we can enable feature learning that is maximal in a sense to be explained shortly. We describe how to
compute this limit exactly (and rigorously) via the Tensor Programs technique developed in [49–52].

∗Work done partly during the Microsoft AI Residency Program

NTKtypestatecityWidth 64Width  (Feature Learning) 
 
 
 
 
 
Feature Learning Inﬁnite-Width Networks on Real Tasks
We explicitly calculate this limit for the tasks of Word2Vec
[32, 33] and few-shot learning on Omniglot via MAML [16],2
two standard tasks relying crucially on feature learning. In
Word2Vec, an important early instance of large-scale language
pretraining, we must learn, in an unsupervised manner, word
embeddings so that similar words have close embeddings. Then
we test the learned embeddings on the word analogy task, which
asks questions of the kind “what to a queen is as a man to a
woman?” In few-shot learning, the model is asked to make
predictions given only a handful (e.g. 5) of labeled examples.
Metalearning/MAML makes this possible by having the model learn good representations of typical
examples that can adapt quickly, via a small number of SGD steps, to new few-shot learning tasks. On
both tasks, we ﬁnd our feature learning inﬁnite-width networks outperform both NTK baselines and
ﬁnite-width networks, with the latter approaching the inﬁnite-width performance as width increases.
Figure right shows this for one of our Word2Vec results. See Section 9 for our other experiments.

abc-Parametrizations This paper studies a natural class of parametrizations, which we call the
abc-Parametrization and describe here. Consider an L-hidden-layer perceptron: For weight matrices
W 1 ∈ Rn×d and W 2, . . . , W L ∈ Rn×n, and nonlinearity φ : R → R, such a neural network on
input ξ ∈ Rd is given by h1(ξ) = W 1ξ ∈ Rn, and

xl(ξ) = φ(hl(ξ)) ∈ Rn,

(1)
and the network output (also called the logit(s)) is f (ξ) = W L+1xL(ξ) for W L+1 ∈ R1×n. An
abc-parametrization is speciﬁed by a set of numbers {al, bl}l ∪ {c} such that

for l = 1, . . . , L − 1,

hl+1(ξ) = W l+1xl(ξ) ∈ Rn,

(a) We parametrize each weight as W l = n−al wl for actual trainable parameter wl
αβ ∼ N (0, n−2bl), and
(b) We initialize each wl
(c) The SGD learning rate is ηn−c for some width-independent η.3 4

Examples: The NTK parametrization (NTP) [26] has a1 = 0 and al = 1/2 for l ≥ 2; bl = 0 for all
l; c = 0. When depth L = 1, the Mean Field parametrization (MFP) [11, 30, 43, 45] has a1 = 0,
a2 = 1; bl = 0 for all l; c = −1. The standard parametrization (SP) available as the default setting in
PyTorch [39]5 has al = 0 for all l; b1 = 0 and bl = 1/2 for l ≥ 2; c = 0. However, we shall see that
c is too small (learning rate too large) in SP. We can deﬁne abc-parametrization and generalize our
results to arbitrary neural architectures (Appendix C), but we shall focus on MLPs in the main text.

Dynamical Dichotomy For any abc-parametrization, if c is too small (i.e. learning rate too large),
SGD can lead to blowup of preactivation and/or logits; we say this parametrization is unstable. In
practice this translates to numerical issues. If c is too large (i.e. learning rate too small), then the
function computed by the network does not change in ﬁnite time; we say this parametrization is
trivial. We prove what we call the Dynamical Dichotomy theorem (Corollary 3.9):

Any nontrivial stable abc-parametrization yields a (discrete-time) inﬁnite-width limit.
This limit either 1) allows the embedding xL(ξ) to evolve nontrivially (Deﬁnition 3.5) or
2) is described by kernel gradient descent in function space (Deﬁnition 3.7), but not both.

We call the former kind a feature learning limit and the latter a kernel limit. For 1-hidden-layer MLPs,
the former is exempliﬁed by MFP, and the latter, NTP. This dichotomy implies that certain functional
dynamics, such as higher order generalizations of the NTK dynamics, are not valid inﬁnite-width
limits (see Remark 3.12). In addition, the neural network function f (deﬁned in Eq. (1)) in any feature
learning limit must be identically 0 at initialization (see Corollary 3.10).6

2Short for Model Agnostic Meta-Learning
3Observe that by changing al, bl while holding al + bl ﬁxed, we effectively give layer l its own learning rate.
4One can further include a set of constants in front of n−al and n−bl , for example powers of input dimension

d, but we shall keep it simple here as we are only concerned with scaling behavior with n.

5This is also known as the “fanin” or “Lecun” initialization; “Kaiming” initialization is the same up to
multiplicative constants. The default in Tensorﬂow [1] uses Glorot initialization, where the variance of an entry
scales like 1/(f anin + f anout). This causes the ﬁrst layer preactivation to converge to 0 as n → ∞, and thus
yields pathological behavior in the limit.

6We stress this is in the n → ∞ limit, so does not contradict the feature learning seen in ﬁnite-width SP NN.

2

51015epoch010203040word analogy accword2vec pretrained on text8log2(width)6.08.010.0NTK/GPStandard Param. Does Not Learn Features We
show that
the SP (resp. NTP) can only allow
O(1/width) (resp. O(1)) learning rate (i.e. c = 1,
resp. c = 0), so as to avoid blowup, and yield kernel
limits (Section 4). Instead, we propose a parametriza-
tion that has Θ(1) max learning rate and admits fea-
ture learning maximally: it allows every parameter
to be updated maximally (in terms of scaling with
width) without leading to blowup (Section 5). We
thus call it the Maximal Update Parametrization (ab-
breviated MUP or µP). It is given by a1 = −1/2,
aL+1 = 1/2, and al = 0 for all 2 ≤ l ≤ L; bl = 1/2
for all l; c = 0. In a 1-hidden-layer MLP, this spe-
cializes to MFP, up to symmetry (see Eq. (5)). The
“feature learning limits” mentioned above in our main
experiments are µP limits. Figure to the right: We
empirically verify our max learning rate predictions
on relu MLP with 2 hidden layers, trained with square loss on CIFAR10. We plot learning rate vs
accuracy in each subplot. Each curve represents MLP with a speciﬁc width. The right edge of each
curve indicates the max learning rate. The diagonal subplots scale the x-axes (log learning rate) in
the correct width-scaling for the corresponding parametrizations. We see, indeed, max learning rate
for SP scales like 1/width but is constant in µP.

Key Theoretical Idea: Tensor Programs
In
Section 7 and Appendix H.4, we describe the
Tensor Programs technique for deriving (rigor-
ously) the inﬁnite-width training dynamics of
any abc-parametrization. The main insight of
this approach is:

When width is large, every activation vec-
tor has roughly iid coordinates, at any time
during training. Using Tensor Programs,
we can recursively calculate such coordi-
nate distributions, and consequently un-
derstand how the neural network function
evolves.

The Tensor Programs technique was developed
in a series of papers [49–52] that proved the architectural universality of the Neural Network-Gaussian
Process (NNGP) Correspondence and the Neural Tangent Kernel (NTK) limits and showed how to
compute the corresponding inﬁnite-width kernels. In the Figure above, the NNGP kernel can be
thought of as the “limit” of the ﬁrst forward pass of a randomly initialized model; the NTK can be
similarly thought of as the “limit” of its ﬁrst backward pass. The mechanics of calculating such limits
is 1) to write down the relevant neural network computation (e.g. the ﬁrst forward pass in the NNGP
case) as a principled composition of matrix multiplication and coordinatewise nonlinearities, called
a Tensor Program, and 2) to recursively calculate the distribution of coordinates of each vector via
what’s called the Master Theorem. In this paper, we follow the exact same recipe, where in 1) we just
write down the entire SGD training instead of only the ﬁrst step. More generally,

To derive the inﬁnite-width limit of any neural computation (e.g. SGD training),
1) express it as a Tensor Program, and 2) mechanically apply the Master Theorem.

For example, we easily recover the (discrete-time) 1-hidden-layer mean ﬁeld limit (Theorem 6.1).
It readily applies to practically any neural architecture (e.g. ResNet and Transformers)7 as well as
many common variants of SGD; however, in this paper, for pedagogical clarity, we only focus on
multilayer perceptrons. The generality of our approach allows us to easily adapt to settings outside the
traditional (CIFAR10-style) supervised classiﬁcation, such as the Word2Vec and few-shot learning
tasks in this paper, or reinforcement learning and image generation outside of our scope.

7e.g. by extending the example programs of [49, 51], which express only the ﬁrst forward and backward

passes, into the entire training computation.

3

10500.10.20.30.40.5Maximal Update Paramvalid accwidth512102420484096819205100.10.20.30.40.5max lr shifts1050log2(lr)0.10.20.30.40.5Standard Paramvalid accmax lr shifts0510log2(lr*width)0.10.20.30.40.5Verifying Max Learning Rate for P and SP1stforward pass1stbackward pass2ndforward pass2ndbackward passTthforward passTthbackward passSGD Training ProgressNNNNGPNTKFeature Learning LimitTake 𝑤𝑖𝑑𝑡ℎ→∞Limit via Tensor ProgramsPrior worksThis workOur Contributions

1. Formulate a natural space of NN parametrizations (abc-parametrizations).

2. Prove Dynamical Dichotomy: Any nontrivial stable abc-parametrization yields either feature

learning or kernel limits, but not both.

3. Show both NTK and standard parametrizations yield kernel limits and propose the Maximal
Update Parametrization (µP) , which admits maximal feature learning in a suitable sense.

4. Use Tensor Programs to derive the inﬁnite-width limit of µP and, more generally, the limit

of any abc-parametrization. We verify our theory using extensive experiments.

5. Show the µP limit outperforms both NNGP/NTK baselines and ﬁnite networks on 1)

Word2Vec and 2) Omniglot few-shot learning, trained via ﬁrst-order MAML.

Tensor Programs Series While this work is self-contained, it is positioned as the 4th paper in the
series, following Yang [49, 51, 52]. We do not extend the Tensor Programs machinery further here,
but instead extract the ﬁrst major payoff of the foundation laid in the earlier works. In fact, this paper
is the original motivation for this series; for a short history, see Appendix A.

2 Related Works

Comparison with Mean Field Limits For 1-hidden-layer MLP, the mean ﬁeld limit [11, 30, 43,
45] is equivalent to the µP limit modulo the symmetry of Eq. (5) (see Section 3.1). Several works also
proposed different versions of mean ﬁeld frameworks for deeper MLPs [5, 15, 34, 35, 46]. However,
they did not consider the typical Gaussian N (0, 1/n) random initialization (or the appropriately
rescaled version in their respective parametrizations)8, which has a Central-Limit effect as opposed
to a Law-of-Large-Numbers effect. For example, [5, 35] can cover the case of N (0, 1/n2), instead
of N (0, 1/n), initialization, which in fact causes the function to be stuck at initialization. See
Appendix E for more explanations. Of these works, the mean ﬁeld limit of [15] has the form
most similar to what we derive here. There, as we do here, the coordinate distribution of each
(pre)activation vector is tracked recursively. The main difference is, while [15] has an atypical
initialization involving (cid:96)2 regression, we consider the usual Gaussian N (0, 1/n) scheme. Such a
(size n×n) Gaussian matrix in the middle of the network has a distinctly different effect, more similar
to that of a Gaussian matrix in the usual NNGP/NTK calculation,9 than the “mean ﬁeld” matrices
considered in [15] and previous works [5, 34, 35, 46], which has an “integral kernel” effect that is the
straightforward generalization of matrices to function spaces. Nevertheless, discrete time versions of
the 1-hidden-layer mean ﬁeld limit and of many of the multilayer limits (such as [15, 35]) can be
derived directly by writing the corresponding initialization and training inside a Tensor Program and
applying the Master Theorem (Theorem 7.4).

Discrete- vs Continuous-Time Gradient Descent At a high level, there are two natural limits
of neural networks training dynamics: large-width and continuous-time. Most prior works on
inﬁnite-width limits of neural networks also took the continuous-time limit simultaneously, e.g.
[11, 26, 30, 43, 45]. In contrast, here we only take the large width limit, so that gradient descent stays
discrete-time. Then the results of these prior works can be recovered by taking another continuous-
time limit. From a practical perspective, the continuous-time limit is often unnatural, e.g. 1) because
the step size is usually as large as possible to speed up training, 2) because of the task (such as
reinforcement learning), or 3) because of the importance of hyperparameters like batch size that are
hidden away in such limits. On the theory side, taking the continuous-time limit can create issues
with 1) well-posedness and 2) existence and uniqueness of the resulting ODE/PDE. While they can
sometimes be proved to hold, they are artifacts of the continuous-time limit, as the corresponding
questions for the discrete time evolution are trivial, and thus not relevant to the behavior of real
networks.

8In fact, empirically we observe such Gaussian random initialization to be crucial to performance compared

to the mean-ﬁeld-style initialization in this literature.

9Actually, it is more similar to the Gaussian matrix in asymmetric message passing [6] in that care must be

taken to keep track of correlation between W and W (cid:62).

4

Technical Assumptions Earlier works on neural tangent or mean ﬁeld limits (e.g. [11, 15, 26,
30, 35, 43, 45]) assume various forms of regularity conditions, such as 1) 0th, 1st, and/or 2nd
order smoothness on the nonlinearity or other related functions, and 2) the support boundedness,
subgaussianity, and/or PDF smoothness of initialization distributions. These are often either unnatural
or difﬁcult to check. In our work, the only assumption needed to rigorously obtain the inﬁnite-width
limit is that the nonlinearity φ has a polynomially bounded weak 2nd derivative and that the loss
function has a continuous derivative w.r.t. the prediction (Assumption H.22). In particular, when
we specialize to the 1-hidden-layer case and derive the discrete time version of the mean ﬁeld limit,
we cover the standard Gaussian initialization; in fact, we can allow any heavy-tailed initialization
that can be written as the image of a Gaussian under a pseudo-Lipschitz function, which include
nonsmooth PDFs and singular distributions.10 This generosity of technical assumptions is due to that
of the Tensor Programs Master Theorems proven in [49, 51, 52].

Training Time Many prior works (e.g. [4, 25, 30]) derived explicit time dependence of the con-
vergence to inﬁnite-width limit, so that a larger width can allow the network to stay close to the
limit for longer. In this paper, our results only concern training time independent of width, since
our primary objective is to investigate the limit itself and its feature learning capabilities. Moreover,
recent evidence suggests that, given a ﬁxed computational budget, it’s always better to train a larger
model for a shorter amount of time [29], which validates the practical relevance of our limit mode.
Nevertheless, it is possible to prove a quantitative version of the Tensor Programs Master Theorem,
by which one can straightforwardly allow training time to increase with width.

Classiﬁcation of Parametrizations
[10] pointed out that the weights move very little in the NTK
limit, so that linearization approximately holds around the initial parameters, in contrast to the mean
ﬁeld limit (for 1-hidden-layer networks) where the weights move substantially. For this reason, they
called the former “lazy training” and the latter “active training,” which are classiﬁed nonrigorously by
a multiplicative scaling factor of the logit (similar to n−aL+1 in this paper). While these terms are not
formally deﬁned, they intuitively correspond to the kernel and feature learning regimes in our paper.
From a different perspective, [31] observed that the NTK and mean ﬁeld limit can be thought of as
short and long time-scale regimes of the mean ﬁeld evolution equations. Neither of the above works
attempted to formally classify natural parametrizations of neural networks. In contrast, [48] studied a
toy class of neural networks in the context of implicit regularization due to the scale α of initialization
(which is closely related to logit multiplier of [10] noted above). They identiﬁed the α → ∞ limit (of
the scale α, not of width) with the “kernel regime” and the α → 0 limit with what they call the “rich
regime”. They showed that the former is implicitly minimizing an (cid:96)2 risk while the latter, an (cid:96)1 risk.
They claim width allows the toy model to enter the kernel regime more naturally, but as we see in this
work, both kernel and feature learning regimes are admissible in the large width limit of a standard
MLP. Closer to our approach, [19] studied what amounts to a 2-dimensional subspace of the space of
stable abc-parametrizations for L = 1. They proposed a notion of stability which is similar to the
combination of stability and nontriviality in this paper. They characterized when the Neural Tangent
Kernel, suitably generalized to any parametrization and playing a role similar to the feature kernel
in this paper, evolves over time. However, to simplify the proofs, they assumed that the gradients
for the different weight matrices are estimated using different inputs, a very unnatural condition.
In contrast, here our results are for the usual SGD algorithm applied to MLPs of arbitrary depth.
In all of the above works and most of existing literature, not much attention is paid to the feature
learning capabilities of neural networks in the right parametrization, as opposed to our focus here. A
notable exception is [12], which showed that the mean ﬁeld limit, but not the NTK limit, can learn
low dimension linear structure of the input distribution resulting in ambient-dimension-independent
generalization bounds.

Other Related Works
[27] proposed a toy model to study how large learning rate can induce
a neural network to move out of the kernel regime in Ω(log(width)) time. Since our dichotomy
result only concerns training for O(1) time (which, as we argue above, is more practically relevant),
there is no contradiction. [47] also noted that standard parametrization leads to unstable training
dynamics. They then injected constants in the NTK parametrization, such as α/
n
and tuned α in the resulting kernel. [2, 3] also observed the lack of feature learning in NNGP and
NTK limits but, in contrast to taking the exact limit of SGD training as we do here, they proposed a
deep kernel process as a way of loosely mimicking feature learning in ﬁnite-width networks. [17]

n instead of 1/

√

√

10We won’t expand further here, but it can be derived straightforwardly from the Master Theorem

(Theorem 7.4).

5

empirically observed that wider networks achieve better downstream performance with linear transfer
learning, even though on the original pretraining task there can be little difference. We ﬁx the input
dimension d in this work, but one can also consider varying d with width n, e.g. [36, 38]. [28] proved
a complexity separation between NTK and ﬁnite-width networks by showing the latter approximates
a sort of inﬁnite-width feature learning network. In the literature surrounding NTK, often there are
subtle differences in parametrization leading to subtle differences in conclusion (e.g. [4, 14, 57]).
Our abc framework encapsulates all such parametrizations, and can easily tell when two ostensibly
different parametrizations (e.g. [14, 57]) are actually equivalent or when they are really different (e.g.
[4, 14]) via Eq. (5).

3 Feature Learning vs Kernel Behavior

In this section, we give a characterization of training procedures that induce feature learning vs kernel
behavior; we will elaborate on what we mean by these two kinds of behavior below. We ﬁrst motivate
this discussion by reviewing the well-known tangent kernel and mean ﬁeld limits of a shallow neural
network.

3.1 Motivating Examples: Neural Tangent Kernel and Mean Field Limits

For simplicity, deﬁne a shallow network f (ξ) with input/output dimension 1 by

f (ξ) = V x(ξ) ∈ R,

x(ξ) = φ(h(ξ)) ∈ Rn,

h(ξ) = U ξ ∈ Rn.

(2)

As a specialization of Eq. (1), we parametrize weights V = n−av v ∈ R1×n and U = n−au u ∈ Rn×1,
where the width n should be thought of as tending to ∞, and v, u should be thought of as the actual
trainable parameters. We will sample vα ∼ N (0, n−2bv ), uα ∼ N (0, n−2bu ) for α ∈ [n]. The
learning rate is ηn−c for some η independent of n.

For example, in the Neural Tangent Parametrization (abbreviated NTP) [26], au = bv = bu = 0,
av = 1/2, c = 0. The Mean Field Parametrization (abbreviated MFP) corresponds to av = 1,
au = bu = bv = 0, c = −1; however, as will be explained shortly, we will use the equivalent
formulation au = −1/2, av = bu = bv = 1/2, c = 0 in this section so c = 0 for both NTP and MFP.
We remark that the GP limit, i.e. training only the last layer of a inﬁnite-wide, randomly initialized
network, is a special case of the NTK limit where the ﬁrst layer is not trained. Everything we discuss
below about the NTK limit specializes to the GP limit appropriately.

Given an input ξ, the gradient of f can be calculated as

dx(ξ) = V,

dh(ξ) = dx(ξ) (cid:12) φ(cid:48)(h(ξ)),

dv(ξ) = n−av x(ξ),

du(ξ) = n−audh(ξ)ξ

where d • (ξ) is shorthand for ∇•f (ξ) (however, note that later in Section 6, d • (ξ) will stand for
n∇•f (ξ)). For loss function L : R × R → R, the loss gradient on a pair (ξ, y) is then given by
L(cid:48)(f (ξ), y)[dv(ξ), du(ξ)] (where L(cid:48) denotes derivative in ﬁrst argument).

Note that one can keep the function f invariant while changing the magnitude of the gradient dv by
changing av, bv, holding av + bv constant; likewise for du. Thus, the trajectory of f stays ﬁxed if,
for any θ ∈ R, we set au ← au + θ, av ← av + θ, bu ← bu − θ, bv ← bv − θ, c ← c − 2θ (also see
Eq. (5)). With θ = −1/2, this explains why the two formulations of MFP above are equivalent. Then,
for both NTP and MFP, we will consider the dynamics of f trained under stochastic gradient descent
with learning rate η = 1 and batch size 1, where the network is fed the pair (ξt, yt) at time t, starting
with t = 0. This simplicity is intended to intuitively illustrate our points below, but we shall state
formal results regarding more common settings in Section 3.2.

Notation and Setup Below, when we say a (random) vector v ∈ Rn has coordinate size O(na)
(written v = O(na)),11 we mean (cid:112)(cid:107)v(cid:107)2/n = O(na) with high probability for large n. Intuitively,
this means that each coordinate has a typical ﬂuctuation of O(na). Likewise if O(na) is replaced
with Θ(na) or Ω(na). See Deﬁnition H.2 for a formal deﬁnition.

Let ft, ht, xt, Ut, Vt, dxt, dht, dvt, dut denote the corresponding objects at time t, with t = 0 corre-
sponding to random initialization. We also abuse notation and write xt = xt(ξt), i.e. applying the
function xt speciﬁcally to tth input ξt; similarly for ft, ht, dxt, dht, dvt, dut. These symbols will

11Contrast this with a common semantics of v = O(na) as (cid:107)v(cid:107) = O(na).

6

never appear by themselves to denote the corresponding function, so this should cause no confusion.
Then SGD effectively updates U and V by

Ut+1 = Ut − χtn−au dut,

Vt+1 = Vt − χtn−av dvt.

where χt
example, after 1 SGD update, we have, for any ξ ∈ R,

def= L(cid:48)(ft, yt). Finally, let ∆•t

def= •t − •0, for all • ∈ {f, h, x, U, V, dx, dh, dv, du}. For

∆h1(ξ) = h1(ξ) − h0(ξ) = −n−au χ0ξdu0 = −n−2au χ0ξ0ξdh0

= −n−2au χ0ξ0ξdx0 (cid:12) φ(cid:48)(h0)

∆f1(ξ) = V0∆x1(ξ) + ∆V1x1(ξ) = V0∆x1(ξ) − n−av dv(cid:62)

0 x1(ξ)

= V0∆x1(ξ) − n−2av x(cid:62)

0 x1(ξ)

(3)

(4)

3.1.1 Key Observations

Let’s list a few characteristics of the NTK and MF limits in the context of the shallow network in
Eq. (2), and then discuss them in the general setting of deep MLP. We will keep our discussion
intuitive to carry across the key ideas.

Feature Evolution For a generic ξ ∈ R, its embedding vector x0(ξ) has coordinates of Θ(1) size
in both NTP and MFP. However, for any t ≥ 1 independent of n, ∆xt(ξ) generically has coordinate
size Θ(1/

n) in NTP but Θ(1) in MFP.

√

(in NTP)

(in NTP)

Example for t = 1: By Eq. (3), we have

∆h1(ξ) = n−2au χ0ξ0ξdx0 (cid:12) φ(cid:48)(h0).

Plug in au = 0 for NTP. Observe that ξ0, ξ, χ0 = Θ(1),12 so

∆h1(ξ) = Θ(1) · dx0 (cid:12) φ(cid:48)(h0).

In addition, φ(cid:48)(h0) = Θ(1) because h0 = Θ(1), so

Finally, dx0 = V0 = Θ(1/

√

∆h1(ξ) = Θ(1) · dx0 (cid:12) Θ(1).

n) in NTP. Altogether, this implies

∆h1(ξ) = Θ(1/
=⇒ ∆x1(ξ) ≈ φ(cid:48)(h0(ξ)) (cid:12) ∆h1(ξ) = Θ(1/

√

n)

√

n) → 0,

as n → ∞.

(in NTP)

On the other hand, in MFP, the only thing different is au = −1/2 and dx0 = Θ(1/n), which implies
(in MFP)

∆h1(ξ) = Θ(n) · Θ(1/n) (cid:12) Θ(1) = Θ(1) =⇒ ∆x1(ξ) = Θ(1).

Feature Kernel Evolution Therefore the feature kernel Ft(ξ, ζ) def= xt(ξ)(cid:62)xt(ζ)/n does not
change in the NTK limit but it does in the MF limit, i.e. for any ﬁxed t ≥ 1,13

lim
n→∞
lim
n→∞

Ft(ξ, ζ) = lim
n→∞
Ft(ξ, ζ) (cid:54)= lim
n→∞

F0(ξ, ζ),

in NTP, but

F0(ξ, ζ),

in MFP, in general.

Indeed, regardless of parametrization, we have

(cid:2)x0(ξ)(cid:62)x0(ζ) + ∆xt(ξ)(cid:62)x0(ζ) + x0(ξ)(cid:62)∆xt(ζ) + ∆xt(ξ)(cid:62)∆xt(ζ)(cid:3) .

Ft(ξ, ζ) =

1
n

In NTP, because ∆xt(ξ) = Θ(1/

√

n) as noted above,
n
(cid:88)

∆xt(ξ)αx0(ζ)α =

1
n

∆xt(ξ)(cid:62)x0(ζ) =

1
n

α=1

1
n

n
(cid:88)

α=1

O(n−1/2) = O(n−1/2),

and likewise the other terms involving ∆xt will vanish as n → ∞. But in MFP, ∆xt(ξ) = Θ(1) will
in general be correlated with x0(ζ) such that 1
It may seem somewhat puzzling how the NTK limit induces change in f without feature or feature
kernel evolution. We give some intuition in Appendix B.

n ∆xt(ξ)(cid:62)x0(ζ) = 1

α=1 Θ(1) = Θ(1).

(cid:80)n

n

12χ0 = L(cid:48)(f0, y0) = Θ(1) because f0 has variance Θ(1).
13here the limit should be construed as almost sure limits; see Theorem 7.4.

7

Pretraining and Transfer Learning The simple fact above about the feature kernel K implies
that the NTK limit is unable to perform linear transfer learning. By linear transfer learning, we mean
the popular style of transfer learning where one discards the pretrained linear classiﬁer layer and
train a new one on top of the features (e.g. x in our example), which are ﬁxed. Indeed, this is a linear
problem and thus only depends on the kernel of the features. If this kernel is the same as the kernel at
initialization, then the pretraining phase has had no effect on the outcome of this “transfer” learning.

In fact, a more sophisticated reasoning shows pretraining in the NTK limit is no better than random
initialization for transfer learning even if ﬁnetuning is performed to the whole network, not just
the classiﬁer layer. This remains true if we replace the linear classiﬁer layer by a new deep neural
network. See Remark H.16 and Theorem H.17. The Word2Vec experiment we do in this paper is a
linear transfer task.

In some other settings, such as some settings of metalearning, like the few-shot learning task in
this paper, the last layer of the pretrained network is not discarded. This is called adaptation. Then
the NTK limit does not automatically trivialize transfer learning. However, as will be seen in our
experiments, the NTK limit still vastly underperforms the feature learning limit, which is exempliﬁed
by the MF limit here.

In NTP, as n → ∞, (cid:104)∇U,V f0(ξ), ∇U,V f0(ζ)(cid:105)
Kernel Gradient Descent in Function Space
converges to some deterministic value K(ξ, ζ) such that K forms a kernel (the NTK). Then, in
this limit, if the learning rate is η, the function f evolves according to kernel gradient descent
ft+1(ξ) = ft(ξ) − ηK(ξ, ξt)χt. However, this shouldn’t be the case for the MF limit. For example,
if φ is identity, then intuitively ft+1(ξ) − ft(ξ) should be quadratic in η, not linear, because two
layers are updated at the same time.

3.2

abc-Parametrizations and Dynamical Dichotomy

In this section, we broaden our scope to the abc-parametrizations of deeper MLPs, deﬁned by Eq. (1),
and their inﬁnite-width limits. In Table 1, we summarize the {al, bl}l ∪ {c} values of various
abc-parametrizations in the literature.
Assumption 3.1. Our main results in this section (and this section only) will assume φ is either tanh
or a smooth version of relu called σ-gelu (see Deﬁnition H.1), for sufﬁciently small σ > 0 (which
means σ-gelu approximates relu arbitrarily well).

Note this assumption is only needed for the classiﬁcation of abc-parametrizations. For deriving the
inﬁnite-width limits, the much weaker Assumption H.22 sufﬁces. We believe our results here will
hold for generic nonlinearities, but making this precise is outside our scope. (See Remark H.15 for an
overview on how Assumption 3.1 is used).

Symmetries of abc-Parametrizations As above, we can scale the parameter gradients ∇wl f
arbitrarily while keeping f ﬁxed, if we vary al, bl while ﬁxing al + bl: ∇wl f is scaled by n−θ if
al ← al + θ, bl ← bl − θ. In other words, changing al, bl this way effectively gives wl a per-layer
learning rate. If we apply this gradient with learning rate ηn−c, then the change in W l is scaled by
ηn−c−2θ. Consequently, if c ← c − 2θ, then W l is not affected by the change in al, bl. In summary,
∀θ ∈ R : ft(ξ) stays ﬁxed for all t and ξ if we set al ← al + θ, bl ← bl − θ, c ← c − 2θ.

(5)

Stable abc-Parametrizations We will only consider abc-parametrizations such that, as n → ∞,
1) the preactivations {hl}l and activations {xl}l have Θ(1) coordinates at initialization, and 2) their
coordinates and the logit f (ξ) all stay O(1) throughout the course of SGD.14 Otherwise, they tend
to ∞ with n, eventually going out of ﬂoating point range. Indeed, this is an acute and real problem
common in modern deep learning, where ﬂoat16 is necessary to train large models. We call any such
parametrization stable (see Deﬁnition H.4 for a formal deﬁnition). Thus unstable parametrizations
are of no practical interest.

It turns out stable abc-parametrizations can be characterized by a set of inequalities on {al, bl}l ∪ {c}
(so that the stable ones form a polyhedron). To present these inequalities succinctly, it’s useful to
deﬁne

14but they may depend on training time and η; in particular, it’s possible that they diverge with time.

8

Table 1: We summarize the abc values of SP (standard), NTP (Neural Tangent), MFP (Mean Field,
for 1-hidden-layer nets), µP (Maximal Update, ours). We show the minimal value of c such that the
parametrization is stable (Deﬁnition H.4). We also list the quantities r, 2aL+1 + c, aL+1 + bL+1 + r
involved in stability, feature learning, and kernel regime properties of the parametrizations. Here we
only focus on scaling with n and ignore dependence on input dimension. Recall the MLP deﬁnition:
h1 = W 1ξ ∈ Rn, xl = φ(hl) ∈ Rn, hl+1 = W l+1xl ∈ Rn, f (ξ) = W L+1xL

Deﬁnition

SP (w/ LR 1
n )

NTP

MFP (L = 1)

µP (ours)

al

W l = n−al wl

0

bl wl

αβ ∼ N (0, n−2bl )
LR = ηn−c
Deﬁnition 3.2

c
r

2aL+1 + c
aL+1 + bL+1 + r
Nontrivial?
Stable?
Feature Learning?
Kernel Regime?

(cid:26)0
1/2

l = 1
l ≥ 2

1
1/2

1
1
(cid:88)
(cid:88)

(cid:88)

(cid:26)0
1/2

l = 1
l ≥ 2

(cid:26)0
1

l = 1
l = 2


−1/2

0

1/2

l = 1
2 ≤ l ≤ L
l = L + 1

0

0
1/2

1
1
(cid:88)
(cid:88)

(cid:88)

0

−1
0

1
1
(cid:88)
(cid:88)
(cid:88)

1/2

0
0

1
1
(cid:88)
(cid:88)
(cid:88)

Deﬁnition 3.2. For any abc-parametrization, we write r for the quantity

r def= min(aL+1 + bL+1, 2aL+1 + c) + c − 1 +

L
min
l=1

[2al + I(l = 1)] .

For example, in NTP, r = 1/2, while in MFP (when L = 1), r = 0. Intuitively, r is the exponent
t (ξ) = Θ(n−r). Thus, to avoid activation blowup, we want r ≥ 0; to perform feature
such that ∆xL
learning, we want r = 0.

Theorem 3.3 (Stability Characterization, c.f. Theorem H.6). An abc-parametrization is stable iff all
of the following are true (with intuitions in parentheses):

1. ((pre)activations xl

0, hl

0 at initialization are Θ(1) and logits f0 are O(1))

a1 + b1 = 0;

al + bl = 1/2, ∀l ∈ [2, L];

aL+1 + bL+1 ≥ 1/2.

(6)

2. (features don’t blowup, i.e. ∆xl

t = O(1) for all l)

r ≥ 0.

3. (logits don’t blow up during training, i.e. ∆W L+1

t

t , W L+1
xL

0 ∆xL

t = O(1))

2aL+1 + c ≥ 1;

aL+1 + bL+1 + r ≥ 1.

(7)

(8)

Nontrivial abc-Parametrizations Among stable abc-parametrizations, there are also those where
f does not change throughout training in the inﬁnite-width limit. We say such parametrizations are
trivial. Our dichotomy result will only apply to nontrivial stable abc-parametrizations.15

Nontrivial abc-parametrizations can also be described by a disjunction of equations on {al, bl}l ∪
{c} (geometrically, they correspond to the union of two faces on the polyhedron of stable abc-
parametrizations).
Theorem 3.4. A stable abc-parametrization is nontrivial iff aL+1 + bL+1 + r = 1 or 2aL+1 + c = 1.

15In particular, it’s possible for the function f to stay ﬁxed with time, but for the features to change.

9

Feature Learning Below, for brevity, we say training routine to mean the package of learning
rate ηn−c, training sequence {(ξt, yt)}t≥0,16 and a loss function L(f (ξ), y) that is continuously
differentiable in the prediction of the model f (ξ). As above, we use •t to denote the object • after t
steps of SGD.
Deﬁnition 3.5 (c.f. Deﬁnitions H.9 and H.11). We say an abc-parametrization admits feature
learning (resp. evolves the feature kernel) if, as n → ∞, ∆xL
t (ξ) has Ω(1) coordinates (resp.
1
n (xL
0 (ζ)) = Ω(1)) for some training routine, time t ≥ 1, and input ξ (resp.
ξ, ζ).17

t (ζ) − xL

0 (ξ)(cid:62)xL

t (ξ)(cid:62)xL

MFP, in the 1-hidden-layer case, is an example of feature learning parametrization.

Intuitively, feature kernel evolution implies feature learning, but a priori it seems possible that the
latter can occur without the former (akin to some kind of rotation of features). If so, then, e.g. in
terms of linear transfer learning, the pretraining ultimately had no beneﬁt. But, in fact,
Theorem 3.6. A nontrivial stable abc-parametrization admits feature learning iff it evolves the
feature kernel iff r = 0.

Kernel Regime While feature learning here is deﬁned by looking at the embedding of an input ξ,
we can also look at the dynamics of the function represented by the neural network.
Deﬁnition 3.7 (c.f. Deﬁnition H.12). We say an abc-parametrization is in kernel regime if there
exists a positive semideﬁnite kernel K such that, for any training routine, time t ≥ 0, and input ξ, in
the n → ∞ limit,

ft+1(ξ) = ft(ξ) − ηK(ξ, ξt)L(cid:48)(ft(ξt), yt),

∀t ≥ 0.

(9)

In other words, SGD reduces to kernel gradient descent in the large n limit.
Theorem 3.8. A nontrivial stable abc-parametrization is in kernel regime iff r > 0.

NTP is a typical example of this, where r = 1/2 and K is given by the NTK.

Dynamical Dichotomy Since a stable abc-parametrization
has either r = 0 or r > 0 by Eq. (7):
Corollary 3.9. A nontrivial stable abc-parametrization either
admits feature learning or is in kernel regime, but not both.

Note that kernel regime (Deﬁnition 3.7) is not deﬁned as lack
of feature learning, so Corollary 3.9 is not a trivial statement.
In addition, Assumption 3.1 is necessary. For example, if φ is
linear, then this dichotomy doesn’t hold, as a 1-hidden-layer
linear network where only the ﬁrst layer is trained would both
admit feature learning and is in kernel regime.

An interesting consequence of Dynamical Dichotomy is
Corollary 3.10. Any nontrivial stable feature learning abc-
parametrization must have limn→∞ f0(ξ) = 0 for all ξ, where
the limit is almost sure.

Theorems 3.6 and 3.8 and Corollary 3.10 are consequences of
the more general classiﬁcation theorem Theorem H.13, which
in addition shows: 1) feature learning in layer l would imply the
same for layers l, . . . , L; 2) in any feature learning parametriza-
tion, ft in the large n limit becomes deterministic, and thus is
incompatible with any Bayesian perspective (in contrast to the
NNGP limit).

Figure 2: A Caricature of abc-
Parametrizations. The nontrivial
stable parametrizations form a high
dimensional polyhedron. Those on
a part of its boundary admit feature
learning, while all others are in ker-
nel regime. µP is a vertex in the for-
mer, while NTP, latter. See Fig. 5
for a more geometrically accurate
depiction.

Dynamical Dichotomy in the shallow perceptron case is illustrated by the NTK and MF limits, as
presented in Section 3.1, which shows the NTK limit exempliﬁes Theorem 3.8 while the MF limit

16For simplicity, we only consider batch size 1; it’s straightforward to generalize to larger batch sizes.
17For the sake of streamlining the main text presentation, we deﬁned feature learning and feature kernel
evolution slightly differently than in Deﬁnition H.9, but ultimately they are equivalent as a result of our theorems.

10

KernelRegimeNeuralTangentStandard𝐿𝑅=Θ(1/𝑤𝑖𝑑𝑡ℎ)MaximalUpdate(ours)UnstableorTrivialSpace of abc-ParametrizationsMean Fieldwhen depth=1Standard𝐿𝑅=Θ(1)exempliﬁes Theorem 3.6. We present a simpliﬁed picture of abc-parametrizations in Fig. 2, but see
Fig. 5 for a more geometrically accurate depiction.

The paragraph above Appendix H.2 gives a quick outline of the proof of Dynamical Dichotomy, and
the beginning of each succeeding section outlines the logic of that section.
Remark 3.11 (Function Space Picture). A kernel regime limit resides solely in the function space
picture, i.e. the evolution of f at any time being solely determined by the function values {lim ft(ζ)}ζ
themselves (as opposed to the internal activations of f as well) along with η, L, and (ξt, yt). Intuitively,
this cannot be true for the feature learning limit, and therefore, at least informally, Dynamical
Dichotomy is also a dichotomy over the sufﬁciency of the function space picture for determining
the training evolution: We can construct two settings where {lim ft(ζ)}ζ, η, L, and (ξt, yt) are the
same but ft+1 are different. 1) The ﬁrst setting is at t = 0, where lim ft(ζ) = 0 for all input ζ by
Corollary 3.10. Here a typical SGD will change f . 2) In the second setting, suppose φ is relu. Design
a sequence of inputs such that training the MLP on them with very large learning rate will make all
relu neurons saturated in the 0 region. Then f is everywhere 0, and an SGD step will not change that.
Remark 3.12 (Not All Dynamics are Inﬁnite-Width Limits). Accordingly, a nonlinear function space
dynamics cannot be a valid inﬁnite-width limit of some abc-parametrization. By nonlinear, we mean
ft+1(ξ) − ft(ξ) is nonlinear in L(cid:48)(ft(ξt), yt). For example, any natural higher-order generalization
of Eq. (9) (perhaps derived from a Taylor expansion at initialization) is not a valid limit.18

Pretraining and Transfer Learning As in the shallow examples, Corollary 3.9 says that any
kernel regime parametrization (including NTP) trivializes pretraining and transfer learning19 in the
inﬁnite-width limit.

By calculating r for the standard parametrization (SP), we can easily see that it cannot admit feature
learning in the sense here without becoming unstable. However, in the next section, we will manually
analyze the training dynamics in an SP MLP to give an intuition why this is the case. In turn, we then
propose a simple modiﬁcation of SP, the Maximal Update Parametrization (MUP or µP), which does
admit feature learning and, in fact, does so maximally in a suitable sense. In the pedagogical spirit,
we will focus on the key insights and stress the right heuristics without dwelling on formal aspects.

4 Standard Parametrization

In this section, we give intuition for why gradient descent of neural network in standard parametriza-
tion (SP) will lead to logits blowup after 1 step, if the learning rate is ω(1/n), where n is the width.
In addition, we will see why, with learning rate O(1/n), SP is in kernel regime. We ﬁrst consider the
simplest example and then state the general result at the end of the section.

To demonstrate the general principle in deep networks, it is necessary to consider the behavior of an
n × n matrix in the middle of the network. Thus, the simplest case is a 2-hidden-layer linear MLP,
i.e. Eq. (1) with L = 2 and φ = id. The standard parametrization is given by

al = 0 ∀l,

b1 = 0,

bl = 1/2 ∀l ≥ 2.

(SP)

We consider 1 step of SGD with learning rate n−c on a single data pair (ξ, y). Then we can without
ambiguity suppress explicit dependence on ξ and write
¯h = W h,

f = V ¯h,

h = U ξ,

(10)

where Uαβ ∼ N (0, 1) and Wαβ, Vαβ ∼ N (0, 1/n) are the trainable parameters (simplifying the
notation in Section 3). As in Section 3, we use •t to denote the quantity • after t step of SGD.
Because we only focus on the 1st step of SGD, we lighten notation and write • = •0.

Initialization Since U, W, V are independently sampled, a standard Central Limit argument would
show that h, ¯h, f all have roughly iid Gaussian coordinates of variance Θ(1).

18It may seem that Neural Tangent Hierarchy [25], which allow some kind of higher order dynamics in the
function space, violates our observation. But their inﬁnite-width limit is identical to NTK in the constant time
t = O(1) regime, which is what Remark 3.12 (and this paper) concerns. Moreover, here we are talking about
functional dynamics that doesn’t depend on n (because we are already at the n → ∞ limit) whereas their
functional dynamics does.

19linear and nonlinear; see Theorem H.17.

11

First Gradient Now let’s consider the gradients of f on the data pair (ξ, y), which are given by

d¯h = V (cid:62),
dV = ¯h,

dh = W (cid:62)d¯h,
dW = d¯h h(cid:62) = V (cid:62)h(cid:62),

dU = dh ξ(cid:62).

(11)

For simplicity, suppose we only update W by learning rate n−c (and leave U, V unchanged); our
conclusion will not change in the general case where we train all layers. Then with χ denoting the
loss derivative L(cid:48)(f, y), we can write

W1 = W − n−cχ dW.

We shall show now that c ≥ 1 or else f1 blows up with the width n after this SGD step.

After First SGD Step At t = 1, h1 = h since we did not update U , but

¯h1 = W1h = ¯h − n−cχ dW h = ¯h − n−cχ · V (cid:62)h(cid:62)h
f1 = V ¯h1 = f − n−cχ V V (cid:62)h(cid:62)h.

(12)

(13)

Now, as noted above, h has iid Θ(1) coordinates, so h(cid:62)h = Θ(n) ∈ R. Similarly, V ∈ R1×n has
Gaussian coordinates of variance Θ(1/n), so V V (cid:62) = Θ(1) ∈ R. Finally, for typical loss function
L like MSE or cross entropy, χ = L(cid:48)(f, y) is of order Θ(1) because f ﬂuctuates on the order Θ(1).
Altogether,

f1 = f − Θ(n1−c).

Therefore, for f1 to remain O(1), we must have c ≥ 1, i.e. the learning rate is O(1/n).

Kernel Regime and Lack of Feature Learning Consequently, the network cannot learn features
in the large width limit if we would like the logits to not blow up. Indeed, this version of SGD where
only W is updated can be seen to correspond to the limit where

a1 = θ,

b1 = −θ,

a2 = 0,

b2 = 1/2,

a3 = θ,

b3 = −θ + 1/2,

θ → ∞.

With c = 1 as derived above, the parametrization is stable and nontrivial, as can be checked from
Theorems 3.3 and 3.4. Then we get r = 1/2 > 0, so by Corollary 3.9, this parametrization is in
kernel regime and does not admit feature learning. We can also see this directly from Eq. (12): from
our calculations above,

¯h1 − ¯h = O(n1−c) V (cid:62) = O(1) V (cid:62)
whose coordinates have size O(n−1/2) since V ’s coordinates do, so there’s no feature learning (at
least in the ﬁrst step). Finally, from Eq. (13), because V V (cid:62) → 1 and n−ch(cid:62)h = n−1h(cid:62)h → (cid:107)ξ(cid:107)2,
we get20

f1 − f → −χK(ξ, ξ) def= −χ(cid:107)ξ(cid:107)2,
i.e. f evolves by kernel gradient descent with the linear kernel. Our derivations here only illustrate
the ﬁrst SGD step, but we can get the same conclusion from all steps of SGD similarly.

We summarize the general case below, which follows trivially from Theorem 3.3 and Corollary 3.9.
Theorem 4.1. An L-hidden-layer MLP in standard parametrization (see Eq. (SP) and Table 1) can
only allow SGD learning rate of order O(1/n) if we require limn→∞ E ft(ξ)2 < ∞ for all training
routine, time t, and input ξ. In this case, it is in kernel regime and does not admit feature learning.

5 Maximal Update Parametrization

As shown in the last section, the standard parametrization does not admit a feature learning inﬁnite-
width limit without blowing up logits. Here we propose simple modiﬁcations of the standard
parametrization to make this possible while maintaining stability: 1) To enable feature learning,
n and use Θ(1) learning rate, i.e. set aL+1 = 1/2, c = 0 on
it sufﬁces to divide the logits by
top of Eq. (SP); 2) to allow every layer to perform feature learning, we should furthermore set
a1 = −1/2, b1 = 1/2. We will see that this essentially means we update each weight matrix as
much as possible without blowing up the logits or activations, so we call this the Maximal Update
Parametrization (abbreviated MUP or µP).

√

20Formally, these are almost sure convergences, but we suppress these details to emphasize on intuition.

12

5.1 Dividing Logits by

√

n

For example, in the 2-hidden-layer linear MLP example above, the network would compute

f (ξ) =

1
√
n

v¯h(ξ),

¯h(ξ) = W h(ξ),

h(ξ) = U ξ,

(14)

where Uαβ ∼ N (0, 1) and Wαβ, vαβ ∼ N (0, 1/n) are the trainable parameters. Compared to SP
(Eq. (10)), h(ξ), ¯h(ξ) stays the same; only the logit f (ξ) is scaled down. Again, to simplify notation,
we abbreviate • = •0 and suppress explicit dependence on ξ. This has two consequences

Logits at Initialization Converge to 0
MLP in SP at initialization).

since f has variance Θ(1/n) (compare to the GP limit of

Θ(1) Learning Rate and Feature Learning Even though f → 0, the loss derivative χ = L(cid:48)(f, y)
stays Θ(1) if y (cid:54)= 0. When we redo the calculation in Eq. (12), we see

¯h1 = ¯h − n−c−1/2χ v(cid:62)h(cid:62)h = ¯h − Θ(n−c+1/2)v(cid:62)
f1 = f − n−c−1χ vv(cid:62)h(cid:62)h = f − Θ(n−c).

(15)

Because v has coordinates of size Θ(n−1/2), we see that ¯h and f both change by Θ(1) coordinatewise
if c = 0 (i.e. learning rate is Θ(1)). This directly illustrates feature learning after just 1 step of SGD.
For general MLPs, we can also check aL+1 = 1/2, c = 0 on top of Eq. (SP) implies r = 0 and thus
admits feature learning by Theorem 3.6.

Kernel Behavior or Lack Thereof The example we have here, where we only train the middle
layer in a linear MLP, actually is in kernel regime. This does not violate Corollary 3.9, however,
which assumes Assumption 3.1. If, for example, we have tanh nonlinearity, then it is easy to see the
µP SGD dynamics does not have a kernel limit: If so, then f1 − f is linear in the learning rate η. But
note ¯h1 − ¯h is Θ(1) as n → ∞ and linear in η, as can be derived similarly to Eq. (15). Because tanh
is bounded, this cannot happen. Contrast this with SP or NTP, where ¯h1 − ¯h is Θ(1/
n) and thus
“resides in the linear regime of tanh”, allowing perfect scaling with η.

√

In addition, even in an linear MLP, if we train the middle layer and the last layer, then the dynamics
intuitively will become quadratic in the weights, so will not have a kernel limit. Contrast this with SP
or NTP, which suppress these higher order interactions because the learning rate is small, and a ﬁrst
order Taylor expansion heuristic holds.

√

√

n) after 1 step of SGD with learning rate Θ(1/

n? As shown
How is this different from standard parametrization with learning rate 1/
above, the logit f blows up like Θ(
n) in the
standard parametrization, but remains Θ(1) in our parametrization here. The reason these two
parametrizations seem similar is because in the 1st step, the weights receive the same updates modulo
the loss derivative χ = L(cid:48)(f, y). Consequently, xL
1 − hL are Θ(1) coordinatewise
in both cases. However, this update makes xL
xL
1 (and f1)
1
1
scales like Θ(n1−aL+1−bL+1) due to Law of Large Numbers. Thus only in our parametrization here
(aL+1 = bL+1 = 1/2) is it Θ(1), while in standard parametrization (aL+1 = 0, bL+1 = 1/2) it
n). Contrast this with the behavior at initialization, where W L+1 and xL are
blows up like Θ(
independent and zero-mean, so W L+1xL scales like Θ(n1/2−aL+1−bL+1) by Central Limit Theorem.

1 − xL and hL
1 correlated with W L+1

, so that W L+1

√

√

5.2 First Layer Parametrization

While this now enables feature learning, the ﬁrst layer preactivation h effectively stays ﬁxed through-
out training even if we were to train U . For example, if we update U in the linear MLP example
Eq. (14), then by Eq. (11),

U1 = U − n−cχ dU = U − n−cχ dhξ(cid:62)
h1 = U1ξ = h − n−cχ dhξ(cid:62)ξ = h − Θ(n−c)dh

since ξ(cid:62)ξ, χ = Θ(1). Now dh = W (cid:62)d¯h = W (cid:62) 1√
of size Θ(1/n), since 1√

n v(cid:62) has roughly iid Gaussian coordinates, each
n v(cid:62) has coordinates of the same size. Therefore, even with c = 0, h changes

13

by at most O(1/n) coordinatewise, which is dominated by its value at initialization. This O(1/n)
change also induces a O(1/n) change in f , which would be dominated by the Θ(1) change due to
W ’s evolution, as seen in Eq. (15).

We therefore propose to set a1 = −1/2, b1 = 1/2 on top of Section 5.1’s parametrization. This
implies the forward pass of f remains the same but U ’s gradient is scaled up by n, so that h now
changes by Θ(1) coordinatewise. In summary, we deﬁne
Deﬁnition 5.1. The Maximal Update Parametrization (abbreviated MUP, or µP), in the context of
an L-hidden-layer MLP (Eq. (1)), is given by

c = 0,

bl = 1/2 ∀l,

al =






−1/2
0
1/2

l = 1
2 ≤ l ≤ L
l = L + 1.

Notice that µP for a 1-hidden-layer perceptron is equivalent to the mean ﬁeld parametrization by
Eq. (5). We also describe µP for any architecture in Appendix C.1.

5.3 What is µP Maximal In?

For technical reasons, we adopt Assumption 3.1 again for the formal results of this section.

t

, h = hl

t for any l ≥ 2 due to learning rate n−c
In an abc-parametrization, the change in weight W = W l
is δW def= −n−c · n−2adh x(cid:62) where we abbreviated x = xl−1
t, a = al. (We will use δ to
denote 1-step change, but ∆ to denote lifetime change). In the next forward pass, δW contributes
δW ¯x = −n1−c−2a(x(cid:62) ¯x/n)dh, where ¯x is the new activation due to change in previous layers’
weights. In general, x and ¯x are strongly correlated. Then x(cid:62) ¯x/n → R for some R (cid:54)= 0 by Law of
Large Numbers (as they both have Θ(1) coordinates in a stable parametrization). One can heuristically
see that dh has the same size as the last layer weights, which is Θ(n−(aL+1+bL+1) + n−(2aL+1+c))
(where the ﬁrst summand is from W L+1
). Thus, δW ¯x is a vector
with Θ(n−rl ) def= Θ((n−(aL+1+bL+1) + n−(2aL+1+c))n1−c−2a) coordinates. If rl > 0, then δW ¯x
contributes vanishingly; if rl < 0, then δW ¯x blows up. For l = 1, we get similar insights after
accounting for the ﬁnite dimensionality of ξ.
Deﬁnition 5.2. For l ∈ [L], we say W l is updated maximally if ∆W l
for some training routine21, time t ≥ 1, and input ξ.
Proposition 5.3. In a stable abc-parametrization, for any l ∈ [L], W l is updated maximally iff

and the other from ∆W L+1

(ξ) has Θ(1) coordinates

t xl−1
t

0

t

rl

def= min(aL+1 + bL+1, 2aL+1 + c) + c − 1 + 2al + I(l = 1) = 0.

Note that r (Deﬁnition 3.2) is the minimum of rl over all l. In µP, we can calculate that rl = 0 for all
l ∈ [L], so all W l, l ∈ [L], are updated maximally. Put another way, the ﬁnal embedding xL(ξ) will
have nonvanishing (nonlinear) contributions from ∆W l of all l. These contributions cause the logit
and ∆W L+1
f (ξ) to change via interactions with W L+1
are too small,
then the logit is ﬁxed to its initial value, so all of the feature learning would have been useless.22 It’s
also possible for one to contribute vanishingly but not the other.23 But both contribute in µP.
Deﬁnition 5.4. We say W L+1 is updated maximally (resp. initialized maximally) if ∆W L+1
xL
t (ξ) =
Θ(1) (resp. W L+1

t
t (ξ) = Θ(1)) for some training routine, time t ≥ 1, and input ξ.

. If both W L+1

and ∆W L+1

0 ∆xL

0

0

t

t

(ξ) ∈ Rn.
Note Deﬁnition 5.4 is similar to Deﬁnition 5.2 except ∆W L+1
Proposition 5.5. In a stable abc-parametrization, W L+1 is 1) updated maximally iff 2aL+1 + c = 1,
and 2) initialized maximally iff aL+1 + bL+1 + r = 1.

t (ξ) ∈ R but ∆W l
xL

t xl−1
t

t

We remark that, by Theorem 3.4, a parametrization is nontrivial iff W L+1 is maximally updated or
initialized. Using Propositions 5.3 and 5.5 and Theorem 3.3, we can now easily conclude

21Recall that training routine means a package of learning rate ηn−c, training sequence {(ξt, yt)}t≥0, and a

loss function L(f (ξ), y) that is continuously differentiable in the prediction of the model f (ξ).

22It is indeed possible to perform feature learning in a trivial parametrization, e.g. bl = 1/2 ∀l, a1 =

−1/2, a2 = 100 + 1/2, c = −100 in a 2-hidden-layer MLP.

23e.g. take aL+1 = 100 + 1/2, bL+1 = −100 + 1/2, then ∆W L+1 is negligible.

14

Theorem 5.6. In µP, W l is updated maximally for every l ∈ [L + 1], and W L+1 is also initialized
maximally. µP is the unique stable abc-parametrization with this property.

6 Deriving Feature Learning Inﬁnite-Width Limit: Intuition and Examples

We propose the Tensor Programs technique for deriving the inﬁnite-width limit of any abc-
parametrization. This ultimately just requires the researcher to mechanically apply a set of rules to
the computation graph underlying SGD. However, while operationally simple, this procedure would
seem “too magical” at ﬁrst. In this section, through a series of examples, we seek to build intuition
for what is being automated by this procedure. Then, in the next section, we formally describe the
Tensor Programs framework.

Setup and Notation For pedagogical simplicity, we only consider input dimension d = 1 and
learning rate η = 1 here, but generalization to d > 1, η (cid:54)= 1 is straightforward. We consider SGD
with a singleton minibatch {(ξt, yt)} at time t = 0, 1, 2, . . ., where ξt is the network input and yt
t for the matrix W l after t steps of such training. For any network input
is the label. We write W l
ξ ∈ R, we write xl
t(ξ), ft(ξ)) for the activation xl (resp. preactivation hl, logits f ) of
ft(ξ) (resp. n∇hl
ft(ξ)) by
the network after t steps of SGD. We denote the scaled gradient n∇xl
t(ξ) (resp. dhl
dxl
t (without being applied to ξ) to also
denote the vector xl
t, ft. We will not use xl
t, dhl
t
on its own to denote the function ξ (cid:55)→ xl
t(ξ) so this should not cause confusion. The loss function is
def= L(cid:48)(ft, yt).
denoted L and the loss derivative L(cid:48)(logit, target) is in the ﬁrst argument. We write χt

t(ξt) (applied speciﬁcally to ξt); likewise for hl

t(ξ)). For brevity, we abuse notation and use xl

t(ξ) (resp. hl

t, dxl

t

t

6.1

1-Hidden-Layer MLP

As mentioned above, for 1 hidden layer, the inﬁnite-width µP limit is the same as the mean ﬁeld
limit of [11, 30, 43, 45]. Nevertheless, we present a slightly different derivation of this that is more
consistent with the philosophy of Tensor Programs. Such a network on input ξ ∈ R is given by

f (ξ) = V x(ξ),

x(ξ) = φ(h(ξ)),
√

h(ξ) = U ξ,
(16)
for U ∈ Rn×1, V ∈ R1×n parametrized like U =
n v and with initialization uαβ, vαβ ∼
N (0, 1/n).24 Then U0 (the initial value of U ) has iid N (0, 1) coordinates. It will turn out to be
convenient to represent each such coordinate distribution as a random variable Z U0 def= N (0, 1).
Likewise, let Z nV0 def= N (0, 1), independent from Z U0 , represent the coordinate distribution of nV0
(we do nV0 instead of V0 so that the Z random variable is always independent of n). We derive
the µP limits of the ﬁrst forward and backward passes manually before stating the general case. To
lighten notation, we suppress the t = 0 subscript (e.g. U = U0, h = h0, f = f0, etc), as we will
spend some time on the ﬁrst SGD step.

nu, V = 1√

First Forward Pass After randomly initialization, the preactivation h = h(ξ) (where ξ = ξ0 ∈ R
is the ﬁrst input) has iid coordinates, each a sample from Z h def= ξZ U ∈ R. Naturally, x = x(ξ) has
iid coordinates as well, each a sample from Z x def= φ(Z h). Finally, f = V x = 1
α=1(nV )αxα →
n
˚f def= E Z nV Z x by Law of Large Numbers as n → ∞.25 In particular, f becomes deterministically
0 in this limit because V and U are independent. For a typical loss function L, the loss derivative
χ def= L(cid:48)(f, y) then also become deterministic, χ → ˚χ def= L(cid:48)(˚f , y).

(cid:80)n

First Backward Pass Similarly, dx = nV (cid:62) (recall dxt
def= n∇xtft) has coordinates distributed like
Z dx def= Z nV and dh = dx(cid:12)φ(cid:48)(h) has coordinates distributed like Z dh def= Z dxφ(cid:48)(Z h) = Z nV φ(cid:48)(Z h).
Then SGD with learning rate 1 makes the following updates:
√

v1 = v − χx/
u1 = u − χξ dh/

n
√

=⇒

n

=⇒

V1 = V − χx/n
U1 = U − χξ dh.

24Again, more generally, we can insert constants in this parametrization, like U =

here for simplicity.

√
√

n
d

u, but we omit them

25All convergence in this section will be almost sure, but to focus on the intuition here and less on the

formalities, we do not explicitly write this down.

15

Since χ converges to a deterministic limit ˚χ, the coordinates of these updates are roughly iid,
corresponding to an update of Z random variables:

Z nV1 = Z nV − ˚χZ x, Z U1 = Z U − ˚χξZ dh.

Second Forward Pass Thus V1 and U1 still have roughly iid coordinates after 1 SGD step. Then,
in the second forward pass, h1 has coordinates

Z h1 def= ξ1Z U1 = ξ1Z U − ξ1˚χξZ dh = ξ1Z U − ξ1˚χξZ nV φ(cid:48)(Z h),

x1 has coordinates Z x1 def= φ(Z h1), and the output is
n
(cid:88)

f1 =

1
n

α=1

(nV1)αxα → ˚f1

def= E Z nV1Z x1 = E(Z nV − ˚χZ x)Z x1

(17)

as n → ∞. Then χ1
have roughly iid coordinates by a similar logic.

def= L(cid:48)(f1, y1) → ˚χ1

def= L(cid:48)(˚f1, y1) becomes deterministic. The gradient vectors

tth Iteration Repeating the above reasoning shows that at any time t (independent of n), we obtain

Theorem 6.1. Consider a 1-hidden-layer MLP in µP (Eq. (16)) and any training routine with
learning rate 1. Suppose φ(cid:48) is pseudo-Lipschitz.26 As n → ∞, for every input ξ, ft(ξ) converges
almost surely to ˚ft(ξ) deﬁned as follows:

ft(ξ) a.s.−−→ ˚ft(ξ) def= E Z nVtZ xt(ξ), Z xt(ξ) def= φ(Z ht(ξ)), Z ht(ξ) def= ξZ Ut,
def= L(cid:48)(˚ft, yt), Z nVt+1 def= Z nVt − ˚χtZ xt, Z Ut+1 def= Z Ut − ˚χtξtZ nVtφ(cid:48)(Z ht),

˚χt

(18)

(19)

with, as initial conditions, Z U0 and Z nV0 being independent standard Gaussians, where in Eq. (19)
we abbreviated ˚ft = ˚ft(ξt), xt = xt(ξt), ht = ht(ξt).

As aforementioned, this is a discrete time, minibatched version of the mean ﬁeld limit of [11, 30,
43, 45].27 When φ is identity, it’s easy to see that Z nVt and Z Ut are always (deterministic) linear
combinations of Z nV0 and Z U0 , say Z nVt = AtZ nV0 + BtZ U0 and Z Ut = CtZ nV0 + DtZ U0. Then
the limit ˚ft depends solely on At, Bt, Ct, Dt. By tracking their evolution, we get the following
greatly simpliﬁed formula for an inﬁnite-width µP linear network.
Corollary 6.2. Consider a 1-hidden-layer linear MLP in µP (Eq. (16)) and any training routine
with learning rate 1. As n → ∞, for every input ξ, ft(ξ) converges almost surely to ˚ft(ξ) deﬁned as
follows:

˚ft(ξ) = (AtCt + BtDt)ξ, ˚χt = L(cid:48)(˚ft, yt),

(At+1, Bt+1) = (At, Bt) − ˚χtξt(Ct, Dt),
(Ct+1, Dt+1) = (Ct, Dt) − ˚χtξt(At, Bt),

with initial condition A0 = D0 = 1, B0 = C0 = 0.

This can be easily generalized to larger input and output dimenions (see Appendix D.1). In a gist, such
an inﬁnite-width µP linear network with input dimension d and output dimension do is equivalent to
a width-(d + do) linear network with the same input/output dimensions but an “diagonal”, instead of
random, initialization. Our Word2Vec and MAML experiments will crucially rely on this simplifying
observation. We remark that, in contrast to our approach, such an observation would be obscured by
the PDE perspective of prior works [11, 30, 43, 45].

26This roughly means that φ(cid:48) has a polynomially bounded weak derivative; see Deﬁnition F.3.
27[11, 30, 43, 45] present the equations in terms of the PDF of Z random variables. Formally, the PDF limit
can be obtained by taking the continous-time limit of Eqs. (18) and (19) and then applying Focker-Planck. Note
our derivation, when formalized using the Tensor Programs framework below, does not require smoothness
and support assumptions on the initialization of U, V in those works: The initialization distribution here can be
replaced with any image of Gaussians under pseudo-Lipschitz functions, which includes nonsmooth and singular
distributions.

16

6.2

2-Hidden-Layer MLP: SGD with Partially Decoupled Backpropagation

A 2-hidden-layer MLP is given by

f (ξ) = V ¯x(ξ),

¯h(ξ) = W x(ξ),

¯x(ξ) = φ(¯h(ξ)),
for U ∈ Rn×1, W ∈ Rn×n, V ∈ R1×n parametrized like U =
n v and with initial-
ization uαβ, Wαβ, vαβ ∼ N (0, 1/n). The presence of the n × n Gaussian matrix W (“∞ × ∞”
as opposed to “∞× ﬁnite” like U or “ﬁnite ×∞” like V ) is new and has two major effects on the
inﬁnite-width training dynamics: 1) A Central Limit effect from the random Gaussian nature of
W and 2) a correlation effect between W and its transpose W (cid:62). We isolate the ﬁrst effect here by
analyzing a slightly different version of backpropagation (which has a different limit than normal
backpropagation), and then discuss the second effect in the next section. We abuse notation and
abbreviate W = W0.

x(ξ) = φ(h(ξ)),
nu, V = 1√

h(ξ) = U ξ,

√

Partially Decoupled Backpropagation In this section, we analyze a version of SGD where the
backpropagation weights are partially decoupled from the forward propagation weights. Here, we
think of ∆Wt as the trainable weights, initialized at 0, and think of the Gaussian W as untrainable
“constants”. The forward pass proceeds normally28 with Wt = W + ∆Wt. But we sample and ﬁx an
iid copy (cid:102)W of W (cid:62) before training, and in the backward pass compute

t )d¯ht

dxt = ((cid:102)W + ∆W (cid:62)

instead of dxt = (W (cid:62) + ∆W (cid:62)

(20)
In particular, at initialization, we would have dx0 = (cid:102)W d¯h0 instead of dx0 = W (cid:62)d¯h0. Everything
else stays the same in the backward pass29. Finally, each weight is still updated by SGD via the usual
outer products: with χt
vt+1 = vt − χt ¯x(cid:62)
t /

def= L(cid:48)(ft, yt),
√
n, ∆wt+1 = ∆wt − χtd¯htx(cid:62)

ut+1 = ut − χtξtdh(cid:62)
t /
nu per µP, this causes the following changes in W s:

n, W = w, U =

Since V = v/

t )d¯ht = W (cid:62)

t d¯ht.

t /n,

(21)

n.

√

√

√

Vt+1 = Vt − χt ¯x(cid:62)

t /n, ∆Wt+1 = ∆Wt − χtd¯htx(cid:62)

t /n, Ut+1 = Ut − χtξtdh(cid:62)
t

(22)

Note here we update ∆w and ∆W instead of w and W .

Why This Decoupled SGD? The reasons we talk about this version of SGD is that it isolates the
effect of having a Gaussian n × n matrix (cid:102)W in the backward pass, and we can derive its inﬁnite-width
limit relatively easily using Central Limit heuristics. In the normal version of SGD, (cid:102)W would equal
W (cid:62), and its correlation with W creates additional terms in the inﬁnite-width dynamics, that are
better explained on their own.

Again, we walk through the ﬁrst few forward and backward passes to gain some intuition for the
inﬁnite-width limit, before stating the general case.

First Forward Pass
deriving the NNGP30.

is similar to that in Section 6.1 and follows the usual calculations involved in

First Backward Pass
is similar to that in Section 6.1 and to calculations involved in deriving
Neural Tangent Kernel, except swapping W (cid:62) with (cid:102)W (which at this point has no visible effect,
because of the Gradient Independence Phenomenon [51]; but the effect will become clear in the
second forward pass)31. We end up with ∆W1 = −χ0d¯h0x(cid:62)

0 , as usual.
28i.e. ft = Vt ¯xt, ¯xt = φ(¯ht), ¯ht = (W + ∆Wt)xt, xt = φ(ht), ht = U ξt.
t , d¯ht = φ(cid:48)(¯ht) (cid:12) d¯xt, dht = φ(cid:48)(ht) (cid:12) dxt
29i.e. d¯xt = nV (cid:62)
301) h0 is iid Gaussian with coordinates drawn from Z h0 = ξ0Z U0 ; 2) x0 has coordinates Z x0 = φ(Z h0 );
3) ¯h0 = W x0 has roughly iid coordinates drawn from a zero-mean Gaussian Z ¯h0 by a Central Limit heuristic,
where Z ¯h0 is correlated with Z ¯h0(ξ) for any ξ (including ξ = ξ0) with covariance Cov(Z ¯h0 , Z ¯h0(ξ)) =
α=1(nV0)α ¯x0α →
limn→∞
˚f0

0 x0(ξ) = E Z x0 Z x0(ξ); 4) ¯x0 has coordinates Z ¯x0 = φ(Z ¯h0 ); 5) f0 = 1

n x(cid:62)

(cid:80)n

n

1

def= E Z nV0 Z ¯x0 by a Law of Large Number heuristic.
311) d¯x0 = nV (cid:62)

0 so Z d¯x0 = Z nV0 ; 2) Z d¯h0 = φ(cid:48)(Z ¯h0 ) (cid:12) Z d¯x0 ; 3) Z dx0 = Z (cid:102)W d¯h0 is Gaussian with
0 d¯h0(ξ) = E Z d¯h0 Z d¯h0(ξ) for any input ξ; 4) Z dh0 =
def=

covariance Cov(Z dx0 , Z dx0(ξ)) = limn→∞
φ(cid:48)(Z h0 ) (cid:12) Z dx0 . Since f converges to a deterministic number ˚f0, we also generically have L(cid:48)(f, y0) → ˚χ0
L(cid:48)(˚f0, y0). Finally, the weights are updated like Eq. (22).

n d¯h(cid:62)

1

17

Second Forward Pass As usual, we have Z h1 = ξ1Z U1 = ξ1Z U0 − ˚χ0ξ1ξ0Z dh0 and Z x1 =
φ(Z h1 ), reﬂecting the coordinate distributions of h1 and x1

32. Next,

¯h1 = W x1 + ∆W1x1 = W x1 − χ0d¯h0

x(cid:62)
0 x1
n

.

(23)

On one hand, 1) x(cid:62)
0 x1
n → E Z x1 Z x0 by a Law of Large Numbers heuristic. On the other hand, 2)
by a Central Limit heuristic, W x1 should roughly have Gaussian coordinates ZW x1 correlated with
Z ¯h0 = ZW x0 with Cov(ZW x1 , ZW x0) = lim x(cid:62)
0 x1
n = E Z x1 Z x0. However, very importantly, this
Central Limit heuristic is correct only because we used (cid:102)W in backprop instead of W (cid:62); otherwise,
h1 has a strong correlation with W through dh0 = φ(cid:48)(h0) (cid:12) (W (cid:62)d¯h0), and thus so does x1, so
that W x1 no longer has Gaussian coordinates. This is the “second major effect” referred to in the
beginning of this section. See Section 6.3 for how to handle this correlation.

In any case, in our scenario here,

Z

¯h1 def= ZW x1 − cZ d¯h0 , where

c = ˚χ0 E Z x1Z x0 ,

is a linear combination of a Gaussian variable and the gradient d¯h0’s coordinate random vari-
able. Finally, Z ¯x1 = φ(Z ¯h1) and the logit is f1 = 1
def= E Z nV1Z ¯x1 =
n
E Z nV0 Z ¯x1 − ˚χ0 E Z ¯x0 Z ¯x1.

α=1(nV1)α ¯x1α → ˚f1

(cid:80)n

Second Backward Pass Everything proceeds just like in the 1-hidden-layer case33 except for the
computation of

1 d¯h1 = (cid:102)W d¯h1 − χ0x0

0 d¯h1
d¯h(cid:62)
dx1 = (cid:102)W d¯h1 − ∆W (cid:62)
n
Like in the computation of ¯h1 in Eq. (23), d¯h(cid:62)
0 d¯h1
n → E Z d¯h0Z d¯h1 and (cid:102)W d¯h1 is roughly Gaussian
(and correlated with (cid:102)W d¯h0 in the natural way). But again, for this Gaussian intuition to be correct, it
is crucial that we use (cid:102)W here instead of W (cid:62), or else d¯x1 (and thus d¯h1) is strongly correlated with
W (cid:62) (through ¯x0 = φ(W x0) inside n∆V1 = −χ0 ¯x(cid:62)
In any case, we have

0 ).

.

Z dx1 = Z (cid:102)W d¯h1 − cZ x0, where

c = ˚χ0 E Z d¯h0 Z d¯h1,

is a sum of Gaussian Z (cid:102)W d¯h1 and a multiple of Z x0 . Then weights are updated according to Eq. (22).

tth Iteration For general t, we always have (true in normal SGD as well)

so that in the forward pass

∆Wt = −

1
n

t−1
(cid:88)

s=0

χsd¯hsx(cid:62)
s

¯ht = W xt + ∆Wtxt = W xt −

t−1
(cid:88)

s=0

χsd¯hs

x(cid:62)
s xt
n

Z

¯ht def= ZW xt −

t−1
(cid:88)

s=0

˚χsZ d¯hs E Z xsZ xt.

(24)

Here ZW xt is Gaussian with covariance Cov(ZW xt, ZW xs) = E Z xt Z xs for any s. This means that
Z ¯ht and Z ¯hs are correlated through ZW xt, ZW xs (but also through Z d¯hr , r ≤ min(t, s)). Likewise,
in the backward pass,

dxt = (cid:102)W d¯ht − ∆W (cid:62)d¯ht = (cid:102)W d¯ht −

t−1
(cid:88)

s=0

χsxs

s d¯ht
d¯h(cid:62)
n

Z dxt def= Z (cid:102)W d¯ht −

t−1
(cid:88)

s=0

˚χsZ xs E Z d¯hs Z d¯ht

32Recall they abbreviate h1(ξ1) and x1(ξ1)
33d¯x1 = nV (cid:62)

1 , d¯h1 = d¯x1 (cid:12) φ(cid:48)(¯h1), dh1 = dx1 (cid:12) φ(cid:48)(h1)

18

Here, Z (cid:102)W d¯ht is Gaussian with covariance Cov(Z (cid:102)W d¯ht, Z (cid:102)W d¯hs) = E Z d¯htZ d¯hs for any s. Thus,
Z dxt and Z dxs are correlated through Z (cid:102)W d¯ht, Z (cid:102)W d¯hs (but also through Z xr , r ≤ min(t, s)). Again,
the Gaussianity of ZW xt and Z (cid:102)W d¯ht depend crucially on the fact that we use (cid:102)W instead of W (cid:62) in
backpropagation.

Other parts of the forward and backward propagations are similar to before. Our reasoning can be
formalized via Tensor Programs to prove the following
Theorem 6.3. Consider a 2-hidden-layer MLP in µP with partially decoupled backpropagation as in
Eq. (20) and any training routine with learning rate 1. Suppose φ(cid:48) is pseudo-Lipschitz.34 As n → ∞,
for every input ξ,

ft(ξ) a.s.−−→ ˚ft(ξ), where ˚ft(ξ) is deﬁned as follows:

(forward pass)

˚ft(ξ) def= E Z nVtZ ¯xt(ξ), Z ¯xt(ξ) def= φ(Z

¯ht(ξ)), Z xt(ξ) def= φ(Z ht(ξ)), Z ht(ξ) def= ξZ Ut

Z

¯ht(ξ) def= ZW xt(ξ) −

t−1
(cid:88)

˚χsZ d¯hs E Z xs Z xt(ξ)

s=0
{ZW xt(ξ)}ξ,t centered, jointly Gaussian with Cov(ZW xt(ξ), ZW xs(ζ)) = E Z xt(ξ)Z xs(ζ)

(backward pass)

χt

def= L(cid:48)(˚ft, yt), Z d¯xt def= Z nVt, Z d¯ht def= φ(cid:48)(Z

¯ht)Z d¯xt Z dht def= φ(cid:48)(Z ht)Z dxt

Z dxt def= Z (cid:102)W d¯ht −

t−1
(cid:88)

s=0

˚χsZ xs E Z d¯hsZ d¯ht

(25)

(26)

{Z (cid:102)W d¯ht}t centered, jointly Gaussian with Cov(Z (cid:102)W d¯ht, Z (cid:102)W d¯hs ) = E Z d¯htZ d¯hs

(U, V updates)

Z nVt+1 def= Z nVt − ˚χtZ ¯xt Z Ut+1 def= Z Ut − ˚χtξtZ dht
with Z U0 and Z nV0 being independent standard Gaussians as initial conditions, and by deﬁnition,
{ZW xt(ξ)}ξ,t, {Z (cid:102)W d¯ht}t, Z U0, and Z nV0 are mutually independent sets of random variables. Here,
if ht appears without argument, it means ht(ξt); likewise for ¯ht, xt, ¯xt, dht, d¯ht, dxt, d¯xt, ˚ft.

6.3

2-Hidden-Layer MLP: Normal SGD

Finally, we dicuss normal SGD for 2-hidden-layer MLP, i.e. in backprop we compute

dxt = W (cid:62)

t d¯ht = (W (cid:62) + ∆W (cid:62))d¯ht.

The ﬁrst forward and backward passes are essentially the same as in the last section. However, as
mentioned there, in the second forward pass, W x1 (a part of ¯h1 = W x1 + ∆W1x1) will no longer be
approximately Gaussian because of the correlation between x1 and W . Let’s ﬁrst get some intuition
for why this is before stating the inﬁnite-width limit formally.

Warmup: φ = id First, as warmup, suppose φ = id. In this case, W x1 will actually still be
Gaussian, but its variance will be different than what’s predicted in the previous section. To lighten
notation, we write x = x1 in this section. Then unwinding the deﬁnition of x, we have

where we abbreviated h = ξ1U0, z = d¯h0, a = −χ0ξ0ξ1. Then W x has coordinates

x = h + aW (cid:62)z

(W x)α = (W h)α + a(W W (cid:62)z)α.
As derived in the ﬁrst forward pass in Section 6.2, (W h)α is approximately Gaussian (particularly
because W, U0 are independent). This is true for (W W (cid:62)z)α as well here because we assumed
φ = id, but not true generally. Indeed,
(W W (cid:62)z)α =

WαβWγβzγ = zα

WαβWγβzγ.

(Wαβ)2 +

(cid:88)

(cid:88)

(cid:88)

(cid:88)

34This roughly means that φ(cid:48) has a polynomially bounded weak derivative; see Deﬁnition F.3.

β,γ

β

β

γ(cid:54)=α

19

We will soon see the derivations of Section 6.2 correspond to ignoring the ﬁrst term: In the second
term, there are n summands of the form (cid:80)
γ(cid:54)=α WαβWγβzγ that are approximately iid with vari-
ance ≈ (cid:107)z(cid:107)2/n2. Thus, the second term itself, by a Central Limit heuristic, should converge to
N (0, limn→∞ (cid:107)z(cid:107)2/n). On the other hand, the ﬁrst term zα
β(Wαβ)2 → zα by Law of Large
Numbers. Tying it all together, (W x)α is a linear combination of two Gaussian terms (W h)α and
(cid:80)
γ(cid:54)=α WαβWγβzγ, as well as as zα (which is Gaussian in the case of φ = id, but not generally).
β

(cid:80)

(cid:80)

Note that, if we did (W (cid:102)W z)α instead of (W W (cid:62)z)α, as in the last section, then the same analysis
would show the ﬁrst term is zα
β Wαβ (cid:102)Wβα → 0, while the second term converge in distribution to
the same Gaussian. Thus, the effect of decoupling in Section 6.2 is killing the copy of z in (W x)α.
We can summarize our derivation here in terms of Z:

(cid:80)

For φ = id: ZW x def= ZW h + aZW W (cid:62)z = ZW h + a( ˆZW W (cid:62)z + Z z),

(27)

where

ˆZW W (cid:62)z def= N (0, E(Z z)2).

Note the Central Limit heuristic in the derivation of ˆZW W (cid:62)z also shows ˆZW W (cid:62)z is jointly Gaussian
with ZW h with Cov( ˆZW W (cid:62)z, ZW h) = E ZW (cid:62)zZ h. So, to put Eq. (27) in a form more suggestive
of the general case, we will write

ZW x = ˆZW x + aZ z, where

ˆZW x = ZW h + a ˆZW W (cid:62)z d= N (0, E(Z x)2).

(28)

General φ Unwinding the deﬁnition of x, we have

By Taylor-expanding φ, we can apply a similar (though more tedious) argument as above to derive

x = φ(h + aW (cid:62)z (cid:12) φ(cid:48)(h0)).

(29)

ZW x = ˆZW x + cZ z

(30)

where c = a E φ(cid:48)(Z h1)φ(cid:48)(Z h0) and ˆZW x d= N (0, E(Z x)2). In the case of φ = id, c reduces to a
as above, recovering Eq. (28). For general φ, we can immediately see that ZW x is not Gaussian
because Z z = Z d¯x0φ(cid:48)(Z ¯h0) is not. In the Tensor Programs framework formalized in Section 7, cZ z
is denoted ˙ZW x.
Similarly, coordinates distribution of dx1 = W (cid:62)

1 d¯h1 will also change in the backward pass.

General t For general t, we obtain dynamical equations in Z identical to those in Theorem 6.3
except that Eq. (25) and Eq. (26) need to be modiﬁed. We state the general result below.
Theorem 6.4. Consider a 2-hidden-layer MLP in µP and any training routine with learning rate
1. Suppose φ(cid:48) is pseudo-Lipschitz.35 As n → ∞, for every input ξ, ft(ξ) a.s.−−→ ˚ft(ξ) where ˚ft(ξ) is
deﬁned the same way as in Theorem 6.3 except that Eq. (25) should be replaced with

Z

¯ht(ξ) def= ˆZW xt(ξ) + ˙ZW xt(ξ) −

t−1
(cid:88)

s=0

˚χsZ d¯hs E Z xsZ xt(ξ)

{ ˆZW xt(ξ)}ξ,t centered, jointly Gaussian with Cov( ˆZW xt(ξ), ˆZW xs(ζ)) = E Z xt(ξ)Z xs(ζ)

and Eq. (26) should be replaced with

Z dxt def= ˆZW (cid:62)d¯ht + ˙ZW (cid:62)d¯ht −

t−1
(cid:88)

s=0

˚χsZ xs E Z d¯hsZ d¯ht

{ ˆZW (cid:62)d¯ht}t centered, jointly Gaussian with Cov( ˆZW (cid:62)d¯ht, ˆZW (cid:62)d¯hs ) = E Z d¯htZ d¯hs.

Like in Theorem 6.3, by deﬁnition, { ˆZW xt(ξ)}ξ,t, { ˆZW (cid:62)d¯ht}t, Z U0, and Z nV0 are mutually indepen-
dent sets of random variables.

35This roughly means that φ(cid:48) has a polynomially bounded weak derivative; see Deﬁnition F.3.

20

Here,

˙ZW xt(ξ) def= (cid:80)t−1

r=0 θrZ d¯hr where θr is calculated like so: Z xt(ξ) by deﬁnition is constructed as

for some function36 Φ : Rt+1 → R. Then

Z xt(ξ) = Φ( ˆZW (cid:62)d¯h0 , . . . , ˆZW (cid:62)d¯ht−1, Z U0)

θr

def= E ∂Φ( ˆZW (cid:62)d¯h0, . . . , ˆZW (cid:62)d¯ht−1 , Z U0 )/∂ ˆZW (cid:62)d¯hr .

Likewise,
structed as

˙ZW (cid:62)d¯ht def= (cid:80)t−1

r=0 θrZ xr where θr is calculated as follows: Z d¯ht by deﬁnition is con-

Z d¯ht = Ψ( ˆZW x0 , . . . , ˆZW xt−1, Z V0)

for some function36 Ψ : Rt+1 → R. Then

θr

def= E ∂Ψ( ˆZW x0 , . . . , ˆZW xt−1 , Z V0)/∂ ˆZW xr .

For example, generalizing Eq. (29), for any input ξ, we have

Z x1(ξ) = Φ(ZW (cid:62)d¯h0, Z U0), where Φ(z, u) def= φ(ξu − ˚χ0ξ0ξφ(cid:48)(ξ0u)z).
Then θ0 = E ∂zΦ(ZW (cid:62)d¯h0, Z U0) = −˚χ0ξ0ξ E φ(cid:48)(Z h1(ξ))φ(cid:48)(Z h0 ), which specializes to c in
Eq. (30). Altogether,
Note that ˆZW xt here does not equal ZW xt
Cov( ˆZW xt, ˆZW xs ) = E Z xtZ xs is affected by the presence of ˙ZW xr for all r ≤ max(s, t).

˙ZW x1(ξ) = −˚χ0ξ0ξZ d¯h0 E φ(cid:48)(Z h1(ξ))φ(cid:48)(Z h0).

in Eq. (25) in general, because the covariance

6.4 MLP of Arbitrary Depth

The µP limit of deeper MLPs can be derived along similar logic; see Appendices H.3 to H.5 for
a rigorous treatment within the Tensor Programs framework, which also covers all stable abc-
parametrizations.

What happens in other feature learning parametrizations
If we are in the feature learning
regime, then any W l that is not maximally updated (Deﬁnition 5.2) will be effectively ﬁxed (to its
initialized value) in the inﬁnite-width limit (i.e. no learning occurs).

6.5 Summary of Main Intuitions for Deriving the µP Limit
Law of Large Numbers Any vector z has roughly iid coordinates given by Z z. For any two vectors
.

(cid:80)n

α → E Z zZ z(cid:48)

z, z(cid:48) ∈ Rn, 1
n

α=1 zαz(cid:48)

1. This is all we needed to derive the 1-hidden-layer dynamics of Section 6.1, since all

the matrices there are size-n vectors.

2. In Sections 6.2 and 6.3, this is also used in calculating the limit of ∆Wtxt.
Central Limit If the underlying computation graph never involves the transpose W (cid:62) of a n × n
Gaussian matrix W in a matrix multiplication, then W z is roughly iid Gaussian with
coordinate ZW z d= N (0, E(Z z)2) (if Wαβ ∼ N (0, 1/n))

1. This along with the last intuition are all we used to derive the 2-hidden-layer decoupled

dynamics of Section 6.2, where W is the middle layer weight matrix.
(W , W (cid:62)) Correlation If W (cid:62) is involved, then W z has coordinates distributed like random variable
ˆZW z + ˙ZW z where ˆZW z is the Gaussian obtained by pretending W is independent from
W (cid:62), and ˙ZW z results from the correlation between W and W (cid:62).
˙ZW z is purely a linear
combination of Z z(cid:48)

for previously deﬁned vectors z(cid:48) such that z depends on W (cid:62)z(cid:48).

1. All three intuitions above are needed to derive the 2-hidden-layer dynamics of normal

SGD (Section 6.3), where W (cid:62) is used in backpropagation.

2. The calculation of ˙ZW x is quite intricate, which is why we ﬁrst discussed decoupled
SGD in Section 6.2, which doesn’t need ˙ZW x calculation, before discussing normal
SGD in Section 6.3.

36that may depend on various scalars such as ˚χs, E Z xs Z xs(cid:48) (ξ), and E Z d¯hs Z d¯hs(cid:48)

21

Figure 3: Graphical overview of the Tensor Programs framework. For the Master Theorem, we
illustrate Theorem 7.4(2) since Theorem 7.4(1) is a corollary of Theorem 7.4(2) for a larger program.

7 Tensor Programs Framework

While the previous section demonstrates the intuition of how to derive the µP limit, it also lays
bare 1) the increasing complexity of a manual derivation as the training goes on, as well as 2) the
mounting uncertainty for whether the intuition still holds after many steps of SGD. This is a perfect
call for the Tensor Programs framework, which automates (and makes rigorous) the limit derivation
for any “computation graph” — including the computation graph underlying SGD. Here we review
this framework (developed in Yang [49, 50, 51, 52]) in the context of µP limit. Fig. 3 graphically
overviews the content of this section.

As seen abundantly in Section 6, the computation underlying SGD can be expressed purely via three
instructions: matrix multiplication (by a Gaussian matrix, e.g. W0x0), coordinatewise nonlinearities
(e.g. φ), and taking coordinatewise average (e.g. 1
α=1(nV1)αx1α). In deriving the µP SGD limit,
n
we focused mostly on keeping track of Rn vectors (e.g. ¯xt or dht), but importantly we also computed
scalars ft and χt by (what amounts to) taking coordinatewise average (e.g. f1 = 1
α=1(nV1)αx1α).
n
We implicitly compute scalars as well inside ∆Wtxt. This motivates the following notion of a
program, which can be thought of as a low-level symbolic representation of a computation graph
common in deep learning (e.g. underlying Tensorflow and Pytorch).

(cid:80)n

(cid:80)n

22

𝑎.𝑠.𝑥𝑊MatMul𝑍𝑊𝑥=ሶ𝑍𝑊𝑥+መ𝑍𝑊𝑥𝑊iid𝒩0,𝜎𝑊2/𝑛entries𝒲=Setup𝑣1𝑣2𝑣3𝑣𝑗𝑍𝒱𝑍𝑣1𝑍𝑣2𝑍𝑣3𝑍𝑣𝑗()=𝒱=𝒩0,𝜎𝑊2𝔼𝑍𝑥2Correction due to (𝑊,𝑥)correlation𝑥1𝑥2𝑥3𝑥𝑘𝜙(((((())))))𝜙𝜙𝜙𝜙𝜙Nonlin𝑍𝑥1𝑍𝑥2𝑍𝑥3𝑍𝑥𝑘()𝜙𝑍𝜙(𝑥1,…,𝑥𝑘)=;;;;;;;ሞ𝜃1ሞ𝜃2ሞ𝜃ℓ𝜃1𝜃2𝜃ℓMomentሞ𝜗=𝑥1𝑥2𝑥3𝑥𝑘𝜙(((((())))))𝜙𝜙𝜙𝜙𝜙𝑍𝑥1𝑍𝑥2𝑍𝑥3𝑍𝑥𝑘()𝜙;;;;;;;ሞ𝜃1ሞ𝜃2ሞ𝜃ℓ𝜃1𝜃2𝜃ℓ𝔼Average1𝑛෍1𝑛𝜗𝒞=ሞ𝜗as 𝑛→∞Master Theorem𝑛→∞𝑛→∞Deﬁnition 7.1. A Tensor Program37 is a sequence of Rn-vectors and R-scalars inductively generated
via one of the following ways from an initial set C of random scalars, V of random Rn vectors, and a
set W of random Rn×n matrices (which will be sampled with iid Gaussian entries in Setup 7.2)

MatMul Given W ∈ Rn×n and x ∈ Rn, we can generate W x ∈ Rn or W (cid:62)x ∈ Rn

Nonlin Given φ : Rk × Rl → R, previous scalars θ1, . . . , θl ∈ R and vectors x1, . . . , xk ∈ Rn, we

can generate a new vector

φ(x1, . . . , xk; θ1, . . . , θl) ∈ Rn

where φ(−; θ1, . . . , θl) applies coordinatewise to each “α-slice” (x1

α, . . . , xk

α).

Moment Given same setup as above, we can also generate a new scalar

1
n

n
(cid:88)

α=1

φ(x1

α, . . . , xk

α; θ1, . . . , θl) ∈ R.

Explanation of Deﬁnition 7.1 The vectors mentioned in Deﬁnition 7.1 are exempliﬁed by
ht, xt, dht, dxt in Section 6. The scalars mentioned are exempliﬁed by ft, χt as well as e.g. x(cid:62)
s xt/n
inside the calculating of ht (Eq. (24)). The θis in Nonlin and Moment rules may appear cryptic at ﬁrst.
These scalars are not needed in the ﬁrst forward and backward passes. But in the second forward pass,
for example for the 1-hidden-layer MLP (Section 6.1), x1 = φ(h1) = φ(ξ1U0 − χ0ξ1ξ0nV0φ(cid:48)(h0))
depends on the scalar χ0, ξ0, ξ1, and can be written in the form of Nonlin as ¯φ(U0, nV0, h0; χ0) for
some ¯φ appropriately deﬁned.
The initial set of scalars C is the training sequence {ξt, yt}t for all three examples of Section 6. In
our 2-hidden-layer MLP examples, the initial set of matrices W is {W } (Section 6.3) or {W, (cid:102)W }
(Section 6.2), i.e. the random Rn×n Gaussian matrices. On the other hand, in the 1-hidden-layer
MLP example (Section 6.1), W is empty. The initial set of vectors V in all three examples are
V = {U0, nV0}.3839 Notice how the vectors of these V are sampled with iid standard Gaussian
coordinates. We formalize a more general setup for arbitrary Tensor Programs:
Setup 7.2. 1) For each initial W ∈ W, we sample iid Wαβ ∼ N (0, σ2
W /n) for some variance
W associated to W , independent of other W (cid:48) ∈ W; 2) for some multivariate Gaussian Z V =
σ2
(cid:8)Z h : h ∈ V(cid:9) ∈ RV , we sample the initial set of vectors V like {hα : h ∈ V} ∼ Z V iid for each
α ∈ [n]. 3) For each initial scalar θ ∈ C, we require θ a.s.−−→ ˚θ for some deterministic ˚θ ∈ R.

In all of our examples, we took σ2
W = 1 for simplicity, but Setup 7.2 allows for other initializations
(e.g. a typical initialization for relu networks is σ2
W = 2); additionally, Z h, h ∈ V, are all standard
Gaussians, independent from one another, since U0, nV0 are sampled this way; and our initial scalars
{ξt, yt}t are ﬁxed with n, so they are their own limits.40

What Does a Tensor Program Vector Look Like? Recall that we represented the coordinate
distribution of each vector h with a random variable Z h in Section 6 and kept track of how different
Zs are correlated with each other. We also calculated scalar limits like ft → ˚ft, χt → ˚χt. These
calculations led to a set of formulas for the µP limit (e.g. Theorems 6.1, 6.3 and 6.4). We can also
construct such Z h and ˚θ for vectors h and scalars θ in any Tensor Program. They intuitively capture
the coordinate distribution of vector h and the deterministic limit of θ. The following deﬁnition
formally deﬁnes Z h and ˚θ, but the connection between Z h (resp. ˚θ) and the coordinates of h (resp.
θ) is not made rigorously until Theorem 7.4 later. The ZMatMul rule below perhaps asks for some
discussion, and we shall do so after the deﬁnition.

37What we refer to as Tensor Program is the same as NETSOR(cid:62)+ in Yang [52]; we will not talk about other

languages (like NETSOR(cid:62)) so this should not cause any confusion

38Here we write nV0 instead of V0 because we want all vectors to have Θ(1) coordinates; see Setup 7.2.
39In Section 6 we assumed input dimension is 1. In general, each column of U0 would be a separate initial
vector. Likewise, if the output dimension is greater than 1, then each row of V0 would be a separate initial vector.
40Since {ξt, yt}t are ﬁxed with n, we can WLOG absorb them into any nonlinearities in Nonlin that they are
involved in, and set C = ∅. But, in kernel regime or nonmaximal feature learning parametrization, we usually
have initial scalars, such as n−2aL+1−c, that tend to 0 with n; see Appendix H.4.

23

Deﬁnition 7.3 (Z h and ˚θ). Given a Tensor Program, we recursively deﬁne Z h for each vector h and
˚θ for each scalar θ as follows.

ZInit If h ∈ V, then Z h is deﬁned as in Setup 7.2. We also set ˆZ h def= Z h and ˙Z h def= 0.
ZNonlin+ Given φ : Rk × Rl → R, previous scalars θ1, . . . , θl ∈ R and vectors x1, . . . , xk ∈ Rn,

we have

Z φ(x1,...,xk;θ1,...,θl) def= φ(Z x1

, . . . , Z xk

; ˚θ1, . . . , ˚θl).

ZMoment Given same setup as above and scalar θ = 1
n

(cid:80)n

α=1 φ(x1

α, . . . , xk

α; θ1, . . . , θl), then

˚θ def= E φ(Z x1

, . . . , Z xk

; ˚θ1, . . . , ˚θl).

Here ˚θ1, . . . , ˚θl are deterministic, so the expectation is taken over Z x1

, . . . , Z xk

.

ZMatMul ZW x def= ˆZW x + ˙ZW x for every matrix W (with N (0, σ2

W /n) entries) and vector x, where
ZHat ˆZW x is a Gaussian variable with zero mean. Let VW denote the set of all vectors in
the program of the form W y for some y. Then { ˆZW y : W y ∈ VW } is deﬁned to be
jointly Gaussian with zero mean and covariance
(cid:16) ˆZW x, ˆZW y(cid:17) def= σ2

for any W x, W y ∈ VW .

E Z xZ y,

Cov

W

Furthermore, { ˆZW y : W y ∈ VW } is mutually independent from { ˆZ v : v ∈ V ∪
(cid:83)

¯W (cid:54)=W V ¯W }, where ¯W ranges over W ∪ {A(cid:62) : A ∈ W}.

ZDot We can always unwind Z x = Φ(· · · ),
i=1; {˚θi}l
}j

for some arguments (· · · ) =
({ ˆZW (cid:62)yi
i=1), zi (cid:54)∈ VW (cid:62) (where VW (cid:62) is deﬁned in ZHat), and
deterministic function Φ : Rk+j+l → R. Deﬁne ∂Z x/∂ ˆZW (cid:62)yi def= ∂iΦ(· · · ). Then we
set

i=1, { ˆZ zi
}k

˙ZW x def= σ2
W

k
(cid:88)

i=1

Z yi E ∂Z x
∂ ˆZW (cid:62)yi

,

(31)

There is some nuance in this deﬁnition, so see Remark F.1 and F.2.

Explanation of Deﬁnition 7.3 Nonlin and Moment should appear only natural. However, we
pause to digest the meaning of ZMatMul by relating back to our examples in Section 6. First notice
that ˙ZW x = 0 if W (cid:62) is not used in the program, so that ZW x = ˆZW x. This is the case in Section 6.2,
where (cid:102)W is used in backprop instead of W (cid:62). There (in Eq. (25)), ZW xt is Gaussian with covariance
Cov(ZW xt, ZW xs ) = E Z xtZ xs for any s, consistent with ZHat. In Section 6.3, however, ˙ZW x (cid:54)= 0
in general. The ZDot rule is a direct generalization of the calculation of ˙Z in Theorem 6.4.

˙ZW xt and ˙ZW (cid:62)d¯ht of Section 6.3 for general t will all be nonzero but have no easy expression.
Here we seek to convey the complexity of computing them; this is optional reading for the ﬁrst time
reader. To calculate ˙ZW xt ( ˙ZW (cid:62)d¯ht is similar), we need to express Z xt as a function of purely
ˆZW (cid:62)d¯hs, s < t, and Z U0 = ˆZ U0 . Then we symbolically differentiate Z xt by ˆZW (cid:62)d¯hs and take
expectation to obtain the coefﬁcient of Z d¯hs in ˙ZW xt. For t = 1 as in the examples in Section 6.3,
this task is easy because ˆZW (cid:62)d¯h0 = ˆZ dx0 = Z dx0. But in general, the calculation can balloon
quickly. Indeed, note Z xt = φ(Z ht) and

Z ht = ξtZ Ut = ξtZ U0 − ξt

t−1
(cid:88)

s=0

˚χsξsZ dhs = ξtZ U0 − ξt

t−1
(cid:88)

s=0

˚χsξsφ(cid:48)(Z hs)Z dxs .

However, each Z dxs is a linear combination of ZW (cid:62)d¯hs = ˆZW (cid:62)d¯hs + ˙ZW (cid:62)d¯hs and Z xr , r < s
t d¯hs). Each of ˙ZW (cid:62)d¯hs and Z xr then needs to be recursively expanded in terms
(coming from ∆W (cid:62)
of ˆZ before we can calculate the symbolic partial derivative ∂Z xt/∂ ˆZW (cid:62)d¯hs .

24

Algorithm 1 Compute the inﬁnite-width limit of an NN in any abc-parametrization and any task

1: Write the computation graph underlying training and inference in a Tensor Program (akin to

writing low level PyTorch or Tensorﬂow code).

2: Calculate Z h for each vector h and ˚θ for each scalar θ in the program, according to Deﬁnition 7.3.
3: The logits ft(ξ) of the neural network at any time t should be written as a collection of scalars,
so ˚ft(ξ) is calculated in the previous step. For t being inference time, ˚ft(ξ) is the output of the
inﬁnite-width network after training.

Master Theorem Finally, we relate the symbolic nature of a Tensor Program given in Deﬁnition 7.3
to the analytic limit of its computation, in the following Master Theorem. Pseudo-Lipschitz functions
are, roughly speaking, functions whose (weak) derivatives are polynomially bounded. We state the
theorem assuming mild regularity conditions (Assumption F.4) that roughly says most nonlinearities
in the program should be pseudo-Lipschitz.

Theorem 7.4 (Tensor Program Master Theorem, c.f. Theorem E.15 of [52]). Fix a Tensor Program
initialized accordingly to Setup 7.2. Adopt Assumption F.4. Then

1. For any ﬁxed k and any pseudo-Lipschitz ψ : Rk → R, as n → ∞,

1
n

n
(cid:88)

α=1

ψ(h1

α, . . . , hk

α) a.s.−−→ E ψ(Z h1

, . . . , Z hk

),

(32)

for any vectors h1, . . . , hk in the program, where Z hi

are as deﬁned in Deﬁnition 7.3.

2. Any scalar θ in the program tends to ˚θ almost surely, where ˚θ is as deﬁned in Deﬁnition 7.3.

, . . . , Z hk

α, . . . , hk

Intuitively, Theorem 7.4(1) says that each “coordinate slice” (h1
α) can be thought of as an iid
copy of (Z h1
).41 This intuition is consistent with our heuristic derivation in Section 6, and
Theorem 7.4 underlies the proof of Theorems 6.1, 6.3 and 6.4. Theorem 7.4(2) allows us to directly
obtain the function learned at the end of training: For example, for a 1-hidden-layer MLP, it shows
that the network’s output on any input ξ at time t converges to ˚ft(ξ) given in Theorem 6.1.
Algorithm 1 summarizes how to compute the inﬁnite-width limit of any network in any abc-
parametrization and for any task, using the Tensor Programs framework laid out in this section.
It generalizes the manual derivations of Section 6. We carry out Algorithm 1 for MLPs in all of our
experiments.

Architectural and algorithmic universality Given that Tensor Programs can express the ﬁrst
forward and backward computation of practically any architecture [49, 51], it should perhaps come
as no surprise that they can also express practically any training and inference procedure — or just
any computation — involving any such architecture. This includes both feature learning and kernel
limits. We leverage this ﬂexibility to derive and compute the µP and kernel limits for metalearning
and Word2Vec; see Section 9.

Extensions We focused on programs whose vectors all have the same dimension n here. But it’s
easy to generalize to the case where vectors have different dimensions, which corresponds to e.g.
when a network’s widths are non-uniform. See [52].

8 Computational Considerations

While the TP framework is very general, computing the feature learning limits analytically is
inherently computationally intensive aside from special cases like the linear 1-hidden-layer MLP
(Corollary 6.2). Here we explain why, so as to motivate our experimental choices below.

41This implies an explicit convergence in distribution (see [52]), but this convergence in distribution is strictly

weaker than the formulation in Theorem 7.4, which is in general much more useful.

25

No closed-form formula for evaluating the expectations (e.g. in Eq. (32)) involving general
nonlinearities except in special cases For example, for a 1-hidden-layer MLP (Section 6.1), after
1 step of SGD, the logit is of the form E(Z1 + bφ(Z2))φ(Z3 + cZ1φ(cid:48)(Z2)) where Zis denote different
(correlated) Gaussians (Eq. (17)). While one can still evaluate this via Monte-Carlo, the error will
compound quickly with training time. On the other hand, because of the nesting of φ(cid:48) inside φ, there
is no closed-form formula for this expectation in general.

Notable Exception: If the nonlinearity φ is polynomial, then the expectation is a polynomial moment
of a multivariate Gaussian and can be evaluated analytically, e.g. using Isserlis’ theorem from the
covariance matrix.

Even with nonlinear polynomial φ, there is exponential computational bottleneck As training
time t increases, due to the nesting of φ and φ(cid:48) in the preactivations, the integrand of the expectation,
e.g. E Z ¯xtZ nVt, will turn out to be a polynomial in Ω(1) Gaussian variables with degree Ω(2t). The
covariance matrix of the Gaussian variables will in general be nontrivial, so evaluating the expectation,
e.g. using Isserlis’ theorem, requires super-exponential time. This is because we would need to
expand the polynomial integrand into monomials, and there would be Ω(2t) monomials, each of
which require Ω(2t) time to evaluate using Isserlis’ theorem.

0xl−1

n × n Gaussian matrices Both points above apply to 1-hidden-layer MLPs. Additional difﬁculties
with deeper networks is caused by the n × n initial Gaussian matrix W l
0, 2 ≤ l ≤ L, in the middle of
the network. 1) In general, due to the nonlinearities, xl−1
t would be linearly independent from xl−1
0xl−1
t xl−1
t + ∆W l
for all s < t. Therefore, in calculating W l
, we create a new Gaussian
t
0xl−1
variable ˆZW l
, s < t. This then requires us
to compute and store the covariance between them. Thus, t steps of SGD costs Ω(t2) space and
time (not mentioning that the computation of each covariance entry can require exponential time,
as discussed above). 2) In addition, due to the interaction between W l
t in the forward pass and
in the backward pass, there is nonzero ˙Z, as demonstrated in Eq. (30). This ˙Z is generally a
W l(cid:62)
t
linear combination of Ω(t) terms, and the coefﬁcients of this combination require evaluation of some
expectations that typically run into the exponential bottleneck discussed above.

linearly independent from all previous ˆZW l

t = W l

t xl−1

s

s

t

Summary From easiest to hardest in terms of µP limit’s computational cost, we have 1) 1-hidden-
layer linear networks; 2) L-hidden-layer linear MLP, L ≥ 2; 3) nonlinear MLP with polynomial
activations; 4) nonlinear MLP with nonpolynomial activations. Nevertheless, 1-hidden-layer linear
networks are more than sufﬁcient to demonstrate feature learning in Word2Vec and few-shot learning
with MAML, as we show below.

9 Experiments

In light of the computational difﬁculties discussed above, we divide our experiments into two
groups: 1) Verifying our theory; 2) Scaling up to realistic datasets to demonstrate feature learning.
The experiments in group 1 focus on stress-testing our theory in many scenarios to show that it
describes empirical phenomena accurately. They will run into the discussed computational difﬁculties
(Section 8), so we cannot train the inﬁnite-width µP networks for very long, but nevertheless long
enough to verify the theory. Those in group 2 focus on real datasets (metalearning and Word2Vec)
where feature learning is critical, and demonstrate that the GP and NTK limits are inadequate for
those tasks. Necessarily, we adopt simpler neural architectures for this purpose so we can scale up.

9.1 Verifying the Theory

In Fig. 4, we analytically computed the µP limits derived in Section 6 for quadratic and linear
activations, and veriﬁed them against ﬁnite width networks.

9.2 Few-Shot Learning on Omniglot via First Order MAML

In few-shot learning, the model is given only a small number of labeled examples before asking to
make predictions on unseen data. Therefore, this tests whether a model contains a good prior that
can adapt quickly to the small amount of data at hand.

26

Figure 4: Empirical Simulation Agrees with Theory. We analytically compute the inﬁnite-width
µP limit for the three kinds of networks (depth 1, depth 2 decoupled, depth 2) described in Section 6,
with either quadratic φ(x) = x2 or linear φ(x) = x activation. The training set is random ξt ∈
{±1}, yt ∈ {±1}, so that the deviation of ﬁnite width from inﬁnite width losses are accentuated. We
compare against ﬁnite width µP networks with width 1024 or 4096. For each width, we randomly
initialize with 100 different seeds and aggregate the loss curves. The mean across these seeds is
plotted as solid curves, and the standard deviation represented by the shade. As discussed in Section 8,
nonlinear activation functions and higher depth face computational difﬁculties exponential with
training time. Thus here we only train for a few steps. We observe that the quadratic network
converges slower to the limit with width. This is expected since the tail of Z xt is fatter for a quadratic
activation than a linear activation.

MAML In Model Agnostic Meta-Learning (MAML), the model performs few-shot learning by
one or more SGD steps on the given training data; this is called adaptation. In a pretraining (also
called meta-training) phase, MAML learns a good initialization of the model parameters for this
adaptation. The training objective is to minimize the loss on a random task’s test set after the model
has adapted to its training set. More precisely, the basic First Order MAML at training time goes as
follows: With fθ denoting the model with parameters θ, and with step sizes (cid:15), η, we do

1. At each time point, sample a few-shot task T

2. From T , sample a training set D

3. Adapt θ(cid:48) ← θ − (cid:15)∇θLD(fθ), where LD(fθ) is the loss of fθ over D
4. Sample a test set D(cid:48) from T

5. Update θ ← θ − η∇θ(cid:48)LD(cid:48)(fθ(cid:48)), where LD(cid:48)(fθ(cid:48)) is the loss of fθ(cid:48) over D(cid:48)

6. Repeat

In practice, we batch the tasks, just like batches in SGD, so that we accumulate all the gradients from
Step 5 and update θ only at the end of the batch.

During meta-test time, we are tested on random unseen few-shot tasks, where each task T provides a
training set D and a test set D(cid:48) as during meta-training. We adapt to D as in Step 3 above (or more
generally we can take multiple gradient steps to adapt better) to obtain adapted parameters θ(cid:48). Finally,
we calculate the accuracy of θ(cid:48) on the test set D. We average this accuracy over many tasks T , which
we report as the meta-test accuracy.

First Order vs Second Order MAML Notice in Step 5, we take the gradient of LD(cid:48)(fθ(cid:48)) with
respect to the adapted parameters θ(cid:48). In Second Order MAML, we would instead take the gradient
against the unadapted parameters θ, which would involve the Hessian ∇θ∇θLD(fθ). Second Order
MAML generally achieves performance slightly better than First Order MAML, but at the cost of
signiﬁcantly slower updates [37]. In order to scale up, we will focus on First Order MAML, hereafter
referred to as just MAML.

27

01230.51.01.5(x)=x2lossdepth 2width10244096inf01230.51.01.5depth 2, decoupled0123450.40.60.8depth 10123456789iter0.40.60.81.0(x)=xloss0123456789iter0.40.60.81.00369121518iter0.40.50.60.7Table 2: Omniglot Meta-Test Accuracies after Pretraining with First Order MAML.
φ = relu

φ = identity ; number = log2 width
13
9
5

11

7

µP

GP/NTK

GP

NTK

1

3

66.42
47.60
±.02 ±.04 ±1.24 ±0.70 ±.15 ±.16 ±.23 ±.22 ±.18 ±.19

55.34

64.54

66.43

66.31

47.82

66.36

66.21

66.41

41.68
±.09

Few-Shot Learning Terminologies An N -way classiﬁcation task asks the model to predict a class
from N possiblities. A K-shot classiﬁcation task provides K input/output pairs per class, for a total
of N K training points for N -way classiﬁcation.

Omniglot Omniglot is a standard few-shot learning benchmark. It consists of 20 instances of 1623
characters from 50 different alphabets, each handwritten by a different person. We test our models on
1-shot 5-way classiﬁcation: We draw 5 random characters, along with 1 training instance and 1 test
instance for each character. After the model adapts to the training instances, it’s asked to predict the
character of the test instances (choosing among the 5 characters).

Models Our main model is the µP limit of a 1-hidden-layer linear MLP. We compare against: 1)
ﬁnite width versions of the same;42 2) the NNGP and NTK limits of the same; 3) the NNGP and
NTK limits of a 1-hidden-layer relu MLP. Note 2) is equivalent to a 0-hidden-layer perceptron,
because the NNGP and NTK there are both linear kernels. In addition, the inﬁnite-width SP limit of a
1-hidden-layer network is the same as the NNGP limit. Both 2) and 3) are equivalent to linear models
with ﬁxed (not learned) features, so MAML’s adaptation only applies to the linear weights. On the
other hand, the µP limit and the ﬁnite µP networks will learn new representations of the data over
time that can quickly adapt to new tasks.43

Hyperparameters We use (task) batch size 32 and adaptation step size 0.4 ((cid:15) in Step 3). We also
clip the gradient in Step 5 if the gradient has norm ≥ 0.5.44 For each model, we tune its weight
initializaton variances and the meta learning rate (η in Step 5). During meta-test time, we take 20
gradient steps during adaptation (i.e. we loop Step 3 above 20 times to obtain θ(cid:48)). See Appendix D.1
for more details.

Findings Our results are summarized in the Figure to the
right and Table 2, where curves indicate means and shades
indicate standard deviations. There are three key takeaways:
1) The feature learning µP limit signiﬁcantly outperforms the
kernel limits. 2) The beneﬁt of feature learning dominates
the beneﬁt of having nonlinearities. 3) As width increases,
the ﬁnite µP networks approach the performance of the µP
limit from below.

9.3 Word2Vec

Word2Vec [32, 33] is an early example of large-scale pretraining and transfer learning in natural
language processing, where one learns a feature vector h(ξ) for every word ξ based on the principle
of distributional semantics. For simplicity, we focus on a speciﬁc scheme of Word2Vec using context
as a bag-of-word (CBOW), negative example sampling, and Sigmoid loss function.

Word2Vec Pretraining Consider training on a corpus with vocabulary V. At each time step, we
sample a sentence for the corpus and choose a word i ∈ V. This word’s context J ⊆ V is a window
of words around it in the sentence, thought of as a bag of words. Let ξi ∈ R|V| be the one-hot vector

42Because we will tune initialization variances, our results also represent ﬁnite-width SP networks.
43Note that the transfer learning comment in Section 3.1 does not apply directly to the few-shot setting here,
because the readout weights of the network carry over from the pretraining phase. Nevertheless, we will see a
large performance gap between the kernel limits (2,3) and the µP limit.

44One can write down gradient clipping easily in a Tensor Program, so the its inﬁnite-width limit can be

computed straightforwardly via Theorem 7.4; see Appendix D.

28

135791113log2(width)0.450.500.550.600.65test set accOmniglot, 1-Shot 5-Waylin finitelin Plin ntk/gprelu gprelu ntkcorresponding to word i. We pass the averaged context ξJ def= 1
|J|
MLP with hidden size n and identity activation:

(cid:80)n

j∈J ξj through a 1-hidden-layer

f (ξJ ) = V h(ξJ ) ∈ R|V|,

h(ξJ ) = U ξJ ∈ Rn,

(33)

where V ∈ R|V|×n, U ∈ Rn×|V| factor as V = n−av v, U = n−au u with initialization vα ∼
N (0, n−2bv ), uα ∼ N (0, n−2bu ), where {av, bv, au, bu} specify the parametrization of the network.
After each forward pass, we sample a target word τ from V: with probability p, we take τ = i; with
probability 1 − p, we sample τ uniformly from V \ {i}. Following [32, 33], we take p = 1/21 ≈
4.76%. The loss is then calculated with the Sigmoid function σ(·) :

L(f (ξJ ), ξτ ) =

(cid:26)log(1 − σ(f (ξJ )(cid:62)ξτ ))

log σ(f (ξJ )(cid:62)ξτ )

τ = i
τ (cid:54)= i

(34)

Then v and u are updated via SGD as usual (causing V and U to update). Conventionally, h(ξ) ∈ Rn
is taken as the Word2Vec embedding for a word ξ after many iterations of forward-backward updates.

Word Analogy Evaluation We evaluate the word embeddings h(ξ) with the word analogy task.
This task asks the question of the kind: What to a ‘queen’ is as a ‘man’ to a ‘woman’? (answer is
‘king’). The Word2Vec model answers this question by computing

argmax
i

h(ξi)(cid:62)(h(ξ‘man’) − h(ξ‘woman’) + h(ξ‘queen’))

(35)

where i ranges over V \ {‘man’, ‘woman’, ‘queen’}. If the argmax here is i = ‘king’, then the model
answers correctly; otherwise, it’s incorrect. The accuracy score is the percentage of such questions
answered correctly.

Dataset We train the models on text8,45 a clean dataset consisting of the ﬁrst 100 million charac-
ters of a 2006 Wikipedia dump. The dataset has been featured in the original Word2Vec codebase and
the Hutter Prize. text8 contains the ﬁrst 100 million characters of fil9, a larger dataset obtained by
ﬁltering the ﬁrst 1 billion characters in the aforementioned Wikipedia dump. We space-separate the
datasets into tokens and keep ones that appear no less than 5 times in the entire dataset for text8 and
10 times for fil9. The resulting datasets have 71,291 and 142,276 unique vocabulary items.

Models Our main model is the µP limit of Eq. (33). We compare against the baselines of 1) ﬁnite-
width versions of the same, and 2) the NTK and GP limits of Eq. (33). As shown in Corollary 3.9, the
features of the NTK limit are ﬁxed at initialization as n → ∞ (and so are those of the GP limit, by
deﬁnition), so its answer to Eq. (35) is uniformly selected from the whole vocabulary.46 Its accuracy
|V|−3 . Since |V| is 71,291 for text8 and 142,276 for fil9, this number is practically 0.
is thus
We compute the µP limit according to Algorithm 1, but we relate more implementation details in
Appendix D.2.

1

Findings We show our re-
sults in Table 3 and Figure
to the right. As expected,
the inﬁnite-width and ﬁnite-
width µP networks signiﬁ-
cantly outperform the NTK
limit.
In addition, we ob-
serve the ﬁnite width µP
networks converge to the
performance of the µP limit
from below, as width in-
creases.

45http://mattmahoney.net/dc/textdata.html
46There is some nuance here because h(ξ)(cid:62)h( ¯ξ) is actually Θ(

√

n) instead of Θ(n) because ξ, ¯ξ are one-hot,

but the conclusion is the same; see Appendix D.2.

29

2.55.07.510.012.515.0epoch1015202530354045word analogy accword2vec pretrained on text8log2(width)6.08.010.012345epoch303540455055word2vec pretrained on fil9log2(width)6.08.010.0Table 3: Test Accuracies on Word Analogy after Pretraining with CBOW Word2Vec.

Dataset

6

number = log2 width
8

µP

10

GP/NTK

text8
fil9

33.35
44.39

41.58
54.24

42.56
55.69

43.31
56.45

0.0
0.0

10 Conclusion

In this paper, we presented a framework, based on the notion of abc-parametrizations and Tensor Pro-
grams technique, that uniﬁes the Neural Tangent Kernel (NTK) and Mean Field limits of large width
neural networks (NNs). In the Dynamical Dichotomy theorem, we classiﬁed the abc-parametrizations
into feature learning and kernel regimes. We identiﬁed the lack of feature learning as a fatal weakness
of NTK as a model for real NN. In fact, we showed the standard parametrization suffers from the
same problem. As a solution, we proposed the Maximal Update Parametrization (µP) and derived its
inﬁnite-width limit, which admits feature learning. Through experiments on Word2Vec and few-shot
learning, we demonstrated that µP is a good model for feature learning behavior in neural networks.

More generally, this paper showcased the power of the Tensor Programs technique: Any computation
expressable in a Tensor Program has a “inﬁnite-width” limit we can derive. Because of the universality
of Tensor Programs for expressing deep learning computation [49, 51], this technique systematically
solves the mathematical problem of taking inﬁnite-width limits which has been dealt with haphazardly
in prior literature. Its immense ﬂexibility means that the theory of reinforcement learning, self-
supervised learning, deep generative models, etc with overparametrized neural networks in the feature
learning regime are now ripe for the picking.

Acknowledgements

In alphabetical order, we thank Sina Alemohammad, Zeyuan Allen-Zhu, Francis Bach, Yasaman
Bahri, Lenaic Chizat, Jeremy Cohen, Yarin Gal, Quanquan Gu, Bobby He, Di He, Jiaoyang Huang,
Arthur Jacot, Jaehoon Lee, Jason Lee, Zhiyuan Li, Etai Littwin, Yiping Lu, Song Mei, Roman Novak,
Vinay Rao, Michael Santacroce, Sam Schoenholz, Lisa Schut, Jascha Sohl-Dickstein, Alessandro
Sordoni, Denny Wu, Huishuai Zhang, and Pengchuan Zhang for discusson and feedback.

References

[1] Martín Abadi, Paul Barham, Jianmin Chen, Zhifeng Chen, Andy Davis, Jeffrey Dean, Matthieu
Devin, Sanjay Ghemawat, Geoffrey Irving, Michael Isard, et al. Tensorﬂow: A system for
large-scale machine learning. In 12th {USENIX} Symposium on Operating Systems Design and
Implementation ({OSDI} 16), pages 265–283, 2016.

[2] Laurence Aitchison. Why bigger is not always better: on ﬁnite and inﬁnite neural networks.
arXiv:1910.08013 [cs, stat], June 2020. URL http://arxiv.org/abs/1910.08013.

[3] Laurence Aitchison, Adam X. Yang, and Sebastian W. Ober. Deep kernel processes.
arXiv:2010.01590 [cs, stat], October 2020. URL http://arxiv.org/abs/2010.01590.

[4] Zeyuan Allen-Zhu, Yuanzhi Li, and Zhao Song. A Convergence Theory for Deep Learning
via Over-Parameterization. arXiv:1811.03962 [cs, math, stat], November 2018. URL http:
//arxiv.org/abs/1811.03962.

[5] Dyego Araújo, Roberto I. Oliveira, and Daniel Yukimura. A mean-ﬁeld limit for certain deep
neural networks. arXiv:1906.00193 [cond-mat, stat], June 2019. URL http://arxiv.org/
abs/1906.00193.

[6] Mohsen Bayati and Andrea Montanari. The dynamics of message passing on dense graphs,
with applications to compressed sensing. IEEE Transactions on Information Theory, 57(2):
764–785, February 2011. ISSN 0018-9448, 1557-9654. doi: 10.1109/TIT.2010.2094817. URL
http://arxiv.org/abs/1001.3448.

30

[7] Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhari-
wal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agar-
wal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh,
Daniel M. Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler,
Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish,
Alec Radford, Ilya Sutskever, and Dario Amodei. Language Models are Few-Shot Learners.
arXiv:2005.14165 [cs], July 2020. URL http://arxiv.org/abs/2005.14165.

[8] Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhari-
wal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agar-
wal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh,
Daniel M. Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler,
Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish,
Alec Radford, Ilya Sutskever, and Dario Amodei. Language Models are Few-Shot Learners.
arXiv:2005.14165 [cs], July 2020. URL http://arxiv.org/abs/2005.14165.

[9] Minmin Chen, Jeffrey Pennington, and Samuel Schoenholz. Dynamical Isometry and a Mean
Field Theory of RNNs: Gating Enables Signal Propagation in Recurrent Neural Networks.
In Proceedings of the 35th International Conference on Machine Learning, volume 80 of
Proceedings of Machine Learning Research, pages 873–882, Stockholmsmässan, Stockholm
Sweden, July 2018. PMLR. URL http://proceedings.mlr.press/v80/chen18i.html.

[10] Lenaic Chizat and Francis Bach. A Note on Lazy Training in Supervised Differentiable

Programming. page 19.

[11] Lenaic Chizat and Francis Bach. On the Global Convergence of Gradient Descent for Over-
parameterized Models using Optimal Transport. arXiv:1805.09545 [cs, math, stat], May 2018.
URL http://arxiv.org/abs/1805.09545.

[12] Lenaic Chizat and Francis Bach. Implicit Bias of Gradient Descent for Wide Two-layer Neural
Networks Trained with the Logistic Loss. arXiv:2002.04486 [cs, math, stat], June 2020. URL
http://arxiv.org/abs/2002.04486.

[13] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. BERT: Pre-training of
Deep Bidirectional Transformers for Language Understanding. arXiv:1810.04805 [cs], May
2019. URL http://arxiv.org/abs/1810.04805. arXiv: 1810.04805 version: 2.

[14] Simon S. Du, Xiyu Zhai, Barnabas Poczos, and Aarti Singh. Gradient Descent Provably
Optimizes Over-parameterized Neural Networks. arXiv:1810.02054 [cs, math, stat], October
2018. URL http://arxiv.org/abs/1810.02054.

[15] Cong Fang, Jason D. Lee, Pengkun Yang, and Tong Zhang. Modeling from Features: a Mean-
ﬁeld Framework for Over-parameterized Deep Neural Networks. arXiv:2007.01452 [cs, math,
stat], July 2020. URL http://arxiv.org/abs/2007.01452. arXiv: 2007.01452.

[16] Chelsea Finn, Pieter Abbeel, and Sergey Levine. Model-Agnostic Meta-Learning for Fast
Adaptation of Deep Networks. arXiv:1703.03400 [cs], July 2017. URL http://arxiv.org/
abs/1703.03400.

[17] Dar Gilboa and Guy Gur-Ari. Wider Networks Learn Better Features. September 2019. URL

https://arxiv.org/abs/1909.11572v1.

[18] Dar Gilboa, Bo Chang, Minmin Chen, Greg Yang, Samuel S. Schoenholz, Ed H. Chi, and
Jeffrey Pennington. Dynamical Isometry and a Mean Field Theory of LSTMs and GRUs.
arXiv:1901.08987 [cs, stat], January 2019. URL http://arxiv.org/abs/1901.08987.

[19] Eugene A. Golikov. Dynamically Stable Inﬁnite-Width Limits of Neural Classiﬁers.
arXiv:2006.06574 [cs, stat], October 2020. URL http://arxiv.org/abs/2006.06574.

[20] Boris Hanin. Which Neural Net Architectures Give Rise To Exploding and Vanishing Gradients?

January 2018. URL https://arxiv.org/abs/1801.03744.

[21] Boris Hanin and David Rolnick. How to Start Training: The Effect of Initialization and
Architecture. arXiv:1803.01719 [cs, stat], March 2018. URL http://arxiv.org/abs/1803.
01719.

31

[22] Souﬁane Hayou, Arnaud Doucet, and Judith Rousseau. On the Selection of Initialization and
Activation Function for Deep Neural Networks. arXiv:1805.08266 [cs, stat], May 2018. URL
http://arxiv.org/abs/1805.08266.

[23] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep Residual Learn-
URL https://www.cv-

ing for
foundation.org/openaccess/content_cvpr_2016/html/He_Deep_Residual_
Learning_CVPR_2016_paper.html.

pages 770–778, 2016.

Image Recognition.

[24] Dan Hendrycks and Kevin Gimpel. Gaussian Error Linear Units (GELUs). arXiv:1606.08415

[cs], July 2020. URL http://arxiv.org/abs/1606.08415.

[25] Jiaoyang Huang and Horng-Tzer Yau. Dynamics of deep neural networks and neural tangent

hierarchy, 2019.

[26] Arthur Jacot, Franck Gabriel, and Clément Hongler. Neural Tangent Kernel: Convergence
and Generalization in Neural Networks. arXiv:1806.07572 [cs, math, stat], June 2018. URL
http://arxiv.org/abs/1806.07572.

[27] Aitor Lewkowycz, Yasaman Bahri, Ethan Dyer, Jascha Sohl-Dickstein, and Guy Gur-Ari. The
large learning rate phase of deep learning: the catapult mechanism. arXiv:2003.02218 [cs, stat],
March 2020. URL http://arxiv.org/abs/2003.02218.

[28] Yuanzhi Li, Tengyu Ma, and Hongyang R. Zhang. Learning over-parametrized two-layer
neural networks beyond ntk. In Jacob Abernethy and Shivani Agarwal, editors, Proceedings
of Thirty Third Conference on Learning Theory, volume 125 of Proceedings of Machine
Learning Research, pages 2613–2682. PMLR, 09–12 Jul 2020. URL http://proceedings.
mlr.press/v125/li20a.html.

[29] Zhuohan Li, Eric Wallace, Sheng Shen, Kevin Lin, Kurt Keutzer, Dan Klein, and Joseph E.
Gonzalez. Train Large, Then Compress: Rethinking Model Size for Efﬁcient Training and
Inference of Transformers. arXiv:2002.11794 [cs], June 2020. URL http://arxiv.org/
abs/2002.11794.

[30] Song Mei, Andrea Montanari, and Phan-Minh Nguyen. A mean ﬁeld view of the landscape
of two-layer neural networks. Proceedings of the National Academy of Sciences, 115(33):
E7665–E7671, August 2018. ISSN 0027-8424, 1091-6490. doi: 10.1073/pnas.1806579115.
URL https://www.pnas.org/content/115/33/E7665.

[31] Song Mei, Theodor Misiakiewicz, and Andrea Montanari. Mean-ﬁeld theory of two-layers
neural networks: dimension-free bounds and kernel limit. arXiv:1902.06015 [cond-mat, stat],
February 2019. URL http://arxiv.org/abs/1902.06015.

[32] Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey Dean. Efﬁcient Estimation of Word
Representations in Vector Space. arXiv:1301.3781 [cs], September 2013. URL http://arxiv.
org/abs/1301.3781.

[33] Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg Corrado, and Jeffrey Dean. Distributed
Representations of Words and Phrases and their Compositionality. arXiv:1310.4546 [cs, stat],
October 2013. URL http://arxiv.org/abs/1310.4546.

[34] Phan-Minh Nguyen. Mean Field Limit of the Learning Dynamics of Multilayer Neural Networks.
arXiv:1902.02880 [cond-mat, stat], February 2019. URL http://arxiv.org/abs/1902.
02880.

[35] Phan-Minh Nguyen and Huy Tuan Pham. A Rigorous Framework for the Mean Field Limit
of Multilayer Neural Networks. arXiv:2001.11443 [cond-mat, stat], January 2020. URL
http://arxiv.org/abs/2001.11443.

[36] Quynh Nguyen and Marco Mondelli. Global convergence of deep networks with one wide layer

followed by pyramidal topology, 2020.

[37] Alex Nichol, Joshua Achiam, and John Schulman. On First-Order Meta-Learning Algorithms.

March 2018. URL https://arxiv.org/abs/1803.02999v3.

32

[38] Samet Oymak and Mahdi Soltanolkotabi. Toward moderate overparameterization: Global
convergence guarantees for training shallow neural networks. IEEE Journal on Selected Areas
in Information Theory, 1(1):84–105, May 2020. ISSN 2641-8770. doi: 10.1109/jsait.2020.
2991332. URL http://dx.doi.org/10.1109/JSAIT.2020.2991332.

[39] Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory
Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, Alban Desmai-
son, Andreas Kopf, Edward Yang, Zachary DeVito, Martin Raison, Alykhan Tejani,
Sasank Chilamkurthy, Benoit Steiner, Lu Fang, Junjie Bai, and Soumith Chintala. Py-
torch: An imperative style, high-performance deep learning library.
In H. Wallach,
H. Larochelle, A. Beygelzimer, F. d'Alché-Buc, E. Fox, and R. Garnett, editors, Ad-
vances in Neural Information Processing Systems 32, pages 8024–8035. Curran Associates,
Inc., 2019. URL http://papers.neurips.cc/paper/9015-pytorch-an-imperative-
style-high-performance-deep-learning-library.pdf.

[40] Jeffrey Pennington, Samuel Schoenholz, and Surya Ganguli. Resurrecting the sigmoid in deep
learning through dynamical isometry:
In I. Guyon, U. V. Luxburg,
S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett, editors, Advances
in Neural Information Processing Systems 30, pages 4788–4798. Curran Associates, Inc.,
2017. URL http://papers.nips.cc/paper/7064-resurrecting-the-sigmoid-in-
deep-learning-through-dynamical-isometry-theory-and-practice.pdf.

theory and practice.

[41] George Philipp and Jaime G. Carbonell. The Nonlinearity Coefﬁcient - Predicting Overﬁtting
in Deep Neural Networks. arXiv:1806.00179 [cs, stat], May 2018. URL http://arxiv.org/
abs/1806.00179.

[42] Ben Poole, Subhaneil Lahiri, Maithreyi Raghu, Jascha Sohl-Dickstein, and Surya Ganguli.
Exponential expressivity in deep neural networks through transient chaos. In Advances In
Neural Information Processing Systems, pages 3360–3368, 2016.

[43] Grant M. Rotskoff and Eric Vanden-Eijnden. Neural Networks as Interacting Particle Systems:
Asymptotic Convexity of the Loss Landscape and Universal Scaling of the Approximation Error.
arXiv:1805.00915 [cond-mat, stat], May 2018. URL http://arxiv.org/abs/1805.00915.

[44] Samuel S. Schoenholz, Justin Gilmer, Surya Ganguli, and Jascha Sohl-Dickstein. Deep Infor-

mation Propagation. 2017. URL https://openreview.net/pdf?id=H1W1UN9gg.

[45] Justin Sirignano and Konstantinos Spiliopoulos. Mean Field Analysis of Neural Networks.

arXiv:1805.01053 [math], May 2018. URL http://arxiv.org/abs/1805.01053.

[46] Justin Sirignano and Konstantinos Spiliopoulos. Mean Field Analysis of Deep Neural Networks.
arXiv:1903.04440 [math, stat], February 2020. URL http://arxiv.org/abs/1903.04440.

[47] Jascha Sohl-Dickstein, Roman Novak, Samuel S. Schoenholz, and Jaehoon Lee. On the inﬁnite
width limit of neural networks with a standard parameterization. arXiv:2001.07301 [cs, stat],
January 2020. URL http://arxiv.org/abs/2001.07301.

[48] Blake Woodworth, Suriya Gunasekar, Jason D. Lee, Edward Moroshko, Pedro Savarese, Itay
Golan, Daniel Soudry, and Nathan Srebro. Kernel and Rich Regimes in Overparametrized
Models. arXiv:2002.09277 [cs, stat], July 2020. URL http://arxiv.org/abs/2002.09277.

[49] Greg Yang. Tensor Programs I: Wide Feedforward or Recurrent Neural Networks of Any Archi-
tecture are Gaussian Processes. arXiv:1910.12478 [cond-mat, physics:math-ph], December
2019. URL http://arxiv.org/abs/1910.12478.

[50] Greg Yang. Scaling Limits of Wide Neural Networks with Weight Sharing: Gaussian Process
Behavior, Gradient Independence, and Neural Tangent Kernel Derivation. arXiv:1902.04760
[cond-mat, physics:math-ph, stat], February 2019. URL http://arxiv.org/abs/1902.
04760.

[51] Greg Yang. Tensor Programs II: Neural Tangent Kernel for Any Architecture. arXiv:2006.14548

[cond-mat, stat], August 2020. URL http://arxiv.org/abs/2006.14548.

[52] Greg Yang. Tensor Programs III: Neural Matrix Laws. arXiv:2009.10685 [cs, math], September

2020. URL http://arxiv.org/abs/2009.10685.

33

[53] Greg Yang and Hadi Salman. A ﬁne-grained spectral perspective on neural networks, 2019.

[54] Greg Yang and Sam S. Schoenholz. Deep mean ﬁeld theory: Layerwise variance and width
variation as methods to control gradient explosion, 2018. URL https://openreview.net/
forum?id=rJGY8GbR-.

[55] Greg Yang and Samuel S. Schoenholz. Mean Field Residual Network: On the Edge of Chaos.

In Advances in neural information processing systems, 2017.

[56] Greg Yang, Jeffrey Pennington, Vinay Rao, Jascha Sohl-Dickstein, and Samuel S. Schoenholz.
A Mean Field Theory of Batch Normalization. arXiv:1902.08129 [cond-mat], February 2019.
URL http://arxiv.org/abs/1902.08129.

[57] Difan Zou, Yuan Cao, Dongruo Zhou, and Quanquan Gu. Stochastic Gradient Descent Optimizes
Over-parameterized Deep ReLU Networks. arXiv:1811.08888 [cs, math, stat], November 2018.
URL http://arxiv.org/abs/1811.08888.

34

A A Short Origin Story of the Tensor Programs Paper Series

The Tensor Programs framework was initially proposed in [50] in February 2019, and was mainly
applied to extend the NNGP and NTK limits to arbitrary architectures (and to make rigorous the
signal propagation literature [9, 18, 20–22, 40–42, 44, 53–56]). While NNGP and NTK amount to
taking limits of neural networks at initialization, it was soon, in April 2019, realized that Tensor
Programs could 1) also trivially take limits of the entire training procedure of neural networks (which
is the main theoretical idea of this paper), and 2) calculate the feature learning limit. However, at
that point, it also became clear that [50] was not written accessibly, and its formulation of Tensor
Programs was cumbersome to use. A question had to be asked: Should the feature learning paper
be written immediately on such an unwieldy foundation, or should signiﬁcant effort be devoted
to ﬁxing this foundation ﬁrst? Eventually, a decision was made in favor of the latter. The Tensor
Programs series was created as way to re-organize and re-present the Tensor Programs machinery in
a user-friendly way to the machine learning audience (the ﬁrst 3 papers [49, 51, 52] of the series),
before extracting payoffs from this foundation (starting from this paper).

B Further Discussions on the Shallow NTK and MF Examples

How does the Function Change? If the NTK limit does not allow features to evolve, then how
does learning occur? To answer this question, note

∆ft(ξ) = V0∆xt(ξ) + ∆Vtx0(ξ) + ∆Vt∆xt(ξ).

In short, then, the evolution of ft(ξ) in the NTK limit is predominantly due to V0∆xt(ξ) and
∆Vtx0(ξ) only, while in the MF limit, ∆Vt∆xt(ξ) also contributes nontrivially.
Example: For t = 1, ∆f1(ξ) = V0∆x1(ξ) + n−2av x(cid:62)
so the term n−2av x(cid:62)
O(1/

0 x0(ξ) = Θ(1) for generic ξ, ξ0. On the other hand, n−2av x(cid:62)

0 ∆x1(ξ). In NTP, av = 1/2,
0 ∆x1(ξ) =

0 x0(ξ) + n−2av x(cid:62)

n) because ∆x1(ξ) = O(1/
V0∆x1(ξ) ≈ V0[φ(cid:48)(h0(ξ)) (cid:12) ∆h1(ξ)] = V0[φ(cid:48)(h0(ξ)) (cid:12) ∆h1(ξ)]

n) as noted above. Likewise,

√

√

= C

n
(cid:88)

α=1

V0αφ(cid:48)(h0(ξ)α)V0αφ(cid:48)(h0α) = C

n
(cid:88)

(V0α)2φ(cid:48)(h0(ξ)α)φ(cid:48)(h0α),

α=1

where C = χ0ξ0ξ = Θ(1). Now (V0α)2 = Θ(1/n) and is almost surely positive. On the other hand,
φ(cid:48)(h0(ξ)α)φ(cid:48)(h0α) = Θ(1) and should have a nonzero expectation over random initialization (for
example, if φ is relu then this is obvious). Therefore, the sum above should amount to V0∆x1(ξ) ≈
Θ(1). In summary, in the NTK limit, ∆f1(ξ) = Θ(1) due to the interactions between V0 and ∆x1(ξ)
and between ∆V1 and x0(ξ), but there is only vanishing interaction between ∆V1 and ∆x1(ξ).
The case for general t, again, can be derived easily using Tensor Programs.

C abc-Parametrization for General Neural Architectures

We can straightforwardly generalize abc-parametrizations to an arbitrary neural architecture. Each
parameter tensor W would get its own aW and bW , such that W = n−aW w and w is the actual
trainable parameter with initialization wαβ ∼ N (0, n−2bW ). The learning rate is still ηn−c for some
ﬁxed η.

C.1 Maximal Update Parametrization

MLP with Biases Suppose in Eq. (1), for each l ∈ [L], we have hl(ξ) = W lxl−1(ξ) + bl instead,
for bias bl ∈ Rn. Then in µP, the bias bl should have abl = −1/2 and bbl = 1/2. We can also have
bias bL+1 in the logits f (ξ) = W L+1xL(ξ) + bL+1. Then we set abL+1 = bbL+1 = 0.

General Neural Architectures More generally, µP can be deﬁned easily for any neural architecture
whose forward pass can be written down as a Tensor Program (e.g. ResNet or Transformer; see
[49] for explicit programs). The learning rate is always independent of width, i.e. c = 0. For any
parameter tensor W , bW is always 1/2, and aW can be deﬁned as follows: If W is not an output
weight matrix, then aW should be set to −1 + 1
2 pW , where pW = limn→∞ logn #(W ) is a) 0 if both

35

sides of W are ﬁxed w.r.t. n; b) 1 if W is a vector (e.g. bias) or with one side being ﬁxed dimensional
(e.g. W 1); and c) 2 if W is a matrix with both sides scaling like n (e.g. weights in the middle of an
MLP). If W is an output weight matrix (and thus the output dimension is ﬁxed w.r.t. n), then aW
should be 1

2 . If W is an output bias, then aW should be 0.

Optimality Properties One can formalize, in this general context, the notion of stability and the
notions of a parameter tensor being updated maximally and (a set of readout weights) being initialized
maximally. Then one can show that µP is the unique stable abc-parametrization such that all of its
parameter tensors are updated maximally and all of its readout weights are initialized maximally.

D Experimental Details

The main models in our experiments are all 1-hidden-layer linear MLPs with input dimension d and
output dimension do. In our experiments, we will consider more advanced forms, but, as warmup, a
basic version of such a network is given by

f (ξ) = V h(ξ),

h(ξ) = U ξ,

(36)

for U ∈ Rn×d, V ∈ Rdo×n parametrized like U =
uαβ, vαβ ∼ N (0, 1/n). In this case, Corollary 6.2 generalizes to
Theorem D.1. Consider a 1-hidden-layer linear MLP in µP (Eq. (36)) and any training routine with
learning rate η. As n → ∞, for every input ξ ∈ Rd, ft(ξ) ∈ Rdo converges almost surely to ˚ft(ξ)
deﬁned as follows:

n v and with initialization

nu, V = 1√

√

˚ft(ξ) = (AtCt + BtDt)ξ ∈ Rdo,

˚χt = L(cid:48)(˚ft, yt) ∈ Rdo ,
(At+1, Bt+1) = (At, Bt) − η˚χt ⊗ (Ctξt, Dtξt),
(Ct+1, Dt+1) = (Ct, Dt) − η(A(cid:62)

t ˚χt, B(cid:62)

t ˚χt) ⊗ ξt,

where ⊗ denotes outer product (u ⊗ v = uv(cid:62)), with initial condition

A0 = Ido ∈ Rdo×do , D0 = Id ∈ Rd×d, B0 = 0 ∈ Rdo×d, C0 = 0 ∈ Rd×do .

While we will not use this theorem, we intend it to give an idea of the mathematical process underneath
our implementations, which we discuss now.

D.1 Few-Shot Learning on Omniglot via MAML

D.1.1 Linear 1-Hidden-Layer µP Network

We consider a linear 1-hidden-layer MLP with bias, input dimension d, output dimension do, given
by

f (ξ) = V h(ξ) ∈ Rdo ,
√

h(ξ) = U ξ + B ∈ Rn,

where ξ ∈ Rd. Following µP, we factor U =
where u, v, β are the trainable parameters. We initialize uαβ ∼ N (0, σ2
β = 0 ∈ Rn. We can cancel the factors of

nu ∈ Rn×d, V = 1√

n and rewrite

√

n v ∈ Rdo×n, B = α

√

nβ ∈ Rn,
v/n),

u/n), vαβ ∼ N (0, σ2

f (ξ) = vh(ξ) ∈ Rdo,

h(ξ) = uξ + b ∈ Rn,

where b = αβ. We will also consider gradient clipping with threshold g and weight decay with
coefﬁcient γ. So in summary, the hyperparameters are

σu, σv (init. std.), α (bias multiplier),

η (LR),

g (grad. clip),

γ (weight decay).

As in Corollary 6.2, it’s easy to see that each column of ut at any time t is always a linear combination
of the columns of u0 and the rows of v0 such that the coefﬁcients of these linear combinations converge
deterministically in the n → ∞ limit; likewise for bt and the rows of vt. To track the evolution of f ,
it sufﬁces to track these coefﬁcients. Therefore, for implementation, we reparametrize as follows:

36

Coefﬁcient matrix and vector Let µ1, . . . , µd, ν1, . . . , νdo ∈ Rn be standard Gaussian vectors
such that the columns of u0 will be initialized as σuµ1/
n and the rows of V0 will
n. Write µ = (µ1, . . . , µd) ∈ Rn×d, ν = (ν1, . . . , νdo ) ∈
be initialized as σvν1/
Rn×do. Deﬁne coefﬁcient matrices

n, . . . , σvνdo/

n, . . . , σuµd/

√

√

√

√

u(cid:62) ∈ Rd×(d+do), v ∈ Rdo×(d+do),

such that at any time, (u, v(cid:62)) ∈ Rn×(d+do) is
initialize

1√
n (µ, ν)(u, v(cid:62)) in the inﬁnite-width limit. We

(cid:19)

(cid:18)u(cid:62)
v

(cid:18)σuI
0

←

(cid:19)

,

0
σvI

i.e. a “diagonal” initialization. Likewise, deﬁne coefﬁcient vector b ∈ Rd+do, initialized at 0,
such that, at any time, b is approximately distributed as 1√
n (µ, ν)b. To track the evolution of the
inﬁnite-width network, we will track the evolution of u, v, b.

In general, we use bold to denote the coefﬁcients (in µ, ν) of a tensor (e.g. b for coefﬁcients of b). We
also use capital letters to denote the batched version (e.g. H for batched version of h). Algorithms 2
and 3 below summarize the SGD training of the ﬁnite- and the inﬁnite-width networks. Note that
aside from initialization and the hidden size (n vs d + do), the algorithms are essentially identical.

Algorithm 2 SGD Training of Finite-Width Lin-
ear µP 1-Hidden-Layer Network
Input: Hyperparameters n, σu, σv, α, η, g, γ.
1: Initialize uαβ ∼ N (0, σ2
2: Initialize vαβ ∼ N (0, σ2
3: Initialize b ← 0
4: for each batch of inputs Ξ ∈ RB×d and la-

u/n)
v/n)

Algorithm 3 SGD Training of Inﬁnite-Width Lin-
ear µP 1-Hidden-Layer Network
Input: Hyperparameters σu, σv, α, η, g, γ.
1: Initialize u(cid:62) ← (σuI, 0)
2: Initialize v ← (0, σvI)
3: Initialize b ← 0
4: for each batch of inputs Ξ ∈ RB×d and la-

bels Y ∈ RB×do do
// Forward Pass
H ← Ξu(cid:62) + b ∈ RB×n
f (Ξ) ← Hv(cid:62) ∈ RB×do
// Backward Pass
χ ← L(cid:48)(f (Ξ), Y ) ∈ RB×do
du ← −v(cid:62)χ(cid:62)Ξ ∈ Rn×d
dv ← −χ(cid:62)H ∈ Rdo×n
db ← −α21(cid:62)χv ∈ Rn
// Gradient Clipping
(cid:107)du(cid:107)2

(cid:113)

5:
6:
7:
8:
9:
10:
11:
12:
13:

14:

α (cid:107)2

F + (cid:107) db

F + (cid:107)dv(cid:107)2

G ←
ρ ← min(1, g/G)
du ← ρdu
dv ← ρdv
db ← ρdb
// Gradient Step w/ Weight Decay
u += ηdu − ηγu ∈ Rd×n
v += ηdv − ηγv ∈ Rdo×n
b += ηdb − ηγb ∈ Rn

15:
16:
17:
18:
19:
20:
21:
22:
23: end for

bels Y ∈ RB×do do
// Forward Pass
H ← Ξu(cid:62) + b ∈ RB×(d+do)
f (Ξ) ← Hv(cid:62) ∈ RB×do
// Backward Pass
χ ← L(cid:48)(f (Ξ), Y ) ∈ RB×do
du ← −v(cid:62)χ(cid:62)Ξ ∈ R(d+do)×d
dv ← −χ(cid:62)H ∈ Rdo×(d+do)
db ← −α21(cid:62)χv ∈ Rd+do
// Gradient Clipping

5:
6:
7:
8:
9:
10:
11:
12:
13:

14:

(cid:113)

α (cid:107)2

(cid:107)du(cid:107)2

F + (cid:107) db

F + (cid:107)dv(cid:107)2

G ←
ρ ← min(1, g/G)
du ← ρdu
dv ← ρdv
db ← ρdb
// Gradient Step w/ Weight Decay
u += ηdu − ηγu ∈ R(d+do)×d
v += ηdv − ηγv ∈ Rdo×(d+do)
b += ηdb − ηγb ∈ Rd+do

15:
16:
17:
18:
19:
20:
21:
22:
23: end for

During inference, we just run the Forward Pass section with Ξ substituted with test data.

The algorithms for MAML can then be obtained by a straightforward modiﬁcation of these algorithms.
(Note that in MAML, we do not clip gradients during adaptation, but rather clip the gradient against
the validation loss of task; we also disable weight decay by setting the coefﬁcient γ to 0).

Hyperparameter Sweep We sweep σu, σv, η and α with the following grid for ﬁnite width and
µP networks.

• σu : [0.5, 1, 2, 4, 8],

37

Algorithm 4 MAML Training of Kernel Model with Kernel K

χi ← L(cid:48)(fQ(ξi), yi)

Draw a batch of tasks
for each task in batch do

end for
for each input/label pair (ξi, yi) ∈ D do

// Adaptation
Sample training set D
for each input/label pair (ξi, yi) ∈ D do

Input: Kernel K, adaptation step size (cid:15), meta learning rate η, batch size B, gradient clip g
1: Initialize Q = {}
2: while True do
3:
4:
5:
6:
7:
8:
9:
10:
11:
12:
13:
14:
15:
16:
17:
18:
19:
20:
21:

end for
// Calculate Test Set Gradient
Sample test set ˆD
for each input/label pair ( ˆξi, ˆyi) ∈ ˆD do

end for
for each input/label pair (ξi, yi) ∈ D do

ˆχi ← L(cid:48)(fQ( ˆξi), ˆyi)

Q.push((ξi, −(cid:15)χi))

Q.pop((ξi, −(cid:15)χi))

end for
// Gradient Clip
(cid:113)(cid:80)

(cid:80)

( ˆξj ,ˆyj )∈ ˆD ˆχi ˆχjK( ˆξi, ˆξj)

( ˆξi,ˆyi)∈ ˆD

G ←
ρ ← min(1, g/G)
// Gradient Update
for each input/label pair ( ˆξi, ˆyi) ∈ ˆD do

Q.push(( ˆξi, −ρη ˆχi))

end for

22:

23:
24:
25:
26:
27:
end for
28:
29: end while

• σv : [2−5, 2−4, 2−3, 2−2, 2−1],

• η : [0.025, 0.05, 0.1, 0.2, 0.4],

• α : [0.25, 0.5, 1, 2, 4]

We are interested in 1-shot, 5-way learning with Omniglot. This means that each task provides
5 training samples, each corresponding to one of the 5 labels of the task. Each hyperparameter
combination above is used to train for 100 epochs over 3 random seeds, where each epoch consists of
100 batches of 32 tasks. We average the validation accuracy across the last 10 epochs and document
the best hyperparameters in Table 4, along with the test accuracy from a 15-seed rerun47 for better
benchmarking. For NTK and GP, we additionally tune the initialization σb for biases, which is set to
0 for both ﬁnite and µP networks for simplicity.

D.1.2 NNGP and NTK for Relu Networks

Consider a kernel K, which in our case will be the NNGP or NTK of a 1-hidden-layer relu network.
WLOG, it is induced by an embedding Φ such that K(ξ, ζ) = (cid:104)Φ(ξ), Φ(ζ)(cid:105) where (cid:104), (cid:105) is the inner
product in the embedding space; we do not care about the details of Φ or (cid:104), (cid:105) as eventually our
algorithm only depends on K.

In our setting, we will train a linear layer W on top of Φ via MAML, f (ξ) def= (cid:104)W, Φ(ξ)(cid:105). One can see
easily that W is always a linear combination of Φ(ζ) for various ζ from the training set we’ve seen
so far. Thus, to track W , it sufﬁces to keep an array Q of pairs (ζ, q) such that W = (cid:80)
(ζ,q)∈Q qΦ(ζ)

47After excluding outliers at least one standard deviation away from the mean.

38

Table 4: Best hyperparameters for the MAML experiment.

log2Width/Limit
1
3
5
7
9
11
13
µP

σu
0.5
0.5
1
1
1
1
1
1

σv
0.5
0.25
0.125
0.125
0.03125
0.03125
0.03125
0.03125

NTK
GP

0.25
1

1
0.25

σb
-
-
-
-
-
-
-
-

1
1

η

0.05
0.1
0.4
0.1
0.1
0.1
0.1
0.1

0.05
0.05

α

2
1
0.5
1
1
1
1
1

1
1

Val. Acc. (%) Test Acc. (%)

46.72 ± 4.30
65.30 ± .27
68.74 ± .18
69.03 ± .04
69.32 ± .07
69.27 ± .11
69.27 ± .14
69.26 ± .13

55.34 ± 1.24
64.54 ± .70
66.21 ± .15
66.31 ± .16
66.43 ± .23
66.36 ± .22
66.41 ± .18
66.42 ± .19

47.47 ± .13
38.92 ± .15

47.82 ± .04
47.60 ± .02

at all times. Let fQ be the function with W given by Q. Then

fQ(ξ) =

(cid:88)

qζK(ζ, ξ).

(ζ,qζ )∈Q

In our case, the number of possible inputs is too large to instantiate a value q for every ζ, so we
gradually grow a dynamic array Q, which we model as a stack. Then MAML can be implemented as
in Algorithm 4.

Hyperparameter Sweep We sweep σu, σv, σb and η with the following grid for GP and NTK.

• σu : [0.25, 0.5, 1, 2, 4],
• σv : [0.25, 0.5, 1, 2, 4],
• σb : [0.25, 0.5, 1, 2, 4],
• η : [0.05, 0.1, 0.2, 0.4, 0.8]

Each hyperparameter combination above is used to train for 5 epochs (the ﬁrst epoch is almost always
the best) over 3 random seeds, where each epoch consists of 100 batches of 32 tasks. We take the
validation accuracy among all epochs and document the best hyperparameters in Table 4, along with
the test accuracy from a 15-seed rerun.

D.2 Word2Vec Experimental Details

D.2.1 µP Limit

We shall derive the training algorithm for µP Word2Vec. First, we introduce the notation for word
embeddings. We denote Φi def= h(ξi). If ξi is a one-hot vector with the ith element set to 1, Φi is
essentially the ith column of the weight matrix U . We also deﬁne the following short-hands for the
context embedding: ΦJ def= Ej∈J Φj = h(ξJ ). Similarly, V (cid:62)ξτ describes a row in V ; we can deﬁne
Φˆτ def= ˆh(ξτ ) def= V (cid:62)ξτ and rewrite the loss function.

L(f (ξJ ), ξτ ) =

(cid:26)log(1 − σ(ΦJ (cid:62)Φˆτ ))

log σ(ΦJ (cid:62)Φˆτ )

τ = i
τ (cid:54)= i.

Consequently, the backward pass becomes:

∆Φj =

1
|J|

∆ΦJ =

η
|J|

∂L
∂ΦJ =

(cid:40) η

|J| Φˆτ (1 − σ(ΦJ (cid:62)Φˆτ ))
− η
|J| Φˆτ σ(ΦJ (cid:62)Φˆτ )

τ = i
τ (cid:54)= i.

(37)

(38)

Following µP, we initialize Uαβ ∼ N (0, σun−1) and Vαβ ∼ N (0, σvn−1), where n is the width of
n in V cancel out because the
the ﬁnite network. (Here the explicit multipliers of
network is linear). The tunable hyperparameters are the initialization std σuand σv, learning rate η

n in U and 1/

√

√

39

and weight decay ratio γ. Rather than tuning the hyperparameters extensively for each width, we pick
some reasonable values and use them for all of our experiments. Speciﬁcally, we have σu = σv = 1,
η = 0.05 and γ = 0.001.
Again, using Corollary 6.2, we can train the µP limit in the coefﬁcient space of u(cid:62) ∈ R|V|×2|V|, v ∈
R|V|×2|V|, with the same “diagonal” initialization:

(cid:19)

(cid:18)u(cid:62)
v

(cid:18)σuI
0

←

(cid:19)

,

0
σvI

We can adopt the embedding notation and represent a row of u with the embedding coefﬁcient vector
Φ• and a column of v with Φˆ•. This is computationally equivalent to training with a hidden size of
2|V| and with embeddings initialized as rows (or columns) of one-hot vectors. The full algorithm is
described in Algorithm 2 and Algorithm 3; in this case, we remove biases and use weight decay with
coefﬁcient γ = 0.001. After training, rows of the weight matrix u (resp. coefﬁcient matrix u), i.e.
Φ• (resp. Φ•), are taken as the word vectors.

D.2.2 NTK Limit

In the NTK parametrization, V and U in Eq. (33) factor as V = 1√
n v and U = u, and the learning
rate is Θ(1). Each column U•i of U is equal to h(ξi). At any ﬁxed time t, it is easy to see via Tensor
Programs that

ht(ξi) = h0(ξi) +

O(1/

n)vj + Ocoord(1/n)

(cid:88)

√

j∈V

where vj denotes the jth row of v at initialization, and where Ocoord(1/n) means a vector that is
O(1/n) coordinatewise. Recall that U = u and v are initialized with iid standard Gaussian entries.
Because ξi is one-hot, this in particular implies h0(ξi) has standard Gaussian entries, and h0(ξi) is
independent from h0(ξj) for i (cid:54)= j. Then for any i (cid:54)= j,

1
√
n

ht(ξi)(cid:62)ht(ξj) −

1
√
n

h0(ξi)(cid:62)h0(ξj) a.s.−−→ 0,

1
√
n

h0(ξi)(cid:62)h0(ξj) d−→ N (0, 1)

by Law of Large Numbers (or more formally, Theorem 7.4) and Central Limit Theorem. In other
words, 1√
n h0(ξi)(cid:62)h0(ξj) is distributed completely randomly, with no regard to the semantic simi-
larities of i and j. Likewise, the inner product in Eq. (35) is random, and the argmax is a uniform
sample.48 Therefore, in the NTK limit, Word2Vec gives random answers and achieves an accuracy of
1
|V|−3 .

E More Detailed Comparison with Deep Mean Field Limits

The key idea of previous works [5, 15, 34, 35, 46] proposing multilayer mean ﬁeld limits of MLPs
is to initialize each n × n matrix W like Wαβ ← F (uα, vβ)/n for some function F and uα ∼
Z u, vβ ∼ Z v sampled iid for each α, β ∈ [n], where Z u and Z v are some ﬁxed (wrt n) random
variables. If x is an activation with approximately iid coordinates distributed like random variable Z x,
then W x looks like (W x)α ≈ EZv,Zx F (uα, Z v)Z x by Law of Large Numbers (LLN), roughly iid
across α. This logic will in fact hold throughout training. For well-chosen (F, Z u, Z v), this does not
get stuck at initialization but this form of initialization is very unnatural. In Nguyen & Pham (2020),
this is adapted to iid initialization straightforwardly. For example, this includes Wαβ ← N (0, 1)/n.
If x is as above, then (W x)α → 0 by LLN because W is sampled independently from x at init and
has 0 mean. This means that preactivations every layer will vanish coordinatewise to 0, from which
it’s easy to see that the gradients vanish where there are more than 2 hidden layers. Hence we say
that the function gets stuck at initialization. Contrast this 1/n scaling with the more typical 1/
n
scaling, i.e. Wαβ ← N (0, 1)/
n, which is what we deal with here. On a technical level, their limit
calculation purely goes through LLN, whereas we need to wrestle with Central Limit effects (from
the 1/

n scaling) as well.

√

√

√

48Here the randomness comes from initialization: the argmax is different for different random initializations,

but it is ﬁxed throughout training in the large width limit.

40

F Nuances of the Master Theorem

Remark F.1 (Partial derivative). The partial derivative in ZDot should be interpreted as follows. By a
simple inductive argument, Z x for every vector x in the program is deﬁned uniquely as a deterministic
function ϕ( ˆZ x1
) of some x1, . . . , xk in V or introduced by MatMul (notationally, we are
suppressing the possible dependence on limit scalars ˚θ1, . . . , ˚θl). For instance, if in a program we
have A ∈ W, v ∈ V, y = Av, x = A(cid:62)y, then Z x = ˆZ x + ˆZ v, so ϕ is given by ϕ(a, b) = a + b.
Then

, . . . , ˆZ xk

∂Z x/∂ ˆZ xi def= ∂iϕ( ˆZ x1

, . . . , ˆZ xk

),

and ∂Z x/∂ ˆZ z def= 0 for any z (cid:54)∈ {x1, . . . , xk}.

Note this deﬁnition depends on the precise way the program is written, not just on the underlying
mathematics. For example, if y, z ∈ V and x = φ(W (y + z)), then Z x = φ( ˆZW (y+z)) so that
∂Z x/∂ ˆZW y = ∂Z x/∂ ˆZW z = 0. If instead, we have x = φ(W y+W z), then Z x = φ( ˆZW y + ˆZW z)
so that ∂Z x/∂ ˆZW (x+y) = 0. However, in both cases,
Remark F.2 (Partial derivative expectation). The quantity E ∂Zx
∂ ˆZW (cid:62) y is well deﬁned if Z x is differen-
tiable in ˆZW (cid:62)y. However, even if this is not the case, e.g. if x = θ(W (cid:62)y) where θ is the Heavyside
step function, we can still deﬁne this expectation by leveraging Stein’s lemma:
In ZDot, suppose {W (cid:62)yi}k
def= E Z yi
C ∈ Rk×k by Cij
and deﬁne the vector b ∈ Rk by bi
(where C + denotes the pseudoinverse of C), then in ZDot we may set

i=1 are all elements of VW (cid:62) introduced before x. Deﬁne the matrix
Z yj
Z x. If a = C +b

˙ZW (cid:62)x = (Z y + Z z) E φ(cid:48)( ˆZW (y+z)).

def= E ˆZW (cid:62)yi

∂ ˆZW (cid:62)yi
This deﬁnition agrees with the partial derivative expectation by Stein’s lemma when the latter is well
deﬁned. Theorem 7.4 holds with this broader deﬁnition of partial derivative expectation.

(39)

E ∂Z x

σ2
W

= ai.

are, roughly speaking, functions whose weak derivatives are polyno-

Pseudo-Lipschitz functions
mially bounded.
Deﬁnition F.3. A function f : Rk → R is called pseudo-Lipschitz of degree d if |f (x) − f (y)| ≤
C(cid:107)x − y(cid:107)(1 + (cid:80)k
i=1 |xi|d + |yi|d) for some C. We say f is pseudo-Lipschitz if it is so for any
degree.

Here are some basic properties of pseudo-Lipschitz functions:

• The norm (cid:107) · (cid:107) in Deﬁnition F.3 can be any norm equivalent to the (cid:96)2 norm, e.g. (cid:96)p, p ≥ 1,

norms. Similarly, (cid:80)k

i=1 |xi|d + |yi|d can be replaced by (cid:107)x(cid:107)d

p + (cid:107)y(cid:107)d

p, for any p ≥ 1.

• A pseudo-Lipschitz function is polynomially bounded.
• A composition of pseudo-Lipschitz functions of degrees d1 and d2 is pseudo-Lipschitz of

degree d1 + d2.

• A pseudo-Lipschitz function is Lipschitz on any compact set.

We adopt the following assumption for the Master Theorem Theorem 7.4.
Assumption F.4. Suppose

1. If a function φ(; −) : R0+l → R with only parameter arguments is used in Moment, then φ

is continuous in those arguments.

2. Any other function φ(−; −) : Rk+l → R with parameters (where k > 0) used in Nonlin or

Moment is pseudo-Lipschitz in all of its arguments (both inputs and parameters).

Statement 1 in Assumption F.4 essentially says that if we have scalars θ1, . . . , θl in the program,
then we can produce a new scalar by applying a continuous function (a weaker restriction than a
pseudo-Lipschitz function) to them. Indeed, if θ1, . . . , θl converge almost surely, then this new scalar
does too. In our setting, statement 1 is used to allow any loss function whose derivative is continuous.

Other versions of the Master Theorem can be found in [52], for example, versions where the we do
not assume any smoothness condition at all on the nonlinearities beyond that they be polynomially

41

bounded, in exchange for assuming what’s called a rank stability condition. This rank stability should
be generically true, but checking it rigorously is subtle, so we are content with the pseudo-Lipschitz
condition in this paper.

G A Rough Sketch of the Geometry of abc-Parametrizations

By the results of Section 3.2, the stable abc-parametrizations form a polyhedron deﬁned by the
inequalities of Theorem 3.3. We call the polyhedron obtained by quotienting Eq. (5) the stable
polyhedron. In this section, we remark on some geometric properties of this polyhedron.

First, observe that the stable polyhedron is unbounded (thus, we say polyhedron instead of polytope).
Indeed, given any stable parametrization, for any l, we can set al ← al + θ, bl ← bl − θ for any
θ ≥ 0 to obtain another stable parametrization. This corresponds decreasing the layer l learning rate,
so that as θ → ∞, W l is not trained.

Second, by Theorem 3.4, the nontrivial parametrizations reside in two facets of the stable polyhedron.
These facets are unbounded for the same reason as above.

Next, we show that NTP (as well as µP) is a vertex on the intersection of these two facets, and NTP
and µP are connected by an edge.
Deﬁnition G.1. Consider a stable abc-parametrization of the MLP in Eq. (1). We say the body of the
t(ξ) = Θ(n−r)
MLP is uniformly updated if, for some training routine, time t ≥ 1, and input ξ, ∆W l
for all l simultaneously, where r is as deﬁned in Deﬁnition 3.2.

t xl

In the results of this section below, we assume Assumption H.22.
Proposition G.2. In a stable abc-parametrization, the MLP body is uniformly updated iff rl = r for
all l ∈ [L], where rl is as deﬁned in Proposition 5.3.
Theorem G.3. In NTP, the MLP body is updated uniformly and W L+1 is both initialized and updated
maximally. Furthermore, at initialization, f0 converges in distribution49 to a Gaussian Process with
nonzero kernel. NTP is the unique (modulo Eq. (5)) stable abc-parametrization with both of these
properties.
Theorem G.4. For any r ∈ [0, 1/2], there is a unique (modulo Eq. (5)) stable abc-parametrization
with 1) that value of r and the property that 2) the MLP body is updated uniformly and W L+1 is both
initialized and updated maximally. We call this parametrization the Uniform Parametrization with
r-value r, denoted UPr. Its abc values are
1
2

I(l = 1) + r ∀l ∈ [L], aL+1 = 1/2;

bl = 1/2 − r;

al = −

c = 0.

In particular, UP0 is µP and UP1/2 is NTP. For r > 1/2, such a uniform parametrization is not
stable because W0 would need to be Θ(nr−1), which would cause the initial GP to blow up. Thus,
geometrically, UPr, r ∈ [0, 1/2], form an edge of the stable polyhedron.
We can deﬁne the uniform stable polyhedron to be the subset of the stable polyhedron corresponding to
parametrizations which update the MLP body uniformly. This is isomorphic to the stable polyhedron
when L = 1. Since stable abc-parametrizations with L = 1 has only 3 degrees of freedom, say
a1, a2, b2 while we ﬁx c = 0 (via Eq. (5)) and b1 = −a1, we can visualize the corresponding stable
polyhedron in 3D. However, the nontrivial parametrizations only reside in the boundary of this
polyhedron. Because of its unbounded nature, we can project its boundary in 2D and visualize it.
This is done in Fig. 5.

H Proofs of Main Results

H.1 Rigorous Statements of Main Results

Applicable Nonlinearities For technical reasons, in our main results we restrict our attention to
the canonical examples of nonlinearities: tanh and relu — or rather, a smooth version of relu called
gelu [24] common in transformer models [8]. More precisely,

49as is conventional in the machine learning literature, the convergence in distribution we mean here is really
over ﬁnite dimensional marginals, i.e. (f0(ξ1), . . . , f0(ξk)) d−→ (˚f0(ξ1), . . . , ˚f0(ξk)) where ˚f0 is the limit GP.

42

Figure 5: 2D Projection of the Boundary of the Uniform Stable Polyhedron (Equivalently, the
Boundary of the Stable Polyhedron for L = 1). Here, we label each facet and edge of the graph
with orange text to indicate the corresponding deﬁning algebraic condition in the L = 1 case (as part
of the stable polyhedron, assuming c = 0 and b1 = −a1), and with black text to indicate the verbal
interpretation valid for all L (as part of the uniform stable polyhedron). We obtain the caricature in
Fig. 2 by taking the nontrivial subspace of the graph here and quotienting the two facets by their
respective points at inﬁnity. Explanation of some captions: GP limit means the training dynamics
amounts to training only the last layer in the inﬁnite-width limit, starting from a nonzero initial GP.
Body NTK limit means NTK dynamics except the last layer does not contribute to the NT kernel.

Deﬁnition H.1. Deﬁne σ-gelu to be the function x (cid:55)→ 1

2 xerf(σ−1x) + σ e−σ−2x2

π + x
2 .

√

2

σ-gelu is a smooth approximation of relu and is the integral of 1
2 (erf(σ−1x) + 1) that is 0 at −∞. The
large σ is, the smoother σ-gelu is. As σ → 0, σ-gelu converges to relu. We believe our results will
hold for generic nonlinearities, but making this precise is outside our scope here. (See Remark H.15
for some discussion).

Notations and Terminologies
Deﬁnition H.2 (Big-O Notation). Given a sequence of scalar random variables c = {cn ∈ R}∞
n=1,
we write c = Θ(n−a) if there exist constants A, B such that An−a ≤ |c| ≤ Bn−a for sufﬁciently
large n, almost surely50. Given a sequence of random vectors x = {xn ∈ Rn}∞
n=1, we say x has
coordinates of size Θ(n−a) and write x = Θ(n−a) to mean the scalar random variable sequence
{(cid:112)(cid:107)xn(cid:107)2/n}n is Θ(n−a). Similarly for the notations O(n−a), Ω(n−a). We use the notations
Θξ(n−a), Oξ(n−a), Ωξ(n−a) if the hidden constants A, B are allowed to depend on some object ξ.
For brevity, we will often abuse notation and say c itself is a random variable or x itself is a random
vector.

Most often, the vector x will have “approximately iid” coordinates, so the notation x = Θ(n−a) can
be interpreted intuitively to say x has coordinates of “standard deviation” Θ(n−a), which justiﬁes
the name.
Deﬁnition H.3. An abc-parametrization is a joint parametrization of an MLP and the learning rate
speciﬁed by the numbers {al, bl}l ∪ {c} as in Eq. (1). Below we will often say abc-parametrization
of an MLP for short, even though the parametrization affects the learning rate as well. A training
routine is a combination of learning rate ηn−c, training sequence {(ξt, yt)}t≥0, and a loss function
L(f (ξ), y) that is continuously differentiable in the prediction of the model f (ξ).

Main Results We will mainly focus on stable parametrizations, deﬁned below, which intuitively
means 1) the preactivations {hl}l and activations {xl}l have Θ(1) coordinates at initialization, and
2) their coordinates and the logit f (ξ) all stay O(1) (i.e. bounded independent of n) throughout the
course of SGD.51 Otherwise, they tend to ∞ with n, eventually going out of ﬂoating point range.

50Here almost surely means for almost every instantiation of c1, c2, . . ., i.e. it is with regard to the product
probability space generated by all of {cn}∞
n=1. In this paper, this probability space will be generated by random
initializations of a neural network at every width n. Very importantly, note the order of the qualiﬁers: we are
saying for almost every instantiation of c1, c2, . . ., for large enough n, An−a ≤ |c| ≤ Bn−a.

51but they may depend on training time and η; in particular, it’s possible that they diverge with time

43

𝑎1=−12𝑎1=0𝑎2+𝑏2=1/2𝑏2=0−𝑎1=𝑎2=𝑏2𝑓0is a nonzero GP𝜇PNTP𝑎2+𝑏2+2𝑎1=0𝑎2+𝑎1=0𝑎2=1/2𝑎2+𝑏2+𝑎1=1/2𝑊𝐿+1updatedmaximally 𝑊𝐿+1init. maximally𝑟=0Feature Learning𝑟>0Kernel RegimeNontrivialfeature learningΔ𝑊𝐿+1dominates𝑊0𝐿+1𝑊0𝐿+1dominatesΔ𝑊𝐿+1Boundary of Uniform Stable PolyhedronTrivialNontrivialLegendBody NTK limitIndeed, this is an acute and real problem common in modern deep learning, where ﬂoat16 is necessary
to train large models.
Deﬁnition H.4 (Stability). We say an abc-parametrization of an L-hidden layer MLP is stable if

1. For every nonzero input ξ ∈ X ,

0(ξ), xl
hl

0(ξ) = Θξ(1), ∀l ∈ [L],

and E f0(ξ)2 = Oξ(1),

(40)

where the expectation is taken over the random initialization.

2. For any training routine, any time t ≥ 0, l ∈ [L], ξ ∈ X , we have

∆hl

t(ξ), ∆xl

t(ξ) = O∗(1), ∀l ∈ [L],

and

ft(ξ) = O∗(1),

where the hidden constant inside O can depend on the training routine, t, ξ, and the initial
function values f0(X ).52

Recall from the main text,
Deﬁnition H.5. For any abc-parametrization, we write r for the quantity

r def= min(aL+1 + bL+1, 2aL+1 + c) + c − 1 +

L
min
l=1

[2al + I(l = 1)] .

Intuitively, r is the exponent such that
t (ξ) = Θξ(n−r). Thus, to avoid activation blowup, we want r ≥ 0; to perform feature learning,

For example, in NTP, r = 1/2, while in µP, r = 0.
∆xL
we want r = 0.
Theorem H.6 (Stability Characterization). Suppose φ is tanh or σ-gelu for sufﬁciently small σ. An
abc-parametrization is stable iff all of the following are true (with intuitions in parentheses):

1. ((pre)activations at initialization are Θ(1) and logits are O(1))

a1 + b1 = 0;

al + bl = 1/2, ∀l ∈ [2, L];

aL+1 + bL+1 ≥ 1/2.

(41)

2. (features don’t blowup, i.e. ∆xl

t = O(1) for all l)
r ≥ 0.

3. (logits don’t blow up during training, i.e. ∆W L+1

2aL+1 + c ≥ 1;

Here, r is as deﬁned in Deﬁnition H.5.

t

t , W L+1
xL

0 ∆xL
aL+1 + bL+1 + r ≥ 1.

t = O(1))

(42)

(43)

t

turns out to be Θ(n−(2aL+1+c)) and is correlated with xL

In Eq. (43), ∆W L+1
t = Θ(1) such that
their product behaves according to Law of Large Numbers; the ﬁrst inequality says this should not
blow up. Similarly, W L+1
t = Θ(n−r) and they will
= Θ(n−(aL+1+bL+1)) and it turns out ∆xL
interact via Law of Large Numbers, so the second inequality says their product shouldn’t blow up.

0

Our main results concern nontrivial parametrizations:
Deﬁnition H.7 (Nontriviality). We say an abc-parametrization of an L-hidden layer MLP is trivial if
for every training routine, ft(ξ) − f0(ξ) a.s.−−→ 0 for any time t ≥ 1 and input ξ ∈ X (i.e. the function
does not evolve in the inﬁnite-width limit). We say the parametrization is nontrivial otherwise.

Theorem H.8 (Nontriviality Characterization). Suppose φ is tanh or σ-gelu for sufﬁciently small σ.
A stable abc-parametrization is nontrivial iff aL+1 + bL+1 + r = 1 or 2aL+1 + c = 1.
Deﬁnition H.9 (Feature Learning). We say an abc-parametrization of an L-hidden layer MLP admits
feature learning in the lth layer if there exists some training routine such that

∆xl

t(ξ) = Ω∗(1)

(44)

52For e.g. the NTK limit, f0 is a GP, so that we should expect the bounds on ∆hl

t(ξ), ∆xl

t(ξ) to depend on

f0.

44

for some t ≥ 0, ξ ∈ X , where the hidden constant inside Ω can depend on the training routine, t, ξ,
and the initial function values f0(X ). We say the parametrization admits feature learning if it does
so in any layer.

We say the parametrization ﬁxes the lth layer features if for all training routine,

(cid:107)∆xl

t(ξ)(cid:107)2/n a.s.−−→ 0

for all t ≥ 0, ξ ∈ X . We say the parametrization ﬁxes all features if it does so in every layer.

We make similar deﬁnitions as above replacing feature with prefeature and xl with hl.

Note that the probabilistic nature of Ω∗(1) means that no feature learning does not imply ﬁxing all
features (because ∆xl
t(ξ) can just ﬂuctuate wildly between 0 and inﬁnity), but we will see that in the
context of nontrivial stable abc-parametrizations, this is true.
Remark H.10. We note that this is a rather weak notion of “feature learning”, as we only require that
the embedding xL
t (ξ) changes from its initialization for some scenario, rather than, say for generic
scenarios; nor do we speak at all about the “quality” of feature learning, e.g. how it helps downstream
tasks. But our proofs (see Appendix H.7) will show that “some scenario” in fact implies much more
general scenarios. In addition, we argue that such formal weakness is more than compensated by
our experiments, which show that inﬁnite-width limits of feature learning (in the sense deﬁned here)
abc-parametrized MLPs outperform ﬁnite MLPs and their NTK limits on tasks (namely, Word2Vec
and few-shot learning) where feature learning, in the colloquial notion of the phrase, is crucial.

A somewhat stronger notion of feature learning is that the feature kernel evolves. This is, for example,
essential for linear transfer learning such as in self-supervised learning of image data.
Deﬁnition H.11 (Feature Kernel Evolution). We say an abc-parametrization of an L-hidden layer
MLP evolves the lth layer feature kernel if there exists some training routine such that
t(ζ)/n − xl

0(ζ)/n = Ω∗(1)

t(ξ)(cid:62)xl
xl

0(ξ)(cid:62)xl

for some t ≥ 0, ξ, ζ ∈ X , where the hidden constant inside Ω can depend on the training routine, t,
ξ, ζ, and the initial function values f0(X ). We say the parametrization evolves feature kernels if it
does so in any layer.

We say the parametrization ﬁxes the lth layer feature kernel if for all training routine,

t(ξ)(cid:62)xl
xl

t(ζ)/n − xl

0(ξ)(cid:62)xl

0(ζ)/n a.s.−−→ 0,

as n → ∞,

for all t ≥ 0, ξ, ζ ∈ X . We say the parametrization ﬁxes all feature kernels if it does so in every layer.

We make similar deﬁnitions as above replacing feature with prefeature and xl with hl.

Intuitively, for a stable parametrization, feature kernel evolution should imply feature learning (one
can see the contrapositive easily). In fact, we shall see below they are equivalent notions.

On the other hand, from the NTK example, we know certain limits can be described entirely through
kernel gradient descent with some kernel. Appropriately, we make the following deﬁnition.
Deﬁnition H.12 (Kernel Regime). We say an abc-parametrization of an L-hidden layer MLP is in
kernel regime if there exists a positive semideﬁnite kernel K : X 2 → R such that for every training
routine, the MLP function evolves under kernel gradient descent, i.e. there exist random variables
˚ft(ξ) for each time t ≥ 0 and input ξ ∈ X such that, as n → ∞,53

{ft(ξ)}t≤T,ξ∈X

d−→ {˚ft(ξ)}t≤T,ξ∈X ,

∀T ≥ 1,

where d−→ denotes convergence in distribution, and

˚ft+1(ξ) = ˚ft(ξ) − ηK(ξ, ξt)L(cid:48)(˚ft(ξt), yt),

∀t ≥ 0.

(45)

Observe that, in kernel regime, ˚ft(ξ) is deterministic conditioned on ˚f0(ξ), as evident inductively
from Eq. (45). For example, in the NTK limit, {˚f0(ξ) : ξ ∈ X } is a nontrivial Gaussian Process (GP),
but the function evolution conditioned on this GP is deterministic.

All of the concepts deﬁned above are related to each other by the following theorem.

53Here because we want to avoid topological issues arising for convergence in distribution of inﬁnite sequences,
we only require convergence in distribution jointly in all ξ ∈ X and time t below some cutoff T for every ﬁnite
T .

45

Theorem H.13 (Classiﬁcation of abc-Parametrizations). Suppose φ is tanh or σ-gelu for sufﬁciently
small σ. Consider a nontrivial stable abc-parametrization of an L-hidden layer MLP. Then

1. The following are equivalent to r = 0

(a) feature learning
(b) feature learning in the Lth layer
(c) feature kernels evolution
(d) feature kernel evolution in the Lth layer
(e) prefeature learning
(f) prefeature learning in the Lth layer
(g) prefeature kernels evolution
(h) prefeature kernel evolution in the Lth layer

2. The following are equivalent to r > 0

(a) kernel regime
(b) ﬁxes all features
(c) ﬁxes features in the Lth layer
(d) ﬁxes all feature kernels
(e) ﬁxes feature kernel in the Lth layer
(f) ﬁxes all prefeatures
(g) ﬁxes prefeatures in the Lth layer
(h) ﬁxes all prefeature kernels
(i) ﬁxes prefeature kernel in the Lth layer

3. If there is feature learning or feature kernel evolution or prefeature learning or prefeature
kernel evolution in layer l, then there is feature learning and feature kernel evolution and
prefeature learning and prefeature kernel evolution in layers l, . . . , L.

4. If r = 0, then for all ξ ∈ X , f0(ξ) a.s.−−→ 0 and ft(ξ) a.s.−−→ ˚ft(ξ) for some deterministic ˚ft(ξ).

However, the converse is not true.

5. If r > 0, aL+1 + bL+1 + r > 1 and 2aL+1 + c = 1, then we have the Neural Network-

Gaussian Process limit.

In particular, Statement 4 implies that feature learning, at least in our context, is incompatible with
Bayesian, distributional perspectives of neural network limits, such as the NNGP limit.

The characterization above then trivially implies the following dichotomy.
Corollary H.14 (Dynamical Dichotomy). For φ being tanh or σ-gelu for sufﬁciently small σ, a
nontrivial stable parametrization of an L-hidden layer MLP either admits feature learning or is in
kernel regime, but not both.
Remark H.15 (The Role of the φ Assumption). The dependence on φ being tanh or σ-gelu for
sufﬁciently small σ is only needed to explicitly construct a training routine that leads to feature
learning for r = 0. We expect this should be true for generic φ, but we leave this for future work. We
expand more on how the φ assumption is used below.

To calculate the inﬁnite width limit of any abc-parametrization rigorously, we only need the nonlin-
earity to have a polynomially bounded 2nd derivative (or more generally pseudo-Lipschitz, so as to
apply the Master Theorem). The speciﬁc choice of tanh or gelu is needed to prove the part of the
Dynamical Dichotomy that says a limit cannot be simultaneously in kernel regime and in feature
learning regime (which, e.g. is not true for linear activation). To do so, we use Properties H.44 and
H.47 of tanh and gelu, expanded below. This is really for a more convenient proof, but we believe a
more general approach should work for general nonlinearities. Our argument is as follows (this is
also overviewed in the start of Appendix H.7): If r = 0, we show that a sufﬁciently small nonzero
learning rate (scaled with width in the corresponding parametrization) in 1 SGD step 1) induces a
change in the features but 2) the resulting change in the NN output is not linear in the loss derivative
χ. 1) means it’s feature learning, and 2) means it’s not in kernel regime. This argument involves
showing certain derivatives of certain expectations with respect to learning rate is positive. In the
case of tanh and gelu, this is checked explicitly using Properties H.44 and H.47.

46

Remark H.16. The equivalence between kernel regime and ﬁxed feature kernel implies that linear
transfer learning is trivialized in any kernel regime limit. This is where the classiﬁer layer of the
pretrained network is discarded and a new one (potentially outputting to a new output space) is trained
on top of the body of the pretrained network. But we can in fact say more: any nonlinear transfer
learning, where we replace the classiﬁer layer with a neural network instead of a linear layer, is
trivialized as well. In addition, linear or nonlinear transfer learning has no effect even if we ﬁnetune
the entire network, instead of just the new classiﬁcation network. The intuitive reason for this is
that, as discussed in Appendix B, the effect of ∆xL(ξ) on the output of the MLP is solely through
the interaction with W L+1
. If W L+1, W L+2, . . . , are sampled anew, then this effect vanishes. We
formalize this below.

0

Theorem H.17 (Kernel Regime Limit Trivializes Transfer Learning). Suppose f is an L-hidden-layer
MLP54 in a stable kernel regime parametrization. Let A and B be two training routines.55
For any T, t ≥ 0,56 we deﬁne a network57 gT ;t as follows. Train f on A for T steps to obtain fT . Then
discard W L+1 in fT and extend the body of fT into an M -hidden-layer MLP g, where M ≥ L.58
Parametrize and initialize the new weights of g according to any stable abc-parametrization that
extends the parametrization of f . Train g on B for t steps to obtain gT ;t.
Then

1. (Finetuning the whole network) As n → ∞, for any ξ ∈ X and T, t ≥ 0,

gT ;t(ξ) − g0;t(ξ) a.s.−−→ 0.

2. (Training only the classiﬁer) The above is true even if we deﬁne gT ;t by only training the

new weights W L+1, . . . , W M in g.

The Organization for the Proof of Our Main Results Above
Deﬁnition H.18. Below, we will abbreviate abc-parametrization of an L-layer MLP to just
parametrization. We will call parametrizations satisfying the conditions of Theorem H.6 pseudostable
while we try to prove Theorem H.6 (which, in this terminology, says stability and pseudostability are
equivalent).

We ﬁrst characterize stability at initialization and prove Eq. (40) holds iff Eq. (41) (Appendix H.2).
Then, we describe the Tensor Program encoding the SGD of an MLP, assuming its parametrization
is pseudostable. The Master Theorem then naturally lets us calculate its inﬁnite-width limit. We
then divide into the case of r > 0 and r = 0. In the former case, we show the inﬁnite-width limit is
described by kernel gradient descent as in Eq. (45). In the latter case, we construct a training routine
where feature learning occurs and where the limit is not given by kernel gradient descent for any
kernel. Finally, in Appendix H.8, we combine all of our analyses to prove the main results in this
section.

H.2 Stability at Initialization

In this section, we characterize stability at initialization, which will form a foundation for our later
results.
Theorem H.19. Assume φ is not zero almost everywhere. For any parametrization, Eq. (40) holds iff
Eq. (41) holds, i.e. the following are equivalent

1. For every nonzero input ξ ∈ X ,

0(ξ), xl
hl

0(ξ) = Θξ(1), ∀l ∈ [L],

and E f0(ξ)2 = Oξ(1),

where the expectation is taken over the random initialization.

54the “pretrained network”
55the “pretraining dataset” and the “ﬁnetuning dataset”
56the “pretraining time” and “ﬁnetuning time”
57the “ﬁnetuned network”
58If M = L, then this is linear transfer learning where we replace just the last layer of f ; otherwise, it’s

nonlinear transfer learning.

47

2. a1 + b1 = 0;

al + bl = 1/2, ∀l ∈ [2, L];

aL+1 + bL+1 ≥ 1/2.

Proof. Fix an input ξ (cid:54)= 0. Here, because we focus on initialization, we will suppress the time 0
subscript and ξ dependence of hl, xl to mean t = 0, applied to ξ.

Obviously, h1 = W 1ξ is a Gaussian vector with N (0, n−(a1+b1)(cid:107)ξ(cid:107)2) coordinates, so h1 = Θξ(1)
iff a1 + b1 = 0. Assume a1 + b1 = 0. By Law of Large Numbers, 1
)2 where
Z h1
= N (0, (cid:107)ξ(cid:107)2). Since φ is not almost everywhere zero and ξ (cid:54)= 0, this expectation is nonzero so
that x1 = Θξ(1).
We construct the following Tensor Program: the lone initial vector is h1, the initial matrices are
(cid:99)W l, 2 ≤ l ≤ L, and initial scalars θl
αβ ∼
N (0, 1/n). Mathematically, we will represent W l = θl(cid:99)W l. The program is then given by

def= n1/2−(al+bl). We sample h1

n (cid:107)x1(cid:107)2 a.s.−−→ E φ(Z h1

α ∼ N (0, (cid:107)ξ(cid:107)2) and (cid:99)W l

xl = φ(hl), ∀l ∈ [L],

ˆhl = (cid:99)W lxl−1, hl = θl

ˆhl, ∀l ∈ [2, L],

= Z ˆhl

= N (0, E φ(Z hl−1

)2 > 0) for all l ≤ L. By the Master Theorem, 1
)2 so this implies hl, xl = Θξ(1) for all l ≤ L as desired.

where we used Nonlin, MatMul, and Nonlin (with parameter θl).
Suppose al + bl = 1/2 (i.e. θl = 1) for all 2 ≤ l ≤ L. Then, Z hl
for each l ≤ L. Because φ is not everywhere zero, this inductively implies E(Z hl
also E(Z xl
E(Z xl
Conversely, suppose m is the smallest l ≥ 2 such that al + bl (cid:54)= 1/2. Then by the above reasoning,
ˆhm = Θξ(1) so hm = Θξ(n1/2−(al+bl)) is either blowing up to ∞ or shrinking to 0 with n. This
shows that hl, xl = Θξ(1) for all l ≤ L iff a1 + b1 = 0 and al + bl = 1/2 for all 2 ≤ l ≤ L.
Finally, if a1 + b1 = 0 and al + bl = 1/2 for all 2 ≤ l ≤ L, then we see E f0(ξ)2 =
(n1/2−(aL+1+bL+1))2 E (cid:107)Z xL
(cid:107)2/n. For large n, this is Θξ((n1/2−(aL+1+bL+1))2) and is Oξ(1) iff
aL+1 + bL+1 ≥ 1/2.

)2)
)2 > 0 (and so
n (cid:107)xl(cid:107)2 a.s.−−→

n (cid:107)hl(cid:107)2 a.s.−−→ E(Z hl

)2 and 1

Deﬁnition H.20. We say a parametrization is initialization-stable if it satisﬁes Eq. (40) (or equiva-
lently, Eq. (41)).

H.3 Program Setup

In the next section, we construct the Tensor Program that encodes the training of an L-hidden layer
MLP under an abc-parametrization. Here we ﬁrst describe the initial matrices, vectors, and scalars of
the program, along with necessary notations.

We ﬁrst remark on a simpliﬁcation we will make to streamline the proof.

t

t

0

0

0

vs ∆W L+1

By construction, W L+1

is at least as large as ∆W L+1

The Size of W L+1
= Θ(n−(aL+1+bL+1)). If xL
t (ξ) = Θ(1)
as in a stable parametrization, then ∆W L+1
= Θ(n−(2aL+1+c)). Therefore, if aL+1 + bL+1 ≤
2aL+1 + c, then W L+1
will stay the same order (in
t
terms of n) for all t. If the reverse inequality is true, then W L+1
for t ≥ 1.
This in particular implies that the gradients at time 0 is smaller than gradients at subsequent times.
For example, we can take aL+1 + bL+1 → ∞ while ﬁxing 2aL+1 + c, in which case W L+1
= 0 and
the weight gradients at initialization are all 0 except for that of W L+1. One can thus think of this as a
“lag” in the training dynamics for 1 step.
Assumption H.21. For clarity of the proof, we will assume aL+1 + bL+1 ≤ 2aL+1 + c, i.e. W L+1
stays the same order for all t. The case of aL+1 + bL+1 > 2aL+1 + c, corresponding to a 1-step “lag”
as explained above, can be dealt with similarly. We will remark whenever this requires some subtlety.

is smaller than W L+1

, so that W L+1

0

0

t

t

t

For the construction of the program and the application of the Master Theorem, we will also assume
the following for the rest of this paper.
Assumption H.22. φ(cid:48) is pseudo-Lipschitz and not almost everywhere zero.

48

Initial Matrices, Vectors, Scalars We will assume the parametrization is initialization-stable. For
ease of presentation, we also assume the input dimension d = 1.

1. Initial matrices: W 2
2. Initial vectors: input layer matrix W 1

0 , . . . , W L

0 , sampled like (W l

0)αβ ∼ N (0, 1/n).

0 ∈ Rn×1 and normalized output layer matrix (cid:99)W L+1

0

def=

W L+1
0

naL+1+bL+1 ∈ R1×n, sampled like (W 1

0 )α, ((cid:99)W L+1

0

)α ∼ N (0, 1).

3. Initial scalars: We deﬁne the following scalars (where we explain the intuition in parenthesis).

The reader can skip this part on a ﬁrst read but come back when referred to.
(a) (n times the scale of coordinates of ∆W l

t ) For l ≥ 2, deﬁne
def= n−(aL+1+bL+1+c−1+2al)

θW l
(b) (scale of coordinates of ∆W 1

t and ∆h1

t ) Deﬁne

(c) (scale of coordinates of ∆W L+1

θ1 = θW 1
)

def= n−(aL+1+bL+1+c+2a1)

(d) (scale of ∆hl

t
θL+1 = θW L+1
t) For l ∈ [L], deﬁne
def= max
m≤l

t and ∆xl
θhl = θxl = θl

def= n−2aL+1−c

θW m = max(θW l , θl−1)

(46)

= n−(aL+1+bL+1+c−1+minl

m=1(2am+I(m=1)))

Note that θL = n−r with r deﬁned in Deﬁnition H.5.

(e) (scale of W L+1

t

)

θf

def= n−(aL+1+bL+1)

(f) (convenience scalars)

θxl−1/hl = θxl−1/θhl
θW l/hl = θW l /θhl
θW lxl−1/hl = θW l θxl−1/θhl
θL+1/f = θL+1/θf

L+1 = nθL+1 = n1−2aL+1−c
θ(cid:48)
Lf = nθLθf = n1−(r+aL+1+bL+1)
θ(cid:48)

(g) Depending on the the value of aL+1 + bL+1, we will also construct the values of f at

initialization as initial scalars. See Appendix H.4.1 for an explanation.

By our assumption that aL+1 + bL+1 ≤ 2aL+1 + c, the pseudostability inequalities of Theorem H.6
imply all of these θs either converge to 0 or stay constant at 1. This means that, assuming appropriate
regularity conditions on the nonlinearities and rank stability, we can apply the Master Theorem (if θ
blows up to ∞ then we can’t do that).

Notations We use := to more clearly denote assignment happening in the program, as opposed to
mathematical equality. To clearly demonstrate the application of Nonlin, we will also freely introduce
function symbols Ψ to put things into Nonlin form.

In the program, for each z ∈ {xl, hl}l, we will construct vectors
Preview of Names for Vectors
δzt(ξ) to mathematically represent θ−1
z (zt(ξ) − zt−1(ξ)) (intuition: change in z scaled to have Θ(1)
coordinates). Similarly, for w ∈ {W L+1, W 1}, we will construct δwt to mathematically represent
θ−1
w (wt − wt−1) (intuition: change in w scaled to have Θ(1) coordinates). Then, mathematically,
zt(ξ) = zt−1(ξ) + θzδzt(ξ), wt = wt−1 + θwδwt.
We will also construct dz to mathematically represent θ−1
have Θ(1) coordinates). For weight changes, we have the following identity

f ∇zf (intuition: gradient ∇zf scaled to

t−1 = −ηn−cχt−1n−2al θf dhl

t−1xl−1(cid:62)

t−1 = −ηχt−1θW l

W l

t − W l
and for l = 1,

1
n

t−1xl−1(cid:62)
hl
t−1 ,

∀l ∈ [2, L], (47)

W l

t − W l

t−1 = −ηn−cχt−1n−2al θf dhl

t−1ξ(cid:62)

t−1 = −ηχt−1θW l hl

t−1ξ(cid:62)

t−1.

(48)

49

H.4 Program Construction

Here we construct the Tensor Program encoding the SGD of an MLP. We separately describe the ﬁrst
forward and backward passes followed by the later forward and backward passes.

H.4.1 First Forward Pass

0 ξ ∈ Rn via Nonlin (as Ψ(W 1
For every ξ ∈ X , we compute h1
0(ξ) := W 1
tion by ξ), and we construct the following vectors via Nonlin and MatMul

0 ; ξ), where Ψ is multiplica-

0(ξ) := φ(hl
xl

0(ξ)) ∈ Rn,

hl+1
0

(ξ) := W l+1

0 xl

0(ξ) ∈ Rn,

for l = 1, . . . , L − 1,

(49)

Function Output The ﬁrst output is f0(ξ) = W L+1
slightly differently.

0

xL
0 (ξ), but we will deﬁne f0(ξ) in the program

Case when aL+1 + bL+1 > 1/2 Then f0(ξ) a.s.−−→ 0 for all ξ ∈ X . In the program, we will
construct f0(ξ) as an initial scalar mathematically deﬁned by W L+1

xL
0 (ξ).5960

0

If aL+1 + bL+1 = 1/2, then f0(ξ) converges to a nontrival
Case when aL+1 + bL+1 = 1/2
Gaussian via CLT [49], so we will condition on f0(ξ) for all ξ ∈ X . Given values g(ξ) ∈ R for all
ξ ∈ X , let E be the event that f0(ξ) = 1√
xL
0 (ξ) equals g(ξ) for all ξ ∈ X . The distribution
of (cid:99)W L+1
0

conditioned on E is given by

n (cid:99)W L+1

0

(cid:99)W L+1
0
is an iid copy of (cid:99)W L+1

√

d=E

nX +g + Π(cid:102)W L+1

0

0

0

0 (ξ) : ξ ∈ X }. Here X + denotes the pseudo-inverse of X.

, g ∈ RX is the vector of {g(ξ) : ξ ∈ X }, X ∈ RX ×n has
where (cid:102)W L+1
xL
0 (ξ) as rows, and Π is the orthogonal projection into the orthogonal complement of the space
spanned by {xL
By standard formulas for pseudo-inverse and orthogonal projection, we can write X + =
n X (cid:62)(XX (cid:62)/n)+, Π = I − 1
1
Let Σ def= XX (cid:62)/n and γ def= (X (cid:102)W L+1
1√
n X (cid:62)Σ+g.

n X (cid:62)(XX (cid:62)/n)+X.

/n). Then Π(cid:102)W L+1

− X (cid:62)Σ+γ, and

= (cid:102)W L+1
0

nX +g =

√

0

0

By the Master Theorem, γ a.s.−−→ 0 because (cid:102)W L+1
is independent from X, and Σ a.s.−−→ ˚Σ for some
PSD matrix ˚Σ. At this point in the program, all scalars we used (like ξ) are constant with n and
can be absorbed into nonlinearities. By the rank stability property of any program without scalars
[52], the rank of Σ is ﬁxed for large enough n, almost surely, so Σ+ a.s.−−→ ˚Σ+ by the continuity of
pseudo-inverse on ﬁxed rank matrices.
We will now replace (cid:99)W L+1

0

0

(cid:99)W L+1
E

(cid:19)

in the program with
(cid:18)
Σ+ g
√
n
(cid:17)

def= X (cid:62)

(cid:16)
Σ+ g√
n

+ (cid:102)W L+1
0

− X (cid:62) (cid:0)Σ+γ(cid:1)

n , Σ+γ a.s.−−→ 0, we have Z (cid:99)W L+1

and (Σ+γ) are ﬁnite dimensional and formally consid-
constructed using Nonlin, where
ered (collections of) scalars involved as coefﬁcients for linear combination of rows of X. Since
E = Z (cid:102)W L+1
Σ+ g√
. Intuitively, this means that, even after conditioning
on f0 = g, the conditional distribution of (cid:102)W L+1
We can then proceed exactly as in the case when aL+1 + bL+1 > 1/2, with (cid:99)W L+1
(cid:102)W L+1
0

is practically the same as the original distribution.
taking the role of

. The program then encodes the evolution of f conditioned on f0(ξ) = g(ξ), ∀ξ ∈ X .61

E

0

0

59It is completely OK to deﬁne an initial scalar using randomness from other parts of the program, as long as

this scalar converges almost surely to a deterministic limit

60We cannot deﬁne it using a Moment instruction because, intuitively, the mechanism of this convergence is

through CLT, not Law of Large Numbers.

61Formally, we can also have {g(ξ) : ξ ∈ X } as initial scalars, but since they are ﬁxed with n, they can be

absorbed into the Nonlin that deﬁnes (cid:99)W L+1

E

.

50

Assumption H.23. For the above reason, we will assume aL+1 + bL+1 > 1/2, and remark whenever
the case aL+1 + bL+1 = 1/2 involves subtleties.

H.4.2 First Backward Pass

Next, we write the backward pass

dxL
dhl
dxl−1
0

0 (ξ) := (cid:99)W L+1
0
0(ξ) (cid:12) φ(cid:48)(hl
0(ξ) := dxl
0 dhl
(ξ) := W l(cid:62)
0(ξ)

0(ξ))

where, recall, dz mathematically equals θ−1

f ∇zf .

For ξ = ξ0 and its label y0, we deﬁne the ﬁrst loss derivative as

where the convergence is because L(cid:48) is continuous by assumption.

χ0 := L(cid:48)(f0(ξ0), y0) a.s.−−→ ˚χ0(ξ) = L(cid:48)(0, y0)

We also deﬁne

δW L+1
1

:= −ηχ0xL

0 (ξ0)

to represent the (normalized) change in W L+1 due to the ﬁrst gradient step.

H.4.3 tth Forward Pass, t ≥ 1

Overview We iteratively deﬁne δzt(ξ) to mathematically represent θ−1
z ∈ {xl, hl}l. Then we eventually set

z (zt(ξ) − zt−1(ξ)), for

zt(ξ) := z0(ξ) + θzδz1(ξ) + · · · + θzδzt(ξ).

Likewise, we will deﬁne δW L+1
t
the program, we will not directly use W L+1

so that W L+1

= θf (cid:99)W L+1
but instead use

0

t

(cid:99)W L+1
t

:= (cid:99)W L+1
0

t
+ θL+1/f (δW L+1

1

+ θL+1(δW L+1

1

+ · · · + δW L+1

t

). In

+ · · · + δW L+1

t

)

(50)

where θL+1/f = θL+1/θf . Mathematically, (cid:99)W L+1
Recall we shorthand zt = zt(ξt) for all z ∈ {xl, hl, dxl, dhl}l ∪ {f, χ}.

f W L+1

= θ−1

.

t

t

The Construction of (Pre)Activations We start with h = h1: By Eq. (48), we have

δht(ξ) := −ηχt−1ξ(cid:62)

t−1ξdht−1 = Ψ(dht−1; ξ(cid:62)

t−1ξ, ηχt−1).

(Notationally, recall we freely introduce function symbols Ψ to clarify the way we apply Nonlin).
For higher layers, if h = hl, x = xl−1, and W = W l, then h = W x. By Eq. (47), we have,
mathematically,

θhδht(ξ) = θxWt−1δxt(ξ) + (Wt − Wt−1)xt(ξ)

(cid:32)

= θx

W0δxt(ξ) +

t−1
(cid:88)

(Ws − Ws−1)δxt(ξ)

(cid:33)

+ (Wt − Wt−1)xt(ξ)

(cid:32)

s=1

= θx

W0δxt(ξ) − ηθW

t−1
(cid:88)

s=1

χs−1

x(cid:62)
s−1δxt(ξ)
n

(cid:33)

dhs−1

− ηχt−1θW

x(cid:62)
t−1xt(ξ)
n

dht−1

Recall θx/h = θ−1
struct

h θx, θW/h = θ−1

h θW , θW x/h = θ−1

h θW θx. With cs denoting x(cid:62)

s δxt(ξ)
n

, we con-

δht(ξ) := θx/hW0δxt(ξ) − ηθW x/h

t−1
(cid:88)

s=1

χs−1cs−1dhs−1 − ηχt−1θW/hct−1dht−1

= Ψ(W0δxt(ξ), dh0, . . . , dht−1; η, θx/h, θW x/h, θW/h, {cs, χs}t−1
s=0)

51

If x = xl, h = hl, then x = φ(h), and (using θx = θh (Eq. (46))),

δxt(ξ) := θ−1

h (φ(ht−1(ξ) + θhδht(ξ)) − φ(ht−1(ξ)))

= Ψ(ht−1(ξ), δht(ξ); θh)

(51)

where Ψ is precisely the difference quotient for the function φ.62

The Function Outputs We do not construct ft(ξ) directly, but rather through scalars δft(ξ) =
ft(ξ) − ft−1(ξ), so that

ft(ξ) := f0(ξ) + δf1(ξ) + · · · + δft(ξ).

Mathematically, δft(ξ) = θL+1δW L+1
differently in the program:

t

t (ξ) + W L+1
xL

t−1 θLδxL

t (ξ), but we shall write it slightly

δft(ξ) := θ(cid:48)

δW L+1
t
n
Lf = nθLθf and (cid:99)W L+1

L+1

xL
t (ξ)

+ θ(cid:48)

Lf

(cid:99)W L+1
t−1 δxL
n

t (ξ)

where θ(cid:48)

L+1 = nθL+1, θ(cid:48)

t−1 is constructed in Eq. (50).

H.4.4 tth Backward Pass, t ≥ 1

In the last layer, we construct

For each l = L, . . . , 1 for dhl and l = L, . . . , 2 for dxl−1, we also calculate

dxL

t (ξ) := (cid:99)W L+1

t

.

dhl

t(ξ) := dxl

t(ξ) (cid:12) φ(cid:48)(hl

t(ξ))

dxl−1
t

(ξ) := W l(cid:62)

0 dhl

t(ξ) − ηθW l

t−1
(cid:88)

χscsxl−1

s

= Ψ(W l(cid:62)

0 dhl

t(ξ), xl−1

0

s=0
, . . . , xl−1

t−1; ηθW l , {χs, cs}t−1
s=0)

where cs = dhl(cid:62)

s dhl
n

t(ξ)

. For ξ = ξt and its label yt, we deﬁne63

χt := L(cid:48)(ft(ξt), yt).

Finally, we compute the (normalized) change in W L+1 after this SGD update.

δW L+1

t+1 := −ηχtxL

t (ξt).

H.5 The Inﬁnite-Width Limit

In this section, we describe the Z random variables (Deﬁnition 7.3) corresponding to the vectors
of the program constructed above. According to the Master Theorem, each such vector z will have
roughly iid coordinates distributed like Z z in the large n limit.
Let ˚θ• denote the limit of any θ• in Appendix H.3. If pseudostability holds, then ˚θ• is either 0 or 1,
as one can easily verify. We can construct the Z random variables for each vector in the program, as
follows.

1. For the ﬁrst forward and backward passes, we have,

Z h1
Z dxL

0(ξ) = ξZW 1
0 ,
0 (ξ) = Z (cid:99)W L+1

0

,

Z xl
Z dhl

0(ξ) = φ(Z hl
0(ξ) = Z dxl

0(ξ)),
0(ξ)φ(cid:48)(Z hl

0(ξ)),

0

Z hl+1
Z dxl−1

0

0

(ξ) = ZW l+1
(ξ) = ZW l(cid:62)

0 dhl

0(ξ)

xl
0(ξ),

62The pseudo-Lipschitzness of φ(cid:48) assumed in Assumption H.22 implies that Ψ here is pseudo-Lipschitz, so

that we can ultimately apply our Master Theorem.

63Here we use Moment with the function φ(; ft(ξt)) = L(cid:48)(ft(ξt), yt) with no input and one parameter
(we absorb yt into φ since it does not change with n). The continuity of L(cid:48) in its ﬁrst argument satisﬁes
Assumption F.4(1), so the Master Theorem can apply.

52

2. For z ∈ {xl, hl}l, we have

Z zt(ξ) = Z z0(ξ) + ˚θzZ δz1(ξ) + · · · + ˚θzZ δzt(ξ)

(52)

3. For l ∈ [L], x = xl, h = hl, we have Z δxt(ξ) = Ψ(Z ht−1(ξ), Z δht(ξ); ˚θh) where Ψ is as in

Eq. (51). If ˚θh = 0 (e.g. if r > 0), then

Otherwise, ˚θh = 1, and

4. For h = h1, we have

Z δxt(ξ) = φ(cid:48)(Z ht−1(ξ))Z δht(ξ).

Z δxt(ξ) = φ(Z ht(ξ)) − φ(Z ht−1(ξ)).

Z δht(ξ) = −η˚χt−1ξ(cid:62)

t−1ξZ dht−1.

5. For l ≥ 2, h = hl, x = xl−1, W = W l, we have

(53)

(54)

Z δht(ξ) = ˚θx/hZW0δxt(ξ) − η˚θW x/h

t−2
(cid:88)

s=0

˚χsZ dhs E Z xsZ xt(ξ)

− η˚χt−1

˚θW/hZ dht−1 E Z xt−1Z xt(ξ)

(55)

where at least one of ˚θx/h and ˚θW/h equals 1. As usual, here we have the ZHat-ZDot
decomposition of ZW0δxt(ξ).

ZW0δxt(ξ) = ˆZW0δxt(ξ) + ˙ZW0δxt(ξ)

= ˆZW0δxt(ξ) +

t−1
(cid:88)

s=0

Z dhs E ∂Z δxt(ξ)

∂ ˆZW (cid:62)

0 dhs

.

6. For last layer weight

and

Z δW L+1

t = −η˚χt−1Z xL

t−1

Z (cid:99)W L+1

t = Z (cid:99)W L+1

0 + ˚θL+1/f (Z δW L+1

1 + · · · + Z δW L+1

t

)

7. The output deltas have limits

δ ˚ft(ξ) = ˚θ(cid:48)

L+1

E Z δW L+1

t Z xL

t (ξ) + ˚θ(cid:48)

Lf

E Z (cid:99)W L+1

t−1 Z δxL

t (ξ)

and

8. For gradients:

˚ft(ξ) = δ ˚f1(ξ) + · · · + δ ˚ft(ξ).

(56)

(57)

(58)

Z dxL
Z dhl

t (ξ) = Z (cid:99)W L+1
t(ξ) = Z dxl

t

t(ξ)φ(cid:48)(Z hl

t(ξ))

Z dxl−1

t

9. Loss derivative

(ξ) = ZW l(cid:62)

0 dhl

t(ξ) − η˚θW l

t−1
(cid:88)

s=0

˚χsZ xl−1

s E Z dhl

s Z dhl

t(ξ)

˚χt = L(cid:48)(˚ft, y0).

The following fact follows from the results of [51] (or can be veriﬁed by straightforward calculation)
and will be useful for us.
˙Z dxl

0(ξ) = 0 and Z dxl

0(ξ) for any ξ ∈ X .

Proposition H.24.

0(ξ) = ˆZ dxl

53

If the parametrization is pseudostable, then all the θ• converge to 0 or 1 so Setup 7.2 is satisﬁed.
Therefore, the Master Theorem applies and says that, for any collection of vectors v1, . . . , vk such
that Z vi

is deﬁned above, we have

1
n

n
(cid:88)

α=1

ψ(v1

α, . . . , vk

α) a.s.−−→ E ψ(Z v1

, . . . , Z vk

)

for any pseudo-Lipschitz ψ. In addition,64

δft(ξ) a.s.−−→ δ ˚ft(ξ),

ft(ξ) a.s.−−→ ˚ft(ξ), χt

a.s.−−→ ˚χt,

∀ξ ∈ X , t ≥ 1.

We now describe some immediate consequences of this.

H.5.1 Some Immediate Results

Proposition H.25. A pseudostable parametrization is trivial if

2aL+1 + c > 1

and aL+1 + bL+1 + r > 1.

Proof. In this case, θ(cid:48)

L+1, θ(cid:48)

Lf , θ(cid:48)

L,L+1 → 0, and δ ˚ft(ξ) = 0 for all t and ξ ∈ X by Eq. (58).

Proposition H.26. A pseudostable parametrization is stable.

Proof. For a pseudostable parametrization, all of θs converge to 1 or 0, and all of the Z δhl
have well deﬁned (ﬁnite) limits, which implies ∆hl
O∗(1).

t(ξ) = O∗(1), ∀l ∈ [L],

t(ξ), ∆xl

t(ξ), Z δxl
t(ξ)
and ft(ξ) =

Proposition H.27. Consider a pseudostable parametrization. If r > 0, then it ﬁxes all (pre)features
and all (pre)feature kernels. In addition, ∆W L+1
t ∆xL

t (ξ) a.s.−−→ 0.

Proof. If r > 0, then θl → 0 for all l ∈ [L], so that for all z ∈ {xl, hl}l, ∆zt(ξ) = zt(ξ) − z0(ξ) =
θzδz1(ξ) + · · · + θzδzt(ξ) has (cid:107)∆zt(ξ)(cid:107)2/n a.s.−−→ 0 by Eq. (52) and the Master Theorem, i.e. all
features are ﬁxed. Similarly, for any pair ξ, ¯ξ ∈ X , zt(ξ)(cid:62)zt( ¯ξ)/n − z0(ξ)(cid:62)z0( ¯ξ)/n a.s.−−→ 0, so all
t (ξ) a.s.−−→ 0
feature kernels are ﬁxed. Finally, r > 0 implies θ(cid:48)
by the Master Theorem.

L,L+1 → 0, which means ∆W L+1

t ∆xL

Proposition H.28. An initialization-stable parametrization with r < 0 is not stable.

Proof. If r < 0, then there is some (cid:96) ∈ [L] such that θL ≥ · · · ≥ θ(cid:96) > 1 ≥ θ(cid:96)−1 ≥ · · · ≥ θ1.
For h = h(cid:96), x = x(cid:96)−1, W = W (cid:96), we would have θx/h = θ(cid:96)−1/θ(cid:96) → 0, θW/h = 1, and θW x/h =
θW/hθ(cid:96)−1 → 0. The Tensor Program up to the deﬁnition of δh1(ξ0) satisﬁes the conditions of the
Master Theorem. Therefore, (cid:107)δh1(ξ0)(cid:107)2/2 a.s.−−→ E(Z δh1(ξ0))2 = E(η˚χt−1Z dh0 E Z x0Z x1(ξ0))2. If
ξ0 (cid:54)= 0, then E(Z dh0 )2 > 0. If η is in addition sufﬁciently small but nonzero, then E Z x0Z x1(ξ0) ≈
E(Z x0 )2 > 0. Therefore, under these conditions, and with a training sequence that has ˚χ0 (cid:54)= 0,
we have E(η˚χt−1Z dh0 E Z x0Z x1(ξ0))2 > 0, so that δh1(ξ0) = Θξ0(1). However, ∆h1(ξ0) =
θhδh1(ξ0) and θh = θ(cid:96) → ∞. Hence ∆h1(ξ0) (cid:54)= Oξ0(1), as desired.

H.6 r > 0 Implies Kernel Regime

In this section, we analyze the case when r > 0. Our main result is deriving the corresponding
inﬁnite-width kernel gradient descent dynamics (Theorem H.32). Nothing here depends on φ being
tanh or σ-gelu.

64Again, if aL+1 + bL+1 = 1/2, remember we are conditioning on f0(ξ), ξ ∈ X .

54

Preliminary Derivations

If r > 0, then ˚θl = ˚θW l = 0 for all l ∈ [L], so that we have
0(ξ), Z dxl

0(ξ), Z (cid:99)W L+1

t(ξ) = Z dxl

0(ξ), Z dhl

Z hl

0(ξ), Z xl

t(ξ) = Z xl

t(ξ) = Z hl

t(ξ) = Z dhl
for all t and ξ ∈ X . Let (cid:96) ∈ [L] be the unique (cid:96) such that 1 = θL/θL = · · · = θ(cid:96)/θL > θ(cid:96)−1/θL ≥
· · · ≥ θ1/θL. Then for l ≥ (cid:96) + 1 and shorthand h = hl, x = xl−1, W = W l, we have ˚θx/h = 1,
˚θW x/h = 0 and, by Eq. (55),

t = Z (cid:99)W L+1

0

Z δht(ξ) = ZW0δxt(ξ) − η˚χt−1
= ZW0δxt(ξ) − η˚χt−1

˚θW/hZ dht−1 E Z xt−1 Z xt(ξ),
˚θW/hZ dh0(ξt−1) E Z x0(ξt−1)Z x0(ξ)

(59)

where ˚θW/h can be either 0 or 1. For l = (cid:96), because θh = θl = maxm≤l θW m = max(θW l , θl−1) =
max(θW l , θx) so ˚θx/h = ˚θW x/h = 0 and ˚θW/h = 1, we also have
Z δht(ξ) = −η˚χt−1Z dht−1 E Z xt−1Z xt(ξ)

= −η˚χt−1Z dh0(ξt−1) E Z x0(ξt−1)Z x0(ξ).

(60)

Finally, for all l ∈ [L], we have, by Eq. (53),

Z δxt(ξ) = φ(cid:48)(Z ht−1(ξ))Z δht(ξ) = φ(cid:48)(Z h0(ξ))Z δht(ξ).

Deﬁnition H.29. For 1 ≤ m ≤ l and ξ, ζ ∈ X , deﬁne
Σml(ξ, ζ) def= E Z xm
We also deﬁne

0 (ζ) × E φ(cid:48)(Z hm+1

0 (ξ)Z xm

0

(ξ))φ(cid:48)(Z hm+1

0

(ζ)) × · · · × E φ(cid:48)(Z hl

0(ξ))φ(cid:48)(Z hl

0(ζ)).

Σ0l(ξ, ζ) def= ξ(cid:62)ζ × E φ(cid:48)(Z hm+1

0

(ξ))φ(cid:48)(Z hm+1

0

(ζ)) × · · · × E φ(cid:48)(Z hl

0(ξ))φ(cid:48)(Z hl

0(ζ))

For example,

and so on.

Σll(ξ, ζ) = E Z xl
Σl,l+1(ξ, ζ) = E Z xl

0(ξ)Z xl
0(ξ)Z xl

0(ζ)

0(ζ) E φ(cid:48)(Z hl+1

0

(ξ))φ(cid:48)(Z hl+1

0

(ζ)),

Notation For brevity, below we will shorthand ϑm = θW m/hm. We write Z x ≡ Z y mod ˆZW • if
Z x − Z y is a linear combination of ˆZW u for various vectors u.
Lemma H.30. For any input ξ, any l ≥ (cid:96), at any time t,

Z δhl

t(ξ) ≡ −η˚χt−1Z dhl

0(ξt−1)

l−1
(cid:88)

m=(cid:96)−1

˚ϑm+1Σm,l−1(ξt−1, ξ) mod ˆZW l

0•.

(61)

Proof. We proceed by induction.

Base Case l = (cid:96): this is given by Eq. (60).

t

, ¯x = xl−1

t−1, x0 = xl−1

, ¯ξ = ξt−1, W = W l

t−1, h0 =
0, i.e. we use ¯• to denote time t − 1 in contrast to • for time t, and we

Induction: Assume Eq. (61) holds for l − 1, and we shall prove it for l.
To alleviate notation, we write x = xl−1
hl−1
0
0, hl
suppress layer index. In contrast, we will write hl
First, note that Z δx(ξ) = φ(cid:48)(Z ¯h(ξ))Z δh(ξ) by Eq. (53). Because Z ¯h(ξ) = Z h0(ξ), and, by induction
hypothesis, Z δh(ξ) is a scalar multiple of Z dh0( ¯ξ) = Z dx0( ¯ξ)φ(cid:48)(Z h0( ¯ξ)), Z δx(ξ) is symbolically
solely a function of Z h0(ξ), Z h0( ¯ξ), Z dx0( ¯ξ),all of which are equal to their ˆZ versions (with the last
0( ¯ξ) is constructed from matrix
due to Proposition H.24). Among these, only Z dx0( ¯ξ) = ZW (cid:62)dhl
multiplication with W (cid:62)

t, and ξ for their usual meanings.

, ¯h = hl−1

, h = hl−1

0

t

0 . Thus,

˙ZW0δx(ξ) = Z dhl

0( ¯ξ) E ∂Z δx(ξ)
∂Z dx0( ¯ξ)

= Z dhl

0( ¯ξ) E φ(cid:48)(Z h0(ξ))

∂Z δh(ξ)
∂Z dx0( ¯ξ)

.

(62)

55

By induction hypothesis,

∂Z δh(ξ)
∂Z dx0( ¯ξ)

= −η˚χt−1φ(cid:48)(Z h0( ¯ξ))

l−2
(cid:88)

m=(cid:96)−1

˚ϑm+1Σm,l−2( ¯ξ, ξ).

Therefore,

E φ(cid:48)(Z h0(ξ))

∂Z δh(ξ)
∂Z dx0( ¯ξ)

= −η˚χt−1 E

(cid:104)

φ(cid:48)(Z h0(ξ))φ(cid:48)(Z h0( ¯ξ))

(cid:105) l−2
(cid:88)

m=(cid:96)−1

˚ϑm+1Σm,l−2( ¯ξ, ξ).

By deﬁnition of Σml, this equals

E φ(cid:48)(Z h0(ξ))

∂Z δh(ξ)
∂Z dx0( ¯ξ)

= −η˚χt−1

l−2
(cid:88)

m=(cid:96)−1

˚ϑm+1Σm,l−1( ¯ξ, ξ).

Plugging this back into Eq. (62), we get

˙ZW0δx(ξ) = −η˚χt−1Z dhl

0( ¯ξ)

l−2
(cid:88)

m=(cid:96)−1

˚ϑm+1Σm,l−1( ¯ξ, ξ).

(63)

Finally, by Eq. (59),

Z δhl

t(ξ) = ˙ZW0δx(ξ) − η˚χt−1
= ˙ZW0δx(ξ) − η˚χt−1

˚ϑlZ dhl
˚ϑlZ dhl

0( ¯ξ) E Z x0( ¯ξ)Z x0(ξ)
0( ¯ξ)Σl−1,l−1( ¯ξ, ξ).

Together with Eq. (63), this completes the induction.

Lemma H.31. Assume pseudostability, r > 0, and aL+1 + bL+1 ≤ 2aL+1 + c. If ˚θL+1/f = 1 then
˚θ(cid:48)
Lf = 0.

Proof. aL+1 + bL+1 ≤ 2aL+1 + c iff θL+1 ≤ θf . So ˚θL+1/f = 1 implies θL+1 = θf . By
pseudostability, nθL+1 ≤ 1. Since θL = n−r, we have θ(cid:48)
Lf = n · n−r · θf = n−r · nθL+1 < 0 since
r > 0. Therefore ˚θ(cid:48)

Lf = 0.

Theorem H.32. Consider a pseudostable parametrization. At any time t, for any input ξ ∈ X , we
have

δ ˚ft(ξ) = −η˚χt−1Σ(ξt−1, ξ),

where the kernel Σ is deﬁned for any ξ, ζ ∈ X by

Σ(ζ, ξ) def= ˚θ(cid:48)

L+1ΣLL(ζ, ξ) + ˚θ(cid:48)

Lf

L−1
(cid:88)

m=(cid:96)−1

˚ϑm+1ΣmL(ζ, ξ).

Observe that in the NTK parametrization, (cid:96) = 1, and ˚θ(cid:48)
Σ = (cid:80)L

m=0 ΣmL is precisely the NTK (for MLP without biases).

L+1 = ˚θ(cid:48)

Lf = ˚ϑm+1 = 1 for all m, so

Proof. By Eqs. (57) and (58),

δ ˚ft(ξ) = ˚θ(cid:48)
Z (cid:99)W L+1

t = Z (cid:99)W L+1

E Z δW L+1

t Z xL
t (ξ) + ˚θ(cid:48)
0 + ˚θL+1/f (Z δW L+1

E Z (cid:99)W L+1
t−1 Z δxL
1 + · · · + Z δW L+1

L+1

Lf

t

).

t (ξ)

Now by Lemma H.31, either ˚θL+1/f = 0 or ˚θ(cid:48)
contributes 0 to δ ˚ft(ξ). So we can replace Z (cid:99)W L+1

Lf = 0. In both cases, (Z δW L+1
t−1 with Z (cid:99)W L+1
above, and write

0

1 + · · · + Z δW L+1

t

)

δ ˚ft(ξ) = ˚θ(cid:48)

L+1

E Z δW L+1

t Z xL

t (ξ) + ˚θ(cid:48)

Lf

E Z (cid:99)W L+1

0 Z δxL

t (ξ).

56

If Eq. (61) is true for l = L, then

E Z (cid:99)W L+1

0 Z δxL

t (ξ) = −η˚χt−1 E Z (cid:99)W L+1

0 Z dhL

0 (ξt−1)φ(cid:48)(Z hL

0 (ξ))

L−1
(cid:88)

m=(cid:96)−1

˚ϑm+1Σm,L−1(ξt−1, ξ)

where the contributions from ˆZW L
Z dhL

0 • in Z δxL
0 (ξ)), we continue

0 (ξ) = Z (cid:99)W L+1

φ(cid:48)(Z hL

0

t (ξ) vanish as they are independent from Z (cid:99)W L+1

0

. Since

E Z (cid:99)W L+1

0 Z δxL

t (ξ) = −η˚χt−1 E

(cid:16)

Z (cid:99)W L+1

0

(cid:17)2

φ(cid:48)(Z hL

0 (ξt−1))φ(cid:48)(Z hL

0 (ξ))

L−1
(cid:88)

m=(cid:96)−1

˚ϑm+1Σm,L−1(ξt−1, ξ)

= −η˚χt−1

L−1
(cid:88)

m=(cid:96)−1

˚ϑm+1ΣmL(ξt−1, ξ).

Similarly, by Eq. (56),

E Z δW L+1

t Z xL

t (ξ) = −η˚χt−1 E Z xL
= −η˚χt−1 E Z xL

t−1(ξt−1)Z xL
0 (ξt−1)Z xL

t (ξ)

0 (ξ) = −η˚χt−1ΣLL(ξt−1, ξ).

Altogether, these prove the desired claim.

Corollary H.33. A pseudostable parametrization with r > 0 is nontrivial iff aL+1 + bL+1 + r = 1
or 2aL+1 + c = 1.

Proof. The kernel Σ in Theorem H.32 is nonzero iff ˚θ(cid:48)
aL+1 + bL+1 + r = 1 or 2aL+1 + c = 1.

L+1 or ˚θ(cid:48)

Lf is 1, which is equivalent to saying

Corollary H.34. An initialization-stable parametrization with r > 0 but aL+1 + bL+1 + r < 1 or
2aL+1 + c < 1 is not stable.

Proof. If aL+1 + bL+1 + r < 1 or 2aL+1 + c < 1, then θ(cid:48)
Lf → ∞. Clearly, from
the deﬁnition, ΣmL(ξ, ξ) > 0 for any ξ (cid:54)= 0 and m ∈ [0, L]. All of our reasoning leading up to
Theorem H.32 applied at t = 1 holds, so Theorem H.32 (along with the Master Theorem) implies
|δft(ξ)| a.s.−−→ ∞.
Corollary H.35. If aL+1 + bL+1 + r > 1 and 2aL+1 + c = 1, then for all ξ ∈ X , ˚ft(ξ) a.s.−−→ 0 and
δ ˚ft(ξ) = −η˚χt−1ΣLL(ξt−1, ξ), i.e. we have the Neural Network-Gaussian Process (NNGP) limit.

L+1 → ∞ or θ(cid:48)

Conventionally, the NNGP limit is associated with only training the last layer and nothing else. This
result says that the same limit can be achieved if we train the body of the network slightly, so that
∆xL
enough (embodied in the inequality aL+1 + bL+1 + r > 1) to
cause changes in ft.

t does not interact with W L+1

0

Proof. The premise implies ˚θ(cid:48)

L+1 = 1 and ˚θ(cid:48)

Lf = 0, and the rest follows from Theorem H.32.

Remark H.36. We have assumed for simplicity of the proof that aL+1 + bL+1 ≤ 2aL+1 + c. If this
is not the case, then we can easily see Corollary H.35 applies anyway.

H.7 r = 0 Implies Feature Learning

In this section, we assume r = 0 and show any such pseudostable parametrization 1) admits
(pre)feature learning and (pre)feature kernel evolution, and 2) is not in kernel regime (Theorem H.51).
The overarching logic goes like this.

1. The Master Theorem shows that the speciﬁc entry 1
1 (ξ0))2.

If the learning rate η = 0, then xL

n (cid:107)xL

nel converges to E(Z xL
E(Z xL

1 (ξ0))2 = E(Z xL

0 )2. We hope to say that as η increases, E(Z xL

1 (ξ0)(cid:107)2 of the feature ker-
1 (ξ0) = xL
0 and
1 (ξ0))2 moves away

57

from E(Z xL
E(Z xL
pute ∂2
η
the next best thing is ∂2
results for prefeatures and for other layers can be derived similarly.

0 )2, which would imply feature kernel evolution in layer L. To do so, we com-
1 (ξ0))2 evaluated at η = 0 and show it is nonzero (it turns out ∂η vanishes, so
η). This then also implies feature learning in layer L. Analogous

2. If the parametrization is in the kernel regime with kernel K, the ﬁrst step of SGD in the large
width limit would look like ˚f1(ξ) − ˚f0(ξ) = −η˚χ0K(ξ, ξ0); in particular, ˚f1(ξ) − ˚f0(ξ)
is linear in η. To show that a pseudostable parametrization with r = 0 is not in the kernel
regime, we will show ∂3
η vanishes,
so the next best thing is ∂3

η(˚f1(ξ) − ˚f0(ξ)) = ∂3
η).

˚f1(ξ) is nonzero. (It turns out ∂2

η

To calculate these η derivatives, we will derive recurrence relations involving quantities deﬁned below
(see Lemma H.38 and Theorem H.41).

Setup and Notation First, write

def= ˆZW lxl−1
def= Z hl
0 is a centered Gaussian independent from ˆZ l

t(ξ0), ˆZ l
t

Z l
t

t

Note that ˜Z l

(ξ0), ˜Z l
0

def= Z dxl
0.

0 = Z l

0. Then we deﬁne

γl(η) def= E φ(Z l

0)φ(Z l

1),

11(η) def= E φ(cid:48)(Z l
γl

0)φ(cid:48)(Z l

1),

02(η) def= E φ(Z l
γl

0)φ(cid:48)(cid:48)(Z l
1)

20(η) def= E φ(cid:48)(cid:48)(Z l
γl

0)φ(Z l

1),

λl(η) def= E φ(Z l

1)2

where the dependence on η is from Z l
we have γl(0), λl(0), γl
Observe that ( ˆZ l

1, ˆZ l

11(0) > 0. Note at η = 0, we have Z l

1. Naturally, since φ and φ(cid:48) are not almost everywhere zero,
0)2.

0, so γl(0) = λl(0) = E φ(Z l

1 = Z l

0) is jointly Gaussian with mean zero and covariance

Γl(η) def=

(cid:18)λl(η) γl(η)
γl(η) λl(0)

(cid:19)

.

(64)

WLOG, for simplicity of notation, we assume we choose a training routine such that ˚χ0 = 1. We
assume ξ0 (cid:54)= 0.
Since r = 0, WLOG we can suppose for some (cid:96) ∈ [L], we have θL = · · · = θ(cid:96) = 1 > θ(cid:96)−1 ≥ · · · ≥
θ1.
Lemma H.37. With the setup above, we have

0 = Z (cid:96)−1
Z (cid:96)−1

1

, . . . , Z 1

0 = Z 1
1 ,

and

1 = ˆZ l
Z l

1 + ηβl ˜Z l

0φ(cid:48)(Z l

0),

∀l ∈ [(cid:96), L],

where βl is deﬁned recursively by

βl = βl(η) def= −γl−1(η) + βl−1(η)γl−1

11 (η)

β(cid:96)−1(η) def= 0.

Additionally, βl(0) < 0 for all l ≥ (cid:96).

Proof. Straightforward calculation using Moment and Zdot. Here, −γl−1(η) comes from
11 (η) comes from ˙Z hl
∆W l
11 (0) > 0 for all l, the
recurrence on βl implies that βl(0) < 0 for all l ≥ (cid:96).

1(ξ0) and βl−1(η)γl−1

1(ξ0). Since γl(0), γl−1

1x1

H.7.1 Deriving Recurrence Relations on ∂ηλl, ∂ηγl, ∂2

ηλl, ∂2

ηγl

Below, we derive the recurrence relations required for our main result. They depend on the following
constants.

κl
1

def= E (cid:2)(φ2)(cid:48)(cid:48)(Z l

0)(cid:3) ,

κl
2

def= E (cid:2)(φ2)(cid:48)(cid:48)(Z l

0)φ(cid:48)(Z l

0)2(cid:3) ,

κl
3

58

def= E (cid:2)φ(Z l

0)φ(cid:48)(cid:48)(Z l

0)φ(cid:48)(Z l

0)2(cid:3) .

Lemma H.38. With the setup above, we have, for all l ∈ [L],

∂ηλl(0) =

∂ηγl(0) =

1
2
1
2

1∂ηλl−1(0)
κl

(65)

02∂ηλl−1(0) + γl
γl

11∂ηγl−1(0).

Proof. We ﬁrst derive the recurrence on ∂ηλl. By Lemma H.39 below, we have

∂ηλl = 2 E φ(Z l

1)∂ηφ(Z l

1) +

1
2

E(φ2)(cid:48)(cid:48)(Z l

1)∂ηλl−1.

Note the second item in the sum evaluated at η = 0 is exactly the RHS of Eq. (65). For the ﬁrst item,
since

∂ηφ(Z l

1) = φ(cid:48)(Z l

1)(βl ˜Z l

0φ(cid:48)(Z l

0) + η ˜Z l

0φ(cid:48)(Z l

0)∂ηβl),

(66)

we compute

1) = E φ(Z l

1)φ(cid:48)(Z l

1)(βl ˜Z l

0φ(cid:48)(Z l

0) + η ˜Z l

0φ(cid:48)(Z l

0)∂ηβl).

E φ(Z l

1)∂ηφ(Z l
0, so that ˜Z l

0 is independent from everything else in the expectation. This implies

1 = Z l

At η = 0, Z l
the expectation is 0 and yields the recurrence for ∂ηλl(0).
(cid:18)γl
02
γl
11

For ∂ηγl, let Σ = Σ(η) def=

γl
11
γl
20

(cid:19)

. With Γl−1 as in Eq. (64), we have

∂ηγl = E φ(Z l

0)∂ηφ(Z l

1) +

1
2

(cid:104)Σ, ∂ηΓl−1(cid:105)

By same reasoning as in Eq. (65), the ﬁrst term of this sum is zero when evaluated at η = 0. Since
∂ηΓl−1(η) def=

(cid:18)∂ηλl−1(η) ∂ηγl−1(η)

, we have

(cid:19)

∂ηγl(η)

0

∂ηγl =

1
2

(cid:104)Σ, ∂ηΓl−1(cid:105) =

1
2

02∂ηλl−1 + γl
γl

11∂ηγl−1.

Lemma H.39. Consider a twice continuously differentiable f and Gaussian vector Z ∼ N (0, Σ)
such that f and Σ both depend on a parameter η. Then

∂η E f (Z) = E ∂ηf (Z) +

1
2

(cid:104)E ∇2f (z), ∂ηΣ(cid:105),

where ∇2 denotes Hessian wrt z, and (cid:104), (cid:105) denotes trace inner product of matrices.

Proof. Let p(z) denote the PDF of Z. We have

∂η E f (Z) = ∂η

(cid:90)

f (z)p(z) dz =

(cid:90)

∂ηf (z)p(z) dz +

(cid:90)

f (z)∂ηp(z) dz

The ﬁrst integral is E ∂ηf (Z). The second integral can be rewritten using integration-by-parts as
(cid:104)E ∇2f (z), ∂ηΣ(cid:105). (e.g. see Lemma F.18 of [56])

We then easily have
Theorem H.40. For all l ∈ [L],

∂ηγl(0) = ∂ηλl(0) = 0.

Proof. For l < (cid:96), we obviously have ∂ηγl(η) = ∂ηλl(0) = 0 for all η. Then this follows from
Lemma H.38 and a simple induction.

Unfortunately, this means that the ﬁrst η derivative doesn’t give us what we need. So we try the
second derivative, which will turn out to work.

59

Theorem H.41. For all l < (cid:96),∂2

ηλl(0) = ∂2

ηγl(0) = 0, and for all l ≥ (cid:96),

ηλl(0) = Cκl
∂2

2 +

ηγl(0) = Cκl
∂2

3 +

1
2
1
2

where C = 2(βl(0))2 E( ˜Z l

0)2 > 0.

1∂2
κl

ηλl−1(0)

02(0)∂2
γl

ηλl−1(0) + γl

11(0)∂2

ηγl−1(0),

Proof. We start with the ∂2
ηλl is a sum of 3 terms, representing 1) 2
derivatives in the integrand, 2) 2 derivatives in the Gaussian variance, and 3) 1 derivative each. When
evaluated at η = 0, only the ﬁrst two terms survive because ∂ηλl−1(0) = 0 by Theorem H.40:

ηλl(0) recurrence. For l ≥ (cid:96), ∂2

ηλl(0) = E ∂2
∂2

ηφ2(Z l

1)|η=0 +

1
2

E(φ2)(cid:48)(cid:48)(Z l

0)∂2

ηλl−1(0).

Now

E ∂2

ηφ2(Z l

1) = 2∂η(E φ(Z l
= 2 E(φ2)(cid:48)(cid:48)(Z l

1)φ(cid:48)(Z l
1)(βl ˜Z l

1)(βl ˜Z l
0φ(cid:48)(Z l

0φ(cid:48)(Z l
0) + η ˜Z l

0) + η ˜Z l
0φ(cid:48)(Z l

0φ(cid:48)(Z l
0)∂ηβl))
0)∂ηβl)2 + · · ·

where other terms appear in this sum but they vanish at η = 0 because ˜Z l
expectation. Thus,

0 appears unpaired in the

Plugging this back in, we get the recurrence on ∂2

E ∂2

ηφ2(Z l

1)|η=0 = 2(βl(0))2 E( ˜Z l
ηλl(0).

0)2 E(φ2)(cid:48)(cid:48)(Z l

0)φ(cid:48)(Z l

0)2.

The ∂2

ηγl(0) recurrence is derived similarly.

The following result will be useful for showing ∂3
η
Theorem H.42. Deﬁne

˚f1(ξ0) (cid:54)= 0.

˙κl
3

def= E (cid:2)φ(cid:48)(cid:48)(cid:48)(Z l

0)φ(cid:48)(Z l

0)3(cid:3) ,

γl
13

def= E φ(cid:48)(Z l

0)φ(cid:48)(cid:48)(cid:48)(Z l

0),

γl
22

def= E φ(cid:48)(cid:48)(Z l

0)2.

Then for all l ≥ (cid:96),

ηγl
∂2

11(0) = C ˙κl

3 +

1
2

13∂2
γl

ηλl−1(0) + γl

22∂2

ηγl−1(0),

where C = 2(βl(0))2 E( ˜Z l

0)2 > 0.

Proof. Similar to the proof of Theorem H.41.

The following result will be useful for showing prefeature kernel evolution.
Theorem H.43. For all l ≥ (cid:96),
∂2
η
0)2 > 0.

where C = 2(βl(0))2 E( ˜Z l

1)2|η=0 = 2C + γl

ηλl−1(0),

11(0)∂2

E(Z l

Proof. Similar to the proof of Theorem H.41.

H.7.2 Applications to σ-Gelu

The following proposition regarding σ-gelu is easy to verify.
Proposition H.44. Let φ be σ-gelu. For any centered Gaussian Z ∈ R with nonzero variance,

E(φ2)(cid:48)(cid:48)(Z), E(φ2)(cid:48)(cid:48)(Z)φ(cid:48)(Z)2, E φ(Z)φ(cid:48)(cid:48)(Z)φ(cid:48)(Z)2, E φ(Z)φ(cid:48)(cid:48)(Z), E φ(cid:48)(cid:48)(Z)2 > 0,

and they converge to 0 as σ → 0. Also,

E φ(cid:48)(cid:48)(cid:48)(Z)φ(cid:48)(Z)3, E φ(cid:48)(Z)φ(cid:48)(cid:48)(cid:48)(Z) < 0,

and they converge to −∞ as σ → 0.

60

13 < 0 and diverges to −∞ with small σ.

This particularly implies that κl
˙κl
3, γl
Theorem H.45. Consider a pseudostable parametrization with r = 0. If φ is σ-gelu, then for all
l ≥ (cid:96),

22 > 0 and converges to 0 with small σ, but

02(0), γl

1, κl

2, κl

3, γl

ηγl(0), ∂2
∂2

ηλl(0) > 0

and they converge to 0 as σ → 0.

Proof. We always have (βl(0))2, E( ˜Z l
Theorem H.41, ∂2
ηγl(0) > 0 for all l ≥ (cid:96) as well. As σ → 0, κl
∂2

ηλl(0) > 0 for all l ≥ (cid:96). By Proposition H.44, κl

0)2 > 0. By Proposition H.44, κl

3, γl
02(0) → 0, so ∂2

1, κl

2, κl

3, γl

1, κl

2 > 0 as well. Thus, by
02(0) > 0, so by Theorem H.41,

ηλl(0), ∂2

ηγL(0) → 0.

Theorem H.46. Consider a pseudostable parametrization with r = 0. Suppose aL+1 + bL+1 + r = 1
or 2aL+1 + c = 1. If φ is σ-gelu for sufﬁciently small σ, then

∂3
η

˚f1(ξ0) (cid:54)= 0.

L+1

E Z δW L+1

Proof. We have ˚f1(ξ0) = ˚θ(cid:48)
Lf and ˚θ(cid:48)
˚θ(cid:48)

1 Z xL
L+1 is 1 because aL+1 + bL+1 + r = 1 or 2aL+1 + c = 1. We have
E Z δW L+1
E Z (cid:99)W L+1

1 (ξ0) = −η E Z xL
1 (ξ0) = E Z (cid:99)W L+1

1 Z xL
0 Z xL

0 Z xL
φ(Z hL

0 − ηZ (cid:99)W L+1

1 (ξ0) + ˚θ(cid:48)

E Z (cid:99)W L+1

0 Z δxL

1 (ξ0)

Lf

0

0

= −η E φ(cid:48)(Z hL

1 (ξ0))φ(cid:48)(Z hL

φ(cid:48)(Z hL
0 ) E Z xL−1

0 Z xL−1
0 ) E Z xL−1
(ξ0)

1

0 Z xL−1

1

(ξ0))

1 (ξ0), where at least one of

where we used Stein’s Lemma for the last equality. Thus

∂3
η

˚f1(ξ0) = −

L+1∂2

ηγL(0) + ˚θ(cid:48)

Lf ∂2

η(γL

(cid:16)˚θ(cid:48)

(cid:17)
11γL−1)(0)

.

Below we will show that for small σ, ∂2
negative, so ∂3
η

˚f1(ξ0) cannot be 0 no matter the values of ˚θ(cid:48)

ηγL(0) is small and positive and ∂2
Lf .

L+1 and ˚θ(cid:48)

η(γL

11γL−1)(0) is large and

ηγL
11(0) = C ˙κl

Claim: For sufﬁciently small σ, ∂2

11(0) < 0. It converges to −∞ as σ → 0.

ηγl

Proof: By Theorem H.42, ∂2
2 γl
by Theorem H.45. Also, by Proposition H.44, ˙κl
−∞, γl
converges to a positive constant as σ → 0 as well. Therefore, for small enough σ, ∂2
as σ → 0, ∂2

ηλl−1(0) ≥ 0
3, γl
13 →
ηλl(0) → 0 by Theorem H.45). One can see that C
11(0) < 0, and

ηγl−1(0). Note ∂2
22∂2
22 > 0, and as σ → 0, ˙κl

ηλl−1(0) + γl
13 < 0, γl

22 → 0 (as well as ∂2

ηγL−1(0), ∂2

13∂2
3, γl

ηγl

3 + 1

11(0) → −∞.

ηγL

Claim: For sufﬁciently small σ, ∂2

η(γL
11γL−1)(0) = ∂2

11γL−1)(0) < 0. It converges to −∞ as σ → 0.
11(0)∂2
ηγL

11(0)γL−1(0) + γL

η(γL

Proof: Observe ∂2
by Theorem H.40. So the above claim and Theorem H.45 yield the desired results.
Finishing the main proof: Therefore, if ˚θ(cid:48)
ηγL(0) > 0; if ˚θ(cid:48)
∂2
0; if ˚θ(cid:48)
L+1 = ˚θ(cid:48)
ηγL(0) → 0 as σ → 0.
∂2

L+1 = 0 but ˚θ(cid:48)
Lf = 1, then −∂3
η

˚f1(ξ0) < 0 for small σ because ∂2

˚f1(ξ0) < 0 for small σ because ∂2

Lf = 0, then −∂3
η

L+1 = 1 but ˚θ(cid:48)

Lf = 1, then −∂3
η

η(γL

˚f1(ξ0) > 0 because
11γL−1)(0) <
11γL−1)(0) → −∞ while

η(γL

ηγL−1(0) because ∂ηγL−1(0) = 0

H.7.3 Applications to Tanh

The following property of tanh is easy to verify.
Proposition H.47. Let φ = tanh. For any centered Gaussian Z ∈ R with nonzero variance,

E(φ2)(cid:48)(cid:48)(Z), E(φ2)(cid:48)(cid:48)(Z)φ(cid:48)(Z)2, E φ(cid:48)(cid:48)(Z)2 > 0,

and

E φ(Z)φ(cid:48)(cid:48)(Z)φ(cid:48)(Z)2, E φ(Z)φ(cid:48)(cid:48)(Z), E φ(cid:48)(cid:48)(cid:48)(Z)φ(cid:48)(Z)3, E φ(cid:48)(Z)φ(cid:48)(cid:48)(cid:48)(Z) < 0.

61

In particular, this means

3, γl
κl
Theorem H.48. Consider a pseudostable parametrization with r = 0. If φ is tanh, then for all l ≥ (cid:96),

02(0), ˙κl

13 < 0.

22 > 0,

1, κl
κl

2, γl

3, γl

ηγl(0) < 0,
∂2

ηλl(0) > 0.
∂2

Proof. Similar to the proof of Theorem H.45, except that here κl
0.

3, γl

02(0) < 0, making ∂2

ηγl(0) <

Theorem H.49. Consider a pseudostable parametrization with r = 0. Suppose aL+1 + bL+1 + r = 1
or 2aL+1 + c = 1. If φ is tanh, then

∂3
η

˚f1(ξ0) (cid:54)= 0.

Proof. Similar to the proof of Theorem H.46, except in the expression

∂3
η

˚f1(ξ0) = −

L+1∂2

ηγL(0) + ˚θ(cid:48)

Lf ∂2

η(γL

(cid:16)˚θ(cid:48)

(cid:17)
11γL−1)(0)

,

ηγL(0) and ∂2
∂2
is because ∂2
Proposition H.47.

η(γL

11γL−1)(0) are both negative. The former is because of Theorem H.48. The latter
ηγL
22 > 0 by

ηγL−1(0) ≤ 0 for the same reason, and ∂2

11(0) < 0 since ˙κl

13 < 0, γl

3, γl

H.7.4 Main Results

Proposition H.50. Suppose φ is tanh or σ-gelu for sufﬁciently small σ. A pseudostable parametriza-
tion with r = 0 is nontrivial iff aL+1 + bL+1 = 1 or 2aL+1 + c = 1.

Proof. If aL+1 + bL+1 + r = 1 or 2aL+1 + c = 1, then Theorem H.46 and Theorem H.49 show that
the parametrization is nontrivial. Otherwise, it is trivial by Proposition H.25.

Theorem H.51. Suppose φ is tanh or σ-gelu for sufﬁciently small σ. For any nontrivial pseudostable
parametrization with r = 0, the following are true of the parametrization:

1. not in kernel regime

2. feature learning

3. feature learning in the Lth layer

4. feature kernels evolution

5. feature kernel evolution in the Lth layer

6. prefeature learning

7. prefeature learning in the Lth layer

8. prefeature kernels evolution

9. prefeature kernel evolution in the Lth layer

10. if there is feature learning or feature kernel evolution or prefeature learning or prefeature
kernel evolution in layer l, then there is feature learning and feature kernel evolution and
prefeature learning and prefeature kernel evolution in layers l, . . . , L.

˚f1(ξ0) (cid:54)= 0 by Theorem H.49 or
Proof. The parametrization cannot be in kernel regime since ∂3
η
Theorem H.46. By Theorem H.45 or Theorem H.48, ∂2
ηλl(0) > 0 for all l ≥ (cid:96), so the feature kernel
evolves in layer (cid:96), . . . , L, for some normalized learning rate η > 0. This implies feature learning in
layer (cid:96), . . . , L, since Z xL
0 (cid:54)= 0, so we
have prefeature learning in layer (cid:96), . . . , L. Prefeature kernel evolution in layer (cid:96), . . . , L is implied by
Theorem H.43. Finally, the last statement follows clearly from our logic above.

0 (cid:54)= 0 in this case. This then implies Z hL

1 (ξ0) − Z hL

1 (ξ0) − Z xL

62

Corollary H.52. Suppose φ is tanh or σ-gelu for sufﬁciently small σ. Consider any initialization-
stable parametrization with r = 0. If aL+1 + bL+1 < 1 or 2aL+1 + c < 1, then the parametrization
is not stable.

0 ∆xL

Lf = n1−(aL+1+bL+1) → ∞ but
Proof. First suppose aL+1 + bL+1 < 1 and 2aL+1 + c ≥ 1. Then θ(cid:48)
L+1 ≤ 1. As in the proof of Theorem H.46, there is some η (cid:54)= 0 such that E Z (cid:99)W L+1
˚θ(cid:48)
0 Z δxL
1 (ξ0) =
a.s.−−→ R =⇒
R for some R (cid:54)= 0. Therefore, by the Master Theorem, 1
δxL
1 (ξ0)
1 (ξ0)| = Θ(n1−(aL+1+bL+1)) → ∞. This dominates ∆W L+1
|W L+1
xL
1 (ξ0), which by sim-
ilar reasoning is O(1). So f1(ξ0) diverges and the parametrization is not stable.
Now suppose aL+1 + bL+1 ≥ 1 and 2aL+1 + c < 1. This violates our simplifying assumption
1 (ξ0) a.s.−−→ −η˚χ0 E Z xL
0 Z xL
that aL+1 + bL+1 ≤ 2aL+1 + c, but it’s easy to see that 1
xL
1 (ξ0).
For η small enough, this is close to −η˚χ0 E(Z xL
xL
1 (ξ0)| =
Θ(n1−(2aL+1+c)) → ∞. This dominates W L+1
1 (ξ0) = O(1), so f1(ξ0) diverges. Therefore,
the parametrization is not stable.

0 )2 and thus is nonzero. Then |∆W L+1

n δW L+1

0 ∆xL

n (cid:99)W L+1

1

1

1

0

Finally, suppose both aL+1 + bL+1, 2aL+1 + c < 1. If aL+1 + bL+1 (cid:54)= 2aL+1 + c, then we
have one of ∆W L+1
1 (ξ0) dominate the other like the above, leading to
divergence. If aL+1 + bL+1 = 2aL+1 + c, then in the case of σ-gelu with small σ, W L+1
1 (ξ0)
will dominate ∆W L+1
xL
1 (ξ0), as in Theorem H.46; and in the case of tanh, both have the same sign,
as in Theorem H.49. In either case, f1(ξ0) diverges, so the parametrization is not stable.

1 (ξ0) and W L+1
xL

0 ∆xL

0 ∆xL

1

1

H.8 Putting Everything Together

Finally, in this section we tie all of our insights above to prove our main theorems.
Theorem H.53. Suppose φ is tanh or σ-gelu for sufﬁciently small σ. A parametrization is stable iff
it is pseudostable.

Proof. The “if” direction is given by Proposition H.26. We now show that when any (in)equality of
pseudostability is violated, the parametrization is not stable.

First, if Eq. (41) is not satisﬁed, then Theorem H.19 shows lack of stability.

Second, if Eq. (41) is satisﬁed but r < 0, then Proposition H.28 shows lack of stability.

Finally, if Eq. (41) is satisﬁed and r ≥ 0 but aL+1 +bL+1 < 1 or 2aL+1 +c < 1, then Corollary H.52
or Corollary H.34 shows lack of stability.

Given this result, we will now just say “stable” instead of “pseudostable” from here on.
Theorem H.8 (Nontriviality Characterization). Suppose φ is tanh or σ-gelu for sufﬁciently small σ.
A stable abc-parametrization is nontrivial iff aL+1 + bL+1 + r = 1 or 2aL+1 + c = 1.

Proof. The case of r = 0 and the case of r > 0 are resp. given by Proposition H.50 and
Corollary H.33.

Theorem H.13 (Classiﬁcation of abc-Parametrizations). Suppose φ is tanh or σ-gelu for sufﬁciently
small σ. Consider a nontrivial stable abc-parametrization of an L-hidden layer MLP. Then

1. The following are equivalent to r = 0

(a) feature learning
(b) feature learning in the Lth layer
(c) feature kernels evolution
(d) feature kernel evolution in the Lth layer
(e) prefeature learning
(f) prefeature learning in the Lth layer
(g) prefeature kernels evolution
(h) prefeature kernel evolution in the Lth layer

63

2. The following are equivalent to r > 0

(a) kernel regime
(b) ﬁxes all features
(c) ﬁxes features in the Lth layer
(d) ﬁxes all feature kernels
(e) ﬁxes feature kernel in the Lth layer
(f) ﬁxes all prefeatures
(g) ﬁxes prefeatures in the Lth layer
(h) ﬁxes all prefeature kernels
(i) ﬁxes prefeature kernel in the Lth layer

3. If there is feature learning or feature kernel evolution or prefeature learning or prefeature
kernel evolution in layer l, then there is feature learning and feature kernel evolution and
prefeature learning and prefeature kernel evolution in layers l, . . . , L.

4. If r = 0, then for all ξ ∈ X , f0(ξ) a.s.−−→ 0 and ft(ξ) a.s.−−→ ˚ft(ξ) for some deterministic ˚ft(ξ).

However, the converse is not true.

5. If r > 0, aL+1 + bL+1 + r > 1 and 2aL+1 + c = 1, then we have the Neural Network-

Gaussian Process limit.

Proof. A nontrivial stable parametrization has either r = 0 or r > 0. By Theorem H.51,
Proposition H.27, and Theorem H.32, r = 0 implies all of the statements in (1) and r > 0 im-
plies all of the statements in (2). Consequently, if feature learning happens, then clearly r cannot be
positive, so r must be 0. Likewise, all of the statements in (1) imply r = 0. Symmetrically, all of the
statements in (2) about ﬁxing features imply r > 0. Finally, if the parametrization is in kernel regime,
then by Theorem H.51(1), r cannot be 0, so r > 0. This proves (1) and (2).

If the premise of (3) holds, then by the above, r = 0, so the conclusion follows from Theorem H.51.
This proves (3).
If r = 0, then nontriviality means aL+1 + bL+1 ≥ 1. This implies f0(ξ) a.s.−−→ 0 for all ξ ∈ X (more
precisely, f0(ξ) has standard deviation Θ(n1/2−(aL+1+bL+1)) → 0 by Central Limit Theorem). The
program describes the unconditional SGD trajectory of f (as opposed to the case when aL+1+bL+1 =
1/2), so ft(ξ) a.s.−−→ ˚ft(ξ) does not depend on f0. The converse is not true, for example because of
Corollary H.35. This prove (4).

(5) follows from Corollary H.35 (which actually allows much more general φ).

Proofs of Theorems 6.1, 6.3 and 6.4 For any ﬁnite subset X of the input space Rd (where
d = 1 here), we can write out the SGD computation as a Tensor Program like in Appendix H.4.
Then the Master Theorem implies the convergence of ft(ξ) a.s.−−→ ˚ft(ξ) for every ξ ∈ X . Let
X1 ⊆ · · · ⊆ Xk ⊆ · · · be an inﬁnite chain of ﬁnite subsets of Rd such that (cid:83)
k Xk is a dense subset of
Rd. Then the convergence of ft(ξ) a.s.−−→ ˚ft(ξ) holds for every ξ ∈ (cid:83)
k Xk (because we have almost
sure convergence). Finally, we apply a continuity argument to get this convergence for all of Rd:
Because φ(cid:48) and thus φ are pseudo-Lipschitz, they are locally Lipschitz (i.e. Lipschitz on any compact
set). In addition, the operator norms of W L are almost surely bounded from standard matrix operator
bounds. Thus one can see that the Tensor Program is locally Lipschitz in ξ. Consequently, ˚ft(ξ) is
continuous in ξ. This allows to pass from (cid:83)
k Xk to Rd.

Proofs of Propositions 5.3, 5.5 and G.2 and Theorems G.3 and G.4 follow by dividing into
cases of r > 0 and r = 0 and easy modiﬁcation of the reasoning in Appendices H.6 and H.7.

Proof of Theorem H.17 follows from straightforward calculations. The basic outline of the
calculations is: 1) During pretraining, f ’s change is purely due to a) the interaction betwen ∆W l, l ≤
L, and W L+1
, and b) the interaction between xL and ∆W L+1. 2) When W L+1 is re-initialized in
g, these interactions are killed. The pretrained ∆W l, l ≤ L, will cause xM to differ by Θ(1/
n)
coordinatewise compared to if ∆W l, l ≤ L, are all reset to 0, but this difference is uncorrelated with

√

0

64

the last layer weights W M +1 of g, so their interaction is subleading in n, i.e. in the inﬁnite-width
limit,

gT ;t(ξ) − g0;t(ξ) a.s.−−→ 0,

whether all of g or just the new weights are trained during ﬁntetuning.

65

