8
1
0
2

l
u
J

2

]

G
L
.
s
c
[

1
v
5
9
5
0
0
.
7
0
8
1
:
v
i
X
r
a

Logical Explanations for Deep Relational
Machines Using Relevance Information

Ashwin Srinivasan1, Lovekesh Vig2, and Michael Bain3

1 Department of Computer Sc. & Information Systems
BITS Pilani, K.K. Birla Goa Campus, Goa
2 TCS Research, New Delhi.
3 School of Computer Science and Engineering
University of New South Wales, Sydney, NSW.

Abstract. Our interest in this paper is in the construction of symbolic
explanations for predictions made by a deep neural network. We will
focus attention on deep relational machines (DRMs: [21]). A DRM is a
deep network in which the input layer consists of Boolean-valued func-
tions (features) that are deﬁned in terms of relations provided as domain,
or background, knowledge. Our DRMs diﬀer from those in [21], which
uses an Inductive Logic Programming (ILP) engine to ﬁrst select features
(we use random selections from a space of features that satisﬁes some
approximate constraints on logical relevance and non-redundancy). But
why do the DRMs predict what they do? One way of answering this was
provided in recent work [33], by constructing readable proxies for a black-
box predictor. The proxies are intended only to model the predictions
of the black-box in local regions of the instance-space. But readability
alone may not enough: to be understandable, the local models must use
relevant concepts in an meaningful manner. We investigate the use of
a Bayes-like approach to identify logical proxies for local predictions of
a DRM. We show: (a) DRM’s with our randomised propositionaliza-
tion method achieve state-of-the-art predictive performance; (b) Models
in ﬁrst-order logic can approximate the DRM’s prediction closely in a
small local region; and (c) Expert-provided relevance information can
play the role of a prior to distinguish between logical explanations that
perform equivalently on prediction alone.

1

Introduction

In [14], a contrast is presented between theories that predict everything, but
explain nothing; and those that explain everything, but predict nothing. Both
are seen as having limited value in the scientiﬁc enterprise, which requires models
with both predictive and explanatory power. Michie [7] adds a further twist to
this by suggesting that the limitations of the human brain may force a “window”
on the complexity of explanations that can be feasibly understood, even if they
are described in some readable form like a natural or a symbolic formal language.
Complexity limits notwithstanding, it is often assumed that that predictive
and explanatory assessments refer the same model. But this does not have to be

 
 
 
 
 
 
2

Ashwin Srinivasan, Lovekesh Vig, and Michael Bain

so: early results in the Machine Learning (ML) literature on behavioural cloning
point to areas where people perform predictive tasks using sub-symbolic models,
for which entirely separate symbolic explanations were constructed by machine
[6]. It is not therefore inevitable that machine learning models be either purely
sub-symbolic or purely symbolic.

There are, of course, problems for which one or the other is much better
suited. Prominent recent examples of successful sub-symbolic learning abound
with the use of deep neural networks in automatic caption learning (“woman in a
white dress standing with tennis racquet”: [15]), speech-recognition (Siri, Google
Now) and machine translation (Google Translate). It is equally evident that
for learning recursive theories, program synthesis, and learning with complex
domain-knowledge, symbolic techniques like Inductive Logic Programming (ILP)
have proved to be remarkably eﬀective. But there are also a number of problems
that could beneﬁt from an approach that requires not one or the other, but both
forms of learning (in his book “Words and Rules”, Steven Pinker conjectures
language learning as one such task).

Our interest here therefore is in investigating the use of separate models for
prediction and explanation. Speciﬁcally, we examine the use of a neural model
for prediction, and a logical model for explanation. But in order to ensure that
the models are about the same kinds of things, the models are not constructed
independent of each other. The neural model is constructed in terms of features
in (ﬁrst-order) logic identiﬁed by a simple form of propositionalization methods
developed in ILP. In turn, the predictions of the neural model are used to con-
struct logical models in terms of the features.4 Figure 1 shows how the various
pieces we have just described ﬁt together.

Having two separate models would appear to be an extravagance that could
be ill-aﬀorded if the neural model wasn’t a very good predictor, or if the logical
model wasn’t a very good explainer. In this paper, we present empirical evidence
that neither of these appears to happen here. For ML practitioners, the principal
points of interest in the paper are these:

(a) We present a simple randomised method for obtaining ﬁrst-order features
that use domain-knowledge for use by a deep neural network. The resulting
deep relational machine, or DRM, achieves high predictive performance, with
small numbers of data instances. This extends existing work on DRMs, which
have in the past required the use of a full-ﬂedged ILP engine for feature-
selection [21]; and

(b) We present a method for extracting local symbolic explanations for predic-
tions made by the DRM by employing a Bayes-like trade-oﬀ of likelihood
and prior preference. The latter is obtained using domain-knowledge of rel-
evance of predicates used in the explanations. This extends the techniques
proposed in [33] in the direction of making explanations both more expressive
(by using ﬁrst-order logic) and meaningful (by using a Bayes-like method of
incorporating domain-speciﬁc preferences into explanations).

4 We restrict ourselves in this paper to queries of the form: “What is the class of in-
stance x?”, but the ideas extend straightforwardly to tasks other than classiﬁcation.

Title Suppressed Due to Excessive Length

3

Fig. 1. What this paper is about. N is a deep neural network, and S is a symbolic
learner. B is domain-speciﬁc background knowledge; L language constraints; E a set
of training examples. N is used to construct a predictive model M . Explanations of
why M predicts the class of a test instance x are to be obtained as a logical model
that uses features, that are pre-deﬁned in B or “invented”. Correctly, B and L on the
right are augmentations of those on the left; and the S’s on right and left are diﬀerent
instantiations of a ﬁrst-order symbolic learner that is capable of producing deﬁnitions
of features (left), or explanations (right).

The rest of the paper is organised as follows. In Section 2 we describe the
DRMs we use for prediction. Section 3 describes the notion of explanations as
used in this paper. Selection amongst several possible explanations is in Section
4, which introduces the use of a relevance-based prior in Section 4.1. Section 5
presents an empirical evaluation of the predictive and explanatory models, using
some benchmark datasets. Appendix 7 contains details of the domain-speciﬁc
relevance information used in the experiments.

2 A Deep Relational Machine for Prediction

One of the most remarkable recent advances in the area of Machine Learning
is a resurgence of interest in neural networks, resulting from the automated
construction of “deep networks” for prediction. Simplistically, Deep Learning is
using neural networks with multiple hidden layers. Mathematically speaking, it
is a composition of multiple simple non-linear functions trying to learn a hier-
archy of intermediate features that most eﬀectively aid the global learning task.
Learning such intermediate features with neural networks has been made pos-
sible by three separate advances: (a) mathematical techniques that allow the

4

Ashwin Srinivasan, Lovekesh Vig, and Michael Bain

training of neural networks with very large numbers of hidden layers; (b) the
availability of very large amounts of data that allow the estimation of param-
eters (weights) in such complex networks; and (c) the advanced computational
capabilities oﬀered by modern day GPUs to train deep models. Despite successes
across a wide set of problems, deep learning is unlikely to be suﬃcient for all
kinds of data analysis problems. The principal diﬃculties appear to lie in the
data and computational requirements to train such models. This is especially the
case if many hidden layers are needed to encode complex concepts (features).
For many industrial processes, acquiring data can incur signiﬁcant costs, and
simulators can be computationally very intensive.

Some of this diﬃculty may be alleviated if knowledge already available in
the area of interest can be taken into account. Consider, for example, a problem
in the area of drug-design. Much may be known already about the target of
interest: small molecules that have proved to be eﬀective, what can and cannot
be synthesized cheaply, and so on. If these concepts are relevant to constructing
a model for predicting good drugs, it seems both unnecessary and ineﬃcient to
require a deep network to re-discover them (the problem is actually worse — it
may not even be possible to discover the concepts from ﬁrst-principles using the
data available). It is therefore of signiﬁcant practical interest to explore ways in
which prior domain-knowledge could be used in deep networks to reduce data
and computational requirements.

Deep Relational Machines, or DRMs, proposed in [21], are deep neural net-
works with ﬁrst-order Boolean functions at the input layer (“function F1 is true
if the instance x is a molecule containing a 7-membered ring connected to a
lactone ring” — deﬁnitions of relations like 7-membered and lactone rings are
expected to be present in the background knowledge). In [21] the functions are
learned by an ILP engine. This follows a long line of research, sometimes called
propositionalization, in which features constructed by ILP have been used by
other learning methods like regression, decision-trees, SVMs, topic-models, and
multiplicative weight-update linear threshold models. In each of these, the ﬁ-
nal model is constructed in two steps: ﬁrst, an ILP engine constructs a set of
“good” features, and then, the ﬁnal model is constructed using these features,
possibly in conjunction with other features already available. Usually the models
show signiﬁcant improvements in predictive performance when an existing fea-
ture set is enriched in this manner. In [21], the deep network with ILP-features
is shown to perform well, although the empirical evidence is limited. For the
DRMs used in this paper we dispense with the requirement for an ILP-based se-
lection of input features. Instead we use random selections of features generated
from a space that has some constraints on logical relevance and redundancy.
We will continue to refer to this kind of model as a DRM, since the features
are still ﬁrst-order Boolean functions deﬁned in terms of relations provided as
background knowledge.

Title Suppressed Due to Excessive Length

5

2.1 First-Order Features as Inputs

In this paper, we do not want to pre-select input features using an ILP engine.
Instead, we would (ideally) like the inputs to consist of all possible relational
features, and let the network’s training process decide on the features that are
actually useful (in the usual manner: a feature that has 0 weights for all out-
going edges is not useful). The diﬃculty with this is that the number of such
features in ﬁrst-order logic can be very large, often impractical to enumerate
completely. We clarify ﬁrst what we mean by a relational feature.

Deﬁnition 1 Relational Examples for Classiﬁcation. The set of examples
provided can be deﬁned as a binary relation Class which is a subset of the Carte-
sian product X ×Y where X denotes the set of relational instances (for simplicity,
and without loss of generality, we will take each instance to be a ground ﬁrst-
order term) and Y denotes the ﬁnite set of class labels. Any single element of
the set of relational examples will be denoted by Class(a, c) where a ∈ X and
c ∈ Y.

Deﬁnition 2 Relational Features. A relational feature is a unary function
: X (cid:55)→ {TRUE , FALSE } deﬁned in terms of a conjunction of predicates
Fi
Cpi(x). Each predicate in Cpi(x) is deﬁned as part of domain- or background-
knowledge B. That is, Fi(x) (cid:55)→ TRUE iﬀ Cpi(x) (cid:55)→ TRUE and FALSE other-
wise. We will represent each relational feature as a single deﬁnite clause ∀x (Fi(x) ←
Cpi(x)) in a logic program, relying on the closed-world assumption for the com-
plete deﬁnition of Fi. We will sometimes call the relational feature Fi simply the
feature Fi, and the deﬁnite-clause deﬁnition for Fi the feature-deﬁnition for Fi.
If the feature-deﬁnition of Fi is in B, we will sometimes say “feature Fi is in
B.” We will usually denote the set of features in B as FB or simply F.

Deﬁnition 3 Classiﬁcation clause. A clause for classifying a relational ex-
ample Class(a, c) is a clause ∀x(Class(x, c) ← Cp(x)), where Cp(x) is a con-
junction of predicates. Each predicate in Cp(x) is deﬁned as part of domain- or
background-knowledge B.

It is evident from the deﬁnitions that some or all of a classiﬁcation clause can
be converted into relational features.

Example 4 The trains problem. A well-known synthetic problem in the ILP
literature (Michalski’s “Trains” problem, originally posed in [24] – see Fig. 2) can
be used to illustrate this For this problem each relational example is a pair, con-
sisting of a ground ﬁrst-order term and a class label. The ground term represents
the train (for example T rain(Car(Long, Open, Rect, 3), Car(Short, Closed, T riangle, 1),
. . .)) and the label denotes whether or not it is Eastbound.

We assume also that we have access to domain (or background) knowledge B
that allows us to access aspects of the relational instance such as Has Car(T rain(. . .),
Car(. . .)), Closed(Car(. . .)) and so on. Then a clause for classifying Eastbound
trains is (we leave out the quantiﬁers for simplicity):

Class(x, East) ← Has Car(x, y), Short(y), Closed(y)

6

Ashwin Srinivasan, Lovekesh Vig, and Michael Bain

Here Cp(x) = Has Car(x, y)∧Short(y)∧Closed(y). The following relational

features can be obtained from this classiﬁcation clause:

F1(x) ← Has Car(x, y), Short(y)

and:

F2(x) ← Has Car(x, y), Closed(y)

The classiﬁcation clause could now be written as:

Class(x, c) ← F1(x), F2(x)

Deﬁnition 5 Feature Vector of a Relational Instance. For a set of fea-
tures F ordered in some canonical sequence (F1, F2, . . . , Fd) we obtain a Boolean-
vector representation a(cid:48) ∈ {0, 1}d of a relational instance a ∈ X using the func-
tion F V : X → {0, 1}d, where the ith component F V i(x) = 1 if Fi(x) (cid:55)→ TRUE ,
and 0 otherwise. We will sometimes say a(cid:48) is the feature-space representation of
a.

Fig. 2. Michalski’s trains problem. There are two sets of trains: Eastbound and West-
bound. Descriptors for the trains include: number, shape and lengths of the car, shape
of load, and so on. The task is to determine classiﬁcation rules to distinguish Eastbound
trains from Westbound ones.

2.2 Logical Relevance and Redundancy of Features

So far, features have been deﬁned purely syntactically. This is usually not suﬃ-
cient to ensure that a feature in the class is relevant to the problem considered, or

Title Suppressed Due to Excessive Length

7

that it is redundant given another feature. For example, the feature F1 deﬁned by
the clause F1(x) ← Has Car(x, y), Open(y), Closed(y)) is clearly irrelevant in
the trains problem, since no car can be both open and closed; and the feature F3
deﬁned by the clause F3(x) ← Has Car(x, y), Short(y), Closed(y) is redundant
given a feature-deﬁnition F2(x) ← Has Car(x, y), Closed(y), Short(y).

In their full scope, both irrelevance and redundancy of features are semantic
notions dependent on domain knowledge. Since features are deﬁned by clauses,
in this paper we will use approximations based on the well-understood concept
of clause subsumption from the ILP literature, the main deﬁnitions of which we
reproduce here for completeness:

Deﬁnition 6 Clause subsumption. We use Plotkin’s partial ordering on the
set of clauses [32]. A clause C subsumes a clause D or C (cid:22)θ D, iﬀ there exists
a substitution θ s.t. Cθ ⊆ D. It is known that if C (cid:22)θ D then C |= D. Further
if C (cid:22)θ D and D (cid:22)θ C we will say C ≡θ D.

Remark 7 Most speciﬁc clause in a depth-bounded mode language.
This notion is due to Muggleton [28]. Given a set of modes M , let Ld(M ) be
the d-depth mode language. Given background knowledge B and an instance e,
let the most speciﬁc clause that logically entails e in the sense described in [28]
be ⊥(B, e). For every ⊥(B, e) it is shown in [28] that: (a) there is a ⊥d(B, e) in
Ld(M ) s.t. ⊥d(B, e) (cid:22) ⊥(B, e); and (b) ⊥d(B, e) is ﬁnite.

Using these notions, we adopt the following deﬁnitions:

Deﬁnition 8 Logical relevance. Given a set of examples E, and background
knowledge B, an independent clause C is relevant if C (cid:22) ⊥d(B, e) for at least
one e ∈ E. We will further restrict this to C ⊆ ⊥d(B, e).

Deﬁnition 9 Logical redundancy. For a pair of independent clauses C and
D, we will say D is redundant given C (and vice-versa), if |C| = |D| and
C ≡θ D.

These deﬁnitions based on subsumption are an approximation to the true logical
deﬁnitions of relevance and redundancy, and can result in some errors (assuming
relevance when it does not exist, and missing redundancy when it does exist).
Nevertheless, they can be computed reasonably eﬃciently. They form the basis
of a method of randomised construction of inputs for the deep network.

2.3 Randomised Feature Selection

The mechanisms for detecting logical relevance and redundancy help to re-
duce the number of possible features within a mode language. But they still
do not guarantee that the numbers will be manageable. We therefore resort to a
rejection-based sampling strategy for selecting inputs (Fig. 3). Of course, inputs
to the DRM are not the features themselves but the feature-space representations
for the relational instances in E (Fig. 4).

8

Ashwin Srinivasan, Lovekesh Vig, and Michael Bain

DrawF eatures(B, E, M, L, d, M axDraws) :

Given: Background knowledge B; examples E; a set of modes M; language constraints
L; a depth-bound d; and an upper-bound on the number of samples M axDraws.

Find: A set of features F s.t. |F| ≤ M axDraws

1. Let F = ∅
2. Let draws = 0
3. Let i = 1
4. Let Drawn be ∅
5. while draws ≤ M axDraws do

(a) Randomly draw with replacement an example ei ∈ E. Let ei = Class(a, c),

where a is a relational instance and c is a class label.

(b) Let ⊥d(B, ei) be the most speciﬁc clause in the depth-limited mode language
Ld(M) that subsumes ⊥(B, ei) (i.e., the most-speciﬁc clause that entails ei,
given B).

(c) Randomly draw a clause Ci s.t. Ci ⊆ ⊥d(B, ei)
(d) if (Ci is not redundant given Drawn) then
i. Let Ci = (Class(x, c) ← Cpi(x))
ii. Let Fi = (Fi(x) ← Cpi(x))
iii. F := F ∪ {Fi}
iv. Drawn := Drawn ∪ {Ci}
v. increment i

(e) increment draws

6. done
7. return F

Fig. 3. A randomised procedure for drawing features from a depth-limited mode lan-
guage. A procedure for randomly drawing clauses subsuming a most-speciﬁc clause
required in Step 5c is described in [43].

3 Logical Explanations for Predictions by a DRM

Predicting the class-label of a relational instance a using the model M in Fig. 4
is a 2-step process: (1) The feature-vector representation a(cid:48) of a is obtained using
the feature-deﬁnitions found during the training stage; and (2) a(cid:48) is provided as
input to the model M found during training, which then computes the class-label
(Fig. 5).

It has long been understood that neural models can compute accurate an-
swers for questions like “What is the prediction for x?”. But what can be said
about why the answer is what it is? Unsurprisingly, there has been a lot of re-
search eﬀort invested into extracting explanations for the predictions made by a
neural network (see [8], Chapter 3 for a full description of this line of research).
Most of this eﬀort has been in the direction of translating the network into
a single symbolic model that guarantees correspondence to the neural model
(for example, [42]). Alternatively, a single symbolic model can be learned that
approximates the behaviour of the neural model over all inputs (for example,

Title Suppressed Due to Excessive Length

9

Fig. 4. More details on the training component in Fig. 1. For simplicity, we are tak-
ing the modes M to be part of the language speciﬁcation L, and do not show some
parameters. The optimiser ﬁnds the best network structure and parameters using the
feature-vectors provided for training. FV(E) denotes the feature-vectors of the rela-
tional instances E. The model M is a DRM.

[5]). Although distinct in their aims, both approaches still result in two sep-
arate models, one neural and the other symbolic. In both cases the symbolic
model is intended to be a readable proxy for the neural model, which can then
form a basis for an explanation of “why” questions. But there are some inherent
trade-oﬀs:

– If we try to replicate exactly the behaviour of the neural network with a
symbolic model (as is done, say, in [42]), then the resulting model may not
be any more comprehensible than the neural network; and

– If we try only to approximate the behaviour of the network using logical
predicates (as is done, say, in [5]), then we run the risk of not being able to
replicate the network’s behaviour suﬃciently accurately over all instances,
because of inadequacies of the logical predicates available.

Both these issues are exacerbated for modern-day deep networks, with many
hidden layers and large numbers of inputs. One way to side-step these diﬃculties
is simply to drop the requirement of translating the entire network model into
a single symbolic model. Recent research called LIME (“Local Interpretable
Model-agnostic Explanations”: [33]) proposes producing readable proxies “on-
demand”, for any kind of black-box predictor. The key feature is that a LIME-
style explanatory model is constructed only when a prediction for an instance is
sought, and the explanation is required to be faithful to the black-box’s (in our
case, a neural network) predictions for the instance and its near-neighbours. The

10

Ashwin Srinivasan, Lovekesh Vig, and Michael Bain

Fig. 5. Prediction of the class of a relational instance a by a DRM. The inputs fi
are the component values of the feature-vector F V i(a). The shaded inputs denote a
feature-value of 1. With a DRM, there is more to fi = 1 than just an assignment of 1 to
feature: it also means that some conjunction of background predicates Cpi(a) is TRUE
for the relational instance a. The shaded circles in hidden layers denote activated nodes.
The shaded output node denotes the prediction of the relational instance is East.

Title Suppressed Due to Excessive Length

11

intuition is that while comprehensible models may not be possible for the entire
black-box model, they may be possible when restricted to local predictions.

The requirement of having to be consistent with only local instances results
in a diﬀerent kind of problem. Since the near-neighbours of an instance may
be quite small, there may be insuﬃcient constraints, in data-theoretic terms, to
narrow down on a unique (or even a small number of) explanations. So, how
then is an explanation to be selected? In LIME, this is left to the loss-function.
In Bayesian terms, this means deﬁning an appropriate prior to guide selection
when the data are insuﬃcient.

3.1 Local Explanations

We now consider a logical formulation of the setting introduced in the LIME
paper that is well-suited to explaining DRM models. We draw on speciﬁc situa-
tions described in [27], namely to clarify what is meant by a local explanation,
and its data-theoretic evaluation. Later we will introduce a domain-dependent
prior to develop a Bayes-like selection of interpretable explanations.

Deﬁnition 10 Explanations. In this paper, we will only be concerned with
explanations for the classiﬁcation of an instance. As before, we assume a set of
relational instances X , and a ﬁnite set of classes Y. Given a relational instance
a ∈ X and a label c ∈ Y, the statement Class(a, c) will denote that the class of
a is c. Let e = Class(a, c). Then given background knowledge B, an explanation
for e is a set of clauses H s.t. B ∪ H |= e.

At this point, we diﬀer from Good [11], for whom it is neither necessary nor
suﬃcient for good explanations to logically imply e, given B. As we present it
here, logical implication is necessary, but as will be seen below, not suﬃcient for
a good explanation. We will seek explanations of a restricted kind, namely those
deﬁned in terms of features.

Deﬁnition 11 Feature-clauses. Let F be a set of features. A deﬁnite clause
is said to be a feature-clause if all negative literals are of the form Fi(x), where
the Fi are in F and the x is universally quantiﬁed.

Single-Clause Explanations Assume we have a set of d features F with
corresponding feature-deﬁnitions in the background knowledge B. We will ﬁrst
consider the case where explanation H consists of a single feature-clause. That is,
H = {C}, where C is a deﬁnite clause ∀x (Class(x, c) ← Body). Here Body is a
conjunction of Fi/1 literals, each of which is deﬁned in B. The ILP practitioner
will recognise that ﬁnding a single feature-clause explanation for Class(a, c)
is an instance of the “Single example clause, single hypothesis clause” situation
identiﬁed in [27], which forms the basis of explanation-based learning (EBL) [26].

Example 12 An explanation in the trains problem. An explanation H =
{C} for the ﬁrst train in the left column of Fig. 2 is the feature-clause:

12

Ashwin Srinivasan, Lovekesh Vig, and Michael Bain

C : Class(t1, East) ← F1(t1), F2(t1), F3(t1)

where B contains:

∀x(F1(x) ← ∃y(Has Car(x, y), Short(y)))

∀x(F2(x) ← ∃y(Has Car(x, y), Closed(y)))

∀x(F3(x) ← ∃y(Has Car(x, y), Long(y)))

Here t1 is used as short-form for a structured term, describing the train, and, as
before, let us assume that appropriate deﬁnitions exist in B for predicates like
Has Car, Long, Short, and Closed to succeed (or fail) on terms like t1.

One explanation for e = Class(a, c) can be obtained immediately as follows.
Consider the set Fa = {Fi : Fi ∈ F s.t. F V i(a) = 1} (let us call these the set
of active features for a). By deﬁnition, if F V i(a) = 1 then Fi(a) (cid:55)→ TRUE .
For simplicity, let us suppose that Fa = {F1, . . . Fk} is the set of active features
for a, and let C : Class(a, c) ← F1(a), F2(a), . . . , Fk(a). Assuming the Fi are
deﬁned in B, H = {C} is an explanation for e.5 Fig. 6 is a simple procedure to
construct a single-clause explanation C (cid:48) that relies on the result that if C (cid:48) (cid:22)θ C,
then C (cid:48) |= C. Therefore, if B ∪ {C} |= e, then evidently B ∪ {C (cid:48)} |= e. That is,
{C (cid:48)} will also be an explanation for e. For reasons that will be apparent, we will
call {C (cid:48)} an “unstructured” explanation.6

ConstructU nstruct(e, B, F) :

Given: A relational instance e; background knowledge B; a set of features F with

deﬁnitions in B.

Find: A single feature-clause explanation H s.t. B ∪ H |= e.

1. Let e = Class(a, c)
2. Let a(cid:48) = F V (a)
3. Let F (cid:48) be the set of features that map to T RU E in a(cid:48)
4. Let H be the subset-lattice of F (cid:48)
5. Let F (cid:48) be any element in H s.t.

– Body is the conjunction of features in F (cid:48);
– C (cid:48) = ∀x(Class(x, c) ← Body)

6. return {C}

Fig. 6. A procedure for identifying a single-clause unstructured explanation.

5 ILP practitioners will recognise C as being analogous to the most-speciﬁc clause in

[28], and we will call it the most-speciﬁc feature-clause for e, given F and B.

6 In practice, the lattice in Step 4 would be represented by a graph, and ﬁnding an
element of the lattice in Step 5 will involve some form of optimal graph-search to
ﬁnd an optimal (or near-optimal) solution.More on this later.

Title Suppressed Due to Excessive Length

13

Multi-Clause Explanations We now extend explanations to a restricted form
of multi-clause explanations, which use features that are not all deﬁned in the
background knowledge B. We will require these “invented” features to be (re-
)expressible in terms of features already deﬁned in B. In this paper we will
require structured and unstructured explanations to be related by the logic-
program transformation operations of folding and unfolding. That is, given an
unstructured explanation H and a structured explanation H1, H can be derived
from H1 using one or more unfolding transformations; and H1 can be derived
from H using one or more folding transformations. We describe the transforma-
tions, along with the conditions that ensure that the computable answers do not
change when the transformation is applied.

Deﬁnition 13 (One-Step) Unfolding of a clause [13]. Given a set of deﬁ-
nite clauses P , W.l.o.g. let C ∈ P s.t. C = Head ← L1, . . . , Li, . . . , Lk (k ≥ 1).
Let C (cid:48) be a clause L(cid:48)
i unify with m.g.u θ. Then the
(one-step) unfolding of C w.r.t L using P is the clause U nf old(C) : (Head ←
L1, . . . , Li−1, Body(cid:48), Li+1, . . . , Lk)θ

i ← Body(cid:48) s.t. Li and L(cid:48)

i ← Body(cid:48) s.t. there is a substitution θ that satisﬁes: (a) Bodyi = Body(cid:48)

Deﬁnition 14 (One-Step) folding of a clause [13]. Given a set of deﬁnite
clauses P , W.l.o.g. let C ∈ P s.t. C = Head ← L1, . . . , Li−1, Bodyi, Li+1, . . . , Lk,
where Bodyi is some literals B1, B2, . . . , Bj (j, k ≥ 1). Let C (cid:48) be a clause
L(cid:48)
iθ; (b)
Every existentially quantiﬁed variable y in C, yθ is a variable that occurs only
in Bodyi and nowhere else; (c) For any pair of distinct existential variables y, z
in C yθ (cid:54)= zθ; and (d) C (cid:48) is the only clause in P whose positive literal uniﬁes
with L(cid:48)
iθ. Then the (one-step) unfolding of C w.r.t. Bodyi using P is the clause
F old(C) : (Head ← L1, . . . , Li−1, L(cid:48)
iθ, Li+1, . . . , Lk).

Remark 15 Correctness of Transformations. In [10] the unfold and fold
transformations are deﬁned above are shown to be correct as replacement rules
w.r.t. the minimal-model (MM) semantics. That is, if P is a deﬁnite-clause
program and C ∈ P , then M M (P ) = M M ((P − {C}) ∪ T rans(C)), where
T rans(C) is F old(C) or U nf old(C).

We use this to construct structured explanations that are correct, in the com-
putational sense just identiﬁed. That is, a structured explanation can replace an
unstructured explanation without altering the minimal model of the (program
containing) the unstructured explanation. It is convenient for us to introduce
the notion of an “invented” feature.

Deﬁnition 16 Invented Feature. Given background knowledge B, a feature
F is said to be an invented feature if: (1) F is not deﬁned in B; and (2) The
deﬁnition of F is a feature-clause C whose body contains features in B only; or
is a clause that unfolds to a feature-clause C containing features only in B. We
will sometimes denote C as U F C(F ) (short for “unfolded feature-clause for F”).

Example 17 Invented features in the trains problem. The features F1,1
and F1,2 are invented features:

14

Ashwin Srinivasan, Lovekesh Vig, and Michael Bain

F1,1(x) ← F2(x), F1,2(x)
F1,2(x) ← F4(x), F9(x)

where the Fi are features deﬁned in background knowledge:

F2(x) ← Has Car(x, y), Short(y)
F4(x) ← Has Car(x, y), W heels(y, 3)
F9(x) ← Has Car(x, y), Load(y, triangle)

Then U F C(F1,1) is:

F1,1,(x) ← F2(x), F4(x), F9(x)

and U F C(F1,2) is identical to the deﬁnition of F1,2.

We distinguish between unstructured and structured deﬁnitions based on the
presence or absence of invented features.

Deﬁnition 18 Unstructured and structured explanations. Let a ∈ X and
Y be a set of class labels, with c ∈ Y. Let N : (cid:60)d → Y be a predictive model such
that N (F V (a)) = c. Given background knowledge B, let H be an explanation
for Class(a, c) containing the feature-clause C : ∀x(Class(x, c) ← Body). Let
Body consist of features FC. Let FC be partitioned into: (a) FC,old, consisting
of features deﬁned in B; and (b) FC,new, consisting of features deﬁned in H − B.
We will call H an unstructured explanation iﬀ: (1) FC contains only old

features (that is, FC,new = ∅); and (2) H = {C}.

We will call H a structured explanation iﬀ: (1) FC contains only invented
features (that is, FC,old = ∅); and (2) H = {C} ∪ InvF , where InvF pnly
contains clauses deﬁning invented features in FC,new; (3) Each feature F in
FC,new there is a single clause deﬁnition in InvF , s.t. F unfolds to a unique
feature-clause deﬁned using features in B only; and (4) At least one F ∈ FC,new
unfolds to a feature-clause that contains at least 2 features from B (that is, there
is at least one invented feature that is not a trivial rewrite of features in B).7

That is, a structured explanation is a set of clauses containing a classiﬁcation
clause C along with deﬁnitions of invented features (and thus is a case of the
“single example clause, multiple hypothesis clauses” situation in [27]).

Example 19 Unstructured and structured explanations for the trains
problem. Suppose we are given an instance a ∈ X , and a set of features F, each
deﬁned by feature-deﬁnitions in background B. Then we will call the following
an unstructured explanation (x is universally quantiﬁed):

C : Class(x, East) ← F2(x), F3(x), F4(x), F9(x)

The following is a structured explanation H1:

7 We assume the case where FC,old (cid:54)= ∅ and FC,new (cid:54)= ∅ will be represented by a
structured explanation in which there is a new feature representing the conjunction
of existing features. See the example that follows.

Title Suppressed Due to Excessive Length

15

Class(x, East) ← F1,1(x), F1,2(x)
F1,1(x) ← F2(x), F3(x)
F1,2(x) ← F4(x), F9(x)

Note that H1 unfolds to H. Another structured explanation that also unfolds to
H is H2 below:

Class(x, East) ← F1,1(x), F1,2(x)
F1,1(x) ← F1(x), F4(x)
F1,2(x) ← F3(x), F9(x)

We will assume that explanations of the form Class(x, East) ← F1,1(x), F3(x), F9(x)
will be represented as H2. Also, by deﬁnition, the following is not a structured
explanation, since all new features are trivial rewrites of existing ones:

Class(x, East) ← F1,1(x), F1,2(x)
F1,1(x) ← F1(x)
F1,2(x) ← F3(x)

But this is a structured explanation:

Class(x, East) ← F1,1(x), F1,2(x)
F1,1(x) ← F1(x)
F1,2(x) ← F3(x), F9(x)

It is obvious enough that the unfolding step that every structured explanation
allows the derivation of a correct unstructured explanation.

Remark 20 Deriving an Unstructured Explanation from a Structured
Explanation. Let H = {C (cid:48)} ∪ InvF be a structured explanation for a relational
example Class(a, c) given background knowledge B. W.l.o.g. from Defn. 18, C (cid:48)
k(x) s.t. each F (cid:48)
2(x), . . . , F (cid:48)
1(x), F (cid:48)
is a clause Class(x, c) ← {F (cid:48)
i unfolds using
InvF to a unique feature-clause F (cid:48)
i(x) ← Fi,1(x), . . . , Fi,ni(x), where the Fi,·
are deﬁned in B. From Defn.13, unfolding C (cid:48) w.r.t. the F (cid:48)
i using InvF results
in the clause C : Class(x, c) ← F1,1, . . . , Fk,nk . From Remark 15, the minimal
model of B ∪ H will be unchanged by replacing C (cid:48) with C. It follows that H
= {C} is an unstructured explanation for Class(a, c) that is computationally
equivalent to H (cid:48).

One or more correct structured explanations follow from an unstructured if the
conditions in Defn.14 hold. Constraints on the invented features can ensure this.

Remark 21 Deriving Structured Explanations from an Unstructured
Explanation. Let H = {C} be an unstructured explanation for a relational
example Class(a, c) given background knowledge B. W.l.o.g. from Defn. 18, C
is a clause Class(x, c) ← {F1(x), F2(x), . . . , Fn(x) s.t. each Fi is deﬁned in
B. Let InvF consist of k features F (cid:48)
i is uniquely de-
ﬁned by features in a block of a k-partition of F1, . . . , Fn. Let C (cid:48) be the clause
k(x). Then H (cid:48) = {C (cid:48)}∪InvF is a structured
Class(x, c) ← F (cid:48)

k s.t. each F (cid:48)

2(x), . . . , F (cid:48)

1, . . . , F (cid:48)

1(x), F (cid:48)

16

Ashwin Srinivasan, Lovekesh Vig, and Michael Bain

explanation for Class(a, c) that is computationally equivalent to B ∪{C}∪InvF .
This follows since the conditions (a)–(d) in Defn. 14 are trivially satisﬁed (condi-
tion (a) follows with θ being a simple renaming substitution; (b)–(c) follow since
there are no existential variables in the deﬁnitions of the F (cid:48)
i; and (d) since there
is a single clause deﬁnition for each feature in InvF ).

Remark 21 suggests a straightforward non-deterministic procedure to con-

struct a correct structured explanation (Fig. 7).

ConstructStruct(H, B, k) :

Given: An unstructured explanation H; background knowledge B; and a number k

(≥ 2)

Find: A structured explanation H (cid:48) that is computationally equivalent to H

1. Let H = ∀x(Class(x, c) ← Body)
2. Let F be the set of features in Body
3. Let Pk be a set s.t:

(a) Either Pk is a k-partition of F s.t. at least one block in Pk contains 2 or more

elements; or

(b) Or, Pk = ∅ (if no such k-partition exists)

4. If Pk = ∅ return ∅.
5. Otherwise:

(a) Let Pk consist of blocks b1, b2, . . . , bk
(b) For each block bi ∈ Pk:

i. Construct a new feature F (cid:48)

i with deﬁnition Ci = ∀x(F (cid:48)

i(x) ← Bodyi),

where Bodyi is the conjunction of the features in bi

(c) Let C0 = ∀x(Class(x, c) ← Body0) where Body0 is the conjunction of the

F (cid:48)

1, F (cid:48)
(d) Let H (cid:48) = (cid:83)k

2, . . . , F (cid:48)

k
i=0 Ck

6. Return H (cid:48).

Fig. 7. A procedure for obtaining a structured explanation from an unstructured ex-
planation, by inventing k features. The condition in Step 3 is not required by Remark
21, but prevents inventing features that are trivial rewrites of existing features (see
Defn. 18).

All explanations so far have only been required to explain a single relational
instance. We extend this to explain the prediction by a black-box classiﬁer of
relational instances that are “close” to each other. This is the requirement iden-
tiﬁed in LIME [33]. Readers familiar with the ILP literature will recognise the
task of ﬁnding such local explanation as as corresponding to either “Multiple
examples, single hypothesis clause” or “Multiple examples, multiple hypothe-
sis clauses” situations identiﬁed in [27] (depending on whether unstructured or
structured explanations are constructed).

Title Suppressed Due to Excessive Length

17

3.2 Local Explanations for a Black-Box Predictor

We are speciﬁcally interested in constructing explanations for the classiﬁcation
of an instances resulting from some (opaque) predictive model.

Deﬁnition 22 Explanation for a prediction by a model. Given a rela-
tional instance a ∈ X and a label c ∈ Y. Assume we have a set of d features
F. Given a predictive model N : (cid:60)d → Y, we will say H is an explanation for
N (F V (a)) = c if H is an explanation for e = Class(a, c).

We have just seen how to construct the most-speciﬁc feature-clause for e =
Class(a, c) from the set of active features for a. In fact, we will now want a little
bit more from an explanation. Following [33], we will seek explanations that are
not only consistent with a predictive model on an instance x, but be consistent
with predictions made by the predictive model in a local neighbourhood of x.

Deﬁnition 23 Neighbourhood. Given relational instance a ∈ X , and a set
of d features F, and given some (cid:15) ∈ (cid:60), we denote the neighbourhood of a as
N bd(a) = {x : x ∈ X and Dis(F V (a), F V (x)) ≤ (cid:15)}. Here Dis is some
appropriate distance measure deﬁned over d-dimensional vectors.

In practice the dimensionality d can be quite large, and since the standard Eu-
clidean distance is known to be problematic in high-dimensional spaces [1] we
will use an alternative measure.

Deﬁnition 24 Locally Consistent Explanations. Given relational instance
a ∈ X , and a predictive model N , let N (F V (a)) = c. We deﬁne the following
subsets of N bd(a): E+(a) = {b : b ∈ N bd(a) and N (F V (b)) = c)} and E−(a) =
{b : b ∈ N bd(a) and N (F V (b)) (cid:54)= c}. Then an explanation H for Class(a, c)
given B is a locally consistent explanation if: (1) for each a(cid:48) ∈ E+(a) H is an
explanation for Class(a(cid:48), c) (that is, B ∪ H |= Class(a(cid:48), c)); and (2) for each
a(cid:48) ∈ E−(a) B ∧ H ∧ ¬Class(a(cid:48), c) (cid:54)|= (cid:50).

Example 25 Locally consistent explanation for the trains problem. For
simplicity, we consider the situation where E− = ∅. Suppose we now know that
the local neighbourhood of the ﬁrst train (t1) in the left column of Fig. 2 only
contains the second train (t2) in that column. Let us assume that F consists just
of the functions deﬁned in Example 12. With those deﬁnitions, and a predictive
model N , let N (F V (t1)) = N (F V (t2)) (= East, say). Then E+(t1) = {t1, t2}.
The most-speciﬁc feature-clause for Class(t1, East) given F and B is:

C1 : Class(t1, East) ← F1(t1), F2(t1), F3(t1)

and for Class(t2, East) is:

C2 : Class(t2, East) ← F1(t2), F2(t2)

where the function deﬁnitions are as before. Then a locally consistent explanation
for Class(t1, East) is the least-general-generalisation (or Lgg) of C1 and C2:

Lgg(C1, C2) : ∀x(Class(x, East) ← F1(x), F2(x))

18

Ashwin Srinivasan, Lovekesh Vig, and Michael Bain

In general, E− (cid:54)= ∅, and results from ILP tell us that it may not be possible
to ﬁnd a single clause that is locally consistent. We next describe a simple,
qualitative form of Bayes Rule that combines likelihood of the data, with a
relevance-based prior preference over explanations.

it is useful before we proceed further to have a numerical measure of the

extent to which an explanation is locally consistent.

Deﬁnition 26 Fidelity. Let a ∈ X . Given a predictive model N , let N (F V (a)) =
c and E+(a), E−(a) as before. Let D = (E+, E−) and H be an explanation
for Class(a, c) given background knowledge B. Let: (1) AgreeP os(H) = {b :
b ∈ E+(a) and B ∧ H |= Class(b, c)}; and (2) AgreeN eg(H) = {b : b ∈
E−(a) and B ∪ H ∧ ¬Class(b, c) (cid:54)|= (cid:50)}. Then Fidelity(H|D, B) = FH,a =
|AgreeP os(H)|+|AgreeN eg(H)|
|E+(a)|+|E−(a)|

.

We note that AgreeP os(H) and AgreeN eg(H) are the same as true positives and
true negatives in the classiﬁcation literature, and so ﬁdelity is a localized form
of accuracy (in [33] the term “local ﬁdelity” is used for a localized form of error).
We note also that if a structured explanation is derived from an unstructured
one, then ﬁdelity will not change.

Remark 27 Structuring preserves ﬁdelity. Let H (cid:48) = {C (cid:48)} ∪ InvF be a
structured explanation derived as in Remark 21 from an unstructured explana-
tion H = {C} for a relational example Class(a, c), given deﬁnite-clauses B and
a predictor N s.t. N (F V (a)) = c. Let E+(a) and E−(a) denote relational exam-
ples obtained from the neighbourhood of a as before, and D = (E+(a), E−(a)).
Then, F idelity(H (cid:48)|D, B) = F idelity(H|D, B).

Let S be any ﬁnite set of ground Class/2 facts and M M (P ) be the mini-
mal model of a deﬁnite-clause program P . Since InvF is a deﬁnite-clause con-
taining deﬁnitions of invented features, S ∩ M M (B ∪ {C}) = S ∩ M M (B ∪
{C} ∪ InvF ). Now, AgreeP os(H) = {Class(a(cid:48), c) : Class(a(cid:48), c) ∈ (E+(a) ∩
M M (B ∪ H) and AgreeN eg(H) = {Class(a(cid:48), c) : Class(a(cid:48), c) ∈ (E−(a) −
(E−(a) ∩ M M (B ∪ H). Since E+(a) and E−(a) are ﬁnite set of ground Class/2
facts, and H = {C}, AgreeP os(H) = {Class(a(cid:48), c) : Class(a(cid:48), c) ∈ (E+(a) ∩
M M (B ∪ {C} ∪ InvF ) and AgreeN eg(H) = {Class(a(cid:48), c) : Class(a(cid:48), c) ∈
(E−(a) − (E−(a) ∩ M M (B ∪ {C} ∪ InvF ). From Remark 21, M M (B ∪ {C} ∪
InvF ) = M M (B ∪{C (cid:48)}∪InvF ). Since H (cid:48) = {C}∪InvF , it follows immediately
that AgreeP os(H) = AgreeP os(H (cid:48)) and AgreeN eg(H) = AgreeN eg(H (cid:48)) and
(cid:50)
therefore F idelity(H|D, B) = F idelity(H (cid:48)|D, B).

4 Selecting a Local Explanation

Given a relational instance a ∈ X , let the prediction of a by a DRM N be
c. But, for a deﬁnition of a neighbourhood, there may be several explanations
for Class(a, c) with the same maximal ﬁdelity. How then should we select a

Title Suppressed Due to Excessive Length

19

single explanation? Provided we have some reasonable way of specifying prior-
preferences, Bayes rule trades-oﬀ the ﬁt to the data (likelihood) against prior
preference (LIME’s minimisation of the sum of a loss and a regularisation term
can be seen as implementing a form of Bayesian selection [33]).

A general setting for selecting amongst logical formulae is provided by la-
belled deductive systems (LDS: see [9]), in which logical formulae are extended
with labels, with an associated algebra. For our purpose, it is suﬃcient simply
to consider the comparison of labelled explanations.

Deﬁnition 28 Labelled Explanation. Given a relational example e = Class(a, c),
and background knowledge B, α : H is a labelled explanation for e given B if: (a)
H is an explanation for e given B; and (b) α is a element of a partially-ordered
set of ground ﬁrst-order terms ∆. ∆ consists of annotations of all explanations
for e given B.

A comparison of labelled explanations follows simply from the partial order-
ing on the labels. That is, α : H1 (cid:22) β : H2 iﬀ α (cid:22) β. Here we will take the label
of an explanation H to be a pair (cid:104)LH , PH (cid:105), which allows several diﬀerent kinds
of comparisons, given background knowledge B and data D (see Deﬁnition 26):

Quantitative. This is appropriate when both LH and PH are on an inter-
val scale (that is, numeric values). Examples are: (a) the usual Bayesian
comparison, using LH = P (D|H, B) and PH = P (H|B); and (cid:104)LH1, PH1(cid:105) :
H1 (cid:22) (cid:104)LH2, PH2 (cid:105) : H2 iﬀ logLH1 + logPH1 ≤ logLH2 + logPH2; (b) Good’s
explicativity (Chapter 23 of [11], which uses the same LH , PH , but uses the
function logLH + γlogPH with 0 < γ < 1; and (c) Likelihood-based, using
LH = P (D|H, B) and PH is the uniform distribution.

Qualitative. Here both LH and PH are both on an ordinal scale (that is,
only comparisons of values are possible). Examples are: (a) The qualitative
Bayesian comparison in the manner proposed by [4]. With some abuse of no-
tation, (cid:104)LH1, PH1 (cid:105) : H1 (cid:22) (cid:104)LH1 , PH1 (cid:105) : H2 iﬀ LH1 (cid:22) LH2 and PH1 (cid:22) PH2.
If it is not the case that (cid:104)LH1, PH1 (cid:105) : H1 (cid:22) (cid:104)LH2 , PH2(cid:105) : H2 or (cid:104)LH2, PH2(cid:105) :
H2 (cid:22) (cid:104)LH1, PH1(cid:105) : H1, then the labelled explanations are not comparable;
and (b) A dictionary-ordering, in which (cid:104)LH1, PH1(cid:105) : H1 (cid:22) (cid:104)LH1, PH1(cid:105) : H2
iﬀ LH1 ≺ LH2, or LH1 = LH2 and PH1 (cid:22) PH2.8

Semi-Quantitative. Here, one of LH or PH is on an interval scale, and the
other is on an ordinal scale. The qualitative comparisons above can be
adapted to this, by replacing (cid:22) and ≺ with ≤ and < for the numeric quantity.

In this paper, we will use the semi-quantitative dictionary ordering with
LH = F idelity(H|D, B) and PH is an ordinal-valued prior based on an as-
sessment of relevance. Using the semi-quantitative setting and the dictionary
ordering has some advantages:

8 This may not yield the same results as the qualitative Bayesian comparison above.
Diﬀerences arise when LH1 ≺ LH2 but PH2 (cid:22) PH1 . Under the dictionary-ordering
H2 would be preferred, but the qualitative Bayesian approach would ﬁnd H1 and
H2 incomparable.

20

Ashwin Srinivasan, Lovekesh Vig, and Michael Bain

(a) Quantitative selection based on a Bayesian score requires a deﬁnition of both
P (D|H, B) and P (H|B). While the ﬁrst can be obtained easily enough, it is
not obvious how to specify a prior distribution over explanations. The usual
approach of using a mathematically convenient function like 2−|H|, where
|H| is some measure of the size of H, may not be appropriate translation of
prior assessment of relevance of explanations; and

(b) A qualitative Bayesian approach as deﬁned above usually ends up with many
incomparable explanations. The dictionary ordering decomposes the task of
identifying explanations into two parts: the ﬁrst part that maximises ﬁdelity,
and the second part that maximises the prior amongst maximal ﬁdelity ex-
planations. Under some circumstances (see the Appendix), maximising ﬁ-
delity is equivalent to maximising log likelihood. In those cases, the dictio-
nary ordering uses the prior to select amongst maximum likelihood explana-
tions. Usefully, there is also an implementation beneﬁt that follows from the
result in Remark 27: since structuring does not alter the ﬁdelity, the ﬁrst
part can simply examine unstructured explanations.

We turn now to prior information PH that captures some aspects of what

constitutes a comprehensible explanation.

4.1 A Relevance-Based Prior

In [39] the authors investigate the utility of including an expert assessment of
the relevance of relations included in the background knowledge.

Example 29 Relevance information in the trains problem. Suppose the
background knowledge B for the trains problem contains deﬁnitions of predicates
like Has Car/2, Short/1, Closed/1, W heels/2 and Load/2. For the problem of
classifying trains as east-bound or west-bound, let us assume we are also given
domain knowledge in the form of the relevance level of (sets) of predicates as
follows: r1 : {W heels/2, Load/2}, r2 : {Short/1, Closed/1}, and r1 ≺ r2. That
is, W heels and Load are less relevant to the problem than Short and Closed.

We note that this is diﬀerent to the notion of logical relevance of features
(deﬁned in terms of the entailment relation, |=). Here, we are concerned with
domain-relevance of predicates used to deﬁne those features. This latter form
of domain-speciﬁc relevance information can also form the basis of a preference
ordering over explanations. Here is the view of the domain expert involved in
[39]:9

I think it is reasonable to argue that [a hypothesis using] more relevant prior
background knowledge is more probable. I think that what makes hypotheses
more probable is also a function of whether the predicates used are related to
each other. Often you see hypotheses that seem to mix apples and oranges
together, which seems to make little sense. Though of course this mixing of

9 R.D. King: personal communication

Title Suppressed Due to Excessive Length

21

predicates may be because the ML system is trying to express something not
easily expressible with the given background predicates.

This suggests that relevance information can constitute an important source of
prior knowledge. One route by which this is exploited by ILP systems is in the
form of search constraints (“hypotheses that do not contain oxygens connected to
hetero-aromatic rings are irrelevant”), or, as in the case of [39], in the incremental
construction of hypotheses. Our interest here is to extend this use of relevance
to selection amongst hypotheses, by devising a relevance-based partial ordering
over hypotheses.10

Deﬁnition 30 Relevance-assignments and orderings. We assume that for
some set of predicates P in background knowledge B, we have domain-speciﬁc
relevance labels drawn from a set R. A relevance assignment is a function R :
P → R. We assume that there is domain-knowledge in the form of a total
ordering ≺r over the elements of R. Then, for a, b ∈ R, a (cid:22)r b iﬀ a ≺r b or
a = b. We will call ≺r a relevance ordering.

A relevance ordering naturally results in the concept of ordered intervals: [a, b]
is an ordered relevance-interval (or simply, a relevance-interval) if a, b ∈ R and
a (cid:22)r b. It is not hard to see that with a ﬁnite set of relevance labels R, the set
of relevance intervals is partially ordered. That is, [a, b] (cid:22) [c, d] iﬀ a (cid:22)r c and
b (cid:22)r d. In fact, the following slightly more general ordering will be more useful
for us.

Deﬁnition 31 Ordering over sets of relevance-intervals Let S and S(cid:48) be
sets of relevance intervals. Then S (cid:22)i S(cid:48) iﬀ for every interval [a, b] in S there
exists at least one interval [c, d] in S(cid:48) s.t. [a, b] (cid:22) [c, d]. That is, a (cid:22)r c and
b (cid:22)r d.

We now construct, in stages, the relevance of an explanation.

Deﬁnition 32 Relevance of features. Given background knowledge B, let ≺r
be a relevance ordering over a set of relevance labels R, and let R : P → R be
a relevance assignment for some subset P of B. Let ∀x(F (x) ← Cp(x)) be the
feature-deﬁnition for F/1 in which Cp(x) is a conjunction containing predicates
from P only. Then the relevance of the feature F is Relev(F ) = [l, h], where l is
the minimum relevance of predicates in Cp(x) according to ≺r and R, and h is
maximum relevance of predicates in Cp(x) according to ≺r and R.

Deﬁnition 33 Relevance of feature-clauses. Let F be a set of features. Let
C be a feature-clause. W.l.o.g. let the features in C be {F1, F2, . . . , Fk} where
the Fi ∈ F. Let Relev(Fi) = [li, hi]. Then Relev(C) = {[l∗, h∗]} where l∗ =
min(l1, l2, . . . , lk) and h∗ = max(h1, h2, . . . , hk).

10 We will often reuse the generic symbols (cid:22) and ≺ to denote partial- and total-

orderings. The context will make it clear which sets these relations refer to.

22

Ashwin Srinivasan, Lovekesh Vig, and Michael Bain

The need to have Relev(C) as a set will become apparent shortly. The rele-
vance of explanations is constructed from the relevance of the feature-clauses in
the explanation.

Deﬁnition 34 Relevance of an explanation Given background knowledge B,
let H be an explanation for Class(a, c) containing the clause C : ∀x(Class(x, c) ←
Body). If H is an unstructured explanation then Relev(H) = Relev(C). Other-
wise, if H is a structured explanation, then Relev(H) = (cid:83)

Fi∈Body Relev(U F C(Fi)).

It is interesting that although a pair of structured explanations may unfold
to the same unstructured explanation (and therefore have the same ﬁdelity),
their relevance may not be the same. Intuitively, structuring any unstructured
explanation will split the relevance-interval of the corresponding feature-clause
into a set of intervals (see Deﬁnition 34). In a “good structuring” each interval
in this set will be “narrower” than the unstructured relevance-interval and will
therefore be preferred under the relevance ordering (Deﬁnition 31).

Example 35 Comparing relevance of explanations in the trains prob-
lem. Suppose we are given a set of features F = {F2, F3, F4, F9} and following
feature-deﬁnitions (omitting quantiﬁers for simplicity):

F2(x) ← Has Car(x, y), Short(y)
F3(x) ← Has Car(x, y), Closed(y)
F4(x) ← Has Car(x, y), W heels(y, 3)
F9(x) ← Has Car(x, y), Load(y, triangle)

Let as assume we are given a set of relevance labels R = {r1, r2}, with r1 ≺ r2.
Let us further assume the following relevance-assignment: {(W heels/1, r1), (Load/2, r1),
(Short/1, r2), (Closed/1, r2)}. That is Relev(F2) = Relev(F3) = [r2, r2], Relev(F4) =
Relev(F9) = [r1, r1]. Suppose we have the structured explanation H1:

Class(x, East) ← F1,1(x), F1,2(x)
F1,1(x) ← F2(x), F3(x)
F1,2(x) ← F4(x), F9(x)

Clearly, F1,1, F1,2 (cid:54)∈ F, and are therefore “invented” features. Let F2 = {F1,1, F1,2}.
Then, Relev(H1) = RH1 = (cid:83)
{[r2, r2]}, Relev(F C(F1,2)) = {[r1, r1]} and RH1 = {[r1, r1], [r2, r2]}.

Relev(F C(F )) ∪ ∅. Now Relev(F C(F1,1)) =

F ∈F2

On the other hand, for the following explanation H2:

Class(x, East) ← F1,3(x), F1,4
F1,2(x) ← F3(x), F4(x)
F1,4(x) ← F2(x), F9(x)

RH2 = {[r1, r2]}. From Defn. 31 RH2 (cid:22)i RH1 . Note: H1 and H2 both unfold
to the unstructured explanation H: Class(x, East) ← F2(x), F3(x), F4(x), F9(x)
for which RH = {[r1, r2]}.

Title Suppressed Due to Excessive Length

23

Thus, although H1,2 both unfold to H, it is possible that a selection criterion

that takes relevance into account may prefer H1 over H2 and H.

Remark 36 Structuring can increase relevance. Let H = {C} be an un-
structured explanation for Class(a, c), and let H (cid:48) be a structured explanation
containing a clause C (cid:48)
: Class(x, c) ← Body that unfolds to C. Let PH =
Relev(H) and PH (cid:48) = Relev(H (cid:48)). Then PH (cid:22) PH (cid:48).
Let C contain the features {F1, . . . , Fl}, where Relev(Fi) = [li, hi] Since H is
an unstructured explanation, Relev(H) = {[l∗, h∗]}, where l∗ = min(l1, . . . , lk)
1, . . . , F (cid:48)
and h∗ = max(h1, . . . , hk). Let C (cid:48) contain the invented features {F (cid:48)
j}.
i unfolds to a clause containing some subset S(cid:48)
Since C (cid:48) unfolds to C, each F (cid:48)
i of
{F1, . . . , Fk}, and (cid:83)j
i = {F1, . . . , Fk}. W.l.o.g. let h∗ = hk. By the con-
straint imposed on structured explanations, there must be at least one invented
feature F (cid:48)
m)) =
m, h(cid:48)
[l(cid:48)
m]. Since
m, h(cid:48)
[l(cid:48)

m]. Clearly, l∗ (cid:22) l(cid:48)
m] ∈ Relev(H (cid:48)), it follows from Defn. 31 that Relev(H) (cid:22) Relev(H (cid:48)).

m that unfolds to a clause containing Fk. Let Relev(U F C(F (cid:48)

m, and therefore [l∗, h∗] (cid:22) [l(cid:48)

m and h∗ = h(cid:48)

i=1 S(cid:48)

m, h(cid:48)

4.2 Implementation

We ﬁnally have the pieces to deﬁne a label for an explanation: each explanation
H will now have the label (cid:104)LH , PH (cid:105), where LH = F idelity(H|D, B) and PH =
Relev(H). Using a dictionary-ordering to compare labelled explanations allows
us to decompose the task of identifying explanations into two parts: the ﬁrst
that maximises ﬁdelity and the second that maximises the relevance. Further,
as we have already seen (Remark 27), structuring cannot increase ﬁdelity, but
can increase relevance (Remark 36). Therefore, with a dictionary ordering on
labels, it suﬃces to search ﬁrst over the space of unstructured explanations, and
then over the space of structured explanations that unfold to the unstructured
explanations with maximal ﬁdelity. Figure 8 extends the previous procedure
of ﬁnding an unstructured explanation (Fig. 6) to obtain the highest-ﬁdelity
unstructured explanation.

Figure 9 extends ConstructStruct in Fig. 7 to return an explanation with
higher-relevance than an unstructured H, if one exists. It is not hard to see that
if Pk = ∅ then Pk+1 = ∅. Therefore, it is only needed to call ConstructExpl
with k = 2, 3, . . . until Pk = ∅. In experiments in this paper, we will adopt the
even simpler strategy of only considering k = 2. That is, we will only consider
2-partitions of the set of features constituting the unstructured explanation H
(in eﬀect, seeking structured explanations with higher relevance than H, but
using the minimum number of invented features). The structured explanations
in Example 19 are examples of structures that can be obtained with k = 2.

Together, ConstructU nstruct and ConstructExpl are used to identify a local

explanation for a relational instance e (Fig.10).

5 Empirical Evaluation

In this section, we evaluate empirically the predictive performance of DRMs and
the explanatory models derived from them. Our aim is investigate the following:

24

Ashwin Srinivasan, Lovekesh Vig, and Michael Bain

ConstructU nstruct(e, B, F, E+, E−) :

Given: A relational example e; background knowledge B; a set of features F with

deﬁnitions in B; and E+, E− as deﬁned in Defn. 24.

Find: A maximal ﬁdelity unstructured explanation H s.t. B ∪ H |= e,

1. Let e = Class(a, c)
2. Let a(cid:48) = F V (a)
3. Let F (cid:48) be the set of features that map to T RU E in a(cid:48)
4. Let D = (E+, E−)
5. Let H be the subset-lattice of F (cid:48)
6. Let F (cid:48) be any element in H s.t.

– Body is the conjunction of features in F (cid:48);
– C = ∀x(Class(x, c) ← Body);
– L = F idelity({C}|D, B); and
– There is no other element F (cid:48)(cid:48) in H s.t. F idelity({Clause(a, F (cid:48)(cid:48))}|D, B) > L.

7. return {C}

Fig. 8. A procedure for identifying an unstructured explanation with maximal ﬁdelity.
In practice, we will need to extend this procedure to return all unstructured explana-
tions with maximal ﬁdelity.

Prediction. We conduct the following experiment:

Expt. 1: Accuracy. Will a DRM constructed using randomly drawn fea-
tures from a depth-limited mode language have good predictive perfor-
mance?

Explanation. We conduct the following experiments:

Expt. 2: Fidelity. Can we construct a local symbolic explanations for an
instance with high ﬁdelity to local predictions made by the DRM?
Expt. 3: Relevance. Does incorporating s prior preference based on rele-

vance have any eﬀect?

Some clariﬁcations are necessary here: (a) By randomly drawn features in Expt.
1, we mean the rejection-sampling method described in Section 2.3; (2) By a
local symbolic explanation in Expt. 2 we mean the use of a graph-search that
returns the unstructured explanation with the highest ﬁdelity, described in Sec-
tion 3.2; and (3) By prior-preference in Expt. 3, we mean the relevance-based
ordering over structured or unstructured explanations as deﬁned in Defn. 34. In
Expt. 3, we conﬁne ourselves to whether the use of the preference can change
the explanation returned (either from a unstructured to a structured one, or
from one unstructured explanation to another). We note that incorporation of
prior preference obtained from a human expert is still not suﬃcient to ensure
comprehensibility of explanations by the expert. Evidence for this requires re-
sults in the form of cross-comparisons on the use of prior expert preference on
explanations against expert comprehensibility of explanations. However, this is
outside the scope of this paper.

Title Suppressed Due to Excessive Length

25

ConstructExpl(H, B, k) :

Given: An unstructured explanation H; background knowledge B; and a number k

(≥ 2)

Find: An explanation H (cid:48) s.t. Relev(H|B) (cid:22) Relev(H (cid:48)|B).

1. Let H = ∀x(Class(x, c) ← Body)
2. Let Relev(H) = RH = {[α, γ]}
3. Let F be the set of features in Body
4. Let Pk be a set s.t:

(a) Either Pk is a k-partition of F that satisﬁes:

– At least one block in Pk contains 2 or more elements; and
– There is at least one block in Pk whose elements have a minimum relevance

β and maximum relevance γ such that α ≺r β (cid:22)r γ

(b) Or, Pk = ∅ (if no such k-partition exists)

5. If Pk = ∅ return H.
6. Otherwise:

(a) Let Pk consist of blocks b1, b2, . . . , bk
(b) For each block bi ∈ Pk:

i. Construct a new feature F (cid:48)

i with deﬁnition Ci = ∀x(F (cid:48)

i(x) ← Bodyi),

where Bodyi is the conjunction of the features in bi

(c) Let C0 = ∀x(Class(x, c) ← Body0) where Body0 is the conjunction of the

F (cid:48)

1, F (cid:48)
(d) Let H (cid:48) = (cid:83)k

2, . . . , F (cid:48)

k
i=0 Ck

7. Return H (cid:48).

Fig. 9. A procedure for obtaining a structured explanation that is at least as relevant as
an unstructured explanation H. The structured explanation is obtained by inventing
k features, the deﬁnition of at least one of which has a higher relevance than the
unstructured explanation.

5.1 Materials

Data We report results from experiments conducted using 7 well-studied real
world problems from the ILP literature. These are: Mutagenesis [16]; Carcinogen-
esis [17]; DssTox [30]; and 4 datasets arising from the comparison of Alzheimer’s
drugs denoted here as Amine, Choline, Scop and T oxic [40]. Each of these
have shown to beneﬁt from the use of a ﬁrst-order representation, and domain-
knowledge but there is still room for improvement in predictive accuracies. Im-
portantly, for each dataset, we also have access to domain-information about the
relevance of predicates for the classiﬁcation task considered.

Of these datasets, the ﬁrst three (Mut188–DssTox) are predominantly rela-
tional in nature, with data in the form of the 2-d structure of the molecules
(the atom and bond structure), which can be of varying sizes, and diverse. Some
additional bulk properties of entire molecules obtained or estimated from this
structure are also available. The Alzheimer datasets (Amine–Toxic) are best
thought of as being quasi-relational. The molecules have a ﬁxed template, but

26

Ashwin Srinivasan, Lovekesh Vig, and Michael Bain

Fig. 10. More details on the testing component in Fig. 1. E is the set of relational
training instances, a is a test instance, and M is a DRM. FV(N bd(a)) denotes the
feature-vectors for relational in E that are within the neighbourhood of a. Pred(N bd(a))
are the class predictions of instances in N bd(a) by the DRM. N etEval evaluates the
model M on input data; F Eval evaluates relational instances and returns their feature-
vectors; and REval constructs relational examples. We assume that F Eval has access
to the feature-deﬁnitions found during the training stage.

Title Suppressed Due to Excessive Length

27

vary in number and kinds of substitutions made for positions on the template.
A ﬁrst-order representation has still been found to be useful, since it allows ex-
pressing concepts about the existence of one or more substitutions and their
properties. The datasets range in size from a few hundred (relational) instances
to a few thousands. This is extremely modest by the usual data requirements
for deep learning. We refer the reader to the references cited for details of the
domain-knowledge used for each problem.

Background Knowledge For the relational datasets (Mut188–DssTox), back-
ground knowledge is in the form of general chemical knowledge of ring-structures
and some functional groups. Background-knowledge contains deﬁnitions used
for concepts like: alcohols, aldehydes, halides, amides, amines, acids. esters,
ethers, imines, ketones, nitro groups, hydrogen donors and acceptors, hydropho-
bic groups, positive- and negatively-charged groups, aromatic rings and non-
aromatic rings, hetero-rings, 5- and 6-carbon rings and so on. These have been
used in structure-activity applications of ILP before [18, 19]. However, we note
that none of these deﬁnitions are speciﬁcally designed for the tasks here. In ad-
dition, for Mut188 and Canc330, there are some bulk properties of the molecules
that are available. For the Alzheimer problems (Amine–Toxic) domain knowl-
edge consists of properties of the substituents in terms of some standard chemical
measures like size, polarity, number of hydrogen donors and acceptors and so on.
Predicates are also available to compare these values across substitutions. Again
we refer the reader to the relevant ILP literature for more details.

In addition to the domain-predicates just described, we will also have infor-
mation in the form of a relevance ordering as described in [39]. That paper only
refers to the Mut188 and Canc330 datasets. The same information is obtained
from the domain-expert involved in that paper for the other problems in this
paper. A complete description of the relevance assignment of predicates for each
problem is in Appendix 7

Algorithms and Machines Random features were constructed on an Intel
Core i7 laptop computer, using VMware virtual machine running Fedora 13,
with an allocation of 2GB for the virtual machine. The Prolog compiler used was
Yap. Feature-construction uses the utilities provided by the Aleph ILP system
[38] for constructing most-speciﬁc clauses in a depth-bounded mode language,
and for drawing clauses subsuming such most-speciﬁc clauses. No use is made of
any of the search procedures within Aleph. The deep networks were constructed
using the Keras library with Theano as the backend, and were trained using an
NVIDIA K-40 GPU card.

5.2 Methods

The methods used for each of the experiments are straightforward

Prediction. Experiment 1 is concerned solely with the predictive performance

of the DRM.

28

Ashwin Srinivasan, Lovekesh Vig, and Michael Bain

For each dataset:
1. Obtain a set of random features F;
2. Compute the Boolean-value for each F ∈ F for the data;
3. Construct a DRM N using training data and obtain its predictions

on test instances;

4. Estimate the overall predictive performance of M (Expt. 1)
Explanation. Experiments 2 and 3 are concerned solely with the explanatory

performance of the local symbolic models.

For each dataset:
1. Construct a DRM using training data
2. For each test instance, obtain the local symbolic unstructured expla-

nation(s) with the highest ﬁdelity;

3. Estimate the overall ﬁdelity of the symbolic explanations (Expt. 2)
4. Estimate the eﬀect of using the relevance-based prior in hypothesis

selection (Expt. 3)

Some clariﬁcations are necessary at this point:

– We use a straightforward Deep Neural Network (DNN) architecture. There
are multiple, fully connected feedforward layers of rectiﬁed linear (ReLU)
units followed by Dropout for regularization (see [12] for a description of
these ideas). The model weights were initialized with a Gaussian distribution.
The number of layers, number of units for each layer, the optimizers, and
other training hyperparameters such as learning rate, were determined via
a validation set, which is part of the training data. Since the data is limited
for the datasets under consideration, after obtaining the model which yields
the best validation score, the chosen model is then retrained on the complete
training set (this includes the validation set) until the training loss exceeds
the training loss obtained for the chosen model during validation.

– We use the Subtle algorithm [2] to perform the subsumption-equivalence test

used to determine redundant features.

– For all the datasets, 10-fold cross-validated estimates of the predictive perfor-
mance using ILP methods are available in the ILP literature for comparison.
We use the same approach. This requires constructing DRMs separately for
each of the cross-validation training sets, and testing them on the corre-
sponding test sets to obtain estimates of the predictive accuracy;

– We use the mode-language and depth constraints for the datasets that have
been used previously in the ILP literature. For the construction of features,
the rejection-sampler performs at most 10, 000 draws;

– We take explanatory ﬁdelity to mean the probability that the prediction
made by H on a randomly drawn instance agrees with the prediction made
by the corresponding DRM. We use the same 10-fold cross-validation strat-
egy for estimating this probability (for eﬃciency, we use the same splits as
those used to estimate predictive accuracy). For a given train-test split T ri
and T ei, we proceed as follows. We obtain a DRM Ni using T ri. We start
with a ﬁdelity count of 0. For each instance x(cid:48)
j in T ei we obtain the class

Title Suppressed Due to Excessive Length

29

j, and corresponding neighbourhood of x(cid:48)

predicted by Ni for x(cid:48)
j in the train-
ing set T ri. The neighbourhood is partitioned into δ+(x(cid:48)
j) using
the predictions by Ni and high-ﬁdelity unstructured explanation(s) Hij are
obtained. This is done using a beam-search over the lattice described in Figs.
6, 8. The size of the beam is 5 (that is, the top 5 unstructured explanations
are returned).

j) and δ−(x(cid:48)

– Assessments of ﬁdelity require the deﬁnition of a neigbourhood. For each
instance x(cid:48)
j in T ei we say any instance x ∈ T ri is in the neighbourhood
of x(cid:48) iﬀ F V (x(cid:48)) and F V (x) diﬀer in no more than k features. This is just
the Hamming distance between the pair of Boolean vectors. That is, the
neighbourhood of a test instance x(cid:48) consists of training instances x’s whose
feature-vector representation are within a k-bit Hamming distance of x(cid:48). We
will consider k = 5 and k = 10 in the experiments.

– In all cases, estimates are obtained using the same 10-fold cross-validation
splits reported in the ILP literature for the datasets used. This will allow a
cross-comparison of the results from Expt.1 to those reports.

5.3 Results

Results of the empirical evaluation are tabulated in Fig. 11, and Fig. 12. Some
supplementary results are in Fig. 13. The principal observations that can be made
from the main tabulations in Figs. 11,12 are these: (1) The predictive accuracy
of the DRMs clearly compare favourably to the best reports in the literature; (2)
High-ﬁdelity symbolic explanations can be obtained for local predictions made
by the DRM; and (3) In 6 of the 7 problems, introducing a prior-preference based
on relevance does aﬀect the selection of explanations.
Together, the results provide evidence for the following:

(a) A deep relational machine (DRM) equipped with domain knowledge and
randomly drawn ﬁrst-order features can construct good predictive models
using (by deep-learning standards) very few data instances;

(b) It is possible to extract symbolic explanations for the prediction made by
the DRM for a randomly drawn (new) instance. The explanations are largely
consistent with the predictions of the network for that instance and its near-
neighbours in the training data examined before by the network; and
(c) It is possible to incorporate domain-knowledge in the form of expert assess-
ment of relevance of background predicates into a preference ordering for
selecting amongst explanations.

Quantitative assessments of the results are also possible, with the usual cau-
tions associated with small numbers and multiple comparisons. The appropriate
test for comparing the predictive accuracy of the DRM is the Wilcoxon signed-
rank test, with the null hypothesis that the DRM’s accuracy is the same as the
method being compared. This yields P -values of < 0.05 for the comparisons
against OptILP and Stat (we omit a comparison against the DRM in [21], due
to lack of data). If the inclusion of relevance does not make a diﬀerence to se-
lecting an explanation, we would expect values of 0.0 in Fig. 12(b). It is evident

30

Ashwin Srinivasan, Lovekesh Vig, and Michael Bain

that the observed values are clearly not 0, for all cases except DssT ox. The ex-
ception is unsurprising, since all predicates used for this problem have the same
relevance (see Appendix 7).

The results obtained are presented in some more detail in Fig. 13. From this

we observe:

(a) The neighbourhood size aﬀects the local explanations constructed. In gen-
eral, the fewer the instances in the local neighbourhood, the lesser the con-
straints imposed on the explanations. This leads to smaller explanations
(fewer literals), and higher ﬁdelity;

(b) Although the networks can often contain 1000’s of input features (the exact
numbers for each problem are not shown here, but range from about 2000
(Amine) to 7000 (Mut188)), a high-ﬁdelity locally-consistent explanation
may only contain a few (“active”) features. This is what makes it possible to
extract relatively compact explanations even for large networks (recall that
we are not attempting to extract a complete symbolic model for the entire
network);

(c) There are clear diﬀerences between the role of the relevance information
amongst the datasets (signiﬁcant eﬀect in Mut188 and Canc330; minor ef-
fect in the Alzheimer datasets; and no eﬀect in DssTox). We conjecture the
following condition for relevance to have an eﬀect on selection:

The larger the range of the relevance assignment, the more likely it is
that relevance will play a role in selection of explanations.

For the datasets here, the range of the relevance assignment has 1 value for
DssTox; 2 values for the Alzheimer’s datasets; and 4 values each for Mut188
and Canc330. The corresponding proportions of explanations where rele-
vance plays a role in selection are: 0.0 (DssTox); 0.29 (Alzheimer datasets);
and 0.80 (Mut188 and Canc330).

(d) Structured explanations only appear to play role for larger neighbourhoods.
We suggest this is not so much to do with the size of the neighbourhood, as
to the corresponding increasing in the size of the explanations. In general,
larger explanations (those with more features) are likely to beneﬁt from
structuring.

Finally, since explanations are generated “on-demand”, it is impractical to
show the explanations for all test instances. A snapshot is nevertheless useful,
and is included in Appendix C. The example is from the Canc330 problem, and
shows 4 possible explanations. The unstructured explanation shown has perfect
ﬁdelity. The 3 alternate structured explanations derived from this unstructured
explanation have the same ﬁdelity (as expected), but higher relevance. In eﬀect,
what the structuring achieves here is to group predicates with the Has property
relation, which has high-relevance; and separate out predicates that mix predi-
cates with low- and high-relevance. Some measure of the explanatory convenience
provided by the symbolic model is apparent if the reader keeps in mind that the
corresponding prediction by the DRM is based on around 2000 input features,
about 400 of which are equal to 1 for the test instance. It is interesting that the

Title Suppressed Due to Excessive Length

31

Accuracy

[41]

DRM
[21]

DRM
(here)

Problem OptILP

Stat
[34]
M ut188 0.88(0.02) 0.85(0.05) 0.90(0.06) 0.91(0.06)
Canc330 0.58(0.03) 0.60(0.02)
0.68(0.03)
DssT ox 0.73(0.02) 0.72(0.01) 0.66(0.02) 0.70(06)
0.89(0.04)
Amine
0.80(0.02) 0.81(0.00)
0.81(0.03)
Choline 0.77(0.01) 0.74(0.00)
0.82(0.06)
0.67(0.02) 0.72(0.02)
Scop
0.93(0.03)
0.87(0.01) 0.84(0.01)
T oxic

–
–
–
–

–

Fig. 11. Experiment 1. Estimated predictive accuracies of DRMs against some of the
best reported performances in the ILP literature. All estimates are from the same
10-fold cross-validation splits in the reports cited.

Problem Fidelity
M ut18
0.99(0.01)
Canc330 0.99(0.01)
DssT ox 0.87(0.03)
Amine
0.98(0.01)
Choline 0.89(0.01)
0.89(0.02)
Scop
0.94(0.02)
T oxic

Problem (cid:104)LH , RH (cid:105) > (cid:104)LH , ∅(cid:105)
M ut188
Canc330
DssT ox
Amine
Choline
Scop
T oxic

0.82(0.08)
0.77(0.07)
0.00(0.00)
0.38(0.15)
0.27(0.03)
0.27(0.06)
0.24(0.14)

(a)

(b)

Fig. 12. Experiments 2 and 3. (a) Mean ﬁdelity of the explanatory (symbolic) model
to the predictive (neural) model. The number tabulated is a 10-fold cross-validation
estimate of the faithfullness of the symbolic model to a DRM’s prediction assessed over
the neighbourhood of a test instance. The entries are for the smallest neighbourhood
(H5: see the “Methods” section for how this is computed). The number in parentheses
are estimates of standard deviations. (b) Relative frequency estimates of how often
we can expect incorporation of a relevance-based prior to aﬀect the selection of ex-
planations. The tabulation is the proportion of explanations for which a Bayes label
using both ﬁdelity and relevance ((cid:104)LH , RH (cid:105)) is better than one that uses ﬁdelity only
((cid:104)LH , ∅(cid:105)). The explanations are for the H5 neighbourhood. Again, the estimates are
from the same 10-fold cross-validation splits used elsewhere.

32

Ashwin Srinivasan, Lovekesh Vig, and Michael Bain

Problem Nbd. Size
H5 H10
M ut188 5(1) 20(3)
Canc330 3(1) 27(4)
DssT ox 19(3) 83(10)
Amine
9(1) 23(20
Choline 14(1) 53(3)
8(1) 26(2)
Scop
12(2) 42(2)
T oxic

(a)

Problem Expl. Size
H5 H10
M ut188 1(1) 2(1)
Canc330 1(1) 3(1)
DssT ox 3(1) 4(1)
Amine
2(1) 2(1)
Choline 2(1) 3(1)
2(1) 2(1)
Scop
2(1) 3(1)
T oxic

Problem

Fidelity

H10

H5
M ut188 0.99(0.01) 0.96(0.01)
Canc330 0.99(0.01) 0.92(0.02)
DssT ox 0.87(0.03) 0.85(0.02)
0.98(0.01) 0.97(0.01)
Amine
Choline 0.89(0.01) 0.83(0.01)
0.89(0.02) 0.86(0.02)
Scop
0.94(0.02) 0.90(0.03)
T oxic

(b)

Problem Struc. Expl.

H5 H10
0.04
M ut188 0.00
0.36
Canc330 0.02
0.00
DssT ox 0.00
Amine
0.00
0.00
Choline 0.00 0 00
0.00
Scop
0.00
T oxic

0.00
0.00

(c)

(d)

Fig. 13. Eﬀect of the neighbourhood. (a) Average numbers of neighbouring instances
for local explanations (rounded up); (b) Average ﬁdelity of local explanations; (c)
Average number of literals in the maximal-ﬁdelity explanations (rounded up); and (d)
Average proportion of structured explanations. All numbers are estimates obtained
from 10-fold cross-validation, and the number in parentheses is an estimate of the
standard deviation.

Title Suppressed Due to Excessive Length

33

domain-expert could correctly identify the class of the example when shown the
symbolic explanation.

6 Other Related Work

We have already noted the key reference to LIME and to reports in the ILP
literature of immediate relevance to the work in this paper. Here we comment
on other related work. The landmark work on structured induction of symbolic
models is that of Shapiro [36]. There, structuring was top-down with machine
learning being used to learn sub-concepts identiﬁed by a domain-expert. The
structuring is therefore hand-crafted, and with a suﬃciently well-developed tool,
a domain-expert can, in principle, invoke a machine learning procedure to con-
struct sub-concepts using examples he or she provides. The technique was shown
to yield more compact models than an unstructured approach on two large-scale
chess problems, using decision-trees induced for sub-concepts.

Clearly the principal diﬃculty in the Shapiro-style of structured induction is
the requirement for human intervention at the structuring step. The following
notable eﬀorts in ILP or closely related areas, have been directed at learning
structured theories automatically:

– Inverse resolution, especially in the Duce system [29] was explicitly aimed at
learning structured sets of rules in propositional logic. The sub-concepts are
constructed bottom-up;

– Function decomposition, using HINT [44], which learns hierarchies of con-
cepts using automatic top-down function decomposition of propositional con-
cepts;

– First-order theories with exceptions, using the GCWS approach [23], which
automatically constructs hiearchical concepts. Structuring is restricted to
learning exceptions to concepts learned at a higher level of the hierarchy;
– First-order tree-learning: an example is the TILDE system: [3]. In this, the
tree-structure automatically imposes a structuring on the models. In addi-
tion, if each node in the tree is allowed a “lookahead”option, then nodes
can contain conjunctions of ﬁrst-order literals, each of which can be seen as
deﬁning a new feature. The model is thus a hierarchy of ﬁrst-order features;
and

– Meta-interpretive learning [31], which allows a very general form of predicate-
invention, by allowing an abduction step when employing a meta-interpreter
to use higher-order templates of rules that be used to construct proofs (in
eﬀect, explanations) for data. In principle, this would allow us not just to
construct explanations on-demand, but also invent features on-demand. If
the higher-order templates can be specialised to the domain, then it should be
possible to control the feature-invention by relevance-information. Of course,
this is unrelated to generating explanations for the predictions of a black-box
classiﬁer.

An entirely diﬀerent, and much more sophisticated kind of hybrid model com-
bining connectionist and logical components has been proposed recently in the

34

Ashwin Srinivasan, Lovekesh Vig, and Michael Bain

form of Lifted Relational Neural Networks (LRNNs: [37]). In this, the logical
component is used to provide a template for ground neural network models,
which are used to learn weights on the logical formulae. While we have largely
stayed within the conﬁnes of classical ILP both for obtaining features and expla-
nations, LRNNs are closely related to probabilistic models for ILP. An area of
common interest arises though in the use of the network structure to invent new
features (although in the LRNN case, this is not for local models as we proposed
here).

7 Concluding Remarks

The recent successes of deep neural networks on predictive tasks have not, to any
large extent, used either domain knowledge or representations signiﬁcantly more
expressive than simple relations (usually over sequences). The price for this has
been a requirement for very large amounts of data, which provide the network
with suﬃcient correlations necessary to identify predictively useful models. This
works for problems where large amounts of data are being routinely generated
automatically; and about which there may be little or no domain knowledge. The
situation with scientiﬁc data is quite the opposite: data are sparse, but there is
signiﬁcant domain-knowledge, often built up over decades of painstaking experi-
mental work. We would like powerful predictive models in such domains, but for
this, the network would need a way of capturing what is known already, in a lan-
guage that is suﬃciently expressive. The knowledge-rich deep networks (DRMs)
we have proposed here is one way forward, and the results suggest that we can
achieve, and often exceed, the levels of predictivity reached by full ﬁrst-order
learners. It is important to understand also what the experimental evidence pre-
sented does not tell us. It does not tell us, for example, that a deep network
without ﬁrst-oder features will not achieve the same performance as those tab-
ulated here. However, it is not immediately apparent how this conjecture could
be tested, since no more data are available for the problems. However it may be
possible to transfer features constructed by a network trained on other problems
with more data.

Our interests in this paper extend beyond prediction. We want to construct
understandable models. For this we start with the approach taken in [33] and
propose the use of a proxy for the DRM that acts as a readable explanation.
The proxy in this paper is in the form of a symbolic model for the predictions
made by the DRM, constructed using techniques developed in Inductive Logic
Programming (ILP). But there are at least three limitations we see arising from
using the approach in [33]. First, the goal is to generate a readable proxy for the
prediction made by a black-box model (for us, the DRM is the black box). This
need not be the same as a readable proxy for the (true-)value of the instance.
Second, readability does not guarantee comprehensibility: in [25] examples are
shown of readable, but still incomprehensible models. Thirdly, an important
quality normally required of good explanations, causality, does not explicitly
play a role. The ﬁrst issue is inherent to the purpose of the model, and we

Title Suppressed Due to Excessive Length

35

have not attempted to change it here. The incorporation of a semantic prior
based on relevance is a ﬁrst attempt to address directly the second issue, and
indirectly may address the third partially (non-causal explanations should have
low relevance). To construct causal explanations correctly we will need more
information than assigning relevance labels to predicates. We will also need the
explanation-generator to pose counterfactual queries to the black-box, and the
black-box to be able answer such queries with high accuracy. At this point,
there is some evidence that symbolic learning could be adapted to suggest new
experiments (see for example [20]), but it is not known how well DRMs will
perform if the distribution of input values is very diﬀerent to those that were
used to train the network. So, at this point, we have restricted ourselves to
constructing readable explanations for predictions that take into account prior
preferences.

There are at least three separate directions in which we plan to extend the
work here. First, interactions with the domain-expert suggests that the relevance
information we have used here can be made much more ﬁne-grained. It is possi-
ble, for example that certain combinations of predicates may be more relevant
than others (or, importantly, certain combinations are deﬁnitely not relevant).
None of this is accounted for in the current feature-generation process, and we
intend to investigate relevance-guided sampling in place of the simple random
sampling we use at present. Secondly, we would like to explore the construction of
causal explanations for DRMs, by combining counterfactual reasoning, with the
use of a generative deep network capable of generating new instances. Thirdly,
it is necessary at some point in the future, to establish the link between local
symbolic explanations and human comprehensibility. For this, we would need
to conduct a cross-comparison of structured and unstructured explanations and
ratings of their comprehensibility by a domain-expert. Recently ([35]) experi-
ments have been reported in the ILP literature on assessing comprehensibility,
when invention of predicates is allowed. Similar experiments will help assess the
human-comprehensibility of local symbolic explanations for black-box classiﬁers.

Acknowledgements

A.S. is a Visiting Professorial Fellow, School of CSE, UNSW Sydney. A.S. is
supported by the SERB grant EMR/2016/002766.

References

1. Charu C. Aggarwal, Alexander Hinneburg, and Daniel A. Keim. On the surprising
behavior of distance metrics in high dimensional spaces.
In Database Theory -
ICDT 2001, 8th International Conference, London, UK, January 4-6, 2001, Pro-
ceedings., pages 420–434, 2001.

2. H.

Blockeel

S.
https://dtai.cs.kuleuven.be/software/subtle/, 2016.

Valevich.

and

Subtle.

Available

at:

3. Hendrik Blockeel. Top-down induction of ﬁrst order logical decision trees. AI

Commun., 12(1-2):119–120, 1999.

36

Ashwin Srinivasan, Lovekesh Vig, and Michael Bain

4. G. Coletti and R. Scozzafava. A coherent qualitative Bayes’ theorem and its ap-
plication in artiﬁcial intelligence. In Proc. 2nd Intl. Symp. Uncertainty Modeling
and Analysis, pages 40–44, 1993.

5. Mark Craven and Jude W. Shavlik. Using sampling and queries to extract rules
from trained neural networks. In Machine Learning, Proceedings of the Eleventh
International Conference, Rutgers University, New Brunswick, NJ, USA, July 10-
13, 1994, pages 37–45, 1994.

6. D. Michie. The superarticulacy phenomenon in the context of software manufac-

ture. Proc. R. Soc. Lond. A, 405:185–212, 1986.

7. D. Michie and R. Johnston. The Creative Computer: Machine Intelligence and

Human Knowledge. Viking Press, 1984.

8. Artur S. d’Avila Garcez, Krysia B. Broda, and Dov M. Gabbay. Neural-Symbolic
Learning Systems: Foundations and Applications. Perspectives in Neural Comput-
ing. Springer, 2002.

9. D. Gabbay. Labelled Deductive Systems, volume 1. Clarendon Press, 1996.

10. Dov M. Gabbay, C.J. Hogger, and J.A. Robinson. Logic Programming. Vol. 5 of
Handbook of Logic in Artiﬁcial Intelligence and Logic Programming. Clarendon
Press, Oxford, 1998.

11. I.J. Good. Good Thinking: The Foundations of Probability and its Applications.

University of Minnesota, Minneapolis, 1983.

12. Ian J. Goodfellow, Yoshua Bengio, and Aaron C. Courville. Deep Learning. Adap-

tive computation and machine learning. MIT Press, 2016.

13. C.J. Hogger. Essentials of Logic Programming. Clarendon Press, Oxford, 1990.
14. Ian Stewart. The Ultimate in Anty-Particles. Scientiﬁc American, July, 1994.
15. Andrej Karpathy and Fei-Fei Li. Deep visual-semantic alignments for generating
image descriptions. In IEEE Conference on Computer Vision and Pattern Recog-
nition, CVPR 2015, Boston, MA, USA, June 7-12, 2015, pages 3128–3137, 2015.
16. R. D. King, S. H. Muggleton, A. Srinivasan, and M J Sternberg. Structure-activity
relationships derived by machine learning: the use of atoms and their bond con-
nectivities to predict mutagenicity by inductive logic programming. Proceedings of
the National Academy of Sciences of the United States of America, 93(1):438–42,
January 1996.

17. R. D. King and A. Srinivasan. Prediction of rodent carcinogenicity bioassays from
molecular structure using inductive logic programming. Environmental Health Per-
spectives, 104:pp. 1031–1040, Oct. 1996.

18. R.D. King, S.H. Muggleton, A. Srinivasan, and M.J.E. Sternberg. Structure-
activity relationships derived by machine learning: The use of atoms and their
bond connectivities to predict mutagenicity by inductive logic programming. Proc.
of the National Academy of Sciences, 93:438–442, 1996.

19. R.D. King and A. Srinivasan. Prediction of rodent carcinogenicity bioassays from
molecular structure using inductive logic programming. Environmental Health Per-
spectives, 104(5):1031–1040, 1996.

20. Ross D. King, Kenneth E. Whelan, Fﬁon M. Jones, Philip G. K. Reiser, Christo-
pher H. Bryant, Stephen H. Muggleton, Douglas B. Kell, and Stephen G. Oliver.
Functional genomic hypothesis generation and experimentation by robot scientist.
Nature, 427:247–252, 2004.

21. Huma Lodhi. Deep relational machines. In Neural Information Processing - 20th
International Conference, ICONIP 2013, Daegu, Korea, November 3-7, 2013. Pro-
ceedings, Part II, pages 212–219, 2013.

Title Suppressed Due to Excessive Length

37

22. Eric McCreath and Arun Sharma. LIME: A system for learning relations. In Al-
gorithmic Learning Theory, 9th International Conference, ALT ’98, Otzenhausen,
Germany, October 8-10, 1998, Proceedings, pages 336–374, 1998.

23. Michael Bain. Experiments in non-monotonic learning.

In Proceedings of the
Eighth International Workshop (ML91), Northwestern University, Evanston, Illi-
nois, USA, pages 380–384, 1991.

24. R.S. Michalski. A theory and methodology of inductive learning. In R. Michalski,
J. Carbonnel, and T. Mitchell, editors, Machine Learning: An Artiﬁcial Intelligence
Approach, pages 83–134. Tioga, Palo Alto, CA, 1983.

25. Donald Michie. Consciousness as an engineering issue, part 2. Journal of Con-

sciousness Studies, 2(1):52–66.

26. T.M. Mitchell, R.M. Keller, and S.T. Kedar-Cabelli. Explanation-Based General-

ization: A Unifying View. Machine Learning, 1(1):47–80, 1986.

27. S. Muggleton. Inductive Logic Programming: derivations, successes and shortcom-

ings. SIGART Bulletin, 5(1):5–11, 1994.

28. S. Muggleton. Inverse Entailment and Progol. New Gen. Comput., 13:245–286,

1995.

29. S.H. Muggleton. Duce, an oracle based approach to constructive induction.

In

IJCAI-87, pages 287–292. Kaufmann, 1987.

30. Stephen Muggleton, Jos´e Carlos Almeida Santos, and Alireza Tamaddoni-Nezhad.
Toplog: ILP using a logic program declarative bias. In Logic Programming, 24th
International Conference, ICLP 2008, Udine, Italy, December 9-13 2008, Proceed-
ings, pages 687–692, 2008.

31. Stephen H. Muggleton, Dianhuan Lin, and Alireza Tamaddoni-Nezhad. Meta-
interpretive learning of higher-order dyadic datalog: predicate invention revisited.
Machine Learning, 100(1):49–73, 2015.

32. G.D. Plotkin. A note on inductive generalisation. In B. Meltzer and D. Michie,
editors, Machine Intelligence 5, pages 153–163. Elsevier North Holland, New York,
1970.

33. Marco T´ulio Ribeiro, Sameer Singh, and Carlos Guestrin. ”why should I trust
you?”: Explaining the predictions of any classiﬁer. In Proceedings of the 22nd ACM
SIGKDD International Conference on Knowledge Discovery and Data Mining, San
Francisco, CA, USA, August 13-17, 2016, pages 1135–1144, 2016.

34. Amrita Saha, Ashwin Srinivasan, and Ganesh Ramakrishnan. What kinds of rela-

tional features are useful for statistical learning? In ILP, 2012.

35. Ute Schmid, Christina Zeller, Tarek R. Besold, Alireza Tamaddoni-Nezhad, and
Stephen Muggleton. How does predicate invention aﬀect human comprehensibil-
ity? In Inductive Logic Programming - 26th International Conference, ILP 2016,
London, UK, September 4-6, 2016, Revised Selected Papers, pages 52–67, 2016.
36. A.D. Shapiro. Structured Induction in Expert Systems. Addison-Wesley, Woking-

ham, 1987.

37. Gustav Sourek, Vojtech Aschenbrenner, Filip Zelezn´y, and Ondrej Kuzelka. Lifted
relational neural networks.
In Proceedings of the NIPS Workshop on Cognitive
Computation: Integrating Neural and Symbolic Approaches co-located with the 29th
Annual Conference on Neural Information Processing Systems (NIPS 2015), Mon-
treal, Canada, December 11-12, 2015., 2015.

38. A. Srinivasan. The Aleph Manual. Available at http://www.comlab.ox.ac.uk/oucl/

research/areas/machlearn/Aleph/, 1999.

39. A. Srinivasan, R.D. King, and M.E. Bain. An empirical study of the use of rele-
vance information in Inductive Logic Programming. Machine Learning Research,
4(Jul):369–383, 2003.

38

Ashwin Srinivasan, Lovekesh Vig, and Michael Bain

40. A. Srinivasan, S. H. Muggleton, M. J. E. Sternberg, and R. D. King. Theories
for mutagenicity: A study in ﬁrst-order and feature-based induction. Artif. Intell.,
85(1-2):277–299, 1996.

41. Ashwin Srinivasan and Ganesh Ramakrishnan. Parameter screening and optimisa-
tion for ILP using designed experiments. Journal of Machine Learning Research,
12:627–662, 2011.

42. Sebastian Thrun. Extracting rules from artiﬁcal neural networks with distributed
representations. In Advances in Neural Information Processing Systems 7, [NIPS
Conference, Denver, Colorado, USA, 1994], pages 505–512, 1994.

43. F. Zelezny, A. Srinivasan, and C.D. Page. Randomised Restarted Search in ILP.

Machine Learning Journal, 64(1,2,3), 2006.

44. Blaz Zupan, Ivan Bratko, Marko Bohanec, and Janez Demsar. Function decom-
position in machine learning. In Machine Learning and Its Applications, Advanced
Lectures, pages 71–101, 2001.

A Fidelity and Likelihood

The model for noisy data in [22] can be adapted to the construction of local
explanations that are not completely consistent with local predictions (that is,
ﬁdelity < 1).

Remark 37 (Bayesian Posterior [22]) Given a relational instance a ∈ X , let
N be predictive model s.t. N (F V (a)) = c for c ∈ Y. Let sets E+ and E− denote
the local neighbourhood of a and D = (E+, E−). Let H be a local explanation
for a (not necessarily consistent), using background knowledge B. Then, if θ(H)
is the proportion of all instances in X × Y covered by H, and (cid:15) is an estimate
of inconsistency (noise) allowed (cid:15) ∈ [0, 1]), the log posterior is given by [22]:

logP (H|D, B) = logP (D|H, B) + logP (H|B) − log(D|B)

where P (H|B) denotes the prior probability, and:

logP (D|H, B) = |T P (H)|log

(cid:18) 1 − (cid:15)
θ(H)

(cid:19)

+ (cid:15)

+|T N (H)|log

(cid:18) 1 − (cid:15)

1 − θ(H)

(cid:19)

+ (cid:15)

+|F P N (H)|log((cid:15))

is the log-likelihood. Here T P (H) is the set {e : e ∈ E+ and B ∧H |= e}; T N (H)
is the set {e : e ∈ E− and B ∧ H ∧ ¬e (cid:54)|= (cid:50)}; and F P N (H) = D − (T P (H) ∪
T N (H)).

It is not hard to see that T P and T N correspond to AgreeP os and AgreeN eg
in Deﬁnition 26. In some cases, it is in fact suﬃcient to maximise ﬁdelity, to
maximise the log-likelihood.

Remark 38 (Fidelity and Log-Likelihood) Let H1,2 be local explanations
for a relational example Class(a, c), given D, B. If θ(H1) = θ(H2), |T P (H2)| ≥
|T P (H1)| and |T N (H2)| ≥ |T N (H1)| then: (a) F idelity(H2|D, B) ≥ F idelity(H1|D, B);

Title Suppressed Due to Excessive Length

39

and (b) logP (D|H2, B) ≥ logP (D|H1, B).

Given D, B, the log-likelihood log P (D|H, B), is given in Defn. 37. From

Defn. 26 F idelity(H1|D, B) = |T P (H1)|+|T N (H1)|
|D|
Since |T P (H2) ≥ |T P (H1)| and |T N (H2)| ≥ |T N (H1)|, it follows trivially that
F idelity(H2|D, B) ≥ F idelity(H1|D, B).

and F idelity(H2|D, B) = |T P (H2)|+|T N (H2)|

|D|

.

For any explanation H, s.t. 0 < θ)(H) < 1 and For a ﬁxed (cid:15) s.t. 0 ≤ (cid:15) ≤ 1,
the log-multipliers k1,2(θ(H)) of |T P (H)| and |T N (H)| in the expression for
log P (D|H, B) are both positive; and the log-multiplier k3 of |F P N (H)| is at
most 0. Therefore logP (D|H) is k1(θ(H))|T P | + k2(θ(H))|T N | − k3(|D|) +
k3(|T P | + |T N |).. If θ(H1) = θ(H2), then k1(θ(H1)) = k1(θ(H2)) = k1, say and
k2(θ(H1) = k2(θ(H2)) = k2, say. Then logP (D|H1) is k1|T P (H1)|+k2|T N (H1)|−
k3(|D|)+k3(|T P (H1)|+|T N (H1)|) and logP (D|H2) is k1|T P (H2)|+k2|T N (H2)|−
k3(|D|)+k3(|T P (H2)|+|T N (H2)|), Since |T P (H2) ≥ |T P (H1)| and |T N (H2)| ≥
|T N (H1)|, and k1,2 > 0 and k3 ≥ 0 it follows trivially that logP (D|H2, B) ≥
logP (D|H1, B).

B Relevance Information

The following problem-speciﬁc relevance assignments for predicates in the back-
ground knowledge were obtained from R.D. King, University of Manchester.

Problem Relevance Predicates
1
Mut188
2
3
4
5
1
2
3
4
1
1
2

Atoms and bonds
3-dimensional distance
Functional groups and rings
LUMO, hydrophobicity
Expert-identiﬁed indicator variables
Atoms and bonds
Functional groups and rings
Carcinogenic alerts
Outcome of genetic tests
Atoms and bonds
Substitutions at templates
Hansch-type predicates (size, polarity etc.)

DssTox
Alzh.
Datasets

Canc330

C Example Explanations

The following explanations are for a test-instance in the Canc330 problem (specif-
ically, test-instance 2 on the 3rd cross-validation split). This example was chosen
since it illustrates a number of interesting aspects: all explanations have perfect
ﬁdelity; structuring increases relevance; and there are several structured expla-
nations possible. The DRM has 2196 features, of which 397 are active for this
instance. The DRM correctly predicts the instance as belonging to the “positive”
class. All the symbolic explanations below predict the same class-values as the
DRM for the test-instance and its neighbours.

40

Ashwin Srinivasan, Lovekesh Vig, and Michael Bain

Unstructured explanation:

Label: (cid:104)LH = 1.0, PH = {[1, 4]}(cid:105)
Explanation H:

Class(x, c) ← F537(x), F1196(x), F610(x), F611(x), F1657(x)

Structured explanation(s):

Label: (cid:104) LH1 = 1.0, PH1 = {[1, 4], [4, 4]}(cid:105)
Explanation H1:

Class(x, c) ← F1,1(x), F1,2(x)
F1,1(x) ← F1657(x)
F1,2 ← F537(x), F1196(x), F610(x), F611(x)

Label: (cid:104) LH2 = 1.0, PH2 = {[1, 4], [4, 4]}(cid:105)
Explanation H2:

Class(x, c) ← F1,1(x), F1,2(x)
F1,1(x) ← F611(x)
F1,2 ← F537(x), F1196(x), F610(x), F1657(x)

Label: (cid:104) LH3 = 1.0, PH3 = {[1, 4], [4, 4]}(cid:105)
Explanation H3:

Class(x, c) ← F1,1(x), F1,2(x)
F1,1(x) ← F1657(x), F611(x)
F1,2 ← F537(x), F1196(x), F610(x)

Feature-deﬁnitions:
F537(x) ← Atm(x, y, h, 3, z), Gteq(z, 0.115)
F1196(x) ← Atm(x, y, c, 22, z), Gteq(z, −0.111), Atm(x, w, c, 22, z)
F610(x) ← N on ar hetero 6 ring(x, u), Has property(x, ames, p)
F611(x) ← Has property(x, salmonella, n), Has property(x, mouse lymph, p)
F1657(x) ← Has property(x, cytogen ca, n), Has property(x, mouse lymph, p), Hasproperty(x, cytogen sce, p)

(Relev = [1,1])
(Relev = [2,4])

(Relev = [1,1])

(Relev = [4,4])

(Relev = [4,4])

