8
1
0
2

n
a
J

9
1

]
L
M

.
t
a
t
s
[

1
v
8
7
3
6
0
.
1
0
8
1
:
v
i
X
r
a

Introducing ReQuEST: an Open Platform for Reproducible
and Quality-Eﬃcient Systems-ML Tournaments

Thierry Moreau1, Anton Lokhmotov2, Grigori Fursin3

1 University of Washington, USA ; 2 dividiti, UK ; 3 cTuning foundation, France

December, 2017

Abstract

1

Introduction

Co-designing eﬃcient machine learning based systems
across the whole hardware/software stack to trade oﬀ
speed, accuracy, energy and costs is becoming extremely
complex and time consuming. Researchers often struggle
to evaluate and compare diﬀerent published works across
rapidly evolving software frameworks, heterogeneous
libraries, algorithms,
hardware platforms, compilers,
data sets, models, and environments.

We 1 present our community eﬀort to develop an
open co-design tournament platform with an online
It will gradually incorporate best
public scoreboard.
research practices while providing a common way for
multidisciplinary researchers to optimize and compare
the quality vs. eﬃciency Pareto optimality of various
workloads on diverse and complete hardware/software
systems. We want to leverage the open-source Collective
Knowledge framework and the ACM artifact evaluation
methodology to validate and share the complete machine
learning system implementations in a standardized,
portable, and reproducible fashion. We plan to
hold regular multi-objective optimization and co-design
tournaments
such as deep
for emerging workloads
learning, starting with ASPLOS’18 (ACM conference
on Architectural Support for Programming Languages
and Operating Systems
forum for
-
multidisciplinary systems research spanning computer
architecture and hardware, programming languages and
compilers, operating systems and networking) to build
a public repository of the most eﬃcient algorithms and
systems which can be easily reused and built upon.
We will also use the feedback from participants to
continue improving our platform and common co-design
methodology.

the premier

1ReQuEST organizers

(A-Z): Luis Ceze, University of
Washington (USA), Natalie Enright Jerger (University of Toronto,
Canada), Babak Falsaﬁ (EPFL, Switzerland), Grigori Fursin,
(cTuning foundation, France), Anton Lokhmotov, (dividiti, UK),
Thierry Moreau,
(University of Washington, USA), Adrian
Sampson, (Cornell University, USA) Phillip Stanley Marbell,
(University of Cambridge, UK)

Machine learning has undergone a rapid pace of progress
over the recent years. Rarely in the scientiﬁc community
have we witnessed such a concerted eﬀort from various
communities (machine learning,
systems, hardware,
security, programming languages etc.) in improving the
performance, accuracy, robustness and cost of machine
learning based systems.

However,

implementing such systems for a given
problem (for example, deep learning algorithm for
ImageNet classiﬁcation), one has to navigate a multitude
of design decisions: what network architecture to deploy
and how to customize it (ResNet vs. MobileNet), what
framework to use (MXNet vs.
TensorFlow), what
libraries and which optimizations to employ (MKL
vs. OpenBLAS), which is generally a consequence of
the target hardware platform (Intel Xeon + NVIDIA
GPU vs. ARM-based mobile SoC). On top of these
implementation decisions, platform-speciﬁc decisions
may aﬀect the performance and overall experience in
deploying the system in question:
details such as
operating system, kernel version, framework and library
versions or dependencies, and custom optimizations. As
a result a given system implementation (for example,
ResNet on TensorFlow on a Intel + NVIDIA hardware
system) can have many incarnations, some of which
may have drastically diﬀerent performance results.
Furthermore, as more papers are being published, it also
becomes challenging to reproduce, reuse, build on top
of, and perform fair comparisons of numerous machine
learning techniques across rapidly evolving systems.

As multiple communities tackle the same challenges
of making machine learning systems faster, cheaper,
smaller, more accurate, and more energy eﬃcient across
diverse platforms from IoT to data centers, we need
a platform to automate and perform apples-to-apples
comparisons of diﬀerent approaches that aim to achieve
the same goal. For example, how does an approximate
analog accelerator
to
algorithmic simpliﬁcations on oﬀ-the-shelf hardware in
terms of accuracy vs. eﬃciency Pareto optimality?

for deep learning compare

1

 
 
 
 
 
 
Figure 1: Collective Knowledge framework as an open platform to support open tournaments for software/hardware
co-design of Pareto-eﬃcient deep learning and other emerging workloads in terms of speed, accuracy, energy and
various costs.

the

We

propose

therefore

and multi-objective

Reproducible
(ReQuEST)
Quality-Eﬃcient Systems Tournament
reproducible,
as a community-driven platform for
comparable,
of
emerging workloads [2]. We plan to host ReQuEST as
a bi-annual workshop alternating between systems and
machine learning communities. Its ﬁrst incarnation will
take place at ASPLOS in March 2018 and will focus
solely on optimizing inference on real systems to test
our platform and use the feedback from participants and
an industrial board to improve it.

optimization

We detail in the next sections how its objectives and
execution diﬀerentiate ReQuEST from other existing
workshop-based competitions.

2 Main goals

ReQuEST is aimed at providing a
Summary:
scalable tournament framework, a common experimental
methodology and an open repository for continuous
evaluation and optimization of the quality vs. eﬃciency
real-world
Pareto optimality of a wide

range of

applications,
libraries, and models across the whole
hardware/software stack on complete platforms as
conceptually shown in Figure 1.

we want

Tournament

experimental
systems

framework goals:
of

to
promote
reproducibility
results
research
and reusability/customization of
artifacts by standardizing evaluation methodologies
and facilitating the deployment of eﬃcient solutions on
heterogeneous platforms. For that reason, packaging
artifacts and experimental results requires a bit more
involvement than sharing some CSV ﬁles or checking
out a given GitHub repository.

That

is why we build our competition on top
of an open-source and portable workﬂow framework
(Collective Knowledge or CK [3]) and a standard
ACM artifact evaluation methodology [1] from premier
ACM systems
(CGO, PPoPP, PACT,
SuperComputing) to provide uniﬁed evaluation and a live
scoreboard of submissions as demonstrated in Figure 2.

conferences

CK is a Python wrapper framework to share artifacts
and workﬂows as customizable and reusable plugins
with a common JSON API and meta description, and

2

Connect AI, ML and systems researchersCollective KnowledgeAI enginesTensorFlowCaffeModelsLibrariesOpenBLASCK JSON APICK JSON APICK JSON APIusing portable, customizable and reusable Collective Knowledge workflowsCK JSON APICK JSON APICK JSON APIRun and optimize emerging workloads such as deep learning across diverse models, data sets and platforms with GPU,DSP,NN accelerators …Open repository of reusable and optimized artifactsCaffeCaffe2CNTKTorchMXNetModelsAlexNetGoogleNetVGGResNetSqueezeNetSqueezeDetSSDMobileNetsDatasetsKITTICOCOVOCImageNetReal dataOpenBLASViennaCLclBLASCLBlastcuBLAScuDNNEigengemmlowpArmCLCK JSON APITargetsCK JSON APILinuxMacOSWindowsAndroid……………Select and share winning SW/HW/model species (configurations) for various trade-offsReproduce, extend and build upon past research;Accelerate knowledge discovery, reduce R&D costs;Enable efficient AI-based systemsFigure 2: An example of a live Collective Knowledge scoreboard to crowd-benchmark inference in terms of speed
and platform cost across diverse deep learning frameworks, models, data sets, and Android devices provided by
volunteers. Red dots are associated with the winning workﬂows (model/software/hardware).

For example,

adaptable to a user platform with Linux, Windows,
MacOS and Android.
it has already
been used and extended in a number of academic
and industrial projects to automate and crowdsource
benchmarking and multi-objective optimization of deep
learning across diverse platforms, environments, and
data sets. Figure 2 shows a proof-of-concept example
of a live scoreboard powered by CK to collaboratively
benchmark inference (speed vs. platform cost) across
diverse deep learning frameworks (TensorFlow, Caﬀe,
MXNet, etc.), models (AlexNet, GoogleNet, SqueezeNet,
ResNet, etc.), real user data sets, and mobile devices
provided by volunteers
results at
cKnowledge.org/repo).

(see the latest

3 Future work

Our goal is to bring multi-disciplinary researchers to

1. release research artifacts of

their on-going or
accomplished research,
evaluation
workﬂows, and facilitate deployment and tech
transfer of state-of-the-art research,

standardize

2. foster exploration of quality-eﬃciency trade-oﬀs,

and

3. create a discussion ground to steer the community
and

new applications,

frameworks,

towards
hardware platforms.

and

stress

Pareto-optimality
quality-awareness

goals:
Metrics
the
to
we want
and
architecture/compilers/systems
resource-awareness
applications/algorithms
the
community and end-users. The submissions and their
evaluation metrics will be maintained in a public
repository that includes a live scoreboard.

community,

to

to

Speciﬁc attention will be brought to submissions close
to a Pareto frontier in a multi-dimensional space of
accuracy, execution time, power/energy consumption,
hardware/code/model footprint, monetary costs etc.

Application goals: in the long term, we will cover a
comprehensive suite of workloads, datasets and models
covering applications domains that are most relevant
to machine learning and systems researchers. This
suite will continue evolving according to feedback and
contributions from the academia and industry. All
artifacts from this suite can be automatically plugged
in to the ReQuEST competition workﬂows to simplify
and automate experimentation.

set

comprehensive

Complete platforms goals: we aim to cover
from
a
data-centers down to sensory nodes,
incorporating
various forms of processors including GPUs, DSPs,
FPGAs, neuromorphic and even analogue accelerators
in the long term.

of hardware

systems

We want

to set a coherent

research roadmap
tournaments
researchers by hosting bi-annual
for
complemented with panel discussions
from both
academia and industry. We hope that as participation
increases, the coverage of problems (vision, speech and
even beyond machine learning) and platforms (novel
hardware accelerators, SoCs, and even exotic hardware
such as analog, neuromorphic, stochasticm, quantum)
will increase.

the premier

ReQuEST is organized as a bi-annual workshop,
alternating between systems-oriented and machine
learning-oriented conferences.
The ﬁrst ReQuEST
workshop will be co-located with ASPLOS in March
2018 (ACM conference on Architectural Support
for Programming Languages and Operating Systems
-
forum for multidisciplinary systems
research spanning computer architecture and hardware,
programming
operating
The workshop will aim
systems and networking).
to present artifacts submitted by participants, along
with a multi-objective scoreboard, where quality-eﬃcient
implementations will be rewarded. The submissions will
be validated by an artifact evaluation committee, and
participants will have the chance to get an artifact paper
published as ACM proceedings.

and compilers,

languages

In addition we wish to nurture a discussion ground
for artifact evaluation in multidisciplinary research,

3

Winning/surviving  species (configurations) on various frontiers Image classificaiton time (sec) Device cost (euros) tournament platform.

gathering perspectives from machine learning, systems,
compilers and architecture experts. We will use
this discussion to continuously improve and extend
functionality of our
For
example, we plan to gradually standardize the API
and meta description of all artifacts and machine
learning workﬂows with the help of the community,
provide architectural simulators and simulator-based
evaluations, cover low-level optimizations, expose more
metrics, and so on.

an

Finally,

industrial

composed

panel
from prominent

of
research-representatives
software
and hardware companies will discuss how tech-transfer
can be facilitated between academia and industry, and
will help craft the roadmap for the ReQuEST workshops
by suggesting new datasets, workloads, metrics, and
hardware platforms.

References

[1] Artifact

systems
for
conferences. http://cTuning.org/ae, 2014–present.

Evaluation

computer

[2] ReQuEST: open tournaments on collaborative,
reproducible and pareto-eﬃcient software/hardware
co-design of emerging workloads using the collective
knowledge technology.
http://cKnowledge.org/
request, 2017–present.

[3] G. Fursin, A. Lokhmotov,

and E. Plowman.
Collective Knowledge: towards R&D sustainability.
the Conference on Design,
In Proceedings of
Automation and Test in Europe (DATE’16), March
2016.

4

