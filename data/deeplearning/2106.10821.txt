Demonstration of Panda:
A Weakly Supervised Entity Matching System

Renzhi Wu†, Prem Sakala†, Peng Li†, Xu Chu†, Yeye He§,
†Georgia Institute of Technology, §Microsoft Research
†{renzhiwu@, premsakala@, pengli@, xu.chu@cc.}gatech.edu, §yeyehe@microsoft.com

1
2
0
2

p
e
S
3
2

]

B
D
.
s
c
[

3
v
1
2
8
0
1
.
6
0
1
2
:
v
i
X
r
a

ABSTRACT
Entity matching (EM) refers to the problem of identifying tuple
pairs in one or more relations that refer to the same real world
entities. Supervised machine learning (ML) approaches, and deep
learning based approaches in particular, typically achieve state-of-
the-art matching results. However, these approaches require many
labeled examples, in the form of matching and non-matching pairs,
which are expensive and time-consuming to label.

In this paper, we introduce Panda, a weakly supervised system
specifically designed for EM. Panda uses the same labeling func-
tion abstraction as Snorkel, where labeling functions (LF) are user-
provided programs that can generate large amounts of (somewhat
noisy) labels quickly and cheaply, which can then be combined
via a labeling model to generate accurate final predictions. To sup-
port users developing LFs for EM, Panda provides an integrated
development environment (IDE) that lives in a modern browser ar-
chitecture. Panda’s IDE facilitates the development, debugging, and
life-cycle management of LFs in the context of EM tasks, similar to
how IDEs such as Visual Studio or Eclipse excel in general-purpose
programming. Panda’s IDE includes many novel features purpose-
built for EM, such as smart data sampling, a builtin library of EM
utility functions, automatically generated LFs, visual debugging of
LFs, and finally, an EM-specific labeling model. We show in this
demo that Panda IDE can greatly accelerate the development of
high-quality EM solutions using weak supervision.

PVLDB Reference Format:
Renzhi Wu, Prem Sakala, Peng Li, Xu Chu, Yeye He. Demonstration of
Panda: A Weakly Supervised Entity Matching System. PVLDB, 14(12): 2735
- 2738, 2021.
doi:10.14778/3476311.3476332

1 INTRODUCTION
Entity matching (EM) refers to the problem of identifying tuples in
one or more tables that refer to the same real world entities. For
example, an e-commerce website would want to identify identical
products from different suppliers for a unified catalog. EM has
been extensively studied in many research communities, including
databases, statistics, NLP, and data mining. The standard approach
to EM uses similarity scores between two tuples as feature vectors,
and then formulates the problem of match/non-match as a binary

This work is licensed under the Creative Commons BY-NC-ND 4.0 International
License. Visit https://creativecommons.org/licenses/by-nc-nd/4.0/ to view a copy of
this license. For any use beyond those covered by this license, obtain permission by
emailing info@vldb.org. Copyright is held by the owner/author(s). Publication rights
licensed to the VLDB Endowment.
Proceedings of the VLDB Endowment, Vol. 14, No. 12 ISSN 2150-8097.
doi:10.14778/3476311.3476332

classification problem given the feature vectors [6]. Supervised
Machine Learning (ML) approaches, particularly deep learning
approaches (e.g., DeepER [5] and DeepMatcher [10]), often achieve
state-of-the-art results for EM [4, 5, 8, 10]. However, they require
large amounts of labeled training data that are expensive and time-
consuming to obtain – for example, it has been reported [4] that
achieving F-measures of ∼99% with random forests can require up
to 1.5M labels even on relatively clean datasets.
Weak supervision and the LF abstraction. Lack of training data
is a common problem in applied ML. Many ML researchers and
practitioners have increasingly resorted to weak supervision meth-
ods, in which large amounts of cheaply generated, but often noisy,
labeled examples are generated in lieu of hand-labeled examples.
The data programming paradigm [12], as implemented in the
Snorkel system [11], proposes to generate weakly supervised sig-
nals using labeling functions (LFs). LFs are user-provided functions
(e.g., in Python) that take each example as input and produces a
possibly noisy label (positive/negative/abstain). A labeling model
then combines noisy labels from all LFs, considers their accuracy
and possible correlations, to produce a final probabilistic label. Data
programming has been successfully adopted across many industries
and application domains.
Weak supervision for EM. We built a new weak supervision
system, Panda, that is specifically designed for EM. Panda adopts
the data programming paradigm, and develops an IDE that allows
users to easily develop LFs in order to build weakly-supervised
solutions for EM tasks.

Figure 1: Tuple pairs in the abt-buy dataset [1]. Blue denotes tuples
from the abt table and purple denotes tuples from the buy table.

Example 1.1. Figure 1 shows a sample of tuple pairs for the abt-
buy dataset [1]. Instead of manually labeling each tuple pair, users
may inspect the record pairs and develop some intuition of how to
label a pair as match vs. non-match. For example, one may label a
tuple pair as a match if the "name" attribute of the two products are
very similar (e.g. the first pair). On the other hand, a tuple pair is
likely a non-match if key attributes of the two products are different
(e.g. the second tuple pair, where the screen sizes are 40’ vs 46’).

Users can encode these intuitions in the form of LFs. Figure 2
shows two example LFs written in Python that correspond to the
example heuristics mentioned above. The name_overlap function
labels a tuple pair a match (+1) or non-match (-1) if the token overlap

 
 
 
 
 
 
of their "name" attribute is high (score > 0.6) or low (score < 0.1);
otherwise, it abstains (0).

As a second example of LF, the size_unmatch function on the
right uses a regular expression to extract the product size (e.g. 40’)
from the "name" and "description" attributes. It labels a tuple pair a
non-match (-1) if the sizes are different, and abstains (0) otherwise.
As one can imagine, LFs so created may not be perfectly accurate
and may have varying levels of coverage. Nevertheless, as long as
LFs are reasonably accurate (e.g., better than random labeling), it
has been shown [12] that a labeling model can reason about the
accuracy/correlation of LFs to produce accurate final predictions.

Figure 2: Example labeling functions for the abt-buy dataset
Since the quality of final labels depends critically on the user-
provided LFs, Panda aims to provide an integrated development
environment (IDE) to support users in the entire life-cycle of labeling
using LFs for EM tasks. Just like a general-purpose IDE (e.g., Visual
Studio or Eclipse) that would include many features to support users
write and debug programs, Panda’s IDE includes many features to
support users to write, debug, and combine LFs via a browser-based
GUI that is both contextual and intuitive (Section 2).
Related Work. Most existing work in the literature focuses on
optimizing individual components of the EM pipeline (e.g., blocking
and matching) in terms of accuracy or efficiency. Magellan [7] is the
closest to us in spirit, as it also enables users to develop EM solutions
end-to-end. However, like most existing solutions, Magellan is a
supervised system that requires labeled data for training, while
Panda is weakly supervised that works on noisy LFs. In addition,
in Magellan, users are in charge of many parts of the pipeline (e.g.
labeling tuple pairs and training matchers), while in Panda users
only need to focus on developing LFs. However, because developers
need to closely inspect EM data to develop LFs and iterate, it calls
for an IDE that no existing EM solutions have looked at.

Snorkel is a general-purpose data programming framework that
is not tailored to EM, and does not have an IDE. In comparison,
our Panda system makes many optimizations to data-programming
in the context of EM, such as pre-built library functions for EM,
automatically-generated LFs, and optimized aggregation in the
labeling model tailored to EM tasks.

2 SOLUTION OVERVIEW
We discuss the IDE in Panda. We build Panda’s interface with
Vue.js1 and embed a Jupyter lab environment2 for various coding
support. We use flask3 as our backend. (1vuejs.org; 2jupyterlab.
readthedocs.io; 3flask.palletsprojects.com)

2.1 Write, Debug, and Combine LFs
1. Writing LFs. Similar to a general-purpose programming IDE
that provides various features (e.g. code completion and debugging)
to assist users write programs, Panda’s IDE also provides essential
support for developers to write LFs for EM.

1.1 Smart data sampling. To write LFs for EM, users need to
examine tuple pairs from a specific EM task, in order to develop
intuitions/heuristics that can be turned into code (LFs) to quickly
label matches/non-matches. Randomly sampled pairs are likely
non-matches in EM due to the class imbalance problem, and hence
are not very useful. Panda includes a smart data sampling strategy
to show tuple pairs that are likely matches, but are not labeled as
matches by the current labeling model. We use pre-trained semantic
sentence models (e.g. sentence-BERT [13]) to obtain an embedding
for each tuple, from which we use standard LSH to perform blocking.
From pairs that remain after blocking, we compute their similarity
scores and sample likely matches that are not currently as so.

1.2 Builtin utility functions for EM. An LF for EM typically in-
vokes standard utility functions such as: (1) text pre-processing (e.g.,
lower casing, stemming), (2) tokenization (e.g., 3-gram or space sep-
aration), (3) token weighting (e.g., equal weight or TF-IDF), and (4)
distance functions (e.g., Jaccard distance or edit distance). Panda
includes a library of utility functions along these four dimensions
(described in detail in Figure 2 of our prior work [9]), to make
writing LFs easy. We plan to expand the utility functions by in-
cluding pre-trained matchers for specific entity types (e.g., People,
Organization, Address, etc) [15], so that users can directly invoke
pre-trained matchers relevant to their EM task in their LFs.

1.3 Automatically generated LFs. Despite the aforementioned
features in Panda, for first-time users it may still be hard to write
LFs, and even for experienced users it is still laborious to write
many LFs. Panda leverages our prior work Auto-FuzzJoin [9]
to automatically generate high-quality LFs tailored to given EM
tasks, so that users can incorporate them directly without writing
a single line of code. Specifically, leveraging the fact that one of the
input tables is likely a reference table with no or few duplicates
(common in data warehouse setting [3] and shown in [9] to hold on
over 90% EM datasets from [2]), Auto-FuzzJoin can estimate the
precision/recall of LFs and automatically generate high-quality LFs,
instantiated using different preprocessing functions, tokenization,
weights, distance functions, and threshold values.

2. Debugging LFs. After users write LFs, they need to test/debug
LFs to ensure that (1) LFs can be applied on tuple pairs without cre-
ating exceptions; and (2) LFs produce high-quality labeling results.
To this end, Panda supports the following two forms of debugging.
2.1 Syntactical debugging. To help users program LFs, we inte-
grate the jupyter lab environment with a debugger extension in
Panda. This supports LF debugging just as in a general-purpose
IDE: users can set breakpoints, step into LFs, execute code line by
line, and see the values of the variables.

2.2 Semantic debugging. LFs that users write may produce incor-
rect labels on parts of the input data, and to help users improve
LF’s quality, Panda has a labeling model to estimate each LF’s false
positive (FP) and false negative (FN) rate. Furthermore, in the IDE
we provide an intuitive GUI where users can point and click to
quickly narrow down to the record pairs where each LF may be
making mistake. After examining these pairs, the user can itera-
tively improve the existing LF, or choose to write a new LF that
complements with existing LFs. Each modification of LF will trigger
FP and FN rates to be re-calculated, so that users can have a holistic
view of the quality of all LFs.

3. Combining LFs. Given a number of LFs (automatically gen-
erated or manually writte), users can trigger the labeling model
in Panda to combine signals from all LFs and produce an overall
label. While existing labeling models (e.g., Snorkel [12]) can be used
here, Panda develops an optimized EM-specific labeling model that
leverages two properties unique to EM.

First, since EM problems typically have a class imbalance prob-
lem (the number of non-matches greatly exceeding the number of
matches), using a single accuracy parameter to model LF quality
(e.g., [12]) is insufficient. We develop two class dependent accuracy
parameters, i.e. labeling accuracy 𝛼𝑀 for matches and labeling accu-
racy 𝛼𝑈 for non-matches. The accuracy parameters and the latent
ground-truth label 𝑦 can then be estimated by an EM algorithm.

Second, tuple pairs in EM are not entirely independent due to
the transitivity property, namely, i.e. tuple pairs (𝑡1, 𝑡2) and (𝑡1, 𝑡3)
both being match leads to (𝑡2, 𝑡3) being a match. To incorporate this
transitivity property, we adopt the method in our prior work Ze-
roER [14]. Specifically, we capture the transitivity property among
any three tuples 𝑡𝑖 , 𝑡 𝑗 and 𝑡𝑘 by an inequality 𝛾𝑖,𝑗,𝑀 ×𝛾𝑖,𝑘,𝑀 ≤ 𝛾 𝑗,𝑘,𝑀
where 𝛾𝑖,𝑗,𝑀 denotes the the probability of tuple pair (𝑡𝑖, 𝑡 𝑗 ) being
a match. All possible triples 𝑡𝑖, 𝑡 𝑗 , 𝑡𝑘 form a feasible set 𝑄 for the
probabilistic labels of the tuple pairs. We then enforce the transi-
tivity constraint by projecting the estimated probabilistic labels to
the feasible set 𝑄 at each E-step. Our preliminary experiments on
real-world benchmark datasets [1] shows that our labeling model
improves the F1-score of the state-of-the-art labeling model [11]
by 12% on average. We defer details of these results to a full paper
on Panda in the future.

2.2 Integrated and Intuitive GUI
Similar to a general-purpose IDE, we provide an integrated de-
velopment environment with an intuitive GUI for developing LFs,
that allows users to visually inspect tuple pairs that are relevant to
the given LFs. Panda’s GUI has the following four main panels as
shown in Figure 3(1).
EM Stats Panel. This panel monitors the EM task’s core statistics
including table sizes, candidate set size, number of matches found,
and estimated precision of the current solution. Users can click
on the estimated precision to trigger the labeling process, where a
sample of matches found will be loaded in the Data Viewer Panel
for users to label.
LF Stats Panel. This panel monitors the core statistics of all LFs
provided to Panda so far. For every LF, it displays the name, number
of matches/non-matches/abstains, and the estimated false-positive/
false-negative rates. Users can also click on the header of any col-
umn to sort all LFs by values in the column. Users can click on the
name of a LF to display its code snippet. In addition, users can also
click on any actual statistics of any chosen LF to display the relevant
tuple pairs in the Data Viewer Panel. For example, in Figure 3(1), if
the user clicks on 0.1402, the estimated false positive rate of the LF
name_overlap, the Data Viewer Panel will show all candidate pairs
that the LF labels as +1, but the labeling model labels as -1.
Data Viewer Panel. This panel displays the actual tuple pairs in
a tabular format, where every row corresponds to a tuple pair. The
first column titled "M/U" shows ground-truth labels (not available
initially), and users may left/right click on the cells to provide
match/non-match labels. The next set of columns show the actual

data values for corresponding attributes in the two input tables.
The data is presented in two sub-rows where the first row (colored
in blue) shows data from the left table and the second row (colored
in purple) shows data from the right table. This side-by-side com-
parison allows users to easily identify similarity/difference between
the two tuples. When the user click on the "show" button, a sample
of likely matches with label -1 or 0 from the current labeling model
will be shown, each associated with a "likelihood" of matching
score (in the last column) provided by the smart sampling method
mentioned above. The user can sort the table by this column to
quickly identify matches as show in Figure 3(1).
LF Authoring Panel. This panel embeds a JupyterLab environ-
ment for users to write and debug LFs. After users click on the
"load data" button, a notebook will be automatically generated with
required dependencies imported in the first cell and automatically
discovered LFs listed in the second cell right away, e.g. the auto_lf_0
function in Figure 3(1). Users can directly add new LFs or modify
the existing auto LFs to create new LFs. The debugger extension of
jupyter lab is enabled to debug python LFs (breakpoints, step-in,
etc.). The last cell of the notebook contains a single line of code that
when triggered, combines existing LFs and updates the EM Stats
Panel and LF Stats Panel. LFs are applied incrementally, i.e. only the
new and modified LFs are executed.

3 DEMONSTRATION SCENARIO
Panda operates in two phases: a development phase and a deploy-
ment phase. In the development phase, users focus on designing a
set of LFs interactively. In the deployment phase, the final LFs are
used to label the entire dataset. The LF development workflow of
Panda is shown in Figure 3(2). We demonstrate these steps using a
real-world dataset Abt-Buy as follows:
Step 1: Uploading dataset and initialization. The user uploads
the dataset Abt-Buy by providing a file path and clicking on the
"Load data" button. Then, the system performs blocking and dis-
covers LFs automatically. A notebook that contains the discovered
LFs (e.g. auto_lf_0 in Figure 3(1)) is generated, and the user opens it
from the LF Authoring Panel. The discovered LFs are combined by
the labeling model to obtain EM & LF stats (e.g. number of found
matches), which are shown in the EM Stats Panel and LF Stats Panel.
Step 2: Viewing tuple pairs and coming up with LF ideas. The
user checks the current EM & LF stats and wants to find more
matches by manually writing more LFs. The user clicks on the
"Show" button. The system performs smart sampling and shows in
Data Viewer Panel some likely matching pairs that are abstained
or labeled as non-match by the current LFs. For example, the Data
Viewer Panel in Figure 3(1) shows such pairs. The user sees that for
tuple pairs that are matches, the "name" attribute likely have a high
overlap of words. She can write an LF, name_overlap, based on this
idea.
Step 3: Writing LF. The user goes to the LF Authoring Panel to
implement the idea. For preprocessing, she wants to convert every-
thing to lower case and split input by spaces. She can navigate to
the auto-generated LFs pre-populated in the same Jupyter notebook,
and directly copy/paste examples of these built-in utility functions
like "lower()" and "splitBySpace()" from auto_lf_0. She can further
copy/paste examples of distance functions like "jaccardDistance()",
from a different LF auto_lf_1.

Figure 3: (1)User interface of Panda. (2)The LF Development Workflow in Panda. Dashed line denotes operations automatically
done by the system; Red line denotes flow of debugging LF quality; Blue line denotes flow of discovering more matches; Purple
text denotes data being sent to the next step.

Having finished writing LFs, the user executes the last cell in the
notebook: "labeler.apply()". This incrementally applies the newly
written LFs and updates the EM Stats Panel and LF Stats Panel
accordingly. If the LFs have bugs causing errors, the user can turn
on the debug mode of Jupyter Lab by clicking on the top-right
toggle button. In the debug mode, user can debug the LF like in a
normal Python IDE, e.g. creating breakpoints, etc.
Step 4: Debugging LF quality. After a few iterations between
Step 2 and Step 3, the user has written quite a few LFs and run out
of ideas for new LFs. The user can now turn to inspect the overall
precision and coverage of existing LFs.

For example, in Figure 3(1), the user first clicks on "Estimated
FPR" to sort the LF Stats Panel by the false positive rate and finds
that the FPR of LF name_overlap is very high at 0.1402. Next, the
user can click on the value 0.1402: the tuple pairs that this LF labels
as 1 but the label model labels as -1 will be shown in Data Viewer
Panel. By examining the pairs, the user finds out that these false
positive pairs do not have enough word overlapping. As a result,
the user goes to the notebook and changes the threshold of being a
match in LF name_overlap from > 0.4 to > 0.6. After re-applying
the LF, the FPR of the LF decreases to 0.0094 and the user is satisfied.
Step 5: Estimate overall EM quality. After a few iterations in
Step 2, Step 3 and Step 4, the user has many high quality LFs. She
is satisfied with the stats and wants to know the overall precision
and recall. The user clicks on the value in the "Estimated Precision"
column in the EM stats column (initialized as "NAN" in Figure 3(1)).
The Data Viewer Panel will show a random sample of matches
predicted by the label model. The user can label these samples with
left/right clicks in the first column to mark them matches/non-
matches, after which an estimated precision will appear based on
these human labels. As for recall, the user can inspect tuple pairs
that are likely matches by clicking on the “Show” button, create
LFs for those tuple pairs (using Step 2 and Step 3), until no more
matches from the sample. This ensures that very few true-matches
are missing and the overall recall is high.

4 DISCUSSION AND FUTURE WORK
We note that Panda currently focuses on enabling developers to
interactively author LFs and produce high-quality EM solutions.

We plan to add features in Panda so that it can handle large tables
with millions of records, e.g., by down-sampling input data for LF
development, which can then be applied to the entire dataset in a
scale-out manner using infrastructures like Spark. In current work,
blocking is done by first obtaining a embedding vector for each
tuple using a pretrained sentence model [13] and then using LSH
of the embedding vectors to obtain candidate set of tuple pairs.

REFERENCES
[1] [n.d.]. Benchmark datasets for entity resolution. https://dbs.uni-leipzig.de/
research/projects/object_matching/benchmark_datasets_for_entity_resolution.
[2] [n.d.]. Magellan Data Repository. https://sites.google.com/site/anhaidgroup/

useful-stuff/data.

[3] Surajit Chaudhuri, Kris Ganjam, Venkatesh Ganti, and Rajeev Motwani. 2003.
Robust and Efficient Fuzzy Match for Online Data Cleaning (SIGMOD ’03). ACM.
[4] Xin Luna Dong and Theodoros Rekatsinas. 2018. Data Integration and Machine

Learning: A Natural Synergy. In SIGMOD. ACM, 1645–1650.

[5] Muhammad Ebraheem, Saravanan Thirumuruganathan, Shafiq Joty, Mourad
Ouzzani, and Nan Tang. 2018. Distributed Representations of Tuples for Entity
Resolution. In PVLDB.

[6] Ahmed K Elmagarmid, Panagiotis G Ipeirotis, and Vassilios S Verykios. 2007.

Duplicate Record Detection: A Survey. IEEETKDE 19, 1 (2007), 1–16.

[7] Pradap Konda, Sanjib Das, Paul Suganthan GC, AnHai Doan, Adel Ardalan, Jef-
frey R Ballard, Han Li, Fatemah Panahi, Haojun Zhang, Jeff Naughton, et al. 2016.
Magellan: Toward building entity matching management systems. Proceedings of
the VLDB Endowment 9, 12 (2016), 1197–1208.

[8] Hanna Köpcke, Andreas Thor, and Erhard Rahm. 2010. Evaluation of entity
resolution approaches on real-world match problems. PVLDB 3 (2010), 484–493.
[9] Peng Li, Xiang Cheng, Xu Chu, Yeye He, and Surajit Chaudhuri. 2021. Auto-
FuzzyJoin: Auto-Program Fuzzy Similarity Joins Without Labeled Examples
(SIGMOD ’21). ACM.

[10] Sidharth Mudgal, Han Li, Theodoros Rekatsinas, AnHai Doan, Youngchoon Park,
Ganesh Krishnan, Rohit Deep, Esteban Arcaute, and Vijay Raghavendra. 2018.
Deep Learning for Entity Matching: A Design Space Exploration. In SIGMOD.

[11] Alexander Ratner, Stephen H. Bach, Henry Ehrenberg, Jason Fries, Sen Wu,
and Christopher Ré. 2017. Snorkel: Rapid Training Data Creation with Weak
Supervision. Proc. VLDB Endow. 11, 3 (Nov. 2017), 269–282.

[12] Alexander J Ratner, Christopher M De Sa, Sen Wu, Daniel Selsam, and Christopher

Ré. 2016. Data Programming: Creating Large Training Sets, Quickly. In NIPS.

[13] Nils Reimers and Iryna Gurevych. 2019. Sentence-BERT: Sentence Embeddings

using Siamese BERT-Networks. In EMNLP.

[14] Renzhi Wu, Sanya Chaba, Saurabh Sawlani, Xu Chu, and Saravanan Thirumu-
ruganathan. 2020. ZeroER: Entity Resolution Using Zero Labeled Examples. In
Proceedings of the 2020 ACM SIGMOD International Conference on Management of
Data (Portland, OR, USA) (SIGMOD ’20). Association for Computing Machinery,
New York, NY, USA, 1149–1164. https://doi.org/10.1145/3318464.3389743
[15] Chen Zhao and Yeye He. 2019. Auto-EM: End-to-End Fuzzy Entity-Matching
Using Pre-Trained Deep Models and Transfer Learning (WWW ’19). ACM.

(1) Panda interface(2) Panda workflowData ViewerEM StatsLF StatsLF Authoring