A Language for Describing Optimization Strategies

Bastian Hagedorn
University of Münster, Germany
b.hagedorn@wwu.de

Johannes Lenfers
University of Münster, Germany
j.le@wwu.de

Thomas Koehler
University of Glasgow, UK
t.koehler.1@research.gla.ac.uk

Sergei Gorlatch
University of Münster, Germany
gorlatch@wwu.de

Michel Steuwer
University of Glasgow, UK
michel.steuwer@glasgow.ac.uk

0
2
0
2

b
e
F
6

]
L
P
.
s
c
[

1
v
8
6
2
2
0
.
2
0
0
2
:
v
i
X
r
a

Abstract
Optimizing programs to run efficiently on modern parallel
hardware is hard but crucial for many applications. The
predominantly used imperative languages - like C or OpenCL
- force the programmer to intertwine the code describing
functionality and optimizations. This results in a nightmare
for portability which is particularly problematic given the
accelerating trend towards specialized hardware devices to
further increase efficiency.

Many emerging DSLs used in performance demanding do-
mains such as deep learning, automatic differentiation, or im-
age processing attempt to simplify or even fully automate the
optimization process. Using a high-level - often functional -
language, programmers focus on describing functionality in
a declarative way. In some systems such as Halide or TVM,
a separate schedule specifies how the program should be
optimized. Unfortunately, these schedules are not written
in well-defined programming languages. Instead, they are
implemented as a set of ad-hoc predefined APIs that the
compiler writers have exposed.

In this paper, we present Elevate: a functional language
for describing optimization strategies. Elevate follows a tra-
dition of prior systems used in different contexts that express
optimization strategies as composition of rewrites. In con-
trast to systems with scheduling APIs, in Elevate program-
mers are not restricted to a set of built-in optimizations but
define their own optimization strategies freely in a compos-
able way. We show how user-defined optimization strategies
in Elevate enable the effective optimization of programs ex-
pressed in a functional data-parallel language demonstrating
competitive performance with Halide and TVM.

1 Introduction
The tremendous gains in performance and efficiency that
computer hardware continues to make are a key driving
force for innovation in computing. This enables entire new
areas of computing such as deep learning to deliver applica-
tions unthinkable even just a few years ago. With the end of
Moore’s law and Denard’s scaling [23], these gains no longer
come for free for software writers. Programs have to be opti-
mized for an increasing diverse set of hardware devices by
exploiting many subtle details of the computer architecture.
Performance portability has emerged as a crucial concern

as software naturally outlives the faster cycle of hardware
generations. In addition, specialized hardware has proven to
offer extreme benefits for performance and energy efficiency
- if the specially optimized software exploits it.

The predominant imperative and low-level programming
approaches such as C, CUDA, or OpenCL force programmers
to intertwine the code describing the functional behavior of
the program with optimization decisions. This makes them
– by design – non performance portable. As an alternative,
higher level domain-specific approaches have emerged that
allow programmers to declaratively describe the functional
behavior without committing to a specific implementation.
Popular examples of this approach are virtually all machine
learning systems such as TensorFlow [1] or PyTorch [27].
For these approaches, the compilers and runtime systems are
responsible to optimize the computations that are expressed
as data-flow graphs. Programmers have limited control about
the optimization process. Instead large teams of engineers at
Google and Facebook provide fast implementations for the
most common hardware platforms, for TensorFlow including
Google’s specialized TPU hardware. This labour intensive
support of new hardware devices is currently only sustain-
able for the biggest companies in the market – and even they
struggle [5]. To overcome this innovation obstacle and to
achieve automated performance portability we will need to
rethink how we separate, describe, and apply optimizations
in a more principled way.

Encoding program transformations as rewrite rules has
been a long established idea. Bird and de Moor [6] stud-
ied an algebraic programming approach where functional
programs are rewritten by exploiting algebraic properties.
The Glasgow Haskell Compiler allows the specification of
rewrite rules for program optimizations [28]. More recently,
Lift [33] encodes optimization and implementation choices
as rewrite rules for optimizing a high-level pattern-based
data-parallel functional language using an automated sto-
chastic search method applying the rewrites. Rewrite based
approaches, such as Lift, have the advantage of being easily
extensible towards new application domains (such as sten-
cils [21]) as well as supporting new hardware features (such
as specialized vector instructions which are encoded as new
low-level patterns and introduced by a rewrite rule [34]).
Unfortunately, these rewrite approaches are limited in their

 
 
 
 
 
 
Bastian Hagedorn, Johannes Lenfers, Thomas Koehler, Sergei Gorlatch, and Michel Steuwer

practicality to deliver the high performance required in many
real-world applications. They lack control over the rewriting
process and the automated rewriting using stochastic search
processes takes a long time to find a high performance im-
plementation. In this paper, we are going to address these
practical limitations of rewrite-based approaches for optimiz-
ing high-performance real-world applications by defining a
strategy language that allows the definition of optimization
strategies that precisely controls the rewrite process.

Halide [30, 31] has introduced the concept of separating
programs into functional descriptions and schedules in the
area of high-performance domain-specific code generators.
A schedule describes the optimizations to be applied to the
Halide algorithm that defines the functional behavior of the
computation. Halide’s schedules – as well similar schedules
in TVM [12] – are implemented using a set of predefined
APIs that expose a fixed set of optimization options. Halide’s
authors describe these APIs as a scheduling language but it
lacks many desirable properties of a programming language.
Most crucially, programmers are not able to define their own
abstractions. Even the composition of existing optimization
primitives is in some cases unintuitive due to the lack of a
clear semantics and Halide’s compiler has default and im-
plicit behavior limiting experts’ control. All of these reasons
make writing schedules in Halide significantly harder than
writing algorithms. Furthermore, for some desirable opti-
mizations it is not sufficient to change the schedule but the
algorithm itself has to be redefined – violating the promise
of separating algorithm and schedule. In this paper, we build
upon Halide’s general idea but provide a proper functional
strategy language, called Elevate, with clear semantics of
individual primitives and how they compose. It enables pro-
grammers to define their own abstractions for building opti-
mization strategies in a composable and reusable way.

The design of Elevate is heavily inspired by research on
strategy languages for rewrite systems used in other con-
texts – and largely unknown to the high-performance code
generation community – such as Stratego [37]. Kirchner [24]
provides a recent overview of the research of the rewriting
community. We claim no novelty in the design foundations
of strategy languages but instead in the strategies we present
and their usage to facilitate the generation of highly efficient
code on modern hardware.

Our paper makes the following key contributions:

• Description of the design of Elevate, a functional lan-
guage for describing optimization strategies for high-
performance code generation (Section 3);

• demonstration of Elevate using three case studies: au-
tomatic differentiation (section 4), image processing
(section 5) and deep learning (section 6). They show
the flexibility and extensibility of Elevate and experi-
mentally evaluate the practicality of a rewrite based
approach for achieving competitive high performance.

2 Motivation and Background
We motivate the need for a strategy language with a closer
look at Halide. We then argue for a more principled language
approach for describing optimizations strategies.

2.1 Halide: Decoupling Algorithm from Schedules

Halide [30] has originally been designed to generate high
performance code for image processing pipelines [31], but
has since inspired similar approaches in other contexts such
as TVM in deep leaning [12]. A crucial idea is the separation
of a program in two parts: the algorithm describing the func-
tional behavior, and the schedule specifying how the program
should be optimized by the underlying Halide compiler.

Listing 1 shows a snippet of Halide code used for generat-
ing an efficient matrix-matrix multiplication for an Nvidia
GPU. Halide is a DSL embedded in C++, so the syntax used
here is C++. The lines 2–4 define the matrix-matrix multipli-
cation computation: A and B are multiplied by performing

1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
34
35
36
37
38
39
40

// functional description of matrix multiplication
Var x("x"), y("y"); Func prod("prod"); RDom r(0, size);
prod(x, y) += A(x, r) * B(r, y);

out(x, y) = prod(x, y);

// schedule for Nvidida GPUs
const int warp_size = 32; const int vec_size = 2;
const int x_tile
= 4;
const int y_unroll = 8; const int r_unroll = 1;
Var xi,yi,xio,xii,yii,xo,yo,x_pair,xiio,ty; RVar rxo,rxi;
out.bound(x, 0, size).bound(y, 0, size)

= 3; const int y_tile

.tile(x, y, xi, yi, x_tile * vec_size * warp_size,

y_tile * y_unroll)
.split(yi, ty, yi, y_unroll)
.vectorize(xi, vec_size)
.split(xi, xio, xii, warp_size)
.reorder(xio, yi, xii, ty, x, y)
.unroll(xio).unroll(yi)
.gpu_blocks(x, y).gpu_threads(ty).gpu_lanes(xii);
prod.store_in(MemoryType::Register).compute_at(out, x)
.split(x, xo, xi, warp_size * vec_size, RoundUp)
.split(y, ty, y, y_unroll)
.gpu_threads(ty).unroll(xi, vec_size).gpu_lanes(xi)
.unroll(xo).unroll(y).update()
.split(x, xo, xi, warp_size * vec_size, RoundUp)
.split(y, ty, y, y_unroll)
.gpu_threads(ty).unroll(xi, vec_size).gpu_lanes(xi)
.split(r.x, rxo, rxi, warp_size)
.unroll(rxi, r_unroll).reorder(xi, xo, y, rxi, ty, rxo)
.unroll(xo).unroll(y);

Var Bx = B.in().args()[0], By = B.in().args()[1];
Var Ax = A.in().args()[0], Ay = A.in().args()[1];
B.in().compute_at(prod, ty).split(Bx, xo, xi, warp_size)

.gpu_lanes(xi).unroll(xo).unroll(By);

A.in().compute_at(prod, rxo).vectorize(Ax, vec_size)

.split(Ax,xo,xi,warp_size).gpu_lanes(xi).unroll(xo)
.split(Ay,yo,yi,y_tile).gpu_threads(yi).unroll(yo);
A.in().in().compute_at(prod, rxi).vectorize(Ax, vec_size)
.split(Ax, xo, xi, warp_size).gpu_lanes(xi)
.unroll(xo).unroll(Ay);

Listing 1. Matrix matrix multiplcation in Halide. Lines 2–
4 define the computation A × B, the other lines define the
schedule specifying the optimizations to be applied by
the compiler. From: https://github.com/halide/Halide/blob/master/apps/
cuda_mat_mul/mat_mul_generator.cpp.

Elevate: A Language for Describing Optimization Strategies

1
2
3
4
5
6

1
2
3
4

Var x, y; Func out; Func in = BC::repeat_edge(input);
out(x, y) = (

1.f * in(x-1,y-1) + 2.f * in(x,y-1) + 1.f * in(x+1,y-1) +
2.f * in(x-1,y)
+
1.f * in(x-1,y+1) + 2.f * in(x,y+1) + 1.f * in(x+1,y+1)

+ 2.f * in(x+1,y)

+ 4.f * in(x,y)

) * (1.f/16.f);

1
2
3

1

val bf = fun(3.3.float)(weights2d => fun(N.M.float)(img =>
img |> pad2D(1) |> slide2D(3)(1) |> map2D(fun(nbh =>

dot(join(weights2d))(join(nbh)) ))))

val sbf = (topDown(separateDot) ‘;‘ lowerToC)(bf)

Var x,y; Func b_x,b_y,out; Func in=BC::repeat_edge(input);
b_y(x, y) =
+ in(x, y+1);
b_x(x, y) = b_y(x-1, y) + 2.f * b_y(x, y) + b_y(x+1, y);
out(x, y) = b_x(x, y) * (1.f/16.f);

in(x, y-1) + 2.f * in(x, y)

Figure 2. 2D binomial filter in a Lift-like language (top) and
the separating optimization strategy in Elevate (bottom).

Figure 1. Two-dimensional binomial filter in Halide (top)
and separated version (bottom).

the dot product for each coordinate pair (x, y). The dot prod-
uct is expressed as pairwise multiplications and reducing
over the reduction domain r using the += operator (line 3).
The other lines in the listing define the schedule specifying
the optimizations to be performed. The Halide compiler takes
this C++ program and produces efficient GPU code coming
close to highly optimized low-level library code.

By looking at the code it is immediately clear that writing
a schedule is significantly more challenging than writing the
algorithm describing matrix-matrix multiplication. Sched-
ules are written using a sequence of API calls on the C++
objects that represent the input (A, B) and output (out) data.
prod represents the reduction operation in Halide’s internal
representation. While the algorithm and schedule are sep-
arated they still share the same C++ identifiers and must,
therefore, be written in the same C++ scope limiting the
reuse of schedules across algorithms.

This schedule uses 12 built-in optimization primitives
(bound, tile, split, vectorize, reorder, unroll, update,
compute at, store in, gpu blocks, gpu threads, gpu lanes).
Some of these optimizations are specific for the hardware
(like vectorize or gpu threads), others are generally use-
ful algorithmic optimizations for many applications (like
tiling to increase data locality), and others are low-level
optimizations (like unroll and reorder that transform loop
nests). Halide is not easily extensible. Adding a new opti-
mization primitive to the schedule API requires extending
the Halide compiler. Even a primitive like tile that can be
implemented with split and reorder1 is represented as a
composition but provided as a built-in abstraction. Halide’s
schedules lack the ability for user-defined abstractions.

The behavior of some primitives is not intuitive and the
documentation provides only informal descriptions, e.g., for
update: “Get a handle on an update step for the purposes of
scheduling it”. The lack of clear descriptions of the optimiza-
tion primitives makes reasoning about the schedule difficult.
For example, is it unclear to us why lines 21–23 are repeated
at lines 25–27 with calls to unroll and update in between.

1See: https://halide-lang.org/tutorials/tutorial_lesson_05_scheduling_1.html

Figure 1 shows the implementation of a two-dimensional
binomial filter in Halide. The listing on top shows the basic
Halide algorithm implementation by computing every out-
put pixel as a weighted sum of the 3 × 3 surrounding pixels.
A more efficient way is shown below where the computation
is separated into a vertical and horizontal filter operating on
3 pixels each. Separability is a well known domain-specific
optimization in image processing and only valid for appro-
priate weights. Halide does not offer this optimization as a
scheduling primitive and, therefore, programmers are forced
to change the algorithm for this optimization – clearly vio-
lating the promise of separating algorithm and schedule.

If no schedules are provided (as in fig. 1), the Halide com-
piler employs a set of implicit default optimizations that are
out of reach of the control of the user. This sometimes leads
to the surprising behavior that algorithms without a sched-
ule perform better (e.g., due to auto-vectorization) than ones
where a schedule is provided.

2.2 Towards an Optimization Strategy Language

Out of the shortcoming of the API approach we identify the
following desirable features for a strategy language:
1. Optimization strategies should be defined clearly sepa-
rated from the computational program facilitating reusablil-
ity of strategies across programs;

2. Strategies should be written as compositions of user-defined
strategies (possibly domain-specific ones); the language
should facilitate the creation of higher-level abstractions;
3. All strategies should have a clear semantics allowing rea-
soning about their application and implicit default behav-
ior should be avoided to empower users to be in control.
Essentially we argue that a strategy language should be build
with the same standards as a language describing computation.
In this paper we present such a language: Elevate.

Figure 2 shows an example of an Elevate strategy (bot-
tom) that optimizes a program (top) written in a completely
separate pattern-based language similar to Lift [21] called
Rise. The optimization strategy is a sequential composition
(‘;‘) of user defined strategies providing a higher level of
abstraction. Strategies are build as compositions of rewrite
rules with a clear semantics and no implicit behavior.

In the remainder of the paper, we will explain the language
for writing such strategies and defining custom abstractions.

Bastian Hagedorn, Johannes Lenfers, Thomas Koehler, Sergei Gorlatch, and Michel Steuwer

3 Elevate: A Language for Describing

Optimization Strategies

In this section, we describe our language for describing opti-
mization strategies: Elevate. It is heavily inspired by earlier
works on strategy languages for term rewriting systems,
particularly Stratego [41]. Our key contribution is not the
design of Elevate itself but rather its application to define
optimization for high-performance code generators. In this
section, we focus on introducing Elevate as a practical pro-
gramming language. For a more formal treatment of strategy
languages see [39].

3.1 Language Features and Types
Elevate is a functional language with a standard feature
set including function recursion, algebraic data types and
pattern matching. Besides the standard scalar data types such
as int, types of interests are function types, tuple types and
list types. Our current implementation is an embedded DSL
in Scala and we use Scala-like notation in the paper.

3.2 Strategies
A strategy is the fundamental building block of Elevate.
Strategies encode program transformations and are modeled
as functions with the following type:

type Strategy[P] = P => RewriteResult[P]

Here P is the type of the program that is rewritten. P could
for example be Rise for programs written in the Rise lan-
guage from fig. 2. A RewriteResult[P] is an algebraic datatype
encoding the success or failure of applying a strategy to a
program:

RewriteResult[P] = Success[P](p: P)

| Failure[P](s: Strategy[P])

In case of a successful application, Success contains the trans-
formed program, in case of a failure, Failure contains the
strategy that has been unsuccessful.

The simplest example of a strategy is the id strategy which

always succeeds and returns its input program:

def id[P]: Strategy[P] = (p: P) => Success(p)

The fail strategy does the opposite and always fails while
recording that the fail strategy was the one failing:

def fail[P]: Strategy[P] = (p: P) => Failure(fail)

3.3 Rewrite Rules as Strategies
In Elevate, rewrite rules are also strategies, i.e., functions
satisfying the same type given above. Let’s look at a concrete
type of programs, such as Rise which is a pattern-based
functional programming language where we want to apply
well-known rewrite rules such as the fusion of two map calls:
(map f ) ◦ (map g) (cid:55)→ map (f ◦ g). In Rise, the left-hand side
of the rule is expressed as:

val p: Rise = fun(xs => map(f)(map(g)(xs)))

Figure 3. Rise’s map-fusion rule as an AST transformation.

The AST representation of the body of this is shown in fig. 3
on the left, with function applications explicit as app nodes.
This is the implementation of the fusion rule in Elevate:
def mapFusion: Strategy[Rise] = p => p match {

case app(app(map, f), app(app(map, g), xs)) =>

Success( map(fun(x => f(g(x))))(xs) )

case _ => Failure(mapFusion)
Note that we are mixing expressions of the Rise language
(i.e., map(f)) and Elevate. The expression nested inside Success
is the rewritten expression shown in fig. 3 on the right.

}

3.4 Strategy Combinators
A key idea that Elevate inherits from Stratego [39] is to de-
scribe strategies as compositions of other strategies. There-
fore, we introduce strategy combinators.

The seq combinator is given two strategies fs and ss and
applies the first strategy to the input program p. Afterwards,
the second strategy is applied to the result.

def seq[P]: Strategy[P] => Strategy[P] => Strategy[P] =

fs => ss => p => fs(p).flatMapSuccess(ss)

The seq strategy is only successful when both strategies are
successfully applied in succession, otherwise seq fails.

In the implementation of seq, we make use of the monadic
interface of strategies: the RewriteResult ADT provides two
versions of map/flatMap to compose strategies – one in case of
a successful strategy application and one in case of failure.
The lChoice combinator is given two strategies and applies

the second strategy only if the first strategy failed.

def lChoice[P]: Strategy[P] => Strategy[P] => Strategy[P] =

fs => ss => p => fs(p).flatMapFailure(_ => ss(p))

We use <+ as notation for lChoice and ‘;‘ for seq and

define two more combinators:

The try combinator applies a strategy and in case of failure

applies the identity strategy. Therefore, try never fails.

def try[P]: Strategy[P] => Strategy[P] =

s => p => (s <+ id)(p)

repeat applies a strategy until it is no longer applicable.

def repeat[P]: Strategy[P] => Strategy[P] =

s => p => try(s ‘;‘ repeat(s) )(p)

map(f)(map(g)(xs))map(fun(x �� f(g(x))))(xs)Elevate: A Language for Describing Optimization Strategies

Figure 4. Two possible locations for applying the map-fusion
rule within the same program.

3.5 Traversal Strategies

The mapFusion strategy we saw in the previous subsection is
implemented as a function in Elevate. Therefore, its match
statement will try to pattern match its argument – the entire
program. This means that a strategy on its own is very hard
to reuse in different circumstances.

In addition, a strategy is often applicable at multiple places
within the same program or only applicable at a specific
location. For example, the mapFusion strategy is applicable
twice in the following Rise program:
val threemaps = fun(xs => map(f)(map(g)(map(h)(xs))))

We may fuse the first or last two maps as shown in fig. 4.

In Elevate, we use traversal strategies to describe at which
exact location a strategy is applied. Luttik et al. [25] proposed
three basic traversal strategies:

def

all[P]: Strategy[P] => Strategy[P]
one[P]: Strategy[P] => Strategy[P]
def
def some[P]: Strategy[P] => Strategy[P]

all applies a given strategy to all sub-expressions of the
current expression and fails if the strategy is not applicable
to all sub-expressions. one applies a given strategy to exactly
one sub-expression and fails it the strategy is not applicable
to any sub-expression. some applies a given strategy to at
least one sub-expression but potentially more if possible.
one and some are allowed to non-deterministically choose
sub-expressions.

In Elevate, we see these three basic traversal strategies as
a type class: an interface that has to be implemented for each
program type P. The implementation for Rise is straightfor-
ward. Rise programs are represented by ASTs such as the
one in fig. 4, therefore, all, one, and some correspond to the
obvious implementations on the tree-based representation.

To fuse the first two maps in fig. 4 we use the one traver-
sal strategy: one(mapFusion)(threemaps). This will apply the
mapFusion strategy not at the root of the AST, but instead
one level down first trying to apply the strategy (unsuccsess-
fully) to the function parameter and then (successfully) to
the function body highlighted in the upper-right blue box.
To fuse the last two maps we use the one traversal strategy
twice to apply mapFusion two levels down in the AST: one(one
(mapFusion))(threemaps). This sucessfully applies the fusion
strategy to the expression highlighed in the lower-left purple
box in fig. 4.

3.6 Language-Specific Traversal Strategies

The traversals we have discussed so far are not specific to
a particular language, such as Rise. These traversals are
flexible, but offer only limited control as for one and some the
selection of sub-expressions is either non-deterministic, or
implementation-dependent (as for Rise) and in our context it
makes rarely sense to apply a strategy to all sub-expressions.
In Elevate, one can easily specify program language spe-
cific traversal primitives. Rise is a functional language using
λ-calculus as its representation. Therefore, it makes sense to
introduce traversals that navigate the two core concepts of
λ-calculus: function abstraction and application.

To apply a strategy to the body of a function abstraction

we define the following traversal strategy:

def body(s: Strategy[Rise]): Strategy[Rise] =

p => p match
case fun(x,b) => s(b).mapSuccess(nb => fun(x,nb))
case _ => Failure(s)

{

}

A strategy s is applied to the function body and if success-

ful a function is build around the transformed body.

Similarly we define traversals function and argument to

traverse function applications:

def function(s: Strategy[Rise]): Strategy[Rise] =

p => p match
case app(f,a) => s(f).mapSuccess(nf => app(nf, a))
}
case _ => Failure(s)

{

def argument(s: Strategy[Rise]): Strategy[Rise] =

p => p match
{
case app(f,a) => s(a).mapSuccess(na => app(f, na))
}
case _ => Failure(s)
For the Rise program shown in fig. 4, we are now able to
precisely describe a traversal path in the AST. To fuse the
first two maps we may write body(mapFusion)(threemaps), and
to fuse the others body(argument(mapFusion))(threemaps).

The traversals defined here are specific to Rise but similar
traversals are obviously possible for any functional language.
If the program is not a functional language, say e.g., a com-
putational graph as used by Tensorflow, different language-
specific traversals (e.g., leftOperand and rightOperand) could
be defined to describe language-specific traversals.

fun(xs �� map(f)(map(g)(map(h)(xs))))Bastian Hagedorn, Johannes Lenfers, Thomas Koehler, Sergei Gorlatch, and Michel Steuwer

3.7 Complete Expression Traversal Strategies

All of the traversal primitives introduced so far apply their
given strategies only to immediate sub-expressions.

Using strategy combinators and traversals, we are able to
define recursive strategies which traverse entire expressions:

def topDown[P](s: Strategy[P]): Strategy[P] =

p => (s <+ one(topDown(s)))(p)

def bottomUp[P](s: Strategy[P]): Strategy[P] =

p => (one(bottomUp(s)) <+ s)(p)

def allTopDown[P](s: Strategy[P]): Strategy[P] =

p => (s ‘;‘ all(allTopDown(s)))(p)

def allBottomUp[P](s: Strategy[P]): Strategy[P] =

p => (all(allBottomUp(s)) ‘;‘ s)(p)

def tryAll[P](s: Strategy[P]): Strategy[P] =
p => (all(tryAll(try(s))) ‘;‘ try(s))(p)

topDown and bottomUp are useful strategies traversing an
expression either from the top or from the bottom, trying
to apply a given strategy at every sub-expression and stop-
ping at the first successful application. If the strategy is not
applicable at any sub-expression, topDown and bottomUp fail.
allTopDown and allBottomUp do not use lChoice insisting

on applying the given strategy to every sub-expression.

The tryAll strategy is often more useful as it wraps its
given strategy in a try and thus never fails but applies the
strategy wherever possible. Also note that the tryAll strategy
traverses the AST bottom-up instead of top-down.

These traversals have also been proposed by Visser [39]
and we use them here with slightly different names more
fitting for our use cases.

3.8 Normalization, Confluence and Termination

When implementing rewrite rules, such as the mapFusion rule,
as strategies, the match statement expects the program ex-
pression to be in a particular syntactic form. For a functional
language like Rise, we might for example expect that expres-
sions are fully β-reduced. To ensure that expressions satisfy
a normal form we define:

def normalize[P](s: Strategy[P]): Strategy[P] =

p => repeat(topDown(s))(p)

The normalize strategy applies a given strategy repeatedly
at every possible sub-expression until it can not be applied
any more. Therefore, after normalize successfully finishes it is
not possible to apply the given strategy to any sub-expression
any more. By defining a strategy for β-reduction and using
it together with normalize we ensure that expressions are in
β-normal-form.

Confluence (multiple non-deterministic rewrite paths even-
tually produce the same result) and termination are desirable
properties for normal forms in term rewriting systems [39].
In Elevate, confluence only becomes a factor when the im-
plementation of one and some are non-deterministic. This can
often be avoided such as for the use cases we consider with
Rise and (cid:101)F that we will discuss in section 4.

Termination of normal forms critically depends on the cho-
sen set of strategies. Therefore, reasoning about terminating
normal forms must be done on a case by case basis. For exam-
ple, it is trivial to build a non-terminating normal form using
the id strategy that is always applicable. We currently, do not
prevent the creation of non-terminating strategies similar as
almost all general purpose computational languages do not
prevent writing non-terminating programs. In the future, we
are interested to introduce a richer type system for Elevate
to better assist the user in writing well behaved strategies.

3.9 Summary
We have introduced Elevate, a language for describing op-
timization strategies. In the next three sections we discuss
three case studies of using Elevate in the domains of auto-
matic differentiation, image processing, and deep learning.

4 Case Study 1: Automatic Differentiation
So far we have seen Rise as the only example of a language
that we transform with Elevate, but Elevate is flexible and
not restricted to a single language. In this first case study, we
will look at the (cid:101)F language that has been introduced in [32].
(cid:101)F is a small functional language capable of automatically
computing the derivative of arbitrary (cid:101)F functions. Imple-
menting automatic differentiation is not too difficult but
making it efficient is non-trivial. (cid:101)F achieves efficiency by
rewriting the differentiated code. In the paper, rewrite rules
are specified alongside examples. Example 5 in the paper
shows that a (cid:101)F program transposing a matrix twice can be
rewritten into a program without transposition, see fig. 5.

The paper does not provide an explanation how the rewrit-
ing between these programs happens or is specified. The
authors only state “by applying the loop fusion rules and per-
forming further partial evaluation the expression is derived”.
We are interested in exploring the flexibility of Elevate
and if we can specify the rewrite rule applications program-
matically. We implemented the (cid:101)F language representing ex-
pressions using an algebraic data type FSmooth. We imple-
mented the rewrite rules from the paper, such as fusion rules:

(build e0 e1)[e2]
length (build e0 e1) (cid:59) e0

(cid:59) e1 e2

1 let MT = build (length M[0]) (fun i ->
2
3
4

build (length M) (fun j -> M[j][i] ) ) in
build (length MT[0])(fun i ->
build (length MT) (fun j -> MT[j][i] ) )

1 build (length M) (fun i ->
2

build (length M[0]) (fun j -> M[i][j] ) )

Figure 5. Transposing a matrix twice in (cid:101)F (top) and the
rewritten program without transposition (bottom). From [32]

Elevate: A Language for Describing Optimization Strategies

These fusion rules are implemented as Elevate strategies:
def buildGet(p: FSmooth): Strategy[FSmooth] = p match
case app(get,(app(build,(e0,e1)),e2)) => Success(app(e1,e2))
Failure(buildGet) }
case _ =>

{

def lenBuild(p: FSmooth): Strategy[FSmooth] = p match
case app(length, app(build, (e0, e1))) => Success(e0)
case _
=> Failure(lenBuild) }
Using these rewrite rules encoded as Elevate strategies,
we use normalize and lChoice to specify multiple strategies
that should be applied repeatedly at every sub-expression:

{

normalize(buildGet <+ lenBuild <+ letPartialEval <+

letApp <+ funToLet)(mt)

This Elevate program successfully rewrites the doubly-
transposed (cid:101)F program into the non-transposed form. By
tracing the execution of the Elevate program we get a se-
quence of 12 basic rewrite rule applications explaining the
program transformation step-by-step. A full version of the
trace is shown in the supplementary material.

We also implemented the other examples in the paper
which contain rewriting in Elevate– and identifying a minor
bug in the description of example 6.

This case study shows Elevate’s flexibility to implement
an existing rewrite system. It also demonstrates Elevate’s
ease of use: we did not have to think about where to apply
the individual rewrite rules thanks to the abstraction pro-
vided by normalize. It is important to stress, that this is not a
built-in abstraction but defined itself in terms of the smaller
building blocks repeat and topDown. In the next case-study,
we discuss how the ability to leverage these abstractions
and build custom ones enables the definition of optimization
strategies for image processing.

5 Case Study 2: Image Processing
In this case study, we are interested to see how our extensible
strategy language Elevate compares directly to Halide. We
will look at the binomial filter example which we already
briefly showed in fig. 1 and 2. We will use Rise as our com-
putational language. Rise is a Lift [33]-like language that
is compiled by rewriting high-level pattern-based programs
into a low-level representation encoding implementation
and optimization choices. It then uses a compilation process
similar to [3] to compile the low-level pattern-based code
into parallel imperative code.

5.1 Halide Schedules for the Binomial Filter

For Halide, we saw two algorithms describing the compu-
tation of the binominal filter in fig. 1. The naive version
on the top is a straightforward implementation as a two-
dimensional stencil. The separated version on the bottom
specifies the computation as a composition of vertical and
horizontal one-dimensional stencils. In Halide, it is not possi-
ble to take the naive algorithm and use a schedule to specify
the separability optimization. When we do not specify a

schedule in Halide, a default schedule is chosen that inlines
computations as much as possible. Therefore, the naive and
the separated version with the default schedule result in
code executing two nested loops and accessing 9 elements
of the input image. We can instruct Halide not to fully in-
line the vertical filter by writing: b_y.compute_at(out, y); This
schedule uses the variable names from the Halide algorithm
implementing the separated version of binomial filter. The
compute_at schedule instruction tells the compiler to perform
the computation of the vertical filter (b_y) inside the for loop
iterating over the y dimension of the out image. This com-
putation will be stored in a temporary that is consumed in
the nested loop iterating over the x dimension of out. We call
this version scanline as the temporary stores an entire line
of the image.

For all versions, it is possible to parallelize the outermost

loop with: out.parallel(y);

5.2 Elevate Strategies Optimizing Binomial Filter
To compare with Halide, we express the binomial filter with
Rise as shown at the top of fig. 2. This formulation is the
naive way to describe a two-dimensional filter in Rise. The
filter is expressed using two-dimensional variations of the
pad, slide, and map high-level patterns in the style of [21]. pad
models the boundary handling, the sliding window pattern
slide describes a neighborhood of values each of which is
then processed by the map pattern. The 2D versions of these
pattens are just macros defined as compositions of the one-
dimensional versions and a few additional basic patterns. The
dot product computation is also defined as a composition:

dot(x)(y) = reduce(add)(0)(map(mult)(zip(x)(y)))
To express the separability as an Elevate strategy we intro-
duce an image-processing specific rewrite rule that describes
how the dot product of the weights and the neighborhood
inside the binomial filter is separated:

def separateDot(w2d:Rise, wh:Rise, wv:Rise): Strategy[Rise] =
p => p match
case app(app(app(reduce, add), 0), app(app(map, mult),

{

app(app(zip, app(join, w)), app(join, nbh)))) if w==w2d =>

Success(nbh |> map(dot(wh)) |> dot(wv))

case _ => Failure(sepDot)

}

The underlined values in the pattern matching indicate that
these must match for the pattern matching to succeed.

The strategy takes three parameters that are all expres-
sions in the computational language Rise. w2d represents the
two-dimensional weights and wh and wv are the separated
horizontal and vertical weights. Elevate does not automat-
ically attempt to separate the weights nor does it attempt
to prove that the horizontal and vertical weights are a valid
separation of the two-dimensional weights. These considera-
tions are left to the user. We aim to empower users to extend
Elevate which such domain-specific strategies.

Bastian Hagedorn, Johannes Lenfers, Thomas Koehler, Sergei Gorlatch, and Michel Steuwer

To rewrite the naive binominal filter into the separated
one, we apply separateDot with topDown as shown earlier as
an Elevate strategy at the bottom of fig. 2 resulting in:

1
2

img |> pad2D(1) |> slide2D(3)(1) |> map2D(fun(nbh =>
nbh |> map(dot(weightsH)) |> map(dot(weightsV)) ))

We express the lowering of high-level Rise expressions
with Elevate strategies that encode low-level implementa-
tion choices such as whether we would like to parallelize
or not. In Halide, the built-in compute_at primitive is used for
the scanline version. In Elevate a user-defined strategy en-
codes the same transformation. We will investigate more
complex optimization and low-level implementation strate-
gies in more detail in the third case study in section 6.

5.3 Performance Evaluation
Even though this paper discusses Elevate and not Rise we
want to evaluate whether a rewrite-based approach by com-
bining them is capable of achieving competitive performance
compared to the industry-strength Halide compiler that uses
more traditional compiler techniques. We have seen that El-
evate allows the definition of optimization strategies in an
extensible way and that this allows to express optimizations
as strategies that are not expressible as Halide schedules
(such as separability). We are now interested to see if these
strategies encoding the same optimization decisions lead
to competitive performance when compiled with Rise and
compared to Halide.

Figure 6 shows the performance of the Halide and Rise
generated code measured on a ARM Cortex A7 quad-core2.
We can see – not surprisingly – that the non-parallel versions
on the left are significantly slower than the parallel versions.
The Halide generated code is 10-15% faster than the Rise
generated code. Improvements to the Rise compiler might
close this gap in the future. Crucially, we observe the same
trend for performance improvements due to optimizations.
This demonstrates that our extensible and rewrite based
approach is capable of achieving competitive performance.

2The 4 LITTLE cores of a Samsung Exynos 5 Octa 5422

s

m
n
i

e
m

i
t
n
u
R

120

90

60

30

0

Halide
Elevate+Rise

naive

scanline

naive-par

scanline-par

separated

separated
-par

Figure 6. Performance evaluation of Halide and Rise gener-
ated code for the binomial filter application. Optimization
decisions for Rise are implemented as Elevate strategies.

6 Case Study 3: Deep Learning
In our first two case studies, we looked at fairly simple strate-
gies. In our final case study, we are interested to explore
the practicability and scalability of Elevate. We are look-
ing at the domain of deep learning where performance op-
timizations are particularly important. In this section, we
explore the implementation of a scheduling language with
Elevate using the ability to define custom abstractions. We
use TVM [12] as an example for a state-of-the-art optimizing
deep learning compiler with a scheduling API implemented
in Python, similar to Halide. We use Rise again as the target
language for the strategies we develop.

We start by looking at how TVM represents schedules
and how we implement simple scheduling primitives such
as parallel and vectorize in Elevate. Then we show how
to implement more complex scheduling primitives like tile
using composition in Elevate whereas it is built-in in TVM.
We follow a tutorial from the TVM authors3 that discusses
seven differently optimized versions of matrix multiplication.
For each one, we show an equivalent strategy implemented
with Elevate and evaluate the performance achieved.

6.1 TVM Schedules for Matrix Multiply

In TVM, computations are expressed similar to TensorFlow:

1
2

C = tvm.compute((M, N), lambda x, y:

tvm.sum(A[x,k] * B[k,y], axis=k), name='C')

The “How to optimize GEMM on CPU” 3 tutorial discusses
seven versions applying different optimizations. The baseline
version uses a default schedule with no instructions. The
blocking version tiles the two outermost loops computing
matrix C and then splits the reduction loop before reordering
the loop nest:

1
2
3
4
5

# blocking version
xo, yo, xi, yi = s[C].tile(C.op.axis[0],C.op.axis[1],32,32)
k,
ko, ki
s[C].reorder(xo, yo, ko, ki, xi, yi)

= s[C].op.reduce_axis
= s[C].split(k, factor=4)

The vectorized version vectorizes the innermost loop and,
in addition, the loop permutation version changes to a cache
friendly access of matrix A by reordering loops in a different
way (switching ki and xi):

1
2
3
4
5
6

# loop permutation version
xo, yo, xi, yi = s[C].tile(C.op.axis[0],C.op.axis[1],32,32)
k,
ko, ki
s[C].reorder(xo, yo, ko, xi, ki, yi)
s[C].vectorize(yi)

= s[C].op.reduce_axis
= s[C].split(k, factor=4)

# added already in vectorized version

The array packing version enables better memory accesses
to matrix B by introducing a temporary packedB but requires
changing the description of the computation in TVM:

3https://docs.tvm.ai/tutorials/optimize/opt_gemm.html

Elevate: A Language for Describing Optimization Strategies

1
2
3
4
5

packedB = tvm.compute((N / 32, K, 32), lambda x, y, z:

B[y, x * 32 + z], name='packedB')

C

= tvm.compute((M, N), lambda x, y:

tvm.sum(A[x,k] * packedB[y // 32, k, tvm.indexmod(y,32)],

axis=k), name='C')

With this rewritten computation we can now also describe
the schedule affecting the computation of the temporary
packedB matrix. Contrary to Halide, this computation is not
inlined by default in TVM and results in a separate loop nest.

1
2
3
4

# same as loop permutation version
x, y, z = s[packedB].op.axis
s[packedB].vectorize(z)
s[packedB].parallel(x)

The write cache for blocks version allocates memory for the
reduction accumulator and unrolls loops. Building up on this,
the fully parallel version parallelizes the outermost loop:

1
2
3
4
5
6
7
8
9
10
11
12
13
14

= s[CC].op.axis
= s[CC].op.reduce_axis
= s[CC].split(k, factor=4)

# parallel version
CC = s.cache_write(C, 'global')
xo, yo, xi, yi = s[C].tile(C.op.axis[0],C.op.axis[1],32,32)
xc, yc
k,
ko, ki
s[CC].reorder(ko, xc, ki, yc)
s[CC].unroll(ki)
s[CC].compute_at(s[C], yo)
s[CC].vectorize(yc)
s[C].parallel(xo)
x, y, z
s[packedB].vectorize(z)
s[packedB].parallel(x)

= s[packedB].op.axis

In the rest of the case study, we attempt to implement a
similar looking scheduling language in Elevate for rewriting
Rise programs. We define abstractions ourselves similar to
the built-in scheduling primitives provided by TVM.

6.2 Basic Scheduling Primitives as Elevate Strategies
The TVM scheduling primitives parallel, split, vectorize, and
unroll can be implemented as rewrite rules for Rise.
def parallel: Strategy[Rise] = p => p match

{

case map => Success( mapPar )
case _

=> Failure( parallel )

}
In Rise, low-level implementation choices such as performing
a computation in parallel are encoded with low-level patterns.
For example, the map pattern that applies a function to each
element of an array might be performed in a data-parallel
fashion as indicated by the mapPar variant of the pattern. This
is precisely what the parallel strategy encodes: rewriting
a map pattern into its parallel variant. A rewrite into the
sequential variant mapSeq is defined in the same style.

def split(n: Int): Strategy[Rise] = p => p match

{

case app(map, f) =>

Success( split(n) >> map(map(f)) >> join )

case app(app(reduce, op), init) =>

Success( split(n) >> reduce(fun(a => fun(y =>

op(a, reduce(op)(init)(y)) )))(init) )

case _ => Failure( split(n) )

}

TVM’s split scheduling primitive implements loop-blocking
(also known as strip-mining). In Rise, this is achieved by
transforming the computation over an array expressed by
map(f): first the input is split into an 2D array using split(n),
then f is mapped twice to apply the function to all elements
of the now nested array, and finally the resulting array is
flattened into the original one-dimensional form using join.
We write f >> g to indicate reverse function composition,
i.e.,: fun(x => g(f(x))). It is important to note, that Rise does
not materialize the intermediate two-dimensional array in
memory, but only uses this representation inside the com-
piler for code generation. There is a second case for splitting
reduce resulting in two nested reductions.

def vectorize(n: Int): Strategy[Rise] = p => p match {

case app(map, f) if isScalarFun(f) =>

Success(asVector(n) >> map(mapVec(f)) >> asScalar)
}

case _ => Failure( vectorize(n) )

Vectorization is most efficient when applied to the innermost
loop of a loop-nest. In Rise, this corresponds to applying the
vectorize strategy to the innermost map of potentially nested
maps. This is achieved in Elevate by bottomUp(vectorize).
The extra constraint isScalarFun(f) ensures that only func-
tions operating on scalars are vectorized by inspecting f’s
type. The restriction to scalar functions for vectorization is
a current limitation of Rise.

def unroll: Strategy[Rise] = p => p match
=> Success( mapSeqUnroll )

case map
case reduce => Success( reduceSeqUnroll )
case _

=> Failure( unroll )

{

}

The unroll strategy rewrites the high-level map and reduce
patterns into Rise low-level patterns that will be unrolled by
the Rise compiler.

Identifying Locations
In TVM, named identifiers describe
the location at which the optimization should be applied.
For example, TVM’s parallel is invoked with an argument
specifying the loop to parallelize. Using named identifiers
allows writing invalid schedules, e.g., trying to vectorize a
reduction axis failing at runtime when TVM detects the error.
Elevate does not use names to identify locations, but
instead uses the traversals we defined in section 3. The
vectorize strategy is – by construction – only applicable
at valid locations within the AST.

By using Elevate’s traversal strategies, we can apply the
basic scheduling strategies in a much more flexible way: e.g.,
topDown(parallel) traverses the AST from top to bottom and
will thus always parallelize the outermost map, correspond-
ing to the outermost for loop. tryAll(parallel) traverses the
whole AST instead and all possible maps are parallelized. De-
pending on the desired use case users are free to combine
different traversals with the basic scheduling strategies.

Bastian Hagedorn, Johannes Lenfers, Thomas Koehler, Sergei Gorlatch, and Michel Steuwer

6.3 Tiling as an Elevate Strategy
Tiling is an important optimization improving the cache hit
rate by exploiting locality within a small neighborhood of
elements. TVM’s tile is a more complicated scheduling prim-
itive to implement because it is essentially a combination of
two traditional loop transformations: loop-blocking and loop-
interchange. In fact, tile in TVM is a built-in combination of
split for loop-blocking and reorder for loop-interchange. We
already saw how to implement split using Elevate and we
will now discuss how to implement tile using a combination
of rules, normal-forms and domain-specific traversals.

We use matrix-matrix multiplication as the illustrative
example to explain the optimization steps. Since we use Rise
as the target language for the tiling strategy, we need to
consider how matrix multiplication is expressed there:

1
2
3
4
5
6
7

val dot = fun((a,b) => zip(a,b) |> map(*) |> reduce(+,0))
val mm = fun(a :: M.K.float => fun(b :: K.N.float =>

map( fun(arow =>

// iterating over M

map( fun(bcol =>
dot(arow, bcol)

)(transpose(b))

)(a)

// iterating over N

// iterating over K

Essentially, the dot product is computed for each combina-
tion of rows and columns of matrix A and B.

In the following, we show how to use Elevate to system-
atically construct a strategy out of simple building blocks
that has the same effect as TVM’s tile scheduling primitive.
Specifically, we construct a generalized strategy that is able
to tile an arbitrary number of dimensions whereas TVM’s
tile only tiles in two-dimensions.

def tileND: List[Int] => Strategy[Rise]

This strategy expects a list of tile sizes, one per tiled dimen-
sion. The two-dimensional tiling, that is equivalent to TVM’s
built-in tile scheduling primitives, is expressed as tileND(
List(x,y))(mm) for this 2D case we also write tile(x,y)(mm).
The intuition for our tileND implementation is simple:
First, we ensure that the required rules are applicable to the
input expression by normalizing the expression using the
DFNF normal form. Then, we apply the previously introduced
split strategy to every map to be blocked, recursively go-
ing from innermost to outermost. Finally, we interchange
dimensions accordingly.

def tileND(n: List[Int]): Strategy[Rise] =

DFNF ‘;‘ (n.size match {

case 1 => function(split(n.head)) // loop-blocking
case i =>

fmap(tileND(d-1)(n.tail)) ‘;‘
function(split(n.head))
‘;‘
interchange(i)

})

// recurse

// loop-blocking

// loop-reorder

In the following, we first introduce the required normal-
forms (e.g., DFNF), then explain how we recursively traverse
(fmap) to apply loop-blocking and finally briefly explain how
we interchange dimensions in Rise (interchange).

In Elevate, we introduced normalize to en-
Normal forms
sure that expressions are in a particular form expected by
the implementation of rewrite rules. λ-calculus (and Rise)
allows for semantically equivalent but syntactically different
expressions. For example, fun(x => f(x)) is equivalent to f
iff x does not appear free in f. Transforming between these
representations is called η-reduction and η-abstraction.

The simplest normal-form we use is the βη-normal-form

(BENF) which exhaustively applies β- and η-reduction:

def BENF = normalize(betaReduction <+ etaReduction)

As not every function abstraction is η-reducible, the func-
tion arguments of Rise’s higher-order patterns map and reduce
might have different forms. Therefore, we introduce a nor-
mal form making the data flow explicit by ensuring a function
abstraction is present in every higher order pattern:

def DFNF = BENF ‘;‘
// the argument of a map is a function abstraction
normalize(argOf(map, not(isFun)‘;‘etaAbstraction))‘;‘
// ... similar normalization for reduce primitive

The definition shows the normalization for map. A similar
case exists for reduce. Using the not and isFun predicates, that
are themselves Elevate strategies, we describe the desired
form in a natural and elegant way.

Recursively Applying Loop-Blocking In order to recur-
sively apply the loop blocking strategy to nested maps, we
make use of the Rise-specific traversal fmap:
def fmap(s:Strategy[Rise])=function(argOf(map,body(s)))

fmap essentially traverses to the function argument of a map
primitive and applies the given strategy s. For example,

fmap(fmap(split(n)))(DFNF(map(map(map(f)))))

skips two maps applying loop-blocking to the innermost map.

Interchange in tile After recursively blocking all map
s, we use interchange to rearrange the dimensions in the
correct order. For simplicity, we describe the two dimensional
case: after loop-blocking the data is four-dimensional and
we must swap the two inner dimensions. To achieve this we
introduce two transpose patterns and then move one of the
transpose into the right position. Doing it this way every
strategy is a semantics-preserving transformation ensuring
the correctness of the overall optimization.

6.4 Reordering as an Elevate Strategy
Due to the loopless nature of Rise, implementing TVM’s
reorder primitive as a strategy is more complicated. Instead
of simply interchanging perfectly nested loops, the same
is achieved in Rise by interchanging the nesting of map and
reduce patterns. Therefore, there are multiple possible combi-
nations to consider and the implementation of each rewrite
rule requires reasoning about why exchanging the specific
patterns is possible in the first place.

Elevate: A Language for Describing Optimization Strategies

We implemented a reorder strategy in Elevate but its im-
plementation is non-trivial and, therefore, it is not discussed
here. While it is possible to implement TVM’s reorder primi-
tive, this particular loop transformation is just not a good fit
for the pattern-based abstractions in the Rise language.

6.5 Matrix Multiply Schedules as Elevate Strategies
Using the TVM-like scheduling abstractions implemented as
Elevate strategies, we are now able to discuss how we com-
bine them together to describe entire schedules in Elevate.
For baseline, TVM does not provide a schedule, but we

describe the implicit behaviour of the compiler explicitly:

For the parallel version, we reuse the prior array packing
strategy only changing the way we lower the high-level code.
We parallelize the outermost loop with topDown(parallel)
and unroll the innermost reduction using bottomUp(isReduce
‘;‘unroll) before lowering the remaining high-level patterns
to sequential code as before.

1
2

(arrayPacking ‘;;‘ topDown(parallel) ‘;;‘
bottomUp(isReduce ‘;‘ unroll) ‘;‘ lowerToC)(mm)

We have demonstrated that it is feasible to implement
a TVM-like scheduling language in Elevate by expressing
schedules as compositions of reusable strategies.

1

(DFNF ‘;‘ topDown(fuseReduceMap) ‘;‘ lowerToC)(mm)

6.6 Experimental Evaluation

The TVM algorithm computes the dot product in a single
statement. The Rise program describes the dot product with
separate patterns which are fused using fuseReduceMap. The
lowerToC strategy lowers every high-level pattern into its
low-level sequential version: map is rewritten into mapSeq and
reduce into reduceSeq.

For the blocking version, we reuse the same lowerToC strat-
egy but first we leverage the abstractions that we have build
emulating the TVM schedule in a similar style:

1
2
3
4

val blocking = ( topDown(tile(32,32)) ‘;;‘

topDown(isReduce ‘;‘ split(4)) ‘;;‘
topDown(reorder(Seq(1,2,5,6,3,4))) )

(blocking ‘;‘ lowerToC)(mm)

First we tile, then we split and then we reorder, just as
specified in the TVM schedule. We describe locations using
the topDown traversal and the isReduce strategy predicate that
applies the following split only if the current expression is
a reduction. We use the ‘;;' combination to normalize using
DFNF between each step.

The loop permutation version incorporates the changes of
the vectorized version by adding vectorize and a different
reordering of dimensions. In contrast to TVM we identify
dimensions by index rather than by name.

1
2
3
4
5

val loopPerm = (topDown(tile(32,32)) ‘;;‘

topDown(isReduce ‘;‘ split(4)) ‘;;‘
topDown(reorder(Seq(1,2,5,3,6,4)))) ‘;;‘
topDown(vectorize(32)))

(loopPerm ‘;‘ lowerToC)(mm)

For the array packing version, we are not required to
change the Rise program manually, but can apply the ar-
ray packing of matrix B as a rewrite step. Afterwards, we can
reuse the loopPerm strategy before the packed representation
of B is vectorized and then copied in parallel.

1
2
3
4

val arrayPacking = (packB ‘;;‘ loopPerm ‘;;‘

topDown(vectorize(32)) ‘;;‘
parallelizeCopy)

(arrayPacking ‘;‘ lowerToC)(mm)

In order to evaluate the practicability and the scalability of
Elevate, we performed two experiments.

In the first experiment, we are
Number of Rewrite Steps
interested in the scalability of our approach by counting
the number of successfully applied rewrites steps performed
when applying a strategy to the Rise matrix multiplication
expression. Figure 7 shows this number for every strategy
shown in the previous subsection. Since no major optimiza-
tion strategies are applied to the baseline version, only 211
rewrite steps are performed. However, as soon as interest-
ing optimizations are applied, we easily reach about 100,000
steps for the next three versions and about 150,000 for the
most complicated optimizations. The loop-permutation case
slightly drops in numbers of applied rewrite rules because
the specific nesting prescribed in TVM’s schedule required
fewer loop-interchanges.

These high numbers clearly show that abstractions are
required to control this many rewrite steps. It also shows the
scalability of our compositional approach in which complex
optimizations are composed out of a small set of fundamen-
tal building blocks. The high-level strategies encode practi-
cal optimizations and hide massive numbers of individual
rewrite steps that are actually performed. Overall, apply-
ing the strategies to the Rise expression took less than 50
seconds per version on a commodity notebook.

s
p
e
t
S

e
t
i
r

w
e
R

150,000

100,000

50,000

0

baseline

vectorization

array-packing

parallel

blocking

loop-perm

cache-blocks

Figure 7. Total number of successful rewrite steps when
applying different optimization strategies.

Bastian Hagedorn, Johannes Lenfers, Thomas Koehler, Sergei Gorlatch, and Michel Steuwer

)
s

m

(

e
m

i
t
n
u
R
e
t
u
l
o
s
b
A

2,000
1,000
500
200
100
50

TVM
Elevate+Rise

baseline

vectorization

array-packing

parallel

blocking

loop-perm

cache-blocks

Figure 8. Performance of TVM vs. Rise generated code that
has been optimized by Elevate strategies.

Performance Comparison In the second experiment, we
are interested in the performance achieved when optimiz-
ing Rise programs with Elevate compared to TVM. Ideally,
the code optimized with Elevate should be similar to the
TVM-generated code and achieve competitive performance.
We generated LLVM code with TVM (version 0.6.dev) and C
code for Rise annotated with OpenMP pragmas for the ver-
sion which include parallelization or vectorization. The Rise
generated C code was compiled with clang (v.9.0.0) using
-Ofast -ffast-math -fopenmp which echoes the settings used
by TVM and Halide4. The measurements were performed
on an Intel core i5-4670K CPU (frequency locked to 3.4GHz)
running Arch Linux (kernel 5.3.11-arch1-1). We measured
wall-time for Rise-generated code and used TVM’s built-in
measurement API. We measured 100 iterations per version
reporting the median runtimes in milliseconds.

Figure 8 shows the performance of Rise and TVM gener-
ated code using a logarithmic scale. The code generated by
Rise controlled by the Elevate optimization strategies per-
forms competitive to TVM. Similar to the results of the Halide
case study, our experiment shows a matching trend when
comparing to TVM’s versions with equivalent optimizations.
The most optimized parallel Rise generated version improves
the performance over the baseline by a factor of about 110x.
This means that the strategies we developed using Elevate,
that are defined in an extensible style by composing individ-
ual rewrite steps, scale to a level where they actually encode
practically useable and relevant optimizations.

7 Related Work
Term Rewriting and Strategy Languages Elevate is in-
spired by existing strategy languages, especially ELAN [7, 8]
and Stratego [39, 41], which introduce combinators to sup-
port user-defined strategies in the context of term rewriting
systems. Similar rewriting systems include [2, 9, 14, 17, 20,
29, 36]. Program transformations using rewrite rules and
strategy languages have since been used in many different
domains including reverse engineering [13], refactoring [18],

4https://github.com/halide/Halide/issues/2905

and obfuscation [15]. Visser [38, 40] and Kirchner [24] pro-
vide surveys covering term rewriting, strategy languages
and their application domains.

To the best of our knowledge, Elevate is the first strat-
egy language that has been used to specify state-of-the-art
compiler optimizations such as tiling focusing on high per-
formance code generation.

Rewriting in Compilers Rewrite rules and rewriting strate-
gies have also been used when building compilers. The Glas-
gow Haskell Compiler [28] uses rewrite rules as a practical
way to optimize Haskell programs and Visser et. al. [41]
describe how to build program optimizers using rewriting
strategies. Other areas include building interpreters [16], in-
struction selection [10] or constant propagation [26]. More
recently, Lift [21, 33, 35] showed how to use rewrite rules to
generate high-performance code targeting accelerators.

Controlling the application of rewrite rules in compilers
still largely is built-in in a fixed way based on heurisitcs. In
this work, we showed how to use Elevate instead, allowing
a more flexible and practical approach towards using rewrite
rules for describing optimizations in compilers.

Schedule-based Compilers Halide [30] introduced the con-
cept of schedules describing program optimizations sepa-
rate from the algorithm describing the computation. This
concept has been adopted by many other frameworks in do-
mains including machine learning (TVM [12]), graph applica-
tions (GraphIt [43]) or polyhedral compilation (Tiramisu [4],
CHiLL [11, 22], AlphaZ [42], URUK [19]).

These existing scheduling APIs are not designed as princi-
pled programming languages. Instead, a fixed set of ad-hoc
built-in primitives is exposed allowing users to specify which
optimizations to apply. In this work, we showed how to use
Elevate to implement scheduling languages from first prin-
ciples as composition of rewrite rules.

8 Conclusion
In this paper, we presented Elevate: a language for describ-
ing optimization strategies. Elevate follows a tradition of
prior systems used in different contexts that express opti-
mization strategies as composition of rewrites. We showed
that, in contrast to existing systems with scheduling APIs
such as Halide and TVM, programmers are not restricted to
a set of built-in optimizations but define their own optimiza-
tion strategies. Using three case studies, we demonstrated
Elevate’s flexiblity to rewrite different languages ((cid:101)F and
Rise), its extensibility to add custom abstractions inlcuding
domain-specific optimizations, and its practicality to scale
to complex optimization strategies requiring 150k rewrite
steps used in deep learning. We showed that Elevate suc-
cessfully optimizes programs in the Rise language achieving
competitive performance compared to Halide and TVM.

Elevate: A Language for Describing Optimization Strategies

Acknowledgments
We would like to thank the entire Rise (rise-lang.org) and
Elevate (elevate-lang.org) teams for their development ef-
forts. The first author was financially supported by an Nvidia
Research Fellowship.

References
[1] Martín Abadi, Ashish Agarwal, Paul Barham, Eugene Brevdo, Zhifeng
Chen, Craig Citro, Greg S. Corrado, Andy Davis, Jeffrey Dean, Matthieu
Devin, Sanjay Ghemawat, Ian Goodfellow, Andrew Harp, Geoffrey
Irving, Michael Isard, Yangqing Jia, Rafal Jozefowicz, Lukasz Kaiser,
Manjunath Kudlur, Josh Levenberg, Dandelion Mané, Rajat Monga,
Sherry Moore, Derek Murray, Chris Olah, Mike Schuster, Jonathon
Shlens, Benoit Steiner, Ilya Sutskever, Kunal Talwar, Paul Tucker, Vin-
cent Vanhoucke, Vijay Vasudevan, Fernanda Viégas, Oriol Vinyals,
Pete Warden, Martin Wattenberg, Martin Wicke, Yuan Yu, and Xi-
aoqiang Zheng. 2015. TensorFlow: Large-Scale Machine Learning
on Heterogeneous Systems. https://www.tensorflow.org/ Software
available from tensorflow.org.

[2] Oana Andrei, Maribel Fernández, Hélène Kirchner, Guy Melançon,
Olivier Namet, and Bruno Pinaud. 2011. PORGY: Strategy-Driven
Interactive Transformation of Graphs. In Proceedings 6th International
Workshop on Computing with Terms and Graphs, TERMGRAPH 2011,
Saarbrücken, Germany, 2nd April 2011. 54–68. https://doi.org/10.4204/
EPTCS.48.7

[3] Robert Atkey, Michel Steuwer, Sam Lindley, and Christophe Dubach.
2017. Strategy Preserving Compilation for Parallel Functional Code.
CoRR abs/1710.08332 (2017).

[4] Riyadh Baghdadi, Jessica Ray, Malek Ben Romdhane, Emanuele Del
Sozzo, Abdurrahman Akkas, Yunming Zhang, Patricia Suriana, Shoaib
Kamil, and Saman P. Amarasinghe. 2019. Tiramisu: A Polyhedral
Compiler for Expressing Fast and Portable Code. In IEEE/ACM In-
ternational Symposium on Code Generation and Optimization, CGO
2019, Washington, DC, USA, February 16-20, 2019. 193–205. https:
//doi.org/10.1109/CGO.2019.8661197

[5] Paul Barham and Michael Isard. 2019. Machine Learning Systems are

Stuck in a Rut. In HotOS. ACM, 177–183.

[6] Richard Bird and Oege de Moor. 1997. Algebra of Programming.

Prentice-Hall, Inc., Upper Saddle River, NJ, USA.

[7] Peter Borovanský, Claude Kirchner, Hélène Kirchner, Pierre-Etienne
Moreau, and Christophe Ringeissen. 1998. An overview of ELAN.
Electr. Notes Theor. Comput. Sci. 15 (1998), 55–70. https://doi.org/10.
1016/S1571-0661(05)82552-6

[8] Peter Borovanský, Claude Kirchner, Hélène Kirchner, Pierre-Etienne
Moreau, and Marian Vittek. 1996. ELAN: A logical framework based
on computational systems. Electr. Notes Theor. Comput. Sci. 4 (1996),
35–50. https://doi.org/10.1016/S1571-0661(04)00032-5

[9] James M Boyle, Terence J Harmer, and Victor L Winter. 1997. The
TAMPR program transformation system: Simplifying the development
of numerical software. In Modern software tools for scientific computing.
Springer, 353–372.

[10] Martin Bravenboer and Eelco Visser. 2002. Rewriting Strategies for
Instruction Selection. In Rewriting Techniques and Applications, 13th
International Conference, RTA 2002, Copenhagen, Denmark, July 22-24,
2002, Proceedings. 237–251. https://doi.org/10.1007/3-540-45610-4_17
[11] Chun Chen, Jacqueline Chame, and Mary Hall. 2008. CHiLL: A frame-
work for composing high-level loop transformations. Technical Report.
Citeseer.

[12] Tianqi Chen, Thierry Moreau, Ziheng Jiang, Lianmin Zheng, Eddie Q.
Yan, Haichen Shen, Meghan Cowan, Leyuan Wang, Yuwei Hu, Luis
Ceze, Carlos Guestrin, and Arvind Krishnamurthy. 2018. TVM: An
Automated End-to-End Optimizing Compiler for Deep Learning. In

13th USENIX Symposium on Operating Systems Design and Implemen-
tation, OSDI 2018, Carlsbad, CA, USA, October 8-10, 2018. 578–594.
https://www.usenix.org/conference/osdi18/presentation/chen
[13] Elliot J. Chikofsky and James H. Cross II. 1990. Reverse Engineering
and Design Recovery: A Taxonomy. IEEE Software 7, 1 (1990), 13–17.
https://doi.org/10.1109/52.43044

[14] Manuel Clavel, Francisco Durán, Steven Eker, Patrick Lincoln, Narciso
Martí-Oliet, José Meseguer, and Jose F. Quesada. 2002. Maude: specifi-
cation and programming in rewriting logic. Theor. Comput. Sci. 285, 2
(2002), 187–243. https://doi.org/10.1016/S0304-3975(01)00359-0
[15] Christian S. Collberg, Clark D. Thomborson, and Douglas Low. 1998.
Manufacturing Cheap, Resilient, and Stealthy Opaque Constructs. In
POPL ’98, Proceedings of the 25th ACM SIGPLAN-SIGACT Symposium
on Principles of Programming Languages, San Diego, CA, USA, January
19-21, 1998. 184–196. https://doi.org/10.1145/268946.268962

[16] Eelco Dolstra and Eelco Visser. 2002. Building Interpreters with Rewrit-
ing Strategies. Electr. Notes Theor. Comput. Sci. 65, 3 (2002), 57–76.
https://doi.org/10.1016/S1571-0661(04)80427-4

[17] Maribel Fernández, Hélène Kirchner, and Olivier Namet. 2011. A
Strategy Language for Graph Rewriting. In Logic-Based Program Syn-
thesis and Transformation - 21st International Symposium, LOPSTR 2011,
Odense, Denmark, July 18-20, 2011. Revised Selected Papers. 173–188.
https://doi.org/10.1007/978-3-642-32211-2_12

[18] Martin Fowler. 1999. Refactoring - Improving the Design of Existing Code.
Addison-Wesley. http://martinfowler.com/books/refactoring.html
[19] Sylvain Girbal, Nicolas Vasilache, Cédric Bastoul, Albert Cohen, David
Parello, Marc Sigler, and Olivier Temam. 2006. Semi-Automatic Com-
position of Loop Transformations for Deep Parallelism and Memory
Hierarchies. International Journal of Parallel Programming 34, 3 (2006),
261–317. https://doi.org/10.1007/s10766-006-0012-3

[20] Joseph A. Goguen, Claude Kirchner, Hélène Kirchner, Aristide Mé-
grelis, José Meseguer, and Timothy C. Winkler. 1987. An Introduc-
tion to OBJ 3. In Conditional Term Rewriting Systems, 1st Interna-
tional Workshop, Orsay, France, July 8-10, 1987, Proceedings. 258–263.
https://doi.org/10.1007/3-540-19242-5_22

[21] Bastian Hagedorn, Larisa Stoltzfus, Michel Steuwer, Sergei Gorlatch,
and Christophe Dubach. 2018. High performance stencil code gen-
eration with lift. In Proceedings of the 2018 International Symposium
on Code Generation and Optimization, CGO 2018, Vösendorf / Vienna,
Austria, February 24-28, 2018. 100–112. https://doi.org/10.1145/3168824
[22] Mary Hall, Jacqueline Chame, Chun Chen, Jaewook Shin, Gabe Rudy,
and Malik Murtaza Khan. 2009. Loop transformation recipes for code
generation and auto-tuning. In International Workshop on Languages
and Compilers for Parallel Computing. Springer, 50–64.

[23] John L. Hennessy and David A. Patterson. 2019. A new golden age for

computer architecture. Commun. ACM 62, 2 (2019), 48–60.

[24] Hélène Kirchner. 2015. Rewriting Strategies and Strategic Rewrite
Programs. In Logic, Rewriting, and Concurrency - Essays dedicated to
José Meseguer on the Occasion of His 65th Birthday. 380–403. https:
//doi.org/10.1007/978-3-319-23165-5_18

[25] Sebastiaan Pascal Luttik, Eelco Visser, et al. 1997. Specification of rewrit-
ing strategies. Universiteit van Amsterdam. Programming Research
Group.

[26] Karina Olmos and Eelco Visser. 2002. Strategies for Source-to-Source
Constant Progagation. Electr. Notes Theor. Comput. Sci. 70, 6 (2002),
156–175. https://doi.org/10.1016/S1571-0661(04)80605-4

[27] Adam Paszke, Sam Gross, Soumith Chintala, Gregory Chanan, Edward
Yang, Zachary DeVito, Zeming Lin, Alban Desmaison, Luca Antiga,
and Adam Lerer. 2017. Automatic differentiation in PyTorch. (2017).
[28] Simon Peyton Jones, Andrew Tolmach, and Tony Hoare. 2001. Playing
by the rules: rewriting as a practical optimisation technique in GHC. In
2001 Haskell Workshop (2001 haskell workshop ed.). ACM SIGPLAN.
[29] Bruno Pinaud, Oana Andrei, Maribel Fernández, Hélène Kirchner,
Guy Melançon, and Jason Vallet. 2017. PORGY : a Visual Analytics

Bastian Hagedorn, Johannes Lenfers, Thomas Koehler, Sergei Gorlatch, and Michel Steuwer

Platform for System Modelling and Analysis Based on Graph Rewrit-
ing. In 17ème Journées Francophones Extraction et Gestion des Con-
naissances, EGC 2017, 24-27 Janvier 2017, Grenoble, France. 473–476.
http://editions-rnti.fr/?inprocid=1002327

[30] Jonathan Ragan-Kelley, Andrew Adams, Dillon Sharlet, Connelly
Barnes, Sylvain Paris, Marc Levoy, Saman P. Amarasinghe, and Frédo
Durand. 2018. Halide: decoupling algorithms from schedules for high-
performance image processing. Commun. ACM 61, 1 (2018), 106–115.
https://doi.org/10.1145/3150211

[31] Jonathan Ragan-Kelley, Connelly Barnes, Andrew Adams, Sylvain
Paris, Frédo Durand, and Saman P. Amarasinghe. 2013. Halide: a
language and compiler for optimizing parallelism, locality, and recom-
putation in image processing pipelines. In PLDI. ACM, 519–530.
[32] Amir Shaikhha, Andrew Fitzgibbon, Dimitrios Vytiniotis, and Simon
Peyton Jones. 2019. Efficient differentiable programming in a func-
tional array-processing language. PACMPL 3, ICFP (2019), 97:1–97:30.
[33] Michel Steuwer, Christian Fensch, Sam Lindley, and Christophe
Dubach. 2015. Generating performance portable code using rewrite
rules: from high-level functional expressions to high-performance
OpenCL code. In ICFP. ACM, 205–217.

[34] Michel Steuwer, Toomas Remmelg, and Christophe Dubach. 2016.
Matrix multiplication beyond auto-tuning: rewrite-based GPU code
generation. In CASES. ACM, 15:1–15:10.

[35] Michel Steuwer, Toomas Remmelg, and Christophe Dubach. 2017.
Lift: a functional data-parallel IR for high-performance GPU code
generation. In Proceedings of the 2017 International Symposium on Code
Generation and Optimization, CGO 2017, Austin, TX, USA, February 4-8,
2017. 74–85. http://dl.acm.org/citation.cfm?id=3049841

[36] Mark van den Brand, Arie van Deursen, Jan Heering, H. A. de Jong,
Merijn de Jonge, Tobias Kuipers, Paul Klint, Leon Moonen, Pieter A.
Olivier, Jeroen Scheerder, Jurgen J. Vinju, Eelco Visser, and Joost Visser.
2001. The ASF+SDF Meta-environment: A Component-Based Lan-
guage Development Environment. In Compiler Construction, 10th In-
ternational Conference, CC 2001 Held as Part of the Joint European

Conferences on Theory and Practice of Software, ETAPS 2001 Genova,
Italy, April 2-6, 2001, Proceedings. 365–370. https://doi.org/10.1007/
3-540-45306-7_26

[37] Eelco Visser. 2001. Stratego: A Language for Program Transformation
Based on Rewriting Strategies. In Rewriting Techniques and Applica-
tions, 12th International Conference, RTA 2001, Utrecht, The Nether-
lands, May 22-24, 2001, Proceedings. 357–362. https://doi.org/10.1007/
3-540-45127-7_27

[38] Eelco Visser. 2001. A Survey of Strategies in Program Transformation
Systems. Electr. Notes Theor. Comput. Sci. 57 (2001), 109–143. https:
//doi.org/10.1016/S1571-0661(04)00270-1

[39] Eelco Visser. 2004. Program transformation with Stratego/XT.

In

Domain-specific program generation. Springer, 216–238.

[40] Eelco Visser. 2005. A survey of strategies in rule-based program
transformation systems. J. Symb. Comput. 40, 1 (2005), 831–873. https:
//doi.org/10.1016/j.jsc.2004.12.011

[41] Eelco Visser, Zine-El-Abidine Benaissa, and Andrew P. Tolmach. 1998.
Building Program Optimizers with Rewriting Strategies. In Proceed-
ings of the third ACM SIGPLAN International Conference on Functional
Programming (ICFP ’98), Baltimore, Maryland, USA, September 27-29,
1998. 13–26. https://doi.org/10.1145/289423.289425

[42] Tomofumi Yuki, Gautam Gupta, DaeGon Kim, Tanveer Pathan, and
Sanjay V. Rajopadhye. 2012. AlphaZ: A System for Design Space
Exploration in the Polyhedral Model. In Languages and Compilers for
Parallel Computing, 25th International Workshop, LCPC 2012, Tokyo,
Japan, September 11-13, 2012, Revised Selected Papers. 17–31. https:
//doi.org/10.1007/978-3-642-37658-0_2

[43] Yunming Zhang, Mengjiao Yang, Riyadh Baghdadi, Shoaib Kamil,
Julian Shun, and Saman P. Amarasinghe. 2018. GraphIt: a high-
performance graph DSL. PACMPL 2, OOPSLA (2018), 121:1–121:30.
https://doi.org/10.1145/3276491

