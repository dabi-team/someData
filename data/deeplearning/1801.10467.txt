Deep Reinforcement Learning for Programming Language Correction

Rahul Gupta, Aditya Kanade, Shirish Shevade
Department of Computer Science and Automation
Indian Institute of Science
Bangalore, KA 560012, India
{rahulg, kanade, shirish}@iisc.ac.in

8
1
0
2

n
a
J

1
3

]
I

A
.
s
c
[

1
v
7
6
4
0
1
.
1
0
8
1
:
v
i
X
r
a

Abstract

Novice programmers often struggle with the formal
syntax of programming languages. To assist them,
we design a novel programming language correc-
tion framework amenable to reinforcement learn-
ing. The framework allows an agent to mimic hu-
man actions for text navigation and editing. We
demonstrate that the agent can be trained through
self-exploration directly from the raw input, that is,
program text itself, without any knowledge of the
formal syntax of the programming language. We
leverage expert demonstrations for one tenth of the
training data to accelerate training. The proposed
technique is evaluated on 6975 erroneous C pro-
grams with typographic errors, written by students
during an introductory programming course. Our
technique ﬁxes 14% more programs and 29% more
compiler error messages relative to those ﬁxed by
a state-of-the-art tool, DeepFix, which uses a fully
supervised neural machine translation approach.

1 Introduction

Programming languages prescribe formal rules of syntactic
validity of program text. A program text that does not adhere
to those rules cannot be compiled and executed. The inabil-
ity to write syntactically correct programs therefore becomes
In large-scale
a stumbling block for novice programmers.
online programming courses, getting personalized feedback
from instructors or TAs is infeasible. On the other hand, com-
piler error messages are often difﬁcult to understand and do
not localize the errors accurately [Traver, 2010]. Therefore,
our aim in this work is to develop a technique to assist novice
programmers by automatically correcting common syntactic
errors in programs.

We approach this problem through reinforcement learn-
ing (RL). When faced with an error, a programmer navigates
through the program text to arrive at the location of error and
then performs an edit to ﬁx the error. We propose a novel
programming language correction framework, in which an
agent can mimic these actions. While the agent can access
and modify the program text, the compiler which checks syn-
tactic validity of the program text is a black-box for the agent.

As noted above, compilers usually do not pinpoint error loca-
tions precisely. Hence, we do not rely on a compiler to aid in
error localization or correction. We instead design a reward
function using only the number of error messages generated
by the compiler. The goal of the agent is to perform edits
necessary for successful compilation of the program.

The challenge in this framework is to learn a control pol-
icy for the agent from program text without the agent hav-
ing any knowledge of the formal syntax of the program-
ming language. Through deep reinforcement learning, agents
have been trained to play visual and text-based games at
expert levels [Mnih et al., 2015; Narasimhan et al., 2015;
Wu and Tian, 2017]. Interestingly, these techniques learn di-
rectly from raw inputs such as pixels and text. In this work,
for the ﬁrst time, we show that this is possible even in the
space of computer programs.

To see our framework in action, consider an example C
program shown in Figure 1 which implements a tax calcula-
tion algorithm. It has two syntactic errors: (1) incorrect use
of semicolon within the scanf in line 4 and (2) a missing
closing brace at the end of line 12. This program is given to
a trained agent which has not seen it during training. The
program is presented in a tokenized form to the agent and
the cursor position of the agent is initialized to the ﬁrst token
in the program. The navigation actions of the agent over the
program text are shown by arrows. As shown by the sequence
of actions taken by the agent in Figure 1, the agent correctly
localizes and ﬁxes both the errors. First, the agent navigates
to the error location at line 4 and replaces the incorrect semi-
colon with a comma (marked by e1). Next, it navigates to the
error location at line 12 and inserts the missing closing brace
(marked by e2). After these edits, the program compiles suc-
cessfully and the agent stops. In comparison, a brute-force
search will have to enumerate all possible edited versions of
the program with up to two simultaneous edits and compile
each of them to identify the right edits. This will be far more
expensive than using the agent.

A video demonstration of this example is available at:

https://youtu.be/S4D6MR728zY

We encode the program text, augmented with the position
of the cursor, using a long short-term memory (LSTM) net-
work [Hochreiter and Schmidhuber, 1997]. The agent is al-
lowed a set of navigation and edit actions to ﬁx the program.

 
 
 
 
 
 
1 #include<stdio.h>

2 int main()(

3

4

5

6

7

8

9

10

11

12

13

14

15

16

float ti, tax;

e1

scanf ( "%f" ; &ti);

if(ti<200001){

printf("ti=0");}

else if(200000<ti && ti<500001){

tax=0.1*(ti-200000);

printf("%.2f", tax);}

else if(500000<ti && ti<1000001){

tax=30000+0.2*(ti-500000);
printf ( "%.2f" , tax ) ;

e2

else if(ti>1000000){

tax=130000+0.3*(ti-1000000);

printf("%.2f", tax);}

return 0;}

Figure 1: An erroneous program and the sequence of actions taken
by a trained agent to ﬁx it: The error locations are highlighted in
the red color. The arrows show how the agent navigates over the
program text. The edit actions are marked by e1 and e2.

It receives a small reward for every edit which ﬁxes some er-
ror and the maximum reward is given for reaching the goal
state, which is an error-free version of the program. The con-
trol policy of the agent is learned using the asynchronous ad-
vantage actor-critic (A3C) algorithm [Mnih et al., 2016].

Training an agent in this setting is non-trivial for two rea-
sons. First, as illustrated by the example above, the agent
needs to both localize the errors and make precise edits at
the error locations to be able to ﬁx a program. A wrong edit
only makes the program worse by introducing more errors
and consequently, makes the task even more difﬁcult. To
overcome this, we conﬁgure the environment to reject such
edits. This signiﬁcantly prunes the state space that an agent
is allowed to explore without sacriﬁcing its ability to ﬁx pro-
grams. Second, reinforcement learning tends to be slow for
the tasks with large state spaces as the time required for gath-
ering information by state exploration increases. One way
to mitigate this problem is to guide the agent with expert
demonstrations [Argall et al., 2009]. Motivated by this, we
design a scheme to enable the agent to take advantage of ex-
pert demonstrations illustrating the required actions for ﬁxing
the programs. As explained later, for our task, these demon-
strations are generated automatically without any human in-
tervention. We call our technique RLAssist.

DeepFix [Gupta et al., 2017] is a state-of-the-art tool for
ﬁxing common programming errors. We compare RLAs-
sist with DeepFix on the task of ﬁxing typographic errors
in 6975 C programs. These programs were written by stu-
dents during an introductory programming course and span
93 different programming problems [Gupta et al., 2017;

Das et al., 2017]1. These programs use non-trivial constructs
of the C language such as multiple procedures, condition-
als, switch statements, nested loops, multi-dimensional ar-
rays and recursion. DeepFix uses a fully supervised neural
machine translation approach.

In contrast, RLAssist is a deep reinforcement learning
based technique. We demonstrate that RLAssist can be
trained through self-exploration using erroneous programs
only, while still matching the performance of DeepFix. We
further show that we can leverage expert demonstrations to
accelerate the training.
In particular, we generate expert
demonstrations for one tenth of the training dataset. The rest
90% of the dataset is used without demonstrations. RLAs-
sist ﬁxes 26.6% programs from the test set completely and
resolves 39.7% error messages. Relative to DeepFix, this is
an improvement of 14% and 29% respectively. Thus, RLAs-
sist outperforms DeepFix using expert demonstrations for a
fraction of training data.

The main contributions of this work are as follows:

1. We design a novel framework of programming language

correction amenable to reinforcement learning.

2. We use A3C for programming language correction and

accelerate training through expert demonstrations.

3. Our experiments show that our technique outperforms a
state-of-the-art tool, DeepFix, using expert demonstra-
tions for only one tenth of the training data. Moreover,
our technique also works without any demonstrations
while still matching the performance of DeepFix.

4. The implementation of RLAssist will be open sourced.

2 Related Work

Natural language correction is a well researched problem.
Some of the earlier works in this area have focused on
identifying and correcting speciﬁc types of grammatical er-
rors such as misuse of verb forms, articles, and prepositions
[Chodorow et al., 2007; Han et al., 2006; Rozovskaya and
Roth, 2010]. The more recent works consider a broader range
of error classes, often relying on language models, and ma-
chine translation [Ng et al., 2014; Rozovskaya et al., 2014].
Although natural languages and programming languages
are similar to some extent, the latter have procedural interpre-
tation and richer structure. Due to the huge popularity of pro-
gramming oriented massive open online courses (MOOCs),
recent years have seen increasing interest in programming
language correction [Gupta et al., 2017; Parihar et al., 2017].
These works either rely on supervised learning techniques or
use hand written rules to ﬁx common programming errors.
This paper proposes a reinforcement learning framework in
which an agent can learn programming language correction
directly from raw program text through self-exploration.

Learning from demonstration (LfD) approaches train an
agent using expert demonstrations. Behavioral cloning [Ross
et al., 2011] is one particular class of LfD techniques to

1https://www.cse.iitk.ac.in/users/karkare/

prutor/prutor-deepfix-09-12-2017.db.gz

make the agent mimic expert demonstrations using super-
vised learning. We complement the self-exploration with ex-
pert demonstrations to accelerate the training. Inverse rein-
forcement learning is another class of LfD techniques. These
use demonstrations to ﬁrst infer a reward function which is
then used to learn the policy [Abbeel and Ng, 2004]. These
methods have been used for the tasks where there is no ob-
vious reward function, e.g., autonomous driving [Abbeel and
Ng, 2004] and acrobatic helicopter maneuvers [Abbeel et al.,
2007]. For our task, the rewards are well deﬁned and can be
calculated easily.

3 Background
3.1 Reinforcement Learning
In reinforcement learning, an agent interacts with its environ-
ment over a number of discrete time steps. At each time step
t, the environment presents a state st ∈ S to the agent. In re-
sponse, the agent selects an action at from the set of allowed
actions A. This selection is controlled by the agent’s policy
π(a|s) = P r{at = a|st = s}. The action is passed on to the
environment and its execution may modify the internal state
of the environment. The agent then receives the updated state
st+1 and a scalar reward rt. This interaction, which is also
called an episode, stops when the agent reaches a goal state.
The objective of the agent is to maximize the expected sum
of discounted future rewards Gt from each state st. For an
episode terminating at time step T , Gt = (cid:80)T −t−1
γkrt+k,
where the discount rate γ ∈ (0, 1] [Sutton and Barto, 1998].

k=0

3.2 Asynchronous Advantage Actor-Critic (A3C)
One of the many ways to solve an RL problem is to use policy
gradient methods. These methods learn a parameterized pol-
icy π(a|s; θ), where θ represents the parameters of a function
approximator, such as a neural network. One example of pol-
icy gradient methods is the actor-critic methods, which learn
both π(a|s; θ), the ‘actor’, and the value function V (s; w), the
‘critic’. Here, V (s; w) deﬁnes the expected reward from state
s, with w being the parameters of a function approximator.
The critic evaluates how advantageous is it to be in the new
state reached by taking an action at sampled from the distri-
bution given by π(st). Based on this evaluation, the param-
eterized policy is updated using an appropriate optimization
technique such as gradient ascent [Sutton and Barto, 1998].

The A3C algorithm uses multiple asynchronous parallel
actor-learner threads to update a shared model, stabilizing the
learning process by reducing the correlation of an agent’s ex-
perience. It has also been observed to reduce training time
by a factor that is roughly linear in the number of parallel
actor-learners [Mnih et al., 2016]. We use A3C in this work.

4 Technical Details
4.1 A Framework for Programming Language

Correction Tasks

When faced with an error, a programmer navigates the pro-
gram text to arrive at the location of error and then performs
an edit operation to ﬁx the error. In the presence of multiple
errors, the programmer can repeat these steps. We present a

programming language (PL) correction framework in which
an agent can mimic these actions. We now describe the com-
ponents of this framework and their instantiation for our task
of correcting syntactic errors in C programs.

States
A state is a pair (cid:104)string, cursor(cid:105), where string is the
program text, and cursor ∈ {1, . . . , len(string)}, where
len(string) denotes the number of tokens in the string.
The environment also keeps track of the number of errors
in string. These errors can either be determined from the
ground truth whenever available or estimated using the er-
ror messages generated by a compiler upon compiling the
string. For compilation, we use the GNU C compiler.

We encode the state into a sequence of tokens as follows.
First, we convert the program string into a sequence of lex-
emes. The lexemes are of different types, such as keywords,
operators, types, functions, literals, and variables.
In ad-
dition, we also retain line-breaks as lexemes to allow two-
dimensional navigation actions over the program text. The
cursor part of the state is represented by a special token,
which is inserted in the sequence of lexemes right after the
token whose index is held by cursor.

Next, we build a shared vocabulary across all programs.
Except some common library functions such as printf and
scanf, all other function and variable identiﬁers are mapped
to a special token ID. Similarly, all the literals are mapped to
special tokens according to their type, e.g., numbers to NUM
and strings to STR. All remaining tokens are included in the
vocabulary without any modiﬁcations. This mapping reduces
the size of the vocabulary seen by the agent.

Note that this encoding is only required for feeding the
state to the agent. The actions predicted by the agent based
on this encoding are executed on the original program string
by the environment.

Actions and Transitions
The agent actions are divided into two categories, the ﬁrst
which update the cursor and the second which modify the
string. We refer to the ﬁrst category of actions as navigation
actions and the latter as edit actions. The navigation actions
allow an agent to navigate through the string. These actions
only change the cursor of a state and not the string. The
edit actions on the other hand, are used for error correction.
They only modify the string and not the cursor. Wrong edit
actions introduce more errors in the string rather than ﬁxing
them. We conﬁgure the environment to reject all such edits
to prune the state space from which ﬁxing the program be-
comes even more difﬁcult. In addition, rejecting wrong edits
prevents arbitrary changes to the program.

For our task, we allow only two navigation actions,
move right and move down. These set the cursor to the next
token on the right or to the ﬁrst token of the next line re-
spectively. The move right (respectively, move down) action
has no effect if the cursor is already set to the last token
of a line (respectively, any token of the last line). Note that
the move down action is possible because we retain the line-
breaks in the state encoding.

Based on a study of common typographic errors that novice
programmers make, we design three types of edit actions.

The ﬁrst is a parameterized insert token action which inserts
the parameter token immediately before the cursor position.
The parameter can be any token from a ﬁxed set of tokens
which we call mutable tokens. The second is the delete ac-
tion, which deletes the token at the cursor position. How-
ever, the token is deleted only if it is from the set of mutable
tokens. We restrict the set of mutable tokens to the following
ﬁve types of tokens: semicolon, parentheses, braces, period,
and comma. The third edit action is a parameterized replace
token1 with token2 action which replaces token1 at the cursor
position with token2. We have the following four actions in
this class: (1) replace ‘;’ with ‘,’, (2) replace ‘,’ with ‘;’,
(3) replace ‘.’ with ‘;’, and (4) replace ‘;)’ with ‘);’. Al-
though atomic replacement actions can be substituted with a
sequence of delete and insert actions, having them prevents
the cases where the constituent delete and/or insert actions
can be rejected by the environment.

Episode, Termination, and Rewards
An episode starts with an erroneous program text as string
and the curosr set to its ﬁrst token. The goal state is reached
when the edited program compiles successfully. An agent is
allowed max episode len number of discrete time steps to
reach the goal state in an episode after which the episode is
terminated. Also, the agent is allowed only one pass over the
program in an episode, i.e., once the agent navigates past the
last token of a program, the episode is terminated.
the agent

is penalized with a small
step penalty in order to encourage it to learn to ﬁx a pro-
gram in the minimum number of steps. Additionally, the
agent is penalized with a relatively higher edit penalty for
taking edit actions. The reason is that the edit actions are
somewhat costly as they need to be veriﬁed by invoking a
compiler. Therefore, we discourage the agent to make ed-
its unnecessarily. The agent is given maximum reward for
reaching the goal state. Also, a small intermediate reward
is given for taking an edit action that ﬁxes at least one error.

In each step,

4.2 Model
We use the A3C algorithm in the PL correction task. Our
model ﬁrst uses a long short-term memory (LSTM) [Hochre-
iter and Schmidhuber, 1997] network for embedding the to-
kenized state into a real vector. The LSTM network maps
each token xi of an input sequence (x1, . . . , xn) to a real vec-
tor yi. The ﬁnal state embedding is calculated by taking an
element-wise mean over all the output vectors (y1, . . . , yn)
following [Narasimhan et al., 2015].

Given this state embedding, we use two separate fully con-
nected linear layers to produce the policy function π(a|s; θ),
and the value function V (s; w) outputs. Finally, before up-
dating the network parameters, the gradients are accumulated
using the following rules:

dθ ← dθ + ∇θ(cid:48) log π(at|st; θ(cid:48))(R − V (st; w(cid:48)))

+ β∇θ(cid:48)H(π(st; θ(cid:48)))
dw ← dw + ∇w(cid:48)(R − V (st; w(cid:48)))2

where R = (cid:80)k−1
i=0 γirt+i + γkV (st+k; w(cid:48)), H is the entropy,
and β is a hyperparameter to control the weight of the en-
tropy regularization term; θ(cid:48) and w(cid:48) are the thread speciﬁc

parameters corresponding to θ and w respectively [Mnih et
al., 2016].

Expert Demonstrations
As we show later in the experiments section, RLAssist can be
trained without using any demonstrations, but requires longer
training time. The reason is that in the A3C algorithm, in each
episode, an agent starts with a random state s and then uses
its policy function to interact with the environment. As the
policy network is initialized randomly, this interaction is gov-
erned with random exploration at the beginning of the train-
ing. With only random exploration, the agent ﬁnds it difﬁcult
to reach the goal state and wins rewards infrequently. Conse-
quently, the training slows down.

We use expert demonstrations to accelerate training. An
expert demonstration is a sequence of actions leading to the
goal state of an episode. Given a pair (p, p(cid:48)) where p is an in-
correct program and p(cid:48) is its correct version, a demonstration
is generated automatically as follows. Starting from the ﬁrst
token in p(cid:48), each unmodiﬁed line (w.r.t. p) is skipped through
a move down action. At the ﬁrst erroneous line, the cursor is
moved rightwards through a move right action until reaching
an error location and the appropriate edit action is generated.
This process is repeated until the last error in the program is
resolved.

We conﬁgure an agent to use expert demonstrations as fol-
lows. For the episodes for which a demonstration is available,
the agent follows the predetermined sequence of actions pro-
vided, instead of sampling driven by the policy. The updates
to the policy network parameters are made as if the predeter-
mined action was sampled. For the rest of the episodes, the
agent takes the actions sampled using the policy, following
the standard A3C algorithm. Note that the demonstrations
are provided at the episode level and not at a ﬁner granularity
of transition level. The reasoning is that the agent needs to
take the right actions throughout the episode to reach the goal
state and earn reward. If it takes intermittent guidance in an
episode then it can still fail to reach the goal state.

5 Experiments

5.1 Dataset
We use the training and test datasets used in [Gupta et al.,
2017] for the task of correcting typographic errors. The pro-
grams in the datasets span 93 programming problems in an
introductory programming course and make use of non-trivial
C language constructs. The program lengths range from 75
to 450 tokens. In addition to the test set of actual student-
written erroneous programs, DeepFix has also been evaluated
on some programs with seeded errors. In this work, we only
use the test set of student-written erroneous programs. This
test set contains 6975 erroneous programs with 16766 com-
pilation error messages together, as shown in Table 1. The
dataset is divided into ﬁve folds for cross validation. Each
fold holds aside about 1/5th of the 93 programming problems.
The student-written erroneous programs belonging to the held
out problems form the test set in that fold. The training data
associated with the remaining 4/5th programming problems
is used for training. Thus, a learning algorithm must learn the

Dataset statistics
Error
Erroneous
msgs.
programs

Completely
ﬁxed
programs

DeepFix results
Partially
ﬁxed
programs

Error
messages
resolved

Completely
ﬁxed
programs

RLAssist results
Partially
ﬁxed
programs

Error
messages
resolved

6975

16766 1625 (23.3%) 1129 (16.2%) 5156 (30.8%) 1854 (26.6%) 1426 (20.4%) 6652 (39.7%)

Table 1: Summary of the test dataset and performance comparison of DeepFix and RLAssist on it.

100

50

0

−50

−100

100

50

0

−50

−100

ﬁx%

epLen

reward

edits

100

50

0

−50

−100

ﬁx%

epLen

reward

edits

ﬁx%

epLen

reward

edits

0

0.5

1

1.5

0

0.5

1

1.5

0

0.5

1

1.5

(a) RLAssist

(b) Baseline1

(c) Baseline2

Figure 2: Training performance comparison of RLAssist with two baselines on one fold of the training dataset. None of the baselines uses
expert demonstrations. Additionally, Baseline2 has edit penalty set to zero. The X-axis shows the number of training episodes in millions.
epLen and ﬁx% stand for episode length and the percentage of error messages resolved, respectively.

syntactic validity as per the language syntax so that it can gen-
eralize to unseen programming problems. Each fold roughly
contains about 160K labeled training examples. For more de-
tails of the dataset, we refer the reader to [Gupta et al., 2017].

5.2 Experiment Conﬁguration and Training
We implement our technique in Tensorﬂow [Abadi et al.,
2016], following an open source implementation2 of A3C.
We ﬁnd a suitable conﬁguration of the PL correction frame-
work and the learning model for our task through experimen-
tation. In particular, the LSTM encoder in our model has two
recurrent layers with 128 cells each. Our vocabulary has 91
tokens, which are embedded into 24-dimensional vectors. We
set the discounting factor γ = 0.99, the maximum number of
exploration steps before a neural network parameter update is
made tmax = 24, and entropy regularization factor β = 0.01.
We use 32 parallel agents, and a learning rate of 0.0001 for
optimizing our model using the ADAM optimizer [Kingma
and Ba, 2015]. We also use gradient clipping to prevent
the gradients from exploding [Pascanu et al., 2013]. We
conﬁgure the PL correction framework for our task by set-
ting max episode len = 100, step penalty = −0.005,
edit penalty = −0.025, maximum reward = 1, and
intermediate reward = 0.045.

In order to accelerate training, we use expert demonstra-
tions for one tenth of examples in the training dataset. In our
experiments, we observed that using more demonstrations did
not result in signiﬁcant speedup in training while reducing
the demonstrations slowed it down. With demonstrations, the
training took about four days for 10 epochs of training on an
Intel(R) Xeon(R) E5-2630 v4 machine, clocked at 2.20GHz
with 32GB of RAM.

2https://github.com/awjuliani/

DeepRL-Agents/

5.3 Evaluation

In this section, we ﬁrst discuss the training performance of
RLAssist with expert demonstrations on one tenth of the
training data. Next, we compare it with DeepFix on the
test dataset described earlier. Later we compare RLAssist
with two baselines which are trained without using any ex-
pert demonstrations. While the ﬁrst baseline fails to learn at
all, the second one is able to learn enough to match the per-
formance of DeepFix, demonstrating that RLAssist can be
trained using only erroneous programs and self-exploration.

Training Performance of RLAssist
In order to evaluate the training performance of RLAssist, we
use the following metrics: (1) the percentage of error mes-
sages resolved, (2) the average episode length, (3) the average
number of edit actions, and (4) the average reward obtained
by the agent as the training progresses. We report the training
performance on one of the folds. For ease of plotting, the av-
erage reward shown in the ﬁgures is scaled by a factor of 100.
Figure 2a illustrates the training performance of RLAssist.

RLAssist learns to solve the task very well and is able to re-
solve about 90% of the error messages after training for about
a million episodes. In the last epoch of training, RLAssist
reaches the goal state for 85% of the episodes. Furthermore,
it manages to resolve 56% of the error messages for the pro-
grams corresponding to the remaining 15% episodes. At the
same time, the average scaled reward also reaches the maxi-
mum of about 52 from −65, the reward obtained at the begin-
ning of the training. The maximum scaled reward is almost
always less than 100 because of the penalty that the agent in-
curs for navigating to the error location. The average length
of an episode reaches about 38 consisting about 31 navigation
and 7 edit actions. For the 85% of the programs that RLAs-
sist ﬁxes in the last epoch, the number of rejected edit actions

Completely
ﬁxed
programs

199
425

Partially
ﬁxed
programs
179
476

Completely or
partially ﬁxed
programs

248
771

DeepFix alone
RLAssist alone

Table 2: Comparison of DeepFix and RLAssist on the number of
programs ﬁxed exclusively by each technique.

per episode is less than 4. This shows that RLAssist not only
learns to ﬁx a program but it also learns to do so by taking
almost precise navigation and edit actions.

Comparison with DeepFix
In Table 1, we show the comparison of RLAssist with Deep-
Fix on the test dataset. For this comparison, we use the num-
ber of error messages resolved, and completely and partially
ﬁxed programs; the same metrics as reported in [Gupta et al.,
2017]. Further in Table 2, we also report the number of pro-
grams which are ﬁxed exclusively by DeepFix or RLAssist.
For comparison, we take the most recent results available
from the DeepFix webpage: https://bitbucket.org/
iiscseal/deepfix.

As shown in Table 1, the test dataset has 16766 error mes-
sages from 6975 erroneous programs out of which DeepFix
resolves 5156 error messages, ﬁxing 1625 programs com-
pletely and 1129 partially. RLAssist resolves 6652 error mes-
sages, ﬁxing 1854 programs completely and 1426 partially.
Thus RLAssist outperforms DeepFix by a relative margin of
29% and 14% in terms of error messages resolved and com-
pletely ﬁxed programs, respectively. Relative to DeepFix, the
percentage of programs ﬁxed partially by RLAssist is 26%
higher. At test time, both RLAssist and DeepFix take less
than a second to ﬁx a program.

One reason for better performance of RLAssist is that
it works at a ﬁner token-level granularity compared to the
coarser line-level granularity of DeepFix. RLAssist can edit
an incorrect line in place, whereas DeepFix has to produce a
complete replacement of the erroneous line. This requires it
to copy the correct token subsequences from the input while
simultaneously rectifying the erroneous tokens. Another rea-
son is that DeepFix halts when it cannot ﬁx a line, i.e., if it
fails to ﬁx an erroneous line, it cannot ﬁx the subsequent er-
roneous lines. This limitation arises because of the iterative
nature of DeepFix. If a ﬁx suggested by DeepFix is accepted
by the compiler, the ﬁx is applied and the updated program is
shown to DeepFix to identify the next ﬁx. However, if it is
rejected, the iterative procedure stops. RLAssist, on the other
If the action taken
hand, does not have such a limitation.
by the agent is rejected by the environment, it takes the next
highest value action and continues to attempt other ﬁxes.

Comparison with Baselines
We now discuss two baselines which do not use expert
demonstrations for training. Baseline1 is identical to RLAs-
sist except that it does not use demonstrations. Figure 2b
shows the training performance of Baseline1. It shows that
the training fails to make any progress even after 1.5 million
episodes. As the edit actions are penalized more than the nav-
igation actions, the agent tries only about 5 edits per episode

Figure 3: PCA projection of embeddings of 350 programs in three
different states. The ﬁrst, the second, and the third states are shown
by diamonds, squares, and circles, respectively.

during exploration, causing the training to fail.

Considering this, in Baseline2, we set the edit penalty to
zero. As seen in Figure 2c, this baseline starts showing signs
of learning after a million episodes. After about 1.5 million
episodes (about 10 epochs), it learns to resolve nearly 70% of
the error messages. With additional 10 epochs (not shown in
Figure 2c), it resolves 76% of the error messages. Further, it
matches the performance of DeepFix on the test dataset. In
fact, it slightly outperforms DeepFix. This shows that RLAs-
sist can also be trained entirely through self-exploration with-
out using any expert demonstrations.

Embedding Visualization
We select 350 test programs containing only one error per
program. Next, we get embeddings corresponding to the fol-
lowing three states of each of these programs: (1) when the
cursor is set to the ﬁrst token of the line preceding the erro-
neous line, (2) when the cursor is set to the ﬁrst token of the
erroneous line, and (3) when the cursor is set to the error lo-
cation and the program has been ﬁxed. Figure 3 shows the
ﬁrst three principal components of these embeddings. It can
be seen that the three states form three distinct clusters with
almost no overlap. This shows that RLAssist’s encoder learns
to capture not only the syntactic validity of the programs but
also the location of the errors in them. This in turn helps the
agent select appropriate actions, e.g., move down in the ﬁrst
state and move right in the second state.

6 Conclusions and Future Work

We address the problem of programming language correction
and present a novel deep reinforcement learning based solu-
tion, called RLAssist, for it. We compare RLAssist with a
state-of-the-art tool, DeepFix, on the task of correcting typo-
graphic errors in 6975 student-written erroneous C programs.
Our experiments show that RLAssist outperforms DeepFix.
Moreover, we show that RLAssist can be trained without any
expert demonstrations while still matching performance of
DeepFix.

RLAssist is programming language agnostic and has been
evaluated on C programs. In future, we will experiment with
other programming languages as well. We plan to extend
RLAssist to target more classes of errors, and devise RL al-
gorithms that can learn and exploit deeper syntactic and se-
mantic properties of programs.

Acknowledgments
We thank Sonata Software Ltd.
work.

for partially funding this

References
[Abadi et al., 2016] Mart´ın Abadi, Paul Barham, Jianmin
Chen, Zhifeng Chen, Andy Davis, Jeffrey Dean, Matthieu
Devin, Sanjay Ghemawat, Geoffrey Irving, Michael Is-
ard, et al. Tensorﬂow: A system for large-scale machine
learning. In Proceedings of the 12th USENIX Symposium
on Operating Systems Design and Implementation, vol-
ume 16, pages 265–283, 2016.

[Abbeel and Ng, 2004] Pieter Abbeel and Andrew Y. Ng.
Apprenticeship learning via inverse reinforcement learn-
ing. In Proceedings of the 21st International Conference
on Machine Learning, page 1, 2004.

[Abbeel et al., 2007] Pieter Abbeel, Adam Coates, Morgan
Quigley, and Andrew Y Ng. An application of reinforce-
ment learning to aerobatic helicopter ﬂight. In Advances in
neural information processing systems, pages 1–8, 2007.

[Argall et al., 2009] Brenna Argall,

Sonia Chernova,
Manuela Veloso, and Brett Browning. A survey of robot
learning from demonstration. Robotics and Autonomous
Systems, 67:469–483, 2009.

[Chodorow et al., 2007] Martin Chodorow, Joel R Tetreault,
and Na-Rae Han. Detection of grammatical errors involv-
ing prepositions. In Proceedings of the 4th ACL-SIGSEM
workshop on prepositions, pages 25–30, 2007.

[Das et al., 2017] Rajdeep Das, Umair Z. Ahmed, Amey
Karkare, and Sumit Gulwani. Prutor: A system for tu-
toring cs1 and collecting student programs for analysis,
2017. https://www.cse.iitk.ac.in/users/
karkare/prutor/.

[Gupta et al., 2017] Rahul Gupta, Soham Pal, Aditya
Kanade, and Shirish Shevade. DeepFix: Fixing common
c language errors by deep learning. In Proceedings of the
31st AAAI Conference on Artiﬁcial Intelligence, pages
1345–1351, 2017.

[Han et al., 2006] Na-Rae Han, Martin Chodorow, and Clau-
dia Leacock. Detecting errors in English article usage
by non-native speakers. Natural Language Engineering,
12(2):115–129, 2006.

[Hochreiter and Schmidhuber, 1997] Sepp Hochreiter and
J¨urgen Schmidhuber. Long short-term memory. Neural
computation, 9(8):1735–1780, 1997.

[Kingma and Ba, 2015] Diederik Kingma and Jimmy Ba.
Adam: A method for stochastic optimization. In The 3rd
International Conference on Learning Representations,
2015.

[Mnih et al., 2015] Volodymyr Mnih, Koray Kavukcuoglu,
David Silver, Andrei A. Rusu, Joel Veness, Marc G. Belle-
mare, Alex Graves, Martin Riedmiller, Andreas K. Fid-
jeland, Georg Ostrovski, Stig Petersen, Charles Beattie,
Amir Sadik, Ioannis Antonoglou, Helen King, Dharshan

Kumaran, Daan Wierstra, Shane Legg, and Demis Has-
sabis. Human-level control through deep reinforcement
learning. Nature, 518(7540):529–533, 2015.

[Mnih et al., 2016] Volodymyr Mnih, Adria Puigdomenech
Badia, Mehdi Mirza, Alex Graves, Timothy Lillicrap, Tim
Harley, David Silver, and Koray Kavukcuoglu. Asyn-
In
chronous methods for deep reinforcement learning.
Proceedings of the 33rd International Conference on Ma-
chine Learning, pages 1928–1937, 2016.

[Narasimhan et al., 2015] Karthik Narasimhan, Tejas D.
Kulkarni, and Regina Barzilay. Language understanding
for text based games using deep reinforcement learning.
In Proceedings of the Conference on Empirical Methods
in Natural Language Processing, pages 1–11, 2015.

[Ng et al., 2014] Hwee Tou Ng, Siew Mei Wu, Ted Briscoe,
Christian Hadiwinoto, Raymond Hendy Susanto, and
Christopher Bryant. The CoNLL-2014 shared task on
In CoNLL Shared Task,
grammatical error correction.
pages 1–14, 2014.

[Parihar et al., 2017] Sagar Parihar, Ziyaan Dadachanji,
Praveen Kumar Singh, Rajdeep Das, Amey Karkare, and
Arnab Bhattacharya. Automatic grading and feedback us-
ing program repair for introductory programming courses.
In Proceedings of the ACM Conference on Innovation and
Technology in Computer Science Education, pages 92–97,
2017.

[Pascanu et al., 2013] Razvan Pascanu, Tomas Mikolov, and
Yoshua Bengio. On the difﬁculty of training recurrent
neural networks. In International Conference on Machine
Learning, pages 1310–1318, 2013.

[Ross et al., 2011] St´ephane Ross, Geoffrey J. Gordon, and
Drew Bagnell. A reduction of imitation learning and struc-
tured prediction to no-regret online learning. In Proceed-
ings of the 14th International Conference on Artiﬁcial In-
telligence and Statistics, pages 627–635, 2011.

[Rozovskaya and Roth, 2010] Alla Rozovskaya and Dan
Roth. Generating confusion sets for context-sensitive error
correction. In Proceedings of the conference on empirical
methods in natural language processing, pages 961–970,
2010.

[Rozovskaya et al., 2014] Alla

Kai-Wei
Chang, Mark Sammons, Dan Roth, and Nizar Habash.
The illinois-columbia system in the CoNLL-2014 shared
task. In CoNLL Shared Task, pages 34–42, 2014.

Rozovskaya,

[Sutton and Barto, 1998] Richard S Sutton and Andrew G
Barto. Reinforcement learning: An introduction, vol-
ume 1. MIT press Cambridge, 1998.

[Traver, 2010] V. Javier Traver. On compiler error messages:
What they say and what they mean. Advances in Human-
Computer Interaction, 2010:3:1–3:26, 2010.

[Wu and Tian, 2017] Yuxin Wu and Yuandong Tian. Train-
ing agent for ﬁrst-person shooter game with actor-critic
curriculum learning. In The 5th International Conference
on Learning Representations, 2017.

