8
1
0
2

g
u
A
4
1

]
E
S
.
s
c
[

2
v
6
0
2
5
0
.
5
0
8
1
:
v
i
X
r
a

DeepMutation:MutationTestingofDeepLearningSystemsLeiMa1,2∗,FuyuanZhang2,JiyuanSun3,MinhuiXue2,BoLi4,FelixJuefei-Xu5,ChaoXie3,LiLi6,YangLiu2,JianjunZhao3,andYadongWang11HarbinInstituteofTechnology,China2NanyangTechnologicalUniversity,Singapore3KyushuUniversity,Japan4UniversityofIllinoisatUrbana–Champaign,USA5CarnegieMellonUniversity,USA6MonashUniversity,AustraliaAbstract—Deeplearning(DL)deﬁnesanewdata-drivenpro-grammingparadigmwheretheinternalsystemlogicislargelyshapedbythetrainingdata.ThestandardwayofevaluatingDLmodelsistoexaminetheirperformanceonatestdataset.Thequalityofthetestdatasetisofgreatimportancetogainconﬁdenceofthetrainedmodels.Usinganinadequatetestdataset,DLmodelsthathaveachievedhightestaccuracymaystilllackgeneralityandrobustness.Intraditionalsoftwaretesting,mutationtestingisawell-establishedtechniqueforqualityevaluationoftestsuites,whichanalyzestowhatextentatestsuitedetectstheinjectedfaults.However,duetothefundamentaldifferencebetweentraditionalsoftwareanddeeplearning-basedsoftware,traditionalmutationtestingtechniquescannotbedirectlyappliedtoDLsystems.Inthispaper,weproposeamutationtestingframeworkspecializedforDLsystemstomeasurethequalityoftestdata.Todothis,bysharingthesamespiritofmutationtestingintraditionalsoftware,weﬁrstdeﬁneasetofsource-levelmutationoperatorstoinjectfaultstothesourceofDL(i.e.,trainingdataandtrainingprograms).Thenwedesignasetofmodel-levelmutationoperatorsthatdirectlyinjectfaultsintoDLmodelswithoutatrainingprocess.Eventually,thequalityoftestdatacouldbeevaluatedfromtheanalysisontowhatextenttheinjectedfaultscouldbedetected.Theusefulnessoftheproposedmutationtestingtechniquesisdemonstratedontwopublicdatasets,namelyMNISTandCIFAR-10,withthreeDLmodels.IndexTerms—Deeplearning,Softwaretesting,Deepneuralnetworks,MutationtestingI.INTRODUCTIONOverthepastdecades,deeplearning(DL)hasachievedtremendoussuccessinmanyareas,includingsafety-criticalapplications,suchasautonomousdriving[1],robotics[2],games[3],videosurveillance[4].However,withthewitnessofrecentcatastrophicaccidents(e.g.,Tesla/Uber)relevanttoDL,therobustnessandsafetyofDLsystemsbecomeabigconcern.Currently,theperformanceofDLsystemsismainlymeasuredbytheaccuracyonthepreparedtestdataset.Withoutasystem-aticwaytoevaluateandunderstandthequalityofthetestdata,itisdifﬁculttoconcludethatgoodperformanceonthetestdataindicatestherobustnessandgeneralityofaDLsystem.Thisproblemisfurtherexacerbatedbymanyrecentlyproposedadversarialtestgenerationtechniques,whichperformsminorperturbation(e.g.,invisibletohumaneyes[5])ontheinputdatatotriggertheincorrectbehaviorsofDLsystems.DuetotheuniquecharacteristicsofDLsystems,newevaluation∗LeiMaisthecorrespondingauthor.Email:malei@hit.edu.cn.criteriaonthequalityofDLsystemsarehighlydesirable,andthequalityevaluationoftestdataisofspecialimportance.Fortraditionalsoftware,mutationtesting(MT)[6]hasbeenestablishedasoneofthemostimportanttechniquestosystematicallyevaluatethequalityandlocatetheweaknessoftestdata.AkeyprocedureofMTistodesignandselectmutationoperatorsthatintroducepotentialfaultsintothesoftwareundertest(SUT)tocreatemodiﬁedversions(i.e.,mutants)ofSUT[6],[7].MTmeasuresthequalityoftestsbyexaminingtowhatextentatestsetcoulddetectthebehaviordifferencesofmutantsandthecorrespondingoriginalSUT.Unliketraditionalsoftwaresystems,ofwhichthedecisionlogicisoftenimplementedbysoftwaredevelopersintheformofcode,thebehaviorofaDLsystemismostlydeterminedbythestructureofDeepNeuralNetworks(DNNs)aswellastheconnectionweightsinthenetwork.Speciﬁcally,theweightsareobtainedthroughtheexecutionoftrainingprogramontrainingdataset,wheretheDNNstructuresareoftendeﬁnedbycodefragmentsoftrainingprograminhigh-levellanguages(e.g.,Python[8],[9]andJava[10]).1Therefore,thetrainingdatasetandthetrainingprogramaretwomajorsourcesofdefectsofDLsystems.FormutationtestingofDLsystems,areasonableapproachistodesignmutationoperatorstoinjectpotentialfaultsintothetrainingdataortheDNNtrainingprogram.Afterthefaultsareinjected,thetrainingprocessisre-executed,usingthemutatedtrainingdataortrainingprogram,togeneratethecorrespondingmutatedDLmodels.Inthisway,anumberofmutatedDLmodels{M01,M02,...,M0n}aregeneratedthroughinjectingvariousfaults.Then,eachofthemutantmodelsM0iisexecutedandanalyzedagainstthetestsetT,incorrespondencetooriginalDLmodelM.Givenatestinputt∈T,tdetectsthebehaviordifferenceofMandM0iiftheoutputsofMandM0areinconsistentont.Similartomutationtestingfortraditionalsoftware[6],themorebehaviordifferencesoftheoriginalDLmodelMandthemutantmodels{M01,M02,...,M0n}couldbedetectedbyT,thehigherqualityofTisindicated.Inthispaper,weproposeamutationtestingframeworkspecializedforDLsystems,toenablethetestdataqual-ityevaluation.Weﬁrstdesigneightsource-levelmutationtestingoperatorsthatdirectlymanipulatethetrainingdata1AlthoughthetrainingprogramofaDNNisoftenwritteninhigh-levellanguages,theDNNitselfisrepresentedandstoredasahierarchicaldatastructure(e.g.,.h5formatforKeras[9]). 
 
 
 
 
 
SourceCodeTraditionalSoftware DeveloperInputOutputExecutableCodeCodingCompilation  TransformationDeep LearningSoftware DeveloperTrainingDataTrainingProgramTrainingDeep LearningModelInputOutputCodingCollectionFig.1:AcomparisonoftraditionalandDLsoftwaredevelopment.andtrainingprograms.ThedesignintentionistointroducepossiblefaultsandproblemsintoDLprogrammingsources,whichcouldpotentiallyoccurintheprocessofcollectingtrainingdataandimplementingthetrainingprogram.Forsource-levelmutationtesting,trainingDNNmodelscanbecomputationallyintensive:thetrainingprocesscantakemin-utes,hours,evenlonger[11].Therefore,wefurtherdesigneightmutationoperatorstodirectlymutateDLmodelsforfaultinclusion.Thesemodel-levelmutationoperatorsnotonlyenablemoreefﬁcientgenerationoflargesetsofmutantsbutalsocouldintroducemoreﬁne-grainedmodel-levelproblemsthatmightbemissedbymutatingtrainingdataorprograms.Wehaveperformedanin-depthevaluationoftheproposedmutationtestingtechniquesontwowidelyuseddatasets,namelyMNISTandCIFAR-10,andthreepopularDLmodelswithdiversestructuresandcomplexity.Theevaluationresultdemonstratestheusefulnessoftheproposedtechniquesasapromisingmeasurementtowardsdesigningandconstructinghigh-qualitytestdatasets,whichwouldeventuallyfacilitatetherobustnessenhancementofDLsystems.ItisworthnotingthattheintentionoftheproposedmutationoperatorsisforfaultinjectiononDLmodelssothattestdataqualitycouldbeevaluated,insteadofdirectlysimulatingthehumanfaults.Currently,testingforDLsoftwareisstillatanearlystage,withsomeinitialresearchworkfocusedonaccuracyandneuroncoverage,suchasDeepXplore[12],DeepGauge[13],andDeepCover[14].Tothebestofourknowledge,ourworkistheﬁrstattempttodesignmutationtestingtechniquesspecializedforDLsystems.Themaincontributionsofthispaperaresummarizedasfollows:•WeproposeamutationtestingframeworkandworkﬂowspecializedforDLsystems,whichenablesthequalityevaluationandweaknesslocalizationofthetestdataset.•Wedesigneightsource-level(i.e.,onthetrainingdataandtrainingprogram)mutationoperatorstointroducefaultsintotheDLprogrammingelements.WefurtherdesigneightmutationoperatorsthatdirectlyinjectfaultsintoDLmodels.•WeproposetwoDL-speciﬁcmutationtestingmetricstoallowquantitativemeasurementfortestquality.•WeevaluatetheproposedmutationtestingframeworkonwidelystudiedDLdatasetsandmodels,todemonstratetheusefulnessofthetechnique,whichcouldalsopotentiallyfacilitatethetestsetenhancement.FilteringOriginal Program PInput Test Set TCreateMutants P'Passed Tests T'Execute T' on P'Analysis &ReportMutation OperatorTestsEnhancementFig.2:Keyprocessofgeneralmutationtesting.II.BACKGROUNDA.ProgrammingParadigmsBuildingdeeplearningbasedsystemsisfundamentallydifferentfromthatoftraditionalsoftwaresystems.Traditionalsoftwareistheimplementationoflogicﬂowscraftedbydevelopersintheformofsourcecode(seeFigure1),whichcanbedecomposedintounits(e.g.,classes,methods,state-ments,branches).Eachunitspeciﬁessomelogicandallowstobetestedastargetsofsoftwarequalitymeasurement(e.g.,statementcoverage,branchcoverage).Afterthesourcecodeisprogrammed,itiscompiledintoexecutableform,whichwillberunninginrespectiveruntimeenvironmentstofulﬁlltherequirementsofthesystem.Forexample,inobject-orientedprogramming,developersanalyzetherequirementsanddesignthecorrespondingsoftwarearchitecture.Eachofthearchitec-turalunits(e.g.,classes)representsspeciﬁcfunctionality,andtheoverallgoalisachievedthroughthecollaborationsandinteractionsoftheunits.Deeplearning,ontheotherhand,followsadata-drivenpro-grammingparadigm,whichprogramsthecorelogicthroughthemodeltrainingprocessusingalargeamountoftrain-ingdata.Thelogicisencodedinadeepneuralnetwork,representedbysetsofweightsfedintonon-linearactivationfunctions[15].ToobtainaDLsoftwareFforaspeciﬁctaskM,aDLdeveloper(seeFigure1)needstocollecttrainingdata,whichspeciﬁesthedesiredbehaviorofFonM,andprepareatrainingprogram,whichdescribesthestructureofDNNandruntimetrainingbehaviors.TheDNNisbuiltbyrunningthetrainingprogramonthetrainingdata.ThemajoreffortforaDLdeveloperistoprepareasetoftrainingdataanddesignaDNNmodelstructure,andDLlogicisdeterminedautomaticallythroughthetrainingprocedure.Incontrasttotraditionalsoftware,DLmodelsareoftendifﬁculttobedecomposedorinterpreted,makingthemunamenabletomostexistingsoftwaretestingtechniques.Moreover,itischallengingtoﬁndhigh-qualitytrainingandtestdatathatrepresenttheproblemspaceandhavegoodcoverageofthemodelstoevaluatetheirgenerality.Original TrainingData DOriginal TrainingProgram PTrainingDeep LearningMutant Model M'Test Set TCreate MutantData D'Data Mutation OperatorProg. Mutation OperatorCreate MutantProgram P'TrainingFilteringDeep LearningModel MAnalysisReportPassed Tests T'TrainingFig.3:Source-levelmutationtestingworkﬂowofDLsystems.B.MutationTestingThegeneralprocessofmutationtesting[6],[16]fortra-ditionalsoftwareisillustratedinFigure2.GivenanoriginalprogramP,asetoffaultyprogramsP0(mutants)arecreatedbasedonpredeﬁnedrules(mutationoperators),eachofwhichslightlymodiﬁesP.Forexample,amutationoperatorcansyntacticallychange‘+’operatorintheprogramto‘−’operator[17]–[19].Astepofpreprocessing,usuallybeforetheactualmutationtestingprocedurestarts,isusedtoﬁlteroutirrelevanttests.Speciﬁcally,thecompletetestsetTisexecutedagainstPandonlythepassedtestsT0(asubsetofT)areusedformutationtesting.Inthenextstep,eachmutantofP0isexecutedonT0.Ifthetestresultforamutantp0∈P0isdifferentfromthatofP,thenp0iskilled;otherwise,p0issurvived.WhenallthemutantsinP0havebeentestedagainstT0,mutationscoreiscalculatedastheratioofkilledmutantstoallthegeneratedmutants(i.e.,#mutantskilled/#mutantsall),whichindicatesthequalityoftestset.Conceptually,atestsuitewithahighermutationscoreismorelikelytocapturerealdefectsintheprogram[20].Afterobtainingthemutationtestingresults,thedevelopercouldfurtherenhancethequalityoftestset(e.g.,byadding/generatingmoretests)basedonthefeedbackfrommutationtesting[21],[22].ThegeneralgoalofmutationtestingistoevaluatethequalityoftestsetT,andfurtherprovidefeedbackandguidethetestenhancement.III.SOURCE-LEVELMUTATIONTESTINGOFDLSYSTEMSIngeneral,traditionalsoftwareismostlyprogrammedbydevelopersintheformofsourcecode(Figure1),whichcouldbeamajorsourceofdefectintroduction.Mutationtestingslightlymodiﬁestheprogramcodetointroducefaults,whichenablesthemeasurementoftestdataqualitythroughdetectingsuchdeliberatelychanges.Withthesamespiritofmutationtestingfortraditionalsoftware,directlyintroducingpotentialdefectsintothepro-grammingsourcesofaDLsystemisareasonableapproachtocreatemutants.Inthissection,weproposeasource-levelmutationtestingtechniqueforDLsystems.WedesignageneralmutationtestingworkﬂowforDLsystems,andproposeasetofmutationoperatorsasthekeycomponents.TABLEI:Source-levelmutationtestingoperatorsforDLsystems.FaultTypeLevelTargetOperationDescriptionDataRepetition(DR)GlobalDataDuplicatestrainingdataLocalDuplicatesspeciﬁctypeofdataLabelError(LE)GlobalDataFalsifyresults(e.g.,labels)ofdataLocalFalsifyspeciﬁcresultsofdataDataMissing(DM)GlobalDataRemoveselecteddataLocalRemovespeciﬁctypesofdataDataShufﬂe(DF)GlobalDataShufﬂeselectedtrainingdataLocalShufﬂespeciﬁctypesofdataNoisePerturb.(NP)GlobalDataAddnoisetotrainingdataLocalAddnoisetospeciﬁctypeofdataLayerRemoval(LR)GlobalProg.RemovealayerLayerAddition(LAs)GlobalProg.AddalayerAct.Fun.Remov.(AFRs)GlobalProg.RemoveactivationfunctionsFurthermore,wedeﬁnethemutationtestingmetricsforquan-titativemeasurementandevaluationofthetestdataquality.A.Source-levelMutationTestingWorkﬂowforDLSystemsFigure3showsthekeyworkﬂowofoursource-levelmutationtestingtechnique.Attheinitializationphase,aDLdeveloperpreparesatrainingprogramPandasetoftrainingdataD.Afterthetrainingprocess,whichrunsPwithD,aDLmodelMisobtained.Whenthemutationtestingstarts,theoriginaltrainingdataDandprogramPareslightlymodiﬁedbyapplyingmutationoperators(deﬁnedinTableI),andthecorrespondingmutantsD0andP0aregenerated.Inthenextstep,eitheratrainingdatamutantortrainingprogrammutantparticipatesinthetrainingprocesstogenerateamutatedDLmodelM0.WhenmutatedDLmodelsareobtained,theyareexecutedandanalyzedagainsttheﬁlteredtestsetT0forevaluatingthequalityoftestdata.2Weemphasizethat,theproposedmutationoperatorsinthispaperarenotintendedtodirectlysimulatehumanfaults;instead,theyaimtoprovidewaysforquantitativemeasurementonthequalityoftestdataset.Inparticular,themorebehaviordifferencesbetweentheoriginalDLmodelandthemutantmodels(generatedbymutationoperators)T0coulddetect,thehigherqualityofT0isindicated.ThedetailedqualitymeasurementmetricsaredeﬁnedinSectionIII-C.B.Source-levelMutationOperatorsforDLSystemsWeproposetwogroupsofmutationoperators,namelydatamutationoperatorsandprogrammutationoperators,whichperformthecorrespondingmodiﬁcationonsourcestointroducepotentialfaults(seeTableI).1)DataMutationOperators:TrainingdataplaysavitalroleinbuildingDLmodels.Thetrainingdataisusuallylargeinsizeandlabeledmanually[23]–[25].Preparingtrainingdataisusuallylaboriousandsometimeserror-prone.Ourdatamutationoperatorsaredesignedbasedontheobservationofpotentialproblemsthatcouldoccurduringthedatacollectionprocess.Theseoperatorscaneitherbeappliedgloballytoalltypesofdata,orlocallyonlytospeciﬁctypesofdatawithintheentiretrainingdataset.2T0isconsistedofthetestdatapointsinTthatarecorrectlyprocessedbytheoriginalDLmodelM.•DataRepetition(DR):TheDRoperatorduplicatesasmallportionoftrainingdata.Thetrainingdataisoftencollectedfrommultiplesources,someofwhicharequitesimilar,andthesamedatapointcanbecollectedmorethanonce.•LabelError(LE):Eachdatapoint(d,l)inthetrainingdatasetD,wheredrepresentsthefeaturedataandlisthelabelford.AsDisoftenquitelarge(e.g.,MNISTdataset[23]contains60,000trainingdata),itisnotun-commonthatsomedatapointscanbemislabeled.TheLEoperatorinjectssuchkindoffaultsbychangingthelabelforadata.•DataMissing(DM):TheDMoperatorremovessomeofthetrainingdata.Itcouldpotentiallyhappenbyinadvertentormistakendeletionofsomedatapoints.•DataShufﬂe(DF):TheDFoperatorshufﬂesthetrainingdataintodifferentordersbeforethetrainingprocess.Theo-retically,thetrainingprogramrunsagainstthesamesetoftrainingdatashouldobtainthesameDLmodel.However,theimplementationoftrainingprocedureisoftensensitivetotheorderoftrainingdata.Whenpreparingtrainingdata,developersoftenpaylittleattentiontotheorderofdata,andthuscaneasilyoverlooksuchproblemsduringtraining.•NoisePerturbation(NP):TheNPoperatorrandomlyaddsnoisetotrainingdata.Adatapointcouldcarrynoisefromvarioussources.Forexample,acamera-capturedimagecouldincludenoisecausedbydifferentweathercondi-tions(i.e.,rain,snow,dust,etc.).TheNPoperatortriestosimulatepotentialissuesrelevanttonoisytrainingdata(e.g.,NPaddsrandomperturbationstosomepixelsofanimage).2)ProgramMutationOperators:Similartotraditionalpro-grams,atrainingprogramiscommonlycodedusinghigh-levelprogramminglanguages(e.g.,PythonandJava)underspeciﬁcDLframework.Thereareplentyofsyntax-basedmutationtestingtoolsavailablefortraditionalsoftware[26]–[30],anditseemsstraightforwardtodirectlyapplythesetoolstothetrainingprogram.However,thisapproachoftendoesnotwork,duetothefactthatDLtrainingprogramsaresensitivetocodechanges.Evenaslightchangecancausethetrainingprogramtofailatruntimeortoproducenoticeabletrainingprocessanomalies(e.g.,obviouslowpredictionaccuracyattheearlyit-erations/epochsofthetraining).ConsideringthecharacteristicsofDLtrainingprograms,wedesignthefollowingoperatorstoinjectpotentialfaults.•LayerRemoval(LR):TheLRoperatorrandomlydeletesalayeroftheDNNsontheconditionthatinputandoutputstructuresofthedeletedlayerarethesame.Althoughitispossibletodeleteanylayerthatsatisﬁesthiscondition,arbitrarilydeletingalayercangenerateDLmodelsthatareobviouslydifferentfromtheoriginalDLmodel.Therefore,theLRoperatormainlyfocusesonlayers(e.g.,Dense,BatchNormalizationlayer[31]),whosedeletiondoesnotmaketoomuchdifferenceonthemutatedmodel.TheLRoperatormimicsthecasethatalineofcoderepresentingaDNNlayerisremovedbythedeveloper.Mutant&decision&boundary&2Mutant&decision&boundary&1Original&decision&boundaryFig.4:ExampleofDLmodelanditstwogeneratedmutantmodelsforbinaryclassiﬁcationwiththeirdecisionboundaries.Intheﬁgure,somedatascatterclosertothedecisionboundary(ingreencolor).Ourmutationtestingmetricsfavortoidentifythetestdatathatlocateinthesensitiveregionnearthedecisionboundary.•LayerAddition(LAs):IncontrasttotheLRopera-tor,theLAsoperatoraddsalayertotheDNNsstruc-ture.LAsfocusesonaddinglayerslikeActivation,BatchNormalization,whichintroducespossiblefaultscausedbyaddingorduplicatingalineofcoderepresentingaDNNlayer.•ActivationFunctionRemoval(AFRs):Activationfunc-tionplaysanimportantroleofthenon-linearityofDNNsforhigherrepresentativeness(i.e.,quantiﬁedasVCdimension[15]).TheAFRsoperatorrandomlyremovesalltheactivationfunctionsofalayer,tomimicthesituationthatthedeveloperforgetstoaddtheactivationlayers.C.MutationTestingMetricsforDLSystemsAfterthetrainingdataandtrainingprogramaremutatedbythemutationoperators,asetofmutantDLmodelsM0canbeobtainedthroughtraining.Eachtestdatapointt0∈T0thatiscorrectlyhandledbytheoriginalDLmodelM,isevaluatedonthesetofmutantmodelsM0.WesaythattestdataT0killmutantm0ifthereexistsatestinputt0∈T0thatisnotcorrectlyhandledbym0.Themutationscoreoftraditionalmutationtestingiscalculatedastheratioofkilledmutantstoallmutants.However,itisinappropriatetousethesamemutationscoremetricsoftraditionalsoftwareasthemetricsformutationtestingofDLsystems.InthemutationtestingofDLsystems,itisrelativelyeasyforT0tokillamutantm0whenthesizeofT0islarge,whichisalsoconvincedfromourexperimentinSectionV.Therefore,ifweweretodirectlyusethemutationscoreforDLsystemsastheratioofkilledmutantstoallmutants,ourmetricwouldlosetheprecisiontoevaluatethequalityoftestdataforDLsystems.Inthispaper,wefocusonDLsystemsforclassiﬁcationproblems.3Supposewehaveak-classiﬁcationproblemandletC={c1,...,ck}beallthekclassesofinputdata.Foratestdatapointt0∈T0,wesaythatt0killsci∈Cofmutant3Although,themutationscoremetricdeﬁnedinthispapermainlyfocusesonclassiﬁcationproblems,thesimilarideacanbeeasilyextendedtohandlenumericalpredicationproblemaswell,withauser-deﬁnedthresholdastheerrorallowancemargin[32].TrainingDeep LearningModel MTest Set TOriginal TrainingData DOriginal TrainingProgram PFilteringAnalysisReportDeep LearningMutant Model M'Model MutationOperatorFig.5:ThemodellevelmutationtestingworkﬂowforDLsystems.m0∈M0ifthefollowingconditionsaresatisﬁed:(1)t0iscorrectlyclassiﬁedascibytheoriginalDLmodelM,and(2)t0isnotclassiﬁedascibym0.WedeﬁnethemutationscoreforDLsystemsasfollows,whereKilledClasses(T0,m0)isthesetofclassesofm0killedbytestdatainT0:MutationScore(T0,M0)=Pm0∈M0|KilledClasses(T0,m0)||M0|×|C|Ingeneral,itcouldbedifﬁculttopreciselypredictthebehaviouraldifferenceintroducedbymutationoperators.ToavoidintroducingtoomanybehaviouraldifferencesforaDLmutantmodelfromitsoriginalcounterpart,weproposeaDLmutantmodelqualitycontrolprocedure.Inparticular,wemeasuretheerrorrateofeachmutantm0onT0.Iftheerrorrateofm0istoohighforT0,wedon’tconsiderm0agoodmutantcandidateasitintroducesalargebehavioraldifference.WeexcludedsuchmutantmodelsfromM0forfurtheranalysis.Wedeﬁneaverageerrorrate(AER)ofT0oneachmutantmodelm0∈M0tomeasuretheoverallbehaviordifferentialeffectsintroducedbyallmutationoperators.AveErrorRate(T0,M0)=Pm0∈M0ErrorRate(T0,m0)|M0|Figure4showsanexampleofaDLmodelforbinaryclassiﬁcation,withthedecisionboundaryoftheoriginalmodelandthedecisionboundariesoftwomutantmodels.Wecanseethatthemutantmodelsaremoreeasilytobekilledbydataingreen,whichliesnearthedecisionboundaryoftheoriginalDLmodel.Thecloseradatapointistothedecisionboundary,thehigherchanceithastokillmoremutantmodels,whichisreﬂectedastheincreaseofthemutationscoreandAERdeﬁnedforDLsystems.Ingeneral,mutationtestingfacilitatestoevaluatetheeffectivenessoftestset,byanalyzingtowhatextentthetestdataisclosedtothedecisionboundaryofDNNs,wheretherobustnessissuesmoreoftenoccur.IV.MODEL-LEVELMUTATIONTESTINGOFDLSYSTEMSInSectionIII,wedeﬁnethesource-levelmutationtestingprocedureandworkﬂow,whichsimulatethetraditionalmu-tationtestingtechniquesdesignedtoworkonsourcecodeTABLEII:Model-levelmutationtestingoperatorsforDLsystems.MutationOperatorLevelDescriptionGaussianFuzzing(GF)WeightFuzzweightbyGaussianDistributionWeightShufﬂing(WS)NeuronShufﬂeselectedweightsNeuronEffectBlock.(NEB)NeuronBlockaneuroneffectonfollowinglayersNeuronActivationInverse(NAI)NeuronInverttheactivationstatusofaneuronNeuronSwitch(NS)NeuronSwitchtwoneuronsofthesamelayerLayerDeactivation(LD)LayerDeactivatetheeffectsofalayerLayerAddition(LAm)LayerAddalayerinneuronnetworkAct.Fun.Remov.(AFRm)LayerRemoveactivationfunctions(seeFigure1).Ingeneral,toimprovemutationtestingef-ﬁcient,manytraditionalmutationtestingtechniquesarede-signedtoworkonalow-levelsoftwarerepresentation(e.g.,Bytecode[18],[30],BinaryCode[33],[34])insteadofthesourcecode,whichavoidtheprogramcompilationandtransformationeffort.Inthissection,weproposethemodel-levelmutationtestingforDLsystemtowardsmoreefﬁcientDLmutantmodelgeneration.A.Model-LevelMutationTestingWorkﬂowforDLSystemsFigure5showstheoverallworkﬂowofDLmodellevelmutationtestingworkﬂow.Incontrasttothesource-levelmutationtestingthatmodiﬁestheoriginaltrainingdataDandtrainingprogramP,modellevelmutationtestingdirectlychangestheDLmodelMobtainedthroughtrainingfromDandP.ForeachgeneratedDLmutantmodelm0∈M0byourdeﬁnedmodel-levelmutationoperatorsinTableII,inputtestdatasetTisrunonMtoﬁlteroutallincorrectdataandthepasseddataaresenttoruneachm0.TheobtainedexecutionresultsadoptthesamemutationmetricsdeﬁnedinSectionIII-Cforanalysisandreport.Similartosource-levelmutationtesting,model-levelmuta-tiontestingalsotriestoevaluatetheeffectivenessandlocatetheweaknessofatestdataset,whichhelpsadevelopertofurtherenhancethetestdatatoexercisethefragileregionsofaDLmodelundertest.SincethedirectmodiﬁcationofDLmodelavoidsthetrainingprocedure,model-levelmutationtestingisexpectedtobemoreefﬁcientforDLmutantmodelgeneration,whichissimilartothelow-level(e.g.,intermediatecoderepresentationsuchasJavaBytecode)mutationtestingtechniquesoftraditionalsoftware.B.Model-levelMutationOperatorsforDLSystemsMutatingtrainingdataandtrainingprogramwilleventuallymutatetheDLmodel.However,thetrainingprocesscanbecomplicated,beingaffectedbyvariousparameters(e.g.,thenumberoftrainingepochs).Toefﬁcientlyintroducepossiblefaults,wefurtherproposemodel-levelmutationoperators,whichdirectlymutatethestructureandparametersofDLmodels.TableIIsummarizestheproposedmodel-levelmu-tationoperators,whichrangefromweightleveltolayerlevelintermsofapplicationscopesoftheoperators.•GaussianFuzzing(GF):WeightsarebasicelementsofDNNs,whichdescribetheimportanceofconnectionsbe-tweenneurons.WeightsgreatlycontributetothedecisionlogicofDNNs.Anaturalwaytomutatetheweightistofuzzitsvaluetochangetheconnectionimportanceitrepresents.TheGFoperatorfollowstheGaussiandistribu-tionN(w,σ2)tomutateagivenweightvaluew,whereσisauser-conﬁgurablestandarddeviationparameter.TheGFoperatormostlyfuzzesaweighttoitsnearbyvaluerange(i.e.,thefuzzedvaluelocatesin[w−3σ,w+3σ]with99.7%probability),butalsoallowsaweighttobechangedtoagreaterdistancewithasmallerchance.•WeightShufﬂing(WS):Theoutputofaneuronisoftendeterminedbyneuronsfromthepreviouslayer,eachofwhichhasconnectionswithweights.TheWSoperatorrandomlyselectsaneuronandshufﬂestheweightsofitsconnectionstothepreviouslayer.•NeuronEffectBlocking(NEB):WhenatestdatapointisreadintoaDNN,itisprocessedandpropagatedthroughconnectionswithdifferentweightsandneuronlayersuntiltheﬁnalresultsareproduced.EachneuroncontributestotheDNN’sﬁnaldecisiontosomeextentaccordingtoitsconnectionstrength.TheNEBoperatorblocksneuroneffectstoalloftheconnectedneuronsinthenextlayers,whichcanbeachievedbyresettingitsconnectionweightsofthenextlayerstozero.TheNEBremovestheinﬂuenceofaneurontotheﬁnalDNN’sdecision.•NeuronActivationInverse(NAI):Theactivationfunctionplaysakeyroleincreatingthenon-linearbehaviorsoftheDNNs.Manywidelyusedactivationfunctions(e.g.,ReLU[35],LeakyReLU[36])showquitedifferentbehav-iorsdependingontheiractivationstatus.TheNAIoperatortriestoinverttheactivationstatusofaneuron,whichcanbeachievedbychangingthesignoftheoutputvalueofaneuronbeforeapplyingitsactivationfunction.Thisfacilitatestocreatemoremutantneuronactivationpatterns,eachofwhichcanshownewmathematicalproperties(e.g.,linearproperties)ofDNNs[37].•NeuronSwitch(NS):TheneuronsofaDNN’slayeroftenhavedifferentimpactsontheconnectedneuronsinthenextlayers.TheNSoperatorswitchestwoneuronswithinalayertoexchangetheirrolesandinﬂuencesforthenextlayers.•LayerDeactivation(LD):EachlayerofaDNNtransformstheoutputofitspreviouslayerandpropagatesitsresultstoitsfollowinglayers.TheLDoperatorisalayerlevelmu-tationoperatorthatremovesawholelayer’stransformationeffectsasifitisdeletedfromtheDNNs.However,simplyremovingalayerfromatrainedDLmodelcanbreakthemodelstructure.WerestricttheLDoperatortolayerswhosetheinputandoutputshapesareconsistent.•LayerAddition(LAm):TheLAmoperatortriestomaketheoppositeeffectsoftheLDoperator,byaddingalayertotheDNNs.SimilartotheLDoperator,theLAmoperatorworksunderthesameconditionstoavoidbreakingoriginalDNNs;besides,theLAmoperatoralsoincludestheduplicationandinsertionofcopiedlayerafteritsoriginallayers,whichalsorequirestheshapeoflayerinputandoutputtobeconsistent.•ActivationFunctionRemoval(AFRm):AFRmoperatorremovestheeffectsofactivationfunctionofawholelayer.TheAFRmoperatordiffersfromtheNAIoperatorintwoTABLEIII:EvaluationsubjectdatasetsandDLmodels.OurselectedsubjectdatasetsMNISTandCIFAR-10arewidelystudiedinpreviouswork.WetraintheDNNsmodelwithitscorrespondingoriginaltrainingdataandtrainingprogram.TheobtainedDLmodelreferstotheoriginalDL(i.e.,theDLmodelMinFigure3and5),whichweuseasthebaselineinourevaluation.EachstudiedDLmodelstructureandtheobtainedaccuracyaresummarizedbelow.MNISTCIFAR-10A(LeNet5)[23]B[38]C[39]Conv(6,5,5)+ReLUConv(32,3,3)+ReLUConv(64,3,3)+ReLUMaxPooling(2,2)Conv(32,3,3)+ReLUConv(64,3,3)+ReLUConv(16,5,5)+ReLUMaxPooling(2,2)MaxPooling(2,2)MaxPooling(2,2)Conv(64,3,3)+ReLUConv(128,3,3)+ReLUFlatten()Conv(64,3,3)+ReLUConv(128,3,3)+ReLUFC(120)+ReLUMaxPooling(2,2)MaxPooling(2,2)FC(84)+ReLUFlatten()Flatten()FC(10)+SoftmaxFC(200)+ReLUFC(256)+ReLUFC(10)+SoftmaxFC(256)+ReLUFC(10)#Train.Para.107,786694,4021,147,978Train.Acc.97.4%99.3%97.1%Test.Acc.97.0%98.7%78.3%perspectives:(1)AFRmworksonthelayerlevel,(2)AFRmremovestheeffectsofactivationfunction,whileNAIop-eratorkeepstheactivationfunctionandtriestoinverttheactivationstatusofaneuron.V.EVALUATIONWehaveimplementedDeepMutation,aDLmutationtest-ingframeworkincludingbothproposedsource-levelandmodel-levelmutationtestingtechniquesbasedonKeras(ver.2.1.3)[9]withTensorﬂow(ver.1.5.0)backend[8].Thesource-levelmutationtestingtechniqueisimplementedbyPythonandhastwokeycomponents:automatedtrainingdatamutantgeneratorandPythontrainingprogrammutantgenerator(seeFigure3andTableI).Themodel-levelmutationtestingautomaticallyanalyzesaDNN’sstructureandusesourdeﬁnedoperatorstomutateonacopyoftheoriginalDNN.Thenthegeneratedmutantmodelsareserializedandstoredas.h5ﬁleformat.Theweight-levelandneuron-levelmutationoperators(seeTableII)areimplementedbymutatingtherandomlyselectedportionoftheDNN’sweightmatrixelements.Theimplementationoflayer-levelmutationoperatorsismorecomplex.WeﬁrstanalyzethewholeDNN’sstructuretoidentifythecandidatelayersoftheDNNthatsatisfythelayer-levelmutationconditions(seeSectionIV-B).Then,weconstructanewDLmutantmodelbasedontheoriginalDLmodelthroughthefunctionalinterfaceofKerasandTensforﬂow[31].Inordertodemonstratetheusefulnessofourproposedmutationtestingtechnique,weevaluatedtheimplementedmutationtestingframeworkontwopracticaldatasetsandthreeDLmodelarchitectures,whichwillbeexplainedintherestofthissection.A.SubjectDatasetandDLModelsWeselectedtwopopularpubliclyavailabledatasetsMNIST[40]andCIFAR-10[41]astheevaluationsubjects.MNISTisforhandwrittendigitimagerecognition,containing60,000trainingdataand10,000testdata,withatotalnumberof70,000datain10classes(digitsfrom0to9).CIFAR-10datasetisacollectionofimagesforgeneralpurposeimageclassiﬁcation,including50,000trainingdataand10,000testdatain10differentclasses(e.g.,airplanes,cars,birds,andcats).Foreachdataset,westudypopularDLmodels[23],[38],[39]thatarewidelyusedinpreviouswork.TableIIIsum-marizesthestructuresandcomplexityofthestudiedDNNs,aswellasthepredictionaccuracyobtainedonourtrainedDNNs.ThestudiedDLmodelsA,B,andCcontain107,786,694,402,and1,147,978trainableparameters,respectively.ThetrainableparametersofDNNsarethoseparametersthatcouldbeadjustedduringthetrainingprocessforhigherlearn-ingperformance.ItisoftenthecasethatthemoretrainableparametersaDLmodelhas,themorecomplexamodelwouldbe,whichrequireshighertrainingandpredictioneffort.Wefollowthetraininginstructionsofthepapers[23],[38],[39]totraintheoriginalDLmodels.Overall,onMNIST,modelAachieves97.4%trainingaccuracyand97.0%testaccuracy;modelBachieves99.3%and98.7%,comparabletothestateoftheart.OnCIFAR-10,modelCachieves97.1%trainingaccuracyand78.3%testaccuracy,similartotheaccuracygivenin[39].Basedontheselecteddatasetsandmodels,wedesignexper-imentstoinvestigatewhetherourmutationtestingtechniqueishelpfultoevaluatethequalityandprovidefeedbackonthetestdata.Tosupportlargescaleevaluation,weruntheexperimentsonahighperformancecomputercluster.EachclusternoderunsaGNU/LinuxsystemwithLinuxkernel3.10.0ona18-core2.3GHzXeon64-bitCPUwith196GBofRAMandalsoanNVIDIATeslaM40GPUwith24G.B.ControlledDatasetandDLMutantModelGeneration1)TestData:Theﬁrststepofthemutationtestingistopreparethetestdataforevaluation.Ingeneral,atestdatasetisoftenindependentofthetrainingdataset,butfollowsasimilarprobabilitydistributionasthetrainingdataset[42],[43].AgoodtestdatasetshouldbecomprehensiveandcoversdiversefunctionalaspectsofDLsoftwareuse-case,soastoassessperformance(i.e.,generalization)andrevealtheweaknessofafullytrainedDLmodel.Forexample,intheautonomousdrivingscenario,thecapturedroadimagesandsignalsfromcamera,LIDAR,andinfraredsensorsareusedasinputsforDLsoftwaretopredictthesteeringangleandbraking/accelerationcontrol[44].Agoodtestdatasetshouldcontainawiderangeofdrivingcasesthatcouldoccurinpractice,suchasstraitroad,curveroad,differentroadsurfaceconditionsandweatherconditions.Ifatestdatasetonlycoverslimitedtestingscenarios,goodperformanceonthetestdatasetdoesnotconcludethattheDLsoftwarehasbeenwelltested.Todemonstratetheusefulnessofourmutationtestingforthemeasurementoftestdataquality,weperformedacontrolledexperimentontwodatasettings(seeTableIV).Settingonesamples5,000datafromoriginaltrainingdatawhilesettingTABLEIV:Thecontrolledexperimentdatapreparationsettings.ControlledMNIST/CIFAR-10DataSetSetting1Setting2Group1Group2Group1Group2SourceTrain.dataTrain.dataTestdataTestdataSamplingUniformNon-uniformUniformNon-uniform#Size5,0005,0001,0001,000twosampled1,000fromtheaccompaniedtestdata,bothofwhichtakeupapproximately10%ofthecorrespondingdataset.4Eachsettinghasapairofdataset(T1,T2),whereT1isuniformlysampledfromallclassesandT2isnon-uniformlysampled.5Theﬁrstgroupofeachsettingcoversmorediverseuse-caseoftheDLsoftwareofeachclass,whilethesecondgroupofdatasetmainlyfocusesonasingleclass.ItisexpectedthatT1shouldobtainahighermutationscore,andwecheckwhetherourmutationtestingconﬁrmsthis.Werepeatthedatasamplingforeachsettingﬁvetimestocounterrandomnesseffectsduringsampling.Thisallowstoobtainﬁvepairsofdataforeachsetting(i.e.,(T1,T2)1,(T1,T2)2,...,(T1,T2)5).EachpairofdataisevaluatedonthegeneratedDLmutantmodels,andweaveragethemutationtestinganalysisresults.Afterthecandidatedataarepreparedformutationtesting,theyareexecutedoneachofcorrespondingoriginalDLmodelstoﬁlteroutthosefailedcases,andonlythepasseddataareusedforfurthermutationanalysis.Thisproceduregeneratesatotalof30(=2settings*3models*5repetition)pairsofcandidatedatasets,whereeachofthethreeDLmodelshas10pairs(i.e.,5foreachsetting)ofdatasetforanalysis.2)DLMutantModelGeneration:Afterpreparingthecon-trolleddatasets,westartthemutationtestingprocedure.OnekeystepistogeneratetheDLmutantmodels.ForeachstudiedDLmodelinTableIII,wegeneratetheDLmutantmodelsusingboththesource-levelandmodel-levelmutantgenerators.Togeneratesource-levelDLmutantmodels,weconﬁgureourdata-levelmutationoperatorstoautomaticallymutate1%oforiginaltrainingdataandapplyeachoftheprogram-levelmutationoperatorstothetrainingprogram(seeTableI).Afterthemutantdataset(program)aregenerated,theyaretrainedontheoriginaltrainingprogram(trainingdata)toobtainthemutantDLmodels.Consideringtheintensivetrainingeffort,weconﬁguretogenerate20DLmutantsforeachdata-levelmutationoperator(i.e.,10forgloballeveland10forlocallevel).Forprogram-levelmutators,wetrytoperformmutationwhenevertheconditionsaresatisﬁedwithamaximal20mutantmodelsforeachprogram-leveloperator.Togeneratemodel-levelmutantsattheweightandneuronlevel,weconﬁguretosample1%,weightsandneuronsfromthestudiedDNNs,andusethecorrespondingmutationoper-atorstorandomlymutatetheselectedtargets(seeTableII).Onthelayerlevel,ourtoolautomaticallyanalyzesthelayersthatsatisfythemutationconditions(seeSectionIV-B)and4Weusesamplinginevaluationsincethegeneralground-truthfortestsetqualityisunavailable5Tobespeciﬁc,weprioritizetoselectonerandomclassdatawith80%probability,whiledatafromotherclassessharetheremaining20%chance.TABLEV:TheaverageerrorrateofcontrolledexperimentdataontheDLmutantmodels.Wecontrolthesamplingmethodanddatasizetobethesame,andletthedataselectionscopeasthevariable.Theﬁrstgroupsampledatafromallclassesoforiginalpassedtestdata,whilethesecondgroupsampledatafromasingleclass.ModelSourceLevel(%)ModelLevel(%)5000train.1000test.5000train.1000test.Samp.Uni.Non.Uni.Non.Uni.Non.Uni.Non.A2.430.130.230.174.554.304.384.06B0.490.280.660.211.671.561.551.47C3.842.9917.2013.449.117.3411.489.00randomlyappliesthecorrespondingmutationoperator.Themodel-levelmutantgenerationisratherefﬁcientwithoutthetrainingeffort.Therefore,foreachweight-andneuron-levelmutationoperatorwegenerate50mutantmodels.Similarly,ourtooltriestogeneratelayer-levelmutantmodelswhenDNN’sstructureconditionsaresatisﬁedwithmaximal50mutantmodelsforeachlayer-levelmutationoperator.C.MutationTestingEvaluationandResultsAfterthecontrolleddatasetsandmutantmodelsaregen-erated,themutationtestingstartstheexecutionphasebyrunningcandidatetestdatasetonmutantmodels,afterwhichwecalculatethemutationscoreandaverageerrorrate(AER)foreachdataset.NotethatthedatasetusedforevaluationarethosedatathatpassedonoriginalDLmodels.Inaddition,wealsointroduceaqualitycontrolprocedureforgeneratedmutantmodels.AfterweobtainedthepassedtestdataT0ontheoriginalmodel(seeFigure3),werunitagainsteachofitscorrespondinggeneratedmutantmodels,andremovethosemodelswithhigherrorrate,6assuchmutantmodelshowbigbehavioraldifferencesfromoriginalmodels.TableVsummarizestheAERobtainedforeachcontrolleddatasetonallDLmutantmodels.WecanseethattheobtainedDLmutantmodelsindeedenabletoinjectfaultsintoDLmodelswiththeAERrangingfrom0.13%to17.20%,wheremostoftheAERsarerelativelysmall.Inalltheexperimentallycontrolleddatasettings,theuniformlysampleddatagroupachieveshigheraverageerrorrateonthemutantmodels,whichindicatestheuniformlysampleddatahashigherdefectdetectionability(betterqualityfromatestingperspective).FormodelC,whenconsideringbothsource-levelandmodel-level,arelativelylowAERisobtainedforthesampledtrainingdatasetsfrom2.99%upto9.11%,butwithahigherAERofsampledtestingdatafrom9.00%to17.20%.ThisindicatesthatthesampledtestdataqualityofmodelCisbetterintermsofkillingthemutantscomparedwiththesampledtrainingdata,althoughthesampledtrainingdatahaslargerdatasize(i.e.,5,000).InlinewiththeAER,theaveragedmutationscoreforeachsettinginTableIVisalsocalculated,asshowninFigure6and7.Again,onallthecontrolleddatapairsettings,ahighermutationscoreisobtainedbyuniformsamplingmethod,which6Thisstudysetstheerrorratebartobe20%.Itcouldbeconﬁguredtosmallervaluestokeepmodelswithevenmoresimilarbehaviorswiththeoriginalmodel.0%20%40%60%80%100%Model AModel BModel C65.09%40.72%25.33%90.07%60.45%48.6%Uniform-SamplingNonUniform-Sampling0%20%40%60%80%100%Model AModel BModel C94.29%9.23%10.07%100%37.43%18.07%(a) 5000 training data sampling(b) 1000 test data samplingFig.6:Theaveragedmutationscoreofsource-levelmutationtesting.0%16%32%48%64%80%Model AModel BModel C53.06%28.52%55.92%66.94%44.94%76.05%Uniform-SamplingNonUniform-Sampling0%16%32%48%64%80%Model AModel BModel C51.48%8.77%28.03%76.2%20.25%56.32%(a) 5000 training data sampling(b) 1000 test data samplingFig.7:Theaveragedmutationscoreofmodel-levelmutationtesting.alsoconﬁrmsourexpectationonthetestdataquality.BesidestheAERthatmeasurestheratioofdatathatdetectthedefectsofmutantmodels,mutationscoremeasureshowwellthetestdatacovermutationmodelsfromthetestinguse-casediversityperspective.ThemutationscoredoesnotnecessarilypositivelycorrelatewiththeAER,asdemonstratedinthenextsection.Intuitively,atestdatasetwithmoredatamightuncovermoredefectsandtestingaspects.However,thisisnotgenerallycorrectasconﬁrmedinourexperiment.InTableV,forsource-levelmutationtestingofmodelB,theobtainedAERof1,000uniformlysampledtestdata(i.e.,0.66%)ishigherthantheoneobtainedfromtheuniformlysampled5,000trainingdata(i.e.,0.49%).ThisismoreobviousonmodelC.Whenthesamesamplingmethodisused,theAERobtainedfromthesampled1000testdataisallhigherthanthesampled5,000trainingdata.Thesameconclusioncouldalsobereachedbyobservingthemutationscore(seeFigure6(a)and(b)).ThemutationscoresonmodelAandBarethecaseswherealargerdatasizeobtainsahighermutationscore,whereastheresultonmodelCshowstheoppositecase.Whenperformedonthesamesetofdata,thesource-levelmutationtestingandmodel-levelmutationtestingshowsomedifferentbehaviors.Notethat,onsource-level,weconﬁguretomutate1%ofthetrainingdata;onthemodel-level,weusethesameratio(i.e.,1%)forweightandneuronlevelmutators.Overall,thegeneratedmutantmodelsbysource-levelandTABLEVI:Themodel-levelMTscoreandaverageerrorrateoftestdatabyclass.Accordingtoourmutationscoredeﬁnition,themaximalpossiblemutationscoreforasingleclassis10%.M.Eval.ClassiﬁcationClass(%)0123456789Amu.sc.7.228.759.036.258.758.198.759.179.729.03avg.err.3.413.501.811.484.822.525.504.2510.453.11Bmu.sc.1.593.298.297.445.494.028.173.665.858.41avg.err.0.411.421.121.551.072.922.951.211.242.11Cmu.sc.8.337.958.979.749.749.629.628.979.747.56avg.err.3.676.2214.808.849.1111.536.8311.488.878.55model-levelmutationtestingbehavedifferently.Forexample,comparingthesamedatapairsettingofFigures6(a)and7(a),thesource-levelmutationtestingobtainslowermutationscoreonmodelA,butobtainshighermutationscoreonmodelB.Thismeansthatthesame1%mutationratioresultsindifferentDLmutantmodeleffectsbysource-levelandmodel-levelmutationtestingprocedure.Forﬂexibility,inourtool,weprovidetheconﬁgurableoptionforbothsource-levelandmodel-levelmutantgeneration.InbothFigure6and7,weobservethatthemutationscoresarestilllowformanycases.Itindicatesthatcorrespondingevaluatedtestsarelow-quality,whichisunderstandableinhigh-dimensionalspace.ThefactthatDLcouldbeeasilyattackedbymanyexistingadversarialtechniquesdespitehighperformanceontestdataalsoconﬁrmsourﬁndings[45].D.MutationTestingofOriginalTestDatabyClassGivenaDLclassiﬁcationtask,thedevelopersoftenpreparethetestdatawithgreatcare.Ononehand,theytrytocollectdatafromdiverseclassesthatcovermoreuse-casescenarios.Ontheotherhand,theyalsotrytoobtainmoresensitivedataforeachclasstofacilitatethedetectionofDNNrobustnessissues.ThesametestdatasetmightshowdifferenttestingperformanceondifferentDLmodels;thedatafromdifferentclassesofthesametestdatamightcontributedifferentlytotestingperformanceaswell.Inthissection,weinvestigatehoweachclassoftheoriginaltestdatasetbehavesfromthemutationtestingperspective.1)TestDataandMutantModels:Similartotheexperimen-talprocedureinSectionV-B,weﬁrstpreparethetestdataofeachclassformutationtesting.FortheaccompaniedoriginaltestdatainMNIST(CIFAR-10),weseparatethemintothecorresponding10testdatasetbyclass(i.e.,t1,t2,...,t10).Foreachclassofthetestdatati,wefollowthesamemutationtestingproceduretoperformdataﬁlteringprocedureonmodelA,BandC,respectively.Intheend,weobtain30testdatasets,including10datasetsbyclass(i.e.,t01,t02,...,t010)foreachstudiedDLmodel.Wereusethegeneratedmodel-levelDLmutantmodelsofSectionV-Bandperformmutationtestingontheprepareddataset.2)MutationTestingResultsofTestDatabyClass:TableVIsummarizestheobtainedmutationscoreandAERforeachmodel.Wecanseethat,ingeneral,thetestdataofdifferentclassesobtaindifferentmutationscoresandAER.ConsidertheresultsofmodelAasanexample,thetestdataofclass3obtainsthelowestmutationscoreandAER(i.e.,6.25%and1.48%).Itindicatesthat,comparedwiththetestdataofotherclasses,thetestdataofclass3couldstillbefurtherenhanced.Inaddition,thisexperimentdemonstratesthatahigherAERdoesnotnecessarilyresultinahighermutationscore.FormodelA,theAERobtainedbyclass1islargerthanclass2whilethemutationscoreofclass1issmaller.Remark.Ourmutationtestingtechniqueenablesthequantitativeanalysisontestdataqualityofeachclass.Italsohelpstolocalizetheweaknessintestdata.Basedonthemutationtestingfeedback,DLdeveloperscouldprioritizetoaugmentandenhancetheweaktestdatatocovermoredefect-sensitivecases.E.ThreatsToValidityTheselectionofthesubjectdatasetsandDLmodelscouldbeathreattovalidity.Inthispaper,wetrytocounterthisissuebyusingtwowidelystudieddatasets(i.e.,MNISTandCIFAR-10),andDLmodelswithdifferentnetworkstructures,com-plexities,andhavecompetitivepredictionaccuracy.Anotherthreattovaliditycouldbetherandomnessintheprocedureoftrainingsource-levelDLmutantmodels.TheTensorFlowframeworkbydefaultusesmultiplethreadsfortrainingpro-cedure,whichcancausethesametrainingdatasettogeneratedifferentDLmodels.Tocountersucheffects,wetriedourbesttoruleoutnon-deterministicfactorsintrainingprocess.Weﬁrstﬁxalltherandomseedsfortrainingprograms,anduseasinglethreadfortrainingbysettingTensorﬂowparameters.SuchasettingenablesthetrainingprogressdeterministicwhenrunningonCPU,whichstillhasnon-deterministicbehaviorwhenrunningonGPU.Therefore,forthecontrolledevaluationdescribedinthispaper,weperformedthesource-levelDLmutantmodeltrainingbyCPUtoreducethethreatcausedbyrandomnessfactorintrainingprocedure.Anotherthreatistherandomnessduringdatasampling.Tocounterthis,werepeatthesamplingprocedureﬁvetimesandaveragetheresults.VI.RELATEDWORKA.MutationTestingofTraditionalSoftwareThehistoryofmutationtestingdatedbackto1971inRichardLiption’sPaper[16],andtheﬁeldstartedtogrowwithDeMilloetal.[46]andHamlet[47]pioneeringworksinlate1970s.Afterwards,mutationtestinghasbeenextensivelystudiedfortraditionalsoftware,whichhasbeenprovedtobeausefulmethodologytoevaluatetheeffectivenessoftestdata.Asakeycomponentinmutationtestingprocedure,muta-tionoperatorsarewidelystudiedanddesignedfordifferentprogramminglanguages.Buddetal.wastheﬁrsttodesignmutationoperatorsforFortran[48],[49].Arawaletal.laterproposedasetof77mutationoperatorsforANSIC[50].Duetothefastdevelopmentofprogramminglanguagesthatincorporatesmanyfeatures(e.g.,ObjectOriented,Aspect-Oriented),mutationoperatorsarefurtherextendedtocovermoreadvancedfeaturesinpopularprogramminglanguages,likeJava[51],[52],C#[53],[54],SQL[55],andAspectJ[56].Differentfromtraditionalsoftware,DLdeﬁnesanoveldata-drivenprogrammingparadigmwithdifferentsoftwarerepre-sentations,causingthemutationoperatorsdeﬁnedfortradi-tionalsoftwareunabletobedirectlyappliedtoDLbasedsoftware.Tothebestofourknowledge,DeepMutationistheﬁrsttoproposemutationtestingframeworksforDLsystems,withthedesignofbothsource-levelandmodel-levelmutators.Besidesthedesignofmutationoperators,greateffortshavealsobeendevotedtootherkeyissuesofmutationtesting,suchastheoreticalaspects[57]–[59]ofmutationtesting,performanceenhancement[7],[16],[60]–[62],platformandtoolsupport[30],[63]–[65],aswellasmoregeneralmuta-tiontestingapplicationsfortestgeneration[7],[21],[66],networks[67],[68].Wereferinterestingreaderstoarecentcomprehensivesurveyonmutationtesting[6].B.TestingandVeriﬁcationofDLSystemsTesting.Testingmachinelearningsystemsmainlyreliesonprobingtheaccuracyontestdatawhicharerandomlydrawnfrommanuallylabeleddatasetsandadhocsimulations[69].DeepXplore[12]proposesawhite-boxdifferentialtestingalgorithmtosystematicallygenerateadversarialexamplesthatcoverallneuronsinthenetwork.Byintroducingthedeﬁnitionofneuroncoverage,theymeasurehowmuchoftheinternallogicofaDNNhasbeentested.DeepCover[14]proposesthetestcriteriaforDNNs,adaptedfromtheMC/DCtestcriteria[70]oftraditionalsoftware.Theirtestcriteriahaveonlybeenevaluatedonsmallscaleneuralnetworks(withonlyDenselayers,andatmost5hiddenlayers,andnomorethan400neurons).Theeffectivenessoftheirtestcriteriaremainunknownonreal-world-sizedDLsystemswithmultipletypesoflayers.DeepGauge[13]proposesmulti-granularitytestingcoverageforDLsystems,whichisbasedontheobservationofDNNs’internalstate.Theirtestingcriteriashowstobeapromisingasaguidanceforeffectivetestgeneration,whichisalsoscalabletocomplexDNNslikeResNet-50(withhundredsoflayersandapproximately100,000neurons).ConsideringthehighthedimensionandlargepotentialtestingspaceofaDNN,DeepCT[71]proposesasetofcombinatorialtestingcriteriabasedontheneuroninputinteractionforeachlayerofDNNs,towardsbalancingthedefectdetectionabilityandareasonablenumberoftests.Veriﬁcation.Anotherinterestingavenueistoprovidereliableguaranteesonthesecurityofdeeplearningsystemsbyformalveriﬁcation.Theabstraction-reﬁnementapproachin[72]veri-ﬁessafetypropertiesofaneuralnetworkwith6neurons.DLV[73]enablestoverifylocalrobustnessofdeepneuralnetworks.Reluplex[74]adoptsanSMT-basedapproachthatveriﬁessafetyandrobustnessofdeepneuralnetworkswithReLUactivationfunctions.Reluplexhasdemonstrateditsusefulnessonanetworkwith300ReLUnodesin[74].DeepSafe[75]usesReluplexasitsunderlyingveriﬁcationcomponenttoidentifysaferegionsintheinputspace.AI2[76]proposestheveriﬁcationofDLsystemsbasedonabstractinterpretation,anddesignsthespeciﬁcabstractdomainsandtransformationoperators.VERIVIS[77]isabletoverifysafetypropertiesofdeepneuralnetworkswheninputsaremodiﬁedthroughgiventransformationfunctions.Butthetransformationfunctionsin[77]arestillsimplerthanpotentialreal-worldtransformations.Theexistingworkofformalveriﬁcationshowsthatfor-maltechniqueforDNNsispromising[72]–[77].However,mostveriﬁcationtechniquesweredemonstratedonlyonsim-pleDNNsnetworkarchitectures.Designingmorescalableandgeneralveriﬁcationmethodstowardscomplexreal-worldDNNswouldbeimportantresearchdirections.DeepMutationoriginallyproposestousemutationtestingtosystematicallyevaluatethetestdataqualityofDNNs,whichismostlyorthogonaltotheseexistingtestingandveriﬁcationtechniques.VII.CONCLUSIONANDFUTUREWORKInthispaper,wehavestudiedtheusefulnessofmutationtestingtechniquesforDLsystems.Weﬁrstproposedasource-levelmutationtestingtechniquethatworksontrainingdataandtrainingprograms.Wethendesignedasetofsource-levelmutationoperatorstoinjectfaultsthatcouldbepotentiallyintroducedduringtheDLdevelopmentprocess.Inaddition,wealsoproposedamodel-levelmutationtestingtechniqueandde-signedasetofmutationoperatorsthatdirectlyinjectfaultsintoDLmodels.Furthermore,weproposedthemutationtestingmetricstomeasurethequalityoftestdata.WeimplementedtheproposedmutationtestingframeworkDeepMutationanddemonstrateditsusefulnessontwopopulardatasets,MNISTandCIFAR-10,withthreeDLmodels.Mutationtestingisawell-establishedtechniqueforthetestdataqualityevaluationintraditionalsoftwareandhasalsobeenwidelyappliedtomanyapplicationdomains.WebelievethatmutationtestingisapromisingtechniquethatcouldfacilitateDLdeveloperstogeneratehigherqualitytestdata.Thehigh-qualitytestdatawouldprovidemorecomprehensivefeedbackandguidanceforfurtherin-depthunderstandingandconstructingDLsystems.Thispaperperformsaninitialexploratoryattempttodemonstratetheusefulnessofmutationtestingfordeeplearningsystems.Infuturework,wewillperformamorecomprehensivestudytoproposeadvancedmutationoperatorstocovermorediverseaspectsofDLsystemsandinvestigatetherelationsofthemutationoperators,aswellashowwellsuchmutationoperatorsintroducefaultscomparabletohumanfaults.Furthermore,wewillalsoinves-tigatenovelmutationtestingguidedautomatedtesting,attackanddefense,aswellasrepairtechniquesforDLsystems.ACKNOWLEDGEMENTSThisworkwaspartiallysupportedbyNationalKeyR&DProgramofChina2017YFC1201200and2017YFC0907500,FundamentalResearchFundsforCentralUniversitiesofChinaAUGA5710000816,JSPSKAKENHIGrant18H04097.WegratefullyacknowledgethesupportofNVIDIAAITechCenter(NVAITC)toourresearch.WealsoappreciateChengZhang,JieZhang,andtheanonymousreviewersfortheirinsightfulandconstructivecomments.REFERENCES[1]P.Holley,“Texasbecomesthelateststatetogetaself-drivingcarservice,”https://www.washingtonpost.com/news/innovations/wp/2018/05/07/texas-becomes-the-latest-state-to-get-a-self-driving-car-service/?noredirect=on&utmterm=.924daa775616,2018.[2]F.Zhang,J.Leitner,M.Milford,B.Upcroft,andP.Corke,“Towardsvision-baseddeepreinforcementlearningforroboticmotioncontrol,”arXiv:1511.03791,2015.[3]D.Silver,A.Huang,C.J.Maddison,A.Guez,L.Sifre,G.VanDenDriessche,J.Schrittwieser,I.Antonoglou,V.Panneershelvam,M.Lanctotetal.,“Masteringthegameofgowithdeepneuralnetworksandtreesearch,”nature,vol.529,no.7587,pp.484–489,2016.[4]A.Karpathy,G.Toderici,S.Shetty,T.Leung,R.Sukthankar,andL.Fei-Fei,“Large-scalevideoclassiﬁcationwithconvolutionalneuralnetworks,”inProceedingsoftheIEEEconferenceonComputerVisionandPatternRecognition,2014,pp.1725–1732.[5]I.J.Goodfellow,J.Shlens,andC.Szegedy,“Explainingandharnessingadversarialexamples,”ICLR,2015.[6]Y.JiaandM.Harman,“Ananalysisandsurveyofthedevelopmentofmutationtesting,”IEEETrans.Softw.Eng.,vol.37,no.5,pp.649–678,Sep.2011.[7]R.A.DeMilloandA.J.Offutt,“Constraint-basedautomatictestdatageneration,”IEEETransactionsonSoftwareEngineering,vol.17,no.9,pp.900–910,Sep.1991.[8]M.Abadi,P.Barham,J.Chen,Z.Chen,A.Davis,J.Dean,M.Devin,S.Ghemawat,G.Irving,M.Isard,M.Kudlur,J.Levenberg,R.Monga,S.Moore,D.G.Murray,B.Steiner,P.Tucker,V.Vasudevan,P.Warden,M.Wicke,Y.Yu,andX.Zheng,“Tensorﬂow:Asystemforlarge-scalemachinelearning,”in12thUSENIXSymposiumonOperatingSystemsDesignandImplementation(OSDI16),2016,pp.265–283.[9]F.Cholletetal.,“Keras,”https://github.com/fchollet/keras,2015.[10]A.Gibson,C.Nicholson,J.Patterson,M.Warrick,A.D.Black,V.Kokorin,S.Audet,andS.Eraly,“Deeplearning4j:Distributed,open-sourcedeeplearningforJavaandScalaonHadoopandSpark,”2016.[11]J.Dean,G.Corrado,R.Monga,K.Chen,M.Devin,M.Mao,A.Senior,P.Tucker,K.Yang,Q.V.Leetal.,“Largescaledistributeddeepnetworks,”inAdvancesinNeuralInformationProcessingsystems,2012,pp.1223–1231.[12]K.Pei,Y.Cao,J.Yang,andS.Jana,“Deepxplore:Automatedwhiteboxtestingofdeeplearningsystems,”inProceedingsofthe26thSymposiumonOperatingSystemsPrinciples,2017,pp.1–18.[13]L.Ma,F.Juefei-Xu,J.Sun,C.Chen,T.Su,F.Zhang,M.Xue,B.Li,L.Li,Y.Liuetal.,“Deepgauge:Multi-granularitytestingcriteriafordeeplearningsystems,”The33rdIEEE/ACMInternationalConferenceonAutomatedSoftwareEngineering(ASE’18),2018.[14]Y.Sun,X.Huang,andD.Kroening,“TestingDeepNeuralNetworks,”ArXive-prints,Mar.2018.[15]I.Goodfellow,Y.Bengio,andA.Courville,DeepLearning.MITPress,2016,http://www.deeplearningbook.org.[16]A.J.OffuttandR.H.Untch,Mutation2000:UnitingtheOrthogonal.Boston,MA:SpringerUS,2001,pp.34–44.[17]K.N.KingandA.J.Offutt,“Afortranlanguagesystemformutation-basedsoftwaretesting,”Softw.Pract.Exper.,vol.21,no.7,pp.685–718,Jun.1991.[18]Y.-S.Ma,J.Offutt,andY.R.Kwon,“Mujava:Anautomatedclassmutationsystem:Researcharticles,”Softw.Test.Verif.Reliab.,vol.15,no.2,pp.97–133,Jun.2005.[19]J.Offutt,P.Ammann,andL.L.Liu,“Mutationtestingimplementsgrammar-basedtesting,”inProceedingsoftheSecondWorkshoponMutationAnalysis(MUTATION’06),2006.[20]R.Just,D.Jalali,andM.D.Ernst,“Defects4j:Adatabaseofexistingfaultstoenablecontrolledtestingstudiesforjavaprograms,”inPro-ceedingsofthe2014InternationalSymposiumonSoftwareTestingandAnalysis(ISSTA’14).ACM,2014,pp.437–440.[21]G.FraserandA.Arcuri,“Wholetestsuitegeneration,”IEEETrans.Softw.Eng.,vol.39,no.2,pp.276–291,Feb.2013.[22]L.Ma,C.Artho,C.Zhang,H.Sato,J.Gmeiner,andR.Ramler,“Grt:Program-analysis-guidedrandomtesting(t),”inProceedingsofthe30thIEEE/ACMInternationalConferenceonAutomatedSoftwareEngineering(ASE’15),Washington,DC,USA,2015,pp.212–223.[23]Y.LeCun,L.Bottou,Y.Bengio,andP.Haffner,“Gradient-basedlearningappliedtodocumentrecognition,”Proc.oftheIEEE,vol.86,no.11,pp.2278–2324,1998.[24]A.KrizhevskyandG.Hinton,“Learningmultiplelayersoffeaturesfromtinyimages,”Master’sthesis,DepartmentofComputerScience,UniversityofToronto,2009.[25]O.Russakovsky,J.Deng,H.Su,J.Krause,S.Satheesh,S.Ma,Z.Huang,A.Karpathy,A.Khosla,M.Bernstein,A.C.Berg,andL.Fei-Fei,“ImageNetLargeScaleVisualRecognitionChallenge,”IJCV,vol.115,no.3,pp.211–252,2015.[26]“CosmicRay:mutationtestingforPython,”https://github.com/sixty-north/cosmic-ray/.[27]“MutPyisamutationtestingtoolforPython,”https://github.com/mutpy/mutpy/.[28]A.DerezinskaandK.Hałas,“Improvingmutationtestingprocessofpythonprograms,”inSoftwareEngineeringinIntelligentSystems,R.Silhavy,R.Senkerik,Z.K.Oplatkova,Z.Prokopova,andP.Silhavy,Eds.,2015,pp.233–242.[29]R.Just,“TheMajormutationframework:EfﬁcientandscalablemutationanalysisforJava,”inProceedingsoftheInternationalSymposiumonSoftwareTestingandAnalysis(ISSTA),SanJose,CA,USA,July23–252014,pp.433–436.[30]H.Coles,T.Laurent,C.Henard,M.Papadakis,andA.Ventresque,“Pit:Apracticalmutationtestingtoolforjava(demo),”inProceedingsofthe25thInternationalSymposiumonSoftwareTestingandAnalysis,ser.ISSTA2016.NewYork,NY,USA:ACM,2016,pp.449–452.[31]C.Francois,DeepLearningwithPython.ManningPublicationsCompany,2017.[32]Y.Tian,K.Pei,S.Jana,andB.Ray,“Deeptest:Automatedtest-ingofdeep-neural-network-drivenautonomouscars,”arXivpreprintarXiv:1708.08559,2017.[33]M.Becker,C.Kuznik,M.M.Joy,T.Xie,andW.Mueller,“Binarymu-tationtestingthroughdynamictranslation,”inIEEE/IFIPInternationalConferenceonDependableSystemsandNetworks(DSN2012),June2012,pp.1–12.[34]M.Becker,D.Baldin,C.Kuznik,M.M.Joy,T.Xie,andW.Mueller,“Xemu:Anefﬁcientqemubasedbinarymutationtestingframeworkforembeddedsoftware,”inProceedingsoftheTenthACMInternationalConferenceonEmbeddedSoftware,ser.EMSOFT’12.NewYork,NY,USA:ACM,2012,pp.33–42.[35]V.NairandG.E.Hinton,“Rectiﬁedlinearunitsimproverestrictedboltzmannmachines,”inProceedingsofthe27thInternationalConfer-enceonInternationalConferenceonMachineLearning,ser.ICML’10.USA:Omnipress,2010,pp.807–814.[36]A.L.Maas,A.Y.Hannun,andA.Y.Ng,“Rectiﬁernonlinearitiesimproveneuralnetworkacousticmodels,”ininICMLWorkshoponDeepLearningforAudio,SpeechandLanguageProcessing,2013.[37]L.Chu,X.Hu,J.Hu,L.Wang,andJ.Pei,“Exactandconsistentinterpretationforpiecewiselinearneuralnetworks:Aclosedformsolution,”inProceedingsofthe24thACMSIGKDDInternationalConferenceonKnowledgeDiscovery&DataMining,ser.KDD’18.NewYork,NY,USA:ACM,2018,pp.1244–1253.[38]C.Xiao,B.Li,J.-Y.Zhu,W.He,M.Liu,andD.Song,“GeneratingAdversarialExampleswithAdversarialNetworks,”ArXiv,Jan.2018.[39]N.CarliniandD.Wagner,“Towardsevaluatingtherobustnessofneuralnetworks,”inIEEESymposiumonSecurityandPrivacy(SP),2017,pp.39–57.[40]Y.LeCunandC.Cortes,“TheMNISTdatabaseofhandwrittendigits,”1998.[41]N.Krizhevsky,H.Vinod,C.Geoffrey,M.Papadakis,andA.Ventresque,“Thecifar-10dataset,”http://www.cs.toronto.edu/kriz/cifar.html,2014.[42]B.D.RipleyandN.L.Hjort,PatternRecognitionandNeuralNetworks,1sted.NewYork,NY,USA:CambridgeUniversityPress,1995.[43]C.M.Bishop,NeuralNetworksforPatternRecognition.NewYork,NY,USA:OxfordUniversityPress,Inc.,1995.[44]S.Thrun,“Towardroboticcars,”Commun.ACM,vol.53,no.4,pp.99–106,Apr.2010.[45]N.CarliniandD.Wagner,“Adversarialexamplesarenoteasilydetected:Bypassingtendetectionmethods,”inProceedingsofthe10thACMWorkshoponArtiﬁcialIntelligenceandSecurity(AISec’17),2017,pp.3–14.[46]R.A.DeMillo,R.J.Lipton,andF.G.Sayward,“Hintsontestdataselection:Helpforthepracticingprogrammer,”Computer,vol.11,no.4,pp.34–41,Apr.1978.[47]R.Hamlet,“Testingprogramswiththeaidofacompiler,”IEEETransactionsonSoftwareEngineering,vol.3,pp.279–290,071977.[48]T.A.BuddandF.G.Sayward,“Usersguidetothepilotmutationsystem,”YaleUniversity,NewHaven,Connecticut,techreport114,1977.[49]T.A.Budd,R.A.DeMillo,R.J.Lipton,andF.G.Sayward,“Thedesignofaprototypemutationsystemforprogramtesting,”inProceedingsoftheAFIPSNationalComputerConference,Anaheim,NewJersey,5-8June1978,pp.623–627.[50]H.Agrawal,R.A.DeMillo,B.Hathaway,W.Hsu,W.Hsu,E.W.Krauser,R.J.Martin,A.P.Mathur,andE.Spafford,“Designofmutantoperatorsforthecprogramminglanguage,”PurdueUniversity,WestLafayette,Indiana,techreportSERC-TR-41-P,Mar.1989.[51]S.Kim,J.A.Clark,andJ.A.McDermid,“Investigatingtheeffectivenessofobject-orientedtestingstrategiesusingthemutationmethod,”inProceedingsofthe1stWorkshoponMutationAnalysis(MUTATION’00),SanJose,California,6-7October2001,pp.207–225.[52]Y.-S.Ma,A.J.Offutt,andY.-R.Kwon,“Mujava:Anautomatedclassmutationsystem,”SoftwareTesting,Veriﬁcation&Reliability,vol.15,no.2,pp.97–133,June2005.[53]A.Derezi´nska,“Advancedmutationoperatorsapplicableinc#pro-grams,”WarsawUniversityofTechnology,Warszawa,Poland,techre-port,2005.[54]——,“Qualityassessmentofmutationoperatorsdedicatedforc#pro-grams,”inProceedingsofthe6thInternationalConferenceonQualitySoftware(QSIC’06),Beijing,China,27-28October2006.[55]W.K.Chan,S.C.Cheung,andT.H.Tse,“Fault-basedtestingofdatabaseapplicationprogramswithconceptualdatamodel,”inProceed-ingsofthe5thInternationalConferenceonQualitySoftware(QSIC’05),Melbourne,Australia,Sep.2005,pp.187–196.[56]F.C.Ferrari,J.C.Maldonado,andA.Rashid,“Mutationtestingforaspect-orientedprograms,”inProceedingsofthe1stInternationalConferenceonSoftwareTesting,Veriﬁcation,andValidation(ICST’08),Lillehammer,Norway,9-11April2008,pp.52–61.[57]R.A.DeMillo,D.S.Guindi,K.N.King,W.M.McCracken,andA.J.Offutt,“Anextendedoverviewofthemothrasoftwaretestingenvironment,”inProceedingsofthe2ndWorkshoponSoftwareTesting,Veriﬁcation,andAnalysis(TVA’88),BanffAlberta,Canada,July1988,pp.142–151.[58]A.J.Offutt,“Thecouplingeffect:Factorﬁction,”ACMSIGSOFTSoftwareEngineeringNotes,vol.14,no.8,pp.131–140,December1989.[59]——,“Investigationsofthesoftwaretestingcouplingeffect,”ACMTransactionsonSoftwareEngineeringandMethodology,vol.1,no.1,pp.5–20,January1992.[60]T.A.Budd,“Mutationanalysisofprogramtestdata,”phdthesis,YaleUniversity,NewHaven,Connecticut,1980.[61]B.J.M.Gr¨un,D.Schuler,andA.Zeller,“Theimpactofequivalentmutants,”inProceedingsofthe4thInternationalWorkshoponMutationAnalysis(MUTATION’09),Denver,Colorado,1-4April2009,pp.192–199.[62]Y.JiaandM.Harman,“Constructingsubtlefaultsusinghigherordermutationtesting,”inProceedingsofthe8thInternationalWorkingConferenceonSourceCodeAnalysisandManipulation(SCAM’08),Beijing,China,Sep.2008,pp.249–258.[63]E.W.Krauser,A.P.Mathur,andV.J.Rego,“Highperformancesoftwaretestingonsimdmachines,”IEEETransactionsonSoftwareEngineering,vol.17,no.5,pp.403–423,May1991.[64]A.P.MathurandE.W.Krauser,“Mutantuniﬁcationforimprovedvectorization,”PurdueUniversity,WestLafayette,Indiana,techreportSERC-TR-14-P,1988.[65]A.J.Offutt,R.P.Pargas,S.V.Fichter,andP.K.Khambekar,“Mu-tationtestingofsoftwareusingamimdcomputer,”inProceedingsoftheInternationalConferenceonParallelProcessing,Chicago,Illinois,August1992,pp.255–266.[66]B.Baudry,F.Fleurey,J.-M.Jezequel,andY.L.Traon,“Genesandbacteriaforautomatictestcasesoptimizationinthe.netenvironment,”inProceedingsofthe13thInternationalSymposiumonSoftwareReliabilityEngineering(ISSRE’02),Annapolis,Maryland,12-15November2002,pp.195–206.[67]D.P.SidhuandT.K.Leung,“Faultcoverageofprotocoltestmethods,”inProceedingsofthe7thAnnualJointConferenceoftheIEEEComputerandCommuncationsSocieties(INFOCOM’88),NewOrleans,Louisiana,Mar.1988,pp.80–85.[68]C.Jing,Z.Wang,X.Shi,X.Yin,andJ.Wu,“Mutationtestingofprotocolmessagesbasedonextendedttcn-3,”inProceedingsofthe22ndInternationalConferenceonAdvancedInformationNetworkingandApplications(AINA’08),Okinawa,Japan,Mar.2008,pp.667–674.[69]I.H.Witten,E.Frank,M.A.Hall,andC.J.Pal,DataMining:Practicalmachinelearningtoolsandtechniques.MorganKaufmann,2016.[70]H.KellyJ.,V.DanS.,C.JohnJ.,andR.LeannaK.,“Apracticaltutorialonmodiﬁedcondition/decisioncoverage,”NASA,Tech.Rep.,2001.[71]L.Ma,F.Zhang,M.Xue,B.Li,Y.Liu,J.Zhao,andY.Wang,“Combinatorialtestingfordeeplearningsystems,”arXivpreprintarXiv:1806.07723,2018.[72]L.PulinaandA.Tacchella,“Anabstraction-reﬁnementapproachtoveriﬁcationofartiﬁcialneuralnetworks,”inInternationalConferenceonComputerAidedVeriﬁcation.Springer,2010,pp.243–257.[73]M.Wicker,X.Huang,andM.Kwiatkowska,“Feature-guidedblack-boxsafetytestingofdeepneuralnetworks,”CoRR,vol.abs/1710.07859,2017.[74]G.Katz,C.W.Barrett,D.L.Dill,K.Julian,andM.J.Kochenderfer,“Reluplex:AnefﬁcientSMTsolverforverifyingdeepneuralnetworks,”CoRR,vol.abs/1702.01135,2017.[75]D.Gopinath,G.Katz,C.S.Pasareanu,andC.Barrett,“Deepsafe:Adata-drivenapproachforcheckingadversarialrobustnessinneuralnetworks,”CoRR,vol.abs/1710.00486,2017.[76]D.D.-C.P.T.S.C.M.V.TimonGehr,MatthewMirman,“Ai2:Safetyandrobustnesscertiﬁcationofneuralnetworkswithabstractinterpretation,”in2018IEEESymposiumonSecurityandPrivacy(SP),2018.[77]K.Pei,Y.Cao,J.Yang,andS.Jana,“Towardspracticalveriﬁcationofmachinelearning:Thecaseofcomputervisionsystems,”CoRR,vol.abs/1712.01785,2017.