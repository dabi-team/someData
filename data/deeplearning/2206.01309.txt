IEEE TRANSACTIONS ON MEDICAL IMAGING, VOL. XX, NO. XX, XXXX 2020

1

H-EMD: A Hierarchical Earth Mover‚Äôs Distance
Method for Instance Segmentation

Peixian Liang, Yizhe Zhang, Yifan Ding, Jianxu Chen, Chinedu S. Madukoma, Tim Weninger,
Joshua D. Shrout, and Danny Z. Chen‚àó

2
2
0
2

n
u
J

2

]

V
C
.
s
c
[

1
v
9
0
3
1
0
.
6
0
2
2
:
v
i
X
r
a

Abstract‚Äî Deep learning (DL) based semantic segmen-
tation methods have achieved excellent performance in
biomedical
image segmentation, producing high quality
probability maps to allow extraction of rich instance in-
formation to facilitate good instance segmentation. While
numerous efforts were put into developing new DL seman-
tic segmentation models, less attention was paid to a key
issue of how to effectively explore their probability maps
to attain the best possible instance segmentation. We ob-
serve that probability maps by DL semantic segmentation
models can be used to generate many possible instance
candidates, and accurate instance segmentation can be
achieved by selecting from them a set of ‚Äúoptimized‚Äù candi-
dates as output instances. Further, the generated instance
candidates form a well-behaved hierarchical structure (a
forest), which allows selecting instances in an optimized
manner. Hence, we propose a novel framework, called
hierarchical earth mover‚Äôs distance (H-EMD), for instance
segmentation in biomedical 2D+time videos and 3D im-
ages, which judiciously incorporates consistent instance
selection with semantic-segmentation-generated probabil-
ity maps. H-EMD contains two main stages. (1) Instance
candidate generation: capturing instance-structured infor-
mation in probability maps by generating many instance
candidates in a forest structure. (2) Instance candidate
selection: selecting instances from the candidate set for
Ô¨Ånal instance segmentation. We formulate a key instance
selection problem on the instance candidate forest as an
optimization problem based on the earth mover‚Äôs distance
(EMD), and solve it by integer linear programming. Exten-
sive experiments on eight biomedical video or 3D datasets
demonstrate that H-EMD consistently boosts DL semantic
segmentation models and is highly competitive with state-
of-the-art methods.

This research was supported in part by NSF Grant CCF-1617735
and US Army Research OfÔ¨Åce Grant W911NF-17-1-0448. Chinedu
Madukoma was supported in part by a fellowship from the Notre Dame
Eck Institute for Global Health. Manuscript received April 2021. Revised
February 2022 and April 2022. Accepted April 2022. The Ô¨Årst two
authors are co-Ô¨Årst authors. Asterisk indicates the corresponding author.
‚àíPeixian Liang, Yifan Ding, Tim Weninger, and Danny Z. Chen are
with the Department of Computer Science and Engineering, University
of Notre Dame, Notre Dame, IN 46556, USA (e-mail: {pliang, yding4,
tweninge, dchen}@nd.edu).

‚àíChinedu Madukoma and Joshua Shrout are with the Department
of Civil and Environmental Engineering and Earth Sciences, University
of Notre Dame, Notre Dame, IN 46556, USA (e-mail: {cmadukom,
joshua.shrout}@nd.edu).

‚àíYizhe Zhang is with the School of Computer Science and Engi-
neering, Nanjing University of Science and Technology, Nanjing 210094,
China, (e-mail: {zhangyizhe}@njust.edu.cn).

‚àíJianxu Chen is with the Leibniz-Institut

44139, German

f ¬®ur Analytische
(e-mail:

Wissenschaften‚ÄìISAS‚Äìe.V., Dortmund
{jianxu.chen}@isas.de).

Index Terms‚Äî instance segmentation, earth mover‚Äôs dis-

tance, integer linear programming, videos, 3D images

I. INTRODUCTION

I NSTANCE segmentation lays a foundation in biomedical

image analysis such as cell migration study [1] and cell
nuclei detection [2]. It aims to not only group together pixels in
different semantic categories to form object instances, but also
distinguish individual objects of the same category. Instance
segmentation is generally quite challenging because (1) objects
of the same class can get crowded tightly together and have
obscure boundaries, (2) small
local pixel-level errors can
disturb instance-level correctness in the neighborhood, and (3)
the number of instances in an input image is unknown during
prediction.

Deep learning (DL) based semantic segmentation methods
are able to obtain effective object segmentation in various
their perfor-
image datasets [3]‚Äì[6]. However,
biomedical
mances on instance segmentation may deteriorate considerably
because they mainly produce probability maps to indicate
pixels for various possible semantic classes but focus less
on how to effectively group pixels together to form object
instances. Many recent biomedical image instance segmenta-
tion methods sought to incorporate instance-level features into
semantic segmentation architectures. There are two main types
of such strategies. The strategies of the Ô¨Årst type encode pixel-
wise correlations across instances [7]‚Äì[10]. With similarity
or distance loss functions, pixel embeddings from the same
instance are pushed together while pixel embeddings from
different instances are pushed further away. The strategies
of the second type aim to utilize instance morphological
properties to distinguish different instances [11]‚Äì[13]. Some of
the best known methods are watershed-based [14], directly ap-
plying watershed as a post-processing [11], [15] or watershed-
inspired deep learning methods such as distance maps [16]‚Äì
[18] and gradient maps [12], which show great performances
on a wide range of biomedical image datasets.

Although much effort has been dedicated to developing
instance segmentation methods for biomedical images, it is
still quite challenging to distinguish instances in hard cases
(e.g., densely packed cells). In temporal instance segmentation
settings (e.g., videos), previous studies [19]‚Äì[21] exhibited
the potential of exploiting temporal consistency of objects
to alleviate the severity of this challenge. But, exploiting

 
 
 
 
 
 
2

IEEE TRANSACTIONS ON MEDICAL IMAGING, VOL. XX, NO. XX, XXXX 2020

temporal consistency does not always show advantages com-
pared to general deep learning segmentation methods, as
suggested in the public leader board of the temporal cell
segmentation challenge benchmark [22]. Early work applied
matching between segmentation masks based on hierarchical
structures [21] or segmentation sets [23]. Instance candidates
were generated mostly using heuristic rules [24], [25] or
handcrafted features [21], [23], and thus they could not
directly beneÔ¨Åt from DL semantic segmentation pipelines.
Recent temporal approaches considered pair-wise correlation
of the same instances across multiple image frames utilizing
context-aware methods such as GRU [10] and LSTM [19].
But,
temporal correlation is preserved only on individual
pixels with pixel-pixel correlation, while clustering/grouping
different pixels is still required in order to obtain segmenta-
tion masks. Consequently, instance-level temporal consistency
between segmentation masks across different image frames is
not guaranteed to preserve.

In this paper, we propose a new framework, called hierar-
chical earth mover‚Äôs distance (H-EMD), which is capable to
simultaneously incorporate semantic segmentation and tem-
poral/spatial consistency among instance segmentation masks
for instance segmentation in biomedical 2D+time videos and
3D images. We observe that probability maps from com-
mon DL semantic segmentation models can allow to directly
generate instance candidates using different threshold values.
These instance candidates carry instance-level features and Ô¨Åt
well with temporal/spatial consistency. To select the correct
instance candidates, we Ô¨Årst distinguish the easy-to-identify
instances which are typically isolated objects with simple
topological structures. From the viewpoint of probability maps,
such instances correspond to connected components with a
single local maximum. Moreover, unselected (hard-to-identify)
instances (e.g., multiple crowded cells) can be extracted by
matching with the already identiÔ¨Åed instances.

two main stages.

SpeciÔ¨Åcally, H-EMD consists of

(1)
Instance candidate generation: Given a probability map P
produced by a DL semantic segmentation model (e.g., a fully
convolutional network (FCN)), we apply different threshold
values of P to generate many sets of instance candidates.
We show that the generated instance candidates form a well-
behaved hierarchical structure, called instance candidate forest
(ICF). (2) Instance candidate selection: Utilizing ICF, we
aim to identify the correct instance candidates. First, easily
segmented instances corresponding to the root nodes of those
special trees in ICF each of which is just a single path (i.e.,
no branches) are identiÔ¨Åed and selected. Then, we develop an
iterative matching and selection algorithm to identify instances
from the unselected nodes in ICF. In each iteration, we Ô¨Årst
conduct matching with the selected instances to reduce re-
dundant matching errors. Through a similar matching scheme,
the selected but unmatched instances are later propagated
to adjacent image frames to select more instances from the
remaining instance candidates. After several iterations, the
remaining unselected instance candidates go through a padding
process. Finally, the padding results combined with all the
selected instance candidates form the Ô¨Ånal output instances.

In the second stage of H-EMD, we formulate a key instance

candidate selection problem from ICF as an optimization
problem based on the earth mover‚Äôs distance (EMD) [26], and
solve it by integer linear programming (ILP).

In summary, our contributions in this work are as follows.
‚Ä¢ We introduce a novel framework, H-EMD, which is
able to utilize both semantic segmentation features and
temporal/spatial consistency among segmentation masks.
‚Ä¢ We present a general method to produce comprehensive
instance candidates directly from probability maps gen-
erated by DL semantic segmentation models.

‚Ä¢ We identify easy instances with high conÔ¨Ådence in
the instance candidates. We further present an iterative
matching and selection method to propagate the selected
instance candidates to match with the unselected hard-to-
identify candidates.

‚Ä¢ We formulate a key instance selection problem from the
candidate forest as an optimization problem based on
the earth mover‚Äôs distance (EMD) [26], and solve it by
integer linear programming (ILP).

‚Ä¢ We conduct experiments on four public and two in-house
temporal instance segmentation datasets (i.e., 2D+time
videos). Built on top of common DL semantic segmen-
tation models, our H-EMD is highly competitive with
state-of-the-art segmentation methods. We also submitted
the H-EMD results to Cell Segmentation Benchmark on
these public datasets. Compared to all the submissions,
H-EMD ranks top-3 in three out of the four datasets.
‚Ä¢ We also conduct experiments on two 3D instance seg-
mentation datasets, and demonstrate that H-EMD is able
to boost performances using spacial consistency on 3D
instance segmentation tasks.

II. RELATED WORK

1) Semantic Segmentation for Instance Segmentation: Deep
learning based semantic segmentation models are one of the
mainstream approaches for instance segmentation in biomedi-
cal images. For input images, an FCN (e.g., U-Net) produces
probability maps that assign a semantic label to each image
pixel. Pixels of the same semantic class in the probability
maps are grouped into connected components as output in-
stances [3], [6], [27], [28]. Semantic segmentation attains
good performance for instance segmentation, and has consid-
erable adaptability in various instance scenarios (e.g., crowded
irregular-shape cells). However, semantic segmentation utilizes
only pixel semantic labels and requires an additional empirical
pixel grouping step (e.g., 0.5-thresholding) in the inference
stage. Semantic segmentation architectures are often not well
suited to directly capturing instance-level features/properties.
Thus, more effective methods are needed for incorporating
instance information to better identify instances.

2) Incorporating Instance Information: Many methods have
been proposed to incorporate instance-level information with
semantic segmentation architectures. There are two main types
of strategies. The strategies of the Ô¨Årst type encode pixel-
wise correlation in an instance fashion to determine indi-
vidual instances, which are also known as pixel embedding
methods [7], [9], [10], [20], [29]. SpeciÔ¨Åcally, pixels for the

AUTHOR et al.: PREPARATION OF PAPERS FOR IEEE TRANSACTIONS ON MEDICAL IMAGING

3

Fig. 1: An overview of our proposed H-EMD instance segmentation framework based on semantic segmentation, which
consists of two main stages. (a) Instance Candidate Generation: Given a probability map produced by a pixel-wise DL
semantic segmentation model, we apply many possible threshold values given by the probability map to generate different
masks. Taking all the connected components resulted from the masks, we produce instance candidates which are organized in
a forest structure (for a simple illustration, only one tree is shown in this Ô¨Ågure). (b) Instance Candidate Selection: Given the
instance candidate forest, we incorporate instance consistency information (i.e., the selected instances from neighboring frames)
to select an optimal instance candidate subset. The selected instances are taken as part of the Ô¨Ånal instance segmentation results.

same instance are pushed together while pixels of different
instances are pushed further away. Instead of predicting a
probability value for a semantic class, an embedding vector is
typically generated for each pixel. Payer et al. [10] proposed
a convolutional gated recurrent unit (ConvGRU) network to
predict an embedding for each instance in a sequence of
images. Kulikov et al. [9] presented a method to explicitly
assign embedding to pixels of the same instance by feeding
instance properties to pre-deÔ¨Åned harmonic functions. Note
that pixel embedding methods still require a pixel clustering
step to obtain Ô¨Ånal instance segmentation results. For example,
mean-shift clustering algorithms [29], [30] have been widely
used to obtain instance masks from the embedding space.

The strategies of the second type seek to utilize morpholog-
ical instance information to identify instances. One of the best
known methods is watershed [14], [31], which takes a topo-
graphic map and multiple markers as input to separate adjacent
regions in an image. The number of markers determines the
number of regions. Watershed has been widely used as a post-
processing method to generate instances [11], [15], [32]. For
example, Eschweiler et al. [11] proposed to generate markers
from the predicted cell centroids and topological map from
the predicted membranes; the Ô¨Ånal cell segmentation results
were generated by applying marker-controlled watershed [33].
Another line of work aimed to incorporate instance-level topo-
logical properties by transforming them into training labels
and corresponding object functions [16], [34]. In distance map
methods [16], [18], [35], the Euclidean distances between each
instance pixel to the nearest instance boundary are utilized as
target labels. Graham et al. [17] proposed to use both horizon-
tal and vertical distances between each instance pixel to the
corresponding instance mass centers. Recently, Cellpose [12]
utilized horizontal and vertical gradients of center distances to
determine instances; it provided a pre-trained model which was
pre-trained on a large dataset. Thus, Cellpose can be applied
out of the box (without re-training).

Different from all the previous work, we explore instance-
level information contained in the probability maps. Given a
probability map generated by a general DL semantic segmen-
tation model, we apply many possible threshold values and
directly obtain different sets of instance candidates.

Instance Segmentation:

3) Consistency in Sequential

In
2D+time videos, the structures and distributions of dynamic
instances between consecutive frames are often similar or con-
sistent. For example, the same cell does not change position,
size, or shape drastically from one frame to the next. The
consistency in 2D image videos is called temporal consistency.
Such consistency also exists in the 2D slices of a 3D image
as spatial consistency. For example, in a 3D cardiovascular
image, cardiovascular tissues show gradual and continuous
changes in shape/color across adjacent 2D slices. Consistency
is preserved well with a higher frame rate or less movements
and morphological changes, while with a low frame rate
or large movements and morphological changes (e.g., cell
mitosis) consistency properties may not be conserved well.

Previous work [10], [19], [36] has shown that consistency
could be explored to improve instance segmentation in sequen-
tial image settings. There are mainly two ways to incorporate
temporal consistency, conducting instance-level matching or
pixel-wise correlation with recurrent neural networks (RNNs).
In instance-level matching methods, instance segmentation is
usually jointly attained with instance matching. Typically, in-
stance candidates are Ô¨Årst generated using handcrafted features
and an optimal matching model is applied to obtain both
instance segmentation and instance matching results. In [23],
instance candidates were generated from over-segmented su-
per pixel images, and a factor graph based matching model
was applied to select candidate instances. In [24], instance
candidates were generated by a simple classiÔ¨Åer trained on
handcrafted features, and the following matching task was
formulated as a constrained network Ô¨Çow problem. Recently,
deep learning methods were explored to utilize pixel correla-

ùë°":0.50Instance Candidate Generationselected instances from neighbor frameùë°#:0.60ùë°$:0.80probability mapthresholdInstance Candidate Selection4

IEEE TRANSACTIONS ON MEDICAL IMAGING, VOL. XX, NO. XX, XXXX 2020

tions between adjacent image frames. In [19], UNet and LSTM
were combined into UNet-LSTM to exploit pixel correlations
between neighboring image frames. An embedding recurrent
network [10] was proposed to consider pixel correlations
within the same frame and across multiple frames.

Different from the known RNN based methods which con-
sider pixel-wise correlations, our method can better preserve
consistency through instance-level matching. Compared to the
previous matching based methods, our generated instance can-
didates come from the probability maps, which take advantage
of the rich features extracted by DL semantic segmentation
models. Besides, our method is generic and easy, free from
complicated variables and data-speciÔ¨Åc hyper-parameters.

4) Segmentation Trees: Tree-like structures are commonly
used in segmentation methods, with each tree node repre-
senting one possible instance candidate. Tree-like structures
are typically generated by traditional methods without DL
networks [37]‚Äì[39]. For example, a tree can be generated from
super-pixels [37], [38]; after the leaf nodes of initial over-
segmented candidates are obtained, a tree structure is built by
iteratively merging similar super-pixels, until a pre-speciÔ¨Åed
stopping criterion is met. Segmentation trees [21], [40] can
also be generated from binary segmentation masks with a set
of Ô¨Ålter banks. A related segmentation tree is called component
tree [41], [42] which is created based directly on intensities
with a set of Ô¨Ålters of different threshold values. Different
from the previous segmentation trees, the instance candidate
forest in our method is built directly on top of probability
maps generated by DL semantic segmentation models.

III. METHODOLOGY

In this section, we present our proposed hierarchical earth
mover‚Äôs distance (H-EMD) framework for instance segmenta-
tion tasks in 2D+time video and 3D image settings. Table I
summarizes the main variables used.

For an input sequence of 2D images (i.e., a 2D+time video
or a 3D image stack), X = (x1, x2, . . . , xW ), the correspond-
ing probability maps P = (p1, p2, . . . , pW ) can be generated
by a pixel-wise DL semantic segmentation model inferring
individually on each image xw. Our H-EMD method performs
two main stages to identify instances in the entire image
sequence X (Fig. 1 gives an overview of these two stages). (1)
Instance Candidate Generation: For each probability map pw,
we use different threshold values to generate many possible
instance candidates. These instance candidates form a forest
structure F w, called instance candidate forest (ICF). We obtain
a list of ICFs, F = (F 1, F 2, . . . , F W ). (2) Instance Candi-
date Selection: We incorporate temporal or spatial instance
consistency to select a subset of instance candidates from the
ICF list F. SpeciÔ¨Åcally, we propose an iterative matching and
selection method (Fig. 3 shows an example of this iterative
selection process). We use a ‚Äústate‚Äù St to represent the instance
candidate selection status at iteration t of the iterative matching
and selection process. Initially, we select all the root nodes
in F which have only one leaf node in their corresponding
instance candidate trees. The selected instance candidates thus
form the initial state, S0. Given a state St (t = 0, 1, . . .)

in iteration t, the next state St+1 is obtained by instance
candidate matching: each selected instance candidate in St can
potentially be matched with unselected instance candidates in
their neighboring image frames. Newly matched (unselected)
instance candidates are then selected and added to St to form
St+1. After T iterations (for a speciÔ¨Åed T ), we terminate the
iterative process. Finally, we apply a padding method to select
from the remaining unselected candidates. All the selected
instance candidates form the Ô¨Ånal output.

A. Instance Candidate Generation

In this stage, for every input

image xw ‚àà X with a
foreground class probability map pw whose values range from
0 to 1, we aim to generate all the possible instance candidates,
i.e., connected components, on xw. This process is somewhat
similar to the generation of component trees with sequential
gray-levels [41], [43]. SpeciÔ¨Åcally, we take all the probability
values of pw (rounded to 2 decimal places), remove the dupli-
2 , . . . , vw
cate ones, and sort them into a list V w = {vw
H }
in increasing order. To remove noisy instance candidates, we
use only threshold values that are not smaller than a pre-
deÔ¨Åned value œÑ , that is, V w = {vw
h ‚â• œÑ }, where œÑ is
h ‚àà V w as a
empirically set to 0.50. We then use each value vw
threshold value to determine the connected components in xw
h . We say that vw
whose pixels‚Äô probability values are all ‚â• vw
h
induces an instance candidate set I vw
h , which is a collection
of mutually disjoint regions (connected components) in xw.
All such instance candidate sets form the instance candidates
for xw, I w. Fig. 2 shows a visual example of the generated
instance candidates.

h | vw

1 , vw

Next, we show that the instance candidates I w in xw thus
generated have some useful properties. First, we discuss the
Containment Relationship among the instance candidates.

i

i

i+1 (vw

i and vw

k ‚àà I vw

vw
i+1
i , i.e., I
j

1) Containment Relationship: For any two consecutive val-
i+1) in the sorted list V w and their
i < vw
ues vw
i and I vw
induced candidate sets I vw
i+1, every candidate region
vw
‚àà I vw
i+1
I
i+1 is a sub-region of one and only one candidate
j
region I vw
‚äÜ I vw
k . We shall prove the
containment relationship below, which is hinged on the mutual
exclusion-inclusion property of the instance candidate regions.
2) The Mutual Exclusion-inclusion Property: For any two
different instance candidate regions (i.e., candidate instances)
R(cid:48) and R(cid:48)(cid:48) in I w, either R(cid:48) and R(cid:48)(cid:48) do not intersect (exclusion),
or one is entirely contained in the other (inclusion). Note that
h ‚àà V w with its induced candidate
for any threshold value vw
set I vw
h , then p belongs
to one and only one candidate region (a connected component)
I vw
j ‚àà I vw
h . Further, for any two different candidate regions R(cid:48)
and R(cid:48)(cid:48), clearly either R(cid:48) and R(cid:48)(cid:48) do not intersect (overlap),
or they do intersect. If R(cid:48) ‚à© R(cid:48)(cid:48) = ‚àÖ, then they are mutually
exclusive. If they intersect, then let v(cid:48) and v(cid:48)(cid:48) be their induced
threshold values, respectively. If v(cid:48) = v(cid:48)(cid:48) and R(cid:48) and R(cid:48)(cid:48)
overlap,
is one connected component, and
thus R(cid:48) and R(cid:48)(cid:48) are not two different candidate regions, a
contradiction. Hence, assume v(cid:48) < v(cid:48)(cid:48). For any pixel p ‚àà R(cid:48)(cid:48),
p‚Äôs probability value is ‚â• v(cid:48)(cid:48) > v(cid:48). Thus, p is also contained
in a candidate region induced by v(cid:48). Hence, R(cid:48)(cid:48) is a sub-region

h , if a pixel p‚Äôs probability value is ‚â• vw

then R(cid:48) ‚à™ R(cid:48)(cid:48)

h

AUTHOR et al.: PREPARATION OF PAPERS FOR IEEE TRANSACTIONS ON MEDICAL IMAGING

5

h

Variables
X = (x1, x2, . . . , xW )
P = (p1, p2, . . . , pW )
V w = {vw
2 , . . . , vw
1 , vw
H }
I vw
F w = (I w, Hw)
Rw
Lw
N w = {N w
T w
n
St = {S1
Mr,d
fr,d

t , . . . , SW

t , S2

q }z

t }

q=1 (z = |Lw|)

TABLE I: The notation of the main variables used in Section III.

Description
A sequence of input raw images
A sequence of probability maps generated by a semantic segmentation model for a sequence X of images
The list of all distinct probability values of a probability map pw, in increasing order
The instance candidate set induced by a probability value vw
The instance candidate forest formed by the instance candidate set I w and the parent function Hw for a probability map pw
The root set of the instance candidate forest F w
The leaf set of the instance candidate forest F w
All the leaf-to-root paths (i.e., sequential node lists from each leaf node to its root node) in the instance candidate forest F w
The instance candidate tree containing a node n in the instance candidate forest F w
The instance candidate selection state at iteration t for all the images in X
The pairwise matching score between a reference instance r and a target instance d
A matching Ô¨Çow (integer variable) between a reference instance r and a target instance d, fr,d ‚àà {0, 1}

h for a probability map pw

Fig. 2: A visual example of some generated instance candidates (with a UNet-LSTM backbone). For a simple illustration, it
shows only the instance candidates generated with two threshold values (th), 0.50 and 0.80. Instance candidate region boundaries
are marked in white color. Yellow arrows point to the instance candidate regions where splitting occurs when the threshold
value increases from 0.50 to 0.80.

of a candidate region induced by v(cid:48). Since R(cid:48) and R(cid:48)(cid:48) overlap
and the candidate regions induced by v(cid:48) are mutually disjoint,
R(cid:48)(cid:48) ‚äÜ R(cid:48) (inclusion).

i

i

‚àà I vw

k ‚àà I vw

i (if I vi+1
j

Based on the containment relationship, for any candidate
vw
i+1, we can Ô¨Ånd its unique parent candidate I vw
i+1
I
k ‚àà
j
vw
I vw
i . Let Hw be the function that maps each candidate I
i+1
‚àà
j
i+1 to its parent candidate I vw
I vw
is a root node,
then Hw returns NIL). In summary, all the instance candidates
I w together form a forest F w = (I w, Hw), where I w can
also be viewed as the node set of all the candidate regions
and Hw is the parent function for each node in I w. Denote
the root node set of the forest F w as Rw and the leaf node
set as Lw. Denote the set of all the leaf-to-root paths in F w
q=1 (z = |Lw|), which includes all the paths
as N w = {N w
from a leaf to its root in F w. The instance candidate forest
F w is composed of instance candidate trees (ICTs). Each node
n belongs to one and only one tree T w
n in F w with the root
node rw
n is associated with a leaf-to-root path
n ) ‚äÜ Lw.
set N (T w

n ) ‚äÜ N w and a leaf set L(T w

n ‚àà Rw, and T w

q }z

B. Instance Candidate Selection

In the instance candidate selection stage, we aim to select
the correct instance candidates from the generated instance
candidate forest list F for the entire image sequence X, for
which we develop an iterative matching and selection method
on top of F.

SpeciÔ¨Åcally, we Ô¨Årst obtain an initial ‚Äústate‚Äù from each
forest F w ‚àà F by identifying and selecting the easy-to-
identify cases. The reason for doing so is that the easy-to-
identify instance candidate regions are quite stable and robust
based on the probability maps, and hence their selection is

of high conÔ¨Ådence. Next, the selected instance candidates
can be potentially matched with (as in tracking) unselected
instance candidates to select more instance candidates. The
matching process is performed for several iterations, and the
state, which contains the selected instance candidates and is
used in the matching process, is iteratively updated. Finally,
we select the remaining unselected instance candidates using
a padding method. The padding-selected candidates and the
selected instance candidates in the state form the Ô¨Ånal results.
Hence, this selection stage is conducted in three phases:
building the initial selected instance candidate state, iterative
matching and selection, and remaining instance candidate
selection by padding. We maintain a ‚Äústate‚Äù to contain the
selected instance candidates (which are part of the Ô¨Ånal
output), both initially and in the iterations. In each iteration,
the state is used by the matching process to help select more
candidates, and the newly selected candidates are added to the
state. Below we present these three phases in detail.

1) Building the Initial Selected Instance Candidate State: For
each ICF F w ‚àà F for the image frame xw in X, all the
root nodes with only one leaf node in their corresponding
instance candidate trees (ICTs) are selected and put into the
initial selected instance candidate state Sw
0 . All these root
instance candidates are easy to identify in its probability map
pw, with a single local maximum probability value in each
such root candidate region. Then, all the nodes in the ICTs
that contain any of these selected root nodes are discarded
from the instance candidates of F w (because their candidate
regions overlap with their root region and thus cannot appear
in the Ô¨Ånal output together with their root region). Let F w
0 be
the forest with the remaining instance candidates for xw. The
initial state construction is performed on the entire ICF list F,

(a) raw image(b) probability mapth:0.5th:0.80(c) instance candidatesth:0.506

IEEE TRANSACTIONS ON MEDICAL IMAGING, VOL. XX, NO. XX, XXXX 2020

Fig. 3: An example for illustrating the instance candidate selection stage on three consecutive image frames (e.g., frames w ‚àí1,
w, and w+1). Suppose instance candidates are already generated from the probability maps. First, high-conÔ¨Ådence segmentation
instance candidates corresponding to the tree roots with only a single path are selected to form the initial state. Then, selected
instance candidates are propagated to the neighboring frames to match with and select more (previously unselected) candidates.
Finally, the remaining unselected candidates are selected by a padding method. All the selected instance candidates are taken
as the Ô¨Ånal instance segmentation results.

t , S2

t , . . . , SW

and F w+1
t

t } and the forests in Ft = {F 1

EMD Matching: Given the instance candidate selection state
t , F 2
St = {S1
t ,
. . . , F W
t }, in iteration t + 1 (t = 0, 1, . . . , T ‚àí 1), our goal is
to use the selected instance candidates in each Sw
to match
t
with and select the unselected instance candidates in forests
F w‚àí1
of the neighboring image frames (if any).
t
Yet, in order to do that, we need to Ô¨Årst perform a matching
between every two consecutive Sw
. The reason for
doing so is that we need to Ô¨Årst identify the (high-conÔ¨Ådence)
instance consistency between every Sw
, that is, the
robust matching pairs between the already selected instance
candidates in Sw
. Note that for correctness, a
selected instance candidate (say) in Sw
that is involved in a
t
matched pair with a selected instance candidate in Sw+1
is
already ‚Äúoccupied‚Äù and thus should not be used to further
match with any unselected instance candidates in F w+1

t and in Sw+1

t and Sw+1

t and Sw+1

.

t

t

t

t

t

t and in Sw+1

To efÔ¨Åciently preform this step, note that all the selected
instance candidate regions in Sw
are mutually
disjoint connected components and do not form a non-trivial
hierarchical structure. Thus, an earth mover‚Äôs distance (EMD)
based matching model as in [26], [44], [45] is sufÔ¨Åcient for
computing an optimal matching between Sw
. This
EMD based matching model is deÔ¨Åned as:

t and Sw+1

t

t

EMD(Sw

t , Sw+1
t

) = max

f

(cid:88)

(cid:88)

r‚ààSw
t

d‚ààSw+1
t

Mr,dfr,d

(4)

fr,d ‚â§ 1, ‚àÄd ‚àà Sw+1

t

,

(cid:88)

r‚ààSw
t

(cid:88)

d‚ààSw+1
t

fr,d ‚â§ 1, ‚àÄr ‚àà Sw
t ,

fr,d ‚àà {0, 1}, if

2 ¬∑ abs(|r| ‚àí |d|)
|r| + |d|

< Œ¥,

Mr,d = IoU(r, d), ‚àÄr ‚àà Sw

t and ‚àÄd ‚àà Sw+1

t

(5)

(6)

(7)

(8)

.

Fig. 4: Illustrating our proposed H-EMD matching model for
video instance segmentation. The probability map of the frame
w gives rise to an instance candidate forest (ICF). The ICF
of frame w is matched with the considered (already selected)
instances in the frame w ‚àí 1 to determine the optimal instance
candidates in the frame w as part of the Ô¨Ånal instances. For
a simple illustration, only a subset of instances/candidates is
shown.

and hence the initially selected instance candidates can appear
across the image frames in X. Thus, we have:

0 = {r | r ‚àà Rw and |N (T w
Sw

r )| = 1},

F w

0 = F w ‚àí {n | n ‚àà T w

r , r ‚àà Sw

0 },

S0 = {S1

0 , S2

0 , . . . , SW

0 }.

(1)

(2)

(3)

2) Iterative Matching and Selection: The matching and selec-
tion process is conducted in T iterations (for a speciÔ¨Åed T ; we
choose T = 10 experimentally). In each iteration, we perform
three major steps: EMD matching, H-EMD matching, and state
updating. We describe these three steps in detail below.

ICF generationmatching flowinitially selectedselected by paddingselected by matchingframe w-1frame wframe w+1initialselectioniter 1iter 2paddinginstance candidate selectiondeep learning modelDLDLDLDLframe wùëñinstance candidate forestframe w-1selected instancesinstance candidate                                          selectionAUTHOR et al.: PREPARATION OF PAPERS FOR IEEE TRANSACTIONS ON MEDICAL IMAGING

7

We utilize the Intersection over Union (IoU) as the matching
score Mr,d between each pair of considered instance candi-
dates across two frames. Only the candidate pairs with an
object size change less than Œ¥ (we set Œ¥ = 0.35 experimentally)
are taken as valid considered matching pairs (see Eq. (7)).
Eq. (7) enables us to deal with cell mitosis. Cell mitosis
often occurs with considerable shape/size changes (e.g., larger
than Œ¥). Thus, if a splitting event occurs (say) from frame
w ‚àí 1 to frame w, the (already) divided cells in frame w are
unaffected by the matching result on frames w ‚àí 1 and w,
while they can be matched (or captured) by the corresponding
divided cells in frame w + 1. We solve this EMD matching
problem by integer linear programming (ILP) to obtain the
optimal matching result (Ô¨Çow) fr,d. Then, the selected instance
candidates in Sw
thus matched are removed from
t
involvement in the next matching step (H-EMD), as:

t and in Sw+1

Sw‚Üíw+1

t

= Sw

t ‚àí {r ‚àà Sw
t

| fr,d = 1, ‚àÄd ‚àà Sw+1

t

},

(9)

= Sw+1
t
and Sw+1

Sw+1‚Üíw

t

‚àí {d ‚àà Sw+1

t

| fr,d = 1, ‚àÄr ‚àà Sw

t }. (10)

t

Note that Sw
, containing the selected instance
t
candidates for frames xw and xw+1 so far, remain unchanged.
H-EMD Matching: Next, we present a new hierarchical earth
mover‚Äôs distance (H-EMD) matching model to use the selected
instance candidates which are unmatched by the above EMD
matching step to match with and select unselected instance
candidates in the neighboring frames. Consider the unmatched
selected instance candidates in Sw‚Üíw+1
and the unselected
instance candidates in F w+1
for the frame xw+1. This match-
ing case is denoted as w ‚Üí w + 1. The reverse case is also
considered symmetrically and denoted as w + 1 ‚Üí w. We de-
Ô¨Åne the case of H-EMD(Sw‚Üíw+1
) below (the reverse
matching case is H-EMD(Sw+1‚Üíw

t

t

t

, F w+1
t
, F w
t ) symmetrically).
(cid:88)
(cid:88)

t

H-EMD(Sw‚Üíw+1

t

, F w+1
t

) = max

f

Mr,dfr,d

r‚ààSw‚Üíw+1

t

d‚ààF w+1
t

fr,d ‚â§ 1, ‚àÄr ‚àà Sw‚Üíw+1

t

,

(11)
(12)

(cid:88)

d‚ààF w+1
t

(cid:88)

(cid:88)

r‚ààSw‚Üíw+1

t

nq‚ààNq

fr,nq ‚â§ 1, ‚àÄNq ‚àà N (F w+1

t

),

(13)

fr,d ‚àà {0, 1}, if

abs(|r| ‚àí |d|)
|r|

< Œ¥,

(14)

Mr,d = IoU(r, d), ‚àÄr ‚àà Sw‚Üíw+1

t

and ‚àÄd ‚àà F w+1

t

,

(15)

t

where N (F w+1
) denotes the set of all the leaf-to-root paths
in F w+1
. Eq. (13) aims to incorporate mutual exclusion in
t
H-EMD. According to the mutual exclusion requirement for
the output instances, one pixel can belong to at most one
output instance, which was used in previous work (e.g., [37]).
SpeciÔ¨Åcally, let Nq denote the node list of a leaf-to-root path
in F w+1
. If an instance candidate (a node) in Nq is selected,
t
then any other candidates in Nq cannot be selected anymore.
Further, note that Eq. (13) also plays a role for H-EMD that
Eq. (5) plays for EMD, that is, it ensures that at most one node
r ‚àà Sw‚Üíw+1
t

can be matched with any node nq ‚àà F w+1

.

t

We solve the H-EMD matching problem by integer linear
programming (ILP) to yield the optimal matching results.
Fig. 4 gives an illustration of the H-EMD matching model.

State Update: Note that for each frame xw with 1 < w < W
(i.e., except the Ô¨Årst and last frames), F w
t can receive matching
results from both the (left) frame xw‚àí1 and the (right) frame
xw+1. There may be conÔ¨Çicts between these two directional
Ô¨Çows to any of the trees in F w
t . Thus, we apply a combining
function Œ¶ with a left-right-competing rule: For each tree
T w ‚àà F w
t , if the sum of the matching scores from the left
frame is larger than or equal to the sum of the matching scores
from the right frame, then we use the left matching results
as the matching results for T w; otherwise, we use the right
matching results as the matching results for T w. That is,

‚àÜSw

t =

Ô£±
Ô£¥Ô£≤

Ô£¥Ô£≥

f w+1‚Üíw
f w‚àí1‚Üíw
Œ¶(f w+1‚Üíw, f w‚àí1‚Üíw) otherwise.

w = 1,
w = W,

(16)

After the instance candidate selection results from each F w
t
are obtained by the above H-EMD matching step in iteration
t+1 by adding Sw
t + 1, we compute the new state Sw
t and the
t from F w
newly selected instance candidates ‚àÜSw
t . Further, for
each selected instance candidate node nc ‚àà ‚àÜSw
t , all the nodes
in any leaf-to-root path containing nc in F w
t are removed from
the remaining unselected instance candidates of F w
(due to
t
mutual exclusion). That is,

(17)

t ‚à™ ‚àÜSw
t ,

t+1 = Sw
Sw
, N w
nc

F w

‚àà N (F w

t+1 = F w

t ‚àí{n | n ‚àà N w
nc

t , nc), nc ‚àà ‚àÜSw

t },
(18)
where N w
nc denotes a leaf-to-root path containing a node nc ‚àà
t , nc) of all such paths in the forest F w
‚àÜSw
t .
t
After performing T iterations of the above matching and
selection process, we Ô¨Ånally complete our instance candidate
selection by applying the ‚Äúpadding‚Äù process below.

in the set N (F w

Remarks: (i) The above matching and selection process
is repeated multiple (T ) iterations in two directions indepen-
dently and locally (both for frames w ‚àí 1 and w, and for
frames w and w + 1) in order to allow matching results to
be propagated not only to the consecutive frames but also
to nearby non-consecutive frames. For example, matching
results on earlier (or later) frames can be propagated gradually
through iterations to multiple nearby later (or earlier) frames
to help select unselected instance candidates on such later (or
earlier) frames.

(ii) The observant readers may have noticed that in the EMD
matching step, some redundant matching may be performed,
which is unnecessary. For example, in iteration t + 1, the
EMD matching step may obtain some matched pairs between
the selected instance candidates in Sw
which
have been obtained by the EMD matching in earlier iterations.
The reason for this is that we keep all the selected instance
candidates for the frame xw in the state Sw
t , which is used
for the EMD matching step. Note that the EMD matching
step is needed to ensure correctness because newly selected
instance candidates will be added in order to produce new
states iteratively, no matter such redundant matching occurs

t and in Sw+1

t

8

IEEE TRANSACTIONS ON MEDICAL IMAGING, VOL. XX, NO. XX, XXXX 2020

t

and Sw+1
t

in the EMD matching step or not. While we could remove
those selected instance candidates in each Sw
that are involved
t
in any matched pairs obtained by the EMD matching in any
iteration, doing so involves a tedious case analysis, since Sw
t
is in general involved with both Sw‚àí1
in the EMD
matching, and Sw‚àí1
may cause a different subset of the
t
to be removed from Sw
selected instance candidates in Sw
t
t
than those caused by Sw+1
, thus giving rise to two versions
of Sw
t before the EMD matching step. For simplicity of our
presentation, we choose to present our matching and selec-
tion process in the current form, tolerating some redundant
matching in the EMD matching step. We should mention that
experimentally, we found that such redundant matching incurs
only very negligible additional computation time.

t

3) Remaining Instance Candidate Selection by Padding:
Finally, we consider the remaining unselected instance can-
didates in each forest F w
T , as there may still be instance
candidates in F w
T that are not selected by the iterative matching
and selection process. We can put these remaining unselected
instance candidates into two cases. (a) Unmatchable case:
The candidates do not show instance consistency in two
consecutive frames. For example,
the object size changes
drastically compared to other corresponding candidates in the
two consecutive frames, or the candidates appear in one frame
but disappear in the other frame. (b) Matchable case: The
candidates do show instance consistency, but none of the
potentially matchable candidates in the neighboring frames is
picked to match by the above matching process. In both these
cases, the matching does not show effect on such unselected
candidates. Thus, we solve these cases in a single-frame seg-
mentation manner, called padding (that is, no longer utilizing
instance consistency across consecutive frames because up to
this point, it could not help select those remaining unselected
candidates).

To select from the remaining unselected instance candidates
in F w
T for each individual frame xw, one can actually apply
any common post-processing methods. We found experimen-
tally that selecting all the remaining unselected root nodes
yields very good results (these root nodes belong to ICT trees
in F w
T with one or more leaf-to-root paths). As shown in
Eq. (19), the Ô¨Ånal instance set Sw
F for each frame xw is formed
by the union of the unselected root instance candidate regions
Rw
T and the already selected instances in the Ô¨Ånal state
Sw

T in F w
T by the above matching and selection process, as
F = Sw
Sw

T ‚à™ Rw
T .

(19)

SF = {S1
segmentation results for the input image sequence X.

F } is taken as the Ô¨Ånal

F , . . . , SW

F , S2

instance

IV. EXPERIMENTS

In the experiments, we apply H-EMD directly onto the prob-
ability maps generated by three representative DL semantic
segmentation models on six biomedical video datasets and
two 3D datasets. Compared to commonly-used post-processing
methods, our H-EMD yields improved instance segmentation
results across all the three DL semantic segmentation models.
Compared to other state-of-the-art segmentation methods, our

H-EMD also shows superiority. Further, we submitted our
results on the public video datasets to the open Cell Segmen-
tation Benchmark from the Cell Tracking Challenge [53], and
it showed that our results are highly competitive compared
to all the other submissions. Finally, we conduct additional
experiments, which demonstrate the effectiveness of the key
components in our H-EMD.

1) Datasets: We evaluate our H-EMD on six 2D+time cell
video datasets, including two in-house datasets (P. aerugi-
nosa [54] and M. xanthus [44]) and four public datasets from
the Cell Tracking Challenge [53] (Fluo-N2DL-HeLa, PhC-
C2DL-PSC, PhC-C2DH-U373, and Fluo-N2DH-SIM+), and
two 3D datasets, including one in-house Fungus [52], [55],
[56] and one public Fluo-N3DH-CHO from the Cell Tracking
Challenge. For the datasets from the Cell Tracking Challenge,
each one contains two training sequences and two challenge
sequences (with no reference annotation provided). Except
the Cell Segmentation Benchmark submission experiment (see
Section IV-.3) which uses both the training sequences as
training data and performs evaluation on the two challenge
sequences, all the other experiments use one training sequence
as the training set and perform inference on the other training
sequence. More speciÔ¨Åcally, Fluo-N2DH-SIM+ uses the 2nd
video for training and the 1st video for testing, and vice versa
for all the other datasets.

For the in-house datasets, instance segmentation annotations
were manually labeled by experts. For the public datasets
from the Cell Tracking Challenge, three types of instance seg-
mentation annotations are provided for the training sequences:
ground truth, gold truth, and silver truth. Ground truth is the
‚Äúexact true‚Äù annotations for simulated datasets. Gold truth
annotations contain human-made reference annotations for a
part of object instances. Silver truth is computer-aided anno-
tations which are generated by fusing the previous submission
results [57]. For the simulated Fluo-N2DH-SIM+ dataset, we
use ground truth. For the other public datasets, we use silver
truth unless otherwise stated. More details on these datasets
are given below.

P. aeruginosa. The in-house P. aeruginosa dataset [54]
contains two videos of 2D Ô¨Çuorescence microscopy images
for segmenting dynamic P. aeruginosa cells. One video of
100 frames is used for training (400 √ó 400 pixels per frame),
and the other video of 40 frames is for testing (513 √ó 513
pixels per frame). The videos contain high density cells (see
Fig. 5) which are difÔ¨Åcult to segment.

M. xanthus. The in-house M. xanthus dataset [44] contains
two videos of 2D electron microscopy images for segmenting
dynamic M. xanthus cells, in which each frame has 317 √ó 472
pixels. One video of 30 frames is used for training, and the
other video of 43 frames is for testing.
public

Fluo-N2DL-HeLa
[53] contains Ô¨Çuorescence microscopy videos of

Fluo-N2DL-HeLa.

The

dataset
dynamic HeLa cells.

PhC-C2DL-PSC. The public PhC-C2DL-PSC dataset [53]
contains phase contrast microscopy videos of dynamic pan-
creatic stem cells.

PhC-C2DH-U373. The public PhC-C2DH-U373 dataset
[53] contains phase contrast microscopy videos of dynamic

AUTHOR et al.: PREPARATION OF PAPERS FOR IEEE TRANSACTIONS ON MEDICAL IMAGING

9

Fig. 5: Visual examples of instance segmentation results on the six video datasets using the U-Net backbone. Yellow arrows
point to some instance errors corrected by our H-EMD method, and green arrows point to better instance boundaries attained
by H-EMD. GT = ground truth; ST = silver truth; PM = probability map; WS = Watershed.

Fig. 6: Visual examples of instance segmentation results on some slices of the two 3D stack datasets using the U-Net backbone.
Yellow arrows point to some instance segmentation errors corrected by H-EMD. GT = ground truth; ST = silver truth; PM =
probability map; WS = watershed.

STSTSTGTGTGTFluo-N2DL-HeLaPhC-C2DL-PSCPhC-C2DH-U373Fluo-N2DH-SIM+P.aeruginosaM.xanthusPMPMPMPMPMPMWSWSWSH-EMDH-EMDH-EMDH-EMDH-EMDH-EMDWSWSWSGTSTPMPMWSWSH-EMDH-EMDFluo-N3DH-CHOFungus10

IEEE TRANSACTIONS ON MEDICAL IMAGING, VOL. XX, NO. XX, XXXX 2020

TABLE II: Instance segmentation results on the six video datasets. A bold score represents the best performance on the
corresponding dataset, and an underline denotes the best post-processing performance with a speciÔ¨Åc backbone. ‚Äú*‚Äù indicates
that our result is statistically signiÔ¨Åcantly better than the second-best post-processing result (with p-value < 0.05).

Fluo-N2DL-HeLa

PhC-C2DL-PSC

PhC-C2DH-U373

Fluo-N2DH-SIM+

P. aeruginosa

M. xanthus

F1 (%) ‚Üë AJI (%) ‚Üë F1 (%) ‚Üë AJI (%) ‚Üë F1 (%) ‚Üë AJI (%) ‚Üë F1 (%) ‚Üë AJI (%) ‚Üë F1 (%) ‚Üë AJI (%) ‚Üë F1 (%) ‚Üë AJI (%) ‚Üë

KTH-SE [46]
Cellpose [12]
Hybrid [44]
Embedding [7]
Mask R-CNN [47]
StarDist [35]
KIT-Sch-GE [18]
nnU-Net [48]

96.3
93.9
‚Äì
‚Äì
90.7¬±1.2
96.7¬±0.1
96.6¬±0.1
93.1¬±0.4

90.0
79.9
‚Äì
‚Äì
77.0¬±2.0
91.1¬±0.2
92.6¬±0.2
83.7¬±0.8

93.8
92.8
‚Äì
‚Äì
89.7¬±0.8
96.4¬±0.1
95.7¬±0.3
91.4¬±0.1

72.9
72.4
‚Äì
‚Äì
71.0¬±1.1
82.0¬±0.4
80.4¬±1.0
77.7¬±0.1

‚Äì
‚Äì
‚Äì
‚Äì
67.6¬± 0.9
93.4¬±0.3
93.2¬±0.8
92.3¬±0.1

93.4¬±0.2
93.2¬±0.2
95.7¬±0.1
91.4¬±0.1
93.3¬±0.2

93.8¬±0.5
93.1¬±0.1
95.9¬±0.1
92.2¬±0.2
93.1¬±0.1

84.8¬±0.4
84.4¬±0.4
91.5¬±0.2
80.0¬±0.2
84.5¬±0.4

82.7¬±0.2
82.0¬±0.1
86.7¬±0.1
80.5¬±0.2
81.8¬±0.1

92.4¬±0.6
0.5-Th
91.3¬±1.4
Otsu
91.6¬±0.6
Watershed
92.5¬±0.3
DenseCRF
91.6¬±0.7
MaxValue
H-EMD 96.4¬±0.1‚àó 92.4¬±0.2‚àó 96.3¬±0.3‚àó 87.2¬±0.1‚àó 93.5¬±0.2‚àó
92.3¬±0.6
89.6¬±1.4
80.9¬±0.3
92.6¬±0.2
0.5-Th
92.1¬±0.5
89.4¬±1.0
80.0¬±0.2
92.0¬±0.1
Otsu
95.5¬±0.4
91.2¬±0.4
86.5¬±0.1
95.8¬±0.1
Watershed
91.0¬±0.6
91.2¬±0.4
78.6¬±0.3
91.1¬±0.2
DenseCRF
92.0¬±0.3
89.0¬±1.3
92.6¬±0.6
79.9¬±0.5
MaxValue
96.6¬±0.4‚àó 87.2¬±0.1‚àó 93.3¬±0.3‚àó
H-EMD 96.1¬±0.2‚àó
88.4¬±0.9
79.6¬±0.5
91.5¬±0.3
92.5¬±0.2
0.5-Th
87.6¬±0.8
78.9¬±0.4
91.1¬±0.3
92.4¬±0.2
Otsu
89.6¬±0.7
85.3¬±0.2
95.7¬±0.1
94.6¬±0.3
Watershed
89.2¬±0.8
79.4¬±0.4
91.7¬±0.3
92.2¬±0.2
DenseCRF
87.1¬±0.9
79.3¬±0.4
91.1¬±0.2
92.4¬±0.2
MaxValue
92.6¬±0.4‚àó
H-EMD 95.7¬±0.1‚àó 90.2¬±0.3‚àó 96.1¬±0.1‚àó
85.4¬±0.2

82.8¬±0.8
82.5¬±0.7
91.4¬±0.6
80.0¬±0.6
83.5¬±1.0
91.6¬±0.2
83.4¬±0.4
83.4¬±0.3
89.1¬±0.6
82.5¬±0.3
83.4¬±0.3

U-Net
[6]

DCAN
[3]

UNet-
LSTM
[19]

‚Äì
‚Äì
‚Äì
‚Äì
62.5¬±2.0
82.2¬±1.0
79.6¬±0.8
86.3¬±0.4

88.5¬±0.5
89.0¬±0.4
89.4¬±0.8
89.0¬±0.4
88.9¬±0.4
89.4¬±0.4
87.6¬±0.6
88.1¬±0.6
88.4¬±0.5
88.1¬±0.6
88.0¬±0.6
88.7¬±0.4
86.4¬±0.3
86.9¬±0.3
88.1¬±0.4
87.0¬±0.3
86.8¬±0.3
87.3¬±0.4

97.9
73.3
‚Äì
89.8¬±0.8
90.4¬±1.3
96.2¬±0.2
95.7¬±0.9
97.9¬±0.1

87.8
64.6
‚Äì
73.2¬±1.2
76.9¬±1.5
79.1¬±0.4
81.5¬±1.8
83.9¬±0.3

‚Äì
‚Äì
‚Äì
‚Äì
69.5¬±0.6
93.1¬±0.4
90.1¬±1.1
93.5¬±0.4

‚Äì
‚Äì
‚Äì
‚Äì
51.5¬±0.5
68.9¬±0.7
65.5¬±0.9
83.7¬±0.6

‚Äì
‚Äì
79.2
‚Äì
62.3¬±0.9
85.4¬±0.5
92.8¬±0.1
95.6¬±0.1

‚Äì
‚Äì
61.1
‚Äì
39.6¬±0.7
62.0¬±0.4
60.2¬±0.9
84.5¬±0.4

75.5¬±1.5
74.9¬±1.5
83.8¬±0.5
73.1¬±0.2
72.1¬±1.6
84.3¬±0.5
76.2¬±0.9
75.6¬±0.8
83.7¬±0.8
74.0¬±0.7
73.3¬±0.9

80.1¬±0.6
81.0¬±0.6
82.0¬±0.9
80.0¬±0.5
80.0¬±0.9
82.1¬±0.4
79.6¬±0.4
80.4¬±0.4
79.9¬±0.6
79.7¬±0.4
79.7¬±0.5

92.7¬±0.3
92.8¬±0.3
93.4¬±0.8
93.2¬±0.2
92.2¬±0.2
94.4¬±0.3‚àó
92.3¬±0.9
92.2¬±1.0
92.5¬±0.3
93.1¬±0.3
91.6¬±1.1

84.8¬±0.9
94.3¬±0.3
92.9¬±0.6
84.2¬±0.8
94.2¬±0.3
92.6¬±0.6
87.5¬±0.5
95.5¬±0.1
97.6¬±0.1
83.9¬±1.0
94.1¬±0.3
91.6¬±0.5
93.9¬±0.4
83.7¬±1.0
91.4¬±0.6
97.1¬±0.6‚àó 89.3¬±0.4‚àó
97.6¬±0.1
82.2¬±0.4
93.3¬±0.3
92.6¬±0.8
81.9¬±0.5
93.0¬±0.3
92.5¬±0.6
87.4¬±0.3
95.6¬±0.2
97.7¬±0.3
81.4¬±0.6
93.1¬±0.3
91.7¬±0.6
91.5¬±0.6
82.1¬±0.5
93.2¬±0.3
98.0¬± 0.1 85.2¬±0.6‚àó 94.2¬±0.3‚àó 80.9¬±0.5‚àó 96.7¬±0.2‚àó 88.4¬±0.4‚àó
81.2¬±0.6
96.2¬±0.5
90.9¬±0.3
81.0¬±0.6
95.9¬±0.4
90.9¬±0.2
85.4¬±0.2
98.3¬±0.2
90.3¬±0.1
80.5¬±0.9
95.5¬±0.4
91.3¬±0.1
90.9¬±0.2
95.7¬±0.3
81.1¬±0.5
92.6¬±0.3‚àó 82.5¬±0.1‚àó 96.7¬±0.2‚àó 87.1¬±0.3‚àó
98.7¬±0.6

82.4¬±1.0
81.8¬±0.8
86.0¬±0.7
81.3¬±1.1
81.3¬±0.8
86.6¬±0.8

80.5¬±0.8
80.5¬±0.7
80.2¬±0.2
81.6¬±0.4
80.5¬±0.7

93.3¬±0.3
93.2¬±0.3
94.9¬±0.2
93.2¬±0.4
93.3¬±0.3

TABLE III: Cell segmentation benchmarking: Evaluation of the submitted methods published by the Cell Tracking Challenge
organizers.

1st
2nd
3rd

1st
2nd
3rd

1st
2nd
3rd

Fluo-N2DL-HeLa
0.957
0.957
0.954
0.946 (6th)
0.923
0.923
0.922
0.903 (8th)
0.994
0.992
0.992
0.990 (7th)

OPCSB

SEG

DET

PhC-C2DL-PSC
0.859
0.849 (2nd)
0.847

PhC-C2DH-U373
0.959
0.958 (2nd)
0.956

Fluo-N2DH-SIM+
0.905
0.899 (2nd)
0.898

0.743
0.733
0.728 (3rd)

0.975
0.972
0.971 (3rd)

0.929 (1st)
0.927
0.927

0.991
0.990
0.988 (3rd)

0.832
0.827 (2nd)
0.826

0.983
0.981
0.979
0.972 (8th)

Our H-EMD
TUG-AT [10]
CALT-US [13]
BGU-IL [19]
KIT-Sch-GE [18]
DKFZ-GE [48]
MU-Ba-US [49]
UNSW-AU [50]
UVA-NL [15]
FR-Ro-GE [6]
RWTH-GE [11]
BRF-GE [51]
KTH-SE [46]

Glioblastoma-astrocytoma U373 cells.

Fluo-N2DH-SIM+. The public Fluo-N2DH-SIM+ dataset

[53] contains simulated Ô¨Çuorescence-stained cells.

Fluo-N3DH-CHO. The public Fluo-N3DH-CHO dataset
[53] contains 3D Ô¨Çuorescence microscopy images of Chinese
Hamster Ovarian (CHO) nuclei.

Fungus. The in-house Fungus dataset [52], [55], [56] con-
tains 4 3D electron microscopy images for segmenting fungus
cells captured from body tissues of ants, whose 2D slices are
of 853 √ó 877 pixels each. 16 slices in one stack are used as
training data, and the other 3 stacks are used as test data. For
each test stack, 16 slices are labeled (48 slices in total) for
evaluation.

2) Comparison Methods: We apply our H-EMD on top
of common DL semantic segmentation models to compute
instance segmentation results from the generated probability
maps. Thus, we compare our H-EMD with two types of known
methods: (1) DL semantic segmentation models with other
post-processing methods; (2) state-of-the-art instance segmen-

Fig. 7: Illustrating the inÔ¨Çuence of the pre-speciÔ¨Åed thresh-
old value œÑ on the instance segmentation results in F1 and
AJI on three video datasets. U373 = PhC-C2DH-U373, P =
P. aeruginosa, and M = M. xanthus.

Threshold valuesScores (%)50607080901000.200.400.600.80U373_F1U373_AJIP_F1P_AJIM_F1M_AJIAUTHOR et al.: PREPARATION OF PAPERS FOR IEEE TRANSACTIONS ON MEDICAL IMAGING

11

Fig. 8: Illustrating the inÔ¨Çuence of the number of matching-and-selection iterations on the instance segmentation results in F1
(left) and AJI (right) on the six video datasets.

TABLE IV: Instance segmentation results on the two 3D
datasets. A bold score represents the best performance on
the corresponding dataset, and an underline denotes the best
post-processing performance with a speciÔ¨Åc backbone. ‚Äú*‚Äù
indicates that our result is statistically signiÔ¨Åcantly better than
the second-best post-processing result (with p-value < 0.05).

KTH-SE [46]
SPDA [52]
Cellpose [12]
PixelEmbedding [7]
Mask R-CNN [47]
StarDist [35]
KIT-Sch-GE [18]
nnU-Net [48]

U-Net [6]

DCAN [3]

UNet-LSTM
[19]

0.5-Th
Otsu
Watershed
DenseCRF
Max Value
H-EMD
0.5 Th
Otsu
Watershed
DenseCRF
Max Value
H-EMD
0.5-Th
Otsu
Watershed
DenseCRF
Max Value
H-EMD

Fluo-N3DH-CHO

Fungus

F1 (%) ‚Üë AJI (%) ‚Üë F1 (%) ‚Üë AJI (%) ‚Üë

83.4
75.1
‚Äì
‚Äì
76.3
66.3
89.0¬±1.0
74.9¬±1.6
89.4¬±0.6
77.4¬±1.7
94.9¬±0.4
83.7¬±0.6
96.2¬±0.2
83.8¬±0.4
94.8¬±0.1
84.0¬±0.1
95.5¬±0.3
85.4¬±0.4
95.6¬±0.2
85.2¬±0.4
96.2 ¬±0.2
85.4¬±0.3
96.0¬±0.3
85.4¬±0.3
85.4¬±0.3
95.4¬±0.3
97.0¬±0.1‚àó 86.4¬±0.3‚àó
84.8¬±0.7
94.7¬±0.6
84.6¬±0.7
94.8¬±0.6
85.1¬±0.6
95.5¬±0.6
84.7¬±0.7
95.7¬±0.5
84.8¬±0.6
94.7¬±0.5
96.9¬±0.5‚àó 85.9¬±0.5‚àó
81.8¬±0.6
90.5¬±1.0
81.7¬±0.5
90.5¬±0.9
78.5¬±0.6
90.5¬±0.8
81.9¬±0.5
92.5¬±0.6
81.7¬±0.5
89.9¬±0.9
82.7¬±0.8
92.7¬±0.4

‚Äì
‚Äì
86.9
77.2
‚Äì
‚Äì
‚Äì
‚Äì
67.3¬±1.1
53.1¬±1.1
84.6¬±0.2
75.0¬±0.3
84.8¬±0.2
73.9¬±0.3
84.5¬±0.2
70.7¬±0.1
87.3¬±0.1
77.3¬±0.7
87.3¬±0.1
77.0¬±0.8
87.3¬±0.1
77.4¬±0.1
86.8¬±0.1
76.8¬±0.6
75.7¬±0.9
87.3¬±0.1
87.4¬±0.1 78.4¬±0.1‚àó
76.8¬±0.5
87.1¬±0.4
76.7¬±0.6
87.0¬±0.3
77.2¬±0.2
87.0¬±0.3
76.5¬±0.6
86.5¬±0.3
76.1¬±0.5
87.0¬±0.3
87.3¬±0.3 78.3¬±0.2‚àó
72.6¬±0.8
85.6¬±0.1
72.5¬±0.7
85.5¬±0.1
75.0¬±0.2
86.0¬±0.3
72.3¬±0.8
85.1¬±0.1
72.4¬±0.7
85.5¬±0.1
85.8¬±0.3 76.2¬±0.2‚àó

tation methods that do not necessarily output probability maps.
We choose the following three commonly-used DL semantic
segmentation models for biomedical image segmentation to
generate probability maps. All these semantic segmentation
models predict three-class semantic segmentation: foreground,
boundary, and background. Foreground-class probability maps
are used as input to our H-EMD method.

‚Ä¢ U-Net [6]: A U-shaped architecture that yields accurate
dense predictions for biomedical image segmentation.
‚Ä¢ DCAN [3]: A model proposed for gland segmentation
which introduces the contour class to separate instances.
integrating Convolutional
Long Short-Term Memory with U-Net for temporal in-
stance segmentation tasks.

‚Ä¢ UNet-LSTM [19]: A model

We compare our H-EMD with Ô¨Åve common post-processing

methods that identify instances from probability maps.

‚Ä¢ 0.5-Th [27]: The probability maps are binarized using a
speciÔ¨Åc threshold value of 0.50, and connected compo-
nents are taken as the Ô¨Ånal instances [3], [27].

‚Ä¢ Otsu [58]: Given an image, the threshold value is au-
tomatically determined by minimizing the intra-class in-
tensity variance. Then pixels are binarized and connected
components are taken as the Ô¨Ånal instances.

‚Ä¢ MaxValue [19]: Each pixel is assigned to one of three
classes (background, boundary, and foreground) with the
maximum probability. Then the connected foreground
pixels are grouped into instances.

‚Ä¢ Watershed [14]: First, the probability maps are binarized
using the 0.50 threshold value. Then a distance map
is computed for each binary image. To reduce over-
the distance map is smoothed. Finally,
segmentation,
the smoothed distance maps are fed to the Watershed
algorithm to produce instance results.

‚Ä¢ DenseCRF [59]: Both the raw images and their prob-
ability maps are fed to the DenseCRF model. Pixels
with similar features (e.g., color and probability) are
assigned to the same semantic class (e.g., foreground
or background). Connected components are taken as the
Ô¨Ånal instance segmentation results.

We also compare our H-EMD with other state-of-the-art

instance segmentation methods.

‚Ä¢ Hybrid [44]: It was proposed for the M. xanthus dataset,
which uses active contour to obtain cell segmentation and
utilizes video tracking to further improve segmentation.
‚Ä¢ KTH-SE [46]: It ranks 1st (in the OPCSB and DET
metrics) in the Cell Segmentation Benchmark on the
Fluo-N3DH-CHO dataset. It uses a bandpass Ô¨Åltering
based segmentation algorithm [53] to segment cells and
applies the Viterbi
tracking algorithm [25] to correct
potential segmentation errors.

‚Ä¢ SPDA [52]: It was the best-known method for the Fungus
dataset. It proposed the superpixel augmentation approach
for training a deep learning model to improve biomedical
image segmentation.

‚Ä¢ PixelEmbedding [7]: It predicts pixel embedding such
that pixels of neighboring instances have large cosine
distances. A seed growing algorithm is applied to group

Matching iterationsF1 score (%)90929496982468101214Fluo-N2DL-HeLaPhC-C2DL-PSCPhC-C2DH-U373Fluo-N2DH-SIM+P.aeruginosaM.xanthusMatching iterationsAJI score (%)75808590952468101214Fluo-N2DL-HeLaPhC-C2DL-PSCPhC-C2DH-U373Fluo-N2DH-SIM+P.aeruginosaM.xanthus12

IEEE TRANSACTIONS ON MEDICAL IMAGING, VOL. XX, NO. XX, XXXX 2020

TABLE V: SEG and DET based evaluation on the four public video datasets (using ground truth for the Fluo-N2DH-SIM+
dataset and gold truth for the other three datasets).

Fluo-N2DL-HeLa

PhC-C2DL-PSC

PhC-C2DH-U373

Fluo-N2DH-SIM+

KTH-SE [46]
Cellpose [12]
Embedding [7]
Mask R-CNN [47]
StarDist [35]
KIT-Sch-GE [18]
nnU-Net [48]

SEG ‚Üë
0.848
0.800
‚Äì

DET ‚Üë
0.983
0.970
‚Äì

SEG ‚Üë
0.680
0.677
‚Äì

DET ‚Üë
0.967
0.930
‚Äì

SEG ‚Üë
‚Äì
‚Äì
‚Äì

DET ‚Üë
‚Äì
‚Äì
‚Äì

0.671¬±0.008 0.850¬±0.007
0.714¬±0.018 0.877¬±0.008 0.611¬±0.006 0.802¬±0.020 0.552¬±0.020 0.663¬±0.260 0.721¬±0.120 0.903¬±0.006
0.870¬±0.002 0.984¬±0.001 0.741¬±0.004 0.976¬±0.001 0.806¬±0.005 0.939¬±0.005 0.774¬±0.004 0.986¬±0.001
0.883¬±0.003 0.987¬±0.001 0.721¬±0.008 0.975¬±0.003 0.802¬±0.005 0.953¬±0.004 0.799¬±0.019 0.966¬±0.015
0.743¬±0.013 0.879¬±0.018 0.693¬±0.005 0.919¬±0.001 0.840¬±0.003 0.954¬±0.001 0.826¬±0.003 0.995¬±0.001

SEG ‚Üë
0.865
0.679

DET ‚Üë
0.989
0.814

U-Net [6]

0.5-Th
Otsu

0.763¬±0.007 0.956¬±0.002 0.748¬±0.002 0.954¬±0.001 0.852¬±0.001 0.951¬±0.001 0.784¬±0.009 0.967¬±0.003
0.758¬±0.007 0.955¬±0.001 0.744¬±0.003 0.950¬±0.001 0.852¬±0.001 0.950¬±0.002 0.780¬±0.009 0.966¬±0.003
Watershed 0.872¬±0.002 0.980¬±0.001 0.761¬±0.004 0.972¬±0.001 0.841¬±0.005 0.957¬±0.002 0.826¬±0.005 0.994¬±0.001
DenseCRF 0.701¬±0.004 0.941¬±0.001 0.725¬±0.005 0.942¬±0.001 0.852¬±0.001 0.952¬±0.001 0.772¬±0.009 0.959¬±0.003
0.760¬±0.007 0.955¬±0.001 0.746¬±0.001 0.951¬±0.001 0.852¬±0.001 0.951¬±0.001 0.760¬±0.009 0.961¬±0.003
MaxValue
H-EMD 0.872¬±0.004 0.982¬±0.001 0.763¬±0.001 0.971¬±0.005 0.853¬±0.001 0.955¬±0.001 0.829¬±0.004 0.996¬±0.001

TABLE VI: Ablation study of the EMD matching.

image segmentation.

F1 (%) ‚Üë

AJI (%)

w/ EMD w/o EMD w/ EMD w/o EMD
93.5¬±0.2 93.5¬±0.2 89.4¬±0.4 89.4¬±0.4
PhC-C2DH-U373
Fluo-N2DH-SIM+ 97.6¬±0.1 97.6¬±0.1 84.3¬±0.5 84.3¬±0.5
94.4¬±0.3 94.3¬±0.4 82.1¬±0.4 82.0¬±0.4
97.1¬±0.6 97.0¬±0.6 89.3¬±0.4 89.2¬±0.5

P. aeruginosa
M. xanthus

TABLE VII: Comparison of our H-EMD matching based
selection method and the NMS selection method.

F1 (%) ‚Üë

AJI (%)

NMS

H-EMD
93.5¬±0.2 93.4¬±0.1 89.4¬±0.4 89.0¬±0.3
PhC-C2DH-U373
Fluo-N2DH-SIM+ 97.6¬±0.1 96.0¬±0.2 84.3¬±0.5 80.9¬±0.9
94.4¬±0.3 94.0¬±0.4 82.1¬±0.4 81.5¬±0.5
97.1¬±0.6 96.5¬±0.2 89.3¬±0.4 88.4¬±0.4

P. aeruginosa
M. xanthus

H-EMD

NMS

pixels together to form the Ô¨Ånal instances. We conduct
experiments on the datasets using the model implemented
by the authors of [7], which show that PixelEmbedding
yields just acceptable results on the Fluo-N3DH-CHO
and Fluo-N2DH-SIM+ datasets.

‚Ä¢ Mask R-CNN [47]: A top-down segmentation approach.
It Ô¨Årst detects instances and then segments instance
masks.

‚Ä¢ StarDist [35]: It encodes cell instances using star-convex
polygons. For each pixel, it predicts an n-dimensional
vector which indicates the distance to the instance bound-
aries along a set of n predeÔ¨Åned radial directions with
equidistant angles. The Ô¨Ånal instances are obtained by
non-maximum suppression (NMS).

‚Ä¢ Cellpose [12]: It is a generalist cell segmentation model
for various types of datasets without re-training. The
model predicts instance gradient maps and then groups
pixels via a post-processing method to generate cells.
‚Ä¢ KIT-Sch-GE [18]: It ranks 1st (in all the metrics) in the
Cell Segmentation Benchmark on the PhC-C2DL-PSC
dataset. The proposed model predicts cell distance maps.
Watershed is then applied to the distance maps to obtain
the Ô¨Ånal instance segmentation results.

‚Ä¢ nnU-Net [48]: It ranks 1st (in the OPCSB and SEG met-
rics) in the Cell Segmentation Benchmark on the Fluo-
N2DH-SIM+ dataset. It is an automatic self-conÔ¨Ågured
UNet-based model including pre-processing, network ar-
chitecture, training, and post-processing for biomedical

We evaluate the performances of these methods using two

widely-used metrics for image instance segmentation [27]:

‚Ä¢ Average Jaccard Index (AJI): An instance segmentation
metric which considers an aggregated intersection over
an aggregated union for all ground truth and segmented
instances. Let G = {g1, g2, . . . , gn} denote a set of
ground truth instances, S = {s1, s2, . . . , sm} denote
a set of segmented instances, and N denote a set of
segmented instances that have no intersection with any
ground truth instances. AJI =
,

i=1 gi‚à©sj

(cid:80)n

(cid:80)n
i=1 gi‚à™sj +(cid:80)

sk ‚ààN sk

‚Ä¢ F1-score: An

.

k

where j = arg max

gi‚à©sk
gi‚à™sk
detection metric. F1 =
instance
2‚àóP recision‚àóRecall
P recision+Recall , where P recision =
TP+FP and
Recall = TP
TP+FN . Basically, a predicted instance is true
positive (TP) if it matches with a ground truth instance.
Two instances match if and only if they have IoU ‚â•
0.5. False positives (FP) are predicted instances that
are not true positives, while false negatives (FN) are
ground truth instances that are not matched with any
true positive instances.

TP

For Hybrid, KTH-SE, and SPDA, we are able to use the two
metrics on the datasets which these methods were designed
for and applied to. For Cellpose, we are able to apply the
pre-trained model to all the datasets with these two metrics.
Cellpose yields reasonable results only on the datasets which
do not appear to have a large domain gap with the training
data. For the other methods, we conduct experiments on all
the datasets using the two metrics (running over Ô¨Åve times to
attain the mean and standard deviation). For PixelEmbedding,
KIT-Sch-GE, and nnU-Net, we use the default parameters in
the applications. For Mask R-CNN, we modify the original
implementation by using smaller anchor boxes in order to
make it more suitable for biomedical cell segmentation. For
StarDist, for elongated shape cells, we slightly increase the
number of rays for the cell representation to achieve the best
possible performance.

3) Main Experimental Results:
Video Results: Table II

instance segmentation
comparison results on the six video datasets: Fluo-N2DL-
HeLa, PhC-C2DL-PSC, PhC-C2DH-U373, Fluo-N2DH-
SIM+, P. aeruginosa, and M. xanthus. First of all, working

shows

AUTHOR et al.: PREPARATION OF PAPERS FOR IEEE TRANSACTIONS ON MEDICAL IMAGING

13

with the three common DL semantic segmentation models,
our H-EMD consistently improves the F1 score and AJI
on most datasets. Compared to all
the post-processing
methods, H-EMD obtains performance improvements on all
the six datasets. More speciÔ¨Åcally, compared to the basic
post-processing method (0.5-Threshold), H-EMD improves
the F1 score by up to 4% and AJI by up to 6.3% on the PhC-
C2DL-PSC dataset with the DCAN backbone. Compared to
the best results of the other post-processing methods, H-EMD
still improves the F1 score by up to 1.8% and AJI by up
to 1.7% on the M. xanthus dataset with the UNet-LSTM
backbone. The improvements over
these post-processing
methods indicate that considerable instance segmentation
errors can come from the post-processing step on probability
maps, and H-EMD can reduce such instance segmentation
errors noticeably. We also notice that even though the
UNet-LSTM model has already utilized temporal information
in the videos, H-EMD can further explore temporal instance
information to improve instance segmentation effectively.

In addition, by incorporating with the three common DL
semantic segmentation models, our H-EMD outperforms the
state-of-the-art (SOTA) instance segmentation methods (e.g.,
with a U-Net backbone) on most the datasets. In comparison,
note that each of the SOTA methods can obtain the best results
on at most one dataset. Fig. 5 showcases some visual results
on video instance segmentation. One can see that for instances
that are over-segmented or under-segmented, H-EMD can
attain correct instance-level segmentation results. Furthermore,
H-EMD obtains more accurate instance boundaries.

Cell Segmentation Benchmark: We submitted our experi-
mental results on four public 2D+time video datasets (Fluo-
N2DL-HeLa, PhC-C2DL-PSC, PhC-C2DH-U373, and Fluo-
N2DH-SIM+) of various dynamic cells to the Cell Segmen-
tation Benchmark from the Cell Tracking Challenge. In the
setting of this challenge, for each dataset, both its two training
videos provided by the challenge organizers are used as the
training set, and its two challenge videos are used as the test
set. The challenge organizers conduct inference using the sub-
mitted methods on the challenge videos, perform evaluation,
and rank the segmentation results of all the submitted methods.
The challenge uses SEG, DET, and OPCSB as evaluation
metrics. SEG shows how well the segmented cell regions
match the actual cell or nucleus boundaries (evaluated using
cell segmentation masks). DET shows how accurately each
target object is detected (evaluated using cell markers, which
does not necessarily consider cell boundaries). OPCSB is the
average of the DET and SEG measures for direct comparisons
of all the submitted methods. We use DCAN as the backbone
with H-EMD on all the four datasets.

Table III shows the comparison results of the four datasets
on the leader board. One can see that in general, different
methods favor different datasets, that is, no method obtains
dominating performances on all the four datasets. Yet, our H-
EMD achieves runner-ups in three out of the four datasets
(PhC-C2DL-PSC, PhC-C2DH-U373, and Fluo-N2DH-SIM+)
in the ranking OPCSB metric. In the SEG metric, H-EMD
attains the best performance on the PhC-C2DH-U373 dataset,
ranks in 2nd on the Fluo-N2DH-SIM+ dataset, and ranks in

3rd on the PhC-C2DL-PSC dataset. In summary, the com-
pelling benchmarking performances validate the effectiveness
of our H-EMD on the 2D+time video datasets.

3D Results: We also conduct experiments on two 3D stack
datasets with 2D slices, the Fluo-N3DH-CHO and Fungus
datasets. To explore spatial instance consistency, we conduct
the experiments in the same setting as for 2D+time
all
temporal datasets and compared to the same methods as in
Table II. Table IV shows the instance segmentation comparison
results on the two 3D datasets. One can see that H-EMD
instance segmentation performances
can consistently boost
using probability maps. Compared to the best scores of the
known post-processing methods on the DCAN model, H-
EMD improves the F1 score by 1.2% and AJI by 0.8%
on the Fluo-N3DH-CHO dataset, and H-EMD improves the
F1 score by 0.2% and AJI by 1.1% on the Fungus dataset.
Compared to the other state-of-the-art instance segmentation
methods, H-EMD also shows considerable superiority working
with various DL semantic segmentation models. On the Fluo-
N3DH-CHO dataset, compared to the other best methods
(e.g., KIT-Sch-GE), H-EMD improves the F1 score by 0.8%
and AJI by 2.6% with the U-Net backbone. On the Fungus
dataset, compared to the other best methods (e.g., KIT-Sch-
GE), H-EMD improves the F1 score by 2.6% and AJI by
4.5% with the U-Net backbone. Fig. 6 shows some visual 2D
slice examples of instance segmentation results on the two
3D datasets. Compared to the watershed method that tends to
incur over-segmentation results (as well as under-segmentation
instances), our H-EMD can yield better instance results.

SigniÔ¨Åcance: We highlight several advantages of our H-

EMD method as follows.

‚Ä¢ DL semantic segmentation based: Our method generates
instance candidates (for possible segmentation) directly
from DL-generated probability maps, thus exploiting the
advantages of the generalizability of FCN pixel-wise
classiÔ¨Åcation models and attaining state-of-the-art perfor-
mances on various datasets.

‚Ä¢ Incorporating temporal/spatial instance consistency: H-
EMD provides a new way to effectively incorporate tem-
poral/spatial instance consistency to instance candidate
masks. Our proposed matching model is relatively simple
and does not need to rely on data-speciÔ¨Åc parameters.
‚Ä¢ Matching as an auxiliary task: Most previous work con-
sidered segmentation jointly with matching. With pos-
sible instance candidates, previous work often designed
complete but complicated tracking schemes. However,
tracking may not be strictly needed for each instance
candidate.
In our model, matching acts only as an
auxiliary task for instance segmentation, and thus the
temporal/spatial consistency property is not overused.

4) Additional Experiments:
Gold Truth (GoT) Evaluation: In our main experiments, we
use silver truth (ST) which supplies computer-aided generation
of full instance mask labels for evaluating the public datasets
(except for the simulated dataset Fluo-N2DH-SIM+, which
contains full ground truth). Here, we further evaluate instance
segmentation results using sparse human-made gold truth
(GoT). Since GoT provides very sparse labels, many predicted

14

IEEE TRANSACTIONS ON MEDICAL IMAGING, VOL. XX, NO. XX, XXXX 2020

instances may not have corresponding ground truth instances
and thus can be mistakenly taken as false positives (FP). We
use the SEG and DET metrics from the cell segmentation
challenge, which give small or no penalty to the FP cases.
Note that for the DET metric, it does not have requirements
on the accuracy of instance boundaries. Table V shows such
instance segmentation comparison results. First, we Ô¨Ånd that
our method can consistently obtain better SEG performances
compared to the most competitive post-processing methods,
which suggests that our method can yield better instance seg-
mentation masks. Second, compared to the other methods, our
method can consistently attain good results (top 3 on almost
all the datasets in all the metrics). Other SOTA methods may
yield the best results on one dataset, but can give relatively
worse results on the other datasets.

InÔ¨Çuence of the Pre-speciÔ¨Åed Value œÑ : In our instance can-
didate generation stage, to reduce noisy instance candidates,
we use only threshold values that are not smaller than a
pre-speciÔ¨Åed value œÑ to generate instance candidates (see
Section III-A). To examine the inÔ¨Çuence of the œÑ value on the
Ô¨Ånal instance segmentation results, we change œÑ from 0.10 to
0.90 with a 0.05 step and evaluate the corresponding instance
segmentation results. We conduct experiments on three video
datasets (PhC-C2DH-U373, P. aeruginosa, and M. xanthus)
with the U-Net backbone (see Fig. 7). First, one can see that
different œÑ values indeed inÔ¨Çuence the Ô¨Ånal instance segmenta-
tion results, and œÑ = 0.50 is in general a good choice. Second,
the performance
we Ô¨Ånd that on most of these datasets,
of our method does not change much in a relatively large
range of œÑ (e.g., œÑ = [0.40, 0.60]), which demonstrates the
stability of our method. Third, we notice that the P. aeruginosa
dataset incurs a large performance decay as the value of œÑ
is increased from 0.45 to larger, while PhC-C2DH-U373 has
relatively small change. This shows that different datasets can
have different instance candidate attributes. The P. aeruginosa
instance candidates form more different levels in the instance
candidate forest (ICF) compared to the PhC-C2DH-U373
dataset, and thus it is more challenging to identify correct
instances from such probability maps. In such cases, it is
harder for common thresholding methods to attain correct
instances, while our method is more Ô¨Çexible to adaptively
utilize instance-dependent threshold values effectively.

InÔ¨Çuence of the EMD Instance-instance Matching: In our
iterative matching and selection phase of the instance candi-
date selection stage, we Ô¨Årst apply an EMD matching model
to identify instance-instance matched pairs that should not
be involved in the next H-EMD matching and selection of
instance candidates (see Section III-B.2). Note that the EMD
instance-instance matching is performed to ensure correction
(e.g., avoiding one selected instance candidate in Sw
to
t
form multiple matched pairs with different selected instance
candidates in Sw+1
). Thus, the EMD matching may
also affect the performance of the Ô¨Ånal instance candidate
selection. We examine the contribution of the EMD matching
by removing the EMD matching step from our iterative
matching and selection phase. Table VI shows the comparison
results. One can see that without the EMD matching, the
instance candidate segmentation performances on the P. aerug-

‚à™ F w+1
t

t

inosa and M. xanthus datasets decrease slightly by ‚àº0.1%,
which demonstrates that EMD matching indeed reduces some
redundant matching which can cause errors in the subsequent
H-EMD matching.

Effect of the H-EMD Matching Model: : In the instance can-
didate selection stage, our H-EMD matching model selects in-
stance candidates from the ICF. To evaluate the effect of our H-
EMD matching model, we compare it with a commonly-used
selection method:
the Non-Maximum Suppression (NMS)
algorithm. NMS is widely-used in many instance segmenta-
tion frameworks [47], [60], which selects objects from given
proposals based on the corresponding objectness scores using
a greedy strategy. Table VII shows the comparison results
with the U-Net backbone. H-EMD consistently outperforms
NMS in accuracy, demonstrating the effectiveness of our H-
EMD matching model, which is an optimization approach
with guaranteed optimal matching solutions. Note that in our
experiments, the number of variables in the ILP model is
relatively small, which gives rise to comparable execution time
as the NMS method.

InÔ¨Çuence of the Number of Matching-and-selection Iterations:
In our instance candidate selection stage, we employ an
iterative matching and selection process to select instance can-
didates. We examine the inÔ¨Çuence of the number of matching
iterations on the Ô¨Ånal instance segmentation results. Fig. 8
shows the comparison results. One can see that our iterative
matching process converges fast for most of the datasets.
In particular, the performances generally do not show large
changes after the 4th iteration. We further notice that the
Ô¨Årst two iterations give large performance gains, and as the
iteration number increases, the improvement decays. It shows
that our matching method can effectively incorporate instance
consistency to select accurate instance candidates. As the
iteration number increases, less remaining instance candidates
can be matched, and thus less improvement gain is obtained. In
general, we choose the number of matching iterations T = 10.

V. CONCLUSIONS

In this paper, we proposed a novel framework, H-EMD,
for instance segmentation in biomedical 2D+time videos and
3D images. H-EMD builds a forest structure of possible
instance candidates from DL semantic-segmentation-generated
probability maps. To effectively select instance candidates, H-
EMD Ô¨Årst selects easy-to-identify instance candidates, and
then propagates the selected instance candidates to match
with other candidates in neighboring frames in an iterative
matching process. Evaluated on six 2D+time video datasets
and two 3D datasets, our H-EMD can consistently improve
instance segmentation performances compared to widely-used
post-processing methods on probability maps. Incorporating
with common DL semantic segmentation models, H-EMD is
highly competitive with state-of-the-art instance segmentation
methods as well. Experimental results validated the effective-
ness of H-EMD to incorporate instance consistency on top of
DL semantic segmentation models.

Our future work will study three main issues. First, in the
current 3D applications, we view 3D images in a 2D+depth

AUTHOR et al.: PREPARATION OF PAPERS FOR IEEE TRANSACTIONS ON MEDICAL IMAGING

15

manner, which incurs some limitations in directly exploring
the whole 3D instance structures (possibly losing some con-
textual information of 3D objects). Thus, incorporating full
3D information in 3D instance segmentation with H-EMD
is an interesting future research target. Second, the current
implementation of our method takes a few hours (1-2 hours
in general) for processing stacks of images in a test set, while
the other methods take only a few minutes (less than 10
minutes in general). In future studies, it could be a useful
target to speed up the H-EMD implementation using parallel
computation with optimized efÔ¨Åciency. Third, in the instance
candidate selection stage, our current matching method uses a
forward-backward matching mechanism. Thus, a whole video
or sequence of images has to be acquired before the images can
be processed, which does not accommodate online tracking
and segmentation applications. A new version of H-EMD for
online applications should be developed in future studies.

REFERENCES

[1] S. S. Lienkamp, K. Liu, C. M. Karner, T. J. Carroll, O. Ronneberger, J. B.
Wallingford, and G. Walz, ‚ÄúVertebrate kidney tubules elongate using a
planar cell polarity‚Äìdependent, rosette-based mechanism of convergent
extension,‚Äù Nature Genetics, vol. 44, no. 12, p. 1382, 2012.

[2] M. N. Gurcan, L. E. Boucheron, A. Can, A. Madabhushi, N. M.
Rajpoot, and B. Yener, ‚ÄúHistopathological image analysis: A review,‚Äù
IEEE Reviews in Biomedical Engineering, vol. 2, pp. 147‚Äì171, 2009.

[3] H. Chen, X. Qi, L. Yu, and P.-A. Heng, ‚ÄúDCAN: Deep contour-aware
networks for accurate gland segmentation,‚Äù in IEEE Conference on
Computer Vision and Pattern Recognition, 2016, pp. 2487‚Äì2496.
[4] S. Graham, H. Chen, J. Gamper, Q. Dou, P.-A. Heng, D. Snead, Y. W.
Tsang, and N. Rajpoot, ‚ÄúMILD-Net: Minimal information loss dilated
network for gland instance segmentation in colon histology images,‚Äù
Medical Image Analysis, vol. 52, pp. 199‚Äì211, 2019.

[5] S. Mishra, Y. Zhang, D. Z. Chen, and X. S. Hu, ‚ÄúData-driven deep
supervision for medical image segmentation,‚Äù IEEE Transactions on
Medical Imaging, 2022, DOI: 10.1109/TMI.2022.3143371.

[6] O. Ronneberger, P. Fischer, and T. Brox, ‚ÄúU-Net: Convolutional net-
works for biomedical image segmentation,‚Äù in International Conference
on Medical Image Computing and Computer-assisted Intervention, 2015,
pp. 234‚Äì241.

[7] L. Chen, M. Strauch, and D. Merhof, ‚ÄúInstance segmentation of
biomedical images with an object-aware embedding learned with local
constraints,‚Äù in International Conference on Medical Image Computing
and Computer-Assisted Intervention, 2019, pp. 451‚Äì459.

[8] P. Voigtlaender, Y. Chai, F. Schroff, H. Adam, B. Leibe, and L.-C.
Chen, ‚ÄúFEELVOS: Fast end-to-end embedding learning for video object
segmentation,‚Äù in IEEE Conference on Computer Vision and Pattern
Recognition, 2019, pp. 9481‚Äì9490.

[9] V. Kulikov and V. Lempitsky, ‚ÄúInstance segmentation of biological
images using harmonic embeddings,‚Äù in IEEE Conf. on Computer Vision
and Pattern Recognition, 2020, pp. 3843‚Äì3851.

[10] C. Payer, D. ÀáStern, M. Feiner, H. Bischof, and M. Urschler, ‚ÄúSegmenting
and tracking cell instances with cosine embeddings and recurrent hour-
glass networks,‚Äù Medical Image Analysis, vol. 57, pp. 106‚Äì119, 2019.
[11] D. Eschweiler, T. V. Spina, R. C. Choudhury, E. Meyerowitz, A. Cunha,
and J. Stegmaier, ‚ÄúCNN-based preprocessing to optimize watershed-
based cell segmentation in 3D confocal microscopy images,‚Äù in IEEE
16th International Symp. on Biomedical Imaging, 2019, pp. 223‚Äì227.

[12] C. Stringer, T. Wang, M. Michaelos, and M. Pachitariu, ‚ÄúCellpose: A
generalist algorithm for cellular segmentation,‚Äù Nature Methods, vol. 18,
no. 1, pp. 100‚Äì106, 2021.

[13] F. A. G. PeÀúna, P. D. M. Fernandez, P. T. Tarr, T. I. Ren, E. M.
Meyerowitz, and A. Cunha, ‚ÄúJ regularization improves imbalanced
multiclass segmentation,‚Äù in IEEE 17th International Symposium on
Biomedical Imaging, 2020, pp. 1‚Äì5.

[14] F. Meyer, ‚ÄúTopographic distance and watershed lines,‚Äù Signal Process-

ing, vol. 38, no. 1, pp. 113‚Äì125, 1994.

[15] F. Lux and P. Matula, ‚ÄúDIC image segmentation of dense cell pop-
ulations by combining deep learning and watershed,‚Äù in IEEE 16th
International Symposium on Biomedical Imaging, 2019, pp. 236‚Äì239.

[16] P. Naylor, M. La¬¥e, F. Reyal, and T. Walter, ‚ÄúSegmentation of nuclei in
histopathology images by deep regression of the distance map,‚Äù IEEE
Transactions on Medical Imaging, vol. 38, no. 2, pp. 448‚Äì459, 2018.

[17] S. Graham, Q. D. Vu, S. E. A. Raza, A. Azam, Y. W. Tsang, J. T.
Kwak, and N. Rajpoot, ‚ÄúHover-Net: Simultaneous segmentation and
classiÔ¨Åcation of nuclei in multi-tissue histology images,‚Äù Medical Image
Analysis, vol. 58, p. 101563, 2019.

[18] T. Scherr, K. L¬®ofÔ¨Çer, O. Neumann, and R. Mikut, ‚ÄúOn improving
an already competitive segmentation algorithm for the cell tracking
challenge-lessons learned,‚Äù bioRxiv, 2021.

[19] A. Arbelle and T. R. Raviv, ‚ÄúMicroscopy cell segmentation via convo-
lutional LSTM networks,‚Äù in International Symposium on Biomedical
Imaging, 2019, pp. 1008‚Äì1012.

[20] C. Payer, D. ÀáStern, T. Neff, H. Bischof, and M. Urschler, ‚ÄúInstance seg-
mentation and tracking with cosine embeddings and recurrent hourglass
networks,‚Äù in International Conference on Medical Image Computing
and Computer-Assisted Intervention, 2018, pp. 3‚Äì11.

[21] S. U. Akram, J. Kannala, L. Eklund, and J. Heikkil¬®a, ‚Äúcell segmentation
and tracking using cell proposals,‚Äù in IEEE 13th International Sympo-
sium on Biomedical Imaging, 2016, pp. 920‚Äì924.

[22] ‚ÄúCell segmentation leaderboard,‚Äù http://celltrackingchallenge.net/latest-

csb-results/, access time: 2021.12.

[23] M. Schiegg, P. Hanslovsky, C. Haubold, U. Koethe, L. Hufnagel, and
F. A. Hamprecht, ‚ÄúGraphical model for joint segmentation and tracking
of multiple dividing cells,‚Äù Bioinformatics, vol. 31, no. 6, pp. 948‚Äì956,
2015.

[24] E. T¬®uretken, X. Wang, C. J. Becker, C. Haubold, and P. Fua, ‚ÄúNet-
work Ô¨Çow integer programming to track elliptical cells in time-lapse
sequences,‚Äù IEEE Transactions on Medical Imaging, vol. 36, no. 4, pp.
942‚Äì951, 2016.

[25] K. E. Magnusson, J. Jald¬¥en, P. M. Gilbert, and H. M. Blau, ‚ÄúGlobal
linking of cell tracks using the Viterbi algorithm,‚Äù IEEE Transactions
on Medical Imaging, vol. 34, no. 4, pp. 911‚Äì929, 2014.

[26] Y. Rubner, C. Tomasi, and L. J. Guibas, ‚ÄúA metric for distributions with
applications to image databases,‚Äù in 6th International Conference on
Computer Vision, 1998, pp. 59‚Äì66.

[27] Y. Zhou, O. F. Onder, Q. Dou, E. Tsougenis, H. Chen, and P.-A. Heng,
‚ÄúCIA-Net: Robust nuclei
instance segmentation with contour-aware
information aggregation,‚Äù in International Conference on Information
Processing in Medical Imaging, 2019, pp. 682‚Äì693.

[28] Q. Kang, Q. Lao, and T. Fevens, ‚ÄúNuclei segmentation in histopatholog-
ical images using two-stage learning,‚Äù in International Conference on
Medical Image Computing and Computer-Assisted Intervention, 2019,
pp. 703‚Äì711.

[29] M. Zhao, A. Jha, Q. Liu, B. A. Millis, A. Mahadevan-Jansen, L. Lu,
B. A. Landman, M. J. Tyska, and Y. Huo, ‚ÄúFaster Mean-shift: GPU-
accelerated clustering for cosine embedding-based cell segmentation and
tracking,‚Äù Medical Image Analysis, vol. 71, p. 102048, 2021.

[30] D. Comaniciu and P. Meer, ‚ÄúMean shift: A robust approach toward
feature space analysis,‚Äù IEEE Transactions on Pattern Analysis and
Machine Intelligence, vol. 24, no. 5, pp. 603‚Äì619, 2002.

[31] M. Couprie, L. Najman, and G. Bertrand, ‚ÄúQuasi-linear algorithms for
the topological watershed,‚Äù Journal of Mathematical Imaging and Vision,
vol. 22, no. 2, pp. 231‚Äì249, 2005.

[32] V. Iglovikov, S. S. Seferbekov, A. Buslaev, and A. Shvets, ‚ÄúTernaus-
NetV2: Fully convolutional network for instance segmentation,‚Äù in
CVPR Workshops, vol. 233, 2018, p. 237.

[33] R. Beare and G. Lehmann, ‚ÄúThe watershed transform in ITK-discussion

and new developments,‚Äù The Insight Journal, vol. 1, pp. 1‚Äì24, 2006.

[34] M. Bai and R. Urtasun, ‚ÄúDeep watershed transform for instance
segmentation,‚Äù in IEEE Conference on Computer Vision and Pattern
Recognition, 2017, pp. 5221‚Äì5229.

[35] U. Schmidt, M. Weigert, C. Broaddus, and G. Myers, ‚ÄúCell detection
with star-convex polygons,‚Äù in International Conf. on Medical Image
Computing and Computer-Assisted Intervention, 2018, pp. 265‚Äì273.

[36] B. Romera-Paredes and P. H. S. Torr, ‚ÄúRecurrent instance segmentation,‚Äù
in European Conference on Computer Vision. Springer, 2016, pp. 312‚Äì
329.

[37] N. Silberman, D. Sontag, and R. Fergus, ‚ÄúInstance segmentation of
indoor scenes using a coverage loss,‚Äù in European Conference on
Computer Vision, 2014, pp. 616‚Äì631.

[38] J. Funke, C. Zhang, T. Pietzsch, M. A. G. Ballester, and S. Saalfeld,
‚ÄúThe candidate multi-cut for cell segmentation,‚Äù in IEEE International
Symposium on Biomedical Imaging, 2018, pp. 649‚Äì653.

[39] H. Fehri, A. Gooya, Y. Lu, E. Meijering, S. A. Johnston, and A. F.
Frangi, ‚ÄúBayesian polytrees with learned deep features for multi-class

16

IEEE TRANSACTIONS ON MEDICAL IMAGING, VOL. XX, NO. XX, XXXX 2020

cell segmentation,‚Äù IEEE Transactions on Image Processing, vol. 28,
no. 7, pp. 3246‚Äì3260, 2019.

[40] S. U. Akram, J. Kannala, M. Kaakinen, L. Eklund, and J. Heikkil¬®a, ‚ÄúSeg-
mentation of cells from spinning disk confocal images using a multi-
stage approach,‚Äù in Asian Conference on Computer Vision.
Springer,
2014, pp. 300‚Äì314.

[41] R. Souza, L. Tavares, L. Rittner, and R. Lotufo, ‚ÄúAn overview of max-
tree principles, algorithms and applications,‚Äù in 2016 29th SIBGRAPI
Conference on Graphics, Patterns and Images Tutorials (SIBGRAPI-T).
IEEE, 2016, pp. 15‚Äì23.

[42] R. Jones, ‚ÄúConnected Ô¨Åltering and segmentation using component trees,‚Äù
Computer Vision and Image Understanding, vol. 75, no. 3, pp. 215‚Äì228,
1999.

[43] E. Carlinet and T. G¬¥eraud, ‚ÄúA comparative review of component
tree computation algorithms,‚Äù IEEE Transactions on Image Processing,
vol. 23, no. 9, pp. 3885‚Äì3895, 2014.

[44] J. Chen, M. Alber, and D. Z. Chen, ‚ÄúA hybrid approach for segmentation
and tracking of Myxococcus xanthus swarms,‚Äù IEEE Transactions on
Medical Imaging, vol. 35, no. 9, pp. 2074‚Äì2084, 2016.

[45] J. Chen, F. Shen, D. Z. Chen, and P. J. Flynn, ‚ÄúIris recognition based
on human-interpretable features,‚Äù IEEE Transactions on Information
Forensics and Security, vol. 11, no. 7, pp. 1476‚Äì1485, 2016.

[46] V. Ulman, M. MaÀáska, K. E. Magnusson, O. Ronneberger, C. Haubold,
N. Harder, P. Matula, P. Matula, D. Svoboda, M. Radojevic et al.,
‚ÄúAn objective comparison of cell-tracking algorithms,‚Äù Nature Methods,
vol. 14, no. 12, pp. 1141‚Äì1152, 2017.

[47] K. He, G. Gkioxari, P. Doll¬¥ar, and R. Girshick, ‚ÄúMask R-CNN,‚Äù in IEEE

International Conference on Computer Vision, 2017, pp. 2961‚Äì2969.

[48] F. Isensee, P. F. Jaeger, S. A. Kohl, J. Petersen, and K. H. Maier-
Hein, ‚ÄúnnU-Net: A self-conÔ¨Åguring method for deep learning-based
biomedical image segmentation,‚Äù Nature Methods, vol. 18, no. 2, pp.
203‚Äì211, 2021.

[49] N. M. Al-Shakarji, F. Bunyak, G. Seetharaman, and K. Palaniappan,
‚ÄúMulti-object tracking cascade with multi-step data association and oc-
clusion handling,‚Äù in 15th IEEE International Conference on Advanced
Video and Signal Based Surveillance (AVSS).

IEEE, 2018, pp. 1‚Äì6.

[50] Y. Zhu and E. Meijering, ‚ÄúAutomatic improvement of deep learning-
based cell segmentation in time-lapse microscopy by neural architecture
search,‚Äù Bioinformatics, vol. 37, no. 24, pp. 4844‚Äì4850, 2021.

[51] N. Koerber, ‚ÄúMIA: An open source standalone deep learning application

for microscopic image analysis,‚Äù bioRxiv, 2022.

[52] Y. Zhang, L. Yang, H. Zheng, P. Liang, C. Mangold, R. G. Loreto, D. P.
Hughes, and D. Z. Chen, ‚ÄúSPDA: Superpixel-based data augmentation
for biomedical image segmentation,‚Äù in International Conference on
Medical Imaging with Deep Learning, 2019, pp. 572‚Äì587.

[53] M. MaÀáska, V. Ulman, D. Svoboda, P. Matula, P. Matula, C. Ederra,
A. Urbiola, T. EspaÀúna, S. Venkatesan, D. M. Balak et al., ‚ÄúA benchmark
for comparison of cell tracking algorithms,‚Äù Bioinformatics, vol. 30,
no. 11, pp. 1609‚Äì1617, 2014.

[54] J. Chen, Y. Cai, C. Wei, L. Yang, M. Alber, and D. Z. Chen, ‚ÄúSeg-
mentation and tracking of Pseudomonas aeruginosa for cell dynamics
analysis in time-lapse images,‚Äù in IEEE 13th International Symposium
on Biomedical Imaging, 2016, pp. 968‚Äì971.

[55] M. Fredericksen, Y. Zhang, M. Hazen, R. Loreto, C. Mangold, D. Z.
Chen, and D. Hughes, ‚ÄúThree-dimensional visualization and a deep-
learning model reveal complex fungal parasite networks in behaviorally
manipulated ants,‚Äù Proceedings of the National Academy of Sciences of
the USA (PNAS), vol. 114, no. 47, pp. 2590‚Äì12 595, 2017.

[56] P. Liang, J. Chen, H. Zheng, L. Yang, Y. Zhang, and D. Z. Chen,
‚ÄúCascade decoder: A universal decoding method for biomedical image
segmentation,‚Äù in 2019 IEEE 16th International Symposium on Biomed-
ical Imaging (ISBI 2019).

IEEE, 2019, pp. 339‚Äì342.

[57] C. E. Akbas, V. Ulman, M. Maska, F. Jug, and M. Kozubek, ‚ÄúAutomatic
fusion of segmentation and tracking labels,‚Äù in ECCV Workshops (6),
2018, pp. 446‚Äì454.

[58] N. Otsu, ‚ÄúA threshold selection method from gray-level histograms,‚Äù
IEEE transactions on systems, man, and cybernetics, vol. 9, no. 1, pp.
62‚Äì66, 1979.

[59] K. Kamnitsas, C. Ledig, V. F. Newcombe, J. P. Simpson, A. D. Kane,
D. K. Menon, D. Rueckert, and B. Glocker, ‚ÄúEfÔ¨Åcient multi-scale 3D
CNN with fully connected CRF for accurate brain lesion segmentation,‚Äù
Medical Image Analysis, vol. 36, pp. 61‚Äì78, 2017.

[60] X. Wang, T. Kong, C. Shen, Y. Jiang, and L. Li, ‚ÄúSOLO: Segmenting
objects by locations,‚Äù in European Conference on Computer Vision.
Springer, 2020, pp. 649‚Äì665.

