1
2
0
2

v
o
N
9
2

]

R
A
.
s
c
[

1
v
7
6
7
4
1
.
1
1
1
2
:
v
i
X
r
a

A Graph Deep Learning Framework for High-Level
Synthesis Design Space Exploration

Lorenzo Ferretti1∗, Andrea Cini2∗, Georgios Zacharopoulos3, Cesare Alippi2,4, Laura Pozzi2
1University of California Los Angeles, 2Università della Svizzera italiana,
3 Hardvard University, 4 Politecnico di Milano

Abstract

The design of efﬁcient hardware accelerators for high-throughput data-processing
applications, e.g., deep neural networks, is a challenging task in computer architec-
ture design. In this regard, High-Level Synthesis (HLS) emerges as a solution for
fast prototyping application-speciﬁc hardware starting from a C/C++ behavioural
description of the application computational ﬂow. In the accelerator synthesis
phase, designers apply HLS directives to optimize the hardware implementation,
by trading-off cost and performance. This Design-Space Exploration (DSE) aims
at identifying Pareto optimal synthesis conﬁgurations whose exhaustive search
is often unfeasible due to the design-space dimensionality and the prohibitive
computational cost of the synthesis process. Within this framework, we effectively
and efﬁciently address the design problem by proposing, for the ﬁrst time in the
literature, graph neural networks that jointly predict acceleration performance
and hardware costs of a synthesized behavioral speciﬁcation given optimization
directives. The learned model can be used to rapidly approach the Pareto curve by
guiding the DSE, taking into account performance and cost estimates. The pro-
posed method outperforms traditional HLS-driven DSE approaches, by accounting
for arbitrary length of computer programs and the invariant properties of the input.
We propose a novel hybrid control and data ﬂow graph representation that enables
training the graph neural network on speciﬁcations of different hardware accelera-
tors; the methodology naturally transfers to unseen data-processing applications too.
Moreover, we show that our approach achieves prediction accuracy comparable
with that of commonly used simulators without having access to analytical models
of the HLS compiler and the target FPGA, while being orders of magnitude faster.
Finally, the learned representation can be exploited for DSE in unexplored conﬁg-
uration spaces by ﬁne-tuning on a small number of samples from the new target
domain. The outcome of the empirical evaluation of this transfer learning shows
strong results against state-of-the-art baselines in relevant benchmarks including
neural processing.

1

Introduction

In the last decade, specialised hardware has become a viable solution to address the end of Moors’s
Law [40] and the breakdown of Dennard scaling [9] and deal with the constant growth in performance
and efﬁciency requirements of computer systems. Whitin this context, short time to market, higher
design productivity and reusability of existing modules are only a subset of the challenges that have
driven the Electronic Design Automation (EDA) industry and research. In particular, the recent
wide-spread of solutions based on artiﬁcial intelligence and deep learning techniques [26, 41] has
lead to an increasing demand of hardware accelerators able to target a large variety of computational
workloads.

∗These two authors contributed equally. Corresponding author: ferrelo@cs.ucla.edu

Preprint. Under review.

 
 
 
 
 
 
Figure 1: Example of HLS design ﬂow. A behavioural description, synthesis directives, technology
library, and a target frequency are given to the HLS tool, which generates as a result an RTL design,
performance and cost reports, as well as the synthesis scripts.

Automating the design process by exploiting the predictive power of modern machine learning (ML)
models is an appealing approach that, while accelerating the development of computer architectures,
would also allow the ML community to beneﬁt from the improved computing platforms. In fact,
progress in one ﬁeld ripples through the other one, thus creating a positive feedback loop and a
virtuous cycle [10]. In this setting, graphs are natural candidates to capture the functionally-dependant
structure of software and hardware systems. It is not surprising, then, that the advent of graph neural
networks (GNNs) [37, 4, 1] led to impressive developments in computer-aided hardware and software
design: from chip placement [32] to compilers for ML computational graphs [56]. In this context, we
advocate for the adoption of analogous automation strategies to ﬁll the gap between the increasing
request for efﬁcient and effective hardware acceleration of existing applications and the hardware
design productivity.

Traditional Integrated Circuits (ICs) design methodologies rely on Hardware Description Lan-
guages (HDLs) to describe logical components and their interaction at a Register Transfer Level (RTL).
This approach requires designers to manually deﬁne the concurrent description of millions of tran-
sistors working in parallel to carry out the desired computations. To reduce the burden of this task,
High-Level Synthesis (HLS) comes into play. HLS tools enable to start the design process from
high-level behavioural speciﬁcations in C/C++/SystemC, hence avoiding designers to deal with the
tedious and error prone task of implementing the functionality at RTL level. Besides specifying
the desired behavior, as shown in Figure 1, designers can guide the synthesis process by applying
directives able to tune the resulting RTL implementation according to target performance and cost
requirements. Synthesis directives allow to specify how to implement in hardware speciﬁc software
constructs such as loops, arrays and functions. For example, the designer can use directives to tune the
degree of hardware parallelization of a loop by specifying a loop unrolling factor. While HLS allows
to explore a vast design space of micro-architectural variations by using different directives, resulting
performance and resource utilization of each implementation cannot be determined a priori. In fact,
exhaustive exploration involves time-consuming syntheses, whose number grows exponentially w.r.t.
the number of applied optimizations. Moreover, among all the possible conﬁgurations (i.e., combi-
nations of directives), only a few are Pareto-optimal from a performance and costs perspective. As
designers are interested in effective methodologies to automate the Design-Space Exploration (DSE)
process, the HLS-driven DSE problem consists in identifying as accurately as possible the set of
Pareto implementations while, at the same time, minimizing the number of synthesis runs.

Recent works demonstrated the possibility to guide DSEs by exploiting the notion of function
similarity and the knowledge acquired from past explorations performed on different functions [14,
24, 47] and have shown strong empirical results besides the small number of source domains. In
light of this, we claim that data-driven approaches are the way forward for HLS-driven DSE. In this
work, we introduce the methodological framework where we ﬁrmly believe that progress in the ﬁeld
is bound to happen. We propose both a data representation able to capture the critical elements of the
HLS process and the data-processing tools to proﬁt from such representation. At ﬁrst, we introduce a
novel graph representation of computer programs, based on an augmented hybrid control and data
ﬂow graph, capturing invariant properties and relevant information from the HLS perspective. Then,
we exploit this representation to train a novel graph neural network model in a supervised fashion, by
ﬁtting the model on a dataset of previously synthesized conﬁgurations (behavioral speciﬁcations plus
optimization directives) to predict the latency and resource utilization corresponding to each point. To

2

1 void sum_scan(int sum[RADIX], int bucket[BUCKETSIZE]){  2   int radixID, bucket_indx; 1  2 3   sum[0] = 0; 4 3 sum_1:for(radixID=1; radixID<RADIX; radixID++){ 5     bucket_indx = radixID*BLOCK - 1; 6     sum[radixID] = sum[radixID-1] + bucket[bucket_indx]; 7   } 8 }  High-level behavioural description (C/C++) of an HLS design.1 #pragma RESOURCE sum <resource> 2 #pragma PARTITIONING sum type=<t> factor=<f> 3 #pragma UNROLL sum_1 factor=<f>Synthesis directivesLibHLSTechnology libraryInputHigh-Level SynthesisOutputPerformance & costs estimatesExample of DSE outcomeSynthesis scriptsRTL designAreaLatencyPareto solutionsRESOURCE sum RAM_2P_BRAM PARTITIONING sum type=cyclic factor=40 UNROLL sum_1 factor=20 RESOURCE sum RAM_1P_BRAM PARTITIONING sum type=cyclic factor=1 UNROLL sum_1 factor=1 Target clock frequencythe best of our knowledge, this is the ﬁrst attempt to use graph representation learning from software
speciﬁcations to perform HLS-driven design space exploration. We show that the learned model can
be used for effective DSE after ﬁne-tuning on a small set of samples and that our method compares
favorably against state-of-the-art baselines on relevant benchmarks. We refer to our framework as
gnn4hls. We believe that the results achieved here constitute a strong signal for the community
that calls for a general effort in collecting large datasets of syntheses to unlock the full potential of
graph deep learning solutions to the HLS-driven DSE problem. In order to accellerate progress in
this direction, methods and datasets presented here together with a platform for data collection will
be open sourced at the end of the blind review process.

The rest of the paper is organized as follows. In Section 2 we deﬁne the problem by introducing the
main concepts and proper terminology for HLS-driven DSE and graph neural networks. Then, in
Section 3 we lay out the details of our approach. In Section 4 we evaluate the proposed method on
relevant benchmarks. Finally, we discuss the related works in Section 5, draw our conclusions and
discuss future works in Section 6.

2 Background

2.1 High-Level Synthesis driven Design-Space Exploration

Given a software functionality, e.g., the sum_scan function from the Radix Sort algorithm in Mach-
suite [36] (used as a running example in the rest of the document), we deﬁne as HLS design, or simply
design, the functionality to be realized in hardware, and as speciﬁcations the behavioural description
of the design in a high-level programming language such as C/C++. The speciﬁcation (SW ) is
given in input to the HLS tool together with the target technology library (LibHLS) and a target
frequency (F ). The result of the synthesis process is named implementation, and it is an automatically
generated RTL code, usually in VHDL or Verilog. The resulting RTL is coupled to a performance
metric (e.g., latency or throughput), and a cost metric (e.g., area or energy costs).

An implementation is generated by applying a set of directives – speciﬁed using compiler pragmas
– to the SW , affecting the resulting performance and costs. The set of directives affecting an
implementation is named conﬁguration. Each directive is associated to a target in the SW , which
can be either a label or a code construct (e.g., labeled statements, function names, variables, etc.).
In addition, a directive is characterized by its type and an associated value. Examples of directive
types are loop unrolling – affecting number of resources required to implement the loop body in
hardware and enabling its parallel execution – and array partitioning – splitting the input array
in multiple memory banks and enabling parallel access to the data. The directive value forces a given
directive type to a speciﬁc value. As an example, an unrolling factor of 2 doubles the logic required
to implement in hardware the loop body, enabling parallel hardware execution of two iterations.

The HLS design ﬂow and the different elements characterizing it are shown in Figure 1. Figure
1(left) shows the inputs of the HLS design ﬂow: a behavioural description of an HLS-design – the
sum_scan function in the Radix Sort benchmark from MachSuite [36] – an example of pragmas
applied to the array and loop constructs of the speciﬁcation, the technology library adopted, and the
target frequency. After being processed by the HLS tool, the resulting implementation is generated as
an RTL desing, with the synthesis scripts and the performance and cost reports–Figure 1(right).

Given a design D, a designer limits the set conﬁgurations to explore during a DSE by deﬁning a
conﬁguration space. The conﬁguration space XD is deﬁned as the Cartesian product among the set
of directive values Vi associated to each i-th directive, i.e., XD = V1 × V2 × ... × VN , where N is the
number of considered directives. The size of the conﬁguration space is given by its cardinality |XD|.
Given a conﬁguration space XD, a design space YD can be deﬁned as the set of implementations
resulting from the synthesis of the conﬁguration in XD.

Task formulation The DSE problem is a Multi-Objective Optimization Problem (MOOP) having
costs and merit as objective functions. In the context of hardware design common performance
measure and cost are latency, and area or power respectively. In this work we use as performance the
effective latency (LAT), i.e., the number of clock cycles required by the hardware implementation
to execute its functionality multiplied by the target clock of the system. For cost we consider the
percentage of resources (silicon) utilization required to implement the IC in hardware. In this work,
since our target architecture is a Field Programmable Gate Array (FPGA), costs are expressed in

3

terms of number of Flip-Flops (FF), Look-Up Tables (LUT), Digital Signal Processor (DSP), and
Block RAM (BRAM)2. In particular, the objective is to identify a subset PD of the conﬁguration
space XD such that: PD = {x|x ∈ XD and x is Pareto}. A Pareto conﬁguration (p) of a design D
is deﬁned as: p ∈ P ⇔ (cid:64) x ∈ XD, x (cid:54)= p | ax ≤ ap ∧ lx ≤ lp. With ap, ax, and lp, lx being the cost
(a) and merit (l) associated to the implementation of p and x respectively.

2.2 Graph neural networks

Input behavioural speciﬁcations (programs) signiﬁcantly differ in terms of size, structure, constructs,
and optimization directives. This variability is hardly captured by vector representations which
have been, we argue, one of the main limiting factors of previous works in attempting to learn
predictive models of the HLS process [30, 47]. Differently, when modeling the problem in the space
of graphs we can use methods that naturally exploit existing functional dependencies, account for the
variability in the structure of different speciﬁcations, and seamlessly transfer learned representations
across different conﬁguration spaces. Furthermore, graph-processing techniques permit to exploit the
properties of graph representations (e.g., permutation invariance) inducing positive inductive biases
that restrict the hypothesis space explored by the learning system to plausible models.

We consider attributed directed graphs, with attributes associated to both nodes and edges. In
particular, a graph G is a tuple (cid:104)V , E, u(cid:105), where V = {1, . . . , N } is a set of nodes, E = {(i, j)|i, j ∈
V } is a set of edges and u ∈ Ru is a global attribute vector. We denote by vi ∈ Rv the raw features
associated with node i ∈ V and with ei,j ∈ Re the attribute vector associated with edge (i, j) ∈ E
connecting nodes i and j.

Different works propose general frameworks to design GNNs: inspired by Gilmer et al. [18] and
Battaglia et al. [1], we consider a very general class of message-passing neural networks (MPNNs)
with global attributes where the t-th propagation layer (or step) can be written as
(cid:110)

(cid:16)

(cid:1) ; (j, i) ∈ E

(cid:111)
, ut−1(cid:17)

(1)

i = τ t
vt
v

ut = τ t
u

, AGGRv

vt−1
i
(cid:16)
ut−1, AGGRu

ψt
v
(cid:110)

ψt
u

(cid:0)vt−1

j

, ej,i

, vt−1
i
i , ut−1(cid:1) ; i ∈ V

(cid:0)vt

(cid:111)(cid:17)

,

(2)

v,τ t

u) and message (ψt

where the update (τ t
u) functions can be implemented by any differentiable
function, e.g., Multi-Layer Perceptrons (MLPs). Aggregation functions AGGRu{ · } and AGGRv{ · } can
be any permutation invariant operation. Multi-layer GNNs are built by stacking several propagation
layers, which allow to aggregate at each node messages from different neighborhoods.

v,ψt

3 Methods

3.1 Data representation

Control Flow Graphs (CFGs) are graphs representing the possible execution paths in a program.
The CFG is a directed graph GCFG deﬁned as the tuple (cid:104)VCFG, ECFG(cid:105), where VCFG is the the set of
nodes corresponding to the basic blocks of the program, and ECFG is the set of edges representing
the possible control ﬂows among basic blocks. An example of CFG for the sum_scan function
is shown in Figure 2-(top). Traditionally, a basic block is deﬁned as a consecutive sequence of
instructions without incoming and outgoing branches except for the ﬁrst and last instructions of
the block respectively. In this work we adopt a different deﬁnition of basic block. In particular,
in our proposed formulation, we differentiate basic blocks according to the type of instructions
they perform. We discriminate between the following types of blocks: loop blocks identifying
basic blocks including loop instructions, read block including a single load instruction from main
memory, writes block including a single store instruction to main memory, function block including a
function invocation instruction, and lastly standard block being basic block including instructions
performing computations that do not belong to any of the above mentioned categories. The effect of
this representation and taxonomy affects the granularity of the CFG representation, increasing the
number of blocks w.r.t.the traditional one. Figure 2-(bottom), shows the differences of the proposed
CFG representation – with a higher block granularity – w.r.t. the traditional one. This choice aims

2For the HLS designs considered in this work, the DSEs have not affected the number of BRAM; therefore,

we have not include BRAM estimation in our experiments.

4

Figure 2: Data representation. Control Flow Graph, Data ﬂow graph and Hybrid Control Data Flow
Graph representations of the behavioural speciﬁcation of the function in Figure 1.

at avoiding the limitation of approaches relying on a vector-based representation of the program
and directives [30], while, at the same time, focusing only on the information that is more relevant
from the HLS and DSE perspective. Notably, recent works highlight the effectiveness of adopting
a similar taxonomy of basic blocks to capture similarities among points in different conﬁguration
spaces [14]. Compared to previous works in program analysis (e.g., Li et al. [29]), we include in our
CFG representation only corse-grained information on the types of operations performed in each
block. In addition to the node type, attribute vectors associated to each node include block-type related
information, such as: number of instruction in a block, number of iterations of a loop block, presence
of loop carried dependencies, and other type of information extracted through static and dynamic
code analysis performed using custom LLVM [25] compiler passes (more details in Section 4). While
CFGs contain information about the execution ﬂow of a program, they do not model the ﬂow of
data and information. Data Flow Graphs (DFGs) address this aspect. DFGs are used to represent
the different dependencies between the instructions of a program. In particular they represent the
use-def chains among variables in the program execution. The DFG is a directed graph GDFG deﬁned
as a tuple (cid:104)VDFG, EDFG(cid:105), where VDFG is the a set of nodes corresponding to a instruction in the input
source code, and EDFG is the set of edges representing the possible data ﬂow among instruction. The
DFG representation for the running example function is shown in Figure 2-(top).

Hybrid Control Data Flow Graph HLS tools use instrumented Control Flow Graphs and Data
Flow Graphs as program representations to decide how to implement in hardware the design function-
ality. In our approach we aim at using a similar representation directly as input of a learning method.
We propose a graph representation of the software description including both CFG and the DFG
information. In particular, we augment the CFG representation by adding data ﬂow edges and nodes
representing the input and output parameters of the function. Data ﬂow edges are added among the
nodes involving operations affecting the input and output parameters. These edges are identiﬁed by
tracking the def-use chains among parameter variables in the DFG and embedding them in the CFG.
In addition, edges among the parameter nodes and the CFG read and write blocks are added to the set
of edges (param ﬂow). We indicate this Hybrid Control Data Flow Graph as Ghls and we use it as
input representation in our methodology. To sum up, Ghls is deﬁned as a tuple (cid:104)Vhls, Ehls, u(cid:105), where
Vhls is the a set of nodes corresponding to basic blocks and function parameters, and Ehls is the set
of attributed edges representing the control, data, and parameter ﬂows. Each edge attribute vector
ei,j is a 3 dimensional feature vector with a one hot encoding representation of the edge type. Lastly,
vi ∈ Vhls is the feature vector associated to each node with a one-hot-encoding representation of
the nodes type and their attributes, plus the value of each optimization directive. Figure 2-(bottom)
shows the Hybrid Control Data Flow Graph representations for the running example function.

3.2 Graph neural networks for high-level synthesis design-space exploration

Given a graph Ghls representing a program annotated with optimization directives, we process it with
message passing neural networks by parametrizing the computation of messages exchanged between
neighboring nodes of the graph with MLPs. A schematic of the model is shown in Figure 3: we
describe at ﬁrst each computational block in detail then discuss the training procedure.

5

[ 1, 0, 0, 0, 0, 0, 5, 0, ..., 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0][ 0, 1, 0 ]StartEndControl Flow GraphData Flow GraphBWBBLBRBBWBBStartBEndHybrid Control Data Flow GraphPPControl flowLegendData flowParam flowBWRLWrite blockRead blockLoop blockStandard blockPParameter blockCFGblocks1st to 5thnode type6th to 19thnode attributes20th to 27thpragma valuesedge typeRFigure 3: gnn4hls. An encoder block maps the pre-process input representation with standard MLPs.
Propagation layers perform message passing and update the global representation with multi-head
attention layers. The ﬁnal block, maps the latent representation learned by the network into the
regression targets.

Encoder The Encoder block maps node, edge, and global graph features into a ﬁrst hidden repre-
sentation without performing any message-passing operation. The Encoder is implemented by using
standard MLPs as update functions

v0
i = MLPenc

v (vi) , u0 = MLPenc

u (u) , eenc

i,j = MLPenc

e

(ei,j) ,

as using feed-forward layers before message passing has shown to be beneﬁcial to ﬁnal performance
in GNNs [49]. The Encoder block is followed by a stack of propagation layers.

Propagation layer Propagation layers are instantiated in the message-passing framework shown in
Equation 1. In particular, the node update and message functions are implemented as:

i = MLPt
vt
τv

(cid:18)

vt−1
i

(cid:107) MEAN

(cid:110)

MLPt

ψv

(cid:0)vt−1

j

(cid:107) eenc
j,i

(cid:1); (j, i) ∈ Ehls

(cid:111)(cid:19)
,

(3)

where (cid:107) is the vector concatenation operator and MEAN{ · } indicates that we aggregate incoming
messages by averaging them out. To update the global representation ut we use a MLP as update
function that takes in input the concatenation of u at the previous propagation step and node attributes
aggregated by exploiting the attention mechanism [44, 45]. In particular, we use a MLP (MLPα( · ))
to compute a raw attention score for each node attribute vector vt
i given the global features ut−1;
raw scores (i.e., logits) are then normalized over the nodes of the graph with a softmax function.
Normalized node scores are used to aggregate node features processed by a third MLP. Putting all
together, each propagation layer updates the global representation as follows:
(cid:111)

(cid:110)

(cid:17)

(cid:16)

score (u, vi) = softmax
(cid:16)

MLPα
(cid:110)
score (cid:0)ut−1, vt

ut−1 (cid:107) SUM

u, vi
(cid:1) (cid:12) MLPt
ψu

; i ∈ Vhls
(cid:0)vt−1

i

i

ut = MLPt
τu

(cid:1); i ∈ Vhls

(cid:111)(cid:17)

,

(4)

where (cid:12) is the element-wise multiplication operator and SUM{ · } indicates aggregation by graph-wise
summation of node features. In practice multiple attention heads can be used in parallel for increased
model capacity.

Regression head After T message-passing blocks, node representations are pooled in a single
vector using a permutation invariant aggregation function (e.g., by taking the sum or the average of
node representations, optionally weighted by learned attention scores). The pooled representation is
then concatenated to the global attributes uT leading to a vector representation of the input graph.
This feature vector is fed trough a last MLP which maps it to a prediction of latency and resources as
shown in Figure 3.

Training procedure and transfer learning We train the GNN by supervised learning to predict
the outcome of the HLS procedure. We use data from several synthesized designs, with program
speciﬁcations relevant to different domains (we refer to Section 4 for more details). While learning to
predict the outcome of the synthesis process for conﬁgurations spaces already partially explored is
interesting from a research perspective, we are interested in assessing the possibility of exploiting the

6

GlobalPooling||EncoderPropagation layersRegression headPredictionMLPMessage passingMessage passingFigure 4: Comparison among
the gnn4hls approach proposed
in this work and a MLP ones
adopted as a baseline.

Figure 5: Ablation study of
gnn4hls graph structure.

Comparison be-
Figure 6:
tween
prior-
ﬁne-tuned,
knowledge [14], and zero-shot
approaches.

model for DSE. In particular, we aim at assessing if the learned representation can support transfer to
different domains (designs) when only a few samples, or none, from the target conﬁguration space
are available. We comment that our method differs from previous approaches, which usually are
domain speciﬁc and tied to the characteristics of the target design space. Instead, our methodology
is general and can easily incorporate knowledge from different design spaces by simply including
synthesized points in the training dataset.

4 Experimental evaluation

The graph representation of HLS designs and the associated pragma values are generated combining
LLVM [25] compiler passes, Clang Abstract Syntax Tree (AST) analysis, Frama-C [8] internal
representation of program dependencies, and HLS synthesis information from a recently published
database of HLS-drived DSEs [15]. The custom LLVM pass generates the CFG representation from
the compiler Intermediate Representation (IR) and performs static program analysis to identify the
block proprieties. In order to account for the information lost in the LLVM IR representation, an
AST visitor extracts and maps the SW information to the CFG blocks. Data ﬂow information is
extracted from the Frama-C program dependency analysis, and used to generate the data ﬂow edges
of Ghls. We generated graph representations from 23 different functions in MachSuite [36]. The
considered functions include a wide range of computational intensive applications such as: matrix-
matrix multiplication, sparse matrix-vector multiplication, sorting algorithms, stencil computations,
molecular dynamics, and forward passes of fully-connected neural networks. For each design, we
used conﬁgurations and synthesis results available from db4hls [15], an open source data base of
HLS DSEs. The total number of conﬁgurations considered in this work is 103093. Conﬁguration
spaces contain from several hundreds, up to several thousand design points. For the graph neural
network, we use as global graph attributes the number of LLVM instructions, the number of input
parameters, and the average value of each directive set within the conﬁguration minus the mean value
of the directive sets computed over the entire conﬁguration space. To increase robustness to outliers,
we also concatenate to the representation the same values minus the median.

In the following at ﬁrst we perform an experiment to asses the accuracy of our model in inferring the
performance and costs of unseen directive combinations, then we switch to the DSE settings. Hyper
parameters of the models and full experimental setup are provided in the supplementary materials.

4.1 Performance and cost estimation

We split the synthesized conﬁguration space of each function in three folds, keeping 70% of the
available points for training, 10% for validation, and 20% for testing. Selecting a proper baseline
for comparison is not easy since none of the approaches existing in the literature can easily be
extended to our settings. The most similar approach is the one introduced by Kwon and Carloni
[24], where a MLP is trained in a multi-domain setting. However their approach, based on multi-task
learning [5], relies on training different input and output layers for each domain, hence limiting
ﬂexibility. Furthermore, they use only optimization directives as an input and predict normalized
scores for performance and costs instead of the actual latency and resources. For these reasons we
consider a DeepSets [50] model to be a more appropriate baseline: in practice we use a node-level
MLP followed by a permutation invariant aggregation and a second MLP to process the aggregated
features. The network architectures were chosen to have a similar model complexity. The boxplot
in Fig. 4 shows, for each ﬁgure of merit, the median, 1st and 3rd quartiles, and interquartile range
of the mean absolute percentage error (MAPE) of the two models over the 23 functions in the
dataset. Results, averaged over 5 independent runs, show that our model drastically outperforms

7

LatencyFFLUTDSP0.000.250.500.751.00MAPEgnn4hlsbaselineLatencyFFLUTDSP0.00.51.0MAPEHCDFGnoParamnodataﬂowonlycfgﬁne-tunedprior-knowl[8]zero-shot10−310−1102ADRSFigure 7: Effect of the inference from multiple
Pareto-frontier on the ADRS and the number of
synthesis.

Figure 8: ADRS Comparison across differ-
ent class of applications among gnn4hls
and prior knowl..

the baseline by achieving an average MAPE, averaged over performance and costs estimates, of
2.7% against 15.4%: an improvement of over 82%. Furthermore, the performance obtained by our
model is qualitatively similar to SoA simulator ones, which exploit an analytical model of the HLS
process, HLScope+ [6] for the latency, and MPSeeker [55] for resources3. In particular, latency
estimation performance are comparable to the best from the SoA (1.1% MAPE of HLScope+ vs.
2.1% of gnn4hls), our area performance estimation outperforms the ones from existing models.
gnn4hls achieves 4.8%, 2.6% and 1.3% estimation for FFs, LUTs, and DSPs compared to the
14.7%, 13.2%, 12.7% respectively from MPSeeker. In addition, inference time is greatly reduced
from seconds to tens of milliseconds w.r.t. SoA alternatives. In particular, the network used here
requires ≈ 20ms to process a single point from the get_delta_matrix_weights1 function on an
Intel(R) Xeon(R) Silver CPU.

4.2 Ablation study

To evaluate the effectiveness of the proposed graph representation, we have performed an ablation
study on the graph edges. Fig. 5 shows results obtained given 3 different ablations on the graph
structure: no data ﬂow edges, no param edges, and both data ﬂow and param edges removed. For all
the representation the same type of nodes and attributes have been considered. Results show that the
proposed Hybrid CDFG representation leads to the best results. Note that the DFG can largely be
inferred from Param edges, thus the similar performance of this setting.

4.3 Design-space exploration

The second set of experiments aims at addressing DSE. Herein, we focus on approximating the
Pareto-frontier of a target HLS design, given the synthesis outcomes of all the considered functions
but the target one (i.e., we perform a leave-one-out evaluation w.r.t. the available functions). We
consider the setting where the designer performs an initial naïve random sampling of the conﬁguration
space, uses the synthesized points to ﬁne tune the model and then uses the model’s estimates over the
conﬁguration space to approximate the Pareto curve.

In particular, we select only the conﬁgurations expected to be Pareto-optimal by considering as cost
the weighted sum of the utilized resources. We assessed the quality of the DSEs measuring the
Average Distance from Reference Set (ADRS) [39, 14, 24] metric among the real Pareto-solutions
and the one estimated to be Pareto-optimal. A low value of ADRS implies a close approximation of
the real Pareto-frontier. To increase robustness to prediction errors we iteratively select candidate
Pareto-optimal points by removing from the conﬁguration space the already selected conﬁgurations
and recomputing the Pareto curve up to 5 times. Results are shown in Figure 6. In particular, we
compare the performance of the ﬁne-tuning approach against the current state-of-the-art one on
the considered dataset, namely the prior-knowl. approach [14] (see Section 5). For the ﬁne-tuning
procedure we use a maximum of 128 points from the target domain, capped at 5% of the conﬁguration
space dimension; we ﬁx the number of SGD updates to 150, with a batch size of 32. We also compare
the performance in the zero-shot setting (no ﬁne-tuning).

Figure 6 shows the performance obtained by the ﬁne-tuned approach (averaged over 40 independent
runs) w.r.t. prior-knowl. and the zero-shot model. As previously mentioned, we consider up to the 5th

3A direct comparison with these methodologies was not possible since these were not available open source.

Thus, we compare w.r.t.

the performance of the original papers.

8

12345Pareto-front10−2101ADRS0100200Avg.n.ofsynth.MLsortlin.algebraothers10−310−1ADRSgnn4hlspriorknowl.[12]Pareto-front and compare against the reported results for prior knowl. which iterates up to the 10th
frontier. We considered only the portion of the conﬁguration space that is actually synthesizable (i.e.,
we considered only conﬁgurations present in the database). Results show the distributions of the
ADRS across the 23 considered functions. In particular, the ﬁne-tuned model obtains Pareto-frontier
approximations comparable to the state of the art, while reducing the number of outliers compared to
prior knowl. and obtaining an average ADRS of 0.20 vs 0.45: a remarkable improvement. This result
is particularly appealing when observing that our approach neither uses any heuristic to perform
the initial sampling, nor does it rely on domain knowledge provided by the designer. Furthermore,
our method provides the user with performance and cost estimates of the candidate conﬁgurations,
which could be instrumental to further reduce the number of syntheses required to obtain the desired
performance and satisfy hardware constraints. Figure 7 shows how the ADRS scores change w.r.t.
the number of iteratively estimated Pareto frontiers during the DSE. While the ADRS decreases
exponentially (plot in logarithmic scale along the y axis), the number of required syntheses grows
linearly. Compared to prior knowl. our approach requires a higher number of syntheses. However,
we argue that we might expect this gap to reduce signiﬁcantly when considering a larger dataset:
the performance of the zero-shot approach, in fact, should be considered in light of the fact that
our dataset contains only 23 different designs. Finally, Figure 8 compares the results of gnn4hls
and prior knowl. across the different class of applications considered in this work (see appendix for
details on the taxonomy and the complete set of results). Our method shows lower variance in the
ML designs, but a higher mean, and lower ADRS in linear algebra designs which are relevant for
deep learning.

5 Related works

Design Space Exploration approaches in High-Level Synthesis
In past years, the hardware
design community has proposed different works to address the HLS-driven DSE problem. A recent
survey from Schafer and Wang [39] summarize them. Among these works we can identify two
main categories: model-based approaches, and reﬁnement-based ones. Model-based approaches
[53, 35, 54, 52, 6] rely on estimates of performance and resource requirements of a given optimization.
These approaches require very few synthesis runs to approximate the Pareto frontier, but, often, have
difﬁculties while dealing with multiple, interdependent optimizations. Conversely, reﬁnement-based
methodologies are agnostic to the number and types of directives considered, and rely on the
outcome of few heuristically sampled synthesis runs as a starting point for DSE. After the initial
synthesis, the models aim to improve the initial solutions using different strategies such as genetic
algorithms [38], simulated annealing [31], clustering [12] or local search techniques [13]. Reﬁnement-
based approaches are not limited by the number and type of synthesis directives pre-characterized,
but usually converge more slowly to the Pareto-frontier with respect to model-based approaches.
Taking a different stance, more recently Ferretti et al. [14] have proposed a DSE strategy able to map
the result of past DSE targeting different design to unseen ones. The proposed approach searches for
similarity in the source code of the already explored designs, and, based on the result of past DSEs,
decides how to optimize the target one. This approach has been used as a baseline to compare the
DSEs performance of the experiment in Section 4.3. Similarly, a recent work from [24] proposes a
neural network model for mixed-sharing multi-domain transfer learning to transfer the knowledge
obtained from previously explored design spaces in exploring a new target one.

Graph neural networks for hardware/software design. First GNNs date back to models devel-
oped by Gori et al. [19] and Scarselli et al. [37] and earlier ideas on how to process graph-structured
data with recurrent neural networks [43, 17]. In recent years, the ﬁeld of graph deep learning has
surged in popularity and several architectural improvements and variants of GNNs have found wide
spread and adoption by the community [27, 23, 33, 20, 45, 2]. Among their many applications to
structured data processing, GNNs have been widely used as the learning system of choice in software
engineering to automate program analysis and code optimization [42, 29, 3, 56]. Furthermore, graphs
have also been used to capture the structure of hardware architectures. Notably, Mirhoseini et al.
[32] used a GNN to learn a transferable representation to tackle the critical hardware design problem
of chip placement. Finally, due to the ﬂexibility of graph representations, GNNs have be used to
automate design processes in many other areas of science and engineering: prime examples are in the
discovery of new molecules [28, 48] and in automatic robot design [46, 51].

9

6 Conclusion and future works

In this work we presented gnn4hls, a graph-based learning framework for HLS-driven DSE. Com-
pared against the state of the art, our method offers tools that are general and that can easily be
applied to any conﬁguration space. A key aspect of gnn4hls is its simplicity w.r.t.the its effectiveness.
We show that our method compares favorably against the state of the art w.r.t. both quantitative
and qualitative metrics. In the future, we plan to investigate more advanced solutions from the
few-shot learning literature to improve transfer to unseen domains and to consider the a ﬁner-grained
representation of basic blocks. Then, we argue for the possibility of replacing existing heuristics
to perform DSE with an exploration policy learned by (model-based) reinforcement learning. To
support breakthroughs in this direction, we renew our invitation to the community in participating
in a common effort for the collection of datasets fully enabling the application of deep learning in
the context of HLS-driven DSE. We believe that this work represents a milestone for the application
of graph deep learning in EDA and a signiﬁcant step towards fully automated design of hardware
accelerators with no human in the loop.

References

[1] P. W. Battaglia, J. B. Hamrick, V. Bapst, A. Sanchez-Gonzalez, V. Zambaldi, M. Malinowski,
A. Tacchetti, D. Raposo, A. Santoro, R. Faulkner, et al. Relational inductive biases, deep
learning, and graph networks. arXiv preprint arXiv:1806.01261, 2018.

[2] F. M. Bianchi, D. Grattarola, L. Livi, and C. Alippi. Graph neural networks with convolutional

ARMA ﬁlters. IEEE Transactions on Pattern Analysis and Machine Intelligence, 2021.

[3] D. Bieber, C. Sutton, H. Larochelle, and D. Tarlow. Learning to execute programs with
instruction pointer attention graph neural networks. Advances in Neural Information Processing
Systems, 33, 2020.

[4] M. M. Bronstein, J. Bruna, Y. LeCun, A. Szlam, and P. Vandergheynst. Geometric deep learning:

going beyond euclidean data. IEEE Signal Processing Magazine, 34(4):18–42, 2017.

[5] R. Caruana. Multitask learning. Machine learning, 28(1):41–75, 1997.

[6] Y.-k. Choi, P. Zhang, P. Li, and J. Cong. Hlscope+: Fast and accurate performance estimation
for fpga hls. In 2017 IEEE/ACM International Conference on Computer-Aided Design (ICCAD),
pages 691–698. IEEE, 2017.

[7] D. Clevert, T. Unterthiner, and S. Hochreiter. Fast and accurate deep network learning by
Exponential Linear Units (ELUs). In 4th International Conference on Learning Representations,
ICLR, 2016.

[8] P. Cuoq, F. Kirchner, N. Kosmatov, V. Prevosto, J. Signoles, and B. Yakobowski. Frama-C: A

software analysis perspective. SEFM’12, page 233–247. Springer-Verlag, 2012.

[9] H. Esmaeilzadeh, E. Blem, R. St. Amant, K. Sankaralingam, and D. Burger. Dark silicon and
the end of multicore scaling. In Proceedings of the 38th Annual International Symposium on
Computer Architecture, ISCA ’11, pages 365–376, 2011.

[10] F. Fahim, B. Hawks, C. Herwig, J. Hirschauer, S. Jindariani, N. Tran, L. P. Carloni,
G. Di Guglielmo, P. Harris, J. Krupa, et al. hls4ml: An open-source codesign workﬂow
to empower scientiﬁc low-power machine learning devices. arXiv preprint arXiv:2103.05579,
2021.

[11] e.

a.

lightning.
https://github.com/PyTorchLightning/pytorch-lightning, 3, 2019.

Pytorch

Falcon,

WA.

GitHub.

Note:

[12] L. Ferretti, G. Ansaloni, and L. Pozzi. Cluster-based heuristic for high level synthesis design
space exploration. IEEE Transactions on Emerging Topics in Computing, (99):1–9, Jan. 2018.

[13] L. Ferretti, G. Ansaloni, and L. Pozzi. Lattice-traversing design space exploration for high level
synthesis. In Proceedings of the International Conference on Computer Design, pages 210–217,
Oct. 2018.

10

[14] L. Ferretti, J. Kwon, G. Ansaloni, G. D. Guglielmo, L. P. Carloni, and L. Pozzi. Leveraging prior
knowledge for effective design-space exploration in high-level synthesis. IEEE Transactions on
Computer-Aided Design of Integrated Circuits and Systems, 39(11):3736–3747, 2020.

[15] L. Ferretti, J. Kwon, G. Ansaloni, G. Di Guglielmo, L. Carloni, and L. Pozzi. DB4HLS: A
database of high-level synthesis design space explorations. arXiv preprint arXiv:2101.00587,
2021.

[16] M. Fey and J. E. Lenssen. Fast graph representation learning with pytorch geometric. arXiv

preprint arXiv:1903.02428, 2019.

[17] P. Frasconi, M. Gori, and A. Sperduti. A general framework for adaptive processing of data

structures. IEEE transactions on Neural Networks, 9(5):768–786, 1998.

[18] J. Gilmer, S. S. Schoenholz, P. F. Riley, O. Vinyals, and G. E. Dahl. Neural message passing
for quantum chemistry. In International Conference on Machine Learning, pages 1263–1272.
PMLR, 2017.

[19] M. Gori, G. Monfardini, and F. Scarselli. A new model for learning in graph domains. In
Proceedings. 2005 IEEE International Joint Conference on Neural Networks, 2005., volume 2,
pages 729–734. IEEE, 2005.

[20] W. L. Hamilton, R. Ying, and J. Leskovec. Inductive representation learning on large graphs. In

NIPS, 2017.

[21] C. R. Harris, K. J. Millman, S. J. van der Walt, R. Gommers, P. Virtanen, D. Cournapeau,
E. Wieser, J. Taylor, S. Berg, N. J. Smith, et al. Array programming with numpy. Nature, 585
(7825):357–362, 2020.

[22] D. P. Kingma and J. Ba. Adam: A method for stochastic optimization.

In International

Conference on Learning Representations, ICLR, 2015.

[23] T. N. Kipf and M. Welling. Semi-supervised classiﬁcation with graph convolutional networks.

In International Conference on Learning Representations (ICLR), 2017.

[24] J. Kwon and L. P. Carloni. Transfer learning for design-space exploration with high-level
synthesis. In Proceedings of the 2020 ACM/IEEE Workshop on Machine Learning for CAD,
MLCAD ’20, page 163–168. Association for Computing Machinery, 2020.

[25] C. Lattner and V. Adve. LLVM: A compilation framework for lifelong program analysis
& transformation. In Proceedings of the international symposium on Code generation and
optimization, page 75, Mar. 2004.

[26] Y. LeCun, Y. Bengio, and G. Hinton. Deep learning. nature, 521(7553):436–444, 2015.

[27] Y. Li, D. Tarlow, M. Brockschmidt, and R. S. Zemel. Gated graph sequence neural networks. In
Y. Bengio and Y. LeCun, editors, 4th International Conference on Learning Representations,
ICLR 2016, San Juan, Puerto Rico, May 2-4, 2016, Conference Track Proceedings, 2016. URL
http://arxiv.org/abs/1511.05493.

[28] Y. Li, O. Vinyals, C. Dyer, R. Pascanu, and P. Battaglia. Learning deep generative models of

graphs. arXiv preprint arXiv:1803.03324, 2018.

[29] Y. Li, C. Gu, T. Dullien, O. Vinyals, and P. Kohli. Graph matching networks for learning the
similarity of graph structured objects. In International Conference on Machine Learning, pages
3835–3845. PMLR, 2019.

[30] H.-Y. Liu and L. P. Carloni. On learning-based methods for design-space exploration with
high-level synthesis. In Proceedings of the 50th Design Automation Conference, pages 1–6,
June 2013.

[31] A. Mahapatra and B. C. Schafer. Machine-learning based simulated annealer method for high
level synthesis design space exploration. In Proceedings of the 2014 Electronic System Level
Synthesis Conference, pages 1–6, 2014.

11

[32] A. Mirhoseini, A. Goldie, M. Yazgan, J. Jiang, E. Songhori, S. Wang, Y.-J. Lee, E. Johnson,
O. Pathak, S. Bae, et al. Chip placement with deep reinforcement learning. arXiv preprint
arXiv:2004.10746, 2020.

[33] F. Monti, D. Boscaini, J. Masci, E. Rodola, J. Svoboda, and M. M. Bronstein. Geometric
deep learning on graphs and manifolds using mixture model cnns. In Proceedings of the IEEE
conference on computer vision and pattern recognition, pages 5115–5124, 2017.

[34] A. Paszke, S. Gross, F. Massa, A. Lerer, J. Bradbury, G. Chanan, T. Killeen, Z. Lin,
N. Gimelshein, L. Antiga, A. Desmaison, A. Kopf, E. Yang, Z. DeVito, M. Raison,
A. Tejani, S. Chilamkurthy, B. Steiner, L. Fang, J. Bai, and S. Chintala. Pytorch: An
imperative style, high-performance deep learning library. In Advances in Neural Information
Processing Systems, pages 8024–8035. 2019. URL http://papers.neurips.cc/paper/
9015-pytorch-an-imperative-style-high-performance-deep-learning-library.
pdf.

[35] N. K. Pham, A. K. Singh, A. Kumar, and M. M. A. Khin. Exploiting loop-array dependencies to
accelerate the design space exploration with high level synthesis. In 2015 Design, Automation
Test in Europe Conference Exhibition (DATE), pages 157–162, 2015.

[36] B. Reagen, R. Adolf, Y. S. Shao, G.-Y. Wei, and D. Brooks. MachSuite: Benchmarks for
accelerator design and customized architectures. In Proceedings of the IEEE International
Symposium on Workload Characterization, pages 110–119, Oct. 2014.

[37] F. Scarselli, M. Gori, A. C. Tsoi, M. Hagenbuchner, and G. Monfardini. The graph neural

network model. IEEE transactions on neural networks, 20(1):61–80, 2008.

[38] B. C. Schafer and K. Wakabayashi. Machine learning predictive modelling high-level synthesis
design space exploration. IET computers & digital techniques, 6(3):153–159, May 2012.

[39] B. C. Schafer and Z. Wang. High-level synthesis design space exploration: Past, present, and
future. IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems, 39
(10):2628–2639, 2020.

[40] R. R. Schaller. Moore’s law: Past, present, and future. IEEE Spectr., 34(6):52–59, June 1997.

[41] J. Schmidhuber. Deep learning in neural networks: An overview. Neural networks, 61:85–117,

2015.

[42] X. Si, H. Dai, M. Raghothaman, M. Naik, and L. Song. Learning loop invariants for program
veriﬁcation. In S. Bengio, H. Wallach, H. Larochelle, K. Grauman, N. Cesa-Bianchi, and
R. Garnett, editors, Advances in Neural Information Processing Systems, volume 31. Curran
Associates, Inc., 2018.

[43] A. Sperduti and A. Starita. Supervised neural networks for the classiﬁcation of structures. IEEE

Transactions on Neural Networks, 8(3):714–735, 1997.

[44] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, Ł. Kaiser, and
I. Polosukhin. Attention is all you need. In Proceedings of the 31st International Conference on
Neural Information Processing Systems, pages 6000–6010, 2017.

[45] P. Veliˇckovi´c, G. Cucurull, A. Casanova, A. Romero, P. Liò, and Y. Bengio. Graph Attention
Networks. International Conference on Learning Representations, 2018. URL https://
openreview.net/forum?id=rJXMpikCZ.

[46] T. Wang, Y. Zhou, S. Fidler, and J. Ba. Neural graph evolution: Towards efﬁcient automatic

robot design. arXiv preprint arXiv:1906.05370, 2019.

[47] Z. Wang, J. Chen, and B. C. Schafer. Efﬁcient and robust high-level synthesis design space
exploration through ofﬂine micro-kernels pre-characterization. In 2020 Design, Automation
Test in Europe Conference Exhibition (DATE), pages 145–150, 2020.

[48] J. You, B. Liu, Z. Ying, V. S. Pande, and J. Leskovec. Graph convolutional policy network for

goal-directed molecular graph generation. In NeurIPS, pages 6412–6422, 2018.

12

[49] J. You, Z. Ying, and J. Leskovec. Design space for graph neural networks. Advances in Neural

Information Processing Systems, 33, 2020.

[50] M. Zaheer, S. Kottur, S. Ravanbakhsh, B. Poczos, R. R. Salakhutdinov, and A. J. Smola.
Deep sets. In I. Guyon, U. V. Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan,
and R. Garnett, editors, Advances in Neural Information Processing Systems, volume 30.
Curran Associates, Inc., 2017. URL https://proceedings.neurips.cc/paper/2017/
file/f22e4747da1aa27e363d86d40ff442fe-Paper.pdf.

[51] A. Zhao, J. Xu, M. Konakovi´c-Lukovi´c, J. Hughes, A. Spielberg, D. Rus, and W. Matusik.
Robogrammar: graph grammar for terrain-optimized robot design. ACM Transactions on
Graphics (TOG), 39(6):1–16, 2020.

[52] J. Zhao, L. Feng, S. Sinha, W. Zhang, Y. Liang, and B. He. COMBA: A comprehensive
model-based analysis framework for high level synthesis of real applications. In Proceedings of
the International Conference on Computer Aided Design, pages 430–437, Oct. 2017.

[53] G. Zhong, V. Venkataramani, Y. Liang, T. Mitra, and S. Niar. Design space exploration of
In Proceedings of the International

multiple loops on FPGAs using high level synthesis.
Conference on Computer Design, pages 456–463, Dec. 2014.

[54] G. Zhong, A. Prakash, Y. Liang, T. Mitra, and S. Niar. Lin-analyzer: a high-level performance
analysis tool for FPGA-based accelerators. In Proceedings of the 53rd Design Automation
Conference, pages 136:1–136:6, June 2016.

[55] G. Zhong, A. Prakash, S. Wang, Y. Liang, T. Mitra, and S. Niar. Design Space Exploration of
fpga-based accelerators with multi-level parallelism. In Design, Automation & Test in Europe
Conference & Exhibition (DATE), 2017, pages 1141–1146. IEEE, 2017.

[56] Y. Zhou, S. Roy, A. Abdolrashidi, D. Wong, P. Ma, Q. Xu, H. Liu, P. Phothilimtha, S. Wang,
A. Goldie, A. Mirhoseini, and J. Laudon. Transferable graph optimizers for ml compil-
In H. Larochelle, M. Ranzato, R. Hadsell, M. F. Balcan, and H. Lin, editors, Ad-
ers.
vances in Neural Information Processing Systems, volume 33, pages 13844–13855. Cur-
ran Associates, Inc., 2020. URL https://proceedings.neurips.cc/paper/2020/file/
9f29450d2eb58feb555078bdefe28aa5-Paper.pdf.

13

A Dataset and experimental set-up

In this section we provide details on the dataset and the experimental setting used for the experiments
presented in the paper. All experiments were carried out on a server with Intel Xeon Silver CPUs and
Nvidia Titan Xp/V GPUs.

For developing the models and the infrastructure to run experiments we relied on the following
open-source tools and libraries:

• LLVM [25];
• FramaC [8];
• numpy [21];
• db4hls [15];
• PyTorch [34];
• PyTorch Geometric [16];
• PyTorch Lightning [11];

The code used to run the experiments is attached as supplementary material and it will be released on
GitHub upon publication.

A.1 Dataset description

The synthesis data used to train and test the model come from the db4hls database [15]. The database
includes a collection of DSEs performed for HLS designs of the MachSuite benchmark suite [36]. In
this work we use the data collected in the database according to the conﬁguration space deﬁnition
described by Ferretti et al. [15]. The performed DSEs include up to 5 different type of pragmas:
resource type, array partitioning type, array partitioning factor, loop unrolling, and function inlining.
The pragma values speciﬁed for array partitioning factor and loop unrolling are limited to power of
twos or integer divisors of the input/output array sizes and loop trip-counts. We consider only a subset
of the functions available in db4hls. In particular, we select only HLS designs which do not require
allocation of arrays or structs. This limitation is due to the LLVM compiler analysis performed to
extract the CFG representation of the software speciﬁcations: this limitation will be addressed in
future works. The original designs from MachSuite are compiled with the lowest optimisation level
(-O0). Then, we process the intermediate representation (IR) produced by LLVM through a custom
LLVM pass in order to generate the CFG and distinguish among the different types of block. Due
to some LLVM optimisation performed on the IR representation, memory allocation are in some
cases transformed into function calls generating an undesired code transformation which affect the
the resulting CFG – i.e., additional call blocks are generated without an explicit function invocation
in the original speciﬁcations. We aim at addressing this limitation in the next future.

In addition to the block type, the LLVM pass extracts block speciﬁc information. This information is
used to populate the block attribute vectors used to represent each node of the hybrid control data
ﬂow graph representation. In order to extract such information, we perform different analyses of the
LLVM IR, the Clang abstract syntax tree (AST) and the source code. An LLVM pass generates the
intermediate representation, which is used to ﬁrst identify basic blocks in the code and the traditional
CFG representation. Then, for each instruction in the basic blocks we check the type of operation
performed and we discriminate among the different types of block. Basic blocks including store
instructions are split separating the store instruction from the predecessor and successor instructions.
The newly created store block is then is connected to the new basic blocks resulting from the split.
Read blocks are generated in the same way. Loop blocks are identiﬁed from the IR. Loop information
is extracted in order to model loop carried dependencies, loop stride, and loop trip count. Similarly,
call blocks are identiﬁed from the IR. In this case information about the number of parameters, number
of function invocation, and number of LLVM instructions of the invoked function are extracted and
included in the attribute vector. From the remaining uncategorized standard blocks, information about
the number of LLVM instructions executed in it are extracted and added to the attribute vector.

Parameter blocks instead are generated combining information from the LLVM IR and the AST
analysis. In particular we extract information associated to the parameter type (pointer or value),
parameter data type – the size of the data type –, and number of elements of pointer parameters. This

14

Table 1: List of the functions considered in this work. The table shows: application domain, the
original benchmark from MachSuite [36], the target HLS design to implement in hardware, number
of lines of code, type of pragmas considered for the exploration, conﬁguration space size. Pragma
types are indicated by different symbols: resource (cid:78), array partition type (cid:72), array partition factor (cid:4),
loop unrolling (cid:7), function inlining (cid:70).

Domain

Benchmark HLS design

Lines of code Type of pragmas

| CS |

Linear
Algebra

gemm

spmv

Sorting

radix sort

Stencil
Stencil

stencil

Biology

md

Machine
Learning

backprop

ncubed
bbgemm
ellpack
hist
init
last_step_scan
sum_scan
local_scan
update
ss_sort
stencil2d
stencil3d
knn
get_delta_matrix_weights1
get_delta_matrix_weights2
get_delta_matrix_weights3
product_with_bias_input_layer
product_with_bias_second_layer
product_with_bias_output_layer
take_difference
get_oracle_activations1
get_oracle_activations2
update_weights

41
45
28
34
29
32
31
32
36
50
39
62
71
45
45
45
48
48
48
44
48
48
127

(cid:72) (cid:4) (cid:7)
(cid:4) (cid:7)
(cid:4) (cid:7)
(cid:72) (cid:4) (cid:7)
(cid:78) (cid:72) (cid:4) (cid:7)
(cid:72) (cid:4) (cid:7)
(cid:78) (cid:72) (cid:4) (cid:7)
(cid:72) (cid:4) (cid:7)
(cid:72) (cid:4) (cid:7)
(cid:4) (cid:7) (cid:70)
(cid:72) (cid:4) (cid:7)
(cid:72) (cid:4) (cid:7)
(cid:4) (cid:7)
(cid:72) (cid:4) (cid:7)
(cid:72) (cid:4) (cid:7)
(cid:72) (cid:4) (cid:7)
(cid:4) (cid:7) (cid:70)
(cid:4) (cid:7) (cid:70)
(cid:4) (cid:7) (cid:70)
(cid:72) (cid:4) (cid:7)
(cid:4) (cid:7)
(cid:4) (cid:7)
(cid:4) (cid:7)

2744
1600
1600
4704
484
800
1280
704
2400
1792
1344
1536
1152
21952
31213
21952
1372
686
392
512
2401
1372
1024

last information is required by the HLS tools in order to know the memory required by the hardware
accelerator. However, since this information is not preserved in the LLVM IR, we have extracted it
from a joint AST and source code analysis. To avoid large numerical differences among attributes,
we use logarithmic scale fot the number of LLVM instructions, loop trip counts, array partitioning
factors, data types, and their related directive values.

Table 1 reports the designs considered in this work. For each function we show: the application
domain (used to group the applications in Figure 8), the original benchmark name in MachSuite, the
HLS design name, the size of the designs in term of lines of code, the type of pragmas considered
during the exploration and the conﬁguration space size.

A.2 Global attributes

In addition to the attribute vectors associated to each node, we introduce a global representation of
the conﬁguration with respect to its conﬁguration space.
Given a design and its associated conﬁguration space, we deﬁne s ∈ R5 as the vector having for each
component the average among the directive values set associated to each pragma type. As an example,
given a conﬁguration space having directive value sets with 2 resource types (one-hot encoded), 2
partitioning types (one-hot encoded), partitioning factors of 1, 2, 4, 8, unrolling factor of 10, 20, 30,
and 2 options for function inlining (one-hot encoded), the resulting conﬁguration space vector will be
s = [0.5, 0.5, 3.75, 20, 0.5]. Then, given a speciﬁc conﬁguration, we generate the vector c ∈ R5 as
the average vector among directive values of the same type in that particular conﬁguration. Similarly,
we generate vectors s(cid:48) and c(cid:48) using the median among the directive set values instead of the mean.
Finally, given the number of instructions in the function l and the number of input parameters p we
deﬁne the global attribute vector of the conﬁguration u as:

u = l (cid:107) p (cid:107) (s − c) (cid:107) (s(cid:48) − c(cid:48)),

(5)

15

where (cid:107) is the vector concatenation operation. Intuitively, this vector representation captures how
much a conﬁguration leans toward a region of the design space. In practice, the vectors (s − c) and
(s(cid:48) − c(cid:48)) are also normalized so that each element has unitary variance across the conﬁguration space.

A.2.1 Design space exploration performance metric

The results of the DSEs have been evaluated in term of Average Distance from Reference Set (ADRS).
The ADRS metric is used to quantify the distance between a reference curve P , and an approximated
one ¯P . In our case, the reference curve is the Pareto-frontier ground truth available computed over
the synthesized design avaiable in the dataset, while the approximated one is the Pareto-frontier
resulting from the DSE performed by using the trained model. The ADRS for two objective functions
is deﬁned as:

ADRS( ¯P , P ) =

(cid:20) 1
|P |

(cid:88)

min
¯p∈ ¯P

(d(¯p, p))

(cid:21)
,

(6)

p∈P
d(¯p, p) = max{0, (A ¯p − Ap)/Ap, (L ¯p − Lp)/Lp}
where A ¯p and L ¯p are the area and latency of an element of the reference Pareto-frontier, while Ap
and Lp are the area and latency of the approximated one. Intuitively, lower ADRS values implies
proximity among the approximated curve and the reference one. We consider as altency (LAT ) the
number of clock cycles required by the hardware implementation to execute the functionality, and,
as measure of the area (A), the number aggregated values of FF , LUT , DSP in form of a linear
combination of their utilisation.

(7)

A =

FF
FF available

+

LUT
LUT available

+

DSP
DSP available

(8)

This formulation allows to obtain a unique metric for the area costs evaluating the overall utilisation
of the resources required by an implementation (FF , LUT , and DSP ) with respect to the ones
available on a speciﬁc FPGA (FF available, LUT available, and DSP available).

B Hyperparameters and additional results

In this section ﬁrst we provide details on the hyperparameters and architecture used for the different
models and how training was performed, then we show additional (more complete) experimental
results.

B.1 Performance and cost estimation experiment

We use a GNN with 4 propagation blocks. All the MLPs required to implement encoding, update
and message functions are implemented as networks with a single hidden layer and ELU activation
function [7]. The MLPs processing the node representations all have 128 hidden units, while we
use a width of 256 to process global graph features, both in the propagation blocks and in the
regression head. The MLPs computing node messages (MLPt
) have and additional linear layer after
the nonlinear one. Finally, the MLP used to compute attention scores has an hidden layer of 256 units
and a number of output units equal to the number of attention heads with LeakyReLU (0.2 negative
slope) activation as in [45]. We use 2 attention heads in parallel and we concatenate their outputs
before processing with the global update function τ t
u. For the baseline we use a node-level MLP with
5 hidden layers with 512 units each and ReLU activation function, followed by SUM{ · } aggregation
and a second global MLP with a single hidden layer with the same activation and number of neurons.

ψv

For training, we use as target values the natural logarithm of the true values, and the mean absolute
error as loss function. Models are trained for 800 epochs with a batch-size of 128 without early
stopping and by taking as ﬁnal model the one that achieves the lowest validation error across the
training epochs. For optimization we use the Adam optimizer [22] with an initial learning rate of
0.001 and cosine annealing (with no restarts) as a schedule with a minimum learning rate of 0.0001.
We also clip the gradient norm to a maximum value of 3 to avoid learning instabilities. We did not
use any form of regularization since we did not observe any sign of overﬁtting. We do not perform
any additional scaling to the input features w.r.t. the preprocessing steps described in A.1.

16

Table 2 and Table 3 show the estimation accuracy for gnn4hls and the DeepSets baseline re-
spectively. In particular, we show performance in terms of Mean Absolute Error (MAE) and Mean
Absolute Percentage Error (MAPE) for all the prediction targets for each design. Note that the MAPE
of the DSP estimate is not deﬁned for some function where DSP is not used, tables report a 0.0%
error for both models in such cases. Results are averaged over 5 independent runs, we do not report
standard deviations for the sake of presentation clarity.

Table 2: Detailed results of the performance and costs estimation experiment for gnnhls.

MAPE

MAE

gnnhls

HLS design

LAT

FF

LUT

DSP

LAT

FF

LUT

DSP

ncubed
bbgemm
ellpack
hist
init
last_step_scan
sum_scan
local_scan
update
ss_sort
stencil2d
stencil3d
knn
get_delta_matrix_weights1
get_delta_matrix_weights2
get_delta_matrix_weights3
product_with_bias_input_layer
product_with_bias_second_layer
product_with_bias_output_layer
take_difference
get_oracle_activations1
get_oracle_activations2
update_weights

3.48% 3.18% 2.37E+05
1.9%
3.67%
2.24% 0.46% 2.42E+05
1.35% 2.8%
2.41% 2.21% 1.14E+04
2.69% 2.58%
6.53E+02
5.41% 0.0%
3.68% 8.54%
4.19E+01
0.0%
2.86% 15.03% 4.7%
6.77E+02
0.0%
8.31% 35.95% 8.0%
5.31E+00
1.19%
0.8%
1.05% 0.0%
8.94E+02
11.61% 7.15% 0.0%
6.5%
4.45E+02
2.35% 0.0%
1.41% 4.49%
1.27% 0.0%
0.68% 1.24%
1.37E+04
2.57% 6.94% 1.19E+04
3.75% 3.62%
6.35% 6.68% 1.74E+04
4.21% 8.65%
1.72% 1.38% 8.17E+03
0.99% 1.6%
0.69% 0.61% 1.97E+02
0.62% 0.83%
2.04% 1.36% 1.40E+03
1.51% 2.45%
7.35E+01
0.65% 0.4%
0.59% 0.78%
1.5%
0.67% 0.82%
0.56% 3.61E+02
0.27% 0.18% 1.22E+03
0.31% 0.28%
0.38% 0.2%
0.29% 0.29%
5.23E+01
0.13% 0.21% 6.10E-01
0.35% 0.18%
2.79% 2.67% 4.32E+03
2.48% 2.7%
2.13% 1.85% 1.93E+02
2.42% 1.46%
0.83% 0.25% 7.76E+03
0.39% 0.72%

6.06E+02
3.13E+02
8.04E+02
6.82E+02
4.04E+01
7.72E+02
1.05E+01
7.87E+02
1.84E+02
5.42E+01
3.07E+02
1.46E+03
3.12E+02
8.47E+01
4.48E+02
2.92E+01
6.62E+01
7.71E+00
3.94E+00
2.53E+00
1.70E+02
8.76E+01
5.13E+01

1.21E+03
2.58E+02
6.08E+02
1.64E+03
3.78E+02
2.47E+03
3.53E+01
1.92E+03
6.11E+02
3.47E+02
9.08E+02
2.75E+03
6.50E+02
1.12E+02
5.05E+02
3.20E+01
1.61E+02
3.38E+01
7.29E+00
2.27E+00
3.19E+02
1.51E+02
1.05E+02

4.80E+00
1.35E-01
6.37E+00
1.73E-03
1.95E-03
2.02E-03
1.22E-03
1.23E-03
8.17E-04
1.40E-03
3.01E+01
1.46E+01
1.68E+00
2.04E+00
1.35E+01
7.06E-01
7.54E-01
2.82E-02
3.18E-02
4.91E-02
1.74E+00
1.46E+00
3.48E-02

Avg.

2.12% 4.85% 2.61% 1.27% 2.43E+04

3.17E+02

6.61E+02

3.39E+00

Table 3: Detailed results of the performance and costs estimation experiment for the baseline.

MAPE

MAE

baseline

HLS design

LAT

FF

LUT

DSP

LAT

FF

LUT

DSP

ncubed
bbgemm
ellpack
hist
init
last_step_scan
sum_scan
local_scan
update
ss_sort
stencil2d
stencil3d
knn
get_delta_matrix_weights1
get_delta_matrix_weights2
get_delta_matrix_weights3
product_with_bias_input_layer
product_with_bias_second_layer
product_with_bias_output_layer
take_difference
get_oracle_activations1
get_oracle_activations2
update_weights

29.81% 17.44%
8.24%
5.34%
11.26% 7.05%
29.99% 36.96% 21.09%
18.67% 49.28% 43.0%
8.47%
27.21% 47.21% 12.9%
4.32%
3.61%
7.61%
17.13% 14.32% 8.67%
19.73% 12.77%
6.46%
2.43%
2.47%
1.79%
12.35% 7.82%
5.51%
23.16% 25.92% 20.36%
20.33%
4.92%
3.02%
5.1%
6.67%
8.23%
40.05% 31.02% 32.77%
4.55%
6.68%
3.59%
3.91%
2.23%
1.53%
5.28%
1.69%
1.24%
3.89%
4.08%
0.79%
1.17%
1.22%
2.26%
5.75%
5.76%
4.74%
5.96%
4.46%
8.85%
5.17%
1.5%
1.18%

29.44% 7.72E+05
4.39%
9.32E+05
92.78% 1.50E+05
2.64E+03
0.0%
9.41E+01
53.91% 273.64% 0.0%
1.64E+03
0.0%
3.08E+01
0.0%
1.93E+03
0.0%
2.05E+03
0.0%
3.56E+04
0.0%
19.29% 3.56E+04
41.69% 6.96E+04
1.80E+04
2.8%
9.1%
1.49E+03
60.15% 2.11E+04
4.25E+02
4.12%
8.43E+02
1.39%
4.99E+03
1.36%
1.44E+02
0.5%
3.48E+00
1.29%
9.97E+03
5.73%
6.97E+02
4.37%
2.33E+04
0.31%

4.39E+03
1.32E+03
5.86E+03
1.79E+03
1.94E+02
1.52E+03
5.21E+01
1.15E+03
6.72E+02
1.14E+02
7.70E+02
4.00E+03
8.60E+02
6.27E+02
2.93E+03
2.67E+02
1.74E+02
3.43E+01
6.12E+01
2.60E+01
5.66E+02
3.30E+02
1.07E+02

4.57E+03
8.01E+02
3.67E+03
8.35E+03
6.10E+02
3.15E+03
1.08E+02
2.23E+03
2.78E+03
6.63E+02
1.92E+03
8.97E+03
8.35E+03
7.78E+02
8.26E+03
2.39E+02
4.84E+02
7.76E+02
8.85E+01
3.07E+01
1.09E+03
4.60E+02
6.40E+02

2.12E+01
1.20E+00
5.13E+01
2.96E-02
5.36E-02
3.97E-02
1.84E-02
4.92E-02
2.02E-02
8.55E-03
6.65E+01
6.51E+01
3.44E+00
1.58E+01
8.60E+01
6.76E+00
1.20E+00
2.11E-01
7.73E-02
4.72E-01
4.23E+00
4.39E+00
4.38E-02

Avg.

10.24% 16.44% 22.78% 12.12% 9.06E+04

1.21E+03

2.56E+03

1.43E+01

17

Table 4: DSE result comparison among gnn4hls and prior-knowl.

HLS design

ADRS

# of syntehsis ADRS

# of synthesis

gnn4hls

prior-knowl

ncubed
bbgemm
ellpack
hist
init
last_step_scan
sum_scan
local_scan
update
ss_sort
stencil2d
stencil3d
knn
get_delta_matrix_weights1
get_delta_matrix_weights2
get_delta_matrix_weights3
product_with_bias_input_layer
product_with_bias_second_layer
product_with_bias_output_layer
take_difference
get_oracle_activations1
get_oracle_activations2
update_weights

0.043 ± 0.042
0.016 ± 0.018
0.032 ± 0.02
0.107 ± 0.199
1.3 ± 2.36
0.211 ± 0.166
0.030 ± 0.02
0.167 ± 0.24
0.002 ± 0.002
0.042 ± 0.02
0.193 ± 0.24
0.115 ± 0.19
0.006 ± 0.004
0.087 ± 0.036
0.054 ± 0.017
0.087 ± 0.037
0.005 ± 0.005
0.0001 ± 0.001
0.003 ± 0.013
0.002 ± 0.005
0.048 ± 0.022
0.013 ± 0.009
0.0002 ± 0.0002

146 ± 27
148 ± 29
115 ± 21
119 ± 20
61 ± 30
104 ± 19
143 ± 17
88 ± 10
97 ± 17
42 ± 10
90 ± 20
97 ± 28
284 ± 42
427 ± 73
525 ± 71
568 ± 88
215 ± 36
49 ± 18
71 ± 32
224 ± 56
220 ± 29
244 ± 41
63 ± 24

0.012
0.007
0.034
0.007
0.078
0.004
0.136
0.005
0.009
0.0005
0.015
1.88
0.006
0.002
0.010
0.030
3.560
0
2.5E-5
0.0002
2.907
0.051
1.1E-5

35
46
65
46
68
90
25
71
28
21
46
16
25
139
77
222
3
30
24
8
67
19
3

B.2 Design space exploration experiment

For the DSE experiments we pretrained a model for each one of the available functions using a
leave-one-out approach. Then we ﬁnetuned each pretrained model on the target domain as described
in Section 4.3. The model architecture here is the same used for the previous experiment. In the
ﬁne-tuning stage we used Adam with a constant learning rate of 0.001 and clipping the gradient norm
to 10. To evaluate the impact of the initial random sampling the ﬁnetuning runs were repeated 20
times with different random seeds.

Table 4 reports the DSEs result obtained by the gnn4hls framework compared against the prior-
knowl. approach from Ferretti et al. [14]. The table lists, for all the considered function, the ADRS
values obtained and the number of synthesis required by the methodologies. For gnn4hls results are
averaged over 40 different runs and we report the standard deviations.

18

