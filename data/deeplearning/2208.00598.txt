A Real-time Edge-AI System for Reef Surveys

Yang Li†
Torsten Merz† Joey Crosswell★ Andy Steven★ Lachlan Tychsen-Smith†

Jiajun Liu† Brano Kusy† Ross Marchant◦† Brendan Do†

David Ahmedt-Aristizabal†

Jeremy Oorloff†
Russ Babcock★ Megha Malpani⋄ Ard Oerlemans⋄∗

Peyman Moghadam†

2
2
0
2

g
u
A
1

]

G
L
.
s
c
[

1
v
8
9
5
0
0
.
8
0
2
2
:
v
i
X
r
a

ABSTRACT
Crown-of-Thorn Starfish (COTS) outbreaks are a major cause
of coral loss on the Great Barrier Reef (GBR) and substan-
tial surveillance and control programs are ongoing to man-
age COTS populations to ecologically sustainable levels. In
this paper, we present a comprehensive real-time machine
learning-based underwater data collection and curation sys-
tem on edge devices for COTS monitoring. In particular, we
leverage the power of deep learning-based object detection
techniques, and propose a resource-efficient COTS detector
that performs detection inferences on the edge device to as-
sist marine experts with COTS identification during the data
collection phase. The preliminary results show that several
strategies for improving computational efficiency (e.g., batch-
wise processing, frame skipping, model input size) can be
combined to run the proposed detection model on edge hard-
ware with low resource consumption and low information
loss.

CCS CONCEPTS
• Computer systems organization → Real-time systems;
• Computing methodologies → Machine learning algo-
rithms.

KEYWORDS
Detection dataset, Real-time object detection and tracking

1 INTRODUCTION
Australia’s Great Barrier Reef (GBR), a national icon and a
World Heritage Site in Australia, has now been under threat
because of the Crown-of-Thorns Starfish (COTS) population
explosion. COTS are a common reef species that feed on
corals and have been an integral part of coral reef ecosys-
tems. However, their populations can explode to thousands
of starfish on individual reefs, eating up most of the coral and
leaving behind a white-scarred reef that will take years to

∗Affiliations: †CSIRO’s Data61, ★CSIRO Oceans & Atmosphere, ◦Queensland
University of Technology, ⋄Google
{yang.li1,
Emails:
torsten.merz,
dan.do,
smith, david.ahmedtaristizabal,
russ.babcock}@csiro.au, {mmalpani, ardoerlemans}@google.com

bren-
laclan.tychsen-
peyman.moghadam,

ross.marchant,

jeremy.oorloff,

joey.crosswell,

andy.steven,

brano.kusy,

jiajun.liu,

Figure 1: The edge ML box deployed on the boat facil-
itates reef surveys.

recover. To protect the reef environment, Australian agencies
are running a major monitoring and management program
to track and control COTS populations to ecologically sus-
tainable levels. One of the key survey methods used for COTS
surveillance is the Manta Tow method, in which a snorkel-
diver is towed behind a boat to perform a visual assessment
of the underwater habitat. Information from such surveys
is used to identify early or existing outbreaks of COTS and
provides key information for the decision support systems
to better target the deployment of COTS control teams. Nev-
ertheless, COTS are cryptic animals and like to hide, thus
in many cases, only part of the starfish body might be visi-
ble. To help marine experts find COTS more effectively, we
propose to collect underwater video data along the defined
transect paths for COTS monitoring and deploy a machine
learning-driven solution to find all visible COTS. Specifically,
we introduce the COTS Underwater Data Analytics System
(COTS-UDAS) which deploys a lightweight object detection
model to automatically find COTS in the captured under-
water video stream. In reef surveys, as shown in Figure 1,
we deploy our system onto the edge ML box that is built
around an Nvidia Jetson Xavier device equipped with a GPS
receiver. The entire box is powered by a portable battery
for 4-hour operations on the boat in the ocean. The system
allows marine experts to review the detection results in the
field to improve survey speed and efficiency.

Survey DevicesReal-time Edge ML BoxEdge ML processing deviceManta TowProSquidGoProVisualisation 
 
 
 
 
 
Figure 2: The overall workflow of data collection, curation, and analytics on our edge device.

One challenge we faced in designing the COTS-UDAS was
that most off-the-shelf real-time object detectors on server
standard operations, such as EfficientDet [3], FCOS [4] and
YOLO [5], are compute-intensive. For example, YOLO takes
about 20 billion floating point operations per second (FLOPS).
It is thus challenging to run these object detectors on an edge
device in a real-time manner without skipping a large num-
ber of frames. To this end, we discuss the factors that may
affect the inference speed of the detector and propose a real-
time target detection algorithm based on Scaled-YOLOv4 [5]
on edge devices. We further integrate an object tracker into
the detection module, which infers the position of each COTS
based on the consecutive frames by optical flow, and can be
helpful in capturing those COTS that are only visible at spe-
cific camera angles.

2 COTS UNDERWATER DATA

ANALYTICS SYSTEM

We collected the data by adapting underwater survey cam-
eras to the Manta Tow method. Specifically, the survey cam-
era is attached to the bottom of the manta tow board that
the diver holds during the survey. Once the survey is started,
the system follows the workflow demonstrated in Figure 2
to store the collected data and provide domain experts with
real-time survey insights. In general, there are four main
components in this workflow:
Survey Recording Devices: GoPro or ProSquid underwa-
ter recording cameras.
Data Importer: A launched process pulls frames from the
survey video, converts them into JPG images, and writes out

2

images including both image data and sensor metadata (i.e.,
timestamp and geolocation).
COTS Detector&Tracker: Both COTS detector and tracker
programs are pre-loaded when the survey starts. The de-
tector consumes the new image frames coming out from
the recording devices, and performs inferences. Then, the
tracker adjusts each detected result based on the optical flow
estimated from consecutive frames.
Graphical User Interface (GUI): A desktop application in-
stalled on the edge device, which visualises live underwater
data and detection results on the screen. The same detected
COTS objects across the video frames are summarised in an
image sequence. The marine experts can review those COTS
image sequences during the survey, and select whether the
detected results are true positive or false positive objects.
Data Archiver: A program that exports the curated data to
a specified data drive.

3 COTS DETECTION AND TRACKING
We developed our COTS detection model based on Scaled-
YOLOv4 architecture [5]. COTS are usually small and like to
hide in corals, which makes them hard to identify. Therefore,
in addition to the original training data, we applied a range
of image augmentation techniques including flipping, scal-
ing, rotation, and multi-image mix-up to create more data
samples for the model training.

As mentioned in Section 1, direct adoption of deep detec-
tors on edge devices will result in extremely slow inference
speed. Thus, to minimise the detector’s prediction latency,
we propose to optimise the detector using the following

Real-time detections during data captureExport data when requestedVisualise live image frames in 10 FPS and the corresponding detection results with bounding boxesGUICOTS Detector&TrackerSurvey Camera DevicesStart SurveyEndSurveyFor each detected COTS, plot the sequence of frames that contain itSummarise sensor metadata, such as timestamp, geolocation, depth, etc.Data ImporterRecord underwater videosPull frames from the survey videosStore the camera and sensor metadataTrack each detected COTS object based on the detection results and video framesFlagged as true positiveIs the detected object a real COTS?Flagged as false positiveData ArchiverYesNostrategies1:
Input size: Intuitively, large-sized images fed into the model
increase the processing time, since the convolutional neural
network (CNN) kernel needs more steps to scan through an
image. Thus, reducing the size of the images obtained from
the survey recording devices can significantly decrease the
processing time.
Batch Size: Instead of processing images one by one in a
video sequence, processing image frames in a batch-wise
manner, i.e., a number of images are fed into the detector at
the same time, can improve the computational efficiency.
Frame Skipping: Since COTS are moving very slowly, we
observe that the same detected COTS objects appear in hun-
dreds of frames in a video frame sequence, and their positions
only change a little between two consecutive frames. Hence,
selectively running the detector on video frames enhances
the resource consumption efficiency on edge devices.
Buffer Queue: The different processing rates of the data
importer, detector and tracker hinder the compute efficiency
since some components have to wait until others finish. To
alleviate this issue and enable parallel computing in our sys-
tem, we introduce two buffer queues that store the input
image frames and prediction results, respectively. In this
way, the detector and tracker can run in parallel as long as
the buffer queues are not empty.

Finally, we describe the details of how the strategies above

are employed in our detection module in the following:
COTS Object Detection: To optimise the detector inference
speed, the model performs detection of the queued images
in batches with mixed precision. All the output detections
that have above-threshold confidence scores are then stored
in another queue shared with the COTS object tracker.
COTS Object Tracking: The COTS object tracker is run-
ning in a separate thread, which works in parallel with the
detector. It obtains the detections from the shared queue
chronologically and estimates each existing track’s new ob-
ject position in the current frame through the optical flow.
For each detection, if it matches any existing track, it will
be linked to that track, otherwise, it will be saved in a new
track. The tracking results are finally returned to the system
interface for visualisation and further analysis.

4 EXPERIMENTS
To verify the efficiency and effectiveness of our detection
model in COTS-UDAS, we conducted preliminary experi-
ments on our public COTS dataset [1]. In particular, it has
three survey transect videos (24,554 images) in the training
set and one video (11,005 images) in the test set. We first

1Our inference pipeline is available at https://github.com/tensorflow/
models/tree/master/official/projects/cots_detector.

3

Table 1: Comparison of the prediction accuracy and
latency of our detection model with different system
parameters.

Model
YOLOv4 (batch size 1, 1080p, w/o TensorRT)
YOLOv4 (batch size 4, 1080p, w/o TensorRT)
YOLOv4 (batch size 4, 720p, w/o TensorRT)
YOLOv4 (batch size 4, 720p, with TensorRT)

F2 Score FPS

0.56
0.56
0.53
0.52

2
5
14
22

train the detector on a cloud server with Nvidia RTX 3090
GPU and then deploy it on Xavier during the test phase. The
evaluation metrics F2 score [2] and FPS are adapted for the
prediction effectiveness and efficiency, respectively. We re-
port the model performance over various settings in Table 1.
From the table, we can have the following observations:
(1) Our model, with a higher input size, i.e., 1080 × 1080,
achieves the best performance as more detailed patterns of
COTS can be captured by the model. However, the large
model size results in high processing latency (2FPS) on edge
devices. Even when the batch processing strategy is em-
ployed, the processing speed (i.e., 5 FPS when batch size is 4)
still cannot meet the real-time requirements.
(2) By scaling down the input size from 1080p to 720p, the
model’s F2 score performance drops slightly from 0.56 to 0.53
but significantly reduces the prediction latency per frame,
which increases from 5 FPS to 14 FPS.
(3) We further optimise the computational graph generated
in TensorFlow with TensorRT. TensorRT brings a 57% infer-
ence speed boost for the 720p model (i.e., from 14 FPS to 22
FPS) and still preserves decent prediction accuracy (i.e., 0.52
F2 score) in the meantime.

5 CONCLUSION
In this paper, we present a real-time underwater analytics
system that employs object detection techniques to facilitate
the data collection and curation process for COTS monitor-
ing. From the preliminary experimental results, we verify the
efficacy of the designed strategies for real-time processing.
In the future, we plan to explore more resource-efficient tech-
niques and extend our system to other reef survey activities
such as seagrass coverage estimation and multi-class marine
species detection.

REFERENCES
[1] Jiajun Liu, Brano Kusy, Ross Marchant, and Others. 2021. The CSIRO

Crown-of-Thorn Starfish Detection Dataset. CoRR (2021).

[2] Yutaka Sasaki. 2007. The truth of the F-measure. Teach Tutor Mater

(2007).

[3] Mingxing Tan, Ruoming Pang, and Quoc V Le. 2020. Efficientdet: Scal-

able and efficient object detection. In CVPR.

[4] Zhi Tian, Chunhua Shen, Hao Chen, and Tong He. 2019. Fcos: Fully

convolutional one-stage object detection. In CVPR.

[5] Chien-Yao Wang, Alexey Bochkovskiy, and Hong-Yuan Mark Liao. 2021.

Scaled-YOLOv4: Scaling Cross Stage Partial Network. In CVPR.

