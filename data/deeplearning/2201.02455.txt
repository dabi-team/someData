Deep Learnable Strategy Templates for Multi-Issue Bilateral
Negotiation

Pallavi Bagga
Royal Holloway, University of London
Egham, United Kingdom
pallavi.bagga@rhul.ac.uk

Nicola Paoletti
Royal Holloway, University of London
Egham, United Kingdom
nicola.paoletti@rhul.ac.uk

Kostas Stathis
Royal Holloway, University of London
Egham, United Kingdom
kostas.stathis@rhul.ac.uk

2
2
0
2

n
a
J

7

]

A
M

.
s
c
[

1
v
5
5
4
2
0
.
1
0
2
2
:
v
i
X
r
a

ABSTRACT
We study how to exploit the notion of strategy templates to learn
strategies for multi-issue bilateral negotiation. Each strategy tem-
plate consists of a set of interpretable parameterized tactics that are
used to decide an optimal action at any time. We use deep reinforce-
ment learning throughout an actor-critic architecture to estimate
the tactic parameter values for a threshold utility, when to accept
an offer and how to generate a new bid. This contrasts with existing
work that only estimates the threshold utility for those tactics. We
pre-train the strategy by supervision from the dataset collected
using â€œteacher strategiesâ€, thereby decreasing the exploration time
required for learning during negotiation. As a result, we build auto-
mated agents for multi-issue negotiations that can adapt to different
negotiation domains without the need to be pre-programmed. We
empirically show that our work outperforms the state-of-the-art in
terms of the individual as well as social efficiency.

KEYWORDS
Multi-Issue Negotiation, Deep Reinforcement Learning, Bilateral
Automated Negotiation, Interpretable Negotiation Strategies

1 INTRODUCTION
We are concerned with the problem of modelling a self-interested
agent negotiating with an opponent over multiple issues while
learning to optimally adapt its strategy. For instance, an agent
trying to buy a laptop, settles the price of a laptop on the behalf of
its owner based on a number of other issues such as laptop type,
delivery time, payment methods and location delivery [18].

For realistic and complex environments, we assume that our
agent has no previous knowledge of the opponentâ€™s preferences
and its negotiating characteristics [5]. Also, the utility of offers ex-
changed during the negotiation decreases over time (in negotiation
scenarios with a discount factor), thus, timely decision on rejecting
or accepting an offer and making acceptable offers are substan-
tial [16]. Moreover, in a multi-issue negotiation, there are likely
to be a number of different offers at any given utility level. Since
they all result in the same utility, our agent is indifferent between
these offers. So, there is another challenge to select the best offer
which maximizes the utility to the opponent, whilst maintaining
our desired utility level (i.e., to aim for the â€œwin-winâ€ solution) [32].
Existing work consists of four main approaches addressing the
above-mentioned challenges. (a) Hand-crafted predefined heuris-
tics â€“ these are proposed in a number of settings with competitive
results [13], and although interpretable (e.g., [1, 2]), they are of-
ten characterized by ad-hoc parameter/weight settings that are

difficult to adapt for different domains. (b) Meta-heuristic (or evolu-
tionary) methods â€“ work well across domains and improve itera-
tively using a fitness function (as a guide for quality); however, in
these approaches every time an agent decision is made, this needs
to be delivered by the meta-heuristic, which is not efficient and
does not result in a human-interpretable and reusable negotiation
strategy. (c) Machine learning algorithms â€“ they show the best
results with respect to run-time adaptability [8, 27], but often their
working hypotheses are not interpretable, a fact that may hinder
their eventual adoption by users due to lack of transparency in the
decision-making that they offer. (d) Interpretable strategy templates
â€“ developed in [10] to guide the use of a series of tactics whose op-
timal use can be learned during negotiation. The structure of such
templates depends upon a number of learnable choice parameters,
determining which acceptance and bidding tactic to employ at any
particular time during negotiation. As these tactics represent hy-
potheses to be tested, defined by the agent developer, they can be
explained to a user, and can in turn depend on learnable parame-
ters. The outcome of this work is an agent model that formulates
a strategy template for bid acceptance and generation so that an
agent that uses it can make optimal decisions about the choice of
tactics while negotiating in different domains [10].

The benefit of (d) is that it can combine (a), (b) and (c) by using
heuristics for the components of the template and meta-heuristics
or machine learning for evaluating the choice parameter values
of these components. The problem with (d), however, is that the
choice parameters of the components for the acceptance and bid-
ding templates are learned once (during training) and used in all
the different negotiation settings (during testing) [10]. This one-
size-fits-all choice of tactics does not accumulate learning experi-
ence and may be unsuitable for unknown domains or unknown
opponents. In other words, the current mechanism for learning
the choice parameter values in [10] abstracts away from what is
learned in a specific domain once the negotiation has finished, and
therefore cannot transfer it to new domains or unseen opponents.
To address the limitation of (d), we propose the idea of using Deep
Reinforcement Learning (DRL) to estimate the choice parameter
values of components in strategy templates. We name the proposed
interpretable strategy templates as â€œDeep Learnable Strategy Tem-
plates (DLST)â€. Our contribution is that we study experimentally
the ideas behind DLSTs so that agents that employ them to learn
parameter values from and across negotiation experiences, hence
being capable of transferring the knowledge from one domain to
the other, or using the experience against one opponent on the
other. This approach leads to â€œadaptiveâ€ and generalizable strategy
templates. We also perform extensive evaluation experiments based

 
 
 
 
 
 
on the ANAC tournaments [22] against agents with learning capa-
bilities (readily available in GENIUS [33]) in a variety of domains
with different sizes and competitiveness levels [33], each with two
different profiles. The agents used for comparison span a wide
range of strategies and techniques1. Empirically, the DLST-based
agent negotiation model outperforms existing strategies in terms
of individual as well as social welfare utilities.

The remainder of the paper is organized as follows. In Section 2,
we discuss the previous work related to learning-based multi-issue
negotiation. In Section 3, we give a description of negotiation set-
tings considered in this paper. Then, in Section 4, the proposed
DLST-based negotiation model is introduced followed by various
methods and methodologies in Section 5. Subsequently, in Section 6,
we experimentally evaluate the performance efficiency of the pro-
posed model. We conclude in Section 7 where we also outline an
open problem worth pursuing in the future, as a result of this work.

2 RELATED WORK
Existing approaches with reinforcement learning have focused on
methods such as Tabular Q-learning for bidding [12] and finding the
optimal concession [34, 35] or DQN for bid acceptance [27], which
are not optimal for continuous action spaces. Such spaces, however,
are the main focus in this work in order to estimate the threshold tar-
get utility value below which no bid is accepted/proposed from/to
the opponent agent. Also, in order to perform this effectively, the
agents are required to conclude many prior negotiations with an
opponent in order to learn the opponentâ€™s behaviour. Consequently,
their approach, and reinforcement learning in general, is not ap-
propriate for one-off negotiation with an unknown opponent. The
recently proposed adaptive negotiation model in [8, 9] uses DRL for
continuous action spaces, but their motivation is significantly dif-
ferent to ours. In our work, the agent attempts to predict the tactic
choices for acceptance and bidding strategies at any particular time
as well as learn the threshold utility which will be used among one
of the tactics to be used in acceptance and bidding strategies, while
[8, 9] uses DRL for a complete agent strategy while negotiating with
multiple sellers concurrently in e-market like scenarios. Moreover,
we focus on building the generalized decoupled and interpretable
decision component, i.e., separate acceptance and bidding strategies
are learned based on interpretable templates containing different
tactics to be employed at different times in different domains. An-
other closely related multi-issue DRL-based negotiation work has
also been seen in [10, 11]. Unlike the use of meta-heuristic opti-
mization to learn the strategy parameter values in [10, 11] and
use it in all the negotiation settings, we use DRL and the strategy
parameter values may differ in different negotiation settings. Also,
unlike [10, 11], we abstract away from handling the user preference
uncertainties and generating the near-Pareto-optimal bids under
preference uncertainties.

3 NEGOTIATION SETTINGS
As in [10], we assume that our negotiation environment ğ¸ consists
of two agents ğ´ğ‘¢ and ğ´ğ‘œ negotiating with each other over some

1E.g., AgreeableAgent2018- Frequency-based opponent modelling, AgentHerb- Logistic
Regression, SAGA -Genetic Algorithm (GA), KakeSoba- Tabu Search, Rubick- Gaussian
distribution, Caduceus2016- Mixture of GA, algorithm portfolio and experts.

, . . . ğ‘£ğ‘–

domain ğ·. A domain ğ· consists of ğ‘› different independent issues,
ğ· = (ğ¼1, ğ¼2, . . . ğ¼ğ‘›), with each issue taking a finite set of ğ‘˜ possible
discrete or continuous values ğ¼ğ‘– = (ğ‘£ğ‘–
ğ‘˜ ). In our experiments,
1
we consider issues with discrete values. An agentâ€™s bid ğœ” is a map-
ping from each issue to a chosen value (denoted by ğ‘ğ‘– for the ğ‘–-th
issue), i.e., ğœ” = (ğ‘£ 1
ğ‘ğ‘› ). The set of all possible bids or outcomes
ğ‘1
is called outcome space Î© s.t. ğœ” âˆˆ Î©. The outcome space is common
knowledge to the negotiating parties and stays fixed during a single
negotiation session.

, . . . ğ‘£ğ‘›

Negotiation protocol. Before the agents can begin the negotiation
and exchange bids, they must agree on a negotiation protocol ğ‘ƒ,
which determines the valid moves agents can take at any state of the
negotiation [17]. Here, we consider the alternating offers protocol
[28], with possible ğ´ğ‘ğ‘¡ğ‘–ğ‘œğ‘›ğ‘  = {offer (ğœ”), accept, reject}. One of the
agents (say ğ´ğ‘¢ ) starts a negotiation by making an offer ğ‘¥ğ´ğ‘¢ â†’ğ´ğ‘œ to
the other agent (say ğ´ğ‘œ ). The agent ğ´ğ‘œ can either accept or reject
the offer. If it accepts, the negotiation ends with an agreement,
otherwise ğ´ğ‘œ makes a counter-offer to ğ´ğ‘¢ . This process of making
offers continues until one of the agents either accepts an offer
(i.e., successful negotiation) or the deadline is reached (i.e., failed
negotiation).

Time Constraints. We impose a realâ€“time deadline ğ‘¡ğ‘’ğ‘›ğ‘‘ on the
negotiation process for both theoretical and practical reasons. The
pragmatic reason is that without a deadline, the negotiation might
go on forever, especially without any discount factors. Secondly,
with unlimited time an agent may simply try a huge amount of
proposals to learn the opponentâ€™s preferences [7]. However, taking
into account a realâ€“time deadline poses many challenges, such as,
agents should be more willing to concede near the deadline, as a
break-off yields zero (or the reserved utility, if any) utility for both
agents; a realâ€“time deadline also makes it necessary to employ
a strategy to decide when to accept an offer; and deciding when
to accept involves some prediction whether or not a significantly
better opportunity might occur in the future.

Moreover, we assume that the negotiations are sensitive to time,
i.e., time impacts the utilities of the negotiating parties. In other
words, the value of an agreement decreases over time.

Negotiation session. Formally, for each negotiation session be-
tween two agents ğ´ğ‘¢ and ğ´ğ‘œ , let ğ‘ğ‘¡
âˆˆ ğ´ğ‘ğ‘¡ğ‘–ğ‘œğ‘›ğ‘  denote the
offer action proposed by agent ğ´ğ‘¢ to agent ğ´ğ‘œ at time ğ‘¡. A negotia-
tion history ğ» ğ‘¡
ğ´ğ‘¢ â†”ğ´ğ‘œ between agents ğ´ğ‘¢ and ğ´ğ‘œ until time ğ‘¡ can
be represented as in (1):

ğ´ğ‘¢ â†’ğ´ğ‘œ

)

(1)

ğ‘ğ‘›â†’ğ‘ğ‘›+1

, Â· Â· Â· , ğ‘¥ğ‘¡ğ‘›

ğ´ğ‘¢ â†”ğ´ğ‘œ := (ğ‘¥ğ‘¡1ğ‘1â†’ğ‘2
ğ» ğ‘¡

, ğ‘¥ğ‘¡2ğ‘3â†’ğ‘4
where, ğ‘¡ğ‘› â‰¤ ğ‘¡ and the negotiation actions are ordered over time.
Also, ğ‘ ğ‘— = ğ‘ ğ‘—+2, i.e., the negotiation process strictly follows the
alternating-offers protocol. Given a negotiation thread between
agents ğ´ğ‘¢ and ğ´ğ‘œ , the action performed by ğ´ğ‘¢ at time ğ‘¡ â€² after re-
ceiving an offer ğ‘¥ğ´ğ‘œ â†’ğ´ğ‘¢ at time ğ‘¡ from ğ´ğ‘œ can be one from the
set Actions if ğ‘¡ â€² < ğ‘¡ğ‘’ğ‘›ğ‘‘ , i.e., negotiation deadline is not reached.
Furthermore, we assume bounded rational agents due to the fact
that given the limited time, information privacy, and limited com-
putational resources, agents cannot calculate the optimal strategy
to be carried out during the negotiation.

Figure 1: Interaction between the components of DLST-based agent negotiation model

Utility. We assume that each negotiating agent has its own pri-
vate preference profile which describes how bids are offered over
the other bids. This profile is given in terms of a utility function
ğ‘ˆ , defined as a weighted sum of evaluation functions, ğ‘’ğ‘– (ğ‘£ğ‘–
ğ‘ğ‘– ) as
shown in (2). Each issue is evaluated separately contributing lin-
early without depending on the value of other issues and hence ğ‘ˆ
is referred to as the Linear Additive Utility space. Here, ğ‘¤ğ‘– are the
normalized weights indicating the importance of each issue to the
user and ğ‘’ğ‘– (ğ‘£ğ‘–
ğ‘ğ‘– value
of the ğ‘–ğ‘¡â„ issue to a utility.

ğ‘ğ‘– ) is an evaluation function that maps the ğ‘£ğ‘–

ğ‘ˆ (ğœ”) = ğ‘ˆ (ğ‘£ 1
ğ‘1

, . . . ğ‘£ğ‘›

ğ‘ğ‘› ) =

ğ‘›
âˆ‘ï¸

ğ‘–=1

ğ‘¤ğ‘– Â· ğ‘’ğ‘– (ğ‘£ğ‘–

ğ‘ğ‘– ), where

ğ‘›
âˆ‘ï¸

ğ‘–=1

ğ‘¤ğ‘– = 1

(2)

Whenever the negotiation terminates without any agreement,
each negotiating party gets its corresponding utility based on the
private reservation2 value (ğ‘¢ğ‘Ÿğ‘’ğ‘  ). In case the negotiation terminates
with an agreement, each agent receives the discounted utility of
the agreed bid, i.e., ğ‘ˆ ğ‘‘ (ğœ”) = ğ‘ˆ (ğœ”)ğ‘‘ğ‘¡
ğ· . Here, ğ‘‘ğ· is a discount factor
in the interval [0, 1] and ğ‘¡ âˆˆ [0, 1] is current normalized time.

4 DLST-BASED NEGOTIATION MODEL
When building a negotiation agent, we normally consider three
phases: pre-negotiation phase (i.e., estimation of agent ownerâ€™s pref-
erences, preference elicitation), negotiation phase (i.e., offer genera-
tion, opponent modelling) and post-negotiation phase (i.e., assessing
the optimality of offers) [23]. In this paper, we are interested in the
second phase, which involves a Decide component for choosing
an optimal action ğ‘ğ‘¡ , i.e., ğ» ğ‘¡ âˆ’1
ğ´ğ‘œ â†’ğ´ğ‘¢ . As in [10], we assume that our
agent ğ´ğ‘¢ is situated in an environment ğ¸ (containing the opponent

2The reservation value is the minimum acceptable utility for an agent. It may vary for
different parties and different domains. In our settings, it is the same for both parties.

agent ğ´ğ‘œ ) where, at any time ğ‘¡, ğ´ğ‘¢ senses the current state ğ‘†ğ‘¡ of ğ¸
and represents it as a set of internal attributes, as shown in Figure 1;
however this component is implicit in [10]. For the estimation of
threshold utility, the set of state attributes include information de-
rived from the sequence of previous bids offered by ğ´ğ‘œ (e.g., utility
of the most recently received bid from the opponent ğœ”ğ‘œ
ğ‘¡ , utility of
the best opponent bid so far ğ‘‚ğ‘ğ‘’ğ‘ ğ‘¡ , average utility of all the oppo-
nent bids ğ‘‚ğ‘ğ‘£ğ‘” and their variability ğ‘‚ğ‘ ğ‘‘ ) and information stored in
ğ´ğ‘¢ â€™s knowledge base (e.g., number of bids ğµ in the given partial
order, ğ‘‘ğ· , ğ‘¢ğ‘Ÿğ‘’ğ‘  , Î©, and ğ‘›), and the current negotiation time ğ‘¡. This
internal state representation, denoted with ğ‘ ğ‘¡ , is used by the agent
(in acceptance and bidding strategies) to decide what action ğ‘ğ‘¡ to ex-
ecute from the set of Actions based on the negotiation protocol ğ‘ƒ at
time ğ‘¡. Action execution then changes the state of the environment
to ğ‘†ğ‘¡ +1. The state ğ‘ ğ‘¡ for acceptance strategy involves the following
attributes in addition to the above-mentioned state attributes: fixed
target utility ğ‘¢, dynamic and learnable target utility Â¯ğ‘¢ğ‘¡ , ğ‘ˆ (ğœ”), ğ‘
quantile value which changes w.r.t time ğ‘¡, ğ‘„
ğ‘¡ ) (ğ‘). On the other
hand, the state ğ‘ ğ‘¡ for bidding strategy involves the following set
of attributes: ğ‘Boulware, ğ‘ƒğ‘† Pareto-optimal bid, ğ‘ğ‘œğ‘ğ‘ (ğœ”ğ‘œ
ğ‘¡ ), U (Î© â‰¥ Â¯ğ‘¢ğ‘¡ )
(as discussed in the subsequent section), in addition to the state
attributes used for estimating the dynamic threshold utility value.
The action ğ‘ğ‘¡ is derived via two functions, ğ‘“ğ‘ and ğ‘“ğ‘ , for the
acceptance and bidding strategies, respectively, as in [10]. The
function ğ‘“ğ‘ takes as inputs ğ‘ ğ‘¡ , a dynamic threshold utility Â¯ğ‘¢ğ‘¡ (defined
later in the Methods section), the sequence of past opponent bids
Î©ğ‘œ
ğ‘¡ , and outputs a discrete action ğ‘ğ‘¡ among accept or reject. When
ğ‘“ğ‘ returns reject, ğ‘“ğ‘ computes what to bid next, with input ğ‘ ğ‘¡ and
Â¯ğ‘¢ğ‘¡ , see (3â€“4). This separation of acceptance and bidding strategies
is not rare, see for instance [6]. Also, ğ‘“ğ‘ and ğ‘“ğ‘ consists of a set of

(cid:98)ğ‘ˆ (Î©ğ‘œ

tactics as defined in [10].
ğ‘“ğ‘ (ğ‘ ğ‘¡ , Â¯ğ‘¢ğ‘¡ , Î©ğ‘œ
ğ‘“ğ‘ (ğ‘ ğ‘¡ , Â¯ğ‘¢ğ‘¡ , Î©ğ‘œ

ğ‘¡ ) = ğ‘ğ‘¡ , ğ‘ğ‘¡ âˆˆ {accept, reject}
ğ‘¡ ) = ğ‘ğ‘¡ , ğ‘ğ‘¡ âˆˆ {offer (ğœ”), ğœ” âˆˆ Î©}

(3)

(4)

We assume incomplete opponent preference information, therefore,
Decide uses the estimated model (cid:98)ğ‘ˆğ‘œ . In particular, (cid:98)ğ‘ˆğ‘œ is estimated
at time ğ‘¡ using information from Î©ğ‘œ
ğ‘¡ , see Methods section for more
details. Unlike [10], we employ DRL in Acceptance strategy tem-
plates as well as Bidding Strategy templates in our work, in addition
to Threshold utility (represented by three green coloured boxes in
Figure 1) in Decide component. Each DRL component is actor-critic
architecture-based [30] and has its own Evaluate and Negotiation
Experience components.

Evaluate refers to a critic helping our agent learn the dynamic
threshold utility Â¯ğ‘¢ğ‘¡ , acceptance strategy template parameters and
bidding strategy template parameters, with the new experience
collected during the negotiation against each opponent agent. More
specifically, it is a function of random ğ¾ (ğ¾ < ğ‘ ) experiences
fetched from the agentâ€™s memory. Here, learning is retrospective,
since it depends on the reward ğ‘Ÿğ‘¡ obtained from ğ¸ by performing ğ‘ğ‘¡
at ğ‘ ğ‘¡ . The reward values for every critic that are used for estimating
the threshold utility (i.e., ğ‘Ÿ Â¯ğ‘¢ğ‘¡
) as well as choice parameter values
ğ‘¡
of acceptance (i.e., ğ‘Ÿğ‘ğ‘–ğ‘‘
) and bidding strategy templates (i.e., ğ‘Ÿ ğ‘ğ‘ğ‘
)
depend on the discounted user utility of the last bid received from
the opponent, ğœ”ğ‘œ
ğ‘¡ , or of the bid accepted by either parties ğœ”ğ‘ğ‘ğ‘ and
defined as (5), (6) and (7) respectively.

ğ‘¡

ğ‘¡

ğ‘Ÿ Â¯ğ‘¢ğ‘¡
ğ‘¡ =

ğ‘ˆğ‘¢ (ğœ”ğ‘ğ‘ğ‘, ğ‘¡),
ï£±ï£´ï£´ï£´ï£²
ğ‘ˆğ‘¢ (ğœ”ğ‘œ
ï£´ï£´ï£´
âˆ’1,
ï£³

ğ‘¡ , ğ‘¡),

on agreement
on received offer
otherwise.

ğ‘Ÿğ‘ğ‘–ğ‘‘
ğ‘¡

=

(cid:40)ğ‘ˆğ‘¢ (ğœ”ğ‘ğ‘ğ‘, ğ‘¡),
âˆ’1,

on agreement
otherwise.

(5)

(6)

ğ‘Ÿ ğ‘ğ‘ğ‘
ğ‘¡

=

ğ‘¡ , ğ‘¡),

ğ‘ˆğ‘¢ (ğœ”ğ‘ğ‘ğ‘, ğ‘¡),
ï£±ï£´ï£´ï£´ï£²
ğ‘ˆğ‘¢ (ğœ”ğ‘œ
ï£´ï£´ï£´
âˆ’1,
ï£³
(5) and ğ‘Ÿğ‘ğ‘–ğ‘‘

on agreement and ğ‘ˆğ‘œ (ğœ”ğ‘ğ‘ğ‘, ğ‘¡) â‰¤ ğ‘ˆğ‘¢ (ğœ”ğ‘ğ‘ğ‘, ğ‘¡)
on rejection and ğ‘ˆğ‘œ (ğœ”ğ‘œ
otherwise.

ğ‘¡ , ğ‘¡) â‰¥ ğ‘ˆğ‘¢ (ğœ”ğ‘œ

ğ‘¡ , ğ‘¡)

ğ‘¡

ğ‘Ÿ Â¯ğ‘¢ğ‘¡
ğ‘¡

(7)
(6) are straight-forward. In (7), ğ‘ˆğ‘œ (ğœ”, ğ‘¡) is used as
the reward value because reward is received from the environment
ğ¸ where the opponent agent resides. In other words, we assume that
ğ¸ has access to ğ´ğ‘œ â€™s real preferences, i.e., ğ‘ˆğ‘œ , but these preferences
are not observable by our agent ğ´ğ‘¢ . The first case of the ğ‘Ÿ ğ‘ğ‘ğ‘
deals
with an agreed bid and returns a positive reward value, if the bid
gives higher utility to our agent than the opponent. The second
case deals with a rejected bid and returns a positive reward value,
if the bid gives lower utility to our agent than the opponent. In
all other cases, it returns a negative value. Also, in (5), (6) and (7),
ğ‘ˆğ‘¢ (ğœ”, ğ‘¡) is the discounted reward of ğœ” defined as (8).

ğ‘¡

ğ‘ˆğ‘¢ (ğœ”, ğ‘¡) = ğ‘ˆğ‘¢ (ğœ”) Â· ğ‘‘ğ‘¡ , ğ‘‘ âˆˆ [0, 1]
In (8), ğ‘‘ is a temporal discount factor to encourage the agent to
negotiate without delay. We should not confuse ğ‘‘, which is typically
unknown to the agent, with the discount factor used to compute
the utility of an agreed bid (ğ‘‘ğ· ).

(8)

Negotiation Experience stores historical information about ğ‘
previous interactions of an agent with other agents. Experience
elements are of the form âŸ¨ğ‘ ğ‘¡ , ğ‘ğ‘¡ , ğ‘Ÿğ‘¡ , ğ‘ ğ‘¡ +1âŸ©, where ğ‘ ğ‘¡ is the internal
state representation of the negotiation environment ğ¸, ğ‘ğ‘¡ is the per-
formed action, ğ‘Ÿğ‘¡ is a scalar reward received from the environment
and ğ‘ ğ‘¡ +1 is the new agent state after executing ğ‘ğ‘¡ .

Strategy templates. The strategy templates of [10] are a general
form of parametric strategies for acceptance and bidding. These
strategies apply different tactics at different phases of the negoti-
ation. The total number of phases ğ‘› and the number of tactics ğ‘›ğ‘–
to choose from at each phase ğ‘– = 1, . . . , ğ‘› are the only parameters
fixed in advance. For each phase ğ‘–, the duration ğ›¿ğ‘– (i.e., ğ‘¡ğ‘–+1 = ğ‘¡ğ‘– +ğ›¿ğ‘– )
and the particular choice of tactic are learnable parameters. The
latter is encoded with choice parameters ğ‘ğ‘–,ğ‘— , where ğ‘– = 1, . . . , ğ‘›
and ğ‘— = 1, . . . , ğ‘›ğ‘– , such that if ğ‘ğ‘–,ğ‘— is true then the (ğ‘–, ğ‘—)-th tactic is
selected for phase ğ‘–. Tactics can be parametric in turn, and depend
on learnable parameters pğ‘–,ğ‘— .

We consider the same set of admissible tactics as [10]. The key
difference is that our approach allows to evolve the entire strategy
(within the space of strategies entailed by the template) at every
negotiation, which makes more adaptable and generalizable. The
tactics used for acceptance strategies are:

â€¢ ğ‘ˆğ‘¢ (ğœ”ğ‘¡ ), the estimated utility of the bid ğœ”ğ‘¡ that our agent

would propose at time ğ‘¡.

â€¢ ğ‘„ğ‘ˆğ‘¢ (Î©ğ‘œ

ğ‘¡ ) (ğ‘ Â· ğ‘¡ + ğ‘), where ğ‘ˆğ‘¢ (Î©ğ‘œ
ğ‘¡ ) is the distribution of (es-
timated) utility values of the bids in Î©ğ‘œ
ğ‘¡ , ğ‘„ğ‘ˆğ‘¢ (ğµğ‘œ (ğ‘¡ )) (ğ‘) is
the quantile function of such distribution, and ğ‘ and ğ‘ are
learnable parameters. In other words, we consider the ğ‘-th
best utility received from the agent, where ğ‘ is a learnable
(linear) function of the negotiation time ğ‘¡. In this way, this
tactic automatically and dynamically decides how much the
agent should concede at time ğ‘¡. Here, pğ‘–,ğ‘— = {ğ‘, ğ‘} .

â€¢ Â¯ğ‘¢ğ‘¡ , the dynamic DRL-based utility threshold.
â€¢ ğ‘¢, a fixed utility threshold.

The bidding tactics are:

â€¢ ğ‘Boulware, a bid generated by a time-dependent Boulware

strategy [15].

â€¢ ğ‘ƒğ‘† (ğ‘ Â· ğ‘¡ + ğ‘) extracts a bid from the set of Pareto-optimal
bids ğ‘ƒğ‘†, derived using the NSGA-II algorithm3 [14] under
ğ‘ˆğ‘¢ and (cid:98)ğ‘ˆğ‘œ . In particular, it selects the bid that assigns a
weight of ğ‘ Â· ğ‘¡ + ğ‘ to our agent utility (and 1 âˆ’ (ğ‘ Â· ğ‘¡ + ğ‘)
to the opponentâ€™s), where ğ‘ and ğ‘ are learnable parameters
telling how this weight scales with the negotiation time ğ‘¡.
The TOPSIS algorithm [20] is used to derive such a bid, given
the weighting ğ‘ Â· ğ‘¡ + ğ‘ as input. Here, pğ‘–,ğ‘— = {ğ‘, ğ‘} .

â€¢ ğ‘ğ‘œğ‘ğ‘ (ğœ”ğ‘œ

ğ‘¡ ), a tactic to generate a bid by manipulating the last
bid received from the opponent ğœ”ğ‘œ
ğ‘¡ . This is modified in a
greedy fashion by randomly changing the value of the least
relevant issue (w.r.t. ğ‘ˆ ) of ğœ”ğ‘œ
ğ‘¡ .
â€¢ ğœ” âˆ¼ U (Î© â‰¥ Â¯ğ‘¢ğ‘¡ ), a random bid above our DRL-based utility

threshold Â¯ğ‘¢ğ‘¡ 4.

3Meta-heuristics (instead of brute-force) for Pareto-optimal solutions have the potential
to deal efficiently with continuous issues.
4 U (ğ‘†) is the uniform distribution over ğ‘†, and Î©â‰¥ Â¯ğ‘¢ğ‘¡ is the subset of Î© whose bids
have estimated utility above Â¯ğ‘¢ğ‘¡ w.r.t. ğ‘ˆ .

Below, we give an example of a concrete acceptance strategy learned
with our model. We use, as we will discuss in Section 6, a specific
domain (Party) and we show how the strategy adapts in other
negotiation domains (Grocery and Outfit) against the opponent
strategy [10].

(a) Party Domain

ğ‘¡ âˆˆ [0.000, 0.0361) â†’ ğ‘ˆğ‘¢ (ğœ”ğ‘œ

ğ‘¡ ) â‰¥ max

ğ‘¡ âˆˆ [0.0361, 1.000] â†’ ğ‘ˆğ‘¢ (ğœ”ğ‘œ

ğ‘¡ ) â‰¥ max

(cid:16)
ğ‘„ğ‘ˆÎ©ğ‘œ
(cid:16)
ğ‘¢, ğ‘„ğ‘ˆÎ©ğ‘œ

ğ‘¡

ğ‘¡

(âˆ’0.20 Â· ğ‘¡ + 0.22), Â¯ğ‘¢ğ‘¡
(cid:17)

(âˆ’0.10 Â· ğ‘¡ + 0.64)

(cid:17)

(b) Grocery Domain
ğ‘¡ âˆˆ [0.000, 0.2164) â†’ ğ‘ˆğ‘¢ (ğœ”ğ‘œ

ğ‘¡ ) â‰¥ max

ğ‘¡ âˆˆ [0.2164, 0.3379) â†’ ğ‘ˆğ‘¢ (ğœ”ğ‘œ

ğ‘¡ ) â‰¥ max

ğ‘¡ âˆˆ [0.3379, 1.000] â†’ ğ‘ˆğ‘¢ (ğœ”ğ‘œ

ğ‘¡ ) â‰¥ max

(c) Outfit Domain

ğ‘¡

(cid:16)
ğ‘ˆğ‘¢ (ğœ”ğ‘¡ ), ğ‘„ğ‘ˆÎ©ğ‘œ
(cid:16)
ğ‘ˆğ‘¢ (ğœ”ğ‘¡ ), ğ‘„ğ‘ˆÎ©ğ‘œ
(cid:16)
ğ‘„ğ‘ˆÎ©ğ‘œ

ğ‘¡

ğ‘¡

(âˆ’0.55 Â· ğ‘¡ + 0.05), Â¯ğ‘¢ğ‘¡

(cid:17)

(âˆ’0.60 Â· ğ‘¡ + 1.40)

(cid:17)

(âˆ’0.22 Â· ğ‘¡ + 0.29), Â¯ğ‘¢ğ‘¡

(cid:17)

ğ‘¡ âˆˆ [0.000, 0.1545) â†’ ğ‘ˆğ‘¢ (ğœ”ğ‘œ

(âˆ’0.50 Â· ğ‘¡ + 0.70)

ğ‘¡ âˆˆ [0.1545, 0.3496) â†’ ğ‘ˆğ‘¢ (ğœ”ğ‘œ
ğ‘¡ âˆˆ [0.3496, 1.000] â†’ ğ‘ˆğ‘¢ (ğœ”ğ‘œ

ğ‘¡ ) â‰¥ ğ‘„ğ‘ˆÎ©ğ‘œ
ğ‘¡
(cid:16)
Â¯ğ‘¢ğ‘¡ , ğ‘„ğ‘ˆÎ©ğ‘œ

ğ‘¡ ) â‰¥ max

ğ‘¡

ğ‘¡ ) â‰¥ ğ‘ˆğ‘¢ (ğœ”ğ‘¡ )

(âˆ’0.50 Â· ğ‘¡ + 0.90)

(cid:17)

We can observe that the duration learned in the left-hand side of
the tactics is different for different domains, e.g., initially in the
first domain (ğ‘ƒğ‘ğ‘Ÿğ‘¡ğ‘¦) the first rule triggers when ğ‘¡ âˆˆ [0.0, 0.0361),
while in the second (ğºğ‘Ÿğ‘œğ‘ğ‘’ğ‘Ÿğ‘¦) and third (ğ‘‚ğ‘¢ğ‘¡ ğ‘“ ğ‘–ğ‘¡) domains, the first
rule triggers at ğ‘¡ âˆˆ [0.0, 0.2164) and ğ‘¡ âˆˆ [0.0, 0.1545) respectively.
Similarly, the parameters on the right-hand side of the tactics rules,
e.g., for the first domain (ğ‘ƒğ‘ğ‘Ÿğ‘¡ğ‘¦) during the very early phase of the
negotiation, the strategy uses a quantile tactic as well as dynamic
threshold utility. However, in the second domain (ğºğ‘Ÿğ‘œğ‘ğ‘’ğ‘Ÿğ‘¦), the
strategy now employs future bid utility along with the quantile
bid and the dynamic threshold utility tactics, whereas, in the third
domain (ğ‘‚ğ‘¢ğ‘¡ ğ‘“ ğ‘–ğ‘¡), it only employs the quantile bid tactic.

5 METHODS
In our approach, we first use supervised learning (SL) to pre-train
the our agent using supervision examples collected from existing
â€œteacherâ€ negotiation strategies as inspired by [9, 10]. Such pre-
trained strategy is then evolved via RL using experience and rewards
collected while interacting with other agents in the negotiation
environment. This combination of SL and RL approaches enhances
the process of learning an optimal strategy. This is because applying
RL alone from scratch would require a large amount of experience
before reaching a reasonable strategy, which might hinder the
online performance of our agent. On the other hand, starting from
a pre-trained policy ensures quicker convergence (as demonstrated
empirically in [9, 10]).

5.1 Data set collection
In order to collect the data set for pre-training our agent via SL, we
have used the GENIUS simulation environment [26]. In particular,
in our experiments we generate supervision data using the existing
DRL-based state-of-the-art agent negotiation model [10] by negoti-
ating it against the winning strategies of ANAC-2019 competition,
i.e., AgentGG, KakeSoba and SAGA (readily available in GENIUS

and requiring minimal changes to work for our negotiation settings)
assuming no user preference uncertainty in three different domains
(Laptop, Holiday, and Party).

5.2 Strategy Representation
We represent both ğ‘“ğ‘ (3) and ğ‘“ğ‘ (4) using artificial neural networks
(ANNs) [19], as these are powerful function approximators and
benefit from extremely effective learning algorithms, unlike [10],
which used the meta-heuristic optimization algorithm. We also use
the same to predict the target threshold utility Â¯ğ‘¢ğ‘¡ as in [10].

5.2.1 ANN. In particular, we use feed-forward neural networks, i.e.,
functions organized into several layers, where each layer comprises
a number of neurons that process information from the previous
layer. More details can be found in [19]. Also, we keep the ANN
configuration same as in [10].

5.2.2 DRL. During our experiments, the agent negotiates with
fixed-but-unknown opponent strategies in a negotiation environ-
ment, which can be learnt by our agent after some simulation runs.
Hence, we consider our negotiation environment as fully-observable.
Following this, for our dynamic and episodic environment, we use a
model-free, off-policy RL approach which generates a deterministic
policy based on the policy gradient method to support continuous
control. More specifically, as in [10], we use Deep Deterministic
Policy Gradient (DDPG) algorithm, which is an actor-critic RL ap-
proach and generates a deterministic action selection policy for
the negotiating agent [25]. We consider a model-free RL approach
because our problem is how to make an agent decide what action
to take next in a negotiation dialogue rather than predicting the
new state of the environment. In other words, we are not learning
a model of the environment, as the strategies of the opponents are
not observable properties of the environmentâ€™s state. Thus, our
agentâ€™s emphasis is more on learning what action to take next and
not the state transition function of the environment. We consider
the off-policy approach (i.e., an agent attempts to evaluate or im-
prove the policy which is different from the one which was used to
take an action) for independent exploration of continuous action
spaces [25]. When being in a state ğ‘ ğ‘¡ , DDPG uses a so-called actor
network ğœ‡ to select an action ğ‘ğ‘ğ‘¡ğ‘¡ , and a so-called critic network
ğ‘„ to predict the value ğ‘„ğ‘¡ at state ğ‘ ğ‘¡ of the action selected by the
actor:

(9)

ğ‘ğ‘ğ‘¡ğ‘¡ = ğœ‡ (ğ‘ ğ‘¡ | ğœƒ ğœ‡ )
ğ‘„ğ‘¡ (ğ‘ ğ‘¡ , ğ‘ğ‘ğ‘¡ğ‘¡ | ğœƒğ‘„ ) = ğ‘„ (ğ‘ ğ‘¡ , ğœ‡ (ğ‘ ğ‘¡ | ğœƒ ğœ‡ ) | ğœƒğ‘„ )
(10)
In (9) and (10), ğœƒ ğœ‡ and ğœƒğ‘„ are, respectively, the learnable param-
eters of the actor and critic neural networks. The parameters of
the actor network are updated by the Deterministic Policy Gradi-
ent method [29]. The objective of the actor policy function is to
maximize the expected return ğ½ calculated by the critic function
using (11). See [24] for further details on DDPG.
ğ½ = E[ğ‘„ (ğ‘ , ğ‘ğ‘ğ‘¡ |ğœƒğ‘„ )|ğ‘ =ğ‘ ğ‘¡ ,ğ‘ğ‘ğ‘¡ =ğœ‡ (ğ‘ ğ‘¡ ) ]
In our experiments, for predicting the dynamic threshold utility,
the actor function is a single-output regression ANN; on the other
hand, for acceptance and bidding strategies, it is a multiple-output
regression ANN. In particular, when predicting Â¯ğ‘¢ğ‘¡ , ğ‘ğ‘ğ‘¡ğ‘¡ corresponds

(11)

to Â¯ğ‘¢ğ‘¡ ; whereas, for acceptance and bidding strategy templates, ğ‘ğ‘ğ‘¡ğ‘¡
consists of a vector of multiple outputs (cid:0)ğ›¿ğ‘–, (ğ‘ğ‘–,ğ‘— , pğ‘–,ğ‘— ) ğ‘—=1,...,ğ‘›ğ‘–
ğ‘–=1,...,ğ‘›
including the duration of each negotiation phase ğ›¿ğ‘– , Boolean choice
parameters ğ‘ğ‘–,ğ‘— and a set of learnable parameters pğ‘–,ğ‘— for each tactic
ğ‘— that can be used in a negotiation phase ğ‘–.

(cid:1)

5.3 Opponent modelling
We consider a negotiation environment with uncertainty about
the opponentâ€™s preferences. To derive an estimate of the opponent
model (cid:98)ğ‘ˆğ‘œ during negotiation, we use the distribution-based fre-
quency model proposed in [31], as also done in [10]. In this model,
the empirical frequency of the issue values in Î©ğ‘œ
ğ‘¡ provides an ed-
ucated guess on the opponentâ€™s most preferred issue values. The
issue weights are estimated by analysing the disjoint windows of
Î©ğ‘œ
ğ‘¡ , giving an idea of the shift of opponentâ€™s preferences from its
previous negotiation strategy over time.

6 EXPERIMENTAL RESULTS AND

DISCUSSIONS

All the experiments are performed using the GENIUS tool [26],
which are designed to prove the following two hypotheses:

â€¢ Hypothesis A: DLST-based negotiation approach outper-
forms the â€œteacherâ€ strategies in known negotiation settings
in terms of individual and social efficiency.

â€¢ Hypothesis B: DLST-based negotiation approach outper-
forms not-seen-before strategies and adapts to different ne-
gotiation settings in terms of individual and social efficiency.

6.1 Performance metrics:
We measure the performance of each agent in terms of six widely-
adopted metrics inspired by the ANAC competition:

â€¢ ğ‘ˆ ğ‘¡ğ‘œğ‘¡ğ‘ğ‘™
ind

: The utility gained by an agent averaged over all the

â€¢ ğ‘ˆ ğ‘ 

negotiations (â†‘);
ind : The utility gained by an agent averaged over all the
successful negotiations (â†‘);

â€¢ ğ‘ˆsoc: The utility gained by both negotiating agents averaged

over all successful negotiations (â†‘);

â€¢ ğ‘ƒavg: Average minimal distance of agreements from the

Pareto Frontier (â†“).

â€¢ ğ‘†%: Proportion of successful negotiations (â†‘).

The first and second measures represent individual efficiency of
an outcome, whereas the third and fourth correspond to the social
efficiency of agreements.

6.2 Experimental settings
Our proposed DLST-based agent negotiation model is evaluated
against state-of-the-art strategies that participated in ANACâ€™17 and
ANACâ€™18, which are designed by different research groups inde-
pendently. Each agent has no information about another agentâ€™s
strategies beforehand. Details of all these strategies are available
in [3, 21]. We evaluate our approach on total of 11 negotiation
domains which are different from each other in terms of size and
opposition [4] to ensure good negotiation characteristics and to
reduce any biases. The domain size refers to the number of issues,

whereas opposition5 refers to the minimum distance from all pos-
sible outcomes to the point representing complete satisfaction of
both negotiation parties (1,1). For the experiments of Hypothesis B,
we choose readily-available 3 small-sized, 2 medium-sized, and 3
large-sized domains. Out of these domains, 2 are with high, 3 with
medium and 3 with low opposition (see [33] for more details).

For each configuration, each agent plays both roles in the nego-
tiation (e.g., buyer and seller in Laptop domain) to compensate for
any utility differences in the preference profiles. We call user profile
the agentâ€™s role along with the userâ€™s preferences. Also, we set the
ğ‘¢ğ‘Ÿğ‘’ğ‘  and ğ‘‘ğ· to their respective default values, whereas the dead-
line is set to 180s, normalized in [0, 1] (known to both negotiating
parties in advance). For NSGA-II during the Pareto-bid generation
phase, we choose the population size of 2% Ã— |Î©|, 2 generations
and mutation count of 0.1. With these hyperparameters, on our
machine6 the run-time of NSGA-II never exceeded the given time-
out of 10s for deciding an action at each turn, while being able to
retrieve empirically good solutions.

6.3 Empirical Evaluation
We evaluate and discuss the two hypotheses introduced at the
beginning of the section.

6.3.1 Hypothesis A: DLST-based agent outperforms â€œteacherâ€ strate-
gies. We performed a total of 1200 negotiation sessions7 to evaluate
the performance of DLST-based agent against the four â€œteacherâ€
strategies (ANESIA [10], AgentGG, KakeSoba and SAGA) in three
domains (Laptop, Holiday, and Party). These strategies were used
to collect the dataset in the same domains for supervised training
before the DRL process begins. Table 1 demonstrates the average
results over all the domains and profiles for each agent. Clearly,
DLST-based agent outperforms the â€œteacherâ€ strategies in terms of
individual efficiency, as well as social efficiency.

6.3.2 Hypothesis B: Adaptive behaviour of DLST-based agents. We
further evaluated the performance of DLST-based agent against the
opponent agents from ANACâ€™17 and ANACâ€™18 unseen during train-
ing and having capability of learning from previous negotiations.
For this, we performed two experiments against ANACâ€™17 and
ANACâ€™18 agents, each with a total of 29120 negotiation sessions8.
Results in Table 2 are averaged over all domains, and demonstrate
that DLST-based agent learns to make the optimal choice of tactics
to be used at run time and outperforms the other 8 strategies in
terms of ğ‘ˆ ğ‘ 
ind and ğ‘ˆsoc. We also observed that our agent outper-
forms the current state-of-the-art (ANESIA) in a tournament with
ANACâ€™17 and ANACâ€™18 strategies in all the domains used for the
purpose of evaluation as shown in Figures 2 â€“ 5. This indicates
that the DLST approach of dynamically adapting the parameters
of acceptance and bidding strategies leads consistently improve
the ANESIA approach of keeping these parameters fixed once the
agent is deployed.

5The value of opposition reflects the competitiveness between parties in the domain.
Strong opposition means a gain of one party is at the loss of the other, whereas, weak
opposition means that both parties either lose or gain simultaneously [4].
6CPU: 8 cores, 2.10GHz; RAM: 32 GB
7ğ‘› Ã— (ğ‘› âˆ’ 1)/2 Ã— ğ‘¥ Ã— ğ‘¦ Ã— ğ‘§ = 1200 where ğ‘› = 5, number of agents in a tournament;
ğ‘¥ = 2, because agents play both sides; ğ‘¦ = 3, number of domains; ğ‘§ = 20, because
each tournament is repeated 20 times.
8ğ‘› Ã— (ğ‘› âˆ’ 1)/2 Ã— ğ‘¥ Ã— ğ‘¦ Ã— ğ‘§ = 29120 where ğ‘› = 14; ğ‘¥ = 2; ğ‘¦ = 8; ğ‘§ = 20.

Agent

DLST-agent
ANESIA
KakeSoba
SAGA
AgentGG*

DLST-agent
ANESIA
KakeSoba
SAGA
AgentGG*

DLST-agent
ANESIA
KakeSoba
SAGA
AgentGG*

ğ‘ƒavg (â†“)

0.0 Â± 0.0
0.0 Â± 0.0
0.03 Â± 0.12
0.01 Â± 0.06
0.22 Â± 0.35

0.05 Â± 0.11
0.06 Â± 0.1
0.21 Â± 0.35
0.19 Â± 0.36
0.46 Â± 0.58

0.15 Â± 0.38
0.37 Â± 0.32
0.33 Â± 0.32
0.15 Â± 0.16
0.38 Â± 0.42

ğ‘ˆsoc (â†‘)

ğ‘ˆ ğ‘¡ğ‘œğ‘¡ğ‘ğ‘™
ind

Laptop Domain

(â†‘)

1.71 Â± 0.03
1.66 Â± 0.20
1.48 Â± 0.53
1.45 Â± 0.48
1.14 Â± 0.65

0.91 Â± 0.02
0.86 Â± 0.03
0.77 Â± 0.20
0.89 Â± 0.13
0.71 Â± 0.38

Holiday Domain

1.74 Â± 0.14
1.74 Â± 0.14
1.53 Â± 0.5
1.55 Â± 0.5
1.16 Â± 0.82

0.96 Â± 0.14
0.85 Â± 0.15
0.84 Â± 0.27
0.70 Â± 0.25
0.74 Â± 0.45

Party Domain

1.53 Â± 0.6
1.06 Â± 0.5
1.11 Â± 0.51
1.36 Â± 0.26
0.92 Â± 0.6

0.74 Â± 0.31
0.52 Â± 0.27
0.64 Â± 0.3
0.61 Â± 0.19
0.62 Â± 0.4

ğ‘ˆ ğ‘ 

ind

(â†‘)

0.91 Â± 0.02
0.86 Â± 0.03
0.82 Â± 0.06
0.89 Â± 0.10
0.91 Â± 0.09

0.96 Â± 0.14
0.85 Â± 0.15
0.92 Â± 0.07
0.77 Â± 0.12
0.96 Â± 0.03

0.77 Â± 0.14
0.62 Â± 0.14
0.75 Â± 0.12
0.63 Â± 0.16
0.77 Â± 0.12

ğ‘†% (â†‘)

1.00
1.00
0.94
0.99
0.78

1.00
1.00
0.91
0.91
0.67

0.87
0.83
0.84
0.87
0.71

Table 1: Performance Comparison of DLST-agent with â€œteacherâ€ strategies for all the three domains (Laptop, Holiday, and
Party - All readily available in GENIUS). Best Results are in bold. Note * means user preference uncertainty is considered.

Agent

DLST-agent
ANESIA
PonpokoAgent
ShahAgent
Mamenchis
AgentKN
Rubick
ParsCat2
SimpleAgent
AgentF
TucAgent
MadAgent
GeneKing
Farma17

DLST-agent
ANESIA
AgentHerb
AgreeableAgent
Sontag
Agent33
AngentNP1
FullAgent
ATeamAgent
ConDAgent
GroupY
Yeela
Libra
ExpRubick

ğ‘ƒavg (â†“)

ğ‘ˆsoc (â†‘)

ind
Comparison of DLST and ANESIA with ANAC 2017 Agent Strategies

ğ‘ˆ ğ‘¡ğ‘œğ‘¡ğ‘ğ‘™
ind

(â†‘)

ğ‘ˆ ğ‘ 

(â†‘)

0.0 Â± 0.0
0.0 Â± 0.0
0.70 Â± 0.49
0.54 Â± 0.54
0.50 Â± 0.05
0.0 Â± 0.0
1.08 Â± 0.0
0.54 Â± 0.54
1.08 Â± 0.0
1.18 Â± 0.0
0.08 Â± 0.29
0.67 Â± 0.05
1.08 Â± 0.0
0.77 Â± 0.49

1.17 Â± 0.12
1.16 Â± 0.12
0.44 Â± 0.70
0.79 Â± 0.79
0.80 Â± 0.80
1.17 Â± 0.0
1.00 Â± 0.0
0.80 Â± 0.08
0.90 Â± 0.0
1.07 Â± 0.06
0.90 Â± 0.03
1.09 Â± 0.17
0.99 Â± 0.14
0.44 Â± 0.70

0.90 Â± 0.0
0.70 Â± 0.25
0.62 Â± 0.19
0.57 Â± 0.07
0.66 Â± 0.16
0.65 Â± 0.05
0.50 Â± 0.09
0.66 Â± 0.16
0.57 Â± 0.14
0.51 Â± 0.0
0.65 Â± 0.38
0.57 Â± 0.0
0.75 Â± 0.0
0.65 Â± 0.19

0.93 Â± 0.0
0.76 Â± 0.26
0.93 Â± 0.04
0.64 Â± 0.04
0.82 Â± 0.18
0.65 Â± 0.05
0.64 Â± 0.04
0.82 Â± 0.04
0.57 Â± 0.14
0.81 Â± 0.0
0.52 Â± 0.16
0.57 Â± 0.0
0.67 Â± 0.24
0.93 Â± 0.04

Comparison of DLST and ANESIA with ANAC 2018 Agent Strategies

0.00 Â± 0.08
0.00 Â± 0.09
0.02 Â± 0.05
0.05 Â± 0.11
0.03 Â± 0.07
0.04 Â± 0.07
0.04 Â± 0.06
0.02 Â± 0.04
0.09 Â± 0.06
0.06 Â± 0.09
0.03 Â± 0.06
0.04 Â± 0.06
0.10 Â± 0.09
0.00 Â± 0.02

1.54 Â± 0.17
1.41 Â± 0.16
0.79 Â± 0.11
1.12 Â± 0.23
0.73 Â± 0.18
0.74 Â± 0.18
0.73 Â± 0.16
0.67 Â± 0.12
0.58 Â± 0.13
1.16 Â± 0.20
0.66 Â± 0.15
0.68 Â± 0.14
0.54 Â± 0.19
1.10 Â± 0.18

0.86 Â± 0.07
0.74 Â± 0.14
0.78 Â± 0.02
0.53 Â± 0.10
0.78 Â± 0.08
0.68 Â± 0.09
0.65 Â± 0.10
0.69 Â± 0.05
0.75 Â± 0.10
0.68 Â± 0.11
0.53 Â± 0.07
0.73 Â± 0.08
0.71 Â± 0.08
0.78 Â± 0.08

0.87 Â± 0.06
0.84 Â± 0.14
0.78 Â± 0.11
0.56 Â± 0.05
0.79 Â± 0.07
0.78 Â± 0.09
0.65 Â± 0.1
0.77 Â± 0.12
0.75 Â± 0.08
0.65 Â± 0.11
0.54 Â± 0.06
0.73 Â± 0.07
0.56 Â± 0.04
0.80 Â± 0.12

ğ‘†% (â†‘)

1.0
0.89
0.89
0.75
0.89
1.0
0.76
0.57
1.0
0.89
0.69
1.0
0.63
0.79

0.91
0.78
0.61
0.54
0.59
0.79
0.69
0.61
0.75
0.56
0.58
0.66
0.77
0.91

Table 2: Performance Comparison of DLST-agent with existing strategies averaged over all the 8 domains (Airport Site, Camera,
Energy, Fitness, Flight, Grocery, Itex-Cypress, Outfit - All are readily available in GENIUS). Best Results are in bold.

Figure 2: Comparison of DLST-agent VS ANESIA in terms of
Agreement rate ğ‘†% (â†‘)

Figure 4: Comparison of DLST-agent VS ANESIA in terms of
individual utility rate over successful negotiations ğ‘ˆ ğ‘ 
ind

(â†‘)

Figure 3: Comparison of DLST-agent VS ANESIA in terms of
Social welfare utility ğ‘ˆsoc (â†‘)

7 CONCLUSIONS AND FUTURE WORK
This work uses an actor-critic architecture based deep reinforce-
ment learning to support negotiation in domains with multiple
issues. In particular, it exploits â€œinterpretableâ€ strategy templates
used in the state-of-the-art to learn the best combination of accep-
tance and bidding tactics at any negotiation time, and among its
tactics, it uses an adaptive threshold utility, all learned using the
DDPG algorithm which derives an initial neural network strategy
via supervised learning. We have empirically evaluated the perfor-
mance of our DLST-based approach against the â€œteacher strategiesâ€
as well as the agent strategies of ANACâ€™17 and ANACâ€™18 com-
petitions (since the tournament allowed learning from previous
negotiations) in different settings, showing that our agent outper-
forms opponents known at training time and can effectively transfer
its knowledge to environments with previously unseen opponent
agents and domains.

Figure 5: Comparison of DLST-agent VS ANESIA in terms of
individual utility rate over all negotiations ğ‘ˆ ğ‘¡ğ‘œğ‘¡ğ‘ğ‘™

(â†‘)

ind

An open problem worth pursuing in the future is how to learn
transferable strategies for concurrent bilateral negotiations over
multiple issues.

REFERENCES
[1] Bedour Alrayes, Ozgur Kafali, and Kostas Stathis. 2014. CONAN: a heuristic strat-
egy for COncurrent Negotiating AgeNts. In Proceedings of the 2014 international
conference on Autonomous agents and multi-agent systems. 1585â€“1586.

[2] Bedour Alrayes, Ã–zgÃ¼r KafalÄ±, and Kostas Stathis. 2018. Concurrent bilateral
negotiation for open e-markets: the CONAN strategy. Knowledge and Information
Systems 56, 2 (2018), 463â€“501.

[3] Reyhan AydoÄŸan, Katsuhide Fujita, Tim Baarslag, Catholijn M Jonker, and
Takayuki Ito. 2018. ANAC 2017: Repeated multilateral negotiation league. In
International Workshop on Agent-Based Complex Automated Negotiation. Springer,
101â€“115.

[4] Tim Baarslag, Katsuhide Fujita, Enrico H Gerding, Koen Hindriks, Takayuki Ito,
Nicholas R Jennings, Catholijn Jonker, Sarit Kraus, Raz Lin, Valentin Robu, et al.
2013. Evaluating practical negotiating agents: Results and analysis of the 2011
international competition. Artificial Intelligence 198 (2013), 73â€“103.

[30] Richard S Sutton and Andrew G Barto. 2018. Reinforcement learning: An intro-

duction. MIT press.

[31] Okan TunalÄ±, Reyhan AydoÄŸan, and Victor Sanchez-Anguix. 2017. Rethinking fre-
quency opponent modeling in automated negotiation. In International Conference
on Principles and Practice of Multi-Agent Systems. Springer, 263â€“279.

[32] Colin R Williams, Valentin Robu, Enrico H Gerding, and Nicholas R Jennings.
2012. Iamhaggler: A negotiation agent for complex environments. In New Trends
in Agent-based Complex Automated Negotiations. Springer, 151â€“158.

[33] Colin R Williams, Valentin Robu, Enrico H Gerding, and Nicholas R Jennings.
2014. An overview of the results and insights from the third automated negoti-
ating agents competition (ANAC2012). Novel Insights in Agent-based Complex
Automated Negotiation (2014), 151â€“162.

[34] Yoshiaki Yasumura, Takahiko Kamiryo, Shohei Yoshikawa, and Kuniaki Uehara.
2009. Acquisition of a concession strategy in multi-issue negotiation. Web
Intelligence and Agent Systems: An International Journal 7, 2 (2009), 161â€“171.
[35] Shohei Yoshikawa, Yoshiaki Yasumura, and Kuniaki Uehara. 2008. Strategy
acquisition on multi-issue negotiation without estimating opponentâ€™s preference.
In KES International Symposium on Agent and Multi-Agent Systems: Technologies
and Applications. Springer, 371â€“380.

[5] Tim Baarslag, Mark JC Hendrikx, Koen V Hindriks, and Catholijn M Jonker. 2016.
Learning about the opponent in automated bilateral negotiation: a comprehensive
survey of opponent modeling techniques. Autonomous Agents and Multi-Agent
Systems 30, 5 (2016), 849â€“898.

[6] Tim Baarslag, Koen Hindriks, Mark Hendrikx, Alexander Dirkzwager, and
Catholijn Jonker. 2014. Decoupling negotiating agents to explore the space
of negotiation strategies. In Novel Insights in Agent-based Complex Automated
Negotiation. Springer, 61â€“83.

[7] Tim Baarslag, Koen Hindriks, Catholijn Jonker, Sarit Kraus, and Raz Lin. 2012.
The first automated negotiating agents competition (ANAC 2010). In New Trends
in agent-based complex automated negotiations. Springer, 113â€“135.

[8] Pallavi Bagga, Nicola Paoletti, Bedour Alrayes, and Kostas Stathis. 2020. A
Deep Reinforcement Learning Approach to Concurrent Bilateral Negotiation.
In Proceedings of the Twenty-Ninth International Joint Conference on Artificial
Intelligence, IJCAI 2020. ijcai.org, 297â€“303.

[9] Pallavi Bagga, Nicola Paoletti, Bedour Alrayes, and Kostas Stathis. 2021.
ANEGMA: an automated negotiation model for e-markets. Autonomous Agents
and Multi-Agent Systems 35, 2 (2021), 1â€“28.

[10] Pallavi Bagga, Nicola Paoletti, and Kostas Stathis. 2020. Learnable strategies for
bilateral agent negotiation over multiple issues. arXiv preprint arXiv:2009.08302
(2020).

[11] Pallavi Bagga, Nicola Paoletti, and Kostas Stathis. 2021. Pareto Bid Estimation
for Multi-Issue Bilateral Negotiation under User Preference Uncertainty. In 2021
IEEE International Conference on Fuzzy Systems (FUZZ-IEEE). IEEE, 1â€“6.

[12] Jasper Bakker, Aron Hammond, Daan Bloembergen, and Tim Baarslag. 2019.
RLBOA: A Modular Reinforcement Learning Framework for Autonomous Nego-
tiating Agents.. In AAMAS. 260â€“268.

[13] Stefania Costantini, Giovanni De Gasperis, Alessandro Provetti, and Panagiota
Tsintza. 2013. A heuristic approach to proposal-based negotiation: with applica-
tions in fashion supply chain management. Mathematical Problems in Engineering
2013 (2013).

[14] Kalyanmoy Deb, Amrit Pratap, Sameer Agarwal, and TAMT Meyarivan. 2002. A
fast and elitist multiobjective genetic algorithm: NSGA-II. IEEE transactions on
evolutionary computation 6, 2 (2002), 182â€“197.

[15] S Shaheen Fatima, Michael Wooldridge, and Nicholas R Jennings. 2001. Optimal
negotiation strategies for agents with incomplete information. In International
Workshop on Agent Theories, Architectures, and Languages. Springer, 377â€“392.

[16] Shaheen S Fatima, Michael Wooldridge, and Nicholas R Jennings. 2002. Multi-
issue negotiation under time constraints. In Proceedings of the first international
joint conference on Autonomous agents and multiagent systems: part 1. 143â€“150.

[17] Shaheen S Fatima, Michael Wooldridge, and Nicholas R Jennings. 2005. A compar-
ative study of game theoretic and evolutionary models of bargaining for software
agents. Artificial Intelligence Review 23, 2 (2005), 187â€“205.

[18] S Shaheen Fatima, Michael J Wooldridge, and Nicholas R Jennings. 2006. Multi-
issue negotiation with deadlines. Journal of Artificial Intelligence Research 27
(2006), 381â€“417.

[19] Ian Goodfellow, Yoshua Bengio, and Aaron Courville. 2016. Deep learning. MIT

press.

[20] Ching-Lai Hwang and Kwangsun Yoon. 1981. Methods for multiple attribute
decision making. In Multiple attribute decision making. Springer, 58â€“191.
[21] Catholijn Jonker, Reyhan Aydogan, Tim Baarslag, Katsuhide Fujita, Takayuki Ito,
and Koen Hindriks. 2017. Automated negotiating agents competition (ANAC). In
Proceedings of the AAAI Conference on Artificial Intelligence, Vol. 31.

[22] Catholijn M Jonker, Valentin Robu, and Jan Treur. 2007. An agent architec-
ture for multi-attribute negotiation using incomplete preference information.
Autonomous Agents and Multi-Agent Systems 15, 2 (2007), 221â€“252.

[23] Usha Kiruthika, Thamarai Selvi Somasundaram, and S Kanaga Suba Raja. 2020.
Lifecycle model of a negotiation agent: A survey of automated negotiation tech-
niques. Group Decision and Negotiation 29, 6 (2020), 1239â€“1262.

[24] Timothy P Lillicrap, Jonathan J Hunt, Alexander Pritzel, Nicolas Heess, Tom Erez,
Yuval Tassa, David Silver, and Daan Wierstra. 2015. Continuous control with
deep reinforcement learning. arXiv preprint arXiv:1509.02971 (2015).

[25] Timothy Paul Lillicrap, Jonathan James Hunt, Alexander Pritzel, Nicolas Heess,
Tom Erez, Yuval Tassa, David Silver, and Daan Wierstra. 2016. Continuous
control with deep reinforcement learning. In Proceedings of the 4th International
Conference on Learning Representations (ICLR 2016).

[26] Raz Lin, Sarit Kraus, Tim Baarslag, Dmytro Tykhonov, Koen Hindriks, and
Catholijn M Jonker. 2014. Genius: An integrated environment for supporting the
design of generic automated negotiators. Computational Intelligence 30, 1 (2014),
48â€“70.

[27] Yousef Razeghi, Celal Ozan Berk Yavaz, and Reyhan AydoÄŸan. 2020. Deep re-
inforcement learning for acceptance strategy in bilateral negotiations. Turkish
Journal of Electrical Engineering & Computer Sciences 28, 4 (2020), 1824â€“1840.

[28] Ariel Rubinstein. 1982. Perfect equilibrium in a bargaining model. Econometrica:

Journal of the Econometric Society (1982), 97â€“109.

[29] David Silver, Guy Lever, Nicolas Heess, Thomas Degris, Daan Wierstra, and
Martin Riedmiller. 2014. Deterministic policy gradient algorithms. In Proceedings
of the 31st International Conference on Machine Learning.

