Multicast Scheduling for Multi-Message over
Multi-Channel: A Permutation-based Wolpertinger
Deep Reinforcement Learning Method

Ran Li, Chuan Huang, Han Zhang, and Shengpei Jiang

1

2
2
0
2

y
a
M
9
1

]
T
I
.
s
c
[

1
v
0
2
4
9
0
.
5
0
2
2
:
v
i
X
r
a

Abstract—Multicasting is an efﬁcient technique to simultane-
ously transmit common messages from the base station (BS) to
multiple mobile users (MUs). The multicast scheduling problem
for multiple messages over multiple channels, which jointly
minimizes the energy consumption of the BS and the latency
of serving asynchronized requests from the MUs, is formulated
as an inﬁnite-horizon Markov decision process (MDP) with
large discrete action space and multiple time-varying constraints,
which has not been efﬁciently addressed in the literatures. By
studying the intrinsic features of this MDP under stationary
policies and reﬁning the reward function, we ﬁrst simplify it
to an equivalent form with a much smaller state space. Then, we
propose a modiﬁed deep reinforcement learning (DRL) algorithm,
namely the permutation-based Wolpertinger deep deterministic
policy gradient (PW-DDPG), to solve the simpliﬁed problem.
Speciﬁcally, PW-DDPG utilizes a permutation-based action em-
bedding module to address the large discrete action space issue
and a feasible exploration module to deal with the time-varying
constraints. Moreover, as a benchmark, an upper bound of the
considered MDP is derived by solving an integer programming
problem. Numerical results validate that the proposed algorithm
achieves close performance to the derived benchmark.

Index Terms—Multicast, deep reinforcement learning (DRL),
Markov decision process (MDP), permutation-based Wolper-
tinger deep deterministic policy gradient (PW-DDPG)

I. INTRODUCTION

The number of mobile users (MUs) and mobile data trafﬁc
keep increasing rapidly in recent years. In 2023, the number of
smartphones is projected to grow from 4.9 billion in 2018 to
6.7 billion [1] and the mobile data trafﬁc is expected to reach
130 exabytes per month, which is 5-fold of the monthly data
trafﬁc in 2018 [2]. Remarkably, video data trafﬁc, including
live streaming, virtual reality (VR) video, and augmented
reality (AR) video, which has a high similarity among the
contents requested by different MUs, is expected to make up
79 percentage of the total mobile data trafﬁc in 2023 [2]. The
current cellular systems can hardly fulﬁll such heavy tasks
with the unicast technique for the downlink transmissions,
by which the base station (BS) assigns individual channel to

R. Li is with the School of Science and Engineering (SSE) and the Future
Network of Intelligence Institute (FNii), The Chinese University of Hong
Kong, Shenzhen 518172, China (e-mail: ranli2@link.cuhk.edu.cn).

C. Huang is with the School of Science and Engineering (SSE) and Future
Network of Intelligence Institute (FNii), The Chinese University of Hong
Kong, Shenzhen 518172, China, and with Peng Cheng Laboratory, Shenzhen
518066, China (e-mails: huangchuan@cuhk.edu.cn).

H. Zhang is with the Institute for Communication Systems, University of
Surrey, Surrey GU27XH, United Kingdom (email: han.zhang@surrey.ac.uk).
S. Jiang is with the SF Technology, Shenzhen 518052, China (e-mail:

philip.jiang@sfmail.sf-express.com).

each MU’s request. By exploiting the similarity of the data
requests, multicast technique can temporarily hold the MU’s
requests until there are sufﬁcient amount of them, and then
serve them simultaneously during one multicast transmission.
Apparently, exploiting multicast in the future cellular network
could signiﬁcantly improve the energy efﬁciency (deﬁned
the
as the average energy cost per request) [3]. Notably,
asynchronous requests from multiple MUs would be served
with different latencies in this multicast mechanism. Therefore,
it is of great interest to balance the energy efﬁciency and
the average latency of the considered multicast scheme in the
cellular networks.

A. Related works

In recent years, many works have studied the multicast
scheduling problem in the cellular networks, especially in
the 5G era where the cellular networks are assisted by the
advanced techniques such as millimeter wave (mmWave)
communication [4], non-orthogonal multiple access (NOMA)
[5], and intelligent reﬂecting surface (IRS) [6]. One major goal
is to maximize the system throughput with limited resources,
and approaches including deep reinforcement learning (DRL),
beamforming, were validated to have promising performances
in [4], [6]–[8]. Along another research avenue, due to the
increasing public attention to green communications, energy
efﬁciency maximization becomes more compelling in the
recent researches [5], [9]–[11] and aims to serve more requests
with as little energy as possible. To tackle this issue, the
authors in [5], [9] utilized beamforming and designed the
optimal precoders at the BS to maximize the energy efﬁ-
ciency for the NOMA and mmWave systems, respectively.
The authors in [10] discussed the device-to-device assisted
multicast scheduling problems for the mmWave system, and
proposed a supervised learning method to maximize the energy
efﬁciency. The authors in [11] discussed the joint unicast-
multicast scheduling for the NOMA system, and proposed a
rate-splitting method to optimize both the spectral and energy
efﬁciency. Another major concern for multicast scheduling
is the fairness issue, which aims to balance the latencies
among different MUs [12], [13]. Obviously, the methods only
optimizing the energy efﬁciency in [5], [9]–[11] prefer to
allocate resources to the MUs with better channel condition,
whereas the MUs with poor channel condition would be served
with more latencies and may even be abandoned by the BS. To
jointly optimize the energy efﬁciency and the fairness, namely

 
 
 
 
 
 
the proportional fairness [14], a wise choice is to optimize
the summation of the logarithm transmission rates of all MUs
[15], where, due to the “strictly concave” and the “increasing”
features of the logarithm function,
improving the already
high transmission rate for the good-channel-condition MUs
is inferior to serving the MUs with poor channel condition.
Another approach is to gather the MU’s requests into request
queues [16]. By utilizing the Lyapunov optimization,
the
rate stability of the request queues was established, which
ensures ﬁnite latencies for the MUs’ requests. However, the
proposed algorithms in [15], [16] only gave a rough control
on the latency to be ﬁnite, and the precise dominations, such
as minimizing the latency subject to certain constraints, are
impossible under the considered framework.

To jointly optimize the energy efﬁciency and the latency in a
more ﬂexible way, the most common approach is to minimize
the weighted summation of them [17]–[20]. However, since
the information about the accumulated requests from the MUs
is required to compute the latency, the multicast scheduling
problem was formulated as an inﬁnite-horizon Markov deci-
sion process (MDP), which is a commonly known difﬁcult
problem due to “the curse of dimensionality” [21]. The authors
in [17]–[20] proposed some low-complexity methods to solve
this problem. Speciﬁcally,
the authors in [17] studied the
multicast scheduling problem where multiple MUs request one
common message and the MUs have mixed latency penalties,
and a suboptimal scheduling policy was developed based on
the optimal stopping theorem. The authors in [18] discussed
the multicast scheduling problem with multiple requested mes-
sages and single available channel, and the optimal scheduling
policy was constructed based on the classical relative value
iteration (RVI) algorithm. The authors in [19], [20] deployed
DRL algorithms to study the joint multicast scheduling and
message caching problems in the ultra dense networks and
the heterogeneous networks, respectively, and again focused
on the scenarios with multiple requested messages and single
available channel. Notably, the authors in [17]–[20] assumed
that the multicasting of each message on each channel starts
and ends synchronously, i.e., all channels are always released
from multicastings simultaneously and available at every
scheduling moment.

To the best of our knowledge, these is no existing work dis-
cussed the scheduling problem for asynchronous multicasting,
where the multicastings of different messages over different
channels consume different time durations and thus different
multicastings would start and end in an asynchronous manner.

B. Main contributions

This paper studies the most general multicast scheduling
problem in the cellular networks: a) multiple MUs randomly
request multiple messages, which are stored at the BS in
advance with the cache technique [19], [20]; b) multiple
channels are available for multicast transmissions at the BS; c)
the multicastings of different messages over different channels
consume different time durations, i.e., asynchronous multicas-
ting. d) the optimal multicast scheduling policy is supposed
to strike the optimal tradeoff between the energy efﬁciency

2

and the latency penalty. We summarize our contributions as
follows:

• We formulate the multicast scheduling problem as an
inﬁnite-horizon MDP, which is challenging due to the
large discrete action space and multiple time-varying
constraints induced by stochastic availability of channels.
By studying the intrinsic features of the considered MDP
under stationary policies and reﬁning the reward function,
we ﬁrst simplify this MDP to an equivalent form with a
much smaller state space. Then, we propose a modiﬁed
DRL algorithm based on deep deterministic policy gra-
dient (DDPG) [22], which is named as the permutation-
based Wolpertinger DDPG (PW-DDPG), to address the
simpliﬁed MDP. Compared with the conventional DDPG,
PW-DDPG contains a permutation-based action embed-
ding module and a feasible action exploration module,
where the former module embeds actions in the “permu-
tation” manner during both the ofﬂine training and online
validating procedures of PW-DDPG and can well address
the large discrete action space issue. To address the time-
varying constraints issue,
the two modules separately
restrict the exploitation and exploration modes of PW-
DDPG to only selecting feasible actions.

• We derive an upper bound of the considered MDP, which
also works as the benchmark to validate the proposed
PW-DDPG. Speciﬁcally, we ﬁrst prove that the time-
varying constraints of the considered MDP can be re-
leased as a set of time-invariant constraints, which reveal
the statistical features of the considered MDP under the
policies satisfying the time-varying constraints. Then, we
convert the released problem to a two-step optimization
problem, where the optimization problem of the ﬁrst
step purely minimizes the latency penalty and the second
step jointly minimizes the energy consumption and the
latency penalty. Finally, we successively solve the two-
step optimizations and obtain an upper bound of the
considered MDP by solving an integer programming
problem.

The remainder of this paper is organized as follows. Section
II introduces the system model and formulates the multicast
scheduling problem. Section III presents the proposed algo-
rithm. Section IV discusses the upper bound benchmark of
the multicast networks. Section V evaluates the performance
of the proposed algorithm. Finally, Section VI concludes this
paper.

II. SYSTEM MODEL AND PROBLEM FORMULATION

Consider a slotted cellular network as depicted in Fig. 1,
which consists of one BS that caches N messages and a
set of E MUs, denoted by E = {MU1, · · · , MUE}. During
each time slot, a subset En ⊆ E of the total MUs with
size En = |En| are likely to send the request to the BS for
downloading the nth message with probability αn. We consider
the case that each MU only has interest in downloading one
message and thus we have En ∩ Em = ∅, ∀n (cid:54)= m, and
E = (cid:80)N
n=1 En. The BS collects the requests from all MUs in
the beginning of each time slot and then stores the numbers

3

Figure 1: Schedule over the multi-message and multi-channel multicast system.

of the new requests for N messages in N request buffers.
Instead of immediately multicasting the messages requested
by the buffered requests, the BS may wait for several time
slots and simultaneously multicast these messages until there
are sufﬁcient number of requests in the request buffers. We
also consider that M downlink channels are available for
multicast transmissions and the multicastings of N messages
may last for multiple consecutive time slots depending on their
sizes. We formulate the multicast scheduling problem for the
above multi-message and multi-channel multicast system as
an inﬁnite-horizon MDP described as follows.

1) State: The state contains request matrix, channel avail-

ability, and channel status.

Request matrix: Denote the number of the requests for the
nth message arriving within the tth time slot as qn(t), which
follows binary distribution as qn(t) ∼ Binary(En, αn), and
the number of time slots after the previous multicasting of
the nth message as ln(t) ∈ N, where N is the set of 0 and
all natural numbers. Then, within the past ln(t) time slots, all
the arrived requests for the nth message are stored in the nth
request buffer of the BS, and we represent these requests by
the request vector qn(t), which is deﬁned as

time slot as cn,m(t) ∈ {0, 1, · · · , Tn,m − 1}: if the BS is
not multicasting the nth message over the mth channel in the
beginning of the tth time slot, cn,m(t) is set to 0; otherwise,
cn,m(t) is equal to the number of the remaining time slots
for the release of the mth channel. Then, deﬁne the channel
availability as a N -by-M -dimension matrix C(t), where the
(n, m)th entry of C(t) is cn,m(t), i.e., [C(t)](n,m) (cid:44) cn,m(t).

Channel status: Denote the number of MUs to be served
in the beginning of the tth time slot by multicasting the
nth message as Kn(t) ∈ N, where Kn(t) = (cid:80)M ∗
τ =1[qn(t)]τ
holds with [qn(t)]τ being the τ th entry of qn(t), and the
indices of these MUs as In,1(t), In,2(t), · · · , In,Kn(t)(t) ∈
{1, 2, · · · , En}. Denote the downlink channel coefﬁcient of the
link between the BS and the In,k(t)th MU and over the mth
channel as hn,m,In,k (t) ∈ C, where 1 ≤ k ≤ Kn(t) and C is
the set of all complex numbers. Then, the smallest downlink
channel gain among Kn(t) BS-MU links is given as

gn,m(t) (cid:44) min

|hn,m,In,k(t)(t)|2.

1≤k≤Kn(t)

(2)

Finally, we denote the channel status as a N -by-M -dimension
matrix G(t), with the (n, m)th entry of G(t) being gn,m(t),
i.e., [G(t)](n,m) (cid:44) gn,m(t).

qn(t) (cid:44)

(cid:104)





qn(t − 1), · · · , qn (t − ln(t)) ,
(cid:105)T

0, · · · , 0
(cid:124) (cid:123)(cid:122) (cid:125)
M ∗−ln(t) zeros
(cid:104)
qn(t − 1), · · · , qn(t − M ∗ + 1),
(cid:80)ln(t)

(cid:105)T

τ =M ∗ qn(t − τ )

ln(t) ≤ M ∗

(1)

ln(t) > M ∗.

To summary, the state of the considered system at the tth
time slot is the triple s(t) (cid:44) (Q(t), C(t), G(t)) and the state
space S is S (cid:44) QN ×M ∗
×CN ×M×GN ×M with dimensionality
of N M ∗ + 2N M , where Q, C, and G are the value spaces of
[qn(t)]τ , cn,m(t), and gn,m(t), respectively.

Here, M ∗ is the size of the request buffer and if ln(t) > M ∗,
the requests arriving before the (t − M ∗)th time slot are
accumulated in the last entry of qn(t). Finally, deﬁne the
request matrix as Q(t) (cid:44) [q1(t), q2(t), · · · , qN (t)]T , which
has the dimensionality of N -by-M ∗.

Channel availability: Each multicasting of the nth message
over the mth channel is considered to consume Tn,m ∈ Z+
consecutive time slots with Z+ being the set of all positive
integers, during which the mth channel is not available for
new multicasting. Denote the availability of the mth channel
for multicasting the nth message in the beginning of the tth

2) Action: Denote the multicast scheduling decision for
in the beginning of the tth time slot as
the mth channel
am(t) ∈ {0, 1, · · · , N }. Speciﬁcally, am(t) > 0 means to
multicast the am(t)th message over the mth channel in the
beginning of the tth time slot; am(t) = 0 means that the mth
channel is not scheduled to multicast any message. Denote
the multicast scheduling decision for all channels as a(t),
i.e., a(t) (cid:44) [a1(t), a2(t), · · · , aM (t)]T , and obviously, a(t)
is the action of the considered system. Notably, if the mth
channel has been reserved for multicasting in previous slots,
i.e., (cid:80)N
n=1 cn,m(t) > 0, it cannot be scheduled to multicast

: the      message     : the      request bufferUpload requests from MUs over uplink channelsDownlink channelsPW-DDPGCached messages1C2CNC1(1)qt−1(2)qt−(1)Nqt−1(3)qt−2(1)qt−(2)Nqt−Request buffersScheduling decisionsMulticasttransmissionsMUs in the cellular networkBSCached messages and the request buffers in the BS1()qt1C()NqtNC       requests for    ,     ,         requests for1B2BNB1()3lt=2()1lt=()2Nlt=xCxBthxthx: the MU requesting : the MU requesting 1CNC4

any new message. That is, the action a(t) is constrained by

N
(cid:88)

n=1

cn,m(t)am(t) = 0, m ∈ {1, 2, · · · , M } .

(3)

Accordingly, we denote the action space at state s(t) as As(t),
the actions satisfying the time-varying
which contains all
constraints in (3).

Channel status: We model the channel coefﬁcient between
the BS and any MU and over any channel as a stationary
ergodic process, i.e.,

Pr {hn,m,k(t + 1) = hj|hn,m,k(t) = hi}

=Pr {hn,m,k(1) = hj|hn,m,k(0) = hi}
(cid:44)Prn,m,k {hj|hi} , ∀t ∈ Z+, k ∈ {1, 2, · · · , En},

(7)

3) Transitions: The transitions are to update request matrix,
channel availability, and channel status. First, we deﬁne the
multicasting status of the nth message at the tth time slot as

where Prn,m,k {hj|hi} is a constant and hi, hj ∈ H with H
being a ﬁnite complex number set. Then, gn,m(t+1) is derived
as

bn(t) (cid:44) I

(cid:32) M
(cid:88)

m=1

(cid:33)

In (am(t))

, n ∈ {0, 1, · · · , N },

(4)

gn,m(t + 1) =

where In(x) equals 1 if x is n, and otherwise, equals 0;
I(x) equals 1 if x is positive, and otherwise, it equals 0.
Apparently, if the nth message is selected for multicasting
over some channels in the beginning of the tth time slot, bn(t)
equals 1, and otherwise, it equals 0. Then, based on bn(t), the
transitions are derived as follows.

the tth

time slot,

Request matrix: We update the request matrix in the
if the nth message is selected for
following two cases:
i.e.,
multicasting in the beginning of
bn(t) = 1, only qn(t) new requests will be buffered in
time slot and qn(t + 1) =
the next
the beginning of
[qn(t), 0, · · · , 0]T ;
if the nth message is not selected for
multicasting, i.e., bn(t) = 0. The buffered requests in the
beginning of the next time slot contain both the previously
i.e., qn(t +
buffered ones and the newly arrived ones,
1) = (cid:2)qn(t), [qn(t)T ]1:M ∗−2, [qn(t)]M ∗−1 + [qn(t)]M ∗
(cid:3)T
,
where [qn(t)T ]1:M ∗−2 is the 1-by-(M ∗ − 2)-dimension vector
containing the ﬁrst to the (M ∗ − 2)th entries of qn(t)T . To
summary, we have

qn(t + 1) =






(cid:105)T

(cid:104)
qn(t), 0, · · · , 0
(cid:124) (cid:123)(cid:122) (cid:125)
M ∗−1 zeros
qn(t), [qn(t)T ](1:M ∗−2),

(cid:104)

bn(t) = 1

(5)

[qn(t)]M ∗−1 +[qn(t)]M ∗

(cid:105)T

bn(t) = 0.

Channel availability: We update the channel availability
in the following three cases: if the mth channel has been
reserved for multicasting the nth message in previous slots,
i.e., cn,m(t) > 0. The remaining time for the release of the
mth channel decreases by one in the beginning of the next time
slot, i.e., cn,m(t + 1) = cn,m(t) − 1; if the mth channel has not
been reserved for multicasting the nth message and is going
to multicast the nth message in the beginning of the tth time
slot, i.e., cn,m(t) = 0 and am(t) = n, the mth channel will be
released after Tn,m − 1 time slots counting from the (t + 1)th
time slot, i.e., cn,m(t + 1) = Tn,m − 1; and if the mth channel
has not been reserved for multicasting the nth message and is
not going to multicast the nth message, i.e., cn,m(t) = 0 and
am(t) (cid:54)= n, cn,m(t + 1) remains 0. To summary, we have

cn,m(t + 1) =






cn,m(t)−1
Tn,m − 1
0

cn,m(t) > 0
cn,m(t) = 0, am(t) = n
cn,m(t) = 0, am(t) (cid:54)= n.

(6)






(cid:110)

min1≤k≤qn(t)|hn,m,In,k(t)(t)|2
min
min1≤k≤qn(t)|hn,m,In,k(t)(t)|2(cid:111)

gn,m(t),

bn(t) = 1

bn(t) = 0.
(8)

4) Reward: Both the energy consumption and the latency

penalty are the concerned performance metrics.

(cid:16)

m=1

(cid:80)M

is Zn,m
gn,m(t)

[23], where Zn,m = τ

Energy consumption: The energy consumption of mul-
ticasting the nth message over the mth channel within the
τ B −1(cid:17)
2 C
tth time slot
is a constant with C, B, and τ being the information bits
of the nth message to be transmitted within one time slot,
the bandwidth of the mth channel, and the duration of one
time slot, respectively. Note that the above energy consump-
tion exists only if am(t) = n or cn,m(t) > 0 holds,
when the mth channel starts to multicast the nth message or
has been reserved for multicasting the nth message. Thus,
the total energy consumption within the tth time slot
is
(cid:80)N

Zn,m
gn,m(t) (In (am(t)) + I(cn,m(t))).
n=1
Latency penalty: The requests buffered at the BS will
produce instant
latency penalty in every time slot before
they get served. Denote a function pn(τ ) ∈ R as the in-
stant latency penalty for delaying one request for the nth
message over τ time slots. Then, according to (1),
the
buffered requests in qn(t) will produce instant latency penalty
of (cid:80)M ∗
the to-
tal instant latency penalty produced at the tth time slot is
(cid:80)N

the tth time slot. Thus,

τ =1[qn(t)]τ pn(τ ) at

n=1
The reward r(t) of the considered system is deﬁned as
the weighted sum of the energy consumption and the latency
penalty, i.e.,
(cid:32)
V

In (am(t))+I(cn,m(t))

r(t) (cid:44)−

τ =1[qn(t)]τ pn(τ ).

(cid:80)M ∗

N
(cid:88)

M
(cid:88)

(cid:16)

(cid:17)

Zn,m
gn,m(t)

(9)

m=1

n=1
M ∗
(cid:88)

N
(cid:88)

(cid:33)

[qn(t)]τ pn(τ )

,

+

n=1

τ =1

where V is the tradeoff parameter.

5) Problem formulation: We aim to minimize the long-term
average reward and thus formulate the multicast scheduling
problem as

(P1) max
{a(t)}

lim
T →∞

E

{qn(t)}N

n=1,Pr{G(cid:48)|G}

s.t.

(3), (5), (6), (7), (8),

1
T

T
(cid:88)

t=1

r(t)

(10)

where the expectation is taken with respect to the request
arrival process {qn(t)}N
n=1 and the transitions on channel
status, i.e., Pr{G(cid:48)|G}. To solve problem (P1), we need to
ﬁnd the optimal policy π (cid:44) (π1, · · · , πt, · · ·), where πt is
the optimal scheduling rule at the tth time slot and maps the
history h(t) (cid:44) (s(1), a(1), · · · , s(t − 1), a(t − 1), s(t)) to
the optimal distribution of a(t), i.e., πt : H(t) × As(t) →
[0, 1] with H(t) being the set of all histories h(t). Ap-
the scope of the required history for πt grows
parently,
linearly with respect to t and to derive such history-based
optimal policies is too difﬁcult [21]. Therefore, we only
investigate the stationary policies for problem (P1), which
determines the action a(t) based on the current state s(t)
and utilizes the same scheduling rule, say π, at every time
slot, and try to derive the stationary policy that achieves
maxπ∈ΠS E
t=1 r(t) with
n=1,Pr{G(cid:48)|G} limT →∞
ΠS being the set of all stationary policies. This modiﬁed
problem, i.e., problem (P1) with solution space containing
only the stationary policies, is named as problem (P1-1), and
to solve it, we ﬁrst simplify it to another MDP (P2):

{qn(t)}N

(cid:80)T

1
T

• State: ˆs(t) (cid:44) (Q(t), ˆc(t), G(t)), where ˆc(t) is de-
ﬁned as ˆc(t) (cid:44) [ˆc1(t), ˆc2(t), · · · , ˆcM (t)]T and ˆcm(t) (cid:44)
(cid:80)N
n=1 cn,m(t) is equal to the remaining time for the
release of the mth channel. The new state space is denoted
as ˆS with dimensionality of N M ∗ + M + N M .

• Action: a(t), which is now constrained by

ˆcm(t)am(t) = 0, m ∈ {1, 2, · · · , M },

(11)

where (11) is derived by (3) and the fact ˆcm(t) (cid:44)
(cid:80)N

n=1 cn,m(t).

• Transitions: (5), (7), and

ˆcm(t+1) =






ˆcm(t)−1
Tam(t),m−1
0

ˆcm(t) > 0
ˆcm(t) = 0, am(t) > 0
ˆcm(t) = 0, am(t) = 0.

(12)

where (12) is derived by (6).
• Reward: ˆr(t), which is deﬁned as

(cid:32)
V

ˆr(t)(cid:44)−

M
(cid:88)

I (am(t)) u(am(t), m, gam(t),m(t))

m=1

N
(cid:88)

M ∗
(cid:88)

+

(cid:33)

[qn(t)]τ pn(τ )

n=1

τ =1

with

(13)

u(n, m, g) (cid:44)

EPr{G(cid:48)|G}

Tn,m−1
(cid:88)

τ =0

Zn,m
gn,m(τ )|gn,m(0)=g

.

(14)

Here, gn,m(τ )|gn,m(0)=g is a random variable represent-
ing the value of gn,m(τ ) given the value of gn,m(0) as
g and its distribution can be easily derived according to
Pr{G(cid:48)|G}.

• Problem formulation: problem (P2) is formulated as

5

Compared with problem (P1-1), problem (P2) has a much
simpler structure:
the dimensionality of the state space is
reduced from N M ∗ +2N M to N M ∗ +M +N M ; the reward
function no longer involves the high-dimension matrix C(t).
Moreover, we obtain the following proposition.

Proposition 2.1: Problems (P1-1) is equivalent
(P2), i.e., their optimal values are equal.

to problem

Proof: Please see Appendix A.

1
T

(cid:80)T

Remark 2.1: Problem (P2) is still challenging due to its three
features: 1) large state space ˆS; 2) large discrete action space
As; and 3) multiple time-varying constraints on the actions
in (11), and existing approaches cannot efﬁciently address it.
(1) Dynamic programming [21] requires value evaluation
for each state s, which is deﬁned as J(s) (cid:44)
t=1 r(t) given s as the initial state. There-
limT →∞
fore, it can barely deal with the MDP with very large
state space, such as problem (P2). Lyapunov optimization
determines the action at each time slot by solving a one-
step optimization problem, which is only related to the
current state and thus has a low computational complexity.
Notably, to deploy the Lyapunov optimization for MDP, a
queue model is ﬁrst formulated and the independency of
the “departure variables” over time is required. However,
the departure variables of problem (P2) are the numbers
of the served MUs at each time slot, which based on (5),
are not independent over time.

(2) DRL algorithms cannot efﬁciently address the MDPs
with large discrete action space or with time-varying
constraints on the actions. First, only a few literatures
have discussed the MDP with large discrete action space,
whereas the most valuable attempt is the Wolpertinger
policy proposed in [24] and it requires the MDP satisfying
that “if two policies always select actions close in L2
distance at each state, the MDP would transit to close
states and feedback close rewards, and thus the two
policies have similar performances”. However, the MDP
in problem (P2) does not have this feature, which can
be easily veriﬁed by checking the state transitions in
(5), (8), (12) and the reward function in (13). Moreover,
almost all existing DRL algorithms utilize the exploitation
versus exploration mechanism, where both the exploitation
and the exploration are unpredictable during the training
phase and should randomly select action. Therefore, for
our proposed MDP with multiple time-varying constraints
on the actions, it is of high probability for the DRL to
select infeasible actions, and consequently, terminate the
training without converging to a stable policy. Thus, DRL
algorithms cannot be directly applied to our problem (P2).

III. PERMUTATION-BASED WOLPERTINGER DEEP
DETERMINISTIC POLICY GRADIENT

(P2) max
π∈ΠS

lim
T →∞

E

{qn(t)}N

n=1,Pr{G(cid:48)|G}

s.t.

(5), (7), (8), (11), (12).

1
T

T
(cid:88)

t=1

ˆr(t)

(15)

To solve problem (P2), this section proposes PW-DDPG,
which mainly comprises of DDPG,
the permutation-based
action embedding (PAE) module, and the feasible action ex-
ploration (FAE) module. Particularly, the PAE module utilizes

6

Figure 2: Structure of the proposed algorithm.

the action embedding technique [24] to address the large
discrete action space issue. To deal with the aforementioned
drawback of the conventional action embedding in remark
2.1, the proposed action embedding method in PAE module
has a permutation-based structure and is perfectly compatible
with the MDP problem (P2). To address the time-varying
constraints issue, the PAE module restricts the exploitation
mode to only selecting the feasible actions and the FAE
module utilizes a modiﬁed (cid:15)-greedy method to guarantee that
the exploration mode also only explores the feasible actions.
In the following, we ﬁrst introduce the structure of the
proposed algorithm in details. Then, we present the ofﬂine
training algorithm, which is based on the interaction with
a simulated ofﬂine environment and aims to train the NNs
embedded in the PW-DDPG. Finally, we illustrate the online
validating algorithm to solve problem (P2).

A. Structure of proposed algorithm

As illustrated in the left part of Fig. 2,

the proposed
algorithm consists of four modules: data reﬁnement (DR)
module, DDPG, PAE module, and FAE module.

1) DR module: This module reﬁnes s(t) to ˆs(t) based on
the fact ˆs(t) = (Q(t), ˆc(t), G(t)). It also calculates the value
of the reﬁned reward, i.e., ˆr(t), based on (13) and (14).

2) DDPG: DDPG consists of the actor network, the critic
network and the experience replay module, and it is deployed
to generate the proto-action ˆa(t) based on the reﬁned state
ˆs(t), where the proto-action will be speciﬁed soon.

• Actor network: The actor network contains an evaluation
NN and a target NN, which are parameterized by θ1, θ(cid:48)
1
and denoted as πθ1, πθ(cid:48)
, respectively. Particularly, two
NNs take the reﬁned state ˆs(t) as inputs and thus have
N M ∗ +M +N M nodes at their input layers. The outputs
of them are set to be with dimensionality of M , which

1

equals the dimensionality of action a(t), and thus their
output layers have M nodes. Finally, we deﬁne the output
of the actor evaluation NN as the proto-action and denote
it as ˆa(t) ∈ RM×1, i.e.,

ˆa(t) (cid:44) πθ1 (ˆs(t)),

(16)

where πθ1 (ˆs(t)) is the output of the actor evaluation NN
with input being ˆs(t).

2

• Critic network: The critic network also contains an evalu-
ation NN and a target NN, which are parameterized by θ2,
θ(cid:48)
2 and denoted as Qθ2 , Qθ(cid:48)
, respectively. Particularly,
two NNs take the state-action pairs (ˆs(t), a(t)) as inputs
and thus have N M ∗ + 2M + N M nodes at the input
layers. Their output layers have only one node, which
represents the evaluated Q values of the state-action pairs.
• Experience replay module: The experience replay mod-
ule contains the experience buffer and the mini-batch.
Speciﬁcally, the experience buffer stores the generated
experiences during the ofﬂine training phase, which are
the four-component tuples (ˆs(t), a(t), ˆr(t), ˆs(t+1)). The
mini-batch randomly samples N0 experiences from the
experience buffer to update NNs, and these sampled
experiences are denoted as {(ˆsi, ai, ˆri, ˆs(cid:48)

i)}N0
i=1.

3) PAE module: This module takes the proto-action ˆa(t)
as the input and outputs the Wolpertinger-action ¯a(t), where
the Wolpertinger-action will be speciﬁed soon. As illustrated
in Fig. 3(b), it contains the following three procedures:

• Rounding and feasibility ﬁtting: Given the proto-action
ˆa(t), we ﬁrst round the entries in ˆa(t) to their closest
integers and store the results in ˆa(1)(t), i.e., ˆa(1)
m (t) (cid:44)
Round(ˆam(t)) with ˆam(t), ˆa(1)
m (t) being the mth entry
of ˆa(t) and ˆa(1)(t), respectively. Then, we map ˆa(1)(t)

EnvironmentQ valueDDPGPolicyActor evaluation NNActor target NNSoftmax UpdateActorCritic evaluation NNCritic target NNSoftmax UpdateCriticExperience bufferMini-batchSamplingExperience replay moduleFeasible action exploration moduleData refinement moduleˆ()ta()ta((),(),ˆˆˆ(),(1))rtttt+sas()ta()taGeneration of experiencesExperience bufferˆˆˆ{(),(),(),(1))(}ttrtt+sasUpdate of NNsCritic evaluation NNLossActor target NNCritic target NNˆis1,()ˆ()ˆiiθss21ˆ(())ˆ,iiQθθssˆ(,)iisa2ˆ(),iiQθsaCritic evaluation NNMini-batchˆˆˆ({,,,)}iiiirsasGradientActor evaluation NNCritic evaluation NNActor evaluation NNˆis1ˆˆ()),(iiθss21ˆˆ,()()iiQaθθss11()ˆiθθsˆirˆˆˆ(),},,{iiiirsasˆˆˆ(),},,{iiiirsas2QθPermutation-based action embedding moduleInputMini-batchˆˆˆ({,,,)}iiiirsasPermutation-based action embedding moduleActor evaluation NNCritic evaluation NNFeasible action exploration moduleOutput()taˆ()ta()ta()taˆˆˆ(),((1)(),)trtt+ss()taˆˆˆ(),((1)(),)trtt+ssˆ()ts2QθOffline training procedure((),(1))tt+ss2()Lθ1()Gθ7

Figure 3: Conventional action embedding [24] vs. the proposed permutation-based action embedding.

to ˆa(2)(t) ∈ {0, 1, · · · , N }M by

m (t) (cid:44)
ˆa(2)






ˆa(1)
0
m (t) ≤ 0
ˆa(1)
N
m (t) ≥ N
ˆa(1)
m (t) otherwise,

(17)

where ˆa(2)
m (t) is the mth entry of ˆa(2)(t). Notably, al-
though ˆa(2)(t) takes discrete values in {0, 1, · · · , N }M ,
it may still violate the time-varying constraints in (11) and
thus be infeasible. Thus, we ﬁnally check the feasibility of
ˆa(2)(t) by validating the constraints in (11) and ﬁgure out
a feasible action ˆa(3)(t) accordingly. Particularly, denote
the mth entry of ˆa(3)(t) as ˆa(3)
m (t) to 0 if
the mth channel has been reserved in previous time slots,
i.e., ˆcm(t) > 0, and otherwise, set ˆa(3)
m (t). To
summary, we have

m (t). We set ˆa(3)

m (t) to ˆa(2)

m (t) (cid:44) ˆa(2)
ˆa(3)

m (t)(1 − I(ˆcm(t))).

(18)

It is easy to check that ˆcm(t)ˆa(3)
m (t) = 0 holds, and thus
ˆa(3)(t) is feasible. We also call ˆa(3)(t) as the candidate-
action.

• Action embedding based on permutation: Based on the
candidate-action ˆa(3)(t), we design action embedding to
derive more feasible actions, which are called as the
permutation-actions and denoted as ˆa(4)(t). Speciﬁcally,
we ﬁrst denote the index set of the currently available
channels as B(t) (cid:44) {m|ˆcm(t) = 0, m ∈ {1, 2, · · · , M }}
and gather the entries of ˆa(3)(t) at these indices in an-
other set C(t) (cid:44) {ˆa(3)
m (t)|m ∈ B(t), m ∈ {1, 2, · · · , M }}.
Then, we set ˆa(4)

m (t) with m ∈ B(t) as

ˆa(4)
m (t) = [Permu(C(t))]im,

where ˆa(4)
m (t) is the mth entry of ˆa(4)(t), Permu(C(t))
is a permutation of C(t), [Permu(C(t))]i is the ith entry
of Permu(C(t)), and im ∈ {1, 2, · · · , |B(t)|} is the index
of m in B(t) with |B(t)| being the number of entries of
B(t). Finally, we set ˆa(4)
m (t) with m /∈ B(t) as 0, where
m /∈ B(t) indicates that ˆcm(t) > 0 holds and the mth

channel has been reserved for multicasting in previous
time slots. To summary, we have

m (t) (cid:44)
ˆa(4)

(cid:26) Permu(C(t))im ˆcm(t) = 0
ˆcm(t) > 0.

0

(19)

Obviously, the generated permutation-action ˆa(4)(t) sat-
isﬁes the constraints in (11) and thus is feasible. De-
note the set of all possible permutation-actions as D(t)
and apparently, D(t) contains up to |B(t)|! permutation-
actions. Then, we randomly pick out k permutation-
actions from D(t), where the ith picked out permutation-
action is denoted as ˆa(4)
(t), i ∈ {1, 2, · · · , k}, and send
them to the next procedure.

i

(t)}k

• Action selection based on Qθ2 and ˆs(t): Send the re-
ﬁned state ˆs(t) together with k picked out permutation-
actions {ˆa(4)
i=1 to the critic evaluation NN Qθ2 ,
and then k outputs are obtained, which are denoted as
{Qθ2(ˆs(t), ˆa(4)
i=1. The permutation-action with the
largest output will be selected as the Wolpertinger-action
¯a(t), i.e.,

(t))}k

i

i

¯a(t) (cid:44) arg max
i∈{1,2,···,k}

Qθ2(ˆs(t), ˆa(4)

i

(t)).

(20)

Remark3.1: The conventional action embedding is proposed in
[24] and illustrated in Fig. 3(a). Speciﬁcally, it ﬁrst obtains k
nearest discrete actions of the proto-action in the L2 distance
manner, and then send these actions together with the reﬁned
state ˆs(t) to the critic evaluation NN. Finally, the action
with the largest output will be selected as the Wolpertinger-
action. However, if the MDP is with multiple constraints,
the k nearest discrete actions of the proto-action are very
likely to be infeasible, and so is the obtained Wolpertinger-
action, which disables the ofﬂine training procedure. In the
proposed permutation-based action embedding, however, we
replace the k nearest actions found in the L2 distance manner
by k permutation-actions, which are near to the proto-action
in the “permutation” manner. As we validated before, these
permutation-actions are always feasible. Moreover, it can be
easily checked that “if two policies always select actions close

k nearest actions Proto-actionk permutation-actionsWolpertinger-actionCandidate-action(a) Conventional action embedding(b) Permutation-based action embeddingAction embedding based on permutationAction selection based on ab and abcRounding and feasibility fitting2Qθˆ()ta(3)ˆ()ta(4)ˆ()ta()taProto-actionWolpertinger-actionAction selection based on ab and abcAction embedding based on b  distanceˆ()ts2Qθˆ()ts2ˆ),(tQθs2ˆ),(tQθs2Lin the “permutation” manner at each state, the MDP problem
(P2) would transit to close states and feedback close rewards,
and thus the two policies have similar performances”. The
proposed permutation-based action embedding takes advan-
tage of this fact and thus is perfectly compatible with the MDP
problem (P2).

4) FAE module: This module takes the Wolpertinger-action
¯a(t) as the input and outputs the action of the considered MDP,
i.e., a(t). Speciﬁcally, it sets a(t) as ¯a(t) with probability of
1 − (cid:15) and as a random feasible action with probability of (cid:15),
i.e.,




a(t)=



¯a(t)
a random action satisfying the
constraints in (11)

with probability 1−(cid:15)

with probability (cid:15).

(21)

Compared with the action exploration methods of conventional
DRL algorithms, such as (cid:15)-greedy exploration introduced in
deep Q network (DQN) [25], Gaussian noise exploration
introduced in DDPG [22], and entropy-based exploration
introduced in proximal policy optimization (PPO) [26], the
proposed FAE module does not explore actions in a purely
random manner and can avoid selecting infeasible action based
on (21).

B. Ofﬂine training

We ﬁrst simulate an ofﬂine environment based on the
historical observations on the MUs’ requests and the channel
status. Then, we alternatingly generate new experiences by
utilizing PW-DDPG with the latest NN parameters to interact
with the ofﬂine environment and update the NN parameters
in PW-DDPG based on the latest generated experiences. The
sketches of the experiences generation and the PW-DDPG
update are illustrated in the right part of Fig. 2.

1) Ofﬂine environment simulation: The simulated ofﬂine
environment is supposed to simulate the value of the new
state s(t + 1) based on the values of s(t) and a(t). Specif-
ically, we ﬁst estimate the number of MUs {En}N
n=1, the
probability of MUs’ requesting messages {αn}N
n=1, and the
transition Prn,m,k (hj|hi) based on the
channel coefﬁcient
historical observations, and simulate the values of {qn(t)}N
n=1
and Pr{G(cid:48)|G} accordingly. Then, given the values of s(t) and
a(t), the new state s(t + 1) can be derived by (5), (6), (7),
and (8).

2) Generation of experiences: First, obtain the value of
s(t) from the simulated ofﬂine environment and send it to the
DR module, by which the reﬁned state ˆs(t) is derived. Next,
send ˆs(t) to the actor evaluation NN and obtain the proto-
action ˆa(t). Then, by deploying the PAE module, derive the
Wolpertinger-action ¯a(t). Finally, derive the action a(t) by
utilizing the FAE module. With the derived a(t), the reﬁned
reward ˆr(t) can be derived by sending s(t) and a(t) to the
data reﬁnement module, the new state s(t+1) can be obtained
from the simulated ofﬂine environment, and the reﬁned new
state ˆs(t + 1) can be derived by sending s(t + 1) to the DR
module. The experience packs the above information into the
tuple (ˆs(t), a(t), ˆr(t), ˆs(t + 1)).

8

Algorithm 1 Ofﬂine training of PW-DDPG for multicast
scheduling

1: Randomly initialize θ1 and θ2 for the actor and critic

evaluation NNs;

2: Initialize actor target NN and critic target NN by θ(cid:48)

1 ← θ1

and θ(cid:48)

2 ← θ2;

3: Initialize replay buffer R with size of NR;
4: Simulate the ofﬂine environment;
5: for episode= 1, 2, · · ·
Let Q(1) = 0N ×M ∗
6:
element in GN ×M ;
s(1) = (Q(1), C(1), G(1));
Send s(1) to the DR module and derive ˆs(1);
for t = 1, 2, · · · , T

7:

, C(1) = 0N ×M . Let G(1) be any

Send ˆs(t) to the actor evaluation NN πθ1 and derive
ˆa(t);
Send ˆa(t) to the PAE module and derive ¯a(t);
Send ¯a(t) to the FAE module and derive a(t);
Derive ˆr(t) from DR module with ˆs(t) and a(t) as
the input;
Obtain s(t + 1) from the ofﬂine environment with
a(t) as the input;
Send s(t+1) to the DR module and derive ˆs(t+1);
Store (ˆs(t), a(t), ˆr(t), ˆs(t + 1)) in the experience
buffer;
if t > NR − 1

Use mini-batch to sample N0 experiences from
the experience buffer;
Update actor evaluation NN based on (22);
Update critic evaluation NN based on (23);
Update the target NNs based on soft update rule;

8:
9:
10:

11:
12:

13:

14:

15:
16:

17:
18:

19:
20:

21:
22:
23:
24: end for

end if
end for

3) Update of the NN parameters in PW-DDPG: After each
generation of one new experience, the experience buffer is
refreshed, i.e., one old experience is replaced by the new
one. Then, we use mini-batch to sample N0 experiences from
the experience buffer as {(ˆsi, ai, ˆri, ˆs(cid:48)
i=1 and utilize them
to update the NN parameters in PW-DDPG. Speciﬁcally, the
actor evaluation NN is updated based on the policy gradient
theorem, i.e.,

i)}N0

G(θ1) =

1
N0

N0(cid:88)

∇aQθ2(s, a)|s=ˆsi,a=πθ1 (ˆsi)·∇θ1πθ1 (s)|s=ˆsi,

i=1

(22)

the critic evaluation NN is updated by backpropogating the
following loss function

L(θ2) =

1
N0

N0(cid:88)

i=1

(cid:0)ˆri + γQθ(cid:48)

2

(cid:0)ˆs(cid:48)

i, πθ(cid:48)

1

(ˆs(cid:48)

i)(cid:1) − Qθ2 (ˆsi, ai)(cid:1)2

,

(23)

where γ ∈ (0, 1) is a discount factor, and two target NNs
utilize soft update rule [22].

The ofﬂine training algorithm is summarized in Algo-

rithm 1.

C. Online validating

After the ofﬂine training, we design the online validating
algorithm to solve problem (P1), which is very similar to
the ofﬂine training algorithm. Speciﬁcally, we ﬁrst derive the
reﬁned state ˆs(t) based on the DR module and the observed
state s(t) from the practical multicast environment. Then, we
sequentially utilize the PAE module and the FAE module to
derive the action a(t). Finally, we execute the action a(t).

IV. DISCUSSION

In this section, we derive the upper bound of the objective
function of problem (P2), and it works as the benchmark
to validate the proposed PW-DDPG in the simulation part.
To derive the upper bound, we ﬁrst release the time-varying
constraints of problem (P2) in (11) to a set of time-invariant
constraints, which reveal the statistical features of problem
(P2) under stationary policies satisfying the constraints in
(11). Then, we convert the released problem to a two-step
optimization problem, where the optimization of the ﬁrst step
purely minimizes the latency penalty and the second step
jointly minimizes the energy consumption and the latency
penalty. Finally, we successively analyze the optimizations of
these two steps, and obtain an upper bound for problem (P2).
1) Release on constraints: We release the time-varying

constraints in (11) with the following proposition.

Proposition 4.1: For any policy of problem (P2) satisfying the
constraints in (11), it follows

N
(cid:88)

γn,m(T )Tn,m ≤ T, m ∈ {1, 2, · · · , M }, T ∈ Z+, (24)

n=1
where γn,m(T ) ∈ N counts how many times the nth message
has been multicasting over the mth channel within the 1st to
T th time slots and is deﬁned as

γn,m(T ) (cid:44)

T
(cid:88)

t=1

In(am(t)),

(25)

with n ∈ {1, 2, · · · , N }, m ∈ {1, 2, · · · , M }, and T ∈ Z+.

Proof: Please see Appendix B.

Proposition 4.1 indicates that the constraints in (24) are
looser than those in (11). Thus, by replacing the constraints
in (11) by (24), we derive a released version of problem (P2)
as

(P3) max
{a(t)}

lim
T →∞

E

{qn(t)}N

n=1,Pr{G(cid:48)|G}

1
T

T
(cid:88)

t=1

ˆr(t)

s.t.

(5), (7), (8), (24),

where the transitions in (12) from problem (P2) are dropped
in problem (P3) since without the constraints in (11), (12)
is no longer needed in problem (P3), and we also release the
solution space such that the actions {a(t)} may not necessarily
be picked following stationary polices.

9

2) Problem conversion: We convert problem (P3) to a two-
step optimization problem. First, we let γn,m(T ) equal ¯γn,mT
for all n ∈ {1, 2, · · · , N }, m ∈ {1, 2, · · · , M }, and T ∈
Z+, where ¯γn,m ∈ [0, 1] is a constant real number. Next, we
only optimize the latency penalty term of problem (P3) by
solving the following problem:

(P4) max
{a(t)}

− lim
T →∞

E

{qn(t)}N

n=1

1
T

T
(cid:88)

N
(cid:88)

M ∗
(cid:88)

t=1

n=1

τ =1

[qn(t)]τ pn(τ )

s.t.

(5),
γn,m(T )=¯γn,mT, n ∈ {1,· · ·,N } ,m ∈ {1,· · ·,M } .

Denote the upper bound of problem (P4) as f (¯γ), where ¯γ is a
(cid:44) ¯γn,m.
N -by-M -dimension matrix and deﬁned by [¯γ](n,m)
Then, we jointly optimize the energy consumption and the
latency penalty terms of problem (P3) by further solving the
following problem:

(P5) max

{a(t)},¯γ

− lim
T →∞

EPr{G(cid:48)|G}

1
T

T
(cid:88)

V

M
(cid:88)

t=1

m=1

I (am(t))

s.t.

u(am(t), m, gam(t),m(t)) + f (¯γ)
(7), (8)
N
(cid:88)

¯γn,mTn,m ≤ 1, m ∈ {1, 2, · · · , M },

(26)

n=1

where the constraints in (26) is derived by (24). Since the
optimal solutions to problems (P4) and (P5) may assign
different values to the design variable {a(t)}, problem (P5) is
a released version of problem (P3) and thus the upper bound
of problem (P5) is also an upper bound of problem (P3). In
the following, we successively analyze problems (P4) and (P5)
and derive their upper bounds, respectively.

3) Upper bound of problem (P4): By splitting both the
objective function and the constraints of problem (P4), we
divide problem (P4) into N sub-problems, where the nth sub-
problem is formulated as

(P4-1) max
{a(t)}

− lim
T →∞

E{qn(t)}

1
T

T
(cid:88)

M ∗
(cid:88)

[qn(t)]τ pn(τ )

t=1

τ =1

s.t.

(5)
γn,m(T ) = ¯γn,mT, m ∈ {1, · · · , M } .

(27)

Note that the optimal solutions to N sub-problems may assign
different values to {a(t)}. Thus, the summation of the optimal
values of N sub-problems is an upper bound of problem (P4).

to T th

Problem (P4-1) can be interpreted as: if the number of the
multicastings for the nth message is ﬁxed as (cid:80)M
m=1 ¯γn,mT
within the 1st
time slots, when should we start
these multicastings to minimize the average latency penalty?
this question, we ﬁrst denote ti with i ∈
To answer
{1, 2, · · · , (cid:80)M
m=1 ¯γn,mT } as the beginning time of the ith
multicastings of the nth message, i.e, bn(ti) = 1 holds, and
m=1 ¯γn,mT = T . Then, it
without loss of generality, we let t(cid:80)M

10

(32)

follows

4) Upper bound of problem (P5): Denote ¯u(n, m) as

lim
T →∞

E{qn(t)}

1
T

T
(cid:88)

M ∗
(cid:88)

[qn(t)]τ pn(τ )

t=1

τ =1

= lim
T →∞

1
T

(cid:80)M

m=1 ¯γn,mT
(cid:88)

E{qn(t)}

ti(cid:88)

M ∗
(cid:88)

[qn(t)]τ pn(τ ), (28)

i=1

t=ti−1+1

τ =1

(cid:80)M ∗

t=ti−1+1

m=1¯γn,mT terms, where the ith

where t0 (cid:44) 0 holds. Based on the above equation,
the objective function of problem (P4-1) can be di-
vided into (cid:80)M
term is
(cid:80)ti
E{qn(t)}
τ =1[qn(t)]τ pn(τ ) and represents the
generated latency penalties after the (i − 1)th multicasting and
before the ith multicasting of the nth message. Moreover, ac-
cording to (5), the values of qn(t) in any two of (cid:80)M
m=1¯γn,mT
terms are independent. Thus, it can be checked that the optimal
policy for problem (P4-1) determines the time slot for new
multicasting only based on the values of qn(t) since the last
multicasting. Therefore, problem (P4-1) can be realigned as

(P4-2) max

π

−Eπ,{qn(t)}

t∗
(cid:88)

M ∗
(cid:88)

[qn(t)]τ pn(τ )

t=1

τ =1

s.t.

(5)
M
(cid:88)

m=1

¯γn,mEπ∗,{qn(t)}t∗ = 1,

(29)

where (29) is derived by (27); t∗ is the time to start new
multicasting given that the last multicasting happened at the
0th time slot, and determined by policy π.

Exhausted search algorithm can be deployed to solve prob-
lem (P4-2). Particularly, in the case with pn(τ ) = 1, i.e., the
case where the latency penalty explicitly represents the latency,
we deﬁne ¯qn(t) (cid:44) (cid:80)M ∗
τ =1[qn(t)]τ and then problem (P4-2) can
be rewritten as

(P4-3) max
¯qn∈N

−E{qn(t)}

s.t.

(29)

t∗
(cid:88)

t=1

¯qn(t)

¯qn(t + 1) =

(cid:26) qn(t)

¯qn(t)+qn(t)

bn(t) = 1
bn(t) = 0

, (30)

where its optimal policy is of a threshold type and starts
new multicasting of the nth message at
the tth time slot
only if the value of ¯qn(t) exceeds the threshold ¯qn; t∗ (cid:44)
min{t ∈ Z+|¯qn(t) > ¯qn} is the exact time slot to start new
multicasting; (30) is derived by (5). Obviously, we can derive
the optimal value of ¯qn, denoted as ¯q∗
n, by directly solving the
equality (29), and then derive the optimal value of problem
(cid:80)min{t∈Z+|¯qn(t)>¯q∗
n}
(P4-3) as −E{qn(t)}
¯qn(t). Therefore, in
the case with pn(τ ) = 1, the upper bound of problem (P4) is
derived as

t=1

f (¯γ) = −

N
(cid:88)

n=1

min{t∈Z+|¯qn(t)>¯q∗
(cid:88)

n}

E{qn(t)}

t=1

¯qn(t).

(31)

¯u(n, m) (cid:44) min
g∈G

u(n, m, g).

Then, based on the deﬁnition of γn,m(T ) in (25), it follows

lim
T →∞

EPr{G(cid:48)|G}

1
T

T
(cid:88)

V

M
(cid:88)

I(am(t))u(am(t),m, gam(t),m(t))

t=1

m=1

≥ lim
T →∞

1
T

V

M
(cid:88)

N
(cid:88)

m=1

n=1

γn,m(T )¯u(n, m).

Therefore, problem (P5) can be released as

(P5-1) max

γ(T )∈NN ×M

− lim
T →∞

1
T

V

M
(cid:88)

N
(cid:88)

γn,m(T )¯u(n, m)+f (

m=1

n=1

γ
T

)

s.t.

(24),

where γ(T ) is deﬁned as [γ(T )](n,m) (cid:44) γn,m(T ). Apparently,
problem (P5-1) can be solved by integer programming [27],
and the derived optimal value of problem (P5-1) is an upper
bound of the objective function of problem (P2).

V. SIMULATION AND NUMERICAL RESULTS

This section evaluates the performance of the proposed PW-
DDPG and compares it with the existing multicast schedul-
ing algorithms. Speciﬁcally, we consider that
the latency
i.e., pn(τ ) = 1
penalty explicitly represents the latency,
holds. The channel coefﬁcient between the BS and any MU,
i.e., hn,m,k(t), is modeled as an i.i.d. random variable over
time, which equals 0.5 with probability of 0.8 and 1 with
probability of 0.2. In the proposed PW-DDPG, the actor and
critic NNs have two hidden layers and each layer has 60
nodes, and the sizes of the experience buffer and the mini-
batch are set as 5000 and 400, respectively. We investigate the
following benchmark algorithms: a) optimal stopping method
[17], which is suboptimal for the scenario with single cached
message and single available channel; b) RVI [18], which is
optimal for the scenario with multiple messages and single
channel and with only a few MUs; c) benchmark by solving
problem (P5-1) (benchmark (P5-1) for short); d) random
policy, which randomly picks one feasible action at each time
slot; e) greedy policy, which picks the feasible action with
the largest instant reward; f) proportion pseudo greedy policy,
which ﬁrst randomly selects a certain proportion of actions and
then picks the feasible one with the largest instant reward.
Remarkably, the RVI in b) cannot address the time-varying
issue and thus can only be applied in the cases with Tn,1 = 1
for all n ∈ {1, 2, · · · , N }.

Fig. 4 investigates the performances of various algorithms
under the scenario where 20 MUs send requests for download-
ing one common message with probability of 0.5 and the BS
serves these MUs over one multicasting channel, i.e., E1 = 20,
N = 1, M = 1, and α1 = 0.5. Moreover, the multicasting
of the message consumes 2 time slots, i.e., T1,1 = 2. From
Fig. 4(a), it is observed that the optimal stopping method pro-
posed in [17] achieves the greatest average reward of -168.92
and is close to -168.88 given by the benchmark (P5-1). The
proposed PW-DDPG converges after around 20000 time slots,

11

Figure 4: Performance comparison of PW-DDPG with the benchmark algorithms for the scenario with N = 1 and M = 1. (a)
average reward as a function of time slots; (b) tradeoff between energy consumption and latency penalty.

Figure 5: Average latency vs. average energy consumption for the scenario with N = 1, M = 1.

where the ﬁrst 5000 time slots are utilized to purely ﬁll up the
experience buffer and thus show no performance gain, and
achieves the average reward of -190.09 with the exploration
rate (cid:15) as 0.1, and -183.48 with (cid:15) as 0.05, which are very close
to the benchmark (P5-1). Fig. 4(b) shows that PW-DDPG
achieves a similar tradeoff between the energy consumption
and the latency penalty compared with the benchmark (P5-1).

Fig. 5 again investigates the single-message and single-
channel scenario while with varied tradeoff parameter V , and
illustrates the tradeoff curves between the average latency and
the average energy consumption of different algorithms. We
simulate the cases with T1,1 = 2 and T1,1 = 4, where each
multicasting consumes 2 and 4 time slots, respectively. And
in both cases, it is observed that the tradeoff curve of the
optimal stopping method coincides with the benchmark (P5-
1), which indicates that the optimal stopping method achieves
the optimal tradeoff. And the tradeoff curve of PW-DDPG is
very close to that of the benchmark (P5-1) and the gap between

them can be further shrunk by decreasing the exploration rate
from 0.1 to 0.05.

Fig. 6 studies the scenario with multiple messages and
single channel and compares the performances of PW-DDPG
with RVI. Due to the high computational complexity and
multiple model-level requirements on applying RVI [18], we
further specify the scenario as: a) 16 MUs send requests
for downloading two common messages; b) the maximum
accumulated numbers of two messages’ requests are limited
to be N1 = 10 and N2 = 10 and the exceeded requests will
be abandoned; c) E1 = 8, E2 = 8, α1 = 0.2, α2 = 0.6,
and T1,1 = T2,1 = 1. It is observed from Fig. 6(a) that
RVI achieves the average latency penalty 2.80, which exactly
equals the benchmark (P5-1). PW-DDPG achieves a very close
value 2.90, while the greedy policy achieves 3.03. Fig. 6(b)
shows that RVI has a threshold structure, i.e., the regions to
multicast two messages are separated by a switch curve. By
executing the online validating for 600000 time slots, PW-

(a)(b)0100200300400500Time slots/102-450-400-350-300-250-200-150-100-500Random policyGreedy policyOptimal stoppingBenchmark (P5-1)PW-DDPG with =0.1PW-DDPG with =0.05Latency penaltyEnergy consumptionMagnitude of reward0100200300400500600Random policyGreedy policyOptimal stoppingBenchmark (P5-1)PW-DDPG with =0.1PW-DDPG with =0.05Average reward012345678Average latency per request051015202530354045=2, PW-DDPG with =0.1=2, PW-DDPG with =0.05=4, PW-DDPG with =0.1=4, PW-DDPG with =0.05=2, Benchmark (P5-1)=2, Optimal stopping=4, Benchmark (P5-1)=4, Optimal stoppingAverage energy consumption per request1,1T1,1T1,1T1,1T1,1T1,1T1,1T1,1T12

Figure 6: Performance comparison of PW-DDPG with the benchmark algorithms for the scenario with N = 2, M = 1, and
N1 = N2 =10. (a) average latency penalty as a function of time slots; (b) scheduling structure of different algorithms.

Figure 7: Performance comparison of PW-DDPG with the benchmark algorithms for the scenario with N = 2, M = 1,
N1 = 22, and N2 = 20. (a) average latency penalty as a function of time slots; (b) scheduling structure of different algorithms.

DDPG is veriﬁed to have an approximate threshold structure,
i.e., it has a very high frequencies (above 97.5%) to multicast
message 1 in the green region and message 2 in the blue
region, and these two regions are separated by almost the same
switch curve with that of RVI. Notably, RVI has a very high
computational complexity, i.e., it is derived by ﬁrst evaluating
(cid:1) policies and then selecting the optimal one, where
(cid:0)N1+N2+2
N1+1
the evaluation of each policy requires one individual long-term
simulation.

Fig. 7 again studies the scenario with two messages and
single channel, while considers E1 = 20, E2 = 20, N1 = 22,
and N1 = 20. Apparently, it is computationally difﬁcult to
ﬁnd the optimal policy by RVI, which requires to evaluate
(cid:1) = 2.0126 × 1012 policies in advance. Fig. 7(a) shows
(cid:0)44
21
that PW-DDPG achieves average latency penalty 7.60 and is
close to the benchmark (P5-1) 6.92. Fig. 7(b) shows that PW-
DDPG remains an approximate threshold structure.

Fig. 8 investigates the performances of different algorithms
under a complex scenario where 800 MUs send requests for
downloading four common messages and the BS serves them
over six channels. Speciﬁcally, we consider E1 = E2 =
E3 = E4 = 200, N = 4, M = 6, (α1, α2, α3, α4) =
(0.8, 0.2, 0.1, 0.05). We also consider that the multicastings
for four messages consume different time durations and the la-
tency penalty functions of four messages are different, neither.
To be speciﬁc, (T1,m, T2,m, T3,m, T4,m) = (1, 2, 3, 4) for all
m ∈ {1, · · · , 6} and (p1(τ ), p2(τ ), p3(τ ), p4(τ )) = (1, 2, 2, 2).
Apparently, the MDP under this scenario has a large discrete
action space of magnitude |A|= 56 = 15625 and multiple
time-varying constraints, where existing algorithms cannot
solve it efﬁciently. The proposed PW-DDPG again deploys
two hidden layers in the actor and critic NNs, while the two
hidden layers now contain 350 and 300 nodes, respectively.
It is observed that PW-DDPG converges after around 75000

0210Number of accumulated requests for message 1024681000.20.40.60.81RVI: multicast message 1RVI: multicast message 2RVI: switch curveGreedy policy: switch curve468Number of accumulated requests for message 2PW-DDPG: frequencies to multicast message 2(a)(b)050100150200Time slots/1022345678Random policyGreedy policyRVIBenchmark (P5-1)PW-DDPGAverage latency penalty05101520Number of accumulated requests for message 1051015201PW-DDPG: switch curveGreedy policy: switch curve0.80.60.40.20No dataPW-DDPG: frequencies to multicast message 2Time slots/102Number of accumulated requests for message 2(a)(b)050100150200246810121416Random policyGreedy policyBenchmark (P5-1)PW-DDPGAverage latency penalty13

Figure 8: Performance comparison of PW-DDPG and the benchmark algorithms for the scenario with four messages and six
channels. (a) average reward as a function of time slots; (b) tradeoff between energy consumption and latency penalty.

iterations and achieves average reward of -1674.8 with (cid:15) as
0.1 and -1567.5 with (cid:15) as 0.05. The random policy has the
worst performance. The 10−3-proportion, 10−2-proportion,
and 10−1-proportion pseudo greedy policies achieve the aver-
aged reward of -1990.9, -1742.2, and -1814.1, respectively, and
need to evaluate 16, 156, and 1563 actions at each time slot.
However, according to the statistics from the online validating,
PW-DDPG needs only to evaluate 2.11 ((cid:15) =0.1) and 2.59
((cid:15) =0.05) actions at each time slot. Thus, PW-DDPG has
a much lower computational complexity. In Fig. 8(b), PW-
DDPG again achieves a better tradeoff than the other two
algorithms.

VI. CONCLUSIONS

We consider the multicast scheduling problem for multiple
messages over multiple channels, which jointly optimizes
the energy efﬁciency and the average latency penalty. This
problem is formulated as an inﬁnite-horizon MDP, which
is challenging due to the large discrete action space and
multiple time-varying constraints. Speciﬁcally, the MDPs with
the former feature cannot be efﬁciently addressed by existing
algorithms and the latter feature induces dynamic action space,
which further complicates the problem. To simplify the formu-
lated MDP, we ﬁrst analyze the intrinsic features of this prob-
lem under stationary policies and succeed to reduce the state
space by reﬁning the reward function. To address the discrete
action space issue, we propose the permutation-based action
embedding method, which embeds action in the “permutation”
manner and overcomes the drawbacks of the conventional
action embedding. To address the time-varying constraints
issue, we design PAE and FAE modules to separately restrict
the exploitation and exploration modes to selecting feasible
actions. Remarkably, the proposed permutation-based action
embedding can also be combined with other DRL algorithms,
such as asynchronous advantage actor-critic (A3C) [28] and

PPO, to better ﬁt the structure of the considered MDPs, and
we will explore these combinations in future works.

APPENDIX A
SKETCH PROOF OF PROPOSITION 2.1

To show that problems (P1-1) and (P2) are equivalent, we
ﬁrst prove that for any stationary policy of problem (P1-1),
there exists another stationary policy of problem (P2) such
that their objective functions are equal. Then, we prove that
for any stationary policy of problem (P2), there also exists
another stationary policy of problem (P1-1) such that their
objective functions are equal. Based on the above two results,
the optimal values of problems (P1-1) and (P2) are obviously
equal.

We ﬁrst show that for each stationary policy π for problem
(P1-1), there exists another policy ¯π for problem (P2) such
that (10)|π= (15)|¯π holds. Speciﬁcally,

(10)|π

(i)
=E

π,{qn(t)}N

n=1,Pr{G(cid:48)|G} lim
T →∞

t+Tam(t),m−1
(cid:88)

τ =t

Zam(t),m
gam(t),m(τ )

+

(cid:34)

µπ
s

Ea∼π(s)

EPr{G(cid:48)|G}

V

(ii)
= −

(cid:88)

s∈S

Zam,m
gam,m(τ )

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)gam,m(0)=[G](n,m),a

(cid:33)

+

(cid:32)

−

V

1
T

T
(cid:88)

t=1

M
(cid:88)

m=1

I(am(t))



[qn(t)]τ pn(τ )



N
(cid:88)

M ∗
(cid:88)

τ =1

n=1
(cid:32)

M
(cid:88)

Tam (t),m−1
(cid:88)

I(am)

m=1

N
(cid:88)

M ∗
(cid:88)

n=1

τ =1

τ =0

(cid:35)

(cid:12)
(cid:12)
(cid:12)
[qn]τ pn(τ )
(cid:12)
(cid:12)Q

Latency penaltyEnergy consumptionMagnitude of reward05001000150020002500300035004000(b)02004006008001000Iterations/102-6000-5500-5000-4500-4000-3500-3000-2500-2000-1500-1000-500Random policy10-1-proportion pseudo greedy policy10-2-proportion pseudo greedy policy10-3-proportion pseudo greedy policyBenchmark (P5-1)PW-DDPG with =0.1PW-DDPG with =0.05(a)Random policy10-1-proportion pseudo greedy policy10-2-proportion pseudo greedy policy10-3-proportion pseudo greedy policyBenchmark (P5-1)PW-DDPG with =0.1PW-DDPG with =0.05Average reward= −

(cid:88)

s∈S

(cid:34)

µπ
s

Ea∼π(s)

V

M
(cid:88)

m=1

Tam (t),m−1
(cid:88)

I(am)

EPr{G(cid:48)|G}

Zam,m
gam,m(τ )|gam ,m(0)=[G](n,m)

(cid:88)

= −





(cid:88)

µπ
s

ˆs∈ ˆS

s∈N (ˆs)

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)a

(cid:34)
Ea∼π(s)
V

τ =0
M ∗
(cid:88)

N
(cid:88)

+

n=1

τ =1

(cid:35)

(cid:12)
(cid:12)
(cid:12)
[qn]τ pn(τ )
(cid:12)
(cid:12)Q

M
(cid:88)

m=1

Tam (t),m−1
(cid:88)

I(am)

EPr{G(cid:48)|G}

τ =0

Zam,m
gam,m(τ )|gam ,m(0)=[G](n,m)

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)a

+

N
(cid:88)

M ∗
(cid:88)

n=1

τ =1

(cid:12)
(cid:12)
(cid:12)
[qn]τ pn(τ )
(cid:12)
(cid:12)Q

(cid:35)

,

(33)

where equality (i) is derived by combing (3) and (6); in equal-
ity (ii), µπ
s is the occurrence probability of state s under policy
Is(s(t))
π in problem (P1-1) and deﬁned by limT →∞
,
T
G and Q are components of s = (G, C, Q), and am is the
mth entry of a; equality (ii) holds since π is a stationary
in (33), ˆs = (G, ˆc, Q) and N (ˆs) is deﬁned as
policy;
{s ∈ S|(cid:80)N
n=1[C](n,m) = ˆcm} with ˆcm being the mth entry
of ˆc.

(cid:80)T

t=1

Then, deﬁne policy ¯π for problem (P2) as

¯π(ˆs, a) =

(cid:88)

s∈N (ˆs)

µπ

s π(s, a),

(34)

where ¯π(ˆs, a) and π(s, a) are the probabilities of selecting
action a at state ˆs and s, respectively. It can be checked
that the occurrence probability of the state ˆs under policy
¯π in problem (P2) equals the summation of the occurrence
probabilities of the states in N (ˆs) under policy π in problem
(P1-1), i.e., µ¯π

s . Therefore, we derive that

s∈N (ˆs) µπ

ˆs = (cid:80)

14

time slots by these multicastings is (cid:80)(cid:80)N
Tni,m
and it equals (cid:80)N
n=1 γn,m(T )Tn,m. Without loss of gener-
n=1 γn,m(T ) ≤
ality, consider 1 ≤ t1 < t2 < · · · < t(cid:80)N
t(cid:80)N
n=1 γn,m (T ),m − 1 ≤ T , where the last
inequality is to cover the last multicasting within T time slots.

n=1 γn,m(T ) + Tn(cid:80)N

n=1 γn,m(T )

i=1

Now, we prove (24). First, it follows

ˆcm(ti +ji) > 0, i ∈ {1,· · ·,

N
(cid:88)

n=1

γn,m(T )}, ji ∈ {1,· · ·,Tni,m −1},

(35)

which is straightforward from (12) and indicates that the mth
th to (ti + Tni,m − 1)th time
channel is occupied from the ti
th message. Then, based on
slot by the multicasting of the ni
(12) and (35), it follows

ti+1 ≥ Tni,m + ti, i ∈ {1, 2, · · · ,

N
(cid:88)

n=1

γn,m(T ) − 1},

(36)

the (i + 1)th multicasting over

the
which indicates that
mth channel should start from or after the (Tni,m + ti)th
time slot. Finally, we sum up both sides of (36) over i ∈
{1, 2, · · · , (cid:80)N
n=1 γn,m(T ) ≥
n=1 γn,m(T ) − 1} and obtain t(cid:80)N
(cid:80)(cid:80)N
i=1

n=1 γn,m(T )−1

T ≥ t(cid:80)N

Tni,m + t1, by which we obtain
n=1 γn,m (T )m − 1

n=1 γn,m(T ) + Tn(cid:80)N
n=1 γn,m(T )
(cid:88)

Tni,m + t1 − 1

(cid:80)N

≥

≥

i=1

γn,m(T )Tn,m.

N
(cid:88)

n=1

Tam(t),m−1
(cid:88)

I(am)

EPr{G(cid:48)|G}

Then, the proof is complete.

REFERENCES

(33)

= −

(cid:88)

ˆs∈ ˆS

(cid:34)

µ¯π
ˆs

Ea∼¯π(s)

V

M
(cid:88)

m=1

Zam,m
gam,m(τ )|gam,m(0)=[G](n,m)

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)a

+

=(15)|¯π.

τ =0

N
(cid:88)

M ∗
(cid:88)

n=1

τ =1

(cid:12)
(cid:12)
(cid:12)
[qn]τ pn(τ )
(cid:12)
(cid:12)Q

(cid:35)

,

We use similar techniques to proof the inverse, i.e., for
each stationary policy ¯π for problem (P2), there exists another
policy π for problem (P1-1) such that (15)|¯π= (10)|π holds,
and the proof is thus omitted.

APPENDIX B
PROOF OF PROPOSITION 4.1

n=1 γn,m(T ) = (cid:80)T

Based on the deﬁnitions of In(x) and I(x) in Section
II-3, we obtain (cid:80)N
n=1 In (am(t)) = I (am(t)). Combining
this fact and (25), we obtain (cid:80)N
n=1
In (am(t)) = (cid:80)T
t=1 I (am(t)), which indicates that there are
in total (cid:80)N
n=1 γn,m(T ) times of multicastings over the mth
channel during T time slots. We start these multicatings at
th time slot and transmit the
the t1
th message, respectively,
n1
i.e.,
am(ti) = ni. Obviously, the number of the total consumed

th, · · ·, t(cid:80)N
· · ·, n(cid:80)N

th, t2
th,

n=1 γn,m(T )

n=1 γn,m(T )

th, n2

(cid:80)N

t=1

[1] Cisco,

“Cisco annual

(2018-2023) white paper,”
Available: https://branden.biz/wp-content/uploads/2020/02/CiscoAnnual-
Internet-Report.pdf, Feb. 2020.

internet

report

[2] Ericsson,

“Ericsson

mobility

report,”

Available:

https://www.ericsson.com/4ad7e9/assets/local/reports-
papers/mobilityreport/documents/2021/ericsson-mobility-report-
november-2021.pdf, Nov. 2021.

[3] A. Biason and M. Zorzi, “Multicast via point to multipoint transmissions
in directional 5G mmWave communications,” IEEE Commun. Mag., vol.
57, no. 2, pp. 88-94, Feb. 2019.

[4] Z. Li, C. Qi, and G. Y. Li, “Low-complexity multicast beamforming for
millimeter wave communications,” IEEE Trans. Veh. Technol., vol. 69,
no. 10, pp. 12317-12320, Oct. 2020.

[5] Y. Li, M. Xia, and Y.-C. Wu, “Energy-efﬁcient precoding for nonorthog-
onal multicast and unicast transmission via ﬁrst-order algorithm,” IEEE
Trans. Wireless Commun., vol. 18, no. 9, pp. 4590-4604, Sep. 2019.
[6] L. Du, S. Shao, G. Yang, J. Ma, Q. Liang, and Y. Tang, “Capacity
characterization for reconﬁgurable intelligent surfaces assisted multiple-
antenna multicast,” IEEE Trans. Wireless Commun., vol. 20, no. 10, pp.
6940-6953, Oct. 2021.

[7] G. Zhou, C. Pan, H. Ren, K. Wang, and A. Nallanathan, “Intelligent
reﬂecting surface aided multigroup multicast MISO communication sys-
tems,” IEEE Trans. Signal Process., vol. 68, pp. 3236-3251, Apr. 2020.
[8] F. Zhou, L. Feng, P. Yu, W. Li, X. Que, and L. Meng, “DRL-based low
latency content delivery for 6G massive vehicular IoT,” IEEE Internet
Things J., early access, Mar. 2021.

[9] W. Hao, G. Sun, F. Zhou, D. Mi, J. Shi, P. Xiao, and V. C. Leung,
“Energy-efﬁcient hybrid precoding design for integrated multicast-unicast
millimeter wave communications with SWIPT,” IEEE Trans. Veh. Tech-
nol., vol. 68, no. 11, pp. 10956-10968, Nov. 2019.

15

[10] N. Chukhno, O. Chukhno, S. Pizzi, A. Molinaro, A. Iera, and G.
Araniti, “Unsupervised learning for D2D-assisted multicast scheduling in
mmWave networks,” in Proc. IEEE BMSB, Chengdu, China, Oct. 2021,
pp. 1-6.

[11] Y. Mao, B. Clerckx, and V. O. K. Li, “Rate-splitting for multi-antenna
non-orthogonal unicast and multicast transmission: Spectral and energy
efﬁciency analysis,” IEEE Trans. Commun., vol. 67, no. 12, pp. 8754-
8770, Sep. 2019.

[12] I.-S. Cho and S. J. Baek, “Optimal multicast scheduling for millimeter
wave networks leveraging directionality and reﬂections,” in Proc. IEEE
INFOCOM, Vancouver, Canada, May 2021, pp. 1-10.

[13] G. Mendler and G. Heijenk, “On the potential of multicast in millimeter
wave vehicular communications,” in Proc. IEEE VTC, Helsinki, Finland,
June 2021, pp. 1-7.

[14] R. O. Afolabi, A. Dadlani, and K. Kim, “Multicast scheduling and
resource allocation algorithms for OFDMA-based systems: A survey,”
IEEE Commun. Surv. Tuts., vol. 15, no. 1, pp. 240-254, 1st Quart. 2013.
[15] H. Won, H. Cai, D. Y. Eun, K. Guo, A. Netravali, I. Rhee, and K.
Sabnani, “Multicast scheduling in cellular data networks,” IEEE Trans.
Wireless Commun., vol. 8, no. 9, pp. 4540-4549, Oct. 2009.

[16] H. Hao, C. Xu, M. Wang, L. Zhong, and D. O. Wu, “Stochastic coop-
erative multicast scheduling for cache-enabled and green 5G networks,”
in Proc. IEEE ICC, Shanghai, China, May 2019, pp. 1-6.

[17] C. Huang, J. Zhang, H. V. Poor, and S. Cui, “Delay-energy tradeoff
in multicast scheduling for green cellular systems,” IEEE J. Sel. Areas
Commun., vol. 34, no. 5, pp. 1235-1249, May 2016.

[18] B. Zhou, Y. Cui, and M. Tao, “Optimal dynamic multicast scheduling for
cache-enabled content-centric wireless networks,” IEEE Trans. Commun.,
vol. 65, no. 7, pp. 2956-2970, Jul. 2017.

[19] Z. Zhang, H. Chen, M. Hua, C. Li, Y. Huang, and L. Yang, “Double
coded caching in ultra dense networks: Caching and multicast scheduling
via deep reinforcement learning,” IEEE Trans. Commun., vol. 68, no. 2,
pp. 1071-1086, Feb. 2020.

[20] L. Zhong, C. Xu, J. Chen, W. Yan, S. Yang, and G.-M. Muntean, “Joint
optimal multicast scheduling and caching for improved performance
and energy saving in wireless heterogeneous networks,” IEEE Trans.
Broadcast., vol. 67, no. 1, pp. 119-130, Mar. 2021.

[21] D. P. Bertsekas, Dynamic programming and optimal control: volume I,

Athena scientiﬁc Belmont, Belmont, MA, 2012.

[22] T. P. Lillicrap, J. J. Hunt, A. Pritzel, N. Heess, T. Erez, Y. Tassa, D.
Silver, and D. Wierstra, “Continuous control with deep reinforcement
learning,” in ICLR, San Juan, Puerto Rico, USA, May 2016.

[23] D. Tse and P. Viswanath, Fundamentals of wireless communication,

Cambridge university press, Cambridge, 2005.

[24] G. Dulac-Arnold, R. Evans, H. van Hasselt, P. Sunehag, T. Lillicrap, J.
Hunt, T. Mann, T. Weber, T. Degris, and B. Coppin, “Deep reinforcement
learning in large discrete action spaces,” arXiv preprint arXiv:1512.07679,
Dec. 2015.

[25] V. Mnih, K. Kavukcuoglu, D. Silver, A. A. Rusu, J. Veness, M. G.
Bellemare, A. Graves, M. Riedmiller, A. K. Fidjeland, G. Ostrovski et
al., “Human-level control through deep reinforcement learning,” Nature,
vol. 518, no. 7540, pp. 529-533, Feb. 2015.

[26] J. Schulman, F. Wolski, P. Dhariwal, A. Radford, and O. Klimov, “Prox-
imal policy optimization algorithms,” arXiv preprint arXiv:1707.06347,
Jul. 2017.

[27] C. Michele, G. Cornujols, and G. Zambelli, Integer programming,

Springer, Berlin, Germany, 2014.

[28] V. Mnih, A. P. Badia, M. Mirza, A. Graves, T. Lillicrap, T. Harley, D.
Silver, and K. Kavukcuoglu, “Asynchronous methods for deep reinforce-
ment learning,” in Proc. ICML, New York City, NY, USA, June 2016,
pp. 1928-1937.

