Leveraging Causal Inference for Explainable
Automatic Program Repair

Jianzong Wang, Shijing Si∗, Zhitao Zhu, Xiaoyang Qu, Zhenhou Hong and Jing Xiao
Ping An Technology (Shenzhen) Co., Ltd., Shenzhen, China
Email: jzwang@188.com, shijing.si@outlook.com, andyzzt@mail.ustc.edu.cn,
quxiaoy@gmail.com, hongzhenhou168@pingan.com.cn, xiaojing661@pingan.com.cn

2
2
0
2

n
u
J

6

]
E
S
.
s
c
[

2
v
2
4
3
3
1
.
5
0
2
2
:
v
i
X
r
a

Abstract—Deep learning models have made signiﬁcant progress
in automatic program repair. However, the black-box nature
of these methods has restricted their practical applications. To
address this challenge, this paper presents an interpretable ap-
proach for program repair based on sequence-to-sequence models
with causal inference and our method is called CPR, short for
causal program repair. Our CPR can generate explanations in the
process of decision making, which consists of groups of causally
related input-output tokens. Firstly, our method infers these
relations by querying the model with inputs disturbed by data
augmentation. Secondly, it generates a graph over tokens from
the responses and solves a partitioning problem to select the most
relevant components. The experiments on four programming
languages (Java, C, Python, and JavaScript) show that CPR can
generate causal graphs for reasonable interpretations and boost
the performance of bug ﬁxing in automatic program repair.

Index Terms—Automated Program Repair, Program Analysis,
Sequence-to-sequence Model, Causal Inference, Interpretability

I. INTRODUCTION

The goal of automatic program repair (APR) techniques
is to automatically correct bug(s) in a piece of existing
troublesome source code. Many researches have been devoted
on applying machine learning techniques to APR [1]. Given
the similarity between a program repair task and generic natural
language processing (NLP) tasks such as sequence-to-sequence
(Seq2Seq) learning and machine translation [2], there has been
a lot of work on applying machine learning for program repair
[3], [4], [5] and [6] in recent years. Similar techniques have
been applied to code related tasks [7], [8], [9].

Although deep learning based Seq2Seq models have achieved
overwhelming success in APR tasks, there still remains a lot of
potential for improvement [10], [11]. One major concern is the
interpretability of the deep Seq2Seq models, which is caused by
the complicated nature of model architectures [12], [13]. The
interpretability of deep models is often categorized into two
types: 1.) model interpretability, which attempts to make the
deep neural architecture itself interpretable and transparent, and
2.) prediction interpretability, which aims to explain particular
predictions of the deep model
[14]. Once a deep APR model
has achieved these two aspects of interpretability, it becomes
trustworthy, transparent, and controllable, which surely makes
deep models more useful for practical deployment.

∗Corresponding author: Shijing Si, sishijing204@pingan.com.cn.

To improve the interpretability of APR models, this paper
introduces causal inference, which has shown great promise
in discovering causal relations in deep learning [15]. Simply
combining APR with causal inference can lead to a few sig-
niﬁcant challenges. This paper aims to address three important
unsolved issues about how to use causal inference in APR
models. Because transparency in the deep APR models is often
very restrictive and challenging to achieve [16], the ﬁrst issue is
how can we alleviate this difﬁcult situation in real application.
The second issue is how to deﬁne the causal graph in the
APR task. The third issue is what kind of performance can be
achieved by APR model combined with causal inference.

For the ﬁrst issue, in this work we concentrate on prediction
interpretability rather than model
transparency. From the
machine learning community, prediction interpretability can be
sought more easily with existing mehtods, like Monte Carlo
Dropout, Ensemble or Bayesian deep neural networks [17]. In
a typical APR problem, the inputs are the original code and
comments, the output is the predicted repaired code. To tackle
the second issue, we can deﬁne the causal graph since we are
focusing on the interpretable relationship between input and
output. In a causal graph, the context is the input of code and
comments, the outcome is the predicted repaired code, and the
confounder is the data disturbance, i.e., the data augmentation.
For the third challenge, we alter the potential confounder, which
is one kind of text data augmentation, and observe the effect
of APR with causal inference.

Speciﬁcally, we propose to utilize data augmentation strategy
to discover the causal relations between the input source (that
is, the code and comments) and the corrected bugs, which
can improve the prediction interpretability of APR models.
Data augmentation methods are used to disturb the original
inputs, which are then fed into the deep Seq2Seq model to infer
the causal relation between the input and output tokens in the
similar manner as [15]. [15] utilized a Variational auto-Encoder
for data disturbation, which is computationally costly and hard
to control. Our method is called CPR, short for Causal Program
Repair, which leverages text data augmentation and easy to
implement.

Our main contributions can be listed as follows:
• We proposed to utilize data augmentation strategy for the
input perturbations required for causal analysis of program
repair

 
 
 
 
 
 
(a) The framework of Seq2Seq model in
neural machine translation (NMT).

(b) The framework of Seq2Seq model in
APR.

(c) The framework of Seq2Seq model in
CPR.

Fig. 1. Comparison of three frameworks of NMT, APR and CPR. The boxes and arrows in blue in (c) mean the causal parts.

• In the experiments, our framework can offer explanations
on various Seq2Seq models in APR, which exhibits its
ability to enhance the performance and provide insights
into how the black-box models make predictions.

II. RELATED WORKS

Our CPR is closely related to two lines of research: Seq2Seq
model for automatic code repair and the causal inference.
Therefore, we discuss the related research in the following two
subsections.

A. Sequence-to-Sequence Models in Automatic Code Repair

There are many existing works that attempt to use machine
translation techniques to automatically ﬁx bugs. Here are
the most notable methods in this area. [18] formulates the
patches generation as a Seq2Seq translation problem and
proposes to use a neural machine translation (NMT) model with
attention-based Encoder-Decoder. To evaluate the performance
of various APR methods, [19] presents BEARS, a project for
collecting and storing bugs into an extensible bug benchmark
for automatic repair studies in Java. SequenceR [4] leverages
a Seq2Seq model by combining an encoder and a decoder
architecture, where recurrent neural networks (RNNs) with
LSTM gates are used for both the encoder and decoder. [10]
propose an APR framework using formal speciﬁcation and
expression templates.

Using a RNN encoder-decoder, [20] investigates the appli-
cation of NMT for candidate patches generation. Applying
NMT techniques and ensemble learning, CoCoNuT [6] can
automatically repair programs of multiple languages in a
end-to-end manner. [21] proposes a new NMT-based APR
technique, called CURE, which leverages pre-training, subword
tokenization and an efﬁcient code-aware search strategy. [22]
proposes a real-time code ﬁx mechanism by semantic code
suggestions, which is shown to improve the speed of repairing
faulty programs. More details on APR can be found in [23].

B. Causal Inference

There is a very large body of work that attempts to address
the issue of ”explanation”, but it is actually quite biased due
to the different deﬁnitions of ”explanation” that are of interest.
In the ﬁeld of machine learning, the area where interpretability
is probably most valued is in medical applications, where

the credibility of a predictive model depends heavily on its
interpretability [24]. With the advent of the deep learning trend,
recent work has enhanced each of the two aspects of inter-
pretability: model transparency [25] and model functionality
[26]. For a broad survey of interpretability in deep learning,
we refer the reader to the survey [27].

Model interpretability methods based on causal inference
(counterfactual samples) have been increasingly applied to
various scenarios. Among them, the sample-based explanation
approach aims to explain the decision and judgment process
of the model by ﬁnding sample examples. [14], [28] proposes
a model that justiﬁes the prediction results using fragments
of the input. One of the most
typical approaches is the
counterfactual explanations that interpret the model’s decisions
by making minimal changes to the features on the existing
samples and obtaining the expected counterfactual results, and
collecting these samples with minor changes. [29] relies on
local perturbations of the instance to explain the predictions of
the black-box classiﬁers. A generic counterfactual generator
with sequential control of perturbation types and positions is
further proposed by [30], which can generate diverse sets of
realistic counterfactuals that can be useful in various distinct
applications.

III. PROPOSED METHOD

Comparing the Fig. 1(a), (b) and (c), we can transition
from NMT to APR with causal inference, which is Fig. 1(c).
Comparing Fig. 1 (a) and (b), the process of APR is similar to
NMT. Both input source text and target text, and the expected
text is inferred through the Seq2Seq model. According to
Fig. 1(b), we propose an APR framework with causal analysis,
as illustrated by Fig. 1(c).

As demonstrated in Fig. 2(a), we deﬁne a causal graph. In
causal inference, capital X represents the treatment, capital A
represents the potential confounder, and capital Y represents
the outcome. In the APR problem, capital X is the input of
code and comments, capital A indicates the data augmentation,
and capital Y is the output of predicted repaired code. Based on
the data augmentation, potential confounder A will inﬂuence
X and Y, so we deﬁne the causal graph like the Fig. 2(a). To
identify the input and output causality, we need to know the
predict interpretability after we’ve deﬁned the causal graph.
Changing the input through data augmentation methods, we

Source Text EmbeddingEncoderDecoderStopLinearTextLinearEnglish Source TextStopTokenPredict Chinese TextContext EmbeddingBuggy CodeEncoderDecoderStopLinearTextLinearStopTokenComment TextPredictRepaired Code+Context EmbeddingEncoderDecoderStopLinearCodeLinearPredictRepaired CodeStopTokenCausalInferenceExplanationSelectionPerturbationInput+Comment TextBuggy Code   →       (a) The causal graph in APR
task.

(b) The input and output
connections.

Fig. 2. The causal graph in APR task.

may ﬁgure out how the input inﬂuences the output, as shown
in Fig. 2(b).

Our approach formalizes this framework through a pipeline
(sketched in Fig. 1(c)) that consists of three main components,
each of which is described in detail in the following section: a
perturbation model for locally exercising F , a causal inference
model for inferring associations between inputs and predictions,
and a selection step for partitioning and selecting the most
relevant sets of associations.

Fig. 3. The framework of CPR.

The ﬁrst step in the explainable APR approach is to obtain
perturbed versions of the input: which involves changing the
elements and their order in original input sentences. In [15],
they propose a variational auto-encoder(VAE) model to perturb
the text. However, training a VAE or bidirectional LSTM
language model takes a lot of time and effort.

Based on [31], [32], here we use the data augmentation to
generate the perturbed comment texts. For word perturbation:
1) Synonym Replacement (SR): Select m words randomly
from the original sentence that does not include stop words.
Then alter these words with one of closeness meaning
chosen at random.

2) Random Insertion (RI): Find a most similar meaning for
a random word in the original sentence (not include stop
words). Insert that most similar word into a stochastic
position in the sentence. Repeat m times.

3) Random Swap (RS): Casually select two words from the
original sentence and switch their positions in m times.
4) Random Deletion (RD): With probability p. Delete words

from the original sentence randomly.

For sentence perturbation, we use Back-Translation (BT):
Translating an existing example x in language A into another
language B and then translating it back into A to obtain an
augmented example ˆx.

Long sentences will perform more word perturbation since
they include more words than short ones. To fair comparison,
we vary the number m for word perturbation based on the

sentence length l with the formula m = (cid:98)αl(cid:99), where α is a
parameter that indicates the percent of the changed words in
a sentence (we use p = α for RD). Furthermore, we generate
mdist disturbed sentences for each original sentence.

To perturb the comment texts, the data augmentations operate
the level of the words, and the symbols like {, : or == e.t.c.
Symbols are also an essential part of a programming language.
This data augmentation strategy achieves similar outcomes
but is considerably easier to employ because it does not involve
the training of a language model and the usage of external
datasets. We argue that it can be easily ported to other similar
models.

IV. EXPERIMENTAL SETUP

A. Datasets

To train the ARP models in different programming languages,
we collected corresponding training datasets for different
programming languages based on the open-source datasets. For
Java, we use Defects4J [33] and QuixBugs [34]. For Python,
we use Python’s version of QuixBugs. For C, we used the
ManyBugs datasets from prior work [35]. In order to compare
with the Java, Python, and C program language, we select 12
JavaScript examples connected with ordinary bug problems in
prior work (BugAID) [36].

(a)

(b)

Fig. 4. We inferred dependency graphs before (left) and after (right) explanation
selection for the prediction. Our framework generates dependency estimates
and explanation graphs. The green circles mean the buggy code, the yellow
circles mean the comments text, and the blue circles mean debugged code. (a)
Raw Dependencies.(b) Explanation Graph.

B. Settings

Training. In order to ensure the completeness of comparison,
the details of training parameters are given here. Following
Fig. 1(c), we have two parts that need to train and inference,
the APR model and the explainable graph. First, we use
hyperband [37] to tune hyper-parameters. We restrict the
range of hyper-parameters to appropriate values. The size of
embedding is 64 to 512, the dimensions of convolutional layer
is 32 to 256, the number of convolutional layers is 1 to 10,
the size of hidden units of LSTM is 128 to 512, the size

XYAAxyxyy12123x3CausalInferenceExplanationSelectionPerturbationInputX   →        (  ∪  , )APR modelYXY~~need to specify the range of list arr, starting from index kforinx[k::arr:infor]xarrneed to specify the range of list arr, starting from index k.forinx[::xarr:infor]karrTABLE I
COMPARISON OF THE FIXED BUGS WITH CAUSAL INFERENCE AND WITHOUT CAUSAL INFERENCE. “CI” MEANS CAUSAL INFERENCE. THE EFFECTIVENESS
OF MODELS IS MEASURED BY THE NUMBER OF FIXED BUGS

Dataset

Model

Paremeters Language

Bugs number

ﬁxed w/o CI

ﬁxed with CI (CPR)

Defects4J
Defects4J
Defects4J

QuixBugs
QuixBugs
QuixBugs

QuixBugs
QuixBugs
QuixBugs

SequenceR
FConv
CodeBERT

SequenceR
FConv
CodeBERT

SequenceR
FConv
CodeBERT

ManyBugs
ManyBugs
ManyBugs CodeBERT

SequenceR
FConv

BugAID
BugAID
BugAID

SequenceR
FConv
CodeBERT

38M
23M
675M

38M
23M
675M

38M
23M
675M

38M
23M
675M

38M
23M
675M

Java
Java
Java

Java
Java
Java

Python
Python
Python

C
C
C

JavaScript
JavaScript
JavaScript

393
393
393

40
40
40

40
40
40

69
69
69

12
12
12

12
12
14

7
8
13

6
9
12

11
9
13

4
3
6

20
21
23

13
11
17

13
14
19

16
15
18

6
3
7

of Transformer heads is 4 to 8, the size of hidden units of
Transformer is 128 to 512. The learning rate is 0.1, 0.01,
0.001, and 0.0001. Then we train APR models for one epoch
on the different training sets with various hyper-parameters
for tuning. We are ranking the hyper-parameters sets based
on their perplexity. Perplexity is a typical metric in NLP that
quantiﬁes how well a model generates a sequence. We make
it stop at convergence or until 20 epochs and ranking the the
top-k (default k=5) models. In inference mode, we employ
beam search with a beamwidth of 784.

Explainable graph. We equally chose a data disturbance
method to generate the disturbed input based on the proposed
data augmentation strategy. For the explainable graph step, we
use the robust clustering method of [38], [39] to generate the
graph in the experiment. These bilateral clustering methods do
not take uncertainty into account.

Infrastructure We use the LSTM, and Transformer is running
on Pytorch [40]. The implementations of FConv is provided
by fairseq-py [41]. We also use the implementation of Code-
BERT [42] running on Pytorch. Our models were trained and
evaluated on an Intel Xeon E5-2695 with 4 NVIDIA V100
GPUs.

Performance The average time it takes to train the APR models
for one epoch during tuning is 51 minutes. Sequentially for the
SequenceR, FConv, and CodeBERT, it takes 93, 67, and 253
hours to train the model until convergence. In the inference
stage, producing 1000 patches for a bug brings 9 seconds on
average.

C. Evaluation and Results

The performance with causal inference Table I displays the
number of bugs ﬁxed by the different approaches with or
without causal inference. For all datasets, causal inference is
used to improve the effect of debugging code. We test in four
languages, and both models have been improved using causal
inference. On the BugAID dataset, which is for JavaScript
language, We observed an intriguing result. The debugs number
for all three models is relatively less, and for FConv, the causal
inference is not working. For SequenceR and CdoeBERT, the
effect of improvement is not obvious. We believe that the main
reason is that the amount of data is too small.

Table II depicts the different consequences of different data
disturbance approaches in our framework. The RI, RS, and
RD approaches would degrade the performance of APR with
causal inference. In the FConv model with the RD method, its
performance is almost the same as that of a model without
causal inference. Likewise, the CodeBERT with the RS method,
which reduces the number of ﬁxed codes to 12, achieves the
same performance without causal inference. When using the
APR model with causal inference, the SR and BT methods are
the best choices. It demonstrates that the SR and BT methods
have the best effect in the APR model with causal inference.
It helps when the APR model is with the explainable graph
because the SR and BT methods can maintain the original
meaning of comments.

Fig. 5 shows the input and output’s interpretable relation with
explanation graphs. The connected blue circle with more gray
lines from green circles is more relevant. The density of gray
lines might show whether or not the error code corresponds

TABLE II
COMPARISON OF DIFFERENT DATA DISTURBANCES IN OUR FRAMEWORK. “CI” MEANS CAUSAL INFERENCE. THE EFFECTIVENESS OF MODELS IS
MEASURED BY THE NUMBER OF FIXED BUGS.

Dataset

Model

Paremeters Language Bugs number Data augmentation

ﬁxed w/o CI

ﬁxed with CI (CPR)

QuixBugs

SequenceR

QuixBugs

SequenceR

QuixBugs

SequenceR

QuixBugs

SequenceR

QuixBugs

SequenceR

QuixBugs

QuixBugs

QuixBugs

QuixBugs

QuixBugs

FConv

FConv

FConv

FConv

FConv

QuixBugs CodeBERT

QuixBugs CodeBERT

QuixBugs CodeBERT

QuixBugs CodeBERT

QuixBugs CodeBERT

38M

38M

38M

38M

38M

23M

23M

23M

23M

23M

675M

675M

675M

675M

675M

Python

Python

Python

Python

Python

Python

Python

Python

Python

Python

Python

Python

Python

Python

Python

40

40

40

40

40

40

40

40

40

40

40

40

40

40

40

to the proper code. As shown in Fig. 5, the incorrect part of
the buggy code is most connected to the correct part in the
repaired code.

(a)

(c)

(b)

(d)

Fig. 5. Dependency estimates and explanation graph generated by our
framework. The green circles means the buggy code, the yellow circles means
the comments text, the blue circles means debugged code.(a) Explanation
graph in Python.(b) Explanation graph in Python.(c) Explanation graph in
Java.(d) Explanation graph in Java.

SR

RI

RS

RD

BT

SR

RI

RS

RD

BT

SR

RI

RS

RD

BT

(a)

(c)

(e)

10

8

7

8

9

11

8

9

9

12

14

11

9

9

13

13

10

11

11

12

14

11

12

10

14

18

15

12

13

19

(b)

(d)

(f)

Fig. 6. Dependency estimates and explanation graph generated with data
disturbances. The green circles mean the buggy code, the yellow circles mean
the comments text, and the blue circles mean debugged code. (a) Explanation
graph in Python, without data disturbances. (b) Explanation graph in Python,
with SR. (c) Explanation graph in Python, with RI. (d) Explanation graph in
Python, with RD. (e) Explanation graph in Python, with RS. (f) Explanation
graph in Python, with BT.

The explanation graph in data perturbation. Fig. 5 shows
the explanation graph when using data augmentation. In Fig. 5
(a), (b), and (c), the buggy code is all connected to the most
relevant input. Especially in Fig. 5(b), the incorrect symbol is
connected to the correct symbol, where the second blue circle
is the right answer. In Fig. 5 (d), the debugged code deletes the
wrong parts compared to the buggy code. Since the solution
is to delete the buggy code, the input is rarely connected to
the output.

Fig. 6 (b), (c), (d), (e), and (f) illustrate the circumstance
when the data augmentation is implemented for the comment
text. It shows that the comment text is still connected to the
most relevant part but the irrelevant connections have increased.
As in Fig. 6 (c), (d), and (e), the second blue circle gets
the connection, but the graph has deleted some unimportant
connections to other circles. Data augmentation like RI, RD

need to specify the range of list arr, starting from index k.forinx[::xarr:infor]karrThe bitwise AND operator is wrong.nn^=&=nn-1-1:for(count){arrIntegerfor(:)count{Integercountsarr is counts in fixed versiondelete the flaatenreturnreturnflatten(arrarr);;The bitwise AND operator is wrong.nn^=&=nn-1-1The bitwise AND operator is incorrectn1n^=nn&=--1The bitwise cat AND operator is incorrect.nnn&=-1^=-1nn-nn&=1The operator is wrong.^=-1nbitwise The is AND wrong.n1nnn1-&=^=-The bitwise AND symbol is inaccurate.nn^=n-1&=-1nand RS will cause some irrelevant connections to grow. The
SR and BT methods can preserve the correct connections as
seen in Fig. 6.

Overall, Fig. 6 shows that our framework can demonstrate
the interpretability of input and output. The data augmentation
strategy and the explanation graph continuously provide im-
provements to model effects, but appropriate data augmentation
approaches will make them more powerful.

V. CONCLUSION

Our data augmentation approach combined with a model-
agnostic framework for prediction interpretability can produce
reasonable, related explanations. We formulate the explanation
problem of APR model in causal inference. Our methods can
be generalized to a variety of settings in which inputs and
outputs can be expressed as sets of features. Also, we used
data augmentation sampling for data disturbances. Experiments
with various data augmentation methodologies suggest that the
APR model can be utilized as a causal inference tool.

ACKNOWLEDGMENT

This paper is supported by the Key Research and De-
velopment Program of Guangdong Province under grant
No.2021B0101400003. Corresponding author is Shijing Si
from Ping An Technology (Shenzhen) Co., Ltd (sishi-
jing204@pingan.com.cn).

REFERENCES

[1] M. Monperrus, “Automatic software repair: a bibliography,” ACM

Computing Surveys (CSUR), vol. 51, no. 1, pp. 1–24, 2018.

[2] D. Sundararaman, V. Subramanian, G. Wang, S. Si, D. Shen, D. Wang,
and L. Carin, “Syntactic knowledge-infused transformer and bert models,”
in Proceedings of the CIKM 2021 Workshops, 2021.

[3] M. Tufano, C. Watson, G. Bavota, M. Di Penta, M. White, and
D. Poshyvanyk, “An empirical investigation into learning bug-ﬁxing
patches in the wild via neural machine translation,” in Proceedings of
ASE, 2018, pp. 832–837.

[4] Z. Chen, S. J. Kommrusch, M. Tufano, L.-N. Pouchet, D. Poshyvanyk,
and M. Monperrus, “Sequencer: Sequence-to-sequence learning for end-
to-end program repair,” IEEE Transactions on Software Engineering,
2019.

[5] Y. Li, S. Wang, and T. N. Nguyen, “Dlﬁx: Context-based code
transformation learning for automated program repair,” in Proceedings
of ICSE, 2020, pp. 602–614.

[6] T. Lutellier, H. V. Pham, L. Pang, Y. Li, M. Wei, and L. Tan, “Coconut:
Combining context-aware neural translation models using ensemble for
program repair,” in Proceedings of the 29th ACM SIGSOFT International
Symposium on Software Testing and Analysis, 2020, pp. 101–114.
[7] S. Iyer, I. Konstas, A. Cheung, and L. Zettlemoyer, “Summarizing source
code using a neural attention model,” in Proceedings of ACL, 2016, pp.
2073–2083.

[8] W. Ling, P. Blunsom, E. Grefenstette, K. M. Hermann, T. Koˇcisk`y,
F. Wang, and A. Senior, “Latent predictor networks for code generation,”
in Proceedings of ACL, 2016, pp. 599–609.

[9] P. Yin and G. Neubig, “A syntactic neural model for general-purpose

code generation,” in Proceedings of ACL, 2017, pp. 440–450.

[10] T.-T. Nguyen, Q.-T. Ta, and W.-N. Chin, “Automatic program repair
using formal veriﬁcation and expression templates,” in International
Conference on Veriﬁcation, Model Checking, and Abstract Interpretation.
Springer, 2019, pp. 70–91.

[11] H. Ye, M. Martinez, T. Durieux, and M. Monperrus, “A comprehensive
study of automatic program repair on the quixbugs benchmark,” Journal
of Systems and Software, vol. 171, p. 110825, 2021.

[12] Y. Dong, H. Su, J. Zhu, and B. Zhang, “Improving interpretability of
deep neural networks with semantic information,” in Proceedings of the
IEEE CVPR, 2017, pp. 4306–4314.

[13] L. Falissard, C. Morgand, W. Ghosn, C. Imbaud, K. Bounebache, G. Rey
et al., “Neural translation and automated recognition of icd-10 medical
entities from natural language: Model development and performance
assessment,” JMIR medical informatics, vol. 10, no. 4, p. e26353, 2022.
[14] T. Lei, R. Barzilay, and T. Jaakkola, “Rationalizing neural predictions,”

in Proceedings of EMNLP, 2016, pp. 107–117.

[15] D. Alvarez-Melis and T. Jaakkola, “A causal framework for explaining the
predictions of black-box sequence-to-sequence models,” in Proceedings
of EMNLP, 2017, pp. 412–421.

[16] M. O’Neill and L. Spector, “Automatic programming: The open issue?”
Genetic Programming and Evolvable Machines, vol. 21, no. 1, pp. 251–
262, 2020.

[17] J. Liu, Z. Lin, S. Padhy, D. Tran, T. Bedrax Weiss, and B. Lak-
shminarayanan, “Simple and principled uncertainty estimation with
deterministic deep learning via distance awareness,” NeurIPS, vol. 33,
pp. 7498–7512, 2020.

[18] H. Hata, E. Shihab, and G. Neubig, “Learning to generate cor-
rective patches using neural machine translation,” arXiv preprint
arXiv:1812.07170, 2018.

[19] F. Madeiral, S. Urli, M. Maia, and M. Monperrus, “Bears: An extensible
java bug benchmark for automatic program repair studies,” in 2019
IEEE 26th International Conference on Software Analysis, Evolution and
Reengineering (SANER).

IEEE, 2019, pp. 468–478.

[20] M. Tufano, C. Watson, G. Bavota, M. D. Penta, M. White, and
D. Poshyvanyk, “An empirical study on learning bug-ﬁxing patches in
the wild via neural machine translation,” ACM Transactions on Software
Engineering and Methodology (TOSEM), vol. 28, no. 4, pp. 1–29, 2019.
[21] N. Jiang, T. Lutellier, and L. Tan, “Cure: Code-aware neural machine
translation for automatic program repair,” in Proceedings of ICSE.
IEEE,
2021, pp. 1161–1173.

[22] D. Campos, A. Restivo, H. S. Ferreira, and A. Ramos, “Automatic pro-
gram repair as semantic suggestions: An empirical study,” in Proceedings
of ICST.

IEEE, 2021, pp. 217–228.

[23] T. Mamatha, B. Reddy, and C. S. Bindu, “A literature review on automated
code repair,” in Proceedings of the 2nd International Conference on
Recent Trends in Machine Learning, IoT, Smart Cities and Applications.
Springer, 2022, pp. 249–260.

[24] R. Caruana, Y. Lou, J. Gehrke, P. Koch, M. Sturm, and N. Elhadad,
“Intelligible models for healthcare: Predicting pneumonia risk and hospital
30-day readmission,” in Proceedings of SIGKDD, 2015, pp. 1721–1730.
“Understanding deep image
the IEEE
IEEE Computer Society, 2015, pp. 5188–5196. [Online].

representations by inverting them,” in Proceedings of
CVPR.
Available: https://doi.org/10.1109/CVPR.2015.7299155

[25] A. Mahendran and A. Vedaldi,

[26] W. Zhao, S. Oyama,

and M. Kurihara,

“Generating natural
counterfactual visual explanations,” in Proceedings of the Twenty-Ninth
International Joint Conference on Artiﬁcial Intelligence, IJCAI 2020,
ijcai.org, 2020, pp. 5204–5205. [Online]. Available:
C. Bessiere, Ed.
https://doi.org/10.24963/ijcai.2020/742

[27] F. Doshi-Velez and B. Kim, “A roadmap for a rigorous science of

interpretability,” arXiv preprint arXiv:1702.08608, vol. 2, p. 1, 2017.

[28] M. T. Ribeiro, S. Singh, and C. Guestrin, “ ¨why should i trust you?”
explaining the predictions of any classiﬁer,” in Proceedings of SIGKDD,
2016, pp. 1135–1144.

[29] P. Cheng, W. Hao, S. Yuan, S. Si, and L. Carin, “Fairﬁl: Contrastive
neural debiasing method for pretrained text encoders,” in ICLR, 2021.
[30] T. Wu, M. T. Ribeiro, J. Heer, and D. S. Weld, “Polyjuice: Generating
counterfactuals for explaining, evaluating, and improving models,” in
Proceedings of ACL/IJCNLP. ACL, 2021, pp. 6707–6723.

[31] J. Wei and K. Zou, “Eda: Easy data augmentation techniques for boosting
performance on text classiﬁcation tasks,” in Proceedings of EMNLP-
IJCNLP, 2019, pp. 6383–6389.

[32] S. Edunov, M. Ott, M. Auli, and D. Grangier, “Understanding back-
translation at scale,” in Proceedings of EMNLP, 2018, pp. 489–500.

[33] R. Just, D. Jalali, and M. D. Ernst, “Defects4j: A database of existing
faults to enable controlled testing studies for java programs,” in
Proceedings of the 2014 International Symposium on Software Testing
and Analysis, 2014, pp. 437–440.

[34] D. Lin, J. Koppel, A. Chen, and A. Solar-Lezama, “Quixbugs: A multi-
lingual program repair benchmark set based on the quixey challenge,”
in Proceedings Companion of the 2017 ACM SIGPLAN International
Conference on Systems, Programming, Languages, and Applications:
Software for Humanity, 2017, pp. 55–56.

[35] C. Le Goues, N. Holtschulte, E. K. Smith, Y. Brun, P. Devanbu, and
S. Forrest, et al., “The manybugs and introclass benchmarks for automated
repair of c programs,” IEEE Transactions on Software Engineering,
vol. 41, no. 12, pp. 1236–1256, 2015.

[36] Q. Hanam, F. S. d. M. Brito, and A. Mesbah, “Discovering bug patterns in
javascript,” in Proceedings of the 2016 24th ACM SIGSOFT international
symposium on foundations of software engineering, 2016, pp. 144–156.
[37] L. Li, K. Jamieson, G. DeSalvo, A. Rostamizadeh, and A. Talwalkar,
“Hyperband: a novel bandit-based approach to hyperparameter optimiza-
tion,” The Journal of Machine Learning Research, vol. 18, no. 1, pp.
6765–6816, 2017.

[38] I. S. Dhillon, “Co-clustering documents and words using bipartite spectral

graph partitioning,” in Proceedings of SIGKDD, 2001, pp. 269–274.

[39] Y. Kluger, R. Basri, J. T. Chang, and M. Gerstein, “Spectral biclustering
of microarray data: coclustering genes and conditions,” Genome research,
vol. 13, no. 4, pp. 703–716, 2003.

[40] A. Paszke, S. Gross, F. Massa, A. Lerer, J. Bradbury, and G. Chanan,
et al., “Pytorch: An imperative style, high-performance deep learning
library.” in NeurIPS, 2019.

[41] J. Gehring, M. Auli, D. Grangier, D. Yarats, and Y. N. Dauphin,
“Convolutional sequence to sequence learning,” in ICML. PMLR, 2017,
pp. 1243–1252.

[42] Z. Feng, D. Guo, D. Tang, N. Duan, X. Feng, and M. Gong, et al.,
“Codebert: A pre-trained model for programming and natural languages,”
in Proceedings of EMNLP: Findings, 2020, pp. 1536–1547.

