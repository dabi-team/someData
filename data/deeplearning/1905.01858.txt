DeepCheck: A Non-intrusive Control-ﬂow Integrity
Checking based on Deep Learning

Jiliang Zhang, Wuqiao Chen, Yuqi Niu
College of Computer Science and Electronic Engineering
Hunan University, China
Email: zhangjiliang@hnu.edu.cn

9
1
0
2

y
a
M
6

]

R
C
.
s
c
[

1
v
8
5
8
1
0
.
5
0
9
1
:
v
i
X
r
a

Abstract—Code reuse attack (CRA) is a powerful attack that
reuses existing codes to hijack the program control ﬂow. Control
ﬂow integrity (CFI) is one of the most popular mechanisms to
prevent against CRAs. However, current CFI techniques are
difﬁcult to be deployed in real applications due to suffering
several issues such as modifying binaries or compiler, extending
instruction set architectures (ISA) and incurring unacceptable
runtime overhead. To address these issues, we propose the ﬁrst
deep learning-based CFI technique, named DeepCheck, where
the control ﬂow graph (CFG) is split into chains for deep neural
network (DNN) training. Then the integrity features of CFG can
be learned by DNN to detect abnormal control ﬂows. DeepCheck
does not interrupt the application and hence incurs zero runtime
overhead. Experimental results on Adobe Flash Player, Nginx,
Proftpd and Firefox show that the average detection accuracy of
DeepCheck is as high as 98.9%. In addition, 64 ROP exploits
created by ROPGadget and Ropper are used to further test the
effectiveness, which shows that the detection success rate reaches
100%.

I. INTRODUCTION

Insecure system programming languages such as C and C++
lead to a number of vulnerabilities in the software. According
to the recent security threat report [1], the number of software
vulnerabilities has increased signiﬁcantly in the past 10 years.
Attackers exploit various vulnerabilities to invade the system
without authorization to steal personal privacy information,
damage computer systems, spread computer viruses, and even
conduct malicious blackmail.

Code Reuse Attacks (CRAs), such as return-oriented pro-
gramming (ROP) [2] and jump-oriented programming (JOP)
[3], exploit the memory overﬂow vulnerabilities and reuse the
existing small code fragments called gadgets that end with the
branch instruction such as ret or jmp to hijack the program
control ﬂow without injecting any malicious codes to perform
malicious actions [2]. CRAs have been extended to different
platforms [4] such as PowerPC, Atmel AVR, SPARC, Harvard
and ARM, and shown the powerful attack on well-known
commercial software such as Adobe Reader [5], Adobe Flash
Player [6] and QuickTime Player [7].

Address Space Layout Randomization (ASLR) and Control
Flow Integrity (CFI) are two mainstream defense techniques
against CRAs. ASLR randomizes the code offset in virtual
memory so that attackers are difﬁcult to derive the addresses of
gadgets to hijack the control ﬂow. ASLR includes instruction
location randomization [8], code pointer randomization [9],
function randomization [10], and real-time code page address

randomization [11]. However, the security of ASLR is affected
by the randomized entropy, and the randomized addresses can
be guessed by brute-force attacks [12]. In addition, ASLR can
be bypassed with advanced CRAs, such as JIT-ROP [13] and
COOP [14]. The CFI security policy dictates that software
execution must follow the paths of a control ﬂow graph
(CFG) determined ahead of time [15]. Current CFI techniques
can be classed into software-based [15]–[18] and hardware-
assisted [19]–[22] CFI. Software-based CFI inserts special
detection instructions into the binary ﬁle [15] or adds a runtime
monitoring module [17], [18], [23] to detect abnormal control
ﬂows without the special hardware support and hence it is easy
to deploy. However, the software-based CFI need to modify the
binary code or the compiler and incurs high runtime overhead.
Recently, hardware-assisted CFI has attracted much attention
as it can reduce the runtime overhead largely by introducing
special control ﬂow checking instructions [20], [24]–[26] or
additional hardware modules [27], [28]. However, hardware-
assisted CFI suffers from the compatibility issue because of the
requirement of ISA extension, compiler modiﬁcation and/or
special hardware support. The advantages and disadvantages
of current defenses are summarized in Table I.

This paper proposes the ﬁrst deep learning-based CFI
technique, named DeepCheck. DeepCheck does not need to
modify ISAs and compilers and will not incur extra runtime
overhead, thus it has good compatibility and practicability. We
have implemented the prototype of DeepCheck on Linux. The
experimental results on four commercial applications show
that DeepCheck has high accuracy (98.9%), low false positive
rate (0.15%) and low false negative rate (0.60%). The real
CRA payloads are further utilized to test the effectiveness of
DeepCheck, which shows that the detection success rate is up
to 100%.

The source code to reproduce our experiment is available

online at http://hardwaresecurity.cn/DeepCheckCode.zip.

The rest of this paper is organized as follows. Section II in-
troduces some related deﬁnitions, concepts and terminologies.
Section III gives a detailed introduction about the proposed
DeepCheck. Experimental results and analysis are reported
in Section IV. Limitations of DeepCheck are discussed in
Section V. Related work is elaborated in Section VI. Finally,
we conclude in Section VII.

 
 
 
 
 
 
TABLE I
ADVANTAGES AND DISADVANTAGES OF CURRENT DEFENSE MECHANISMS.

Techniques
Advantages

ASLR
Easy to deploy

Software-based CFI
Easy to deploy

Disadvantages

Easy to bypass

High runtime overhead
Binary rewriting

Hardware-assisted CFI
Low runtime overhead
ISA extension
Compiler modiﬁcation
Special hardware support

II. BACKGROUND

In this section, we provide a brief overview of the technical
concepts we use in the rest of this paper and give a detailed
explanation when necessary.

A. Code Reuse Attacks

There are many gadgets in a binary executable ﬁle. Each
gadget can complete a small step for a complete attack, such
as changing the value in a ﬁxed register or calling a system
function. CRAs can hijack the control ﬂow by ﬁnding the
existing gadgets with different functions in the original binary
and chain them to implement the achieve attack instead of
injecting malicious codes. Advanced CRAs differ in the use
of gadgets, e.g., COOP [14] regards a function as a gadget.

ROP and JOP are two representative CRAs. The ROP attack
utilizes the gadget ending with the ret instruction. The attacker
chains gadgets by overwriting the original return address of
the stack with the address of the required gadget through the
program vulnerability such as buffer overﬂow. When the ret
instruction is executed, the CPU will get the address of the
gadget instead of the original address of the program. In this
way, the attacker can hijack the control ﬂow of program. The
JOP attack uses indirect jmp instead of ret instruction to
change the control ﬂow. The attacker uses a dispatcher gadget
as a control unit to implement control ﬂow transfer from one
gadget to another. Since the destination address of the jmp
instruction is stored in the register, attackers need to modify
the register indirectly by modifying the contents of stack to
chain gadgets.

B. Control Flow Integrity

Fig. 1. An example of a forward CFI

Attackers utilize memory vulnerabilities to hijack the con-
trol ﬂow in order to execute the code in any other location in
memory. CFI forces the control ﬂow of a program to follow
the CFG by verifying the execution of branch instructions.
Before running the program, CFI analyzes the source program
or binary code to derive the normal execution path of the
program and generates a CFG. The security of CFI depends on
the accuracy of the CFG. CFI with ﬁne-grained CFG is more
secure than coarse-grained CFI. However, all static analysis
methods that statically analyze binary ﬁles can only build the
coarse-grained CFG [29].

Control ﬂow transfer can be classed into the forward and
backward. The forward control ﬂow is transferred by branch
instructions such as call and jmp. The backward control
ﬂow is transferred by the ret instruction. Fig. 1 shows an
example of a forward CFI. An attacker attempts to change
the control ﬂow from function c to function f. However, since
there is no direct edge from c pointing to f in CFG, such
malicious action will be prevented by CFI.

C. Intel Processor Trace

Intel Processor Trace (IPT) [30] is a new hardware feature
introduced in the Intel Core M 5th-generation processor for
software debugging and performance analysis. IPT tracks
the execution of each application dynamically and records
control ﬂow information in packets. Then these packets will
be compressed and stored in the physical memory. Since the
packets are not cached, the performance overhead of IPT is
low. Taken Not-Taken (TNT) packets, Target IP (TIP) packets,
and Flow Update Packets (FUP) are used for control ﬂow
tracking. The TNT collects the conditional branch information
and records its two states (Taken (1) or Not Taken (0)) with
1-bit. TIP records the destination address of indirect branches
(such as callq, ret, jmpq). Asynchronous events such as
exceptions and interrupts are saved in FUP.

IPT can track the entire execution of one or more programs.
Since the data packet only records the target address infor-
mation, it is essential to combine the recorded data packet
with the binary information to construct a complete CFG. The
administrator can set the IPT privilege to the kernel level or
user level by conﬁguring the speciﬁed parameters, which can
track the control ﬂow in different states. DeepCheck is used
to protect the user program. Hence, we set the IPT to the user
level tracking.

III. THE PROPOSED DEEPCHECK

We assume that some basic security mechanisms have been
deployed in the system, such as data execution prevention

DetectionTraining   ①③②void   a (int  x){ b(x); c(x);}void   b (int x){ e(x); f(x);}void   c (int x){ g(x);}a: …… call b: …… call c: ……b: …… call e: …… call f: ……c: …… call g: ……e: …… ……f: …… ……g: …… ……Binary FileGadgetsCoarse-Grained CFGConstructionDisassemblyTNT/TIP packageFine-Grained CFGIPTDNNTrainTNT/TIP packageDNNDecodeClassificationBenign GadgetIPTPayloadDetectionTraining DecodeGadget1Gadget2Gadget3Gadget4Gadget6Gadget5Gadget7Gadget10Gadget8Gadget9Gadget12Gadget11Edge 1Edge 2Edge 3Edge 4Edge 5Edge 6Edge 10Edge 7Edge 8Edge 9Push rbp;  0x55Mov rbp,rsi;  0x4889f5Ret  0xc3554889f55c39090554889f55c390..36753a31f64889e2e83..c0753a...8105ac3909090..362fc883f81753d9090..4151c3L = Maximum Gadget LengthGadget:DataSet:............InputLayer1000 neuronsReLUFollowed by 50% DropoutReLU,Followed by 50% DropoutReLU,Followed by 50% Dropout600 neurons200 neurons50 neuronsOutputLayerSoftmaxBinary FileGadgetsCoarse-Grained CFGConstructionDisassemblyTNT/TIP packageFine-Grained CFGIPTDNNTrainDNNDecodeClassificationBenign GadgetIPTPayloadDetectionTraining Decodepop rbxret...mov rax,rcxret...pop rdijmp rax...pop rcxretVirtual Memory...@gadget 1(data1)@gadget 2@gadget 3(data2)...StackFine-Grained CFGIPTCoarse-Grained CFGDecoderBinary FilePT PacketDecoderDisassembleData EncoderClassifierG2G3G4G5G1G6G7G8G9G10G11G12213574698101211G：Gadget...InputLayer1000 neuronsReLU,Followed by 50% DropoutReLU,Followed by 50% DropoutReLU,Followed by 50% Dropout600 neurons200 neurons50 neuronsOutputLayerSoftmaxEdge 11Edge 12Detection PhaseBinary FileCoarse-Grained CFGBranch InformationFine-Grained CFGBenign Gadget ChainsMalicious Gadget ChainsClassifierBranch InformationGadget ChainsClassifierNormal Control FlowCRATraining PhaseDisassembleIPTConstructSplitTrainIPTChainClassifyFig. 2. The framework of DeepCheck

(DEP). Therefore, the attacker cannot inject malicious code.
An attacker can read/write data segments arbitrarily, but can
only read/execute code segments. We also assume that the ap-
plication is trusted. However, an attacker can exploit program
vulnerabilities, such as buffer overﬂows, to obtain information
about any memory location. In addition, the program can’t
generate code dynamically or contain self-modifying code so
that
the CFG obtained by static analysis is correct. Most
applications can meet this assumption.

A. The Framework of DeepCheck

The basic framework of our proposed DeepCheck is shown
in Fig. 2. DeepCheck includes the training phase and detection
phase. In the training phase, the binary ﬁle is disassembled to
build a CFG where each node indicates a gadget. However,
such static analysis method can only construct a coarse-grained
CFG. Therefore, IPT is used to monitor the control ﬂow to
collect address information which is combined with the coarse-
grained CFG to build a ﬁne-grained CFG. Then, the CFG that
consists of gadgets is split to meet the requirement of the
input data for neural network training. In a CFG, two gadgets
are connected by one edge. Each chain split from the CFG is
a correct control ﬂow trace. These chains constitute a benign
training sample of the neural network model. The gadgets that
have no edges in the CFG can be connected to build malicious
samples because CRAs will connect some nodes that are not
connected in the CFG to violate normal control ﬂow.

Since neural network models can only recognize input data
in a ﬁxed format, we propose to encode the split gadget chains
with the coding method introduced in section C.

In the detection phase, IPT tracks each indirect branch
instruction during the program execution and records the
destination address in packets. Then the packets are parsed
and the destination address information is matched with the

time. It

encoded gadgets to construct the test data. Finally, the data are
submitted to the trained neural network for detection. If the
gadget chain is classiﬁed as an illegal control ﬂow transfer, the
program will be interrupted. Speciﬁcally, DeepCheck collects
IPT packets from the memory buffer in real
is
necessary to combine the obtained address with the binary
ﬁles to derive the instruction information related to the control
ﬂow transfer which requires to be encoded for DNN detection.
However, it will bring high performance overhead. To address
this issue, in the training phase, we calculate the offset of the
target address from the binary base address, and the offset is
combined with the generated gadget code to form an [offset,
gadget] pair. Therefore, in the detection phase, the encoded
data for the gadget can be obtained by using the address
information in IPT packets. When the program performs an
indirect control ﬂow transfer, the gadget at the indirect branch
is combined with the gadget at its destination address to form
a gadget chain. Then the chain is input to the trained classiﬁer
for detection. If the chain is detected as a negative sample, a
CRA will be detected.

The reason for constructing negative samples based on the
original CFG is: 1) A large number of data samples are
required for training the neural network. However, it is difﬁcult
to get sufﬁcient training samples in the real world. In addition,
it is arduous to collect different types of samples because the
payload that is used to build CRAs has myriad variations.
Fortunately, if CRAs are parsed from the control ﬂow transfer,
CRA variations can be controlled in a range. The original
CFG contains different types of gadgets that can be utilized to
conduct CRAs. Hence, a large number of negative samples can
be constructed. 2) In CRAs, the control ﬂow is changed from
a gadget to another. In the corresponding CFG, this process is
to connect a node to another node. Therefore, the negative
samples constructed based on the original CFG have little

DetectionTraining   ①③②void   a (int  x){ b(x); c(x);}void   b (int x){ e(x); f(x);}void   c (int x){ g(x);}a: …… call b: …… call c: ……b: …… call e: …… call f: ……c: …… call g: ……e: …… ……f: …… ……g: …… ……Binary FileGadgetsCoarse-Grained CFGConstructionDisassemblyTNT/TIP packageFine-Grained CFGIPTDNNTrainTNT/TIP packageDNNDecodeClassificationBenign GadgetIPTPayloadDetectionTraining DecodeGadget1Gadget2Gadget3Gadget4Gadget6Gadget5Gadget7Gadget10Gadget8Gadget9Gadget12Gadget11Edge 1Edge 2Edge 3Edge 4Edge 5Edge 6Edge 10Edge 7Edge 8Edge 9 Push rbp;  0x55 Mov rbp,rsi;  0x4889f5 Ret  0xc3554889f55c39090554889f55c390..36753a31f64889e2e83..c0753a...8105ac3909090..362fc883f81753d9090..4151c3L = Maximum Gadget LengthGadget:DataSet:............InputLayer1000 neuronsReLUFollowed by 50% DropoutReLU,Followed by 50% DropoutReLU,Followed by 50% Dropout600 neurons200 neurons50 neuronsOutputLayerSoftmaxBinary FileGadgetsCoarse-Grained CFGConstructionDisassemblyTNT/TIP packageFine-Grained CFGIPTDNNTrainDNNDecodeClassificationBenign GadgetIPTPayloadDetectionTraining Decodepop rbxret...mov rax,rcxret...pop rdijmp rax...pop rcxretVirtual Memory...@gadget 1(data1)@gadget 2@gadget 3(data2)...StackFine-Grained CFGIPTCoarse-Grained CFGDecoderBinary FilePT PacketDecoderDisassembleData EncoderClassifierG2G3G4G5G1G6G7G8G9G10G11G12213574698101211G：Gadget...InputLayer1000 neuronsReLU,Followed by 50% DropoutReLU,Followed by 50% DropoutReLU,Followed by 50% Dropout600 neurons200 neurons50 neuronsOutputLayerSoftmaxEdge 11Edge 12Detection PhaseBinary FileCoarse-Grained CFGBranch InformationFine-Grained CFGBenign Gadget ChainsMalicious Gadget ChainsClassifierBranch InformationGadget ChainsClassifierNormal Control FlowCRATraining PhaseDisassembleIPTConstructSplitTrainIPTChainClassifydifference with the real CRAs.

B. Fine-grained CFG Construction

CFG, the neural network learns the control ﬂow features based
on all applications. However, CRA changes the control ﬂow
of a particular application, which is difﬁcult to be detected.

The construction of CFG is critical for DeepCheck. Our
goal is to build a ﬁne-grained CFG to improve the detection
accuracy of neural networks. Assuming that the binary ﬁle
used for analysis has not been maliciously modiﬁed, that is,
the constructed CFG is trusted.

CFG is obtained by disassembling and statically analyzing
the executable binary. In [31]–[33], coarse-grained CFGs are
constructed by binary analysis. BinCFI [33] generates a CFG
by statically analyzing the EIF ﬁle. CCFIR [31] statically
analyzes the PE ﬁle to ﬁnd the target of all indirect branch
instructions. TypeArmor [32] uses use-def and liveness anal-
ysis to limit indirect call targets, and obtains indirect jump
targets by the binary analysis framework. The collected ofﬂine
CFG collected ofﬂine is coarse-grained. If the neural network
is trained based on the coarse-grained CFG, a high false
negative rate will be produced. Therefore, we use IPT to track
the execution process of the program and obtain the runtime
information by collecting the IPT package to improve the CFG
accuracy.

In the packets generated by IPT, we use TNT and TIP
to construct CFG. The TNT packet is used to record direct
conditional branch information. The direct branch includes
the general direct branch and the conditional direct branch.
The general direct branch has a ﬁxed target, and all such
branch information can be derived when the binary ﬁle is
analyzed statically. Therefore, only conditional direct branch
information needs to be obtained by TNT. The TIP packet is
used to record indirect branch information. Since the targets
of indirect branch can be changed arbitrarily, it is a challenge
to obtain all the information accurately. Therefore, the TIP
package is collected as much as possible to make the indirect
branch target accurate. Speciﬁcally, we need to generate a
variety of different input data. For each set of input data,
TNT and TIP packets are collected during the execution. The
collected address information is combined with the coarse-
grained CFG, and the legal control ﬂow transfer path is added
to the original CFG to construct a ﬁne-grained CFG. The
dataset is used to train the neural network model which can
automatically learn the features of the control ﬂow transfer. In
DeepCheck, the CFG is split into many gadget chains. Each
chain reﬂects a part of control ﬂow features.

C. CFG Splitting

CFG is split into chains and each chain is encoded into
data for training the neural network. If the CFG is directly
used for training instead of being split into gadget chains,
two issues will arise: 1) Difﬁcult data collection. This paper
builds CFGs from applications, and each application generates
a unique CFG. If the entire CFG is utilized as input data,
then massive CFGs are required to collect. However, a large
number of dataset can be constructed by splitting the CFG,
which solves the problem of difﬁcult collecting data. 2) Poor
detection effectiveness. If the model is trained with the entire

Fig. 3. An example of partial CFG

As shown in Fig. 3, CFG consists of nodes and edges. Each
node is a gadget. Two gadgets are connected by a directed
edge, representing the path of the control ﬂow transfer. After
splitting the CFG in a single edge, Gadget1, Gadget2 and the
directed edge (Edge1) form a gadget chain. Similarly, Gadget2,
Gadget3 and Edge 2 can form a gadget chain. If the splitting is
done in units of two edges, Gadget1, Gadget2, Gadget3, Edge
1 and Edge 2 form a gadget chain. During the execution of
the program, if the control ﬂow is transferred from Gadget4
to Gadget8 (see the dotted line), the detector considers that
an attack has occurred because there is no such edge in the
actual CFG.

Branch instructions are classed into direct branches and
indirect branches. For indirect branches, they may have many
targets and branch targets can be changed. Thus, a single edge
is used as the basic unit for splitting, which ensures that the
target of each indirect branch is in the benign training dataset.
For direct branches, as the targets cannot be modiﬁed, two
edges are used as the basic unit for splitting. The ratio of
direct branches in gadget chains can be reduced by two edges
(or more) to increase the proportion of indirect branches in
the data set and improve the accuracy of the neural network.
Malicious gadget chains are derived by connecting gadgets
that have no edge in the CFG. In this way, a large number of
negative samples can be generated without manually construct-
ing the payload. As shown in Fig. 3, Gadget3, Gadget4, and
Gadget8 can form a malicious gadget chain because Gadget4
and Gadget8 are randomly connected. When the control ﬂow is
transferred from Gadget3 to Gadget4, the detection mechanism
will judge that it is a normal execution ﬂow. However, when
the program performs control ﬂow transfer from Gadget4 to
Gadget8, it will be considered as an attack.

The proposed splitting algorithm is shown in Algorithm
1. The input data is CFG. The output is the gadget chains
obtained by splitting the CFG. At
the beginning of the
algorithm, two adjacent nodes in the CFG are taken. If the
ﬁrst node ends with an indirect branch, the two nodes are

DetectionTraining   ①③②void   a (int  x){ b(x); c(x);}void   b (int x){ e(x); f(x);}void   c (int x){ g(x);}a: …… call b: …… call c: ……b: …… call e: …… call f: ……c: …… call g: ……e: …… ……f: …… ……g: …… ……Binary FileGadgetsCoarse-Grained CFGConstructionDisassemblyTNT/TIP packageFine-Grained CFGIPTDNNTrainTNT/TIP packageDNNDecodeClassificationBenign GadgetIPTPayloadDetectionTraining DecodeGadget1Gadget2Gadget3Gadget4Gadget6Gadget5Gadget7Gadget10Gadget8Gadget9Gadget12Gadget11Edge 1Edge 2Edge 3Edge 4Edge 5Edge 6Edge 10Edge 7Edge 8Edge 9 push rbp;  0x55 mov rbp,rsi;  0x4889f5 ret  0xc3554889f55c39090554889f55c390..36753a31f64889e2e83..c0753a...8105ac3909090..362fc883f81753d9090..4151c3L = Maximum Gadget LengthGadget:DataSet:............InputLayer1000 neuronsReLUFollowed by 50% DropoutReLU,Followed by 50% DropoutReLU,Followed by 50% Dropout600 neurons200 neurons50 neuronsOutputLayerSoftmaxBinary FileGadgetsCoarse-Grained CFGConstructionDisassemblyTNT/TIP packageFine-Grained CFGIPTDNNTrainDNNDecodeClassificationBenign GadgetIPTPayloadDetectionTraining Decodepop rbxret...mov rax,rcxret...pop rdijmp rax...pop rcxretVirtual Memory...@gadget 1(data1)@gadget 2@gadget 3(data2)...StackFine-Grained CFGIPTCoarse-Grained CFGDecoderBinary FilePT PacketDecoderDisassembleData EncoderClassifierG2G3G4G5G1G6G7G8G9G10G11G12213574698101211G：Gadget...InputLayer1000 neuronsReLU,Followed by 50% DropoutReLU,Followed by 50% DropoutReLU,Followed by 50% Dropout600 neurons200 neurons50 neuronsOutputLayerSoftmaxEdge 11Edge 12Detection PhaseBinary FileCoarse-Grained CFGBranch InformationFine-Grained CFGBenign Gadget ChainsMalicious Gadget ChainsClassifierBranch InformationGadget ChainsClassifierNormal Control FlowCRATraining PhaseDisassembleIPTConstructSplitTrainIPTChainClassifylinked into a benign gadget chain. If the last instruction of
the node is a direct branch, the two nodes, together with the
adjacent third node, are linked into a gadget chain. Finally,
malicious gadget chains are generated by randomly connecting
three unconnected nodes.

Algorithm 1 Split Control Flow Graph
Input: Control Flow Graph G
Output: Benign Gadget Chains S1, Malicious Gadget Chains

S2

network. Each piece of training data can be composed of two
or three gadgets. And the training data will be ﬁlled to the
same length with nop.

if g1 is Direct branch then

1: for g1 ∈ G, g2 ∈ G do
2:
3:
4:

get g3 in G
S1 ← connect(g1, g2, g3)
else if g1 is Indirect branch then

S2 ← connect(g1, g2)

5:
6:
end if
7:
8: end for
9: for g1 ∈ G do
10:

11:
12:
end if
13:
14: end for
15: return S1, S2

g1, g2 ← random(G)
if g1, g2, g3 is not a chain in G then

S2 ← connect(g1, g2, g3)

Fig. 4. An example of data coding

D. Data Representation

The input data of neural network must be the numerical
data in a uniform format with a ﬁxed length. If the binary
instructions are used as training data directly, they need to be
ﬁlled to a ﬁxed length with 0 or 1. For example, if 0 is used to
ﬁll each instruction in the binary form of the gadget {push
rbp; mov rbp, rsi; push rbx; ret} to the same
length of 3 bytes, the whole data becomes a string consisting
of a large number of 0 and 1. It is difﬁcult for the neural
network to distinguish the difference among the data. If one-
hot encoding is adopted, all
instructions in a gadget are
required to be split into a single byte. For example, ﬁrstly,
the gadget {push rbp; mov rbp, rsi; push rbx;
ret} is split into [0x55, 0x48, 0x89, 0xf5, 0x53, 0xc3]. Then
each byte is represented by a 256-bit binary string. Take 0x55
as an example, 0x55 in decimal is 85, so the 85th bit of the
256-bit binary string is 1 and the other bits are all 0, which is
represented like [0, 0, 0, ..., 1, ..., 0]. However, when a lot of
instructions are used to constitute a gadget, each item of data
generated by the one-hot encoding method is extremely long,
which increases the complexity of the input data greatly.

To address the above issues, we propose to encode the
gadget using the method shown in Fig. 4. Each instruction
in the gadget is represented as a hexadecimal form and 4 bits
are used as the basic unit for splitting. Then all the values
are arranged in the order of the instructions. Since the length
of the gadget is different, the nop command (0x90) is used
to ﬁll all of them to the same length at the end of the data,
thus converting a gadget into data recognizable by the neural

E. Architecture of the Deep Neural Network

Deep Neural Network (DNN) is utilized to classify the data
sets we build. Compared with traditional machine learning
algorithms, DNN can get more high-dimensional features
to achieve higher accuracy. Speciﬁcally, a six-layer neural
network model including an input layer, an output layer, and
four hidden layers is designed. The number of nodes in the
four hidden layers is 1024, 512, 128, and 32, respectively. The
rectiﬁed linear unit (ReLU) is used as the activation function
in each hidden layer. At the same time, a 50% dropout is set to
speed up the training process and prevent overﬁtting. Softmax
is used to convert the result to a probability value between 0
and 1.

We assume that the label of the training data is correct.
Although some of the sub-chains in the execution ﬂow of
the payload are benign, it does not affect the neural network
to detect the attack. In training process, the learning rate of
model is set to 0.01, and the method of stochastic gradient
descent is used to optimize the model. During the training of
each batch, the DNN updates the weight of each layer in the
network through a back-propagation algorithm and repeats the
optimization process until the error converges.

IV. EVALUATION

This section will evaluate the effectiveness of DeepCheck

in detecting real-world CRAs from three aspects:

• Can DeepCheck achieve high detection accuracy, low
false positive rate, and low false negative rate when it
is used to detect real applications?

DetectionTraining   ①③②void   a (int  x){ b(x); c(x);}void   b (int x){ e(x); f(x);}void   c (int x){ g(x);}a: …… call b: …… call c: ……b: …… call e: …… call f: ……c: …… call g: ……e: …… ……f: …… ……g: …… ……Binary FileGadgetsCoarse-Grained CFGConstructionDisassemblyTNT/TIP packageFine-Grained CFGIPTDNNTrainTNT/TIP packageDNNDecodeClassificationBenign GadgetIPTPayloadDetectionTraining DecodeGadget1Gadget2Gadget3Gadget4Gadget6Gadget5Gadget7Gadget10Gadget8Gadget9Gadget12Gadget11Edge 1Edge 2Edge 3Edge 4Edge 5Edge 6Edge 10Edge 7Edge 8Edge 9 push rbp;  0x55 mov rbp,rsi;  0x4889f5 ret  0xc3554889f55c39090554889f55c390..36753a31f64889e2e83..c0753a...8105ac3909090..362fc883f81753d9090..4151c3L = Maximum Gadget LengthGadget:DataSet:............InputLayer1000 neuronsReLUFollowed by 50% DropoutReLU,Followed by 50% DropoutReLU,Followed by 50% Dropout600 neurons200 neurons50 neuronsOutputLayerSoftmaxBinary FileGadgetsCoarse-Grained CFGConstructionDisassemblyTNT/TIP packageFine-Grained CFGIPTDNNTrainDNNDecodeClassificationBenign GadgetIPTPayloadDetectionTraining Decodepop rbxret...mov rax,rcxret...pop rdijmp rax...pop rcxretVirtual Memory...@gadget 1(data1)@gadget 2@gadget 3(data2)...StackFine-Grained CFGIPTCoarse-Grained CFGDecoderBinary FilePT PacketDecoderDisassembleData EncoderClassifierG2G3G4G5G1G6G7G8G9G10G11G12213574698101211G：Gadget...InputLayer1000 neuronsReLU,Followed by 50% DropoutReLU,Followed by 50% DropoutReLU,Followed by 50% Dropout600 neurons200 neurons50 neuronsOutputLayerSoftmaxEdge 11Edge 12Detection PhaseBinary FileCoarse-Grained CFGBranch InformationFine-Grained CFGBenign Gadget ChainsMalicious Gadget ChainsClassifierBranch InformationGadget ChainsClassifierNormal Control FlowCRATraining PhaseDisassembleIPTConstructSplitTrainIPTChainClassifyTABLE II
THE NUMBER OF GADGET CHAINS.

Program
Adobe ﬂash 11.2.202.33
Nginx 1.4.0
Proftpd 1.3.0a
Firefox 3.5.10

Vulnerability
CVE-2014-0502
CVE-2013-2028
CVE-2006-6563
CVE-2010-1214

Benign Gadget Chains Malicious Gadget Chains

201097
124476
92042
211509

167580
103980
60035
195920

• Is DeepCheck superior to traditional machine learning
methods in terms of accuracy, false positive rate, and false
negative rate?

• Is the runtime overhead of DeepCheck better than tradi-

tional CFI methods?

The prototype of DeepCheck has been implemented on the
Ubuntu 16.04 with the kernel 4.3. Perf is used to conﬁgure
the IPT tracking application and parse TNT and TIP packets.
In addition, binary instructions are obtained by Capstone
[34] and the neural network model is built by the open source
framework TensorFlow. Commercial applications are used to
train the neural network model. Each application trains a
classiﬁer separately. We use real payloads to evaluate the
DeepCheck, and compare DeepCheck with traditional machine
learning methods including SVM and logistic regression (LR).
the
To further demonstrate the advantages of DeepCheck,
runtime overhead of DeepCheck is compared with traditional
CFI mechanisms.

A. Testing on Real-World Applications

In this section, DeepCheck is evaluated with four com-
mercial applications Adobe Flash Player, Nginx, Proftpd, and
Firefox. Firstly, data is collected from these programs. To
maximize the accuracy of CFG, perf is used to collect
IPT packets and implement a decoder to extract TNT and
TIP packets. Then, Algorithm1 is used to split CFG and
build gadget chains. Finally, these gadget chains are encoded
to generate a training data set and label both benign and
malicious samples. The number of gadget chains generated
by each application is shown in Table I.
First of all, Adobe Flash Player

is used to evaluate
DeepCheck. 80% of the data randomly selected from the data
set is used for training, 10% for validation, and 10% for test.
80% of the data set contains 80% of benign gadget chains
and 80% of malicious gadget chains in the entire data set.
Hence, the training set and validation set contain a total of
180,987 benign gadget chains and 150,822 malicious gadget
chains. The test set consists of 20,109 benign gadget chains
and 16,758 malicious gadget chains.

For Adobe Flash Player, the accuracy of DeepCheck is
99.2%, the false positive rate is 0.28%, and the false negative
rate is 0.47%. The effectiveness of DeepCheck is further
evaluated on the other three real applications. For each appli-
cation, 80% of the data set is chosen as training data, 10%
for validation, and 10% for testing. The evaluation results
are shown in Table III. The average accuracy, average false
positive rate and average false negative rate of DeepCheck
are respectively 98.9%, 0.15%, and 0.6%. The experimental

results show that the neural network trained by the real-world
application can get high detection rate,
low false positive
rate and false negative rate. Therefore, DeepCheck learns the
features of CRAs well. The reason is that DeepCheck is able
to ﬁnd complex features in large data sets, rather than over-
ﬁtting small training data and losing key features.

B. Testing on Real Payloads

In the above section, the data used to train DNN is generated
by splitting the CFG. However, these data are different from
real CRA exploits. In this section, we evaluate DeepCheck on
real payloads and prove that DeepCheck can detect real CRA
exploits.

There are 4 real exploit samples in the Exploit-DB [35]. To
further create test samples, ROPGadget [36] and Ropper
[37] are used to generate the exploits that execute mprotect
or execve. These exploits are manually modiﬁed, for exam-
ple, swapping the order of multiple gadgets without changing
the attack behavior, or replacing some gadgets with new
gadgets which have the same effect. Finally, we get 64
real payloads. The experimental result shows that the neural
network model classiﬁes them as real CRA payloads correctly,
which further proves the feasibility of our method based on
CFG splitting in detecting CRAs.

C. Comparison

Traditional machine learning methods have been proposed
for ROP detection. For example, David et al. [38] statically
detected ROP via SVM. [39] detects ROP using the hidden
Markov model. In this section, DeepCheck is compared with
SVM and LR. Speciﬁcally, SVM with radial basis function
(RBF) kernel and LR are used as classiﬁers, and 80% of
Firefox data set is used for training. The results are shown
in Table IV.

Experimental results indicate that DeepCheck is superior to
SVM and LR in terms of the accuracy, false positive rate and
false negative rate, which proves that neural networks have
signiﬁcant advantages in learning complex data sets and can
better capture these spatial features than SVM and LR.

In addition, DeepCheck is compared with several traditional
CFI methods. The evaluation metrics include security level,
ISA extensions, compiler modiﬁcation, runtime overhead, and
non-intrusive detection. The security level includes [40]:

Level I: Only defend against ROP;
Level II: Defend against ROP and JOP;
Level III: Defend against ROP, JOP and some advanced

CRAs;

Level IV: Defend against all potential CRAs.

TABLE III
RESULTS OF DIFFERENT APPLICATIONS.

Program
Adobe ﬂash
Nginx
Proftpd
Firefox
Average

Total Dataset
368677
228756
152077
407429
/

Training Data
294941
183004
121661
325943
/

False Positive
0.28%
0.04%
0.26%
0.02%
0.15%

False Negative
0.47%
0.63%
0.89%
0.41%
0.60%

Precision
99.2%
98.5%
98.8%
99.4%
98.9%

TABLE IV
COMPARISON OF DNN, SVM AND LR.

Method
LR
SVM
DNN

False Positive
7.1%
8.7%
0.15%

False Negative
21.3%
24.5%
0.60%

Precision
82.4%
74.6%
98.9%

As shown in Table V, DeepCheck does not modify the
compiler or extend ISAs, and incurs zero runtime overhead. It
is worth noting that DeepCheck can achieve level III security
because it can learn the features of the abnormal control
ﬂow to detect CRAs. In addition, traditional CFIs usually
require inserting checking instruction or implementing the
runtime detection module, which incurs additional overhead to
the original program. However, DeepCheck is a non-intrusive
defense mechanism that does not require instrumentation or
any other form of modiﬁcation. It uses the control ﬂow
information generated by IPT to implement CRAs detection.
Therefore, there is no additional runtime overhead. Although
DeepCheck has a certain delay in detecting CRAs, it has no
inﬂuence on identifying attack behavior before the attack is
completed. The reason is that the attack process of CRAs
is not completed in one step (multiple gadgets need to be
executed). As the control ﬂow information generated by IPT
in real time, the malicious behavior of changing control ﬂow
can be detected before completing the attack.

V. LIMITATIONS

This section will discuss the limitations of our proposed

DeepCheck.

1) The data that trains the neural network is derived from a
CFG generated by a speciﬁc application, which ensures
accurate detection results on the application. However,
we need to train a classiﬁer for each application sep-
arately, which is a common issue with existing CFI
mechanisms.

2) Since the neural network model is trained through a
speciﬁc application, the classiﬁer may need to be re-
trained when the application version is upgraded. If new
functions are added to the program, the corresponding
gadgets are required to be added to the data set. If some
instructions are deleted, their corresponding gadgets are
also removed from the data set, and the offset between
gadgets needs to be adjusted. For infrequent software
updates, the cost of retraining the neural network is
within an acceptable range.

3) DeepCheck has a certain false negative rate because
the gadget is deﬁned widely. The gadget consists of
all the instructions from the next adjacent instruction
of a branch instruction to the next branch instruction,
which makes the data set contain some data of non-real
gadgets. These data will enable the neural network to
learn the characteristics of some non-gadgets, thereby
increasing the false positive rate. However, this non-
heuristic way makes DeepCheck more difﬁcult for at-
tackers to bypass.

4) The accuracy of the CFG directly determines the quality
of the data set, which affects the detection of the neural
network model. However, generating the complete CFG
for a real-world software is still an open research prob-
lem [21], and there is no way to implement a completely
accurate CFG currently. In addition, the negative sam-
ples generated by arbitrarily connecting the CFG nodes
deviate from the actual payloads, which would affect the
neural network model to learn the features of CRAs. In
our future work, we will explore better data construction
methods to improve detection accuracy, and reduce the
false positive rate and the false negative rate.

VI. RELATED WORK AND MOTIVATION
In 2005, CFI is proposed to defend against CRAs [15]. This
technique inserts an ID and the ID checking code for each
indirect jump instruction to enforce the execution ﬂow of the
program to follow the original CFG. Abnormal control ﬂows
will be detected due to the mismatch of inserted IDs. In CFI,
the indirect branch and its target are divided into equivalence
classes. All targets of an indirect branch belong to the same
equivalence class, indicating that the indirect branch can target
any targets in the equivalence class [23]. For example, the
target of branch instruction A includes B, C, and D. Then
B, C, and D constitute the equivalence class of A. The imple-
mentation of CFI can be coarse-grained and ﬁne-grained. Fine-
grained CFI has a separate equivalence class for each branch
target. In contrast, coarse-grained CFI usually classiﬁes the
targets of all indirect branches into an equivalence class [23].
Therefore, ﬁne-grained CFI methods will incur high runtime
overhead while having higher security than coarse-grained
[13], [14], [43]. Security, runtime overhead and compatibility
are main metrics to evaluate CFI methods which can be classed
into hardware-assisted and software-based methods.

A. Hardware-Assisted CFI

In 2009, Francillon et al. [44] proposed a stack protection
mechanism that adds a microprocessor to the original com-

TABLE V
COMPARISON OF DIFFERENT CFI MECHANISMS.

Security
Level
I
I
III
I
II
III
III

ISA
Extensions
N
Y
N
Y
N
N
N

Compiler
modiﬁcation
N
Y
N
Y
N
N
N

Runtime
Overhead
21%
3%-8%
6.9%
2%
0.95%
10%
0%

Non-intrusive

N
N
N
N
N
N
Y

CFI [15]
CCFI [24]
CodeArmor [41]
HAFIX [20]
HCIC [40]
µCFI [42]
DeepCheck

puter architecture to protect the return address in the stack
from the modiﬁcation by attackers. It requires an additional
microprocessor and the modiﬁcation of ISAs, which increases
hardware overhead and reduces the compatibility. In [25],
[26], Last Branch Record (LBR) that records the most recent
control ﬂow transfers is utilized to implement CFI, which
brings lower runtime overhead than the software-based CFI.
However, due to the limitation of the stack capacity, LBR
can only detect a small number of executed branch targets.
To overcome this limitation, [45] proposed to implement a
runtime monitoring unit that only records indirect control ﬂow
transfers. Nonetheless, an interrupt is triggered whenever 16
control ﬂow transfer information is recorded, which has a great
impact on the performance. Subsequently, some hardware-
assisted CFI schemes that implement CFI by extending the
ISAs were proposed to check abnormal control ﬂows [20]–
[22], [27].

Hardware-assisted CFI ensures that indirect jumps must
follow the new CFI instructions by assigning different labels
to each jump instruction. In 2016, Qiu et al. [46] proposed to
protect the control ﬂow by using physical unclonable function
(PUF) responses to linearly encrypt the return address without
extending ISAs and modifying compiler. However, attackers
are easy to infer the encryption key by obtaining an encrypted
address. In order to address this issue, LEA-AES [22] uses the
AES integrated in Intel processor to encrypt the return address.
However, it still needs to extend the ISAs. Later on, HCIC [40]
uses the Hamming distance matching method to avoid the key
leakage without changing the compiler and ISA.

Recently, IPT is used to implement the ﬁne-grained CFI.
FlowGuard [47] utilizes the information of the original IPT
package and assigns different weights to control ﬂow edges
by continuous training to build a ﬁne-grained CFG. According
to the pre-generated credit-labeled, the control ﬂow edges
can be divided into the slow path and fast path to improve
the detection speed. PT-CFI [48] exploits IPT to implement
shadow stack protection for backward CFI. GRIFFIN [49]
implements the ﬁne-grained CFI at the basic block level by
means of a bitwise matrix. Compared with FlowGuard and
PT-CFI, GRIFFIN has a ﬁner CFI granularity.

B. Software-based CFI

In 2010, Li et al. [50] proposed a compiler-based ROP
defense, which eliminates the gadgets that can be exploited
by attackers via rewriting the source code in the operating

system kernel. In 2013, Zhang et al. [31] proposed a compact
CFI and randomization protection method, named CCFIR,
which creates a space in memory where an entry is created
for the target address of each branch instruction. Branch
instruction of the program can only jump to the entry in
this space. However, in CCFIR, the ret instruction can be
returned to any call instruction that can point to the entry
of any other functions, which allows attackers to construct the
malicious code fragment to hijack the control ﬂow of program.
BinCFI [51] proposed to extract the CFG from binary ﬁles by
combining the linear and recursive disassembly. TypeArmor
[32] utilizes the binary analysis and considers the high-level
program semantics such as the number of function parameters
and the return value to improve the precision of the CFG.
However, all mechanisms based on static binary rewriting
can only obtain coarse-grained CFGs. ROPdefender [25] uses
dynamic binary instrumentation to implement the ﬁne-grained
CFG which incurs high runtime overhead.

In recent years, machine learning (ML), especially deep
learning, has shown great advantages in autonomous vehi-
cles, image/speech recognition, robotics, network security, and
natural language processing (NLP). ML has been used to
learn the feature of CRAs for CRA detection [38], [52].
However, they are different from our work. In [38], Pfaff
et al. collect micro-architectural events generated by the
hardware performance counter and use traditional Support
Vector Machine (SVM) algorithm to train a classiﬁer to realize
ROP detection, which interrupts the application irregularly
and incurs 5% performance overhead. In [52], Elsabagh et al.
collect microarchitecture-independent program characteristics
and use unsupervised statistical model as a classiﬁer to detect
ROP. However, the detection accuracy is only 81%. In 2018,
deep learning was proposed for CRA detection [53]. It detects
the ROP using the address space layout guided disassembly,
which treats the input data as a memory address and randomly
searches for the instruction sequences that may be gadgets
to generate a large number of benign data sets. Then ROP
detection is completed by training a convolutional neural
network without a pooling layer. However, such scheme can
only detect ROP attacks generated by HTTP requests, PDF
ﬁles, and images, and the detection range is limited. This
paper proposes the ﬁrst CFI checking technique based on deep
learning to prevent against all CRAs and implements such CFI
strategy by splitting the ﬁne-grained CFG. DeepCheck does

not modify ISAs, compilers and binary ﬁles. Furthermore,
it has high detection accuracy, good compatibility and zero
runtime overhead.

[13] K. Z. Snow, F. Monrose, L. Davi, A. Dmitrienko, C. Liebchen, and
A. Sadeghi, “Just-in-time code reuse: On the effectiveness of ﬁne-
grained address space layout randomization,” in Proceedings of the IEEE
Symposium on Security and Privacy, May 2013, pp. 574–588.

VII. CONCLUSION

This paper proposes the ﬁrst deep learning-based CFI check-
ing method, called DeepCheck, to resist CARs. DeepCheck
has addressed several issues that traditional CFI suffered, such
as extending ISAs, modifying compilers and incurring high
runtime overhead. DeepCheck uses IPT to build ﬁne-grained
CFGs and then splits CFGs for neural network training. When
the program is running, the branch information is acquired
in real time and detected by the neural network model. The
experimental results show that DeepCheck has high accuracy
(98.9%), low false positive rate (0.15%) and false negative
rate (0.60%). In addition, it can successfully detect the real
ROP exploits with a detection success rate of up to 100%.
DeepCheck is a practical non-intrusive CRA detection method
with zero-runtime overhead. Therefore, compared with previ-
ous CFI methods, DeepCheck has huge advantages. In future
work, we will train a more efﬁcient neural network model to
further improve its detection performance.

REFERENCES

[1] S. Corporation, “Internet Security Threat Report

(ISTR) 2018,”

https://www.symantec.com.

[2] R. Roemer, E. Buchanan, H. Shacham, and S. Savage, “Return-Oriented
Programming: Systems, Languages, and Applications,” ACM Transac-
tions on Information and System Security, vol. 15, no. 1, pp. 1–34, Mar
2012.

[3] T. Bletsch, X. Jiang, V. W. Freeh, and Z. Liang, “Jump-oriented pro-
gramming: A New Class of Code-Reuse Attack,” in Proceedings of the
6th ACM Symposium on Information, Computer and Communications
Security, 2011, pp. 30–40.

[4] S. Checkoway, L. Davi, A. Dmitrienko, A.-r. Sadeghi, H. Shacham, and
M. Winandy, “Return-oriented programming without returns,” in Pro-
ceedings of the 17th ACM conference on Computer and communications
security, 2010, pp. 559–572.

[5] Rapid7,

“The Latest Adobe Exploit

and Session Upgrading,”

https://community.rapid7.com/community/metasploit/blog/2010/03/18/the-
latest-adobe-exploit-and-session-upgrading.

[6] “Security Advisory for Flash Player, Adobe Reader and Acrobat,”
http://www.adobe.com/support/security/advisories/apsa10-01.html.

backdoor

[7] “Apple QuickTime

peril,”
https://www.theregister.co.uk/2010/08/30/apple quicktime critical vuln/.
[8] J. Hiser, A. Nguyen-Tuong, M. Co, M. Hall, and J. W. Davidson, “ILR:
Where’d My Gadgets Go?” in Proceedings of the IEEE Symposium on
Security and Privacy, May 2012, pp. 571–585.

code-execution

creates

[9] K. Lu, C. Song, B. Lee, S. P. Chung, T. Kim, and W. Lee, “ASLR-
Guard: Stopping Address Space Leakage for Code Reuse Attacks,” in
Proceedings of the 22nd ACM SIGSAC Conference on Computer and
Communications Security, 2015, pp. 280–291.

[10] J. Fu, Y. Lin, and X. Zhang, “Code Reuse Attack Mitigation Based on
Function Randomization without Symbol Table,” in Proceedings of the
IEEE Trustcom/BigDataSE/ISPA, Aug 2016, pp. 394–401.

[11] P. Chen, J. Xu, Z. Hu, X. Xing, M. Zhu, B. Mao, and P. Liu, “What you
see is not what you get! thwarting just-in-time rop with chameleon,” in
Proceedings of the 47th Annual IEEE/IFIP International Conference on
Dependable Systems and Networks, June 2017, pp. 451–462.

[12] L. Liu, J. Han, D. Gao, J. Jing, and D. Zha, “Launching Return-Oriented
Programming Attacks against Randomized Relocatable Executables,”
in Proceedings of the IEEE 10th International Conference on Trust,
Security and Privacy in Computing and Communications, Nov 2011,
pp. 37–44.

[14] F. Schuster, T. Tendyck, C. Liebchen, L. Davi, A.-r. Sadeghi, and
T. Holz, “Counterfeit Object-oriented Programming: On the Difﬁculty of
Preventing Code Reuse Attacks in C++ Applications,” in Proceedings of
the IEEE Symposium on Security and Privacy, May 2015, pp. 745–762.
´U. Erlingsson, and J. Ligatti, “Control-ﬂow
integrity,” in Proceedings of the 12th ACM conference on Computer
and communications security, pp. 340–353.

[15] M. Abadi, M. Budiu,

[16] M. Budiu,

´U. Erlingsson, and M. Abadi, “Architectural support for
software-based protection,” in Proceedings of
the 1st workshop on
Architectural and system support for improving software dependability,
2006, pp. 42–51.

[17] B. Niu and G. Tan, “Modular control-ﬂow integrity,” ACM SIGPLAN

Notices, vol. 49, no. 6, pp. 577–587, Jun 2014.

[18] V. Kuznetzov, L. Szekeres, M. Payer, G. Candea, R. Sekar, and D. Song,
“The continuing arms race,” 2018, ch. Code-pointer Integrity, pp. 81–
116.

[19] N. Christoulakis, G. Christou, E. Athanasopoulos, and S. Ioannidis,
“HCFI: Hardware-enforced Control-Flow Integrity,” in Proceedings of
the Sixth ACM on Conference on Data and Application Security and
Privacy, 2016, pp. 38–49.

[20] L. Davi, M. Hanreich, D. Paul, A.-r. Sadeghi, P. Koeberl, D. Sullivan,
O. Arias, and Y. Jin, “HAFIX: Hardware-Assisted Flow Integrity eXten-
sion,” in Proceedings of the 52nd ACM/EDAC/IEEE Design Automation
Conference, 2015, pp. 1–6.

[21] D. Sullivan, O. Arias, L. Davi, P. Larsen, A.-r. Sadeghi, and Y. Jin,
“Strategy without tactics: policy-agnostic hardware-enhanced control-
ﬂow integrity,” in Proceedings of the 53rd Annual Design Automation
Conference, 2016, pp. 1–6.

[22] P. Qiu, Y. Lyu, J. Zhang, D. Wang, and G. Qu, “Control Flow Integrity
Based on Lightweight Encryption Architecture,” IEEE Transactions on
Computer-Aided Design of Integrated Circuits and Systems, vol. 37,
no. 7, pp. 1358–1369, 2018.

[23] B. Niu, “Per-Input Control-Flow Integrity,” in Proceedings of the 22nd
ACM SIGSAC Conference on Computer and Communications Security,
2015, pp. 914–926.

[24] A. J. Mashtizadeh, A. Bittau, D. Boneh, and D. Mazi`eres, “Ccﬁ:
Cryptographically enforced control ﬂow integrity,” in Proceedings of
the 22Nd ACM SIGSAC Conference on Computer and Communications
Security, 2015, pp. 941–951.

[25] Y. Cheng, Z. Zhou, M. Yu, X. Ding, and R. H. Deng, “ROPecker: A
Generic and Practical Approach For Defending Against ROP Attacks,”
in Proceedings of the 21st Network and Distributed System Security
Symposium, no. February, 2014, pp. 23–26.

[26] V. van der Veen, D. Andriesse, E. G¨okta, B. Gras, L. Sambuc, A. Slowin-
ska, H. Bos, and C. Giuffrida, “Practical Context-Sensitive CFI,” in
Proceedings of the 22nd ACM SIGSAC Conference on Computer and
Communications Security, 2015, pp. 927–940.

[27] L. Davi, P. Koeberl, and A.-r. Sadeghi, “Hardware-Assisted Fine-Grained
Control-Flow Integrity : Towards Efﬁcient Protection of Embedded
Systems Against Software Exploitation,” in Proceedings of the 51st
ACM/EDAC/IEEE Design Automation Conference, 2014, pp. 1–6.
[28] S. Das, W. Zhang, and Y. Liu, “A Fine-Grained Control Flow Integrity
Approach Against Runtime Memory Attacks for Embedded Systems,”
IEEE Transactions on Very Large Scale Integration (VLSI) Systems,
vol. 24, no. 11, pp. 3193–3207, Nov 2016.

[29] N. Burow, S. A. Carr, J. Nash, P. Larsen, M. Franz, S. Brunthaler,
and M. Payer, “Control-Flow Integrity: Precision, Security, and Perfor-
mance,” ACM Computing Surveys, vol. 50, no. 1, pp. 1–33, Apr 2017.
[30] I. Corporation, “Part 3 3C Intel R(cid:13) 64 and IA-32 Architectures Software

Developer’s Manual, Volume 3C System Programming Guide,” 2016.

[31] C. Zhang, T. Wei, Z. Chen, L. Duan, L. Szekeres, S. McCamant,
D. Song, and W. Zou, “Practical control ﬂow integrity and randomization
for binary executables,” in Proceedings of the IEEE Symposium on
Security and Privacy, 2013, pp. 559–573.

[32] V. v. d. Veen, E. Gktas, M. Contag, A. Pawoloski, X. Chen, S. Rawat,
H. Bos, T. Holz, E. Athanasopoulos, and C. Giuffrida, “A tough
call: Mitigating advanced code-reuse attacks at the binary level,” in
Proceedings of the IEEE Symposium on Security and Privacy, May 2016,
pp. 934–953.

[33] M. Zhang and R. Sekar, “Control Flow and Code Integrity for COTS
binaries,” in Proceedings of the 31st Annual Computer Security Appli-
cations Conference, 2015, pp. 91–100.

[34] N. A. Quynh, “Capstone,” https://github.com/aquynh/capstone.
[35] O. Security, “Exploit Database,” https://www.exploit-db.com/, 2018.
[36] J. Salwan, “ROPgadget,” https://github.com/JonathanSalwan/.
[37] S. Schirra, “Ropper,” https://github.com/sashs/Ropper.
[38] D. Pfaff, S. Hack, and C. Hammer, “Learning How to Prevent Return-

Oriented Programming Efﬁciently,” 2015, pp. 68–85.

[39] T. Usui, T. Ikuse, M. Iwamura, and T. Yada, “POSTER: Static ROP
Chain Detection Based on Hidden Markov Model Considering ROP
Chain Integrity,” in Proceedings of the ACM SIGSAC Conference on
Computer and Communications Security, 2016, pp. 1808–1810.
[40] J. Zhang, B. Qi, Z. Qin, and G. Qu, “HCIC: Hardware-assisted control-
ﬂow integrity checking,” IEEE Internet of Things Journal, vol. 6, no. 1,
pp. 458–471, Feb 2019.

[41] X. Chen, H. Bos, and C. Giuffrida, “CodeArmor: Virtualizing the Code
Space to Counter Disclosure Attacks,” in Proceedings of the IEEE
European Symposium on Security and Privacy, Apr 2017, pp. 514–529.
[42] H. Hu, C. Qian, C. Yagemann, S. P. H. Chung, W. R. Harris, T. Kim,
and W. Lee, “Enforcing Unique Code Target Property for Control-Flow
Integrity,” in Proceedings of the ACM SIGSAC Conference on Computer
and Communications Security, 2018, pp. 1470–1486.

[43] N. Carlini, A. Barresi, M. Payer, D. Wagner, and T. R. Gross, “Control-
ﬂow bending: On the effectiveness of control-ﬂow integrity,” in Proceed-
ings of the 24th USENIX Conference on Security Symposium, 2015, pp.
161–176.

[44] A. Francillon, D. Perito, and C. Castelluccia, “Defending embedded
systems against control ﬂow attacks,” in Proceedings of the ﬁrst ACM
workshop on Secure execution of untrusted code, 2009, pp. 19–26.
[45] P. Yuan, Q. Zeng, and X. Ding, “Hardware-assisted ﬁne-grained code-
reuse attack detection,” Lecture Notes in Computer Science (including
subseries Lecture Notes in Artiﬁcial Intelligence and Lecture Notes in
Bioinformatics), vol. 9404, pp. 66–85, 2015.

[46] P. Qiu, Y. Lyu, J. Zhang, X. Wang, D. Zhai, D. Wang, and G. Qu,
“Physical unclonable functions-based linear encryption against code
reuse attacks,” in Proceedings of the 53rd Annual Design Automation
Conference, 2016, pp. 1–6.

[47] Y. Liu, P. Shi, X. Wang, H. Chen, B. Zang, and H. Guan, “Trans-
parent and Efﬁcient CFI Enforcement with Intel Processor Trace,” in
Proceedings of the IEEE International Symposium on High Performance
Computer Architecture, Feb 2017, pp. 529–540.

[48] Y. Gu, Q. Zhao, Y. Zhang, and Z. Lin, “PT-CFI: Transparent Backward-
Edge Control Flow Violation Detection Using Intel Processor Trace,” in
Proceedings of the Seventh ACM on Conference on Data and Application
Security and Privacy, 2017, pp. 173–184.

[49] X. Ge, W. Cui, and T. Jaeger, “GRIFFIN: Guarding Control Flows Using
Intel Processor Trace,” ACM SIGARCH Computer Architecture News,
vol. 45, no. 1, pp. 585–598, Apr 2017.

[50] J. Li, Z. Wang, X. Jiang, M. Grace, and S. Bahram, “Defeating return-
oriented rootkits with ” Return-Less ” kernels,” in Proceedings of the
5th European conference on Computer systems, 2010, pp. 195–208.
[51] K. Anand, A. Kotha, M. Smithson, R. Barua, and D. Angelos,
“Retroﬁtting Security in COTS Software with Binary Rewriting,” In-
ternational Federation for Information Processing, pp. 154–172, 2011.
[52] M. Elsabagh, D. Barbara, D. Fleck, and A. Stavrou, “Detecting ROP
with Statistical Learning of Program Characteristics,” in Proceedings of
the Seventh ACM on Conference on Data and Application Security and
Privacy, 2017, pp. 219–226.

[53] X. Li, Z. Hu, Y. Fu, P. Chen, M. Zhu, and P. Liu, “ROPNN: Detection

of ROP Payloads Using Deep Neural Networks,” arXiv.org, Jul 2018.

