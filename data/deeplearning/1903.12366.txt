USING STRUCTURED INPUT AND MODULARITY FOR IMPROVED LEARNING

9
1
0
2

r
a

M
9
2

]
E
N
.
s
c
[

1
v
6
6
3
2
1
.
3
0
9
1
:
v
i
X
r
a

Zehra Sura 1 Tong Chen 1 Hyojin Sung 1

ABSTRACT
We describe a method for utilizing the known structure of input data to make learning more efﬁcient. Our work is
in the domain of programming languages, and we use deep neural networks to do program analysis. Computer
programs include a lot of structural information (such as loop nests, conditional blocks, and data scopes), which is
pertinent to program analysis. In this case, the neural network has to learn to recognize the structure, and also
learn the target function for the problem. However, the structural information in this domain is readily accessible
to software with the availability of compiler tools and parsers for well-deﬁned programming languages.

Our method for utilizing the known structure of input data includes: (1) pre-processing the input data to expose
relevant structures, and (2) constructing neural networks by incorporating the structure of the input data as an
integral part of the network design. The method has the effect of modularizing the neural network which helps
break down complexity, and results in more efﬁcient training of the overall network. We apply this method to
an example code analysis problem, and show that it can achieve higher accuracy with a smaller network size
and fewer training examples. Further, the method is robust, performing equally well on input data with different
distributions.

1

INTRODUCTION

In this paper we focus on using deep learning for computer
program analysis and code optimization, which are essential
for performance. Achieving high performance for an
application program can be complex, made even more
complex by the intricate and heterogeneous hardware
platforms in use today. Software tools such as compilers and
runtime systems are used to manage this complexity. These
tools rely on accurate program analysis to optimize code and
deliver performance. Some of the analysis and optimization
decisions are simple with clear and efﬁcient computation
models, but some are complex or even unknown (i.e. no
analytical model exists). In the latter case, heuristic models
are used in the computation. The heuristics need to be
developed using knowledge of the hardware characteristics
and application domain, but it can be costly or infeasible to
develop good heuristics because of the intractable number
of factors and system interactions that impact performance.
Instead, the heuristics can be substituted with machine
learning techniques to incorporate a data-driven approach
in program analysis and optimization.

There is a lot of prior work on applying machine learning
techniques in program analysis and optimization, for
example choosing the strategies and parameters for loop

1IBM Research, Yorktown Heights, NY, USA. Correspondence

to: Zehra Sura <zehrasura@gmail.com>.

optimization and parallelization, making decisions on code
placement and scheduling, and choosing the set and ordering
of transformations to apply.
In (Memeti et al., 2018),
the authors present a comprehensive literature review on
using machine learning for parallel systems covering work
published since the year 2000. For each relevant paper in
their survey, the authors identify the objective, machine
learning techniques applied, and code/system features
considered as part of the training input data. In (Ashouri
et al., 2018), the authors survey the progress of work in the
area of machine learning for compiler optimization.

Much of this prior work uses a list of static and/or dynamic
program features as input data, or else it directly uses the
program code in the format of a string of characters. We take
a different approach where we apply domain knowledge to
determine structures in the code (e.g. loops, basic blocks,
statements, etc.) that are relevant for the machine learning.
We encode this structural information in the input data,
which is now represented as a sequence of tokens. For
this purpose, we re-use functionality in compiler toolchains
that are already good at extracting structural information in
programs. By encoding the structural information explicitly
in the training data, we make it easier for the automated
machine learning to learn from it. The data representation
we use has several advantages:

1. It provides input data directly in the form of code
structures rather than lists of secondary features derived
from the code.

 
 
 
 
 
 
Using Structured Input and Modularity for Improved Learning

2. It provides expressibility: the encoded structures can
be derived from the program code using custom rules
and they need not directly match existing structures
deﬁned in the programming language.

3. It allows ﬂexibility in encoding additional property
information associated with different structures, and
allows embedding them at appropriate contextual
points in the input data sequence.

We are interested in exploring whether machine learning can
be applied to more complex problems in program analysis
and code optimization, therefore we base our work on using
deep learning which is known to be a powerful machine
learning technique. For example, consider the problem
of estimating the dynamic instruction count of a program.
The instruction count may be different for different types
of operations, e.g. a scalar arithmetic operation, a vector
computation, or a memory access operation. When the
input program is a sequence of operations with no control
ﬂow, only localized properties in the sequence (i.e. type of
operation) contribute to the instruction count, and current
deep learning methods perform very well in this case.
Likewise, when the input program has very simple control
ﬂow with only one level nested loops, the information
needed to compute the instruction count is relatively easy to
ﬁnd in the program sequence (iteration count of the closest
enclosing loop), and current LSTM-based deep learning
again performs very well. However, when the structure
of the program gets more complicated, it becomes more
difﬁcult to extract the relevant contextual information from
a program sequence (e.g. iteration counts of all enclosing
loops). In this case, the performance of current LSTM-based
techniques falls short and accuracy of the learned models
is low. To address this, we designed a new technique for
using deep neural networks (which we call modular neural
networks) that is capable of processing complicated program
structures encoded in the input data to train more efﬁciently.

The main idea in our work is to translate high-level
knowledge of the problem domain into the design (layout)
of the deep neural network. Each input item in the training
data is a sequence of tokens, where a token corresponds
to a high-level semantic structure in the problem domain.
For example, in the case of program analysis, the types
of structures may include a type corresponding to the
beginning of a loop, the end of a loop, or a basic block
of statements. A modular neural network is composed of
multiple component neural networks, with one component
neural network corresponding to each structure type. The
modular neural network includes control logic to route
individual tokens within an input sequence to the appropriate
component neural network.
logic also
incorporates interconnection rules for updating the neural
network state before/after the processing of each individual

The control

token. These interconnection rules are determined using
knowledge of the problem domain.

Our modular design approach addresses several of the
challenges highlighted by the study in (Memeti et al., 2018),
including the following challenges that are important for
program analysis:

1. Complexity: Prior work has successfully demonstrated
the use of machine learning for some individual
problems in isolation (e.g. deciding whether to use
a GPU or not for select kernels(Fonseca & Cabral,
2013)). However, the ability to solve more complicated
problems using machine learning is still unknown
(e.g. using a sequence of transformations for overall
optimization of loop nests). In general, more powerful
learning is needed to tackle complex problems. Our
modular approach is speciﬁcally designed to break
down the complexity of learning.

2. Training data: Often the size of training data is limited
for problems in the area of program analysis. There
have been some efforts to collect and maintain common
data repositories for this purpose(Fursin et al., 2018;
Chen et al., 2017), but these efforts are still nascent.
Our modular approach is able to learn faster, achieving
high accuracy using less training data.

3. Generality: Many prior works can only handle speciﬁc
programming models or domain speciﬁc languages,
or else they rely on programmer annotations. Our
technique is applicable to all programming languages.

In this paper we make the following contributions:

• We introduce a novel method of designing neural
networks using modular neural networks. Our method
naturally incorporates the relevant information about
program structures (domain knowledge) into the design
of the neural network itself. The resulting network is
capable of handling more complex learning problems
than a monolithic network of equivalent (or larger) size
designed using current techniques.

• We compare an LSTM-based deep neural network
designed using current state-of-the-art techniques with
a modular neural network designed using our method
to solve an example code analysis problem. We show
that our method is able to achieve higher accuracy with
less training data and with an overall smaller size of
network.

In Section 2, we describe how relevant structural
information in computer programs can be made explicit
in the input data representation and can be used to design a

Using Structured Input and Modularity for Improved Learning

modular neural network. In Section 3, we give a systematic
method for our technique to use structural information and
modularity in deep learning. In Section 4 we detail our
experiments and results. In Section 5 we describe related
work and in Section 6 we present our conclusions.

2 OUR APPROACH

There are two main considerations when designing a
(1) the input data and
neural network based solution:
the representation used for it, and (2) the design of the
neural network structure. In current practice, the choices
made in the design of these two aspects are relatively
independent. However, in our design approach the two are
tightly integrated and the choice of input data representation
directly impacts the design of the neural network as well. In
this section, we highlight the advantages of our method for
both of these aspects of the design.

2.1

Input Data Representation

When applying machine learning for program analysis and
optimization, typically the input data is a set of features that
capture the characteristics of the program code and/or the
system platform being used for program execution. The
features can be either static or dynamic. For example,
static code features may include number of basic blocks,
number of branch instructions, etc.; dynamic code features
may include loop iteration counts, branch frequencies,
total number of instructions, etc.; static system features
may include cache sizes, peak ﬂops, etc.; dynamic system
features may include values of various hardware counters.
The literature review in (Memeti et al., 2018) gives a
detailed listing of the various input features used in prior
work. The problem with using a set of such features
as input is that these features are secondary properties
inferred by the designer to be relevant to the objective of
the machine learning.However, they may not completely
capture the ﬁrst-order properties that directly impact the
learning objective.For example, if the objective is to predict
the execution time of programs on a hardware device, the
ﬁrst-order input property is the program code, and the
ﬁrst-order output property is the observed runtime for that
code when executed on the hardware device.

More recent work such as (Cummins et al., 2017) directly
uses the program code as training input by encoding the
program as a sequence of tokens embedded in vector space.
The problem with this approach is that the training input is
represented at a very high-level, and it requires powerful
machine learning capability to redact the parts of the input
that are not relevant to the learning objective. Prior work
has applied this technique to relatively simple problems and
it is unclear if it will work for complex problems.

the input

In our approach,
is a sequence of tokens
representing program structures. The speciﬁc program
structures to be encoded into the sequence are selectively
chosen depending on the objective function to be learned.
Figure 1 shows an example to illustrate how input data
can be represented. The tokens for the identiﬁed program
structures are encoded using integers, and they can
optionally be followed by another value. In the example,
the BB and LB tokens take integer values and the IF token
takes a ﬂoat value, as illustrated in the ﬁgure. These optional
values make it easy to embed speciﬁc contextual information
at appropriate points in the program sequence. Also, even
though the example uses structures that map directly to
traditional programming language constructs, in general the
identiﬁed structures can be more complex (e.g. instances of
a LoopBegin immediately followed by an If statement can
be treated as a single structure), and can be tailored to be
speciﬁc to the function being learned. Existing functionality
in compiler toolchains, speciﬁcally lexers and parsers that
translate programs to an intermediate format, can be used
to process program code and generate the required input
sequences.

Objective Function:

Estimate dynamic instruction count

Structures in Input:

BasicBlock(BB), LoopBegin(LB), LoopEnd(LE),
If(IF), ElseIf(ELS), EndIf(EIF)

Example Code:

1
2
3
4
5
6
7

int x = rand();
for (int i=0; i< N; i++) {

int y = rand();
if (y > M)

x += y;

}
return x;

Input Data Sequence:

BB n1 LB n2 BB n3 IF n4 BB n5 EIF LE BB n6
n1,n5,n6 = instructions to compute line 1,5,7
n2 = iteration count for loop on line 2
n3 = instructions to compute lines 3 and 4
n4 = branch taken probability for line 4

Expected Output:

n1 + n2 ∗ (n3 + n4 ∗ n5) + n6

Figure 1. Example to illustrate our input data representation

For the simple example in Figure 1, the objective function is
known and can be easily computed as part of an analytical
program analysis model. However, the machine learning
approach uses data to learn this function, and the same
method can be applied to cases where the objective function
is unknown, for example when the objective is to estimate
the number of execution cycles for a processor supporting
speculative out-of-order execution instead of the dynamic
instruction count.

Using Structured Input and Modularity for Improved Learning

For our
input data representation, a designer with
domain-speciﬁc knowledge can think in terms of the
ﬁrst-order properties of the problem being addressed, and
need not derive a list of secondary properties. At the same
time, the input data is now curated to be more speciﬁc
to the objective being learned, and this helps reduce the
complexity of the problem for automated learning. Thus, our
approach balances the requirements on input data collection
and processing with the limitations of automated machine
learning. In general, choosing the correct level of balance
between data processing and automated learning is an active
research topic.

2.2 Neural Network Design

Network design determines the structure of the neural
network which includes the type, number, and organization
of the network layers and the interconnections between
layers. There has been much progress in training and
using sophisticated deep learning networks,
including
the use of recursive neural networks or RNNs(Jain &
Medsker, 1999) and Long Short Term Memory networks or
LSTMs(Hochreiter & Schmidhuber, 1997). These networks
have been successfully applied to problem domains such as
natural language processing(NLP) and machine translation
(Soutner & Müller, 2013; Wu et al., 2016). However, it is
still unknown how well they apply to complex problems in
the domain of program analysis.

In our work, we ﬁrst did some experiments to assess the
capability of current well known deep learning techniques
in our application context. For our experiments, we chose
the objective function to be a weighted count of instructions
in the program, where the weights are determined based on
some programming constructs. We used different criteria
for assigning weights, including the following:

1. INST-TYPE: weight is based on the type of instruction

(e.g. arithmetic or memory operation).

2. LL-1: weight is based on the iteration counts of
enclosing loops, with only 1-level loop nests in the
dataset.

3. LL-2: weight is based on the iteration counts of
enclosing loops, with upto 2-level loop nests in the
dataset.

4. LL-3: weight is based on the iteration counts of
enclosing loops, with upto 3-level loop nests in the
dataset.

5. LL-5-simple: weight is based on the nesting depth of
the closest enclosing loop, with upto 5-level loop nests
in the dataset.

Figure 2. Effect of objective function complexity on accuracy

We trained neural network models, one for each of these
criteria. We carefully tuned the network designs to
achieve maximum accuracy, using current state-of-the-art
techniques and a network structure composed of a number of
LSTM layers followed by a number of fully connected (FC)
layers. Our data generation and experimental methodologies
are described in Sections 4.1.1 and 4.2. Figure 2 shows the
accuracies obtained for the different trained models.

We observe that for the simpler criteria, the accuracy
achieved is close to 100%, but as the complexity of the
problem increases, the accuracy drops off. This motivated
us to develop new methods to help improve the automated
learning process. We aim for a neural network design that
has better learning capability (higher accuracy with less
training data), so that we can apply it to progressively more
complex problems.

Our approach is to use modularity to break down the
complexity of learning. As an example, consider the
problem of computing weighted instruction counts using
the nesting depth of the enclosing loop as the weight.
We can modularize the problem by breaking it down
into three sub-computation parts:
(1) computation on
increment a current nesting depth
a LoopBegin (i.e.
counter), (2) computation on a LoopEnd (i.e. decrement
the current nesting depth counter), and (3) computation on
an instruction or BasicBlock of instructions (i.e. accumulate
a running count of instructions appropriately). For an
automated machine learning process, the computations for
all three parts are unknown and have to be learned, but
the three parts are distinct and it is easier to learn them
individually than altogether as a monolithic function. Note
that the three parts correspond to distinct structures in the
program, and our input data representation is tailored to
explicitly identify these structures. Our neural network
design has some layers dedicated to learning each of the
sub-computations separately. The next section describes
in detail a general method for constructing such modular
networks.

99.9	99.6	86.8	85.4	65.6	50	55	60	65	70	75	80	85	90	95	100	INST-TYPE	LL-1	LL-2	LL-5-simple	LL-3	Percentage	accuracy	Weight	assignment	criteria	Using Structured Input and Modularity for Improved Learning

3 MODULAR DESIGN METHOD

3.2 Step 2: Pre-process Data

The black-box approach to neural network learning is
to train the neural network with a sufﬁcient amount of
data, and let the network automatically ﬁgure out what the
relevant information in the data is. Instead of using a purely
data-driven black-box approach, a second more discriminate
approach is to pre-process the input data according to
the learning objective (e.g. by annotating the data, or by
using different encoding techniques), in order to make the
relevant information explicit in the input data (Kolokolov
(2002); Berdibaeva et al. (2017)). We introduce a third
approach where some relevant information is encoded in the
neural network itself. In our approach, the neural network
is designed such that the network structure reﬂects the
structural information in the input data domain.

Our approach is applicable when:

1. the input data domain is structured, i.e. there exists
a known formal grammar that deﬁnes all possible
instances of the input data, and

2. the learning objective has a dependency on the structure

of the input data.

Both of these conditions are satisﬁed for many problems
in the area of program analysis, with the programming
language grammar readily available.

We deﬁne a method for constructing modular neural
networks, which are networks designed to utilize the
structure within individual instances of input data. The
objective is to learn a function F (x), where x is a member of
grammar G. Our method for constructing a modular neural
network is a four-step process described in the following
subsections.

3.1 Step 1: Identify Relevant Structures

Since the learning objective depends on structure within
individual instances of the input data, we ﬁrst apply domain
knowledge to identify a set of relevant structures (S).
These structures will be explicitly marked in the input
data. Knowing the problem domain and the application
requirements allows choosing structures at the right level of
abstraction. The structures are expressed using tokens and
expressions from the known grammar G. A grammar G(cid:48)
can be derived based on G such that there is one token in G(cid:48)
for each member of S. Then, for each input data x member
of G there is a corresponding x(cid:48) member of G(cid:48), and x(cid:48) is a
sequence of token values tij, where i denotes the position
of t in the sequence and j denotes the structure sj in set S
that the token t corresponds to.

We pre-process each input data instance x (member of G)
to transform it to the corresponding x(cid:48) (member of G(cid:48)).
This transformation process can be easily automated using
existing tools (e.g. compiler toolchains). For each structure
sj in set S, it can optionally be deﬁned to carry extra
property information, e.g. LoopBegin structures may be
deﬁned to carry an integer value giving the loop iteration
count. The format of this extra information can be tailored
individually for each structure sj in set S, without worrying
about normalizing the range or using a uniform size across
all structures. This is because the design of our neural
network is modular and it uses separate component neural
networks to process different structures in the input data.
Thus x(cid:48) is a sequence of token values tij, where for speciﬁc
values of j, the token tij is followed by an annotation value
vij. The objective now is transformed to learn the function
F (x(cid:48)).

3.3 Step 3: Associate Sub-functions

We associate a function Fj with each sj that is a member
of S. We deﬁne a new hyperparameter in the overall neural
network called state vector or SV . The state vector is used
to keep track of and to pass along historical contextual
information between modular components of the neural
network. Each Fj takes as input a current state vector
value, and produces one output which is used to compute
the next state vector value. If there is an annotation value
attached to instances of sj, then the corresponding Fj will
take the annotation value as a second input. In the common
case, there is one neural network corresponding to each
sj. However, it is possible to have ﬂexible mappings from
sj to component neural networks, including 1-to-1 and
It is also possible to use known predeﬁned
many-to-1.
functions for some Fj, while learning only the remaining
subset of functions. This ﬂexibility makes it possible to
utilize domain knowledge and improve the design of the
neural network to make learning more efﬁcient.

3.4 Step 4: Deﬁne Composition Rules

For each Fj, we deﬁne a composition rule F (cid:48)
j. The
composition rules primarily inﬂuence how the input state
vector for Fj is determined, and how the output of Fj is
used to compute the new value of the state vector (SV ).
As an example, the composition rules can deﬁne simple
sequential semantics where the input state vector is the
current value of SV , and the output is the next value of SV ,
i.e. F (cid:48)
j(x) is mapped to SV = Fj(SV, x). In the common
case, the same composition rules apply uniformly to all
functions Fj that are to be learned. However, the rules can
be tailored for speciﬁc functions based on semantics of the
application domain. For example, in the case of functions

Using Structured Input and Modularity for Improved Learning

that operate on recursive structures, the rules can include
a stack mechanism to help track the ﬂow of information
across nested constructs. Consider s1 = LoopBegin
and s2 = LoopEnd. Then F (cid:48)
1(x) can be mapped to
{SV = F1(SV, x); Stack_P ush(SV )}, and F (cid:48)
2(x) can
be mapped to {SV = Stack_P op(); SV = F2(SV, x)}.

The initial value of the state vector SV is a hyperparameter
of the overall neural network. The ﬁnal constructed neural
network has the individual neural networks for each Fj as
its components.

Figure 3. Baseline network design

For a given input x(cid:48) = t1j1 t2j2 ...tnjn , this neural network
learns (or infers):
F (x(cid:48)) = F (cid:48)
(t1j1 , _)...)).
jn
In comparison, a traditional RNN works on input
x = t1t2...tn and learns (or infers):
F (x) = F (tn, F (tn−1, ...F (t1, _)...)).

(t(n−1)j(n−1) , ...F (cid:48)
j1

(tnjn , F (cid:48)

j(n−1)

For backward propagation during training, the deltas are
computed and propagated in reverse through the individual
components that compose the overall neural network. For
this to work properly, the following conditions must be met:

1. Neural networks for functions to be learned must use
differentiable components, as in current practice.
2. Functions that are predeﬁned must either have an
inverse, or a corresponding reverse relationship must
be deﬁned for all points in the data domain.

3. Composition rules must have a corresponding reverse
rule, e.g. Stack_P ush in the forward pass transforms
to a Stack_P op in the backward pass, and vice versa.

Figure 4. Modular network design

pre-processing, and (iii) modular neural networks with
network structures designed in conjunction with the format
of the input data. We perform a controlled experiment using
a simple objective function and try to minimize the impact
from factors such as feature selection, collection of training
examples, and choice of neural network design parameters.
We ﬁrst describe the learning objective, then describe our
experimental methodology, and ﬁnally present our results.

3.5 Network Design

4.1 Learning Objective

Our input data is a sequence of tokens, so we used LSTMs in
our network design. The network structure for the baseline
comparison is composed of a number of LSTM layers,
followed by a number of fully connected (FC) layers, as
shown in Figure 3.

Figure 4 illustrates the network design for our modular
method. There is an instance of the LSTM network for
each function Fj to be learned. The fully connected (FC)
layers are placed at the end for output generation, analogous
to the baseline network design. The Mappings and Rules
for Modular Network represents the logic in functions F (cid:48)
j
that governs how inputs and outputs are routed between the
component LSTMs as the input data sequence is processed.

4 EXPERIMENTS

The goal of our experiments is to compare the learning
capability of the three approaches described in the beginning
of Section 3:
(i) purely data-driven black-box neural
networks, (ii) neural networks with discriminate data

To perform a detailed analysis of the learning capability of
our modular method we choose a problem that is complex
enough to challenge the baseline method, is relevant to real
problems in program analysis, and yet is simple enough to
allow us to perform a controlled set of experiments. We
assume a programming language that includes loops and
statements among other structured constructs. In the case
of loop nests, we observed from Figure 2 that there are
two factors contributing to complexity: (1) the non-linearity
introduced by multiplying iteration counts of multiple loops
(as shown by the drop in accuracy for LL-2 and LL-3),
and (2) the structural variations of having loop nests of
varying lengths (as shown by the drop in accuracy for
LL-5-simple). For the experiments in this paper, we focus
on the second factor i.e. complexity due to structure. We
deﬁne the function to be learned as a weighted count of
instructions in the program, where the weight for each
instruction is the loop nesting depth of that instruction. This
function has analog output values. In our experiments, when
computing the loss function or when testing for accuracy,
we characterize the output value to be correct if it is within

LSTM Fully Connected Input Output LSTM cell size m LSTM cell size m LSTM cell size m 1 2 n … … 1 … 1 2 m 1 … 2 2 m 1 … n 2 m LSTM1 Fully Connected Input Output LSTMs … Mappings and Rules for Modular Network … m 1 2 … 1 m 1 2 … 2 m 1 2 … n LSTM cell size m LSTM cell size m LSTM cell size m 1 2 n … LSTM cell size m LSTM cell size m LSTM cell size m 1 2 n … Using Structured Input and Modularity for Improved Learning

a relative error range of the exact numerical value (set to 5%
in experiments).

The training data comprises of a set of computer programs
in a programming language that is deﬁned by grammar G.
These programs can be parsed using an existing compiler
toolchain and converted into a sequence of tokens in G.
We identify three structures in the set S: LoopBegin
(LB), LoopEnd (LE), and BasicBlock (BB). LoopBegin
and LoopEnd are also expressions/tokens in grammar
G, whereas BasicBlock is only deﬁned in the derived
grammar G(cid:48), which is as follows:

P rogram → StmtList

StmtList → Stmts | StmtList Stmts
Stmts → BasicBlock | Loop
Loop → LoopBegin StmtList LoopEnd

All three structures in S are tokens in grammar G(cid:48). The
transformed input data (x(cid:48) in G(cid:48)) contains sequences of
tokens corresponding to the three identiﬁed structures, with
each token followed by a property value. The property value
for BasicBlock is the number of instructions in the basic
block. The property value for LoopBegin and LoopEnd is
the loop iteration count.

We compare four different methods for using neural
networks to learn the objective function:

Baseline This is the black-box method that relies on the
neural network to automatically ﬁgure out the relevant
information from the input data.

Modular This is our newly introduced method that
identiﬁes relevant structures in the data, and constructs a
modular neural network based on the identiﬁed structures.
For our experimental evaluation, both the Baseline and
Modular methods use an identical input data format.

PartiallyExplicit This method pre-processes the data to
explicitly mark the outermost boundary of loop nests.
It introduces 2 new tokens: LoopN estBegin (LNB)
and LoopN estEnd (LNE). The input data instances
are transformed to insert these new tokens before the
LoopBegin and LoopEnd tokens corresponding only to
the outermost loops. As an example, the following sequence
(shown without the annotation values):

BB LB LB BB LE BB LE LB BB LE

is tranformed to:

BB LN B LB LB BB LE BB LN E LE LN B

LB BB LN E LE
The input data instances used in this method have a longer
sequence length than the input data instances used in all
other methods, with redundant information added into
the input data. The data pre-processing introduced in
this approach is somewhat generic, in that it can apply
to multiple learning objectives in the domain of program

Table 1. Value ranges in the dataset

Y
value

Sequence Max loop
nest level

length

Instructions
in BB

Min
Max

4,000
18,000

16
32

1
5

1
1000

Table 2. Network parameters

LSTM cell
size

# LSTM
layers

# FC
neurons

# FC
layers

Baseline
Modular

128
64

3
1

256
256

6
6

analysis.

FullyExplicit This method pre-processes the data to
explicitly identify the loop nesting depth for each
BasicBlock. It assumes a maximum nesting depth of N , and
replaces the token BasicBlock with pre-processed tokens:
BasicBlock_0, BasicBlock_1,
..., or BasicBlock_N .
The input data instances are transformed to use the
corresponding token to represent basic blocks at different
nesting depths. The data pre-processing introduced in this
approach is overly speciﬁc to the objective function being
targeted. In a real application scenario (e.g. an optimizing
compiler performing many different kinds of analyses),
this will require transforming or storing a version of the
entire input dataset for each analysis component, and this
can be prohibitively expensive. Further, this approach is
impractical in its assumption of a maximum nesting depth.
Even though it is infeasible in practice, we include this
approach in our experiments in order to better understand
the learning capabilities of different methods.

4.1.1 Data Generation

We generate our dataset using the value ranges shown in
Table 1. For each labeled example, we randomly choose the
sequence length, the series of tokens in the sequence, and
the number of instructions associated with each BasicBlock
in the sequence. We generate an equal number of input
data instances with maximum loop nesting depth equal to
1, 2, 3, 4, and 5. The total number of instances in the
generated dataset are 25,000. Note that the mapping of
inputs to outputs is many-to-one: the number of instructions
(Y-value) depends on the program sequence length, the
size of individual basic blocks, the nesting of loops, and
the structural arrangement of basic blocks and loops, Also,
we allow imperfectly nested loops in program sequences.
These factors allow for a very large number of permutations
of input data sequences, and make the objective function
sufﬁciently complex for evaluating the learning capabilities
of different methods.

We want to choose training examples in our dataset such

Using Structured Input and Modularity for Improved Learning

that the input data instances evenly represent all values
across the range of the target function being learned. This
is because we want to test the learning capability across
the entire output value range. Evenly spreading out the
training examples prevents the case when learning ends up
concentrated in a narrow range of the output space as a
consequence of the dataset used for training. To control the
impact of the dataset on training, we generate the datasets
used in our experiments. Since we treat all output values
within a 5% range of the actual value as correct, each actual
output value y corresponds to a set of correct values with
the size of the set proportional to the value y. Therefore,
the input data instances are generated using a reciprocal
distribution which is proportional to 1/y.
In addition,
we experiment with a dataset generated using a uniform
distribution over values of y.

4.2 Methodology

experimented with

the Baseline, Modular,
We
PartiallyExplicit, and FullyExplicit methods to compare
their
Table 2 shows network
learning capability.
the different methods
for
conﬁguration parameters
(PartiallyExplicit and FullyExplicit use the same network
structure and size as the Baseline network). We varied the
training set size in the range 1250 to 21250, and determined
how the training set size affects accuracy in each case.

We tuned the design of the neural network for the Baseline
approach as much as we could in order to achieve maximum
accuracy. For this purpose, we tried different values
for multiple parameters to learn their effects on accuracy
and found an optimized set of parameter values. These
parameters include batch size, learning rate, optimization
algorithm, dropout rate, normalization, data encoding
schemes, network sizes, and required number of training
steps. Based on the results of these experiments, we ﬁxed
the size of the LSTM and FC layers in the Baseline network
and the Modular network as shown in Table 2.

We randomly shufﬂed the dataset and partitioned it into
training, test, and validation sets. We used the validation
set to check for accuracy during training, and obtained ﬁnal
accuracy numbers using the test set. We ran individual
experiments multiple times to ensure there are no outliers.
All code was implemented in Tensorﬂow version 1.5 on a
system equipped with NVIDIA Pascal P100 GPUs.

4.3 Results

4.3.1 Comparison of Learning Methods

We compare the learning capability of the four methods
described in Section 4.1. Figure 5 shows how accuracy
varies with training set size for each method. The Modular
network is able to achieve higher accuracy (up to 29%)

than any of the other approaches. The FullyExplicit method
achieves accuracy that is close to the Modular method, but
it requires pre-processing the input data in a manner that is
unrealistic in real applications. Further, the Modular method
achieves its performance with a smaller overall network
size than the other three methods. The PartiallyExplicit
method achieves accuracy that is signiﬁcantly higher than
the Baseline method, indicating that providing structural
information can simplify the learning process.

Figure 5. Effect of training set size on accuracy

We determined that 600,000 iterations were sufﬁcient to
converge on accuracy for all our training experiments.
Figure 6(a) shows how accuracy varies with training time
for the Baseline method and Figure 6(b) shows it for the
Modular method, both for training set size of 2500. In this
case, the Baseline method reaches 67% accuracy and the
Modular method reaches 96% accuracy. We observe that
both methods converge on accuracy, though the Modular
method converges much faster, and shows more stable
convergence behavior.

4.3.2

Impact of Input Data Distribution

Figure 7(a) and 7(b) show how the distribution of training
examples affects accuracy across the range of the function
being learned (output Y-values). The range of Y-values is
divided into intervals. The plots in the ﬁgures are histograms
showing the number of training examples in each interval,
together with the accuracy achieved for input data instances
whose output values are within the interval. Figure 7(a)
shows the behavior when using the reciprocal distribution as
described in Section 4.1.1, with trendlines showing accuracy
for both the Baseline and Modular methods. The plot
validates that this choice of distribution for generating the
training data indeed results in uniform learning behavior
across the target range of the function being learned. Note
that the accuracy is near-constant across the range for both
methods. Figure 7(b) shows the behavior when using data
generated with a uniform distribution across the range of
In this case, the Modular method still
output Y-values.
shows near-constant accuracy across the range, but the

0.5	0.6	0.7	0.8	0.9	1.0	1250	2500	3750	5000	6250	11250	21250	Accuracy	Training	set	size	Baseline	FullyExplicit	PartiallyExplicit	Modular	Using Structured Input and Modularity for Improved Learning

(a) Accuracy over time for Baseline

(b) Accuracy over time for Modular

Figure 6. Comparing the convergence of accuracy for Baseline and Modular methods

accuracy of the Baseline method varies across the range.
This shows that the Modular method is more robust than the
Baseline method, due to its ability to converge faster and
achieve higher accuracy with less data.

4.3.3 Impact of Neural Network Size

Figure 8(a) shows how accuracy varies with the LSTM cell
size and number of LSTM layers for the Baseline method.
Likewise Figure 8(b) shows how accuracy varies with the
number of neurons per FC layer and the number of FC layers.
We observe that the accuracy does not keep improving
beyond a certain neural network size. On the contrary,
the accuracy starts degrading at higher LSTM cell sizes
or number of layers, as the neural network starts to suffer
from overﬁtting. Based on trends observed in Figure 8(a)
and Figure 8(b), we ﬁxed the LSTM cell size, the number
of LSTM layers, the number of neurons per FC layer, and
the number of FC layers to the conﬁgurations shown in
Table 2. Note that the Modular network in our experiments
uses 3 LSTM sub-networks, one for each of the 3 identiﬁed
structures. Thus, it contains a total of 3 LSTM layers with
cell size 64 and 6 FC layers with 256 neurons per layer. As
a result, the Modular network is smaller in size compared
to the network used for the other three methods.

5 RELATED WORK

Neural Module Networks (Andreas et al., 2016) are very
similar to our modular neural networks in that they use
pre-processing to determine which portions of the input to
process through which network component (module) and
how to interconnect the outputs and inputs of the various
modules. The main difference between this work and ours
is that they operate on natural language inputs whereas
we operate on computer programs as input. For natural
language inputs, the choice of modules and interconnection
rules is not clear, and an LSTM is used to learn an
it is possible to
approximate mapping.

In our case,

deterministically pre-process the input to map it to a ﬁxed
set of semantic structures, which then correspond exactly to
the components in our network.

Routing Networks (Rosenbaum et al., 2018) are composed
of multiple function blocks and a router that dynamically
determines the ﬂow of input through the network. The entire
input data is routed to a function block, the result goes back
to the router, and the process repeats for a ﬁxed number
of iterations. Both the router and the function blocks are
trained during learning. Our modular neural network also
has a router and multiple component networks, but it works
on input sequences, and only partial inputs are forwarded
to appropriate component networks. Also, in our case the
routing rules are ﬁxed and based on domain knowledge.

NLP and Trees:
NLP has a rich body of work
exploiting syntactic or semantic language structures to
better understand the contents, intention, and sentiments
of texts. Mou et al. (2016) proposed a tree-based CNN to
learn explicit structural information in computer programs.
Their model traverses a program AST to extract structural
information and then applies dynamic pooling to gather
information over different parts of the tree. Their approach
to computer programs as structured data resonates with us.
However, their model focuses on applying the same CNN
to different parts of programs in a tree-based traversal while
our work proposes modularizing the network design based
on structural characteristics.
Tree-LSTM (Tai et al., 2015) proposed a tree-structured
generalization of the sequential LSTM architecture for
improved representation of structured input.
The
tree-based topology enables capturing structural information
in encoded sentence representations and shows better
performance than the traditional LSTM on longer sentences.
While this work effectively supports our approach of using
data representations with structural information, it still relies
on LSTM units to learn the structures in input data instead
of leveraging them to decompose the networks.

Using Structured Input and Modularity for Improved Learning

(a) Reciprocal distribution

(b) Uniform distribution

Figure 7. Distribution of training data and corresponding accuracy for output values

(a) Effect of LSTM size on accuracy

(b) Effect of FC size on accuracy

Figure 8. Effect of neural network size on accuracy

Compositional Networks
(Haykin, 1998; Kasabov &
Song, 2002) proposed composing multiple simpler machine
learning models that address sub-problems to eventually
solve a complicated problem. They also used domain
knowledge to design sub-models and control how to divide
input and integrate output. However, they partition the
training set and build a separate specialized model for each
set, while our work routes structurally different parts within
an individual training example to specialized models.
More recently, Hudson & Manning (2018) proposed a
differential architecture that performs structured reasoning
by sequencing a new recurrent cell with separate control
and memory. The proposed cell performs one reasoning
step at
Their network solves problems by
decomposing them into a sequence of attention-based
reasoning operations without relying on externally provided
In
structured representations or specialized modules.
contrast, we explicitly try to use known structural
information in the design of our neural network.

its core.

Capsule Networks (Sabour et al., 2017) have multiple
components in the overall network design, and adopt
a completely data-driven approach to discovering the
structural and contextual information required for learning
accuracy. We also use multiple components in our network,
but we use domain knowledge to customize the design so
that it can more easily capture structural and contextual
information.

Other Approaches: Szegedy et al. (2016); Vaswani et al.
(2017); Serban et al. (2016) and Wu et al. (2016) represent
recent efforts focused on efﬁcient construction of networks
using structural characteristics of input data or learning
objectives. While these approaches share commonality with
our work in building neural networks in a more structured
way, we directly leverage domain knowledge to simplify
the task of learning structures while they still rely on the
network to automatically learn the structure from data.

Machine Learning for Program Analysis: Memeti
et al. (2018) and Ashouri et al. (2018) provide a good
survey of related work in the area of program optimization.
Some other applications of using machine learning in
program analysis include clone detection and code similarity
analysis (White et al., 2016; Xue et al., 2018), code security
analysis (Su et al., 2018), and program synthesis (Ellis et al.,
2018; Lee et al., 2018a). Our work is also in the domain
of program analysis, but our technique of using structural
domain knowledge to improve learning has not been applied
before.

6 CONCLUSION

We have presented a method for modular construction
of neural networks to utilize structure within individual
instances of input data, and evaluated it on an example
code analysis problem. Compared to the baseline neural

0.0	0.1	0.2	0.3	0.4	0.5	0.6	0.7	0.8	0.9	1.0	0	100	200	300	400	500	600	4000	4700	5400	6100	6800	7500	8200	8900	9600	10300	11000	11700	12400	13100	13800	14500	15200	15900	16600	Accuracy	for	samples	in	range	Number	of	training	examples	Y	value	training	examples	accuracy	of	modular	accuracy	of	baseline	0.0	0.1	0.2	0.3	0.4	0.5	0.6	0.7	0.8	0.9	1.0	0	50	100	150	200	250	300	4000	4700	5400	6100	6800	7500	8200	8900	9600	10300	11000	11700	12400	13100	13800	14500	15200	15900	16600	Accuracy	for	samples	in	range	Number	of	training	examples	Y	value	training	examples	accuracy	of	modular	accuracy	of	baseline	1	2	3	0.0	0.1	0.2	0.3	0.4	0.5	0.6	0.7	4	8	16	32	64	128	256	512	Number	of	layers	Accuracy	Cell	size	1	2	3	2	3	4	5	6	0.0	0.2	0.4	0.6	0.8	8	16	32	64	128	256	Number	of	layers	Accuracy	Neurons	per	layer	2	3	4	5	6	Using Structured Input and Modularity for Improved Learning

network (which is based on a tuned LSTM with optimized
parameters), our modular neural network can achieve higher
accuracy using a smaller sized network and fewer training
examples. It also shows more robust learning, performing
equally well on input data with different distributions,
whereas the baseline neural network cannot do so. The
method has two main advantages: (1) it pre-processes data
to expose relevant structural information in the input which
simpliﬁes learning, and (2) it modularizes the network
structure so it is easier to train efﬁciently and can handle
more complex problems.

Even though our work is focused on programming languages
and code analysis, we note that our method is applicable to
other domains with data sources that are structured. This
includes data that is formatted using XML schemas, data
contained within relational databases, and music written
using well-known musical notations.

REFERENCES

Andreas, J., Rohrbach, M., Darrell, T., and Klein, D.
Learning to compose neural networks for question
answering. CoRR, abs/1601.01705, 2016. URL http:
//arxiv.org/abs/1601.01705.

Ashouri, A. H., Killian, W., Cavazos, J., Palermo, G.,
and Silvano, C. A survey on compiler autotuning
using machine learning. ACM Comput. Surv., 51(5):
96:1–96:42, September 2018.
ISSN 0360-0300. doi:
10.1145/3197978. URL http://doi.acm.org/10.
1145/3197978.

Bengio, Y., Louradour, J., Collobert, R., and Weston,
In Proceedings of the 26th
J. Curriculum learning.
Annual International Conference on Machine Learning,
ICML ’09, pp. 41–48, New York, NY, USA, 2009.
ACM. ISBN 978-1-60558-516-1. doi: 10.1145/1553374.
1553380. URL http://doi.acm.org/10.1145/
1553374.1553380.

Berdibaeva, G. K., Bodin, O. N., Kozlov, V. V.,
Nefed’ev, D. I., Ozhikenov, K. A., and Pizhonkov,
Y. A. Pre-processing voice signals for voice recognition
systems. In 2017 18th International Conference of Young
Specialists on Micro/Nanotechnologies and Electron
Devices (EDM), pp. 242–245, June 2017. doi: 10.1109/
EDM.2017.7981748.

Bohossian, V., Hasler, P., and Bruck, J. Programmable
In 1997 Proceedings Second Annual
neural logic.
IEEE International Conference on Innovative Systems
in Silicon, pp. 13–21, Oct 1997. doi: 10.1109/ICISS.
1997.630242.

Chen, Z., Gong, Z., Szaday, J. J., Wong, D. C., Padua, D.,
Nicolau, A., Veidenbaum, A. V., Watkinson, N., Sura, Z.,

Maleki, S., Torrellas, J., and DeJong, G. Lore: A loop
repository for the evaluation of compilers. In 2017 IEEE
International Symposium on Workload Characterization
(IISWC), pp. 219–228, Oct 2017. doi: 10.1109/IISWC.
2017.8167779.

Cummins, C., Petoumenos, P., Wang, Z., and Leather,
H. End-to-end deep learning of optimization heuristics.
In 2017 26th International Conference on Parallel
Architectures and Compilation Techniques (PACT), pp.
219–232, Sept 2017. doi: 10.1109/PACT.2017.24.

Ellis, K., Ritchie, D., Solar-Lezama, A., and Tenenbaum, J.
Learning to infer graphics programs from hand-drawn
images. In Advances in Neural Information Processing
Systems 31: Annual Conference on Neural Information
Processing Systems 2018, NeurIPS 2018, 3-8 December
2018, Montréal, Canada., pp. 6062–6071, 2018.
http://papers.nips.cc/paper/
URL
7845-learning-to-infer-graphics-programs-from-hand-drawn-images.

Feng, Y., Martins, R., Bastani, O., and Dillig, I. Program
synthesis using conﬂict-driven learning. In Proceedings
of the 39th ACM SIGPLAN Conference on Programming
Language Design and Implementation, PLDI 2018,
pp. 420–435, New York, NY, USA, 2018. ACM.
doi: 10.1145/3192366.
ISBN 978-1-4503-5698-5.
3192382. URL http://doi.acm.org/10.1145/
3192366.3192382.

Fonseca, A. and Cabral, B.

ÆminiumGPU: An
Intelligent Framework for GPU Programming, pp.
96–107. Springer Berlin Heidelberg, Berlin, Heidelberg,
2013.
doi: 10.1007/
978-3-642-35893-7_9. URL https://doi.org/10.
1007/978-3-642-35893-7_9.

ISBN 978-3-642-35893-7.

Franco, L. and Cannas, S. A. Solving arithmetic problems
using feed-forward neural networks. Neurocomputing,
doi:
18(1):61 – 79, 1998.
https://doi.org/10.1016/S0925-2312(97)00069-6. URL
http://www.sciencedirect.com/science/
article/pii/S0925231297000696.

ISSN 0925-2312.

collaborative

Fursin, G., Lokhmotov, A., Savenko, D.,

and
A Collective Knowledge workﬂow
into multi-objective
techniques.
learning
URL http://cknowledge.

Upton, E.
for
autotuning
January 2018.
org/repo/web.php?wcid=report:
rpi3-crowd-tuning-2017-interactive.

research
and machine

Haykin, S. Neural Networks: A Comprehensive Foundation.
Prentice Hall PTR, Upper Saddle River, NJ, USA, 2nd
edition, 1998. ISBN 0132733501.

Using Structured Input and Modularity for Improved Learning

Hochreiter, S. and Schmidhuber, J. Long short-term
memory. Neural Comput., 9(8):1735–1780, November
1997.
doi: 10.1162/neco.1997.
9.8.1735. URL http://dx.doi.org/10.1162/
neco.1997.9.8.1735.

ISSN 0899-7667.

In Proceedings
programming language processing.
the Thirtieth AAAI Conference on Artiﬁcial
of
Intelligence, AAAI’16, pp. 1287–1293. AAAI Press,
URL http://dl.acm.org/citation.
2016.
cfm?id=3015812.3016002.

Hudson, D. A. and Manning, C. D. Compositional
CoRR,
attention networks for machine reasoning.
abs/1803.03067, 2018. URL http://arxiv.org/
abs/1803.03067.

Jain, L. C. and Medsker, L. R. Recurrent Neural Networks:
Design and Applications. CRC Press, Inc., Boca Raton,
FL, USA, 1st edition, 1999. ISBN 0849371813.

Kaiser, L. and Bengio, S. Can active memory replace

attention? In NIPS, pp. 3774–3782, 2016.

Kasabov, N. K. and Song, Q. Denﬁs: dynamic evolving
neural-fuzzy inference system and its application for
IEEE Transactions on Fuzzy
time-series prediction.
Systems, 10(2):144–154, Apr 2002. ISSN 1063-6706.
doi: 10.1109/91.995117.

Kolokolov, A. S.

Signal preprocessing for speech
recognition. Autom. Remote Control, 63(3):494–501,
March 2002.
10.1023/
ISSN 0005-1179.
A:1014714820229. URL https://doi.org/10.
1023/A:1014714820229.

doi:

using

Lee, W., Heo, K., Alur, R., and Naik, M. Accelerating
search-based
learned
program synthesis
probabilistic models. In Proceedings of the 39th ACM
SIGPLAN Conference on Programming Language Design
and Implementation, PLDI 2018, pp. 436–449, New
York, NY, USA, 2018a. ACM. ISBN 978-1-4503-5698-5.
URL http:
doi:
//doi.acm.org/10.1145/3192366.3192410.

10.1145/3192366.3192410.

using

Lee, W., Heo, K., Alur, R., and Naik, M. Accelerating
learned
program synthesis
search-based
probabilistic models. In Proceedings of the 39th ACM
SIGPLAN Conference on Programming Language Design
and Implementation, PLDI 2018, pp. 436–449, New
York, NY, USA, 2018b. ACM. ISBN 978-1-4503-5698-5.
URL http:
doi:
//doi.acm.org/10.1145/3192366.3192410.

10.1145/3192366.3192410.

Memeti, S., Pllana, S., Binotto, A. P. D., Kolodziej, J.,
and Brandic, I. Using meta-heuristics and machine
learning for software optimization of parallel computing
CoRR,
systems: A systematic literature review.
abs/1801.09444, 2018. URL http://arxiv.org/
abs/1801.09444.

Mou, L., Li, G., Zhang, L., Wang, T., and Jin, Z.
Convolutional neural networks over tree structures for

Pratt, L. Y. Discriminability-based transfer between
neural networks. In Hanson, S. J., Cowan, J. D., and
Giles, C. L. (eds.), Advances in Neural Information
Processing Systems 5, pp. 204–211. Morgan-Kaufmann,
1993. URL http://papers.nips.cc/paper/
641-discriminability-based-transfer-between-neural-networks.
pdf.

Rosenbaum, C., Klinger, T., and Riemer, M. Routing
networks: Adaptive selection of non-linear functions
In International Conference
for multi-task learning.
on Learning Representations, 2018. URL https://
openreview.net/forum?id=ry8dvM-R-.

Sabour, S., Frosst, N., and Hinton, G. E. Dynamic
In Advances in Neural
routing between capsules.
Information Processing Systems 30: Annual Conference
on Neural Information Processing Systems 2017, 4-9
December 2017, Long Beach, CA, USA, pp. 3859–3869,
2017. URL http://papers.nips.cc/paper/
6975-dynamic-routing-between-capsules.

Serban, I. V., Sordoni, A., Lowe, R., Charlin, L., Pineau,
J., Courville, A. C., and Bengio, Y. A hierarchical latent
variable encoder-decoder model for generating dialogues.
CoRR, abs/1605.06069, 2016. URL http://arxiv.
org/abs/1605.06069.

Siu, K. Y., Bruck, J., Kailath, T., and Hofmeister, T.
Depth efﬁcient neural networks for division and related
problems. IEEE Transactions on Information Theory,
39(3):946–956, May 1993.
doi:
10.1109/18.256501.

ISSN 0018-9448.

Socher, R., Huval, B., Manning, C. D., and Ng,
Semantic compositionality through recursive
A. Y.
In Proceedings of the 2012
matrix-vector spaces.
Joint Conference on Empirical Methods in Natural
Language Processing and Computational Natural
Language Learning,
pp.
1201–1211, Stroudsburg, PA, USA, 2012. Association
for Computational Linguistics. URL http://dl.acm.
org/citation.cfm?id=2390948.2391084.

EMNLP-CoNLL ’12,

Soutner, D. and Müller, L. Application of lstm neural
In Habernal, I. and
networks in language modelling.
Matoušek, V. (eds.), Text, Speech, and Dialogue, pp.
105–112, Berlin, Heidelberg, 2013. Springer Berlin
Heidelberg. ISBN 978-3-642-40585-3.

Using Structured Input and Modularity for Improved Learning

Learning and Programming Languages, MAPL 2018,
pp. 11–19, New York, NY, USA, 2018. ACM.
ISBN 978-1-4503-5834-7.
doi: 10.1145/3211346.
3211347. URL http://doi.acm.org/10.1145/
3211346.3211347.

Stern, R. M. and Acero, A. Acoustical pre-processing
In Proceedings of the
for robust speech recognition.
Workshop on Speech and Natural Language, HLT ’89,
pp. 311–318, Stroudsburg, PA, USA, 1989. Association
for Computational Linguistics.
ISBN 1-55860-112-0.
doi: 10.3115/1075434.1075488. URL https://doi.
org/10.3115/1075434.1075488.

Su, F.-H., Bell, J., Kaiser, G., and Ray, B. Obfuscation
In
resilient search through executable classiﬁcation.
Proceedings of the 2Nd ACM SIGPLAN International
Workshop on Machine Learning and Programming
Languages, MAPL 2018, pp. 20–30, New York, NY,
ISBN 978-1-4503-5834-7. doi:
USA, 2018. ACM.
10.1145/3211346.3211352. URL http://doi.acm.
org/10.1145/3211346.3211352.

Szegedy, C., Ioffe, S., and Vanhoucke, V.

Inception-v4,
inception-resnet and the impact of residual connections
on learning. 02 2016.

Tai, K. S., Socher, R., and Manning, C. D.

Improved
semantic representations from tree-structured long
short-term memory networks. CoRR, abs/1503.00075,
URL http://arxiv.org/abs/1503.
2015.
00075.

I.

Attention is all you need.

Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit,
J., Jones, L., Gomez, A. N., Kaiser, L. u., and
Polosukhin,
In
Guyon,
I., Luxburg, U. V., Bengio, S., Wallach,
H., Fergus, R., Vishwanathan, S., and Garnett, R.
(eds.), Advances in Neural Information Processing
Systems 30, pp. 5998–6008. Curran Associates, Inc.,
2017. URL http://papers.nips.cc/paper/
7181-attention-is-all-you-need.pdf.

White, M., Tufano, M., Vendome, C., and Poshyvanyk, D.
Deep learning code fragments for code clone detection.
In Proceedings of the 31st IEEE/ACM International
Conference on Automated Software Engineering, ASE
2016, pp. 87–98, New York, NY, USA, 2016. ACM.
ISBN 978-1-4503-3845-5.
doi: 10.1145/2970276.
2970326. URL http://doi.acm.org/10.1145/
2970276.2970326.

Wu, Y., Schuster, M., Chen, Z., V. Le, Q., Norouzi, M.,
Macherey, W., Krikun, M., Cao, Y., Gao, Q., Macherey,
K., Klingner, J., Shah, A., Johnson, M., Liu, X., Kaiser,
u., Gouws, S., Kato, Y., Kudo, T., Kazawa, H., and Dean,
J. Google’s neural machine translation system: Bridging
the gap between human and machine translation. 09 2016.

Xue, H., Venkataramani, G., and Lan, T. Clone-hunter:
Accelerated bound checks elimination via binary
In Proceedings of the 2Nd
code clone detection.
ACM SIGPLAN International Workshop on Machine

