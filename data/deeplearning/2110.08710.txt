NeuralArTS: Structuring Neural Architecture Search with Type Theory

Robert Wu1, Nayan Saxena , Rohan Jain

University of Toronto
ML Collective
1rupert@cs.toronto.edu

1
2
0
2

v
o
N
5

]

G
L
.
s
c
[

3
v
0
1
7
8
0
.
0
1
1
2
:
v
i
X
r
a

Abstract

Neural Architecture Search (NAS) algorithms automate the
task of ﬁnding optimal deep learning architectures given an
initial search space of possible operations. Developing these
search spaces is usually a manual affair with pre-optimized
search spaces being more efﬁcient, rather than searching from
scratch. In this paper we present a new framework called Neu-
ral Architecture Type System (NeuralArTS) that categorizes
the inﬁnite set of network operations in a structured type sys-
tem. We further demonstrate how NeuralArTS can be applied
to convolutional layers and propose several future directions.

Introduction
Neural Architecture Search (NAS) has proven to be a com-
plex but important area of deep learning research. The aim of
NAS is to automatically design architectures for neural net-
works. Most NAS frameworks involve sampling operations
from a search space (Zoph and Le 2016). For example, Ef-
ﬁcient Neural Architecture Search (ENAS) uses a controller
based on reinforcement learning (RL) to sample child net-
works (Pham et al. 2018). Yu et al. (2019) and other recent
works have identiﬁed ﬂaws in NAS algorithms, motivating
improved methods for network construction. Search spaces
are usually manually developed pre-search, rely heavily on
researchers’ domain knowledge, and often involve trial and
error; it’s more of an art than a science. Additionally, the
domain of network operations is inﬁnite given the multitude
of basic operations and hyperparameters therein. It’s hard to
know which operations produce better performance in learn-
ing tasks. In this paper, we introduce a framework to poten-
tially improve search spaces using generation and heuristics.

Neural Architecture Type System
Artiﬁcial neural networks can be interpreted as a program-
ming domain, where operations can be categorized into type
systems. A type system T is a formal system in which every
element has a type τ , which deﬁnes its meaning and the op-
erations that may be performed on it (Coquand 2018). One
intuitive property of networks is the shape of the data as it
moves through layers. Classes of operations such as pooling
or convolution layers have mappings between input/output
(I/O) dimensions, which can range from totally ﬂexible (can
be placed anywhere in a network) to complex (perhaps re-
quiring a speciﬁc input and output shape). Therefore, it is

possible to categorize operations into a type system based
on these I/O mappings, opening new possibilities for search
space optimisation in NAS. This idea can be extended to
network subgraphs, since they can be abstracted as a block
of operations with compound dimension functions. A con-
sequence is that layers and subgraphs are also interoperable.

τ =

,

,

?

L0

?

Figure 1: Possible interchange of L0 ∈ τ with other ˜L ∈ τ
or subgraph [ ˜L1, . . . , ˜Ln] ∈ τ . The unspeciﬁed topologies
on either end represent previous/subsequent layers.

This idea is intuitive and not entirely new; it’s explored
in Elsken et al. (2021), albeit informally. To be formal and
precise, the domain U of network operations can be catego-
rized with a type system T centred around data shape com-
patibility. Compatibility is fundamental in informing which
operations can precede, follow, or replace each other. For
each operation layer L with shape dimensions {1, . . . , d},
deﬁne IL = (I (1)
L ) to
be the shapes of I/O data. Assume I/O shapes have the same
number of dimensions for simplicity.

L ) and OL = (O(1)

L , . . . , O(d)

L , . . . , I (d)

Dimension Functions
While these shapes can be constants, they are generally map-
pings that can be deﬁned as a dimension function fL. Opera-
tion layer L can have several other properties such as depth,
stride, or bias values. Some or all of these properties may
inﬂuence OL, and they can be encapsulated in fL. A dimen-
sion function can therefore be abstracted as OL := fL(IL).

Equivalence Properties
NeuralArTS can centre around replacement/interchange of
operation layers. Let LA, LB ∈ U be arbitrary layers with
I/O dimensions (ILA , OLA ) and (ILB , OLB ) respectively.
Deﬁnition 1 (Complete-Equivalence). LA and LB are com-
pletely equivalent if all of their properties are equivalent.
LA = LB ⇐⇒ (ILA, . . . , OLA ) = (ILA , . . . , OLB ).

 
 
 
 
 
 
Deﬁnition 2 (Type-Equivalence). LA and LB are consid-
ered type-equivalent if their I/O dimension functions are
equivalent. In other words, LA and LB belong to the same
type, τ . LA ∼ LB ⇐⇒ fLA = fLB
Deﬁnition 3 (Instant-Equivalence). LA and LB are instant-
equivalent at input size I if their I/O dimension functions
intersect at I. LA ⊥I LB ⇐⇒ fLA (I) = fLB (I)

Sequential Compatibility
Let IL be the domain of acceptable input shapes into L, and
OL be the range of output shapes produced from L.
Deﬁnition 4 (Forward-Compatibility). LA is forward com-
patible to LB if all output shapes of LA are acceptable as
input to LB. LA → LB ⇐⇒ OLA ⊆ ILB
Deﬁnition 5 (Complete-Compatibility). LA is completely
compatible to LB if they’re mutually forward-compatible.
LA ↔ LB ⇐⇒ LA → LB ∧ LB → LA

These properties regarding sequential compatibility of LA
and LB are potentially useful in network construction; the
controller in ENAS can change to consider compatibility in
making direct and skip connections (Pham et al. 2018).

Generative Example: Convolutional Layers

∼

∼

Figure 2: Type-equivalent convolutions that can be inter-
changed: a 5x5; a 7x7 with p = 1; and a 3x3 with d = 2.

Let S0 be the original ENAS search space (Pham et al.
2018) and C ∈ S0 be a convolution with kernel size k,
padding p, and dilation d. These properties dictate type-
equivalent convolutions. To test the efﬁcacy of NeuralArTS,
we ﬁrst added some type-equivalent dilated variants of con-
volutions to S0. We found that adding even a single such
convolution can outperform the baseline.

SEARCH SPACE
S0 (Baseline)
S0 + Conv(k = 3, p = 2, d = 2)

VAL ACCURACY
80.47%
81.91%

TEST ACCURACY
74.67%
78.64%

Table 1: Performance of ENAS with a dilated convolution.

(cid:36)

f (i)
C (I (i)

C ) :=

C + 2p(i)
I (i)

C − d(i)
C (k(i)
s(i)

C

C − 1) − 1

(cid:37)

+ 1 (1)

C (I (i)

We introduce a generation technique that, without loss of
generality, bounds two of (k(cid:48), p(cid:48), d(cid:48)) to (K, P, D) and de-
rives the third using the convolution’s I/O dimension func-
tion f (i)
C ) detailed in equation 1. Let (k, p, d) be the
properties of the original seed operation L0 ∈ τ . Let
(K, P, D) be the generation parameters, which include ex-
actly one None and two positive integer ranges (inclusive).
Algorithm 1 explores the Cartesian product of (K, P, D) to
produce candidate tuples (k(cid:48), p(cid:48), d(cid:48)) of properties. Each tu-
ple’s None value is replaced and derived from the other two
values and the original properties (k, p, d).

Algorithm 1: GenerateTypeEquivalentConvs

1: let settings = []
2: assert (K, P, D).count(None) = 1
3: for (k(cid:48), p(cid:48), d(cid:48)) ∈ (K × P × D) do
4:

if K = None then derive k(cid:48) = 2p(cid:48)−2p−d(k−1)
else if P = None derive p(cid:48) = d(cid:48)(k(cid:48)−1)
else if D = None derive d(cid:48) = 2p(cid:48)
k(cid:48)−1
if k(cid:48), p(cid:48), d(cid:48) ∈ N then settings.append((k(cid:48), p(cid:48), d(cid:48)))

d(cid:48)+1

2

5:
6:
7:
8: end for
9: return settings

Conclusion and Directions for Future Work
This generation method can be improved with “smarter” do-
mains of operation properties; if performance proves to be
continuous with respect to these properties, linear or man-
ifold optimization might help generate more reﬁned search
spaces to speed up NAS. Another exciting prospect is that
NeuralArTS can act as a heuristic for pre-optimized search
spaces. It can na¨ıvely eliminate completely-equivalent (or
even type-equivalent) operations in preprocessing. More
practical is changing the controller to modulate operation
likelihoods at the type (rather than operation) level. If per-
formance for types could be generalized, NeuralArTS can
also be used to hierarchize NAS by performing shallow type-
searches ﬁrst, and then choosing random or “best” repre-
sentative(s) from each type. We hypothesize these directions
might lead to improvements of NAS algorithms.

Acknowledgements
We would like to thank George-Alexandru Adam (Vector In-
stitute; University of Toronto) & Chuan-Yung Tsai (Vector
Institute) for their comments and discussions that greatly in-
ﬂuenced this paper. We are also grateful to Qi Jia Gao (Uni-
versity of Toronto) for assistance in the experimental setup.
Finally, we thank the ML Collective community for the gen-
erous computational support, as well as helpful discussions,
ideas, and feedback on experiments.

References
Coquand, T. 2018. Type Theory. The Stanford Encyclopedia
of Philosophy (Fall 2018 Edition), Edward N. Zalta (ed.).
Elsken, T.; Stafﬂer, B.; Zela, A.; Metzen, J. H.; and Hutter, F.
2021. Bag of Tricks for Neural Architecture Search. arXiv
preprint arXiv:2107.03719.
Pham, H.; Guan, M.; Zoph, B.; Le, Q.; and Dean, J. 2018.
Efﬁcient Neural Architecture Search via Parameters Shar-
In International Conference on Machine Learning,
ing.
4095–4104. PMLR.
Yu, K.; Sciuto, C.; Jaggi, M.; Musat, C.; and Salzmann, M.
2019. Evaluating the Search Phase of Neural Architecture
Search. arXiv preprint arXiv:1902.08142.
Zoph, B.; and Le, Q. V. 2016.
Search with Reinforcement Learning.
arXiv:1611.01578.

Neural Architecture
arXiv preprint

Supplementary Material

Motivation: Dilated Convolutions
Upon inspection of the Efﬁcient Neural Architecture Search
(ENAS) codebase, we discovered that the pre-optimized
search spaces included only operations that had the same
mapping of input-to-output data shapes (Pham et al. 2018).
Intuitively, this pre-optimized search space S0 was designed
such that operations could be arranged and connected in any
network structure and still remain compatible from input
data to classiﬁcation.

Our experiments entailed injecting type-equivalent con-
volutions to the original ENAS convolutions that had kernel
size k = 3, 5. To be precise, we increased the padding p
and dilation d values in these variant convolutions. Some
experiments also include some dilated transposed convo-
lutions. The experiments are labelled in this document as
3a/b/c/d/E/F/G for legacy reasons, and their speciﬁcations
are outlined in Table 2.

Outperforming Baseline in ENAS
In fact, some of experiments were close to or outperformed
the ENAS baseline. Results are listed in Table 2 and vali-
dation accuracy over time is illustrated in Figure 3. Experi-
ments 3d and 3E outperform the baseline in validation accu-
racy. In ﬁnal test accuracy, 3c, 3d, and 3E outperform base-
line by a few percentage points; 3E is able to achieve 78.64%
as compared to baseline’s 74.67%.

VAL ACCURACY
80.47%

TEST ACCURACY
74.67%

73.24%

68.93%

Figure 3: Validation accuracy graphed over time of ENAS
on various search spaces with added dilated convolutions.
ENAS baseline is labelled Original in with a red graph. Note
3d and 3E outperform baseline in validation accuracy.

Code
Baseline
3a

3b

3c

3d

3E
3F

3G

SEARCH SPACE
S0
S0 + {Conv(k = 3, p = 2, d = 2),
Conv(k = 3, p = 3, d = 3),
Conv(k = 5, p = 4, d = 2),
Conv(k = 5, p = 6, d = 3),
Conv(k = 5, p = 12, d = 6)}
S0 + 6{Conv(k = 3, p = 2, d = 2)
Conv(k = 3, p = 3, d = 3),
Conv(k = 5, p = 4, d = 2),
Conv(k = 5, p = 6, d = 3),
Conv(k = 5, p = 12, d = 6)}
S0 + 20{Conv(k = 3, p = 2, d = 2)
Conv(k = 3, p = 3, d = 3),
Conv(k = 5, p = 4, d = 2),
Conv(k = 5, p = 6, d = 3),
Conv(k = 5, p = 12, d = 6)}
S0 + 50{Conv(k = 3, p = 2, d = 2)
Conv(k = 3, p = 3, d = 3),
Conv(k = 5, p = 4, d = 2),
Conv(k = 5, p = 6, d = 3),
Conv(k = 5, p = 12, d = 6)}
S0 + {Conv(k = 3, p = 2, d = 2)}
S0 + {Conv(k = 3, p = 2, d = 2)
Conv(k = 3, p = 3, d = 3)}
S0 + {Conv(k = 3, p = 2, d = 2)
Conv(k = 3, p = 3, d = 3),
Conv(k = 5, p = 4, d = 2),
Conv(k = 5, p = 6, d = 3),
Conv(k = 5, p = 12, d = 6),
ConvT (k = 3, p = 2, d = 2),
ConvT (k = 3, p = 3, d = 3),
ConvT (k = 5, p = 4, d = 2),
ConvT (k = 5, p = 6, d = 3),
ConvT (k = 5, p = 12, d = 6)}

74.26%

73.71%

78.53%

76.64%

80.49%

77.86%

81.91%
76.40%

78.64%
70.26%

70.35%

68.11%

Table 2: Performance numbers of ENAS on various search
spaces with added dilated convolutions. Bolded values in
3c, 3d, 3E outperformed baseline. Results are averaged over
three runs.

Further Inspiration

It is not surprising that pre-optimized search spaces (Elsken
et al. 2021) – and the baseline of ENAS in particular (Pham
et al. 2018) – are not actually optimal. It would be nearly im-
possible to create a search space that is the very best for the
task because there likely exist inﬁnite better search spaces
not considered.

However, we didn’t expect that simply adding a few
na¨ıvely dilated convolutions (especially in the case of 3E)
would improve upon algorithms such as ENAS. The fact that
these were easily found means that the problem of ﬁnding
better search spaces might be tractable given a type system
like we proposed.

Search Space Generation

One simple application of a type system is extending search
spaces with generated operations. Such operations would be
type-equivalent to existing operations in the original search
space, which would ensure compatibility. This would au-
tomate the trial-and-error process we previously used in
adding dilated convolutions.

Brute-Force Search Space Generation
As discussed in the main paper, the GenerateTypeEquiva-
lentConvs algorithm bounds any two properties of (k, p, d),
and while exploring the Cartesian product of their ranges,
the third is derived.

Code
T1 kd04
T1 kd08
T1 kd12
T1 kd16
T1 kp04
T1 kp08
T1 kp12
T1 kp16
T1 pd04
T1 pd08
T1 pd12
T1 pd16

K
[1, 4]
[1, 8]
[1, 12]
[1, 16]
[2, 4]
[2, 8]
[2, 12]
[2, 16]
None
None
None
None

P
None
None
None
None
[1, 4]
[1, 8]
[1, 12]
[1, 16]
[1, 4]
[1, 8]
[1, 12]
[1, 16]

D
[1, 4]
[1, 8]
[1, 12]
[1, 16]
None
None
None
None
[1, 4]
[1, 8]
[1, 12]
[1, 16]

Table 3: Generation parameters for brute-force search with
bound starting from 1 or 2 ranging up to 4, 8, 12, 16.

Our Files
• The ﬁles in which these extended search spaces can be

found are in enas types/.

• The properties of the generate convolutions for each
search space can be found in the folder generated/.
results

can
ENAS-Experiments/results.

experimental

• Initial

found

be

in

Experimental Setup
The software used includes Python (3.6.x-3.8.x) and Py-
Torch (1.9), with CUDA (10.2, 11.1). Our hardware varies:
CPUs included 2nd gen Xeon E, 3rd gen Core i5, 8th gen
Core i3; GPUs included Nvidia GTX {1050, 1050 Ti, 1080},
RTX 2060, Tesla {K80, P100 (Google Colaboratory)}, TI-
TAN Xp. Each experiment was processed with only one
GPU with 3GB of allocated VRAM, and took between 18-
36 real-world hours.

