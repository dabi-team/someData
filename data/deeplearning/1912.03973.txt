Deep Teams: Decentralized Decision Making with
Finite and Inﬁnite Number of Agents

Jalal Arabneydi, Member, IEEE, and Amir G. Aghdam, Senior Member, IEEE

1

0
2
0
2

n
u
J

2

]

C
O
.
h
t
a
m

[

4
v
3
7
9
3
0
.
2
1
9
1
:
v
i
X
r
a

Abstract—Inspired by the concepts of deep learning in artiﬁcial
intelligence and fairness in behavioural economics, we introduce
deep teams in this paper. In such systems, agents are partitioned
into a few sub-populations so that the dynamics and cost of
agents in each sub-population is invariant to the indexing of
agents. The goal of agents is to minimize a common cost function
in such a manner that the agents in each sub-population are
not discriminated or privileged by the way they are indexed.
Two non-classical information structures are studied. In the ﬁrst
one, each agent observes its local state as well as the empirical
distribution of the states of agents in each sub-population, called
deep state, whereas in the second one, the deep states of a
subset (possibly all) of sub-populations are not observed. Novel
dynamic programs are developed to identify globally optimal
and sub-optimal solutions for the ﬁrst and second information
structures, respectively. The computational complexity of ﬁnding
the optimal solution in both space and time is polynomial
(rather than exponential) with respect to the number of agents
in each sub-population and is linear (rather than exponential)
with respect to the control horizon. This complexity is further
reduced in time by introducing a forward equation, that we
call deep Chapman-Kolmogorov equation, described by multiple
convolutional layers of Binomial probability distributions. Two
different prices are deﬁned for computation and communication,
and it is shown that under mild conditions they converge to zero
as the number of quantization levels and the number of agents
tend to inﬁnity. In addition, the main results are extended to
inﬁnite-horizon discounted models and arbitrarily asymmetric
cost functions. Finally, a service-management example with 200
users is presented.

Index Terms—Team theory, deep structure, controlled Markov

chains, large-scale systems, non-classical information.

I. INTRODUCTION

Team theory studies cooperative decision making and is
used in numerous applications such as smart grids, swarm
robotics, transportation networks, social networks, and emer-
gent behaviours, to name only a few. Such applications often
consist of a group of interconnected decision makers, modelled
as Markov decision processes,
that wish to accomplish a
common task in the presence of limited computation and
communication resources. Historically, team theory can be
traced back to the seminal work of Radner [1] and Marschack
and Radner [2] on static teams as well as Witsenhausen [3]

This work has been supported in part by the Natural Sciences and
Engineering Research Council of Canada (NSERC) under Grant RGPIN-
262127-17, and in part by Concordia University under Horizon Postdoctoral
Fellowship.

Jalal Arabneydi and Amir G. Aghdam are with the Department
of Electrical and Computer Engineering, Concordia University, 1455
de Maisonneuve Blvd. West, Montreal, QC, Canada, Postal Code:
H3G 1M8. Email:jalal.arabneydi@mail.mcgill.ca and
Email:aghdam@ece.concordia.ca

and Ho [4] on dynamic teams. For a comprehensive literature
overview, the interested reader is referred to [5] and references
therein.

When centralized information structure is feasible (i.e., joint
state is known to all decision makers), optimal solution is
given by the celebrated dynamic programming principle [6].
In this case, the number of computational resources (in space
and time)1 to identify the optimal solution increases expo-
nentially with the number of decision makers, in general;
a phenomenon known as the “curse of dimensionality”. For
example, a centralized system consisting of 100 decision
makers with binary states requires the computational resources
of order 2100 ≈ 1030, which is intractable. On the other hand,
centralized information structure may not even be feasible
due to limited communication resources, specially when the
number of decision makers is large. In such cases, it is desired
to have some form of decentralized information structure.
However, a lower communication requirement comes at the
cost of a harder optimization problem to solve because the
decision makers in a decentralized structure may have different
perception about the system [7]. Due to the high complexity of
the decentralized control problems, an explicit optimal solution
may be intractable for systems with more than two or three
decision makers [8], [9]. As a result, after nearly 60 years of
research, there is still a big gap between theory and practice.
In this paper, inspired by the concepts of deep learning
in artiﬁcial intelligence [10] and fairness in behavioural eco-
nomics [11], we introduce the notion of deep structured team
(deep team for short) as an attempt to establish a bridge
between team theory and its applications. It is to be noted
that deep learning and deep team share some resemblances;
for example, they both involve multi-stage stochastic opti-
mization problems over a number of i.i.d. random variables,
and more importantly, the deep team problem resembles a
deep neural network. However, the deep team approach is
conceptually different from deep learning because it is not
data-driven and its depth refers to the number of decision
makers. In simple words, deep team approach is an endeavour
to provide a systematic framework to make the classical single-
agent control algorithms deep with respect to the number of
decision makers, which in turn makes them applicable to large-
scale control systems by exploiting the notion of invariance
principle. To this end, we borrow the notion of partially

1In this paper, the computational resources in space and time, respectively,
refer to the size of memory and the number of iterations that an algorithm
needs to be run in order to perform a task. It is to be noted that the compu-
tational complexity in time is different from the computational complexity in
control horizon.

 
 
 
 
 
 
exchangeable systems, analogous to the notion of invariance
of coordinates in physics. A multi-agent system is said to
be partially exchangeable if the population of agents can be
partitioned into a few sub-populations in such a way that the
order in which the agents are indexed in each sub-population
does not matter.

It is well-known that Markov decision processes with par-
tially exchangeable agents are equivalent to Markov decision
processes coupled through the empirical distribution of states
and actions of agents in each sub-population [12]. Subse-
quently, without loss of generality, we restrict attention to
the latter formulation in this paper. We consider two non-
classical information structures: deep-state sharing and partial
deep-state sharing, where deep state refers to the empirical
distribution of the states of agents in each sub-population.
Under deep-state sharing structure, each decision maker has
access to its local state as well as the deep states of all
sub-populations whereas under partial deep-state sharing, it
has access to its local state and the deep states of only
a subset (which can be empty) of sub-populations. Under
these decentralized information structures, we study micro-
scopic and macroscopic behaviours of the decision makers
and identify globally optimal and sub-optimal fair strategies.
is a complete version of authors’ recent
This manuscript
work in networked control systems [13], [14], [15] and is the
generalization of the concept of mean-ﬁeld teams introduced
in [12].

In the context of game theory, mean-ﬁeld games study the
non-cooperative behaviour of a large number of exchangeable
players [16], [17], [18]. The solution concept is Nash equi-
librium and the proof revolves around the fact that the effect
of a single player on others is negligible when the number of
players is sufﬁciently large such that it can be considered as
inﬁnite. This reduces the inﬁnite-population game to a two-
player game between a generic player and an inﬁnite popula-
tion, and since the deep state of the inﬁnite population (i.e.,
mean-ﬁeld) has deterministic dynamics, a Nash solution may
be obtained in terms of the solution of two coupled forward-
backward nonlinear partial differential equations (i.e., Fokker-
Planck-Kolmogorov and Hamilton-Jacobi-Bellman equations).
The existence of such a solution is established by imposing
various Lipschitz-type ﬁxed-point conditions (that generally
hold for small time horizons) or monotonicity-type assump-
tions (that are often difﬁcult to verify). For the special case
of a common cost function, mean-ﬁeld games may be used to
identify an approximate person-by-person (Nash-bargaining)
solution. It is to be noted that person-by-person optimality
is weaker than global optimality, in general. For example,
consider a simple static two-agent control problem with the
cost function | max(x1, x2)|, x1, x2 ∈ R≥0. This problem has
uncountably many person-by-person optimal solutions (e.g.
x1 = x2 = x, ∀x ∈ R≥0) but admits a unique globally optimal
solution (i.e. x1 = x2 = 0).

Another relevant ﬁeld of research that is closely related to
mean-ﬁeld games is mean-ﬁeld-type control problems [19],
[20], [21]. When the cost functions and control
laws of
players in this type of problem are identical, the inﬁnite-
population social optimization problem reduces to a single-

2

agent stochastic optimal control problem, whose dynamics and
cost depend on the distribution of the state (also known as
McKean-Vlasov type). A solution to this problem is not known
in general, as it is time-inconsistent [22], [20]. However, it is
possible to ﬁnd a person-by-person time-consistent solution
by formulating the mean-ﬁeld type control in terms of two
coupled forward-backward equations, similar to those in mean-
ﬁeld games, with the distinction that the mean-ﬁeld is replaced
by the probability density function of the state of the generic
player [22], [23].

Despite the differences between cooperative mean-ﬁeld
games and mean-ﬁeld-type control, they both use the simpli-
ﬁcation afforded by the inﬁnite-population model to propose
strategies that are sub-optimal, fair, person-by-person, closed-
loop in the local state and open-loop in the population state.
To ensure that the inﬁnite-population solution is a reasonable
approximate solution for the ﬁnite population model,
the
standard approach is to assume that the solution is a continuous
function. Finding a numerical solution for the general case of
nonlinear state dynamics, to the best of our knowledge, is
still an open problem (specially when neither ﬁxed-point nor
monotonicity-type conditions are imposed on the model).

We take a different

route in this paper and consider
deep team problems that have ﬁnite state and action spaces
with non-convex cost functions and possibly multiple person-
by-person solutions. More precisely, we study an arbitrary
number of agents (not necessarily large) wherein the effect
of each agent on other agents is not negligible. Note that
this is a more challenging problem compared to the typical
mean-ﬁeld problems where the effect of a single agent on
others is normally neglected. We propose a team-theoretic
approach to identify the exact globally optimal fair strategy
under deep-state sharing information structure (that is closed-
loop in the population state) and a globally sub-optimal fair
strategy under partial deep-state sharing information structure
(that includes open-loop strategies). The proposed dynamic
programs are time-consistent for any number of agents and
their minimization is carried out over the space of local
control laws. In addition, we show that the performance gap
between the optimal and sub-optimal solutions converges to
zero as the number of agents goes to inﬁnity, without imposing
any continuity assumption on the solution. Furthermore, we
propose a quantization technique for both ﬁnite- and inﬁnite-
population models to numerically compute the corresponding
sub-optimal solutions without restricting to any ﬁxed-point
or monotonicity condition. It is to be noted that since the
approach is neither based on the negligible effect of individual
agents nor dependent on the future trajectory of the population
state, its extension to major-minor and common-noise prob-
lems [24], [25] introduces no additional complication.

The rest of the paper is organized as follows. In Section II,
the problem is formulated and the main contributions of this
paper are outlined. To identify an optimal solution under deep-
state sharing and a sub-optimal one under partial deep-state
sharing, two novel dynamic programs are proposed in Sec-
tions III and IV, respectively. To alleviate the computational
complexity of these dynamic programs, their quantized coun-
terparts are presented in Section V. In Sections VI and VII,

TABLE I

P(X ) = {(α1, . . . , α|X |)(cid:12)

Space of probability measures over X
(cid:12)αi ∈ [0, 1], i ∈ N|X |, (cid:80)|X |

i=1 αi = 1}

Space of empirical distributions over X with n samples
n , . . . , 1}, i ∈ N|X |, (cid:80)|X |

E n(X ) = {(α1, . . . , α|X |)(cid:12)

(cid:12)αi ∈ {0, 1

i=1 αi = 1}

I(X ) = {(α1, . . . , α|X |)(cid:12)

(cid:12)αi ∈ [0, 1], i ∈ N|X |}

|X |-fold unit interval

I(X ) uniformly quantized withr levels

Qr(X ) = {(α1, . . . , α|X |)(cid:12)

(cid:12)αi ∈ {0, 1

r , . . . , 1}, i ∈ N|X |}

the main results are analogously extended to the inﬁnite-
horizon discounted and arbitrarily asymmetric cost functions.
The special case of major-minor deep teams is presented in
Section VIII followed by a numerical example in Section IX.
Finally, the paper is concluded in Section X.

II. PROBLEM FORMULATION
Throughout the paper, N, R≥0, and R>0 denote the set of
natural numbers, non-negative real numbers, and positive real
numbers, respectively. Nk denotes the ﬁnite set of integers
{1, . . . , k}. In addition, P(·) is the probability of a random
variable; E[·]
is the expectation of an event; 1(·) is the
indicator function of a set; (cid:107) · (cid:107) denotes the inﬁnity norm of
a vector; ξ(·) is the empirical distribution of a set; | · | is the
absolute value of a real number or the cardinality of a set, and
δ· denotes the Dirac measure with a unit mass concentrated at
one point. The short-hand notation x1:t is used to denote vector
(x1, . . . , xt). For any pair of integers i, j ≤ n ∈ N, σi,j(x)
denotes the permuted version of vector x = (x1, . . . , xn)
such that the i-th element of σi,j(x) is xj, the j-the element
of σi,j(x) is xi, and the other elements are the same as of
x. The abbreviation a.s. stands for almost surely. For any
n ∈ N, binopdf(·, n, p) denotes the binomial probability
distribution of n trials with success probability p ∈ [0, 1].
Furthermore, for any n, r ∈ N and any ﬁnite set X , different
spaces are deﬁned as described in Table I. Let x1:n denote
a vector of n ∈ N samples from set X := {a1, . . . , a|X |},
where xi ∈ X , i ∈ Nn. The empirical distribution function
ξ : (cid:81)n
i=1 X → E n(X ) is deﬁned as a real-valued vector of size
(cid:80)n
|X | such that ξ(x1:n)(aj) = 1
n

1(xi = aj), j ∈ N|X |.

i=1

A. Model

There are various applications in which the population of
agents can be partitioned into a few sub-populations in such a
way that the order of indexing of agents in each sub-population
is not important. For example, in a smart grid, demands in a
region may be classiﬁed as residential and commercial, and
the numbering of the demands in each class does not affect
the aggregate consumed energy. Similarly, in swarm robotics,
robots may be categorized into a few groups with identical
characteristics such as leaders and followers where neither the
dynamics of motion nor the status of the swarm depends on the
way the robots are indexed in each group. Such applications
may be modelled as described next.

3

Consider a discrete-time control system consisting of a
ﬁnite population of agents, where agents are partitioned into
K ∈ N disjoint sub-populations. Denote by K the set of sub-
populations, by N k the agents of sub-population k ∈ K, and
by N the entire population of agents; note that N = ∪k∈KN k.
Given the control horizon T ∈ N, let the state, action, and the
noise of agent i ∈ N k of sub-population k ∈ K at time t ∈ NT
be denoted by xi
t ∈ W k, respectively.
The spaces X k, U k and W k of every sub-population k ∈ K are
ﬁnite and do not depend on the size of sub-population k ∈ K,
i.e. |N k|. For the entire population, the joint state, joint action,
and joint noise are analogously denoted by xt = (xi
t)i∈N ∈ X ,
t)i∈N ∈ U, and wt = (wi
ut = (ui
t)i∈N ∈ W at time t ∈ NT .
Let Dk
t denote the empirical distribution of the states and

t ∈ U k, and wi

t ∈ X k, ui

actions of sub-population k ∈ K at time t ∈ NT , i.e.,
(cid:1) ∈ E |N k|(X k × U k).

t = ξ (cid:0)(xi

t)i∈N k

t, ui

Dk

(1)

Similarly, let dk
of sub-population k ∈ K at time t ∈ NT , i.e.,

t denote the empirical distribution of the states

t = ξ (cid:0)(xi
dk

t)i∈N k

(cid:1) ∈ E |N k|(X k).
t ) and dt := (d1

t , . . . , DK

Deﬁne Dt := (D1
t ). For
ease of reference, the empirical distribution of states is called
deep state in the sequel. Denote by E the space of realizations
dt, i.e., E := (cid:81)

k∈K E |N k|(X k), t ∈ NT .

t , . . . , dK

(2)

the initial state of agent i of sub-
1 ∈ X k, and at time t ∈ NT

For any k ∈ K,

population k is denoted by xi
its state evolves as follows:
t+1 = f k
xi

t (xi

t, Dt, wi

t, ui
where the primitive random variables {x1, w1, . . . , wT } are
deﬁned on a common probability space and are mutually
independent. The dynamics (3) can be equivalently represented
in terms of the transition probability matrix such that:

t),

(3)

Pk(xi

t+1 | xi
(cid:88)

t, ui
1(xi

t, Dt) :=
t+1 = f k

t (xi

t, ui

t, Dt, wi

t))P(wi

t = wi).

(4)

wi∈W k

In the sequel, we occasionally interchange the two equivalent
representations (3) and (4), for ease of display. Let the per-step
cost function be uniformly bounded and denoted by ct(Dt) ∈
R≥0 at time t. Note that the social cost function is a special
case of the above cost function. To see this, let ck
t, Dt)
denote the per-step cost of agent i ∈ N k of sub-population
k ∈ K. Then, one arrives at:
1
|N k|

t (xi, ui
ck

ct(Dt) :=

t (xi, ui

t, Dt).

(cid:88)

(cid:88)

k∈K

i∈N k

In this paper, we make the following three assumptions on

the primitive random variables.

Assumption 1. For any sub-population k ∈ K, the primitive
random variables (wi

t)i∈N k , t ∈ NT , are exchangeable.

Assumption 2. The primitive random variables (wi
t)i∈N , t ∈
NT , are mutually independent across agents. In addition, for
each sub-population k ∈ K, random variables (wi
t)i∈N k are
.
identically distributed with probability mass functions PW k

t

Remark 1. Note that Assumptions 1 and 2 do not impose any
restriction on the probability distribution of initial states.

Assumption 3. The primitive random variables (xi
1)i∈N
are mutually independent across agents, and for each sub-
population k ∈ K, random variables (xi
1)i∈N k are identically
distributed with probability mass functions PX k

.

1

Denote by I i

t ⊆ {x1:t, u1:t−1} the information set of agent

i ∈ N at time t ∈ NT , i.e.,

t = gi
ui

t(I i

t ),

(5)

where function gi
t ∈ NT . The set of control laws g := {(gi
as the strategy of the system.

t is called the control law of agent i at time
t=1 is deﬁned

t)i∈N }T

Deﬁnition 1 (Partially exchangeable (fair) strategies). A
strategy g is said to be partially exchangeable if for any
pair of agents (i, j) ∈ N k of any sub-population k ∈ K at
t (I s
any time t ∈ NT , σi,j (gs
t ))s∈N , i.e.,
exchanging agents i and j has no effect on the strategy.

t ))s∈N = (gs

t (σi,jI s

B. Admissible strategy

The set of admissible strategies is deﬁned as the set
of partially exchangeable (fair) strategies, where no agent
is privileged or discriminated by the way it is indexed in
a sub-population. The restriction to fair strategies may be
viewed from two angles: a constraint that must be satisﬁed
or an assumption that is limiting. In this paper, we focus
on the former case and present applications in which such
a restriction is desirable and practical. It is substantiated by
numerous experimental data in behavioural economics that
humans make their decisions based not only on rationality
but also on whether or not the decisions are fair [11], [26].
For example, an unfair resource allocation in a grid can lead
to protest and anarchy among users, due to the discrimination
against some users, even if such a strategy yields the lowest
possible social cost function. This means that a Pareto-optimal
unfair strategy is not necessarily a sustainable equilibrium. To
learn more about the importance of fairness, the interested
reader is referred to a pivotal counterexample in behavioural
economics called the ultimatum game [27] for a seemingly
irrational behaviour. In addition, fair strategies are important
in control theory because they provide robustness, where, for
instance, it is desirable to distribute the total load of a network
in a fairly manner among servers in order to increase the life-
time of the servers as well as the robustness of the network
(in case a server fails). Furthermore, since ﬁnding a Pareto-
optimal solution is computationally expensive, as described in
Section I, the optimal fair strategy is of particular interest in
practice, as a simpler (yet more tractable) alternative.

Two decentralized information structures are investigated in
this paper. The ﬁrst one is referred to as the deep-state sharing
(DSS), where for any i ∈ N , agent i at time t ∈ NT observes
its local state xi
t as well as the history of the deep states of
all sub-populations, i.e.,

t = {xi
I i

t, d1:t}.

(DSS)

4

In practice, there are different ways to share the deep state
among agents. For example, in cellular communications, the
deep state may be collected and transmitted to all agents by
the base station, while in swarm robotics the deep state may
be computed in a distributed manner using consensus-based
algorithms. The second information structure is more general
than DSS, and is called the partial deep-state sharing (PDSS),
where for any i ∈ N , agent i ∈ N at time t ∈ NT observes
its local state xi
t and the history of the deep states of a subset
of sub-populations S ⊆ K, i.e.,

t = {xi
I i

t, (dk

1:t)k∈S }.

(PDSS)

Note that if S = K, PDSS is the same as DSS and if S = ∅,
PDSS is fully decentralized. Note also that DSS and PDSS
respect the privacy of agents by not sharing the local state of
each agent. The performance of any strategy g is described
by:

JN (g) = Eg(cid:2)

T
(cid:88)

t=1

ct(Dt)(cid:3),

(6)

where the subscript N denotes the dependence of the cost
function to the number of agents, and the expectation is taken
with respect to the probability measures induced by g.

Problem 1. For deep-state sharing information structure, ﬁnd
the optimal fair strategy g∗ such that for every fair strategy g,
JN (g∗) ≤ JN (g).

Denote by S c the complement set of S, i.e., S c = K\S, and
let n be the size of the smallest sub-population whose deep
state is not observed, i.e., n := mink∈S c |N k|.

Problem 2. For partial deep-state sharing information struc-
ture, ﬁnd an ε(n)−optimal fair strategy g such that JN (g) ≤
JN (g∗) + ε(n), where ε(n) ∈ R>0 and limn→∞ ε(n) = 0.

Remark 2. For an exchangeable system, Pareto-optimal and
optimal fair strategies are not necessarily the same; however,
for a linear quadratic model, they are identical under DSS [12].

Since both DSS and PDSS are non-classical information
structures, the computational complexity of ﬁnding a solution
to Problems 1 and 2 is NEXP, in general [7]. The main
contributions of the present paper are spelled out below.

1) We develop a dynamic program that can be used to
ﬁnd an optimal solution for Problem 1 (Theorem 2).
We also show that
the computational complexity of
ﬁnding the solution in both space and time is polynomial
to the number of agents in each sub-
with respect
to the control
population and is linear with respect
horizon (Corollary 1).

2) Although polynomial complexity is less than than expo-
nential, it could still be high when the size of population
is medium or large. For medium populations, we pro-
pose Theorems 3 and 5 to alleviate the computational
complexity (in time and space). In particular, show
that the dynamics of the deep state in Theorem 3 is
described by an equation that we refer to as the Deep
Chapman-Kolmogorov (DCK) equation, with a structure

analogous to that in convolutional neural networks (Sub-
section III-A).

3) For a large population, we consider Problem 2 because
when some sub-populations are large, it may not be
feasible to collect and share their deep states among
agents. We show that such information sharing has a
negligible effect on the optimal performance of the sys-
tem. In particular, we develop a dynamic programming
decomposition for Problem 2 that provides an ε(n)-
optimal strategy, where ε(n) converges to zero at the
n (Theorem 4). This dynamic program does
rate 1/
not depend on the size of sub-populations S c.

√

4) For the numerical computation of the dynamic program
of Theorem 4, it is required, in general, to solve a non-
smooth non-convex optimization problem. We propose a
quantized solution and prove that the quantization error
converges to zero at a rate inversely proportional to the
number of quantization levels (Theorem 6). An imme-
diate consequence of this result is that if the number of
n, then the quantized
quantization levels is greater than
solution will converge to the optimal solution at the same
rate that the unquantized solution does (Corollary 2).
5) We extend our main results to inﬁnite-horizon dis-
counted cost (Theorems 7 and 8). It is shown that DSS
strategy is stationary with respect to the observed deep
states whereas PDSS strategy is not (Remark 12).
We deﬁne a number of short-hand notations to ease the
exposition of the results and proofs in the sequel. Let I :=
(cid:81)
k∈K I(X k), and given any subset R ⊆ K and any scalar

√

r ∈ N, deﬁne the following spaces:






PR := (cid:81)
QR := (cid:81)
:= (cid:81)
IR

k∈R E|N k|(X k) × (cid:81)
k∈R Qr(X k) × (cid:81)
k∈R I(X k) × (cid:81)

k∈Rc P(X k),
k∈Rc E|N k|(X k),
k∈Rc E|N k|(X k),

(7)

where E ⊂ PR ⊂ I and QR ⊂ IR ⊂ I. Denote by Q :
IR → QR the quantizer function that maps every point z ∈ IR
to its nearest point q ∈ QR, i.e., Q(z) ∈ argminq∈QR (cid:107)z−q(cid:107),
which implies that: (cid:107)z − Q(z)(cid:107) ≤ 1

2r , ∀z ∈ IR.

Let γk : X k → U k be the mapping from the local state
space to the local action space of sub-population k ∈ K,
and γ := {γ1, . . . , γK} ∈ G, where G denotes the space of
all mappings γ. Let also wk
t and wt denote the empirical
distribution of the local noises of sub-population k ∈ K and
the entire system, respectively, at time t ∈ NT , i.e.,

(cid:40)

wk
t
wt

:= ξ (cid:0)(wi
:= (w1

t)i∈N k
t , . . . , wK

(cid:1) ∈ E|N k|(W k),
t ) ∈ W := (cid:81)

k∈K E|N k|(W k).

(8)

Then, deﬁne the following functions at every time t ∈ NT and
for any z = {z1, . . . , zK} ∈ I, γ ∈ G, wt ∈ W, and k ∈ K:
1) For any x ∈ X k and u ∈ U k, φk(z, γ)(x, u) ∈ [0, 1] is
deﬁned as zk(x)1(u = γk(x)), where its augmented form is:

φ(z, γ) := (φ1(z, γ), . . . , φK(z, γ)).

(9)

2) For any y ∈ X k, ¯f k
(cid:88)

(cid:88)

zk(x)1(f k

t )(y) ∈ [0, 1] is deﬁned as

t (z, γ, wk
t (x, γk(x), φ(z, γ), w) = y)wk

t (w),

w∈W k

x∈X k

(10)

where its augmented form is:
¯ft(z, γ, wt) := ( ¯f 1
3) For any y ∈ X k, ˆf k
(cid:88)

t (z, γ, w1

t ), . . . , ¯f K

t (z, γ, wK

t )).

t (z, γ)(y) ∈ [0, 1] is deﬁned as:

zk(x)Pk(y | x, γk(x), φ(z, γ)).

5

(11)

(12)

x∈X k

4) For any per-step cost ct, deﬁne the following nonnegative

real function:

(cid:96)t(z, γ) := ct(φ(z, γ)).

(13)

III. MAIN RESULTS FOR PROBLEM 1

To ﬁnd a solution to Problem 1, Witsenhausen’s standard
form could be used to develop a dynamic programming
decomposition [28]. However, the resultant dynamic program
would be intractable, and since the size of its information
state increases with time, it could not be extended to inﬁnite
horizon. Alternatively, one can use the common information
approach [29] to construct a dynamic program in terms of the
conditional probability of the joint state, given the common
information, i.e., P(xt | d1:t). In such a case, it is shown
in [30] by forward induction that if the initial states as well
as noise processes are exchangeable, P(xt | d1:t) is also ex-
changeable under fair strategies, and hence can be represented
by dt. For the general case of non-exchangeable initial states,
however, the result of [30] does not hold as P(xt | d1:t) is not
necessarily exchangeable. Thus, we present a direct approach
to obtain a dynamic programming decomposition in terms of
dt, regardless of the probability distribution of initial states. A
salient feature of this method is to identify the structure of the
transition probability matrix of deep state dt, which proves to
be useful not only for the numerical computations but also for
the convergence analysis of Problem 2.

Lemma 1. When attention is focused on fair strategies, the
control laws of DSS and PDSS strategies are identical in each
t =: gk
sub-population, i.e., gi

t , i, j ∈ N k, k ∈ K.

t = gj

Proof. The proof follows directly from equation (5),
deﬁnitions of DSS and PDSS, and Deﬁnition 1.

the
(cid:4)

According to equation (5) and Lemma 1, for any k ∈ K
and i ∈ N k, the control law of agent i of sub-population k at
time t ∈ NT under DSS is gk
t = gk
ui

t : E t ×X k → U k, i.e.,
t (xi
Using the information decomposition proposed in [29], we
split gk
into two parts for any k ∈ K and t ∈ NT . More
t
precisely, deﬁne function ψk

t, d1:t).

(14)

t : E t → G as follows:
t (·, d1:t).

t (d1:t) := gk
ψk
Then, from (14) and (15), one has:
t (xi

t = γk
ui

t),

where γk

t : X k → U k is deﬁned by ψk
t := ψk
γk
t (d1:t).

t , i.e.,

(15)

(16)

(17)

According to (16) and (17), the action of agent i of sub-
population k at time t is determined by two functions ψk
t

t . In the sequel, we refer to the functions γt
laws and to ψt

and γk
{γ1
{ψ1
follows that for any k ∈ K, t ∈ NT , x ∈ X k and u ∈ U k:

:=
t } ∈ G as the local
:=
t } as the global laws. From (1), (2) and (16), it

t , . . . , γK
t , . . . , ψK

Dk

t (x, u) =

1
|N k|

(cid:88)

i∈N k

1(xi

t = x)1(γk

t (xi

t) = u).

Subsequently, it results from (9) and (18) that:

Dt = φ(dt, γt).

(18)

(19)

We show that the deep state evolves in a Markovian manner
with respect to the local laws, which means that the history
of the deep state except the most recent one can be ignored.

Theorem 1. Let Assumption 1 hold. For any k ∈ K and t ∈
NT , the dynamics of the deep state of sub-population k at
time t can be expressed by:

dk
t+1

a.s.
=

¯f k
t (dt, γt, wk

t ),

(20)

where wk
addition, the deep state of the entire population evolves as:

t are deﬁned by (8) and (10), respectively. In

t and ¯f k

dt+1

a.s.
=

¯ft(dt, γt, wt),

(21)

where wt and ¯ft are given by (8) and (11), respectively.
Proof. According to (2), (3), (16) and (19), for any y ∈ X k,
one has:

dk
t+1(y) =

1
|N k|

(cid:88)

i∈N k

1(f k

t (xi

t, γk

t (xi

t), Dt, wi

t) = y).

(22)

For any j ∈ N|N k|, let σj shift the vector N|N k| circularly by
j positions, i.e., σj(i) := i + j if i + j ≤ |N k|; otherwise,
σj(i) := i + j − |N k|. From Assumption 1, it follows that:

dk
t+1(y)a.s.
=

1
|N k|

(cid:88)

i∈N k

1(f k

t (xi

t, γk

t (xi

t), Dt, wσj (i)

t

) = y).

It is now possible to compute |N k|dk
(23) over all j ∈ N|N k|, which is almost surely equal to:

(23)
t+1(y) by summing up

1
|N k|

=

=

(cid:88)

(cid:88)

1(f k

t (xi

t, γk

t (xi

t), Dt, wσj (i)

t

) = y)

j∈N k
(cid:88)

i∈N k
1
|N k|

(cid:88)

(cid:88)

(cid:88)

1(xi

t = x)1(wσj (i)

t

= w)

w∈W k

x∈X k

i∈N k

j∈N k

t (x, γk
× 1(f k
(cid:88)
(cid:88)
(cid:88)

t (x), Dt, w) = y)
t = x)1(f k

1(xi

t (x, γk

t (x), Dt, w) = y)

w∈W k

x∈X k
1
|N k|

×

i∈N k
(cid:88)

1(wσj (i)
t

= w)

(cid:88)

(cid:88)

(a)
=

j∈N k
(cid:88)

1(xi

t=x)1(f k

t (x, γk

t (x), Dt, w)=y)wk

t (w)

6

equation. In addition, equation (21) follows from (8), (11)
(cid:4)
and (20).

Remark 3. It is to be noted that the result of Theorem 1 holds
irrespective of the global laws ψ1:t.

From Theorem 1, the transition probability matrix of the

deep state of entire population can be presented as follows:
1(dt+1= ¯ft(dt,γt,w))P(wt=w). (24)

P(dt+1|dt,γt)=

(cid:88)

w∈W

Remark 4. The deep-state process d1:T is not necessarily a
controlled Markov process under joint actions u1:T −1, i.e.,

P(dt+1 | d1:t, u1:t) (cid:54)= P(dt+1 | dt, ut).

t + xi

t+1 = (1 − xi

As a counterexample, consider a homogeneous population of
agents, where xi
t)ui
t ∈ {0, 1},
i ∈ N\{1, 2}, t ∈ NT , with known initial states x1. It is not
possible to express dt+1 as a function of dt and ut, but it is
feasible to write it as a function of u1:t (by simply computing
x1:t due to the deterministic dynamics). Consequently, the
Markov property does not hold.

t(1 − ui

t), xi

t, ui

Deﬁne value functions {V d

T +1} backward in
T +1(d) = 0, ∀d ∈ E . Also, for any t ∈ NT

1 , . . . , V d

T , V d

time such that V d
and d ∈ E, deﬁne:

V d
t (d) = min
γ∈G

(cid:0)(cid:96)t (d, γ) + E[V d

t+1

(cid:0) ¯ft(d, γ, wt)(cid:1)](cid:1) .

(25)

t

(d), . . . , ψ∗,K

t (d) = {ψ∗,1

Denote by ψ∗
(d)} an argmin of
the right-hand side of the above dynamic program. Let agent
i ∈ N k of sub-population k ∈ K at time t ∈ NT take the
following action:
t = g∗,k
ui

t ∈ X k, dt ∈ E .
xi

t, dt) := ψ∗,k

(dt)(xi

(xi

t),

t

t

t

A preliminary version of the next
in [30].

(26)
theorem was presented

Theorem 2. Let Assumption 1 hold. Then, strategy (26) is
optimal for Problem 1.

Proof. The proof follows from the fact that the deep state dt is
an information state for Problem 1. In particular, according to
Theorem 1, d1:T is a controlled Markov process with control
actions γ1:T −1. Furthermore, according to equations (13)
and (18), the per-step cost is a function of dt and γt. Thus, the
dynamic programming decomposition follows from standard
(cid:4)
results in Markov decision theory [6].

The cardinality of E|N k|(X k) is upper-bounded polynomi-
ally in the number of the agents of sub-population k ∈ K,
i.e.,

|E|N k|(X k)| ≤ (|N k| + 1)|X k|.

(27)

w∈W k
= |N k|

(b)

x∈X k
(cid:88)

i∈N k
(cid:88)

w∈W k

x∈X k

t (x)1(f k
dk

t (x, γk

t (x), Dt, w) = y)wk

t (w),

where (a) and (b) follow from (2) and (8) , respectively. Equa-
tion (20) follows now from equations (10), (18) and the above

Consequently, the space of dynamic program (25) increases
at most polynomially with respect to the number of agents in
each sub-population k ∈ K, and is independent of time.

Remark 5. The decomposition proposed in equation (25) is
a non-standard dynamic program because the minimization is
over local law γt ∈ G rather than joint control action ut ∈ U.

The optimal strategy of Theorem 2 can be implemented
in a distributed manner since every agent can independently
compute the dynamic program (25) and observe the deep
state.2 According to (26), for any k ∈ K, i ∈ N k and t ∈ NT ,
the action (role) of agent i of sub-population k at time t is
determined by three factors:

1) global law ψt that depends on the agents’ dynamics,
per-step cost, and underlying probability distributions;
2) deep state dt that provides the statistical information on

the states of agents, and

3) local state xi

t that is private information for agent i and

unknown to others.

In general, randomization can improve the performance of
a fair strategy [31]. On the other hand, any parametrized
randomized strategy can be formulated as a deterministic one
wherein the randomization is embedded into the transition
probability and cost function, and the action space is replaced
by the parameter space. Therefore, ﬁnite parametrization does
not lead to an enhancement of the formulation.

Corollary 1. The computational complexity of solving the
dynamic program (25) in both space and time is polynomial
with respect to the number of agents in each sub-population
|N k|, k ∈ K, and is linear with respect
to the control
horizon T ∈ N.

Proof. The proof is presented in Appendix A.

(cid:4)

is to be noted that

Remark 6. It
the fairness of admis-
sible strategies is essential for establishing Corollary 1. In
addition, although the computational complexity of dynamic
program (25) is polynomial with respect to the number of
agents, it is exponential with respect to the cardinality of the
local state space, which means that the proposed strategy is
only tractable for small state spaces. When the model has a
special structure, however, it is possible to relax the restriction
to fair strategies as well as to small state spaces. Such special
structures include, for example, linear quadratic model wherein
the state space is an inﬁnite set and the optimal strategy is not
necessarily exchangeable [32].

For any realization dt ∈ E, local law γt ∈ G, sub-population
k ∈ K, and pair of states x, y ∈ X k at time t ∈ NT , deﬁne
Bk

t (y, x, dt, γt) ∈ P({0, 1, . . . , |N k|dk

t (x)}) as:

Bk

t (y, x, dt, γt) := 1(dk
× binopdf (cid:0)·, |N k|dk

t (x) = 0)δ0 + 1(dk
t (x), Pk(y | x, γk

t (x) > 0)
t (x), φ(dt, γt))(cid:1) ,

where Pk and φ are given by (4) and (18), respectively.
Furthermore, deﬁne ¯Bk
t (y, dt, γt) ∈ P({0, 1, . . . , |N k|}) as
the convolution of the vector-valued functions Bk
t (y, x, dt, γt)
over all states x ∈ X k = {x1, . . . , x|X k|}, i.e.,

(28)

¯Bk

t (y, dt, γt) := Bk

t (y, x1, dt, γt) ∗ . . . ∗ Bk

t (y, x|X k|, dt, γt).
(29)

2In the case of multiple minimizers, agents agree upon a deterministic rule

to choose one using argmin.

7

Theorem 3. Let Assumption 2 hold. The transition probability
matrix of the deep state can be computed efﬁciently in time
such that for any dt ∈ E, γt ∈ G, k ∈ K, y ∈ X k and
j ∈ N|N k|+1 at time t ∈ NT , we have:

P(dk

t+1(y) =

j − 1
|N k|

| dt, γt) = ¯Bk

t (y, dt, γt)(j).

(30)

Proof. From equations (18) and (22), it results that for any
dt ∈ E, γt ∈ G, k ∈ K and y ∈ X k at time t ∈ NT :

|N k|dk

t+1(y) =

(cid:88)

(cid:88)

1(xi

t = x)

x∈X k
× 1(f k

i∈N k
t (x, γk

t (x), φ(dt, γt), wi

t) = y).

(31)

For any sub-population k ∈ K and any pair of states x, y ∈ X k
at time t ∈ NT , deﬁne ˜Bk
t (y, x, dt, γt) ∈ P({0, 1}) as the
probability distribution function of the binary random variable
1(f k
t) = y). From (3), (4), and (16),
one has

t (x), φ(dt, γt), wi

t (x, γk

P(1(f k

t (x, γk

t (x), φ(dt, γt), wi

t) = y) = 1 | dt, γt)

= Pk(y | x, γk

t (x), φ(dt, γt)).

(32)

Thus, it follows from (32) that

˜Bk

t (y, x, dt, γt) = (1 − Pk(y | x, γk
+ Pk(y | x, γk

t (x), φ(dt, γt)))δ0
t (x), φ(dt, γt))δ1.

(33)

t (y, x, dt, γt) ∈ P({0, 1, . . . , |N k|dk

Let Bk
t (x)}) denote the
probability distribution function of the sum of |N k|dk
t (x)
binary random variables associated with state x in (31), i.e.,
t (y, x, dt, γt) = P((cid:80)
i∈N k 1(f k
Bk
t) =
t (x) = 0, it means Bk
y)). Therefore, if dk
t (y, x, dt, γt) = δ0;
otherwise, from equation (33), Assumption 2, and the fact that
the probability distribution function of the sum of independent
random variables is equal to the convolution of their probabil-
ity distribution functions, we have

t (x), φ(dt, γt), wi

t (x, γk

Bk

t (y, x, dt, γt) = ˜Bk

t (y, x, dt, γt) ∗ . . . ∗ ˜Bk

(cid:124)

t (y, x, dt, γt)
(cid:125)

.

(cid:123)(cid:122)
|N k|dk

t (x)

Let ¯Bk
t (y, dt, γt) ∈ P({0, 1, . . . , |N k|}) be the probability
distribution function of (31) (that is the sum of |N k| indepen-
dent binary random variables); hence, it is the convolution
t (y, x, dt, γt) over all states x ∈ X k. Notice that
of Bk
Bk
t (x)−fold convolution power of
˜Bk
t (y, x, dt, γt) is a binomial prob-
ability distribution function with success probability Pk(y |
x, γk
t (y, x, dt, γt) is also a binomial
probability distribution with |N k|dk
t (x) trials and success
(cid:4)
probability Pk(y | x, γk

t (y, x, dt, γt) is an |N k|dk
t (y, x, dt, γt), where ˜Bk

t (x), φ(dt, γt)). Thus, Bk

t (x), φ(dt, γt)), given by (28).

A. Deep Chapman-Kolmogorov (DCK) equation

According to Theorem 3, the structure of the transition prob-
ability matrix of the deep state involves multiple convolution
functions in (29) and several Binomial probability distributions
in (28), triggered by some activation functions. This structure
resembles a deep neural network with convolutional layers
and Gaussian ﬁlters, which obeys a similar invariance feature

called spatial invariance. Inspired by this resemblance, we re-
fer to (30) as the Deep Chapman-Kolmogorov (DCK) equation,
which can be traced back to the seminal work of Bogolyubov
and Krylov (see [33]). In what follows, we present some of
the special cases of the DCK equation.

Consider a homogeneous population with no control action,

where K is a singleton set. The following holds:

• For the single-agent case, the local state and deep state
have equivalent
the single-
agent DCK can be represented equivalently in the form
of the (classical) Chapman-Kolmogorov equation.

information, meaning that

• For the inﬁnite-population case with coupled dynamics,
deep state dt appears in the transition probability matrix
as a diffusion term; hence, the macroscopic DCK equa-
tion may be viewed as the discrete-time discrete-space
counterpart of the Fokker-Planck-Kolmogorov and master
equations in the continuous-time models. For the spe-
cial case of decoupled dynamics, the macroscopic DCK
equation simpliﬁes to the classical Chapman-Kolmogorov
x∈X dt(x)P(y | x) =
equation,
(cid:80)
x∈X P(y | x)P(x | z), where dt−1 := δz, y, z ∈ X .
the limit of deep state dt+1(·), as the
Consequently,
population grows, which is in fact, mean-ﬁeld, can be
interpreted as the inﬁnite-sample distribution of a generic
random variable xt+1 with the transition probability
P(xt+1 = y|xt−1 = z).

i.e., dt+1(y) = (cid:80)

For more details on the connection with deep neural networks,
the interested reader is referred to [32], [34].

IV. MAIN RESULTS FOR PROBLEM 2

So far, we have assumed that the deep states of all sub-
populations are shared among agents. However, for large sub-
populations it might be difﬁcult to collect and share their
deep states among agents. In such cases, we are interested
in Problem 2, where the deep states of some sub-populations
S c ⊆ K are not observed. We propose a sub-optimal strategy,
where the optimality gap converges to zero as the size of sub-
populations S c goes to inﬁnity. We impose Assumptions 2
and 3 on the primitive random variables and the following
two assumptions on the model.

Assumption 4. The deep states of sub-populations S c do not
affect the dynamics of the agents of sub-populations S.

Remark 7. Notice that the deep states of sub-populations S
can affect the dynamics of the agents of sub-populations S c.
In addition, Assumption 4 automatically holds for S = ∅ (i.e.,
when the information structure is completely decentralized).

An immediate implication of Assumption 4 is that

the
in (10) for any k ∈ S,

following relationship holds for ¯f k
t
t ∈ NT , z ∈ I, γ ∈ G and wt ∈ W:

¯f k
t ((zk)k∈S , γ, wk

t ) = ¯f k

t (z, γ, wk

t ).

(34)

8

t (independent of |N k|), such that

Assumption 5. For every x, y ∈ X k, u ∈ U k, k ∈ K and
Z1, Z2 ∈ (cid:81)
k∈K I(X k×U k), there exist positive real constants
H k,1
, H 2
t
(cid:12)Pk(y | x, u, Z1) − Pk(y | x, u, Z2)(cid:12)
(cid:12)
(cid:12)¯ct(Z1) − ¯ct(Z2)(cid:12)
(cid:12)
Remark 8. Any polynomial function of Z is a Lipschitz
function in Z because Z is conﬁned to a bounded interval.
It is worth highlighting that any continuous function can be
approximated as closely as desired by polynomial functions
according to the Weierstrass Approximation Theorem.

t (cid:107)Z1 − Z2(cid:107),
t (cid:107)Z1 − Z2(cid:107).

(cid:12) ≤ H k,1
(cid:12) ≤ H 2

To distinguish from the optimal solution under DSS, we use
superscript p to denote the parameters associated with the sub-
optimal solution under PDSS. Denote by gp the sub-optimal
strategy and by dp
t ∈ E the deep state of the system under
strategy gp at t ∈ NT . Similar to Section III, we split the
strategy gp into two parts as follows. For every sub-population
k ∈ K and any (dp,k
1:t )k∈S , t ∈ NT , deﬁne:
(cid:40)
1:t )k∈S ) := gp,k
(·, (dp,k
t
((dp,k
1:t )k∈S ),

((dp,k
:= ψp,k
t

ψp,k
t
γp,k
t

1:t )k∈S ),

(35)

t

t

t

t

(xi

((dp,k

t, (dp,k

t) = γp,k

1:t )k∈S )(xi

1:t )k∈S ) = ψp,k

where γp,k
: X k → U k, k ∈ K. Therefore, the control action
of agent i ∈ N k of sub-population k ∈ K under PDSS is
given by:
t = gp,k
ui

(xi
t).
(36)
Notice that the local laws in (36) are different from those
in (16) because the information structures are different. How-
ever, the ﬁnite space G is the same for both DSS and PDSS,
i.e., γp
} ∈ G. Since Theorem 1 holds
t
regardless of the global laws ψ1:t (Remark 3), it also holds
for (ψp,1
1:t ) as a restriction function of ψ1:t. Hence,
the dynamics of the deep state of sub-population k ∈ K at time
t ∈ NT can be written as follows, according to Theorem 1:
¯f k
t (dp

1:t , . . . , ψp,K

, . . . , γp,K

:= {γp,1

t , wk

t , γp

dp,k
t+1

(37)

a.s.
=

t ).

t

t

In the augmented form, we have
¯ft(dp

dp

a.s.
=

t+1

t , γp

t , wt).

(38)

The remainder of this section is organized as follows. We ﬁrst
introduce a stochastic process that is controlled by local laws
γp
1:T and is implementable under PDSS. Then, we propose
a dynamic program based on this process and show that its
solution is sub-optimal, by comparing its performance with
the optimal solution of Theorem 2.

A. A mixed state

Let mk

t ∈ P(X k) denote the mean-ﬁeld of sub-population
k ∈ S c at time t ∈ NT , where |N k| is regarded as inﬁnity.
Deﬁne a mixed state pt ∈ PS consisting of the deep states of
sub-populations S and the mean-ﬁelds of sub-populations S c,
i.e.,
1 := dp,k
pk

1 = dk
1,

1 := mk

1 = PX k

and pk

k ∈ S,

,

1

k ∈ S c.
(39)

Moreover, for t > 1, pt evolves under γp

t ∈ G as follows:

Proof. The proof is presented in Appendix C.

9

(cid:4)

pk
t+1 :=

(cid:40) ¯f k
t (pt, γp
ˆf k
t (pt, γp

t , wk
t ),

t ), k ∈ S,
k ∈ S c,

(40)

where ¯f k
Denote by ¯f S

t and ˆf k

t are given by (10) and (12), respectively.
t , t ∈ NT , the augmented form of (40) as follows:

pt+1 = ¯f S

t (pt, γp

t , wt).

(41)

Proposition 1. Under Assumptions 2, 3, and 4, the stochas-
is adapted to the ﬁltration generated by
tic process p1:t
(dp,k
1:t )k∈S . In particular, p1 is speciﬁed by (39), and for any
t ∈ NT : pk
t ), k ∈ S c.
(cid:4)

Proof. The proof is presented in Appendix B.

t+1, k ∈ S and pk

t+1 = ˆf k

t (pt, γp

= dp,k
a.s.

t+1

B. Dynamic program for Problem 2

We now propose a dynamic program for Problem 2. Deﬁne
T , V p
T +1} backward in time
T +1(p) = 0 for any p ∈ PS . Also, for any

real-valued functions {V p
such that V p
t ∈ NT and p ∈ PS, deﬁne

1 , . . . , V p

V p
t (p) = min
γ∈G

(cid:0)(cid:96)t (p, γ) + E[V p

t+1( ¯f S

t (p, γ, wt))](cid:1).

(42)

implying that

The right-hand side of (42) admits at least one minimizer
1:t ) and γp
because pt ∈ PS can be expressed by (dp,k
1:t−1
according to Proposition 1,
its feasible set
is ﬁnite. As a result, the minimization in (42) is a search
over ﬁnite alphabets, meaning that it always has a minimizer
(solution). With a slight abuse of notation, let ψp
t (p) :=
{ψp,1
(p)} be one of the minimizers of the right-
t
hand side of (42). The proposed control law for agent i ∈ N k
of sub-population k ∈ K at time t ∈ NT is given by:

(p), . . . , ψp,K

t

t = gp,k
ui

t

(xi

t, pt) := ψp,k

t

(pt)(xi

t),

t ∈ X k, pt ∈ PS .
xi

(43)

Remark 9. An important feature of the dynamic program (42)
is that it is independent of the size of sub-populations S c, and
so is the strategy gp. This is due to the fact that PS, (cid:96)t and ¯f S
t
given, respectively, by (7), (13), and (39) are all independent
of |N k|, k ∈ S c.

Compared to the DSS value function (25) and strategy (26),
the source of error in the proposed PDSS value function (42)
and strategy (43) is the substitution of the model with dy-
namics ¯f1:T and deep state process d1:T by a model with
dynamics ¯f S
1:T and mixed-state process p1:T , respectively. In
what follows, we show that the propagation of such error in
the dynamics and value functions are bounded by Lipschitz
gains related to those introduced in Assumption 5.

C. Upper bounds on the dynamics and value functions

Lemma 2. Let Assumption 5 hold. There exist positive con-
stants H 3
t , k ∈ K, t ∈ NT (independent of |N k|), such
that for any z1, z2 ∈ I and γ ∈ G, we have

t , H 4

t (z1, γ) − ˆf k
(cid:107) ˆf k
(cid:12)(cid:96)t(z1, γ) − (cid:96)t(z2, γ)(cid:12)
(cid:12)

t (z2, γ)(cid:107) ≤ H 3
(cid:12) ≤ H 4

t (cid:107)z1 − z2(cid:107),
t (cid:107)z1 − z2(cid:107).

Lemma 3. [13, Lemma 2] Consider a random vector w1:n
consisting of n ∈ N i.i.d. random variables with probability
function PW . Then, E[(cid:107)ξ (w1:n) − PW (cid:107)] ≤ 1√

n .

Lemma 4. Let Assumption 5 hold. For any z1, z2 ∈ I, γ ∈ G
and k ∈ K at time t ∈ NT , we have

E[(cid:107) ¯f k

t (z1,γ,wk

t )− ˆf k

t (z2,γ)(cid:107)]≤H 3

t (cid:107)z1−z2(cid:107)+O(

Proof. The proof is presented in Appendix D.

1√

|N k|

).

(cid:4)

Lemma 5. Let Assumption 5 hold. For any z1, z2 ∈ I, γ ∈ G
and k ∈ K at time t ∈ NT , we have

E[(cid:107) ¯f k

t (z1, γ, wk

t ) − ¯f k

t (z2, γ, wk

t )(cid:107)] ≤ H 3

t (cid:107)z1 − z2(cid:107).

Proof. The proof is presented in Appendix E.

(cid:4)

Lemma 6. Let Assumption 5 hold. Given any function
f j
: I ×G × W → I, j ∈ {1, 2}, t ∈ NT , deﬁne real-
t
valued functions {V j
T +1} such that for any zj ∈ I,
V j
T +1(zj) = 0, and for any t ∈ NT and zj ∈ I, one has

1 , . . . , V j

V j
t (zj) = min
γj ∈G

(cid:0)(cid:96)t

(cid:0)zj, γj(cid:1)+E[V j

t+1(f j

t (zj, γj, wt))](cid:1). (44)

Suppose there exist scalars H z
t ∈ R>0 and ∆ ∈ R≥0 such
that for every z1, z2 ∈ I and γ ∈ G, the following inequality
holds:

E[(cid:107)f 1

t (z1, γ, wt)−f 2

t (z2, γ, wt)(cid:107)] ≤ H z

t (cid:107)z1−z2(cid:107)+∆. (45)

Then, at every time t ∈ NT , we have

|V 1

t (z1) − V 2

t (z2)| ≤ H 5

t (cid:107)z1 − z2(cid:107) + H 6

t ∆,

(46)

where H 5

T +1 := H 6

T +1 := 0, and for any t ∈ NT ,

H 5

t := H 4

t + H 5

t+1H z
t ,

and H 6

t := H 5

t+1 + H 6

t+1. (47)

Proof. The proof is presented in Appendix F.

(cid:4)

Lemma 7. Consider Lemma 6 without the min operator in
equation (44). Then, inequality (46) holds for any γ := γ1 =
γ2.

Proof. The proof follows along the same lines as that of
(cid:4)
Lemma 6 with no minimization operator.

Lemma 8. Under Assumption 2, for any d ∈ E, p ∈ PS and
γ ∈ G at time t ∈ NT , we have

E[(cid:107) ¯ft(d, γ, wt) − ¯f S

t (p, γ, wt)(cid:107)] ≤ H 3

t (cid:107)d − p(cid:107) + O(

1
√
n

),

where n = mink∈S c |N k| in Problem 2.

Proof. The proof follows from equations (11) and (40), and
(cid:4)
Lemmas 4 and 5.

D. Convergence result for Problem 2

t

)k∈K}T

Let g∗ = {(g∗,k

t=1 and gp = {(gp,k

t=1 denote
the strategies proposed in (26) and (43), respectively. Inspired
by the notion of the price of information in [35], we deﬁne the
price of information in a slightly different manner as follows.

)k∈K}T

t

Deﬁnition 2 (Price of information). The price of information
(PoI) is deﬁned as the loss of performance due to using the
PDSS strategy gp rather than the DSS strategy g∗, i.e.,

P oI := |JN (gp) − JN (g∗)|.

(48)

Theorem 4. Let Assumptions 2, 3, 4, and 5 hold. The price
n, i.e.,
of information (48) converges to zero at the rate 1/

√

P oI ≤ (H 5

1 + H 6

1 )O(

1
√
n

),

1 and H 6

where O( 1√
n ) is independent of the control horizon T , and the
positive constants H 5
1 are computed recursively for
t ∈ NT from the Lipschitz constants of Lemma 2 as follows:
t + H 5
H 5
:= H 4
t+1, where
t
T +1 := H 6
H 5
T +1 := 0. Hence, strategy (43) is an ε(n)-optimal
strategy for Problem 2.

t and H 6
t

t+1 + H 6

t+1H 3

:= H 5

Proof. From the triangle inequality, |JN (gp) − JN (g∗)| is
upper-bounded by

|JN (g∗) − E[V p

1 (p1)]| + |JN (gp) − E[V p

1 (p1)]|.

(49)

The ﬁrst term in (49) is associated with the difference between
the optimal performance under DSS, given by Theorem 2,
and an approximate cost-to-go function proposed in (42). The
upper bounds developed in Subsection IV-C yield:

(b)

(cid:12)V d

|JN (g∗) − E[V p
≤ E[(cid:12)

1 (p1)]|(a)
1 (p1)(cid:12)
1 (d1) − V p
1
√
n

1 + H 6

≤ (H 5

1 )O(

(d)

),

= |E[V d
(cid:12)](c)
≤ H 5

1 (d1)] − E[V p
1 (p1)]|
1 E(cid:107)d1 − p1(cid:107) + H 6

1 O(

1
√
n

)

(50)

where (a) follows from Theorem 2, i.e., JN (g∗) = E[V d
1 (d1)];
(b) follows from the fact that for every random variable y ∈ R,
|E[y]| ≤ E[|y|]; (c) follows from equations (25), (42), and
Lemmas 6 and 8, where H z
t := H 3
n ), and (d)
follows from Assumption 3, equation (39), and Lemma 3.

t and ∆ = O( 1√

The second term in (49) corresponds to the difference
between the performance of the system under the proposed
strategy (43) and the approximate cost-to-go function deﬁned
in (42). Let γp
t (pt) be a minimizer of equation (42);
then, equation (42) simpliﬁes to:

t = ψp

V p
1 (p1) = E[

T
(cid:88)

t=1

(cid:96)t(pt, γp

t ) | p1].

(51)

10

From equations (38), (41), and Lemma 8, the approximation
error between processes dp
1:T and p1:T is bounded as follows:
for any dp

t ∈ E and pt ∈ PS,

E[(cid:107)dp

t+1 −pt+1(cid:107)] ≤ H 3

t (cid:107)dp

t −pt(cid:107)+O(

1
√
n

),

t ∈ NT . (53)

Therefore, it results from (51) and (52) that:

|JN (gp) − E[V p

1 (p1)]| = |E(cid:2)

T
(cid:88)

(cid:96)t(dp

t , γp

t ) −

(e)

≤ H 5

1 E(cid:107)dp

1 − p1(cid:107) + H 6

1 O(

)(f )
≤ (H 5

1 + H 6

1 )O(

t=1
1
√
n

T
(cid:88)

t=1

(cid:96)t(pt, γp

t )]|

1
√
n

),

(54)

:= H 3

t and ∆ = O( 1√

where (e) follows from equation (53), Lemma 7 (where
H z
n )), and the fact that for every
t
random variable y ∈ R, |E[y]| ≤ E[|y|], and (f ) follows
from Assumption 3, equation (39) and Lemma 3. The proof
(cid:4)
of Theorem 4 results from (49), (50), and (54).

V. QUANTIZATION RESULTS

In this section, we present quantized solutions for Prob-
lems 1 and 2 to reduce the complexity in space for dynamic
programs (25) and (42) in Theorems 2 and 4, respectively.

Given a subset R ⊆ K of sub-populations and the number of
quantization levels r ∈ N, we are interested to ﬁnd an ε(r)-
optimal solution for Problem 1 such that the computational
complexity of ﬁnding this solution in space is independent
of the size of sub-populations R. To distinguish this solution
from the optimal one under DSS, we use superscript q for the
parameters associated with the quantized solution. Denote by
gq the strategy of the quantized solution and by dq
t ∈ E the
deep state of the system under strategy gq at t ∈ NT , where
dq
1 = d1. We impose an assumption similar to Assumption 4
as follows.

Assumption 6. Assumption 4 holds for sub-populations R,
where S c is substituted by R.

Deﬁne real-valued functions {V q

T +1} backward
T +1(q) = 0 for any q ∈ QR . Also, for

1 , . . . , V q

T , V q

in time such that V q
any t ∈ NT and q ∈ QR, deﬁne

V q
t (q) = min
γ∈G

(cid:0)(cid:96)t (q, γ) + E[V q

t+1

(cid:0)Q( ¯ft(q, γ, wt))(cid:1)](cid:1) .

(55)
Denote by ψq
(q)} an argmin of the
right-hand side of (55). Let agent i ∈ N k of sub-population
k ∈ K at time t ∈ NT take the following action:

t (q) = {ψq,1

(q), . . . , ψq,K

t

t

t = gq,k
ui

t

(xi

t, Q(dq

t )) := ψq,k

t

Let also gq = {(gq,k
in (56). We deﬁne the price of computation as follows.

)k∈K}T

t

t),

(Q(dq

t ))(xi

t ∈ X k, dq
xi

t ∈ E .
(56)
t=1 denote the strategy proposed

According to equations (6), (13), (19) and (38), the perfor-
mance of the proposed strategy (43) is given by:

Deﬁnition 3 (Price of computation). The price of computa-
tion (PoC) is deﬁned as the loss of performance due to using
the strategy gq rather than the optimal strategy g∗, i.e.,

J(gp) = E[

T
(cid:88)

t=1

(cid:96)t(dp

t , γp

t )].

(52)

P oC := |JN (gq) − JN (g∗)|.

(57)

Theorem 5. Let Assumptions 2, 5, and 6 hold. The price of
computation (57) converges to zero at the rate 1/r, i.e.,

in Subsection II-B. Given a discount factor β ∈ (0, 1), the
performance of any strategy g is described by:

11

P oC ≤ (H 5

1 + H 6

1 )(cid:0) 1
r

(cid:1),

where H 5
strategy (56) is an ε(r)-optimal strategy for Problem 1.

1 are given in Theorem 4. This implies that

1 and H 6

Proof. The proof follows along the same lines as that of
Theorem 4 with the distinction that the source of error is the
quantization error, that is bounded by 1
2r from the deﬁnition
of the quantizer function Q. Using the same upper bounds
developed in Subsection (IV-C), it can be shown that the
propagation of the quantization error over the dynamics and
value functions are bounded by the same Lipschitz gains
(cid:4)
introduced in Theorem 4.

In general, ﬁnding an exact solution to the dynamic pro-
gram (42) is challenging because set PS is uncountable for
S (cid:54)= K. To address this challenge, we propose a quantized
dynamic program similar to that in Theorem 5. Let R in
Theorem 5 be equal to S c in Theorem 4. We are interested
in ﬁnding an ε(n, r)-optimal solution for Problem 2 such
that not only the deep states of sub-populations S c are not
communicated among agents, but also the computational com-
plexity of ﬁnding such a solution is independent of the size of
sub-populations S c. We use superscript pq for the parameters
associated to this solution.

1 , . . . , V pq

T +1} back-
T +1(q) = 0 for any q ∈ QSc . Also,

T , V pq

Deﬁne real-valued functions {V pq

ward in time such that V pq
for any t ∈ NT and q ∈ QSc , deﬁne
(cid:0)(cid:96)t (q, γ) + E[V pq

V pq
t (q) = min
γ∈G

t+1

(cid:0)Q( ¯f S

t (q, γ, wt))(cid:1)](cid:1) .

(58)
Denote by ψpq
(q)} an argmin of
the right-hand side of (58). Let agent i ∈ N k of sub-population
k ∈ K at time t ∈ NT take the following action:

t (q) = {ψpq,1

(q), . . . , ψpq,K

t

t

gpq,k
t

(xi

t, Q(pt)) := ψpq,k

t

(Q(pt))(xi

t),

t ∈ X k.
xi

)k∈S }T

Theorem 6. Let Assumptions 2, 3, 4 and 5 hold. Then, gpq :=
{(gpq,k
t=1 is an ε(n, r)-optimal solution for Problem 2,
t
i.e.,
(cid:12)
(cid:12)J(gpq) − J(g∗)(cid:12)

(cid:12) ≤ ε(n, r) = (H 5

1 + H 6

1 )(O(

(cid:1)),

1
√
n

) + (cid:0) 1
r

where H 5

1 , H 6

1 , and O( 1√

n ) are given in Theorem 4.

Proof. The proof follows from Theorems 4 and 5. To avoid
(cid:4)
repetition, the detailed proof is omitted.

Corollary 2. If r ≥
solution of Theorem 6 is the same as that of Theorem 4.

n, then the rate of convergence of the

√

VI. INFINITE HORIZON DISCOUNTED COST

In this section, we generalize our main results to the inﬁnite-
horizon discounted cost. It is assumed that the dynamics and
per-step cost in Problems 1 and 2 are time-homogeneous,
and that the information structures are the same as those

N (g) = Eg(cid:2)
J β

∞
(cid:88)

t=1

βt−1c(Dt)(cid:3).

Theorem 7. Let Assumption 1 hold. The optimal solution of
Problem 1 with the inﬁnite-horizon discounted cost is obtained
from the following Bellman equation, i.e., for any d ∈ E:

V d(d) = min
γ∈G

(cid:0)(cid:96)(d, γ) + βE[V d( ¯f (d, γ, w))](cid:1),

(59)

where the above expectation is taken with respect to w ∈ W.
Let ψ∗(d) = {ψ∗,1(d), . . . , ψ∗,K(d)} be an argmin of the
right-hand side of (59). Then, the optimal control law of agent
i ∈ N k of sub-population k ∈ K at time t ∈ N is given by:

g∗,k(xi

t, dt) := ψ∗,k(dt)(xi

t),

t ∈ X k, dt ∈ E .
xi

Proof: Consider the dynamic program of Theorem 2 for any
ﬁnite horizon T . From [6], make a change of variable for any
d ∈ E and t ∈ NT such that

W d

t (d) := β−T +t−1V d

T −t+2(d),

(60)

1 (d) := β−T V d

where W d
manipulations and setting t = 1, we arrive at: W d
minγ∈G((cid:96)(d, γ) + βE[W d
Bellman operator is contractive [6], we have

T +1(d) = 0. By simple algebraic
T +1(d) =
T ( ¯f (d, γ, w))]). Since the above

lim
T →∞

W d

T = W d

∞ =: V d.

(61)

Remark 10. Although the computational complexity of ﬁnd-
ing the solution of Bellman equation (59) is polynomial with
respect to the number of agents in each sub-population, it
is exponential in time, in general. However, one can ﬁnd
an ε-optimal solution in polynomial time by using the value
iteration method, where ε converges to zero exponentially [6].

For Problem 2 with the inﬁnite-horizon discounted cost, we

impose the following assumption on the model.

Assumption 7. It is assumed that βH 3 < 1, where H 3 is the
Lipschitz constant in Lemma 2.

When the dynamics of agents are decoupled, Assumption 7
holds because H 3 = 1 satisﬁes Lemma 2. This is an immediate
consequence of equation (12) and the fact that the probability
of any event is upper-bounded by 1.

Theorem 8. Let Assumptions 2, 3, 4, 5 and 7 hold. Then, an
ε(n)-optimal solution for Problem 2 with the inﬁnite-horizon
discounted cost function is identiﬁed by the following Bellman
equation for any p ∈ PS:

V p(p) = min
γ∈G

(cid:0)(cid:96)(p, γ) + βE[V p( ¯f S (p, γ, w))](cid:1),

(62)

where the above expectation is taken with respect to w ∈ W.
Let ψp(p) = {ψp,1(p), . . . , ψp,K(p)} be an argmin of the
right-hand side of (62). The control law of agent i ∈ N k of
sub-population k ∈ K at time t ∈ NT is given by:
t, pt) = ψp,k(pt)(xi

t ∈ X k, pt ∈ PS,
xi

t = gp,k(xi
ui

t),

and the optimality gap is bounded as follows:

|J β

N (gp) − J β

N (g∗)| ≤ ε(n) =

H 4
(1 − β)(1 − βH 3)

O(

1
√
n

),

where H 3 and H 4 are the Lipschitz constants in Lemma 2.

Proof. Consider the dynamic program of Theorem 4 for any
ﬁnite horizon T . Make a change of variables for any p ∈ PS
and t ∈ NT such that

t (p) := β−T +t−1V p
W p
ˆH 5
t := β−T +t−1H 5

T −t+2(p),

T −t+2, ˆH 6

t := β−T +t−1H 6

T −t+2,

(63)

1 := β−T H 6

1 := β−T H 5

T +1(p) = 0, ˆH 5

1 (p) := β−T V p

where W p
T +1 = 0
and ˆH 6
T +1 = 0. From Theorem 4, one arrives at
the following relations by simple algebraic manipulations and
setting t = 1:



T ( ¯f S (p, γ, w))](cid:1),

(cid:0)(cid:96)(p, γ) + βE[W p

W p
ˆH 5
ˆH 6

T +1(p) = minγ∈G
T +1 = H 4 + β ˆH 5
T + ˆH 6
T +1 = β( ˆH 5



T H 3 = H 4 (cid:80)T
(cid:80)T
T ) ≤ ˆH 5

τ =1(βH 3)τ −1,
τ =1 βT −τ +1.

T

(64)
Since the above Bellman operator is contractive [6], we have

Assumption 8. For any sub-population k ∈ K, initial states
(xi

1)i∈N k are exchangeable.

12

Proposition 2. Let Assumptions 1 and 8 hold. When atten-
tion is restricted to fair strategies, there exists a function
ˆ(cid:96)t : E ×G → R, independent of the global laws ψ1:t, such
that: ˆ(cid:96)t(dt, γt) := E[ct(xt, ut) | d1:t, γ1:t] = (cid:80)
u∈U
(cid:81)
t (xi))ct(x, u)P(xt = x | d1:t, γ1:t).
Proof. The proof follows from a forward induction proposed
in [30, Lemma 2] which shows that P(xt = x | d1:t, γ1:t) is
(cid:4)
partially exchangeable, i.e., it is representable by dt.

i∈N k 1(ui = γk

k∈K

x∈X

(cid:80)

(cid:81)

According to Proposition 2, dynamic programs proposed
in Sections III–VI extend naturally to any arbitrary asym-
metric cost function. Note that the complexity of computing
ˆ(cid:96)t(dt, γt) in time is exponential with respect to the number
of agents. However,
this computation can be carried out
off-line by machine learning methods or circumvented by
reinforcement learning techniques [37], [34]. In general, the
exploration space of an arbitrary asymmetric cost function
grows exponentially with the number of agents while that
of its deep state projection grows polynomially, according to
Proposition 2, which is a considerable reduction in complexity.

lim
T →∞

W p

T = W p

∞ =: V p.

(65)

VIII. MAJOR-MINOR SETUP: A SPECIAL CASE

In addition, from Lemmas 6 and 8 as well as equations (60)
and (63), for any d ∈ E and p ∈ PS, we obtain

(cid:107)W d

T +1(d) − W p

H 5

1 (cid:107)d−p(cid:107)+H 6

1 O(

T +1(p)(cid:107) = (cid:107)V d
) = ˆH 5

1 (d) − V p
T +1(cid:107)d−p(cid:107)+ ˆH 6

1 (p)(cid:107) ≤

T +1O(

1
√
n

1
√
n

).

(66)

Therefore, from equations (61), (65) and (66), it results that

(cid:107)W d

∞(m1) − W p

∞(p1)(cid:107) = (cid:107)V d(d1) − V p(p1)(cid:107)
∞(cid:107)d1 − p1(cid:107) + ˆH 6

≤ ˆH 5

∞O(

βH 4

∞ ≤

where from Assumption 7 and equation (64): ˆH 5
1−βH 3
and ˆH 6
(1−β)(1−βH 3) . The rest of the proof follows along
the same lines as the proof of Theorem 4, starting from the
triangle inequality, where inequality (c) in (50) and inequality
(cid:4)
(e) in (54) are replaced by inequality (67).

Remark 11. Since PS is a polish space, there always exists a
minimizer ψp(p) for (62). Under Assumption 7, the dynamic
programs in Theorems 5 and 6 can be extended to the inﬁnite-
horizon case by replacing (H 5

H 4
(1−β)(1−βH 3) .

1 + H 6

1 ) with

(67)

),

1
√
n
∞ = H 4

Consider a special case where a sub-population has only
one agent. In mean-ﬁeld game theory, this case is known
as major-minor mean-ﬁeld game which was ﬁrst introduced
in [38]. The sub-population with one agent is referred to
as the major player because it can directly inﬂuence other
players, called minor players, through its local state whereas
other players can only inﬂuence the major player through their
collective behaviour (mean-ﬁeld). In this type of scenario, the
classical mean-ﬁeld game approach is not directly applicable
as the mean-ﬁeld of minor players is no longer deterministic
(hence, unpredictable) due to the randomness of the major
player’s state [24]. A similar setup may be considered in deep
teams with the distinction that no additional complication is
introduced as the deep team solution is not in the form of
coupled forward-backward equations, i.e., it is independent
of the future trajectory of the deep state. To connect our
results to major-minor setup, consider a sub-population k ∈ K
consisting of one agent, i.e., |N k| = 1. Since the local state
and the deep state of this agent are identical, the split of
strategy gp in (35) can be done slightly different as follows:
((dp,k
t = ψp,k
ui
1:t )k∈S ). In this case, the
t : X k → U k (that takes |U k||X k|
local law of major agent γk
values) simpliﬁes to its local control action ui
t ∈ U k, i ∈ N k,
(that takes |U k| values).

1:t )k∈S ) = gp,k

((dp,k

t

t

Remark 12. Although strategy gp in Theorem 8 is stationary
with respect to (xi
t, pt), it is not stationary with respect to xi
t,
t ∈ NT . This implies that the assumption of stationary strategy
in [36] is rather restrictive.

VII. ARBITRARILY ASYMMETRIC COST FUNCTION

Let ct(xt, ut) : X × U → R≥0 be any arbitrarily-coupled

(asymmetric) per-step cost function at time t ∈ NT .

IX. NUMERICAL EXAMPLE
Example 1. Consider a company that provides a particular
service (e.g., internet, electricity or cellular phone) for n ∈ N
users. Assume that each user i ∈ Nn at time t ∈ N makes
an independent request with probability µ ∈ (0, 1) to receive
service. For simplicity, it is also assumed that each user is
not allowed to make a new request until its current request
is served. Therefore, the state of user i at time t is binary,

t ∈ X := {0, 1}, where xi

i.e., xi
t = 1 means that user i has a
request at time t, and xi
t = 0 means that it does not have a
request at that time. The initial states of users are distributed
identically and independently with respect to the probability
mass function PX . Let dt ∈ E := {0, 1
n , . . . , 1} denote
the empirical distribution of the requests at time t, i.e., dt =
1
t = 1). To serve its users, the company has h ∈ N
n
options which depend on different contracts and resources.
Deﬁne the following terms for every u ∈ U = {1, . . . , h}:

i=1 1(xi

n , 2

(cid:80)n

• Participation rate: This is the probability according to
which a user is incentivized to delay its request. The
company may use different contracts and price proﬁles to
incentivize users to postpone their requests. For example
in smart grids, the independent service operator may offer
discounts to motivate users to delay their demands during
peak-load time. Denote this probability by α(u) ∈ [0, 1]
and assume that it is independent of the request proba-
bility; hence, the probability of receiving a request from
a participant user is (1 − α(u))µ ≤ µ.

• Service rate: This is the probability according to which
a request is served at each time instant. The company
may use various suppliers (resources) to provide service;
hence, the service rate may not be the same for different
suppliers. Denote this probability by q(u) ∈ (0, 1].

• Base price: This is the price of serving a user when it has
no request (i.e., x = 0). This price is used for maintain-
ing resources such as data storage and communications.
Denote this price by cB(u, 1 − dt) ∈ R≥0.

• Service price: This is the price of serving a user when
it has a request (i.e., x = 1). This includes the price
of service as well as the price of maintenance (but may
not be a simple sum of the two). Denote this price by
cS(u, dt) ∈ R≥0.
Let ui
t ∈ U denote the option assigned to user i ∈ Nn at
time t. Then, the transition probability matrix of user i under
option ui

t at time t can be written as follows:

P(xi

t+1 | xi

t, ui

t) =

(cid:20) 1 − (1 − α(ui

t))µ (1 − α(ui
t))µ
1 − q(ui
t)

(cid:21)

.

q(ui
t)

t ˆwi

t(ui

t, ui

tcS(ui

t) + xi

t)wi(ui

t)cB(ui

t), ˆwi(ui

t+1 = (1 − xi

t, dt) = (1 − xi

t) = 1) = (1 − α(ui

Alternatively, one can express the above-mentioned dynam-
ics in the form of (3), i.e., xi
t),
where wi(ui
t) ∈ {0, 1} are the underlying Bernoulli
random variables associated with the rates of request, partici-
pation, and service such that: P(wi(ui
t))µ
and P( ˆwi(ui
t) = 1) = 1−q(ui
t). In addition, the cost of serving
user i at time t is given by: c(xi
t, 1 −
dt) + xi
t, dt). It is desirable for the company to adapt its
nominal capacity with the empirical distribution of requests.
t ∈ X 0 denote the nominal capacity of a server located
Let x0
in the company at time t ∈ N, where X 0 is a ﬁnite set with
values in [0, 1]. At each time instant, the server must decide the
next nominal capacity. Let u0
t ∈ U 0 =: X 0 denote the action
of the server at time t ∈ N. Subsequently, the state dynamics
of the server evolves as x0
t+1 = x0
t ), where
{w0
t ∈ {0, 1}, t ∈ N} is a Bernoulli process that captures the
probability of failure, i.e., w0
t = 1 implies that there is a fault
at time t, and w0
t = 0 implies that there is no fault. Denote
by pw0 ∈ [0, 1] the probability according to which a fault

t (1 − w0

t + u0

t w0

13

may occur. When a fault happens, no capacity is patched or
dispatched. Deﬁne (cid:96) : X 0 × U 0 → R≥0 as the cost function of
the server such that (cid:96)(x0
t |, t ∈ N,
where (cid:96)C(x0
t ∈ X 0 and
(cid:96)P |u0
t −x0
t |, (cid:96)P ∈ R≥0, is the cost of patching and dispatching
capacities x0

t ) ∈ R≥0 is the price of capacity x0

t ) := (cid:96)C(x0

t ) + (cid:96)P |u0

t − x0

t , u0

t and u0
t .

The objective of the company is to manage the users
(consumption side) as well as the server (generation side) in
such a way that the empirical distribution of requests is close
to the nominal capacity while incurring the lowest possible
price. More precisely, given β ∈ (0, 1) and λ ∈ R≥0, it is
desired to minimize the following cost

E(cid:2)

∞
(cid:88)

t=1

βt−1(cid:0) 1
n

n
(cid:88)

i=1

c(xi

t, ui

t, dt) + (cid:96)(x0

t , u0

t ) + λ(dt − x0

t )2(cid:1)(cid:3),

where the ﬁrst term is the average price of users, the second
term is the cost of the server, and the third term is the
penalty for the empirical distribution of requests deviating
from the nominal capacity. Since the state space X is binary,
the empirical distribution of requests dt is a sufﬁcient statistic
to identify the deep state, i.e., (1 − dt, dt), t ∈ N. Hence,
to ease the presentation, dt is referred to as the deep state
hereafter.

To illustrate the results, suppose the company has 3 options
(h = 3). The ﬁrst option, denoted by u = 1, is to provide
service at a ﬂat rate. The second option, denoted by u = 2,
is to give 20% discount to users in order to incentivize them
to cooperate with the company.3 In such a case, users are
motivated to delay their requests (with a probability directly
dependent on the participation rate) and let the company serve

3The monetary discount is considered to be an extra cost for the company.

Fig. 1. The optimal strategy for serving the users as well as adjusting the
nominal capacity in Example 1. (a) The optimal option for a user with no
request (x = 0); (b) the optimal option for a user with request (x = 1), and
(c) The optimal nominal capacity for the server.

14

as well as arbitrarily coupled cost, and demonstrated their
effectiveness by a numerical example in service management.
In practice, agents often have limited computation and
communication resources. These practical limitations are the
root causes of many challenges in team theory. We showed
that in deep teams, the number of computational resources
increases polynomially (rather than exponentially) with the
number of agents in each sub-population. The size (length) of
the shared information (i.e., deep state) among agents, on the
other hand, is independent of the number of agents in each sub-
population. Moreover, agents may decide not to communicate
their states at all, in which case the error of such compromise
n) and hence goes to zero
was shown to be bounded by O(1/
as the number of agents n increases. Furthermore, agents may
use a quantized solution whose computational complexity in
space is independent of the number of agents. We showed that
the error of such compromise is bounded by O(1/r), where
the error tends to zero as the number of quantization levels r
increases.

√

The main results of this paper can naturally be general-
ized to randomized strategies and partially observable deep
states by simply replacing the action space and deep state
with the space of probability measures and belief deep state,
respectively. To further enhance the computational complexity
of a deep team, one can use various approximation methods
such as deep reinforcement
the
resultant design may be viewed as doubly deep in the number
of decision makers (that represent parallel computation) as
well as the number of hidden layers (that represent sequential
computation).

learning. In such a case,

REFERENCES

[1] R. Radner, “Team decision problems,” The Annals of Mathematical

Statistics, pp. 857–881, 1962.

[2] J. Marschack and R. Radner, “Economic theory of teams,” New Haven:

Yale University Press, Feb. 1972.

[3] H. Witsenhausen, “Separation of estimation and control for discrete time
systems,” Proc. of IEEE, vol. 59, no. 11, pp. 1557–1566, Nov. 1971.
[4] Y.-C. Ho, “Team decision theory and information structures,” Proceed-

ings of the IEEE, vol. 68, no. 6, pp. 644–654, 1980.

[5] S. Y¨uksel and T. Bas¸ar, Stochastic Networked Control Systems.

Birkhauser, New York, 2013.

[6] P. R. Kumar and P. Varaiya, Stochastic systems: Estimation, identiﬁca-
tion, and adaptive control. SIAM (Classics in Applied Mathematics),
Philadelphia, USA, 2016, vol. 75.

[7] D. S. Bernstein, R. Givan, N. Immerman, and S. Zilberstein, “The
complexity of decentralized control of Markov decision processes,”
Mathematics of operations research, vol. 27, no. 4, pp. 819–840, 2002.
[8] Y. Ouyang and D. Teneketzis, “Signaling for decentralized rout-
ing in a queueing network,” Annals of Operations Research, DOI:
10.1007/s10479-015-1850-4, pp. 1–39, 2015.

[9] L. Lessard and S. Lall, “Optimal control of two-player systems with
output feedback,” IEEE Transactions on Automatic Control, vol. 60,
no. 8, pp. 2129–2144, 2015.

[10] Y. LeCun, Y. Bengio, and G. Hinton, “Deep learning,” nature, vol. 521,

no. 7553, pp. 436–444, 2015.

[11] M. Rabin, “Incorporating fairness into game theory and economics,” The
American economic review, vol. 83, no. 5, pp. 1281–1302, 1993.
[12] J. Arabneydi, “New concepts in team theory: Mean ﬁeld teams and
reinforcement learning,” Ph.D. dissertation, Electrical and Computer
Engineering department, McGill University, Montreal, Canada, 2016.

[13] J. Arabneydi and A. G. Aghdam, “A certainty equivalence result in team-
optimal control of mean-ﬁeld coupled Markov chains,” in Proceedings
of the 56th IEEE Conference on Decision and Control, 2017, pp. 3125–
3130.

Fig. 2. A trajectory of the deep state (empirical distribution of requests) and
the state of the server (nominal capacity).

Fig. 3. The convergence of the PDSS solution J(gpq) with n equidistant
quantization levels to the DSS solution J(g∗) = E[V d(d1, x0
1)] as the
number of users n increases, according to Theorem 6 and Remark 11.

their existing requests at a lower rate. The third option, denoted
by u = 3, is that the company purchases the desired service
from another service company at a variable rate. In addition,
the following parameters are chosen for the simulations:

n = 200,

β = 0.8, µ = 0.8,

cB(1, ·) = 0.59,

α = [0, 0.85, 0],
cB(2, ·) = (1 + 0.2)cB(1, ·),
cB(3, 1 − d) = 0.3(1 + 1 − d),
λ = 15,
(cid:96)C = [0.02, 0.04, 0.07, 0.12, 0.15, 0.2],

q = [0.1, 0.05, 0.2],
cS(1, ·) = 0.65,
cS(2, ·) = (1 + 0.2)cS(1, ·),
cS(3, d) = 0.5(1 + d),

(cid:96)P = 0.5, X 0 = U 0 = {0.3, 0.4, 0.5, 0.6, 0.7, 0.8}

pw0 = 0.05.

When there is no request from user i ∈ Nn at time t ∈ N,
the cheapest option for the user is chosen from Figure 1.a at
that time, and when there is a request from that user, its best
affordable option is selected from Figure 1.b. In addition, the
nominal capacity of the server is given by Figure 1.c based on
the current capacity and the deep state. Figure 2 demonstrates
the trajectory of the deep state and nominal capacity. In
Figure 3, the optimality gap between DSS and PDSS solutions
with respect to the number of users can be observed, where
the initial state of the server is 0.3 and the initial states of
users are identically and independently distributed according
to the probability distribution [0.2, 0.8].

X. CONCLUSIONS

In this paper, we introduced deep structured teams and
proposed novel dynamic programs to ﬁnd optimal and sub-
optimal strategies for agents in a team, under two non-classical
information structures, namely, deep-state sharing and partial
deep-state sharing. In addition, we developed some results
to alleviate the computational complexity of the proposed
solutions when the population is medium or large. We then
extended our results to the inﬁnite-horizon discounted cost

102030405060708090100Time00.10.20.30.40.50.60.70.80.91Trajectory of the deep-state and the state of the serverDeep-stateState of server05101520253035404550Number of agents6810121416182022The performance values  under  DSS and PDSSDSSPDSS[14] ——, “Optimal dynamic pricing for binary demands in smart grids: A
fair and privacy-preserving strategy,” in Proceedings of IEEE American
Control Conference, 2018, pp. 5368–5373.

[15] ——, “A mean-ﬁeld team approach to minimize the spread of infection
in a network,” in Proceedings of IEEE American Control Conference,
2019, pp. 2747–2752.

[16] M. Huang, R. P. Malham´e, and P. E. Caines, “Large population stochastic
dynamic games: closed-loop Mckean-Vlasov systems and the Nash
certainty equivalence principle,” Communications in Information &
Systems, International Press of Boston, vol. 6, no. 3, pp. 221–252, 2006.
[17] G. Y. Weintraub, C. L. Benkard, and B. Van Roy, “Markov perfect
industry dynamics with many ﬁrms,” Econometrica, vol. 76, no. 6, pp.
1375–1411, 2008.

[18] J.-M. Lasry and P.-L. Lions, “Mean ﬁeld games,” Springer, Japanese

Journal of Mathematics, vol. 2, no. 1, pp. 229–260, 2007.

[19] D. Andersson and B. Djehiche, “A maximum principle for SDEs of
mean-ﬁeld type,” Applied Mathematics & Optimization, vol. 63, no. 3,
pp. 341–356, 2011.

[20] A. Bensoussan, J. Frehse, and P. Yam, Mean ﬁeld games and mean ﬁeld

type control theory. Springer-Verlag New York, 2013.

[21] R. Carmona, F. Delarue, and A. Lachapelle, “Control of Mckean–
Vlasov dynamics versus mean ﬁeld games,” Mathematics and Financial
Economics, vol. 7, no. 2, pp. 131–166, 2013.

[22] R. Carmona and F. Delarue, Probabilistic Theory of Mean Field Games

with Applications I-II. Springer, 2018.

[23] N. Saldi, T. Bas¸ar, and M. Raginsky, “Markov–Nash equilibria in
mean-ﬁeld games with discounted cost,” SIAM Journal on Control and
Optimization, vol. 56, no. 6, pp. 4256–4287, 2018.

[24] M. Nourian and P. E. Caines, “(cid:15)-Nash mean ﬁeld game theory for
nonlinear stochastic dynamical systems with major and minor agents,”
SIAM Journal on Control and Optimization, vol. 51, no. 4, pp. 3302–
3331, 2013.

[25] P. Cardaliaguet, F. Delarue, J.-M. Lasry, and P.-L. Lions, The master
equation and the convergence problem in mean ﬁeld games. Annals of
Mathematics Studies, Princeton University Press, 2019, vol. 201.
[26] A. Falk and U. Fischbacher, “A theory of reciprocity,” Games and

economic behavior, vol. 54, no. 2, pp. 293–315, 2006.

[27] W. G¨uth, R. Schmittberger, and B. Schwarze, “An experimental analysis
of ultimatum bargaining,” Journal of Economic Behavior & Organiza-
tion, vol. 3, no. 4, pp. 367–388, 1982.

[28] H. S. Witsenhausen, “A standard form for sequential stochastic control,”

Springer, Math. Sys. Theory, vol. 7, no. 1, pp. 5–11, 1973.

[29] A. Nayyar, A. Mahajan, and D. Teneketzis, “Decentralized stochastic
control with partial history sharing: A common information approach,”
IEEE Transactions on Automatic Control, vol. 58, no. 7, pp. 1644–1658,
2013.

[30] J. Arabneydi and A. Mahajan, “Team optimal control of coupled
subsystems with mean-ﬁeld sharing,” in Proceedings of the 53rd IEEE
Conference on Decision and Control, 2014, pp. 1669–1674.

[31] F. C. Schoute, “Symmetric team problems and multi access wire
communication,” Automatica, vol. 14, no. 3, pp. 255–269, 1978.
[32] J. Arabneydi and A. G. Aghdam, “Deep teams with risk-sensitive linear
quadratic structure: A gauge transformation,” [Online]. Available at
https://arxiv.org/abs/1912.03951, 2020.

[33] N. N. J. Bogolyubov and D. P. Sankovich, “N. N. Bogolyubov and
statistical mechanics,” Russian Mathematical Surveys, vol. 49, no. 5,
pp. 19–49, 1994.

[34] J. Arabneydi and A. G. Aghdam, “Data collection versus data es-
timation: A fundamental trade-off in dynamic networks,” to appear
in IEEE Transactions on Network Science and Engineering, DOI:
10.1109/TNSE.2020.2966504, 2020.

[35] T. Bas¸ar and Q. Zhu, “Prices of anarchy, information, and cooperation
in differential games,” Dynamic Games and Applications, vol. 1, no. 1,
pp. 50–73, 2011.

[36] H. Tembine, J.-Y. Le Boudec, R. El-Azouzi, and E. Altman, “Mean
ﬁeld asymptotics of Markov decision evolutionary games and teams,”
in Proceedings of IEEE conference on Game Theory for Networks, pp.
140–150, 2009.

[37] J. Arabneydi and A. Mahajan, “Reinforcement learning in decentralized
stochastic control systems with partial history sharing,” in Proceedings
of IEEE American Control Conference, 2015, pp. 5449–5456.

[38] M. Huang, “Large-population LQG games involving a major player: the
Nash certainty equivalence principle,” SIAM Journal on Control and
Optimization, vol. 48, no. 5, pp. 3318–3353, 2010.

15

APPENDIX A
PROOF OF COROLLARY 1

To numerically solve the dynamic program (25), at each step
t ∈ NT , it is required to store | E |2|G| entries corresponding
to the transition probability matrix of deep state (24), and to
run |W| iterations to compute each entry. Each entry occupies
a certain amount of space and each iteration takes a certain
amount of time. In addition, to solve the minimization problem
in (25), | E ||G| iterations are needed for searching over all
possible cases. On the other hand, from (27), | E | and |W| are
bounded by (cid:81)
k∈K(|N k| + 1)|W k|,
respectively. Furthermore, |G| = (cid:81)
k∈K |U k||X k| is ﬁnite and
independent of the number of agents in each sub-population
as well as control horizon T , which completes to the result.

k∈K(|N k| + 1)|X k| and (cid:81)

APPENDIX B
PROOF OR PROPOSITION 1

The proof proceeds by induction. According to equation
time t = 1. Suppose
(39), Proposition 1 holds at
p1:t is adapted to the ﬁltration (dp,k
1:t )k∈S at time t. From
Assumption 4 and equation (34), the following equality holds
for any d ∈ E, p ∈ PS and γ ∈ G at any time t ∈ NT ,

initial

¯f k
t ((dk)k∈S , γ, wk
¯f k
t ((pk)k∈S , γ, wk

t ) = ¯f k
t ) = ¯f k

t (d, γ, wk
t (p, γ, wk

t ),
t ),

k ∈ S,

k ∈ S.

(68)

1:t+1 and pk

Consequently, processes dp,k
1:t+1, k ∈ S, start
from identical initial values and evolve identically in time
almost surely under identical local laws and noise processes,
according to equations (37), (39), (40), (68), and Assump-
tion 4. Thus, pk
t+1 evolves
deterministically according to (40), where pt and γp
t are
adapted to (dp,k
1:t )k∈S from the induction assumption and (35),
respectively. Therefore, p1:t+1 is adapted to (dp,k

t+1. For any k ∈ S c, pk

= dp,k
a.s.

1:t+1)k∈S .

t+1

APPENDIX C
PROOF OF LEMMA 2

It is well known that any linear combination and product
of Lipschitz functions is also a Lipschitz function. According
to (9), for any γ ∈ G, function φ(z, γ) is Lipschitz in z.
Consequently, for any y, x ∈ X k and any γ ∈ G,
the
transition probability Pk(y|x, γk(x), φ(z, γ)) is Lipschitz in z
due to Assumption 5. Hence, function ˆf k
t (z, γ), given by
(12), is Lipschitz in z because it is a linear combination
of Lipschitz functions (and their products). Let H k,3
denote
the corresponding Lipschitz constant, and deﬁne H 3
t as the
:= maxk∈K H k,3
maximum Lipschitz constant,
.
t
Similarly, function (cid:96)t(z, γ), given by (13), is Lipschitz in z due
to Assumption 5, on noting that the function φ is Lipschitz.

i.e., H 3
t

t

APPENDIX D
PROOF OF LEMMA 4

For every y ∈ X k, k ∈ K, it follows that

(cid:3)

(a)

E(cid:2)(cid:12)
t (z1, γ, dt)(y) − ˆf k
(cid:12) ¯f k
= E(cid:2)(cid:12)
(cid:12) ¯f k
t (z1, γ, dt)(y) ± ˆf k
≤ E[(cid:12)
t (z1, γ, dt)(y) − ˆf k
(cid:12) ¯f k
+ (cid:12)
(cid:12) ˆf k
t (z1, γ)(y) − ˆf k
(cid:88)
1 (x)1(f k
zk

t (z2, γ)(y)(cid:12)
(cid:12)
t (z1, γ)(y) − ˆf k
t (z1, γ)(y)(cid:12)
(cid:12)]
t (z2, γ)(y)(cid:12)
(cid:12)
t (x, γk(x), φ(z1, γ), w) = y)

t (z2, γ)(y)(cid:12)
(cid:12)

≤ E(cid:2)(cid:12)
(cid:12)

(cid:88)

(b)

(cid:3)

w∈W k

× (wk
(cid:88)

x∈X k
t (w) − PW k
1 (x)E[|wk
zk

t

(w))(cid:12)
(cid:12)

(cid:3) + H 3

t (w) − PW k

t

t (cid:107)z1 − z2(cid:107)
(w)|] + H 3

t (cid:107)z1 − z2(cid:107)

(cid:88)

(c)
≤

w∈W k
1√

(d)
≤ O(

x∈X k

|N k|

) + H 3

t (cid:107)z1 − z2(cid:107),

where (a) follows from the triangle inequality and the
monotonicity of the expectation operator; (b) follows from
equations (4), (10), (12) and Lemma 2; (c) follows from
t (x, γk(x), φ(z1, γ), w) = y) ≤ 1, the triangle inequality
1(f k
and the monotonicity of the expectation operator, and (d)
from Lemma 3, zk
1 (x) ≤ 1, ∀x ∈ X k, and the fact that the
cardinality of spaces X k and W k does not depend on |N k|.

APPENDIX E
PROOF OF LEMMA 5

We ﬁrst show that for any z ∈ I, γ ∈ G, wk

t ∈ M|N k|(W k)

and y ∈ X k at time t ∈ NT , ¯f k

t (z, γ, wk

t )(y) is equal to:

(cid:88)

(cid:88)

w∈W k

x∈X k

zk(x)1(f k

t (x, γk(x), φ(z, γ), w) = y)wk

t (w)

(a)
=

×

(cid:88)

(cid:88)

zk(x)1(f k

t (x, γk(x), φ(z, γ), w) = y)

x∈X k

w∈ ˘W k
wk
PW k

t (w)
(w)

t

PW k

t

(w)

≤ max
w∈ ˘W k

(

wk
PW k

t (w)
(w)

(cid:88)

(cid:88)

)

zk(x)

t

w∈ ˘W k
t (x, γk(x), φ(z, γ), w) = y)PW k

x∈X k

× 1(f k

(w)

t

(b)
= max
w∈ ˘W k

(

wk
PW k

t (w)
(w)

t

) ˆf k

t (z, γ)(y),

(69)

where in (a) set ˘W k := W k\{w ∈ W k|PW k
(w) = 0} con-
tains all realizations that have non-zero probability measures
(this equality holds a.s.), and (b) follows from (4), (9), (12)
and (19). By similar argument, for any y ∈ X k, we have

t

¯f k
t (z, γ, wk

t )(y) ≥ min
w∈ ˘W k

(

wk
PW k

t (w)
(w)

t

) ˆf k

t (z, γ)(y).

(70)

16

E(cid:107) ¯f k

From (69) and (70), it results that for any z1, z2 ∈ I and
γ ∈ G at time t ∈ NT ,
t ) − ¯f k
t (z1, γ, wk
wk
t (w)
(w)
PW k
wk
PW k

ˆf k
t (z1, γ)− min
w∈ ˘W k

ˆf k
t (z1, γ)−max
w∈ ˘W k

(c)
≤ E(cid:107) max
w∈ ˘W k

≤ E(cid:107) max
w∈ ˘W k

ˆf k
t (z2, γ)(cid:107)

ˆf k
t (z2, γ)(cid:107)

t (z2, γ, wk

t (w)
(w)

t (w)
(w)

t (w)
(w)

t )(cid:107)

t

t

+ E(cid:107)( max
w∈ ˘W k
wk
PW k

= E[ max
w∈ ˘W k

wk
PW k

t (w)
(w)

t

− min
w∈ ˘W k

) ˆf k

t (z2, γ)(cid:107)

t (z1, γ) − ˆf k

t (z2, γ)(cid:107)

t

wk
PW k
wk
PW k
wk
PW k

t

t

t (w)
(w)

(d)

](cid:107) ˆf k

t (w)
(w)

wk
PW k
t (cid:107)z1 − z2(cid:107),

= (cid:107) ˆf k

t (z2, γ)(cid:107)

+ E[ max
w∈ ˘W k
t (z1, γ) − ˆf k

− min
w∈ ˘W k
≤ H 3
where (c) follows from the triangle inequality and the mono-
tonicity of the expectation operator, and (d) holds for every
w ∈ ˘W k, E[ wk
PW k
t
and (e) follows from Lemma 2.

i∈N k 1(wi
|N k|PW k
t

|N k|PW k
t
|N k|PW k
t

(w)
(w) = 1,

t=w)]
(w)

(w) ] =

t (w)

E[(cid:80)

=

t

t

](cid:107) ˆf k

t (w)
(w)
wk
t (w)
(w)
PW k
t (z2, γ)(cid:107)(e)

t

APPENDIX F
PROOF OF LEMMA 6
The proof proceeds by backward induction. At t = T ,

(cid:96)T (z1, γ1)
(cid:12)(cid:96)T (z1, γ1) − (cid:96)T (z2, γ1)(cid:12)
(cid:0)(cid:12)
T (cid:107)z1 − z2(cid:107) + (cid:96)T (z2, γ1)(cid:1)
(cid:0)H 4

T (z1) = min
V 1
γ1∈G
(a)
≤ min
γ1∈G
(b)
≤ min
γ1∈G
= H 4
T (cid:107)z1 − z2(cid:107) + min
γ2∈G
T (z2),
T (cid:107)z1 − z2(cid:107) + V 2
= H 4

(cid:96)T (z2, γ2)

(d)

(c)

(cid:12) + (cid:96)T (z2, γ1)(cid:1)

where (a) follows from the triangle inequality, the fact that
(cid:96)T (·) ∈ R≥0, and the monotonicity of the minimum operator;
(b) follows from Lemma 2 and the monotonicity of the
minimum operator; (c) follows from the fact that the space
of minimization G is the same for both γ1 and γ2, and (d)
follows from (44). Therefore, (46) holds at t = T , where
H 5
T = H 4
T = 0. Now, assume that (46) holds at time
t+1(z2)| ≤ H 5
t + 1, i.e., |V 1
t+1∆.
It is desired to prove (46) for time t. To this end, the following
relations are derived:

t+1(cid:107)z1 − z2(cid:107) + H 6

t+1(z1) − V 2

T and H 6

t (z1) = min
V 1
γ1∈G

t (z1, γ1, dt))](cid:1)

(cid:0)(cid:96)t(z1, γ1) + E[V 1
t+1(f 1
(cid:16)(cid:12)
(cid:12)(cid:96)t(z1, γ1) − (cid:96)t(z2, γ1)(cid:12)
(e)
≤ min
(cid:12)
γ1∈G
+ E[(cid:12)
t (z1, γ1, dt)) − V 2
t+1(f 1
(cid:12)V 1
t+1(f 2
+ (cid:96)t(z2, γ1) + E[V 2

t+1(f 2
(cid:17)
t (z2, γ1, dt))]

t (z2, γ1, dt))(cid:12)
(cid:12)]

(f )

≤ (H 4

t+1H z
t + H 5
(cid:16)
(cid:96)t(z2, γ2) + E[V 2

t )(cid:107)z1 − z2(cid:107) + (H 5
t+1(f 2

t+1 + H 6
t (z2, γ2, dt))]

t+1)∆
(cid:17)

+ min
γ2∈G
t (cid:107)z1 − z2(cid:107) + H 6

(g)

= H 5

t ∆ + V 2

t (z2),

17

where (e) follows from the triangle inequality, the fact that
(cid:96)t(·), V j
t+1(·) ∈ R≥0, j ∈ {1, 2}, and the monotonicity of
the expectation and minimum operators; (f ) follows from
Lemma 2, equations (45), the induction assumption at time
t + 1,
the monotonicity of the expectation and minimum
operators, and the fact that the space G is the same for both
γ1 and γ2, and (g) follows from (44) and (47).

Jalal Arabneydi received the Ph.D. degree in Elec-
trical and Computer Engineering from McGill Uni-
versity, Montreal, Canada in 2017. He is currently
a postdoctoral fellow at Concordia University. He
was the recipient of the best student paper award
at the 53rd Conference on Decision and Control
(CDC), 2014. His principal research interests include
stochastic control, robust optimization, game theory,
large-scale system, multi-agent reinforcement learn-
ing with applications in complex networks including
smart grids, swarm robotics, and ﬁnance. His current
research interest is focused on what he calls deep planning, which bridges
decision making theory and artiﬁcial intelligence. The ultimate goal is to
deﬁne proper mathematical tools and solution concepts in order to develop
large-scale decision-making algorithms that work under imperfect information
and incomplete knowledge with analytical performance guarantees.

Amir G. Aghdam received the Ph.D. degree in elec-
trical and computer engineering from the University
of Toronto, Toronto, ON, Canada, in 2000, and is
currently a Professor in the Department of Electrical
and Computer Engineering at Concordia University,
Montreal, Canada. He is a member of Professional
Engineers Ontario, chair of the Conference Editorial
Board of IEEE Control Systems Society, Editor-in-
Chief of the IEEE Systems Journal, and has served
as an Associate Editor of the IEEE Transactions
on Control Systems Technology, IEEE Access, IET
Control Theory & Applications, and the Canadian Journal of Electrical and
Computer Engineering. He has been a member of the Technical Program
Committee of a number of conferences, including the IEEE Conference on
Systems, Man and Cybernetics (IEEE SMC) and the IEEE Multiconference
on Systems and Control (IEEE MSC). He has served as a member of
the Review Panel/Committee for the NSF, Italian Research and University
Evaluation Agency (ANVUR), Innovation Fund Denmark Projects, and the
Natural Sciences and Engineering Research Council of Canada (NSERC) ECE
Evaluation Group. Dr. Aghdam was the 2014–2015 President of IEEE Canada
and Director (Region 7), IEEE, Inc., and was also a member of the IEEE
Awards Board for this period. He was a Visiting Scholar at Harvard University
in fall 2015, and was an Associate at the Harvard John A. Paulson School
of Engineering and Applied Sciences from September 2015 to December
2016. His research interests include multi-agent networks, distributed control,
optimization and sampled-data systems. Prof. Aghdam is a recipient of the
2009 IEEE MGA Achievement Award, and is currently a member of the
IEEE Medal of Honor Committee Award and 2020 IEEE Canada J. M. Ham
Outstanding Engineering Educator Award. Dr. Aghdam was a member of the
IEEE Medal of Honor Committee for 2017-2019 and is currently the Vice-
Chair of the IEEE Medals Council.

PARTIALLY EXCHANGEABLE AGENTS [12, CHAPTER 2]

18

Consider a discrete-time control system consisting of a
ﬁnite population of agents, where agents are partitioned into
K ∈ N disjoint sub-populations. Denote by K the set of sub-
populations, by N k the agents of sub-population k ∈ K, and
by N the entire population of agents; note that N = ∪k∈KN k.
Given the control horizon T ∈ N, let the state, action, and the
noise of agent i ∈ N k of sub-population k ∈ K at time t ∈ NT
be denoted by xi
t ∈ W k, respectively.
For the entire population, the joint state, joint action, and
joint noise are analogously denoted by xt = (xi
t)i∈N ∈ X ,
ut = (ui
t)i∈N ∈ W at time t ∈ NT .
the initial state of agent i of sub-
1 ∈ X k, and at time t ∈ NT

t)i∈N ∈ U, and wt = (wi

For any k ∈ K,

t ∈ U k, and wi

t ∈ X k, ui

population k is denoted by xi
its state evolves as follows:

t+1 = f i
xi

t (xt, ut, wi

t),

(71)

where f i
: X × U × W k → X k describes the dynamics
t
of agent i at time t ∈ NT . The primitive random variables
{x1, w1, . . . , wT } are deﬁned on a common probability space.
In short, the dynamics of the entire population may be ex-
pressed in an augmented form as:

xt+1 = ft(xt, ut, wt).

A per-step cost ct(xt, ut) : X × U → R≥0 is incurred at
any time t ∈ NT , which reﬂects the desirable behaviour of
the system at time t. Let I i
t ⊆ {x1:t, u1:t−1} denote the
information of agent i at time t ∈ NT for any i ∈ N . In
particular, the action of agent i at time t is given by

t = gi
ui

t(I i

t ),

where deterministic function gi
agent i at
{(gi
t)i∈N }T
control performance is described by

t is called the control law of
laws g :=
t=1 is deﬁned as the strategy of the system and the

time t ∈ NT . The set of control

J(g) = Eg(cid:2)

T
(cid:88)

ct(xt, ut)(cid:3),

t=1
where the expectation is taken with respect to the probability
measures induced by the choice of the strategy g.

Deﬁnition 4 (Partially exchangeable agents). The multi-
agent system described above is said to be a system with
partially exchangeable agents if the following two conditions
hold for any pair of agents (i, j) ∈ N k of any sub-population
k ∈ K at any time t ∈ NT :

1) σi,j

(cid:0)ft(xt, ut, wt)(cid:1) = ft

(cid:0)σi,j(xt), σi,j(ut), σi,j(wt)(cid:1),
i.e., exchanging agents i and j has no effect on the
system dynamics.

2) ct(xt, ut) = ct(σi,j(xt), σi,j(ut)),

i.e., exchanging

agents i and j does not impact the cost.

Remark 13. Note that the notion of exchangeable agents is
different from the notion of exchangeable random variables
often used in the probability literature (e.g., de Finetti’s
Theorem). See a counterexample in [12, Chapter 2] where
it is illustrated that neither is necessarily implied by the other.

Fig. 4. Partially exchangeable agents wherein the system is invariant to
permutation in each sub-population [12, Chapter 2].

A. Empirical distribution

Let b = (b1, . . . , bn) denote a vector of n ∈ N samples from
set B := {a1, . . . , a|B|}, where bi ∈ B, i ∈ Nn. The empirical
distribution function ξ : (cid:81)n
i=1 B → En(B) is deﬁned as a
real-valued vector of size |B| such that

ξ(b)(aj) =

1
n

n
(cid:88)

i=1

1(bi = aj),

j ∈ N|B|.

Lemma 9. Let h denote an arbitrary exchangeable function
over the product space (cid:81)n
i=1 B. Then, there exists a function ¯h
such that h(b) = ¯h(ξ(b)).
Proof. Deﬁne vector s ∈ (cid:81)n
i=1 B such that its i-th element,
i ∈ Nn, is equal to aj ∈ B, where index j ∈ N|B| satisﬁes
the following inequalities: 1(j (cid:54)= 1)n (cid:80)j−1
l=1 ξ(b)(al) < i ≤
n (cid:80)j
l=1 ξ(b)(al). As a result, vector s is an exchanged version
of vector b and it is completely determined by ξ(b). Since
h(b) is exchangeable, we get h(b) = h(s) =: ¯h(ξ (b)). (cid:4)

B. Structure of deep teems with partially exchangeable agents
The short-hand notation σ(x) is used to denote any arbitrary

permuted version of vector x.

Proposition 3. For any controlled Markov chain system with
partially exchangeable agents described in Deﬁnition 4, there
exist functions (f k
t )k∈K and ¯ct, t ∈ NT , such that the dynamics
of agent i of sub-population k ∈ K can be written as

t+1 = f k
xi

t (xi

t, ui

t, Dt, wi

t),

and the per-step cost at time t ∈ NT , can be expressed as

¯ct(Dt) = ct(xt, ut).

t

and u−i

Proof. Consider agent i ∈ N k of sub-population k ∈ K. Let
x−i
respectively denote the joint state xt and joint
t
action ut of all agents except agent i. The dynamics given
by (71) can be rewritten as follows:
t, x−i
t

t, ui
Let σ−i denote any arbitrary permutation of the entire pop-
ulation excluding agent i, where the permutation is allowed
only within the sub-populations. According to condition 1 of
Deﬁnition 4, arbitrarily permuting the state and action of other
agents will not change the dynamics of agent i ∈ N k. Thus,

t+1 = f i
xi

, u−i
t

t (xi

, wi

t).

t (xi
f i

t, ui

t, x−i
t

, u−i
t

, wi

t) = f i

t (xi

t, ui

t, σ−i(x−i
t

, u−i

t ), wi

t).

By applying Lemma 9 to all sub-populations successively, it
results that there exists a function ¯f i such that

19

t, ui

t, ui

t (xi

t) = ¯f i

, u−i

, u−i
t

t (xi
f i

t, x−i
t

t ), wi

t, ξ(x−i
, wi
t),
(72)
t
where, by a slight abuse of notation, ξ (cid:0)x−i
(cid:1) denotes a
vector consisting of the empirical distributions of states and
actions of agents, except agent i, in all sub-populations. Note
that ξ (cid:0)x−i
(cid:1) can be identiﬁed by ξ(xt, ut) = Dt and
t). Thus, from equation (72) there exists a function ˜ft
(xi
t, ui
such that

, u−i
t

, u−i
t

t

t

t (xi
f i

t, ui

t, x−i
t

, u−i
t

, wi

t) = ˜f i

t (xi

t, ui

t, Dt, wi

t).

Now, consider two arbitrary agents i and j of sub-population k.
From condition 1 of Deﬁnition 4, we have

t+1 = ˜f i
xi

t (xi

t, ui

t, Dt, wi

t) = ˜f j

t (xi

t, ui

t, Dt, wi

t),

and

t+1 = ˜f j
xj
t = ˜f j

t , Dt, wj
t , uj
t (xj
t , ∀i, j ∈ N k.
t =: f k

t ) = ˜f i

Hence, ˜f i

t (xj

t , uj

t , Dt, wj

t ).

A similar argument applies to the per-step cost. Let σ denote
any arbitrary permutation of the entire population, where
the permutation is allowed only within the sub-populations.
Condition 2 of Deﬁnition 4, implies (directly) that arbitrarily
permuting the agents of sub-population k ∈ K does not change
the cost. Thus, by applying the result of Lemma 9 to all sub-
populations k ∈ K, there exists a function ¯ct such that

ct(xt, ut) = ct(σ(xt, ut)) =: ¯ct(ξ(xt, ut)) = ¯ct(Dt).

(cid:4)

