2
2
0
2

b
e
F
2
2

]

C
O
.
h
t
a
m

[

2
v
2
0
5
1
1
.
9
0
1
2
:
v
i
X
r
a

Inequality Constrained Stochastic Nonlinear Optimization via
Active-Set Sequential Quadratic Programming

Sen Na1,2, Mihai Anitescu3, and Mladen Kolar4

1Department of Statistics, University of California, Berkeley
2International Computer Science Institute
3Mathematics and Computer Science Division, Argonne National Laboratory
4Booth School of Business, The University of Chicago

Abstract

We study nonlinear optimization problems with a stochastic objective and deterministic equality
and inequality constraints, which emerge in numerous applications including ﬁnance, manufacturing,
power systems and, recently, deep neural networks. We propose an active-set stochastic sequential
quadratic programming algorithm that uses a diﬀerentiable exact augmented Lagrangian as the merit
function. The algorithm adaptively selects the penalty parameters of the augmented Lagrangian,
and performs stochastic line search to decide the stepsize. The global convergence is established:
for any initialization, the “liminf” of the KKT residuals converges to zero almost surely. Our
algorithm and analysis further develop the work of Na et al. Na et al. (2021) by allowing nonlinear
inequality constraints without requiring the strict complementary condition. We demonstrate the
performance of the algorithm on a subset of nonlinear problems collected in CUTEst test set.

1

Introduction

We study stochastic nonlinear optimization problems with deterministic equality and inequality
constraints:

f pxq “ Erf px; ξqs,

min
xPRd
s.t. cpxq “ 0,
gpxq ď 0,

(1)

where f : Rd Ñ R is a stochastic objective, c : Rd Ñ Rm are deterministic equality constraints,
g : Rd Ñ Rr are deterministic inequality constraints, and ξ „ P is a random variable following the
distribution P. With slight abuse of the notation, we use f p¨ ; ξq to denote a realization of f . In
stochastic optimization regime, the direct evaluation of f and its derivatives is not accessible. Instead,
it is assumed that one can generate independent and identically distributed samples tξiui from P,
and estimate f and its derivatives based on the realizations tf p¨ ; ξiqui.

Problem (1) widely appears in a variety of industrial applications including ﬁnance, transporta-
tion, manufacturing, and power systems (Birge, 1997; Silvapulle, 2004). For example, Cleef and
Gual (1982) studied a project scheduling problem; Morton (2003) studied a newsvendor problem;

1

 
 
 
 
 
 
and Morton and Popova (2004) studied an employee scheduling problem. These classical industrial
problems can all be cast as (1). Problem (1) also includes constrained empirical risk minimization
(ERM) as a special case, where P can be regarded as a uniform distribution over n data points
tξi “ pyi, ziqun
i“1, with pyi, ziq being the feature-outcome pairs. Thus, the objective has a ﬁnite-sum
form as
nÿ

nÿ

f pxq “

1
n

f px; ξiq “

i“1

f px; yi, ziq.

i“1

1
n

The goal of (1) is to ﬁnd the optimal parameter x‹ that ﬁts the data best. One of the most common
choices of f is the negative log-likelihood of the underlying distribution of pyi, ziq. In this case, the
optimizer x‹ is called the maximum likelihood estimator (MLE). Constraints on parameters are also
common in practice, which are used to encode prior model knowledge or to restrict model complexity.
For example, Liew (1976a,b) studied inequality constrained least-squares problems, where inequality
constraints maintain structural consistency such as non-negativity of the elasticities. Phillips (1991);
Onuk et al. (2015) studied statistical properties of constrained MLE, where constraints characterize
the parameters space of interest. More recently, a growing literature on training constrained neural
networks has been reported (Goh et al., 2018; Chen et al., 2018; Livieris and Pintelas, 2019a,b),
where constraints are imposed to avoid weights either vanishing or exploding, and objectives are in
the ﬁnite-sum form.

This paper aims to develop a numerical procedure to solve (1) with a global convergence guar-
antee. When the objective f is deterministic, numerous nonlinear optimization methods with
well-understood convergence results are applicable, such as exact penalty method, augmented La-
grangian method, sequential quadratic programming (SQP), and interior point method (Nocedal
and Wright, 2006). However, methods to solve constrained stochastic nonlinear problems with satis-
factory convergence guarantees have been developed only recently. In particular, with only equality
constraints, Berahas et al. (2021c) designed a very ﬁrst stochastic SQP (StoSQP) scheme using an
(cid:96)1-penalized merit function, and showed that for any initialization, the KKT residuals tRtut converge
in two diﬀerent regimes, determined by a prespeciﬁed deterministic stepsize-related sequence tαtut:
αpt`1q ` Υα for

(a) (constant sequence) if αt “ α for some small α ą 0, then 1
t`1

i s ď Υ

ř
t
i“0

ErR2

some Υ ą 0;

ř

ř

8
t“0 α2

t ă 8, then lim inf tÑ8 ErR2

(b) (decaying sequence) if αt satisﬁes

8
t“0 αt “ 8 and
t s “ 0.
Both convergence regimes are well known for unconstrained stochastic problems where Rt “ }∇f pxtq}
(see Bottou et al. (2018) for a recent review), while Berahas et al. (2021c) generalized the results
to equality constrained problems. Within the algorithm of Berahas et al. (2021c), the authors
designed a stepsize selection scheme (based on the prespeciﬁed deterministic sequence tαtut) to bring
some sort of adaptivity into the algorithm. However, it turns out that the prespeciﬁed sequence,
which can be overestimated or underestimated, still highly aﬀects the performance. To address the
adaptivity issue, Na et al. (2021) proposed an alternative StoSQP, which exploits a diﬀerentiable
exact augmented Lagrangian merit function, and enables a stochastic line search procedure to
adaptively select the stepsize. Under a diﬀerent setup (where model is precisely estimated with high
probability), Na et al. (2021) proved a diﬀerent guarantee: for any initialization, lim inf tÑ8 Rt “ 0
almost surely. Subsequently, a series of extensions have been reported. Berahas et al. (2021b)
developed a StoSQP scheme to deal with rank-deﬁcient constraints; Curtis et al. (2021b) studied
StoSQP with inexact Newton directions; Oztoprak et al. (2021) studied a deterministic SQP where
objective and constraints are evaluated with noise; and Curtis et al. (2021a) studied the worst-case

2

iteration complexity of a StoSQP. However, all aforementioned works do not allow inequality
constraints.

Our paper develops this line of research by designing a method that works with nonlinear
inequality constraints. In order to do so, we have to overcome a number of intrinsic diﬃculties
that arise in dealing with inequality constraints, which were already noted in classical nonlinear
optimization literature (Bertsekas, 1982; Nocedal and Wright, 2006). Our work is built upon Na
et al. (2021), where we exploit an augmented Lagrangian merit function under the SQP framework.
However, the analysis of this paper is more involved. To generalize Na et al. (2021) to allow
inequality constraints, we have to address the following two subtleties.

(a) With inequalities, SQP subproblems are inequality constrained (nonconvex) quadratic programs
(IQPs), which themselves are diﬃcult to solve in most cases. Some SQP literature (e.g. Boggs
and Tolle (1995)) supposes to apply a QP solver to solve IQPs exactly, however, a practical
scheme should embed a ﬁnite number of inner loop iterations of active-set method or interior
point method into the main SQP loop, to solve IQPs approximately. Then, the inner loop
may lead to an approximation error for search direction in each iteration, which complicates
the analysis.

(b) When applied to deterministic objectives with inequalities, the SQP search direction is a
descent direction of the augmented Lagrangian only in a neighborhood of a KKT point (Pillo
and Lucidi, 2002, Propositions 8.3, 8.4). This is in contrast to equality constrained problems,
where the descent property of the SQP search direction is ensured globally, provided the
penalty parameters of the augmented Lagrangian are suitably chosen. Such a diﬀerence is
indeed brought by inequality constraints: to make the search direction produced by SQP
informative, the estimated active set has to be close to the optimal active set (see Lemma 3.7
for details). Thus, simply changing the merit function in Na et al. (2021) does not work for
Problem (1).

The existing literature on inequality constrained SQP has addressed (a) and (b) via various tools for
deterministic objectives (Kanzow, 2001), while we provide new insights into stochastic objectives.
In particular, to resolve (a), we design an active-set StoSQP scheme. Given the current iterate, we
ﬁrst identify an active set, which includes all inequality constraints that are likely to be equalities.
Then, we derive the search direction by solving a SQP subproblem, where we include all inequality
constraints in the identiﬁed active set but regard them as equalities. In this case, the subproblem is
an equality constrained QP (EQP), and can be solved exactly provided the matrix factorization is
within the computational budget. To resolve (b), we provide a back up direction to the scheme. In
each iteration, we check if the SQP subproblem is solvable and generates a descent direction of the
augmented Lagrangian. If yes, we maintain the SQP direction as it typically enjoys a fast local
rate; if no, we switch to performing one regularized Newton step (or simply one steepest descent
step) of the augmented Lagrangian, which still decreases the augmented Lagrangian, although the
convergence is not as eﬀective as that of SQP.

Furthermore, to develop a procedure that can adaptively select the penalty parameters and
stepsizes, additional challenges need to be addressed. In particular, there are unknown deterministic
thresholds for penalty parameters to ensure the one-to-one correspondence between a stationary
point of augmented Lagrangian merit function and a KKT point of Problem (1). Due to the
stochasticity of iterates, the stabilized penalty parameters are random; and we are unsure if the
stabilized values are above (or below, depending on the deﬁnition) of the thresholds in each run.
Thus, we cannot directly conclude that the iterates converge to a KKT point, even if we ensure a

3

suﬃcient decrease on augmented Lagrangian in each step, and enforce the iterates to converge to
one of its stationary points. This issue has also been observed for the (cid:96)1-penalized merit function
for stochastic problems (Berahas et al., 2021c, Section 3.2.1). Na et al. (2021) resolved this issue by
modifying the SQP scheme when selecting the penalty parameters. We generalize that technique
to inequality constraints. We also select the stepsize by stochastic line search, which improves
algorithm’s adaptivity and gets rid of the prespeciﬁed deterministic (stepsize-related) sequences
that fully determine the algorithm convergence behavior. With all above components, we ﬁnally
prove that the KKT residual Rt satisﬁes lim inf tÑ8 Rt “ 0 almost surely for any initialization. This
result matches Paquette and Scheinberg (2020) for unconstrained problems and Na et al. (2021) for
equality constrained problems; while, as introduced before, is diﬀerent from the convergence of the
expected KKT residual ErR2

t s established in Berahas et al. (2021c,b); Curtis et al. (2021b).

Related work. A number of methods have been proposed to optimize stochastic objectives without
constraints, varying from ﬁrst-order methods to second-order methods (Bottou et al., 2018). For all
methods, adaptively choosing the stepsize is particularly important for practical deployment. A line
of literature selects the stepsize by adaptively controlling the batch size and embedding natural
(stochastic) line search into the schemes (Friedlander and Schmidt, 2012; Byrd et al., 2012; Kreji´c and
Krklec, 2013; De et al., 2017; Bollapragada et al., 2018). Although empirical experiments suggest the
validity of stochastic line search, a rigorous analysis is missing. Until recently, researchers revisited
unconstrained stochastic optimization via the lens of classical nonlinear optimization methods, and
have been able to show promising convergence guarantees. In particular, Bandeira et al. (2014);
Chen et al. (2017); Gratton et al. (2017); Blanchet et al. (2019) studied stochastic trust region
method, and Cartis and Scheinberg (2017); di Seraﬁno et al. (2020); Paquette and Scheinberg (2020);
Berahas et al. (2021a) studied stochastic line search method. Moreover, Berahas et al. (2021c);
Na et al. (2021); Berahas et al. (2021b); Curtis et al. (2021b) designed diﬀerent StoSQP schemes
to solve equality constrained stochastic problems. Our paper contributes to this line of works by
designing an active-set StoSQP scheme to handle inequality constraints, which enable much wider
and more realistic applications. Same as Na et al. (2021) and diﬀerent from Berahas et al. (2021c,b);
Curtis et al. (2021b), we embed stochastic line search into SQP scheme. Our analysis for inequalities
does not require the strict complementary condition, which is often imposed to apply (squared)
slack variables to transfer nonlinear inequality constraints into other forms (Zavala and Anitescu,
2014; Fukuda and Fukushima, 2017).

With constraints, nonlinear problems with deterministic objectives can be solved with numerous
methods. See Nocedal and Wright (2006); Gill et al. (2005) and references therein. Our method
is based on sequential quadratic programming (SQP). Within SQP schemes, an exact penalty
function is used as the merit function to monitor the progress of the iterates towards a KKT point.
We exploit an exact augmented Lagrangian merit function, which was ﬁrst proposed for equality
constrained problems by Pillo and Grippo (1979); Pillo et al. (1980), and then extended to inequality
constrained problems by Pillo and Grippo (1982, 1985). Pillo and Lucidi (2002) further improved
this series of works by designing a new augmented Lagrangian, and established the exact property
under weaker conditions. Although not crucial for that exact property analysis, Pillo and Lucidi
(2002) did not include equality constraints. In this paper, we enhance augmented Lagrangian in
Pillo and Lucidi (2002) by considering both equality and inequality constraints; and study the case
where the objective is stochastic, so that all quantities associated to the objective are accessed with
random noise.

Structure of the paper. We introduce the augmented Lagrangian merit function and the active-set

4

SQP in Section 2. To motivate our algorithm, we ﬁrst design a non-practical, non-adaptive StoSQP
scheme in Section 3, and then enhance this scheme by designing a practical, adaptive scheme in
Section 4. The experiments and conclusions are presented in Sections 5 and 6.

Notation. We use boldface letter to denote column vectors, and use t to denote iteration index.
For scalars, t is used as subscript; while for vectors and matrices, t is used as superscript. By } ¨ } we
denote (cid:96)2 norm for vectors and spectrum norm for matrices. For two scalars a, b, a ^ b “ minta, bu
and a _ b “ maxta, bu. For two vectors a, b with the same dimension, minta, bu is a vector by taking
entrywise minimum. For a P Rr, diagpaq P Rrˆr is a diagonal matrix whose diagonal entries are
speciﬁed by a sequentially. We use I to denote the identity matrix whose dimension is clear from the
context. For an index set A Ď t1, 2, . . . , ru and a vector a P Rr (or a matrix A P Rrˆd), aA P R|A|
(or AA P R|A|ˆd) is a sub-vector (or a sub-matrix) including only the indices in A; ΠAp¨q : Rr Ñ Rr
(or Rrˆd Ñ Rrˆd) is a projection operator with rΠApaqsi “ ai if i P A and rΠApaqsi “ 0 if i R A (for
A P Rrˆd, ΠApAq is applied column-wise); Ac “ t1, 2, . . . , ruzA. Finally, we reserve the notation
for the Jacobian matrices of constraints: Jpxq “ ∇T cpxq “ p∇c1pxq, . . . , ∇cmpxqqT P Rmˆd and
Gpxq “ ∇T gpxq “ p∇g1pxq, . . . , ∇grpxqqT P Rrˆd.

2 Preliminaries

The Lagrangian function of (1) is

We denote by

the feasible set and

Lpx, µ, λq “ f pxq ` µT cpxq ` λT gpxq.

Ω “ tx P Rd : cpxq “ 0, gpxq ď 0u

Ipxq “ ti : 1 ď i ď r, gipxq “ 0u

(2)

(3)

the active set. We aim to ﬁnd a KKT point px‹, µ‹, λ‹q of (1) satisfying

∇xLpx‹, µ‹, λ‹q “ 0,

cpx‹q “ 0,

gpx‹q ď 0,

λ‹ ě 0,

pλ‹qT gpx‹q “ 0.

(4)

When a constraint qualiﬁcation holds, existing a dual pair pµ‹, λ‹q to satisfy (4) is a ﬁrst-order
necessary condition for x‹ to be a local solution of (1). In most cases, it is diﬃcult to have an
initial iterate that satisﬁes all inequality constraints, and enforce inequality constraints to hold as
the iteration proceeds. This motivates us to consider the perturbed set. For ν ą 0, we let

Ω Ĺ Tν :“

!
x P Rd : apxq ď

)

ν
2

where apxq “

rÿ

i“1

maxtgipxq, 0u3.

We also deﬁne the function

qνpx, λq “

aνpxq
1 ` }λ}2

with aνpxq “ ν ´ apxq.

(5)

(6)

Here, ν ą 0 is a parameter to be chosen: given the current primal iterate xt, we choose ν “ νt large
enough so that xt P Tν. Note that while it is diﬃcult to have xt P Ω, it is easy to choose ν to have
xt P Tν. The denominator 1 ` }λ}2 of qνpx, λq penalizes the magnitude of λ. It is easy to see that

ν
2p1 ` }λ}2q

ď qνpx, λq ď ν @px, λq P Tν ˆ Rr,

and qνpx, λq Ñ 0 as }λ} Ñ 8.

5

With (6) and a parameter (cid:15) ą 0, we deﬁne a function to measure the dual feasibility of inequality
constraints:

w(cid:15),νpx, λq :“ gpxq ´ b(cid:15),νpx, λq

:“ gpxq ´ mint0, gpxq ` (cid:15)qνpx, λqλu “ maxtgpxq, ´(cid:15)qνpx, λqλu.

(7)

The following lemma justiﬁes the reasonability of the deﬁnition (7). The proof is immediate and
omitted.
Lemma 2.1. Let (cid:15), ν ą 0. For any px, λq P Tν ˆRr, w(cid:15),νpx, λq “ 0 ô gpxq ď 0, λ ě 0, λT gpxq “ 0.
An implication of Lemma 2.1 is that, when the iteration sequence converges to a KKT point,
w(cid:15),νpx, λq converges to 0, i.e., gpxq “ b(cid:15),νpx, λq. This motivates us to deﬁne the following augmented
Lagrangian for (1):

L(cid:15),ν,ηpx, µ, λq “ Lpx, µ, λq `

1
2(cid:15)

}cpxq}2 `

1
2(cid:15)qνpx, λq
ˆ
η
2

›
›
›
›

`

`

}gpxq}2 ´ }b(cid:15),νpx, λq}2

˘

Jpxq∇xLpx, µ, λq
Gpxq∇xLpx, µ, λq ` diag2pgpxqqλ

˙›
›
2
›
›

,

(8)

where η ą 0 is a prespeciﬁed parameter, which can be any positive number throughout the paper.
The augmented Lagrangian (8) generalizes the one in Pillo and Lucidi (2002) by including equality
constraints and introducing η to enhance ﬂexibility (η “ 2 in Pillo and Lucidi (2002)). Without
inequalities, (8) reduces to the augmented Lagrangian that is adopted for designing the StoSQP
scheme in Na et al. (2021). The penalty in (8) consists of two parts. The ﬁrst part characterizes
the feasibility error and consists of }cpxq}2 and }gpxq}2 ´ }b(cid:15),νpx, λq}2. The latter term is rescaled
by 1{qνpx, λq to penalize the large magnitude of λ. In fact, if }λ} Ñ 8, then qνpx, λqλ Ñ 0 so
that b(cid:15),νpx, λq Ñ mint0, gpxqu (cf. (7)). Thus, the penalty p}gpxq}2 ´ }b(cid:15)px, λq}2q{qνpx, λq Ñ 8,
which is impossible when we generate iterates to decrease L(cid:15),ν,η. The second part characterizes the
optimality error and does not depend on the parameters (cid:15), ν.

The exact property of (8) can be studied similarly as in Pillo and Lucidi (2002), however this is
incremental and not crucial for our analysis. We will only use (a stochastic version of) (8) to monitor
the progress of the iterates. By direct calculation, we obtain the gradients ∇L(cid:15),ν,η. We suppress the
evaluation point for conciseness, and deﬁne the following matrices

Q11 “p∇2

xLqJ T ,

Q12 “

mÿ

p∇2ciqp∇xLqeT

i,m,

Q1 “ Q11 ` Q12 P Rdˆm,

Q21 “p∇2

xLqGT ,

Q22 “

i“1
rÿ

p∇2giqp∇xLqeT

i,r,

Q23 “ 2GT diagpgqdiagpλq,

(9)

i“1

´

¯

´

¯

Q2i P Rdˆr,

M “

M11 M12
M21 M22

“

JJ T
GJ T GGT `diag2pgq

JGT

P Rpm`rqˆpm`rq,

Q2 “

3ÿ

i“1

where ei,m P Rm is the i-th canonical basis of Rm (similar for ei,r P Rr). Then,

∇xL(cid:15),ν,η “∇xL ` η

˜

ˆ

∇µL(cid:15),ν,η
∇λL(cid:15),ν,η

˙

“

c
w(cid:15),ν ` }w(cid:15),ν }2

(cid:15)aν

λ

ˆ

`

˘

Q1 Q2
¸

J∇xL
G∇xL ` diag2pgqλ
˙ ˆ

ˆ

˙

`

1
(cid:15)

J T c `

1
(cid:15)qν

GT w(cid:15),ν `

3}w(cid:15),ν}2
2(cid:15)qνaν

GT l,

(10)

˙

,

` η

M11 M12
M21 M22

J∇xL
G∇xL ` diag2pgqλ

6

where l “ lpxq “ diagpmaxtgpxq, 0uq maxtgpxq, 0u. The evaluation of ∇L(cid:15),ν,η requires ∇f and ∇2f ,
which will be replaced by their stochastic counterparts for Problem (1). Based on (10), we notice
that, if the feasibility error vanishes, ∇L(cid:15),ν,η “ 0 implies that the KKT conditions (4) hold. This is
summarized in the following lemma.

Lemma 2.2. Let (cid:15), ν, η ą 0 and let px‹, µ‹, λ‹q P Tν ˆ Rm ˆ Rr be a primal-dual triple.
If
}cpx‹q} “ }w(cid:15),νpx‹, λ‹q} “ }∇L(cid:15),ν,ηpx‹, µ‹, λ‹q} “ 0, then px‹, µ‹, λ‹q satisﬁes (4) and, hence, is a
KKT point of Problem (1).

Proof. See Appendix A.1.

We emphasize that Lemma 2.2 holds without any constraint qualiﬁcations. We end this section
by brieﬂy introducing a general active-set SQP scheme. We will modify the scheme in Section 3 to
adapt it for the augmented Lagrangian merit function.

Given the iterate pxt, µt, λtq and an identiﬁed active set At Ď t1, 2, . . . , ru, let us ease notation
pAtqc (similarly

by denoting J t “ Jpxtq, Gt “ Gpxtq (similarly ∇f t, ct, gt, . . .), and λt
a, gt
gt

c “ λt
c, . . .). Then, the active-set SQP solves the following EQP subproblem

a “ λt

At, λt

a, Gt

c, Gt

p∆xtqT Bt∆xt ` p∇f tqT ∆xt,

1
2

min
∆xt
s.t. ct ` J t∆xt “ 0,
a∆xt “ 0,
a ` Gt
gt

(11)

for some Bt that approximates the Hessian ∇2
its dual solutions by (cid:101)µt`1 and (cid:101)λt`1
a ´ λt
and (cid:101)∆λt
a and (cid:101)∆λt
consists of (cid:101)∆λt

xLt. Suppose the subproblem (11) is solvable and denote
. Then, the dual search directions are given by (cid:101)∆µt “ (cid:101)µt`1 ´ µt
cq (here, we mean (cid:101)∆λt
c and (cid:101)∆λt “ p (cid:101)∆λt
c, with indices being ordered from 1 to r). Finally, the iterate is updated as

a
a. We further let (cid:101)∆λt

a “ (cid:101)λt`1

c “ ´λt

a, (cid:101)∆λt

˛

‚“

¨

˝

xt`1
µt`1
λt`1

¨

˝

xt
µt
λt

˛

‚` αt

˛

‚

¨

˝

∆xt
(cid:101)∆µt
(cid:101)∆λt

with αt chosen to ensure a certain suﬃcient decrease on the merit function.

3 A Local Non-Adaptive Scheme

Based on the SQP framework introduced in Section 2, we look into a simple, local, non-adaptive
scheme for solving Problem (1). The scheme requires the iterates to lie in a small neighborhood of a
KKT point px‹, µ‹, λ‹q; and requires a small enough penalty parameter (cid:15), a large enough boundary
parameter ν, and a prespeciﬁed deterministic stepsize sequence tαtut. Although the scheme itself is
not practical, it reveals the main diﬃculties in solving inequality constrained problems, which will
be resolved in the next section.

3.1 The local scheme

Let (cid:15), ν ą 0 be ﬁxed. Given the t-th iterate pxt, µt, λtq P Tν ˆ Rm ˆ Rr, we let

At

(cid:15),ν :“ A(cid:15),νpxt, λtq :“ ti : 1 ď i ď r, gipxtq ě ´(cid:15)qνpxt, λtqλt
iu

(12)

7

be the identiﬁed active set. Given two independent samples ξt
∇f pxt; ξt

2q. We use ∇f pxt; ξt

2q, and ∇2f pxt; ξt

1q to compute

1, ξt

2 „ P, we compute ∇f pxt; ξt

1q,

¯∇xLt :“ ∇f pxt; ξt

1q ` pJ tqT µt ` pGtqT λt,

(13)

and use ∇f pxt; ξt
¯∇xLt and p ¯Qt
1, ¯Qt
λt

(cid:15),ν qc (similar for Gt

2q to compute ¯Qt

2q, ∇2f pxt; ξt
2q are independent as well. Given the active set At

(cid:15),ν, we write λt
c, . . .). Then, we solve the following coupled linear system

2 deﬁned in (9). Since ξt

1 and ¯Qt

1 and ξt

a, Gt

pAt

2 are independent,
a “ λt
c “
At

, λt

(cid:15),ν

Kt
hkkkkkkkkkkikkkkkkkkkkj
a
ˆ

˙ ˜

aqT

Bt pJ tqT pGt
J t
Gt
a

´

J tpJ tqT
GtpJ tqT GtpGtqT `diag2pgtq
loooooooooooooooomoooooooooooooooon

J tpGtqT

¸

¯

¯∆xt
¯
(cid:101)∆µt
¯
(cid:101)∆λt
a
´

¯∆µt
¯∆λt

ˆ

“ ´

" ´

“ ´

¯

¯∇xLt´pGt

cqT λt
c

ct
gt
a

˙

,

J t ¯∇xLt
Gt ¯∇xLt`Πcpdiag2pgtqλtq

(14a)

¯

´

`

p ¯Qt
p ¯Qt

1qT
2qT

¯

*

¯∆xt

,

(14b)

M t

for some Bt that approximates the Hessian ∇2
xLt. We assume the approximation Bt is deterministic
conditional on the iterate pxt, µt, λtq. Our active-set SQP direction is then ¯∆t :“ p ¯∆xt, ¯∆µt, ¯∆λtq;
and the iterate is updated as

˛

‚“

¨

˝

xt`1
µt`1
λt`1

¨

˝

xt
µt
λt

˛

‚` αt

¨

˝

¯∆xt
¯∆µt
¯∆λt

˛

‚,

(15)

where the stepsize αt is deterministically prespeciﬁed in this section.

We denote p∆xt, (cid:101)∆µt, (cid:101)∆λt
1, ¯Qt

aq, p∆µt, ∆λtq the deterministic solutions obtained by solving (14a)
and (14b) with ¯∇xLt, ¯Qt
2 replaced by their deterministic counterparts ∇xLt, Qt
2; and
denote ∆t “ p∆xt, ∆µt, ∆λtq. Our direction ∆t is diﬀerent from (11), introduced, for example, in
(Pillo and Lucidi, 2002, (8.9)). We explain it in the next remark.

1, Qt

1, Qt

a ` (cid:101)∆λt
¯
(cid:101)∆λt

Remark 3.1. Consider the deterministic system (14) where ∇xLt, Qt
2 are used in place of their
stochastic counterparts. The system (14a) is nothing but the KKT conditions of EQP in (11). Thus,
the solution p∆xt, µt ` (cid:101)∆µt, λt
aq of the deterministic system (14a) (recall that we suppress
¯
the “bar” of notation p ¯∆xt,
(cid:101)∆µt,
aq for the deterministic system) is also the primal-dual solution
of (11). However, diﬀerent from basic SQP scheme in Section 2, our dual search direction for both
active and inactive constraints, p∆µt, ∆λtq, is given by (14b), instead of by p (cid:101)∆µt, (cid:101)∆λt
cq. It
turns out that this adjustment is crucial for using the augmented Lagrangian merit function (8).
A similar, coupled SQP system is employed for equality constrained problems (Lucidi, 1990; Na
et al., 2021), while we generalize to inequality constraints here. In fact, (Pillo and Lucidi, 2002,
(cid:15),ν,η if pxt, µt, λtq is
Proposition 8.2) showed that p∆xt, (cid:101)∆µt, (cid:101)∆λt
xLt. In contrast,
near a KKT point and Bt “ ∇2
xLt. However, that result does not hold if Bt ‰ ∇2
xLt.
as shown in Lemma 3.7, ∆t is a descent direction even when Bt is not close to ∇2

cq is a descent direction of Lt

a, ´λt

a, ´λt

8

3.2 Convergence analysis of the local scheme

We study the convergence of the scheme in Section 3.1. The KKT residual of a triple px, µ, λq is
deﬁned as

˛

Rpx, µ, λq “

.

(16)

›
¨
›
›
›
˝
›
›

∇xLpx, µ, λq
cpxq
maxtgpxq, ´λu

›
›
›
›
‚
›
›

If Rt :“ Rpxt, µt, λtq Ñ 0, then any accumulation point of tpxt, µt, λtqut is a KKT point. The
following lemma connects } maxtgpxq, ´λu} with }w(cid:15),νpx, λq}. Recall that w(cid:15),νpx, λq is deﬁned in
(7) with qνpx, λq deﬁned in (6).
Lemma 3.2. Let (cid:15), ν ą 0 and px, λq P Tν ˆ Rr. Then

}w(cid:15),νpx, λq}
(cid:15)qνpx, λq _ 1

ď } maxtgpxq, ´λu} ď

}w(cid:15),νpx, λq}
(cid:15)qνpx, λq ^ 1

.

Proof. See Appendix B.1.

From now on, let us suppose functions f, g, c in (1) are thrice continuously diﬀerentiable; (14)
is solvable; and the iterate pxt, µt, λtq P X ˆ M ˆ Λ Ď Tν ˆ Rm ˆ Rr lies in a convex compact set
that contains a KKT point px‹, µ‹, λ‹q. The conditions are formally stated later.

An observation is that the augmented Lagrangian L(cid:15),ν,η in (8) is an SC1 function in Tν ˆRm ˆRr1.
As a result, its Hessian may not exist for points tpx, µ, λq : D1 ď i ď r, gipxq “ ´(cid:15)qνpx, λqλiu, but
its generalized Hessian B2L(cid:15),ν,η in Clarke’s sense is well-deﬁned (Clarke, 1990). The generalized
Hessian B2L(cid:15),ν,η is a convex, compact set of symmetric matrices. Further, a similar Taylor expansion
holds with the Hessian being replaced by a matrix in the generalized Hessian set (Facchinei, 1995,
Proposition 2.3); and the point-to-set map px, µ, λq Ñ B2L(cid:15),ν,ηpx, µ, λq is bounded on bounded sets
(Hiriart-Urruty et al., 1984). Therefore, there exists a constant Υ(cid:15),ν,η (depending on parameters (cid:15), ν, η)
such that }Hpx, µ, λq} ď Υ(cid:15),ν,η for any px, µ, λq P X ˆMˆΛ and any Hpx, µ, λq P B2L(cid:15),ν,ηpx, µ, λq.
By the update (15), there exists (cid:101)H t P B2L(cid:15),ν,ηp(cid:101)xt, (cid:101)µt, (cid:101)λtq with (cid:101)xt “ ζxt ` p1 ´ ζqxt`1 for some

ζ P p0, 1q (same deﬁnition for (cid:101)µt, (cid:101)λt) such that

(cid:15),ν,η “ Lt
Lt`1

(cid:15),ν,η ` αtp∇Lt

(cid:15),ν,ηqT ¯∆t `

α2
t
2

p ¯∆tqT (cid:101)H t ¯∆t ď Lt

(cid:15),ν,η ` αtp∇Lt

(cid:15),ν,ηqT ¯∆t `

Υ(cid:15),ν,ηα2
t
2

} ¯∆t}2. (17)

We focus on the last two terms on the right hand side. For notational simplicity, we use Etr¨s “ Er¨ |
pxt, µt, λtqs to denote the conditional expectation given the t-th iterate pxt, µt, λtq. Recall that
∆t “ p∆xt, ∆µt, ∆λtq is the deterministic search direction that is obtained by solving (14) without
random sampling.
Lemma 3.3. Suppose Kt
a and M t in (14) are nonsingular and pxt, µt, λtq P X ˆ M ˆ Λ. Suppose
also that Eξr}∇f pxt; ξq ´ ∇f pxtq}2s ď ψg and Eξr}∇2f pxt; ξq ´ ∇2f pxtq}2s ď ψH for constants
ψg, ψH ą 0. Then, there exists a constant Υ∆ ą 0, which may depend on ψg, ψH , but not on
parameters (cid:15), ν, η, such that

Etr ¯∆ts “ ∆t,

Etr} ¯∆t}2s ď Υ∆p1 _ }pM tq´1}2qp1 _ }pKt

aq´1}2q

`
}∆t}2 ` ψg

˘

.

1An SC1 function is a function which is continuously diﬀerentiable with a semismooth gradient. The SC1 class is
between C1 and C2 classes. A common SC1 function that appears in constrained optimization is } maxtgpxq, 0u}2 (cf.
(Hiriart-Urruty et al., 1984, Example 2.1)), which has the same form as }b(cid:15),ν px, λq}2 in (8). See (Pillo and Lucidi,
2002, Section 6) for more discussions on the Hessian of L(cid:15),ν,η.

9

Proof. See Appendix B.2.

By Lemma 3.3, we can take the conditional expectation on both sides of (17). Ideally, we hope
that the quadratic term of (17) contributes to a higher order error for small enough stepsize, while
the linear term p∇Lt

(cid:15),ν,ηqT ∆t ensures a suﬃcient descent on Lt

(cid:15),ν,η.

Unfortunately, diﬀerent from equality constrained problems, ∆t may not be a descent direction
(cid:15),ν,η for some points. To see it clearly, we suppress the iteration index, and divide ∇L(cid:15),ν,η into

of Lt
two parts. By (10), we deﬁne

¨

˚
˝

¨

˚
˝

∇xLp1q
(cid:15),ν,η
∇µLp1q
(cid:15),ν,η
∇λLp1q
(cid:15),ν,η
∇xLp2q
(cid:15),ν,η
∇µLp2q
(cid:15),ν,η
∇λLp2q
(cid:15),ν,η

˛

‹
‚“

¨
I
˝

1
(cid:15)qν

GT

1
(cid:15) J T
I

˛

¨

‚

˝

I
˛

‹
‚` η

˛

¨

‹
‚“

˚
˝

GT l

3}w(cid:15),ν }2
2(cid:15)qν aν
0
}w(cid:15),ν }2
(cid:15)aν

λ

˛

‚` η

¨

˝

Q1 Q2
M11 M12
M21 M22

˛

‚

´

J∇xL
G∇xL`Πcpdiag2pgqλq

¯

,

∇xL
c
w(cid:15),ν

˛

‚diag2pgaqλa,

(18)

¨

˝

Q2,a
M12,a
M22,a

and have

∇L(cid:15),ν,η “ ∇Lp1q

(cid:15),ν,η ` ∇Lp2q

(cid:15),ν,η.

The ﬁrst term ∇Lp1q
λcq; while the second term ∇Lp2q
quadratic in pga, λcq.

(cid:15),ν,η contains all dominating terms of ∇L(cid:15),ν,η, which are linear in p∇xL, c, ga,
(cid:15),ν,η contains all higher order terms of ∇L(cid:15),ν,η, which are at least

Loosely speaking (see Lemma 3.7 for a rigorous result), p∇Lp1q
(cid:15),ν,ηqT ∆ provides a suﬃcient decrease
provided the penalty parameters are suitably chosen, while p∇Lp2q
(cid:15),ν,ηqT ∆ has no such guarantee in
general. Since ∇Lp2q
(cid:15),ν,η depends on ga, λc quadratically, to ensure ∇LT
(cid:15),ν,η∆ ă 0, we require }ga}_}λc}
to be small enough to let the linear term p∇Lp1q
(cid:15),ν,ηqT ∆ dominate. This essentially requires the iterate
to be close to a KKT point, since }ga} “ }λc} “ 0 at a KKT point. With this discussion in mind,
if the iterate is far from a KKT point, ∆ may not be a descent direction of L(cid:15),ν,η. In fact, for an
iterate that is far from a KKT point, the KKT matrix Kt
a) is likely to be
singular due to the imprecisely identiﬁed active set. Thus, Newton system (14) is not solvable at
such an iterate at all, let alone it generates a descent direction. Without inequalities, the quadratic
term ∇Lp2q
(cid:15),ν,η disappears and our analysis reduces to the one in Na et al. (2021). We realize that the
existence of ∇Lp2q
(cid:15),ν,η results in a very diﬀerent augmented Lagrangian to the one in Na et al. (2021);
and brings diﬃculties in designing a global algorithm to deal with inequality constraints.

a (and its component Gt

We point out that the requirement on having a good initial iterate is not an artifact of the proof
technique. Such a requirement is imposed for diﬀerent search directions in related literature. For
example, Pillo and Lucidi (2002) showed that the SQP direction obtained by either EQP or IQP is
a descent direction of L(cid:15),ν,η in a neighborhood of a KKT point (cf. Propositions 8.2 and 8.4). That
work also required Bt “ ∇2
xLt, which we relax by considering a coupled Newton system. Similarly,
Pillo et al. (2008, 2011a) studied truncated Newton directions, whose descent property holds only
locally as well (cf. (Pillo et al., 2008, Proposition 3.7), (Pillo et al., 2011a, Proposition 10)).

Now, we formalize our assumptions and discuss their implications. Recall that px‹, µ‹, λ‹q is

any (target) KKT point.

10

Assumption 3.4 (Linear independence constraint qualiﬁcation (LICQ)). We assume at x‹ that
Ipx‹qpx‹qq P Rdˆpm`|Ipx‹q|q has full column rank, where Ipx‹q is the active inequality set
pJ T px‹q GT
deﬁned in (3).

By Assumption 3.4, if the set X Q x‹ is small enough, then pJ T pxq GT

Ipx‹qpxqq has full column

rank for all x P X . Furthermore, by (9), for any pa, bq P Rm`r,

0 “ paT bT qM pxq

ˆ

˙

a
b

ùñ bIcpx‹q “ 0

ùñ }J T pxqa ` GT

Ipx‹qpxqbIpx‹q} “ 0 ùñ pa, bq “ 0,

(19)

where the ﬁrst implication is due to diagpgpxqqb “ 0 and I cpx‹q Ď I cpxq (since X is small), and
the second implication is due to }J T pxqa ` GT pxqb} “ 0. Thus, M pxq is invertible. Moreover, for
any A Ď Ipx‹q, we have

!´

¯

)

!´

¯

σmin

Jpxq
GApxq

p J T pxq GT

Apxq q

ě σmin

Jpxq
GIpx‹qpxq

)

p J T pxq GT

Ipx‹qpxq q

ą 0,

(20)

where σminp¨q denotes the least singular value of a matrix. By (19), (20), and the compactness of X ,
we know that there exists γH P p0, 1s2 such that

M pxq ľ γH I,

ˆ

˙

Jpxq
GApxq

`

J T pxq GT

˘
Apxq

ľ γH I,

@x P X and A Ď Ipx‹q.

(21)

To further ensure ppJ tqT pGt
(12) satisﬁes At
by the following lemma.

(cid:15),ν in
(cid:15),ν Ď Ipx‹q, noting the condition on the active set in (21). This is guaranteed locally

aqT q has a full column rank, we need the identiﬁed active set At

Lemma 3.5. Let (cid:15), ν ą 0, Ipx‹q be the active set deﬁned in (3), and

I `px‹, λ‹q “ ti P Ipx‹q : λ‹

i ą 0u .

There exists a convex compact set X(cid:15),ν ˆ Λ(cid:15),ν Ď X ˆ Λ (depending on (cid:15), ν), such that px‹, λ‹q P
X(cid:15),ν ˆ Λ(cid:15),ν and

I `px‹, λ‹q Ď A(cid:15),νpx, λq Ď Ipx‹q,

@px, λq P X(cid:15),ν ˆ Λ(cid:15),ν.

Proof. See Appendix B.3.

Lemma 3.5 suggests that At

(cid:15),ν, deﬁned in (12), indeed correctly identiﬁes the true active set
locally. We emphasize that the strict complementarity condition is not required in our analysis,
under which I `px‹, λ‹q “ Ipx‹q. The constants Υ∆, γH appearing in Lemma 3.3 and (21) hold
for all points in X ˆ M ˆ Λ. Therefore, those bounds hold for subset X(cid:15),ν ˆ M ˆ Λ(cid:15),ν as well.
Combining (21) and Lemma 3.5, we have

M t ľ γH I,

`

pJ tqT

pGt

aqT

˘

ľ γH I,

(22)

ˆ

˙

J t
Gt
a

for pxt, λtq P X(cid:15),ν ˆ Λ(cid:15),ν. Moreover, to ensure Kt
condition on the Hessian approximation Bt.

a in (14a) is invertible, we need the following

2The requirement on γH ď 1 (similar for other constants deﬁned later) is inessential, which is imposed only for

simplifying the presentation. Without such requirement, all results hold by replacing γH with γH ^ 1.

11

Assumption 3.6. For all t and z P tz P Rd : J tz “ 0, Gt
}Bt} ď ΥB for constants ΥB ě 1 ě γB ą 0.

az “ 0u, we have zT Btz ě γB}z}2 and

The above condition on Bt is standard in nonlinear optimization literature (Bertsekas, 1982). In
fact, Bt “ I with γB “ ΥB “ 1 is suﬃcient for the analysis in this paper. Combining (22) with
Assumption 3.6, we know Kt
a is invertible and hence the system (14a) is solvable (Nocedal and
Wright, 2006, Lemma 16.1). Furthermore, we can show

}pKt

aq´1} ď 8Υ2

B{pγBγH q.

(23)

See, for example, Lemma 3.2 in Na et al. (2021) for a simple proof.

The next lemma characterizes p∇Lp1q

(cid:15),ν,ηqT ∆ and p∇Lp2q

(cid:15),ν,ηqT ∆.

Lemma 3.7. Let ν, η ą 0. Suppose Assumptions 3.4 and 3.6 hold. There exist a constant
Υ ą 0 independent of pν, η, γH , γBq, where γH is from (21) and γB is from Assumption 3.6, and
a convex compact set X(cid:15),ν,η ˆ M ˆ Λ(cid:15),ν,η around px‹, µ‹, λ‹q, depending on p(cid:15), ν, ηq, such that if
pxt, µt, λtq P X(cid:15),ν,η ˆ M ˆ Λ(cid:15),ν,η with (cid:15) satisfying

1
(cid:15)

ě

p1 _ νqΥ
H γ2
γ4
BpγB ^ ηq

,

p∇Lt piq

(cid:15),ν,ηq∆t ď ci

ˆ

›
›
›
›

∆xt
J t∇xLt
Gt∇xLt`Πcpdiag2pgtqλtq

2

˙›
›
›
›

c1 “ ´

γB ^ η
2

and

c2 “

γB ^ η
4

.

then

with

Proof. See Appendix B.4.

From the proof of Lemma 3.7, we see that p∇Lp1q

small enough. However, from the equation (B.12) in the proof, we also see that p∇Lp2q
upper bounded by

(cid:15),ν,ηqT ∆ ensures a suﬃcient descent provided (cid:15) is
(cid:15),ν,ηqT ∆ is only

p∇Lt p2q

(cid:15),ν,ηq∆t ď Υ1

ˆ

1 _ ν
(cid:15)p1 ^ ν2q

˙

_ η

p}ga} ` }λc}q

›
ˆ
›
›
›

∆xt
J t∇xLt
Gt∇xLt`Πcpdiag2pgtqλtq

˙›
›
2
›
›

,

where the constant Υ1 ą 0 is independent of p(cid:15), ν, ηq. Thus, to ensure the inner product ∇LT
(cid:15),ν,η∆
is negative, we consider a neighborhood, whose radius depends on p(cid:15), ν, ηq, in which }ga} _ }λc} is
small enough so that Υ1p 1_ν
(cid:15)p1^ν2q _ ηqp}ga} ` }λc}q ď pγB ^ ηq{4. By Lemma 3.5, this is achievable
near the KKT pair px‹, λ‹q, where the active set is correctly identiﬁed.

Combining (17) with Lemmas 3.3 and 3.7, we arrive at the following convergence guarantee.

Theorem 3.8. Suppose f, g, c are thrice continuously diﬀerentiable. Let px‹, µ‹, λ‹q be a KKT
point and (cid:15), ν, η ą 0. Suppose Assumptions 3.4 and 3.6 hold, and the random sampling satisﬁes

Eξr}∇f pxt; ξq ´ ∇f pxtq}2s ď ψg,

Eξr}∇2f pxt; ξq ´ ∇2f pxtq}2s ď ψH .

Then, there exist thresholds (cid:15)thres, αthres ą 0, a constant C (independent of αt), and a convex
compact set X(cid:15),ν,η ˆ M ˆ Λ(cid:15),ν,η, such that if tpxt, µt, λtqut P X(cid:15),ν,η ˆ M ˆ Λ(cid:15),ν,η with (cid:15) ď (cid:15)thres, we
have two cases.

12

(a) If αt “ α ď αthres, @t ě 0, then

1
Γ ` 1

(b) If αt ď αthres, @t ě 0, and

ř

Proof. See Appendix B.5.

Γÿ

t“0

ErR2

t s ď

C
αpΓ ` 1q

` Cα.

8
t“0 αt “ 8 and

ř

8
t“0 α2

t ă 8, then

lim inf
tÑ8

Rt “ 0,

almost surely.

We mention that Theorem 3.8 is not a global convergence result since the set X(cid:15),ν,η ˆ Λ(cid:15),ν,η
has to be small around px‹, λ‹q. Within the neighborhood, the “almost sure” convergence result
matches our later result in Theorem 4.15 for a line search algorithm, and matches the result of
equality constrained problems in (Na et al., 2021, Theorem 4.12). The “almost sure” convergence is
diﬀerent from the convergence in expectation established in Berahas et al. (2021c,b); Curtis et al.
(2021b). In addition to requiring a good initial iterate, a clear drawback of the scheme introduced in
this section is the lack of adaptivity. The parameters (cid:15), ν are ﬁxed without any adjustment according
to the iterates. The stepsize sequence tαtut is prespeciﬁed and deterministic, which is likely to be
either conservative or aggressive.

Achieving adaptivity on ν is straightforward. We can enlarge the perturbed set Tν by increasing

ν whenever we observe xt R Tν. However, adaptivity on (cid:15) is critical and challenging.

(a) By Lemma 3.7, if (cid:15) is large, the SQP direction may not be a descent direction of L(cid:15),ν,η, even if

we are close to a KKT point.

(b) By exact property of the augmented Lagrangian (Pillo and Lucidi, 2002, Theorem 5.3), there
is a deterministic threshold of (cid:15) to ensure the equivalence between a stationary point of
augmented Lagrangian and a KKT point of Problem (1). If (cid:15) is large, it is possible that we
converge to a stationary point of L(cid:15),ν,η, but not a KKT point of (1).

In Section 4, we reﬁne the scheme introduced here. In particular, we globalize the scheme by
providing an alternative back up search direction, such as a Newton step or a steepest descent step
of L(cid:15),ν,η. If the SQP system is not solvable or is solvable, but does not generate a descent direction,
we search along the alternative direction to decrease the merit function. However, since the SQP
direction usually enjoys a fast local rate, we prefer to preserve it as much as possible. In addition,
we adaptively select (cid:15), ν, and select the stepsize αt via stochastic line search.

4 A Global Adaptive Scheme

We design an adaptive scheme for Problem (1) by incorporating stochastic line search, originally
analyzed for unconstrained problems in Cartis and Scheinberg (2017); Paquette and Scheinberg
(2020), into active-set StoSQP. There are two challenges to design an adaptive scheme for constrained
problems. First, the merit function in line search has penalty parameters that are random and
adaptively speciﬁed; while for unconstrained problems one simply uses objective function in line
search. To establish the convergence, it is important to show that the stochastic parameters are
stabilized almost surely. Thus, for each run, after a number of iterations, we always target a stabilized
merit function, although the stabilized merit function may diﬀer in diﬀerent runs. Otherwise, if
each iteration decreases a diﬀerent merit function, then the decreases across iterations may not

13

accumulate. Second, since the stabilized parameters are random, they may not below deterministic
thresholds. Such a condition is critical to ensure the equivalence between stationary points of the
merit function and KKT points of Problem (1). Thus, it is not necessarily true that stationary
points of the stabilized merit function are always KKT points of Problem (1).

With only equality constraints, Berahas et al. (2021c); Na et al. (2021) addressed the ﬁrst
challenge under a boundedness condition, and our paper follows the same type of analysis. Similar
boundedness condition is also required for deterministic analysis to have penalty parameters stabilized
(Bertsekas, 1982, Chapter 4.3.3). Berahas et al. (2021c) resolved the second challenge for certain
noise distributions (e.g., Gaussian), while Na et al. (2021) resolved it by adjusting the SQP scheme
when selecting penalty parameters. We generalize the technique of Na et al. (2021) to enable
inequality constraints. As revealed in Section 3, the generalization from equality to inequality leads
to a much more involved analysis, as some properties, such as the descent property of the SQP
direction, fail to hold when the active set is imprecisely identiﬁed. Following the notation style in
Section 3, we use ¯p¨q to denote random quantities, except for the iterate pxt, µt, λtq. For example,
we use ¯αt to denote the stepsize in what follows.

4.1 The proposed scheme

Let η, αmax, κgrad, κf ą 0; ρ ą 1; γB P p0, 1s; β, pgrad, pf P p0, 1q be ﬁxed tuning parameters. Given
quantities pxt, µt, λt, ¯νt, ¯(cid:15)t, ¯αt, ¯δtq at t-th iteration with xt P T¯νt, we perform ﬁve steps to derive
quantities at the pt ` 1q-th iteration.
Step 1: Estimate objective derivatives. We generate a batch of independent samples ξt
compute

1 and

¯∇f t “

∇f pxt; ξq,

¯∇2f t “

∇2f pxt; ξq.

(24)

1
|ξt
1|

ÿ

ξPξt
1

1
|ξt
1|

ÿ

ξPξt
1

We slightly abuse the notation ξt
Using (24), we compute ¯∇xLt, ¯∇2
monotonically increasing and large enough so that the event E t
1,
˙›
›
ˆ
›
›
¯∇xLt
›
›
ct
›
›
looooooooomooooooooon
maxtgt,´λtu

1 from (13) to let ξt
1, ¯Qt
xLt, ¯Qt

›
› ¯∇2f t ´ ∇2f t

›
› ¯∇f t ´ ∇f t

›
› ď κgrad ¯αt

›
› _

E t
1 “

"

2 as deﬁned in (9). We assume that the size |ξt

1 denote a set of independent realizations.
1| is

satisﬁes

¯Rt

˘

`

E t
1

Pξt

1

ě 1 ´ pgrad.

1

pE t

1q “ P pE t

p¨q to denote the probability that is evaluated only over the randomness
We use the notation Pξt
1 from P, while the other random quantities are conditioned on, such as pxt, µt, λtq
of sampling ξt
and ¯αt. More precisely, we mean Pξt
1 | Ft´1q where the σ-algebra Ft´1 contains all
the randomness from 0-th to pt ´ 1q-th iterations (cf. deﬁnition (38) below). We note that if
the KKT residual Rt ‰ 0, then (26) is satisﬁed for large |ξt
1|, since the bound in (25) converges
to κgrad ¯αtRt ą 0 as |ξt
1 implies that the approximation error
} ¯∇Lt
(cid:15),ν,η} is uniformly small for any (cid:15) and ν, since the approximation error is independent
of (cid:15), ν. This property allows sampling before setting the penalty parameters.
Step 2: Set parameter ¯(cid:15)t. With current ¯νt, we decrease ¯(cid:15)t Ð ¯(cid:15)t{ρ until ¯(cid:15)t is small enough to
satisfy the following two conditions simultaneously:

1| Ñ 8. Furthermore, the event E t

(cid:15),ν,η ´ ∇Lt

1

14

*

,

(25)

(26)

(a) the feasibility error is bounded by the gradient of the merit function
›
ˆ
›
›
›

˙›
›
›
› ď

›
› ¯∇Lt

›
› ;

¯(cid:15)t,¯νt,η

ct
wt

¯(cid:15)t,¯νt

(b) if (14) is solvable, then we obtain ¯∆t “ p ¯∆xt, ¯∆µt, ¯∆λtq, and require

p ¯∇Lt p1q

¯(cid:15)t,¯νt,ηqT ¯∆t ď ´

ˆ

›
›
›
›

pγB ^ ηq
2

¯∆xt
J t ¯∇xLt
Gt ¯∇xLt`Πcpdiag2pgtqλtq

˙›
›
2
›
›

.

(27)

(28)

We prove in Lemma 4.3 and Lemma 4.5 that both (27) and (28) can be satisﬁed for suﬃciently
small ¯(cid:15)t. In fact, Lemma 3.7 has already established (28) for the deterministic case. Even though
¯∆t is not always used as the search direction, we still require (28) to hold for p ¯∇Lt p1q
¯(cid:15)t,¯νt,ηqT ¯∆t. The
reason for this is to avoid ruling out ¯∆t just because ¯(cid:15)t is not small enough, which might result
in a positive dominated term p ¯∇Lt p1q
¯(cid:15)t,¯νt,ηqT ¯∆t (cf. Lemma 3.7). If (14) is not solvable, which can
happen when the iterate is too far from a KKT point, so that Kt
a or M t is singular, then (28) is not
required. As analyzed in Section 3, under mild assumptions (cf. Assumptions 3.4 and 3.6) and due
a and M t are always nonsingular locally.
to the property of the identiﬁed active set in Lemma 3.5, Kt
The condition (27) is novel in constrained stochastic optimization and not required in determinis-
tic SQP schemes. This condition is critical in ensuring that a stationary point of the merit function
is a KKT point of (1). Motivated by Lemma 2.2, we know that “stationarity of the merit function
plus vanishing feasibility error” implies vanishing KKT residual. The condition (27) enforces that
the feasibility error is bounded by the gradient of the merit function. Thus, the stationary point
we are converging to is indeed a KKT point. The conditions (28), (27) address the two challenges
discussed at the end of Section 3.
Step 3: Decide the search direction. We may obtain a stochastic SQP direction ¯∆t from
Step 2. However, if (14) is not solvable, or it is solvable, but ¯∆t is not a suﬃcient descent direction
because
›
ˆ
›
›
›

p ¯∇Lt p2q

˙›
›
2
›
›

¯(cid:15)t,¯νt,ηq ¯∆t ą

(29)

,

¯∆xt
J t ¯∇xLt
Gt ¯∇xLt`Πcpdiag2pgtqλtq

pγB ^ ηq
4

then an alternative direction (cid:98)∆t must be employed to ensure the decrease of the merit function. In
particular, we can perform a regularized Newton step as

(cid:98)H t (cid:98)∆t “ ´ ¯∇Lt

¯(cid:15)t,¯νt,η,

(30)

where (cid:98)H t captures some second-order information of Lt
the generalized Hessian H t provided by (Pillo and Lucidi, 2002; Pillo et al., 2008) being3

¯(cid:15)t,¯νt,η. We let (cid:98)H t “ H t ` pγB ` }H t}qI with

H t

xx “ Bt ` ηBt

(cid:32)

´

H t

pµ,λqx “

pJ tqT J t ` pGtqT Gt
¯

´

(

Bt `

1
¯(cid:15)t
J tpGtqT

pJ tqT J t `
¯ ´

1
¯(cid:15)tqt
¯νt
¯
J t
Bt,
Gt

pGt

aqT Gt
a,

J t
ΠapGtq
´

` η

J tpJ tqT
GtpJ tqT GtpGtqT `diag2pΠcpgtqq
´

¯

H t

pµ,λqpµ,λq “

0
0 ´¯(cid:15)tqt
¯νt

0

diagpΠcp1qq

` η

J tpJ tqT
GtpJ tqT GtpGtqT `diag2pΠcpgtqq

J tpGtqT

(31)

¯
2

.

3See (6.1)-(6.3) in Pillo and Lucidi (2002) for a similar expression to (31). Our H t generalizes that deﬁnition
xL2 by Bt. Pillo and Lucidi (2002) has no

by including equality constraints and approximating the Hessian ∇2
regularization term pγB ` }H t}qI since that work considers local analysis.

15

¯(cid:15)t,¯νt is deﬁned in (12); 1 “ p1, . . . , 1q P Rr is the all one vector; and Πap¨q is
Here, ¯(cid:15)t is from Step 2; At
the projection operator deﬁned in Section 1. Clearly, we have (cid:98)H t ľ γBI. The motivation for using
(31) is that H t P B2Lt
¯(cid:15)t,¯νt,η if pxt, µt, λtq is close to a KKT point (Pillo and Lucidi, 2002, Proposition
6.1). However, using Newton direction is not crucial for our scheme. As analyzed in Section 3, the
SQP direction ¯∆t will be accepted locally, while (cid:98)∆t is a back up and adopted only when ¯∆t fails.
We can simply let (cid:98)H t “ I in (30), so that (cid:98)∆t becomes the steepest descent direction, which is also
allowed in our analysis. We numerically implement both the regularized Newton and the steepest
descent in Section 5.
Step 4: Estimate the merit function. Let q∆t denote the adopted search direction, so q∆t “ ¯∆t
from (14) or q∆t “ (cid:98)∆t from (30). We aim to perform stochastic line search by checking the Armijo
condition (36) at the trial point

xst “ xt ` ¯αt

q∆xt,

µst “ µt ` ¯αt

q∆µt,

λst “ λt ` ¯αt

q∆λt.

If the Armijo condition holds, we accept the trial point; otherwise, we reject the trial point and
decrease the stepsize. We estimate the merit function in this step and perform line search in the
next step.

First, we check if the trial primal point xst is in T¯νt.

In particular, if xst R T¯νt, that is
ast “ apxstq ą ¯νt{2 (cf. (5)), then we stop the current iteration, reject the trial point by letting
pxt`1, µt`1, λt`1q “ pxt, µt, λtq, and let ¯(cid:15)t`1 “ ¯(cid:15)t, ¯αt`1 “ ¯αt, ¯δt`1 “ ¯δt. We also increase ¯νt by
letting

¯νt`1 “ ρj ¯νt with

(32)
where rys denotes the least integer that exceeds y. The deﬁnition of j ě 1 in (32) ensures xst P T¯νt`1.
However, j “ 1 works as well, since xt`1 “ xt P T¯νt Ď T¯νt`1, as required for performing the next
iteration. In the case of xst R T¯νt, particularly if ast ě ¯νt, evaluating Lst
¯(cid:15)t,¯νt,η is not informative since
the penalty in Lst
¯(cid:15)t,¯νt,η may be rescaled by a negative number. Thus, we increase ¯νt and rerun the
iteration at the current point.

j “ rlogp2ast{¯νtq{ log ρs,

Otherwise xst P T¯νt, then we generate a batch of independent samples ξt

2, that are independent

from ξt

1 as well, and compute

¯f t “

¯¯∇f t “

1
|ξt
2|

1
|ξt
2|

ÿ

ξPξt
2
ÿ

ξPξt
2

f pxt; ξq,

¯f st “

∇f pxt; ξq,

¯¯∇f st “

1
|ξt
2|

1
|ξt
2|

ÿ

ξPξt
2
ÿ

ξPξt
2

f pxst; ξq,

∇f pxst; ξq.

We distinguish ¯¯∇f t from ¯∇f t in (24). While both of them are estimates of ∇f t, the former is
1. Using ¯f t, ¯¯∇f t, ¯f st, ¯¯∇f st, we compute
computed based on ξt
¯(cid:15)t,¯νt,η and ¯Lst
¯Lt
2| is large enough such that the
event E t
2,
)

2 and the latter is computed based on ξt
¯(cid:15)t,¯νt,η according to (8). We assume that the size |ξt
ˇ
ˇ ¯Lst

ˇ
ˇ ď ´κf ¯α2

!ˇ
ˇ ¯Lt

ˇ
ˇ _

¯(cid:15)t,¯νt,η ´ Lst

¯(cid:15)t,¯νt,ηqT q∆t

¯(cid:15)t,¯νt,η ´ Lt

t p ¯∇Lt

E t
2 “

¯(cid:15)t,¯νt,η

¯(cid:15)t,¯νt,η

satisﬁes

and

E

ξt
2

˘

`
E t
2

Pξt

2

ě 1 ´ pf

r| ¯Lt

¯(cid:15)t,¯νt,η ´ Lt

¯(cid:15)t,¯νt,η|2s _ E
ξt
2

r| ¯Lst

¯(cid:15)t,¯νt,η ´ Lst

¯(cid:15)t,¯νt,η|2s ď ¯δ2
t .

16

,

(33)

(34)

(35)

p¨q and E

2

Similar to (26), Pξt
ξt
2 from P, while the other random quantities are conditioned on. That is, Pξt
where the σ-algebra Ft´0.5 “ Ft´1 Y σpξt
the bias of the estimate, while (35) characterizes the variance.

r¨s are used to indicate that the randomness is only taken over sampling
2 | Ft´0.5q
1q is deﬁned in (38) below. The condition (34) characterizes

2q “ P pE t

pE t

ξt
2

2

Step 5: Perform line search. With the merit function estimates, we check the Armijo condition
next.
(a) If the Armijo condition holds,

¯(cid:15)t,¯νt,η ď ¯Lt
¯Lst

¯(cid:15)t,¯νt,η ` β ¯αtp ¯∇Lt

¯(cid:15)t,¯νt,ηqT q∆t,

(36)

then the trial point is accepted by letting pxt`1, µt`1, λt`1q “ pxst, µst, λstq and the stepsize is
increased by ¯αt`1 “ ρ¯αt ^ αmax. Furthermore, we check if the decrease of the merit function is
reliable. In particular, if

´β ¯αtp ¯∇Lt

¯(cid:15)t,¯νt,ηqT q∆t ě ¯δt,

(37)
then we relax the variance condition (35) by increasing ¯δt by ¯δt`1 “ ρ¯δt; otherwise, we decrease ¯δt
by ¯δt`1 “ ¯δt{ρ.
(b) If the Armijo condition (36) does not hold, then the trial point is rejected by letting pxt`1, µt`1, λt`1q “
pxt, µt, λtq; and we decrease ¯αt by ¯αt`1 “ ¯αt{ρ and ¯δt by ¯δt`1 “ ¯δt{ρ.

Finally, for both cases (a) and (b), we let ¯(cid:15)t`1 “ ¯(cid:15)t, ¯νt`1 “ ¯νt and repeat the procedure from

Step 1.

The proposed scheme is summarized in Algorithm 1. We deﬁne three types of iterations for line
search. If the Armijo condition (36) holds, we call the iteration a successful step, otherwise we call
it an unsuccessful step. For a successful step, if the suﬃcient decrease (37) is satisﬁed, we call it a
reliable step, otherwise we call it an unreliable step. Same notion is used in Cartis and Scheinberg
(2017); Paquette and Scheinberg (2020); Na et al. (2021).

We comment on the similarities and diﬀerences between Algorithm 1 and StoSQP in Na et al.
(2021). The event E t
1 in Step 1 simpliﬁes the deﬁnition in Na et al. (2021), while Step 2 generalizes
the technique to enable inequality constraints. Step 3 is a new step in our algorithm. Steps 4 and 5
perform line search and are similar to Na et al. (2021) except for adjustments required to handle
inequality constraints.

2u8

1, ξt

Let us introduce the ﬁltration induced by the randomness of the algorithm. Given a random
2ut
sample sequence tξt
j“0q, t ě 0, be the σ-algebra generated by all
the samples till t; Ft´0.5 “ σptξj
1q, t ě 0, be the σ-algebra generated by all the samples
till t ´ 1 and the sample ξt
1. For consistency, we let F´1 be the trivial σ-algebra generated by the
initial iterate (which is deterministic). Throughout the presentation, we let ¯(cid:15)t be the quantity after
the While loop of Step 2; that is, ¯(cid:15)t satisﬁes (27) and (28). With this setup, it is easy to see that

t“0,4 we let Ft “ σptξj
2ut´1

j“0 Y ξt

1, ξj

1, ξj

σpxt, µt, λtq Y σp¯νtq Y σp¯αtq Y σp¯δtq ĎFt´1,
σpxst, µst, λstq Y σp ¯∆t, (cid:98)∆t, q∆tq Y σp¯(cid:15)tq ĎFt´0.5.

(38)

We analyze Algorithm 1 in the next section.

4We note that ξt
we suppose a sample ξt

2 may not be generated if Lines 13 and 14 of Algorithm 1 are performed. However, for simplicity

2 is still generated in this case, although no quantity is determined by this sample.

17

Algorithm 1 An Adaptive Stochastic Scheme with Augmented Lagrangian
1: Input: initial iterate px0, µ0, λ0q, and parameters ¯α0 “ αmax ą 0, η, ¯(cid:15)0, ¯δ0, κgrad ą 0, ρ ą 1,

γB P p0, 1s, pgrad, pf , β P p0, 1q, κf P p0, β{p4αmaxqs, ¯ν0 “ 2

ř

r
i“1 maxtg0

i , 0u3 ` 1;

2: for t “ 0, 1, 2 . . . do
Generate ξt
3:

1 with |ξt

1| ě |ξt´1

1

in (9);

| ` 1 (|ξ´1

1 | “ 0) so that (26) holds; compute ¯∇xLt, ¯Qt

1, ¯Qt

2 as
Ź Step 1. estimate gradients

4:

5:

6:

7:

8:

9:

10:

11:

12:

13:

14:

15:

16:

17:

18:

19:

20:

21:

22:

23:

24:

25:

26:

27:

while {(27) not holds} OR {(14) is solvable AND (28) not holds} do

¯(cid:15)t Ð ¯(cid:15)t{ρ;

end while

Ź Step 2. set ¯(cid:15)t

if {(14) is not solvable} OR {(14) is solvable AND (29) holds} then

Solve (30) to obtain (cid:98)∆t and q∆t “ (cid:98)∆t;

Ź Step 3. decide q∆t

else

q∆t “ ¯∆t;

end if

if xst R T¯νt then

Ź Step 4. estimate merit function

pxt`1, µt`1, λt`1q “ pxt, µt, λtq, ¯αt`1 “ ¯αt, ¯δt`1 “ ¯δt, ¯(cid:15)t`1 “ ¯(cid:15)t;
¯νt`1 “ ρj ¯νt with j “ rlogp2ast{¯νtq{ log ρs;

else

Generate ξt
if ¯Lst

2, compute ¯Lt

¯(cid:15)t,¯νt,η, ¯Lst

¯(cid:15)t,¯νt,η so that (34), (35) hold;

¯(cid:15)t,¯νt,η ` β ¯αtp ¯∇Lt

¯(cid:15)t,¯νt,η ď ¯Lt
pxt`1, µt`1, λt`1q “ pxst, µst, λstq, ¯αt`1 “ ρ¯αt ^ αmax;
if ´β ¯αtp ¯∇Lt
¯δt`1 “ ρ¯δt;

¯(cid:15)t,¯νt,ηqT q∆t ě ¯δt then

¯(cid:15)t,¯νt,ηqT q∆t then

else

¯δt`1 “ ¯δt{ρ;

end if

else

pxt`1, µt`1, λt`1q “ pxt, µt, λtq, ¯αt`1 “ ¯αt{ρ, ¯δt`1 “ ¯δt{ρ;

end if
¯(cid:15)t`1 “ ¯(cid:15)t, ¯νt`1 “ ¯νt;

Ź Step 5. line search
Ź successful step
Ź reliable step

Ź unreliable step

Ź unsuccessful step

end if

28:
29: end for

4.2 Assumptions and stability of parameters

We study the stability of the parameter sequence t¯(cid:15)t, ¯νtut. We will show that, for each run of the
algorithm, they are stabilized after a ﬁnite number of iterations. Thus, Lines 5 and 14 of Algorithm 1
will not be performed when the iteration number is large enough. We begin by introducing
assumptions.

Assumption 4.1 (Regularity condition). We assume the iterate tpxt, µt, λtqu and trial point
tpxst, µst, λstqu are contained in a convex compact region X ˆ M
ˆ Λ. Further, if xst P T¯νt, then the segment tζxt ` p1 ´ ζqxst : ζ P p0, 1qu Ď Tθ¯νt for some θ P r1, 2q.

18

We also assume the functions f, g, c are thrice continuously diﬀerentiable over X , and realizations
|f px, ξq|, }∇f px, ξq}, }∇2f px, ξq} are uniformly bounded over x P X and ξ „ P.

Assumption 4.2 (Constraint qualiﬁcation). For any x P X zΩ, we assume the linear system

cipxq ` ∇T cipxqz “ 0,
gipxq ` ∇T gipxqz ď 0,

i : cipxq ‰ 0,

i : gipxq ą 0,

(39)

has a solution for z P Rd. For any x P Ω, we assume pJ T pxq GT
Ω is the feasible set in (2) and Ipxq is the active set in (3).

Ipxqpxqq has full column rank, where

The boundedness condition for realizations in Assumption 4.1 is widely used in StoSQP analysis
to have a well-behaved stochastic penalty parameter sequence (Berahas et al., 2021c; Na et al., 2021;
Berahas et al., 2021b; Curtis et al., 2021b). The third derivatives of f, g, c are only required in the
analysis and not needed in the implementation. They are required because the existence of the
generalized Hessian of augmented Lagrangian needs the third derivatives. See, for example, (Pillo
and Lucidi, 2002, Section 6) for the same requirement. The compactness condition on the iterates
is common for augmented Lagrangian analysis (Bertsekas, 1982, Chapter 4) and SQP analysis
(Nocedal and Wright, 2006, Chapter 18). The convexity of M ˆ Λ can be removed by considering
the closed convex hull convpMq ˆ convpMq. However, the convexity of the set for primal iterates
is essential to enable a valid Taylor expansion. See, for example, (Pillo et al., 2011b, Proposition
2.2 and Section 4) (Pillo et al., 2005, Proposition 2.4 and (14)) and references therein for the same
requirement for doing line search with (8) and applying its Taylor expansion.

In particular, by the design of Algorithm 1, we have xt P T¯νt for any t while the trial iterate xst
may be outside T¯νt. If xst R T¯νt, we enlarge ¯νt (Line 14) and rerun the iteration from the beginning.
Assumption 4.1 states that if it turns out that xst P T¯νt, then the whole segment ζxt ` p1 ´ ζqxst,
which may not completely lie in T¯νt as T¯νt may be nonconvex, is supposed to lie in a larger space
Tθ¯νt with θ P r1, 2q. Since L¯(cid:15)t,¯νt,η is SC1 in T ˝
2¯νt denotes the
interior of T2¯νt, the second-order Taylor expansion at pxt, µt, λtq is allowed. Note that the range of
θ is inessential. If we replace ν{2 in (5) by ν{κ for any κ ą 1, then we would allow the existence
of θ in r1, κq. In other words, θ can be as large as any κ. In fact, the condition on the segment
always holds when the input αmax, the upper bound of ¯αt (cf. Line 18), is suitably upper bounded.
q∆xt} ď Υ (ensured by compactness of iterates), for
Speciﬁcally, supposing supX }∇apxq} _ supt }
any θ ą 1 and ζ P p0, 1q, as long as αmax ď pθ ´ 1q¯ν0{p2Υ2q, we have ζxt ` p1 ´ ζqxst P Tθ¯νt by
noting that

2¯νt ˆ Rm ˆ Rr and Tθ¯νt Ď T ˝

2¯νt, where T ˝

apζxt ` p1 ´ ζqxstq “ apxt ` ¯αtp1 ´ ζq

ď

q∆xtq ď apxtq ` ¯αtp1 ´ ζqΥ2
¯νt
2

` αmaxΥ2 ď

pθ ´ 1q¯ν0
2

¯νt
2

`

ď

¯νt
2

`

pθ ´ 1q¯νt
2

“

θ¯νt
2

.

Clearly, the condition on the segment is not required if Tν in (5) is a convex set, which is the case,
for example, if we have linear inequality constraints x ď 0; or more generally, each gip¨q is a convex
function.

By the compactness condition and noting that ¯νt is increased by at least a factor of ρ each time

in (32), we immediately know that ¯νt stabilizes when t is large. Moreover, if we let

(cid:101)ν “ ρ(cid:101)j ¯ν0 with

(cid:101)j “ rlogp2 max

X

apxq{¯ν0q{ log ρs,

(40)

19

then ¯νt ď (cid:101)ν, t ě 0, almost surely. We will show a similar result for ¯(cid:15)t.

Assumption 4.2 imposes the constraint qualiﬁcations. In particular, for feasible points Ω, we
assume the linear independence constraint qualiﬁcation (LICQ), which is a common condition to
ensure the existence and uniqueness of Lagrangian multiplier (Nocedal and Wright, 2006). For
infeasible points X zΩ, we assume that the solution set of the linear system (39) is nonempty. The
condition (39) restricts the behavior of constraint functions outside the feasible set, which, together
with compactness condition, implies Ω ‰ H (cf. (Lucidi, 1992, Proposition 2.5)). In fact, the
condition (39) weakens the generalized Mangasarian-Fromovitz constraint qualiﬁcation (MFCQ)
(Xu et al., 2014, Deﬁnition 2.5); and relates to the weak MFCQ, which is proposed for problems with
only inequalities in (Lucidi, 1992, Deﬁnition 1) and adopted in (Pillo and Lucidi, 2002, Assumption
A3) and (Pillo et al., 2008, Assumption 3.2). However, Lucidi (1992) required the weak MFCQ to
hold for feasible points as well, in addition to LICQ; while Pillo and Lucidi (2002); Pillo et al. (2008)
and this paper remove such a condition. The condition (39) simpliﬁes and generalizes the weak
MFCQ in Lucidi (1992); Pillo and Lucidi (2002); Pillo et al. (2008) by including equality constraints.
We note that the weak MFCQ is slightly weaker than (39). In particular, by the Gordan’s theorem
(Goldman and Tucker, 1957), (39) implies that tci ¨ ∇ciui:ci‰0 Y t∇giui:gią0 are positively linearly
independent:

ÿ

ÿ

aici∇ci `

bi∇gi ‰ 0,

i:gią0

i:ci‰0
ř
i a2

i ` b2

for any coeﬃcients ai, bi ě 0 and
i ą 0. In contrast, the weak MFCQ only requires that the
linear combination is nonzero for a particular set of coeﬃcients. However, we adopt the simpliﬁed,
but stronger, condition only because (39) has a cleaner form and a clearer connection to SQP
subproblems. The coeﬃcients of the weak MFCQ in Lucidi (1992); Pillo and Lucidi (2002); Pillo
et al. (2008) are relatively hard to interpret; instead of regarding the constraint qualiﬁcation as
the essence of constraints, those coeﬃcients depend on particular choice of the merit function,
although that assumption statement is sharper. That said, (39) is still weaker than other literature
on augmented Lagrangian (Pillo and Grippo, 1982, 1986; Lucidi, 1988); and weaker than what is
widely assumed in SQP analysis (Boggs and Tolle, 1995), where the IQP system, ci ` ∇T ciz “ 0,
1 ď i ď m, gi ` ∇T giz ď 0, 1 ď i ď r, is supposed to have a solution. Moreover, we do not require
strict complementary condition, which is imposed for procedures that apply (squared) slack variables
to convert inequality constraints and deﬁne related merit functions (Zavala and Anitescu, 2014, A2),
(Fukuda and Fukushima, 2017, Proposition 3.8).

The ﬁrst lemma shows that (27) is satisﬁed for a suﬃciently small ¯(cid:15)t. Although (27) is inspired
by (Na et al., 2021, (19)) for inequalities, the proof is quite diﬀerent from that paper (cf. Lemma
4.4 there).
Lemma 4.3. Under Assumptions 4.1 and 4.2, there exists a deterministic threshold (cid:101)(cid:15)1 ą 0 such
that (27) holds for ¯(cid:15)t ď (cid:101)(cid:15)1.
Proof. See Appendix C.1.

The second lemma shows that (28) is satisﬁed for small ¯(cid:15)t as well. The analysis is similar to

Lemma 3.7. We need an additional condition on Newton system (14).
Assumption 4.4. We assume that, whenever (14) is solvable, ppJ tqT pGt
and there exist positive constants ΥB ě 1 ě γB _ γH such that

aqT q has full column rank,

Bt ĺ ΥBI,

M t ľ γH I,

20

ˆ

˙

J t
Gt
a

`

pJ tqT

pGt

aqT

˘

ľ γH I,

and zT Btz ě γB}z}2, @z P tz P Rd : J tz “ 0, Gt

az “ 0u.

Assumption 4.4 is a restatement of Assumptions 3.4 and 3.6. As analyzed in Section 3, the
conditions on M t and ppJ tqT pGt
aqT q hold locally. The deterministic (conditional on xt) Hessian
approximation Bt is easy to construct to make the assumption hold, e.g., Bt “ I. To ease the
notation, we use γB from the deﬁnition of (cid:98)H t in (31) for the assumption statement, which is an
input of the algorithm, although a diﬀerent lower bound constant for Bt is certainly allowed.

With Assumption 4.4, we have a similar result to Lemma 3.7.

Lemma 4.5. Under Assumptions 4.1 and 4.4, there exists a deterministic threshold (cid:101)(cid:15)2 ą 0 such
that (28) holds for ¯(cid:15)t ď (cid:101)(cid:15)2.
Proof. See Appendix C.2.

We summarize (40), Lemmas 4.3 and 4.5 in the next theorem.

Theorem 4.6. Under Assumptions 4.1, 4.2, and 4.4, there exist deterministic thresholds (cid:101)ν, (cid:101)(cid:15) ą 0
such that t¯νt, ¯(cid:15)tut generated by Algorithm 1 satisfy ¯νt ď (cid:101)ν, ¯(cid:15)t ě (cid:101)(cid:15). Moreover, almost surely, there
exists a iteration threshold ¯t ă 8, such that ¯(cid:15)t “ ¯(cid:15)¯t, ¯νt “ ¯ν¯t, t ě ¯t.

Proof. The existence of (cid:101)ν is showed in (40). By Lemmas 4.3 and 4.5, and deﬁning (cid:101)(cid:15) “ p(cid:101)(cid:15)1 ^ (cid:101)(cid:15)2q{ρ,
we show the existence of (cid:101)(cid:15). The existence of the iteration threshold ¯t is ensured by noting that
t¯νt, 1{¯(cid:15)tut are bounded from above; and each update increases the parameters by at least a factor of
ρ ą 1.

We mention that the iteration threshold ¯t is random for stochastic schemes and it changes
between diﬀerent runs. However, it always exists. The following analysis supposes t is large enough
such that t ě ¯t and ¯(cid:15)t, ¯νt have stabilized. We also condition our analysis on the σ-algebra F¯t, which
means that we only consider randomness of generated samples after ¯t ` 1 iterations, and, by (38),
the parameters ¯(cid:15)¯t, ¯ν¯t are ﬁxed. For t ě ¯t ` 1, Lines 5 and 14 of Algorithm 1 will not be performed.

4.3 Convergence analysis

We now conduct the convergence analysis for Algorithm 1. We will show that lim inf tÑ8 Rt “ 0
almost surely, where Rt is deﬁned in (16). We assume that the batch samples, ξt
2, are
generated such that conditions (26), (34), (35) hold. We defer the discussion of batch sizes that
make these conditions hold to Section 4.4. It is fairly easy to see that all conditions hold for
large batch sizes.

1 and ξt

Our proof structure follows the prior work (Na et al., 2021). Our analysis is more involved in
Lemmas 4.8, 4.10, 4.11, 4.12, slightly adjusted in Theorems 4.14, 4.15, and the same in Lemma 4.9
and Theorem 4.13 (hence these proofs are omitted). The potential function (or Lyapunov function) is

Θt

¯(cid:15)¯t,¯ν¯t,η,ω “ ωLt

¯(cid:15)¯t,¯ν¯t,η `

1 ´ ω
2

¯αt

›
›∇Lt

¯(cid:15)¯t,¯ν¯t,η

›
›2

`

1 ´ ω
2

¯δt,

t ě ¯t ` 1,

(41)

where ω P p0, 1q is a coeﬃcient to be speciﬁed later. The potential function (41) contains three
components, which is diﬀerent from deterministic line search where ω “ 1. Using Lt
¯(cid:15)¯t,¯ν¯t,η by itself
to monitor the iteration progress is not suitable in the stochastic setting, because it is possible
that Lt
¯(cid:15)¯t,¯ν¯t,η,ω linearly combines diﬀerent

¯(cid:15)¯t,¯ν¯t,η decreases. In contrast, Θt

¯(cid:15)¯t,¯ν¯t,η increases while ¯Lt

21

components and has a composite measure of the progress. For example, the decrease of Θt
may come from ¯δt (Lines 22 and 25 of Algorithm 1).

¯(cid:15)¯t,¯ν¯t,η,ω

Since parameters ¯(cid:15)¯t, ¯ν¯t, η in L¯(cid:15)¯t,¯ν¯t,η are ﬁxed (conditional on F¯t), we denote Θt

¯(cid:15)¯t,¯ν¯t,η,ω for
notational simplicity. We also only track the algorithmic parameters pβ, αmax, κgrad, κf , pgrad, pf q
in the presentation of theoretical results. In particular, we use C1, C2 . . . and Υ1, Υ2 . . . to denote
generic deterministic constants that are independent from pβ, αmax, κgrad, κf , pgrad, pf q, but may
depend on constants pγB, γH , ΥB, ρ, η, ¯(cid:15)0, ¯ν0q, and hence depend on deterministic thresholds (cid:101)(cid:15), (cid:101)ν.
Note that pγB, γH , ΥBq come from Assumption 4.4, while pρ, η, ¯(cid:15)0, ¯ν0q come from the algorithm
input and are deterministic.

ω “ Θt

The next lemma presents a preliminary result.

Lemma 4.7. Under Assumptions 4.1, 4.2, 4.4, the following results hold deterministically conditional
on Ft´1.

(a) There exists C1 ą 0 such that

›
› ¯∇Lt

(cid:15),ν,η ´ ∇Lt

(cid:15),ν,η

›
› ď C1

(cid:32)›
› ¯∇f t ´ ∇f t

›
› _

›
› ¯∇2f t ´ ∇2f t

›
(
›

for any iteration t ě 0, any parameters (cid:15), ν, and any generated samples ξt
1.

(b) There exists C2 ą 0 such that

›
› ¯∇xLt

›
›

"

ď C2

} ¯∇Lt

¯(cid:15)t,¯νt,η} `

›
›
›
›

ˆ

ct
wt

¯(cid:15)t,¯νt

˙›
*
›
›
›

,

for any t ě 0 and ξt
1.

(c) There exists C3 ą 0 such that, if (14) is solvable, then
˙›
›
›
› ,

¯∆xt
J t ¯∇xLt
Gt ¯∇xLt`Πcpdiag2pgtqλtq

›
› ¯∇Lt

›
ˆ
›
›
›

ď C3

¯(cid:15)t,¯νt,η

›
›

for any t ě 0 and ξt
1.

Proof. See Appendix C.3.

1 for computing ¯∇Lt

The bounds in Lemma 4.7 hold deterministically conditional on Ft´1, because, by the statement,
(cid:15),ν,η, ¯∇xLt are supposed to be given as well. The following result
¯(cid:15)¯t,¯ν¯t,η are precisely
2 happens (cf. (25), (33)), then there is a uniform lower

samples ξt
suggests that if both the gradient ∇Lt
estimated, in the sense that the event E t
bound on ¯αt to make the Armijo condition hold.

¯(cid:15)¯t,¯ν¯t,η and the function evaluations Lt

¯(cid:15)¯t,¯ν¯t,η, Lst

1 X E t

Lemma 4.8. For t ě ¯t ` 1, suppose E t
satisﬁes the Armijo condition (36) (i.e., is a successful step) if

1 X E t

2 happens. There exists Υ1 ą 0 such that the t-th step

¯αt ď

1 ´ β
Υ1pκgrad ` κf ` 1q

.

Proof. See Appendix C.4.

The next result suggests that, if the function evaluations Lt

in the sense that the event E t
decrease of Lt

2 happens, then a suﬃcient decrease of ¯Lt
¯(cid:15)¯t,¯ν¯t,η. The proof directly follows (Na et al., 2021, Lemma 4.6), hence is omitted.

¯(cid:15)¯t,¯ν¯t,η, Lst

¯(cid:15)¯t,¯ν¯t,η are precisely estimated,
¯(cid:15)¯t,¯ν¯t,η implies a suﬃcient

22

Lemma 4.9. For t ě ¯t ` 1, suppose E t
then

2 happens. If the t-th step satisﬁes the Armijo condition (36),

Lst
¯(cid:15)¯t,¯ν¯t,η ď Lt

¯(cid:15)¯t,¯ν¯t,η `

¯αtβ
2

p ¯∇Lt

¯(cid:15)¯t,¯ν¯t,ηqT q∆t.

ω in (41). Our analysis is separated into three cases according to the events: E t

Based on Lemmas 4.8 and 4.9, we are able to derive an error recursion for the potential function
1qc X E t
2
2, while may increase in the other
ω decreases in expectation.

Θt
and pE t
1 X E t
two cases. However, by letting pgrad and pf be small enough, Θt

ω decreases in the case of E t

2qc. We will show that Θt

1 X E t

2, pE t

Lemma 4.10. For t ě ¯t ` 1, suppose E t

1 X E t

2 happens. There exists Υ2 ą 0, such that if ω satisﬁes

ω
1 ´ ω

ě

Υ2pκgradαmax ` αmax ` 1q2
β

_ 18pρ ´ 1q,

(42)

then

Θt`1

ω ´ Θt

ω ď ´

ˆ

p1 ´ ωq

1 ´

˙ ´

1
ρ

›
›∇Lt

¯(cid:15)¯t,¯ν¯t,η

¯αt

›
›2

¯

` ¯δt

.

1
2

Proof. See Appendix C.5.

Lemma 4.11. For t ě ¯t ` 1, suppose pE t

1qc X E t

2 happens. Under (42), we have

Θt`1

ω ´ Θt

ω ď ρp1 ´ ωq¯αt

›
›∇Lt

¯(cid:15)¯t,¯ν¯t,η

›
›2 .

Proof. See Appendix C.6.

Lemma 4.12. For t ě ¯t ` 1, suppose pE t

2qc happens. Under (42), we have

Θt`1

ω ´ Θt

ω ď ρp1 ´ ωq¯αt

›
›∇Lt

¯(cid:15)¯t,¯ν¯t,η

›
›2

` ω

(cid:32)ˇ
ˇ ¯Lst

¯(cid:15)¯t,¯ν¯t,η ´ Lst

¯(cid:15)¯t,¯ν¯t,η

ˇ
ˇ `

ˇ
ˇ ¯Lt

¯(cid:15)¯t,¯ν¯t,η ´ Lt

¯(cid:15)¯t,¯ν¯t,η

(

ˇ
ˇ

.

Proof. See Appendix C.7.

Combining Lemmas 4.10, 4.11, 4.12, we derive the one-step error recursion. The proof is the

same as that of (Na et al., 2021, Theorem 4.10).

Theorem 4.13 (One-step error recursion). For t ě ¯t ` 1, suppose ω satisﬁes (42) and pgrad and pf
satisfy

"

*

pgrad `

?

pf

p1 ´ pgradqp1 ´ pf q

ď

ρ ´ 1
8ρ

1
ρ

^

1 ´ ω
ω

.

(43)

Then

E

“
Θt`1

ω ´ Θt

ω | Ft´1

‰

ď ´

1
4

p1 ´ pgradqp1 ´ pf qp1 ´ ωq

1 ´

ˆ

˙ ´

¯δt ` ¯αt

1
ρ

›
›∇Lt

¯(cid:15)¯t,¯ν¯t,η

¯

›
›2

.

With Theorem 4.13, we derive the convergence of ¯αtR2

t in the next theorem, where Rt is the

KKT residual deﬁned in (16).

Theorem 4.14. Under the conditions of Theorem 4.13, lim
tÑ8

¯αtR2

t “ 0 almost surely.

Proof. See Appendix C.8.

23

Finally, we complete the global convergence analysis of Algorithm 1.

Theorem 4.15 (Global convergence). Consider Algorithm 1 under Assumptions 4.1, 4.2, 4.4.
Suppose ω satisﬁes (42) and pgrad, pf satisfy (43). Then, almost surely, we have that

Proof. See Appendix C.9.

lim inf
tÑ8

Rt “ 0.

Our analysis extends the results of (Na et al., 2021) to inequality constrained problems. The
“almost sure” convergence result in Theorem 4.15 matches Na et al. (2021) for equality constrained
problems. It is consistent with Theorem 3.8(b), while two schemes have diﬀerent stepsize behavior—
the stepsize αt in Theorem 3.8(b) has to decay to zero, while ¯αt from line search is automatically
adjusted based on the iterate.

4.4 Discussion on sample complexity

The stochastic line search is performed by requiring a more precise model, which requires the
generation of batch samples. This is standard in the existing literature on adaptive algorithms
for unconstrained stochastic optimization, which adaptively control the batch size based on the
iteration progress (Friedlander and Schmidt, 2012; Byrd et al., 2012; Kreji´c and Krklec, 2013; De
et al., 2017; Bollapragada et al., 2018). We discuss the required batch size to ensure conditions (26),
q∆t} do not
(34) and (35). We show that, if the KKT residual Rt and stochastic search direction }
vanish, then all of the conditions are satisﬁed for large |ξt

1|, |ξt
2|.

In particular, by matrix Bernstein inequality (Tropp, 2015, Theorem 7.7.1),
¸

˜

`

P

E t
1 | Ft´1

˘

ě 1 ´ pgrad

if

|ξt

1| ě O

logpd{pgradq
¯R2
grad ¯α2
κ2
t
t

.

(44)

We note that ¯Rt on the right hand side has to be evaluated by samples ξt
1. A practical algorithm
1, then compute ¯Rt, and ﬁnally check if (44) holds. For example, a While loop can
can ﬁrst specify ξt
be designed to generate batches ξt
1 of increasing size until (44) holds (see (Na et al., 2021, Algorithm
4) as an example). Such a While loop always terminates in ﬁnite time, because as |ξt
1| increases,
¯Rt Ñ Rt almost surely (by the law of large number), and Rt ą 0. In other words, the right hand
ˆ

˙

side of (44) does not diverge, but converges to O

logpd{pgradq
t R2
grad ¯α2
κ2
t

, which is a ﬁxed number conditional

on Ft´1.

For conditions (34) and (35), we note that if q∆t “ ¯∆t,
›
ˆ
›
›
›

¯(cid:15)t,¯νt,ηqT q∆t

t pγB ^ ηq

t p ¯∇Lt

´κf ¯α2

(C.21)
ě

κf ¯α2

4

¯∆xt
J t ¯∇xLt
Gt ¯∇xLt`Πcpdiag2pgtqλtq

˙›
›
2
›
›

(C.22)

ě Opκf ¯α2

t } ¯∆t}2q ą 0;

and if q∆t “ (cid:98)∆t,

´κf ¯α2

t p ¯∇Lt

¯(cid:15)t,¯νt,ηqT q∆t

(C.25)

ě κf ¯α2

t γB

›
›2 (C.26)

ě Opκf ¯α2

t } (cid:98)∆t}2q ą 0.

›
› ¯∇Lt

¯(cid:15)t,¯νt,η

24

Thus, ´κf ¯α2

t p ¯∇Lt

ˇ
ˇ ¯Lt

¯(cid:15)t,¯νt,η ´ Lt

¯(cid:15)t,¯νt,η

¯(cid:15)t,¯νt,ηqT q∆t ą 0 as long as q∆t ‰ 0. Moreover
ˇ
ˇ
ˇ
ˇ _
¯(cid:15)t,¯νt,η
´ˇ
ˇ ¯f t ´ f t

ˇ
ˇ ¯f st ´ f st

¯(cid:15)t,¯νt,η ´ Lst

ˇ
ˇ ¯Lst

ď O

_

ˇ
ˇ

ˇ
ˇ

›
›
› ¯¯∇f t ´ ∇f t

›
›
› _

›
›
› ¯¯∇f st ´ ∇f st

›
¯
›
›

.

_

Thus, by Bernstein inequality,

`
E t
2 | Ft´0.5

˘

P

ě 1 ´ pf

if

|ξt

2| ě O

˜

logpd{pf q

f ¯α4
κ2

t pp ¯∇Lt

¯(cid:15)t,¯νt,ηqT q∆tq2

¸

.

(45)

ˇ
ˇ ¯f t ´ f t
Furthermore, we have Er
2| ě Op1{¯δ2
satisﬁed if |ξt
t q. Together with (45), we have
˜

| Ft´0.5s ď Op1{|ξt

ˇ
ˇ2

2|q and the same for other terms. Thus, (35) is

|ξt

2| ě O

logpd{pf q
¯(cid:15)t,¯νt,ηqT q∆tq2 ^ ¯δ2

t

f ¯α4
κ2

t pp ¯∇Lt

¸

.

(46)

Diﬀerent from choosing |ξt
2; hence a While loop is
not needed. We also note that the batch sizes (44) and (46) are the same as the ones in Cartis and
Scheinberg (2017); Paquette and Scheinberg (2020); Na et al. (2021).

2| in (46) does not depend on ξt

1|, the bound on |ξt

5 Numerical Experiments

We implement the following three algorithms on 39 nonlinear problems collected in CUTEst test set
(Gould et al., 2014). We select problems that have a non-constant objective with less than 1000
free variables. We also require problems to have at least one inequality constraint, no infeasible
constraints, no network constraints, and the number of constraints is less than the number of
variables. The setup of each algorithm is as follows.

(a) NonAdap: the non-adaptive scheme in Section 3. We let (cid:15) “ 0.001 and ν “ 2apx0q ` 1, where
x0 is the initial point. Although ν is ﬁxed and not adaptive for the scheme in Section 3, we
prefer to adaptively set it in the implementation because it is easily achievable. In particular,
given the t-th iterate pxt, µt, λtq, we ﬁrst check if xt P Tν. If xt R Tν, we let

ν Ð ρjν with

j “ rlogp2apxtq{¯νtq{ log ρs,

so that xt P Tν. Then, we follow the scheme by ﬁrst identifying the active set At
(cid:15),ν as in (12),
followed by solving the coupled Newton system (14), and ﬁnally updating the iterate (15)
with αt. We try six stepsizes, including four constant stepsizes αt “ 0.01, 0.1, 0.5, 1 and two
decaying stepsizes αt “ 1{t0.6, 1{t0.9. If (14) is not solvable in some iteration, we immediately
stop the procedure.

(b) AdapNewton: the adaptive scheme in Algorithm 1 with the alternative search direction given
by the regularized Newton (30). We let ¯α0 “ αmax “ 1.5, η “ 0.001, γB “ 0.1, ¯ν0 “ 2apx0q`1,
¯(cid:15)0 “ ¯δ0 “ 1, β “ 0.3, ρ “ 2, κgrad “ 1, κf “ β{p4αmaxq “ 0.05, pgrad “ pf “ 0.1. When
using (44) and (46) for deciding batch sizes, we try multiple constants C P t1, 5, 10, 50u in the
big “O” notation to test the sensitivity of the algorithm to parameters. Note that parameters
pf , pgrad, κf , κgrad play the same role as the constant C; all of them only aﬀect the batch sizes.

25

(c) AdapGD: the adaptive scheme in Algorithm 1 with the alternative search direction obtained

by the steepest descent, i.e., (cid:98)H t “ I in (30). We use the same setup as (b).

For all algorithms, the initial iterate px0, µ0, λ0q is speciﬁed by CUTEst package. The package
also provides the deterministic function, gradient and Hessian evaluation, f t, ∇f t, ∇2f t in each
iteration. We generate their stochastic counterparts by adding a Gaussian noise with variance σ2.
In particular, we let ¯f t „ N pf t, σ2q, ¯∇f t „ N p∇f t, σ2pI ` 11T qq, and p ¯∇2f tqij „ N pp∇f tqij, σ2q.
We try ﬁve levels of variance: σ2 P t10´8, 10´4, 10´2, 10´1, 1u. Throughout the implementation, we
let Bt “ I and set the maximum iteration budget to be 105. The stopping criteria is

¯αt}

q∆t} ď 10´6 OR Rt ď 10´5 OR t ě 105.

The former two cases suggests that the iteration converges within the budget. For each algorithm,
each problem, and each setup, we average the results of all convergent runs among 5 runs. Our code is
available at https://github.com/senna1128/Constrained-Stochastic-Optimization-Inequality.
Figure 1 shows boxplots of the KKT residuals of NonAdapSQP. For comparisons, we also show
the results of AdapNewton and AdapGD with C “ 1. We see that NonAdapSQP does not perform
well, and does not even converge for most cases with large stepsizes and large noise variance. This
is consistent with our analysis in Section 3; the scheme requires a good initial point to make the
identiﬁed active set accurate and further make SQP direction eﬀective. This illustrates the necessity
of our design in Section 4 (especially Step 3).

Figure 2 shows boxplots of the KKT residuals of AdapNewton and AdapGD. For both methods,
the median of KKT residuals increases when σ2 increases, which is consistent with the intuition.
On the other hand, the diﬀerences between noise levels σ2 are mild; this is because both methods
generate a batch of samples in each iteration, thus the variance of estimates is suﬃciently reduced.
Figure 2 also shows that AdapNewton and AdapGD do not diﬀer much in the result. This is
reasonable because the SQP direction will always be employed eventually. We also see that both
methods are robust to tuning parameters.

Figures 3 and 4 show the total number of samples generated for evaluating the objective and its
gradient. We see that, when using diﬀerent constants C, AdapNewton requires less samples than
AdapGD, although the improvement in gradient evaluation is more signiﬁcant. This is because,
even though the calculation of the regularized Hessian (cid:98)H t is heavier than the steepest descent,
AdapNewton could converge to a local neighborhood of the KKT point faster than AdapGD due to
the second-order information in (cid:98)H t.

Figure 5 plots the stepsize processes selected by stochastic line search for both AdapNewton
and AdapGD. For each setup of σ2, we randomly pick 5 convergent problems to show the process.
Although there is no clear trend for the process due to the stochasticity, we see that the stepsize
can increase signiﬁcantly from a very small value and even exceed 1. This exclusive property of
line search ensures a fast convergence of the scheme, compared to non-adaptive schemes that use
deterministic prespeciﬁed stepsize sequences.

6 Conclusion

This paper studied inequality constrained stochastic nonlinear optimization problems. We designed
an active-set StoSQP algorithm that exploits the exact augmented Lagrangian merit function. The
algorithm adaptively selects the penalty parameters of the augmented Lagrangian and selects the
stepsize via stochastic line search. We proved that the “liminf” of KKT residuals converges to zero

26

(a) αt “ 0.01

(b) αt “ 0.1

(c) αt “ 0.5

(d) αt “ 1

(e) αt “ k´0.6

(f) αt “ k´0.9

Figure 1: KKT residual boxplots. Each ﬁgure corresponds to a setup of αt for NonAdap. The results
of AdapNewton and AdapGD do not change across ﬁgures and correspond to the setup with C “ 1.

27

(a) C “ 1

(b) C “ 5

(c) C “ 10

(d) C “ 50

Figure 2: KKT residual boxplots. Each panel corresponds to a constant setup.

(a) C “ 1

(b) C “ 5

(c) C “ 10

(d) C “ 50

Figure 3: Objective evaluation boxplots. Each panel corresponds to a constant setup.

28

(a) C “ 1

(b) C “ 5

(c) C “ 10

(d) C “ 50

Figure 4: Gradient evaluation boxplots. Each panel corresponds to a constant setup.

(a) AdapNewton

(b) (cid:96)1 AdapGD

Figure 5: Stepsize process. Each ﬁgure has ﬁve rows, from top to bottom, corresponding to
σ2 “ 10´8, 10´4, 10´2, 10´1, 1. Each plot has 5 lines corresponding to the stepsizes sequences of 5
convergent problems. The dash line corresponds to the unit stepsize.

29

almost surely, which generalizes the result for equality constrained stochastic problems (Na et al.,
2021) to enable wider and more realistic applications.

The extension of this work includes studying more advanced StoSQP schemes. For example,
recently, Curtis et al. (2021b) designed a StoSQP where an inexact Newton direction is employed;
Berahas et al. (2021b) designed a StoSQP to relax LICQ condition. It is still open how to design
related algorithms to achieve relaxation with inequality constraints. Besides SQP, there are other
classical schemes for solving nonlinear problems that can be exploited to solve stochastic objectives,
such as the augmented Lagrangian method and interior point method. Diﬀerent methods have
diﬀerent beneﬁts and all of them deserve studying in future.

Finally, it is known in the deterministic regime that the diﬀerentiable merit functions can
overcome the Maratos eﬀect locally and achieve fast local rate, while non-smooth merit functions
(without advanced local adjustment) cannot. This raises questions: what is the local rate of the
proposed StoSQP, and is the local rate better than the one using non-smooth merit functions, as it
is the case in the deterministic regime? To answer these questions, we need to understand the local
behavior of stochastic line search. Such a local study would complement the existing global analysis
and bridge the gap between stochastic SQP and deterministic SQP.

Acknowledgments

This material was completed in part with resources provided by the University of Chicago Research
Computing Center. This material was based upon work supported by the U.S. Department of
Energy, Oﬃce of Science, Oﬃce of Advanced Scientiﬁc Computing Research (ASCR) under Contract
DE-AC02-06CH11347 and by NSF through award CNS-1545046.

A Proofs of Section 2

A.1 Proof of Lemma 2.2

By Lemma 2.1 and w(cid:15),νpx‹, λ‹q “ 0, we know gpx‹q ď 0, λ‹ ě 0, pλ‹qT gpx‹q “ 0. This
implies diag2pgpx‹qqλ‹ “ 0. Furthermore, by cpx‹q “ 0, w(cid:15),νpx‹, λ‹q “ 0, aνpx‹q, η, (cid:15) ą 0, and
∇µ,λL(cid:15),ν,ηpx‹, µ‹, λ‹q “ 0, we obtain from (10) that

ˆ

˙ ˆ

˙

M11px‹q M12px‹q
M21px‹q M22px‹q

Jpx‹q
Gpx‹q

∇xLpx‹, µ‹, λ‹q “ 0.

(A.1)

Recalling the deﬁnition of M px‹q in (9) and denoting ∇L‹ “ ∇Lpx‹, µ‹, λ‹q, we multiply the
matrix ∇T

xL‹pJ T px‹q GT px‹qq from the left and have

(A.1)
“ ∇T

0

xL‹
`

`

J T px‹q GT px‹q

˘ ´

Jpx‹qJ T px‹q
Gpx‹qJ T px‹q Gpx‹qGT px‹q`diag2pgpx‹qq

Jpx‹qGT px‹q

¯ ˆ

˙

Jpx‹q
Gpx‹q

∇xL‹

" ˆ

Jpx‹q
Gpx‹q

˙

ˆ

“ ∇T

xL‹

˘
J T px‹q GT px‹q

`

˘
J T px‹q GT px‹q

›
`
›

˘
J T px‹qJpx‹q ` GT px‹qGpx‹q

“

∇xL‹

` }diagpgpx‹qqGpx‹q∇xL‹}2 .

`

0

0
0 diag2pgpx‹qq
›
›2

˙ * ˆ

˙

Jpx‹q
Gpx‹q

∇xL‹

30

˘
`
J T px‹qJpx‹q ` GT px‹qGpx‹q

∇xL‹ “ 0. Multiplying ∇xL‹ from the left, we further
This implies
have Jpx‹q∇xL‹ “ 0 and Gpx‹q∇xL‹ “ 0. Plugging into (10) and noting that ∇xL(cid:15),ν,η “ 0,
qνpx‹, λ‹q ą 0, and diag2pgpx‹qqλ‹ “ 0, we obtain ∇xL‹ “ 0. This shows px‹, µ‹, λ‹q satisﬁes (4)
and completes the proof.

B Proofs of Section 3

We use Υ1, Υ2 . . . to denote generic upper bounds, which are independent of p(cid:15), ν, η, γH , γBq, but
may change from line to line. An exception is the proof of Theorem 3.8, where we use C1, C2 . . . to
denote generic upper bounds that are independent of the stepsize αt. Without loss of generality,
we assume Υi ě 1, @i. The existence of Υi, Ci is ensured by the compactness of the iterates, i.e.,
the assumption that px, µ, λq P X ˆ M ˆ Λ. Under the above setting, we only track the constants
p(cid:15), ν, η, γH , γBq in the proofs of all results. The exception is the stepsize in the proof of Theorem 3.8.

B.1 Proof of Lemma 3.2

To prove Lemma 3.2, we require the following lemma.

Lemma B.1. For any two scalars a, b and a scalar c ą 0, | maxta, bu| ď 1

c^1 | maxta, cbu|.

c a “ 1

c | maxta, cbu|. If cb ă b ă a, then | maxta, bu| “ a ď 1

c | maxta, cbu|. If cb ă a ď b, then
c | maxta, cbu|. Thus,

Proof. Without loss of generality, we assume b ‰ 0 and c ‰ 1. We consider four cases.
Case 1: b ą 0, c ă 1. If a ď cb ă b, then | maxta, bu| “ b “ 1
| maxta, bu| “ b ď 1
the result holds.
Case 2:
b ą 0, c ą 1. If a ď b ă cb, then | maxta, bu| “ b ď cb “ | maxta, cbu|. If b ă a ď cb, then
| maxta, bu| “ a ď cb “ | maxta, cbu|. If b ă cb ă a, then | maxta, bu| “ a “ | maxta, cbu|. Thus, the
result holds.
Case 3: b ă 0, c ă 1. If a ď b ă cb, then | maxta, bu| “ |b| “ 1
| maxta, bu| “ |a| ď |b| “ 1
Thus, the result holds.
Case 4: b ă 0, c ą 1. If a ď cb ă b, then | maxta, bu| “ |b| ď c|b| “ | maxta, cbu|. If cb ă a ď b, then
| maxta, bu| “ |b| ď |a| “ | maxta, cbu|. If cb ă b ă a, then | maxta, bu| “ |a| “ | maxta, cbu|. Thus,
the result holds.
Combining the above four cases, we complete the proof.

c | maxta, cbu|. If b ă a ď cb, then
c “ 1
c | maxta, cbu|.

c | maxta, cbu|. If b ă cb ă a, then | maxta, bu| “ |a| ď |a|

Since (cid:15), ν ą 0, px, λq P Tν ˆ Rr, and qνpx, λq ą 0, we have for any i P t1, 2, . . . , ru,

|pw(cid:15),νpx, λqqi| “ | maxtgipxq, ´(cid:15)qνpx, λqλiu| ď

1
(cid:15)qν px,λq ^ 1

1

| maxtgipxq, ´λiu|

“ p(cid:15)qνpx, λq _ 1q ¨ | maxtgipxq, ´λiu| ď

(cid:15)qνpx, λq _ 1
(cid:15)qνpx, λq ^ 1

| maxtgipxq, ´(cid:15)qνpx, λqλiu|

“

(cid:15)qνpx, λq _ 1
(cid:15)qνpx, λq ^ 1

|pw(cid:15),νpx, λqqi| ,

where both inequalities are from Lemma B.1. Taking (cid:96)2 norm on both sides, we ﬁnish the proof.

31

B.2 Proof of Lemma 3.3

Conditioning on pxt, µt, λtq, the active set At
comes from sampling of ¯∇xLt and ¯Qt
¸ﬀ

«˜

1, ¯Qt
ˆ

„

2. We have

Et

¯∆xt
¯
(cid:101)∆µt
¯
(cid:101)∆λt
a

(14a)
“ ´ Et

˙

„ˆ

aq´1
pKt
ˆ

¯∇xLt´pGt

cqT λt
c

ct
gt
a

∇xLt´pGt

cqT λt
c

ct
gt
a

˙

ˆ

“

aq´1Et

“ ´pKt
˙

∆xt
(cid:101)∆µt
(cid:101)∆λt
a

,

“ ´ pKt

aq´1

˙

¯∇xLt´pGt

cqT λt
c

ct
gt
a

(cid:15),ν in (12) is ﬁxed. Thus, the randomness in (14) only

and

„ˆ

Et

¯∆µt
¯∆λt

˙

„

(14b)
“ ´ Et

"ˆ

pM tq´1
"

„ˆ

J t ¯∇xLt
Gt ¯∇xLt ` Πcpdiag2pgtqλtq
J t ¯∇xLt
Gt ¯∇xLt ` Πcpdiag2pgtqλtq

J t∇xLt
Gt∇xLt ` Πcpdiag2pgtqλtq
J t∇xLt
Gt∇xLt ` Πcpdiag2pgtqλtq

˙

ˆ

˙

*

`

˙

p ¯Qt
1qT
p ¯Qt
2qT
„ˆ

¯∆xt
˙

` Et
„ˆ

p ¯Qt
p ¯Qt
˙

1qT
2qT

*

¯∆xt

*

‰

˙

˙

` Et
ˆ

`

p ¯Qt
1qT
p ¯Qt
2qT
˙
1qT
2qT

pQt
pQt

“

Et
*

¯∆xt
ˆ

˙

,

∆µt
∆λt

∆xt

“

“ ´ pM tq´1

“ ´ pM tq´1

“ ´ pM tq´1

Et
"ˆ

"ˆ

where the third equality is due to the independence between ξt

2. Moreover,

Etr} ¯∆xt}2s

¯∇xLt´pGt

cqT λt
c

ct
gt
a

1 and ξt
ﬀ
˙›
›
2
›
›

˙

ˆ

«›
›
`
›
›

I 0 0

`
I 0 0

˘

(14a)
“ Et
«›
›
›
›

“Et

ˆ

˘

pKt

aq´1

"ˆ

∇xLt´pGt

cqT λt
c

pKt

aq´1

`

ct
gt
a
aq´1}2Etr}∇f pxt; ξt
1q ´ ∇f pxtq}2s
aq´1}2ψg ď p1 _ }pKt

aq´1}2q

`

ď}∆xt}2 ` }pKt
ď}∆xt}2 ` }pKt

∇f pxt;ξt

1q´∇f pxtq
0
0

ﬀ

˙*›
›
2
›
›

}∆xt}2 ` ψg

˘

,

(B.1)

where the third inequality is because the cross term has mean zero. Similarly,

«›
›
›
›

Et

ˆ

¯∆µt
¯∆λt

ﬀ

˙›
›
2
›
›

"ˆ

«›
›
›
›pM tq´1
˙›
›
2
›
›

(14b)
“ Et
›
ˆ
›
›
›

ď

J t ¯∇xLt
Gt ¯∇xLt ` Πcpdiag2pgtqλtq
ˆ

˙

∆µt
∆λt
ˆ

`

` }pM tq´1}2Et
˙

ˆ

p ¯Qt
p ¯Qt

1qT
2qT

¯∆xt ´

pQt
pQt

1qT
2qT

„›
›
›
›

J t
Gt
˙

p∇f pxt; ξt
›
›
2
›
›



.

∆xt

˙

ˆ

`

p ¯Qt
p ¯Qt

1qT
2qT

˙

ﬀ

2

*›
›
›
›

¯∆xt

1q ´ ∇f pxtqq

(B.2)

By the condition in the lemma, we have

Etr} ¯∆xt ´ ∆xt}2s

(14a)
“ Et

«›
›
›
›

`
I 0 0

˘

pKt

aq´1

ˆ

∇f pxt;ξt

1q´∇f pxtq
0
0

ﬀ

˙›
›
2
›
›

ď }pKt

aq´1}2ψg.

(B.3)

32

Furthermore, using the decomposition
ˆ
ˆ

˙

˙

ˆ

p ¯Qt
p ¯Qt

1qT
2qT

¯∆xt ´

∆xt “

pQt
pQt

1qT
2qT

p ¯Qt
p ¯Qt

1 ´ Qt
2 ´ Qt

1qT
2qT

˙

`

p ¯∆xt ´ ∆xtq
ˆ

˙

pQt
pQt

1qT
2qT

p ¯∆xt ´ ∆xtq `

ˆ

p ¯Qt
p ¯Qt

1 ´ Qt
2 ´ Qt

1qT
2qT

˙

∆xt

and the independence between ξt

1 and ξt

p∇f pxt; ξt

1q ´ ∇f pxtqq `

«›
ˆ
›
›
›

Et

˙

J t
Gt
«›
ˆ
›
›
›

˙

J t
Gt

2, we have
ˆ
˙
p ¯Qt
1qT
p ¯Qt
2qT
ˆ

ˆ

pQt
pQt

1qT
2qT

˙

∆xt

›
›
2
›
›

ˆ

¯∆xt ´

˙

p ¯∆xt ´ ∆xtq `

ﬀ

˙

ﬀ

›
›
2
›
›

∆xt

p ¯Qt
p ¯Qt

1 ´ Qt
2 ´ Qt

1qT
2qT

“ Et

p∇f pxt; ξt

1q ´ ∇f pxtqq `

«›
ˆ
›
›
›

˙

p ¯Qt
p ¯Qt

1 ´ Qt
2 ´ Qt

1qT
2qT

` Et

p ¯∆xt ´ ∆xtq

1qT
pQt
2qT
pQt
ﬀ
›
›
2
›
›

.

Since pxt, µt, λtq P X ˆ M ˆ Λ, there exist constants Υ1, Υ2, Υ3 ą 0 independent of (cid:15), ν, η, such that

›
›
› ď Υ1,
›ppJ tqT pGtqT q

›
›pQt

1 Qt
2q

›
› ď Υ2,

and

«›
›
›
›

Et

ˆ

p ¯Qt
p ¯Qt

1 ´ Qt
2 ´ Qt

1qT
2qT

ﬀ

˙›
›
2
›
›

`

Et

ďΥ3

“
}∇2f pxt; ξt

2q ´ ∇2f t}2

‰

“

` Et

}∇f pxt; ξt

2q ´ ∇f t}2

‰˘

ďΥ3 pψH ` ψgq .

Combining the above three displays,

«›
›
›
›

Et

ˆ

˙

J t
Gt

p∇f pxt; ξt

ˆ

1q ´ ∇f pxtqq `

1qT
2qT
1q ´ ∇f pxtq}2s ` 3Υ2
2

p ¯Qt
p ¯Qt

Etr}∇f pxt; ξt

ď 3Υ2
1
` Υ3pψg ` ψH qEtr} ¯∆xt ´ ∆xt}2s

˙

ˆ

¯∆xt ´

pQt
pQt

1qT
2qT

˙

ﬀ

2

›
›
›
›

∆xt

Etr} ¯∆xt ´ ∆xt}2s ` 3Υ3pψg ` ψH q}∆xt}2

2}pKt

1ψg ` 3Υ2

(B.3)
ď 3Υ2
“ 3Υ3pψg ` ψH q}∆xt}2 `
}∆xt}2 ` p1 ` }pKt
ď Υ4

(cid:32)

3Υ2
aq´1}2qψg

(

,

aq´1}2ψg ` 3Υ3pψg ` ψH q}∆xt}2 ` Υ3pψg ` ψH q}pKt
˘
2 ` Υ3pψg ` ψH q

aq´1}2ψg ` 3Υ2

¨ }pKt

1ψg

`

aq´1}2ψg

where we deﬁne

Υ4 “ 3Υ3pψg ` ψH q _ 3Υ2

2 ` Υ3pψg ` ψH q _ 3Υ2
1

to let the last inequality hold. Combining the above display with (B.2) and using Υ4 ě 1,

ˆ

«›
›
›
›

Et

ﬀ

˙›
›
2
›
›

ˆ

›
›
›
›

ď

˙›
›
2
›
›

∆µt
∆λt

¯∆µt
¯∆λt

` Υ4}pM tq´1}2

(cid:32)

}∆xt}2 ` p1 ` }pKt

aq´1}2qψg

(

ďp1 ` Υ4}pM tq´1}2q}∆t}2 ` Υ4}pM tq´1}2p1 ` }pKt
aq´1}2qψg
ď2Υ4p1 _ }pM tq´1}2q}∆t}2 ` 2Υ4p1 _ }pM tq´1}2qp1 _ }pKt
ď2Υ4p1 _ }pM tq´1}2qp1 _ }pKt

}∆t}2 ` ψg

aq´1}2q

`

˘

.

aq´1}2qψg

33

Combining with (B.1), we complete the proof by deﬁning Υ∆ :“ 2Υ4 ` 1.

B.3 Proof of Lemma 3.5

Let radiuspΛq “ maxλPΛ }λ}. Then, for any px, λq P X ˆ Λ, we have

qνpx, λq ě

ν
2

¨

1
1 ` radius2pΛq

“: κν.

(B.4)

For any i P I `px‹, λ‹q, we know gipx‹q “ 0 and λ‹
ball Bx
i “ tx : }x ´ x‹} ď riu X X and Bλ
(depending on (cid:15), ν), we have Bx

i Ď X ˆ Λ and

i ˆ Bλ

i ą 0. Thus, gipx‹q ` (cid:15)κνλ‹

i ą 0. Consider the
i “ tλ : }λ ´ λ‹} ď riu X Λ. For a suﬃciently small ri

gipxq ě ´(cid:15)κνλi

(B.4)
ě ´(cid:15)qνpx, λqλi.

The ﬁrst inequality is due to the continuity of gi. This implies i P A(cid:15),νpx, λq. Thus, for any px, λq
in the convex compact set XiPI`px‹,λ‹qBx
i , we have I `px‹, λ‹q Ď A(cid:15),νpx, λq. The argument
A(cid:15),νpx, λq Ď Ipx‹q can be proved in the same way.

i ˆ Bλ

B.4 Proof of Lemma 3.7

We suppress the iteration index t. Recall from the beginning of Appendix B that Υ1, Υ2 . . . are generic
upper bounds that are independent of p(cid:15), ν, η, γB, γH q. As they are upper bounds, without loss of
generality, Υi ě 1, @i. We conduct our analysis in any convex compact set X ˆMˆΛ Ď Tν ˆRm ˆRr
around px‹, µ‹, λ‹q, and will ﬁnally restrict to a subset. All bounds that hold for points in X ˆMˆΛ
also hold for points in any subset.

We start from p∇Lp1q

(cid:15),ν,ηqT ∆. We have

`

˘ ´

Q1 Q2
ˆ

˙T ˆ

J∇xL
G∇xL`Πcpdiag2pgqλq
ˆ

˙

˙T ˆ

∆xT J T c
˙ ´

¯

`

1
(cid:15)

∆µ
∆λ

c
w(cid:15),ν

` η

∆µ
∆λ
`

∆xT GT w(cid:15),ν `

∆µ
∆λ

M11 M12
M21 M22
˘T

c

p

w(cid:15),ν q ´ η
´

1
(cid:15)qν

∆xT GT

a ga ` p c

ga qT

¯

J∇xL
G∇xL`Πcpdiag2pgqλq
›
´
›
›

J∇xL
G∇xL`Πcpdiag2pgqλq

¯

∆µ
∆λa

´ (cid:15)qν∆λT

c λc

(cid:15),ν,ηqT ∆

p∇Lp1q
(18)
“ ∆xT ∇xL ` η∆xT

`

1
(cid:15)qν

∆xT GT w(cid:15),ν `

(14b)
“ ∆xT ∇xL `

1
(cid:15)

∆xT J T c `

(7),(12)

“ ∆xT p∇xL ´ GT
›
´
›
›

´ η

c λcq `

J∇xL
G∇xL`Πcpdiag2pgqλq
ˆ

˜

˙T

1
(cid:15)qν
∆xT J T c `
¯›
›
›

2

1
(cid:15)

¸

(14a)
“ ´∆xT B∆x `
›
´
›
›

´ η

c
ga

J∇xL
G∇xL`Πcpdiag2pgqλq

(cid:101)∆µ ` ∆µ
(cid:101)∆λa ` ∆λa
2

¯›
›
›

.

¯›
›
2
›

(B.5)

´

1
(cid:15)

}c}2 ´

1
(cid:15)qν

}ga}2 ´ (cid:15)qν∆λT

c λc

34

Since px, µ, λq P X ˆ M ˆ Λ, there exists Υ1 ě 1 such that

›
ˆ
›
›
›

˙›
›
›
›

∆µ
∆λ

(14b)
“

(21)
ď

›
›
›M ´1
›
´
›
›

1
γH

´

¯

J∇xL
G∇xL`Πcpdiag2pgqλq
¯›
›
› `

J∇xL
G∇xL`Πcpdiag2pgqλq

` M ´1

´

QT
1
QT
2

Υ1
γH

}∆x} ď

›
›pQt
¯

1 Qt
2q
›
›
›
∆x
›
ˆ
›
›
›

2Υ1
γH

›
› ď Υ1. Thus,

∆x
J∇xL
G∇xL`Πcpdiag2pgqλq

˙›
›
›
› . (B.6)

Moreover, we have

"´

¯ `

J
Ga
Gc

J T GT

a Gc

ˆ

˘

`
´

J
Ga
Gc

(14a)
“ ´

0
0
0 diag2pgaq
0
0
¯

0
0
diag2pgcq
ˆ

˙* ˆ

˙

(cid:101)∆µ
(cid:101)∆λa
´λc

J∇xL
Ga∇xL
Gc∇xL`diag2pgcqλc
ˆ

˙

ˆ

`

˙

0
diag2pgaq (cid:101)∆λa
0
ˆ
˙

B∆x ´
¯

´

(14a)
“ ´

J
Ga
Gc

B∆x ´

J∇xL
Ga∇xL
Gc∇xL`diag2pgcqλc

´

0
diagpgaqdiagp (cid:101)∆λaqGa∆x
0

˙

.

Again, since px, µ, λq P X ˆ M ˆ Λ, there exist Υ2, Υ3, Υ4 ě 1 such that

›
›ppJ tqT pGtqT q

›
› ď Υ2,

} (cid:101)∆λa}

(14a)
ď

ˆ

›
›
›
›pKt

aq´1

∇xLt´pGt

cqT λt
c

ct
gt
a

˙›
›
›
›

(23)
ď

Υ3
γH γB

,

and further

}diagpgaqdiagp (cid:101)∆λaqGa} ď

Υ4
γH γB

.

Combining the above three displays and noting that γH _ γB ď 1,

›
ˆ
›
›
›

(cid:101)∆µ
(cid:101)∆λa
´λc

˙›
›
›
›

(21)
ď

1
γH

ˆ

Υ2}B∆x} `

˙

}∆x}

Υ4
γH γB

`

ˆ

›
›
›
›

1
γH
Υ2ΥB ` Υ4 ` 1
γ2
H γB

J∇xL
G∇xL ` Πcpdiag2pgqλq
ˆ
∆x
J∇xL
G∇xL`Πcpdiag2pgqλq

›
›
›
›

ď

˙›
›
›
›

˙›
›
›
› ,

(B.7)

where the second inequality uses }B} ď ΥB from Assumption 3.6. Combining (B.5), (B.6), (B.7),
and using 0 ă qν ď ν and γH _ γB ď 1,
›
ˆ
›
›
›

(B.5)
ď ´∆xT B∆x `

˙›
›
2
›
›

p∇Lp1q

˙›
›
›
›

˙›
›
›
›

›
ˆ
›
›
›

›
ˆ
›
›
›

(cid:15),ν,ηqT ∆

+

´

#›
˜
›
›
›
›

(cid:101)∆µ
(cid:101)∆λa

¸›
›
›
›
› `

1
(cid:15)p1 _ νq

c
ga

∆µ
∆λa

c
ga
›
ˆ
›
›
›

J∇xL
G∇xL ` Πcpdiag2pgqλq

2

˙›
›
›
›

›
ˆ
›
›
›

˙›
›
›
›

2Υ1 ` Υ2ΥB ` Υ4 ` 1
γ2
H γB
2Υ1pΥ2ΥB ` Υ4 ` 1q
γ3
H γB

` (cid:15)ν ¨

c
ga

˙›
›
2
›
›

ˆ

ˆ

›
›
›
›
›
›
›
›

∆x
J∇xL
G∇xL`Πcpdiag2pgqλq

∆x
J∇xL
G∇xL`Πcpdiag2pgqλq

˙›
›
›
›
˙›
›
›
›

2

›
›
›
›

ˆ

c
ga

` (cid:15)ν}∆λc}}λc} ´ η

(B.6)
(B.7)
ď ´∆xT B∆x `

1
(cid:15)p1 _ νq
›
ˆ
›
›
›

´

´ η

J∇xL
G∇xL ` Πcpdiag2pgqλq

2

˙›
›
›
›

35

ď ´∆xT B∆x `

ˆ

›
›
›
›

`

(cid:15)νΥ5
γ3
H γB

ˆ

›
›
›
›

˙›
›
›
›

c
ga

Υ5
γ2
H γB
∆x
J∇xL
G∇xL`Πcpdiag2pgqλq

ˆ

›
›
∆x
›
J∇xL
›
G∇xL`Πcpdiag2pgqλq
˙›
›
2
›
›

›
ˆ
›
›
›

´ η

˙›
›
›
› ´

1
(cid:15)p1 _ νq

2

˙›
›
›
›

›
ˆ
›
c
›
›
ga
˙›
›
2
›
›

,

(B.8)

J∇xL
G∇xL ` Πcpdiag2pgqλq

where the last inequality holds by deﬁning

Υ5 “ 2Υ1 ` Υ2ΥB ` Υ4 ` 1 _ 2Υ1pΥ2ΥB ` Υ4 ` 1q.

To deal with ∆xT B∆x in (B.8), we decompose ∆x as ∆x “ ∆u ` ∆v where
(

(cid:32)

(

(cid:32)

∆u P Image

pJ T GT
a q

and ∆v P Ker

pJ T GT

a qT

.

Note that
ˆ

˙

˙

ˆ

J
Ga

“

∆x “

˙

ˆ

J
Ga

∆u ùñ∆µ “ ´

`

c
ga

´

(21)
ùñ}∆µ} ď

Thus, by Assumption 3.6,

´∆xT B∆x

"ˆ

˘

J
Ga

˙

`

˘

J T GT
a

˙

´1 ˆ
*

c
ga

(B.9)

J T GT
a
˙›
›
ˆ
›
›
›
›
› .
›

c
ga

Υ2
γH

“ ´ ∆vT B∆v ´ 2∆uT B∆v ´ ∆uT B∆u ď ´γB}∆v}2 ` 2ΥB}∆v}}∆u} ` ΥB}∆u}2
3γB
4
ˆ

4Υ2
B
γB

q}∆u}2

3γB
4

3γB
4

ď ´

`

q}∆u}2 “ ´
›
›
›
›

3γB
4

Υ2
2
γ2
H

q

}∆x}2 ` pΥB `
˙›
›
2
›
›

c
ga

}∆v}2 ` pΥB `

4Υ2
B
γB
4Υ2
}∆x}2 ` pΥB `
B
γB
›
ˆ
›
›
›

}∆x}2 `

Υ6
γ2
H γB

c
ga

`
˙›
›
2
›
›

,

(B.10)

(B.9)
ď ´

3γB
4

ď ´

3γB
4

where we let

Υ6 “ Υ2

2pΥB ` 4Υ2

B ` 1q,

and the third inequality is by Young’s inequality: 2ΥB}∆v}}∆u} ď γB}∆v}2{4 ` 4Υ2
Combining the above display with (B.8) and using the following Young’s inequality,

B}∆u}2{γB.

›
›
›
›

ˆ

c
ga

ˆ

˙›
›
›
›

›
›
›
›

Υ5
γ2
H γB

∆x
J∇xL
G∇xL`Πcpdiag2pgqλq

˙›
›
›
›

´

ď

γB
8

^

η
4

¯ ›
ˆ
›
›
›

∆x
J∇xL
G∇xL`Πcpdiag2pgqλq

˙›
›
2
›
›

`

2Υ2
5
BpγB ^ ηq

H γ2
γ4

›
ˆ
›
›
›

c
ga

2

˙›
›
›
›

,

36

we have

p∇Lp1q

(cid:15),ν,ηqT ∆ ď ´
"

3γB
4

}∆x}2 `

"´

¯

γB
8

^

η
4

∆x
J∇xL
G∇xL`Πcpdiag2pgqλq

2

˙›
›
›
›

* ›
ˆ
›
›
›
˙›
›
2
›
›

`

(cid:15)νΥ5
γ3
H γB
* ›
ˆ
›
c
›
›
ga

›
ˆ
›
›
›

´ η

J∇xL
G∇xL ` Πcpdiag2pgqλq

2

˙›
›
›
›

`

ď ´

"

´

`

Υ6
γ2
H γB
"
γB ^ η
2

2Υ2
5
H γ2
γ4
BpγB ^ ηq
¯
´
γB
8

`

´

1
(cid:15)p1 _ νq
* ›
ˆ
›
›
›

(cid:15)νΥ5
γ3
H γB

1
(cid:15)p1 _ νq

´

H γ2
γ4

´

Υ6
γ2
H γB

´

^

η
4
2Υ2
5
BpγB ^ ηq

˙›
›
2
›
›

∆x
J∇xL
G∇xL`Πcpdiag2pgqλq
˙›
* ›
ˆ
›
›
2
›
›
›
›

.

c
ga

Therefore, as long as

γB
8

^

η
4

ě

(cid:15)νΥ5
γ3
H γB

ðù

1
(cid:15)p1 _ νq

´

2Υ2
5
BpγB ^ ηq

H γ2
γ4

´

Υ6
γ2
H γB

ě 0 ðù

1
(cid:15)

1
(cid:15)

ě

ě

8νΥ5
γ3
H γBpγB ^ ηq
p1 _ νqp2Υ2
H γ2
γ4

BpγB ^ ηq

5 ` Υ6q

,

(B.11a)

,

(B.11b)

we have

p∇Lp1q

(cid:15),ν,ηqT ∆ ď ´

›
ˆ
›
›
›

γB ^ η
2

∆x
J∇xL
G∇xL`Πcpdiag2pgqλq

˙›
›
2
›
›

.

Thus, letting Υ “ 8Υ5 _ 2Υ2
ﬁrst part of the statement.

5 ` Υ6 and noting that (B.11a) is implied by (B.11b), we complete the

We now prove the second part of the statement. By (18), (B.4), the compactness of the iterates,

and the fact that aν ě ν{2, there exists Υ7 ą 0 such that

p∇Lp2q

(cid:15),ν,ηqT ∆

(18)
“

3}w(cid:15),ν}2
2(cid:15)qνaν

}w(cid:15),ν}2
(cid:15)aν

∆λT λ

∆xT GT l ` η∆xT Q2,adiag2pgaqλa `
˙

ˆ

` ηp∆µT ∆λT q
"

`

M12,a
M22,a

}ga}2 ` (cid:15)2ν2}λc}2

diag2pgaqλa

˘

}∆x} ` η}ga}2}∆x}

`

}ga}2 ` (cid:15)2ν2}λc}2

˘

}∆λ} ` η}ga}2}p∆µ, ∆λq}

.

*

ďΥ7

`

1
(cid:15)ν2
1
(cid:15)ν

Since (cid:15) ď 1 by (B.11) (noting that Υ ě 1 ě γH _ γB), we simplify the above display by

"

p∇Lp2q

(cid:15),ν,ηqT ∆ ďΥ7

1 _ ν2
(cid:15)νp1 ^ νq

"

?

ď

2Υ7

?

2Υ7

ď2

1 _ ν2
(cid:15)νp1 ^ νq
ˆ
1 _ ν
(cid:15)p1 ^ ν2q

p}ga}2 ` }λc}2qp}∆x} ` }∆λ}q `

2η}ga}2}p∆x, ∆µ, ∆λq}

?

*

*

p}ga}2 ` }λc}2q}p∆x, ∆λq} ` η}ga}2}p∆x, ∆µ, ∆λq}

˙

_ η

p}ga}2 ` }λc}2q}p∆x, ∆µ, ∆λq}.

37

˛

Noting that
›
›
›
›
‚
›
›

›
¨
›
›
›
˝
›
›

∆x
∆µ
∆λ

ď }∆x} `

›
ˆ
›
›
›

˙›
›
›
›

∆µ
∆λ

(B.6)
ď }∆x} `

›
ˆ
›
›
›

2Υ1
γH

∆x
J∇xL
G∇xL`Πcpdiag2pgqλq

˙›
›
›
› ď

ˆ

›
›
›
›

3Υ1
γH

∆x
J∇xL
G∇xL`Πcpdiag2pgqλq

˙›
›
›
› ,

and

›
›
›
›

ˆ

ga
λc

˙›
›
›
› ď }ga} ` }λc}

(14a)
(B.7)
ď Υ2}∆x} `

Υ2ΥB ` Υ4 ` 1
γ2
H γB
∆x
J∇xL
G∇xL`Πcpdiag2pgqλq

›
ˆ
›
›
›

∆x
J∇xL
G∇xL`Πcpdiag2pgqλq

ˆ

›
›
›
›
˙›
›
›
› ,

˙›
›
›
›

ď

Υ2pΥB ` 1q ` Υ4 ` 1
γ2
H γB

where we use Υ1 ě 1 ě γH _ γB, we deﬁne Υ8 “ 6

?

2Υ7Υ1pΥ2pΥB ` 1q ` Υ4 ` 1q and have

p∇Lp2q

(cid:15),ν,ηqT ∆ ď

ˆ

Υ8
γ3
H γB

1 _ ν
(cid:15)p1 ^ ν2q

˙

_ η

p}ga} ` }λc}q

ˆ

›
›
›
›

∆x
J∇xL
G∇xL`Πcpdiag2pgqλq

2

˙›
›
›
›

.

(B.12)

By Lemma 3.5, there exists a subset X(cid:15),ν ˆ Λ(cid:15),ν Ď X ˆ Λ such that if px, λq P X(cid:15),ν ˆ Λ(cid:15),ν, then
A(cid:15),ν Ď Ipx‹q and Ac
(cid:15),ν Ď tI `px‹, λ‹quc. Furthermore, we let X(cid:15),ν,η ˆ Λ(cid:15),ν,η Ď X(cid:15),ν ˆ Λ(cid:15),ν be a convex
compact subset small enough such that

and

Then (B.12) leads to

}ga} ď}gIpx‹q} ď

ˆ

γ3
H γB
Υ8

(cid:15)p1 ^ ν2q
1 _ ν

^

1
η

˙

γB ^ η
8

,

}λc} ď}λpI`px‹,λ‹qqc} ď

ˆ

γ3
H γB
Υ8

(cid:15)p1 ^ ν2q
1 _ ν

^

1
η

˙

γB ^ η
8

.

p∇Lp2q

(cid:15),ν,ηqT ∆ ď

ˆ

›
›
›
›

γB ^ η
4

∆x
J∇xL
G∇xL`Πcpdiag2pgqλq

˙›
›
2
›
›

.

This completes the proof.

B.5 Proof of Theorem 3.8

We use C1, C2, C3 . . . to denote positive constants that are independent of the stepsize. Note that
we only track the dependence on the stepsize, as stated in the theorem. The set X(cid:15),ν,η ˆ M ˆ Λ(cid:15),ν,η
and (cid:15)thres are given in Lemma 3.7.

38

Taking conditional expectation on both sides of (17), for some constants C1, C2 ą 0, we have

EtrLt`1

(cid:15),ν,ηs ďLt

(cid:15),ν,η ` αtp∇Lt

(cid:15),ν,ηqT Etr ¯∆ts `

Etr} ¯∆t}2s

Lemma 3.3
ď

(cid:15),ν,η ` αtp∇Lt
Lt

(cid:15),ν,ηqT ∆t `

p1 _ }pM tq´1}2qp1 _ }pKt

aq´1}2qp}∆t}2 ` ψgq

(22)
(23)
ď Lt

(cid:15),ν,η ` αtp∇Lt
"

Lemma 3.7
(B.6)
ď

Lt

(cid:15),ν,η ´

(cid:15),ν,ηqT ∆t `

C1α2
t
2

αtpγB ^ ηq
4

´

C2α2
t
2

∆xt
J t∇xLt
Gt∇xLt`Πcpdiag2pgtqλtq

˙›
›
2
›
›

`

C1α2
t ψg
2

.

(B.13)

Υ(cid:15),ν,ηα2
t
2
Υ∆Υ(cid:15),ν,ηα2
t
2

p}∆t}2 ` ψgq
* ›
ˆ
›
›
›

Now, we establish a relation between the KKT residual Rt and the middle term. By Lemma 3.2,

Rt ď

1
ν ^ 1

(cid:15)qt

›
ˆ
›
›
›

∇xLt
ct
wt
(cid:15),ν

˙›
›
›
›

(B.4)
ď

1
(cid:15)κν ^ 1

›
˜
›
›
›
›

∇xLt
ct
gt
a
ν λt
´(cid:15)qt
c

¸›
›
›
›
› ď

(cid:15)ν _ 1
(cid:15)κν ^ 1

˜

›
›
›
›
›

∇xLt
ct
gt
a
λt
c

¸›
›
›
›
› .

(B.14)

For ∇xLt, we have the following decomposition

#

∇xLt “

I ´ ppJ tqT pGt

aqT q

ppJ tqT pGt

loooooooooooooooooooooooooooooooooooooomoooooooooooooooooooooooooooooooooooooon

∇xLt ` pI ´ P t

JGq∇xLt.

"ˆ

˙

J t
Gt
a

˙+

´1 ˆ
*
aqT q

J t
Gt
a

By (21) and the compactness of the iterates, we know }pI ´ P t
for some constant C3 ą 0. Furthermore, for a constant C4 ą 0,

JGq∇xLt} ď C3}pJ t∇xLt, Gt

a∇xLtq}

P t

JG

}P t

JG∇xLt}

(14a)
“

›
›
›P t

JG

!
Bt∆xt ` pJ tqT (cid:101)∆µt ` pGt

aqT (cid:101)∆λt

ď }P t

JGBt∆xt} ` }P t

JGpGt

cqT λt
c}

a ` pGt
(B.7)
ď C4

cqT λt
c
›
›
›
›

)›
›
›
ˆ

∆xt
J t∇xLt
Gt∇xLt`Πcpdiag2pgtqλtq

˙›
›
›
› .

Combining the last two displays, we have

}∇xLt} ď pC3 ` C4q

›
ˆ
›
›
›

∆xt
J t∇xLt
Gt∇xLt`Πcpdiag2pgtqλtq

˙›
›
›
› .

Moreover, there exist C5, C6 ą 0 such that
˙›
›
›
›

(14a)
ď C5}∆x},

›
ˆ
›
›
›

ct
gt
a

(B.7)
ď C6

›
ˆ
›
›
›

∆xt
J t∇xLt
Gt∇xLt`Πcpdiag2pgtqλtq

˙›
›
›
› .

}λt
c}

Plugging (B.15) and (B.16) into (B.14), we have

Rt ď tC3 ` C4 ` C5 ` C6u
loooooooooooomoooooooooooon

C7

ˆ

›
›
›
›

∆xt
J t∇xLt
Gt∇xLt`Πcpdiag2pgtqλtq

˙›
›
›
› .

39

(B.15)

(B.16)

(B.17)

Thus, by (B.13), if we let

then

αtpγB ^ ηq
8

ě

C2α2
t
2

ðñ αt ď

γB ^ η
4C2

“: αthres,

EtrLt`1

(cid:15),ν,ηs ďLt

(cid:15),ν,η ´

αtpγB ^ ηq
8

›
ˆ
›
›
›

(B.17)

ď Lt

(cid:15),ν,η ´

αtpγB ^ ηq
8C2
7

R2

∆xt
J t∇xLt
Gt∇xLt`Πcpdiag2pgtqλtq
C1α2
t ψg
2

t `

.

˙›
›
2
›
›

`

C1α2
t ψg
2

(B.18)

Thus, if αt “ α ď αthres, then we take full expectation on both sides of (B.18), sum over t “ 0, . . . , Γ,
and obtain

min
X ˆMˆΛ

L(cid:15),ν,η ´ L0

(cid:15),ν,η ď ´

αpγB ^ ηq
8C2
7

Rearranging the above inequality leads to

Γÿ

t“0

ErR2

t s `

C1pΓ ` 1qψg
2

α2.

1
Γ ` 1

Γÿ

t“0

ErR2

t s ď

8C2
7
γB ^ η

L0

(cid:15),ν,η ´ minX ˆMˆΛ L(cid:15),ν,η
pΓ ` 1qα

`

4C1C2
7 ψg
γB ^ η

α ď C8

ˆ

˙

1
pΓ ` 1qα

` α

where C8 “ 8C2

7 pL0
Furthermore, if αt satisﬁes

(cid:15),ν,η ´ minX ˆMˆΛ L(cid:15),ν,ηq{pγB ^ ηq _ 4C1C2
8
t“0 αt “ 8 and

7 ψg{pγB ^ ηq.
t ă 8, let us deﬁne

8
t“0 α2

ř

ř

χ1,t “ Lt

(cid:15),ν,η `

C1ψg
2

8ÿ

i“t

α2
t ,

χ2,t “

αtpγB ^ ηq
8C2
7

R2
t .

By (B.18), we know that Etrχ1,t`1s ď χ1,t ´ χ2,t. Since χ2,t ě 0, tχ1,t ´ minX ˆMˆΛ L(cid:15),ν,ηut is a
positive supermartingale. By (Durrett, 2019, Theorem 4.2.12), we have χ1,t converges to a random
variable χ1 almost surely, with Erχ1s ď χ1,0 ă 8. Thus,

8ÿ

Er

t“0

χ2,ts “

8ÿ

t“0

Erχ2,ts ď

8ÿ

t“0

Erχ1,ts ´ Erχ1,t`1s ă 8,

ř

which implies
completes the proof.

8
t“0 χ2,t ă 8 almost surely. Since

ř

8
t“0 αt “ 8, then lim inf tÑ8 Rt “ 0, which

C Proofs of Section 4

C.1 Proof of Lemma 4.3

We prove the result by contradiction. We aim to show that, D(cid:101)(cid:15) ą 0 such that @ξ1, @ν P r¯ν0, (cid:101)νs where
¯ν0 is a ﬁxed initial input of Algorithm 1 and (cid:101)ν is deﬁned in (40), and @px, µ, λq P X ˆ M ˆ Λ with
x P Tν, if (cid:15) ď (cid:101)(cid:15), then
›
›
› ,
› ¯∇L(cid:15),ν,ηpx, µ, λq

˙›
›
›
› ď

›
ˆ
›
›
›

cpxq
w(cid:15),νpx, λq

40

where ¯∇L(cid:15),ν,η is computed using samples in ξ1 and η ą 0 is any given positive constant. Note
that everything above is deterministic; that is, our analysis does not depend on a speciﬁc iteration
sequence tpxt, λt, λtqut. Thus, the threshold (cid:101)(cid:15) is deterministic.

Suppose the statement is false, then there exist a sequence t(cid:15)j, ξj

1, νjuj and an evaluation point

sequence tpxj, µj, λjquj P X ˆ M ˆ Λ such that νj P r¯ν0, (cid:101)νs, xj P Tνj , (cid:15)j Œ 0 and
›
›
› ă

›
›
› ¯∇Lj

@j ě 0,

˙›
›
›
› ,

›
›
›
›

ˆ

(cid:15)j ,νj ,η

cj
wj

(cid:15)j ,νj

(C.1)

(cid:15)j ,νj ,η is computed using samples ξj

where ¯∇Lj
1 and η ą 0 is a ﬁxed constant. By compactness, we
suppose pxj, µj, λjq Ñ p(cid:101)x, (cid:101)µ, (cid:101)λq P X ˆ M ˆ Λ and νj Ñ ν as j Ñ 8 (otherwise, we consider the
convergent subsequence, which must exist due to the compactness). Noting that cj “ cpxjq and
wj
(cid:15)j ,νj “ maxtgpxjq, ´(cid:15)jqνj pxj, λjqλju are bounded due to the compactness of pxj, µj, λjq and the
boundedness of νj, we have from (C.1) that

(cid:15)j} ¯∇xLj

(cid:15)j ,νj ,η} Ñ 0

as

j Ñ 8.

(C.2)

Moreover, since xj P Tνj ,
by (10), (C.2), and the convergence of pxj, µj, λjq, we get

i“1 maxtgj

r

i , 0u3 ď νj{2; taking limit j Ñ 8 leads to (cid:101)x P Tν. Furthermore,

ř

J T p(cid:101)xqcp(cid:101)xq `

1
qνp(cid:101)x, (cid:101)λq

GT p(cid:101)xq maxtgp(cid:101)xq, 0u `

3} maxtgp(cid:101)xq, 0u}2
2qνp(cid:101)x, (cid:101)λqaνp(cid:101)xq

Gp(cid:101)xqT lp(cid:101)xq “ 0,

which is further simpliﬁed as

ÿ

#

ÿ

cip(cid:101)xq∇cip(cid:101)xq `

i:cip(cid:101)xq‰0

i:gip(cid:101)xqą0

1
qνp(cid:101)x, (cid:101)λq

`

3} maxtgp(cid:101)xq, 0u}2gip(cid:101)xq
2qνp(cid:101)x, (cid:101)λqaνp(cid:101)xq

+

gip(cid:101)xq∇gip(cid:101)xq “ 0.

(C.3)

Suppose (cid:101)x P X zΩ and let Icp(cid:101)xq “ ti : 1 ď i ď m, cip(cid:101)xq ‰ 0u, and Igp(cid:101)xq “ ti : 1 ď i ď r, gip(cid:101)xq ą

0u. By Assumption 4.2, the set
)
!
z P Rd : cip(cid:101)xq∇T cip(cid:101)xqz ă 0, i P Icp(cid:101)xq and ∇T gip(cid:101)xqz ă 0, i P Igp(cid:101)xq

is nonempty. By Gordan’s theorem (Goldman and Tucker, 1957), we know that for any ai, bi ě 0
such that

ÿ

ÿ

aicip(cid:101)xq∇cip(cid:101)xq `

bi∇gip(cid:101)xq “ 0,

iPIcp(cid:101)xq

iPIgp(cid:101)xq

(C.4)

then ai “ bi “ 0. Comparing (C.4) with (C.3), and noting that the coeﬃcients of (C.3) are all
positive (since (cid:101)x P Tν), we immediately get the contradiction. Thus, (cid:101)x P Ω.

By Assumption 4.2, M p(cid:101)xq is invertible, following the same reasoning as (19), and hence is
positive deﬁnite. Thus, M j is invertible for large enough j. Let us suppose }pM jq´1} ď ΥM for
some ΥM ą 0. In addition, by direct calculation, we have

diagpgjqλj “ diagpλjqwj

(cid:15)j ,νj ´

1
(cid:15)jqj
νj

41

pdiagpgjq ´ diagpwj

(cid:15)j ,νj qqwj

(cid:15)j ,νj .

(C.5)

Thus, we further have

˙

ˆ

J j
Gj

¯∇xLj

(cid:15)j ,νj ,η

(10)
“

˙

ˆ

J j
Gj

1
(cid:15)j

`
"

¯∇xLj ` η
ˆ
˙ ˆ

ˆ

J j
Gj

˙ ´

¯ ´

Qj

1 Qj

2

¯

J j ¯∇xLj
Gj ¯∇xLj `diag2pgj qλj
˙ ˆ

˙

J j
Gj
ˆ
J j
Gj
J j
Gj

" ˆ

pJ jqT

˙ ´

pGj qT
qj
νj
¯* ´

Qj

1 Qj

2

˙ ˆ

`

3pGj qT lj wT
2qj

νj aj
νj

(cid:15)j ,νj

J j ¯∇xLj
Gj ¯∇xLj `diag2pgj qλj
˙

pJ jqT

pGj qT
qj
νj

`

3pGj qT lj wT
2qj

νj aj
νj

(cid:15)j ,νj

cj
wj
¯

(cid:15)j ,νj

diag2pgj q´diagpgj qdiagpw

0
j
(cid:15)j ,νj

q

q

j
νj

J j ¯∇xLj
Gj ¯∇xLj `diag2pgj qλj

¯

`

1
(cid:15)j

Hj
2

cj
wj

(cid:15)j ,νj

´(cid:15)j diagpgj qdiagpλj q
ˆ

˙

˙

¸ * ˆ

cj
wj

(cid:15)j ,νj

.

(C.6)

(C.5)
“

I ` η

1
(cid:15)j
˜

0

0

`

`

´

“:Hj
1

Let us focus on Hj

2. We know that

ˆ

Hj

2 “

J j pJ j qT
Gj pJ j qT tGj pGj qT `diag2pgj qu{qj
νj

J j pGj qT {qj
νj

¨

˚
˚
˚
˝

0

0

˙

`

2q

3

j
νj
3

a

j
νj

J j pGj qT lj wT

(cid:15)j ,νj

Gj pGj qT lj wT

(cid:15)j ,νj

2q

j
νj
diagpgj qdiagpw

j
νj

a

j
(cid:15)j ,νj

q

´

looooooooooooooooooooooooomooooooooooooooooooooooooon

j
q
νj

´(cid:15)j diagpgj qdiagpλj q

˛

‹
‹
‹
‚

“M j

˜

I
0

¸

0
1
qj
νj

I

` ∆Hj
2.

∆Hj
2

Recalling that σminp¨q denotes the least singular value of a matrix, by Weyl’s inequality,

#

˜

¸+

σminpHj

2q ě σmin

M j

I
0

0
1
qj
νj

I

´ }∆Hj

2} ě

σminpM jq
1 _ qj
νj

´ }∆Hj

2}.

Since (cid:15)j Ñ 0 and wj
that M j Ñ M p(cid:101)xq, which is positive deﬁnite; and note that qj
ϕ ą 0 and suﬃciently large j,

(cid:15)j ,νj Ñ 0 as j Ñ 8 (because (cid:101)x P Ω), we know ∆Hj

2 Ñ 0. In addition, we have
νj ď νj “ (cid:101)ν. Thus, for some constant

σminpHj

2q ě ϕ.

(C.7)

Now we bound the ﬁrst term in (C.6). By (10) and the invertibility of M j, we know

›
´
›
›

J j ¯∇xLj
Gj ¯∇xLj `diag2pgj qλj

¯›
›
› (10)
“

1
η

#ˆ

›
›
›
›pM jq´1
›
#ˆ

ď

ΥM
η

¯∇µLj
¯∇λLj

(cid:15)j ,νj ,η

(cid:15)j ,νj ,η

¯∇µLj
¯∇λLj
˙

(cid:15)j ,νj ,η

(cid:15)j ,νj ,η
›
´
›
›

`

˙

˜

´

wj

(cid:15)j ,νj `

cj
}w

¯›
›
› `

cj
wj
(cid:15)j ,νj

}wj

}2

λj

j
(cid:15)j ,νj
j
(cid:15)j a
νj
(cid:15)j ,νj }2}λj}
(cid:15)jaj
νj

¸+›
›
›
›
›

+

42

(C.1)
ď

ΥM
η

(6)
ď

2ΥM
η

#

#

›
´
›
›

cj
wj
(cid:15)j ,νj

´

cj
wj
(cid:15)j ,νj

2
#
›
›
›

¯›
›
› `

¯›
›
› `

+

+

}wj

(cid:15)j ,νj }2}λj}
(cid:15)jaj
νj
(cid:15)j ,νj }2}λj}

}wj

(cid:15)jνj

+

ď

2ΥM
η(cid:15)j

(cid:15)j `

}w(cid:15)j ,νj }}λj}
νj

}pcj, wj

(cid:15)j ,νj q}.

(C.8)

1} ď Υ1 and }ppJ jqT pGjqT q} ď Υ2 for some constants

Moreover, by the compactness condition, }Hj
Υ1, Υ2 ą 0. Combining (C.7), (C.8) with (C.6), we have
›
›
› ě(cid:15)j

›
›
› ¯∇xLj

(cid:15)jΥ2

˙

(cid:15)j ,νj ,η

ˆ

J j
Gj
´

›
›
›
›
›
›
›Hj
2
›
´
›
›

›
›
›
›
›
›
›Hj
1
›
´
›
›

´

(cid:15)j ,νj ,η

¯∇xLj
¯›
›
› ´ (cid:15)j

cj
wj
(cid:15)j ,νj

¯›
›
› ´ (cid:15)jΥ1

cj
wj
(cid:15)j ,νj

(C.6)
ě

(C.7)
ě ϕ ¨
˜

(C.8)
ě

ϕ ´

2(cid:15)jΥ1ΥM
η

´

¯›
›
›

¯›
›
›
ˆ

J j ¯∇xLj
Gj ¯∇xLj `diag2pgj qλj

J j ¯∇xLj
Gj ¯∇xLj `diag2pgj qλj
¸ ›
›
›
›

(cid:15)j ,νj }}λj}

2Υ1ΥM }wj
ηνj

˙›
›
›
›

cj
wj

(cid:15)j ,νj

“:pϕ ´ ϕjq}pcj, wj

(cid:15)j ,νj q}.

Noting that ϕj Ñ 0 as j Ñ 8 (since wj

(cid:15)j ,νj Ñ 0 and (cid:15)j Ñ 0), thus for large j, we obtain

(cid:15)jΥ2}pcj, wj

(cid:15)j ,νj q}

(C.1)
ě (cid:15)jΥ2

›
›
› ¯∇xLj

(cid:15)j ,νj ,η

›
›
› ě

ϕ
2

}pcj, wj

(cid:15)j ,νj q},

which cannot hold because (cid:15)j Œ 0. This is a contradiction, and hence we complete the proof.

C.2 Proof of Lemma 4.5

The proof closely follows the proof of Lemma 3.7 in Appendix B.4. We suppress the iteration t
and assume ξt
1 is any sample set. Our analysis is independent of the sample set ξt
1 for computing
¯∇Lt
¯(cid:15)t,¯νt,η, and we will see that the threshold is independent of t. Like Lemma 3.7, we use Υ1, Υ2, . . .
to denote generic constants that are independent of p¯(cid:15)t, ¯νt, η, γB, γH q, whose existence is ensured by
the compactness of iterates.

Following derivation of (B.5), we have

p ¯∇Lp1q

¯(cid:15),¯ν,ηqT ¯∆ “ ´ ¯∆xT B ¯∆x ` p c

ga qT

˙

ˆ

¯
(cid:101)∆µ` ¯∆µ
¯
(cid:101)∆λa` ¯∆λa

´

1
¯(cid:15)

}c}2 ´

1
¯(cid:15)q¯ν

´ ¯(cid:15)q¯ν ¯∆λT

c λc ´ η

}ga}2
›
´
›
›

J ¯∇xL
G ¯∇xL`Πcpdiag2pgqλq

¯›
›
2
›

.

(C.9)

Following derivation of (B.6), there exists Υ1 ą 0 such that
›
ˆ
›
›
›

˙›
›
›
› ď

›
ˆ
›
›
›

Υ1
γH

¯∆x
J ¯∇xL
G ¯∇xL`Πcpdiag2pgqλq

¯∆µ
¯∆λ

˙›
›
›
› .

(C.10)

43

Following derivation of (B.7), there exists Υ2 ą 0 such that
›
˜
›
›
›
›

¸›
›
›
›
› ď

›
ˆ
›
›
›

¯∆x
J ¯∇xL
G ¯∇xL`Πcpdiag2pgqλq

Υ2
γ2
H γB

¯
(cid:101)∆µ
¯
(cid:101)∆λa
´λc

˙›
›
›
› .

(C.11)

Following derivation of (B.8) by combining (C.10), (C.11) with (C.9); noting that 0 ă q¯ν ď ¯ν ď (cid:101)ν
where (cid:101)ν is deﬁned in (40); there exists Υ3 ą 0 such that
›
ˆ
›
›
›

¯(cid:15),¯ν,ηqT ¯∆ ď ´ ¯∆xT B ¯∆x `

˙›
›
›
› ´

p ¯∇Lp1q

ga q}2

}p c

}p c

ga q}

Υ3
γ2
H γB
¯∆x
J ¯∇xL
G ¯∇xL`Πcpdiag2pgqλq

¯∆x
J ¯∇xL
G ¯∇xL`Πcpdiag2pgqλq
˙›
›
´
›
2
›
›
›
›

´ η

J ¯∇xL
G ¯∇xL`Πcpdiag2pgqλq

1
¯(cid:15)p1 _ (cid:101)νq
¯›
›
2
›

.

›
ˆ
›
›
›

`

¯(cid:15)(cid:101)νΥ3
γ3
H γB

(C.12)

Following derivation of (B.10), there exists Υ4 ą 0 such that
Υ4
γ2
H γB

´ ¯∆xT B ¯∆x ď ´

} ¯∆x}2 `

3γB
4

}p c

ga q}2 .

Combining the above display with (C.12) and using the following Young’s inequality

Υ3
γ2
H γB

}p c

ga q}

ˆ

›
›
›
›

˙›
›
›
›

¯∆x
J ¯∇xL
G ¯∇xL`Πcpdiag2pgqλq
´

ď

¯ ›
ˆ
›
›
›

γB
8

^

η
4

¯∆x
J ¯∇xL
G ¯∇xL`Πcpdiag2pgqλq

˙›
›
2
›
›

`

2Υ2
3
BpγB ^ ηq

H γ2
γ4

}p c

ga q}2 ,

we have

p ¯∇Lp1q

(cid:15),ν,ηqT ¯∆ ď ´

`

ď ´

´

"

"

Therefore, as long as

} ¯∆x}2 `

3γB
4
"

`

Υ4
γ2
H γB
γB ^ η
2

1
¯(cid:15)p1 _ (cid:101)νq

`

* ›
ˆ
›
›
›
*

"´

¯

`

´

^

η
4

γB
8
2Υ2
3
H γ2
γ4
BpγB ^ ηq
´
η
γB
4
8
2Υ2
3
BpγB ^ ηq

H γ2
γ4

´

^

¯

´

¯(cid:15)(cid:101)νΥ3
γ3
H γB
1
¯(cid:15)p1 _ (cid:101)νq
* ›
ˆ
›
›
›

´

Υ4
γ2
H γB

¯(cid:15)(cid:101)νΥ3
γ3
H γB

¯∆x
J ¯∇xL
G ¯∇xL`Πcpdiag2pgqλq

˙›
›
2
›
›

}p c

ga q}2 ´ η

J ¯∇xL
G ¯∇xL`Πcpdiag2pgqλq

›
´
›
›

¯›
›
›

2

˙›
›
2
›
›

¯∆x
J ¯∇xL
G ¯∇xL`Πcpdiag2pgqλq
*

}p c

ga q}2 .

1
¯(cid:15)p1 _ (cid:101)νq

´

2Υ2
3
BpγB ^ ηq

H γ2
γ4

´

we have

Thus, we can deﬁne

p ¯∇Lp1q

(cid:15),ν,ηqT ¯∆ ď ´

γB
8

^

η
4

ě

¯(cid:15)(cid:101)νΥ3
γ3
H γB

ðù

ě 0 ðù

Υ4
γ2
H γB
›
ˆ
›
›
›

γB ^ η
2

1
¯(cid:15)

1
¯(cid:15)

ě

ě

3 ` Υ4q

,

8(cid:101)νΥ3
γ3
H γBpγB ^ ηq
p1 _ (cid:101)νqp2Υ2
H γ2
γ4
BpγB ^ ηq
˙›
›
2
›
›

.

¯∆x
J ¯∇xL
G ¯∇xL`Πcpdiag2pgqλq

(C.13)

,

(cid:101)(cid:15)2 :“

H γ2
γ4

BpγB ^ ηq
3 ` 8Υ3 ` Υ4qp(cid:101)ν _ 1q

p2Υ2

,

which implies (C.13), and complete the proof.

44

C.3 Proof of Lemma 4.7

We let C1, C2, C3 . . . be generic constants that are independent of pβ, αmax, κgrad, κf , pgrad, pf q.
There constants may not be consistent with the constants C1, C2, C3 in the statement. However,
the existence of C1, C2, C3 in the statement follows directly from our proof.
(a). By the deﬁnition of ∇L(cid:15),ν,η in (10), all quantities depending on (cid:15), ν do not depend on batch
samples. By the compactness condition in Assumption 4.1 and the triangular inequality, there exists
C1 ą 0 (depending on η) such that

›
›∇Lt

(cid:15),ν,η ´ ¯∇Lt

(cid:15),ν,η

›
›

›
›

›
› ¯∇2

`›
› ¯∇xLt ´ ∇xLt
xLt ´ ∇2
xLt
`
˘
} ¯∇f t ´ ∇f t} ` } ¯∇2f t ´ ∇2f t}
˘
`
} ¯∇f t ´ ∇f t} _ } ¯∇2f t ´ ∇2f t}

`

.

ďC1
“C1
ď2C1

›
˘
›

(b). By (10) and the compactness condition in Assumption 4.1, there exists C2 ą 0 such that

} ¯∇xLt} ď } ¯∇xL¯(cid:15)t,¯νt,η} ` C2

"›
›
›
›

ˆ

J t ¯∇xLt
Gt ¯∇xLt

˙›
›
›
› ` }diag2pgtqλt}

*

`

C2
¯(cid:15)tp1 ^ qt

¯νtq

›
ˆ
›
›
›

ct
wt

¯(cid:15)t,¯νt

˙›
›
›
› `

C2
¯νtat
¯νt

¯(cid:15)tqt

}wt

¯(cid:15)t,¯νt}2.

Since

¯(cid:15)0 ě ¯(cid:15)t ě (cid:101)(cid:15),

(cid:101)ν ě ¯νt ě qt

¯νt

(B.4)
ě κ¯νt ě κ¯ν0,

(cid:101)ν ě ¯νt ě at

¯νt ě

¯νt
2

ě

¯ν0
2

,

(C.14)

there exists C3 ą 0 such that

} ¯∇xLt} ď } ¯∇xL¯(cid:15)t,¯νt,η} ` C3

"›
›
›
›

ˆ

J t ¯∇xLt
Gt ¯∇xLt

˙›
›
›
› ` }diag2pgtqλt} `

›
ˆ
›
›
›

ct
wt

¯(cid:15)t,¯νt

˙›
›
›
› ` }wt

¯(cid:15)t,¯νt}2

*

.

Moreover, there exist C4, C5 ą 0 such that

}diag2pgtqλt} ď C4

›
ˆ
›
›
›

gt
a
λt
c

˙›
›
›
› ď

›
›
›
›

C4
¯νt ^ 1

¯(cid:15)tqt

ˆ

gt
a
¯νtλt
´¯(cid:15)tqt
c

˙›
›
›
›

(C.14)
ď

C4
(cid:101)(cid:15)κ¯ν0 ^ 1

}wt

¯(cid:15)t,¯νt},

and

}wt

¯(cid:15)t,¯νt}

Lemma 3.2
ď

C5p¯(cid:15)tqt

¯νt _ 1q ď C5p¯(cid:15)0(cid:101)ν _ 1q.

Combining the above three displays, there exists C6 ą 0 such that
›
›
›
›

} ¯∇xLt} ď } ¯∇xL¯(cid:15)t,¯νt,η} ` C6

˙›
›
›
› `

"›
›
›
›

ˆ

ˆ

J t ¯∇xLt
Gt ¯∇xLt

˙›
*
›
›
›

.

ct
wt

¯(cid:15)t,¯νt

(C.15)

(C.16)

(C.17)

We deal with the middle term. We know that

ˆ

M t
M t

11 M t
12
21 M t
22

˙

˙ ˆ

J t ¯∇xLt
Gt ¯∇xLt

(10)
“

1
η

ˆ

¯∇µLt
¯∇λLt

¯(cid:15)t,¯νt,η

¯(cid:15)t,¯νt,η

˜

˙

´

1
η

wt

¯(cid:15)t,¯νt `

ct
}wt

¯(cid:15)t,¯νt
¯(cid:15)tat
¯νt

¸

}2

λt

´

˙

ˆ

M t
12
M t
22

diag2pgtqλt.

(C.18)

45

Multiplying ppJ t ¯∇xLtqT pGt ¯∇xLtqT q on both sides, there exists C7 ą 0 such that

›
›pJ tqT J t ¯∇xLt ` pGtqT Gt ¯∇xLt

›
›2

ď

˙T ˆ

ˆ

J t ¯∇xLt
Gt ¯∇xLt

M t
M t

(C.18),(C.14)´(C.16)
ď

C7} ¯∇xLt}

˙ ˆ

˙

11 M t
12
21 M t
22
"›
ˆ
›
¯∇µLt
›
›
¯∇λLt

J t ¯∇xLt
Gt ¯∇xLt
˙›
›
›
› `

¯(cid:15)t,¯νt,η

¯(cid:15)t,¯νt,η

›
ˆ
›
›
›

ct
wt

¯(cid:15)t,¯νt

˙›
*
›
›
›

.

(C.19)

Furthermore,

›
ˆ
›
›
›

J t ¯∇xLt
Gt ¯∇xLt

˙›
›
2
›
›

ď } ¯∇xLt}

›
›
pJ tqT J t ¯∇xLt ` pGtqT Gt ¯∇xLt

›
›

(C.19)
ď

a

C7} ¯∇xLt}

3
2

"›
ˆ
›
›
›

¯∇µLt
¯∇λLt

¯(cid:15)t,¯νt,η

¯(cid:15)t,¯νt,η

˙›
›
›
› `

›
›
›
›

ˆ

ct
wt

¯(cid:15)t,¯νt

˙›
* 1
›
2
›
›

.

Combining the above display with (C.17), there exists C8 ą 0 such that
›
ˆ
›
›
›

"
›
› ¯∇Lt

"
›
› ¯∇Lt

`

›
›

3
4

¯(cid:15)t,¯νt,η

ct
wt

˙›
*
›
›
›
›
ˆ
›
›
›

¯(cid:15)t,¯νt
›
› `

¯(cid:15)t,¯νt,η

¯(cid:15)t,¯νt,η
˙ "
›
› ¯∇Lt

C8
4

` C1{4
8 } ¯∇xLt}
˙›
*
›
›
›

`

ct
wt

3
4

¯(cid:15)t,¯νt

} ¯∇xLt} ďC8
ˆ

} ¯∇xLt},

ď

C8 `

›
›

`

›
ˆ
›
›
›

˙›
* 1
›
4
›
›

ct
wt

¯(cid:15)t,¯νt

where the second inequality is due to Young’s inequality a3{4b1{4 ď 3a{4 ` b{4. Thus,
"
›
› ¯∇Lt

} ¯∇xLt} ď 5C8

˙›
›
›
›

›
›
›
›

`

ˆ

*

›
›

.

¯(cid:15)t,¯νt,η

ct
wt

¯(cid:15)t,¯νt

(c). By (10) and using (C.14), (C.15) and (C.16), there exists C9 ą 0 such that

›
› ¯∇L¯(cid:15)t,¯νt,η

›
›

ď} ¯∇xLt} ` C9

ď} ¯∇xLt} ` C9

›
´
›
›
›
´
›
›

(C.14)

ď } ¯∇xLt} ` C9

J t ¯∇xLt
Gt ¯∇xLt`Πcpdiag2pgtqλtq

J t ¯∇xLt
Gt ¯∇xLt`Πcpdiag2pgtqλtq
›
´
›
›

J t ¯∇xLt
Gt ¯∇xLt`Πcpdiag2pgtqλtq

´

›
›
›

ct
wt
¯(cid:15)t,¯νt

¯›
›
› ` C9
¯›
›
› ` C9p¯(cid:15)tqt
¯›
›
› ` C9p¯(cid:15)0(cid:101)ν _ 1q

¯›
›
›
›
ˆ
›
›
›
›
ˆ
›
›
›

¯νt _ 1q

˙›
›
›
›
˙›
›
›
› .

ct
gt
a
λt
c
ct
gt
a
λt
c

Following derivation of (B.15), (B.16), but replacing ∇xLt with ¯∇xLt, we immediately have for
some C10 that
›
ˆ
›
›
›

˙›
›
›
› ď C10

} ¯∇xLt} _

˙›
›
›
› .

›
›
›
›

ˆ

¯∆xt
J t ¯∇xLt
Gt ¯∇xLt`Πcpdiag2pgtqλtq

ct
gt
a
λt
c

Combining the above two displays completes the proof.

C.4 Proof of Lemma 4.8

Analogous to the proof of Lemma 4.7, we only track constants pβ, αmax, κgrad, κf , pgrad, pf q. We use
Υ1, Υ2, . . . to denote generic constants that are independent of pβ, αmax, κgrad, κf , pgrad, pf q. Note

46

that Υ1 in the proof may not be consistent with Υ1 in the statement, while the existence of Υ1 in
the statement follows directly from our proof.

Let Υ(cid:15),ν,η be the upper bound of the generalized Hessian of L(cid:15),ν,η in the compact set pX X Tθνq ˆ
M ˆ Λ. In particular, Υ(cid:15),ν,η “ suppX XTθν qˆMˆΛ }B2L(cid:15),ν,η}. Without loss of generality, we suppose (cid:101)(cid:15)
in Theorem 4.13 satisﬁes (cid:101)(cid:15) “ ¯(cid:15)0{ρ(cid:101)i for some integer (cid:101)i. Then, with deﬁnition (cid:101)j in (40), we let

and have Υ¯(cid:15)¯t,¯ν¯t,η ď Υ

(cid:101)(cid:15),(cid:101)ν,η “ maxtΥ(cid:15),ν,η : (cid:15) “ ¯(cid:15)0{ρi, ν “ ρj ¯ν0, 1 ď i ď (cid:101)i, 1 ď j ď (cid:101)ju
Υ
(cid:101)(cid:15),(cid:101)ν,η. Noting that xst, xt P T¯ν¯t, we apply the Taylor expansion and have

Lst
¯(cid:15)¯t,¯ν¯t,η ďLt

¯(cid:15)¯t,¯ν¯t,η ` ¯αtp∇Lt

q∆t}2
}

t

(cid:101)(cid:15),(cid:101)ν,η ¯α2
Υ
2

¯(cid:15)¯t,¯ν¯t,ηqT q∆t `
¯(cid:15)¯t,¯ν¯t,ηqT q∆t ` ¯αtp∇Lt
(cid:32)
¯(cid:15)¯t,¯ν¯t,ηqT q∆t ` C1 ¯αt
q∆t}2

Lemma 4.7paq
ď

“Lt

¯(cid:15)¯t,¯ν¯t,η ` ¯αtp ¯∇Lt

¯(cid:15)¯t,¯ν¯t,η ` ¯αtp ¯∇Lt
Lt
(cid:101)(cid:15),(cid:101)ν,η ¯α2
Υ
2
¯(cid:15)¯t,¯ν¯t,η ` ¯αtp ¯∇Lt

`

}

t

E t
1
ďLt

¯(cid:15)¯t,¯ν¯t,η ´ ¯∇L¯(cid:15)¯t,¯ν¯t,ηqT q∆t `

(cid:101)(cid:15),(cid:101)ν,η ¯α2
Υ
2
(
} ¯∇f t ´ ∇f t} _ } ¯∇2f t ´ ∇2f t}

t

q∆t}2

}

q∆t}

¨ }

¯(cid:15)¯t,¯ν¯t,ηqT q∆t ` C1κgrad ¯α2

t ¨ ¯Rt}

q∆t} `

(cid:101)(cid:15),(cid:101)ν,η ¯α2
Υ
2

t

q∆t}2.
}

(C.20)

We now consider two cases.
Case 1, q∆t “ ¯∆t. Combining (28) with (the reversed) (29), we have

p ¯∇Lt

¯(cid:15)¯t,¯ν¯t,ηqT ¯∆t ď ´

›
ˆ
›
›
›

γB ^ η
4

¯∆xt
J t ¯∇xLt
Gt ¯∇xLt`Πcpdiag2pgtqλtq

2

˙›
›
›
›

.

By (B.6), there exists Υ1 ą 0 such that

} ¯∆t} ď Υ1

›
ˆ
›
›
›

¯∆xt
J t ¯∇xLt
Gt ¯∇xLt`Πcpdiag2pgtqλtq

˙›
›
›
› .

Furthermore, by (B.14), (B.15), (B.16) and (C.14), there exists Υ2 ą 0 such that
›
ˆ
›
›
›

¯Rt ď Υ2

˙›
›
›
› .

¯∆xt
J t ¯∇xLt
Gt ¯∇xLt`Πcpdiag2pgtqλtq

Plugging the above two displays into (C.20), we have

(C.21)

(C.22)

(C.23)

Lst
¯(cid:15)¯t,¯ν¯t,η ďLt

¯(cid:15)¯t,¯ν¯t,η ` ¯αtp ¯∇Lt
"

`

C1Υ1Υ2κgrad `

¯(cid:15)¯t,¯ν¯t,ηqT ¯∆t
(cid:101)(cid:15),(cid:101)ν,ηΥ2
Υ
2
"

1

*

›
ˆ
›
›
›

¯α2
t

(C.21)

ď Lt

¯(cid:15)¯t,¯ν¯t,η ` ¯αtp ¯∇Lt

¯(cid:15)¯t,¯ν¯t,ηqT ¯∆t ´

C1Υ1Υ2κgrad `

2

˙›
›
›
›

¯∆xt
J t ¯∇xLt
Gt ¯∇xLt`Πcpdiag2pgtqλtq
*
(cid:101)(cid:15),(cid:101)ν,ηΥ2
4¯α2
Υ
t
γB ^ η
2

1

ď Lt

¯(cid:15)¯t,¯ν¯t,η ` ¯αt t1 ´ Υ3 pκgrad ` 1q ¯αtu p ¯∇Lt

¯(cid:15)¯t,¯ν¯t,ηqT ¯∆t,

where Υ3 “ 4C1Υ1Υ2{pγB ^ ηq _ 2Υ2

1Υ

(cid:101)(cid:15),(cid:101)ν,η{pγB ^ ηq.

47

p ¯∇Lt

¯(cid:15)¯t,¯ν¯t,ηqT ¯∆t

(C.24)

Case 2, q∆t “ (cid:98)∆t. We have

p ¯∇Lt

¯(cid:15)¯t,¯ν¯t,ηqT (cid:98)∆t “ ´p ¯∇Lt

¯(cid:15)¯t,¯ν¯t,ηqT (cid:98)H t ¯∇Lt

¯(cid:15)¯t,¯ν¯t,η ď ´γB

›
› ¯∇Lt

¯(cid:15)¯t,¯ν¯t,η

›
›2 ,

and

} (cid:98)∆t}

(30)
“

›
›
›p (cid:98)H tq´1 ¯∇Lt

¯(cid:15)¯t,¯ν¯t,η

›
›
› ď

1
γB

›
› ¯∇Lt

¯(cid:15)¯t,¯ν¯t,η

›
› .

By Lemma 4.7(b), Lemma 3.2, (27), and (C.14), there exists Υ4 ą 0 such that

Plugging the above two displays into (C.20), we have

¯Rt ď Υ4} ¯∇Lt

¯(cid:15)¯t,¯ν¯t,η}.

(C.25)

(C.26)

(C.27)

Lst
¯(cid:15)¯t,¯ν¯t,η ďLt

¯(cid:15)¯t,¯ν¯t,η ` ¯αtp ¯∇Lt

¯(cid:15)¯t,¯ν¯t,ηqT (cid:98)∆t `

ˆ

C1Υ4κgrad
γB
C1Υ4κgrad
γ2
B

t ¨ } ¯∇Lt
¯α2

¯(cid:15)¯t,¯ν¯t,η}2 `
˙

(cid:101)(cid:15),(cid:101)ν,η ¯α2
Υ
2γ2
B

t

} ¯∇Lt

¯(cid:15)¯t,¯ν¯t,η}2

t ¨ p ¯∇Lt
¯α2

¯(cid:15)¯t,¯ν¯t,ηqT (cid:98)∆t

(C.25)

ď Lt

¯(cid:15)¯t,¯ν¯t,ηqT (cid:98)∆t ´

¯(cid:15)¯t,¯ν¯t,η ` ¯αtp ¯∇Lt
¯(cid:15)¯t,¯ν¯t,η ` ¯αt t1 ´ Υ5 pκgrad ` 1q ¯αtu p ¯∇Lt

ďLt

`

Υ
(cid:101)(cid:15),(cid:101)ν,η
2γ3
B
¯(cid:15)¯t,¯ν¯t,ηqT (cid:98)∆t,

where Υ5 “ C1Υ4{γ2

B _ Υ

(cid:101)(cid:15),(cid:101)ν,η{p2γ3

Bq.

Combining (C.24) and (C.28) and letting Υ6 “ Υ3 _ Υ5 _ 2, we obtain

Lst
¯(cid:15)¯t,¯ν¯t,η ď Lt

¯(cid:15)¯t,¯ν¯t,η ` ¯αt t1 ´ Υ6 pκgrad ` 1q ¯αtu p ¯∇Lt

¯(cid:15)¯t,¯ν¯t,ηqT q∆t.

By the event E t
2,

¯Lst

¯(cid:15)¯t,¯ν¯t,η

¯(cid:15)¯t,¯ν¯t,η ´ κf ¯α2

t p ¯∇Lt

¯(cid:15)¯t,¯ν¯t,ηqT q∆t

E t
2
ď Lst
(C.29)

ď Lt
E t
2
ď ¯Lt
Υ6ě2
ď ¯Lt

¯(cid:15)¯t,¯ν¯t,η ` ¯αt t1 ´ Υ6 pκgrad ` 1q ¯αt ´ κf ¯αtu p ¯∇Lt

¯(cid:15)¯t,¯ν¯t,η ` ¯αt t1 ´ Υ6 pκgrad ` 1q ¯αt ´ 2κf ¯αtu p ¯∇Lt
¯(cid:15)¯t,¯ν¯t,η ` ¯αt t1 ´ Υ6 pκgrad ` κf ` 1q ¯αtu p ¯∇Lt

¯(cid:15)¯t,¯ν¯t,ηqT q∆t
¯(cid:15)¯t,¯ν¯t,ηqT q∆t
¯(cid:15)¯t,¯ν¯t,ηqT q∆t.

(C.28)

(C.29)

Therefore, as long as

1 ´ Υ6 pκgrad ` κf ` 1q ¯αt ě β ðñ ¯αt ď

1 ´ β
Υ6pκgrad ` κf ` 1q

,

we have

¯(cid:15)¯t,¯ν¯t,η ` ¯αtβp ¯∇Lt
This means the Armijo condition (36) holds; thus the step is a successful step, which completes the
proof.

¯Lst
¯(cid:15)¯t,¯ν¯t,η ď ¯Lt

¯(cid:15)¯t,¯ν¯t,ηqT q∆t.

48

C.5 Proof of Lemma 4.10

The line search of Algorithm 1 has three types of steps: a reliable step (Line 19), an unreliable step
(Line 21), and an unsuccessful step (Line 24). For each type of step, q∆t “ ¯∆t or q∆t “ (cid:98)∆t. Thus, we
analyze in the following six cases.
Case 1a, reliable step, q∆t “ ¯∆t. By Lemma 4.9, we have

¯(cid:15)¯t,¯ν¯t,η ´ Lt
Lt`1

¯(cid:15)¯t,¯ν¯t,η ď

¯αtβ
2

p ¯∇Lt

¯(cid:15)¯t,¯ν¯t,ηqT ¯∆t

(37)
ď

4¯αtβ
9

(C.21)

ď ´

¯αtβpγB ^ ηq
9

Note that

p ¯∇Lt

¯(cid:15)¯t,¯ν¯t,ηqT ¯∆t ´

¯δt
18
¯∆xt
J t ¯∇xLt
Gt ¯∇xLt`Πcpdiag2pgtqλtq

›
ˆ
›
›
›

˙›
›
2
›
›

´

¯δt
18

.

(C.30)

›
›∇Lt

¯(cid:15)¯t,¯ν¯t,η

›
› ď
Lemma 4.7paq
ď

›
›∇Lt
¯(cid:15)¯t,¯ν¯t,η ´ ¯∇Lt
(cid:32)›
› ¯∇f t ´ ∇f t

C1

¯(cid:15)¯t,¯ν¯t,η
›
›

_

›
›

›
› ¯∇Lt

›
› `
¯(cid:15)¯t,¯ν¯t,η
›
› ¯∇2f t ´ ∇2f t

(

›
›

`

›
› ¯∇Lt

¯(cid:15)¯t,¯ν¯t,η

›
› E t

1

ď C1κgrad ¯αt ¯Rt `

›
› ¯∇Lt

¯(cid:15)¯t,¯ν¯t,η

›
› .

Combining the above display with (C.23), Lemma 4.7(c), and using ¯αt ď αmax, there exists Υ1 ą 0
such that

›
›∇Lt

¯(cid:15)¯t,¯ν¯t,η

›
›

ď Υ1pκgradαmax ` 1q

›
ˆ
›
›
›

¯∆xt
J t ¯∇xLt
Gt ¯∇xLt`Πcpdiag2pgtqλtq

˙›
›
›
› .

(C.31)

Combining the above inequality with (C.30), we have

¯(cid:15)¯t,¯ν¯t,η ´ Lt
Lt`1

¯(cid:15)¯t,¯ν¯t,η ď ´

¯αtβpγB ^ ηq
18

›
ˆ
›
›
›

¯∆xt
J t ¯∇xLt
Gt ¯∇xLt`Πcpdiag2pgtqλtq

2

˙›
›
›
›

By Line 20 of Algorithm 1, ¯δt`1 ´ ¯δt “ pρ ´ 1q¯δt. By the Taylor expansion and ¯αt`1 ď ρ¯αt (Line
18), there exists Υ2 ą 0 such that

´

¯αtβpγB ^ ηq
1pκgradαmax ` 1q2

18Υ2

›
›∇Lt

¯(cid:15)¯t,¯ν¯t,η

›
›2

´

¯δt
18

.

›
›∇Lt`1

¯(cid:15)¯t,¯ν¯t,η

›
›2

¯αt`1

´ ¯αt

›
›∇Lt

›
›2

¯(cid:15)¯t,¯ν¯t,η
#
›
›∇Lt

!›
›∇Lt

¯(cid:15)¯t,¯ν¯t,η

›
›2

ď 2ρ¯αt

›
›2

¯(cid:15)¯t,¯ν¯t,η

` Υ2

(cid:101)(cid:15),(cid:101)ν,ηα2

maxΥ2

(cid:101)(cid:15),(cid:101)ν,η ¯α2
` Υ2
›
ˆ
›
›
›

)

t } ¯∆t}2

¯∆xt
J t ¯∇xLt
Gt ¯∇xLt`Πcpdiag2pgtqλtq

(C.22)

ď 2ρ¯αt

+

2

˙›
›
›
›

.

(C.32)

Combining the above two displays with (41), we obtain

ˆ

ˆ

ˆ

Θt`1

ω ´ Θt

ω ď ´

´

´

(cid:101)(cid:15),(cid:101)ν,ηα2

´ p1 ´ ωqρΥ2

ωβpγB ^ ηq
maxΥ2
18
˙
ωβpγB ^ ηq
1pκgradαmax ` 1q2 ´ p1 ´ ωqρ
˙
p1 ´ ωqpρ ´ 1q
´
2

18Υ2
ω
18

¯δt.

˙

›
ˆ
›
›
›

¯αt

¯∆xt
J t ¯∇xLt
Gt ¯∇xLt`Πcpdiag2pgtqλtq

˙›
›
2
›
›

›
›∇Lt

¯αt

¯(cid:15)¯t,¯ν¯t,η

›
›2

49

Let

ωβpγB ^ ηq
36

ě p1 ´ ωqρΥ2

(cid:101)(cid:15),(cid:101)ν,ηα2

maxΥ2 ðñ

36Υ2

ωβpγB ^ ηq
1pκgradαmax ` 1q2 ě p1 ´ ωqρ ðñ
ðñ

p1 ´ ωqpρ ´ 1q
2

ω
36

ě

which is further implied by

ě

ě

36ρΥ2

maxΥ2

(cid:101)(cid:15),(cid:101)ν,ηα2
βpγB ^ ηq

,

36ρΥ2

1pκgradαmax ` 1q2
βpγB ^ ηq

,

(C.33)

ě 18pρ ´ 1q,

ω
1 ´ ω
ω
1 ´ ω
ω
1 ´ ω

Υ3pκgradαmax ` αmax ` 1q2
β

_ 18pρ ´ 1q

(C.34)

ě

ω
1 ´ ω
(cid:101)(cid:15),(cid:101)ν,ηΥ2 _ 36ρΥ2

ˆ

›
›
›
›

if we deﬁne Υ3 “ p36ρΥ2

1q{pγB ^ ηq. Then, we obtain

Θt`1

ω ´ Θt

ω ď ´

ωβpγB ^ ηq
36

¨ ¯αt

¯∆xt
J t ¯∇xLt
Gt ¯∇xLt`Πcpdiag2pgtqλtq

˙›
›
2
›
›

Case 2a, unreliable step, q∆t “ ¯∆t. By Lemma 4.9, we have

´

ωβpγB ^ ηq
1pκgradαmax ` 1q2 ¨ ¯αt

36Υ2

›
›∇Lt

¯(cid:15)¯t,¯ν¯t,η

›
›2

´

ω
36

¯δt.

(C.35)

¯(cid:15)¯t,¯ν¯t,η ´ Lt
Lt`1

(C.21)

ď ´

¯(cid:15)¯t,¯ν¯t,η ď

¯αtβ
2
¯αtβpγB ^ ηq
8

(C.31)

ď ´

¯αtβpγB ^ ηq
16

p ¯∇Lt
›
ˆ
›
›
›
›
ˆ
›
›
›

¯(cid:15)¯t,¯ν¯t,ηqT ¯∆t
¯∆xt
J t ¯∇xLt
Gt ¯∇xLt`Πcpdiag2pgtqλtq
¯∆xt
J t ¯∇xLt
Gt ¯∇xLt`Πcpdiag2pgtqλtq

˙›
›
2
›
›
˙›
›
2
›
›

´

¯αtβpγB ^ ηq
1pκgradαmax ` 1q2

16Υ2

›
›∇Lt

¯(cid:15)¯t,¯ν¯t,η

›
›2 .

By Line 22 of Algorithm 1, ¯δt`1 ´ ¯δt “ ´p1 ´ 1{ρq¯δt, while (C.32) still holds. Thus, under (C.34),
we have

Θt`1

ω ´ Θt

ω ď ´

ωβpγB ^ ηq
36

¨ ¯αt

¯∆xt
J t ¯∇xLt
Gt ¯∇xLt`Πcpdiag2pgtqλtq

ˆ

›
›
›
›

˙›
›
2
›
›

´

ωβpγB ^ ηq
1pκgradαmax ` 1q2 ¨ ¯αt

36Υ2

›
›∇Lt

¯(cid:15)¯t,¯ν¯t,η

›
›2

´

1
2

ˆ

p1 ´ ωq

1 ´

˙

1
ρ

¯δt.

(C.36)

Case 3a, unsuccessful step, q∆t “ ¯∆t. In this case, pxt`1, µt`1, λt`1q “ pxt, µt, λtq, ¯αt`1 “ ¯αt{ρ
and ¯δt`1 “ ¯δt{ρ. Thus, we immediately have

Θt`1

ω ´ Θt

ω ď ´

1
2

p1 ´ ωq

1 ´

ˆ

˙ ´

›
›∇Lt

¯(cid:15)¯t,¯ν¯t,η

›
›2

¯αt

1
ρ

¯

` ¯δt

.

(C.37)

Combining (C.35), (C.36), (C.37), and noting that
˙

ˆ

ωβpγB ^ ηq
1pκgradαmax ` 1q2 ě

36Υ2

1 ´ ω
2
1 ´ ω
2

ω
36

ě

1 ´

ˆ

1 ´

˙

ðù

ðù

ω
1 ´ ω
ω
1 ´ ω

ě

18Υ2

1pκgradαmax ` 1q2

βpγB ^ ηq

,

ě 18pρ ´ 1q,

1
ρ
1
ρ

50

as implied by (C.33) and further by (C.34), we know (C.37) holds for all three cases with q∆t “ ¯∆t.
Case 1b, reliable step, q∆t “ (cid:98)∆t. By Lemma 4.9, we have

¯(cid:15)¯t,¯ν¯t,η ´ Lt
Lt`1

¯(cid:15)¯t,¯ν¯t,η ď

¯αtβ
2

p ¯∇Lt

¯(cid:15)¯t,¯ν¯t,ηqT (cid:98)∆t

(37)
ď

¯αtβ
3

p ¯∇Lt

¯(cid:15)¯t,¯ν¯t,ηqT (cid:98)∆t ´

¯δt
6

(C.25)

ď ´

¯αtβγB
3

›
› ¯∇Lt

¯(cid:15)¯t,¯ν¯t,η

›
›2

´

¯δt
6

.

Note that

›
›∇Lt

›
› ď
¯(cid:15)¯t,¯ν¯t,η
Lemma 4.7paq
ď

›
›∇Lt
¯(cid:15)¯t,¯ν¯t,η ´ ¯∇Lt
¯(cid:15)¯t,¯ν¯t,η
¯(cid:15)¯t,¯ν¯t,η
›
›
›
(cid:32)›
(
›
› ¯∇2f t ´ ∇2f t
›
› ¯∇f t ´ ∇f t

›
› ¯∇Lt

›
› `

_

›
›

C1

›
› ¯∇Lt

¯(cid:15)¯t,¯ν¯t,η

`

›
› E t

1

ď C1κgrad ¯αt ¯Rt `

›
› ¯∇Lt

¯(cid:15)¯t,¯ν¯t,η

›
› .

Combining the above display with (C.27) and using ¯αt ď αmax, there exists Υ4 ą 0 such that
›
› ď Υ4pκgradαmax ` 1q

›
›∇Lt

›
› ¯∇Lt

›
› .

(C.38)

¯(cid:15)¯t,¯ν¯t,η

¯(cid:15)¯t,¯ν¯t,η

Combining the above three displays,

¯(cid:15)¯t,¯ν¯t,η ´ Lt
Lt`1

¯(cid:15)¯t,¯ν¯t,η ď ´

¯αtβγB
6

›
› ¯∇Lt

¯(cid:15)¯t,¯ν¯t,η

›
›2

´

¯αtβγB

6Υ2

4pκgradαmax ` 1q2

›
›∇Lt

¯(cid:15)¯t,¯ν¯t,η

›
›2

´

¯δt
6

.

By Line 20 of Algorithm 1, ¯δt`1 ´ ¯δt “ pρ ´ 1q¯δt. By the Taylor expansion and ¯αt`1 ď ρ¯αt (Line 18),

›
›∇Lt`1

¯(cid:15)¯t,¯ν¯t,η

›
›2

¯αt`1

´ ¯αt

›
›∇Lt

¯(cid:15)¯t,¯ν¯t,η

›
›2

ď 2ρ¯αt

›
›2

!›
›∇Lt
¯(cid:15)¯t,¯ν¯t,η
#
›
›∇Lt

(C.26)

ď 2ρ¯αt

)

` Υ2

t } (cid:98)∆t}2
(cid:101)(cid:15),(cid:101)ν,η ¯α2
›
(cid:101)(cid:15),(cid:101)ν,ηα2
Υ2
›2
γ2
B

`

¯(cid:15)¯t,¯ν¯t,η

max

+

›
›2

›
› ¯∇Lt

¯(cid:15)¯t,¯ν¯t,η

.

(C.39)

Combining the above two displays,

˜

ˆ

ˆ

Θt`1

ω ´ Θt

ω ď ´

´

´

¸

ωβγB
6

¯αt

max

´ p1 ´ ωqρ

(cid:101)(cid:15),(cid:101)ν,ηα2
Υ2
γ2
B

›
› ¯∇Lt
˙
4pκgradαmax ` 1q2 ´ p1 ´ ωqρ
p1 ´ ωqpρ ´ 1q
´
2

ωβγB

¯δt.

¯αt

˙

6Υ2
ω
6

›
›2

¯(cid:15)¯t,¯ν¯t,η

›
›∇Lt

›
›2

¯(cid:15)¯t,¯ν¯t,η

Let

ωβγB
12

ě p1 ´ ωqρ

max

(cid:101)(cid:15),(cid:101)ν,ηα2
Υ2
γ2
B

ðñ

ωβγB

12Υ2

4pκgradαmax ` 1q2 ě p1 ´ ωqρ ðñ
pρ ´ 1q ðñ
ě

ω
12

1 ´ ω
2

ě

ě

12ρΥ2

12ρΥ2

,

max

(cid:101)(cid:15),(cid:101)ν,ηα2
βγ3
B
4pκgradαmax ` 1q2

βγB

ě 6pρ ´ 1q,

ω
1 ´ ω
ω
1 ´ ω
ω
1 ´ ω

,

(C.40)

51

which is implied by (C.34) if we re-deﬁne Υ3 Ð Υ3 _ p12ρΥ2
›
› ¯∇Lt

ωβγB

Θt`1

(cid:101)(cid:15),(cid:101)ν,η _ 12ρΥ2
›
›∇Lt

4γ2

›
›2

ω ´ Θt

ω ď ´

¨ ¯αt

´

¯(cid:15)¯t,¯ν¯t,η

12Υ2

4pκgradαmax ` 1q2 ¨ ¯αt

ωβγB
12

Bq{γ3
B. Then,
›
›2

´

ω
12

¯δt.

¯(cid:15)¯t,¯ν¯t,η

(C.41)

Case 2b, unreliable step, q∆t “ (cid:98)∆t. By Lemma 4.9, we have
›
¯αtβγB
¯αtβ
› ¯∇Lt
2
2

¯(cid:15)¯t,¯ν¯t,η ´ Lt
Lt`1

p ¯∇Lt

ď ´

(C.25)

›
›2

¯(cid:15)¯t,¯ν¯t,η

¯(cid:15)¯t,¯ν¯t,η ď
(C.38)

ď ´

¯αtβγB
4

¯(cid:15)¯t,¯ν¯t,ηqT (cid:98)∆t
›
› ¯∇Lt

¯(cid:15)¯t,¯ν¯t,η

›
›2

´

¯αtβγB
4 pκgradαmax ` 1q2

4Υ2

›
›∇Lt

¯(cid:15)¯t,¯ν¯t,η

›
›2 .

By Line 22 of Algorithm 1, ¯δt`1 ´ ¯δt “ ´p1 ´ 1{ρq¯δt, while (C.39) still holds. Thus, under (C.34),

Θt`1

ω ´ Θt

ω ď ´

ωβγB
12

¨ ¯αt

´

›
› ¯∇Lt

¯(cid:15)¯t,¯ν¯t,η

›
›2

ωβγB

12Υ2

4pκgradαmax ` 1q2 ¨ ¯αt

›
›∇Lt

¯(cid:15)¯t,¯ν¯t,η

›
›2

´

1
2

ˆ

p1 ´ ωq

1 ´

˙

1
ρ

¯δt.

(C.42)

Case 3b, unsuccessful step, q∆t “ (cid:98)∆t. In this case, (C.37) holds. Combining (C.41), (C.42),
(C.37), and noting that

ˆ

˙

12Υ2

ωβγB

4pκgradαmax ` 1q2 ě

1
ρ
1
ρ
as implied by (C.40) and further by (C.34), we know (C.37) holds for all three cases with q∆t “ (cid:98)∆t.
In summary, under (C.34), (C.37) holds for all cases. This completes the proof.

4pκgradαmax ` 1q2
βγB

ω
1 ´ ω
ω
1 ´ ω

1 ´ ω
2
1 ´ ω
2

ě 6pρ ´ 1q,

ω
12

6Υ2

ðù

ðù

1 ´

1 ´

ě

ě

˙

ˆ

,

C.6 Proof of Lemma 4.11

The proof follows the proof of Lemma 4.10, except (C.31) and (C.38) do not hold due to pE t
consider the following six cases.
Case 1a, reliable step, q∆t “ ¯∆t. By Lemma 4.9, we have

1qc. We

¯(cid:15)¯t,¯ν¯t,η ´ Lt
Lt`1

¯(cid:15)¯t,¯ν¯t,η ď

¯αtβ
2

p ¯∇Lt

¯(cid:15)¯t,¯ν¯t,ηqT ¯∆t

(37)
ď

4¯αtβ
9
(C.21)

ď ´

¯δt
18

p ¯∇Lt

¯(cid:15)¯t,¯ν¯t,ηqT ¯∆t ´
›
ˆ
›
›
›

¯αtβpγB ^ ηq
9

¯∆xt
J t ¯∇xLt
Gt ¯∇xLt`Πcpdiag2pgtqλtq

˙›
›
2
›
›

´

¯δt
18

.

By Line 20 of Algorithm 1, ¯δt`1 ´ ¯δt “ pρ ´ 1q¯δt, while (C.32) still holds. By the condition of ω in
(C.33) and (C.34), we know that under (42) (which implies (C.34)),
›
›
›
›

˙›
›
2
›
›

ω ´ Θt

ω ď ´

Θt`1

¨ ¯αt

ˆ

¯∆xt
J t ¯∇xLt
Gt ¯∇xLt`Πcpdiag2pgtqλtq

ωβpγB ^ ηq
36

›
›∇Lt

¯(cid:15)¯t,¯ν¯t,η

›
›2

´

ω
36

¯δt.

(C.43)

` ρp1 ´ ωq¯αt

52

Case 2a, unreliable step, q∆t “ ¯∆t. By Lemma 4.9, we have

¯(cid:15)¯t,¯ν¯t,η ´ Lt
Lt`1

¯(cid:15)¯t,¯ν¯t,η ď

¯αtβ
2

p ¯∇Lt

¯(cid:15)¯t,¯ν¯t,ηqT ¯∆t

(C.21)

ď ´

¯αtβpγB ^ ηq
8

›
ˆ
›
›
›

¯∆xt
J t ¯∇xLt
Gt ¯∇xLt`Πcpdiag2pgtqλtq

˙›
›
2
›
›

.

By Line 22 of Algorithm 1, ¯δt`1 ´ ¯δt “ ´p1 ´ 1{ρq¯δt, while (C.32) still holds. Thus, under (42),

Θt`1

ω ´ Θt

ω ď ´

ωβpγB ^ ηq
36

¨ ¯αt

ˆ

›
›
›
›

¯∆xt
J t ¯∇xLt
Gt ¯∇xLt`Πcpdiag2pgtqλtq

˙›
›
2
›
›

` ρp1 ´ ωq¯αt

›
›∇Lt

¯(cid:15)¯t,¯ν¯t,η

›
›2

´

1
2

ˆ

p1 ´ ωq

1 ´

˙

1
ρ

¯δt.

(C.44)

Case 3a, unsuccessful step, q∆t “ ¯∆t. In this case, (C.37) holds. Combining (C.43), (C.44), and
(C.37),

Θt`1

ω ´ Θt

ω ď ρp1 ´ ωq¯αt

›
›∇Lt

¯(cid:15)¯t,¯ν¯t,η

›
›2 .

(C.45)

Case 1b, reliable step, q∆t “ (cid:98)∆t. By Lemma 4.9, we have

¯(cid:15)¯t,¯ν¯t,η ´ Lt
Lt`1

¯(cid:15)¯t,¯ν¯t,η ď

¯αtβ
2

p ¯∇Lt

¯(cid:15)¯t,¯ν¯t,ηqT (cid:98)∆t

(37)
ď

¯αtβ
3

p ¯∇Lt

¯(cid:15)¯t,¯ν¯t,ηqT (cid:98)∆t ´

¯δt
6

(C.25)

ď ´

¯αtβγB
3

›
› ¯∇Lt

¯(cid:15)¯t,¯ν¯t,η

›
›2

´

¯δt
6

.

By Line 20 of Algorithm 1, ¯δt`1 ´ ¯δt “ pρ ´ 1q¯δt, while (C.39) still holds. By the condition of ω in
(C.40), we know that under (42) (which implies (C.40)),

Θt`1

ωβγB
12
Case 2b, unreliable step, q∆t “ (cid:98)∆t. By Lemma 4.9, we have

ω ´ Θt

` ρp1 ´ ωq¯αt

ω ď ´

¯(cid:15)¯t,¯ν¯t,η

¨ ¯αt

›
› ¯∇Lt

›
›2

›
›∇Lt

¯(cid:15)¯t,¯ν¯t,η

›
›2

´

ω
12

¯δt.

(C.46)

¯(cid:15)¯t,¯ν¯t,η ´ Lt
Lt`1

¯(cid:15)¯t,¯ν¯t,η ď

¯αtβ
2

p ¯∇Lt

¯(cid:15)¯t,¯ν¯t,ηqT (cid:98)∆t

(C.25)

ď ´

¯αtβγB
2

›
› ¯∇Lt

¯(cid:15)¯t,¯ν¯t,η

›
›2 .

By Line 22 of Algorithm 1, ¯δt`1 ´ ¯δt “ ´p1 ´ 1{ρq¯δt, while (C.39) still holds. Thus, under (42),

Θt`1

ω ´ Θt

ω ď ´

ωβγB
12

¨ ¯αt

›
› ¯∇Lt

¯(cid:15)¯t,¯ν¯t,η

›
›2

` ρp1 ´ ωq¯αt

›
›∇Lt

¯(cid:15)¯t,¯ν¯t,η

›
›2

´

1 ´ ω
2

ˆ

1 ´

˙

1
ρ

¯δt.

(C.47)

Case 3b, unsuccessful step, q∆t “ (cid:98)∆t. In this case, (C.37) holds. Combining (C.46), (C.47), and
(C.37), we note that (C.45) holds as well. Thus, (C.45) holds for all six cases. This completes the
proof.

C.7 Proof of Lemma 4.12

The proof follows the proof of Lemma 4.11, except Lemma 4.9 is not applicable. We consider the
following six cases.

53

Case 1a, reliable step, q∆t “ ¯∆t. We have
ˇ
ˇ ¯Lst
¯(cid:15)¯t,¯ν¯t,η ď ¯Lst
¯(cid:15)¯t,¯ν¯t,η ´ Lst
ˇ
ˇ ¯Lst

¯(cid:15)¯t,¯ν¯t,η ´ Lt
Lt`1

¯(cid:15)¯t,¯ν¯t,η ´ ¯Lt

ď¯αtβp ¯∇Lt
4¯αtβ
(37)
ď
5

¯(cid:15)¯t,¯ν¯t,η `
¯(cid:15)¯t,¯ν¯t,ηqT ¯∆t `
p ¯∇Lt

¯(cid:15)¯t,¯ν¯t,ηqT ¯∆t ´
›
›
›
›

ˆ

¯(cid:15)¯t,¯ν¯t,η
¯(cid:15)¯t,¯ν¯t,η ´ Lst
ˇ
¯δt
ˇ ¯Lst
5

`

¯(cid:15)¯t,¯ν¯t,η

¯(cid:15)¯t,¯ν¯t,η ´ Lst

ˇ
ˇ
ˇ ¯Lt
ˇ `
¯(cid:15)¯t,¯ν¯t,η ´ Lt
ˇ
ˇ
ˇ ¯Lt
ˇ `

ˇ
ˇ

ˇ
ˇ

¯(cid:15)¯t,¯ν¯t,η

¯(cid:15)¯t,¯ν¯t,η

¯(cid:15)¯t,¯ν¯t,η ´ Lt
ˇ
ˇ
ˇ ¯Lt
ˇ `
¯δt
5

¯(cid:15)¯t,¯ν¯t,η
˙›
›
2
›
›

`

(C.21)

ď ´

ˇ
ˇ ¯Lst

¯αtβpγB ^ ηq
5
¯(cid:15)¯t,¯ν¯t,η ´ Lst

¯∆xt
J t ¯∇xLt
Gt ¯∇xLt`Πcpdiag2pgtqλtq
ˇ
ˇ `
¯(cid:15)¯t,¯ν¯t,η ´ Lt
By Line 20 of Algorithm 1, ¯δt`1 ´ ¯δt “ pρ ´ 1q¯δt, while (C.32) still holds. By the condition of ω in
(C.33) and (C.34), we know that under (42) (which implies (C.34)),
›
›
›
›
ˇ
ˇ `

¯∆xt
J t ¯∇xLt
Gt ¯∇xLt`Πcpdiag2pgtqλtq
ˇ
(
ˇ ¯Lt

ω ď ´
(cid:32)ˇ
ˇ ¯Lst

ωβpγB ^ ηq
36

ω ´ Θt

›
›∇Lt

` ρp1 ´ ωq¯αt

˙›
›
2
›
›

(C.48)

ˇ
ˇ ¯Lt

Θt`1

¯(cid:15)¯t,¯ν¯t,η

¯(cid:15)¯t,¯ν¯t,η

` ω

¨ ¯αt

ˇ
ˇ .

¯δt.

›
›2

´

´

ˆ

ˇ
ˇ

¯(cid:15)¯t,¯ν¯t,η ´ Lst

¯(cid:15)¯t,¯ν¯t,η

¯(cid:15)¯t,¯ν¯t,η ´ Lt

¯(cid:15)¯t,¯ν¯t,η

¯(cid:15)¯t,¯ν¯t,η

ω
36

¯(cid:15)¯t,¯ν¯t,η ´ Lt

¯(cid:15)¯t,¯ν¯t,η

ˇ
ˇ

Case 2a, unreliable step, q∆t “ ¯∆t. We have
ˇ
ˇ ¯Lst
¯(cid:15)¯t,¯ν¯t,η ď ¯Lst

¯(cid:15)¯t,¯ν¯t,η ´ Lt
Lt`1

¯(cid:15)¯t,¯ν¯t,η ´ ¯Lt
(36)
ď ¯αtβp ¯∇Lt
(C.21)

ď ´

¯(cid:15)¯t,¯ν¯t,η `
¯(cid:15)¯t,¯ν¯t,ηqT ¯∆t `
›
ˆ
›
›
›

¯αtβpγB ^ ηq
4
¯(cid:15)¯t,¯ν¯t,η ´ Lt

ˇ
ˇ ¯Lt

`

¯(cid:15)¯t,¯ν¯t,η

ˇ
ˇ

`

ˇ
ˇ ¯Lt
ˇ
ˇ

`

¯(cid:15)¯t,¯ν¯t,η ´ Lst

¯(cid:15)¯t,¯ν¯t,η

ˇ
ˇ ¯Lst

¯(cid:15)¯t,¯ν¯t,η

¯(cid:15)¯t,¯ν¯t,η ´ Lst
¯∆xt
J t ¯∇xLt
Gt ¯∇xLt`Πcpdiag2pgtqλtq
ˇ
ˇ .

ˇ
ˇ

ˇ
ˇ

¯(cid:15)¯t,¯ν¯t,η

¯(cid:15)¯t,¯ν¯t,η ´ Lt

¯(cid:15)¯t,¯ν¯t,η

ˇ
ˇ ¯Lt
˙›
›
2
›
›

¯(cid:15)¯t,¯ν¯t,η ´ Lt
ˇ
ˇ ¯Lst

`

¯(cid:15)¯t,¯ν¯t,η ´ Lst

¯(cid:15)¯t,¯ν¯t,η

ˇ
ˇ

By Line 22 of Algorithm 1, ¯δt`1 ´ ¯δt “ ´p1 ´ 1{ρq¯δt, while (C.32) still holds. Thus, under (42),

Θt`1

ω ´ Θt

ω ď ´

ˆ

›
›
›
›

¨ ¯αt

ωβpγB ^ ηq
36
(cid:32)ˇ
ˇ ¯Lst

¯∆xt
J t ¯∇xLt
Gt ¯∇xLt`Πcpdiag2pgtqλtq
ˇ
ˇ ¯Lt
¯(cid:15)¯t,¯ν¯t,η ´ Lt

ˇ
ˇ `

˙›
›
2
›
›

´
ˇ
(
ˇ

1
2

p1 ´ ωq

ˆ

˙

¯δt

1
1 ´
ρ
›
›∇Lt

›
›2 .

` ω

(C.49)
Case 3a, unsuccessful step, q∆t “ ¯∆t. In this case, (C.37) holds. Combining (C.48), (C.49), and
(C.37), we obtain

` ρp1 ´ ωq¯αt

¯(cid:15)¯t,¯ν¯t,η

¯(cid:15)¯t,¯ν¯t,η

¯(cid:15)¯t,¯ν¯t,η

¯(cid:15)¯t,¯ν¯t,η ´ Lst

Θt`1

ω ´ Θt

ω ď ω

(cid:32)ˇ
ˇ ¯Lst

¯(cid:15)¯t,¯ν¯t,η ´ Lst

¯(cid:15)¯t,¯ν¯t,η

ˇ
ˇ `

ˇ
ˇ ¯Lt

¯(cid:15)¯t,¯ν¯t,η ´ Lt

¯(cid:15)¯t,¯ν¯t,η

(

ˇ
ˇ

` ρp1 ´ ωq¯αt

›
›∇Lt

¯(cid:15)¯t,¯ν¯t,η

›
›2 .

(C.50)

¯(cid:15)¯t,¯ν¯t,η ´ ¯Lt

¯(cid:15)¯t,¯ν¯t,η ´ Lt
Lt`1

Case 1b, reliable step, q∆t “ (cid:98)∆t. We have
ˇ
ˇ ¯Lst
¯(cid:15)¯t,¯ν¯t,η ď ¯Lst
¯(cid:15)¯t,¯ν¯t,η ´ Lst
¯(cid:15)¯t,¯ν¯t,η
ˇ
ˇ ¯Lst
¯(cid:15)¯t,¯ν¯t,η ´ Lst
ˇ
¯δt
ˇ ¯Lst
¯(cid:15)¯t,¯ν¯t,η ´ Lst
2
ˇ
ˇ ¯Lst

¯(cid:15)¯t,¯ν¯t,ηqT (cid:98)∆t ´
›
›
›2
› ¯∇Lt

ď¯αtβp ¯∇Lt
¯αtβ
(37)
ď
2

¯(cid:15)¯t,¯ν¯t,η `
¯(cid:15)¯t,¯ν¯t,ηqT (cid:98)∆t `

p ¯∇Lt

¯(cid:15)¯t,¯ν¯t,η

(C.25)

ď ´

´

`

¯(cid:15)¯t,¯ν¯t,η

¯αtβγB
2

`
¯δt
2

¯(cid:15)¯t,¯ν¯t,η ´ Lst

ˇ
ˇ
ˇ ¯Lt
ˇ `
¯(cid:15)¯t,¯ν¯t,η ´ Lt
ˇ
ˇ
ˇ ¯Lt
ˇ `

ˇ
ˇ

ˇ
ˇ

¯(cid:15)¯t,¯ν¯t,η

¯(cid:15)¯t,¯ν¯t,η

¯(cid:15)¯t,¯ν¯t,η

ˇ
ˇ

¯(cid:15)¯t,¯ν¯t,η ´ Lt
ˇ
ˇ ¯Lt
ˇ
ˇ `

`

¯(cid:15)¯t,¯ν¯t,η

ˇ
ˇ

¯(cid:15)¯t,¯ν¯t,η

¯(cid:15)¯t,¯ν¯t,η ´ Lt
ˇ
ˇ ¯Lt

¯(cid:15)¯t,¯ν¯t,η ´ Lt

ˇ
ˇ .

¯(cid:15)¯t,¯ν¯t,η

54

By Line 20 of Algorithm 1, ¯δt`1 ´ ¯δt “ pρ ´ 1q¯δt, while (C.39) still holds. By the condition of ω in
(C.40), we know that under (42) (which implies (C.40)),

Θt`1

ω ´ Θt

ω ď ´

ωβγB
12

¨ ¯αt

›
› ¯∇Lt

¯(cid:15)¯t,¯ν¯t,η

›
›2

` ω

(cid:32)ˇ
ˇ ¯Lst

¯(cid:15)¯t,¯ν¯t,η ´ Lst

¯(cid:15)¯t,¯ν¯t,η

ˇ
ˇ `

ˇ
ˇ ¯Lt

¯(cid:15)¯t,¯ν¯t,η ´ Lt
›
›∇Lt

¯(cid:15)¯t,¯ν¯t,η
›
›2

¯(cid:15)¯t,¯ν¯t,η

(

ˇ
ˇ

´

ω
12

¯δt.

(C.51)

` ρp1 ´ ωq¯αt

¯(cid:15)¯t,¯ν¯t,η ´ Lt
Lt`1

Case 2b, unreliable step, q∆t “ (cid:98)∆t. We have
ˇ
ˇ ¯Lst
¯(cid:15)¯t,¯ν¯t,η ´ Lst
ˇ
ˇ ¯Lst
›
›2

¯(cid:15)¯t,¯ν¯t,η `
¯(cid:15)¯t,¯ν¯t,ηqT (cid:98)∆t `
›
› ¯∇Lt

¯(cid:15)¯t,¯ν¯t,η
¯(cid:15)¯t,¯ν¯t,η ´ Lst
ˇ
ˇ ¯Lst

ď¯αtβp ¯∇Lt
(C.25)

¯(cid:15)¯t,¯ν¯t,η ď ¯Lst

¯(cid:15)¯t,¯ν¯t,η ´ ¯Lt

ď ´¯αtβγB

`

¯(cid:15)¯t,¯ν¯t,η

¯(cid:15)¯t,¯ν¯t,η

¯(cid:15)¯t,¯ν¯t,η ´ Lst

ˇ
ˇ

ˇ
ˇ

¯(cid:15)¯t,¯ν¯t,η

¯(cid:15)¯t,¯ν¯t,η

ˇ
ˇ
ˇ ¯Lt
ˇ `
¯(cid:15)¯t,¯ν¯t,η ´ Lt
ˇ
ˇ
ˇ ¯Lt
ˇ `
¯(cid:15)¯t,¯ν¯t,η ´ Lt
ˇ
ˇ
ˇ ¯Lt
ˇ `

¯(cid:15)¯t,¯ν¯t,η

¯(cid:15)¯t,¯ν¯t,η ´ Lt

¯(cid:15)¯t,¯ν¯t,η

ˇ
ˇ .

By Line 22 of Algorithm 1, ¯δt`1 ´ ¯δt “ ´p1 ´ 1{ρq¯δt, while (C.39) still holds. Thus, under (42),

Θt`1

ω ´ Θt

ω ď ´

ωβγB
12

¨ ¯αt

›
› ¯∇Lt

¯(cid:15)¯t,¯ν¯t,η

›
›2

` ω

(cid:32)ˇ
ˇ ¯Lst

¯(cid:15)¯t,¯ν¯t,η

¯(cid:15)¯t,¯ν¯t,η ´ Lst
›
›∇Lt

¯(cid:15)¯t,¯ν¯t,η

(

ˇ
ˇ

ˇ
ˇ ¯Lt

ˇ
ˇ `
›
›2

¯(cid:15)¯t,¯ν¯t,η ´ Lt
1
2

p1 ´ ωq

´

¯(cid:15)¯t,¯ν¯t,η
ˆ

1 ´

` ρp1 ´ ωq¯αt

˙

1
ρ

¯δt.

(C.52)

Case 3b, unsuccessful step, q∆t “ (cid:98)∆t. In this case, (C.37) holds. Combining (C.51), (C.52), and
(C.37), we note that (C.50) holds as well. Thus, (C.50) holds for all six cases. This completes the
proof.

C.8 Proof of Theorem 4.14

We write at “ Opbtq if at ď Cbt for some constant C and for all suﬃciently large t.

By Lemma 3.2, (C.14) and repeating the proof of Lemma 4.7(b) without sampling, there exists

Υ1 ą 0 such that

Rt ď Υ1

›
¨
›
›
›
˝
›
›

∇xLt
ct
wt

¯(cid:15)¯t,¯ν¯t

˛

›
›
›
›
‚
›
›

Lemma 4.7pbq
ď

"

Υ1pC2 ` 1q

}∇Lt

¯(cid:15)¯t,¯ν¯t,η} `

›
›
›
›

ˆ

ct
wt

¯(cid:15)¯t,¯ν¯t

˙›
*
›
›
›

.

For t ě ¯t ` 1, two parameters ¯(cid:15)¯t, ¯ν¯t are ﬁxed conditional on any σ-algebra F Ě F¯t. Thus,
˙›
›
›
› | Ft´1
“›
› ¯∇Lt

›
› | Ft´1
¯(cid:15)¯t,¯ν¯t,η
›
‰
› | Ft´1

ct
wt
¯(cid:15)¯t,¯ν¯t
¯(cid:15)¯t,¯ν¯t,η} ` E

˙›
›
›
› “E

¯(cid:15)¯t,¯ν¯t,η ´ ∇Lt

“›
› ¯∇Lt

(27)
ď E

ď}∇Lt

ct
wt

›
ˆ
›
›
›

„›
›
›
›

¯(cid:15)¯t,¯ν¯t,η

¯(cid:15)¯t,¯ν¯t

ˆ



‰

.

Since |ξt
estimate ¯∇Lt
Combining with the above two displays, there exists Υ2 ą 0 such that

1| is increasing, we know |ξt
¯(cid:15)¯t,¯ν¯t,η is in order of 1{|ξt

1| ě t. Furthermore, by Lemma 4.7(a), the variance of the
1| “ Op1{tq. Thus, Er} ¯∇Lt
¯(cid:15)¯t,¯ν¯t,η ´ ∇Lt
tq.

¯(cid:15)¯t,¯ν¯t,η} | Ft´1s “ Op1{

?

Rt ď Υ2}∇Lt

¯(cid:15)¯t,¯ν¯t,η} ` Op1{

tq.

?

55

Since ¯αt ď αmax, we have ¯αt{t Ñ 0 as t Ñ 8. Thus, it suﬃces to show the convergence of
¯(cid:15)¯t,¯ν¯t,η}2. By Theorem 4.13, we sum up the error recursion for t ě ¯t ` 1, take conditional
¯αt}∇Lt
expectation on F¯t, and have

8ÿ

t“¯t`1

Er¯αt}∇Lt

¯(cid:15)¯t,¯ν¯t,η}2 | F¯ts

ď

4ρ
p1 ´ pgradqp1 ´ pf qp1 ´ ωqpρ ´ 1q

ď

4ρ
p1 ´ pgradqp1 ´ pf qp1 ´ ωqpρ ´ 1q

8ÿ

t“¯t`1
ˆ

ErΘt

ω | F¯ts ´ E

“

‰
| F¯t

Θt`1
ω
˙

Θ¯t`1

ω ´ min

X ˆMˆΛ

ωL¯(cid:15)¯t,¯ν¯t,η

ă 8.

Thus, we have

“

‰
¯(cid:15)¯t,¯ν¯t,η}2 | F¯t

E

¯αt}∇Lt

lim
tÑ8
¯(cid:15)¯t,¯ν¯t,η}2 is bounded, we apply the dominated convergence theorem
¯(cid:15)¯t,¯ν¯t,η}2 is non-negative, we further obtain

“ 0.

Noting that ¯αt ď αmax and }∇Lt
and have Er lim
tÑ8
limtÑ8 ¯αt}∇Lt

¯αt}∇Lt

¯(cid:15)¯t,¯ν¯t,η}2 | F¯ts “ 0. Since ¯αt}∇Lt
¯(cid:15)¯t,¯ν¯t,η}2 “ 0 almost surely. This completes the proof.

C.9 Proof of Theorem 4.15

We adapt the proof of (Na et al., 2021, Theorem 4.12). By Theorem 4.14, it suﬃces to show that
the “limsup” of the random stepsize sequence t¯αtut is lower bounded away from zero. To show this,
we deﬁne two stepsize sequences as follows. For any t ą ¯t ` 1, we let

φt “ logp¯αtq,
ϕt “ mintlogpτ q, 1E t´1

1 XE t´1

2

plogpρq ` ϕt´1q ` p1 ´ 1E t´1

1 XE t´1

2

qpϕt´1 ´ logpρqqu,

and let φ¯t`1 “ ϕ¯t`1 “ logp¯α¯t`1q. Here, τ is a deterministic constant such that

τ ď

1 ´ β
Υ1pκgrad ` κf ` 1q

^ αmax

and τ “ ρ´iαmax for some i ą 0. The ﬁrst constant comes from Lemma 4.8. We aim to show
φt ě ϕt, @t ě ¯t ` 1.

First, we note that by the stepsize speciﬁcation in Lines 18 and 25 of Algorithm 1 (Line 13 is
not performed since t ě ¯t ` 1), ¯αt “ ρjtτ for some integer jt. Second, we note that φt and ϕt are
both Ft´1-measurable, that is, they are ﬁxed conditional on Ft´1. Third, we show that φt ě ϕt by
induction. Note that φ¯t`1 “ ϕ¯t`1. Suppose φt ě ϕt, we consider the following three cases.
(a). If φt ą logpτ q, then φt ě logpτ q ` logpρq. Thus, φt`1 ě φt ´ logpρq ě logpτ q ě ϕt`1.
(b). If φt ď logpτ q and 1E t

“ 1, then Lemma 4.8 leads to

1XE t
2

φt`1 “ mintlogpαmaxq, φt ` logpρqu ě mintlogpτ q, ϕt ` logpρqu “ ϕt`1.

(c). If φt ď logpτ q and 1E t

1XE t
2

“ 0, then

φt`1 ě φt ´ logpρq ě ϕt ´ logpρq ě ϕt`1.

56

Combining the above three cases, we have φt ě ϕt, @t ě ¯t ` 1. Note that, conditional on F¯t,
tϕtutě¯t`1 is a random walk with a maximum and a drift upward (cf. (Gallager, 2013, Example
6.1.2)). Thus, lim suptÑ8 ϕt ě logpτ q almost surely. In particular, we have
˙
ż

˙

ˆ

ˆ

P

lim sup
tÑ8

φt ě logpτ q

“

P

φt ě logpτ q | Fi, ¯t “ i

lim sup
tÑ8

ˆ

P pFi, ¯t “ iq
˙

ϕt ě logpτ q | Fi, ¯t “ i

P pFi, ¯t “ iq

P

Fi

lim sup
tÑ8

P pFi, ¯t “ iq

8ÿ

i“0

Fi
ż

8ÿ

φtěϕt
ě

i“0
ż

8ÿ

“

i“0
“ 1,

Fi

which means that the “limsup” of ¯αt is lower bounded almost surely. Using Theorem 4.14, we
complete the proof.

References

A. S. Bandeira, K. Scheinberg, and L. N. Vicente. Convergence of trust-region methods based on

probabilistic models. SIAM Journal on Optimization, 24(3):1238–1264, 2014.

A. S. Berahas, L. Cao, and K. Scheinberg. Global convergence rate analysis of a generic line search

algorithm with noise. SIAM Journal on Optimization, 31(2):1489–1518, 2021a.

A. S. Berahas, F. E. Curtis, M. J. O’Neill, and D. P. Robinson. A stochastic sequential quadratic op-
timization algorithm for nonlinear equality constrained optimization with rank-deﬁcient jacobians.
arXiv preprint arXiv:2106.13015, 2021b.

A. S. Berahas, F. E. Curtis, D. Robinson, and B. Zhou. Sequential quadratic optimization for
nonlinear equality constrained stochastic optimization. SIAM Journal on Optimization, 31(2):
1352–1379, 2021c.

D. Bertsekas. Constrained Optimization and Lagrange Multiplier Methods. Elsevier, Belmont, Mass,

1982.

J. R. Birge. State-of-the-art-survey—stochastic programming: Computation and applications.

INFORMS Journal on Computing, 9(2):111–133, 1997.

J. Blanchet, C. Cartis, M. Menickelly, and K. Scheinberg. Convergence rate analysis of a stochastic
trust-region method via supermartingales. INFORMS Journal on Optimization, 1(2):92–119,
2019.

P. T. Boggs and J. W. Tolle. Sequential quadratic programming. In Acta numerica, 1995, volume 4

of Acta Numer., pages 1–51. Cambridge University Press (CUP), 1995.

R. Bollapragada, R. Byrd, and J. Nocedal. Adaptive sampling strategies for stochastic optimization.

SIAM Journal on Optimization, 28(4):3312–3343, 2018.

57

L. Bottou, F. E. Curtis, and J. Nocedal. Optimization methods for large-scale machine learning.

SIAM Review, 60(2):223–311, 2018.

R. H. Byrd, G. M. Chin, J. Nocedal, and Y. Wu. Sample size selection in optimization methods for

machine learning. Mathematical Programming, 134(1):127–155, 2012.

C. Cartis and K. Scheinberg. Global convergence rate analysis of unconstrained optimization

methods based on probabilistic models. Mathematical Programming, 169(2):337–375, 2017.

C. Chen, F. Tung, N. Vedula, and G. Mori. Constraint-aware deep neural network compression. In

Computer Vision – ECCV 2018, pages 409–424. Springer International Publishing, 2018.

R. Chen, M. Menickelly, and K. Scheinberg. Stochastic optimization using a trust-region method

and random models. Mathematical Programming, 169(2):447–487, 2017.

F. H. Clarke. Optimization and Nonsmooth Analysis. Society for Industrial and Applied Mathematics,

1990.

H. Cleef and W. Gual. Project scheduling via stochastic programming. Mathematische Operations-

forschung und Statistik. Series Optimization, 13(3):449–468, 1982.

F. E. Curtis, M. J. O’Neill, and D. P. Robinson. Worst-case complexity of an sqp method for
nonlinear equality constrained stochastic optimization. arXiv preprint arXiv:2112.14799, 2021a.

F. E. Curtis, D. P. Robinson, and B. Zhou. Inexact sequential quadratic optimization for minimizing
a stochastic objective function subject to deterministic nonlinear equality constraints. arXiv
preprint arXiv:2107.03512, 2021b.

S. De, A. Yadav, D. Jacobs, and T. Goldstein. Automated Inference with Adaptive Batches.
volume 54 of Proceedings of Machine Learning Research, pages 1504–1513, Fort Lauderdale, FL,
USA, 2017. PMLR.

D. di Seraﬁno, N. Kreji´c, N. K. Jerinki´c, and M. Viola. Lsos: Line-search second-order stochastic

optimization methods. arXiv preprint arXiv:2007.15966, 2020.

R. Durrett. Probability, volume 49. Cambridge University Press, 2019.

F. Facchinei. Minimization of SC1 functions and the maratos eﬀect. Operations Research Letters,

17(3):131–137, 1995.

M. P. Friedlander and M. Schmidt. Hybrid deterministic-stochastic methods for data ﬁtting. SIAM

Journal on Scientiﬁc Computing, 34(3):A1380–A1405, 2012.

E. H. Fukuda and M. Fukushima. A note on the squared slack variables technique for nonlinear

optimization. Journal of the Operations Research Society of Japan, 60(3):262–270, 2017.

R. G. Gallager. Stochastic Processes. Cambridge University Press, 2013.

P. E. Gill, W. Murray, and M. A. Saunders. SNOPT: An SQP algorithm for large-scale constrained

optimization. SIAM Review, 47(1):99–131, 2005.

58

C. K. Goh, Y. Liu, and A. W. K. Kong. A constrained deep neural network for ordinal regression.

In 2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition. IEEE, 2018.

A. J. Goldman and A. W. Tucker. 4. theory of linear programming. In Linear Inequalities and

Related Systems. (AM-38), pages 53–98. Princeton University Press, 1957.

N. I. M. Gould, D. Orban, and P. L. Toint. CUTEst: a constrained and unconstrained testing
environment with safe threads for mathematical optimization. Computational Optimization and
Applications, 60(3):545–557, 2014.

S. Gratton, C. W. Royer, L. N. Vicente, and Z. Zhang. Complexity and global rates of trust-region
methods based on probabilistic models. IMA Journal of Numerical Analysis, 38(3):1579–1597,
2017.

J.-B. Hiriart-Urruty, J.-J. Strodiot, and V. H. Nguyen. Generalized hessian matrix and second-order
optimality conditions for problems withC 1,1 data. Applied Mathematics & Optimization, 11(1):
43–56, 1984.

C. Kanzow. An active set-type newton method for constrained nonlinear systems. In Complemen-

tarity: Applications, Algorithms and Extensions, pages 179–200. Springer US, 2001.

N. Kreji´c and N. Krklec. Line search methods with variable sample size for unconstrained optimiza-

tion. Journal of Computational and Applied Mathematics, 245:213–231, 2013.

C. K. Liew. A two-stage least-squares estimation with inequality restrictions on parameters. The

Review of Economics and Statistics, 58(2):234, 1976a.

C. K. Liew. Inequality constrained least-squares estimation. Journal of the American Statistical

Association, 71(355):746–751, 1976b.

I. E. Livieris and P. Pintelas. An adaptive nonmonotone active set – weight constrained – neural

network training algorithm. Neurocomputing, 360:294–303, 2019a.

I. E. Livieris and P. Pintelas. An improved weight-constrained neural network training algorithm.

Neural Computing and Applications, 32(9):4177–4185, 2019b.

S. Lucidi. New results on a class of exact augmented lagrangians. Journal of Optimization Theory

and Applications, 58(2):259–282, 1988.

S. Lucidi. Recursive quadratic programming algorithm that uses an exact augmented lagrangian

function. Journal of Optimization Theory and Applications, 67(2):227–245, 1990.

S. Lucidi. New results on a continuously diﬀerentiable exact penalty function. SIAM Journal on

Optimization, 2(4):558–574, 1992.

D. P. Morton. Testing solution quality in stochastic programs. In Operations Research Proceedings

2002, pages 395–400. Springer Berlin Heidelberg, 2003.

D. P. Morton and E. Popova. A bayesian stochastic programming approach to an employee scheduling

problem. IIE Transactions, 36(2):155–167, 2004.

59

S. Na, M. Anitescu, and M. Kolar. An adaptive stochastic sequential quadratic programming with

diﬀerentiable exact augmented lagrangians. arXiv preprint arXiv:2102.05320, 2021.

J. Nocedal and S. J. Wright. Numerical Optimization. Springer Series in Operations Research and

Financial Engineering. Springer New York, 2nd edition, 2006.

A. E. Onuk, M. Akcakaya, J. P. Bardhan, D. Erdogmus, D. H. Brooks, and L. Makowski. Constrained
maximum likelihood estimation of relative abundances of protein conformation in a heterogeneous
mixture from small angle x-ray scattering intensity measurements. IEEE Transactions on Signal
Processing, 63(20):5383–5394, 2015.

F. Oztoprak, R. Byrd, and J. Nocedal. Constrained optimization in the presence of noise. arXiv

preprint arXiv:2110.04355, 2021.

C. Paquette and K. Scheinberg. A stochastic line search method with expected complexity analysis.

SIAM Journal on Optimization, 30(1):349–376, 2020.

R. F. Phillips. A constrained maximum-likelihood approach to estimating switching regressions.

Journal of Econometrics, 48(1-2):241–262, 1991.

G. D. Pillo and L. Grippo. A new class of augmented lagrangians in nonlinear programming. SIAM

Journal on Control and Optimization, 17(5):618–628, 1979.

G. D. Pillo and L. Grippo. A new augmented lagrangian function for inequality constraints in
nonlinear programming problems. Journal of Optimization Theory and Applications, 36(4):
495–519, 1982.

G. D. Pillo and L. Grippo. A continuously diﬀerentiable exact penalty function for nonlinear
programming problems with inequality constraints. SIAM Journal on Control and Optimization,
23(1):72–84, 1985.

G. D. Pillo and L. Grippo. An exact penalty function method with global convergence properties

for nonlinear programming problems. Mathematical Programming, 36(1):1–18, 1986.

G. D. Pillo, L. Grippo, and F. Lampariello. A method for solving equality constrained optimization
problems by unconstrained minimization. In Optimization Techniques, volume 23 of Lecture Notes
in Control and Information Sci., pages 96–105. Springer-Verlag, 1980.

G. D. Pillo, G. Liuzzi, and S. Lucidi. An exact penalty-lagrangian approach for large-scale nonlinear

programming. Optimization, 60(1-2):223–252, 2011a.

G. D. Pillo and S. Lucidi. An augmented lagrangian function with improved exactness properties.

SIAM Journal on Optimization, 12(2):376–406, 2002.

G. D. Pillo, S. Lucidi, and L. Palagi. Convergence to second-order stationary points of a primal-dual
algorithm model for nonlinear programming. Mathematics of Operations Research, 30(4):897–915,
2005.

G. D. Pillo, G. Liuzzi, S. Lucidi, and L. Palagi. A truncated newton method in an augmented
lagrangian framework for nonlinear programming. Computational Optimization and Applications,
45(2):311–352, 2008.

60

G. D. Pillo, , G. Liuzzi, and S. L. and. A primal-dual algorithm for nonlinear programming exploiting
negative curvature directions. Numerical Algebra, Control & Optimization, 1(3):509–528, 2011b.

S. Silvapulle. Constrained Statistical Inference, volume 912. John Wiley & Sons, 2004.

J. A. Tropp. An introduction to matrix concentration inequalities. Foundations and Trends® in

Machine Learning, 8(1-2):1–230, 2015.

M. Xu, J. J. Ye, and L. Zhang. Smoothing augmented lagrangian method for nonsmooth constrained

optimization problems. Journal of Global Optimization, 62(4):675–694, 2014.

V. M. Zavala and M. Anitescu. Scalable nonlinear programming via exact diﬀerentiable penalty

functions and trust-region Newton methods. SIAM J. Optim., 24(1):528–558, 2014.

Government License: The submitted manuscript has been created by UChicago Argonne, LLC, Operator of Argonne
National Laboratory (“Argonne”). Argonne, a U.S. Department of Energy Oﬃce of Science laboratory, is operated
under Contract No. DE-AC02-06CH11357. The U.S. Government retains for itself, and others acting on its behalf, a
paid-up nonexclusive, irrevocable worldwide license in said article to reproduce, prepare derivative works, distribute
copies to the public, and perform publicly and display publicly, by or on behalf of the Government. The Department
of Energy will provide public access to these results of federally sponsored research in accordance with the DOE Public
Access Plan. http://energy.gov/downloads/doe-public-access-plan.

61

