Design Smells in Deep Learning Programs: An
Empirical Study

Amin Nikanjam, Foutse Khomh
SWAT Lab., Polytechnique Montr´eal, Montr´eal, Canada
{amin.nikanjam, foutse.khomh}@polymtl.ca

1
2
0
2

l
u
J

7

]
E
S
.
s
c
[

2
v
9
7
2
2
0
.
7
0
1
2
:
v
i
X
r
a

Abstract—Nowadays, we are witnessing an increasing adop-
tion of Deep Learning (DL) based software systems in many
industries. Designing a DL program requires constructing a
deep neural network (DNN) and then training it on a dataset.
This process requires that developers make multiple architectural
(e.g., type, size, number, and order of layers) and conﬁguration
(e.g., optimizer, regularization methods, and activation functions)
choices that affect the quality of the DL models, and consequently
software quality. An under-speciﬁed or poorly-designed DL
model may train successfully but is likely to perform poorly
when deployed in production. Design smells in DL programs
are poor design and–or conﬁguration decisions taken during the
development of DL components, that are likely to have a negative
impact on the performance (i.e., prediction accuracy) and then
quality of DL based software systems. In this paper, we present
a catalogue of 8 design smells for a popular DL architecture,
namely deep Feedforward Neural Networks which is widely
employed in industrial applications. The design smells were iden-
tiﬁed through a review of the existing literature on DL design and
a manual inspection of 659 DL programs with performance issues
and design inefﬁciencies. The smells are speciﬁed by describing
their context, consequences, and recommended refactorings. To
provide empirical evidence on the relevance and perceived impact
of the proposed design smells, we conducted a survey with 81
DL developers. In general, the developers perceived the proposed
design smells as reﬂective of design or implementation problems,
with agreement levels varying between 47% and 68%.

Index Terms—Design smells, Deep Learning, Software Quality

I. INTRODUCTION

Nowadays, we are observing an increasing deployment
of software systems based on Deep Learning (DL) in real
life, from personal banking to autonomous driving [1]. A
DL program encodes the network structure of a desirable
DL model and the process by which the model learns from
a training dataset. Easy-to-use libraries such as Keras have
been introduced to simplify the development process of DL
programs. However, leveraging these libraries to implement a
DL program is still challenging, in particular for developers
who are not experts in Machine Learning (ML) and neural
networks. A developer must make multiple architectural (e.g.,
type, size, number, and order of layers) and conﬁguration
(e.g., optimizer, regularization methods, and activation func-
tions) choices that affect the quality of the DL models, and
consequently software quality. A poorly-designed DL model
may train successfully but is likely to perform poorly when
deployed in production. Design smells in DL programs are
poor design and–or conﬁguration decisions that can have a
negative impact on the performance and then quality of a DL-

based software system. By performance, we mean accuracy
of prediction,
like precision of classifying samples in the
correct target class, that may affect the quality of ﬁnal deci-
sions. In software engineering, traditionally code/design smells
deal with non-functional requirements such as testability or
maintainability, but in ML-based systems the accuracy can be
regarded as a functional requirement. In this paper, we deﬁne
design smells in DL programs as poorly designed/conﬁgured
models that may affect the entire performance, i.e. prediction
accuracy, of DL-based systems. An example of a poor design
decision in a DL model and its refactored version are shown
in Fig. 1. When training the model to detect images of hand-
written digits, the developer selected an inadequate optimiser
at the last line; i.e., “Adam” in compile function instead of
Stochastic Gradient Descent (SGD) optimizer as pointed in
the correct answer, which caused the accuracy of the model to
remained unchanged between epochs 2 to 10. Consequently,
the model was not able to train well on the data, leading to
a low classiﬁcation accuracy. Such low classiﬁcation accuracy
results in poor decisions like misclassiﬁcation of input images.
Changing the optimizer led to successfully addressing the
problem and the performance improved signiﬁcantly.

Deploying a DL model with poor performance can have
severe consequences, especially in the context of safety-critical
systems. It is therefore important to raise the awareness of
development teams about poor design and conﬁguration issues
that are likely to have a negative impact on the quality of DL
models. Design smells can cause a program to exhibit extraor-
dinary poor accuracy or other low quality outputs during the
execution phase. Having a list of known bad design practices
for DL models can help developers avoid pitfalls during the
development of their DL programs; resulting in better software
quality. Although poor design choices and performance issues
in DL programs have been studied previously [2]–[5], to the
best of our knowledge, this paper is the ﬁrst empirical study
on design smells in DL programs.

In this paper, we propose a catalog of 8 design smells in DL
models with a focus on deep Feedforward Neural Networks
(FNN) that use convolutional components. Fig. 2 illustrates
the schematic diagram of our study in this paper. We start by
conducting an investigation to determine the type of smells and
their prevalence using two main sources: (1) previous research
studies that highlighted bad practices in designing DL models,
and (2) DL programs with design or performance issues.
We have identiﬁed two main categories of design smells:

 
 
 
 
 
 
Fig. 1. A poorly-designed model (left) and its refactored version (right). The optimizer has been changed to improve the performance in a classiﬁcation
problem. The recommended changes have been highlighted by the red color (simpliﬁed from SO 37213388).

Formation of the feature map and usage of regularization
methods. Context, consequences and recommended refactor-
ings for removing each smell are speciﬁed in the catalogue
with some examples from real DL programs. Finally,
the
relevance of design smells are assessed by running a survey
among 81 eligible DL developers/researchers. In general, the
developers perceived the proposed design smells as reﬂective
of design or implementation problems, with agreement levels
varying between 47% and 68%. The contributions of this
paper are: 1) proposing a catalogue of 8 design smells in DL
models, and 2) validating the catalogue through a survey with
81 eligible DL developers/researchers.

The remainder of this paper is organised as follows. Sec-
tion II brieﬂy reviews background knowledge about DL, deep
FNNs and the development of DL program/models. Section
III introduces the methodology adopted for the identiﬁcation
of smells and a full description of the identiﬁed design smells
in DL models. Section IV presents the design of the survey
used to validate the proposed design smells, and the obtained
results. Section V discusses threats to the validity of this study.
Finally, we conclude the paper and discuss future work in
Section VI.

II. BACKGROUND

A. Feedforward Neural Networks (FNN)

FNN [6] is the principal neural network architecture used for
solving classiﬁcation and function approximation problems,
where the task is to learn a mapping function capable of
converting input data to a target output. FNN consists of
several, and sometimes diverse, sequences of layers of compu-
tational units. These computational layers are trained to extract
features hierarchically. This starts from low-level features in
early layers to high-level ones in middle layers. FNN, then,
detects discriminative and informative patterns in last layers,
which serve it to derive either the class label (in classiﬁcation
problems) or continuous outcome (in function approximation
problems). It is called feedforward because the information
ﬂows in a forward manner from the input layer, through the
hidden layers and to the output layer, e.g., a class probability or
a predicted real value. The basic FNN architecture consists of
stacking dense layers, where all the neurons of two consecutive
layers are fully-connected.

The regularization is required to improve the convergence
and generalizability of the training procedure of DNNs. Many
regularization techniques have been proposed and the most
used ones are dropout and batch normalisation (batchnorm).
Dropout [7] masks at every training iteration a random subset
of units (i.e., nullify them). The stochasticity injected into the
inference calculation, only during the training, prevents the
co-adaptation of feature detectors and encourages the DNN
to learn robust patterns against partially-hidden information.
Batchnorm [8] acts differently on activations by normalizing
their values using statistics (i.e., mean and variance) of the
current batch of data during the training. During the testing,
it updates internally, the population statistics of all batches
for each level of activations in order to switch to normalizing
against population, rather than batch, statistics. This normal-
ization of intermediary inputs data has shown its effectiveness
in smoothing the loss landscape, which ensures faster and safer
training convergence with high potential to escape weak local
minima.

Convolutional architectures represent a particular type of
FNN designed for multi-dimensional input data, such as 2D
images, audio spectrograms, or 3D videos [9]. The beneﬁt
of Convolutional Neural Networks (CNN) lies in their ability
to take into account the spatial information in their feature
extraction process. To do that, CNNs stack, earlier,
two
specialized layers:

• Convolutional layer: it applies spatial ﬁlters over the input
data and each ﬁlter’s weights are learned to detect relevant
features supporting the network’s task. Thus, it yields a
feature map for each learned ﬁlter, where each unit is
connected to a local region (i.e., size of spatial ﬁltering
window) in its previous layer’s feature maps.

• Pooling layer: this layer performs spatial pooling over the
computed feature map to reduce its dimensionality and
retain the most relevant information. The spatial pooling
can be either average or max aggregation that computes,
respectively, the average or max of all the units in the
speciﬁed spatial window.

Indeed, some bad conﬁgurations and poor design choices
may deﬁnitely introduce inefﬁciencies on the internal func-
tioning of the FNN or one of its components, which can hinder
the expressiveness of mapping functions or computational

Fig. 2. Schematic diagram of our study.

resource consumption. Such conﬁgurations or design choices
have been reported in several studies as a root cause of bad
performance in DL programs [2], [3]. DL researchers have
studied performance issues in DL models [4], [5] as well.
Moreover, other researchers have reported some principles and
best practices for designing CNN [10], [11].

B. Developing DL programs

The development of DL programs lies in constructing the
Deep Neural Network (DNN) by calling built-in DL routines
to create layers (processing units), then connecting them by
either feeding one or more layers’ outputs as inputs to another.
Then, the developer should train the DNN by conﬁguring a
learning algorithm on a dataset. The training process consists
in updating iteratively the DNN’s parameters, towards mini-
mizing the loss of DNN’s predictions compared to the training
data. A loss/cost function is deﬁned to estimate the average
distance between predicted and actual outcomes. Commonly,
the best-ﬁtted FNN is found after multiple epochs (i.e., passes
over all the training data).

However, leveraging DL libraries to implement a DNN and
then a training program for the designed DNN is not straight-
forward and it can be error-prone. DL libraries often have to
trade off between the coverage of novel DL functionalities
and the ease of rapid implementation and extension of DNN
software prototypes. As a compromise solution,
they uni-
formly include, for each newly-implemented DL functionality,
a bundle of automated steps and default settings following
its common usage trends. This enables quick prototyping
of regular DNNs while keeping the ﬂexibility to try other
conﬁgurations with the tweakable setting options available for
every provided DL routine. As a consequence, DL developers
should be aware of the intricacies of these DL libraries to
choose the appropriate conﬁgurations and avoid breaking their
implicit assumptions in regard to the usage of their built-in
routines.

III. DESIGN SMELLS IN DL MODELS

In this section, ﬁrst we describe our methodology for
eliciting design smells by analyzing existing literature and
related DL programs. Then, we explain identiﬁed design
smells in feedforward DL models in detail. We explain the

context of each smell, its characteristics, consequences, and the
recommended refactoring to address it, following the template
provided by Brown et al. [12]. Moreover, code snippets are
provided as examples in some cases.

A. Methodology

In this study, we focus speciﬁcally on FNNs. This pop-
ular architecture inside the DL community is considered as
“quintessential” in DL and they has many industrial appli-
cations like object recognition from images [6]. In fact, a
special feedforward architecture which is called Convolutional
Neural Network (CNN) has shown its effectiveness on public
computer vision datasets and competitions such as ImageNet
classiﬁcation [13] or COCO object detection [14]. Moreover,
FNN is a conceptual milestone on the road to recurrent
networks that are employed widely in Natural Language
applications. Thus, we limit our study to deep FNNs and do
not consider other DL models.

The goal of this study is to identify design smells that
could affect the performance of a DL program. We examined
two main sources of information to identify such smells: (1)
previous research studies that highlighted performance issues
in DL models, and (2) DL programs that exhibited design or
performance issues. We reviewed empirical research studies on
DNN design principles and bad performance in DL programs
to identify frequent and inﬂuential design smells in deep
FNNs, including poor design choices/conﬁgurations that lead
to bad performance in DL programs [2], [3], performance
issues in DL models [4], [5], and reported principles and best
practices for designing CNN [10], [11].

The second source of information about design smells is
real DL programs that have design inefﬁciencies. To ﬁnd a
proper set of real-world design smells in DL programs, we
have used two main sources: 1) samples found by directly
searching over SO with keywords related to such issues, and 2)
public datasets of faulty DL programs (from SO and GitHub)
released by previous research studies. For the former, we chose
SO because it is the most popular Q&A forum for software
development and has been leveraged by previous studies on
DL software systems [2], [3], [15]. Since TensorFlow and
Keras are very popular among DL developers, in this paper

659 DL programsPublic datasetsfrom Stack Overflow and GitHubStack OverflowResearch papers8 design smells in DL programsAssessmentOnline survey81 RespondentsAnalysis Findingswe searched SO posts tagged by one of these libraries with
the objective of collecting relevant DL models/programs. We
reﬁned our search queries with keywords related to the scope
of our study: “low performance”, “bad performance” and
“design issues”. We consider SO posts, containing full code
scripts or code snippets that are related to one or multiple
issues since we need to investigate the code to understand the
potential design smell. Also, we have searched for publicly
released datasets of faulty DL programs (including design
issues and low performance) by checking replication packages
of all published papers that studied problems in DL programs.
Finally, we obtained four publicly available datasets of faulty
DL programs gathered from SO and GitHub [2], [3], [16], [17].
All these studies investigated various faulty DL programs from
SO and GitHub for their own research objectives including
empirical study of bugs occurring in DL software systems
written by TensorFlow, PyTorch and Caffe [2], [3], proposing
a taxonomy of real faults occurred in DL software systems
[16] and bug ﬁx patterns in DL programs [17].

For inspecting collected DL programs from either direct
searching over SO or public datasets, we relied on certain
inclusion and exclusion criteria to ﬁnd relevant programs for
identifying design smells:

• The program must have performance issues (e.g., low

accuracy or detection precision),

• The issue must not

lead to program crash, hang or
incorrect functionality. The program should be able to
run and produce results,

• The DL program must be developed using TensorFlow

or Keras,

• The DL model must be FNN,
This process left us with 659 DL programs to be analyzed.
We have manually inspected all these artifacts to ﬁnd relevant
examples to identify design smells. We have used an open
coding procedure [18]. A shared document including the link
to all artifacts have been used to make it possible for all
authors to work together during the analysis. Each artifact was
inspected by reading speciﬁc parts of its document (code snip-
pet, comment, description) and all related discussion provided
by the developer or other users (for samples from SO). Each
sample was inspected by at least two of the authors to make
sure that the root cause of the performance issue was a design
inefﬁciency and was not related to generic programming faults
or implementation issues.

After analyzing all these data sources, we have derived a cat-
alogue of 8 distinct design smells in deep FNN (a popular DL
architecture). Since the arrangement of convolutions/poolings
layers for extracting features and type/location of regularizers
are two signiﬁcant factors that affect the performance of deep
FNNs, so we present the smells organised in two categories:
Formation of the feature map and usage of regularization.

B. Formation of the feature map, convolutions and poolings
layers

Context: Conventionally, a CNN architecture incorporates
a bundle of convolutional layers with increasing ﬁlters count

Fig. 3. A part of DL program mentioned in SO 50426349 as an example of
design smell No. 1.

and separated by pooling layers to shrink gradually the
feature map area. Hence, the extracted feature space tends
to become deeper and narrower
the network
until it becomes ready to be ﬂatten and fed to the dense
layers in charge of mapping the features into the target output.

throughout

1. Non-expanding feature map
Bad smell description: A possible design mistake in CNNs
is keeping the number of features the same (or even decrease
it) as the architecture gets deeper. There should be a balance
between retaining the detected features (and corresponding
spatial relationship between them) and increasing the depth
of the network [19].
Consequences: If the developer fails to have a proper balance
between the depth and size of the feature map, the overall
performance would be negatively affected. While the stack of
convolution and pooling layers extract and then compress the
relevant feature map, if the architecture cannot increase the
number of features, it will fail to deliver promising features
to the dense layers.
Recommended Refactoring: The number of feature maps
should be gradually expanded while the feature map area is
retracted. The growth of feature maps count is recommended
[19] to compensate the loss of representational expressiveness
caused by the continuous decreasing of the spatial resolution
of the learned feature maps. Therefore, throughout the layers,
the feature space becomes synchronously narrower and deeper
until it gets ready to be ﬂatten and fed as input vector to the
dense layers.
Example: An example of this bad smell is illustrated in Fig.
3 extracted from SO post #50426349. The developer did not
grow the number of feature maps through layers 4 to 6. The
number of layers and the size of 2-Dimensional convolution
layers in the code snippet are highlighted in red.

2. Losing local correlation
Bad smell description: In CNNs, promising features are
extracted and then delivered to the dense layers by the stack
of convolutional layers. For an effective feature extraction,
setting proper window size for spatial ﬁltering is crucial.

stacking a set convolution and pooling layers without
appropriate conﬁguration is a bad practice among DL
developers. Even with proper adjustment of the number of
features, the size of the local window, and the area of feature
map along convolutional/pooling layers (as mentioned in the
Non-expanding feature map and the Losing local correlation
smells), efﬁcient feature extraction can be affected by the
lack of sufﬁcient convolutional blocks [23]. DL developers
are used to deﬁne only one convolutional layer at each stage
of a cascade of convolutional/pooling layers and increase the
kernel size if it does not work properly. Depending on the
application and the input data, usually, only one block of
convolutional with large spatial ﬁltering size at each stage
is the minimum that
the model needs to extract effective
features efﬁciently.
Consequences: Only one convolutional block may not be
enough for providing the required nonlinearity of feature
extraction. On the other hand,
large kernel sizes increase
the computational burden signiﬁcantly. As an example,
recent NVIDIA cuDNN library (version 5.x or higher) is not
optimized for larger kernels such as 5 × 5 and 7 × 7, whereas
CNN with entirely 3 × 3 ﬁlters achieved a substantial boost
in cuDNN performance [24].
Recommended refactoring: Deep CNN should favor blocks
of 2, 3, or even 4 homogeneous convolutional layers with
similar characteristics. Advanced CNN architectures [9], [23],
[25] have shown the beneﬁt of having several homogeneous
groups of layers, where each one is specialized to achieve a
particular goal. Indeed, building blocks of convolutional layers
with similar characteristics (i.e., the same number of feature
maps and feature map sizes) increases the homogeneity and
the structure symmetry within the CNN. Hence, larger kernels
can be replaced into a cascade of smaller ones, e.g., one 5 ×
5 can be replaced by two 3 × 3, or four 2 × 2 kernels. Spatial
ﬁltering with reduced size enhances the nonlinearity and
yields better accuracy [21]. Moreover, it massively decreases
the computation power requirement.

4. Too much down-sampling
Bad smell description: Usually DL developers deﬁne a
pooling layer (down-sampling) after any convolutional layer.
While down-sampling is inevitable in CNN models, it is not
a good practice to perform the down-sampling right after
each convolutional layer particularly for early layers.
Consequences: Larger feature-maps, especially in the early
layers, provide more valuable information for the CNN to
utilize and improve its discriminative power [22], [26], [27].
Therefore, it is crucial to avoid prematurely down-sampling
the model
and excessive appliance of pooling. Otherwise,
will lose some information extracted in early layers resulting
in poor performance.
Recommended refactoring: Deep CNN should not apply
pooling after every convolution. For instance, we use, as an
approximation, the minimum of 10 layers to consider a CNN
deep and 1/3 as threshold for the proportion of pooling layers
with respect to the total of convolutional layers (convolution

Fig. 4. A part of DL model from SO 38584268 as an example of design
smell No. 2.

If the developer does not grow the window size when the
model gets deeper, the model will fail to extract the relevant
features [20]. Some developers start with a relatively large
window size for spatial ﬁltering and keep it the same for all
convolutional layers which is a bad practice leading to loss
of feature information. In fact, some developers only rely on
the internal mechanism of convolutional and pooling layers
for extracting relevant information without proper parameter
settings/tuning.
Consequences: If the model does not start with a relatively
small window size (for gathering low-level information) and
then grow the window size gradually (to extract high-level
features), it will fail to extract useful features for the next
processings. It makes sense that by using CNNs, the locality
of information is crucial for performing the task. Thus, it is
important to preserve locality throughout CNN to guarantee
its success in detecting various features and relations between
them [20]. Furthermore, early convolutional
layers learn
lower level features while deeper ones learn more high-level
and domain speciﬁc concepts.
Recommended refactoring: The local window size for spatial
ﬁltering should generally increase or stay the same throughout
the convolutional layers. It is recommended to start with small
spatial ﬁltering to collect much local information and then
gradually increase it to represent more compound information
[21], [22].
Example: Fig. 4 shows a part of the code from SO post
#38584268 that deﬁnes a CNN with two convolutional layers
The developer increased the kernel size (local window size)
in successive convolution layers while should increase or at
least keep it the same. The affected layers and corresponding
API’s arguments are marked in red in the code snippet.

3. Heterogeneous blocks of CNNs
Bad smell description: Building a deeper model by only

Fig. 5. A part of DL program from GitHub as an example of design smell
No. 5.

Fig. 6. A part of DL program mentioned in SO 60566498 as an example of
design smell No. 6.

+ pooling) to pinpoint a high amount of pooling.

5. Non-dominating down-sampling
Bad smell description: In fact, down-sampling [28] in the
cascade of CNNs can be done by max- or average-pooling or
strided convolution (strides greater than 1). Using average-
pooling is recognized as a bad design choice for CNN models
[29], particularly for image-like data.
Consequences: Average-pooling ignores some invariances in
data. Since extracting invariant features (those are not affected
by scaling or various transformations) is crucial for image
processing and object recognition, failure to deliver such
features to the dense layers leads to an accuracy degradation
of classiﬁcation. Moreover, it can affect the generalization
capability of the model.
Recommended refactoring: Max-pooling is the preferred
the down-sampling is
so all
down-sampling strategy,
recommended to be changed to max-pooling. Max-pooling
for
operation has been shown to be extremely superior
capturing invariances
information,
compared to other down-sampling operations [29].
Example: Fig. 5 illustrates a part of code from a GitHub
repository1 as an example of this bad smell. It is highlighted
in the code snippet
that developer used average-pooling
instead of recommended max-pooling.

in data with spatial

C. Using regularization

Context: Order and combination of regularization can
affect the performance of FNN signiﬁcantly [8], [10], [30].
the regularization functionality may interfere
Moreover,
with other FNN’s components. Therefore,
regularization
should be used properly (place, order and combination) to
ensure their effectiveness. The following smells discuss bad
practices on the usage of regularizations in a FNN architecture.

6. Useless Dropout
Bad smell description: It is well-known among DL developers
that dropout helps to avoid overﬁtting, however, using it

1https://github.com/yumatsuoka/comp DNNfw/commit/

30e0973892bc344aa17cd36a63dc61a062ad93e4

the maximum pooling layer

before down-sampling layers will counteract its effect [10].
Consequences: Dropping out the activation before the pooling
could have no effect except in cases where the masked units
correspond to maximums within input pooling windows. The
reason is that the max-pooling keeps only these maximums
as inputs for next layers. With the neutralized dropouts, the
model will suffer from overﬁtting and poor performance.
Recommended refactoring: Dropout layer must be placed
after
to be more effective.
Considering the case studies with max-pooling layers [7], the
dropout has been applied on the pooled feature maps, which
becomes a heuristic followed by the state-of-the-art CNN
architectures [10], [11].
Example: In the example shown in Fig. 6, extracted from SO
post #60566498, the developer has used “Dropout” before
“MaxPooling2D” (both underlined by red in the code). The
developer complained about increasing validation loss and
bad performance of his model in the post.

7. Bias with Batchnorm
Bad smell description: Normally learning layers in FNN
beneﬁts from bias with different initializations. When using
batchnorm, keeping bias values in layers is not a good
practice [8].
Consequences: Actually,
the effect of batchnorm will be
diminished in the presence of a bias. Batchnorm applies, after
the normalization, a linear transformation to scale and shift
the normalized activations ˆa = αa + β, where α and β are
learnable parameters. This allows DNN to compensate for
any loss of information by the value distortions in order to
preserve its expressive power. Since, batchnorm already adds
a β term fulﬁlling the same role of bias, “its effect will be
canceled” [8] in the presence of a bias.
Recommended refactoring: The bias should be removed or
ignored in a learning layer that is equipped with a batchnorm.
Example: The code snippet in Fig. 7, extracted from SO post
#49117607, shows that the developer has used two learning
layers (“Conv2D”) without turning off the bias along with
Batchnorm (both underlined by red in the code with 1 and 2

Fig. 8. A part of DL program mentioned in SO 55776436 as an example of
design smell No. 8.

A. Survey Design

Our survey was created using Google Forms [31], a well-
known online tool for creating and sharing online surveys and
quizzes. The survey is organized in three parts. In the ﬁrst part,
we ask some demographic questions about the participant:
i) their role in the organization or job title (e.g., developer,
researcher, student), ii) their number of years of work/research
experience in ML/DL and iii) their used programming lan-
guages/frameworks. The second part asks speciﬁc questions
about the design smells. We provide a description for each
of our 8 design smells and a multiple-choice question asking
the participant about the perceived relevance of the smell.
The participant is instructed to provide a score on a 5-level
Likert scale [32]. Moreover, for each question, we provide
an open comment box to the participants, asking for their
the deﬁnition of the design smell. In the
feedback about
ﬁnal part, we ask (i) if the participant has observed any
other frequent/signiﬁcant design issues that have not been
considered in our survey. (ii) We also ask them if a tool
for detecting such smells would be useful or not, and (iii)
whether they would opt for using such tool. We ask this last
question because one could ﬁnd a tool useful, but more for
others (like junior developers/researches) than for themselves.
At the end of the survey, we provided an open comment box
allowing participants to share any additional comments (that
they wished) with us.

The target group of candidates for this survey is developers,
practitioners, or researchers with a good experience in DL
and particularly in FNNs. The ﬁrst group of candidates was
derived from authors’ personal contacts, actually 16 experts.
The second group of candidates came from GitHub. To ﬁnd
participants with a good understanding of FNNs over GitHub,
we used its REST APIs [33]. First, we identiﬁed the rele-
vant repositories that include “feedforward neural networks”
and “convolutional neural networks” in their description. We
excluded repositories that were not active since 2019. Fi-
nally, we extracted active contributors’ emails from 12192
selected repositories. This process left us with 3650 unique
email addresses and we successfully distributed the survey
participation request to 3605 email addresses. The third group
of candidates came from Reddit. To recruit participants, the
questionnaire was posted on two relevant Reddit channels:
deeplearning and MachineLearning. When sending/posting
the questionnaire, we explained the purpose, scope and the

Fig. 7. A part of DL program mentioned in SO 49117607 as an example of
design smell No. 7.

respectively).

8. Non-representative Statistics Estimation
Bad smell description: Another bad practice regarding
regularizations
is using batchnorm after dropout. The
developers usually use different regularization techniques to
they
maintain and improve performance of DL, however,
should be careful about the internal mechanism and effects
of these two different regularization techniques [30].
Consequences: If the batchnorm is placed after the dropout, it
will compute non-representative global statistics (i.e., moving
average and moving variance) on the dropped outputs of the
layer. Li et al. [30] discussed the effects of this disharmony
between dropout and batchnorm and showed experimental
results asserting their explanation.
Recommended refactoring: Batchnorm should be applied
before dropout. Therefore, a substitution in the model design
is recommended if batchnorm is applied after dropout
to
address the issue.
Example: Fig. 7 illustrates a part of program presented in
SO post #55776436, showing that “Dropout” has been used
before the “BatchNormalization” (a red box indicates affected
lines and they are highlighted both with 1 and 2 respectively).
The developer in his post complained about low classiﬁcation
accuracy.

IV. RELEVANCE ASSESSMENT OF DESIGN SMELLS

After identifying bad design smells in DL models, we
wanted to assess them. Our goal was to know whether de-
velopers/researchers evaluate them as relevant and possibly
worthwhile to be addressed. Hence, we run a survey to validate
our catalogue of DL design smells and collect views of DL
developers/researchers about
the
methodology followed to conduct the survey is explained, then
the results are presented.

it. In the following, ﬁrst

estimated participation duration (5-10 minutes) of the survey
in a quick message. Moreover, we asserted that the survey is
kept anonymous, but the respondents were able to provide their
emails for further communication and receiving a summary of
the study.

B. Validation results

The survey was open for three weeks resulting in 81
responses in total. Regarding our question on work/research
experience in DL, 20 respondents had less than 1 year expe-
rience, 41 between 1 and 3 years, 10 between 3 and 5 years,
and 10 had more than 5 years. Almost all of the respondents
(80 of 81) were using Python for DL development and only
one indicated C++ as his favorite programming language.
Among DL frameworks, TensorFlow was the most popular one
with 59 votes. Keras and PyTorch received 45 and 42 votes
respectively. Fig. 9 shows the results of relevance assessment
for 8 identiﬁed smells in the form of diverging stacked
bar charts. Dark/light green color indicates the proportion
of “Strongly agree” and “Agree” responses, while dark/light
brown indicates the proportion of “Strongly disagree” and
“Disagree” responses. Non-representative Statistics Estimation
is the most popular smell in our survey as it received 68% of
positive votes (“Strongly agree” and “Agree”) while Bias With
Batchnorm received the minimum positive rate of 47%. On the
other hand, the highest negative feedback (“Strongly disagree”
and “Disagree”) was recorded for Losing local correlation
with 27%. In the following, we discuss the validation results
and received comments for each smell.

1. Non-expanding feature map: In general, respondents
agree (about 63% of positive responses: “Strongly agree”
and “Agree”) that keeping the number of features the same
(or even decrease it) as the architecture gets deeper is a
design mistake in DL models, e.g., one commented that: “I
strongly agree with this statement. The number of channels
must be increased so as to capture more complex features
which appear as the layers grow deeper”. However, there are
some neutral and negative responses. Some of them asserted
that this is the case only for classiﬁcation tasks. Most of the
negative/neutral comments explained that this design smell is
not always true and the expansion of the feature map depends
on data, application (task that DL model designed for) or
network architecture. They used to consider the size of the
feature map as a hyperparameter that should be tuned on the
validation loss, e.g., “According to me the size of feature
map is a hyperparameter and will depend on the size of
the network (Depth) hence I neither agree or disagree with
the given statement, since sometimes a combination of small
and larger feature maps work well like in inception model.”.
Another respondent mentioned that s/he preferred to see an
only slightly decreasing number of information processing
units as the model gets deeper, and if the number of points
is quartered (e.g., by max-pooling), the number of feature
channels should be doubled or tripled.

2. Losing local correlation: This smell receives a low
positive response rate of 49%, the highest negative feedback

among all smells (27%: “Strongly disagree” and “Disagree”)
from respondents and 24% of neutral responses. While respon-
dents agree that the window size is an important factor and
should be adjusted as the network gets deeper (e.g., “I agree
with this statement however increasing the window size will
slow the training but our aim for a better model is achieved”),
they believe that non-growing window size across the network
is not always a bad practice (e.g., “I think the windows
size for spatial ﬁltering should be directly proportional to
how deep the network’s layers are”). They mentioned that
there are plenty of simple applications where ﬁxing a window
size is enough to achieve a reasonable performance and this
approach makes implementation easier and hyperparameter
tuning simpler (e.g., “The models I’ve worked with are all
relatively small but I kept the window size the same, it worked
ﬁne”). There are comments stating that
if we start by a
small dimension and grow it, we may have false correlation
as a result of the larger subsequent layers in some cases.
Another respondent rephrased our statement as “start with
and keep (or slightly grow) a small window size”. Three other
comments mentioned autoencoder networks (since they beneﬁt
from CNNs) by stating that this characteristic is observed on
the second half (decoder) of autoencoders but not in the ﬁrst
half, so this design smell can be true or false depending on
context. From neutral responses, we have: “I have seen a case
where ﬁrst a large spatial ﬁlter after that constant ﬁlter size
provided more performance than gradually increasing ﬁlter
size in a larger CNN model. Though I have also seen the
logic above working well”.

3. Heterogeneous blocks of CNNs: Respondents have an
agreement (64%) with soundness and prevalence of this smell.
Also, it received the minimum negative response of 10% in
our survey. They believed that we need multiple symmetric
blocks of CNNs for effective feature extraction particularly in
large models with enough depth not in small or medium ones.
It was acknowledged that multiple layers are needed, not only
to map complex relationships but also to be able to generate
a sufﬁciently large receptive ﬁeld: “a higher representation
level is obtained with every additional convolutional layer”.
However, we received opposite views mentioning different
aspects. Some experts commented that the designer should not
spend too much effort on interpreting the activity of a single
block and not try to set a goal for each block a priori, for
example: “I agree with your claim except the last sentence”.
Others stated that convolutional blocks may be made of a
single, several homogeneous or heterogeneous ones, and the
design choice depends on the application: “the network size
is determined primarily by the dataset size”.

4. Too much down-sampling: More than half of re-
spondents vote positively for this case (56%), and the same
proportion vote neutrally and negatively (22%). We observed
an agreement on the necessity of a balance between down-
sampling vs. feature detection and not using too much down-
sampling (“Too much down sampling can provide rigged
results” or “You do want to avoid downsampling too much,
mostly because you’re going to bottleneck all your information

Fig. 9. Validation results: Perceived relevance of the 8 design smells

to nothing”). However, controversial opinions are on accepting
it as a rule and on the suggested 1/3 threshold. Some comments
mentioned that there is no ﬁx ratio and the optimum ratio
that ﬁts perfectly could be achieved by hyperparameter tuning,
for example: “but I’ve seen optimal architectures in which
that ratio is much higher (e.g.: 1:1) as well as much lower
(e.g.: 1:10)” or “I think it would be difﬁcult to prove such
rules apply to every CNN and every problem domain. Also,
I have seen and used CNNs with no down-sampling layers”.
Another respondent mentioned that hesitancy to down-sample
may increase CNN processing time while mostly preserving
“junk” data in the network so the designer should be careful
about it.

5. Non-dominating down-sampling: Similar to the previ-
ous smell, there is a marginal agreement on this one by 56% of
positive responses. Moreover, this case received a substantial
rate of negative reactions, i.e., 26%. According to the sub-
mitted comments, respondents acknowledged max-pooling as
a dominant choice in most cases supported by results-driven
(e.g., natural image data) and neuroscience-driven arguments.
However, this is not the case always: “max pooling proves
better than avg pooling but it cannot be completely ruled
out”, “Indiscriminate use of average pooling may suggest a
code smell” or “the decision I would say should be based
on what features are being extracted and what is the model
trying to learn”. They mentioned that for some applications
like extraction of a global parameter from an image, average-
pooling can be more useful. Another respondent suggested
using average-pooling instead of max-pooling in Generative
Adversarial Networks (GAN) to avoid sparse loss. Finally, we
found this comment very helpful: “Although contrast is a good
way to see things, nuance is important. Nuance is lost with
max-pooling especially with aggressive down-sampling or at
later layers”.

6. Useless Dropout: According to received responses,
56% of respondents indicate their agreement with this smell.
Although there were some strong positive comments like: “I
generally don’t include dropout before pooling” or “ it’s a

rough heuristic to keep dropouts after pooling but it works
well”, negative responses expressed two main points against
the statement of the smell: 1) type of dropout: element-wise
vs. feature-wise, and 2) its effectinevess compared to batch-
norm. Three respondents proposed that feature-wise dropout
(dropping some proportion of feature maps rather than pixels
or spatial dropout) should be more effective than random
dropout for most applications by considering that “it does not
matter at all whether it’s used before or after pooling (since
entire feature maps are dropped)”. Two others suggested that
dropout was being deprecated by batchnorm.

7. Bias With Batchnorm: Less than half of respondents
went positively with this smell (47%) while it received the
most neutral votes in our survey by 33%. Responders with
positive votes stated that using bias with batchnorm is a bad
practice and they avoid it generally. By reviewing comments,
we come to the conclusion that negative and neutral voters
believed that using bias with batchnorm is not harmful: “The
conv bias is redundant with the BN bias, but I don’t think
it’s harmful to keep it (just wasteful)”, “I cannot see the
presence of bias nodes being a problem” or “the additional
bias will simply ”cancel” and the same representation is
learned anyway”. Therefore, the design smell does not look
wrong and avoiding it can be helpful at least for keeping the
model simpler.

8. Non-representative Statistics Estimation: There is a
general agreement in this case since we received 68% of
positive votes as the most popular smell in our survey. A
majority of respondents believed that using batchnorm after
dropout would lead to non-representative statistics: “if batch
normalisation is done after dropout then it will normalise the
output coming after dropping the some connection (nodes)”.
However, there were also some negative comments on the
smell. The main criticism was that the order of batchnorm and
dropout does not have a signiﬁcant impact on the performance
of a DL model.

The results of our questions about

the usefulness of a
potential tool for detecting the identiﬁed smells are shown in
Fig. 10. A signiﬁcant majority of respondents, actually 90%,

Design smells Votes15%15%6%14%19%14%9%10%6%12%4%9%7%7%11%5%45%32%42%36%34%31%32%33%17%17%22%17%22%25%16%35%Non-expanding feature mapLosing local correlationHeterogeneous blocks of CNNsToo much down-samplingNon-dominating down-samplingUseless DropoutBias With BatchnormNon-representative Statistics EstimationStrongly disagreeDisagreeBorderlineAgreeStrongly agreeFig. 10. Survey results about a detection tool

expressed a positive opinion for such a detection tool. Our
follow-up question regarding whether they would use this tool
if it became available, received another high positive reaction
rate of 86%. We attribute the slight drop to some experienced
respondents recognizing that a detection tool would be useful
but not necessary to them. Finally, all respondents surprisingly
answered our question about other frequent/signiﬁcant smells
not considered in this survey and further identiﬁcation of
smells. They suggested the investigation of potential design
smells related to various components of DL programs, in-
cluding: (i) Initialization methods, (ii) Other architectures like
fully and autoencoder CNNs, (iii) Some hyperparameter: like
learning rate for different layers, (iv) The choice and location
of activation functions, (v) Attention layers, (vi) Transfer
learning.

C. Discussion

Among the comments received in our survey, some respon-
dents mentioned that although the proposed design smells
have stated promising points for sketching DL models, hy-
perparameter tuning is inevitable after any initial design and
the model’s performance can be improved signiﬁcantly by
a proper hyperparameter search, for example: “... just set
up your hypermodel to accept these as tunable parameters
and search the space” or “... allowing users to perform a
ﬂexible hyperparameter to ﬁt the model to their particular
needs”. They stated that given the range of applications for
DL, many design/conﬁguration choices are domain-, data- and
preprocessing-dependent. Therefore, experiments (including
for hyperparameter tuning) may be required in some cases
to identify the issues. However, we believe that having a
catalogue of known bad practices while designing DL models,
will help developers to avoid smells in their models. Even
if the proposed smells do not cover all domains, they are
still useful for the covered architecture/domains. Moreover,
avoiding those smells will save time, effort and computational
resources during test or hyperparameter tuning.

V. THREATS TO VALIDITY

First of all, threats to construct validity may affect the
relevance of the identiﬁed design smells which is assessed
by a survey. In our survey, respondents were requested to
indicate the perceived signiﬁcance of smells described by
a short explanation of the problem/situation. We have used
relevant terminology and provided technical details in our
descriptions to address this threat. Moreover, respondents
were able to mention comments for each smell in the survey

and we have not observed any comment complaining about
possible misunderstanding in the description or context. It
is also possible that our descriptions in the survey affected
participant’s view directing them toward our proposed design
smells. To address this concern, we asked participants at the
end of our survey to freely comment on missing issues in our
study.

There are internal threats to the validity of this research
that may affect its achievements. The identiﬁcation of design
smells could be biased during reviewing previous works and
manual inspection of artifacts. To address this issue, a clear
systematic approach is followed in our study. We have inves-
tigated only “closed” issues from GitHub and questions with
“at least one accepted” answer from SO; ensuring that we
analyzed only issues that were solved. Moreover, participants
in the survey have not been involved in the process of identi-
fying smells and have different levels of expertise/background.
Although the catalogue was prepared using DL programs
developed by two popular frameworks of TensorFlow and
the title and description of the smells as
Keras, we kept
general as possible and we believe that they are helpful for
developers/researchers working with other frameworks as well.
External validity threats may impact the generalization of
our ﬁndings. We indeed are aware that the proposed catalogue
is not complete. Since our paper is a ﬁrst step in identifying
design smells in DL programs, further studies are required
to comprehensively investigate design smells in DL programs
utilizing various structures. Furthermore, some smells can be
extended in future work since currently they are speciﬁed for
particular cases.

VI. CONCLUSION

In this paper, we have speciﬁed 8 design smells in DL
programs. Due to the prevalence and effectiveness of deep
CNNs in real-world applications (particularly with image-
like data), we have focused on this architecture. Basically,
these smells are structural inefﬁciencies in DL models, that
affect the performance of DL programs. We evaluated the
validity and relevance of this catalogue by running a survey
with 81 DL developers/researchers. In general, the developers
perceived the proposed design smells as reﬂective of design
or implementation problems, with agreement levels varying
between 47% and 68%. The analysis of the multiple comments
indicates that almost all
received for each of the smells,
the design smells are found to be relevant and helpful by
respondents. Many of the survey respondents encountered
similar design issues described by the smells.

There are several directions for future work. First, we plan
to introduce a detection tool for the proposed smells. An
automatic method for ﬁnding design smells in DL programs
will help developers to improve their DL models prior to
deployment. Second, we plan to generalize some of the already
identiﬁed smells to cover other contexts. Finally, a more
comprehensive variety of smells can be proposed by covering
other DL architectures.

19%14%9%10%7%7%11%5%34%30%32%34%22%26%15%34%Non-dominating down-samplingUseless DropoutBias With BatchnormNon-representative Statistics EstimationStrongly disagreeWeakly disagreeStrongly disagreeBorderlineWeakly agreeStrongly agree90%86%10%14%Can a detection tool fordesign smells be helpful?Do you use such a tool?YesNoREFERENCES

[1] J. Heaton, “Applications of deep neural networks,” arXiv preprint

arXiv:2009.05673, 2020.

[2] Y. Zhang, Y. Chen, S.-C. Cheung, Y. Xiong, and L. Zhang, “An empirical
study on tensorﬂow program bugs,” in Proceedings of the 27th ACM
SIGSOFT International Symposium on Software Testing and Analysis,
pp. 129–140, 2018.

[3] M. J. Islam, G. Nguyen, R. Pan, and H. Rajan, “A comprehensive study
on deep learning bug characteristics,” in Proceedings of the 2019 27th
ACM Joint Meeting on European Software Engineering Conference and
Symposium on the Foundations of Software Engineering, pp. 510–520,
2019.

[4] L. N. Smith and N. Topin, “Deep convolutional neural network design

patterns,” arXiv preprint arXiv:1611.00847, 2016.

[5] S. H. Hasanpour, M. Rouhani, M. Fayyaz, M. Sabokrou, and E. Adeli,
“Towards principled design of deep convolutional networks: introducing
simpnet,” arXiv preprint arXiv:1802.06205, 2018.

[6] I. Goodfellow, Y. Bengio, and A. Courville, Deep Learning. MIT Press,

2016.

[7] N. Srivastava, G. Hinton, A. Krizhevsky, I. Sutskever, and R. Salakhut-
dinov, “Dropout: a simple way to prevent neural networks from overﬁt-
ting,” The journal of machine learning research, vol. 15, no. 1, pp. 1929–
1958, 2014.

[8] S. Ioffe and C. Szegedy, “Batch normalization: Accelerating deep
network training by reducing internal covariate shift,” in International
conference on machine learning, pp. 448–456, PMLR, 2015.

[9] A. Krizhevsky, I. Sutskever, and G. E. Hinton, “Imagenet classiﬁcation
with deep convolutional neural networks,” in Advances in neural infor-
mation processing systems, pp. 1097–1105, 2012.

[10] D. Mishkin, N. Sergievskiy, and J. Matas, “Systematic evaluation of
convolution neural network advances on the imagenet,” Computer Vision
and Image Understanding, vol. 161, pp. 11–19, 2017.

[11] A. Canziani, A. Paszke, and E. Culurciello, “An analysis of deep
neural network models for practical applications,” arXiv preprint
arXiv:1605.07678, 2016.

[12] W. J. Brown, R. C. Malveau, H. W. McCormick III, and T. J. Mowbray,
Refactoring software, architectures, and projects in crisis. John Wiley
and Sons, Inc, 1998.

[13] J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-Fei, “Imagenet:
A large-scale hierarchical image database,” in 2009 IEEE conference on
computer vision and pattern recognition, pp. 248–255, Ieee, 2009.
[14] T.-Y. Lin, M. Maire, S. Belongie, J. Hays, P. Perona, D. Ramanan,
P. Doll´ar, and C. L. Zitnick, “Microsoft coco: Common objects in
context,” in European conference on computer vision, pp. 740–755,
Springer, 2014.

[15] T. Zhang, C. Gao, L. Ma, M. R. Lyu, and M. Kim, “An empirical study
of common challenges in developing deep learning applications,” in The
30th IEEE International Symposium on Software Reliability Engineering
(ISSRE), 2019.

[16] N. Humbatova, G. Jahangirova, G. Bavota, V. Riccio, A. Stocco,
and P. Tonella, “Taxonomy of real faults in deep learning systems,”
in Proceedings of the ACM/IEEE 42nd International Conference on
Software Engineering, pp. 1110–1121, 2020.

[17] M. J. Islam, R. Pan, G. Nguyen, and H. Rajan, “Repairing deep neural
networks: Fix patterns and challenges,” in Proceedings of the ACM/IEEE
42nd International Conference on Software Engineering, ICSE ’20,
p. 1135–1146, 2020.

[18] C. B. Seaman, “Qualitative methods in empirical studies of software
engineering,” IEEE Transactions on software engineering, vol. 25, no. 4,
pp. 557–572, 1999.

[19] Y. LeCun, L. Bottou, Y. Bengio, and P. Haffner, “Gradient-based learning
applied to document recognition,” Proceedings of the IEEE, vol. 86,
no. 11, pp. 2278–2324, 1998.

[20] Y. LeCun, Y. Bengio, and G. Hinton, “Deep learning,” nature, vol. 521,

no. 7553, pp. 436–444, 2015.

[21] K. Simonyan and A. Zisserman, “Very deep convolutional networks for
large-scale image recognition,” arXiv preprint arXiv:1409.1556, 2014.
[22] C. Szegedy, V. Vanhoucke, S. Ioffe, J. Shlens, and Z. Wojna, “Rethinking
the inception architecture for computer vision,” in Proceedings of the
IEEE conference on computer vision and pattern recognition, pp. 2818–
2826, 2016.

[23] K. He, X. Zhang, S. Ren, and J. Sun, “Deep residual learning for image
recognition,” in Proceedings of the IEEE conference on computer vision
and pattern recognition, pp. 770–778, 2016.

[24] “Documentation of NVIDIA deep learning cuDNN,” 2020. https://docs.

nvidia.com/deeplearning/cudnn/.

[25] F. Iandola, M. Moskewicz, S. Karayev, R. Girshick, T. Darrell, and
K. Keutzer, “Densenet: Implementing efﬁcient convnet descriptor pyra-
mids,” arXiv preprint arXiv:1404.1869, 2014.

[26] K. He and J. Sun, “Convolutional neural networks at constrained time
cost,” in Proceedings of the IEEE conference on computer vision and
pattern recognition, pp. 5353–5360, 2015.

[27] F. N. Iandola, S. Han, M. W. Moskewicz, K. Ashraf, W. J. Dally,
and K. Keutzer, “Squeezenet: Alexnet-level accuracy with 50x fewer
parameters and < 0.5 mb model size,” arXiv preprint arXiv:1602.07360,
2016.

[28] J. T. Springenberg, A. Dosovitskiy, T. Brox, and M. Riedmiller,
“Striving for simplicity: The all convolutional net,” arXiv preprint
arXiv:1412.6806, 2014.

[29] D. Scherer, A. M¨uller, and S. Behnke, “Evaluation of pooling operations
in convolutional architectures for object recognition,” in International
conference on artiﬁcial neural networks, pp. 92–101, Springer, 2010.

[30] X. Li, S. Chen, X. Hu, and J. Yang, “Understanding the disharmony
between dropout and batch normalization by variance shift,” in Proceed-
ings of the IEEE conference on computer vision and pattern recognition,
pp. 2682–2690, 2019.

[31] “Google forms.” https://www.google.ca/forms/about/, 2021.
[32] A. N. Oppenheim, Questionnaire design,

interviewing and attitude

measurement. Bloomsbury Publishing, 2000.

[33] “Github REST API,” 2021. https://docs.github.com/en/rest.

