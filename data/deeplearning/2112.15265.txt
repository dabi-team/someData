2
2
0
2

r
p
A
0
2

]
L
M

.
t
a
t
s
[

2
v
5
6
2
5
1
.
2
1
1
2
:
v
i
X
r
a

Entropy Regularized Optimal Transport Independence Criterion

Lang Liu1

Soumik Pal2

Zaid Harchaoui1

1 Department of Statistics, University of Washington
2 Department of Mathematics, University of Washington

Abstract

We introduce an independence criterion based on entropy regularized optimal transport. Our criterion
can be used to test for independence between two samples. We establish non-asymptotic bounds for
our test statistic and study its statistical behavior under both the null hypothesis and the alternative
hypothesis. The theoretical results involve tools from U-process theory and optimal transport theory.
We also oﬀer a random feature type approximation for large-scale problems, as well as a diﬀerentiable
program implementation for deep learning applications. We present experimental results on existing
benchmarks for independence testing, illustrating the interest of the proposed criterion to capture both
linear and nonlinear dependencies in synthetic data and real data.

1

Introduction

Statistical independence measures have been widely used in machine learning and statistics, ranging from
independent component analysis (Bach and Jordan, 2002; Gretton et al., 2005) to causal inference (Pﬁster
et al., 2018; Chakraborty and Zhang, 2019), and recently in self-supervised learning (Li et al., 2021) and
representation learning (Ozair et al., 2019). Classical dependence measures such as Pearson’s correlation co-
eﬃcient, Spearman’s ρ, and Kendall’s τ (Hoeﬀding, 1948; Kruskal, 1958; Lehmann, 1966) focus on real-valued
one dimensional random variables and thus are not suitable for high dimensional data; see also (Schweizer
and Wolﬀ, 1981; Nikitin, 1995).

One popular choice of independence measures in high dimension is the Hilbert-Schmidt independence
criterion (HSIC) (Gretton et al., 2005). This criterion was used to test independence by Gretton et al.
(2007b). Several extensions of HSIC are available, such as a relative dependency measure (Bounliphone
et al., 2015) and a joint independence measure among multiple random elements (Pﬁster et al., 2018).
Another choice is the distance covariance (dCov) of Székely et al. (2007). dCov was originally developed in
Euclidean spaces using characteristic functions and later generalized to metric spaces (Lyons, 2013). In fact,
in their most general form, HSIC and dCov are equivalent as shown by Sejdinovic et al. (2013).

A diﬀerent line of research explored optimal transport to measure dependence. The Wasserstein distance
naturally deﬁnes a dependence measure when it is used to quantify the dissimilarity between the joint
distribution and the product of marginals; see, e.g., (Cifarelli and Regazzini, 2017). The normalized version—
the so-called Wasserstein correlation coeﬃcient—has recently gained attention in (Wiesel, 2021; Mordant and
Segers, 2021; Nies et al., 2021). Following the classical rank-based tests such as Pearson’s ρ, optimal transport
is also used to deﬁne multivariate ranks and the subsequent independence tests (Shi et al., 2020; Deb and Sen,
2021). However, these tests can suﬀer from the curse of dimensionality or high computational complexity,
limiting their practical usefulness; see (Peyré and Cuturi, 2019) for a discussion.

A remedy to this challenge is to use the entropy regularized formulation of optimal transport. This is par-
ticularly attractive from both a computational viewpoint (Cuturi, 2013) and a statistical viewpoint (Rigollet
and Weed, 2018). Moreover the empirical counterpart of entropy regularized optimal transport enjoys as
an estimator a parametric rate of convergence and thus appears to overcome the curse of dimensional-
ity (Genevay et al., 2019; Mena and Weed, 2019). The Sinkhorn divergence (Feydy et al., 2019), its centered
version, deﬁnes a semi-metric on probability measures which metrizes weak convergence. Ramdas et al.
(2017) used it for two-sample testing and Genevay et al. (2018) for generative modeling; see also (Salimans

1

 
 
 
 
 
 
et al., 2018; Sanjabi et al., 2018). Our approach to independence testing shall build upon this recent progress
on entropic regularization.

Outline.
In Section 2, we introduce the entropy regularized optimal transport independence criterion
(ETIC) and discuss its key properties. We propose the Tensor Sinkhorn algorithm with a random feature
approximation to compute ETIC, which admits a quadratic scaling in time and space. We also show how
to approximate ETIC using random features, and how to diﬀerentiate through ETIC in a framework of
diﬀerentiable programming. In Section 3, we give our main theoretical results, i.e., non-asymptotic bounds,
characterizing the statistical behavior of the empirical estimator of ETIC under both the null and alternative
hypotheses. In Section 4, we compare the empirical behavior of ETIC with HSIC on both synthetic and real
data. The Appendix contains detailed proofs as well as additional experiments.

Related Work. Statistical metrics on the space of probability measures form the backbone of many
dependence measures. On the machine learning side, distributions are compared by embedding them into
reproducing kernel Hilbert spaces (Gretton et al., 2007a, 2012). The Hilbert-Schmidt independence criterion
(HSIC) uses Hilbertian embeddings of probability distributions to compare the joint distribution and the
product of marginals (Gretton et al., 2005, 2007b). On the statistics side, distributions deﬁned on Euclidean
spaces are compared via their characteristic functions, leading to the so-called energy distance (Székely and
Rizzo, 2004).

A closely related dependence measure is the distance covariance (Székely et al., 2007). These distances
were later generalized to general metric spaces of negative type by Lyons (2013), unifying the two notions via
the Barycenter map—a quantity similar to the feature map in kernel methods. In fact, the kernel-based and
distance-based approaches are equivalent (Sejdinovic et al., 2013). Their corresponding empirical estimators
all admit a U-statistics expression, and enjoy a convergence rate that is independent of the dimension. These
results can be established using tools from U-statistics theory; see, e.g., (Serﬂing, 1980).

On the other hand, Wasserstein distances provide a class of metrics on the space of probability measures
with nice geometric properties (Ambrosio et al., 2005). However, it is known that its empirical estimate
suﬀers from the curse of dimensionality (Dudley, 1969; Fournier and Guillin, 2015; Weed and Bach, 2019;
Lei, 2020), limiting their usage in high-dimension problems. A remedy to this issue is to introduce the entropic
regularization. Genevay et al. (2019) showed that the plug-in estimator of the entropy regularized optimal
transport cost possesses a parametric rate of convergence when the measures are compactly supported. Their
results can be extended to sub-Gaussian distributions (Mena and Weed, 2019) with tools from empirical
process theory.

The independence criterion we propose uses entropy regularized optimal transport to compare the joint
distribution and the product of marginals. The empirical counterpart involves a product of two empirical
measures, leading to a two-sample U-process on paired samples. The resulting U-process requires a sophis-
ticated analysis of its statistical behavior; common tools from empirical processes are ineﬀective here. Using
the decoupling technique from Peña and Giné (1999) and duality theory (Peyré and Cuturi, 2019), we prove
a rate of convergence roughly O(σ3dn−1/2), where n is the sample size, d is the ambient dimension, and σ is
the sub-Gaussian parameter, recovering previous results for two sample statistics.

2 Entropy Regularized Optimal Transport Independence Criterion

We introduce in this section the entropy regularized optimal transport independence criterion (ETIC) and dis-
cuss its key properties. We design an independence test based on ETIC and develop an eﬃcient algorithm—
the Tensor Sinkhorn algorithm—to compute its test statistic. We also provide an approximation using
random features for large-scale problems, as well as a diﬀerentiable program implementation for deep learn-
ing applications.

Notation. For a Euclidean space Z equipped with the Borel σ-algebra, let M1(Z) be the set of probability
measures deﬁned on Z. Let (X, Y ) be a pair of random vectors with respective dimension d1 and d2 following
some joint distribution PXY ∈ M1(Rd) where d := d1 + d2. Denote PX ∈ M1(Rd1 ) and PY ∈ M1(Rd2 )
the marginal distributions of X and Y , respectively. Given Q ∈ M1(Rd1 × Rd2) and a real-valued function

2

Algorithm 1 Tensor Sinkhorn Algorithm
1: Input: A, B, K1, and K2.
2: Initialize U ← 1n×n and V ← 1n×n.
3: while not converge do
U ← A (cid:11) (K1V K (cid:62)
4:
5: end while
6: Output: U and V .

2 ) and V = B (cid:11) (K (cid:62)

1 U K2).

f on the same domain, we denote Q[f ] the expectation E(X,Y )∼Q[f (X, Y )]. We adopt the notation from
the empirical process theory and write (cid:107)Q(cid:107)F := supf ∈F |Q[f ]| for a real-valued function class F. We say
Q is sub-Gaussian with parameter σ2, denoted as subG(σ2), if EQ e(cid:107)(X,Y )(cid:107)2/(2dσ2) ≤ 2; see, e.g., Vershynin
(2018). We write E := EPXY for short.

ETIC. Let c : Rd × Rd → R+ be a continuous cost function satisfying c((x, y), (x(cid:48), y(cid:48))) = 0 iﬀ (x, y) =
(x(cid:48), y(cid:48)). We introduce the entropy regularized optimal transport independence criterion (ETIC):

T (X, Y ) := Tε(X, Y ) := ¯Sε(PXY , PX ⊗ PY ),

(1)

where ¯Sε is a divergence deﬁned as ¯Sε(PXY , PX ⊗ PY ) := Sε(PXY , PX ⊗ PY ) − 1
2 Sε(PX ⊗
PY , PX ⊗ PY ). Here Sε(PXY , PX ⊗ PY ) is the entropy regularized optimal transport cost between PXY and
PX ⊗ PY , i.e.,

2 Sε(PXY , PXY ) − 1

(cid:20)(cid:90)

(cid:21)
cdγ + ε KL(γ(cid:107)PXY ⊗ (PX ⊗ PY ))

,

inf
γ

(2)

where the inﬁmum is over Π(PXY , PX ⊗ PY ) which is the set of couplings (or joint distributions) on Rd×d
with marginals PXY and PX ⊗ PY , ε > 0 is the regularization parameter, and KL is the Kullback-Leibler
divergence. The other two terms are deﬁned similarly and are omitted for the sake of space.

As we will show later,

i.e.,
c((x, y), (x(cid:48), y(cid:48))) = c1(x, x(cid:48)) + c2(y, y(cid:48)). For this type of cost functions, we prove that the resulting ETIC is
a valid independence criterion as long as the induced Gibbs kernels

it is computationally convenient to work with additive cost functions,

k1(x, x(cid:48)) = exp {−c1(x, x(cid:48))/ε}

and k2(y, y(cid:48)) = exp {−c2(y, y(cid:48))/ε}

(3)

are positive universal; see Appendix A for the proof.

Proposition 1. Let X ⊂ Rd1 and Y ⊂ Rd2 be compact subsets equipped with Lipschitz costs c1 and c2,
respectively. Assume that the Gibbs kernels deﬁned in (3) are positive universal. Then, the ETIC is a valid
dependency measure on M1(X × Y), i.e., T (X, Y ) ≥ 0 and

T (X, Y ) = 0 iﬀ PXY = PX ⊗ PY .

(4)

Moreover, the claim holds true for measures with a bounded support on X × Y = Rd with the costs c1(x, x(cid:48)) =
(cid:107)x − x(cid:48)(cid:107)p /λ1 and c2(y, y(cid:48)) = (cid:107)y − y(cid:48)(cid:107)p /λ2 for p ∈ {1, 2} and for all λ1, λ2 > 0.

A running example we consider in this paper is the weighted quadratic cost.

Example 1 (Weighted quadratic cost). Let λ1, λ2 ∈ (0, ∞). Consider the cost function

c((x, y), (x(cid:48), y(cid:48))) =

1
λ1

(cid:107)x − x(cid:48)(cid:107)2 +

1
λ2

(cid:107)y − y(cid:48)(cid:107)2 .

(5)

This cost induces two universal kernels k1(x, x(cid:48)) = e−(cid:107)x−x(cid:48)(cid:107)2/(ελ1) and k2(y, y(cid:48)) = e−(cid:107)y−y(cid:48)(cid:107)2/(ελ2). They play
a similar role as the two kernels used in HSIC, and ελ1 and ελ2 serve as two kernel parameters.

3

Table 1: Comparison of complexities, in time and in space, of Sinkhorn and Tensor Sinkhorn algorithms,
Exact or Random Features approx.

Sinkhorn

Exact

RF

Tensor Sinkhorn
Exact

RF

Time
Space

O(n4) O(n2)
O(n4)

O(n3)
O(n4) O(n2)

O(n2)
O(n2)

ETIC-Based Independence Test. Given an i.i.d. sample {(Xi, Yi)}n
i=1 from PXY , we are interested in
determining whether X is independent of Y , which can be formalized as the following hypothesis testing
problem:

H0 : PXY = PX ⊗ PY ↔ H1 : PXY (cid:54)= PX ⊗ PY .

For this purpose, we use the empirical estimate of T (X, Y ) as the test statistic, that is,

Tn(X, Y ) := Tn,ε(X, Y ) := ¯Sε( ˆPXY , ˆPX ⊗ ˆPY ),

(6)

(7)

(cid:80)n

(cid:80)n

i=1 δ(Xi,Yi) is the empirical measure of the pairs, and ˆPX := 1

i=1 δXi and ˆPY :=
where ˆPXY := 1
n
1
i=1 δYi are the empirical measures of the two samples, respectively. Note that this is diﬀerent from the
n
standard plug-in estimator since the product measure PX ⊗ PY is estimated by n2 dependent (rather than
independent) pairs {(Xi, Yj)}n
i,j=1. It raises challenges in the analysis of its statistical behavior as elaborated
in Section 3. The statistical test (or decision rule) is then deﬁned as

n

(cid:80)n

ψ(α) := 1{Tn(X, Y ) > Hn(α)},

(8)

where α is a prescribed signiﬁcance level, e.g., α = 0.05, and Hn(α) is a threshold chosen such that the type
I error rate P(ψ(α) = 1 | H0) is bounded by α. Here {ψ(α) = 1} indicates the rejection the null hypothesis.
The (statistical) power of the test is deﬁned as P(ψ(α) = 1 | H1).

To avoid tuning the regularization parameter ε, we also consider an adaptive version of the test:

ψa(α) := 1

(cid:26)

max
ε∈E

¯Tn,ε(X, Y ) > Hn,E (α)

(cid:27)

,

(9)

where E is a ﬁnite set of positive numbers selected by the user and

¯Tn,ε(X, Y ) := [Tn,ε(X, Y ) − E[Tn,ε(X, Y )]]/Sd(Tn,ε(X, Y ))

is the studentized version of Tn,ε(X, Y ). In practice, E[Tn,ε(X, Y )] and Sd(Tn,ε(X, Y )) can be estimated via
resampling.

Tensor Sinkhorn Algorithm. We then derive an eﬃcient algorithm to compute the test statistic. When
PXY admits a density, ˆPX ⊗ ˆPY is supported on n2 items {xi}n
i=1 almost surely. If we compute
the ETIC statistic naively using the Sinkhorn algorithm (Cuturi, 2013), each iteration costs O(n4) time
and space due to the matrix-vector product of sizes n2 × n2 and n2 × 1. To speed up its computation, we
develop a variant of the Sinkhorn algorithm to solve the EOT problem between two measures supported on
the Cartesian product {xi}n

i=1 × {yi}n

Let A and B be two probability measures on {xi}n

i=1, where xi ∈ Rd1 and yj ∈ Rd2. For
convenience, both A and B are represented as a matrix, i.e., Aij = A(xi, yj). For instance, if we choose
A = ˆPXY and B = ˆPX ⊗ ˆPY , then, in its matrix form, A = In/n and B = 1n×n/n2. Consider an
additive cost function c, e.g., the weighted quadratic cost, such that c((x, y), (x(cid:48), y(cid:48))) = c1(x, x(cid:48)) + c2(y, y(cid:48))
for x, x(cid:48) ∈ {xi}n
j=1,
respectively. Deﬁne Gibbs matrices K1 := e−C1/ε and K2 := e−C2/ε, where the exponential function is

j=1. Let C1 and C2 be the cost matrices of {xi}n

i=1 and y, y(cid:48) ∈ {yj}n

i=1 and {yj}n

i=1 × {yi}n

i=1 × {yi}n

i=1.

4

element-wise. We show in Proposition 8 in Appendix A that Algorithm 1 can be used to compute Sε(A, B),
where (cid:11) represents element-wise division. We refer to it as the Tensor Sinkhorn algorithm. Each iteration
in the Tensor Sinkhorn algorithm takes O(n3) time and O(n2) space, thanks to the additive cost function
being used. This algorithm can be generalized to measures supported on the Cartesian product of p > 2
sets, which is also noted in (Peyré and Cuturi, 2019, Remark 4.17).

ETIC with Random Features. To further speed up the computation, we apply the random feature
technique introduced by Scetbon and Cuturi (2020). On a high level, we approximate the gram matrix
for i ∈ {1, 2}, where ξi ∈ Rn×p is the matrix of random features.
Ki by its low-rank approximation ξiξ(cid:62)
i
Concretely, let ρ1 and ρ2 be two probability measures on measurable spaces U and V, respectively. Following
Scetbon and Cuturi (2020, Section 3), we focus on Gibbs kernels of the form

(cid:90)

(cid:90)

k1(x, x(cid:48)) =

k2(y, y(cid:48)) =

ϕ(x, u)(cid:62)ϕ(x(cid:48), u)dρ1(u)

ψ(y, v)(cid:62)ψ(y(cid:48), v)dρ2(v),

i=1 × U → Rq1

where ϕ : {xi}n
weighted quadratic cost admit this expression. For p ∈ N+, we obtain two i.i.d. samples {ui}p
from ρ1 and ρ2, respectively. We denote u := (u1, . . . , up) and approximate k1(x, x(cid:48)) by

+ . Note that the Gibbs kernels induced by the
i=1

+ and ψ : {yi}n

i=1 and {vi}p

i=1 × V → Rq2

k1,u(x, x(cid:48)) :=

1
p

p
(cid:88)

k=1

ϕ(x, uk)(cid:62)ϕ(x(cid:48), uk).

This new kernel induces the cost c1,u(x, x(cid:48)) := −ε log k1,u(x, x(cid:48)). Similarly, we deﬁne v, k2,v(y, y(cid:48)), and
c2,v(y, y(cid:48)). It is clear that Algorithm 1 with inputs A, B, K1,u, and K2,v solves the entropy-regularized
optimal transport problem with cost cu,v((x, y), (x(cid:48), y(cid:48))) := c1,u(x, x(cid:48)) + c2,v(y, y(cid:48)). Let Sε,cu,v (A, B) be
the entropic cost. The next proposition provides a high-probability guarantee for this random feature
approximation.

Assumption 1. There exists a constant C > 0 such that, for all x, x(cid:48) ∈ {xi}n
and v ∈ V,

i=1, y, y(cid:48) ∈ {yj}n

j=1, u ∈ U,

ϕ(x, u)(cid:62)ϕ(x(cid:48), u)/k1(x, x(cid:48)) ≤ C
ψ(y, v)(cid:62)ψ(y(cid:48), v)/k2(y, y(cid:48)) ≤ C.

Proposition 2. Let δ > 0, τ > 0, and p = Ω
1 − δ, it holds that

(cid:16) C2

τ 2 log n

δ

(cid:17)

. Under Assumption 1, with probability at least

(cid:12)
(cid:12)Sε,cu,v (A, B) − Sε,c(A, B)(cid:12)

(cid:12) ≤ τ.

Replacing K1 and K2 by their random feature approximations K1,u and K2,v in Algorithm 1 leads to
an algorithm with O(pn2) time complexity and O(n2) space complexity in each iteration. We note that if
one applies the random feature approximation directly to the original Sinkhorn algorithm, then the resulting
algorithm would have the same time complexity O(pn2) but O(n4) space complexity; see Table 1 for a
comparison.

Dual Representation. The entropy regularized formulation of OT (2) is known as the Schrödinger bridge
problem (Föllmer, 1988; Léonard, 2012, 2014) in continuum and the Sinkhorn distance (Cuturi, 2013; Fer-
radans et al., 2014) in the discrete case. It admits a dual representation (Genevay et al., 2016):

sup
f,g∈C(Rd1 ×Rd2 )

(cid:20) (cid:90)

(cid:90)

f dPXY +

gd(PX ⊗ PY ) + ε − ε

(cid:90)

(cid:21)
e−Dε(x,y,x(cid:48),y(cid:48))dPXY (x, y)dPX (x(cid:48))dPY (y(cid:48))

,

5

where C is the set of real-valued continuous functions and Dε(x, y, x(cid:48), y(cid:48)) = 1
ε [c1(x, x(cid:48)) + c2(y, y(cid:48)) − f (x, y) −
g(x(cid:48), y(cid:48))]. Due to (Csiszar, 1975; Rüschendorf and Thomsen, 1993), the optimal potentials (fε, gε), to be
called the Schrödinger potentials, satisfy

(cid:90)

e−Dε(x,y,x(cid:48),y(cid:48))dPX (x(cid:48))dPY (y(cid:48)) a.s.= 1

(cid:90)

e−Dε(x,y,x(cid:48),y(cid:48))dPXY (x, y) a.s.= 1.

(10)

Gradient Backpropagation through ETIC. We describe here how ETIC can ﬁt into a diﬀerentiable
programming framework, i.e., how one can run the reverse mode automatic diﬀerentiation through statistical
quantities based on ETIC. Recently, Li et al. (2021) proposed a self-supervised learning approach using
HSIC which we summarize below. Let (W, Y ) be a pair of image and its identity. Given an i.i.d. sample
{(Wi, Yi)}n
i=1, the goal is to learn a feature embedding model φθ such that the dependence between the image
feature X := φθ(W ) and its identity Y is maximized, i.e., maxθ∈Θ HSICn(φθ(W ), Y ). Similarly, one could
also maximize the dependence measured by ETIC instead. This boils down to gradient backpropagating
through Tn(φθ(W ), Y ). We use the strategy in (Peyré and Cuturi, 2019, Section 9.1.3) and illustrate it on
the entropy regularized OT Sε( ˆPXY , ˆPX ⊗ ˆPY ) in (2). For the forward pass, we construct the computational
graph via the following steps. Firstly, we run Algorithm 1 (or its random feature variant) with A = In/n,
B = 1n×n/n2, K1 = (cid:0)k1(φθ(Wi), φθ(Wj))(cid:1)
n×n for L iterations to get U (L) and
V (L). Secondly, we obtain the associated Schrödinger potentials F (L) := ε log U (L) and G(L) := ε log V (L).
Thirdly, we approximate Sε( ˆPXY , ˆPX ⊗ ˆPY ) by ˆSε(θ) := (cid:104)F (L), A(cid:105)F +(cid:104)G(L), B(cid:105)F where (cid:104)·, ·(cid:105)F is the Frobenius
inner product. For the backward pass, we call the reverse mode automatic diﬀerentiation to evaluate ∇θ ˆSε(θ).
Since computing ˆSε(θ) only requires simple operations between matrices, the time complexity of the above
procedure is of the same order as the one of Algorithm 1 for the computation of ˆSε(θ).

n×n, and K2 = (cid:0)k2(Yi, Yj)(cid:1)

Connection to Previous Work. As shown in Appendix A, T (X, Y ) tends to the OT-based independence
criterion OT(PXY , PX ⊗ PY ) as ε → 0. If the cost c is chosen as the Euclidean distance to the power p ≥ 1,
it induces a distance (known as the Wasserstein-p distance) on the space of probability measures (Villani,
2016). As a result, OT(PXY , PX ⊗ PY ) is a valid independence criterion, i.e., OT(PXY , PX ⊗ PY ) = 0 iﬀ
PXY = PX ⊗ PY . The study of this independence criterion can be dated back to Gini; see (Cifarelli and
Regazzini, 2017) for a discussion. Its normalized version—the so-called Wasserstein correlation coeﬃcient—
has recently gained attention in (Wiesel, 2021; Mordant and Segers, 2021; Nies et al., 2021). When ε → ∞,
T (X, Y ) tends to 0 if the cost is additive; if the cost is multiplicative, i.e., c((x, y), (x(cid:48), y(cid:48))) = c1(x, x(cid:48))c2(y, y(cid:48)),
it recovers the negative of HSIC with kernels c1 and c2.

The quantity ¯Sε is known as the Sinkhorn divergence. It has been used in two-sample problems, where
the goal is to quantify the distance of two distributions given i.i.d. samples from each of them. In particular,
it is applied to two-sample testing (Ramdas et al., 2017) and generative modeling (Genevay et al., 2018).
It is shown in (Feydy et al., 2019) that ¯Sε deﬁnes a semi-metric (metric without the triangle inequality) on
the space of probability measures with bounded support if the Gibbs kernel induced by the cost is positive
universal. The limiting behavior of the empirical estimator is to date not known in the literature, though
non-asymptotic bounds are attainable using results in (Genevay et al., 2019; Mena and Weed, 2019). Our
results also recover the two-sample case.

3 Main Results

We give non-asymptotic bounds for the ETIC statistic with quadratic cost. We present the main results and
their proof sketches here. We use C to denote a constant whose value may change from line to line, where
subscripts are used to emphasize the dependency on other quantities. For instance, Cd represents a constant
depending only on the dimension d. The detailed proofs are deferred to Appendices B and C.

Consistency. We ﬁrst show that the ETIC statistic is a consistent estimator of its population counterpart
under both the null and alternative.

6

Assumption 2. We make the following assumptions:

(i) c is chosen as the quadratic cost.

(ii) PX and PY are subG(σ2).

The quadratic cost is chosen for the sake of concision. We extend the results to weighted quadratic cost

in Appendix B.

Theorem 3. Under Assumption 2, we have

E |Tn(X, Y ) − T (X, Y )| ≤ Cd

(cid:18)

1 +

σ(cid:100)5d/2(cid:101)+6
ε(cid:100)5d/4(cid:101)+3

(cid:19) ε
√
n

.

Remark 2. According to Theorem 3, when ε = εn is chosen such that εn = ω(n−1/((cid:100)5d/2(cid:101)+4)) and εn = o(1),
we have Tn(X, Y ) converges in L1 to OT(PXY , PX ⊗ PY ) as n → ∞.

We can upper bound the above L1 loss by the supremum of an empirical process and a U-process

(cid:13)
ˆPXY − PXY
(cid:13)
(cid:13)

(cid:13)
2
(cid:13)
(cid:13)

F s

and

(cid:13)
ˆPX ⊗ ˆPY − PX ⊗ PY
(cid:13)
(cid:13)

(cid:13)
2
(cid:13)
(cid:13)

F s

,

respectively, where F s is the set of real-valued functions satisfying

|f (x, y)| ≤ Cs,d(1 + (cid:107)(x, y)(cid:107)2)
|Dαf (x, y)| ≤ Cs,d(1 + (cid:107)(x, y)(cid:107)|α|),

∀1 < |α| ≤ s.

Mena and Weed (2019) used a similar strategy in their proofs. Empirical process theory has a long history
in statistics and there are well-established tools to control them; see, e.g., (van der Vaart and Wellner, 1996).
However, the theory of U-processes is much less well-developed. Moreover, many of the previous works focus
on one-sample U-processes; see, e.g., (Peña and Giné, 1999). The second U-process here is a two-sample
U-process on a paired sample, bringing about additional challenges in its analysis, compared to, e.g., Mena
and Weed (2019). In order to control it, we develop the following results.

The ﬁrst result is a metric entropy bound for degenerate two-sample U-processes. The main challenge
i,j=1 f (Xi, Yj). We get around that using the decou-

comes from the dependence among the summands in (cid:80)n
pling technique presented in (Peña and Giné, 1999).

Proposition 4. Let F be a class of real-valued functions that are degenerate under PX ⊗ PY , i.e.,

EPX ⊗PY [f (X, Y ) | X] a.s.= EPX ⊗PY [f (X, Y ) | Y ] a.s.= 0

for any f ∈ F. Under Assumption 2, we have

E

(cid:13)
ˆPX ⊗ ˆPY − PX ⊗ PY
(cid:13)
(cid:13)

(cid:13)
2
(cid:13)
(cid:13)

F

≤

E

C
n

(cid:32)(cid:90) B

(cid:113)

0

log N (τ, F, L2( ˆPX ⊗ ˆPY ))dτ

,

(cid:33)2

where B is any measurable upper bound of 2 maxf ∈F (cid:107)f (cid:107)L2( ˆPX ⊗ ˆPY ).

Remark 3. In classical two-sample U-statistics literature, it is usually assumed that the two samples are
independent, i.e., X is independent of Y . However, Proposition 4 allows the sample to be paired since
(X, Y ) ∼ PXY .

With Proposition 4 at hand, we can control the U-process (cid:107) ˆPX ⊗ ˆPY − PX ⊗ PY (cid:107)2

F s by upper bounding
its covering number N (τ, F s, L2( ˆPX ⊗ ˆPY )). The proof is inspired by (Mena and Weed, 2019) and relies
on a result in (van der Vaart and Wellner, 1996, Chapter 2.7) to control the covering number of a class of
smooth functions.

7

Figure 1: Power versus dimension in the linear dependency model (11).

Proposition 5. Under Assumption 2, there exists a random variable L ≥ 1 depending on the samples
{(Xi, Yi)}n

i=1 with E[L] ≤ 2 such that, for any s ≥ 2,

log N (τ, F s, L2( ˆPX ⊗ ˆPY )) ≤ Cs,dτ −d/sLd/2s(1 + σ2d)

and

In particular, when s > d/2, we have

max
f ∈F s

(cid:107)f (cid:107)2

L2( ˆPX ⊗ ˆPY ) ≤ Cs,d(1 + Lσ4).

E

(cid:13)
ˆPX ⊗ ˆPY − PX ⊗ PY
(cid:13)
(cid:13)

(cid:13)
2
(cid:13)
(cid:13)

F s

≤ Cs,d(1 + σ2d+4)

1
n

.

Exponential Tail Bound. We also prove an exponential tail bound for the ETIC statistic. It follows
from Theorem 3 and the McDiarmid inequality.

Theorem 6. Let c be the quadratic cost. Assume that PX and PY are supported on a bounded domain of
radius D. Then we have, with probability at least 1 − δ,

|Tn(X, Y ) − T (X, Y )| ≤ Cd

1 +

(cid:32)

D5d+16
ε5d/2+8

(cid:114)

log

(cid:33)

6
δ

ε
√
n

.

Under H0, we have T (X, Y ) = 0, so Theorem 6 implies that

(cid:32)

|Tn(X, Y )| > Cd

1 +

D5d+16
ε5d/2+8

(cid:114)

log

(cid:33)

6
δ

ε
√
n

with probability at most δ. It gives an estimate of the tail behavior of Tn(X, Y ) which suggests that the
critical value Hn(α) in (8) should be of order O(n−1/2). Under H1, Theorem 6 implies that

Tn(X, Y ) > T (X, Y ) − Cd

1 +

(cid:32)

D5d+16
ε5d/2+8

(cid:114)

log

(cid:33)

6
δ

ε
√
n

with probability at least 1 − δ. When T (X, Y ) > 0, it is clear that the right hand side in the above inequality
exceeds the threshold Hn(α) for large n. Hence, the ETIC test has power converging to 1 as n → ∞.

4 Experiments

We examine the empirical behavior of the proposed ETIC test for independence testing on both synthetic
and real data. We consider synthetic benchmarks from (Gretton et al., 2007b; Jitkrittum et al., 2017; Zhang

8

246810Dimension0.40.60.81.0PowerETIC246810DimensionETIC-RF246810DimensionHSICr = 0.25r = 0.5r = 1.0r = 2.0r = 4.0Figure 2: Power versus sample size in the Gaussian-sign model (12).

et al., 2018) and revisit an application from (Gretton et al., 2007b) with recent feature representations for
text data. The performance of the adaptive ETIC test is also investigated but is deferred to Appendix E.2
due to the space limit. The code to reproduce the experiments is available online1.

We focus on the weighted quadratic cost

c((x, y), (x(cid:48), y(cid:48))) =

1
λ1

(cid:107)x − x(cid:48)(cid:107)2 +

1
λ2

(cid:107)y − y(cid:48)(cid:107)2 .

For convenience, we absorb the regularization parameter ε into the weights and set ε = 1. It then induces
two Gibbs kernels

k1(x, x(cid:48)) = exp

(cid:110)
− (cid:107)x − x(cid:48)(cid:107)2 /λ1

(cid:111)

and k2(y, y(cid:48)) = exp

(cid:110)
− (cid:107)y − y(cid:48)(cid:107)2 /λ2

(cid:111)

with λi being the parameter of kernel ki for i ∈ {1, 2}. To select the weights, we apply the median heuristic
(Gretton et al., 2007b) widely used for HSIC, i.e.,

λ1 = r1Mx

and λ2 = r2My

with r1 and r2 ranging from 0.25 to 4, where Mx and My are the medians of the quadratic costs {(cid:107)Xi − Xj(cid:107)2}
and {(cid:107)Yi − Yj(cid:107)2}, respectively. We also examine its variant ETIC-RF discussed in Section 2, where the
number of random features is set to be 100 unless otherwise noted. We compare them with the HSIC
statistic with kernels k1 and k2. For a fair comparison, we calibrate these tests by a Monte Carlo resampling
technique (Feuerverger, 1993) with 200 permutations. For each of the experiment, we repeat the whole
procedure 200 times and report the rejection frequency as either the type I error rate (when the null is true)
or power (when the null is not true). Note that, even though we are using the same λ1 and λ2 in the cost and
kernels, that does not mean we should compare ETIC and HSIC under the same hyperparameters. Our goal
is to explore their performance over a range of values of the hyperparameters controlling the regularization
penalties.

Our main ﬁndings are: 1) Both ETIC and ETIC-RF are consistent in power as the sample size approaches
inﬁnity. 2) In some scenarios, ETIC and ETIC-RF outperforms HSIC signiﬁcantly; in the linear dependency
model in particular, their power is much more robust than HSIC to the value of the hyperparameters.
3) ETIC-RF performs reasonably good compared to ETIC with a moderate number (i.e., 100) of random
features. 4) All three tests beneﬁt from large hyperparameters in detecting simple linear dependency, but
smaller values lead to higher power when the dependency is more complicated.

Hilbert-Schmidt Independence Criterion. Before we present our results, let us recall the deﬁnition
of HSIC. Let k : Rd1 × Rd1 → R and l : Rd2 × Rd2 → R be two positive semi-deﬁnite kernels. The
Hilbert-Schmidt independence criterion (HSIC) between X and Y , HSIC(X, Y ), is deﬁned as

E[k(X, X (cid:48))l(Y, Y (cid:48))] + E[k(X, X (cid:48))] E[l(Y, Y (cid:48))] − 2 E[E[k(X, X (cid:48)) | X] E[l(Y, Y (cid:48)) | Y ]],

1https://github.com/langliu95/etic-experiments.

9

100200300400500Sample size0.00.20.40.60.81.0PowerETIC100200300400500Sample sizeETIC-RF100200300400500Sample sizeHSICr = 0.25r = 0.5r = 1.0r = 2.0r = 4.0Figure 3: Power versus parameter in the subspace dependency model.

where (X (cid:48), Y (cid:48)) is an independent copy of (X, Y ). Given an i.i.d. sample {(Xi, Yi)}n
estimate HSIC(X, Y ) by

i=1 from PXY , we can

1
n2

n
(cid:88)

i,j=1

kijlij +

1
n4

n
(cid:88)

i,j,s,t=1

kijlst −

2
n3

n
(cid:88)

i,j,s=1

kijlis,

where kij := k(Xi, Xj) and lij := l(Yi, Yj). We refer to it as the HSIC statistic.

4.1 Synthetic Data

We ﬁrst compare the performance of ETIC and ETIC-RF with HSIC on synthetic data. We consider
synthetic benchmarks from (Zhang et al., 2018), (Jitkrittum et al., 2017), and (Gretton et al., 2007b). To
facilitate the exploration, we set r1 = r2 = r ∈ {0.25, 0.5, 1, 2, 4} in this section. Moreover, we examine the
performance of two other independence tests discussed in (Gretton and Györﬁ, 2008) and summarize the
results and ﬁndings in Appendix E.3.

Linear Dependency. We begin with a simple linear dependency model. Concretely,

X ∼ Nd(0, Id) and Y = X1 + Z,

(11)

where X1 is the ﬁrst coordinate of X, and Z ∼ N (0, 1) is independent with X. We ﬁx n = 50 and plot the
power versus d ∈ [1, 10] in Figure 1. All the tests have decaying power as the dimension increases. This is as
expected since larger dimension results in weaker dependency between X and Y . It is clear that the power
of both ETIC and HSIC increases as r increases, with the former more robust than the latter. While the
performance of HSIC is similar to ETIC when r is large, it is much worse than ETIC when r is small. As
for ETIC-RF, it has similar power curves as ETIC.

Gaussian Sign. We then consider a Gaussian sign model, i.e.,

X ∼ Nd(0, Id) and Y = |Z|

d
(cid:89)

i=1

sgn(Xi),

(12)

where sgn(·) is the sign function and Z ∼ N (0, 1) is independent with X. This problem is challenging since Y
is independent with any strict subset of {X1, . . . , Xd}. We ﬁx d = 3 and plot the power versus n ∈ [100, 500]
in Figure 2. All the tests have improved power as the sample size increases. Additionally, they all beneﬁt
from a small regularization parameter, with HSIC performs the best and the other two perform similarly in
this particular example.

10

0.00.20.40.60.81.0(/4)0.00.20.40.60.81.0PowerETIC0.00.20.40.60.81.0(/4)ETIC-RF0.00.20.40.60.81.0(/4)HSICr = 0.25r = 0.5r = 1.0r = 2.0r = 4.0Figure 4: Heatmaps of power on the partially dependent sample of the bilingual data. The x-axis is for r1
and y-axis is for r2. The indices from 0 to 11 correspond to equally spaced values from 0.25 to 4. Lighter
color indicates larger power.

Subspace Dependency. One important application of independence testing is independent component
analysis (Gretton et al., 2005), which involves separating random variables from their linear mixtures.
We construct our data by i) generating n i.i.d. copies of two random variables following independently
0.5N (0.98, 0.04) + 0.5N (−0.98, 0.04), ii) mixing the two random variables by a rotation matrix parameter-
ized by θ ∈ [0, π/4] (larger θ leads to stronger dependency), iii) appending Nd−1(0, Id−1) to each of the two
mixtures, and iv) multiplying each vector by an independent random d-dimensional orthogonal matrix. We
refer to it as the subspace dependency model. We ﬁx n = 64, d = 2, and plot the power versus θ ∈ [0, π/4] in
Figure 3. As expected, the power of all three tests improves as θ becomes closer to π/4. Moreover, they all
have improved power as r decreases. ETIC and ETIC-RF performs similarly, and they are outperformed by
HSIC in this particular example.

4.2 Dependency between Bilingual Text

Inspired by Gretton et al. (2007b), we now investigate the performance of the proposed tests on bilingual data
using recent developments in natural language processing. Our dataset is taken from the parallel European
Parliament corpus (Koehn, 2005) which consists of a large number of documents of the same content in
diﬀerent languages. Note that it is also used in (Bounliphone et al., 2015) to test for relative dependency.
For the hyperparameters, we consider diﬀerent values of r1 and r2 ranging from 0.25 to 4.

To be more speciﬁc, we randomly select n = 64 English documents and a paragraph in each document
from the corpus. We then 1) pair each paragraph with the corresponding paragraph in French to form the
dependent sample, 2) pair each paragraph with a random paragraph in the same document in French to form
the partially dependent sample, and 3) pair each paragraph with a random paragraph in French to form the
independent sample.

Finally, we use LaBSE (Feng et al., 2020) to embed all the paragraphs into a common feature embedding
space of dimension 768 and perform independence testing on these feature vectors. LaBSE is a state-of-
the-art, language agnostic, sentence embedding model based on Bidirectional Encoder Representations from
Transformers (BERT). This allows us to revisit the idea of Gretton et al. (2007b) yet with more modern
feature embeddings.

Both ETIC and HSIC perform perfectly on the dependent sample (with power 1) and the independent
sample (with low type I error) across all values of r1 and r2 considered. The results on the partially dependent
sample is shown in Figure 4. ETIC performs better than HSIC when one of r1 and r2 is large; while HSIC has
larger power when r1 or r2 is small. Overall ETIC appears to perform better than HSIC for large amounts
of regularization parameters.

As for ETIC-RF, the high-dimensional natural of the feature embeddings imposes challenges on the
random feature approximation. For its performance to be comparable, we ﬁrst apply the principal component
analysis (PCA) on the text embeddings to reduce the dimension, and then we perform ETIC-RF on the low-
dimensional features. As presented in Appendix E.1, the random feature approximation equipped with PCA

11

0123456789101101234567891011ETIC01234567891011HSIC0.400.450.500.550.60demonstrates similar performance as the exact ETIC with enough random features.

Conclusion. We introduced a new independence criterion ETIC based on entropy regularized optimal
transport. The proposed criterion can be approximated using a random feature approximation. We es-
tablished non-asymptotic bounds using U-process theory and optimal transport theory. The experimental
results show that ETIC can exhibit stable behavior w.r.t. its hyperparameters. The extension of ETIC to
multi-way dependence is an interesting venue for future work.

Acknowledgements

The authors would like to thank M. Scetbon for fruitful discussions. L. Liu is supported by NSF CCF-
2019844 and NSF DMS-2023166. S. Pal is supported by NSF DMS-2052239 and a PIMS CRG (PIHOT). Z.
Harchaoui is supported by NSF CCF-2019844, NSF DMS-2134012, NSF DMS-2023166, CIFAR-LMB, and
faculty research awards. Part of this work was done while Z. Harchaoui was visiting the Simons Institute
for the Theory of Computing. The authors would like to thank the Kantorovich Initiative of the Paciﬁc
Institute for the Mathematical Sciences (PIMS) for supporting this collaboration.

References

L. Ambrosio, N. Gigli, and G. Savaré. Gradient Flows: In Metric Spaces and in the Space of Probability

Measures. Birkhäuser Basel, 1 edition, 2005.

F. R. Bach and M. I. Jordan. Kernel independent component analysis. Journal of Machine Learning Research,

3, 2002.

G. Blanchard, G. Lee, and C. Scott. Generalizing from several related classiﬁcation tasks to a new unlabeled

sample. In NIPS, 2011.

W. Bounliphone, A. Gretton, A. Tenenhaus, and M. Blaschko. A low variance consistent test of relative

dependency. In ICML, 2015.

S. Chakraborty and X. Zhang. Distance metrics for measuring joint dependence with application to causal

inference. Journal of the American Statistical Association, 114(528), 2019.

D. M. Cifarelli and E. Regazzini. On the centennial anniversary of Gini’s theory of statistical relations.

Metron, 2017.

G. M. Constantine and T. H. Savits. A multivariate Faà di Bruno formula with applications. Transactions

of the American Mathematical Society, 348(2), 1996.

I. Csiszar. I-divergence geometry of probability distributions and minimization problems. Annals of Proba-

bility, 3, 1975.

M. Cuturi. Sinkhorn distances: Lightspeed computation of optimal transport. In NIPS, 2013.

N. Deb and B. Sen. Multivariate rank-based distribution-free nonparametric testing using measure trans-

portation. Journal of the American Statistical Association, 2021.

R. M. Dudley. The speed of mean Glivenko-Cantelli convergence. The Annals of Mathematical Statistics, 40

(1), 1969.

P. Dvurechensky, A. Gasnikov, and A. Kroshnin. Computational optimal transport: Complexity by acceler-

ated gradient descent is better than by Sinkhorn’s algorithm. In ICML, 2018.

F. Feng, Y. Yang, D. Cer, N. Arivazhagan, and W. Wang. Language-agnostic BERT sentence embedding.

arXiv Preprint, 2020.

S. Ferradans, N. Papadakis, G. Peyré, and J.-F. Aujol. Regularized discrete optimal transport. SIAM

Journal on Imaging Sciences, 7(3), 2014.

12

A. Feuerverger. A consistent test for bivariate dependence. International Statistical Review, 61(3), 1993.

J. Feydy, T. Séjourné, F. Vialard, S. Amari, A. Trouvé, and G. Peyré.
transport and MMD using Sinkhorn divergences. In AISTATS, 2019.

Interpolating between optimal

H. Föllmer. Random ﬁelds and diﬀusion processes. In École d’été de probabilités de Saint-Flour XV-XVII-

1985-87, volume 1362 of Lecture Notes in Mathematics. Springer, 1988.

N. Fournier and A. Guillin. On the rate of convergence in Wasserstein distance of the empirical measure.

Probability Theory and Related Fields, 162, 2015.

A. Genevay, M. Cuturi, G. Peyré, and F. R. Bach. Stochastic optimization for large-scale optimal transport.

In NIPS, 2016.

A. Genevay, G. Peyré, and M. Cuturi. Learning generative models with Sinkhorn divergences. In AISTATS,

2018.

A. Genevay, L. Chizat, F. Bach, M. Cuturi, and G. Peyré. Sample complexity of Sinkhorn divergences. In

AISTATS, 2019.

E. Giné and R. Nickl. Mathematical Foundations of Inﬁnite-Dimensional Statistical Models. Cambridge

University Press, 2015.

A. Gretton and L. Györﬁ. Nonparametric independence tests: Space partitioning and kernel approaches. In

ALT, 2008.

A. Gretton, R. Herbrich, A. J. Smola, O. Bousquet, and B. Schölkopf. Kernel methods for measuring

independence. Journal of Machine Learning Research, 6, 2005.

A. Gretton, K. Borgwardt, M. Rasch, B. Schölkopf, and A. Smola. A kernel method for the two-sample

problem. In NIPS, 2007a.

A. Gretton, K. Fukumizu, C. H. Teo, L. Song, B. Schölkopf, and A. J. Smola. A kernel statistical test of

independence. In NIPS, 2007b.

A. Gretton, K. M. Borgwardt, M. J. Rasch, B. Schölkopf, and A. J. Smola. A kernel two-sample test. Journal

of Machine Learning Research, 13, 2012.

W. Hoeﬀding. A non-parametric test of independence. The Annals of Mathematical Statistics, 19(4), 1948.

W. Jitkrittum, Z. Szabó, and A. Gretton. An adaptive test of independence with analytic kernel embeddings.

In ICML, 2017.

P. Koehn. Europarl: A parallel corpus for statistical machine translation.

In Proceedings of Machine

Translation Summit, 2005.

W. H. Kruskal. Ordinal measures of association. Journal of the American Statistical Association, 53(284),

1958.

E. L. Lehmann. Some concepts of dependence. The Annals of Mathematical Statistics, 37(5), 1966.

J. Lei. Convergence and concentration of empirical measures under Wasserstein distance in unbounded

functional spaces. Bernoulli, 26(1), 2020.

C. Léonard. From the Schrödinger problem to the Monge-Kantorovich problem. Journal of Functional

Analysis, 262(4), 2012.

C. Léonard. A survey of the Schrödinger problem and some of its connections with optimal transport.

Discrete and Continuous Dynamical Systems, 34(4), 2014.

Y. Li, R. Pogodin, D. J. Sutherland, and A. Gretton. Self-supervised learning with kernel dependence

maximization. arXiv Preprint, 2021.

13

R. Lyons. Distance covariance in metric spaces. Annals of Probability, 41(5), 2013.

G. Mena and J. Weed. Statistical bounds for entropic optimal transport: sample complexity and the central

limit theorem. In NeurIPS, 2019.

G. Mordant and J. Segers. Measuring dependence between random vectors via optimal transport. arXiv

Preprint, 2021.

T. G. Nies, T. Staudt, and A. Munk. Transport dependency: Optimal transport based dependency measures.

arXiv Preprint, 2021.

Y. Nikitin. Asymptotic Eﬃciency of Nonparametric Tests. Cambridge University Press, 1995.

S. Ozair, C. Lynch, Y. Bengio, A. van den Oord, S. Levine, and P. Sermanet. Wasserstein dependency

measure for representation learning. In NeurIPS, 2019.

V. d. l. Peña and E. Giné. Decoupling: From Dependence to Independence. Springer, 1 edition, 1999.

G. Peyré and M. Cuturi. Computational optimal transport: With applications to data science. Foundations

and Trends in Machine Learning, 11(5-6), 2019.

N. Pﬁster, P. Bühlmann, B. Schölkopf, and J. Peters. Kernel-based tests for joint independence. Journal of

the Royal Statistical Society: Series B (Statistical Methodology), 80(1), 2018.

A. Ramdas, N. García Trillos, and M. Cuturi. On Wasserstein two-sample testing and related families of

nonparametric tests. Entropy, 19(2), 2017.

P. Rigollet and J. Weed. Entropic optimal transport is maximum-likelihood deconvolution. Comptes Rendus

Mathematique, 356(11), 2018.

L. Rüschendorf and W. Thomsen. Note on the Schrödinger equation and I-projections. Statistics & Probability

Letters, 17, 1993.

T. Salimans, H. Zhang, A. Radford, and D. N. Metaxas. Improving GANs using optimal transport. In ICLR,

2018.

M. Sanjabi, J. Ba, M. Razaviyayn, and J. D. Lee. On the convergence and robustness of training GANs with

regularized optimal transport. In NeurIPS, 2018.

F. Santambrogio. Optimal Transport for Applied Mathematicians. Progress in Nonlinear Diﬀerential Equa-

tions and Their Applications. Birkhäuser Basel, 2015.

M. Scetbon and M. Cuturi. Linear time Sinkhorn divergences using positive features. In NeurIPS, 2020.

B. Schweizer and E. F. Wolﬀ. On nonparametric measures of dependence for random variables. The Annals

of Statistics, 9(4), 1981.

D. Sejdinovic, B. Sriperumbudur, A. Gretton, and K. Fukumizu. Equivalence of distance-based and RKHS-

based statistics in hypothesis testing. The Annals of Statistics, 41(5), 2013.

R. J. Serﬂing. Approximation Theorems of Mathematical Statistics. John Wiley & Sons, 1980.

H. Shi, M. Drton, and F. Han. Distribution-free consistent independence tests via center-outward ranks and

signs. Journal of the American Statistical Association, 2020.

G. J. Székely and M. L. Rizzo. Testing for equal distributions in high dimension. InterStat, 5, 2004.

G. J. Székely, M. L. Rizzo, and N. K. Bakirov. Measuring and testing dependence by correlation of distances.

Annals of Statistics, 35(6), 2007.

A. van der Vaart and J. Wellner. Weak Convergence and Empirical Processes: With Applications to Statistics.

Springer, 1 edition, 1996.

14

R. Vershynin. High-Dimensional Probability: An Introduction with Applications in Data Science. Cambridge

Series in Statistical and Probabilistic Mathematics. Cambridge University Press, 2018.

C. Villani. Topics in Optimal Transportation. American Mathematical Society, 2016.

M. J. Wainwright. High-Dimensional Statistics: A Non-Asymptotic Viewpoint. Cambridge Series in Statis-

tical and Probabilistic Mathematics. Cambridge University Press, 2019.

J. Weed and F. Bach. Sharp asymptotic and ﬁnite-sample rates of convergence of empirical measures in

Wasserstein distance. Bernoulli, 25(4A), 2019.

J. Wiesel. Measuring association with Wasserstein distances. arXiv Preprint, 2021.

Q. Zhang, S. Filippi, A. Gretton, and D. Sejdinovic. Large-scale kernel methods for independence testing.

Statistics and Computing, 28, 2018.

15

A Properties of ETIC

In this section, we prove the properties of ETIC discussed in Section 2. For the sake of generality, we state
the problem for general notations P and Q while keeping in mind that P, Q ∈ {PXY , PX ⊗ PY } in our case.
Let P ∈ M1(Rd1 × Rd2) and PX and PY be the marginals on Rd1 and Rd2, respectively. Deﬁne Q, QX , and
QY similarly. We are interested in the EOT cost between P and Q under the cost function c:

Sε(P, Q) := inf

γ∈Π(P,Q)

(cid:20)(cid:90)

cdγ + ε KL(γ(cid:107)P ⊗ Q)

.

(cid:21)

(13)

When ε = 0, S0(P, Q) is the optimal transport cost between P and Q. When ε > 0, it admits a dual
representation:

Sε(P, Q) :=

sup
f,g∈C(Rd1 ×Rd2 )

(cid:20)(cid:90)

(cid:90)

f dP +

gdQ + ε − ε

(cid:90)

1

(cid:21)
ε [f (z)+g(z(cid:48))−c(z,z(cid:48))]dP (z)dQ(z(cid:48))

.

e

(14)

The Schrödinger bridge potentials (fε, gε) satisfy the optimality conditions:

(cid:90)

(cid:90)

1

ε [fε(z)+gε(z)−c(z,z(cid:48))]dQ(z(cid:48)) a.s.= 1

e

1

ε [fε(z)+gε(z)−c(z,z(cid:48))]dP (z) a.s.= 1.

e

(15)

We ﬁrst derive the limit ETIC as ε → 0 and ε → ∞.

Proposition 7. Let c be a continuous cost function. If either c is bounded or P and Q have compact support,
it holds that

Tε(X, Y ) →

(cid:40)
0
− 1

2 HSICc1,c2(X, Y )

if c = c1 ⊕ c2
if c = c1 ⊗ c2,

as ε → ∞.

Moreover, if both P and Q are densities (or discrete measures), then

Tε(X, Y ) → S0(PXY , PX ⊗ PY ),

as ε → 0.

Proof. To show (16), we claim that, for all P, Q ∈ M1(Rd),

and

S0(P, Q) ≤ Sε(P, Q) ≤ (P ⊗ Q)[c],

lim
ε→∞

Sε(P, Q) = (P ⊗ Q)[c].

In fact, for any ε1 < ε2, we have
(cid:90)

cdγ + ε1 KL(γ(cid:107)P ⊗ Q) ≤

(cid:90)

cdγ + ε2 KL(γ(cid:107)P ⊗ Q),

for all γ ∈ Π(P, Q).

This yields that

and thus (18) follows.

Sε1(P, Q) ≤ Sε2(P, Q),

for all ε1 ≤ ε2,

(16)

(17)

(18)

(19)

We then study the limit of Sε as ε → ∞. By the assumption that c is bounded or P and Q have compact

support, there exists M > 0 such that supγ∈Π(P,Q)

(cid:82) cdγ ≤ M < ∞. As a result,

sup
γ∈Π(P,Q)

(cid:90)

(cid:12)
(cid:12)
(cid:12)
(cid:12)

1
ε

(cid:12)
(cid:12)
cdγ + KL(γ(cid:107)P ⊗ Q) − KL(γ(cid:107)P ⊗ Q)
(cid:12)
(cid:12)

≤

M
ε

,

16

which implies that

inf
γ∈Π(P,Q)

(cid:90)

(cid:20) 1
ε

cdγ + KL(γ(cid:107)P ⊗ Q)

→ inf

γ∈Π(P,Q)

KL(γ(cid:107)P ⊗ Q) = 0,

as ε → ∞.

(cid:21)

By the strict convexity of KL, the problem on the LHS has a unique minimizer γε and the problem on
the RHS has a unique minimizer γ∗ = P ⊗ Q. Now, by the tightness of Π(P, Q) (e.g., (Santambrogio,
2015, Theorem. 1.7)), every sequence of {γε} has a weakly converging subsequence whose limit must be γ∗.
Therefore, the claim (19) holds true.

Let c = c1 ⊕ c2. According to (19), we have

lim
ε→∞

Sε(PXY , PX ⊗ PY ) = (PXY ⊗ PX ⊗ PY )[c] = (PX ⊗ PX )[c1] + (PY ⊗ PY )[c2].

Similarly, it holds that

lim
ε→∞

Sε(PXY , PXY ) = (PX ⊗ PX )[c1] + (PY ⊗ PY )[c2]

lim
ε→∞

Sε(PX ⊗ PY , PX ⊗ PY ) = (PX ⊗ PX )[c1] + (PY ⊗ PY )[c2].

Consequently, limε→∞ Tε(X, Y ) = 0. An analogous argument implies that, when c = c1 ⊗ c2

Tε(X, Y ) = EPXY [EPX [c1(X, X (cid:48)) | X] EPY [c2(Y, Y (cid:48)) | Y ]]
1
2

[c1(X, X (cid:48))c2(Y, Y (cid:48))] −

E(PX ⊗PY )2[c1(X, X (cid:48))c2(Y, Y (cid:48))] = −

1
2

P 2

E

XY

1
2

HSICc1,c2(X, Y ).

lim
ε→∞

−

Note that

lim
ε→0

Sε(P, Q) = S0(P, Q)

when both P and Q are densities (Léonard, 2012) and when both of them are discrete measures (Peyré and
Cuturi, 2019, Proposition 4.1). The statement (17) follows immediately from the fact that S0(P, P ) = 0 for
all P .

We then prove the validity of ETIC as a dependence measure as stated in Proposition 1.

Proof of Proposition 1. Due to Blanchard et al. (2011, Lemma 5.2), the Gibbs kernel

kε(z, z(cid:48)) := e−c(z,z(cid:48))/ε = k1(x, x(cid:48))k2(y, y(cid:48))

is universal since both kx and ky are. It is also clear that kε is positive since both kx and ky are. Consequently,
the Sinkhorn divergence ¯Sε deﬁnes a semi-metric on M1(X × Y) according to Feydy et al. (2019, Theorem
1). Hence, if PXY , PX ⊗ PY ∈ M1(X × Y), then Tε(X, Y ) := ¯Sε(PXY , PX ⊗ PY ) = 0 iﬀ PXY = PX ⊗ PY .

Finally, we analyze the computational complexity of the Tensor Sinkhorn algorithm for additive cost

functions, i.e.,

c(z, z(cid:48)) := c1(x, x(cid:48)) + c2(y, y(cid:48)),

(20)

where z = (x, y) and z(cid:48) = (x(cid:48), y(cid:48)).

Let {xi}n

i=1 and {yj}n

j=1 be two sets of atoms. Note that the two sets are assumed to be of the same
size for convenience. Let A and B be two probability measures on {xi}n
j=1. For convenience,
both A and B are represented as a matrix, i.e., Aij = A(xi, yj). For instance, if we choose A = ˆPXY and
B = ˆPX ⊗ ˆPY , then, in its matrix form, A = In/n and B = 1n×n/n2. Denote C1 and C2 as the cost matrices
j=1, respectively. Deﬁne Gibbs matrices K1 := e−C1/ε and K2 := e−C2/ε, where the
of {xi}n
exponential function is applied element-wisely. Let K := K2 ⊗ K1 ∈ Rn2×n2
be the Gibbs matrix associated
with the cost matrix on the pairs {(x1, y1), (x2, y1), . . . , (xn, yn)}, where ⊗ is the Kronecker product.

i=1 and {yj}n

i=1 × {yj}n

17

Proposition 8. The Tensor Sinkhorn algorithm outputs an δ-accurate estimate of the entropic cost S(A, B)
in O (cid:0)n3 log(κ1κ2κ3)/δ(cid:1) arithmetic operations, where κ1 := maxi,i(cid:48) k−1
2 (yj, yj(cid:48)),
and κ3 := maxi,j{a−1

1 (xi, xi(cid:48)), κ2 := maxj,j(cid:48) k−1

ij , b−1

ij }.

Proof. Let a := Vec(A) ∈ Rn2
respectively. Denote u := Vec(U ) ∈ Rn2
has the following two update steps:

and b := Vec(B) ∈ Rn2

and v := Vec(V ) ∈ Rn2

be the probability vectors corresponding to A and B,
. The Sinkhorn algorithm to solve Sε(a, b)

u = a (cid:11) Kv

and v = b (cid:11) K (cid:62)u.

By the identity Vec(M N L) = (L(cid:62) ⊗ M ) Vec(N ) for matrices M , N , and L of compatible dimensions, we

obtain

Vec(K1V K (cid:62)

2 ) = (K2 ⊗ K1) Vec(V ) = Kv.

Thus, the update U = A (cid:11) (K1V K (cid:62)
1 U K2)
is equivalent to v = b (cid:11) K (cid:62)u. Due to Dvurechensky et al. (2018, Theorem 1), the Tensor Sinkhorn algorithm
therefore outputs an δ-accurate estimate in O(log(κ1κ2κ3)/δ) iterations. Since each iteration costs O(n3)
time, it has overall time complexity O(n3 log(κ1κ2κ3)/δ).

2 ) is equivalent to u = a (cid:11) Kv. Similarly, the updated V = B (cid:11) (K (cid:62)

Remark 4. A direct application of the Sinkhorn algorithm leads to O(n4 log(κ1κ2κ3)/δ) time complexity,
which is n times slower than the Tensor Sinkhorn algorithm.

We then characterize the convergence of the Tensor Sinkhorn algorithm with the random feature approx-

imation as presented in Proposition 2.

Proof of Proposition 2. The proof is heavily inspired by Scetbon and Cuturi (2020, Proof of Theorem 3.1).
In consideration of the space, we only present the part that is signiﬁcantly diﬀerent from theirs, i.e., a
counterpart of Scetbon and Cuturi (2020, Proposition 3.1). This proposition gives a uniform tail bound for
the ratio between the approximated kernel and the original kernel. In our case, we are approximating the
kernel K := K2 ⊗ K1 by Ku,v := K2,v ⊗ K1,u. Hence, it suﬃces to bound

x,x(cid:48)∈{xi}n

sup
i=1,y,y(cid:48)∈{yi}n

i=1

(cid:12)
(cid:12)
(cid:12)
(cid:12)

k1,u(x, x(cid:48))k2,v(y, y(cid:48))
k1(x, x(cid:48))k2(y, y(cid:48))

(cid:12)
(cid:12)
− 1
(cid:12)
(cid:12)

.

Note that

k1,u(x, x(cid:48))
k1(x, x(cid:48))

=

1
p

p
(cid:88)

k=1

ϕ(x, uk)(cid:62)ϕ(x(cid:48), uk)
k1(x, x(cid:48))

is a sum of nonnegative i.i.d. random variables with mean 1. Due to Assumption 1, they are also bounded.
It follows from the Hoeﬀding inequality that

P

(cid:18)(cid:12)
(cid:12)
(cid:12)
(cid:12)

k1,u(x, x(cid:48))
k1(x, x(cid:48))

(cid:12)
(cid:12)
− 1
(cid:12)
(cid:12)

(cid:19)

(cid:18)

≥ t

≤ 2 exp

−

(cid:19)

.

pt2
C 2

The same inequality holds for the ratio k2,v(y, y(cid:48))/k2(y, y(cid:48)). Since
(cid:12)
(cid:12)
(cid:12)
(cid:12)
− 1
(cid:12)
(cid:12)
(cid:12)
(cid:12)

k1,u(x, x(cid:48))k2,v(y, y(cid:48))
k1(x, x(cid:48))k2(y, y(cid:48))

k1,u(x, x(cid:48))
k1(x, x(cid:48))

k2,v(y, y(cid:48))
k2(y, y(cid:48))

(cid:12)
(cid:12)
− 1
(cid:12)
(cid:12)

− 1

(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)
(cid:12)
(cid:12)

≤

+

(cid:12)
(cid:12)
(cid:12)
(cid:12)

k1,u(x, x(cid:48))
k1(x, x(cid:48))

− 1

(cid:12)
(cid:12)
(cid:12)
(cid:12)

+

(cid:12)
(cid:12)
(cid:12)
(cid:12)

k2,v(y, y(cid:48))
k2(y, y(cid:48))

− 1

(cid:12)
(cid:12)
(cid:12)
(cid:12)

,

it follows that
(cid:18)(cid:12)
(cid:12)
(cid:12)
(cid:12)

P

k1,u(x, x(cid:48))k2,v(y, y(cid:48))
k1(x, x(cid:48))k2(y, y(cid:48))

≤ t2 + 2t

(cid:19)

≥ P

(cid:12)
(cid:12)
− 1
(cid:12)
(cid:12)

= P

− 1

(cid:12)
(cid:12)
(cid:12)
(cid:12)

≤ t

(cid:27) (cid:92) (cid:26)(cid:12)
(cid:12)
(cid:12)
(cid:12)

k2,v(y, y(cid:48))
k2(y, y(cid:48))

− 1

− 1

(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:19)

P

≤ t

(cid:18)(cid:12)
(cid:12)
(cid:12)
(cid:12)

k2,v(y, y(cid:48))
k2(y, y(cid:48))

(cid:12)
(cid:12)
− 1
(cid:12)
(cid:12)

≤ t

(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:27)(cid:19)

≤ t

(cid:19)

(cid:18)(cid:26)(cid:12)
k1,u(x, x(cid:48))
(cid:12)
(cid:12)
k1(x, x(cid:48))
(cid:12)
k1,u(x, x(cid:48))
k1(x, x(cid:48))
(cid:18)

(cid:18)(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:19)

.

pt2
C 2

≥ 1 − 4 exp

−

18

Equivalently,

P

(cid:18)(cid:12)
(cid:12)
(cid:12)
(cid:12)

k1,u(x, x(cid:48))k2,v(y, y(cid:48))
k1(x, x(cid:48))k2(y, y(cid:48))

− 1

(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:19)

(cid:18)

≥ t

≤ 4 exp

−

√

p(

t + 1 − 1)2

C 2

(cid:19)

.

A uniform bound yields

(cid:32)

P

x,x(cid:48)∈{xi}n

sup
i=1,y,y(cid:48)∈{yi}n

i=1

(cid:12)
(cid:12)
(cid:12)
(cid:12)

k1,u(x, x(cid:48))k2,v(y, y(cid:48))
k1(x, x(cid:48))k2(y, y(cid:48))

(cid:12)
(cid:12)
− 1
(cid:12)
(cid:12)

(cid:33)

≥ t

≤ 4n4 exp

√

(cid:18)

p(

−

t + 1 − 1)2

C 2

(cid:19)

.

Remark 5. Let ˆSε,cu,v (A, B) be the cost computed from Algorithm 1. Following Dvurechensky et al. (2018,
Theorem 1), we can get that

(cid:12)
(cid:12)
ˆSε,cu,v (A, B) − Sε,cu,v (A, B)
(cid:12)
(cid:12)
(cid:12) ≤ τ
(cid:12)
in O (cid:0)pn2 log(κ1κ2κ3)/τ (cid:1) arithmetic operations, where κ1 := maxi,i(cid:48) k−1
and κ3 := maxi,j{a−1

ij , b−1

ij }.

1,u(xi, xi(cid:48)), κ2 := maxj,j(cid:48) k−1

2,v(yj, yj(cid:48)),

B Consistency of ETIC

In this section, we prove the main results in Section 3. For the sake of generality, we start by considering
the formulation in (13). We focus on the weighted quadratic cost function

c(z, z(cid:48)) := w1 (cid:107)x − x(cid:48)(cid:107)2 + w2 (cid:107)y − y(cid:48)(cid:107)2 ,

where z = (x, y), z(cid:48) = (x(cid:48), y(cid:48)) and w1, w2 ∈ R+. Denote w := max{w1, w2}. Due to Lemma 24, we assume,
w.l.o.g., that ε = 1 and write S(P, Q) := S1(P, Q).

B.1 Smoothness Properties of the Schrödinger Potentials

We start by deriving some smoothness properties of the Schrödinger potentials. Our proofs are deeply
inspired by Mena and Weed (2019). Our results generalize theirs to weighted quadratic cost functions.

Assumption 3. We assume that PX , PY , QX , and QY are all subG(σ2).

Proposition 9. Under Assumption 3. there exist smooth Schrödinger potentials (f, g) for S(P, Q) such that
the optimality conditions (15) hold for all z, z(cid:48) ∈ Rd. Moreover, we have

f (z) ≥ −dσ2 (cid:104)

2w1 + 2w2 + 4w2
1(

(cid:112)

2d1σ + (cid:107)x(cid:107))2 + 4w2
2(

(cid:112)

2d2σ + (cid:107)y(cid:107))2(cid:105)

− 1

f (z) ≤ w1((cid:107)x(cid:107) +

(cid:112)

2d1σ)2 + w2((cid:107)y(cid:107) +

(cid:112)

2d2σ)2,

and for g similarly.

Proof. Let (f0, g0) be a pair of Schrödinger potentials. Since (f0 + C, g0 − C) is also a pair of Schrödinger
potentials for any constant C ∈ R, we assume, w.l.o.g., that P [f0] = Q[g0] = 1

2 S(P, Q) ≥ 0. Deﬁne

f (z) := − log

(cid:90)

eg0(z(cid:48))−c(z,z(cid:48))dQ(z(cid:48)) and g(z(cid:48)) := − log

(cid:90)

ef (z)−c(z,z(cid:48))dP (z).

(21)

We claim that the pair (f, g) satisﬁes the requirements.

Since (f0, g0) is a pair of Schrödinger potentials, it holds that

g0(z(cid:48)) a.s.= − log

(cid:90)

ef0(z)−c(z,z(cid:48))dP (z) ≤ −P [f0] + w1 EPX [(cid:107)X − x(cid:48)(cid:107)2] + w2 EPY [(cid:107)Y − y(cid:48)(cid:107)2],

19

by Jensen’s inequality. Note that P [f0] ≥ 0 and, by Lemma 18, EPX [(cid:107)X(cid:107)2] ≤ 2d1σ2. It follows that
(cid:105)

(cid:104)

(cid:104)
2d1σ2 + 2 (cid:107)x(cid:48)(cid:107) (

(cid:112)

(cid:105)
2d1σ + (cid:107)x(cid:107))

+ w2

2d2σ2 + 2 (cid:107)y(cid:48)(cid:107) (

2d2σ + (cid:107)y(cid:107))

,

(cid:112)

g0(z(cid:48)) − c(z, z(cid:48)) ≤ w1

and thus

(cid:90)

eg0(z(cid:48))−c(z,z(cid:48))dQ(z(cid:48)) ≤ e2(w1d1+w2d2)σ2 (cid:20)(cid:90)

√
e4w1(cid:107)x(cid:48)(cid:107)(

≤ 2e2(w1d1+w2d2)σ2

e4d1σ2w2
1(

√

2d1σ+(cid:107)x(cid:107))2+4d2σ2w2
2(

2d2σ+(cid:107)y(cid:107))2

< ∞,

by Lemma 18.

2d1σ+(cid:107)x(cid:107))dQX (x(cid:48))
√

(cid:90)

√
e4w2(cid:107)y(cid:48)(cid:107)(

2d2σ+(cid:107)y(cid:107))dQY (y(cid:48))

(cid:21)1/2

Hence, f (z) is well-deﬁned for all z ∈ Rd. Moreover, we have the lower bound

f (z) ≥ −d1σ2 (cid:104)
≥ −dσ2 (cid:104)

2w1 + 4w2
1(
(cid:112)

4w + 4w2
1(

(cid:112)

2d1σ + (cid:107)x(cid:107))2(cid:105)

2d1σ + (cid:107)x(cid:107))2 + 4w2
2(

− d2σ2 (cid:104)
(cid:112)

2w2 + 4w2
2(
2d2σ + (cid:107)y(cid:107))2(cid:105)

− 1

(cid:112)

2d2σ + (cid:107)y(cid:107))2(cid:105)

− 1

For the upper bound, by Jensen’s inequality, it holds that

f (z) ≤ −Q[g0] + w1 EQX (cid:107)x − X (cid:48)(cid:107)2 + w2 EQY (cid:107)y − Y (cid:48)(cid:107)2
2d1σ)2 + w2((cid:107)y(cid:107) +

≤ w1((cid:107)x(cid:107) +

2d2σ)2.

(cid:112)

(cid:112)

Similar arguments prove the claim for g. Now, it remains to show that (f, g) satisﬁes the optimality conditions
(15) for all z, z ∈ Rd. By deﬁnition, it is clear that
(cid:90)

(cid:90)

ef (z)+g(z(cid:48))−c(z,z(cid:48))dP (z) = 1 and

ef (z)+g0(z(cid:48))−c(z,z(cid:48))dQ(z(cid:48)) = 1,

∀z, z(cid:48) ∈ Rd.

Since (f0, g0) is a pair of Schrödinger potentials, we also have

(cid:90)

ef0(z)+g0(z(cid:48))−c(z,z(cid:48))dP (z)dQ(z(cid:48)) = 1.

Consequently, by Jensen’s inequality

(cid:90)

(f − f0)dP +

(cid:90)

(g − g0)dQ

ef0−f dP − log

(cid:90)

eg0−gdQ

ef0(z)+g0(z(cid:48))−c(z,z(cid:48))dP (z)dQ(z(cid:48)) − log

(cid:90)

ef (z)+g0(z(cid:48))−c(z,z(cid:48))dP (z)dQ(z(cid:48))

(cid:90)

(cid:90)

≥ − log

= − log

= 0.

Since both (f0, g0) and (f, g) are Schrödinger potentials, the above equality holds true. This implies that
(cid:82) (g0 − g)dQ = log (cid:82) eg0−gdQ, and thus g = g0 + C Q-almost surely by the strict concavity of log. Therefore,
we have

(cid:90)

ef (z)+g(z(cid:48))−c(z,z(cid:48))dQ(z(cid:48)) = eC

(cid:90)

ef (z)+g0(z(cid:48))−c(z,z(cid:48))dQ(z(cid:48)) = eC,

∀z, z(cid:48) ∈ Rd.

Taking integrals with respect to P implies that C = 0, which completes the proof.

The next proposition shows that there exist Schrödinger potentials satisfying Hölder-type conditions.

Deﬁnition 4. For any σ ∈ R+, d ∈ N+, and w = (w1, w2) ∈ R2
functions such that, for any k ∈ N+ and any multi-index α with |α| = k,

+, let Fσ := Fσ,d,w be the set of smooth

(cid:12)
(cid:12)

(cid:12)Dα (cid:16)

f (x, y) − w1 (cid:107)x(cid:107)2 − w2 (cid:107)y(cid:107)2(cid:17)(cid:12)
(cid:12)
(cid:12) ≤ Ck,d,w

(cid:40)

(1 + σ4)
σk(1 + σ)k

if k = 0
otherwise,

(22)

20

if (cid:107)z(cid:107) ≤

√

dσ, and

(cid:12)
(cid:12)

(cid:12)Dα (cid:16)

f (x, y) − w1 (cid:107)x(cid:107)2 − w2 (cid:107)y(cid:107)2(cid:17)(cid:12)
(cid:12)
(cid:12) ≤ Ck,d,w

(cid:40)

[1 + (1 + σ2) (cid:107)x(cid:107)2]
σk((cid:112)σ (cid:107)x(cid:107) + σ (cid:107)x(cid:107))k

if k = 0
otherwise,

(23)

√

if (cid:107)z(cid:107) >

dσ, where Ck,d,w is a constant depending on k, d, and w.

Proposition 10. Under Assumption 3, there exist Schrödinger potentials (f, g) such that the optimality
conditions (15) hold for all z, z(cid:48) ∈ Rd and f, g ∈ Fσ.

Proof. Let (f, g) be a pair of Schrödinger potentials satisfying the requirements in Proposition 9. Denote
¯f (x, y) := f (x, y) − w1 (cid:107)x(cid:107)2 − w2 (cid:107)y(cid:107)2. Note that

¯f (z) = − log e− ¯f (x,y) = − log
(cid:90)

= − log

eg(z(cid:48))−w1(cid:107)x(cid:48)(cid:107)2

(cid:90)

ew1(cid:107)x(cid:107)2+w2(cid:107)y(cid:107)2+g(z(cid:48))−c(z,z(cid:48))dQ(z(cid:48))

−w2(cid:107)y(cid:48)(cid:107)2

+2w1(cid:104)x,x(cid:48)(cid:105)+2w2(cid:104)y,y(cid:48)(cid:105)dQ(z(cid:48)).

The desired inequalities for k = 0 follow directly from Proposition 9. We focus on k > 0. According to the
multivariate Faá di Bruno formula (Constantine and Savits, 1996), we have

Dα ¯f (z) =

(cid:88)

Cα,λ1,...,λk

λ1+···+λk=α

k
(cid:89)

i=1

Mλi,

where

Mλ =

(cid:111)
(cid:110)
(cid:82) (˜z(cid:48))λ exp
g(z(cid:48)) − w1 (cid:107)x(cid:48)(cid:107)2 − w2 (cid:107)y(cid:48)(cid:107)2 + 2w1(cid:104)x, x(cid:48)(cid:105) + 2w2(cid:104)y, y(cid:48)(cid:105)
(cid:111)
(cid:110)
(cid:82) exp
g(z(cid:48)) − w1 (cid:107)x(cid:48)(cid:107)2 − w2 (cid:107)y(cid:48)(cid:107)2 + 2w1(cid:104)x, x(cid:48)(cid:105) + 2w2(cid:104)y, y(cid:48)(cid:105)

dQ(z(cid:48))

dQ(z(cid:48))

.

(24)

Here ˜z(cid:48) = (2w1x(cid:48); 2w2y(cid:48)) and zλ = (cid:81)d

i=1 zλi

i

. By Lemma 11 below, it holds that

(cid:12)
(cid:12)Dα ¯f (z)(cid:12)

(cid:12) ≤ Ck,d,w

(cid:40)

σk(1 + σk)
σk(σ (cid:107)z(cid:107) + (cid:112)σ (cid:107)z(cid:107))k

if (cid:107)z(cid:107) ≤
if (cid:107)z(cid:107) >

√
√

dσ
dσ,

which proves the claim.

Lemma 11. Recall Mλ in (24). Under Assumption 3, for |λ| > 0, we have

|Mλ| ≤ C|λ|,d,w

(cid:40)

σ|λ|(σ + σ2)|λ|
σ|λ|(σ (cid:107)z(cid:107) + (cid:112)σ (cid:107)z(cid:107))|λ|

if (cid:107)z(cid:107) ≤
if (cid:107)z(cid:107) >

√
√

dσ
dσ

.

Proof. We ﬁrst bound the denominator. By the optimality conditions (15), it holds that

(cid:18)(cid:90)

(cid:111)
(cid:110)
g(z(cid:48)) − w1 (cid:107)x(cid:48)(cid:107)2 − w2 (cid:107)y(cid:48)(cid:107)2 + 2w1(cid:104)x, x(cid:48)(cid:105) + 2w2(cid:104)y, y(cid:48)(cid:105)

exp

dQ(z(cid:48))

(cid:19)−1

= ef (x,y)−w1(cid:107)x(cid:107)2−w2(cid:107)y(cid:107)2

≤ ew1(2d1σ2+2

√

2d1σ(cid:107)x(cid:107))+w2(2d2σ2+2

√

2d2σ(cid:107)y(cid:107)),

where the last inequality follows from Proposition 9. To bound the numerator, we use the truncation
technique. Let A := {(x(cid:48), y(cid:48)) : (cid:107)2w1x(cid:48)(cid:107) ≤ K, (cid:107)2w2y(cid:48)(cid:107) ≤ K} for some constant K to be determined later. On
the set A, it is clear that (˜z(cid:48))λ ≤ (cid:107)˜z(cid:48)(cid:107)|λ| ≤ K |λ|, and thus

(cid:82)

(cid:111)
(cid:110)
g(z(cid:48)) − w1 (cid:107)x(cid:48)(cid:107)2 − w2 (cid:107)y(cid:48)(cid:107)2 + 2w1(cid:104)x, x(cid:48)(cid:105) + 2w2(cid:104)y, y(cid:48)(cid:105)
A(˜z(cid:48))λ exp
(cid:111)
(cid:110)
(cid:82) exp
g(z(cid:48)) − w1 (cid:107)x(cid:48)(cid:107)2 − w2 (cid:107)y(cid:48)(cid:107)2 + 2w1(cid:104)x, x(cid:48)(cid:105) + 2w2(cid:104)y, y(cid:48)(cid:105)

dQ(z(cid:48))

dQ(z(cid:48))

≤ K |λ|.

21

On the set Ac, we proceed as follows. According to Proposition 9, we have

eg(x(cid:48),y(cid:48))−w1(cid:107)x(cid:48)(cid:107)2

−w2(cid:107)y(cid:48)(cid:107)2

√
≤ ew1(2d1σ2+2

2d1σ(cid:107)x(cid:48)(cid:107))+w2(2d2σ2+2

√

2d2σ(cid:107)y(cid:48)(cid:107)),

which yields

(cid:90)

Ac

(˜z(cid:48))λ exp

(cid:111)
(cid:110)
g(z(cid:48)) − w1 (cid:107)x(cid:48)(cid:107)2 − w2 (cid:107)y(cid:48)(cid:107)2 + 2w1(cid:104)x, x(cid:48)(cid:105) + 2w2(cid:104)y, y(cid:48)(cid:105)

dQ(z(cid:48))

≤ e2(w1d1+w2d2)σ2 (cid:20)(cid:90)

(˜z(cid:48))2λdQ(z(cid:48))

Ac

(cid:90)

Ac

√
e2w1(cid:107)x(cid:48)(cid:107)((cid:107)x(cid:107)+

2d1σ)+2w2(cid:107)y(cid:48)(cid:107)((cid:107)y(cid:107)+

√

2d2σ)dQ(z(cid:48))

(cid:21)1/2

.

For any z(cid:48) ∈ Ac, we have either (cid:107)2w1x(cid:48)(cid:107) > K or (cid:107)2w2y(cid:48)(cid:107) > K. If the former is true, then

(cid:90)

Ac

(˜z(cid:48))2λdQ(z(cid:48)) ≤

− K2
16w2

1 d1σ2 e

e

(cid:90)

Ac

(cid:107)2w1x(cid:48)(cid:107)2
16w2

1 d1σ2 (˜z(cid:48))2λdQ(z(cid:48)) ≤ C|λ|,d,we− K2

16w2dσ2 σ2|λ|,

where w = max{w1, w2}. The same bound holds if the latter is true. Furthermore, by the Cauchy-Schwartz
inequality and Lemma 18 in Appendix D, we have

√
e2w1(cid:107)x(cid:48)(cid:107)((cid:107)x(cid:107)+

2d1σ)+2w2(cid:107)y(cid:48)(cid:107)((cid:107)y(cid:107)+

√

(cid:90)

Ac

Putting all together, we get

2d2σ)dQ(z(cid:48)) ≤ e4w2

1d1σ2((cid:107)x(cid:107)+

√

2d1σ)2+4w2

2d2σ2((cid:107)y(cid:107)+

√

2d2σ)2

(cid:82)

(cid:111)
(cid:110)
g(z(cid:48)) − w1 (cid:107)x(cid:48)(cid:107)2 − w2 (cid:107)y(cid:48)(cid:107)2 + 2w1(cid:104)x, x(cid:48)(cid:105) + 2w2(cid:104)y, y(cid:48)(cid:105)

Ac(˜z(cid:48))λ exp
(cid:111)
(cid:110)
(cid:82) exp
g(z(cid:48)) − w1 (cid:107)x(cid:48)(cid:107)2 − w2 (cid:107)y(cid:48)(cid:107)2 + 2w1(cid:104)x, x(cid:48)(cid:105) + 2w2(cid:104)y, y(cid:48)(cid:105)
≤ C|λ|,d,we− K2
≤ C|λ|,d,we− K2

32w2dσ2 e2w2dσ2[((cid:107)x(cid:107)+

32w2dσ2 e2w2

2dσ)2]σ|λ|

1d1σ2((cid:107)x(cid:107)+

2d2σ2((cid:107)y(cid:107)+

2dσ)2+((cid:107)y(cid:107)+

2d1σ)2+2w2

2d2σ)2

σ|λ|

√

√

√

√

dQ(z(cid:48))

dQ(z(cid:48))

√

√

√

When (cid:107)z(cid:107) ≤
2dσ and (cid:107)y(cid:107) ≤
for some suﬃciently large constant C|λ|,d,w, then we have

dσ, it holds that (cid:107)x(cid:107) ≤

2dσ. Hence, if we choose K 2 = C|λ|,d,w(σ4 + σ6)

When (cid:107)z(cid:107) >

√

dσ, if we choose K 2 = C|λ|,d,w(σ4 (cid:107)z(cid:107)2 + σ3 (cid:107)z(cid:107)), then we have

|Mλ| ≤ C|λ|,d,wσ|λ|(σ + σ2)|λ|.

|Mλ| ≤ C|λ|,d,wσ|λ| (cid:16)

σ (cid:107)z(cid:107) + (cid:112)σ (cid:107)z(cid:107)

(cid:17)|λ|

.

When P and Q have bounded support, we can further show that the Schrödinger potentials can be chosen

to be bounded.

Proposition 12. Assume that P and Q are supported on a bounded domain of radius D. Then there
exist Schrödinger potentials (f, g) such that 1) the optimality conditions (15) hold for all x, y ∈ Rd and 2)
(cid:107)f (cid:107)∞ ≤ 8wD2 and (cid:107)g(cid:107)∞ ≤ 8wD2.

Proof. Let (f, g) the Schrödinger potentials deﬁned in (21). By the proof of Proposition 9, they satisfy (15)
everywhere. Moreover, we have

f (z) ≤ w1 EQX (cid:107)x − X (cid:48)(cid:107)2 + w2 EQY (cid:107)y − Y (cid:48)(cid:107)2 ≤ 8wD2

and g similarly.

22

B.2 Controlling the Empirical Process and the U-Process
We then upper bound the L1 loss E |Tn(X, Y ) − T (X, Y )| by empirical processes and U-processes.

Proposition 13 (Corollary 2 (Mena and Weed, 2019)). Let P, Q, P (cid:48), Q(cid:48) ∈ M1(Rd) be subG(σ2). Then we
have

|S(P (cid:48), Q(cid:48)) − S(P, Q)| ≤ sup
f ∈Fσ

(cid:90)

(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)
f (dP (cid:48) − dP )
(cid:12)
(cid:12)

(cid:90)

(cid:12)
(cid:12)
(cid:12)
(cid:12)

+ sup
g∈Fσ

g(dQ(cid:48) − dQ)

(cid:12)
(cid:12)
(cid:12)
(cid:12)

,

where Fσ is deﬁned in Deﬁnition 4.

To simply the function class Fσ, we show in Lemma 22 in Appendix D that (1 + σ3s)−1Fσ ⊂ F s for F s

deﬁned below. Consequently, we can separate the sub-Gaussian parameter σ from the function class Fσ.

Deﬁnition 5. For any s ≥ 2, d ∈ N+, and w = (w1, w2) ∈ R2
satisfying

+, let F s := F s,d,w be the set of functions

|f (z)| ≤ Cs,d,w(1 + (cid:107)z(cid:107)2)
|Dαf (z)| ≤ Cs,d,w(1 + (cid:107)z(cid:107)|α|),

∀1 ≤ |α| ≤ s,

where Cs,d,w is a constant depending on s, d, and w.

In order to handle the U-process, we also need a variant function class of F s which we also deﬁne below.

Deﬁnition 6. For any σ ∈ R+, s ≥ 2, d ∈ N+, and w = (w1, w2) ∈ R2
functions satisfying

+, let F s

σ := F s,d,w

σ

be the set of

|f (z)| ≤ Cs,d,w(1 + max{(cid:107)z(cid:107)2 , σ2})
|Dαf (z)| ≤ Cs,d,w(1 + max{(cid:107)z(cid:107)|α| , σ|α|}),

∀1 ≤ |α| ≤ s,

where Cs,d,w is a constant depending on s, d, and w.

Let us control the complexity of F s and F s

σ, which is achieved by the following covering number bound.

Proposition 14. Let P ∈ M1(Rd) be subG(σ2). Let {Zi}n
There exists a random variable L ≥ 1 depending on the sample {Zi}n

i=1

i.i.d.∼ P and ˆPn be the empirical measure.

i=1 with E[L] ≤ 2 such that

log N (τ, F s, L2( ˆPn)) ≤ Cs,d,wτ −d/sLd/2s(1 + σ2d) and max
f ∈F s

(cid:107)f (cid:107)2

L2( ˆPn) ≤ Cs,d,w(1 + Lσ4).

Moreover, the same bounds hold for F s
σ.

≥ 1. By the sub-Gaussianity of P , we have E[L] ≤
Proof of Proposition 14. Deﬁne L := 1
n
2. In order to apply (van der Vaart and Wellner, 1996, Corollary 2.7.4), we partition Rd into ∪j≥1Bj where
B1 := [−σ, σ]d and Bj := [−jσ, jσ]d\[−(j − 1)σ, (j − 1)σ]d for j ≥ 2. Since Bj is not convex for j ≥ 2, we
further partition it into disjoint hypercubes {Bj,k}2d

i=1 e(cid:107)Zi(cid:107)2/2dσ2

k=1, e.g.,

(cid:80)n

Take any j ≥ 2 and k ∈ [2d]. Firstly, it holds that

Bj,1 = [(j − 1)σ, jσ] × [−jσ, jσ]d−1.

λ{x : d(x, Bj,k) ≤ 1} ≤ (σ + 2)(2jσ + 2)d−1 ≤ Cd(1 + jdσd),

where λ is the Lebesgue measure. Secondly, the mass that ˆPn assigns to Bj,k can be bounded as follows:

ˆPn(Z ∈ Bj,k) ≤ ˆPn

(cid:16)

(cid:107)Z(cid:107)2 > dσ2(j − 1)2(cid:17)

≤ ˆPn

e(cid:107)Z(cid:107)2/2dσ2 (cid:105)
(cid:104)

e−(j−1)2/2 = Le−(j−1)2/2.

(25)

23

Finally, we prove that F s ⊂ Cs
functions satisfying

M (Bj,k) with M = Cs,d,w(1 + jsσs), where Cs

M (Bj,k) is the set of continuous

(cid:107)f (cid:107)s := max
|α|≤s

sup
z∈Bj,k

|Dαf (z)| + max
|α|=s

sup
z,w∈Bj,k

|Dαf (z) − Dαf (w)| ≤ M.

In fact, for any f ∈ F s, we have

max
|α|≤s

sup
z∈Bj,k

|Dαf (z)| ≤ Cs,d,w sup
z∈Bj,k

(1 + (cid:107)z(cid:107)s) ≤ Cs,d,w(1 + jsσs),

and

max
|α|=s

sup
z,w∈Bj,k

|Dαf (z) − Dαf (w)| ≤ 2 max
|α|=s

sup
z∈Bj,k

|Dαf (z)| ≤ Cs,d(1 + jsσs).

Note that the same argument holds for any f ∈ F s
Now, applying (van der Vaart and Wellner, 1996, Corollary 2.7.4) with r = 2 and V = d/s leads to

σ since we can simply replace 1+(cid:107)z(cid:107)s by 1+max{(cid:107)z(cid:107)s , σs}.

log N (τ, F s, L2( ˆPn)) ≤ Cs,d,wτ −d/sLd/2s

1 +



∞
(cid:88)

2d
(cid:88)

(1 + jdσd)

2s

d+2s (1 + jsσs)

j=2

k=1



≤ Cs,d,wτ −d/sLd/2s(1 + σ2d)

2d

∞
(cid:88)

j=1

j

4ds

d+2s e− d(j−1)2

d+2s



d+2s
2s



2d

d+2s e− d(j−1)2

d+2s



d+2s
2s



≤ Cs,d,wτ −d/sLd/2s(1 + σ2d),

by the summability.

To verify the second inequality, we obtain

max
f ∈F s

(cid:107)f (cid:107)2

L2( ˆPn) = max
f ∈F s

ˆPn[|f (Z)|2] ≤ Cs,d,w ˆPn[(1 + (cid:107)Z(cid:107)4)].

(26)

Note that (cid:107)Z(cid:107)4 ≤ Cde(cid:107)Z(cid:107)2/2dσ2

σ4. It follows that ˆPn[(cid:107)Z(cid:107)4] ≤ CdLσ4, and thus

max
f ∈F s

(cid:107)f (cid:107)2

L2( ˆPn) ≤ Cs,d,w(1 + Lσ4).

Again, the same argument hold for F s

σ by replacing (cid:107)Z(cid:107)4 with max{(cid:107)Z(cid:107)4 , σ4}.

With this covering number bound at hand, we can control the empirical process by the metric entropy.

Proposition 15. Let P ∈ M1(Rd) be subG(σ2). Let {Zi}n
Then,

i=1

i.i.d.∼ P and ˆPn be the empirical measure.

E

(cid:13)
ˆPn − P
(cid:13)
(cid:13)

(cid:13)
2
(cid:13)
(cid:13)

F s

≤ Cs,d,w(1 + σ2d+4)

1
n

,

for all s > d/2.

Moreover, the same bound holds for F s
σ.

Proof. Deﬁne the symmetrized version of

(cid:13)
ˆPn − P
(cid:13)
(cid:13)

(cid:13)
(cid:13)
(cid:13)F s
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:13)
ˆSn
(cid:13)
(cid:13)

(cid:13)
(cid:13)
(cid:13)F s

:= sup
f ∈F s

by

1
n

n
(cid:88)

i=1

(cid:12)
(cid:12)
(cid:12)
εif (Zi)
(cid:12)
(cid:12)

,

(27)

where {εi}n
(Wainwright, 2019, Proposition 4.11), it holds that

i=1 are i.i.d. Rademacher random variables that are independent with {Zi}n

i=1. According to

E

(cid:13)
ˆPn − P
(cid:13)
(cid:13)

(cid:13)
2
(cid:13)
(cid:13)

F s

≤ 4 E

(cid:13)
ˆSn
(cid:13)
(cid:13)

(cid:13)
2
(cid:13)
(cid:13)

F s

.

24

Conditioning on {Zi}n
i=1 εif (Zi) is a linear combination of indepen-
dent Rademacher random variables. Hence, Z(f ) is a sub-Gaussian process (see Deﬁnition 7) with respect
to

i=1, the random variable Z(f ) := 1√
n

(cid:80)n

(cid:107)f − g(cid:107)L2( ˆPn) =

(cid:118)
(cid:117)
(cid:117)
(cid:116)

1
n

n
(cid:88)

[f (Zi) − g(Zi)]2.

i=1

It then follows from Proposition 16 below that

Eε sup
f ∈F s

|Z(f )|2 ≤ C

(cid:32)(cid:90) 2 maxf ∈F s (cid:107)f (cid:107)L2( ˆPn)

(cid:113)

log N (τ, F s, L2( ˆPn))dτ

(cid:33)2

0

(cid:32)(cid:90) Cs,d

√

1+Lσ4

τ −d/2sLd/4s(cid:112)

1 + σ2ddτ

(cid:33)2

≤ Cs,d,w

,

by Proposition 14

0

= Cs,d,w(1 + σ2d)Ld/2s(1 + Lσ4)1−d/2s,
≤ Cs,d,w(1 + σ2d+4)L,

by L ≥ 1.

by s > d/2

Note that E

(cid:13)
ˆSn
(cid:13)
(cid:13)

(cid:13)
2
(cid:13)
(cid:13)

F s

= 1
n

E supf ∈F s |Z(f )|2. Consequently, we have

The same argument holds for F s

E

(cid:13)
2
(cid:13)
(cid:13)

(cid:13)
ˆPn − P
(cid:13)
(cid:13)

1
n
σ since Proposition 14 holds true for F s
σ.

≤ Cs,d,w(1 + σ2d+4)

F s

.

(28)

The following proposition controls the L2 norm of the supremum of a sub-Gaussian process. It can be

obtained from Giné and Nickl (2015, Exercise 2.3.1). We give its proof here for self-completeness.

Proposition 16. Let {Z(θ)}θ∈Θ be a sub-Gaussian process with respect to a metric ρ in Θ such that
(cid:82) ∞
0

(cid:112)log N (τ, Θ, ρ)dτ < ∞. Then it holds that, for any separable version of Z,

(cid:13)
(cid:13)
(cid:13)
(cid:13)

sup
θ∈Θ

|Z(θ)|

(cid:13)
(cid:13)
(cid:13)
(cid:13)L2

≤ (cid:107)Z(θ0)(cid:107)L2 + C

(cid:90) D

0

(cid:112)log N (τ, Θ, ρ)dτ,

where θ0 ∈ Θ is arbitrary and D is the ρ-diameter of Θ.

Proof. Due to the separability, it suﬃces to prove

(cid:13)
(cid:13)
(cid:13)
(cid:13)

sup
θ∈Θ(cid:48)

|Z(θ)|

(cid:13)
(cid:13)
(cid:13)
(cid:13)L2

≤ (cid:107)Z(θ0)(cid:107)L2 + C

(cid:90) D

0

(cid:112)log N (τ, Θ, ρ)dτ

(29)

(30)

for any ﬁnite Θ(cid:48) ⊂ Θ. When the diameter D = 0, the claim holds trivially and thus we only need to focus
on the case when |Θ(cid:48)| ≥ 2. By considering (Z(θ) − Z(θ0))/(1 + δ)D and ρ/(1 + δ)D instead of Z(θ) and ρ
for some any small δ > 0, we may assume that Z(θ0) = 0 and D ∈ (1/2, 1). Our proof relies on the classical
chaining argument.

Step 1. Construct a chain of projections. Let r1 ∈ N be such that, for any θ ∈ Θ, the ball B(θ, 2−r1 )
centered at θ of radius 2−r1 contains at most 1 element in Θ(cid:48). Denote Θr1 := Θ(cid:48) and Θ0 := {θ0}. For each
1 ≤ r < r1, we take a 2−r covering of Θ and let Θr be the collection of these centers. By deﬁnition, we get
|Θr| ≤ N (2−r, Θ, ρ) for all 0 ≤ r ≤ r1. For each θ ∈ Θ(cid:48), we construct a chain (πr1 (θ), πr1−1(θ), . . . , π0(θ))
such that πr(θ) ∈ Θr as follows. For r = r1, we let πr(θ) = θ. For any 0 ≤ r < r1, we deﬁne πr(θ) to be
a point in Θr for which the ball B(πr(θ), 2−r) contains πr+1(θ). Note that there may be multiple points
satisfying this requirement, but we select the same one for θ and θ(cid:48) as long as πr+1(θ) = πr+1(θ(cid:48)).

Step 2. Telescoping. By the triangle inequality, we have

(cid:13)
(cid:13)
(cid:13)
(cid:13)

max
θ∈Θ(cid:48)

|Z(θ)|

(cid:13)
(cid:13)
(cid:13)
(cid:13)L2

=

(cid:13)
(cid:13)
(cid:13)
(cid:13)

max
θ∈Θ(cid:48)

|Z(πr1(θ)) − Z(π0(θ))|

(cid:13)
(cid:13)
(cid:13)
(cid:13)L2

≤

r1(cid:88)

r=1

(cid:13)
(cid:13)
(cid:13)
(cid:13)

max
θ∈Θ(cid:48)

|Z(πr(θ)) − Z(πr−1(θ))|

(cid:13)
(cid:13)
(cid:13)
(cid:13)L2

.

25

Note that

|{(πr(θ), πr−1(θ)) : θ ∈ Θ(cid:48)}| = |{πr(θ) : θ ∈ Θ(cid:48)}| ≤ |Θr| ≤ N (2−r, Θ, ρ).

According to (Giné and Nickl, 2015, Lemma 2.3.3), we obtain

(cid:13)
(cid:13)
(cid:13)
(cid:13)

max
θ∈Θ(cid:48)

|Z(πr(θ)) − Z(πr−1(θ))|

(cid:13)
(cid:13)
(cid:13)
(cid:13)L2

≤ C(cid:112)log N (2−r, Θ, ρ) max

θ∈Θ(cid:48)

(cid:107)Z(πr(θ)) − Z(πr−1(θ))(cid:107)

≤ C2−r+1(cid:112)log N (2−r, Θ, ρ).

Consequently, it holds that

(cid:13)
(cid:13)
(cid:13)
(cid:13)

max
θ∈Θ(cid:48)

|Z(θ)|

(cid:13)
(cid:13)
(cid:13)
(cid:13)L2

≤ C

r1(cid:88)

r=1

2−r+1(cid:112)log N (2−r, Θ, ρ) ≤ C

(cid:90) 1

0

(cid:112)log N (τ, Θ, ρ)dτ,

which completes the proof.

B.3 Proofs of Main Results

We now prove the main consistency results in Section 3. For simplicity of the notation, we focus on the
quadratic cost function, i.e., w1 = w2 = 1, and drop the dependency on w (e.g., we write Cs,d = Cs,d,w.
The proofs can be adapted to weighted quadratic costs with minor modiﬁcations. Let PX ∈ M1(Rd1) and
PY ∈ M1(Rd2) with d := d1 + d2. Suppose that {(Xi, Yi)}n
i=1 is an i.i.d. sample from some joint distribution
PXY with marginals PX and PY , where PXY may or may not equal PX ⊗ PY . Let ˆPn and ˆQn be the
empirical measures of {Xi}n

i=1, respectively.

i=1 and {Yi}n

Proof of Proposition 4. Step 1. Decoupling. Due to the degeneracy, it suﬃces to bound



E

(cid:13)
ˆPX ⊗ ˆPY
(cid:13)
(cid:13)

(cid:13)
2
(cid:13)
(cid:13)

F

= E


sup

f ∈F

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

1
n2

n
(cid:88)

i,j=1

f (Xi, Yj)

2


 .

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

(31)

We prove in the following that it boils down to control (31) under the product measure PX ⊗ PY . When
PXY = PX ⊗ PY , the claim holds trivially. When PXY (cid:54)= PX ⊗ PY , we use the decoupling technique (Peña
and Giné, 1999). Note that, by the Cauchy-Schwarz inequality,



E


sup

f ∈F

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

1
n2

n
(cid:88)

i,j=1

f (Xi, Yj)

2
 ≤ C E





sup

f ∈F

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

1
n2

n
(cid:88)

i(cid:54)=j

f (Xi, Yj)

2

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

1
n2

n
(cid:88)

i=1

+ sup
f ∈F

f (Xi, Yi)

2
(cid:12)
(cid:12)
(cid:12)

 .
(cid:12)
(cid:12)

Note that the second term on the RHS is a lower order term and can be taken care of by Proposition 15.
Hence, it suﬃces to upper bound the ﬁrst term. Let {εi}n
i=1 be i.i.d. Rademacher random variables and
i=1 be an independent copy of {(Xi, Yi)}n
{(X (cid:48)

i=1. Deﬁne

i )}n

i, Y (cid:48)

Ai :=

(cid:40)

Xi
X (cid:48)
i

if εi = 1
if εi = −1

and Bi :=

(cid:40)

Y (cid:48)
i
Yi

if εi = 1
if εi = −1

.

For any functional F : F → R+, let Φ(F ) := supf ∈F F (f )2. For instance, we deﬁne

UX,Y (f ) :=

1
n2

(cid:12)
(cid:12)
(cid:88)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

i(cid:54)=j

(cid:12)
(cid:12)
(cid:12)
f (Xi, Yj)
(cid:12)
(cid:12)
(cid:12)

.

It is clear that Φ is convex and increasing, and the target reads
(cid:12)

(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)

E (cid:2)f (Xi, Yj) + f (X (cid:48)

E [Φ(UX,Y )] = E

1
n2

Φ

(cid:88)

i(cid:54)=j



i, Yj) + f (Xi, Y (cid:48)

j ) + f (X (cid:48)

i, Y (cid:48)







 ,

(cid:12)
(cid:12)
(cid:12)
j ) | Z(cid:3)
(cid:12)
(cid:12)
(cid:12)

26

where Z := {(Xi, Yi)}n

i=1. Since, for any i (cid:54)= j,

f (Xi, Yj) + f (X (cid:48)

i, Yj) + f (Xi, Y (cid:48)

j ) + f (X (cid:48)

i, Y (cid:48)

j ) = 4 E [f (Ai, Bj) | Z, Z (cid:48)] ,

it follows from the convexity and the monotonicity of Φ that

E [Φ(UX,Y )] ≤ E [Φ(4UA,B)] .

Finally, the joint distribution of (X1, . . . , Xn, Y (cid:48)

1 , . . . , Y (cid:48)

n) is the same as (A1, . . . , An, B1, . . . , Bn), so we have

E [Φ(UX,Y )] ≤ E [Φ(4UX,Y (cid:48))] .

Adding back the diagonal terms proves the claim since (Xi, Y (cid:48)

i ) ∼ PX ⊗ PY .

Step 2. Randomization. We work under the measure PXY = PX ⊗ PY . Note that



E


sup

f ∈F

1
n2

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)


n
(cid:88)

i,j=1

(cid:12)
(cid:12)
(cid:12)
f (Xi, Yj)
(cid:12)
(cid:12)
(cid:12)

2




= EY EX


sup

f ∈F

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

1
n2

n
(cid:88)





n
(cid:88)

i=1

j=1

f (Xi, Yj) − EX (cid:48)

(cid:104) n
(cid:88)

j=1

¯f (X (cid:48)

(cid:105)
i, Yj)

2


 ,

(cid:12)

(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)

by (36)

≤ EY EX,X (cid:48)




sup

f ∈F

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

1
n2

n
(cid:88)





n
(cid:88)

i=1

j=1

f (Xi, Yj) −

n
(cid:88)

j=1

¯f (X (cid:48)

i, Yj)

(cid:12)

(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)

2


 ,

by Jensen’s inequality

= EY EX,X (cid:48),ε




sup

f ∈F

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

1
n2

n
(cid:88)

εi





n
(cid:88)

i=1

j=1

¯f (Xi, Yj) −

n
(cid:88)

j=1

¯f (X (cid:48)

i, Yj)

2




(cid:12)

(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)



≤ C E


sup

f ∈F

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

1
n2

n
(cid:88)

n
(cid:88)

i=1

j=1

(cid:12)
(cid:12)
(cid:12)
εif (Xi, Yj)
(cid:12)
(cid:12)
(cid:12)

2


 ,

Repeating above arguments gives

by the Cauchy-Schwarz inequality.



E


sup

f ∈F

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

1
n2

n
(cid:88)

i,j=1

f (Xi, Yj)

2
(cid:12)
(cid:12)
(cid:12)
 ≤ C E

(cid:12)
(cid:12)
(cid:12)




sup

f ∈F



≤ C E


sup

f ∈F

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

1
n2

1
n2

n
(cid:88)

i,j=1

n
(cid:88)

i,j=1

εiε(cid:48)

jf (Xi, Yj)

εiε(cid:48)

jf (Xi, Yj)

2




2


 ,

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

where the last inequality follows from the Cauchy-Schwarz inequality and Jensen’s inequality. Hence, it
suﬃces to bound

A := E sup
f ∈F

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

1
n2

n
(cid:88)

i,j=1

εiε(cid:48)

jf (Xi, Yj)

2

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

.

Step 3. Metric entropy. Deﬁne the process Z(f ) := 1
n3/2

that it is a sub-Gaussian process with respect to

(cid:80)n

i,j=1 εiε(cid:48)

jf (Xi, Yj) for any f ∈ F. We claim

(cid:107)f − g(cid:107)L2( ˆPn⊗ ˆQn) =

(cid:118)
(cid:117)
(cid:117)
(cid:116)

1
n2

n
(cid:88)

[f (Xi, Yj) − g(Xi, Yj)]2.

(32)

i,j=1

27

To prove it, let us control the moment generating function of the increment Z(f ) − Z(g). Denote ai :=
(cid:80)n

j[f (Xi, Yj) − g(Xi, Yj)]. Conditioning on {Xi, Yi, ε(cid:48)

i=1,

i}n

j=1 ε(cid:48)

Z(f ) − Z(g) =

1
n3/2

n
(cid:88)

i=1

aiεi

is a linear combination of independent Rademacher random variables. Consequently,

Eε exp {λ[Z(f ) − Z(g)]} ≤ exp

(cid:26) λ2 (cid:80)n
i=1 a2
i
2n3

(cid:27)

.

(33)

Note that, by the Cauchy-Schwarz inequality,





n
(cid:88)

(ε(cid:48)

j)2









n
(cid:88)

[f (Xi, Yj) − g(Xi, Yj)]2



 = n





n
(cid:88)

[f (Xi, Yj) − g(Xi, Yj)]2



 .

j=1

j=1

j=1

a2
i ≤

This yields that

Eε exp {λ[Z(f ) − Z(g)]} ≤ exp

(cid:40) λ2 (cid:80)n

i,j=1[f (Xi, Yj) − g(Xi, Yj)]2

(cid:41)

2n2

= exp

(cid:40) λ2 (cid:107)f − g(cid:107)2
L2( ˆPn⊗ ˆQn)
2

(cid:41)

, (34)

and thus the claim follows. Therefore, the conclusion in Proposition 4 holds true due to Proposition 16.

Proof of Proposition 5. The proof of the ﬁrst part is similar to Proposition 14. Deﬁne L1 := ˆPX [e(cid:107)X(cid:107)2/2dσ2
]
and L2 := ˆPY [e(cid:107)Y (cid:107)2/2dσ2
]. It is clear that L1 ≥ 1 and L2 ≥ 1. Moreover, it follows from the sub-Gaussian
assumption that E[L1] ≤ 2 and E[L2] ≤ 2. There are two places in the proof of Proposition 14 where the
measure is involved. The ﬁrst place is (25), where we replace it by

( ˆPX ⊗ ˆPY ){(X, Y ) ∈ Bj,k} ≤ ( ˆPX ⊗ ˆPY )

(cid:107)X(cid:107)2 + (cid:107)Y (cid:107)2 > dσ2(j − 1)2(cid:111)
(cid:110)

(cid:34)

(cid:32)

≤ ( ˆPX ⊗ ˆPY )

exp

(cid:33)(cid:35)

(cid:107)X(cid:107)2 + (cid:107)Y (cid:107)2
4dσ2

= L1L2e−(j−1)2/4.

e−(j−1)2/4,

by Chernoﬀ bound

The second place is (26), where we replace it by

max
f ∈F s

(cid:107)f (cid:107)2

L2( ˆPX ⊗ ˆPY ) = max
f ∈F s

( ˆPX ⊗ ˆPY )[|f (X, Y )|2] ≤ Cs,d( ˆPX ⊗ ˆPY )[1 + (cid:107)X(cid:107)4 + (cid:107)Y (cid:107)4].

Note that (cid:107)Z(cid:107)4 ≤ Cde(cid:107)Z(cid:107)2/2dσ2
claim holds true for L := (L1 + L2)/2.

For the second part, we deﬁne θf := EPX ⊗PY [f (X, Y )],

σ4. It follows that ( ˆPX ⊗ ˆPY )[(cid:107)X(cid:107)4 + (cid:107)Y (cid:107)4] ≤ Cd(L1 + L2)σ4. Hence, the

f1,0(X) := EPX ⊗PY [f (X, Y ) | X] and f0,1(Y ) := EPX ⊗PY [f (X, Y ) | Y ]

for each f ∈ F s. As a result, ¯f (x, y) := f (x, y) − f1,0(x) − f0,1(y) + θf satisﬁes

EPX ⊗PY [ ¯f (X, Y ) | X] a.s.= 0 a.s.= EPX ⊗PY [ ¯f (X, Y ) | Y ].

(35)

(36)

28

Note that

E

(cid:13)
ˆPX ⊗ ˆPY − PX ⊗ PY
(cid:13)
(cid:13)


(cid:13)
2
(cid:13)
(cid:13)

F s

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

1
n2

n
(cid:88)

i,j=1

(cid:0)f (Xi, Yj) − θf

(cid:1)

2




(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

= E


 sup

f ∈F s



≤ C E


 sup

f ∈F s



≤ C E


 sup

f ∈F s

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

1
n2

1
n2

n
(cid:88)

i,j=1

2

(cid:12)
(cid:12)
(cid:12)
¯f (Xi, Yj)
(cid:12)
(cid:12)
(cid:12)

n
(cid:88)

i,j=1

2

(cid:12)
(cid:12)
(cid:12)
¯f (Xi, Yj)
(cid:12)
(cid:12)
(cid:12)

+ sup
f ∈F s

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

1
n

n
(cid:88)

i=1

f1,0(Xi) − θf

2

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

+ sup
f ∈F s

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

1
n

n
(cid:88)

i=1

f0,1(Yi) − θf

2



(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

+

(cid:13)
ˆPX − PX
(cid:13)
(cid:13)

(cid:13)
2
(cid:13)
(cid:13)

F s
σ

+

(cid:13)
ˆPY − PY
(cid:13)
(cid:13)

(cid:13)
2
(cid:13)
(cid:13)

F s
σ




 ,

by Lemma 23.

(37)

Since the last two terms above can be controlled by Proposition 15, it remains to consider the ﬁrst term.
Analogous to the proof of Proposition 15, we obtain, by Proposition 4 and the ﬁrst part, that

Therefore, by (37), we have



E


 sup

f ∈F s

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

1
n2

n
(cid:88)

i,j=1

(cid:12)
(cid:12)
(cid:12)
¯f (Xi, Yj)
(cid:12)
(cid:12)
(cid:12)

2
 ≤ Cs,d(1 + σ2d+4)


1
n

.

E

(cid:13)
(cid:13)
2
ˆPn ⊗ ˆQn − P ⊗ Q
(cid:13)
(cid:13)
(cid:13)
(cid:13)

F s

≤ Cs,d(1 + σ2d+4)

1
n

.

Now we are ready to prove Theorem 3.

Proof of Theorem 3. We prove the statement for ε = 1 and write S := S1. The result for general ε > 0
follows immediately from Lemma 24. By the triangle inequality, it holds that

|Tn(X, Y ) − T (X, Y )| ≤

(cid:12)
(cid:12)
(cid:12)S( ˆPXY , ˆPX ⊗ ˆPY ) − S(PXY , PX ⊗ PY )
(cid:12)
(cid:12)
(cid:12) +

(cid:12)
(cid:12)S( ˆPXY , ˆPXY ) − S(PXY , PXY )
(cid:12)

(cid:12)
(cid:12)
(cid:12)

1
2

+

1
2

(cid:12)
(cid:12)S( ˆPX ⊗ ˆPY , ˆPX ⊗ ˆPY ) − S(PX ⊗ PY , PX ⊗ PY )
(cid:12)

We begin with deriving the bound for the ﬁrst term

A :=

(cid:12)
(cid:12)
(cid:12)S( ˆPXY , ˆPX ⊗ ˆPY ) − S(PXY , PX ⊗ PY )
(cid:12)
(cid:12)
(cid:12) .

(cid:12)
(cid:12)
(cid:12) .

(38)

(39)

Step 1. Upper bound via empirical processes. According to Lemma 19 and Lemma 20, the joint distri-

bution PXY is subG(2σ2), and thus there exist a zero-measure set SPXY ⊂ Ω and a random variable σ2
PXY
such that ˆPXY (ω) and PXY are subG(σ2
. Similarly, by Lemma 21, there exist
such that ˆPX (ω) ⊗ ˆPY (ω) and PX ⊗ PY are
a zero-measure set SPX ,PY ⊂ Ω and a random variable σ2
subG(σ2
}. It follows
PXY
that ˆPXY (ω), ˆPX (ω) ⊗ ˆPY (ω), PXY , and PX ⊗ PY are subG(¯σ2(ω)) for every ω ∈ S. Now, by Proposition 13,

P,Q(ω)) for every ω ∈ Sc

(ω)) for every ω ∈ Sc

and ¯σ2 := max{σ2

. Take S := Sc

PX ,PY
∩ Sc

PX ,PY

PX ,PY

PX ,PY

, σ2

PXY

PXY

PXY

(cid:12)
(cid:12)
(cid:12)S( ˆPXY (ω), ˆPX (ω) ⊗ ˆPY (ω)) − S(PXY , PX ⊗ PY )
(cid:12)
(cid:12)
(cid:12)

≤ sup

f ∈F¯σ(ω)

(cid:90)

(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)
f (d ˆPXY (ω) − dPXY )
(cid:12)
(cid:12)

+ sup

g∈F¯σ(ω)

(cid:90)

(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)
g(d ˆPX (ω) ⊗ ˆPY (ω) − dPX ⊗ PY )
(cid:12)
(cid:12)

,

∀ω ∈ S.

29

Note that P(S) = P(Sc

PXY

∩ Sc

PX ,PY

) = 1. This implies, almost surely,

A ≤ sup
f ∈F¯σ

(cid:90)

(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)
f (d ˆPXY − dPXY )
(cid:12)
(cid:12)

+ sup
g∈F¯σ

(cid:90)

(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)
g(d ˆPX ⊗ ˆPY − dPX ⊗ PY )
(cid:12)
(cid:12)

.

(40)

According to Lemma 22, we have

E[A] ≤ E

(cid:104)

(1 + ¯σ3s)

(cid:13)
ˆPXY − PXY
(cid:13)
(cid:13)

≤ (cid:112)E[(1 + ¯σ3s)2]

(cid:34)(cid:114)

E

(cid:105)

(cid:13)
(cid:13)
(cid:13)F s

+ E

(cid:104)
(1 + ¯σ3s)

(cid:13)
ˆPX ⊗ ˆPY − PX ⊗ PY
(cid:13)
(cid:13)

(cid:13)
ˆPXY − PXY
(cid:13)
(cid:13)

(cid:13)
2
(cid:13)
(cid:13)

F s

+

(cid:114)

E

(cid:13)
ˆPX ⊗ ˆPY − PX ⊗ PY
(cid:13)
(cid:13)

(cid:105)

(cid:13)
(cid:13)
(cid:13)F s
(cid:35)
(cid:13)
2
(cid:13)
(cid:13)

F s

.

Step 2. Control empirical processes via metric entropy. Let s = (cid:100)d/2(cid:101) + 1. Since the joint probability

PXY is subG(2σ2), it follows from Proposition 15 that

(cid:114)

E

(cid:13)
ˆPXY − PXY
(cid:13)
(cid:13)

(cid:13)
2
(cid:13)
(cid:13)

F s

≤ Cd(1 + σd+2)

1
√
n

.

(41)

The same bound holds for

(cid:114)

E

(cid:13)
ˆPX ⊗ ˆPY − PX ⊗ PY
(cid:13)
(cid:13)

(cid:13)
2
(cid:13)
(cid:13)

F s

by Proposition 4. Note that

E[(1 + ˜σ3s)2] ≤ C(1 + E ˜σ6s) ≤ Cs(1 + E σ6s

PXY

+ E σ6s

PX ,PY

) ≤ Cs(1 + σ6s),

where the last inequality follows from Lemma 19 and Lemma 21. Recall that we have chosen s = (cid:100)d/2(cid:101) + 1.
As a result, E[A] ≤ Cd(1 + σ(cid:100)5d/2(cid:101)+6)n−1/2. A similar argument shows that the same bound hold for the
second and third term in (38). Hence,

E |Tn(X, Y )| ≤ Cd(1 + σ(cid:100)5d/2(cid:101)+6)

1
√
n

.

(42)

C Exponential Tail Bounds

We now prove the exponential tail bound in Section 3. For simplicity of the notation, we focus on the
quadratic cost function, i.e., w1 = w2 = 1, and drop the dependency on w (e.g., we write Cs,d = Cs,d,w.
The proofs can be adapted to weighted quadratic costs with minor modiﬁcations. Let PX ∈ M1(Rd1) and
PY ∈ M1(Rd2) with d := d1 + d2. Suppose that {(Xi, Yi)}n
i=1 is an i.i.d. sample from some joint distribution
PXY with marginals PX and PY , where PXY may or may not equal PX ⊗ PY . Let ˆPn and ˆQn be the
empirical measures of {Xi}n

i=1, respectively.

i=1 and {Yi}n

Proposition 17. For any b-uniformly bounded class of functions F, we have

P

(cid:110)(cid:13)
ˆPX ⊗ ˆPY − PX ⊗ PY
(cid:13)
(cid:13)

(cid:13)
(cid:13)
(cid:13)F

− E

(cid:13)
ˆPX ⊗ ˆPY − PX ⊗ PY
(cid:13)
(cid:13)

(cid:111)

> t

(cid:13)
(cid:13)
(cid:13)F

(cid:18)

≤ exp

−

(cid:19)

nt2
8b2

,

for any t ≥ 0.

Proof. For any function f deﬁned on Rd, we deﬁne ¯f (x, y) = f (x, y) − (PX ⊗ PY )[f ]. As a results, we have
(cid:13)
ˆPX ⊗ ˆPY − PX ⊗ PY
(cid:13)
(cid:13)

(cid:12)
(cid:12)
(cid:12). Consider the function

¯f (Xi, Yj)

= supf ∈F

(cid:80)n

i,j=1

1
n2

(cid:12)
(cid:12)
(cid:12)

(cid:13)
(cid:13)
(cid:13)F

F (z1, . . . , zn) := sup
f ∈F

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

1
n2

n
(cid:88)

i,j=1

(cid:12)
(cid:12)
(cid:12)
¯f (xi, yj)
(cid:12)
(cid:12)
(cid:12)

,

(43)

30

where zi = (xi, yi) ∈ Rd. We claim that F satisﬁes the bounded diﬀerence property required in the McDi-
armid inequality. Since F is permutation invariant, it suﬃces to verify the property for the ﬁrst coordinate.
Let z(cid:48)

1 (cid:54)= z1 and z(cid:48)

i = zi for all i (cid:54)= 1. It holds that
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

¯f (xi, yj)

1, . . . , z(cid:48)

− F (z(cid:48)

n
(cid:88)

i,j=1

1
n2

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

n) ≤

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

1
n2

n
(cid:88)

i,j=1

−

n
(cid:88)

(cid:12)
(cid:12)
(cid:12)
¯f (xi, yj)
(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12) ¯f (xi, yj) − ¯f (x(cid:48)

1
n2

i,j=1

¯f (x(cid:48)

(cid:12)
(cid:12)
(cid:12)
i, y(cid:48)
j)
(cid:12)
(cid:12)
(cid:12)
4b
n

,

i, y(cid:48)

j)(cid:12)
(cid:12) ≤

≤

1
n2

(cid:88)

i=1 or j=1

where the last inequality uses the boundedness of f . Taking the supremum over F yields that F (z1, . . . , zn)−
F (z(cid:48)
1, . . . , z(cid:48)
n)| ≤ 4b/n. Note that
{Zi := (Xi, Yi)}n

n) ≤ 4b/n. By symmetry, it follows that |F (z1, . . . , zn) − F (z(cid:48)

i=1 is an i.i.d. sample. According to the McDiarmid inequality, it holds that

1, . . . , z(cid:48)

P

(cid:110)(cid:13)
ˆPX ⊗ ˆPY − PX ⊗ PY
(cid:13)
(cid:13)

(cid:13)
(cid:13)
(cid:13)F

− E

(cid:13)
ˆPX ⊗ ˆPY − PX ⊗ PY
(cid:13)
(cid:13)

(cid:111)

> t

(cid:13)
(cid:13)
(cid:13)F

(cid:18)

≤ exp

−

(cid:19)

nt2
8b2

,

for any t ≥ 0.

Proof of Theorem 6. We prove the statement for ε = 1 and write S := S1. The result for general ε > 0
follows immediately from Lemma 24. By the bounded support assumption, it holds that PX and PY are both
subG(D2/d). According to the proof of Lemma 19, we have { ˆPX }n≥1, { ˆPY }n≥1, PX , and PY are uniformly
subG(τ 2) for τ 2 := D2e1/2/d ≤ 2D2/d. Moreover, it follows from Lemma 20 that { ˆPXY }n≥1 and PXY are
uniformly subG(2τ 2). As a result, we obtain, by Proposition 13,
(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)S( ˆPXY , ˆPX ⊗ ˆPY ) − S(PXY , PX ⊗ PY )
(cid:12)

A :=

≤ sup
f ∈F2τ

(cid:90)

(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)
f (d ˆPXY − dPXY )
(cid:12)
(cid:12)

+ sup
g∈F2τ

(cid:90)

(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)
g(d ˆPX ⊗ ˆPY − dPX ⊗ PY )
(cid:12)
(cid:12)

.

Fix s = (cid:100)d/2(cid:101) + 1. According to Lemma 22, we have

A ≤ Cd(1 + D3d+12)

(cid:104)(cid:13)
ˆPXY − PXY
(cid:13)
(cid:13)

(cid:13)
(cid:13)
(cid:13)F s

+

(cid:13)
ˆPX ⊗ ˆPY − PX ⊗ PY
(cid:13)
(cid:13)

(cid:105)

,

(cid:13)
(cid:13)
(cid:13)F s

(44)

where we have used τ 3s ≤ CdD3d+12. Proposition 12 shows that we can further constraint the function class
b := {f ∈ F s : (cid:107)f (cid:107)∞ ≤ b} for b = 2D2. Hence, by (Wainwright, 2019, Theorem 4.10), it holds that
F s to F s

P

(cid:26)(cid:13)
ˆPXY − PXY
(cid:13)
(cid:13)

(cid:13)
(cid:13)
(cid:13)F s

b

− E

(cid:13)
ˆPXY − PXY
(cid:13)
(cid:13)

(cid:13)
(cid:13)
(cid:13)F s

b

(cid:27)

(cid:18)

> t

≤ exp

−

(cid:19)

nt2
2b2

,

for any t ≥ 0.

It is clear from Proposition 15 that

E

(cid:13)
ˆPXY − PXY
(cid:13)
(cid:13)

(cid:13)
(cid:13)
(cid:13)F s

b

≤ E

(cid:13)
ˆPXY − PXY
(cid:13)
(cid:13)

(cid:13)
(cid:13)
(cid:13)F s

≤ Cd(1 + D2d+4)

1
√
n

.

Consequently, we get

P

(cid:26)(cid:13)
ˆPXY − PXY
(cid:13)
(cid:13)

(cid:13)
(cid:13)
(cid:13)F s

b

> t + Cd(1 + D2d+4)

(cid:27)

1
√
n

(cid:18)

≤ exp

−

(cid:19)

nt2
2b2

Similarly, using Proposition 4 and Proposition 17, we obtain

P

(cid:26)(cid:13)
ˆPXY − PXY
(cid:13)
(cid:13)

(cid:13)
(cid:13)
(cid:13)F s

b

> t + Cd(1 + D2d+4)

(cid:27)

1
√
n

(cid:18)

≤ exp

−

(cid:19)

nt2
8b2

,

,

for any t ≥ 0.

for any t ≥ 0.

Now it follows from (44) that

(cid:26)

P

A ≥ Cd(1 + D3d+12)

(cid:20)
t + (1 + D2d+4)

(cid:21)(cid:27)

1
√
n

(cid:18)

≤ 2 exp

−

(cid:19)

nt2
8b2

,

for any t ≥ 0.

31

Analogously, we have, for any t ≥ 0

(cid:26)

P

B ≥ Cd(1 + D3d+12)

(cid:26)

P

B(cid:48) ≥ Cd(1 + D3d+12)

(cid:20)
t + (1 + D2d+4)

(cid:20)
t + (1 + D2d+4)

(cid:21)(cid:27)

(cid:21)(cid:27)

1
√
n
1
√
n

(cid:18)

≤ 2 exp

−

(cid:18)

≤ 2 exp

−

(cid:19)

(cid:19)

,

nt2
8b2
nt2
8b2

(cid:12)
(cid:12)S( ˆPXY , ˆPXY ) − S(PXY , PXY )
(cid:12)

where B :=
Since |Tn(X, Y ) − T (X, Y )| ≤ A + B

(cid:12)
(cid:12)
(cid:12) and B(cid:48) :=
2 , it holds that

2 + B(cid:48)

(cid:12)
(cid:12)
(cid:12)S( ˆPX ⊗ ˆPY , ˆPX ⊗ ˆPY ) − S(PX ⊗ PY , PX ⊗ PY )
(cid:12)
(cid:12)
(cid:12).

(cid:26)

P

|Tn(X, Y ) − T (X, Y )| ≥ Cd(1 + D3d+12)

(cid:20)

t + (1 + D2d+4)

(cid:21)(cid:27)

1
√
n

(cid:18)

≤ 6 exp

−

(cid:19)

.

nt2
8b2

(45)

Therefore, we have, with probability at least 1 − δ,

|Tn(X, Y ) − T (X, Y )| ≤ Cd

1 + D2d+2

(cid:32)

(cid:114)

log

(cid:33)

6
δ

D3d+14
√
n

.

D Technical Lemmas

In this section, we give several technical lemmas used to prove the main results. We use C to denote a
constant whose value may change from line to line.

Lemma 18. If P ∈ M1(Rd) is subG(σ2), then, for any k ∈ N+,

EP [(cid:107)Z(cid:107)2k] ≤ (2dσ2)kk!.

Moreover, for any v ∈ Rd, it holds that

EP e(cid:104)v,Z(cid:105) ≤ EP e(cid:107)v(cid:107)(cid:107)Z(cid:107) ≤ 2edσ2(cid:107)v(cid:107)2/2.

(46)

Proof. By Taylor’s expansion, we have

e(cid:107)Z(cid:107)2/2dσ2

− 1 ≥

(cid:107)Z(cid:107)2k
(2dσ2)kk!

.

Taking the expectation on both sides gives

EP [(cid:107)Z(cid:107)2k] ≤ (2dσ2)kk!.

The inequalities (46) follows from the Cauchy-Schwarz inequality and the sub-Gaussianity of P .

Lemma 19. Let P ∈ M1(Rd) be subG(σ2) and ˆPn be the empirical measure. There exist a zero-measure set
SP ⊂ Ω and a random variable σ2
P (ω))
for any ω ∈ Sc

i=1 such that ˆPn(ω) and P are subG(σ2

P depending on the sample {Zi}n

P , and, for any k ∈ N+,

E σ2k

P ≤ 2kkσ2k.

Proof. By the strong law of large numbers, there exists a zero-measure set SP ⊂ Ω such that, for all ω ∈ SP ,

ˆPn(ω)

e(cid:107)Z(cid:107)2/2dσ2 (cid:105)
(cid:104)

→ P

e(cid:107)Z(cid:107)2/2dσ2 (cid:105)
(cid:104)

≤ 2,

as n → ∞.

(47)

32

Let τ 2 := supn
Jensen’s inequality, we obtain, for all ω ∈ SP

ˆPn

(cid:104)

e(cid:107)Z(cid:107)2/2dσ2(cid:105)

. It follows from (47) that τ 2(ω) is ﬁnite for all ω ∈ SP . Since τ 2(ω) ≥ 1, by

ˆPn(ω)

(cid:104)

e(cid:107)Z(cid:107)2/2dσ2τ 2(ω)(cid:105)

≤

(cid:16) ˆPn(ω)

(cid:104)

e(cid:107)Z(cid:107)2/2dσ2(cid:105)(cid:17)1/τ 2(ω)

= (cid:0)τ 2(ω)(cid:1)1/τ 2(ω)

< 2.

As a result, ˆPn(ω) is subG(σ2τ 2(ω)). Moreover, P is also subG(σ2τ 2(ω)) since τ 2(ω) ≥ 1. Applying the
same argument to τ 2
k (ω)). Deﬁne
P := mink≥1 kσ2τ 2
σ2

implies that ˆPn(ω) and P are both subG(kσ2τ 2

e(cid:107)Z(cid:107)2/2kdσ2(cid:105)
(cid:104)

ˆPn

k := supn
k . Then we have, for each k ≥ 1,
(cid:104) ˆPn

kkσ2ke(cid:107)Z(cid:107)2/2dσ2(cid:105)(cid:105)

P ] ≤ EP

EP [σ2k

(cid:104)

= kkσ2k EP [e(cid:107)Z(cid:107)2/2dσ2

] ≤ 2kkσ2k.

The sub-Gaussianity of two marginals implies the sub-Gaussianity of the joint.

Lemma 20. If PX and PY are subG(σ2), then PXY is subG(2σ2) for any PXY ∈ Π(PX , PY ).

Proof. By the Cauchy-Schwarz inequality,

EPXY e(cid:107)Z(cid:107)2/4dσ2

= EPXY [e(cid:107)X(cid:107)2/4dσ2

e(cid:107)Y (cid:107)2/4dσ2

] ≤

(cid:113)

EPX [e(cid:107)X(cid:107)2/2dσ2] EPY [e(cid:107)Y (cid:107)2/2dσ2].

Since PX and PY are subG(σ2), it follows that EPXY e(cid:107)Z(cid:107)2/4dσ2

≤ 2 and thus PXY is subG(2σ2).

The next result is for the uniform sub-Gaussianity of the product of two empirical measures.

Lemma 21. If PX and PY are subG(σ2), then there exist a zero-measure set SPX ,PY ⊂ Ω and a ran-
i=1 such that ˆPX (ω) ⊗ ˆPY (ω) and PX ⊗ PY are
dom variable σ2
subG(σ2

depending on the sample {(Xi, Yi)}n
, and, for any k ∈ N+,

(ω)) for any ω ∈ Sc

PX ,PY

PX ,PY

PX ,PY

Proof. Similar to Lemma 19.

E σ2k

PX ,PY

≤ 2k+1kkσ2k.

The sub-Gaussian processes play an central role in our analysis. We give its deﬁnition here; see, e.g.,

(Wainwright, 2019, Section 5.3).

Deﬁnition 7 (Sub-Gaussian process). Let {Z(θ) : θ ∈ Θ} be a collection of mean-zero random variables.
We call it a sub-Gaussian process with respect to a metric ρ in Θ if

E[eλ(Z(θ)−Z(θ(cid:48)))] ≤ exp (cid:2)λ2ρ2(θ, θ(cid:48))/2(cid:3) .

To facilitate the analysis of Fσ deﬁned in Deﬁnition 4, it is convenient to separate the sub-Gaussian
parameter from the function class by the following lemma. Note that this result is used in (Mena and Weed,
2019) without proof.

Lemma 22. For any σ > 0 and s ≥ 2. we have

1

1+σ3s Fσ ⊂ F s, where F s := F s,d,w is deﬁned in Deﬁnition 5.

Proof. Take any f ∈ Fσ, it suﬃces to show f /(1 + σ3s) ∈ F s. According to Proposition 10, it holds that

|f (z)| − w1 (cid:107)x(cid:107)2 − w2 (cid:107)y(cid:107)2 ≤

Consequently,

(cid:12)
(cid:12)

(cid:12)f (z) − w1 (cid:107)x(cid:107)2 − w2 (cid:107)y(cid:107)2(cid:12)

(cid:12)
(cid:12) ≤ Ck,d,w

(cid:40)

(1 + σ4)
[1 + (1 + σ2) (cid:107)z(cid:107)2]

if (cid:107)z(cid:107) ≤
if (cid:107)z(cid:107) >

√
√

dσ
dσ.

(cid:12)
(cid:12)
(cid:12)
(cid:12)

f (z)
1 + σ3s

(cid:12)
(cid:12)
(cid:12)
(cid:12)

≤ Ck,d,w

(cid:40) 1+σ4
1+σ3s
1+(1+σ2)(cid:107)z(cid:107)2
1+σ3s

if (cid:107)z(cid:107) ≤

if (cid:107)z(cid:107) >

√

√

dσ

dσ.

33

Since s ≥ 2, it is clear that 1+σ4

1+σ3s ≤ C, and thus

1+σ3s ≤ C and 1+σ2
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

f (z)
1 + σ3s

≤ Ck,d,w(1 + (cid:107)z(cid:107)2).

The other inequality can be proved analogously.

Lemma 23. Let P ∈ M1(Rd1 ) and Q ∈ M1(Rd2) be subG(σ2). Denote d := d1 + d2. For any s ≥ 1 and
f ∈ F s, there exist constants Cs,d,w such that f1,0 ∈ F s
σ is deﬁned in Deﬁnition 6,

(cid:90)

f1,0(x) :=

f (x, y)dQ(y) and f0,1(y) :=

f (x, y)dP (x).

σ, where F s
σ and f0,1 ∈ F s
(cid:90)

Proof. We only prove it for f1,0. By Jensen’s inequality, it holds that

(cid:90)

|f1,0(x)| ≤

|f (x, y)| dQ(y) ≤ Cs,d,w

(cid:18)

1 + (cid:107)x(cid:107)2 +

(cid:90)

(cid:19)

(cid:107)y(cid:107)2 dQ(y)

≤ Cs,d,w(1 + max{(cid:107)x(cid:107)2 , σ2}),

where the last inequality follows from Lemma 18. The inequality for |Dαf1,0(x)| can be veriﬁed similarly.

The next lemma suggests that it is enough to consider the case ε = 1 for Sε.

Lemma 24. Let ε > 0. For any P, Q ∈ M1(Rd), it holds that

Sε(P, Q) = εS(P ε, Qε),

where P ε and Qε are the pushforwards of P and Q under the map x (cid:55)→ ε−1/2x, respectively.

Proof. By a change of variable argument.

E Additional Experimental Results

E.1 ETIC-RF on Bilingual Text

We examine the ETIC-RF test on bilingual data discussed in Section 4.2. The feature embeddings are of
high dimension (i.e., 768), which imposes challenges on the random feature approximation. Hence, we ﬁrst
use dimension reduction (PCA) on the English embeddings and French embeddings separately to reduce
the dimension to d(cid:48) (cid:28) 768, and then perform ETIC-RF on the low-dimensional embeddings. Since the
dimension reduction step does not utilize information about the joint distribution PXY , it will not violate
the level consistency of the test. This is also validated in our experimental results, i.e., all the tests have
type I error rate close to 0.05 as expected.

As shown in Figure 5, The number of PCs d(cid:48) has an interesting eﬀect on the power. Intuitively, the
larger d(cid:48) is the less information we lose, and thus the larger power the test has. This can be seen at
the lower right corner where both r1 and r2 are large. However, larger d(cid:48) also means the random feature
approximation is harder, especially when r1 and r2 are small. This is reﬂected at the upper left corner where
the power decreases as d(cid:48) increases. We then investigate the eﬀect of p—the number of random features. As
shown in Figure 6, the power increases with the number of random features. Overall, the random feature
approximation demonstrates similar performance as the exact ETIC with enough random features.

E.2 Adaptive ETIC Test

To ensure that (Tn,ε(X, Y ))ε are roughly on the same scale for diﬀerent values of ε, we use

ψa(α) := 1

(cid:26)

max
ε∈E

¯Tn,ε(X, Y ) > Hn,E (α)

(cid:27)

.

where each Tn,ε(X, Y ) is studentized via resampling (20 permutations) under the null to yield ¯Tn,ε(X, Y ).

34

Figure 5: Heatmaps of power for ETIC-RF with p = 700 random features and d(cid:48) PCs on the partially
dependent sample of the bilingual data. The x-axis is for r1 and y-axis is for r2. The indices from 0 to 11
correspond to equally spaced values from 0.25 to 4. Lighter color indicates larger power.

Figure 6: Heatmaps of power for ETIC-RF with d(cid:48) = 10 PCs and p random features on the partially
dependent sample of the bilingual data. The x-axis is for r1 and y-axis is for r2. The indices from 0 to 11
correspond to equally spaced values from 0.25 to 4. Lighter color indicates larger power.

Following the trick in Section 4, we select the cost function to be the weighted quadratic cost with weights
given by the median heuristic. We set E = {0.25, 1, 4} and perform the adaptive ETIC test on the linear
dependency model and the subspace dependency model. As shown in Figure 7, it is slightly worse than the
best ETIC test in both models. We also run it on the bilingual text data. The power and type I error rate
of adaptive ETIC are 1 and 0.07 on the dependent sample and the independent sample, respectively. The
power achieved is 0.535 on the partially dependent sample; whereas the worst and best power of ETIC are
0.38 and 0.635, respectively.

Finally, we consider a Bonferroni-type ETIC test which is adaptive to both the regularization parameter
and the weights in the cost function. Following the formulation in Section 4, we let ψr1,r2 (α) be the decision
rule of ETIC with hyper-parameters r1 and r2. Consider the following Bonferroni-type ETIC test

ψ(α) := max

r1,r2∈R

ψr1,r2(α/ (cid:12)

(cid:12)R2(cid:12)
(cid:12)).

We perform this Bonferroni-type ETIC test on the linear dependency model and the subspace dependency
model for R = {0.25, 4}. As shown in Figure 8, it is slightly worse than the best ETIC test in both models.
Compared to the adaptive ETIC test, it performs similar in the linear dependency model and slightly better
in the subspace dependency model. We also run it on the bilingual text data. The power and type I error
rate of adaptive ETIC are 1 and 0.045 on the dependent sample and the independent sample, respectively.
The power achieved is 0.5 on the partially dependent sample, which is smaller than the power of the adaptive
ETIC.

35

0123456789101101234567891011d0 = 1001234567891011d0 = 200.300.350.400.450.500.550123456789101101234567891011p = 70001234567891011p = 15000.3500.3750.4000.4250.4500.4750.5000.5250.550Figure 7: Power curves in the linear dependency model (left) and subspace dependency model (right).

Figure 8: Power curves in the linear dependency model (left) and subspace dependency model (right).

E.3 Comparison with Other Independence Tests

We implemented another two independence tests considered in (Gretton and Györﬁ, 2008): the L1 test and
the Log-likelihood test. We apply them to the Gaussian sign model and the subspace dependency model
and compare them with ETIC and ETIC-RF with r = 0.25. As shown in Figure 9, they slightly outperform
ETIC in the Gaussian sign model. In the subspace dependency model, they perform similarly to ETIC for
small θ and signiﬁcantly worse for large θ.

36

246810Dimension0.700.750.800.850.900.951.00Power0.00.20.40.60.81.00.00.20.40.60.81.0r = 0.25r = 0.5r = 1.0r = 2.0r = 4.0Adaptive246810Dimension0.700.750.800.850.900.951.00Power0.00.20.40.60.81.00.00.20.40.60.81.0r = 0.25r = 0.5r = 1.0r = 2.0r = 4.0BonferroniFigure 9: Power curves in the Gaussian sign model (left) and subspace dependency model (right).

37

100200300400500Sample size0.30.40.50.60.70.80.91.0Power0.00.20.40.60.81.00.00.20.40.60.81.0ETICETIC-RFL1Log-lik