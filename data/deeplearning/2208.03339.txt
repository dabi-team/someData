Going Beyond Approximation: Encoding Constraints for Explainable
Multi-hop Inference via Differentiable Combinatorial Solvers

Mokanarangan Thayaparan†‡, Marco Valentino†‡, André Freitas†‡
Department of Computer Science, University of Manchester, United Kingdom†
Idiap Research Institute, Switzerland‡
{firstname.lastname}@manchester.ac.uk

2
2
0
2

g
u
A
5

]
I

A
.
s
c
[

1
v
9
3
3
3
0
.
8
0
2
2
:
v
i
X
r
a

Abstract

Integer Linear Programming (ILP) provides a
viable mechanism to encode explicit and con-
trollable assumptions about explainable multi-
hop inference with natural language. How-
ever, an ILP formulation is non-differentiable
and cannot be integrated into broader deep
learning architectures. Recently, Thayaparan
et al. (2021a) proposed a novel methodology
to integrate ILP with Transformers to achieve
end-to-end differentiability for complex multi-
hop inference. While this hybrid framework
has been demonstrated to deliver better answer
and explanation selection than transformer-
based and existing ILP solvers,
the neuro-
symbolic integration still relies on a convex
relaxation of the ILP formulation, which can
produce sub-optimal solutions. To improve
these limitations, we propose Diff-Comb Ex-
plainer, a novel neuro-symbolic architecture
based on Differentiable BlackBox Combinato-
rial solvers (DBCS) (Poganˇci´c et al., 2019).
Unlike existing differentiable solvers, the pre-
sented model does not require the transforma-
tion and relaxation of the explicit semantic
constraints, allowing for direct and more ef-
ﬁcient integration of ILP formulations. Diff-
Comb Explainer demonstrates improved accu-
racy and explainability over non-differentiable
solvers, Transformers and existing differen-
tiable constraint-based multi-hop inference
frameworks.

1

Introduction

Given a question expressed in natural language,
ILP-based Multi-hop Question Answering (QA)
aims to construct an explanation graph of intercon-
nected facts (i.e., natural language sentences) to
support the answer (see Figure 1). This framework
provides a viable mechanism to encode explicit and
controllable assumptions about the structure of the
inference (Khashabi et al., 2018; Khot et al., 2017;
Khashabi et al., 2016). For this reason, inference
based on constrained optimisation is generally re-

Figure 1: Example of question, answer and explana-
tions as graph (Xie et al., 2020; Jansen et al., 2018)

garded as interpretable and transparent, providing
structured explanations in support of the underlying
reasoning process (Thayaparan et al., 2020).

However, ILP solvers are non-differentiable and
cannot be integrated as part of a broader deep
learning architecture (Paulus et al., 2021; Poganˇci´c
et al., 2019). Moreover, these approaches are often
limited by the exclusive adoption of hard-coded
heuristics for the inference and cannot be opti-
mised end-to-end on annotated corpora to achieve
performance comparable to deep learning counter-
parts (Thayaparan et al., 2021a; Khashabi et al.,
2018).

In an attempt to combine the best of both worlds,
Thayaparan et al. (2021a) proposed a novel neuro-
symbolic framework (Diff-Explainer) that inte-
grates explicit constraints with neural representa-
tions via Differentiable Convex Optimisation Lay-
ers (Agrawal et al., 2019). Diff-Explainer combines
constraint optimisation solvers with Transformers-
based representations, enabling end-to-end train-
ing for explainable multi-hop inference. The non-
differenitability of ILP solvers is alleviated by ap-
proximating the constraints using Semi-Deﬁnite
programming (Helmberg, 2000). This approxima-

refractionAnswer: Question:Explanations: a convex lens is used in camerato focus means to concentratea convex lens causes light to refract and concentrate to magnify distant objectsF1:F2:F3:F4:Light rays are focused by the lens of a camera through the process ofrefraction is a kind of process 
 
 
 
 
 
tion usually requires non-trivial transformations of
ILP formulations into convex optimisation prob-
lems.

Since constraint-based multi-hop inference is
typically framed as optimal subgraph selection via
binary optimization (0, 1), The semi-deﬁnite re-
laxation employed in Diff-Explainer necessitates
a continuous relaxation of the discrete variables
(from {0, 1} to [0, 1]). While this process can pro-
vide tight approximations for ILP problems, this
relaxation can still lead to sub-optimal solutions in
practice (Yoshida, 2011; Thapper and Živn`y, 2018)
leading to errorneous answer and explanation pre-
diction.

To improve on these limitations, we propose
Diff-Comb Explainer, a novel neuro-symbolic ar-
chitecture based on Differentiable BlackBox Com-
binatorial solvers (DBCS) (Poganˇci´c et al., 2019).
The proposed algorithm transforms a combinato-
rial optimisation solver into a composable build-
ing block of a neural network. DBCS achieves
this by leveraging the minimisation structure of the
combinatorial optimisation problem, computing a
gradient of continuous interpolation to address the
non-differenitability of ILP solvers. In contrast to
Diff-Explainer (Thayaparan et al., 2021a), DBCS
makes it possible to compute exact solutions for
the original ILP problem under consideration, ap-
proximating the gradient.

Our experiments on multi-hop question an-
swering with constraints adopted from Explana-
tionLP (Thayaparan et al., 2021b) yielded an im-
provement of 11% over non-differentiable solvers
and 2.08% over Diff-Explainer. Moreover, we
demonstrate that the proposed approach produces
more accurate and faithful explanation-based infer-
ence, outperforming a non-differentiable solver and
Diff-Explainer by 8.41% and 3.63% on explanation
selection.

In summary, the contributions of the chapter are

as follows:

1. A novel constrained-based natural language
solver that combines a differentiable black box
combinatorial solver with transformer-based ar-
chitectures.

2. Empirically demonstrates that differentiable
combinatorial solvers combined with trans-
former architectures provide improved per-
formance for explanation and answer selec-
tion when compared to differentiable and non-
differentiable counterparts.

3. Demonstrate that Diff-Comb Explainer better re-
ﬂects the underlying explanatory inference pro-
cess leading to the ﬁnal answer prediction, out-
performing differentiable and non-differentiable
counterparts in terms of faithfulness and consis-
tency.

2 Related Work

Constraint-based multi-hop inference
ILP
has been applied for
structured representa-
tion (Khashabi et al., 2016) and over semi-
structured representation extracted from text (Khot
et al., 2017; Khashabi et al., 2018).
Early
approaches were unsupervised. However, re-
cently Thayaparan et al. (2021b) proposed the
ExplanationLP model optimised towards answer
selection via Bayesian optimisation.
Expla-
nationLP was limited to ﬁne-tuning only nine
parameters and used pre-trained neural embedding.
Diff-Explainer (Thayaparan et al., 2021a) was
the ﬁrst approach to integrate constraints into a
deep-learning network via Differentiable Convex
Optimisation Layer (Agrawal et al., 2019) by
approximating ILP constraints using Semi-deﬁnite
programming.

Hybrid reasoning with Transformers
Clark
et al. (2021) proposed “soft theorem provers” op-
erating over explicit theories in language. This
hybrid reasoning solver integrates natural language
rules with transformers to perform deductive rea-
soning. Saha et al. (2020) improved on top of it,
enabling the answering of binary questions along
with the proofs supporting the prediction. The mul-
tiProver (Saha et al., 2021) evolves on top of these
conceptions to produce an approach that is capa-
ble of producing multiple proofs supporting the an-
swer. While these hybrid reasoning approaches pro-
duce explainable and controllable inference, they
assume the existence of natural language rules and
have only been applied to synthetic datasets. On
the other hand, our approach does not require ex-
tensive rules set and can tackle complex scientiﬁc
and commonsense QA.

Differentiable Blackbox Combinatorial Optimi-
sation Solver Given the following bounded inte-
ger problem:

min
x∈X

c · x

subject to

Ax ≤ b,

(1)

where X ∈ Zn, c ∈ Rn, x are the variables, A =
[a1, . . . , am] ∈ Rm×n is the matrix of constraint

coefﬁcients and b ∈ Rm is the bias term. The
output of the solver g(c) returns the arg minx∈X
of the integer problem.

Differentiable Combinatorial Optimisation
Solver (Poganˇci´c et al., 2019) (DBCS) assumes
that A, b are constant and the task is to ﬁnd the
dL/dc given global loss function L with respect
to solver output x at a given point ˆx = g(ˆc).
However, a small change in c is typically not going
to change the optimal ILP solution resulting in the
true gradient being zero.

In order to solve this problem, the approach sim-
pliﬁes by considering the linearisation f of L at the
point ˆx.

f (x) = L(ˆx) +

dL
dx

(ˆx) · (x − ˆx)

(2)

to derive:

df (g(c))
dc

=

dL
dc

(3)

By introducing the linearisation, the focus is now
to differentiating the piecewise constant function
f (g(c)). The approach constructs a continuous in-
terpolation of f (g(c)) by function fλ(w). Here
the hyper-parameter λ > 0 controls the trade-off
between informativeness of the gradient and faith-
fulness to the original function.

3 Diff-Comb Explainer: Differentiable
Blackbox Combinatorial Solver for
Explainable Multi-Hop Inference

ILP-based QA is typically applied to multiple-
choice question answering (Khashabi et al., 2018;
Khot et al., 2017; Khashabi et al., 2016; Thaya-
paran et al., 2021b). Given Question (Q) and the set
of candidate answers C = {c1, c2, c3,
. . . , cn}
the aim is to select the correct answer cans.

In order to achieve this, ILP-based approaches
convert question answer pairs into a list of hypoth-
esis H = {h1, h2, h3,
. . . , hn} (where hi is the
concatenation of Q with ci) and typically adopt a
retrieval model (e.g: BM25, FAISS (Johnson et al.,
2017)), to select a list of candidate explanatory
facts F = {f1, f2, f3, . . . , fk}. Then construct a
weighted graph G = (V, E, W ) with edge weights
W : E → R where V = {{hi} ∪ F }, edge
weight Wik of each edge Eik denote how relevant
a fact fk is with respect to the hypothesis hi.

Given this premise, ILP-based Multi-hop QA
can be deﬁned as follows (Thayaparan et al.,
2021a):

Deﬁnition 3.1 (ILP-Based Multi-Hop QA). Find a
subset V ∗ ⊆ V , h ∈ V ∗ and E∗ ⊆ E such that the
induced subgraph G∗ = (V ∗, E∗) is connected,
weight W [G∗ = (V ∗, E∗)]
e∈E∗ W (e)
is maximal and adheres to set of constraints Mc
designed to emulate multi-hop inference. The
hypothesis hi with the highest subgraph weight
W [G∗ = (V ∗, E∗)] is selected to be the correct
answer cans.

:= (cid:80)

As illustrated in Figure 2, Diff-Comb Explainer
has 3 major parts: Graph Construction, Subgraph
Selection and Answer/Explanation Selection. In
Graph Construction, for each candidate answer ci
we construct graph Gi = (V i, Ei, W i) where the
V i = {hi} ∪ {F } and weights W i
ik of each edge
Ei
ik denote how relevant a fact fk is with respect
to the hypothesis hi. These edge weights (W i
ik)
are calculated using a weighted (θ) sum of scores
calculated using transformer-based (ST rans) em-
beddings and lexical overlap.

In the Subgraph Selection step, for each Gi
Differentiable Blackbox Combinatorial Solver
(DBCS) with constraints are applied to extract sub-
graph G∗. In this paper, we adopt the constraints
proposed for ExplanationLP (Thayaparan et al.,
2021b). ExplanationLP explicit abstraction by
grouping facts into abstract and grounding facts.
Abstract facts are core scientiﬁc facts that a ques-
tion is attempting to test, and grounding facts link
concepts in the abstract facts to speciﬁc terms in the
question/answer. For example, in Figure 1 the core
scientiﬁc fact is about the nature of convex lens and
how they refract light (F1). Facts F2, F3, F4 help
to connect the abstract fact to the question/answer.
Finally, in Answer/Explanation Selection the
the correct answer cans
model
and relevant explanations Fexp. During training
time, the loss is calculated based on gold an-
swer/explanations to ﬁne-tune the transformers
(ST rans) and weights (θ).

is to predict

The rest of the section explains each of the com-

ponents in detail.

3.1 Graph Construction

In order to facilitate grounding abstract chains, the
retrieved facts F are classiﬁed into grounding facts
3 , ..., f g
FG = {f g
l } and abstract facts FA =
2 , f a
1 , f a
{f a
m} such that F = FA ∪ FG and
l + m = k.

1 , f g
2 , f g
3 , ..., f a

Similarly to Diff-Explainer (Thayaparan et al.,
2021a), we use two relevance scores: semantic

Figure 2: End-to-end architectural diagram of Diff-Comb Explainer. The integration of Differentiable Blackbox
Combinatorial solvers will result in better explanation and answer prediction.

and lexical scores, to calculate the edge weights.
We use a Sentence-Transformer (STrans) (Reimers
et al., 2019) bi-encoder architecture to calculate
the semantic relevance. The semantic relevance
score from STrans is complemented with the lexical
relevance score. The semantic and lexical relevance
scores are calculated as follows:
Semantic Relevance (s): Given a hypothesis hi
and fact fj we compute sentence vectors of (cid:126)hi =
ST rans(hi) and (cid:126)fj = ST rans(fj) and calcu-
late the semantic relevance score using cosine-
similarity as follows:

3.2 Subgraph Selection via Differentiable
Blackbox Combinatorial Solvers

Given the above premises, the objective function is
deﬁned as:

min

− 1(W i · Y i)

(7)

edge

We

adopt

the

{0, 1}(n+1)×(n+1) where Y i
the value of 1 iff edge Ei
and Y i
subgraph.

jj takes the value of 1 iff V i

variable Y i
j,k (j

∈
(cid:54)= k) takes
jk belongs to the subgraph
j belongs to the

sij = S( (cid:126)hi , (cid:126)fj ) =

(cid:126)hi · (cid:126)fj
(cid:107) (cid:126)hi (cid:107)(cid:107) (cid:126)fj (cid:107)

Given the above variable, the constraints are de-

(4)

ﬁned as follows:

Lexical Relevance (l): The lexical relevance score
of hypothesis hi and fj is given by the percent-
age of overlaps between unique terms (here, the
function trm extracts the lemmatized set of unique
terms from the given text):

Answer selection constraint The candidate hy-
pothesis should be part of the induced subgraph:

(cid:88)

Y i
jj = 1

j ∈ {hi}

(8)

lij = L(hi, fj) =

|trm(hi) ∩ trm(fj)|
max(|trm(hi)|, |trm(fj)|)

(5)
Given the above scoring function, we construct

k are selected then edges Ei

Edge and Node selection constraint
and V i
be selected. If node V i
will also be selected:

If node V i
j
kj will
j is selected, then edge Ejj

jk and Ei

the edge weights matrix (W ) as follows:

W i

jk =






−θggljk
−θaaljk
θgaljk
θqglljk + θqgssjk
θqalljk + θqalsjk

(j, k) ∈ FG
(j, k) ∈ FA
j ∈ FG, k ∈ FA
j ∈ FG, k = hi
j ∈ FA, k = hi

jk ≤ Y i
Y i
jj
Y i
jk ≤ Ykk
Y i
jk ≥ Yjj + Ykk − 1

∀ (j, k) ∈ E

∀ (j, k) ∈ E

∀ (j, k) ∈ E

(9)

(10)

(11)

Abstract fact selection constraint Limit the
number of abstract facts selected to M :

(6)
Here relevance scores are weighted by θ param-

eters which are clamped to [0, 1].

Y i
jj ≤ M

(cid:88)

i

∀j ∈ FA

(12)

Solved  Adjacency VariableSTRANSSTRANSSTRANS. . . . .: Shared WeightsSTRANSCosCosCosGraph ConstructionCrossLosscansBCE LossBCE Loss1/0Minimizes.t  WeightsLoss Back Propagation. . . . .. . . . .Differentiable BlackboxCombinatorial SolverTransformBCE Loss1/01/0Edge  Weights Lexical Relevance  ScoresDBCSEdge SelectionCalculate Objective  ScoreCalculate Semantic RelevanceSubgraph SelectionAnswer & Explanation SelectionFor each candidate answer 3.3 Answer and Explanation Selection
The solved adjacency variable ˆY i represents the
selected edges for each candidate answer choice ci.
Not all datasets provide gold explanations. More-
over, even when the gold explanations are available,
they are only available for the correct answer with
no explanations for the wrong answer.

In order to tackle these shortcomings and ensure
end-to-end differentiability, we use the softmax (σ)
of the objective score (W i · ˆY i) as the probability
score for each choice.

We multiply each objective score W i · ˆY i value
by the temperature hyperparameter (T ) to obtain
soft probability distributions γi (where γi = (W i ·
ˆY i) · T ). The aim is for the correct answer cans to
have the highest probability.

In order to achieve this aim, we use the cross
entropy loss lc as follows to calculate the answer
selection loss Lans as follows:

Lans = lc(σ(γ1, γ2,

· · · γn), cans)

(13)

If gold explanations are available, we comple-
ment Lans with explanation loss Lexp. We employ
binary cross entropy loss lb between the selected
explanatory facts and gold explanatory facts Fexp
for the explanatory loss as follows:

Lexp = lb( ˆY ans[f1, f2,

. . . , fk], Fexp)

(14)

We calculate the total loss (L) as weighted by

hyperparameters λans, λexp as follows:

L = λansLans + λexpLexp

(15)

The pseudo-code to train Diff-Comb Explainer

end-to-end is summarized in Algorithm 1.

4 Empirical Evaluation

4.1 Answer and Explanation Selection

We use the WorldTree corpus (Xie et al., 2020) for
training the evaluation of explanation and answer
selection. The 4,400 question and explanations in
the WorldTree corpus are split into three different
subsets: train-set, dev-set and test-set. We use the
dev-set to assess the explainability performance
since the explanations for test-set are not publicly
available.

Algorithm 1: Training Diff-Comb Explainer

Data: A, b ← Multi-hop Inference Constraints
Data: fw ← Graph weight Function
Data: λ ← Hyperparameter for DBCS interpolation
epoch ← 0;
while epoch ≤ max_epochs do
foreach hi ∈ H do

Gi ← fact-graph-construction(hi, F );
li ← L(hi, F );
θ ← clamp([0, 1]);
(cid:126)F ← ST rans(F );
(cid:126)hi ← ST rans(hi);
si ← S( (cid:126)hi, (cid:126)F );
W i ← fw(si, li; θ);
ˆY i ← DBCS(−W i, A, b; λ);
γi ← (W · ˆY i) · T ;

end
Lans = lc(σ(γ1, γ2,
if Fexp is available then

· · · γn), cans);

Lexp = lb( ˆY ans[f1, f2,
L = λansLans + λexpLexp;

. . . , fk], Fexp);

else

L = Lans;

end
update θ, ST rans using AdamW optimizer by

minimizing loss;
epoch ← epoch + 1;

end
Result: Store best θ and ST rans

Baselines: We use the following baselines to
compare against our approach for the WorldTree
corpus:

1. BERTBase and BERTLarge (Devlin et al.,
2019): To use BERT for this task, we concate-
nate every hypothesis with k retrieved facts,
using the separator token [SEP]. We use the
HuggingFace (Wolf et al., 2019) implementa-
tion of BertForSequenceClassiﬁcation, taking
the prediction with the highest probability for
the positive class as the correct answer.

2. ExplanationLP: Non-differentiable version of
ExplanationLP. Using the constraints stated in
Section 3, we ﬁne-tune the θ parameters using
Bayesian optimization and frozen STrans rep-
resentations. This baseline aims to evaluate the
impact of end-to-end ﬁne-tuning over the non-
differentiable solver.

3. Diff-Explainer:

has

Diff-Explainer

al-
ready exhibited better performance over
other explainable multi-hop inference ap-
proaches,
including ILP-based approaches
including TableILP (Khashabi et al., 2016),
TupleILP (Khot et al., 2017) and graph-based
neural approach PathNet (Kundu et al., 2019).
Similar to our approach, we use ExplanationLP

Model

Baselines
BERTBase
BERTLarge

Fact Retrieval (FR) Only
BERTBase + FR
BERTLarge + FR

ExplanationLP
Diff-Explainer

Diff-Comb Explainer
- Answer selection only
- Answer and explanation
selection

Explanation Selection (dev)

Precision

Explanatory
Consistency

Faithfulness

@2

@1

@3

@2

@1

Answer
Selection
(test)

-
-

30.19
-
-

40.41
41.91

-
-

38.49
-
-

51.99
56.77

-
-

21.42
-
-

29.04
39.04

-
-

15.69
-
-

14.14
20.64

-
-

11.64
-
-

11.79
17.01

45.75
47.57

61.01
63.23

49.04
43.33

29.99
33.36

18.88
20.71

-
-

-
52.65
51.23

71.11
72.22

73.37
74.47

45.43
49.63

-
58.06
59.32

62.57
71.48

72.04
73.46

Table 1: Comparison of explanation and answer selection of Diff-Comb Explainer against other baselines. Expla-
nation Selection was carried out on the dev set as the test explanation was not public available.

constraints with Diff-Explainer. We use similar
hyperparameters and knowledge base used in
Thayaparan et al. (2021a).

Experimental Setup: We employ the following
experimental setup:
• Sentence Transformer Model: ALL-MPNET-

BASE-V2 (Song et al., 2020).
• Fact retrieval representation:

ALL-MPNET-
BASE-V2 trained with gold explanations of
WorldTree Corpus to achieve a Mean Average
Precision of 40.11 in the dev-set.

• Fact retrieval: FAISS retrieval (Johnson et al.,

2017) using pre-cached representations.

• Background knowledge: 5000 abstract facts from
the WorldTree table store (WTree) and over
100,000 is-a grounding facts from ConceptNet
(CNet) (Speer et al., 2017).

• The experiments were carried out for k =
{1, 2, 3, 5, 10, 20, 30, 40, 50} and the best conﬁg-
uration for each model is selected.

• The hyperparameters λ, λans, λexp, T were ﬁne-
tuned for 50 epochs using the Adpative Experi-
mentation Platform.

• M =2 for ExplanationLP, Diff-Explainer and Diff-

Comb Explainer.

Metrics The answer selection is evaluated using
accuracy. For evaluation of explanation selection,

we use Precision@K. In addition to Precision@K,
we introduce two new metrics to evaluate the truth-
fulness of the answer selection to the underlying
inference. The metrics are as follows:
Explanatory Consistency@K: Question/answer
pair with similar explanations indicates similar un-
derlying inference (Atanasova et al., 2022). The
expectation is that similar underlying inference
would produce similar explanations (Valentino
et al., 2021, 2022). Given a test question Qt and
retrieved explanations Et we ﬁnd set of Questions
Qs
. . . } with at least K overlap gold
explanations along with the retrieved explanations
Es
. . . }. Given this premise, Explana-
tory Consistency@K is deﬁned as follows:

t = {Q1

t = {e1

t , Q2
t ,

t , e2
t ,

(cid:80)

[ei

t ∈ Et]
|ei
t|

t∈Es
ei
t
(cid:80)

t∈Es
ei
t

(16)

Explanatory Consistency measures out of ques-
tions/answer pairs with at least K similar gold ex-
planations and how many of them share a common
retrieved explanation.
Faithfulness: The aim is to measure how much
percentage of the correct prediction is derived from
correct inference and wrong prediction is derived
from wrong inference over the entire set. Let’s say
that the set of questions correctly answered as AQc,
wrongly answered questions AQw , set of questions

with at least one correctly retrieved explanation
as AQ1 and set of questions where no correctly
retrieved explanations AQ0. Given this premise,
Faithfulness is deﬁned as follows:

|AQw ∩ AQ0| + |AQc ∩ AQ1|
|AQc ∪ AQw |

(17)

A higher faithfulness implies that the underlying
inference process is reﬂected in the ﬁnal answer
prediction.

Table 1 illustrates the explanation and answer
selection performance of Diff-Comb Explainer and
the baselines. We report scores for Diff-Comb Ex-
plainer trained for only the answer and optimised
jointly for answer and explanation selection.

Since BERT does not provide explanations, we
use facts retrieved from the fact retrieval for the best
k conﬁguration (k = 3) as explanations. We also
report the scores for BERT without explanations.

We draw the following conclusions from the re-
sults obtained in Table 1 (The performance increase
here are expressed in absolute terms):
(1) Diff-Comb Explainer improves answer selec-
tion performance over the non-differentiable solver
by 9.47% with optimising only on answer selection
and 10.89% with optimising on answer and expla-
nation selection. This observation underlines the
impact of the end-to-end ﬁne-tuning framework.
We can also observe that strong supervision with
optimising explanation selection yields better per-
formance than weak supervision with answer selec-
tion.
(2) Diff-Comb Explainer outperforms the best
transformer-based model by 14.14% for answer se-
lection. This increase in performance demonstrates
that integrating constraints with transformer-based
architectures leads to better performance.
(3) Diff-Comb Explainer outperform the best Diff-
Explainer conﬁguration (answer and explanation
selection) by 0.56% even in the weak supervision
setting (answer only optimisation). We also outper-
form Diff-Explainer by 1.98% in the best setting.
(4) Diff-Comb Explainer is better for selecting rele-
vant explanations over the other constraint-based
solvers. Diff-Comb Explainer outperforms the non-
differentiable solver at Precision@K by 8.41%
(k =1) and 6.05% (k =2). We also outperform
Diff-Explainer by 3.63% (k =1) and 4.55% (k =2).
The improvement of Precision@K over the Fact
Retrieval only (demonstrated with BERT + FR) by
16.98% (k =1) and 24.74% (k =2) underlines the

robustness of our approach to noise propagated by
the upstream fact retrieval.
(5) Our models also exhibit higher Explanatory
Consistency over the other solvers. This perfor-
mance shows that the optimisation model is learn-
ing and applying consistent inference across differ-
ent instances. We also outperform the fact retrieval
model which was also a transformer-based model
trained on gold explanations.
(6) Answer prediction by black-box models like
BERT do not reﬂect the explanation provided. This
fact is indicated by the low Faithfulness score ob-
tained by both BERTBase/BERTLarge.
In con-
trast, the high constraint-based solver’s Faithful-
ness scores emphasise how the underlying infer-
In partic-
ence reﬂects on the ﬁnal prediction.
ular, our approach performs better than the non-
differentiable models and Diff-Explainer.

In summary, despite the fact that Diff-Explainer
and Diff-Comb Explainer approaches use the same
set of constraints, our model yields better perfor-
mance, indicating that accurate predictions gener-
ated by ILP solvers are better than approximated
sub-optimal results.

4.2 Qualitative Analysis

Both explanations and answer predictions in Ques-
tion (1) are entirely correct for our model. In this
example, both ExplanationLP and Diff-Explainer
have failed to retrieve any correct explanations or
predict the correct answer. Both the approaches are
distracted by the strong lexical overlaps with the
wrong answer.

Question (2) at least one explanation is correct
and a correct answer prediction for our model. In
the example provided, Diff-Explainer provides the
correct answer prediction with both the retrieved
facts not being explanatory. Diff-Explainer arrives
at the correct answer prediction with no explanation
addressing the correct answer.

In Question (3) both our model and Diff-
Explainer provide the correct answer but with both
facts not being explanations. The aforementioned
qualitative (Question 1 and 2) and quantitative mea-
sures (Explanatory Consistency@K, Faithfulness)
indicate how the underlying explanatory inference
results in the correct prediction; there are cases
where false inference still leads to the correct an-
swer with our model as well. In this case, the in-
ference is distracted by the strong lexical overlaps
irrelevant to the question.

Question (1): Which measurement is best expressed in light-years?: Correct Answer: the distance between
stars in the Milky Way.
ExplanationLP
Answer: the time it takes for planets to complete their orbits. Explanations: (i) a complete revolution; orbit of a
planet around its star takes 1; one planetary year, (ii) a light-year is used for describing long distances
Diff-Explainer
Answer: the time it takes for planets to complete their orbits. Explanations: (i) a light-year is used for describing
long distances, (ii) light year is a measure of the distance light travels in one year
Diff-Comb Explainer
Answer: the distance between stars in the Milky Way. Explanations: (i) light years are a astronomy unit used
for measuring length, (ii) stars are located light years apart from each other

Question (2): Which type of precipitation consists of frozen rain drops?: Correct Answer: sleet.
ExplanationLP
Answer: snow. Explanations: (i) precipitation is when snow fall from clouds to the Earth, (ii) snow falls
Diff-Explainer
Answer: sleet. Explanations: (i) snow falls, (ii) precipitation is when water falls from the sky
Diff-Comb Explainer
Answer: sleet. Explanations: (i) sleet is when raindrops freeze as they fall, (ii) sleet is made of ice

Question (3): Most of the mass of the atom consists of?: Correct Answer: protons and neutrons.
ExplanationLP
Answer: neutrons and electrons. Explanations: (i) neutrons have more mass than an electron, (ii) neutrons have
more mass than an electron
Diff-Explainer
Answer: protons and neutrons. Explanations: (i) the atomic mass is made of the number of protons and
neutrons, (ii) precipitation is when water falls from the sky
Diff-Comb Explainer
Answer: protons and neutrons. Explanations: (i) the atomic mass is made of the number of protons and
neutrons, (ii) precipitation is when water falls from the sky

Table 2: Example of predicted answers and explanations (Only CENTRAL explanations) obtained from our model
with different levels of ﬁne-tuning.

However, from the qualitative analysis, we can
conclude that the explainable inference that hap-
pens with our model is more robust and coher-
ent when compared to the Diff-explainer and non-
differentiable models.

4.3 Knowledge aggregation with increasing

distractors

One of the key characteristics identiﬁed by Thaya-
paran et al. (2021a) is the robustness of Diff-
Explainer to distracting noise. In order to evaluate
if our model also exhibits the same characteristics,
we ran our model for the increasing number of
retrieved facts k and plotted the answer selection
accuracy for WorldTree in Figure 3.

As illustrated in the Figure, similar to Diff-
Explainer, our approach performance remains sta-
ble with increasing distractors. We also continue
to outperform Diff-Explainer across all sets of k.

BERT performance drops drastically with in-
creasing distractors. This phenomenon is in line
with existing work (Thayaparan et al., 2021a; Ya-
dav et al., 2019b). We hypothesise that with in-

Figure 3: Comparison of accuracy for different number
of retrieved facts.

creasing distractors, BERT overﬁts quickly with
spurious inference correlation. On the other hand,
our approach circumvents this problem with the
inductive bias provided by the constraint optimisa-
tion layer.

Model

BERTLarge

IR Solver (Clark et al., 2016)
TupleILP (Khot et al., 2017)
TableILP (Khashabi et al.,
2016)
ExplanationLP
(Thayaparan et al., 2021b)
DGEM (Clark et al., 2016)
KG2 (Zhang et al., 2018)
ET-RR (Ni et al., 2019)
Unsupervised AHE (Yadav
et al., 2019a)
Supervised AHE (Yadav et al.,
2019a)
AutoRocc (Yadav et al.,
2019b)
Diff-Explainer
(ExplanationLP) (Thayaparan
et al., 2021a)

Diff-Comb Explainer
(ExplanationLP)

Explainable Accuracy

No

Yes
Yes
Yes

Yes

Partial
Partial
Partial
Partial

35.11

20.26
23.83
26.97

40.21

27.11
31.70
36.61
33.87

Partial

34.47

Partial

41.24

Yes

42.95

Yes

43.21

Table 3: ARC challenge scores compared with other Fully
or Partially explainable approaches trained only on the ARC
dataset.

4.4 Comparing Answer Selection with ARC

Baselines

Table 3 presents a comparison of publicly reported
baselines on the ARC Challenge-Corpus (Clark
et al., 2018) and our approach. These ques-
tions have proven to be challenging to answer for
other LP-based question answering and neural ap-
proaches.

While models such as UniﬁedQA (Khashabi
et al., 2020) and AristoBERT (Xu et al., 2021)
have demonstrated performance of 81.14 and 68.95,
they have been trained on other question-answering
datasets, including RACE (Lai et al., 2017). More-
over, despite its performance, UniﬁedQA does not
provide explanations supporting its inference.

In Table 3, to provide a rigorous comparison, we
only list models that have been trained only on the
ARC corpus and provides explanations supporting
its inference to ensure fair comparison. Here the ex-
plainability column indicates if the model delivers
an explanation for the predicted answer. A subset
of the models produces evidence for the answer
but remains intrinsically black-box. These models
have been marked as Partial.

As illustrated in the Table 3, Diff-Comb Ex-
plainer outperforms the best non-differentiable
constraint-solver model (ExplanationLP) by 2.8%.

We also outperform a transformer-only model Au-
toRocc by 1.97%. While our improvement over
Diff-Explainer is small, we still demonstrate per-
formance improvements for answer selection. On
top of performances obtained for explanation and
answer selection with WorldTree corpus, we have
also established better performances than leader-
board approaches.

5 Conclusion

This paper proposed a novel framework for encod-
ing explicit and controllable assumptions as part of
an end-to-end learning framework for explainable
multi-hop inference using Differentiable Blackbox
Combinatorial Solvers (Poganˇci´c et al., 2019). We
empirically demonstrated improved answer and ex-
planation selection performance compared with the
existing differentiable constraint-based solver for
multi-hop inference (Thayaparan et al., 2021a). We
also demonstrated performance gain and increased
robustness to noise when combining constraints
with transformer-based architectures. In this paper,
we adopted the constraints of ExplanationLP, but
it is possible to encode more complex inference
constraints within the model.

Diff-Comb Explainer builds on previous work
by Thayaparan et al. (2021a) and investigates the
combination of symbolic knowledge (expressed via
constraints) with neural representations. We hope
this work will encourage researchers to encode dif-
ferent domain-speciﬁc priors, leading to more ro-
bust, transparent and controllable neuro-symbolic
inference models for NLP.

Acknowledgements

The work is partially funded by the EPSRC grant
EP/T026995/1 entitled “EnnCore: End-to-End
Conceptual Guarding of Neural Architectures” un-
der Security for all in an AI enabled society and by
the SNSF project NeuMath (200021_204617). We
would like to thank the Computational Shared Fa-
cility of the University of Manchester for providing
the infrastructure to run our experiments.

References

Akshay Agrawal, Brandon Amos, Shane Barratt,
Stephen Boyd, Steven Diamond, and J. Zico Kolter.
2019. Differentiable convex optimization layers. In
Advances in Neural Information Processing Systems,
volume 32. Curran Associates, Inc.

Pepa Atanasova, Jakob Grue Simonsen, Christina Li-
oma, and Isabelle Augenstein. 2022. Diagnostics-
In Proceedings of
guided explanation generation.
the AAAI Conference on Artiﬁcial Intelligence, vol-
ume 36, pages 10445–10453.

Peter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot,
Ashish Sabharwal, Carissa Schoenick, and Oyvind
Tafjord. 2018. Think you have solved question an-
swering? try arc, the ai2 reasoning challenge. arXiv
preprint arXiv:1803.05457.

Peter Clark, Oren Etzioni, Tushar Khot, Ashish Sab-
harwal, Oyvind Tafjord, Peter D Turney, and Daniel
Khashabi. 2016. Combining retrieval, statistics, and
inference to answer elementary science questions.
In AAAI, pages 2580–2586. Citeseer.

Peter Clark, Oyvind Tafjord, and Kyle Richardson.
2021. Transformers as soft reasoners over language.
In Proceedings of the Twenty-Ninth International
Conference on International Joint Conferences on
Artiﬁcial Intelligence, pages 3882–3890.

Jacob Devlin, Ming-Wei Chang, Kenton Lee, and
Kristina Toutanova. 2019. Bert: Pre-training of
deep bidirectional transformers for language under-
standing. In Proceedings of the 2019 Conference of
the North American Chapter of the Association for
Computational Linguistics: Human Language Tech-
nologies, Volume 1 (Long and Short Papers), pages
4171–4186.

Christoph Helmberg. 2000. Semideﬁnite programming

for combinatorial optimization.

Peter Jansen, Elizabeth Wainwright, Steven Mar-
morstein, and Clayton Morrison. 2018. Worldtree:
A corpus of explanation graphs for elementary sci-
ence questions supporting multi-hop inference.
In
Proceedings of the Eleventh International Confer-
ence on Language Resources and Evaluation (LREC
2018).

Jeff Johnson, Matthijs Douze, and Hervé Jégou. 2017.
Billion-scale similarity search with gpus. CoRR,
abs/1702.08734.

Tushar Khot, Ashish Sabharwal, and Peter Clark. 2017.
Answering complex questions using open informa-
tion extraction. arXiv preprint arXiv:1704.05572.

Souvik Kundu, Tushar Khot, Ashish Sabharwal, and
Peter Clark. 2019. Exploiting explicit paths for
In Proceedings
multi-hop reading comprehension.
of the 57th Annual Meeting of the Association for
Computational Linguistics, pages 2737–2747, Flo-
rence, Italy. Association for Computational Linguis-
tics.

Guokun Lai, Qizhe Xie, Hanxiao Liu, Yiming Yang,
and Eduard Hovy. 2017. RACE: Large-scale ReAd-
In
ing comprehension dataset from examinations.
Proceedings of the 2017 Conference on Empirical
Methods in Natural Language Processing, pages
785–794, Copenhagen, Denmark. Association for
Computational Linguistics.

Jianmo Ni, Chenguang Zhu, Weizhu Chen, and Julian
McAuley. 2019. Learning to attend on essential
terms: An enhanced retriever-reader model for open-
domain question answering. In Proceedings of the
2019 Conference of the North American Chapter of
the Association for Computational Linguistics: Hu-
man Language Technologies, Volume 1 (Long and
Short Papers), pages 335–344.

Anselm Paulus, Michal Rolínek, Vít Musil, Brandon
Amos, and Georg Martius. 2021. Comboptnet: Fit
the right np-hard problem by learning integer pro-
gramming constraints. In International Conference
on Machine Learning, pages 8443–8453. PMLR.

Marin Vlastelica Poganˇci´c, Anselm Paulus, Vit Musil,
Georg Martius, and Michal Rolinek. 2019. Differen-
tiation of blackbox combinatorial solvers. In Inter-
national Conference on Learning Representations.

Nils Reimers, Iryna Gurevych, Nils Reimers, Iryna
Gurevych, Nandan Thakur, Nils Reimers, Johannes
Daxenberger, and Iryna Gurevych. 2019. Sentence-
bert: Sentence embeddings using siamese bert-
networks. In Proceedings of the 2019 Conference on
Empirical Methods in Natural Language Processing.
Association for Computational Linguistics.

Daniel Khashabi, Tushar Khot, Ashish Sabharwal, Pe-
ter Clark, Oren Etzioni, and Dan Roth. 2016. Ques-
tion answering via integer programming over semi-
structured knowledge. In Proceedings of the Twenty-
Fifth International Joint Conference on Artiﬁcial In-
telligence, pages 1145–1152.

Swarnadeep Saha, Sayan Ghosh, Shashank Srivastava,
and Mohit Bansal. 2020. Prover: Proof generation
In Proceed-
for interpretable reasoning over rules.
ings of the 2020 Conference on Empirical Methods
in Natural Language Processing (EMNLP), pages
122–136.

Daniel Khashabi, Tushar Khot, Ashish Sabharwal, and
Dan Roth. 2018. Question answering as global rea-
soning over semantic abstractions. In Proceedings
of the AAAI Conference on Artiﬁcial Intelligence,
volume 32.

Daniel Khashabi, Sewon Min, Tushar Khot, Ashish
Sabharwal, Oyvind Tafjord, Peter Clark, and Han-
naneh Hajishirzi. 2020. Uniﬁedqa: Crossing format
boundaries with a single qa system. arXiv preprint
arXiv:2005.00700.

Swarnadeep Saha, Prateek Yadav, and Mohit Bansal.
2021. multiprover: Generating multiple proofs for
improved interpretability in rule reasoning. In Pro-
ceedings of the 2021 Conference of the North Amer-
ican Chapter of the Association for Computational
Linguistics: Human Language Technologies, pages
3662–3677.

Kaitao Song, Xu Tan, Tao Qin, Jianfeng Lu, and Tie-
Yan Liu. 2020. Mpnet: Masked and permuted pre-
training for language understanding. Advances in

Vikas Yadav, Steven Bethard, and Mihai Surdeanu.
2019a. Alignment over heterogeneous embeddings
for question answering. In Proceedings of the 2019
Conference of the North American Chapter of the
Association for Computational Linguistics: Human
Language Technologies, Volume 1 (Long and Short
Papers), pages 2681–2691.

Vikas Yadav, Steven Bethard, and Mihai Surdeanu.
2019b. Quick and (not so) dirty: Unsupervised se-
lection of justiﬁcation sentences for multi-hop ques-
In Proceedings of the 2019 Con-
tion answering.
ference on Empirical Methods in Natural Language
Processing and the 9th International Joint Confer-
ence on Natural Language Processing (EMNLP-
IJCNLP), pages 2578–2589, Hong Kong, China. As-
sociation for Computational Linguistics.

Yuichi Yoshida. 2011. Optimal constant-time approxi-
mation algorithms and (unconditional) inapproxima-
bility results for every bounded-degree csp. In Pro-
ceedings of the forty-third annual ACM symposium
on Theory of computing, pages 665–674.

Yuyu Zhang, Hanjun Dai, Kamil Toraman, and
Le Song. 2018. Kgˆ 2: Learning to reason science
exam questions with contextual knowledge graph
embeddings. arXiv preprint arXiv:1805.12393.

Neural Information Processing Systems, 33:16857–
16867.

Robyn Speer, Joshua Chin, and Catherine Havasi. 2017.
Conceptnet 5.5: An open multilingual graph of gen-
eral knowledge. In Thirty-ﬁrst AAAI conference on
artiﬁcial intelligence.

Johan Thapper and Stanislav Živn`y. 2018.

The
limits of sdp relaxations for general-valued csps.
ACM Transactions on Computation Theory (TOCT),
10(3):1–22.

Mokanarangan Thayaparan, Marco Valentino, Deborah
Ferreira, Julia Rozanova, and André Freitas. 2021a.
Diff-explainer: Differentiable convex optimization
for explainable multi-hop inference.

Mokanarangan Thayaparan, Marco Valentino, and An-
dré Freitas. 2020. A survey on explainability in
arXiv preprint
machine reading comprehension.
arXiv:2010.00389.

Mokanarangan Thayaparan, Marco Valentino, and An-
dré Freitas. 2021b.
Explainable inference over
grounding-abstract chains for science questions. In
Findings of the Association for Computational Lin-
guistics: ACL-IJCNLP 2021, pages 1–12.

Marco Valentino, Mokanarangan Thayaparan, Deborah
Ferreira, and André Freitas. 2022. Hybrid autore-
gressive inference for scalable multi-hop explana-
tion regeneration. In Proceedings of the AAAI Con-
ference on Artiﬁcial Intelligence, volume 36, pages
11403–11411.

Marco Valentino, Mokanarangan Thayaparan, and An-
dré Freitas. 2021. Uniﬁcation-based reconstruction
of multi-hop explanations for science questions. In
Proceedings of the 16th Conference of the European
Chapter of the Association for Computational Lin-
guistics: Main Volume, pages 200–211.

Thomas Wolf, Lysandre Debut, Victor Sanh, Julien
Chaumond, Clement Delangue, Anthony Moi, Pier-
ric Cistac, Tim Rault, Rémi Louf, Morgan Funtow-
icz, and Jamie Brew. 2019. Huggingface’s trans-
formers: State-of-the-art natural language process-
ing. CoRR, abs/1910.03771.

Zhengnan Xie, Sebastian Thiem, Jaycie Martin, Eliz-
abeth Wainwright, Steven Marmorstein, and Peter
Jansen. 2020. Worldtree v2: A corpus of science-
domain structured explanations and inference pat-
In Proceed-
terns supporting multi-hop inference.
ings of The 12th Language Resources and Evalua-
tion Conference, pages 5456–5473.

Weiwen Xu, Huihui Zhang, Deng Cai, and Wai Lam.
2021. Dynamic semantic graph construction and
reasoning for explainable multi-hop science ques-
In Findings of the Association
tion answering.
for Computational Linguistics: ACL-IJCNLP 2021,
pages 1044–1056.

• λ: 152

• λexp: 0.72

• λans: 0.99

• T : 8.77

• max epochs: 8

• gradient accumulation steps: 1

• learning rate: 1e-5

• weight decay: 0.0

• adam epsilon: 1e-8

• warmup steps: 0

• max grad norm: 1.0

• seed: 42

The hyperparameters adopted for BERT are as

follows:

• gradient accumulation steps: 1

• learning rate: 1e-5

• weight decay: 0.0

• adam epsilon: 1e-8

• warmup steps: 0

• max grad norm: 1.0

• seed: 42

We ﬁne-tuned using 4 Tesla V100 GPUs for 10
epochs in total with batch size 32 for Base and 16
for Large.

6.4 Data

WorldTree Dataset: Data can be obtained from:
http://cognitiveai.org/explanationbank/
ARC-Challenge Dataset: https://allenai.or
g/data/arc. Only used the Challenge split.

6 Appendix

The rest of the section details the hyper parameters,
code bases and datasets used in our approach to
reproduce our experiments.

6.1 External code-bases

• Differentiable Blackbox Combinatorial
Solvers Examples: https://github.com/m
artius-lab/blackbox-differentiation-
combinatorial-solvers

• Sentence Transformer code-base: https://
huggingface.co/sentence-transforme
rs/all-mpnet-base-v2

6.2

Integer Linear Programming
Optimization

The components of the linear programming system
is as follows:

• Solver: Gurobi Optimization https://www.
gurobi.com/products/gurobi-optimiz
er/

The hyperparatemers used in the ILP constraints:

• Maximum number of abstract facts (M ): 2

Infrastructures used:

• CPU Cores: 32

• CPU Model: Intel(R) Core(TM) i7-6700 CPU

@ 3.40GHz

• Memory: 128GB

6.3 Hyperparameters

For Diff-Comb Explainer we had to ﬁne-tune hyper-
parameters λ, λans, λexp, T . We ﬁne-tune for 50
epochs using the Adpative Experimentation Plat-
form with seed of 42.

The bounds of the hyperparameters are as fol-

lows:

• λ: [100, 300]

• λexp: [0.0, 1.0]

• λans: [0.0, 1.0]

• T : [1e − 2, 100]

The hyperparameters adopted for our approach

are as follows:

