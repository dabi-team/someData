Approximating meta-heuristics with homotopic recurrent neural networks

Alessandro Bay
Cortexica Vision Systems Ltd.
London, UK

Biswa Sengupta
Imperial College London
Cortexica Vision Systems Ltd.
London, UK

7
1
0
2

p
e
S
7

]
L
M

.
t
a
t
s
[

1
v
4
9
1
2
0
.
9
0
7
1
:
v
i
X
r
a

Abstract

Much combinatorial optimisation problems constitute a non-
polynomial (NP) hard optimisation problem, i.e., they can not
be solved in polynomial time. One such problem is ﬁnding the
shortest route between two nodes on a graph. Meta-heuristic
algorithms such as A∗ along with mixed-integer program-
ming (MIP) methods are often employed for these problems.
Our work demonstrates that it is possible to approximate solu-
tions generated by a meta-heuristic algorithm using a deep re-
current neural network. We compare different methodologies
based on reinforcement learning (RL) and recurrent neural
networks (RNN) to gauge their respective quality of approx-
imation. We show the viability of recurrent neural network
solutions on a graph that has over 300 nodes and argue that
a sequence-to-sequence network rather than other recurrent
networks has improved approximation quality. Additionally,
we argue that homotopy continuation – that increases chances
of hitting an extremum – further improves the estimate gen-
erated by a vanilla RNN.

Introduction
NP (non-deterministic polynomial time) hard problems are
the mainstay of some ﬁelds, from problems in graph the-
ory (routing/scheduling, etc.) to mathematical program-
ming (knapsack, 3d-matching, etc.). A widely studied
NP-hard problem is the Travelling Salesman Problem
(TSP) and its derivatives that include ﬁnding the shortest
routes between two nodes of a graph. Apart from mixed-
integer programming (MIP), solutions of such problems
are well approximated by meta-heuristic formulations such
as tabu search, simulated annealing, genetic algorithms
and evolutionary optimisations. Of notable mention are the
Dantzig-Fulkerson-Johnson algorithm (Dantzig, Fulkerson,
and Johnson 1954), branch-and-cut algorithms (Naddef and
Rinaldi 2001), neural networks (Ali and Kamoun 1993), etc.
The shortest-path problem is central to many real-life
scenarios, from inventory delivery (courier, food, vehicles,
etc.) to laying out circuitry on a printed circuit board. This
problem has evolved from a discrete integer programming
(bottom-up) problem to a data-based continuous optimi-
sation problem (top-down). Some studies have used spe-
cialised reinforcement learning (RL) algorithms – such as
q-learning – to approximate the optimal solutions for the

Copyright c(cid:13) 2018

shortest path problem (Boyan and Littman 1994). Along
with the resurgence of deep neural networks (DNN), tech-
niques that merge the scalability of deep neural networks
and the theoretical framework of Markov Decision Pro-
cesses (MDP) have emerged (aka. deep reinforcement learn-
ing (DRL)) (Bello et al. 2016; Dai et al. 2017). The intrin-
sic non-convexity of the loss function means both DRL and
DNN struggle to ﬁnd the global optimisers of the loss func-
tion. This becomes more of a problem as the graph size in-
creases.

For a deep neural network, ﬁnding the shortest routes be-
tween two points can be framed as a sequence learning prob-
lem. Indeed, in this paper, we show how synthetic routes
generated by an A∗ algorithm can be approximated using
recurrent neural networks. With the rise of the Seq2Seq (se-
quence to sequence) architecture, recurrent neural networks
based on Long Short Term Memory (LSTMs), Gated Re-
current Units (GRUs) and others can be readily used as a
sub-module. In this paper, we concentrate on three shortest
path ﬁnding algorithms on a reasonably sized graph of more
than 300 nodes. We use – (a) meta-heuristics based A∗ al-
gorithm (Hart, Nilsson, and Raphael 1968), (b) q-learning
based Q-routing algorithm (Boyan and Littman 1994) and
(c) a Seq2Seq recurrent neural network (Sutskever, Vinyals,
and Le 2014), amongst other vanilla recurrent architectures.
Moving on, we argue that the value of extremums generated
by the RNN could be further improved via the homotopic
continuation of the neural network’s loss function.

Methods

Datasets
The graph is based on the road network of Minnesota1.
Each node represents the intersections of roads while the
edges represent the road that connects the two points of
intersection. Speciﬁcally, the graph we considered has 376
nodes and 455 edges, as we constrained the coordinates of
the nodes to be in the range [−97, −94] for the longitude
and [46, 49] for the latitude, instead of the full extent of
the graph, i.e., a longitude of [−97, −89] and a latitude of
[43, 49], with a total number of 2,642 nodes.

1https://www.cs.purdue.edu/homes/dgleich/

packages/matlab_bgl

 
 
 
 
 
 
Algorithms
The A∗ meta-heuristics The A∗ algorithm is a best-ﬁrst
search algorithm wherein it searches amongst all of the pos-
sible paths that yield the smallest cost. This cost function
is made up of two parts – particularly, each iteration of the
algorithm consists of ﬁrst evaluating the distance travelled
or time expended from the start node to the current node.
The second part of the cost function is a heuristic that es-
timates the cost of the cheapest path from the current node
to the goal. Without the heuristic part, this algorithm opera-
tionalises the Dijkstra’s algorithm (Dijkstra 1959). There are
many variants of A∗; in our experiments, we use the vanilla
A∗ with a heuristic based on the Euclidean distance. Other
variants such as Anytime Repairing A∗ has been shown to
give superior performance (Likhachev, Gordon, and Thrun
2004). A∗ generated paths between two randomly selected
nodes are calculated. On an average, the paths are 19 hops
long and follow the distribution represented by histograms
in Figure 1.

Figure 1: Distribution of paths lengths. After selecting two
nodes uniformly at random, we compute the shortest paths
using the A∗ algorithm. The average path length is 19 hops.

De-centralised Q-routing Q-Learning is an Off-Policy
algorithm for temporal difference learning. Boyan and
Littman (1994) have formulated the Q-routing algorithm by
building a routing table based on node distance/time (q-
values). This algorithm utilises q-learning wherein the nodes
that are in the neighbourhood of the current node communi-
cate the expected future waiting time. In Q-routing, every
source node x has to choose the interim node that leads it to
the destination node d. Q-learning enables us to learn the ex-
pected travel time to d for each of the possible interim node
y. A q-table Qx is created for each node x that is updated at
discrete intervals t as,

Qx

t (d, y) = (1 − α) Qx

(cid:16)

+ α

bx
t + min

z

t−1 (d, y)
Qy

(cid:17)
t−1 (d, z)

.

(1)

Here, 0 < α < 1 is the learning rate and bt is the
time spent at node x before being sent off at time t. Q-
learning can thus estimate the value/cost (for Q-routing it is
the estimated transit time from the current node x to des-
tination d via node z) function V for being in state d as
V = min Qx (d, z). A greedy policy is then executed to
transact optimally.

Recurrent deep networks We utilised a variety of recur-
rent neural networks for shortest route path predictions:

• A vanilla RNN (Goodfellow, Bengio, and Courville

2016):

(cid:26)h(t) = tanh(Ax(t) + Bh(t − 1) + b)

y(t) = f (Ch(t) + c)

,

(2)

with weights {A, B, b, C, c, h0 = h(0)}, which takes as
input x(t) the current node, described by a one-hot repre-
sentation, and estimates the following one. The function
f can be a softmax, since we expect a probability distri-
bution on the following node, or a log softmax, that gives
more numerical stability during training. During the test
phase, we use the predicted node as input for the next time
step and compute two paths, one starting from the source
and one from the destination, that form an intersection to
obtain the shortest path.

• A Seq2Seq-based model (Sutskever, Vinyals, and Le
2014): We start with the tuple [source, destination] as an
input sequence, which is encoded by a vanilla RNN (Fig-
ure 2). The context vector, i.e., the encoding, is then de-
coded by another RNN, LSTM or a GRU (Cho et al. 2014)
to obtain the shortest path connecting the source to the
destination.

• A Seq2Seq-based model with attention (Bahdanau, Cho,
and Bengio 2014) enables us to focus on the encoded
states with varying extent.

• A vanilla RNN, trained with homotopy continuation (Vese
1999) – convolution of the loss with a Gaussian kernel, as
explained in the following section.

Homotopy continuation
In algebraic topology, two continuous functions in topologi-
cal space that can be transformed (deformed) from one to an-
other are known as homotopies. In the context of neural net-
works, such homotopies enable us to pose different instanti-
ations of the loss function with the aim of obtaining a global
minimum. Deep neural networks always have a non-convex
loss with no convergence guarantees; this makes learning
optimal parameters a numerical exercise. A homotopy con-
tinuation allows us to gradually deform the loss function to
a non-convex one from another that has a well-deﬁned min-
imiser. The minimisers from the i-th sub-problem are the
starting points for the subsequent (i + 1)-th subproblem.

Assume that g(x) = 0 represents a problem with global
optimizers whilst f (x) is another function where we have
no a priori knowledge of the optimizers. Then the homo-
topy function becomes h (x, t) ≡ (1 − t) g (x) + tf (x) :
t ∈ [0, 1]. The path traced out by the states are governed by

01020304050length of paths020406080100120number of pathsFigure 2: Sequence-to-Sequence architecture for approx-
imating the A∗ meta-heuristics. Here, the ﬁrst two mod-
ules on the left are the encoder while the last four represent
the decoded output, representing the shortest route between
Holborn and Bank. The network is trained using shortest
route snippets that have been generated using an A∗ algo-
rithm. W represents the context vector.

˙x = −[Jx (x, t)]−1 ∂h
∂t . If the Jacobian J is of full-rank, one
can guarantee that the path between the two functions are
continuously differentiable. In our treatise, we use a contin-
uation path deﬁned by the heat equation. In a similar vein
to Mobahi (2016), we convolve the loss function with an
isotropic Gaussian (the Weierstrass transform) of standard
deviation σ; this enables us to obtain objective functions
with varying smoothness. σ then becomes the continuation
parameter for the diffused cost function. We illustrate the
basic methodology using a single dimensional function, fol-
lowed by the diffusion of the RNN’s loss function.

(cid:19)

(cid:18) −x2
2σ2

√

(3)

exp

K(x, σ) =

1
2πσ
As an illustration, we construct a cost function that has
both a local and global minima as well as a saddle point,
i.e., y = sin(4πx)x2, constrained to the interval [−0.4, 0.6].
This function is shown in Figure 3 as a solid black line,
along with its diffused forms, obtained by the convolution
with the Gaussian kernel (Eqn. 3) with standard deviation
σ = {0.2, 0.1, 0.07, 0.03}. As one can see, for higher values
of σ (dashed red line), the smoothed function has only one
minimum, which can be easily attained via an arbitrary op-
timisation method. Then, starting from this solution, we can
avoid falling into the local minimum and eventually reach
the global extremiser for smaller values of σ (other dashed
lines). Repeating this until σ attains low values allows the
diffused function to match the original, thereby obtaining
the global minimum.

Diffused cost function In this section, we illustrate the
equations leading to the diffused loss function for the RNN.
Notably, given a set of S shortest sequences, each of length

Figure 3: Example of a diffused cost function. We choose a
cost function equal to sin(4πx)x2, which has both local and
global minima for x ∈ [−0.4, 0.6]. Applying the Weierstrass
transform we obtain smoothed forms for the cost function,
which converges to the original cost as σ decreases.

Ts, and a loss function d, we can state the minimisation
problem, of the cost function w.r.t. the weights in a vanilla
RNN (Eqn. 2) as,

min
A,B,b,C,c,h0

1
S

S
(cid:88)

Ts(cid:88)

s=1

t=1

d(f (zy(t)), y(t)),

(4)

s.t. zy(t) = C tanh(zh(t)) + c

zh(t) = Ax(t) + Bh(t − 1) + b,

where h(0) = h0 is the initial hidden state.

Moreover, if we consider zy and zh as independent vari-
ables, we can write the following unconstrained Lagrangian,

min
A,B,b,C,c,h0,Zh,Zy

(cid:18)

1
S

S
(cid:88)

Ts(cid:88)

s=1

t=1

d(f (zy(t)), y(t))

+ λ

p(C tanh(zh(t)) + c − zy(t))

(5)

+ p(Ax(t) + Bh(t − 1) + b − zh(t))

,

(cid:19)

where all the vectors zh(t) and zy(t) are collected in the
columns of the matrices Zh and Zy, respectively, λ is a reg-
ularization parameter (we will use λ = 0.01), and p(·) is a
penalty function.

Since we consider f (·) = log softmax(·), then the natural
choice for the loss d is the negative log-likelihood, while for
the penality function p, we choose the squared Euclidean
norm. Therefore, Eqn. 5 now becomes

HolbornRNN/LSTM/GRUEncoderRNN/LSTM/GRUDecoderBankHolbornWChanceryLaneSt Paul’sBank-0.200.20.4x-0.2-0.100.10.20.30.4y = sin(4 : x) x2original<=0.2<=0.1<=0.07<=0.03min
A,B,b,C,c,h0,Zh,Zy

1
S

S
(cid:88)

Ts(cid:88)

s=1

t=1

−f (zy(t))(cid:62)y(t)

(cid:18)

(cid:107)C tanh(zh(t)) + c − zy(t)(cid:107)2
2

+ λ

(6)

+ (cid:107)Ax(t) + Bh(t − 1) + b − zh(t)(cid:107)2
2

(cid:19)

.

Finally, deﬁning the convolution of the non-linear func-

tions tanh and f with the Gaussian kernel (Eqn. 3) as
tanhσ(·) := tanh(·) ∗ K(·, σ)
fσ(·) := f (·) ∗ K(·, σ),
respectively, we can derive the constrained diffused cost
w.r.t. the weights of the RNN, as

1
S

min
A,B,b,C,c,h0
(cid:18)

+ λ

(cid:107)Cdiag(

S
(cid:88)

Ts(cid:88)

−fσ(zy(t))(cid:62)y(t)

t=1

s=1
(cid:113)

(tanh2)σ(zh(t)))(cid:107)2
F

− (cid:107)Cdiag(tanhσ(zh(t)))(cid:107)2
F
+ σ2outDim(cid:107) tanhσ(zh(t))(cid:107)2
2

(7)

(cid:113)

(tanh2)σ(zh(t − 1)))(cid:107)2
F

+ (cid:107)Bdiag(
− (cid:107)Bdiag(tanhσ(zh(t − 1)))(cid:107)2
F

+ σ2stateDim(cid:107) tanhσ(zh(t − 1))(cid:107)2
2

s.t. zy(t) = C tanhσ(zh(t)) + c

(cid:19)
,

zh(t) = Ax(t) + B tanhσ(zh(t − 1)) + b,
where diag(v) denotes a diagonal matrix with the vector
v on the diagonal, (cid:107) · (cid:107)F is the Frobenius norm of a ma-
trix, and stateDim and outDim represent the number of
neurons in the hidden state and of the output, respectively.
Notice that we have also used the identities that convolu-
tion of (cid:107)Af (x) + b(cid:107)2
2 with K(x, s) is equal to (cid:107)Afσ(x) +
b(cid:107)2
F and
(x(cid:62)y)2 ∗ K(x, σ) = (x(cid:62)y)2 + σ2(cid:107)y(cid:107)2

F − (cid:107)A diag(fσ(x))(cid:107)2
2 (Mobahi 2016).

2 + (cid:107)A diag((cid:112)(f 2)σ(x))(cid:107)2

Approximation of activation functions Another crucial
component of this method is the computation of the diffused
non-linearities for the considered RNN. Following (Mobahi
√
π
2016) convolving tanh(x) ≈ erf(
2 x) with K(x, σ), we
obtain

tanhσ(x) = tanh(x) ∗ K(x, σ)

= erf

= erf

(cid:18) √
π
2
(cid:32) √

π
2
(cid:32)

(cid:19)

x

∗ K(x, σ)

(cid:33)

x
(cid:112)1 + π

2 σ2
(cid:33)

x
(cid:112)1 + π

2 σ2

= tanh

.

(8)

However, the output non-linearity that we use is the log
softmax, which cannot be analytically convolved. To address
this problem, our ﬁrst attempt was the numerical compu-
tation of the convolved function, but in this case, we ob-
tained numerical errors, especially on the boundaries. This
makes the approximation less accurate. To sidestep this phe-
nomenon, we consider a linear interpolation y = mx + q
of the central interval, obtaining the following slope, which
depends on the standard deviation σ:

(cid:18)

m =

1 −

(cid:19)

1
π

exp(−πσ2) +

1
π

,

(9)

and the shift is simply the constant q = − log((cid:80) exp(x)).
As we can see in Figure 4, some errors affect the numer-
ical convolution, especially on the boundaries (dash-dotted
lines). Thus, a linear equation with slope as in Eqn. 9 is more
accurate and converges to the original logsoftmax function
(solid black line) as σ becomes smaller (in Figure 4 we il-
lustrate the results for σ = {5, 0.7, 0.3, 0.1}).

Figure 4: Approximation for the convolved log softmax.
Since log softmax function cannot be analytically convolved
with the Gaussian kernel, we approximate this operation nu-
merically. It is apparent that the quality of the approxima-
tion is affected by the numerical errors, especially on the
boundaries (dash-dotted lines). Therefore, we constrain the
approximation via linear interpolation of the central interval
(solid lines) at the edges.

Table 1 illustrates the diffused forms of the most popular

activation functions.

Results
For the graph of Minnesota with 376 nodes and 455 edges,
we generated 3,000 shortest routes between two randomly
picked nodes using the A∗ algorithm. We used these routes
as the training set for the Q-routing and the RNN algorithms
using a 67-33% training-test splits.

For the Q-routing algorithm, we set the learning coefﬁ-
cient α = 0.5 and use 1,000 iterations for each path to up-
date the q-table and enable the algorithm to converge. We
obtain an accuracy on the shortest paths of 70% for the test
data-set, and 97% of the test paths reach the destination, al-
beit they are not necessarily the shortest ones.

-0.8-0.6-0.4-0.200.20.40.60.8x-5.6-5.4-5.2-5-4.8-4.6-4.4-4.2-4-3.8y = logsoftmax(x)originalapprox conv <=5numerical conv <=5approx conv <=0.7numerical conv <=0.7approx conv <=0.3numerical conv <=0.3approx conv <=0.1numerical conv <=0.1function

error

tanh

sign

relu

original

erf(αx)

tanh(x)






+1
0
−1

if x > 0
if x = 0
if x < 0

max(x, 0)

logsoftmax

x − log

(cid:18)

(cid:19) (cid:18)

(cid:80) exp(x)

(cid:18)

erf

tanh

diffused

αx√
(cid:18)

1+2(ασ)2

x√
1+ π

2 σ2

(cid:19)

(cid:19)

erf

(cid:17)

(cid:16) x√

2σ

σ√

2π

(cid:0)1 − 1

π

(cid:17)

(cid:16) −x2
exp
2 x
2σ2
(cid:1) exp (cid:0)−πσ2(cid:1) + 1

+ 1

(cid:16)

(cid:19)

π

1 + erf

(cid:16) x√

(cid:17)(cid:17)

2σ
x − log((cid:80)(exp(x)))

Table 1: List of diffused forms (Weierstrass transform).
We report the most popular non-linear activation functions
along with their diffused form. This is obtained by convolv-
ing the function with the heat kernel K(x, σ). This table ex-
tends the work in (Mobahi 2016) by an analytic approxima-
tion of the log softmax function.

On the other hand, we choose a hidden state with 256
units for the RNN-based methods and run the training for
200 epochs updating the parameters with an Adam opti-
misation scheme (Kingma and Ba 2014) with parameters
β1 = 0.9 and β2 = 0.999, starting from a learning rate
equal to 1e-3.

method
Q-routing
RNN vanilla
RNN intersect
RNN withDiff
Seq2Seq noAttn
Seq2Seq withAttn
Seq2Seq intersect noAttn
Seq2Seq intersect withAttn

shortest
70%
33%
60%
63%
40%
73%
59%
78%

successful
97%
33%
98%
99%
40%
73%
83%
88%

Table 2: Results on the Minnesota graph. Percentage of
shortest path and successful paths (that are not necessarily
shortest) are shown for a RL algorithm (i.e., Q-routing) and a
wide-variety of RNNs, where intersect means the computa-
tion of the shortest path as intersection of the predicted path
and the one obtained from the destination to the source node.
All scores are relative to an A∗ algorithm, that achieves a
shortest path score of 100%.

The prediction accuracy on the test data-set is shown in
Table 2. A vanilla RNN predicting the next node gives poor
results; especially we notice that most of the time it cannot
even predict a path from the source to the destination node.
Nevertheless, on occasions when it reaches the destination,
the path is always the shortest. A ﬁrst attempt to improve its
performance was simply to compute two paths – one from
the source to the destination node and another one from the
destination to the source, and intersecting the two paths to
obtain a route between the source and the destination. Such
a scheme improved the performance to an accuracy equal
to 98% for paths linking source and destination and to 60%
for shortest ones. Then, we train a vanilla RNN through an
homotopy continuation method (see Eqn. 7) varying σ ∈
{30, 5, 1, 0.0001}, which gives a further improvement (i.e.,
63% and 99%) and converges after only 80 epochs (instead

of 200 epochs).

Alternatively, we can also alter the architecture of the
RNN, getting inspiration from the recent success of the
sequence-to-sequence model (Sutskever, Vinyals, and Le
2014). We involve it, encoding the [source, destination] tu-
ple via a vanilla RNN and then decoding the context vec-
tor into the shortest path sequence. We implemented the de-
coder in two different ways: it can be either a vanilla RNN
or a GRU, or a vanilla RNN or a GRU with attention (Bah-
danau, Cho, and Bengio 2014). In particular, by embedding
attention, we can outperform the accuracy of the Q-routing
algorithm on the shortest paths (73%), especially if we com-
pute two paths and intersect them (78%).

Therefore, the most accurate algorithm for ﬁnding the
shortest path is the Seq2Seq model with attention, where
during the test phase we evaluate the intersection between
two paths: one from the source to the destination node and
another from the destination to the source node. On the other
hand, a vanilla RNN, when trained with diffusion, computes
successful paths from source to destination, albeit they are
not necessarily the shortest ones. This suggests that further
improvements can be obtained by training a Seq2Seq model
with diffusion.

Discussion
In this paper, we illustrate that recurrent neural networks
have the ﬁdelity in approximating routes generated from an
A∗ algorithm. As the node size increases, Seq2Seq models
have increased ﬁdelity compared to vanilla recurrent neural
networks or for that matter a vanilla q-learning based rout-
ing algorithm. Our work is yet another testament to the util-
ity that deep recurrent networks have in approximating so-
lutions of NP hard problems.

When parameter evolution is constrained to evolve ac-
cording to a heat equation, it has resulted in superior pos-
terior estimates when utilised in a Bayesian belief updating
scheme (Cooray et al. 2017). Similarly, combining annealed
importance sampling with Riemannian samplers have also
shown promise in producing better posterior distributions
(Penny and Sengupta 2016). Homotopy continuation pro-
vides a rigorous theoretical foundation for the afore men-
tioned results. It is well-known that the convex envelope of
a non-convex loss function can be obtained using an evolu-
tion equation, i.e., a (non-linear) partial differential equation
(PDE) (Vese 1999). The heat equation simply provides an
afﬁne solution to this non-linear PDE. One important direc-
tion for future work is, therefore, tackling the original non-
linear PDE with computationally efﬁcient algebraic or geo-
metric multi-grid methods (Heroux and Willenbring 2012;
Sundar et al. 2012).

We have used Q-routing as a benchmark to learn the
action-value pair that gives us the expected utility of taking
a prescribed action; in the last few years, many other archi-
tectures have evolved such as the deep-Q-network (Mnih et
al. 2015), duelling architecture (Wang et al. 2015), etc. – it
remains to be seen how such deep reinforcement learning
architectures cope with the problem at hand. Another vital
direction to pursue is the scalability of inverse reinforcement
learning algorithms (Ziebart et al. 2008) wherein given the

policy and the system dynamics the aim is to recover the
reward function.

An important issue arising from using recurrent neural
networks for computing shortest path is to memorise long
sequences, such as routes that range hundreds of nodes.
LSTM (Hochreiter and Schmidhuber 1997) alleviated some
of the problems; the other proposition has been to use the
second-order geometry as has been long proposed (LeCun
et al. 2012). Other efforts have gone towards using evo-
lutionary optimisation algorithms such as CoDeepNEAT
for ﬁnessing the neural network architecture (Miikkulainen
et al. 2017). Similarly, Neural Turing Machines (Graves,
Wayne, and Danihelka 2014) are augmented RNNs that have
a (differentiable) external memory that they can selectively
read/write, enable the network to store the latent structure
of the sequence. Attention networks, as has been used here,
enable the RNNs to attend to snippets of their inputs (cf.
in conversational modelling (Vinyals and Le 2015)). Never-
theless, for the long-term dependencies that shortest paths
in large graphs have, these methods are steps towards alle-
viating the central problem of controlling spectral radius in
recurrent neural networks.

The Q-routing algorithm reduces the search-space by con-
straining the search to the connected neighbours of the cur-
rent node. Whereas, the RNN variants have a large state-
space to explore. The Seq2Seq architecture, therefore, has
at least two orders of magnitude increase in its exploration
space. In future, the accuracy of the RNN variants can be
increased by constraining the search space to the neighbour-
hood of the current node.

In a real-world scenario, computing the shortest path be-
tween two nodes of a graph is also constrained by the com-
putational time of the algorithm. Thus, the deep recurrent
networks, as utilised here, are faced with two objectives –
ﬁrst, to reduce the prediction error and second to reduce the
computational effort. In general, it is quite uncommon that
an obtained solution can be obtained that minimises both
objectives. In lieu, one can concentrate effort on achiev-
ing solutions on the Pareto front. Utilising, Bayesian op-
timisation (Brochu, Cora, and De Freitas 2010) for such
multi-objective optimisation problem is a path forward for
research that is not only theoretically illuminating but also
commercially useful.

Acknowledgments

BS is thankful to the Issac Newton Institute for Mathemat-
ical Sciences for hosting him during the “Periodic, Almost-
periodic, and Random Operators” workshop.

References

Ali, M. K. M., and Kamoun, F. 1993. Neural networks
for shortest path computation and routing in computer net-
IEEE Transactions on Neural Networks 4(6):941–
works.
954.
Bahdanau, D.; Cho, K.; and Bengio, Y. 2014. Neural ma-
chine translation by jointly learning to align and translate.
arXiv preprint arXiv:1409.0473.

Bello, I.; Pham, H.; Le, Q. V.; Norouzi, M.; and Bengio,
S. 2016. Neural combinatorial optimization with reinforce-
ment learning. arXiv preprint arXiv:1611.09940.
Boyan, J. A., and Littman, M. L. 1994. Packet routing in
dynamically changing networks: A reinforcement learning
approach. In Advances in neural information processing sys-
tems, 671–678.
Brochu, E.; Cora, V. M.; and De Freitas, N. 2010. A tu-
torial on Bayesian optimization of expensive cost functions,
with application to active user modeling and hierarchical re-
inforcement learning. arXiv preprint arXiv:1012.2599.
Cho, K.; Van Merri¨enboer, B.; Bahdanau, D.; and Ben-
2014. On the properties of neural machine
gio, Y.
arXiv preprint
translation: Encoder-decoder approaches.
arXiv:1409.1259.
Cooray, G. K.; Rosch, R.; Baldeweg, T.; Lemieux, L.; Fris-
ton, K.; and Sengupta, B. 2017. Bayesian belief updating
of spatiotemporal seizure dynamics. In ICML Time Series
Workshop.
Dai, H.; Khalil, E. B.; Zhang, Y.; Dilkina, B.; and Song, L.
2017. Learning combinatorial optimization algorithms over
graphs. arXiv preprint arXiv:1704.01665.
Dantzig, G.; Fulkerson, R.; and Johnson, S. 1954. Solution
of a large-scale traveling-salesman problem. Operations Re-
search 2:393–410.
Dijkstra, E. W. 1959. A note on two problems in connexion
with graphs. Numerische mathematik 1(1):269–271.
Goodfellow, I.; Bengio, Y.; and Courville, A. 2016. Deep
Learning. MIT Press.
Graves, A.; Wayne, G.; and Danihelka, I. 2014. Neural
turing machines. arXiv preprint arXiv:1410.5401.
Hart, P. E.; Nilsson, N. J.; and Raphael, B. 1968. A for-
mal basis for the heuristic determination of minimum cost
paths. IEEE transactions on Systems Science and Cybernet-
ics 4(2):100–107.
2012. A new
Heroux, M. A., and Willenbring, J. M.
overview of the trilinos project. Scientiﬁc Programming
20(2):83–88.
Hochreiter, S., and Schmidhuber, J. 1997. Long short-term
memory. Neural Comput. 9(8).
Kingma, D., and Ba, J. 2014. Adam: A method for stochastic
optimization. arXiv preprint arXiv:1412.6980.
LeCun, Y. A.; Bottou, L.; Orr, G. B.; and M¨uller, K.-R. 2012.
Efﬁcient backprop. In Neural networks: Tricks of the trade.
Springer. 9–48.
Likhachev, M.; Gordon, G. J.; and Thrun, S. 2004. ARA*:
Anytime A* with provable bounds on sub-optimality. In Ad-
vances in Neural Information Processing Systems, 767–774.
Miikkulainen, R.; Liang, J.; Meyerson, E.; Rawal, A.; Fink,
D.; Francon, O.; Raju, B.; Navruzyan, A.; Duffy, N.; and
Hodjat, B. 2017. Evolving deep neural networks. arXiv
preprint arXiv:1703.00548.
Mnih, V.; Kavukcuoglu, K.; Silver, D.; Rusu, A. A.; Ve-
ness, J.; Bellemare, M. G.; Graves, A.; Riedmiller, M.;

Fidjeland, A. K.; Ostrovski, G.; et al.
2015. Human-
level control through deep reinforcement learning. Nature
518(7540):529–533.
Mobahi, H. 2016. Training recurrent neural networks by
diffusion. arXiv preprint arXiv:1601.04114.
Naddef, D., and Rinaldi, G. 2001. The vehicle routing prob-
lem. Philadelphia, PA, USA: Society for Industrial and Ap-
plied Mathematics. chapter Branch-and-cut Algorithms for
the Capacitated VRP, 53–84.
Penny, W., and Sengupta, B. 2016. Annealed importance
sampling for neural mass models. PLoS computational bi-
ology 12(3):e1004797.
Sundar, H.; Biros, G.; Burstedde, C.; Rudi, J.; Ghattas, O.;
and Stadler, G. 2012. Parallel geometric-algebraic multigrid
In Proceedings of the
on unstructured forests of octrees.
International Conference on High Performance Computing,
Networking, Storage and Analysis, 43. IEEE Computer So-
ciety Press.
Sutskever, I.; Vinyals, O.; and Le, Q. V. 2014. Sequence
to sequence learning with neural networks. In Advances in
neural information processing systems, 3104–3112.
Vese, L. 1999. A method to convexify functions via curve
evolution. Communications in partial differential equations
24(9-10):1573–1591.
Vinyals, O., and Le, Q. 2015. A neural conversational
model. arXiv preprint arXiv:1506.05869.
Wang, Z.; Schaul, T.; Hessel, M.; Van Hasselt, H.; Lanc-
tot, M.; and De Freitas, N. 2015. Dueling network ar-
chitectures for deep reinforcement learning. arXiv preprint
arXiv:1511.06581.
Ziebart, B. D.; Maas, A. L.; Bagnell, J. A.; and Dey, A. K.
2008. Maximum entropy inverse reinforcement learning. In
AAAI, volume 8, 1433–1438. Chicago, IL, USA.

