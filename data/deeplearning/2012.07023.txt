InferCode: Self-Supervised Learning of Code Representations
by Predicting Subtrees

Nghi D. Q. Bui
School of Information Systems
Singapore Management University
dqnbui.2016@phdcs.smu.edu.sg

Yiyun Yu
School of Computing & Communications
The Open University, UK
y.yu@open.ac.uk

Lingxiao Jiang
School of Information Systems
Singapore Management University
lxjiang@smu.edu.sg

0
2
0
2
c
e
D
5
1

]
E
S
.
s
c
[

2
v
3
2
0
7
0
.
2
1
0
2
:
v
i
X
r
a

Abstract—Learning code representations has found many uses
in software engineering, such as code classiﬁcation, code search,
code comment generation, and bug prediction. Although repre-
sentations of code in tokens, syntax trees, dependency graphs,
paths in trees, or the combinations of their variants have been
proposed, existing learning techniques have a major limitation
that these models are often trained on datasets labeled for
speciﬁc downstream tasks, and the code representations may
not be suitable for other tasks. Even though some techniques
generate representations from unlabeled code, they are far from
satisfactory when applied to downstream tasks. To overcome the
limitation, this paper proposes InferCode, which adapts the self-
supervised learning idea from natural language processing to
the abstract syntax trees (ASTs) of code. The key novelty lies
in the training of code representations by predicting subtrees
automatically identiﬁed from the context of ASTs. With Infer-
Code, subtrees in ASTs are treated as the labels for training
the code representations without any human labeling effort or
the overhead of expensive graph construction, and the trained
representations are no longer tied to any speciﬁc downstream
tasks or code units.

We have trained an instance of InferCode model using Tree-
Based Convolutional Neural Network (TBCNN) as the encoder
of a large set of Java code. This pre-trained model can then
be applied to downstream unsupervised tasks such as code
clustering, code clone detection, cross-language code search,
or be reused under a transfer learning scheme to continue
training the model weights for supervised tasks such as code
classiﬁcation and method name prediction. Comparing to prior
techniques applied to the same downstream tasks, such as
code2vec, code2seq, ASTNN, using our pre-trained InferCode
model higher performance results are achieved with a signiﬁcant
margin for most of the tasks, including those involving different
programming languages. The implementation of InferCode and
the trained embeddings are made available at the anonymous
link: https://github.com/ICSE21/infercode.

I. INTRODUCTION

Learning code representations (a.k.a. embeddings) and
building a prediction model for programs have been found
useful in many software engineering tasks, such as classifying
program functionality [1, 2], code search [3, 4, 5], code
comment generation [6, 7, 8], predicting bugs [9, 10, 11],
translating programs [12, 13], etc. While offering promising
performance for the tasks, the prior learning techniques have
major limitations that hinder their performance and generaliz-
ability.
• Most of the code representations and program models are
trained in a (semi-)supervised learning paradigm. Human

needs to manually label the data for a speciﬁc downstream
task, or engineer some special intermediate representations
and corresponding training techniques for the task, and the
code representations are trained with respect to the speciﬁc
task. Not to mention the efforts needed to provide many
labels and specially engineered features, such trained code
representations are speciﬁc to one particular task and may
not be easily transferred to other tasks.

• Even though there are techniques [8, 14] aiming to produce
code representations that are transferable to different tasks,
their trained code representations are only for some ﬁxed
units of code, such as tokens, statements, and functions, and
are not ﬂexible to produce embeddings for different code
units. Such techniques may miss useful information across
different kinds of code units, and the trained representa-
tions may not perform well for various downstream tasks
either. Some other techniques based on graph embeddings
[15, 16, 17] share a similar drawback and in addition need
the overheads of graph construction which may introduce
inaccurate information in the graphs.
Such limitations have been illustrated in a recent study:
Kang et al. [18] show that
the pre-trained code2vec [8]
representation does not perform well for other tasks when it
was trained speciﬁcally for the method-name prediction task.
Towards addressing the limitations, the aim of this paper is
to develop a new technique for learning code representations,
and it should be: (1) trainable without any manual human
labeling, (2) ﬂexible in producing embeddings for any code
unit that can be parsed into syntax trees, and (3) general
enough so that its trained representations for code can perform
well for various downstream tasks.

We have two pillars that support the realization of our aim.
One is the large amount of source code available on public
code hosting platforms, such as Github, Bitbucket, Gitlab.
Although the code often lacks accurate labels for downstream
tasks, the syntax of the code itself can be relatively easily
checked by parsers. It is desirable to leverage such unlabeled
data to pre-train code representations reusable for building
various program prediction models for downstream tasks.

The second supporting pillar

is the advances of self-
supervised learning in the machine learning community [19,
20, 21, 22, 23]. Such techniques enable the training of neural
networks without the need for human labels. Usually, a self-

 
 
 
 
 
 
supervised learning technique reformulates an unsupervised
learning problem as a supervised one by automatically gen-
labels from existing (unlabeled) data. The
erating virtual
self-supervised task, also known as a pretext
task, guides
us to a supervised loss function. While minimizing the loss
function for the pretext task, the technique can also produce
intermediate representations for the data corresponding to the
virtual label. Because the pretext task can be trained using any
data, it is expected that such representations can carry good
information of diverse data and be beneﬁcial to a variety of
downstream tasks. This notion of self-supervised learning is
very suitable for our aim. Little effort has been invested in
the literature to exploit the uses of self-supervised learning
for code representation learning. Although some recent work,
such as [21], presents a self-supervised learning paradigm for
program repair, it is designed speciﬁcally for the speciﬁc task.
Our key idea is thus to train a pretext task suitable for
any source code. Different from self-supervised learning in
natural language processing and visual learning areas that use
words or object regions as labels, we utilize the fact that it
is relatively easy to obtain the abstract syntax tree (AST) of
any syntactically valid code snippet via parsers and it is also
easy to identify all the subtrees in ASTs, and automatically
use each subtree as the label for the pretext task to predict
the probability of the subtree appearing in a particular AST.1
Fig. 1 shows an example of this intuition. The two code
snippets implement the same functionality, bubble sort. If we
view these two code snippets as two ASTs, there are many
similar subtrees between these two AST. For example, the
subtree that represents the conditional expression arr[j]
> arr[j+1] of the left snippets is similar to arr[i] >
arr[i+1] although the textual information is quite different.
This means that if we can exploit such information, we do
not need any label to build a representation learning model
for source code. Also different from recent uses of neural
document embedding models (e.g., doc2vec [24, 25]) for
source code (e.g., [26, 27, 28, 29, 30, 31]), our technique learns
subtrees in ASTs without the overheads and accuracy losses
of constructing customized graphs, while they learn mostly
code tokens and node types, although we are all inspired by
the same idea of doc2vec. We also provide an alternative to
graph-based [15, 32] or execution traces-based [33] embedding
techniques as we believe ASTs are more readily available for
all kinds of programming languages and may have contained
all the code information (although some are hidden).

Based on the key idea, we propose InferCode, a self-
supervised learning technique for source code by predicting
syntax subtrees. As far as we know, we are the ﬁrst to apply
the notation of self-supervised learning to syntax subtrees and
can produce code representations for any syntactically valid

Fig. 1. Example of two code snippets that implement bubble sort in Java that
share similar ﬁne-grained code elements.

code snippet without the need of human labeling.
• InferCode can serve as an encoder that maps any parsable
code snippet into a vector representation (embedding), and
this vector can be used for various downstream tasks, such
as code clustering, clone detection, and code search.

• InferCode can serve as a pre-trained model and its weights
can be reused in downstream training of the models for
supervised learning tasks, which can speed up the training
and alleviate the issue of lacking data for a particular task.
• We implement InferCode on top of the ASTs produced by
SrcML [34]. It provides a combined vocabulary of AST
node types for multiple languages (e.g., Java, C, C++, C#),
which implies that our InferCode can be polyglot, producing
code representations suitable for tasks involving different
languages, such as cross-language code search, as long as
the ASTs for a code snippet can be recognized by SrcML.
We have trained an instance of InferCode based on a large
set of Java code and evaluated the usefulness of the pretrained
code representations in ﬁve downstream tasks, three of which
are unsupervised (code clustering, code clone detection via
similarity measurement, cross-language code search, two are
supervised (code classiﬁcation and method name prediction).
For the three unsupervised tasks, we utilize the vectors produce
by InferCode and different vector similarity metrics to achieve
the goal of each task: For code clustering, our results using
InferCode outperform the best baseline (Code2vec) by 12% in
term of Adjusted Rand Index; For code clone detection, our
results outperform the best baseline (Code2vec) by 15% in
term of F1 score; For cross-language code search, our results
outperform the best baseline (CLIR) on 13% (on average for
multiple languages setting) in term of Mean Reciprocal Rank.
For the two supervised tasks, we utilize the weights of the
pre-trained model from InferCode to ﬁne-tune the speciﬁc
prediction model for each task: our results using the ﬁne-
tuning process increases the performance of TBCNN for code
classiﬁcation by 4% in term of accuracy, which is comparable
to ASTNN, the state-of-the-art model for code classiﬁcation,
, and increase the performance TBCNN for method name
prediction by 8%, which is comparable to code2seq, a state-
of-the-art model for method name prediction.

II. RELATED WORK

1An underlying assumption, for such trained representations to capture
code meanings, is that code snippets with the same semantics should involve
some syntactically similar code elements. Even though two pieces of code
implementing the same functionality can be syntactically very different, there
could still be some ﬁne-grained elements in the code or other pieces of code
that use these two that are syntactically similar, when the code base is large.

Self-Supervised Learning has made great progress recently
for visual data [35, 36, 37, 38, 39, 40]: Gidaris et al. [36]
proposed a method to generate different viewpoints of an
image by a number of rotations on certain degrees at random
and formulate the learning part as a multi-class classiﬁcation

2

problem over the rotations. This pretext task drives the model
to learn semantic concepts of objects as the parameters of
the CNN image encoder; Zhang et al. [37] proposed to use
colorization as the pretext task by giving colours to a grayscale
input image to map this image to a distribution over quantized
color value outputs.

There has been tremendous effort to explore self-supervised
learning in Natural Language Processing research for quite
a while [24, 25, 41, 42]. Word2vec [24] is a form of self-
supervised learning, which aims to learn good representation
for words by taking a small chunk of the text of certain
window size. Doc2vec [25] shares the same principle with
word2vec which aims to use a document to predict the words
inside it so that similar documents will have similar embed-
dings; Skip-thought vectors [41] builds a language model by
predicting the neighbouring sentences of a center sentence;
BERT [42] advances language models by masking the words
in a text randomly in order to predict them.

Deep Learning Models of Code: There has been a huge
in applying deep learning techniques for software
interest
engineering tasks such as program functionality classiﬁca-
tion [43, 44], bug localization [45, 46], function name predic-
tion [47], code clone detection [44], program refactoring [6],
program translation [12], and code synthesis [48]. Allamanis
et al. [49] extend ASTs to graphs by adding a variety of
code dependencies as edges among tree nodes, intended to
represent code semantics, and apply Gated Graph Neural
Networks (GGNN) [50] to learn the graphs; Code2vec [8],
Code2seq [14], and ASTNN [44] are designed based on
splitting ASTs into smaller ones, either as a bag of path-
contexts or as ﬂattened subtrees representing individual state-
ments. They use various kinds of Recurrent Neural Network
(RNN) to learn such code representations. Unfortunately, there
is little effort that invests to design the source code model
with unlabeled data: Yasunaga and Liang [21] presents a self-
supervised learning paradigm for program repair; Survey on
code embeddings [27, 30] presents evidence to show that there
is a strong need to alleviate the requirement of labeled data for
code modeling and encourage the community to invest more
effort in the methods on learning source code with unlabeled
data.

Our approach differs from existing ways to reuse the pre-
trained code learning model: Kang et al. [18] reuse the token
embeddings from Code2vec for downstream tasks only to
ﬁnd that lower performance than simpler word embedding
methods like Word2vec. In contrast, we use the weights of
the pretrained model and the code vector (cid:126)v produced by the
encoder instead of the token embeddings.

III. PRELIMINARIES

A. Source Code Representation Learning

Source code representation learning usually contains the
following two phases: (1) represent a code snippet into an
intermediate representation (IR), such as token streams, ASTs,
AST paths or graphs; and (2) design a neural network suitable

3

to process such intermediate representations. The neural net-
work can also be called as an encoder. The encoder receives
the code IR and maps it into a code vector embedding (cid:126)v
(usually a combination of various kinds of code elements),
then (cid:126)v can be fed into the next layer(s) of a learning system
and trained for an objective function of the speciﬁc task of the
learning system. For example, in Code2vec [8], (cid:126)v is a combina-
tion of different AST paths. In GGNN [49] or TBCNN [43],
(cid:126)v is a combination of AST nodes. A trained model, either
on supervised learning or self-supervised learning task can
produce the (cid:126)v. In our work, we will evaluate how a (cid:126)v trained
on a self-supervised learning objective over a large set of
unlabeled data can be useful for different tasks.

B. Neural Document Embedding Models

Doc2vec [25] is an extension to word2vec [24]. Doc2vec
uses an instance of the skip-gram model called paragraph
vector-distributed bag of words (interchangeably referred as
doc2vec skip-gram) that is capable of learning representations
of word sequences of arbitrary lengths, such as sentences,
paragraphs and even whole large documents. More speciﬁ-
cally, given a set of documents {d1, d2, ...dn} and a sequence
of words {..., wij, ...} sampled from the document di, skip-
gram learns a D-dimensional embeddings of the document
di and each word wij sampled, i.e., (cid:126)vi, (cid:126)vij ∈ RD, respec-
tively. The model works by considering a word wij to be
occurring in the context of document di and tries to maximize
the following log likelihood: (cid:80)
j log P r(wij|di), where the
exp((cid:126)vi·(cid:126)vij )
w∈V exp((cid:126)vi· (cid:126)w) , where V
probability P r(wij|di) is deﬁned as
is the vocabulary of all the words across all documents.

(cid:80)

In this paper, we consider ASTs analogous to documents
and subtrees in ASTs analogous to words in documents, and
adapt the idea of document embedding to learn embeddings of
ASTs of any size by using an encoder that can encode ASTs
of any parsable code snippet.

C. Self-supervised Learning Formulation

The goal of self-supervised learning is to train an encoder
E such that E can map an object into a vector representation
(embedding). In our case, the embedding (cid:126)v is for the AST
representation T of a code snippet C. Training the encoder
E is to learn its parameters (or weights) so that E is able
to produce the embeddings for the code snippets such that
the vectors for the snippets having similar syntactical and
semantic information will be close in the vector space. In
visual learning, Convolutional Neural Networks are usually
chosen as the encoder for images. In NLP, Recurrent Neural
Networks, or recently, BERT, is used as the encoder for text
sequences. In our case, we choose Tree-based CNN as the
source code encoder as it has been successfully used before
[43, 51, 52, 53] and justiﬁed further in Section VIII.

Given a dataset X, for each data Xi

there is a
corresponding pseudo label Pi automatically generated for a
predeﬁned pretext task without involving any human annota-
tion. Given a set of n training data D = {Pi}n
i=1, the aim is to
(cid:80)n
minimize the loss function: loss(D) = 1
i=1 loss(Xi, Pi).
n

in X,

Fig. 2. a) Doc2vec’s skipgram model - Given a document d, it samples c words and considers them as co-occurring in the same context of d to learn d’s
representation; (b) InferCode - Given an AST T , it samples s subtrees from T and uses them as the context to learn T ’s representation.

We can easily identify subtrees in ASTs as the pseudo labels P
automatically without human annotations so that our learning
technique can be self-supervised.

IV. APPROACH DETAILS

A. Overview

Figure 2 presents a high-level view of our InferCode ap-
proach as an analogy to Doc2vec by treating an entire AST as
a document and treating the subtrees as words. Given a set of
ASTs {T1, T2, ...Tn}, and a set of all subtrees {..., Tij, ...}
of Ti, we represent Ti, Tij by D-dimensional embedding
vectors (cid:126)vi, (cid:126)vij ∈ RD, respectively. By considering a subtree
Tij ∈ Ti
to be occurring in the context of the AST Ti,
we aim to maximize the following logarithmic likelihood:
(cid:80)

j log P r(Tij|Ti).
Different from doc2vec, InferCode does not query the
embedding vectors directly from an embedding matrix for the
whole documents; instead, we ﬁrst encode the entire AST to
obtain the (cid:126)vi, then use it to predict the subtrees. The steps of
our technique are as follows:
• For each AST in our dataset, we identify a set of subtrees,
and all of the subtrees are accumulated into a vocabulary of
subtrees (Section IV-B);

• We feed an AST into a Tree-Based CNN (TBCNN) encoder
to produce a code vector (cid:126)vi. Then (cid:126)vi is used to predict the
subtrees identiﬁed in the previous step;

• After the encoder has been trained, we can then use it as

the pretrained model for downstream tasks.

B. Process to Identify Subtrees

Fig. 3. Example to generate subtrees from a code snippet
By traversing an AST, every visited node satisfying a certain
condition, e.g., of the type expr, leads to a subtree rooted at
the visited node. In our experiments, we chose to select the
subtrees whose root nodes are of the types {expr_stmt,
decl_stmt, expr, condition}, We consider these rel-
atively ﬁne-grained code elements because they are usually
meaningful but yet still small enough to be considered as

4

the frequent “words” in the vocabulary of subtrees from a
large code base. Such small code elements often have similar
meaning when their syntactical structure is similar even though
their textual appearance may be different (due to different
identiﬁer names, such as int n = arr.length versus
int m = x.length). In addition, we also consider nodes
that represent for a single keyword, such as if, for,
while. Noted that these nodes can be seen as the sutrees
with size = 1.

We do not consider too coarse-grained subtrees, such as the
whole if, while, for statements, as those subtrees are
often big so that (1) each of them, as an individual vocabulary
word, may appear too infrequent in the code base for the
encoder to learn a meaningful representation for it directly; (2)
syntactical differences among the big subtrees do not necessar-
ily mean the corresponding code has different meanings, while
the encoder may have harder time to recognize the semantic
similarity among them.

Figure 3 shows a sample bubble sort code snippet written
in Java and the identiﬁed subtrees on the right side. This
snippet
is parsed into an AST, and certain subtrees are
identiﬁed automatically. For example, the statement int n
= arr.length contains an expression arr.length. Both
int n = arr.length and arr.length are identiﬁed.

C. Learning Source Code Representation

Once we have the subtrees, we can use it to learn the source
code encoder under a self-supervision mechanism. Here we
choose TBCNN [43] as the source code encoder. There are
two major differences between our TBCNN and the original
design of [43]: we include the textual information into the
node initialization embedding instead of using only the type
information, and we replace the dynamic max pooling with an
attention mechanism to combine node embeddings. Figure 4
shows an overview of the workﬂow of the TBCNN with the
modiﬁcations that we made. There are 3 steps to learn the
weights of the encoder, which can be described as:
• Learning Nodes Representation: This step is to learn
the representation of the node of the input AST T . The
information of the tree will propagate from bottom to top,
i.e., a parent node will accumulate the information of its
descendant in the AST. After the accumulation step, each
node will contain the information of its descendants.

• Aggregating Nodes Information: Since we want to repre-
sent the AST representation of the code snippet into a ﬁxed
dimension vector (cid:126)v, we need to combine all of the node

pooling to combine the nodes. However, max pooling may
discard a lot of important information, so we replace it with the
attention mechanism to aggregate nodes. Formally, an attention
vector (cid:126)a ∈ RD is initialised randomly and learned simultane-
ously with updates of the networks. Given n node state vectors:
{ (cid:126)h1, ..., (cid:126)hn}, the attention weight αi of each (cid:126)hi is computed
as the normalised inner product between the node state vector
and the global attention vector: αi =
. The
exponents in this equation are used to make the attention
weights positive, and they are divided by their sum to have a
max value of 1, as done by a standard softmax function.

exp( (cid:126)hi
·(cid:126)a)
T
j=1 exp( (cid:126)hj

(cid:80)n

·(cid:126)a)

T

The aggregated code vector (cid:126)v ∈ RD represents the whole
code snippet. It is a linear combination of the node state
vectors { (cid:126)h1, ..., (cid:126)hn} weighted by their attention scores:

(cid:126)v =

n
(cid:88)

i=1

αi · (cid:126)hi

(1)

3) Predicting Subtrees: From the process to extract the
subtrees, we have a vocabulary of all subtrees from our training
dataset. The embeddings of subtrees are learn-able parameters,
formally deﬁned as Wsubtrees ∈ R|L|×D, where L is the set of
subtrees extracted from the training corpus. The embedding of
subtreesi is row i of Wsubtrees. The predicted distribution
of the model q (l) is computed as the (softmax-normalized)
dot product between the code vector (cid:126)v and each of the subtree
exp((cid:126)vT ·Wsubtrees
embeddings: f or li ∈ L : q (li) =
lj ∈L exp((cid:126)vT ·Wsubtrees
)
where q (li) is the normalized dot product between the vector
of li and the code vector (cid:126)v, i.e., the probability that a subtrees
li appears given code snippet C. This is aligned with Eq.
III-B in Doc2vec to predict the likelihood of a word given a
document.

(cid:80)

)

i

i

Totally, we need to learn these parameters of

Infer-
Code: Wtype, Wtoken, Wt, Wl, Wr ∈ RD×D, a ∈
RD, Wsubtrees ∈ R|L|×D.

D. Usage of the Model after Training

We have presented the pipeline to train InferCode by
predicting subtrees as the labels. Note that in self-supervised
learning, one does not usually care about the performance
of the pretext task. Instead, we care about the weights that
have been learned and the ability of the model to generate the
embeddings. The trained TBCNN encoder of InferCode can be
used to produce an embedding vector (cid:126)v for any parsable code
snippet by (1) parsing the code into an AST and (2) feeding the
AST through the encoding step presented in Figure 4 to get the
vector. The weights in the trained model can also be used for
the prediction models in downstream supervised learning tasks
to save training costs and potentially improve their prediction
accuracies. We illustrate the usages in next sections.

Fig. 4. Workﬂow of Tree-based Convolutional Neural Network [43] with 2
modiﬁcations: (1)including the token information to initialize the node vector;
and (2) use the attention mechanism to aggregate nodes information

embeddings into one ﬁxed single embedding. We use the
attention layer for this purpose.

• Predicting Subtrees: Once we have the vC, we use it
to predict the subtrees extracted from T . Intuitively, this
process is similar to Eq. III-B, where the task is to predict
the probability of a subtree given the embedding vC.
1) Learning Nodes Representation with TBCNN: We
brieﬂy introduce the Tree-based Convolutional Neural Net-
works (TBCNN, [43]) for processing AST inputs.

A tree T = (V, E, X) consists of a set of nodes V , a
set of node features X, and a set of edges E. An edge in
a tree connects a node and its children. Each node in an
AST also contains its corresponding texts (or tokens) and its
type (e.g., operator types, statement types, function types, etc.)
from the underlying code. Initially, we annotate each node
v ∈ V with a D-dimensional real-valued vector (cid:126)xv ∈ RD
representing the features of the node. We associate every node
v with a hidden state vector (cid:126)hv, initialized from the feature
embedding (cid:126)xv. In [43], the node is initialized only with the
type embedding. In our case, we initialize the node with a
fusion of the embeddings of its texts and through a linear layer.
The embedding matrices for the texts and types are learn-
able in the whole model training pipeline, formally deﬁned as
Wtype and Wtoken, respectively.

In TBCNN, a convolution window over an AST is emulated
via a binary tree, where the weight matrix for each node is a
weighted sum of three ﬁxed matrices Wt, Wl, Wr ∈ RD×D
(each of which is the weight for the “top”, “left”, and “right”
node respectively) and a bias term b ∈ RD Hence, for a
convolutional window of depth d in the original AST with
K = 2d − 1 nodes (including the parent nodes) belong to
that window with vectors [x1, ..., xK], where xi ∈ RD, the
convolutional output y of that window can be deﬁned as
y = tanh((cid:80)K
i Wr]xi + b), where
iWl + ηr
ηt
i , ηl
i are weights calculated corresponding to the depth
and the position of the nodes.

iWt + ηl

i=1[ηt

i, ηr

2) Attention Mechanism to Aggregate Nodes: After the
nodes representation has been learned, we need an aggregation
method to combine all the nodes in to one ﬁxed embedding
that represent for the code snippet. Mou et al. [43] use max

V. USE CASES

In this section, we brieﬂy describe how InferCode can be

adapted into 5 different downstream tasks.

5

A. Code Embedding Vectors for Unsupervised Tasks

1) Code Clustering: Code clustering task is to put similar
code snippets automatically into the same cluster without any
supervision. Given the code vectors (cid:126)v produced by the pre-
trained InferCode for any code snippets, we can realize the task
by deﬁning a similarity metric based on Euclidean distance and
applying a clustering algorithm such as K-means[54].

2) Code Clone Detection: There are supervised and un-
supervised approaches to detect clones. While deep learning
methods are applied to detect code clones, they require labelled
data to train a supervised learning model [16, 44, 55]. As
such, one needs human annotators to mark pairs of snippets
as clones, limiting the ability to detect clones by the amount
of the data one can collect.

To alleviate the need of having labelled pairwise data to train
supervised clone detector, we opt to use the unsupervised ap-
proach based on a similarity measurement: For a pair of code
snippets, we measure the similarity of the two vectors for the
pair by using the cosine similarity; when the cosine similarity
between the vectors are higher than a certain threshold, we
treat the pair as clones. In this work, we choose 0.8 as the
threshold.

3) Cross Language Code-to-Code Search: Code-to-code
search is useful for developers to ﬁnd other code in a large
code base that is similar to a given code query. For example,
a developer working on a task to migrate a sorting algorithm
implemented in Java to another language (e.g., C#) might want
to see if there exists an implementation of the same sorting
algorithm in C#, instead of rewriting the code in C# from
scratch. Existing code-to-code search engine such as Krugle,
Facoy [4], Aroma [56], only consider the searching problem
within one programming language. Considering the more
challenging use case that enables code-to-code search across
multiple languages, our pre-trained InferCode model can be
useful. The backbone of InferCode is ASTs, and we used the
ASTs from SrcML because it is a combined vocabulary for
the AST node types in ﬁve main-stream languages (Java, C,
C++, C# and Objective C). Our pre-trained model can receive
SrcML AST structure of any code snippets within these 5
languages. Given a code snippet in one language as a query, we
aim to retrieve other code snippets that are functionally similar
to the given code snippet in other programming languages.
Since all code snippets can be represented in the form of
vector representations, this problem can be formalized as the
nearest-neighbor query in the vector space.

B. Fine-Tuning for Supervised Learning Tasks

A paradigm to make a good use of large amount of
unlabelled data is self-supervised pretraining followed by a
supervised ﬁne-tuning [19, 20], which reuses parts (or all) of
a trained neural network on a certain task and continue to
train it or simply using the embedding output for other tasks.
Such ﬁne-tuning processes usually have the beneﬁts of (1)
speeding up the training as one does not need to train the
model from randomly initialized weights and (2) improving

Fig. 5. Code features are learned through the training process of TBCNN
encoder to solve a predeﬁned pretext task. After ﬁnishing the training, the
learned parameters serve as a pre-trained model and can be transferred to
other downstream tasks by ﬁne-tuning. The performance on these downstream
tasks is used to evaluate the quality of the learned features.

the generalizability of the downstream model even when there
are only small datasets with labels.

As shown in Figure 5, The TBCNN encoder of InferCode
serves as a pretrained model, in which the weights resulted
from the self-supervised learning are transferred to initialize
the model of the downstream supervised learning task.

1) Code classiﬁcation: We use code classiﬁcation [43] as
a downstream task to demonstrate the usefulness of the ﬁne-
tuning process. This task is to, given a piece of code, classify
the functionality class it belongs to.

2) method name prediction: We use Method name pre-
diction [8] as the second downstream task. This task is to,
given a piece of code (without its function header), predict a
meaningful name that reﬂects the functionality of the code. .

VI. EMPIRICAL EVALUATION

In this section, we evaluate InferCode on the ﬁve use cases
presented in Section V. We want to see to what degree the pre-
trained model is applicable to different use cases even when
the cases involve multiple programming languages.

To train our model, we reuse the Java-Large dataset that
has been used in Code2vec [8] and Code2seq [14]. This
dataset contains a large number of Java projects collected
from Github (4 million ﬁles). We parse all the ﬁles into ASTs
using SrcML [34]. Then we identify all the subtrees to form
a vocabulary of subtrees. Having the ASTs, and the subtrees
as the pseudo labels, we train the InferCode model by using
the softmax cross-entropy as the objective loss function and
choose Adam [57] as the optimizer with an initial learning
rate of 0.001 on an Nvidia Tesla P100 GPU.

A. Code Clustering

1) Datasets, Metrics, and Baselines: We use two datasets
for this task. The ﬁrst is the OJ dataset that contains 52,000
C code snippets known to belong to 104 classes [43]. The
second is the Sorting Algorithm (SA) dataset used in [58],
which consists of 10 classes of sorting algorithm written in
Java, each algorithm has approximately 1000 code snippets.
Our clustering task here is to cluster all the code snippets
(without class labels) according to the similarity among the
code vectors: For the OJ dataset, we use K-means (K=104)
to cluster the code into 104 clusters; For the SA dataset, we
use K-means (K=10) to cluster the code. Then we use the

6

class labels in the datasets to check if the clusters are formed
appropriately.

We use the Adjusted Rand Index [59] as the metric to
evaluate the clustering results. Here we present the deﬁnition
of Rand Index. Let C be the ground truth class assignment,
and K be the number of clusters assigned by a clustering
algorithm. Let a be the number of pairs of elements that are in
the same set in C and the same set in K; and b as the number
of pairs of elements that are in different sets in C and different
sets in K. Rand Index for two datasets can be deﬁned as:
(cid:1)
, where the combinatorial number (cid:0)nsamples
RI =
is the total number of possible pairs in the dataset (without
ordering). However, the RI score does not guarantee that
random label assignments will get a value close to zero (esp. if
the number of clusters is in the same order of magnitude as
the number of samples). To counter this effect, Adjusted Rand
Index is deﬁned by discounting the expected RI of random
labelling as followed: ARI = RI−E[RI]

a+b
(nsamples

max(RI)−E[RI] .

)

2

2

For the baselines, if we treat source code as text, the self-
supervised learning techniques in NLP can also be applied for
code. As such, we include two well-known baselines from
NLP, Word2vec [24], and Doc2vec [25]. We also include
another baseline from [60], a state-of-the-art method to learn
sentence representation. This method uses a Sequential De-
noising Auto Encoder (SAE) method to encode the text into
an embedding, and reconstruct the text from such embedding.
We also compare with two baselines for code modeling,
Code2vec [8] and Code2seq [14]. Code2vec works by training
a path encoder on bag-of-paths extracted from the AST. The
path encoder will encode the paths into an embedding (cid:126)v, then
use (cid:126)v to predict the method name. Code2seq shares a similar
principle, but the (cid:126)v is used to generate text summary of code.
In either case, we use the path encoders of Code2vec and
Code2seq to produce the code vectors and also perform the
same clustering process as InferCode.

2) Results: Table I shows the results of code clustering
using different models. InferCode performs the best for both
datasets. The NLP methods underperform other code learning
methods. This is reasonable because both Code2vec and
Code2seq capture structural information from code, while NLP
methods treat code as text sequences. We will provide a deeper
analysis of the clusters by providing visualizations of the
vectors produced by different methods (see Section VII-A).

TABLE I
RESULTS OF CODE CLUSTERING IN ADJUSTED RAND INDEX (ARI)

Model

Word2vec
Doc2vec
SAE

Code2vec
Code2seq

InferCode

Performance (ARI)

OJ Dataset (C)
0.28
0.42
0.41

SA Dataset (Java)
0.24
0.29
0.31

0.58
0.53

0.70

0.51
049

0.62

B. Code Clone Detection

1) Datasets, Metrics and Baselines: We use two datasets
in two languages. One is the OJ Dataset again that contains
52000 C programs. The other is the BigCloneBench, a Java
dataset that has been widely used to benchmark code clone
detection techniques, which consists of projects from 25,000
projects, cover 10 functionalities and including 6,000,000 true
clone pairs and 260,000 false clone pairs. For the OJ Dataset,
we followed the process in [44] to construct a set of code pairs
for clone detection based on pair-wise similarity measurement,
so-called OJClone: We choose 500 programs from each of the
ﬁrst 15 programming problems in OJ. It would produce a total
of 1.8 million clone pairs and 26.2 million non-clone pairs,
which are extremely time-consuming for comparison. So that
we randomly select 50000 samples clone pairs and 50000 non-
clone pairs for measuring the performance of various clone
detectors.

We use the well-known Precision, Recall, and F1 scores.
Since the task is unsupervised, in this paper we compare
InferCode only with unsupervised clone detectors that do not
require labeled data (although the pretrained InferCode can
also be applied to supervised clone detection). The baselines
include Deckard [61], SourcererCC [62], DLC [63], and a de-
tector using the code vectors extracted from Code2vec [8, 18]
and the same cosine similarity threshold used for InferCode.
2) Results: Table II shows the overall precision, recall and
F1 for InferCode and other baselines. The detector based
on InferCode has the highest recall (except for SourcererCC
whose precision is relatively low). Overall in terms of F1, it
outperforms other unsupervised clone detectors.

Note that we do not compare with techniques such as
Oreo [55], CCD [16], ASTNN [44] because they use super-
vised learning techniques to build clone classiﬁers. We believe
that the code embeddings or the weights from the pretrained
InferCode can be used for training supervised clone classiﬁers
too, and with further improvement on self-supervised learning
techniques, such as improving the encoder, the auto-identiﬁed
labels, and the loss function, the performance of unsupervised
code clone detection may also get close to supervised ones.
We leave these evaluations for future work.

TABLE II
RESULTS OF CODE CLONE DETECTION IN PRECISION, RECALL AND F1

Methods

Deckard
DLC
SourcererCC
Code2vec
InferCode

BigCloneBench (Java)

P
0.93
0.95
0.88
0.82
0.90

R
0.02
0.01
0.02
0.40
0.56

F1
0.03
0.01
0.03
0.60
0.75

OJClone (C)
R
0.05
0.00
0.74
0.69
0.70

F1
0.10
0.00
0.14
0.61
0.64

P
0.99
0.71
0.07
0.56
0.61

C. Cross Language Code-to-Code Search

1) Datasets, Metrics, and Baselines: Given the implemen-
tation of an algorithm in one language, this task is to search
for other implementations of the same algorithm written in
other languages. So we need a dataset that contains multiple
languages. We
implementations of algorithms in different

7

construct such a codebase for search from the Rosetta Code2
and other code from GitHub: We collect code in Java, C, C++,
C# from Rosetta Code which results in around 3000 samples;
then we collect 5000 random program ﬁles from Github for
each of the languages and mix them with the samples.

For instance, for Java, we collect a large set of Java projects
from Github that have at least 10 stars. There is a possibility
that the collected GitHub projects contain implementations
of the algorithms in the Rosetta Code. So we perform a
simple text ﬁltering to exclude all the ﬁles that contain a
token of any of the algorithm name. Let us take 3 algorithms
as examples (Bubble-sort, Singly-linked-list-Traversal, Yin-
yang3): We exclude any ﬁle that contains any of these tokens:
{bubble, sort, singly, linked, list, traversal, yin, yang}. Then
for the remaining Java ﬁles, we sample a subset of 5000 ﬁles
and mix them with the Java implementations of the algorithms
from the Rosetta dataset. We do the same for C#, C++, C, so
that we get in total about 23,000 ﬁles in our search codebase.
With the constructed code base, we perform the evaluation
for cross-language search as follows: For each of the 3000
code ﬁles from Rosetta Code, say a bubble sort implementation
written in Java, we use it as the query to retrieve other ﬁles
containing top-K similar code, we choose K = 10 in this
evaluation. The ideal query results should only return a list of
code snippets that are from Rosetta Code but implement the
same bubble sort algorithm in C++, C#, and C; other results
would be considered as false positives. Since our assumption
is that there is only one relevant result for the query, we use
the well-known Mean Reciprocal Rank (MRR) as the metric
to evaluate the actual query results.

Since this task can be formulated as the information retrieval
(IR) problem and the neural IR techniques are widely applied
recently for text data [64, 65, 66], we include Word2vec,
Doc2vec, CLIR [66], a cross-lingual
information retrieval
system for text. We also follow Sachdev et al. [5] to include
ElasticSearch, a fuzzy text search baseline. Although there are
recent methods designed speciﬁcally for code-to-code search,
such as Facoy [4] and Aroma [56], they are designed only for
monolingual code search, thus we do not compare with them
directly.

2) Results: Table III shows the results for InferCode and
other baselines. The performance of InferCode is the best
among all
the models. ElasticSearch, on the other hand,
performs the worst; this is expected because ElasticSearch is
a simple fuzz text search technique not designed to capture
structural information of code. The performance of

D. Fine-Tuning for Supervised Learning Tasks

1) Datasets, Metrics, and Baselines:

a) Code Classiﬁcation: We again use the OJ Dataset for
this task. We split this dataset into three parts for training,
testing, and validation by the ratio of 70:20:10. Out of the
training data, we feed X% to the neural model, where X = 1,

TABLE III
RESULTS OF CROSS-LANGUAGE CODE-TO-CODE SEARCH IN MEAN
RECIPROCAL RANK (MRR)

Approach

ElasticSearch
Word2vec
Doc2vec
CLIR
InferCode

Performance (MRR)

Java
0.13
0.33
0.32
0.29
0.57

C#
0.18
0.36
0.34
0.32
0.45

C++
0.22
0.30
0.38
0.34
0.51

C
0.21
0.32
0.30
0.39
0.54

10, 100. We then initialize the neural model either randomly
or with the weights from the pre-trained InferCode. Therefore,
we have four settings for training the supervised model for
comparison: ﬁne-tuning the TBCNN encoder with 1%, 10%,
or 100% of the labeled training data respectively, and the
randomly initialized model. Using only 1% or 10% is to
demonstrate that given a pre-trained model, one only needs
a small amount of labeled data to achieve reasonably good
performance for the downstream task.

We use the accuracy metric widely used for classiﬁcation
tasks. As the baselines, we include the ASTNN [44] trained
from scratch, which is a state-of-the-art model for code
classiﬁcation on the OJ dataset, and TextCNN [67] and Bi-
LSTM [68] trained with 100% of the training data, which are
widely used for text classiﬁcation.

b) Method Name Prediction: We use the Java-Small
dataset widely used as a benchmark for method name predic-
tion and has been used in Code2vec [8] and Code2seq [14].
This dataset has already been split into three parts, namely
training, testing, and validation. We perform the same eval-
uation protocol as the code classiﬁcation task by ﬁne-tuning
the model with 1%, 10%, and 100% of the labeled training
data, in contrast to random initialization of the model without
ﬁne-tuning. To predict the method name, we follow Code2vec
to use the code vector (cid:126)v to predict the embedding of a method
name from a lookup table (see Section 4.2 in Code2vec [8]).
We measure prediction performance using precision (P), recall
(R), and F1 scores over the sub-words in generated names,
following the metrics used by Alon et al. [8]. For example,
a predicted name result_compute is considered as an
exact match of the ground-truth name computeResult;
predicted compute has full precision but only 50% recall;
and predicted compute_model_result has full recall but
only 67% precision.

2) Results: Table IV shows the results for code classiﬁca-
tion. Fine-tuning on 10% of the training data gets comparable
results with the NLP baselines. Fine-tuning on 100% of the
training data gets comparable with ASTNN, a state-of-the-art
model for code classiﬁcation on the OJ dataset.

Table V shows the results for method name prediction. We
get a comparable result with Code2seq when ﬁne-tuning with
100% labeled data.

E. Summary

2http://www.rosettacode.org, https://github.com/acmeism/RosettaCodeData
3These are taken from the names of the algorithms at https://github.com/

acmeism/RosettaCodeData/tree/master/Task

InferCode outperforms most of the baselines across ﬁve
tasks, including three unsupervised ones (code clustering, code

8

TABLE IV
RESULTS OF CODE CLASSIFICATION IN ACCURACY WITH FINE-TUNING
(FT) ON THE OJ DATASET

Approach
InferCode
TextCNN
Bi-LSTM
ASTNN

FT (1%)
70.4%
-
-
-

FT (10%)
87.6%
-
-
-

FT (100%)
98.0%
-
-
-

Supervised
94%
88.7%
88.0%
97.8%

TABLE V
RESULT OF METHOD NAME PREDICTION IN F1 WITH FINE-TUNING (FT)
ON THE JAVA-SMALL DATASET

Approach
InferCode
Code2vec
Code2seq

FT (1%)
20.31%
-
-

FT (10%)
30.54%
-
-

FT (100%)
43.33%
-
-

Supervised
35.67%
18.62%
43.02%

clone detection via similarity measurement), cross-language
code-to-code search), and two supervised ones (code classiﬁ-
cation and method name prediction).

Note that this does not mean that the TBCNN encoder in
InferCode is better than ASTNN, Code2vec, or Code2seq, as
those neural models can be used as the encoder in InferCode
too. It only means that pre-training a model on large unla-
beled data using self-supervised learning to predict subtrees
can produce more transferable models while maintaining the
performance of such models for various code learning tasks.
The performance of the self-supervised learning models
may be improved further with different encoders. We leave
those explorations for future work.

VII. ANALYSIS

This section analyses the effects of various parameters on

the performance of different tasks.

A. Cluster Visualization

To help understand why the vectors produced by InferCode
are better than the vectors produced by others, we visualize
the vectors of the programs from the OJ dataset that have
been used for the code clustering. We choose the embeddings
produced by Doc2vec, Code2vec, and InferCode for the ﬁrst
9 classes of the OJ dataset,
then we use T-SNE [69] to
reduce the dimension of the vectors into two-dimensional
space and visualize. As shown in Figure 6, (1) the vectors
produced by InferCode group similar code snippets into the
same cluster with clearer boundaries, and (2) The boundaries
among clusters produced by Doc2vec and Code2vec are less
clear, which makes it more difﬁcult for the K-means algorithm
to cluster the snippets correctly. This is aligned with the
performance of the code clustering task (Table I). Also, we
observe that some points marked in the same color (e.g., red)
are somewhat far away from each other even in the vectors
from InferCode, while they are supposed to be close according
to the ground truth. This could indicate further improvements
to Infercode can be made in future work.

B. Effect of Textual Information in TBCNN

The original TBCNN in Mou et al. [43] does not include
textual information in AST nodes to initialize the node em-

bedding. In our implementation, we include the textual infor-
mation by fusing it with the node type information through
a linear layer. To help understand the effect of such a fusion
process, we perform an ablation study by training InferCode
with different
initialization information on the Java-Large
dataset and perform the evaluations on the three unsupervised
tasks: code clustering (CC), code clone detection (CCD), and
cross-language code-to-code search (CLCS) with the same
settings for each of the tasks in Section VI. Table VI shows
the results of this study. Using only type or token information
will result in worse performance for all three tasks.

TABLE VI
EFFECTS OF DIFFERENT INITIALIZATION METHODS

Task

CC
CCD
CLCS

Dataset

Metric

OJ
BigCloneBench
Rosetta Stone

ARI
P
MRR

Initial Information

Type
0.57
0.45
0.18

Token
0.28
0.49
0.39

Combine
0.70
0.90
0.57

C. Alternative Choices to the Pretext Task Labels

There are a few alternatives when we use subtrees as the
pseudo labels for the pretext task in InferCode. One can easily
replace the subtrees with tokens so that the code vector (cid:126)v can
predict the tokens of the code snippets (similar to Doc2vec).
Or one can use all the method names as the pseudo labels
and train the (cid:126)v to predict the names, similar to Code2vec [8].
In this section, we perform an ablation study to measure how
different types of labels can affect performance. As shown in
Table VII, the performance using the subtrees as the labels is
the best while using tokens as the labels result in the worst
performance. Although using the method name can result
in reasonable performance, it is still worse than using the
subtrees. An explanation for this is that by predicting method
names, the model is forced to learn some incorrect patterns
due to similar names in the code base that actually refer to
different code. For example, Jiang et al. [70] found that a
large number code snippets contain similar method names but
the actual implementations of the method bodies are different,
but their code vectors would be forced to predict the similar
method names, thus these vectors will be close in the vector
space despite that they should not be. This is a potential reason
to make the model trained by predicting method names a worse
choice for pretext task than using subtrees.

TABLE VII
EFFECTS OF DIFFERENT WAYS TO SET UP LABELS OF THE PRETEXT TASK

Task

CC
CCD
CLCS

Dataset

Metric

OJ
BigCloneBench
Rosetta Stone

ARI
P
MRR

Label

Token Method Name
0.23
0.45
0.32

0.58
0.81
0.41

Subtree
0.70
0.90
0.57

VIII. DISCUSSION

In this section, we want to discuss our choice on the decoder.
We choose TBCNN because of its ability to capture structural
features of code that lie in ASTs and the modiﬁcation we
made to TBCNN can also capture textual information into
the model. There are many neural network designs that can

9

Fig. 6. Visualization of the Code Vectors of the Programs from 9 classes in the OJ Dataset produced by InferCode, Code2vec and Doc2vec

be used as a replacement of the TBCNN encoder, such as
ASTNN [44], Code2vec [8] or GGNN [49], however, most
of them, especially the graph-based models, are unable to
scale and generalize for different programming languages. For
example, we can use the path encoder of Code2vec to encode
the AST paths into the code vector (cid:126)v and infer the subtrees.
GGNN is similar, one can pre-train the GGNN over a self-
supervised learning task. Although the graph representation
proposed by Narayanan et al. [15], Allamanis et al. [49] has
been proved to work well on tasks, such as supervised clone
detection, code summarization, variable name prediction, etc.,
choosing the suitable edges to be included in the graph
representations for such tasks can be time-consuming and not
generalizable. LambdaNet [71] is another graph-based model
that also contains semantic edges designed speciﬁcally for
the type prediction task. As such, it is not straightforward to
transfer a pre-trained graph learning model through different
code learning tasks and it is not easy to scale the graph
representation of code into multiple languages. Similar reasons
can also be applied for path-based models, such as Code2vec
and Code2seq, or execution trace-based models [33]. On the
other hand, TBCNN is designed to receive the AST directly
with minimal engineering effort to process it. AST is relatively
easy to produce accurately for most programming languages
given their grammars,
thus building a tree-based learning
model on top of ASTs implies that we can have a model that is
easier to generalize across languages, which is the advantage
to choose tree-based models over others. Note that this is
not to say that other models do not perform well on all the
code learning tasks; they can still perform well when training
data and time are specially utilized, and they may be used
together with each other as the encoder in the self-supervised
learning framework to improve the performance for various
tasks further. We leave all the exciting explorations for future
work.

IX. CONCLUSIONS

We have proposed InferCode, a self-supervised learning
technique for source code learning on unlabeled data. The
key intuition is that similar ASTs will have similar subtrees,
which is aligned with the principle to learn document embed-
dings, where similar documents should contain similar words.
InferCode works by using a Tree-based CNN to encode the
ASTs into a code vector and use it to predict the subtrees.

We perform the training of InferCode on a large scale dataset.
Then the encoder of InferCode, which is the Tree-based CNN
can be reused as a pre-trained model. This pre-trained model
is able to map the AST of any code snippet into an embedding
and use it for other downstream tasks, such as code clustering,
code clone detection, or code-to-code search. Our evaluation
of these tasks show that the embeddings produce by InferCode
are useful and outperform the other baselines with signiﬁcant
margins. Another use case of the pre-trained model is that
its weights can be used under the notion of self-supervised
pretraining followed by supervised ﬁne-tuning. We have shown
that the ﬁne-tuning process on a pre-trained model outperforms
the supervised model trained from scratch. In the future, we
will explore more on different choices of the encoder. We will
also adapt InferCode into other tasks, such as bug localization,
defect prediction, etc.

REFERENCES

[1] R. Nix and J. Zhang, “Classiﬁcation of android apps and
malware using deep neural networks,” in International
Joint Conference on Neural Networks, May 2017, pp.
1871–1878.

[2] G. E. Dahl, J. W. Stokes, L. Deng, and D. Yu, “Large-
scale malware classiﬁcation using random projections
and neural networks,” in IEEE International Conference
on Acoustics, Speech and Signal Processing, 2013, pp.
3422–3426.

[3] X. Gu, H. Zhang, and S. Kim, “Deep code search,” in

40th ICSE, 2018, pp. 933–944.

[4] K. Kim, D. Kim, T. F. Bissyand´e, E. Choi, L. Li, J. Klein,
and Y. L. Traon, “FaCoY: a code-to-code search engine,”
in ICSE, 2018, pp. 946–957.

[5] S. Sachdev, H. Li, S. Luan, S. Kim, K. Sen, and S. Chan-
dra, “Retrieval on source code: A neural code search,” in
2nd ACM SIGPLAN International Workshop on Machine
Learning and Programming Languages, 2018, p. 31–41.
[6] X. Hu, G. Li, X. Xia, D. Lo, and Z. Jin, “Deep code
comment generation,” in ICPC, 2018, pp. 200–210.
[7] Y. Wan, Z. Zhao, M. Yang, G. Xu, H. Ying, J. Wu, and
P. S. Yu, “Improving automatic source code summariza-
tion via deep reinforcement learning,” in 33rd ASE, New
York, NY, USA, 2018, p. 397–407.

10

[8] U. Alon, M. Zilberstein, O. Levy,

and E. Ya-
hav, “Code2vec: Learning distributed representations of
code,” in POPL, 2019, pp. 40:1–40:29.

[9] X. Yang, D. Lo, X. Xia, Y. Zhang, and J. Sun, “Deep
learning for just-in-time defect prediction,” in IEEE QRS,
2015, pp. 17–26.

[10] J. Li, P. He, J. Zhu, and M. R. Lyu, “Software defect
prediction via convolutional neural network,” in IEEE
QRS, 2017, pp. 318–328.

[11] Y. Zhou, S. Liu, J. K. Siow, X. Du, and Y. Liu, “Devign:
Effective vulnerability identiﬁcation by learning compre-
hensive program semantics via graph neural networks,”
in NeurIPS, 2019, pp. 10 197–10 207.

[12] X. Chen, C. Liu, and D. Song, “Tree-to-tree neural
networks for program translation,” in NeurIPS, 2018, pp.
2547–2557.

sentences and documents,” in ICML, 2014, pp. 1188–
1196.

[26] H. Wei and M. Li, “Supervised deep features for soft-
ware functional clone detection by exploiting lexical and
syntactical information in source code,” in IJCAI, 2017,
pp. 3034–3040.
Ingram.

(2018) A comparative study of various
code embeddings in software semantic matching. https:
//github.com/waingram/code-embeddings.

[27] B.

[28] S. O. Broggi, “Bug prediction with neural nets using
regression- and classiﬁcation-based approaches,” 2018,
http://scg.unibe.ch/news/2018-02-06 19-15-01Brog18a.
[29] H. Aman, S. Amasaki, T. Yokogawa, and M. Kawahara,
“A doc2vec-based assessment of comments and its appli-
cation to change-prone method analysis,” in 25th APSEC,
2018, pp. 643–647.

[13] X. Gu, H. Zhang, D. Zhang, and S. Kim, “DeepAM:
Migrate apis with multi-modal sequence to sequence
learning,” in IJCAI, 2017, pp. 3675–3681.

[30] Z. Chen and M. Monperrus, “A literature study of embed-
dings on source code,” arXiv preprint arXiv:1904.03061,
2019.

[14] U. Alon, S. Brody, O. Levy, and E. Yahav, “code2seq:
Generating sequences from structured representations of
code,” in ICLR, 2019.

[15] A. Narayanan, M. Chandramohan, R. Venkatesan,
L. Chen, Y. Liu, and S. Jaiswal, “graph2vec: Learn-
ing distributed representations of graphs,” CoRR, vol.
abs/1707.05005, 2017.

[16] C. Fang, Z. Liu, Y. Shi, J. Huang, and Q. Shi, “Functional
code clone detection with syntax and semantics fusion
learning,” in 29th ISSTA, 2020, pp. 516–527.

[17] W. Wang, G. Li, B. Ma, X. Xia, and Z. Jin, “Detect-
ing code clones with graph neural network and ﬂow-
augmented abstract syntax tree,” in 27th SANER, 2020,
pp. 261–271.

[18] H. J. Kang, T. F. Bissyand´e, and D. Lo, “Assessing the
generalizability of code2vec token embeddings,” in 34th
ASE, 2019, pp. 1–12.

[19] G. E. Hinton, S. Osindero, and Y.-W. Teh, “A fast learn-
ing algorithm for deep belief nets,” Neural computation,
vol. 18, no. 7, pp. 1527–1554, 2006.

[20] T. Chen, S. Kornblith, M. Norouzi, and G. Hinton,
“A simple framework for contrastive learning of visual
representations,” arXiv preprint arXiv:2002.05709, 2020.
[21] M. Yasunaga and P. Liang, “Graph-based, self-supervised
program repair from diagnostic feedback,” arXiv preprint
arXiv:2005.10636, 2020.

[22] C. Doersch and A. Zisserman,

“Multi-task self-
supervised visual learning,” in ICCV, 2017, pp. 2051–
2060.

[23] A. Kolesnikov, X. Zhai, and L. Beyer, “Revisiting self-
supervised visual representation learning,” in CVPR,
2019, pp. 1920–1929.

[24] T. Mikolov, I. Sutskever, K. Chen, G. S. Corrado,
and J. Dean, “Distributed representations of words and
phrases and their compositionality,” in NeurIPS, 2013,
pp. 3111–3119.

[25] Q. Le and T. Mikolov, “Distributed representations of

11

[31] S. Akbar and A. Kak, “SCOR: source code retrieval with
semantics and order,” in 16th MSR, 2019, pp. 1–12.
[32] M. Tufano, C. Watson, G. Bavota, M. Di Penta,
M. White, and D. Poshyvanyk, “Deep learning similar-
ities from different representations of source code,” in
15th MSR, 2018, pp. 542–553.

[33] K. Wang and Z. Su, “Learning blended, precise semantic
program embeddings,” ArXiv, vol. abs/1907.02136, 2019.
[34] M. L. Collard, M. J. Decker, and J. I. Maletic, “srcml: An
infrastructure for the exploration, analysis, and manipu-
lation of source code: A tool demonstration,” in ICSM,
2013, pp. 516–519.

[35] A. Mahendran, J. Thewlis, and A. Vedaldi, “Cross pixel
optical-ﬂow similarity for self-supervised learning,” in
Asian Conference on Computer Vision, 2018, pp. 99–116.
[36] S. Gidaris, P. Singh, and N. Komodakis, “Unsupervised
representation learning by predicting image rotations,”
arXiv preprint arXiv:1803.07728, 2018.

[37] R. Zhang, P. Isola, and A. A. Efros, “Colorful image col-
orization,” in European conference on computer vision,
2016, pp. 649–666.

[38] B. Korbar, D. Tran, and L. Torresani, “Cooperative
learning of audio and video models from self-supervised
synchronization,” in NeurIPS, 2018, pp. 7763–7774.
[39] D. Kim, D. Cho, and I. S. Kweon, “Self-supervised video
representation learning with space-time cubic puzzles,” in
AAAI, vol. 33, 2019, pp. 8545–8552.

[40] B. Fernando, H. Bilen, E. Gavves, and S. Gould, “Self-
supervised video representation learning with odd-one-
out networks,” in CVPR, 2017, pp. 3636–3645.

[41] R. Kiros, Y. Zhu, R. R. Salakhutdinov, R. Zemel, R. Ur-
tasun, A. Torralba, and S. Fidler, “Skip-thought vectors,”
in NeurIPS, 2015, pp. 3294–3302.

[42] J. Devlin, M.-W. Chang, K. Lee, and K. Toutanova,
transformers
preprint

“Bert: Pre-training of deep bidirectional
for
language
arXiv:1810.04805, 2018.

understanding,”

arXiv

[43] L. Mou, G. Li, L. Zhang, T. Wang, and Z. Jin, “Con-
volutional neural networks over tree structures for pro-
gramming language processing,” in AAAI, 2016.

[44] J. Zhang, X. Wang, H. Zhang, H. Sun, K. Wang, and
X. Liu, “A novel neural source code representation based
on abstract syntax tree,” in 41st ICSE, 2019, pp. 783–794.
[45] M. Pradel and K. Sen, “Deepbugs: A learning approach
to name-based bug detection,” ACM on Programming
Languages, vol. 2, no. OOPSLA, p. 147, 2018.

[46] R. Gupta, A. Kanade, and S. Shevade, “Neural attribution
for semantic bug-localization in student programs,” in
NeurIPS, 2019, pp. 11 861–11 871.

[47] P. Fernandes, M. Allamanis, and M. Brockschmidt,

“Structured neural summarization,” in 7th ICLR, 2019.

[48] M. Brockschmidt, M. Allamanis, A. L. Gaunt, and
O. Polozov, “Generative code modeling with graphs,” in
7th ICLR, 2019.

[49] M. Allamanis, M. Brockschmidt, and M. Khademi,
“Learning to represent programs with graphs,” in ICLR,
2018.

[50] Y. Li, D. Tarlow, M. Brockschmidt, and R. Zemel, “Gated

graph sequence neural networks,” in ICLR, Nov. 2016.

[51] L. Mou, H. Peng, G. Li, Y. Xu, L. Zhang, and Z. Jin,
“Discriminative neural sentence modeling by tree-based
convolution,” in EMNLP, 2015, pp. 2315–2325.

[52] N. D. Bui, L. Jiang, and Y. Yu, “Cross-language
learning for program classiﬁcation using bilateral tree-
based convolutional neural networks,” arXiv preprint
arXiv:1710.06159, 2017.

[53] H. Yu, W. Lam, L. Chen, G. Li, T. Xie, and Q. Wang,
“Neural detection of semantic code clones via tree-based
convolution,” in 27th ICPC, 2019, pp. 70–80.

[54] T. Kanungo, D. M. Mount, N. S. Netanyahu, C. D.
Piatko, R. Silverman, and A. Y. Wu, “An efﬁcient k-
means clustering algorithm: Analysis and implementa-
tion,” IEEE Trans. Pattern Anal. Mach. Intell., vol. 24,
no. 7, pp. 881–892, 2002.

[55] V. Saini, F. Farmahinifarahani, Y. Lu, P. Baldi, and C. V.
Lopes, “Oreo: Detection of clones in the twilight zone,”
in 26th ESEC/FSE, 2018, pp. 354–365.

[56] S. Luan, D. Yang, C. Barnaby, K. Sen, and S. Chan-
dra, “Aroma: Code recommendation via structural code
search,” ACM on Programming Languages, vol. 3, no.
OOPSLA, pp. 1–28, 2019.

[57] D. P. Kingma and J. Ba, “Adam: A method for stochastic

optimization,” arXiv preprint arXiv:1412.6980, 2014.
[58] B. D. Q. Nghi, Y. Yu, and L. Jiang, “Bilateral dependency
neural networks for cross-language algorithm classiﬁca-
tion,” in 26th SANER, X. Wang, D. Lo, and E. Shihab,
Eds., 2019, pp. 422–433.

[59] J. M. Santos and M. Embrechts, “On the use of the
adjusted rand index as a metric for evaluating supervised
classiﬁcation,” in International conference on artiﬁcial
neural networks, 2009, pp. 175–184.

[60] F. Hill, K. Cho, and A. Korhonen, “Learning distributed
representations of sentences from unlabelled data,” arXiv
preprint arXiv:1602.03483, 2016.

[61] L. Jiang, G. Misherghi, Z. Su, and S. Glondu, “Deckard:
Scalable and accurate tree-based detection of code
clones,” in 29th ICSE, 2007, pp. 96–105.

[62] H. Sajnani, V. Saini, J. Svajlenko, C. K. Roy, and C. V.
Lopes, “SourcererCC: Scaling code clone detection to
big-code,” in 38th ICSE, 2016, pp. 1157–1168.

[63] M. White, M. Tufano, C. Vendome, and D. Poshyvanyk,
“Deep learning code fragments for code clone detection,”
in 31st ASE, 2016, pp. 87–98.

[64] L. Wang, J. Lin, and D. Metzler, “A cascade ranking
model for efﬁcient ranked retrieval,” in 34th SIGIR, 2011,
pp. 105–114.

[65] S. Wan, Y. Lan, J. Guo, J. Xu, L. Pang, and X. Cheng,
“A deep architecture for semantic matching with multiple
positional sentence representations.” in AAAI, vol. 16,
2016, pp. 2835–2841.

[66] I. Vuli´c and M.-F. Moens, “Monolingual and cross-
lingual information retrieval models based on (bilingual)
word embeddings,” in 38th SIGIR, 2015, pp. 363–372.

[67] Y. Kim, “Convolutional neural networks for sentence

classiﬁcation,” arXiv preprint arXiv:1408.5882, 2014.

[68] M. Schuster and K. K. Paliwal, “Bidirectional recurrent
neural networks,” IEEE Trans. Signal Process., vol. 45,
no. 11, pp. 2673–2681, 1997.

[69] L. v. d. Maaten and G. Hinton, “Visualizing data using t-
sne,” Journal of Machine Learning Research, vol. 9, no.
Nov, pp. 2579–2605, 2008.

[70] L. Jiang, H. Liu, and H. Jiang, “Machine learning based
recommendation of method names: how far are we,” in
34th ASE, 2019, pp. 602–614.

[71] J. Wei, M. Goyal, G. Durrett, and I. Dillig, “Lamb-
danet: Probabilistic type inference using graph neural
networks,” arXiv preprint arXiv:2005.02161, 2020.

12

