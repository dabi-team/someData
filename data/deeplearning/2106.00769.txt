1
2
0
2

t
c
O
6
2

]

G
L
.
s
c
[

2
v
9
6
7
0
0
.
6
0
1
2
:
v
i
X
r
a

Improving Compositionality of Neural Networks by
Decoding Representations to Inputs

Mike Wu, Noah Goodman, Stefano Ermon
Department of Computer Science
Stanford University
Stanford, CA 94303
{wumike,ngoodman,ermon}@stanford.edu

Abstract

In traditional software programs, it is easy to trace program logic from variables
back to input, apply assertion statements to block erroneous behavior, and compose
programs together. Although deep learning programs have demonstrated strong
performance on novel applications, they sacriﬁce many of the functionalities of
traditional software programs. With this as motivation, we take a modest ﬁrst
step towards improving deep learning programs by jointly training a generative
model to constrain neural network activations to “decode” back to inputs. We call
this design a Decodable Neural Network, or DecNN. Doing so enables a form of
compositionality in neural networks, where one can recursively compose DecNN
with itself to create an ensemble-like model with uncertainty. In our experiments,
we demonstrate applications of this uncertainty to out-of-distribution detection,
adversarial example detection, and calibration — while matching standard neural
networks in accuracy. We further explore this compositionality by combining
DecNN with pretrained models, where we show promising results that neural
networks can be regularized from using protected features.

1

Introduction

Traditional hand-written computer programs are comprised of a computational graph of typed
variables with associated semantic meaning. This structure enables practitioners to interact with
programs in powerful ways (even if they are not the author) — such as debug code by tracing variables
back to inputs, apply assertions to block errors, and compose programs together for more complex
functionality. However, traditional software has its limitations: it is difﬁcult to hand-write programs
to classify images or extract sentiment from natural language. For these functionalities, deep learning
and neural networks [41] have become the dominant approach [23, 2, 46].

While neural networks have made impressive progress on complex tasks, they come at a sacriﬁce of
many of the desirable properties of traditional software. Speciﬁcally, the closest approximation to
a “variable” in a neural network is an activation. Yet it is difﬁcult to understand a neural network’s
computation from an activation value, and there is little to associate an activation with semantic
meaning. A practitioner cannot write assertion statements to constrain valid neural network logic:
checking the values that an activation takes is usually not enough to gauge correctness nor meaning.
Moreover, given multiple neural networks, composing them together requires retraining from scratch.

In this paper, we take a modest ﬁrst step towards bridging the expressivity of deep learning with the
engineering practicality of traditional software. We aim to uncover new ways for practitioners to build
and use neural networks by leveraging compositionality. Speciﬁcally, we propose to train a neural
network classiﬁer jointly with a generative model whose role is to map the classiﬁer’s activations
back to the input, approximating invertibility. For any input, a neural network’s computation can

35th Conference on Neural Information Processing Systems (NeurIPS 2021), Sydney, Australia.

 
 
 
 
 
 
be represented by a sequence of activations (from each layer), each of which can now be mapped
back to the input space. It is this insight that enables a special form of compositionality for neural
networks, as inputs derived from activations can be fed back into other models.

In our experiments, we study this compositionality by (1) recursively composing a neural network
with itself, creating an ensemble-like model with a measure of uncertainty that is useful for out-of-
distribution detection, adversarial example detection, and model calibration; (2) composing neural
networks with pretrained models as forms of regularization and distillation; and (3) using decodable
activations to discourage a neural network from using protected attributes. Throughout, we show that
decodability comes at low cost as we ﬁnd equivalent accuracy to a standard neural network.

2 Background

Neural Networks We will focus on supervised neural networks for classiﬁcation, although it
is simple to extend to regression tasks. We denote a neural network by fθ where θ represents its
trainable parameters. A neural network fθ maps an example x to predicted probabilities over K
classes. Typically, a neural network is composed of L blocks. For example, if fθ is a multi-layer
perception, each block is a linear layer followed by a non-linearity. If fθ is a ResNet [17], each
layer is a residual block. In computing its prediction, a neural network fθ produces activations
{h1, . . . , hL} where hl is the output of the l-th block.

To train a neural network, we solve an optimization function of the form

Lnn(x; θ) = log p(y|fθ(x)) + βΩ(x; θ)
using stochastic gradient descent. In Equation 1, the notation x denotes an input example and y its
label. The function Ω represents an auxiliary objective such as a regularizer. The hyperparameter
β > 0 is used to scale the auxiliary loss. For classiﬁcation, log p(y|·) is cross entropy.

(1)

Generative Models Broadly, we are given a latent variable H and an observed variable X. A
generative model gφ captures the distribution p(x|h), mapping samples of the latent code h to samples

Figure 1: Decodable Neural Networks and Applications: The decodable neural network (DecNN) is composed
of a main network, f and a generative model, g. Each activation in f is decoded back to inputs through g.
Subﬁgure (a) shows an incorrect classiﬁcation: the network predicted the man is not wearing a hat despite the
true label. Consequently, we see the decoded activations “interpolate” as the hat is removed. Subﬁgures (b-d)
show the compositionality of DecNNs. First, (b) shows self-composition, where a decoded activation can be fed
as input back into the network f to create an ensemble with uncertainty. Second, (c) shows how a pretrained
classiﬁer, m can be composed with DecNNs for regularization. Finally, (d) shows how to constrain DecNNs to
ignore “protected features” through pretrained networks by regularizing for uniformity.

2

no hatReconstruction loss per layer(total of 4 here)has hatPredictionInputDecoded activations through reconstruction. Information is discarded (removal of hat) as activations interpolate between the input and (incorrect) prediction.Hidden layers of the neural network, True labelCross entropy lossh2h3fθgφh4h1(a) Decodable Neural Networksgφ(b) Ensembling through Self-CompositionContinue to recurseParameter SharingParameter SharingTreat decoded activation as inputHas hat UncertaintyNo hathas hathas hatno hatfθfθfθ(c) Composition with Pretrained ClassifiersPretrained classifier takes decoded activation as inputhas hathas hatfθmgφRegularize f  (x) to agree with m(g  (h))θφ(d) Constraints with Protected FeaturesConstrain m(g  (h)) to be close to Uniform (i.e., not use the protected feature)φPretrained classifier for a protected feature (e.g. has hair) that could be confounding.has hathas hairfθmgφ Uniform Distributionof the observed variable x. For a deep generative model, gφ is parameterized by a neural network. In
the simplest case where gφ is an autoencoder, we are also given an encoder q that maps observed
samples x to latent codes h. Then, we optimize the objective

Lae(x; φ) = log p(x|gφ(q(x)))
(2)
where for continuous x, the expression log p(x|h) is the log Gaussian probability with mean h and
ﬁxed variance (or equivalently, mean squared error). More complex generative models might impose
a prior distribution on h [22], an adversarial discriminator [29, 14], or invertibility of gφ [35].

3 Neural Networks with Decodable Activations

In Section 1, we motivated our desiderata for activations that can be mapped back to the input space.
We now propose to do this by “inverting” a neural network at a particular activation. Concretely,
given an input x, a neural network fθ, we can compute an activation h by computing the forward
pass fθ(x). Then, suppose we could construct the set:

S(h) = {ˆx : ˆx ∼ pD, fθ(ˆx) = h}
(3)
where pD represents the empirical distribution that x is sampled from. Now, any input ˆx ∈ S(h) is a
valid mapping of h back to the input space. (Trivially, x ∈ S(h).) We call this ˆx a decoding of the
activation h. Unfortunately, in practice we do not know how to construct such a set S(h).

But, we do know how to (approximately) sample from S(h). We can train a generative model gφ
to reconstruct the original input x from neural network activations h (recall x ∈ S(h)). Then, we
can sample x ∼ gφ(h), which we know how to do, to sample from S(h). If we jointly train this
generative model gφ with the neural network fθ, we can optimize for activations that both solve the
classiﬁcation task and decode back to inputs at once.

So far, we have only considered a single activation despite our neural network having L such
activations. Although one could train a separate generative model per layer, we found it efﬁcient and
performant to share weights φ across all layers. Our joint objective is:

L(x; θ, φ) = log p(y|fθ(x)) + β

1
L

L
(cid:88)

l=1

log p(x|gφ(hl)),

h1, ..., hL come from fθ(x)

(4)

where the ﬁrst term is cross entropy as in Equation 1 and the second term is a reconstruction error
(Equation 2) averaged over layers. Comparing Equation 4 to Equation 1, we can interpret the
generative loss as a regularization term Ω. Intuitively, in a standard neural network, the activations
h1, . . . , hL are unconstrained, chosen only to minimize the loss. However, in Equation 4, the network
fθ is constrained to produce activations that the generative model gφ can invert. In practice, gφ is
a ResNet-18 decoder with upsampling layers (see Appendix A.1). We call this setup a Decodable
Neural Network, or DecNN. Figure 1 depicts an illustration of the DecNN architecture.

We highlight that although Equation 4 requires good reconstruction, it does not enforce that recon-
structed inputs f (gφ(h)) must map to the same label y as f (x). We show a few examples of this by
visualizing decoded activations throughout a network in Appendix A.3.

4 Composing Decodable Neural Networks

In traditional software, we can easily compose two compatible functions f1 and f2 together by
f2(f1(x)) for some input x. With this as loose inspiration, we wish to similarly compose neural
networks together. As motivation, we may want to do this to faciliate interaction between networks,
such as for regularization or distillation. But given two image classiﬁers, it is not clear how to tie
outputs from one neural network to inputs for the other? Both networks expect images as input.

The workaround comes from decodable activations. Since they are images of the same shape as
inputs, we are free to treat them as inputs for another model. So, we can compose neural networks
together using decoded activations as an intermediary: given two models f1 and f2, and an input
x, we can take an activation h coming from f1(x), decode it, then provide it to the other model,
f2(gφ(h)). In the following subsections, we explore two instances of this compositionality: one
by composing a model with itself (i.e., f1 = f2), and the other by composing a model f1 with a
pretrained one, f2. See Figure 1b,c for a graphical depiction of both compositions.

3

4.1 Recursive Self-Composition

l

l

Our ﬁrst composition will be to compose a DecNN model with itself. But if we can do that, we can
also recursively compose a DecNN model with itself inﬁnitely:
Given a decoded activation gφ(h(0)
) for any layer l, we can feed it as input back into the classiﬁer,
fθ(gφ(h(0)
)) and maximize agreement with the true label y. Intuitively, if gφ(hl) is a good reconstruc-
tion of the input x (i.e. gφ(hl) and x belong to the same set S(hl)), then a good classiﬁer fθ should
predict the correct label with it as input. Now, we also observe that the computation fθ(gφ(h(0)
))
produces L additional activations, h(1)
L where we use the superscript to represent the number
of compositions. We can repeat this exact process with the same intuition, decoding h(1)
and passing
l
it to fθ to produce h(2)
. Computational limits aside, this can be repeated ad inﬁnitum.
One interpretation of this procedure is as data augmentation: we saw in Section A.3 that decoded
activations share features relevant to the prediction task (e.g. generated celebrities wear hats) but
vary other irrelevant features (e.g. face shape, hair color, etc.). Notably, these generated images are
not in the original dataset. Thus, training a model using decoded activations as inputs is similar to
augmenting the initial dataset. The only difference is that gradients are propagated back through
recursion to the inputs, which is also a form of regularization.

1 , . . . , h(1)

l

l

Admittedly, what we have described so far is intractable. Besides recursing inﬁnitely, each activation
produces L more activations, creating a tree (with root node x) with a large branching factor L. In
practice, we make two adjustments — First, we limit the recursion to a max depth D. Second, instead
of constructing the full graph of activations, which would have O(LD) nodes, we randomly sample
“paths” in the graph. Speciﬁcally, starting at the root, we compute {h(0)
L } through fθ(x) and
randomly choose an integer l0 ∈ [1, L]. Then, we compute {h(1)
L } through fθ(gφ(h(0)
))
l0
i.e. we classify the l0-th decoded activation from depth 0. Again, we randomly pick l1 to compute
{h(2)
)) and repeat until depth D.

L } through fθ(gφ(h(1)
l1
Building on Equation 4, we can write this “recursive” objective as:

1 , . . . , h(2)

1 , . . . , h(1)

1 , . . . , h(0)

ˆL(x; θ, φ) = log p(y|fθ(x)) +

D
(cid:88)

d=1

αd log p(y|fθ(gφ(h(d−1)
ld−1

))) + β

D
(cid:88)

d=1

αd log p(x|gφ(h(d−1)
ld−1

)) (5)

where the sequence of integers l0, . . . lD−1 are each sampled uniformly from [1, L]. We introduce a
new hyperparameter α ∈ [0, 1] that geometrically downweights later depths (if α = 0, Equation 5
reduces to maximum likelihood). The ﬁrst term in Equation 5 is the usual classiﬁcation objective.
The second term is a sum of cross entropy terms that encourage the decoded activation at depth d to
predict the correct label through fθ. The third and ﬁnal term is a sum of reconstruction losses that
encourage the decoded activation at depth d to approximate the original input x.

We call this model a “Recursively Decodable Neural Network”, abbreviated ReDecNN. Note that
ReDecNN is a special case of an ensemble network. That is, every path sampled in the activation tree
by picking l0, . . . lD−1 builds a legitimate classiﬁer. There are O(LD) such classiﬁers implicit in the
ReDecNN. We can interpret optimizing Equation 5 as training individual classiﬁers that are Monte
Carlo sampled from the tree every gradient step. See Figure 1b for an illustration.

Our goal in the next few paragraphs is to utilize this ensemble to measure uncertainty.

Method

MNIST

Fashion

CelebA

Standard
DecNN
ReDecNN
Dropout
MC-Dropout
BayesNN
Ensemble

97.1 (0.11)
97.8 (0.17)
97.5 (0.20)
95.9 (0.13)
91.3 (0.14)
96.3 (0.51)
97.6 (0.11)

87.7 (0.19)
88.8 (0.19)
88.1 (0.16)
80.7 (0.24)
78.6 (0.23)
87.0 (0.36)
88.4 (0.12)

90.8 (<0.1)
90.8 (<0.1)
90.8 (0.16)
88.5 (0.12)
87.8 (0.19)
88.2 (0.22)
90.9 (0.11)

(a) Classiﬁcation performance on test set.

Method

MNIST

Fashion

CelebA

ReDecNN
MC-Dropout
BayesNN
Ensemble

0.869
0.878
0.537
0.531

0.795
0.818
0.574
0.559

0.692
0.657
0.587
0.605

(b) ROC-AUC of separating correctly classiﬁed
and misclassiﬁed examples using Eq. 6 as a score.
The stdev. for all entries over 3 runs are <0.01.

Measuring Uncertainty As a ﬁrst step, we compare ReDecNN to several baselines: a standard
neural network, a neural network with dropout, two kinds of Bayesian neural networks — Monte

4

Carlo dropout [11], abbreviated MC-Dropout, and weight uncertainty [4], abbreviated BayesNN —
and a naive ensemble network where we train D copies of the standard neural network with different
initializations. For all experiments, see Appendix A.1 and A.2 for training and task details. Table 1a
shows accuracies over a held-out test set, averaged over three runs. While we observe lower accuracy
from Bayesian NNs, we observe (perhaps surprisingly) equal performance of DecNN and ReDecNN
to a standard neural network. Typically, adding uncertainty to neural networks comes at a cost to
performance (as is true for other baselines), but this seems to not be the case here.

The next step is to compare the quality of model uncertainty. For the ReDecNN, for a given input x,
we measure uncertainty as follows: sample N different classiﬁers from the activation tree, and make
predictions with each classiﬁer on x. The uncertainty is the entropy of the predictions:

Uncertainty(x) = −

K
(cid:88)

c=1

ˆp(y = c) log ˆp(y = c)

(6)

where ˆp(y = c) is the empirical probability (e.g. normalized count) of predicting class c out of N
classiﬁers. A higher uncertainty metric would represent more disagreement between classiﬁers. We
can compute the same metric for a naive ensemble, as well as for MC-Dropout (where repeated calls
drop different nodes) and BayesNN (by sampling weights from the learned posterior).

We want to test if the model’s uncertainty is useful. One way is to correlate the uncertainty with
when the model makes prediction errors (we call this “misclassiﬁcation”). It would be useful if the
model was less uncertain on correct predictions and more uncertain on incorrect ones. A practitioner
could then use uncertainty to anticipate when a model might make a mistake. Table 1b reports the
area under the ROC curve, or ROC-AUC of separating correctly and incorrectly classiﬁed examples
in the test set (using uncertainty as a score). A higher ROC-AUC, closer to 1, represents a better
separation. We ﬁnd that ReDecNN is competitive with MC-Dropout, even out performing it on
CelebA. In Section A.5, we include examples of images with low and high uncertainty.

Out-of-Distribution Detection A second application of uncertainty is detecting out-of-distribution
(OOD) inputs. Given an input that is far from the training distribution, a model is likely to make
a mistake. Instead, the model should recognize its uncertainty, and refuse to make a prediction.
There is a rich literature on detecting OOD inputs using downstream computation on a trained model
[18, 25, 24, 52, 19]. Unlike those works, we study OOD detection using uncertainty alone.

Adversarial (FGSM)
OOD (MNIST)
OOD (FashionMNIST)
OOD (CelebA)
Corrupt (Mean)
Corrupt (Stdev)

ReDecNN

0.787
-
0.812
0.893
0.727
0.075

MNIST
MC

0.817
-
0.888
0.943
0.785
0.113

Ensemble

ReDecNN

FashionMNIST
MC

Ensemble

ReDecNN

0.540
-
0.534
0.675
0.566
0.087

0.711
0.793
-
0.753
0.676
0.072

0.760
0.864
-
0.793
0.691
0.108

0.589
0.501
-
0.704
0.606
0.106

-
0.647
0.641
-
0.686
0.038

CelebA
MC

-
0.548
0.572
-
0.569
0.017

Ensemble

-
0.591
0.599
-
0.632
0.016

Table 2: ROC-AUC of predicting which examples are out-of-distribution (OOD). We vary OOD examples to be
adversarial, corrupted, or from a different dataset. Standard deviation for all entries are < 0.01 over three runs.

We explore three kinds of OOD examples: (1) adversarial examples crafted for a single classiﬁer
using FGSM [15], (2) examples from a different dataset (e.g. train on MNIST and use FashionMNIST
as OOD) as done in [25], and (3) corrupted examples by 14 image transformations (e.g. adding pixel
noise), borrowed from [31]. See Appendix A.4 for expanded corruption results.

Table 2 reports the ROC-AUC of separating inlier examples taken from the test split of the dataset the
models were trained on, and outlier examples. From Table 2, we observe ReDecNN is just under
MC-Dropout, but while MC-Dropout achieves these results at the cost of classiﬁcation accuracy,
ReDecNN does not (Table 1a). Furthermore, we ﬁnd ReDecNN generalizes better to CelebA, a more
complex image dataset, where it outperforms MC-Dropout (and other baselines).

Focusing on CelebA, we study a domain-speciﬁc OOD challenge by holding out all images with
positive annotations for a single attribute as out-of-distribution. Here, inlier and outlier examples are
very similar in distribution. We report ROC-AUCs in Table 3 for three held-out attributes (randomly
chosen): is the celebrity in the image “wearing a hat”, has “blonde hair”, or “bald”? As this is a more
challenging problem (since inlier and outlier examples are from the same dataset versus different
datasets), the performance is lower than in Table 2. But while MC-Dropout, BayesNN, and a naive
ensemble all perform near chance, ReDecNN is consistently near 0.6.

5

OOD: Wearing a Hat

Method

ROC-AUC

ReDecDNN
MC-Dropout
BayesNN
Ensemble

0.604 (<0.01)
0.519 (<0.01)
0.510 (0.01)
0.526 (<0.01)

OOD: Blond Hair

OOD: Bald

Method

ROC-AUC

Method

ROC-AUC

ReDecDNN
MC-Dropout
BayesNN
Ensemble

0.593 (<0.01)
0.502 (<0.01)
0.508 (0.01)
0.509 (<0.01)

ReDecDNN
MC-Dropout
BayesNN
Ensemble

0.615 (<0.01)
0.502 (<0.01)
0.508 (<0.01)
0.503 (<0.01)

Table 3: We hold out a group of CelebA attributes, such as those wearing a hat, when training. We
compute the ROC-AUC of labeling the held-out group as OOD (mean and stdev. over 3 runs).

Calibration A third application of uncertainty we explore is calibration of predicted probabilities.
Standard neural networks are notoriously over-conﬁdent and are incentived in training to predict
with extreme probabilities (e.g. 0.99). A model with useful uncertainty would make predictions with
proper probabilities. Although many calibration algorithms exist [33, 16], we want to measure how
calibrated each model is out-of-the-box to compare the quality of model uncertainties.

Table 4a reports the expected calibration error, or ECE [32], which approximates the difference in
expectation between conﬁdence and accuracy using discrete bins. A lower number (closer to 0)
represents a more calibrated model. We observe that while a standard neural network has a relatively
high ECE, many of the approaches (dropout, MC-Dropout, BayesNN, and ReDecNN) reduce ECE.
On the other hand, naive ensembles increase the calibration error, as they are more prone to overﬁtting.
In two of three datasets, ReDecNN achieves the lowest ECE whereas MC-Dropout achieves the lowest
in the third. These results should be viewed in parallel to Table 1a, which measures “sharpness”.
Otherwise, we can trivially reduce ECE to zero by ignoring the input.

Method

MNIST

Fashion

CelebA

Standard
ReDecNN
Dropout
MC-Dropout
BayesNN
Ensemble

1.18 (0.10)
0.33 (0.05)
0.63 (0.09)
0.41 (0.06)
0.91 (0.20)
1.80 (0.12)

0.56 (0.14)
0.21 (0.03)
0.48 (0.09)
0.13 (0.03)
0.18 (0.03)
0.89 (0.07)

2.75 (0.13)
1.92 (0.13)
2.51 (0.13)
2.99 (0.15)
2.41 (0.19)
3.12 (0.12)

(a) We compare the expected calibration error (ECE)
of ReDecNN to a standard neural network with var-
ious regularization and ensembling.

Depth

Acc.

OOD (Fashion)

OOD (CelebA)

2
4
6
8

97.7
97.6
97.7
97.5

0.665
0.788
0.805
0.812

0.752
0.804
0.868
0.893

(b) Effect of recursion depth D on classiﬁcation
accuracy and OOD detection for a ReDecNN model
(8-layer MLP) trained on MNIST.

Effect of Depth
Finally, we study the effect of depth on the efﬁcacy of ReDecNN. For the
experiments above, we chose the recursive depth to be equal to the number of layers in the MLP
(D = L = 8). In Table 4b, we vary the depth from 2 to 8 (while keeping the number of layers
ﬁxed to 8) and study the effect on classiﬁcation accuracy and OOD detection. We ﬁnd increasing
OOD performance as D increases, although with marginally decreasing gains. Moreover, we observe
constant accuracy, matching standard neural networks regardless of the choice of D.

4.2 Composing with Pretrained Models

Apart from self-composition, we can also compose our neural networks with off-the-shelf pretrained
models as a form of regularization or distillation. Suppose we have a pretrained model m that maps
an input x to a class prediction. We can modify the ReDecNN objective as follows:

˜L(x; θ, φ) = ˆL(x; θ, φ) +

D
(cid:88)

d=1

αd log p(fθ(gφ(h(d−1)
ld−1

))|m(gφ(h(d−1)
ld−1

)))

(7)

where ˆL is as deﬁned in Equation 5. The additional term acts as a divergence, bringing the classifer’s
predictions close to those of the pretrained model. A similar edit can be made to the DecNN. We
revisit Section 4 experiments now using two pretrained models — a linear classiﬁer or a ResNet-18
classiﬁer. For each, we optimize Equation 7 and compute performance on OOD detection.

Table 5 compares the results of composing with either (1) a linear model, (2) a residual network,
or (3) no composition (None). Using a linear model, we observe a slight drop in test accuracy but
an increase across all OOD experiments and calibration. In fact, these new results rival or surpass
MC-Dropout from Table 2. Conversely, with ResNet-18, we observe a 1 point increase in test

6

Method

None

Accuracy
Misclassﬁcation
Adversarial
OOD (MNIST)
OOD (Fashion)
OOD (CelebA)
Corruption (Mean)
Calibration (ECE)

97.50 (0.20)
0.869 (<0.01)
0.787 (<0.01)
-
0.812 (<0.01)
0.893 (<0.01)
0.727 (<0.01)
0.334 (0.05)

MNIST
Linear

97.17 (0.12)
0.904 (<0.01)
0.805 (0.01)
-
0.845 (<0.01)
0.928 (<0.01)
0.790 (<0.01)
0.296 (0.01)

ResNet

None

98.30 (0.24)
0.742 (<0.01)
0.729 (<0.01)
-
0.650 (<0.01)
0.862 (<0.01)
0.694 (<0.01)
0.385 (0.02)

90.80 (<0.1)
0.692 (<0.01)
-
0.647 (<0.01)
0.641 (<0.01)
-
0.686 (<0.01)
1.927 (0.13)

CelebA
Linear

89.76 (0.11)
0.732 (<0.01)
-
0.693 (<0.01)
0.673 (<0.01)
-
0.708 (<0.01)
1.937 (0.04)

ResNet

91.28 (0.13)
0.615 (<0.01)
-
0.609 (<0.01)
0.585 (<0.01)
-
0.616 (<0.01)
3.002 (0.10)

Table 5: OOD detection using uncertainty for ReDecNN composed with pretrained models.

accuracy but notable drops in OOD and calibration results. These polarizing outcomes show two use
cases of pretrained models: using simpler models allow for regularization that trades off accuracy for
robustness while more complex models encourage distillation, making the opposing tradeoff.

Eq. 7

Method

Direct Reg.

96.72
0.828
0.707
0.699
0.782
0.667
0.280

97.17
0.904
0.805
0.845
0.928
0.790
0.296

Accuracy
Misclass.
Adversarial
OOD (Fashion)
OOD (CelebA)
Corruption (Mean)
Calibration (ECE)

An open question is how much Equation 7 ben-
eﬁts from regularizing fθ(gφ(h)) to be close to
m(gφ(h)) versus regularizing it to be close to m(x).
The former is a pretrained embedding of a recon-
struction whereas the latter is of a static input. Ta-
ble 6 compares the two on MNIST, where we call
the latter “direct regularization”. We observe that
Equation 7 outperforms direct regularization (other
than calibration), often by a large margin. We rea-
son this to be because the input x and the recon-
struction gφ(h) can be very different, especially in
later layers as shown in Appendix A.3. This im-
plies that m(x) and m(g(h)) can be very different
as well. Intuitively, constraining the model to be similar to what a pretrained model thinks of the
reconstruction (which is a dynamic value that varies with model weights) is a stronger learning
signal than what the pretrained model thinks of the original input (which is static). In other words,
regularizing fθ(gφ(hk)) to be like m(x) at all layers l is too strict of a constraint.

Table 6: Composition (Eq. 7) versus direct regular-
ization with pretrained models on MNIST.

5 Constraining Decodable Computation

A milestone for deep learning would be the ability to specify what information an activation should
not capture. This is especially important in cases when we have protected attributes that should not
be abused in optimizing an objective. While this milestone remains out of grasp, we take a small
step towards it by optimizing a neural network to “be indifferent” with respect to a protected attribute
through composition with pretrained models.

Suppose we have access to a pretrained classiﬁer m that predicts the assignment for a K-way protected
feature. Then, building on Section 4.2, we can optimize our classiﬁer to “ignore” information about
the protected feature through m. To do this, we deﬁne the objective:

˜L(x; θ, φ) = ˆL(x; θ, φ) +

D
(cid:88)

d=1

αd log p(m(gφ(h(d−1)
ld−1

))|

1
K

1K)

(8)

where 1K is a K-dimensional vector of ones. That is, Equation 8 encourages the decoded activation
gφ(h(d−1)
), when passed through the protected classiﬁer m, to predict chance probabilities i.e. have
ld−1
no information about the protected attribute. A high performing classiﬁer trained in this manner must
have solved the prediction task without abusing the protected attribute(s).

To test this, we extract two attributes from CelebA: “beardedness” and “baldness”. Suppose we wish
to design a neural network to predict beardedness. If we were to naively do so, there could be a
discrepancy in performance between groups of individuals — for example, between bald individuals
and individuals with hair, the latter group is more common in the CelebA dataset. Indeed, Table 7
shows a 10 point difference in F1 and a 5 point difference in average precision (AP) in classifying
beardedness between the two groups. The second row of Table 7 provides a baseline ReDecNN

7

Model

F1 (Bald / Not Bald)

AP (Bald / Not Bald)

ECE (Bald / Not Bald)

Standard
ReDecNN (Eq. 4)
ReDecNN (Eq. 8)

0.338/0.438 (<0.01 / <0.01)
0.327/0.484 (<0.01 / <0.01)
0.461/0.501 (0.01 / <0.01)

19.4/23.9 (0.08 / 0.09)
18.6/22.3 (0.11 / 0.14)
27.2/28.9 (0.10 / 0.10)

4.76/3.56 (0.10 / 0.06)
4.74/3.54 (0.08 / 0.07)
3.80/3.28 (0.12 / 0.09)

Table 7: We use a pretrained model on CelebA that predicts baldness to optimize the activations of a
second classiﬁer to ignore baldness when predicting beardedness.

optimized without knowledge of the protected attribute. Unsurprisingly, we ﬁnd a similar discrepancy
between groups, as with a standard neural network. In the third row, we evaluate a ReDecNN that
was optimized to ignore the “baldness” attribute using a pretrained classiﬁer for baldness. Critically,
we ﬁnd much more balanced (and higher) F1, AP, and ECE across the two groups.

Method

Standard
ReDecNN (Eq. 5)
ReDecNN (Eq. 8)

F1

0.383
0.371
0.108

Table 8: Quantitatively accessing
ability to classify baldness.

One unsatisfying but plausible explanation for Table 7 is that the
generative model learned to construct adversarial examples such
that the pretrained model m cannot predict the protected feature,
but that the protected feature is still being used to make predictions
by f . To show this is not the case, we take a trained ReDecNN
optimized by Equation 8, freeze its parameters, and ﬁt a small
linear head on top of the last hidden layer to predict the protected
feature. If the model learned to ignore this feature, it should not
be able to perform the task well. Table 8 reports a poor F1 score
(0.1) whereas doing the same with a standard neural network or
with an unconstrained ReDecNN gets 3 times the F1 score.

6 Generalizing to other Modalities

While we have focused on image classiﬁcation, the proposed approach is more general and can be
extended to other modalities. We apply the same decodable representations to speech, in particular
utterance and action recognition. We compare DecNN and ReDecNN to the same baselines, measuring
accuracy and uncertainty through similar experiments as we did for images.

Method

Acc

Misclass.

OOD

ECE

Method

Acc

Misclass.

OOD

ECE

Standard
DecNN
ReDecNN
MC-Dropout
BayesNN
Ensemble

94.5 (0.1)
93.8 (0.2)
93.4 (0.2)
82.1 (0.4)
91.6 (0.7)
96.3 (0.1)

-
-
0.766
0.788
0.545
0.529

-
-
0.705
0.745
0.518
0.506

-
-
0.225 (0.1)
0.429 (0.2)
1.039 (0.4)
1.103 (0.2)

Standard
DecNN
ReDecNN
MC-Dropout
BayesNN
Ensemble

42.4 (0.4)
41.4 (0.7)
41.2 (0.5)
34.5 (0.7)
40.3 (1.2)
44.1 (0.1)

-
-
0.642
0.629
0.523
0.541

-
-
0.605
0.562
0.500
0.522

-
-
0.523 (0.2)
0.515 (0.2)
0.918 (0.1)
1.189 (0.5)

(a) AudioMNIST

(b) Fluent Speech Commands

Table 9: Performance on speech classiﬁcation. If not speciﬁed, stdev. is <0.01 over three test runs.

We utilize the AudioMNIST [3] and Fluent speech com-
mands [27] datasets, the former being a corpus of 60k
utterances of the digits 0 through 9, and the latter a corpus
of 100k recordings of 97 speakers interacting with smart-
home appliances for one of six actions. Audio waveforms
are preprocessed to log Mel spectrograms, outputting a 32
by 32 matrix [47]. Figure 2 shows the input on the left-
most column along with the 2nd, 4th, 6th, and 8th decoded
activations for 3 random test examples. Table 9 reports
the ﬁndings, where like the image experiments, we ﬁnd
ReDecNN has strong performance on OOD detection and
calibration in return for only a small drop in performance
compared to a standard neural network.

8

Figure 2: Decoding activations to im-
ages for Fluent spectrograms.

7 Related Work

Autoencoders Although DecNNs are reminiscent of autoencoders, there are differences: while
autoencoders only reconstruct the ﬁnal hidden layer, DecNN has a reconstruction for every single
hidden layer. This, and because DecNNs have a supervised objective (which autoencoders do not have)
critically changes what information is captured in the hidden layers. Whereas the autoencoder seeks
perfect reconstruction, the DecNN does not, as evidenced by loss of information for reconstructions
later in the network (see Appendix A.3). Finally, we point out that the applications of autoencoders
are very different from the DecNN. The former is not used for calibration or composition.

Invertible and Ensemble Networks
The DecNN is akin to invertible generative models [6, 36] as
DecNN is training a generative model to “invert” the classiﬁer up to a hidden layer. However, unlike
invertible ﬂows, DecNN does not impose an explicit prior on the activations (i.e. latent variables),
and further, DecNN has a supervised objective. Our proposed recursive network, ReDecNN, has
similarities to Bayesian neural networks [11, 4] and ensemble networks [48] — two baselines we
compare against for evaluating model uncertainty. However, we ﬁnd ReDecNN to be easier to train
than doing inference over weights, and cheaper in parameter count than naive ensembles.

Probing Neural Networks A related line of work seeks to probe neural network post-training to
understand the underlying computation [1]. Most relevant to our work is an approach that inverts
CNN activations through reconstruction [28]. Unlike this approach, we are not proposing any
additional computation post-hoc. Rather, we are interested in optimizing neural networks such that
their representations are more easily mapped back to the input domain.

Robustness and Protected Attributes Out-of-distribution detection [26, 24, 52, 40, 20, 19],
selective classiﬁcation [12, 13], adversarial perturbation detection [49, 10, 30, 34], and neural
network calibration [16, 51, 50, 32, 38] each have a rich subﬁeld of algorithms. While task-speciﬁc
methods likely outperform our results, we are excited about representations that enable uncertainty
without task information or additional algorithms. Our approach also shares similarities to [42]
where the authors minimize the mutual information between learned representations and a protected
attribute. We view Equation 8 as an approximation of this where we treat the pretrained classiﬁer for
the protected attribute as a proxy for the mutual information.

8 Limitations and Future Work

CelebA

MNIST

Method

Fashion

13.5 (0.2)
99.2 (9.6)
78.6 (6.6)
14.0 (0.1)
14.0 (0.1)
132.6 (8.5)

13.6 (0.3)
80.5 (6.8)
96.7 (9.7)
13.9 (0.6)
13.9 (0.6)
109.6 (7.1)

82.6 (6.2)
486.2 (12.9)
521.6 (9.2)
79.5 (5.7)
79.5 (5.7)
659.4 (8.2)

Standard
DecNN
ReDecNN
Dropout
MC-Dropout
Ensemble

To close, we discuss a few limitations. First, our
approach is bottlenecked by the quality of the gen-
erative model. Without a good reconstruction, op-
timization will be intractable. However, in light
recent work [45, 44, 5, 21, 43], this is becoming
less of a problem as new generative models surface.
Second, in the main text, we only explored feedfor-
ward classiﬁers for simplicitly. Our approach ex-
tends naturally to residual and transformer blocks,
and future research could explore this direction. As
a start, we provide experiments for extensions to
CNN architectures in Appendix A.6. Third, opti-
mizing recursive networks, although parameter efﬁcient, costs more compute as backpropagation is
more expensive. See Table 10 for timings of 1 epoch in seconds. The proposed method does have
signiﬁcant computational overhead, averaging about 6x cost, while baseline methods like Dropout
and MC-Dropout impose little to no overhead. On the other hand, ReDecNN has comparable cost to
DecNN despite requiring recursive gradients. Also, both DecNN and ReDecNN are both cheaper
than naive ensembles, signiﬁcantly in the CelebA case. Future work could explore weight sharing
across layers (not just depth) to reduce compute.

Table 10: Cost (seconds) of 1 epoch on a Titan
X GPU (averaged over 10 epochs).

In summary, we explored building neural networks with decodable representations, and leveraged
this new property to compose models together. By re-purposing decoded activations as novel inputs,
we were able to join neural networks together in useful ways for out-of-distribution detection and
calibration, as well as inﬂuence what information a network retains in optimization. We are optimistic
about promising results and look to future research for more applications of decodability.

9

Acknowledgments and Disclosure of Funding

We thank the Ermon group for their comments and suggestions. We thank the reviewers for their
many iterations of feedback that helped this paper improve signiﬁcantly. MW is supported by the
Stanford Interdisciplinary Graduate Fellowship as the Karr Family Fellow.

References

[1] Guillaume Alain and Yoshua Bengio. Understanding intermediate layers using linear classiﬁer

probes. arXiv preprint arXiv:1610.01644, 2016.

[2] Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural machine translation by jointly

learning to align and translate. arXiv preprint arXiv:1409.0473, 2014.

[3] Sören Becker, Marcel Ackermann, Sebastian Lapuschkin, Klaus-Robert Müller, and Wojciech
Samek. Interpreting and explaining deep neural networks for classiﬁcation of audio signals.
CoRR, abs/1807.03418, 2018.

[4] Charles Blundell, Julien Cornebise, Koray Kavukcuoglu, and Daan Wierstra. Weight uncertainty
in neural network. In International Conference on Machine Learning, pages 1613–1622. PMLR,
2015.

[5] Prafulla Dhariwal and Alex Nichol. Diffusion models beat gans on image synthesis. arXiv

preprint arXiv:2105.05233, 2021.

[6] Laurent Dinh, Jascha Sohl-Dickstein, and Samy Bengio. Density estimation using real nvp.

arXiv preprint arXiv:1605.08803, 2016.

[7] Piero Esposito. Blitz - bayesian layers in torch zoo (a bayesian deep learing library for torch).

[8] et

https://github.com/piEsposito/blitz-bayesian-deep-learning/, 2020.
GitHub.

lightning.
https://github.com/PyTorchLightning/pytorch-lightning, 3, 2019.

Pytorch

Falcon,

WA.

al.

Note:

[9] William Falcon and Kyunghyun Cho. A framework for contrastive self-supervised learning and

designing a new approach. arXiv preprint arXiv:2009.00104, 2020.

[10] Reuben Feinman, Ryan R Curtin, Saurabh Shintre, and Andrew B Gardner. Detecting adversarial

samples from artifacts. arXiv preprint arXiv:1703.00410, 2017.

[11] Yarin Gal and Zoubin Ghahramani. Dropout as a bayesian approximation: Representing model
uncertainty in deep learning. In international conference on machine learning, pages 1050–1059.
PMLR, 2016.

[12] Yonatan Geifman and Ran El-Yaniv. Selective classiﬁcation for deep neural networks. arXiv

preprint arXiv:1705.08500, 2017.

[13] Yonatan Geifman and Ran El-Yaniv. Selectivenet: A deep neural network with an integrated
reject option. In International Conference on Machine Learning, pages 2151–2159. PMLR,
2019.

[14] Ian J Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil
Ozair, Aaron Courville, and Yoshua Bengio. Generative adversarial networks. arXiv preprint
arXiv:1406.2661, 2014.

[15] Ian J Goodfellow, Jonathon Shlens, and Christian Szegedy. Explaining and harnessing adversar-

ial examples. arXiv preprint arXiv:1412.6572, 2014.

[16] Chuan Guo, Geoff Pleiss, Yu Sun, and Kilian Q Weinberger. On calibration of modern neural
networks. In International Conference on Machine Learning, pages 1321–1330. PMLR, 2017.

[17] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image
recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition,
pages 770–778, 2016.

[18] Dan Hendrycks and Kevin Gimpel. A baseline for detecting misclassiﬁed and out-of-distribution

examples in neural networks. arXiv preprint arXiv:1610.02136, 2016.

[19] Dan Hendrycks, Mantas Mazeika, and Thomas Dietterich. Deep anomaly detection with outlier

exposure. arXiv preprint arXiv:1812.04606, 2018.

10

[20] Dan Hendrycks, Mantas Mazeika, Saurav Kadavath, and Dawn Song. Using self-supervised
learning can improve model robustness and uncertainty. In Advances in Neural Information
Processing Systems, pages 15663–15674, 2019.

[21] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. arXiv

preprint arXiv:2006.11239, 2020.

[22] Diederik P Kingma and Max Welling. Auto-encoding variational bayes. arXiv preprint

arXiv:1312.6114, 2013.

[23] Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. Imagenet classiﬁcation with deep
convolutional neural networks. Advances in neural information processing systems, 25:1097–
1105, 2012.

[24] Kimin Lee, Kibok Lee, Honglak Lee, and Jinwoo Shin. A simple uniﬁed framework for
detecting out-of-distribution samples and adversarial attacks. In Advances in Neural Information
Processing Systems, pages 7167–7177, 2018.

[25] Shiyu Liang, Yixuan Li, and R Srikant. Enhancing the reliability of out-of-distribution image
detection in neural networks. In International Conference on Learning Representations, 2018.

[26] Shiyu Liang, Yixuan Li, and Rayadurgam Srikant. Enhancing the reliability of out-of-
distribution image detection in neural networks. arXiv preprint arXiv:1706.02690, 2017.

[27] Loren Lugosch, Mirco Ravanelli, Patrick Ignoto, Vikrant Singh Tomar, and Yoshua Bengio.
Speech model pre-training for end-to-end spoken language understanding. arXiv preprint
arXiv:1904.03670, 2019.

[28] Aravindh Mahendran and Andrea Vedaldi. Understanding deep image representations by invert-
ing them. In Proceedings of the IEEE conference on computer vision and pattern recognition,
pages 5188–5196, 2015.

[29] Alireza Makhzani, Jonathon Shlens, Navdeep Jaitly, Ian Goodfellow, and Brendan Frey. Adver-

sarial autoencoders. arXiv preprint arXiv:1511.05644, 2015.

[30] Jan Hendrik Metzen, Tim Genewein, Volker Fischer, and Bastian Bischoff. On detecting

adversarial perturbations. arXiv preprint arXiv:1702.04267, 2017.

[31] Norman Mu and Justin Gilmer. Mnist-c: A robustness benchmark for computer vision. arXiv

preprint arXiv:1906.02337, 2019.

[32] Mahdi Pakdaman Naeini, Gregory Cooper, and Milos Hauskrecht. Obtaining well calibrated
probabilities using bayesian binning. In Proceedings of the AAAI Conference on Artiﬁcial
Intelligence, volume 29, 2015.

[33] Alexandru Niculescu-Mizil and Rich Caruana. Predicting good probabilities with supervised
learning. In Proceedings of the 22nd international conference on Machine learning, pages
625–632, 2005.

[34] Tianyu Pang, Chao Du, Yinpeng Dong, and Jun Zhu. Towards robust detection of adversarial

examples. In NeurIPS, 2018.

[35] George Papamakarios, Eric Nalisnick, Danilo Jimenez Rezende, Shakir Mohamed, and Balaji
Lakshminarayanan. Normalizing ﬂows for probabilistic modeling and inference. arXiv preprint
arXiv:1912.02762, 2019.

[36] George Papamakarios, Theo Pavlakou, and Iain Murray. Masked autoregressive ﬂow for density

estimation. arXiv preprint arXiv:1705.07057, 2017.

[37] Guim Perarnau, Joost Van De Weijer, Bogdan Raducanu, and Jose M Álvarez. Invertible

conditional gans for image editing. arXiv preprint arXiv:1611.06355, 2016.

[38] John Platt et al. Probabilistic outputs for support vector machines and comparisons to regularized

likelihood methods. Advances in large margin classiﬁers, 10(3):61–74, 1999.

[39] Robin Rombach, Patrick Esser, and Bjorn Ommer. Network-to-network translation with
conditional invertible neural networks. Advances in Neural Information Processing Systems, 33,
2020.

[40] Chandramouli Shama Sastry and Sageev Oore. Detecting out-of-distribution examples with

in-distribution examples and gram matrices. arXiv preprint arXiv:1912.12510, 2019.

11

[41] Jürgen Schmidhuber. Deep learning in neural networks: An overview. Neural networks,

61:85–117, 2015.

[42] Jiaming Song, Pratyusha Kalluri, Aditya Grover, Shengjia Zhao, and Stefano Ermon. Learning
controllable fair representations. In The 22nd International Conference on Artiﬁcial Intelligence
and Statistics, pages 2164–2173. PMLR, 2019.

[43] Jiaming Song, Chenlin Meng, and Stefano Ermon. Denoising diffusion implicit models. arXiv

preprint arXiv:2010.02502, 2020.

[44] Yang Song and Stefano Ermon. Generative modeling by estimating gradients of the data

distribution. arXiv preprint arXiv:1907.05600, 2019.

[45] Yang Song, Jascha Sohl-Dickstein, Diederik P Kingma, Abhishek Kumar, Stefano Ermon, and
Ben Poole. Score-based generative modeling through stochastic differential equations. arXiv
preprint arXiv:2011.13456, 2020.

[46] Ilya Sutskever, Oriol Vinyals, and Quoc V Le. Sequence to sequence learning with neural

networks. arXiv preprint arXiv:1409.3215, 2014.

[47] Alex Tamkin, Mike Wu, and Noah Goodman. Viewmaker networks: Learning views for

unsupervised representation learning. arXiv preprint arXiv:2010.07432, 2020.

[48] Sean Tao. Deep neural network ensembles. In International Conference on Machine Learning,

Optimization, and Data Science, pages 1–12. Springer, 2019.

[49] Weilin Xu, David Evans, and Yanjun Qi. Feature squeezing: Detecting adversarial examples in

deep neural networks. arXiv preprint arXiv:1704.01155, 2017.

[50] Bianca Zadrozny and Charles Elkan. Obtaining calibrated probability estimates from decision
trees and naive bayesian classiﬁers. In Icml, volume 1, pages 609–616. Citeseer, 2001.
[51] Bianca Zadrozny and Charles Elkan. Transforming classiﬁer scores into accurate multiclass
probability estimates. In Proceedings of the eighth ACM SIGKDD international conference on
Knowledge discovery and data mining, pages 694–699, 2002.

[52] Ev Zisselman and Aviv Tamar. Deep residual ﬂow for out of distribution detection.

In
Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages
13994–14003, 2020.

12

Checklist

1. For all authors...

(a) Do the main claims made in the abstract and introduction accurately reﬂect the paper’s

contributions and scope? [Yes]

(b) Did you describe the limitations of your work? [Yes, please refer to Section 8.]
(c) Did you discuss any potential negative societal impacts of your work? [Yes, please

refer to Section 8.]

(d) Have you read the ethics review guidelines and ensured that your paper conforms to

them? [Yes]

2. If you are including theoretical results...

(a) Did you state the full set of assumptions of all theoretical results? [No theoretical

results]

(b) Did you include complete proofs of all theoretical results? [No theoretical results]

3. If you ran experiments...

(a) Did you include the code, data, and instructions needed to reproduce the main ex-
perimental results (either in the supplemental material or as a URL)? [Yes, we have
included anonymized code in the supplement. All data used is public.]

(b) Did you specify all the training details (e.g., data splits, hyperparameters, how they

were chosen)? [Yes, all choices are speciﬁed in detail in the Appendix.]

(c) Did you report error bars (e.g., with respect to the random seed after running experi-

ments multiple times)? [Yes, please see the tables and captions.]

(d) Did you include the total amount of compute and the type of resources used (e.g., type
of GPUs, internal cluster, or cloud provider)? [Yes, please refer to Appendix.]
4. If you are using existing assets (e.g., code, data, models) or curating/releasing new assets...
(a) If your work uses existing assets, did you cite the creators? [Yes, for code we borrowed
from public implementations, we cite the authors and include URL references in
the Appendix.]

(b) Did you mention the license of the assets? [Yes, please refer to Appendix.]
(c) Did you include any new assets either in the supplemental material or as a URL? [Yes,

we include our source code.]

(d) Did you discuss whether and how consent was obtained from people whose data you’re

using/curating? [Not applicable.]

(e) Did you discuss whether the data you are using/curating contains personally identiﬁable

information or offensive content? [Not applicable.]

5. If you used crowdsourcing or conducted research with human subjects...

(a) Did you include the full text of instructions given to participants and screenshots, if

applicable? [No human subjects experiments]

(b) Did you describe any potential participant risks, with links to Institutional Review

Board (IRB) approvals, if applicable? [No human subjects experiments]

(c) Did you include the estimated hourly wage paid to participants and the total amount

spent on participant compensation? [No human subjects experiments]

13

A Appendix

A.1 Training Details

In our experiments, the classiﬁer fθ is a 8-layer MLP with 128 hidden dimensions per layer. We
assume the same dimensionality per layer for simplicity but the approach easily supports MLPs with
variable hidden dimensionalities: one can train seperate generative models or use the technique from
[39] where smaller dimensionalities are padded. The generative model gθ is a ResNet18 decoder that
maps a 128 dimensional vector to a 3 by 32 by 32 pixel image for RGB images and 1 by 32 by 32 for
grayscale images and speech spectrograms. We repurpose the implementation from the PyTorch
Lightning Bolts [9] repository: https://github.com/PyTorchLightning/lightning-bolts/
blob/master/pl_bolts/models/autoencoders/components.py (Apache 2.0 License). (All
input images are reshaped to 32 by 32 pixels. No additional image transformations were used in train-
ing nor evaluation.) All models are trained for 50 epochs with batch size 128, Adam optimizer with
learning rate 1e-4. We set β = 10 in all cases. For all ReDecNN models, we use a max depth D = 8
and set α = 0.5 in all cases. To make for a fair comparison, for all naive ensemble networks, we train
8 copies of a neural network with different weight initializations. For baseline models using dropout
and MC-Dropout, we use 0.5 dropout probability. For weight uncertainty (i.e. BayesNN), we use
the Blitz library [7], https://github.com/piEsposito/blitz-bayesian-deep-learning
(GNU V3 License). All pretrained models used were ResNet-18 classiﬁers. Our ResNet classiﬁer
implementation was adapted from torchvision. For each of the architectures, to support 32x32
images (which are smaller than the standard), we replace the ﬁrst 7x7 convolutional layer with a
3x3 convolutional layer and reduce the ﬁrst max pooling layer. We did not use their pretrained
weights from torchvision and instead trained them ourselves on MNIST, FashionMNIST,
and CelebA from scratch (again 50 epochs). All models were trained on a single Titan Xp
GPU with 4 CPUs for data loading. An average model takes 1 hours to train for MNIST and
FashionMNIST and 4 hours for CelebA, and speech experiments. In our experiments, we compute
uncertainty using 30 random samples (e.g.
for ReDecNN, this is 30 paths; for MC-Dropout,
this is 30 different dropout conﬁgurations, etc.). ROC-AUC computation is done through
scikit-learn (BSD License). In CelebA, only a subset of 18 attributes are used, chosen for
visual distinctiveness as done in [37]. Conveniently, this removes many trivial features that are
mostly of a single class as well. In experiments, we utilize the PyTorch Lightning framework
[8] (Apache 2.0 License) and Weights and Biases (MIT License) for tracking. For speech
experiments, the AudioMNIST dataset is found at https://github.com/soerenab/AudioMNIST
(MIT License) and the Fluent dataset can be downloaded at https://fluent.ai/
fluent-speech-commands-a-dataset-for-spoken-language-understanding-research/
(academic license). All speech spectrograms are normalized using a mean and standard deviation
computed from the training dataset. We use torchaudio and librosa to efﬁciently compute Mel
spectrograms.

A.2 Dataset & Task Details

We describe the setup for each set of results presented in the main text, paying close attention to the
data used, and any special setup required.

Table 1a
For classiﬁcation, we use datasets as in standard practice. MNIST and FashionMNIST
are both classiﬁcation tasks with 10 classes. CelebA has 18 binary attributes, and we optimize an
objective containing the sum of 18 binary cross entropy terms, one for each attribute. Accuracy is
reported using the respective test set.

Table 1b Given all test examples, we make predictions and get uncertainty scores using the trained
model. We split the test set into two groups, those we correctly classiﬁed, and those we misclassiﬁed.
Then, we compute the ROC-AUC with uncertainty as the prediction score. An ROC-AUC of 1 would
mean that the model perfectly assigned high uncertainty to examples it misclassiﬁed.

Table 2 OOD detection experiments are very similar to the misclassiﬁcation experiments except
we deﬁne groups differently. Generally, there is inlier group and an outlier group. We compute
uncertainty scores over both groups and then compute ROC-AUC with uncertainty as the prediction
score. An ROC-AUC of 1 would mean that the model perfectly assigned high uncertainty to outlier
examples. Table 2 has three different ways of deﬁning inliers and outliers. First, ﬁxing a dataset and
trained classiﬁer, we can compute adversarial examples with FGSM on the test set. This serves as the

14

outlier set while the original test set serves as the inlier. Second, for rows labeled OOD (XXX), we
take the test set from dataset XXX to be the outlier set while the test set from training dataset acts as
the inlier set. Finally, for corruptions, we compute image transformations on the test set (e.g. blur
all image in the test set) and treat this as the outlier set (again, the original test set acts as the inlier
set). We do this separately for each corruption transformation, and report the average and standard
deviation (over corruptions) in the main text.

Table 3
In Table 2, we limited OOD experiments to using a separate dataset as outliers. For
CelebA, we can consider a more difﬁcult task by holding out all images with a positive label for a
single attribute. We consider three such attributes: hold out all people wearing a hat, all people with
blond hair, or all people who are bald. Because a model with uncertainty has not seen any images of
people wearing hats (for example), it should assign these images higher uncertainty. The difﬁculty
of this task comes from outlier and inlier inputs being very similar in features; they are now both
images of celebrities, rather than images of digits. In all cases, the inlier set is deﬁned to be all test
set images without a positive label for the held out attribute.

Table 4a
and CelebA seperately. The labels used for binning are the standard dataset annotations.

ECE is computed on predicted probabilities using the test set for MNIST, FashionMNIST,

Table 4b

Same setup as in Table 2 but we vary the recursion depth for ReDecNN.

Table 5 Design of experiments in the Table are as described above for misclassiﬁcation, OOD, and
calibration. We pretrain a linear model and a ResNet-18 model using the training set of MNIST and
CelebA, separately. For CelebA, pretrained models optimize a sum of 18 binary cross entropy terms.

Table 6

Same setup as in Table 5.

Table 7 Using the CelebA dataset, we discard all attributes except the “baldness” and “beardedness”
columns (note this does not change the size of the dataset). We ﬁt a ResNet-18 model to predict
baldness only (here, we do not use the beardedness label). Following this, we train a ReDecNN
with Equation 8 to ignore the protected attribute (baldness) when predicting beardedness. We then
compute the F1, AP, and ECE scores separately for two groups, one containing images of all bald
individuals in the test set and one containing images of all non-bald individuals in the test set. These
two groups will not be equal in size but are sufﬁciently large to compute statistics.

Table 8 Given the CelebA dataset, we only use the “baldness” attribute. Given a trained model
(a standard neural network or a ReDecNN), we freeze its parameters, ﬁnd the last hidden layer (the
L-th one), and initialize linear head on top. This linear head returns a binary prediction: bald or not
bald. We optimize this model with binary cross entropy and see if the frozen weights contain the
information to predict baldness.

Table 9
See Appendix A.1 for the dataset links for AudioMNIST and Fluent Speech Commands.
We preprocess waveforms in each to be mel spectrograms of ﬁxed size 1x32x32, mimicking the
CIFAR10 setup (note that there is only 1 channel). See Figure 2 left-most column for three examples
of input spectrograms. AudioMNIST has 10 output labels whereas Fluent Speech Commands has 6
output labels. The task and data setup remain the same as in Table 2 for OOD, misclassiﬁcation, and
calibration experiments.

Table 10 Given a dataset and model, we use the time.time function in Python to record the start
and end times for each epoch on a Titan X GPU. We do so for 10 contiguous epochs and report the
average (and standard deviation). Eight data workers are used to load images (increasing this will
decrease the cost per epoch).

Table 11
but reports each individually.

This is identical to Table 2 but does not report the average over corruption experiments,

Table 12

This is identical to Table 2 but replaces an MLP with stacked convolutional layers.

A.3 Visualizing Decodable Activations

The most straightforward application of decodable activations is that we can visualize them. Figure 3
shows randomly chosen examples taken from DecNNs trained on FashionMNIST and CelebA. In
each subﬁgure, the leftmost column is from the dataset while the remaining eight columns are decoded
activations from layer 1 to 8 (fθ is an MLP with L = 8 layers).

15

(a)

(b)

(c)

(d)

(e)

(f)

Figure 3: Visualizing activations: the top row shows decoded activations when the model correctly
classiﬁed examples; the bottom row shows misclassiﬁcations.

The top row of three subﬁgures shows decoded activations for correctly-classiﬁed examples whereas
the bottom row of three subﬁgures shows mis-classiﬁed examples, all randomly chosen. For correctly-
classiﬁed examples, we observe that the decoded activations tends toward class prototypes. Decoded
images from the 7th or 8th activation lose details (e.g. patterns, logos, or shoe straps disappear from
clothing). The top row of Figure 3b presents a good example: the tank-top (a rare form of the t-shirt)
is projected to a more prototypical t-shirt. Furthermore, we also ﬁnd that mis-classiﬁed examples
morph into images of the incorrect class. The seventh row of Figure 3e morphs the two articles of
clothing that compose the dress to build a shirt. The seventh row of Figure 3f show the celebrity’s hat
transform into a background color.

Although we may be tempted to interpret these activations as revealing what the neural network
is learning, the objective (Equation 4) does not guarantee this, as reconstructions could appear
visually similar to the input x but map to completely different outputs y, and as such, serve as a
poor explanation as to what the underlying function f is doing. Future work could consider also
constraining reconstructions to map to the same label as f (x).

A.4 Extended OOD Results

We provide a more thorough breakdown of performance for the 14 different image corruptions. In
the main paper, we only show the average performance over all corruptions.

A.5 Visualizing OOD Examples

In the main paper, we present OOD results using ROC-AUC. Here, we visually inspect some examples
the model deems as OOD. Figure 4 shows one image from each class (of MNIST, FashionMNIST,
and CIFAR10) with low and high uncertainty. We observe that high uncertainty images are less
prototypical. For example, the high uncertainty digits in Figure 4d have accented curvature, whereas
the high uncertainty clothing in Figure 4e have more atypical designs, and the high uncertainty images
of celebrities in Figure 4f, although annotated as not bald, are wearing hats or have thin hairlines and
exposed foreheads. On the other hand, low uncertainty images in all datasets look more prototypical.

16

h1xh2h3h4h5h6h7h8zeroonetwothreefourfivesixseveneightninezeroonetwothreefourfivesixseveneightnineh1xh2h3h4h5h6h7h8t-shirt/toptrouserpulloverdresscoatsandalshirtsneakerbagankle boott-shirt/toptrouserpulloverdresscoatsandalshirtsneakerbagankle booth1xh2h3h4h5h6h7h8is baldis baldis baldzerozerozeroh1xh2h3h4h5h6h7h8oneonetwotwothreethreefourfourfivefivesixsixsevenseveneightninenineh1xh2h3h4h5h6h7h8t-shirt/toptrouserpulloverdresscoatsandalshirtsneakerbagankle boottrousertrouserdressdressdresssandalshirtbagbagbagh1xh2h3h4h5h6h7h8no hathas hat8Adversarial (FGSM)
OOD (MNIST)
OOD (FashionMNIST)
OOD (CelebA)
Corrupt (Brightness)
Corrupt (Dotted Line)
Corrupt (Glass Blur)
Corrupt (Impulse Noise)
Corrupt (Rotate)
Corrupt (Shear)
Corrupt (Spatter)
Corrupt (Translate)
Corrupt (Canny Edges)
Corrupt (Fog)
Corrupt (Scale)
Corrupt (Shot Noise)
Corrupt (Stripe)
Corrupt (Zigzag)
Corrupt (Mean)
Corrupt (Stdev)

ReDecNN

0.787
-
0.812
0.893
0.851
0.672
0.664
0.702
0.689
0.685
0.647
0.809
0.696
0.868
0.768
0.617
0.804
0.709
0.727
0.075

MNIST
MC

0.817
-
0.888
0.943
0.955
0.684
0.718
0.848
0.714
0.661
0.683
0.889
0.824
0.940
0.846
0.559
0.892
0.785
0.785
0.113

Ensemble

ReDecNN

FashionMNIST
MC

Ensemble

ReDecNN

0.540
-
0.534
0.675
0.702
0.546
0.515
0.551
0.523
0.502
0.502
0.511
0.521
0.732
0.535
0.526
0.755
0.514
0.566
0.087

0.711
0.793
-
0.753
0.732
0.602
0.617
0.702
0.754
0.611
0.617
0.725
0.703
0.748
0.808
0.541
0.684
0.626
0.676
0.072

0.760
0.864
-
0.793
0.667
0.589
0.584
0.746
0.839
0.809
0.615
0.826
0.807
0.670
0.805
0.537
0.522
0.658
0.691
0.108

0.589
0.501
-
0.704
0.816
0.535
0.525
0.642
0.600
0.543
0.534
0.555
0.571
0.810
0.521
0.528
0.772
0.532
0.606
0.106

todo
0.647
0.641
-
0.653
0.647
0.641
0.683
0.645
0.750
0.715
0.673
0.649
0.729
0.750
0.685
0.674
0.714
0.686
0.038

CelebA
MC

todo
0.548
0.572
-
0.558
0.559
0.596
0.555
0.585
0.581
0.569
0.577
0.569
0.608
0.562
0.542
0.563
0.555
0.569
0.017

Ensemble

todo
0.591
0.599
-
0.623
0.626
0.652
0.640
0.652
0.636
0.641
0.610
0.650
0.643
0.632
0.592
0.628
0.627
0.632
0.016

Table 11: We report the ROC-AUC of predicting which examples are out-of-distribution (OOD) using uncer-
tainty. We vary OOD examples to be adversarial, corrupted, or taken from a different dataset.

(a) MNIST (certain)

(b) FashionMNIST (certain)

(c) Not Bald (certain)

(d) MNIST (uncertain)

(e) FashionMNIST (uncertain)

(f) Not Bald (uncertain)

Figure 4: Randomly sampled images with low and high uncertainty. Images with high uncertainty appear less
“prototypical”. Figure 4(c,h) shows images from CelebA annotated as “not bald”. We observe images that the
model is uncertain about depict indviduals with a thin hair line, exposed foreheads, or wearing helmets.

A.6 Extensions to CNN Architectures

In the main text, we limited the experiments to MLP architectures for simplicity. Here, we include a
subset of the experiments in Section 4, replacing the MLP with stacked convolutional layers.

For this experiment, we use architectures of 8 total convolutional layers with ReLU nonlinearity and
64 ﬁlters. We use a U-Net decoder (again with 64 ﬁlters) as the generative model, which is better
suited to convolutional activations (which are now three dimensional rather than two). Like before,
we optimize for 50 epochs, batch size 128, learning rate 1e-4, and use Adam. For a baseline, we
compare against MC-Dropout, the most competitive baseline from the main text.

Method

Acc.

Misclass.

OOD (MNIST)

OOD (CelebA)

Corruption (Mean)

Calibration (ECE)

Standard
DecNN
ReDecNN
MC-Dropout

90.2
90.6
89.5
85.9

-
-
0.876
0.863

-
-
0.883
0.776

-
-
0.844
0.895

-
-
0.712
0.723

1.848
-
0.837
0.889

Table 12: DecNN with convolutional layers trained on FashionMNIST.

From Table 12, we see similar ﬁndings to the MLP results, although overall performance is higher due
to a more expressive architecture. Namely, ReDecNN, DecNN, and a standard NN have similar test
accuracy whereas MC-Dropout has a 4 point lower accuracy. When using the uncertainty score to de-
tect misclassiﬁcation, OOD, and corruption, we ﬁnd much closer performance between MC-Dropout
and ReDecNN (whereas in the paper, MC-Dropout outperformed ReDecNN on FashionMNIST

17

consistently), which is promising. Finally, for calibration, MC-Dropout and ReDecNN are again
comparable, with the latter having a slightly lower ECE score.

18

