6
1
0
2

v
o
N
2
2

]
I

N
.
s
c
[

1
v
0
0
4
7
0
.
1
1
6
1
:
v
i
X
r
a

A Deep Learning Based DDoS Detection System
in Software-Deﬁned Networking (SDN)

Quamar Niyaz∗, Weiqing Sun, Ahmad Y Javaid
{quamar.niyaz, weiqing.sun, ahmad.javaid}@utoledo.edu

College of Engineering

The University of Toledo

Toledo, OH-43606, USA

Abstract
Distributed Denial of Service (DDoS) is one of the most prevalent attacks that an organizational
network infrastructure comes across nowadays. We propose a deep learning based multi-vector
DDoS detection system in a software-deﬁned network (SDN) environment. SDN provides ﬂexi-
bility to program network devices for diﬀerent objectives and eliminates the need for third-party
vendor-speciﬁc hardware. We implement our system as a network application on top of an SDN
controller. We use deep learning for feature reduction of a large set of features derived from
network traﬃc headers. We evaluate our system based on diﬀerent performance metrics by
applying it on traﬃc traces collected from diﬀerent scenarios. We observe high accuracy with a
low false-positive for attack detection in our proposed system.
Keywords: Network security, Deep Learning, Multi-vector DDoS detection, Software Deﬁned
Networking

1

Introduction

Distributed denial of service (DDoS) attacks results in unavailability of network services by con-
tinuously ﬂooding its servers with undesirable traﬃc. Low-price Internet subscriptions and readily
available attack tools led to a vast increase in volume, size, and complexity of these attacks in the
recent past. According to the forecast of Cisco Visual Networking Index (VNI) [1], DDoS incidents
will reach up to 17 million in 2020, a threefold increment compared to 2015. The nature of attacks
has also changed to being multi-vector rather than having a single type of ﬂooding. A study reported
that 64% attacks until mid-2016 were multi-vectors that include TCP SYN ﬂoods and DNS/NTP
ampliﬁcation combined together [10]. Adversaries or hacktivists use DDoS attacks for extortion,
revenge, misguided marketing, and online protest. Many ﬁnancial, public sector, media, and social
entertainment sites are recent victims [9, 2, 11] and suﬀered from monetary and reputation damages.
Therefore, detection and mitigation of these attacks in real-time have become a prime concern for
large organizations.

Recently, both software-deﬁned networking (SDN) and deep learning (DL) have found several use-
ful and interesting applications in the industry as well as the research community. SDN provides cen-
tralized management, global view of the entire network, and programmable control plane; makes net-
work devices ﬂexible for diﬀerent applications. These features of SDN oﬀer better network monitoring

∗Corresponding author: quamar.niyaz@utoledo.edu

1

 
 
 
 
 
 
and enhanced security of the managed network compared to traditional networks [30, 33]. On the
other hand, DL based approaches outperformed existing machine learning techniques when applied to
various classiﬁcation problems. They improve feature extraction/reduction from a high-dimensional
dataset in an unsupervised manner by inheriting the non-linearity of neural networks [31]. Re-
searchers have also started to apply DL for the implementation of various intrusion detection systems
and observed desirable results discussed in Section 2. In this work, we implement a DDoS detection
system that incorporates stacked autoencoder (SAE) based DL approach in an SDN environment
and evaluate its performance on a dataset that consists of normal Internet traﬃc and various DDoS
attacks.

The organization of this paper is as follows. Section 2 discusses related work on DDoS detection
in an SDN environment and use of DL for network intrusion detection. Section 3 gives an overview of
SDN and SAE. In Section 4, we discuss the architecture of our proposed system. Section 5 presents
experimental set-up and performance evaluation of the system. Finally, Section 6 concludes the
paper with future work directions.

2 Related Work

We discuss the related work from two perspectives, one in which DL has been used for network
intrusion detection and the other in which DDoS detection is addressed in an SDN environment.

2.1 Intrusion detection using DL

In [29], Mostafa et al. used deep belief network (DBN) based on restricted Boltzmann machine
(RBM) for feature reduction with support vector machine (SVM) as a classiﬁer to implement a
network intrusion detection system (NIDS) on NSL-KDD [34] intrusion dataset. In [13], Ugo et al.
used discriminative RBM (DRBM) to develop a semi-supervised learning based network anomaly
detection and evaluated its performance in an environment where network traﬃc for training and
test scenarios were diﬀerent. They used real-world traﬃc traces and KDD Cup-99 [4] intrusion
dataset in their implementation. In [14], Gao et al. used RBM based DBN with a neural network as
a classiﬁer to implement an NIDS on KDD-Cup 99 dataset. In [17], Kang et al. proposed an NIDS
for the security of in-vehicular networks using DBN and improved detection accuracy compared to
previous approaches. In [26], we implemented a deep learning based NIDS using NSL-KDD dataset.
We employed self-taught learning [28] that uses sparse autoencoder instead of RBM for feature
reduction and evaluated our model separately on training and test datasets.
In [21], Ma et al.
proposed a system that combines spectral clustering (SC) and sparse autoencoder based deep neural
network (DNN). They used KDD-Cup99, NSL-KDD, and a sensor network dataset to evaluate the
performance of their model.

2.2 DDoS detection in SDN environment

In [12], Braga et al. proposed a light-weight DDoS detection system using self-organized map
(SOM) in SDN. Their implementation uses features extracted from ﬂow-table statistics collected at
a certain interval to make the system light-weight. However, it has limitation in handling traﬃc
that does not have any ﬂow rules installed. In [15], Giotis et al. combined an OpenFlow (OF) and
sFlow for anomaly detection to reduce processing overhead in native OF statistics collection. As
the implementation was based on ﬂow sampling using sFlow, false-positive was quite high in attack
detection. In [20], Lim et al. proposed a DDoS blocking application (DBA) using SDN to eﬃciently
block legitimately looking DDoS attacks. The system works in collaboration with the targeted

2

servers for attack detection. The prototype was demonstrated to detect HTTP ﬂooding attack.
In [24], Mousavi et al. proposed a system to detect DDoS attacks in the controller using entropy
calculation. Their implementation depends on a threshold value for entropy to detect attacks which
they select after performing several experiments. The approach may not be reliable since threshold
value will vary in diﬀerent scenarios. In [35], Wang et al. proposed an entropy based light-weight
DDoS detection system by exporting the ﬂow statistics process to switches. Although the approach
reduces the overhead of ﬂow statistics collection in the controller, it attempts to bring back the
intelligence in network devices.

In contrast to the discussed work, we use SAE based DL model to detect multi-vector DDoS
attacks in SDN. We use a set of large number of features extracted from network packet headers and
then use DL to reduce this set in an unsupervised manner. We apply our model on traﬃc dataset
collected in diﬀerent environments. The proposed system attempts to detect attacks on both the
SDN control plane and the data plane, and is implemented completely on the SDN controller.

3 Background Overview

We discuss SDN and SAE before describing our DDoS detection system.

3.1 Software-Deﬁned Networking (SDN)

As discussed earlier, the SDN architecture decouples the control plane and data plane from net-
work devices, also termed as ‘switches’, and makes them simple packet forwarding elements. The
decoupling of control logic and its uniﬁcation to a centralized controller oﬀers several advantages
compared to the current network architecture that integrates both the planes tightly. Administra-
tors can implement policies from a single point, i.e. controller, and observe their eﬀects on the entire
network that makes management simple, less error-prone, and enhances security. Switches become
generic and vendor-agnostic. Applications that run inside a controller can program these switches
for diﬀerent purposes such as layer 2/3 switch, ﬁrewall, IDS, load balancer using API oﬀered by a
controller to them [19].

Figure 1a shows the SDN architecture with its diﬀerent planes and applications. Switches, end
hosts, and communication between them form the data plane. The controller is either a single server
or a group of logically centralized distributed servers. The controller can run on commodity hardware
and communicates with switches using standard APIs called southbound interfaces. One of the de
facto standards for southbound interfaces is OpenFlow (OF) protocol [22]. The controller servers
communicate with each other using east-westbound interfaces. Network applications communicate
with the controller using northbound interfaces.

The controller and switches exchange various types of messages using OF protocol over either a
TLS/SSL encrypted or open channel. These messages set-up switch connection with the controller,
inquire network status or manage traﬃc ﬂows in the network. Switches have ﬂow tables for ﬂow
rules that contain match-ﬁelds, counters, and actions to handle traﬃc ﬂows in the network. SDN
deﬁnes ﬂow as a group of network packets that have same values for certain packet header ﬁelds.
The controller installs ﬂow rules for traﬃc ﬂows based on the policies dictated by the network
applications.

Flow rule installation takes place in switches either in reactive mode or proactive mode. The
reactive mode works as follows. When a packet enters a switch, it looks up for a ﬂow rule inside
its ﬂow tables that matches with the packet headers.
If a rule exists for the packet, the switch
takes an action that may involve packet forwarding, drop or header modiﬁcation. If a table-miss

3

(a) Diﬀerent planes and network ap-
plications in SDN

(b) Reactive traﬃc ﬂow set-up in SDN [27]

Figure 1: An SDN architecture and basic traﬃc ﬂow in SDN

happens, i.e., there are no ﬂow rules for an incoming ﬂow, the switch sends a packet in message to
the controller that encapsulates packet headers for the incoming ﬂow. The controller extracts packet
headers from the received message and sends a packet out or ﬂow mod message to switches for the
received packet’s ﬂow. The controller installs ﬂow rules inside switches using ﬂow mod messages and
switches perform actions on subsequent packets of the installed ﬂow, without forwarding them to
the controller. Figure 1b demonstrates the reactive mode set-up in SDN. Flow rules may expire after
a certain time to manage the limited memory size of switches. The controller does not install rules
in switches, instead, it instructs them to forward the packet from a single or multiple port(s) using
packet out messages.
In contrast, the controller pre-installs ﬂow rules into switches in proactive
mode.

3.2 Stacked Autoencoder (SAE)

Stacked Autoencoder (SAE) is a DL approach that consists of stacked sparse autoencoders and soft-
max classiﬁer for unsupervised feature learning and classiﬁcation, respectively. We discuss sparse
autoencoder before SAE. A sparse autoencoder is a neural network that consists of three layers in
which the input and output layers contain M nodes, and the hidden layer contains N nodes. The
M nodes at the input represent a record with M features, i.e., X = {x1, x2, ..., xm}. For the training
purpose, the output layer is made an identity function of the input layer, i.e., ˆX = X shown in
Figure 2a. The sparse autoencoder network ﬁnds optimal values of weight matrices, U ∈ (cid:60)N ×M and
(cid:48) ∈ (cid:60)M ×1 while trying to learn an approximation of
U (cid:48) ∈ (cid:60)M ×N , and bias vectors, b1 ∈ (cid:60)N ×1 and b1
the identity function, i.e. ˆX ≈ X using back-propagation algorithm [25]. Many diﬀerent functions
are used for activation of hidden and output nodes, we use Sigmoid function, g(z) = 1
1+e−z , for the

4

Figure 2: A stacked autoencoder based deep learning model

activation of gU,b1 shown in Eqn. 1:

gU,b1(X) = g(U X + b1) =

1
1 + e−(U X+b1)

J =

1
2r

r
(cid:88)

i=1

(cid:107)Xi − ˆXi(cid:107)2 +

λ
2

(cid:88)

+

2 +

b1

(cid:88)

(cid:48)2) + β

b1

n,m
N
(cid:88)

(cid:88)
(

U 2 +

(cid:88)

U (cid:48)2

m,n

KL(ρ(cid:107)ˆρj)

(1)

(2)

n

n

j=1

Eqn. 2 represents the cost function for optimal weight learning in sparse autoencoder.

It is
minimized using back-propagation. The ﬁrst term in the RHS represents an average of sum-of-
square errors for all the input values and their corresponding output values for all r records in the
dataset. The second term is a weight decay term with λ as the decay parameter to avoid over-ﬁtting.
The last term is a sparsity penalty term that puts constraint on the hidden layer to maintain low
average activation values and expressed using Kullback-Leibler (KL) divergence shown in Eqn. 3:
ρ
ˆρj

KL(ρ(cid:107)ˆρj) = ρlog

1 − ρ
1 − ˆρj

+ (1 − ρ)log

(3)

where ρ ∈ {0, 1} is a sparsity constraint parameter and β controls the sparsity penalty term.
The KL(ρ(cid:107)ˆρj) becomes minimum when ρ = ˆρj, where ˆρj is the average activation value of a hidden
unit j over all the training inputs.

Multiple sparse autoencoders are stacked with each other in a way that the outputs of each layer
is fed into the inputs of the next layer to create an SAE. Greedy-wise training is used to obtain
optimal values of the weight matrices and bias vectors for each layer. For illustration, the ﬁrst layer,
(cid:48). The layer g encodes the raw input, X, using U
g, on raw input x is trained to obtain U , U (cid:48), b1, b1
and b1. Then, the encoded values are used as inputs to train the second layer to obtain parameters
(cid:48) shown in Figure 2b. This process goes further until the last hidden layer is trained.
V , V (cid:48), b2, b2
The output of last hidden layer is fed into a classiﬁer. Finally, all layers of SAE are treated as a
single model and ﬁne-tuned to improve the performance of the model shown in Figure 2c.

5

TCP

ICMP
UDP
Src IP
Window Src IP
Src IP
Dst IP
Dst IP
SYN
Dst IP
ICMP Type
Src Port
ACK
Src Port
ICMP Code
Dst Port
URG
Dst Port
Protocol
Protocol
FIN
Protocol
Data Size Data Size
Data Size RST
TTL
TTL

PUSH

TTL

Table 1: Diﬀerent headers extracted from TCP, UDP, and ICMP packets

4

Implementation of DDoS Detection System

In an SDN, attacks can occur either on the data plane or control plane. Attacks on the former are
similar to traditional attacks and aﬀect a few hosts. However, attacks on the latter attempt to bring
down the entire network.
In this second kind of attack, adversaries ﬁngerprint an SDN for ﬂow
installation rules and then send new traﬃc ﬂows, resulting in ﬂow table-misses in the switch [32].
This phenomenon forces the controller to handle every packet and install new ﬂow rules in switches
In our previous work [27], we
that consume system resources on the controller and switches.
empirically evaluated the impact of SDN adversarial attacks on network services. In the current
work, we implement a DDoS detection system as a network application in SDN to handle attacks
for both cases.

Figure 3: A DDoS detection system implemented in SDN

The detection system consists of three modules as shown in Figure 3: i) Traﬃc Collector and Flow
installer (TCFI), ii) Feature Extractor (FE), and iii) Traﬃc Classiﬁer (TC). It should be emphasized
here that to minimize false-positives, our system relies on every packet for ﬂow computation and
attack detection instead of sampling ﬂows using some tools such as sFlow.

6

Algorithm 1: TCFI Module

Data: Incoming network packets at the controller
Result: List of extracted packet headers for TCP, UDP, and ICMP
begin

packets list ←− ∅
f lows list ←− ∅
while T imer for the FE is not triggered do

Receive a packet from switch
Store headers in packets list
if Packet arrives due to ﬂow table miss then

Compute f low for the packet
Compute symmetric ﬂow, symf low, for f low
if symf low ∈ f lows list then

Remove symf low from f lows list
Install ﬂow rule for symf low in switch(es)
Install ﬂow rule for f low in switch(es)

else if f low /∈ f lows list then
Add f low in f lows list
Output the packet to desired port

else

Output the packet to desired port

4.1 Traﬃc Collector and Flow Installer (TCFI)

The TCFI module runs concurrently with the FE and TC modules which are triggered using a timer
function. It examines OF message type for an incoming packet at the controller. A message type
determines the reason for a packet’s arrival which is either due to a ﬂow table-miss or an installed
ﬂow rule that forwards a packet towards the controller and desired physical ports. The TCFI extracts
various header ﬁelds from a packet to identify its ﬂow. A ﬂow in TCP or UDP traﬃc is a group of
packets having same values for protocol type, source and destination IP addresses, and source and
destination port numbers. An ICMP ﬂow has similar header ﬁelds, except for port numbers it has
ICMP message type and code. The TCFI extracts few more header ﬁelds from a packet that help
in features extraction from ﬂows. It stores all of these extracted headers in a list for every packet
coming to the controller. Table 1 shows the headers for TCP, UDP, and ICMP traﬃc that the TCFI
extracts. It performs this task when a packet arrives due to pre-installed ﬂow rules.

However, when a packet arrives due to a ﬂow table-miss, it performs following tasks in addition
to the one mentioned above. It looks up a symmetric ﬂow corresponding to the packet’s ﬂow in
the ﬂow list. Two ﬂows are symmetric for TCP or UDP traﬃc if the source IP address and port
number of one ﬂow are similar to the destination IP and port number of the other, and vice-versa.
For ICMP traﬃc, two ﬂows are symmetric if they are request and response types. If a symmetric
ﬂow exists for an incoming ﬂow, then it installs forwarding rules for both of them in SDN switches
and removes the symmetric one from the list. The rules include an action that forwards packets to
desired physical ports and the controller for the incoming and its symmetric ﬂows. The reason for
installing rules only for symmetric ﬂows is built on the assumption that attackers, in general, spoof
their IP addresses to prevent responses towards them from victims. Therefore, the TCFI installs

7

ﬂow rules for legitimate traﬃc and avoids any ﬂow table saturation attacks in switches. If it does
not ﬁnd any symmetric ﬂow for an incoming packet, it looks up whether a ﬂow already exists in the
list for the same. If a ﬂow exists, it forwards the packet from switches without installing any rules.
Otherwise, it adds the packet’s ﬂow in the list and then forwards it. Algorithm 1 shows various
steps involved in the TCFI. Although the algorithm appears similar to maximum entropy detector
in [23], i) it considers ﬂow in general instead of ﬂags based ii) a packet arrives at the controller
either due to a table-miss or a forwarding rule towards the controller. iii) it stores packet headers
for each packet arrives at the controller.

4.2 Feature Extractor (FE) and Traﬃc Classiﬁer (TC)

The detection system triggers the FE module using a timer function. The FE takes packet headers
from the packets list populated by the TCFI and extracts features from them for a set interval and
resets the packet list to store headers for the next interval. Table 2, 3, and 4 show the list of 68
features that the FE extracts for TCP (34), UDP (20), and ICMP (14) ﬂows, respectively. We
derived this feature set after detailed literature survey and use SAE to reduce it. The FE computes
these features for all hosts in a network which has incoming traﬃc ﬂows for that particular interval.
Although we perform computations on all packets in the network, we extract features by grouping
them in ﬂows. The FE computes median for a number of bytes and packets per ﬂow in feature #
9-12, 43-46, and 63-67. It computes the entropy, H(F ), for feature # 8, 14, 16, 18, 20, 42, 48, 50,
54, 62, and 68 which is deﬁned as follows:

H(F ) = −

fi
j=1 fj
where set F ={f1, f2, ..., fn} denotes the frequency of each distinct value. Once the FE extracts these
features, it invokes the TC module implemented using SAE. It classiﬁes traﬃc in one of the eight
classes which includes one normal and seven types of DDoS attack classes based on TCP, UDP or
ICMP vectors that adversaries launch either separately or in combinations.

fi
j=1 fj

× log2

(cid:80)n

(cid:80)n

(4)

i=1

n
(cid:88)

5 Experimental Set-up, Results, and Discussion

To evaluate our system, we collected network traﬃc from a real network and a private network
testbed. We discuss them along with the performance evaluation results.

5.1 Experimental Set-up

We used a home wireless network (HWN) connected to the Internet for normal traﬃc collection. The
HWN comprised of around 12 network devices including laptops and smartphones. These devices
were not uniformly active for all the time which led to variation in the traﬃc intensity. We saved
HWN traﬃc of 72 hours in a Linux system using tcpdump [7] and port mirroring at a Wi-Fi access
point. The traﬃc of ﬁrst 48 hours were used as normal ﬂows. The traﬃc of last 24 hours was mixed
with the attack data that we collected separately and it was labeled as an attack in the presence
of normal traﬃc. The collected traﬃc comprises data from web browsing, audio/video streaming,
real-time messengers, and online gaming. To collect attack traﬃc, we created a private network in
a segregated laboratory environment using VMWare ESXi host. The private network consists of 10
DDoS attacker and 5 victim hosts. We used hping3 [3] to launch diﬀerent kinds of DDoS attacks
with diﬀerent packet frequencies and sizes. We launched one class of attack at a time so that it can
be labeled easily while extracting features. After traﬃc collection in trace ﬁles, we created an SDN

8

Entropy of window size for incoming TCP ﬂows

Feature Description
#
# of incoming TCP ﬂows
1
Fraction of TCP ﬂows over total incoming ﬂows
2
# of outgoing TCP ﬂows
3
Fraction of TCP ﬂows over total outgoing ﬂows
4
Fraction of symmetric incoming TCP ﬂows
5
Fraction of asymmetric incoming TCP ﬂows
6
# of distinct src IP for incoming TCP ﬂows
7
Entropy of src IP for incoming TCP ﬂows
8
Bytes per incoming TCP ﬂow
9
10
Bytes per outgoing TCP ﬂow
11 # of packets per incoming TCP ﬂow
12 # of packets per outgoing TCP ﬂow
13 # of distinct window size for incoming TCP ﬂows
14
15 # of distinct TTL values for incoming TCP ﬂows
16
Entropy of TTL values for incoming TCP ﬂows
17 # of distinct src ports for incoming TCP ﬂows
18
19 # of distinct dst ports for incoming TCP ﬂows
20
21
22
23
24
25
26
27
28
29
30
31
32
33
34

Entropy of dst ports for incoming TCP ﬂows
Fraction of dst ports ≤ 1024 for incoming TCP ﬂows
Fraction of dst port > 1024 for incoming TCP ﬂows
Fraction of TCP incoming ﬂows with SYN ﬂag set
Fraction of TCP outgoing ﬂows with SYN ﬂag set
Fraction of TCP incoming ﬂows with ACK ﬂag set
Fraction of TCP outgoing ﬂows with ACK ﬂag set
Fraction of TCP incoming ﬂows with URG ﬂag set
Fraction of TCP outgoing ﬂows with URG ﬂag set
Fraction of TCP incoming ﬂows with FIN ﬂag set
Fraction of TCP outgoing ﬂows with FIN ﬂag set
Fraction of TCP incoming ﬂows with RST ﬂag set
Fraction of TCP outgoing ﬂows with RST ﬂag set
Fraction of TCP incoming ﬂows with PUSH ﬂag set
Fraction of TCP outgoing ﬂows with PUSH ﬂag set

Entropy of src port for incoming TCP ﬂows

Table 2: Features extracted for TCP ﬂows

9

Fraction of UDP ﬂows over total incoming ﬂows

Feature Description
#
35 # of incoming UDP ﬂows
36
37 # of outgoing UDP ﬂows
Fraction of UDP ﬂows over total outgoing ﬂows
38
Fraction of symmetric incoming UDP ﬂows
39
Fraction of asymmetric incoming UDP ﬂows
40
41 # of distinct src IP for incoming UDP ﬂows
Entropy of src IP for incoming UDP ﬂows
42
Bytes per incoming UDP ﬂow
43
44
Bytes per outgoing UDP ﬂow
45 # of packets per incoming UDP ﬂow
46 # of packets per outgoing UDP ﬂow
47 # of distinct src ports for incoming UDP ﬂows
48
49 # of distinct dst ports for incoming UDP ﬂows
50
51
52
53 # of distinct TTL values for incoming UDP ﬂows
54

Entropy of dst ports for incoming UDP ﬂows
Fraction of dst port ≤ 1024 for incoming UDP ﬂows
Fraction of dst port > 1024 for incoming UDP ﬂows

Entropy of TTL values for incoming UDP ﬂows

Entropy of src ports for incoming UDP ﬂows

Table 3: Features extracted for UDP ﬂows

Fraction of ICMP ﬂows over total incoming ﬂows

Fraction of ICMP ﬂows over total outgoing ﬂows
Fraction of symmetric incoming ICMP ﬂows

#
Feature Description
55 # of incoming ICMP ﬂows
56
57 # of outgoing ICMP ﬂows
58
59
60 # of asymmetric incoming ICMP ﬂows
61 # of distinct src IP for incoming ICMP ﬂows
62
Entropy of src IP for incoming ICMP ﬂows
63
Bytes per incoming ICMP ﬂow
Bytes per outgoing ICMP ﬂow
64
65 # of packets per incoming ICMP ﬂow
66 # of packets per outgoing ICMP ﬂow
67 # of distinct TTL values for incoming ICMP ﬂows
68

Entropy of TTL values for incoming ICMP ﬂows

Table 4: Features extracted for ICMP ﬂows

10

Traﬃc class

Normal (N)
TCP (T)
UDP (U)
ICMP (I)
TCP & UDP (TU)
TCP & ICMP (TI)
UDP & ICMP (UI)
All (A)

# of records
Training Test
49179
5471
5273
1602
4694
4739
4437
5615

21076
2344
2260
686
2011
2031
1902
2407

Attack

Table 5: Number of records in the training and test datasets for normal and diﬀerent attack traﬃc

testbed on the same ESXi host similar to [16] that consists of an SDN controller, an OF switch, and
a network host using Ubuntu Linux systems. We used POX [6], a Python based controller, with our
DDoS detection application running on it in the controller system and installed OpenvSwitch [5] in
the switch to use it as an OF switch. In the host system, we used tcpreplay [8] to replay traﬃc
traces for normal and attack traﬃc one at a time. We saved features computed by the FE for each
interval in dataset ﬁles for the training of TC module. We set the interval 60s in the timer function
to trigger the FE module for feature extraction. We divided the dataset ﬁles into training and test
datasets. Table 5 shows the distribution of records in the dataset. Traﬃc features in the datasets
are real-valued positive numbers. We normalize them using max − min normalization shown in
Eqn. 5, before passing them to the TC module.

, ∀xi ∈ X

(5)

Xnorm =

xi − xmin
xmax − xmin

xmin = smallest value in X
xmax = largest value in X

5.2 Results

We evaluated the performance of our system on the datasets speciﬁed in Table 5 using parameters
including accuracy, precision, recall, f-measure, ROC. We use confusion matrix to calculate precision,
recall, and f-measure. A confusion matrix, M , is an N × N matrix where N is the number of classes.
It represents the actual and predicted classes in such a way that columns are labeled for actual
classes, and rows are labeled for predicted classes for all records. The diagonal elements of the matrix
represent the true-positive (TP) for each class, sum of the matrix elements along a row excluding the
diagonal element represents the number of false-positive (FP) for a class corresponding to that row,
sum of the matrix elements along a column excluding the diagonal element represents the number
of false-negative (FN) for a class corresponding to that column. Following are deﬁnitions of various
performance parameters:

• Accuracy (A): percentage of accurately classiﬁed records in a dataset

A =

Accurately classif ied records
T otal records

× 100

(6)

• Precision (P): number of accurately predicted records over all predicted records for a particular

11

class. Using the confusion matrix, M , precision for each class, j, can be deﬁned as follows:

Pj =

=

T Pj
T Pj + F Pj

Mj,j
Mj,j + (cid:80)N

i=1
i(cid:54)=j

× 100

× 100

Mj,i

(7)

• Recall (R): number of accurately predicted records over all the records available for a particular
class in the dataset. Using the confusion matrix, M , recall for each class, j, can be deﬁned as
follows:

Rj =

=

T Pj
T Pj + F Nj

Mj,j
Mj,j + (cid:80)N

i=1
i(cid:54)=j

× 100

× 100

Mi,j

(8)

• F-measure (F): It uses precision and recall for the holistic evaluation of a model and is repre-

sented as the harmonic mean of them. For each class, j, it is deﬁned as follows:

Fj =

2 × Pj × Rj
Pj + Rj

× 100

(9)

• Receiver Operating Curve (ROC): It helps in visualizing a classiﬁer’s performance by plotting
the true-positive rate against false-positive rate of the classiﬁer. The area under the ROC
gives an estimate of an average performance of the classiﬁer. Higher the area, greater is the
performance.

Figure 4: Confusion matrix for 8-class classiﬁcation in the SAE model

We used the training dataset to develop the SAE classiﬁcation model for the TC module and test
dataset for performance evaluation. First, we developed the model for 8-class traﬃc classiﬁcation
including normal and seven kinds of DDoS attack that occur in combination with TCP, UDP, and
ICMP based traﬃc. To make a better comparison, we also developed separate attack detection
models with soft-max and neural network (NN) which are building blocks of SAE. As observed from
Table 6, the SAE model achieved better performance compared to the soft-max and neural network
model in terms of accuracy. We computed precision, recall, f-measure for each traﬃc class. Figure 5
shows their values which are derived from the confusion matrix shown in Figure 4. As seen from

12

Figure 5: Precision, recall, and f-measure for 8-class

Figure 6: ROC curve for 8-class classiﬁcation

13

Method
Soft-max
Neural Network
SAE

Accuracy (in %)
94.30
95.23
95.65

Table 6: Classiﬁcation accuracy comparison among soft-max, neural network, and SAE based models

the ﬁgure, the model has f-measure value above 90% for normal, TCP, UDP, and UDP with ICMP
attacks traﬃc. It has comparatively low values of f-measure for TCP with ICMP and TCP with
UDP attacks due to their classiﬁcation of other kinds of attacks as observed from the Figure 4.
However, it is observed from the same ﬁgure that the fraction of their classiﬁcation as normal traﬃc
is less than 0.2. Figure 6 shows the ROC curve for 8 diﬀerent classes. From the ﬁgure, we observe
that the true positive rate is above 90% with a false-positive rate of below 5% for all kinds of traﬃc
that results in the area under the ROC curve close to unity.

Figure 7: Confusion matrix for 2-class classiﬁcation

We evaluated our model for 2-class classiﬁcation by considering all kinds of DDoS attacks as a
single attack class to make a comparison with other works. Due to the unique nature of this work
involving deep learning based attack detection in an SDN, and unavailability of existing literature
in this speciﬁc domain, it was diﬃcult to compare our work with other works. Figure 8 shows
the performance for 2-class classiﬁcation. The model achieved detection accuracy of 99.82% with
f-measure values as 99.85% and 99.75% for normal and attack classes, respectively, derived from the
confusion matrix shown in Figure 7. On the contrary, the two closely related works [12] and [24],
achieved a detection accuracy of 99.11% in data plane and 96% in control plane, respectively. It
should be noted that both of these works did not address attack detection in the other plane.

We also measured the computational time for training and classiﬁcation in our model using a
machine with Intel (R) Core i7 CPU @ 3.40 GHz processor and 16 GB RAM running Matlab 2016a
on Windows 7. Table 7 shows computational time for the training of 81,010 records and classiﬁcation
of 34,717 records speciﬁed in Table 5.

14

Figure 8: Accuracy, precision, recall, and f-measure for 2-class classiﬁcation

Training time Classiﬁcation time

524s

.0835s

Table 7: Average computational time for the training and classiﬁcation in the SAE model

5.3 Discussion

With our DDoS detection system, we identify individual DDoS attack class and also determine
whether an incoming traﬃc is normal or attack. A clear advantage in identifying each attack traﬃc
type separately is enabling the mitigation technique to block only a speciﬁc type of traﬃc causing
the attack, instead of all kinds of traﬃc coming towards the victim(s). Although we implemented
a detection system, we separately extracted features for each host which has incoming traﬃc for an
interval. Therefore, we can identify the hosts with normal traﬃc and the ones with attack traﬃc.
Accordingly, the controller can install ﬂow rules inside the switches to block the traﬃc for a particular
host if it undergoes an attack.

Our proposed system has a few limitations in terms of processing capabilities. The TCFI and
FE modules collect every packet to extract features and are implemented on the controller for low
false-positive in detection. However, this approach may limit the controller’s performance in large
networks. We can overcome it by adopting a hybrid approach that can either use ﬂow sampling or
individual packet capturing based on the observed traﬃc in the organizational network. Another
approach that could be employed to handle DDoS attacks in the data plane is to deploy the TCFI and
FE modules in another host, send all packets to it instead of the controller for features processing,
and then periodically notify the controller with extracted features for the TC module. To reduce the
time in feature extraction, we can also apply distributed processing similar to our another previous
work [18].

6 Conclusion

In this work, we implemented a deep learning based DDoS detection system for multi-vector attack
detection in an SDN environment. The proposed system identiﬁes individual DDoS attack class with
an accuracy of 95.65%. It classiﬁes the traﬃc in normal and attack classes with an accuracy of 99.82%
with very low false-positive compared to other works. In the future, we aim to reduce the controller’s
bottleneck and implement an NIDS that can detect diﬀerent kinds of network attacks in addition

15

to DDoS attack. We also plan to use deep learning for feature extraction from raw bytes of packet
headers instead of feature reduction from the derived features in future NIDS implementation.

References

[1] Cisco Visual Networking Index Predicts Near-Tripling of IP Traﬃc by 2020.

https://

newsroom.cisco.com/press-release-content?articleId=1771211 Accessed 14 Nov 2016

[2] DDoS Attack on BBC May Have Been Biggest in History.

http://www.csoonline.com/

article/3020292/cyber-attacks-espionage/ddos-attack-on-bbc-may-have-been-
biggest-in-history.html Accessed 14 Nov. 2016

[3] Hping3. http://wiki.hping.org Accessed 14 Nov. 2016

[4] KDD Cup 99. http://kdd.ics.uci.edu/databases/kddcup99/kddcup99.html Accessed 14

Nov. 2016

[5] Open vSwitch. http://www.openvswitch.org Accessed 14 Nov. 2016

[6] POX Wiki: Open Networking Lab. https://openflow.stanford.edu/display/ONL/POX+Wiki

Accessed 14 Nov. 2016

[7] Tcpdump. http://www.tcpdump.org Accessed 14 Nov. 2016

[8] Tcpreplay. http://tcpreplay.synfin.net Accessed 14 Nov. 2016

[9] The Recent DDoS Attacks on Banks: 7 Key Lessons. https://www.neustar.biz/resources/

whitepapers/recent-ddos-attacks-on-banks Accessed 14 Nov. 2016

[10] Verisign Q2 2016 DDoS Trends: Layer 7 DDoS Attacks a Grwoing Trend.
//blog.verisign.com/security/verisign-q2-2016-ddos-trends-layer-7-ddos-
attacks-a-growing-trend/ Accessed 14 Nov. 2016

https:

[11] ‘World Of Warcraft:

Legion’ Goes Down As Blizzard Servers Hit With DDoS.

http://www.forbes.com/sites/erikkain/2016/09/01/world-of-warcraft-legion-goes-
down-as-blizzard-servers-hit-with-ddos/ Accessed 14 Nov. 2016

[12] Braga, R., Mota, E., Passito, A.: Lightweight DDoS Flooding Attack Detection Using
In: Proceedings of the 2010 IEEE 35th Conference on Local Computer

NOX/OpenFlow.
Networks, LCN ’10, pp. 408–415. IEEE Computer Society, Washington, DC, USA (2010)

[13] Fiore, U., Palmieri, F., Castiglione, A., De Santis, A.: Network Anomaly Detection with the

Restricted Boltzmann Machine. Neurocomputing 122, 13 – 23 (2013)

[14] Gao, N., Gao, L., Gao, Q., Wang, H.: An Intrusion Detection Model Based on Deep Belief
Networks. In: Advanced Cloud and Big Data (CBD), 2014 Second International Conference
on, pp. 247–252 (2014)

[15] Giotis, K., Argyropoulos, C., Androulidakis, G., Kalogeras, D., Maglaris, V.: Combining Open-
Flow and sFlow for an Eﬀective and Scalable Anomaly Detection and Mitigation Mechanism
on SDN Environments. Computer Networks 62, 122 – 136 (2014)

[16] Hartpence, B.: The RIT SDN Testbed and GENI (2015)

16

[17] Kang, M.J., Kang, J.W.: Intrusion Detection System Using Deep Neural Network for In-Vehicle

Network Security. PLoS ONE 11(6), 1–17 (2016)

[18] Karimi, A.M., Niyaz, Q., Sun, W., Javaid, A.Y., Devabhaktuni, V.K.: Distributed Network
Traﬃc Feature Extraction for a Real-time IDS. In: 2016 IEEE International Conference on
Electro Information Technology (EIT) (2016)

[19] Kreutz, D., Ramos, F.M.V., Verssimo, P.E., Rothenberg, C.E., Azodolmolky, S., Uhlig, S.:
Software-Deﬁned Networking: A Comprehensive Survey. Proceedings of the IEEE 103(1),
14–76 (2015)

[20] Lim, S., Ha, J., Kim, H., Kim, Y., Yang, S.: A SDN-oriented DDoS Blocking Scheme for Botnet-
based Attacks. In: 2014 Sixth International Conference on Ubiquitous and Future Networks
(ICUFN), pp. 63–68 (2014)

[21] Ma, T., Wang, F., Cheng, J., Yu, Y., Chen, X.: A Hybrid Spectral Clustering and Deep Neural
Network Ensemble Algorithm for Intrusion Detection in Sensor Networks. Sensors 16(10), 1701
(2016). URL http://www.mdpi.com/1424-8220/16/10/1701

[22] McKeown, N., Anderson, T., Balakrishnan, H., Parulkar, G., Peterson, L., Rexford, J., Shenker,
S., Turner, J.: OpenFlow: Enabling Innovation in Campus Networks. SIGCOMM Comput.
Commun. Rev. 38(2), 69–74 (2008)

[23] Mehdi, S.A., Khalid, J., Khayam, S.A.: Revisiting Traﬃc Anomaly Detection Using Software

Deﬁned Networking, pp. 161–180 (2011)

[24] Mousavi, S.M., St-Hilaire, M.: Early detection of DDoS attacks against SDN controllers. In:
Computing, Networking and Communications (ICNC), 2015 International Conference on, pp.
77–81 (2015)

[25] Ng, A.: Sparse Autoencoder (2011)

[26] Niyaz, Q., Javaid, A., Sun, W., Alam, M.: A Deep Learning Approach for Network Intrusion
Detection System. In: Proceedings of the 9th EAI International Conference on Bio-inspired
Information and Communications Technologies (Formerly BIONETICS), BICT’15, pp. 21–26
(2016)

[27] Niyaz, Q., Sun, W., Alam, M.: Impact on SDN Powered Network Services Under Adversarial

Attacks. Procedia Computer Science 62, 228 – 235 (2015)

[28] Raina, R., Battle, A., Lee, H., Packer, B., Ng, A.Y.: Self-taught Learning: Transfer Learn-
ing from Unlabeled Data. In: Proceedings of the 24th International Conference on Machine
Learning, ICML ’07, pp. 759–766. ACM, New York, NY, USA (2007)

[29] Salama, M.A., Eid, H.F., Ramadan, R.A., Darwish, A., Hassanien, A.E.: Hybrid Intelligent In-
trusion Detection Scheme. In: Soft Computing in Industrial Applications, pp. 293–303. Springer
Berlin Heidelberg, Berlin, Heidelberg (2011)

[30] Schehlmann, L., Abt, S., Baier, H.: Blessing or Curse? Revisiting Security Aspects of Software-
Deﬁned Networking. In: 10th International Conference on Network and Service Management
(CNSM) and Workshop, pp. 382–387 (2014). DOI 10.1109/CNSM.2014.7014199

17

[31] Schmidhuber, J.: Deep Learning in Neural Networks: An Overview. Neural Networks 61, 85 –

117 (2015)

[32] Shin, S., Gu, G.: Attacking Software-deﬁned Networks: A First Feasibility Study. In: Proceed-
ings of the Second ACM SIGCOMM Workshop on Hot Topics in Software Deﬁned Networking,
HotSDN ’13, pp. 165–166. ACM, New York, NY, USA (2013)

[33] Shin, S., Xu, L., Hong, S., Gu, G.: Enhancing Network Security through Software Deﬁned
Networking (SDN). In: 2016 25th International Conference on Computer Communication and
Networks (ICCCN), pp. 1–9 (2016)

[34] Tavallaee, M., Bagheri, E., Lu, W., Ghorbani, A.: A Detailed Analysis of the KDD CUP 99
Data Set. In: Computational Intelligence for Security and Defense Applications, 2009. CISDA
2009. IEEE Symposium on, pp. 1–6 (2009). DOI 10.1109/CISDA.2009.5356528

[35] Wang, R., Jia, Z., Ju, L.: An Entropy-Based Distributed DDoS Detection Mechanism in
Software-Deﬁned Networking. In: Proceedings of the 2015 IEEE Trustcom/BigDataSE/ISPA
- Volume 01, TRUSTCOM ’15, pp. 310–317. IEEE Computer Society, Washington, DC, USA
(2015)

18

