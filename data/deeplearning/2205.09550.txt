Data Valuation for Ofﬂine Reinforcement Learning

Amir Abolfazli, Gregory Palmer and Daniel Kudenko

2
2
0
2

y
a
M
9
1

]

G
L
.
s
c
[

1
v
0
5
5
9
0
.
5
0
2
2
:
v
i
X
r
a

Abstract— The success of deep reinforcement learning (DRL)
hinges on the availability of training data, which is typically
obtained via a large number of environment interactions. In
many real-world scenarios, costs and risks are associated with
gathering these data. The ﬁeld of ofﬂine reinforcement learning
addresses these issues through outsourcing the collection of
data to a domain expert or a carefully monitored program
and subsequently searching for a batch-constrained optimal
policy. With the emergence of data markets, an alternative to
constructing a dataset in-house is to purchase external data.
However, while state-of-the-art ofﬂine reinforcement learning
approaches have shown a lot of promise, they currently rely
on carefully constructed datasets that are well aligned with
the intended target domains. This raises questions regarding
the transferability and robustness of an ofﬂine reinforcement
learning agent trained on externally acquired data. In this
paper, we empirically evaluate the ability of
the current
state-of-the-art ofﬂine reinforcement learning approaches to
coping with the source-target domain mismatch within two
MuJoCo environments, ﬁnding that current state-of-the-art
ofﬂine reinforcement learning algorithms underperform in the
target domain. To address this, we propose data valuation for
ofﬂine reinforcement learning (DVORL), which allows us to
identify relevant and high-quality transitions, improving the
performance and transferability of policies learned by ofﬂine
reinforcement learning algorithms. The results show that our
method outperforms ofﬂine reinforcement learning baselines on
two MuJoCo environments.

I. INTRODUCTION

Ofﬂine Reinforcement Learning (RL) – also known as
batch-constrained RL – is a class of RL methods that requires
to learn from a static dataset of pre-collected
the agent
experiences without further environment interaction [1]. This
learning paradigm disentangles exploration from exploita-
tion, lending itself to the tasks in which exploration for
collecting data is costly, time-consuming, or risky [2], [3].
By taking advantage of pre-collected datasets, ofﬂine RL can
mitigate the technical concerns associated with online data
collection, and has potential beneﬁts for a number of real
environments, such as human-robot collaboration [4].

Ofﬂine reinforcement learning outsources the collection
of data to a domain expert and subsequently searches for a
batch-constrained optimal policy. However, this task is chal-
lenging, as ofﬂine RL methods suffer from the extrapolation
error [3], [5]. This pathology occurs when ofﬂine deep RL
methods are trained under one distribution but evaluated on a
different one. More speciﬁcally, value functions implemented
by a function approximator have a tendency to predict
unrealistic values for unseen state-action pairs for standard
off-policy deep RL algorithms (e. g., DQN and DDPG). This

L3S Research Center, Leibniz University Hannover, Germany

{abolfazli, gpalmer, kudenko}@l3s.de

raises the need for approaches that restrict the action space,
forcing the agent to learn a behavior that is close to on-policy
with respect to a subset of the given source data [3].

For ofﬂine RL methods that are designed to mitigate the
extrapolation error [3], [5], [6], there remains the challenge
that external data (e. g., purchased from data markets) may
not be well aligned with the intended target domain. There-
fore, the learned policy induces a different visited state-action
distribution that results in a degradation in the performance
of the ofﬂine RL agent.

In recent years there have been a number of efforts within
the paradigm of supervised learning for overcoming the
source-target domain mismatch problem via valuating data,
including data Shapley [7] and data valuation using rein-
forcement learning (DVRL) [8]. Such methods have shown
promising results on several application scenarios such as
data-value quantiﬁcation, corrupted sample discovery, robust
learning with noisy labels, and domain adaptation [8]. This
raises the question: data valuation improve the transferability
and robustness of an ofﬂine RL agent trained on externally
acquired data?

To investigate the above question, we propose a data
valuation approach that selects a subset of samples in the
source dataset that are relevant to the target task. Our main
contributions can be summarized as follows:

• Inspired by DVRL [8], we propose Data Valuation for
Ofﬂine Reinforcement Learning (DVORL) that for a
given ofﬂine RL method, a ﬁxed source dataset, and
a small target dataset, identiﬁes those samples of the
source buffer that are relevant to the target task.

• We contribute a benchmark on two Gym MuJoCo
domains (Hopper and Walker2d) for which parameteri-
zations (friction and mass of torso) for the target domain
are different from those of the source domain.

• We show that the state-of-the-art ofﬂine RL methods fail
to generalize to different target domain conﬁgurations.
• We show how our data valuation approach can improve
the generalizability of the RL agent to the target domain
by outperforming the existing state-of-the-art methods
on all considered target domain conﬁgurations.

The rest of this paper is organized as follows. Section II
gives the background on (ofﬂine) RL. In Section III, we for-
mally deﬁne the source-target domain mismatch problem for
ofﬂine RL, and provide a motivating example in Section IV.
Section V gives an overview of related work. In Section VI,
we introduce our DVORL framework. Section VII describes
our experiment setup. We discuss our results in Section VIII,
and in Section IX, we discuss our main ﬁndings. Finally in
Section X we conclude with suggestions for future work.

 
 
 
 
 
 
II. BACKGROUND

A. Reinforcement Learning

The RL problem is typically modeled by a Markov deci-
sion process (MDP), formulated as a tuple (X , U, p, r, γ),
with a state space X , an action space U, and transition
dynamics p (x(cid:48) | x, u). At each discrete time step the agent
performs an action u ∈ U in a state x ∈ X , and transitions
to a new state x(cid:48) ∈ X based on the transition dynamics
p (x(cid:48) | x, u), and receives a reward r (x, u, x(cid:48)). The goal
of the agent is to maximize the expectation of the sum
of discounted rewards, also known as the return Rt =
(cid:80)∞
i=t+1 γir (xi, ui, xi+1), which weighs future rewards with
respect to the discount factor γ ∈ [0, 1), determining the
effective horizon. The agent makes decisions via a policy
π : X → P(U), which maps a given state x to a probability
distribution over the action space U. For a given policy π, the
value function is deﬁned as the expected return of an agent,
starting from state x, performing action u, and following
the policy Qπ(s, a) = Eπ [Rt | x, u]. The state-action value
function can be computed through the Bellman equation of
the Q function:

Qπ(x, u) = Es(cid:48)∼p [r (x, u, x(cid:48)) + γEu(cid:48)∼πQπ(x(cid:48), u(cid:48))] .

(1)

Given Qπ, the optimal policy π∗ = maxu Q∗(x, u), can be
obtained by greedy selection over the optimal value function
Q∗(x, u) = maxπ Qπ(x, u). For environments confronting
agents with the curse of dimensionality the value can be es-
timated with a differentiable function approximator Qθ(x, u),
with parameters θ.

In this work, we focus on continuous control problems,
where, given a parameterized policy πϑ our objective is to
ﬁnd an optimal policy π∗
ϑ, with respect to the parameters ϑ,
which maximizes the expected return from the start distri-
bution J(ϑ) = Exi∼pπ,ui∼π[R0] [9]. The policy parameters
ϑ can be updated by taking the gradient of the expected
return ∇ϑJ(ϑ). A popular approach to optimizing the policy
is to use actor-critic methods, where the actor (policy) can
be updated through the deterministic policy gradient algo-
rithm [10]: ∇ϑJ(ϑ) = Ex∼pπ [∇uQπ
θ (x, u)|u,π(x)∇ϑπϑ(x)],
in which the value function Qπ

θ (x, u) is the critic.

B. Ofﬂine Reinforcement Learning

Standard off-policy deep RL algorithms such as deep Q-
learning (DQN) [11] and deep deterministic policy gradient
(DDPG) [9] are applicable in batch RL as they are based
on more fundamental batch RL algorithms [12]. However,
they suffer from a phenomenon, known as extrapolation
error, which occurs when there is a mismatch between the
given ﬁxed batch of data and true state-action visitation
of the current policy [3]. This is problematic as incorrect
values of state-action pairs, which are not contained in the
batch of data, are propagated through temporal difference
updates of most off-policy algorithms [13], resulting in poor
performance of the model [14]. Below we give an overview
of approaches designed to address the extrapolation error,
which will serve as our baselines.

BCQ. Batch-Constrained deep Q-learning [3] is an ofﬂine
RL method for continuous control that restricts the action
space, thereby eliminating actions that are unlikely to be
selected by the behavioral policy πb and therefore rarely
observed in the batch. Given a dataset of N transitions
D = {xt, ut, rt+1, xt+1}N
t=0, collected by a behavior pol-
icy πb, BCQ consists of four parameterized networks: a
generative model Gω : X → U, parameterized with ω, a
perturbation model ξφ(x, u, Φ), parameterized with φ, and
two Q-networks Qϑ1(x, u), Qϑ2(x, u), parameterized with
ϑ1 and ϑ2, respectively. The generative model Gω selects
the most likely action given the state with respect to the
data in the batch. Since modeling the distribution of data
in the high dimensional continuous control environments
is not straightforward, a variational autoencoder (VAE) is
used to approximate it. The policy is deﬁned by sampling n
actions from Gω(x) and selecting the highest valued action
according to a Q-network as it is easier to sample from
πb(u | x) than modeling πb(u | x) in a continuous action
space. The perturbation model ξφ(x, u, Φ), parameterized
with φ, models the distribution of data in the batch and is a
residual added to the sampled actions in the range [−Φ, Φ].
This model is trained with the DDPG [10] and can be thought
of as a behavioral cloning model. Since the perturbation
model together with the sampling can be considered as a
hierarchical policy, BCQ can also be considered an actor-
critic method.

CQL. To prevent the training policy from overestimating
the Q-values, Conservative Q-Learning (CQL) [6] utilizes a
penalized empircal RL objective. More precisely, CQL opti-
mizes the value function not only to minimize the temporal
difference error based on the interactions seen on the dataset
but also minimizes the value of actions that the currently
trained policy takes, while at the same time maximizing the
value of actions taken by the behavioral policy during data
generation. This results in a conservative Q-function.

TD3+BC. Twin Delayed Deep Deterministic (TD3) policy
gradient with Behavior Cloning (BC) is a model-free algo-
rithm that does not explicitly learn a model of the behavioral
policy, while trains a policy to mimic the behavior policy
from the data [15]. It directly penalizes Euclidean distance
to the actions that were recorded in the dataset.

C. Data Valuation

The training samples that machine learning (ML) mod-
els are trained with are not all equally valuable [16]. A
sample can be considered low-quality due to noisy input
values, a noisy label, or the source-target domain mismatch
problem. Removing low-quality samples has been shown
to increase the performance of ML models [7], [8]. The
task of quantifying the quality of individual datum to the
overall performance is referred to as data valuation. In
supervised learning, data valuation is formally deﬁned as
follows. Given a source (training) dataset Ds = {(xi, yi)}N
i=1
and a target (test) dataset DT = (cid:8)(cid:0)xT
where
x ∈ X is a d-dimensional feature vector, and y ∈ Y is
a corresponding label, the goal is to ﬁnd a subset D∗ =

j , yT
j

(cid:1)(cid:9)M

j=1

{(xk, yk) | (xk, yk) ∈ DS }K
k=1 of the source dataset DS that
maximizes the performance of the trained model on the target
dataset DT [7], [17].

III. PROBLEM DESCRIPTION
In this section, we formally deﬁne the data valuation

a

We

i , x(cid:48)
i
i , x(cid:48)
i

the
S , rS
T , rT

problem for ofﬂine RL.
assume
i , uS
i , uT

availability of
i , eS
i , eT

source dataset
i )}N
DS = {(xS
i=1 ∼ PS and a target dataset
i=1 ∼ PT , where x ∈ Rm is
DT = {(xT
i )}M
a state; u ∈ Rn is the action that the agent performs at
the state x; r ∈ R is the reward that the agent gets by
performing the action u in the state x; x(cid:48) ∈ Rm is the state
that the agent transitions to (i.e. next state); and e ∈ {0, 1}
indicates whether the x(cid:48) is a terminal state. We assume that
the target dataset DT is much smaller than the the source
dataset DS , therefore N (cid:29) M . Furthermore, the source
distribution PS can be different from the target distribution
PT , confronting our learner with the source-target domain
mismatch problem. As in supervised learning, our goal is
to ﬁnd a (sub)set D∗ ⊆ DS , and seek a batch-constrained
policy π, that when trained on D∗ can generalize to the target
domain used to construct DT . Therefore, we have a transfer
learning problem.

To formally deﬁne transfer learning for ofﬂine RL, we
draw on the formulation from Zhu et al. [18]. Let ΨS =
{ΨS | ΨS ∈ ΨS } be a set of source domains and ΨT be
a target domain, where each domain corresponds to an
MDP. Therefore, the MDPs in the source domain ΨS and
target domain ΨT are deﬁned as (XS , US , pS , rS , γS ) and
(XT , UT , pT , rT , γT ), respectively. We assume prior knowl-
edge DS provided by the set of source domains ΨS and
accessible to the target domain ΨT .

By leveraging the prior information DS from the source
domain ΨS as well as information DT provided by ΨT ,
transfer learning aims to learn an optimal policy π∗ for the
target domain ΨT , such that:

π∗ = arg max

π

Ex∼XT ,u∼π

(cid:2)Qπ
ΨT

(x, u)(cid:3) ,

(2)

where π = φ (DS ∼ ΨS , DT ∼ ΨT ) : XT → UT is a
function mapping the states to actions for the target domain
ΨT , learned based on information from both DT and DS .
The source and target domains can have distinct state
spaces, but their action spaces have to be the same and their
transition function and reward function have to be similar as
they share internal dynamics. We focus on policy transfers
where XS = XT , US = UT , rS = rT , but pS (cid:54)= pT .

The source and target domains can have distinct transition
functions because a change in environment parameters (e. g.,
mass of torso and friction) results in a different probability
function p, which itself is conditioned on (x, u, x(cid:48)), where
x, u, and x(cid:48) denote state, action, and next state, respectively.
However, since the target dataset is very small, compared
to the source dataset (N (cid:29) M ), state-action transition
(x, u, x(cid:48)) is too restrictive. Thus, we also consider the case
(x) in which only the change in the distribution of the state
space is taken into account.

IV. MOTIVATING EXAMPLE

For our motivating example, we consider two scenarios
involving a cobot, depicted in Figure 1. In the source domain
the cobot is performing pick-and-place task while the target
domain confronts the cobot with a sorting task. Clearly,
a learning agent trained on the source task will perform
poorly on the target task. However, our hypothesis is that
data valuation can help us identify samples that are relevant
for both tasks. For instance, both tasks have pick-up and
place actions in common. Therefore, the goal is to ﬁnd the
relevant subset of the source dataset that allows the agent to
learn a policy that generalizes to the target domain.

V. RELATED WORK

The RL literature contains numerous techniques for deal-
ing with the source-target domain mismatch problem. Note-
worthy contributions here include: EPOPT [19], which is
a combination of policy transfer through source domain
ensemble and learning from limited demonstrations for the
fast adaptation to the target domain; UP-OSI [20] trains
robust agent policies using a large number of synthetic
demonstrations from a simulator to deal with environments
with unknown dynamics; CAD2RL [21] learns latent repre-
sentations from observations in the source domain that are
generally applicable to the target domain; DARLA [22], a
zero-shot transfer learning method that learns disentangled
representations which are robust against domain shifts; and
SAFER [23], which accelerates policy learning on complex
control tasks by considering safety constraints.

Meanwhile, the literature on off-policy RL includes prin-
cipled experience replay memory sampling techniques. Pri-
oritized Experience Replay (PER) [24] (e.g., [25], [26],
[27]) attempts to sample transitions that contribute the most
toward learning. prioritized replay with weighted importance
sampling can improve BCQ. However, the majority of the
work to date on ofﬂine RL is focused on preventing the
training policy from being too disjoint with the behavioral
policy [3], [6], [28]. To increase the generalization capacity
of ofﬂine RL methods, Kostrikov et al. [29] propose in-
sample Q-learning (IQL), which approximates the policy
improvement step by considering the state value function as
a random variable with some randomness determined by the
action, and then taking a state-conditional expectile of this
random variable to estimate the value of the best actions in
that state.

In contrast to the ofﬂine RL methods listed above, our
work focuses on valuating the suitability of state transition
tuples for a given target domain. Here we draw inspiration
from the literature on data valuation for supervised learning.
Ghorbani et al. in [30] propose the distributional Shapley,
which is a framework in which the value of a point is
deﬁned in the context of underlying data distribution. The
reformulation of the data Shapley value as a distributional
quantity reduces the dependence on the speciﬁc draw of data
as the valuation function does not depend on a ﬁxed data
set. Ghorbani and Zou propose the Neuron Shapley frame-
work [31] to quantify how individual neurons contribute to

the prediction and performance of a deep neural network
(DNN). However, the limitation of Shapely based methods
is that it’s computationally expensive or even impossible to
quantify the contribution of each individual sample to the
overall performance of the model, in particular, for complex
models such as DNNs [8].

Wang et al. [32] propose a minimax game-based transfer
learning technique for selective transfer learning that con-
sists of a selector, a discriminator, and a transfer learning
module, playing a minimax game to ﬁnd useful source data.
Yoon et al. [8] introduce a framework for data valuation in
supervised learning tasks, making use of RL to determine
how likely each training sample is used in the training of
the predictor model. This method integrates data valuation
into the training procedure of the predictor model, making
the predictor and data value estimator able to improve each
other’s performance.

Despite the success of the existing works on data valua-
tion, they are only applicable to the supervised learning tasks
in which the availability of labels is assumed. Therefore, they
are not directly applicable to the RL setting where there is no
ground-truth for the transitions. This emphasizes a need for
a data valuation method that is applicable to the RL setting.

VI. PROPOSED METHOD

DVORL consists of two learnable functions: (1) a data
value estimator (DVE) model vφ and (2) an ofﬂine reinforce-
ment learning model. Inspired by DVRL [8], we adapt the
REINFORCE algorithm [33] and use it as the DVE. We use
a DNN for the DVE. The goal is to ﬁnd the parameters φ∗ of
the DNN so that the network returns the optimal probability
distribution over the sample space.

The DVE model vφ : X × U × X (cid:48) × R × E → [0, 1]
is optimized to output data values corresponding to the
relevance of training samples to the target task. We formulate
the corresponding optimization problem as:

(cid:90)

=

(cid:104)

the sample i of the source buffer is used for training the
ofﬂine reinforcement learning model.

Our adapted version of REINFORCE algorithm, has the

following object function for the policy πφ:

(cid:104)
rφ((xS , uS , x

(cid:48)S ), (xT , uT , x

(cid:105)
(cid:48)T ))

J (πφ) = E

(xS ,uS ,x
(xT ,uT ,x

(cid:48) S )∼P S
(cid:48) T )∼P T
(cid:48),·)

w∼πφ(DS

P T (cid:16)

(xS , uS , x

(cid:48)S )

(cid:17) (cid:88)

πφ(DS

(cid:48), w)

w∈[0,1]N
(cid:48)S ), (xT , uT , x

(cid:48)T ))

(cid:105)

(cid:16)

(cid:17)

(cid:48)S )

·

d

(xS , uS , x

rφ((xS , uS , x

In the above equation, πφ(DS

.
(cid:48), w) is the probability that
the selection probability vector w occurs. In this way, the
policy directly uses the values output by the DVE. This is
different from the DVRL [8], which uses a binary selection
vector s = (s1, . . . , sBs) where sBs denotes the batch size,
si ∈ {0, 1}, and P (si = 1) = wi. Thus, in our training,
the DVE has no control over exploration and just provides
weightings for the given samples and is tuned accordingly.
It should be noted that we use the whole input information
of the source buffer (i.e., (x, u, x(cid:48), r, e)) for calculating
(cid:48), w)); however,
the data values (i.e., DS
we only use the information of the state-action transition
(x, u, x(cid:48)) for calculating the reward signal, used for updating
the DVE, that is consistent with our formulation of transfer
learning where pS (cid:54)= pT as the transition probability function
is conditioned on the state-action transition (x, u, x(cid:48)).

(cid:48) in policy πφ(DS

We calculate the gradient of the above objective function

in the following.
P T (cid:16)

∇φJ (πφ) =

(cid:90)

(xS , uS , x

(cid:48)S )

(cid:17) (cid:88)

∇φπφ(DS

(cid:48), w)

rφ((xS , uS , x

w∈[0,1]N
(cid:48)S ), (xT , uT , x


(cid:104)

·

(cid:90)

=

P T (cid:16)

(xS , uS , x

(cid:17)
(cid:48)S )

(cid:88)



x∈[0,1]N

(cid:48), w)

∇φπφ(DS
πφ(DS

(cid:48), w)

(cid:105)
(cid:48)T )))

(cid:16)

d

(xS , uS , x

(cid:48)S )

(cid:17)

· πφ(DS

(cid:48), w)

(cid:104)

rφ((xS , uS , x

(cid:48)S ), (xT , uT , x

(cid:48)T ))

(cid:105)



 d

(cid:16)

(xS , uS , x

(cid:48)S )

(cid:17)

maxφ J (πφ) = E

(xS ,uS ,x
(xT ,uT ,x

(cid:104)

(cid:48)S )∼P S
(cid:48)T )∼P T

where

rφ((xS , uS , x

(cid:48)S ), (xT , uT , x

(cid:48)T ))

(cid:105)
,

(3)

(cid:90)

=

P T (cid:16)

(xS , uS , x

(cid:17)
(cid:48)S )





(cid:88)

∇φ log (cid:0)πφ(DS

(cid:48), w)(cid:1)

w∈[0,1]N

rφ =

1
DKL(P(xS ,uS ,x(cid:48) S ) (cid:107) P(xT ,uT ,x(cid:48) T ))

(4)

corresponds to the reciprocal of the KL divergence be-
tween the batch of source dataset and target dataset.
Therefore,
to assign
high probabilities to samples whose reward function value
rφ((xS , uS , x

(cid:48)S ), (xT , uT , x

the objective of

the network is

(cid:48)T )) is high.

Training. As shown in Algorithm 1 (lines 4 to 10), all the
samples of source buffer DS are divided into batches and
(cid:48) is given as input to the DVE (with shared
each batch DS
parameters across the batch). The KL divergence between
the distribution of the state-action transition (x, u, x
) of the
given batch and that of the small target buffer DT is calcu-
lated and used as the reward signal rsig for training the DVE.
i , eS
Let w = vφ(xS
i ) denote the probability that

i , uS

S , rS

i , x(cid:48)
i

(cid:48)

· πφ(DS

(cid:48), w)

(cid:104)

rφ((xS , uS , x

(cid:48)S ), (xT , uT , x

(cid:48)T ))

(cid:105)



 d

(cid:16)

(xS , uS , x

(cid:48)S )

(cid:17)

(cid:104)

=

E
(xS ,uS ,x
(xT ,uT ,x

(cid:48) S )∼P S
(cid:48) T )∼P T

rφ((xS , uS , x

(cid:48)S ), (xT , uT , x

(cid:48)T ))

(cid:105)

w∼πφ(DS ,·)
· ∇φ log (cid:0)πφ(DS

(cid:48), w)(cid:1) .

To enhance the stability of the DVE, we use the moving
average rrolling of the previous signal rewards with the
window size ω as the baseline. The baseline reduces the
variance of the gradient estimates [34].

Inference. As shown in Algorithm 1 (lines 11 to 18), after
all the samples of the source buffer are used for training
the DVE, the fully-trained DVE is used for outputting the
data values of the original source buffer. The samples whose
corresponding data values are lower than the selection thresh-
old (cid:15) are ﬁltered out and the remaining subset of samples
form the new source buffer D∗
S that is relevant to the target

domain:

(cid:110)(cid:16)

D∗

S =

i , uS
xS

i , x(cid:48)
i

S , rS

i , eS
i

(cid:17)

∈ DS | i = 1, . . . , N ; wi ≥ (cid:15)

(cid:111)
.

(5)

Finally, the buffer D∗

S is given to an ofﬂine RL model for

training.

Algorithm 1 Data Valuation for Ofﬂine RL

1: Input: Fixed source and target buffers DS and DT ; mini-
batch size Bs > 0; learning rate α; moving average
window size ω; selection threshold (cid:15).
2: Output: A subset of source buffer D∗

S which is relevant

S ← ∅; DVE parameters φ; rφ ← 0.

to target domain.

3: Initialize: D∗
4: for batch Bj in DS do
5:

)

6:

(cid:107) P

(cid:48)T )

j, rj, ej)

1
(cid:48) S )

(xT ,uT ,s

ω rrolling + 1

wj = vφ(Bj) = vφ(xj, uj, x(cid:48)
rφ ←
DKL(P
(xS ,uS ,s
rsig = rφ − rrolling
φ ← φ − α [rsig] ∇φ log πφ (Bj, wj)
rrolling ← ω−1

7:
8:
9:
10: end for
11: for batch Bk in DS do
12:
13:
14:
15:
16:

wk = vφ(Bk) = vφ(xk, uk, x(cid:48)
for sample i in Bk do
if wi ≥ (cid:15) then
S = D∗

S ∪ (xi, ui, x(cid:48)

D∗
end if

ω rφ

i, ri, ei)

k, rk, ek)

parametrization, we trained a DDPG agent for one million
iterations and used the fully-trained agent for generating the
buffers with the size one million and ten thousand for the
source and target, respectively.

C. Domains

In this work, we use the following two MuJoCo domains:
• Hopper-v3: The Hopper is a simulated monopod robot
with 4 body links including the torso, upper leg, lower
leg, and foot,
together with 3 actuated joints. This
domain has an 11-dimensional state space including
joint angles and joint velocities and a 3-dimensional
action space corresponding to torques at the joints. The
goal is to make the hopper hop forward as fast as
possible.

• Walker2d-v3: The Walker is a simulated bipedal robot
consisting of 7 body links including to two legs and
a torso, along with 6 actuated joints. This domain
has a 17-dimensional state space including joint angles
and joint velocities and a 4-dimensional action space
corresponding to joint torques. The goal is to make the
robot walk forward as fast as possible.

TABLE I
DOMAIN CONFIGURATIONS.

Domain

Source Conﬁg

Target Conﬁg

Name

end for

17:
18: end for
19: return D∗
S

A. Baselines

Hopper-v3

F: 2.0
T: 0.05

VII. EXPERIMENTS

Walker2d-v3

F: 0.9
T: 0.05

F: 2.0
T: 0.05

F: 2.5
T: 0.075

F: 3.0
T: 0.075

F: 0.9
T: 0.05

F: 1.125
T: 0.075

F: 1.35
T: 0.075

Hopper-Source

Hopper-Target1

Hopper-Target2

Walker2d-Source

Walker2d-Target1

Walker2d-Target2

In this work, we use the following ofﬂine RL methods
discussed in Section II as baselines: (Vanilla) BCQ, CQL and
TD3+BC. We also evaluated the performance of BEAR [5]
on our considered domains. However, as found by [6], we
found CQL and TD3+BC outperformed BEAR, and therefore
focused our evaluation on the above three methods and our
DVORL.

We use the (Vanilla) BCQ as the base model

in our
DVORL, and we refer to it as Data Valuation based BCQ
(DVBCQ). The reason for using the Vanilla BCQ is that it is
the most commonly used ofﬂine RL algorithm, and we also
intend to show how the selection of relevant transitions can
help a base model, underperforming other methods in most
cases, outperform the state-of-the-art methods in terms of
transferability of learned policy to different target conﬁgura-
tions. We consider two versions of DVBCQ: i) DVBCQ (x)
using information of states, and ii) DVBCQ (x, u, x
) using
information of state-action transition, for calculating the
similarity between source and target buffers.

(cid:48)

B. Gathering samples for the replay buffer

D. Source and target domain settings

For our experiments, we shall distinguish between source
and target domains. The source domain is the one within
which the samples are gathered by a fully-trained DDPG.
The target domain is the domain within which the DVORL
agent is to be deployed. To study the extent to which DVBCQ
can cope with modiﬁed domain conﬁgurations, we consider
two scenarios with respect to the source and target domains:
1) Identical Source and Target Domains: Domain con-
ﬁguration for gathering samples and training DVORL
agent remain the same. This is the simplest setting
where the DDPG agent gathers samples in an envi-
ronment with a domain parameterization identical to
the domain within which the DVORL agent will be
deployed. For this setting, we consider two datasets
“Hopper-Source” and “Walker-Source”.

The DVORL agent learns from a dataset collected by a do-
main expert. In our experiments, for each domain and domain

2) Transfer Learning: Samples are gathered from a
source domain with a parameterization that differs from

Fig. 1.
Illustration of the DVORL framework. (1) A batch of source buffer samples is given as input to the data value estimator (DVE). (2) KL Divergence
between the distribution of the state-action transitions of the given batch and the distribution of the state-action transitions of target buffer (whose transition
items are collected by a domain expert on the target domain) is calculated and (3) used as the reward signal for updating the DVE. (4) After all the samples
of the source buffer are used for training the DVE, (5) the fully trained DVE is used for outputting the data values of the source buffer (6) that are ﬁltered
out by removing those values being lower than the selection threshold. (7) This results in a subset of the source buffer that is relevant to the target domain
and is used for training the given base ofﬂine RL algorithm.

the target domain. More precisely, the target domain will
have different mass of torso and friction coefﬁcients
compared with the source domain. For this setting,
we consider four datasets “Hopper-Target1”, “Hopper-
Target2”, “Walker2d-Target1”, and “Walker2d-Target2”.
All the considered source and target domain conﬁgurations
are presented in Table I.

E. Parameter Tuning

For all the competitors, we used the default parameters
values reported in the corresponding papers. Hyperparame-
ters of DVORL are selected by grid search. Since we used the
BCQ in our DVORL method, we report the used parameter
values of Data Valuation based BCQ (DVBCQ), listed in
Table II. The parameters values of the baseline (Vanilla) BCQ
are the same as those of the base agent in our DVBCQ.

TABLE II
PARAMETERS VALUES OF OUR DVBCQ MODEL.

Parameter

Value

Description

dve batch size
dve hidden layers
moving average window size
selection threshold
mini batch size
discount
tau
lambda
phi

200
[128, 128]
20
0.1
100
0.9
0.005
0.75
0.05

batch size for DVE
Number of nodes in hidden layers
Window size for the moving average
selection threshold
Batch size for ofﬂine RL model
Discount factor
Target network update rate of BCQ
Weighting for clipped double Q-learning
Max perturbation parameter for BCQ

F. Implementation

Our implementation of the DVORL builds on OpenAI
gym’s [35] control environments with the MuJoCo [36]
physics simulator.

VIII. RESULTS
A. Evaluation of ofﬂine RL methods on source and target
domains

Figure 2 shows the performance of BCQ, CQL, TD3+BC,
) on the source domain

DVBCQ (x) and DVBCQ (x, u, x

(cid:48)

(cid:48)

and two different target domain conﬁgurations described in
Section VII-D. The models are trained for 1M iterations,
and the results are averaged over ten runs on target domain
environments with randomly-chosen seeds.
Identical source-domain: For DVBCQ and other baselines,
we report the average return achieved by the best policy with
respect to checkpoints saved throughout the run. For the iden-
tical source-domain setting, both DVBCQ (x) and DVBCQ
) signiﬁcantly outperform all baselines on Hopper
(x, u, x
environment, and their performance is superior to BCQ
and CQL, while underperforming TD3+BC on Walker2d
environment.
Transfer learning: For transfer learning setting, DVBCQ
(x) outperforms both target domains whose conﬁgurations
(mass of torso and friction) differ from those of the source
domain, on both Hopper and Walker2d environments. How-
) underperforms CQL on Hopper
ever, DVBCQ (x, u, x
environment and TD3+BC on Walker environment but it has
competitive performance compared with other baselines.

(cid:48)

It should be noted that the there is a signiﬁcant difference
between the performance of DVBCQ and its base model
BCQ.

B. Removing high/low value transitions from source dataset

Figure 3 shows the performance of BCQ models trained
on the source dataset with different selection thresholds
and evaluated on a different
target domain conﬁguration
(Walker2d-Target2), where all the models are trained for
200K iterations, and a ﬁxed seed is used for the evaluation
environment. We consider ﬁve thresholds (0.0, 0.1, ..., 0.4)
for excluding the high/low-value data samples of the source
dataset. In addition, we report the average return of the best
policy (with respect to checkpoints saved throughout the run)
learned for each point.

As shown in Figure 3, removing low-value samples from
the source dataset can help the RL agent learn only those
transitions relevant to the target domain conﬁguration and
therefore achieve better performance on the target domain

Pick-and-Place TaskSorting TaskDataValueEstimatorOﬄine RLAgentBatchesSource Replay BuﬀerTarget Replay BuﬀerFilteringdatavaluesrewardMeasuring the diﬀerence between                    of each batch and target datasetusing KL divergenceFig. 2. Performance of BCQ, CQL, TD3+BC, and DVBCQ on the source domain and two different target domain conﬁgurations (described in Section VII-
D), where the models are trained for 1M iterations, and the results are averaged over ten runs on target domain environments with randomly-chosen seeds.
For DVBCQ and other baselines, we report the performance achieved by the best policy with respect to checkpoints saved throughout the run.

(green line). On the other hand, removing high-value samples
from the source dataset signiﬁcantly deteriorates the RL
agent’s performance (red line).

The ﬁndings in Figure 3 support the opening hypothesis
that excluding high-value samples worsens the performance
of the ofﬂine RL methods.

Fig. 3.
Performance of BCQ models trained on the source dataset with
different selection thresholds and evaluated on a different target domain
the models are trained for
conﬁguration (Walker2d-Target2), where all
200.000 iterations, and a ﬁxed seed is used for the evaluation environment.
Excluding high-value samples (red line) aggravates the performance of the
ofﬂine RL methods. However, excluding low-value samples (green line) does
not deteriorate the performance as much as that of the high-value samples.

IX. DISCUSSION & FUTURE WORK
Our results suggest that DVORL can improve the ofﬂine
reinforcement learning methods on both identical source-
target and transfer learning settings. In addition, our method
helps the ofﬂine RL methods achieve signiﬁcantly higher per-
formance with fewer iterations, making them more efﬁcient.
Furthermore, our method can identify the relevant samples of
the source domain to different target domain conﬁgurations.
This is of high importance and has many use cases, such as
learning from an externally acquired dataset and safe RL.

It should be noted that our goal is not to show that our
proposed method outperforms all the state-of-the-art ofﬂine
RL methods on both source and target domains, but to show
that the data valuation for the ofﬂine reinforcement learning
(DVORL) framework can improve the performance of the
baseline algorithms.

For future work, we aim to examine whether the size of
target buffer plays a role in the performance of DVORL. We
intend conduct some experiments on real-world domains and
compare our results to other data valuation methods like Data
Shapely. Moreover, we plan to improve our reward function
by taking into account dynamics of the model.

We also aim to investigate the extent to which DVORL
can identify the safe transitions within a safe reinforcement
learning setting. We also plan to apply the idea of transition
valuation to the safe multi-agent reinforcement learning [37],

where different data value estimators are optimized for the
corresponding agent with respect to the tasks that they need
to perform. In addition, we aim to incorporate a mechanism
for auto-tuning the selection threshold into the training as the
optimal value for this parameter may vary from one domain
conﬁguration to another one.

X. CONCLUSION

In this work, we proposed a data valuation framework that
selects a subset of samples in the source dataset that are
relevant to the target task. The data values are estimated using
a deep neural network, trained using reinforcement learning
with a reward that corresponds to the similarity between the
distribution of the state-action transition of the given data
and the target dataset. We show that DVORL outperforms
baselines on different target domain conﬁgurations and has
a competitive performance on the source domain in which
the reinforcement learning agent is trained. We ﬁnd that
our method can identify relevant and high-quality transitions
and improve the performance and transferability of policy
learned by ofﬂine RL algorithms. Moreover, we contributed
a benchmark on two Gym MuJoCo domains (Hopper and
Walker2d) for which domain conﬁgurations (friction and
mass of torso) for the target domain differ from those of
the source domain.

ACKNOWLEDGMENT

The authors gratefully acknowledge, that the proposed
research is a result of the research project “IIP-Ecosphere”,
granted by the German Federal Ministry for Economics and
Climate Action (BMWK) via funding code 01MK20006A.

REFERENCES

[1] S. Lange, T. Gabel, and M. Riedmiller, “Batch reinforcement learning,”

in Reinforcement learning. Springer, 2012, pp. 45–73.

[2] D. Isele, A. Nakhaei, and K. Fujimura, “Safe reinforcement learning
on autonomous vehicles,” in 2018 IEEE/RSJ International Conference
on Intelligent Robots and Systems (IROS).

IEEE, 2018, pp. 1–6.

[3] S. Fujimoto, D. Meger, and D. Precup, “Off-policy deep reinforcement
learning without exploration,” in Proceedings of the 36th International
Conference on Machine Learning, ser. Proceedings of Machine
Learning Research, K. Chaudhuri and R. Salakhutdinov, Eds., vol. 97.
[Online]. Available:
PMLR, 09–15 Jun 2019, pp. 2052–2062.
http://proceedings.mlr.press/v97/fujimoto19a.html

[4] C. Breazeal and A. L. Thomaz, “Learning from human teachers with
socially guided exploration,” in 2008 IEEE International Conference
on Robotics and Automation.

IEEE, 2008, pp. 3539–3544.

[5] A. Kumar, J. Fu, G. Tucker, and S. Levine, “Stabilizing off-
policy q-learning via bootstrapping error reduction,” arXiv preprint
arXiv:1906.00949, 2019.

[6] A. Kumar, A. Zhou, G. Tucker, and S. Levine, “Conservative q-
learning for ofﬂine reinforcement learning,” Advances in Neural In-
formation Processing Systems, vol. 33, pp. 1179–1191, 2020.

[7] A. Ghorbani and J. Zou, “Data shapley: Equitable valuation of data for
machine learning,” in International Conference on Machine Learning.
PMLR, 2019, pp. 2242–2251.

[8] J. Yoon, S. Arik, and T. Pﬁster, “Data valuation using reinforcement
learning,” in International Conference on Machine Learning. PMLR,
2020, pp. 10 842–10 851.

[9] T. P. Lillicrap, J. J. Hunt, A. Pritzel, N. Heess, T. Erez, Y. Tassa,
D. Silver, and D. Wierstra, “Continuous control with deep reinforce-
ment learning,” arXiv preprint arXiv:1509.02971, 2015.

[11] V. Mnih, K. Kavukcuoglu, D. Silver, A. A. Rusu, J. Veness, M. G.
Bellemare, A. Graves, M. Riedmiller, A. K. Fidjeland, G. Ostrovski,
et al., “Human-level control through deep reinforcement learning,”
nature, vol. 518, no. 7540, pp. 529–533, 2015.

[12] S. Fujimoto, E. Conti, M. Ghavamzadeh, and J. Pineau, “Bench-
marking batch deep reinforcement learning algorithms,” arXiv preprint
arXiv:1910.01708, 2019.

[13] R. S. Sutton, “Learning to predict by the methods of temporal
differences,” Machine learning, vol. 3, no. 1, pp. 9–44, 1988.
[14] S. Thrun and A. Schwartz, “Issues in using function approximation for
reinforcement learning,” in Proceedings of the Fourth Connectionist
Models Summer School. Hillsdale, NJ, 1993, pp. 255–263.

[15] S. Fujimoto and S. S. Gu, “A minimalist approach to ofﬂine reinforce-
ment learning,” Advances in Neural Information Processing Systems,
vol. 34, 2021.

[16] A. Lapedriza, H. Pirsiavash, Z. Bylinskii, and A. Torralba, “Are all
training examples equally valuable?” arXiv preprint arXiv:1311.6510,
2013.

[17] S. Durga, R. Iyer, G. Ramakrishnan, and A. De, “Training data
subset selection for regression with controlled generalization error,”
in International Conference on Machine Learning. PMLR, 2021, pp.
9202–9212.

[18] Z. Zhu, K. Lin, and J. Zhou, “Transfer learning in deep reinforcement

learning: A survey,” arXiv preprint arXiv:2009.07888, 2020.

[19] A. Rajeswaran, S. Ghotra, B. Ravindran, and S. Levine, “Epopt:
Learning robust neural network policies using model ensembles,”
arXiv preprint arXiv:1610.01283, 2016.

[20] W. Yu, J. Tan, C. K. Liu, and G. Turk, “Preparing for the unknown:
Learning a universal policy with online system identiﬁcation,” arXiv
preprint arXiv:1702.02453, 2017.

[21] F. Sadeghi and S. Levine, “CAD2RL: Real single-image ﬂight without
a single real image,” arXiv preprint arXiv:1611.04201, 2016.
[22] I. Higgins, A. Pal, A. Rusu, L. Matthey, C. Burgess, A. Pritzel,
M. Botvinick, C. Blundell, and A. Lerchner, “Darla: Improving zero-
shot transfer in reinforcement learning,” in International Conference
on Machine Learning. PMLR, 2017, pp. 1480–1490.

[23] D. Slack, Y. Chow, B. Dai, and N. Wichers, “Safer: Data-efﬁcient
and safe reinforcement learning via skill acquisition,” arXiv preprint
arXiv:2202.04849, 2022.

[24] T. Schaul, J. Quan, I. Antonoglou, and D. Silver, “Prioritized experi-

ence replay,” arXiv preprint arXiv:1511.05952, 2015.

[25] Y. Hou, L. Liu, Q. Wei, X. Xu, and C. Chen, “A novel ddpg
method with prioritized experience replay,” in 2017 IEEE international
conference on systems, man, and cybernetics (SMC).
IEEE, 2017,
pp. 316–321.

[26] D. Horgan, J. Quan, D. Budden, G. Barth-Maron, M. Hessel, H. van
Hasselt, and D. Silver, “Distributed prioritized experience replay,” in
International Conference on Learning Representations, 2018.
[27] C. Kang, C. Rong, W. Ren, F. Huo, and P. Liu, “Deep deterministic
policy gradient based on double network prioritized experience replay,”
IEEE Access, vol. 9, pp. 60 296–60 308, 2021.

[28] R. Kidambi, A. Rajeswaran, P. Netrapalli, and T. Joachims, “Morel:
Model-based ofﬂine reinforcement learning,” Advances in neural in-
formation processing systems, vol. 33, pp. 21 810–21 823, 2020.
[29] I. Kostrikov, A. Nair, and S. Levine, “Ofﬂine reinforcement learning
with implicit q-learning,” in International Conference on Learning
Representations, 2022.

[30] A. Ghorbani, M. Kim, and J. Zou, “A distributional framework for
data valuation,” in International Conference on Machine Learning.
PMLR, 2020, pp. 3535–3544.

[31] A. Ghorbani and J. Y. Zou, “Neuron shapley: Discovering the respon-
sible neurons,” Advances in Neural Information Processing Systems,
vol. 33, pp. 5922–5932, 2020.

[32] B. Wang, M. Qiu, X. Wang, Y. Li, Y. Gong, X. Zeng, J. Huang,
B. Zheng, D. Cai, and J. Zhou, “A minimax game for instance based
selective transfer learning,” in Proceedings of the 25th ACM SIGKDD
International Conference on Knowledge Discovery & Data Mining,
2019, pp. 34–43.

[33] R. J. Williams, “Simple statistical gradient-following algorithms for
connectionist reinforcement learning,” Machine learning, vol. 8, no. 3,
pp. 229–256, 1992.

[10] D. Silver, G. Lever, N. Heess, T. Degris, D. Wierstra, and M. Ried-
miller, “Deterministic policy gradient algorithms,” in International
conference on machine learning. PMLR, 2014, pp. 387–395.

[34] E. Greensmith, P. L. Bartlett, and J. Baxter, “Variance reduction
techniques for gradient estimates in reinforcement learning.” Journal
of Machine Learning Research, vol. 5, no. 9, 2004.

[35] G. Brockman, V. Cheung, L. Pettersson, J. Schneider, J. Schul-
man, J. Tang, and W. Zaremba, “Openai gym,” arXiv preprint
arXiv:1606.01540, 2016.

[36] E. Todorov, T. Erez, and Y. Tassa, “Mujoco: A physics engine for
model-based control,” in 2012 IEEE/RSJ international conference on
intelligent robots and systems.

IEEE, 2012, pp. 5026–5033.

[37] I. ElSayed-Aly, S. Bharadwaj, C. Amato, R. Ehlers, U. Topcu, and
L. Feng, “Safe multi-agent reinforcement learning via shielding,” in
Proceedings of the 20th International Conference on Autonomous
Agents and MultiAgent Systems, 2021, pp. 483–491.

