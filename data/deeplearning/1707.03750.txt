DeepProf: Performance Analysis for Deep Learning
Applications via Mining GPU Execution Patterns

Jiazhen Gu∗, Huan liu∗, Yangfan Zou∗ and Xin Wang∗
∗School of Computer Science
Fudan University, Shanghai, China
Email:{16210240029, 1521024081, zyf, xinw}@fudan.edu.cn

7
1
0
2

l
u
J

2
1

]
E
S
.
s
c
[

1
v
0
5
7
3
0
.
7
0
7
1
:
v
i
X
r
a

are

learning

applications

Abstract—Deep

computation-
intensive and often employ GPU as the underlying computing
devices. Deep learning frameworks provide powerful program-
ming interfaces, but the gap between source codes and practical
GPU operations make it difﬁcult to analyze the performance of
deep learning applications. In this paper, through examing the
features of GPU traces and deep learning applications, we use the
sufﬁx tree structure to extract the repeated patten in GPU traces.
Performance analysis graphs can be generated from the prepro-
cessed GPU traces. We further present DeepProf, a novel tool
to automatically process GPU traces and generate performance
analysis reports for deep learning applications. Empirical study
veriﬁes the effectiveness of DeepProf in performance analysis
and diagnosis. We also ﬁnd out some interesting properties of
Tensorﬂow, which can be used to guide the deep learning system
setup.

I. INTRODUCTION

Recently machine learning techniques, especially deep
learning [1], have been proven effective to many sophisti-
cated applications like audio recognition [2], natural language
processing [3] and computer vision [4]. Many handy soft-
ware frameworks, e.g., Tensorﬂow [5], have been proposed
to improve the development efﬁciency of deep learning al-
gorithms. With such tools, deep learning applications can
be implemented with quite simple and concise codes. For
example, with Tensorﬂow, a neural network to perform high-
performance image recognition task can be realized in less
than 100 lines of codes. Short as they are, such programs still
require a long execution time. They are computation-intensive
in nature, which are generally designed to process tremendous
input data, and may incur lots of iterative runs. Bad design
can result in intolerable execution time, which may even lead
to their failed application in real-world scenarios. Therefore,
execution efﬁciency is a critical concern of such programs.

However, analyzing the performance of deep learning ap-
plications is difﬁcult. A key reason is that concise codes do
not indicate the simplicity of the underlying computation.
Actually, to get the best computational performance, deep
learning applications usually employ NVIDIA GPUs (Graph-
ics Processing Unit) as underlying computing devices [6].
Deep learning frameworks, like Tensorﬂow, provide simple
upper-layer APIs (Application Program Interface) that wrap a
sequence of complicated underlying GPU operations. When
executing a deep learning application, the source codes will
be optimized and transformed into concurrent GPU operations

[7]. This may lead to a great gap between the codes written
by developers and the operations actually executed. Therefore,
the underlying execution details of applications are concealed
from developers as well as potential performance issues, as a
price for the convenience of programming.

Such a complex framework realization, along with the
massive data transfer between host and GPUs, can be too
complicated to comprehend even for an experienced machine
learning developer. Moreover, current performance analysis
tools are generally designed for applications running on CPU
and cannot be used for GPU-based deep learning applications
because the execution mechanism of GPU is different from
that of CPU [6]. For example, the calls to CPU executions
are blocked while computing operations are asynchronous on
GPU, thus, CPU proﬁling cannot reﬂect the real execution
procedure of GPU operations, not to mention performance
analysis. On the other hand, Nvidia provides tools for GPU
traces proﬁling [8], but the results can hardly be understood
by deep learning developers, who are typically unfamiliar with
GPU operations. Therefore, how to bridge the execution gap
between the program codes and the underlying execution, and
to analyze the performance of deep learning applications is
yet to be well-addressed.

In this paper, we ﬁrst analyze the features of Tensorﬂow
programs and the corresponding GPU traces. Based on the
features, we summarize some execution rules of Tensorﬂow
applications. We then use the rules to bridge the execution gap
through recognizing repetition patterns in GPU traces. Since
the size of GPU traces is generally large, we leverage sufﬁx-
tree structure [9] to partition the unordered GPU operations
in O(n) space. By doing this, we simplify the tedious GPU
traces into the granularity of iteration, since deep learning
applications are typically iterative programs. Based on the
second generated GPU proﬁles, we generate performance
analysis graphs to help ﬁnding potential performance issues.
We implement DeepProf (Deep learning Proﬁler) to au-
tomatically process GPU traces and generate performance
analysis of deep learning applications. Through DeepProf,
we further summarize several execution properties of Tensor-
ﬂow applications. In particular, we proﬁle different Tensorﬂow
applications under different GPU devices with DeepProf.
The results reveal some potential GPU usage properties of
Tensorﬂow applications, which can be used as guidance for
system setup.

 
 
 
 
 
 
The rest of this paper is organized as follows. We introduce
some preliminary knowledge on machine learning and deep
learning frameworks in Section II, using Tensorﬂow as a
typical example. Section III illustrates a motivating example.
In Section IV, we analyze the execution details of Tensorﬂow
applications and elaborate how to extract patterns from GPU
traces. We also describe the implementation of DeepProf in
this section. Case studies are discussed in Section V. Section
VI presents related work. Finally, we conclude the paper in
Section VII.

II. BACKGROUND KNOWLEDGE

In this section, we describe some preliminary knowledge
on machine learning applications. First, we introduce the main
procedure of machine learning applications. Then we illustrate
how to use Tensorﬂow, a typical deep learning framework, to
execute the program on GPUs.

A. Machine Learning Applications

Machine learning is a ﬁeld that focuses on giving ”comput-
ers the ability to learn without being explicitly programmed.”
[10] Machine learning applications generally construct algo-
rithms that can learn from and make predictions on data [11].
Through building a model from sample inputs, the algorithms
are able to overcome the strictly static program instructions
and make data-driven predictions or decisions [12]. Machine
learning models are deﬁned by many model parameters and to
get the right values for all the parameters, iterative algorithms
are essential. The iterative aspect of machine learning makes
the models able to independently adapt when exposed to new
data [12]. Consequently, a machine learning application must
have at least one loop to processing input data and update
models, which we call the training step.

Deep learning is a class of machine learning algorithms
which learn multiple levels of representation and abstraction
that help to make sense of data [1]. Since deep learning
applications are designed to process large amount of data, the
data is partitioned into batches and execute batch by batch in
the training step. Thus iteration also plays an important role
in deep learning applications.

B. Tensorﬂow Applications

A powerful framework is essential

to developing deep
learning applications. Tensorﬂow is one of the most popular
deep learning frameworks [7]. With Tensorﬂow, developers
and researchers can focus on designing deep learning algo-
rithms, with no regards for the complex underlying operations.
To achieve this, Tensorﬂow leverages data ﬂow graphs to
represent the computation procedure, as well as other deep
learning frameworks [13]. A dataﬂow graph is consisted of
nodes and edges. Nodes in the graph represent mathematical
operators, while edges represent data arrays communicated be-
tween nodes. To execute a deﬁned dataﬂow graph, Tensorﬂow
ﬁrst generates GPU computing operations according to the
optimized dataﬂow graph, and then deploys the underlying
program to GPU.

In general, Tensorﬂow applications can be abstracted into
two parts, graph construction and iterative training. Ten-
sorﬂow provides developers with powerful APIs to create
dataﬂow graphs. In graph construction part, developers con-
struct the dataﬂow graph using Tensorﬂow APIs. The graph
consists not only essential computing operations of the train-
ing step, but also calculations of intermediate results or per-
formance measurements. In iterative training part, developers
have to create a tensorﬂow session, which contains the whole
predeﬁned dataﬂow graph. In order to execute the graph,
calling session.run method in a loop is required. The session
initializes the whole graph at the ﬁrst time session.run is
called. Every session.run call takes a list of parameters that
assign the execution part in the dataﬂow graph. The session
and dataﬂow graph mechanism allow developers to develop
deep learning applications without any underlying details.

C. CUDA

As mentioned in previous sections, deep learning applica-
tions are actually executed on NVIDIA GPUs to get the most
computation resources. The underlying computation platform
used by most deep learning frameworks is CUDA [14]. CUDA
is a parallel computing platform provided by NVIDIA. Deep
learning applications usually employ GPUs to accelerate ex-
ecution time because the applications generally involve huge
amount of matrix multiplications and other operations. These
operations can be massively parallelized and thus sped up on
GPUs. With CUDA, developers are capable of deploy their
programs on NVIDIA GPUs for high performance computing.
However, GPU based applications are much more complicated
than normal CPU-based ones. To run programs on a GPU,
developers have to deal with additional data management,(e.g.,
data copy from host to GPUs), as well as concurrency control.
To realize a simple matrix multiplication requires more than
100 lines of C++ codes, not to consider sending massive data
and executing complicated computations.

Tensorﬂow, like most other deep learning frameworks, hides
complex underlying CUDA realization from developers. Every
Tensorﬂow operators in the dataﬂow graph corresponds to one
or more CUDA operations including computing and memory
management. With Tensorﬂow, developers can focus on the
design of deep learning algorithms and leave the intricate
CUDA operations to the powerful framework. In next Section,
we illustrate the urgency and difﬁculty of deep learning
performance analyzers by a motivating example.

III. A MOTIVATING EXAMPLE

The dataﬂow graph mechanism of Tensorﬂow inevitably
results in complicated underlying executions. Consequently,
even simple codes may potentially contain subtle performance
issues, which are particularly hard to be detected by current
proﬁling tools. We demonstrate this by a simple example in
Figure 1.

The program in Figure 1 intends to build a gradient descent
model for recognizing images. The program creates the model
through line 2 to line 6. After constructing the dataﬂow graph,

TABLE I: Attributes of GPU trace

Attribute

Meaning

Start

Time from the program starts to the operation is called

Duration

Execution time of the operation

Size

Data copy size

Throughput

Throughput of data copy operation

Device

Stream

Name

Device name the operation executed on

Stream number the operation belongs to

Operation name

Fig. 1: An example of tensorﬂow program

a session is required to launch the graph. In line 8, the program
creates a Tensorﬂow session instance sess and initializes all
variables of the graph in line 9. Finally, the training step of this
program contains the deﬁned train and dbl loss operations, by
sending the operations as parameters to the sess.run call. Since
all the practical computations and underlying data copy are
managed by the tensorﬂow session, we successfully implement
the core algorithm within 20 lines of codes.

However, this simple program contains a subtle performance
issue. In line 13, the overloaded operator “*” implicitly calls
tf.convert to tensor function which leads to adding a new
node to the graph. Since the sess need to initialize the whole
the program has to add a node
graph before computing,
and re-initialize the graph in every loop, which becomes the
bottleneck of the performance. Without adding new nodes to
the graph, the sess only need to initialize the graph when the
ﬁrst time sess.run is called. In the above example, the sess
has to initialize the graph at every loop step, which causes
massive overhead to the execution time.

The performance issue is a typical memory leak problem
caused by graph growth. The subtle operators that add nodes
to the graph in training loop will result graph growth issues.
It is however difﬁcult to be unveiled. First, the issue is not
easy to identify because the execution time depends on several
factors. Underlying GPU performance and data transfer rate
can affect the execution time as well. It is hard to tell whether
the running time is abnormal or not because many developers
cannot estimate the expected execution time precisely. Even if
the performance issue is noticed, current tools cannot provide
enough implications to help locate the problematic codes.

Nvprof, provided by NVIDIA, is the ofﬁcial proﬁling tool
to collect CUDA-related activities [15]. Although nvprof
is
capable of listing all CUDA operations during the execution,
the results are difﬁcult for deep learning developers to under-
stand. The GPU trace collected by nvprof only contains GPU
operations which have little relation to source codes of the
program and make CPU proﬁling useless. Furthermore, the
GPU traces generated by nvprof can be quite large (58MB in
our simple example). The human effort to analyzing the results
manually is unaffordable for any individual or company.

Another tool provided by Tensorﬂow is timeline. The time-

line tool is capable of proﬁling the Tensorﬂow operators in
one sess.run call and generate a .json ﬁle. However, the graph
growth in the example happens before session.run and hence
the results are useless. Moreover, a simple application call
sess.run for thousands of times so that this tool will create
thousands of json ﬁles. As a result, developers can’t observe
the overall performance of applications but the disconnected
training step.

Therefore, lack of appropriate solutions to analyzing GPU
traces makes it difﬁcult to detect performance issues. In the
next Section, through detailed analyzing Tensorﬂow applica-
tions and GPU traces, we propose DeepProf, a novel tool
that helps developers to analyze deep learning application
performance and to ﬁnd potential problems.

IV. GPU TRACE ANALYSIS AND PROCESSING

to extract helpful

To measure the application performance precisely, we ex-
information from the tedious and
pect
unreadable GPU traces. We ﬁrst analyze the GPU traces in
detail and extract the ordered GPU computing operations of
Tensorﬂow applications. Leveraging the iterative property of
deep learning algorithms, we use the sufﬁx tree structure and
an approximate matching algorithm to partition all comput-
ing operations to different parts. Every part refers to the
underlying execution caused by one iteration step in deep
learning algorithms. Based on the generated GPU proﬁle
with ﬁne granularity, we deﬁne some metrics to measure
the execution performance intuitively. We further implement
DeepProf to automatically analyze the performance for deep
learning applications. Although our approach can adapt to
multi-loop scenarios, for simplicity, we only discuss the one-
loop condition in analysis part. We show how to deal with
multi-loop applications in the end of this section.

A. GPU Trace Analysis

Table 1 shows the attributes with their meanings of GPU
traces collected through nvprof. We introduce the concepts
of CUDA operation and CUDA stream ﬁrst. We classify the
operations in GPU traces according to name and stream
attributes and then summarize some important rules about the
underlying execution of Tensorﬂow applications.

1   #Create the model2   ...3   loss=...4   train=tf.train.GradientDescentOptimizer5   (0.01).minimize(loss)6   init=tf.initialize_all_variables()7   #Create Session8   withtf.Session()assess:9   sess.run(init)10   #Train11   for_inrange(1000000):12   sess.run(train)13   dbl_loss=loss*2.0#potential issue14   sess.run(dbl_loss)Fig. 2: An example on GPU concurrency

1) CUDA Operations: According to the name attribute,
we notice that there are two main kinds of operations in
GPU traces, kernels and memcpy (memory copy) operations. A
kernel is a function deﬁned by CUDA api and can be executed
on GPU. The kernels can be deﬁned by either CUDA or
Tensorﬂow. The memcpy operations can be further divided into
three types according to the source and destination of the data
copy: memcpyHtoD, memcpyDtoH and memcpyDtoD. The ’H’
means host while ’D’ means GPU device. In the GPU trace
of deep learning applications, we can directly distinguish two
kinds of CUDA operations deﬁned above, according to the
throughput attribute. Speciﬁcally, computing operations do
not have throughput attribute while memcpy operations do.

2) CUDA Stream: CUDA platform use stream mechanism
to achieve concurrency. A stream contains a sequence of
operations that execute in issue-order on GPU [14]. The
operations in different streams may run concurrently and be
interleaved. Note that although the operations in one stream are
executed in order, one operation can utilize many processors
and be executed in parallel on GPU. Figure 2 shows a simple
concurrency example on GPU. After copying data to the GPU,
the Kernel and M emcpyDtoH operations are executed by
4 streams and get 1.33x performance improvement than serial
execution. Note that in t moment, only K1 is executed on
GPU and thus this operation can utilize all GPU processors
for computing.

Through the above analysis, we can reveal some underlying
the
features of Tensorﬂow applications. As stated before,
operations in the same stream are executed in their issue-
order. Through extracting all operations that have the same
stream value and sorting them on the start value, we get
the ordered operations of different streams during the execu-
tion. We continue to use the program in Section III as an
example. After applying the extraction and sorting operations
to the GPU trace, we summarize the distribution of different
operations in each stream in ﬁgure 3. We notice that only
stream 13 contains computing operations while stream 14 and
15 only consists of memcpyHtoD operations and memcpyDtoH
operations respectively. The rest stream (stream 7) has only

Fig. 3: An example of streams used by Tensorﬂow

two entries, one memset (memory set) and one memcpyHtoD.
Based on above observation, we classify underlying streams
used by TensorFlow applications into three kinds. The speciﬁc
deﬁnitions are as follows:

1. Main stream: Streams contain Tensorﬂow deﬁned kernels,

like stream 13 in the example, are called main streams

2. Copy stream: Streams only contain memcpy operations,

like stream 14 and 15, are called copy streams.

3. Assist stream: All streams that do not belong to main
stream and copy stream are called assist streams, like stream
7.

With above deﬁnitions on CUDA operations and streams,
we further analyze the GPU traces of different Tensorﬂow ap-
plications, and summarize two general rules of the underlying
Tensorﬂow realization as follows:

Rule 1: Tensorﬂow only creates one main stream [7], which
includes all kernels deﬁned by Tensorﬂow. Besides kernels,
this stream may contain memcpyDtoD operations as well. This
design allows the operations in main stream to utilize all
processers and memory spcace in GPU if necessary.

Rule 2: Two copy streams are created by Tensorﬂow to
copy data between host and GPU. One stream deals with
memcpyHtoD operations while the other copys data from GPU
to host.

Rule 3: Assist streams are used to handle trivial GPU
operations. The number of assist streams is depended on the
realization of applications, but Tensorﬂow creates at least one
assist stream, just like stream 7, to initialize GPU memory.

According to Rule 1 and stream mechanism of CUDA, we
can conclude that the GPU computing procedure of Tensorﬂow
is sequential. Based on this conclusion, we leverage the
iteration property of deep learning algorithms to partition the
GPU traces.

B. Sufﬁx Tree Based Pattern Search

Let’s review the structure of Tensorﬂow applications. Any
deep learning algorithm requires a training step, which refers
to a subgraph of the deﬁned dataﬂow graph in Tensorﬂow.
In iterative training part, a session is created to execute
the subgraph iteratively. We deﬁne the basic iteration as
one session.run on the subgraph of training step. Once the
dataﬂow graph is deﬁned, the GPU operations caused by the
basic iteration is determined. According to Rule 1, all the

Kernel<>MemcpyDtoHSerialMemcpyHtoDK1DH1MemcpyHtoDK2K3K4DH2DH3DH4Streamsexecute K1 with all processorsGPUtimetimet......MemcpyDtoD......CUDA Kernels......TF Kernels......Stream 13......MemcpyHtoDMemcpyHtoD......MemcpyHtoDMemcpyHtoD......Stream 14......MemcpyDtoHMemcpyDtoH......MemcpyDtoHMemcpyDtoH......Stream 15MemsetMemcpyHtoDStream 7computing operations of Tensorﬂow are issued by one stream
and thus have a strict execution order. As a result, executing
basic iteration will lead to the same underlying computing
operations. Since applications execute the basic iteration for
many times, there must be a repeated pattern in the collected
GPU traces. Note that the repeated pattern may not occur
in every iteration because applications are likely to calculate
some statistics periodically, which leads to the underlying
operations different from those of basic iteration. Under this
circumstance, applications actually execute the basic iteration
with the operators performing statistic calculation. The GPU
trace of such an iteration should be an insertion of some
GPU operations to the basic iteration trace. Now we get the
following criterion:

Criterion 1: The GPU trace consists of the basic iteration,
which is interleaved by GPU operations for other purposes,
such as statistic calculation.
Based on Criterion 1, we can extract the basic iteration
caused operations through mining frequent patterns in GPU
traces. We use the sufﬁx tree structure to preprocess the
original GPU traces.

1) Sufﬁx Tree: Sufﬁx trees allow efﬁcient query processing
on text data, which was ﬁrst introduced in [9]. Every internal
node in a sufﬁx tree represents a substring while every leaf
node represents a sufﬁx of the original string. Figure 4 shows a
sufﬁx tree of the string “banana$”. The “$” symbol represents
the string terminator. The example tree consists of 4 internal
nodes and 7 leaves. Note that each node in the sufﬁx tree
represents a substring of the input string. More precisely,
a leaf node corresponds to a sufﬁx while a branch node
corresponds to a preﬁx of the sufﬁx, i.e., a substring. For
example, the internal node “5” in Figure 4 represents the
substring ”ana”, and the leaf node “9” represents the sufﬁx
“anana”. Furthermore every node and its descendants forms a
subtree in the sufﬁx tree. The number of leaves in a subtree
determined by a node is the times that the corresponding
substring repeats in the input string. For example, node “5” has
2 leaves in its subtree so “Ana” repeats 2 times in “banana”.
2) Why Sufﬁx Tree: As mentioned in section III, the GPU
trace of a deep learning application is often extremely large.
Both the iteration times and the complexity of algorithm
can cause a great amount of underlying computations in
GPU trace. Thus we cannot afford a high space complexity
algorithm to discover frequent patterns. Constructing a sufﬁx
tree is quite efﬁcient. The space and time complexity of
constructing a sufﬁx tree is O(n), thus we choose sufﬁx tree
structure to deal with the massive original traces.

Next we introduce the details of pattern mining in GPU
traces. For ease of reference, we summarize the symbols we
use in Table 2.

We preprocess the original data and treat all operation
names in the main stream as a long string S. S is a two-
dimension array and every element of S is a GPU operation
name. We then construct a sufﬁx tree ST of S. Each substring
repeats c times in S is represented by an internal node that
has c leaves as its descendants in ST . Moreover, the length l

Fig. 4: The sufﬁx tree of string “banana$”.

TABLE II: Symbol Table

Symbol

Meaning

S

ST

lstr

cstr

lp

cp

i

k0

The long string constructed by all operation names
in a main stream

A sufﬁx tree

The length of the string str

The repetition times of the string str in S

The length of the pattern

The repetition times of the pattern

The total number of iterations in a application

The maximum unmatched string length

of the repeated substrings should be appropriate because short
patterns cannot form an iteration step while long patterns may
contain too many operations. If we know the right cp and lp
values, we can scan all nodes in ST and select the frequent
substring in O(n) time.

3) Parameter discussion: The value of cp and lp are related
to the number of basic iteration executed. If the application
executes exactly the same instructions in every iteration, cp
to i, which is the total number of iterations. If
is equal
some iterative steps execute additional instructions, according
to Criterion 1, the number of total operations increases.
Considering there are some initial operations,
that do not
belong to any iteration,
the average GPU operations per
iteration must be larger than lp. Thus far we get the following
two criterions for cp and lp:

Criterion 2: (i − ε) < cp ≤ i, where ε is the threshold we

deﬁned.

Criterion 3: lp < lS

and i is the number of iterations in the application. lS
the number of GPU operations per iteration.

i , where lS is the total length of string S
i means

the longest substring P that repeats at

Consequently, we can scan all nodes in ST and select
frequent substrings that satisfy Criterion2 and 3. We only
least i − ε
select
times, and is shorter than lS
i , where ε is a priori threshold.
If no internal node satisﬁes the ﬁlter conditions, the program
doubles the value of ε and re-search the sufﬁx tree. In the

02315671098banana$$naanana$$$na$$4Algorithm 1 Performance Metrics Table
Input: substring P , string S
Output: all matching positions in array res

1: res ← ∅
2: while i ≤ length(S) do
k ← 0, i0 ← i
3:
for j ← 1 · · · length(P ) do
4:
while S[i0] (cid:54)= P [j] do
5:
6:
7:

k ← k + 1, i0 ← i0 + 1
if k > k0 then
goto label

8:
9:
10:
11:
12: label:
13:

end if
end while

end for

if k ≤ k0 then

append(res, pair(i, i0)); i ← i0 + 1

else

14:
15:
16:
end if
17:
18: end while
19: return res

i ← i + 1

TABLE III: Metrics Table

Metrics

Meaning

avg interval

Average of iteration interval

max interval

The maximum value of all iteration interval

avg overlap

Average value of interval overlap

avg operation

Average interval time of two GPU operations

avg size

Average data copy size from host to GPU per iteration

because copying data from GPU to host can be executed
concurrently and has little effect on the start time of the
next iteration. A high interval overlap implies that the data
transfer operations become the bottleneck. We also calculate
the average value of iteration interval and interval overlap,
as well as some intuitional metrics. Detailed meanings of
different metrics are shown in Table 3.

worst scenario, the total time complexity of constructing ST
and select the frequent substring is O(mn), where m is the
re-search times for an eligible node in the tree.

Finally, we search for all patterns that approximately match
P in O(mn) time, where m is the length of P and n is
the length of S, as shown in Algorithm 1. The parameter
k0 represents the maximum unmatched string length. In other
words, the approximate patterns can have at most k0 more
operations than substring P . The algorithm checks all possible
positions that may have a approximate match and return the
start and end position of the matching substrings.

Fig. 5: State graph of iteration

C. Performance Metrics Analysis

D. DeepProf realization

Through the sufﬁx-tree-based approximate match algo-
rithms, we successfully divide the ruleless GPU trace into
different parts. Based on the partitioned trace, we further
calculate a serious of metrics to measure the execution perfor-
mance of deep learning applications.

We deﬁne the iteration interval as the leisure time during
two adjacent iterations. In other words, iteration interval is the
time difference between the start of one iteration and the end
of its previous one. The latter iteration cannot start until GPU
receives the input data and CPU ﬁnishes the other instructions
in the loop of source codes, as the state graph shown in
ﬁgure 5. Note that although data copy state is independent
to GPU and CPU operations, the GPU iteration has to start
after the corresponding data copy operations. Therefore, the
size of iteration interval represents the execution time of CPU
instructions and data copy operations.

We further deﬁne the interval overlap as the ratio of data
copy time to compute interval. We only consider the time
of memcpyHtoD operations during each iteration interval,

Based on the above discussion, we implement DeepProf,
a novel tool to automatically mine patterns in original GPU
traces and generate performance proﬁles for deep learning ap-
plications. The processing framework adopted in DeepProf
is shown in ﬁgure 6. There are four main components of
DeepProf. In preprocessing part, DeepProf ﬁrst extract all
GPU operations belong to the main stream and then combine
these operations into a long string S. In the pattern mining part,
DeepProf construct the sufﬁx tree of S, and mine the fre-
quent pattern P according to the number of iterations, which
can be fetched through analyzing the source codes. The start
and end positions of all approximately matched patterns of P
are generated in the approximate match part. According to the
results from approximate match part, DeepProf partitions
the GPU traces and calculates the performance metrics in the
metrics generation part. The results generated by DeepProf
contain a summary of average performance metrics and a ﬁle
contains detailed metrics of every iteration. The results can be
explained intuitively by the state graph shown in ﬁgure 6.

CPUInstructionsGPUIterationDataCopy1) Adapt to multi-loop: The above DeepProf framework
can adapt
to more complicated applications with multiple
loops, which means GPU traces can have several patterns.
In the sufﬁx tree,
the multi-loop is property reﬂected in
several frequent substrings. The repetition times and length
of each substring still satisfy Criterion2 and 3. Knowing the
number of loops and corresponding iterative steps, we just
need to search the sufﬁx tree with different parameters. All
the parameters required to mine the frequent patterns can be
obtained from the source codes, so DeepProf is capable of
dealing with multi-loop applications.

Next, we use DeepProf to analyze and diagnose the
performance of Tensorﬂow applications. The results show the
effectiveness of DeepProf, and reveal some hidden features
of Tensorﬂow.

V. EMPIRICAL STUDY

In this section, we evaluate the effectiveness of DeepProf
in two aspects, performance analysis and diagnosis. We ﬁrst
analyze the performance of computation-intensive Tensorﬂow
applications as well as IO-intensive ones on different GPUs
by DeepProf. The results reveal a few execution features of
Tensorﬂow, which can be used as guidance for system setup.
The analysis results generated by DeepProf can also be used
to diagnose performance issues. We demonstrate how to detect
the potential performance issues according to the abnormal
metrics using two cases. The ﬁrst issue is shown in Section
III, which is caused by graph growth. The second issue resides
in data transfer procedure where oversize data copy becomes
the bottleneck. We conduct the experiment on a Ubuntu 16.04
server with i7 6700K CPU and NVIDIA 1080Ti graphics card.

A. Performance Analysis

can be

Tensorﬂow applications

classiﬁed into two
types, computation-intensive and IO-intensive. Computation-
intensive applications employ a complex model to improve
the prediction accuracy. In this kind of application, every
training step requires a long execution time and the data
transfer operations has little effect on performance. On the
other hand, data-intensive applications are designed to process
a large amount of input data, while large data transfer between
host and GPU is common in this kind of applications. In
general, developers intend to allocate the whole GPU memory
to one application. In other words, one GPU only executes

Fig. 6: Overview of DeepProf framework

(a) Execution time with different memory size

(b) GPU utilization with different memory size

Fig. 7: Single execution result

one application at a time. So our question is whether running
several applications concurrently on one GPU will cause great
performance loss. To address the problem, we ﬁrst execute
one application with a limited GPU memory size and use
DeepProf to analyze the performance.

We test both computation-intensive and IO-intensive ap-
plications with a limited GPU memory percentage, and we
measure the how the allocated memory size affect execution
time and GPU utilization. The results are surprising, as shown
in Figure 7. Allocating too little GPU memory to applications
leads to ’out of memory’ error, so there is no time and
utilization with little memory percentage in Figure 7. The
execution time and GPU utilization of both applications stay
almost the same, regardless of how much GPU memory is
used. The results imply that once the application get enough
memory to ﬁnish the execution, the performance cannot be
improved with more GPU memory.

We then execute two applications concurrently on one GPU
and ﬁnd if the two applications will inﬂuence each other. To
control variables, we test the computation-intensive application
with different memory size when data-intensive application
is executed with a ﬁxed memory size, and vice versa. We
test the applications with 25% and 50% memory occupied. So
the corresponding memory percentage left is up to 75% and
50%, as shown in Figure 8. Comparing to running on the GPU
singly, the execution time of both applications increases, which

PreprocessingPatternMiningStringApproximateMatchPatternMetricsCalculationGPUtraceParametersResults020406080100memory percentage (%)0102030405060time (s)computation intensiveI/O intensive102030405060708090100memory percentage (%)020406080100utilization (%)computation intensiveI/O intensive(a) Result of IO-intensive application

(b) Result of computation-intensive application

Fig. 8: Concurrent execution result

proves that the executing several applications on one GPU
will cause performance loss. Noted that the execution time of
one application still stay the same, regardless of the memory
size allocated to the other application. Even though there is
available GPU memory space, the applications may sustain
performance loss. Through the results shown in Figure 7 and
Figure 8, we draw the conclusion that although Tensorﬂow
limited,
applications may occupy all GPU memory if not
the memory size actually has little inﬂuence on execution
performance.

Based the above analysis, we ﬁnd that although Tensorﬂow
applications intend to use as much GPU as they can, high
memory usage does not imply high performance.

B. Performance Diagnosis

DeepProf can also be used for simple performance diag-
nosis. We show two typical performance issues that can be
detected using DeepProf.

The ﬁrst case is the graph growth issues. As mentioned
in Section III, adding new nodes to the dataﬂow graph leads
to graph re-initialization. In fact, the session has to initialize
the graph before execution. Without graph change, the session
only needs to initialize the graph once in the ﬁrst iteration

(a) Case 1 results

(b) Case 2 results

Fig. 9: Results of DeepProf for performance diagnosis

and keeps the initialized graph for later invocation. The graph
growth causes the session to initialize the graph in every
iteration and produce a huge overhead. DeepProf can help
developers to diagnose such performance issues. Figure 9 (a)
shows the execution state graph of the example program in
Figure 3, along with the metrics calculated by DeepProf.
We can ﬁnd that the avg interval is much larger than avg
operation, implying the performance bottleneck exists outside
the iteration. In other words, no GPU computing operation
somehow blocks the underlying execution. As the state graph
shows, both slow data copy and CPU instructions may cause
such block. In Figure 9 (a), the avg overlap metric, represent-
ing the average ratio of data copy time during the iteration
interval, is small. Small avg overlap implies the block is
not caused by data copy operations since data copy only
contributes little to the interval time. Therefore, it is very
likely that the CPU instructions are the origin of the issue and
developers can check the training loop part of source codes.
DeepProf successfully detects the graph growth issues and
help locating the slow code in this case.

The second case is the oversize data copy. Data transfer is
a well-known bottleneck in deep learning ﬁeld. Deep learning
applications are generally designed to process tremendous data
but the data transfer efﬁciency between host and GPU can’t
match the great computing power of GPU. Although data
transfer efﬁciency is an inherent problem, performance issues
can be unexpectedly caused by inappropriate parameters. Deep
learning applications usually process the data in batch, thus
improper batch size may cause unnecessary overheads. Figure
9 (b) shows the execution state graph of an application with an
overlarge batch size. In the state graph, we can see avg interval
is large, and so does avg overlap. This phenomenon implies
that the GPU iteration is blocked by data copy operations,
since 12 percent of interval time is spent on waiting data
copy operations. A smaller batch size in the application can
improve the execution performance. On the other hand, some
deep learning algorithms require certain amount of data for
processing and the batch size cannot be smaller. In this case,
developers should decide on the tradeoff between performance
and effectiveness. DeepProf diagnose the data copy bottle-

020406080100memory percentage (%)050100150200time (s)no memory occupied25% memory occupied50% memory occupied102030405060708090100memory percentage (%)0102030405060time (s)no memory occupied25% memory occupied50% memory occupiedCPUInstructionsGPUIterationDataCopyavg size:504.7 KBavg interval:0.0783 msavg overlay:0.0471 %avg operation:0.0013 msmax interval:0.1082 msCPUInstructionsGPUIterationDataCopyavg size:2603.7 KBavg interval:0.0240 msavg overlay:13.963 %avg operation:0.0003 msmax interval:0.0421 msneck in this case.

The two cases above are all based on real Tensorﬂow
applications and inexperienced developers often make such
mistakes. DeepProf can help these inexperienced developers
to rapidly diagnose the application performance.

generate operational proﬁles [28]. However, GPU traces are
much more complicated than logs since one upper API call
may cause several GPU operations. Furthermore, DeepProf
provides execution summaries from GPU traces which can
help developers to analyze application performance.

C. Discussions

Through the empirical study, DeepProf shows the power
of analyzing the performance of deep learning applications.
DeepProf is also capable of detecting the common perfor-
mance bottleneck and help developers to identify mistakes.
The space complexity of DeepProf is O(n), which means
DeepProf can deal with large GPU traces. Moreover, the
results generated by DeepProf are based on the number
of iterations of the application and GPU traces collected by
nvprof, thus the design ideas of DeepProf can be applied to
most GPU-based deep learning frameworks.

However, there are also some defects in DeepProf. The
performance metrics generated by DeepProf are straightfor-
ward and how to ﬁnd more subtle metrics with the partitioned
GPU trace is worth studying. In addition, DeepProf focuses
on analyzing the performance of applications executed on sin-
gle GPU device. Analyzing the performance of deep learning
applications under multi-GPU circumstance remains an open
question.

VI. RELATED WORK

A. Proﬁling

[21],

[20],

The usefulness of proﬁling has long been recognized.
Proﬁling is a kind of dynamic program analysis which is
widely used in functional fault detection [16], [17], [18],
[22].
fault detection [19],
and non-functional
Jiang et al. utilized execution proﬁlers that possibly contain
faults to simplify programs and scale down the complexity
of programs for in-house testing [23]. AppInsight, provided
by Ravindranath et al., instruments mobile-app binaries to
identify the critical path in user transactions automatically
[24]. Coppa et al. proposed an approach to measure how
the size of input effects performance, and used it to ﬁnd out
performance faults by analyzing proﬁles [25]. Chilimbi et al.
provided HOLMES, a tool to proﬁle the selected parts of the
application, and then rank and identify the critical paths which
can predict the failures through path proﬁles [18]. Han et al.
proposed StackMine, which extracts effective subsequences of
function calls by a costly-pattern mining algorithm, to help
the performance debugging. Shen et al. proposed GA-Prof,
which used search-based input-sensitive application proﬁling
for automating performance bottlenecks detection [26]. These
work identify critical paths and help identify performance
problems through proﬁling. DeepProf takes the core idea
of proﬁling analysis and focus on handling GPU traces.

Several work focus on deriving operational proﬁles with
clustering algorithms [27]. Gittens et al. increased the proﬁling
applicability by an extended operational proﬁle model to ad-
dress the heterogeneity of software. Nagappan et al. proposed
a sufﬁx-array based algorithm to parse the execution logs and

B. Detecting and ﬁxing performance problems

Detecting and ﬁxing performance problems were shown to
be challenging [29]. Several techniques have been proposed
to identify performance problems such as slow code [21],
[30], [31], [32], [33] and increasing execution time [25],
[26], [34]. Grechanik et al. proposed FOREPOST, a feedback-
directed black-box approach for detecting performance prob-
lems and identifying bottlenecks [35]. Liu et al. designed
to identify existence
an innovative system, AutoAnalyzer,
of performance bottlenecks by clustering algorithms and to
locate the bottlenecks using searching algorithm [36]. Song
and Lu investigated design points in statistical debugging
for problem diagnosis [37]. Chis pinpointed memory issues
through detecting memory anti-patterns from memory catalogs
[38]. Nistor et al. designed Toddler, which detecting per-
formance bugs through similar memory-access patterns [39].
Chen et al. proposed a framework to detect object-relational
mapping performance anti-patterns automatically [40]. Other
work focused on concurrency performance problems [41],
performance testing [36] and latent performance bugs [42].
DeepProf is designed for analyzing performance of deep
learning applications through mining patterns in GPU traces,
and such scenario is not covered by previous work. To the best
of our knowledge, we are the ﬁrst to analyze the performance
of GPU-based deep learning applications.

VII. CONCLUSION

Deep learning applications are computation-intensive in
nature and may incur incredibly lengthy execution time. Al-
though powerful frameworks, like Tensorﬂow, make it con-
venient to develop deep learning applications, the complex
underlying GPU operations make it difﬁcult to measure the
performance of applications. In this paper, through analyz-
ing the execution procedure of Tensorﬂow applications in
detail, we propose a novel analysis tool, DeepProf,
to
automatically mine the patterns from GPU traces and analyze
application performance. We further verify the effectiveness of
DeepProf through the empirical study on performance anal-
ysis and diagnosis. We also conclude the underlying execution
features of Tensorﬂow, which can be used as guidance for the
deep learning system setup. To the best of out knowledge, we
are the ﬁrst to analyze the performance of GPU-based deep
learning applications. Although DeepProf is designed for
Tensorﬂow applications, the preprocessing procedure of GPU
traces can adapt to all GPU-based deep learning applications.
Finally, in our future work, we are interested in extending
DeepProf to analyze the performance of application using
multi-GPU and to achieve bug detection by taking CPU
proﬁling into consideration.

[1] Y. LeCun, Y. Bengio, and G. Hinton, “Deep learning,” Nature, vol. 521,

IEEE software, vol. 10, no. 2, pp. 14–32, 1993.

REFERENCES

[27] J. D. Musa, “Operational proﬁles in software-reliability engineering,”

no. 7553, pp. 436–444, 2015.

[2] G. Hinton, L. Deng, D. Yu, G. E. Dahl, A.-r. Mohamed, N. Jaitly,
A. Senior, V. Vanhoucke, P. Nguyen, T. N. Sainath et al., “Deep neural
networks for acoustic modeling in speech recognition: The shared views
of four research groups,” IEEE Signal Processing Magazine, vol. 29,
no. 6, pp. 82–97, 2012.

[3] R. Collobert and J. Weston, “A uniﬁed architecture for natural language
processing: Deep neural networks with multitask learning,” in Proc. of
the 25th ACM International Conference on Machine Learning (ICML).,
2008, pp. 160–167.

[4] A. Krizhevsky, I. Sutskever, and G. E. Hinton, “Imagenet classiﬁcation
with deep convolutional neural networks,” in Advances in neural infor-
mation processing systems, 2012, pp. 1097–1105.

[5] M. Abadi, P. Barham, J. Chen, Z. Chen, A. Davis, J. Dean, M. Devin,
S. Ghemawat, G. Irving, M. Isard et al., “Tensorﬂow: A system for
large-scale machine learning,” in Proc. of the 12th USENIX Symposium
on Operating Systems Design and Implementation (OSDI)., 2016.

[6] “Nvidia gpus,” http://developer.nvidia.com/deep-learning/.
[7] “Tensorﬂow,” https://www.tensorﬂow.org/.
[8] “Gpu proﬁler,” http://docs.nvidia.com/cuda/proﬁler-users-guide/index.

html/.

[9] P. Weiner, “Linear pattern matching algorithms,” in Proc. of the IEEE
Conference Record of 14th Annual Symposium on Switching and Au-
tomata Theory (SWAT)., 1973, pp. 1–11.

[10] E. Alpaydin, Introduction to machine learning. MIT press, 2014.
[11] R. Kohavi and F. Provost, “Glossary of terms,” Machine Learning,

vol. 30, no. 2-3, pp. 271–274, 1998.

[12] C. M. Bishop, “Pattern recognition,” Machine Learning, vol. 128, pp.

1–58, 2006.

[13] “Theano,” http://deeplearning.net/software/theano/index.html.
[14] “Cuda toolkit,” https://developer.nvidia.com/cuda-toolkit/.
[15] “nvprof tool,” http://docs.nvidia.com/cuda/proﬁler-users-guide.
[16] M. Pradel and T. R. Gross, “Leveraging test generation and speciﬁcation
mining for automated bug detection without false positives,” in Proc.
of the 34th IEEE International Conference on Software Engineering
(ICSE)., 2012, pp. 288–298.

[17] S. Zhang and M. D. Ernst, “Automated diagnosis of software conﬁgu-
ration errors,” in Proc. of the 35th IEEE International Conference on
Software Engineering (ICSE)., 2013, pp. 312–321.

[18] T. M. Chilimbi, B. Liblit, K. Mehra, A. V. Nori, and K. Vaswani,
“Holmes: Effective statistical debugging via efﬁcient path proﬁling,” in
Proc. of the 31st IEEE International Conference on Software Engineer-
ing (ICSE)., 2009, pp. 34–44.

[19] D. Yan, G. Xu, and A. Rountev, “Uncovering performance problems in
java applications with reference propagation proﬁling,” in Proc. of the
34th IEEE International Conference on Software Engineering (ICSE).,
2012, pp. 134–144.

[20] G. Xu and A. Rountev, “Precise memory leak detection for java software
using container proﬁling,” in Proc. of the 30th ACM/IEEE International
Conference On Software Engineering (ICSE)., 2008, pp. 151–160.
[21] S. Han, Y. Dang, S. Ge, D. Zhang, and T. Xie, “Performance debugging
in the large via mining millions of stack traces,” in Proc. of the 34th
IEEE International Conference on Software Engineering (ICSE)., 2012,
pp. 145–155.

[22] B. Chen, Y. Liu, and W. Le, “Generating performance distributions via
probabilistic symbolic execution,” in Proceedings of the 38th Interna-
tional Conference on Software Engineering. ACM, 2016, pp. 49–60.
[23] L. Jiang and Z. Su, “Proﬁle-guided program simpliﬁcation for effective
testing and analysis,” in Proc. of the 16th ACM International Symposium
on Foundations of software engineering (SIGSOFT)., 2008, pp. 48–58.
[24] L. Ravindranath, J. Padhye, S. Agarwal, R. Mahajan, I. Obermiller, and
S. Shayandeh, “Appinsight: Mobile app performance monitoring in the
wild.” in Proc. of the 12th USENIX Symposium on Operating Systems
Design and Implementation (OSDI)., vol. 12, 2012, pp. 107–120.
[25] E. Coppa, C. Demetrescu, and I. Finocchi, “Input-sensitive proﬁling,”

ACM SIGPLAN Notices, vol. 47, no. 6, pp. 89–98, 2012.

[26] D. Shen, Q. Luo, D. Poshyvanyk, and M. Grechanik, “Automating per-
formance bottleneck detection using search-based application proﬁling,”
in Proc. of the ACM International Symposium on Software Testing and
Analysis (ISSTA), 2015, pp. 270–281.

[28] M. Nagappan, K. Wu, and M. A. Vouk, “Efﬁciently extracting opera-
tional proﬁles from execution logs using sufﬁx arrays,” in Proc. of the
20th IEEE International Symposium on Software Reliability Engineering
(ISSRE)., 2009, pp. 41–50.

[29] G. Jin, L. Song, X. Shi, J. Scherpelz, and S. Lu, “Understanding
and detecting real-world performance bugs,” ACM SIGPLAN Notices,
vol. 47, no. 6, pp. 77–88, 2012.

[30] X. Xiao, S. Han, D. Zhang, and T. Xie, “Context-sensitive delta inference
for identifying workload-dependent performance bottlenecks,” in Proc.
of the ACM International Symposium on Software Testing and Analysis
(ISSTA)., 2013, pp. 90–100.

[31] X. Yu, S. Han, D. Zhang, and T. Xie, “Comprehending performance from
real-world execution traces: A device-driver case,” in ACM SIGPLAN
Notices, vol. 49, no. 4, 2014, pp. 193–206.

[32] C. Sauvanaud, K. Lazri, M. Kaˆaniche, and K. Kanoun, “Anomaly
detection and root cause localization in virtual network functions,” in
Proc. of the 27th IEEE International Symposium on Software Reliability
Engineering (ISSRE)., 2016, pp. 196–206.

[33] W. Wen, T. Yu, and J. H. Hayes, “Colua: Automatically predicting
conﬁguration bug reports and extracting conﬁguration options,” in Proc.
of
the 27th IEEE International Symposium on Software Reliability
Engineering (ISSRE)., 2016, pp. 150–161.

[34] A. Nistor, P.-C. Chang, C. Radoi, and S. Lu, “Caramel: detecting and
ﬁxing performance problems that have non-intrusive ﬁxes,” in Proc.
of the 37th IEEE International Conference on Software Engineering
(ICSE)., 2015, pp. 902–912.

[35] M. Grechanik, C. Fu, and Q. Xie, “Automatically ﬁnding performance
problems with feedback-directed learning software testing,” in Proc.
of the 34th IEEE International Conference on Software Engineering
(ICSE)., 2012, pp. 156–166.

[36] X. Liu, J. Zhan, K. Zhan, W. Shi, L. Yuan, D. Meng, and L. Wang,
“Automatic performance debugging of spmd-style parallel programs,”
Journal of Parallel and Distributed Computing, vol. 71, no. 7, pp. 925–
937, 2011.

[37] L. Song and S. Lu, “Statistical debugging for real-world performance
problems,” ACM SIGPLAN Notices, vol. 49, no. 10, pp. 561–578, 2014.

[38] A. E. Chis, “Automatic detection of memory anti-patterns,” in Com-
panion to the 23rd ACM SIGPLAN conference on Object-oriented
programming systems languages and applications, 2008, pp. 925–926.

[39] A. Nistor, L. Song, D. Marinov, and S. Lu, “Toddler: Detecting perfor-
mance problems via similar memory-access patterns,” in Proc. of the
35th IEEE International Conference on Software Engineering (ICSE).,
2013, pp. 562–571.

[40] T.-H. Chen, W. Shang, Z. M. Jiang, A. E. Hassan, M. Nasser, and
P. Flora, “Detecting performance anti-patterns for applications developed
using object-relational mapping,” in Proc. of the 36th ACM International
Conference on Software Engineering (ICSE)., 2014, pp. 1001–1012.

[41] J. Oh, C. J. Hughes, G. Venkataramani, and M. Prvulovic, “Lime: A
framework for debugging load imbalance in multi-threaded execution,”
in Proc. of the 33rd ACM International Conference on Software Engi-
neering (ICSE)., 2011, pp. 201–210.

[42] C. Killian, K. Nagaraj, S. Pervez, R. Braud, J. W. Anderson, and
R. Jhala, “Finding latent performance bugs in systems implementations,”
in Proc. of the 18th ACM International Symposium on Foundations of

Software Engineering (SIGSOFT)., 2010, pp. 17–26.

