7
1
0
2

g
u
A
8
1

]

G
L
.
s
c
[

2
v
5
9
0
0
0
.
6
0
7
1
:
v
i
X
r
a

Using GPI-2 for Distributed Memory Paralleliziation of the Caffe Toolbox to

Speed up Deep Neural Network Training

Martin Kuehn, Janis Keuper and Franz-Josef Pfreundt
Competence Center High Performance Computing
Fraunhofer Institute for Industrial Mathematics
Fraunhofer-Platz 1, D-67663 Kaiserslautern, Germany

August 21, 2017

Abstract

1

Introduction

Deep Neural Network (DNN) are currently of great inter-
est in research and application. The training of these net-
works is a compute intensive and time consuming task. To
reduce training times to a bearable amount at reasonable
cost we extend the popular Caffe toolbox for DNN with
an efﬁcient distributed memory communication pattern.
To achieve good scalability we emphasize the overlap of
computation and communication and prefer ﬁne granu-
lar synchronization patterns over global barriers. To im-
plement these communication patterns we rely on the the
”Global address space Programming Interface” version 2
(GPI-2) communication library. This interface provides a
light-weight set of asynchronous one-sided communica-
tion primitives supplemented by non-blocking ﬁne gran-
ular data synchronization mechanisms. Therefore, Caf-
feGPI is the name of our parallel version of Caffe. First
benchmarks demonstrate better scaling behavior com-
pared with other extensions, e.g., the IntelTMCaffe. Even
within a single symmetric multiprocessing machine with
four graphics processing units, the CaffeGPI scales bet-
ter than the standard Caffe toolbox. These ﬁrst results
demonstrate that the use of standard High Performance
Computing (HPC) hardware is a valid cost saving ap-
I/O is an other bottleneck
proach to train large DDNs.
to work with DDNs in a standard parallel HPC setting,
which we will consider in more detail in a forthcoming
paper.

Deep Neural Network (DNN) architectures have im-
proved considerably the accuracy in data classiﬁcation
opening the door for a plethora of use cases in image clas-
siﬁcation, speech recognition or semantic text understand-
ing. However, the training of DNNs is a very compute
intensive task. So, the raising interest in these architec-
tures created a tremendous demand for compute resources
which is further intensiﬁed by a race to greater sizes of
DNNs.

Another important factor is the time necessary for train-
ing DNNs. To train a popular architecture like, e.g.,
GoogLeNet can easily take several days on a Graphics
Processing Unit (GPU). To make things worse the train-
ing usually is an iterative process of trials and modiﬁca-
tions in the DNN architecture. So, keeping training times
tolerably is a key requirement to actually apply DNNs in
research and industry.

In response to this challenge, hardware vendors
brought to market special hardware, e.g., the DGX-1 sold
by NVIDIA or the S822LC (”Minsky”) sold by IBM.
They try to integrate as much compute power in terms
of ﬂoating point operations per second (FLOP/s) as possi-
ble in a single compute node. While this special hardware
comes also with a special price it is also not as ﬂexible
to apply to other problems in computer science. On the
other hand, there already exists a plethora of compute sys-
tems in the world used for High Performance Computing

 
 
 
 
 
 
Table 1. APPROXIMATE COMPUTATION TIMES FOR ALEXNET
WITH BATCH SIZE B = 256 AND 450k ITERATIONS,
GOOGLENET AND INCEPTION V3 WITH B = 32 AND
1400k,2000k ITERATIONS. SETUP CAFFE WITH CUDA 8 AND
CUDNN 6 FOR GPUS AND INTELTMCAFFE WITH MKL17 FOR
CPUS.

AlexNet [2]:
time per iteration
time till convergence
GoogLeNet [3]:
time per iteration
time till convergence
Inception V3:
time per iteration
time till convergence

CPU K80

P100 KNL

2s
250h

0.9s
112h

0.1s
13h

0.6s
75h

1.3s
361h

0.36s
100h

0.08s
31h

0.32s
89h

-
-

-
-

0.33s
180h

-
-

(HPC) [1]. Usually these consist of hundreds of nodes
usually connected with high bandwidth, low latency net-
works like InﬁniBand networks. A considerable number
of them are even equipped with GPU accelerators.

Our aim is to make these HPC resources available to
the ﬁeld of data analytics. The advantage of this approach
is twofold. First, it provides the data analytics community
access to the needed hardware quickly because it is al-
ready up and running. Secondly, in the long run it avoids
the separation of resources that are used in the ﬁeld of data
analytics and in the traditional HPC ﬁeld. This not only
simpliﬁes the buildup and the operation of these compute
resources but it also increases the ﬂexibility to mix data
analytics and other compute jobs on the same cluster. The
latter increases the load factor and reduces costs.

The toolbox Caffe [4] is very popular to build and train
DNNs. It is easy to use and a wealth of predeﬁned DNNs
are available to get to results quickly. The popular Con-
volutional Neural Networks usually have a performance
advantage on Caffe versus the TensorFlow framework.
However, the parallelization of Caffe, as it is provided
in its original version, is limited to a single Symmetric
Multi Processing (SMP) compute node. IntelTMdeveloped
a MPI based prallel version of Caffe, which we will use
as benchmark. Our goal is to provide a parallel version
of Caffe based on a Partitioned Global Address Space
(PGAS) Application Programming Interface (API), that
can exploit one sided commmunication more efﬁciently

than MPI. This speeds up the DNN training by distribut-
ing the computational load on several servers equipped
with a GPU or a set of powerful Central Processing Units
(CPUs).

To ensure high scalability it is crucial to organize the
inter node communication efﬁciently. To exploit the pre-
cious network bandwidth to the maximum it is important
to overlay as much of the inter node communication with
computation as possible. The Global address space Pro-
gramming Interface version 2 (GPI-2) library [5] devel-
oped by our group provides an efﬁcient interface for one
sided, asynchronous data transfers. This interface is the
basis of an efﬁcient and well scaling distributed memory
parallelization of the Caffe toolbox. So, we call our par-
allel version of Caffe CaffeGPI.

The rest of the text is organized as follows. In Section
2 details on our parallelization approach and communica-
tion pattern are given. In Section 3 the implementation of
the communication pattern is described. In Section 4 our
ﬁrst benchmarks are presented and in Section 5 the results
are discussed.

2 Parallelization Approach

Although the numerical operations involved in the train-
ing of DNNs are typically basic linear algebra operations
on dense matrices, which have a rather good FLOP to byte
ratio, the bandwidth of the communication networks must
be used efﬁciently to keep the latency low between the
training iterations.

2.1 Stochastic Gradient Descend

The Stochastic Gradient Descend (SGD) [6] algorithm is
a standard for training DNN and is implemented in the
Caffe toolbox. It is the standard choice for such famous
DNNs like, e.g., GoogLeNet or AlexNet. The training
data xi is partitioned into batches which are iteratively
applied to the DNN to modify the weights w which are
also called the model. Each iteration consists of a forward
propagation and a backward propagation. In the forward
propagation, the data batch is inferred while during the
backward propagation a gradient on the weights is com-
puted and later on applied on the model.

randomly draw batch M ← B samples from X
Init∆wt = 0
for all x ∈ M do

aggregate update ∆w ← ∂wxj(wt)

Require: (cid:15) > 0

1: for all t = 0 . . . T do
2:
3:
4:
5:
6:
7:
8: end for
9: Return wT

end for
update wt+1 ← wt − (cid:15)∆wt

Figure 1. Mini-Batch SGD with samples X = {x0, . . . , xm},
iterations T , step-size (cid:15), batch size B

The two basic parallelization strategies commonly used
for SGD are data parallelism or model parallelism [7].
In the model parallelism, the net and its weights are dis-
tributed among different ranks of the parallel computer,
while the batch stays the same for all ranks. In data par-
allelism the DNN is duplicated on every rank while the
data batch is split into equal fractions for each rank. The
computed gradients on each rank are aggregated and then
applied to the model which is distributed back to all the
ranks. We concentrate on the data parallelism approach
here which is favorable for DNN containing many con-
volutional layers like the popular AlexNet or GoogLeNet.
An advantage of this approach is a constant aggregated IO
turnover over the ranks fetching the training data.

As the DNN is organized in layers we write the model
of layer l in the iteration k as w(l,k), the partial gradient
on rank r as ∆w(l,k)
. Using this notation the iterative data
parallel SGD algorithm can be shortly noted as

r

wl,k+1 := wl,k − (cid:15)

s
(cid:88)

r=1

∆wl,k
r .

(1)

Higher order terms are neglected here without loss of
generality. These terms usually depend on the history of
the models wl,k, which are inherently broadcasted to all
the ranks to perform the next forward propagation phase.

2.2 Design Principles

To exploit the full potential of this parallelization ap-
proach the data transfers of the model and gradients have

Figure 2. Data Parallel SGD as frequently implemented.

to be designed very carefully, since the total amount of
data that has to be transferred during each iteration grows
with the number of compute ranks.

As a consequence a typical HPC interconnect, like
EDR InﬁniBand, should be used to provide enough band-
width to exploit the scalability of the problem. These net-
works are commonly used to connect the nodes in current
HPC clusters. Another important aspect is to use these
interconnects efﬁciently. Foremost this means to overlap
computation and communication to avoid that their run
times add up fully. Instead both should ideally take place
at the same time so that no additional time for commu-
nication is necessary. Unfortunately this is not the case
for the standard Caffe which enters separate phases for
computation and communication (see Figure 2). The sec-
ond principle is to avoid global synchronization points be-
tween the ranks. Instead data dependencies are enforced
locally and very ﬁne granular. The communicated data
chunks are pipelined to keep the ranks busy most of the
time with either computing, communication or both. Bar-
riers to synchronize the ranks are avoided and used only
where absolutely necessary. However, it is important to
note that our approach strictly regards all the data depen-
dencies immanent of the SGD algorithm unlike the so
called ”asynchronous SGD” algorithms described in lit-
erature [8].

2.3 Overlapping Computation and Com-

munication

During the backward propagation the partial gradients
∆w(l,k)
are computed separately for each layer and in an
r
inverse order. The next read access to the model of a cer-
tain layer is in the forward propagation of the following
time step. So, especially for the layers at the bottom of

the DNN quite some time is available to reduce the partial
gradients and to update the model of that layer (see Figure
4 for illustration).

r

The communication pattern is turn based, one layer of
the DNN per turn. In each turn, a partial gradient ∆w(l,k)
is computed for the respective layer li and forwarded to
the receiving ranks.
Incoming partial gradients of pre-
vious layers from other ranks are checked, aggregated if
available and forwarded to receiving ranks as well. In the
same manner, updates on the model of previous layers are
forwarded to the receiving nodes. In all these cases the
data transfers are only triggered but not awaited for con-
clusion.

r

compute local gradient ∆w(k,l)
check for arrived gradients from previous layers
reduce arrived gradient data locally
trigger sends of available gradient data
trigger sends of arrived model data

1: for all l = L, . . . , 1 do
2:
3:
4:
5:
6:
7: end for
8: ﬁnalize local communication phase

Figure 3. Turn based communication pattern on rank r.

In the ﬁnalization phase, all loose ends of the commu-
nication to the local rank r are ﬁnalized. After that the
local instance of the DNN is ready for the next iteration.
Please note that even between iterations there is no global
barrier applied. So, every rank that received a complete
update of the model can immediately start with the next
training phase without having to wait for other ranks to
get their full update.

Particularly, this scheme allows to overlap the compu-
tation of the gradient ∆w(l,k)
with the communication of
gradients and models of previous layers. And it avoids
global barriers between the ranks.

r

3 Implementation

Our parallel version of the Caffe framework is done as
minimally invasive as possible. Basically, the setup rou-
tine of the DDN and the backward propagation routines
are modiﬁed. Additionally, a layer-wise model update is
introduced in the Solver class.

Figure 4. Sketch of the data parallel SGD communication pattern
implemented in this work.

During the backward propagation over the layers, the
calculations of the gradients are followed by calls to the
newly added communication routines to reduce the local
gradients and to broadcast the updated model.

3.1 Basics of GPI-2

To implement the overlapping, one sided communica-
tion pattern the GPI-2 library has been used, which is a
PGAS communication API for C/C++ and Fortran appli-
cations. Fulﬁlling the Global Address Space Program-
ming Interface (GASPI) speciﬁcation (see webpage [9]),
it provides truly asynchronous one-sided communication
primitives supplemented by a non-blocking light-weight
and ﬁne granular data synchronization mechanism. GPI-2
exploits interconnects supporting Remote Direct Memory
Access (RDMA) as, e.g., InﬁninBand networks. On these
networks the data transfers can be almost completely of-
ﬂoaded to the network infrastructure reducing the load on
the computational resources to a minimum. No intermedi-
ate copies are necessary which saves memory bandwidth.
Apart from that, GPI-2 is a very lean library and gives the
user more direct control over the particular data transfers
as, e.g. the usual Message Passing Interface (MPI) library.

All these features make GPI-2 a perfect match to im-
plement the overlap of computation and inter node com-
munication in Caffe. Being an open source library GPI-2
can be downloaded at [5].

Layer 1Layer 2Layer n-1Layer n...t====forwardLocal backwardDistributed model accumulation ++++global update3.2

Implementation of Data Transfers

3.2.2 Broadcast of the model

In the Caffe data structures that deﬁne the DNN the
arrays that carry the model and the gradient data are
Providing the
placed inside GPI-2 data segments.
gaspi segment use function, GPI-2 cooperates perfectly
with special memory regions in Caffe which are allocated
by cudaMallocHost to enhance data transfers to the GPU.
The GPI-2 library allows to write remotely (inter node)
and directly to these segments. All the data transfers are
triggered with a gaspi write notify call to the library. The
receiver of the data chunk checks on the respective notiﬁ-
cation and acts on the received data if necessary.

The gradient data is reduced in a reduction tree pattern
aggregating the ﬁnal gradient on the master rank. The
master rank performs the update on the current model to
compute an update for the next iteration. Then the up-
dated model is broadcasted to all the other ranks in an-
other tree pattern. The gradient data is always sent from
higher rank numbers to lower rank numbers while the up-
dated model is broadcasted from lower rank numbers to
higher rank numbers. As the size of the gradients equals
the size of the models we take advantage of the full duplex
feature of switched networks.

The reduction and the broadcast trees are build from
scratch to keep control over the tree topolgy and to inter-
wine closely the communication pattern with the compu-
tation.

3.2.1 Reduction of the Gradient

To reduce the local gradients in a binomial, tree each rank
checks for incoming gradient data in its receive buffer. If
available the gradient data is reduced (added to) with its
own gradient data of that layer. If the rank has a receiver
in the tree pattern the reduced gradient is forwarded to this
receiver using a call to gaspi write notify. The communi-
cation tasks are performed once in the loop over the layers
as depicted in section 2.3. The gradient data is processed
as available. No waiting takes place for a speciﬁc data
chunk.

The broadcast of the model is performed similarly as the
gradient reduction. As no reduction steps are necessary
the incoming model data from the sender is just forwarded
to its receiver ranks using a call to gaspi write notify.

4 First Benchmarks

To evaluate our parallelization approach we start to com-
pare CaffeGPI with the original Caffe on a SMP machine
containing 4 GPUs. This setup is quite similar to spe-
cialized workstations produced to train DNNs. The origi-
nal Caffe uses the standard thread parallel communication
pattern in this benchmark. In the CaffeGPI benchmark,
4 independent processes are started on the same node,
one for each GPU, communicating through the network
card. The SMP machine is a single node of the Taurus
cluster at the ZIH in Dresden containing 2 IntelTMXeon
E5-2680v3 CPUs, 64GB Random-access memory (RAM)
and 4 NVIDIA K80 GPU. The network is InﬁniBand
FDR. As DNN we choose the familiar AlexNet. Figure
5 depicts the scaling behavior at 1, 2, and 4 GPU.

Figure 5. Scaling results for AlexNet an overall batch size of 256 on a
single node with multiple GPUs interconnected via PCIe. Based on
Caffe 1.15, Cuda 8, CuDNN 5.1.

On a ﬁrst glance our CaffeGPI should have a small dis-
advantage in this benchmark compared with the standard
Caffe toolbox because it needs to communicate via mem-
ory copies inside the node. At a closer look the reduction

of the partial gradients puts a lot of load on the mem-
ory system, the PCIe bus and the QuickPath Interconnect
between the CPUs.
In both cases, memory copies be-
tween GPU-RAM and CPU-RAM have to be performed.
However in the benchmark of the standard Caffe all the
GPUs execute their communication phase at the same
time leaving precious bandwidth idle during the compu-
tation phase.
In the CaffeGPI benchmark, the memory
copies between GPU-RAM and CPU-RAM are not over-
lapped but interleaved with computation of the partial gra-
dients. As not all the GPUs perform their copies at the
same time, the memory transfers are distributed over a
longer period of time. The memory copies across the
two sockets are performed by the network card and over-
lapped with the computation. Finally our implementation
can demonstrate superior scaling behavior as depicted in
Figure 5. The second benchmark compares CaffeGPI to
IntelTMCaffe (see webpage [10]), a distributed memory
extension of Caffe based on the MPI. In this benchmark,
1, 2 or 4 nodes of the same cluster were used, but only
one GPU per node. Here distributed memory data trans-
fers are performed in both scenarios. The benchmark in
Figure 6 demonstrates a superior scaling behavior of our
implementation in comparison to the IntelTMCaffe frame-
work. A speedup of 2.4 on 4 nodes compared to one node
delivers a reasonable performance ﬁgure to train AlexNet
in a reasonable time frame on standard HPC hardware.

Figure 6. Scaling results for AlexNet with an overall batch size of 256
on distributed nodes with single GPUs interconnected via Inﬁniband.
Based on IntelTMCaffe 1.14, Cuda 8, CuDNN 5.1

5 Conclusion and Future Work

The preliminary benchmarks presented in this work
demonstrate that our distributed memory communication
pattern implemented in the CaffeGPI framework scales
well on four distributed memory nodes equipped with one
GPU per node. The total performance is similar or even
better than using the standard SMP-parallel approach of
Caffe on a SMP node equipped with 4 GPU. Even on this
single SMP node with 4 GPUs, our CaffeGPI scales much
better than the standard Caffe framework.

These results demonstrate that data scientists can rely
on available HPC compute resources to train their DNNs
in a reasonable time frame. Our toolbox CaffeGPI can
help to satisfy the need for more compute power in the
area of data science without having to buy vast amounts
of specialized hardware, which is difﬁcult to apply eco-
nomically for other tasks in computer science.

We will continue to benchmark various hardware con-
ﬁgurations, e.g. a NVIDIA DGX-1 or an IBM S822LC
(”Minsky”). Further benchmarks will be done to analyze
the communication pattern introduced in CaffeGPI. Alter-
native patterns will be evaluated that might improve the
reduction and the broadcast operations. We will also ex-
tend our benchmarks on more DNNs and to wider batch
sizes to evaluate their scaling behavior and to ﬁnd perfor-
mance optimized training parameters.

Acknowledgment

The authors thank the Center for Information Services and
High Performance Computing (ZIH) at TU Dresden for
generous allocations of computer time. We also grate-
fully acknowledge the support of NVIDIA Corporation
with the donation of the Titan X Pascal GPU used for this
research.

References

[1] “TOP 500,” http://www.top500.org, accessed: 2017-

05-15.

[2] A. Krizhevsky, I. Sutskever, and G. E. Hinton, “Im-
agenet classiﬁcation with deep convolutional neural

networks,” in Advances in neural information pro-
cessing systems, 2012, pp. 1097–1105.

[3] C. Szegedy et al., “Going deeper with convolutions,”
in Proceedings of the IEEE Conference on Com-
puter Vision and Pattern Recognition, 2015, pp. 1–9.

[4] Y. Jia et al., “Caffe: Convolutional architec-
ture for fast feature embedding,” arXiv preprint
arXiv:1408.5093, 2014.

[5] “GPI 2.0,” http://www.gpi-site.com/gpi2, accessed:

2017-05-15.

[6] L. Bottou, “Large-scale machine learning with
stochastic gradient descent,” in Proceedings of
COMPSTAT’2010. Springer, 2010, pp. 177–186.

[7] J. Keuper and F.-J. Preundt, “Distributed train-
ing of deep neural networks: Theoretical and
practical
limits of parallel scalability,” in Pro-
ceedings of the Workshop on Machine Learning
in High Performance Computing Environments,
Piscataway, NJ, USA: IEEE
ser. MLHPC ’16.
Press, 2016, pp. 19–26.
[Online]. Available:
https://doi.org/10.1109/MLHPC.2016.6

[8] J. Keuper and F.-J. Pfreundt, “Asynchronous parallel
stochastic gradient descent: A numeric core for
scalable distributed machine learning algorithms,”
in Proceedings of
the Workshop on Machine
Learning in High-Performance Computing Environ-
ments, ser. MLHPC ’15. New York, NY, USA:
ACM, 2015, pp. 1:1–1:11. [Online]. Available:
http://doi.acm.org/10.1145/2834892.2834893

[9] “GASPI,” http://www.gaspi.de, accessed: 2017-05-

15.

[10] “IntelTMcaffe,”

https://github.com/intelcaffe/caffe,

accessed: 2017-05-15.

