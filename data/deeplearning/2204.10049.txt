On Distribution Shift in Learning-based Bug Detectors

Jingxuan He 1 Luca Beurer-Kellner 1 Martin Vechev 1

Abstract

Deep learning has recently achieved initial suc-
cess in program analysis tasks such as bug detec-
tion. Lacking real bugs, most existing works con-
struct training and test data by injecting synthetic
bugs into correct programs. Despite achieving
high test accuracy (e.g., >90%), the resulting bug
detectors are found to be surprisingly unusable in
practice, i.e., <10% precision when used to scan
real software repositories. In this work, we argue
that this massive performance difference is caused
by a distribution shift, i.e., a fundamental mis-
match between the real bug distribution and the
synthetic bug distribution used to train and eval-
uate the detectors. To address this key challenge,
we propose to train a bug detector in two phases,
ﬁrst on a synthetic bug distribution to adapt the
model to the bug detection domain, and then on a
real bug distribution to drive the model towards
the real distribution. During these two phases, we
leverage a multi-task hierarchy, focal loss, and
contrastive learning to further boost performance.
We evaluate our approach extensively on three
widely studied bug types, for which we construct
new datasets carefully designed to capture the real
bug distribution. The results demonstrate that our
approach is practically effective and successfully
mitigates the distribution shift: our learned de-
tectors are highly performant on both our test set
and the latest version of open source repositories.
Our code, datasets, and models are publicly avail-
able at https://github.com/eth-sri/
learning-real-bug-detector.

2
2
0
2

n
u
J

9
1

]

G
L
.
s
c
[

2
v
9
4
0
0
1
.
4
0
2
2
:
v
i
X
r
a

Test set I:
50% synthetic bugs
50% non-buggy

Test set II:
50% real bugs
50% non-buggy

Test set III (real-world):
0.5% real bugs
99.5% non-buggy

CuBERT
Random

GNN
Random

GNN
BugLab

94

91

85

79

75

82

87

72

65

35

36

51

35

36

51

3

1

1

Figure 1: Performance of variable misuse classiﬁers are
drastically reduced from synthetic test sets to a real-world
test set.

are precision and recall, respectively.

and

cess of deep learning-based bug detectors (Vasic et al., 2019;
Hellendoorn et al., 2020; Allamanis et al., 2021; Kanade
et al., 2020; Chen et al., 2021b). These detectors can dis-
cover hard-to-spot bugs such as variable misuses (Allama-
nis et al., 2018) and wrong binary operators (Pradel & Sen,
2018), issues that greatly impair software reliability (Rice
et al., 2017; Karampatsis & Sutton, 2020) and cannot be
handled by traditional formal reasoning techniques.

Lacking real bugs, existing works build training sets by in-
jecting few synthetic bugs, e.g., one (Kanade et al., 2020),
three (Hellendoorn et al., 2020), or ﬁve (Allamanis et al.,
2021), into each correct program. The learned bug detec-
tors then achieve high accuracy on test sets created in the
same way as the training set. However, when scanning real-
world software repositories, these detectors were found to
be highly imprecise and practically ineffective, achieving
only 2% precision (Allamanis et al., 2021) or <10% preci-
sion (He et al., 2021). The key question then is: what is the
root cause for this massive drop in performance?

1. Introduction

The increasing amount of open source programs and ad-
vances in neural code models have stimulated the initial suc-

1Department of Computer Science, ETH Zurich, Switzerland.

Correspondence to: Jingxuan He <jingxuan.he@inf.ethz.ch>.

Proceedings of the 39 th International Conference on Machine
Learning, Baltimore, Maryland, USA, PMLR 162, 2022. Copy-
right 2022 by the author(s).

Unveiling Distribution Shifts We argue that the root
cause is distribution shift (Koh et al., 2021), a fundamental
mismatch between the real bug distribution found in public
code repositories and the synthetic bug distribution used to
train and evaluate existing detectors. Concretely, real bugs
are known to be different from synthetic ones (Yasunaga
& Liang, 2021), and further, correct programs outnumber
buggy ones in practice, e.g., around 1:2000 as reported
in (Karampatsis & Sutton, 2020). This means that the real
bug distribution inherently exhibits extreme data imbalance.

 
 
 
 
 
 
On Distribution Shift in Learning-based Bug Detectors

Figure 1 reproduces the performance drop, showing that
existing detectors indeed fail to capture these two key factors.
As in (Kanade et al., 2020), we ﬁne-tune a classiﬁer based on
CuBERT for variable misuse bugs, using a balanced dataset
with randomly injected synthetic bugs. Non-surprisingly,
the ﬁne-tuned model is close-to-perfect on test set I created
in the same way as the ﬁne-tuning set (top-left of Figure 1).
Then, we replace the synthetic bugs in test set I with real
bugs extracted from GitHub to create test set II (top-mid of
Figure 1). The precision and recall drop by 7% and 56%,
respectively, meaning that the model is signiﬁcantly worse
at ﬁnding real bugs. Next, we evaluate the classiﬁer on
test set III created by adding a large amount of non-buggy
code to test set II so to mimic the real-world data imbalance.
The model achieves a precision of only 3% (top-right of
Figure 1). A similar performance loss occurs with graph
neural networks (GNNs) (Allamanis et al., 2018), trained
on either the dataset for ﬁne-tuning the previous CuBERT
model (mid row of Figure 1) or another balanced dataset
where synthetic bugs are injected by BugLab (Allamanis
et al., 2021), a learned bug selector (bottom row of Figure 1).

This Work: Alleviating Distribution Shifts
In this
work, we aim to alleviate such a distribution shift and learn
bug detectors capturing the real bug distribution. To achieve
this goal, we propose to train bug detectors in two phases:
(1) on a balanced dataset with synthetic bugs, similarly to
existing works, and then (2) on a dataset that captures data
imbalance and contains a small number of real bugs, possi-
ble to be extracted from GitHub commits (Allamanis et al.,
2021) or industry bug archives (Rice et al., 2017). In the ﬁrst
phase, the model deals with a relatively easier and larger
training dataset. It quickly learns relevant features and cap-
tures the synthetic bug distribution. The second training
dataset is signiﬁcantly more difﬁcult to learn from due to
only a small number of positive samples and the extreme
data imbalance. However, with the warm-up from the ﬁrst
training phase, the model can catch new learning signals and
adapt to the real bug distribution. Such a two-phase learning
process resembles the pre-training and ﬁne-tuning scheme
of large language models (Devlin et al., 2019) and self-
supervised learning (Jing & Tian, 2021). The two phases
are indeed both necessary: as we show in Section 5, without
either of the two phases or mixing them into one, the learned
detectors achieve sub-optimal bug detection performance.

To boost performance, our learning framework also lever-
ages additional components such as task hierarchy (Søgaard
& Goldberg, 2016), focal loss (Lin et al., 2017), and a con-
trastive loss term to differentiate buggy/non-buggy pairs.

Datasets, Evaluation, and Effectiveness
In our work,
we construct datasets capturing the real bug distribution.
To the best of our knowledge, these datasets are among the

ones containing the largest number of real bugs (e.g., 1.7x
of PyPIBugs (Allamanis et al., 2021)) and are the ﬁrst to
capture data imbalance, for the bug types we handle. We
use half of these datasets for our second phase of training
and the other two quarters for validation and testing, respec-
tively. Our extensive evaluation shows that our method is
practically effective: it yields highly performant bug detec-
tors that achieve matched precision (or the precision gap
is greatly reduced) on our constructed test set and the lat-
est version of open source repositories. This demonstrates
that our approach successfully mitigates the challenge of
distribution shift and our dataset is suitable for evaluating
practical usefulness of bug detectors.

2. Background

We now deﬁne the bug types our work handles and describe
the state-of-the-art pointer models for detecting them.

Detecting Token-based Bugs We focus on token-based
bugs caused by misuses of one or a few program tokens.
One example is var-misuse where a variable use is wrong
and should be replaced by another variable deﬁned in the
same scope. Formally, we model a program p as a sequence
of n tokens T = (cid:104)t1, t2, . . . , tn(cid:105). Fully handling a speciﬁc
type of token-based bug in p involves three tasks:

• Classiﬁcation: classify if p is buggy or not.

• Localization: if p is buggy, locate the bug.

• Repair: if p is buggy, repair the located bug.

Note that these three tasks form a dependency where the
later task depends on the prior tasks. To complete the three
tasks, we ﬁrst extract Loc ⊆ T , a set of candidate tokens
from T where a bug can be located. If Loc = ∅, p is non-
buggy, Otherwise, we say that p is eligible for bug detection
and proceed with the classiﬁcation task. We assign a value
in {±1} to p, where −1 (resp., 1) means that p is non-buggy
(resp., buggy). If 1 is assigned to p, we continue with the
localization and repair tasks. For localization, we identify a
bug location token tloc ∈ Loc. For repair, we apply simple
rules (see Appendix B) to extract Rep, a set of candidate
tokens that can be used to repair the bug located at tloc and
ﬁnd a token trep ∈ Rep as the ﬁnal repair token. Loc and
Rep are deﬁned based on the speciﬁc type of bug to detect.
For example with var-misuse, Loc is the set of variable
uses in p and Rep is the set of variables deﬁned in the scope
of the wrong variable use. The above deﬁnition is general
and applies to the three popular types of token-based bugs
handled in this work: var-misuse, wrong binary opera-
tor (wrong-binop), and argument swapping (arg-swap).
These bugs were initially studied in (Allamanis et al., 2018;
Pradel & Sen, 2018). We provide examples of those bugs in
Figure 2 and describe them in detail in Appendix B.

On Distribution Shift in Learning-based Bug Detectors

def compute_area(width, height):

return width * width

+ - * /
def compute_area(width, height):

return width + height

def buy_with(account):

return lib.withdraw(120.0, account)

Figure 2: Example bugs handled by our work (left: var-misuse, middle: wrong-binop, right: arg-swap). The bug
, respectively.
location and the repair token are marked as

and

Existing Pointer Models for Token-based Bugs State-
of-the-art networks for handling token-based bugs follow
the design of pointer models (Vasic et al., 2019), which
identify bug locations and repair tokens based on predicted
pointer vectors. Given the program tokens T , pointer models
ﬁrst apply an embedding method φ to convert each token ti
into an m-dimensional feature vector hi ∈ Rm:

πcls

πloc

πrep

Bk

πrep

Bk

Bk−1

Bk−2

Bk−1

πcls

Bk−2

πloc

B1. . . Bk−3

B1. . . Bk−3

[h1, . . . , hn] = φ((cid:104)t1, . . . , tn(cid:105)).

Figure 3: Standard feature sharing (left) v.s. an example of
our task hierarchy (right).

Existing works instantiate φ as GNNs and GREATs (Hel-
lendoorn et al., 2020), LSTMs (Vasic et al., 2019), or
BERT (Kanade et al., 2020). Then, a feedforward network
πloc is applied on the feature vectors to obtain a probability
vector P loc = [ploc
n ] pointing to the bug location.
The repair probabilities P rep = [prep
, . . . , prep
] are com-
puted in a similar way with another feedforward network
πrep. We omit the steps for computing P loc and P rep for
brevity and elaborate on them in Appendix A.

1 , . . . , ploc

1

l

Importantly, in existing pointer models, classiﬁcation is
done jointly with localization. That is, each program has
a special NO BUG location (typically the ﬁrst token). When
the localization result points to NO BUG, the classiﬁcation
result is −1. Otherwise, the localization result points to a
bug location and the classiﬁcation result is 1.

Training Existing Pointer Models For training,
two
masks, C loc and C rep, are required as ground truth labels.
C loc sets the index of the correct bug location as 1 and other
indices to 0. Similarly, C rep sets the indices of the correct
repair tokens as 1 and other indices to 0. The localization
and repair losses are:

Lloc = −

Lrep = −

(cid:88)

(cid:88)

i

i

i × C loc[i],
ploc
prep
i × C rep[i].

The additive loss L = Lloc+Lrep is optimized. In Section 3,
we introduce additional loss terms to L.

3. Learning Distribution-Aware Bug

Detectors

Building on the pointer models discussed in Section 2, we
now present our framework for learning bug detectors capa-
ble of capturing the real bug distribution.

3.1. Network Architecture with Multi-Task Hierarchy

We ﬁrst describe architectural changes to the pointer model.

Adding Classiﬁcation Head In our early experiments,
we found that a drawback of pointer models is mixing the
classiﬁcation and localization results in one pointer vector.
As a result, the model can be confused by the two tasks. We
propose to perform the two tasks individually by adding a
binary classiﬁcation head: We treat the ﬁrst token t1 as the
classiﬁcation token t[cls] and apply a feedforward network
πcls : Rm → R2 over its feature vector h[cls] to compute
the classiﬁcation probabilities pcls

−1 and pcls
1 :

[pcls

−1, pcls

1 ] = softmax(πcls(h[cls])).

Task Hierarchy To exploit the inter-dependence of the
cls, loc, and rep tasks, we formulate a task hierarchy for
the pointer model. This allows the corresponding compo-
nents to reinforce each other and improve overall perfor-
mance. Task hierarchies are a popular multi-task learning
technique (Zhang & Yang, 2021) and are effective in nat-
ural language (Søgaard & Goldberg, 2016) and computer
vision (Guo et al., 2018). To the best of our knowledge, this
work is the ﬁrst to apply a task hierarchy on code tasks.

Using a task hierarchy, we process each task one by one
following a speciﬁc order instead of addressing all tasks
in the same layer. Formally, to encode a task hierarchy,
we consider the feature embedding function φ to consist
of k feature transformation layers B1, . . . , Bk, which is a
standard design of existing pointer models (Hellendoorn
et al., 2020; Allamanis et al., 2021; Kanade et al., 2020):

φ(T ) = (Bk ◦ Bk−1 ◦ . . . ◦ B1)(T ).

We order our tasks cls, loc and rep, and apply their feed-
forward networks separately on the last three feature trans-

On Distribution Shift in Learning-based Bug Detectors

formation layers. Figure 3 shows a task hierarchy with the
order [loc, cls, rep] and compares it with feature sharing.

The ﬁnal loss in the ﬁrst training phase is the addition of
four loss terms: Lcls, Lloc, Lrep, and βLcontrastive, where
β is the tunable weight of the contrastive loss.

3.2. Imbalanced Classiﬁcation with Focal Loss

To handle the extreme data imbalance in practical bug classi-
ﬁcation, we leverage the focal loss from imbalanced learning
in computer vision and natural language processing (Lin
et al., 2017; Li et al., 2020). Focal loss is deﬁned as:

Lcls = FL([pcls

−1, pcls

1 ], y) = −(1 − pcls

y )γ log(pcls

y ),

where y is the ground truth label. Compared with the stan-
dard cross entropy loss, focal loss adds an adjusting factor
y )γ serving as an importance weight for the current
(1 − pcls
sample. When the model has high conﬁdence with large
pcls
y , the adjusting factor becomes exponentially small. This
helps the model put less attention on the large volume of
easy, negative samples and focus on hard samples which the
model is unsure about. γ is a tunable parameter that we set
to 2 according to (Lin et al., 2017).

3.3. Two-phase Training

Bug detectors are ideally trained on a dataset containing a
large number of real bugs as encountered in practice. How-
ever, since bugs are scarce, the ideal dataset does not exist
and is hard to obtain with either manual or automatic ap-
proaches (He et al., 2021). Only a small number of real
bugs can be extracted from GitHub commits (Allamanis
et al., 2021; Karampatsis & Sutton, 2020) or industry bug
archives (Rice et al., 2017), which are not sufﬁcient for data
intensive deep models. Our two-phase training overcomes
this dilemma by utilizing both synthetic and real bugs.

Phase 1: Training with large amounts of synthetic bugs
In the ﬁrst phase, we train the model using a dataset con-
taining a large number of synthetic bugs and their correct
version. Even though learning from such a dataset does not
yield a ﬁnal model that captures the real bug distribution, it
drives the model to learn relevant features for bug detection
and paves the way for the second phase. We create this
dataset following existing work (Kanade et al., 2020): we
obtain a large set of open source programs P , inject syn-
thetic bugs into each program p ∈ P , and create a buggy
program p(cid:48), which results in an 1 : 1 balanced dataset.

Since p and p(cid:48) only differ in one or a few tokens, the model
sometimes struggles to distinguish between the two, which
impairs classiﬁcation performance. We alleviate this issue
by forcing the model to produce distant classiﬁcation feature
embeddings h[cls] for p and h(cid:48)
[cls] for p(cid:48). This is achieved
by a contrastive loss term measuring the cosine similarity
of h[cls] and h(cid:48)

[cls]:

Lcontrastive = cos(h[cls], h(cid:48)

[cls]).

Phase 2: Training with real bugs and data imbalance
The second training phase provides additional supervision
to drive the model trained in phase 1 to the real bug dis-
tribution. To achieve this, we leverage a training dataset
that mimics the real bug distribution. The dataset contains
a small number of real bugs (typically hundreds) extracted
from GitHub commits, which helps the model to adapt
from synthetic bugs to real ones. Moreover, we add a large
amount of non-buggy samples (e.g., hundreds of thousands)
to mimic the real data imbalance. For more details on how
we construct this dataset, please see Section 4. The loss for
the second training phase is the sum of the three task losses
Lcls, Lloc, and Lrep.

4. Implementation and Dataset Construction

In this section, we discuss the implementation of our learn-
ing framework and our dataset construction procedure.

Leveraging CuBERT We implement our framework
on top of CuBERT (Kanade et al., 2020), a BERT-like
model (Devlin et al., 2019) pretrained on source code. Still,
our framework is general and can be applied to any existing
pointer models (as we show in Section 5.1, our two-phase
training brings improvement to the GNN model in (Alla-
manis et al., 2021)). We chose CuBERT mainly because,
according to (Chen et al., 2021b), it achieves top-level per-
formance on many programming tasks, including detecting
synthetic var-misuse and wrong-binop bugs. Moreover,
the reproducibility package provided by the CuBERT au-
thors is of high quality and easy to extend. We present
implementation details in Appendix B.

Dataset Construction We focus on Python code. Fig-
ure 4 shows our dataset construction process. Careful dedu-
plication was applied throughout the entire process (Allama-
nis, 2019). After construction, we obtain a balanced dataset
with synthetic bugs, called syn-train, used for the ﬁrst
training phase. Moreover, we obtain an imbalanced dataset
with real bugs, which is randomly split into real-train
(used for the second training phase), real-val (used as
the validation set), and real-test (used as the blind test
set). The split ratio is 0.5:0.25:0.25. Instead of splitting
by ﬁles, we split the dataset by repositories. This prevents
distributing ﬁles from the same repositories into different
splits and requires generalization across codebases (Koh
et al., 2021). Note that we do not evaluate on synthetic bugs
as it does not reﬂect the practical usage. The statistics of the
constructed datasets are given in Table 1.

On Distribution Shift in Learning-based Bug Detectors

Table 1: The statistics of our constructed dataset.

Bug Type

repo

buggy

non-buggy

repo

buggy

non-buggy

repo

buggy

non-buggy

repo

buggy

non-buggy

syn-train

real-train

real-val

real-test

var-misuse
wrong-binop
arg-swap

2, 654
4, 944
2, 009

147, 409
150, 825
157, 530

147, 409
150, 825
157, 530

339
368
372

626
872
469

118, 888
73, 015
82, 442

169
184
186

347
356
218

63, 703
20, 341
40, 305

170
185
185

336
491
246

61, 539
41, 303
48, 473

Real bugs
found?

No

Yes

Open source
repositories

Inject
bugs

syn-train

real-train

real-val
real-test

Figure 4: Our data construction process. For the precise
sizes the datasets, refer to Table 1.

In Figure 4, we start the construction with a set of open
source repositories (ETH Py150 Open (eth, 2022; Raychev
et al., 2016) for var-misuse and wrong-binop, and on
top of that 894 additional repositories for arg-swap to
collect enough real bugs). We go over the commit history of
the repositories and extract real bugs that align with the bug-
inducing rewrite rules of (Allamanis et al., 2021) applied to
both versions of a changed ﬁle. The repositories are then
split into two sets depending on whether any real bug is
found. To construct syn-train, we extract ∼150k eligible
functions as non-buggy samples for each bug type from the
repositories in which no real bugs were found (
). Then,
we inject bugs into
to create synthetic buggy samples
(
). The bugs are selected uniformly at random among all
bug candidates. We leave the use of more advanced learning
methods for bug selection (Patra & Pradel, 2021; Yasunaga
& Liang, 2021) as future work. To construct real-train,
we combine the found real bugs (
) and other eligible
functions from the same repositories, which serve as non-
buggy samples (
). Since the number of eligible functions
is signiﬁcantly larger than the number of real bugs, real
data imbalance is preserved. Finally, we perform random
splitting to obtain real-datasets as discussed before.

5. Experimental Evaluation

In this section, we present an extensive evaluation of our
framework. We ﬁrst describe our experimental setup.

Training and Model We perform training per bug type
because the three bug types have different characteristics.
We tried a number of training conﬁgurations and found
that our full method with all the techniques described in
Section 3 performed the best on the validation set. For

var-misuse, wrong-binop, and arg-swap, the best or-
ders for the task hierarchy are [cls, loc, rep], [rep, loc, cls],
and [loc, cls, rep], respectively. The best β for the con-
trastive loss are 0.5, 4, and 0.5, respectively. We provide
training and model details in Appendix B.

Testing and Metrics We perform testing on the
real-test dataset. For space reasons, we only discuss the
testing results for var-misuse and wrong-binop in this
section. We show the results for arg-swap in Appendix C.

Instead of accuracy (Allamanis et al., 2021; Hellendoorn
et al., 2020; Vasic et al., 2019), we use precision and recall
that are known to be better suited for data imbalance set-
tings (wik, 2022; Saito & Rehmsmeier, 2015). They are
computed per evaluation target tgt as follows:

Ptgt =

tptgt
tptgt + f ptgt ,

Rtgt =

tptgt
#buggy

,

where #buggy is the number of samples labeled as buggy.
When tptgt + f ptgt = 0, we assign Ptgt = 0. We consider
three evaluation targets cls, cls-loc, and cls-loc-rep:

• cls: binary classiﬁcation. tpcls means that the classiﬁ-
cation prediction and the ground truth are both buggy. A
sample is an f pcls when the classiﬁcation result is buggy
but the ground truth is non-buggy.

• cls-loc: joint classiﬁcation and localization. A sample
is a tpcls-loc when it is a tpcls and the localization token
is correctly predicted. A sample is an f pcls-loc when it
is a f pcls or a tpcls with wrong localization result.

• cls-loc-rep: joint classiﬁcation, localization, and re-
pair. A sample is a tpcls-loc-rep when it is a tpcls-loc
and the repair token is correctly predicted. A sample is an
f pcls-loc-rep when it is a f pcls-loc or a tpcls-loc with
wrong repair result.

The dependency between classiﬁcation, localization, and
repair determines how a bug detector is used in practice.
That is, the users will look at the localization result only
when the classiﬁcation result is buggy. And the repair result
is worth checking only when the classiﬁcation returns buggy
and the localization result is correct. Our metrics conform
to this dependency by ensuring that the performance of the
later target is bounded by the previous target.

During model comparison, we ﬁrst compare precision and

On Distribution Shift in Learning-based Bug Detectors

Table 2: Changing training phases (var-misuse).

Table 3: Changing training phases (wrong-binop).

cls

cls-loc

cls-loc-rep

cls

cls-loc

cls-loc-rep

Method

Only Synthetic
Only Real
Mix
Two Synthetic

P

3.43
0
5.66
35.59

R

P

R

P

35.42
0
24.11
6.25

2.45
0
4.61
32.20

25.30
0
19.64
5.65

2.10
0
4.19
30.51

Our Full Method

64.79

13.69

61.97

13.10

56.34

R

21.73
0
17.86
5.36

11.90

Method

Only Synthetic
Only Real
Mix
Two Synthetic

P

9.69
47.74
12.97
26.85

R

P

R

P

49.08
25.87
41.55
5.91

8.93
45.49
12.02
25.00

45.21
24.64
38.49
5.50

8.09
42.11
10.93
21.30

Our Full Method

52.30

43.99

51.09

42.97

49.64

R

40.94
22.81
35.03
4.68

41.75

100

s
l
c
P

80

60

40

20

0

Our Full Method (AP = 20.96)
Mix (AP = 7.27)
Only Synthetic (AP = 5.24)

100

s
l
c
P

80

60

40

20

0

Our Full Method (AP = 39.85)
Mix (AP = 11.14)
Only Synthetic (AP = 9.67)

100

e
c
n
a
m
r
o
f
r
e
P

75

50

25

0

0

20

40

60

80

100

0

20

40

60

80

100

Rcls
(a) var-misuse

Rcls
(b) wrong-binop

AP
Rcls-loc

Pcls
Pcls-loc-rep

Rcls
Rcls-loc-rep

Pcls-loc

100

e
c
n
a
m
r
o
f
r
e
P

75

50

25

0

20 21 22 23 24 25 26

190

Non-buggy/buggy ratio
(a) var-misuse

20 21 22 23 24 25 26 84
Non-buggy/buggy ratio
(b) wrong-binop

Figure 5: The effectiveness of our two-phase training
demonstrated by precision-recall curves and AP.

Figure 6: Model performance with various non-
buggy/buggy ratios in the second training phase.

AP

Pcls

Rcls

Pcls-loc

Rcls-loc

Pcls-loc-rep

Rcls-loc-rep

100

e
c
n
a
m
r
o
f
r
e
P

75

50

25

0

0

21 22 23 24 25 26100

Percentage
(a) synthetic-train
var-misuse

100

e
c
n
a
m
r
o
f
r
e
P

75

50

25

0

0

21 22 23 24 25 26100

Percentage
(d) real-train
var-misuse

100

e
c
n
a
m
r
o
f
r
e
P

75

50

25

0

0

21 22 23 24 25 26100

Percentage
(b) synthetic-train
wrong-binop

100

e
c
n
a
m
r
o
f
r
e
P

75

50

25

0

0

21 22 23 24 25 26100

Percentage
(e) real-train
wrong-binop

Figure 7: Model performance with subsampled syn-train or real-train.

recall without manual tuning of the thresholds to purely
assess learnability. We prefer higher precision when the
recall is comparable. This is because high precision reduces
the burden of manual inspection to rule out false positives.
When it is necessary to compare the full bug detection abil-
ity, we plot precision-recall curves by varying classiﬁcation
thresholds and compare average precision (AP).

5.1. Evaluation Results on Two-phase Training

We present the evaluation results on our two-phase training.

Changing Training Phases We create four baselines by
only changing the training phases: (i) Only Synthetic: train-
ing only on syn-train; (ii) Only Real:
training only
on real-train; (iii) Mix: combining syn-train and
real-train into a single training phase; (iv) Two Syn-

thetic: two-phase training, ﬁrst on syn-train and then on
a new training set constructed by replacing the real bugs
in real-train with synthetic ones while maintaining the
imbalance. They are compared with Our Full Method in
Tables 2 and 3. We make the following observations:

• Unable to capture data imbalance, Only Synthetic is ex-
tremely imprecise (i.e., <10% precision), matching the
results from previous works (Allamanis et al., 2021; He
et al., 2021). Such imprecise detectors will ﬂood users
with false positives and are practically useless.

• For var-misuse, Only Real classiﬁes all test samples
as non-buggy. For wrong-binop, Only Real has signiﬁ-
cantly lower recall than Our Full Method. These results
show that our ﬁrst training phase can make the learning
stable or improve model performance.

On Distribution Shift in Learning-based Bug Detectors

Table 4: Applying our two-phase training on GNN and
BugLab (Allamanis et al., 2021) (var-misuse).

Table 5: Applying our two-phase training on GNN and
BugLab (Allamanis et al., 2021) (wrong-binop).

cls

cls-loc

cls-loc-rep

cls

cls-loc

cls-loc-rep

Model

Training Phases

GNN
GNN
GNN
GNN

Only Synthetic
Synthetic + Real
Only BugLab
BugLab + Real

P

1.31
57.14
1.16
66.67

R

P

R P

36.31
1.19
50.60
0.60

0.57
57.14
0.35
66.67

15.77
1.19
15.48
0.60

0.41
42.86
0.22
66.67

Our Model

Synthetic + Real

64.79

13.69

61.97

13.10

56.34

R

11.31
0.89
9.82
0.60

11.90

Model

Training Phases

GNN
GNN
GNN
GNN

Only Synthetic
Synthetic + Real
Only BugLab
BugLab + Real

P

5.59
44.62
3.10
51.80

R

P

R P

42.57
11.81
55.60
32.18

4.79
43.85
2.08
51.15

36.46
11.61
37.27
31.77

3.64
43.85
1.47
50.82

Our Model

Synthetic + Real

52.30

43.99

51.09

42.97

49.64

R

27.70
11.61
26.48
31.57

41.75

100

s
l
c
P

80

60

40

20

0

Our Model (Synthetic + Real) (AP = 20.96)
GNN (Synthetic + Real) (AP = 4.28)
GNN (Only Synthetic) (AP = 2.38)

100

GNN (BugLab + Real) (AP = 3.84)
GNN (Only BugLab) (AP = 1.60)

100

s
l
c
P

80

60

40

20

0

s
l
c
P

80

60

40

20

0

Our Model (Synthetic + Real) (AP = 39.85)
GNN (Synthetic + Real) (AP = 25.87)
GNN (Only Synthetic) (AP = 5.35)

100

GNN (BugLab + Real) (AP = 29.99)
GNN (Only BugLab) (AP = 3.07)

s
l
c
P

80

60

40

20

0

0

20 40 60 80 100

0

20 40 60 80 100

0

20 40 60 80 100

0

20 40 60 80 100

Rcls
(a) var-misuse (Synthetic)

Rcls
(d) var-misuse (BugLab)

Rcls
(b) wrong-binop (Synthetic)

Rcls
(e) wrong-binop (BugLab)

Figure 8: Precision-recall curves and AP for GNN and Our Model with different training phases.

• Mix does not perform well even if it is trained on real
bugs. This is because, in the mixed dataset, synthetic
bugs outnumber real bugs. As a result, the model does
not receive enough learning signals from real bugs.

• By capturing data imbalance, Two Synthetic is more pre-
cise than Only Synthetic but sacriﬁces recall. Moreover,
Our Full Method reaches signiﬁcantly higher precision
and recall than Two Synthetic, showing the importance of
real bugs in training.

A precision-recall trade-off exists between Only Synthetic,
Mix, and Our Full Method. To fully compare their bug
classiﬁcation capability, we plot their precision-recall curves
and AP in Figure 5. The results show that Our Full Method
signiﬁcantly outperforms Only Synthetic and Mix with 3-4x
higher AP. This means that our two-phase training helps the
model generalize better to the real bug distribution.

Varying Data Imbalance Ratio To show how the data
imbalance in the second training phase helps the model
training, we vary the number of non-buggy training samples
and keep the buggy training samples the same, resulting
in different non-buggy/buggy ratios (20-6 and the original
ratio). The results are plotted in Figure 6, showing that the
non-buggy/buggy ratio affects the precision-recall trade-off.
Moreover, AP increases with data imbalance ratio. From
1:1 to the original ratio, AP increases from 7.40 to 20.96 for
var-misuse and from 19.84 to 39.85 for wrong-binop.

Varying Amount of Training Data We also vary the size
of our training sets syn-train and real-train. In each
experiment, we subsample one training set (with percentage

0, 2, 4, 8, 16, 32, and 64) and fully use the other train-
ing set. The subsampling is done by repositories. The
results are plotted in Figure 7. A general observation is that
more training data, in either syn-train or real-train,
improves AP. More data in syn-train increases Rcls.
For var-misuse, the model starts to classify samples as
buggy only when given a sufﬁcient amount of data from
syn-train. For wrong-binop, the amount of data in
syn-train does not affect the precision. For real-train,
we can make consistent observations across var-misuse
and wrong-binop: ﬁrst, more data in real-train im-
proves Pcls; second, as the amount of data in real-train
increases from 0, the Rcls ﬁrst decreases but then starts
increasing from around 16%-32%.

Applying Two-phase Training to Existing Methods
Next, we demonstrate that our two-phase training method
can beneﬁt other methods. We consider BugLab, a learned
bug selector for injecting synthetic bugs, and its GNN im-
plementation (Allamanis et al., 2021). We train four GNN
models with different training phases:

• Only Synthetic: train only on syn-train.

• Synthetic + Real: two-phase training same as ours, ﬁrst

on syn-train and then on real-train.

• Only BugLab: train only on a balanced dataset where

bugs are created by BugLab.

• BugLab + Real: two-phase training, ﬁrst on a balanced
dataset created by BugLab and then on real-train.

In Tables 4 and 5, we show the results of the trained variants

On Distribution Shift in Learning-based Bug Detectors

Table 6: Evaluating other techniques (var-misuse).

Table 7: Evaluating other techniques (wrong-binop).

cls

cls-loc

cls-loc-rep

cls

cls-loc

cls-loc-rep

Method

No cls Head
No Hierarchy
No Focal Loss
No Contrastive

P

49.45
51.82
64.18
60.00

R

P

R

P

13.39
16.96
12.80
14.29

48.35
50.91
61.19
57.50

13.10
16.67
12.20
13.69

46.15
48.18
58.21
53.75

Our Full Method

64.79

13.69

61.97

13.10

56.34

R

12.50
15.77
11.61
12.80

11.90

Method

No cls Head
No Hierarchy
No Focal Loss
No Contrastive

P

48.20
48.70
49.32
47.96

R

P

R

P

43.58
45.62
44.60
43.18

47.30
47.83
48.42
47.06

42.77
44.81
43.79
42.36

45.95
46.52
47.97
46.15

Our Full Method

52.30

43.99

51.09

42.97

49.64

R

41.55
43.58
43.38
41.55

41.75

together with Our Model (Synthetic + Real) which corre-
sponds to Our Full Method. Comparing models trained with
Synthetic + Real, we can see that Our Model clearly outper-
forms GNN with signiﬁcantly higher precision and recall.
This is likely because Our Model starts from a CuBERT
model pretrained on a large corpus of code. Moreover, com-
pared with Only Synthetic, GNN trained with Synthetic +
Real achieves signiﬁcantly higher precision. The same phe-
nomenon also applies when the ﬁrst training phase is done
with BugLab. We provide precision-recall curves and AP
of the trained variants in Figure 8, showing that our second
training phase can help GNN, trained with either Only Syn-
thetic or Only BugLab, achieve higher AP, especially for
wrong-binop bugs.

5.2. Evaluation Results on Other Techniques

We show the effectiveness of our other techniques with
four baselines listed as follows. Each baseline excludes one
technique as described below and keeps the other techniques
the same as Our Full Method:

• No cls Head: no classiﬁcation head. Classiﬁcation and
localization are done jointly like existing pointer models.

• No Hierarchy: no task hierarchy. All tasks are performed

after the last feature transformation layer.

• No Focal Loss: use cross entropy loss for classiﬁcation.

• No Contrastive: no contrastive learning with β = 0.

The above baselines have similar-level recall but noticeably
lower precision than Our Full Method. This means all the
evaluated techniques contribute to the high performance of
Our Full Method. The classiﬁcation head and task hierarchy
play a major role for var-misuse. We provide results with
different task orders and β values in Appendix C.

5.3. Scanning Latest Open Source Repositories

In an even more practical setting, we evaluate our method on
the task of scanning the latest version of open source reposi-
tories. To achieve this, we obtain 1118 (resp., 2339) GitHub
repositories for var-misuse and wrong-binop (resp.,
arg-swap). Those repositories do not overlap with the ones
used to construct syn-train and real-train. We apply

Table 8: Manual inspection result on the reported warnings.

Bug Type

Bugs

Quality Issues

False Positives

var-misuse
wrong-binop
wrong-binop-filter
arg-swap

50
6
37
17

10
80
11
3

40
14
52
80

our full method on all eligible functions in the repositories,
without any sample ﬁltering or threshold tuning, and dedu-
plicate the reported warnings together with the extracted
real bugs. This results in 427 warnings for var-misuse,
2102 for wrong-binop, and 203 for arg-swap.

For each bug type, we manually investigate 100 randomly
sampled warnings and, following (Pradel & Sen, 2018),
categorize them into (i) Bugs: warnings that cause wrong
program behaviors, errors, or crashes; (ii) Code Quality
Issues: warnings that are not bugs but impair code quality
(e.g., unused variables), or do not conform to Python coding
conventions, and therefore should be raised and ﬁxed; (iii)
False Positives: the rest. To reduce human bias, two authors
independently assessed the warnings and discussed differing
opinions to reach an agreement. We show the inspection
statistics in Table 8 and present case studies in Appendix D.
Moreover, we report a number of bugs to the developers and
the links to the bug reports are provided in Appendix E.

For var-misuse, most code quality issues are unused vari-
ables. For wrong-binop, most warnings are related to ==,
!=, is, or is not. Our detector ﬂags the use of == and
!= for comparing with None, and the use of is (resp., is
not) for equality (resp., inequality) check with primitive
types. Those behaviors, categorized by us as code quality
issues, do not conform to Python coding conventions and
even cause bugs in rare cases (sof, 2022). Our model learns
to detect them because such samples exist as real bugs in
real-train. Depending on the demand on code quality,
users might want to turn off such warnings. We simulate
this case by ﬁltering out those behaviors and inspect another
100 random warnings from the 255 warnings after ﬁlter-
ing. The results are shown in row wrong-binop-filter
of Table 8. The bug ratio becomes signiﬁcantly higher
than the original version. For arg-swap, our model mostly
detects bugs with Python standard library functions, such

On Distribution Shift in Learning-based Bug Detectors

as isinstance and super, or APIs of popular libraries
such as TensorFlow. Most false positives are reported on
repository-speciﬁc functions not seen during training.

The inspection results demonstrate that our detectors are per-
formant and useful in practice. Counting both bugs and code
quality issues as true positives, the precision either matches
the evaluation results with real-test (var-misuse and
wrong-binop) or the performance gap discussed in Sec-
tion 1 is greatly reduced (arg-swap). This demonstrates
that our method is able to handle the real bug distribution
and our dataset can be reliably used for measuring the prac-
tical effectiveness of bug detectors.

6. Related Work

We now discuss works most closely related to ours.

Machine Learning for Bug Detection GNNs (Alla-
manis et al., 2018), LSTMs (Vasic et al., 2019), and
GREAT (Hellendoorn et al., 2020) are used to detect
var-misuse bugs. Deepbugs (Pradel & Sen, 2018)
learns classiﬁers based on code embeddings to detect
wrong-binop, arg-swap, and incorrect operands bugs.
Hoppity (Dinella et al., 2020) learns to perform graph trans-
formations representing small code edits. A number of mod-
els are proposed to handle multiple coding tasks including
bug detection. This includes PLUR (Chen et al., 2021b), a
uniﬁed graph-based framework for code understanding, and
pre-trained code models such as CuBERT (Kanade et al.,
2020) and CodeBert (Feng et al., 2020).

The above works mainly use datasets with synthetic bugs to
train and evaluate the learned detectors. Some spend efforts
on evaluation with real bugs but none of them completely
capture the real bug distribution: the authors of (Vasic et al.,
2019) and (Hellendoorn et al., 2020) evaluate their models
on a small set of paired code changes from GitHub (i.e.,
buggy/non-buggy ratio is 1:1). The PyPIBugs (Allamanis
et al., 2021) and ManySStuBs4J (Karampatsis & Sutton,
2020) datasets use real bugs from GitHub commits but do
not contain non-buggy samples. Hoppity (Dinella et al.,
2020) is trained and evaluated on small code edits in GitHub
commits, which are not necessarily bugs and can be refactor-
ing, version changes, and other code changes (Berabi et al.,
2021). Compared with the above datasets, our datasets with
real bugs are the closest to the real bug distribution so far.

Other works focus on complex bugs such as security vul-
nerabilities (Li et al., 2018; Zhou et al., 2019; Chen et al.,
2022). We believe that the characteristics of bugs discussed
in our work are general and extensible to complex bugs.

Distribution Shift in Bug Detection and Repair A few
works try to create realistic bugs for training bug detectors

or ﬁxers. BugLab (Allamanis et al., 2021) jointly learns a
bug selector with the detector to create bugs for training.
Since no real bugs are involved in the training process, it is
unclear if the learned selector actually constructs realistic
bugs. Based on code embeddings, SemSeed (Patra & Pradel,
2021) learns manually deﬁned bug patterns from real bugs,
to create new, realistic bugs, which can be used to train bug
detectors. Unlike SemSeed, our bug detectors learn directly
from real bugs which avoids one level of information loss.
BIFI (Yasunaga & Liang, 2021) jointly learns a breaker for
injecting errors to code and a ﬁxer for ﬁxing errors. Focus-
ing on ﬁxing parsing and compilation errors, BIFI assumes
a perfect external error classiﬁer (e.g., AST parsers and
compilers), while our work learns a classiﬁer for software
bugs. Namer (He et al., 2021) proposes a similar two-step
learning recipe for ﬁnding naming issues. Different from
our work, Namer relies on manually deﬁned patterns and
does not beneﬁt from training with synthetic bugs.

Neural Models of Code Apart from bug detection, neural
models are adopted for a number of other code tasks includ-
ing method name suggestion (Alon et al., 2019; Allamanis
et al., 2016; Z¨ugner et al., 2021), type inference (Wei et al.,
2020; Allamanis et al., 2020), code editing (Brody et al.,
2020; Yin et al., 2019), and program synthesis (Alon et al.,
2020; Brockschmidt et al., 2019; Mukherjee et al., 2021).
More recently, large language models are used to generate
real-world code (Austin et al., 2021; Chen et al., 2021a).

Code Rewriting for Data Augmentation Semantics-
preserving code rewriting is used for producing programs,
e.g., for adversarial training of type inference models (Bielik
& Vechev, 2020) and contrastive learning of code clone de-
tectors (Jain et al., 2021). The rewritten and the original
programs are considered to be similar. In the setting of bug
detection, however, the programs created by bug-injection
rules and the original ones should be considered by the
model to be distinct (Patra & Pradel, 2021; Allamanis et al.,
2021), which is captured by our contrastive loss.

7. Conclusion

In this work, we revealed a fundamental mismatch between
the real bug distribution and the synthetic bug distribution
used to train and evaluate existing learning-based bug de-
tectors. To mitigate this distribution shift, we proposed a
two-phase learning method combined with a task hierarchy,
focal loss, and contrastive learning. Our evaluation demon-
strates that the method yields bug detectors able to capture
the real bug distribution. We believe that our work is an
important step towards understanding the complex nature of
bug detection and learning practically useful bug detectors.

On Distribution Shift in Learning-based Bug Detectors

References

ETH Py150 Open Corpus,

2022.

URL

https://github.com/google-research-
datasets/eth_py150_open.

What

is the difference between ”is None” and ”==
None”, 2022. URL https://stackoverflow.
com/questions/3257919/what-is-the-
difference-between-is-none-and-none.

Wikipedia - Precision and Recall for Imbalanced Data,
2022. URL https://en.wikipedia.org/wiki/
Precision_and_recall#Imbalanced_data.

Allamanis, M. The adverse effects of code duplication
in machine learning models of code. In Masuhara, H.
and Petricek, T. (eds.), Onward!, 2019. URL https:
//doi.org/10.1145/3359591.3359735.

Allamanis, M., Peng, H., and Sutton, C. A convolutional
attention network for extreme summarization of source
code. In ICML, 2016. URL http://proceedings.
mlr.press/v48/allamanis16.html.

Allamanis, M., Brockschmidt, M., and Khademi, M.
Learning to represent programs with graphs. In ICLR,
2018. URL https://openreview.net/forum?
id=BJOFETxR-.

Allamanis, M., Barr, E. T., Ducousso, S., and Gao, Z. Typ-
ilus: neural type hints. In PLDI, 2020. URL https:
//doi.org/10.1145/3385412.3385997.

Allamanis, M., Jackson-Flux, H., and Brockschmidt,
M.
In
Self-supervised bug detection and repair.
NeurIPS, 2021. URL https://arxiv.org/abs/
2105.12787.

Alon, U., Zilberstein, M., Levy, O., and Yahav, E. code2vec:
learning distributed representations of code. Proc. ACM
Program. Lang., 3(POPL):40:1–40:29, 2019. URL
https://doi.org/10.1145/3290353.

Alon, U., Sadaka, R., Levy, O.,

language models of code.

and Yahav, E.
In ICML,
Structural
Proceedings of Machine Learning Research, 2020.
URL http://proceedings.mlr.press/v119/
alon20a.html.

Austin, J., Odena, A., Nye, M., Bosma, M., Michalewski,
H., Dohan, D., Jiang, E., Cai, C. J., Terry, M., Le, Q. V.,
and Sutton, C. Program synthesis with large language
models. CoRR, abs/2108.07732, 2021. URL https:
//arxiv.org/abs/2108.07732.

Bielik, P. and Vechev, M. Adversarial robustness for code.
In ICML, 2020. URL http://proceedings.mlr.
press/v119/bielik20a.html.

Brockschmidt, M., Allamanis, M., Gaunt, A. L., and Polo-
zov, O. Generative code modeling with graphs. In ICLR,
2019. URL https://openreview.net/forum?
id=Bke4KsA5FX.

Brody, S., Alon, U., and Yahav, E. A structural model for
contextual code changes. Proc. ACM Program. Lang., 4
(OOPSLA):215:1–215:28, 2020. URL https://doi.
org/10.1145/3428283.

Chen, M., Tworek, J., Jun, H., Yuan, Q., de Oliveira Pinto,
H. P., Kaplan, J., Edwards, H., Burda, Y., Joseph, N.,
Brockman, G., Ray, A., Puri, R., Krueger, G., Petrov,
M., Khlaaf, H., Sastry, G., Mishkin, P., Chan, B., Gray,
S., Ryder, N., Pavlov, M., Power, A., Kaiser, L., Bavar-
ian, M., Winter, C., Tillet, P., Such, F. P., Cummings,
D., Plappert, M., Chantzis, F., Barnes, E., Herbert-
Voss, A., Guss, W. H., Nichol, A., Paino, A., Tezak,
N., Tang, J., Babuschkin, I., Balaji, S., Jain, S., Saun-
ders, W., Hesse, C., Carr, A. N., Leike, J., Achiam,
J., Misra, V., Morikawa, E., Radford, A., Knight, M.,
Brundage, M., Murati, M., Mayer, K., Welinder, P., Mc-
Grew, B., Amodei, D., McCandlish, S., Sutskever, I.,
and Zaremba, W. Evaluating large language models
trained on code. CoRR, abs/2107.03374, 2021a. URL
https://arxiv.org/abs/2107.03374.

Chen, Z., Hellendoorn, V. J., Lamblin, P., Maniatis, P.,
Manzagol, P.-A., Tarlow, D., and Moitra, S. Plur: A
unifying, graph-based view of program learning, under-
standing, and repair. In NeurIPS, 2021b. URL https:
//research.google/pubs/pub50846/.

Chen, Z., Kommrusch, S. J., and Monperrus, M. Neu-
ral transfer learning for repairing security vulnerabili-
IEEE Transactions on Software Engi-
ties in c code.
neering, 2022. URL https://ieeexplore.ieee.
org/document/9699412.

Devlin, J., Chang, M., Lee, K., and Toutanova, K. BERT:
pre-training of deep bidirectional transformers for lan-
guage understanding. In NAACL, 2019. URL https:
//doi.org/10.18653/v1/n19-1423.

Dinella, E., Dai, H., Li, Z., Naik, M., Song, L., and Wang,
K. Hoppity: Learning graph transformations to detect
and ﬁx bugs in programs. In ICLR, 2020. URL https:
//openreview.net/forum?id=SJeqs6EFvB.

Berabi, B., He, J., Raychev, V., and Vechev, M. Tﬁx: Learn-
ing to ﬁx coding errors with a text-to-text transformer.
In ICML, 2021. URL http://proceedings.mlr.
press/v139/berabi21a.html.

Feng, Z., Guo, D., Tang, D., Duan, N., Feng, X., Gong,
M., Shou, L., Qin, B., Liu, T., Jiang, D., and Zhou,
Codebert: A pre-trained model for program-
M.
In Findings of EMNLP,
ming and natural languages.

On Distribution Shift in Learning-based Bug Detectors

2020. URL https://doi.org/10.18653/v1/
2020.findings-emnlp.139.

Guo, M., Haque, A., Huang, D., Yeung, S., and Fei-Fei,
L. Dynamic task prioritization for multitask learning. In
ECCV, 2018. URL https://doi.org/10.1007/
978-3-030-01270-0_17.

He, J., Lee, C., Raychev, V., and Vechev, M. Learning to ﬁnd
naming issues with big code and small supervision. In
PLDI, 2021. URL https://doi.org/10.1145/
3453483.3454045.

Hellendoorn, V. J., Sutton, C., Singh, R., Maniatis, P., and
Bieber, D. Global relational models of source code.
In ICLR, 2020. URL https://openreview.net/
forum?id=B1lnbRNtwr.

Lin, T., Goyal, P., Girshick, R. B., He, K., and Doll´ar, P. Fo-
cal loss for dense object detection. In ICCV, 2017. URL
https://doi.org/10.1109/ICCV.2017.324.

Mukherjee, R., Wen, Y., Chaudhari, D., Reps, T. W.,
Chaudhuri, S., and Jermaine, C. Neural program gen-
eration modulo static analysis. 2021. URL https:
//arxiv.org/abs/2111.01633.

Paszke, A., Gross, S., Massa, F., Lerer, A., Bradbury, J.,
Chanan, G., Killeen, T., Lin, Z., Gimelshein, N., Antiga,
L., Desmaison, A., K¨opf, A., Yang, E. Z., DeVito, Z.,
Raison, M., Tejani, A., Chilamkurthy, S., Steiner, B.,
Fang, L., Bai, J., and Chintala, S. Pytorch: An imper-
ative style, high-performance deep learning library. In
NeurIPS 2019, 2019. URL https://arxiv.org/
abs/1912.01703.

Jain, P., Jain, A., Zhang, T., Abbeel, P., Gonzalez, J.,
and Stoica, I. Contrastive code representation learn-
In EMNLP, 2021. URL https://doi.org/
ing.
10.18653/v1/2021.emnlp-main.482.

Patra, J. and Pradel, M. Semantic bug seeding: a learning-
In ES-
based approach for creating realistic bugs.
EC/FSE, 2021. URL https://doi.org/10.1145/
3468264.3468623.

Jing, L. and Tian, Y. Self-supervised visual feature learn-
ing with deep neural networks: A survey. TPAMI, 43
(11):4037–4058, 2021. URL https://doi.org/10.
1109/TPAMI.2020.2992393.

Pradel, M. and Sen, K. Deepbugs: a learning approach to
name-based bug detection. Proc. ACM Program. Lang., 2
(OOPSLA):147:1–147:25, 2018. URL https://doi.
org/10.1145/3276517.

Kanade, A., Maniatis, P., Balakrishnan, G., and Shi, K.
Learning and evaluating contextual embedding of source
code. In ICML, 2020. URL http://proceedings.
mlr.press/v119/kanade20a.html.

Raychev, V., Bielik, P., and Vechev, M.

Probabilis-
In OOP-
tic model for code with decision trees.
SLA, 2016. URL https://doi.org/10.1145/
2983990.2984041.

Karampatsis, R. and Sutton, C. How often do single-
statement bugs occur?: The manysstubs4j dataset.
In
MSR, 2020. URL https://doi.org/10.1145/
3379597.3387491.

Koh, P. W., Sagawa, S., Marklund, H., Xie, S. M., Zhang,
M., Balsubramani, A., Hu, W., Yasunaga, M., Phillips,
R. L., Gao, I., Lee, T., David, E., Stavness, I., Guo, W.,
Earnshaw, B., Haque, I., Beery, S. M., Leskovec, J., Kun-
daje, A., Pierson, E., Levine, S., Finn, C., and Liang, P.
WILDS: A benchmark of in-the-wild distribution shifts.
In ICML, 2021. URL http://proceedings.mlr.
press/v139/koh21a.html.

Li, X., Sun, X., Meng, Y., Liang, J., Wu, F., and Li,
J. Dice loss for data-imbalanced NLP tasks. In ACL,
2020. URL https://doi.org/10.18653/v1/
2020.acl-main.45.

Li, Z., Zou, D., Xu, S., Ou, X., Jin, H., Wang, S., Deng, Z.,
and Zhong, Y. Vuldeepecker: A deep learning-based
system for vulnerability detection. In NDSS, 2018. URL
http://wp.internetsociety.org/ndss/
wp-content/uploads/sites/25/2018/02/
ndss2018_03A-2_Li_paper.pdf.

Rice, A., Aftandilian, E., Jaspan, C., Johnston, E., Pradel,
M., and Arroyo-Paredes, Y. Detecting argument selection
defects. Proc. ACM Program. Lang., 1(OOPSLA):104:1–
104:22, 2017. URL https://doi.org/10.1145/
3133928.

Saito, T. and Rehmsmeier, M. The precision-recall plot
is more informative than the roc plot when evaluating
binary classiﬁers on imbalanced datasets. PloS one, 10
(3):e0118432, 2015. URL https://www.ncbi.nlm.
nih.gov/pmc/articles/PMC4349800/.

Søgaard, A. and Goldberg, Y. Deep multi-task learning
with low level tasks supervised at lower layers. In ACL,
2016. URL https://doi.org/10.18653/v1/
p16-2038.

Vasic, M., Kanade, A., Maniatis, P., Bieber, D., and Singh, R.
Neural program repair by jointly learning to localize and
repair. In ICLR, 2019. URL https://openreview.
net/forum?id=ByloJ20qtm.

Wei, J., Goyal, M., Durrett, G., and Dillig, I. Lambdanet:
Probabilistic type inference using graph neural networks.
In ICLR, 2020. URL https://openreview.net/
forum?id=Hkx6hANtwH.

On Distribution Shift in Learning-based Bug Detectors

Wolf, T., Debut, L., Sanh, V., Chaumond, J., Delangue,
C., Moi, A., Cistac, P., Rault, T., Louf, R., Funtow-
icz, M., and Brew, J. Huggingface’s transformers:
State-of-the-art natural language processing. CoRR,
abs/1910.03771, 2019. URL http://arxiv.org/
abs/1910.03771.

Yasunaga, M. and Liang, P. Break-it-ﬁx-it: Unsuper-
In ICML, 2021.
vised learning for program repair.
URL http://proceedings.mlr.press/v139/
yasunaga21a.html.

Yin, P., Neubig, G., Allamanis, M., Brockschmidt, M.,
and Gaunt, A. L. Learning to represent edits. In ICLR,
2019. URL https://openreview.net/forum?
id=BJl6AjC5F7.

Zhang, Y. and Yang, Q. A survey on multi-task learn-
ing. IEEE Transactions on Knowledge and Data Engi-
neering, 2021. URL https://ieeexplore.ieee.
org/stamp/stamp.jsp?arnumber=9392366.

Zhou, Y., Liu, S., Siow, J. K., Du, X., and Liu, Y. Devign:
Effective vulnerability identiﬁcation by learning compre-
hensive program semantics via graph neural networks. In
URL https://proceedings.
NeurIPS, 2019.
neurips.cc/paper/2019/hash/
49265d2447bc3bbfe9e76306ce40a31f-
Abstract.html.

Z¨ugner, D., Kirschstein, T., Catasta, M., Leskovec, J., and
G¨unnemann, S. Language-agnostic representation learn-
ing of source code from structure and context. In ICLR,
2021. URL https://openreview.net/forum?
id=Xh5eMZVONGF.

A. Computing Localization and Repair Probabilities

On Distribution Shift in Learning-based Bug Detectors

1

, . . . , prep

Now we discuss how pointer models compute the localization probabilities P loc = [ploc
n ] and the repair probabilities
P rep = [prep
]. Given the feature embeddings [h1, . . . , hn] for program tokens T = (cid:104)t1, t2, . . . , tn(cid:105), pointer
models ﬁrst compute a score vector Sloc = [sloc
reﬂects the likelihood of token ti to be the
bug location. If ti ∈ Loc, i.e., token ti is a candidate bug location, a feedforward network πloc : Rm → R is applied on the
feature vector hi to compute sloc
. Otherwise, it is unlikely that ti is the bug location so a minus inﬁnity score is assigned.
Formally, sloc

n ] where each score sloc

is computed as follows:

1 , . . . , ploc

1 , . . . , sloc

i

i

l

i

where M loc is the localization candidate mask:

sloc
i =

(cid:40)

πloc(hi)
−inf

if M loc[i] = 1,
otherwise,

M loc[i] =

(cid:40)
1
0

if ti ∈ Loc,
otherwise.

Sloc is then normalized to localization probabilities with the softmax function: P loc = [ploc

n ] = softmax(Sloc).
Depending on the bug type, the set of repair tokens, Rep, can be drawn from T (e.g., for var-misuse and arg-swap) or
ﬁxed (e.g., for wrong-binop). For the former case, P rep is computed in the same way as computing P loc, except that
another feedforward network, πrep, and the repair candidate mask, M rep, are used instead of πloc and M loc. When Rep is
a ﬁxed set of l tokens, the repair prediction is basically an l-class classiﬁcation problem. We treat the ﬁrst token t1 of T as
the repair token t[rep] and apply πrep : Rm → Rl over its feature vector h[rep] to compute scores S = πrep(h[rep]). Then,
the ﬁnal repair score is set to S[i] if M rep indicates that the i-th repair token is valid, or to minus inﬁnity otherwise. Overall,
the repair scores Srep = [srep

] are computed as follows:

1 , . . . , ploc

, . . . , srep

1

l

srep
i =

(cid:40)

S[i]
−inf

if M rep[i] = 1,
otherwise,

Srep is then normalized to repair probabilities with the softmax function: P rep = [prep

1

, . . . , ploc

l

] = softmax(Srep).

B. Implementation, Model and Training Details

In this section, we provide details in implementation, models, and training.

Constructing the Test Sets in Figure 1
In Figure 1, we mention three test sets used to reveal distribution shift in existing
learning-based var-misuse detectors. Test set I is a balanced dataset with synthetic bugs, created by randomly selecting
336 non-buggy samples from real-test and injecting one synthetic bug into each non-buggy sample. Test set II is a
balanced dataset with real bugs, created by replacing the synthetic bugs in the ﬁrst test set by the 336 real bugs in real-test.
Test set III is real-test.

Three Bug Types Handled by Our Work The deﬁnition of var-misuse (resp., wrong-binop and arg-swap) can be
found in (Allamanis et al., 2018; Vasic et al., 2019) (resp., in (Pradel & Sen, 2018)). To determine Loc and Rep, we mainly
follow (Allamanis et al., 2021; Kanade et al., 2020) and add small adjustments to capture more real bugs:

• var-misuse: we include all appearances of all local variables in Loc, as long as the appearance is not in a function
deﬁnition and the variable has been deﬁned before the appearance. When constructing Rep for each bug location variable,
we include all local variable deﬁnitions that can be found in the scope of the bug location variable, except for the ones that
deﬁne the bug location variable itself.

• wrong-binop: we deal with three sets of binary operators: arithmetics {+, *, -, /, %}, comparisons {==, !=, is, is
not, <, <=, >, >=, in, not in}, and booleans {and, or}. If a binary operator belongs to any of the three sets, it is
added to Loc. The set that the operator belongs to, excluding the operator itself, is treated as Rep. The repair candidate
mask M rep is of size 17, i.e., it includes all the operators in the three sets. M rep sets the operators in Rep to 1 and the
other operators to 0.

On Distribution Shift in Learning-based Bug Detectors

Table 9: The number of epochs, learning rate (LR), and time cost for the two training phases.

First phase

Second phase

Bug Type

Epochs

var-misuse
wrong-binop
arg-swap

1
1
1

LR

10-6
10-5
10-5

Time

Epochs

15h
15h
15h

2
2
1

LR

10-6
10-6
10-6

Time

10h
6h
2h

• arg-swap: We handle most function arguments but exclude keyworded and variable-length arguments that are less likely
to be mistaken. In contrast to the other bug types, we also support the swapping of arguments that consist of more than a
single token (e.g.., an expression), by simply marking the ﬁrst token as the bug location or the repair token. Moreover,
we consider only functions that have two or more handled arguments. We put all candidate arguments in Loc. For each
argument in Loc, Rep is the other candidate arguments used in the same function.

For bug injection and real bug extraction, we apply the bug-inducing rewriting rules in (Allamanis et al., 2021) given the
deﬁnitions of Loc and Rep above.

Implementation with CuBERT Here, we describe the implementation of our techniques with CuBERT. CuBERT
tokenizes the input program into a sequence of sub-tokens. When constructing the masks M loc, C loc, M rep, and C rep from
Loc and Rep, we set the ﬁrst sub-token of each token to 1. As standard with BERT-like models (Devlin et al., 2019), the ﬁrst
sub-token of the input sequence to CuBERT is always [CLS] used as aggregate sequence representation for classiﬁcation
tasks. We also use this token and its corresponding feature embedding for bug classiﬁcation (all three tasks) and repair
(only wrong-binop). CuBERT consists of a sequence of BERT layers and thus naturally aligns with our task hierarchy.
Our two-phase training is technically a two-phase ﬁne-tuning procedure when applied to pre-trained models like CuBERT.
CuBERT requires the input sequence to be of ﬁxed length, meaning that shorted sequences will be padded and longer
sequences will be truncated. We chose length 512 due to contraints on hardware: CuBERT is demanding in terms of GPU
memory and longer lengths caused out-of-memory errors on our machines. When extracting real bugs and injecting bugs
into open source programs, we only consider bugs for which the bug location and at least one correct repair token are within
the ﬁxed length. This includes most real bugs we found.

Model Details CuBERT is a BERT-Large model with 24 hidden layers, 16 attention heads, 1024 hidden units, and in total
340M parameters. Our classiﬁcation head πcls is a two-layer feedforward network. The localization head πloc is just a
linear layer. The repair head πrep is a linear layer for var-misuse and arg-swap and a two-layer feedforward network
for wrong-binop. The size of the hidden layers is 1024 for all task heads. The implementation of our model is based on
Hugging Face (Wolf et al., 2019) and PyTorch (Paszke et al., 2019).

Training Details Our experiments were done on servers with NVIDIA RTX 2080 Ti and NVIDIA TITAN X GPUs. As
described in Section 3.3, our training procedure consists of two phases. In the ﬁrst phase, we load a pretrained CuBERT
model provided by the authors (Kanade et al., 2020) and ﬁne-tune it with syn-train. In the second phase, we load the
model trained from the ﬁrst phase and perform fresh ﬁne-tuning with real-train. The number of epochs, learning rate,
and the time cost of the two training phases are shown in Table 9. Both training phases require at most two epochs to achieve
good performance, highlighting the power of pretrained models to quickly adapt to new tasks and data distributions. In each
batch, we feed two samples into the model as larger batch size will cause out-of-memory errors.

For fair comparison, when creating synthetic bugs with BugLab, we do not perform their data augmentation rewrite rules for
all models. Those rules apply to all models and can be equally beneﬁcial. When training GNN models with syn-train
and real-train, we follow (Allamanis et al., 2021) to use early stopping over real-val. When training with BugLab,
we use 80 meta-epochs, 5k samples (buggy/non-buggy raito 1:1) per meta-epoch, and 40 model training epochs within one
meta-epoch. This amounts to a total of around 6 days of training time for GNN.

C. More Evaluation Results

In this section, we present additional evaluation results.

On Distribution Shift in Learning-based Bug Detectors

Evaluation Results for arg-swap We repeat the experiments in Sections 5.1 and 5.2 for arg-swap. The results are
shown in Tables 10 to 12 and Figures 9 to 12. Most observations that we can make from those results are similar to what we
discussed in Sections 5.1 and 5.2 for var-misuse and wrong-binop. We highlight two difference: ﬁrst, Our Full Method
does not have a clear advantage over Only Synthetic and Mix in terms of AP (see Figure 9); second, the data imbalance
and the amount of training data do not clearly improve the AP (see Figures 10 and 11). These different points are likely
due to the distinct characteristics of arg-swap bugs. We leave it as an interesting future work item to further improve the
performance of arg-swap detectors.

Parameter Selection for Task Hierarchy and Contrastive Learning
In Tables 15 to 17 (resp., Tables 18 to 20), we
show the model performance by only changing weight β of the contrastive loss (resp., the task order in our task hierarchy).
For var-misuse and wrong-binop, Our Full Method (highlighted with (cid:63)) performs the best among all the conﬁgurations.
In terms of β and var-misuse, Our Full Method (β = 0.5) is less precise but has signiﬁcantly higher recall than β = 8.
For arg-swap, Our Full Method performs the best on the validation set but not on the test set.

On Distribution Shift in Learning-based Bug Detectors

100

s
l
c
P

80

60

40

20

0

AP
Pcls-loc
Rcls-loc-rep

Pcls
Rcls-loc

Rcls
Pcls-loc-rep

Our Full Method (AP = 8.49)
Mix (AP = 9.33)
Only Synthetic (AP = 8.31)

100

e
c
n
a
m
r
o
f
r
e
P

75

50

25

0

0

20

60

40
Rcls

80 100

20 21 22 23 24 25 26

176

Non-buggy/buggy ratio

Figure 9: Precision-recall
curve and AP for methods
in Table 10 (arg-swap).

Figure 10: Varying data
skewness in the second train-
ing phase (arg-swap).

AP
Rcls-loc

Pcls
Pcls-loc-rep

Rcls
Rcls-loc-rep

Pcls-loc

100

e
c
n
a
m
r
o
f
r
e
P

75

50

25

0

0

21 22 23 24 25 26100

Percentage
(a) synthetic-train

100

e
c
n
a
m
r
o
f
r
e
P

75

50

25

0

0

21 22 23 24 25 26100

Percentage
(b) real-train

Figure 11: Model performance with subsampled
syn-train or real-train (arg-swap).

100

s
l
c
P

80

60

40

20

0

Our Model (Synthetic + Real) (AP = 8.49)
GNN (Synthetic + Real) (AP = 6.44)
GNN (Only Synthetic) (AP = 5.42)

100

GNN (BugLab + Real) (AP = 5.14)
GNN (Only BugLab) (AP = 1.52)

s
l
c
P

80

60

40

20

0

0

20 40 60 80 100

0

20 40 60 80 100

Rcls
(a) Synthetic

Rcls
(b) BugLab

Figure 12: Precision-recall curves and AP for GNN and Our
Model with different training phases (arg-swap).

Table 10: Changing training phases (arg-swap).

cls

cls-loc

cls-loc-rep

Method

Only Synthetic
Only Real
Mix
Two Synthetic

P

1.31
0
2.02
44.19

R

P

R

P

39.84
0
32.93
7.72

1.00
0
1.69
44.19

30.49
0
27.64
7.72

0.79
0
1.59
44.19

Our Full Method

73.68

5.69

73.68

5.69

73.68

R

23.98
0
26.02
7.72

5.69

Table 11: Applying our two-phase training on GNN and
BugLab (Allamanis et al., 2021) (arg-swap).

cls

cls-loc

cls-loc-rep

Model

Training Phases

GNN
GNN
GNN
GNN

Only Synthetic
Synthetic + Real
Only BugLab
BugLab + Real

P

0.99
83.33
0.81
81.82

R

P

R P

50.00
4.07
51.63
3.66

0.68
83.33
0.50
81.82

34.15
4.07
32.11
3.66

0.43
75.00
0.37
72.73

Our Model

Synthetic + Real

73.68

5.69

73.68

5.69

73.68

R

21.95
3.66
23.58
3.25

5.69

Table 12: Evaluating other techniques (arg-swap).

cls

cls-loc

cls-loc-rep

Method

No cls Head
No Hierarchy
No Focal Loss
No Contrastive

P

34.21
61.29
73.68
46.15

Our Full Method

73.68

R

P

R

P

5.28
7.72
5.69
7.32

5.69

34.21
61.29
73.68
46.15

73.68

5.28
7.72
5.69
7.32

5.69

34.21
61.29
73.68
46.15

73.68

R

5.28
7.72
5.69
7.32

5.69

On Distribution Shift in Learning-based Bug Detectors

Table 15: Different weight β (var-misuse).

Table 18: Different task order (var-misuse).

Weight β of
Contrastive Loss

0
0.25
(cid:63) 0.5
1
2
4
8
16

cls

cls-loc

cls-loc-rep

cls

cls-loc

cls-loc-rep

P

60.00
59.21
64.79
61.90
61.67
58.62
71.43
63.64

R

P

R

P

14.29
13.39
13.69
11.61
11.01
10.12
1.49
6.25

57.50
56.58
61.97
57.14
58.33
55.17
71.43
63.64

13.69
12.80
13.10
10.71
10.42
9.52
1.49
6.25

53.75
55.26
56.34
55.56
58.33
51.72
71.43
60.61

R

12.80
12.50
11.90
10.42
10.42
8.93
1.49
5.95

Task Order

No Hierarchy
(cid:63) cls, loc, rep
cls, rep, loc
loc, cls, rep
loc, rep, cls
rep, cls, loc
rep, loc, cls

P

51.82
64.79
57.53
53.66
57.69
51.69
52.38

R

P

R

P

16.96
13.69
12.50
13.10
13.39
13.69
13.10

50.91
61.97
54.79
50.00
53.85
49.44
50.00

16.67
13.10
11.90
12.20
12.50
13.10
12.50

48.18
56.34
54.79
47.56
51.28
47.19
46.43

R

15.77
11.90
11.90
11.61
11.90
12.50
11.61

Table 16: Different weight β (wrong-binop).

Table 19: Different task order (wrong-binop).

Weight β of
Contrastive Loss

0
0.25
0.5
1
2
(cid:63) 4
8
16

cls

cls-loc

cls-loc-rep

cls

cls-loc

cls-loc-rep

P

47.96
47.92
47.62
48.51
51.54
52.30
48.35
50.23

R

P

R

P

43.18
44.60
44.81
46.44
44.20
43.99
41.75
43.79

47.06
47.05
46.75
47.66
50.36
51.09
47.41
49.30

42.36
43.79
43.99
45.62
43.18
42.97
40.94
42.97

46.15
46.17
46.10
46.60
49.64
49.64
46.70
48.36

R

41.55
42.97
43.38
44.60
42.57
41.75
40.33
42.16

Task Order

No Hierarchy
cls, loc, rep
cls, rep, loc
loc, cls, rep
loc, rep, cls
rep, cls, loc
(cid:63) rep, loc, cls

P

48.70
49.43
48.29
49.66
46.44
50.82
52.30

R

P

R

P

45.62
44.40
43.18
44.20
46.44
44.20
43.99

47.83
48.30
47.38
48.74
45.42
49.88
51.09

44.81
43.38
42.36
43.38
45.42
43.38
42.97

46.52
47.39
46.01
47.60
44.81
48.71
49.64

R

43.58
42.57
41.14
42.36
44.81
42.36
41.75

Table 17: Different weight β (arg-swap).

Weight β of
Contrastive Loss

0
0.25
(cid:63) 0.5
1
2
4
8
16

cls

cls-loc

cls-loc-rep

P

46.15
60.00
73.68
69.57
63.64
72.73
76.47
86.67

R

P

R

P

7.32
4.88
5.69
6.50
5.69
6.50
5.28
5.28

46.15
60.00
73.68
69.57
63.64
72.73
76.47
86.67

7.32
4.88
5.69
6.50
5.69
6.50
5.28
5.28

46.15
60.00
73.68
69.57
54.55
72.73
76.47
86.67

R

7.32
4.88
5.69
6.50
4.88
6.50
5.28
5.28

Table 20: Different task order (arg-swap).

cls

cls-loc

cls-loc-rep

Task Order

No Hierarchy
cls, loc, rep
cls, rep, loc
(cid:63) loc, cls, rep
loc, rep, cls
rep, cls, loc
rep, loc, cls

P

61.29
94.12
53.57
73.68
55.56
76.47
63.64

R

P

R

P

7.72
6.50
6.10
5.69
6.10
5.28
5.69

61.29
94.12
53.57
73.68
55.56
76.47
63.64

7.72
6.50
6.10
5.69
6.10
5.28
5.69

61.29
88.24
53.57
73.68
55.56
76.47
63.64

R

7.72
6.10
6.10
5.69
6.10
5.28
5.69

D. Case Studies of Inspected Warnings

On Distribution Shift in Learning-based Bug Detectors

In the following we present case studies of the warnings we inspect in Section 5.3. We showcase representative bugs and
code quality issues raised by our models. Further, we also provide examples of false positives and discuss potential causes
for the failure. We visualise the bug location with

and the repair token with

.

D.1. var-misuse: bug in repository aleju/imgaug

Our bug detector model correctly identiﬁes the redundant check on x px instead of y px.

def translate(self, x_px, y px):

if x_px < 1e-4 or x_px > 1e-4 or y_px < 1e-4 or x px > 1e-4:

matrix = np.array([[1, 0, x_px], [0, 1, y_px], [0, 0, 1]], dtype=np.float32)
self._mul(matrix)

return self

D.2. var-misuse: bug in repository babelsberg/babelsberg-r

The model identiﬁes that w read was already checked but not w write.

def test_pipe(self, space):

w_res = space.execute("""
return IO.pipe
""")
w_read, w write = space.listview(w_res)
assert isinstance(w_read, W_IOObject)
assert isinstance(w read, W_IOObject)
w_res = space.execute("""
r, w, r_c, w_c = IO.pipe do |r, w|

r.close
[r, w, r.closed?, w.closed?]

end
return r.closed?, w.closed?, r_c, w_c
""")
assert self.unwrap(space, w_res) == [True, True, True, False]

D.3. var-misuse: code quality issue in repository JonnyWong16/plexpy

The model proposes to replace c by snowman, since snowman is otherwise unused. Even though this replacement does not
suggest a bug, the warning remains useful as the unused variable snowman must be considered a code quality issue.

def test_ensure_ascii_still_works(self):

# in the ascii range, ensure that everything is the same
for c in map(unichr, range(0, 127)):

self.assertEqual(

json.dumps(c, ensure_ascii=False),
json.dumps(c))

snowman = u’\N{SNOWMAN}’
self.assertEqual(

json.dumps(c, ensure_ascii=False),

’"’

+ c +

’"’)

D.4. var-misuse: false positive in repository ceache/treadmill

The model proposes to replace fmt with server, although the surrounding code clearly implies that server is None at
this point in the program. Therefore, we consider it as a false positive. In this case, the two preceding method calls with
server in their name, were given the server variable as second argument. This may have affected the prediction of the
model, disregarding the surrounding conditional statement with respect to server.

On Distribution Shift in Learning-based Bug Detectors

def server_cmd(server, reason, fmt, clear):

"""Manage server blackout."""
if server is not None:

if clear:

_clear_server_blackout(context.GLOBAL.zk.conn, server)

else:

_blackout_server(context.GLOBAL.zk.conn, server, reason)

else:

_list_server_blackouts(context.GLOBAL.zk.conn, fmt)

D.5. wrong-binop: bug in repository Amechi101/concepteur-market-app

The model detects the presence of the string formatting literal %s in the string and in consequence raises a warning about a
wrong binary operator.

+ * -
def buildTransform(inputProfile, outputProfile, inMode, outMode,

%

/

renderingIntent=INTENT_PERCEPTUAL, flags=0):

if not isinstance(renderingIntent, int) or not (0 <= renderingIntent <=3):
raise PyCMSError("renderingIntent must be an integer between 0 and 3")

if not isinstance(flags, int) or not (0 <= flags <= _MAX_FLAG):

raise PyCMSError("flags must be an integer between 0 and %s" + _MAX_FLAG)

try:

if not isinstance(inputProfile, ImageCmsProfile):

inputProfile = ImageCmsProfile(inputProfile)

if not isinstance(outputProfile, ImageCmsProfile):
outputProfile = ImageCmsProfile(outputProfile)

return ImageCmsTransform(inputProfile, outputProfile, inMode, outMode,

renderingIntent, flags=flags)

except (IOError, TypeError, ValueError) as v:

raise PyCMSError(v)

D.6. wrong-binop: bug in repository maestro-hybrid-cloud/heat

The model correctly raises a warning since the comparison must be a containment check instead of an equality check.

!= is

==
is not
def suspend(self):

<

<=

>

>=

in

not in

# No need to suspend if the stack has been suspended
if self.state == (self.SUSPEND, self.COMPLETE):

LOG.info(_LI(’%s is already suspended’), six.text_type(self))
return

self.updated_time = datetime.datetime.utcnow()
sus_task = scheduler.TaskRunner(

self.stack_task,
action=self.SUSPEND,
reverse=True,
error_wait_time=cfg.CONF.error_wait_time)

sus_task(timeout=self.timeout_secs())

D.7. wrong-binop: code quality issue in repository tomspur/shedskin

The model identiﬁes the unconventional use of the != operator when comparing with None.

!= is

==
<
def getFromEnviron():

is not

On Distribution Shift in Learning-based Bug Detectors

<=

> >=

in not in

if HttpProxy.instance is not None:
return HttpProxy.instance

url = None
for key in (’http_proxy’, ’https_proxy’):

url = os.environ.get(key)
if url: break

if not url:

return None
dat = urlparse(url)
port = 80 if dat.scheme == ’http’ else 443
if dat.port != None: port = int(dat.port)
host = dat.hostname
return HttpProxy((host, port), dat.username, dat.password)

D.8. wrong-binop: false positive in repository wechatpy/wechatpy

Our model mistakenly raises a wrong-binop warning with the == operator and proposes to replace it with > operator. In
this case, the log message below the conditional check may have triggered the warning.

!= is

is not
==
def add_article(self, article):

> >=

<=

<

in not in

if len(self.articles) == 10:

raise AttributeError("Can’t add more than 10 articles in an ArticlesReply")

articles = self.articles
articles.append(article)
self.articles = articles

D.9. arg-swap: bug in repository sgiavasis/nipype

Our model identiﬁes the invalid use of the NumPy function np.savetxt(streamlines, out file + ’.txt’)2, which
expects ﬁrst the ﬁle name, i.e., out file + ’.txt’ in this case.

def _trk_to_coords(self, in_file, out_file=None):
from nibabel.trackvis import TrackvisFile
trkfile = TrackvisFile.from_file(in_file)
streamlines = trkfile.streamlines

if out_file is None:

out_file, _ = op.splitext(in_file)

np.savetxt(streamlines, out file + '.txt')
return out_file + '.txt'

D.10. arg-swap: false positive in repository davehunt/bedrock

Our model mistakenly raises an argument swap warning with the SpacesPage constructor. In fact, with this speciﬁc
repository our model repeatedly raised issues at similar code locations where the Selenium library is used. This is likely due
to not having encountered similar code during training and hence due to lack of repository-speciﬁc information.

2NumPy Documentation, https://numpy.org/doc/stable/reference/generated/numpy.savetxt.html

On Distribution Shift in Learning-based Bug Detectors

@pytest.mark.nondestructive
def test_spaces_list(base_url, selenium):

page = SpacesPage(base url, selenium).open()
assert page.displayed_map_pins == len(page.spaces)
for space in page.spaces:

space.click()
assert space.is_selected
assert space.is_displayed
assert 1 == page.displayed_map_pins

E. Bug Reports to the Developers

We report a number of bugs found during our manual inspection as pull requests to the developers. For forked repositories,
we trace the buggy code in the original repository. If the original repository has the same code, we create a bug report in the
original repository. Otherwise, we do not report the bug. We also found that 7 bugs are already ﬁxed in the latest version
of the repository. The links to the pull requests are listed below. We also mark the pull requests for which we received a
conﬁrmation from the developers before the deadline for the ﬁnal version of this paper (two days after we reported them).

var-misuse:
(merged) https://github.com/numpy/numpy/pull/21764
(merged) https://github.com/frappe/erpnext/pull/31372
(merged) https://github.com/spirali/kaira/pull/31
(merged) https://github.com/pyro-ppl/pyro/pull/3107
(merged) https://github.com/nest/nestml/pull/789
(merged) https://github.com/cupy/cupy/pull/6786
(merged) https://github.com/funkring/fdoo/pull/14
(conﬁrmed) https://github.com/apache/airflow/pull/24472
https://github.com/topazproject/topaz/pull/875
https://github.com/inspirehep/inspire-next/pull/4188
https://github.com/CloCkWeRX/rabbitvcs-svn-mirror/pull/6
https://github.com/amonapp/amon/pull/219
https://github.com/mjirik/io3d/pull/9
https://github.com/jhogsett/linkit/pull/30
https://github.com/aleju/imgaug/pull/821
https://github.com/python-diamond/Diamond/pull/765
https://github.com/python/cpython/pull/93935
https://github.com/orangeduck/PyAutoC/pull/3
https://github.com/damonkohler/sl4a/pull/332
https://github.com/vyrus/wubi/pull/1
https://github.com/shon/httpagentparser/pull/89
https://github.com/midgetspy/Sick-Beard/pull/991
https://github.com/sgala/gajim/pull/3
https://github.com/tensorflow/tensorflow/pull/56468

wrong-binop:
(merged) https://github.com/python-pillow/Pillow/pull/6370
(merged) https://github.com/funkring/fdoo/pull/15
(false positive) https://github.com/kovidgoyal/calibre/pull/1658
https://github.com/kbase/assembly/pull/327
https://github.com/maestro-hybrid-cloud/heat/pull/1
https://github.com/gramps-project/gramps/pull/1380
https://github.com/scikit-learn/scikit-learn/pull/23635
https://github.com/pupeng/hone/pull/1
https://github.com/edisonlz/fruit/pull/1
https://github.com/certsocietegenerale/FIR/pull/275

On Distribution Shift in Learning-based Bug Detectors

https://github.com/MediaBrowser/MediaBrowser.Kodi/pull/117
https://github.com/sgala/gajim/pull/4
https://github.com/mapsme/omim/pull/14185
https://github.com/tensorflow/tensorflow/pull/56471
https://github.com/catapult-project/catapult-csm/pull/2

arg-swap:
(merged) https://github.com/clinton-hall/nzbToMedia/pull/1889
(merged) https://github.com/IronLanguages/ironpython3/pull/1495
(false positive) https://github.com/python/cpython/pull/93869
https://github.com/google/digitalbuildings/pull/646
https://github.com/quodlibet/mutagen/pull/563
https://github.com/nipy/nipype/pull/3485

