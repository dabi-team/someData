DARWIN: A Highly Flexible Platform for Imaging
Research in Radiology

Lufan Chang, Wenjing Zhuang, Richeng Wu, Sai Feng, Hao Liu, Jing Yu, Jia Ding, Ziteng Wang, Jiaqi Zhang

1

0
2
0
2

p
e
S
2

]

V

I
.
s
s
e
e
[

1
v
8
0
9
0
0
.
9
0
0
2
:
v
i
X
r
a

Abstract—To conduct a radiomics or deep learning research
experiment, the radiologists or physicians need to grasp the
needed programming skills, which, however, could be frustrating
and costly when they have limited coding experience. In this
paper, we present DARWIN, a ﬂexible research platform with
a graphical user interface for medical imaging research. Our
platform is consists of a radiomics module and a deep learning
module. The radiomics module can extract more than 1000 di-
mension features(ﬁrst-, second-, and higher-order) and provided
many draggable supervised and unsupervised machine learning
models. Our deep learning module integrates state of the art
architectures of classiﬁcation, detection, and segmentation tasks.
It allows users to manually select hyperparameters, or choose an
algorithm to automatically search for the best ones. DARWIN
also offers the possibility for users to deﬁne a custom pipeline for
their experiment. These ﬂexibilities enable radiologists to carry
out various experiments easily.

Index Terms—GUI platform, radiomics, deep learning, hyper-

parameters tuning,

I. INTRODUCTION

ical

M ACHINE learning becomes ubiquitous in recent med-

imaging analysis research. The radiomics and
deep learning techniques have facilitated numerous disease
diagnosis with high precision [1]–[3]. There has been a
series of applications of machine learning to medical imaging.
Physicians can use machine learning techniques to explore
the relationships between images processed using radiomics,
clinical outcomes and radiation dose data to help improve
cancer treatment with radiotherapy [2]. Bayesian Network
(BN), decision tree (DT), and Support Vector Machine (SVM)
have been employed in various studies to predict 2-year
survival [4]–[6]. Numerous machine learning models and fea-
ture selection techniques for Non-small-cell Lung Carcinoma
(NSCLC) cancer patients has been investigated in [7]. Machine
learning models also facilitate many tasks such as lung nodules
classiﬁcation [8], [9],
lesions detection [10], segmentation
[11], [12], and image registration [13].

Many machine learning platforms for general tasks have
been developed by some high technology companies such
as Google, Amazon, and Microsoft. These platforms greatly
reduce duplication of efforts for developers. However, coding
experience is needed for conducting experiments on these
platforms, which makes it difﬁcult for physicians with limited
programming skills. Furthermore, these platforms do not focus

Authors are with the Yizhun Medical AI Co.Ltd, Beijing, China
Emails: {lufan.chang, wenjing.zhuang,

richeng.wu,

sai.feng, hao.liu,

jing.yu, jia.ding, ziteng.wang, jiaqi.zhang}@yizhun-ai.com

Manuscript received April 19, 2005; revised August 26, 2015.

on medical images, some requirements from physicians or
radiologists may not be covered.

A few platforms targeting medical images have come out
to facilitate radiologists to carry out their research [14]–[17].
Imaging Biomarker Explorer (IBEX) [15] is a graphical-user-
interface (GUI) based platform for medical image analysis.
It
integrates ROI labeler, radiomic feature extractor, and
feature/model viewer. IBEX offers the ﬂexibility for users
to choose features, however, it only works on the Windows
system, and the dependency ﬁles make the installation rather
difﬁcult. RayPlus [17] has been proposed to solve this prob-
lem. RayPlus is a web-based platform across all systems. The
features provided include ﬁve image ﬁlters with histogram,
volumetric, morphologic, and texture features. These features
could be downloaded and further analyzed in R or SPSS.

These GUI based platforms enable users to do image
analysis with minimal coding experience, however, still have
notable weaknesses: 1) Only certain types of images such
as CT, PET are supported, which cannot satisfy the needs
of physicians to study various types of images. 2) Limited
numbers of features are included, making it hard to generalize
well on different tasks. 3) Only radiomic features provided,
and users still need to write programs on machine learning
models in a way they can make clinical predictions using these
features. 4) State of the art deep learning methods are not
covered in these platforms.

Therefore we developed DARWIN: an artiﬁcial intelligence
research platform for medical imaging. The platform integrates
numerous data preprocessing methods, machine learning mod-
els, and data visualization methods with a web-based GUI. The
platform does not have steep learning curves for radiologists
to master it. The main contributions of the platform mainly
lie on the following four aspects: 1) incorporated PyRadiomics
library [18] to extract abundant radiomics features. 2)provided
several deep learning models with hyperparameter tuning
algorithms 3) provided customized pipelines for different
applications. 4) provided a user-friendly interaction mode.

II. OVERALL FEATURES OF THE PLATFORM
DARWIN research platform is a software mainly designed
for radiologists to conduct machine learning research. It is
convenient for physicians or radiologists to build an ex-
periment pipeline to validate their hypotheses immediately.
Fig 1 presents the architecture of DARWIN. It is based on
Browser/Server (B/S) structure. Users can access the platform
with a web browser on any operating system (Mac OSX,
is implemented using Javascript,
Windows, and Linux). It
Python, and C++.

 
 
 
 
 
 
2

Fig. 1. Computational Architecture of the DARWIN Platform. Users can use the web browser to upload the experiment images, annotate region of interests
(ROIs), construct a computational graph, download results, and retrieve history records. The four databases at the server-side store the images, masks, radiomic
features ,and models respectively.

The main functionalities of DARWIN include the following:

• Image Labeler: A GUI for DICOM image loading and
display. Users can also draw a region of interest (ROI)
and annotate it with a self-deﬁned label (E.g. malignant
or benign).

• Computational Graph: A GUI to build a computational
pipeline, which includes image loader, preprocessing, fea-
ture extraction, visualization, etc (Fig 2). Deep learning
and radiomic experiments have separate computational
graphs. Each computational graph is a tree structure with
computational nodes.

Fig. 3. Semi-automatic segmentation tools. The ﬁrst image is a coarse ROI
drawn by the user on T1WI. The second image shows the ROI generated
by the ’Fit Boundary’ tool. The third one is the same ROI automatically
transferred to T2WI via the ’Copy/Paste’ tool.

III. IMAGE LABELER AND RADIOMIC FEATURE
EXTRACTOR

The image labeler can load multimodal DICOM images
including CT, MRI, CR/DXR, PET, and so on. Like Picture
Archiving and Communication System (PACS), we can not
only view pictures on it, but also do window level/width
adjusting, image scaling, and 3D-reconstruction. ROI is always
necessary for analyzing, and we offer tools for ROI loading,
drawing, and modifying. For breast cancer and pulmonary
nodules, we have built-in convolutional neural networks that
can automatically detect and segment lesions. For other types
of images and ROIs, the regions can also be drawn manually
with the 2D and 3D drawing tool. To help physicians draw the
ROIs effortlessly, we provided semi-automatic segmentation
tools (Fig. 3). The ’Fit Boundary’ tool implement using the
seeded region growing algorithm [19] helps to segment out
the bright/dark ROI within a manually drawn curve, and
will spread out
to 3D neighboring slices if allowed. The
’Copy/Paste’ tool allows users to copy an ROI to different
series in the same study with automatically registered.

A 2D ROI in our system is a polygon,

thus we save
it using the position of all its vertexes. Accordingly, it is
straightforward for users to modify any vertex. Furthermore,
for each ROI, we can annotate labels on it, such as malignant
and benign or BI-RADS (0,1,2,3,4,5). An important thing for

3

Fig. 2. The computational graph of a radiomic experiment. The left sidebar menu presents entries to Laboratory, Image Gallery, and other Projects. The left
part of the page is the node-selecting area. Users can drag the nodes to the center and build up a computational graph. All the parameters could be changed
on the right side.

our labeler is that we can manually or automatically establish
some mappings between ROIs. For example, in some cases,
we could have Cranial-Caudal (CC) and mediolateral-oblique
(MLO) views for mammograms, and usually, a lesion appears
on both views. In our system, We can link the two ROIs to
distinguish whether they are from the same lesion, while they
share the same annotation information and this relationship
should be emphasized in analysis. All of the operations above
will be sent to the server and stored in the database for future
use.

When the user ﬁnished labeling one ROI and submitted the
results, our system will automatically extract the radiomics
features and save them in the database. This operation reduced
the delay in radiomic experiments and remarkably improved
user experience. We offered more than 1300 dimensions of
radiomics features for each ROI. The features consist of on
average ∼ 90 features (Table.II) for the original and derived
images (8 levels of Wavelet decompositions; Laplacian of
Gaussian, Square, Square Root, Logarithm and Exponential
ﬁlters). These feature extractors are implemented using the
PyRadiomics [18] library. We also provided a robust feature
extraction choice (Fig. 4). Users can perform perturbations on
ROI and extract perilesional features to get more stable results.

IV. COMPUTATIONAL GRAPH

A Computational Graph is a directed graph represents an
experimental pipeline. The users are able to build a graph using
our GUI in several minutes instead of writing scripts in some

Fig. 4. Robust feature extraction. The left one is an example of ROI
perturbation. The middle and right ones are visualizations of perilesional
features.

programming languages, which is non-trivial for radiologists
with limited coding experience. Once the graph is established,
we can still easily modify it by adding/deleting or replacing
some nodes on it.

A. Graph Nodes

Each node in the graph is a basic computational unit, it has
speciﬁed input and output format. A node receives data from
one or more nodes and the output will be passed to several
nodes. When an error raised on one node, it does not have
inﬂuences on other independent ones. E.g., the error occurred
on leaf nodes will not inﬂuence others. Deep learning and
radiomics laboratories have separate classes of nodes.

1) Radiomics:

a) Dataloader: The dataloader class is used to load
image features. Users can choose to upload the existing
features or use the ’Annotated Data Input’ node to read the
feature information from our database which is generated by
the image labeler. The dataloader will also read the annotation

and linking information. The data loaded will be split into
a training set and a validation set randomly. Users can also
assign each image for training or validation manually.

b) Preprocessing: Data preprocessing is usually needed
before feeding data to models. Our platform provides more
than ﬁve different nodes for data preprocessing. The most
common ones are normalizing to [−1, 1] or [0, 1] which are
’Max-Abs Scaler’ and ’Min-Max Scaler’ respectively. We also
present ’Standard Scaler’ and ’Normalizer’ which represent
for projecting all data onto a unit ball and a standard normal
distribution separately. In addition, users are also allowed
to write a self-deﬁned transformation function in Python for
personal use in the ’Custom Transformer’ node.

c) Feature Selection: The radiomics feature extractor
will output more than 1300 dimensions of features which
is usually redundant. Feature selection is an important step
that will heavily inﬂuence the model performance. To select
out the most useful features, we provide numerous feature
selection methods. Speciﬁcally, we have ﬁve different kinds
of nodes. (i) Variance Threshold: choosing the features with
higher variance. (ii) Select K Best / Select K Percentile: users
can set how many features s/he wants and what measure to
use. The measure can be Chi-Square statistics, ANOVA-F
statistics, or mutual information [20] between each feature and
annotated label. (iii) Select From Model: selecting the features
by the weights from a model. We have L1 Lasso, SVM,
and many decision tree-based methods. (iv) Recursive Feature
Elimination: recursively decrease the feature size according to
a model. SVM and other tree-based methods are supported. (v)
Select Stable Feature: select k features that stable to different
feature selection algorithms. For any feature selection node,
one can single-click the node result to check the features name
and corresponding importance score they get from the node
when the computation ﬁnished.

d) Visualization: Heatmap and dimension reduction are
common ways to visualize the overall distribution of high
dimension(≥ 3) data. They will give a comprehensive im-
pression of the goodness of feature selection results. These
dimension reduction methods can also be used to show the
results of hierarchical data clustering which is a common
unsupervised method for data analyzing. We provided 2 nodes
for data visualization, heatmap and T-SNE [21].

e) Machine Learning Models: The key component in the
computational graph is to choose the best machine learning
model to learn the relationship between our processed features
and the human annotated labels. We have implemented many
classical classiﬁers including SVM, Logistic Regression, De-
cision Tree, Gradient Boosting Decision Tree, Random Forest,
XGboost, and Adaboost. As for the hyperparameter of each
model, we automatically apply grid search cross validation
to choose the best ones which will be very convenient for
those who are not so familiar with these ML models. After
training the model, We will draw the ROC curve and compute
the AUC and AP score for the model. Also, the p-values of
the hypothesis test for AUC and AP will be reported to show
statistical signiﬁcance.
2) Deep learning:

Radiomic Models

Classiﬁcation

Regression
Clustering

Deep Learning Models

Classiﬁcation

Detection

Segmentation

4

SVM
Logistic Regression
Decision Tree
Random Forest
GBDT
XGBoost
Nomogram
Kmeans
HDBSCAN
VGG-16,19
Resnet-18,34,50,101
Densnet-121,169
Resnext-50, 101
Incpetion V4
Inception-Resnet-V2
Xception
SSD
YOLOv2,v3
Faster RCNN
Mask RCNN
RetinaNet
U-net
FCN
Mask RCNN

TABLE I
MODELS PROVIDED IN DARWIN PLATFORM. BOTH 2D AND 3D IMAGES
ARE SUPPORTED

a) Image Input: Different from the radiomics module
which takes features as input, deep learning module has to take
images as input. The Image Input class provides two kinds of
nodes, which allows users to choose to use the whole images or
only ROI regions for the experiment. Similar to the radiomics
module, the training and validation set will be randomly split.
b) Preprocessing: Deep learning preprocessing class in-
cludes the normalization and standardization node similar
to radiomics. It also provides a feature-based image align-
ment node for multi-modal images. Augmentation is also an
essential part of deep learning experiments. We integrated
albumentations library [22] for 2D cases, and implement 3D
augmentation functions. More than 20 types of augmentation
including Flip, Transpose, Crop, Rotate, Blur, Elastic are
supported.

c) DL models: Our platform support all the 2D and
3D tasks of Detection, Segmentation, and Classiﬁcation. The
classiﬁcation node provides VGG, Resnet, Densenet, Resnext,
Inception, and Xception. The object detection node includes
Faster R-CNN, YOLO-v2, SSD, Mask RCNN, and RetinaNet.
The segmentation network includes U-net, Mask RCNN, FCN,
and its variants. Table I lists the network architectures currently
supported in our platform. More state of the art models will
be added in regularly updating.

d) Training: In the training class, users can choose to
upload the pretrain weights and then train or ﬁnetune the
network. All the hyper-parameters including the total epochs,
batch size, learning rate, learning rate scheduler, optimizer
can be manually selected. We also provide hyper-parameter
searching algorithms including random search, ﬁrst in ﬁrst
out (FIFO), and hyperband. These algorithms are implemented
with Auto-Gluon library [23]. During training, there will be a
graph shows information about GPU usage, estimated training
training and validation loss,
time, current
accuracy, AUC scores. This will help users to ﬁnd potential

iteration steps,

5

Shape

Firstorder

GLDM

Elongation
Flatness
LeastAxisLength
MajorAxisLength
Maximum2DDiameterColumn
Maximum2DDiameterRow
Maximum2DDiameterSlice
Maximum3DDiameter
MeshVolume
MinorAxisLength
Sphericity
SurfaceArea
SurfaceVolumeRatio
VoxelVolume
10Percentile
90Percentile
Energy
Entropy
InterquartileRange
Kurtosis
Maximum
Mean
MeanAbsoluteDeviation
Median
Minimum
Range
RobustMeanAbsoluteDeviation
RootMeanSquared
Skewness
TotalEnergy
Uniformity
Variance
DependenceEntropy
DependenceNonUniformity
DependenceNonUniformityNormalized
DependenceVariance
GrayLevelNonUniformity
GrayLevelVariance
HighGrayLevelEmphasis
LargeDependenceEmphasis
LargeDependenceHighGrayLevelEmphasis
LargeDependenceLowGrayLevelEmphasis
LowGrayLevelEmphasis
SmallDependenceEmphasis
SmallDependenceHighGrayLevelEmphasis
SmallDependenceLowGrayLevelEmphasis

GLCM

Autocorrelation
ClusterProminence
ClusterShade
ClusterTendency
Contrast
Correlation
DifferenceAverage
DifferenceEntropy
DifferenceVariance
Id
Idm
Idmn
Idn
Imc1
Imc2
InverseVariance
JointAverage
JointEnergy
JointEntropy
MCC
MaximumProbability
SumAverage
SumEntropy
SumSquares

GLRLM GrayLevelNonUniformity

GrayLevelNonUniformityNormalized
GrayLevelVariance
HighGrayLevelRunEmphasis
LongRunEmphasis
LongRunHighGrayLevelEmphasis
LongRunLowGrayLevelEmphasis
LowGrayLevelRunEmphasis
RunEntropy
RunLengthNonUniformity
RunLengthNonUniformityNormalized
RunPercentage
RunVariance
ShortRunEmphasis
ShortRunHighGrayLevelEmphasis
ShortRunLowGrayLevelEmphasis

GLSZM GrayLevelNonUniformity

NGTDM GrayLevelNonUniformity

GrayLevelNonUniformityNormalized
GrayLevelVariance
HighGrayLevelZoneEmphasis
LargeAreaEmphasis
LargeAreaHighGrayLevelEmphasis
LargeAreaLowGrayLevelEmphasis
LowGrayLevelZoneEmphasis
SizeZoneNonUniformity
SizeZoneNonUniformityNormalized
SmallAreaEmphasis
SmallAreaHighGrayLevelEmphasis
SmallAreaLowGrayLevelEmphasis
ZoneEntropy
ZonePercentage
ZoneVariance

GrayLevelNonUniformityNormalized
GrayLevelVariance
HighGrayLevelZoneEmphasis
LargeAreaEmphasis
LargeAreaHighGrayLevelEmphasis
LargeAreaLowGrayLevelEmphasis
LowGrayLevelZoneEmphasis
SizeZoneNonUniformity
SizeZoneNonUniformityNormalized
SmallAreaEmphasis
SmallAreaHighGrayLevelEmphasis
SmallAreaLowGrayLevelEmphasis
ZoneEntropy
ZonePercentage
ZoneVariance

TABLE II
RADIOMIC FEATURES PROVIDED IN DARWIN PLATFORM

errors in an early stage. Same as the radiomics module, users
are able to view the ROC curve, AUC score, confusion matrix
after training is completed.
e) Visualization:

In deep learning, a commonly used
way for visualization is class activation mapping (CAM). In
this node, users need to upload the weights, select the target
images, and specify the layer to visualize. The generated
heatmap will help physicians understand how the network
makes a decision, and could carry some potential underlying
messages with medical meanings.

f) Ensemble: Ensembling is a widely used trick in deep
learning competitions. Ensemble a group of models usually
gives better performance than any single one. Here we present
two kinds of ensemble methods: voting and averaging. Ra-
diomics models are also supported in this ensemble node.

B. Graph Building

There are three steps to build a graph, selecting the nodes,
dragging these nodes to the canvas, and connecting them by
mouse. Once the graph is built, the users only need to click the
’Run’ button. The graph will be sent to the server and the data
ﬂow will pass through each node sequentially until all nodes
have ﬁnished their computations. After that, one can click each
node to check the output of it. The connection between all
nodes is ﬂexible. For instance, one can feed the radiomics
feature to preprocessing nodes, feature selection nodes, and
machine learning nodes directly. To compare different feature
selection methods simultaneously, s/he could just feed the
radiomics features to several feature selection nodes parallelly
speciﬁed by different functions. The process is similar for
machine learning nodes when comparing different models.
Additionally, visualization nodes can receive a set of features
and corresponding values which means it can follow feature
nodes, preprocessing nodes, and feature selection nodes. Fig. 2
shows the completed computational graph for a radiomic
experiment.

C. Model Retrieval

Once an experiment completed, the computational graph
and corresponding model weights will be saved in the model
database. Users can retrieve the history results and compare
the performance. For each history record, users are allowed to
specify a new test set for the model test.

V. EXPERIMENTS

A. Radiomic Performance Evaluation

To verify the performance of the radiomic module in the
platform, we carried out a demo project of identifying an ROI
is on the left or the right lung. We randomly chose 19 images
from LIDC-IDRI [24] and manually annotated 424 3D ROIs.
The experiment runs on a server with 12 cores Intel i7-8700K
CPU @ 3.70GHz. The average time for extracting features of
one ROI is 1.39 seconds. We use 339 ROIs for training, 85
ROIs for validation.

We constructed a computational graph in Fig.5. Forty fea-
tures are selected from logistic regression, and then feed to

6

Fig. 5. Computational Graph for Lung ROI Classiﬁcation.

the SVM classiﬁer. The running time of this whole pipeline
is 234.25 seconds. Classiﬁcation results are shown in Fig.6.
It achieves a high performance with over 0.97 AUC and 0.99
average precision (AP) on the validation set.

Fig. 6. Lung ROI Classiﬁcation Results.

B. Deep Learning Performance Evaluation

We collected 10,701 lung CT scans from 10 hospitals
and constructed a model for lung nodules detection. The
computational graph is shown in Fig. 7. We randomly chose
260 scans for validation, the rest for training. A 3D RetinaNet

Fig. 7. Computational Graph for Lung Nodules Detection.

model trained from scratch for this experiment. The training
pipeline runs 8 hours on a sever with 8 TITAN RTX.

Fig.8 shows the Free-Response ROC (FROC) result. The
model achieves a high performance with sensitivity over 0.95
at 8 false positive per scan on the validation set.

7

C. Case Study: Identifying Recurrent Glioma from Radiation-
induced Temporal Lobe Necrosis in Contrast MRI

Doctors collected examinations of 83 patients for this study.
All the patients have tumor-shape areas in their CE-T1WI
after 2 weeks of operation. We balanced split 70% of the
examinations for training, the rest for test.

The computational graph is shown in Fig.9. The input 1223
dimension features are reduced to 100 by ANOVA F-value.
Doctors further selected 4 features based on the weights of
logistic regression and SVM.

Fig. 9. The Computational Graph for Glioma and Necrosis Classiﬁcation.

Fig.10 shows the 4 features and the corresponding feature
importance in SVM. Tumors usually have a higher level of
heterogeneity than necrosis, thus an ROI with higher entropy
is more likely to be a glioma. Maximal correlation coefﬁcient
(MCC) is a measure of the complexity of the texture. Skewness
describes the brightness distribution of the ROI. Strength is
an indication of the image primitives. The 4 features suggest
the images of temporal lobe necrosis have a higher level of
unbalance in distribution, and have more complex textures.
This is consistent with the histological features of the two
diseases. When the recurrent glioma occurs, tumor cells grow
rapidly and blood vessels are forming [25]. In radiation injury,
multiple factors including telangiectasias, thrombosis, ﬁbrinoid
necrosis of vessel walls, hyaline degeneration, and hemorrhage
are performing, which led the images with more complex con-
tents [26]. The AUC scores for the four features JointEntropy,
Strength, MCC, Skewness in glioma classiﬁcation are 0.78,
0.74, 0.74, 0.72 respectively.

Fig. 8. Lung Nodules Detection Results.

Fig. 10. Features Selected in Glioma and Necrosis Classiﬁcation.

D. Case Study: Predict the Invasibility of Adenocarcinoma in
Pure Ground-Glass Nodules.

In this project, doctors studied the invasibility of adeno-
carcinoma in pure ground-glass nodules (pGGNs). Doctors
collected the CT examinations of 136 patients, uploaded them
to our system, and manually segmented ROIs. Our feature
extractor automatically extracted 1223 features.

8

Fig. 11. The Computational Graph for pGGNs Prediction

Doctors divided the data into a training set (95 patients),
and a test set (41 patients). They constructed a computational
graph (Fig. 11) in the laboratory of our Darwin Research Plat-
form. The 1223 features are normalized into 0-1 for prepos-
sessing. Doctors used Select-K-Best and Recursive-Feature-
Elimination using LASSO for feature selection and ultimately
got 6 features. The SVM model is used for classiﬁcation.

The platform automatically generated the coefﬁcients and
loss paths in the feature selection period. It also presents
the coefﬁcients of the ﬁnal selected features. The heatmap
of the selected features is shown in Fig. 12. Table III and
Fig. 13 show the model performance on test set. It achieves
an accuracy of over 90%, which outperform the experienced
doctors.

Radiologist 1 (10-year experience)
Radiologist 2 (20-year experience)
SVM prediction

Accuracy
75.61%
80.49%
90.24%

Sensitivity
50.00%
75.00%
91.67%

Speciﬁcity
86.21%
82.76%
89.66%

TABLE III
PREFORMANCE OF THE SVM MODEL ON TEST SET

Fig. 12. Heatmap of the Selected Features in pGGNs prediction

Fig. 13. ROC curve of the SVM model on test set

effortless way for radiologists to carry out machine learning
experiments.

VI. CONCLUSION

REFERENCES

In this paper, we present the DARWIN research platform.
Radiologists could use the platform to analyze medical imag-
ing data. The DARWIN research platform streamlines the col-
lection, annotation, data preprocessing, model training, model
selection, and analysis. The platform provides various choices
for users with a user-friendly GUI. Physicians could conduct
their experiments just by clicking the mouse and dragging
the related icons. The platform provides ways to visualize the
high-dimension data, allowing users to comprehend the learn-
ing period straightforwardly. The generated experiment reports
also facilitate doctors with their research papers, enabling an

[1] S. M. McKinney, M. Sieniek, V. Godbole, J. Godwin, N. Antropova,
H. Ashraﬁan, T. Back, M. Chesus, G. C. Corrado, A. Darzi et al.,
“International evaluation of an ai system for breast cancer screening,”
Nature, vol. 577, no. 7788, pp. 89–94, 2020.

[2] A. Vial, D. Stirling, M. Field, M. Ros, C. Ritz, M. Carolan, L. Holloway,
and A. A. Miller, “The role of deep learning and radiomic feature ex-
traction in cancer-speciﬁc predictive modelling: a review,” Translational
Cancer Research, vol. 7, no. 3, pp. 803–816, 2018.

[3] A. Wibmer, H. Hricak, T. Gondo, K. Matsumoto, H. Veeraraghavan,
D. Fehr, J. Zheng, D. Goldman, C. Moskowitz, S. W. Fine et al.,
“Haralick texture analysis of prostate mri: utility for differentiating
non-cancerous prostate from prostate cancer and differentiating prostate
cancers with different gleason scores,” European radiology, vol. 25,
no. 10, pp. 2840–2850, 2015.

9

database resource initiative (idri): a completed reference database of
lung nodules on ct scans,” Medical physics, vol. 38, no. 2, pp. 915–931,
2011.

[25] A. Al Sayyari, R. Buckley, C. McHenery, K. Pannek, A. Coulthard, and
S. Rose, “Distinguishing recurrent primary brain tumor from radiation
injury: a preliminary study using a susceptibility-weighted mr imaging-
guided apparent diffusion coefﬁcient analysis strategy,” American jour-
nal of neuroradiology, vol. 31, no. 6, pp. 1049–1054, 2010.

[26] B. C. Oh, P. G. Pagnini, M. Y. Wang, C. Y. Liu, P. E. Kim, C. Yu,
and M. L. Apuzzo, “Stereotactic radiosurgery: Adjacent tissue injury
and response afterhigh-dose single fraction radiation: Part ihistology,
imaging, and molecularevents,” Neurosurgery, vol. 60, no. 1, pp. 31–45,
2007.

[4] A. Dekker, S. Vinod, L. Holloway, C. Oberije, A. George, G. Goozee,
G. P. Delaney, P. Lambin, and D. Thwaites, “Rapid learning in practice:
A lung cancer survival decision support system in routine patient care
data,” Radiotherapy and Oncology, vol. 113, no. 1, pp. 47–53, 2014.
[5] C. Parmar, P. Grossmann, D. Rietveld, M. M. Rietbergen, P. Lambin,
and H. J. Aerts, “Radiomic machine-learning classiﬁers for prognostic
biomarkers of head and neck cancer,” Frontiers in oncology, vol. 5, p.
272, 2015.

[6] S. H. Hawkins, J. N. Korecki, Y. Balagurunathan, Y. Gu, V. Kumar,
S. Basu, L. O. Hall, D. B. Goldgof, R. A. Gatenby, and R. J. Gillies,
“Predicting outcomes of nonsmall cell
image
features,” IEEE access, vol. 2, pp. 1418–1426, 2014.

lung cancer using ct

[7] C. Parmar, P. Grossmann, J. Bussink, P. Lambin, and H. J. Aerts, “Ma-
chine learning methods for quantitative radiomic biomarkers,” Scientiﬁc
reports, vol. 5, p. 13087, 2015.

[8] W. Shen, M. Zhou, F. Yang, C. Yang, and J. Tian, “Multi-scale convo-
lutional neural networks for lung nodule classiﬁcation,” in International
Conference on Information Processing in Medical Imaging. Springer,
2015, pp. 588–599.

[9] S. Hussein, K. Cao, Q. Song, and U. Bagci, “Risk stratiﬁcation of
lung nodules using 3d cnn-based multi-task learning,” in International
conference on information processing in medical imaging.
Springer,
2017, pp. 249–260.

[10] K. Yan, M. Bagheri, and R. M. Summers, “3d context enhanced region-
based convolutional neural network for end-to-end lesion detection,” in
International Conference on Medical Image Computing and Computer-
Assisted Intervention. Springer, 2018, pp. 511–519.

[11] K. Kamnitsas, C. Ledig, V. F. Newcombe, J. P. Simpson, A. D. Kane,
D. K. Menon, D. Rueckert, and B. Glocker, “Efﬁcient multi-scale 3d
cnn with fully connected crf for accurate brain lesion segmentation,”
Medical image analysis, vol. 36, pp. 61–78, 2017.

[12] P. F. Christ, M. E. A. Elshaer, F. Ettlinger, S. Tatavarty, M. Bickel,
P. Bilic, M. Rempﬂer, M. Armbruster, F. Hofmann, M. DAnastasi et al.,
“Automatic liver and lesion segmentation in ct using cascaded fully
convolutional neural networks and 3d conditional random ﬁelds,” in
International Conference on Medical Image Computing and Computer-
Assisted Intervention. Springer, 2016, pp. 415–423.

[13] G. Balakrishnan, A. Zhao, M. R. Sabuncu, J. Guttag, and A. V.
Dalca, “An unsupervised learning model for deformable medical image
registration,” in Proceedings of the IEEE conference on computer vision
and pattern recognition, 2018, pp. 9252–9260.

[14] Y. D. Cid, J. Castelli, R. Schaer, N. Scher, A. Pomoni, J. O. Prior, and
A. Depeursinge, “Quantimage: an online tool for high-throughput 3d
radiomics feature extraction in pet-ct,” in Biomedical Texture Analysis.
Elsevier, 2017, pp. 349–377.

[15] L. Zhang, D. Fried, X. Fave, L. Hunter, J. Yang, and L. E. Court, “Ibex:
An open infrastructure software platform to facilitate collaborative work
in radiomics,” Medical Physics, vol. 42, no. 3, pp. 1341–1353, 2015.

[16] C. Nioche, F. Orlhac, S. Boughdad, S. Reuze, J. Goyaouti, C. Robert,
C. Pellotbarakat, M. Soussan, F. Frouin, and I. Buvat, “Lifex: A
freeware for radiomic feature calculation in multimodality imaging to
accelerate advances in the characterization of tumor heterogeneity,”
Cancer Research, vol. 78, no. 16, pp. 4786–4789, 2018.

[17] R. Yuan, S. Shi, J. Chen, and G. Cheng, “Radiomics in rayplus: a web-
based tool for texture analysis in medical images,” Journal of Digital
Imaging, vol. 32, no. 2, pp. 269–275, 2019.

[18] J. J. Van Griethuysen, A. Fedorov, C. Parmar, A. Hosny, N. Aucoin,
V. Narayan, R. G. Beets-Tan, J.-C. Fillion-Robin, S. Pieper, and H. J.
Aerts, “Computational radiomics system to decode the radiographic
phenotype,” Cancer research, vol. 77, no. 21, pp. e104–e107, 2017.
[19] R. Adams and L. Bischof, “Seeded region growing,” IEEE Transactions
on pattern analysis and machine intelligence, vol. 16, no. 6, pp. 641–
647, 1994.

[20] R. G. Lomax, Statistical concepts: A second course. Lawrence Erlbaum

Associates Publishers, 2007.

[21] L. v. d. Maaten and G. Hinton, “Visualizing data using t-sne,” Journal

of machine learning research, vol. 9, no. Nov, pp. 2579–2605, 2008.

[22] A. V. Buslaev, A. Parinov, E. Khvedchenya, V. Iglovikov, and A. A.
Kalinin, “Albumentations: Fast and ﬂexible image augmentations,”
Information-an International Interdisciplinary Journal, vol. 11, no. 2,
p. 125, 2020.

[23] N. Erickson, J. Mueller, A. Shirkov, H. Zhang, P. Larroy, M. Li, and
A. Smola, “Autogluon-tabular: Robust and accurate automl for structured
data,” arXiv preprint arXiv:2003.06505, 2020.

[24] S. G. Armato III, G. McLennan, L. Bidaut, M. F. McNitt-Gray, C. R.
Meyer, A. P. Reeves, B. Zhao, D. R. Aberle, C. I. Henschke, E. A.
Hoffman et al., “The lung image database consortium (lidc) and image

