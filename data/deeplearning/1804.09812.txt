Improved Classiﬁcation Based on Deep Belief
Networks

Jaehoon Koo
Northwestern University
Evanston, IL, USA
jaehoonkoo2018@u.northwestern.edu

Diego Klabjan
Northwestern University
Evanston, IL, USA
d-klabjan@northwestern.edu

9
1
0
2

g
u
A
2
1

]
L
M

.
t
a
t
s
[

2
v
2
1
8
9
0
.
4
0
8
1
:
v
i
X
r
a

Abstract—For better classiﬁcation generative models are used
to initialize the model and model features before training a
classiﬁer. Typically it is needed to solve separate unsupervised and
supervised learning problems. Generative restricted Boltzmann
machines and deep belief networks are widely used for unsuper-
vised learning. We developed several supervised models based
on DBN in order to improve this two-phase strategy. Modifying
the loss function to account for expectation with respect to
the underlying generative model,
introducing weight bounds,
and multi-level programming are applied in model development.
The proposed models capture both unsupervised and supervised
objectives effectively. The computational study veriﬁes that our
models perform better than the two-phase training approach.
Index Terms—deep learning, neural networks, classiﬁcation

I. INTRODUCTION

to deﬁne an input distribution,

Restricted Boltzmann machine (RBM), an energy-based
model
is widely used to
extract latent features before classiﬁcation. Such an approach
combines unsupervised learning for feature modeling and
supervised learning for classiﬁcation. Two training steps are
needed. The ﬁrst step, called pre-training, is to model features
used for classiﬁcation. This can be done by training RBM that
captures the distribution of input. The second step, called ﬁne-
tuning, is to train a separate classiﬁer based on the features
from the ﬁrst step [1]. This two-phase training approach for
classiﬁcation is also used for deep networks. Deep belief
networks (DBN) are built with stacked RBMs, and trained in
a layer-wise manner [2]. Two-phase training based on a deep
network consists of DBN and a classiﬁer on top of it.

The two-phase training strategy has three possible problems.
1) It requires two training processes; one for training RBMs
is not guaranteed
and one for training a classiﬁer. 2) It
that the modeled features in the ﬁrst step are useful in the
classiﬁcation phase since they are obtained independently of
the classiﬁcation task. 3) It
is an effort to decide which
classiﬁer is the best for each problem. Therefore, there is a
need for a method that can conduct feature modeling and
classiﬁcation concurrently [1].

To resolve these problems, recent papers suggest to trans-
form RBM to a model that can deal with both unsupervised
and supervised learning. Since RBM calculate the joint and
conditional probabilities, the suggested prior models combine
this
a generative and discriminative RBM. Consequently,

hybrid discriminative RBM is trained concurrently for both
objectives by summing the two contributions [1], [3]. In a sim-
ilar way a self-contained RBM for classiﬁcation is developed
by applying the free-energy function based approximation to
RBM, which was used for a supervised learning method,
reinforcement learning [4]. However, these approaches are
limited to transforming RBM that is a shallow network.

In this study, we developed alternative models to solve a
classiﬁcation problem based on DBN. Viewing the two-phase
training as two separate optimization problems, we applied
optimization modeling techniques in developing our models.
Our ﬁrst approach is to design new objective functions. We
design an expected loss function based on p(h|x) built by DBN
and the loss function of the classiﬁer. Second, we introduce
constraints that bound the DBN weights in the feed-forward
phase. The constraints keep a good representation of input
as well as regularize the weights during updates. Third, we
applied bilevel programming to the two-phase training method.
The bilevel model has a loss function of the classiﬁer in
its objective function but it constrains the DBN values to
the optimal to phase-1. This model searches possible optimal
solutions for the classiﬁcation objective only where DBN
objective solutions are optimal.

Our main contributions are several classiﬁcation models
combining DBN and a loss function in a coherent way. In
the computational study we verify that the suggested models
perform better than the two-phase method.

II. LITERATURE REVIEW

The two-phase training strategy has been applied to many
classiﬁcation tasks on different types of data. Two-phase train-
ing with RBM and support vector machine (SVM) has been
explored in classiﬁcation tasks on images, documents, and
network intrusion data [5], [6], [7], [8]. Logistic regression
replacing SVM has been explored [9], [10]. Gehler et al. [11]
used the 1-nearest neighborhood classiﬁer with RBM to solve
a document classiﬁcation task. Hinton and Salakhutdinov [2]
suggested DBN consisting of stacked RBMs that is trained in
a layer-wise manner. Two-phase method using DBN and deep
neural network has been studied to solve various classiﬁcation
problems such as image and text recognition [2], [12], [13].
Recently, this approach has been applied to motor imagery

 
 
 
 
 
 
classiﬁcation in the area of brain–computer interface [14],
biomedical research, classiﬁcation of Cytochrome P450 1A2
inhibitors and non-inhibitors [15], and web spam classiﬁcation
that detects web pages deliberately created to manipulate
search rankings
[16]. All these papers rely on two distinct
phases, while our models assume a holistic view of both
aspects.

Many studies have been conducted to improve the problems
of two-phase training. Most of the research has been focused
on transforming RBM so that the modiﬁed model can achieve
generative and discriminative objectives at the same time.
Schmah et al. [17] proposed a discriminative RBM method,
and subsequently classiﬁcation is done in the manner of a
Bayes classiﬁer. However, this method cannot capture the
relationship between the classes since the RBM of each class
is trained separately. Larochelle et al. [1], [3] proposed a self-
contained discriminative RBM framework where the objective
function consists of the generative learning objective p(x, y),
and the discriminative learning objective, p(y|x). Both dis-
tributions are derived from RBM. Similarly, a self-contained
discriminative RBM method for classiﬁcation is proposed [4].
The free-energy function based approximation is applied in
the development of this method, which is initially suggested
for reinforcement learning. This prior paper relying on RBM
conditional probability while we handle general loss functions.
Our models also hinge on completely different principles.

III. BACKGROUND

a) Restricted Boltzmann Machines: RBM is an energy-
based probabilistic model, which is a restricted version of
Boltzmann machines (BM) that is a log-linear Markov Ran-
dom Field. It has visible nodes x corresponding to input
and hidden nodes h matching the latent features. The joint
distribution of the visible nodes x ∈ RJ and hidden variable
h ∈ RI is deﬁned as

p(x, h) =

1
Z

e−E(x,h), E(x, h) = −hW x − ch − bx

where W ∈ RI×J , b ∈ RJ , and c ∈ RI are the model
parameters, and Z is the partition function. Since units in a
layer are independent in RBM, we have the following form of
conditional distributions:

I

J

p(h|x) =

p(hi|x), p(x|h) =

p(xj|h).

i=1
Y

j=1
Y

For binary units where x ∈ {0, 1}J and h ∈ {0, 1}I, we
can write p(hi = 1|h) = σ(ci + Wix) and p(xj = 1|h) =
σ(bj +Wjx) where σ() is the sigmoid function. In this manner
RBM with binary units is an unsupervised neural network
with a sigmoid activation function. The model calibration
of RBM can be done by minimizing negative log-likelihood
through gradient descent. RBM takes advantage of having the
above conditional probabilities which enable to obtain model
samples easier through a Gibbs sampling method. Contrastive
divergence (CD) makes Gibbs sampling even simpler: 1) start
a Markov chain with training samples, and 2) stop to obtain

samples after k steps. It is shown that CD with a few steps
performs effectively [18], [19].

b) Deep Belief Networks: DBN is a generative graphical
model consisting of stacked RBMs. Based on its deep structure
DBN can capture a hierarchical representation of input data.
Hinton et al. (2006) introduced DBN with a training algorithm
that greedily trains one layer at a time. Given visible unit x
and ℓ hidden layers the joint distribution is deﬁned as [18],
[20]

p(x, h1, · · · , hℓ) = p(hℓ−1, hℓ)

p(hk|hk+1)

p(x|h1).

ℓ−2

k=1
Y

!

Since each layer of DBN is constructed as RBM, training each
layer of DBN is the same as training a RBM.

Classiﬁcation is conducted by initializing a network through
DBN training
[12], [20]. A two-phase training can be
done sequentially by: 1) pre-training, unsupervised learning
of stacked RBM in a layer-wise manner, and 2) ﬁne-tuning,
supervised learning with a classiﬁer. Each phase requires
solving an optimization problem. Given training dataset D =
{(x(1), y(1)), . . . , (x(|D|), y(|D|))} with input x and label y, the
pre-training phase solves the following optimization problem
at each layer k

min
θk

1
|D|

|D|

i=1 h
X

−log p(x(i)

k ; θk)
i

where θk = (Wk, bk, ck) is the RBM model parameter that
denotes weights, visible bias, and hidden bias in the energy
function, and x(i)
is visible input to layer k corresponding
k
to input x(i). Note that in layer-wise updating manner we
need to solve ℓ of the problems from the bottom to the top
hidden layer. For the ﬁne-tuning phase we solve the following
optimization problem

min
φ

1
|D|

|D|

i=1 h
X

L(φ; y(i), h(x(i)))
i

(1)

where L() is a loss function, h denotes the ﬁnal hidden
layer ℓ, and φ denotes the parameters of the
features at
classiﬁer. Here for simplicity we write h(x(i)) = h(x(i)
ℓ ).
When combining DBN and a feed-forward neural networks
(FFN) with sigmoid activation, all the weights and hidden bias
parameters among input and hidden layers are shared for both
training phases. Therefore, in this case we initialize FFN by
training DBN.

IV. PROPOSED MODELS

We model an expected loss function for classiﬁcation.
Considering classiﬁcation of two phase method is conducted
on hidden space, the probability distribution of the hidden
variables obtained by DBN is used in the proposed models.
The two-phase method provides information about modeling
parameters after each phase is trained. Constraints based on
the information are suggested to prevent the model param-
eters from deviating far from good representation of input.

 
Optimal solution set for unsupervised objective of the two-
phase method is good candidate solutions for the second phase.
Bilevel model has the set to ﬁnd optimal solutions for the
phase-2 objective so that it conducts the two-phase training at
one-shot.

a) DBN Fitting Plus Loss Model: We start with a naive
model of summing pre-trainning and ﬁne-tuning objectives.
This model conducts the two-phase training strategy simul-
taneously; however, we need to add one more hyperparam-
eter ρ to balance the impact of both objectives. The model
(DBN+loss) is deﬁned as

min
θL,θDBN

Ey,x[L(θL; y, h(x))] + ρ Ex[− log p(x; θDBN )]

and empirically based on training samples D,

has a constraint that the DBN parameters are bounded by
their optimal values at the end of both phases. This model
regularizes parameters with those that are ﬁtted in both the
unsupervised and supervised manner. Therefore, it can achieve
better accuracy even though we need an additional training to
the two-phase trainings. Given training samples D the model
(EL-DBNOPT) reads

min
θL,θDBN

s.t.

|D|

1
|D|
i=1 "
h
X
X
|θDBN − θ∗
DBN −OP T | ≤ δ

p(h|x(i))L(θL; y(i), h(θDBN ; x(i)))
#

(3)
DBN −OP T are the optimal values of DBN parameters

where θ∗
after two-phase training and δ is a hyperparameter.

min
θL,θDBN

1
|D|

|D|

L(θL; y(i), h(x(i))) − ρ log p(x(i); θDBN )

i=1 h
X

i(2)
where θL, θDBN are the underlying parameters. Note that
θL = φ from (1) and θDBN = (θk)k=1. This model has
already been proposed if the classiﬁcation loss function is
based on the RBM conditional distribution [1], [3].

b) Expected Loss Model with DBN Boxing: We ﬁrst de-
sign an expected loss model based on conditional distribution
p(h|x) obtained by DBN. This model conducts classiﬁcation
on the hidden space. Since it minimizes the expected loss, it
should be more robust and thus it should yield better accuracy
on data not observed. The mathematical model that minimizes
the expected loss function is deﬁned as

min
θL,θDBN

Ey,h|x[L(θL; y, h(θDBN ; x))]

and empirically based on training samples D,

|D|

min
θL,θDBN

1
|D|

i=1 "
X

h
X

p(h|x(i))L(θL; y(i), h(θDBN ; x(i)))
#

.

With notation h(θDBN ; x(i)) = h(x(i)) we explicitly show
the dependency of h on θDBN . We modify the expected
loss model by introducing a constraint that sets bounds on
DBN related parameters with respect to their optimal values.
This model has two beneﬁts. First, the model keeps a good
representation of input by constraining parameters ﬁtted in
the unsupervised manner. Also, the constraint regularizes the
model parameters by preventing them from blowing up while
being updated. Given training samples D the mathematical
form of the model (EL-DBN) reads

min
θL,θDBN

s.t.

|D|

1
|D|
i=1 "
h
X
X
|θDBN − θ∗
DBN | ≤ δ

p(h|x(i))L(θL; y(i), h(θDBN ; x(i)))
#

where θ∗
DBN are the optimal DBN parameters and δ is a
hyperparameter. This model needs a pre-training phase to
obtain the DBN ﬁtted parameters.

c) Expected Loss Model with DBN Classiﬁcation Boxing:
Similar to the DBN boxing model, this expected loss model

d) Feed-forward Network with DBN Boxing: We also
propose a model based on boxing constraints where FFN is
constrained by DBN output. The mathematical model (FFN-
DBN) based on training samples D is

min
θL,θDBN

s.t.

|D|

1
|D|
i=1 h
X
|θDBN − θ∗

L(θL; y(i), h(θDBN ; x(i)))
i

DBN | ≤ δ.

(4)

e) Feed-forward Network with DBN Classiﬁcation Box-
ing: Given training samples D this model (FFN-DBNOPT),
which is a mixture of (3) and (4), reads

min
θL,θDBN

s.t.

|D|

1
|D|
i=1 h
X
|θDBN − θ∗

L(θL; y(i), h(θDBN ; x(i)))
i

DBN −OP T | ≤ δ.

f) Bilevel Model: We also apply bilevel programming to
the two-phase training method. This model searches optimal
solutions to minimize the loss function of the classiﬁer only
where DBN objective solutions are optimal. Possible candi-
dates for optimal solutions of the ﬁrst level objective function
are optimal solutions of the second level objective function.
This model (BL) reads

min
θL,θ∗

DBN
s.t.

Ey,x[L(θL; y, h(θ∗

DBN ; x))]

θ∗
DBN = arg min

θDBN

Ex[−log p(x; θDBN )]

and empirically based on training samples,

min
θL,θ∗

DBN

1
|D|

|D|

i=1 h
X

L(θL; y(i), h(θ∗

DBN ; x(i)))
i

|D|

s.t.

θ∗
DBN = arg min

θDBN

1
|D|

−log p(x(i); θDBN )
i

.

i=1 h
X

One of the solution approaches to bilevel programming is to
apply Karush–Kuhn–Tucker (KKT) conditions to the lower
level problem. After applying KKT to the lower level, we

obtain

min
θL,θ∗

DBN
s.t.

Ey,x[L(θL; y, h(θ∗

DBN ; x))]

∇θDBN Ex[−log p(x; θDBN )|θ∗

DBN ] = 0.

Furthermore, we transform this constrained problem to an
unconstrained problem with a quadratic penalty function:

min
θL,θ∗

DBN

Ey,x[L(θL; y, h(θ∗

DBN ; x))]+

µ
2

||∇θDBN Ex[−log p(x; θDBN )]|θ∗

DBN ||2

Sd.

Mean
1.33% 0.03%
DBN-FFN
1.84% 0.14%
DBN+loss
1.46% 0.05%
EL-DBN
1.33% 0.04%
EL-DBNOPT
FFN-DBN
1.34% 0.04%
FFN-DBNOPT 1.32% 0.03%
1.85% 0.07%
BL

(5)

TABLE I: Classiﬁcation errors with respect to the best DBN
structure for the MNIST.

where µ is a hyperparameter. The gradient of the objective
function is derived in the appendix.

V. COMPUTATIONAL STUDY

To evaluate the proposed models classiﬁcation tasks on three
datasets were conducted: the MNIST hand-written images 1,
the KDD’99 network intrusion dataset (NI)2, and the isolated
letter speech recognition dataset (ISOLET) 3. The experimen-
tal results of the proposed models on these datasets were
compared to those of the two-phase method.

In FFN, the sigmoid functions in the hidden layers and
the softmax function in the output layer were chosen with
negative log-likelihood as a loss function of the classiﬁers.
We selected the hyperparameters based on the settings used
in [21], which were ﬁne-tuned. We ﬁrst implemented the two-
phase method with DBNs of 1, 2, 3 and 4 hidden layers to
ﬁnd the best conﬁguration for each dataset, and then applied
the best conﬁguration to the proposed models.

Implementations were done in Theano using GeForce GTX
TITAN X. The mini-batch gradient descent algorithm was used
to solve the optimization problems of each model. To calculate
the gradients of each objective function of the models Theano’s
built-in functions, ’theano.tensor.grad’, was used. We denote
by DBN-FFN the two-phase approach.

A. MNIST

The task on the MNIST is to classify ten digits from 0 to
9 given by 28 × 28 pixel hand-written images. The dataset
is divided in 60,000 samples for training and validation, and
10,000 samples for testing. The hyperparameters are set as:
there are 1,000 hidden units in each layer; the number of pre-
training epochs per layer is 100 with the learning rate of 0.01;
the number of ﬁne-tuning epochs is 300 with the learning rate
of 0.1; the batch size is 10; and ρ in the DBN+loss and µ
in the BL model are diminishing during epochs. Note that
DBN+Loss and BL do not require pre-training.

DBN-FFN with three-hidden layers of size, 784-1000-1000-
1000-10, was the best, and subsequently we compared it to the
proposed models with the same size of the network. We com-
puted the means of the classiﬁcation errors and their standard
deviations for each model averaged over 5 random runs. In
each table, we stressed in bold the best three models with

1http://yann.lecun.com/exdb/mnist/
2http://kdd.ics.uci.edu/databases/kddcup99/kddcup99.html
3https://archive.ics.uci.edu/ml/datasets/ISOLET

ties broken by deviation. In Table 1, the best mean test error
rate was achieved by FFN-DBNOPT, 1.32%. Furthermore, the
models with the DBN classiﬁcation constraints, EL-DBNOPT
and FFN-DBNOPT, perform similar to, or better than the
two-phase method. This shows that DBN classiﬁcation boxing
constraints regularize the model parameters by keeping a good
representation of input.

B. Network Intrusion

The classiﬁcation task on NI is to distinguish between nor-
mal and bad connections given the related network connection
information. The preprocessed dataset consists of 41 input
features and 5 classes, and 4,898,431 examples for training and
311,029 examples for testing. The experiments were conducted
on 20%, 30%, and 40% subsets of the whole training set,
which were obtained by stratiﬁed random sampling. The
hyperparameters are set as: there are 15 hidden units in each
layer; the number of pre-training epochs per layer is 100 with
the learning rate of 0.01; the number of ﬁne-tuning epochs is
500 with the learning rate of 0.1; the batch size is 1,000; and
ρ in the DBN+loss and µ in the BL model are diminishing
during epochs.

On NI the best structure of DBN-FFN was 41-15-15-5 for
all three datasets. Table 2 shows the experimental results of the
proposed models with the same network as the best DBN-FFN.
BL performed the best in all datasets, achieving the lowest
mean classiﬁcation error without the pre-training step. The
difference in the classiﬁcation error between our best model,
BL, and DBN-FFN is statistically signiﬁcant since the p-values
are 0.03, 0.01, and 0.03 for 20%, 30%, and 40% datasets,
respectively. This showed that the model being trained con-
currently for unsupervised and supervised purpose can achieve
better accuracy than the two-phase method. Furthermore, both
EL-DBNOPT and FFN-DBNOPT yield similar to, or lower
mean error rates than DBN-FFN in all of the three subsets.

C. ISOLET

The classiﬁcation on ISOLET is to predict which letter-
name was spoken among the 26 English alphabets given 617
input features of the related signal processing information. The
dataset consists of 5,600 for training, 638 for validation, and
1,559 examples for testing. The hyperparameters are set as:
there are 1,000 hidden units in each layer; the number of pre-
training epochs per layer is 100 with the learning rate of 0.005;

20% dataset
Sd.

30% dataset
Sd.

Mean

Mean
8.14% 0.12% 8.18% 0.12%
DBN-FFN
8.07% 0.06% 8.13% 0.09%
DBN+loss
8.30% 0.09% 8.27% 0.07%
EL-DBN
8.14% 0.14% 8.15% 0.15%
EL-DBNOPT
FFN-DBN
8.17% 0.09% 8.20% 0.08%
FFN-DBNOPT 8.07% 0.12% 8.12% 0.11%
7.93% 0.09% 7.90% 0.11%
BL
40% dataset
Sd.

Mean
DBN-FFN
8.06% 0.02%
8.05% 0.05%
DBN+loss
EL-DBN
8.29% 0.14%
EL-DBNOPT
8.08% 0.10%
8.07% 0.11%
FFN-DBN
FFN-DBNOPT 7.95% 0.11%
7.89% 0.10%
BL

FFN-DBN, do not perform better than DBN-FFN in almost all
datasets. Regularizing the model parameters with unsupervised
learning is not so effective in solving a supervised learning
problem. Third, the models with DBN classiﬁcation boxing,
EL-DBNOPT and FFN-DBNOPT, perform better than DBN-
FFN in almost all of the experiments. FFN-DBNOPT is
consistently one of the best three performers in all instances.
This shows that classiﬁcation accuracy can be improved by
regularizing the model parameters with the values trained for
unsupervised and supervised purpose. One drawback of this
approach is that one more training phase to the two-phase ap-
proach is necessary. Last, BL shows that one-step training can
achieve a better performance than two-phase training. Even
though it worked in one instance, improvements to current
BL can be made such as applying different solution search
algorithms, supervised learning regularization techniques, or
different initialization strategies.

TABLE II: Classiﬁcation errors with respect to the best DBN
structure for NI

REFERENCES

Sd.

Mean
3.94% 0.22%
DBN-FFN
4.38% 0.20%
DBN+loss
3.91% 0.18%
EL-DBN
3.75% 0.14%
EL-DBNOPT
FFN-DBN
3.94% 0.19%
FFN-DBNOPT 3.75% 0.13%
4.43% 0.18%
BL

TABLE III: Classiﬁcation errors with respect to the best DBN
structure for ISOLET.

the number of ﬁne-tuning epochs is 300 with the learning rate
of 0.1; the batch size is 20; and ρ in the DBN+loss and µ in
the BL model are diminishing during epochs.

In this experiment the shallow network performed better
than the deep network; 617-1000-26 was the best structure for
DBN-FFN. One possible reason for this is its small size of
training samples. EL models performed great for this instance.
EL-DBNOPT achieved the best mean classiﬁcation error, tied
with FFN-DBNOPT. With the same training effort, EL-DBN
achieved a lower mean classiﬁcation error and smaller standard
deviation than the two-phase method, DBN-FFN. Considering
a relatively small sample size of ISOLET, EL shows that it
yields better accuracy on unseen data as it minimizes the
expected loss, i.e., it generalizes better. In this data set, p-value
is 0.07 for the difference in the classiﬁcation error between our
best model, FFN-DBNOPT, and DBN-FFN.

VI. CONCLUSIONS

DBN+loss performs better than two-phase training DBN-
FFN only in one instance. Aggregating two unsupervised
and supervised objectives without a speciﬁc treatment is not
effective. Second, the models with DBN boxing, EL-DBN and

[1] H. Larochelle, M. Mandel, R. Pascanu, and Y. Bengio, “Learning
algorithms for the classiﬁcation restricted Boltzmann machine,” Journal
of Machine Learning Research, vol. 13, pp. 643–669, 2012.

[2] G. E. Hinton and R. R. Salakhutdinov, “Reducing the dimensionality of
data with neural networks,” Science, vol. 313, no. 5786, pp. 504–507,
2006.

[3] H. Larochelle and Y. Bengio, “Classiﬁcation using discriminative re-
stricted Boltzmann machines,” in International Conference on Machine
Learning (ICML) 25, (Helsinki, Finland), pp. 536–543, 2008.

[4] S. Elfwing, E. Uchibe, and K. Doya, “Expected energy-based restricted
Boltzmann machine for classiﬁcation,” Neural Networks, vol. 64, pp. 29–
38, 2015.

[5] E. P. Xing, R. Yan, and A. G. Hauptmann, “Mining associated text and
images with dual-wing Harmoniums,” in Conference on Uncertainty in
Artiﬁcial Intelligence (UAI), vol. 21, (Edinburgh, Scotland), pp. 633–
641, 2005.

[6] M. Norouzi, M. Ranjbar, and G. Mori, “Stacks of convolutional restricted
Boltzmann machines for shift-invariant feature learning,” in IEEE Com-
puter Society Conference on Computer Vision and Pattern Recognition
Workshops (CVPR), (Miami, FL, USA), pp. 2735–2742, 2009.

[7] M. A. Salama, H. F. Eid, R. A. Ramadan, A. Darwish, and A. E.
Hassanien, “Hybrid intelligent intrusion detection scheme,” Advances
in Intelligent and Soft Computing, pp. 293–303, 2011.

[8] G. E. Dahl, R. P. Adams, and H. Larochelle, “Training restricted
Boltzmann machines on word observations,” in International Conference
on Machine Learning (ICML) 29, vol. 29, (Edinburgh, Scotland, UK),
pp. 679–686, 2012.

[9] A. Mccallum, C. Pal, G. Druck, and X. Wang, “Multi-conditional
learning : generative / discriminative training for clustering and classiﬁ-
cation,” in National Conference on Artiﬁcial Intelligence (AAAI), vol. 21,
pp. 433–439, 2006.

[10] K. Cho, A. Ilin, and T. Raiko, “Improved learning algorithms for
restricted Boltzmann machines,” in Artiﬁcial Neural Networks and
Machine Learning (ICANN), vol. 6791, Springer, Berlin, Heidelberg,
2011.

[11] P. V. Gehler, A. D. Holub, and MaxWelling, “The rate adapting Poisson
(RAP) model for information retrieval and object recognition,” in
International Conference on Machine Learning (ICML) 23, vol. 23,
(Pittsburgh, PA, USA), pp. 337–344, 2006.

[12] Y. Bengio and P. Lamblin, “Greedy layer-wise training of deep net-
works,” in Advances in Neural Information Processing Systems (NIPS)
19, vol. 20, pp. 153–160, MIT Press, 2007.

[13] R. Sarikaya, G. E. Hinton, and A. Deoras, “Application of deep belief
networks for natural language understanding,” IEEE/ACM Transactions
on Audio, Speech, and Language Processing, vol. 22, no. 4, pp. 778–
784, 2014.

c) Bilevel Model: From Eq. (5), ∇θDBN log p(x) in the

objective function is approximated for i = 0, 1, · · · , ℓ as

[∇θDBN log p(x)]i =

∂ log p(x)
∂ θi

DBN

h1,h2,··· ,hℓ p(x, h1, h2, · · · , hℓ)
(cid:17)

∂ log

∂ log (

=

≈

(cid:16)P

∂ θi
DBN
hi+1 p(hi, hi+1))
∂ θi

DBN

(6)
where θDBN = (θ0
DBN ). The
gradient of this approximated quantity is then the Hessian
matrix of the underlying RBM.

P
DBN , · · · , θi
DBN , θ2

DBN , · · · θℓ

[14] N. Lu, T. Li, X. Ren, and H. Miao, “A deep learning scheme for motor
imagery classiﬁcation based on restricted Boltzmann machines,” IEEE
Transactions on Neural Systems and Rehabilitation Engineering, vol. 25,
pp. 566–576, 2017.

[15] L. Yu, X. Shi, and T. Shengwei, “Classiﬁcation of Cytochrome P450
1A2 of inhibitors and noninhibitors based on deep belief network,”
International Journal of Computational Intelligence and Applications,
vol. 16, p. 1750002, 2017.

[16] Y. Li, X. Nie, and R. Huang, “Web spam classiﬁcation method based on
deep belief networks,” Expert Systems With Applications, vol. 96, no. 1,
pp. 261–270, 2018.

[17] T. Schmah, G. E. Hinton, R. S. Zemel, S. L. Small, and S. Strother,
“Generative versus discriminative training of RBMs for classiﬁcation of
fMRI images,” in Advances in Neural Information Processing Systems
(NIPS) 21, vol. 21, pp. 1409–1416, Curran Associates, Inc., 2009.
[18] Y. Bengio, “Learning deep architectures for AI,” Foundations and Trends

in Machine Learning, vol. 2, no. 1, pp. 1–127, 2009.

[19] G. E. Hinton, “Training products of experts by minimizing contrastive
divergence.,” Neural computation, vol. 14, no. 8, pp. 1771–1800, 2002.
[20] G. E. Hinton, S. Osindero, and Y. W. Teh, “A fast learning algorithm
for deep belief nets.,” Neural computation, vol. 18, no. 7, pp. 1527–54,
2006.

[21] B. Wang and D. Klabjan, “Regularization for unsupervised deep neural
nets,” in National Conference on Artiﬁcial Intelligence (AAAI), vol. 31,
pp. 2681–2687, 2017.

[22] A. Fischer and C. Igel, “An introduction to restricted Boltzmann
machines,” Progress in Pattern Recognition, Image Analysis, Computer
Vision, and Applications, vol. 7441, pp. 14–36, 2012.

We write the approximated ||∇θDBN − log p(x)||2 at the

layer i as

||[∇θDBN − log p(x)]i||2 ≈ ||

∂ − log (

hi+1 p(hi, hi+1))

||2

APPENDIX

∂ − log p(hi)
∂θi
11

2

+

(cid:19)

=

" (cid:18)

∂ θi
P
∂ − log p(hi)
∂θi
12

DBN
2

+

(cid:19)

(cid:18)
2

DBN deﬁnes the joint distribution of the visible unit x and

the ℓ hidden layers, h1, h2, · · · , hℓ as

p(x, h1, · · · , hℓ) = p(hℓ−1, hℓ)

with h0 = x.

ℓ−2

k=0
Y

p(hk|hk+1)

!

a) DBN Fitting Plus Loss Model: From Eq. (2), p(x) in
the second term of the objective function is approximated as

p(x; θDBN ) =

p(x, h1, · · · , hℓ) ≈

p(x, h1).

Xh1,h2,··· ,hℓ

Xh1

b) Expected Loss Models: p(h|x) in the objective func-

tion is approximated as

p(hℓ|x) ≈ p(hℓ|x, h1, · · · , hℓ)

=

=

=

=

p(hℓ, hℓ−1, · · · , h1, x)
p(hℓ−1, hℓ−2, · · · , h1, x)
p(hℓ−1, hℓ)

p(hℓ−2, hℓ−1)

(cid:16)Q

(cid:16)Q
p(hℓ−1, hℓ)p(hℓ−2|hℓ−1)
p(hℓ−2, hℓ−1)
p(hℓ−1, hℓ)p(hℓ−2, hℓ−1)
p(hℓ−2, hℓ−1)p(hℓ−1)

ℓ−2
k=0 p(hk|hk+1)
(cid:17)
ℓ−3
k=0 p(hk|hk+1)
(cid:17)

= p(hℓ|hℓ−1).

∂ − log p(hi)
∂θi

nm

· · · +

(cid:18)

#

(cid:19)

where m and n denote dimensions of hi and hi+1 and θi
pq
denotes the pth and qth component of the θi
DBN . The gradient
of the approximated ||∇θDBN − log p(x)||2 at the layer i is

∂
θi
pq  

p,q (cid:18)
X

= 2

" (cid:18)

2

∂ − log p(hi)
∂θi
pq

(cid:19) (cid:18)

∂ − log p(hi)
∂θi
11
∂ − log p(hi)
∂θi
12
(cid:19) (cid:18)
∂ − log p(hi)
∂θi
pq
∂ − log p(hi)
∂θi

(cid:18)

nm

(cid:18)

!

∂θi

(cid:19)
∂2 − log p(hi)
11θi
pq
∂2 − log p(hi)
12∂θi

(cid:19)

+

+

∂θi
pq (cid:19)
∂2 − log p(hi)
pq∂θi
∂2 − log p(hi)
nmθi

∂θi

∂θi

pq (cid:19)

pq (cid:19) #

+

(cid:19) (cid:18)

(cid:19) (cid:18)

(cid:18)

· · · +

· · · +

for p = 1, ...n, q = 1, ...m. This shows that the gradient of
the approximated ||∇θDBN − log p(x)||2 in (5) is then the
Hessian matrix times the gradient of the underlying RBM. The
stochastic gradient of −log p(x) of RBM with binary input x
and hidden unit h with respect to θDBN wpq is

∂RBM
∂wpq

= p(hp = 1|x)xq −

p(x)p(hp = 1|x)xq

x
X

where RBM denotes −log p(x) [22]. We derive the Hessian

 
matrix with respect to wpq as

∂2RBM
∂w2
pq
∂
wpq

=

[p(hp = 1|x)xq)] −

= σ(

netp)(1 − σ(

netp))x2

netp)(1 − σ(
g

x
X
netp))x2
q],

p(x)σ(
g
∂2RBM
∂wpk∂wpq
∂
wpk

=

g

g

[p(hp = 1|x)xq)] −

= σ(

netp)(1 − σ(

netp))xqxk −

p(x)σ(
g

netp)(1 − σ(
g

x
X
netp))xqxk],

g

g

∂
wpq

[p(x)p(hp = 1|x)xq)]

[

∂p(x)
∂wpq

p(hp = 1|x)xq+

x
X
q −

∂
wpk

[

x
X

p(x)p(hp = 1|x)xq)]

[

∂p(x)
∂wpk

p(hp = 1|x)xq+

∂2RBM
∂wkq∂wpq
∂
wkq

=

= −

[

x
X
∂2RBM
∂wkp∂wpq

[p(hp = 1|x)xq)] −

p(x)p(hp = 1|x)xq]

∂
wkq

[

x
X

∂p(x)
∂wkq

p(hp = 1|x)xq + p(x)

∂
∂wkq

[p(hp = 1|x)xq]],

∂p(x)
∂wkp

= −

[

x
X

p(hp = 1|x)xq + p(x)]

where σ() is the sigmoid function,
q wpqxq +cp, and
cp is the hidden bias. Based on what we derive above we can
calculate the gradient of approximated ||[∇θDBN −logp(x)]i||2.

netp is

P

g

