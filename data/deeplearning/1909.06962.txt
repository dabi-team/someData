0
2
0
2

t
c
O
1

]

Y
S
.
s
s
e
e
[

2
v
2
6
9
6
0
.
9
0
9
1
:
v
i
X
r
a

Dynamic Pricing and Fleet Management for Electric

Autonomous Mobility on Demand Systems

Berkay Turan Ramtin Pedarsani Mahnoosh Alizadeh

Abstract

The proliferation of ride sharing systems is a major drive in the advancement of autonomous and electric

vehicle technologies. This paper considers the joint routing, battery charging, and pricing problem faced

by a proﬁt-maximizing transportation service provider that operates a ﬂeet of autonomous electric vehicles.

We ﬁrst establish the static planning problem by considering time-invariant system parameters and deter-

mine the optimal static policy. While the static policy provides stability of customer queues waiting for

rides even if consider the system dynamics, we see that it is inefﬁcient to utilize a static policy as it can lead

to long wait times for customers and low proﬁts. To accommodate for the stochastic nature of trip demands,

renewable energy availability, and electricity prices and to further optimally manage the autonomous ﬂeet

given the need to generate integer allocations, a real-time policy is required. The optimal real-time policy

that executes actions based on full state information of the system is the solution of a complex dynamic pro-

gram. However, we argue that it is intractable to exactly solve for the optimal policy using exact dynamic

programming methods and therefore apply deep reinforcement learning to develop a near-optimal control

policy. The two case studies we conducted in Manhattan and San Francisco demonstrate the efﬁcacy of our

real-time policy in terms of network stability and proﬁts, while keeping the queue lengths up to 200 times

less than the static policy.

Keywords— autonomous mobility-on-demand systems, optimization and optimal control, reinforcement

learning

1 Introduction

The rapid evolution of enabling technologies for autonomous driving coupled with advancements in eco-

friendly electric vehicles (EVs) has facilitated state-of-the-art transportation options for urban mobility. Ow-

ing to these developments in automation, it is possible for an autonomous-mobility-on-demand (AMoD)

ﬂeet of autonomous EVs to serve the society’s transportation needs, with multiple companies now heavily

investing in AMoD technology [1].

This work is supported by the NSF Grant 1847096. B. Turan, R. Pedarsani, and M. Alizadeh are with the Department of Electrical

and Computer Engineering, University of California, Santa Barbara, CA, 93106 USA e-mail:{bturan,ramtin,alizadeh}@ucsb.edu.

1

 
 
 
 
 
 
The introduction of autonomous vehicles for mobility on demand services provides an opportunity for
better ﬂeet management. Speciﬁcally, idle vehicles can be rebalanced throughout the network in order to
prevent accumulating at certain locations and to serve induced demand at every location. Autonomous ve-

hicles allow rebalancing to be performed centrally by a platform operator who observes the state of all the

vehicles and the demand, rather than locally by individual drivers. Furthermore, EVs provide opportunities

for cheap and environment-friendly energy resources (e.g., solar energy). However, electricity supplies and

prices differ among the network both geographically and temporally. As such, this diversity can be exploited

for cheaper energy options when the ﬂeet is operated by a platform operator that is aware of the electricity

prices throughout the whole network. Moreover, a dynamic pricing scheme for rides is essential to maximize

proﬁts earned by serving the customers. Coupling an optimal ﬂeet management policy with a dynamic pric-
ing scheme allows the revenues to be maximized while reducing the rebalancing cost and the waiting time of

the customers by adjusting the induced demand.

We consider a model that captures the opportunities and challenges of an AMoD ﬂeet of EVs, and con-

sists of complex state and action spaces. In particular, the platform operator has to consider the number of

customers waiting to be served at each location (ride request queue lengths), the electricity prices, trafﬁc

conditions, and the states of the EVs (locations, battery energy levels) in order to make decisions. These

decisions consist of pricing for rides for every origin-destination (OD) pair and routing/charging decision for

every vehicle in the network. Upon taking an action, the state of the network undergoes through a stochastic

transition due to the randomness in customer behaviour, electricity prices, and travel times.

We ﬁrst adopt the common approach of network ﬂow modeling to develop an optimal static pricing,

routing, and charging policy that we use as a baseline in this paper. However, ﬂow-based solutions generate

fractional ﬂows which can not directly be implemented. Moreover, a static policy executes same actions

independent of the network state and is oblivious to the stochastic events that occur in the real setting. Hence,

it is not optimal to utilize the static policy in a real dynamic environment. Therefore, a real-time policy that

generates integer solutions and acknowledges the network state is required, and can be determined by solving

the underlying dynamic program. Due to the continuous and high dimensional state-action spaces however, it

is infeasible to develop an optimal real-time policy using exact dynamic programming algorithms. As such,

we utilize deep reinforcement learning (RL) to develop a near-optimal policy. Speciﬁcally, we show that it is

possible to learn a policy via Proximal Policy Optimization (PPO) [2] that increases the total proﬁts generated

by jointly managing the ﬂeet of EVs (by making routing and charging decisions) and pricing for the rides.

We demonstrate the performance of our policy by using the total proﬁts generated and the queue lengths as

metrics.

Our contributions can be summarized as follows:

1. We formalize a vehicle and network model that captures the aforementioned characteristics of an

AMoD ﬂeet of EVs as well as the stochasticity in demand and electricity prices.

2. We analyze the static problem, where we consider a time-invariant environment (time-invariant arrivals,

electricity prices, etc.) to characterize the family of policies that guarantee stability of the dynamic

system, to gain insight towards the actual dynamic problem, and to further provide a baseline for

2

Figure 1: The schematic diagram of our framework. Our deep RL agent processes the state of the vehicles, queues and

electricity prices and outputs a control policy for pricing as well as autonomous EVs’ routing and charging.

comparison.

3. We employ deep RL methods to learn a joint pricing, routing and charging policy that effectively

stabilizes the queues and increases the proﬁts.

We visualize our real-time framework as a schematic diagram in Fig-

ure 1 and preview our results in Figure 2, showing that a real-time pric-

ing and routing policy can successfully keep the queue lengths 400 times
lower than the static policy. This policy is also able to decrease the charg-
ing costs by 25% by utilizing smart charging strategies (which will be
demonstrated in Section 5).

Related work: Comprehensive research perceiving various aspects of
AMoD systems is being conducted in the literature. Studies surround-

ing ﬂeet management focus on optimal EV charging in order to reduce

electricity costs as well as optimal vehicle routing in order to serve the

customers and to rebalance the empty vehicles throughout the network so

as to reduce the operational costs and the customers’ waiting times. Time-
invariant control policies adopting queueing theoretical [3], ﬂuidic [4],

network ﬂow [5], and Markovian [6] models have been developed by us-

ing the steady state of the system. The authors of [7] consider ride-sharing

systems with mixed autonomy. However, the proposed control policies

(a)

(b)

Figure 2: (a) The optimal static pol-

icy manages to stabilize the queues

over a very long time period but is

unable to clear them whereas (b) RL

control policy stabilizes the queues

in these papers are not adaptive to the time-varying nature of the future

and manages to keep them signiﬁ-

demand. As such, there is work on developing time-varying model pre-

cantly low (note the scales).

3

0.000.250.500.751.00Time(5mins)×1060246TotalQueueLength×103StaticPolicy0.000.250.500.751.00Time(5mins)×1060.000.250.500.751.00TotalQueueLength×103Real−TimePolicydictive control (MPC) algorithms [8–12]. The authors of [10, 11] propose data-driven algortihms and the

authors of [12] propose a stochastic MPC algorithm focusing on vehicle rebalancing. In [8], the authors also

consider a ﬂeet of EVs and hence propose an MPC approach that optimizes vehicle routing and scheduling

subject to energy constraints. Using a ﬂuid-based optimization framework, the authors of [13] investigate

tradeoffs between ﬂeet size, rebalancing cost, and queueing effects in terms of passenger and vehicle ﬂows

under time-varying demand. The authors in [14] develop a parametric controller that approximately solves

the intractable dynamic program for rebalancing over an inﬁnite-horizon. Similar to AMoD, carsharing sys-

tems also require rebalancing in order to operate efﬁciently. By adopting a Markovian model, the authors

of [15] introduce a dynamic proactive rebalancing algorithm for carsharing systems by taking into account an

estimate of the future demand using historical data. In [16], the authors develop an integrated multi-objective
mixed integer linear programming optimization and discrete event simulation framework to optimize vehicle

and personnel rebalancing in an electric carsharing system. Using a network-ﬂow based model, the authors

of [17] propose a two-stage approximation scheme to establish a real-time rebalancing algorithm for shared

mobility systems that accounts for stochasticity in customer demand and journey valuations.

Aside from these, there are studies on applications of RL methods in transportation such as adaptive

routing [18], trafﬁc management [19, 20], trafﬁc signal control [21, 22], and dynamic routing of autonomous

vehicles with the goal of reducing congestion in mixed autonomy trafﬁc networks [23]. Relevant studies to

our work aim to develop dynamic policies for rebalancing as well as ride request assignment via decentralized

reinforcement learning approaches [24–27]. In these works however, the policies are developed and applied

locally by each autonomous vehicle and this decentralized approach may sacriﬁce system level optimality. A

centralized deep RL approach tackling the rebalancing problem is proposed in [28], which is closest to the

approach we adopt in this paper. Although their study adopts a centralized deep RL approach similar to our

paper, they have a different system model and solely focus on the rebalancing problem and do not consider

pricing for rides as a control variable for the queues nor the charging problem of EVs as reviewed next.

Regarding charging strategies for large populations of EVs, [29–31] provide in-depth reviews and studies

of smart charging technologies. An agent-based model to simulate the operations of an AMoD ﬂeet of EVs

under various vehicle and infrastructure scenarios has been examined in [32]. By augmenting optimal battery

management of autonomous electric vehicles to the classic dial-a-ride problem (DARP), the authors of [33]

introduce the electric autonomous DARP that aims to minimize the total travel time of all the vehicles and

riders. The authors of [34] propose an online charge scheduling algorithm for EVs providing AMoD services.

By adopting a static network ﬂow model in [35], the beneﬁts of smart charging have been investigated and

approximate closed form expressions that highlight the trade-off between operational costs and charging

costs have been derived. Furthermore, [36] studies interactions between AMoD systems and the power grid.

In addition, [37] studies the implications of pricing schemes on an AMoD ﬂeet of EVs. In [38], the authors

propose a dynamic joint pricing and routing strategy for non-electric shared mobility on demand services. [39]

studies a quadratic programming problem in order to jointly optimize vehicle dispatching, charge scheduling,

and charging infrastructure, while the demand is deﬁned exogenously.

To the best of our knowledge, there is no existing work on centralized real-time management for electric

4

AMoD systems addressing the joint optimization scheme of vehicle routing and charging as well as pricing

for the rides. In this paper we aim to highlight the beneﬁts of a real-time controller that jointly: (i) routes

the vehicles throughout the network in order to serve the demand for rides as well as to relocate the empty

vehicles for further use, (ii) executes smart charging strategies by exploiting the diversity in the electricity

prices (both geographically and temporally) in order to minimize charging costs, and (iii) adjusts the demand

for rides by setting prices in order to stabilize the system (i.e., the queues of customers waiting for rides)

while maximizing proﬁts.

Paper Organization: The remainder of the paper is organized as follows.
system model and deﬁne the platform operator’s optimization problem. In Section 3, we discuss the static

In Section 2, we present the

planning problem associated with the system model and characterize the optimal static policy. In Section

4, we propose a method for developing a near-optimal real-time policy using deep reinforcement learning.

In Section 5, we present the numerical results of the case studies we have conducted in Manhattan and San

Francisco to demonstrate the performance of our real-time control policy. Finally, we conclude the paper in

Section 6.

2 System Model and Problem Deﬁnition

Network and Demand Models: We consider a ﬂeet of AMoD EVs operating within a transportation network
nodes that can each serve as a
characterized by a fully connected graph consisting of

=

M

1, . . . , m
{

}

∈ {

0, 1, 2, . . .

trip origin or destination. We study a discrete-time system with time periods normalized to integral units
t
. In this discrete-time system, we model the arrival of the potential riders with OD pair (i, j)
}
as a Poisson process with an arrival rate of λij(t) in period t, where λii(t) = 0. We adopted a price-responsive
rider model studied in [40]. We assume that the riders are heterogeneous in terms of their willingness to pay.
In particular, if the price for receiving a ride from node i to node j in period t is set to (cid:96)ij(t), the induced
) is the cumulative
arrival rate for rides from i to j is given by Λij(t) = λij(t)(1
distribution of riders’ willingness to pay with a support of [0, (cid:96)max]1. Thus, the number of new ride requests
in time period t is Aij(t)

F ((cid:96)ij(t))), where F (
·

Pois(Λij(t)) for OD pair (i, j).

−

∼

Vehicle Model: To capture the effect of trip demand and the associated charging and routing (routing also
implies rebalancing of the empty vehicles) decisions on the costs associated with operating the ﬂeet (mainte-

nance, mileage, etc.), we assume that each autonomous vehicle in the ﬂeet has a per period operational cost
of β. Furthermore, as the vehicles are electric, they have to sustain charge in order to operate. Without loss of
generality, we assume there is a charging station placed at each node i
. To charge at node i during time
period t, the operator pays a price of electricity pi(t) per unit of energy. We assume that all EVs in the ﬂeet
have a battery capacity denoted as vmax
,
N
where

Z+; therefore, each EV has a discrete battery energy level v
∈ V
. In our discrete-time model, we assume each vehicle takes one period
}

∈ M

vmax

v
{

≤

≤

=

∈

∈

V

0

v

|

1For brevity of notation, we uniformly set (cid:96)max to be the maximum willingness to pay for all OD pairs without loss of generality.
max is the maximum willingness to pay for OD

max, where (cid:96)ij

Our results can be derived in a similar fashion by replacing (cid:96)max with (cid:96)ij
pair (i, j).

5

to charge one unit of energy and τij periods to travel between OD pair (i, j), while consuming vij units of
energy2.

Ride Hailing Model: The platform operator dynamically routes the ﬂeet of EVs in order to serve the demand
at each node. Customers that purchase a ride are not immediately matched with a ride, but enter the queue for
OD pair (i, j). After the platform operator executes routing decisions for the ﬂeet, the customers in the queue
for OD pair (i, j) are matched with rides and served in a ﬁrst-come, ﬁrst-served discipline. A measure of the
expected wait time is not available to each arriving customer. However, the operator knows that longer wait

times will negatively affect their business and hence seeks to minimize the total wait time experienced by
users. Denote the queue length for OD pair (i, j) by qij(t). If after serving the customers, the queue length
qij(t) > 0, the platform operator is penalized by a ﬁxed cost of w per person at the queue to account for the
value of time of the customers.

Platform Operator’s Problem: We consider a proﬁt-maximizing AMoD operator that manages a ﬂeet of
EVs that make trips to provide transportation services to customers. The operator’s goal is to maximize proﬁts

by 1) setting prices for rides and hence managing customer demand at each node; 2) optimally operating the

AMoD ﬂeet (i.e., charging and routing) to minimize operational and charging costs. We will study two types

of control policies the platform operator utilizes: 1) a static policy, where the pricing, routing and charging

decisions are time invariant and independent of the state of the system; 2) a real-time policy, where the

pricing, routing and charging decisions are dependent on the system state.

3 Analysis of the Static Problem

In this section, we establish and discuss the static planning problem to provide a measure for comparison

and demonstrate the efﬁcacy of the real-time policy (which will be discussed in Section 4). To do so, we

consider the ﬂuid scaling of the dynamic network and characterize the static problem via a network ﬂow

formulation. Under this setting, we use the expected values of the variables (arrivals and prices of electricity)

and ignore their time dependent dynamics, while allowing the vehicle routing decisions to be ﬂows (real

numbers) rather than integers. The static problem is convenient for determining the optimal static pricing,
routing, and charging policy, under which the queueing network of the dynamic system is stable [41]3.

3.1 Static Proﬁt Maximization Problem

We formulate the static optimization problem via a network ﬂow model that aims to maximize the platform

operator’s proﬁts. The platform operator maximizes its proﬁts by setting prices and making routing and

charging decisions such that the system remains stable.

2In this paper, we consider the travel times to be constant and exogenously deﬁned for the time period the policy is developed for.
This is because we assume that the number of AMoD vehicles is much less compared to the rest of the trafﬁc. Also, to consider changing
trafﬁc conditions throughout the day, it is possible to train multiple static and real-time control policies for the different time intervals.

3The stability condition that we are interested in is rate stability of all queues. A queue for OD pair (i, j) is rate stable if

lim
t→∞

qij (t)/t = 0.

6

Let (cid:96)ij be the prices for rides for OD pair (i, j), xv

ij be the number of vehicles at node i with energy level
ic be the number of vehicles charging at node i starting with energy level v.

v being routed to node j, and xv
We state the platform operator’s proﬁt maximization problem as follows:

max

ic,xv
xv

ij ,(cid:96)ij

(cid:88)

(cid:88)

i∈M

j∈M

λij(cid:96)ij(1

(cid:88)

vmax−1
(cid:88)

−

i∈M

v=0

(β + pi)xv

ic −

F ((cid:96)ij))

−
vmax(cid:88)

subject to

λij(1

−

F ((cid:96)ij))

≤

v=vij
ij = xv−1
xv

ic +

xv
ic +

(cid:88)

j∈M

i, j

,
∈ M

xv
ij ∀
(cid:88)

j∈M

xv+vji
ji

i
∀

,
∈ M

v
∀

,

∈ V

,

,
i
∈ M
∀
v < vij,

xvmax
ic = 0
xv
ij = 0
xv
ic ≥
ic = xv
xv

∀
0, xv
ij ≥
ij = 0

0

∀
i, j

i, j

,
∈ M
,
∈ M
i, j
,

v
∀

∀
v /

∀

∈ V

∀

∈ V
.
∈ M

(cid:88)

(cid:88)

vmax(cid:88)

β

i∈M

j∈M

v=vij

xv
ijτij

(1a)

(1b)

(1c)

(1d)

(1e)

(1f)

(1g)

The ﬁrst term in the objective function in (1) accounts for the aggregate revenue the platform generates by
F ((cid:96)ij)) number of riders with a price of (cid:96)ij. The second term is the operational
providing rides for λij(1
and charging costs incurred by the charging vehicles (assuming that pi(t) = pi
t under the static setting),
and the last term is the operational costs of the trip-making vehicles (including rebalancing trips).

−

∀

The constraint (1b) requires the platform to operate at least as many vehicles to serve all the induced de-
mand between any two nodes i and j (The rest are the vehicles travelling without passengers, i.e., rebalancing
vehicles). We will refer to this as the demand satisfaction constraint. The constraint (1c) is the ﬂow balance
constraint for each node and each battery energy level, which restricts the number of available vehicles at
node i and energy level v to be the sum of arrivals from all nodes (including idle vehicles) and vehicles that
1. The constraint (1d) ensures that the vehicles with full battery do not
are charging with energy level v
charge further, and the constraint (1e) ensures the vehicles sustain enough charge to travel between OD pair
(i, j).

−

The solution to the optimization problem in (1) is the optimal static policy that consists of optimal prices

as well as optimal vehicle routing and charging decisions. This policy can not directly be implemented in
a real environment because it does not yield integer valued solutions. It is possible generate integer-valued

solutions to be implemented in a real environment using the fractional ﬂows (e.g., randomizing the vehicle

decisions according to the ﬂows, which we do in Section 5), yet the methodology is not the focus of our

work. Instead, we highlight a sufﬁcient condition for a realizable policy (generating integer valued actions)

to provide stability according to the feasible solutions of (1):

˜(cid:96)ij, ˜xv

ij, ˜xv

Proposition 1. Let
be a feasible solution of (1). Let µ be a policy that generates integer
actions and can be implemented in the real environment. Then, µ guarantees stability of the system if for all
OD pairs (i, j):

ic}

{

7

1. The time average of the induced arrivals equals (1

−
2. The time average of the routed vehicles equals (cid:80)vmax
v=vij

˜xv
ij.

F (˜(cid:96)ij)), and

The proof of Proposition 1 is provided in Appendix A. According to Proposition 1, for a static pricing
ij, there exists an integer-valued routing and charging policy that maintains

policy with the optimal prices (cid:96)∗
stability of the system.

Corollary 1.1. An example policy that generates integer-valued actions is randomizing according to the
ﬂows. Precisely, given a feasible solution
of (1), integer-valued actions can be generated by
routing a vehicle at node i with energy level v to node j with probability

˜(cid:96)ij, ˜xv

ij, ˜xv

ic}

{

ψv

ij =

˜xv
ij
ik + xv
k=1 ˜xv
ic

(cid:80)m

,

and charging with probability

˜xv
ic
ik + xv
k=1 ˜xv
ic
. Combining this randomized policy with a static pricing policy of (cid:96)ij(t) = ˜(cid:96)ij,

ic =

(cid:80)m

ψv

,

t,

∀

i, j

and

v
∀

∈ M

∀
results in a policy satisfying the criteria in Proposition 1.

∈ V

The optimization problem in (1) is non-convex for a general F (
·

). Nonetheless, when the platform’s
)), it can be rewritten as a convex optimization problem
proﬁts are convex in the induced demand λij(1
and can be solved exactly. Hence, we assume that the rider’s willingness to pay is uniformly distributed in
[0, (cid:96)max], i.e., F ((cid:96)ij) = (cid:96)ij
(cid:96)max

F (
·

−

4.

Marginal Pricing: The prices for rides are a crucial component of the proﬁts generated. The next proposition
highlights how the optimal prices (cid:96)∗
ij for rides are related to the network parameters, prices of electricity, and
the operational costs.

Proposition 2. Let ν∗
OD pair (i, j). The optimal prices (cid:96)∗

ij are:

ij be optimal the dual variable corresponding to the demand satisfaction constraint for

(cid:96)∗
ij =

(cid:96)max + ν∗
ij
2

.

These prices can be upper bounded by:

(cid:96)∗
ij ≤

(cid:96)max + β(τij + τji + vij + vji) + vijpj + vjipi
2

(2)

(3)

4It is also possible to use other distributions that might reﬂect real willingness-to-pay distributions more accurately (such as pareto
distribution, exponential distribution, triangular distribution, constant elasticity distribution, and normal distribution). Among these,
pareto, exponential, and constant elasticity distributions preserve convexity and therefore the static planning problem can be solved
efﬁciently. Triangular and normal distributions are not convex in their support and therefore the static planning problem is not a convex
optimization problem. Nevertheless, it can still be solved numerically for the optimal static policy. Using these distributions however we
cannot derive the closed-form results that allow us to interpret the pricing policy of the platform operator. The real-time policy proposed
in Section 4 uses model-free Reinforcement Learning and therefore can be applied using other distributions or any other customer price
response model.

8

Moreover, with these optimal prices (cid:96)∗

ij, the proﬁts generated per period is:

P =

m
(cid:88)

m
(cid:88)

i=1

j=1

λij
(cid:96)max

((cid:96)max

ij)2.
(cid:96)∗

−

(4)

The proof of Proposition 2 is provided in Appendix B. Observe that the proﬁts in Equation (4) are de-

creasing as the prices for rides increase. Thus expensive rides generate less proﬁts compared to the cheaper
rides and it is more beneﬁcial if the optimal dual variables ν∗
ij are small and prices are close to (cid:96)max/2. We
can interpret the dual variables ν∗
ij as the cost of providing a single ride between i and j to the platform. In
the worst case scenario, every single requested ride from node i requires rebalancing and charging both at
the origin and the destination. Hence the upper bound on (3) includes the operational costs of passenger-

carrying, rebalancing and charging vehicles (both at the origin and the destination); and the energy costs of

both passenger-carrying and rebalancing trips multiplied by the price of electricity at the trip destinations.

Similar to the taxes applied on products, whose burden is shared among the supplier and the customer; the

costs associated with rides are shared among the platform operator and the riders (which is why the price paid

by the riders include half of the cost of the ride).

Although the static policy guarantees stability (by appropriate implementation of integer-valued actions as

dictated by Proposition 1), it does not perform well in a real dynamic setting because it does not acknowledge

the stochastic dynamics of the system. On the other hand, a real-time policy that executes decisions based on
the current state of the environment would likely perform better (e.g., if the queue length for OD pair (i, j) is
very large, then it is probably better for the platform operator to set higher prices to prevent the queue from

growing further). Accordingly, we present a practical way of implementing a real-time policy in the next

section.

4 The Real-Time Policy

The static policy established in the previous section has three major issues:

1. Because it is based on a ﬂow model, it generates static fractional ﬂows that are not directly imple-

mentable in the real setting.

2. It neglects the stochastic events that occur in the dynamic setting (e.g., the induced arrivals), and as-

sumes everything is deterministic. Hence, it does consider the unexpected occurrences (e.g., queues

might build in the dynamic setting, whereas the static model assumes no queues) when executing ac-

tions.

3. It assumes perfect knowledge of the network parameters (arrivals, trip durations, energy consumptions

of the trips, and prices of electricity).

Due to the above reasons, it is impractical to implement the static policy in the dynamic environment.

A real-time policy that generates integer solutions and takes into account the current state of the network

9

which is essential for decision making is necessary, and can be determined by solving the dynamic program

that describes the system (with full knowledge of the network parameters) for the optimal policy. Such solu-

tions would address issues 1 and 2 outlined above. Inspired by our theoretical model, the state information

that describes the network fully consists of the vehicle states (locations, energy levels), queue lengths for

each OD pair, and electricity prices at each node. Upon obtaining the full state information, the actions

have to be executed for pricing for rides and ﬂeet management (vehicle routing and charging). Consequent

to taking actions, the platform operator observes a reward (consisting of revenue gained by arrivals, queue

costs, and operational and charging costs), and the network transitions into a new state (Although the tran-

sition into the new state is stochastic, the random processes that govern this stochastic transition is known

if the network parameters are known). The solution of this dynamic program is the optimal policy that de-
termines which action to take for each state the system is in, and can nominally be derived using classical

exact dynamic programming algorithms (e.g., value iteration). However, the complexity and the scale of our

dynamic problem presents a difﬁculty here: Aside from having a large dimensional state space (for instance,
m = 10, vmax = 5, τij = 3
spaces are not ﬁnite (queues can grow unbounded, prices are continuous). Considering that the computational
3) [42],
are the state space and the action space, respectively, the problem is computationally in-

the state has dimension 1240) and action space, the cardinality of these

complexity per iteration for value iteration is

2) and for policy iteration

(
|A||S|

|A||S|

where

2 +

i, j:

and

|S|

O

O

∀

(

tractable to solve using classical dynamic programming. Even if we did make them ﬁnite by putting a cap

S

A

on the queue lengths and discretizing the prices, curse of dimensionality renders the problem intractable to

solve with classical exact dynamic programming algorithms. As such, we resort to approximate dynamic

programming methods. Speciﬁcally, we deﬁne the policy via a deep neural network that takes the full state
information of the network as input and outputs the best action5. Subsequently, we apply a model-free rein-
forcement learning algorithm to train the neural network in order to improve the performance of the policy.

Since it is model-free, it does not require a modeling of the network (hence, it does not require knowledge of

the network parameters), which resolves the third issue associated with the static policy.

We adopted a practical policy gradient method, called Proximal Policy Optimization (PPO), developed

in [2], which is effective for optimizing large nonlinear policies such as neural networks. We chose PPO
mainly because it supports continuous state-action spaces and guarantees monotonic improvement.6

We note that it is possible to apply reinforcement learning to learn a policy in any environment, real or

artiﬁcial, as long as there is data available. In this work we use our theoretical model described in Section 2

to create the environment and generate data, mainly because there is no electric AMoD microsimulation

environment available and also to verify our ﬁndings about the static policy. Developing a microsimulator

for electric AMoD (like SUMO [43]) and integrating it with a deep reinforcement learning library to create

a framework for real trafﬁc experiments remains a future work. To ensure that our numerical experiments

5In general, the policy is a stochastic policy and determines the probabilities of taking the actions rather than deterministically

producing an action.

6Although the policy outputs a continuous set of actions, integer actions can be generated by randomizing. This is done during both
training and testing, therefore the RL agent observes the integer state transitions and learns as if the policy outputs integer actions. We
discuss how to generate integer actions in more detail in Section 4.1.

10

are reproducible, in the next subsection, we describe the Markov Decision Process (MDP) that governs this

dynamic environment, which is a direct extension of our static model. It is also possible to enrich the environ-

ment and the MDP to reﬂect real life constraints more accurately such as road capacity and charging station

constraints. Since the approach we adopt to develop the real-time policy is model-free, it can be applied

identically.

In Section 5 we present numerical results on real-time policies developed through reinforcement learning

based on dynamic environments generated through our theoretical model. The goal of the experiments is to

primarily answer the following questions:

1. Can we develop a real-time control and pricing policy for AMoD using reinforcement learning and

what are its potential beneﬁts over the static policy?

2. How does the policy trained for a speciﬁc network perform, if the network parameters change?

3. Can we develop a global policy that can be utilized in any network with moderate ﬁne tuning?

The reader may skip reading Section 4.1 if they are not interested in the details of the MDP model used

in our numerical experiment.

4.1 The Real-Time Problem as MDP

We deﬁne the MDP by the tuple (
transition operator and r is the reward function. We describe these elements as follows:

is the state space,

, r), where

,
A

A

S

S

T

,

is the action space,

is the state

T

S

1.

: The state space consists of prices of electricity at each node, the queue lengths for each origin-

−

(cid:80)m

−
≥0 (We include all the non-negative valued vectors, however, only m2

destination pair, and the number of vehicles at each node and each energy level. However, since travelling
from node i to node j takes τij periods of time, we need to deﬁne intermediate nodes. As such, we deﬁne
1 number of intermediate nodes between each origin and destination pair, for each battery energy level
τij
v. Hence, the state space consists of sd = m2 + (vmax + 1)(((cid:80)m
m2 + 2m) dimensional
vectors in Rsd
m entries can grow to
inﬁnity because they are queue lengths, and the rest are always upper bounded by ﬂeet size or maximum price
of electricity). As such, we deﬁne the elements of the state vector at time t as s(t) = [p(t) q(t) sveh(t)],
where p(t) = [pi(t)]i∈M is the electricity prices state vector, q(t) = [qij(t)]i,j∈M;i(cid:54)=j is the queue lengths
ijk(t)]∀i,j,k,v is the vehicle state vector, where sv
state vector, and sveh(t) = [sv
ijk(t) is the number of vehicles
at vehicle state (i, j, k, v). The vehicle state (i, j, k, v) speciﬁes the location of a vehicle that is travelling
between OD pair (i, j) as the k’th intermediate node between nodes i and j, and speciﬁes the battery energy
level of a vehicle as v (The states of the vehicles at the nodes i
with energy level v is denoted by
(i, i, 0, v)).

j=1 τij)

∈ M

i=1

−

2.

: The action space consists of prices for rides at each origin-destination pair and routing/charging de-
at each energy level v. The price actions are continuous in range [0, (cid:96)max].

cisions for vehicles at nodes i

A

∈ M

11

,

−

∈ V

i
∀

v
∀

∈ M

Each vehicle at state (i, i, 0, v) (
) can either charge, stay idle or travel to one of the remain-
1 nodes. To allow for different transitions for vehicles at the same state (some might charge, some
ing m
might travel to another node), we deﬁne the action taken at time t for vehicles at state (i, i, 0, v) as an m + 1
dimensional probability vector with entries in [0, 1] that sum up to 1: αv
ic(t)],
where αvmax
ij(t) = 0 if v < vij. The action space is then all the vectors a of dimension
ic
ad = m2
m entries are the prices and the rest are the proba-
bility vectors satisfying the aforementioned properties. As such, we deﬁne the elements of the action vector
at time t as a(t) = [(cid:96)(t) α(t)], where (cid:96)(t) = [(cid:96)ij]i,j∈M,i(cid:54)=j is the vector of prices and α(t) = [αv
i (t)]∀i,v is
the vector of routing/charging actions.

(t) = 0 and αv
m + (vmax + 1)(m2 + m), whose ﬁrst m2

i (t) = [αv

i1(t) . . . αv

im(t) αv

−

−

T

3.

: The transition operator is deﬁned as

s(t) = i, a(t) = k). We can
|
deﬁne the transition probabilities for electricity prices p(t + 1), queue lengths q(t + 1), and vehicle states
sveh(t + 1) as follows:

ijk = P r(s(t + 1) = j

T

Electricity Price Transitions: Since we assume that the dynamics of prices of electricity are exogenous
to our AMoD system, P r(p(t + 1) = p2
p(t) = p1), i.e., the
|
dynamics of the price are independent of the action taken. Depending on the setting, new prices might either
(t), which
be deterministic or distributed according to some probability density function at time t: p(t)
is determined by the electricity provider.

p(t) = p1, a(t)) = P r(p(t + 1) = p2

∼ P

|

i (t). Each vehicle transitions into state (i, j, 1, v

Vehicle Transitions: For each vehicle at node i and energy level v, the transition probability is deﬁned
by the action probability vector αv
vij) with probability
αv
ij(t), stays idle in state (i, i, 0, v) with probability αv
ii(t) or charges and transitions into state (i, i, 0, v + 1)
with probability αv
ic(t). The vehicles at intermediate states (i, j, k, v) transition into state (i, j, k + 1, v) if
k < τij
1 with probability 1. The total transition probability to the vehicle states
sveh(t + 1) given sveh(t) and α(t) is the sum of all the probabilities of the feasible transitions from sveh(t)
to sveh(t + 1) under α(t), where the probability of a feasible transition is the multiplication of individual
vehicle transition probabilities (since the vehicle transition probabilities are independent). Note that instead

1 or (j, j, 0, v) if k = τij

−

−

−

of gradually dissipating the energy of the vehicles on their route, we immediately discharge the required

energy for the trip from their batteries and keep them constant during the trip. This ensures that the vehicles

have enough battery to complete the ride and does not violate the model, because the vehicles arrive to their

destinations with true value of energy and a new action will only be taken when they reach the destination.

Queue Transitions: The queue lengths transition according to the prices and the vehicle routing decisions.
For prices (cid:96)ij(t) and induced arrival rate Λij(t), the probability that Aij(t) new customers arrive in the queue
(i, j) is:

P r(Aij(t)) =

e−Λij (t)Λij(t)Aij (t)
(Aij(t))!

Let us denote the total number of vehicles routed from node i to j at time t as xij(t), which is given by:

xij(t) =

vmax(cid:88)

v=vij

xv
ij(t) =

vmax(cid:88)

v=vij

sv−vij
ij1

(t + 1).

(5)

12

Figure 3: The schematic diagram representing the state transition of our MDP. Upon taking an action, a vehicle at state
(i, i, 0, v) charges for a price of pi(t) and transitions into state (i, i, 0, v + 1) with probability αv
ic(t), stays idle at state
(i, i, 0, v) with probability αv
ii(t), or starts traveling to another node j and transitions into state (i, j, 1, v − vij) with
probability αv
ij(t). Furthermore, Aij(t) new customers arrive to the queue (i, j) depending on the price (cid:96)ij(t). After the
routing and charging decisions are executed for all the EVs in the ﬂeet, the queues are modiﬁed.

Given sveh(t + 1) and xij(t), the probability that the queue length qij(t + 1) = q is:

P r(qij(t + 1) = q

s(t), a(t), sveh(t + 1)) = P r(Aij(t) = q
|

−

qij(t) + xij(t)),

if q > 0, and P r(Aij(t)
that the queue vector q(t + 1) = q is:

≤ −

qij(t) + xij(t)) if q = 0. Since the arrivals are independent, the total probability

P r(q(t + 1) = q

s(t), a(t), sveh(t + 1)) =
|

(cid:89)

(cid:89)

i∈M

j∈M
j(cid:54)=i

P r(qij(t + 1)

|

s(t), a(t), sveh(t + 1)).

Hence, the transition probability is deﬁned as:

P r(s(t + 1)

|

s(t), a(t)) =P r(p(t + 1)

p(t))
|

×

P r(sveh(t + 1)

s(t), α(t))

|

P r(q(t + 1)

s(t), α(t), sveh(t + 1))
|

×

(6)

We illustrate how the vehicles and queues transition into new states consequent to an action in Figure 3.

4. r: The reward function r(t) is a function of state-action pairs at time t: r(t) = r(a(t), s(t)). Let
xv
ic(t) denote the number of vehicles charging at node i starting with energy level v at time period t. The
reward function r(t) is deﬁned as:

(cid:88)

(cid:88)

r(t) =

(cid:96)ij(t)Aij(t)

i∈M

j∈M
j(cid:54)=i

w

−

(cid:88)

(cid:88)

∈M

j∈M
j(cid:54)=i

qij(t)

−

(cid:88)

vmax−1
(cid:88)

(β + pi)xv

ic(t)

i∈M

v=0

β

−

(cid:88)

(cid:88)

i∈M

j∈M
j(cid:54)=i

xij(t)

β

−

(cid:88)

(cid:88)

τij −1
(cid:88)

vmax−1
(cid:88)

sv
ijk(t)

i∈M

j∈M
j(cid:54)=i

k=1

v=0

(7)

The ﬁrst term corresponds to the revenue generated by the passengers that request a ride for a price (cid:96)ij(t), the
second term is the queue cost of the passengers that have not yet been served, the third term is the charging

13

and operational costs of the charging vehicles and the last two terms are the operational costs of the vehicles

making trips. Note that revenue generated is immediately added to the reward function when the passengers

enter the network instead of after the passengers are served. Since the reinforcement learning approach is

based on maximizing the cumulative reward gained, all the passengers eventually have to be served in order

to prevent queues from blowing up and hence it does not violate the model to add the revenues immediately.

Using the deﬁnitions of the tuple (

, r), we model the dynamic problem as an MDP. Given large-
dimensional state and action spaces with inﬁnite cardinality, we can not solve the MDP using exact dynamic

,
A

S

T

,

programming methods. As a solution, we characterize the real-time policy via a deep neural network and

execute reinforcement learning in order to develop a real-time policy.

4.2 Reinforcement Learning Method

In this subsection, we go through the preliminaries of reinforcement learning and brieﬂy explain the idea of

the algorithm we adopted.

4.2.1 Preliminaries

The real-time policy associated with the MDP is deﬁned as a function parameterized by θ:

s) = π :
πθ(a
|

S × A →

[0, 1],

i.e., a probability distribution in the state-action space. Given a state s, the policy returns the probability for
taking the action a (for all actions), and samples an action according to the probability distribution. The goal
is to derive the optimal policy π∗, which maximizes the discounted cumulative expected rewards Jπ:

Jπ∗ = max

π

Jπ = max

π

π∗ = arg max

π

Eπ

(cid:34) ∞
(cid:88)

t=0

(cid:34) ∞
(cid:88)

(cid:35)
γtr(t)

,

Eπ

t=0
(cid:35)
γtr(t)

,

(0, 1] is the discount factor. The value of taking an action a in state s, and following the policy π

where γ
afterwards is characterized by the value function Qπ(s, a):

∈

Qπ(s, a) = Eπ

(cid:34) ∞
(cid:88)

t=0

γtr(t)
s(0) = s, a(0) = a
|

.

(cid:35)

The value of being in state s is formalized by the value function Vπ(s):

Vπ(s) = Ea(0),π

(cid:34) ∞
(cid:88)

t=0

(cid:35)

γtr(t)
|

s(0) = s

,

and the advantage of taking the action a in state s and following the policy π thereafter is deﬁned as the
advantage function Aπ(s, a):

Aπ(s, a) = Qπ(s, a)

Vπ(s).

−

14

The methods used by reinforcement learning algorithms can be divided into three main groups: 1) critic-

only methods, 2) actor-only methods, and 3) actor-critic methods, where the word critic refers to the value

function and the word actor refers to the policy [44]. Critic-only (or value-function based) methods (such as

Q-learning [45] and SARSA [46]) improve a deterministic policy using the value function by iterating:

a∗ = arg max

a

Qπ(s, a),

π(a∗

s)

|

←−

1.

Actor-only methods (or policy gradient methods), such as Williams’ REINFORCE algorithm [47], improve
the policy by updating the parameter θ by gradient ascent, without using any form of a stored value function:

θ(t + 1) = θ(t) + α

θEπθ(t)

∇

(cid:34)

(cid:88)

(cid:35)
γτ r(τ )

.

τ

The advantage of policy gradient methods is their ability to generate actions from a continuous action space

by utilizing a parameterized policy.

Finally, actor-critic methods [48, 49] make use of both the value functions and policy gradients:

θ(t + 1) = θ(t) + α

θEπθ(t)

(cid:2)Qπθ(t) (s, a)(cid:3) .

∇

Actor-critic methods are able to produce actions in a continuous action space, while reducing the high vari-

ance of the policy gradients by adding a critic (value function).

All of these methods aim to update the parameters θ (or directly update the policy π for critic-only
methods) to improve the policy. In deep reinforcement learning, the policy π is deﬁned by a deep neural
network, whose weights constitute the parameter θ. To develop a real-time policy for our MDP, we adopt a
practical policy gradient method called Proximal Policy Optimization (PPO).

4.2.2 Proximal Policy Optimization

PPO is a practical policy gradient method developed in [2], and is effective for optimizing large non-linear

policies such as deep neural networks. It preserves some of the beneﬁts of trust region policy optimization

(TRPO) [50] such as monotonic improvement, but is much simpler to implement because it can be optimized

by a ﬁrst-order optimizer, and is empirically shown to have better sample complexity.

In TRPO, an objective function (the “surrogate” objective) is maximized subject to a constraint on the

size of the policy update so that the new policy is not too far from the old policy:

maximize
θ

subject to

(cid:21)

ˆAt

ˆEt

st)
st)

(cid:20) πθ(at
|
πθold(at
|
ˆEt [KL [πθold(
·|

st), πθ(

st)]]

·|

(8a)

(8b)

δ,

≤

where πθ is a stochastic policy and ˆAt is an estimator of the advantage function at timestep t. The expectation
ˆEt[. . . ] indicates the empirical average over a ﬁnite batch of samples and KL [πθold(
st)] denotes

st), πθ(

·|

·|

15

the Kullback–Leibler divergence between πθold and π. Although TRPO solves the above constrained max-
imization problem using conjugate gradient, the theory justifying TRPO actually suggests using a penalty

instead of a constraint, i.e., solving the unconstrained optimization problem

maximize
θ

ˆEt

(cid:20) πθ(at
st)
|
st)
πθold(at

|

ˆAt

−

βKL [πθold(

st), πθ(

·|

(cid:21)
st)]

·|

,

(9)

for some penalty coefﬁcient β. TRPO uses a hard constraint rather than a penalty because it is hard to
choose a single value of β that performs well. To overcome this issue and develop a ﬁrst-order algorithm
that emulates the monotonic improvement of TRPO (without solving the constrained optimization problem),

two PPO algorithms are constructed by: 1) clipping the surrogate objective and 2) using adaptive KL penalty

coefﬁcient [2].

1. Clipped Surrogate Objective: Let rt(θ) denote the probability ratio rt(θ) = πθ(at|st)

πθold (at|st) , so r(θold) = 1.

TRPO maximizes

(cid:20) πθ(at
st)
|
st)
πθold(at
subject to the KL divergence constraint. Without a constraint however this would lead to a large policy

L(θ) = ˆEt

rt(θ) ˆAt

= ˆEt

(10)

ˆAt

(cid:21)

(cid:105)

(cid:104)

.

|

update. To prevent this, PPO modiﬁes the surrogate objective to penalize changes to the policy that
move rt(θ) away from 1:

LCLIP (θ) = ˆEt

(cid:104)
min(rt(θ) ˆAt, clip(rt(θ), 1

(cid:15), 1 + (cid:15)) ˆAt)

(cid:105)

,

−

(11)

(cid:15), 1 + (cid:15)) ˆAt) modiﬁes the
where (cid:15) is a hyperparameter, usually 0.1 or 0.2. The term clip(rt(θ), 1
surrogate objective by clipping the probability ratio, which removes the incentive for moving rt outside
(cid:15), 1 + (cid:15)]. By taking the minimum of the clipped and the unclipped objective, the
of the interval [1
ﬁnal objective becomes a lower bound on the unclipped objective.

−

−

2. Adaptive KL Penalty Coefﬁcient: Another approach is to use a penalty on KL divergence and to adapt
the penalty coefﬁcient so that some target value of the KL divergence dtarg is achieved at each policy
update. In each policy update, the following steps are performed:

• Using several epochs of minibatch SGD, optimize the KL-penalized objective
(cid:20) πθ(at
st)
|
st)
πθold(at
|

LKLP EN (θ) = ˆEt

βKL [πθold(
·|

st), πθ(

ˆAt

−

·|

(cid:21)
st)]

(12)

• Compute d = ˆEt [KL [πθold (
·|
β/2

– If d < dtarg/1.5, β

st), πθ(

st)]]

·|

←

– If d > dtarg

×

1.5, β

2.

β

×

←

The updated β is then used for the next policy update. This scheme allows β to adjust if KL divergence
is signiﬁcantly different than dtarg so that the desired KL divergence between the old and the updated
policy is attained.

16

A PPO algorithm using ﬁxed-length trajectory segments is summarized in Algorithm 1. Each iteration, each
of N (parallel) actors collect T timesteps of data. Then the surrogate loss on these N T timesteps of data is
constructed and optimized with minibatch SGD for K epochs.

Algorithm 1: PPO, Actor-Critic Style

for iteration = 0, 1, 2, . . . do

for actor = 1, 2, . . . , N do

Run policy πθold in environment for T timesteps.
Compute advantage estimates ˆA1, . . . , ˆAT

end
Optimize surrogate LCLIP or LKLP EN w.r.t. θ, with K epochs and minibatch size M
θold

θ

end

←

N T .

≤

In this work, we used the PPO algorithm with the clipped surrogate objective, because experimentally it

it shown to have better performance than the PPO algorithm with adaptive KL penalty coefﬁcient [2]. We

refer the reader to [2] for a comprehensive study on PPO algorithms.

In the next section, we present our numerical studies demonstrating the performance of the RL policy.

5 Numerical Study

In this section, we discuss the numerical experiments and results for the performance of reinforcement learn-

ing approach to the dynamic problem and compare with the performance of several static policies, including

the optimal static policy outlined in Section 3. We solved for the optimal static policy using CVX, a package

for specifying and solving convex programs [51]. To implement the dynamic environment compatible with

reinforcement learning algorithms, we used Gym toolkit [52] developed by OpenAI to create an environment.

For the implementation of the PPO algorithm, we used Stable Baselines toolkit [53].

We chose an operational cost of β = $0.1 (by normalizing the average price of an electric car over 5
years [54]) and maximum willingness to pay (cid:96)max = $30. For prices of electricity pi(t), we generated
random prices for different locations and different times using the statistics of locational marginal prices
in [55]. We chose a maximum battery capacity of 20kWh. We discretrized the battery energy into 5 units,
where one unit of battery energy is 4kWh. The time it takes to deliver one unit of charge is taken as one time
epoch, which is equal to 5 minutes in our setup. The waiting time cost for one period is w = $2 (average
hourly wage is around $24 in the United States [56]).

Note that the dimension of the state space grows signiﬁcantly with battery capacity vmax, because it
expands the states each vehicle can have by vmax. Therefore, for computational purposes, we conducted
two case studies: 1) Non-electric AMoD case study with a larger network in Manhattan, 2) Electric AMoD

case study with a smaller network in San Francisco. We picked two different real world networks in order to

demonstrate the universality of reinforcement learning method in establishing a real-time policy. In particular,

17

our intention is to support the claim that the success of the reinforcement learning method is not restricted

to a single network, but generalizes to multiple real world networks. Both experiments were performed on a
laptop computer with Intel® CoreTM i7-8750H CPU (6

2.20 GHz) and 16 GB DDR4 2666MHz RAM.

×

5.1 Case Study in Manhattan

In a non-electric AMoD network, the energy dimension v vanishes. Because there is no charging action7,
we can perform coarser discretizations of time. Speciﬁcally, we can allow each discrete time epoch to cover
τij minutes, and normalize the travel times τij and w accordingly (For EV’s, because charging takes
5

min
i,j|i(cid:54)=j

×

a non-negligible but shorter time than travelling, in general we have τij > 1, and larger number of states).
The static proﬁt maximization problem in (1) for AMoD with non-electric vehicles can be rewritten as:

max
xij ,(cid:96)ij

(cid:88)

(cid:88)

i∈M

j∈M

λij(cid:96)ij(1

F ((cid:96)ij))

βg

−

−

(cid:88)

(cid:88)

xijτij

i∈M

j∈M

subject to λij(1
(cid:88)

F ((cid:96)ij))
(cid:88)

−
xij =

xij

≤
xji

j∈M

j∈M

xij

0

i, j

∀

.
∈ M

≥

i
∀

∀

i, j

,
∈ M
,
∈ M

(13)

The operational costs βg = $2.5 (per 10 minutes, [57]) are different than those of electric vehicles.
Because there is no “charging” (or refueling action, since it takes negligible time), βg also includes fuel cost.
The optimal static policy is used to compare and highlight the performance of the real-time policy8.

We divided Manhattan into 10 regions as in Figure 4, and using the yellow taxi data

from the New York City Taxi and Limousine Commission dataset [58] for May 04, 2019,

Saturday between 18.00-20.00, we extracted the average arrival rates for rides and trip

durations between the regions (we exclude the rides occurring in the same region). We

trained our model by creating new induced random arrivals with the same average ar-

rival rate using prices determined by our policy. For the ﬂeet size, we used a ﬂeet of

1200 autonomous vehicles (according to the optimal ﬂeet size emerging from the static

problem).

For training, we used a neural network with 4 hidden layers and 128 neurons in each

hidden layer. The rest of the parameters are left as default as speciﬁed by the Stable

Baselines toolkit [53]. In order to get the best policy, we train 3 different models using

Figure 4: Man-

DDPG [59], TRPO [50], and PPO. We trained the models for 10 million iterations, and

the performances of the trained models are summarized in Table 1 using average rewards

hattan divided into
m = 10 regions.

7The vehicles still refuel, however this takes negligible time compared to the trip durations.
8The solution of the static problem yields vehicle ﬂows. In order to make the policy compatible with our environment and to generate
integer actions that can be applied in a dynamic setting, we randomized the actions by dividing each ﬂow for OD pair (i, j) (and energy
level v) by the total number of vehicles in i (and energy level v) and used that fraction as the probability of sending a vehicle from i to
j (with energy level v).

18

and queue lengths as metrics. Our experiments indicate that the model trained using PPO is performing the

best among the three, hence we use that model as our real-time policy.

Algorithms

DDPG

TRPO

PPO

Metrics

Average Rewards

9825.69

13142.47

15527.34

Average Queue Length

431.76

87.96

68.11

Table 1: Performances of RL policies trained with different algorithms.

We compare different policies’ performance using the rewards and total queue length as metrics. The

results are demonstrated in Figure 5. In Figure 5a we compare the rewards generated and the total queue

length by applying the static and the real-time policies as deﬁned in Sections 3 and 4. We can observe that

while the optimal static policy provides rate stability in a dynamic setting (since the queues do not blow up),

it fails to generate proﬁts as it is not able to clear the queues. On the other hand, the real-time policy is able

to keep the total length of the queues 100 times shorter than the static policy while generating higher proﬁts.

The optimal static policy fails to generate proﬁts and is not necessarily the best static policy to apply in

a dynamic setting. As such, in Figure 5b we demonstrate the performance of a sub-optimal static policy,
where the prices are 5% higher than the optimal static prices to reduce the arrival rates and hence reduce
the queue lengths. Observe that the proﬁts generated are higher than the proﬁts generated using optimal

static policy for the static planning problem while the total queue length is less. This result indicates that

under the stochasticity of the dynamic setting, a sub-optimal static policy can perform better than the optimal

static policy. Furthermore, we summarize the performances of other static policies with higher static prices,
namely with 5%, 10%, 20%30%, and 40% higher prices than the optimal static prices in Table 2. Among
these, an increase of 10% performs the best in terms of rewards. Nevertheless, this policy does still do worse
in terms of rewards and total queue length compared to the real-time policy, which generates around 10%
more rewards and results in 70% less queues. Lastly we note that although a 40% increase in prices results
in minimum average queue length, this is a result of signiﬁcantly reduced induced demand and therefore it

generates very low rewards.

Metrics

% of opt. static prices

105%

110%

120%

130%

140%

Average Rewards

12234.13

14112.77

13739.35

12046.91

9625.82

Average Queue Length

584.05

231.93

74.64

30.88

14.20

Table 2: Performances of static pricing policies for Manhattan case study.

Next, we showcase that even some heuristic modiﬁcations which resemble what is done in practice can
do better than the optimal static policy. We utilize the optimal static policy, but additionally utilize a surge-

pricing policy. The surge-pricing policy aims to decrease the arrival rates for longer queues so that the

19

(a)

(c)

(b)

(d)

Figure 5: Comparison of different policies for Manhattan case study. The legends for all ﬁgures are the same as the top

left ﬁgure, where red lines correspond to the real-time policy and blue lines correspond to the static policies (We excluded

the running averages for (d), because the static policy diverges). In all scenarios, we use the rewards generated and the

total queue length as metrics. In (a), we demonstrate the results from applying the real-time policy and the optimal static
policy. In (b), we compare the real-time policy with the static policy that utilizes 5% higher prices than the optimal static

policy. In (c), we utilize a surge pricing policy along with the optimal static policy and compare with the real-time policy.

In (d), we employ the real-time policy and static policy developed for May 4, 2019, Saturday for the arrivals on May 11,

2019, Saturday.

20

−6−4−202Rewards($)×104Real-TimePolicyReal-TimePolicyRun.Avg.StaticPolicyStaticPolicyRun.Avg.0.00.51.01.52.02.53.03.54.0Time(10mins)×1050.00.40.8TotalQueueLength×104Real−Timevs.StaticPolicy0.81.21.6Rewards($)×104012345Time(10mins)×1040.00.20.40.60.8TotalQueueLength×103Real−Timevs.StaticPolicy(+5%Prices)0.81.21.6Rewards($)×1040.00.20.40.60.81.0Time(10mins)×104012TotalQueueLength×102Real−Timevs.SurgePricingPolicy−6−4−202Rewards($)×1040.00.20.40.60.81.0Time(10mins)×1030.00.20.40.60.81.01.2TotalQueueLength×104Real−Timevs.StaticPolicy(NextWeek)queues will stay shorter and the rewards will increase. At each time period, for all OD pairs, the policy is
to increase the price by 50% if the queue is longer than 100% of the induced arrival rate. The results are
displayed in Figure 5c. New arrivals bring higher revenue per person and the total queue length is decreased,

which stabilizes the network while generating more proﬁts than the optimal static policy. The surge pricing

policy results in stable short queues and higher rewards compared to the optimal static policy for the static
setting, however, both the real-time policy and the static pricing policy with 10% higher prices are superior.
Performances of other surge pricing policies that multiply the prices by 1.25/1.5/2 if the queue is longer than
50%/100%/200% of the induced arrival rates can be found in Table 3. Accordingly, the best surge pricing
policy maximizing the rewards is to multiply the prices by 1.25 if the queue is longer than 50% of the induced
arrival rate. Yet, our real-time policy still generates around 20% more rewards and results in 32% less queues.
We note that a surge pricing policy that multiplies the prices by 2 when the queues are longer than 50% of the
induced arrival rates minimizes the queues by decreasing the induced arrival rates signiﬁcantly, which results

in substantially low rewards.

Queue Threshold

50%

100%

200%

Surge Multiplier

Queue

Rewards

Queue

Rewards

Queue

Rewards

1.25

1.5

2

101.25

13022.83

186.56

12897.30

380.34

12357.33

91.89

83.15

12602.90

178.22

12589.71

370.18

12233.95

5272.04

162.99

6224.69

337.01

7485.75

Table 3: Performances of surge pricing policies for Manhattan case study.

Finally, we test how the static and the real-time policies are robust to variations in input statistics. We
compare the rewards generated and the total queue length applying the static and the real-time policies for

the arrival rates of May 11, 2019, Saturday between 18.00-20.00. The results are displayed in Figure 5d.

Even though the arrival rates between May 11 and May 4 do not differ much, the static policy is not resilient

and fails to stabilize when there is a slight change in the network. The real-time policy, on the other hand, is

still able to stabilize the network and generate proﬁts. The neural-network based policy is able to determine

the correct pricing and routing decisions by considering the current state of the network, even under different

arrival rates.

These experiments show us that we can indeed develop a real-time policy using deep reinforcement

learning and this policy is resilient to small changes in the network parameters. The next study investigates

the idea of generality, i.e., whether we can develop a global real-time policy and ﬁne-tune it to a speciﬁc
environment with few-shots of training, rather than developing a new policy from scratch.

Few-shot Learning: A common problem with reinforcement learning approaches is that because the agent
is trained for a speciﬁc environment, it fails to respond to a slightly changed environment. Hence, one would

need to train a different model for different environments (different network conﬁgurations, different arrival

rates). However, this is not a feasible solution considering that training one model takes millions of iterations.

As a more tractable solution, one could train a global model using different environments, and then calibrate

21

Figure 6: Performances of the speciﬁc model that is trained from scratch and

ﬁne-tuned global model (for different amounts of ﬁne-tuning as speciﬁed in the

legend): rewards (left) and queue lengths (right).

Figure 7: San Francisco divided into
m = 7 regions. Map obtained from

the San Francisco County Trans-

portation Authority [60].

it to the desired environment with fewer iterations rather than training a new model from scratch. We tested

this phenomenon by training a global model for Manhattan using various arrival rates and network conﬁgura-

tions that we extracted from different 2-hour intervals (We trained the global model for 10 million iterations).
We then trained this model for the network conﬁguration and arrival rates on May 6, 2019, Monday between

15.00-17.00. The results are displayed in Figure 6. Even with no additional training, the global model per-

forms better than the speciﬁc model trained from scratch for 2 million iterations. Furthermore, with only few

iterations, it is possible to improve the performance of the global model signiﬁcantly. This is an anticipated

result, because although the network conﬁgurations and arrival rates for different 2-hour intervals are differ-

ent, the environments are not fundamentally different (the state transitions are governed by similar random

processes) and hence it is possible to generalize a global policy and ﬁne-tune it to the desired environment

with fewer number of iterations.

5.2 Case Study in San Francisco

We conducted the case study in San Francisco by utilizing an EV ﬂeet of 420 vehicles. We divided San

Francisco into 7 regions as in Figure 7, and using the traceset of mobility of taxi cabs data from CRAWDAD

[61], we obtained the average arrival rates and travel times between regions (we exclude the rides occurring

in the same region).

In Figure 8, we compare the charging costs paid under the real-time policy and the static policy. The

static policy is generated by using the average value of the electricity prices, whereas the real-time policy

takes into account the current electricity prices before executing an action. Therefore, the real-time policy

provides cheaper charging options by utilizing smart charging strategies, decreasing the average charging
costs by 25%.

In Figure 9a, we compare the rewards and the total queue length resulting from the real-time policy and
the static policy. In Figure 9b, we compare the RL policy to the static policy with 5% higher prices than
the optimal static policy, and summarize performances of several other static pricing policies in Table 5. In

22

0.00.20.40.60.81.0Time(10mins)×1040.00.51.0AverageRewards($)×104Speciﬁcmodel(2miterations)Globalmodel-noadditionaltrainingGlobalmodel+1kiterationsGlobalmodel+10kiterationsGlobalmodel+100kiterations0.00.20.40.60.81.0Time(10mins)×1040.00.51.0AverageQueueLength×102Few−shotLearning(a)

(b)

(c)

Figure 9: Comparison of different policies for San Francisco case study. The legends for all ﬁgures are the same as the

top left ﬁgure, where red lines correspond to the real-time policy and blue lines correspond to the static policies. In all

scenarios, we use the rewards generated and the total queue length as metrics. In (a), we demonstrate the results from

applying the real-time policy and the optimal static policy. In (b), we compare the real-time policy with a sub-optimal
static policy, where the prices are 5% higher than the optimal static policy. In (c), we utilize a surge pricing policy along

with the optimal static policy and compare with the real-time policy.

Figure 9c, we use the static policy but also utilize a surge pricing policy that multiplies the prices by 1.5
if the queues are longer than 100% of the induced arrival rates. The performances of other surge pricing
policies are also displayed in Table 4. Similar to the case study in Manhattan, the results demonstrate that

the performance of the trained real-time policy is superior to the other policies. In particular, the RL policy
is able to generate around 24% more rewards and result in around 75% less queues than the best heuristic
policy, which utilizes 30% higher static prices than the optimal static policy.

Queue Threshold

50%

100%

200%

Surge Multiplier

Queue Rewards Queue Rewards Queue Rewards

1.25

1.5

2

67.62

25.16

14.06

718.66

650.90

331.21

75.92

34.32

20.55

715.02

687.71

455.25

99.56

49.94

44.44

687.45

708.38

611.23

Table 4: Performances of surge pricing policies for San Francisco case study.

23

−1.0−0.50.0Rewards($)×104Real-TimePolicyStaticPolicyStaticPolicyRun.Avg.Real-TimePolicyRun.Avg.0.00.51.01.52.02.53.03.54.0Time(5mins)×1050246TotalQueueLength×103Real−Timevs.StaticPolicy−0.8−0.40.00.40.81.21.6Rewards($)×103012345Time(5mins)×1040246TotalQueueLength×102Real−Timevs.StaticPolicy(+5%Prices)0.40.81.21.6Rewards($)×1030.00.20.40.60.81.0Time(5mins)×1040.000.250.500.751.00TotalQueueLength×102Real−Timevs.SurgePricingPolicyMetrics

% of opt. static prices

105% 110% 120% 130% 140%

Average Rewards

4.98

485.65

696.38

721.89

682.76

Average Queue Length

456.83

211.04

87.15

45.28

25.66

Table 5: Performances of static pricing policies for San Francisco case study.

6 Conclusion

In this paper, we developed a real-time control policy

based on deep reinforcement learning for operating an

AMoD ﬂeet of EVs as well as pricing for rides. Our real-

time control policy jointly makes decisions for: 1) vehicle

routing in order to serve passenger demand and to rebal-

ance the empty vehicles, 2) vehicle charging in order to

sustain energy for rides while exploiting geographical and

temporal diversity in electricity prices for cheaper charg-

ing options, and 3) pricing for rides in order to adjust
the potential demand so that the network is stable and

the proﬁts are maximized. Furthermore, we formulated

the static planning problem associated with the dynamic

problem in order to deﬁne the optimal static policy for the

static planning problem. When implemented correctly,

Figure 8: Charging costs for the optimal static policy

and the real-time policy in San Francisco case study.

the static policy provides stability of the queues in the dynamic setting, yet it is not optimal regarding the

proﬁts and keeping the queues sufﬁciently low. Finally, we conducted case studies in Manhattan and San

Francisco that demonstrate the performance of our developed policy. The two case studies on different

networks indicate that reinforcement learning can be a universal method for establishing well performing

real-time policies that can be applied to many real world networks. Lastly, by doing the Manhattan study

with non-electric vehicles and San Francisco study with electric vehicles, we have also demonstrated that

a real-time policy using reinforcement learning can be established for both electric and non-electric AMoD

systems.

References

[1] [Online]. Available:

https://www.cbinsights.com/research/autonomous-

driverless-vehicles-

corporations-list/.

[2] J. Schulman, F. Wolski, P. Dhariwal, A. Radford, and O. Klimov, “Proximal policy optimization algo-

rithms,” arXiv preprint arXiv:1707.06347, 2017.

24

012345Time(5mins)×103020406080ChargingCosts(Cents/kWh)Real−Timevs.StaticPolicyChargingCostsReal-timePolicyStaticPolicyStaticPolicyRun.Avg.Real-timePolicyRun.Avg.[3] R. Zhang and M. Pavone, “Control of robotic Mobility-on-Demand systems: A queueing-theoretical

perspective,” In Int. Journal of Robotics Research, vol. 35, no. 1–3, pp. 186–203, 2016.

[4] M. Pavone, S. L. Smith, E. Frazzoli, and D. Rus, “Robotic load balancing for Mobility-on-Demand

systems,” Int. Journal of Robotics Research, vol. 31, no. 7, pp. 839–854, 2012.

[5] F. Rossi, R. Zhang, Y. Hindy, and M. Pavone, “Routing autonomous vehicles in congested transportation
networks: Structural properties and coordination algorithms,” Autonomous Robots, vol. 42, no. 7, pp.
1427–1442, 2018.

[6] M. Volkov, J. Aslam, and D. Rus, “Markov-based redistribution policy model for future urban mobility
networks,” Conference Record - IEEE Conference on Intelligent Transportation Systems, pp. 1906–
1911, 09 2012.

[7] Q. Wei, J. A. Rodriguez, R. Pedarsani, and S. Coogan, “Ride-sharing networks with mixed autonomy,”

arXiv preprint arXiv:1903.07707, 2019.

[8] R. Zhang, F. Rossi, and M. Pavone, “Model predictive control of autonomous mobility-on-demand
systems,” in 2016 IEEE International Conference on Robotics and Automation (ICRA), May 2016.

[9] F. Miao, S. Han, S. Lin, J. A. Stankovic, H. Huang, D. Zhang, S. Munir, T. He, and G. J. Pappas, “Taxi

dispatch with real-time sensing data in metropolitan areas: A receding horizon control approach,”
CoRR, vol. abs/1603.04418, 2016. [Online]. Available: http://arxiv.org/abs/1603.04418

[10] R. Iglesias, F. Rossi, K. Wang, D. Hallac, J. Leskovec, and M. Pavone, “Data-driven model predictive
control of autonomous mobility-on-demand systems,” CoRR, vol. abs/1709.07032, 2017. [Online].
Available: http://arxiv.org/abs/1709.07032

[11] F. Miao, S. Han, A. M. Hendawi, M. E. Khalefa, J. A. Stankovic, and G. J. Pappas, “Data-driven distribu-
tionally robust vehicle balancing using dynamic region partitions,” in 2017 ACM/IEEE 8th International
Conference on Cyber-Physical Systems (ICCPS), April 2017, pp. 261–272.

[12] M. Tsao, R. Iglesias, and M. Pavone, “Stochastic model predictive control for autonomous mobility on
demand,” CoRR, vol. abs/1804.11074, 2018. [Online]. Available: http://arxiv.org/abs/1804.11074

[13] K. Spieser, S. Samaranayake, and E. Frazzoli, “Vehicle routing for shared-mobility systems with time-

varying demand,” in 2016 American Control Conference (ACC), July 2016, pp. 796–802.

[14] R. M. A. Swaszek and C. Cassandras, “Load balancing in mobility-on-demand systems: Realloca-
tion via parametric control using concurrent estimation,” 2019 IEEE Intelligent Transportation Systems
Conference (ITSC), pp. 2148–2153, 2019.

[15] M. Repoux, M. Kaspi, B. Boyacı, and N. Geroliminis, “Dynamic prediction-based relocation

policies

in one-way station-based carsharing systems with complete journey reservations,”

25

Transportation Research Part B: Methodological, vol. 130, pp. 82 – 104, 2019. [Online]. Available:
http://www.sciencedirect.com/science/article/pii/S019126151930102X

[16] B. Boyacı, K. G. Zografos,

and N. Geroliminis,

“An integrated optimization-simulation

framework for vehicle and personnel relocations of electric carsharing systems with reservations,”
Transportation Research Part B: Methodological, vol. 95, pp. 214 – 237, 2017. [Online]. Available:
http://www.sciencedirect.com/science/article/pii/S0191261515301119

[17] J. Warrington and D. Ruchti, “Two-stage stochastic approximation for dynamic rebalancing of shared
mobility systems,” Transportation Research Part C: Emerging Technologies, vol. 104, pp. 110 – 134,
2019. [Online]. Available: http://www.sciencedirect.com/science/article/pii/S0968090X18314104

[18] C. Mao and Z. Shen, “A reinforcement learning framework for the adaptive routing problem in stochastic
time-dependent network,” Transportation Research Part C: Emerging Technologies, vol. 93, pp. 179 –
197, 2018. [Online]. Available: http://www.sciencedirect.com/science/article/pii/S0968090X18307617

[19] F. Zhu and S. V. Ukkusuri, “Accounting for dynamic speed limit control

trafﬁc

tic
environment:
Part C: Emerging Technologies,
http://www.sciencedirect.com/science/article/pii/S0968090X1400028X

A reinforcement

pp. 30 – 47,

learning approach,”

vol. 41,

in a stochas-
Transportation Research

2014.

[Online]. Available:

[20] E. Walraven, M. T. Spaan, and B. Bakker, “Trafﬁc ﬂow optimization: A reinforcement learning
approach,” Engineering Applications of Artiﬁcial Intelligence, vol. 52, pp. 203 – 212, 2016. [Online].
Available: http://www.sciencedirect.com/science/article/pii/S0952197616000038

[21] F. Zhu, H. A. Aziz, X. Qian, and S. V. Ukkusuri, “A junction-tree based learning algorithm to optimize
network wide trafﬁc control: A coordinated multi-agent framework,” Transportation Research Part C:
Emerging Technologies, vol. 58, pp. 487 – 501, 2015, special Issue: Advanced Road Trafﬁc Control.
[Online]. Available: http://www.sciencedirect.com/science/article/pii/S0968090X14003593

[22] L. Li, Y. Lv, and F. Wang, “Trafﬁc signal timing via deep reinforcement learning,” IEEE/CAA Journal

of Automatica Sinica, vol. 3, no. 3, pp. 247–254, 2016.

[23] D. A. Lazar, E. Bıyık, D. Sadigh, and R. Pedarsani, “Learning how to dynamically route autonomous

vehicles on shared roads,” arXiv preprint arXiv:1909.03664, 2019.

[24] M. Han, P. Senellart, S. Bressan, and H. Wu, “Routing an autonomous taxi with reinforcement learning,”

in CIKM, 2016.

[25] M. Gu´eriau and I. Dusparic, “Samod: Shared autonomous mobility-on-demand using decentralized
reinforcement learning,” in 2018 21st International Conference on Intelligent Transportation Systems
(ITSC), Nov 2018, pp. 1558–1563.

26

[26] J. Wen, J. Zhao, and P. Jaillet, “Rebalancing shared mobility-on-demand systems: A reinforcement
learning approach,” in 2017 IEEE 20th International Conference on Intelligent Transportation Systems
(ITSC), Oct 2017, pp. 220–225.

[27] K. Lin, R. Zhao, Z. Xu, and J. Zhou, “Efﬁcient large-scale ﬂeet management via multi-agent deep
reinforcement learning,” in Proceedings of the 24th ACM SIGKDD International Conference on
Knowledge Discovery & Data Mining, ser. KDD ’18. New York, NY, USA: Association for Computing
Machinery, 2018, p. 1774–1783. [Online]. Available: https://doi.org/10.1145/3219819.3219993

[28] C. Mao, Y. Liu,

and Z.-J. M.

Shen,

“Dispatch

of

A deep

reinforcement

learning

approach,”

autonomous

for
Transportation Research

vehicles

Technologies,

vol.

115,

p.

102626,

2020.

[Online]. Available:

taxi
services:
Part C: Emerging

http://www.sciencedirect.com/science/article/pii/S0968090X19312227

[29] E. Veldman and R. A. Verzijlbergh, “Distribution grid impacts of smart electric vehicle charging from
different perspectives,” IEEE Transactions on Smart Grid, vol. 6, no. 1, pp. 333–342, Jan 2015.

[30] W. Su, H. Eichi, W. Zeng, and M. Chow, “A survey on the electriﬁcation of transportation in a smart
grid environment,” IEEE Transactions on Industrial Informatics, vol. 8, no. 1, pp. 1–10, Feb 2012.

[31] J. C. Mukherjee and A. Gupta, “A review of charge scheduling of electric vehicles in smart grid,” IEEE

Systems Journal, vol. 9, no. 4, pp. 1541–1553, Dec 2015.

[32] T. D. Chen, K. M. Kockelman, and J. P. Hanna, “Operations of a Shared, Autonomous, Electric Vehicle
Fleet: Implications of Vehicle & Charging Infrastructure Decisions,” Transportation Research Part A:
Policy and and Practice, vol. 94, pp. 243–254, 2016.

[33] C. Bongiovanni, M. Kaspi, and N. Geroliminis, “The electric autonomous dial-a-ride problem,”
Transportation Research Part B: Methodological, vol. 122, pp. 436 – 456, 2019. [Online]. Available:
http://www.sciencedirect.com/science/article/pii/S0191261517309669

[34] N. Tucker, B. Turan, and M. Alizadeh, “Online Charge Scheduling for Electric Vehicles in Autonomous
Mobility on Demand Fleets,” In Proc. IEEE Int. Conf. on Intelligent Transportation Systems, 2019.

[35] B. Turan, N. Tucker, and M. Alizadeh, “Smart Charging Beneﬁts in Autonomous Mobility on Demand
Systems,” In Proc. IEEE Int. Conf. on Intelligent Transportation Systems, 2019. [Online]. Available:
https://arxiv.org/abs/1907.00106

[36] F. Rossi, R. Iglesias, M. Alizadeh, and M. Pavone, “On the interaction between autonomous mobility-
on-demand systems and the power network: models and coordination algorithms,” Robotics: Science
and Systems XIV, Jun 2018.

[37] T. D. Chen and K. M. Kockelman, “Management of a shared autonomous electric vehicle ﬂeet: Impli-
cations of pricing schemes,” Transportation Research Record, vol. 2572, no. 1, pp. 37–46, 2016.

27

[38] Y. Guan, A. M. Annaswamy, and H. E. Tseng, “Cumulative prospect theory based dynamic pricing
for shared mobility on demand services,” CoRR, vol. abs/1904.04824, 2019. [Online]. Available:
http://arxiv.org/abs/1904.04824

[39] C. J. R. Sheppard, G. S. Bauer, B. F. Gerke, J. B. Greenblatt, A. T. Jenn, and A. R. Gopal, “Joint

optimization scheme for the planning and operations of shared autonomous electric vehicle ﬂeets
serving mobility on demand,” Transportation Research Record, vol. 2673, no. 6, pp. 579–597, 2019.
[Online]. Available: https://doi.org/10.1177/0361198119838270

[40] K. Bimpikis, O. Candogan, and D. Sab´an, “Spatial pricing in ride-sharing networks,” Operations Re-

search, vol. 67, pp. 744–769, 2019.

[41] R. Pedarsani, J. Walrand, and Y. Zhong, “Robust scheduling for ﬂexible processing networks,” Advances

in Applied Probability, vol. 49, no. 2, pp. 603–628, 2017.

[42] L. P. Kaelbling, M. L. Littman, and A. W. Moore, “Reinforcement learning: A survey,” Journal of

artiﬁcial intelligence research, vol. 4, pp. 237–285, 1996.

[43] P. A. Lopez, M. Behrisch, L. Bieker-Walz, J. Erdmann, Y.-P. Fl¨otter¨od, R. Hilbrich, L. L¨ucken,
J. Rummel, P. Wagner, and E. Wießner, “Microscopic trafﬁc simulation using sumo,” in The 21st IEEE
International Conference on Intelligent Transportation Systems.
IEEE, 2018. [Online]. Available:
https://elib.dlr.de/124092/
(cid:104)

[44] I. Grondman, L. Busoniu, G. A. D. Lopes, and R. Babuska, “A survey of actor-critic reinforcement
learning: Standard and natural policy gradients,” IEEE Transactions on Systems, Man, and Cybernetics,
Part C (Applications and Reviews), vol. 42, no. 6, pp. 1291–1307, Nov 2012.

[45] C. J. C. H. Watkins and P. Dayan, “Q-learning,” Machine Learning, vol. 8, no. 3, pp. 279–292, May

1992. [Online]. Available: https://doi.org/10.1007/BF00992698

[46] G. A. Rummery and M. Niranjan, “On-line q-learning using connectionist systems,” Tech. Rep., 1994.

[47] R. J. Williams, “Simple statistical gradient-following algorithms for connectionist reinforcement

learning,” Machine Learning, vol. 8, no. 3, pp. 229–256, May 1992.
https://doi.org/10.1007/BF00992696

[Online]. Available:

[48] A. G. Barto, R. S. Sutton, and C. W. Anderson, “Neuronlike adaptive elements that can solve difﬁcult
learning control problems,” IEEE Transactions on Systems, Man, and Cybernetics, vol. SMC-13, no. 5,
pp. 834–846, Sep. 1983.

[49] I. H. Witten, “An adaptive optimal controller for discrete-time markov environments,” Information and

Control, vol. 34, pp. 286–295, 1977.

[50] J. Schulman, S. Levine, P. Moritz, M. I. Jordan, and P. Abbeel, “Trust region policy optimization,”

CoRR, vol. abs/1502.05477, 2015. [Online]. Available: http://arxiv.org/abs/1502.05477

28

[51] M. Grant and S. Boyd, “CVX: Matlab software for disciplined convex programming, version 2.1,”

http://cvxr.com/cvx, Mar. 2014.

[52] G. Brockman, V. Cheung, L. Pettersson, J. Schneider, J. Schulman, J. Tang, and W. Zaremba, “Openai

gym,” 2016.

[53] A. Hill, A. Rafﬁn, M. Ernestus, A. Gleave, R. Traore, P. Dhariwal, C. Hesse, O. Klimov,

A. Nichol, M. Plappert, A. Radford, J. Schulman, S. Sidor, and Y. Wu, “Stable baselines,”

https://github.com/hill-a/stable-baselines, 2018.

[54] The average electric car in the US is getting cheaper. [Online]. Available: https://qz.com/1695602/the-

average-electric-vehicle-is-getting-cheaper-in-the-us/.

[55] [Online]. Available: http://oasis.caiso.com

[56] United States Average Hourly Wages. [Online]. Available: https://tradingeconomics.com/united-

states/wages.

[57] How much

does

driving

your

car

cost,

per minute?

[Online]. Available:

https://www.bostonglobe.com/ideas/2014/08/08/how-much-driving-really-costs-per-

minute/BqnNd2q7jETedLhxxzY2CI/story.html.

[58] [Online]. Available: https://www1.nyc.gov/site/tlc/about/tlc-trip-record-data.page

[59] T. P. Lillicrap, J. J. Hunt, A. Pritzel, N. Heess, T. Erez, Y. Tassa, D. Silver, and D. Wierstra, “Continuous

control with deep reinforcement learning,” arXiv preprint arXiv:1509.02971, 2015.

[60] [Online]. Available: http://tncstoday.sfcta.org/

[61] M. Piorkowski, N. Saraﬁjanovic-Djukic, and M. Grossglauser, “CRAWDAD dataset epﬂ/mobility (v.

2009-02-24),” Downloaded from https://crawdad.org/epﬂ/mobility/20090224, Feb. 2009.

[62] J. G. Dai, “On positive harris recurrence of multiclass queueing networks: A uniﬁed approach via ﬂuid

limit models,” Annals of Applied Probability, vol. 5, pp. 49–77, 1995.

29

Appendices

A Proof of Proposition 1

To prove Proposition 1, we ﬁrst formulate the static optimization problem via a network ﬂow model that char-
acterizes the capacity region of the network for a given set of prices (cid:96)ij(t) = (cid:96)ij
t).
The capacity region is deﬁned as the set of all arrival rates [Λij]i,j∈M, where there exists a charging and rout-
ing policy under which the queueing network of the system is stable. Let xv
i be the number of vehicles
available at node i, αv
ij be the fraction of vehicles at node i with energy level v being routed to node j, and
αv
ic be the fraction of vehicles charging at node i starting with energy level v. We say the static vehicle
allocation for node i and energy level v is feasible if αv

t (Hence, Λij(t) = Λij

αv

1.

∀

∀

ic + (cid:80)

j∈M
j(cid:54)=i

ij ≤

The optimization problem that characterizes the capacity region of the network ensures that the total
number of vehicles routed from i to j is at least as large as the nominal arrival rate to the queue (i, j).
Namely, the vehicle allocation problem can be formulated as follows:

min

i ,αv
xv

ij ,αv
ic

subject to

ρ

Λij

vmax(cid:88)

≤

v=vij

i αv
xv

ij ∀

i, j

,
∈ M

αv

ic +

ρ

≥

(cid:88)

j∈M
j(cid:54)=i

αv

i
ij ∀

,
∈ M

v
∀

,

∈ V

i = xv−1
xv

i αv−1

ic +

(cid:88)

j∈M

xv+vji
i

αv+vji
ji

i
∀

,
∈ M

v
∀

,

∈ V

αvmax

ic = 0
ij = 0

αv

∀
0, αv
ij ≥
ic = αv

xv
i ≥
i = αv
xv

,
i
∀
∈ M
v < vij,
0 αv

∀
ic ≥

ij = 0

i, j

∈ M
i, j

0,

∀
v /

,

∀

∈ V

∀

,

,
∈ M
i, j

v
∀

∈ V
.
∈ M

(14a)

(14b)

(14c)

(14d)

(14e)

(14f)

(14g)

(14h)

The constraint (14c) upper bounds the allocation of vehicles for each node i and energy level v. The con-
ic/xv
i ,

straints (14d)-(14f) are similar to those of optimization problem (1) with xv
and αv

j∈M xv

ic = xv

ic+(cid:80)

i = xv

ij, αv

ij = xv

ij/xv
i .

Lemma 1. Let the optimal value of (14) be ρ∗. Then, ρ∗
stability of the system under some routing and charging policy.

≤

1 is a necessary and sufﬁcient condition of rate

Proof. Consider the ﬂuid scaling of the queueing network, Qrt
the stability of ﬂuid models), and let Qt
follows:

(see [62] for more discussion on
ij be the corresponding ﬂuid limit. The ﬂuid model dynamics is as

r

ij = qij ((cid:98)rt(cid:99))

Qt

ij = Q0

ij + At

ij −

X t

ij,

30

where At
X t
policy under which for all t
is differentiable for all (i, j). Then, for all (i, j),
˙X t1
other hand,
all (i, j) and there exists αv
vector [αv

ij is the total number of riders from node i to node j that have arrived to the network until time t and
ij is the total number of vehicles routed from node i to j up to time t. Suppose that ρ∗ > 1 and there exists a
ij = 0. Pick a point t1, where Qt1
ij
ij = Λij. On the
i αv
xv
ij for
ic at time t1 such that the ﬂow balance constraints hold and the allocation
ic + (cid:80)m

ij is the total number of vehicles routed from i to j at t1. This implies Λij = (cid:80)vmax

0 and for all origin-destination pairs (i, j), Qt

ij = Λij, this implies ˙X t1

1. This contradicts ρ∗ > 1.

ic] is feasible, i.e. αv

ij = 0. Since ˙At1

ij and αv

˙Qt1

v=vij

αv

≥

ij αv
Now suppose ρ∗

ij ≤

j=1
j(cid:54)=i
ij αv∗

≤

1 and α∗ = [αv∗
lative number of vehicles routed from node i to j up to time t is St
Suppose that for some origin-destination pair (i, j), the queue Qt1
(0, t1) such that Qt0
continuity of the ﬂuid limit, there exists t0
˙Qt

ij > 0 implies Λij > (cid:80)vmax

ij, which is a contradiction.

i αv
xv

∈

v=vij

ic ] is an allocation vector that solves the static problem. The cumu-
Λijt.

ijt = (cid:80)vmax

ij = (cid:80)vmax

v=0 xv

i αv
xv

i αv

ijt

v=vij
(cid:15) > 0 for some positive t1 and (cid:15). By
[t0, t1]. Then,

ij > 0 for t

ij ≥
ij = (cid:15)/2 and Qt

≥

∈

By Lemma 1, the capacity region CΛ of the network is the set of all Λij

optimal solution to the optimization problem (14) satisﬁes ρ∗
and charging policy such that the queues will be bounded away from inﬁnity.

1. As long as ρ∗

≤

≤

∈

R+ for which the corresponding
1, there exists a routing

The platform operator’s goal is to maximize its proﬁts by setting prices and making routing and charging

decisions such that the system remains stable. In its most general form, the problem can be formulated as

follows:

max
i ,αv

ij ,αv
ic

(cid:96)ij ,xv

U (Λij((cid:96)ij), xv

i , αv

ij, αv
ic)

(15)

subject to

[Λij((cid:96)ij)]i,j∈M

CΛ,

∈

ic = xv

) is the utility function that depends on the prices, demand for rides and the vehicle decisions.
where U (
·
Recall that xv
j∈M αv

ij = 1
when ρ∗
1, the platform operator’s proﬁt maximization problem can be stated as (1). A feasible solution
of (1) guarantees rate stability of the system, since the corresponding vehicle allocation problem (14) has
solution ρ∗

ij. Using these variables and noting that αv

ic and xv

ic + (cid:80)

ij = xv

i αv

i αv

≤

1.

≤

B Proof of Proposition 2

For brevity of notation, let β +pi = Pi. Let νij be the dual variables corresponding to the demand satisfaction
constraints and µv
i be the dual variables corresponding to the ﬂow balance constraints. Since the optimization
)) and Slater’s condition
problem (1) is a convex quadratic maximization problem (given a with uniform F (
·
is satisﬁed, strong duality holds. We can write the dual problem as:

min
νij ,µv
i

max
(cid:96)ij

subject to

m
(cid:88)

m
(cid:88)

(cid:18)

λij(1

i=1

j=1

(cid:96)ij
(cid:96)max

−

) ((cid:96)i

νij)

−

(cid:19)

νij
0,
≥
νij + µv

i −

31

µv−vij

βτij

0,

≤

−

(16a)

(16b)

(16c)

For ﬁxed νij and µv

i , the inner maximization results in the optimal prices:

µv
i −

µv+1
i −

Pi

≤

0

i, j, v.

∀

(cid:96)∗
ij =

(cid:96)max + νij
2

.

(16d)

(17)

ij and
∗, which completes the ﬁrst part of the proposition. The dual problem with optimal prices in (17) can be

By strong duality, the optimal primal solution satisﬁes the dual solution with optimal dual variables ν∗
µv
i
written as:

min
νij ,µv
i

subject to

m
(cid:88)

m
(cid:88)

i=1

j=1

λij
(cid:96)max

(cid:18) (cid:96)max

−
2

(cid:19)2

νij

νij
0,
≥
νij + µv
µv
i −

i −
µv+1
i −

µv−vij
j

Pi

≤

−
0

βτij

0,

≤
i, j, v.

∀

(18a)

(18b)

(18c)

(18d)

The objective function in (18a) with optimal dual variables, along with (17) suggests:

P =

m
(cid:88)

m
(cid:88)

i=1

j=1

λij
(cid:96)max

((cid:96)max

ij)2,
(cid:96)∗

−

where proﬁts P is the value of the objective function of both optimal and dual problems. To get the upper
bound on prices, we go through the following algebraic calculations using the constraints. The inequality
(18d) gives:

and equivalently:

The inequalities (18c) and (18b) yield:

and equivalently:

Inequalities (19) and (21):

And ﬁnally, the constraint (18c):

νij

µv−vji
i

µv−vij
j

≤

≤

vjiPi + µv
i ,

vijPj + µv
j .

µv
i −

µv−vij
j

βτij

0,

≤

−

µv
j −

µv−vji
i

βτji

0,

≤

−

µv
j ≤

µv

i + βτji + vjiPi.

(19)

(20)

(21)

(22)

≤
(20)

≤
(22)

≤

βτij + µv−vij

j

µv
i

−

βτij + vijPj + µv

j −

µv
i

βτij + vijPj + βτji + vjiPi.

32

Replacing Pi = pi + β and rearranging the terms:

νij

≤

β(τij + τji + vij + vji) + vijpj + vjipi.

(23)

Using the upper bound on the dual variables νij and (17), we can upper bound the optimal prices.

33

