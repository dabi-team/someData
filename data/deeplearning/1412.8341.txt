MASARYK UNIVERSITY
Faculty of Science
Department of Theoretical physics and Astrophysics

Master’s thesis

Spectral classiﬁcation using convolutional neural networks

Bc. Pavel Hála

Brno 2014

4
1
0
2
c
e
D
9
2

]

V
C
.
s
c
[

1
v
1
4
3
8
.
2
1
4
1
:
v
i
X
r
a

 
 
 
 
 
 
Bibliographic entry

Author:

Bc. Pavel Hála
Faculty of Science, Masaryk University
Department of Theoretical Physics and Astrophysics

Title of thesis:

Spectral classiﬁcation using
convolutional neural networks

Degree programme:

Physics

Field of study:

Theoretical Physics and Astrophysics

Supervisor:

Mgr. Filip Hroch, Ph.D.

Year of defence:

2014

Keywords:

classiﬁcation, spectra, galaxies,
deep learning, artiﬁcial intelligence, quasars,
stars, convolutional neural network

Abstract:

There is a great need for accurate and autonomous spectral classiﬁcation methods in astro-
physics. This thesis is about training a convolutional neural network (ConvNet) to recognize
an object class (quasar, star or galaxy) from one-dimension spectra only. Author developed
several scripts and C programs for datasets preparation, preprocessing and postprocessing
of the data. EBLearn library (developed by Pierre Sermanet and Yann LeCun) was used
to create ConvNets. Application on dataset of more than 60000 spectra yielded success
rate of nearly 95%. This thesis conclusively proved great potential of convolutional neural
networks and deep learning methods in astrophysics.

I’d like to thank my girlfriend Elena Lindišová for her moral support during research that
led to writing this thesis.

This research has made use of the HEAsoft software for astrophysical data analysis, created
by NASA Goddard Space Flight Center.

Funding for SDSS-III has been provided by the Alfred P. Sloan Foundation, the Partici-
pating Institutions, the National Science Foundation, and the U.S. Department of Energy
Oﬃce of Science. The SDSS-III web site is http://www.sdss3.org/.

SDSS-III is managed by the Astrophysical Research Consortium for the Participating
Institutions of the SDSS-III Collaboration including the University of Arizona, the Brazil-
ian Participation Group, Brookhaven National Laboratory, Carnegie Mellon University,
University of Florida, the French Participation Group, the German Participation Group,
Harvard University, the Instituto de Astroﬁsica de Canarias, the Michigan State/Notre
Dame/JINA Participation Group, Johns Hopkins University, Lawrence Berkeley National
Laboratory, Max Planck Institute for Astrophysics, Max Planck Institute for Extraterres-
trial Physics, New Mexico State University, New York University, Ohio State University,
Pennsylvania State University, University of Portsmouth, Princeton University, the Span-
ish Participation Group, University of Tokyo, University of Utah, Vanderbilt University,
University of Virginia, University of Washington, and Yale University.

I declare that I wrote my diploma thesis independently and exclusively using sources cited.
I agree with borrowing the work and its publishing.

In Brno, May 14th 2014

Pavel Hála

Contents

1 Introduction

1.1 Motivation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
1.2 Goals

7
7
8

2 Data description

11
2.1 SDSS-III DR10 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 11
2.2 BOSS . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 12
2.3 Spectroscopic Pipeline . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 13
2.4 NOQSO parameter . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 14

3 Deep learning

15
3.1 Artiﬁcial intelligence . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 15
3.2 Classiﬁcation based on pattern recognition . . . . . . . . . . . . . . . . . . . 15
3.3 Multilayer perceptron . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 16
3.4 Review of the multilayer perceptron . . . . . . . . . . . . . . . . . . . . . . 19
3.5 Shallow and deep architectures
. . . . . . . . . . . . . . . . . . . . . . . . . 21
3.6 Visual cortex . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 22
3.7 How to build a deep network . . . . . . . . . . . . . . . . . . . . . . . . . . 24

4 Convolutional neural networks

27
4.1 Description . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 27
4.2 Advantages
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 30
4.3 Achievements . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 30

5 Practical Application

33
5.1 Preparing the dataset
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 33
5.2 Preprocessing . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 41
5.3 Designing the ConvNet . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 42

5

6 Results

45
6.1 Training . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 45
6.2 Classiﬁcation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 50
6.3 Examples of classiﬁed spectra . . . . . . . . . . . . . . . . . . . . . . . . . . 51
6.3.1 Correctly classiﬁed objects . . . . . . . . . . . . . . . . . . . . . . . . 52
6.3.2
Incorrectly classiﬁed objects . . . . . . . . . . . . . . . . . . . . . . . 55
6.3.3 Wrong labels in the BOSS catalog . . . . . . . . . . . . . . . . . . . 56

7 Conclusion

59

A Software manual

61
A.1 Datasets preparation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 62
A.2 Preprocessing . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 65
A.3 Training and classiﬁcation . . . . . . . . . . . . . . . . . . . . . . . . . . . . 67

6

Chapter 1

Introduction

1.1 Motivation

My diploma thesis should be considered as a loose continuation of my bachelor’s thesis.
In that work [24], I utilised state of the art technology (recently launched Fermi’s LAT
telescope) to create spectral studies of quasars and blazars with unprecedented angular
precision and time resolution. It gave me two important impressions.

The ﬁrst was that there is vast amount of spectral event data. Although I studied only
several objects, I was dealing with substantial amount of data and it took me a lot of
time and eﬀort to obtain even basic spectral charasteristics. Large scale study of hundreds
or thousands of objects is unimaginable for an individual and very diﬃcult for a team of
people. And the amount of data is rising each day as the Fermi telescope continues in
collecting fresh data.

Second impression was that obtained data were very noisy and chaotic. I had to deploy
sophisticated algorithms and mathematical procedures in order to reconstruct the spectra. I
often wasn’t able to ﬁnd logic behind those algorithms due to their complexity. This feeling
wasn’t unjustiﬁed because many of these algorithms were later updated or rewritten. The
telescope was brand new and instrumental functions were aﬄicted by incorrect preﬂight
assumptions of engineers who developed the instrument.

Both of these impressions led me to hunt for an alternative approach to spectral analy-
sis of active galaxies. I found out there already is a branch of astrophysics dealing with the
problems I mentioned above. It’s an emerging ﬁeld of astroinformatics. I began to experi-
ment with artiﬁcial neural networks, namely FANN library[32]. I spent two years mastering
feedforward ANNs, developing optimization algorithms to enhance their sucess rate. I ap-
plied them not only in astrophysics, but also in econometrics and quantitative ﬁnance. I
came to conclusion that although for some applications their performance is pretty good,
human time invested into pre and post processing has to be extensive. The results were
very sensitive to network parameters and data normalization. This deﬁnitely isn’t how I

7

imagine an artiﬁcial intelligence. Luckily less than a year ago I began to study deep learn-
ing, only recently discovered and the most promising ﬁeld of artiﬁcial intelligence. And I
was literally blown away by it’s performance. I decided this is the path for more eﬃcient
analysis of galactic spectra.

When I had the tool I needed to ﬁnd a source of suitable spectral data. It had to oﬀer
diverse and large enough database of specra. Besides, I needed the spectra to be correctly
classiﬁed with reliable basic spectral parameters. This is not something Fermi’s data archive
could oﬀer. After some research I decided to proceed with Sloan Digital Sky Survey (SDSS),
namely the BOSS [1], which is aimed on study of galaxies and quasars. SDSS has a long
track record of quality data capture with correct spectral classiﬁcation. Archive of the data
acquired by BOSS instrument contains approximately 1.5 million spectra.

1.2 Goals

This diploma thesis is focused on the algorithmic part of the spectral analysis. The main
eﬀort is development of eﬃcient method of spectral classiﬁcation. I want to highlight that
I won’t focus on physical interpretation of spectral features of speciﬁc objects. Rather
I will try to develop a universal framework everybody can easily use for determination
of whatever spectral features he wishes. Reliability, adaptability and robustness are the
qualities I want to maximize.

My goal is to simulate real conditions of spectral analysis. In that situation astronomers
often don’t have any information about the source they are observing and are unaware of
instrumental discrepancies of the detector. As an input into my algorithm I will use only
spectra themselves. No additional information about the source will be included. By study-
ing the data from SDSS archive, I found out there are not many spectral parameters that
are reliably deﬁned. Actually only class of the object (galaxy, quasar, star) and its redshift
are determined for every object.

Goals of the thesis are:

• Clarify the background of deep learning and introduce the most advanced represen-

tative of computer vision — convolutional neural networks.

• Use a convolutional neural network to construct an algorithm capable of reliable
object type classiﬁcation (galaxy, quasar, star) based on object’s optical spectrum
only.

• Test the algorithm on large subset of spectra and evaluate its accuracy.

8

These goals will be particularly hard given the nature of the classiﬁcation. Choosing
the correct type of object can be often unambiguous, especially selecting between galaxy
and quasar. This is exactly the type of human-like classiﬁcation that is hard to automate.
But the size of the database (1.5 mil objects) underlines the need for robust automation
methods. If sucessful, most promising will be utilization of this method in future data-
intensive missions without long history of spectral analysis tools like the SDSS.

9

10

Chapter 2

Data description

2.1 SDSS-III DR10

I’ve been using data from the third Sloan Digital Sky Survey in its tenth data release
(DR10). It’s one of the largest sky surveys available and its data are freely available to
the public. Major changes in Data Release 10 are the new infrared spectra from APOGEE
spectrograph and addition of hundreds of thousands spectra from the BOSS instrument.
For my purpose is important that DR10 incorporates new reductions and galaxy parameter
ﬁts for all the BOSS data.

Figure 2.1: DR10 SDSS/BOSS sky coverage [2].

11

2.2 BOSS

BOSS stands for Baryon Oscillation Spectroscopic Survey. It maps the spatial distribution
of luminous red galaxies and quasars to detect the characteristic scale imprinted by baryon
acoustic oscillations in the early universe. Sound waves that propagated in the early uni-
verse, like spreading ripples in a pond, imprinted a characteristic scale on cosmic microwave
background ﬂuctuations. These ﬂuctuations have evolved into today’s walls and voids of
galaxies, meaning this baryon acoustic oscillation scale (about 150 Mpc) is visible among
galaxies today.

Figure 2.2: Illustration of baryon acoustic oscilations [1].

For my purpose it’s important that BOSS’s main target are galaxies and quasars. In
order to map distibution of galaxies and quasars it has to identify all objects ﬁrst. I assume
there is a great emphasis placed on the precision of the identiﬁcation. That’s important. For
example the BOSS’s own classiﬁcation pipeline is doing a subclassiﬁcation after it classiﬁes
the object as a galaxy, quasar or star. In this subclassiﬁcation one estimates stellar class (if
the object is a star) or galaxy subtype (starburst, starforming, etc.). Previously, I wanted
to include this subclass in my algorithm. But I subsequently learned that this subclass
identiﬁcation in the BOSS catalog is quite inaccurate. It has never been a priority and
there is no plan to ﬁx it [3].

12

2.3 Spectroscopic Pipeline

BOSS project uses idlspec2d software for automated object classiﬁcation and redshift
measurement [17]. The spectral classiﬁcation and redshift-ﬁnding analysis is performed
as a χ2 minimization. Linear ﬁts are made to each observed spectrum using multiple
templates and combinations of templates evaluated for all allowed redshifts, and the global
minimum-χ2 solution is adopted for the output classiﬁcation and redshift. The least-squares
minimization is performed by comparison of each spectrum to full range of templates
spanning galaxies, quasars, and stars[17].

Figure 2.3: Redshift χ2 minimization process [17].

The processed spectra are cut oﬀ into 3600 Å–10400 Å range. Z_WARNING ﬂag is assigned
to each spectrum after the classiﬁcation. Only spectra with Z_WARNING equal to zero are
considered good for scientiﬁc analysis.

Overall precision of object classiﬁcation and redshift is highly dependent on many

Bit Name
0
1
2
3
4
5

SKY
LITTLE_COVERAGE
SMALL_DELTA_CHI2 ∆χ2
NEGATIVE_MODEL
MANY_OUTLIERS
Z_FITLIMIT

Deﬁnition
Sky ﬁber
Insuﬃcient wavelength coverage

r between best and second-best ﬁt <0.01

Synthetic spectrum negative
More than 5% of points above 5σ from synth. spectrum
χ2 minimum for best model at the edge of the redshift
range

6
7

NEGATIVE_EMISSION Negative emission in a quasar line >3σ
UNPLUGGED

Broken or unplugged ﬁber

Table 2.1: Redshift and classiﬁcation warning ﬂags [17].

13

zχr2trialzz2nd bestχr2∆too∆zsmallparabola fit toestimate errorδzbestvariables. One of them is the quality of templates used for ﬁtting observed spectra. Another
critical parameter is δχ2
r in the minimization process (ﬁgure 2.3). Alternative method with
less parameters would be desirable. Especially for sky surveys with not as long track record
as SDSS.

2.4 NOQSO parameter

I had an issue with galaxy spectra. According to [3], all the galactic spectral parameters
have to use the NOQSO appendix. For instance, instead of Z to use Z_NOQSO, etc. It’s because
the QSO emission lines can ﬁt galaxy absorption lines resulting in erroneous ﬁts. Using
the NOQSO appendix assures the QSO templates are excluded when performing classiﬁca-
tion/redshift ﬁts.

However my personal experience is that using NOQSO appendix leads to incorrect object
classes and erroneous redshifts. I suspect the information on the SDSS website [3] and in the
corresponding paper [17] to be relevant for older data releases only. Further clariﬁcation
would be helpful from somebody in the BOSS team. I made several mass crosschecks
and veriﬁed random group of spectra manually. That reassured me to don’t use NOQSO
parameter.

14

Chapter 3

Deep learning

3.1 Artiﬁcial intelligence

In the ﬁrst half of the 20th century, people thought about artiﬁcial intelligence (AI) in
direct connection with robots. During decades, advancement in robotics proved to be quite
disconnected with advancement in AI. Today we have robots all around us, but they tend
to be somewhat dumb. Our modern computers have exponentially higher computational
performance than our grandparents would imagine. However powerful doesn’t meen intel-
ligent. Even the comically looking robots from black-and-white ﬁlms from the 30s, respond
more intelligently than today’s hexacore servers. Artiﬁcial intelligence we are seeking is
the ability to behave eﬃciently in a real world. That’s an environment with blurred inputs
and unexpected events.

3.2 Classiﬁcation based on pattern recognition

Real development in the ﬁeld of artiﬁcial intelligence started to happen in second half of the
20th century. And ironically up to this date, most promising way seems to be simulation
of real biological processes in living creatures. I will be doing spectral classiﬁcation in this
thesis. Therefore I have to describe how an AI-like approach to pattern recognition was
originally developed. The following description is based on paper [18] by David Bouchain.
The problem in general can be described as a classiﬁcation of input data, represented
as vectors, into several categories. We need to ﬁnd a suitable classiﬁcation function F that
ﬁts the equation

yp = F (xp, W )

(3.1)

where p is the number of pattern, xp is the p-th input vector (an image, spectrum, etc),
yp is the output vector, and W is a set of trainable parameters for the classiﬁer (3.1). If

15

there exists a vector Tp representing the desired output for each input vector xp, then Θ
is called a training set and is deﬁned as

(cid:110)

(xp, Tp) : xp ∈ Rd, Tp ∈ Rn, p = 1, ..., P

(cid:111)

Θ =

(3.2)

Adjusting or training the parameters based on comparison between output yp and
label Tp is called a supervised learning. To make a good classiﬁer we have to determine
the amount of adjustment for the parameters W . This is done through error function E(W )
for all patterns, or Ep(W ) for the p-th pattern and returns error of the classiﬁer. Our task
during learning is to minimize this error. Probably the best method for pattern recognition
is called gradient-based learning. The idea is to sequentially adjust the parameters W =
W (t), where t is the current training step. The minimization of Ep(W ) will occur by using

W (t + 1) = W (t) − (cid:15)

∂Ep(W (t))
∂W (t)

(3.3)

equation for each pattern. (cid:15) is a learning rate.

3.3 Multilayer perceptron

One of the ﬁrst representatives of this approach were the artiﬁcial neural networks (ANNs)
[4]. It’s a computational model inspired by the nervous system. It’s capable of learning
patterns in the underlying data (inputs) and can be trained to give us anticipated answers
(outputs). It can handle noisy data and can cope with highly nonlinear data. It was sucess-
fully applied to AI-like types of problems (hard or impossible to solve by computers) like
handwriting recognition or speech recognition.

ANN is composed of interconnected artiﬁcial neurons. Like a biological neuron has
dendrites, artiﬁcial neuron has inputs that are feeding the data into the neuron. Each
input has assigned weight. Weighted sum is made of all inputs and the result is fed into
the activation function. That is continuous, usually (but not necessarily) highly nonlinear
function. The neuron ampliﬁes the inputs according to their relevance in minimizing the
error rate and transforms the output.

The model of neuron in ﬁgure 3.1 is called a perceptron. Inputs from other neurons
form a vector x. Each input has weight assigned to it. All inputs from the vector x are
multiplied by corresponding weights from the vector w. The product is summed up and
bias θ is added.

u =

n
(cid:88)

k=1

xkwk − θ

(3.4)

Activation function f (u) ampliﬁes/deampliﬁes it and produces output y from the neu-
ron. Most common acivation function is a sigmoid function in equation (3.6) (others are

16

Figure 3.1: Schematics of a perceptron [18].

gauss, stepwise, linear, etc.). The equation (3.5) and the whole process is obviously similar
to the equation (3.1).

y = f (u)

f (u) =

1
1 + e−βu

(3.5)

(3.6)

Figure 3.2: Multilayer perceptron model [18].

17

Multiple perceptrons can be connected together to form a multilayer perceptron (MLP).
MLPs are usually fully connected 3-layer networks. First layer consists of input neurons that
don’t perform any calculation. They just source the data and distribute them to neurons
in the second, hidden layer. Hidden layer and output layer follow the steps described in
equations (3.4) and (3.5). MLP is a feedforward neural network model. Information ﬂows
only one way from the input layer through the hidden layer to the output layer. MLP uses
supervised learning by backpropagation algorithm. Supervised learning means that inputs
are labeled. Therefore the network knows what’s a desired output from each input dataset.
After each run of the network, errors are computed by comparing the output to the desired
output. Errors then propagate back through the network and algorithm computes deltas
of all weights. Finally all weights are updated and the network is ready for another run.

The error function E(W ) over all patterns p is deﬁned as

E(W ) =

Ep(W ) =

(cid:88)

p

(cid:88)

p

||Tp − yp||2

(3.7)

where yp is output from the output layer, Tp is desired output from equation (3.2). W is
a matrix of weights wij between i-th and j-th neuron. After diﬀerentiation of Ep by wij
and slight modiﬁcations the learning rule for neurons in the output layer is obtained:

wij(t + 1) = wij(t) + (cid:15)yiδj
δj = (Tj − yj)f (cid:48)(uj)
where yj is output of neuron j in the output layer, yi is output of neuron i in the hidden
layer and Tj is desired output. Learning function for the hidden layer is then

(3.9)

(3.8)

wki(t + 1) = wki(t) + (cid:15)xkδi

δi =

(cid:88)

j

δjwijf (cid:48)(ui)

(3.10)

(3.11)

where j respresents neurons from output layer, i neurons from hidden layer and k input
neurons. ui and uj are respective dendrite potentials.

Important issue in neural nets is bias and variance. It is directly related to overﬁtting
which is a common problem in ANNs. There is a tendency to craft the architecture and
parameters of the net in a way that keeps its ability to generalize. The net is always trained
only on a subset of available data. Good ANN should be able to perform sucessfully on data
it hasn’t seen before. We tend to keep the network compact, with only as little number of
neurons as possible. Huge net quickly overﬁts during training. Rather than learn decisive
features of the system, it ﬁts to the noise and its performance on out-of-sample data is
poor.

On the other hand if we shrink the network too much, its simplicity prevents it to learn
ﬁne patterns in the data. These patterns are often crucial for correct classiﬁcation. The
net performs similarly on all datasets — similarly poor.

18

3.4 Review of the multilayer perceptron

I have multiyear experience with artiﬁcial neural networks, namely the multilayer percep-
tron with a single hidden layer. I’ve been using FANN library written in C [32]. I’ve written
various utility bash scripts and optimization C programs for this library. I’ve come to some
important conclusions about MLPs that are otherwise diﬃcult to assume. There is a great
deal of misconception about neural networks. They are sometimes unjustiﬁably gloriﬁed
or being blindly denounced on the other hand. As always, the truth is somewhere in the
middle.

Examples of use on a physical system include classiﬁcation, prediction of future states
of the system, ﬁnding if and how the system inﬂuences other systems or just discovering
hidden patterns in the data. According to my experience, these are the advantages of
MLPs:

• Versatility - MLP can be applied to many ﬁelds. From physics to econometrics or
biology. And best thing about it is that you don’t have to be an expert in those ﬁelds
to produce results as good as experts. You just have to create good enough training
sample and encode the inputs and outputs correctly.

• Multidimensionality - You made an observation of some phenomenon and need
to make a prediction of future states of the system or just how measured physical
quantities depend on themselves. When you have two dimensional problem (y some-
how depends on x), it’s easy. You make a chart, do linear regression, polynomial
regression, etc. When it’s three dimensional, it’s still ﬁne. When it’s four or ﬁve di-
mensional, it’s hard. When it’s ﬁfty dimensional, it’s nearly impossible. Not so with
neural networks. Dimensionality of the input data is just a matter of scalability in
case of MLP.

• Scalability - Neural nets are easily scalable. If you are trying to model physical
system that can be described by one or two diﬀerential equations, it’s quite easy. You
create a simple model and you’re done. But when the system gets more complicated,
your model is useless. You then have to create a completely new and much more
complicated model. With neural nets, you only adjust few parameters, add some
neurons to the hidden layer and that’s it. It takes you few minutes to scale the
network, so it’s able to model behaviour of exponentially more complicated system.

• Chaotic data - There are many papers such as [30] or [31] that demonstrate very
good ability of neural networks to work even on very chaotic data. Systems manifest-
ing deterministic chaos are nearly impossible to model with classical methods. That’s
the reason why ANNs are often used for weather numerical prediction models.

19

• Human-like - Probably the greatest feature of neural nets is their ability to model
human-like behaviour. That’s because human emotions are often hard to predict. In
this kind of applications can ANNs prove why they really earn the title of artiﬁcial
intelligence. To demostrate this, I suggest you to try the simple brain.js demo
developed by Heather Arthur [14]. It’s a web based app you can use to train a simple
JavaScript neural network to recognize your preferred color contrast. After sucessful
training you can test it on various colors and compare the results of the network to
YIQ formula [5]. General feeling is that the network learns your habits really well.
However results will diﬀer from person to person. By testing it on myself I found out
that in colors where network’s result is conﬂicting with YIQ formula, I usually prefer
the network’s version.

However MLP neural networks have also signiﬁcant disadvantages such as:

• It’s a black box - You can sucessfully model almost any physical system with ANN.
However the ANN source the inputs and produce outputs (no matter how sucess-
fuly). You don’t learn anything about the mechanism or processes inside the physical
system. This is undoubtedly a weakness. But this is how we expect an artiﬁcial in-
telligence to work. There are even some problems, often involving classiﬁcation, that
can’t be described by an actual mechanism or set of equations.

• Not suitable for everything - MLP is a universal tool. However that doesn’t mean
it’s always the best option. There are much simpler tools with better performance
for many kind of tasks. It’s wise to deploy MLP only on problems where it would be
beneﬁcial.

• Need for optimization - Before training the ANN, we must specify some param-
eters of the network. There are not too many of them, usually much less than in
classical models. But in classical models, we usually guess most of these parameters
from nature of the problem. In MLP, these parameters are unrelated to the problem.
The talk is about net structure, number of neurons in hidden layers, type of activa-
tion functions, steepness of the activation functions, number of training epochs, etc. I
tested genetic algorithms to estimate these parameters and found out they work, but
often drastically increase the training time. I personally didn’t use them regularly
and prefer the manual estimation. It’s quick and with some experience yields the
same results as GA. I also tried Cascade training algorithm in FANN library [6], but
found it not very useful. The algorithm basicaly tries to ﬁnd the optimal architecture
of the network. Training starts with empty net and neurons are continuously added
one by one into the layers until the best conﬁguration is found. The algorithm often
diverged and and created absurdly huge nets with poor performance.

• Bias and variance - There is no best solution to this problem. The line between
good generalization and overﬁtting is very thin. Cross validation on out-of-sample

20

data is necessary. Bias to diﬀerent datasets is important. Careful preparation and
normalization of training datasets is a priority and have huge impact on quality of
the result.

My work with feedforward neural networks, namely multilayer perceptrons, was aimed
on predicting behaviour of various systems. Application on physical systems was perfect.
I got precise results and training wasn’t complicated. But things get interesting when I
tried something more complicated. I wanted to predicit chaotic econometric data. And the
results were no longer so good. I was able to get successful prediction only in some periods
when network was lucky and identiﬁed a pattern in recent data. And even in that case,
it identiﬁed correctly only the direction of the change, estimating value of the change was
impossible.

In contradiction to widespread beliefs, performance was best when training set was
very small, comprised from recent data only. Raising number on neurons in the hidden
layer proved to have little eﬀect on success rate. It only added to overﬁtting. I also found
that essential parameters of the network (number of neurons in hidden layer, steepness of
activation functions and number of training epochs) strongly depends on each other. For
instance increasing number of hidden neurons had the same eﬀect as rising the amount of
training epochs.

Multilayer perceptrons are powerful, versatile and revolutional. They are very capable
regressors. But nothing more. I miss the ability to abstract and cope with the problem
more independently, without the requirement for massive optimization.

3.5 Shallow and deep architectures

The model of multilayer perceptron I was reviewing in previous section was a shallow
architecture. These are architectures with zero or only one hidden layer. Using MLP with
only one hidden layer is a common practice. That’s because in 1989, George Cybenko
proved that a three-layer neural network (MLP with one hidden layer) with a continuous
sigmoidal activation functions can approximate all continuous, real-valued functions with
any desired precision [20].

But ever since the birth of artiﬁcial neural networks in the 40’s, there has been an
interest in deep architectures - those with multiple hidden layers. Why’s that? There are
several reasons:

• Shallow architectures oﬀer little or no invariance to shifting, scaling, and other forms

of distortion.

• Topology of the input data is completely ignored in shallow networks, yielding similar

training results for all permutations of the input vector.

21

• Visual system in human brain is deep. It has about ten to twenty layers of neurons
from the retina to the inferotemporal cortex (where object categories are encoded).

3.6 Visual cortex

Figure 3.3: Model of the human visual processing system [11].

With advancements in human neuroscience, it was discovered that human visual system
is in fact a deep architecture. This had huge impact on the ﬁeld of computer vision, where
feedforward networks such as multilayer perceptron belong. It questioned the wide belief
that shallow architectures are the best. It was shown that human ability to abstract requiers
human visual cortex to have a deep structure.

Figure 3.4: Lateral geniculate nucleus (LGN) in the human brain [7].

The visual input goes through layers of retina and retinal ganglion cells to lateral
geniculate nucleus (LGN) as can be seen in the ﬁgure 3.3. LGN consists of six layers of
neurons. Each layer receives information from the retinal hemi-ﬁeld of one eye. All the
neurons in LGN regions form a topographical map of the visual ﬁeld from its projection
onto the retina [15].

22

Figure 3.5: Primary visual cortex (V1) [15].

From the LGN, the signal goes into the primary visual cortex (V1 in ﬁgure 3.3). The
primary visual cortex is an important and most studied area of the human visual system.
It’s a six layer, deep neural network with diﬀerent types of vertically and laterally connected
neurons. The ﬁgure 3.5 shows its structure. Neurons in V1 responds primarily to basic
properties of an object, such as edges, length, width or motion [15].

Figure 3.6: Primary visual cortex - V1 (blue area), dorsal pathway (green) and ventral
pathway (purple) [8].

V1 is an important crossroads for the visual signal. From there, the visual signal is
transmitted to two streams — dorsal pathway and ventral pathway. Illustration of this
process is in ﬁgure 3.6 and 3.7. Dorsal pathway begins in V1 and continues through V2
and V5/MT into the posterior parietal cortex. Is is responsible for "where" characteristics
and is associated with motion, representation of object’s location and control of the eyes
and arms. Ventral pathway also begins in V1. The signal goes through V2, visual area V4
into the inferior temporal cortex. It’s called a "what pathway" and is associated with form

23

recognition and object representation [8]. Obviously the ventral stream will be my point
of interest.

Figure 3.7: Human visual system [16].

Ventral pathway stands as a model for the computer vision. I has deep architecture
and hierarchically abstracts features of an object. Image, which is at the retina level repre-
sented as a matrix of pixels, is run through the ventral stream. In V1, edges of the object
are detected, V2 detects primitive shapes and in V4 higher level features and colors are
abstracted. Inferior temporal cortex produces "what" the object is, i.e. assigns a label and
category to the object.

3.7 How to build a deep network

Theoretically you can create a deep neural network by adding hidden layers to the mul-
tilayer perceptron and making the layers two dimensional (we need to process images).
However there are few problems associated with this approach. In deep MLP, early hidden
layers don’t get trained well. The reason is the error attenuates as it propagates to earlier
layers. Gradients are much smaller at deeper levels and training slows signiﬁcantly. This is
called a difussion of gradient.

The problem is exacerbated by the fact that in fully connected neural network, number
of connections rises with (h − 1)x4, where x is the pixel dimension of the input image
(assuming it’s a square) and h is the number of network’s layers (excluding the output

24

layer and assuming hidden layers and input layer have the same dimensions). For a 30x30
image and network with three hidden layers, we get 3240000 connections. In combination
with the diﬀusion of gradient, the network can’t be trained eﬀectively in a ﬁnite time.
When training the deep MLP, top layers usually learns any task pretty well and thus the
error to earlier layers drops quickly as the top layers mostly solve the task. Lower layers
never get the opportunity to use their capacity to improve results, they just do a random
feature map.

This obstacle led to a crisis in artiﬁcial intelligence in the 90’s and decline of artiﬁcial
neural networks and rise of much simpler architectures such as support vector machines
(SVM) and linear classiﬁers. AI research was facing a lot of pessimism. The common
practice in computer vision was preprocessing with manually handcrafted feature extractor
and learning with simple trainable classiﬁer. Shallow kernel architectures such as SVM was
a preferred machine learning architecture. This was very time-consuming and had to be
repeated for diﬀerent tasks.

Luckily during the years, major breakthroughs happened in deep learning. It turned

out there are several ways how to build and run deep architectures:

• Very recent work has shown signiﬁcant accuracy improvements by "patiently" train-
ing deeper multilayer perceptrons with backpropagation using GPGPU (massive gen-
eral purpose parallel computations on graphic cards).

• Convolutional neural nets developed by Yann LeCun is the ﬁrst truly deep archi-
tecture. It’s the architecture that closely resembles human visual system. It can be
trained supervised using the gradient descent method.

• Geoﬀrey Hinton developed deep belief networks which use greedy layer-wise training.
Each layer is a restricted boltzmann machine and is trained unsupervised. Final
outputs are fed to a supervised model.

• Team around Yann LeCun found how to train unsupervised, sparse convolutional
neural nets. This reduced training time and yielded better performance on tasks
where not large enough training set is available.

These deep architectures greatly outperform SVM and other shallow kernel topologies.
They also require little human input and preprocessing in comparison to SVM. Deep learn-
ing is attracting a lot of attention and has been implemented in commercial applications.

25

26

Chapter 4

Convolutional neural networks

4.1 Description

The following description is based on papers [18] by David Bouchain and [22], [26], [29] by
Yann LeCun and others.

Concept of convolutional neural networks (ConvNets) was introduced in 1995 by Yann
LeCun and Yoshua Bengio [27]. Their work was neurobiologically motivated by the ﬁndings
of locally sensitive and orientation-selective nerve cells in the visual cortex of a cat [18].
Cats have a high performance visual system close to that of primates, making it a very
coveted subject for researches to reveal the functional aspects of this complex system [15].

Figure 4.1: Example of a two stage convolutional neural network [29].

Crucial element of ConvNets is restricting the neural weights of one layer to a local
receptive ﬁeld in the previous layer. This receptive ﬁeld is usually a two dimensional kernel.
Each element in the kernel window has a weight independent of that of another element.

27

The same kernel (with the same set of weights) is moved over neurons from the prior layer.
Therefore the weights are shared for each feature map, they diﬀer only between diﬀerent
feature maps in each feature map array (layer). Making several feature maps in each layer
enables the network to extract multiple features for each location.

Figure 4.2: Structure of one stage of the ConvNet [29].

Convolutional neural network is composed of several feature map arrays clustered into
one, two or three stages, followed by a classiﬁer at the end. Each stage serves as a feature
extractor. The clasiﬁer is usually a fully connected neural network which computes the
product of the feature vector. Each stage is made by ﬁlter bank layer, nonlinearity layer
and feature pooling layer.

• Filter bank layer - Input is a set of n1 two dimensional feature maps of size n2 ×n3.
Each feature map is denoted xi and each neuron xijk. Output is a diﬀerent set of
feature maps yj. A two dimensional trainable kernel kij connects feature map xi and
yj. The ﬁlter computes

yj = bj +

(cid:88)

i

kij ∗ xi

(4.1)

where bj is a trainable bias parameter and ∗ operator performs two dimesional discrete
convolution. Number of feature maps at the output is determined by how many
diﬀerent convolutional kernels we use. Each kernel can extract diﬀerent features from
the image (edges, etc.).

• Nonlinearity layer - This is usually a pointwise tanh() sigmoid function

or more preferred rectiﬁed sigmoid

yj = tanh(xi)

yj = |gi · tanh(xi)|

(4.2)

(4.3)

where gi is a trainable gain parameter. This is followed by a subtractive and divisive
local normalization which enforces local competition between neighboring features

28

in a feature map and between features at the same spatial location. The subtractive
normalization is made by

vijk = xijk −

(cid:88)

ipq

wpq · xi,j+p,k+q

(4.4)

where wpq is a normalized truncated Gaussian weighting window. The divisive nor-
malization is then

where

yijk =

vijk
max(mean(σjk), σjk)

σjk =

(cid:115)(cid:88)

ipq

wpq · v2

i,j+p,k+q

(4.5)

(4.6)

• Feature pooling layer - The purpose of this layer is to reduce the spatial resolution
of preceding feature maps. This is very beneﬁcial, because it eliminates some position
information about features, therefore builds some level of distortion invariance in the
representation. The transformation may be simply computation of an average or
maximal value over the receptive ﬁeld. Most common is the subs layer doing simple
average over the kernel, or more sophisticated l2pool. That’s a variant of Lp-Pooling,
which is a biologically inspired pooling process modelled on complex cells [34]. Its
operation is described by equation

(cid:88) (cid:88)

O = (

I(i, j)P × G(i, j))1/P

(4.7)

where G is a Gaussian kernel, I is the input feature map and O is the output feature
map. It can be imagined as giving an increased weight to stronger features and
suppressing weaker features. Two special cases of Lp-Pooling are notable. P = 1
corresponds to a simple Gaussian averaging, whereas P = ∞ corresponds to max-
pooling (i.e only the strongest signal is activated) [34].

Some ConvNets use stride larger than one to further reduce the spatial resolution.
Pooling is followed by tanh() nonlinearity in most networks, but in some recent
models don’t.

All layers are trained at the same time using supervised gradient based descent de-
scribed in the previous chapter. In some recent models, a method for unsupervised training
has been proposed. It’s called a Predictive Sparse Decomposition (PSD) [25].

29

4.2 Advantages

• ConvNet implicitly extracts relevant features from the image. It’s naturally best
architecture for computer vision. That’s because it’s working on the same priciple
as the human visual system. It hierarchically abstracts unique features of the input
as the visual cortex. Convolution and subsampling is a native process in the human
visual system, which can be proved by various visual tricks [11].

• By being a deep architecture and by reducing spatial resolution of the feature map,
ConvNets achieve a great degree of shift and distortion invariance. This is something
SVM and other shallow architectures completely lack.

• Convolutional layers have signiﬁcantly less connections thanks to shared weights con-
cept. Subsampling layers have usually only one trainable weight and one trainable
bias. Overall this massively reduced the number of parameters in comparison to mul-
tilayer perceptrons. This makes ConvNets exponentially easier to train than MLPs.

• Unlike SVM and similar shallow kernel template matchers, ConvNets feature ex-
tractors are trained automatically and don’t have to be manually handcrafted. This
greatly saves human time and more closely resembles true artiﬁcial intelligence.

• Unlike multilayer perceptrons, where performance was sensitive to networks param-
eters, ConvNets perform universally good with various topologies and aren’t so sen-
sitive to pretraining normalization.

4.3 Achievements

ConvNet was the ﬁrst truly deep architecture. Therefore it has long history of sucessful
application in OCR and handwriting recognition. However most excitement came from
more recent results.

• AT&T developed a bank check reading system using ConvNet in the 90’s. It was
deployed on check reading ATM machines of large US and European banks. By the
end of 90’s it was reading over 10% of all checks in the US.

• Microsoft deployed ConvNets in a number of handwriting recognition and OCR sys-

tems for Arabic [13] and Chinese [19] characters.

• Google used a ConvNet in its Google StreetView service. It was a component of

algorithms that search and blur human faces and license plates [21].

• Video surveillance system based on ConvNets has been deployed in several airports

in the US.

30

• ConvNets trained unsupervised with sparsifying non-linearity achieved record results
in handwritten digits recognition using MNIST dataset with error rate as low as
0.39%, outperforming all methods previously used [33].

• ConvNet is the most suitable architecture for object recognition. It achieved the best
score on NORB dataset (96x96 stereo images) with error only 5.8%. Second best
architecture (Pairwise SVM) achieved 11.3% [26].

• It was sucessfully applied to house numbers digit classiﬁcation. This was similar
task as the MNIST dataset - the task was numbers recognition on 32x32 images.
However in this case it was an order of magnitude bigger dataset (600000 labeled
digits), contained color information and various natural backgrounds. The best used
architecture (multi-stage ConvNet trained supervised) obtained accuracy of 95.1%,
very close to the human performance of 98% [34].

• Maybe the most notable application was the traﬃc signs and pedestrian recognition.
Team around Pierre Sermanet and Yann LeCun trained an unsupervised ConvNet
using Predictive Sparse Decomposition. They tried using various transformations of
the dataset and the best obtained accuracy was an outstanding 99.17% (others were
98.97% and 98.89%). However the most important fact about this experiment was
that for the ﬁrst time, the AI was able to achieve better performace than human
(98.81% in this case) in a typical human-like problem [36].

31

32

Chapter 5

Practical Application

5.1 Preparing the dataset

One thing I’ve learned during my previous work with multilayer perceptrons is that quality
of the training set is critical. It’s the most inﬂuential factor in training of neural networks.
Not the topology of the network, type of learning algorithm or anything else. Even the most
advanced and ﬁne tuned machine learning algorithms are useless if you feed them with
improperly selected data that don’t describe the underlying pattern accurately. Second
most important factor in case of multilayer perceptron was the preprocessing and data
representation.

Convolutional neural networks are not so dependent on preprocessing. They can handle
data with little or no preprocessing without problems. However, preprocessed or not, the
data still have to represent the underlying problem as accurately as possible. Therefore it’s
no surprise I spent a great deal of time designing the selection algorithm. I downloaded
the BOSS survey catalog from [2]. It’s a 1.6GB ﬁts ﬁle containing a giant list of 1.5 million
spectra obtained by BOSS spectrograph.

Class
Total
Galaxies
Quasars
Stars
Sky
Unknown

Total Unique
1 391 792
859 322
166 300
144 968
138 491
89 003

1 507 954
927 844
182 009
159 327
144 503
101 550

Table 5.1: Amount of spectra in the BOSS survey [2].

The amount of spectra taken in BOSS survey is substantial. I can’t utilize them all.

33

BOSS archive with reduced spectra takes 6.59TB of space. That’s a magnitude over my ca-
pabilities, not mentioning the computational time required for processing all these spectra.
I decided to create smaller, but still large enough dataset for my analysis.

Dataset
Training
Validation
Testing

Size
31 775
3 103
60 329

Table 5.2: Size (number of spectra) of my selected datasets.

Each spectrum is described by three numbers that, when combined together, create a
unique identiﬁer of the spectrum. It’s a number of the spectrographic plate, modiﬁed Julian
date (MJD) when spectrum was obtained, and a number of the ﬁber on the spectrographic
plate. Each spectrum has its own record in the BOSS catalog. The record includes many
ﬂags and parameters describing instrumental response, quality of the spectrum (ZWARNING
ﬂag), and products of the spectroscopic pipeline. The parameters we are interested in are:

Class Total
PLATE number of the spectrographic plate

MJD modiﬁed Julian date when the spectrum was taken

FIBERID number of the optic ﬁber plugged into the plate

SPECBOSS

ZWARNING

CLASS

Z

indicates whether spectrum is primary (the best observation of
that location)
a ﬂag indicating whether the spectrum had any problems in the
spectroscopic pipeline
object type classiﬁcation (galaxy, quasar or star), product of the
spectroscopic pipeline
estimated redshift, product of the spectroscopic pipeline

Table 5.3: Parameters extracted from the BOSS catalog for each spectrum.

I used the catalog to construct as good training set as possible. It had to equally
represent every object type. Therefore the ratio of galaxies, quasars and stars in the training
set is roughly 1:1:1. This was an easy task. Ensuring the equal distribution of objects by
redshift among individual object classes proved to be more challenging. This is important
mainly for quasars that span over wide interval of redshifts. Narrowing this distribution is
desirable from two reasons. First, spectra are distorted becauses of doppler eﬀect:

z =

λobsv
λemit

− 1

34

(5.1)

This means spectral lines or any other characteristic elements for the classiﬁcation are
shifted in high z spectra. Second problem is the selection bias. High z quasars are very
distant objects. In fact these objects can be as far as the edge of the observable universe.
Because they are so far, it is right to assume the objects we observe are the ones that are
most bright. If they would be faint, we won’t see them at such a great distance. This was
scientiﬁcally proved. Brightness of the quasar depends on the inclination of its jet towards
our line of sight. The most luminous quasars are called blazars and their jets are aimed
directly towards us, ampliﬁed by the relativistic beaming eﬀect [9]. Spectra of blazars
are dominated by nonthermal eﬀects such as synchrotron emission and inverse compton
scattering at higher energies. So would the distribution of quasars in the training set be
skewed towards high redshift, the classiﬁcation algorithm would consider a typical quasar
spectra to look similar to that of high z blazar. Which would be incorrect assumption and
classiﬁcation algorithm can then perform poorly on low z quasar spectra.

Figure 5.1: Distribution of all the good (SPECBOSS=1 and ZWARNING=0) quasar spectra in
the BOSS catalog.

As is shown in ﬁgure 5.1, the distribution of quasars by redshift in the catalog is highly
nonhomogenous. There is tens of thousands of objects in intervals from two to three and
zero to one, while only few hundreds of objects in interval from ﬁve to seven. I developed a
selection algorithm that divides the redshift range into equidistant intervals and required
amount of samples for each inteval is computed. The samples are then picked from each

35

 0 5000 10000 15000 20000 25000 30000 35000 40000 0 1 2 3 4 5 6 7 8 0 5000 10000 15000 20000 25000 30000 35000 40000number of objectsredshiftHistogram of train_qsointerval using modiﬁed pseudorandom generator. After this procedure, the distribution of
quasars by redshift is in the ﬁgure 5.2, which are in fact objects that will be included in
the training set.

Figure 5.2: Distribution of quasar spectra in the training set.

The distribution in ﬁgure 5.2 for objects with redshift over four is not constant because
there is not enough objects with so high value of redshift in the BOSS catalog. The dif-
ference can be statistically highlighted by comparing cumulative distribution functions for
both sets. This was done in ﬁgure 5.3. This proves the selection alorithm is very eﬀective.
Unlike quasars, galactic (non-quasistellar) and stellar objects are distributed over much
narrower interval of redshifts. Applying the selection algorithm on them doesn’t bring such
an advantage as in quasar’s case, but even though it’s still beneﬁcial. Corresponding charts
for non-quasistellar galaxies are in ﬁgures 5.4, 5.5, 5.6 and for stars in ﬁgures 5.7, 5.8, 5.9.
The selection algorithm is applied also for creation of validation and testing sets, al-

though quality of these sets is not so important.

36

 0 50 100 150 200 250 300 350 400 450 500 0 1 2 3 4 5 6 7 8 0 50 100 150 200 250 300 350 400 450 500number of objectsredshiftHistogram of train_qsoFigure 5.3: Cumulative distribution functions (z is redshift) for all good quasars in the
BOSS catalog (red) and for quasars selected into the training set (blue).

Figure 5.4: Distribution of all the good (SPECBOSS=1 and ZWARNING=0) galactic (non-
quasistellar) spectra in the BOSS catalog.

37

 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 0 1 2 3 4 5 6 7 8 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1F(z)zCumulative distribution functions of qso redshiftsoriginal CDFafter normalization 0 10000 20000 30000 40000 50000 60000 70000-0.2 0 0.2 0.4 0.6 0.8 1 1.2 0 10000 20000 30000 40000 50000 60000 70000number of objectsredshiftHistogram of train_galaxyFigure 5.5: Distribution of galactic (non-quasistellar) spectra in the training set.

Figure 5.6: Cumulative distribution functions (z is redshift) for all good galaxies (non-qso)
in the BOSS catalog (red) and for galaxies (non-qso) selected into the training set (blue).

38

 0 50 100 150 200 250-0.2 0 0.2 0.4 0.6 0.8 1 1.2 0 50 100 150 200 250number of objectsredshiftHistogram of train_galaxy 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1-0.2 0 0.2 0.4 0.6 0.8 1 1.2 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1F(z)zCumulative distribution functions of galaxy redshiftsoriginal CDFafter normalizationFigure 5.7: Distribution of all the good (SPECBOSS=1 and ZWARNING=0) stellar spectra in
the BOSS catalog.

Figure 5.8: Distribution of stellar spectra in the training set.

39

 0 5000 10000 15000 20000 25000 30000 35000 40000 45000-0.005-0.004-0.003-0.002-0.001 0 0.001 0.002 0.003 0.004 0.005 0 5000 10000 15000 20000 25000 30000 35000 40000 45000number of objectsredshiftHistogram of train_star 0 100 200 300 400 500 600 700 800 900-0.005-0.004-0.003-0.002-0.001 0 0.001 0.002 0.003 0.004 0.005 0 100 200 300 400 500 600 700 800 900number of objectsredshiftHistogram of train_starFigure 5.9: Cumulative distribution functions (z is redshift) for all good stars in the BOSS
catalog (red) and for stars selected into the training set (blue).

40

 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1-0.005-0.004-0.003-0.002-0.001 0 0.001 0.002 0.003 0.004 0.005 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1F(z)zCumulative distribution functions of star redshiftsoriginal CDFafter normalization5.2 Preprocessing

After creation of training, validation and testing datasets, the spectra corresponding to
records in datasets were downloaded from the Science Archive Server [12]. Each downloaded
spectrum is stored as a table in a separate ﬁts ﬁle. This means I got 95207 ﬁts ﬁles to deal
with. Although there are ways how to deal directly with the ﬁts ﬁles (using routines from
the CFITSIO library), I decided to extract required data into simple text ﬁles, so I can work
then with the spectra using standard FILE structure from stdio header ﬁle. Extraction of
the data from ﬁts ﬁles was performed using heatools package from HEAsoft software pack
[10]. I have positive experience with heatools from my bachelor thesis, where I successfully
used it for handling huge multi-hdu ﬁts ﬁles containing high energy astrophysical data [24].

Figure 5.10: Reduced spectrum of quasar 5374-55947-0860 (PLATE-MJD-FIBERID) as a
one dimensional vector of intensities.

Figure 5.11: The same spectrum of quasar 5374-55947-0860 normalized and converted into
a two dimensional 60x60px image.

I’ve written two preprocessing routines. The ﬁrst is responsible for extraction of spectra,
reduction, binning and ﬁltering. The spectra are stored as a one dimensional vector of ﬂux

41

at corresponding wavelenghts. Flux is calibrated in 10−17erg/s/cm2/Ang and wavelength
in log10 Ang. I decided to arbitrarily reduce the spectra into interval of [3.60000; 3.96000]
which corresponds to wavelengths from 3981 Å to 9120 Å. This is beneﬁcial because edges
of most spectra are usually very noisy and doesn’t posess any meaningful information for
the classiﬁcation. I incorporated a binning algorithm, so the spectra can ﬁt into a square
matrix. Default specral vector has 3601 elements after the reduction and most common
binning I used was a 60x60 matrix (eﬀectively no binning). Filtering subroutine searches
for spectra that are somehow impaired and removes them from the dataset.

The spectra are one dimensional vectors, while input into the ConvNet has to be a two
dimensional matrix (image). Second routine transforms the spectra from one dimensional
vector into two dimensional matrix, normalizes elements of this matrix and ﬁnally trans-
forms the matrix into an image. I had several dimensionality transformation algorithms in
mind, but decided to simply add elements line after line into the matrix. I was inspired by
the way CRT monitors rasterize image on the shadow mask. The output matrices of inten-
sities were normalized using linear normalization and 8-bit grayscale encoding. Finally the
product was then converted into an actual PNG image using imagemagick scripting tools,
which is the fastest and most reliable approach I have found. The process is illustrated in
ﬁgures 5.10 and 5.11.

5.3 Designing the ConvNet

I’ve used EBLearn library written by Pierre Sermanet and Yann LeCun [35] in my scripts
and programs for training and testing convolutional neural networks. I deployed two con-
volutional neural network architectures, both of them designed by Yann LeCun.

Figure 5.12: LeNet-5 topology used in handwriting recognition on MNIST dataset [28].

The ﬁrst is LeNet-5 displayed in ﬁgure 5.12. It’s the architecture that was deployed on
MNIST dataset, which are 32x32 images of handwritten numbers. I intend to use 60x60
images as an input. The network has to be redesigned in order to incorporate bigger input. I
have a lot of experience with designing multilayer perceptron feedforward neural nets. MLP

42

is a two dimensional topology, where it’s quite easy to make sure layers and connections
match together. ConvNet, on the other side, is a four dimensional body with some layers
not fully connected together, where it’s obviously much more complicated to design the
topology. It took me some time to fully understand how to compute it. In case the input
image is smaller than the network is designed for, padding the image to required size is
the best practice. When the image is larger, the only way is adjusting correctly size of
kernels, or choosing completely diﬀerent architecture. In case of 60x60 input image and
LeNet-5 architecture, I found out the optimal way is enlarging the last convolution kernel
between layers S4 and C5 from 5x5 to 13x13 and keeping kernels in previous layers the
same. Enlarging all kernels a little (by which way the architecture can also work) doesn’t
yield as good performance as massively adjusting only the last convolutional kernel.

Figure 5.13: LeNet-7 topology used in stereo image recognition on NORB dataset [22].

Another way how to classify 60x60 images is using completely diﬀerent architecture,
such as LeNet-7. The problem is that documentation for EBLearn library is quite poor
and there is no conﬁg ﬁle for this architecture anywhere. Therefore I was forced to try to
build such an architecture myself. Luckily I was successful. Only diﬀerence between my
architecture and the LeNet-7 displayed in ﬁgure 5.13 is that mine is using only one image
as an input, convolution kernel between layers S2 and C3 is 5x5 instead of 6x6, and sub-
sampling kernel between layers C3 and S4 is 2x2 instead of 3x3. Obviously the dimensions
of all feature maps are diﬀerent than in ﬁgure 5.13. But besides that the topology is sim-
ilar, with the same number of feature maps in each layer, the same connection matrices
and therefore same abstraction capabilities. The expectation was, that this more massive
topology would perform better on my 60x60 images, than more lightweight LeNet-5.

43

44

Chapter 6

Results

6.1 Training

Before the training could happen, I had to transform training and validation datasets into
MAT ﬁles required by EBLearn train routine. This was done using dscompile routine.
Crucial part of training is a creation of conﬁguration ﬁle. It speciﬁes how the architecture
has to be built, all required parameters and paths to EBLearn routines and datasets. It
took me a lot of time until I fully undestood how to make it, because the documentation
provided with EBLearn library is insuﬃcient.

First choice for training was modiﬁed LeNet-5 architecture using average subsampling
(subs) in feature pooling layers, subtractive normalization (wstd ) in normalization layers
and tanh function in nonlinearity layers. ConvNet was trained for 20 epochs (iterations)
on training dataset composed of 31775 spectra (galaxies, quasars and stars in ratio roughly
1:1:1) and crosschecked on validation dataset composed of 3103 spectra (galaxies, quasars
and stars in ratio roughly 1:1:1), as speciﬁed in table 5.2.

The result is in ﬁgure 6.1. Considering it’s the ﬁrst shot, results are very good. Success
rate on validation set gets over 96% during training. The problem is that in the 17th epoch
the rate collapses. This is not a coincidence. When the training is extended for 100 epochs
it became obvious this collapse is a periodical event repeating roughly every 25 epochs.
This is displayed in ﬁgure 6.2.

Based on my experience with multilayer perceptrons, I suspected this instability to be
somehow related to the learning rate (eta). Therefore I incorporated a learning rate decay
into the conﬁguration ﬁle. My theory proved to be correct as can be seen in the ﬁgure 6.3.
Training was stable and success rate for validation dataset converged at 96.49%.

I also tried l2pool alogithm instead of subs in feature pooling layers. The convergence
seems to be slower (ﬁgure 6.4), but success rate at the end (96.16%) was close to that with
subs layers.

I was eager to ﬁnd out how my modiﬁed LeNet-7 architecture would perform. I was

45

Figure 6.1: Training LeNet-5 architecture (subs pooling) for 20 epochs.

Figure 6.2: Training LeNet-5 architecture (subs pooling) for 100 epochs.

46

 90 91 92 93 94 95 96 97 98 99 100 0 2 4 6 8 10 12 14 16 18 20 90 91 92 93 94 95 96 97 98 99 100correctly classified [%]training epochRatio of correctly classified inputs during trainingtraining setvalidation set 30 40 50 60 70 80 90 100 0 10 20 30 40 50 60 70 80 90 100 30 40 50 60 70 80 90 100correctly classified [%]training epochRatio of correctly classified inputs during trainingtraining setvalidation setFigure 6.3: Training LeNet-5 architecture (subs pooling, eta decay) for 100 epochs.

Figure 6.4: Training LeNet-5 architecture (l2pool pooling, eta decay) for 100 epochs.

47

 93 94 95 96 97 98 99 100 0 10 20 30 40 50 60 70 80 90 100 93 94 95 96 97 98 99 100correctly classified [%]training epochRatio of correctly classified inputs during trainingtraining setvalidation set 91 92 93 94 95 96 97 98 99 100 0 10 20 30 40 50 60 70 80 90 100 91 92 93 94 95 96 97 98 99 100correctly classified [%]training epochRatio of correctly classified inputs during trainingtraining setvalidation setexpecting it to be better than LeNet-5, because it has more feature maps in each layer and
was originally designed for bigger images (96x96) than LeNet-5 (32x32). Surprisingly, this
assumption proved to be false as can be seen in ﬁgure 6.5. The convergence is slower and
success rate at the end of training is "only" 94.91%. Unfortunatelly I couldn’t ﬁgure out
how to train LeNet-7 architecture with l2pool pooling due to unspeciﬁed error, so only the
subs variant is included.

Figure 6.5: Training LeNet-7 architecture (subs pooling, eta decay) for 100 epochs.

One thing I want to clarify is why it’s better to use unbinned spectra. Preprocessing
program I developed transformed one dimensional reduced spectral vector with 3601 ele-
ments into two dimensional 60x60 matrix (which was then transformed into an image). To
prove my thesis I created alternative datasets, composed of the same spectra, but binned
into 28x28 matrix (image). I modiﬁed the conﬁguration ﬁle and trained the best performing
LeNet-5 architecture with subs pooling and eta decay using these binned spectra datasets.
Only thing I had to change in the architecture was size of the last subsampling kernel,
which had to be shrinked to 5x5 (from 13x13 used for 60x60 images). Training process is
visualized in ﬁgure 6.6. Performance is signiﬁcantly worse than with the unbinned spectra.
This result has one important implication. Binning the spectra eﬀectively blurred spec-
tral lines, meaning the ConvNet was making classiﬁcation based on shape of the continuum,
rather than position/shape of spectral lines. Signiﬁcantly worse performace on binned spec-
tra implies the core features ConvNet is exploiting for classiﬁcation are the spectral lines

48

 65 70 75 80 85 90 95 100 0 10 20 30 40 50 60 70 80 90 100 65 70 75 80 85 90 95 100correctly classified [%]training epochRatio of correctly classified inputs during trainingtraining setvalidation setFigure 6.6: Training LeNet-5 architecture (subs pooling, eta decay) for 100 epochs using
binned spectra.

and not the continuum.

Overall performance for both architectures and diﬀerent training setups is summed up
in table 6.1. Two success rates for each variant are included, ﬁrst is the best achieved during
training, and second is the rate at the end of training.

Input Architecture Pooling Eta decay

60x60
60x60
60x60
60x60
28x28

LeNet-5
LeNet-5
LeNet-5
LeNet-7
LeNet-5

subs
subs
l2pool
subs
subs

no
yes
yes
yes
yes

Success rate
Best achieved At the end

96.39%
96.58%
96.62%
95.13%
83.95%

94.46%
96.49%
96.16%
94.91%
79.95%

Table 6.1: Summarized performace during training. Expressed success rates are always for
validation sets.

49

 78 80 82 84 86 88 90 92 94 96 98 100 0 10 20 30 40 50 60 70 80 90 100 78 80 82 84 86 88 90 92 94 96 98 100correctly classified [%]training epochRatio of correctly classified inputs during trainingtraining setvalidation set6.2 Classiﬁcation

Next phase was an application of trained networks on the large testing dataset, composed
of 60329 spectra. This proved to be one of the greatest obstacles I had to overcome. The
problem was that EBLearn has a detect routine for testing. However this routine isn’t
suitable for classiﬁcation of whole images. It’s best suited for detection of particular pat-
terns in images. For instance when somebody wants to detect human faces in a photo.
Some people in EBLearn support forum recommend to adjust bounding boxes in conﬁgu-
ration ﬁle. I tried this approach, but even then the detect routine results weren’t reliable.
Success rate was small and results were very unstable. Another fact I found out in the
support forum was that there was a classify routine in previous versions of the library,
but it was later discontinued and removed.

Only way how to overcome this, was to write my own classiﬁcation routine from scratch.
I found out how to modify the conﬁguration ﬁle in such a way, that it forced the train
routine to disable the training subroutine and perform only testing on validation set with a
saved network conﬁguration. I replaced the validation dataset with the testing dataset and
masked it to pretend to be the validation dataset. This way I could successfully perform
classiﬁcation on the testing dataset using already trained network conﬁguration.

But this approach had one considerable weakness. The train routine isn’t designated
for testing. When a testing dataset is fed into the routine, the output is only the total
sucess rate and sucess rates for each class (galaxy, qso, star). We won’t obtain actual
classiﬁcations, i.e. into which class each object from the dataset belongs. To overcome this
I invented a trick. I didn’t fed the testing dataset at once into the routine, but rather
wrote a feeding script that takes one object from the dataset after another. The script
run the object through the dscompile routine and then feed it into the train routine,
which is forced to work in classiﬁcation mode using the method described above. As I said
before, I get no additional information from the train routine, but success rate for each
class. Because input is only one object, the success rate for one class has to be 100% and
0% for two remaining classes. The object has to belong into one of those three classes
(galaxy, qso, star). Obviously the class that’s assigned the 100% success rate is the class
into which the object conclusively belongs to. The script also has to remember what’s the
object’s identiﬁcation (PLATE, MJD, FIBERID) during each run, because this information
is otherwise lost after the data goes through the dscompile routine. Object’s assigned class
is then compared with the true class speciﬁed in the BOSS catalog.

My feeding script together with various support scripts is combined into my own
classify.sh routine that fully substitutes the original EBLearn classify routine which
is no longer available. I ran the testing dataset through this routine for each architecture
trained in the previous section. I obtained two lists for each run. The ﬁrst with identiﬁers of
correctly classiﬁed objects and the second with incorrectly classiﬁed objects, together with
classes assigned by the ConvNet and those from the BOSS catalog. By comparing these
two lists I expressed ConvNet’s performance on the testing dataset for each architecture.

50

Input Architecture Pooling Eta decay

60x60
60x60
60x60
60x60
28x28

LeNet-5
LeNet-5
LeNet-5
LeNet-7
LeNet-5

subs
subs
l2pool
subs
subs

no
yes
yes
yes
yes

Success rate
Best achieved At the end

94.78%
94.72%
94.81%
93.95%
85.16%

93.54%
94.71%
94.89%
93.94%
81.77%

Table 6.2: Summarized performace of trained ConvNet architectures on the testing dataset
(60329 objects). Performace is expressed for two classiﬁcation runs. The ﬁrst using Con-
vNet’s best achieved conﬁguration (measured by success rate on the validation set) and
the second using network’s conﬁguration at the end of training.

Results show the LeNet-5 architecture using unbinned spectra is undoubtedly the best
conﬁguration. The learning rate decay carries indisputable advantage. Although it doesn’t
have signiﬁcant impact on the best achieved success rate, it removes collapses during train-
ing, speeds up training convergence and most importantly the quality of trained ConvNet
doesn’t depend so heavily on the training epoch. The variant with l2pool feature pooling
algorithm outperformed the subs pooling on both validation and testing datasets. And tak-
ing l2pool more sophisticated nature into account, it shall be considered superior against
subs pooling. However it would be better to examine this claim more deeply, because the
absolute diﬀerence in performance between the l2pool and subs conﬁgurations may be
within statistical errors.

6.3 Examples of classiﬁed spectra

I’m including some examples of various spectra. All of them are from the testing dataset
and were classiﬁed using the best performing LeNet-5 architecture, l2pool, eta decay and
unbinned spectra. Red lines in pictures are absorbtion spectral lines identiﬁed by the BOSS
spectroscopic pipeline (blue lines stand for emission).

51

6.3.1 Correctly classiﬁed objects

Figure 6.7: Example of a nice stellar spectrum, absorbtion lines are clearly visible.

Figure 6.8: Again a star, this time not so nice example. Still no problem for the ConvNet.

52

Figure 6.9: A galactic spectrum, clearly recognizable.

Figure 6.10: Uncommon galactic spectrum (probabaly starforming galaxy).

53

Figure 6.11: A nice example of quasar’s spectrum with broad emission lines.

Figure 6.12: Noisy and indistinct spectrum of a quasar.

54

6.3.2

Incorrectly classiﬁed objects

Figure 6.13: This object is labeled as a star in the BOSS catalog. ConvNet indentiﬁed it
as a galaxy.

Figure 6.14: This object is labeled as a quasar in the BOSS catalog. ConvNet indentiﬁed
it as a galaxy.

55

Figure 6.15: This object is labeled as a star in the BOSS catalog. ConvNet indentiﬁed it
as a quasar.

6.3.3 Wrong labels in the BOSS catalog

When I was browsing through the list of incorrectly classiﬁed objects in the testing dataset,
I discovered there are many cases when the object’s class in the BOSS catalog (ﬁts ﬁle)
doesn’t match the object’s class displayed on the web of the DR10 Science Archive Server. I
roughly estimate maybe half of all incorrectly classiﬁed spectra are aﬄicted by this. When
inspecting the spectra manually, I found out labels on the web are usually the correct ones.
That means the ConvNet classiﬁes the object correctly, but because the object’s class in
the catalog is incorrect, it’s evaluated as an incorrect classiﬁcation by the classify.sh
routine. This problem artiﬁcially lowers the success rate. The best performing LeNet-5
conﬁguration has nearly 95% success rate on the testing dataset. Error rate is therefore
5%. Assuming half of the incorrectly classiﬁed spectra have wrong label in the catalog,
real success rate of the ConvNet should be approximately 2.5% higher (error rate divided
by half).

But how’s it even possible that there are so many incorrectly labeled spectra in the
BOSS catalog? Well, maybe it isn’t such a huge problem after all. My estimate that half of
the incorrectly classiﬁed spectra have in fact wrong label, is only a rough estimate. I came to
this number by manually inspecting few tens of objects. But there are approximately 3000
objects identiﬁed by the classify.sh routine as incorrectly classiﬁed. It’s not humanly
possible to check all these spectra. So the real number of spectra with wrong label can be
lower. But even if it’s the one half, that would represent about 2.5% of all spectra, as was
showed in the previous paragarph. Which is not a big deal.

What can be the real cause of this problem. Obviously I suspected the NOQSO to have

56

something to do with it. So I tried using the NOQSO appendix for all galactic spectral param-
eters. But the performace of ConvNets during training and testing was then a magnitude
worse (tens of %). This was again veriﬁed by manually inspecting a subset of spectra.
My decision not to use the NOQSO appendix is therefore right. By taking all these facts
into account and considering the estimated amount of aﬄicted spectra to be only 2.5%, I
came to a conclusion that most probable cause of this problem is that the BOSS catalog
(specObj-BOSS-dr10.fits ﬁle) is just some obsolete version of the true BOSS catalog. It’s
hard to believe this, considering the ﬁlename itself has the latest data release in its name
("dr10"). But I can’t ﬁnd out any other logical explanation. Clariﬁcation from somebody
in the BOSS project would be very beneﬁcial.

57

58

Chapter 7

Conclusion

I have clariﬁed the the analogy between convolutional neural networks and the human
visual system. I’ve proved ConvNets are very powerful and eﬃcient tool for spectral classi-
ﬁcation. The success rate of nearly 95% on the testing dataset of more than 60000 spectra
speaks by itself. Most notable fact is the simplicity and elegance of the analysis. Only spec-
tra themselves served as an input. Preprocessing was straigtforward and the whole process
required little or no human interaction. This is in clear contrast with much more compli-
cated conventional approach of ﬁtting wide set of (often handcrafted) spectral templates.
Considerable achievement is the software suite I’ve developed [23]. By using the at-
tached manual, anybody can replicate my results or use the software for his own research
involving SDSS spectra. This thesis proved the artiﬁcial intelligence deserves its place in
modern astrophysics. ConvNets would be most beneﬁcial in new sky surveys, where in-
strumental response still carry a lot of uncertainity and conventional analysis is not very
accurate (my personal experience with Fermi’s LAT instrument three years ago). I can
imagine trained ConvNets to be included in Virtual Oservatory, where anybody can use
it to quickly determine spectral parameters of selected object even if it has never gone
through regular scientiﬁc analysis.

Several interesting facts were discovered during training and classiﬁcation. Contrary to
expectations prior to training, simpler LeNet-5 architecture achieved better sucess rates
than LeNet-7. Learning rate decay is beneﬁcial for training. Lp-Pooling achieved better per-
formance than subsampling in both training and testing, but its outperformance should be
more deeply researched before declaring deﬁnitive winner. Surprising contrast against mul-
tilayer perceptron was the uniform performance of ConvNets. They weren’t much sensitive
to initial parameters. I atribute this to their deep, hierarchical structure, which enables
them to abstract and brings great degree of shift and distortion invariance. Particularly im-
portant observation is the great outperformance of conﬁgurations using unbinned spectra.
This implies the spectral lines are much more important features for spectral classiﬁcation
than shape of the continuum.

59

If not for the wrong labels in the BOSS catalog, average success rates on testing dataset
may be as much as 2% higher. And by looking at the spectra that were truly incorrectly
classiﬁed, they are often very noisy and hard to classify even for an experienced astrophysi-
cist. It would be therefore very interesting to compare my results with real humans. It’s
probably a strong statement, but if the quality of the training dataset is further improved,
I think it won’t be unimaginable for my algorithm to achive better success rate in spec-
tral classiﬁcation than is humanly possible. It wouldn’t be the ﬁrst time the convolutional
neural network beat the humans [36].

60

Appendix A

Software manual

This is a manual for the software pack composed of bash scripts and C programs I developed
for the purpose of this thesis. Source codes are available at this link: [23]. This step by
step manual will guide you through datasets preparation, preprocessing of spectra and
ConNet’s construction/training/classiﬁcation. All the source codes are attached to this
thesis [23], C programs have to be compiled. Beside them, you will need the EBLearn
library [35], MAT File I/O Library, heatools package from the HEAsoft pack [10], GNU
coreutils, imagemagick utils, awk and gnuplot. The software was ran and debugged on
amd64 Debian distribution of GNU/Linux.

All bash scripts and compiled C programs should be placed into a three-folder structure:

Datasets preparation will happen in the catalog folder, spectral preprocessing in the

spectra folder and ConvNet handling in the class folder.

61

A.1 Datasets preparation

Let’s begin with the catalog folder. These ﬁles should be located in this folder by default:

The ﬁts ﬁle is the BOSS catalog. list.sh is the ﬁrst script we have to run. Make it

executable and don’t forget to initialize heatools ﬁrst:

1 shell$ heainit
2 shell$ ./list.sh

The script uses heatools to extract required spectral features and identiﬁers into text
ﬁles and reduces those lists, so they can be processed by other routines. Next step is
creation of training, validation and testing datasets. This is done by set.sh script and set
program compiled from set.c source code. Open the set.sh ﬁle ﬁrst and look for these
lines:

1

2

3

if [[ $file == *qso ]]; then size_of_the_set="16850"; fi
if [[ $file == *galaxy ]]; then size_of_the_set="11680"; fi
if [[ $file == *star ]]; then size_of_the_set="34200"; fi

This block of lines is with slight variations presented on three places in the code. First
for the training set, second for the validation set and third for the testing set. Size of
subsets for each dataset depends on the green parameters (variable size_of_the_set). By
adjusting these numbers, you inﬂuence what will be the size of each subset (galaxy, qso,
star) in each dataset (train, valid, test). But remember they doesn’t correspond to actual
size of each subset. Numbers set in the script by default correspond to datasets sizes and
distributions as speciﬁed in table 5.2. Then simply run the set.sh script (don’t forget to
make it executable):

62

1 shell$ ./set.sh

Datasets created by this script are saved into the sets folder. catalog folder then looks

like this:

The sets folder itself contains these text ﬁles:

First part of the ﬁle’s name speciﬁes the dataset (train, valid, test) and second part
the subset (object class). However the set folder isn’t the important one. It serves as

63

a source of data for generating histograms and cumulative distribution function charts
(using histogram.sh script or cdfplot.sh for comparative studies), but nothing more.
The most important folder is the spectra_sets. It contains lists of objects in each dataset.

For instance here is an excerpt of few lines from the train ﬁle with columns description:

1 #PLATE MJD FIBERID CLASS REDSHIFT
3.5992600000
2 5169
3.5994100000
3 3819
3.6002400000
4 4210
3.6002500000
5 5890
3.6008700000
6 5484
3.6011600000
7 3770
3.6015000000
8 5992
9 5459
3.6018900000
10 ...

524 1
279 1
184 1
504 1
109 1
160 1
454 1
554 1
... ... ...

56045
55540
55444
56037
56039
55234
56066
56035
...

The CLASS label is encoded (0=galaxy, 1=qso, 2=star). spectra_sets will be later used
for spectral preprocessing and classiﬁcation. download_sets folder is similar to spectra_sets,
but text ﬁles in it contains only identiﬁers (PLATE, MJD, FIBERID). These lists will be
used to download spectra from the SDSS Science Archive Server.

64

A.2 Preprocessing

Preprocessing of spectra will take place in the spectra folder. This is the folder’s structure
by default:

Folder spectra_sets is just copied from the catalog folder. train, valid and test folders
contain spectra (ﬁts ﬁles) downloaded from the Science Archive Server. Only thing you
have to do is to simply run the reduce.sh script (if you closed the terminal after the
preparation of datesets, reinitalize heatools):

1 shell$ ./reduce.sh

This operation extracts the spectra from ﬁts ﬁles, reduces them and does several other
modiﬁcations. There is one important thing to remember. This operation alters the spec-
tra_sets folder (removes blacklisted spectra from datasets). Therefore if you wish to run
reduce.sh script again (for instance trying diﬀerent parameters), don’t forget to replace
the altered spectra_sets folder with the original one from catalog folder.

Next step is the preobj.sh script. It bins spectra and transforms them from one di-
mensional vectors into two dimensional matrices (and then converts them into images).
Find out following lines in the script:

1

2

mati="60"
matj="60"

They specify dimensions of the output matrix. You can adjust these dimensions, but
keep two things in mind. The output matrix has to be a square and number of its elements
has to be smaller or equal than number of elements in the input vector. Then you can
simply execute the script:

65

1 shell$ ./preobj.sh

The folder at the end contains these ﬁles:

imgs folder contains spectral images sorted into directories by dataset and subdirec-
tories by object class. Content of one of these subdirectories (only fraction) can look like
this:

66

A.3 Training and classiﬁcation

I will describe a sample training of the convolutional neural network and classiﬁcation on
the testing dataset. Our working environment will be the class folder:

imgs and spectra_sets directories were copied from the spectra folder. Files with .conf
extension are conﬁguration ﬁles for EBLearn library. I won’t go into much detail about
their creation and modiﬁcation because it would be beyond this manual. You can use ﬁles
attached in the Appendix B, they are conﬁguration ﬁles for the best performing LeNet-5
architecture with l2pool and eta decay. These attached conﬁguration ﬁles will work. You
only have to adjust paths:

1 # paths ###################################################################
2 name
3 ebl
4 root

= object
= /home/moja/eblearn_1.2_r2631
= /home/moja/Desktop/dip/data/class/lenet5_pool_decay_i100/inputs

#eblearn root

#data root

5 tblroot

= ${ebl}/tools/data/tables # location of table files

ebl variable is path to the EBLearn library and root is path to the compiled in-
puts directory (product of the compile.sh script). To train the network, you have to run
compile.sh and then train.sh scripts. Only parameter you have to adjust is the

1 dim="60"

parameter in dscompile.sh script (it’s also in dscompile.sh). It’s the dimension of

input images. Then execute the scripts:

67

1 shell$ ./compile.sh
2 shell$ ./train.sh

Product of training is the train_output ﬁle, which sums up the training process and
net directory that containes trained network conﬁguration ﬁles for each epoch. You can
visualize the training process (success rates) by running the plot.sh script. Classiﬁcation
on the testing dataset is performed by classify.sh script:

1 shell$ ./classify.sh

The class folder will have this structure at the end:

classify_match is a product of the classiﬁcation. It contains correctly classiﬁed objects
from the testing dataset (with labels). classify_mismatch on the other hand contains ob-
jects where ConvNet’s classiﬁcation is in conﬂict with the BOSS catalog. You can compute
success rate on the testing dataset by comparing these two ﬁles.

Here’s an excerpt from the classify_mismatch ﬁle:

1 #PLATE MJD
2 3586
3 3586
4 3587
5 3587
6 3588
7 3588
8 ...

55181
55181
55182
55182
55184
55184
...

FIBERID
45
94
733
739
70
280
...

catalog: qso
catalog: galaxy
catalog: star
catalog: galaxy
catalog: qso
catalog: qso
...

convnet: galaxy
convnet: star
convnet: qso
convnet: qso
convnet: galaxy
convnet: galaxy
...

68

Bibliography

[1] http://www.sdss3.org/surveys/boss.php.

[2] http://www.sdss3.org/dr10/.

[3] http://www.sdss3.org/dr10/spectro/caveats.php.

[4] http://en.wikipedia.org/wiki/Artificial_neural_network.

[5] http://en.wikipedia.org/wiki/YIQ.

[6] http://leenissen.dk/fann/html/files/fann_cascade-h.html.

[7] http://en.wikipedia.org/wiki/Lateral_geniculate_nucleus.

[8] http://en.wikipedia.org/wiki/Visual_cortex.

[9] http://en.wikipedia.org/wiki/Relativistic_beaming.

[10] http://heasarc.gsfc.nasa.gov/docs/software/lheasoft/.

[11] Brain and behaviour: 5. processing visual information. http://www.floiminter.net/

?page_id=176.

[12] Dr10 science archive server – sdss-iii. http://data.sdss3.org/bulkSpectra.

[13] Ahmad AbdulKader. A two-tier arabic oﬄine handwriting recognition based on con-
ditional joining rules. In Arabic and Chinese Handwriting Recognition, pages 70–81.
Springer, 2008.

[14] Heather Arthur. brain.js demo: Train a neural network to recognize color contrast.

http://harthur.github.io/brain/.

[15] Lyes Bachatene, Vishal Bharmauria, and Stéphane Molotchnikoﬀ. Adaptation and

neuronal network in visual cortex. 2012.

[16] Yoshua Bengio. Algorithmes d’apprentissage et architectures profondes. http://www.

iro.umontreal.ca/~pift6266/H10/intro_diapos.pdf.

69

[17] Adam S Bolton, David J Schlegel, Éric Aubourg, Stephen Bailey, Vaishali Bhardwaj,
Joel R Brownstein, Scott Burles, Yan-Mei Chen, Kyle Dawson, Daniel J Eisenstein,
et al. Spectral classiﬁcation and redshift measurement for the sdss-iii baryon oscillation
spectroscopic survey. The Astronomical Journal, 144(5):144, 2012.

[18] David Bouchain. Character recognition using convolutional neural networks. Institute

for Neural Information Processing, 2007, 2006.

[19] Kumar Chellapilla, Patrice Simard, et al. A new radical based approach to oﬄine
In Tenth International Workshop on

handwritten east-asian character recognition.
Frontiers in Handwriting Recognition, 2006.

[20] George Cybenko. Approximation by superpositions of a sigmoidal function. Mathe-

matics of control, signals and systems, 2(4):303–314, 1989.

[21] Andrea Frome, German Cheung, Ahmad Abdulkader, Marco Zennaro, Bo Wu,
Alessandro Bissacco, Hartwig Adam, Hartmut Neven, and Luc Vincent. Large-scale
privacy protection in google street view. In Computer Vision, 2009 IEEE 12th Inter-
national Conference on, pages 2373–2380. IEEE, 2009.

[22] Fu Jie Huang and Yann LeCun. Large-scale learning with svm and convolutional
for generic object categorization. In Computer Vision and Pattern Recognition, 2006
IEEE Computer Society Conference on, volume 1, pages 284–291. IEEE, 2006.

[23] Pavel Hála. http://physics.muni.cz/~hala/source.tar.gz.

[24] Pavel Hála. Multiwavelength study of active galactic nuclei. Bachelor’s thesis,
Masaryk university, Faculty of science, 2011. https://is.muni.cz/th/323919/prif_
b/?lang=en.

[25] Koray Kavukcuoglu, Marc’Aurelio Ranzato, and Yann LeCun. Fast inference in
sparse coding algorithms with applications to object recognition. arXiv preprint
arXiv:1010.3467, 2010.

[26] Yann LeCun. Deep learning tutorial.

http://www.cs.nyu.edu/~yann/talks/

lecun-ranzato-icml2013.pdf.

[27] Yann LeCun and Yoshua Bengio. Convolutional networks for images, speech, and time

series. The handbook of brain theory and neural networks, 3361, 1995.

[28] Yann LeCun, Patrick Haﬀner, Léon Bottou, and Yoshua Bengio. Object recognition
In Shape, contour and grouping in computer vision,

with gradient-based learning.
pages 319–345. Springer, 1999.

70

[29] Yann LeCun, Koray Kavukcuoglu, and Clément Farabet. Convolutional networks and
applications in vision. In Circuits and Systems (ISCAS), Proceedings of 2010 IEEE
International Symposium on, pages 253–256. IEEE, 2010.

[30] J Mańdziuk and R Mikołajczak. Chaotic time series prediction with feed-forward and

recurrent neural nets. Control and Cybernetics, 31:383–406, 2002.

[31] Peter C McCluskey. Feedforward and recurrent neural networks and genetic programs
for stock market and time series forecasting. Master’s thesis, Brown University, 1993.

[32] Steﬀen Nissen. Implementation of a fast artiﬁcial neural network library (fann). Re-
port, Department of Computer Science University of Copenhagen (DIKU), 31, 2003.

[33] Christopher Poultney, Sumit Chopra, Yann L Cun, et al. Eﬃcient learning of sparse
representations with an energy-based model. In Advances in neural information pro-
cessing systems, pages 1137–1144, 2006.

[34] Pierre Sermanet, Soumith Chintala, and Yann LeCun. Convolutional neural networks
applied to house numbers digit classiﬁcation. arXiv preprint arXiv:1204.3968, 2012.

[35] Pierre Sermanet, Koray Kavukcuoglu, and Yann LeCun. Eblearn: Open-source energy-
In Tools with Artiﬁcial Intelligence, 2009. ICTAI’09. 21st

based learning in c++.
International Conference on, pages 693–697. IEEE, 2009.

[36] Pierre Sermanet, Koray Kavukcuoglu, and Yann LeCun. Traﬃc signs and pedestri-
In Snowbird Machine Learning

ans vision with multi-scale convolutional networks.
Workshop, volume 2, page 8, 2011.

71

