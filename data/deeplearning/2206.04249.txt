An Optimization Method-Assisted Ensemble Deep
Reinforcement Learning Algorithm to Solve Unit
Commitment Problems

Jingtao Qin, Student Member, IEEE, Yuanqi Gao, Member, IEEE, Mikhail Bragin, Member, IEEE,
and Nanpeng Yu, Senior Member, IEEE,

2
2
0
2

n
u
J

9

]

Y
S
.
s
s
e
e
[

1
v
9
4
2
4
0
.
6
0
2
2
:
v
i
X
r
a

Abstract—Unit commitment (UC) is a fundamental problem
in the day-ahead electricity market, and it is critical to solve
UC problems efﬁciently. Mathematical optimization techniques
like dynamic programming, Lagrangian relaxation, and mixed-
integer quadratic programming (MIQP) are commonly adopted
for UC problems. However, the calculation time of these methods
increases at an exponential rate with the amount of generators
and energy resources, which is still the main bottleneck in
industry. Recent advances in artiﬁcial intelligence have demon-
strated the capability of reinforcement learning (RL) to solve
UC problems. Unfortunately, the existing research on solving
UC problems with RL suffers from the curse of dimensionality
when the size of UC problems grows. To deal with these
problems, we propose an optimization method-assisted ensemble
deep reinforcement learning algorithm, where UC problems are
formulated as a Markov Decision Process (MDP) and solved
by multi-step deep Q-learning in an ensemble framework. The
proposed algorithm establishes a candidate action set by solving
tailored optimization problems to ensure a relatively high perfor-
mance and the satisfaction of operational constraints. Numerical
studies on IEEE 118 and 300-bus systems show that our algorithm
outperforms the baseline RL algorithm and MIQP. Furthermore,
the proposed algorithm shows strong generalization capacity
under unforeseen operational conditions.

Index Terms—Deep reinforcement learning, multi-step return,

optimization methods, unit commitment.

I. INTRODUCTION

U NIT Commitment (UC) is a crucial decision-making tool

used by Independent System Operators (ISOs) in the
day-ahead electricity market. In UC problems, the optimal
schedule of generators needs to be determined given the
supply offers, demand bids, transmission network situations,
and operational limits. The UC problems can be classiﬁed into
different subgroups in a few ways [1]. With respect to security
constraints, UC problems can be divided into conventional
UC problems and security-constrained UC (SCUC) problems
[2, 3]. According to whether uncertainty is considered and
whether the probabilistic distribution of uncertain parameters
is known [4], UC can be categorized into deterministic UC
problems, stochastic UC problems [5, 6], and robust UC
problems [7–10].

It is critical for enhancing the efﬁciency of the day-ahead
to obtain near-optimal solutions to UC
electricity market
problems. The existing solution approaches to UC problems
include heuristic algorithms [11, 12], mathematical optimiza-
tion algorithms, intelligent optimization algorithms [13–15],

and machine learning (ML) based approaches. Among these
approaches, mathematical optimization algorithms including
dynamic programming (DP) [16], branch-and-cut algorithm
[17], Lagrangian relaxation (LR) [18], Benders decomposition
[19], outer approximation [20], ordinal optimization [21],
and column-and-constraint generation [22] have been widely
studied in UC problems. Even though satisfactory performance
is achieved by these methods, their calculation time grows at
an exponential rate with the amount of energy resources. Thus,
obtaining a near-optimal UC solution efﬁciently can be difﬁ-
cult when the renewable energy resources and corresponding
uncertainties keep rising. To improve the performance of large-
scale UC problem solvers, some researchers try to improve
the tightness and compactness of the UC problem formulation
as a MILP model [23–25]. Besides, a decomposition and
coordination approach is proposed in [26], which leverages
Surrogate Lagrangian Relaxation (SLR) to solve large-scale
UC problems with near-optimal solutions within time limits.
Furthermore, Surrogate Absolute-Value Lagrangian Relaxation
(SAVLR) is enhanced in [27] by embedding the ordinal-
optimization method to drastically reduce the solving time of
subproblems. Recently, a novel quantum distributed model is
proposed to solve large-scale UC problems in a decomposi-
tion and coordination-supported framework [28]. A temporal
decomposition method was proposed in [29] which system-
atically decouples the long-horizon MIP problem into several
sub-horizon models.

The main limitation of mathematical optimization algo-
rithms is that they assume one-shot optimization where the
UC problems need to be solved from scratch each time. In
practice, UC problems are solved on a daily basis in the day-
ahead market with changes to the input data [30] while the
structure of the problem formulation stays the same. Thus, the
previous UC problems’ solutions provide useful information
that can be utilized to improve the solution quality of similar
UC problems.

The recent advances in artiﬁcial intelligence motivate the
development of machine learning-based methods to solve UC
problems [31]. A series of machine learning techniques are
proposed to extract valuable information from solved instances
of UC problems to enhance the warm-start capabilities of
MIP solvers in [30]. Neural networks are developed to imitate
expert heuristics and speed up the branch-and-bound (B&B)

 
 
 
 
 
 
algorithm, which achieves signiﬁcant improvements on large-
scale real-world application datasets including Electric Gird
Optimization [32]. Unlike supervised learning, which requires
labeled data, reinforcement learning (RL) is a mathematical
tool for learning to solve sequential decision-making prob-
lems such as volt-var control problems in power distribution
systems [33]. In [34], UC problems for a system with 4 units
are modeled as multi-stage decision-making tasks, and RL
solutions are formulated through the pursuit method. Three RL
algorithms including approximate policy iteration, tree search,
and back sweep are proposed to minimize operational costs on
a 12-unit system [35]. The UC problem with 10 units is tackled
as a multi-agent fuzzy RL task, and units play as agents to
corporately reduce the overall operation cost [36]. A method
based on decentralized Q-learning to ﬁnd a solution to UC
problems on a system with up to 10 units is introduced in [37].
An RL-based guided tree search algorithm is developed to
solve stochastic UC problems for a system with 30 generation
units [38], which uses a pre-trained policy to reduce the action
space and designs a neural network as a binary classiﬁer that
sequentially predicts each bit in the action sequence. Most
of the existing RL-based algorithms have only been tested
on small-scale UC problems because they suffer from the
curse of dimensionality. Speciﬁcally, the number of states and
feasible actions increases exponentially with the size of the UC
problems. Furthermore, many operational constraints such as
the transmission line capacity limit can not be strictly enforced
in these RL-based algorithms.

To address the limitations of the existing algorithms, we
synergistically combine the mathematical optimization method
with RL and propose an optimization method-assisted ensem-
ble deep reinforcement learning algorithm to solve determin-
istic UC problems. The overall framework of the proposed
approach is shown in Fig. 1. First, we establish a candidate
action set by solving a series of simpliﬁed optimization prob-
lems to ensure that the solutions are feasible and can achieve
decent performance. These candidate actions will serve as
inputs to the RL-based solution. Then, we design a multi-
step deep Q-learning algorithm to ﬁnd good sequential unit
commitment decisions. By leveraging the multi-step return,
the proposed algorithm explicitly accounts for the fact that the
total impacts of a unit commitment decision may not instantly
appear in the system operational cost and could inﬂuence the
costs of many subsequent time steps. Finally, we propose an
ensemble framework consisting of a group of deep Q-learning
agents which are trained separately in parallel threads with
different initial model parameters to ﬁnal a better UC solution.
This design can alleviate the problem that the gradient-based
training is prone to being trapped by a locally optimal solution.
The performance of our proposed algorithm, a baseline
optimization method, as well as a state-of-the-art RL algorithm
[38] are evaluated on two IEEE test systems. The experimental
results show that our proposed algorithm identiﬁes feasible
unit commitment solutions with lower costs than both the
PPO-based guided tree search algorithm and the MIQP given
the same amount of computation time. Moreover, additional

scenario analysis demonstrates that our proposed algorithm
possesses reasonable generalization capability and can still
achieve decent UC results when there is a generation unit or
transmission line outage.

Fig. 1. The overall framework of optimization method-assisted
ensemble RL algorithm.

The unique contributions of this paper are as follows:
• This paper proposes an optimization method-assisted
ensemble multi-step deep reinforcement learning algorithm
which synergistically combines the merits of mathematical op-
timization and reinforcement learning to solve UC problems.
• The proposed algorithm establishes a candidate action set
by solving a series of simpliﬁed UC problems, which ensures
high-quality unit commitment solutions that satisfy operational
constraints.

• The proposed multi-step deep Q-learning algorithm uses a
multi-step return to improve the sample efﬁciency and speed
up the process of learning a near-optimal unit commitment
solution.

• The ensemble RL framework reduces the operational costs
of the power system by mitigating the problem that gradient-
based training of neural networks is prone to be trapped by a
locally optimal solution.

• The proposed optimization method-assisted ensemble
multi-step deep reinforcement learning algorithm has great
generalization capability to solve unit commitment problems
when there is a loss of generation units or transmission lines.
The remainder of the paper is organized as follows: Section
II gives the formulation of UC problems. Section III introduces
the technical methods. Section IV discusses the experimental
and algorithm setup as well as the results of numerical studies.
Section V makes the conclusion.

II. PROBLEM FORMULATION
In this section, we ﬁrst give the formulation of Unit Com-
mitment (UC) problems, then we go over the preliminaries of
the Markov decision process (MDP). At last, the UC problems
are formulated as an MDP.

A. Formulation of Unit Commitment Problems

As shown in (1), the goal of UC problems is to obtain
the optimal schedule of generators which yields the minimum

overall operation cost, while subject to certain operational
constraints. UC problems for arbitrary optimization horizons
can be formulated as follows. Here we take ﬁve types of
constraints into consideration, speciﬁcally, the load/spinning
reserve demand constraints (2)-(3), generation constraints (4)-
(5), ramping limits (6)-(8), minimum up and down time limits
(9)-(12), and transmission line capacity constraints (13):

min

T
(cid:88)

N
(cid:88)

t=1

i=1

(cid:8)cp

i (t) + cu

i (t) + cd

i (t)(cid:9) ,

N
(cid:88)

i=1

pi(t) =

M
(cid:88)

j=1

dj(t), t = 1, · · · , T,

¯pi(t) ≥

M
(cid:88)

j=1

dj(t) + R(t), t = 1, · · · , T,

s.t.

N
(cid:88)

i=1

P ivi(t) ≤ pi(t) ≤ ¯pi(t), i = 1, · · · , N, t = 1, · · · , T,
0 ≤ ¯pi(t) ≤ ¯Pivi(t), i = 1, · · · , N, t = 1, · · · , T,

¯pi(t) ≤ pi(t − 1) + RUivi(t − 1) + ¯Pi(1 − vi(t))

(1)

(2)

(3)

(4)

(5)

+ SUi[vi(t) − vi(t − 1)], i = 1, · · · , N, t = 1, · · · , T,
(6)

unit i. vi(t) is the commitment status of unit i in period t.
RUi and RDi are the ramping up and down constraints of unit
i. SUi and SDi are the startup and shutdown ramp constraints
of unit i, Gi and Li are the numbers of periods during which
unit i must be on or off in the beginning. F −
l are the
negative and positive power ﬂow limit of line j. ΓP
i,l and ΓP
i,l
are the power transfer distribution factor from unit i to line l.
σu
i (t), σd
i (t) are the number of periods that unit i must be on
or off from period t on, which are given in (14)-(15) below:

l and F +

σu
i (t) =

σd
i (t) =

(cid:40)

min(UTi, T − t),
0,

(cid:40)

min(DTi, T − t),
0,

if H > Gi,
else,

if H > Li,
else.

(14)

(15)

UTi and DTi are the minimum up and down time of unit i.
The production cost, startup cost, and shutdown cost in (1)

are speciﬁcally deﬁned as follows:

1) Production Cost: Here we use the quadratic production

cost function [39] as in equation (16).

cp
i (t) = aivi(t) + bipi(t) + cip2

i (t),
∀i = 1, · · · , N, ∀t = 1, · · · , T,

(16)

¯pi(t) ≤ ¯Pivi(t + 1) + SDi[vi(t) − vi(t + 1)],
i = 1, · · · , N, t = 1, · · · , T,

pi(t − 1) ≤ pi(t) + RDivi(t) + SDi[vi(t − 1) − vi(t)]

+ ¯Pi[1 − vi(t − 1)], i = 1, · · · , N, t = 1, · · · , T,
(8)

min(Gi,T )
(cid:88)

[1 − vi(t)] = 0, i = 1, · · · , N, t = 1, · · · , T,

t=1
min(t+UTi,T )
(cid:88)

n=t+1

vi(n) ≥ σu

i (t)[vi(t) − vi(t − 1)],

i = 1, · · · , N, t = Gi, · · · , T − 1,

(7)

where ai, bi and ci are the coefﬁcients of the quadratic
production cost function.

2) Startup Cost: A mixed-integer linear function for the

stair-wise startup cost is formulated in (17):

(cid:34)

i (t) ≥ CUk
cu
i

vi(t) −

(cid:35)

vi(t − n)

,

k
(cid:88)

n=1

(17)

(9)

(10)

∀i = 1, · · · , N, ∀t = 1, · · · , T, ∀k = 1, · · · , NDi,

cu
i (t) ≥ 0, ∀i = 1, · · · , N, ∀t = 1, · · · , T,

(18)

where CUk
is the stair-wise startup cost of unit i in period
i
t. NDi is the number of intervals of the staircase startup cost
function of unit i.

3) Shutdown Cost: The shutdown cost is shown in (19):

vi(t) = 0, i = 1, · · · , N,

(11)

cd
i (t) ≥ CDi(vi(t − 1) − vi(t)),

min(Li,H)
(cid:88)

t=1

min(t+DTi,H)
(cid:88)

n=t+1

[1 − vi(n)] ≥ σd

i (t)[vi(t − 1) − vi(t)],

i = 1, · · · , N, t = Li, · · · , T − 1,

F −

l ≤

N
(cid:88)

i=1

pi(t)ΓP

i,l −

M
(cid:88)

j=1

dj(t)ΓD

j,l ≤ F +
l ,

l = 1, · · · , L, t = 1, · · · , T.

(12)

(13)

i (t), cd

where T is the number of time periods of optimization.
N and M are the number of units and number of buses.
cp
i (t), cu
i (t) are the production cost, startup cost and
shutdown cost of unit i in period t, respectively. pi(t) and
¯pi(t) are the power output and maximum available output of
unit i in period t. dj(t) is the load demand and at bus j in
period t. R(t) is spinning reserve requirement of the system
in period t. ¯Pi, P i are the capacity and minimum output of

∀i = 1, · · · , N, ∀t = 1, · · · , T,

cd
i (t) ≥ 0, ∀i = 1, · · · , N, ∀t = 1, · · · , T,

(19)

(20)

where CDi is the shutdown cost of unit i.

B. Preliminaries of Markov Decision Process

As a classical mathematical framework to formulate sequen-
tial decision making problems, Markov Decision Process can
be deﬁned as a tuple (S, A, P, R, γ), which consists of a state
space S, an action space A, a state transition probability P,
a reward function R and a discount factor γ (0 ≤ γ ≤ 1)
[40]. The agent chooses an action at ∈ A at every time step t
depending on the current state st, and it gains a certain reward
rt+1, then the environment shifts to the next state st+1 based
on P(st+1|st, at).

The agent aims to ﬁnd a policy π(a|s) that gives the
maximum anticipated discounted return J(π) = E[G(τ )].

Here G(τ ) = (cid:80)T
t=0 γtrt+1, T is the length of the episode, and
τ is a trajectory of states and actions. In order to demonstrate
the value of states and state-action pairs given a policy π, we
give the deﬁnition of two crucial value functions vπ(s) and
qπ(s, a):

4) Rewards: To refrain from the early termination of an
episode, the agent receives a large penalty when no feasible
action can be found in the current state. So, the reward function
is given as (24):

rt+1 = −

(cid:40)

Ct+1,
ζ,

if At+1 (cid:54)= ∅,
if At+1 = ∅.

(24)

vπ(s) = Eπ[Gt|St = s]

= Eπ

(cid:104)(cid:80)T

(cid:105)
k=0 γkrt+k+1|St = s
,

qπ(s, a) = Eπ[Gt|St = s, At = a]
(cid:104)(cid:80)T

(cid:105)
k=0 γkrt+k+1|St = s, At = a

= Eπ

(21)

.

(22)

We deﬁne the best policy as π(a|s) = arg maxπ vπ(s) for
all s ∈ S or π(a|s) = arg maxπ qπ(s, a) for all s ∈ S and
a ∈ A(s).

C. Formulate the UC Problems as an MDP

The UC problems are constructed as an MDP in this
subsection. We give the following deﬁnition of episode, state,
action, and reward functions.

1) Episode and Time steps: Each operation period is de-
ﬁned as a time step. Since the UC problems are solved daily
in the day-ahead market, we formulate them as continuous
tasks, which means an episode ends only when no feasible
action can be found.

2) States: In order to ensure the environment is Markovian,
the state at time t is deﬁned as st = (t, vvvt, pppt, uuut, dddt), where
t is the global time, vvvt is a vector of the commitment status
vi(t) of generator i in time t (1 if the unit is on, 0 otherwise),
pppt is a vector of the power generation pi(t) of unit i in time
t, uuut is a vector of the number of periods that unit i has been
running or ofﬂine until time t, and the transition function of
ui(t) can be formulated as (23):

(cid:40)

ui(t) =

ui(t − 1) + 1,
1,

if vi(t) = vi(t − 1),
otherwise.

(23)

Here vi(0) is the on/off state of unit i in the beginning of
the episode, and ui(0) is the amount of periods that unit i has
been running or ofﬂine before the initial period of the episode.
Finally, dddt is a vector [d(t + 1), d(t + 2), · · · , d(t + k)] of load
predictions for the next k periods.

3) Actions: We deﬁne the action at at time t as shifting
the commitment status of all generators to vvvt+1 in period
t + 1. Because of the operational limits of generators, there
are numerous infeasible statuses. To avoid missing the best
the feasible actions in the
action, we have to obtain all
current state. However, the space of the feasible actions set
remains prohibitively large even though we can ﬁlter out
infeasible actions. Besides, it will be extremely difﬁcult for
reinforcement learning to learn a good policy from a huge
action space. So we use the optimization method to down-
select candidate solutions and build the feasible action subset
At, which will be introduced later in subsection III-A.

Here Ct+1 is the negative of the operational cost in period
t + 1 as shown in (25):
N
(cid:88)

N
(cid:88)

N
(cid:88)

cu
i (t + 1) +

cd
i (t + 1), (25)

cp
i (t + 1) +

Ct+1 =

i=1

i=1
i=1
where the production cost cp
i (t+1) is derived by solving a one-
period economic dispatch (ED) after the commitment status
vi(t + 1) is obtained. Here we use uuut to directly calculate the
startup cost as in (26):

(cid:40)

cu
i (t + 1) =

SCUi [min{NDi, ui(t)}] ,
0,

if vi(t + 1) > vi(t)
otherwise,

(26)
(cid:3) is a list of the staircase
where SCUi = (cid:2)CU1
i
startup cost of unit i, and the symbol SCUi[j] represents the
j-th element of the vector SCUi.

i , · · · , CUNDi

III. TECHNICAL METHODS

In this section, we present

the proposed optimization
method-assisted ensemble RL algorithm. First, we give the
process of ﬁnding candidate actions using optimization meth-
ods, then we introduce a multi-step deep Q-learning algorithm
to solve the UC problems. Finally, we design an ensemble
framework to further boost performance.

A. Finding Candidate Actions Using Optimization methods

The process of ﬁnding candidate actions can be divided into
two parts. First, based on the current state, we solve a tailored
UC problem for the next couple of periods to obtain a unit
commitment schedule as the cardinal action. Then, given the
cardinal action, we obtain more candidate actions by turning
on or off more units based on the priority of units.

1) Finding the Base Action: Starting from period t, the
mathematical form of a H-period UC problem can be formu-
lated as follows:
N
(cid:88)

t+H
(cid:88)

{Ci(k) + ωt (vi(k + 1) − vi(k)) ρi, }

min

(27)

k=t+1

i=1

(2) − (13)

s.t.
i (k) + cu

where Ci(k) = cp
i (k) + cd
i (k) is the production cost
of unit i in period k. ωt is a coefﬁcient related to t which is a
positive constant when t > 0 and equals to zero when t = 0.
ρi is the average fuel price per output power of unit i, which
is given in the following formula (28):

ρi =

ai + bi ¯Pi + ci ¯P 2
i
¯Pi
After solving the UC problem above, we can obtain the unit
commitment schedule of next H-period as vvvt+1, · · · , vvvt+H
and we set vvvt+1 to be the cardinal action vvv∗
t+1of next period.

(28)

.

2) Obtaining More Candidate Actions: Assume there are
X units which are turned on or off at period t + 1 if we
take action vvv∗
i (t + 1) − vi(t)|. Then,
instead of turning on/off X units, we turn on or off z units
which have the higher priority by solving the following single
period UC problems:

t+1, where X = (cid:80)N

i=1 |v∗

N
(cid:88)

min

(vi(t + 1) − vi(t)) ρi,

s.t.

i=1
vj(t + 1) = vj(t), ∀j ∈ Θt,
N
(cid:88)

|vi(t + 1) − vi(t)| = z,

i=1

(2) − (8), (13)

(29)

(30)

(31)

where z gradually increases from max(X − Y −, 0) to
min(X + Y +, N ). Y − and Y + denote the parameters of
the searching range for unit status change beyond X. Θt
denotes the set of indexes of the units which cannot be turned
on/off due to the minimum up/down-time limit and shutdown
ramping limit at period t + 1.

Note that here we can keep the top K best solutions of
the single period UC problem, which means we can obtain
|Z| × K candidate actions, |Z| is the number of elements in
the range of z. Finally, there are |Z| × K + 1 candidate actions
in the action subset At.

B. Multi-Step Deep Q-Learning for UC Problems

a(cid:48)

For the purpose of solving MDPs with continuous state
space [41], Deep Q-learning integrates the standard Q-learning
with a deep neural network named deep Q network (DQN)
Q(st, at|θ) to estimate the action-value function in (22). We
use Adam gradient descent to train DQN to minimize the
mean-squared temporal difference error L(θ):
(cid:16)

(cid:17)2

r + γ max

Q(s(cid:48), a(cid:48)|θ(cid:48)) − Q(s, a|θ)

L(θ) = E(s,a,r,s(cid:48))∼D

(32)
where Q(s(cid:48), a(cid:48)|θ(cid:48)) is the target neural network with the same
structure as Q(st, at|θ). In order to make the training process
stable, we update the parameters θ(cid:48) of the target network from
the parameters θ of Q(st, at|θ) in a periodical manner. D is
the replay buffer to collect the transition tuples (s, a, r, s(cid:48)).

However, adopting DQN to solve UC problems without
modiﬁcations can be inefﬁcient, since taking an action may
not only affect the next reward, but also inﬂuence the rewards
multiple steps later. We need several updates to propagate
the reward to the related preceding states and actions [42],
which makes the training process both time-consuming and
tremendously sample-inefﬁcient.

To address this issue, we use the multi-step return method

[43] to update the action-value function Q(st, at|θ):

L(θ) = (Q(st, at|θ) − R(t))2,

(33)

where R(t) is deﬁned as:
R(t) = rt+1+γrt+2+· · ·+γn−1rt+n+ max
a(cid:48)∈A(cid:48)

γnQ(st+n, a(cid:48)|θ(cid:48))
(34)

Note that we make the agent-environment interact for n steps
to acquire the rewards rt+k, k = 1, . . . , n and the n-step next
state st+n, and then calculate (34).

Using the n-step return, the long-term as well as the short-
term impacts of taking an action can be studied by propagating
toward the exact reward, instead of bootstrapping from the
target network Q(s, a|θ(cid:48)). Therefore, the learning efﬁciency
of UC problems is prominently enhanced by applying multi-
step deep Q-learning algorithm [44].

C. Ensemble RL Framework for UC Problems

We now present a multi-threaded ensemble reinforcement
learning framework. The aim of designing this framework is to
mitigate the problem of reaching a bad local optimal solution
from a single random initialization of deep Q-network’s pa-
rameters. Speciﬁcally, we initialize the aforementioned multi-
step deep Q-learning algorithm in different threads with dif-
ferent random seeds and run them in parallel. The pseudocode
for training the ensemble multi-step deep Q-learning for UC
problems is shown in algorithm 1. After the ensemble multi-
step deep Q-learning algorithm is trained, the M instances
of RL agents can be run in parallel during the testing phase.
The multi-step deep Q-learning agent that identiﬁes the unit
commitment solution with the lowest operational cost will be
selected as the ﬁnal solution.

Here are some key implementation details of algorithm 1.
• Q-network structure: We adopt the feed-forward neural
networks as the Q-networks, whose inputs are state-action
pairs and outputs are the resulting Q value. The reason for
selecting this architecture is that it can scale linearly with the
number of generators.

• Episode initialization: Since UC problems are formulated
as continuous tasks, which means we aim to maximize the
overall reward received in all training episodes, we get the
initial state of the current day from the ﬁnal period of the
previous day. Thus, the historical data of the next day will
not be utilized for training until a policy that meets all load
demands of the current day is found.

• Global Time encoding: In order to present the periodic
nature of the problem, the global time step t is decomposed
into two coordinates [cos(2πt/24), sin(2πt/24)] [45], which
varies from 0 to 23.

IV. NUMERICAL STUDIES

A. Experimental and Algorithm Setup

In this subsection, we give the experimental and algorithm
setups. All algorithms are executed on a server with a 32-core
AMD Ryzen Threadripper 3970X 3.7GHz CPU.

1) Experimental Setup: We apply the proposed method to
solve a 48-hour period UC problem for the IEEE 118-bus
system and 300-bus system [46]. The parameters of units such
as minimum up and down time limit are obtained from [47].
The detailed experimental setup for two systems can be found
in our open-source repository1. The aforementioned minimum

1https://github.com/jqin020/Emsemble-Deep-RL-for-UC-problems

Algorithm 1 Training of Ensemble Multi-Step Deep Q-
learning for UC Problems
Initialize M evaluation Q-network with random parameters
θ1 · · · θM
Initialize M target Q-network with parameters θ(cid:48)
θ(cid:48)
M = θM
1: for thread m = 1, · · · , M do
2:
3:
4:
5:
6:

Input historical data set of Nd days and set day d = 1
Initialize replay buffer D as a queue with a maximum
length of n
Initialize learning counter ν = 0
for episode = 1, · · · , Γ do

1 = θ1, · · · ,

7:
8:
9:
10:

11:

12:

13:

14:

15:
16:
17:
18:
19:

20:
21:
22:

23:
24:
25:

26:

27:

Input historical load data of day d
Formulate initial state s1 of day d
for t = 1, · · · , T do

Obtain candidate action set At of state st using
optimization method.
With (cid:15) choose a random action at from At,
otherwise choose at = maxa∈AtQ(st, a|θ(cid:48)
m).
Obtain the schedule of units on next period t+1
based on action at.
Solve a single period ED and calculate reward
rt+1 according to (24).
Calculate uuut+1 according to (23) and then
formulate the next state st+1.
Use optimization method to calculate At+1.
Set εt = 1 if At+1 = ∅ else 0
Store (st, at, rt+1, st+1, At+1, εt) in D
if length(D) = n or εt = 1 then

R = 0 if εt = 1 else maxa Q(st+1, a|θ(cid:48)
for i = t, t − 1, · · · , t − length(D), do

m)

Set R = ri + γR
Perform a gradient descent step on
(R − Q(si, ai|θm))2

Set ν = ν + 1
if mod(ν, Itarget) = 0 then

Update θ(cid:48)
if day d is over then

m = θm

d = mod(d + 1, Nd)

and maximum of staircase startup cost CU of all units are
equivalent to their hot start cost and cold start cost. Initial
on/off time is the number of periods that one generator has
been running or ofﬂine before the ﬁrst period of the starting
day. Here we give four different initial status setups to verify
the generalization ability of our algorithm. The historical load
data of the California Independent System Operator (CASIO)
[48] from January 1, 2021 to July 5, 2021 is used and scaled
to be suitable for the two IEEE systems. Note that we use
90 days for training, one week for validation, and two weeks
from different months for testing.

2) RL Algorithm Setup: The hyperparameters of the bench-
mark PPO guided tree search [38] and the proposed ensemble
multi-step deep Q-learning algorithm are summarized in Table

I. We tune all parameters separately to achieve the optimal
performance.

Table I: Hyperparameters of Benchmark and Proposed RL
Algorithms

Ensemble multi-step deep-Q

PPO guided tree search

Shared parameters

Number of threads M
Number of steps n
Load forecast steps k
Learning rate α
Update frequency Itarget
Greedy Arrange ¯(cid:15)
Optimization Horizon H
Search Down Y −
Search Up Y +
Top K Best Actions
Actor learning rate
Critic learning rate
Number of epochs
Clipping ξ
Search Depth H
Branching Threshold ρ
Number of hidden units
Number of hidden layers
Discount factor γ
Number of episode
Optimizer

10
24
9
0.0001
60
[0.01,1.0]
2
1
1
1
0.003
0.001
80
0.2
2
0.1
{150, 150}
1
0.99
50
Adam

B. Performance Comparison

In this subsection, we compare the performance of the PPO
guided tree search, the MIQP algorithm with Gurobi 9.1 [49]
solver, and our proposed algorithm. We set the time limit to 10
minutes and the MIP gap to 0.1% for Gurobi in all following
analyses. Note that the optimization periods of MIQP are 48
hours and we obtain the operation cost of the ﬁrst day from
the optimization result. The mean daily operational cost of
our proposed algorithm during the validation days of two IEEE
systems under four initial status setups are reported after every
training episode in Fig. 2. The beginning commitment status
of generators on the ﬁrst validation day is the same during the
training process. The solid curves and shaded areas represent
the average values and standard deviations across different
runs in 10 threads, respectively.

As shown in Fig. 2, the average daily costs of validation
days decrease rapidly as the training continues and maintain
a low level under all four initial status setups. When training
processes are done, the testing days are adopted to evaluate
the algorithms. For PPO guided tree search and our proposed
algorithm, we use the network parameters that yield the min-
imum average daily costs of the validation dataset for testing.
The average daily operation cost of testing days across four
initial status setups of the two IEEE systems are summarized
in Table II and Table III, respectively. The percentage variation
of both the PPO guided tree search and the proposed Ensemble
N-step Q-learning from the MIQP, δ1 and δ2 respectively, are
also given to compare their performance.

From Table II and Table III we can see the average daily
operation cost, as well as percent variation of the proposed
algorithm from MIQP is much smaller than that of the PPO
guided tree search. Additionally, the total computation time

Table III: Daily Operation Cost of the 300-bus System

Week
-Day

PPO guided
tree search ($)

Ensemble
N-step Q ($)

MIQP
($)

δ1(%)

δ2(%)

1-1
1-2
1-3
1-4
1-5
1-6
1-7
2-1
2-2
2-3
2-4
2-5
2-6
2-7

2,844,044
2,548,734
2,881,019
2,894,951
2,934,550
2,902,320
2,866,755
3,124,671
3,046,178
3,054,133
2,890,670
2,908,886
3,193,841
3,552,765

2,736,283
2,503,574
2,769,865
2,798,182
2,798,752
2,772,518
2,761,019
3,069,440
3,003,487
3,003,764
2,852,659
2,865,494
3,152,416
3,499,896

2,702,568
2,484,569
2,755,564
2,780,654
2,778,608
2,746,544
2,738,949
3,011,760
2,977,668
2,972,566
2,807,185
2,831,723
3,121,763
3,471,958

5.23
2.58
4.55
4.11
5.61
5.67
4.67
3.75
2.30
2.74
2.97
2.72
2.31
2.33

1.25
0.76
0.52
0.63
0.72
0.95
0.81
1.92
0.87
1.05
1.62
1.19
0.98
0.80

Table IV: Total Computation Time of Testing weeks

PPO tree
search (s)

Ensemble
N-step Q (s)

118-bus

300-bus

First test week
Second test week
First test week
Second test week

583
601
741
756

113
122
125
144

MIQP
(s)

4,278
4,283
4,355
4,353

(a) 118-bus system

(b) 300-bus system

Fig. 3. Comparison of three algorithms

(a) 118-bus system

(b) 300-bus system

Fig. 2. The average daily cost of validation days

Table II: Daily Operation Cost of the 118-bus System

Week
-Day

PPO tree
search ($)

Ensemble
N-step Q ($)

MIQP
($)

δ1(%)

δ2(%)

1-1
1-2
1-3
1-4
1-5
1-6
1-7
2-1
2-2
2-3
2-4
2-5
2-6
2-7

2,816,927
2,647,468
2,853,497
2,870,898
2,876,388
2,847,783
2,831,246
3,058,294
3,021,903
3,010,909
2,906,699
2,885,750
3,081,577
2,796,340

2,251,095
2,106,565
2,322,511
2,344,690
2,343,923
2,339,326
2,330,793
2,535,563
2,524,269
2,524,673
2,399,318
2,412,025
2,647,766
2,935,628

2,245,754
2,073,649
2,308,647
2,327,352
2,323,781
2,301,149
2,285,692
2,511,276
2,490,130
2,487,975
2,348,203
2,365,566
2,606,349
2,907,972

1.45
3.30
4.82
5.64
5.93
5.95
5.81
0.91
2.64
3.90
3.93
4.03
2.87
2.01

0.24
1.59
0.60
0.74
0.87
1.66
1.97
0.97
1.37
1.48
2.18
1.96
1.59
0.95

of testing weeks of two systems are shown in Table IV. Here
we only compare the testing time of RL-based and MIQP
algorithms since the training process of RL-based algorithms
can be done in an ofﬂine manner.

To further demonstrate the improvement of our proposed
algorithm, the total operation cost of the ﬁrst test week of
two systems computed by PPO guided tree search, ensemble
multi-step deep Q-learning, and MIQP with respect to the
computation time are shown in Fig. 3. From the ﬁgure, we can
see that the performance of PPO guided tree search is close

to MIQP while our proposed algorithm substantially surpasses
MIQP. In other words, to identify a unit commitment solution
of the same total operation cost, our proposed ensemble n-step
deep Q-learning only needs a fraction of the computation time
required by PPO-guided tree search and the MIQP algorithm.
Given sufﬁcient computation time, the MIQP algorithm will
as expected eventually identify a solution, which has a lower
operational cost than our proposed method.

C. Ablation Study

In this subsection, we study the impact of systematically
removing some features on our proposed algorithm. We start
by comparing the performance of using one-step return and
using multi-step return during the training process under the
ﬁrst initial status setup. As shown in Fig. 4, the average daily
costs of validation days calculated by ensemble multi-step
deep Q-learning stabilize at a lower level than that of ensemble
one-step deep Q-learning, and the standard deviations of
average daily costs across different runs received by using
multi-step return are much smaller than that of using one-step
return for both systems.

(a) 118-bus system

Q learning. It can be seen that the total costs of test weeks
are smaller when using a multi-step return and an ensemble
framework.

Table V: Total cost of Test Weeks Using Different RL Tech-
niques

118-bus

300-bus

One-step return
Multi-step return
One-step return
Multi-step return

Traditional (k$)
34,373
34,152
41,087
40,719

Ensemble (k$)
34,197
33,985
40,703
40,488

D. Generalization Capability

In this subsection, we show the generalization capability
of our proposed algorithm under some unforeseen operating
conditions, i.e. losing one generation unit or one transmission
line due to regular maintenance. First, we show the generation
ability of ensemble multi-step deep-Q learning as well as PPO
guided tree search when losing one unit. Note that for our
proposed algorithm and PPO guided tree search, we set the
on/off status and power output of the losing unit to zero and
set the time that the losing unit has been on/off to its minimum
on/off time limit. Table VI gives the total costs of testing weeks
of three algorithms for two systems when losing one unit and
we can see that our proposed algorithm yields smaller total
operation costs than PPO guided tree search, which are very
close to MIQP for both systems.

Table VI: Total Cost of Testing weeks when Losing One Unit

Loss of Unit

PPO tree
search (k$)

Ensemble
N-step Q (k$)

118-bus

300-bus

Unit 3
Unit 16
Unit 28
Unit 35
Unit 47
Avg. Time
Unit 5
Unit 14
Unit 26
Unit 37
Unit 43
Unit 58
Avg. Time

34,699
34,762
34,786
34,725
34,687
659s
41,737
41,535
41,746
41,663
41,783
41,632
804s

33,884
33,916
33,867
33,886
33,880
128s
40,368
40,263
40,412
40,316
40,365
40,309
146s

MIQP
(k$)

33,514
33,519
33,505
33,513
33,511
4,273s
40,067
40,071
40,069
40,074
40,070
40,074
4,357s

(b) 300-bus system

Fig. 4. Comparison of one-step and multi-step return

After the training process ends, we summarize the average
total cost of two test weeks under the third initial status setup
using different combinations in Table V. Note that here we use
the ﬁnal parameters of the neural networks for both ensemble
one-step and multi-step deep Q-learning, and we calculate the
average total cost of M runs for one-step and multi-step deep-

Then, the total operation costs of testing weeks of three
algorithms for two systems when losing one transmission
line are given in Table VII. Similar to the scenario where
systems lose one unit, our proposed algorithm shows greater
generalization capability than the PPO guide tree search when
losing one line. The baseline RL algorithm may not be
able to generalize to unforeseen system operation states. By
leveraging a simpliﬁed optimization method to identify can-
didate solutions and combining it with the RL algorithm, the
generalization capability of the proposed method is enhanced
signiﬁcantly.

Table VII: Total Cost of Testing weeks when Losing One Line

Loss of Line

PPO tree
search (k$)

Ensemble
N-step Q (k$)

118-bus

300-bus

Line 3
Line 38
Line 61
Line 98
Line 139
Line 152
Avg. Time
Line 10
Line 42
Line 60
Line 92
Line 224
Line 337
Avg. Time

34,843
34,742
34,667
34,761
35,010
34,688
594s
41,816
41,604
41,650
41,872
41,549
41,824
745s

34,225
34,376
33,948
33,852
34,155
33,880
128s
40,301
40,508
40,639
40,239
40,368
40,266
134s

MIQP
(k$)

33,726
33,779
33,505
33,497
33,781
33,508
4,273s
40,070
40,216
40,304
40,049
40,073
40,115
4,354s

V. CONCLUSION

This paper proposes an optimization method-assisted en-
semble deep reinforcement learning algorithm to solve unit
commitment problems. We establish a candidate action set by
solving simpliﬁed optimization problems. Multi-step return is
used to speed up the learning process and improve the sample
efﬁciency of the reinforcement learning agent. The proposed
ensemble framework can mitigate the adverse effects that
the gradient-based training could lead to a bad local optimal
solution. Numerical studies show that given a time limit of
solution, our algorithm can achieve a better performance than
the benchmark PPO guided tree search algorithm as well
as MIQP. Furthermore, our proposed optimization method-
assisted ensemble deep reinforcement learning algorithm has
great generalization ability under unforeseen operating condi-
tions. In the future, we plan to further improve the scalability
of the proposed algorithm and tackle the security-constrained
unit commitment problems on larger power systems.

REFERENCES

[1] I. Abdou and M. Tkiouat, “Unit commitment problem
in electrical power system: A literature review.” Int. J.
Electr. Comput. Eng. (2088-8708), vol. 8, no. 3, 2018.

[2] Y. Fu, Z. Li, and L. Wu, “Modeling and solution of the
large-scale security-constrained unit commitment,” IEEE
Trans. Power Syst., vol. 28, no. 4, pp. 3524–3533, 2013.
[3] J. Wang, M. Shahidehpour, and Z. Li, “Security-
constrained unit commitment with volatile wind power
generation,” IEEE Trans. Power Syst., vol. 23, no. 3, pp.
1319–1327, 2008.

[4] N. Yang, Z. Dong, L. Wu, L. Zhang, X. Shen, D. Chen,
B. Zhu, and Y. Liu, “A comprehensive review of security-
constrained unit commitment,” J. Mod. Power Syst. Clean
Energy, 2021.

[5] Q. Wang, J. Wang, and Y. Guan, “Stochastic unit com-
mitment with uncertain demand response,” IEEE Trans.
Power Syst., vol. 28, no. 1, pp. 562–563, 2012.

[6] C. Zhao and Y. Guan, “Data-driven stochastic unit com-
mitment for integrating wind generation,” IEEE Trans.
Power Syst., vol. 31, no. 4, pp. 2587–2596, 2015.
[7] D. Bertsimas, E. Litvinov, X. A. Sun, J. Zhao, and
T. Zheng, “Adaptive robust optimization for the security
constrained unit commitment problem,” IEEE Trans.
Power Syst., vol. 28, no. 1, pp. 52–63, 2012.

[8] S. Wang, C. Zhao, L. Fan, and R. Bo, “Distributionally
robust unit commitment with ﬂexible generation re-
sources considering renewable energy uncertainty,” IEEE
Trans. Power Syst., 2022.

[9] M. Zhang, J. Fang, X. Ai, B. Zhou, W. Yao, Q. Wu, and
J. Wen, “Partition-combine uncertainty set for robust unit
commitment,” IEEE Trans. Power Syst., vol. 35, no. 4,
pp. 3266–3269, 2020.

[10] Y. Chen, Q. Guo, H. Sun, Z. Li, W. Wu, and Z. Li, “A
distributionally robust optimization model for unit com-
mitment based on kullback–leibler divergence,” IEEE
Trans. Power Syst., vol. 33, no. 5, pp. 5147–5160, 2018.
[11] S. Wang, X. Xu, X. Kong, and Z. Yan, “Extended priority
list and discrete heuristic search for multi-objective unit
commitment,” Int. Trans. Electr. Energy Syst., vol. 28,
no. 2, p. e2486, 2018.

[12] T. Senjyu, T. Miyagi, A. Y. Saber, N. Urasaki, and
T. Funabashi, “Emerging solution of large-scale unit
commitment problem by stochastic priority list,” Electr.
Power Syst. Res., vol. 76, no. 5, pp. 283–292, 2006.
[13] M. Nemati, M. Braun, and S. Tenbohlen, “Optimization
of unit commitment and economic dispatch in microgrids
based on genetic algorithm and mixed integer linear
programming,” Appl. Energy, vol. 210, pp. 944–963,
2018.

[14] T. Ting, M. Rao, and C. Loo, “A novel approach for
unit commitment problem via an effective hybrid particle
swarm optimization,” IEEE Trans. Power Syst., vol. 21,
no. 1, pp. 411–418, 2006.

[15] D. N. Simopoulos, S. D. Kavatza, and C. D. Vournas,
“Unit commitment by an enhanced simulated annealing
algorithm,” IEEE Trans. Power Syst., vol. 21, no. 1, pp.
68–76, 2006.

[16] W. L. Snyder, H. D. Powell, and J. C. Rayburn, “Dy-
namic programming approach to unit commitment,”
IEEE Trans. Power Syst., vol. 2, no. 2, pp. 339–348,
1987.

[17] Q. Gao, Z. Yang, W. Yin, W. Li, and J. Yu, “Internally
induced branch-and-cut acceleration for unit commitment
based on improvement of upper bound,” IEEE Trans.
Power Syst., 2022.

[18] W. Ongsakul and N. Petcharaks, “Unit commitment by
enhanced adaptive lagrangian relaxation,” IEEE Trans.
Power Syst., vol. 19, no. 1, pp. 620–628, 2004.

[19] C. Liu, M. Shahidehpour, and L. Wu, “Extended benders
decomposition for two-stage scuc,” IEEE Trans. Power
Syst., vol. 25, no. 2, pp. 1192–1194, 2010.

[20] L. Yang, J. Jian, Z. Dong, and C. Tang, “Multi-cuts
outer approximation method for unit commitment,” IEEE

Trans. Power Syst., vol. 32, no. 2, pp. 1587–1588, 2016.
[21] H. Wu and M. Shahidehpour, “Stochastic scuc solution
with variable wind energy using constrained ordinal
optimization,” IEEE Trans. Sustain. Energy, vol. 5, no. 2,
pp. 379–388, 2013.

[22] Y. An and B. Zeng, “Exploring the modeling capacity
of two-stage robust optimization: Variants of robust unit
commitment model,” IEEE Trans. Power Syst., vol. 30,
no. 1, pp. 109–122, 2014.

[23] G. Morales-Espa˜na, J. M. Latorre, and A. Ramos, “Tight
and compact milp formulation for the thermal unit com-
mitment problem,” IEEE Trans. Power Syst., vol. 28,
no. 4, pp. 4897–4908, 2013.

[24] S. Atakan, G. Lulli, and S. Sen, “A state transition mip
formulation for the unit commitment problem,” IEEE
Trans. Power Syst., vol. 33, no. 1, pp. 736–748, 2017.

[25] B. Yan, P. B. Luh, T. Zheng, D. A. Schiro, M. A.
Bragin, F. Zhao, J. Zhao, and I. Lelic, “A systematic
formulation tightening approach for unit commitment
problems,” IEEE Trans. Power Syst., vol. 35, no. 1, pp.
782–794, 2019.

[26] X. Sun, P. B. Luh, M. A. Bragin, Y. Chen, J. Wan,
and F. Wang, “A novel decomposition and coordination
approach for large day-ahead unit commitment with
combined cycle units,” IEEE Trans. Power Syst., vol. 33,
no. 5, pp. 5297–5308, 2018.

[27] J. Wu, P. Luh, Y. Chen, M. Bragin, and B. Yan, “A novel
optimization approach for sub-hourly unit commitment
with large numbers of units and virtual transactions,”
IEEE Trans. Power Syst., 2021.

[28] N. Nikmehr, P. Zhang, and M. Bragin, “Quantum dis-
tributed unit commitment,” IEEE Trans. Power Syst.,
2022.

[29] K. Kim, A. Botterud, and F. Qiu, “Temporal decompo-
sition for improved unit commitment in power system
production cost modeling,” IEEE Trans. Power Syst.,
vol. 33, no. 5, pp. 5276–5287, 2018.

[30] A. S. Xavier, F. Qiu, and S. Ahmed, “Learning to
solve large-scale security-constrained unit commitment
problems,” INFORMS J Comput, vol. 33, no. 2, pp. 739–
756, 2021.

[31] Y. Yang and L. Wu, “Machine learning approaches to
the unit commitment problem: Current trends, emerging
challenges, and new strategies,” Electr. J., vol. 34, no. 1,
p. 106889, 2021.

[32] V. Nair, S. Bartunov, F. Gimeno, I. von Glehn, P. Li-
chocki, I. Lobov, B. O’Donoghue, N. Sonnerat, C. Tjan-
draatmadja, P. Wang et al., “Solving mixed inte-
ger programs using neural networks,” arXiv preprint
arXiv:2012.13349, 2020.

[33] W. Wang, N. Yu, Y. Gao, and J. Shi, “Safe off-policy deep
reinforcement learning algorithm for volt-var control in
power distribution systems,” IEEE Trans. Smart Grid,
vol. 11, no. 4, pp. 3008–3018, 2020.

[34] E. Jasmin and I. A. TP, “Reinforcement learning solution
for unit commitment problem through pursuit method,”

in Int. Conf. Adv. Commun. Control Comput. Technol.
IEEE, 2009, pp. 324–327.

[35] G. Dalal and S. Mannor, “Reinforcement learning for
the unit commitment problem,” in 2015 IEEE Eindhoven
PowerTech.

IEEE, 2015, pp. 1–6.
[36] N. K. Navin and R. Sharma, “A fuzzy reinforcement
learning approach to thermal unit commitment problem,”
Neural. Comput. Appl, vol. 31, no. 3, pp. 737–750, 2019.
[37] F. Li, J. Qin, and W. X. Zheng, “Distributed Q-learning-
based online optimization algorithm for unit commitment
and dispatch in smart grid,” IEEE Trans. Cybern., vol. 50,
no. 9, pp. 4146–4156, 2019.

[38] P. de Mars and A. O’Sullivan, “Applying reinforcement
learning and tree search to the unit commitment prob-
lem,” Appl. Energy, vol. 302, p. 117519, 2021.

[39] A. J. Wood, B. F. Wollenberg, and G. B. Shebl´e, Power
John Wiley & Sons,

generation, operation, and control.
2013.

[40] R. S. Sutton and A. G. Barto, Reinforcement learning:

An introduction. MIT Press, 2018.

[41] V. Mnih, K. Kavukcuoglu, D. Silver, A. Graves,
I. Antonoglou, D. Wierstra, and M. Riedmiller, “Playing
atari with deep reinforcement learning,” in NIPS Deep
Learning Workshop, 2013.

[42] V. Mnih, A. P. Badia, M. Mirza, A. Graves, T. Lillicrap,
T. Harley, D. Silver, and K. Kavukcuoglu, “Asynchronous
learning,” in ICML.
methods for deep reinforcement
PMLR, 2016, pp. 1928–1937.

[43] C. Watkins, “Learning from delayed rewards,” PhD the-
sis, King’s College, University of Cambridge, 1989.
[44] J. Qin, N. Yu, and Y. Gao, “Solving unit commitment
problems with multi-step deep reinforcement learning,”
in 2021 2019 IEEE Int. Conf. Commun.
IEEE, 2021,
pp. 140–145.

[45] Y. Gao, W. Wang, and N. Yu, “Consensus multi-agent
reinforcement
in power
distribution networks,” IEEE Trans. Smart Grid, vol. 12,
no. 4, pp. 3594–3604, 2021.

learning for volt-var control

[46] R. D. Zimmerman, C. E. Murillo-S´anchez, and R. J.
Thomas, “Matpower: Steady-state operations, planning,
and analysis tools for power systems research and ed-
ucation,” IEEE Trans. Power Syst., vol. 26, no. 1, pp.
12–19, 2010.

[47] M. Carri´on and J. M. Arroyo, “A computationally ef-
ﬁcient mixed-integer linear formulation for the thermal
unit commitment problem,” IEEE Trans. Power Syst.,
vol. 21, no. 3, pp. 1371–1378, 2006.

[48] CASIO, “California ISO Demand Forecast website,”
http://oasis.caiso.com/

[Online]. Available:

2022.
mrioasis/logon.do
[49] Gurobi Optimization,

Reference Manual,”
https://www.gurobi.com

LLC,
2022.

“Gurobi Optimizer
[Online]. Available:

