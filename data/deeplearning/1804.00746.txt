8
1
0
2

t
c
O
2

]
L
P
.
s
c
[

4
v
6
4
7
0
0
.
4
0
8
1
:
v
i
X
r
a

The Simple Essence of Automatic Diﬀerentiation

Extended version ∗

Conal Elliott

Target

conal@conal.net

March, 2018

Abstract

Automatic diﬀerentiation (AD) in reverse mode (RAD) is a central component of deep learning and
other uses of large-scale optimization. Commonly used RAD algorithms such as backpropagation, however,
are complex and stateful, hindering deep understanding, improvement, and parallel execution. This paper
develops a simple, generalized AD algorithm calculated from a simple, natural speciﬁcation. The general
algorithm is then specialized by varying the representation of derivatives. In particular, applying well-known
constructions to a naive representation yields two RAD algorithms that are far simpler than previously known.
In contrast to commonly used RAD implementations, the algorithms deﬁned here involve no graphs, tapes,
variables, partial derivatives, or mutation. They are inherently parallel-friendly, correct by construction, and
usable directly from an existing programming language with no need for new data types or programming
style, thanks to use of an AD-agnostic compiler plugin.

1

Introduction

Accurate, eﬃcient, and reliable computation of derivatives has become increasingly important over the last several
years, thanks in large part to the successful use of backpropagation in machine learning, including multi-layer
neural networks, also known as “deep learning” [Lecun et al., 2015; Goodfellow et al., 2016]. Backpropagation
is a specialization and independent invention of the reverse mode of automatic diﬀerentiation (AD) and is
used to tune a parametric model to closely match observed data, using gradient descent (or stochastic gradient
descent). Machine learning and other gradient-based optimization problems typically rely on derivatives of
functions with very high dimensional domains and a scalar codomain—exactly the conditions under which reverse-
mode AD is much more eﬃcient than forward-mode AD (by a factor proportional to the domain dimension).
Unfortunately, while forward-mode AD (FAD) is easily understood and implemented, reverse-mode AD (RAD)
and backpropagation have had much more complicated explanations and implementations, involving mutation,
graph construction and traversal, and “tapes” (sequences of reiﬁed, interpretable assignments, also called “traces”
or “Wengert lists”). Mutation, while motivated by eﬃciency concerns, makes parallel execution diﬃcult and
so undermines eﬃciency as well. Construction and interpretation (or compilation) of graphs and tapes also
add execution overhead. The importance of RAD makes its current complicated and bulky implementations
especially problematic. The increasingly large machine learning (and other optimization) problems being solved
with RAD (usually via backpropagation) suggest the need to ﬁnd more streamlined, eﬃcient implementations,
especially with the massive hardware parallelism now readily and inexpensively available in the form of graphics
processors (GPUs) and FPGAs.

Another diﬃculty in the practical application of AD in machine learning (ML) comes from the nature of many
currently popular ML frameworks, including Caﬀe [Jia et al., 2014], TensorFlow [Abadi et al., 2016], and Keras
[Chollet, 2016]. These frameworks are designed around the notion of a “graph” (or “network”) of interconnected
nodes, each of which represents a mathematical operation—a sort of data ﬂow graph. Application programs

∗The appendices of this extended version include proofs omitted in the conference article [Elliott, 2018].

1

 
 
 
 
 
 
2

Conal Elliott

construct these graphs explicitly, creating nodes and connecting them to other nodes. After construction, the
graphs must then be processed into a representation that is more eﬃcient to train and to evaluate. These graphs
are essentially mathematical expressions with sharing, hence directed acyclic graphs (DAGs). This paradigm of
graph construction, compilation, and execution bears a striking resemblance to what programmers and compilers
do all the time:

• Programs are written by a human.

• The compiler or interpreter front-end parses the program into a DAG representation.

• The compiler back-end transforms the DAGs into a form eﬃcient for execution.

• A human runs the result of compilation.

When using a typical ML framework, programmers experience this sequence of steps at two levels: working
with their code and with the graphs that their code generates. Both levels have notions of operations, variables,
information ﬂow, values, types, and parametrization. Both have execution models that must be understood.

A much simpler and cleaner foundation for ML would be to have just the programming language, omitting
the graphs/networks altogether. Since ML is about (mathematical) functions, one would want to choose a
programming language that supports functions well, i.e., a functional language, or at least a language with strong
functional features. One might call this alternative “diﬀerentiable functional programming”. In this paradigm,
programmers directly deﬁne their functions of interest, using the standard tools of functional programming, with
the addition of a diﬀerentiation operator (a typed higher-order function, though partial since not all computable
functions are diﬀerentiable). Assuming a purely functional language or language subset (with simple and precise
mathematical denotation), the meaning of diﬀerentiation is exactly as deﬁned in traditional calculus.

How can we realize this vision of diﬀerentiable functional programming? One way is to create new languages,
but doing so requires enormous eﬀort to deﬁne and implement eﬃciently, and perhaps still more eﬀort to
evangelize. Alternatively, we might choose a suitable purely functional language like Haskell and then add
diﬀerentiation. The present paper embodies the latter choice, augmenting the popular Haskell compiler GHC
with a plugin that converts standard Haskell code into categorical form to be instantiated in any of a variety of
categories, including diﬀerentiable functions [Elliott, 2017].
This paper makes the following speciﬁc contributions:

• Beginning with a simple category of derivative-augmented functions, specify AD simply and precisely by
requiring this augmentation (relative to regular functions) to be homomorphic with respect to a collection
of standard categorical abstractions and primitive mathematical operations.

• Calculate a correct-by-construction AD implementation from the homomorphic speciﬁcation.

• Generalizing AD by replacing linear maps (general derivative values) with an arbitrary cartesian category
[Elliott, 2017], deﬁne several AD variations, all stemming from diﬀerent representations of linear maps:
functions (satisfying linearity), “generalized matrices” (composed representable functors), continuation-based
transformations of any linear map representation, and dualized versions of any linear map representation.
The latter two variations yield correct-by-construction implementations of reverse-mode AD that are
much simpler than previously known and are composed from generally useful components. The choice
of dualized linear functions for gradient computations is particularly compelling in simplicity. It also
appears to be quite eﬃcient—requiring no matrix-level representations or computations—and is suitable
for gradient-based optimization, e.g., for machine learning. In contrast to conventional reverse-mode
AD algorithms, all algorithms in this paper are free of mutation and hence naturally parallel. A similar
construction yields forward-mode AD.

2 What’s a Derivative?

Since automatic diﬀerentiation (AD) has to do with computing derivatives, let’s begin by considering what
derivatives are. If your introductory calculus class was like mine, you learned that the derivative f (cid:48) x of a
function f :: R → R at a point x (in the domain of f ) is a number, deﬁned as follows:

f (cid:48) x = lim
ε→0

f (x + ε) − f x
ε

(1)

The Simple Essence of Automatic Diﬀerentiation

3

That is, f (cid:48) x tells us how fast f is scaling input changes at x .

How well does this deﬁnition hold up beyond functions of type R → R? It will do ﬁne with complex numbers
(C → C), where division is also deﬁned. Extending to R → Rn also works if we interpret the ratio as dividing a
vector (in Rn) by a scalar in the usual way. When we extend to Rm → Rn (or even Rm → R), however, this
deﬁnition no longer makes sense, as it would rely on dividing by a vector ε :: Rm.

This diﬃculty of diﬀerentiation with non-scalar domains is usually addressed with the notion of “partial
derivatives” with respect to the m scalar components of the domain Rm, often written “∂f /∂xj” for j ∈ {1, . . . , m}.
When the codomain Rn is also non-scalar (i.e., n > 1), we have a matrix J (the Jacobian), with Jij = ∂fi/∂xj
for i ∈ {1, . . . , n}, where each fi projects out the i th scalar value from the result of f .

So far, we’ve seen that the derivative of a function could be a single number (for R → R), or a vector (for
R → Rn), or a matrix (for Rm → Rn). Moreover, each of these situations has an accompanying chain rule, which
says how to diﬀerentiate the composition of two functions. Where the scalar chain rule involves multiplying two
scalar derivatives, the vector chain rule involves “multiplying” two matrices A and B (the Jacobians), deﬁned as
follows:

(A · B)ij =

Aik · Bkj

m
(cid:88)

k=1

Since one can think of scalars as a special case of vectors, and scalar multiplication as a special case of
matrix multiplication, perhaps we’ve reached the needed generality. When we turn our attention to higher
derivatives (which are derivatives of derivatives), however, the situation gets more complicated, and we need yet
higher-dimensional representations, with correspondingly more complex chain rules.

Fortunately, there is a single, elegant generalization of diﬀerentiation with a correspondingly simple chain

rule. First, reword Deﬁnition 1 above as follows:

Equivalently,

lim
ε→0

f (x + ε) − f x
ε

− f (cid:48) x = 0

f (x + ε) − (f x + ε · f (cid:48) x )
ε
Notice that f (cid:48) x is used to linearly transform ε. Next, generalize this condition to say that f (cid:48) x is a linear map
such that

lim
ε→0

= 0

(cid:107)f (x + ε) − (f x + f (cid:48) x ε)(cid:107)
(cid:107)ε(cid:107)

lim
ε→0

= 0.

In other words, f (cid:48) x is a local linear approximation of f at x . When an f (cid:48) x satisfying this condition exists, it is
indeed unique [Spivak, 1965, chapter 2].

The derivative of a function f :: a → b at some value in a is thus not a number, vector, matrix, or higher-
dimensional variant, but rather a linear map (also called “linear transformation”) from a to b, which we will
write as “a (cid:40) b”. The numbers, vectors, matrices, etc mentioned above are all diﬀerent representations of linear
maps; and the various forms of “multiplication” appearing in their associated chain rules are all implementations
of linear map composition for those representations. Here, a and b must be vector spaces that share a common
underlying ﬁeld. Written as a Haskell-style type signature (but omitting vector space constraints),

D :: (a → b) → (a → (a (cid:40) b))

From the type of D, it follows that diﬀerentiating twice has the following type:1

D2 = D ◦ D :: (a → b) → (a → (a (cid:40) a (cid:40) b))

The type a (cid:40) a (cid:40) b is a linear map that yields a linear map, which is the curried form of a bilinear map.
Likewise, diﬀerentiating k times yields a k-linear map curried k − 1 times. For instance, the Hessian matrix H
corresponds to the second derivative of a function f :: Rm → R, having m rows and m columns (and satisfying
the symmetry condition Hi,j ≡ Hj,i).

1As with “→”, we will take “(cid:40)” to associate rightward, so u (cid:40) v (cid:40) w is equivalent to u (cid:40) (v (cid:40) w ).

4

Conal Elliott

3 Rules for Diﬀerentiation

3.1 Sequential Composition

With the shift to linear maps, there is one general chain rule, having a lovely form, namely that the derivative of
a composition is a composition of the derivatives [Spivak, 1965, Theorem 2-2]:

Theorem 1 (compose/“chain” rule)

D (g ◦ f ) a = D g (f a) ◦ D f a

If f :: a → b and g :: b → c, then D f a :: a (cid:40) b, and D g (f a) :: b (cid:40) c, so both sides of this equation have type
a (cid:40) c.2

Strictly speaking, Theorem 1 is not a compositional recipe for diﬀerentiating sequential compositions, i.e., it
is not the case D (g ◦ f ) can be constructed solely from D g and D f . Instead, it also needs f itself. Fortunately,
there is a simple way to restore compositionality. Instead of constructing just the derivative of a function f ,
suppose we augment f with its derivative:

+ :: (a → b) → ((a → b) × (a → (a (cid:40) b)))
+ f = (f , D f )

D0
D0

-- ﬁrst try

As desired, this altered speciﬁcation is compositional:

D0

+ (g ◦ f )
= (g ◦ f , D (g ◦ f ))
= (g ◦ f , λa → D g (f a) ◦ D f a)

+
-- deﬁnition of D0
-- Theorem 1

Note that D0

+ f , which is to say from g, D g, f ,
+ (g ◦ f ) is assembled entirely from components of D0
+ (g ◦ f ) a both involve f a.
and D f . Writing out g ◦ f as λa → g (f a) underscores that the two parts of D0
Computing these parts independently thus requires redundant work. Moreover, the chain rule itself requires
applying a function and its derivative (namely f and D f ) to the same a. Since the chain rule gets applied
recursively to nested compositions, this redundant work multiplies greatly, resulting in an impractically expensive
algorithm.

+ g and D0

Fortunately, this eﬃciency problem is easily ﬁxed. Instead of pairing f and D f , combine them:3

D+ :: (a → b) → (a → b × (a (cid:40) b))
D+ f a = (f a, D f a)

-- better!

Combining f and D f into a single function in this way enables us to eliminate the redundant computation of
f a in D+ (g ◦ f ) a, as follows:

Corollary 1.1 (Proved in Appendix C.1) D+ is (eﬃciently) compositional with respect to (◦). Speciﬁcally,

D+ (g ◦ f ) a = let {(b, f (cid:48)) = D+ f a; (c, g (cid:48)) = D+ g b } in (c, g (cid:48) ◦ f (cid:48))

3.2 Parallel Composition

The chain rule, telling how to diﬀerentiate sequential compositions, gets a lot of attention in calculus classes and
in automatic and symbolic diﬀerentiation. There are other important ways to combine functions, however, and
examining them yields additional helpful tools. Another operation (pronounced “cross”) combines two functions
in parallel [Gibbons, 2002]:4

(×) :: (a → c) → (b → d ) → (a × b → c × d )
f × g = λ(a, b) → (f a, g b)

While the derivative of a sequential composition is a sequential composition of derivatives, the derivative of a

parallel composition is a parallel composition of the derivatives [Spivak, 1965, variant of Theorem 2-3 (3)]:

2I adopt the common, if sometimes confusing, Haskell convention of sharing names between type and value variables, e.g., with
a (a value variable) having type a (a type variable). Haskell value and type variable names live in diﬀerent name spaces and are
distinguished by syntactic context.

3The precedence of “×” is tighter than that of “→” and “(cid:40)”, so a → b × (a (cid:40) b) is equivalent to a → (b × (a (cid:40) b)).
4By “parallel”, I mean without data dependencies. Operationally, the two functions can be applied simultaneously or not.

The Simple Essence of Automatic Diﬀerentiation

5

Theorem 2 (cross rule)

D (f × g) (a, b) = D f a × D g b

If f :: a → c and g :: b → d , then D f a :: a (cid:40) c and D g b :: b (cid:40) d , so both sides of this equation have type
a × b (cid:40) c × d .

Theorem 2 gives us what we need to construct D+ (f × g) compositionally:

Corollary 2.1 (Proved in Appendix C.2) D+ is compositional with respect to (×). Speciﬁcally,

D+ (f × g) (a, b) = let {(c, f (cid:48)) = D+ f a; (d , g (cid:48)) = D+ g b } in ((c, d ), f (cid:48) × g (cid:48))

An important point left implicit in the discussion above is that sequential and parallel composition preserve
linearity. This property is what makes it meaningful to use these forms to combine derivatives, i.e., linear maps,
as we’ve done above.

3.3 Linear Functions

A function f is said to be linear when it distributes over (preserves the structure of) vector addition and scalar
multiplication, i.e.,

f (a + a (cid:48)) = f a + f a (cid:48)
f (s · a) = s · f a

In addition to Theorems 1 and 2, we will want one more broadly useful rule, namely that the derivative of

every linear function is itself, everywhere [Spivak, 1965, Theorem 2-3 (2)]:

Theorem 3 (linear rule) For all linear functions f , D f a = f .

This statement may sound surprising at ﬁrst, but less so when we recall that the D f a is a local linear
approximation of f at a, so we’re simply saying that linear functions are their own perfect linear approximations.
For example, consider the function id = λa → a. Theorem 3 says that D id a = id . When expressed via
typical representations of linear maps, this property may be expressed as saying that D id a is the number
one or is an identity matrix (with ones on the diagonal and zeros elsewhere). Likewise, consider the (linear)
function fst (a, b) = a, for which Theorem 3 says D fst (a, b) = fst. This property, when expressed via typical
representations of linear maps, would appear as saying that D fst a comprises the partial derivatives one and
zero if a, b :: R. More generally, if a :: Rm and b :: Rn, then the Jacobian matrix representation has shape
m × (m + n) (i.e., m rows and m + n columns) and is formed by the horizontal juxtaposition of an m × m
identity matrix on the left with an m × n zero matrix on the right. This m × (m + n) matrix, however, represents
fst :: Rm × Rn (cid:40) Rm. Note how much simpler it is to say D fst (a, b) = fst, and with no loss of precision!

Given Theorem 3, we can construct D+ f for all linear f :

Corollary 3.1 For all linear functions f , D+ f = λa → (f a, f ). (Proof: immediate from the D+ deﬁnition and
Theorem 3.)

4 Putting the Pieces Together

The deﬁnition of D+ on page 4 is a precise speciﬁcation; but it is not an implementation, since D itself is not
computable [Pour-El and Richards, 1978, 1983]. Corollaries 1.1 through 3.1 provide insight into the compositional
nature of D+ in exactly the form we can now assemble into a correct-by-construction implementation.

Although diﬀerentiation is not computable when given just an arbitrary computable function, we can instead
build up diﬀerentiable functions compositionally, using exactly the forms introduced above, (namely (◦), (×)
and linear functions), together with various non-linear primitives having known derivatives. Computations
expressed in this vocabulary are diﬀerentiable by construction thanks to Corollaries 1.1 through 3.1. The building
blocks above are not just a random assortment, but rather a fundamental language of mathematics, logic, and
computation, known as category theory [Mac Lane, 1998; Lawvere and Schanuel, 2009; Awodey, 2006]. While it
would be unpleasant to program directly in such an austere language, its foundational nature enables instead an
automatic conversion from programs written in more conventional functional languages [Lambek, 1980, 1986;
Elliott, 2017].

6

4.1 Categories

Conal Elliott

The central notion in category theory is that of a category, comprising objects (generalizing sets or types) and
morphisms (generalizing functions between sets or types). For the purpose of this paper, we will take objects
to be types in our program, and morphisms to be enhanced functions. We will introduce morphisms using
Haskell-style type signatures, such as “f :: a (cid:59) b ∈ U”, where “(cid:59)” refers to the morphisms for a category U,
with a and b being the domain and codomain objects/types for f . In most cases, we will omit the “∈ U”, where
choice of category is (hopefully) clear from context. Each category U has a distinguished identity morphism
id :: a (cid:59) a ∈ U for every object/type a in the category. For any two morphisms f :: a (cid:59) b ∈ U and g :: b (cid:59) c ∈ U
(note same category and matching types/objects b), there is also the composition g ◦ f :: a (cid:59) c ∈ U. The category
laws state that (a) id is the left and right identity for composition, and (b) composition is associative. You are
probably already familiar with at least one example of a category, namely functions, in which id and (◦) are the
identity function and function composition.

Although Haskell’s type system cannot capture the category laws explicitly, we can express the two required

operations as a Haskell type class, along with a familiar instance:

class Category k where

id :: a ‘k ‘ a
(◦) :: (b ‘k ‘ c) → (a ‘k ‘ b) → (a ‘k ‘ c)

instance Category (→) where

id = λa → a
g ◦ f = λa → g (f a)

Another example is linear functions, which we’ve written “a (cid:40) b” above. Still another example is diﬀerentiable
functions5, which we can see by noting three facts:

• The identity function is diﬀerentiable, as witnessed by Theorem 3 and the linearity of id .

• The composition of diﬀerentiable functions is diﬀerentiable, as Theorem 1 attests.

• The category laws (identity and associativity) hold, because diﬀerentiable functions form a subset of all

functions.

Each category forms its own world, with morphisms relating objects within that category. To bridge between
these worlds, there are functors, which connect a category U to a (possibly diﬀerent) category V. Such a functor
F maps objects in U to objects in V, and morphisms in U to morphisms in V. If f :: a (cid:59) b ∈ U is a morphism,
then a functor F from U to V transforms f ∈ U to a morphism F f :: F a (cid:57)(cid:57)(cid:75) F b ∈ V, i.e., the domain and
codomain of the transformed morphism F f ∈ V must be the transformed versions of the domain and codomain
of f ∈ U. In this paper, the categories use types as objects, while the functors map these types to themselves.6
The functor must also preserve “categorical” structure:7

F id = id

F (g ◦ f ) = F g ◦ F f

Crucially to the topic of this paper, Corollaries 3.1 and 1.1 say more than that diﬀerentiable functions form
a category. They also point us to a new, easily implemented category, for which D+ is in fact a functor. This
new category is simply the representation that D+ produces: a → b × (a (cid:40) b), considered as having domain a
and codomain b. The functor nature of D+ will be exactly what we need to in order to program in a familiar
and direct way in a pleasant functional language such as Haskell and have a compiler convert to diﬀerentiable
functions automatically.

To make the new category more explicit, package the result type of D+ in a new data type:

newtype D a b = D (a → b × (a (cid:40) b))

Then adapt D+ to use this new data type by simply applying the D constructor to the result of D+:

ˆD :: (a → b) → D a b
ˆD f = D (D+ f )

5There are many examples of categories besides restricted forms of functions, including relations, logics, partial orders, and even

matrices.

6In contrast, Haskell’s functors stay within the same category and do change types.
7Making the categories explicit, F (id ∈ U ) = (id ∈ V) and F (g ◦ f ∈ U ) = (F g ◦ F f ∈ V).

The Simple Essence of Automatic Diﬀerentiation

7

Our goal is to discover a Category instance for D such that ˆD is a functor. This goal is essentially an algebra
problem, and the desired Category instance is a solution to that problem. Saying that ˆD is a functor is equivalent
to the following two conditions for all suitably typed functions f and g:8

id = ˆD id

ˆD g ◦ ˆD f = ˆD (g ◦ f )

Equivalently, by the deﬁnition of ˆD,

id = D (D+ id )

D (D+ g) ◦ D (D+ f ) = D (D+ (g ◦ f ))

Now recall the following results from Corollaries 3.1 and 1.1:

D+ id = λa → (id a, id )

D+ (g ◦ f ) = λa → let {(b, f (cid:48)) = D+ f a; (c, g (cid:48)) = D+ g b } in (c, g (cid:48) ◦ f (cid:48))

Then use these two facts to rewrite the right-hand sides of the functor speciﬁcation for ˆD:

id = D (λa → (a, id ))

D (D+ g) ◦ D (D+ f ) = D (λa → let {(b, f (cid:48)) = D+ f a; (c, g (cid:48)) = D+ g b } in (c, g (cid:48) ◦ f (cid:48)))

The id equation is trivially solvable by deﬁning id = D (λa → (a, id )). To solve the (◦) equation, generalize it
to a stronger condition:9

D g ◦ D f = D (λa → let {(b, f (cid:48)) = f a; (c, g (cid:48)) = g b } in (c, g (cid:48) ◦ f (cid:48)))

The solution of this stronger condition is immediate, leading to the following instance as a suﬃcient condition
for ˆD being a functor:

linearD :: (a → b) → D a b
linearD f = D (λa → (f a, f ))

instance Category D where

id = linearD id
D g ◦ D f = D (λa → let {(b, f (cid:48)) = f a; (c, g (cid:48)) = g b } in (c, g (cid:48) ◦ f (cid:48)))

Factoring out linearD will also tidy up treatment of other linear functions.

Before we get too pleased with this deﬁnition, let’s remember that for D to be a category requires more than
having deﬁnitions for id and (◦). These deﬁnitions must also satisfy the identity and composition laws. How
might we go about proving that they do? Perhaps the most obvious route is take those laws, substitute our
deﬁnitions of id and (◦), and reason equationally toward the desired conclusion. For instance, let’s prove that
id ◦ D f = D f for all D f :: D a b:10

id ◦ D f

= D (λb → (b, id )) ◦ D f
= D (λa → let {(b, f (cid:48)) = f a; (c, g (cid:48)) = (b, id )} in (c, g (cid:48) ◦ f (cid:48)))
= D (λa → let {(b, f (cid:48)) = f a } in (b, id ◦ f (cid:48)))
= D (λa → let {(b, f (cid:48)) = f a } in (b, f (cid:48)))
= D (λa → f a)
= D f

-- deﬁnition of id for D
-- deﬁnition of (◦) for D
-- substitute b for c and id for g (cid:48)
-- id ◦ f (cid:48) = f (cid:48) (category law)
-- replace (b, f (cid:48)) by its deﬁnition
-- η-reduction

We can prove the other required properties similarly. Fortunately, there is a way to bypass the need for
these painstaking proofs, and instead rely only on our original speciﬁcation for this Category instance, namely

8The id and (◦) on the left-hand sides are for D, while the ones on the right are for (→).
9The new f is the old D+ f and so has changed type from a → b to a → b × (a (cid:40) b). Likewise for g.
10Note that every morphism in D has the form D f for some f , so it suﬃces to consider this form.

8

Conal Elliott

that D+ is a functor. To buy this proof convenience, we have to make one concession, namely that we consider
only morphisms in D that arise from ˆD, i.e., only ˆf :: D a b such that ˆf = ˆD f for some f :: a → b. We can
ensure that indeed only such ˆf do arise by making D a b an abstract type, i.e., hiding its data constructor . The
slightly more specialized requirement of our ﬁrst identity property is then id ◦ ˆD f = ˆD f for any f :: a → b,
which follows easily:

id ◦ ˆD f
= ˆD id ◦ ˆD f
= ˆD (id ◦ f )
= ˆD f

-- functor law for id (speciﬁcation of ˆD)
-- functor law for (◦)
-- category law

The other identity law is proved similarly. Associativity has a similar ﬂavor as well:

ˆD h ◦ ( ˆD g ◦ ˆD f )

= ˆD h ◦ ˆD (g ◦ f )
= ˆD (h ◦ (g ◦ f ))
= ˆD ((h ◦ g) ◦ f )
= ˆD (h ◦ g) ◦ ˆD f
= ( ˆD h ◦ ˆD g) ◦ ˆD f

-- functor law for (◦)
-- functor law for (◦)
-- category law
-- functor law for (◦)
-- functor law for (◦)

Note how mechanical these proofs are. Each uses only the functor laws plus the particular category law on
functions that corresponds to the one being proved for D. The proofs rely on nothing about the nature of D or
ˆD beyond the functor laws. The importance of this observation is that we never need to perform these proofs
when we specify category instances via a functor.

4.2 Monoidal Categories

Section 3.2 introduced parallel composition. This operation generalizes to play an important role in category
theory as part of the notion of a monoidal category:

class Category k ⇒ Monoidal k where

(×) :: (a ‘k ‘ c) → (b ‘k ‘ d ) → ((a × b) ‘k ‘ (c × d ))

instance Monoidal (→) where
f × g = λ(a, b) → (f a, g b)

More generally, a category k can be monoidal over constructions other than products, but cartesian products
(ordered pairs) suﬃce for this paper.

Two monoidal categories can be related by a monoidal functor, which is a functor that also preserves the
monoidal structure. That is, a monoidal functor F from monoidal category U to monoidal category V, besides
mapping objects and morphisms in U to counterparts in V while preserving the category structure (id and (◦)),
also preserves the monoidal structure:

F (f × g) = F f × F g

Just as Corollaries 1.1 and 3.1 were key to deriving a correct-by-construction Category instance from the
speciﬁcation that ˆD is a functor, Corollary 2.1 leads to a correct Monoidal instance from the speciﬁcation that
ˆD is a monoidal functor, as we’ll now see.

Let F be ˆD in the reversed form of the monoidal functor equation above, and expand ˆD to its deﬁnition as

D ◦ D+:

D (D+ f ) × D (D+ g) = D (D+ (f × g))

By Corollary 2.1,

D+ (f × g) = λ(a, b) → let {(c, f (cid:48)) = D+ f a; (d , g (cid:48)) = D+ g b } in ((c, d ), f (cid:48) × g (cid:48))

Now substitute the left-hand side of this equation into the right-hand side of the of the monoidal functor property
for ˆD, and strengthen the condition by generalizing from D+ f and D+ g:

The Simple Essence of Automatic Diﬀerentiation

9

D f × D g = D (λ(a, b) → let {(c, f (cid:48)) = f a; (d , g (cid:48)) = g b } in ((c, d ), f (cid:48) × g (cid:48)))

This strengthened form of the speciﬁcation can be converted directly to a suﬃcient deﬁnition:

instance Monoidal D where

D f × D g = D (λ(a, b) → let {(c, f (cid:48)) = f a; (d , g (cid:48)) = g b } in ((c, d ), f (cid:48) × g (cid:48)))

4.3 Cartesian Categories

The Monoidal abstraction provides a way to combine two functions but not separate them. It also gives no way
to duplicate or discard information. These additional abilities require another algebraic abstraction, namely that
of cartesian category, adding operations for projection and duplication:

class Monoidal k ⇒ Cartesian k where

instance Cartesian (→) where

exl
:: (a × b) ‘k ‘ a
exr :: (a × b) ‘k ‘ b
dup :: a ‘k ‘ (a × a)

exl = λ(a, b) → a
exr = λ(a, b) → b
dup = λa → (a, a)

Two cartesian categories can be related by a cartesian functor, which additionally preserves the cartesian
structure. That is, a cartesian functor F from cartesian category U to cartesian category V, besides mapping
objects and morphisms in U to counterparts in V while preserving the category and monoidal structure (id , (◦),
and (×)), also preserves the cartesian structure:

F exl = exl
F exr = exr
F dup = dup

Just as Corollaries 1.1 through 3.1 were key to deriving a correct-by-construction Category and Monoidal
instances from the speciﬁcation that ˆD is a functor and a monoidal functor respectively, Corollary 3.1 enables a
correct-by-construction Cartesian instance from the speciﬁcation that ˆD is a cartesian functor. Let F be ˆD in
the reversed forms of cartesian functor equations above, and expand ˆD to its deﬁnition as D ◦ D+:

exl = D (D+ exl )
exr = D (D+ exr )
dup = D (D+ dup)

Next, by Corollary 3.1, together with the linearity of exl , exr , and dup,

D+ exl = λp → (exl p, exl )
D+ exr = λp → (exr p, exr )
D+ dup = λa → (dup a, dup)

Now substitute the left-hand sides of these three properties into the right-hand sides of the of the cartesian
functor properties for ˆD, and recall the deﬁnition of linearD:

exl = linearD exl
exr = linearD exr
dup = linearD dup

This form of the speciﬁcation can be turned directly into a suﬃcient deﬁnition:

instance Cartesian D where

exl = linearD exl
exr = linearD exr
dup = linearD dup

10

Conal Elliott

newtype a →+ b = AddFun (a → b)

instance Category (→+) where
type Obj (→+) = Additive
id = AddFun id
AddFun g ◦ AddFun f = AddFun (g ◦ f )

instance Monoidal (→+) where

AddFun f × AddFun g = AddFun (f × g)

instance Cartesian (→+) where

exl = AddFun exl
exr = AddFun exr
dup = AddFun dup

instance Cocartesian (→+) where

inl = AddFun inlF
inr = AddFun inrF
jam = AddFun jamF

inlF :: Additive b ⇒ a → a × b
inrF :: Additive a ⇒ b → a × b
jamF :: Additive a ⇒ a × a → a

inlF = λa → (a, 0)
inrF = λb → (0, b)
jamF = λ(a, b) → a + b

Figure 1: Additive functions

4.4 Cocartesian Categories

Cartesian categories have a dual, known as cocartesian categories, in which each cartesian operation has a mirror
image with morphisms reversed (swapping domain and codomain) and coproducts replacing products. In general,
each category can have its own notion of coproduct, e.g., sum (disjoint union) types for the (→) category. In
this paper, however, coproducts will coincide with categorical products, i.e., we’ll be using biproduct categories
[Macedo and Oliveira, 2013]:

class Category k ⇒ Cocartesian k where

inl
:: a ‘k ‘ (a × b)
inr :: b ‘k ‘ (a × b)
jam :: (a × a) ‘k ‘ a

Unlike the other classes, there is no Cocartesian (→) instance, and fortunately we will not need such an instance
below. (There is an instance when using sums instead of cartesian products for coproducts.) Instead, we
can deﬁne a category (→+) of additive functions that will have a Cocartesian instance and that we can use to
represent derivatives, as shown in Figure 1. These instances rely on one more feature of the Category class not
yet mentioned, namely an associated constraint [Bolingbroke, 2011] Obj k . In the actual class deﬁnitions, Obj k
constrains the types involved in all categorical operations.

Unsurprisingly, there is a notion of cocartesian functor, saying that the cocartesian structure is preserved,

i.e.,

F inl = inl
F inr = inr
F jam = jam

4.5 Derived Operations

With dup, we can deﬁne an alternative to (×) that takes two morphisms sharing a domain:

((cid:77)) :: Cartesian k ⇒ (a ‘k ‘ c) → (a ‘k ‘ d ) → (a ‘k ‘ (c × d ))
f (cid:77) g = (f × g) ◦ dup

The ((cid:77)) operation is particularly useful for translating the λ-calculus to categorical form [Elliott, 2017, Section
3].

Dually, jam lets us deﬁne a second alternative to (×) for two morphisms sharing a codomain:

The Simple Essence of Automatic Diﬀerentiation

11

((cid:79)) :: Cocartesian k ⇒ (c ‘k ‘ a) → (d ‘k ‘ a) → ((c × d ) ‘k ‘ a)
f (cid:79) g = jam ◦ (f × g)

The ((cid:77)) and ((cid:79)) operations are invertible in uncurried form [Gibbons, 2002]:

:: Cartesian
fork
unfork :: Cartesian

k ⇒ (a ‘k ‘ c) × (a ‘k ‘ d ) → (a ‘k ‘ (c × d ))
k ⇒ (a ‘k ‘ (c × d )) → (a ‘k ‘ c) × (a ‘k ‘ d )

join
:: Cocartesian k ⇒ (c ‘k ‘ a) × (d ‘k ‘ a) → ((c × d ) ‘k ‘ a)
unjoin :: Cocartesian k ⇒ ((c × d ) ‘k ‘ a) → (c ‘k ‘ a) × (d ‘k ‘ a)

where

fork (f , g) = f (cid:77) g
unfork h = (exl ◦ h, exr ◦ h)

join (f , g) = f (cid:79) g
unjoin h = (h ◦ inl , h ◦ inr )

4.6 Numeric Operations

So far, the vocabulary we’ve considered comprises linear functions and combining forms ((◦) and (×)) that
preserve linearity. To make diﬀerentiation interesting, we’ll need some non-linear primitives as well. Let’s now
add these primitives, while continuing to derive correct implementations from simple, regular speciﬁcations
in terms of homomorphisms (structure-preserving transformations). We’ll deﬁne a collection of interfaces for
numeric operations, roughly imitating Haskell’s numeric type class hierarchy.

Haskell provides the following basic class:

class Num a where
negate :: a → a
(+), (∗) :: a → a → a
...

Although this class can accommodate many diﬀerent types of “numbers”, the class operations are all committed
to being functions. A more ﬂexible alternative allows operations to be non-functions:

class NumCat k a where

instance Num a ⇒ NumCat (→) a where

negateC :: a ‘k ‘ a
addC :: (a × a) ‘k ‘ a
mulC :: (a × a) ‘k ‘ a
...

negateC = negate
addC = uncurry (+)
mulC = uncurry (·)
...

Besides generalizing from (→) to k , we’ve also uncurried the operations, so as to demand less of supporting
categories k . There are similar classes for other operations, such as division, powers and roots, and transcendental
functions (sin, cos, exp etc). Note that the (→) instance uses the operations from the standard numeric classes
(Num etc).

Diﬀerentiation rules for these operations are part of basic diﬀerential calculus:11

D (negate u) = negate (D u)
D (u + v ) = D u + D v
D (u · v ) = u · D v + v · D u

This conventional form is unnecessarily complex, as each of these rules implicitly involves not just a numeric
operation, but also an application of the chain rule. This form is also imprecise about the nature of u and v .
If they are functions, then one needs to explain arithmetic on functions; and if they are not functions, then
diﬀerentiation of non-functions needs explanation.

A precise and simpler presentation is to remove the arguments and talk about diﬀerentiating the primitive
operations in isolation. We have the chain rule to account for context, so we do not need to involve it in every

11The conventional diﬀerentiation rules shown here treat derivatives as numbers rather than linear maps.

12

Conal Elliott

numeric operation. Since negation and (uncurried) addition are linear, we already know how to diﬀerentiate
them. Multiplication is a little more involved [Spivak, 1965, Theorem 2-3 (2)]:

D mulC (a, b) = λ(da, db) → da · b + a · db

Note the linearity of the right-hand side, so that the derivative of mulC at (a, b) for real values has the expected
type: R × R (cid:40) R.12 To make the linearity more apparent, and to prepare for variations later in this paper,
let’s now rephrase D mulC without using lambda directly. Just as Category, Monoidal , Cartesian, NumCat,
etc generalize operations beyond functions, it will also be handy to generalize scalar multiplication as well:

class Scalable k a where
scale :: a → (a ‘k ‘ a)

instance Num a ⇒ Scalable (→+) a where

scale a = AddFun (λda → a · da)

Since uncurried multiplication is bilinear, its partial application as scale a (for functions) is linear for all a. Now
we can rephrase the product rule in terms of more general, linear language, using the derived ((cid:79)) operation
deﬁned in Section 4.5:

D mulC (a, b) = scale b (cid:79) scale a

This product rule, along with the linearity of negation and uncurried addition, enables using the same style
of derivation as with operations from Category, Monoidal , and Cartesian above. As usual, specify the NumCat
instance for diﬀerentiable functions by saying that ˆD preserves (NumCat) structure, i.e., ˆD negateC = negateC ,
ˆD addC = addC , and ˆD mulC = mulC . Reasoning as before, we get another correct-by-construction instance
for diﬀerentiable functions:

instance NumCat D where
negateC = linearD negateC
addC = linearD addC
mulC = D (λ(a, b) → (a · b, scale b (cid:79) scale a))

Similar reasoning applies to other numeric operations, e.g.,

instance FloatingCat D where

sinC = D (λa → (sin a, scale (cos a)))
cosC = D (λa → (cos a, scale (−sin a)))
expC = D (λa → let e = exp a in (e, scale e))

...

In what follows, the scale operation will play a more important role than merely tidying deﬁnitions.

5 Examples

Let’s now look at some AD examples, to which we will return later in the paper:

sqr :: Num a ⇒ a → a
sqr a = a · a

magSqr :: Num a ⇒ a × a → a
magSqr (a, b) = sqr a + sqr b

cosSinProd :: Floating a ⇒ a × a → a × a
cosSinProd (x , y) = (cos z , sin z ) where z = x · y

A compiler plugin converts these deﬁnitions to categorical vocabulary [Elliott, 2017]:

sqr = mulC ◦ (id (cid:77) id )

12The derivative of uncurried multiplication generalizes to an arbitrary bilinear function f :: a × b → c [Spivak, 1965, Problem

2-12]: D f (a, b) = λ(da, db) → f (da, b) + f (a, db).

The Simple Essence of Automatic Diﬀerentiation

13

Figure 2: magSqr

Figure 3: cosSinProd

Figure 4: ˆD magSqr

Figure 5: ˆD cosSinProd

magSqr = addC ◦ (mulC ◦ (exl (cid:77) exl ) (cid:77) mulC ◦ (exr (cid:77) exr ))

cosSinProd = (cosC (cid:77) sinC ) ◦ mulC

To visualize computations before diﬀerentiation, we can interpret these categorical expressions in a category of
graphs [Elliott, 2017, Section 7], with the results rendered in Figures 2 and 3. To see the diﬀerentiable versions,
interpret these same expressions in the category of diﬀerentiable functions (D from Section 4.1), remove the D
constructors to reveal the function representation, convert these functions to categorical form as well, and ﬁnally
interpret the result in the graph category. The results are rendered in Figures 4 and 5. Some remarks:

• The derivatives are (linear) functions, as depicted in boxes.

• Work is shared between the function’s result (sometimes called the “primal”) and its derivative in Figure 5.

• The graphs shown here are used solely for visualizing functions before and after diﬀerentiation, playing no

role in the programming interface or in the implementation of diﬀerentiation.

6 Programming as Deﬁning and Solving Algebra Problems

Stepping back to consider what we’ve done, a general recipe emerges:

• Start with an expensive or even non-computable speciﬁcation (here involving diﬀerentiation).

• Build the desired result into the representation of a new data type (here as the combination of a function

and its derivative).

• Try to show that conversion from a simpler form (here regular functions) to the new data type—even if
not computable—is compositional with respect to a well-understood collection of algebraic abstractions
(here Category etc).

• If compositionality fails (as with D, unadorned diﬀerentiation, in Section 3.1), examine the failure to ﬁnd
an augmented speciﬁcation, iterating as needed until converging on a representation and corresponding
speciﬁcation that is compositional.

• Set up an algebra problem whose solution will be an instance of the well-understood algebraic abstraction
for the chosen representation. These algebra problems always have a particular stylized form, namely that
the operation being solved for is a homomorphism for the chosen abstractions (here including a category
homomorphism, also called a “functor”).

In    ×   ×   +  OutIn    ×  cos  sin   OutIn    ×   ×   ×   ×   +   Out  +   +   +  OutIn  In    ×   ×   ×  cos  sin   ×    Out  ×   +   Out negate In  14

Conal Elliott

• Solve the algebra problem by using the compositionality properties.

• Rest assured that the solution satisﬁes the required laws, at least when the new data type is kept abstract,

thanks to the homomorphic speciﬁcation.

The result of this recipe is not quite an implementation of our homomorphic speciﬁcation, which may after all be
non-computable. Rather, it gives a computable alternative that is nearly as useful: if the input to the speciﬁed
conversion is expressed in the vocabulary of the chosen algebraic abstraction, then a re-interpretation of that
vocabulary in the new data type is the result of the (possibly non-computable) speciﬁcation. Furthermore, if
we can automatically convert conventionally written functional programs into the chosen algebraic vocabulary
[Elliott, 2017], then those programs can be re-interpreted to compute the desired speciﬁcation.

7 Generalizing Automatic Diﬀerentiation

Corollaries 1.1 through 3.1 all have the same form: an operation on D (diﬀerentiable functions) is deﬁned
entirely via the same operation on ((cid:40)) (linear maps). Speciﬁcally, the sequential and parallel composition of
diﬀerentiable functions rely (respectively) on sequential and parallel composition of linear maps, and likewise for
each other operation. These corollaries follow closely from Theorems 1 through 3, which relate derivatives for
these operations to the corresponding operations on linear maps. These properties make for a pleasantly poetic
theory, but they also have a powerful, tangible beneﬁt, which is that we can replace linear maps by any of a
much broader variety of underlying categories to arrive at a greatly generalized notion of AD.

A few small changes to the non-generalized deﬁnitions derived in Section 4 result in the generalized AD

deﬁnitions shown in Figure 6:

• The new category takes as parameter a category k that replaces ((cid:40)) in D.

• The linearD function takes two arrows, previously identiﬁed.

• The functionality needed of the underlying category becomes explicit.

• The constraint Obj Dk

and Obj k (needed for all instances).

is deﬁned to be the conjunction of Additive (needed for the Cocartesian instance)

8 Matrices

As an aside, let’s consider matrices—the representation typically used in linear algebra—and especially the
property of rectangularity. There are three (non-exclusive) possibilities for a nonempty matrix W :

• width W = height W = 1;

• W is the horizontal juxtaposition of two matrices U and V with height W = height U = height V , and

width W = width U + width V ; or

• W is the vertical juxtaposition of two matrices U and V with width W = width U = width V , and

height W = height U + height V .

These three shape constraints establish and preserve rectangularity.

The vocabulary we have needed from generalized linear maps so far is exactly that of Category, Cartesian,

Cocartesian, and Scalable. Let’s now extract just three operations from this vocabulary:

scale :: a → (a ‘k ‘ a)
((cid:79))
((cid:77))

:: (a ‘k ‘ c) → (b ‘k ‘ c) → ((a × b) ‘k ‘ c)
:: (a ‘k ‘ c) → (a ‘k ‘ d ) → (a ‘k ‘ (c × d ))

These operations exactly correspond to the three possibilities above for a nonempty matrix W , with the width
and height constraints captured neatly by types. When matrices are used to represent linear maps, the domain
and codomain types for the corresponding linear map are determined by the width and height of the matrix,
respectively (assuming the convention of matrix on the left multiplied by a column vector on the right), together
with the type of the matrix elements.

The Simple Essence of Automatic Diﬀerentiation

15

newtype Dk a b = D (a → b × (a ‘k ‘ b))

linearD :: (a → b) → (a ‘k ‘ b) → Dk a b
linearD f f (cid:48) = D (λa → (f a, f (cid:48)))

instance Category k ⇒ Category Dk where

= Additive ∧ Obj k

type Obj Dk
id = linearD id id
D g ◦ D f = D (λa → let {(b, f (cid:48)) = f a; (c, g (cid:48)) = g b } in (c, g (cid:48) ◦ f (cid:48)))

instance Monoidal k ⇒ Monoidal Dk where

D f × D g = D (λ(a, b) → let {(c, f (cid:48)) = f a; (d , g (cid:48)) = g b } in ((c, d ), f (cid:48) × g (cid:48)))

instance Cartesian k ⇒ Cartesian Dk where

exl = linearD exl exl
exr = linearD exr exr
dup = linearD dup dup

instance Cocartesian k ⇒ Cocartesian Dk where

inl = linearD inlF inl
inr = linearD inrF inr
jam = linearD jamF jam

instance Scalable k s ⇒ NumCat Dk s where

negateC = linearD negateC negateC
addC
mulC = D (λ(a, b) → (a · b, scale b (cid:79) scale a))

= linearD addC

addC

Figure 6: Generalized automatic diﬀerentiation

16

Conal Elliott

9 Extracting a Data Representation

The generalized form of AD in Section 7 allows for diﬀerent representations of linear maps (as well as alternatives
to linear maps). One simple choice is to use functions, as in Figures 4 and 5. Although this choice is simple and
reliable, sometimes we need a data representation. For instance,

• Gradient-based optimization (including in machine learning) works by searching for local minima in the
domain of a diﬀerentiable function f :: a → s, where a is a vector space over the scalar ﬁeld s. Each step in
the search is in the direction opposite of the gradient of f , which is a vector form of D f .

• Computer graphics shading models rely on normal vectors. For surfaces represented in parametric form,
i.e., as f :: R2 → R3, normal vectors are calculated from the partial derivatives of f as vectors, which are
the rows of the 3 × 2 Jacobian matrix that represents the derivative of f at any given point p :: R2.

Given a linear map f (cid:48) ::U (cid:40) V represented as a function, it is possible to extract a Jacobian matrix (including
the special case of a gradient vector) by applying f (cid:48) to every vector in a basis of U . A particularly convenient
basis is the sequence of column vectors of an identity matrix, where the i th such vector has a one in the i th
position and zeros elsewhere. If U has dimension m (e.g., U = Rm), this sampling requires m passes.
If m is
small, then this method of extracting a Jacobian is tolerably eﬃcient, but as dimension grows, it becomes quite
expensive. In particular, many useful problems involve gradient-based optimization over very high-dimensional
spaces, which is the worst case for this technique.

10 Generalized Matrices

Rather than representing derivatives as functions and then extracting a (Jacobian) matrix, a more conventional
alternative is to construct and combine matrices in the ﬁrst place. These matrices are usually rectangular arrays,
representing Rm (cid:40) Rn, which interferes with the composability we get from organizing around binary cartesian
products, as in the Monoidal , Cartesian, and Cocartesian categorical interfaces.

There is, however, an especially convenient perspective on linear algebra, known as free vector spaces. Given
a scalar ﬁeld s, any free vector space has the form p → s for some p, where the cardinality of p is the dimension
of the vector space (and only ﬁnitely many p values can have non-zero images). Scaling a vector v :: p → s
or adding two such vectors is deﬁned in the usual way for functions. Rather than using functions directly as
a representation, one can instead use any representation isomorphic to such a function. In particular, we can
represent vector spaces over a given ﬁeld as a representable functor, i.e., a functor F such that ∃p ∀s F s ∼= p → s
(where “∼=” denotes isomorphism) This method is convenient in a richly typed functional language like Haskell,
which comes with libraries of functor-level building blocks. Four such building blocks are functor product,
functor composition, and their corresponding identities, which are the unit functor (containing no elements)
and the identity functor (containing one element) [Magalhães et al., 2010; Magalhães et al., 2011]. One must
then deﬁne the standard functionality for linear maps in the form of instances of Category, Monoidal , Cartesian,
Cocartesian, and Scalable. Details are worked out by Elliott [2017, Section 7.4 and Appendix A]. One can use
other representable functors as well, including length-typed vectors [Hermaszewski and Gamari, 2017].

All of these functors give data representations of functions that save recomputation over a native function
representation, as a form of functional memoization Hinze [2000]. They also provide a composable, type-safe
alternative to the more commonly used multi-dimensional arrays (often called “tensors”) in machine learning
libraries.

11 Eﬃciency of Composition

With the function representation of linear maps, composition is simple and eﬃcient, but extracting a matrix can
be quite expensive, as described in Section 9. The generalized matrix representation of Section 10 eliminates
the need for this expensive extraction step but at the cost of more expensive construction operations used
throughout.

One particularly important eﬃciency concern is that of (generalized) matrix multiplication. Although matrix
multiplication is associative (because it correctly implements composition of linear maps represented as matrices),
diﬀerent associations can result in very diﬀerent computational cost. The problem of optimally associating a

The Simple Essence of Automatic Diﬀerentiation

17

chain of matrix multiplications can be solved via dynamic programming in O(n3) time [Cormen et al., 2001,
Section 15.2] or in O(n log n) time with a more subtle algorithm [Hu and Shing, 1981]. Solving this problem
requires knowing only the sizes (heights and widths) of the matrices involved, and those sizes depend only on the
types involved for a strongly typed linear map representation. One can thus choose an optimal association at
compile time rather than waiting for run-time and then solving the problem repeatedly. A more sophisticated
version of this question is known as the “optimal Jacobian accumulation” problem and is NP-complete [Naumann,
2008].

Alternatively, for some kinds of problems we might want to choose a particular association for sequential
composition. For instance, gradient-based optimization (including its use in machine learning) uses “reverse-mode”
automatic diﬀerentiation (RAD), which is to say fully left-associated compositions. (Dually, “foward-mode” AD
fully right-associates.) Reverse mode (including its specialization, backpropagation) is much more eﬃcient for
these problems, but is also typically given much more complicated explanations and implementations, involving
mutation, graph construction, and “tapes”. One of the main purposes of this paper is to demonstrate that these
complications are inessential and that RAD can instead be speciﬁed and implemented quite simply.

12 Reverse-Mode Automatic Diﬀerentiation

The AD algorithm derived in Section 4 and generalized in Figure 6 can be thought of as a family of algorithms.
For fully right-associated compositions, it becomes forward mode AD; for fully left-associated compositions,
reverse-mode AD; and for all other associations, various mixed modes.

Let’s now look at how to separate the associations used in formulating a diﬀerentiable function from the
associations used to compose its derivatives. A practical reason for making this separation is that we want to
do gradient-based optimization (calling for left association), while modular program organization results in a
mixture of compositions. Fortunately, a fairly simple technique removes the tension between eﬃcient execution
and modular program organization.

Given any category k , we can represent its morphisms by the intent to left-compose with some to-be-given
morphism h. That is, represent f :: a ‘k ‘ b by the function (◦ f ) :: (b ‘k ‘ r ) → (a ‘k ‘ r ), where r is any object in
k .13 The morphism h will be a continuation, ﬁnishing the journey from f all the way to the codomain of the
overall function being assembled. Building a category around this idea results in converting all composition
patterns into fully left-associated form. This trick is akin to conversion to continuation-passing style [Reynolds,
1972; Appel, 2007; Kennedy, 2007]. Compositions in the computation become compositions in the continuation.
For instance, g ◦ f with a continuation k (i.e., k ◦ (g ◦ f )) becomes f with a continuation k ◦ g (i.e., (k ◦ g) ◦ f ).
The initial continuation is id (because id ◦ f = f ).

Now package up the continuation representation as a transformation from category k and codomain r to a

new category, Cont r
k :

newtype Cont r

k a b = Cont ((b ‘k ‘ r ) → (a ‘k ‘ r ))

cont :: Category k ⇒ (a ‘k ‘ b) → Cont r
cont f = Cont (◦ f )

k a b

As usual, we can derive instances for our new category by homomorphic speciﬁcation:

Theorem 4 (Proved in Appendix C.3) Given the deﬁnitions in Figure 7, cont is a homomorphism with respect
to each instantiated class.

Note the pleasant symmetries in Figure 7. Each Cartesian or Cocartesian operation on Cont r
dual Cocartesian or Cartesian operation, together with the join/unjoin isomorphism.

k is deﬁned via the

The instances for Cont r

k constitute a simple algorithm for reverse-mode AD. Figures 8 and 9 show the results
k corresponding to Figures 2 and 3 and Figures 4 and 5. The derivatives are represented as (linear)

of Cont r
functions again, but reversed (mapping from codomain to domain).

13Following Haskell notation for right sections, “(◦ f )” is shorthand for λh → h ◦ f .

18

Conal Elliott

newtype Cont r

k a b = Cont ((b ‘k ‘ r ) → (a ‘k ‘ r ))

instance Category k ⇒ Category Cont r

k where

id = Cont id
Cont g ◦ Cont f = Cont (f ◦ g)

instance Monoidal k ⇒ Monoidal Cont r

k where

Cont f × Cont g = Cont (join ◦ (f × g) ◦ unjoin)

instance Cartesian k ⇒ Cartesian Cont r

k where

exl = Cont (join ◦ inl )
exr = Cont (join ◦ inr )
dup = Cont (jam ◦ unjoin)

instance Cocartesian k ⇒ Cocartesian Cont r

k where

inl = Cont (exl ◦ unjoin)
inr = Cont (exr ◦ unjoin)
jam = Cont (join ◦ dup)

instance Scalable k a ⇒ Scalable Cont r

k a where

scale s = Cont (scale s)

Figure 7: Continuation category transformer (speciﬁed by functoriality of cont)

Figure 8: magSqr in DCont R

(→+)

Figure 9: cosSinProd in DCont R

(→+)

In    ×   ×   ×   ×   +   Out  +   +   OutIn In    ×   ×   ×  sin  cos   ×    Out  ×   Out  − In  The Simple Essence of Automatic Diﬀerentiation

19

13 Gradients and Duality

As a special case of reverse-mode automatic diﬀerentiation, let’s consider its use to compute gradients, i.e.,
derivatives of functions with a scalar codomain, as with gradient-based optimization.

Given a vector space A over a scalar ﬁeld s, the dual of A is A (cid:40) s, i.e., the linear maps to the underlying
ﬁeld [Lang, 1987].14 This dual space is also a vector space, and when A has ﬁnite dimension, it is isomorphic to
its dual. In particular, every linear map in A (cid:40) s has the form dot u for some u :: A, where dot is the curried
dot product:

class HasDot s u where dot :: u → (u (cid:40) s)

instance HasDot R R where dot = scale

instance (HasDot s a, HasDot s b) ⇒ HasDot s (a × b) where dot (u, v ) = dot u (cid:79) dot v

The Cont r

k construction from Section 12 works for any type/object r , so let’s take r to be the scalar ﬁeld
((cid:40)) a b is (b (cid:40) s) → (a (cid:40) s), which is isomorphic to b → a. Call this

s. The internal representation of Cont s
representation the dual (or “opposite”) of k :

newtype Dual k a b = Dual (b ‘k ‘ a)

To construct dual representations of (generalized) linear maps, it suﬃces to convert from Cont s
functor we will now derive. Composing this new functor with cont :: (a ‘k ‘ b) → Cont s
from k to Dual k . The new to-be-derived functor:

k to Dual k by a
k a b will give us a functor

asDual :: (HasDot s a, HasDot s b) ⇒ Cont s
asDual (Cont f ) = Dual (onDot f )

k a b → Dual k a b

where onDot uses both halves of the isomorphism between a (cid:40) s and a:

onDot :: (HasDot s a, HasDot s b) ⇒ ((b (cid:40) s) → (a (cid:40) s)) → (b (cid:40) a)
onDot f = dot −1 ◦ f ◦ dot

As usual, we can derive instances for our new category by homomorphic speciﬁcation:

Theorem 5 (Proved in Appendix C.4) Given the deﬁnitions in Figure 10, asDual is a homomorphism with
respect to each instantiated class.

Note that the instances in Figure 10 exactly dualize a computation, reversing sequential compositions and

swapping corresponding Cartesian and Cocartesian operations. Likewise for the derived operations:

Corollary 5.1 (Proved in Appendix C.5) The ((cid:77)) and ((cid:79)) operations mutually dualize:

Dual f (cid:77) Dual g = Dual (f (cid:79) g)

Dual f (cid:79) Dual g = Dual (f (cid:77) g)

Recall from Section 8, that scale forms 1 × 1 matrices, while ((cid:79)) and ((cid:77)) correspond to horizontal and vertical
juxtaposition, respectively. Thus, from a matrix perspective, duality is transposition, turning an m × n matrix
into an n × m matrix. Note, however, that Dual k involves no actual matrix computations unless k does. In
particular, we can simply use the category of linear functions (→+).

Figures 11 and 12 show the results of reverse-mode AD via DDual (→+)

. Compare Figure 11 with Figures 4 and

8.

14These linear maps are variously known as “linear functionals”, “linear forms”, “one-forms”, and “covectors”.

20

Conal Elliott

instance Category k ⇒ Category Dual k where

id = Dual id
Dual g ◦ Dual f = Dual (f ◦ g)

instance Monoidal k ⇒ Monoidal Dual k where

Dual f × Dual g = Dual (f × g)

instance Cartesian k ⇒ Cartesian Dual k where

exl = Dual inl
exr = Dual inr
dup = Dual jam

instance Cocartesian k ⇒ Cocartesian Dual k where

inl = Dual exl
inr = Dual exr
jam = Dual dup

instance Scalable k ⇒ Scalable Dual k where

scale s = Dual (scale s)

Figure 10: Dual category transformer (speciﬁed by functoriality of asDual )

Figure 11: magSqr in DDual (→+)

Figure 12: λ((x , y), z ) → cos (x + y · z ) in DDual (→+)

In    ×   ×   +   +   +    OutIn     ×   +   ×   ×  cos  sin     Out negate  negate  negate The Simple Essence of Automatic Diﬀerentiation

21

14 Forward-Mode Automatic Diﬀerentiation

It may be interesting to note that we can turn the Cont and Dual techniques around to yield category transformers
that perform full right- instead of left-association, converting the general, mode-independent algorithm into
forward mode, thus yielding an algorithm preferable for low-dimensional domains (rather than codomains):

newtype Begin r

k a b = Begin ((r ‘k ‘ a) → (r ‘k ‘ b))

begin :: Category k ⇒ (a ‘k ‘ b) → Begin r
begin f = Begin (f ◦)

k a b

As usual, we can derive instances for our new category by homomorphic speciﬁcation (for begin). Then choose r
to be the scalar ﬁeld s, as in Section 13, noting that (s (cid:40) a) ∼= a.

15 Scaling Up

So far, we have considered binary products. Practical applications, including machine learning and other
optimization problems, often involve very high-dimensional spaces. While those spaces can be encoded as nested
binary products, doing so would result in unwieldy representations and prohibitively long compilation and
execution times. A practical alternative is to consider n-ary products, for which we can again use representable
functors. To construct and consume these “indexed” (bi)products, we’ll need an indexed variant of Monoidal ,
replacing the two arguments to (×) by a (representable) functor h of morphisms:

class Category k ⇒ MonoidalI k h where

instance Zip h ⇒ MonoidalI (→) h where

crossI :: h (a ‘k ‘ b) → (h a ‘k ‘ h b)

crossI = zipWith id

Note that the collected morphisms must all agree in domain and codomain. While not required for the general
categorical notion of products, this restriction accommodates Haskell’s type system and seems adequate in
practice so far.

Where the Cartesian class has two projection methods and a duplication method, the indexed counterpart

has a collection of projections and one replication method:15

class MonoidalI k h ⇒ CartesianI k h where

:: h (h a ‘k ‘ a)

exI
replI :: a ‘k ‘ h a

instance (Representable h, Zip h, Pointed h) ⇒ CartesianI (→) h where

exI = tabulate (ﬂip index )
replI = point

Dually, where the Cocartesian class has two injection methods and a binary combination method, the indexed
counterpart has a collection of injections and one collection-combining method:

class MonoidalI k h ⇒ CocartesianI k h where

:: h (a ‘k ‘ h a)

inI
jamI :: h a ‘k ‘ a

There are also indexed variants of the derived operations ((cid:77)) and ((cid:79)) from Section 4.5:

forkI :: CartesianI k h ⇒ h (a ‘k ‘ b) → (a ‘k ‘ h b)
forkI fs = crossI fs ◦ replI

unforkF :: CartesianI k h ⇒ (a ‘k ‘ h b) → h (a ‘k ‘ b)
unforkF f = fmap (◦ f ) exI

joinI :: CartesianI k h ⇒ h (b ‘k ‘ a) → (h b ‘k ‘ a)

15The index and tabulate functions convert from functor to function and back [Kmett, 2011].

22

Conal Elliott

instance (MonoidalI k h, Zip h) ⇒ MonoidalI Dk h where

crossI fs = D (second crossI ◦ unzip ◦ crossI (fmap unD fs))

instance (CartesianI (→) h, CartesianI k h, Zip h) ⇒ CartesianI Dk h where

exI = linearD exI exI
replI = zipWith linearD replI replI

instance (CocartesianI k h, Zip h) ⇒ CocartesianI Dk h where

inI = zipWith linearD inIF inI
jamI = linearD sum jamI

unD :: D a b → (a → (b × (a (cid:40) b)))
unD (D f ) = f

unzip :: Functor h ⇒ h (a × b) → h a × h b
unzip = fmap exl (cid:77) fmap exr

second :: Monoidal k ⇒ (b ‘k ‘ d ) → ((a × b) ‘k ‘ (a × d ))
second g = id × g

inIF :: (Additive a, Foldable h) ⇒ h (a → h a)
inIF = tabulate (λi a → tabulate (λj → if i = j then a else 0))

class Zip h where zipWith :: (a → b → c) → h a → h b → h c

Figure 13: AD for indexed biproducts

joinI fs = jamI ◦ crossI fs

unjoinPF :: CocartesianI k h ⇒ (h b ‘k ‘ a) → h (b ‘k ‘ a)
unjoinPF f = fmap (f ◦) inI

As usual, we can derive instances by homomorphic speciﬁcation:

Theorem 6 (Proved in Appendix C.6) Given the deﬁnitions in Figure 13, ˆD is a homomorphism with respect
to each instantiated class.

These indexed operations are useful in themselves but can be used to derive other operations. For instance,

note the similarity between the types of crossI and fmap:

crossI :: Monoidal h ⇒ h (a → b) → (h a → h b)
h ⇒ (a → b) → (h a → h b)
fmap :: Functor

In fact, the following relationship holds: fmap = crossI ◦ replI . This equation, together with the diﬀerentiation
rules for crossI , replI , and (◦) determines diﬀerentiation for fmap f .

As with Figure 6, the operations deﬁned in Figure 13 rely on corresponding operations for the category
parameter k . Fortunately, all of those operations are linear or preserve linearity, so they can all be deﬁned on the
various representations of derivatives (linear maps) used for AD in this paper. Figures 14 and 15 show instances
for two of the linear map representations deﬁned in this paper, with the third left as an exercise for the reader.
(The constraint Additive 1 h means that ∀a.Additive a ⇒ Additive (h a).)

16 Related Work

The literature on automatic diﬀerentiation is vast, beginning with forward mode [Wengert, 1964] and later
reverse mode [Speelpenning, 1980; Rall, 1981], with many developments since [Griewank, 1989; Griewank and

The Simple Essence of Automatic Diﬀerentiation

23

instance (Zip h, Additive 1 h) ⇒ MonoidalI (→+) h where

crossI = AddFun ◦ crossI ◦ fmap unAddFun

instance (Representable h, Zip h, Pointed h, Additive 1 h) ⇒ CartesianI (→+) h where

exI = fmap AddFun exI
replI = AddFun replI

instance (Foldable h, Additive 1 h) ⇒ CocartesianI (→+) h where

inI = fmap AddFun inIF
jamI = AddFun sum

Figure 14: Indexed instances for (→+)

instance (MonoidalI k h, Functor h, Additive 1 h) ⇒ MonoidalI (Dual k ) h where

crossI = Dual ◦ crossI ◦ fmap unDual

instance (CocartesianI k h, Functor h, Additive 1 h) ⇒ CartesianI (Dual k ) h where

exI = fmap Dual inI
replI = Dual jamI

instance (CartesianI k h, Functor h, Additive 1 h) ⇒ CocartesianI (Dual k ) h where

inI = fmap Dual exI
jamI = Dual replI

Figure 15: Indexed instances for Dual k

Walther, 2008]. While most techniques and uses of AD have been directed at imperative programming, there are
also variations for functional programs [Karczmarczuk, 1999, 2000, 2001; Pearlmutter and Siskind, 2007, 2008;
Elliott, 2009]. The work in this paper diﬀers in being phrased at the level of functions/morphisms and speciﬁed
by functoriality, without any mention or manipulation of graphs or other syntactic representations.16 Moreover,
the speciﬁcations in this paper are simple enough that the various forms of AD presented can be calculated into
being [Bird and de Moor, 1996; Oliveira, 2018], and so are correct by construction.

Pearlmutter and Siskind [2008] make the following observation:

In this context, reverse-mode AD refers to a particular construction in which the primal data-ﬂow
graph is transformed to construct an adjoint graph that computes the sensitivity values. In the
adjoint, the direction of the data-ﬂow edges are reversed; addition nodes are replaced by fanout
nodes; fanout nodes are replaced by addition nodes; and other nodes are replaced by multiplication
by their linearizations. The main constructions of this paper can, in this context, be viewed as a
method for constructing scaﬀolding that supports this adjoint computation.

The Cont and Dual category transformers described in Sections 12 and 13 (shown in Figures 7 and 10) above
explain this “adjoint graph” construction without involving graphs. Data-ﬂow edge reversal corresponds to the
reversal of (◦) (from Category), while fanout and addition correspond to dup and jam (from Cartesian and
Cocartesian respectively), which are mutually dual. Pearlmutter and Siskind [2008] further remark:

The main technical diﬃculty to be faced is that reverse-mode AD must convert fanout (multiple use
of a variable) in the untransformed code into addition in the reverse phase of the transformed code.
We address this by expressing all straight-line code segments in A-normal form, which makes fanout
lexically apparent.

16Of course the Haskell compiler itself manipulates syntax trees, and the compiler plugin that converts Haskell code to categorical
form helps do so, but both are entirely domain-independent, with no knowledge of or special support for diﬀerentiation or linear
algebra [Elliott, 2017].

24

Conal Elliott

The categorical approach in this paper also makes fanout easily apparent, as occurrences of dup, which are
produced during translation from Haskell to categorical form [Elliott, 2017] (via ((cid:77)) as deﬁned in Section 4.5
above). This translation is speciﬁed and implemented independently of AD and so presents no additional
complexity.

Closely related to our choice of derivatives as linear maps and their categorical generalizations is the work
of Macedo and Oliveira [2013], also based on biproducts (though not addressing diﬀerentiation). That work
uses natural numbers as categorical objects to capture the dimensions of vectors and matrices, while the current
paper uses vector spaces themselves. The diﬀerence is perhaps minor, however, since natural numbers can be
thought of as representing ﬁnite sets (of corresponding cardinality), which are bases of ﬁnite-dimensional free
vector spaces (as in Section 10). On the other hand, the duality-based gradient algorithm of Section 13 involves
no matrices at all in their traditional representation (arrays of numbers) or generalized sense of Section 10
(representable functors).

Also sharing a categorical style is the work of Fong et al. [2017], formulating the backpropropagation
algorithm as a functor. That work, which also uses biproducts (in monoidal but not cartesian form), does
not appear to be separable from the application to machine learning, and so would seem to complement this
paper. Backpropagation is a specialization of reverse-mode AD to the context of machine learning, discovered by
Linnainmaa [1970] and popularized by Rumelhart et al. [1988].

The continuation transformation of Section 12 was inspired by Mitch Wand’s work on continuation-based
program transformation [Wand, 1980]. He derived a variety of algorithms based on a single elegant technique:
transform a simple recursive program into continuation-passing form, examine the continuations that arise,
and ﬁnd a data (rather than function) representation for them. Each such representation is a monoid, with its
identity and associative operation corresponding to identity and composition of the continuations. Monoids are
categories with only one object, but the technique extends to general categories. Cayley’s theorem for groups (or
monoids) captures this same insight and is a corollary (in retrospect) of the Yoneda lemma [Riehl, 2016, Section
2.2]. The idea of using data representations for functions (“defunctionalization”) was pioneered by Reynolds
[1972] and further explored by Danvy and Nielsen [2001].

The notion of derivatives as linear maps is the basis of calculus on manifolds Spivak [1965] and was also used
for AD by Elliott [2009]. The latter addressed only forward-mode AD but also included all orders of derivatives.
While there are many forward-mode AD libraries for Haskell, reverse mode (RAD) has been much more
diﬃcult. The most successful implementation appears to be in the ad library [Kmett et al., 2010]. One RAD
implementation in that library uses stable names [Peyton Jones et al., 1999] and reiﬁcation [Gill, 2009] to recover
sharing information. Another maintains a Wengert list (or “tape”) with the help of a reﬂection library [Kiselyov
and Shan, 2004]. Both implementations rely on hidden, carefully crafted use of side eﬀects.

Chris Olah [2015] shared a vision for “diﬀerentiable functional programming” similar to that in Section 1. He
pointed out that most of the patterns now used in machine learning are already found in functional programming:

These neural network patterns are just higher order functions—that is, functions which take
functions as arguments. Things like that have been studied extensively in functional programming.
In fact, many of these network patterns correspond to extremely common functions, like fold. The
only unusual thing is that, instead of receiving normal functions as arguments, they receive chunks of
neural network.

The current paper carries this perspective further, suggesting that the essence is diﬀerentiable functions, with
“networks” (graphs) being an unnecessary (and apparently unwise) operational choice.

This paper builds on a compiler plugin that translates Haskell programs into categorical form to be specialized
to various speciﬁc categories, including diﬀerentiable functions [Elliott, 2017]. (The plugin knows nothing about
any speciﬁc category, including diﬀerentiable functions.) Another instance of generalized AD given there is
automatic incremental evaluation of functional programs. Relative to that work, the new contributions are the
Cont r
k and Dual k categories, their use to succinctly implement reverse-mode AD (by instantiating the generalized
diﬀerentiation category), the precise speciﬁcation of instances for D, Cont r
k , and Dual k via functoriality, and the
calculation of implementations from these speciﬁcations.

The implementations in this paper are quite simple and appear to be eﬃcient as well. For instance, the duality-
based version (Section 13) involves no matrices. Moreover, typical reverse-mode AD (RAD) implementations
use mutation to incrementally update derivative contributions from each use of a variable or intermediate
computation, holding onto all of these accumulators until the very end of the derivative computation. For this
reason, such implementations tend to use considerable memory. In contrast, the implementations in this paper

The Simple Essence of Automatic Diﬀerentiation

25

(Sections 12 and 13) are free of mutation and can easily free (reuse) memory as they run, keeping memory use
low. Given the prominent use of AD, particularly with large data, performance is crucial, so it will be worthwhile
to examine and compare time and space use in detail. Lack of mutation also makes the algorithms in this paper
naturally parallel, potentially leading to considerable speed improvement, especially when using the functor-level
(bulk) vocabulary in Section 15.

17 Conclusions

This paper develops a simple, mode-independent algorithm for automatic diﬀerentiation (AD) (Section 4),
calculated from a simple, natural speciﬁcation in terms of elementary category theory (functoriality). It then
generalizes the algorithm, replacing linear maps (as derivatives) by an arbitrary biproduct category (Figure
6). Specializing this general algorithm to two well-known categorical constructions (Figures 7 and 10)—also
calculated—yields reverse-mode AD (RAD) for general derivatives and for gradients. These RAD implementations
are far simpler than previously known. In contrast to common approaches to AD, the new algorithms involve no
graphs, tapes, variables, partial derivatives, or mutation, and are usable directly from an existing programming
language with no need for new data types or programming style (thanks to use of an AD-agnostic compiler
plugin). Only the simple essence remains.

Future work includes detailed performance analysis (compared with backpropagation and other conventional
AD algorithms); eﬃcient higher-order diﬀerentiation; and applying generalized AD to derivative-like notions,
including subdiﬀerentiation [Rockafellar, 1966] and automatic incrementalization (continuing previous work
[Elliott, 2017]).

AD is typically said to be about the chain rule for sequential composition (Theorem 1). This paper rounds
out the story with two more rules: one for parallel composition and one for all linear operations (Theorems 2
and 3). Parallel composition is usually left implicit in the special-case treatment of a collection of non-unary
operations, such as addition, multiplication, division, and dot products. With explicit, general support for
parallel composition, all operations come to be on equal footing, regardless of arity (as illustrated in Figure 6).
AD is also typically presented in opposition to symbolic diﬀerentiation (SD), with the latter described as
applying diﬀerentiation rules symbolically. The main criticism of SD is that it can blow up expressions, resulting
a great deal of redundant work. Secondly, SD requires implementation of symbolic manipulation as in a computer
algebra system. In contrast, AD is described as a numeric method and can retain the complexity of the original
function (within a small constant factor) if carefully implemented, as in reverse mode. The approach explored in
this paper suggests a diﬀerent perspective: automatic diﬀerentiation is symbolic diﬀerentiation performed by
a compiler. Compilers already work symbolically and already take care to preserve sharing in computations,
addressing both criticisms of SD.

The speciﬁcation and implementation of AD in a simple, correct-by-construction, and apparently eﬃcient
manner, together with its use from a typed functional language (here via a compiler plugin), make a step toward
the vision of diﬀerentiable functional programming for machine learning and other uses, as outlined in Section 1.
Programmers then deﬁne their functions just as they are accustomed, diﬀerentiating where desired, without the
intrusion of operational notions such as graphs with questionably deﬁned, extralinguistic semantics.

In retrospect, two key principles enable the results in this paper:

1. Focus on abstract notions (speciﬁed denotationally and/or axiomatically) rather than particular repre-
sentations (here, derivatives as linear maps rather than as matrices). Then transform a correct, naive
representation into subtler, more eﬃcient representations.

2. Capture the main concepts of interest directly, as ﬁrst-class values (here, diﬀerentiable functions).

The second principle leads us into a quandary, because most programming languages (including Haskell) are
much better suited to expressing regular computable functions than other function-like things, and yet the main
AD concept is exactly a function-like thing (diﬀerentiable functions). This imbalance in suitability stems from
built-in language support for functions—such as lambda, application, and variables—and would seem to explain
two common strategies in AD: use of explicit graph representations (complicating use and implementation), and
overloading numeric operators (abandoning the second principle, encouraging forward mode AD, and leading
to incorrect nested diﬀerentiation [Siskind and Pearlmutter, 2008]). Fortunately, we can instead extend the
notational convenience of functions to other function-like things by writing in a conventional functional language
and automatically translating to other categories [Elliott, 2017].

26

Conal Elliott

18 Acknowledgments

The investigation of reverse-mode AD and its specialization to scalar-valued functions (as in backpropagation)
were inspired by a conversation with Wang Ruikang.

A Terminal and Initial Objects

In the biproduct setting of this paper, terminal and initial objects coincide and may be taken to be any singleton
type. We may as well choose the unit type, having exactly one element, representing a canonical zero-dimensional
vector space, and written “()” in Haskell:1718

class Terminal k where it ::
class Initial k

a ‘k ‘ ()
where ti :: Additive a ⇒ () ‘k ‘ a

instance Terminal (→) where it = λ → ()
instance Initial (→) where ti = λ() → 0

Diﬀerentiation is trivial, since it and ti on functions are both linear.

B Abelian Categories

Another perspective on the operations we’ve considered is that morphisms sharing any particular domain and
codomain (i.e., hom-sets) form an abelian group. The zero for a ‘k ‘ b results from the composition of initial and
terminal morphisms:

instance (Cartesian k , Cocartesian k , Terminal k , InitialCat k ) ⇒ Additive (a ‘k ‘ b) where

0 = ti ◦ it
f + g = jam ◦ (f × g) ◦ dup

-- = jam ◦ (f (cid:77) g) = (f (cid:79) g) ◦ dup.

The following identities hold (with “◦” binding more tightly than “+”) [Macedo and Oliveira, 2013, Equations 16
and 17]:

u (cid:77) v = u ◦ exl + v ◦ exr
u (cid:79) v = inl ◦ u + inr ◦ v

In particular,

u (cid:77) 0 = u ◦ exl
0 (cid:77) v = v ◦ exr

u (cid:79) 0 = inl ◦ u
0 (cid:79) v = inr ◦ v

C Proofs

C.1 Corollary 1.1

D+ (g ◦ f ) a

= ((g ◦ f ) a, D (g ◦ f ) a)
= (g (f a), D (g ◦ f ) a)
= (g (f a), D g (f a) ◦ D f a)
= let b = f a in (g b, D g b ◦ D f a)
= let {(b, f (cid:48)) = D+ f a; (c, g (cid:48)) = D+ g b } in (c, g (cid:48) ◦ f (cid:48))

-- deﬁnition of D+
-- deﬁnition of (◦)
-- Theorem 1
-- refactoring to share f a
-- refactoring to show compositionality

17In a more general categorical setting, terminal and initial objects need not coincide and are deﬁned per category.
18As with Cocartesian, in the actual implementation, the Initial deﬁnition has no Additive constraint or Initial (→) instance,

and instead has a Initial instance for additive functions.

The Simple Essence of Automatic Diﬀerentiation

27

C.2 Corollary 2.1

D+ (f × g) (a, b)

= ((f × g) (a, b), D (f × g) (a, b))
= ((f a, g b), D (f × g) (a, b))
= ((f a, g b), D f a × D g b)
= let {(c, f (cid:48)) = (f a, D f a); (d , g (cid:48)) = (g b, D g b)} in ((c, d ), f (cid:48) × g (cid:48))
= let {(c, f (cid:48)) = D+ f a; (d , g (cid:48)) = D+ g b } in ((c, d ), f (cid:48) × g (cid:48))

-- deﬁnition of D+
-- deﬁnition of (×)
-- Theorem 2
-- refactoring
-- deﬁnition of D+

C.3 Theorem 4

Recall the deﬁnition of cont:

cont :: Category k ⇒ (a ‘k ‘ b) → Cont r
cont f = Cont (◦ f )

k a b

To say that cont is a functor (Category homomorphism) is equivalent to the following two equalities:

cont id = id

cont (g ◦ f ) = cont g ◦ cont f

Simplify the ﬁrst homomorphism equation:

cont id
= Cont (◦ id )
= Cont (λh → h ◦ id )
= Cont (λh → h)
= Cont id

-- deﬁnition of cont
-- deﬁnition of right section
-- category law
-- deﬁnition of id for functions

The ﬁrst homomorphism equation is thus equivalent to id = Cont id , which is in solved form. For the second
homomorphism equation, simplify both sides:

cont g ◦ cont f

= Cont (◦ g) ◦ Cont (◦ f )

-- deﬁnition of cont

cont (g ◦ f )
= cont (◦ (g ◦ f ))
= cont (λh → h ◦ (g ◦ f ))
= cont (λh → (h ◦ g) ◦ f )
= cont (λh → (◦ f ) ((◦ g) h))
= Cont ((◦ f ) ◦ (◦ g))

-- deﬁnition of cont
-- deﬁnition of right section
-- category law
-- deﬁnition of right section
-- deﬁnition of (◦)

The simpliﬁed requirement:

Cont (◦ g) ◦ Cont (◦ f ) = Cont ((◦ f ) ◦ (◦ g))

Generalize to a stronger condition, replacing (◦ g) and (◦ f ) with g and f (appropriately re-typed):

Cont g ◦ Cont f = Cont (f ◦ g)

This strengthened condition is also in solved form. Notice the reversal of composition (and, more subtly, of id ).

The monoidal functor (i.e., a Monoidal homomorphism) property:

cont (f × g) = cont f × cont g

Simplify both sides:

28

Conal Elliott

cont f × cont g

= Cont (◦ f ) × Cont (◦ g)

-- deﬁnition of cont

cont (f × g)
= Cont (◦ (f × g))
= Cont (λh → h ◦ (f × g))
= Cont (λh → join (unjoin h) ◦ (f × g))
= Cont (λh → let (ha , hb) = unjoin h in join (ha , hb) ◦ (f × g))
= Cont (λh → let ... in (ha (cid:79) hb) ◦ (f × g))
= Cont (λh → let ... in (ha ◦ f (cid:79) hb ◦ g))
= Cont (λh → let ... in ((◦ f ) ha (cid:79) (◦ g) hb))
= Cont (λh → let ... in join ((◦ f ) ha , (◦ g) hb))
= Cont (λh → let ... in join (((◦ f ) × (◦ g)) (ha , hb)))
= Cont (λh → join (((◦ f ) × (◦ g)) (unjoin h)))
= Cont (join ◦ ((◦ f ) × (◦ g)) ◦ unjoin)

-- deﬁnition of cont
-- deﬁnition of right section
-- join ◦ unjoin = id
-- refactor
-- deﬁnition of join
-- [Gibbons, 2002, Section 1.5.2]
-- deﬁnition of right section
-- deﬁnition of join
-- deﬁnition of (×)
-- eliminate let
-- deﬁnition of (◦)

The crucial trick here was to note that h :: (a × b) ‘k ‘ r can be split into two continuations ha :: a ‘k ‘ r and
hb :: b ‘k ‘ r thanks to join/unjoin isomorphism from Section 4.5. Now, strengthen the massaged speciﬁcation,
generalizing from (◦ f ) and (◦ g) as usual, resulting in a suﬃcient condition in solved form:

Cont f × Cont g = Cont (join ◦ (f × g) ◦ unjoin)

Next, derive Cartesian and Cocartesian instances from the speciﬁcation that cont is a cartesian functor and

a cocartesian functor (i.e., Cartesian and Cocartesian homomorphisms), i.e.,

cont exl = exl
cont exr = exr
cont dup = dup

cont inl = inl
cont inr = inr
cont jam = jam

Reversing each of these equations puts them in solved form, so they can be used directly as deﬁnitions. While
these deﬁnitions are correct, they can be made more eﬃcient. For instance,

cont exl

= Cont (λh → h ◦ exl )
= Cont (λh → h (cid:79) 0)
= Cont (λh → join (h, 0))
= Cont (λh → join (inl h))
= Cont (join ◦ inl )

-- deﬁnition of cont
-- Appendix B
-- deﬁnition of join
-- deﬁnition of inl for functions
-- deﬁnition of (◦) for functions

Similarly, cont exr = Cont (join ◦ inr ). For dup :: a ‘k ‘ (a × a), we’ll have h :: (a × a) (cid:59) r , so we can split h
with unjoin:

cont dup

= Cont (λh → h ◦ dup)
= Cont (λh → join (unjoin h) ◦ dup)
= Cont (λh → let (ha , hb) = unjoin h in (ha (cid:79) hb) ◦ dup)
= Cont (λh → let (ha , hb) = unjoin h in ha + hb)
= Cont (λh → let (ha , hb) = unjoin h in jam (ha , hb))
= Cont (λh → jam (unjoin h))
= Cont (jam ◦ unjoin)

-- deﬁnition of cont
-- join ◦ unjoin = id
-- refactor; deﬁnition of join
-- Appendix B
-- deﬁnition of jam for functions
-- eliminate the let
-- deﬁnition of (◦) on functions

For Cocartesian, we reason dually:

cont inl

= Cont (λh → h ◦ inl )
= Cont (λh → join (unjoin h) ◦ inl )
= Cont (λh → let (ha , hb) = unjoin h in (ha (cid:79) hb) ◦ inl )

-- deﬁnition of inl
-- join ◦ unjoin = id
-- deﬁnition of join

The Simple Essence of Automatic Diﬀerentiation

29

= Cont (λh → let (ha , hb) = unjoin h in ha )
= Cont (λh → exl (unjoin h))
= Cont (exl ◦ unjoin)

-- [Gibbons, 2002, Section 1.5.2]
-- deﬁnition of exl for functions
-- deﬁnition of (◦) for functions

Similarly, cont inr = Cont (exr ◦ unjoin). Next,

cont jam

= Cont (λh → h ◦ jam)
= Cont (λh → h ◦ (id (cid:79) id ))
= Cont (λh → h ◦ id (cid:79) h ◦ id )
= Cont (λh → h (cid:79) h)
= Cont (λh → join (h, h))
= Cont (join ◦ dup)

-- deﬁnition of cont
-- a law for jam and ((cid:79))
-- [Gibbons, 2002, Section 1.5.2]
-- category law
-- deﬁnition of join
-- deﬁnition of dup on functions

The ﬁnal element of our linear vocabulary is scalar multiplication:

cont (scale s)

= Cont (λh → h ◦ scale s)
= Cont (λh → scale s ◦ h)
= Cont (λh → scale s h)
= Cont (scale s)

-- deﬁnition of cont
-- linearity of h
-- deﬁnition of scale for functions/maps
-- η-reduction

These optimized solved forms match the deﬁnitions in Figure 7.

C.4 Theorem 5

To derive instances for Dual k , we’ll need some properties.

Lemma 7 The following properties hold:

1. dot is linear.

2. dot −1 is linear.

3. unjoin ◦ dot = dot × dot

4. dot −1 ◦ join = dot −1 × dot −1

5. dot u (cid:79) dot v = dot (u, v )

6. dot 0 = 0 (zero vector vs zero morphism)

Proof:

1. Follows from the bilinearity of uncurried dot product:

dot (u + v )

= λw → dot (u + v ) w
= λw → dot u w + dot v w -- bilinearity of uncurried dot product
= dot u + dot v

-- deﬁnition of (+) of functions

-- η-expansion

dot (s · u)

= λw → dot (s · u) w
= λw → s · dot u w
= s · dot u

-- η-expansion
-- bilinearity of uncurried dot product
-- deﬁnition of (·) on functions

2. Invertible linear functions have linear inverses. In particular,

dot −1 (u + v )

= dot −1 (dot (dot −1 u) + dot (dot −1 v ))

-- dot ◦ dot −1 = id

30

Conal Elliott

= dot −1 (dot (dot −1 u + dot −1 v ))
= dot −1 u + dot −1 v

-- linearity of dot
-- dot −1 ◦ dot = id

dot −1 (s · u)

= dot −1 (s · dot (dot −1 u))
= dot −1 (dot (s · dot −1 u))
= s · dot −1 u

-- dot ◦ dot −1 = id
-- linearity of dot
-- dot −1 ◦ dot = id

3. Noting that the argument of both sides is a pair,

unjoin ◦ dot

= λ(u, v ) → unjoin (dot (u, v ))
= λ(u, v ) → (dot (u, v ) ◦ inl , dot (u, v ) ◦ inr )
= λ(u, v ) → (λx → dot (u, v ) (inl x ), λy → dot (u, v ) (inr y))
= λ(u, v ) → (λx → dot (u, v ) (x , 0), λy → dot (u, v ) (0, y))
= λ(u, v ) → (λx → dot u x + dot v 0, λy → dot u 0 + dot v y)
= λ(u, v ) → (λx → dot u x , λy → dot v y)
= λ(u, v ) → (dot u, dot v )
= dot × dot

-- η-expansion
-- deﬁnition of unjoin
-- def’n of (◦) for (→)
-- def’n of inl for ((cid:40))
-- def’n of dot for pairs
-- linearity of dot
-- η-reduction
-- def’n of (×) for (→)

4. Follows from inverting each side of part 3.

5. Noting again that the argument of both sides is a pair,

dot u (cid:79) dot v

= jam ◦ (dot u × dot v )
= λ(x , y) → jam ((dot u × dot v ) (x , y))
= λ(x , y) → jam (dot u x , dot v y)
= λ(x , y) → dot u x + dot v y
= λ(x , y) → dot (u, v ) (x , y)
= dot (u, v )

-- deﬁnition of ((cid:79))
-- deﬁnition of (◦) for functions
-- deﬁnition of (×) for functions
-- deﬁnition of jam for functions
-- deﬁnition of dot for pairs
-- η-reduction

6. Immediate from linearity and the deﬁnition of 0 for functions.

End of proof of Lemma 7.

Recall the deﬁnition of asDual from Section 13:

asDual :: (HasDot s a, HasDot s b) ⇒ Cont s
asDual (Cont f ) = Dual (onDot f )

k a b → Dual k a b

where

onDot :: (HasDot s a, HasDot s b) ⇒ ((b (cid:40) s) → (a (cid:40) s)) → (b (cid:40) a)
onDot f = dot −1 ◦ f ◦ dot

For the Category instance of Dual k , we’ll need that id = asDual id . Simplifying the RHS,

asDual id

= asDual (Cont id )
= Dual (dot −1 ◦ id ◦ dot)
= Dual (dot −1 ◦ dot)
= Dual id

-- deﬁnition of id for Cont r
-- deﬁnition of asDual
-- Category law for id /(◦)
-- dot −1 ◦ dot = id

k (Figure 7)

We also need asDual (g ◦ f ) = asDual g ◦ asDual f , or (without loss of generality)

The Simple Essence of Automatic Diﬀerentiation

31

asDual (Cont g ◦ Cont f ) = asDual (Cont g) ◦ asDual (Cont f )

Simplifying both sides,

asDual (Cont g ◦ Cont f )

= asDual (Cont (f ◦ g))
= Dual (dot −1 ◦ f ◦ g ◦ dot)
= Dual (dot −1 ◦ f ◦ dot ◦ dot −1 ◦ g ◦ dot)
= Dual (onDot f ◦ onDot g)

-- deﬁnition of (◦) for Cont r
k
-- deﬁnition of asDual
-- dot ◦ dot −1 = id
-- deﬁnition of onDot

asDual (Cont g) ◦ asDual (Cont f )
= Dual (onDot g) ◦ asDual (onDot f )

-- deﬁnition of asDual

As usual, strengthen this equality by replacing onDot g and onDot f by re-typed g and f , and read oﬀ a suﬃcient
deﬁnition:

Dual (f ◦ g) = Dual g ◦ asDual f

For Monoidal , the homomorphism condition is asDual (f × g) = asDual f × asDual g. Simplify both sides:

asDual (Cont f ) × asDual (Cont g)

= Dual (onDot f ) × Dual (onDot g)

asDual (Cont f × Cont g)

-- deﬁnition of asDual

= asDual (Cont (join ◦ (f × g) ◦ unjoin))
= Dual (onDot (join ◦ (f × g) ◦ unjoin))
= Dual (dot −1 ◦ join ◦ (f × g) ◦ unjoin ◦ dot)
= Dual ((dot −1 × dot −1) ◦ (f × g) ◦ (dot × dot))
= Dual (dot −1 ◦ f ◦ dot × dot −1 ◦ g ◦ dot −1)
= Dual (onDot f × onDot g)

-- deﬁnition of (×) on Cont
-- deﬁnition of asDual
-- deﬁnition of onDot
-- Lemma 7, parts 3 & 4
-- law about (×)/(◦)
-- deﬁnition of onDot

Strengthening from onDot f and onDot g gives a simple suﬃcient condition:

Dual f × Dual g = Dual (f × g)

For Cartesian,

exl

= asDual exl
= asDual (Cont (join ◦ inl ))
= Dual (onDot (join ◦ inl ))
= Dual (dot −1 ◦ join ◦ inl ◦ dot)
= Dual (λu → dot −1 (join (inl (dot u))))
= Dual (λu → dot −1 (join (dot u, 0)))
= Dual (λu → dot −1 (dot u (cid:79) 0))
= Dual (λu → dot −1 (dot u (cid:79) dot 0))
= Dual (λu → dot −1 (dot (u, 0)))
= Dual (λu → (u, 0))
= Dual (λu → inl u)
= Dual inl

-- speciﬁcation
-- deﬁnition of exl for Cont r
k
-- deﬁnition of asDual
-- deﬁnition of onDot, and associativity of (◦)
-- deﬁnition of (◦) for functions
-- deﬁnition of inl for functions
-- deﬁnition of join
-- Lemma 7, part 6
-- Lemma 7, part 5
-- dot −1 ◦ dot = id
-- deﬁnition of inl for functions
-- η-reduction

exrP
= Dual inr

dup

-- as with exlP

= asDual dup
= asDual (Cont (jam ◦ unjoin))

-- speciﬁcation
-- deﬁnition of dup for Cont r
k

32

Conal Elliott

= Dual (onDot (jam ◦ unjoin))
= Dual (dot −1 ◦ jam ◦ unjoin ◦ dot)
= Dual (λ(u, v ) → dot −1 (jam (unjoin (dot (u, v )))))
= Dual (λ(u, v ) → dot −1 (jam (dot u, dot v )))
= Dual (λ(u, v ) → dot −1 (dot u + dot v ))
= Dual (λ(u, v ) → dot −1 (dot u) + dot −1 (dot v ))
= Dual (λ(u, v ) → u + v )
= Dual jam

-- deﬁnition of asDual
-- deﬁnition of onDot
-- deﬁnition of (◦) for functions
-- Lemma 7, part 3
-- deﬁnition of jam for functions
-- Lemma 7, part 2
-- dot −1 ◦ dot = id
-- deﬁnition of jam for functions

The Cocartesian instance comes out similarly:

inl

= asDual inl
= asDual (Cont (exl ◦ unjoin))
= Dual (onDot (exl ◦ unjoin))
= Dual (dot −1 ◦ exl ◦ unjoin ◦ dot)
= Dual (λ(u, v ) → dot −1 (exl (unjoin (dot (u, v )))))
= Dual (λ(u, v ) → dot −1 (exl (dot u, dot v )))
= Dual (λ(u, v ) → dot −1 (dot u))
= Dual (λ(u, v ) → u)
= Dual exl

-- speciﬁcation
-- deﬁnition of inl for Cont r
k
-- deﬁnition of asDual
-- deﬁnition of onDot
-- deﬁnition of (◦) for functions
-- Lemma 7, part 3
-- deﬁnition of exl on functions
-- dot −1 ◦ dot = id
-- deﬁnition of exl for functions

inr

= Dual exr

jam

-- . . . as with inl . . .

= asDual jam
= asDual (Cont (join ◦ dup))
= Dual (onDot (join ◦ dup))
= Dual (dot −1 ◦ join ◦ dup ◦ dot)
= Dual (λu → dot −1 (join (dup (dot u))))
= Dual (λu → dot −1 (join (dot u, dot u)))
= Dual (λu → dot −1 (dot u (cid:79) dot u))
= Dual (λu → dot −1 (dot (u, u)))
= Dual (λu → (u, u))
= Dual (λu → dup u)
= Dual dup

-- speciﬁcation
-- deﬁnition of jam on Cont
-- deﬁnition of asDual
-- deﬁnition of onDot
-- deﬁnition of (◦) on functions
-- deﬁnition of dup for functions
-- deﬁnition of join
-- Lemma 7, part 5
-- dot −1 ◦ dot = id
-- deﬁnition of dup on functions
-- η-reduction

Finally, scaling:

scale s

= asDual (scale s)
= asDual (Cont (scale s))
= Dual (onDot (scale s))
= Dual (dot −1 ◦ scale s ◦ dot)
= Dual (scale s ◦ dot −1 ◦ dot)
= Dual (scale s)

-- speciﬁcation
-- deﬁnition of scale for Cont r
k
-- deﬁnition of asDual
-- deﬁnition of onDot
-- Lemma 7, part 2
-- dot −1 ◦ dot = id

C.5 Corollary 5.1

Given the deﬁnitions in Figure 10,

Dual f (cid:77) Dual g

= (Dual f × Dual g) ◦ dup

-- deﬁnition of ((cid:77))

The Simple Essence of Automatic Diﬀerentiation

33

= Dual (f × g) ◦ dup
-- deﬁnition of (×) for Dual k
= Dual (f × g) ◦ Dual jam -- deﬁnition of dup for Dual k
-- deﬁnition of (◦) for Dual k
= Dual (jam ◦ (f × g))
-- deﬁnition of ((cid:79))
= Dual (f (cid:79) g)

Dual f (cid:79) Dual g

= jam ◦ (Dual f × Dual g)
= jam ◦ Dual (f × g)
= Dual dup ◦ Dual (f × g)
= Dual ((f × g) ◦ dup)
= Dual (f (cid:77) g)

-- deﬁnition of ((cid:79))
-- deﬁnition of (×) for Dual k
-- deﬁnition of jam for Dual k
-- deﬁnition of (◦) for Dual k
-- deﬁnition of ((cid:77))

C.6 Theorem 6

We will need an indexed counterpart to Theorem 2, which says

D (f × g) (a, b) = D f a × D g b

Letting cross = uncurry (×), we can rephrase this theorem:

D (f × g)

= λ(a, b) → D f a × D g b
= λ(a, b) → cross (D f a, D g b)
= λ(a, b) → cross ((D f × D g) (a, b))
= cross ◦ (D f × D g)

-- Theorem 2
-- deﬁnition of cross
-- deﬁnition of (×) on functions
-- deﬁnition of (◦) on functions

Likewise, extend from binary to n-ary:

Theorem 8 (indexed cross rule)

D (crossI fs) = crossI ◦ crossI (fmap D fs)

If fs :: h (a → b), then both sides of this equation have type h a → (h a (cid:40) h b). The proof is similar to Theorem
2 [Spivak, 1965, variant of Theorem 2-3 (3)].

Theorem 8 gives us what we need to construct D+ (crossI fs) compositionally:

Corollary 8.1 D+ is compositional with respect to crossI . Speciﬁcally,

D+ (crossI fs) = second crossI ◦ unzip ◦ crossI (fmap D+ fs)

The proof is analogous to that of Corollary 2.1:

D+ (crossI fs) as

= (crossI fs as, D (crossI fs) as)
= (crossI fs as, crossI (crossI (fmap D fs) as))
= second crossI (crossI fs as, crossI (fmap D fs) as)
= second crossI ((crossI fs (cid:77) crossI (fmap D fs)) as)
= (second crossI ◦ (crossI fs (cid:77) crossI (fmap D fs))) as
= (second crossI ◦ unzip ◦ crossI (zipWith ((cid:77)) fs (fmap D fs))) as
= (second crossI ◦ unzip ◦ crossI (fmap D+ fs)) as

-- deﬁnition of D+
-- Theorem 8
-- deﬁnition of second (Figure 13)
-- deﬁnition of ((cid:77)) on functions
-- deﬁnition of (◦) on functions
-- Lemma 9 below
-- deﬁnition of D+

For the second-to-last step,

Lemma 9 crossI fs (cid:77) crossI gs = unzip ◦ crossI (zipWith ((cid:77)) fs gs).

For now, let’s prove just the binary version of this lemma, namely

(f × f (cid:48)) (cid:77) (g × g (cid:48)) = transpose ◦ ((f (cid:77) g) × (g (cid:48) (cid:77) g (cid:48)))

34

where

transpose :: ((a × b) × (c × d )) → ((a × c) × (b × d ))
transpose ((a, b), (c, d )) = ((a, c), (b, d ))

Proof:

(f × f (cid:48)) (cid:77) (g × g (cid:48))

Conal Elliott

= (inl ◦ f (cid:79) inr ◦ f (cid:48)) (cid:77) (inl ◦ g (cid:79) inr ◦ g (cid:48))
= (inl ◦ f (cid:77) inl ◦ g) (cid:79) (inr ◦ f (cid:48) (cid:77) inr ◦ g (cid:48))
= transpose ◦ inl ◦ (f (cid:77) g) (cid:79) transpose ◦ inr ◦ (f (cid:48) (cid:77) g (cid:48))
= transpose ◦ (inl ◦ (f (cid:77) g) (cid:79) inr ◦ (f (cid:48) (cid:77) g (cid:48)))
= transpose ◦ ((f (cid:77) g) × (f (cid:48) (cid:77) g (cid:48)))

-- [Macedo and Oliveira, 2013, Equation (17)]
-- exchange law [Gibbons, 2002, Section 1.5.4]
-- Lemma 10 below
-- [Gibbons, 2002, Section 1.5.2]
-- [Macedo and Oliveira, 2013, Equation (17)]

For the third step, we need two more properties.

Lemma 10

inl ◦ f (cid:77) inl ◦ g = transpose ◦ inl ◦ (f (cid:77) g)
inr ◦ f (cid:77) inr ◦ g = transpose ◦ inr ◦ (f (cid:77) g)

Below is a proof in the (→) category, which suﬃce for our purpose. (To do: does the property hold for general
biproduct categories?)

inl ◦ f (cid:77) inl ◦ g

= λa → (inl ◦ f (cid:77) inl ◦ g) a
= λa → (inl (f a), inl (g a))
= λa → ((f a, 0), (g a, 0))
= λa → transpose ((f a, g a), (0, 0))
= λa → transpose ((f a, g a), 0)
= λa → transpose (inl (f a, g a))
= transpose ◦ inl ◦ (f (cid:77) g)

-- η-expand
-- deﬁnition of ((cid:77)) for functions
-- deﬁnition of inl for functions
-- deﬁnition of transpose
-- deﬁnition of 0 for pairs
-- deﬁnition of inl for functions
-- deﬁnition of (◦) for functions

Similarly for the second property (with inr ), noting that ((0, f a), (0, g a)) = transpose ((0, 0), (f a, g a)).

The CartesianI and CocartesianI instances follow from linearity (Theorem 3).

References

Martin Abadi, Paul Barham, Jianmin Chen, Zhifeng Chen, Andy Davis, Jeﬀrey Dean, Matthieu Devin, Sanjay
Ghemawat, Geoﬀrey Irving, Michael Isard, Manjunath Kudlur, Josh Levenberg, Rajat Monga, Sherry Moore,
Derek G. Murray, Benoit Steiner, Paul Tucker, Vijay Vasudevan, Pete Warden, Martin Wicke, Yuan Yu, and
Xiaoqiang Zheng. TensorFlow: A system for large-scale machine learning. In 12th USENIX Symposium on
Operating Systems Design and Implementation (OSDI 16), pages 265–283, 2016.

Andrew W. Appel. Compiling with Continuations. Cambridge University Press, 2007.

Steve Awodey. Category theory, volume 49 of Oxford Logic Guides. Oxford University Press, 2006.

Richard Bird and Oege de Moor. The Algebra of Programming. Prentice-Hall, 1996.

Max Bolingbroke. Constraint kinds for GHC. Blog post, September 2011. http://blog.omega-prime.co.uk/

2011/09/10/constraint-kinds-for-ghc/.

François Chollet. Keras resources. GitHub repository, 2016. URL https://github.com/fchollet.

Thomas H. Cormen, Charles E. Leiserson, Ronald L. Rivest, and Cliﬀord Stein. Introduction to Algorithms,

Third Edition. The MIT Press and McGraw-Hill Book Company, 2001.

Olivier Danvy and Lasse R. Nielsen. Defunctionalization at work. In Proceedings of the 3rd ACM SIGPLAN
International Conference on Principles and Practice of Declarative Programming, PPDP ’01, pages 162–174,
2001.

The Simple Essence of Automatic Diﬀerentiation

35

Conal Elliott. Beautiful diﬀerentiation. In International Conference on Functional Programming (ICFP), 2009.

Conal Elliott. Compiling to categories. Proceedings of the ACM on Programming Languages, 1(ICFP), September

2017.

Conal Elliott. The simple essence of automatic diﬀerentiation. Proceedings of the ACM on Programming

Languages, 2(ICFP), September 2018.

Brendan Fong, David I. Spivak, and Rémy Tuyéras. Backprop as functor: A compositional perspective on

supervised learning. CoRR, abs/1711.10455, 2017.

Jeremy Gibbons. Calculating functional programs. In Algebraic and Coalgebraic Methods in the Mathematics of

Program Construction, volume 2297 of Lecture Notes in Computer Science. Springer-Verlag, 2002.

Andy Gill. Type-safe observable sharing in Haskell. In Proceedings of the 2nd ACM SIGPLAN Symposium on

Haskell, Haskell ’09, 2009.

Ian Goodfellow, Yoshua Bengio, and Aaron Courville. Deep Learning. MIT Press, 2016.

Andreas Griewank. On automatic diﬀerentiation. In In Mathematical Programming: Recent Developments and

Applications, 1989.

Andreas Griewank and Andrea Walther. Evaluating Derivatives. Principles and Techniques of Algorithmic

Diﬀerentiation. Society for Industrial and Applied Mathematics, second edition, 2008.

Joe Hermaszewski and Ben Gamari. vector-sized, 2017. URL http://github.com/expipiplus1/vector-sized.

Haskell library.

Ralf Hinze. Memo functions, polytypically! In 2nd Workshop on Generic Programming, pages 17–32, 2000.

T. C. Hu and M. T. Shing. Computation of matrix chain products, part i, part ii. Technical Report STAN-CS-

TR-81-875, Stanford University, Department of Computer Science, 1981.

Yangqing Jia, Evan Shelhamer, Jeﬀ Donahue, Sergey Karayev, Jonathan Long, Ross B. Girshick, Sergio
Guadarrama, and Trevor Darrell. Caﬀe: Convolutional architecture for fast feature embedding. CoRR,
abs/1408.5093, 2014.

Jerzy Karczmarczuk. Functional coding of diﬀerential forms. In Scottish Workshop on Functional Programming,

1999.

Jerzy Karczmarczuk. Adjoint codes in functional framework, 2000.

Jerzy Karczmarczuk. Functional diﬀerentiation of computer programs. Higher-Order and Symbolic Computation,

14(1), 2001.

Andrew Kennedy. Compiling with continuations, continued. In ACM SIGPLAN International Conference on

Functional Programming, October 2007.

Oleg Kiselyov and Chung-chieh Shan. Functional pearl: Implicit conﬁgurations—or, type classes reﬂect the

values of types. In Proceedings of the 2004 ACM SIGPLAN Workshop on Haskell, Haskell ’04, 2004.

Edward Kmett. The adjunctions package. https://hackage.haskell.org/package/adjunctions, 2011.

Haskell library.

Edward Kmett, Barak Pearlmutter, and Jeﬀrey Mark Siskind. The ad package. https://hackage.haskell.

org/package/ad, 2010. Haskell library.

Joachim Lambek. From λ-calculus to cartesian closed categories. In J.P. Seldin and J.R. Hindley, editors, To

H.B. Curry: Essays on Combinatory Logic, Lambda Calculus, and Formalism. Academic Press, 1980.

Joachim Lambek. Cartesian closed categories and typed lambda-calculi. In Thirteenth Spring School of the

LITP on Combinators and Functional Programming Languages, pages 136–175, 1986.

36

Conal Elliott

Serge Lang. Linear Algebra. Springer-Verlag, 3rd edition, 1987.

F. William Lawvere and Stephen H. Schanuel. Conceptual Mathematics: A First Introduction to Categories.

Cambridge University Press, 2nd edition, 2009.

Yann Lecun, Yoshua Bengio, and Geoﬀrey Hinton. Deep learning. Nature, 521(7553):436–444, 5 2015. ISSN

0028-0836.

Seppo Linnainmaa. The representation of the cumulative rounding error of an algorithm as a taylor expansion of

the local rounding errors. Master’s thesis, University of Helsinki, 1970.

Saunders Mac Lane. Categories for the Working Mathematician. Graduate Texts in Mathematics. Springer New

York, 1998.

Hugo Daniel Macedo and José Nuno Oliveira. Typing linear algebra: A biproduct-oriented approach. Science of

Computer Programming, 78(11):2160–2191, 2013.

José Pedro Magalhães, Atze Dijkstra, Johan Jeuring, and Andres Löh. A generic deriving mechanism for Haskell.

In Haskell Symposium, pages 37–48, 2010.

José Pedro Magalhães et al. GHC.Generics, 2011. URL https://wiki.haskell.org/GHC.Generics. Haskell

wiki.

Uwe Naumann. Optimal Jacobian accumulation is NP-complete. Mathematical Programming, 112:427–441, 2008.

Chris Olah. Neural networks, types, and functional programming. Blog post, sep 2015. http://colah.github.

io/posts/2015-09-NN-Types-FP/.

José Nuno Oliveira. Program Design by Calculation. Draft of textbook in preparation, 2018.

Barak A. Pearlmutter and Jeﬀrey Mark Siskind. Lazy multivariate higher-order forward-mode AD. In Proceedings
of the 34th Annual ACM SIGPLAN-SIGACT Symposium on Principles of Programming Languages, POPL
’07, 2007.

Barak A. Pearlmutter and Jeﬀrey Mark Siskind. Reverse-mode AD in a functional framework: Lambda the

ultimate backpropagator. ACM TOPLAS, 30(2), March 2008.

Simon L. Peyton Jones, Simon Marlow, and Conal Elliott. Stretching the storage manager: Weak pointers and

stable names in Haskell. In Implementation of Functional Languages, 1999.

Marian Boykan Pour-El and Ian Richards. Diﬀerentiability properties of computable functions—A summary.

Acta Cybernetica, 4(1):123–125, 1978.

Marian Boykan Pour-El and Ian Richards. Computability and noncomputability in classical analysis. Transactions

of the American Mathematical Society, 275(2):539–560, 1983.

Louis B. Rall. Automatic Diﬀerentiation: Techniques and Applications. Springer-Verlag, 1981.

John C. Reynolds. Deﬁnitional interpreters for higher-order programming languages. In Reprinted from the

proceedings of the 25th ACM National Conference, pages 717–740. ACM, 1972.

Emily Riehl. Category Theory in Context. Dover Publications, 2016.

R. Tyrrell Rockafellar. Characterization of the subdiﬀerentials of convex functions. Paciﬁc Journal of Mathematics,

17(3):497–510, 1966.

David E. Rumelhart, Geoﬀrey E. Hinton, and Ronald J. Williams. Learning representations by back-propagating

errors. In Neurocomputing: Foundations of Research. MIT Press, 1988.

Jeﬀrey Mark Siskind and Barak A. Pearlmutter. Nesting forward-mode AD in a functional framework. Higher

Order Symbolic Computation, 21(4):361–376, 2008.

The Simple Essence of Automatic Diﬀerentiation

37

Bert Speelpenning. Compiling fast partial derivatives of functions given by algorithms. PhD thesis, University of

Illinois at Urbana-Champaign, 1980.

Michael Spivak. Calculus on Manifolds: A Modern Approach to Classical Theorems of Advanced Calculus.

Addison-Wesley, 1965.

Mitchell Wand. Continuation-based program transformation strategies. Journal of the ACM, 27(1):164–180,

1980.

R. E. Wengert. A simple automatic derivative evaluation program. Communications of the ACM, 7(8):463–464,

1964.

