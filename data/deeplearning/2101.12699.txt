1
2
0
2

p
e
S
8

]

G
L
.
s
c
[

3
v
9
9
6
2
1
.
1
0
1
2
:
v
i
X
r
a

Exploring Deep Neural Networks via Layer-Peeled
Model: Minority Collapse in Imbalanced Training

Cong Fanga, Hangfeng Heb, Qi Longc, and Weijie J. Sud,1

aDepartment of Key Laboratory of Machine Perception (MOE), Peking University; bDepartment of Computer and Information Science, University of Pennsylvania; cDepartment
of Biostatistics, Epidemiology and Informatics, University of Pennsylvania; dDepartment of Statistics and Data Science, University of Pennsylvania

This manuscript was compiled on September 8, 2021

In this paper, we introduce the Layer-Peeled Model, a nonconvex yet
analytically tractable optimization program, in a quest to better un-
derstand deep neural networks that are trained for a sufﬁciently long
time. As the name suggests, this new model is derived by isolat-
ing the topmost layer from the remainder of the neural network, fol-
lowed by imposing certain constraints separately on the two parts
of the network. We demonstrate that the Layer-Peeled Model, albeit
simple, inherits many characteristics of well-trained neural networks,
thereby offering an effective tool for explaining and predicting com-
mon empirical patterns of deep learning training. First, when work-
ing on class-balanced datasets, we prove that any solution to this
model forms a simplex equiangular tight frame, which in part ex-
plains the recently discovered phenomenon of neural collapse (1).
More importantly, when moving to the imbalanced case, our analysis
of the Layer-Peeled Model reveals a hitherto unknown phenomenon
that we term Minority Collapse, which fundamentally limits the perfor-
mance of deep learning models on the minority classes. In addition,
we use the Layer-Peeled Model to gain insights into how to mitigate
Minority Collapse. Interestingly, this phenomenon is ﬁrst predicted
by the Layer-Peeled Model before being conﬁrmed by our computa-
tional experiments.

deep learning | surrogate model | optimization | class imbalance

1. Introduction

In the past decade, deep learning has achieved remarkable
performance across a range of scientiﬁc and engineering do-
mains (2–4). Interestingly, these impressive accomplishments
were mostly achieved by heuristics and tricks, though often
plausible, without much principled guidance from a theoretical
perspective. On the ﬂip side, however, this reality suggests
the great potential a theory could have for advancing the
development of deep learning methodologies in the coming
decade.

Unfortunately, it is not easy to develop a theoretical foun-
dation for deep learning. Perhaps the most diﬃcult hurdle lies
in the nonconvexity of the optimization problem for training
neural networks, which, loosely speaking, stems from the inter-
action between diﬀerent layers of neural networks. To be more
precise, consider a neural network for K-class classiﬁcation (in
logits), which in its simplest form reads∗

f (x; Wfull) = bL+WLσ (bL−1 + WL−1σ(· · · σ(b1 + W1x) · · · )) .

Here, Wfull := {W1, W2, . . . , WL} denotes the weights of the
L layers, {b1, b2, . . . , bL} denotes the biases, and σ(·) is a
nonlinear activation function such as the ReLU. Owing to
the complex and nonlinear interaction between the L layers,

when applying stochastic gradient descent to the optimization
problem

min
Wfull

1
N

K
X

nkX

k=1

i=1

L(f (xk,i; Wfull), yk) +

λ
2

kWfullk2

[1]

with a loss function L for training the neural network, it
becomes very diﬃcult to pinpoint how a given layer inﬂuences
the output f (above, {xk,i}nk
i=1 denotes the training examples
in the k-th class, with label yk, N = n1 + · · · + nK is the
total number of training examples, λ > 0 is the weight decay
parameter, and k · k throughout the paper is the ‘2 norm).
Worse, this diﬃculty in analyzing deep learning models is
compounded by an ever growing number of layers.

Therefore, any attempt to develop a tractable and com-
prehensive theory for demystifying deep learning would pre-
sumably ﬁrst need to simplify the interaction between a large
number of layers. Following this intuition, in this paper we
introduce the following optimization program as a surrogate
model for Eq. (1) with the goal of unveiling quantitative pat-
terns of deep neural networks:

min
WL,H

s.t.

1
N

1
K

K
X

nkX

k=1

i=1

L(WLhk,i, yk)

K
X

k=1

kwkk2 ≤ EW ,

1
K

K
X

k=1

1
nk

nkX

i=1

khk,ik2 ≤ EH ,

[2]

Signiﬁcance Statement

The remarkable development of deep learning over the past
decade relies heavily on sophisticated heuristics and tricks.
To better exploit its potential in the coming decade, perhaps
a rigorous framework for reasoning about deep learning is
needed, which however is not easy to build due to the intricate
details of neural networks. For near-term purposes, a practical
alternative is to develop a mathematically tractable surrogate
model yet maintaining many characteristics of neural networks.
This paper proposes a model of this kind that we term the Layer-
Peeled Model. The effectiveness of this model is evidenced
by, among others, its ability to reproduce a known empirical
pattern and to predict a hitherto unknown phenomenon when
training deep learning models on imbalanced datasets.

C.F., H.H, Q.L., and W.J.S. designed research; C.F., H.H, and W.J.S. performed research and
analyzed data; and C.F., H.H, Q.L., and W.J.S. wrote the paper.

The authors declare no competing interest.

∗

The softmax step is implicitly included in the loss function and we omit other operations such as
max-pooling for simplicity.

1To whom correspondence should be addressed. E-mail: suw@wharton.upenn.edu.

PNAS | September 10, 2021 |

vol. XXX | no. XX | 1–31

 
 
 
 
 
 
where WL = [w1, . . . , wK ]> ∈ RK×p is, as in Eq. (1), com-
prised of K linear classiﬁers in the last layer, H = [hk,i : 1 ≤
k ≤ K, 1 ≤ i ≤ nk] ∈ Rp×N corresponds to the p-dimensional
last-layer activations/features of all N training examples, and
EH and EW are two positive scalars. Note that the bias terms
are omitted for simplicity. Although still nonconvex, this new
optimization program is presumably much more amenable to
analysis than the old one Eq. (1) as the interaction now is
only between two layers.

Balanced Data. Roughly speaking, neural collapse (1) refers
to the emergence of certain geometric patterns of the last-layer
features σ(WL−1σ(· · · σ(W1xk,i) · · · )) and the last-layer clas-
siﬁers WL, when the neural network for balanced classiﬁcation
problems is well-trained in the sense that it is toward not only
zero misclassiﬁcation error but also negligible† cross-entropy
loss. Speciﬁcally, the authors observed the following properties
in their massive experiments: the last-layer features from the
same class tend to be very close to their class mean; these
K class means centered at the global-mean have the same
length and form the maximally possible equal-sized angles
between any pair; moreover, the last-layer classiﬁers become
dual to the class means in the sense that they are equal to
each other for each class up to a scaling factor. See a more
precise description in Section B.

While it seems hopeless to rigorously prove neural collapse
for multiple-layer neural networks Eq. (1) at the moment,
alternatively, we seek to show that this phenomenon emerges
in the surrogate model Eq. (2). More precisely, when the size of
each class nk = n for all k, is it true that any global minimizer
W ?
k,i : 1 ≤ k ≤ K, 1 ≤ i ≤ n] of
Eq. (2) exhibits neural collapse? The following result answers
this question in the aﬃrmative:

K ]> , H ? = [h?

1 , . . . , w?

L = [w?

Finding 1. Neural collapse occurs in the Layer-Peeled Model.

A formal statement of this result and a detailed discussion

are given in Section 3.

This result applies to a family of loss functions L, par-
ticularly including the cross-entropy loss and the contrastive
loss (see, e.g., (10)). As an immediate implication, this re-
sult provides evidence of the Layer-Peeled Model’s ability to
characterize well-trained deep learning models.

Imbalanced Data. While a surrogate model would be satisfac-
tory if it explains some already observed phenomenon, we
set a higher standard for the model, asking whether it can
predict a new common empirical pattern. Encouragingly, the
Layer-Peeled Model happens to meet this standard. Speciﬁ-
cally, we consider training deep learning models on imbalanced
datasets, where some classes contain many more training ex-
amples than others. Despite the pervasiveness of imbalanced
classiﬁcation in many practical applications (11), the liter-
ature remains scarce on its impact on the trained neural
networks from a theoretical standpoint. Here we provide
mathematical insights into this problem by using the Layer-
Peeled Model. In the following result, we consider optimal
solutions to the Layer-Peeled Model on a dataset with two
diﬀerent class sizes: the ﬁrst KA majority classes each contain
nA training examples (n1 = n2 = · · · = nKA = nA), and
the remaining KB := K − KA minority classes each contain
nB examples (nKA+1 = nKA+2 = · · · = nK = nB). We call
R := nA/nB > 1 the imbalance ratio.

Finding 2.
the last-layer
In the Layer-Peeled Model,
classiﬁers corresponding to the minority classes, namely
w?
K , collapse to a single vector when R
is suﬃciently large.

KA+2, . . . , w?

KA+1, w?

This result is elaborated on in Section 4. The derivation
involves some novel elements to tackle the nonconvexity of the
Layer-Peeled Model Eq. (2) and the asymmetry due to the
imbalance in class sizes.

†

Strictly speaking, in the presence of an ‘2 regularization term, which is equivalent to weight decay,
the cross-entropy loss evaluated at any global minimizer of Eq. (1) is bounded away from 0.

(a) 1-Layer-Peeled Model

(b) 2-Layer-Peeled Model

Fig. 1. Illustration of Layer-Peeled Models. The right panel represents the 2-Layer-
Peeled Model, which is discussed in Section 6. For each panel, we preserve the
details of the white (top) box, whereas the gray (bottom) box is modeled by a simple
decision variable for every training example.

In relating Eq. (2) to Eq. (1), a ﬁrst simple observa-
tion is that f (xk,i; Wfull) = WLσ(WL−1σ(· · · σ(W1xk,i) · · · ))
in Eq. (1) is replaced by WLhk,i in Eq. (2). Put diﬀer-
ently, the black-box nature of the last-layer features, namely
σ(WL−1σ(· · · σ(W1xk,i) · · · )), is now modeled by a simple de-
cision variable hk,i for each training example, with an overall
constraint on their ‘2 norm. Intuitively speaking, this sim-
pliﬁcation is done by peeling oﬀ the topmost layer from the
neural network. Thus, we call the optimization program (2)
the 1-Layer-Peeled Model, or simply the Layer-Peeled Model.
At a high level, the Layer-Peeled Model takes a top-down
approach to the analysis of deep neural networks. As illus-
trated in Figure 1, the essence of the modeling strategy is to
break down the neural network from top to bottom, speciﬁ-
cally singling out the topmost layer and modeling all bottom
layers collectively as a single variable. In fact, the top-down
perspective that we took in the development of the Layer-
Peeled Model was inspired by a recent breakthrough made
by Papyan, Han, and Donoho (1), who discovered a mathe-
matically elegant and pervasive phenomenon termed neural
collapse in deep learning training. This top-down approach
was also taken in (5–9) to investigate various aspects of deep
learning models.

A. Two Applications. Despite its plausibility, the ultimate test
of the Layer-Peeled Model lies in its ability to faithfully ap-
proximate deep learning models through explaining empirical
observations and, better, predicting new phenomena. In what
follows, we provide convincing evidence that the Layer-Peeled
Model is up to this task by presenting two ﬁndings. To be
concrete, we remark that the results below are concerned with
well-trained deep learning models, which correspond to, in
rough terms, (near) optimal solutions of Eq. (1).

2 |

frame (ETF) up to scaling.

(NC3) Up to scaling, the last-layer classiﬁers each collapse
to the corresponding class means.

(NC4) The network’s decision collapses to simply choosing
the class with the closest Euclidean distance between its
class mean and the activations of the test example.

Now we give the formal deﬁnition of ETF (1, 34).

Deﬁnition 1. A K-simplex ETF is a collection of points in
Rp speciﬁed by the columns of the matrix

r

M ? =

(cid:16)

K
K − 1

P

IK −

1K 1>
K

(cid:17)

,

1
K

where IK ∈ RK×K is the identity matrix, 1K is the ones vector,
and P ∈ Rp×K (p ≥ K)§ is a partial orthogonal matrix such
that P >P = IK .

A common setup of the experiments for validating neural
collapse is the use of the cross-entropy loss with ‘2 regulariza-
tion, which corresponds to weight decay in stochastic gradient
descent. Based on convincing arguments and numerical evi-
dence, (1) demonstrated that the symmetry and stability of
neural collapse improve deep learning training in terms of
generalization, robustness, and interpretability. Notably, these
improvements occur with the benign overﬁtting phenomenon
(see (35–39)) during the terminal phase of training—when the
trained model interpolates the in-sample training data.

In passing, we remark that concurrent works (40–43) pro-
duced neural collapse using diﬀerent surrogate models. In
slightly more detail, (40–42) obtained their models by peeling
oﬀ the topmost layer. The diﬀerence, however, is that (41, 42)
considered models that impose a norm constraint for each
class, as opposed to an overall constraint as employed in the
Layer-Peeled Model. Moreover, (40) analyzed gradient ﬂow
with an unconstrained features model using the squared loss
instead of the cross-entropy loss. The work (43) provided an
insightful perspective for the analysis of neural networks using
convex duality. Relying on a convex formulation that is in the
same spirit as our semideﬁnite programming relaxation, the
authors of (43) observed neural collapse in their ReLU-based
model by leveraging strong duality under certain conditions.

2. Derivation

In this section, we heuristically derive the Layer-Peeled Model
as an analytical surrogate for well-trained neural networks.
Although our derivation lacks rigor, the goal is to reduce the
complexity of the optimization problem Eq. (1) while roughly
2 kWfullk2 corre-
preserving its structure. Notably, the penalty λ
sponds to weight decay used in training deep learning models,
which is necessary for preventing this optimization program
from attaining its minimum at inﬁnity when L is the cross-
entropy loss. For simplicity, we omit the biases in the neural
network f (xk,i; Wfull).

Taking a top-down standpoint, our modeling strategy starts
by singling out the weights WL of the topmost layer and

To be complete, we only require p ≥ K − 1. When p = K − 1, we can choose P such that

(cid:2)P >, 1K

(cid:3) is an orthogonal matrix.

PNAS | September 10, 2021 |

vol. XXX | no. XX | 3

Fig. 2. Minority Collapse predicted by the Layer-Peeled Model (LPM, in dotted lines)
and empirically observed in deep learning (DL, in solid lines) on imbalanced datasets
with KA = 7 and KB = 3. The y-axis denotes the average cosine of the angles
between any pair of the minority classiﬁer w?
K for both LPM and
DL. The datasets we use are subsets of the CIFAR10 datasets (12) and the size
of the majority classes is ﬁxed to 5000. The experiments use VGG13 (13) as the
deep learning architecture, with weight decay (wd) λ = 5 × 10−3, 5 × 10−4. The
prediction is especially accurate in capturing the phase transition point where the
cosine becomes 1 or, equivalently, the minority classiﬁers become parallel to each
other. More details can be found in Section C.

KA+1, . . . , w?

In slightly more detail, we identify a phase transition as
the imbalance ratio R increases: when R is below a threshold,
the minority classes are distinguishable in terms of their last-
layer classiﬁers; when R is above the threshold, they become
indistinguishable. While this phenomenon is merely predicted
by the simple Layer-Peeled Model Eq. (2), it appears in our
computational experiments on deep neural networks. More
surprisingly, our prediction of the phase transition point is
in excellent agreement with the experiments, as shown in
Figure 2.

This phenomenon, which we refer to as Minority Collapse,
reveals the fundamental diﬃculty in using deep learning for
classiﬁcation when the dataset is widely imbalanced, even in
terms of optimization, not to mention generalization. This is
not a priori evident given that neural networks have a large
approximation capacity (see, e.g., (14)). Importantly, Minority
Collapse emerges at a ﬁnite value of the imbalance ratio rather
than at inﬁnity. Moreover, even below the phase transition
point of this ratio, we ﬁnd that the angles between any pair of
the minority classiﬁers are already smaller than those of the
majority classes, both theoretically and empirically.

B. Related Work. There is a venerable line of work attempting
to gain insights into deep learning from a theoretical point
of view (15–29). See also the reviews (30–33) and references
therein.

The work of neural collapse by (1) in this body of work is
particularly noticeable with its mathematically elegant and
convincing insights. In brief, (1) observed the following four
properties of the last-layer features and classiﬁers in deep
learning training on balanced datasets:‡

(NC1) Variability collapse: the within-class variation of
the last-layer features becomes 0, which means that these
features collapse to their class means.

(NC2) The class means centered at their global mean
collapse to the vertices of a simplex equiangular tight

§

‡

See the mathematical description of neural collapse in Theorem 1.

1101001000+Imbalance Ratio (R)-0.200.20.40.60.81Cosinewd=5e-3 (LPM)wd=5e-3 (DL)wd=5e-4 (LPM)wd=5e-4 (DL)rewriting Eq. (1) as

min
WL,H

1
N

+

K
X

nkX

L(WLh(xk,i; W−L), yk)

i=1

[3]

k=1
λ
2

kWLk2 +

λ
2

kW−Lk2,

where the last-layer feature function h(xk,i; W−L)
:=
σ(WL−1σ(· · · σ(W1xk,i) · · · )) and W−L denotes the weights
from all layers but the last layer. From the Lagrangian dual
viewpoint, a minimum of the optimization program above is
also an optimal solution to

min
WL,W−L

1
N

K
X

nkX

k=1

i=1

L(WLh(xk,i; W−L), yk)

[4]

s.t.

kWLk2 ≤ C1, kW−Lk2 ≤ C2,

for some positive numbers C1 and C2.¶ To clear up any
confusion, note that due to its nonconvexity, Eq. (3) may
admit multiple global minima and each in general corresponds
to diﬀerent values of C1, C2. Next, we can equivalently write
Eq. (4) as

min
WL,H

1
N

K
X

nkX

L(WLhk,i, yk)

k=1
s.t. kWLk2 ≤ C1,

i=1

[5]

H ∈ (cid:8)H(W−L) : kW−Lk2 ≤ C2

(cid:9) ,

where H = [hk,i : 1 ≤ k ≤ K, 1 ≤ i ≤ nk] denotes a decision
variable and the function H(W−L) is deﬁned as H(W−L) :=
[h(xk,i; W−L) : 1 ≤ k ≤ K, 1 ≤ i ≤ nk] for any W−L.

To simplify Eq. (5), we make the ansatz that the range of
h(xk,i; W−L) under the constraint kW−Lk2 ≤ C2 is approxi-
mately an ellipse in the sense that
(cid:8)H(W−L) : kW−Lk2 ≤ C2

(cid:9)

(

≈

H :

K
X

k=1

1
nk

nkX

i=1

)

[6]

khk,ik2 ≤ C 0
2

for some C 0
2 > 0. Loosely speaking, this ansatz asserts that H
should be regarded as a variable in an ‘2 space. To shed light
on the rationale behind the ansatz, note that hk,i intuitively
lives in the dual space of W in view of the appearance of the
product W hk,i in the objective. Furthermore, W is in an ‘2
space for the ‘2 constraint on it. Last, note that ‘2 spaces are
self-dual.

Inserting this approximation into Eq. (5), we obtain the
following optimization program, which we call the Layer-Peeled
Model:

For simplicity, above and henceforth we write W := WL ≡
[w1, . . . , wK ]> for the last-layer classiﬁers/weights and the
thresholds EW = C1/K and EH = C 0

2/K.

This optimization program is nonconvex but, as we will
show soon, is generally mathematically tractable for analysis.
On the surface, the Layer-Peeled Model has no dependence
on the data {xk,i}, which however is not the correct picture,
since the dependence has been implicitly incorporated into the
threshold EH .

In passing, we remark that neural collapse does not emerge
if the second constraint of Eq. (7) uses the ‘q norm for any
q 6= 2 (strictly speaking, ‘q is not a norm when q < 1), in
place of the ‘2 norm. This fact in turn justiﬁes in part the
ansatz Eq. (6). This result is formally stated in Proposition 2
in Section 6.

3. Layer-Peeled Model for Explaining Neural Collapse

In this section, we consider training deep neural networks on
a balanced dataset—that is, nk = n for all classes 1 ≤ k ≤ K.
Our main ﬁnding is that the Layer-Peeled Model displays
the neural collapse phenomenon, just as in deep learning
training (1). The proofs are all deferred to SI Appendix.
Throughout this section, we assume p ≥ K −1 unless otherwise
speciﬁed. This assumption is satisﬁed in many popular network
architectures, where p is usually tens or hundreds of times of
K.

A. Cross-Entropy Loss. The cross-entropy loss is perhaps the
most popular loss used in training deep learning models for
classiﬁcation tasks. This loss function takes the form

L(z, yk) = − log

exp(z(k))
k0=1 exp(z(k0))

PK

!

,

[8]

where z(k0) denotes the k0-th entry of the logit z. Recall that
yk is the label of the k-th class and the feature z is set to
W hk,i in the Layer-Peeled Model Eq. (7). In contrast to the
complex deep neural networks, which are often considered a
black-box, the Layer-Peeled Model is much easier to deal with.
As an exemplary use case, the following result shows that
any minimizer of the Layer-Peeled Model Eq. (7) with the
cross-entropy loss admits an almost closed-form expression.

Theorem 1. In the balanced case, any global minimizer W ? ≡
[w?
k,i : 1 ≤ k ≤ K, 1 ≤ i ≤ n] of Eq. (7)
with the cross-entropy loss obeys

K ]> , H ? ≡ [h?

1 , . . . , w?

h?

k,i = Cw?

k = C 0m?
k

[9]

for all 1 ≤ i ≤ n, 1 ≤ k ≤ K, where the constants C =
pEH /EW , C 0 =
K ] forms
a K-simplex ETF speciﬁed in Deﬁnition 1.

EH , and the matrix [m?

1, . . . , m?

√

min
W ,H

s.t.

1
N

1
K

1
K

K
X

nkX

L(W hk,i, yk)

k=1

i=1

K
X

k=1

K
X

k=1

kwkk2 ≤ EW ,

1
nk

nkX

i=1

khk,ik2 ≤ EH .

¶

Denoting by (W ?
and C2 = kW ?

L, W ?
−Lk2.

−L) an optimal solution to Eq. (3), then we can take C1 = kW ?

Lk2

4 |

Remark 2. Note that the minimizers (W ?, H ?)’s are equivalent
to each other up to rotation. This is because of the rational
invariance of simplex ETFs (see P in Deﬁnition 1).

[7]

This theorem demonstrates the highly symmetric geometry
of the last-layer features and weights of the Layer-Peeled
Model, which is precisely the phenomenon of neural collapse.
Explicitly, Eq. (9) says that all within-class (last-layer) features
k,i0 for all 1 ≤ i, i0 ≤ n; next, it also says
are the same: h?
k,i = h?
that the K class-mean features h?
k,i together exhibit

k := h?

 
a K-simplex ETF up to scaling, from which we immediately
conclude that

cos (cid:93)(h?

k, h?

k0 ) = −

1
K − 1

[10]

for any k 6= k0 by Deﬁnition 1;‖ in addition, Eq. (9) also dis-
plays the precise duality between the last-layer classiﬁers and
features. Taken together, these facts indicate that the mini-
mizer (W ?, H ?) satisﬁes exactly (NC1)–(NC3). Last, Prop-
erty (NC4) is also satisﬁed by recognizing that, for any given
k · h,
last-layer features h, the predicted class is arg maxk w?
where a · b denotes the inner product of the two vectors. Note
that the prediction satisﬁes

arg max
k

w?

k · h = arg max

h?

k · h = arg min

kh?

k − hk2.

k

k

Conversely, the presence of neural collapse in the Layer-
Peeled Model oﬀers evidence of the eﬀectiveness of our model
as a tool for analyzing neural networks. To be complete,
we remark that other models were very recently proposed to
justify the neural collapse phenomenon (40–42) (see also (44)).

B. Extensions to Other Loss Functions. In the modern prac-
tice of deep learning, various loss functions are employed to
take into account the problem characteristics. Here we show
that the Layer-Peeled Model continues to exhibit the phe-
nomenon of neural collapse for some popular loss functions.

Contrastive Loss. Contrastive losses have been extensively
used recently in both supervised and unsupervised deep learn-
ing (10, 45–47). These losses pull similar training examples
together in their embedding space while pushing apart dissim-
ilar examples. Here we consider the supervised contrastive
loss (48), which (in the balanced case) is deﬁned through the
last-layer features by introducing Lc as

1
n

n
X

j=1

− log

exp(hk,i · hk,j/τ )
Pn

‘=1 exp(hk,i · hk0,‘/τ )

PK

k0=1

!

,

[11]

where τ > 0 is a parameter. Note that this loss function uses
the label information implicitly. As the loss does not involve
the last-layer classiﬁers explicitly, the Layer-Peeled Model in
this case takes the form∗∗

min
H

s.t.

1
N

1
K

K
X

n
X

Lc(hk,i, yk)

k=1

i=1

K
X

k=1

1
n

n
X

i=1

khk,ik2 ≤ EH .

[12]

We show that this Layer-Peeled Model also exhibits neu-
ral collapse in its last-layer features, even though the label
information is not explicitly explored in the loss.

Theorem 3. Any global minimizer of Eq. (12) satisﬁes

Theorem 3 shows that the contrastive loss in the associated
Layer-Peeled Model does a perfect job in pulling together
training examples from the same class. Moreover, as seen
from the denominator in Eq. (11), minimizing this loss would
intuitively render the between-class inner products of last-layer
features as small as possible, thereby pushing the features to
form the vertices of a K-simplex ETF up to scaling.

Softmax-Based Loss. The cross-entropy loss can be thought
of as a softmax-based loss. To see this, deﬁne the softmax
transform as

"

S(z) =

exp(z(1))
k=1 exp(z(k))

PK

, . . . ,

exp(z(K))
k=1 exp(z(k))

PK

#>

for z ∈ RK . Let g1 be any nonincreasing convex function
and g2 be any nondecreasing convex function, both deﬁned on
(0, 1). We consider a softmax-based loss function that takes
the form

L(z, yk) = g1 (S(z)(k)) +

K
X

k0=1, k06=k

(cid:0)S(z)(k0)(cid:1) .

g2

[14]

Here, S(z)(k) denotes the k-th element of S(z). Taking
g1(x) = − log x and g2 ≡ 0, we recover the cross-entropy loss.
Another example is to take g1(x) = (1 − x)q and g2(x) = xq
for q > 1, which can be implemented in most deep learning
libraries such as PyTorch (49).

We have the following theorem regarding the softmax-based

loss functions in the balanced case.

√

4.

Theorem
>
Assume
K log (cid:0)K 2√
EH EW + (2K − 1)(K − 1)(cid:1).
K−1
For any loss
function deﬁned in Eq. (14), (W ?, H ?) given by Eq. (9) is a
global minimizer of Eq. (7). Moreover, if g2 is strictly convex
and at least one of g1, g2 is strictly monotone, then any global
minimizer must be given by Eq. (9).

EH EW

In other words, neural collapse continues to emerge with
softmax-based losses under mild regularity conditions. The
ﬁrst part of this theorem does not preclude the possibility that
the Layer-Peeled Model admits solutions other than Eq. (9).
When applied to the cross-entropy loss, it is worth pointing
out that this theorem is a weak version of Theorem 1, albeit
more general. Regarding the ﬁrst assumption in Theorem 4,
note that EH and EW would be arbitrarily large if the weight
decay λ in Eq. (1) is suﬃciently small, thereby meeting the
assumption concerning

EH EW in this theorem.

√

We remark that Theorem 4 does not require the convexity
of the loss L. To circumvent the hurdle of nonconvexity, our
proof in SI Appendix presents several novel elements.

In passing, we leave the experimental conﬁrmation of neural

collapse with these loss functions for future work.

h?

k,i =

√

EH m?
k

[13]

4. Layer-Peeled Model for Predicting Minority Collapse

for all 1 ≤ k ≤ K and 1 ≤ i ≤ n, where [m?
a K-simplex ETF.

1, . . . , m?

K ] forms

‖

∗∗

Note that the cosine value − 1
K−1 corresponds to the largest possible angle for any K points
that have an equal ‘2 norm and equal-sized angles between any pair. As pointed out in (1), the
largest angle implies a large-margin solution (6).
In Eq. (11), hk,i ≡ h(xk,i, W−L) depends on the data, whereas in Eq. (12) hk,i’s
form the decision variable H.

Deep learning models are often trained on datasets where there
is a disproportionate ratio of observations in each class (50–
52). For example, in the Places2 challenge dataset (53), the
number of images in its majority scene categories is about
eight times that in its minority classes. Another example is
the Ontonotes dataset for part-of-speech tagging (54), where
the number of words in its majority classes can be more

PNAS | September 10, 2021 |

vol. XXX | no. XX | 5

 
than one hundred times that in its minority classes. While
empirically the imbalance in class sizes often leads to inferior
model performance of deep learning (see, e.g., (11)), there
remains a lack of a solid theoretical footing for understanding
its eﬀect, perhaps due to the complex details of deep learning
training.

In this section, we use the Layer-Peeled Model to seek a
ﬁne-grained characterization of how class imbalance impacts
neural networks that are trained for a suﬃciently long time. In
particular, neural collapse no longer emerges in the presence
of class imbalance (see numerical evidence in Figure S2 in
SI Appendix). Instead, our analysis predicts a phenomenon
we term Minority Collapse, which fundamentally limits the
performance of deep learning especially on the minority classes,
both theoretically and empirically. All omitted proofs are
relegated to SI Appendix.

A. Technique: Convex Relaxation. When it comes to imbal-
anced datasets, the Layer-Peeled Model no longer admits a
simple expression for its minimizers as in the balanced case,
due to the lack of symmetry between classes. This fact results
in, among others, an added burden on numerically computing
the solutions of the Layer-Peeled Model.

To overcome this diﬃculty, we introduce a convex opti-
mization program as a relaxation of the nonconvex Layer-
Peeled Model Eq. (7), relying on the well-known result for
relaxing a quadratically constrained quadratic program as a
semideﬁnite program (see, e.g., (55)). To begin with, deﬁn-
ing hk as the feature mean of the k-th class (i.e., hk :=
1
i=1 hk,i), we introduce a new decision variable X :=
nk
(cid:2)h1, h2, . . . , hK , W >(cid:3)> (cid:2)h1, h2, . . . , hK , W >(cid:3) ∈ R2K×2K . By
deﬁnition, X is positive semideﬁnite and satisﬁes

Pnk

X(k, k) =

1
K

K
X

k=1

khkk2

a
≤

1
K

K
X

k=1

1
nk

nkX

i=1

khk,ik2 ≤ EH

1
K

K
X

k=1

and

1
K

2K
X

k=K+1

X(k, k) =

1
K

K
X

k=1

kwkk2 ≤ EW ,

a
≤ follows from the Cauchy–Schwarz inequality. Thus,
where
we consider the following semideﬁnite programming problem:††

Lemma 1. Assume p ≥ 2K and the loss function L is convex
in its ﬁrst argument. Let X ? be a minimizer of the convex
program (15). Deﬁne (H ?, W ?) as

2, . . . , h?

(cid:2)h?
1, h?
k,i = h?
h?
k,

K , (W ?)>(cid:3) = P (X ?)1/2,
for all 1 ≤ i ≤ n, 1 ≤ k ≤ K,

[16]

where (X ?)1/2 denotes the positive square root of X ? and P ∈
Rp×2K is any partial orthogonal matrix such that P >P = I2K .
Then (H ?, W ?) is a minimizer of Eq. (7). Moreover, if all
k=1 X ?(k, k) = EH , then all the solutions
X ?’s satisfy 1
K
of Eq. (7) are in the form of Eq. (16).

PK

This lemma in eﬀect says that the relaxation does not lead
to any loss of information when we study the Layer-Peeled
Model through a convex program, thereby oﬀering a compu-
tationally eﬃcient tool for gaining insights into the terminal
phase of training deep neural networks on imbalanced datasets.
An appealing feature is that the size of the program (15) is
independent of the number of training examples. Besides, this
lemma predicts that even in the imbalanced case the last-layer
features collapse to their class means under mild conditions.
Therefore, Property (NC1) is satisﬁed (see more discussion
about the condition in SI Appendix).

The assumption of the convexity of L in the ﬁrst argument
is satisﬁed by a large class of loss functions. The condition that
the ﬁrst K diagonal elements of any X ? make the associated
constraint saturated is also not restrictive. For example, we
prove in SI Appendix that this condition is satisﬁed for the
cross-entropy loss. We also remark that Eq. (15) is not the
unique convex relaxation. An alternative is to relax Eq. (7)
via a nuclear norm-constrained convex program (56, 57) (see
more details in SI Appendix).

B. Minority Collapse. With the technique of convex relaxation
in place, now we numerically solve the Layer-Peeled Model on
imbalanced datasets, with the goal of identifying possible non-
trivial patterns. As a worthwhile starting point, we consider a
dataset that has KA majority classes each containing nA train-
ing examples and KB minority classes each containing nB train-
ing examples. That is, assume n1 = n2 = · · · = nKA = nA
and nKA+1 = nKA+2 = · · · = nK = nB. For convenience,
call R := nA/nB > 1 the imbalance ratio. Note that the case
R = 1 reduces to the balanced setting.

min
X∈R2K×2K

K
X

nk
N

k=1
s.t. X (cid:23) 0,

L(zk, yk)

[15]

1
K

K
X

k=1

X(k, k) ≤ EH ,

1
K

2K
X

k=K+1

X(k, k) ≤ EW ,

for all 1 ≤ k ≤ K,
zk = [X(k, K + 1), X(k, K + 2), . . . , X(k, 2K) ]> .

Lemma 1 below relates the solutions of Eq. (15) to that of

Eq. (7).

††

Although Eq. (15) involves a semideﬁnite constraint, it is not a semideﬁnite program in the strict
sense because a semideﬁnite program uses a linear objective function.

6 |

(a) EW = 1, EH = 5

(b) EW = 1, EH = 10

Fig. 3. The average cosine of the angles between any pair of the minority classiﬁer
solved from the Layer-Peeled Model. The average cosine reaches 1 once R is above
some threshold. The total number of classes KA + KB is ﬁxed to 10. The gray
dash-dotted line indicates the value of − 1
K−1 , which is given by Eq. (10). The
between-majority-class angles can still be large even when Minority Collapse emerges.
Notably, our simulation suggests that the minority classiﬁers exhibit an equiangular
frame and so do the majority classiﬁers.

100101102103104Imbalance Ratio (R)-0.200.20.40.60.81CosineKA=3KA=5KA=7100101102103104Imbalance Ratio (R)-0.200.20.40.60.81CosineKA=3KA=5KA=7An important question is to understand how the KB last-
layer minority classiﬁers behave as the imbalance ratio R
increases, as this is directly related to the model performance
on the minority classes. To address this question, we show that
the average cosine of the angles between any pair of the KB
minority classiﬁers in Figure 3 by solving the simple convex
program (15). This ﬁgure reveals a two-phase behavior of the
minority classiﬁers w?

K as R increases:

KA+2, . . . , w?

KA+1, w?

(1) When R < R0 for some R0 > 0, the average between-
minority-class angle becomes smaller as R increases.

(2) Once R ≥ R0, the average between-minority-class angle
become zero and, in addition, the minority classiﬁers have
about the same length. This implies that all the minority
classiﬁers collapse to a single vector.

Above, the phase transition point R0 depends on the class
sizes KA, KB and the thresholds EH , EW . This value becomes
smaller when EW , EH , or the numer of minority classes KB is
smaller while ﬁxing the other parameters (see more numerical
examples in Figure S2 in SI Appendix).

We refer to the phenomenon that appears in the second
phase as Minority Collapse. While it can be expected that the
minority classiﬁers become closer to each other as the level
of imbalance increases, surprisingly, these classiﬁers become
completely indistinguishable once R hits a ﬁnite value. Once
Minority Collapse takes place, the neural network would pre-
dict equal probabilities for all the minority classes regardless
of the input. As such, its predictive ability is by no means
better than a coin toss when conditioned on the minority
classes. This situation would only get worse in the presence
of adversarial perturbations. This phenomenon is especially
detrimental when the minority classes are more frequent in the
application domains than in the training data. Even outside
the regime of Minority Collapse, the classiﬁcation might still
be unreliable if the imbalance ratio is large as the softmax
predictions for the minority classes can be close to each other.
To put the observations in Figure 3 on a ﬁrm footing, we
prove in the theorem below that Minority Collapse indeed
emerges in the Layer-Peeled Model as R tends to inﬁnity.

Theorem 5. Assume p ≥ K and nA/nB → ∞, and ﬁx
KA and KB. Let (H ?, W ?) be any global minimizer of the
Layer-Peeled Model Eq. (7) with the cross-entropy loss. As
R ≡ nA/nB → ∞, we have

lim w?

k − w?

k0 = 0p,

for all KA < k < k0 ≤ K.

To intuitively see why Minority Collapse occurs, ﬁrst note
that the majority classes become the predominant part of the
risk function as the level of imbalance increases. The minimiza-
tion of the objective, therefore, pays too much emphasis on the
majority classiﬁers, encouraging the between-majority-class
angles to grow and meanwhile shrinking the between-minority-
class angles to zero. As an aside, an interesting question for
future work is to prove that w?
k0 are exactly equal for
suﬃciently large R.

k and w?

C. Experiments. At the moment, Minority Collapse is merely a
prediction of the Layer-Peeled Model. An immediate question
thus is: does this phenomenon really occur in real-world neural
networks? At ﬁrst glance, it does not necessarily have to be the
case since the Layer-Peeled Model is a dramatic simpliﬁcation
of deep neural networks.

To address this question, we resort to computational ex-
periments.‡‡ Explicitly, we consider training two network
architectures, VGG and ResNet (58), on the FashionMNIST
(59) and CIFAR10 datasets, and in particular, replace the
dropout layers in VGG with batch normalization (60). As
both datasets have 10 classes, we use three combinations of
(KA, KB) = (3, 7), (5, 5), (7, 3) to split the data into majority
classes and minority classes. In the case of FashionMNIST
(CIFAR10), we let the KA majority classes each contain all
the nA = 6000 (nA = 5000) training examples from the cor-
responding class of FashionMNIST (CIFAR10), and the KB
minority classes each have nB = 6000/R (nB = 5000/R) ex-
amples randomly sampled from the corresponding class. The
rest experiment setup is basically the same as (1). In detail, we
use the cross-entropy loss and stochastic gradient descent with
momentum 0.9 and weight decay λ = 5 × 10−4. The networks
are trained for 350 epochs with a batch size of 128. The initial
learning is annealed by a factor of 10 at 1/3 and 2/3 of the 350
epochs. The only diﬀerence from (1) is that we simply set the
learning rate to 0.1 instead of sweeping over 25 learning rates
between 0.0001 and 0.25. This is because the test performance
of our trained models is already comparable with their best
reported test accuracy. Detailed training and test performance
is displayed in Tables S1 and S2 in SI Appendix.

The results of the experiments above are displayed in Fig-
ure 4. This ﬁgure clearly indicates that the angles between
the minority classiﬁers collapse to zero as soon as R is large
enough. Moreover, the numerical examination in Table 1
shows that the norm of the classiﬁer is constant across the
minority classes. Taken together, these two pieces clearly give
evidence for the emergence of Minority Collapse in these neu-
ral networks, thereby further demonstrating the eﬀectiveness
of our Layer-Peeled Model. Besides, Figure 4 also shows that
the issue of Minority Collapse is compounded when there are
more majority classes, which is consistent with Figure 3.

Next, in order to get a handle on how Minority Collapse
impacts the test accuracy, we plot the results of another nu-
merical study in Figure 5. The setting is the same as Figure 4,
except that now we randomly sample 6 or 5 examples per class
for the minority classes depending on whether the dataset is
FashionMNIST or CIFAR10. The results show that the perfor-
mance of the trained model deteriorates in the test data when
the imbalance ratio R = 1000, when Minority Collapse has
occurred or is about to occur. This is by no means intuitive a
priori as the test performance is only restricted to the minority
classes and a large value of R only leads to more training data
in the majority classes without aﬀecting the minority classes
at all.

It is worthwhile to mention that the emergence of Minority
Collapse would prevent the model from achieving zero training
error. This is because its prediction is uniform over the minor-
ity classes and, therefore, the “argmax” rule does not give the
correct label for a training example from a minority class. As
such, the occurrence of Minority Collapse is a departure from
the terminal phase of deep learning training. While this fact
seems to contradict conventional wisdom on the approxima-
tion power of deep learning, it is important to note that the
constraints in the Layer-Peeled Model or, equivalently, weight
decay in neural networks limits the expressive power of deep
learning models. Besides, it is equally important to recognize

‡‡

Our code is publicly available at https://github.com/HornHehhf/LPM.

PNAS | September 10, 2021 |

vol. XXX | no. XX | 7

Dataset
Network architecture
No. of majority classes
Norm variation

KA = 3
2.7 × 10−5

VGG11
KA = 5
4.4 × 10−8

Variability of the lengths of the minority classiﬁers when R = ∞.

Table 1.
Std(kw?
notes the average. The results indicate that the classiﬁers of the minority classes have almost the same length.

KA = 3
KA = 7
5.4 × 10−5
5.4 × 10−8
Each number in the row of “norm variation” is
Bk) denotes the standard deviation of the lengths of the KB classiﬁers and the denominator de-

Bk), where Std(kw?

Bk)/Avg(kw?

KA = 7
5.2 × 10−8

KA = 3
1.4 × 10−5

KA = 7
6.3 × 10−8

KA = 7
6.0 × 10−8

KA = 3
1.4 × 10−4

ResNet18
KA = 5
5.0−8

VGG13
KA = 5
9.0 × 10−7

ResNet18
KA = 5
3.5 × 10−7

FashionMNIST

CIFAR10

(a) VGG11 on FashionMNIST

(b) VGG13 on CIFAR10

(c) ResNet18 on FashionMNIST

(d) ResNet18 on CIFAR10

Fig. 4. Occurrence of Minority Collapse in deep neural networks. Each curve denotes
the average between-minority-class cosine. We ﬁx KA + KB = 10. In particular,
Figure 4(b) shares the same setting with Figure 2 in Section 1, where the LPM-based
predictions are given by (EW , EH ) such that the two constraints in the Layer-Peeled
Model become active for the weights of the trained networks. For ResNet 18, Minority
Collapse also occurs as long as R is sufﬁciently large. Speciﬁcally, the average
cosine would hit 1 for KA = 7 when R = 5000 on CIFAR10, and when R = 3000
on FashionMNIST.

that the training error, which mostly occurs in the minority
classes, is actually very small when Minority Collapse emerges
since the minority examples only account for a small portion
of the entire training set. In this spirit, the aforementioned
departure is not as signiﬁcant as it appears at ﬁrst glance
since the training error is generally, if not always, not exactly
zero (see, e.g., (1)). From an optimization point of view, a
careful examination indicates that Minority Collapse can be
attributed to the two constraints in the Layer-Peeled Model or
the ‘2 regularization in Eq. (1). For example, Figure 2 shows
that Minority Collapse occurs earlier with a larger value of
λ. However, this issue does not disappear by simply setting
a small penalty coeﬃcient λ as the imbalance ratio can be
arbitrarily large.

5. How to Mitigate Minority Collapse?

In this section, we further exploit the use of the Layer-Peeled
Model in an attempt to lessen the detrimental eﬀect of Mi-
nority Collapse. Instead of aiming to develop a full set of
methodologies to overcome this issue, which is beyond the
scope of the paper, our aim is to evaluate some simple tech-
niques used for imbalanced datasets.

Among many approaches to handling class imbalance in
deep learning (see the review (11)), perhaps the most popular
one is to oversample training examples from the minority
classes (61–64). In its simplest form, this sampling scheme
retains all majority training examples while duplicating each
training example from the minority classes for wr times, where
the oversampling rate wr is a positive integer. Oversampling
in eﬀect transforms the original problem to the minimization
of a new optimization problem by replacing the risk term in
Eq. (1) with

(a) VGG11 on FashionMNIST

(b) VGG13 on CIFAR10

1
nAKA + wrnBKB

" KAX

nAX

k=1

i=1

L(f (xk,i; Wfull), yk)

K
X

nBX

+ wr

k=KA+1

i=1

# [17]

L(f (xk,i; Wfull), yk)

(c) ResNet18 on FashionMNIST

(d) ResNet18 on CIFAR10

Fig. 5. Comparison of the test accuracy on the minority classes between R = 1
and R = 1000. We ﬁx KA + KB = 10 and use nB = 6 (nB = 5) training
examples from each minority class and nA = 6R (nA = 5R) training examples
from each majority class in FashionMNIST (CIFAR10). Note that when R = 1000,
the test accuracy on the minority classes can be lower than 10% because the trained
neural networks misclassify many examples in the minority classes as some majority
classes.

2 kWfullk2. Note that oversam-
while keeping the penalty term λ
pling is closely related to weight adjusting (see more discussion
in SI Appendix).

A close look at Eq. (17) suggests that the neural network
obtained by minimizing this new program might behave as if it
were trained on a (larger) dataset with nA and wrnB examples
in each majority class and minority class, respectively. To
formalize this intuition, as earlier, we start by considering the

8 |

1101001000+Imbalance Ratio (R)-0.200.20.40.60.81CosineKA=3KA=5KA=71101001000+Imbalance Ratio (R)-0.200.20.40.60.81CosineKA=3KA=5KA=71101001000+Imbalance Ratio (R)-0.200.20.40.60.81CosineKA=3KA=5KA=71101001000+Imbalance Ratio (R)-0.200.20.40.60.81CosineKA=3KA=5KA=7KA=3KA=5KA=7-10010203040506070Test Accuracy (%)R=1R=1000KA=3KA=5KA=7-10010203040506070Test Accuracy (%)R=1R=1000KA=3KA=5KA=7-10010203040506070Test Accuracy (%)R=1R=1000KA=3KA=5KA=7-10010203040506070Test Accuracy (%)R=1R=1000Layer-Peeled Model in the case of oversampling:

min
H,W

1
N 0

" KAX

nAX

k=1

i=1

L(W hk,i, yk) + wr

K
X

nBX

#

L(W hk,i, yk)

k=KA+1

i=1

s.t.

1
K

K
X

k=1

kwkk2 ≤ EW ,

[18]

1
K

KAX

k=1

1
nA

nAX

i=1

khk,ik2 +

1
K

K
X

k=KA+1

1
nB

nBX

i=1

khk,ik2 ≤ EH ,

where N 0 := nAKA + wrnBKB.

(a) VGG11 on FashionMNIST

(b) VGG13 on CIFAR10

(c) ResNet18 on FashionMNIST

(d) ResNet18 on CIFAR10

Fig. 6. Effect of oversampling when the imbalance ratio is R = 1000. Each
plot shows the average cosine of the between-minority-class angles. The results
indicate that increasing the oversampling rate would enlarge the between-minority-
class angles.

The following result conﬁrms our intuition that oversam-
pling indeed boosts the size of the minority classes for the
Layer-Peeled Model.

Proposition 1. Assume p ≥ 2K and the loss function L is
convex in the ﬁrst argument. Let X ? be any minimizer of the
convex program (15) with n1 = n2 = · · · = nKA = nA and
nKA+1 = nKA+2 = · · · = nK = wrnB. Deﬁne (H ?, W ?) as

(cid:2)h?
1, h?
2, . . . , h?
h?
k,i = h?
k,
k,i = h?
h?
k,

K , (W ?)>(cid:3) = P (X ?)1/2,
for all 1 ≤ i ≤ nA, 1 ≤ k ≤ KA,
for all 1 ≤ i ≤ nB, KA < k ≤ K,

[19]

where P ∈ Rp×2K is any partial orthogonal matrix such that
P >P = I2K . Then (H ?, W ?) is a global minimizer of the
oversampling-adjusted Layer-Peeled Model Eq. (18). More-
k=1 X ?(k, k) = EH , then all the
over, if all X ?’s satisfy 1
K
solutions of Eq. (18) are in the form of Eq. (19).

PK

Together with Lemma 1, Proposition 1 shows that the
number of training examples in each minority class is now
in eﬀect wrnB instead of nB in the Layer-Peeled Model. In
the special case wr = nA/nB ≡ R, the results show that all
the angles are equal between any given pair of the last-layer

classiﬁers, no matter if they fall in the majority or minority
classes.

We turn to Figure 6 for an illustration of the eﬀects of
oversampling on real-world deep learning models, using the
same experimental setup as in Figure 5. From Figure 6, we see
that the angles between pairs of the minority classiﬁers become
larger as the oversampling rate wr increases. Consequently, the
issue of Minority Collapse becomes less detrimental in terms
of training accuracy as wr increases. This again corroborates
the predictive ability of the Layer-Peeled Model.

Network architecture

No. of majority classes

VGG11
KA = 3 KA = 5 KA = 7

ResNet18
KA = 3 KA = 5 KA = 7

Original (minority)
Oversampling (minority)
Improvement (minority)

Original (overall)
Oversampling (overall)
Improvement (overall)

15.29
41.13
25.84

40.10
58.25
18.15

20.30
57.22
36.92

57.61
76.17
18.56

17.00
30.50
13.50

69.09
73.37
4.28

30.66
37.86
7.20

50.88
55.91
5.03

34.26
53.46
19.20

64.89
74.56
9.67

5.53
8.13
2.60

66.13
67.10
0.97

Table 2. Test accuracy (%) on FashionMNIST when R = 1000. For
example, “Original (minority)” means that the test accuracy is eval-
uated only on the minority classes and oversampling is not used.
When oversampling is used, we report the best test accuracy among
four oversampling rates: 1, 10, 100, and 1000. The best test accuracy
is never achieved at wr = 1000, indicating that oversampling with a
large wr would impair the test performance.

Next, we refer to Table 2 for eﬀect on the test perfor-
mance. The results clearly demonstrate the improvement in
test accuracy using oversampling, with certain choices of the
oversampling rate. The improvement is noticeable on both
the minority classes and all classes.

Behind the results of Table 2, however, it reveals an issue
when addressing Minority Collapse by oversampling. Speciﬁ-
cally, this technique might lead to degradation of test perfor-
mance using a very large oversampling rate wr, which though
can mitigate Minority Collapse. How can we eﬃciently select
an oversampling rate for optimal test performance? More
broadly, Minority Collapse does not seem likely to be fully
resolved by sampling-based approaches alone, and the doors
are widely open for future investigation.

6. Discussion

In this paper, we have developed the Layer-Peeled Model as
a simple yet eﬀective modeling strategy toward understand-
ing well-trained deep neural networks. The derivation of this
model follows a top-down strategy by isolating the last layer
from the remaining layers. Owing to the analytical and nu-
merical tractability of the Layer-Peeled Model, we provide
some explanation of a recently observed phenomenon called
neural collapse in deep neural networks trained on balanced
datasets (1). Moving to imbalanced datasets, an analysis of
this model suggests that the last-layer classiﬁers corresponding
to the minority classes would collapse to a single vector once
the imbalance level is above a certain threshold. This new
phenomenon, which we refer to as Minority Collapse, occurs
consistently in our computational experiments.

The eﬃcacy of the Layer-Peeled Model in analyzing well-
trained deep learning models implies that the ansatz Eq. (6)—a
crucial step in the derivation of this model—is at least a useful
approximation. Moreover, this ansatz can be further justiﬁed
by the following result in an indirect manner, which, together
with Theorem 1, shows that the ‘2 norm suggested by the

PNAS | September 10, 2021 |

vol. XXX | no. XX | 9

1101001000Oversampling Rate-0.200.20.40.60.81CosineKA=3KA=5KA=71101001000Oversampling Rate-0.200.20.40.60.81CosineKA=3KA=5KA=71101001000Oversampling Rate-0.200.20.40.60.81CosineKA=3KA=5KA=71101001000Oversampling Rate-0.200.20.40.60.81CosineKA=3KA=5KA=7ansatz happens to be the only choice among all the ‘q norms
that is consistent with empirical observations. Its proof is
given in SI Appendix.

Proposition 2. Assume K ≥ 3 and p ≥ K.§§ For any
q ∈ (0, 2) ∪ (2, ∞), consider the optimization problem

min
W ,H

s.t.

1
N

1
K

1
K

K
X

n
X

k=1

i=1

L(W hk,i, yk)

K
X

k=1

K
X

k=1

kwkk2 ≤ EW ,

[20]

1
n

n
X

i=1

khk,ikq

q ≤ EH ,

where L is the cross-entropy loss. Then, any global minimizer
of this program does not satisfy Eq. (9) for any positive numbers
C and C 0. That is, neural collapse does not emerge in this
model.

While the Layer-Peeled Model has demonstrated its notice-
able eﬀectiveness, it requires future investigation for consoli-
dation and extension. First, an analysis of the gap between
the Layer-Peeled Model and well-trained deep learning mod-
els would be a welcome advance. For example, how does
the gap depend on the neural network architectures? How
to take into account the sparsity of the last-layer features
when using the ReLU activation function? From a diﬀerent
angle, a possible extension is to retain multiple layers follow-
ing the top-down viewpoint. Explicitly, letting 1 ≤ m < L
be the number of the top layers we wish to retain in the
model, we can represent the prediction of the neural network
as f (x, Wfull) = f (h(x; W1:(L−m)), W(L−m+1):L) by letting
W1:(L−m) and W(L−m+1):L be the ﬁrst L − m layers and
the last m layers, respectively. Consider the m-Layer-Peeled
Model:

min
W ,H

s.t.

1
N

1
K

1
K

K
X

nkX

k=1

i=1

L(f (hk,i, W(L−m+1):L), yk)

kW(L−m+1):Lk2 ≤ EW ,

K
X

k=1

1
nk

nkX

i=1

khk,ik2 ≤ EH .

The two constraints might be modiﬁed to take into account
the network architectures. An immediate question is whether
this model with m = 2 is capable of capturing new patterns
of deep learning training.

From a practical standpoint, the Layer-Peeled Model to-
gether with its convex relaxation Eq. (15) oﬀers an analytical
and computationally eﬃcient technique to identify and miti-
gate bias induced by class imbalance. An interesting question
is to extend Minority Collapse from the case of two-valued
class sizes to general imbalanced datasets. Next, as suggested
by our ﬁndings in Section 5, how should we choose loss func-
tions in order to mitigate Minority Collapse (64)? Last, a
possible use case of the Layer-Peeled Model is to design more
eﬃcient sampling schemes to take into account fairness con-
siderations (65–67).

§§

See discussion in the case K = 2 in SI Appendix.

10 |

Broadly speaking, insights can be gained not only from the
Layer-Peeled Model but also from its modeling strategy. The
details of empirical deep learning models, though formidable,
can often be simpliﬁed by rendering a certain part of the net-
work modular. When the interest is about the top few layers,
for example, this paper clearly demonstrates the beneﬁts of
taking a top-down strategy for modeling neural networks es-
pecially in consolidating our understanding of previous results
and in discovering new patterns. Owing to its mathematical
convenience, the Layer-Peeled Model shall open the door for
future research extending these beneﬁts.

ACKNOWLEDGMENTS. We are grateful to X.Y. Han for helpful
discussions about some results of (1) and feedback on an early
version of the manuscript. We thank Gang Wen and Qinqing Zheng
for helpful comments. We thank the two anonymous referees for
their constructive comments that helped improve the presentation
of this work. This work was supported in part by NIH through
RF1AG063481, NSF through CAREER DMS-1847415 and CCF-
1934876, an Alfred Sloan Research Fellowship, and the Wharton
Dean’s Research Fund.

1. V Papyan, X Han, DL Donoho, Prevalence of neural collapse during the terminal phase of

deep learning training. Proc. Natl. Acad. Sci. 117, 24652–24663 (2020).

2. A Krizhevsky, I Sutskever, GE Hinton, Imagenet classiﬁcation with deep convolutional neural

networks. Commun. ACM 60, 84–90 (2017).

3. Y LeCun, Y Bengio, G Hinton, Deep learning. Nature 521, 436–444 (2015).
4. D Silver, et al., Mastering the game of go with deep neural networks and tree search. Nature

529, 484–489 (2016).

5. AR Webb, D Lowe, The optimised internal representation of multilayer classiﬁer networks

performs nonlinear discriminant analysis. Neural Networks 3, 367–375 (1990).

6. D Soudry, E Hoffer, MS Nacson, S Gunasekar, N Srebro, The implicit bias of gradient descent

on separable data. The J. Mach. Learn. Res. 19, 2822–2878 (2018).

7. S Oymak, M Soltanolkotabi, Toward moderate overparameterization: Global convergence
guarantees for training shallow neural networks. IEEE J. on Sel. Areas Inf. Theory 1, 84–105
(2020).

8. Y Yu, KHR Chan, C You, C Song, Y Ma, Learning diverse and discriminative representations
via the principle of maximal coding rate reduction. Adv. Neural Inf. Process. Syst. 33 (2020).
9. O Shamir, Gradient methods never overﬁt on separable data. arXiv:2007.00028 (10 Sep

2020).

10. T Chen, S Kornblith, M Norouzi, G Hinton, A simple framework for contrastive learning of vi-
sual representations in Proceedings of the 37th International Conference on Machine Learn-
ing, Proceedings of Machine Learning Research, eds. HD III, A Singh. (PMLR), Vol. 119, pp.
1597–1607 (2020).

11. JM Johnson, TM Khoshgoftaar, Survey on deep learning with class imbalance. J. Big Data 6,

27 (2019).

12. A Krizhevsky, Master’s thesis (University of Toronto) (2009).
13. K Simonyan, A Zisserman, Very deep convolutional networks for large-scale image recogni-

tion in International Conference on Learning Representations. (2015).

14. D Yarotsky, Error bounds for approximations with deep ReLU networks. Neural Networks 94,

103–114 (2017).

15. A Jacot, F Gabriel, C Hongler, Neural tangent kernel: Convergence and generalization in

neural networks in Advances in Neural Information Processing Systems. (2018).

16. SS Du, JD Lee, H Li, L Wang, X Zhai, Gradient descent ﬁnds global minima of deep neural

networks in International Conference on Machine Learning. (2019).

17. Z Allen-Zhu, Y Li, Z Song, A convergence theory for deep learning via over-parameterization

in International Conference on Machine Learning. pp. 2388–2464 (2019).

18. D Zou, Y Cao, D Zhou, Q Gu, Stochastic gradient descent optimizes over-parameterized

deep relu networks in Advances in Neural Information Processing Systems. (2018).

19. L Chizat, E Oyallon, F Bach, On lazy training in differentiable programming in Advances in

Neural Information Processing Systems. (2019).

20. W E, C Ma, L Wu, A comparative analysis of the optimization and generalization property
of two-layer neural network and random feature models under gradient descent dynamics.
arXiv:1904.04326 (21 Feb 2020).

21. P Bartlett, D Foster, M Telgarsky, Spectrally-normalized margin bounds for neural networks.

Adv. Neural Inf. Process. Syst. 30, 6241–6250 (2017).

22. H He, WJ Su, The local elasticity of neural networks in International Conference on Learning

Representations. (2020).

23. T Poggio, A Banburski, Q Liao, Theoretical issues in deep networks. Proc. Natl. Acad. Sci.

117, 30039–30045 (2020).

24. S Mei, A Montanari, PM Nguyen, A mean ﬁeld view of the landscape of two-layer neural

networks. Proc. Natl. Acad. Sci. 115, E7665–E7671 (2018).

25. J Sirignano, K Spiliopoulos, Mean ﬁeld analysis of neural networks: A central limit theorem.

Stoch. Process. their Appl. 130, 1820–1852 (2020).

26. GM Rotskoff, E Vanden-Eijnden, Neural networks as interacting particle systems: Asymptotic
convexity of the loss landscape and universal scaling of the approximation error in Advances
in Neural Information Processing Systems. (2018).

27. C Fang, JD Lee, P Yang, T Zhang, Modeling from features: a mean-ﬁeld framework for over-

parameterized deep neural networks. arXiv:2007.01452 (3 July 2020).

28. R Kuditipudi, et al., Explaining landscape connectivity of low-cost solutions for multilayer nets

(2018).

in Advances in Neural Information Processing Systems. pp. 14601–14610 (2019).

29. B Shi, WJ Su, MI Jordan, On learning rates and Schrödinger operators. arXiv:2004.06977

66. J Zou, L Schiebinger, AI can be sexist and racist—it’s time to make it fair (2018).
67. N Mehrabi, F Morstatter, N Saxena, K Lerman, A Galstyan, A survey on bias and fairness in

(15 Apr 2020).

machine learning. arXiv:1908.09635 (17 Sep 2019).

30. C Fang, H Dong, T Zhang, Mathematical models of overparameterized neural networks.

68. L Bottou, FE Curtis, J Nocedal, Optimization methods for large-scale machine learning. Siam

arXiv:2012.13982 (27 Dec 2020).

Rev. 60, 223–311 (2018).

31. F He, D Tao, Recent advances in deep learning theory. arXiv:2012.10931 (20 Dec 2020).
32. J Fan, C Ma, Y Zhong, A selective overview of deep learning. arXiv:1904.05526 (15 Apr

2019).

69. C Fang, CJ Li, Z Lin, T Zhang, Spider: Near-optimal non-convex optimization via stochastic
path-integrated differential estimator in Advances in Neural Information Processing Systems.
pp. 689–699 (2018).

33. R Sun, Optimization for deep learning: theory and algorithms. arXiv:1912.08957 (19 Dec

70. C Fang, Z Lin, T Zhang, Sharp analysis for nonconvex SGD escaping from saddle points in

2019).

Annual Conference on Learning Theory. pp. 1192–1234 (2019).

34. T Strohmer, RW Heath, Grassmannian frames with applications to coding and communica-

tion. Appl. Comput. Harmon. Analysis 14, 257–275 (2003).

35. S Ma, R Bassily, M Belkin, The power of interpolation: Understanding the effectiveness of
sgd in modern over-parametrized learning in International Conference on Machine Learning.
(PMLR), pp. 3325–3334 (2018).

36. M Belkin, D Hsu, S Ma, S Mandal, Reconciling modern machine-learning practice and the

classical bias–variance trade-off. Proc. Natl. Acad. Sci. 116, 15849–15854 (2019).

37. T Liang, A Rakhlin, Just interpolate: Kernel “ridgeless” regression can generalize. Annals

Stat. 48, 1329–1347 (2020).

38. PL Bartlett, PM Long, G Lugosi, A Tsigler, Benign overﬁtting in linear regression. Proc. Natl.

Acad. Sci. 117, 30063–30070 (2020).

39. Z Li, W Su, D Sejdinovic, Benign overﬁtting and noisy features. arXiv:2008.02901 (6 Aug

2020).

40. DG Mixon, H Parshall, J Pi, Neural collapse with unconstrained features. arXiv:2011.11619

(23 Nov 2020).

41. W E, S Wojtowytsch, On the emergence of tetrahedral symmetry in the ﬁnal and penultimate

layers of neural network classiﬁers. arXiv:2012.05420 (19 Dec 2020).

42. J Lu, S Steinerberger, Neural collapse with cross-entropy loss. arXiv:2012.08465 (18 Jan

2021).

43. T Ergen, M Pilanci, Convex duality of deep neural networks. arXiv preprint arXiv:2002.09773

(22 Feb 2020).

44. T Poggio, Q Liao, Explicit regularization and implicit bias in deep network classiﬁers trained

with the square loss. arXiv:2101.00072 (31 Dec 2020).

45. J Pennington, R Socher, CD Manning, Glove: Global vectors for word representation in
Proceedings of the 2014 conference on empirical methods in natural language processing
(EMNLP). pp. 1532–1543 (2014).

46. N Saunshi, O Plevrakis, S Arora, M Khodak, H Khandeparkar, A theoretical analysis of con-
trastive unsupervised representation learning in Proceedings of the 36th International Confer-
ence on Machine Learning, Proceedings of Machine Learning Research, eds. K Chaudhuri,
R Salakhutdinov. (PMLR), Vol. 97, pp. 5628–5637 (2019).

47. A Baevski, H Zhou, A Mohamed, M Auli, wav2vec 2.0: A framework for self-supervised

learning of speech representations. arXiv:2006.11477 (22 Oct 2020).

48. P Khosla, et al., Supervised contrastive learning. arXiv:2004.11362 (10 Dec 2020).
49. A Paszke, et al., Pytorch: An imperative style, high-performance deep learning library in

Advances in neural information processing systems. pp. 8026–8037 (2019).

50. S Wang, et al., Training deep neural networks on imbalanced data sets in 2016 international

joint conference on neural networks (IJCNN). (IEEE), pp. 4368–4374 (2016).

51. C Huang, Y Li, CC Loy, X Tang, Learning deep representation for imbalanced classiﬁcation in
Proceedings of the IEEE conference on computer vision and pattern recognition. pp. 5375–
5384 (2016).

52. K Madasamy, M Ramaswami, Data imbalance and classiﬁers: impact and solutions from a

big data perspective. Int. J. Comput. Intell. Res. 13, 2267–2281 (2017).

53. B Zhou, A Khosla, A Lapedriza, A Torralba, A Oliva, Places: An image database for deep

scene understanding. arXiv:1610.02055 (6 Oct 2016).

54. E Hovy, M Marcus, M Palmer, L Ramshaw, R Weischedel, Ontonotes:

the 90% solution
in Proceedings of the human language technology conference of the NAACL, Companion
Volume: Short Papers. pp. 57–60 (2006).

55. JF Sturm, S Zhang, On cones of nonnegative quadratic functions. Math. Oper. Res. 28,

246–267 (2003).

56. F Bach, J Mairal, J Ponce, Convex sparse matrix factorizations. arXiv:0812.1869 (10 Dec

2008).

57. BD Haeffele, R Vidal, Structured low-rank matrix factorization: Global optimality, algorithms,
and applications. IEEE transactions on pattern analysis machine intelligence 42, 1468–1482
(2019).

58. K He, X Zhang, S Ren, J Sun, Deep residual learning for image recognition in Proceedings
of the IEEE conference on computer vision and pattern recognition. pp. 770–778 (2016).
59. H Xiao, K Rasul, R Vollgraf, Fashion-mnist: a novel image dataset for benchmarking machine

learning algorithms. arXiv:1708.07747 (15 Sep 2017).

60. S Ioffe, C Szegedy, Batch normalization: Accelerating deep network training by reducing
internal covariate shift in International Conference on Machine Learning. pp. 448–456 (2015).
61. M Buda, A Maki, MA Mazurowski, A systematic study of the class imbalance problem in

convolutional neural networks. Neural Networks 106, 249–259 (2018).

62. J Shu, et al., Meta-weight-net: Learning an explicit mapping for sample weighting in Advances
in Neural Information Processing Systems, eds. H Wallach, et al. (Curran Associates, Inc.),
Vol. 32, pp. 1919–1930 (2019).

63. Y Cui, M Jia, TY Lin, Y Song, S Belongie, Class-balanced loss based on effective number
of samples in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern
Recognition. pp. 9268–9277 (2019).

64. K Cao, C Wei, A Gaidon, N Arechiga, T Ma, Learning imbalanced datasets with label-
Information Processing Systems.

distribution-aware margin loss in Advances in Neural
Vol. 32, pp. 1567–1578 (2019).

65. J Buolamwini, T Gebru, Gender shades: Intersectional accuracy disparities in commercial
gender classiﬁcation in Conference on fairness, accountability and transparency. pp. 77–91

PNAS | September 10, 2021 |

vol. XXX | no. XX | 11

For simplicity, we deﬁne [m1 : m2] := {m1, m1 + 1, . . . , m2} for m1, m2 ∈ N with m1 ≤ m2 and [m2] := [1 : m2] for m2 ≥ 1.

A. Balanced Case.

A.1. Proofs of Theorem 1 and Proposition 2 . Because there are multiplications of variables in the objective function, Eq. (7) is nonconvex.
Thus the KKT condition is not suﬃcient for optimality. To prove Theorem 1, we directly determine the global minimum of Eq. (7).
During this procedure, one key step is to show that minimizing Eq. (7) is equivalent to minimize a symmetric quadratic function:

n
X

  K
X





!>   K
X

hk,i

!

wk

− K

under suitable conditions. The detail is shown below.

i=1

k=1

k=1



h>

k,iwk



K
X

k=1

Proof of Theorem 1. By the concavity of log(·), for any z ∈ RK , k ∈ [K], constants Ca, Cb > 0, letting Cc =

Cb
(Ca+Cb)(K−1)

, we have

− log

!

z(k)

PK

k0=1

z(k0)

= − log(z(k)) + log

!

z(k0)

  K
X

k0=1

= − log(z(k)) + log

Ca
Ca + Cb

(cid:16) (Ca + Cb) z(k)
Ca

(cid:17)

+ Cc

K
X

k0=1,k06=k

!

.

z(k0)
Cc

[21]

Recognizing the equality

Ca
Ca + Cb

+ Cc + · · · + Cc
}
{z
K−1

|

=

Ca
Ca + Cb

+ (K − 1)

Cb
(Ca + Cb)(K − 1)

= 1

and the concavity of log(·), we see that the Jensen inequality gives

log

Ca
Ca + Cb

(cid:16) (Ca + Cb) z(k)
Ca

(cid:17)

+ Cc

K
X

k0=1,k06=k

!

z(k0)
Cc

≥

Ca
Ca + Cb

log

(cid:16) (Ca + Cb) z(k)
Ca

(cid:17)

+ Cc

K
X

k0=1,k06=k

log

(cid:16) z(k0)
Cc

(cid:17)

.

[22]

Plugging this inequality into Eq. (21), we get

− log

!

z(k)

PK

k0=1

z(k0)

≥ − log(z(k)) +

Ca
Ca + Cb

log

(cid:16) (Ca + Cb) z(k)
Ca

(cid:17)

+ Cc

log

(cid:17)

(cid:16) z(k0)
Cc

K
X

k0=1,k06=k
#

= −

Cb
Ca + Cb

"

log(z(k)) −

1
K − 1

K
X

k0=1,k06=k

log(z(k0))

+ Cd,

where the constant Cd := Ca
prove Theorem 1, we set Ca := exp (cid:0)√
the objective of Eq. (7). Applying Eq. (21) to the objective, we have

) + Cb
(cid:1) and Cb := exp (cid:0)−

Ca
EH EW

log( Ca+Cb

Ca+Cb

Ca+Cb

log(1/Cc). Note that in Eq. (21), Ca and Cb can be any positive numbers. To
EH EW /(K − 1)(cid:1), which shall lead to the tightest lower bound for

√

1

N

K
X

n
X

k=1

i=1

L(W hk,i, yk)

≥

Cb
(Ca + Cb)N (K − 1)

n
X

  K
X





!>   K
X

hk,i

!

wk

− K

i=1

k=1

k=1


 + Cd.

h>

k,iwk

K
X

k=1

Deﬁning ¯hi := 1

K

PK

k=1

hk,i for i ∈ [n], it follows from the simple inequality 2ab ≤ a2 + b2 that

n
X

  K
X





!>   K
X

hk,i

!

wk

− K

i=1

k=1

k=1

n
X

=K

K
X

(¯hi − hk,i)>wk



h>

k,iwk



K
X

k=1

i=1

k=1

≥ −

K
2

K
X

n
X

k=1

i=1

k¯hi − hk,ik2/Ce −

CeN
2

K
X

k=1

kwkk2,

[23]

[24]

where we pick Ce := pEH /EW . The two terms in the right hand side of Eq. (24) can be bounded via the constraints of Eq. (7).
Speciﬁcally, we have

CeN
2

K
X

k=1

kwkk2 ≤

√

KN

EH EW
2

,

[25]

12 |

 
 
 
 
and

K
2

K
X

n
X

k=1

i=1

k¯hi − hk,ik2/Ce

a=

K2
2Ce

n
X

i=1

1

K

K
X

k=1

!

khk,ik2 − k¯hik2

≤

K
2Ce

K
X

n
X

k=1

i=1

khk,ik2 ≤

√

KN

EH EW
2

,

where a= uses the fact that Eka − E[a]k2 = Ekak2 − kE[a]k2. Thus plugging Eq. (24), Eq. (25), and Eq. (26) into Eq. (23), we have

1

N

K
X

n
X

k=1

i=1

L(W hk,i, yk) ≥ −

K

Cb
Ca + Cb

√

EH EW
K − 1

+ Cd := L0.

Now we check the conditions that reduce Eq. (27) to an equality.

By the strict concavity of log(·), Eq. (22) reduces to an equality only if

for k0 6= k. Therefore, Eq. (23) reduces to an equality only if

(Ca + Cb) z(k)
Ca

=

z(k0)
Cc

(Ca + Cb)h>

k,iwk

Ca

=

h>

k,iwk0
Cc

.

Recognizing Cc =

Cb
(Ca+Cb)(K−1)

and taking the logarithm of both sides of the above equation, we obtain

hk,iwk = hk,iwk0 + log

(cid:16) Ca(K − 1)
Cb

(cid:17)

,

for all (k, i, k0) ∈ {(k, i, k0) : k ∈ [K], k0 ∈ [K], k0 6= k, i ∈ [n]}. Eq. (24) becomes equality if and only if

¯hi − hk,i = −Cewk,

k ∈ [K], i ∈ [n].

Eq. (25) and Eq. (26) become equalities if and only if:

[26]

[27]

1

K
X

1

n
X

K

n

(cid:13)
(cid:13)hk,i

(cid:13)
2 = EH ,
(cid:13)

1

K
X

K

kwkk2 = EW ,

¯hi = 0p, i ∈ [n].

i=1
Applying Lemma 2 shown in the end of the section, we have (H, W ) satisﬁes Eq. (9).

k=1

k=1

Reversely, it is easy to verify that Eq. (27) reduces to equality when (H, W ) admits Eq. (9). So L0 is the global minimum of Eq. (7)

and Eq. (9) is the unique form for the minimizers. We complete the proof of Theorem 1.

Lemma 2. Suppose (H, W ) satisﬁes

r

¯hi − hk,i = −

EH
EW

wk,

k ∈ [K],

i ∈ [n],

and

1

K

K
X

k=1

1

n

n
X

i=1

(cid:13)
(cid:13)hk,i

(cid:13)
2 = EH ,
(cid:13)

1

K

K
X

k=1

kwkk2 = EW ,

¯hi = 0p, i ∈ [n],

[28]

[29]

hk,i with i ∈ [n]. Moreover, there exists a constant C such that for all (k, i, k0) ∈ {(k, i, k0) : k ∈ [K], k0 ∈ [K], k0 6=

PK
where ¯hi := 1
k, i ∈ [n]}, we have

K

k=1

hk,i · wk = hk,i · wk0 + C.

Then (H, W ) satisﬁes Eq. (9).

Proof. Combining Eq. (28) with the last equality in Eq. (29), we have

Thus it remains to show

where M ? is a K-simplex ETF.
q EW
EH

Plugging hk = hk,i =

and

W =

r

EW
EH

h
h1, . . . , hK

i>

,

hk,i = hk, k ∈ [K], i ∈ [n].

W = p

EW (M ?)> ,

wk into Eq. (30), we have, for all (k, k0) ∈ {(k, k0) : k ∈ [K], k0 ∈ [K], k0 6= k},

r

EH
EW

kwkk2 = hk · wk = hk · wk0 + C =

r

EH
EW

wk · wk0 + C,

r

EH
EW

kwk0 k2 = hk0 · wk0 = hk0 · wk + C =

r

EH
EW

wk0 · wk + C.

[30]

[31]

PNAS | September 10, 2021 |

vol. XXX | no. XX | 13

 
Therefore, from 1
K

PK

k=1

kwkk2 = EW , we have kwkk =

√

Furthermore, recalling that ¯hi = 0p for i ∈ [n], we have PK

follows from hkwk0 = C0 and hkwk =

√

k=1
EH EW that hkwk0 = −

√

EW and hkwk0 = C0 :=

EH EW − C.
hk = 0p, which further yields PK
k=1
√
EH EW /(K − 1). Thus we obtain
(cid:17)i
(cid:16)

h K

1

K − 1

IK −

K

1K 1>
K

,

W [h1, . . . , hK ] = EW

W W > =

r

EW
EH

hk · wk0 = 0 for k0 ∈ [K]. Then it

which implies Eq. (31). We complete the proof.

Proof of Proposition 2. We introduce the set SR as

SR :=






(H, W ) :

√

|B1| ≤

[h1, . . . , hK ] = B1bP (cid:2)(a + 1)IK − 1K 1>
(cid:3) ,
K
W = B2B3b (cid:2)(a + 1)IK − 1K 1>
P >,

(cid:3)>
k ∈ [K], i ∈ [n],

K

EH , |B2| ≤

b ≥ 0, a ≥ 0, bq[aq + (K − 1)] = 1,
EW , B3 ≥ 0, B2
P ∈ Rp×K , P >P = IK .

hk,i = hk,
√

3 b2[a2 + (K − 1)] = 1,






We can examine that SR admits the constraints of Eq. (7). So any (H, W ) ∈ SR is a feasible solution. Moreover, one can observe that
this feasible solution has a special symmetry structure: for each k ∈ [K], the features in class k collapse to their mean hk, i.e., (NC1), and
wk is parallel to hk, i.e., (NC3). However, weights do not form the vertices of ETF unless a = K − 1. Therefore, it suﬃces to show that
the minimizer of 1
N

L(W hk,i, yk) in the set SR do not satisfy a = K − 1.

In fact, for any (H, W ) ∈ SR, the objective function value can be written as a function of B1, B2, B3, a, and b. We have

PK

Pn

k=1

i=1

K
X

n
X

L(W hk,i, yk)

1

N

i=1

k=1
(cid:18)

= − log

exp(B1B2B3b2[a2 + (K − 1)])
exp(B1B2B3b2[a2 + K − 1]) + (K − 1) exp(B1B2B3b2[K − 2 − 2a])

(cid:19)

= − log

(cid:16)

1
1 + (K − 1) exp(−B1B2B3b2(a + 1)2)

(cid:17)

.

Then it follows to maximize B1B2B3b2(a + 1)2 or equivalently (cid:2)B1B2B3b2(a + 1)2(cid:3)2. By B2
we have

3 b2[a2 + (K − 1)] = 1 and bq[aq + (K − 1)] = 1,

(cid:2)B1B2B3b2(a + 1)2(cid:3)2 a

≤ EH EW

= EH EW

(cid:2)B2
(cid:20) (a + 1)2
a2 + (K − 1)

3 b2(a + 1)2(cid:3) (cid:2)b2(a + 1)2(cid:3)
(cid:21) h (a + 1)q
aq + K − 1

i2/q

,

[32]

where

a
≤ picks B1 =

√

EH and B2 =

√

EW . Let us consider function g : [0, +∞) → R : g(x) =

h (x+1)2

x2+(K−1)

i h (x+1)q
xq +K−1

i2/q

. Note that by

the ﬁrst-order optimality, once if g0(K − 1) 6= 0, then Eq. (32) cannot achieve the maximum at a = K − 1, which is our desired result.
Indeed, we have

g0(K − 1) =

2K4
[(K − 1)2 + (K − 1)] [(K − 1)q + K − 1]2/q+1

(cid:2)(K − 1) − (K − 1)q−1(cid:3) .

Therefore, a = K − 1 ≥ 2 is not the maximizer of Eq. (32), unless q = 2. We complete the proof.

Following the proof of Proposition 2, for completeness we discuss the structure of the global minimizers of Program (20) in the case
K = 2. In short, we show that when q ∈ (1, 2) ∪ (2, ∞), although the global minimizers of Eq. (20) remain in the form of Eq. (9), they are
no longer rotationally invariant due to certain constraints on the solutions. This is in contrast to a K-simplex ETF, which is rotationally
invariant (see Deﬁnition 1).

For simplicity of notation, we assume that there is one training example in each class (the case of multiple training examples can be

directly extended). Program (20) in the case K = 2 takes the following form:

− log

min
W ,H

s.t.

(cid:18)

1 h1)

exp(w>
1 h1) + exp(w>
exp(w>
kw1k2 + kw2k2 ≤ 2EW ,
kh1kq
q ≤ 2EH .
q

+ kh2kq

2 h1)

(cid:19)

(cid:18)

− log

2 h2)

exp(w>
1 h2) + exp(w>

2 h2)

exp(w>

(cid:19)

[33]

We show that the optimal solution to (33) satisﬁes some speciﬁc ETF structures. In brief, when q > 2, both the features and weights are
parallel to a certain vector, and when 1 < q < 2, the solution is sparse in the sense that only one entry is nonzero for both the features
and the weights.

Lemma 3. For q > 2, any global minimizer of Eq. (33) satisﬁes
= C1w?
1
(cid:1)1/q

= −h?
h?
2
1
, C2 = (cid:0) EH

where the constants C1 = (cid:0) EH
or −1 (there are in total 2p such vectors). For 1 < q < 2, any global minimizer of Eq. (33) satisﬁes

(cid:1)1/q (cid:0) EW
p

(cid:1)−1/2

p

p

= −C1w?
2

= C2(±1p),

[34]

, and ±1p denotes a p-dimensional vector such that each entry is either 1

where the constants C3 = E1/q

H E−1/2

W

= C3w?
1

h?
= −h?
1
2
and C4 = E1/q
H .

= −C3w?
2 ,

kh?

1k0 = 1,

kh?

1k = C4,

[35]

14 |

[36]

[37]

[38]

[39]

(cid:1)q

≤

[40]

Proof. For any constants Ca, Cb > 0, letting Cc = Cb

, using the same arguments as Eq. (21) and Eq. (23), we have

(cid:18)

− log

Ca+Cb
1 h1)

exp(w>
1 h1) + exp(w>

2 h1)

exp(w>

(cid:19)

(cid:18)

− log

(cid:2)(h1 + h2)>(w1 + w2) − 2(h>

1 w1 + h>

exp(w>

(cid:19)

2 h2)

exp(w>
1 h2) + exp(w>
2 w2)(cid:3) + Cd.

2 h2)

Then it follows that

≥

Cb
Ca + Cb

(h1 + h2)>(w1 + w2) − 2(h>

1 w1 + h>

2 w2) = −(h1 − h2)>(w1 − w2) ≥ − kh1 − h2k kw1 − w2k .

We have

and

kw1 − w2k2 = kw1k2 + kw2k2 − 2w>

1 w2 ≤ 2 kw1k2 + 2 kw2k2 ≤ 4EW

kh1 − h2k2 =

p
X

i=1

|h1(i) − h2(i)|2 ≤

p
X

i=1

(|h1(i)| + |h2(i)|)2

a
≤ 22−2/q

(|h1(i)|q + |h2(i)|q)

#

2
q

,

" p

X

i=1

where h1(i) and h2(i) denotes the i-th entry of h1 and h2, respectively. In
|h1(i)|q +|h2(i)|q
2

since |x|q is strictly convex.

• When 1 < q < 2, we pick Ca = exp

and Cb = 1/Ca. We have

(cid:16)

H E1/2
E1/q

W

(cid:17)

a

≤, we use Jensen’s inequality that (cid:0) |h1(i)|+|h2(i)|

2

p
X

i=1

(|h1(i)|q + |h2(i)|q)

2
q ≤

!2/q

|h1(i)|q + |h2(i)|q

≤ 22/qE2/q
H ,

  p

X

i=1

where the ﬁrst inequality uses that Pp
{xi}p

i=1

i=1

is at most 1. Then by plugging Eq. (38), Eq. (39), and Eq. (40) into Eq. (36), using Eq. (37), we have

|xi|o ≤ (cid:0)Pp

|xi|(cid:1)o for o > 1 and the equality holds if and only if the non-zero elements of

i=1

(cid:18)

− log

1 h1)

exp(w>
1 h1) + exp(w>

2 h1)

exp(w>

(cid:19)

(cid:18)

− log

1 h2)

exp(w>
1 h2) + exp(w>

2 h2)

exp(w>

(cid:19)

≥ −

Cb
Ca + Cb

q

24−4/qEW E2/q
H

+ Cd.

[41]

Now we check the conditions that reduce Eq. (41) to an equality. Eq. (37) reduces to an equality if and only if there exists a constant
C5 > 0 such that h1 − h2 = C5(w1 − w2). Eq. (38) reduces to an equality if and only if w1 = −w2 and kw1k2 = EW . Eq. (39) reduces
to an equality if and only if h1 = −h2. Finally, Eq. (40) reduces to an equality if and only if there is only one non-zero entry i such that
we exactly have |h1(i)|q + |h2(i)|q = 2EH . We can obtain Eq. (34).

When q > 2, we pick Ca = exp

(cid:16)

p (cid:0) EH

p

(cid:1)1/q (cid:0) EW
p

(cid:1)1/2(cid:17)

and Cb = 1/Ca. We have

p
X

(|h1(i)|q + |h2(i)|q)

2
q ≤ p1−2/q

  p

X

!2/q

|h1(i)|q + |h2(i)|q

≤ p1−2/q22/qE2/q
H ,

[42]

i=1
where the ﬁrst inequality uses Jensen’s inequality that (cid:0) 1
|xi|(cid:1)a
|xi|a for a > 1 since |x|a is strictly convex with respect
p
to x, and let a = q/2, and xi = (|h1(i)|q + |h2(i)|q|)2/q ≥ 0. Then by plugging Eq. (38), Eq. (39), and Eq. (42) into Eq. (36), using
Eq. (37), we have

≤ 1
p

Pp

Pp

i=1

i=1

i=1

(cid:18)

(cid:19)

(cid:18)

− log

1 h1)

exp(w>

exp(w>
1 h1) + exp(w>

exp(w>
1 h2) + exp(w>
Now we check the conditions that reduce Eq. (41) to an equality. In fact, by the strict convexity, Eq. (42) reduces to an equality if and
only if |h1(i)|q + |h2(i)|q = |h1(j)|q + |h2(j)|q for all i 6= j and Pp
|h1(i)|q + |h2(i)|q = 2EH . Then by combining the conditions to
reduce Eq. (37), Eq. (38), and Eq. (39) to equalities, we can obtain Eq. (34).

24−4/qp1−2/qEW E2/q
H

Cb
Ca + Cb

exp(w>

2 h1)

2 h2)

1 h2)

+ Cd.

− log

≥ −

[43]

i=1

(cid:19)

q

Figure 7 displays simulation results concerning the last-layer weights for binary classiﬁcation using deep neural networks. The results
show that last-layer weights exhibit neither the all-ones nor the sparse pattern as in Lemma 3, thereby implying that the ‘2 norm is the
best choice among all ‘q norms for modeling deep neural networks using the Layer-Peeled Model.

In the case where q ≤ 1, we conjecture that the ‘q norm regularizer would also render the solution to Eq. (33) sparse. We leave this for

future work.

A.2. Proofs of Theorems 3 and 4. The proofs of Theorems 3 and 4 follow from the similar argument of Theorem 1.

Proof of Theorem 3. For k ∈ [K], i ∈ [n], and k0 ∈ [K], deﬁne

Ek,i,k0 :=

1

n

n
X

j=1

exp(hk,i · hk0,j /τ ).

PNAS | September 10, 2021 |

vol. XXX | no. XX | 15

Fig. 7. Histograms of last-layer weights of VGG11 trained on the ﬁrst two classes in FashionMNIST. Each histogram shows the empirical distribution of all the entries of w1 or
w2. If the prediction of Lemma 3 applied to real neural networks for binary classiﬁcation, then we would observe a mixture of one or two point masses in the histograms, which
however is not the case. There are 6000 examples in each class, and we use the same experimental settings as in Section C. The training and test accuracies are 100% and
99.75%, respectively.

For constants Ca := exp (cid:0)√
we have for j ∈ [n],

EH EW

(cid:1) and Cb := (K − 1) exp (cid:0)−

√

EH EW

(cid:1), let Cc :=

Cb
(Ca+Cb)(K−1)

. Using a similar argument as Eq. (21),

[44]

− log

exp(hk,i · hk,j /τ )

PK

k0=1

Ek,i,k0

!

= − hk,i · hk,j /τ + log

Ca
Ca + Cb

(cid:16) (Ca + Cb) Ek,i,k
Ca

(cid:17)

+ Cc

K
X

k0=1, k06=k

!

Ek,i,k0
Cc

a
≥ − hk,i · hk,j /τ +

Ca
Ca + Cb

log

(cid:16) (Ca + Cb) Ek,i,k
Ca

(cid:17)

+ Cc

K
X

k0=1, k06=k

log

(cid:17)

(cid:16) Ek,i,k0
Cc

b= − hk,i · hk,j /τ +

Ca
Ca + Cb

log (cid:0)Ek,i,k

(cid:1) + Cc

K
X

k0=1, k06=k

log (cid:0)Ek,i,k0

(cid:1) + Cd

c
≥ − hk,i · hk,j /τ +

Ca
(Ca + Cb)n

n
X

‘=1

hk,i · hk,‘/τ +

Cc
n

K
X

n
X

k0=1, k06=k

‘=1

hk,i · hk0,‘/τ + Cd.

a
≥ and

c

where
into the objective function, we have

≥ apply the concavity of log(·) and in b= we deﬁne Cd := Ca

Ca+Cb

log( Ca+Cb

Ca

) + Cb

Ca+Cb

log(1/Cc). Then plugging Eq. (44)

!

[45]

1

N

1

N

K
X

n
X

k=1

i=1

K
X

n
X

k=1

i=1

1

n

1

n

n
X

j=1

n
X

j=1

=

− log

− log

exp(hk,i · hk,j /τ )
Pn

exp(hk,i · hk0,‘)

k0=1

‘=1

PK

exp(hk,i · hk,j /τ )

PK

k0=1

Ek,i,k0

!

+ log(n)

Eq. (44)
≥

CbK
(Ca + Cb)N (K − 1)τ

K
X

n
X

k=1

i=1

−

1

n

n
X

j=1

hk,i · hk,j −

1

K

K
X

k0=1

!!

hk,i · hk0,j

+ Cd + log(n).

16 |

 
 
 
 
 
 
Now deﬁning ¯hi := 1

K

PK

k=1

hk,i for i ∈ [n], a similar argument as Eq. (24) and Eq. (26) gives that

K
X

n
X

k=1

i=1

K
X

n
X

=

k=1

i=1

−

−

1

n

1

n

n
X

j=1

n
X

j=1

hk,i · hk,j −

!!

hk,i · hk0,j

1

K
X

K

k0=1
!

hk,i · (hk,j − ¯hj )

a
≥ −

b
≥ −

1
2

1
2

K
X

n
X

k=1

i=1

K
X

n
X

k=1

i=1

(cid:13)
(cid:13)hk,i

(cid:13)
2
(cid:13)

−

1
2

K
X

n
X

k=1

i=1

(cid:13)
(cid:13)hk,i − ¯hi

(cid:13)
2
(cid:13)

(cid:13)
(cid:13)hk,i

(cid:13)
2
(cid:13)

−

K
2

n
X

i=1

1

K

K
X

k=1

!

(cid:13)
(cid:13)hk,i

(cid:13)
2
(cid:13)

− (cid:13)
¯hi
(cid:13)

(cid:13)
2
(cid:13)

≥ −

K
X

n
X

k=1

i=1

(cid:13)
(cid:13)hk,i

(cid:13)
(cid:13)

2 c

≥ −N EH ,

[46]

a
where
≥ follows from 2ab ≤ a2 + b2,
plugging Eq. (46) into Eq. (45) yields that

b
≥ follows from Eka − E[a]k2 = Ekak2 − kE[a]k2, and

c
≥ uses the constraint of Eq. (11). Therefore,

1

N

≥ −

K
X

n
X

1

n
X

− log

n

k=1

i=1
j=1
CbKEH
(Ca + Cb)(K − 1)τ

exp(hk,i · hk,j /τ )
Pn

exp(hk,i · hk0,‘/τ )

‘=1

PK

k0=1

!

+ Cd + log(n).

[47]

Now we check the conditions that can make Eq. (47) reduce equality. By the strictly concavity of log(·), Eq. (44) reduce to equalities

only if for all (k, i, k0) ∈ {(k, i, k0) : k ∈ [K], k0 ∈ [K], k0 6= k, i ∈ [n]},

Eq. (46) reduce to equalities if and only if:

Ek,i,k
Ca(K − 1)

=

Ek,i,k0
Cb

.

hk,i = hk, i ∈ [n], k ∈ [K],

1

K

K
X

k=1

khkk2 = EH ,

K
X

k=1

hk = 0p.

Plugging hk,i = hk into Eq. (48), we have for (k, k0) ∈ {k, k0 : k ∈ [K], k0 ∈ [K], k0 6= k},

exp(khkk2)
Ca(K − 1)

=

exp(hk · hk0 )
Cb

=

exp(khk0 k2)
Ca(K − 1)

.

Then it follows from 1
K

PK

k=1

khkk2 = EH that khkk2 = EH for k ∈ [K]. Moreover, since PK

k=1

hk = 0p, we obtain

for (k, k0) ∈ {k, k0 : k ∈ [K], k0 ∈ [K], k0 6= k}. Therefore,

[h1, . . . , hK ]>[h1, . . . , hK ] = EH

h K

K − 1

(cid:0)IK − 1K 1>

K

(cid:1)i

,

hk · hk0 = −

EH
K − 1

[48]

[49]

which implies Eq. (13).

Reversely, it is easy to verify that the equality for Eq. (47) is reachable when H admits Eq. (13). We complete the proof of Theorem

3.

Proof of Theorem 4. We ﬁrst determine the minimum of Eq. (7). For the simplicity of our expressions, we introduce zk,i := W hk,i for
k ∈ [K] and i ∈ [n]. By the convexity of g2, for any k ∈ [K] and i ∈ [n], we have

K
X

k0=1, k06=k

(cid:0)S(zk,i)(k0)(cid:1) ≥ (K − 1)g2

g2

1
K − 1

K
X

!

S(zk,i)(k0)

a= (K − 1)g2

(cid:16)

1 −

k0=1, k06=k
1
K − 1

S(zk,i)(k)

(cid:17)

,

[50]

PNAS | September 10, 2021 |

vol. XXX | no. XX | 17

 
 
 
 
 
 
where a= uses PK

k=1

S(a)(k) = 1 for any a ∈ RK . Then it follows by the convexity of g1 and g2 that

1

N

1

N

1

N

=

Eq. (50)
≥

K
X

n
X

L(W hk,i, yk)

k=1

i=1

n
X

K
X

"

(cid:0)S(zk,i)(k)(cid:1) +

g1

K
X

#

(cid:0)S(zk0,i)(k0)(cid:1)

g2

i=1

k=1

n
X

K
X

h

i=1

k=1

k0=1, k06=k

(cid:0)S(zk,i)(k)(cid:1) + (K − 1)g2

g1

(cid:16)

1 −

1
K − 1

(cid:17)i

S(zk,i)(k)

≥g1

1

N

n
X

K
X

i=1

k=1

!

S(zk,i)(k)

+ (K − 1)g2

1 −

1
N (K − 1)

n
X

K
X

i=1

k=1

!

S(zk,i)(k)

.

Because g1(x) + (K − 1)g2(1 − x

K−1

) is monotonously deceasing, it suﬃces to maximize

i=1
To begin with, for any zk,i with k ∈ [K] and i ∈ [n], by convexity of exponential function and the monotonicity of q(x) = a
a+x
a > 0, we have

k=1

1

n
X

K
X

N

S(zk,i)(k).

S(zk,i)(k) =

exp(zk,i(k))

PK

k0=1

exp(zk,i(k0))

≤

=

exp(zk,i(k)) + (K − 1) exp

K−1

k0=1, k06=k

exp(zk,i(k))
(cid:16) 1

PK

(cid:17)

zk,i(k0)

1 + (K − 1) exp

(cid:16) 1

K−1

1
PK

k0=1, k06=k

zk,i(k0) − zk,i(k)

(cid:17) .

Consider function g0 : R → R as g0(x) =

1
1+C exp(x)

with C := (K − 1) ≥ 1. We have

g00
0

(x) = −

exp(x)(1 + C exp(x))(1 − C exp(x))
(1 + C exp(x))4

.

For any feasible solution (H, W ) of Eq. (7), we divide the index set [n] into two subsets S1 and S2 deﬁned below:

(i) i ∈ S1 if there exists at least one k ∈ [K] such that

1
K − 1

K
X

k0=1, k06=k

zk,i(k0) − zk,i(k) ≥ log

(cid:16) 1

K − 1

(cid:17)

.

(ii) i ∈ S2 if for all k ∈ [K],

(cid:1) .
Clearly, S1 ∩ S2 = ∅. Let |S1| = t, then |S2| = n − t. Deﬁne function L : [n] → R as
(cid:18)

zk,i(k0) − zk,i(k) < log (cid:0) 1

k0=1, k06=k

1
K−1

PK

K−1

1

2 t +

1+exp(cid:0) K

K−1

√

K(n−t)

√

n/(n−t)

EH EW −log(K−1)(cid:1)

L(t) :=




N −



N − n
2 ,

(cid:19)

,

t ∈ [0 : n − 1],

t = n.

We show in Lemma 4 (see the end of the proof) that

Plugging Eq. (55) into Eq. (51), the objective function can be lower bounded as:

1

N

n
X

K
X

i=1

k=1

S(zk,i)(i) ≤

1

N

L(0).

1

N

K
X

n
X

k=1

i=1

L(W hk,i, yk) ≥ g1

(cid:17)

L(0)

(cid:16) 1
N

+ (K − 1)g2

(cid:16)

1 −

1
N (K − 1)

(cid:17)

L(0)

:= L0.

[51]

for x > 0 if

[52]

[53]

[54]

[55]

[56]

On the other hand, one can directly verify that the equality for Eq. (56) is reachable when (H, W ) satisﬁes Eq. (9). So L0 is the global
minimum of Eq. (7) and Eq. (9) is a minimizer of Eq. (7).

Now we show all the solutions are in the form of Eq. (9) under the assumption that g2 is strictly convex and g1 (or g2) is strictly

monotone.

By the strict convexity of g2, the equality in Eq. (50) holds if and only if for any k ∈ [K] and i ∈ [n] and k0 ∈ [K], k00 ∈ [K] such that

for all k0 6= k and k00 6= k, we have

S(zi,j )(k0) = S(zi,j )(k00),

18 |

 
 
which indicates that

hk,i · wk0 = hk,i · wk00 .

Again, by the strict convexity of g2, Eq. (51) holds if and only if for all k ∈ [K], i ∈ [n], and a suitable number C0 ∈ (0, 1), we have

Combining Eq. (57) with Eq. (58), we have for all (k, i, k0) ∈ {(k, i, k0) : k ∈ [K], k0 ∈ [K], k0 6= k, i ∈ [n]},

S(zk,i)(k) := C0.

[57]

[58]

which implies that

exp(hk,i · wk)
exp(hk,i · wk0 )

=

C0(K − 1)
1 − C0

,

(cid:16) C0(K − 1)
1 − C0
On the other hand, by the strict monotonicity of g1(x) + (K − 1)g2(1 − x
1
N

S(zk,i)(k) = L(0). Thus Lemma 4 reads

hk,i · wk = hk,i · wk0 + log

PK

Pn

K−1

k=1

i=1

(cid:17)

.

), the equality in Eq. (56) holds if and only if

and

r

¯hi − hk,i = −

EH
EW

wk,

k ∈ [K],

i ∈ [n],

1

K

K
X

k=1

1

n

n
X

i=1

(cid:13)
(cid:13)hk,i

(cid:13)
2 = EH ,
(cid:13)

1

K

K
X

k=1

kwkk2 = EW ,

¯hi = 0p, i ∈ [n],

where ¯hi := 1
uniqueness argument. We complete the proof of Theorem 4.

k=1

K

PK

hk,i with i ∈ [n]. Putting the pieces together, from Lemma 2, we have (H, W ) satisﬁes Eq. (9), achieving the

Lemma 4. For any feasible solution (H, W ), we have

n
X

K
X

i=1

k=1

S(W hk,i)(k) ≤ L(0),

[59]

with L deﬁned in Eq. (54). Moreover, recalling the deﬁnition of S1 and S2 in (i) and (ii), respectively, the equality in Eq. (59) holds if
and only if |S1| = 0,

r

¯hi − hk,i = −

EH
EW

wk,

k ∈ [K],

i ∈ [n],

and

1

K

K
X

k=1

1

n

n
X

i=1

(cid:13)
(cid:13)hk,i

(cid:13)
2 = EH ,
(cid:13)

1

K

K
X

k=1

kwkk2 = EW ,

¯hi = 0p, i ∈ [n],

where ¯hi := 1

K

PK

k=1

hk,i with i ∈ [n].

Proof of Lemma 4. For any feasible solution (H, W ), we separately consider S1 and S2 deﬁned in (i) and (ii), respectively. Let t := |S1|.
(cid:1), where zk,i := W hk,i. By the monotonicity
of g0(x), it follows from Eq. (52) that S(zk,i)(k) ≤ 1/2. Furthermore, for any other index k0 ∈ [K] such that k0 6= k, using that

• For i ∈ S1, let k ∈ [K] be any index such that

zk,i(k0)−zk,i(k) ≥ log (cid:0) 1

1
K−1

k06=k

P

K−1

exp(zk0 ,i(k0))

PK

k00=1

exp(zk0,i)(k00)

≤ 1, we have

X

K
X

i∈S1

k=1

S(zk,i)(k) ≤ t(1/2 + K − 1).

• For i ∈ S2, by the concavity of g0(x) when x < log (cid:0) 1

K−1

(cid:1) from Eq. (53), we have, for S2 6= ∅,

[60]

[61]

S(zk,i)(k)

X

K
X

i∈S2

k=1

X

K
X

Eq. (52)
≤

i∈S2

k=1

1 + (K − 1) exp

(cid:16) 1

K−1

≤

1 + (K − 1) exp

(cid:16)

1
(n−t)K

P

i∈S2

1
PK

k0=1, k06=k
(n − t)K
(cid:16) 1

PK

k=1

K−1

zk,i(k0) − zk,i(k)

(cid:17)

PK

k0=1, k06=k

zk,i(k0) − zk,i(k)

(cid:17)(cid:17) .

PNAS | September 10, 2021 |

vol. XXX | no. XX | 19

We can bound P

recalling ¯hi = 1

K

PK

(cid:16) 1

PK

i∈S2
PK

k=1

K−1

k0=1, k06=k
k=1
hk,i for i ∈ [n], we have

zk,i(k0) − zk,i(k)

(cid:17)

using the similar arguments as Eq. (24) and Eq. (26). Speciﬁcally,

X

K
X

i∈S2

k=1

1
K − 1

K
X

k0=1, k06=k

!

zk,i(k0) − zk,i(k)

[62]

=

1
K − 1

X

  K
X





!>   K
X

hk,i

!

wk

− K

i∈S2

k=1

k=1



h>

k,iwk



K
X

K=1

Eq. (24)

≥ −

K
2(K − 1)

Eq. (26)

≥ −

K
2(K − 1)

≥ −

K
2(K − 1)

K
X

X

k=1

i∈S2

K
X

X

k=1

i∈S2

K
X

n
X

k=1

i=1

k¯hi − hk,ik2/C00 −

C00K(n − t)
2(K − 1)

K
X

k=1

kwkk2

khk,ik2/C00 −

C00K(n − t)
2(K − 1)

khk,ik2/C00 −

C00K(n − t)
2(K − 1)

K
X

kwkk2

k=1

K
X

k=1

kwkk2

≥ −

K2
(K − 1)

pEH EW (n − t)n,

where in the last inequality we follow from the constraints of Eq. (7) and set C00 :=

q nEH

(n−t)EW

.

We combine the above two cases. When t ∈ [0, n − 1], by plugging Eq. (62) into Eq. (61), using the monotonicity of g0(x), and adding
Eq. (60), we have

S(zk,i)(k) ≤ N −





1
2

t +

n
X

K
X

k=1

i=1

= L(t).

1 + exp

(cid:16) K

K−1

K
pn/(n − t)

√

EH EW − log(K − 1)



(cid:17) (n − t)



[63]

And when t = n, it directly follows from Eq. (61) that

n
X

K
X

k=1

i=1

S(zk,i)(k) ≤ N −

n
2

= L(n).

Therefore, it suﬃces to show L(t) ≤ L(0) for all t ∈ [0 : n]. We ﬁrst consider the case when t ∈ [0 : N − 1]. We show that L(t) is
monotonously decreasing. Indeed, deﬁne

q(t) :=

1 + exp

(cid:16) K

K−1

K
pn/(n − t)

√

EH EW − log(K − 1)

(cid:17) .

We have

− 1

2 K exp

q0(t) =

(cid:16) K

pn/(n − t)
(cid:16) K

K−1
h
1 + exp

K−1

√

EH EW − log(K − 1)

(cid:17) K

√

K−1

EH EW n(n − t)−3/2

pn/(n − t)

√

EH EW − log(K − 1)

(cid:17)i2

≥

1 + exp

− 1
2
(cid:16) K

K−1

√

K2
K−1

EH EW n(n − t)−3/2

pn/(n − t)

√

EH EW − log(K − 1)

(cid:17) ,

which implies that

L0(t) = −

h 1
2

− q(t) + q0(t)(n − t)

i

1
2

K2
K−1
(cid:16) K

K−1

√

EH EW n(n − t)−1/2 + K

pn/(n − t)

√

EH EW − log(K − 1)

1
2

(cid:17) −

≤

=

1 + exp

(cid:16) K

K−1

K

pn/(n − t)
h

√

(cid:17)

EH EW

+ 2K − 1 − exp

(cid:16) K

K−1

pn/(n − t)

√

2

1 + exp

(cid:16) K

K−1

pn/(n − t)

√

EH EW − log(K − 1)

EH EW − log(K − 1)
(cid:17)i

(cid:17)

.

Consider function f (x) : (cid:2) K

K−1

√

EH EW , K
K−1

√

EH EW n(cid:3) → R as:

f (x) = Kx + 2K − 1 − exp(x − log(K − 1)).

20 |

 
We have

when x ∈ (cid:2) K

K−1

√

EH EW , K
K−1

√

EH EW n(cid:3), where we use the assumption that

f 0(x) = K − exp(x)/(K − 1) < 0

p

EH EW >

K − 1
K

(cid:16)

K2p

log

EH EW + (2K − 1)(K − 1)

(cid:17)

≥

K − 1
K

log (K(K − 1)) .

Therefore, for all x ∈ (cid:2) K

K−1

√

EH EW , K
K−1

√

f (x) ≤ f

(cid:16) K

K − 1

p

EH EW

EH EW + 2K − 1 −

1
K − 1

exp

(cid:16) K

p

K − 1

EH EW

(cid:17) a

< 0,

EH EW n(cid:3), we have
(cid:17)

p

=

K2
K − 1

a
< use our assumption again. We obtain L0(t) < 0 for all t ∈ [0 : N − 1]. So L(t) reaches the maximum if and only if t = 0 when

where
t ∈ [0 : N − 1]. Moreover, under our assumption, one can verify that L(N ) < L(0). We obtain Eq. (59) from Eq. (63) with t = 0.

When t = 0, the ﬁrst inequality of Eq. (62) reduces to equality if and only if:

The second and third inequalities of Eq. (62) reduce to equalities if and only if:

¯hi − hk,i = −

r

EH
EW

wk,

k ∈ [K],

i ∈ [n].

1

K

K
X

k=1

1

n

n
X

i=1

(cid:13)
(cid:13)hk,i

(cid:13)
2 = EH ,
(cid:13)

1

K

K
X

k=1

kwkk2 = EW ,

¯hi = 0p, i ∈ [n].

We obtain Lemma 4.

B. Imbalanced Case.

B.1. Proofs of Lemma 1 and Proposition 1.

Proof of Lemma 1. For any feasible solution (H, W ) for the original program Eq. (7), we deﬁne

hk :=

1

nk

nkX

i=1

hk,i, k ∈ [K],

and X := (cid:2)h1, h2, . . . , hK , W >(cid:3)> (cid:2)h1, h2, . . . , hK , W >(cid:3) .

Clearly, X (cid:23) 0. For the other two constraints of Eq. (15), we have

and

1

K

K
X

k=1

X(k, k) =

1

K

K
X

k=1

khkk2

a
≤

1

K

K
X

k=1

1

nk

nkX

i=1

(cid:13)
(cid:13)hk,i

1

K

2K
X

k=K+1

X(k, k) =

1

K

K
X

k=1

kwkk2

c
≤ EW ,

2 b

(cid:13)
(cid:13)

≤ EH ,

a
≤ applies Jensen’s inequality and

where
Eq. (15). Letting L0 be the global minimum of Eq. (15), for any feasible solution (H, W ), we obtain

c
≤ use that (H, W ) is a feasible solution. So X is a feasible solution for the convex program

b
≤ and

1

N

K
X

nkX

k=1

i=1

L(W hk,i, yk) =

a
≥

K
X

k=1

K
X

k=1

nk
N

nk
N

"

1

nk

nkX

k=1

#

L(W hk,i, yk)

L(W hk, yk) =

K
X

k=1

nk
N

L(zk, yk) ≥ L0,

[64]

where in

a
≥, we use L is convex on the ﬁrst argument, and so L(W h, yk) is convex on h given W and k ∈ [K].

On the other hand, considering the solution (H ?, W ?) deﬁned in Eq. (16) with X ? being a minimizer of Eq. (15), we have
K , (W ?)>(cid:3) = X ? (p ≥ 2K guarantees the existence of (cid:2)h?
K , (W ?)>(cid:3)). We can verify
1, h?

K , (W ?)>(cid:3)> (cid:2)h?

(cid:2)h?
that (H ?, W ?) is a feasible solution for Eq. (7) and have

2, . . . , h?

2, . . . , h?

2, . . . , h?

1, h?

1, h?

1

N

K
X

nkX

k=1

i=1

L(W ?h?

k,i, yk) =

K
X

k=1

nk
N

L(z?

k, yk) = L0,

[65]

where z?
k

= [X ?(k, 1 + K), X ?(k, 2 + K), . . . , X ?(k, 2K) ]> for k ∈ [K].

Combining Eq. (64) and Eq. (65), we conclude that L0 is the global minimum of Eq. (7) and (H ?, W ?) is a minimizer.
Suppose there is a minimizer (H 0, W 0) that cannot be written as Eq. (16). Let

h0
k

=

1

nk

nkX

i=1

h0
k,i, k ∈ [K],

and X 0 = (cid:2)h0

1, h0

2, . . . , h0

K , (W 0)>(cid:3)> (cid:2)h0

1, h0

2, . . . , h0

K , (W 0)>(cid:3) .

PNAS | September 10, 2021 |

vol. XXX | no. XX | 21

Eq. (64) implies that X 0 is a minimizer of Eq. (15). As (H 0, W 0) cannot be written as Eq. (16) with X ? = X 0, then there is a k0 ∈ [K],
i, j ∈ [nk0 ] with i 6= j such that hk0,i 6= hk0,j . We have

X 0(k, k) =

1

K

K
X

k=1

kh0

kk2

1

nk

1

nk

1

nk

nkX

i=1

nkX

i=1

nkX

i=1

(cid:13)
(cid:13)h0

k,i

(cid:13)
2
(cid:13)

−

1

K

K
X

k=1

1

nk

K
X

k=1

kh0

k,i − h0

kk2

(cid:13)
(cid:13)h0

k,i

(cid:13)
2
(cid:13)

−

1

1

K

nk0

(kh0

k0,i − h0

k0 k2 + kh0

k0,j − h0

k0 k2)

(cid:13)
(cid:13)h0

k,i

(cid:13)
2
(cid:13)

−

1

K

1
2nk0

kh0

k0,i − h0

k0,j k2

1

K

1

K

1

K

1

K

=

≤

≤

K
X

k=1

K
X

k=1

K
X

k=1

K
X

k=1

<EH .

By contraposition, if all X ? satisfy that 1
K
complete the proof.

PK

k=1

X ?(k, k) = EH , then all the solutions of Eq. (7) are in the form of Eq. (16). We

Proposition 1 can be obtained by the same argument. We omit the proof here.

B.2. Proof of Theorem 5. To prove Theorem 5, we ﬁrst study a limit case where we only learn the classiﬁcation for a partial classes. We
solve the optimization program:

1

KAX

nAX

L(W hk,i, yk)

k=1

i=1

1

nk

nkX

i=1

(cid:13)
(cid:13)hk,i

(cid:13)
2
(cid:13)

≤ EH ,

kwkk2 ≤ EW ,

[66]

min
H,W

s.t.

KAnA

1

K

1

K

K
X

k=1

K
X

k=1

where n1 = n2 = · · · = nKA
Eq. (66).

= nA and nKA+1 = nKA+2 = · · · = nK = nB. Lemma 5 characterizes useful properties for the minimizer of

Lemma 5. Let (H, W ) be a minimzer of Eq. (66). We have hk,i = 0p for all k ∈ [KA + 1 : K] and i ∈ [nB]. Let L0 be the global
minimum of Eq. (66). We have

L0 =

1

KAX

nAX

KAnA

k=1

i=1

L(W hk,i, yk).

Then L0 only depends on KA, KB, EH , and EW . Moreover, for any feasible solution (H 0, W 0), if there exist k, k0 ∈ [KA + 1 : K] such
that kwk − wk0 k = ε > 0, we have

1

KAX

nAX

KAnA

k=1

i=1

L (cid:0)W 0h0

k,i, yk

(cid:1) ≥ L0 + ε0,

where ε0 > 0 depends on ε, KA, KB, EH , and EW .

Now we are ready to prove Theorem 5. The proof is based on the contradiction.

Proof of Theorem 5. Consider sequences n‘
for ‘ = 1, 2, . . . . We have R‘ → ∞. For each optimization
A
program indexed by ‘ ∈ N+, we introduce (H ‘,?, W ‘,?) as a minimizer and separate the objective function into two parts. We consider

with R‘ := n‘

and n‘
B

A/n‘
B

L‘ (cid:0)H ‘, W ‘(cid:1) =

KAn‘
A
+ KBn‘
B

KAn‘
A

(cid:0)H ‘, W ‘(cid:1) +

L‘
A

KBn‘
B
+ KBn‘
B

KAn‘
A

(cid:0)H ‘, W ‘(cid:1) ,

L‘
B

(cid:0)H ‘, W ‘(cid:1) :=

L‘
A

1
KAn‘
A

KAX

n‘
AX

k=1

i=1

L (cid:0)W ‘h‘

k,i, yk

(cid:1)

(cid:0)H ‘, W ‘(cid:1) :=

L‘
B

1
KBn‘
B

K
X

n‘
BX

k=KA+1

i=1

L (cid:0)W ‘h‘

k,i, yk

(cid:1) .

with

and

22 |

We deﬁne (cid:0)H ‘,A, W ‘,A(cid:1) as a minimizer of the optimization program:

min
H‘,W ‘

s.t.

(cid:0)H ‘, W ‘(cid:1)

L‘
A

1

K

1

K

K
X

k=1

KAX

k=1

(cid:13)
(cid:13)w‘

k

(cid:13)
2
(cid:13)

≤ EW ,

1
n‘
A

n‘
AX

i=1

(cid:13)
(cid:13)h‘

k,i

2 +

(cid:13)
(cid:13)

1

K

K
X

k=KA+1

1
n‘
B

n‘
BX

i=1

(cid:13)
(cid:13)h‘

k,i

(cid:13)
2
(cid:13)

≤ EH ,

and (cid:0)H ‘,B, W ‘,B(cid:1) as a minimizer of the optimization program:

min
H‘,W ‘

s.t.

(cid:0)H ‘, W ‘(cid:1)

L‘
B

1

K

1

K

K
X

k=1

KAX

k=1

(cid:13)
(cid:13)w‘

k

(cid:13)
2
(cid:13)

≤ EW ,

1
n‘
A

n‘
AX

i=1

(cid:13)
(cid:13)h‘

k,i

2 +

(cid:13)
(cid:13)

1

K

K
X

k=KA+1

1
n‘
B

n‘
BX

i=1

(cid:13)
(cid:13)h‘

k,i

(cid:13)
2
(cid:13)

≤ EH .

Note that Programs Eq. (67) and Eq. (68) and their minimizers have been studied in Lemma 5. We deﬁne:

LA := L‘
A

(cid:0)H ‘,A, W ‘,A(cid:1)

and LB := L‘
B

(cid:0)H ‘,B, W ‘,B(cid:1) .

Then Lemma 5 implies that LA and LB only depend on KA, KB, EH , and EW , and are independent of ‘. Moreover, since h‘,A
k,i
all k ∈ [KA + 1 : K] and i ∈ [nB], we have

(cid:0)H ‘,A, W ‘,A(cid:1) = log(K).

L‘
B

[67]

[68]

= 0p for

[69]

Now we prove Theorem 5 by contradiction. Suppose there exists a pair (k, k0) such that lim‘→∞ w‘,?
(cid:13)wa‘,?

6= 0p. Then there exists
(cid:13)
ε > 0 such that for a subsequence {(H a‘,?, W a‘,?)}∞
(cid:13) ≥ ε. Now we ﬁgure
out a contradiction by estimating the objective function value on (H a‘,?, W a‘,?). In fact, because (H a‘,?, W a‘,?) is a minimizer of
L‘(H ‘, W ‘), we have

and an index ‘0 when ‘ ≥ ‘0, we have (cid:13)

k − w‘,?
k0
k − wa‘,?
k0

‘=1

La‘ (H a‘,?, W a‘,?) ≤ La‘ (cid:0)H a‘,A, W a‘,A(cid:1) Eq. (69)=

= LA +

KAna‘
A
1
KRRa‘ + 1

KAna‘
A
+ KBna‘
B

LA +

KBna‘
B
+ KBna‘
B

KAna‘
A

log(K)

(log(K) − LA) ‘→∞

→ LA,

[70]

where we deﬁne KR := KA/KB and use R‘ = n‘
k − wa‘,?

However, when ‘ > ‘0, because (cid:13)

(cid:13)wa‘,?

A/n‘
B
(cid:13)
(cid:13) ≥ ε > 0, Lemma 5 implies that

.

k0

La‘
A

(H a‘,?, W a‘,?) ≥ LA + ε2,

where ε2 > 0 only depends on ε, KA, KB, EH , and EW , and is independent of ‘. We obtain
KBna‘
B
+ KBna‘
B

KAna‘
A
+ KBna‘
B

La‘ (H a‘,?, W a‘,?) =

(H a‘,?, W a‘,?) +

La‘
A

a
≥

=

La‘
A

KAna‘
A
KAna‘
A
+ KBna‘
B

KAna‘
A
KAna‘
A
+ KBna‘
B
1
KRRa‘ + 1

KAna‘
A

= LA + ε2 +

KAna‘
A
KBna‘
B
+ KBna‘
B

KAna‘
A

(H a‘,?, W a‘,?) +

(LA + ε2) +

KBna‘
B
+ KBna‘
B

LB

KAna‘
A
(LB − LA − ε2) ‘→∞

→ LA + ε2,

La‘
B

La‘
B

(H a‘,?, W a‘,?)

(cid:0)H a‘,B, W a‘,B(cid:1)

[71]

≥ uses (cid:0)H a‘,B, W a‘,B(cid:1) is the minimizer of Eq. (68). Thus we meet contradiction by comparing Eq. (70) with Eq. (71) and achieve

a

where
Theorem 5.

Proof of Lemma 5. For any constants Ca > 0, Cb > 0, and Cc > 0, deﬁne C0
a
log(C0
a

Cb
Ca+(KA−1)Cb+KB Cc
Ce :=
> 0. Using a similar argument as Theorem 1,
we show in Lemma 6 (see the end of the proof), for any feasible solution (H, W ) of Eq. (66), the objective value can be bounded from

∈ (0, 1), Cd := −C0
a
KACb+KB Cc
Ca+(KA−1)Cb+KB Cc

Ca
Ca+(KA−1)Cb+KB Cc
(KA − 1) log(C0
) − C0
b
b

∈ (0, 1), C0
b
log(C0
) − KBC0
c
c

Cc
Ca+(KA−1)Cb+KB Cc

∈ (0, 1), and Cg :=

∈ (0, 1), and C0
c

KACb
KACb+KB Cc

KB Cc
KACb+KB Cc

∈ (0, 1), Cf :=

:=

:=

:=

),

PNAS | September 10, 2021 |

vol. XXX | no. XX | 23

below by:

1

KAX

nAX

KAnA

k=1

i=1

L(W hk,i, yk)

a
≥ −

p

Cg
KA

KEH

v
u
u
t

KAX

k=1

(cid:13)
(cid:13)CewA + Cf wB − wk

(cid:13)
2 + Cd
(cid:13)

b
≥ −

p

Cg
KA

KEH

v
u
u
tKEW − KA

(cid:18)

1/KR − C2

f −

(cid:19)

C4
f
Ce(2 − Ce)

kwBk2 −

K
X

k=KA+1

kwk − wBk2 + Cd,

[72]

where wA := 1
KA
k ∈ [KA + 1 : K] and i ∈ [nB].

PKA
k=1

wk, wB := 1
KB

PK

k=KA+1

wk, and KR := KA
KB

. Moreover, the equality in

a
≥ holds only if hk,i = 0p for all

Though Ca, Cb, and Cc can be any positive numbers, we need to carefully pick them to exactly reach the global minimum of Eq. (66).

In the following, we separately consider three cases according to the values of KA, KB, and EH EW .

(i) Consider

the case when KA = 1. We pick Ca

:= exp

(cid:16)pKB(1 + KB)EH EW

(cid:17)

, Cb

:= 1, and Cc

:=

(cid:16)

exp

−p(1 + KB)EH EW /KB

(cid:17)

.

Then from

a
≥ in Eq. (72), we have

1

KAX

nAX

L(W hk,i, yk)

KAnA

a
≥ − CgCf

i=1

k=1
p

KEH

= − CgCf
b
≥ − CgCf

p

p

KEH

KEH

c
≥ − CgCf

p

KEH

p

p

kw1 − wBk2 + Cd

kw1k2 − 2w>

1 wB + kwBk2 + Cd

p(1 + 1/KB)(kw1k2 + KBkwBk2) + Cd
v
u
u
t(1 + 1/KB)

KEW −

kwk − wBk2

K
X

!

+ Cd

≥ − CgCf

p

KEH

p(1 + 1/KB)KEW + Cd := L1,

k=2

where
PK

k=2

a
≥ uses Ce + Cf = 1,
kwkk2 = KBkwBk2 + PK

k=2

b
≥ follows from 2ab ≤ a2 + b2, i.e., −2w>

1 wB ≤ (1/KB)kw1k2 + KBkwBk2, and

kwk − wBk2 and the constraint that PK

kwkk2 ≤ KEW .

k=1

On the other hand, when (H, W ) satisﬁes that

[73]

c
≥ follows from

w1 = p

KBEW u, wk = −p1/KBEW u, k ∈ [2 : K],

h1,i =p(1 + KB)EH u, i ∈ [nA],

hk,i = 0p, k ∈ [2 : K], i ∈ [nB],

where u is any unit vector, the inequalities in Eq. (73) reduce to equalities. So L1 is the global minimum of Eq. (66). Moreover, L1
a
≥ in Eq. (72) reduces to inequality. From Lemma 66, we have that any minimizer satisﬁes that hk,i = 0p for all
is achieved only if
k ∈ [KA + 1 : K] and i ∈ [nB].
Finally, for any feasible solution (H 0, W 0), if there exist k, k0 ∈ [KA + 1 : K] such that kwk − wk0 k = ε > 0, we have

K
X

k=KA+1

kwk − wBk2 ≥ kwk − wBk2 + kwk0 − wBk2 ≥

kwk − wk0 k2
2

= ε2/2.

[74]

It follows from

c
≥ in Eq. (73) that
nAX

KAX

1

KAnA

k=1
with ε1 > 0 depending on ε, KA, KB, EH , and EW .

i=1

L(W hk,i, yk) ≥ −CgCf

p

KEH

p(1 + 1/KB) (KEW − ε2/2) + Cd := L1 + ε1

• (ii) Consider the case when KA > 1 and exp (cid:0)(1 + 1/KR)

exp (cid:0)(1 + 1/KR)

√

EH EW

(cid:1), Cb := exp (cid:0)− 1

(1 + 1/KR)

EH EW

KA−1

√

EH EW /(KA − 1)(cid:1) <
(cid:1), and Cc := 1.

√

1 + KR + 1.

Let us pick Ca :=

b
≥ in Eq. (72), we know if 1/KR − C2

1
KAnA

PKA
k=1

PnA
i=1

f −

Ce(2−Cf ) > 0, then
L(W hk,i, yk) ≥ −Cg(1 + 1/KR)

√

EH EW + Cd := L2.

[75]

√

C4
f

Following from

24 |

 
In fact, we do have 1/KR − C2

f −

C4
f

Ce(2−Cf ) > 0 because

1/KR > C2

f −
r 1

C4
f
Ce(2 − Ce)
(cid:16)

(cid:0)by Ce + Cf = 1(cid:1)

by Ce =

KBCc
KACb + KBCc

(cid:17)

⇐⇒ Ce >

⇐⇒

Cb
Cc

⇐⇒ exp

>

(cid:16)

√

1 + KR
1
1 + KR + 1
(1 + 1/KR)p

EH EW /(KA − 1)

(cid:17)

p1 + KR + 1.

<

On the other hand, when (H, W ) satisﬁes that

(cid:2)w1, w2, . . . , wKA

r

(cid:3) =

h

EW
EH

h1, . . . , hKA

i>

= p(1 + 1/KR)EW (M ?

A

)>,

hk,i =hk,
hk,i =wk = 0p,

k ∈ [KA], i ∈ [nA]

where M ?
A

k ∈ [KA + 1 : K], i ∈ [nB],
is a KA-simplex ETF, Eq. (75) reduces to equality. So L2 is the global minimum of Eq. (66). Moreover, L2 is achieved
a
≥ of Eq. (72) reduces to equality. From Lemma 6, we have that any minimizer satisﬁes that hk,i = 0p for all k ∈ [KA + 1 : K]

only if
and i ∈ [nB].
Finally, for any feasible solution (H 0, W 0), if there exist k, k0 ∈ [KA + 1 : K] such that kwk − wk0 k = ε > 0, plugging Eq. (74) into
b
≥ in Eq. (72), we have

1

KAX

nAX

KAnA

k=1

i=1

L(W hk,i, yk) ≥ −

p

Cg
KA

p

KEH

KEW − ε2/2 + Cd := L2 + ε2,

[76]

with ε2 > 0 depending on ε, KA, KB, EH , and EW .
• (iii) Consider the case when KA > 1 and exp((1 + 1/KR)

√

EH EW /(KA − 1)) ≥

√

For x ∈ [0, 1], we deﬁne:

1 + KR + 1. Let C0
f

:=

1√

KR+1

and C0
e

:= 1 − C0
f

.

r

gN (x) : =

(1 + KR)EW

KRx2 + (KR + K2
R


)(1 − x)2

,

ga(x) : = exp

gb(x) : = exp

gc(x) : = exp

















gN (x)p(1 + KR)EH /KR
r

x2 +

(cid:17)2

(cid:16)

1 + C0
e
C0
f

(1 − x)2

gN (x)p(1 + KR)EH /KR
r

x2 +

(cid:17)2

(cid:16)

1 + C0
e
C0
f

(1 − x)2

gN (x)p(1 + KR)EH /KR
r

x2 +

(cid:17)2

(cid:16)

1 + C0
e
C0
f

(1 − x)2

(cid:20)

x2 +

(cid:18)

1 +

(cid:19)

C0
e
C0
f

(1 − x)2



(cid:21)





,

(cid:20)

−

1
KA − 1

(cid:18)

1 +

x2 +

(cid:19)

C0
e
C0
f

(1 − x)2



(cid:21)





,

(cid:20)

(cid:18)

−

1 +

(cid:19)

C0
e
C0
f

KR(1 − x)2



(cid:21)





.

Let x0 ∈ [0, 1] be a root of the equation

gb(x)/gc(x) =

1/C0

f − 1
KR

.

We ﬁrst show that the solution x0 exists. First of all, one can directly verify when x ∈ [0, 1], gb(x)/gc(x) is continuous. It suﬃces to
f −1
prove that (A) gb(0)/gc(0) ≥
(A) When x = 0, we have gb(x)/gc(x) ≥ exp(0) = 1. At the same time, 1/C0

and (B) gb(1)/gc(1) ≤

≤ 1. Thus (i) is

1/C0
KR

1/C0
KR

KR+1−1

1√

f −1

f −1

√

=

=

.

KR

KR

KR+1+1

achieved.

(B) When x = 1, we have gN (1) = p(1 + 1/KR)EW , so
−(1 + 1/KR)p

gb(1)/gc(1) = exp

(cid:16)

EH EW /(KA − 1)

(cid:17) a
≤

√

1
KR + 1 + 1

=

1/C0

f − 1
KR

.

where

a
≤ is obtained by the condition that

(cid:16)

exp

(1 + 1/KR)p

EH EW /(KA − 1)

(cid:17)

p1 + KR + 1.

≥

PNAS | September 10, 2021 |

vol. XXX | no. XX | 25

Now we pick Ca := ga(x0), Cb := gb(x0), and Cc := gc(x0), because Cb
Cc

1/KR = C2
f

+

C4
f
Ce(2−Ce)

. Then it follows from

b
≥ in Eq. (72) that

= 1/C0
KR

f −1

, we have Ce = C0
e

and Cf = C0
f

and

1
KAnA
On the other hand, consider the solution (H, W ) that satisﬁes

PKA
k=1

PnA
i=1

L(W hk,i, yk) ≥ −Cg(1 + 1/KR)

EH EW + Cd = L2.

[77]

√

x0
p(KA − 1)KA
KAX

"

wk = gN (x0)PA

(KAyk − 1KA

) +

1 − x0√
KA

#

1KA

,

k ∈ [KA],

k ∈ [KA + 1 : K],

wk = −

k=1

C2

PA

wk,

Ce(2 − Ce)
f KA
p(1 + 1/KR)EH
PKA
k=1
k ∈ [KA + 1 : K], i ∈ [nB],

kwi + Ce

Cf KA

wkk

PA

"

wi +

hk,i =

hk,i = 0p,

Ce
Cf KA

KAX

k=1

#

wk

,

k ∈ [KA], i ∈ [nA],

A PA = IKA

. We have exp (cid:0)h>

where yk ∈ RK is the vector containing one in the k-th entry and zero elsewhere and PA ∈ Rp×KA is a partial orthogonal matrix
(cid:1) = ga(x0) for i ∈ [nA] and k ∈ [KA], exp (cid:0)h>
(cid:1) = gb(x0) for i ∈ [nA] and
such that P >
(cid:1) = gc(x0) for i ∈ [nA], k ∈ [KA], and k0 ∈ [KB]. Moreover, (H, W ) can achieve the
k, k0 ∈ [KA] such that k 6= k0, and exp (cid:0)h>
equality in Eq. (77). Finally, following the same argument as Case (ii), we have that (1) L2 is the global minimum of Eq. (66); (2)
any minimizer satisﬁes that hk,i = 0p for all k ∈ [KA + 1 : K] and i ∈ [nB]; (3) for any feasible solution (H 0, W 0), if there exist
k, k0 ∈ [KA + 1 : K] such that kwk − wk0 k = ε > 0, then Eq. (76) holds.

k,iwk0

k,iwk0

k,iwk

Combining the three cases, we obtain Lemma 5, completing the proof.

Lemma 6. For any constants Ca > 0, Cb > 0, and Cc > 0, deﬁne C0
a
(0, 1), and C0
) − C0
c
b
KB Cc
Cf :=
∈ (0, 1), and Cg :=
KACb+KB Cc
Eq. (66) can be bounded from below by:

∈ (0, 1), Cd := −C0
a
KACb+KB Cc
Ca+(KA−1)Cb+KB Cc

Cc
Ca+(KA−1)Cb+KB Cc

log(C0
a

:=

:=

Ca
Ca+(KA−1)Cb+KB Cc

∈
∈ (0, 1),
) − KBC0
c
> 0. For any feasible solution (H, W ) of Eq. (66), the objective value of

(KA − 1) log(C0
b

KACb
KACb+KB Cc

∈ (0, 1), C0
b
log(C0
c

Cb
Ca+(KA−1)Cb+KB Cc

), Ce :=

:=

1

KAX

nAX

KAnA

k=1

i=1

L(W hk,i, yk)

a
≥ −

Cg
KA

p

KEH

v
u
u
t

KAX

k=1

(cid:13)
(cid:13)CewA + Cf wB − wk

(cid:13)
2 + Cd
(cid:13)

b
≥ −

Cg
KA

p

KEH

v
u
u
tKEW − KA

(cid:18)

1/KR − C2

f −

(cid:19)

C4
f
Ce(2 − Ce)

kwBk2 −

K
X

k=KA+1

kwk − wBk2 + Cd,

[78]

PKA
k=1

where wA := 1
KA
k ∈ [KA + 1 : K].
Remark 6. Note that the case hk,i = 0p does not imply that the network activations all die for the classes k ∈ [KA + 1 : K]. This is
because our analysis does not include the bias term for simplicity.

. Moreover, the equality in

wk, wB := 1
KB

wk, and KR := KA
KB

k=KA+1

a
≥ hold only if hk,i = 0p for all

PK

Proof of Lemma 6. For k ∈ [KA] and i ∈ [nk], we introduce zk,i = W hk,i. Because that C0
a
and C0

c > 0, by the concavity of log(·), we have
!

− log

exp(zk,i(i))

PK

k0=1

exp(zk0,i(k))

+ (KA − 1)C0
b

+ KBC0
c

= 1, C0

a > 0, C0

b > 0,

[79]

= − zk,i(k) + log

C0
a

(cid:16) exp(zk,i(k))
C0
a

(cid:17)

+

KAX

k0=1, k06=k

C0
b

(cid:18) exp(zk,i(k0))
C0
b

(cid:19)

+

K
X

k0=KA+1

C0
c

(cid:18) exp(zk,i(k0))
C0
c

(cid:19)!

≥ − zk,i(k) + C0

azk,i(k) + C0
b

zk,i(k0) + C0
C

K
X

zi,j (k) + Cd

KAX

k0=1, k06=k

!

zk,i(k0) − zk,i(k)

+ CgCf

k0=KA+1

K
X

k0=KA+1

1

KB

!

zk,i(k0) − zk,i(k)

+ Cd.

=CgCe

1

KA

KAX

k0=1

26 |

 
 
 
 
Therefore, integrating Eq. (79) with k ∈ [KA] and i ∈ [nA], recalling that wA = 1
KA

PKA
k=1

wk and wB = 1
KB

PK

k=KA+1

wk, we have

1

KAX

nAX

KAnA

≥

1

KAnA

k=1

i=1

KAX

nAX

k=1

i=1

L(W hk,i, yk)

[80]

(cid:2)Ce(hk,iwA − hk,iwk) + Cf (hk,iwB − hk,iwk)(cid:3) + Cd

Cg

KAX

h>
k

(CewA + Cf wB − wk) + Cd,

a=

Cg
KA

where in a=, we introduce hk := 1
nk
Cf wB − wk). By the Cauchy–Schwarz inequality, we have

k=1
Pnk
i=1

hk,i for k ∈ [K], and use Ce + Cf = 1. Then it is suﬃcient to bound PKA

h>
k

(CewA +

k=1

KAX

k=1

h>
k

(CewA + Cf wB − wk) ≥ −

a
≥ −

v
u
u
t

v
u
u
t

KAX

k=1

KAX

k=1

khkk2

v
u
u
t

KAX

k=1

(cid:13)
(cid:13)CewA + Cf wB − wk

(cid:13)
2
(cid:13)

1

nk

nkX

i=1

khk,ik2

v
u
u
t

KAX

k=1

(cid:13)
(cid:13)CewA + Cf wB − wk

(cid:13)
2
(cid:13)

b
≥ −

p

KEH

v
u
u
t

KAX

k=1

(cid:13)
(cid:13)CewA + Cf wB − wk

(cid:13)
2
(cid:13)

,

[81]

a
b
Pnk
≥ follows from Jensen’s inequality 1
≥ uses the constraint that 1
khk,ik2 ≥ hk for k ∈ [KA] and
≤
nk
K
i=1
(cid:13)
(cid:13)
2 = EH only if hk,i = 0p for all k ∈ [KA + 1 : K]. Plugging Eq. (81) into Eq. (80), we
(cid:13)hk,i
(cid:13)

Pnk
i=1

(cid:13)
(cid:13)hk,i

PK

(cid:13)
2
(cid:13)

1
nk

k=1

Pnk
i=1

k=1

1
nk

where
EH . Moreover, we have PKA
a
≥ in Eq. (78).
obtain
We then bound PKA
k=1

(cid:13)
(cid:13)CewA + Cf wB − wk

2. First, we have

(cid:13)
(cid:13)

1

KAX

KA

=

1

KA

a=

1

KA

k=1

KAX

k=1

KAX

k=1

(cid:13)
(cid:13)CewA + Cf wB − wk

(cid:13)
2
(cid:13)

kwkk2 − 2

1

KAX

KA

k=1

wk · (CewA + Cf wB) + kCewA + Cf wBk2

kwkk2 − 2C2

f w>

A wB − Ce(2 − Ce)kwAk2 + C2

f kwBk2.

where a= uses PKA
k=1

wk = KAwA. Then using the constraint that PK

k=1

kwkk ≤ KEW yields that

1

KAX

KA

k=1

kwkk2 − 2C2

f w>

A wB − Ce(2 − Ce)kwAk2 + C2

f kwBk2

≤

K
KA

E2

W −

1

KA

K
X

kwkk2 − Ce(2 − Cf )

k=KA+1

(cid:13)
(cid:13)
(cid:13)
(cid:13)

wA +

C2
f
Ce(2 − Ce)

wB

(cid:18)

(cid:13)
2
(cid:13)
+
(cid:13)
(cid:13)

C2
f

+

(cid:19)

C4
f
Ce(2 − Ce)

kwBk2

(cid:18)

a=

K
KA

E2

W −

1/KR − C2

f −

(cid:19)

C4
f
Ce(2 − Ce)

kwBk2 −

1

KA

K
X

kwk − wBk2 ,

k=KA+1

[82]

[83]

where

a

≥ applies PK

k=KA+1

kwkk2 = KBkwBk2 + PK

k=KA+1

kwk − wBk2. Plugging Eq. (82) and Eq. (83) into

a
≥ in Eq. (78), we obtain

b
≥ in Eq. (78), completing the proof.

C. Additional Results.

Comparison of Oversampling and Weighted Adjusting. Oversampling and weight adjusting are two commonly-used tricks in deep learning
(11). Both of them actually consider the same objective as Eq. (17), but applies diﬀerent optimization algorithms to minimize the objective.
It was observed that oversampling is more stable than weight adjusting in optimization. As a by product of this work, we compare the two
algorithms below and shows that the variance of updates for oversampling will be potentially much smaller than that of weight adjusting.
It was well-known in stochastic optimization ﬁeld that the variance of the updates decides the convergence of an optimization algorithm

PNAS | September 10, 2021 |

vol. XXX | no. XX | 27

(see e.g, (68–70)). Thus we oﬀer a reasonable justiﬁcation for the stability of the oversampling technique. We simply consider sampling
the training data without replacement. It slightly diﬀers from the deep learning training methods in practice. Besides, we only consider
sampling a single data in each update. The analysis can be directly extended to the mini-batch setting.

We ﬁrst introduce the two methods. The weight adjusting algorithm in each update randomly samples a training data, and updates

the parameters Wfull by the Stochastic Gradient Descent algorithm as

[84]

[85]

[86]

where W t

full

We have

and

full − ηwvt
w,
denotes the parameters at iteration step t, ηw is a positive step size, and the stochastic gradient vt
w

t = 0, 1, 2, . . . ,

= W t

W t+1
full

satisﬁes that

vt
w

=

(cid:26)∇Wfull L(f (xk,i; W t

full
wr∇Wfull L(f (xk,i; W t

full

), yk),

k ∈ [KA], i ∈ [nA], with probability
), yk), k ∈ [KA + 1 : KB], i ∈ [nB], with probability

1
KAnA+KB nB

,

1
KAnA+KB nB

.

E (cid:2)vt

w | W t

full

(cid:3)

=

1
nAKA + nBKB

" KAX

nAX

k=1

i=1

∇Wfull L(f (xk,i; W t

full

), yk) + wr

K
X

nBX

∇Wfull L(f (xk,i; W t

full

), yk)

,

#

k=KA+1

i=1

E (cid:2)kvt

wk2 | W t

full

(cid:3) =

1
nAKA + nBKB

KAX

nAX

k=1

i=1

(cid:13)
(cid:13)∇Wfull L(f (xk,i; W t

full

), yk)(cid:13)
2
(cid:13)

+

w2
r
nAKA + nBKB

K
X

nBX

(cid:13)
(cid:13)∇Wfull L(f (xk,i; W t

full

), yk)(cid:13)
2
(cid:13)

.

k=KA+1
For the oversampling method, the algorithm in eﬀect duplicates the data by wr times and runs Stochastic Gradient Descent on the

i=1

“whole” data. Therefore, the update goes as

where vt
s

satisﬁes that

vt
s

=

We obtain

and

W t+1
full

= W t

full − ηsvt
s,

t = 0, 1, 2, . . . ,

[87]

(cid:26)∇Wfull L(f (xk,i; W t
∇Wfull L(f (xk,i; W t

full

full

), yk), k ∈ [KA], i ∈ [nA], with probability
), yk), k ∈ [KA + 1 : KB], i ∈ [nB], with probability

1
KAnA+KB wr nB

,
wr
KAnA+KB wr nB

.

E (cid:2)vt

s | W t

full

(cid:3) =

1
nAKA + wrnBKB

KAX

nAX

k=1

i=1

∇Wfull L(f (xk,i; W t

full

), yk)

+

wr
nAKA + wrnBKB

K
X

nBX

k=KA+1

i=1

∇Wfull L(f (xk,i; W t

full

), yk),

E (cid:2)kvt

sk2 | W t

full

(cid:3) =

1
nAKA + wrnBKB

KAX

nAX

k=1

i=1

(cid:13)
(cid:13)∇Wfull L(f (xk,i; W t

full

), yk)(cid:13)
2
(cid:13)

+

wr
nAKA + wrnBKB

K
X

nBX

k=KA+1

i=1

(cid:13)
(cid:13)∇Wfull L(f (xk,i; W t

full

), yk)(cid:13)
2
(cid:13)

.

[88]

full

full

s | W t

w | W t

(cid:3) = ηsE (cid:2)vt

We suppose the two updates in expectation are in a same scale. That means we assume

ηw = nAKA+wr nB KB
ηs. Then
nAKA+nB KB
ηwE (cid:2)vt
(cid:3). In fact, if KA (cid:16) 1, KB (cid:16) 1, nA (cid:29) nB, and 1 (cid:28) wr (cid:46) (nA/nB), we have nAKA+wr nB KB
(cid:16) 1 and
nAKA+nB KB
so ηw (cid:16) ηs. Now by comparing Eq. (86) with Eq. (88), we obtain that the second moment of ηwvt
is much smaller than that of ηsvt
s
w
since the order of wr for the latter is larger by 1. For example, let us assume that all the norms of the gradients are in a same order, i.e.,
(cid:13)
(cid:13) (cid:16) a for all k and i, where a > 0. Then Eq. (88) implies that E (cid:2)kηsvt
(cid:13)∇Wfull L(f (xk,i; W t
s a2. However, Eq. (86)
(cid:3) (cid:16) η2
reads that E (cid:2)kηwvt
nAKA+w2
r nB KB
s wra2. Thus
wk2 | W t
nAKA+wr nB KB
(cid:3)(cid:13)
the second moment for ηwvt
is around wr times of that for ηsvt
(cid:13) (cid:16) ηsa
s
w
and the property that Ekx − E[x]k2 = Ekxk2 − kE[x]k2 for any random variable x. Therefore, we can conclude that the variance of
updates for oversampling is potentially much smaller than that of weight adjusting.

sk2 | W t
a2. Furthermore, if we set wr (cid:16) nA/nB, then E (cid:2)kηwvt

. And this fact also holds for the variance because (cid:13)

(cid:3) (cid:16) η2
wk2 | W t

(cid:3) (cid:16) η2
s | W t

(cid:13)ηsE (cid:2)vt

), yk)(cid:13)

full

full

full

full

full

s

More Discussions on Convex Relaxation and Cross-Entropy Loss. We show Program Eq. (7) can also be relaxed as a nuclear norm-
constrained convex optimization. The result heavily relies on the progress of matrix decomposition, e.g. (56, 57). We will use the equality
(see e.g., (56, Section 2)) that for any matrix Z and a > 0,

kZk∗ = inf
r∈N+

inf
U ,V :U V >=Z

a
2

kU k2 +

1
2a

kV k2,

[89]

28 |

where r is the number of columns for U and k · k∗ denotes the nuclear norm.

For any feasible solution (H, W ) for the original program Eq. (7), we deﬁne

hk =

1

nk

nkX

i=1

We consider the convex program:

hk,i, k ∈ [K],

˜H = [h1, h2, . . . , hK ] ∈ Rp×K , and Z = W ˜H ∈ RK×K .

where Zk denotes the k-th column of Z for k ∈ [K].

min
Z∈RK×K

K
X

k=1

nk
N

L(Zk, yk)

s.t. kZk∗ ≤ K

p

EH EW .

[90]

[91]

Lemma 7. Assume p ≥ K and the loss function L is convex on the ﬁrst argument. Let Z? be a minimizer of the convex program
Eq. (91). Let r be the rank of Z? and consider thin Singular Value Decomposition (SVD) of Z? as Z? = U ?Σ?V ?. Introduce two
Σ?(i, i)/p|Σ?(i, i)| for

p|Σ?(i, i)| and Σ?

2 with the entries deﬁned as Σ?
1

(i, i) =

(i, i) =

2

q EW
EH

q EH
EW

diagonal matrices Σ?
i ∈ [r], respectively. Let (H ?, W ?) be

1 and Σ?

W = U ?Σ?
= h?
h?
k,

k,i

1P >,

[h?
1, h?
k ∈ [K], i ∈ [nk],

2, . . . , h?
K

] = P Σ?

2V ?,

[92]

where P ∈ Rp×r is any partial orthogonal matrix such that P >P = Ir. Then (H ?, W ?) is a minimizer of Eq. (7).

Proof of Lemma 7. For any feasible solution (H, W ) for the original program Eq. (7), deﬁne hk for k ∈ [K], ˜H, and Z by Eq. (90). We
show Z is a feasible solution for the convex program Eq. (91). In fact, by Eq. (89) with r = K and a = pEH /EW , we have

kZk∗ ≤

a
≤

pEH /EW
2
pEH /EW
2

kW k2 +

pEW /EH
˜H(cid:13)
(cid:13)
2
(cid:13)
(cid:13)
2
pEW /EH
2

kwkk2 +

K
X

k=1

K
X

k=1

1

nk

nkX

i=1

(cid:13)
(cid:13)hk,i

(cid:13)
2
(cid:13)

p

≤ K

EH EW ,

[93]

where

a
≤ applies Jensen’s inequality as:

˜H(cid:13)
(cid:13)
(cid:13)
(cid:13)

2 =

K
X

k=1

khkk2 ≤

K
X

k=1

1

nk

nkX

i=1

(cid:13)
(cid:13)hk,i

(cid:13)
2
(cid:13)

.

Let L0 be the global minimum of the convex problem Eq. (91). Since L is convex on the ﬁrst argument, by the same argument as Eq. (64),
we obtain, for any feasible solution (H, W ),

1

N

K
X

nkX

k=1

i=1

L(W hk,i, yk) =

≥

K
X

k=1

K
X

k=1

nk
N

nk
N

"

1

nk

nkX

k=1

#

L(W hk,i, yk)

L(W hk, yk) =

K
X

k=1

nk
N

L(Zk, yk) ≥ L0.

[94]

On the other hand, for the solution (H ?, W ?) deﬁned in Eq. (92) with Z?, we can verify that (H ?, W ?) is a feasible solution for Eq. (7)
and

1

N

K
X

nkX

k=1

i=1

L(W ?h?

k,i, yk) =

K
X

k=1

nk
N

L(Z?

k , yk) = L0.

[95]

Combining Eq. (94) and Eq. (95), we have that L0 is the global minimum of Eq. (7) and (H ?, W ?) is a minimizer.

Property 1. For the cross-entropy loss, we have the following properties.

(i) Any minimizer Z? of Eq. (91) satisﬁes that kZk∗ =
(ii) Any minimizer (H ?, W ?) of Eq. (7) satisﬁes

√

EH EW .

1

K

K
X

k=1

1

n

n
X

i=1

(cid:13)
(cid:13)h?

k,i

(cid:13)
2 = EH ,
(cid:13)

and

1

K

K
X

k=1

(cid:13)
(cid:13)w?

k

(cid:13)
2 = EW .
(cid:13)

(iii) Any minimizer X ? of Eq. (15) satisﬁes that

1

K

K
X

k=1

X ?(k, k) = EH ,

and

1

K

2K
X

k=K+1

X ?(k, k) = EW .

PNAS | September 10, 2021 |

vol. XXX | no. XX | 29

(a) EW = 0.5, EH = 5

(b) EW = 0.5, EH = 10

(c) EW = 1, EH = 5

(d) EW = 1, EH = 10

Fig. 8. The average cosine of the angles between any pair of the minority classiﬁer solved from the Layer-Peeled Model. The average cosine reaches 1 once R is above some
threshold. The total number of classes KA + KB is ﬁxed to 10. The gray dash-dotted line indicates the value of − 1

K−1 , which is given by Eq. (10).

Proof of Property 1. We ﬁrst prove (i). Let Z? be any minimizer of Eq. (91). Then by the Karush–Kuhn–Tucker conditions, there is a
pair (λ, ξ) with λ ≥ 0 and ξ ∈ ∂kZ?k∗ such that

∇Z

" K
X

k=1

nk
N

#

L(Z?

k , yk)

+ λξ = 0K×K ,

N L(Zk, yk)(cid:3) 6= 0K×K
where ∂kZk∗ denotes the set of sub-gradient of kZk∗. For the cross-entropy loss, one can verify that ∇Z
for all Z. So λ 6= 0. By the complementary slackness condition, we have that Z will reach the boundary of the constraint, achieving (i).
< EW .
EH EW ,

(cid:13)
(cid:13)w?
k
Letting Z? deﬁned by Eq. (90), it follows from Eq. (94) that Z? is a minimizer of Eq. (91). However, by Eq. (93), we have kZ?k∗ <
which is contradictory to (i). We obtain (ii).

For (ii), suppose there is a minimizer (H ?, W ?) of Eq. (7) such that 1
K

< EH or 1
K

(cid:13)
(cid:13)h?
k,i

(cid:2)PK

(cid:13)
2
(cid:13)
√

PK

PK

Pn

(cid:13)
2
(cid:13)

k=1

k=1

k=1

i=1

nk

1
n

For (iii), suppose there is a minimizer X ? of Eq. (15) such that 1
K

PK

k=1

X ?(k, k) < EH or 1
K

P2K

k=K+1

(H ?, W ?) deﬁned by Eq. (16), (H ?, W ?) is a minimizer of Eq. (7) from Theorem 1. However, we have 1
K

X ?(k, k) < EW . Then letting
(cid:13)
PK
(cid:13)h?
k,i

< EH

Pn

(cid:13)
2
(cid:13)

1
n

k=1

i=1

or 1
K

PK

k=1

(cid:13)
(cid:13)w?
k

(cid:13)
2
(cid:13)

< EW , which contradicts to (ii). We complete the proof.

D. Additional Experimental Results. In this part, we provide some additional experimental results for Minority Collapse. As for the
experiments for Minority Collapse in Figure 4, the corresponding training and test accuracy are shown in Tables 3-4. Furthermore, we ﬁnd
that the pre-trained neural networks on ImageNet (an imbalanced dataset with K = 1000 classes) that are oﬃcially released by Pytorch¶¶
also do not converge to a Simplex ETF, indicating that neural collapse does not emerge during the terminal phase of imbalanced training.
Speciﬁcally, the minimal (maximal) between-class angle of pre-trained classiﬁers for VGG19 and ResNet152 are 43◦ (103◦) and 37◦ (102◦),
respectively. The corresponding standard deviation of between-class angles of pre-trained classiﬁers for VGG19 and ResNet152 are 4.1◦
and 3.6◦, respectively. More details can be found in Figure 9. The phase transition point of the imbalance ratio is in Figure 8 with
multiple choices of EW and EH .

Dataset
Network architecture
No. of majority classes
R = 1
R = 10
R = 100
R = 1000
R = inf

FashionMNIST

CIFAR10

VGG11
KA = 3 KA = 5 KA = 7
100
100
100
99.94
100

100
100
100
99.97
100

100
100
100
99.87
100

ResNet18
KA = 3 KA = 5 KA = 7
100
100
100
99.93
100

100
100
100
99.97
100

100
100
100
99.97
100

VGG13
KA = 3 KA = 5 KA = 7
100
100
100
99.90
100

100
100
100
99.80
100

100
100
100
99.96
100

ResNet18
KA = 3 KA = 5 KA = 7
100
100
100
100
100

100
100
100
99.90
100

100
100
100
99.97
100

Table 3. Training accuracy (%) for different settings.

Dataset
Network architecture
No. of majority classes
R = 1
R = 10
R = 100
R = 1000
R = inf

FashionMNIST

CIFAR10

VGG11
KA = 3 KA = 5 KA = 7
93.02
93.02
93.02
92.00
89.79
87.12
88.03
85.00
73.48
69.09
57.61
40.10
63.86
47.61
29.39

ResNet18
KA = 3 KA = 5 KA = 7
93.80
93.80
93.80
92.78
88.77
86.07
86.24
84.62
70.82
66.13
57.95
45.51
64.59
47.72
29.44

VGG13
KA = 3 KA = 5 KA = 7
88.62
88.62
88.62
80.41
71.80
65.55
64.52
48.36
30.87
61.82
45.44
28.48
61.40
44.87
28.31

ResNet18
KA = 3 KA = 5 KA = 7
88.72
88.72
88.72
78.79
66.44
58.66
62.91
45.97
28.90
60.89
45.10
28.57
61.16
45.27
28.44

Table 4. Test accuracy (%) for different settings.

¶¶

https://pytorch.org/vision/stable/models.html.

30 |

100101102103104Imbalance Ratio (R)-0.200.20.40.60.81CosineKA=3KA=5KA=7100101102103104Imbalance Ratio (R)-0.200.20.40.60.81CosineKA=3KA=5KA=7100101102103104Imbalance Ratio (R)-0.200.20.40.60.81CosineKA=3KA=5KA=7100101102103104Imbalance Ratio (R)-0.200.20.40.60.81CosineKA=3KA=5KA=7(a) Pre-trained VGG19 on ImageNet

(b) Pre-trained ResNet152 on ImageNet

Fig. 9. The neural networks that are pre-trained on ImageNet by PyTorch do not converge to a Simplex ETF.

PNAS | September 10, 2021 |

vol. XXX | no. XX | 31

