2
2
0
2

g
u
A
6
2

]

G
L
.
s
c
[

2
v
6
0
6
4
0
.
7
0
2
2
:
v
i
X
r
a

SparseTIR: Composable Abstractions for Sparse Compilation in
Deep Learning

Zihao Ye∗
University of Washington
Seattle, USA
zhye@cs.washington.edu

Ruihang Lai†
Carnegie Mellon University
Pittsburgh, USA
ruihangl@cs.cmu.edu

Junru Shao
OctoML
Seattle, USA
jshao@octoml.ai

Tianqi Chen‡
Carnegie Mellon University
Pittsburgh, USA
tqchen@cmu.edu

Luis Ceze‡
University of Washington
Seattle, USA
luisceze@cs.washington.edu

ABSTRACT
Sparse tensors are rapidly becoming critical components of modern
deep learning workloads. However, developing high-performance
sparse operators can be difficult and tedious, and existing vendor
libraries cannot satisfy the escalating demands from new operators.
Sparse tensor compilers simplify the development of operators,
but efficient sparse compilation for deep learning remains chal-
lenging because a single sparse format cannot maximize hardware
efficiency, and single-shot compilers cannot keep up with latest
hardware and system advances. We show that the key to addressing
both challenges is two forms of composability. In this paper, we
propose SparseTIR, a sparse tensor compilation abstraction that
offers composable formats and composable transformations for
deep learning workloads. SparseTIR constructs a search space over
these composable components for performance tuning. With these
improvements, SparseTIR obtains consistent performance speedups
vs vendor libraries on GPUs for single operators: 1.1-3.3x for GNN
operators and 1.1-4.4x for sparse transformer operators. Sparse-
TIR also accelerates end-to-end GNNs by 1.1-2.2x for GraphSAGE
training and 0.9-26x for RGCN inference.

1 INTRODUCTION
Sparsity is becoming ubiquitous in deep learning due to the appli-
cation of deep learning to graphs and the need for more efficient
backbone models. Graph neural networks (GNNs) [28, 38, 62] have
made substantial progress in modeling relations in social networks,
proteins, point clouds, etc., using highly sparse matrices. Sparse
transformers [4, 10, 13] reduce both the time and space complexity
of transformers [61] by making the attention mask sparse using
manually designed and moderately sparse matrices.

Existing vendor libraries, such as cuSPARSE [16], dgSPARSE [20]
and Intel MKL [64], support only a few sparse operators. As such,
they fail to accelerate rapidly evolving emerging workloads such as
GNNs on heterogeneous graphs [35, 53, 70] and hypergraphs [24].
Manually optimizing sparse operators can be difficult and tedious.
Sparse matrices are stored in compressed formats, and programmers
must write manual code to compress or decompress coordinates

to access non-zero elements. Furthermore, the compressed sparse
formats vary, and operators designed for one format cannot gen-
eralize to others. Therefore, we need a more scalable and efficient
approach to developing optimized sparse operators.

Sparse tensor compilers, such as MT1 [8] and TACO [40], greatly
simplify the development of sparse operators by decoupling format
specification and format-agnostic computation descriptions. How-
ever, applying sparse compilation to deep learning must overcome
two major challenges. First, modern deep learning workloads are
quite diverse, making them hard to fit into a single sparse format
pattern provided by existing solutions. Second, harware backend are
evolving and becoming heterogeneous, making it hard for single-shot
compilers to keep up with the latest hardware and system advances.

Figure 1: Format composability enables us to leverage mul-
tiple formats for different parts in sparse pattern we face
in deep learning, and maximize the use of underlying hard-
ware resources.

∗Part of this work was done during internship at OctoML.
†Part of this work was done at Shanghai Jiao Tong University.
‡Also with OctoML.

Our key observation is that we can resolve all challenges by

introducing two forms of composability:

00Single FormatComposable FormatsTensorCoresthreads0threads0 
 
 
 
 
 
Zihao Ye, Ruihang Lai, Junru Shao, Tianqi Chen, and Luis Ceze

Figure 2: Single-shot sparse compilers vs SparseTIR. The composable formats and composable transformations enable us to
create optimizations that fit into broader range of deep learning workloads and leverage more advances in hardware backends.

Format composability. We propose to go beyond the single for-
mat option provided by most existing solutions to composable
formats (Figure 1) that store different parts of a sparse matrix in
the different formats that best fit their local patterns. The com-
pilation process decomposes the original computations into sub-
computation routines to enable efficient executions on each local
pattern that better match the characteristics of the corresponding
deep learning workloads.

Transformation composability. We reconfigure the single-shot
sparse tensor program compilation process into a composable set
of program transformations. Additionally, we enable a design that
incorporates existing loop-level abstractions in dense tensor com-
pilers. This design lets us define our own transformations for sparse
data while reusing hardware-specific optimizations (such as ten-
sorization and GPU mapping) from existing solutions, increasing
our overall efficiency to incorporate advances in hardware back-
ends.

Combining both forms of composability, we propose SparseTIR,
an abstraction that generates efficient sparse operators for deep
learning. Our contributions include the following.

• We propose an intermediate representation (IR) with com-
posable formats and composable transformations to accelerate
sparse operators by decomposing formats and specifying
schedules.

• We build a performance-tuning system that searches over
the parameter space of possible composable formats and
composable transformations.

• We evaluate SparseTIR on several important sparse deep

learning workloads.

SparseTIR offers consistent speedup for single operators relative
to vendor libraries on GPUs: 1.1-3.3x for GNN operators and 1.1-
4.4x for sparse transformer operators. SparseTIR also accelerates
end-to-end GNNs by 1.1-2.2x for GraphSAGE [28] training and by
0.9-26x for RGCN [53] inference.

2 SYSTEM OVERVIEW
This section provides an overview of SparseTIR. Figure 2 summa-
rizes our overall design and compare it with existing approaches.
The figure’s left side shows the design of most existing sparse tensor
compilers [55]. Their inputs are (1) tensor expressions, (2) format
annotations/specifications that allow only a single format for each
matrix, and (3) user-defined schedules. Schedules are tracked in
intermediate data structures, such as provenance graphs or sched-
ule trees, and then used to guide code generation. The effects of
schedules cannot be materialized immediately; we refer to such
compilation flows as single-shot compilation: the complexity of code-
generation and scheduling grows significantly as the possible sched-
ule set becomes larger due to the coupling of schedules primitives
and intermediate data structures. Though tensor compilers such as
Halide [49] and TVM [11] implement schedule primitives and code
generation on multiple backends, it is difficult to re-use these in-
frastructures because code generation is coupled with provenance
graph data structure in single-shot sparse compilers.

SparseTIR builds on top of these previous approaches, and intro-
duces a design that enables composable formats and composable
transformations. It contains three IR stages. The first stage presents
computation in coordinate space, where we describe sparse tensor
computations; like previous work, we decouple format specification

Single FormatComposable FormatsTensorExpressionFormat Annotation Provenance GraphTargetCodeCSRSingle-Shot Compilation BSR(2)ELL(2)00Stage I Coordinate-Space Computation (section 3.2)Stage II Position-SpaceComputation (section 3.3)Stage III Loop-based IR (section 3.4)ComposableTransformationsStage I Schedules (section 3.2.2)SchedulesStage II Schedules (section 3.3.2)CUDA/LLVM codeFormatDecomposition (section 3.2.1) Sparse Iteration Lowering pass (section 3.3.1)Sparse Buffer Lowering pass (section 3.4.1)Program transformationsTarget specific code generation (section 3.5)Single-Shot Sparse CompilersSparseTIRSparseTIR: Composable Abstractions for Sparse Compilation in Deep Learning

and computations. Unlike a single-shot sparse compiler that accepts
a single format for each sparse tensor, SparseTIR lets users specify
composable formats. The second stage describes computation in
non-zero position space. The last stage of SparseTIRis a loop-level
IR in existing tensor compilers, such as TVM [11], AKG [74] and the
affine dialect in MLIR [60]. We design two passes on the IR, namely,
sparse iteration lowering and sparse buffer lowering, to transform
code from stage I to stage II and stage II to stage III, respectively.
Instead of single-shot compilation, all schedules in SparseTIR
are performed as composable program transformations (which do
not change the stage of the IR) on the IR instantly. The composable
design lets us transform the IR step-by-step and stage-by-stage. To
manipulate the coordinate space computation in stage I IR, we can
define new schedules as composable transformations applied to
stage I (i.e., stage I schedules). For stages compatible with target
loop-level IR, we can apply schedules defined for backend tensor
compilers (i.e., stage II schedules). Notably, format decomposition
can also be formulated as a program transformation at stage I (see
§3.2.1).

SparseTIR constructs a joint search space of composable for-
mats and composable transformations for performance tuning of
sparse operators. Users can customize the parameterized search
space by specifying format and schedule templates based on their
domain-specific knowledge about the operator and sparse tensor
characteristics. When the sparse structure is present at compile-
time, we can search for the best formats and schedules that achieve
optimal runtime performance in advance. Though the compilation
might take some time due to the large search space, the overhead
can be amortized because the compiled operator will be re-used
many times during training or inference for a fixed sparse structure
(as is typical in deep learning).

The rest of the paper is organized as follows. We introduce the
SparseTIR design of each stage and compiler passes in Section 3. In
Section 4 we evaluate our system in real world sparse deep learning
workloads. Section 5 positions SparseTIR relative to related work.
Finally, we discuss future work in Section 6 and conclude our work
in Section 7.

3 OUR APPROACH
In this section, we introduce the language constructs in SparseTIR,
then describe each compilation stage and transformations in the
order they appeared in the flow.

3.1 Language Constructs
The SparseTIR has three major components: axes, sparse buffers
and sparse iterations.

Axes. An axis is a data structure that defines sparse iteration
spaces, which generalize the idea of abstraction levels in previous
work [14]. Each axis in SparseTIR has two orthogonal attributes,
dense/sparse and fixed/variable, denoting whether the index of
non-zero elements in the axis is contiguous or not and whether the
number of non-zero elements in the axis is fixed or not. Variable
axes are associated with a indptr field that points to the address
of the indices pointer array; sparset s axes are associated with an
indices field that points to the address of the indices array. Each

Figure 3: Language constructs in the SDDMM operator. Users
specify axis dependencies and metadata to create axes. The
match_sparse_buffer defines sparse buffers and binds them
to pointers to their value, and sp_iter creates a sparse iter-
ation structure.

axis has a parent field that directs to the axis it depends on; a dense-
fixed axis has no dependency, and its parent field is always set
to none. Axis metadata includes its indices’ data type, maximum
length, number of accumulated non-zeros in this dimension (if
variable), and number of non-zeros per row in this dimension (if
fixed).

Sparse buffers. A sparse buffer is SparseTIR’s data structure for a
sparse matrix. We use defined axes to compose the format specifi-
cation of sparse matrices. We split sparse structure-related auxil-
iary data and values: axes store auxiliary data, and sparse buffers
store only values. Figure 4 shows the decoupled storage of sparse
buffers/axes in the Sampled Dense-Dense Matrix Multiplication
(SDDMM) [46] operator. This design lets us re-use auxiliary data
if several sparse buffers share the sparse layout (e.g., 𝐵 and 𝐴 in
SDDMM). The composition of axes is expressive to describe various
sparse formats, including CSR, BSR, DIA, ELL, Ragged Tensor [18],
etc.

Sparse iterations. Sparse iterations create iterators over a space
composed of defined axes. The body of sparse iterations can be
any statements (including a sparse iteration itself) that operates
on elements in sparse buffers. Notably, we do not pose any restric-
tions on the indices of sparse buffer access; this means we can
write A[i + j], B[X[i]] inside a sparse iteration body, a fea-
ture not supported in previous work. A single SparseTIR program
could contain multiple sparse iterations, allowing us to describe a
decomposed computation.

Figure 3 shows how to define these constructs in SparseTIR for
the SDDMM operator.1 In a SparseTIR program, axes are used to
construct both sparse buffers and sparse iterations. This design lets
us iterate over a sparse iteration space that is not bound to any
sparse buffers.

1SparseTIR has round-trip compatibility with Python, and this paper presents only its
Python form.

I = dense_fixed(m, "int32")J = sparse_variable(I, (n, nnz), (indptr,indices), "int32")J_ = dense_fixed(n, "int32")K = dense_fixed(feat_size, "int32")X = match_sparse_buffer(x, (I, K), "float32")Y = match_sparse_buffer(y, (K, J_), "float32")A = match_sparse_buffer(a, (I, J), "float32")B = match_sparse_buffer(b, (I, J), "float32")with sp_iter([I, J, K], "SSR", "sddmm") as [i, j, k]:    with init():        B[i, j] = 0.0    B[i, j] = B[i, j] + X[i, k] * A[i, j] * Y[k, j]Axis declarationsSparse buffer declarationsSparse iteration declarationsZihao Ye, Ruihang Lai, Junru Shao, Tianqi Chen, and Luis Ceze

Figure 4: Internal storage of axes and sparse buffers in an
SDDMM: 𝐵𝑖 𝑗 = 𝑋𝑖𝑘𝐴𝑖 𝑗𝑌𝑘 𝑗 . Sparse buffers store their axes’
composition and pointers to their value; axes store dense/s-
parse and fixed/variable attributes, metadata, their depen-
dent axes, and pointers to indices and indptr arrays.

3.2 Stage I: Coordinate Space Computation
In stage I SparseTIR defines sparse computations inside sparse
iterations, where we iterate over non-zero elements and access
sparse buffers in the coordinate space. At this stage, we can define
program transformations, such as format decomposition and sparse
iteration fusion, that manipulate only the three constructs of the
SparseTIR.

Format Decomposition. Format decomposition is a transfor-
3.2.1
mation that decomposes computations for composable formats
(introduced in Section 1). The transformation accepts a list of for-
mat descriptions and rewrites the IR according to these formats.
Figure 5 shows the generated IR for the Sparse Matrix-Matrix mul-
tiplication (SpMM) operation after decomposing the computation
in the CSR format to a computation in the BSR format, with block
size 2 and an ELLPACK format with 2 non-zero columns per row.
In addition to SpMM computations on the new formats, another
two sparse iterations that copy data from original to new formats
are generated, as well. In practice, we perform data copying at
the pre-processing step to avoid the overhead of run-time format
conversion.

Unlike work in Chou et al. [15], which automatically generates
code that infers indices and indices pointers array of output format
at runtime, SparseTIR requires users to pre-compute these values
and specify them as input arguments, which simplifies our design.
Users need only a dozen lines of Python code to infer these indices
in our evaluations.

Stage I Schedules. We define two schedule primitives at stage

3.2.2
I, sparse_reorder and sparse_fuse.

Sparse reorder. The order of sparse axes in the sparse iteration
influences the order of generated loops in stage II. This primitive
enables manipulation of the order of sparse axes.

Figure 5: Format decomposition for SpMM. New axes and
sparse buffers are created for decomposed formats BSR and
ELL. New sparse iterations are generated to copy data from
original to new formats and for computations on these new
formats.

Sparse fuse. This schedule primitive fuses several iterators in a
given sparse iteration into one. It is helpful when we want a single
loop rather two nested loops that iterate over all non-zero elements,
such as in the SDDMM [46] operator.

Figure 6 shows how stage I schedules transform the IR.

3.3 Stage II: Position Space Computation
Stage II SparseTIR removes the sparse iteration constructs and
creates iteration space using nested loops. Unlike stage I, stage II
operates on position space instead of coordinate space, with “posi-
tion” referring to an element’s non-zero index. For example, if the
coordinate of the first 4 non-zero elements in a row is {1, 3, 9, 10},
the position of 9 is 2 (assuming the index is 0-based). In stage II

Axes: valueAxes: valueAxes: valueAxes: valuedense,fixedParent: Nonedense,fixedParent: Nonedense,fixedParent: Nonesparse,variableParent: length: length: nnz: indptrindiceslength: length: Original Layout of  Sparse Tensors Axes' Storage Sparse Buffers' Storage # Generated sparse iteration for copying data to BSR(2) with sp_iter([IO, II, JO, JI], "SSSS", "copy_bsr_2") as [    oi, ii, jo, ji]:    A_bsr[io, jo, ii, ji] = A[io * 2 + ii, jo * 2 + ji]# Generated sparse iteration for copying data to ELL(2)with sp_iter([I2, J2], "SS", "copy_ell_2") as [i, j]:    A_ell[i, j] = A[i, j]# Generated sparse iteration for BSR(2)with sp_iter([IO, II, JO, JI, K], "SSSSR", "spmm_bsr_2") as [    io, ii, jo, ji, k]:    with init():        C[io * 2 + ii, k] = 0.0    C[io * 2 + ii, k] = C[io * 2 + ii, k] +\        A_bsr[io, jo, ii, ji] * B[jo * 2 + ji, k]# Generated sparse iteration for ELL(2)with sp_iter([I2, J2, K], "SRS", "spmm_ell_2") as [i, j, k]:    with init():        C[i, k] = 0.0    C[i, k] = C[i, k] + A_ell[i, j] * B[j, k]Generated sparse iterations# Generated axes for BSR(2) IO = dense_fixed(m // 2, "int32")II = dense_fixed(2, "int32")JO = sparse_variable(IO, (n // 2, nnz_bsr),    (indptr_bsr, indices_bsr), "int32") JI = dense_fixed(2, "int32") # Generated axes for ELL(2) I2 = dense_fixed(2, "int32") J2 = sparse_fixed(I2, (n, nnz_cols_ell), indices_ell, "int32") # Generated sparse buffersA_bsr = match_sparse_buffer(a_bsr, [IO, JO, II, JI], "float32") A_ell = match_sparse_buffer(a_ell, [I2, J2], "float32")Generated new axes and sparse buffers00Decompose  to  and with rule [BSR(2), ELL(2)]: CSR: BSR(2): ELL(2)I = dense_fixed(m, "int32")J = sparse_variable(i, (n, nnz), (indptr, indices), "int32")J1 = dense_fixed(n, "int32")K = dense_fixed(feat_size, "int32")A = match_sparse_buffer(a, (I, J), "float32")B = match_sparse_buffer(b, (J1, K), "float32")C = match_sparse_buffer(c, (I, K), "float32")with sp_iter([I, J, K], "SRS", "spmm") as [i, j, k]:    with init():        C[i, k] = 0.0    C[i, k] = C[i, k] + A[i, j] * B[j, k]Stage I IR of SpMM operatorSparseTIR: Composable Abstractions for Sparse Compilation in Deep Learning

Figure 6: Stage I schedules sequentially applied to stage I IR.

we can define program transformations that manipulate loops (e.g.,
unroll/split/reorder/vectorize), computation bodies (e.g., tensorize,
compute-inline), and buffers (e.g., cache-read/write).

Sparse Iteration Lowering. This pass transforms stage I IR to

3.3.1
stage II IR. It consists of the following 3 steps.

Step 1: Auxiliary buffer materialization. Pointers to the indices
pointer array and indices array are specified as arguments when
creating axes. In stage II we need to declare these auxiliary buffers
explicitly to access their value when determining loop range and
translating coordinates. Figure 7 shows how the materialization
works. In addition to auxiliary buffers, we also create hints that
indicate the domain of buffer values; these are used for integer set
analysis in stage II when performing schedules.

Figure 7: Example of auxiliary buffer materialization.
Sparse buffers storing auxiliary information are created.

Step 2: Nested loop generation. This step converts sparse iterations
in stage I to nested loops in stage II: we emit one loop per axis in the
sparse iteration. The generated loops start from 0, and the extent
is determined by whether the axis is fixed or variable. They are
separated by block constructs, which set up boundaries to avoid
cross-block loop reordering. The body of original sparse iterations
is kept in the innermost emitted blocks.

Figure 8 shows the emitted nested loop structures of different
sparse iterations. In the first case, the loops I and J cannot not be
reordered in stage II because they are separated by a block; in the
second case, we fuse 𝐼, 𝐽 and emit only one loop (ij).

Figure 8: Nested loop generation in sparse iteration lower-
ing. One loop is emitted per axis in the sparse iteration.

Step 3: Coordinate translation. This step rewrites the indices used
to access sparse buffers from coordinate space to non-zero position
space to bridge the semantic gap between stages I and II. Figure 9
shows an example; assume {𝐼, 𝐽, 𝐾 } is a chain of dependent axes,
𝐴 is a sparse buffer whose axis on second dimension is 𝐿, and
we have already translated the index of the first dimension from
coordinate space (coord1) to position space (pos1). Suppose the
original index on the second dimension is 𝑔(𝑖, 𝑗, 𝑘), which was
defined in coordinate space.

To rewrite it to position space, we must first decompress the
coordinates corresponding to positions 𝑖, 𝑗, 𝑘 by function 𝑓 , which
either returns the identity (for dense axes) or looks up the indices
array (for sparse axes):

𝑓 (𝐾, 𝑥) =

(cid:40)𝑥
K_indices[𝑖] [ 𝑗] [𝑘]

is-dense(𝐾)
is-sparse(𝐾)

(1)

Then, we apply the 𝑔 to compute the coordinate on the second
dimension. Finally, we compress the coordinate to position space
by 𝑓 −1, which either returns the identity (for dense axes) or looks
up the position of a given coordinate in the last dimension of the
indices buffer (for sparse axes):

𝑓 −1 (𝐿, 𝑥) =

(cid:40)𝑥
lookup(L_indices[pos1] [:], 𝑥)

is-dense(𝐿)
is-sparse(𝐿)

(2)

Like previous work [55], the coordinate lookup is computed us-
ing a binary search in SparseTIR. 2 We create nested loops and
temporary buffers to perform binary search and store results. This
design avoids duplicate computation when a lookup query is re-
quested multiple times. The binary search can be computed either
at run-time or pre-processed in another kernel, depending on how
the user schedules computations in stage II.

2A special case is a sparse iteration (e.g., 𝑖, 𝑗 ∈ (𝐼, 𝐽 )) that exactly matches the sparse
buffer access (𝐴 [𝑖, 𝑗 ], 𝐴 : (𝐼, 𝐽 )); in this case we can directly return 𝑖 and 𝑗 when
translating the first and second index.

with sp_iter([I, J, K], "SSR", "sddmm") as [i, j, k]:     ...with sp_iter([K, I, J], "RSS", "sddmm") as [k, i, j]:     ...with sp_iter([K, fuse(I, J)], "RSS", "sddmm") as [k, i, j]:     ...Stage I IRStage I IR (after reorder)sparse_reorder([K, I, J])sparse_fuse([I, J])Stage I IR (after iteration fusion)I = dense_fixed(m, "int32") J = sparse_variable(I, (n1, nnz_1), (j_indptr, j_indices), "int32")I = dense_fixed(m, "int32") J = sparse_variable(I, (n1, nnz_1), (j_indptr, j_indices), "int32") J_dense = dense_variable(I, (n1, nnz_1), j_indptr, "int32") J_indptr = match_sparse_buffer(j_indptr, (I,), "int32") J_indices = match_sparse_buffer(j_indices, (I, J_dense), "int32") assume_buffer_domain(J_indptr, [0, nnz_1]) assume_buffer_domain(J_indices, [0, n1])Stage I IRStage II IRwith sp_iter([K, I, J], "SSR", "sddmm") as [k, i, j]:   ...for k in range(k):   for i in range(m):     with block("sddmm_0"):       for j in range(J_indptr[i + 1], J_indptr[i]):  with block("sddmm_1"):           ...with sp_iter([K, fuse(I, J)], "RSS", "sddmm") as [k, i, j]:   ...for k in range(k):   for ij in range(nnz):     with block("sddmm_0"):       ...Sparse iteration in Stage IGenerated nested loop in Stage IISparse iteration in Stage IGenerated nested loop in Stage IIno fusionwith fusionZihao Ye, Ruihang Lai, Junru Shao, Tianqi Chen, and Luis Ceze

where nnz(Tree(𝐴𝑖 )) refers to the number of non-zero elements
of the sparse iteration space composed by the tree with 𝐴𝑖 as its
root. Figure 10 shows an example of sparse buffer lowering: axes
𝐼 and 𝐽 are removed, and sparse buffers 𝐴, 𝐵, 𝐶 are flattened. The
buffer access 𝐴[𝑖, 𝑗] is translated to 𝐴[J_indptr[𝑖] + 𝑗] by equation
3.

Figure 9: Coordinates translated from coordinate space to
position space in the sparse iteration lowering pass.

Stage II Schedules. The stage II schedules are responsible
3.3.2
for manipulating loops (fusion/reorder/tiling), moving data across
the memory hierarchy, binding loops to physical/logical threads
to parallelize them, and using vectorized/tensorized instructions
in hardware. The list of schedule primitives we support at stage II
is the same as TVM schedules 3. Stage II retains the sparse buffer
construct, which we treats as ordinary dense buffer in underlying
loop-level IR, this enables schedule primitives such as tensorize
and cache_read/cache_write. This is not possible in stage III,
where high-dimensional sparse buffers are flattened.

3.4 Stage III: Loop-Level IR
Stage III removes all SparseTIR constructs. It keeps only the nested
loop structures whose body includes statements that operate on
flattened buffers. This stage should be compatible with loop-level
IR in existing tensor compilers. In practice, we select the existing
loop-level IR in Apache TVM [11].

Sparse Buffer Lowering. Sparse buffer lowering removes all
3.4.1
axes, flattens all multi-dimensional sparse buffers to 1-dimension,
and rewrites memory access to these buffers. Suppose the original
sparse buffer 𝐴 is composed of axes {𝐴𝑖 }𝑛
𝑖=1. For memory access
𝐴[𝑥1, ..., 𝑥𝑛], the overall offset after flattening is computed by:

𝑛
∑︁

𝑖=1

is_leaf(𝐴𝑖 ) × offset(𝑖) × stride(𝑖 + 1),

(3)

where is_leaf(𝐴𝑖 ) means that if axis 𝐴𝑖 has no dependence in
{𝐴𝑗 }𝑛

𝑗=𝑖+1, offset and stride are defined as:

offset(𝑖) =

(cid:40)𝑥𝑖
𝐴𝑖 _indptr[offset( 𝑗)] + 𝑥𝑖 𝐴𝑗 is parent of 𝐴𝑖

is_root(𝐴𝑖 )

stride(𝑖) =

1
nnz(Tree(𝐴𝑖 )) × stride(𝑖 + 1)
stride(𝑖 + 1)

𝑖 > 𝑛
is_root(𝐴𝑖 )
otherwise,





(4)

(5)

Figure 10: Sparse buffer lowering: sparse constructs are
totally removed, and memory accesses are flattened to 1-
dimension.

3.5 Target-Specific Code Generation
SparseTIR re-uses the backend provided by existing tensor com-
pilers for target-specific code generation. The composable format
emits multiple kernels, which incur extra kernel-launching over-
head on the GPU. We insert a horizontal fusion [23, 42] pass to the
TVM backend to reduce this overhead.

4 EVALUATION
We now study how composable formats and composable trans-
formations help optimize sparse deep learning workloads in both
single operator and end-to-end setting. In summary, compared to
vendor libraries, SparseTIR obtains a 1.1-3.3x speedup on GNN
operators and a 1.1-4.4x speedup on sparse transformer operators.
When used in an end-to-end setting, SparseTIR obtains a 1.1-2.2x
speedup on end-to-end GraphSAGE training and a 0.9-26x speedup
on end-to-end RGCN inference.

4.1 Experiment Setup

Environment. We evaluate all experiments under two different

3https://tvm.apache.org/docs/reference/api/python/tir.html#tvm.tir.Schedule

GPU environments:

Coordinate spacePosition spaceLoop iteratorsIndexwith sp_iter([I, J, K], "SSS", "sp_iter_1") as [i, j, k]:     A[coord_1, g(i, j, k), ...] = ...for i in ...   for j in ...     for k in ...       A[pos_1, f_inv(L, g(f(I, i), f(J, j), f(K, k))), ...] = ...Stage I IRStage II IRI = dense_fixed(m, "int32") J = sparse_variable(I, (n, nnz), (indptr, indices), "int32") A = match_sparse_buffer(a, [I, J], "float32") B = match_sparse_buffer(b, [I], "float32") J_indptr = match_sparse_buffer(indptr, [I], "int32") assume_buffer_bomain(J_indptr, [0, nnz]) for i in range(m):   with block("csr_reduce_0"):     for j in range(J_indptr[i + 1] - J_indptr[i]):       with block("csr_reduce_1"):  with init():    b[i] = 0         B[i] = B[i] + A[i, j]A = match_buffer(a, [nnz], "float32") B = match_buffer(b, [m], "float32") J_indptr = match_buffer(indptr, [m + 1], "int32") for i in range(m):   with block("csr_reduce_0"):     for j in range(J_indptr[i + 1] - J_indptr[i]):       with block("csr_reduce_1"):  with init():      b[i] = 0         B[i] = B[i] + A[J_indptr[i] + j]Stage II IRStage III IRSparseTIR: Composable Abstractions for Sparse Compilation in Deep Learning

(1) NVIDIA RTX 3070 (46 Ampere SMs at 1.500 Ghz, 8 GB

SpMM. SpMM is the most generic sparse operator in deep learn-

GDDR6, 512 GB/s bandwidth)

ing, which can be formulated as:

(2) NVIDIA Tesla V100 (80 Volta SMs at 1.370 GHz, 16 GB HBM2,

900 GB/s bandwidth))

Baselines. cuSPARSE [16] is NVIDIA’s official library for sparse
tensor algebra, which includes high-performance implementation
of common sparse operators. dgSPARSE [20] is a collection of
state-of-the-art sparse kernel implementations, which includes GE-
SpMM [36], DA-SpMM [17] and PRedS [73]. PyG [25] and DGL [65]
are two open-source frameworks that support GNN training and
inference. TACO [40] is an open-source sparse tensor compiler.
Triton [59] is a tiling-based IR for programming neural networks.
For SpMM, we select TACO-generated operator, cuSPARSE 11.6,
and dgSPARSE 0.1 as baselines. For SDDMM, we select TACO-
generated operator, cuSPARSE 11.6, dgSPARSE 0.1 and DGL 0.8.2’s
implementation as baselines. The DGL’s SDDMM implementation
uses the optimizations proposed in FeatGraph [34]; we do not in-
clude PyG’s implementation because it does not support SDDMM.
For end-to-end GNN training, we compare a GraphSAGE model
written in PyTorch that integrates a SparseTIR-tuned kernel with
DGL. For RGCN, we select the Graphiler [71], DGL and PyG imple-
mentations as our baseline.4 For transformers, we select Triton’s
block-sparse kernel as our baseline.

4.2 Graph Neural Networks
We evaluate the performance of most generic operators in GNNs
on widely used graphs with different sizes. Table 1 describes the
graph characteristics; on the table, #etypes refers to the number
of edge types. We use graphs with single types, i.e., homogeneous
graphs, to evaluate SpMM, SDDMM and end-to-end training; we
use graphs with multiple edge types, i.e., heterogeneous graphs, to
evaluate RGCN operators. Homogeneous graphs are represented as
a two-dimensional sparse matrix: its rows and columns have corre-
spondence with source and destination nodes in the graphs. Het-
erogeneous graphs are represented as a three-dimensional sparse
matrix, where the first dimension refers to edge types and the last
two dimensions refer to source and destination nodes.

Graph

#nodes

#edges

#etypes

cora [54]
citeseer [54]
pubmed [54]
ppi [28]
ogbn-arxiv [32]
ogbn-proteins [32]
reddit [28]
AIFB [51]
MUTAG [51]
BGS [51]
ogbl-biokg [32]
AM [51]

2,708
3,327
19,717
44,906
169,343
132,534
232,965
7,262
27,163
94,806
93,773
1,885,136

10,556
9,228
88,651
1,271,274
1,166,243
39,561,252
114,615,892
48,810
148,100
672,884
4,762,678
5,668,682

1
1
1
1
1
1
1
45
46
96
51
96

Table 1: The number of nodes, edges and edge types of
graphs we use in GNN experiments.

4Both DGL and PyG provide several different official implementations of RGCN; we
select the best performing among them.

𝑛
∑︁

𝑌𝑖,𝑘 =

𝐴𝑖,𝑗 𝑋 𝑗,𝑘,

𝑗=1
where 𝐴 is a sparse matrix and 𝑋, 𝑌 are dense matrices. A high
performing SpMM kernel on a GPU requires efficient memory
access patterns and load balancing [72]. Runtime load balancing,
well studied in SpMM acceleration literature, always incurs runtime
overhead. The composable format and composable transformation
can help generate kernels that achieve compile-time load balancing
and better cache utilization.

Figure 11: Example of ℎ𝑦𝑏 (2, 2): the original matrix is decom-
posed to 6 ELLPACK sub-matrices; elements in partition 1
are stored in sub-matrices 1-3, and elements in partition 2
are stored in sub-matrices 4-6.

We design a parametrized composable format ℎ𝑦𝑏 (𝑐, 𝑘) for sparse
matrix 𝐴 with two parameters 𝑐 and 𝑘. We partition columns of the
sparse matrix by the given factor 𝑐, so that each column partition
has width 𝑤. For each column partition, we collect the rows with
length 𝑙 that satisfy 2𝑖−1 < 𝑙 ≤ 2𝑖 to bucket 𝑖, and we pad the length
of these rows to 2𝑖 ; each bucket then forms a sub-matrix with the
ELL format. Figure 11 shows a special case, hyb(2, 2).

For bucket 𝑖 of each column partition, we group each 2𝑘−𝑖 rows
and map them to a unique thread block in GPUs. The number of
non-zero elements in 𝐴 that are processed by each thread block
is 2𝑘 , which is implemented with TVM’s split and bind primi-
tives. We use the schedule proposed in GE-SpMM [36] for each
sub-matrix for the remaining dimensions. The column partition in
our design is intended to improve cache locality; when processing
column partition 𝑗, only 𝐵 [ 𝑗𝑤 : ( 𝑗 + 1)𝑤] would be accessed for 𝐵.
Featgraph [34] proposes to apply column partitions for SpMM on
CPUs; however, it does not extend the idea to GPUs. Our bucketing
technique was designed to achieve compile-time load balancing.

ELL(1)Partition 1Partition 2ELL submatrix with row length ELL submatrix with row length ELL submatrix with row length Sub-matrix 1Sub-matrix 4ELL(2)Sub-matrix 2ELL(4)Sub-matrix 3ELL(1)ELL(2)Sub-matrix 5ELL(4)Sub-matrix 6In practice, we searches for the best 𝑐 over {1, 2, 4, 8, 16} and let
𝑘 = (cid:6)log2

(cid:7), which generally works well.

𝑛𝑛𝑧
𝑛

We evaluate the SpMM written in SparseTIR with and with-
out the proposed ℎ𝑦𝑏 format on real world GNN datasets for both
V100 and RTX3070.5 We measure the geometric mean speedup
of different SpMM implementations against cuSPARSE for feature
size 𝑑 ∈ {32, 64, 128, 256, 512}. Figure 13 shows our results. The
SparseTIR kernel on ℎ𝑦𝑏 format obtains a 1.2-3.3x speedup on V100
and a 1.2-2.7x speedup on RTX 3070 compared to cuSPARSE. We
also achieve consistently better performance than state-of-the-art
open source sparse library dgSPARSE [20], which implements DA-
SpMM [17], and TACO with auto-scheduling enabled [55]. Though
TACO also explores compile-time load balancing, it does not sup-
port caching the partially aggregated result in registers, which is
critical to kernel performance on GPUs, and the irregularity of the
CSR format limits the application of loop unrolling. In SparseTIR
we can perform these optimizations in stage II schedules.

Importance of composable formats. We evaluate the SparseTIR
kernel without format decomposition (see SparseTIR(no-hyb) in
the figure). Results suggest that the SparseTIR kernel without for-
mat decomposition and per-format scheduling performs generally
worse: ogbn-arxiv is a citation network graph whose degrees obey
power-law distribution, and our designed format can perform sig-
nificantly better because of more efficient load balancing. The de-
gree distribution of the ogbn-proteins graph is centralized, and
the benefit of using a hybrid format is compensated for the extra
overhead introduced by padding. To evaluate the effect of column
partitioning, we fix the feature size to 128 and measure several
kernel metrics generated by SparseTIR on a Reddit dataset under
a different column partition setting. Figure 12 shows the results;
L1 and L2’s cache hit-rates improve as we increase the number
of column partitions. However, more partitions will increase the
required memory transactions of the kernel because we will need
to update the results matrix 𝑐 times if the number of partitions is
𝑐. As a result, the benefit of column partitioning saturates as we
increase the number of partitions.

Zihao Ye, Ruihang Lai, Junru Shao, Tianqi Chen, and Luis Ceze

SDDMM. SDDMM can be formulated as the following:

𝐵𝑖,𝑗 =

𝑑
∑︁

𝑘=1

𝐴𝑖,𝑗 𝑋𝑖,𝑘𝑌𝑘,𝑗 ,

where 𝐴 and 𝐵 are two sparse matrices that share a sparse struc-
ture, 𝑋, 𝑌 are dense matrices, and 𝑑 is the feature size. In SDDMM,
the computation per (𝑖, 𝑗) is independent, and the workload per
position is the same, so we need not worry about load balancing
issues if we parallelize the computation by each non-zero (𝑖, 𝑗). The
sparse_fuse schedule primitive introduced in Section 3.2.2 helps
us iterate over non-zero (𝑖, 𝑗) directly instead of first iterating over
𝑖 and then iterating over non-zero 𝑗 for each 𝑖.

PRedS [73] is the state-of-the-art open-source SDDMM imple-
mentation, which optimizes SDDMM in two ways. First, it uses
vectorized load/store intrinsics in CUDA, such as float4/float2,
which improves memory throughput. Second, it performs the re-
duction in two stages: (1) intra-group reduction, which computes
the reduction inside each group independently, and (2) inter-group
reduction, which summarizes the reduction result per group. We for-
mulate the optimization in PRedS as composable transformations in
SparseTIR with vectorize and rfactor [57] schedule primitives,
and we generalize the parameters, such as group size, vector length
and number of workloads per threadblock, as tunable parameters.
Figure 14 shows the geometric mean speedup of different SD-
DMM implementations vs our baseline for feature size 𝑑 ∈ {32, 64,
128, 256, 512}. We do not use composable formats in SDDMM. The
baseline we select is DGL 0.8’s SDDMM implementation, which uses
the optimization proposed in Featgraph [34]. cuSPARSE’s SDDMM
implementation is not optimized for highly sparse matrices such
as graphs and thus achieves very low performance. We obtain gen-
erally better performance than dgSPARSE [20], which implements
the PRedS [73] algorithm, because of the parametrized scheduling
space. SparseTIR significantly outperforms the DGL baseline and
the TACO scheduled kernel because these implementations do not
support two-stage reduction and vectorized load/store.

Relational graph convolution. The relational graph convolutional
network (RGCN) [53] is a widely used model for heterogeneous
graphs. The core operator in an RGCN is:

𝑅
∑︁

𝑛
∑︁

𝑑𝑖𝑛
∑︁

𝑌𝑖,𝑙 =

𝐴𝑟,𝑖,𝑗 𝑋 𝑗,𝑘𝑊𝑟,𝑘,𝑙 ,

𝑟 =1

𝑗=1

𝑘=1
where 𝑅 refers to the number of relations (edge types) in the het-
erogeneous graph, 𝐴 is a 3D sparse tensor storing a 2D adjacency
matrix for each type of graph (edge type is the first dimension),
and 𝑊 is a 3D dense matrix storing the weight matrix of each type.
However, this operator has no corresponding implementations in
any vendor libraries. Existing GNN libraries implement the operator
in a two-stage approach:

Figure 12: The kernel duration and L1/L2 hit-rate of Sparse-
TIR SpMM kernels under different column partitioning set-
tings.

5We do not include Reddit and ogbn-proteins on RTX3070 due to the out-of-memory
issue.

𝑑𝑖𝑛
∑︁

𝑘=1
𝑅
∑︁

𝑇𝑟,𝑗,𝑙 =

𝑌𝑖,𝑙 =

𝑋 𝑗,𝑘𝑊𝑟,𝑘,𝑙

𝑛
∑︁

𝐴𝑟,𝑖,𝑗𝑇𝑟,𝑗,𝑙

𝑟 =1

𝑗=1

(6)

(7)

 0 20 40 60 80 100124816 20 30 40 50 60 70Cache Hit Rate %Kernel duration (ms)#Column PartitionsL1-hit-rate31.5%32.8%35.8%37.7%39.4%L2-hit-rate24.8%29.8%50.5%73.3%88.8%Duration64.6ms53.3ms40.6ms30.6ms27.3msSparseTIR: Composable Abstractions for Sparse Compilation in Deep Learning

Figure 13: Normalized speedup against cuSPARSE for SpMM. SparseTIR consistently out-performs vendor libraries and TACO.
Comparing SparseTIR(no-hyb) and SparseTIR(hyb) demonstrates the importance of format composability.

Figure 14: Normalized speedup against Featgraph for SDDMM. SparseTIR beats the state-of-the-art vendor library dgSPARSE
on average by parametrizing scheduling space.

The first stage is implemented with an irregular batched GEMM
operator, while the second stage is implemented via an SpMM
operator. Though we could separately optimize both operators with
SparseTIR, a more direct way is to implement and optimize the
entire operator in a single kernel. We implement this operator
in SparseTIR and measure its performance in end-to-end RGCN
inference setting, with a feature size fixed at 16. Compared to the
state-of-the-art Graphiler compiler, we obtain a 0.9-26x speedup on
V100 and a 1.0-13x speedup on RTX 3070; see Figure 15. We also
consume much less GPU memory because we do not need to store
the intermediate result 𝑇 in two-stage implementations.

End-to-end GraphSAGE training. We also integrate SparseTIR-
generated SpMM operators in a GraphSAGE[28] model written in
PyTorch and compare the end-to-end speedup to DGL. Figure 17
shows that we obtain a 1.09-2.16x speedup on V100 and a 1.08-1.23x
speedup on RTX 3070.6

6We include the datasets that would encounter out-of-memory issues during training.

4.3 Sparse Transformers
Sparse transformers use batched SpMM:

𝑌𝑏,𝑖,𝑘 =

𝑛
∑︁

𝑗=1

𝐴𝑏,𝑖,𝑗 𝑋𝑏,𝑗,𝑘,

and batched SDDMM:

𝐵𝑏,𝑖,𝑗 =

𝑑
∑︁

𝑘=1

𝐴𝑏,𝑖,𝑗 𝑋𝑏,𝑖,𝑘𝑌𝑏,𝑗,𝑘,

where there is batch dimension 𝑏 in the front of each matrix. The
batch dimension is also known as the “head” in the context of multi-
head attention in the transformer [61]. The sparse matrices used
in sparse transformers are mostly manually designed and have a
block-sparse pattern to better utilize tensor cores in modern GPUs.
We select two examples: Longformer [4] and Pixelated Butterfly
Transformer [10], whose sparse structures are band matrix and but-
terfly matrix [47], respectively. We implement the batched-SpMM
and batched-SDDMM operators for both CSR and BSR formats. For
BSR operators, we use the tensorize primitive during stage II IR
schedules to use tensorized instructions in CUDA.

 0 0.5 1 1.5 2 2.5 3 3.5CoraCiteseerPubmebPPIogbn-arxivogbn-proteinsRedditNormalized SpeedupDataset1.01.01.01.01.01.01.01.51.71.31.11.01.31.10.80.70.50.50.60.40.71.31.61.41.00.41.31.13.33.01.91.21.51.32.3V100CoraCiteseerPubmedPPIogbn-arxiv cuSPARSE1.01.01.01.01.0dgSPARSE1.61.91.51.21.0TACO0.80.90.80.90.5SparseTIR(no-hyb)1.31.71.61.10.6SparseTIR(hyb)2.62.51.81.21.3RTX3070 0 0.5 1 1.5 2 2.5 3CoraCiteseerPubmebPPIogbn-arxivogbn-proteinsRedditNormalized SpeedupDataset0.10.10.00.00.00.10.01.01.01.01.01.01.01.00.80.81.21.60.91.61.21.21.21.61.91.21.91.30.50.40.81.10.91.01.01.21.11.62.21.72.21.4V100CoraCiteseerPubmebPPIogbn-arxivcuSPARSE0.10.10.00.10.0dgl1.01.01.01.01.0dgSPARSE-csr1.31.01.01.30.8dgSPARSE-coo1.51.41.32.10.9TACO0.60.61.01.20.9SparseTIR1.41.21.52.81.7RTX3070Zihao Ye, Ruihang Lai, Junru Shao, Tianqi Chen, and Luis Ceze

Figure 15: Normalized speedup against Graphiler on RGCN. The SparseTIR-generated fused kernel is significantly faster than
Graphiler and other frameworks.

speedup on multi-head SpMM and a 1.72-4.34x speedup on multi-
head SDDMM.

Importance of composable transformations. The performance of
SparseTIR-CSR is an order of magnitude slower than SparseTIR-
BSR. This difference comes from the use of a tensorize schedule on
SparseTIR-BSR that is enabled by our composable transformations.

Figure 16: GPU memory footprint for RGCN inference.
SparseTIR saves significantly more memory due to kernel
fusion.

Figure 18: Normalized speedup against Triton on sparse
transformer operators.

5 RELATED WORK

Deep learning compilers. Halide [49] and TVM [11, 12] are tensor
compilers that decouple kernel description and schedules for dense
computation. XLA [19] and Relay [52] proposed computational-
graph-level abstractions for deep learning, where we can apply
optimizations such as kernel fusion and graph substitution [37].
However, these compilers have limited support for representing
and optimizing sparse operators, impeding the wider deployment
of sparse deep learning workloads such as GNNs.

Sparse compilation. MT1 [5, 6, 8] introduces the idea of compil-
ing kernels for a given sparse data structure and a kernel descrip-
tion. TACO [14, 39, 40] proposes abstractions for sparse formats
and a merge lattice-based code generation routine. Senanayake et
al. [55] propose a sparse-iteration space transformation framework
for scheduling sparse operators. Chou et al. [15] introduce an ap-
proach for generating efficient kernels for sparse format conversion,

Figure 17: Normalized speedup of PyTorch+SparseTIR
against DGL on end-to-end GraphSAGE training.

Figure 18 shows different implementations’ speedup against
Triton’s [59] block-sparse operator. We fix the matrix size to 4096 ×
4096, batch(head) size to 12, band size to 256, and feature size per
head to 64. Results show that SparseTIR-BSR obtains a 1.06-1.12x

 0.1 1 10AIFBMUTAGBGSogbl-biokgAMNormalized SpeedupDataset6.61.70.80.20.11.60.60.60.30.21.01.01.01.01.0267.12.72.50.9V100AIFBMUTAGBGSogbl-biokgAM PyG2.30.60.30.1DGL1.30.50.50.60.3Graphiler1.01.01.01.01.0SparseTIR132.81.83.41.0RTX3070100101102103104AIFBMUTAGBGSogbl-biokgAMGPU Memory Footprint(MB)DatasetPyG5416675352446458DGL10311429501240Graphiler9.9321378871274SparseTIR0.41.75.86.6116 0.6 0.8 1 1.2 1.4 1.6 1.8 2 2.2 2.4CoraCiteseerPubmedPPIogbn-arxivRedditV100DatasetNormalized Speedup against DGL1.291.351.301.091.132.16CoraCiteseerPubmedPPIogbn-arxivRTX30701.221.231.181.081.14 0.2 1 5ButterflyLongformerMulti-Head SpMMMulti-Head SDDMMNormalized SpeedupSparse Pattern1.001.000.070.081.121.09V100ButterflyLongformer Triton1.001.00SparseTIR-CSR0.080.08SparseTIR-BSR1.061.11RTX3070ButterflyLongformer 1.001.000.060.061.721.74V100ButterflyLongformer1.001.000.070.074.214.34RTX 3070SparseTIR: Composable Abstractions for Sparse Compilation in Deep Learning

while Henry et al. [29] extend this idea to sparse array program-
ming. Taichi [33] decouple data structure and kernel description
for physics simulation programming; its compiler optimization
focuses on spatial sparse data, unsuitable for DL. Tiramisu [3] sup-
ports automatic selection of dense/sparse kernels at computational
graph-level. However, it lacks tensor-level sparse code generation.
COMET [58] and MLIR Sparse Dialect [7] are two MLIR dialects that
explore composable IR design for sparse tensor algebra. Both treat
sparse tensors with format annotation as first-class members in the
IR; however, neither considers decomposable formats. CoRA [23]
proposes a compiler infrastructure for ragged tensors [18], which
can be viewed as a special case of sparse tensors. The operation
splitting in CoRA is also a special case of format decomposition in
SparseTIR. SparTA [76] proposes sparse annotations for optimizing
neural networks with weight/activation sparsity; its annotation is
still dense and thus not applicable to highly sparse matrices used
in GNNs. SparseLNR [21] proposes branched iteration graph to sup-
port factoring reductions and loop-fusion for sparse tensor algebra,
these schedules can be formulated as stage-I schedules in SparseTIR.

GNN systems and compilers. PyG [25] and DGL [65] propose
programming interfaces for the programming message-passing
[27] modules in GNN models. Both frameworks accelerate specific
message-passing patterns with vendor libraries and handwritten op-
erators. Featgraph [34] optimizes generic GNN operators with TVM.
However, it fails to support more operators because TVM lacks spar-
sity support. FusedMM [50] fuses SDDMM and SpMM operators,
thus accelerating GNN training and saving GPU memory footprint.
FusedMM can be described and optimized in SparseTIR. Seastar [69]
and Graphiler [71] compile user-defined message-passing functions
to their intermediate representations (IR) and then optimize the
IR and emit template-based, target-specific code: these templates
still have limited expressiveness and cannot consider a wide range
of the optimization space. SparseTIR could serve as a backend for
these GNN compilers. GNN Advisor [68] proposes a CUDA tem-
plate for GNN computations and uses graph characteristics to guide
the performance tuning of GNN training. QGTC [67] introduces
a quantized graph neural network (QGNN) model that could ben-
efit from GPU tensor cores. TC-GNN [66] propose to condense
sparse matrix to improve the utilization of Tensor Cores for GNN,
the condense matrix can be treated as a new sparse format. The
contribution of these papers is orthogonal to SparseTIR.

Sparse kernel optimizations. Merge-SpMM [72], ASpT [31], GE-
SpMM [36], Sputnik [26] and DA-SpMM [17] explore different
schedule spaces for SpMM optimization on GPUs. We carefully
examined the optimizations suggested in theses papers for use in
SparseTIR and propose a composable abstraction to apply these
optimizations within our IR. OSKI [63] is a library that provides au-
tomatically tuned sparse operators used in sparse solvers. It focuses
on optimizing operators on cache-based, super-scalar architectures
such as CPUs, and users cannot define their own operators.

Sparse format optimizations. Pichon et al. [48] propose to reorder
rows and columns in 2D sparse matrices to increase the block
granularity of sparse matrices. Li et al. [43] study the problem of
reordering sparse matrices to improve the spatial and temporal
cache locality of operators on them. Mehrabi et al. [45] and Wang

et al. [68] propose to reorder rows and columns of sparse matrices
to accelerate SpMM on GPUs. These reordering algorithms can act
as pre-processing steps for SparseTIR to further discover structure
information underlying the sparse matrix and thus improve overall
performance.

Hardware-efficient algorithms. ES-SpMM [44] compresses the
sparse matrix in GNN to more hardware-friendly ELLPACK format
to reduce the amount of computation of GNN inference without
performance loss. Network pruning also brings sparsity to deep
learning by removing redundant weights in the network [30]. Re-
searchers propose pruning algorithms with block-sparsity [41] and
bank-sparsity [9, 78] to better utilize tensor cores in GPUs. With the
help of format composability in SparseTIR researchers can explore
more complex sparse patterns with ideal performance on modern
hardware.

6 FUTURE WORK

Automatic scheduling. SparseTIR still requires users to specify
schedule templates like they do for the first-generation of Halide
and TVM. The Halide auto-scheduler [1], FlexTensor [77], An-
sor [75] and Meta-scheduler [56] have been proposed to automati-
cally generate schedule templates for dense tensor compilers. We
expect these techniques would also prove helpful for sparse compi-
lation. Searching for the optimal configuration is time consuming
if we must re-run the compiled kernel each time. Ahrens et al. [2]
propose an asymptotic cost model for sparse tensor algebra to nar-
row the schedule space of sparse kernels, which could also benefit
our work.

Automatic format decomposition. In this paper we explore only
manually designed format decomposition rules. We leave automatic
format decomposition for future work.

Integration with graph-level IR. SparseTIR models only tensor-
level sparsity. However, some MoE (Mixture-of-Experts) based mod-
els, such as Switch Transformer [22], exhibit sparsity at the com-
putational graph level. Also, though composable formats could
improve the performance of a single operator, they may introduce
extra format conversion when placed in a computational graph.
Finding the optimal format decomposition plan requires global
information. Both ends encourage us to extend the constructs in
SparseTIR to graph-level IR, like XLA [19] and Relay [52].

7 CONCLUSION
We introduce SparseTIR, a composable abstraction for sparse oper-
ators in deep learning. Its key innovation is the use of composable
formats and composable transformations, and together they form
the parameter search space for performance tuning. Evaluations
on generic sparse deep learning show that SparseTIR achieves sig-
nificant performance improvements over existing vendor libraries
and frameworks.

REFERENCES
[1] Andrew Adams, Karima Ma, Luke Anderson, Riyadh Baghdadi, Tzu-Mao Li,
Michaël Gharbi, Benoit Steiner, Steven Johnson, Kayvon Fatahalian, Frédo Du-
rand, and Jonathan Ragan-Kelley. 2019. Learning to optimize halide with tree
search and random programs. ACM Trans. Graph. 38, 4 (2019), 121:1–121:12.
https://doi.org/10.1145/3306346.3322967

[2] Peter Ahrens, Fredrik Kjolstad, and Saman Amarasinghe. 2022. Autoscheduling
for Sparse Tensor Algebra with an Asymptotic Cost Model. In Proceedings of the
43rd ACM SIGPLAN International Conference on Programming Language Design
and Implementation (San Diego, CA, USA) (PLDI 2022). Association for Comput-
ing Machinery, New York, NY, USA, 269–285. https://doi.org/10.1145/3519939.
3523442

[3] Riyadh Baghdadi, Abdelkader Nadir Debbagh, Kamel Abdous, Fatima-Zohra
Benhamida, Alex Renda, Jonathan Elliott Frankle, Michael Carbin, and Saman P.
Amarasinghe. 2020. TIRAMISU: A Polyhedral Compiler for Dense and Sparse
Deep Learning. CoRR abs/2005.04091 (2020). arXiv:2005.04091 https://arxiv.org/
abs/2005.04091

[4] Iz Beltagy, Matthew E. Peters, and Arman Cohan. 2020. Longformer: The Long-
Document Transformer. CoRR abs/2004.05150 (2020). arXiv:2004.05150 https:
//arxiv.org/abs/2004.05150

[5] A.J.C. Bik and H.A.G. Wijshoff. 1995. Advanced Compiler Optimizations for
Sparse Computations. J. Parallel and Distrib. Comput. 31, 1 (1995), 14–24. https:
//doi.org/10.1006/jpdc.1995.1141

[6] A.J.C. Bik and H.A.G. Wijshoff. 1996. Automatic data structure selection and
transformation for sparse matrix computations. IEEE Transactions on Parallel
and Distributed Systems 7, 2 (1996), 109–126. https://doi.org/10.1109/71.485501
[7] Aart J.C. Bik, Penporn Koanantakool, Tatiana Shpeisman, Nicolas Vasilache,
Bixia Zheng, and Fredrik Kjolstad. 2022. Compiler Support for Sparse Tensor
Computations in MLIR. ACM Trans. Archit. Code Optim. (jun 2022). https:
//doi.org/10.1145/3544559 Just Accepted.

[8] Aart J. C. Bik. 1996. Compiler Support for Sparse Matrix Computations. Ph.D.

Dissertation.

[9] Shijie Cao, Chen Zhang, Zhuliang Yao, Wencong Xiao, Lanshun Nie, Dechen
Zhan, Yunxin Liu, Ming Wu, and Lintao Zhang. 2019. Efficient and Effective
Sparse LSTM on FPGA with Bank-Balanced Sparsity. In Proceedings of the 2019
ACM/SIGDA International Symposium on Field-Programmable Gate Arrays (Sea-
side, CA, USA) (FPGA ’19). Association for Computing Machinery, New York, NY,
USA, 63–72. https://doi.org/10.1145/3289602.3293898

[10] Beidi Chen, Tri Dao, Kaizhao Liang, Jiaming Yang, Zhao Song, Atri Rudra, and
Christopher Ré. 2021. Pixelated Butterfly: Simple and Efficient Sparse training
for Neural Network Models. CoRR abs/2112.00029 (2021). arXiv:2112.00029
https://arxiv.org/abs/2112.00029

[11] Tianqi Chen, Thierry Moreau, Ziheng Jiang, Lianmin Zheng, Eddie Yan, Haichen
Shen, Meghan Cowan, Leyuan Wang, Yuwei Hu, Luis Ceze, Carlos Guestrin,
and Arvind Krishnamurthy. 2018. TVM: An Automated End-to-End Optimizing
Compiler for Deep Learning. In 13th USENIX Symposium on Operating Systems
Design and Implementation (OSDI 18). USENIX Association, Carlsbad, CA, 578–
594. https://www.usenix.org/conference/osdi18/presentation/chen

[12] Tianqi Chen, Lianmin Zheng, Eddie Yan, Ziheng Jiang, Thierry Moreau, Luis
Ceze, Carlos Guestrin, and Arvind Krishnamurthy. 2018. Learning to Optimize
Tensor Programs. In Proceedings of the 32nd International Conference on Neural
Information Processing Systems (Montréal, Canada) (NIPS’18). Curran Associates
Inc., Red Hook, NY, USA, 3393–3404.

[13] Rewon Child, Scott Gray, Alec Radford, and Ilya Sutskever. 2019. Generat-
ing Long Sequences with Sparse Transformers. CoRR abs/1904.10509 (2019).
arXiv:1904.10509 http://arxiv.org/abs/1904.10509

[14] Stephen Chou, Fredrik Kjolstad, and Saman Amarasinghe. 2018. Format Abstrac-
tion for Sparse Tensor Algebra Compilers. Proc. ACM Program. Lang. 2, OOPSLA,
Article 123 (oct 2018), 30 pages. https://doi.org/10.1145/3276493

[15] Stephen Chou, Fredrik Kjolstad, and Saman Amarasinghe. 2020. Automatic
Generation of Efficient Sparse Tensor Format Conversion Routines. In Proceedings
of the 41st ACM SIGPLAN Conference on Programming Language Design and
Implementation (London, UK) (PLDI 2020). Association for Computing Machinery,
New York, NY, USA, 823–838. https://doi.org/10.1145/3385412.3385963

[16] NVIDIA Corporation. [n.d.]. cuSPARSE :: CUDA Toolkit Documentation. https:

//docs.nvidia.com/cuda/cusparse/index.html.

[17] Guohao Dai, Guyue Huang, Shang Yang, Zhongming Yu, Hengrui Zhang, Yufei
Ding, Yuan Xie, Huazhong Yang, and Yu Wang. 2022. Heuristic Adaptability to In-
put Dynamics for SpMM on GPUs. CoRR abs/2202.08556 (2022). arXiv:2202.08556
https://arxiv.org/abs/2202.08556

[18] Tensorflow Developers. [n.d.]. Ragged tensors | TensorFlow Core. https://www.

tensorflow.org/guide/ragged_tensor.

[19] Tensorflow Developers. [n.d.]. XLA: Optimizing Compiler for Machine Learning.

https://www.tensorflow.org/xla.

[20] dgSPARSE team. [n.d.]. dgSPARSE Library. https://github.com/dgSPARSE/

dgSPARSE-Library.

[21] Adhitha Dias, Kirshanthan Sundararajah, Charitha Saumya, and Milind Kulkarni.
2022. SparseLNR: Accelerating Sparse Tensor Computations Using Loop Nest
Restructuring. In Proceedings of the 36th ACM International Conference on Super-
computing (Virtual Event) (ICS ’22). Association for Computing Machinery, New
York, NY, USA, Article 15, 14 pages. https://doi.org/10.1145/3524059.3532386

[22] William Fedus, Barret Zoph, and Noam Shazeer. 2022. Switch Transformers:
Scaling to Trillion Parameter Models with Simple and Efficient Sparsity. Journal
of Machine Learning Research 23, 120 (2022), 1–39. http://jmlr.org/papers/v23/21-

Zihao Ye, Ruihang Lai, Junru Shao, Tianqi Chen, and Luis Ceze

0998.html

[23] Pratik Fegade, Tianqi Chen, Phillip B. Gibbons, and Todd C. Mowry. 2022. The
CoRa Tensor Compiler: Compilation for Ragged Tensors with Minimal Padding.
In Proceedings of Machine Learning and Systems, A. Smola, A. Dimakis, and
I. Stoica (Eds.).

[24] Yifan Feng, Haoxuan You, Zizhao Zhang, Rongrong Ji, and Yue Gao. 2019. Hy-
pergraph Neural Networks. In The Thirty-Third AAAI Conference on Artificial
Intelligence, AAAI 2019, The Thirty-First Innovative Applications of Artificial Intelli-
gence Conference, IAAI 2019, The Ninth AAAI Symposium on Educational Advances
in Artificial Intelligence, EAAI 2019, Honolulu, Hawaii, USA, January 27 - February
1, 2019. AAAI Press, 3558–3565. https://doi.org/10.1609/aaai.v33i01.33013558

[25] Matthias Fey and Jan E. Lenssen. 2019. Fast Graph Representation Learning with
PyTorch Geometric. In ICLR Workshop on Representation Learning on Graphs and
Manifolds.

[26] Trevor Gale, Matei Zaharia, Cliff Young, and Erich Elsen. 2020. Sparse GPU
Kernels for Deep Learning. In Proceedings of the International Conference for High
Performance Computing, Networking, Storage and Analysis (Atlanta, Georgia) (SC
’20). IEEE Press, Article 17, 14 pages.

[27] Justin Gilmer, Samuel S. Schoenholz, Patrick F. Riley, Oriol Vinyals, and George E.
Dahl. 2017. Neural Message Passing for Quantum Chemistry. In Proceedings of
the 34th International Conference on Machine Learning - Volume 70 (Sydney, NSW,
Australia) (ICML’17). JMLR.org, 1263–1272.

[28] William L. Hamilton, Rex Ying, and Jure Leskovec. 2017. Inductive Representation
Learning on Large Graphs. In Proceedings of the 31st International Conference on
Neural Information Processing Systems (Long Beach, California, USA) (NIPS’17).
Curran Associates Inc., Red Hook, NY, USA, 1025–1035.

[29] Rawn Henry, Olivia Hsu, Rohan Yadav, Stephen Chou, Kunle Olukotun, Saman
Amarasinghe, and Fredrik Kjolstad. 2021. Compilation of Sparse Array Pro-
gramming Models. Proc. ACM Program. Lang. 5, OOPSLA, Article 128 (oct 2021),
29 pages. https://doi.org/10.1145/3485505

[30] Torsten Hoefler, Dan Alistarh, Tal Ben-Nun, Nikoli Dryden, and Alexandra Peste.
2021. Sparsity in Deep Learning: Pruning and growth for efficient inference
and training in neural networks. J. Mach. Learn. Res. 22 (2021), 241:1–241:124.
http://jmlr.org/papers/v22/21-0366.html

[31] Changwan Hong, Aravind Sukumaran-Rajam, Israt Nisa, Kunal Singh, and P.
Sadayappan. 2019. Adaptive Sparse Tiling for Sparse Matrix Multiplication. In Pro-
ceedings of the 24th Symposium on Principles and Practice of Parallel Programming
(Washington, District of Columbia) (PPoPP ’19). Association for Computing Ma-
chinery, New York, NY, USA, 300–314. https://doi.org/10.1145/3293883.3295712
[32] Weihua Hu, Matthias Fey, Marinka Zitnik, Yuxiao Dong, Hongyu Ren, Bowen
Liu, Michele Catasta, and Jure Leskovec. 2020. Open Graph Benchmark: Datasets
for Machine Learning on Graphs. arXiv preprint arXiv:2005.00687 (2020).
[33] Yuanming Hu, Tzu-Mao Li, Luke Anderson, Jonathan Ragan-Kelley, and Frédo
Durand. 2019. Taichi: A Language for High-Performance Computation on Spa-
tially Sparse Data Structures. ACM Trans. Graph. 38, 6, Article 201 (nov 2019),
16 pages. https://doi.org/10.1145/3355089.3356506

[34] Yuwei Hu, Zihao Ye, Minjie Wang, Jiali Yu, Da Zheng, Mu Li, Zheng Zhang, Zhiru
Zhang, and Yida Wang. 2020. FeatGraph: A Flexible and Efficient Backend for
Graph Neural Network Systems. In Proceedings of the International Conference for
High Performance Computing, Networking, Storage and Analysis (Atlanta, Georgia)
(SC ’20). IEEE Press, Article 71, 13 pages.

[35] Ziniu Hu, Yuxiao Dong, Kuansan Wang, and Yizhou Sun. 2020. Heterogeneous
Graph Transformer. In WWW ’20: The Web Conference 2020, Taipei, Taiwan, April
20-24, 2020. ACM / IW3C2, 2704–2710. https://doi.org/10.1145/3366423.3380027
[36] Guyue Huang, Guohao Dai, Yu Wang, and Huazhong Yang. 2020. GE-SpMM:
General-Purpose Sparse Matrix-Matrix Multiplication on GPUs for Graph Neural
Networks. In Proceedings of the International Conference for High Performance
Computing, Networking, Storage and Analysis (Atlanta, Georgia) (SC ’20). IEEE
Press, Article 72, 12 pages.

[37] Zhihao Jia, Oded Padon, James Thomas, Todd Warszawski, Matei Zaharia, and
Alex Aiken. 2019. TASO: Optimizing Deep Learning Computation with Automatic
Generation of Graph Substitutions. In Proceedings of the 27th ACM Symposium on
Operating Systems Principles (Huntsville, Ontario, Canada) (SOSP ’19). Association
for Computing Machinery, New York, NY, USA, 47–62. https://doi.org/10.1145/
3341301.3359630

[38] Thomas N. Kipf and Max Welling. 2017. Semi-Supervised Classification with
Graph Convolutional Networks. In 5th International Conference on Learning
Representations, ICLR 2017, Toulon, France, April 24-26, 2017, Conference Track
Proceedings. OpenReview.net. https://openreview.net/forum?id=SJU4ayYgl
[39] Fredrik Kjolstad, Peter Ahrens, Shoaib Kamil, and Saman Amarasinghe. 2019.
Tensor Algebra Compilation with Workspaces. (2019), 180–192. http://dl.acm.
org/citation.cfm?id=3314872.3314894

[40] Fredrik Kjolstad, Shoaib Kamil, Stephen Chou, David Lugato, and Saman Amaras-
inghe. 2017. The Tensor Algebra Compiler. Proc. ACM Program. Lang. 1, OOPSLA,
Article 77 (oct 2017), 29 pages. https://doi.org/10.1145/3133901

[41] François Lagunas, Ella Charlaix, Victor Sanh, and Alexander Rush. 2021. Block
Pruning For Faster Transformers. In Proceedings of the 2021 Conference on

SparseTIR: Composable Abstractions for Sparse Compilation in Deep Learning

Empirical Methods in Natural Language Processing. Association for Computa-
tional Linguistics, Online and Punta Cana, Dominican Republic, 10619–10629.
https://doi.org/10.18653/v1/2021.emnlp-main.829

[42] Ao Li, Bojian Zheng, Gennady Pekhimenko, and Fan Long. 2022. Automatic
Horizontal Fusion for GPU Kernels. In Proceedings of the 20th IEEE/ACM Interna-
tional Symposium on Code Generation and Optimization (Virtual Event, Republic
of Korea) (CGO ’22). IEEE Press, 14–27. https://doi.org/10.1109/CGO53902.2022.
9741270

[43] Jiajia Li, Bora Uçar, Ümit V. Çatalyürek, Jimeng Sun, Kevin Barker, and Richard
Vuduc. 2019. Efficient and Effective Sparse Tensor Reordering. In Proceedings
of the ACM International Conference on Supercomputing (Phoenix, Arizona) (ICS
’19). Association for Computing Machinery, New York, NY, USA, 227–237. https:
//doi.org/10.1145/3330345.3330366

[44] Chien-Yu Lin, Liang Luo, and Luis Ceze. 2021. Accelerating SpMM Kernel with
Cache-First Edge Sampling for Graph Neural Networks. CoRR abs/2104.10716
(2021). arXiv:2104.10716 https://arxiv.org/abs/2104.10716

[45] Atefeh Mehrabi, Donghyuk Lee, Niladrish Chatterjee, Daniel J. Sorin, Benjamin C.
Lee, and Mike O’Connor. 2021. Learning Sparse Matrix Row Permutations for
Efficient SpMM on GPU Architectures. In 2021 IEEE International Symposium on
Performance Analysis of Systems and Software (ISPASS). 48–58. https://doi.org/10.
1109/ISPASS51385.2021.00016

[46] Israt Nisa, Aravind Sukumaran-Rajam, Sureyya Emre Kurt, Changwan Hong, and
P. Sadayappan. 2018. Sampled Dense Matrix Multiplication for High-Performance
Machine Learning. In 2018 IEEE 25th International Conference on High Performance
Computing (HiPC). 32–41. https://doi.org/10.1109/HiPC.2018.00013

[47] D Stott Parker. 1995. Random butterfly transformations with applications in

computational linear algebra. (1995).

[48] Gregoire Pichon, Mathieu Faverge, Pierre Ramet, and Jean Roman. 2017. Re-
ordering Strategy for Blocking Optimization in Sparse Linear Solvers. SIAM J.
Matrix Anal. Appl. 38, 1 (2017), 226–248. https://doi.org/10.1137/16M1062454
arXiv:https://doi.org/10.1137/16M1062454

[49] Jonathan Ragan-Kelley, Connelly Barnes, Andrew Adams, Sylvain Paris, Frédo
Durand, and Saman Amarasinghe. 2013. Halide: A Language and Compiler
for Optimizing Parallelism, Locality, and Recomputation in Image Processing
Pipelines. In Proceedings of the 34th ACM SIGPLAN Conference on Programming
Language Design and Implementation (Seattle, Washington, USA) (PLDI ’13).
Association for Computing Machinery, New York, NY, USA, 519–530. https:
//doi.org/10.1145/2491956.2462176

[50] Md. Khaledur Rahman, Majedul Haque Sujon, and Ariful Azad. 2021. FusedMM:
A Unified SDDMM-SpMM Kernel for Graph Embedding and Graph Neural Net-
works. In 2021 IEEE International Parallel and Distributed Processing Symposium
(IPDPS). 256–266. https://doi.org/10.1109/IPDPS49936.2021.00034

[51] Petar Ristoski, Gerben Klaas Dirk de Vries, and Heiko Paulheim. 2016. A Collec-
tion of Benchmark Datasets for Systematic Evaluations of Machine Learning on
the Semantic Web. In The Semantic Web – ISWC 2016, Paul Groth, Elena Simperl,
Alasdair Gray, Marta Sabou, Markus Krötzsch, Freddy Lecue, Fabian Flöck, and
Yolanda Gil (Eds.). Springer International Publishing, Cham, 186–194.

[52] Jared Roesch, Steven Lyubomirsky, Logan Weber, Josh Pollock, Marisa Kirisame,
Tianqi Chen, and Zachary Tatlock. 2018. Relay: A New IR for Machine Learning
Frameworks. In Proceedings of the 2nd ACM SIGPLAN International Workshop on
Machine Learning and Programming Languages (Philadelphia, PA, USA) (MAPL
2018). Association for Computing Machinery, New York, NY, USA, 58–68. https:
//doi.org/10.1145/3211346.3211348

[53] Michael Schlichtkrull, Thomas N Kipf, Peter Bloem, Rianne van den Berg, Ivan
Titov, and Max Welling. 2017. Modeling Relational Data with Graph Convolu-
tional Networks. arXiv preprint arXiv:1703.06103 (2017).

[54] Prithviraj Sen, Galileo Namata, Mustafa Bilgic, Lise Getoor, Brian Gallagher, and
Tina Eliassi-Rad. 2008. Collective Classification in Network Data. AI Mag. 29, 3
(2008), 93–106. https://doi.org/10.1609/aimag.v29i3.2157

[55] Ryan Senanayake, Changwan Hong, Ziheng Wang, Amalee Wilson, Stephen
Chou, Shoaib Kamil, Saman Amarasinghe, and Fredrik Kjolstad. 2020. A Sparse
Iteration Space Transformation Framework for Sparse Tensor Algebra. Proc.
ACM Program. Lang. 4, OOPSLA, Article 158 (Nov. 2020), 30 pages.
https:
//doi.org/10.1145/3428226

[56] Junru Shao, Xiyou Zhou, Siyuan Feng, Bohan Hou, Ruihang Lai, Hongyi Jin,
Wuwei Lin, Masahiro Masuda, Cody Hao Yu, and Tianqi Chen. 2022. Tensor
Program Optimization with Probabilistic Programs. https://doi.org/10.48550/
ARXIV.2205.13603

[57] Patricia Suriana, Andrew Adams, and Shoaib Kamil. 2017. Parallel Associative
Reductions in Halide. In Proceedings of the 2017 International Symposium on Code
Generation and Optimization (Austin, USA) (CGO ’17). IEEE Press, 281–291.
[58] Ruiqin Tian, Luanzheng Guo, Jiajia Li, Bin Ren, and Gokcen Kestor. 2021. A
High Performance Sparse Tensor Algebra Compiler in MLIR. In 2021 IEEE/ACM
7th Workshop on the LLVM Compiler Infrastructure in HPC (LLVM-HPC). 27–38.
https://doi.org/10.1109/LLVMHPC54804.2021.00009

[59] Philippe Tillet, H. T. Kung, and David Cox. 2019. Triton: An Intermediate Language
and Compiler for Tiled Neural Network Computations. Association for Computing
Machinery, New York, NY, USA, 10–19. https://doi.org/10.1145/3315508.3329973

[60] Nicolas Vasilache, Oleksandr Zinenko, Aart J. C. Bik, Mahesh Ravishankar,
Thomas Raoux, Alexander Belyaev, Matthias Springer, Tobias Gysi, Diego Ca-
ballero, Stephan Herhut, Stella Laurenzo, and Albert Cohen. 2022. Composable
and Modular Code Generation in MLIR: A Structured and Retargetable Approach
to Tensor Compiler Construction. CoRR abs/2202.03293 (2022). arXiv:2202.03293
https://arxiv.org/abs/2202.03293

[61] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones,
Aidan N. Gomez, Łukasz Kaiser, and Illia Polosukhin. 2017. Attention is All You
Need. In Proceedings of the 31st International Conference on Neural Information
Processing Systems (Long Beach, California, USA) (NIPS’17). Curran Associates
Inc., Red Hook, NY, USA, 6000–6010.

[62] Petar Veličković, Guillem Cucurull, Arantxa Casanova, Adriana Romero, Pietro
Liò, and Yoshua Bengio. 2018. Graph Attention Networks. In International Confer-
ence on Learning Representations. https://openreview.net/forum?id=rJXMpikCZ
[63] Richard Vuduc, James W Demmel, and Katherine A Yelick. 2005. OSKI: A library
of automatically tuned sparse matrix kernels. Journal of Physics: Conference Series
16 (jan 2005), 521–530. https://doi.org/10.1088/1742-6596/16/1/071

[64] Endong Wang, Qing Zhang, Bo Shen, Guangyong Zhang, Xiaowei Lu, Qing
Wu, and Yajuan Wang. 2014. Intel math kernel library. In High-Performance
Computing on the Intel® Xeon Phi™. Springer, 167–188.

[65] Minjie Wang, Da Zheng, Zihao Ye, Quan Gan, Mufei Li, Xiang Song, Jinjing Zhou,
Chao Ma, Lingfan Yu, Yu Gai, Tianjun Xiao, Tong He, George Karypis, Jinyang
Li, and Zheng Zhang. 2019. Deep Graph Library: A Graph-Centric, Highly-
Performant Package for Graph Neural Networks. arXiv preprint arXiv:1909.01315
(2019).

[66] Yuke Wang, Boyuan Feng, and Yufei Ding. 2021. TC-GNN: Accelerating Sparse
Graph Neural Network Computation Via Dense Tensor Core on GPUs. CoRR
abs/2112.02052 (2021). arXiv:2112.02052 https://arxiv.org/abs/2112.02052
[67] Yuke Wang, Boyuan Feng, and Yufei Ding. 2022. QGTC: Accelerating Quantized
Graph Neural Networks via GPU Tensor Core. In Proceedings of the 27th ACM
SIGPLAN Symposium on Principles and Practice of Parallel Programming (Seoul,
Republic of Korea) (PPoPP ’22). Association for Computing Machinery, New York,
NY, USA, 107–119. https://doi.org/10.1145/3503221.3508408

[68] Yuke Wang, Boyuan Feng, Gushu Li, Shuangchen Li, Lei Deng, Yuan Xie, and
Yufei Ding. 2021. GNNAdvisor: An Adaptive and Efficient Runtime System for
GNN Acceleration on GPUs. In 15th USENIX Symposium on Operating Systems
Design and Implementation (OSDI 21). USENIX Association, 515–531. https:
//www.usenix.org/conference/osdi21/presentation/wang-yuke

[69] Yidi Wu, Kaihao Ma, Zhenkun Cai, Tatiana Jin, Boyang Li, Chengguang Zheng,
James Cheng, and Fan Yu. 2021. Seastar: vertex-centric programming for graph
neural networks. In EuroSys ’21: Sixteenth European Conference on Computer
Systems, Online Event, United Kingdom, April 26-28, 2021, Antonio Barbalace,
Pramod Bhatotia, Lorenzo Alvisi, and Cristian Cadar (Eds.). ACM, 359–375. https:
//doi.org/10.1145/3447786.3456247

[70] Wang Xiao, Ji Houye, Shi Chuan, Wang Bai, Cui Peng, Yu P., and Ye Yanfang.

2019. Heterogeneous Graph Attention Network. WWW (2019).

[71] Zhiqiang Xie, Minjie Wang, Zihao Ye, Zheng Zhang, and Rui Fan. 2022. Graphiler:
Optimizing Graph Neural Networks with Message Passing Data Flow Graph.
In Proceedings of Machine Learning and Systems, D. Marculescu, Y. Chi, and
C. Wu (Eds.), Vol. 4. 515–528. https://proceedings.mlsys.org/paper/2022/file/
a87ff679a2f3e71d9181a67b7542122c-Paper.pdf

[72] Carl Yang, Aydin Buluç, and John D. Owens. 2018. Design Principles for Sparse
Matrix Multiplication on the GPU. In Euro-Par 2018: Parallel Processing - 24th
International Conference on Parallel and Distributed Computing, Turin, Italy, August
27-31, 2018, Proceedings (Lecture Notes in Computer Science, Vol. 11014), Marco
Aldinucci, Luca Padovani, and Massimo Torquati (Eds.). Springer, 672–687. https:
//doi.org/10.1007/978-3-319-96983-1_48

[73] Zhongming Yu, Guohao Dai, Guyue Huang, Yu Wang, and Huazhong Yang.
2021. Exploiting Online Locality and Reduction Parallelism for Sampled Dense
Matrix Multiplication on GPUs. In 39th IEEE International Conference on Computer
Design, ICCD 2021, Storrs, CT, USA, October 24-27, 2021. IEEE, 567–574. https:
//doi.org/10.1109/ICCD53106.2021.00092

[74] Jie Zhao, Bojie Li, Wang Nie, Zhen Geng, Renwei Zhang, Xiong Gao, Bin Cheng,
Chen Wu, Yun Cheng, Zheng Li, Peng Di, Kun Zhang, and Xuefeng Jin. 2021. AKG:
Automatic Kernel Generation for Neural Processing Units Using Polyhedral Trans-
formations. In Proceedings of the 42nd ACM SIGPLAN International Conference
on Programming Language Design and Implementation (Virtual, Canada) (PLDI
2021). Association for Computing Machinery, New York, NY, USA, 1233–1248.
https://doi.org/10.1145/3453483.3454106

[75] Lianmin Zheng, Chengfan Jia, Minmin Sun, Zhao Wu, Cody Hao Yu, Ameer
Haj-Ali, Yida Wang, Jun Yang, Danyang Zhuo, Koushik Sen, Joseph E. Gonzalez,
and Ion Stoica. 2020. Ansor: Generating High-Performance Tensor Programs
for Deep Learning. In 14th USENIX Symposium on Operating Systems Design and
Implementation (OSDI 20). USENIX Association, 863–879. https://www.usenix.
org/conference/osdi20/presentation/zheng

[76] Ningxin Zheng, Bin Lin, Quanlu Zhang, Lingxiao Ma, Yuqing Yang, Fan Yang,
Yang Wang, Mao Yang, and Lidong Zhou. 2022. SparTA: Deep-Learning Model

Sparsity via Tensor-with-Sparsity-Attribute. In 16th USENIX Symposium on Oper-
ating Systems Design and Implementation (OSDI 22). USENIX Association, Carls-
bad, CA, 213–232. https://www.usenix.org/conference/osdi22/presentation/
zheng-ningxin

[77] Size Zheng, Yun Liang, Shuo Wang, Renze Chen, and Kaiwen Sheng. 2020. Flex-
Tensor: An Automatic Schedule Exploration and Optimization Framework for
Tensor Computation on Heterogeneous System. In ASPLOS ’20: Architectural

Support for Programming Languages and Operating Systems, Lausanne, Switzer-
land, March 16-20, 2020, James R. Larus, Luis Ceze, and Karin Strauss (Eds.). ACM,
859–873. https://doi.org/10.1145/3373376.3378508

[78] Aojun Zhou, Yukun Ma, Junnan Zhu, Jianbo Liu, Zhijie Zhang, Kun Yuan, Wenxiu
Sun, and Hongsheng Li. 2021. Learning N: M Fine-grained Structured Sparse
Neural Networks From Scratch. In 9th International Conference on Learning Rep-
resentations, ICLR 2021, Virtual Event, Austria, May 3-7, 2021. OpenReview.net.
https://openreview.net/forum?id=K9bw7vqp_s

Zihao Ye, Ruihang Lai, Junru Shao, Tianqi Chen, and Luis Ceze

