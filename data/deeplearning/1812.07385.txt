8
1
0
2

c
e
D
5
1

]

G
L
.
s
c
[

1
v
5
8
3
7
0
.
2
1
8
1
:
v
i
X
r
a

Perturbation Analysis of Learning Algorithms:
A Unifying Perspective on Generation
of Adversarial Examples

Emilio Rafael Balda, Arash Behboodi, and Rudolf Mathar∗

December 19, 2018

Abstract

Despite the tremendous success of deep neural networks in various learning problems,
it has been observed that adding an intentionally designed adversarial perturbation to
inputs of these architectures leads to erroneous classiﬁcation with high conﬁdence in the
prediction. In this work, we propose a general framework based on the perturbation analysis
of learning algorithms which consists of convex programming and is able to recover many
current adversarial attacks as special cases. The framework can be used to propose novel
attacks against learning algorithms for classiﬁcation and regression tasks under various new
constraints with closed form solutions in many instances. In particular we derive new attacks
against classiﬁcation algorithms which are shown to achieve comparable performances to
notable existing attacks. The framework is then used to generate adversarial perturbations for
regression tasks which include single pixel and single subset attacks. By applying this method
to autoencoding and image colorization tasks, it is shown that adversarial perturbations can
eﬀectively perturb the output of regression tasks as well.

1

Introduction

Deep Neural Networks (DNNs) excelled in recent years in many learning tasks and demonstrated
outstanding achievements in speech analysis [HDY+12] and visual tasks [KSH12, HZRS16,
SLJ+15, RHGS17]. Despite their success, they have been shown to suﬀer from instability in
their classiﬁcation under adversarial perturbations [SZS+14]. Adversarial perturbations are
intentionally worst case designed noises that aim at changing the output of a DNN to an incorrect
one. The explosion of research during past years makes it almost impossible to refer to all
important works in this area and do justice to all excellent works. However, we refer to several
important results from the literature, that are highly connected to this paper.

Although DNNs might achieve robustness to random noise [FMDF16], it was shown that there
is a clear distinction between the robustness of a classiﬁer to random noise and its robustness to
adversarial perturbations. The existence of adversarial perturbations was known for machine
learning algorithms [BNJT10], however, they were ﬁrst noticed in deep learning research in
[SZS+14]. The peculiarity of adversarial perturbations lied in the fact that they managed to fool
state of the art networks into making conﬁdent and wrong decisions in classiﬁcation tasks, and
they, nevertheless, appeared unperceived to the naked eye. These discoveries gave rise to extensive
research on understanding the instability of DNNs, exploring various attacks and devising multiple
defenses (for instance refer to [AM18, WGQ17, FFF15] and references therein). Most adversarial
attacks fall generally into two classes, white-box and black-box attacks. In white-box attacks,
the attacker knows completely the architecture of the target algorithms and additionally, there

∗Institute for Theoretical Information Technology (TI), RWTH Aachen University.

1

 
 
 
 
 
 
are attacks with partial knowledge of the architecture. However, black-box attacks require no
information about the target neural network, see for instance [SBMC17]. In this work, the focus
is on white-box attacks. The overall aim of attacks, as in [GSS14, MDFF16, RRN12], is to apply
perturbations to the system inputs, that are not perceived by the system’s administrator, such
that the performance of the system is severely degraded.

Adversarial perturbations were obtained in [SZS+14] to maximize the prediction error at
the output and were approximated using box-constrained L-BFGS. The Fast Gradient Sign
Method (FGSM) in [GSS14] was based on ﬁnding the scaled sign of the gradient of the cost
function. Note that the FGSM aims at minimizing (cid:96)∞-norm of the perturbation while the former
algorithm minimizes (cid:96)2-norm of the perturbation under box constraint on the perturbed example.
More eﬀective attacks utilize either iterative procedures or randomizations. The algorithm
DeepFool [MDFF16] conducts an iterative linearization of the DNN to generate perturbations
that are minimal in the (cid:96)p-norm for p > 1. In [KGB16] the authors propose an iterative version
of FGSM, called Basic Iterative Method (BIM). This method was later extended in [MMS+18],
where randomness was introduced in the computation of adversarial perturbations. This attack
is called the Projected Gradient Descent (PGD) method and was employed in [MMS+18] to
devise a defense against adversarial examples. An iterative algorithm based on PGD combined
with randomization was introduced in [ACW18] and has been used to dismantle many defenses
so far [AC18]. Another popular way of generating adversarial examples is by constraining the
(cid:96)0-norm of the perturbation. These types of attacks are known as single pixel attacks [SVK17]
and multiple pixel attacks [PMJ+16].

An interesting feature of these perturbations is their generalization over other datasets and
DNNs [RRN12, GSS14]. These perturbations are called universal adversarial perturbations. This
is partly explained by the fact that certain underlying properties of the perturbation, such as
direction in case of image perturbation, matters the most and is therefore generalized through
diﬀerent datasets. For example, the attack from [TKP+18] shows that adversarial examples
transfer from one random instance of a neural network to another. In that work, the authors
showed the eﬀectiveness of these types of attacks for enhancing the robustness of neural networks,
since they provide diverse perturbations during adversarial training. Moreover, [MDFFF17]
showed the existence of universal adversarial perturbations that are independent from the system
and the target input.

Since the rise of adversarial examples for image classiﬁcation, novel algorithms have been
developed for attacking other types of systems. In the ﬁeld of computer vision, [MKBF17]
constructed an attack on image segmentation, while [XWZ+17] designed attacks for object
detection. The Houdini attack [CANK17] aims at distorting speech recognition systems. Moreover,
[PMSH16] taylored an attack for recurrent neural networks, and [LHL+17] for reinforcement
learning. Adversarial examples exist for probabilistic methods as well. For instance, [KFS18]
showed the existence of adversarial examples for generative models. For regression problems,
[TTV16] designed an attack that speciﬁcally targets variational autoencoders.

There are various theories regarding the nature of adversarial examples and the subject is
heavily investigated. Initially, the authors in [GSS14] proposed the linearity hypothesis where the
existence of adversarial images is attributed to the approximate linearity of classiﬁers, although
this hypothesis has been challenged in [TG16]. Some other theories focus mostly on decision
boundaries of classiﬁers and their analytic properties [FMDF16, FMDF17]. The work from
[RSL18] provides a framework for determining the robustness of a classiﬁer against adversarial
examples with some performance guarantees. For a more recent theoretical approach to this
problem refer to [TSE+18].

There exist several types of defenses against adversarial examples, as well as subsequent
methods for bypassing them. For instance, the authors in [CW17] proposed three attacks to
bypass defensive distillation of the adversarial perturbations [PMW+16]. Moreover, the attacks
from [ACW18], bypassed 7 out of 9 non-certiﬁed defenses of ICLR 2018 that claimed to be

2

white-box secure. The most common defense is adding adversarial examples to the training
set, also known as adversarial training. For that purpose diﬀerent adversarial attacks may be
employed. Recently, the PGD attack is used in [MMS+18] to provide the state of the art defense
against adversarial examples for various image classiﬁcation datasets.

1.1 Our Contribution

In this work, we focus on the generation of adversarial examples with a suﬃciently general
framework that includes many existing attacks and can be easily extended to generate new
attacks for diﬀerent scenarios. We build upon our previous work [BBM18] to introduce a
connection between perturbation analysis of learning algorithms and adversarial perturbations.
This leads to a general formulation for the problem of generating adversarial examples using
convex programming. The general framework includes many existing attacks as special cases,
provides closed form solutions and can be easily extended to generate new algorithms.
In
particular, we derive novel algorithms for designing adversarial attacks for classiﬁcation which
are benchmarked with state of the art attacks.

Another contribution of this paper is to employ this framework in context of adversarial
perturbations for regression problems, a topic that has not been yet widely explored. Regression
loss functions diﬀer from classiﬁcation loss functions in that it is suﬃcient to maximize the output
perturbation, for instance measured in (cid:96)2-norm. In classiﬁcation tasks, such a maximization
might not necessarily change the output label particularly because these perturbations might
push the instances far away from classiﬁcation margins. There is no natural margin in regression
tasks. We address various technical diﬃculties of this problem and use our framework to generate
adversarial examples for regression tasks. In particular single pixel and single subset attacks are
discussed. It is shown that this problem is related to the MaxCut problem and hence diﬃcult to
solve. We propose a greedy algorithm to overcome this issue.

Finally, the proposed algorithms are experimentally evaluated using state of the art bench-
marks for classiﬁcation and regression tasks. It is shown that our proposed method achieves
comparable and sometimes better performance than many existing attacks for classiﬁcation prob-
lems. Furthermore, it is shown that regression tasks such as image colorization and autoencoding
suﬀer from adversarial perturbations as well.

2 Fooling Classiﬁers with First-Order Perturbation Analysis

The perturbation analysis, also called sensitivity analysis, is used in signal processing for
analytically quantifying the error at the output of a system that occurs as consequence of a
known perturbation at the system’s input. Adversarial images can also be considered as a slightly
perturbed version of original images that manage to change the output of the classiﬁer. Indeed,
the generation of adversarial examples in [MDFF16, GSS14] is implicitly based on maximizing
the eﬀect of an input perturbation on a relevant function which is either the classiﬁer function
or the cost function used for training. In the FGSM, given in [GSS14], the perturbation at the
output of the training cost function is ﬁrst analyzed using ﬁrst-order perturbation analysis of
the cost function and then maximized to fool the algorithm. The DeepFool method, given in
[MDFF16], maximizes the output perturbation for the linearized approximation of the underlying
classiﬁer which is indeed its ﬁrst order-perturbation analysis. We develop further the connection
between perturbation analysis and adversarial examples in this section.

2.1 Adversarial Perturbation Design

As it was mentioned above, adversarial examples can be considered as perturbed version of
training examples by an adversarial perturbation η. The perturbation analysis of classiﬁers is
particularly diﬃcult in general since the classiﬁer function maps inputs to discrete set of labels

3

and therefore it is not diﬀerentiable. Instead, the classiﬁcation problem is slightly modiﬁed as
follows.

Deﬁnition 1 (Classiﬁcation). A classiﬁer is deﬁned by the mapping k : RM → [K]1 that maps
an input x ∈ RM to its estimated class k (x) ∈ [K]. The mapping k(·) is itself deﬁned by

k(x) = argmax

{fl (x)} ,

l∈[K]

(1)

where fl(x) : RM → R’s are called score functions representing the probability of class belonging.

The function f (x) given by the vector (f1(x), . . . , fm(x)) can be assumed to be diﬀerentiable

almost everywhere for many classiﬁers.

The problem of adversarial generation consists of ﬁnding a perturbation that changes the
classiﬁer’s output. However, it is desirable for adversarial perturbations to modify training
instances only in an insigniﬁcant and unnoticeable way. This is controlled by adding a constraint
on the adversarial perturbation. For instance, the perturbation generated by the FGSM is bounded
in the (cid:96)∞-norm and the DeepFool method directly minimizes the norm of the perturbation that
changes the classiﬁer’s output. While DeepFool might generate perturbations that are perceptible,
the FGSM might not change the classiﬁer’s output.

An intriguing property of adversarial examples is that the perturbation does not distort the
image signiﬁcantly so that the naked eye can not detect any notable change in the images. One
way of imposing this property in adversarial design is to constrain the input perturbation to keep
the output of the ground truth classiﬁer, also called oracle classiﬁer [WGQ17], intact. The oracle
classiﬁer represents the naked eye in case of image classiﬁcation. The score functions of the oracle
classiﬁer are denoted by gl(·). The undetectability constraint for an adversarial perturbation η is
formulated as

Lg(x, η) = gk(x)(x + η) − max
l(cid:54)=k(x)

gl(x + η) > 0 .

(2)

Therefore the problem of adversarial design can be formulated as follows.

Problem 1 (Adversarial Generation Problem). For a given x ∈ RM , ﬁnd a perturbation η ∈ RM
to fool the classiﬁer k(·) by the adversarial sample ˆx = x + η such that k(x) (cid:54)= k(ˆx) and the
oracle classiﬁer is not changed, i.e.,

Find : η

s.t. Lf (x, η) = fk(x)(x + η) − max
l(cid:54)=k(x)
Lg(x, η) = gk(x)(x + η) − max
l(cid:54)=k(x)

fl(x + η) < 0

gl(x + η) > 0

(AGP)

The problem (AGP) is too general to be useful in practice directly. Next we explore diﬀerent
methods for making this problem tractable in some cases of interest. Since x and f are ﬁxed for
the attacker, we simplify the notation by dropping the subscript f and assuming that gradients
are always with respect to η, that is L(x, ·) = Lf (x, ·) and ∇L(x, ·) = ∇ηLf (x, ·). We keep
these shorthand notations throughout the paper.

2.2 Perturbation Analysis

There are two problems with the above formulation. First, the oracle function is not known
in general and second the function L(x, ·) can be non-convex. One solution is to approximate
L(x, ·) with a tractable function like linear functions which can be obtained through perturbation
analysis of each individual function. The constraint on the oracle function can also be replaced

1We denote the set {1, . . . , n} by [n] for n ∈ N.

4

with constraints on the perturbation itself, for instance by imposing upper bounds on the (cid:96)p-norm
of the perturbation. Diﬀerent classes of attacks can be obtained for diﬀerent choices of p and
are well known in the literature such as (cid:96)∞-attacks, (cid:96)2-attacks and (cid:96)1-attacks (see the survey in
[AM18] for details).

The ﬁrst order perturbation analysis of L yields

L(x, η) = L(x, 0) + ηT∇L(x, 0) + O((cid:107)η(cid:107)2

2),

where O((cid:107)η(cid:107)2
2) contains higher order terms. The condition that corresponds to the oracle function
can be approximated by (cid:107)η(cid:107)p ≤ ε for suﬃciently small ε ∈ R+. This means that the noise is
suﬃciently small in (cid:96)p-norm sense so that the observer does not notice it. These gradient and
norm relaxations yield the following alternative optimization problem

Find: η

s.t. L(x, 0) + ηT∇L(x, 0) < 0,

(cid:107)η(cid:107)p ≤ ε.

(3)

The above problem was also derived in [HA17] and is a convex optimization problem that can be
eﬃciently solved. As we will see later, this formulation of the problem can be relaxed into some
well known existing adversarial methods. However it is interesting to observe that this problem
is not always feasible as stated in the following proposition.

Theorem 1. The optimization problem (3) is not feasible if for q = p
p−1

ε(cid:107)∇L(x, 0)(cid:107)q < L(x, 0).

(4)

Proof. The proof follows a simple duality argument and is an elementary optimization theory
result. A similar result can be inferred from [HA17]. We repeat the proof for completeness.
Note that the dual norm of (cid:96)p is deﬁned by

(cid:107)x(cid:107)∗

p = sup{aTx : (cid:107)a(cid:107)p ≤ 1}.

p = (cid:107)x(cid:107)q for q = p

Furthermore (cid:107)x(cid:107)∗
ηT∇L(x, 0) is always bigger than −ε(cid:107)∇L(x, 0)(cid:107)∗
have

p−1 . Since the (cid:96)p-norm of η is bounded by ε, the value of
p. However if the condition (4) holds, then we

L(x, 0) + ηT∇L(x, 0) ≥ L(x, 0) − ε(cid:107)∇L(x, 0)(cid:107)∗

p > 0.

Therefore, the problem is not feasible.

Theorem 1 shows that given a vector x, the adversarial perturbation should have at least
is small, then it is easier to
(cid:96)p-norm equal to
fool the network by the (cid:96)p-attacks. In that sense, Theorem 1 provides an insight into the stability
of classiﬁers. In [MDFF16], the authors suggest that the robustness of the classiﬁers can be
measured as

. In other words if the ratio

L(x,0)
(cid:107)∇L(x,0)(cid:107)q

L(x,0)
(cid:107)∇L(x,0)(cid:107)q

ˆρ1(f ) =

1
|D|

(cid:88)

x∈D

(cid:107)ˆr(x)(cid:107)p
(cid:107)x(cid:107)p

,

where D denotes the test set and ˆr(x) is the minimum perturbation required to change the
classiﬁer’s output. The above theorem suggests that one can also use the following as the measure
of robustness

ˆρ2(f ) =

(cid:88)

1
|D|

L(x, 0)
(cid:107)∇L(x, 0)(cid:107)q

.

x∈D
The lower ˆρ2(f ), the easier it gets to fool the classiﬁer and therefore it becomes less robust
to adversarial examples. One can also look at other statistics related to
in order to
evaluate the robustness of classiﬁers.

L(x,0)
(cid:107)∇L(x,0)(cid:107)q

5

Theorem 1 shows that the optimization problem (3) might not be feasible. We propose to
get around this issue by solving an optimization problem which keeps only one of the constraints,
depending on the scenario, and selects an appropriate the objective function to preserve the
other constraint as much as possible. The objective function in this sense models the deviation
from the constraint and is minimized in the optimization problem. We consider two optimization
problems for this purpose.

First, the norm-constraint on the perturbation is preserved. The following optimization
problem, called gradient-based norm-constrained method (GNM), aims at minimizing L(x, 0) +
ηT∇L(x, 0) by solving the following problem:

(cid:8)L(x, 0) + ηT∇L(x, 0)(cid:9) s.t. (cid:107)η(cid:107)p ≤ ε .

min
η

(5)

This method ﬁnds the best perturbation under the norm-constraint. The constraint aims at
guaranteeing that the adversarial images are still imperceptible by an ordinary observer. Note
that (5) is fundamentally diﬀerent from [MDFF16, HA17], where the norm of the noise does not
appear as a constraint. Using a similar duality argument, the problem (5) has a closed form
solution given below.

Theorem 2. If ∇L(x, η) = ( ∂L(x,η)
the problem (5) is given by

∂η1

, . . . , ∂L(x,η)

∂ηM

), the closed form solution to the minimizer of

η = −ε

1
(cid:107)∇L(x, 0)(cid:107)q−1

q

sign(∇L(x, 0)) (cid:12) |∇L(x, 0)|q−1

(6)

for q = p
p−1 , where sign(·) and | · |q−1 are applied element-wise, and (cid:12) denotes the element-wise
(Hadamard) product. Particularly for p = ∞, we have q = 1 and the solution is given by the
following

η = −ε sign(∇L(x, 0)) .

(7)

Proof. Based on the duality argument from convex analysis, it is known that

sup
(cid:107)η(cid:107)p≤1

ηT∇L(x, 0) = (cid:107)∇L(x, 0)(cid:107)∗
p,

where (cid:107) · (cid:107)∗ is the dual norm. This implies that the objective function is lower bounded by
L(x, 0) − ε(cid:107)∇L(x, 0)(cid:107)∗

p. It is easy to verify that the minimum is attained by (6).

The advantage of (5), apart from being convex and enjoying computationally eﬃcient solutions,
is that one can incorporate other convex constraints into the optimization problem to guarantee
additional required properties of the perturbation. Note that the introduced method in (5) can
also be used for other target functions or learning problems. If the training cost function is
maximized under a norm constraint, as in [GSS14], the solution of (5) with p = ∞ recovers
the adversarial perturbations obtained via the FGSM. The problem (5) guarantees that the
perturbation is small, however, it might not change the classiﬁer’s output.

The second optimization problem, on the other hand, preserves the constraint for changing
the classiﬁer’s output and minimizes the perturbation norm instead. The feasibility problem of
(3) can therefore be simpliﬁed to

min
η

(cid:107)η(cid:107)p

s.t. L(x, 0) + ηT∇L(x, 0) ≤ 0 ,

(8)

which recovers the result in [MDFF16] although without the iterative procedure. This problem
has a similar closed form solution.

6

Proposition 1. If ∇L(x, η) = ( ∂L(x,η)
is given by

∂η1

, . . . , ∂L(x,η)

∂ηM

), the closed form solution to the problem (8)

η = −

L(x, 0)
(cid:107)∇L(x, 0)(cid:107)q−1

q

sign(∇L(x, 0)) (cid:12) |∇L(x, 0)|q−1

(9)

for q = p

p−1 .

Note that the perturbation found in Proposition 1, like the solution to GNM, aligns with the
gradient of the classiﬁer function and they only diﬀer in their norm. Although the perturbation
in (9), unlike the solution to GNM, is able to fool the classiﬁer, the perturbation in (9) might be
perceptible by the oracle classiﬁer. There are other variants of adversarial generation methods
that rely on an implicit perturbation analysis of a relevant function. These methods can be easily
obtained by small modiﬁcation of the methods above.

Iterative procedures can be easily adapted to the current formulation by repeating the
optimization problem until the classiﬁer output changes while keeping the perturbation small
at each step. Later we provide an iterative version of the GNM and compare it with DeepFool
[MDFF16], as well as other methods.

Another class of methods relies on introducing randomness in the generation process. A
notable example is the PGD attack introduced in [MMS+18] which is one of the state of the art
attacks. The ﬁrst-order approximation is then taken around another point ˜η with ˜ε (cid:44) (cid:107) ˜η(cid:107)p ≤ ε.
In other words we approximate L(x, ·) by a linear function around the point ˜η within an ˜ε-radius
from η = 0. This new point ˜η can be computed at random using arbitrary distributions with
(cid:96)p-norm bounded by ε. Changing the center of the ﬁrst order approximation from 0 to ˜η does
not change the nature of the problem since L(x, η) ≈ L(x, ˜η) + (η − ˜η)T∇L(x, ˜η) leads to the
following problem

L(x, ˜η) + (η − ˜η)T∇L(x, ˜η)

s.t. (cid:107)η(cid:107)p ≤ ε,

min
η

which is equivalent to:

ηT∇L(x, ˜η)

s.t. (cid:107)η(cid:107)p ≤ ε .

min
η

(10)

From this result one can add randomness to the computation of adversarial examples by
selecting ˜η in a random fasion. This is desirable when training models with adversarial examples
since it increases the diversity of the adversarial perturbations during training [TKP+18].

3 From Classiﬁcation to Regression

In classical statistical learning theory, regression problems are deﬁned in the following manner.
Given N ∈ N samples {(xi, yi)}N
i=1 drawn according to some unknown distribution PX,Y , a
regression model computes a function f : RM → RK that aims to minimize the expected loss
EP (L(f (x), y)), where L : RM × RK → R is a function that measures the similarity between
f (x) and y. While logarithmic losses are popular in classiﬁcation problems, the squared loss
L(f (x), y) = (cid:107)f (x) − y(cid:107)2
2 is mostly used for the general regression setting. For the sake of
notation, given y and f , let us redeﬁne L(x, η) as L(x, η) = L(f (x + η), y).

For a given f , x and y, an adversarial attacker ﬁnds an additive perturbation vector η that
is imperceptible to the administrator of the target system, while maximizing the loss of the
perturbed input L(x, η) as

max
η

L(x, η)

s.t. η is imperceptible .

In contrast with classiﬁcation problems where maximum perturbations at the output might not
change the class, adversarial instances maximize the output perturbation in regression problems.

7

As in (5), a constraint on the (cid:96)p-norm of η models imperceptibility leading to the following

formulation of the problem

max
η

(cid:107)y − f (x + η)(cid:107)2
2

s.t. (cid:107)η(cid:107)p ≤ ε .

(11)

Consider the image colorization problem where the goal is to add proper coloring on top of gray
scale images. In this problem, f (·) is the regression algorithm and assumed to be known however
the ground truth colorization y is generally unknown. Without knowing y, the optimization
problem (11) is ill posed and cannot be solved in general. There are some cases where the output
y is known by the nature of the problem, for instance, when f (·) is an encoder-decoder pair as in
autoencoders for which y = x.

Since the goal is to perturb the acting regression algorithm, we can assume that y ≈ f (x)
which means that the algorithm provides a good although not perfect approximation of the
ground truth function. We use the formulation in (11) and discuss the implications of applying
the approximation y ≈ f (x) in later sections.

3.1 A Quadratic Programming Problem

In general f (x) is a non-linear and non-convex function, so we have that L(x, ·) is non-convex.
Here again the perturbation analysis of f (·) can be used to relax (11) and to obtain a convex
formulation of the adversarial problem. The ﬁrst order perturbation analysis of f (x) yields the
approximation f (x + η) ≈ f (x) + Jf (x)η, where Jf (·) is the Jacobian matrix of f (·). This
approximation leads to the following convex approximation of L(x, ·):

L(x, η)

≈ (cid:107)y(cid:107)2
= (cid:107)y(cid:107)2

2 − 2yT(f (x) + Jf (x)η) + (cid:107)f (x) + Jf (x)η(cid:107)2
2
2 − 2yTf (x) + (cid:107)f (x)(cid:107)2
2

+ 2 (f (x) − y)T Jf (x)η + (cid:107)Jf (x)η(cid:107)2
2 .

Since the ﬁrst three terms of this expression do not depend on η, the optimization problem from
(11) is reduced to

max
η

2 (f (x) − y)T Jf (x)η + (cid:107)Jf (x)η(cid:107)2
2

s.t. (cid:107)η(cid:107)p ≤ ε .

(12)

The above convex maximization problem is, in general, challenging and NP-hard. Nevertheless,
since y is usually not known, we may use the assumption that y ≈ f (x), which simpliﬁes the
problem to

max
η

(cid:107)Jf (x)η(cid:107)2
2

s.t. (cid:107)η(cid:107)p ≤ ε .

(13)

Although this problem is a convex quadratic maximization under an (cid:96)p-norm constraint and
in general challenging, it can be solved eﬃciently in some cases. For general p, the maximum
value is indeed related to the operator norm of Jf (x) [HJ13]. This norm is central in stability
analysis of many signal processing algorithms (for instance see [FR13]). The operator norm of a
matrix A ∈ Cm×n between (cid:96)p and (cid:96)q is deﬁned as

(cid:107)A(cid:107)p→q (cid:44) sup
(cid:107)x(cid:107)p≤1

(cid:107)Ax(cid:107)q.

Using this notion, we can see that (cid:107) η
ε (cid:107)2 ≤ ε(cid:107)Jf (x)(cid:107)p→2.
Therefore, the problem of ﬁnding a solution to (13) amounts to ﬁnding the operator norm
(cid:107)Jf (x)(cid:107)p→2. First observe that the maximum value is achieved on the border namely for

ε (cid:107)p ≤ 1 leads to (cid:107)Jf (x)η(cid:107)2 = ε(cid:107)Jf (x) η

8

(cid:107)η(cid:107)p = ε. In the case where p = 2, this problem has a closed-form solution. If vmax is the unit
(cid:96)2-norm eigenvector corresponding to the maximum eigenvalue of Jf (x)TJf (x), then

η∗ = ±ε vmax

(14)

solves the optimization problem. The maximum eigenvalue of Jf (x)TJf (x) corresponds to the
square of the spectral norm (cid:107)Jf (x)(cid:107)2→2.

Another interesting case is when p = 1.

In general, the (cid:96)1-norm is usually used as a
regularization technique to promote sparsity. When the solution of a problem should satisfy
a sparsity constraint, the direct introduction of this constraint into the optimization leads to
NP-hardness of the problem. Instead the constraint is relaxed by adding (cid:96)1-norm regularization.
The adversarial perturbation designed in this way tends to have only a few non-zero entries. This
corresponds to scenarios like single pixel attacks where only a few pixels are supposed to change.
For this choice, we have

(cid:107)A(cid:107)1→2 = max
k∈[n]

(cid:107)ak(cid:107)2,

where ak’s are the columns of A. Therefore, if the columns of the Jacobian matrix are given by
Jf (x) = [J1 . . . JM ], then

and the maximum is attained with

(cid:107)Jf (x)η(cid:107)2 ≤ ε max
k∈[M ]

(cid:107)Jk(cid:107)2,

η∗ = ±εek∗

for

k∗ = argmax
k∈[M ]

(cid:107)Jk(cid:107)2,

(15)

where the vector ei is the i-th canonical vector. For the case of gray-scale images, where each
pixel is represented by a single entry of x, this constitutes a single pixel attack. Some additional
constraints must be added in the case of RGB images, where each pixel is represented by a set of
three values.

Finally, the case where the adversarial perturbation is bounded with the (cid:96)∞-norm is also
of particular interest. This bound guarantees that the noise entries have bounded values. The
problem of designing adversarial noise corresponds to ﬁnding (cid:107)Jf (x)(cid:107)∞→2. Unfortunately, this
problem turns out to be NP-hard [Roh00]. However, it is possible to approximate this norm
using semi-deﬁnite programming as proposed in [HH15]. Semi-deﬁnite programming scales badly
with input dimension in terms of computational complexity, namely O(n6) with n the underlying
dimension, and therefore might not be suitable for fast generation of adversarial examples when
the input dimension is very high. We address these problems later in Section 4, where we obtain
fast approximate solutions for (cid:107)Jf (x)(cid:107)∞→2 and single pixel attacks.

3.2 A Linear Programming Problem

The methods derived in Section 3.1 suﬀer from one main drawback, they require storing Jf (x) ∈
RK×M into memory. While this may be doable for some applications, it is not feasible for others.
For example, if the target system is an autoencoder for RGB images with size 680 × 480, that
is M = K = 680 · 480 · 3 ≈ 9 · 105, storing Jf (x) ∈ R9·105×9·105 requires loading around 8 · 1011
values into memory, which is in most cases not tractable. Note that, in order to solve (13) for
p = 2, we would require computing the eigenvalue decomposition of Jf (x)TJf (x) as well. This
motivates us to relax the problem into a linear programming problem as in Section 2, where
Jf (x) is computed implicitly and we do not require to store it. To that end, we relax (11) by
directly applying a ﬁrst order approximation of L, that is L(x, η) ≈ L(x, 0) + ηT∇L(x, 0) . Using
this approximation the problem from (11) is now simpliﬁed to

max
η

∇L(x, 0)Tη s.t. (cid:107)η(cid:107)p ≤ ε ,

(16)

9

where ∇L(x, 0) = −2Jf (x)T (y − f (x)). Note that the attacks discussed in Section 2 for
classiﬁcation follow the same formulation with another choice of L(x, ·). Therefore, the closed-
form solution of (16) can be obtained from (6).

Unfortunately using y ≈ f (x) yields zero gradient in (16), thus leaving this approximation
useless for obtaining adversarial perturbations. This problem is tackled by taking the approxima-
tion around another random point ˜η within and ˜ε-ball radius from η = 0 as in (10), with ˜ε ≤ ε.
As it was mentioned above, this dithering mechanism is also used in classiﬁcation problems for
instance in [MMS+18].

4 Single Subset Attacks

Another popular way of modeling undetectability, in the ﬁeld of image recognition, is by
constraining the number of pixels that can be modiﬁed by the attacker. This gave birth to single
and multiple pixel attacks. Note that, for the case of gray-scale images, the solutions obtained in
(15) and (6) provide already single pixels attacks. This is not true for RGB images where each
pixel is represented by a subset of three values. Since our analysis is not limited to image based
systems, we refer to these type of attacks which target only a subset of entries as single subset
attacks.

Since perturbations belong to RM , let us partition [M ] = {1, . . . , M } into S possible subsets
S1, . . . , SS. The sets can in general have diﬀerent cardinalities. However, we assume here that
all of them have the same cardinality of Z = M/S, where Ss = {i1
s } ⊆ [M ]. We deﬁne
the mixed zero-S norm (cid:107) · (cid:107)0,S of a vector, for the partition S = {S1, . . . , SS}, as the number of
subsets containing at least one index associated to a non-zero entry of x2:

s, . . . , iZ

(cid:107)x(cid:107)0,S =

S
(cid:88)

i=1

1((cid:107)xSi(cid:107) (cid:54)= 0).

Therefore, (cid:107)η(cid:107)0,S counts the number of subsets modiﬁed by an attacker. To guarantee that only
one subset is active, an additional constraint can be added to the optimization problem. This
leads to the following formulation of the single subset attack for the regression problem.

max
η

(cid:107)y − f (x + η)(cid:107)2
2

s.t. (cid:107)η(cid:107)∞ ≤ ε , (cid:107)η(cid:107)0,S = 1 .

(17)

A similar formulation holds as well for classiﬁcation problems. The mixed norm (cid:107).(cid:107)0,S in widely
used in signal processing and compressed sensing to promoting group sparsity [RRN12].

4.1 Single Subset Attack for the Quadratic Problem

As in Section 3.1, the approximations f (x + η) ≈ f (x) + Jf (x)η and y ≈ f (x) simplify the
problem (17) to

max
η

(cid:107)Jf (x)η(cid:107)2
2

s.t. (cid:107)η(cid:107)∞ ≤ ε , (cid:107)η(cid:107)0,S = 1 .

(18)

As it was mentioned above, the problem is NP-hard without the mixed-norm constraint. We try
to ﬁnd an approximate solution to a simpler problem where only the set Ss is to be modiﬁed by
the attacker for s ∈ [S]. Finding the perturbation on this set amounts to solving the following
problem:

ηs = argmax

η

(cid:107)Jf (x)η(cid:107)2

2 s.t. (cid:107)η(cid:107)∞ ≤ ε , (η)iz

s = 0 ∀iz

s /∈ Ss ,

(19)

s-th entry of η. As discussed in Section 3.1, this problem is NP-hard.
where (η)iz
Since the maximization of a quadratic bowl over a box constraint lies in the corner points of the

s denotes the iz

2Similar to the so-called (cid:96)0-norm, this is not a proper norm.

10

feasible set, we have:

with ρ∗
s
as follows:

(cid:44) (ρ∗
i1
s

, . . . , ρ∗
iZ
s

ηs = ε

Z
(cid:88)

z=1

ρ∗
iz
s

eiz

s

)T ∈ {−1, +1}Z. The optimization problem can be equivalently formulated

ρ∗

s = argmax

ρs∈{−1,+1}Z

= argmax

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
Z
(cid:88)

Jf (x)(ε

Z
(cid:88)

z=1

ρiz

s )
s eiz

(cid:13)
2
(cid:13)
(cid:13)
(cid:13)
(cid:13)
2

Z
(cid:88)

ρiz

s ρiw

s JT
iz
s

s ,
Jiw

ρs∈{−1,+1}Z

z=1

w=1

s

s

, . . . , ρiZ

for ρs (cid:44) (ρi1
)T ∈ {−1, +1}Z and Jk the k-th column of Jf (x). This problem is indeed
related to the well known MaxCut problem introduced by [GW95]. The literature is abound with
works on the MaxCut problem, the eﬃcient solutions and their recovery guarantees. A common
solution to this problem is a relaxation by a semi-deﬁnite programming problem. However, as we
discussed semi-deﬁnite programming solvers scales badly with the input dimension. Therefore, in
the spirit of obtaining fast and scalable approximate solutions, that can later be used to design
adversarial perturbations through iterative approximations, we propose to obtain approximate
solutions using a greedy approach. To that end, and without loss of generality, let us assume
that for a given Ss, the indices i1
(cid:107)2. An
approximate solution for ρ∗
= 1 and recursively
iz
s
calculating

(cid:107)2 ≥ · · · ≥ (cid:107)JiZ
is calculated in a greedy manner by setting ρ∗
i1
s

s ∈ Ss are sorted such that (cid:107)Ji1

s, . . . , iZ

s

s

ρ∗
iz
s

= sign






ρ∗
ij
s

Jij

s



Jiz

s


 ∀ z = 2, . . . , Z .

(20)







T



z−1
(cid:88)

j=1

As for greedy algorithms, this solution is fast, however, there is no optimality guarantee for it.
For the case where S = 1 and S = M , the expression (20) is an approximate solution for (13)
under the (cid:96)∞-norm constraint on the perturbation (i.e., p = ∞).

This method provides an approximate solution to the problem for a given choice of Ss. The

solution to (18) can then be obtained by solving the following problem:

η∗ = ηs∗ ,

(21)

with s∗ = argmax

s

(cid:107)Jf (x)ηs(cid:107)2

2 and ηs = ε

Z
(cid:88)

z=1

ρ∗
iz
s

eiz

s

This is based on naive exhaustive research over the subsets which is tractable only when the
number of subsets is small enough.

4.2 Single Subset Attack for the Linear Problem

Following the steps from Section 3.2, we make use of the approximation L(x, η) ≈ L(x, ˜η) + (η −
˜η)T∇L(x, ˜η) which leads to the formulation of (17) as a linear programming problem

ηT∇L(x, ˜η)

max
η

s.t. (cid:107)η(cid:107)∞ ≤ ε , (cid:107)η(cid:107)0,S = 1 .

(22)

In the same manner as Section 4.1, for a given subset Ss we deﬁne ηs as in (19). For this linear
problem that results in

ηs = argmax

η

∇L(x, ˜η)Tη s.t. (cid:107)η(cid:107)∞ ≤ ε , (η)iz

s = 0 ∀iz

s /∈ Ss .

11

Type of Attack Relaxed Problem Closed-Form Solution

(cid:96)2 / (cid:96)∞
constrained
Single-Subset
attack

(16)
(13)
(18)
(22)

(14) / (21)(cid:5)
(6)(cid:72)
(21)(cid:5)
(23)(cid:72)

Table 1: Summary of the obtained closed-form solutions. Remarks: ((cid:72)) valid for regression
and classiﬁcation, ((cid:5)) only an approximate solution.

Algorithm
FGSM [GSS14]
DeepFool [MDFF16]
BIM [KGB16]
PGD [MMS+18]
Ensemble [TKP+18]
Targeted
Ours

Objective function L
cross-entropy
(2) with l chosen using ˆρ1(f )
cross-entropy
cross-entropy
cross-entropy using another f
(2) with l ﬁxed to the target
(2)

Iterative Dithering

×
(cid:88)
(cid:88)
(cid:88)
(cid:88)
(cid:88)
(cid:88)

×
×
×
(cid:88)
×
(cid:88)
(cid:88)

Table 2: Recovering Existing Attacks in Classiﬁcation using this Framework.

In contrast to the deﬁnition of ηs from (19), in this case we have a closed form solution for ηs as

ηs = ε

Z
(cid:88)

z=1

sign((∇L(x, ˜η))iz

s ,
s )eiz

which implies that ∇L(x, ˜η)Tηs = (cid:80)Z
single subset attack (22) has the closed form solution

(cid:12)
(cid:12)(∇L(x, ˜η))iz

z=1

s

(cid:12)
(cid:12). Therefore, the linear problem for the

η∗ = ηs∗ , with s∗ = argmax

s

Z
(cid:88)

z=1

(cid:12)
(cid:12)(∇L(x, ˜η))iz

s

(cid:12)
(cid:12)

(23)

and ηs = ε (cid:80)Z
replacing L with L(x, η) = −(fk(x)(x + η) − maxl(cid:54)=k(x) fl(x + η)).

z=1 sign((∇L(x, ˜η))iz

s )eiz

s . This results are valid for classiﬁcation as well when

5

Iterative Versions of the Linear Problem

In the previous sections we have formulated several variations of the problem of generating
In the same spirit as
adversarial perturbations. These results are summarized in Table 1.
DeepFool, we make use of the obtained closed form solutions to design adversarial perturbations
using iterative approximations. In Algorithm 1 an iterative method based on the linear problem
(16) is introduced. This corresponds to a gradient ascent method for maximizing L(x, η) with a
ﬁxed number of iterations and steps of equal (cid:96)p-norm.

While generalizing the results for (16) into a gradient ascent method is trivial, the same is not
true for the quadratic problem (13). The main reason for this is that, using the approximation
y ≈ f (x), we were able to simplify (12) into (13) since y − f (x) ≈ 0. For an iterative version
of this solution we must successively approximate f (·) around diﬀerent points ˜x, which leads
to y − f (˜x) (cid:54)= 0 even if y = f (x). We leave the task of investigating alternatives for designing
iterative methods with the results for (13) for future works, and in Section 6 show that the
non-iterative solutions for this method are still competitive.

Finally, replacing line 5 of Algorithm 1 with

η∗

t ← argmax

η

ηT∇L(x, ˜ηt) s.t. (cid:107)η(cid:107)p ≤ ε , (cid:107)η(cid:107)0,S = 1

12

Algorithm 1 Iterative extension for (cid:96)p constrained methods.

input: x, f , T , ε, ˜ε1, . . . , εT .
output: η∗.
Initialize η1 ← 0.
for t = 1, . . . , T do

˜ηt ← ηt + random(˜εt)
η∗
t ← argmaxη ηT∇L(x, ˜ηt) s.t. (cid:107)η(cid:107)p ≤ ε/T (Table 1)
ηt+1 ← ηt + η∗
t

end for
return: η∗ ← ηT

original

adv

original

adv

nine

zero

airplane

ship

eight

three

truck

car

two

three

cat

dog

(a) MNIST

(b) CIFAR-10

Figure 1: Examples of correctly classiﬁed images that are misclassiﬁed when adversarial noise is
added using Algorithm 1.

leads to a multiple subset attack, since we modify the values of one subset at every iteration. At
every iteration, we may exclude the previously modiﬁed subsets from S in order to ensure that a
new subset is modiﬁed.

6 Experiments

In this section, the proposed methods are used to fool neural networks in classiﬁcation and
regression problems. The goal of this section is twofold. First, we would like to examine the
performance of the newly proposed attack in classiﬁcation tasks, thereby showing the utility of
current adversarial generation framework. Secondly we generate adversarial perturbations for
regression tasks which only received small attention in the literature. For this purpose we use
the MNIST [LCB10], CIFAR-10 [KH09], and STL-10 datasets.

6.1 Classiﬁcation

As discussed in Section 2, the appropriate loss function L(x, η) for image classiﬁcation tasks that
should be used in (5) is given by (AGP). For this problem, (cid:107)η(cid:107)∞ ≤ ε is a common constraint
that models the undetectability, for suﬃciently small ε, of adversarial noise by an observer.

13

(a) Autoencoder (96% compression)

(b) Autoencoder (50% compression)

(c) Image Colorization

Figure 2: Adversarial examples for (a): MNIST autoencoder obtained using quadratic-(cid:96)∞,
(b): CIFAR-10 autoencoder obtained using linear-pixel-100, (c): STL-10 colorization network
obtained using linear-(cid:96)∞-20.

14

inputoutputoriginaladversarialnoisy(random)inputoutputinputoutputoriginaladversarialnoisy(random)inputoutputinputoutputoriginaladversarialnoisy(random)inputoutput(a) FCNN

(b) LeNet-5

(c) NIN

(d) DenseNet

Figure 3: (a) and (b): Fooling ratio of the adversarial samples for diﬀerent values of ε on the
MNIST test dataset. (c) and (d): Fooling ratio of the adversarial samples for diﬀerent values of
ε on the CIFAR-10 test datasets.

However solving (5) involves ﬁnding the function L(x, 0) which is deﬁned as the minimum of
K − 1 functions with K being the number of diﬀerent classes. In large problems, this may
signiﬁcantly increase the computations required to fool one image. Therefore, we include a
simpliﬁed version of this algorithm in our simulations. The non-iterative methods might not
guarantee the fooling of the underlying network but on the other hand, the iterative methods
might suﬀer from convergence problems.

To benchmark the proposed adversarial algorithms, we consider following methods tested on

the aforementioned datasets:

• Algorithm 1: This algorithm solves (5) with L(x, ·) given by (AGP). Note that, for
evaluating L at a given x one must search over all l (cid:54)= k(x). This can be computationally
expensive when the number of possible classes (i.e., the number of possible values for l)
is large. The (cid:96)∞-norm is chosen for the constraint. Moreover, an example of adversarial
images obtained using this algorithm is shown in Figure 2.

• Algorithm 1-T : This is the iterative version of Algorithm 1 with T iterations. The
adversarial perturbation is the sum of T perturbation vectors with (cid:96)∞-norm of ε/T
computed through T successive approximations.

• Algorithm 2: This algorithm approximates (AGP) with L(x, η) ≈ fk(x)(x + η), thus
reducing the computation of L(x) when the number of classes is large. Note that we cannot
use L(x, η) < 0 to guarantee that we have fooled the network. Nevertheless, the lower the
value of L(x, η) the most likely it is that the network has been fooled. The same reasoning
is valid for the FGSM algorithm.

15

0.000.020.040.060.080.10ε020406080100FoolingRatio(in%)DeepFoolAlg1-10Alg1-5Alg1Alg2FGSMPGDrandom0.0000.0250.0500.0750.1000.1250.1500.1750.200ε020406080100FoolingRatio(in%)0.0000.0050.0100.0150.0200.0250.030ε020406080100FoolingRatio(in%)0.0020.0040.0060.0080.010ε020406080100FoolingRatio(in%)(a) MNIST: (cid:96)2 constrained

(b) MNIST: (cid:96)∞ constrained

(c) CIFAR-10: Multiple pixel attack(d) STL-10 (colorization): (cid:96)∞ constrained

Figure 4: Output PSNR for (a): MNIST autoencoder under (cid:96)2-norm constraint, (b): MNIST
autoencoder under (cid:96)∞-norm constraint, (c): CIFAR-10 autoencoder under multiple pixel attacks,
(d): STL-10 colorization network under (cid:96)∞-norm constraint.

• FGSM: This well-known method was proposed by [GSS14] where L(x, η) is replaced by
the negative training loss for the input x + η. Usually the cross-entropy loss is used for
this purpose. With the newly replaced function, (5) is solved for p = ∞.

• PGD: This method is the iterative version of FGSM (T > 1) with ˜ε1 = ε and ˜εt = 0 for

all t > 1. It constitutes one of the state of the art attacks in the literature.

• DeepFool: This method was proposed in [MDFF16] and makes use of iterative approxi-
mations. Every iteration of DeepFool can be written within our framework by replacing L
by

L(x, η) = fk(x)(x + η) − fˆl(x + η) , where
(cid:27)

(cid:26) |fk(x)(x) − fl(x)|

ˆl = argmin
l(cid:54)=k(x)

(cid:107)∇fk(x)(x) − ∇fl(x)(cid:107)q

.

The adversarial perturbations are computed using p = ∞, thus q = 1, with a maximum
of 50 iterations. These parameters were taken from [MDFF16]. Note that ˆl is chosen to
minimize the robustness ˆρ1(f ) for D = {x}.

• Random: For benchmarking purposes, we also consider random perturbations with
2 . This helps to demarcate

independent Bernoulli distributed entries with P(ε) = P(−ε) = 1
the essential diﬀerence of adversarial and random perturbations.

Note that these methods from the literature can be expressed in terms of the proposed framework
as summarized in Table 1. In that table we also include the black-box ensemble attack from

16

10152025303540inputPSNR(dB)121416182022outputPSNR(dB)rand-(cid:96)2linear-(cid:96)2-1linear-(cid:96)2-10linear-(cid:96)2-20quadratic-(cid:96)2-110152025303540inputPSNR(dB)121416182022outputPSNR(dB)rand-(cid:96)∞linear-(cid:96)∞-1linear-(cid:96)∞-10linear-(cid:96)∞-20quadratic-(cid:96)∞-10.10.20.30.40.50.60.7inputPSNR(dB)141618202224outputPSNR(dB)linear-pixel-1linear-pixel-10rand-pixel-1rand-pixel-1010152025303540inputPSNR(dB)152025303540outputPSNR(dB)rand-(cid:96)∞linear-(cid:96)∞-1linear-(cid:96)∞-10linear-(cid:96)∞-20[TKP+18] and targeted attacks [CW17, PMJ+16, BF17, CANK17, SBMC17]. In [TKP+18] the
target neural network function is not known thus another known neural network function f is
used instead, hoping that the obtained adversarial example transfers to the unknown network.
Targeted attacks are used when the objective is to generate adversarial examples that are classiﬁed
by target system as belonging to some given target class (cid:96) ∈ [K]. That corresponds to ﬁxing the
l in (2), that is L(x, η) = fk(x)(x + η) − fl(x + η).

The above methods are tested on the following deep neural network architectures:

• MNIST : A fully connected network with two hidden layers of size 150 and 100 respectively,

as well as the LeNet-5 architecture [LHBB99].

• CIFAR-10 : The Network In Network (NIN) architecture [LCY13], and a 40 layer DenseNet

[HLWvdM17].

As a performance measure, we use the fooling ratio deﬁned in [MDFF16] as the percentage
of correctly classiﬁed images that are missclassiﬁed when adversarial perturbations are applied.
Of course, the fooling ratio depends on the constraint on the norm of adversarial examples.
Therefore, in Figure 3 we observe the fooling ratio for diﬀerent values of ε on the aforementioned
neural networks. As expected, the increased computational complexity of iterative methods
such as DeepFool and Algorithm 1-T translates into increased performance with respect to
non-iterative methods. Nevertheless, as shown in Figures 3(a) and (c), the performance gap
between iterative and non-iterative algorithms is not always signiﬁcant. For the case of iterative
algorithms, the proposed Algorithm 1-T outperforms DeepFool and PGD. The same holds true
for Algorithm 1 with respect to other non-iterative methods such as FGSM, while Algorithm 2
obtains competitive performance with respect to FGSM. However, note that adversarial training
using PGD is the state of the art defense against adversarial examples, thus PGD may still be a
better choice than Algorithm 1-T for adversarial training.

Finally, we measure the robustness of diﬀerent networks using ˆρ1(f ) and ˆρ2(f ), with p = ∞.
We also include the minimum ε, such that DeepFool obtains a fooling ratio greater than 99%,
as a performance measure as well. These results are summarized in Table 3, where we obtain
coherent results between the 3 measures.

FCNN (MNIST)
LeNet-5 (MNIST)
NIN (CIFAR-10)
DenseNet (CIFAR-10)

Test
error
1.7%
0.9%
13.8%
5.2%

ˆρ1(f )
[MDFF16]
0.036
0.077
0.012
0.006

fooled
ˆρ2(f )
>99%
(ours)
0.034
ε =0.076
0.061 ε =0.164
0.004 ε =0.018
ε =0.010
0.002

Table 3: Robustness measures for diﬀerent classiﬁers.

6.2 Regression

For the sake of clarity we use the notation quadratic-(cid:96)p to denote the method of computing
adversarial perturbations by solving the quadratic problem (13) under the (cid:96)p-norm constraint.
In the same manner, Algorithm 1 with T iterations and the (cid:96)p-norm constraint is referred to
as linear-(cid:96)p-T . Since the experiments carried out in this section are exclusively image based,
we use the notation linear-pixel-T to denote the multiple subset attack with (cid:107)η(cid:107)0,S = T . Since
the aim of the proposed attacks is to maximize the MSE of the target system, we use the
Peak-Signal-to-Noise Ratio (PSNR), which is a common measure for image quality and is deﬁned
as PSNR = (maximum pixel value)2/MSE, as the performance metric.

Similarly to [MDFF16], we show the validity of our methods by comparing their performance
against appropriate types of random noise. For p = 2 the random perturbation is computed as

17

η = ε w/(cid:107)w(cid:107)2, where the entries of w are independently drawn from a Gaussian distribution.
For p = ∞ the random perturbation η has independent Bernoulli distributed entries with
P(ε) = P(−ε) = 1/2. In the case of multiple subset attacks we perform the same approach as for
p = ∞ but only on T randomly chosen pixels, while setting the other pixels of η to zero. In order
to keep a consistent notation, we refer to these 3 methods of generating random perturbations as
random-(cid:96)2, random-(cid:96)∞, and random-pixel-T respectively. For our experiments we use the MNIST,
CIFAR-10 and STL-10 datasets. A diﬀerent neural network is trained for each of these datasets.
As in [TTV16], we also consider autoencoders. For MNIST and CIFAR-10 we have trained
fully connected autoencoders with 96% and 50% compression rates respectively. In addition,
we go beyond autoencoders and train the image colorization architecture from [BMRG17] for
the STL-10 dataset. Diﬀerent example images obtained from applying the proposed methods
on these networks are shown in Figure 2. For instance, in Figure 2(a) we observe that the
autoencoder trained on MNIST is able to denoise random perturbation correctly but fails to do
so with adversarial perturbations obtained using the quadratic-(cid:96)∞ method. Similarly, in Figure
2(b), the random-pixel-100 algorithm distorts the output signiﬁcantly more than its random
counterpart. These two experiments align with the observation of [TTV16] that autoencoders
tend to be more robust to adversarial attacks than deep neural networks used for classiﬁcation.
The deep neural network trained for colorization is highly sensitive to adversarial perturbations
as illustrated in Figure 2(c), where the original and adversarial images are nearly identical.

While the results shown in Figure 2 are for some particular images, in Figure 4 we measure
the performance of diﬀerent adversarial attacks using the average output PSNR over 20 randomly
selected images from the corresponding datasets. In Figures 4(a) and 4(b) we observe how
computing adversarial perturbations through successive linearizations improves the performance.
This behavior is more pronounced in Figure 4(d), where iterative linearizations are responsible for
more than 10 dB of output PSNR reduction. Note that, in Figures 4(a) and 4(b) the non-iterative
quadratic-(cid:96)p algorithm performs competitively, even when compared to iterative methods. In
Figure 4(b) we observe that the autoencoder trained on CIFAR-10 is robust to single pixel
attacks. However, an important degradation of the systems performance, with respect to random
noise, can be obtained through adversarial perturbations in the 100 pixels attack (≈ 9.7% of
the total number of pixels). Finally, in Figure 4(d), we can clearly observe the instability of the
image colorization network to adversarial attacks. These experiments show that, even though
autoencoders are somehow robust to adversarial noise, this may not be true for deep neural
networks in other regression problems.

7 Conclusion

The perturbation analysis of diﬀerent learning algorithms leads to a framework for generating
adversarial examples via convex programming. For classiﬁcation we have formulated already
existing methods as special cases of the proposed framework as well as proposing novel methods
for designing adversarial perturbations under various desirable constraints. This includes in
particular single-pixel and single-subset attacks. The framework is additionally used to demon-
strate adversarial vulnerability of regression algorithms by generating adversarial perturbations.
We numerically evaluate the applicability of this framework ﬁrst by benchmarking the newly
introduced algorithms for classiﬁcation through empirical simulations of the fooling ratio bench-
marked against the well-known FGSM, DeepFool, and PGD methods. Through experiments we
have shown the existence of adversarial examples in regression for the case of autoencoders and
image colorization tasks.

18

References

[AC18]

[ACW18]

[AM18]

[BBM18]

Anish Athalye and Nicholas Carlini. On the Robustness of the CVPR 2018 White-
Box Adversarial Example Defenses. arXiv:1804.03286 [cs, stat], April 2018. arXiv:
1804.03286.

Anish Athalye, Nicholas Carlini, and David Wagner. Obfuscated Gradients Give
a False Sense of Security: Circumventing Defenses to Adversarial Examples. In
International Conference on Machine Learning, 2018.

N. Akhtar and A. Mian. Threat of Adversarial Attacks on Deep Learning in
Computer Vision: A Survey. IEEE Access, 6:14410–14430, 2018.

Emilio Rafael Balda, Arash Behboodi, and Rudolf Mathar. On generation of
adversarial examples using convex programming. In 52-th Asilomar Conference
on Signals, Systems, and Computers, pages 1–6, Paciﬁc Grove, California, USA,
October 2018.

[BF17]

S. Baluja and I. Fischer. Adversarial Transformation Networks: Learning to
Generate Adversarial Examples. arXiv e-prints, March 2017.

[BMRG17]

Federico Baldassarre, Diego González Morín, and Lucas Rodés-Guirao. Deep
koalarization: Image colorization using cnns and inception-resnet-v2. arXiv preprint
arXiv:1712.03400, 2017.

[BNJT10]

Marco Barreno, Blaine Nelson, Anthony D. Joseph, and J. D. Tygar. The security
of machine learning. Machine Learning, 81(2):121–148, November 2010.

[CANK17] Moustapha Cisse, Yossi Adi, Natalia Neverova, and Joseph Keshet. Houdini:
Fooling deep structured prediction models. arXiv preprint arXiv:1707.05373, 2017.

[CW17]

Nicholas Carlini and David Wagner. Towards evaluating the robustness of neural
networks. In Security and Privacy (SP), 2017 IEEE Symposium on, pages 39–57.
IEEE, 2017.

[FFF15]

Alhussein Fawzi, Omar Fawzi, and Pascal Frossard. Fundamental limits on
adversarial robustness. Proceedings of ICML, Workshop on Deep Learning, 2015.

[FMDF16]

[FMDF17]

[FR13]

[GSS14]

[GW95]

Alhussein Fawzi, Seyed-Mohsen Moosavi-Dezfooli, and Pascal Frossard. Robustness
of classiﬁers: from adversarial to random noise. In Advances in Neural Information
Processing Systems 29, pages 1632–1640. 2016.

A. Fawzi, S. M. Moosavi-Dezfooli, and P. Frossard. The Robustness of Deep
Networks: A Geometrical Perspective. IEEE Signal Processing Magazine, 34(6):50–
62, November 2017.

Simon Foucart and Holger Rauhut. A Mathematical Introduction to Compressive
Sensing. Applied and Numerical Harmonic Analysis. Springer New York, New
York, NY, 2013.

Ian J. Goodfellow, Jonathon Shlens, and Christian Szegedy. Explaining and
Harnessing Adversarial Examples. In International Conference on Learning Repre-
sentations, December 2014.

Michel X Goemans and David P Williamson. Improved approximation algorithms
for maximum cut and satisﬁability problems using semideﬁnite programming.
Journal of the ACM (JACM), 42(6):1115–1145, 1995.

19

[HA17]

[HDY+12]

[HH15]

[HJ13]

Matthias Hein and Maksym Andriushchenko. Formal guarantees on the robustness
of a classiﬁer against adversarial manipulation. In NIPS, 2017.

G. Hinton, L. Deng, D. Yu, G. E. Dahl, A. r Mohamed, N. Jaitly, A. Senior,
V. Vanhoucke, P. Nguyen, T. N. Sainath, and B. Kingsbury. Deep Neural Networks
for Acoustic Modeling in Speech Recognition: The Shared Views of Four Research
Groups. IEEE Signal Processing Magazine, 29(6):82–97, November 2012.

David Hartman and Milan Hladík. Tight Bounds on the Radius of Nonsingularity.
In Scientiﬁc Computing, Computer Arithmetic, and Validated Numerics, Lecture
Notes in Computer Science, pages 109–115. Springer, Cham, September 2015.

Roger A. Horn and Charles R. Johnson. Matrix analysis. Cambridge Univ. Press,
Cambridge, 2. ed edition, 2013.

[HLWvdM17] Gao Huang, Zhuang Liu, Kilian Q Weinberger, and Laurens van der Maaten.
Densely connected convolutional networks. In Proceedings of the IEEE conference
on computer vision and pattern recognition, volume 1, page 3, 2017.

[HZRS16]

K. He, X. Zhang, S. Ren, and J. Sun. Deep Residual Learning for Image Recognition.
In 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR),
pages 770–778, June 2016.

[KFS18]

[KGB16]

[KH09]

[KSH12]

[LCB10]

[LCY13]

[LHBB99]

[LHL+17]

J. Kos, I. Fischer, and D. Song. Adversarial Examples for Generative Models. In
2018 IEEE Security and Privacy Workshops (SPW), pages 36–42, May 2018.

Alexey Kurakin, Ian Goodfellow, and Samy Bengio. Adversarial examples in the
physical world. arXiv preprint arXiv:1607.02533, 2016.

Alex Krizhevsky and Geoﬀrey Hinton. Learning multiple layers of features from
tiny images. 2009.

Alex Krizhevsky, Ilya Sutskever, and Geoﬀrey E Hinton. Imagenet classiﬁcation
In Advances in neural information
with deep convolutional neural networks.
processing systems, pages 1097–1105, 2012.

Yann LeCun, Corinna Cortes, and CJ Burges. Mnist handwritten digit database.
AT&T Labs [Online]. Available: http://yann. lecun. com/exdb/mnist, 2, 2010.

Min Lin, Qiang Chen, and Shuicheng Yan. Network in network. arXiv preprint
arXiv:1312.4400, 2013.

Yann LeCun, Patrick Haﬀner, Léon Bottou, and Yoshua Bengio. Object recognition
with gradient-based learning. In Shape, contour and grouping in computer vision,
pages 319–345. Springer, 1999.

Yen-Chen Lin, Zhang-Wei Hong, Yuan-Hong Liao, Meng-Li Shih, Ming-Yu Liu,
and Min Sun. Tactics of Adversarial Attack on Deep Reinforcement Learning
Agents. In Proceedings of the 26th International Joint Conference on Artiﬁcial
Intelligence, IJCAI’17, pages 3756–3762, Melbourne, Australia, 2017. AAAI Press.

[MDFF16]

Seyed Mohsen Moosavi Dezfooli, Alhussein Fawzi, and Pascal Frossard. Deepfool:
a simple and accurate method to fool deep neural networks. In Proceedings of 2016
IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2016.

[MDFFF17] Seyed-Mohsen Moosavi-Dezfooli, Alhussein Fawzi, Omar Fawzi, and Pascal
Frossard. Universal adversarial perturbations. arXiv preprint, 2017.

20

[MKBF17]

[MMS+18]

[PMJ+16]

Jan Hendrik Metzen, Mummadi Chaithanya Kumar, Thomas Brox, and Volker
Fischer. Universal adversarial perturbations against semantic image segmentation.
stat, 1050:19, 2017.

Aleksander Madry, Aleksandar Makelov, Ludwig Schmidt, Dimitris Tsipras, and
Adrian Vladu. Towards Deep Learning Models Resistant to Adversarial Attacks.
In International Conference on Learning Representations, 2018.

Nicolas Papernot, Patrick McDaniel, Somesh Jha, Matt Fredrikson, Z Berkay
Celik, and Ananthram Swami. The limitations of deep learning in adversarial
settings. In Security and Privacy (EuroS&P), 2016 IEEE European Symposium
on, pages 372–387. IEEE, 2016.

[PMSH16]

Nicolas Papernot, Patrick McDaniel, Ananthram Swami, and Richard Harang.
Crafting adversarial input sequences for recurrent neural networks. In Military
Communications Conference, MILCOM 2016-2016 IEEE, pages 49–54. IEEE, 2016.

[PMW+16] Nicolas Papernot, Patrick McDaniel, Xi Wu, Somesh Jha, and Ananthram Swami.
Distillation as a defense to adversarial perturbations against deep neural networks.
In Security and Privacy (SP), 2016 IEEE Symposium on, pages 582–597. IEEE,
2016.

[RHGS17]

S. Ren, K. He, R. Girshick, and J. Sun. Faster R-CNN: Towards Real-Time Object
Detection with Region Proposal Networks. IEEE Transactions on Pattern Analysis
and Machine Intelligence, 39(6):1137–1149, June 2017.

[Roh00]

[RRN12]

[RSL18]

[SBMC17]

[SLJ+15]

[SVK17]

[SZS+14]

[TG16]

Jiří Rohn. Computing the norm (cid:107)A(cid:107)∞,1 is NP-hard. Linear and Multilinear
Algebra, 47(3):195–204, May 2000.

Nikhil Rao, Ben Recht, and Robert Nowak. Universal Measurement Bounds for
Structured Sparse Signal Recovery. In Artiﬁcial Intelligence and Statistics, pages
942–950, March 2012.

Aditi Raghunathan, Jacob Steinhardt, and Percy Liang. Certiﬁed Defenses against
Adversarial Examples. In International Conference on Learning Representations,
2018.

Sayantan Sarkar, Ankan Bansal, Upal Mahbub, and Rama Chellappa. Upset and an-
gri: Breaking high performance image classiﬁers. arXiv preprint arXiv:1707.01159,
2017.

Christian Szegedy, Wei Liu, Yangqing Jia, Pierre Sermanet, Scott Reed, Dragomir
Anguelov, Dumitru Erhan, Vincent Vanhoucke, and Andrew Rabinovich. Going
deeper with convolutions. In Proceedings of the IEEE Conference on Computer
Vision and Pattern Recognition, pages 1–9, 2015.

Jiawei Su, Danilo Vasconcellos Vargas, and Sakurai Kouichi. One pixel attack for
fooling deep neural networks. arXiv preprint arXiv:1710.08864, 2017.

Christian Szegedy, Wojciech Zaremba, Ilya Sutskever, Joan Bruna, Dumitru
Erhan, Ian Goodfellow, and Rob Fergus. Intriguing properties of neural networks.
International Conference on Learning Representations, 2014.

Thomas Tanay and Lewis Griﬃn. A Boundary Tilting Persepective on the Phe-
nomenon of Adversarial Examples. arXiv:1608.07690 [cs, stat], August 2016. arXiv:
1608.07690.

21

[TKP+18]

[TSE+18]

[TTV16]

[WGQ17]

[XWZ+17]

Florian Tramèr, Alexey Kurakin, Nicolas Papernot, Ian Goodfellow, Dan Boneh,
and Patrick McDaniel. Ensemble Adversarial Training: Attacks and Defenses. In
International Conference on Learning Representations, 2018.

Dimitris Tsipras, Shibani Santurkar, Logan Engstrom, Alexander Turner, and
Aleksander Madry. Robustness May Be at Odds with Accuracy. arXiv:1805.12152
[cs, stat], May 2018. arXiv: 1805.12152.

Pedro Tabacof, Julia Tavares, and Eduardo Valle. Adversarial images for variational
autoencoders. arXiv preprint arXiv:1612.00155, 2016.

Beilun Wang, Ji Gao, and Yanjun Qi. A Theoretical Framework for Robustness of
(Deep) Classiﬁers against Adversarial Examples. In International Conference on
Learning Representations, 2017.

Cihang Xie, Jianyu Wang, Zhishuai Zhang, Yuyin Zhou, Lingxi Xie, and Alan
Yuille. Adversarial examples for semantic segmentation and object detection. In
International Conference on Computer Vision. IEEE, 2017.

22

