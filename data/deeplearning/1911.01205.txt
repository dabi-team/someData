9
1
0
2

v
o
N
4

]

G
L
.
s
c
[

1
v
5
0
2
1
0
.
1
1
9
1
:
v
i
X
r
a

Learning to Fix Build Errors with Graph2Diff Neural Networks

Daniel Tarlow
Google

Subhodeep Moitra
Google

Zimin Chen∗
KTH Royal Institute of Technology

Pierre-Antoine Manzagol
Google

Andrew Rice
University of Cambridge
& Google

Charles Sutton
Google

Edward Aftandilian
Google

ABSTRACT
Professional software developers spend a significant amount of
time fixing builds, but this has received little attention as a prob-
lem in automatic program repair. We present a new deep learning
architecture, called Graph2Diff, for automatically localizing and
fixing build errors. We represent source code, build configuration
files, and compiler diagnostic messages as a graph, and then use a
Graph Neural Network model to predict a diff. A diff specifies how
to modify the code’s abstract syntax tree, represented in the neural
network as a sequence of tokens and of pointers to code locations.
Our network is an instance of a more general abstraction which we
call Graph2Tocopo, which is potentially useful in any development
tool for predicting source code changes. We evaluate the model on
a dataset of over 500k real build errors and their resolutions from
professional developers. Compared to the approach of DeepDelta
[23], our approach tackles the harder task of predicting a more
precise diff but still achieves over double the accuracy.

ACM Reference Format:
Daniel Tarlow, Subhodeep Moitra, Andrew Rice, Zimin Chen, Pierre-Antoine
Manzagol, Charles Sutton, and Edward Aftandilian. 2019. Learning to Fix
Build Errors with Graph2Diff Neural Networks. In Preprint. ACM, New York,
NY, USA, 15 pages.

1 INTRODUCTION
Professional software developers spend a significant amount of
time fixing builds; for example, one large-scale study found that
developers build their code 7–10 times per day [31], with a signif-
icant number of builds being unsuccessful. Build errors include
simple errors such as syntax errors, but for professional developers
these are a small minority of errors; instead, the majority are link-
ing errors such as unresolved symbols, type errors, and incorrect
build dependencies [31]. A recent paper by Google reports roughly
10 developer-months of effort are spent every month fixing small

∗Work done during internship at Google.

Permission to make digital or hard copies of all or part of this work for personal or
Unpublished working draft.
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation
on the first page. Copyrights for components of this work owned by others than ACM
must be honored. Abstracting with credit is permitted. To copy otherwise, or republish,
to post on servers or to redistribute to lists, requires prior specific permission and/or a
fee. Request permissions from permissions@acm.org.
Preprint, Aug 2019, not for distribution.
© 2019 Association for Computing Machinery.
ACM ISBN 0000. . . $15.00

2019-11-05 02:46. Page 1 of 1–15.

build errors [23]. Therefore, automatically repairing build errors
is a research problem that has potential to ease a frequent pain
point in the developer workflow. Happily, there are good reasons
to think that automatic build repair is feasible: fixes are often short
(see Section 3), and we can test a proposed fix before showing it to
a developer simply by rebuilding the project. Build repair is thus a
potential “sweet spot” for automatic program repair, hard enough
to require new research ideas, but still just within reach.

However, there is only a small literature on repairing build errors.
Previous work on syntax errors has been very successful generating
repairs resolving missing delimiters and parentheses [10, 15]. In
contrast, in our corpus of professional build errors (Table 1), fixes are
more subtle, often requiring detailed information about the project
APIs and dependencies. Recently, the DeepDelta system [23] aimed
to repair build errors by applying neural machine translation (NMT),
translating the text of the diagnostic message to a description of
the repair in a custom domain-specific language (DSL). Although
this work is very promising, the use of an off-the-shelf NMT system
severely limits the types of build errors that it can fix effectively.
To this end, we introduce a new deep learning architecture, called
Graph2Diff networks, specifically for the problem of predicting edits
to source code, as a replacement for the celebrated sequence-to-
sequence model used for machine translation. Graph2Diff networks
map a graph representation of the broken code to a diff 1 in a domain-
specific language that describes the repair. The diff can contain not
only tokens, but also pointers into the input graph (such as “in-
sert token HERE”) and copy instructions (i.e. “copy a token from
HERE in the input graph”). Thus, Graph2Diff networks combine, ex-
tend, and generalize a number of recent ideas from neural network
models for source code [3, 4, 22, 44].

Graph2Diff networks are based on three key architectural ideas
from deep learning: graph neural networks, pointer models, and
copy mechanisms. Each of these ideas addresses a key challenge in
modelling source code and in program repair. First, graph neural
networks [19, 29] can explicitly encode syntactic structure, semantic
information, and even information from program analysis in a form
that neural networks can understand, allowing the network to learn
to change one part of the code based on its relationship to another
part of the code. Second, pointer models [38] can generate locations
in the initial AST to be edited, which leads to a compact way of
generating changes to large files (as diffs). Much work on program

1We slightly abuse terminology here and use “diff” to mean a sequence of edit opera-
tions that can be applied to the broken AST to obtain the fixed AST.

 
 
 
 
 
 
Preprint, Aug 2019, not for distribution.

Daniel Tarlow, Subhodeep Moitra, Andrew Rice, Zimin Chen, Pierre-Antoine Manzagol, Charles Sutton, and Edward Aftandilian

repair divides the problem into two separate steps of fault local-
ization and generating the repair; unfortunately, fault localization
is a difficult problem [21]. Using pointers, the machine learning
component can predict both where and how to fix. Finally, the copy
mechanism addresses the well-known out-of-vocabulary problem of
source code [5, 16, 17]: source code projects often include project-
specific identifiers that do not occur in the training set of a model. A
copy mechanism can learn to copy any needed identifiers from the
broken source code, even if the model has never encountered the
identifier in the training set. Essentially, the copy mechanism is a
way to encode into a neural network the insight from prior research
on program repair [6, 18] that often code contains the seeds of its
own repair. Copy mechanisms give the model a natural way to
generate these fixes. Graph2Diff networks are an instantiation of a
more general abstraction that we introduce, called Graph2Tocopo,
which encapsulates the key ideas of graphs, copy mechanisms, and
pointers into a simple conceptual framework that is agnostic to the
machine learning approach, i.e., not just deep learning.

Our contributions are:

(1) We study the challenges in repairing build errors seen in production
code drawn from our collected dataset of 500, 000 build repairs
(Sec. 3). These observations motivate the requirements for a build
repair tool: source code context is required, disjoint but correlated
changes are often required, and repairs do not always take place
at diagnostic locations.

(2) We introduce the Graph2Tocopo abstraction (Sec. 4) and Graph2Diff
network for predicting source code edits (Sec. 6). They are particu-
larly well-suited to code-to-edit problems and have desirable prop-
erties relative to Sequence-to-Sequence models and other Graph-
to-Sequence models. In this paper we show the value of this archi-
tecture for build repair, but in general this formulation is relevant
to other tasks which require predicting changes to code.

(3) Based on an extensive evaluation of our large historical data set of
build errors, we find that the Graph2Diff networks have remarkable
performance, achieving a precision of 61% at producing the exact
developer fix when suggesting fixes for 46% of the errors in our
data set. They also achieve over double the accuracy of the state-
of-the-art DeepDelta system. Finally, we show that in some cases
where the proposed fix does not match the developer’s fix, the
proposed fix is actually preferable.

Overall, our results suggest that incorporating the syntactic and
semantic structure of code has a significant benefit in conjunction
with deep learning. In future work, we hope that our work provides
a framework for enhancing deep learning with more sophisticated
semantic information from programs, ranging from types to pro-
gram analysis.

2 PROBLEM FORMULATION
Here we formulate the problem of resolving a set of build diag-
nostics. The input is the state of the source tree at the time of a
broken build and a list of diagnostics returned by the compiler.
The target output is a diff that can be applied to the code to re-
solve all of the diagnostics. For our purposes, a diff is a sequence
of transformations to apply to the original source code to generate
the repaired version. Compiler diagnostics do not always identify
the source of the fault that needs to be repaired, so we require the

models to predict the locations that need changing in addition to
the repairs. This combines the well-studied problems of automated
fault localization and automated program repair.

2.1 Input data format
We represent source code files as Abstract Syntax Trees (ASTs) in
order to capture the syntactic structure of the code (e.g., a state-
ment is within a block that is within a for loop within a method
declaration within a class, etc). Following DeepDelta [23], we also
parse build configuration files into a similar structure.

Build errors are represented as a set of compiler diagnostics.
A compiler diagnostic includes the kind of diagnostic (e.g., com-
piler.err.cant.resolve), the text associated with the diagnostic (e.g.,
“Cannot resolve symbol WidgetMaker”), and a location that is com-
posed of a filename and line number. We further assume that the
diagnostic text can be decomposed into a text template and a list of
arguments (e.g., template “Cannot resolve symbol” and arguments
list [“WidgetMaker”]).

2.2 Output data format
The target output is a sequence of edits that can be applied to the
ASTs of the initial “broken” code in order to resolve all the diag-
nostics (thus producing the “fixed” ASTs). In general, a resolution
may require changing multiple files, but in this paper we restrict
attention to fixes that only require changing a single file. To enable
the use of ASTs, we also discard broken code that cannot be parsed,
not counting these cases in the results for this paper.

We use the GumTree code differencing tool [14] to compute
the difference between the broken AST and the fixed AST. We
convert the tree differences into an edit script, which is a sequence
of insertion, deletion, move, and update operations that can be
applied to the broken AST to produce the fixed AST. There is a
question of how to represent edit scripts so that they can most easily
be generated by machine learning models. In Sec. 4 we present a
general abstraction for representing edit scripts, and in Sec. 5 we
present the specific instantiation that we use for fixing build errors.

2.3 Problem Statement
We can now state the problem that we focus on. Given a set of
build diagnostics, the AST of the file that needs to be changed, and
the associated BUILD file AST, generate the edit script that the
developer applied to resolve the diagnostics.

We view the problem as the second, core stage of a two-stage
prediction process. Stage 1 predicts the file that needs to be changed
from the build diagnostics. Stage 2 uses the result of Stage 1 to
construct the AST of the file that needs to be changed and an
AST of the associated BUILD file. The Stage 2 problem is then the
problem statement above. Because Stage 2 is the core challenge
that we are interested in, we use a heuristic for Stage 1 of choosing
to edit the file with the most diagnostics associated with it, and we
limit our experiments to examples where the Stage 1 prediction is
correct (∼90% in our data). In practice if one were deploying such a
system, the 10% of cases where the Stage 1 prediction is incorrect
should be treated as errors.

2019-11-05 02:46. Page 2 of 1–15.

Learning to Fix Build Errors with Graph2Diff Neural Networks

Preprint, Aug 2019, not for distribution.

3 DATASET
Before describing our approach, we describe the data in more detail,
as it motivates some of the modeling decisions. To collect data, we
take advantage of the highly-instrumented development process at
Google [26], extending previous work by [23].

3.1 Build Repair Dataset
We collected data by looking at one year of build logs and finding
successive build attempts where the first build produced compiler
diagnostics and the second resulted in no diagnostics. We then
retrieved the associated “broken” and “fixed” snapshots of the code.
These are examples where a developer repaired one or more build
errors. We limit attention to examples with edit scripts of 7 or
fewer edit operations, where the broken snapshot can be parsed
successfully, and that fix the build by changing the Java code of
the file with the most number of diagnostics (i.e., discard fixes
that only change build configuration files, command line flags, and
unexpected files). We do not restrict the kinds of diagnostics, the
vocabulary used in the edit scripts, or the sizes of input files. The
result is a dataset of ∼500k fixes. Figure 1 shows six examples
from the dataset (with identifiers renamed) and Figure 2 shows
quantitative statistics. We make the following observations:

Variable Misuse [3] errors occur. Row A shows one of 6% of
the cases where the fix replaces a single wrong usage of a variable.
Source-code context is required since the same diagnostic
kind has many different resolutions. Rows A-C are cant.resolve
diagnostics, and rows D-F are incompatible.types diagnostics.
Each requires a different replacement pattern. Figure 2(c) shows
the frequency of diagnostic kinds in the dataset. A small number of
diagnostic kinds dominate, but the graph has a heavy tail and there
are numerous resolution patterns per diagnostic, which means that
a learning-based solution (as opposed to attempting to build a hand
crafted tool for these diagnostics) seems a good option.

Edit scripts can be relatively long. Row E requires an edit
script of length 4; by comparison, a Variable Misuse bug such as
in Row A can be fixed with an edit script of length 1. Figure 2(a)
shows the overall distribution of edit script lengths.

Fixes do not always occur at the diagnostic location. Row
A shows an example where the identifier in the diagnostic is not the
one that needs changing. Rows C and F show examples where the
diagnostics indicate a different line to the one that needs changing.
36% of cases require changing a line not pointed to by a diagnostic.
There can be multiple diagnostics. Row C shows an example
where there are multiple diagnostics. Figure 2(b) shows the distri-
bution of diagnostic frequency per build. Approximately 25% of
failures had more than one diagnostic.

Single fixes can span multiple locations. In Row G, multiple
code locations need to be changed in order to fix the error. The
changes at the different locations are part of a single fix but require
different changes at the different locations. This shows that we
need a flexible model which does not assume that fixes at different
locations are independent as in DeepFix [15], or that multi-hunk
fixes apply the same change at different locations as in [28]. 21% of
the data requires editing more than one contiguous region.

2019-11-05 02:46. Page 3 of 1–15.

4 GRAPH2TOCOPO ABSTRACTION
Our aim is to develop a machine learning approach that can handle
all of the complexities described in the previous section. These chal-
lenges appear not only in fixing build errors, but also in many other
code editing tasks. To match the generality of the challenges, we
start by developing an abstraction for code editing called Graph2Tocopo.
Graph2Tocopo aims to formalize the interface between the machine
learning method and the code-editing problem — like program re-
pair, auto-completion, refactoring, etc. Graph2Tocopo is a single
abstraction that unifies ideas from many recent works in modelling
source code [3, 4, 8, 22, 35, 44]. Though we will take a deep learning
approach in Sec. 6, Graph2Tocopo is not specific to deep learning.
Graph2Tocopo aims to crystallize three key concepts that recur
across code editing tasks: representing code as graphs, representing
pointers to code elements, and copying names from code. Represen-
tation of code as a graph gives a convenient way of representing
code abstractly; of combining multiple sources of information, such
as code, error messages, documentation, historical revision infor-
mation, and so on; and for integrating statistical, syntactic, and
semantic structure by constructing edges. Graphs are a natural
choice because they are already a lingua franca for (non-learning
based) syntactic and semantic analysis of code. At a high level,
the goal of the Graph2Tocopo abstraction is to do for code-editing
tasks what the celebrated Seq2Seq abstraction [34] has done for
natural language processing (NLP): Graph2Tocopo aims to serve as
an interface between tool developers, that is, software engineering
researchers who create new development tools based on predict-
ing code edits, and model developers, machine learning researchers
developing new model architectures and training methods. Part of
our goal is to encourage modularity between these two research
areas, so advances one on side can immediately benefit the other.
When designing a tool for a particular code-editing task, we
envision that the tool developer will develop two formalisms, one
for the input and one for the output. On the input side, the tool
developer designs a graph to represent the program to be edited,
abstracting the code in a way that reveals the most important infor-
mation for the task (Sec. 4.1). On the output side, the tool developer
develops an edit-domain specific language (eDSL) that formalizes
the class of code edits that is necessary for the tool. Statements
in the eDSL are Tocopo sequences (Sec. 4.2), that is, sequences that
contain either tokens or two different types of location references,
called copy operations and pointers. Afterwards, we envision that
the tool developer could choose between many different learning
methods for Graph2Tocopo (such as Sec. 6), without going into the
details of how the models are implemented.

4.1 Code as Graphs
We represent code and related context as a directed multi-graph
with discrete labels on nodes and edges. For different tasks, the
graph can include different information such as abstract syntax
trees, error messages, results of program analyses, and edges re-
lating these components, e.g., a diagnostic line number refers to a
location in the code. Each node is specified by an integer index i
and is associated with a tuple of node features (ti , vi ), where ti is a
member of a finite set of node types T and vi is a string called the
node value. E.g., to represent ASTs, T can be the set of nonterminals

Preprint, Aug 2019, not for distribution.

Daniel Tarlow, Subhodeep Moitra, Andrew Rice, Zimin Chen, Pierre-Antoine Manzagol, Charles Sutton, and Edward Aftandilian

Row

Diagnostics

Fix

A

B

C

D

E

F

G

cannot find symbol ‘widgetSet()’

cannot find symbol ‘of’

(line 10) cannot find symbol ‘longName’
(line 15) cannot find symbol ‘longName’

incompatible types: ParamsBuilder cannot be
converted to Optional<Params>

incompatible types: RpcFuture<LongNameResponse>
cannot be converted to LongNameResponse

incompatible types: WidgetUnit cannot be
converted to Long

cannot find symbol ‘of(Widget,Widget)’

-
+

-
+

-
+

-
+

-
+

-
+

widgetsForX . widgetSet () . stream () . forEach (
widgetCounts . widgetSet () . stream () . forEach (

Framework . createWidget ( new FooModule . of ( this )). add ( this ) ;
Framework . createWidget ( FooModule . of ( this )). add ( this ) ;

String longname = " reallylongstringabcdefghijklmnopqrstuvw ..."
String longName = " reallylongstringabcdefghijklmnopqrstuvw ..."
x = f( longName )
// ...
y = g(a , b , c , longName ) // ( line 15)

Diagnostic pointed here

Diagnostic pointed here

// ( line 10)

return new ParamsBuilder ( args );
return new ParamsBuilder ( args ). build () ;

LongNameResponse produceLongNameResponseFromX (
ListenableFuture < LongNameResponse > produceLongNameResponseFromX (

public Widget setDefaultWidgetUnit ( WidgetUnit defaultUnit ) {
public Widget setDefaultWidgetUnit ( Long defaultUnit ) {

this . defaultUnit = defaultUnit ;

// Diagnostic pointed here

- import com . google . common . collect . ImmutableCollection ;
+ import com . google . common . collect . ImmutableSet ;
// ...
-
+

ImmutableCollection . of (
ImmutableSet . of (

Figure 1: Example diagnostics and fixes from our dataset.

s ::= s1, . . . , sM
s ::=

Tocopo sequence
Tocopo expression

TOKEN(t)
COPY(n)
INPUT_POINTER(n)
OUTPUT_POINTER(m)

Token expression
Copy expression
Input pointer expression
Output pointer expression

(a) Fix length

(b) # diagnostics

(c) Diagnostic freq.

Figure 2: Quantitative data statistics.

Figure 3: Syntax of Tocopo sequences. Here t denotes a to-
ken, and n and m integers.

used by Java, and node values vi could represent literals, keywords,
and identifiers. Edges are specified by a triple (i, j, e), which means
that there is an edge from i → j with type e.

4.2 Edits as Tocopo Sequences
The second challenge is how to represent a code edit. We propose
to use a sequence of code locations and tokens that we call a Tocopo
sequence. In more detail, a Tocopo sequence is a sequence of To-
copo expressions, where a Tocopo expression is one of (a) a token
expression of the form TOKEN(t). These represent literal tokens,
which could be commonly occurring identifier names, or editing
commands like INSERT and UPDATE, (b) a copy expression, which
refers to a value in the input graph (c) an input pointer expression,
which refers to a specific node in the input graph, and (d) an output
pointer expression which refers to a previous element in the Tocopo
sequence. This syntax is given in Figure 3.

We assume that a tool designer creates an eDSL to represent
edits to source code in a way that is appropriate for the task at
hand. The tool designer chooses a set of keywords for the edit DSL
and how they are combined with code locations and code tokens
to represent an edit. Just as a programming language is a subset of
the set of all sequences of tokens, an edit DSL is as a subset of the

set of all Tocopo sequences, that is, sequences of tokens, copy, and
pointer operations. These three types of expressions are useful for
constructing eDSLs for a variety of code editing tasks.

The concept of Tocopo sequence is extremely generic, and does
not say anything about what the edits do. What we can say about
Tocopo sequences at this general level is what the references mean,
that is, the pointers and the copy operations. Given a graph G, a to-
ken expression TOKEN(t) can be interpreted simply as referring to t .
A copy expression COPY(n) refers to the value vn of node n. An input
pointer expression INPUT_POINTER(n) refers to node index n in G.
Finally, given a Tocopo sequence s1 . . . sM , OUTPUT_POINTER(j) for
j < M refers to sj . This allows us to define two Tocopo sequences
s1 . . . sM and s ′
M to be referentially equivalent, which we write
s ⇔G s′, if for all i ≤ M, the two expression si and s ′
i refer to the
same entity. In practice the equivalence arises when token t referred
to in one expression is equivalent to the node value referred to by
a copy operation in the other expression. A key constraint on the
design of eDSLs, which we will leverage in the learning algorithm
(Sec. 4) is that if s′ ⇔G s, then s and s′ specify the same edit to G.
Remarks. First, it may be unclear why we need output pointers.
These are useful for adding new larger subtrees to code, because
using output pointers we can add multiple new nodes to the AST

1 . . . s ′

2019-11-05 02:46. Page 4 of 1–15.

Learning to Fix Build Errors with Graph2Diff Neural Networks

Preprint, Aug 2019, not for distribution.

with a desired relationship among them. Second, the distinction
between pointers and copy operations is subtle, but important.
Sometimes, it is important to specify an exact code location, such as
when specifying where a new AST node should be inserted. This is a
pointer. Other times, it is useful to refer to the value of an input node,
such as when inserting a usage of a previously-defined variable,
but any other location with the same node value will do just as well.
This is a copy. Essentially, this is the code-editing version of the
classic distinction between pass-by-value and pass-by-reference in
programming languages.

4.3 Program Repair as Graph2Tocopo
To be more concrete, we give examples of several program repair
methods from the literature, showing how they can be represented
as Graph2Tocopo problems. (An example of a build repair specified
as a Tocopo sequence is shown in Figure 4.) First, Allamanis et al.
[3] propose learning to repair variable misuse errors, such as the
one in Figure 1A, using a graph neural network. They proposed
using a graph called an augmented AST, that is, the AST of the
program with additional edges to represent adjacent tokens, data
flow, and control flow. Additionally, in each program, there is one
identifier expression for which we know a previously-declared
variable should be used, but we do not know which one. This
expression is represented by a special node in the graph of type
HOLE, which has child nodes of type CANDIDATE that represent
all of the variables in scope. The goal is to predict which of the
candidate nodes is the correct variable. This task can represented
as a Graph2Tocopo problem in a very simple way. The input graph
is simply the augmented AST, and the output Tocopo sequence has
the form INPUT_POINTER(j), where j is the node ID of one of the
candidate nodes in the graph.

Vasic et al. [35] suggest a method for jointly localizing and repair-
ing variable misuse errors, using two pointers: the first is a reference
to the variable usage that is an error (or to a special sequence posi-
tion that indicates no error), and the second pointer is a reference to
another variable that should be used in place of the first. This can be
represented as a Tocopo sequence of INPUT_POINTER(i) COPY(j),
where i is the node id of the incorrect variable usage, and j is the
node id of any usage of the correct replacement. Note the difference
between pointers and copy operations. A pointer is necessary for
the first usage, while a copy provides more flexibility for the second.

4.4 Learning for Graph2Tocopo
Combining the graph-structured input with the Tocopo-structured
output results in the Graph2Tocopo abstraction. A training set
is collected of code snapshots represented as graphs, and target
edits represented as Tocopo sequences in the eDSL in question. We
can then treat learning as a supervised learning problem to map
graphs to Tocopo sequences. A variety of learning algorithms can
be applied to this task. Many learning methods, especially in deep
learning, can define a probability distribution p(s|G, w) over Tocopo
sequences s given graphs G and learnable weights w. (For now, we
treat this distribution as a black box; see Sec. 6 for how it is defined in
the build repair application.) Then, given a training set D = {(G, s)}
of graphs and target Tocopo sequences, the learning algorithm

2019-11-05 02:46. Page 5 of 1–15.

chooses weights to maximize the probability of the data, that is, to
maximize the objective function L(w) = (cid:205)

(G,s)∈D log p(s|G, w).

For Graph2Tocopo sequences, though, we can do better than
this standard approach, in a way that eases the burden on the tool
designer to choose which of potentially many reference-equivalent
sequences should be provided as target output. Consider a single
example (G, s) from the training set. The tool designer should not
worry about which equivalent sequence is desired when they all
correspond to the same edit. Thus, we recommend training the
model to maximize the probability assigned to the set of expressions
equivalent to s. That is, let IG (s) = {s′ | s′ ⇔G s} be the set of
equivalent expressions, and train using the objective function

L(w) = (cid:213)
(G,s)∈D

log

(cid:213)

p(s ′ | G, w).

(1)

s ′ ∈IG (s )

This rewards a model for producing any Tocopo sequence that is
reference-equivalent to the provided target sequence, i.e., the model
is free to use the copy mechanism as it sees fit. It might seem that
computing the objective function (1) is computationally intractable,
as it may involve a sum over many sequences. However, it can often
be computed efficiently, and Graph2Diff models are constrained so
that it becomes inexpensive to compute.

5 BUILD REPAIR AS GRAPH2TOCOPO
Now we cast build repair as a Graph2Tocopo problem.

5.1 Input Graph Representation
The input graph is composed of several subgraphs:

Code Subgraph. We roughly follow [3] to represent source code
as a graph, creating nodes for each node in the AST. For identifiers
and literals, the node value is the string representation that appears
in the source code text. For internal nodes in the AST, the node
value is a string rendering of the node kind. The node type is the
kind of AST node as determined by the Java compiler.

Diagnostic Subgraphs. There is one diagnostic subgraph for each
compiler diagnostic. Nodes in this subgraph come from four sources.
First, there is a node representing the diagnostic kind as reported by
the compiler, for example, compiler.err.cant.resolve. Second,
the text of the diagnostic message is tokenized into a sequence of
tokens, each of which is added as a node in the graph. Third, there
is one node for each diagnostic argument (see Sec. 2.1) from the
parsed diagnostic message. Finally, there is a diagnostic root node.
The subgraph has a backbone tree structure where the root node
is a parent of each other listed node, and the nodes are ordered as
above. For purposes of creating edges, we treat this tree as an AST.
BUILD File Subgraph. BUILD file are usually an XML-style doc-
ument (e.g., BUILD file in Bazel, POM file in Maven, build.xml in
Ant), which we encode as a tree.

The subgraphs are connected by several types of edges, and
we are planning to add more edge types. An ablation study that
removes all edges (Supplementary Materials) in the input graphs
shows the importance of these edges. Currently, we have:

• AST child: Connects all parents to their children.
• Next node: Connects to the next node in a depth-first traversal.

Preprint, Aug 2019, not for distribution.

Daniel Tarlow, Subhodeep Moitra, Andrew Rice, Zimin Chen, Pierre-Antoine Manzagol, Charles Sutton, and Edward Aftandilian

• Next lexical use: Connects nodes with a given value in a chain
structure, with neighbors in the chain being nearest uses when
nodes are ordered by the depth-first traversal.

• Diagnostic location: Connects each diagnostic root node to all
nodes in the code subgraph on the line where the error occurred.
• Diagnostic argument: Connects diagnostic argument nodes to
the corresponding nodes in the code subgraph where its string
value is equivalent to the diagnostic argument.

5.2 Output eDSL Design
Here we describe the eDSL that we use for representing repairs
to build errors. A program in our eDSL, which we call an edit
script, specifies how to transform the broken AST into the fixed
AST. Our goals in designing the eDSL are (a) given two ASTs for
a broken file and a manually repaired file (which is what we have
in our data), it is easy to generate a corresponding edit script, and
(b) edit scripts should fully specify the change to the AST.2 An
edit script is a sequence of edit operations. Each edit operation
specifies one change to be made to the broken AST. As shorthand,
we write TOKEN(t) as t and use POINTER to mean that either an
INPUT_POINTER or OUTPUT_POINTER is valid:

• INSERT POINTER(<parentId>) POINTER(<siblingId>) <type>
<value>. Inserts a new node of node type type and value value
into the AST as a child of the node specified by parentId. It is
inserted into the children list after the referenced previousSibling
node. If the new node should be the first sibling, then the special
FIRST_CHILD token is used in place of POINTER(<siblingId>).

• DELETE INPUT_POINTER(<nodeId>) deletes a node.
• UPDATE INPUT_POINTER(<nodeId>) <value> sets the value of

the referenced node to value.

• MOVE INPUT_POINTER(<sourceId>) POINTER(<newParentId>)
POINTER(<newSiblingId>) moves the subtree rooted at source
so that it is a child of newParent, occuring just after newSibling.

• DONE indicates the end of the edit script.

For example, Figure 4 shows an edit script that implements the
fix from Figure 1E. Each operation in the edit script adds one node of
a three-node subtree in the AST that specifies a Java parameterized
type to be inserted, and then the old Java type is deleted. This uses
input pointers, output pointers, and values.

6 GRAPH2DIFF ARCHITECTURE
Finally we are able to describe our new deep learning architec-
ture for Graph2Tocopo problems. This architecture, which we call
Graph2Diff has two components. The first is a graph encoder that
converts the input graph into a N × H matrix called the node states,
where N is the number of nodes in the input graph, and H is the
dimension of the hidden states in the network. Each row of the
node states matrix is a vector, which we call a node representation,
that corresponds to one node in the input graph, and represents all
the information about that node that the model has learned might
be useful in predicting a fix. The second component of Graph2Diff
is an edit-script decoder that predicts an edit script one Tocopo ex-
pression at a time, taking the node states and previous predictions
as input. The decoder is based on modern deep learning ideas for
sequences, namely the celebrated Transformer architecture [36]

2As natural as requirement (b) sounds, it is not always respected in previous work.

which is used in models like GPT-2 [27] and BERT [13], but requires
custom modifications to handle the special Tocopo features of input
pointers, output pointers, and copy operations. Due to space con-
straints, we provide only a high-level description here. Full details
are in the Supplementary Materials.

6.1 Graph Encoder
Inspired by [3], we use a Gated Graph Neural Network (GGNN)
to encode the graph into a matrix of node states. At a high level,
a GGNN consists of a series of propagation steps. At each step, a
new representation of each node is computed by passing the repre-
sentations of the node’s neighbors at the last step through a neural
network. To initialize the node representations, we use a learnable
continuous embedding for node’s type and value, summing them
together with a positional embedding [36] based on the order in the
depth-first traversal. We run GGNN propagation for a fixed number
of steps (see Sec. 7.1.1 for details), resulting in a representation for
each node in the graph.

6.2 Edit-Script Decoder
The decoder is a neural network that predicts the edit script one
Tocopo expression at a time. If V is the size of the vocabulary, a
Tocopo expression is either one of V token expressions, one of N
input pointer expressions, or one of N copy expressions. So we can
treat predicting the next Tocopo expression as a classification prob-
lem with V + 2N outputs and predict this with a neural network.3
The inputs to the decoder are (a) the node representations from the
graph encoder, and (b) a representation of the partial edit script
generated so far. Our decoder then builds on the decoder from the
Transformer model, which is based on a type of neural network
called an attention operation. An attention operation is a network
that updates the representation of a target sequence of length N2,
represented as a N2 × H matrix, based on information from a source
sequence of length N1, represented as a N1 × H matrix. The atten-
tion operation produces an updated N2 × H matrix representing the
target. For mathematical details, see our Supplementary Material.
Our edit-script decoder extends the Transformer decoder to han-
dle the pointer and copy operations of Tocopo. Two main extensions
are needed. First, the partial edit script contains Tocopo expressions,
not just tokens as in Transformer, so we need a way of representing
Tocopo expressions as vectors that can be used within a deep net-
work. To do this, we start with an “output embedding” step, which
produces a T × H matrix of hidden states, where T is the length of
the partial edit script. Then several layers of attention operations
alternate between (i) exchanging information amongst the outputs,
via an attention operation where both the source and target are the
partial edit script (known as “causal self-attention”), and (ii) sending
information from the sequence of nodes in the input graph to the
output edit script, via an attention operation where the source is
the input graph and the target is the partial edit script (which we
call “output-input attention”). This results finally in a T × H matrix
representing the partial edit script, which is the input to an output
layer, which predicts the next expression.

3Our current implementation of the decoder handles output pointers in a simplified
way, predicting only where output pointers occur, but not predicting what they point
to. Therefore, we treat OUTPUT_POINTER as a vocabulary item in this discussion.

2019-11-05 02:46. Page 6 of 1–15.

Learning to Fix Build Errors with Graph2Diff Neural Networks

Preprint, Aug 2019, not for distribution.

-
+

LongNameResponse produceLongNameResponseFromX (
ListenableFuture < LongNameResponse > produceLongNameResponseFromX (

0: MethodDef
1:
Type
2:
3:
4:
5:

Args

Id

LongNameResponse

produceLongNameResponseFormX

0:INSERT 1:INPUT_POINTER(1) 2:INPUT_POINTER(2) 3:PARAMETERIZED_TYPE 4:TYPEAPPLY
5:INSERT 6:OUTPUT_POINTER(0) 7:FIRST_CHILD 8:IDENTIFIER 9:ListenableFuture
10:INSERT 11:OUTPUT_POINTER(0) 12:OUTPUT_POINTER(5) 13:IDENTIFIER 14:LongNameResponse
15:DELETE 16:INPUT_POINTER(2)
17:DONE

Figure 4: An example edit script that makes the change specified in Figure 1E. (Top) a textual diff of the change, (Left) A subset
of the original AST annotated with input ids. (Right) An edit script implementing the change, annotated with output ids.

This leads to the second extension. The output layer must pro-
duce a V + 2N sized output of token, copy, and pointer predictions
(whereas Transformer outputs just tokens). To do this, our output
layer makes three separate predictions, which we call “heads”. The
token head is an H × V matrix that maps the final hidden state to
a length V vector of token scores. The copy head and the pointer
heads are both attention operations with different parameters to
produce length N vectors of copy scores and pointer scores, re-
spectively, as in [38]. The three output vectors are concatenated
into the V + 2N outputs, and a softmax is used to turn this into a
distribution over predictions.

A final point is important but a bit technical. In order to be able
to efficiently train under the objective from (1), we require that
the representation of the Tocopo prefix provided to the decoder
is the same for all reference-equivalent prefixes; i.e., the network
should make the same future predictions regardless of whether
previous predictions used a token or an equivalent copy operation.
We impose this constraint by representing the partial edit script as
a list of sets of all Tocopo expressions that are reference-equivalent
to each expression in the partial edit script. These sets can be used
within our attention operations with only minor modifications. For
details, see the Supplementary Material.

7 EXPERIMENTS
We have several goals and research questions (RQs) for the empiri-
cal evaluation. First, we would like to evaluate our design choices
in Graph2Diff networks and better understand how they make use
of available information. We ask RQ1: How is Graph2Diff per-
formance affected by (a) the amount of code context in the
input graph? (b) the model architectural choices? and (c) the
amount of training data available? This question is important
because the performance of deep learning methods can often be
sensitive to model architectural choices. To understand how our
results fit in context with existing literature, we ask RQ2: How do
Graph2Diff networks compare to previous work on Seq2Seq
for fixing build errors? We compare to the most closely related
work, which is DeepDelta [23]. Even though DeepDelta can only be
evaluated on a less-stringent task than exact developer fix match,
we find that Graph2Diff networks achieve over double the accuracy,
which shows that Graph2Diff networks are far more accurate than
previous work. Turning attention to how the system would be used
in practice, we ask RQ3: How often do incorrect predictions
build successfully?, because fixes that fail to build can be filtered
out and not presented to developers. We find that 26% ± 13% of
the incorrect predictions build successfully, leading to an estimated

2019-11-05 02:46. Page 7 of 1–15.

precision of 61% at producing the exact developer fix when sug-
gesting fixes for 46% of the errors in our data set. Finally, we ask
the qualitative RQ4: What kinds of fixes does the model get
correct? What kinds of predictions are incorrect but build
successfully? There are some cases where the fix is semantically
incorrect and not desirable, but also cases where the predicted fix
is preferable over the one provided by the developer.

7.1 RQ1: Graph2Diff performance
7.1.1 Experimental details. We follow standard machine learning
experimental details, using train, validation and test splits and grid
search for choosing hyperparameters. Details appear in the Sup-
plementary Materials. Our main metric is sequence-level accuracy,
which is how often the model predicts the full sequence correctly.
This is a strict metric that only gives credit for exactly matching
the developer’s change. In the future we plan to show proposals to
developers and measure how often they find them useful.

7.1.2 Effect of context size and model depth. It is possible to reduce
the size of the input graphs by pruning nodes that are far away
from the source of an error. Reducing graph sizes increases training
throughput because less computation is needed for each example,
and it may be possible for the learning problem to become easier if
the removed nodes are irrelevant to the fix.

However, pruning nodes may also hurt performance of the model,
for three reasons. First, if the error produced by the compiler is not
near the source of the fault, then pruning can remove the location of
code that needs to be changed and make it impossible for the model
to correctly output the pointer elements in the output DSL. Second,
the fix may require generating tokens that are out of vocabulary but
present as the value of some input node. In this case, it is possible
to generate the correct fix by outputting a copy element. However,
if pruning removes all potential copy targets, then it will become
impossible for the model to generate the correct answer. Third,
there may be context in the distant nodes that are useful for the
machine learning model in a less clear-cut way.

Our first experiment explores this question. Results appear in
Figure 5 (left), showing that including more context and performing
more propagation steps helps performance.

7.1.3 Effect of dataset size. To measure the effect of amount of data,
we trained models using a random subsampling of the data. Figure 5
(right) shows the best resulting validation accuracy versus the
number of training examples. The x-axis is on a log-scale but clearly
shows that increasing data size leads to improved performance.

Preprint, Aug 2019, not for distribution.

Daniel Tarlow, Subhodeep Moitra, Andrew Rice, Zimin Chen, Pierre-Antoine Manzagol, Charles Sutton, and Edward Aftandilian

Prune
Distance

Avg. # Nodes

# Possible

Sequence-level Validation Accuracy
4 prop steps

8 prop steps

2 prop steps

1
2
4
8
12

25
41
192
1524
2385

125k
145k
240k
310k
315k

14.9%
16.3%
19.9%
23.8%
22.8%

15.5%
16.9%
22.4%
26.3%
25.6%

15.7%
16.7%
23.4%
28.0%
27.1%

Figure 5: (Left) Best sequence-level validation accuracy achieved for various degrees of graph pruning. As the Prune Distance
increases, more nodes are included in the graph, it becomes possible to get more training examples correct (the needed loca-
tions and vocabulary appear in the input graph), and accuracy generally increases. More propagation steps leads to improved
performance in most cases. (Right) Best validation accuracy vs training set size.

7.2 RQ2: Comparison to Sequence2Sequence
DeepDelta [23] casts the problem of fixing build errors in the Se-
quence2Sequence framework. Here we compare Graph2Diff to
DeepDelta across the two axes that they vary: Graph versus Se-
quence as input, and Diff versus Sequence as output.

(Imprec): the output format from DeepDelta; (2) ImpreciseWith-
Pointers (Imprec+P): ImpreciseDiff but representing locations
more precisely with pointers; (3) ImpreciseWithPointersAndSi-
blings (Imprec+PS): ImpreciseWithPointers but adding previous
sibling pointers; and (4) Diff: the Graph2Diff output DSL.

Input: Graph vs Sequence. The main difference in the input rep-
resentation is the amount of information provided to the models
about the source code surrounding the error. Within the context of
Graph2Diff models, we can test how this choice affects performance
while holding all else fixed. To use the same input information as
DeepDelta, we prune all nodes in the input graph except for the
nodes on a path from the location of a diagnostic to the root of the
AST. We leave the diagnostic subgraph the same as in Graph2Diff
models. The result is a family of graph models that have a sequence
input representation like that used in DeepDelta. We call these
models SeqGraph2X models, because they have sequential code in-
put representations but are implemented within the Graph2Tocopo
framework. A benefit of the Graph2Tocopo framework is that they
have a copy mechanism, unlike DeepDelta.

Output: Diff vs Sequence. Our diff output is more precise than
the sequence output of DeepDelta in three ways: (a) we refer to
locations by pointing to nodes in the input graph, which resolves
ambiguity when more than one input node has a given value (e.g.,
when changing a private modifier to public it becomes clear
which private to change); (b) we include a previous sibling pointer
to specify where we should insert into the list of children under
the specified node, which resolves ambiguity about, e.g., order in
argument lists; (c) we generate AST types of new nodes to insert
along with their value, which, e.g., resolves ambiguity between
generating method invocations and method references. The ex-
tra specificity in the diff output is important, because it provides
enough information to automatically apply a fix generated by the
model without additional heuristics or human judgement, which is
crucial towards putting the system into practice. Further, evaluating
correctness in terms of matching an imprecise target output gives
an overestimate of how the system will perform in practice.

The Graph2Tocopo framework makes it possible to run a se-
ries of experiments that gradually change the specificity of the
output DSL from our precise diff output to the imprecise output
from DeepDelta. We compare four output DSLs: (1) ImpreciseDiff

Graph2Diff vs DeepDelta. Finally, we compare to a more direct
reimplementation of DeepDelta, which uses the same sequential in-
put representation but uses the Google Neural Machine Translation
model [40] for the Seq2Seq learning. There is no pointer mecha-
nism in this model, so it is not possible to evaluate on the more
specific output DSLs and we compare just on ImpreciseDiff. We
equivalently refer to the DeepDelta method as Seq2ImpreciseDiff.

Experiment details and results. We used the same experimen-
tal protocol as in the previous section to train the cross-product
of options {Seq, SeqGraph, Graph} × {Diff, ImpreciseWithPointer-
sAndSiblings, ImpreciseWithPointers, ImpreciseDiff}. Accuracy is
measured based on whether the model predicted the full sequence
of its corresponding output correct (so generally predicting more
abstract outputs is expected to produce higher accuracy). We report
differences in absolute performance compared to the Graph2Diff
model on validation data. Results appear in Figure 6.

Comparing the first to the second row, we see that the Seq-
Graph2Tocopo formulation improves over the pure Seq2Seq formu-
lation, which we attribute primarily to the copy mechanism that
comes with the Tocopo-based output model. This is inline with
other recent work that shows a benefit of a copy mechanism in
program repair [11]. Comparing the second row to the third, the
graph-structured input improves performance regardless of the out-
put DSL, and the importance of the graph grows as the specificity of
the output DSL increases. Also, as expected, performance increases
as the output DSL becomes more abstract (but recall that we ex-
pect those other than Diff to overestimate real-world performance).
One other interesting comparison is Graph2ImpreciseWithPointers
versus Graph2ImpreciseDiff. These output DSLs are the same ex-
cept ImpreciseWithPointers represents locations with pointers and
ImpreciseDiff represents locations with the values of the nodes. By
using the copy mechanism, it would be possible in principle for the
ImpreciseDiff model to mimic the ImpreciseWithPointers model. We
suspect the difference in performance comes from the stronger su-
pervision in the ImpreciseWithPointers model—supervision about
locations points to exactly the region of the graph that needs to be

2019-11-05 02:46. Page 8 of 1–15.

Learning to Fix Build Errors with Graph2Diff Neural Networks

Preprint, Aug 2019, not for distribution.

Diff

Imprec+PS

Imprec+P

Seq2

SeqGraph2
Graph2

—
-20.3%
0.0%

—
-13.5%
+3.3%

—
-11.4%
+8.0%

Imprec
-12.9%
(DeepDelta)
-4.8%
+4.7%

Figure 6: Absolute sequence-level accuracy difference ver-
sus Graph2Diff model. Rows correspond to input representa-
tions and columns correspond to output DSLs with increas-
ingly less precision. Only the Diff column contains precise
information needed to apply the change unambiguously.

edited. In the ImpreciseDiff model, the supervision about locations
only narrows the location to a set of possible locations that could
be copied from. We also evaluated test accuracy for the DeepDelta
and Graph2Diff models that achieve best validation accuracy. Deep-
Delta test accuracy is 10% and Graph2Diff is 26%. In other words,
Graph2Diff has more than double the accuracy of DeepDelta, even
though Graph2Diff predicts the change more precisely.

7.3 RQ3: How often do incorrect fixes build?
When deploying the system in practice, we can increase precision
by filtering out suggestions that do not actually result in a successful
build. In this section we evaluate how effective this filtering step is.
As a first filtering step, we remove all proposed test set fixes from
the best Graph2Diff model that do not follow the grammar of our
DSL (5%). From the remaining incorrect examples, we sample 50
examples and attempt to build the predicted change. Of these, 13
(26%) build successfully. If we extrapolate these results, this means
that the model is able to make suggestions for 46% of the build
errors in our data set. Of these, 61% of the time, the fix exactly
matches the one that was eventually suggested by the developer.

7.4 RQ4: Where is the model successful & not?
7.4.1 Example Correct and Incorrect Fixes. To illustrate correct and
incorrect predictions, we choose to zoom in on a single diagnostic
kind (Incompatible types), because it gives a clearer sense of the
variety of fix patterns needed to resolve similar errors. It is also
interesting because it is not the most common diagnostic kind (it is
fifth), so this allows exploration of what the model has learned about
the long tail of potential repairs. Figure 7 shows examples where
the model predicted the full output sequence correctly (top) and
incorrectly (bottom). Interestingly, in many of these cases the fixes
seem to depend on details of the relevant APIs. For example, in the
first and fourth correct examples, it generates an extra method call,
presumably using the diagnostic message and surrounding context
to determine the right method to call. The second example replaces
the type in a declaration, which requires generating a rare token
via the copy mechanism. The third example is a relatively small
change in terms of text but takes 17 tokens in the output sequence
to generate (see Figure 4 (b)). The third example correctly converts
an integer literal to a long literal. At the bottom, the first example
illustrates that one limitation of the approach is understanding the
type signatures of rare methods (getWidget). The last example is
simply hard to predict without knowing more developer intent.

2019-11-05 02:46. Page 9 of 1–15.

7.4.2 Accuracy by Diagnostic Kind. Figure 8 reports accuracy by
the kind of the first diagnostic message. We show results for di-
agnostic kinds that appear at least 10 times in the validation data.
The model learns to fix many kinds of errors, although there is a
clear difference in the difficulty of different kinds. For example, the
model never correctly fixes a “missing return statement” error. We
suspect these fixes are difficult because they are closer to program
synthesis, where the model needs to generate a new line of code
that satisfies a variety of type constraints imposed by the context.

Incorrect Fixes that Build Successfully. Finally, we provide
7.4.3
three examples from the sampled predictions where the fix is not
equivalent to the ground truth, but still builds. The ground truth fix
is marked by // Ground truth and the Graph2Diff fix is marked
by // Graph2Diff fix. Listing 1 shows that Graph2Diff is able to
import from a different package with the same method name. In
fact, in this case, the package imported by the developer is depre-
cated and the model’s proposed fix is preferred. Listing 2 renamed
the method differently compared to the ground truth fix. It is one
example of suggesting new method name, which has been explored
by previous approach [2]. Listing 3 is one example where the pre-
dicted fix is semantically different to the ground truth fix, and it is
unlikely that the predicted fix is what the developer intended to do.
This is a example of false positive, and it is known as the overfitting
problem in the automated program repair community [32].

+ import static junit.framework.Assert.assertFalse; // Ground truth
+ import static org.junit.Assert.assertFalse; // Graph2Diff fix

Listing 1: Import different package

− public void original_method_name() throws Exception
+ public void ground_truth_method_name() throws Exception // Ground truth
+ public void predicted_method_name() throws Exception // Graph2Diff fix

Listing 2: Changed to a different method name

− if (id.isEmpty() || Long.parseLong(id).equals(0L))
+ if (id.isEmpty() || Long.valueOf(id).equals(0L)) // Ground truth
+ if (id.isEmpty() || Long.parseLong(id) != 0) // Graph2Diff fix

Listing 3: Semantically different bug fix

8 RELATED WORK

Graph Neural Networks to Sequences. There has been much re-
cent work on graph neural networks (GNNs) [41] but less work
on using them to generate sequential outputs. Li et al. [19] map
graphs to a sequence of outputs including tokens and pointers to
graph nodes. The main difference is in the decoder model, which we
improve by adding a copy mechanism, feeding back previous out-
puts (see Supplementary Materials for experiments demonstrating
improved performance), and training under weak supervision. Xu
et al. [42] present a Graph2Seq model for mapping from graphs to
sequences using an attention-based decoder. Beck et al. [7] develop
a graph-to-sequence model with GNNs and an attention-based de-
coder. In both cases, there is no copy or pointer mechanism. Song
et al. [33] develop a model for generating text from Abstract Mean-
ing Representation graphs, which maps from a graph structured
input to a sequence output that also has a copy mechanism but not
an equivalent of our pointer mechanism. Finally, there are also some
similarities between our model and generative models of graphs

Preprint, Aug 2019, not for distribution.

Daniel Tarlow, Subhodeep Moitra, Andrew Rice, Zimin Chen, Pierre-Antoine Manzagol, Charles Sutton, and Edward Aftandilian

Diagnostics

Fix

incompatible types: Builder cannot be converted to
WidgetGroup

incompatible types: RpcFuture<LongNameResponse>
cannot be converted to LongNameResponse

incompatible types: int cannot be converted to Long

incompatible types: ListenableFuture<FooResult>
cannot be converted to FooResult

incompatible types: GetWidgetResponse cannot be
converted to Widget

incompatible types: FooResponse cannot be
converted to Optional<BarResponse>

-
+

-
+

-
+

-
+

-
+

-
+

WidgetGroup widgetGroup = converter . getWidgetGroup () ;
WidgetGroup widgetGroup = converter . getWidgetGroup () . build () ;

LongNameResponse produceLongNameResponseFromX (
ListenableFuture < LongNameResponse > produceLongNameResponseFromX (

Long jobId = 1;
Long jobId = 1L;

FooResult x = client . sendFoo ( request , protocol );
FooResult x = client . sendFoo ( request , protocol ) . get () ;

return widget . start () . get () ;
return widget . start () . get () . getWidget () ;

return LONG_CONSTANT_NAME ;
return Optional . empty () ;

Figure 7: Incompatible type error validation examples predicted (Top) correctly and (Bottom) incorrectly.

Acc

86%
69%
45%
42%
33%
29%
29%
20%
17%
13%
12%
10%
9%
9%
9%
6%
3%
0%

First diagnostic kind
compiler.err.unreachable.stmt
compiler.err.cant.assign.val.to.final.var
compiler.err.unreported.exception.need.to.catch.or.throw
compiler.err.non-static.cant.be.ref
compiler.err.cant.resolve
compiler.misc.inconvertible.types
compiler.err.var.might.not.have.been.initialized
compiler.err.except.never.thrown.in.try
compiler.err.doesnt.exist
compiler.err.class.public.should.be.in.file
compiler.err.cant.apply.symbols
compiler.err.cant.apply.symbol
compiler.misc.incompatible.upper.lower.bounds
compiler.err.abstract.cant.be.instantiated
compiler.err.cant.deref
compiler.err.does.not.override.abstract
compiler.err.already.defined
compiler.err.missing.ret.stmt

Figure 8: Accuracy by kind of the first diagnostic for those
that appeared at least 10 times in validation data.

[9, 20, 43], in that these models map from a graph to a sequence
of decisions that can include selecting nodes (to determine edges),
though it does not appear that either subsumes the other.

Learning Program Repair. We refer the reader to [24] for a com-
prehensive review of program repair. We focus here on the most
similar methods. SequenceR [11] addresses the problem of program
repair based on failing test cases and uses an external fault localiza-
tion tool to propose buggy lines. A sequence-based neural network
with copy mechanism is used to predict a fixed line, using context
around the buggy line including the method containing the buggy
line and surrounding method signatures. The main differences are
that our approach can learn to edit anywhere in the input graph
and the use of a graph-structured input representation.

Allamanis et al. [3] introduce the Variable Misuse problem and
builds a GNN model to predict which variable should be used in a
given location. It does not directly address the problem of deciding
where to edit, instead relying on an external enumerative strategy.
Vasic et al. [35] uses a recurrent neural network with two output
pointers to learn to localize and repair Variable Misuse bugs. In our
context, the first pointer, which localizes the error, can be thought
of as a Tocopo pointer, and the second pointer, which points to

the variable that should replace the buggy one, can be thought of
as a copy operation in Tocopo. Similar to [19], the first predicted
pointer is not fed back into the prediction of the second pointer.

DeepFix is an early work that uses deep learning to fix compila-
tion errors [15]. They use a seq2seq neural network with attention
mechanism to repair a single code line, and multi-line errors can
be fixed by multiple passes. The input is the whole program, and
variables are renamed to reduce the vocabulary. Then, the model
takes the input and predicts the line number along with the bug
fix. TRACER [1] followed the same idea and improved upon Deep-
Fix. The most significant changes compared to DeepFix are: 1) The
fault localization and patch generation step are separated. TRACER
relies on the line number reported by the compiler to localize the
bug, while DeepFix outputs a line number and the corresponding
bug fix. 2) TRACER’s input to the model is much smaller; only the
lines surrounding the buggy line are used as input. We have taken
different design decisions compared to these two approaches. First,
we use a copy mechanism to solve the out-of-vocabulary problem
instead of renaming the variables, as we believe variable names
contain valuable information for understanding the source code.
Second, we take into account the whole program as well as the
diagnostic information. Third, we do not assume that multi-line
bugs are independent (e.g., row G in Figure 1) and fix them in multi-
ple passes. Instead, we use the pointer network to specify different
locations and generate all bug fixes simultaneously.

The approach by [23] to fix compilation errors is the closest
related work, though we focus on all kinds of errors rather than a
few common kinds. We compared experimentally and discussed
extensively in Sec. 7.2. Similarly, Getafix also only focuses on a
few kinds of errors [30]. They use a clustering algorithm to extract
common edit patterns from past bug fixes, and try to apply them on
new unseen programs. We have achieved similar results (Figure 8),
but on more error types. Our post filtering steps also allow us to
obtain a higher precision rate.

9 DISCUSSION
We have presented an end-to-end neural network-based approach
for localizing and repairing build errors that more than doubles the
accuracy of previous work. Evaluation on a large dataset of errors
encountered by professional developers doing their day-to-day
work shows that the model learns to fix a wide variety of errors.

2019-11-05 02:46. Page 10 of 1–15.

Learning to Fix Build Errors with Graph2Diff Neural Networks

Preprint, Aug 2019, not for distribution.

We hope that the Graph2Tocopo abstraction is particularly useful
for developing new tools to predict code changes. Graph2Tocopo
provides two ways for tool developers to incorporate their domain
expertise into machine learning models. First, the input graphs can
be expanded to include arbitrary information about the program,
including type information and the results of static and dynamic
analysis. Once these are added to the graph, the deep learning
method can learn automatically when and how this information is
statistically useful for predicting fixes. Second, the edit DSLs can be
augmented with higher-level actions that perform more complex
edits that are useful for a specific tool, such as inserting common
idioms, applying common big fixes, or even refactoring operations.
Having designed these, the tool developer gains access to state of the
art neural network approaches. The framework generalizes several
recent works [3, 11, 35], and it would be straightforward to express
them as Graph2Tocopo problems. We are also looking forward
to working with tool developers to develop new Graph2Tocopo
problems. We have already benefited from the generality of the
Graph2Tocopo abstraction when running the experiments with
different output DSLs in Sec. 7.2, where it was easy to use the same
abstraction for a variety of input and output design choices.

More broadly, we hope that fixing build errors is a stepping stone
to related code editing problems: there is a natural progression from
fixing build errors to other software maintenance tasks that require
generating larger code changes.

ACKNOWLEDGMENTS
We thank Petros Maniatis for several valuable discussions and com-
ments on earlier drafts.

REFERENCES
[1] Umair Z Ahmed, Pawan Kumar, Amey Karkare, Purushottam Kar, and Sumit
Gulwani. 2018. Compilation error repair: for the student programs, from the
student programs. In 2018 IEEE/ACM 40th International Conference on Software
Engineering: Software Engineering Education and Training (ICSE-SEET). IEEE,
78–87.

[2] Miltiadis Allamanis, Earl T Barr, Christian Bird, and Charles Sutton. 2015. Sug-
gesting accurate method and class names. In Proceedings of the 2015 10th Joint
Meeting on Foundations of Software Engineering. ACM, 38–49.

[3] Miltiadis Allamanis, Marc Brockschmidt, and Mahmoud Khademi. 2018. Learning

to represent programs with graphs. In ICLR.

[4] Miltiadis Allamanis, Hao Peng, and Charles Sutton. 2016. A convolutional at-
tention network for extreme summarization of source code. In International
Conference on Machine Learning. 2091–2100.

[5] Miltiadis Allamanis and Charles Sutton. 2013. Mining source code repositories
at massive scale using language modeling. In Proceedings of the 10th Working
Conference on Mining Software Repositories. IEEE Press, 207–216.

[6] Earl T Barr, Yuriy Brun, Premkumar Devanbu, Mark Harman, and Federica Sarro.
2014. The plastic surgery hypothesis. In Proceedings of the 22nd ACM SIGSOFT
International Symposium on Foundations of Software Engineering. ACM, 306–317.
[7] Daniel Beck, Gholamreza Haffari, and Trevor Cohn. 2018. Graph-to-sequence
learning using gated graph neural networks. arXiv preprint arXiv:1806.09835
(2018).

[8] Avishkar Bhoopchand, Tim Rocktäschel, Earl Barr, and Sebastian Riedel. 2016.
Learning python code suggestion with a sparse pointer network. arXiv preprint
arXiv:1611.08307 (2016).

[9] Marc Brockschmidt, Miltiadis Allamanis, Alexander L Gaunt, and Oleksandr Polo-
zov. 2018. Generative code modeling with graphs. arXiv preprint arXiv:1805.08490
(2018).

[10] Joshua Charles Campbell, Abram Hindle, and José Nelson Amaral. 2014. Syntax
errors just aren’t natural: improving error reporting with language models. In
Working Conference on Mining Software Repositories (MSR). 252–261.

[11] Zimin Chen, Steve Kommrusch, Michele Tufano, Louis-Noël Pouchet, Denys
Poshyvanyk, and Martin Monperrus. 2018. SequenceR: Sequence-to-Sequence
Learning for End-to-End Program Repair. arXiv preprint arXiv:1901.01808 (2018).

2019-11-05 02:46. Page 11 of 1–15.

[12] Kyunghyun Cho, Bart van Merrienboer, Caglar Gulcehre, Fethi Bougares, Holger
Schwenk, and Yoshua Bengio. 2014. Learning Phrase Representations using RNN
Encoder-Decoder for Statistical Machine Translation. In EMNLP.

[13] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. BERT:
Pre-training of Deep Bidirectional Transformers for Language Understanding. In
Conference of the North American Chapter of the Association for Computational
Linguistics. 4171–4186. https://www.aclweb.org/anthology/N19-1423

[14] Jean-Rémy Falleri, Floréal Morandat, Xavier Blanc, Matias Martinez, and Martin
Monperrus. 2014. Fine-grained and accurate source code differencing. In Pro-
ceedings of the 29th ACM/IEEE international conference on Automated software
engineering. ACM, 313–324.

[15] Rahul Gupta, Soham Pal, Aditya Kanade, and Shirish Shevade. 2017. Deepfix: Fix-
ing common C language errors by deep learning. In Thirty-First AAAI Conference
on Artificial Intelligence.

[16] Vincent J. Hellendoorn and Premkumar Devanbu. 2017. Are Deep Neural Net-
works the Best Choice for Modeling Source Code?. In Proceedings of the 2017 11th
Joint Meeting on Foundations of Software Engineering (ESEC/FSE 2017). ACM, New
York, NY, USA, 763–773. https://doi.org/10.1145/3106237.3106290

[17] Rafael-Michael Karampatsis and Charles Sutton. 2019. Maybe Deep Neural
Networks are the Best Choice for Modeling Source Code. CoRR abs/1903.05734
(2019). arXiv:1903.05734 http://arxiv.org/abs/1903.05734

[18] Claire Le Goues, ThanhVu Nguyen, Stephanie Forrest, and Westley Weimer. 2011.
Genprog: A generic method for automatic software repair. Ieee transactions on
software engineering 38, 1 (2011), 54–72.

[19] Yujia Li, Daniel Tarlow, Marc Brockschmidt, and Richard Zemel. 2016. Gated

graph sequence neural networks. In ICLR.

[20] Yujia Li, Oriol Vinyals, Chris Dyer, Razvan Pascanu, and Peter Battaglia. 2018.
Learning deep generative models of graphs. arXiv preprint arXiv:1803.03324
(2018).

[21] Kui Liu, Anil Koyuncu, Tegawendé F Bissyandé, Dongsun Kim, Jacques Klein,
and Yves Le Traon. 2019. You cannot fix what you cannot find! an investigation
of fault localization bias in benchmarking automated program repair systems. In
2019 12th IEEE Conference on Software Testing, Validation and Verification (ICST).
IEEE, 102–113.

[22] Chris Maddison and Daniel Tarlow. 2014. Structured generative models of natural

source code. In International Conference on Machine Learning.

[23] Ali Mesbah, Andrew Rice, Emily Johnston, Nick Glorioso, and Edward Aftandilian.
2019. DeepDelta: Learning To Repair Compilation Errors. In Proceedings of the
2019 27th ACM Joint Meeting on European Software Engineering Conference and
Symposium on the Foundations of Software Engineering (ESEC/FSE 2019). ACM,
New York, NY, USA.

[24] Martin Monperrus. 2018. The Living Review on Automated Program Repair. Tech-

nical Report hal-01956501. HAL/archives-ouvertes.fr.

[25] Niki Parmar, Ashish Vaswani, Jakob Uszkoreit, Łukasz Kaiser, Noam Shazeer,
Image transformer. arXiv preprint

Alexander Ku, and Dustin Tran. 2018.
arXiv:1802.05751 (2018).

[26] Rachel Potvin and Josh Levenberg. 2016. Why Google stores billions of lines of

code in a single repository. Commun. ACM 59, 7 (2016), 78–87.

[27] Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, and Ilya
Sutskever. 2019. Language Models are Unsupervised Multitask Learners. (2019).
[28] Seemanta Saha, Ripon K. Saha, and Mukul R. Prasad. 2019. Harnessing Evolution
for Multi-hunk Program Repair. In Proceedings of the 41st International Conference
on Software Engineering (ICSE ’19). IEEE Press, Piscataway, NJ, USA, 13–24. https:
//doi.org/10.1109/ICSE.2019.00020

[29] Franco Scarselli, Marco Gori, Ah Chung Tsoi, Markus Hagenbuchner, and Gabriele
Monfardini. 2009. The graph neural network model. IEEE Transactions on Neural
Networks (2009).

[30] Andrew Scott, Johannes Bader, and Satish Chandra. 2019. Getafix: Learning to

fix bugs automatically. arXiv preprint arXiv:1902.06111 (2019).

[31] Hyunmin Seo, Caitlin Sadowski, Sebastian Elbaum, Edward Aftandilian, and
Robert Bowdidge. 2014. Programmers’ build errors: A case study (at Google). In
International Conference on Software Engineering. ACM, 724–734.

[32] Edward K Smith, Earl T Barr, Claire Le Goues, and Yuriy Brun. 2015. Is the cure
worse than the disease? overfitting in automated program repair. In Proceedings
of the 2015 10th Joint Meeting on Foundations of Software Engineering. ACM,
532–543.

[33] Linfeng Song, Yue Zhang, Zhiguo Wang, and Daniel Gildea. 2018. A graph-to-
sequence model for AMR-to-text generation. arXiv preprint arXiv:1805.02473
(2018).

[34] Ilya Sutskever, Oriol Vinyals, and Quoc V Le. 2014. Sequence to sequence learning
with neural networks. In Advances in Neural Information Processing Systems.
[35] Marko Vasic, Aditya Kanade, Petros Maniatis, David Bieber, and Rishabh Singh.
2019. Neural Program Repair by Jointly Learning to Localize and Repair. arXiv
preprint arXiv:1904.01720 (2019).

[36] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones,
Aidan N Gomez, Łukasz Kaiser, and Illia Polosukhin. 2017. Attention is all
you need. In Advances in neural information processing systems. 5998–6008.

Preprint, Aug 2019, not for distribution.

Daniel Tarlow, Subhodeep Moitra, Andrew Rice, Zimin Chen, Pierre-Antoine Manzagol, Charles Sutton, and Edward Aftandilian

A.1 Background Ops
We begin by reviewing two operations that we use repeatedly, sparse
attention [36, 37] and graph propagation [19].

In a sparse attention op, we have M target entities, each attending
to a subset of N source entities. Each target entity has a hidden state
associated with it, and these hidden states are stacked into an M ×H
matrix denoted U , where H is the dimension of the hidden states.
Similarly, there is a N × H matrix of hidden states associated with
source entities, denoted V . Each sparse attention op has three dense
layers associated with it: the query layer fQ transforms U into an
M × H matrix of per-target queries; the key layer fK transforms V
into an N × H matrix of per-source keys; and the value layer fV
transforms V into an N ×H matrix of per-source values. Further, the
op requires an M × N sparse binary tensor S, where Sji = 1 if target
j is allowed to attend to source i and 0 otherwise. The attention
weight from target j to source i is αji = exp (cid:10)fQ (U i ), fK (V j )(cid:11), and
the result of sparse attention is a M × H matrix where row j is equal
to 1
i:Sji =1 αji . A special case of
Z
sparse attention is causal self-attention [36], where U and V are
both set to be the hidden states associated with output timesteps
(i.e., M=N =#output steps) and Sji = 1 if i ≤ j and 0 otherwise.

i:Sji =1 αji fV (Vi ) where Z = (cid:205)

(cid:205)

In a GGNN graph propagation op [19], we have a hidden state
for each of N nodes in a graph, stacked into a N × H matrix U .
There are E edge types, and each is associated with a sparse tensor
S(e) and an associated dense layer fe . S
= 1 if there is an edge
of type e from i to j and 0 otherwise. The first step is to send
messages across all of the edges. For each node j, the incoming
(cid:205)
messages are defined as mj = (cid:205)
fe (U i ). The second step
e
is to update U by applying a GRU operation [12] to each node to
get U ′
= GRU(Ui , mi ). The result of this update for each node is
i
stacked into a N × H resulting matrix U ′.

i:S (e )
ji

(e)
ji

=1

A.2 Accommodating Weak Supervision
As discussed in Sec. 4.4, we would like to train our models to maxi-
mize the log probability assigned to the set of Tocopo sequences that
are reference-equivalent to a given target s, which means summing
over all valid Tocopo sequences s ′ ∈ IG (s). Our decoder design is
motivated by the observation that some architectural choices allow
this summation to be computed efficiently.

Note that we can decompose IG (s) into a Cartesian product
of per-timestep sets. Overloading notation, let IG (s) be the set of
Tocopo expressions that evaluate to a target value s. Then IG (s) =
IG (s1) × . . . × IG (sM ) where × denotes Cartesian product. In other
words, the set of equivalent Tocopo expressions at time m does not
depend on which equivalent Tocopo expression is chosen at other
time steps. As shorthand, let Im = IG (sm ).

We can leverage the above Cartesian product structure to sim-
plify the training of our models with training objective of (1), but
we need to be careful about how the decoder is structured. Consider
defining a model for predicting Tocopo expression sm , and suppose
the model has already predicted prefix s1 ∈ I1, . . . , sm−1 ∈ Im−1.
If we define predictions in terms of p(sm | s1, . . . , sm−1, G), where
the predicted probability depends on the sequence of Tocopo expres-
sions (i.e., how the prefix was generated), then the summation in
(1) requires summing over all valid prefix sequences, which could

2019-11-05 02:46. Page 12 of 1–15.

Figure 9: Graph2Diff model.

[37] Petar Veličković, Guillem Cucurull, Arantxa Casanova, Adriana Romero, Pietro
Lio, and Yoshua Bengio. 2017. Graph attention networks. arXiv preprint
arXiv:1710.10903 (2017).

[38] Oriol Vinyals, Meire Fortunato, and Navdeep Jaitly. 2015. Pointer Networks. In
Advances in Neural Information Processing Systems 28, C. Cortes, N. D. Lawrence,
D. D. Lee, M. Sugiyama, and R. Garnett (Eds.). Curran Associates, Inc., 2692–2700.
http://papers.nips.cc/paper/5866-pointer-networks.pdf

[39] Oriol Vinyals, Łukasz Kaiser, Terry Koo, Slav Petrov, Ilya Sutskever, and Geoffrey
Hinton. 2015. Grammar as a foreign language. In Advances in neural information
processing systems. 2773–2781.

[40] Yonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V Le, Mohammad Norouzi,
Wolfgang Macherey, Maxim Krikun, Yuan Cao, Qin Gao, Klaus Macherey, et al.
2016. Google’s neural machine translation system: Bridging the gap between
human and machine translation. arXiv preprint arXiv:1609.08144 (2016).
[41] Zonghan Wu, Shirui Pan, Fengwen Chen, Guodong Long, Chengqi Zhang, and
Philip S Yu. 2019. A comprehensive survey on graph neural networks. arXiv
preprint arXiv:1901.00596 (2019).

[42] Kun Xu, Lingfei Wu, Zhiguo Wang, Yansong Feng, Michael Witbrock, and Vadim
Sheinin. 2018. Graph2seq: Graph to sequence learning with attention-based
neural networks. arXiv preprint arXiv:1804.00823 (2018).

[43] Jiaxuan You, Rex Ying, Xiang Ren, William L Hamilton, and Jure Leskovec. 2018.
GraphRNN: Generating realistic graphs with deep auto-regressive models. arXiv
preprint arXiv:1802.08773 (2018).

[44] Rui Zhao, David Bieber, Kevin Swersky, and Daniel Tarlow. 2019. Neural Networks
for Modeling Source Code Edits. arXiv preprint arXiv:1904.02818 (2019).

APPENDIX
TABLE OF CONTENTS
The appendix contains the following:

• A detailed description of the Graph2Diff neural network architec-

ture and training objective.

• Formal semantics of Tocopo sequences.
• Additional experimental details and results.

A DETAILED DESCRIPTION OF GRAPH2DIFF

ARCHITECTURE

In this appendix we describe the Graph2Diff architecture in more
detail. A diagram of the architecture appears in Figure 9.

12834567Diagnostic 1Diagnostic 2Java AST91011121314Graph Neural NetworkTarget Output SequenceToken Embeddings Input Node HiddensCopy Embeddings Pointer Embeddings Output Embeddings Output Hiddens Embedding LookupSparseAttentionVocab EmbeddingsEmbedding Lookup+Causal Self AttentionAttentionOutput Hiddens AttentionWeightsCopy LogitsAttentionWeightsPointer LogitsDenseVocab Logitspositional encodingN xInput GraphLearning to Fix Build Errors with Graph2Diff Neural Networks

Preprint, Aug 2019, not for distribution.

be exponentially expensive:

(cid:213)

log

s ∈I1×...×Im

p(sm | s1, . . . , sm−1, G).

(2)

However, suppose we define predictions in terms of p(sm |
I1, . . . , Im−1, G), where the predicted probability depends on the
sequence of equivalence sets. Then we can write
log p(Im | I1, . . . , Im−1, G) = log

p(sm | I1, . . . , Im−1, G).

(cid:213)

sm ∈Im

(3)

In going from Eq. 2 to Eq. 3, we lose the ability to assign different
probabilities to p(sm | s1, . . . , sm−1, G) and p(sm | s ′
m−1, G)
for two different prefixes s and s ′ that are reference-equivalent,
but it allows us to perform the marginalization needed for training
with weak supervision in linear rather than exponential time.

1, . . . , s ′

Our full training objective is thus
log p(IG (s) | G) = (cid:213)

(cid:213)

log

p(sm | I1, . . . , Im−1, G).

(4)

m

sm ∈Im

From the perspective of the decoder, this means that when feeding
back in predictions from previous time steps, we need to feed back
the sets I1, . . . , Im−1 and define a neural network model that takes
the sets as inputs to the decoder. This is accomplished via the Output
Embedding model described next.

We note that a simpler alternative that preserves efficiency is to
not feed back denotation prefixes at all, and instead define objective

log p(IG (s) | G) = (cid:213)

log

(cid:213)

p(sm | m, G).

(5)

m

sm ∈Im

In this case, the predictions at the different time steps are condi-
tionally independent of each other given the input graph. This is
a good choice when there is no uncertainty in the outputs given
the inputs, and it is a choice that has been made in previous related
work [19, 35]. However, it cannot represent certain important distri-
butions over output sequences (e.g., .5 probability to make change
A at position X and .5 probability to make change B at position Y).
We show in Sec. C.2.1 that this alternative leads to significantly
worse performance on our problem.

A.3 Tocopo Decoder
The decoder predicts the next element of the Tocopo sequence
given the input graph and the previous predicted Tocopo elements.

Output Embedding. When predicting the next token in the target
output sequence, we condition on the previously generated denota-
tions. This section describes how to embed them into hidden states
that can be processed by later layers of the decoder. There are three
stages of the output embedding.

First, we embed the token associated with each output. For token
outputs, this is a standard lookup table into a learnable vector
representation per output vocabulary element. Out of vocabulary
tokens share a vector. There is no token associated with an input
pointer, but we assume they have a special “POINTER” token at
this stage (i.e., all pointers share a “POINTER” embedding).

Second, we incorporate copy information. For each output token,
we track which input nodes could have been copied from to generate
the token. This information can be represented as a sparse indicator

2019-11-05 02:46. Page 13 of 1–15.

matrix with rows corresponding to output elements and columns
corresponding to input nodes, with a 1 entry indicating that the
output can be generated by copying from the node. We then perform
a sparse attention operation using embeddings from the first output
embedding stage as queries and the outputs of the graph encoder
as keys and values. The result of this “copy attention” is a weighted
average of the final node embeddings of nodes that could be copied
from to generate each token, passed through a learnable dense layer.
For empty rows of the sparse matrix, the result is the zero vector.
Finally, we incorporate pointer information. This follows similar
to the copy embeddings but is simpler because there can be at most
one pointer target per output sequence. In terms of the sparse matrix
mentioned above, each row has at most a single 1. Thus, we can
apply the analogous operation by selecting the node embedding for
the column with the 1 and passing it through a different learnable
dense layer. For empty rows of the sparse matrix, the result is the
zero vector.

The output of each stage is an embedding vector per timestep
(which may be the zero vector). The total output embeddings are
the sum of the embeddings from the three stages.

Output Propagation. This stage propagates information from the
input graph to the output sequence and between elements of the
output sequence. We repeatedly alternate steps of output-to-input
attention and output-to-output causal self-attention. The result
of each step of the decoder is an updated hidden state for each
output step, which is fed as input for the next decoder step. It is
initialized to the output of the embedding step above. The result of
the propagation stage is the final hidden state for each output step.
Output-to-input attention uses the current output hidden states
as keys for a dense attention operation. The keys and values are the
final input graph node hidden states. As in [36], keys, queries, and
values are passed through separate learnable dense layers before
performing the attention. The result of attention is a vector for
each output step. These are treated as messages and combined with
the previous output hidden states using a GRU cell as in GGNNs
[19]. The dense layers and GRU cell parameters are shared across
propagation steps. Note that this step allows the output to depend
on the entire input graph, even if the input graph has diameter
greater than the number of input propagation steps.

Output-to-output attention follows similarly to above but instead
of input node hidden states as keys and values, it uses the current
output hidden states and masks results so that it is impossible
for information about future outputs to influence predictions of
previous outputs (i.e., it is causal self-attention [36]). The output
hidden states are updated using a GRU cell as above. The dense
layers and GRU cell parameters are shared across propagation steps,
but there are separate parameter sets for output-to-input attention
and output-to-output attention.

Output Prediction. Given the result of output propagation, which
is a hidden state per output timestep, the final step is to predict a
distribution over next outputs. At training time, we simultaneously
make predictions for all outputs at once. The output-output propa-
gation ensures that information only flows from previous to future
timesteps, so the final hidden state for output step t only includes
information about the input graph and outputs up through time t.

Preprint, Aug 2019, not for distribution.

Daniel Tarlow, Subhodeep Moitra, Andrew Rice, Zimin Chen, Pierre-Antoine Manzagol, Charles Sutton, and Edward Aftandilian

s1 . . . sM
(cid:74)
TOKEN(t)
(cid:74)
COPY(i)
(cid:74)
INPUT_POINTER(i)
(cid:74)
OUTPUT_POINTER(j)
(cid:74)

G
(cid:75)
G
(cid:75)
G
(cid:75)
G
(cid:75)
G
(cid:75)

sM
(cid:74)

G )
(cid:75)

=
=
=
=
=

G ,
(cid:75)

concat(
s1 . . . sM −1
(cid:74)
(Token, t)
(Token, vi )
(InputPointer, i)
(OutputPointer, j)

Table 1: Dereferencing semantics of Tocopo sequences. Here
s1 . . . sM are Tocopo expressions, t ∈ L is a token, and i ∈
1 . . . N indexes a node in the input graph, and j ≥ 0 is an
integer.

We can thus simply define a mapping from output hidden state t to
a distribution over the output t + 1.

Our approach is to define three output heads. A token head, a
copy head, and a pointer head. Letting H be the hidden size, V be
the size of the output vocabulary, and N be the number of nodes
in the input graph, the token head passes output hidden state t
through a dense layer of size H × V . The result is a vector of length
V of “vocab logits.” The copy head passes output hidden state t
through a dense layer of size H × H and then computes an inner
product of the result with the final representation of each node in
the input graph. This gives a size N vector of “copy logits.” The
pointer head performs the same operation but with a different dense
layer, yielding a size N vector of “pointer logits.” The outputs of the
three heads are concatenated together to yield a size V + 2N vector
of predictions. We apply a log softmax over the combined vector
to get log probabilities for a distribution over next outputs and
take the log sum of probabilities associated with correct outputs to
compute the training objective.

B FORMAL SEMANTICS OF TOCOPO

SEQUENCES

We give a formal semantics of what it means for two Tocopo se-
quences to refer to the same nodes in the graph.

See Table 1. Here ∅ represents an empty sequence, and concat(·)
adds an element to the end of a sequence. To interpret this, notice
that many node values in G, for example, leaf nodes in the syntax
tree, are tokens from L. Intuitively, for s and s′ to be equivalent, this
means that copy operations are matched either to copies on nodes
that have the same value, or an equal token from L. Intuitively,
this semantics dereferences all of the pointers and copy operations,
but otherwise leaves the Tocopo sequence essentially unchanged.
This is the maximum amount of semantic interpretation that we
can do without specifying an eDSL.

Now we can define a notion of equivalence. Our notion of equiv-
alence will be based on all of Two Tocopo sequences s and s′ are
equivalent with respect to a graph G, which we write s ⇔G s′,
if the sequences have equal derferencing semantics
G .
(cid:75)
Because it is just checking the location reference, this notion equiv-
alence that is generic across eDSLs. If s ⇔G s′, are equivalent
Tocopo sequences, they should be equivalent semantically in any
reasonable eDSL.

G =
(cid:75)

s′
(cid:74)

s
(cid:74)

C ADDITIONAL EXPERIMENTAL DETAILS

AND RESULTS

C.1 Experimental details
For these experiments, we randomly split resolutions into 80% for
training, 2500 examples for validation, and 10% for test. Unless
otherwise specified, all experiments use grid searches over learn-
ing rates {5e − 3, 1e − 3, 5e − 4, 1e − 4}, gradient clipping by max
norm of {.1, 1.0, 10.0}, hidden dimension of {64, 128} and number
of propagation steps {1, 2, 4, 8, 12}. Batching follows that of [3],
packing individual graphs into one large supergraph that contains
the individual graphs as separate disconnected components. We
add graphs to the supergraph as long as the current supergraph has
fewer than 20, 000 nodes. For some of the hyperparameter configu-
rations we exceeded GPU memory, in which case we discarded that
configuration. We allowed training for up to 1M updates, which
took on the order of 1 week per run on a single GPU for the larger
models. We report results from the training step and hyperparame-
ter configuration that achieved the best accuracy on the validation
data. For Graph2Tocopo models, we use the following vocabulary
sizes: 10000 for input graph node values, 1000 for input node types,
and 1000 for output vocabulary. For DeepDelta, we use input and
output vocab sizes of 30k, as that is the value used by [23] and we
found it achieve better performance than the smaller vocab sizes
used in Graph2Tocopo.

C.2 Effect of autoregressive feedback
As discussed in Sec. A.2, there is a simpler decoder choice employed
by [19] in their latent hiddens model and [35] that does not feed
back previous outputs when generating a sequence from a graph.
We evaluate an ablated form of our model that generates output se-
quences in the same way, removing the autoregressive component
of our method. In Figure 10, we see that feeding in previously pre-
dicted outputs to the model is an important component. Leaving it
out costs 5-6% absolute performance (20-26% relative) degradation.

C.2.1 Effect of graph structure. We ran an experiment removing
edge information from the model, preserving only a basic notion
of ordering. We rendered the diagnostic, build, and Java subgraphs
in a linear order via a depth-first pre-order traversal of nodes and
removed all edges in the input graphs. We added back edges con-
necting each node to all nodes within a distance of 10 in the linear
ordering of nodes. This is meant to mimic a Transformer-style
model with local attention [25], working on a linearization of tree
structures as input [39].

No hyperparameter setting was able to achieve more than 3%
sequence-level validation accuracy, but the best performing edge-
ablated model came from pruning at diameter 1 and doing 12 prop-
agation steps. We speculate that the edges contain critical infor-
mation for localizing where the edit needs to take place, and this
gets lost in the ablation. The diameter 1 problem suffers the least
because it has extracted the subgraph around the errors, which
makes the localization problem easier (at the cost of not being able
to solve as many problems by just looking at that local context).

2019-11-05 02:46. Page 14 of 1–15.

Learning to Fix Build Errors with Graph2Diff Neural Networks

Preprint, Aug 2019, not for distribution.

2 prop steps

4 prop steps

8 prop steps

Prune distance 8 (from above)
Prune distance 8, no feeding back previous
outputs

Difference (absolute)

23.8%

17.5%

-6.3%

26.3%

20.8%

-5.5%

28.0%

22.3%

-5.7%

Figure 10: Effect of removing autoregressive feedback in the decoder.

2019-11-05 02:46. Page 15 of 1–15.

