Resource Allocation Using Gradient Boosting
Aided Deep Q-Network for IoT in C-RANs

Yifan Luo, Jiawei Yang, Wei Xu, Senior Member, IEEE, Kezhi Wang, Member, IEEE,
and Marco Di Renzo, Senior Member, IEEE

1

9
1
0
2

t
c
O
9
2

]
T
I
.
s
c
[

1
v
4
8
0
3
1
.
0
1
9
1
:
v
i
X
r
a

Abstract—In this paper, we investigate dynamic resource allo-
cation (DRA) problems for Internet of Things (IoT) in real-time
cloud radio access networks (C-RANs), by combining gradient
boosting approximation and deep reinforcement learning to solve
the following two major problems. Firstly, in C-RANs, the deci-
sion making process of resource allocation is time-consuming and
computational-expensive, motivating us to use an approximation
method,
i.e. the gradient boosting decision tree (GBDT) to
approximate the solutions of second order cone programming
(SOCP) problem. Moreover, considering the innumerable states
in real-time C-RAN systems, we employ a deep reinforcement
learning framework,
i.e., deep Q-network (DQN) to generate
a robust policy that controls the status of remote radio heads
(RRHs). We propose a GBDT-based DQN framework for the
DRA problem, where the heavy computation to solve SOCP
problems is cut down and great power consumption is saved
in the whole C-RAN system. We demonstrate that the generated
policy is error-tolerant even the gradient boosting regression may
not be strictly subject to the constraints of the original problem.
Comparisons between the proposed method and existing baseline
methods conﬁrm the advantages of our method.

Index Terms—Cloud radio access networks (C-RANs), resource
allocation, Internet of Things (IoT), gradient boosting, reinforce-
ment learning, deep Q-network, ensemble learning.

I. INTRODUCTION

T HE requirement and development of Internet of Things

(IoT) services, a key challenge in 5G, have been con-
tinuously rising, with the expanding diversity and density of
IoT devices [1]. Cloud radio access networks (C-RANs) [2]
are regarded as the promising mobile network architecture
to meet this new challenge. Speciﬁcally, C-RANs separate
base stations into radio units, which are commonly referred as
remote radio heads (RRHs), and signal processing centralized
baseband unit (BBU) Pool. In a C-RAN, BBU can be placed
in a convenient and easily accessible place, and RRHs can be
deployed up on poles or rooftops on demand. It is expected
that C-RAN architecture will be an integral part of future
deployments to enable efﬁcient IoT services.

Y. Luo and J. Yang made the same contribution to this paper.
Y. Luo and W. Xu are with the National Mobile Communications Research
Laboratory (NCRL), Southeast University, Nanjing 210096, China (e-mail:
roey.luoyif@gmail.com; wxu@seu.edu.cn).

J. Yang is with the Department of Computer Science and Engi-
neering, Southeast University, Nanjing 210096, China (e-mail: Jiawei-
Young@outlook.com).

K. Wang is with the Department of Computer and Information Sciences,

Northumbria University, Newcastle upon Tyne NE1 8ST, U.K.

M. Di Renzo is with the Laboratoire des Signaux et Syst`emes,
CNRS, CentraleSup´elec, Univ Paris Sud, Universit´e Paris-Saclay, 3 rue
Joliot Curie, Plateau de Saclay, 91192 Gif-sur-Yvette, France (e-mail:
marco.direnzo@centralesupelec.fr).

Dynamic resource allocation (DRA) for IoT in C-RANs is
indispensable to maintain acceptable performance. In order to
get the optimal allocation strategy, several works have tried
to apply convex optimizations, like second order cone pro-
gramming (SOCP) in [3], semi-deﬁnite programming (SDP)
in [4] and mix-integer programming (MIP) in [5]. However,
in real-time C-RANs where the environment keeps changing,
the efﬁciency of the above methods in ﬁnding the optimal
decision faces great challenges. Attempts have been made in
reinforcement learning (RL) to increase the efﬁciency of the
solution procedure in [3] [6].

RL has shown its great advantages to solve DRA problems
in wireless communication systems and for IoT. Existing
methods to DRA problem in RANs generally model it as a
RL problem [6] [7] [8], by setting different parameters as the
reward. For instance, the work in [6] regarded the successful
transmission probability of the user requests as the reward,
and another work in [8] set the sum of average quality of
service (QoS) and averaged resource utilization of the slice as
the reward. However, with the increase of the complexity in
allocation problems, the search space of solutions tends to be
inﬁnite, which is hard to be tackled.

With the combination of RL and deep neural network
(DNN) [9], deep reinforcement
learning (DRL) has been
proposed and applied to address the above problems in [10]
[11] [12]. By utilizing the ability of extracting useful features
directly from the high-dimensional state space of DNN, DRL
is able to perform end-to-end RL [9]. With the assistance of
DNN, problems of large search space and continuous states
are no longer the insurmountable challenges.

To apply DRL framework in DRA problems, the design of
reward, action and state becomes vital. The action set needs to
be enumerable in most circumstances. The work in [3] used a
two-step decision framework to guarantee its enumerability, by
changing the state of one RRH at each epoch, which performs
well in the models with innumerable states.

Furthermore, in DRA problems, how to get optimal alloca-
tion strategy will be ﬁnally turned into another optimization
problem in most cases, i.e., convex optimization problem [13],
which can be solved mathematically. Unfortunately, traditional
algorithms [14] [15] [16] for solving the convex optimization
problem, such as SOCP still faces signiﬁcant limitations, such
as time-consuming, making it hard to generate a policy for
large-scale systems.

Recent works have achieved signiﬁcant

in
computational efﬁciency by applying the DNN approximator
[17] [18] [19] [20] to DRA problems. However, the unstable

improvement

 
 
 
 
 
 
performance of DNN in regression process makes it hard to
achieve good performance [21]. With a large number of hyper-
parameters, ﬁne tuning becomes even harder in practical sys-
tem. Some researchers discussed and investigated this problem
in computability theory and information theory domains, e.g.,
in [22].

Gradient boosting machine (GBM) [23] is one member
of boosting algorithms family [24] [25], a sub-branch of
ensemble learning [26] [27] [28]. It has been ﬁrmly established
as one of state-of-the-art approaches in machine learning (ML)
community, and it has played a dominating role in existing data
mining and machine learning competitions [29] due to its fast
training and excellent performance. However, to the best of
our knowledge, few works applied this method to the DRA
problem, even to other regression problems in communication
systems.

In this paper, to efﬁciently address DRA problem for IoT in
C-RANs with innumerable states, one common form of DRL,
namely the deep Q-network (DQN) is employed. Moreover,
to tackle the difﬁculties in obtaining the reward in DQN in
low latency, a tree-based GBM, i.e., gradient boosting decision
tree (GBDT) is utilized to approximate the solutions of SOCP.
Then, we demonstrate the improvement of our method by
comparing it to the traditional methods under simulations.
The main contributions of this paper are as follows:
• We ﬁrst give the model of dynamic resource allocation
problem for IoT in the real-time C-RAN. Then, we pro-
pose a GBDT-based regressor to approximate the SOCP
solution of the optimal transmitting power consumption,
which serves as the immediate reward needed in DQN.
By doing so, there is no need to solve the original SOCP
problem every time, and therefore great computational
cost can be saved.

• Next, we aggregate the GBDT-based regressor with a
DQN to propose a new framework, where the immediate
reward is obtained from GBDT-based regressor instead of
SOCP solutions, to generate the optimal policy to control
the states of RRHs. The proposed framework can save the
power consumption of the whole C-RAN system for IoT.
• We show the performance gain and complexity reduction
of our proposed solution by comparing it with the existing
methods.

The remainder of this paper is organized as follows. Section
II presents the related works, whereas system model is given
in Section III. Section IV introduces the proposed GBDT-
based DQN framework. The simulation results are reported in
Section V, followed by the conclusions presented in Section
VI.

II. RELATED WORKS

The resource allocation problem under C-RANs is normally
interpreted into an optimization problem, where one needs to
search the decision space to ﬁnd an optimal combinatorial set
of decisions to optimize different goals [13] [30] [31] based
on current situations. Although numerous researchers devoted
their time in ﬁnding solutions to optimization problems, most
of them are still hard or impossible to be tackled with

2

traditional pure mathematical methods. RL has been recently
applied to address those problems.

In [32], a model-free RL model was adopted to solve the
adaptive selection problem between backhaul and fronthaul
transfer modes, which aimed to minimize the long-term deliv-
ery latency in fog radio access network (F-RAN). Speciﬁcally,
an online on-policy value-based strategy State-Action-Reward-
State-Action (SARSA) with linear approximation was applied
in this system. Moreover, some works have proposed more
efﬁcient RL methods to overcome slow convergence and
scalability issues in traditional RL-based algorithms, such as
Q-learning. In [33], four methods, i.e. state space reduction
techniques, convergence speed up methods, demand forecast-
ing combined with RL algorithm and DNN were proposed to
handle the aforementioned problems, especially to deal with
the huge state space.

Furthermore, as reported in [34], DQN achieved a better
performance on resource allocation problems, compared with
the traditional Q-learning based method. In practice, the size
of possible state space may be very large or even inﬁnite,
which makes it impossible to traverse each state that required
by the traditional Q-learning. Approximation methods can
address this kind of problem that they maps the continuous
and innumerable state space to a near-optimal Q-value space
in consecutive setting, rather than Q-table. DNN shows its
advantage of approximation in the high-dimensional space in
many domains. Therefore, the adoption of DNN to estimate
Q-value can improve the system performance and computing
efﬁciency, as reported in the simulation results from [34].

In [3], a two-step decision framework was adopted to solve
the enumerability problem of action space in C-RANs. The
DRL agent ﬁrst determined which RRH to turn on or turn
off, and then the agent got the resource allocation solution by
solving a convex optimization problem. Any other complex
actions can be decomposed into the two-step decision, reduc-
ing the action space signiﬁcantly. Moreover, the work in [6]
shows the impractical use of SA (i.e., Single BS Association)
scheme even in a small-scale C-RAN. Speciﬁcally, SA scheme
abandoned the collaboration of each RRH and only supported
few users. This research is a guidance to our research.

The works in [6] and [8] all adopted the DRL method to
solve resource allocation problems in the RAN settings. In
[8], the concept of intelligent allocation based on DRL was
proposed to tackle the cache resource optimization problem in
F-RAN. To satisfy user’s QoS, the caching schemes should be
intelligent, i.e. more effective and self-adaptive. Considering
the limitation of cache space, this requirement challenges the
design of schemes, and it motivates the adoption of DRL
technique.

As reported in [6], a DRL-based framework is used in
more complicated resource allocation problems, i.e., virtual-
ized radio access networks. Based on the average QoS utility
and resource utilization of users, the DQN-based autonomous
resource management framework can make virtual operations
to customize their own utility function and objective function
based on different requirements.

In this paper, to improve the system efﬁciency, we propose
a novel gradient-boosting-based DQN framework for resource

3

According to the Shannon formula, the data rate of user ui

can be given as:

(cid:18)

Ri = B log2

1 +

(cid:19)

SIN Ri
Γm

, ui ∈ U

(2)

where B is the channel bandwidth and Γm is the SINR margin
depending on a couple of practical considerations, e.g., the
modulation scheme.

The relationship of the transmitting power and the power
consumed by the base station can be approximated to be nearly
linear, according to [36]. Then, we apply the linear power
model for each RRH as:

Pi =






Pri,A +

Pri,S

1
η

Pri,T

ri ∈ A

ri ∈ S

(3)

where Pri,T = (cid:80)
uj ∈U |wriuj |2 is the transmitting power of
RRH ri; η is a constant denoting the drain efﬁciency of the
power ampliﬁer; and Pri,A is the power consumption of RRH
ri when ri is active without transmitting signals. In the case
of no need for transmission, ri can be set to the sleep mode,
whose power can be given by Pri,S. Thus, one has A∪S = R.
In addition, we take consideration of the power consumption
for the state transition of RRHs, i.e. the power consumed to
change RRHs’ states. We put the RRHs which reverse states
in the current epoch to the set T and use Pri,T to denote the
power to change the mode between Active and Sleep, i.e. we
assume they share the same power consumption. Therefore, in
the current epoch, the total power consumption of all RRHs
can be written as:

PTotal =

(cid:88)

(cid:88)

(cid:12)
(cid:12)wriuj

(cid:12)
(cid:12)

2

+

(cid:88)

Pri,A +

1
η

(cid:88)

ri∈S

Pri,S

uj ∈U

ri∈A
(cid:124)

(cid:123)(cid:122)
Transmitting Power

ri∈A
(cid:124)

(cid:125)

(cid:123)(cid:122)
State Power

(cid:125)

(4)

(cid:88)

+

Pri,T

.

ri∈T
(cid:124)
(cid:125)
(cid:123)(cid:122)
Transition Power

B. CP-beamforming

From Equation (4), one can see that the latter two parts
are easy to be calculated, which are composed by some
constants and only relying on the current state and action.
To minimize PTotal, it is necessary for us to calculate the
minimal transmitting power in each epoch, which depends on
the allocation scheme of beamforming weights in active RRHs.
Therefore, this optimization problem can be expressed as:
• Control Plane (CP)-Beamforming:

min
wriuj

PT

s.t. PT =

(cid:88)

(cid:88)

(cid:12)
(cid:12)wriuj

(cid:12)
2
(cid:12)

ri∈A

uj ∈U
(cid:18)

Ri ≤ B log2
(cid:88)

(cid:12)
(cid:12)wriuj

(cid:12)
(cid:12)

uj ∈U

1 +

(cid:19)

SIN Ri
Γm

, ui ∈ U

2

≤ Pri, ri ∈ A

(5)

(5.1)

(5.2)

(5.3)

Fig. 1. Dynamic Resource Allocation for IoT under DRL framework in C-
RANs.

allocation problem, which signiﬁcantly improves the system
performance through ofﬂine training and online running.

To the best of our knowledge, there is few works to apply
gradient boosting machine to approximate solutions of convex
optimization problems in wireless communication and we are
the ﬁrst to propose this framework.

III. SYSTEM MODEL

A. Network Model

We consider a typical C-RAN architecture where there is
a single cell model with a set of m RRHs denoted by R =
{r1, r2, ..., rm} and a set of n users which can be some IoT
devices denoted by U = {u1, u2, ..., un}. In the DRA for IoT
in C-RAN as shown in Fig. 1, we can get the current states,
i.e. the state of each RRHs and the demands of IoT device
users, from the networks in K-th decision epoch tk. All the
RRHs are connected to the centralized BBU pool, meaning all
information can be shared and processed by the DQN-based
agent to make decisions, i.e. turning on or off the RRHs. We
simplify the model by making assumption that all RRHs and
users are equipped with a single antenna, which is readily to
be generalized into the multi-antenna case by using technique
proposed in [35].

Then,

the corresponding signal-to-interference-plus-noise

2

2

(cid:12)
(cid:12)

(cid:80)

uj (cid:54)=ui

+ σ2

, ui ∈ U

SIN Ri =

ratio (SINR) at the receiver of i-th user ui can be given as:
(cid:12)
(cid:12)
(cid:12)hT
uiwui
(cid:12)
(cid:12)
(cid:12)hT
ui wuj
where hui = [hr1ui, hr2ui , ..., hruui]T denotes the chan-
nel gain vector and each element hrlui denotes the chan-
nel gain from RRH rl ∈ R to user ui; wui =
[wr1ui, wr2ui, ..., wrmui]T denotes the vector of all RRHs
beamforming to user ui and each element wrlui denotes the
weight of beamforming vector in RRH rl ∈ R distributed to
user ui and σ2 is the noise.

(1)

Mobile Backhaul NetworkBBU PoolRRH2RRH1RRH3User1User2DRL-based AgentStates and RewardsActionswhere the objective is to get the minimal total transmitting
power given the states of RRHs and user demands. Also,
the variables wriuj are distributive weights corresponding
to beamforming power; Ri is deﬁned as the user demand;
SIN Ri is given by Equation (1) and Pri is the transmitting
power constraint for RRH ri. Also, Constraint (5.2) ensures
the demand of all users will be met, whereas Constraint (5.3)
ensures the limitation of transmitting power in each RRH.

As shown in [4], the above CP-beamforming can be trans-
formed into a SOCP problem. Therefore, we rewrite the above
optimizations as:
• Modiﬁed CP-Beamforming:

min
wriuj

s.t.

PT

(cid:88)

uj ∈U

(cid:12)
(cid:12)hH

uiwuj

(cid:12)
(cid:12)

2

+ σ2 ≤ µui

(cid:12)
(cid:12)hH

uiwui

(cid:12)
2
(cid:12)

, ui ∈ U

(6)

(cid:88)

(cid:12)
(cid:12)wriuj

(cid:12)
2
(cid:12)

≤ Pri , ri ∈ A

uj ∈U
(cid:88)

(cid:88)

(cid:12)
(cid:12)wriuj

(cid:12)
(cid:12)

2

≤ PT

ri∈A

µi =

uj ∈U
ιi + 1
ιi
ιi = Γm(2

, ui ∈ U

Ri
B − 1)

(6.1)

(6.2)

(6.3)

(6.4)

(6.5)

where we apply variable PT to replace the optimization (5.1)
by adding Constraint (6.3), which is a common method in
transformation process [37]. We also rewrite Constraint (5.2)
as Constraint (6.1) and apply some simple manipulations to
get the above modiﬁed optimization.

it

Now,

is ready to see that

the above Modiﬁed CP-
Beamforming optimization is the same as a standard SOCP
problem. By using the iterative algorithm mentioned proposed
in [38], we can get the optimal solutions. It is worth noting
that the CP-Beamforming optimization may have no feasible
solutions. In this case, it means more RRHs should be activated
to satisfy the user demands. In this case, we will give a large
negative reward to the DQN agent and jump out of the current
training loop.

Then, we can calculate the total power consumption by
applying Equation (4). In the following part, we propose the
DQN-based framework to predict the states of RRHs and adopt
GBDT to approximate the solutions of the aforementioned
SOCP problems.

IV. GBDT AIDED DEEP Q-NETWORK FOR DRA IN
C-RANS

A. State, Action Space and Reward Function

Our goal in the aforementioned DRA problem is to generate
a policy that minimizes the system’s power consumption at
any state by taking the best action. Here, the best action
refers to the action that contributes the least to overall power
consumption in a long term but also satisﬁes user demands,
system requirements and constraints among all the available
idea of RL-based method is to
actions. The fundamental
abstract an agent and an environment from the given problem

4

to generate the environment model [39] and employ the agent
to ﬁnd the optimal action in each state, so as to maximize the
cumulative discounted reward by exploring the environment
and receiving immediate reward signalled by the environment.
To apply RL method in our problem, we transform the
system model deﬁned in Section III into a RL model. The
general assumption that future reward is discounted by a
factor of γ per time-step is made here. Then, the cumulative
discounted reward from time-step t can be expressed as:

(cid:34) ∞
(cid:88)

(cid:35)
γkrk(sk, ak)|st = s, at = a

Rt = E

(7.1)

k=t

where E(·) denotes mathematical expectation; rk(·) denotes
the k-th reward; sk denotes the k-th state and γ ∈ (0, 1]
denotes the discount factor. If γ tends to 0, the agent only
considers the immediate reward; whereas if γ tends to 1, the
agent focuses on the future reward. Moreover, the inﬁnity over
the summation sign indicates the endless sequence in DRA
problem.

Leveraging the common deﬁnition in Q-learning, the opti-
mal action-value function Q∗(s, a) is deﬁned as the greatest
mathematical expected cumulative discounted reward reached
by taking action a in state s and then following a subsequently
optimal policy, which guarantees the optimality of cumulative
future reward. The function Q∗(s, a) strongly follows the
Bellman equation, a well-known identity in optimality theory.
In this model, the optimal action-value function Q(s, a) to
represent the maximum cumulative reward from state s with
action a can be expressed as:

Q∗(s, a) = E

(cid:104)

r(s,a) + γ max

a(cid:48)

(cid:105)
Q∗(s(cid:48), a(cid:48))|s, a

(7.2)

where r(s,a) denotes the immediate reward received at state
s if action a is taken; a(cid:48) denotes the possible action in the
next state s(cid:48), and other symbols are of the same meaning
as Equation (7.1). The expression means that the agent takes
action a in the state s, receiving the immediate reward r(s,a),
and then subsequently follows an optimal trajectory that leads
to greatest Q(s, a) value.

In a general view, Q∗(s, a) demonstrates how promising the
ﬁnal expected cumulative reward will be if action a is taken in
state s in a quantitative way. That is to say, in DRA problem,
how much power consumption the C-RAN can cut down if it
decides to take the action a, i.e switches on or off one selected
RRH when observing the state s, i.e. a set of user demands
and the states (i.e. sleep/active) of RRHs. Since the true value
of Q∗(s, a) can never be known, our goal is to employ DNN
to learn an approximation Q(s, a). For the following sections,
Q(s, a) just denotes the approximated Q∗(s, a) and has all the
same properties of Q(s, a).

The generic policy function π(s) deﬁned in the context of

RL is used here, which can be expressed as:

π(s) = arg max

a

Q∗(s, a)

(7.3)

where π(s) is the argmax of the action-value function Q(s, a)
over all possible actions in a speciﬁc state s. The policy
function leads to the action that maximize the Q(s, a) values
in all states.

5

To sum up, the policy mentioned in this work is a function
that maps the current state s, the set of user demand and RRHs
status, to the best action a, turning on or off one RRH, that
minimizes the overall power consumption of the whole system.

B. Gradient Boosting Decision Tree

GBM is a gradient boosting framework that can be applied
to any classiﬁers or regressors. To be more speciﬁc, GBM is
the aggregation of base estimators (i.e., classiﬁers or regres-
sors) that any base estimators like K nearest neighbor, neural
network and naive Bayesian estimators can be ﬁtted into the
GBM. Better base estimators advocate higher performance.
Among all kinds of GBM, a prominent one is based on
decision tree, called gradient boosting decision tree (GBDT),
which has been gaining its popularity for years due to its
competitive performance in different areas. In our framework,
the GBDT is applied to the regression task due to its prominent
performance.

The concept of GBDT is to optimize the empirical risk
via steepest gradient descent in hypothesis space by adding
more base tree etimators. Considering the regression task in
our work, given a dataset with n entities of different states
and their corresponding rewards generated by simulation and
solving SOCP, one can have

D = (xi, pi)(|D| = n, xi ∈ D ∪ Y, pi ∈ S)

1, yi

2, ..., di

2, ..., yi

where xi denotes the state representation [yi
1, di
di
n] of system model, whereas pi denotes the corre-
sponding solution of SOCP solver from Equation (6), in line
with the deﬁnition of the Reward function. To optimize the
empirical risk of regression is to minimize the expectation of
a well-deﬁned loss function over the given dataset D, which
can be express as:

m,

ED[L( (cid:98)P , P )] = l( (cid:98)P , P ) + Ω(φ) = l(f (X), P ) + Ω(φ)

(8)

where φ denotes the model
itself and f (X) is the ﬁnal
mapping to approximate P , which is our ﬁtting object, the
power comsumption. X is the set of xi representing system
model, and P is the set of pi representing solution of SOCP
solver. Here the ﬁrst term is model prediction loss, which is a

Fig. 2. The DQN-based scheme for dynamic resource allocation.

The state, action and reward deﬁned in our problem are

given as:

• State: The state has two components that one is a set
of states of RRHs and the other is a set of demands
from users. Speciﬁcally, Y = [y1, y2, ..., ym] is deﬁned
as the set of all m RRHs’ states, in which yi ∈ {0, 1}
denotes the state of RRH i. In the case of yi = 0,
RRH i is in the sleep state, whereas yi = 1 means
that
is
deﬁned as the set of all n users’ demands, and dj ∈
[dmin, dmax] denotes the demand of user j, in which
dmin is the minimum of all demands and dmax is the
maximal demand. Thus, the state of RL is expressed as
D ∪ Y = [y1, y2, ..., ym, d1, d2, ..., dn] and its cardinality
is m + n.

is in the active state. D = [d1, d2, ..., dn]

it

• Action: In each decision epoch, we enable the RL agent
to determine the next state of one RRH. We use a set
of A = {α1, α2, · · · , αm} to denote the action space, in
which αi ∈ {0, 1}, i = 1, 2, · · · , m. If αi = 1, it means
RRH i changes the state, otherwise the RRH remains its
current state in next epoch. Then, the action space can
be substantially reduced. It is noteworthy that we set the
constriction that 0 (cid:54) (cid:80)m
i=1 αi (cid:54) 1, which means only
one or none of all RRH states will alter its state and
reduces the space into the size of m + 1.

• Reward: To minimize the total power consumption, we
deﬁne the immediate reward as the difference between
the upper bound of power consumption. The actual power
consumption is expressed as:

∇k = PUB − Ptotal

where PUB denotes the upper bound of the power con-
sumption obtained from the system setting, and Ptotal
denotes the actual total power consumption of the system
that is composed of three parts deﬁned in Equation (4).
To be more speciﬁc, the reward is deﬁned to minimize
the system power consumption under the condition of
satisfying the user demands, which requires us to solve
the optimization problem according to Equation (6),
shown in Section III.

StateAgentDNNPolicyΠθ(s,a) Take an actionReceive an immediate reward...xTree 1Tree 2Tree kΣ GBDT regressionInput Features:SOCPTree SplitsTransformedFeaturesSummationOutput: PowerDQNHidden layersStates of m RRHs and n user demandsStates of m RRHs and n user demandsEnvironmentTurn on/off one RRHyObservestates()differentiable convex function to measure the distance between
true power consumption and estimated power consumption;
and L2 loss (i.e., mean-square error) is applied in this task. The
latter term is the regularization penalty applied to constrain
model complexity, contributing to ﬁnalize a model with less
over-ﬁtting and better generalization performance.

The choice of prediction loss and regularization penalty
alters circumstantially. Also, the penalty function is given by:

Ω(f ) = βT +

λ(cid:107)w(cid:107)2

1
2

where β and λ are two hyper-parameters, while T and w are
the numbers of trees ensembled and weights owned by each
tree, respectively. When the regularization parameter is set to
zero, the loss function falls back to the traditional gradient tree
boosting method [40].

In GBDT, it starts with a weak model that simply predicts
the mean value of P at each leaf and improves the prediction
by aggregating K additive ﬁxed size decision trees as base
estimators to predict the pseudo-residuals of previous results.
The ﬁnal prediction is linear combination of all the output from
K regression trees. The ﬁnal estimator function as adverted in
(9) can be expressed as follow:

f (x) =

K
(cid:88)

k=0

fk(x) = f0(x) +

K
(cid:88)

k=0

θkφk(x)

(9)

where f0 is the initial guess, φk(x) is the base estimator at
the iteration k and θk is the weight for the kth estimator or a
ﬁxed learning rate. The product θkφk(x) denotes the step at
iteration k.

C. GBDT-based Deep Q-Network (DQN)

In this section, we will show how to apply GBDT-based
DQN scheme to solve our DRA problem for IoT in real-time
C-RAN, by using the previously deﬁned states, actions and
reward. Traditional RL methods, like Q-learning, compute and
store the Q value for each state-action group into a table. It
is unrealistic to apply those methods in our problem, as the
state-action groups are countless and the demands of users in
a state are continuous variables. Therefore, DQN is considered
to be best solutions for this problem. Similar with the related
works, e.g. [8] [34], we also apply experience replay buffer
and ﬁxed Q-targets in this work to estimate the action-value
function Q (s, a).

In our framework,

two stages are included,

i.e., ofﬂine
training and online decision making as well as regular training:
• For ofﬂine training stage, we pre-train DQN to estimate
the value of taking each action in any speciﬁc states.
To achieve this, millions of system data are generated in
terms of all RRHs states, user demands and its corre-
sponding system power consumption by simulation and
solving SOCP problem given in equation (6). Then, the
GBDT is employed to estimate the immediate reward to
alleviate the expensive computation in solving the SOCP
problem for further training and tuning.

• For online decision making and regular tuning, we load
the pre-trained DQN to generate the best action to take for

6

our proposed DRA problem in real-time. This is achieved
by employing the policy function deﬁned in (7.3), which
maximizes the Q(s, a) in state s. To emphasize,
the
Q(s, a) function tells how much the system can cut down
the power consumption if it decides to take the action
a when seeing the state s. Then, the DQN observes the
immediate reward rt obtained from GBDT approximation
and observes next state st+1. In an online regular tuning
scheme, the DQN will not immediately update model
parameters when observing new states but to store the
new observations to memory buffer. Then, under some
given conditions, the DQN will ﬁne-tune its parameters
according to that buffer. This allows DQN to dynamically
adapt to new patterns regularly.

The whole algorithm is given in Algorithm 1, whereas the
framework of GBDT-based DQN is given by Fig. 2. The θ
denotes the set of model parameters. The loss function is L2
loss (i.e., mean-square error), which indicates the difference
between Q target and model output. Si refers to the step i in
Algorithm 1.

Algorithm 1 GBDT-based DQN framework

Ofﬂine:
S1: Generate millions of data randomly via SOCP solver;
S2: Pre-train GBDT with those data and save its model;
S3: Pre-train DQN with those data from S6 to S15, and

then save the well-trained network and its correspond-
ing experience memory D;

Online:
S4: Load replay memory with capacity N from ofﬂine-

trained experience memory D and load GBDT model;
S5: Set action-value function Q with weights θ from ofﬂine-

trained network;
S6: For each episode t-th
S7:

a) Ofﬂine: with probability ε select a random action
at, otherwise select at = π(s) when observing a
new state s;
b) Online: directly select at = π(s) when observing
a new state s;
Execute action at;
Obtain reward rt from a) Ofﬂine: SOCP solver or
b) Online: GBDT machine, and observe st+1;
Store transition (st, at, rt, st+1) in D;
if a) Ofﬁne or b) Online: reach the given condition
then execute from S12 to S14;

Sample random mini-batch of transitions
(sj, aj, rj, sj+1) from D;
Set

(cid:40) rj,

yj =

if episode terminates at step j+1
Q(φj+1, a(cid:48); θ−), otherwise

rj + γ max

a(cid:48)
Apply the gradient descent step on
(yi − Q(φj, aj; θ))2 with respect to the net-
work parameters θ;

S8:
S9:

S10:
S11:

S12:

S13:

S14:

S15: EndFor

In Fig. 2, one can see that the left side describes a DQN
framework, illustrating the agent, the environment and how
to get the reward. Speciﬁcally, the agent will observe a new

state from the environment after taking an action and then
it will receive an immediate reward signalled by the reward
function from GBDT approximator. Traditional DQN obtains
the reward by solving the SOCP optimization, which can not
be real-time, as explained before. In our architecture, we adopt
GBDT regression (i.e., the right side of Fig. 2) to obtain the
reward, which can operate in a online process in real-time.

We also give the training process of GBDT in the Appendix.

D. Error Tolerance Examination (ETE)

Our target is to use GBDT to approximate the typical SOCP
problem in C-RANs under the framework of DQN. Thus, it
is important to evaluate its practical performance. The error
from GBDT or DNN will inﬂuence the optimality of the given
scheme, even worsening the performance of whole system
power consumption. Therefore, the examination of error in-
ﬂuence is of vital signiﬁcance. Considering its important role
in the whole DRA problem, we emphasize the concept of
error tolerance examination (ETE) here. Speciﬁcally, in the
simulation, we will ﬁrst compare the result of the optimal
decision provided by CP-Beamforming solution with the near-
optimal decision from GBDT or DNN approximation solution,
and then evaluate its performance in the dynamic resource
allocation settings.

V. SIMULATION RESULTS

In this section, we present

the simulation settings and
performance of the proposed GBDT-based DQN solutions. We
take the deﬁnition of channel fading mode from previous work
as [41]:

hr,u = 10

−L(dr,u )
20

√

ϕr,usr,uGr,u

(4.1)

where L(dr,u) is the path loss with the distance of dr,u; ϕr,u
is the antenna gain; sr,u is the shadowing coefﬁcient and Gr,u
is the small-scale fading coefﬁcient. The simulation settings
are summarized in Table I.

All

training and testing processes are conducted in the
environment equipped with 8GB RAM, Intel core i7-6700HQ
(2.6GHz), python 3.5.6, tensorﬂow 1.13.1 and lightGBM 2.2.3.
We compare our DQN-based solution containing GBDT
approximator (abbreviated as DQN) with two other schemes:
1) All RRHs Open (AO): all RHHs are turned on, which

can serve each user;

2) One RRH Closed (OC): one of those RHHs (chosen
randomly) stays in the sleep state, which cannot serve any
user.

It is noteworthy that the in previous work [42], another
solution in which only one random RRH is turned on, is
also discussed in the dynamic resource allocation problem.
However, it can hardly be applied to the practical systems [3].
Therefore, we do not compare it in this paper.

7

TABLE I
SIMULATION SETTINGS

Parameters
Symbol
B
Channel bandwidth
Pr,max Max transmit power
Pr,A
Pr,S
Pr,T
σ2
ϕr,u
sr,u
Gr,u

Active power
Sleep power
Transition power
Background noise
Antenna gain
Log-normal shadowing
Rayleigh
small-scale
fading
Path loss with a dis-
tance of (km)
Distance

dr,u

dr,u

ηl

Power ampliﬁer efﬁ-
ciency

Value
10 MHz
1.0 W
6.8 W
4.3 W
2.0 W
-102 dBm
9 dBi
8 dB
CN (0, I)

148.1 + 37.6log2dr,u dB

Uniformly distributed in [0, 800]
m
25%

aW = Watt, dB = decibel, dBm = decibel-milliwatts, dBi =
dB(isotropic).

TABLE II
COMPUTATIONAL COMPLEXITY COMPARISON

System Input Setup

Average Time Per Input
GBDT
0.00079s
0.00077s
0.00070s
0.00075s

6 RRHs and 3 users
8 RRHs and 4 users
12 RRHs and 6 users
18 RRHs and 9 users
aThe time in above table is obtained by averaging 1000 different system
inputs, each of which is recalculated by 10000 times through two
algorithms respectively.

SOCP
0.08281s
0.09387s
0.16240s
0.42803s

and user demands. In addition, both the GBDT approximator
and the traditional SOCP method are executed to predict or
compute the outputs of that test set for 10000 times, respec-
tively. One can see from Table II that GBDT approximator is
much faster than SOCP solver, which prove the efﬁciency of
GBDT approximator.

2) Fitting Property: Then, we analyse the performance
of GBDT approximator in speciﬁc situations, where we set
that there are 8 RRHs and 4 users of IoT devices whose
demands are ranging from 20Mbps to 40Mbps respectively.
We compare it with DNN approximator. It applies the fully-
connected net with 3 layers, each of which with 32, 64, 1
neurons respectively. Its activation function is a rectiﬁed linear
unit (ReLU). Firstly, in Fig. 3(a), we assume that all 8 RRHs
are turned on. One can see from this ﬁgure that GBDT has
better ﬁtting performance than DNN. Then, we assume that
there is one RRH switched off. One can see from Fig. 3(b)
that GBDT still ﬁts very well with the SOCP solutions. In
Fig. 3(c), we assume that the states of all 8 RRH are set
switched on or off randomly. As expected, GBDT has much
better ﬁtting performance, compared with the SOCP solutions.

A. GBDT-based SOCP Approximator

B. Training Effect of GBDT and DNN

1) Computational Complexity: We compare computational
complexity between a GBDT approximator and solutions from
traditional SOCP solver in [33]. Firstly, a test set of 1000
entities are randomly generated in terms of status of RRHs

We demonstrate the training performance between the
GBDT approximator and DNN aproximator by comparing the
training effect in Fig. 4. Mean squared error (MSE) is used
here to calculate the loss. From Fig. 4, one can see that even

8

(a) All RRHs open

(b) One RRH closed

(c) Random states of RRHs

Fig. 3. Approximation performance of GBDT approximator.

GBDT-based DQN solutions.

C. System Performance

In this section, we consider there are 8 RRHs and 4 users,
whose demands are randomly selected. We change the user
demands every 100 ms. The performance of AO, OC and
GBDT-based DQN is compared next.

1) Instant Power: We examine the instant system power
consumption in this subsection. In the top ﬁgures of Fig. 5(a)
and Fig. 5(b), we compare the strategies of AO and DQN,
where we set all the RRHs open initially and then all RRHs
stay active in AO schemes. In the bottom ﬁgures of Fig. 5(a)
and Fig. 5(b), we turn off one RRH randomly at the beginning
for both OC and DQN and then one RRH stay switched off
in OC scheme. Moreover, we set user demands are selected
randomly from the set of 20Mbps to 40Mbps in Fig. 5(a),
whereas we randomly select user demands from the set of
20Mbps to 60Mbps in Fig. 5(b). One can see from all the
ﬁgures in Fig. 5 that our proposed DQN always outperforms
AO and OC. This is because DQN controls RRHs to turn
on and off depending on the current states of the systems,
whereas AO always turns on all the RRHs and OC randomly
turns off one RRH, which may not be the optimal strategy and
contribute to larger power consumption than DQN.

One can also see that when we increase the upper limit
of user demands from 40Mbps in Fig. 5(a) to 60Mbps in
Fig. 5(b), the performance of all DQN, OC and AO become
more unstable. However, our proposed DQN still has the best
performance when compared with AO and OC.

Moreover, one can see that although there may be some
errors caused by GBDT approximator, our proposed DQN
framework has considerable performance, which shows the
good ability of error tolerance in our proposed solution.

2) Average Power: In Fig. 6, we show the performance
comparison between GBDT-based DQN, AO and OC in the
long term. The DQN with reward obtained from SOCP solver
is also depicted. We compare the average system power
consumption by averaging all instant system power in the past
time slots.

We ﬁrst analyse the performance under the condition of
user demands below 40Mbps between both DQN schemes
(including GBDT and SOCP) and AO scheme. We set all the

(a) Training effect of GBDT within 4 seconds

(b) Training effect of DNN over 3 hours

Fig. 4. Training effect of GBDT and DNN.

trained with far more time, the loss of DNN is still higher than
that of GBDT. One also notices that GBDT has less parameters
to adjust and therefore has quicker training process.

The speciﬁc comparison is not unfolded here, as it is not the
focus of this paper. Next, we will examine the performance of

246810121416182022242628303234363840Input Entity Index54565860626466System Power Consumption (Watts)SOCPGBDTDNN246810121416182022242628303234363840Input Entity Index52545658606264System Power Consumption (Watts)SOCPGBDTDNN246810121416182022242628303234363840Input Entity Index444648505254565860System Power Consumption (Watts)SOCPGBDTDNN9

(a) Demands from 20Mbps to 40Mbps

(a) AO VS. DQN

(b) Demands from 20Mbps to 60Mbps

(b) OC VS. DQN

Fig. 5. Total instant power consumption with different user demands and
allocation schemes.

Fig. 6. Total average power consumption with different allocation schemes.

RRH switched on and set user demands changed every 100 ms
per slot and lasting for 500s. One can see from Fig. 6(a) that
both DQN schemes outperform AO and can save power around
8 Watts per time slot. The slight ﬂuctuation comes from the
randomness of the requirement. Moreover, one can see from
Fig. 6(a) that DQN with GBDT have the similar performance
as the DQN scheme with SOCP solver, which shows the error
tolerance feature of our proposed solutions.

Then we turn one RRH off and continue to analyse the
average system power consumption under DQN and OC
scheme. One can see from Fig. 6(b) that both DQN schemes
still outperform OC scheme, as expected. Also, one can see
that DQN scheme with GBDT has the similar performance as
SOCP solver, similarly with above.

3) Overall Performance of GBDT-based DQN: To evaluate
the overall performance of GBDT-based DQN in different
situations, we set user demands from 20Mbps to 60Mbps with
10Mbps interval, and keep other factors unchanged. One can
see from Fig. 7(a) and Fig. 7(b) that with the increase of

user demands, the power consumption of AO, OC and DQN
increase as well. One also sees that our proposed GBDT-based
DQN have much better performance than AO and OC, as
expected, which prove the effectiveness of our scheme.

VI. CONCLUSION

In this paper, we presented a GBDT-based DQN framework
to tackle the dynamic resource allocation problem for IoT
in the real-time C-RANs. We ﬁrst employed the GBDT to
approximate the solutions of the SOCP problem. Then, we
built the DQN framework to generate a efﬁcient resource
allocation policy regarding to the status of RRHs in C-RANs.
Furthermore, we demonstrated the ofﬂine training, online
decision making as well as regular tuning processes. Lastly,
we evaluated the proposed framework with the comparison to
two other methods, AO and OC, and examined its accuracy
and the ability of error tolerance compared with SOCP-based
DQN scheme. Simulation results showed that the proposed
GBDT-based DQN can achieve a much better performance
in terms of power saving than other baseline solutions under

24681012141618204045505560Instant System Power (W)AODQN2468101214161820Time Slot (100ms per slot)4045505560Instant System Power (W)OCDQN24681012141618204045505560Instant System Power (W)AODQN2468101214161820Time Slot (100ms per slot)4045505560Instant System Power (W)OCDQN010002000300040005000Time Slot (100ms per slot)40.042.545.047.550.052.555.057.560.0Average System Power Consumption (Watts)AODQN (GBDT)DQN (SOCP)010002000300040005000Time Slot (100ms per slot)40.042.545.047.550.052.555.057.560.0Average System Power Consumption (Watts)OCDQN (GBDT)DQN (SOCP)10

Algorithm 2 Training process of GBDT

Initialization
(1) Set the iteration counter m = 0. Initialize the additive
predictor (cid:98)f [0] with a starting value, e.g. (cid:98)f [0] := (0)i=1,··· ,n.
Specify a set of base-learners h1(x1), · · · , hp(xp)
Fit the negative gradient
(2) Set m := m + 1
(3) Compute the negative gradient vector u of the loss

function at the previous iteration:

u|m| = (u|m|

i

)i=1,··· ,n = (−

∂
∂f

ρ(yi, f )|f = (cid:98)f |m−1|(·))i=1,··· ,n

(4) Fit the negative gradient vector u|m| separately to every

base-learner:

u|m| base−learner

−−−−−−−−−→ (cid:98)h[m]

j

(xj) for j = 1, · · · , p

(a) AO VS. DQN

Update one component
(5) Select the component j∗ that best ﬁts the negative

gradient vector:

j∗ = arg min
1≤j≤p

n
(cid:88)

(u|m|

i − (cid:98)h|m|

j

(xj))2

i=1

(6) Update the additive predictor (cid:98)f with the component
(cid:98)f [m](·) = (cid:98)f [m−1](·) + sl · (cid:98)h|m|

j∗ (xj∗ )

where sl is a small step length (0 < sl (cid:28) 1 and a
typical value in practice is 0.1.

Iteration
Iterate steps (2) to (6) until m = mstop

Fig. 7. Average power consumption versus different user demands.

(b) OC VS. DQN

the real-time setting. Future work is in progress to let GBDT
approximator meet the strict constraints of practical problems,
which is expected to be employed in a wide range of scenarios.

APPENDIX
TRAINING AND PREDICTING PROCESS OF GBDT

The training process of GBDT is shown in Algorithm 2.
The GBDT is consisted of two concepts, where one is called
the gradient and the other is boosting. In training process, the
0-th tree is ﬁtted to the given training dataset, and it predicts
the mean value of ytrue in the training set regardless of what
the input is; the predicted values of 0-th tree are denoted as
ypredicted0. However, the predictions ypredicted0 from the 0-th tree
still have residuals between true values ytrue. Then, another
additive tree is applied to ﬁt to the new dataset that the inputs
are same as the 0-th tree, but the ﬁtting target y’s are the
residuals (ytrue −ypredicted0
). Then, the predictions of the GBDT
are the linear combination of the predictions from the 0-th tree
+ γ1 ∗
and the new additive tree, namely ypredicted = ypredicted0
ypredicted1, where γ1 is the weight attributed to this tree. Next,

+

another tree is ﬁtted to the new residuals ytrue − (ypredicted0
γ1 ∗ ypredicted1

) and follow the same process as before.

From above process, one can see that the boosting concept is
to utilize the residuals between the previous ensembled results
and true values. By learning from the residual, the model can
make progress when new trees are added. The gradient part of
concept can be explained as that the whole training process is
supervised and guided by the gradient of objective function,
where it is typically expressed as 0.5∗(ytrue−ypredicted)2, whose
derivative is the pseudo-residual between ytrue and ypredicted.

REFERENCES

[1] J. Lin et al., “A survey on Internet of Things: Architecture enabling
technologies security and privacy and applications,” IEEE Internet Things
J., vol. 4, no. 5, pp. 1125-1142, Oct. 2017.

[2] A. Checko et al., “Cloud RAN for mobile networks–A technology
overview,” IEEE Commun. Surveys Tuts., vol. 17, no. 1, pp. 405–426,
Sep. 2014.

[3] Z. Xu, Y. Wang, J. Tang, J. Wang, and M. C. Gursoy, “A deep reinforce-
ment learning based framework for power-efﬁcient resource allocation in
cloud RANs,” in Proc. IEEE Int. Conf. Commun. (ICC), pp. 1–6, 2017.
[4] A. Wiesel, Y. C. Eldar, and S. Shamai, “Linear precoding via conic
optimization for ﬁxed MIMO receivers,” IEEE Trans. Signal Process.,
vol. 54, no. 1, pp. 161–176, 2006.

[5] M. Gerasimenko et al., “Cooperative radio resource management

in
heterogeneous cloud radio access networks,” IEEE Access, vol. 3, pp.
397–406, 2015.

[6] Y. Zhou et al., “Deep reinforcement learning based coded caching scheme
in fog radio access networks,” 2018 IEEE/CIC International Conference
on Communications in China (ICCC Workshops), pp. 309–313, 2018.

20.030.040.050.060.0Maximal user demands (Mbps)40.042.545.047.550.052.555.057.560.0Average system power consumption (Watts)AODQN20.030.040.050.060.0Maximal user demands (Mbps)40.042.545.047.550.052.555.057.560.0Average system power consumption (Watts)OCDQN11

[35] B. Dai and W. Yu, “Energy efﬁciency of downlink transmission strategies
for cloud radio access networks,” IEEE J. Sel. Areas Commun., vol. 34,
no. 4, pp. 1037–1050, Apr. 2016.

[36] G. Auer et al., “How much energy is needed to run a wireless network,”

IEEE Wirel. Commun., vol. 18, no. 5, pp. 40–49, 2011.

[37] S. Boyd and L. Vandenberghe, Convex optimization, Cambridge univer-

sity press, 2004.

[38] M. S. Lobo, L. Vandenberghe, S. Boyd, and H. Lebret, “Applications
of second-order cone programming,” Linear Algebra and its Applications
Journal, Vol. 284, No. 1, 1998, pp. 193–228.

[39] R. S. Sutton and A. G. Barto, Introduction to reinforcement learning,

Cambridge: MIT press, 1998.

[40] T. Chen and C. Guestrin, “Xgboost: A scalable tree boosting system,”
Proceedings of the 22nd acm sigkdd international conference on knowl-
edge discovery and data mining. (ACM), 2016.

[41] Y. Shi, J. Zhang, and K. B. Letaief, “Group sparse beamforming for
green cloud-RAN,” IEEE Trans. Wireless Commun., vol. 13, no. 5, pp.
2809–2823, May 2014.

[42] B. Dai and W. Yu, “Energy efﬁciency of downlink transmission strategies
for cloud radio access networks,” IEEE J. Sel. Areas Commun., vol. 34,
no. 4, pp. 1037–1050, 2016.

[7] P. Rost et al., “Cloud technologies for ﬂexible 5G radio access networks,”

IEEE Commun. Mag., vol. 52, no. 5, pp. 68–76, 2014.

[8] G. Sun et al., “Dynamic reservation and deep reinforcement learning
based autonomous resource slicing for virtualized radio access networks,”
in IEEE Access, vol. 7, pp. 45758–45772, 2019.

[9] V. Franois-Lavet et al. “An introduction to deep reinforcement learning.”
Foundations and Trends in Machine Learning, vol. 11, no. 3–4, pp. 219–
354, 2018.

[10] H. He et al., “Model-driven deep learning for physical layer communi-

cations,” arXiv preprint arXiv:l809.06059, 2019.

[11] H. Zhu et al., “Caching transient data for Internet of Things: A deep
reinforcement learning approach,” IEEE Internet Things J., vol. 6, no. 2,
pp. 2074–2083, Apr. 2019.

[12] H. Zhu, Y. Cao, W. Wang, T. Jiang, and S. Jin, “Deep reinforcement
learning for mobile edge caching: Review new features and open issues,”
IEEE Netw., vol. 32, no. 6, pp. 50–57, Nov. 2018.

[13] D. Liu et al., “User association in 5G networks: A survey and an
outlook,” IEEE Commun. Surveys Tuts., vol. 18, no. 2, pp. 1018–1044,
2nd Quart. 2015.

[14] A. Domahidi, E. Chu, and S. Boyd, “Ecos: An socp solver for embedded
systems,” Control Conference (ECC) 2013 European, pp. 3071–3076,
2013.

[15] E. Andersen and K. Andersen, “The MOSEK interior point optimizerfor
linear programming: an implementation of the homogeneousalgorithm,”
High Performance Optimization, vol. 33, pp. 197–232, 2000.

[16] J. F. Sturm, “Using SeDuMi 1.02, a Matlab toolbox for optimization
over symmetric cones,” Optimization Methods and Software, vol. 11, no.
1–4, pp. 625–653, 1999.

[17] K. Gregor and Y. LeCun, “Learning fast approximations of sparse cod-
ing,” Proceedings of the 27th International Conference on International
Conference on Machine Learning. Omnipress, pp. 399–406, 2010.
[18] J. R. Hershey, J. Le Roux, and F. Weninger, “Deep unfolding:
Model–based inspiration of novel deep architectures,” arXiv preprint
arXiv:1409.2574, 2014.

[19] C. Lu, W. Xu, S. Jin, and K. Wang, “Bit-level optimized neural network
for multi-antenna channel quantization,” IEEE Commun. Lett. (Early
Access), pp. 1–1, Sep. 2019.

[20] C. Lu, W. Xu, H. Shen, J. Zhu, and K. Wang “MIMO channel
information feedback using deep recurrent network,” IEEE Commun.
Lett., vol. 23, no. 1, pp. 188–191, Jan. 2019.

[21] Z. H. Zhou and J. Feng, “Deep forest: Towards an alternative to deep

neural networks,” arXiv preprint arXiv:1702.08835, 2017.

[22] H. Sun et al., “Learning to optimize: Training deep neural networks for
interference management,” IEEE Trans. Signal Process., vol. 66, no. 20,
pp. 5438–5453, Oct 2018.

[23] J. H. Friedman, “Greedy function approximation: a gradient boosting

machine,” Annals of statistics, pp. 1189–1232, 2001.

[24] L. Breiman, “Bias, variance, and arcing classiﬁers,” Tech. Rep. 460,
Statistics Department, University of California, Berkeley, CA, USA, 1996.
[25] Z. H. Zhou, “Ensemble methods: foundations and algorithms,” Chapman

and Hall/CRC, 2012.

[26] D. Opitz and R. Maclin, “Popular ensemble methods: An empirical
study,” Journal of Artiﬁcial Intelligence Research, pp. 169–198, 1999.
[27] R. Polikar, “Ensemble based systems in decision making,” IEEE Circuits

Syst. Mag., vol. 6, no. 3, pp. 21–45, 2006.

[28] L. Rokach, “Ensemble–based classiﬁers,” Artiﬁcial Intelligence Review,

vol. 33, no. 1–2, pp. 1–39, 2010.

[29] A. Natekin and A. Knoll, “Gradient boosting machines, a tutorial,”

Frontiers in neurorobotics, vol. 7, no. 21, 2013.

[30] T. P. Do and Y. H. Kim, “Resource allocation for a full-duplex wireless-
powered communication network with imperfect self-interference cance-
lation,” IEEE Commun. Lett., vol. 20, no. 12, pp. 2482–2485, Dec. 2016.
[31] J. Miao, Z. Hu, K. Yang, C. Wang, and H. Tian, “Joint power and band-
width allocation algorithm with QoS support in heterogeneous wireless
networks,” IEEE Commun. Lett., vol. 16, no. 4, pp. 479–481, 2012.

[32] J. Moon et al., “Online reinforcement

learning of X-Haul content
delivery mode in fog radio access networks,” IEEE Signal Process. Lett.,
vol. 26, no. 10, pp. 1451–1455, 2019.

[33] I. John, A. Sreekantan, and S. Bhatnagar, “Efﬁcient adaptive resource
provisioning for cloud applications using Reinforcement Learning,” 2019
IEEE 4th International Workshops on Foundations and Applications of
Self* Systems (FAS*W), Umea, Sweden, pp. 271–272, 2019.

[34] J. Li, H. Gao, T. Lv, and Y. Lu, “Deep reinforcement learning based
computation ofﬂoading and resource allocation for MEC,” 2018 IEEE
Wireless Communications and Networking Conference (WCNC), pp. 1–6,
April 2018.

