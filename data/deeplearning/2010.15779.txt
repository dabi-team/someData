0
2
0
2

t
c
O
0
3

]

M
P
.
n
i
f
-
q
[

2
v
9
7
7
5
1
.
0
1
0
2
:
v
i
X
r
a

Discrete-time portfolio optimization under maximum drawdown
constraint with partial information and deep learning resolution

Carmine DE FRANCO∗

Johann NICOLLE†

Huyˆen PHAM ‡

November 2, 2020

Abstract

We study a discrete-time portfolio selection problem with partial information and maxi-
mum drawdown constraint. Drift uncertainty in the multidimensional framework is modeled
by a prior probability distribution. In this Bayesian framework, we derive the dynamic pro-
gramming equation using an appropriate change of measure, and obtain semi-explicit results
in the Gaussian case. The latter case, with a CRRA utility function is completely solved
numerically using recent deep learning techniques for stochastic optimal control problems.
We emphasize the informative value of the learning strategy versus the non-learning one by
providing empirical performance and sensitivity analysis with respect to the uncertainty of
the drift. Furthermore, we show numerical evidence of the close relationship between the
non-learning strategy and a no short-sale constrained Merton problem, by illustrating the
convergence of the former towards the latter as the maximum drawdown constraint vanishes.

1

Introduction

This paper is devoted to the study of a constrained allocation problem in discrete time with
partial information. We consider an investor who is willing to maximize the expected utility
of her terminal wealth over a given investment horizon. The risk-averse investor is looking for
the optimal portfolio in ﬁnancial assets under a maximum drawdown constraint. The maximum
drawdown is a common metric in ﬁnance and represents the largest drop in the portfolio value.
Our framework incorporates this constraint by setting a threshold representing the proportion
of the current maximum of the wealth process that the investor is willing to keep.

The expected rate of assets’ return (drift) is unknown, but information can be learnt by
progressive observation of the ﬁnancial asset prices. The uncertainty about the rate of return is
modeled by a probability distribution, i.e., a prior belief on the drift. To take into account the in-
formation conveyed by the prices, this prior will be updated using a Bayesian learning approach.

An extensive literature exists on parameters uncertainty and especially on ﬁltering and learn-
ing techniques in a partial information framework. To cite just a few, see Lakner (1998), Rogers
(2001), Cvitani´c et al. (2006), Karatzas and Zhao (2001), Bismuth et al. (2019), and De Franco,
Nicolle, and Pham (2019a). Some articles deal with risk constraints in a portfolio allocation
framework, see for instance the paper by Redeker and Wunderlich (2018) which tackles dynamic
risk constraints and compares the continuous and discrete time trading. Other papers especially
focus on drawdown constraints, see in particular the seminal paper by Grossman and Zhou

∗OSSIAM, E-mail: carmine.de-franco@ossiam.com
†OSSIAM and LPSM, Universit´e de Paris, E-mail: johann.nicolle@ossiam.com
‡LPSM, Universit´e de Paris, E-mail: pham@lpsm.paris

1

 
 
 
 
 
 
(1993) or Cvitani´c and Karatzas (1994). More recently, Elie and Touzi (2008) study inﬁnite-
horizon optimal consumption-investment problem in continuous-time, and Boyd et al. (2019)
use forecasts of the mean and covariance of ﬁnancial returns from a multivariate hidden Markov
model with time-varying parameters to build the optimal controls.

As it is not possible to solve analytically our constrained optimal allocation problem, we
have applied a machine learning algorithm developed in Bachouch et al. (2018a) and Bachouch
et al. (2018b). This algorithm, called Hybrid-Now, is particularly suited for solving stochastic
control problems in high dimension using deep neural networks.

Our main contributions to the literature is twofold: a detailed theoretical study of a discrete-
time portfolio selection problem including both drift uncertainty and maximum drawdown con-
straint, and a numerical resolution using a deep learning approach for an application to a model
of three risky assets, leading to a ﬁve-dimensional problem. We derive the dynamic program-
ming equation (DPE), which is in general of inﬁnite-dimensional nature, following the change
of measure suggested in Elliott et al. (2008). In the Gaussian case, the DPE is reduced to a
ﬁnite-dimensional equation by exploiting the Kalman ﬁlter. In the particular case of constant
relative risk aversion (CRRA) utility function, we reduce furthermore the dimensionality of the
problem. Then, we solve numerically the problem in the Gaussian case with CRRA utility func-
tions using the deep learning Hybrid-Now algorithm. Such numerical results allow us to provide
a detailed analysis of the performance and allocations of both the learning and non-learning
strategies benchmarked with a comparable equally-weighted strategy. Finally, we assess the per-
formance of the learning compared to the non-learning strategy with respect to the sensitivity
of the uncertainty of the drift. Additionally, we provide empirical evidence of convergence of
the non-learning strategy to the solution of the classical Merton problem when the parameter
controlling the maximum drawdown vanishes.

The paper is organized as follows: Section 2 sets up the ﬁnancial market model and the
associated optimization problem. Section 3 describes, in the general case, the change of measure
and the Bayesian ﬁltering, the derivation of the dynamic programming equation and details
some properties of the value function. Section 4 focuses on the Gaussian case. Finally, Section 5
presents the neural network techniques used, and shows the numerical results.

2 Problem setup

On a probability space (Ω, F, P) equipped with a discrete ﬁltration (Fk)k=0, ..., N satisfying the
usual conditions, we consider a ﬁnancial market model with one riskless asset assumed normal-
ized to one, and d risky assets. The price process (Si
k)k=0,...,N of asset i ∈ [[1, d]] is governed by
the dynamics

Si
k+1 = Si

keRi

k+1,

k = 0, . . . , N − 1,

(1)

where Rk+1 = (R1
and modeled as:

k+1, . . . , RN

k+1) is the vector of the assets log-return between time k and k + 1,

Rk+1 = B + (cid:15)k+1.

(2)

The drift vector B is a d-dimensional random variable with probability distribution (prior) µ0
of known mean b0 = E[B] and ﬁnite second order moment. Note that the case of known drift
B means that µ0 is a Dirac distribution. The noise (cid:15) = ((cid:15)k)k is a sequence of centered i.i.d.
random vector variables with covariance matrix Γ = E[(cid:15)k(cid:15)(cid:48)
k], and assumed to be independent of
B. We also assume the fundamental assumption that the probability distribution ν of (cid:15)k admits
a strictly positive density function g on Rd with respect to the Lebesgue measure.

2

(3)

i = eRi

k+1 for i ∈

The price process S is observable, and notice by relation (1) that R can be deduced from S,
and vice-versa. We will then denote by Fo = {F o
k }k=0, ..., N the observation ﬁltration generated
by the process S (hence equivalently by R) augmented by the null sets of F, with the convention
that for k = 0, F o

0 is the trivial algebra.

An investment strategy is an Fo-progressively measurable process α = (αk)k=0, ..., N −1, valued
in Rd, and representing the proportion of the current wealth invested in each of the d risky assets
at each time k = 0, . . . , N − 1. Given an investment strategy α and an initial wealth x0 > 0,
the (self-ﬁnanced) wealth process X α evolves according to

(cid:40)

X α

k+1 = X α
k
X α
0 = x0.

(cid:0)1 + α(cid:48)
k

(cid:0)eRk+1 − 1d

(cid:1)(cid:1) ,

k = 0, . . . , N − 1,

where eRk+1 is the d-dimensional random variable with components (cid:2)eRk+1(cid:3)
[[1, d]], and 1d is the vector in Rd with all components equal to 1.

Let us introduce the process Zα

k , as the maximum up to time k of the wealth process X α,

i.e.,

Zα
k

:= max
0≤(cid:96)≤k

X α
(cid:96) ,

k = 0, . . . , N.

The maximum drawdown constraints the wealth X α
current historical maximum Zα
as the set of investment strategies α such that

k to remain above a fraction q ∈ (0, 1) of the
k . We then deﬁne the set of admissible investment strategies Aq
0

X α

k ≥ qZα
k ,

a.s.,

k = 0, . . . , N.

In this framework, the portfolio selection problem is formulated as

V0 := sup
α∈Aq
0

E [U (X α

N )] ,

(4)

where U is a utility function on (0, ∞) satisfying the standard Inada conditions: continuously
diﬀerentiable, strictly increasing, concave on (0, ∞) with U (cid:48)(0) = ∞ and U (cid:48)(∞) = 0.

3 Dynamic programming system

In this section, we show how Problem (4) can be characterized from dynamic programming in
terms of a backward system of equations amenable for algorithms. In a ﬁrst step, we will update
the prior on the drift uncertainty, and take advantage of the newest available information by
adopting a Bayesian ﬁltering approach. This relies on a suitable change of probability measure.

3.1 Change of measure and Bayesian ﬁltering

We start by introducing a change of measure under which R1,..., RN are mutually independent,
identically distributed random variables and independent from the drift B, hence behaving like
a noise. Following the methodology detailed in (Elliott et al., 2008) we deﬁne the σ-algebras

G0
k := σ(B, R1, . . . , Rk),

k = 0, . . . , N,

and G = (Gk)k the corresponding complete ﬁltration. We then deﬁne a new probability measure
P on (Ω, (cid:87)N

k=1 Gk) by

dP
dP

(cid:12)
(cid:12)
(cid:12)
(cid:12)Gk

:= Λk,

k = 0, . . . , N,

3

with

Λk

:=

k
(cid:89)

(cid:96)=1

g(R(cid:96))
g((cid:15)(cid:96))

,

k = 1, . . . , N, Λ0 = 1.

The existence of P comes from the Kolmogorov’s theorem since Λk is a strictly positive martingale
with expectation equal to one. Indeed, for all k = 1, ..., N ,

• Λk > 0 since the probability density function g is strictly positive
• Λk is Gk-adapted,
• As (cid:15)k ⊥⊥ Gk−1, we have

E[Λk|Gk−1] = Λk−1E

(cid:105)

(cid:12)
(cid:12)Gk−1

(cid:104) g(B + (cid:15)k)
g((cid:15)k)
g(B + e)
g(e)

Rd

(cid:90)

= Λk−1

g(e)de = Λk−1

(cid:90)

Rd

g(z)dz = Λk−1.

Proposition 3.1. Under P, (Rk)k=1,...,N , is a sequence of i.i.d. random variables, independent
from B, having the same probability distribution ν as (cid:15)k.

Proof. See Appendix 6.1.

(cid:50)
Conversely, we recover the initial measure P under which ((cid:15)k)k=1,...,N is a sequence of in-
dependent and identically distributed random variables having probability density function g
where (cid:15)k = Rk − B. Denoting by Λk the Radon-Nikodym derivative dP/dP restricted to the
σ-algebra Gk:

we have

dP
dP

(cid:12)
(cid:12)
(cid:12)
(cid:12)Gk

= Λk,

Λk =

k
(cid:89)

i=1

g(Ri − B)
g(Ri)

.

It is clear that, under P, the return and wealth processes have the form stated in equations (2)
and (3). Moreover, from Bayes formula, the posterior distribution of the drift, i.e. the conditional
law of B given the asset price observation, is

µk(db) := P(cid:2)B ∈ db|F o

k

(cid:3) =

πk(db)
πk(Rd)

,

k = 0, . . . , N,

(5)

where πk is the so-called unnormalized conditional law

πk(db) := E(cid:2)Λk1{B∈db}|F o

k

(cid:3),

k = 0, . . . , N.

We then have the key recurrence linear relation on the unnormalized conditional law.

Proposition 3.2. We have the recursive linear relation

π(cid:96) = ¯g(R(cid:96) − ·)π(cid:96)−1,

(cid:96) = 1, . . . , N,

(6)

with initial condition π0 = µ0, where

¯g(R(cid:96) − b) =

g(R(cid:96) − b)
g(R(cid:96))

,

b ∈ Rd,

and we recall that g is the probability density function of the identically distributed (cid:15)k under P.
(cid:50)
Proof. See Appendix 6.2 .

4

3.2 The static set of admissible controls

In this subsection, we derive some useful characteristics of the space of controls which will turn
out to be crucial in the derivation of the dynamic programming system.

Given time k ∈ [[0, N ]], a current wealth x = X α

k > 0, and current maximum wealth z =
k ≥ x that satisﬁes the drawdown constraint qz ≤ x at time k for an admissible investment
k(x, z) ⊂ Rd the set of static controls a = αk such that the
k+1 ≥ qZα
k+1. From the relation (3),

Zα
strategy α ∈ Aq
drawdown constraint is satisﬁed at next time k + 1, i.e. X α
and noting that Zα

0, we denote by Aq

k+1 = max[Zα

k , X α

k+1], this yields

Aq

k(x, z) =

(cid:110)
a ∈ Rd : 1 + a(cid:48)(cid:0)eRk+1 − 1d

(cid:1) ≥ q max

(cid:104) z
x

, 1 + a(cid:48)(cid:0)eRk+1 − 1d

(cid:1)(cid:105)

(cid:111)

a.s.

.

(7)

Recalling from Proposition 3.1, that the random variables R1, ..., RN are i.i.d. under P, we notice
that the set Aq
k(x, z) does not depend on the current time k, and we will drop the subscript k
in the sequel, and simply denote by Aq(x, z).

Remembering that the support of ν, the probability distribution of (cid:15)k, is Rd, the following

lemma characterizes more precisely the set Aq(x, z).
Lemma 3.3. For any (x, z) ∈ S q := (cid:8)(x, z) ∈ (0, ∞)2 : qz ≤ x ≤ z(cid:9), we have

Aq(x, z) =

(cid:110)

a ∈ Rd

+ : |a|1 ≤ 1 − q

(cid:111)
,

z
x

where |a|1 = (cid:80)d

i=1 |ai| for a = (a1, . . . , ad) ∈ Rd
+.

Proof. See Appendix 6.3.

Let us prove some properties on the admissible set Aq(x, z).

Lemma 3.4. For any (x, z) ∈ S q, the set Aq(x, z) satisﬁes the following properties:

1. It is decreasing in q: ∀q1 ≤ q2, Aq2(x, z) ⊆ Aq1(x, z),

2. It is continuous in q,

3. It is increasing in x: ∀x1 ≤ x2, Aq(x1, z) ⊆ Aq(x2, z),

4. It is a convex set,

5. It is homogeneous: a ∈ Aq(x, z) ⇔ a ∈ Aq(λx, λz), for any λ > 0.

Proof. See Appendix 6.4.

(cid:50)

(cid:50)

3.3 Derivation of the dynamic programming equation

The change of probability detailed in Subsection 3.1 allows us to turn the initial partial infor-
mation Problem (4) into a full observation problem as

V0 := sup
α∈Aq
0

E[U (X α

N )] = sup
α∈Aq
0

= sup
α∈Aq
0

= sup
α∈Aq
0

5

E[ΛN U (X α

N )]

(cid:104)
E

E(cid:2)ΛN U (X α

N )(cid:12)

(cid:12)F o
N

(cid:3)(cid:105)

(cid:104)
E

U (X α

(cid:105)
N )πN (Rd)

,

(8)

from Bayes formula, the law of conditional expectations, and the deﬁnition of the unnormal-
ized ﬁlter πN valued in M+, the set of nonnegative measures on Rd. In view of Equation (3),
Proposition 3.1, and Proposition 3.2, we then introduce the dynamic value function associated
to Problem (8) as

vk(x, z, µ) =

with

sup

α∈Aq

k(x,z)

Jk(x, z, µ, α),

k ∈ [[0, N ]], (x, z) ∈ S q, µ ∈ M+,

(cid:104)
Jk(x, z, µ, α) = E

U (cid:0)X k,x,α
N

(cid:1)πk,µ

(cid:105)
N (Rd)

,

(cid:96)

k(x, z), and (πk,µ

)(cid:96)=k,...,N is the solution to (6) on M+, starting from πk,µ

where X k,x,α is the solution to Equation (3) on [[k, N ]], starting at X k,x,α
= x at time k, controlled
by α ∈ Aq
k = µ, so
that V0 = v0(x0, x0, µ0). Here, Aq
k(x, z) is the set of admissible investment strategies embedding
≥ qZk,x,z,α
the drawdown constraint: X k,x,α
, (cid:96) = k, . . . , N , where the maximum wealth process
(cid:96)
= max[Zk,x,z,α
Zk,x,z,α follows the dynamics: Zk,x,z,α
(cid:96)+1 ], (cid:96) = k, . . . , N − 1, starting from
Zk,x,z,α
= z at time k. The dependence of the value function upon the unnormalized ﬁlter µ
means that the probability distribution on the drift is updated at each time step from Bayesian
learning by observing assets price.

, X k,x,α

(cid:96)+1

k

k

(cid:96)

(cid:96)

The dynamic programming equation associated to (8) is then written in backward induction

as






vN (x, z, µ) = U (x)µ(Rd),
(cid:104)
E
vk(x, z, µ) =

sup

α∈Aq

k(x,z)

vk+1

(cid:0)X k,x,α

k+1 , Zk,x,z,α

k+1

, πk,µ
k+1

(cid:1)(cid:105)

,

k = 0, . . . , N − 1.

Recalling Proposition 3.2 and Lemma 3.3, this dynamic programming system is written more
explicitly as





vN (x, z, µ) = U (x)µ(Rd),
(cid:104)
E
vk(x, z, µ) =
vk+1

(x, z) ∈ S q, µ ∈ M+,
x(cid:0)1 + a(cid:48)(cid:0)eRk+1 − 1d

(cid:16)

sup
a∈Aq(x,z)

(cid:1)(cid:1),

max (cid:2)z, x(cid:0)1 + a(cid:48)(cid:0)eRk+1 − 1d

(cid:1)(cid:1)(cid:3), ¯g(Rk+1 − ·)µ

(9)

(cid:17)(cid:105)

,

for k = 0, . . . , N − 1. Notice from Proposition 3.1 that the expectation in the above formula
is only taken with respect to the noise Rk+1, which is distributed under P according to the
probability distribution ν with density g on Rd.

3.4 Special case: CRRA utility function

In the case where the utility function is of CRRA (Constant Relative Risk Aversion) type, i.e.,

U (x) =

xp
p

,

x > 0,

for some 0 < p < 1,

(10)

one can reduce the dimensionality of the problem. For this purpose, we introduce the process ρ
= (ρk)k deﬁned as the ratio of the wealth over its maximum up to current as:

ρα
k =

X α
k
Zα
k

,

k = 0, . . . , N.

6

This ratio process lies in the interval [q, 1] due to the maximum drawdown constraint. Moreover,
1
recalling (3), and observing that Zα
max[z,x] =
min[ 1

x ], we notice that the ratio process ρ can be written in inductive form as

k+1], together with the fact that

k+1 = max[Zα

k , X α

z , 1

ρα
k+1 = min

(cid:104)
1, ρα
k

(cid:0)1 + α(cid:48)
k

(cid:0)eRk+1 − 1d

(cid:1)(cid:1)(cid:105)
,

k = 0, . . . , N − 1.

The following result states that the value function inherits the homogeneity property of the

utility function.

Lemma 3.5. For a utility function U as in (10), we have for all (x, z) ∈ S q, µ ∈ M+, k ∈
[[0, N ]],

vk(λx, λz, µ) = λpvk(x, z, µ),

λ > 0.

Proof. See Appendix 6.5.

(cid:50)
In view of the above Lemma, we consider the sequence of functions wk, k ∈ [[0, N ]], deﬁned

by

wk(r, µ) = vk(r, 1, µ),

r ∈ [q, 1], µ ∈ M+,

so that vk(x, z, µ) = zpwk( x
z , µ), and we call wk the reduced value function. From the dynamic
programming system satisﬁed by vk, we immediately obtain the backward system for (wk)k as






wN (r, µ) = rp
wk(r, µ) =

p µ(Rd),
(cid:104)
E
sup
a∈Aq(r)

for k = 0, . . . , N − 1, where

r ∈ [q, 1], µ ∈ M+,

wk+1

(cid:0) min (cid:2)1, r(cid:0)1 + a(cid:48)(cid:0)eRk+1 − 1d

(cid:1)(cid:1)(cid:3), ¯g(Rk+1 − ·)µ(cid:1)(cid:105)

,

(11)

Aq(r) =

(cid:110)

a ∈ Rd

+ : a(cid:48)1d ≤ 1 −

(cid:111)
.

q
r

We end this section by stating some properties on the reduced value function.

Lemma 3.6. For any k ∈ [[0, N ]], the reduced value function wk is nondecreasing and concave
in r ∈ [q, 1].

Proof. See proof in Appendix 6.6.

4 The Gaussian case

(cid:50)

We consider in this section the Gaussian framework where the noise and the prior belief on
the drift are modeled according to a Gaussian distribution. In this special case, the Bayesian
ﬁltering is simpliﬁed into the Kalman ﬁltering, and the dynamic programming system is reduced
to a ﬁnite-dimensional problem that will be solved numerically. It is convenient to deal directly
with the posterior distribution of the drift, i.e. the conditional law of the drift B given the assets
price observation, also called normalized ﬁlter. From (5) and Proposition 3.2, it is given by the
inductive relation

µk(db) =

g(Rk − b)µk−1(db)
(cid:82)
Rd g(Rk − b)µk−1(db)

,

k = 1, . . . , N.

(12)

7

4.1 Bayesian Kalman ﬁltering

We assume that the probability law ν of the noise (cid:15)k is Gaussian: N (0, Γ), and so with density
function

g(r) = (2π)− d

2 |Γ|− 1

2 e− 1

2 r(cid:48)Γ−1r,

r ∈ Rd.

(13)

Assuming also that the prior distribution µ0 on the drift B is Gaussian with mean b0, and
invertible covariance matrix Σ0, we deduce by induction from (12) that the posterior distribution
µk is also Gaussian: µk ∼ N ( ˆBk, Σk), where ˆBk = E[B|F o
k ] and Σk satisfy the well-known
inductive relations:

ˆBk+1 = ˆBk + Kk+1(Rk+1 − ˆBk),
Σk+1 = Σk − Σk(Σk + Γ)−1Σk,

k = 0, . . . , N − 1

(14)

(15)

where Kk+1 is the so-called Kalman gain given by
Kk+1 = Σk(Σk + Γ)−1,

k = 0, . . . , N − 1.

(16)
We have the initialization ˆB0 = b0, and the notation for Σk is coherent at time k = 0 as it
corresponds to the covariance matrice of B. While the Bayesian estimation ˆBk of B is updated
from the current observation of the log-return Rk, notice that Σk (as well as Kk) is deterministic,
and is then equal to the covariance matrix of the error between B and its Bayesian estimation,
i.e. Σk = E[(B − ˆBk)(B − ˆBk)(cid:48)]. Actually, we can explicitly compute Σk by noting from Equation
(12) with g as in (13) and µ0 ∼ N (b0, Σ0) that

(cid:16)
b−(Σ−1

0 +Γ−1k)−1(Γ−1 (cid:80)k

j=1 Rj +Σ−1

0 b0)(cid:17)(Σ−1

0 +Γ−1k)(cid:16)

b−(Σ−1

0 +Γ−1k)−1(Γ−1 (cid:80)k

j=1 Rj +Σ−1

0 b0)(cid:17)

− 1
e
2

µk ∼

(2π)

d

2 |(Σ−1

0 + Γ−1k)−1|

1
2

By identiﬁcation, we then get

Σk = (Σ−1

0 + Γ−1k)−1 = Σ0(Γ + Σ0k)−1Γ.

.

(17)

Moreover, the innovation process (˜(cid:15)k)k, deﬁned as
˜(cid:15)k+1 = Rk+1 − E[Rk+1|F o

k ] = Rk+1 − ˆBk,

k = 0, . . . , N − 1,

(18)

is a Fo-adapted Gaussian process. Each ˜(cid:15)k+1 is independent of F 0
mutually independent), and is a centered Gaussian vector with covariance matrix:

k (hence ˜(cid:15)k, k = 1, . . . , N are

˜(cid:15)k+1 ∼ N (cid:0)0, ˜Γk+1

(cid:1),

with ˜Γk+1 = Σk + Γ.

We refer to Kalman (1960) and Kalman and Bucy (1961) for these classical properties about
the Kalman ﬁltering and the innovation process.
Remark 4.1. From (14), and (18), we see that the Bayesian estimator ˆBk follows the dynamics

(cid:40) ˆBk+1 = ˆBk + Kk+1˜(cid:15)k+1,

k = 0, . . . , N − 1

ˆB0 = b0,

which implies in particular that ˆBk has a Gaussian distribution with mean b0, and covariance
matrix satisfying

Var( ˆBk+1) = Var( ˆBk) + Kk+1(Σk + Γ)K(cid:48)

k+1 = Var( ˆBk) + Σk(Σk + Γ)−1Σk.

Recalling the inductive relation (15) on Σk, this shows that Var( ˆBk) = Σ0 − Σk. Note that,
from Equation (15), (Σk)k is a decreasing sequence which ensures that Var( ˆBk) is positive semi-
♦
deﬁnite and is nondecreasing with time k.

8

4.2 Finite-dimensional dynamic programming equation

From (18), we see that our initial portfolio selection Problem (4) can be reformulated as a full
observation problem with state dynamics given by

(cid:40)

(cid:16)

(cid:0)e ˆBk+˜(cid:15)k+1 − 1d

(cid:1)(cid:17)

,

X α
k+1 = X α
k
ˆBk+1 = ˆBk + Kk+1˜(cid:15)k+1,

1 + α(cid:48)
k

k = 0, . . . , N − 1.

(19)

We then deﬁne the value function on [[0, N ]] × S q × Rd by

˜vk(x, z, b) =

sup

α∈Aq

k(x,z)

E(cid:2)U (X k,x,b,α
N

)(cid:3),

k ∈ [[0, N ]], (x, z) ∈ S q, b ∈ Rd,

where the pair (X k,x,b,α, ˆBk,b) is the process solution to (19) on [[k, N ]], starting from (x, b) at
time k, so that V0 = ˜v0(x0, x0, b0). The associated dynamic programming system satisﬁed by
the sequence (˜vk)k is


˜vN (x, z, b) = U (x),

˜vk(x, z, b) =

sup
a∈Aq(x,z)

(x, z) ∈ S q, b ∈ Rd,
(cid:104)
E
˜vk+1

(cid:16)

x(cid:0)1 + a(cid:48)(cid:0)eb+˜(cid:15)k+1 − 1d

(cid:1)(cid:1),

max (cid:2)z, x(cid:0)1 + a(cid:48)(cid:0)eb+˜(cid:15)k+1 − 1d

(cid:1)(cid:1)(cid:3), b + Kk+1˜(cid:15)k+1

(cid:17)(cid:105)

,




for k = 0, . . . , N − 1. Notice that in the above formula, the expectation is taken with respect to
the innovation vector ˜(cid:15)k+1, which is distributed according to N (0, ˜Γk+1).

Moreover, in the case of CRRA utility functions U (x) = xp/p, and similarly as in Section

3.4, we have the dimension reduction with

˜wk(r, b) = ˜vk(r, 1, b),

r ∈ [q, 1], b ∈ Rd,

z , b), and this reduced value function satisﬁes the backward system

so that ˜vk(x, z, b) = zp ˜wk( x
on [q, 1] × Rd:



˜wN (r, b) = rp
p ,
sup
˜wk(r, b) =
a∈Aq(r)



r ∈ [q, 1], b ∈ Rd,
(cid:16)

(cid:104)
E
˜wk+1

min (cid:2)1, r(cid:0)1 + a(cid:48)(cid:0)eb+˜(cid:15)k+1 − 1d

(cid:1)(cid:1)(cid:3), b + Kk+1˜(cid:15)k+1

(cid:17)(cid:105)

,

for k = 0, . . . , N − 1.

Remark 4.2 (No short-sale constrained Merton problem). In the limiting case when q = 0, the
drawdown constraint is reduced to a non-negativity constraint on the wealth process, and by
Lemma 3.3, this means a no-short selling and no borrowing constraint on the portfolio strategies.
When the drift B is also known, equal to b0, and for a CRRA utility function, let us then consider
the corresponding constrained Merton problem with value function denoted by vM
k , k = 0, . . . , N ,
which satisﬁes the standard backward recursion from dynamic programming:

, x > 0,

xp
p


vM
N (x) =

vM
k (x) = sup

a(cid:48)1d≤1
a∈[0,1]d

(cid:104)
vM
E
k+1

(cid:0)x(cid:0)1 + a(cid:48)(cid:0)eb0+(cid:15)k+1 − 1d

(cid:1)(cid:1)(cid:105)
,

k = 0, . . . , N − 1.

(20)

Searching for a solution of the form vM
that the sequence (Kk)k satisﬁes the recursive relation:

k (x) = Kkxp/p, with Kk ≥ 0 for all k ∈ [[0, N ]], we see

Kk = SKk+1,

k = 0, . . . , N − 1,

9

starting from KN = 1, where

S := sup

a(cid:48)1d≤1
a∈[0,1]d

(cid:104)(cid:16)

E

1 + a(cid:48)(cid:0)eb0+(cid:15)1 − 1d

(cid:1)(cid:17)p(cid:105)
,

by recalling that (cid:15)1, . . . , (cid:15)N are i.i.d. random variables. It follows that the value function of the
constrained Merton problem, unique solution to the dynamic programming system (20), is equal
to

k (x) = SN −k xp
vM

p

,

k = 0, . . . , N,

and the constant optimal control is given by

aM
k = argmax
a(cid:48)1≤1
a∈[0,1]d

E

(cid:104)(cid:0)1 + a(cid:48) (cid:0)eR1 − 1d

(cid:1)(cid:1)p(cid:105)

k = 0, . . . , N − 1.

♦

5 Deep learning numerical resolution

In this section, we exhibit numerical results to promote the beneﬁts of learning from new in-
formation. To this end, we compare the learning strategy (Learning) to the non-learning one
(Non-Learning) in the case of the CRRA utility function and the Gaussian distribution for the
noise. The prior probability distribution of B is the Gaussian distribution N (b0, Σ0) for Learning
while it is the Dirac distribution concentrated at b0 for Non-Learning.

We use deep neural network techniques to compute numerically the optimal solutions for
both Learning and Non-Learning. To broaden the analysis, in addition to the learning and
non-learning strategies, we have computed an ”admissible” equally weighted (EW) strategy.
More precisely, this EW strategy will share the quantity Xk − qZk equally among the d assets.
Eventually, we show numerical evidence that the Non-Learning converges to the optimal strategy
of the constrained Merton problem, when the loss aversion parameter q vanishes.

5.1 Architectures of the deep neural networks

Neural networks (NN) are able to approximate nonlinear continuous functions, typically the
value function and controls of our problem. The principle is to use a large amount of data to
train the NN so that it progressively comes close to the target function. It is an iterative process
in which the NN is tuned on a training set, then tested on a validation set to avoid over-ﬁtting.
For more details, see for instance Hornik (1991) and G´eron (2019).

The algorithm we use, relies on two dense neural networks: the ﬁrst one is dedicated to the
controls (AN N ) and the second one to the value function (V FN N ). Each NN is composed of four
layers: an input layer, two hidden layers and an output layer:

(i) The input layer is d + 1-dimensional since it embeds the conditional expectations of each
of the d assets and the ratio of the current wealth to the current historical maximum ρ.

(ii) The two hidden layers give the NN the ﬂexibility to adjust its weights and biases to
approximate the solution. From numerical experiments, we see that, given the complexity
of our problem, a ﬁrst hidden layer with d + 20 neurons and a second one with d + 10 are
a good compromise between speed and accuracy.

10

Parameter

Initializer
Regularizers
Activation functions
Optimizer
Learning rates: step N-1

steps k = 0,...,N-2

Scale
Number of elements in a training batch
Number of training batches
Size of the validation batches
Penalty constant
Number of epochs: step N-1

steps k = 0,...,N-2
Size of the training set: step N-1

steps k = 0,...,N-2
Size of the validation set: step N-1

steps k = 0,..., N-2

ANN

uniform(0, 1)
L2 norm

VFNN

He uniform
L2 norm

Elu and Sigmoid for output layer Elu and Sigmoid for output layer

Adam
5e-3
6.25e-4
1e-3
3e2
1e2
1e3
3e-1
2e3
5e2
6e7
1.5e7
2e6
5e5

Adam
1e-3
5e-4
1e-3
3e2
1e2
1e3
NA
2e3
5e2
6e7
1.5e7
2e6
5e5

Table 1: Parameters for the neural networks of the controls ANN and the value function VFNN.

(iii) The output layer is d-dimensional for the controls, one for each asset representing the
weight of the instrument, and is one-dimensional for the value function. See Figures 1 and
2 for an overview of the NN architectures in the case of d = 3 assets.

Figure 1: AN N architecture with d = 3 assets

Figure 2: V FN N architecture with d = 3 assets

We follow the indications in G´eron (2019) to setup and deﬁne the values of the various inputs

of the neural networks which are listed in Table 1.

To train the NN, we simulate the input data. For the conditional expectation ˆBk, we use
its time-dependent Gaussian distribution (see Remark 4.1): ˆBk ∼ N (b0, Σ0 − Σk), with Σk as
in Equation (17). On the other hand, the training of ρ is drawn from the uniform distribution
between q and 1, the interval where it lies according to the maximum drawdown constraint.

5.2 Hybrid-Now algorithm

We use the Hybrid-Now algorithm developped in Bachouch et al. (2018b) in order to solve
numerically our problem. This algorithm combines optimal policy estimation by neural networks
and dynamic programming principle which suits the approach we have developped in Section 4.

11

With the same notations as in Algorithm 1 detailed in the next insert, at time k, the algo-
rithm computes the proxy of the optimal control ˆαk with AN N , using the known function ˆVk+1
calculated the step before, and uses VN N to obtain a proxy of the value function ˆVk. Starting
from the known function ˆVN := U at terminal time N , the algorithm computes sequentially ˆαk
and ˆVk with backward iteration until time 0. This way, the algorithm loops to build the optimal
controls and the value function pointwise and gives as output the optimal strategy, namely the
optimal controls from 0 to N − 1 and the value function at each of the N time steps.

The maximum drawdown constraint is a time-dependent constraint on the maximal propor-
tion of wealth to invest (recall Lemma 3.3). In practice, it is a constraint on the sum of weights
of each asset or equivalently on the output of AN N . For that reason, we have implemented an
appropriate penalty function that will reject undesirable values:

GP enalty(A, r) = Kmax

(cid:16)

|A|1 ≤ 1 −

(cid:17)

, 0

q
r

, A ∈ [0, 1]d, r ∈ [q, 1].

This penalty function ensures that the strategy respects the maximum drawdown constraint at
each time step, when the parameter K is chosen suﬃciently large.

Algorithm 1: Hybrid-Now

Input: the training distributions µU nif and µk

Gauss;

(cid:46) µU nif = U(q, 1)
Gauss = N (b0, Σ0 − Σk)

(cid:46) µk

Output:
- estimate of the optimal strategy (ˆak)N −1
k=0 ;
(cid:17)N −1
- estimate of the value function
;
Set ˆVN = U ;
for k = N − 1, . . . , 0 do

(cid:16) ˆVk

k=0

Compute:

ˆβk ∈

argmin
β∈R2d2+56d+283

(cid:104)

E

GP enalty(AN N (ρk, ˆBk; β), ρk) − ˆVk+1

(cid:16)

ρβ
k+1, ˆBk+1

(cid:17)(cid:105)

where ρk ∼ µU nif , ˆBk ∼ µk
ˆBk+1 = ˜Hk( ˆBk, ˜(cid:15)k+1) and ρβ

Gauss,
k+1 = F

(cid:16)

ρk, ˆBk, AN N

(cid:16)

(cid:46) F (ρ, b, a, (cid:15)) = min

ρk, ˆBk; β
(cid:16)

(cid:17)

(cid:17)

, ˜(cid:15)k+1
(cid:16)
1 + (cid:80)d

;
i=1 ai (cid:16)
1, ρ
(cid:46) ˜Hk(b, (cid:15)) = b + Σ0(Γ + Σ0k)−1(cid:15)

ebi+(cid:15)i

(cid:17)(cid:17)(cid:17)

− 1

Set ˆak = AN N

(cid:16)

.; ˆβk

(cid:17)

;

Compute:

ˆθk ∈

argmin
θ∈R2d2+54d+261

Set ˆVk = V FN N

(cid:16)

., ˆθk

(cid:17)

;

(cid:46) ˆak is the estimate of the optimal control at time k.

(cid:20)(cid:16) ˆVk+1

E

(cid:16)

ˆβk
k+1, ˆBk+1
ρ

(cid:17)

− V FN N

(cid:16)

ρk, ˆBk; θ

(cid:17)(cid:17)2(cid:21)

(cid:46) ˆVk is the estimate of the value function at time k.

A major argument behind the choice of this algorithm is that, it is particularly relevant for
problems in which the neural network approximation of the controls and value function at time
k, are close to the ones at time k + 1. This is what we expect in our case. We can then take
a small learning rate for the Adam optimizer which enforces the stability of the parameters’
update during the gradient-descent based learning procedure.

12

Parameter

Value

Number of risky assets d
Investment horizon in years T
Number of steps/rebalancing N
Number of simulations/trajectories ˜N
Degree of the CRRA utility function p
Parameter of risk aversion q
Annualized expectation of the drift B

Annualized covariance matrix of the drift B

Annualized volatility of (cid:15)

Correlation matrix of (cid:15)

Annualized covariance matrix of the noise (cid:15)

3
1
24
1000
0.8
0.7
(cid:2)0.05 0.025 0.12(cid:3)


0.22
0
0
0.152
0
0
0.12
0
0
(cid:2)0.08 0.04 0.22(cid:3)
0.2
−0.1
1
−0.25
1
−0.1
1
0.2 −0.25
0.00352
0.0064 −0.00032
0.0016 −0.0022
0.0484

−0.00032
0.00352 −0.0022





















Table 2: Values of the parameters used in the simulation.

5.3 Numerical results

In this section, we explain the setup of the simulation and exhibit the main results. We have
used Tensorﬂow 2 and deep learning techniques for Python developped in G´eron (2019). We
consider d = 3 risky assets and a riskless asset whose return is assumed to be 0, on a 1-year
investment horizon for the sake of simplicity. We consider 24 portfolio rebalancing during the
1-year period, i.e., one every two weeks. This means that we have N = 24 steps in the training
of our neural networks. The parameters used in the simulation are detailed in Table 2.

First, we show the numerical results for the learning and the non-learning strategies by
presenting a performance and an allocation analysis in Subsection 5.3.1. Then, we add the
admissible constrained EW to the two previous ones and use this neutral strategy as a benchmark
in Subsection 5.3.2. Ultimately, in Subsection 5.3.3, we illustrate numerically the convergence of
the non-learning strategy to the constrained Merton problem when the loss aversion parameter
q vanishes.

5.3.1 Learning and non-learning strategies

We simulate ˜N = 1000 trajectories for each strategy and exhibit the performance results with
an initial wealth x0 = 1. Figures 3 illustrates the average historical level of the learning and
non-learning strategies with a 95% conﬁdence interval. Learning outperforms signiﬁcantly Non-
Learning with a narrower conﬁdence interval revealing that less uncertainty surrounds Learning
performance, thus yielding less risk.

An interesting phenomenon, visible in Fig. 3, is the nearly ﬂat curve for Learning between
time 0 and time 1. Indeed, whereas Non-Learning starts investing immediately, Learning adopts
a safer approach and needs a ﬁrst time step before allocating a signiﬁcant proportion of wealth.
Given the level of uncertainty surrounding b0, this ﬁrst step allows Learning to ﬁne-tune its
allocation by updating the prior belief with the ﬁrst return available at time 1. On the contrary,
Non-Learning, which cannot update its prior, starts investing at time 0.

13

Fig. 4 shows the ratio of Learning over Non-Learning. A ratio greater than one means
that Learning outperforms Non-Learning and underperforms when less than one. It shows the
signiﬁcant outperformance of Learning over Non-Learning except during the ﬁrst period where
Learning was not signiﬁcantly invested and Non-Learning had a positive return. Moreover, this
graph reveals the typical increasing concave curve of the value of information described in Keppo
et al. (2018), in the context of investment decisions and costs of data analytics, and in De Franco,
Nicolle, and Pham (2019a) in the resolution of the Markowitz portfolio selection problem using
a Bayesian learning approach.

Figure 3: Historical Learning and Non-Learning levels with

Figure 4: Historical ratio of Learning over Non-Learning lev-

a 95% conﬁdence interval.

els.

Table 3 gathers relevant statistics for both Learning and Non-Learning such as: average total
performance, standard deviation of the terminal wealth XT , Sharpe ratio computed as average
total performance over standard deviation of terminal wealth. The maximum drawdown (MD)
is examined through two statistics: noting M D˜s
(cid:96) the maximum drawdown of the (cid:96)-th trajectory
of a strategy ˜s, the average MD is deﬁned as,

Avg MD˜s =

1
˜N

˜N
(cid:88)

(cid:96)=1

MD˜s
(cid:96),

for ˜N trajectories of the strategy ˜s, and the worst MD is deﬁned as,

Worst MD˜s = min

(cid:16)

M D˜s

1, . . . , M D˜s
˜N

(cid:17)

.

Finally, the Calmar ratio, computed as the ratio of the average total performance over the av-
erage maximum drawdown, is the last statistic exhibited.

With the simulated dataset, Learning delivered, on average, a total performance of 9.34%
while Non-Learning only 6.40%. Integrating the most recent information yielded a 2.94% ex-
cess return. Moreover, risk metrics are signiﬁcantly better for Learning than for Non-Learning.
Learning exhibits a lower standard deviation of terminal wealth than Non-Learning (11.88%
versus 16.67%), with a diﬀerence of 4.79%. More interestingly, the maximum drawdown is no-
tably better controlled by Learning than by Non-Learning, on average (−1.53% versus −6.54%)
and in the worst case (−11.74% versus −27.18%). This result suggests that learning from new
observations, helps the strategy to better handle the dual objective of maximizing total wealth
while controlling the maximum drawdown. We also note that learning improves the Sharpe ratio
by 104.95% and the Calmar ratio by 525.26%.

Fig. 5 and 6 focus more precisely on the portfolio allocation. The graphs of Fig. 5 show
the historical average allocation for each of the three risky assets. First, none of the strategies

14

Statistic

Learning Non-Learning Diﬀerence

Avg total performance
Std dev. of XT
Sharpe ratio
Avg MD
Worst MD
Calmar ratio

9.34%
11.88%
0.79
-1.53%
-11.74%
6.12

6.40%
16.67%
0.38
-6.54%
-27.18%
0.98

2.94%
-4.79%
104.95%
5.01%
15.44%
525.26%

Table 3: Performance metrics: Learning and Non-Learning. The diﬀerence for ratios are computed as relative improvement.

invests in Asset 2 since it has the lowest expected return according to the prior, see Table 2.
Whereas Non-Learning focuses on Asset 3, the one with the highest expected return, Learning
performs an optimal allocation between Asset 1 and Asset 3 since this strategy is not stuck with
the initial estimate given by the prior. Therefore, Learning invests little at time 0, then balances
nearly equally both Assets 1 and 3, and then invests only in Asset 3 after time step 12. Instead,
Non-Learning is investing only in Asset 3, from time 0 until the end of the investment horizon.

Figure 5: Historical Learning and Non-Learning asset allocations.

The curves in Fig. 6 recall each asset’s optimal weight, but the main features are the colored
areas that represent the average historical total percentage of wealth invested by each strategy.
The dotted line represents the total allocation constraint they should satisfy to be admissible. To
satisfy the maximum drawdown constraint, admissible strategies can only invest in risky assets
the proportion of wealth that, in theory, could be totally lost. This explains why the non-learning
strategy invests at full capacity on the asset that has the maximum expected return according
to the prior distribution.

We clearly see that both strategies satisfy their respective constraints. Indeed, looking at the
left panel, Learning is far from saturating the constraint. It has invested, on average, roughly
10% of its wealth while its constraint was set around 30%. Non-learning invests at full capacity
saturating its allocation constraint. Remark that this constraint is not a straight line since it
depends on the value of the ratio: current wealth over current historical maximum, and evolves
according to time.

15

Figure 6: Historical Learning and Non-Learning total allocations.

5.3.2 Learning, non-learning and constrained equally-weighted strategies

In this section, we add a simple constrained equally-weighted (EW) strategy to serve as a
benchmark for both Learning and Non-Learning. At each time step, the constrained EW strategy
invests, equally across the three assets, the proportion of wealth above the threshold q.

Fig. 7 shows the average historical levels of the three strategies: Learning, Non-Learning
and constrained EW. We notice Non-Learning outperforms constrained EW and both have
similar conﬁdence intervals. It is not surprising to see that Non-Learning outperforms constrained
EW since Non-Learning always bets on Asset 3, the most performing, while constrained EW
diversiﬁes the risks equally among the three assets.

Figure 7: Historical Learning, Non-Learning and constrained EW (Const. EW) levels with a 95% conﬁdence interval.

Fig. 8 shows the ratio of Learning over constrained EW: it depicts the same concave shape
as Fig. 4. The outperformance of Non-Learning with respect to constrained EW is plot in Fig.
9 and conﬁrms, on average, the similarity of the two strategies.

16

Statistic

Const. EW

L

NL

L - Const. EW NL - Const. EW

Avg total performance
Std dev. of XT
Sharpe ratio
Avg MD
Worst MD
Calmar ratio

3.85%
13.80%
0.28
-4.70%
-21.83%
0.82

9.34%
6.40%
11.88% 16.67%

0.79

0.38

-1.53% -6.54%
-11.74% -27.18%

6.12

0.98

5.49%
-1.92%
182.08%
3.17%
10.09%
647.56%

2.55%
2.87%
37.63%
-1.84%
-5.34%
-19.56%

Table 4: Performance metrics: Constrained EW (Const. EW) vs Learning (L) and Non-Learning (NL). The diﬀerence for

ratios are computed as relative improvement.

Figure 8: Ratio Learning over constrained EW (Const. EW)

Figure 9: Ratio Non-Learning over constrained EW (Const.

according to time.

EW) according to time.

Table 4 collects relevant statistics for the three strategies. Learning clearly surpasses con-
strained EW: it outperforms by 5.49% while reducing uncertainty on terminal wealth by 1.92%
resulting in an improvement of 182.08% of the Sharpe ratio. Moreover, it better handles maxi-
mum drawdown regarding both the average and the worst case, exhibiting an improvement of
3.17% and 10.09% respectively, enhancing the Calmar ratio by 647.56%.

The Non-Learning and the constrained EW have similar proﬁles. Even if Non-Learning out-
performs constrained EW by 2.5%, it has a higher uncertainty in terminal wealth (+2.87%).
This results in similar Sharpe ratios. Maximum drawdown, both on average and considering
the worst case are better handled by constrained EW (−4.70% and −21.83% respectively) than
by Non-Learning (−6.54% and −27.18% respectively) thanks to the diversiﬁcation capacity of
constrained EW. The better performance of Non-Learning compensates the better maximum
drawdown handling of constrained EW, entailing a better Calmar ratio for Non-Learning 0.98
versus 0.82 for constrained EW.

5.3.3 Non-learning and Merton strategies

We numerically analyze the impact of the drawdown parameter q, and compare the non-learning
strategies (assuming that the drift is equal to b0), with the constrained Merton strategy as
described in Remark 4.2. Fig. 10 conﬁrms that when the loss aversion parameter q goes to zero,
the non-learning strategy approaches the Merton strategy.

17

Figure 10: Wealth curves resulting from the Merton strategy and the non-learning strategy for diﬀerent values of q.

In terms of assets’ allocation, the Merton strategy saturates the constraint only by investing
in the asset with the highest expected return, Asset 3, while the non-learning strategy adopts a
similar approach and invests at full capacity in the same asset. To illustrate this point, we easily
see that the areas at the top and bottom-left corner converge to the area at the bottom-right
corner of Fig. 11.

Figure 11: Asset 3 average weights of the non-learning strategies with q ∈ {0.7, 0.4, 0.1} and the Merton strategy.

As q vanishes, we observe evidence of the convergence of the Merton and the non-learning
strategies, materialized by a converging allocation pattern and resulting wealth trajectories. It
should not be surprising since both have in common not to learn from incoming information
conveyed by the prices.

18

5.4 Sensitivities analysis

In this subsection, we study the eﬀect of changes in the uncertainty about the beliefs of B. These
beliefs take the form of an estimate b0 of B, and a degree of uncertainty about this estimate,
the covariance of Σ0 of B. For the sake of simplicity, we design Σ0 as a diagonal matrix whose
diagonal entries are variances representing the conﬁdence the investor has in her beliefs about
the drift. To easily model a change in Σ0, we deﬁne the modiﬁed covariance matrix ˜Σ as

˜Σunc := unc ∗ Σ0,

where unc > 0. From now on, the prior of B is N (b0, ˜Σunc).

A higher value of unc means a higher uncertainty materialized by a lower conﬁdence in the
prior estimate of the expected return of B, b0. We consider learning strategies with values of
unc ∈ {1/6, 1, 3, 6, 12}. The value unc = 1 was used for Learning in Subsection 5.3.

Equation (2) implies that the returns’ probability distribution depends upon unc. It implies
that for each value of unc, we need to compute both Learning and Non-Learning on the returns
sample drawn from the same probability law to make relevant comparisons.
Therefore, from a sample of a thousand returns paths’ draws, we plot in Fig. 12 the average
curves of the excess return of Learning over its associated Non-Learning, for diﬀerent values of
the uncertainty parameter unc.

Figure 12: Excess return of Learning over Non-Learning with a 95% conﬁdence interval for diﬀerent levels of uncertainty.

Looking at Fig. 12, we notice that when uncertainty about b0 is low, i.e. unc = 1/6, Learn-
ing is close to Non-Learning and unsurprisingly the associated excess return is small. Then, as
we increase the value of unc the curves steepen increasingly showing the eﬀect of learning in
generating excess return.

Table 5 summarises key statistics for the ten strategies computed in this section. When
unc = 1/6, Learning underperforms Non-Learning. This is explained by the fact that Non-
Learning has no doubt about b0 and knows Asset 3 is the best performing asset acoording to
its prior, whereas Learning, even with low uncertainty, needs to learn it generating a lag which
explains the underperformance on average. For values of unc ≥ 1 Learning outperforms Non-
learning increasingly, as can be seen on Fig. 13, at the cost of a growing standard deviation of
terminal wealth.

The Sharpe ratio of terminal wealth is higher for Learning than for Non-Learning for any
value of unc. Nevertheless, an interesting fact is that the ratio rises from unc = 1/6 to unc = 1,
then reaches a level close to 0.8 for values of unc = 1, 3, 6 then decreases when unc = 12.

19

Statistic

L

NL

L

NL

L

NL

L

NL

L

NL

unc = 1/6

unc = 1

unc = 3

unc = 6

unc = 12

Avg total performance
Std dev. of XT
Sharpe ratio
Avg MD
Worst MD
Calmar ratio

3.87%
5.81%
0.67

9.45%

4.35%
6.00% 19.96% 10.25% 90.03% 16.22% 130.07% 30.44%
9.22% 12.10% 17.28% 25.01% 28.18 % 113.69% 41.24% 222.77% 70.84%
0.47

0.58

0.43

0.36

0.79

0.80

0.78

0.35

0.39

-2.51% -5.21% -1.40% -6.78% -1.90% -8.40%
-2.68% -10.14% -3.58% -11.35%
-7.64% -17.88% -5.46% -24.01% -7.99% -26.68% -15.62% -29.22% -16.98% -29.47%

1.54

0.83

6.77

0.89

10.49

1.22

33.65

1.60

36.32

2.68

Table 5: Performance and risk metrics: Learning (L) vs Non-Learning (NL) for diﬀerent values of uncertainty unc.

This phenomenon is more visible on Fig. 14 that displays the Sharpe ratio of terminal wealth of
Learning and Non-Learning according to the values of unc, and the associated relative improve-
ment. Clearly, looking at Figures 13 and 14, we remark that while increasing unc gives more
excess return, too high values of unc in the model turn out to be a drag as far as Sharpe ratio
improvement is concerned.

Figure 13: Average total performance of Learning (L) and
Non-Learning (NL), and excess return, for unc ∈
{1/6, 1, 3, 6, 12}.

Figure 14: Sharpe ratio of terminal wealth of Learning (L)
and Non-Learning (NL), and relative improve-
ment, for unc ∈ {1/6, 1, 3, 6, 12}.

For any value of unc, Learning handles maximum drawdown signiﬁcantly better than Non-
Learning whatever it is the average or the worst. This results in a better performance per unit
of average maximum drawdown (Calmar ratio), for Learning. We also see that the maximum
drawdown constraint is satisﬁed for every strategies of the sample and for any value of unc since
the worst maximum drawdown is always above −30%, the lowest admissible value with a loss
aversion parameter q set at 0.7.

Figure 15: Average maximum drawdown of Learning (L) and Non-Learning (NL) and the gain from learning for unc ∈

{1/6, 1, 3, 6, 12}.

Fig. 15 reveals how the average maximum drawdown behaves regarding the level of uncertainty.
Non-Learning maximum drawdown behaves linearly with uncertainty: the wider the range of

20

possible values of B the higher the maximum drawdown is on average. It emphasizes its inabil-
ity to adapt to an environment in which the returns have diﬀerent behaviors compared to their
expectations. Learning instead, manages to keep a low maximum drawdown for any value of unc.
Given the previous remarks, it is obvious that the gain in maximum drawdown from learning
grows with the level of uncertainty.

Figures 16-20 represent portfolio allocations averaged over the simulations. They depict, for
each value of the uncertainty parameter unc, the average proportion of wealth invested, in each
of the three assets, by Learning and Non-Learning. The purpose is not to compare the graphs
with diﬀerent values of unc since the allocation is not performed on the same sample of returns.
Rather, we can identify trends that are typically diﬀerentiating Learning from Non-Learning
allocations.

Since the maximum drawdown constraint is satisﬁed by the capped sum of total weights
that can be invested, the allocations of both Learning and Non-Learning are mainly based on
the expected returns of the assets.
Non-Learning, by deﬁnition, does not depend on the value of the uncertainty parameter. Hence,
no matter the value of unc, its allocation is easy to characterize since it saturates its constraint
investing in the asset that has the best expected return according to the prior. In our setup,
Asset 3 has the highest expected return, so Non-Learning invests only in it and saturates its
constraint of roughly 30% during all the investment period. The slight change of the average
weight in Asset 3 comes from ρ, the ratio wealth over maximum wealth, changing over time.

Figure 16: Learning and Non-Learning historical assets’ allocations with unc = 1/6.

Unlike Non-Learning, depending of the value of unc, Learning can perform more sophisticated
allocations because it can adjust the weights according to the incoming information. Nonetheless,
in Fig. 16, when unc is low, Learning and Non-Learning look similar regarding their weights
allocation since both strategies invest, as of time 0, a signiﬁcant proportion of their wealth only
in Asset 3.
On the right panel of Fig. 16, the progressive increase in the weight of Asset 3 illustrates the
learning process. As time goes by, Learning progressively increases the weight in Asset 3 since it
has the highest expected return. It also explains why Learning underperforms Non-Learning for
low values of unc; contrary to Non-Learning which invests at full capacity in Asset 3, Learning
needs to learn that Asset 3 is the optimal choice.

21

Figure 17: Learning and Non-Learning historical assets’ allocations with unc = 1.

Figure 18: Learning and Non-Learning historical assets’ allocations with unc = 3.

However, as uncertainty increases, Learning and Non-Learning strategies start diﬀerentiating.
When unc ≥ 1, Learning invests little, if any, at time 0. In addition, an increase in unc allows
the inital drift to lie in a wider range and generates investment opportunities for Learning. This
explains why Learning invests in Asset 1 when unc = 1, 3, 6, 12 although the estimate b0 for
this asset is lower than for Asset 3. In Fig. 19, we see that Learning even invests in Asset 2
which has the lowest expected drift.

Figure 19: Learning and Non-Learning historical assets’ allocations with unc = 6.

22

Figure 20: Learning and Non-Learning historical assets’ allocations with unc = 12.

Figures 21-25 illustrate the historical total percentage of wealth allocated for Learning and
Non-Learning with diﬀerent levels of uncertainty. As seen previously, Non-Learning has fully
invested in Asset 3 for any value of unc.

Figure 21: Historical total allocations of Learning and Non-Learning with unc = 1/6.

Moreover, Learning has always less investment that Non-Learning for any level of uncer-
tainty. It suggests that Learning yields a more cautious strategy than Non-Learning. This fact,
in addition to its wait-and-see approach at time 0 and its ability to better handle maximum
drawdown, makes Learning a safer and more conservative strategy than Non-Learning. This can
be seen in Fig. 21, where both Learning and Non-Learning have invested in Asset 3, but not at
the same pace. Non-Learning goes fully in Asset 3 at time 0, whereas Learning increments slowly
its weight in Asset 3 reaching 25% at the ﬁnal step. When unc is low, there is no value added
to choose Learning over Non-Learning from a performance perspective. Nevertheless, Learning
allows for a better management of risk as Table 5 exhibits.

As unc increases, in addition to being cautious, Learning mixes allocation in diﬀerent assets,

see Figures 22-25, while Non-Learning is stuck with the highest expected return asset.

23

Figure 22: Historical total allocations of Learning and Non-Learning with unc = 1.

Figure 23: Historical total allocations of Learning and Non-Learning with unc = 3.

Learning is able to be opportunistic and changes its allocation given the prices observed. For
example in Fig. 22, Learning starts investing in Asset 1 and 3 at time 1 and stops at time 12 to
weigh Asset 1 while keeping Asset 3. Similar remarks can be made for Fig. 23, where Learning
puts non negligeable weights in all three risky assets for unc = 6 in Fig. 24.

Figure 24: Historical total allocations of Learning and Non-Learning with unc = 6.

24

Figure 25: Historical total allocations of Learning and Non-Learning with unc = 12.

6 Conclusion

We have studied a discrete-time portfolio selection problem by taking into account both drift
uncertainty and maximum drawdown constraint. The dynamic programming equation has been
derived in the general case thanks to a speciﬁc change of measure. More explicit results have
been provided in the Gaussian case using the Kalman ﬁlter. Moreover, a change of variable has
reduced the dimensionality of the problem in the case of CRRA utility functions. Next, we have
provided extensive numerical results in the Gaussian case with CRRA utility functions using
recent deep neural network techniques. Our numerical analysis has clearly shown and quantiﬁed
the better risk-return proﬁle of the learning strategy versus the non-learning one. Indeed, besides
outperforming the non-learning strategy, the learning one provides a signiﬁcantly lower standard
deviation of terminal wealth and a better controlled maximum drawdown. Conﬁrming the results
established in De Franco, Nicolle, and Pham (2019b), this study exhibits the beneﬁts of learning
in providing optimal portfolio allocations.

Appendix

6.1 Proof of Proposition 3.1

For all k = 1, ..., N , the law under P, of Rk given the ﬁltration Gk−1 yields the unconditional
law under P of (cid:15)k. Indeed, since (Λk)k is a (P, G)-martingale, we have from Bayes formula, for
all Borelian F ⊂ Rd,

P[Rk ∈ F |Gk−1] = E[1{Rk∈F }|Gk−1] =

= E[

Λk
Λk−1

1{Rk∈F }|Gk−1] = E

1{Rk∈F }

(cid:12)
(cid:12)Gk−1

(cid:21)

1{B+e∈F }g(e)de =

g(z)1{z∈F }dz

E[Λk1{Rk∈F }|Gk−1]
E[Λk|Gk−1]

(cid:20) g(B + (cid:15)k)
g((cid:15)k)
(cid:90)

Rd

(cid:90)

=

g(B + e)
g(e)
Rd
= P[(cid:15)k ∈ F ].

This means that, under P, Rk is independent from B and from R1, .., Rk−1 and that Rk has the
(cid:50)
same probability distribution as (cid:15)k.

25

6.2 Proof of Proposition 3.2

For any borelian function f : Rd (cid:55)→ R we have, on one hand, by deﬁnition of πk+1:

E(cid:2)Λk+1f (B)|F o

k+1

(cid:3) =

(cid:90)

Rd

f (b)πk+1(db),

and, on the other hand, by deﬁnition of Λk:

E[Λk+1f (B)|F o

k+1] = E

(cid:20)

Λkf (B)

(cid:12)
(cid:12)
(cid:12)
(cid:12)
= E (cid:2)Λkf (B)g(Rk+1 − B)(cid:12)
(cid:12)F o

g(Rk+1 − B)
g(Rk+1)

F o

(cid:21)

k+1

=

(cid:90)

Rd

f (b)

g(Rk+1 − b)
g(Rk+1)

πk(db),

(cid:3) (g(Rk+1))−1

k+1

where we use in the last equality the fact that Rk+1 is independent of B under P (recall Propo-
(cid:50)
sition 3.1). By identiﬁcation, we obtain the expected relation.

6.3 Proof of Lemma 3.3

Since the support of the probability distribution ν of (cid:15)k is Rd, we notice that the law of the
random vector Yk := eRk − 1d has support equal to (−1, ∞)d. Recall from (7) that a ∈ Aq
k(x, z)
iﬀ

1 + a(cid:48)Yk+1 ≥ q max

, 1 + a(cid:48)Yk+1

(cid:105)

,

a.s.

(cid:104) z
x

(21)

(i) Take some a ∈ Aq
M = {Y i
event Ωi
from (21) that

k(x, z), and assume that ai < 0 for some i ∈ [[1, d]]. Let us then deﬁne the
M ] > 0. It follows

k+1 ∈ [0, 1], j (cid:54)= i}, for M > 0, and observe that P[Ωi

k+1 ≥ M, Y M

1 + aiM + max
j(cid:54)=i

|aj| ≥ q

z
x

,

on Ωi

M ,

k(x, z) ⊂ Rd
+.

which leads to a contradiction for M large enough. This shows that ai ≥ 0 for all i ∈ [[1, d]], i.e.
Aq
(ii) For ε ∈ (0, 1), let us deﬁne the event Ωε = {Y i
P[Ωε] > 0. For a ∈ Aq(x, z), we get from (21), and since a ∈ Rd

k+1 ≤ −1 + ε, i = 1, . . . , d}, which satisﬁes

+ by Step (i):

1 − (1 − ε)a(cid:48)1d ≥ q

z
x

,

on Ωε.

By taking ε small enough, this shows by a contradiction argument that

Aq

k(x, z) ⊂

(cid:110)

a ∈ Rd

+ : 1 − a(cid:48)1d ≥ q

(cid:111)

. =: ˜Aq(x, z).

z
x

(22)

(iii) Let us ﬁnally check the equality in (22). Fix some a ∈ ˜Aq(x, z). Since the random vector
Yk+1 is valued in (−1, ∞)d, it is clear that

and thus

1 + a(cid:48)Yk+1 ≥ 1 − a(cid:48)1d ≥ q

z
x

≥ 0,

a.s.,

1 + a(cid:48)Yk+1 ≥ q(cid:2)1 + a(cid:48)Yk+1

(cid:3),

a.s.,

which proves (21), hence the equality Aq(x, z) = ˜A(x, z).

(cid:50)

26

6.4 Proof of Lemma 3.4

1. Fix q1 ≤ q2 and (x, z) ∈ S q2 ⊂ S q1. We then have

a ∈ Aq2(x, z) ⇒ a ∈ Rd

+ and a(cid:48)1d ≤ 1 − q2

z
x

≤ 1 − q1

z
x

=⇒ a ∈ Aq1(x, z),

which means that Aq2(x, z) ⊆ Aq1(x, z).
n , n ∈ N∗. For any (x, z) ∈ S qn,
2. Fix q ∈ (0, 1), and consider the decreasing sequence qn = q + 1
we then have Aqn(x, z) ⊆ Aqn+1(x, z) ⊂ Aa(x, z), which implies that the sequence of increasing
sets Aqn(x, z) admits a limit equal to

lim
n→∞

Aqn(x, z) = ∪
n≥1

Aqn(x, z) = Aq(x, z),

since limn→∞ qn = q. This shows the right continuity of q (cid:55)→ Aq(x, z). Similarly, by considering
n , n ∈ N∗, we see that for any (x, z) ∈ Aq(x, z), the sequence
the increasing sequence qn = q − 1
of decreasing sets Aqn(x, z) admits a limit equal to

lim
n→∞

Aqn(x, z) = ∩
n≥1

Aqn(x, z) = Aq(x, z),

since limn→∞ qn = q. This proves the continuity in q of the set Aq(x, z).
3. Fix q ∈ (0, 1), and (x1, z), (x2, z) ∈ S q s.t. x1 ≤ x2. Then,

a ∈ Aq(x1, z) =⇒ a ∈ Rd

+ and a(cid:48)1d ≤ 1 − q

z
x1

≤ 1 − q

z
x2

=⇒ a ∈ Aq(x2, z),

which shows that Aq(x1, z) ⊆ Aq(x2, z).

4. Fix q ∈ (0, 1), (x, z) ∈ Aa(x, z). Then, for any a1, a2 of the set Aq(x, z), and β ∈ (0, 1)], and
denoting by a3 = βa1 + (1 − β)a2 ∈ Rd

+, we have

a(cid:48)
3

1d = βa(cid:48)
1

1d + (1 − β)a(cid:48)
2

1d ≤ β(cid:0)1 − q

(cid:1) + (1 − β)(cid:0)1 − q

z
x

(cid:1) = 1 − q

z
x

z
x

.

This proves the convexity of the set Aq(x, z).
4. The homogeneity property of Aq(x, z) is obvious from its very deﬁnition.

(cid:50)

6.5 Proof of Lemma 3.5

We prove the result by backward induction on time k from the dynamic programming equation
for the value function.

• At time N , we have for all λ > 0,

vN (λx, λz, µ) =

(λx)p
p

= λpvN (x, z, µ),

which shows the required homogeneity property.
• Now, assume that the homogeneity property holds at time k + 1, i.e vk+1(λx, λz, µ) =
λpvk+1(x, z, µ) for any λ > 0. Then, from the backward relation (9), and the homogeneity prop-
erty of Aq(x, z) in Lemma 3.4, it is clear that vk inherits from vk+1 the homogeneity property.
(cid:50)

27

6.6 Proof of Lemma 3.6

1. We ﬁrst show by backward induction that r (cid:55)→ wk(r, ·) is nondecreasing in on [q, 1] for all
k ∈ [[0, N ]].
• For any r1, r2 ∈ [q, 1], with r1 ≤ r2, and µ ∈ M+, we have at time N

wN (r1, µ) = U (r1)µ(Rd) ≤ U (r2)µ(Rd) = wN (r2, µ).

This shows that wN (r, ·) is nondecreasing on [q, 1].
• Now, suppose by induction hypothesis that r (cid:55)→ wk+1(r, ·) is nondecreasing. Denoting by Yk
:= eRk − 1d the random vector valued in (−1, ∞)d, we see that for all a ∈ Aq(r1)
(cid:1)(cid:105)

(cid:1)(cid:105)

(cid:104)

(cid:104)

min

1, r1

(cid:0)1 + a(cid:48)Yk+1

≤ min

1, r2

(cid:0)1 + a(cid:48)Yk+1

,

a.s.

since 1+a(cid:48)Yk+1 ≥ 1−a(cid:48)1d ≥ q 1
r1
(11), and noting that Aq(r1) ⊂ Aq(r2), we have

≥ 0. Therefore, from backward dynamic programming Equation

wk(r1, µ) =

sup
a∈Aq

( r1)

(cid:104)
E
wk+1

(cid:0) min (cid:2)1, r1

(cid:0)1 + a(cid:48)Yk+1

(cid:1)(cid:3), ¯g(Rk+1 − ·)µ(cid:1)(cid:105)

≤

sup
a∈Aq(r2)

(cid:104)
E

wk+1

(cid:0) min (cid:2)1, r2

(cid:0)1 + a(cid:48)Yk+1

(cid:1)(cid:3), ¯g(Rk+1 − ·)µ(cid:1)(cid:105)

= wk(r2, µ),

which shows the required nondecreasing property at time k.

2. We prove the concavity of r ∈ [q, 1] (cid:55)→ wk(r, ·) by backward induction for all k ∈ [[0, N ]]. For
r1, r2 ∈ [q, 1], and λ ∈ (0, 1), we set r = λr1 + (1 − λ)r2, and for a1 ∈ Aq(r1), a2 ∈ Aq(r2), we set
a = (cid:0)λr1a1 + (1 − λ)r2a2
+, we have a ∈ Rd
+,
and

(cid:1)/r which belongs to Aq(r). Indeed, since a1, a2 ∈ Rd

a =

(cid:16) λr1a1 + (1 − λ)r2a2
r

(cid:17)(cid:48)

1d ≤

λr1
r

(cid:0)1 −

(cid:1) +

q
r1

(1 − λ)r2
r

(cid:0)1 −

q
r2

(cid:1) = 1 −

q
r

.

• At time N , for ﬁxed µ ∈ M+, we have

wN

(cid:0)λr1 + (1 − λ)r2, µ(cid:1) = U (λr1 + (1 − λ)r2)

≥ λU (r1) + (1 − λ)U (r2) = λwN (r1, µ) + (1 − λ)wN (r2, µ),

since U is concave. This shows that wN (r, ·) is concave on [q, 1].
• Suppose now the induction hypothesis holds true at time k + 1: wk+1(r, ·) is concave on [q, 1].
From the backward dynamic programming relation (11), we then have

λwk(r1, µ) + (1 − λ)wk(r2, µ)

(cid:104)
≤ λE

wk+1

(cid:0) min[1, r1(1 + a(cid:48)

1Yk+1)], ¯g(Rk+1 − ·)µ(cid:1)(cid:105)

(cid:104)
+(1 − λ)E
wk+1

(cid:0) min[1, r2(1 + a(cid:48)

2Yk+1)], ¯g(Rk+1 − ·)µ(cid:1)(cid:105)

(cid:104)
≤ E
(cid:104)
= E

wk+1

wk+1

(cid:0)λ min[1, r1(1 + a(cid:48)
(cid:0) min[1, r(1 + a(cid:48)Yk+1)], ¯g(Rk+1 − ·)µ(cid:1)(cid:105)

1Yk+1)] + (1 − λ) min[1, r2(1 + a(cid:48)

≤ wk(r, µ),

2Yk+1)], ¯g(Rk+1 − ·)µ(cid:1)(cid:105)

where we used for the second inequality, the induction hypothesis joint with the concavity of
x (cid:55)→ min(1, x), and the nondecreasing monotonicity of r (cid:55)→ wk+1(r, ·). This shows the required
(cid:50)
inductive concavity property of r (cid:55)→ wk(r, ·) on [q, 1].

28

References

Bachouch, A., C. Hur´e, N. Langren´e, and H. Pham (2018a). Deep neural networks algorithms for
stochastic control problems on ﬁnite horizon, part 1: convergence analysis. arXiv:1812.04300 .

Bachouch, A., C. Hur´e, N. Langren´e, and H. Pham (2018b). Deep neural networks algorithms for
stochastic control problems on ﬁnite horizon, part 2: numerical applications. arXiv preprint
arXiv:1812.05916, to appear in Methodology and Computing in Applied Probability.

Bismuth, A., O. Gu´eant, and J. Pu (2019). Portfolio choice, portfolio liquidation, and portfolio
transition under drift uncertainty. Mathematics and Financial Economics 13 (4), 661–719.

Boyd, S., E. Lindstr¨om, H. Madsen, and P. Nystrup (2019). Multi-period portfolio selection

with drawdown control. Annals of Operations Research 282 (1-2), 245–271.

Cvitani´c, J. and I. Karatzas (1994). On portfolio optimization under ”drawdown” constraints.

Constraints, IMA Lecture Notes in Mathematics & Applications 65 .

Cvitani´c, J., A. Lazrak, L. Martellini, and F. Zapatero (2006). Dynamic portfolio choice with
parameter uncertainty and the economic value of analysts? recommendations. The Review of
Financial Studies 19 (4), 1113–1156.

De Franco, C., J. Nicolle, and H. Pham (2019a). Bayesian learning for the markowitz portfolio

selection problem. International Journal of Theoretical and Applied Finance 22 (07).

De Franco, C., J. Nicolle, and H. Pham (2019b). Dealing with drift uncertainty: a bayesian

learning approach. Risks 7 (1), 5.

Elie, R. and N. Touzi (2008). Optimal lifetime consumption and investment under a drawdown

constraint. Finance and Stochastics 12 (3), 299.

Elliott, R. J., L. Aggoun, and J. B. Moore (2008). Hidden Markov Models: Estimation and

Control. Springer.

G´eron, A. (2019). Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow:

Concepts, Tools, and Techniques to Build Intelligent Systems. O’Reilly Media.

Grossman, S. J. and Z. Zhou (1993). Optimal investment strategies for controlling drawdowns.

Mathematical ﬁnance 3 (3), 241–276.

Hornik, K. (1991). Approximation capabilities of multilayer feedforward networks. Neural Net-

works 4 (2), 251–257.

Kalman, R. E. (1960). A new approach to linear ﬁltering and prediction problems. Transactions

of the ASME–Journal of Basic Engineering 82, 35–45.

Kalman, R. E. and R. S. Bucy (1961, 03). New Results in Linear Filtering and Prediction

Theory. Journal of Basic Engineering 83 (1), 95–108.

Karatzas, I. and X. Zhao (2001). Bayesian Adaptative Portfolio Optimization. In Option Pricing,

Interest Rates and Risk Management. Cambridge University Press.

Keppo, J., H. M. Tan, and C. Zhou (2018). Investment decisions and falling cost of data analytics.

Lakner, P. (1998). Optimal trading strategy for an investor: the case of partial information.

Stochastic Processes and their Applications 76 (1), 77–97.

29

Redeker, I. and R. Wunderlich (2018). Portfolio optimization under dynamic risk constraints:

Continuous vs. discrete time trading. Statistics & Risk Modeling 35 (1-2), 1–21.

Rogers, L. C. G. (2001). The relaxed investor and parameter uncertainty. Finance and Stochas-

tics 5 (2), 131–154.

30

