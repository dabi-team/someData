2
2
0
2

t
c
O
0
1

]

G
L
.
s
c
[

2
v
2
1
2
1
0
.
3
0
2
2
:
v
i
X
r
a

A Quantitative Geometric Approach to
Neural-Network Smoothness

Zi Wang
University of Wisconsin-Madison
zw@cs.wisc.edu

Gautam Prakriya
The Chinese University of Hong Kong
gautamprakriya@gmail.com

Somesh Jha
University of Wisconsin-Madison
jha@cs.wisc.edu

Abstract

Fast and precise Lipschitz constant estimation of neural networks is an important
task for deep learning. Researchers have recently found an intrinsic trade-off be-
tween the accuracy and smoothness of neural networks, so training a network with
a loose Lipschitz constant estimation imposes a strong regularization, and can hurt
the model accuracy signiﬁcantly. In this work, we provide a uniﬁed theoretical
framework, a quantitative geometric approach, to address the Lipschitz constant
estimation. By adopting this framework, we can immediately obtain several the-
oretical results, including the computational hardness of Lipschitz constant esti-
mation and its approximability. We implement the algorithms induced from this
quantitative geometric approach, which are based on semideﬁnite programming
(SDP).1 Our empirical evaluation demonstrates that they are more scalable and
precise than existing tools on Lipschitz constant estimation for ℓ∞-perturbations.
Furthermore, we also show their intricate relations with other recent SDP-based
techniques, both theoretically and empirically. We believe that this uniﬁed quan-
titative geometric perspective can bring new insights and theoretical tools to the
investigation of neural-network smoothness and robustness.

1 Introduction

The past decade has witnessed the unprecedented success of deep learning in many machine learn-
ing tasks (Krizhevsky et al., 2017; Mikolov et al., 2013). Despite the growing popularity of deep
learning, researchers have also found that neural networks are very vulnerable to adversarial at-
tacks (Szegedy et al., 2014; Goodfellow et al., 2015; Papernot et al., 2016). As a result, it is impor-
tant to train neural networks that are robust against those attacks (Madry et al., 2018). In recent years,
the deep learning community starts to focus on certiﬁably robust neural networks (Albarghouthi,
2021; Hein and Andriushchenko, 2017; Katz et al., 2017; Cohen et al., 2019; Raghunathan et al.,
2018; Wang et al., 2022; Leino et al., 2021). One way to achieve certiﬁed robustness is to estimate
the smoothness of neural networks, where the smoothness is measured by the Lipschitz constant of
the neural network. Recent works have found that to achieve both high accuracy and low Lipschitz-
ness, the network has to signiﬁcantly increase the model capacity (Bubeck and Sellke, 2021). This
implies that there is an intrinsic tension between the accuracy and smoothness of neural networks.

Commonly considered adversarial attacks are the ℓ∞ and ℓ2-perturbations in the input space.
Leino et al. (2021); Cohen et al. (2019) have successfully trained networks with low ℓ2-Lipschitz

1Our code is available at https://github.com/z1w/GeoLIP.

36th Conference on Neural Information Processing Systems (NeurIPS 2022).

 
 
 
 
 
 
(2021)

trained networks with low local Lipschitzness for ℓ∞-
constant, and Huang et al.
perturbations. There are few well-established techniques to train neural networks with low global
Lipschitzness for ℓ∞-perturbations. The techniques for the ℓ2-perturbation do not easily transfer to
the ℓ∞-case (Leino et al., 2021). One critical step is to measure the Lipschitz constant more pre-
cisely and efﬁciently. Jordan and Dimakis (2020) showed that for ReLU networks, it is NP-hard
to approximate the Lipschitz constant for ℓ∞-perturbations within a constant factor , and proposed
an exponential-time algorithm to compute the exact Lipschitz constant. However, researchers are
interested in more scalable approaches to certify and train networks. In this work, we consider the
Formal Global Lipschitz constant (FGL) (See Equation (3)), which is roughly the maximum of the
gradient operator norm, assuming all activation patterns on hidden layers are independent and possi-
ble. FGL is an upper bound of the exact Lipschitz constant and has been used in Raghunathan et al.
(2018); Fazlyab et al. (2019); Latorre et al. (2020).

We address the Lipschitz constant estimation from the quantitative geometric perspective. Quanti-
tative geometry aims to understand geometric structures via quantitative parameters, and has con-
nections with many mathematical disciplines, such as functional analysis and combinatorics (Naor,
2013). In computer science, quantitative geometry plays a central role in understanding the approx-
imability of discrete optimization problems. We approach those hard discrete optimization problems
by considering the efﬁciently solvable continuous counterparts, and analyze the precision loss due
to relaxation, which is often the SDP relaxation (Goemans and Williamson, 1995; Nesterov, 1998;
Alon and Naor, 2004). The natural SDP relaxations for the intractable problems usually induce the
optimal known polynomial time algorithms (Bhattiprolu et al., 2022). By adopting the quantitative
geometric approach, we can immediately understand the computational hardness and approximabil-
ity of FGL estimations. Our algorithms on two-layer networks are the natural SDP relaxations from
the quantitative geometric perspective.

Latorre et al. (2020) employed polynomial optimization methods on the FGL estimation for ℓ∞-
perturbations. Polynomial optimization is a very general framework, and many problems can be
cast in this framework (Motzkin and Straus, 1965; Goemans and Williamson, 1995). Therefore,
we argue that this is not a precise characterization of the FGL-estimation problem. On the other
hand, there are also several SDP-based techniques for FGL estimations. Raghunathan et al. (2018)
proposed an SDP algorithm to estimate the FGL for ℓ∞-perturbations of two layer networks, and
Fazlyab et al. (2019) devised an SDP algorithm to estimate the ℓ2-FGL. We will demonstrate the
intricate relationships between our algorithms and these existing SDPs on two-layer networks.

Several empirical studies have found that techniques on one ℓp-perturbations often do not trans-
fer to another ones, even though the authors claim that in theory these techniques should trans-
fer (Fazlyab et al., 2019; Leino et al., 2021). This in-theory claim usually comes from a qualitative
perspective. In ﬁnite-dimensional space, one can always bound one ℓp-norm from another one, so
techniques for one ℓp-perturbations can also provide another bound for a different ℓp-perturbations.
However, this bound is loose and in practice not useful (Latorre et al., 2020). Instead, we believe
that when transferring techniques from one norm to another one, we should consider the quantitative
geometric principle: we should separate the geometry-dependent component from the geometry-
independent one in those techniques, and modify the geometry-dependent component accordingly
for a different normed space. As we will demonstrate, our whole work is guided by this princi-
ple. We hope that our uniﬁed quantitative geometric framework can bring insights to the empirical
hard-to-transfer observations, and new tools to address these issues.

Contributions. To summarize, we have made the following contributions:

1. We provide a uniﬁed theoretical framework to address FGL estimations, which immediately
yields the computational hardness and SDP-based approximation algorithms on two-layer
networks (Section 3).

2. We demonstrate the relations between our algorithms and other SDP-based tools, which in
return inspires us to design the algorithms for multi-layer networks. This provides more
insightful and compositional interpretations of existing works, and makes them easier-to-
generalize (Sections 4 and 5).

3. We implement the algorithms and name the tool GeoLIP. We empirically validate our
theoretical claims, and compare GeoLIP with existing methods to estimate FGL for ℓ∞-
perturbations. The result shows that GeoLIP provides a tighter bound (20%-60% improve-

2

ments) than existing tools on small networks, and much better results than the naive matrix-
norm-product method for deep networks, which existing tools cannot handle (Section 6).

2 Preliminaries

Notation. Let [n] = {1, . . . , n}. For two functions f and g, f ◦ g(x) = f (g(x)). A 0-1 cube
is {0, 1}n, and a norm-1 cube is {−1, 1}n for some integer n > 0. R+ = [0, ∞). For any vector
v ∈ Rn, diag(v) is an n × n diagonal matrix, with diagonal values v. Let en = (1, . . . , 1) ∈ Rn be
an n-dimensional vector of all 1’s; and In = diag(en), the identity matrix. Let ||v||p denote the ℓp
norm of v. We use q to denote the Hölder conjugate of p as a convention, i.e., 1
q = 1. If v is
an operator in the ℓp-space, the operator norm of v is then ||v||q. Throughout the paper, we consider
the ℓp-norm of the input’s perturbation, and therefore, the ℓq-norm of the gradient, which acts as an
operator on the perturbation. A square matrix X (cid:23) 0 means that X is positive semideﬁnite (PSD).
Let tr(X) be the trace of a square matrix X. If a, b ∈ Rn, let ha, bi be the inner product of a and b.

p + 1

Lipschitz function. Given two metric spaces (X, dX ) and (Y, dY ), a function f : X → Y is
Lipschitz continuous if there exists K > 0 such that for all x1, x2 ∈ X,

dY (f (x2), f (x1)) ≤ KdX (x2, x1).

(1)

The smallest such K satisfying Equation (1), denoted by Kf , is called the Lipschitz constant of f .
For neural networks, X is in general Rm equipped with the ℓp-norm. We will only consider the
case when Y = R. In actual applications such as a classiﬁcation task, a neural network has multiple
outputs. The prediction is the class with the maximum score. One can then use the margin between
each pair of class predictions and its Lipschitz constant to certify the robustness of a given predic-
tion (Raghunathan et al., 2018; Leino et al., 2021). From Rademacher’s theorem, if f is Lipschitz
continuous, then f is is almost everywhere differentiable, and Kf = supx ||∇f (x)||q.

Neural network as function. A neural network f : Rm → R is characterized by:
f1(x) = W 1x + b1; fi(x) = W iσ(fi−1(x)) + bi, i = 2, . . . , d.
where W i ∈ Rni+1×ni is the weight matrix between the layers, n1 = m, d is the depth of the neural
network, σ denotes an activation function, bi ∈ Rni+1 is the bias term, and f = fd ◦· · ·◦f1. Because
we only consider the R as the codomain of f , W d = u ∈ R1×nd is a vector. From chain rule, the
gradient of this function is

∇f (x) = (W 1)T [diag(σ′(f1(x)))(W 2)T · · · diag(σ′(fd−1(x)))(W d)T ].
Common activation functions, including ReLU (Nair and Hinton, 2010), sigmoid functions, and
ELU (Clevert et al., 2016) are almost everywhere differentiable. As a result, we are interested in the
supremum operator norm of Equation (2).

(2)

However, checking all possible inputs x is infeasible, and common activation functions have
bounded derivative values, say [a, b]. We are then interested in the following value instead:

max
vi∈[a,b]ni

||(W 1)T · diag(v2) · · · · · diag(vd)(W d)T ||q,

(3)

where ni is the dimension of each diag(vi). We call this value the formal global Lipschitz constant
(FGL) because we treat all activation functions independent but in reality not all activation patterns
are feasible. Therefore, this is an upper bound of the true global Lipschitz constant of the neural net-
work. However, it is the value studied in most global Lipschitzness literature (Scaman and Virmaux,
2018; Fazlyab et al., 2019; Latorre et al., 2020), and also turns out useful in certifying the robustness
of neural networks (Raghunathan et al., 2018; Leino et al., 2021; Pauli et al., 2022). We use ℓp-FGL
to denote the FGL for ℓp-perturbations.

In this paper, we focus on the ℓp-perturbation on the input, where p = ∞ or p = 2. Because q is the
Hölder conjugate of p, we are interested in the value of Equation (3), when q = 1 (for p = ∞), and
q = 2 (for p = 2). Notice that in the ReLU-network case, [a, b] is [0, 1]. We will use ReLU-networks
as the illustration for the rest of the paper because of the popularity of ReLU in practice and the easy
presentation of the 0-1-cube. However, the algorithms presented in this work can be adapted with
minor adjustments to other common activation functions.

3

Remark 2.1. FGL considers all possible activation patterns on the hidden layers, while some of the
activation patterns might be unachievable in reality. Therefore, FGL is an upper bound of the true
Lipschitz constant. Notice that the activation pattern induced from an input is also decided by the
bias term. Therefore, to ﬁnd the true Lipschitz constant, one has to incorporate the information from
the bias term.

3 Two-layer neural networks

In this section, we consider the two-layer neural network case. We reduce the FGL estimation
to the matrix mixed-norm problem. This immediately yields the computational complexity and
approximation algorithms for FGL estimations. In Appendix A.1, we show that we can consider
{0, 1} instead of [0, 1] in Equation (3) for two-layer networks.

Problem description. For a two-layer network where W 1 = W ∈ Rn×m and W d = u ∈ R1×n,
its FGL (as in Equation (3)) is maxy∈{0,1}n ||W T diag(y)uT ||q, where we use y to denote v1
in this case.
If we expand the matrix multiplication, it is easy to check that this equals to
maxy∈{0,1}n ||W T diag(u)y||q. Let A = W T diag(u), then the ℓp-FGL is

max
y∈{0,1}n

||Ay||q.

(4)

3.1 ℓ∞-FGL estimation

We consider a natural SDP relaxation to Equation (4) when q = 1, and analyze the result using the
celebrated Grothendieck Inequality, which is a fundamental tool in functional analysis.

Mixed-norm problem. The ∞ → 1 mixed-norm of a matrix is deﬁned as

||A||∞→1 = max

||x||∞=1

||Ax||1.

The mixed-norm problem appears similar to Equation (4) when q = 1, except for that instead of
a norm-1-cube, the cube in Equation (4) is a 0-1-cube. Alon and Naor (2004) showed that it is
NP-hard, speciﬁcally MAXSNP-hard, to compute the ∞ → 1 mixed-norm of a matrix A, via a
reduction to the graph Max-Cut problem. Moreover, Alon and Naor (2004) constructed a natural
SDP relaxation for the mixed-norm problem:

max tr(BX)

s.t. X (cid:23) 0, Xii = 1,i ∈ [n + m],

(5)

where A is a submatrix of B. We provide the detailed derivation of this relaxation in Appendix A.3.
In fact, this relaxation admits a constant approximation factor. Grothendieck (1956) developed the
local theory of Banach spaces, and showed that there exists an absolute value KG such that
Theorem 3.1. For any m, n ≥ 1, A ∈ Rn×m, and any Hilbert space H, the following holds:

max
ui,vj ∈B(H)

i,j
X

Aij hui, vjiH ≤ KG||A||∞→1,

where B(H) denotes the unit ball of the Hilbert space.

The precise value of KG is still an outstanding open problem, and it is known that KG <
1.783 (Krivine, 1979; Braverman et al., 2011). The approximation factor of the SDP relaxation
in Equation (5) is KG. Similar to the mixed-norm problem, we show that the ℓ∞-FGL estimation is
MAXSNP-hard and provide an SDP relaxation, which also admits the KG-approximation ratio. We
provide a detailed explanation on why KG is the approximation ratio and how we can view the SDP
relaxation as a geometric transformation in Appendix A.4.
Theorem 3.2. ℓ∞-FGL estimation is MAXSNP-hard.

From 0-1 cube to norm-1 cube.
If we can transform the 0-1 cube in Equation (4) to a norm-1 cube,
and formulate an equivalent optimization problem, then one can apply the SDP program in Equa-
tion (5) to compute an upper bound of the FGL. Indeed, we provide a cube rescaling technique,

4

and it allows us to construct the SDP for the ℓ∞-FGL estimation. We provide the full detail of this
technique in Appendix A.5, and the result SDP for the ℓ∞-FGL estimation is

1
2
s.t. X (cid:23) 0, Xii = 1, i ∈ [n + m + 1],

tr(BX)

max

(6)

. As a result, we have:

0

0
A Aen

(cid:18)

0
0

(cid:19)

where B is a (n + 1 + m) × (n + 1 + m) matrix, and B =

Theorem 3.3. There exists a polynomial-time approximation algorithm to estimate the ℓ∞-FGL of
two-layer neural networks, moreover, the approximation ratio is KG.

3.2 ℓ2-FGL estimation

Scaman and Virmaux (2018) showed that the ℓ2-FGL estimation is NP-hard. If q = 2 in Equa-
tion (4), the objective becomes similar to the ∞ → 2 mixed-norm problem. This is a quadratic op-
timization problem with a PSD weight matrix over a cube, and can be viewed as a generalization of
the graph Max-Cut problem. In the quadratic optimization formulation of Max-Cut, the weight ma-
trix is the Laplacian of the graph, a special PSD matrix (Goemans and Williamson, 1995). Nesterov
(1998) generalized Goemans-Williamson’s technique and analyzed the case when the weight matrix
is PSD, showing that the natural SDP relaxation in this case has a π
2 -approximation ratio. This pro-
π
2 -approximation algorithm for the ℓ2-FGL estimation. The approximation ratio comes
vides a
from a similar inequality to the one in Theorem 3.1, known as the Little Grothendieck Inequality.
The SDP for ℓ2-FGL estimation is:

p

max

tr(

1
2 s

AT Aen
AT A
n AT Aen
n AT A eT
eT
(cid:19)
s.t. X (cid:23) 0, Xii = 1, i ∈ [n + 1].

(cid:18)

X)

(7)

π
2 .

The full derivation is provided in Appendix A.8, and we have the following theorem:
Theorem 3.4. There exists a polynomial-time approximation algorithm to estimate the ℓ2-FGL of
two-layer neural networks with an approximation factor
Remark 3.5. As we have discussed, for two-layer networks, the ℓp-FGL estimation is essentially the
∞ → q mixed-norm problem. Indeed the mixed-norm problem is an outstanding topic in theoretical
computer science. As discussed in Bhattiprolu et al. (2018), the ∞ → q mixed norm problem has
constant approximation algorithms if q ≤ 2, and is hard to approximate within almost polynomial
factors when q > 2. Because when q > 2, its Hölder conjugate p < 2. This implies that for
two-layer networks, the FGL estimation can be much harder for ℓp-perturbations when p < 2.
Briët et al. (2017) showed that it is NP-hard to approximate the ∞ → 2 mixed-norm problem
π
better than
2 . Raghavendra and Steurer (2009) proved that assuming the unique games conjec-
ture (Khot, 2002), it is NP-hard to approximate the ∞ → 1 mixed-norm problem better than KG.
These optimal approximation ratios match our SDP relaxations for FGL estimations accordingly.

p

p

4 Relations to existing SDP works

Before introducing our approach for multi-layer networks, we ﬁrst examine some existing SDP
works on FGL estimations, and discuss their relationships with our natural SDP relaxations in Sec-
tion 3. ℓ∞-FGL estimation. Raghunathan et al. (2018) formulated an SDP that only works for
two-layer networks. Theirs is essentially the same as ours in Equation (6) (See the detailed com-
parison in Appendix A.10). However, we provide a rigorous derivation and simpler formulation,
and also a sound theoretical analysis of the bound, which illustrate more insights to this problem.
Raghunathan et al. (2018) treated the SDP relaxation as a heuristic to a hard quadratic programming
problem. We prove that this relaxation is not only a heuristic, but in fact induces an approximation
algorithm with a tight bound.

ℓ2-FGL estimation. Fazlyab et al. (2019) proposed LipSDP, another SDP-based algorithm for the
ℓ2-FGL estimation problem. Fazlyab et al. (2019) provided several variants of LipSDP to bal-
ance the precision and scalability. Pauli et al. (2022) demonstrated that the most precise version

5

of LipSDP, LipSDP-Network, fails to produce an upper bound for ℓ2-FGL. In this paper, all the
references of LipSDP are to LipSDP-Neuron, the less precise version. Surprisingly, even though
the approach in LipSDP appears quite different from Equation (7), we show that LipSDP is dual
of Equation (7) to estimate the ℓ2-FGL on two-layer networks. LipSDP for two-layer networks is:

min
ζ,λ

ζ :

−2abW T W T − ζIm (a + b)W T T
−2T + uT u

(a + b)T W

(cid:22) 0, λi ≥ 0

,

np

(cid:18)
where T = diag(λ) for λ ∈ Rn
+; a and b are the lower and upper bounds of the activation’s derivative.
We will construct a new quadratic program, which we show is equivalent to Equation (4) when
q = 2, and LipSDP is its dual SDP relaxation.

(cid:19)

o

Let the input of the i-th activation node on diag(y) be yi, and wi be the row vector of W . Hence,
yi = wix. Let ∆x ∈ Rm be a perturbation on x, so ∆yi = wi∆x. Let ∆σ(y) ∈ Rn denote the
induced perturbation on diag(y). The constraint from the activation function is ∆σ(y)i
∈ [a, b], in
∆yi
other words, ∆σ(y)i

wi∆x ∈ [a, b]. One can write the range constraint as

(∆σ(y)i − a · wi∆x)(∆σ(y)i − b · wi∆x) ≤ 0.

This can be written in the quadratic form:

T

wi∆x
∆σ(y)i

−2ab a + b
a + b −2

wi∆x
∆σ(y)i

≥ 0,

∀i ∈ [n].

(8)

(cid:18)

(cid:19)

(cid:18)

(cid:19)

(cid:19) (cid:18)
Since for the two layer network, f (x) = uσ(y), then ∆f (x) = u∆σ(y). The objective for ℓ2-FGL
estimation is max∆x,∆σ(y)
. The equivalence between this program and Equation (4)
when q = 2 is presented in Appendix A.11.
Remark 4.1. Another interpretation for the quadratic program is that we want to quantify how the
output changes given a data-independent input change, i.e., ∆x. In other words, we want to analyze
the effect of ∆x propagating from the input to the output, with symbolic values rather than actual
inputs. The idea is similar to symbolic execution from program analysis (Baldoni et al., 2018).

(u∆σ(y))2
(∆x)2

q

Duality to LipSDP. Now we will show that LipSDP is the dual SDP to the program formulated
above. The dual SDP derivation is of similar form in Ben-Tal and Nemirovski (2001, Ch.4.3.1). Let
us introduce a variable ζ such that ζ − (u∆σ(y))2

(∆x)2 ≥ 0. In other words,

ζ(∆x)2 − (u∆σ(y))2 ≥ 0.

(9)

For each constraint in Equation (8), let us introduce a dual variable λi ≥ 0. Multiply each constraint

with λi, then

∆x
∆σ(y)i

(cid:18)

T

−2abλiwT
i wi
(a + b)λiwi

(a + b)λiwT
i
−2λi

∆x
∆σ(y)i

≥ 0,

∀i ∈ [n].

(cid:19)

(cid:18)

Sum all of them, then we have

∆x
∆σ(y)
−2T
T = diag(λ) is the n × n diagonal matrix of dual variables λ1, . . . , λn.

(cid:18)

(cid:19)

(cid:18)

(cid:19) (cid:18)
−2abW T W T (a + b)W T T
(a + b)T W

(cid:19)

T

∆x
∆σ(y)

(cid:19) (cid:18)

(cid:19)

≥ 0, where

Equation (9) can be rewritten as:

(cid:19)
program for the new optimization program is

(cid:18)

∆x
∆σ(y)

T

ζIm
0

(cid:18)

0
−uT u

∆x
∆σ(y)

(cid:19) (cid:18)

≥ 0. As a result, the dual

(cid:19)

min
ζ,λ

ζ :

(cid:18)

−2abW T W T − ζIm (a + b)W T T
−2T + uT u

(a + b)T W

(cid:22) 0, λi ≥ 0

.

(cid:19)

np
π
Remark 4.2. In Remark 3.5, we mention that
2 is the optimal approximation ratio for the ∞ → 2
mixed-norm problem, which matches the approximation ratio in Theorem 3.4. Hence, improving
the natural SDP relaxation in Equation (7) can be very hard. The duality provides another evidence
of LipSDP-Neuron’s correctness, and hints that LipSDP-Network, the improved variant, may be
wrong.

p

o

6

5 ℓ∞-FGL estimation for multi-layer networks

For a multi-layer neural network, the formal gradient becomes a high-degree polynomial, and its
ℓq-norm estimation becomes a high-degree polynomial optimization problem over a cube, which is
in general a hard problem (Lasserre, 2015). We provide a discussion of the polynomial optimization
approach of FGL estimation in Appendix B. Here we provide an SDP dual program of the ℓ∞-FGL
estimation inspired by the dual SDP approach in Section 4. The difference is that now we consider
ℓ∞-perturbations to the input space instead of ℓ2. Hence, the objective becomes

max
∆x,∆σ(y)

|u∆σ(y)|
||∆x||∞

.

If we add an extra constraint ||∆x||∞ = 1, the above objective becomes

max
∆x,∆σ(y)

1
2

(u∆σ(y) + u∆σ(y)).

(10)

The constraints are
T

wi∆x
∆σ(y)i

(cid:18)

(cid:19)

2ab
−(a + b)

(cid:18)

We can write ∆(x)2

j ≤ 1 as

−(a + b)
2

wi∆x
∆σ(y)i

T

1
∆xj

(cid:19) (cid:18)

1
0
0 −1

1
∆xj

≥ 0.

≤ 0, ∆(x)2

j ≤ 1, ∀i ∈ [n], j ∈ [m].

(cid:19)

(cid:18)
Now let us introduce n + m non-negative dual variables (τ, λ), where τ ∈ Rm
multiply each dual variable with the constraint and add all the constraints together, we will have

+ and λ ∈ Rn

(cid:19) (cid:18)

(cid:19)

(cid:19)

(cid:18)

+. If we

T

1
∆x
∆σ(y)!

m
j=1 τj
0
0



P

0
−2abW T W T2 − T1
(a + b)T2W

0
(a + b)W T T2
−2T2



1
∆x
∆σ(y)!

≥ 0,



where T1 = diag(τ ) and T2 = diag(λ). As a result, we can incorporate the objective Equation (10)
and obtain the dual SDP for the ℓ∞-FGL estimation:
m
j=1 τj − ζ
0
uT

0
−2abW T W T2 − T1
(a + b)T2W

u
(a + b)W T T2
−2T2

(cid:22) 0, λi, τj ≥ 0

min
ζ,λ,τ

(11)

ζ
2

P





:

.



Remark 5.1. The SDP programs in Section 3 are strictly feasible because the identity matrix is a
positive deﬁnite solution. Hence, Slater’s condition is satisﬁed and strong duality holds.





n

o

Multi-layer extension. We can simply extend the dual program to multiple-layer networks. We
ﬁrst vectorize all the units in the input layer and hidden layers, and then constrain them using layer-
wise inequalities to formulate an optimization problem. Let us consider a general d-layer multi-layer
network, where W i ∈ Rni+1×ni for i ∈ [d − 1], and W d = u ∈ R1×nd . Let ∆x denote the
perturbation on the input layer, ∆zi be the perturbation on the i-th hidden layer, and wi
j be the j-th
row vector of W i. The only difference between two layer networks and multi-layer networks is that
we have the additional constraints:

T

∆zi
∆zi+1

j (cid:19)

(cid:18)

−2ab(wi+1

)T wi+1
j

j
(a + b)wi+1

j

(cid:18)

(a + b)(wi+1
−2

j

)T

∆zi
∆zi+1

j (cid:19)

(cid:19) (cid:18)

≥ 0.

Let Λi ∈ Rni
all the constraints together and formulate the following SDP program:

+ and Ti = diag(Λi) for i ∈ [d]. Following the similar SDP dual approach, we can add

where

L =









0

0
0 −2ab(W 1)T W 1T2
0
...
0

(a + b)T2W 1
...
0

min
ζ,Λi

ζ
2

n

: (L + N ) (cid:22) 0, i ∈ [d]

,

0
(a + b)(W 1)T T2
−2T2 − 2ab(W 2)T W 2T3
...
0

7

o
. . .
. . .
. . .
. . .
. . .

0
0
0
...

0
0
0
...

(a + b)TdW d−1 −2Td

(12)



,






(13)

 
 
Table 1: ℓ∞-FGL estimation of various methods: DGeoLIP and NGeoLIP induce the same values
on two layer networks. DGeoLIP always produces tighter estimations than LiPopt and MP do.

Network

DGeoLIP NGeoLIP

LiPopt

MP

Sample

BruF

2-layer/16 units
2-layer/256 units
8-layer/64 units per layer

185.18
425.04
8327.2

185.18
425.04
—–

259.44
1011.65
N/A

578.54
2697.38
8.237 ∗ 107

175.24
306.98
1130.6

175.24
N/A
N/A

Table 2: Running time (in seconds) of various tools on ℓ∞-FGL estimation: DGeoLIP and NGeoLIP
are faster than LiPopt. Notice that the running time is implementation and solver-dependent.

Network

DGeoLIP NGeoLIP

LiPopt

BruF

2-layer/16 units
2-layer/256 units
8-layer/64 units

28.1
976.0
329.5

1572
2690
N/A

4.8
N/A
N/A

22.3
70.9
—–

0
−T1
...
0

n1
k=1 Λ1k − ζ
0
...
uT

N = 

P





. . .
. . .
. . .
· · ·

u
0
...
0



.





Remark 5.2. If we expand the matrix inequality derived from the compact neural-network repre-
sentation in Fazlyab et al. (2019, Theorem 2), we will have exactly the same matrix for network
constraints as L (Equation (13)) in the dual program formulation. In other words, we provide a
compositional optimization interpretation to the compact neural-network representation in LipSDP.
With this interpretation, one can extend the SDP to beyond feed-forward structures, such as skip con-
nections (He et al., 2016). Notice that if we apply the similar reasoning to the multi-layer network
ℓ2-FGL estimation, we will obtain LipSDP-Neuron.

6 Evaluation and discussion

The primary goal of our work is to provide a theoretical framework, and also algorithms for ℓ∞-FGL
estimations on practically used networks. The ℓ2-FGL can be computed using LipSDP. We have
implemented the algorithms using MATLAB (MATLAB, 2021), the CVX toolbox (CVX Research,
2020) and MOSEK solver (ApS, 2019), and name the tool GeoLIP. To validate our theory and the
applicability of our algorithms, we want to empirically answer the following research questions:

RQ1: Is GeoLIP better than existing methods in terms of precision and scalability?
RQ2: Are the dual SDP programs devised throughout the paper valid?

As we shall see, GeoLIP is indeed better than existing methods in terms of precision and scalability;
and the dual SDP programs produce the same values as their natural-SDP-relaxation counterparts.

6.1 Experimental design

To answer RQ1, we will run GeoLIP and existing tools that measure the ℓ∞-FGL on various feed-
forward neural networks trained with the MNIST dataset (LeCun and Cortes, 2010). We will record
the computed ℓ∞-FGL to compare the precision, and the computation time to compare the scalabil-
ity.

To answer RQ2, we will run the natural SDP relaxations for ℓp-FGL estimations proposed in Sec-
tion 3, LipSDP for ℓ2-FGL estimation, and the dual program Equation (11) for ℓ∞-FGL on two-layer
neural networks, and compare their computed FGLs.

8

Measurements. Our main baseline tool is LiPopt (Latorre et al., 2020), which is an ℓ∞-FGL esti-
mation tool.2 Notice that LiPopt is based on the Python Gurobi solver (Gurobi Optimization, LLC,
2022), while we use the MATLAB CVX and MOSEK solver. LiPopt relies on a linear programming
(LP) hierarchy for the polynomial optimization problem. We use LiPopt-k to denote the k-th degree
of the LP hierarchy. BruF stands for an brute-force exhaustive enumeration of all possible activa-
tion patterns. This is the ground truth for FGL estimations. However, this is an exponential-time
search, so we can only run it on networks with a few hidden units. Sample means that we randomly
sample 200, 000 points in the input space and compute the gradient norm at those points. Notice
that this is a lower bound of the true Lipschitz constant, and thus a lower bound of the FGL. MP
stands for the weight-matrix-norm-product method. This is a naive upper bound of FGL. We use
NGeoLIP to denote the natural SDP relaxations devised in Section 3, and DGeoLIP to denote the
dual SDP Equation (12) for ℓ∞-FGL estimation. Notice that NGeoLIP only applies to two-layer
networks.

We use “—” in the result tables to denote that the experimental setting is not in the scope of the
tool’s application, and “N/A” to denote the computation takes too much time (> 20 hours).

Network setting. We run the experiments on fully-connected feed-forward neural networks, trained
with the MNIST dataset for 10 epochs using the ADAM optimizer (Kingma and Ba, 2015). All the
trained networks have accuracy greater than 92% on the test data. For two-layer networks, we use 8,
16, 64, 128, 256 hidden nodes. For multiple-layer networks, we consider 3, 7, 8-layer networks, and
each hidden layer has 64 ReLU units. Because MNIST has 10 classes, we report the estimated FGL
with respect to label 8 as in Latorre et al. (2020), and the average running time per class: we record
the total computation time for all 10 classes from each tool, and report the average time per class.

6.2 Discussion

We present selected results in Tables 1 and 2, and related major discussions here. The full results,
more experimental setup and additional discussions can be found in Appendix C.

RQ1. In the experiments of LiPopt, we only used LiPopt-2. In theory, if one can go higher in the
LP hierarchy in LiPopt, the result becomes more precise. However, in the case of fully-connected
neural networks, using degree-3 in LiPopt is already impractical. For example, on the simplest
network that we used, i.e., the single-hidden-layer neural network with 8 hidden units, using LiPopt-
3, one ℓ∞-FGL computation needs at least 200 hours projected by LiPopt. As a result, for all the
LiPopt-related experiments, we were only able to run LiPopt-2. As Latorre et al. (2020) pointed out,
the degree has to be at least the depth of the network to compute a valid bound, so we have to use at
least LiPopt-k for k-layer networks. LiPopt is unable to handle neural networks with more than two
layers because this requires LiPopt with degrees beyond 2. Even if we only consider LiPopt-2 on
two-layer networks, the running time is still much higher compared to GeoLIP. This demonstrates
the great advantage of GeoLIP in terms of scalability compared with LiPopt. If we compare LiPopt-
2 with GeoLIP on two-layer networks from Table 1, it is clear that GeoLIP produces more precise
results. For networks with depth greater than 2, we can only compare GeoLIP with the matrix-norm-
product method. As we can see from all experiments, GeoLIP’s estimation on the FGL is always
much lower than MP.

We have also shown that the two-layer network ℓ∞-FGL estimation from GeoLIP has a theoretical
guarantee with the approximation factor KG < 1.783 (Theorem 3.3). If we compare the two-layer
network results from GeoLIP and Sampling in Table 1, which is a lower bound of true Lipschitz
constant, the ratio is within 1.783. This validates our theoretical claim.

RQ2.
In Section 4, we have demonstrated the duality between NGeoLIP and LipSDP for the
ℓ2-FGL estimation on two-layer networks, even though the approaches appear drastically different.
The experiments show that on two-layer networks, LipSDP and NGeoLIP for ℓ2-FGL estimations
(Table 7 in the appendix), and DGeoLIP and NGeoLIP for ℓ∞-FGL estimations produce the same
values. These results empirically validate the duality arguments, and also all the related SDP pro-
grams.

2Another method was proposed by Chen et al. (2020), however, the code is not available and we are not

able to compare it with GeoLIP.

9

Applying SDP on intractable

SDP relaxation.
combinatorial optimization problem
the Max-Cut prob-
was pioneered by the seminal Goemans-Williamson algorithm for
lem (Goemans and Williamson, 1995).
For two-layer networks, we have reduced the FGL
estimation to the mixed-norm problem, and provide approximation algorithms with ratios com-
patible with the known optimal constants in the corresponding mixed-norm problems. Improving
them can be a very hard task. We also provide a compositional SDP interpretation of LipSDP-
Neuron. Although Pauli et al. (2022) demonstrated the ﬂaw in LipSDP-Network, our compositional
SDP interpretation shows that LipSDP-Neuron is correct.
In fact, from the compositional SDP
interpretation, the program is only constrained by the underlying perturbation geometry and the
layer-wise restriction from each hidden unit, so the constraints and objective exactly encode the
FGL-estimation problem without additional assumptions. Because often the SDP relaxation for
intractable problems gives the optimal known algorithms, we conjecture that GeoLIP and LipSDP
are also hard to improve on FGL estimations.

Latorre et al. (2020) used polynomial optimization to address the ℓ∞-FGL estimation. We argue
that approaching FGL-estimations from the perspective of polynomial optimization loses the ac-
curate characterization of this problem. For example, for two-layer networks, we have provided
constant approximation algorithms to estimate FGLs in both ℓ∞ and ℓ2 cases. However, for a gen-
eral polynomial optimization problem on a cube, we cannot achieve constant approximation. For
example, the maximum independent set of a graph can be encoded as a polynomial optimization
problem over a cube (Motzkin and Straus, 1965), but the maximum independent set problem cannot
be approximated within a constant factor in polynomial time unless P = NP (Trevisan, 2004).

7 Related work

Chen et al. (2020) employed polynomial optimization to compute the true Lipschitz constant of
ReLU-networks for ℓ∞-perturbations, and used Lasserre’s hierarchy of SDPs (Lasserre, 2001) to
solve the polynomial optimization problem. However, their approach is highly tailored to ReLU
networks, while ours, like LipSDP, can handle common activations, such as sigmoid and ELU.

Latorre et al. (2020) also proposed to use LiPopt to estimate the local Lipschitz constant. However,
estimating this quantity is not the problem studied in our work, and there are tools speciﬁcally
designed for local perturbations and the Lipschitz constant (Laurel et al., 2022; Zhang et al., 2019).

Lipschitz regularization of neural networks is an important task, and recent works (Aziznejad et al.,
2020; Bungert et al., 2021; Gouk et al., 2021; Krishnan et al., 2020; Terjék, 2020) have investigated
this problem. However, here we study a related but different problem, i.e., Lipschitzness measure-
ment of neural networks. Our work can motivate new Lipschitz regularization techniques.

8 Conclusion

In this work, we have provided a quantitative geometric framework for FGL estimations, and also
algorithms for the ℓ∞-FGL estimation. One important lesson is that when transferring techniques
from one perturbations to another ones, we should also transfer the underlying geometry. One future
work is to train smooth neural networks using the SDPs proposed in this paper.

Acknowledgments and Disclosure of Funding

The authors thank Vijay Bhattiprolu for introducing recent progress on the mixed-norm problems.
The work is partially supported by Air Force Grant FA9550-18-1-0166, the National Science Foun-
dation (NSF) Grants CCF-FMitF-1836978, IIS-2008559, SaTC-Frontiers-1804648, CCF-2046710
and CCF-1652140, and ARO grant number W911NF-17-1-0405. Zi Wang and Somesh Jha are
partially supported by the DARPA-GARD problem under agreement number 885000.

References

Aws Albarghouthi. 2021. Introduction to Neural Network Veriﬁcation. arXiv:2109.10317 [cs.LG]

10

Noga Alon and Assaf Naor. 2004. Approximating the Cut-Norm via Grothendieck’s Inequality.
In Proceedings of the Thirty-Sixth Annual ACM Symposium on Theory of Computing (Chicago,
IL, USA) (STOC ’04). Association for Computing Machinery, New York, NY, USA, 72–80.
https://doi.org/10.1145/1007352.1007371

MOSEK ApS. 2019.

The MOSEK optimization toolbox for MATLAB manual. Version 9.0.

http://docs.mosek.com/9.0/toolbox/index.html

Shayan Aziznejad, Harshit Gupta,

2020.
and Controlled Lipschitz Con-
IEEE TRANSACTIONS ON SIGNAL PROCESSING 68 (2020), 4688–4699.

Deep Neural Networks With Trainable Activations
stant.
https://doi.org/10.1109/TSP.2020.3014611

and Michael Unser.

Joaquim Campos,

Roberto Baldoni, Emilio Coppa, Daniele Cono D’Elia, Camil Demetrescu, and Irene Finocchi. 2018.

A Survey of Symbolic Execution Techniques. ACM Comput. Surv. 51, 3, Article 50 (2018).

Aharon Ben-Tal and Arkadi Nemirovski. 2001. Lectures on Modern Convex Optimization. Soci-
ety for Industrial and Applied Mathematics. https://doi.org/10.1137/1.9780898718829
arXiv:https://epubs.siam.org/doi/pdf/10.1137/1.9780898718829

Vijay Bhattiprolu, Mrinalkanti Ghosh, Venkatesan Guruswami, Euiwoong Lee, and Madhur Tulsiani.
2018. Inapproximability of Matrix p→q Norms. Electron. Colloquium Comput. Complex. 25
(2018), 37.

Vijay Bhattiprolu, Euiwoong Lee, and Madhur Tulsiani. 2022. Separating the NP-Hardness of the
Grothendieck Problem from the Little-Grothendieck Problem. In 13th Innovations in Theoretical
Computer Science Conference (ITCS 2022) (Leibniz International Proceedings in Informatics
(LIPIcs), Vol. 215), Mark Braverman (Ed.). Schloss Dagstuhl – Leibniz-Zentrum für Informatik,
Dagstuhl, Germany, 22:1–22:17. https://doi.org/10.4230/LIPIcs.ITCS.2022.22

Mark Braverman, Konstantin Makarychev, Yury Makarychev,

The Grothendieck Constant
IEEE 52nd Annual
https://doi.org/10.1109/FOCS.2011.77

is Strictly Smaller
Symposium on Foundations

than Krivine’s Bound.
Science.
of Computer

and Assaf Naor. 2011.
2011
In
453–462.

Jop Briët, Oded Regev,

and Rishi Saket. 2017.

Commutative Grothendieck Problem.
https://doi.org/10.4086/toc.2017.v013a015

the Non-
Theory of Computing 13, 15 (2017), 1–24.

Tight Hardness of

Sebastien Bubeck and Mark Sellke. 2021. A Universal Law of Robustness via Isoperimetry. In
Advances in Neural Information Processing Systems, A. Beygelzimer, Y. Dauphin, P. Liang, and
J. Wortman Vaughan (Eds.). https://openreview.net/forum?id=z71OSKqTFh7

Leon Bungert, René Raab, Tim Roith, Leo Schwinn, and Daniel Tenbrinck. 2021. CLIP: Cheap
Lipschitz Training of Neural Networks. In Scale Space and Variational Methods in Computer
Vision, Abderrahim Elmoataz, Jalal Fadili, Yvain Quéau, Julien Rabin, and Loïc Simon (Eds.).
Springer International Publishing, Cham, 307–319.

Tong Chen,

Jean B Lasserre, Victor Magron, and Edouard Pauwels. 2020.

Semial-
in
Information Processing Systems, H. Larochelle, M. Ranzato, R. Hadsell,
Inc., 19189–19200.

gebraic Optimization for Lipschitz Constants of ReLU Networks.
Neural
M.F. Balcan,
https://proceedings.neurips.cc/paper/2020/file/dea9ddb25cbf2352cf4dec30222a02a5-Paper.pdf

and H. Lin (Eds.), Vol. 33. Curran Associates,

In Advances

Djork-Arné Clevert, Thomas Unterthiner, and Sepp Hochreiter. 2016. Fast and Accurate Deep Net-
work Learning by Exponential Linear Units (ELUs). In 4th International Conference on Learning
Representations, ICLR 2016, San Juan, Puerto Rico, May 2-4, 2016, Conference Track Proceed-
ings, Yoshua Bengio and Yann LeCun (Eds.). http://arxiv.org/abs/1511.07289

Jeremy Cohen, Elan Rosenfeld, and Zico Kolter. 2019. Certiﬁed Adversarial Robustness via Ran-
domized Smoothing. In Proceedings of the 36th International Conference on Machine Learning
(Proceedings of Machine Learning Research, Vol. 97), Kamalika Chaudhuri and Ruslan Salakhut-
dinov (Eds.). PMLR, 1310–1320. https://proceedings.mlr.press/v97/cohen19c.html

11

Inc. CVX Research. 2020. CVX: Software for Disciplined Convex Programming, Version 2.2, Build

1148. http://cvxr.com/cvx.

Steven Diamond and Stephen Boyd. 2016. CVXPY: A Python-embedded modeling language for

convex optimization. Journal of Machine Learning Research 17, 83 (2016), 1–5.

Mahyar Fazlyab, Alexander Robey, Hamed Hassani, Manfred Morari, and George Pappas.
2019.
Efﬁcient and Accurate Estimation of Lipschitz Constants for Deep Neural Net-
works. In Advances in Neural Information Processing Systems, H. Wallach, H. Larochelle,
A. Beygelzimer, F. d'Alché-Buc, E. Fox, and R. Garnett (Eds.), Vol. 32. Curran Associates, Inc.
https://proceedings.neurips.cc/paper/2019/file/95e1533eb1b20a97777749fb94fdb944-Paper.pdf

Michel X. Goemans and David P. Williamson. 1995.

Improved Approximation Algorithms for
Maximum Cut and Satisﬁability Problems Using Semideﬁnite Programming. J. ACM 42, 6 (nov
1995), 1115–1145. https://doi.org/10.1145/227683.227684

Ian J. Goodfellow, Jonathon Shlens, and Christian Szegedy. 2015. Explaining and Harnessing Ad-
versarial Examples. In 3rd International Conference on Learning Representations, ICLR 2015,
San Diego, CA, USA, May 7-9, 2015, Conference Track Proceedings, Yoshua Bengio and Yann
LeCun (Eds.). http://arxiv.org/abs/1412.6572

Henry Gouk, Eibe Frank, Bernhard Pfahringer, and Michael J. Cree. 2021. Regularisation of Neu-
ral Networks by Enforcing Lipschitz Continuity. Mach. Learn. 110, 2 (feb 2021), 393–416.
https://doi.org/10.1007/s10994-020-05929-w

A. Grothendieck. 1956. Résumé de la théorie métrique des produits tensoriels topologiques. Bol.

Soc. Mat. São Paulo 8, 1-79 (1956)..

Gurobi Optimization,

LLC.

2022.

Gurobi Optimizer

Reference Manual.

https://www.gurobi.com

Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. 2016. Deep Residual Learning for Image
Recognition. In 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR).
770–778. https://doi.org/10.1109/CVPR.2016.90

Matthias Hein and Maksym Andriushchenko. 2017. Formal Guarantees on the Robustness of a
Classiﬁer against Adversarial Manipulation. In Proceedings of the 31st International Conference
on Neural Information Processing Systems (Long Beach, California, USA) (NIPS’17). Curran
Associates Inc., Red Hook, NY, USA, 2263–2273.

Yujia Huang, Huan Zhang, Yuanyuan Shi,

and Anima Anandkumar.
Training Certiﬁably Robust Neural Networks with Efﬁcient Local Lips-
Information Processing Systems.

In Thirty-Fifth Conference on Neural

2021.
chitz Bounds.
https://openreview.net/forum?id=FTt28RYj5Pc

J Zico Kolter,

Matt Jordan and Alexandros G Dimakis. 2020. Exactly Computing the Local Lipschitz Constant of
ReLU Networks. In Advances in Neural Information Processing Systems, H. Larochelle, M. Ran-
zato, R. Hadsell, M.F. Balcan, and H. Lin (Eds.), Vol. 33. Curran Associates, Inc., 7344–7353.
https://proceedings.neurips.cc/paper/2020/file/5227fa9a19dce7ba113f50a405dcaf09-Paper.pdf

Ravindran Kannan. 2010.

Spectral Methods for Matrices and Tensors. In Proceedings of
the Forty-Second ACM Symposium on Theory of Computing (Cambridge, Massachusetts,
USA) (STOC ’10). Association for Computing Machinery, New York, NY, USA, 1–12.
https://doi.org/10.1145/1806689.1806691

Guy Katz, Clark Barrett, David L. Dill, Kyle Julian, and Mykel J. Kochenderfer. 2017. Reluplex:
An Efﬁcient SMT Solver for Verifying Deep Neural Networks. In Computer Aided Veriﬁcation,
Rupak Majumdar and Viktor Kunˇcak (Eds.). Springer International Publishing, Cham, 97–117.

Subhash Khot. 2002. On the Power of Unique 2-Prover 1-Round Games. In Proceedings
of the Thiry-Fourth Annual ACM Symposium on Theory of Computing (Montreal, Quebec,
Canada) (STOC ’02). Association for Computing Machinery, New York, NY, USA, 767–775.
https://doi.org/10.1145/509907.510017

12

Diederik P. Kingma and Jimmy Ba. 2015. Adam: A Method for Stochastic Optimization.
In 3rd International Conference on Learning Representations, ICLR 2015, San Diego, CA,
USA, May 7-9, 2015, Conference Track Proceedings, Yoshua Bengio and Yann LeCun (Eds.).
http://arxiv.org/abs/1412.6980

Vishaal Krishnan, Abed AlRahman Al Makdah, and Fabio Pasqualetti. 2020. Lipschitz Bounds and
Provably Robust Training by Laplacian Smoothing. In Proceedings of the 34th International Con-
ference on Neural Information Processing Systems (Vancouver, BC, Canada) (NIPS’20). Curran
Associates Inc., Red Hook, NY, USA, Article 917, 12 pages.

J.L Krivine. 1979. Constantes de Grothendieck et fonctions de type positif sur les sphères. Advances
in Mathematics 31, 1 (1979), 16–30. https://doi.org/10.1016/0001-8708(79)90017-3

Alex Krizhevsky, Ilya Sutskever, and Geoffrey E. Hinton. 2017.

ImageNet Classiﬁcation
Commun. ACM 60, 6 (may 2017), 84–90.

with Deep Convolutional Neural Networks.
https://doi.org/10.1145/3065386

Lasserre.

B.
Problem of Moments.

Jean
the
(2001),
arXiv:https://doi.org/10.1137/S1052623400366802

796–817.

Global

2001.

Optimization

and
3
https://doi.org/10.1137/S1052623400366802

on Optimization

SIAM Journal

Polynomials

with

11,

Jean Bernard Lasserre. 2015. An Introduction to Polynomial and Semi-Algebraic Optimization. Cam-

bridge University Press. https://doi.org/10.1017/CBO9781107447226

Fabian Latorre, Paul Rolland, and Volkan Cevher. 2020. Lipschitz constant estimation of Neural
Networks via sparse polynomial optimization. In International Conference on Learning Repre-
sentations. https://openreview.net/forum?id=rJe4_xSFDB

Jacob Laurel, Rem Yang, Gagandeep Singh, and Sasa Misailovic. 2022. A Dual Number Abstraction
for Static Analysis of Clarke Jacobians. Proc. ACM Program. Lang. 6, POPL, Article 56 (jan
2022), 30 pages. https://doi.org/10.1145/3498718

Yann LeCun

2010.
http://yann.lecun.com/exdb/mnist/. (2010). http://yann.lecun.com/exdb/mnist/

and Corinna Cortes.

MNIST handwritten

digit

database.

Klas Leino, Zifan Wang, and Matt Fredrikson. 2021. Globally-Robust Neural Networks. In Interna-

tional Conference on Machine Learning (ICML).

Aleksander Madry, Aleksandar Makelov, Ludwig Schmidt, Dimitris Tsipras, and Adrian Vladu.
2018. Towards Deep Learning Models Resistant to Adversarial Attacks. In International Confer-
ence on Learning Representations. https://openreview.net/forum?id=rJzIBfZAb

MATLAB. 2021. 9.11.0.1837725 (R2021b) Update 2. The MathWorks Inc., Natick, Massachusetts.

Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg Corrado, and Jeffrey Dean. 2013. Distributed
Representations of Words and Phrases and Their Compositionality. In Proceedings of the 26th
International Conference on Neural Information Processing Systems - Volume 2 (Lake Tahoe,
Nevada) (NIPS’13). Curran Associates Inc., Red Hook, NY, USA, 3111–3119.

T. S. Motzkin and E. G. Straus. 1965.

of a Theorem of Turán.
https://doi.org/10.4153/CJM-1965-053-6

Maxima for Graphs and a New Proof
Canadian Journal of Mathematics 17 (1965), 533–540.

Vinod Nair and Geoffrey E. Hinton. 2010. Rectiﬁed Linear Units Improve Restricted Boltzmann
Machines. In Proceedings of the 27th International Conference on International Conference on
Machine Learning (Haifa, Israel) (ICML’10). Omnipress, Madison, WI, USA, 807–814.

Assaf Naor. 2013. Quantitative geometry.
ences 110, 48 (2013), 19202–19205.
arXiv:https://www.pnas.org/content/110/48/19202.full.pdf

Proceedings of

the National Academy of Sci-
https://doi.org/10.1073/pnas.1320388110

13

Yu

1998.

Nesterov.
optimization.
141–160.
arXiv:https://doi.org/10.1080/10556789808805690

Optimization Methods

Semideﬁnite

relaxation
and

quadratic
(1998),
https://doi.org/10.1080/10556789808805690

and
Software

nonconvex

1-3

9,

Nicolas Papernot, Patrick McDaniel, Somesh Jha, Matt Fredrikson, Z. Berkay Celik,
and Ananthram Swami. 2016.
The Limitations of Deep Learning in Adversarial Set-
tings. In 2016 IEEE European Symposium on Security and Privacy (EuroS&P). 372–387.
https://doi.org/10.1109/EuroSP.2016.36

Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan,
Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, Alban Desmaison, An-
dreas Kopf, Edward Yang, Zachary DeVito, Martin Raison, Alykhan Tejani, Sasank
Py-
Chilamkurthy, Benoit Steiner, Lu Fang, Junjie Bai, and Soumith Chintala. 2019.
In Advances
Torch: An Imperative Style, High-Performance Deep Learning Library.
in Neural Information Processing Systems 32, H. Wallach, H. Larochelle, A. Beygelz-
imer, F. d'Alché-Buc, E. Fox, and R. Garnett (Eds.). Curran Associates, Inc., 8024–8035.
http://papers.neurips.cc/paper/9015-pytorch-an-imperative-style-high-performance-deep-learning-library.pdf

Patricia Pauli, Anne Koch, Julian Berberich, Paul Kohler, and Frank Allgöwer. 2022. Training
Robust Neural Networks Using Lipschitz Bounds. IEEE Control Systems Letters 6 (2022), 121–
126. https://doi.org/10.1109/LCSYS.2021.3050444

Prasad Raghavendra and David Steurer. 2009. Towards Computing the Grothendieck Constant. In
Proceedings of the Twentieth Annual ACM-SIAM Symposium on Discrete Algorithms (New York,
New York) (SODA ’09). Society for Industrial and Applied Mathematics, USA, 525–534.

Aditi Raghunathan,

Certiﬁed Defenses
against Adversarial Examples. In International Conference on Learning Representations.
https://openreview.net/forum?id=Bys4ob-Rb

and Percy Liang. 2018.

Jacob Steinhardt,

R. Rietz. 1974. A proof of the Grothendieck inequality. Israel Journal of Mathematics 19 (1974),

271–276. https://doi.org/10.1007/BF02757725

Kevin Scaman and Aladin Virmaux. 2018. Lipschitz Regularity of Deep Neural Networks: Analysis
and Efﬁcient Estimation. In Proceedings of the 32nd International Conference on Neural Informa-
tion Processing Systems (Montréal, Canada) (NIPS’18). Curran Associates Inc., Red Hook, NY,
USA, 3839–3848.

Christian Szegedy, Wojciech Zaremba, Ilya Sutskever, Joan Bruna, Dumitru Erhan, Ian J.
Intriguing properties of neural networks. In 2nd
Goodfellow, and Rob Fergus. 2014.
International Conference on Learning Representations,
ICLR 2014, Banff, AB, Canada,
April 14-16, 2014, Conference Track Proceedings, Yoshua Bengio and Yann LeCun (Eds.).
http://arxiv.org/abs/1312.6199

Dávid Terjék. 2020. Adversarial Lipschitz Regularization. In International Conference on Learning

Representations. https://openreview.net/forum?id=Bke_DertPB

Luca Trevisan. 2004.

Inapproximability of Combinatorial Optimization Problems.

arXiv:cs/0409043 [cs.CC]

Zi Wang, Aws Albarghouthi, Gautam Prakriya, and Somesh Jha. 2022. Interval Universal Approxi-
mation for Neural Networks. Proc. ACM Program. Lang. 6, POPL, Article 14 (jan 2022), 29 pages.
https://doi.org/10.1145/3498675

Huan Zhang, Pengchuan Zhang, and Cho-Jui Hsieh. 2019. RecurJac: An Efﬁcient Recursive
Algorithm for Bounding Jacobian Matrix of Neural Networks and Its Applications. Pro-
ceedings of the AAAI Conference on Artiﬁcial Intelligence 33, 01 (Jul. 2019), 5757–5764.
https://doi.org/10.1609/aaai.v33i01.33015757

14

Checklist

1. For all authors...

(a) Do the main claims made in the abstract and introduction accurately reﬂect the pa-
per’s contributions and scope? [Yes] Our main claims in the abstract and introduction
accurately reﬂect the paper’s contributions and scope. In particular, we listed our con-
tributions in Section 1, and forward referenced each contribution to the corresponding
section in the paper.

(b) Did you describe the limitations of your work? [Yes] We clearly deﬁned what is the
quantity to measure and what the network is in Section 2, and what the assumptions
are for each theorem in Section 3.

(c) Did you discuss any potential negative societal impacts of your work? [Yes] We

discussed them in Appendix D.

(d) Have you read the ethics review guidelines and ensured that your paper conforms to
them? [Yes] We have read the ethics review guidelines. Because our work is to
measure the smoothness of neural networks (see Sections 1 and 2), and we only used
the standard MNIST dataset (see Section 6.1), the paper conforms to guidelines.

2. If you are including theoretical results...

(a) Did you state the full set of assumptions of all theoretical results? [Yes] We clearly

deﬁned the quantity to measure and the network structures in Sections 2 to 5.

(b) Did you include complete proofs of all theoretical results? [Yes] We provided impor-
tant intuition and ideas in the main paper (see Sections 3 to 5), and included complete
proofs in Appendix A.

3. If you ran experiments...

(a) Did you include the code, data, and instructions needed to reproduce the main experi-
mental results (either in the supplemental material or as a URL)? [Yes] We included
the code, data, and instructions as a URL.

(b) Did you specify all the training details (e.g., data splits, hyperparameters, how they
were chosen)? [Yes] We included major experimental setting in Section 6.1, and
detailed speciﬁcation in Appendix C.1.

(c) Did you report error bars (e.g., with respect to the random seed after running exper-
iments multiple times)? [N/A] Our experiments are deterministic. Given a neural
network, our algorithm always returns the same result.

(d) Did you include the total amount of compute and the type of resources used (e.g.,
type of GPUs, internal cluster, or cloud provider)? [Yes] This was provided in Ap-
pendix C.1.

4. If you are using existing assets (e.g., code, data, models) or curating/releasing new assets...

(a) If your work uses existing assets, did you cite the creators? [Yes] In terms of dataset,
we only used the standard MNIST, and cited the creators. All the tools used in the
paper were properly cited. See Section 6.1 and Appendix C.1.
(b) Did you mention the license of the assets? [Yes] See Appendix C.1.
(c) Did you include any new assets either in the supplemental material or as a URL? [Yes]

We included our code as a URL.

(d) Did you discuss whether and how consent was obtained from people whose data

you’re using/curating? [N/A]

(e) Did you discuss whether the data you are using/curating contains personally identiﬁ-

able information or offensive content? [N/A]

5. If you used crowdsourcing or conducted research with human subjects...

(a) Did you include the full text of instructions given to participants and screenshots, if

applicable? [N/A]

(b) Did you describe any potential participant risks, with links to Institutional Review

Board (IRB) approvals, if applicable? [N/A]

(c) Did you include the estimated hourly wage paid to participants and the total amount

spent on participant compensation? [N/A]

15

A Elided background, derivations and proofs

A.1 Additional analysis background

Gradient as operator. If a function g : Rm → R is a differentiable function at a ∈ Rm, then the
total derivative of g at a is

Dg(a) = [

(a), . . . ,

(a)],

∂g
∂x1

∂g
∂xm

and the gradient of g at a is ∇g(a) is the transpose of Dg(a). The linear approximation of g at a
is hDg(a), dxi. Equivalently, we can view the change of a function with respect to an inﬁnitesimal
perturbation as the inner product of ∇g(a) and dx. In this sense, the gradient acts as an operator on
the perturbation.

Differentiable activation. Because we want to upper bound the true Lipschitz constant, we only
need to show that the quantity considered in the paper indeed upper bounds the true Lipschitz con-
stant considered in the paper. If the activation function is differentiable, then the neural network f
is also differentiable, so Equation (3) is trivially true, as proved and applied in Latorre et al. (2020,
Theorem 1 and Equation 4).

ReLU activation. For ReLU networks, it is true if we have [a, b] = [0, 1]. One can consider the
(Clarke) generalized Jacobian as in Jordan and Dimakis (2020). At each input point, the Clarke
Jacobian is contained in {(W 1)T · diag(v2) · · · · · diag(vd)(W d)T | vi ∈ [a, b]ni}. Alternatively,
we can also use the perturbation propagation argument in Section 4 to see this upper bound. Note
that Raghunathan et al. (2018) used this interval representation for ReLU’s derivative.

Maximum over hypercube. Now we want to show that the optimization problems over hypercubes
considered in this work attain the maximum at the vertices. Without of loss of generality, let us
assume the hypercube is [−1, 1]n. Otherwise, we can transform the hypercube to [−1, 1]n. Let
A ∈ Rm×n, x ∈ Rn, y ∈ Rm, and z ∈ Rn.
We will use the following facts

1. ||x||1 = maxz∈{−1,1}n hx, zi;
2. ℓ∞ is the dual of ℓ1;
3. Let U ⊆ Rn. When maxx,z∈U hAx, Azi is well-deﬁned, we have maxx∈U hAx, Axi =

maxx,z∈U hAx, Azi.

The ﬁrst fact is from ||x||1 = |x1| + . . . + |xn| = maxz∈{−1,1}nhx, zi. The second fact is from
Hölder’s inequality for ﬁnite-dimensional vector space. For the third one, hAx, Azi is maximized
only when Ax = Az. Now we can show that the maximization problems considered in this paper
attain the maximum at the hypercube vertices.

max
||x||∞=1

||Ax||1 =

max
||x||∞=1,y∈{−1,1}m

hAx, yi

(From fact (1))

hx, AT yi

=

max
||x||∞=1,y∈{−1,1}m
||AT y||1

= max

y∈{−1,1}m

=

=

max
x∈{−1,1}n,y∈{−1,1}m

hx, AT yi

max
x∈{−1,1}n,y∈{−1,1}m

hAx, yi.

(From fact (2))

(14)

max
||x||∞=1

||Ax||2

2 = max

||x||∞=1

hAx, Axi

=

=

max
||x||∞=1,||z||∞=1

hAx, Azi

max
||x||∞=1,||z||∞=1

hAT Ax, zi.

(From fact (3))

16

Using the similar idea in Equation (14), we have

max
||x||∞=1

||Ax||2

2 =

max
x∈{−1,1}n,z∈{−1,1}n

hAx, Azi = max

x∈{−1,1}n

hAx, Axi.

More generally, in the bilinear forms considered above, if x = x1 ⊗ · · · ⊗ xd is generated by the
tensor product of variables over cubes, we can ﬁx one variable and write x as a matrix product,
and then move the ﬁxed variable to the hypercube vertices. We can repeat this process to move all
variables to the vertices.

A.2 Additional deﬁnitions

For any neural network f , let OP T (f ) be the optimal value of Equation (3). We say an algorithm
A is an approximation algorithm for Equation (3) with approximation ratio α > 1, if OP T (f ) ≤
A(f ) ≤ αOP T (f ).

A.3 SDP for the ∞ → 1 mixed-norm problem

Recall that for v ∈ Rm, ||v||1 = max||u||∞=1hu, vi. We can reformulate the mixed-norm problem
as follows:

max
x∈{−1,1}n

||Ax||1 =

max
(x,y)∈{−1,1}n+m

hAx, yi.

If we let z =

xT

yT

, we can have

(cid:0)

(cid:1)

max
(x,y)∈{−1,1}n+m

hAx, yi =

max
z∈{−1,1}n+m

z · B · zT ,

where B is a (m + n) × (m + n) matrix. The last m rows and ﬁrst n columns of B is A, and the

rest are 0: B =

0
0
A 0

.
(cid:19)

(cid:18)

The natural SDP relaxation of the ∞ → 1 mixed-norm problem is:

max tr(BX)

s.t. X (cid:23) 0, Xii = 1,i ∈ [n + m].

In other words, we treat X as the SDP matrix relaxed from the rank-1 matrix zT · z.

A.4 SDP relaxation and Grothendieck inequalities

In this work, we used the Grothendieck inequality as in Theorem 3.1:

max
ui,vj ∈B(H)

i,j
X

Aij hui, vjiH ≤ KG||A||∞→1,

for any A ∈ Rn×m; and the little Grothendieck inequality:

max
ui,vj ∈B(H)

(AT A)ij hui, vjiH ≤

i,j
X

π
2

||A||2

∞→2.

Notice that AT A is a PSD matrix.

As discussed in Appendix A.3,

The natural SDP relaxation is

||A||∞→1 =

max
z∈{−1,1}n+m

z · B · zT .

max tr(BX)

s.t. X (cid:23) 0, Xii = 1,i ∈ [n + m].

(15)

(16)

Because X (cid:23) 0, X = M M T for some M ∈ R(m+n)×d, where d ≥ 1. Let Mi be the i-th row vector
of M . Xij = hMi, Mji, and Xii = 1 means hMi, Mii = 1. As a result, tr(BX) =
i,j Aij Xij =

17

P

i,j AijhMi, MjiH , where H is the Hilbert space of Rd equipped with the canonical inner product.
Thus, Equation (15) implies that KG is the approximation ratio in the SDP relaxation for the mixed-
P
norm problem.

In contrast, in the mixed-norm problem, the variable to Bij is zizj, the product of two scalars. If
d = 1 in the SDP relaxation, M is a column vector, and X is a rank-1 matrix. In this case, the
SDP coincides with the combinatorial problem, because the inner product degenerates to the multi-
plication of two scalars. Hence, the SDP relaxation can be viewed as a continuous relaxation of a
discrete problem, and Equation (15) quantiﬁes this geometric transformation. Another interpretation
for the SDP relaxation is that SDP drops the rank-1 constraint in the quadratic formulation of the
mixed-norm problem.

A.5 Rescaling from 0-1 cube to norm-1 cube

Now let us show how we transform the 0-1 cube in Equation (4) to a norm-1 cube, and formulate
an equivalent optimization problem. As a result, we can apply the SDP program in Equation (5) to
compute an upper bound of the ℓ∞-FGL.
Let xi = (ti + 1)/2, where ti ∈ {−1, 1}. We have

max
x∈{0,1}n

||Ax||1

=

max
x∈{0,1}n,y∈{−1,1}m

yT Ax

=

max
t∈{−1,1}n,y∈{−1,1}m

1
2

yT A(t + en).

Let OP T1 be the optimal value of

max
(t,y)∈{−1,1}n+m

yT A(t + en).

Introduce another variable τ ∈ {−1, 1}, and let OP T2 be the optimal value of

max
(t,y,τ )∈{−1,1}n+m+1

yT A(t + τ en).

Lemma A.1. OP T1 = OP T2.

(17)

(18)

Proof. Clearly OP T2 ≥ OP T1.
Now if (ˆt, ˆy, τ = −1) is an optimal solution to Equation (18), then (−ˆt, −ˆy, τ = 1) is also an
optimal solution, so OP T2 ≤ OP T1.

Now let z = (t, τ ), and we can verify that yT A(t + τ en) = yT Bz, where B = (A Aen).
As a result, the semideﬁnite program to the ℓ∞-FGL constant is

max

1
2

tr(BX)

s.t. X (cid:23) 0, Xii = 1, i ∈ [n + m + 1],

where B is a (n + 1 + m) × (n + 1 + m) matrix, and B =

A.6 Proof of Theorem 3.2

0

0
A Aen

(cid:18)

0
0

.
(cid:19)

Proof. We will use the cube rescaling techniques introduced in Appendix A.5. Alon and Naor
(2004) showed that matrix cut-norm is MAXSNP-hard. We will show that if one can solve the
FGL estimation problem, then one can ﬁnd the cut norm of a matrix.
Given a matrix A, the cut norm of a matrix A ∈ Rm×n is deﬁned as

CN (A) =

max
x∈{0,1}n,y∈{0,1}m

hAx, yi.

18

We need to transform y from 0-1 cube to norm-1 cube, so similarly let yi = (ti + 1)/2, where
ti ∈ {−1, 1}. The we will have

CN (A) =

max
x∈{0,1}n,y∈{0,1}m

hAx, yi =

1
2

max
x∈{0,1}n,t∈{−1,1}m

hAx, (t + em)i.

Let B =

A
eT
mA

(cid:18)

(cid:19)

. From above we know that

max
x∈{0,1}n,t∈{−1,1}m

hAx, (t + em)i =

max
x∈{0,1}n,(t,τ )∈{−1,1}m+1

hBx, (t, τ )i.

One can then construct a two layer neural network, where the ﬁrst weight matrix is BT , and the
second weight matrix is (1, . . . , 1) ∈ Rn. Because the network we consider has only one output, the
second weight matrix is only a vector. The FGL of this network is exactly twice of the cut norm of
A.

A.7 Proof of Theorem 3.3

Proof. Let B =

. Combing Equations (5), (17) and (18), the approximation algo-

rithm for Equation (4) where q = 1 is induced by the following SDP program:

0

0
A Aen

(cid:18)

0
0
(cid:19)

max

1
2

tr(BX)

s.t. X (cid:23) 0, Xii = 1, i ∈ [n + m + 1].

A.8 Natural SDP relaxation of ℓ2-FGL estimation

Now let q = 2 in Equation (4), we will have:

max
y∈{0,1}n

||Ay||2.

In other words, we only need to solve the following program:

max
z∈{0,1}n

zT (AT A)z.

(19)

Let M = AT A, then M is a PSD matrix. We have demonstrated the scaling techniques in Ap-
pendix A.5. Let x ∈ {−1, 1}n+1, one can verify that

max
z∈{0,1}n

zT M z =

1
4

max
x∈{−1,1}n+1

xT ˆM x,

(20)

where ˆM =

M
n M eT
eT

M en
n M en

.
(cid:19)

(cid:18)

It is easy to verify that if M is PSD, ˆM is also PSD. Because M = AT A, ˆM = (A, Aen)T ·(A, Aen).
Now we can consider the following natural SDP relaxation to maxx∈{−1,1}n+1 xT ˆM x:

max tr( ˆM X)

s.t. X (cid:23) 0, Xii = 1, i ∈[n + 1].

(21)

This SDP relaxation admits a π
1998).

2 -approximation factor from Equation (16) (Rietz, 1974; Nesterov,

19

A.9 Proof of Theorem 3.4

Proof. Let ˆM =

, where M = AT A. Combining Equations (19) to (21), the

approximation algorithm for Equation (4) where q = 2 is induced by the following SDP program:

M
n M eT
eT

M en
n M en

(cid:18)

(cid:19)

max

1
2

q

tr( ˆM X)

s.t. X (cid:23) 0, Xii = 1, i ∈[n + 1].

(22)

A.10 Comparison with Raghunathan et al. (2018)

Raghunathan et al. (2018) formulated the following SDP to upper bound the ℓ∞-FGL on two-layer
neural networks:

max

1
4

tr(CX)

s.t. X (cid:23) 0, Xii = 1, i ∈[n + m + 1],

(23)

where C is a (m + n + 1) × (m + n + 1) matrix, and C =

0
0

0
0
A Aen



AT
n AT
eT
.
0 


If we compare Equations (6) and (23), C = B + BT . Because X is symmetric, tr(CX) = 2tr(BX).
Therefore, Equations (6) and (23) produce the same result.



A.11 Equivalence between the new optimization program and Equation (19)

Notice that because u ∈ R1×n, u∆σ(x) is a scalar. We can view each zi in Equation (19) as ∆σ(x)i
,
∆yi
the derivative of σ(x)i without the limit. Therefore, ∆σ(x)i = zi∆yi. Recall that from Section 4,
n
∆yi = wi∆x, so ∆σ(x)i = wizi∆x, then u∆σ(x) = ∆x
i uiziwi = ∆x(Az), where A =
W T diag(u) as deﬁned in Equation (4).

As a result, from Cauchy–Schwarz inequality, the above objective is

P

max
∆x,∆σ(x)

(u∆σ(x))2

(∆x)2 = max
z
s.t. z ∈[a, b]n.

(Az)2,

This demonstrates the equivalence between the new optimization program and Equation (19) when
[a, b] = [0, 1] for σ = ReLU.

B Polynomial optimization approach to the FGL estimation

We brieﬂy discuss the gradient approach to estimate the FGL. Let us use a three layer network as an
example:

f (x) = uσ(V σ(W x + b1) + b2),

where x ∈ Rl×1, W ∈ Rn×l, b1 ∈ Rn, V ∈ Rm×n, b2 ∈ Rm and u ∈ R1×m .
The formal gradient vector of this network is

where diag(y) ∈ Rn×n and diag(z) ∈ Rm×m. The i-th component of this vector is then

W T diag(y)V T diag(z)uT ,

m

n

k=1
X

j=1
X

(ukVkj Wji) · (yjzk).

20

Table 3: ℓ∞-FGL estimations of different methods for two-layer networks: DGeoLIP and NGeoLIP
induce the same estimations, and they are also close to the sampled lower bounds. In the meantime,
the result from GeoLIP is tighter than LiPopt’s result.

#UNITS DGEOLIP NGEOLIP

LIPOPT-2

MP

SAMPLE

BRUF

8
16
64
128
256

142.19
185.18
287.60
346.27
425.04

142.19
185.18
287.60
346.27
425.04

180.38
259.44
510.00
780.46
1011.65

411.90
578.54
1207.70
2004.34
2697.38

134.76
175.24
253.89
266.22
306.98

134.76
175.24
N/A
N/A
N/A

Therefore, the ℓp-norm estimation of the formal gradient ends up being a polynomial optimization
problem over a cube. For example, the ℓ1-norm (corresponding to ℓ∞-perturbations) of the gradient
is

l,n,m

where Tijk = WjiVkj uk.

max
xi∈{−1,1},yj∈{0,1},zk∈{0,1}

i,j,k=1
X

Tijk · xiyjzk,

(24)

This is essentially a tensor cut-norm problem, and it is an open problem whether there exists an
approximation algorithm within a constant factor to the general tensor cut-norm problem (Kannan,
2010). Notice that Equation (24) is not a general tensor-cut-norm problem, because the tensor is
generated from the weight matrices. For example, if we ﬁx j, the projected matrices of T are of
rank-1. Each vector in T:,j,: is the product of Vkj uk with the vector Wj::

∀k : T:,j,k = Wj:Vkj uk.

However, we do not have the theoretical technique to exploit the low-rank structure of these special
polynomial optimization problems. The perturbation analysis in Sections 4 and 5 can be viewed as
exploiting this structure in practice.

C Complete experimental speciﬁcations and results

GeoLIP is available at https://github.com/z1w/GeoLIP. To accommodate users who do not
have access to MATLAB, we also implement a version based on CVXPY (Diamond and Boyd,
2016). However, the MATLAB implementation works more efﬁciently in terms of memory and
speed, and we encourage users to work with the MATLAB version when possible. We conducted
all the GeoLIP-related experiments with the MATLAB version.

C.1 Experimental speciﬁcations

Tools. We obtain the LiPopt implementation from https://github.com/latorrefabian/lipopt,
under the MIT License.

Server speciﬁcation. All the experiments are run on a workstation with forty-eight Intel® Xeon®
Silver 4214 CPUs running at 2.20GHz, and 258 GB of memory, and eight Nvidia GeForce RTX
2080 Ti GPUs. Each GPU has 4352 CUDA cores and 11 GB of GDDR6 memory.

Dataset and split. We used the standard MNIST dataset from the PyTorch package (Paszke et al.,
2019). We used the “train” parameter in the MNIST function to split training and testing data.

C.2 Experimental results

Single hidden layer. We consider the ℓ∞-FGL estimation on two layer neural networks with
different numbers of hidden units. The results are summarized in Tables 3 and 4.

Multiple hidden layers. We consider the ℓ∞-FGL estimation on 3, 7, 8-layer neural networks.
Each hidden layer in the network has 64 ReLU units. The results are summarized in Tables 5 and 6.

21

Table 4: Average running time (in seconds) of different methods for two-layer-network ℓ∞-FGL
estimations: GeoLIPs are faster than LiPopt.

# HIDDEN UNITS DGEOLIP NGEOLIP

LIPOPT-2 BRUF

8
16
64
128
256

23.1
28.1
93.4
292.5
976.0

21.5
22.3
31.7
42.2
70.9

1533
1572
1831
2055
2690

< 0.1
4.8
N/A
N/A
N/A

Table 5: ℓ∞-FGL estimations of different methods for multi-layer networks: GeoLIP’s result is
much tighter than the matrix-product method, and LiPopt is unable to handle these networks.

# LAYERS GEOLIP MATRIX PRODUCT

SAMPLE

LIPTOPT

3
7
8

529.42
5156.5
8327.2

9023.65
1.423 ∗ 107
8.237 ∗ 107

311.88
1168.8
1130.6

N/A
N/A
N/A

ℓ2-FGL estimation. We measure the ℓ2-FGL on two-layer networks mainly to compare
whether Equation (7) and LipSDP produce the same result. Additionally, we also want to empir-
ically examine the approximation guarantee from Theorem 3.4. Still, we consider networks with 8,
16, 64, 128, 256 hidden nodes. The results are summarized in Tables 7 and 8.

C.3 Additional discussions

Duality.
The results in Table 7 show that the results of LipSDP and GeoLIP on two-layer-
network ℓ2-FGL estimation are exactly the same, which empirically demonstrates the duality be-
tween LipSDP and GeoLIP, as discussed in Section 4. Though Pauli et al. (2022) showed that
the most precise version of LipSDP is invalid for estimating an upper bound of ℓ2-FGL, our dual-
program argument shows that the less precise version of LipSDP is correct.

Precision. We showed that GeoLIP’s approximation factor for the ℓ2-FGL estimation on two layer
π
2 ≈ 1.253 in Theorem 3.4. The ℓ2-FGL from GeoLIP is very close to the sampled
networks is
lower bound of true Lipschitz constant in Table 7. On the other hand, because the result from
GeoLIP is an upper bound of FGL, and this result is not much greater than the sampled lower bound
of true Lipschitz constant, this empirically demonstrates that the true Lipschitz constant is not very
different from the FGL on two-layer networks.

p

Running time. If we compare the running time in Tables 4 and 8, the dual program takes more time
to solve than the natural relaxation. This is particularly true when the number of hidden neurons in-
creases. From the reported numbers of variables and equality constraints by CVX, the dual program
and natural relaxation have similar numbers. It is also observed that the CPU usage is higher when
the natural relaxation is being solved. We want to point out that the running time and optimization
algorithm are solver-dependent, and efﬁciently solving SDP is beyond the scope of this work. It
is an interesting future direction to exploit the block structure of the dual programs, and develop
algorithms that are compatible with those programs, because training smooth networks is a critical
task, and it is promising to incorporate the SDP programs.

ℓ2 versus ℓ∞ FGLs.
If we compare results from Tables 3 and 7, we can also ﬁnd that the
discrepancy between matrix product method and sampled lower bound is much smaller in the ℓ2
case. This could also explain why Gloro works for ℓ2-perturbations but not the ℓ∞ case in practice,
where Leino et al. (2021) used matrix-norm product to upper bound the Lipschitz constant of the
network in Gloro.

Sampling.
Sampling can only give a lower bound of the true Lipschitz constant, while we are
trying to estimate an upper bound. We use sampling as a sanity check to ensure that the SDP

22

Table 6: Average running time (in seconds) of GeoLIP for multi-layer-network ℓ∞-FGL estimations.

3-LAYER NET

7-LAYER NET

8-LAYER NET

120.9

284.3

329.5

Table 7: ℓ2-FGL estimations of different methods for two-layer networks: LipSDP and NGeoLIP
induce the same estimations, and these results are also close to the sampled lower bounds.

#UNITS NGEOLIP

LIPSDP

MP

SAMPLE

BRUF

8
16
64
128
256

6.531
8.801
12.573
15.205
18.590

6.531
8.801
12.573
15.205
18.590

11.035
13.936
22.501
30.972
35.716

6.527
8.795
11.901
13.030
14.610

6.527
8.799
N/A
N/A
N/A

method is at least sound and indeed provides an upper bound of the FGL. It is interesting to see
that in networks where we can brute-force enumerate all the activation patterns, sampling provides
very close results to the ground-truth ones. Notice that for those networks, there are only a few
hidden units (8 or 16), while we sample many (200,000) inputs, which might activate all or most
of the patterns. However, for networks with many activation nodes, it is infeasible to have a brute-
force enumeration of all the activation patterns, so we do not have the ground-truth information.
Sampling has no guarantee whether it can activate all patterns unless we have sampled all possible
inputs, which is also impractical.

Multi-layer network guarantees. The discrepancy between the results from sampling and GeoLIP
is relatively large for multi-layer networks. The approximation guarantee of GeoLIP is in terms of
the FGL, rather than true Lipschitz constant. It is unclear how large the gap between true Lipschitz
constant and the FGL is for multi-layer networks. Narrowing this gap is an interesting research
direction and beyond the scope of this work. We do not know whether for multi-layer networks,
GeoLIP has an approximation guarantee that is independent of the network. We leave this as an
open problem.

D Negative societal impacts

Our work is mainly theoretical and to measure an intrinsic mathematical property of neural networks,
and can beneﬁt the veriﬁcation of deep-learning systems. A misuse of our work can give a false sense
of safety, so the practical use of our work should be careful.

23

Table 8: Average running time (in seconds) of LipSDP and NGeoLIP for two-layer-network ℓ2-FGL
estimations.

# HIDDEN UNITS

LIPSDP NGEOLIP

BRUF

8
16
64
128
256

11.5
15.7
64.2
216.1
758.1

1.2
1.2
1.3
1.7
4.1

< 0.1
5.1
N/A
N/A
N/A

24

