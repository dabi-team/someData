Robust Deep Graph Based Learning for Binary
Classiﬁcation
1Minxiang Ye, 1Vladimir Stankovic, Senior Member, IEEE, 1Lina Stankovic, Senior Member, IEEE,
2Gene Cheung, Senior Member, IEEE

1

9
1
0
2

c
e
D
6

]

G
L
.
s
c
[

1
v
1
2
3
3
0
.
2
1
9
1
:
v
i
X
r
a

Abstract—Convolutional neural network (CNN)-based feature
learning has become state of the art, since given sufﬁcient training
data, CNN can signiﬁcantly outperform traditional methods for
various classiﬁcation tasks. However, feature learning becomes
more difﬁcult if some training labels are noisy. With traditional
regularization techniques, CNN often overﬁts to the noisy training
labels, resulting in sub-par classiﬁcation performance. In this
paper, we propose a robust binary classiﬁer, based on CNNs, to
learn deep metric functions, which are then used to construct
an optimal underlying graph structure used to clean noisy labels
via graph Laplacian regularization (GLR). GLR is posed as a
convex maximum a posteriori (MAP) problem solved via convex
quadratic programming (QP). To penalize samples around the
decision boundary, we propose two regularized loss functions for
semi-supervised learning. The binary classiﬁcation experiments
on three datasets, varying in number and type of features,
demonstrate that given a noisy training dataset, our proposed
networks outperform several state-of-the-art classiﬁers, including
label-noise robust support vector machine, CNNs with three
different robust loss functions, model-based GLR, and dynamic
graph CNN classiﬁers.

Index Terms—deep learning, graph Laplacian regularization,

binary classiﬁcation, semi-supervised learning

X
r
Yr

˙X, ˙Yr

E =

ei,j
{

}

W =

wi,j
{

}

LIST OF SYMBOLS

A set of observations
GLR iteration number
Labels corresponding to X at r-th GLR
iteration
A subset of Yr corresponding to the train-
ing samples in ˙X
Binary matrix that represents the edge con-
nectivity with each entry ei,j corresponding
the edge connecting node i to node j
A weight matrix with each entry wi,j as-
signed to the corresponding edge ei,j

),
D(
Z
·
f r(x)

HU(

)
·

Z

g(x)
xa
xp

xn

P

Q

αE, αW

γr

y1
i

y1
U

Θ

Π =

πi,j
{

}

πa,p, πa,n
Φ
κ
µr
β
εr

D

H

)
·

U(

) and
(
·

Shallow feature maps in
Observations associated with r-th GLR it-
eration
Observations associated with graph update
A random node a selected from X
A random node p selected from X, with
same label as xa
A random node n selected from X, with
opposite label as xa
A set of edges linked between nodes with
same label
A set of edges linked between nodes with
opposite label
Minimum margin between deep metric
based distances of P and Q edges for
LossE and LossW
Maximum number of nodes connected to
each node in graph Gr
Encoded vector corresponding to the label
yi for node i
Encoded matrix corresponding to the labels
for neighboring nodes
Activation function that estimates how
much attention is paid on each edge
Attention matrix with each entry πi,j cor-
responds to edge loss
Attentions on P and Q edges
Edge Attention Activation
Conditional number
Smoothness prior factor
Soft edge connectivity for graph update
Thresholds in Θ and Φ

G = (Ψ, E, W) A undirected graph that comprises a set of
nodes Ψ, edge matrix E and corresponding
weights W
Combinatorial graph Laplacian matrix
Adjacency matrix
Degree matrix
Maximum degree of node in G
Deep feature maps
Deep feature maps associated with r-th
GLR iteration

L
A
D
dmax
),
(
H
·
r(
),
C
·

)
·
)
·

U(
r(

D
V

The authors 1 are with the Department of Electronic & Electrical Engi-

neering, University of Strathclyde, Glasgow, G1 1XW, UK.

The author 2 is with the Department of Electrical Engineering & Computer

Science, York University, Toronto, M3J 1P3, Canada.

I. INTRODUCTION

Supervised and semi-supervised deep learning techniques
have shown excellent performance for feature extraction and
classiﬁcation tasks [1], but are particularly sensitive to the
quality of the training dataset, since they tend to overﬁt the
models when learning from incorrect labels [2]–[4]. Since
labels assigned to training samples are sometimes corrupted,
studies on classiﬁer learning with “noisy” labels are of prac-
tical importance [5]–[7].

Conventional approaches to overcome model overﬁtting,
based on various regularization techniques, e.g., l1
or l2-
norm penalty on weights [8], dropout [9], batch normalization
[10], skip-connections [11], [12] etc., are not effective in

−

 
 
 
 
 
 
mitigating the effects of incorrect labels. This has given rise
to different approaches to learning using noisy training labels.
These approaches can be grouped into methods based on:
(a) inserting additional “trusted” labels, e.g., [13], [14], and
(b) loss function correction, e.g., [15], [16], [17]. In [13], a
probabilistic model is integrated into the deep neural network
in [14], a loss
(DNN) to correct noisy labels. Similarly,
correction technique is introduced to mitigate the unreliability
of noisy training labels. However, these methods require clean
data to prevent the models from drifting away. A robust loss
function that is less sensitive to label outliers is introduced
in [15]. A combination of training labels and predicted labels
is used in [16] to avoid directly modeling the noisy training
labels, but requires pre-training to achieve good results. [17]
adds another softmax layer to further augment the correction
model via a noise transition matrix, which is hard to estimate
in practice, especially in multi-class classiﬁcation problems.

An alternative approach is to restore corrupted training
labels by representing them as piece-wise smooth signals on
graphs and applying a graph signal smoothness prior [18]–
[21]. In [22], an image denoising scheme is proposed using
graph Laplacian regularization (GLR), given either small- or
large-scale datasets. Building on [22], [23] integrates GLR into
the DNN to perform semi-supervised classiﬁcation of nodes
in a citation network, considering, during label propagation,
the local consistency of nodes with similar features. However,
these hybrid methods [23], [22], are only performed and
evaluated for a ﬁxed graph (i.e., a 2D grid of image pixels).
Without prior knowledge of the graph structure, more recently,
[24] proposes a regularized triplet loss correction function to
mitigate the effects of insufﬁcient clean training samples via
a sparse K-nearest neighbor (KNN) graph construction and
GLR.

In this paper, we further extend the previous studies on
binary classiﬁcation in the presence of noisy labels with DNN-
based classiﬁer learning using GLR without prior knowledge
of the underlying graph. We propose an end-to-end trainable
network for semi-supervised binary classiﬁcation, that incor-
porates an attention mechanism to capture important feature
information and guide learning through a regularized triplet
loss and iterative GLR, given a sufﬁcient number of training
samples, many of them corrupted. In summary,
the main
contributions of this paper are:

1) To avoid over-ﬁtting the classiﬁer, we propose a graph-
based regularized loss function that incorporates atten-
tion mechanism to regularize the proposed network;
2) To mitigate the effects of noisy training labels and
improve the reliability of classiﬁer learning, we develop
a graph-based semi-supervised classiﬁer that iteratively
performs both online denoising of training labels and
classiﬁcation at the same time;

3) To assign a degree of freedom for graph connectivity
learning that is robust to noisy labels, we introduce a
graph update procedure to better reﬂect the node-to-node
correlation based on convolution of iteratively updated
edges.

4) A complete

classiﬁcation
scheme, tested for a range of classiﬁcation tasks and

semi-supervised

binary

2

benchmarked against state-of-the-art classiﬁers in the
presence of noisy training labels.

Our proposed network is evaluated against several clas-
sic and state-of-art methods, designed speciﬁcally for the
“noisy label” problem, These methods include support vector
machines (SVM) [25], convolutional neural network (CNN),
dynamic graph CNN [26], deep metric-based KNN classiﬁer
[27], label noise robust SVM [28], GLR-based approach [20]
and CNN with robust loss corrections [4], [15], [16]. For the
ablation study of the overall network, we disable different
proposed components to study their effect on performance.

The rest of the paper is structured as follows. First, an
overview of related work is provided in Section II. Then,
in Section III, we introduce the notation and formulate the
“noisy label” classiﬁer learning problem. In Section IV, we
describe the implementation of the proposed network. Finally,
in Section V we evaluate and discuss the performance of the
proposed classiﬁer against state-of-the-art methods and present
ﬁndings of the ablation study for three different datasets.

II. RELATED WORK

In this section, we ﬁrst provide an overview of the related
work on robust graph-based classiﬁer learning and robust
DNN-based classiﬁer learning,
in the presence of “noisy
labels”. Then we discuss graph-based methods integrated with
DNN that do not consider noisy data. Finally, the weaknesses
of the state-of-the-art classiﬁers are discussed to motivate the
present work.

A. Robust Graph-based Learning

A label propagation method is proposed in [29] to evenly
spread, throughout the graph, label distributions from selected
labeled nodes, which are usually noisy and with heuris-
tic information. A KNN-sparse graph-based semi-supervised
learning approach is proposed in [30] to remove most of the
semantically-unrelated edges and adopt a reﬁnement strategy
to handle noisy labels.

To achieve more robust binary classiﬁcation, in [20], neg-
ative edge weights are introduced into the graph to separate
the nodes in two different clusters. A perturbation matrix is
found to perform generalized GLR for binary classiﬁcation via
iterative re-weighted least squares strategy [31]. The results
demonstrate the applicability of negative edge weights for
robust graph-based classiﬁer learning for small amount of
data without learning feature representation. We evaluate this
approach in this paper when sufﬁcient but noisy training labels
are provided.

B. Robust DNN-based Classiﬁer Learning

Many studies investigate methods to accommodate a wide
range of label noise levels and types, which often focus on
data augmentation, network design-based regularization and
loss correction. Data augmentation techniques have been suc-
cessfully used in [32]–[34] to automatically annotate unlabeled
samples and use these samples for retraining. In [35], training
examples are assigned weights by a proposed meta-learning

algorithm to minimize the loss on a clean unbiased validation
set based on gradient direction.

The effectiveness of dropout regularization for cleaning
noisy labels is shown in [36]. For image classiﬁcation, [37]
indicate that increasing the batch size and downscaling the
learning rate is a practical approach to mitigate the effects
of label noise for a DNN-based classiﬁer, given a sufﬁciently
large training set.

The loss correction approach of [15] proposes a boosting
algorithm ‘SavageBoost’ that is less sensitive to outliers and
converges faster than conventional methods, such as Ada, Real,
or LogitBoost. A noise-aware model is formulated in [38] to
handle label omission and registration errors for improving
labeling of aerial images. A dimensionality-driven learning
strategy is discussed in [4] to avoid overﬁtting by identifying
the transition from an early learning stage of dimensionality
compression to an overﬁtting learning stage when the local
intrinsic dimensionality steadily increases. Unlike the above
loss correction studies to handle noisy and incomplete labeling,
[16] use a combination of training labels and the prediction
from the current model to update the training targets and
perform weakly-supervised learning. Similarly, [39] integrate
the Expectation-Maximization (EM) algorithm into CNN to
detect and correct noisy labels, but require a properly pre-
trained model. [7] propose an iterative learning framework
to facilitate ‘robustness to label noise’ classiﬁer learning
by jointly performing iterative label detection, discriminative
feature learning and re-weighting.

C. Graph-based classiﬁer learning with DNN

Recent years have seen integration of graph-based learning
with deep learning. Given a ﬁxed graph structure, [40]–[42]
design CNNs for feature learning by feeding a polynomial
of the graph Laplacian. [43] adopt edge convolution to learn
combinational spatial features from neighboring nodes given a
ﬁxed skeleton graph. Based on the ideas of edge convolution,
[26] propose a deeper CNN model to learn the underlying
KNN graph structure of point cloud data by iteratively updat-
ing the graph. The results demonstrate the capability of edge
convolution for feature generalisation on point-cloud data.
[22] propose a deep image denoising framework that cou-
ples encapsulation of the fully-differentiable Graph Laplacian
regularization layer and learning 8-connected pixel adjacency
graph structures via CNNs. The results indicate that given a
small dataset, the method of [22] outperforms state-of-the-art
approaches.

The problem of insufﬁcient data or incorrect training labels
has not been investigated in the above graph-based hybrid
methods for classiﬁer learning. For incomplete or imprecise
categories of tags (observations) in the training samples, [44]
combine CNN and GLR using the sum of the cross-entropy
loss and the GLR term for multi-label image annotation, where
CNN is used to construct the fully-connected similarity-based
graph. Unlike [44], in our conference paper [24], we integrate
GLR into CNN with a graph-based loss correction function
to tackle the problem of insufﬁcient training samples through
semi-supervised graph learning.

3

D. Novelty with respect to reviewed literature

In this paper, we further extend our conference contribution
[24] to a more generalized end-to-end CNN-based approach
given noisy binary classiﬁer signal, to perform iteratively GLR
(similar to [22]) as a classiﬁer signal restoration operator, up-
date the underlying graph and regularize CNNs. Compared to
the previous graph-based classiﬁers [18], [20], [21], [23], [29],
[40], [41], [45], [46], by adopting edge convolution, iteratively
updating graph and operating GLR, we learn a deeper feature
representation, and assign the degree of freedom for learning
the underlying data structure. Given noisy training labels, in
contrast to the classical robust DNN-based classiﬁers [4], [7],
[15], [16], [35], [39], we bring together the regularization
beneﬁts of GLR and the beneﬁts of the proposed loss functions
to perform more robust deep metric learning. We further adopt
a rank-sampling strategy to ﬁnd those training samples with
high predictive performance that beneﬁts inference.

III. ROBUST DEEP GRAPH BASED CLASSIFIER LEARNING

In this section, we ﬁrst introduce notation and formulate
the robust classiﬁer learning problem following the related
work [18], [20]–[22], [27]. Then, we describe the main concept
behind the proposed Dynamic Graph Laplacian Regularization
(DynGLR) neural network that learns robust deep feature map
to effectively perform GLR when parts of the labeled data
available to train the model are noisy.

A. Problem Formulation and Notation

{

}

∈ R

∈
Y =

x1, . . . , xN

, where xi

Given observations X =

n,
the task of a binary classiﬁer is to learn
i = 1, . . . N ,
an approximate mapping function that maps each observa-
X into a corresponding binary discrete variable
tion x
, called classiﬁcation label, where
y
yi

y1, . . . , yN
{
, i = 1, . . . , N .
1, +1
}
˙Y0 =
Y, 0 < M <
{−
N , be a set of known (possibly noisy) labels that correspond
to instances ˙X =
X used for training. Let
x1, . . . , xM
{
} ⊂
˙Y0, 0N −M
Y0 =
M unknown
, where we set to zero all N
}
{
labels (to be estimated during testing).

}
M =
1, 1
}

y1, . . . , yM
{

∈
∈ {−
Let

} ⊂

−

Given X, the problem addressed in this paper, is to learn the
robust mapping function to assign a classiﬁcation label to each
˙Y0,
observation x
used for training the model, are incorrect.

X when some classiﬁcation labels y

∈

∈

{

}

∈ {

ei,j
{

1, . . . , N

, i, j
}

ψ1, . . . , ψN

Let G = (Ψ, E, W) be an undirected graph, where
Ψ =
is a set of nodes, each corresponding
to one instance in X, E =
, is
}
a matrix representing the edge connectivity of G; that is,
ei,j = 1 if there is an edge connecting vertices i and j
and ei,j = 0 otherwise; and each entry wi,j in the weight
matrix W =
corresponds to the
weight associated with edge ei,j. Then, Y0 can be seen as
a graph signal that indexes the graph G. The combinatorial
graph Laplacian matrix is given by L = D
A, where A
N adjacency matrix with each entry
is a symmetric N
ej,i), and D is a degree matrix
ai,j = max(wi,j
·
with entries di,i =

×
ei,j, wj,i
N
j=1 ai,j, and di,j = 0 for i

wi,j
{

1, . . . , N

= j.

∈ {

, i, j

−

}

}

·

(cid:80)

(cid:54)
Similarly to [27], we deﬁne triplets as observations
X corresponding to vertices
(xa, xp, xn), xa, xp, xn
Ψ, respectively, such that ya = yp
= yn, and
ψa, ψp, ψn
∈
˙Y. Let P be a set of all edges ea,p, such that
ya, yp, yn
∈
ya = yp, and Q a set of all edges ea,n, for which ya
= yn,
that is, P and Q are sets of all edges that connect nodes with
the same and opposite labels, respectively.

∈

Motivated by CNNs ability to extract discriminative features
and GLRs to ‘clean’ unreliable labels, we formulate graph-
based classiﬁer learning as a two-stage learning process:
(1) graph learning - extract deep feature maps,
i.e., ﬁnd
a deep metric function that returns the most discriminative
feature maps, and then generate an initial graph by learning
the underlying E to maximize/minimize similarity between
any two nodes in G that are indexed by the same/opposite
labels. (2) classiﬁer learning - iteratively reﬁne the graph and
effectively performing GLR to restore the corrupted classiﬁer
signal. In the following, we describe these two stages.

B. Initialization

Given the observation sets X and corresponding, potentially
˙Y0, the ﬁrst task is to learn a discriminative
0(
) and generate an initial underlying graph for
·

noisy labels,
feature map
the learnt feature map.

V

Let

di,j(

V

0) =

0(xi)

0(xj)
(cid:107)

2
2,

− V

(cid:107)V

∈

be the Euclidean distance between the corresponding feature
Ψ, let
maps. For a node ψi
i be a set containing all
E
vertices except ψi in ascending order with respect to the metric
0), k = 1, . . . , N
di,k(
i
E
containing the ﬁrst γi elements of
i contains
γi most correlated vertices to vertex ψi according to metric
di,k(

S
i, that is, the set
E

i be a subset of

= i. Let

1, k

0).

−

V

S

To effectively perform GLR, as in [20], [22], the underlying
graph should be a sparsely connected graph. To control the
sparsity of the resulting graph whilst maintaining connectivity,
we use an indicator operator to minimize the number of Q
edges. A typical option is a KNN indicator that keeps only
a maximum of γi edges for each individual node i, and sets
others to zero. That is, each graph edge ei,j is set to:

V

4

Fig. 1: The block diagram of the unweighted graph generation
scheme. V 0(·) is a CNN-based feature map learnt by minimizing
a loss function in order to reﬂect the node-to-node correlation. The
implementation of the proposed CNN and loss function is described
in Sec. IV-A.

Let r > 0 be the iteration index, initialized to 1, and let
Gr = (Ψ, Er−1, Wr) be the graph, with Wr to be learnt,
and Yr the noisy labels in the r-th iteration. Thus, G1 is an
N -node graph with edges set by (1). In the r-th iteration, each
vertex ψi is indexed by a label yr−1
Yr−1 (graph signal),
and is associated to a feature vector

i

∈
r(xi).
V

Typically, the edge weight is computed using a Gaussian

kernel function with a ﬁxed scaling factor σ, i.e., exp
(cid:107)xi−xj (cid:107)2
2
2σ2

−
, to quantify the node-to-node correlation. Instead
of using a ﬁxed σ as in [18], [20], [21], motivated by [47],
we introduce an auto-sigma Gaussian kernel function to assign
i,j in Gr by maximizing the margin between
edge weight wr
the edge weights assigned to P-edges and Q-edges, as:

(cid:17)

(cid:16)

σ∗ = arg max

σ

wr

i,j = exp

−

ω2

{ψa,ψp}
2σ2
(cid:17)
2
r(xj)
2
(cid:107)

exp

−
(cid:16)
r(xi)

(cid:104)
(cid:107)V

− V
2σ∗2

exp

−

−

(cid:16)

ω2

{ψa,ψn}
2σ2

(cid:17)(cid:105)

(cid:17)

(cid:16)

(2)
where ω{ψa,ψp} and ω{ψa,ψn} compute the mean Euclidean
distances between nodes connected by P-edges and Q-edges,
respectively. By setting the ﬁrst derivative to zero, we obtain
{ψa ,ψp}) , which

the resulting optimal σ∗ =
is used to assign edge weights of the graph.

ω2
{ψa ,ψn}−ω2
2 log(ω2

{ψa ,ψn }/ω2

{ψa ,ψp }

(cid:114)

Closely following related work [20], [22], we obtain the
restored classiﬁer signal by ﬁnding the smoothest graph signal
Yr as:

ei,j =

1,
0,

(cid:40)

j or ψj

if ψi
otherwise.

∈ S

i
∈ S

(1)

Yr = arg min

B

(

Yr−1
(cid:107)

B
(cid:107)

−

2 + µrBLrBT ).
2

(3)

0(

Once an optimal edge matrix E0 =

is computed through
) and (1), we obtain an initial undirected and unweighted
V
·
graph G0 = (Ψ, E0, W0 = 1). The block diagram is shown
in Fig. 1. Note that, in our implementation, we start with γ1 =
= γN = γ0, that is learnt as explained in Sec. IV-A.

e0
i,j}

{

· · ·

C. Proposed Classiﬁer Learning with Iterative Graph Update

If the noisy training labels are seen as a piece-wise smooth
graph signal, Y0, then one can iteratively perform GLR for
denoising the labels and performing semi-supervised classiﬁ-
cation, while reﬁning the set of deep feature maps and the
underlying graph.

The minimization above ﬁnds a solution that
is close to
the observed set of labels in the previous iteration, Yr−1,
while preserving piece-wise smoothness. To guarantee that
the solution Yr to the quadratic programming (QP) problem
(3) is numerically stable, we adopt Theorem 1 from [22] by
setting an appropriate conditional number κ. The maximum
value of the smoothness prior factor µr is then calculated as:
µr
max is the maximum degree
of the vertices in graph Gr. See Fig. 2(a).

max), where dr

max = (κ

1)/(2dr

−

Between each two GLR iterations, we use CNN to reﬁne the
feature map based on the denoised label signal, Yr−1 obtained
in the previous GLR iteration. See an illustration in Fig. 2(b)
for the graph update after r-th GLR iteration. We update the

CNNEq.(1)(cid:54)
(cid:54)
(cid:54)
5

Fig. 3: The overall block diagram of the proposed DynGLR-Net for
r = 2. Given observations X, G-Net (see Subsec. IV-A) ﬁrst learns an
initial undirected and unweighted KNN-graph by minimizing LossE.
The resulting edge matrix E0 is then used in the following, ﬁrst, GLR
iteration. The learnt shallow feature map f 1(X) = {X, ZD(X)} is
then used as input to learn a CNNC1 network for assigning weights to
the initial graph edges. Given a subset of, potentially noisy labels, ˙Y,
we perform GLR on the constructed undirected and weighted graph
to restore the labels. The resulting restored labels are used in the
following GLR iterations (see Subsec. IV-B). To assign the degree
of freedom for reﬁning graph connectivity, we update the graph edge
sets by minimizing LossW1 (HU) given neighbor information for
each node based on the resulting denoised classiﬁer signal from the
ﬁrst GLR iteration. We then reassign edge weights to the updated
graph edge sets to perform better node classiﬁcation in the second
GLR iteration (see Subsec. IV-C).

the corrupted classiﬁer signal Yr. (3) U-Net (graph update
network) used to reﬁne Er to better reﬂect
the node-to-
node correlation based on the restored classiﬁer signal
in
the previous iteration Yr−1. We clarify each network in the
following subsections.

A. G-Net

In order to learn the optimal metric space, as in [27], we
use a CNN, denoted by CNND, to learn a mapping function
). The detailed architecture of CNND is shown in Fig. 4.
(
·
For a random observation triplet (xa, xp, xn), such that
Q, we minimize the following loss

D

P and ea,n

ea,p
function to learn the feature map:

∈

∈

a,p,n

(
·

LossE = (cid:80)

2 + (cid:107)D(xa) − D(xp)(cid:107)2
2

(cid:2)αE − (cid:107)D(xa) − D(xn)(cid:107)2

) is a CNN-based feature map function,

(cid:3)
+,
(5)
where
to be
learnt, that returns a feature vector corresponding to the input
observation, αE is the minimum margin, and operator
+ is
a Rectiﬁed Linear Units (ReLU) activation function which is
(cid:3)
D(x) be the learnt feature map
equivalent to max(

D

(cid:2)

·

, 0). Let
·

Z

(a) The block diagram of
the proposed classiﬁer
scheme. CNN is learnt by minimizing the loss function
to better reﬂect the node-to-node correlation. The edge
matrix Er−1 is used as a mask when assigning edge
weights to construct adjacency matrix Ar. We perform
GLR to restore the corrupted classiﬁer signal Yr−1
given the resulting sparse graph Laplacian Lr and apply
the constrained smoothness prior factor µr to ensure the
numerical stability of QP solver. The implementation
of V r(·) varies depending on the data scale and the
dimension of the input observations. The output is the
new set of ‘denoised’ labels Yr.

(b) The block diagram of the proposed graph update
scheme. Based on the adjacency matrix Ar and restored
classiﬁer signal Yr, we learn a CNN to better reﬁne the
graph structure. The edge matrix Er is updated via (4)
and (1) based on both the previous restored classiﬁer
signal and the regularized deep feature map. The output
of this block is the new edge matrix Er that will be
used in the next iteration.

Fig. 2: The proposed graph-based classiﬁer and graph update scheme.
The green and blue colors denote input and output, respectively. The
implementation details are given in Sec. IV-B.

individual degree of Vertex i as:

˚er
i,j =

1,
0,

(cid:40)
N

if er
if er

i,j ∈
i,j ∈

γr
i =

˚er
i,j,

Pr & ar
Qr & ar

i,j > β
β,

i,j ≤

(4)

j=1
(cid:88)

where Pr and Qr sets are formed based on the denoised
classiﬁer signal Yr. The edge er
i,j is removed if it connects
vertices with opposite labels or the corresponding entry to
adjacency matrix is less than β, which is heuristically set to
0.1.

IV. PROPOSED NETWORK

Based on the concepts described in the previous section,
in this section we present the algorithmic ﬂow and describe
the architecture used to implement the proposed DynGLR
network.

The block diagram of the proposed DynGLR-Net is pre-
sented in Fig. 3. Our overall network consists of three sub-
networks: (1) G-Net (graph generator network) used to learn
a deep metric function to construct an undirected and un-
weighted KNN graph G0 = (Ψ, E0, W0 = 1). (2) W-
Net (graph weighting and classiﬁer network) used to assign
edge weights Wr for effectively performing GLR to restore

Eq.(2)Graph ConstructionQP Solver Eq.(3)CNNEq.(1)Eq.(4)CNNGraph GenerationKNN ClassifierGraph-based                                   ClassifierG-NetOptimizerupdateAccOptimizerGLR (r=1)updateW-NetGraph UpdateGraph-based ClassifierOptimizerupdateU-NetOptimizerGLR (r=2)updateW-Net6

loss weights, assigned to the edge connecting vertices ψi and
ψj. Note that
) is the feature map learnt by minimizing
(6).

r(
·

C

The architectures for r = 1 and r = 2 are shown in Fig. 5.
Since, at the ﬁrst iteration r = 1, we expect many noisy labels,
the residual network architecture will be different to the r > 1
case [49].

Fig. 5: CNNCr neural nets. The stride ρ1 and the number of neurons
ρ2, ρ3 vary depending on the input data (see details in Sec. V-A).

C

1(
·

The architecture presented in Fig. 5 (top) is used as the
), after G-Net, to construct the graph G1 =
feature map
(Ψ, E0, W1) by minimizing LossW1 (
1) taking as input
C
undirected graph G0 learned via G-Net. The input to CNNC1
is the concatenated observations X and “shallow feature maps”
learned via G-Net, i.e., the output of the second to last layer
of CNND as presented in Fig. 4, denoted by

D(X).

The r = 2 architecture is shown in Fig. 5 (bottom), with
observations X and “shallow feature maps” learned via U-
Net (described in the next subsection) to facilitate the regu-
2) based on the
larization of CNNC2 by minimizing LossW2(
C
denoised labels, convolution on both feature maps, denoised
classiﬁer signal and their differences across neighbors.

Z

Unlike [27], we introduce edge attention activation Θ in (6)
to dropout some edges with relatively large changes between
˙Yr and ˙Yr−1 via GLR. This helps to focus learning on edges
with high conﬁdence given noisy training labels. Therefore,
the overall training performance is better than the standard
dropout layer approach, which drops out random neuron units
in the network. We implement the edge attention activation Θ
and Φ as:

Φ( ˙yr−1
i

, ˙yr

if
if

i ) =

˙yr−1
1,
i −
˙yr−1
0,
i −
j ) =min(Φ( ˙yr−1
, ˙yr

εr
˙yr
i | ≤
> εr
˙yr
i |
i ), Φ( ˙yr−1

|
|
i

(cid:40)

j

, ˙yr

j )),

Θ( ˙yr−1
i

, ˙yr

i , ˙yr−1
j

, ˙yr

(7)
where threshold εr is used to determine whether a node’s label
can be trusted and also helps to control the sparsity of edge
attention matrix Π r. That is, if the difference between the
signal label in the previous and current iteration is large, this
means that the label most likely changed sign (from -1 to +1 or
vice versa) and is unreliable in this iteration. To reﬂect the fact
that there might be many noisy (unreliable) labels at the start
we heuristically set ε1 = 0.6 for the ﬁrst GLR iteration (see the
results in Sec. V). Since after applying GLR the classiﬁcation
signal is expected to be cleaner, we heuristically set threshold
ε2 = 0.15 for the second stacked W-Net during training to
ensure that we regularize CNNs with less concern about the
over-ﬁtting issue introduced by noisy labels.

Fig. 4: CNND neural network: ‘pool/q/w’ refers to a max-pooling
layer with q=pool size and w=stride size. ‘x conv y/ρ1’ refers to a 2D
convolutional layer with y ﬁlters each with kernel size x and stride
size ρ1. ‘fc x’ means the fully connected layer with x=number of
neurons. The stride size ρ1 varies depending on the input data (see
details in Sec. V-A).

output at the second to the last layer of CNND (see Fig. 4)
obtained by minimizing the loss (5).

The loss function (5) promotes a community structure graph
that has relatively small Euclidean distance between the feature
maps of vertices connected by the edges in P, and a large
distance between the vertices connected by the edges in Q,
while keeping a minimum margin αE between these two
distances.

Since we do not have a priori knowledge of the connectivity
of the nodes, we generate the initial graph as a fully connected
graph; justiﬁcation for starting with a fully connected graph
is provided in [48]. A sparse E0 minimizes the number of
Q edges by keeping only the connections with γ0 neighbors
per individual node. We adopt KNN-graph construction based
on (1), where optimal maximum number of neighbors γ0 is
obtained via grid-search by evaluating classiﬁcation accuracy
of the KNN classiﬁer (denoted by Acc in Fig. 3) using the
validation data with the same amount of noisy labels as the
training dataset. Note that, as we do not have any prior
knowledge of the optimal maximum degree of each individual
node, we initially set all γ0 = γ1 = . . . = γN . Once the
optimal number of neighbors γ0 is obtained, the resulting
graph edges E0 are used in the following section for pruning
edge weights during edge weighting and are updated based on
the regularized metric function and the difference between the
classiﬁer signal, before and after GLR.

B. W-Net

For assigning edge weights Wr to the graph Gr, we ﬁrst
employ a CNN, denoted by CNNCr , to learn a deep metric
function. We propose a robust graph-based triplet loss function
r, as:
to better learn feature map
LossWr (V) = (cid:88)

(cid:2)αW − (cid:107)V r(f r(xa)) − V r(f r(xn))(cid:107)2

V

2

ψa,ψp,ψn
·π(ψa,ψn|ea,n∈Q) + (cid:107)V r(f r(xa)) − V r(f r(xp))(cid:107)2
2
·π(ψa,ψp|ea,p∈P)
πψi,ψj }
{

Θ( ˙yr
{

i , ˙yr−1
i

j , ˙yr−1
j

, ˙yr

)
}

=

+

(cid:3)

.

Π r =

(6)
Θ is an edge attention activation function (see (7) for the
particular function we used) that estimates how much at-
˙Yr =
tention should be given to each edge and Yr =
{
is the restored classiﬁer signal ob-
[
−
tained via (3) starting from the classiﬁer signal in the previous
iteration, Yr−1. πψi,ψj is the amount of attention, i.e., edge

1, 1]N −M

1, 1]M , [

−

}

3x1 conv, 16, /(cid:2251)(cid:2778)fc 32pool, /2, /2fc 323x1 conv, 16, /(cid:2251)(cid:2778)pool, /2, /2fc 32(cid:28575)(cid:28575)3x1 conv, 16, /(cid:2251)(cid:2778)pool, /2, /2fc 64fc 323x1 conv, 16, /(cid:2251)(cid:2778)pool, /2, /2fc (cid:2251)(cid:2779)fc (cid:2251)(cid:2780)C. U-Net

Edge convolution has been proven recently to be a rich
feature representation method [26], [43], [50]. We adopt edge
convolution in deeper feature map learning, i.e., after GLR
is, given A1, from the ﬁrst GLR iteration,
r = 1. That
each node’s feature representation is enhanced by considering
observations of both X and classiﬁer signal Y1 from its six
nearest neighbors (set heuristically), which are most-likely to
have the same label.

Fig. 6: CNNHU neural nets. The stride size ρ1 and the number
of neurons ρ4, ρ5 vary depending on the input data (see details in
Sec. V-A).

Incorporating an additional CNN, denoted by CNNHU ,
shown in Fig. 6, we construct a richer feature representa-
tion
U(g(x)) to enhance the graph-based classiﬁer learning
H
with a single input to the network, g(x), comprising xi and
i , y1
y1
i denotes a tuple (yi, 0), if yi > 0 or
{
(0, yi) otherwise. y1
2 matrix formed by concatenating
×
y1
y1
i with the nearest six neighboring nodes. Finally, y1
i
is obtained by subtracting each row of y1

, where y1
U is a 6

y1
i }

U −

U −

U by y1
i .

Graph edge Er−1 is updated by (4) based on the learnt
) in order to better reﬂect the
regularized feature map
·
node-to-node correlation. The new edge matrix E1 and the
denoised classiﬁer signal Y1 are then used in the second
graph-based classiﬁer iteration.

U(

H

Though we can continue iterating between W-Net and U-
Net, in our practical implementation, only two iterations are
performed, to reduce computation complexity, since heuristi-
cally we did not observe improvement after r = 2 iterations.

V. SIMULATIONS

In this section, we present our simulation results, including
the ablation study, visualization results, and comparison of
the performance against different, classic and state-of-the-art
classiﬁers, under different label noise levels.

A. Simulation setup: Datasets, Benchmarks, Parameters and
Performance Measure

1) Datasets: We select

three binary-class datasets from
Knowledge Extraction based on Evolutionary Learning dataset
(KEEL) [51] that vary in the number and type of fea-
tures; these sets are, from low dimensional feature sets to
higher ones: (1) Phoneme: contains nasal (class 0) and oral

7

sounds (class 1), with 5404 instances (frames) described by
5 phonemes of digitized speech. (2) Magic: contains images
generated by primary gammas (class 0) from the images
of hadronic showers initiated by cosmic rays in the upper
atmosphere (class 1), where 19020 instances are generated for
simulation using the imaging technique, with each instance
containing 10 attributes to characterize simulated images. (3)
Spambase: to determine whether an email is spam (class 0)
or not (class 1), with 4597 email messages summarized by 57
particular words or characters.

2) Benchmark classiﬁers: We compare the proposed net-
work against 10 different classiﬁcation methods: (1) SVM with
radial basis function kernel (SVM-RBF) (2) a classical CNN,
consisting of two CNN blocks and two fully connected layers
afterwards, where each CNN block has a convolution layer, a
max pooling layer and one dropout layer (3) a graph CNN with
multiple graph construction blocks, where each block con-
structs a KNN graph based on multiple graph structures learnt
via edge convolution; batch normalization with decay is used
(called DynGraph-CNN [26]) (4) a KNN classiﬁer using CNN-
based deep metric learning (used CNN is the same as CNND)
[27] (called DML-KNN) (5) a rank-sampling [52] based KNN
classiﬁer using CNN-based deep metric learning (CNN used
is same as in DynGraph-CNN), where sampling is performed
on the training set by calculating the resulting classiﬁcation
accuracy using randomly sampled samples. We use the top 480
training samples with relatively high classiﬁcation accuracy in
the validation set. During inference, 480 selected training sam-
ples are divided into 6 equal-size batches by stratiﬁed random
sampling and the predictions using each batch are averaged
to obtain the ﬁnal decision (6) label noise robust SVM with
RBF kernel [28] (LN-Robust-SVM-RBF) (7) a graph-based
classiﬁer with negative edge weights assigned between the
centroid sample pairs and between the boundary sample pairs
(named Graph-Hybrid [20]) (8) a CNN network (same as in
DynGraph-CNN) trained by savage loss (called CNN-Savage
[15]) (9) a CNN network (same as in DynGraph-CNN) trained
by bootstrap-hard loss (called CNN-BootStrapHard [16]) (10)
a CNN network (same as in DynGraph-CNN) trained via
dimensionality-driven learning strategy (called CNN-D2L [4]).
Note that Classiﬁers (1)-(4) are classical methods for normal,
‘noise-free’, conditions, and methods (5)-(9) are proposed to
avoid overﬁtting under noisy training labels. All CNN-based
methods adopt l2 regularization for each layer. Similar to
Benchmark (5), we also adopt rank-sampling technique on
the training set to select trusted samples that will further
facilitate the predictive performance and consistency of our
model, denoted by ‘s’ appended to the model name.

3) Ablation study: To understand how different components
of our proposed architecture affect the results, we perform an
ablation study by removing some components. The resulting
architectures are denoted by DynGLR-G-number, where ‘G’
refers to Graph generation and ‘1’ refers to edge weighting,
‘2’ to GLR, and ‘3’ graph update. That is, the following
variants of the proposed scheme are compared: (1) DynGLR-
G-2: we import the unweighted graph G0 generated by G-
Net (see Fig. 1) into GLR for classiﬁcation. (2) DynGLR-G-
12: we assign weights to the unweighted graph G0 via an

fc (cid:2251)(cid:2781)-(cid:2251)(cid:2782)3x1 conv, 16, /(cid:2251)(cid:2778)pool, /2, /2fc (cid:2251)(cid:2781)zero pador(cid:28577)concat(cid:28663)(cid:28575)(cid:28663)fc(cid:2251)(cid:2782)concat8

of our iterative graph update scheme in Subsec. V-B4 by
visualizing the learnt underlying graph in spectral domain.
To analyze the impact of different components on the clas-
siﬁcation accuracy, we show the classiﬁcation error rates in
Subsec. V-B5 and discuss the ﬁndings of our ablation study
during the testing phase and show comparison with state-of-
the-art schemes.

Fig. 7: Mean Edge Weight Proportion (cid:37) for the proposed DynGLR-
Nets for all three datasets when 25% labels used for training are
wrong (denoted by ‘Noise’) or when all training labels are correct
(denoted by ‘Ref’ without use of GLR). G-2, G-12, G-1232, and
G-12312 schemes are described in the previous subsection.

1) Evaluating graph update block: First, we evaluate ability
of the graph update block to clear noisy labels. We use the
mean edge weight proportion measure deﬁned as:

(cid:37) =

N
p,n wp,n
1(wi,j > 0)

N
(cid:80)
i,j

,

(8)

(cid:80)

where ψp and ψn are two nodes with the opposite labels,
wp,n is the weight of the edge ep,n and 1(c) is an operator
that returns 1 if condition c is fulﬁlled, and 0 otherwise. The
results are shown in Fig. 7, which shows that, for all datasets,
the number of connections between nodes with opposite labels
decreases with iterations and becomes similar to that without
any noise, indicating that the graph update manages to restore
the noisy labels.

Clean Vertices

Noisy Vertices

adaptive Gaussian kernel function (see (Eq. 2)); the resulting
undirected and weighted graph is then used to perform node
classiﬁcation via GLR. (3) DynGLR-G-1232: we update the
graph edge sets by considering the neighbors of each node
with denoised classiﬁer signal and observed feature maps (see
Fig. 2); the resulting unweighted graph is then used for clas-
siﬁcation. (4) DynGLR-G-12312: we reassign weights to the
updated unweighted graph to effectively perform classiﬁcation;
we perform rank-sampling for all architectures to evaluate
the beneﬁts, denoted by ‘s’ appended to the name of each
proposed architecture.

4) Simulation setup, performance measure and parameters:
We design our experiments by splitting each dataset
into
training, validation and testing sets with 40%, 20%, 40%
of instances, respectively, with balanced class distribution.
To evaluate the robustness of different classiﬁcation methods
against label noise, we randomly sample subsets of instances
from both training and validation sets and reverse their la-
bels. Classiﬁcation error rates are measured by running 20
experiments per label noise level (0% to 25%). We use the
same random seed setting across all classiﬁcation methods and
remove all duplicated instances to ensure a fair comparison.
Hyper-parameters used for each experiment are obtained
from the validation sets by grid search. All used parameters
are listed in Table I.
TABLE I: Parameters for the proposed architectures. x ⇒ y means
that the learning rate decreases linearly from x to y with the epoch
number.

Hyper-parameters
ρ1, ρ2, ρ3, ρ4, ρ5
G-Net learning rates
G-Net epochs
W-Net(r=1) learning rates
W-Net(r=1) epochs
U-Net learning rates
U-Net epochs
W-Net(r=2) learning rates
W-Net(r=2) epochs

Phoneme

Magic

1,256,64,256,6 1,128,32,128,4

0.02⇒0.01
160
0.02⇒0.01
320
0.002⇒0.001
120
0.01⇒0.002
60

0.02⇒0.01
160
0.02⇒0.01
320
0.002⇒0.001
180
0.01⇒0.002
40

Spambase
2,32,32,64,6
0.02⇒0.01
60
0.02⇒0.012
80
0.002⇒0.001
100
0.02⇒0.01
40

To guarantee the solution Yr to (3) is numerically stable,
we heuristically set conditional number κ = 60 and µr =
0.67µr
max in all our experiments. We use the distance margin
αE = αW = 10 in both (5) and (6). For each epoch, we use
batch size of 16, each batch comprising 80 labeled instances
from training set and 20 unlabeled instances from validation
100 instances. This results
set; thus we randomly select 16
in 16 graphs to regularize training per epoch.

·

All CNNs are learnt by ADAM optimizer, classiﬁcation
accuracy and classiﬁer signal changes are used as metric
for rank-sampling for further improving the predictive per-
formance.

B. Results and Discussion

This section is organized as follows. As part of the ablation
study, Subsecs. V-B1 and V-B3 evaluate the ability of the
proposed schemes to clear the noisy labels and observation
samples during the training phase, respectively. We analyze the
sensitivity of hyper-parameters that affect the regularization
performance in Subsec. V-B2. We show the effectiveness

Fig. 8: Classiﬁer signal changes | ˙Yr−1 − ˙Yr| density visualization
for Phoneme dataset after the ﬁrst (top row) and second (bottom row)
GLR iteration during the training period. Vertices with clean/incorrect
labels are shown in the ﬁrst/second column. The intensity of the clas-
siﬁer signal changes across all experiments are represented through
colormaps.

In Eq. (7) we use a threshold to distinguish reliable nodes
from unreliable nodes. In order to evaluate the used approach

(cid:28610)(cid:28643)(cid:28637)(cid:28647)(cid:28633)(cid:28614)(cid:28633)(cid:28634)(cid:28610)(cid:28643)(cid:28637)(cid:28647)(cid:28633)(cid:28614)(cid:28633)(cid:28634)(cid:28610)(cid:28643)(cid:28637)(cid:28647)(cid:28633)(cid:28614)(cid:28633)(cid:28634)(cid:28603)(cid:28577)(cid:28580)(cid:28582)(cid:28611)(cid:28609)(cid:28614)(cid:28618)(cid:28616)(cid:28611)(cid:28609)(cid:28613)(cid:28618)(cid:28613)(cid:28611)(cid:28609)(cid:28614)(cid:28620)(cid:28619)(cid:28611)(cid:28609)(cid:28613)(cid:28615)(cid:28611)(cid:28609)(cid:28614)(cid:28615)(cid:28616)(cid:28611)(cid:28609)(cid:28612)(cid:28616)(cid:28614)(cid:28603)(cid:28577)(cid:28581)(cid:28582)(cid:28611)(cid:28609)(cid:28614)(cid:28617)(cid:28618)(cid:28611)(cid:28609)(cid:28613)(cid:28617)(cid:28614)(cid:28611)(cid:28609)(cid:28614)(cid:28620)(cid:28614)(cid:28611)(cid:28609)(cid:28613)(cid:28613)(cid:28618)(cid:28611)(cid:28609)(cid:28614)(cid:28613)(cid:28616)(cid:28611)(cid:28609)(cid:28612)(cid:28613)(cid:28619)(cid:28603)(cid:28577)(cid:28581)(cid:28582)(cid:28583)(cid:28582)(cid:28611)(cid:28609)(cid:28614)(cid:28611)(cid:28619)(cid:28611)(cid:28609)(cid:28613)(cid:28618)(cid:28612)(cid:28611)(cid:28609)(cid:28613)(cid:28620)(cid:28616)(cid:28611)(cid:28609)(cid:28612)(cid:28619)(cid:28616)(cid:28611)(cid:28609)(cid:28613)(cid:28613)(cid:28618)(cid:28611)(cid:28609)(cid:28612)(cid:28615)(cid:28619)(cid:28603)(cid:28577)(cid:28581)(cid:28582)(cid:28583)(cid:28581)(cid:28582)(cid:28611)(cid:28609)(cid:28613)(cid:28619)(cid:28612)(cid:28611)(cid:28609)(cid:28613)(cid:28616)(cid:28612)(cid:28611)(cid:28609)(cid:28613)(cid:28620)(cid:28612)(cid:28611)(cid:28609)(cid:28612)(cid:28618)(cid:28620)(cid:28611)(cid:28609)(cid:28613)(cid:28612)(cid:28620)(cid:28611)(cid:28609)(cid:28612)(cid:28612)(cid:28618)(cid:28640)(cid:28660)(cid:28666)(cid:28668)(cid:28662)(cid:28595)(cid:28603)(cid:28612)(cid:28611)(cid:28604)(cid:28646)(cid:28675)(cid:28660)(cid:28672)(cid:28661)(cid:28660)(cid:28678)(cid:28664)(cid:28595)(cid:28603)(cid:28616)(cid:28618)(cid:28604)(cid:28643)(cid:28667)(cid:28674)(cid:28673)(cid:28664)(cid:28672)(cid:28664)(cid:28595)(cid:28603)(cid:28616)(cid:28604)00.10.20.30.4(cid:49)(cid:82)(cid:76)(cid:86)(cid:72)(cid:53)(cid:72)(cid:73)(cid:49)(cid:82)(cid:76)(cid:86)(cid:72)(cid:53)(cid:72)(cid:73)(cid:49)(cid:82)(cid:76)(cid:86)(cid:72)(cid:53)(cid:72)(cid:73)(cid:51)(cid:75)(cid:82)(cid:81)(cid:72)(cid:80)(cid:72)(cid:48)(cid:68)(cid:74)(cid:76)(cid:70)(cid:54)(cid:83)(cid:68)(cid:80)(cid:69)(cid:68)(cid:86)(cid:72)G-2G-12G-1232G-12312152103155206258320Epochs0.000.200.600.801.001.40ﬂﬂﬂ˙Y1−˙Y0ﬂﬂﬂ152103155206258320Epochs0.000.200.600.801.001.40ﬂﬂﬂ˙Y1−˙Y0ﬂﬂﬂ191929394960Epochs0.000.100.200.30ﬂﬂﬂ˙Y2−˙Y1ﬂﬂﬂ191929394960Epochs0.000.100.200.30ﬂﬂﬂ˙Y2−˙Y1ﬂﬂﬂ0%10%20%30%40%|

−

˙Yr

˙Yr−1
|

and to set thresholds, we show the change of the classiﬁer
signal
during the ﬁrst two iterations in Fig. 8.
We can see that when all the labels are clean (left column)
the difference between the signals before and after GLR is
mainly below 0.6 and 0.15, in the ﬁrst and the second iteration,
respectively. Thus, by setting the thresholds at ε1
0.6
and ε2
0.15 for the ﬁrst and the second GLR iteration,
respectively, we can distinguish the vertices with potentially
noisy labels. Similar observations are made for the Spambase
and Magic datasets.

≈

≈

Heuristically, we observed that as more GLR iterations are
performed, the overlap between the clean and noisy vertices
distribution of classiﬁer signal changes is high, resulting in
reduced ability to use thresholding for distinguishing if a node
is sufﬁciently cleaned. That is why in all our experiments, to
reduce complexity, we perform the graph update only after the
ﬁrst iteration, i.e., for r=1. We next assess sensitivity of the
network to threshold values.

(a) GLR (r=1)

(b) GLR (r=2)

Fig. 9: Classiﬁcation Error Rate (%) for Spambase dataset using
different ε1,2 in DynGLR-G-12312.

r−1, ˙yi

2) Hyper-parameter sensitivity: We use attention activation
(Eq. 7) to detect the label of vertex ψi as possibly noisy
r) = 1. To analyze the sensitivity of hyper-
if Φ( ˙yi
parameters ε1 and ε2 (thresholds) in (Eq. 7), we show the
classiﬁcation error rate for the Spambase dataset during train-
ing using different values of ε1 and ε2 in Fig. 9. We observe
that the thresholds ε1 = 0.6, ε2 = 0.15, from the density
visualization of classiﬁer signal changes
of Fig. 8,
are appropriate to improve the regularization of CNNs. We also
show that the classiﬁcation error rate reduces from ﬁrst GLR
iteration to second GLR iteration.

˙Yr−1
|

˙Yr

−

|

Fig. 10: The mean noise level of the training labels after GLR is
performed in all proposed DynGLR-Nets when 25% labels used for
training are incorrect.

3) Evaluating Rank-sampling: To evaluate the denoising
effects of GLR, in Fig. 10, we show the mean noise level
of the training labels after GLR is performed across all our
proposed architectures with and without sampling. It is clear

9

from Fig. 10, for all three datasets, that the mean noise level
is lower with rank-sampling than without. This conﬁrms that
with rank-sampling, one can further reduce the effects of
noisy training labels by dropping out the less reliable training
samples.

4) Graph spectrum visualization: Graph Fourier Transform
(GFT) is another approach to represent the smoothness and
connectivity of an underlying graph. As in [53], we visualize
the magnitude of the GFT coefﬁcients in Figures 11 and 12
along the graph update iterations.

Unweighted

Weighted

Fig. 11: The magnitude of the Graph Fourier Transform coefﬁcients
for Phoneme Dataset. The density of each Eigenvalue λ(cid:96) across all
experiments on the testing sets is represented through colormaps.
Top row shows the result after initialization and before GLR (G-Net
output) and the second and third row show the result after the ﬁrst
(r = 1) and the second iteration (r = 2), respectively.

In accordance with [53], where it is shown that the mag-
nitude of GFT coefﬁcients decay rapidly for a smooth signal,
in our results, we can clearly observe that the magnitude
of GFT coefﬁcients is decaying more rapidly along spectral
frequencies once the graph is updated (in iteration r = 1). Fur-
thermore, comparing the visualization results in the ﬁrst and
the second column, we can see that the graph weighting (W-
Net) smooths the graph data, with low frequency components
becoming more prominent.

5) Classiﬁcation Error Rate Comparison: Tabs. II–IV show
comparison between the proposed DynGLR networks and all
the benchmarks in terms of classiﬁcation error rate. From the
tables, it can be seen that the proposed DynGLR networks
outperform all the benchmarks.

11020304050607080Epochs9.779.9710.1710.3710.5710.7810.9811.18ClassiﬁcationErrorRate(%)ε1=2.0(withoutedgeattention)ε1=1.0ε1=0.8ε1=0.7ε1=0.6ε1=0.41481216202428323640Epochs9.129.189.259.319.389.449.519.57ClassiﬁcationErrorRate(%)ε2=2.0(withoutedgeattention)ε2=0.25ε2=0.20ε2=0.15ε2=0.10ε2=0.050.160.170.18G-2G-12G-1232(cid:51)(cid:75)(cid:82)(cid:81)(cid:72)(cid:80)(cid:72)G-12312(cid:58)(cid:76)(cid:87)(cid:75)(cid:82)(cid:88)(cid:87)(cid:3)(cid:54)(cid:68)(cid:80)(cid:83)(cid:79)(cid:76)(cid:81)(cid:74)(cid:58)(cid:76)(cid:87)(cid:75)(cid:3)(cid:54)(cid:68)(cid:80)(cid:83)(cid:79)(cid:76)(cid:81)(cid:74)0.140.150.160.17G-2G-12G-123120.070.090.11G-2G-12G-12312G-1232(cid:48)(cid:68)(cid:74)(cid:76)(cid:70)G-1232(cid:54)(cid:83)(cid:68)(cid:80)(cid:69)(cid:68)(cid:86)(cid:72)010203040506070λℓ0246810|ˆf(λℓ)|010203040506070λℓ0246810|ˆf(λℓ)|010203040506070λℓ0246810|ˆf(λℓ)|010203040506070λℓ0246810|ˆf(λℓ)|010203040506070λℓ0246810|ˆf(λℓ)|010203040506070λℓ0246810|ˆf(λℓ)|0.00%0.24%0.48%0.72%0.97%Unweighted

Weighted

TABLE III: Classiﬁcation Error Rate (%) For Magic Dataset

10

% label noise
SVM-RBF
CNN
DynGraph-CNN
DML-KNN
DML-KNN-s

0
18.42
16.45
17.74
15.33
15.33
LN-Robust-SVM-RBF∗ 18.57
24.82
16.31
16.34
16.34
15.35
15.22
15.22
15.22
15.22
15.22

Graph-Hybrid∗
CNN-Savage∗
CNN-BootStrapHard∗
CNN-D2L∗
DynGLR-G-2∗
DynGLR-G-12∗
DynGLR-G-12s∗
DynGLR-G-1232∗
DynGLR-G-12312∗
DynGLR-G-12312s∗

5
19.07
16.87
18.69
15.51
15.51
18.70
25.92
16.74
16.89
16.79
15.51
15.47
15.47
15.46
15.46
15.45

10
19.63
16.91
19.33
15.80
15.78
18.80
27.23
16.99
17.02
17.21
15.77
15.68
15.68
15.66
15.66
15.65

15
20.18
17.62
21.05
15.94
15.89
19.05
28.84
17.41
17.46
17.48
15.94
15.85
15.83
15.85
15.85
15.83

20
20.61
18.09
24.15
16.83
16.58
19.39
30.79
18.10
18.13
18.20
16.75
16.60
16.52
16.58
16.55
16.49

25
21.13
18.86
27.40
18.29
17.08
19.82
33.23
18.87
18.65
18.75
18.03
17.33
16.91
17.18
17.17
16.85

TABLE IV: Classiﬁcation Error Rate (%) For Spambase Dataset

% label noise
SVM-RBF
CNN
DynGraph-CNN
DML-KNN
DML-KNN-s
LN-Robust-SVM-RBF∗
Graph-Hybrid∗
CNN-Savage∗
CNN-BootStrapHard∗
CNN-D2L∗
DynGLR-G-2∗
DynGLR-G-12∗
DynGLR-G-12s∗
DynGLR-G-1232∗
DynGLR-G-12312∗
DynGLR-G-12312s∗

0
8.09
7.69
8.33
7.84
7.81
7.84
18.34
8.04
7.69
7.73
7.84
7.73
7.72
7.65
7.55
7.55

5
8.50
8.27
9.01
8.41
7.35
8.38
19.19
8.36
8.30
8.46
8.37
8.13
8.11
8.05
7.99
7.94

10
8.98
8.89
10.45
8.49
8.42
8.89
20.37
8.90
9.24
9.05
8.42
8.37
8.35
8.22
8.21
8.18

15
9.75
9.85
12.68
8.98
8.86
9.68
21.85
9.80
9.68
9.87
8.95
8.78
8.75
8.67
8.64
8.61

20
10.68
10.8
16.78
9.83
9.74
10.56
24.17
10.42
10.05
10.96
9.85
9.44
9.35
9.15
9.01
8.96

25
11.49
12.47
22.34
11.02
10.34
11.32
26.68
12.13
12.01
12.17
10.83
9.82
9.63
9.56
9.18
9.13

base dataset, due to iterative design, incorporating edge
weighting.

• By comparing DynGLR-G-2 and DML-KNN, we can ob-
serve performance improvements due to replacing KNN-
based classiﬁcation with GLR; larger gains can be ob-
served as noise level increases.

• Semi-supervised classiﬁers DML-KNN and all DynGLRs
with sampling (DynGLR-G-12s and DynGLR-G-12312s)
beneﬁt from rank-sampling, which also reduces the scale
of training set without sacriﬁcing the performance.

Furthermore, our ﬁndings are that the importance of the
following algorithmic steps, in order of largest to least im-
to the performance can be summarized as: (I)
portance,
iterative graph update Eq.(4) - disconnect Q-edges and con-
nect/reconnect P-edges based on the restored labels after
each GLR iteration to reﬁne the graph structure, (II) edge
convolution operation - performing feature and denoised la-
bel aggregation on neighboring nodes provides richer and
smoother inputs and results in a spatially sparse graph, (III)
edge attention function (Eq. 7) - to better reﬂect node-to-node
correlation, regularizing CNN training by weighting the edge
loss based on classiﬁer signal changes before and after GLR.

VI. CONCLUSIONS

Fig. 12: The magnitude of the Graph Fourier Transform Coefﬁcients
for Spambase Dataset. The density of each Eigenvalue λ(cid:96) across
all experiments on the testing sets is represented through colormaps.
Top row shows the result after initialization and before GLR (G-Net
output) and the second and third row show the result after the ﬁrst
(r = 1) and the second iteration (r = 2), respectively.

TABLE II: Classiﬁcation Error Rate (%) For Phoneme Dataset

% label noise
SVM-RBF
CNN
DynGraph-CNN
DML-KNN
DML-KNN-s

0
18.33
17.58
17.66
17.04
17.01
LN-Robust-SVM-RBF∗ 18.57
22.01
17.52
17.46
17.47
17.04
16.93
16.89
16.90
16.87
16.87

Graph-Hybrid∗
CNN-Savage∗
CNN-BootStrapHard∗
CNN-D2L∗
DynGLR-G-2∗
DynGLR-G-12∗
DynGLR-G-12s∗
DynGLR-G-1232∗
DynGLR-G-12312∗
DynGLR-G-12312s∗

5
18.75
17.77
19.04
17.54
17.49
19.42
23.77
17.72
17.72
17.80
17.50
17.36
17.36
17.29
17.19
17.18

10
19.13
18.00
20.80
17.82
17.71
19.65
25.58
18.04
18.00
17.96
17.70
17.64
17.62
17.36
17.34
17.32

15
19.57
18.57
22.44
18.58
18.43
19.70
27.97
18.51
18.31
18.41
18.34
18.23
18.21
18.16
18.03
17.91

20
20.07
19.01
25.20
19.64
19.24
20.03
30.33
19.02
18.84
18.91
18.81
18.52
18.52
18.48
18.38
18.24

25
20.87
20.00
28.84
21.00
20.41
20.28
33.39
19.87
20.15
20.04
20.03
19.59
19.54
19.47
19.43
19.18

C. Summary of ﬁndings

The DynGLR-G networks at the bottom of performance
tables Tabs. II–IV show the outcomes of the ablation study.
Speciﬁcally:

• DynGLR-G-12 consistently outperforms DynGLR-G-2,

showing the effect of edge-weighting.

• The improvement due to graph update can be observed

between DynGLR-G-12 and DynGLR-G-1232

• By comparing DynGLR-G-1232 and DynGLR-G12312,
we observe small gains, except for low noise in Spam-

In this paper, we introduce an end-to-end iterative graph-
based deep learning architecture design to tackle the overﬁtting

0102030405060λℓ0246810|ˆf(λℓ)|0102030405060λℓ0246810|ˆf(λℓ)|0102030405060λℓ0246810|ˆf(λℓ)|0102030405060λℓ0246810|ˆf(λℓ)|0102030405060λℓ0246810|ˆf(λℓ)|0102030405060λℓ0246810|ˆf(λℓ)|0.00%0.25%0.50%0.75%1.00%REFERENCES

11

problem caused by the effects of noisy training labels. We
ﬁrst propose a CNN-based graph generator G-Net to build an
initial graph. Relying on the proposed graph-based regularized
loss functions, we then propose a graph-based classiﬁer W-
Net to perform online label denoising of the training samples
that potentially have noisy labels. Based on the denoised
training labels, we update the underlying graph structure by
learning the proposed graph update U-Net. Finally, we learn a
reﬁned graph-based classiﬁer W-Net to perform classiﬁcation
using the updated underlying graph structure. The validation
on three different binary classiﬁcation datasets demonstrate
that our proposed architecture outperforms the state-of-the-
art classiﬁcation methods when partial
training labels are
incorrect. Furthermore, the rank-sampling method is proven to
be another enhancement for this semi-supervised classiﬁcation
problem.

VII. ACKNOWLEDGEMENT

This project has received funding from the European
Union’s Horizon 2020 research and innovation pro-
gramme under the Marie Skłodowska-Curie grant agree-
ment No 734331. The University of Strathclyde grate-
fully acknowledges the support of NVIDIA Corporation
with the donation of the Titan Xp GPU used for this
research.

REFERENCES
[1] A. Krizhevsky, I. Sutskever, et al., “Imagenet classiﬁca-
tion with deep convolutional neural networks,” in NIPS,
2012, pp. 1097–1105.

[2] S. Sukhbaatar, J. Bruna, et al., “Training convolutional

networks with noisy labels,” ArXiv:1406.2080, 2014.

[3] G. Patrini, A. Rozza, et al., “Making deep neural net-
works robust to label noise: A loss correction approach,”
in IEEE CVPR, 2017, pp. 2233–2241.

[5]

[4] X. Ma, Y. Wang, et al., “Dimensionality-driven learning
with noisy labels,” in ICML, 2018, pp. 3361–3370.
J. Tang, R. Hong, et al., “Image annotation by
knn-sparse graph-based label propagation over noisily
tagged web images,” ACM TIST, vol. 2, no. 2, p. 14,
2011.

[6] B. Fr´enay and M. Verleysen, “Classiﬁcation in the
presence of label noise: A survey,” IEEE TNNLS, vol.
25, no. 5, pp. 845–869, 2014.

[7] Y. Wang, W. Liu, et al., “Iterative learning with open-set

noisy labels,” ArXiv:1804.00092, 2018.

[8] P. Lemberger, “On generalization and regularization in

deep learning,” ArXiv:1704.01312, 2017.

[9] G. E. Hinton, N. Srivastava, et al., “Improving neural
networks by preventing co-adaptation of feature detec-
tors,” ArXiv:1207.0580, 2012.

[10] S. Ioffe and C. Szegedy, “Batch normalization: Ac-
celerating deep network training by reducing internal
covariate shift,” in ICML, 2015, pp. 448–456.

[11] K. He, X. Zhang, et al., “Deep residual learning for
image recognition,” in IEEE CVPR, 2016, pp. 770–778.
[12] G. Huang, Z. Liu, et al., “Densely connected convolu-

tional networks,” in IEEE CVPR, vol. 1, 2017, p. 3.

[13] T. Xiao, T. Xia, et al., “Learning from massive noisy
labeled data for image classiﬁcation,” in IEEE CVPR,
2015, pp. 2691–2699.

[14] D. Hendrycks, M. Mazeika, et al., “Using trusted data
to train deep networks on labels corrupted by severe
noise,” ArXiv:1802.05300, 2018.

[15] H. Masnadi-Shirazi and N. Vasconcelos, “On the design
of loss functions for classiﬁcation: Theory, robustness
to outliers, and savageboost,” in NIPS, 2009, pp. 1049–
1056.

[17]

[16] S. Reed, H. Lee, et al., “Training deep neural networks
on noisy labels with bootstrapping,” ArXiv:1412.6596,
2014.
J. Goldberger and E. Ben-Reuven, “Training deep
neural-networks using a noise adaptation layer,” 2016.
[18] A. Sandryhaila and J. M. F. Moura, “Classiﬁcation via
regularization on graphs,” in IEEE GlobalSIP, Dec.
2013, pp. 495–498.

[19] C. Gong, T. Liu, et al., “Deformed graph laplacian for
semisupervised learning,” IEEE TNNLS, vol. 26, no. 10,
pp. 2261–2274, Oct. 2015.

[20] G. Cheung, W. Su, et al., “Robust semisupervised graph
classiﬁer learning with negative edge weights,” IEEE T-
SIPN, vol. 4, no. 4, pp. 712–726, Nov. 2018.

[21] C. Yang, G. Cheung, et al., “Alternating binary classiﬁer
and graph learning from partial labels,” in APSIPA, Nov.
2018.
J. Zeng,
regularization for robust denoising of real
ArXiv:1807.11637, 2018.

J. Pang, et al., “Deep graph laplacian
images,”

[22]

[23] B. Jiang and D. Lin, “Graph laplacian regularized graph
convolutional networks for semi-supervised learning,”
ArXiv:1809.09839, 2018.

[24] M. Ye, V. Stankovic, et al., “Deep graph based learning
for binary classiﬁcation,” in IEEE ICASSP, 2019.
[25] C. Cortes and V. Vapnik, “Support-vector networks,”

Machine learning, vol. 20, no. 3, pp. 273–297, 1995.

[26] Y. Wang, Y. Sun, et al., “Dynamic graph cnn for

learning on point clouds,” ArXiv:1801.07829, 2018.

[27] E. Hoffer and N. Ailon, “Deep metric learning us-
ing triplet network,” in International Workshop on
Similarity-Based Pattern Recognition, Springer, 2015,
pp. 84–92.

[28] B. Biggio, B. Nelson, et al., “Support vector machines
under adversarial label noise,” in ACML, vol. 20, South
Garden Hotels and Resorts, Taoyuan, Taiwain, Nov.
2011, pp. 97–112.

[30]

[29] M. Speriosu, N. Sudan, et al., “Twitter polarity classiﬁ-
cation with label propagation over lexical links and the
follower graph,” in Proceedings of the First workshop
on Unsupervised Learning in NLP, 2011, pp. 53–63.
J. Tang, R. Hong, et al., “Image annotation by
knn-sparse graph-based label propagation over noisily
tagged web images,” ACM TIST, vol. 2, no. 2, p. 14,
2011.
I. Daubechies, R. DeVore, et al., “Iteratively reweighted
least squares minimization for sparse recovery,” Com-

[31]

12

[51]

[50] X. Zhang, C. Xu, et al., “Graph edge convolutional
neural networks for skeleton-based action recognition,”
IEEE TNNLS, pp. 1–14, Sep. 2019.
J. Alcala-Fdez, A. Fern´andez, et al., “Keel data-mining
software tool: Data set repository, integration of algo-
rithms and experimental analysis framework,” Journal
of Multiple-Valued Logic and Soft Computing, vol. 17,
pp. 255–287, Jan. 2010.

[52] Y Wang, Z Pan, et al., “A training data set cleaning
method by classiﬁcation ability ranking for the k-nearest
neighbor classiﬁer.,” IEEE transactions on neural net-
works and learning systems, 2019.

[53] D. I. Shuman, S. K. Narang, et al., “The emerging
ﬁeld of signal processing on graphs: Extending high-
dimensional data analysis to networks and other irreg-
ular domains,” IEEE signal processing magazine, vol.
30, no. 3, pp. 83–98, 2013.

munications on Pure and Applied Mathematics, vol. 63,
no. 1, pp. 1–38, 2010.

[32] A. Prest, C. Leistner, et al., “Learning object class
detectors from weakly annotated video,” in IEEE CVPR,
2012, pp. 3282–3289.
I. Misra, A. Shrivastava, et al., “Watch and learn: Semi-
supervised learning for object detectors from video,” in
IEEE CVPR, 2015, pp. 3593–3602.

[33]

[34] A. Kuznetsova, S. Ju Hwang, et al., “Expanding object
detector’s horizon: Incremental learning framework for
object detection in videos,” in IEEE CVPR, 2015,
pp. 28–36.

[35] M. Ren, W. Zeng, et al., “Learning to reweight exam-
ples for robust deep learning,” ArXiv:1803.09050, 2018.
I. Jindal, M. Nokleby, et al., “Learning deep networks
from noisy labels with dropout regularization,” in IEEE
ICDM, 2016, pp. 967–972.

[36]

[37] D. Rolnick, A. Veit, et al., “Deep learning is robust to
massive label noise,” ArXiv:1705.10694, 2017.
[38] V. Mnih and G. E. Hinton, “Learning to label aerial
images from noisy data,” in ICML, 2012, pp. 567–574.
[39] T. Xiao, T. Xia, et al., “Learning from massive noisy
labeled data for image classiﬁcation,” in IEEE CVPR,
2015, pp. 2691–2699.
J. Bruna, W. Zaremba, et al., “Spectral networks and lo-
cally connected networks on graphs,” ArXiv:1312.6203,
2013.

[40]

[41] T. N. Kipf and M. Welling, “Semi-supervised classiﬁca-
tion with graph convolutional networks,” CoRR, 2016.
[42] M. Defferrard, X. Bresson, et al., “Convolutional neural
networks on graphs with fast localized spectral ﬁlter-
ing,” in NIPS, D. D. Lee, M. Sugiyama, et al., Eds.,
2016, pp. 3844–3852.

[43] X. Zhang, C. Xu, et al., “Graph edge convolutional
neural networks for skeleton based action recognition,”
ArXiv:1805.06184, 2018.
J. Mojoo, K. Kurosawa, et al., “Deep cnn with graph
laplacian regularization for multi-label image annota-
tion,” in ICIAR, Springer, 2017, pp. 19–26.

[44]

[45] V. N. Ekambaram, G. Fanti, et al., “Wavelet-regularized
graph semi-supervised learning,” in IEEE GlobalSIP,
2013, pp. 423–426.

[46] A. Gadde, A. Anis, et al., “Active semi-supervised
learning using sampling theory for graph signals,” in
Proceedings of the 20th ACM SIGKDD international
conference on Knowledge discovery and data mining,
2014, pp. 492–501.

[47] L. Zelnik-Manor and P. Perona, “Self-tuning spectral
clustering,” in Proceedings of the 17th International
Conference on Neural Information Processing Systems,
ser. NIPS, Vancouver, British Columbia, Canada: MIT
Press, 2004, pp. 1601–1608.

[48] H. E. Egilmez, E. Pavez, et al., “Graph learning from
data under laplacian and structural constraints,” IEEE
JSTSP, vol. 11, no. 6, pp. 825–841, 2017.

[49] K. He, X. Zhang, et al., “Deep residual learning for
image recognition,” in IEEE CVPR, 2016, pp. 770–778.

