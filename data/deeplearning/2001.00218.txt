0
2
0
2

b
e
F
2
2

]

G
L
.
s
c
[

3
v
8
1
2
0
0
.
1
0
0
2
:
v
i
X
r
a

Lossless Compression of Deep Neural Networks

Thiago Serra1, Abhinav Kumar2, and Srikumar Ramalingam2

1 Bucknell University, USA
thiago.serra@bucknell.edu
2 The University of Utah, USA
abhinav.kumar@utah.edu,srikumar@cs.utah.edu

Abstract. Deep neural networks have been successful in many predic-
tive modeling tasks, such as image and language recognition, where large
neural networks are often used to obtain good accuracy. Consequently,
it is challenging to deploy these networks under limited computational
resources, such as in mobile devices. In this work, we introduce an al-
gorithm that removes units and layers of a neural network while not
changing the output that is produced, which thus implies a lossless com-
pression. This algorithm, which we denote as LEO (Lossless Expressive-
ness Optimization), relies on Mixed-Integer Linear Programming (MILP)
to identify Rectiﬁed Linear Units (ReLUs) with linear behavior over the
input domain. By using (cid:96)1 regularization to induce such behavior, we
can beneﬁt from training over a larger architecture than we would later
use in the environment where the trained neural network is deployed.

Keywords: Deep Learning · Mixed-Integer Linear Programming · Neu-
ral Network Pruning · Neuron Stability · Rectiﬁed Linear Unit.

Table 1. Compression of 2-hidden-layer rectiﬁer networks trained on MNIST. Each line
summarizes tests on 31 networks. Depending on how the network is trained, the higher
incidence of stable units allows for more compression while preserving the trained net-
work accuracy. For example, training with (cid:96)1 regularization induces such stability and
then inactive units can be removed. Interestingly, the small amount of regularization
that improves accuracy during training also helps compressing the network later.

Layer width (cid:96)1 weight Accuracy (%)
95.76 ± 0.05
0.001
97.24 ± 0.02
0.0002
96.68 ± 0.03
0
96.05 ± 0.04
0.001
97.81 ± 0.02
0.0002
97.62 ± 0.02
0
97.14 ± 0.02
0.0005
98.14 ± 0.01
0.0001
98.00 ± 0.01
0

25
25
25
50
50
50
100
100
100

Units removed

1st layer
5.7 ± 0.3
1.2 ± 0.1
0 ± 0

2nd layer
5.1 ± 0.3
3.0 ± 0.4
0 ± 0

16.9 ± 0.6 12.5 ± 0.6
7.5 ± 0.5
7.6 ± 0.4
0 ± 0
0 ± 0

36.7 ± 0.7 24.9 ± 0.6
18.6 ± 0.5 11.1 ± 0.7

0 ± 0

0 ± 0

Network
compression (%)
22 ± 1
8.3 ± 0.7
0 ± 0
29.4 ± 0.7
15.1 ± 0.6
0 ± 0
30.8 ± 0.5
14.9 ± 0.4
0 ± 0

 
 
 
 
 
 
2

1

T. Serra et al.

Introduction

Deep Neural Networks (DNNs) have achieved unprecedented success in many
domains of predictive modeling, such as computer vision [60,17,36,91,43], nat-
ural language processing [90], and speech [45]. While complex architectures are
usually behind such feats, it is not fully known if these results depend on such
DNNs being as wide or as deep as they currently are for some applications.

In this paper, we are interested in the compression of DNNs, especially to
reduce their size and depth. More generally, that relates to the following question
of wide interest about neural networks: given a neural network N1, can we ﬁnd
an equivalent neural network N2 with a diﬀerent architecture? Since a trained
DNN corresponds to a function mapping its inputs to outputs, we can formalize
the equivalence among neural networks as follows [78]:

Deﬁnition 1 (Equivalence). Two deep neural networks N1 and N2 with asso-
ciated functions f1 : Rn0 → Rm and f2 : Rn0 → Rm, respectively, are equivalent
if f1(x) = f2(x) ∀ x ∈ Rn0.

In other words, our goal is to start from a trained neural network and identify
neural networks with fewer layers or smaller layer widths that would produce the
exact same outputs. Since the typical input for certain applications is bounded
along each dimension, such as x ∈ [0, 1]n0 for the MNIST dataset [63], we can
consider a broader family of neural networks that would be regarded as equiv-
alent in practice. We formalize that idea with the concept of local equivalence:

Deﬁnition 2 (Local Equivalence). Two deep neural networks N1 and N2 with
associated functions f1 : Rn0 → Rm and f2 : Rn0 → Rm, respectively, are local
equivalent with respect to a domain D ⊆ Rn0 if f1(x) = f2(x) ∀ x ∈ D.

For a given application, local equivalence with respect to the domain of pos-
sible inputs suﬃces to guarantee that two networks have the same accuracy in
any test. Hence, ﬁnding a smaller network that is local equivalent to the original
network implies a compression in which there is no loss. In this paper, we show
that simple operations such as removing or merging units and folding layers of a
DNN can yield such lossless compression under certain conditions. We denote as
folding the removal of a layer by directly connecting the adjacent layers, which
is accompanied by adjusting the weights and biases of those layers accordingly.

2 Background

We study feedforward DNNs with Rectiﬁed Linear Unit (ReLU) activations [38],
which are comparatively simpler than other types of activations. Nevertheless,
ReLUs are currently the type of unit that is most commonly used [64], which is
in part due to landmark results showing their competitive performance [77,35].
Every network has input x = [x1 x2 . . . xn0 ]T from a bounded domain X
and corresponding output y = [y1 y2 . . . ym]T , and each hidden layer l ∈ L =

Lossless Compression of Deep Neural Networks

3

1 hl

]T from ReLUs indexed by i ∈ Nl =
{1, 2, . . . , L} has output hl = [hl
2 . . . hl
nl
{1, 2, . . . , nl}. Let W l be the nl × nl−1 matrix where each row corresponds to
the weights of a neuron of layer l, and let bl be vector of biases associated with
the units in layer l. With h0 for x and hL+1 for y, the output of each unit i
in layer l consists of an aﬃne function gl
i followed by the ReLU
activation hl
i > 0
and inactive otherwise. DNNs consisting solely of ReLUs are denoted rectiﬁer
networks, and their associated functions are always piecewise linear [7].

i}. The unit i in layer l is denoted active when hl

i = max{0, gl

i hl−1 + bl

i = W l

2.1 Mixed-Integer Linear Programming

Our work is primarily based on the fast growing literature on applications of
Mixed-Integer Linear Programming (MILP) to deep learning. MILP can be
used to map inputs to outputs of each ReLU and consequently of rectiﬁer net-
works. Such formulations have been used to produce the image [71,27] and
estimate the number of pieces [87,86] of the piecewise linear function associ-
ated with the network, generate adversarial perturbations to test the network
robustness [16,31,96,89,106,6], and implement controllers based on DNN mod-
els [85,109].

For each unit i in layer l, we can map hl−1 to gl

i with a formulation that
also includes a binary variable zi denoting if the unit is active or not, a variable
(cid:9),
i denoting the output of a complementary ﬁctitious unit ¯hl
¯hl
i that are positive and as large as hl
and constants H l

i = max (cid:8)0, −gl
i and ¯hl
i can be:

i and ¯H l

i and hl

i

W l

i = gl
i hl−1 + bl
i
i − ¯hl
i = hl
gl
i
i zl
hl
i ≤ H l
i
¯hl
i ≤ ¯H l
i (1 − zl
i)
hl
i ≥ 0
¯hl
i ≥ 0
zl
i ∈ {0, 1}

(1)

(2)

(3)

(4)

(5)

(6)

(7)

This formulation can be strengthened by using the smallest possible values for
i [31,96] and valid inequalities to avoid fractional values of zl
H l

i [6,86].

i and ¯H l
The largest possible value of gl

i, which we denote Gl

i, can be obtained as

Gl(cid:48)
i = max
s.t.

W l(cid:48)

i hl(cid:48)−1 + bl(cid:48)
i
(1)–(7)
x ∈ X

∀l ∈ {1, . . . , l(cid:48) − 1}, i ∈ Nl

(8)

(9)

(10)

i > 0, then Gl
i . Otherwise, hl

If Gl
H l
to obtain G

i is also the largest value of hl
i = 0 for any input x ∈ X. We can also minimize W l

i and it can be used for constant
i hl−1 + bl
i
i if

i for constant ¯H l

i, and use −G

l
i, the smallest possible value of gl

l

4

T. Serra et al.

i and ¯H l

l
i < 0; whereas G

l
i > 0 implies that hl

i > 0 for any input x ∈ X. By solving
G
these formulations from the ﬁrst to the last layer, we have the tightest values
for H l
i for l ∈ {1, . . . l(cid:48) − 1} when we reach layer l(cid:48). Units with only
zero or positive outputs were ﬁrst identiﬁed using MILP in [96], where they are
denoted as stably inactive and stably active, and their incidence was induced
with (cid:96)1 regularization. That was later exploited to accelerate the veriﬁcation of
robustness by making the corresponding MILP formulation easier to solve [106].
In this paper, we use the stability of units to either remove or merge them
while preserving the outputs produced by the DNN. The same idea could be
extended to other architectures with MILP mappings, such as Binarized Neural
Networks (BNNs) [18,78]. MILP has been used in BNNs for adversarial test-
ing [56] and along with Constraint Programming (CP) for training [52]. BNNs
have characteristics that also make them suitable under limited resources [78].

2.2 Related Work

Our work relates to the literature on neural network compression, and more
speciﬁcally to methods that simplify a trained DNN. Such literature includes
low-rank decomposition [22,53,112,101,81,26], quantization [83,18,105,97], archi-
tecture design [91,51,48,49,94], non-structured pruning [66], structured pruning
[40,65,74,39,72,1,110], sparse learning [68,114,4,102], automatic discarding of lay-
ers in ResNets [98,111,44], variational methods [113], and the recent Lottery
Ticket Hypothesis [32] by which training only certain subnetworks in the DNN
— the lottery tickets — might be good enough. However, network compression
is often achieved with side eﬀects to the function associated with the DNN.

In contrast to many lossy pruning methods that typically focus on remov-
ing unimportant neurons and connections, our approach focuses on developing
lossless transformations that exactly preserve the expressiveness during the com-
pression. A necessary criterion for equivalent transformation is that the resulting
network is as expressive as the original one. Methods to study neural network
expressiveness include universal approximation theory [19], VC dimension [9],
trajectory length [82], and linear regions [79,76,75,82,7,87,41,42,86].

We can also consider our approach as a form of reparameterization, or equiv-
alent transformation, in graphical models [59,100,103]. If two parameter vectors
θ and θ(cid:48) deﬁne the same energy function (i.e., E(x|θ)) = E(x|θ(cid:48)), ∀ x), then θ(cid:48) is
called a reparameterization of θ. Reparameterization has played a key role in sev-
eral inference problems such as belief propagation [100], tree-weighted message
passing [99], and graph cuts [57]. The idea is also associated with characterizing
the functions that can be represented by DNNs [46,19,95,67,7,73,62].

Finally, our work complements the vast literature at the intersection of math-
ematical optimization and Machine Learning (ML). General-purpose methods
have been applied to train ML models to optimality [12,13,52,84]. Conversely,
ML models have been extensively applied in optimization [11,34]. To mention a
few lines of work, ML has been used to ﬁnd feasible solutions [33,24] and predict
good solutions [25,21]; determine how to branch on [55,3,69,8,47] or add cuts [93],
when to linearize [14], or when to decompose [61] an optimization problem; how

Lossless Compression of Deep Neural Networks

5

to adapt algorithms for each problem [54,50,88,58,20,10,23]; obtain better opti-
mization bounds [15]; embed the structure of the problem as a layer of a neural
network [5,108,2,29]; and predict the resolution by a time-limit [30], the feasibil-
ity of the problem [107], and the problem itself [28,70,92].

3 LEO: Lossless Expressiveness Optimization

Algorithm 1, which we denote LEO (Lossless Expressiveness Optimization), loops
over the layers to remove units with constant outputs regardless of the input,
some of the stable units, and any layers with constant output due to those two
types of units. These modiﬁcations of the network architecture are followed by
changes to the weights and biases of the remaining units in the network to
preserve the outputs produced. The term expressiveness is commonly used to
refer to the ability of a network architecture to represent complex functions [86].
First, LEO checks the weights and stability of each unit and decides whether
to immediately remove them. A unit with constant output, which is either stably
inactive or has zero input weights, is removed as long as there are other units left
in the layer. A stably active unit with varying output is removed if the column
of weights of that unit is linearly dependent on the column of weights of stably
active units with varying outputs that have been previously inspected in that
layer. We consider the removal of such stably active units as a merging operation
since the output weights of other stable units need to be adjusted as well.

Second, LEO checks if layers can be removed in case the units left in the
layer are all stable or have constant output. If they are all stably active with
varying output, then the layer is removed by directly joining the layers before
and after it, which we denote as a folding operation. In the particular case that
only one unit is left with constant output, be it stably inactive or not, then all
hidden layers are removed because the network has an associated function that is
constant in D. We denote the latter operation as collapsing the neural network.
Figure 1 shows examples of units being removed and merged on the left as well
as of a layer being folded on the right. Although possible, folding or collapsing
a trained neural network is not something that we would expect to achieve in
practice unless we are compressing with respect to a very small domain D ⊂ X.

Theorem 1. For a neural network N1, Algorithm 1 produces a neural network
N2 such that N1 and N2 are local equivalent with respect to an input domain D.

i < 0, then hl

i = 0 for any input in D and unit i in layer l can be
Proof. If Gl
regarded as stably inactive. Otherwise, if W l
i = 0, then the output of the unit
is positive but constant. Those two types of units are analyzed by the block
starting at line 5. If there are other units left in the layer, which are either not
stable or stably active but not removed, then removing unit a stably inactive
unit i does not aﬀect the output of subsequent units since the output of the unit
is always 0 in D. Likewise, in the case that W l
i > 0, then the output
of the network remains the same after removing that unit if such removal of hl
i
from each unit j in layer l + 1 is followed by adding wl+1

i = 0 and bl

.

ji bl

i to bl+1
j

6

T. Serra et al.

Fig. 1. Examples of output-preserving neural network compression obtained with LEO.
On the left, two units in white are stably inactive and three units indexed by set S in
darker blue are stably active, where rank(W 2
S )=2. In such a case, we can remove the
stably inactive units and merge the stably active units to produce the same input to the
next layer using only two units. On the right, an entire layer is stably active. In such a
case, we can fold the layer by directly connecting the layers before and after it. In both
cases, the red arcs correspond to coeﬃcients that need to be adjusted accordingly.

If ¯Gl

i = W l

i hl−1 + bl

i > 0, then hl

i for any input in D and unit i in layer l
can be regarded as stably active. Those units are analyzed by the block starting
at line 14. If the rank of the submatrix W l
S consisting of the weights of stably
active units in set S is the same as that of W l
i (cid:54)= 0 for
every i ∈ S, then S (cid:54)= ∅ and hl
i. Since

S∪{i} and given that W l

khl−1 + bl

i = (cid:80)

i = (cid:80)

k) + bl

k − bl

αk(hl

αkwl

k∈S

k∈S

there would be other units left in the layer, the output of the network remains
the same after removing the unit if such removal of hl
i from each unit j in layer
(cid:18)
jk and wl+1
l + 1 is followed by adding αkwl+1
αkbl
k

i − (cid:80)
bl

to bl+1
j

to wl+1

(cid:19)

ji

ji

.

k∈A

If all units left in layer l are stably active and |S| > 0, then layer l is equivalent
to an aﬃne transformation and it is possible to directly connect layers l − 1 and
l + 1, as in the block starting at line 30. Since hl
k for each stably
khl−1 + bl
W l
k

active unit k in layer l, then hl+1

j hl−1 + bl
(cid:18) nl(cid:80)

i = W l+1

+

(cid:19)

i

hl +bl+1
(cid:18)

k = W l
i = W l+1
(cid:19)
.

wl+1

ik bl
k

i

(cid:80)
k∈S

k=1

i = (cid:80)
bl+1

j∈nl−1

(cid:18)

(cid:80)
k∈S

(cid:19)

wl

kjwl+1

ik

j + bl+1
hl−1

i +

If the only unit i left in layer l is stably inactive or stably active but has zero
weights, then any input in D results in hl
i}, and consequently the
neural network is associated with a constant function f : x → Υ in D. Therefore,
it is possible to remove all hidden layers and replace the output layer with a
constant function mapping to Υ as in the block starting at line 39. (cid:3)

i = max{0, bl

Implementation We do not need to solve (8)–(10) to optimality to determine if
Gl
i < 0: it suﬃces to ﬁnd a negative upper bound to guarantee that, or a solution
with positive value to refute that. A similar reasoning applies to ¯Gl

i > 0.

Lossless Compression of Deep Neural Networks

7

i

k

else

(cid:46) wl

if Gl

if W l

i =0 then

i > 0 then

k∈S αkwl

i = 0 and bl

if rank(cid:0)W l

(cid:1) > |S| then

S∪{i}
S ← S ∪ {i}

end if
else if ¯Gl

(cid:46) Unit i is not necessary

i > 0 for x ∈ D then

(cid:46) Adjust activations in layer l + 1

j ← bl+1
bl+1
end for

for j ← 1, . . . , nl+1 do
j + wl+1
ji bl

end if
Remove unit i from layer l

(cid:46) Unit i has constant output
(cid:46) Layer l still has other units

S ← {}
Unstable ← False
for i ← 1, . . . , nl do

(cid:46) Set of stable units left in layer l
(cid:46) If there are unstable units in layer l

i < 0 for x ∈ D or W l
if i < nl or |S| > 0 or Unstable then

Find {αk}k∈S such that wl
for j ← 1, . . . , nl+1 do
jk + (cid:80)
wl+1
jk ← wl+1
bl+1
j + wl+1
j ← bl+1
end for
Remove unit i from layer l

(cid:46) Unit i is stably active
i is linearly independent
(cid:46) Keep unit in the network
(cid:46) Output of unit i is linearly dependent
i = (cid:80)

Algorithm 1 LEO produces a smaller equivalent neural network with respect to
a domain D by removing units and layers while adjusting weights and biases
1: for l ← 1, . . . , L do
2:
3:
4:
5:
6:
7:
8:
9:
10:
11:
12:
13:
14:
15:
16:
17:
18:
19:
20:
21:
22:
23:
24:
25:
26:
27:
28:
29:
30:
31:
32:
33:
34:
35:
36:
37:
38:
39:
40:
41:
42:
43:
44:
45: end for

(cid:46) Only unit left in layer l has constant output
Compute output Υ for any input χ ∈ D
(cid:46) Function is constant
(W L+1, bL+1) ← (0, Υ )
(cid:46) Set constant values in output layer
Remove layers 1 to L and break (cid:46) Remove all hidden layers and leave

end for
Remove layer l; replace parameters in next layer with ¯W and ¯b

Create matrix ¯W ∈ Rnl−1×nl+1 and vector ¯b ∈ Rnl+1
for i ← 1, . . . , nl+1 do

(cid:46) All units left in layer l are stable
(cid:46) The units left have varying outputs

end for
if not Unstable then
if |S| > 0 then

i + (cid:80)
¯bi ← bl+1
k∈S wl+1
for j ← 1, . . . , nl−1 do

(cid:46) Directly connect layers l − 1 and l + 1

(cid:46) Adjust activations in layer l + 1

(cid:46) At least one unit is not stable

(cid:46) Unit i is no longer necessary

k∈S αkwl+1
i + (cid:80)

Unstable ← True

k∈S αkbl
k)

¯wij ← (cid:80)

kjwl+1
ik

k∈S wl

end for

ji (bl

end if

end if

end if

end if

ik bl

else

else

ji

k

8

T. Serra et al.

4 Experiments

We conducted experiments to evaluate the potential for network compression
using LEO. In these experiments, we trained rectiﬁer networks on the MNIST
dataset [63] with input size 784, two hidden layers of same width, and 10 outputs.
The widths of the hidden layers are 25, 50, and 100. For each width, we identiﬁed
in preliminary experiments a weight for (cid:96)1 regularization on layer weights that
improved the network accuracy in comparison with no regularization: 0.0002,
0.0002, and 0.0001, respectively. We trained 31 networks with that regularization
weight, with 5 times the same weight to induce more stability, and with zero
weight as a benchmark. We use the negative log-likelihood as the loss function
after taking a softmax operation on the output layer, a batch size of 64 and SGD
with a learning rate of 0.01, and momentum of 0.9 for training the model to 120
epochs. The learning rate is decayed by a factor of 0.1 after every 50 epochs. The
weights of the network were initialized with the Kaiming initialization [43] and
the biases were initialized to zero. The models were trained using Pytorch [80] on
a machine with 40 Intel(R) Xeon(R) CPU E5-2640 v4 @ 2.40GHz processors and
132 GB of RAM. The MILPs were solved using Gurobi 8.1.1 [37] on a machine
with Intel(R) Core(TM) i5-6200U CPU @ 2.30 GHz and 16 GB of RAM. We
used callbacks to check bounds and solutions and then interrupt the solver after
determining unit stability and bounds for each MILP.

Tables 1 and 2 summarize our experiments with mean and standard error. We
note that the compression grows with the size of the network and the weight of (cid:96)1
regularization, which induces the weights of each unit to be orders of magnitude
smaller than its bias. The compression identiﬁed is all due to removing stably
inactive units. Most of the runtime is due to solving MILPs for the second hidden
layer. Given the incidence of stably active units, we conjecture that inducing rank
deﬁciency in the weights or negative values in the biases could also be beneﬁcial.

Table 2. Additional details about the experiments for each type of network, including
runtime per test, incidence of stably active units, and overall network stability.

Layer width (cid:96)1 weight Runtime (s)
27.9 ± 0.3
0.001
29 ± 1
0.0002
28.4 ± 0.3
0
103 ± 2
0.001
106 ± 3
0.0002
112 ± 3
0
421 ± 4
0.0005
456 ± 8
0.0001
385 ± 2
0

25
25
25
50
50
50
100
100
100

Stably active units
2nd layer
1st layer
7.4 ± 0.4
2.5 ± 0.3
1.0 ± 0.2
0 ± 0
0 ± 0
0 ± 0
24.9 ± 0.6
15 ± 0.5
8.8 ± 0.5
2.7 ± 0.3
0 ± 0
0 ± 0

35.7 ± 0.6 57.7 ± 0.7
18 ± 0.7
11.1 ± 0.5
0 ± 0
0 ± 0

Network
stability (%)
41.3 ± 0.6
10.4 ± 0.7
0 ± 0
69.3 ± 0.4
26.6 ± 0.6
0 ± 0
77.5 ± 0.2
29.4 ± 0.5
0 ± 0

Lossless Compression of Deep Neural Networks

9

5 Conclusion

We introduced a lossless neural network compression algorithm, LEO, which relies
on MILP to identify parts of the neural network that can be safely removed
after reparameterization. We found that networks trained with (cid:96)1 regularization
are particularly amenable to such compression. In a sense, we could interpret (cid:96)1
regularization as inducing a subnetwork to represent the function associated with
the DNN. Future work may explore the connection between these subnetworks
identiﬁed by LEO and lottery tickets, bounding techniques such as in [104] to
help eﬃciently identifying stable units, and other forms of inducing posterior
compressibility while training. Concomitantly, we have shown another form in
which discrete optimization can play a key role in deep learning applications.

References

1. Aghasi, A., Abdi, A., Nguyen, N., Romberg, J.: Net-trim: Convex pruning of deep

neural networks with performance guarantee. In: NeurIPS (2017)

2. Agrawal, A., Amos, B., Barratt, S., Boyd, S., Diamond, S., Kolter, Z.: Diﬀeren-

tiable convex optimization layers. In: NeurIPS (2019)

3. Alvarez, A., Louveaux, Q., Wehenkel, L.: A machine learning-based approxima-

tion of strong branching. INFORMS Journal on Computing (2017)

4. Alvarez, J., Salzmann, M.: Learning the number of neurons in deep networks. In:

NeurIPS (2016)

5. Amos, B., Kolter, Z.: Optnet: diﬀerentiable optimization as a layer in neural

networks. In: ICML (2017)

6. Anderson, R., Huchette, J., Tjandraatmadja, C., Vielma, J.: Strong mixed-integer

programming formulations for trained neural networks. In: IPCO (2019)

7. Arora, R., Basu, A., Mianjy, P., Mukherjee, A.: Understanding deep neural net-

works with rectiﬁed linear units. In: ICLR (2018)

8. Balcan, M.F., Dick, T., Sandholm, T., Vitercik, E.: Learning to branch. In: ICML

(2018)

9. Bartlett, P., Maiorov, V., Meir, R.: Almost linear VC-dimension bounds for piece-

wise polynomial networks. Neural computation (1998)

10. Bello, I., Pham, H., Le, Q.V., Norouzi, M., Bengio, S.: Neural combinatorial op-

timization with reinforcement learning. In: ICLR (2017)

11. Bengio, Y., Lodi, A., Prouvost, A.: Machine learning for combinatorial optimiza-

tion: a methodological tour d’horizon. CoRR abs/1811.06128 (2018)

12. Bertsimas, D., Dunn, J.: Optimal classiﬁcation trees. Machine Learning (2017)
13. Bienstock, D., Mu˜noz, G., Pokutta, S.: Principled deep neural network training

through linear programming. CoRR abs/1810.03218 (2018)

14. Bonami, P., Lodi, A., Zarpellon, G.: Learning a classiﬁcation of mixed-integer

quadratic programming problems. In: CPAIOR (2018)

15. Cappart, Q., Goutierre, E., Bergman, D., Rousseau, L.M.: Improving optimiza-
tion bounds using machine learning: Decision diagrams meet deep reinforcement
learning. In: AAAI (2019)

16. Cheng, C., N¨uhrenberg, G., Ruess, H.: Maximum resilience of artiﬁcial neural

networks. In: ATVA (2017)

17. Ciresan, D., Meier, U., Masci, J., Schmidhuber, J.: Multi column deep neural

network for traﬃc sign classiﬁcation. Neural Networks (2012)

10

T. Serra et al.

18. Courbariaux, M., Hubara, I., Soudry, D., El-Yaniv, R., Bengio, Y.: Binarized
neural networks: Training deep neural networks with weights and activations con-
strained to+ 1 or-1. NeurIPS (2016)

19. Cybenko, G.: Approximation by superpositions of a sigmoidal function. Mathe-

matics of Control, Signals and Systems (1989)

20. Dai, H., Khalil, E.B., Zhang, Y., Dilkina, B., Song, L.: Learning combinatorial

optimization algorithms over graphs. In: NeurIPS (2017)

21. Demirovi´c, E., Stuckey, P., Bailey, J., Chan, J., Leckie, C., Ramamohanarao,
K., Guns, T.: An investigation into prediction + optimisation for the knapsack
problem. In: CPAIOR (2019)

22. Denton, E., Zaremba, W., Bruna, J., LeCun, Y., Fergus, R.: Exploiting lin-
ear structure within convolutional networks for eﬃcient evaluation. In: NeurIPS
(2014)

23. Deudon, M., Cournut, P., Lacoste, A., Adulyasak, Y., Rousseau, L.M.: Learning

heuristics for the TSP by policy gradient. In: CPAIOR (2018)

24. Ding, J.Y., Zhang, C., Shen, L., Li, S., Wang, B., Xu, Y., Song, L.: Accelerating
primal solution ﬁndings for mixed integer programs based on solution prediction.
CoRR abs/1906.09575 (2019)

25. Donti, P., Amos, B., Kolter, Z.: Task-based end-to-end model learning in stochas-

tic optimization. In: NeurIPS (2017)

26. Dubey, A., Chatterjee, M., Ahuja, N.: Coreset based neural network compression.

In: ECCV (2018)

27. Dutta, S., Jha, S., Sankaranarayanan, S., Tiwari, A.: Output range analysis for

deep feedforward networks. In: NFM (2018)

28. Elmachtoub, A., Grigas, P.:

Smart

predict,

then

optimize. CoRR

abs/1710.08005 (2017)

29. Ferber, A., Wilder, B., Dilkina, B., Tambe, M.: MIPaaL: Mixed integer program

as a layer. In: AAAI (2020)

30. Fischetti, M., Lodi, A., Zarpellon, G.: Learning MILP resolution outcomes before

reaching time-limit. In: CPAIOR (2019)

31. Fischetti, M., Jo, J.: Deep neural networks and mixed integer linear optimization.

Constraints (2018)

32. Frankle, J., Carbin, M.: The lottery ticket hypothesis: Finding sparse, trainable

neural networks. In: ICLR (2019)

33. Galassi, A., Lombardi, M., Mello, P., Milano, M.: Model agnostic solution of CSPs

via deep learning: A preliminary study. In: CPAIOR (2018)

34. Gambella, C., Ghaddar, B., Naoum-Sawaya, J.: Optimization models for machine

learning: A survey. CoRR abs/1901.05331 (2019)

35. Glorot, X., Bordes, A., Bengio, Y.: Deep sparse rectiﬁer neural networks. In:

AISTATS (2011)

36. Goodfellow, I., Warde-Farley, D., Mirza, M., Courville, A., Bengio, Y.: Maxout

networks. In: ICML (2013)

37. Gurobi Optimization, L.: Gurobi optimizer reference manual (2018), http://www.

gurobi.com

38. Hahnloser, R., Sarpeshkar, R., Mahowald, M., Douglas, R., Seung, S.: Digital
selection and analogue ampliﬁcation coexist in a cortex-inspired silicon circuit.
Nature 405 (2000)

39. Han, S., Pool, J., Narang, S., Mao, H., Tang, S., Elsen, E., Catanzaro, B., Tran,
J., Dally, W.: Dsd: regularizing deep neural networks with dense-sparse-dense
training ﬂow. arXiv preprint arXiv:1607.04381 (2016)

Lossless Compression of Deep Neural Networks

11

40. Han, S., Pool, J., Tran, J., Dally, W.: Learning both weights and connections for

eﬃcient neural network. In: NeurIPS (2015)

41. Hanin, B., Rolnick, D.: Complexity of linear regions in deep networks. In: ICML

(2019)

42. Hanin, B., Rolnick, D.: Deep relu networks have surprisingly few activation pat-

terns. In: NeurIPS (2019)

43. He, K., Zhang, X., Ren, S., Sun, J.: Deep residual learning for image recognition.

In: CVPR (2016)

44. Herrmann, C., Bowen, R., Zabih, R.: Deep networks with probabilistic gates.

CoRR abs/1812.04180 (2018)

45. Hinton, G., Deng, L., Dahl, G., Mohamed, A., Jaitly, N., Senior, A., Vanhoucke,
V., Nguyen, P., Sainath, T., Kingsbury, B.: Deep neural networks for acoustic
modeling in speech recognition. IEEE Signal Processing Magazine (2012)

46. Hornik, K., Stinchcombe, M., White, H.: Multilayer feed-forward networks are

universal approximators. Neural Networks (1989)

47. Hottung, A., Tanaka, S., Tierney, K.: Deep learning assisted heuristic tree search
for the container pre-marshalling problem. Computers & Operations Research
(2020)

48. Howard, A., Zhu, M., Chen, B., Kalenichenko, D., Wang, W., Weyand, T., An-
dreetto, M., Adam, H.: Mobilenets: Eﬃcient convolutional neural networks for
mobile vision applications. arXiv preprint arXiv:1704.04861 (2017)

49. Huang, G., Liu, Z., Maaten, L.V.D., Weinberger, K.: Densely connected convolu-

tional networks. In: CVPR (2017)

50. Hutter, F., Hoos, H.H., Leyton-Brown, K.: Sequential model-based optimization

for general algorithm conﬁguration. In: LIOn (2011)

51. Iandola, F., Han, S., Moskewicz, M., Ashraf, K., Dally, W., Keutzer, K.:
Squeezenet: Alexnet-level accuracy with 50x fewer parameters and < 0.5 MB
model size. arXiv preprint arXiv:1602.07360 (2016)

52. Icarte, R., Illanes, L., Castro, M., Cire, A., McIlraith, S., Beck, C.: Training
binarized neural networks using MIP and CP. In: International Conference on
Principles and Practice of Constraint Programming (CP) (2019)

53. Jaderberg, M., Vedaldi, A., Zisserman, A.: Speeding up convolutional neural net-

works with low rank expansions. BMVC (2014)

54. Kadioglu, S., Malitsky, Y., Sellmann, M., Tierney, K.: ISAC Instance-Speciﬁc

Algorithm Conﬁguration. In: ECAI (2010)

55. Khalil, E., Bodic, P., Song, L., Nemhauser, G., Dilkina, B.: Learning to branch

in mixed integer programming. In: AAAI (2016)

56. Khalil, E., Gupta, A., Dilkina, B.: Combinatorial attacks on binarized neural

networks. In: ICLR (2019)

57. Kolmogorov, V., Rother, C.: Minimizing nonsubmodular functions with graph

cuts-a review. TPAMI (2007)

58. Kotthoﬀ, L.: Algorithm selection for combinatorial search problems: A survey. AI

Magazine 35(3) (2014)

59. Koval, V., Schlesinger, M.: Two-dimensional programming in image analysis prob-

lems. USSR Academy of Science, Automatics and Telemechanics (1976)

60. Krizhevsky, A., Sutskever, I., Hinton, G.: Imagenet classiﬁcation with deep con-

volutional neural networks. In: NeurIPS (2012)

61. Kruber, M., L¨ubbecke, M., Parmentier, A.: Learning when to use a decomposition.

In: CPAIOR (2017)

62. Kumar, A., Serra, T., Ramalingam, S.: Equivalent and approximate transforma-

tions of deep neural networks. arXiv preprint arXiv:1905.11428 (2019)

12

T. Serra et al.

63. LeCun, Y., Bottou, L., Bengio, Y., Haﬀner, P.: Gradient-based learning applied

to document recognition. Proceedings of the IEEE (1998)

64. LeCun, Y., Bengio, Y., Hinton, G.: Deep learning. Nature 521 (2015)
65. Li, H., Kadav, A., Durdanovic, I., Samet, H., Graf, H.: Pruning ﬁlters for eﬃcient

convnets. arXiv preprint arXiv:1608.08710 (2016)

66. Lin, C., Zhong, Z., Wei, W., Yan, J.: Synaptic strength for convolutional neural

network. In: NeurIPS (2018)

67. Lin, H., Jegelka, S.: Resnet with one-neuron hidden layers is a universal approxi-

mator. In: NeurIPS (2018)

68. Liu, B., Wang, M., Foroosh, H., Tappen, M., Pensky, M.: Sparse convolutional

neural networks. In: CVPR (2015)

69. Lodi, A., Zarpellon, G.: On learning and branching: a survey. Top 25(2) (2017)
70. Lombardi, M., Milano, M.: Boosting combinatorial problem modeling with ma-

chine learning. In: IJCAI (2018)

71. Lomuscio, A., Maganti, L.: An approach to reachability analysis for feed-forward

ReLU neural networks. CoRR abs/1706.07351 (2017)

72. Luo, J.H., Wu, J., Lin, W.: Thinet: A ﬁlter level pruning method for deep neural

network compression. In: ICCV (2017)

73. Mhaskar, H., Poggio, T.: Function approximation by deep networks. CoRR

abs/1905.12882 (2019)

74. Molchanov, P., Tyree, S., Karras, T., Aila, T., Kautz, J.: Pruning convolu-
tional neural networks for resource eﬃcient transfer learning. arXiv preprint
arXiv:1611.06440 (2016)

75. Mont´ufar, G.: Notes on the number of linear regions of deep neural networks. In:

SampTA (2017)

76. Mont´ufar, G., Pascanu, R., Cho, K., Bengio, Y.: On the number of linear regions

of deep neural networks. In: NeurIPS (2014)

77. Nair, V., Hinton, G.: Rectiﬁed linear units improve restricted boltzmann ma-

chines. In: ICML (2010)

78. Narodytska, N., Kasiviswanathan, S., Ryzhyk, L., Sagiv, M., Walsh, T.: Verifying

properties of binarized deep neural networks. In: AAAI (2018)

79. Pascanu, R., Mont´ufar, G., Bengio, Y.: On the number of response regions of deep

feedforward networks with piecewise linear activations. In: ICLR (2014)

80. Paszke, A., Gross, S., Chintala, S., Chanan, G., Yang, E., DeVito, Z., Lin, Z., Des-
maison, A., Antiga, L., Lerer, A.: Automatic diﬀerentiation in pytorch. NeurIPS
Workshops (2017)

81. Peng, B., Tan, W., Li, Z., Zhang, S., Xie, D., Pu, S.: Extreme network compression

via ﬁlter group approximation. In: ECCV (2018)

82. Raghu, M., Poole, B., Kleinberg, J., Ganguli, S., Dickstein, J.: On the expressive

power of deep neural networks. In: ICML (2017)

83. Rastegari, M., Ordonez, V., Redmon, J., Farhadi, A.: Xnor-net: Imagenet classi-

ﬁcation using binary convolutional neural networks. In: ECCV (2016)

84. Ryu, M., Chow, Y., Anderson, R., Tjandraatmadja, C., Boutilier, C.: Caql: Con-

tinuous action Q-learning. CoRR abs/1909.12397 (2019)

85. Say, B., Wu, G., Zhou, Y.Q., Sanner, S.: Nonlinear hybrid planning with deep
net learned transition models and mixed-integer linear programming. In: IJCAI
(2017)

86. Serra, T., Ramalingam, S.: Empirical bounds on linear regions of deep rectiﬁer

networks. In: AAAI (2020)

87. Serra, T., Tjandraatmadja, C., Ramalingam, S.: Bounding and counting linear

regions of deep neural networks. In: ICML (2018)

Lossless Compression of Deep Neural Networks

13

88. Serra, T.: On deﬁning design patterns to generalize and leverage automated con-

straint solving (2012)

89. Singh, G., Gehr, T., Pschel, M., Vechev, M.: Robustness certiﬁcation with reﬁne-

ment. In: ICLR (2019)

90. Sutskever, I., Vinyals, O., Le, Q.: Sequence to sequence learning with neural

networks. In: NeurIPS (2014)

91. Szegedy, C., Liu, W., Jia, Y., Sermanet, P., Reed, S., Anguelov, D., Erhan, D.,
Vanhoucke, V., Rabinovich, A.: Going deeper with convolutions. In: CVPR (2015)
92. Tan, Y., Delong, A., Terekhov, D.: Deep inverse optimization. In: CPAIOR (2019)
93. Tang, Y., Agrawal, S., Faenza, Y.: Reinforcement learning for integer program-

ming: Learning to cut. CoRR abs/1906.04859 (2019)

94. Tang, Z., Peng, X., Li, K., Metaxas, D.: Towards eﬃcient u-nets: A coupled and

quantized approach. TPAMI (2019)

95. Telgarsky, M.: Beneﬁts of depth in neural networks. In: COLT (2016)
96. Tjeng, V., Xiao, K., Tedrake, R.: Evaluating robustness of neural networks with

mixed integer programming. In: ICLR (2019)

97. Tung, F., Mori, G.: Clip-q: Deep network compression learning by in-parallel

pruning-quantization. In: CVPR (2018)

98. Veit, A., Belongie, S.: Convolutional networks with adaptive computation graphs.

CoRR abs/1711.11503 (2017)

99. Wainwright, M., Jaakkola, T., Willsky, A.: Map estimation via agreement on
(hyper)trees: Message-passing and linear-programming approaches. IEEE Trans.
Information Theory (2005)

100. Wainwright, M., Jaakkola, T., Willsky, A.: Tree consistency and bounds on the
performance of the max-product algorithm and its generalizations. Statistics and
Computing (2004)

101. Wang, W., Sun, Y., Eriksson, B., Wang, W., Aggarwal, V.: Wide compression:

Tensor ring nets. In: CVPR (2018)

102. Wen, W., Wu, C., Wang, Y., Chen, Y., Li, H.: Learning structured sparsity in

deep neural networks. In: NeurIPS (2016)

103. Werner, T.: A linear programming approach to max-sum problem: A review.
Technical Report CTU-CMP-2005-25, Center for Machine Perception (2005)
104. Wong, E., Kolter, J.Z.: Provable defenses against adversarial examplesvia the

convex outer adversarial polytope. In: ICML (2018)

105. Wu, J., Leng, C., Wang, Y., Hu, Q., Cheng, J.: Quantized convolutional neural

networks for mobile devices. In: CVPR (2016)

106. Xiao, K., Tjeng, V., Shaﬁullah, N., Madry, A.: Training for faster adversarial

robustness veriﬁcation via inducing ReLU stability. ICLR (2019)

107. Xu, H., Koenig, S., Kumar, T.S.: Towards eﬀective deep learning for constraint

satisfaction problems. In: CP (2018)

108. Xue, Y., van Hoeve, W.J.: Embedding decision diagrams into generative adver-

sarial networks. In: CPAIOR (2019)

109. Ye, Z., Say, B., Sanner, S.: Symbolic bucket elimination for piecewise continuous

constrained optimization. In: CPAIOR (2018)

110. Yu, R., Li, A., Chen, C.F., Lai, J.H., Morariu, V., Han, X., Gao, M., Lin, C.Y.,
Davis, L.: Nisp: Pruning networks using neuron importance score propagation.
In: CVPR (2018)

111. Yu, X., Yu, Z., Ramalingam, S.: Learning strict identity mappings in deep residual

networks. In: CVPR (2018)

112. Zhang, X., Zou, J., Ming, X., He, K., Sun, J.: Eﬃcient and accurate approxima-

tions of nonlinear convolutional networks. In: CVPR (2015)

14

T. Serra et al.

113. Zhao, C., Ni, B., Zhang, J., Zhao, Q., Zhang, W., Tian, Q.: Variational convolu-

tional neural network pruning. In: CVPR (2019)

114. Zhou, H., Alvarez, J., Porikli, F.: Less is more: Towards compact CNNs. In: ECCV

(2016)

