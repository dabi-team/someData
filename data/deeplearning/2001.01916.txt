High-Performance Statistical Computing in
the Computing Environments of the 2020s

Seyoon Ko∗ Hua Zhou Jin J. Zhou Joong-Ho Won

Department of Biostatistics, University of California, Los Angeles, e-mail:kos@ucla.edu;
huazhou@ucla.edu.

Departments of Medicine, University of California, Los Angeles, e-mail:jinjinzhou@ucla.edu.

Departments of Statistics, Seoul National University, e-mail:won.j@snu.ac.kr.

Abstract: Technological advances in the past decade, hardware and software alike,
have made access to high-performance computing (HPC) easier than ever. We review
these advances from a statistical computing perspective. Cloud computing makes ac-
cess to supercomputers aﬀordable. Deep learning software libraries make programming
statistical algorithms easy and enable users to write code once and run it anywhere —
from a laptop to a workstation with multiple graphics processing units (GPUs) or a
supercomputer in a cloud. Highlighting how these developments beneﬁt statisticians,
we review recent optimization algorithms that are useful for high-dimensional models
and can harness the power of HPC. Code snippets are provided to demonstrate the
ease of programming. We also provide an easy-to-use distributed matrix data structure
suitable for HPC. Employing this data structure, we illustrate various statistical ap-
plications including large-scale positron emission tomography and (cid:96)1-regularized Cox
regression. Our examples easily scale up to an 8-GPU workstation and a 720-CPU-core
cluster in a cloud. As a case in point, we analyze the onset of type-2 diabetes from the
UK Biobank with 200,000 subjects and about 500,000 single nucleotide polymorphisms
using the HPC (cid:96)1-regularized Cox regression. Fitting this half-million-variate model
takes less than 45 minutes and reconﬁrms known associations. To our knowledge, this
is the ﬁrst demonstration of the feasibility of penalized regression of survival outcomes
at this scale.

Keywords and phrases: high-performance statistical computing, graphics processing
units (GPUs), cloud computing, deep learning, MM algorithms, ADMM, PDHG, Cox
regression.

1
2
0
2

l
u
J

6
1

]

O
C

.
t
a
t
s
[

3
v
6
1
9
1
0
.
1
0
0
2
:
v
i
X
r
a

∗This article is partly based on the ﬁrst author’s doctoral dissertation (Ko, 2020).

1

 
 
 
 
 
 
S. Ko, H. Zhou, J. J. Zhou, and J.-H. Won/High-performance statistical computing

2

1. Introduction

Clock speeds of the central processing units (CPUs) on the desktop and laptop computers
hit the physical limit more than a decade ago, and there will likely be no major breakthrough
until quantum computing becomes practical. Instead, the increase in computing power is
now accomplished by using multiple cores within a processor chip. High-performance com-
puting (HPC) means computations that are so large that their requirement on storage,
main memory, and raw computational speed cannot be met by a single (desktop) computer
(Hager and Wellein, 2010). Modern HPC machines are equipped with more than one CPU
that can work on the same problem (Eijkhout, 2016). Often, special-purpose co-processors
such as graphics processing units (GPUs) are attached to the CPU to improve the speed by
orders of magnitude for certain tasks. First developed for rendering graphics on a computer
screen, a GPU can be thought of a massively parallel matrix-vector multiplier and vector
transformer on a data stream. With increasing needs to analyze petabyte-scale data, the suc-
cess of large-scale statistical computing relies on eﬃciently engaging HPC in the statistical
practice.

About a decade ago, the second author discussed the potential of GPUs in statistical
computing: Zhou et al. (2010) predicted that “GPUs will fundamentally alter the landscape
of computational statistics.” Yet, it does not appear that GPU computing, or HPC in gen-
eral, has completely permeated the statistical community. Part of the reason for this may
be attributed to the fear that parallel and distributed code is diﬃcult to program, espe-
cially in R (R Core Team, 2021), the lingua franca of statisticians.1 On the other hand,
the landscape of scientiﬁc computing in general, including so-called data science (Donoho,
2017), has indeed substantially changed. Many high-level programming languages, such as
Python (van Rossum, 1995) and Julia (Bezanson et al., 2017), support parallel computing
by design or through standard libraries. Accordingly, many software tools have been de-
veloped in order to ease programming in and managing HPC environments. Last but not
least, cloud computing (Fox, 2011) is getting rid of the necessity for purchasing expensive
supercomputers and scales computation as needed.

Concurrently, easily parallelizable algorithms for ﬁtting statistical models with hundreds
of thousand parameters have also seen signiﬁcant advances. Traditional Newton-Raphson or
quasi-Newton type of algorithms face two major challenges in contemporary problems: 1)
explosion of dimensionality renders storage and inversion of Hessian matrices prohibitive; 2)
regularization of model complexity is almost essential in high-dimensional settings, which is
often realized by nondiﬀerentiable penalties; this leads to high-dimensional, nonsmooth opti-
mization problems. For these reasons, nonsmooth ﬁrst-order methods have been extensively
studied during the past decade (Beck, 2017), since Hessian matrix inversion can be com-
pletely avoided. For relatively simple, decomposable penalties (Negahban et al., 2012), the
proximal gradient method (Beck and Teboulle, 2009; Combettes and Pesquet, 2011; Parikh
and Boyd, 2014; Polson et al., 2015) produces a family of easily parallelizable algorithms.
For the prominent example of the Lasso (Tibshirani, 1996), this method contrasts to the
highly eﬃcient sequential coordinate descent method of Friedman et al. (2010) and smooth
approximation approaches, e.g., Hunter and Li (2005). Decomposability or separability of
variables is often the key to parallel and distributed algorithms. The alternating direction

1Although there exist several R packages for high-performance computing (Eddelbuettel, 2021), their
functionalities and usability appear not to match what is available in other languages. In particular, the
authors were not able to come up with a simple implementation of the computational tasks presented in
this paper without writing low-level C/C++ code or using an interface to Python.

S. Ko, H. Zhou, J. J. Zhou, and J.-H. Won/High-performance statistical computing

3

method of multipliers (ADMM, Gabay and Mercier, 1976; Boyd et al., 2011) achieves this
goal through variable splitting, while often resulting in nontrivial subproblems to solve. As
an alternative, the primal-dual hybrid gradient (PDHG) algorithm (Zhu and Chan, 2008;
Esser et al., 2010; Chambolle and Pock, 2011; Condat, 2013; V˜u, 2013) has a very low per-
iteration complexity, useful for complex penalties such as the generalized lasso (Tibshirani
and Taylor, 2011; Ko et al., 2019; Ko and Won, 2019). Another route toward separability
is the majorization-minimization (MM) principle (Lange et al., 2000; Hunter and Lange,
2004; Lange, 2016), which has been explored in Zhou et al. (2010). In fact, the proximal
gradient method can be viewed as a realization of the MM principle. Recent developments
in the application of this principle include distance majorization (Chi et al., 2014) and
proximal distance algorithms (Keys et al., 2019). When the matrix to be inverted to solve
the optimality condition has many independent components, nonsmooth Newton methods
(Kummer, 1988; Qi and Sun, 1993) can be a viable option; see Huang et al. (2021) for recent
applications to sparse regression. Nonsmooth Newton methods can also be combined with
ﬁrst-order methods for more complex nonsmooth penalties (Chu et al., 2020; Won, 2020).
The goal of this paper is to review the advances in parallel and distributed computing
environments during the past decade and demonstrate how easy it has become to write
code for large-scale, high-dimensional statistical models and run it on various distributed
environments. In order to make the contrast clear, we deliberately take examples from Zhou
et al. (2010), namely positron emission tomography (PET), nonnegative matrix factorization
(NMF), and multidimensional scaling (MDS). The diﬀerence lies in the scale of the examples:
our experiments deal with data of size at least 10, 000 × 10, 000 and as large as 200, 000 ×
200, 000 for dense data, and 810, 000 × 179, 700 for sparse data. This contrasts with the
size of at best 4096 × 2016 of Zhou et al. (2010). This level of scaling is possible because
the use of multiple GPUs in a distributed fashion has become handy, as opposed to the
single GPU, C-oriented programming environment of 2010. Furthermore, using the power
of cloud computing and modern deep learning software, we show that exactly the same,
easy-to-write code can run on multiple CPU cores and/or clusters of workstations. Thus we
bust the common misconception that deep learning software is dedicated to neural networks
and heuristic model ﬁtting. Wherever possible, we apply more recent algorithms in order to
cope with the scale of the problems. In addition, a new example of large-scale proportional
hazards regression model is investigated. We demonstrate the potential of our approach
through a single multivariate Cox regression model regularized by the (cid:96)1 penalty on the UK
Biobank genomics data (with 200,000 subjects), featuring time-to-onset of Type 2 Diabetes
(T2D) as outcome and 500,000 genomic loci harboring single nucleotide polymorphisms
as covariates. To our knowledge, such a large-scale joint genome-wide association analysis
has not been attempted. The reported Cox regression model retains a large proportion of
bona ﬁde genomic loci associated with T2D and recovers many loci near genes involved in
insulin resistance and inﬂammation, which may have been missed in conventional univariate
analysis with moderate statistical signiﬁcance values.

The rest of this article is organized as follows. We review HPC systems and see how they
have become easy to use in Section 2. In Section 3, we review software libraries employing the
“write once, run everywhere” principle (especially deep learning software) and discuss how
they can be employed for ﬁtting high-dimensional statistical models on the HPC systems
of Section 2. In Section 4, we review modern scalable optimization techniques well-suited
to HPC environments. We present how to distribute a large matrix over multiple devices in
Section 5, and numerical examples in Section 6. The article is concluded in Section 7.

S. Ko, H. Zhou, J. J. Zhou, and J.-H. Won/High-performance statistical computing

4

2. Accessible High-Performance Computing Systems

2.1. Preliminaries

Since modern HPC relies on parallel computing, in this section we review several concepts
from parallel computing literature at a level minimally necessary for the subsequent discus-
sions. Further details can be found in Nakano (2012); Eijkhout (2016).

Data parallelism. While parallelism can appear
such as
instruction-level and task-level, what is most relevant to statistical computing is data-level
parallelism or data parallelism. If data can be subdivided into several pieces that can be
processed independently of each other, then we say there is data parallelism in the problem.
Many operations such as scalar multiplication of a vector, matrix-vector multiplication, and
summation of all elements in a vector can exploit data parallelism using parallel architec-
tures, which will be discussed shortly.

at various

levels

Memory models.
In any computing system, processors (CPUs or GPUs) need to access
data residing in the memory. While physical computer memory uses complex hierarchies (L1,
L2, and L3 caches; bus- and network-connected, etc.), systems employ abstraction to provide
programmers an appearance of transparent memory access. Such logical memory models can
be categorized into the shared memory model and the distributed memory model. In the
shared memory model, all processors share the address space of the system’s memory even
if it is physically distributed. For example, when two processors refer to a variable x, the
variable is stored in the same memory address. Hence, if one processor alters the variable,
then the other processor is aﬀected by the modiﬁed value. Modern CPUs that have several
cores within a processor chip fall into this category. On the other hand, in the distributed
memory model, the system has memory both physically and logically distributed. Processors
have their own memory address spaces and cannot see each other’s memory directly. If two
processors refer to a variable x, then there are two separate memory locations, each of
which belongs to each processor under the same name. Hence the memory does appear
distributed to programmers, and some explicit communication mechanism is required in
order for processors to exchange data with each other. The advantage at the cost of this
complication is scalability — the number of processors that can work in a tightly coupled
fashion is much greater in distributed memory systems (say 100,000) than shared memory
systems (say four, as many recent laptops are equipped with a CPU chip with 4 cores).
Hybrids of the two memory models are also possible. A typical computer cluster consists
of multiple nodes interconnected in a variety of network topology. A node is a workstation
that can run standalone, with its main memory shared by several processors installed on
the motherboard. Hence within a node, it is a shared memory system, whereas across the
nodes the cluster is a distributed memory system.

Parallel programming models. For shared-memory systems, programming models based
on threads are most popular. A thread is a stream of machine language instructions that can
be created and run in parallel during the execution of a single program. OpenMP is a widely
used extension to the C and Fortran programming languages based on threads. It achieves
data parallelism by letting the compiler know what part of the sequential program is par-
allelizable by creating multiple threads. Simply put, each processor core can run a thread
operating on a diﬀerent partition of the data. In distributed-memory systems, parallelism

S. Ko, H. Zhou, J. J. Zhou, and J.-H. Won/High-performance statistical computing

5

is diﬃcult to achieve via a simple modiﬁcation of sequential code. The programmer needs
to coordinate communications between processors not sharing memory. A de facto standard
for such processor-to-processor communication is the message passing interface (MPI). MPI
routines mainly consist of point-to-point communication calls that send and receive data
between two processors, and collective communication calls that all processors in a group
participate in. Typical collective communication calls include

• Scatter: one processor has a data array, and each other processor receives a partition

of the array;

• Gather: one processor collects data from all the processors to construct an array;
• Broadcast: one processor sends its data to all other devices;
• Reduce: gather data and produce a combined output on a root process based on an

associative binary operator, such as sum or maximum of all the elements.

There are also all-gather and all-reduce, where the output is shared by all processors. At
a higher abstraction level, MapReduce (Dean and Ghemawat, 2008), a functional program-
ming model in which a “map” function transforms each datum into a key-value pair, and
a “reduce” function aggregates the results, is a popular distributed data processing model.
While basic implementations are provided in base R, both the map and reduce operations
are easy to parallelize. Distributed implementations such as Hadoop (Apache Software Foun-
dation, 2021) handle communications between nodes implicitly. This programming model
is inherently one-pass and stateless, and iterations on Hadoop require frequent accesses to
external storage (hard disks), hence slow. Apache Spark (Zaharia et al., 2010) is an imple-
mentation that substitutes external storage with memory caching, yet iterative algorithms
are an order of magnitude slower than their MPI counterparts (Jha et al., 2014; Reyes-Ortiz
et al., 2015; Gittens et al., 2016).

Parallel architectures. To realize the above models, a computer architecture that al-
lows simultaneous execution of multiple machine language instructions is needed. Single
instruction, multiple data (SIMD) architecture has multiple processors that execute the
same instruction on diﬀerent parts of the data. The GPU falls into this category of archi-
tectures, as its massive number of cores can run a large number of threads sharing memory.
Multiple instruction, multiple data (MIMD), or single program, multiple data (SPMD) ar-
chitecture has multiple CPUs that execute independent parts of program instructions on
their own data partition. Most computer clusters fall into this category.

2.2. Multiple CPU nodes: clusters, supercomputers, and clouds

Computing on multiple nodes can be utilized in many diﬀerent scales. For mid-sized data, one
may build his/her own cluster with a few nodes. This requires determining the topology and
purchasing all the required hardware, along with resources to maintain it. This is certainly
not an expertise of virtually all statisticians. Another option may be using a well-maintained
supercomputer in a nearby HPC center. A user can take advantage of the facility with up
to hundreds of thousand cores. The computing jobs on these facilities are often controlled
by a job scheduler, such as Sun Grid Engine (Gentzsch, 2001), Slurm (Yoo et al., 2003), and
Torque (Staples, 2006). However, access to supercomputers is almost always limited. Even
when the user has access to them, he/she often has to wait in a very long queue until the
requested computation job is started by the scheduler.

S. Ko, H. Zhou, J. J. Zhou, and J.-H. Won/High-performance statistical computing

6

In recent years, cloud computing, which refers to both the applications delivered as ser-
vices over the Internet, and the hardware and systems software in the data centers that
provide these services (Armbrust et al., 2010), has emerged as a third option. Information
technology giants such as Amazon, Microsoft, and Google lend their practically inﬁnite
computing resources to users on demand by wrapping the resources as “virtual machines,”
which are charged per CPU hours and storage. Users basically pay utility bills for their use
of computing resources. An important implication of this infrastructure to end-users is that
the cost of using 1000 virtual machines for one hour is almost the same as using a single
virtual machine for 1000 hours. Therefore a user can build his/her own virtual cluster “on
the ﬂy,” increasing the size of the cluster as the size of the problem to solve grows. A catch
here is that a cluster does not necessarily possess the power of HPC as suggested in Section
2.1: a requirement for high performance is that all the machines should run in tight lockstep
when working on a problem (Fox, 2011). However, early cloud services were more focused
on web applications that do not involve frequent data transmissions between computing
instances, and less optimized for HPC, yielding discouraging results (Evangelinos and Hill,
2008; Walker, 2008). For instance, “serverless computing” services such as AWS Lambda,
Google Cloud Functions, and Azure Functions allow users to run a function on a large
amount of data, in much the same fashion as supplying it to lapply() in base R. These
services oﬀer reasonable scalability on a simple map-reduce-type jobs such as image featur-
ization, word count, and sorting. Nevertheless, their restrictions on resources (e.g., single
core and 300 seconds of runtime in AWS Lambda) and the statelessness of the functional
programming approach result in high latency for iterative algorithms, such as consensus
ADMM (Aytekin and Johansson, 2019).

Eventually, many improvements have been made at hardware and software levels to make
HPC on clouds feasible. At hardware level, cloud service providers now support CPU in-
stances such as c4, c5, and c5n instances of Amazon Web Services (AWS), with up to 48
physical cores of higher clock speed of up to 3.4 GHz along with support for accelerated
SIMD computation. If network bandwidth is critical, the user may choose instances with
faster networking (such as c5n instances in AWS), allowing up to 100 Gbps of network band-
width. At the software level, these providers support tools that manage resources eﬃciently
for scientiﬁc computing applications, such as ParallelCluster (Amazon Web Services, 2021)
and ElastiCluster (University of Zurich, 2021). These tools are designed to run programs in
clouds in a similar manner to proprietary clusters through a job scheduler. In contrast to a
physical cluster in an HPC center, a virtual cluster on a cloud is exclusively created for the
user; there is no need for waiting in a long queue. Consequently, over 10 percent of all HPC
jobs are running in clouds, and over 70 percent of HPC centers run some jobs in a cloud as
of June 2019; the latter is up from just 13 percent in 2011 (Hyperion Research, 2019).

In short, cloud computing is now a cost-eﬀective option for statisticians who demand high

performance, without a steep learning curve.

2.3. Multi-GPU node

In some cases, HPC is achieved by installing multiple GPUs on a single node. A key feature
of GPUs is their ability to apply a mapping to a large array of ﬂoating-point numbers simul-
taneously. The mapping (called a kernel ) can be programmed by the user. This feature is
enabled by integrating a massive number of simple compute cores in a single processor chip,
forming a SIMD architecture. While this architecture of GPUs was created for high-quality
video games to generate a large number of pixels in a hard time limit, the programmabil-

S. Ko, H. Zhou, J. J. Zhou, and J.-H. Won/High-performance statistical computing

7

ity and high throughput soon gained attention from the scientiﬁc computing community.
Matrix-vector multiplication and elementwise nonlinear transformation of a vector can be
computed several orders of magnitude faster on GPU than on CPU. Early applications of
general-purpose GPU programming include physics simulations, signal processing, and geo-
metric computing (Owens et al., 2007). Technologically savvy statisticians demonstrated its
potential in Bayesian simulation (Suchard, Holmes and West, 2010; Suchard, Wang, Chan,
Frelinger, Cron and West, 2010) and high-dimensional optimization (Zhou et al., 2010; Yu
et al., 2015). Over time, the number of cores has increased from 240 (Nvidia GTX 285,
early 2009) to 4608 (Nvidia Titan RTX, late 2018) and more local memory — separated
from CPU’s main memory — has been added (from 1GB of GTX 285 to 24GB for Titan
RTX). GPUs could only use single-precision for their ﬂoating-point operations, but they now
support double- and half-precisions. More sophisticated operations such as tensor multipli-
cation are also supported. High-end GPUs are now being designed speciﬁcally for scientiﬁc
computing purposes, sometimes with fault-tolerance features such as error correction.

Major drawbacks of GPUs are smaller memory size, compared to CPU, and data transfer
overhead between CPU and GPU. These limitations can be addressed by using multiple
GPUs: recent GPUs can be installed on a single node and communicate with each other
without the meddling of CPU; this eﬀectively increases the local memory of a collection of
GPUs.2 It is relatively inexpensive to construct a node with 4–8 desktop GPUs compared
to a cluster of CPU nodes with a similar computing power (if the main computing tasks are
well suited for the SIMD model), and the gain is much larger than the cost. A good example
would be linear algebra operations that frequently occur in high-dimensional optimization.
Programming environments for GPU computing have been notoriously hostile to pro-
grammers for a long time. The major hurdle is that a programmer needs to write two suits
of code, the host code that runs on a CPU and kernel functions that run on GPU cores. Data
transfer between CPU and GPU(s) also has to be taken care of. Moreover, kernel functions
need to be written in special extensions of C, C++, or Fortran, e.g., the Compute Uniﬁed
Device Architecture (CUDA, Kirk, 2007) or Open Computing Language (OpenCL, Munshi,
2009). Combinations of these technical barriers prevented casual programmers, especially
statisticians, from writing GPU code despite its computational gains. There were eﬀorts to
sugar-coat these hostile environments with a high-level language such as R (Buckner et al.,
2009) or Python (Tieleman, 2010; Kl¨ockner et al., 2012; Lam et al., 2015), but these at-
tempts struggled to garner large enough user base since the functionalities were often limited
and inherently hard to extend.

Fortunately, GPU programming environments have been revolutionized since deep learn-
ing (LeCun et al., 2015) brought sensation to many machine learning applications. Deep
learning is almost synonymous to deep neural networks, which refer to a repeated (“lay-
ered”) application of an aﬃne transformation of the input followed by identical elementwise
transformations through a nonlinear link function, or “activation function.” Fitting a deep
learning model is almost always conducted via (approximate) minimization of the speciﬁed
loss function through a clever application of the chain rule to the gradient descent method,
called “backpropagation” (Rumelhart et al., 1986). These computational features ﬁt well
to the SIMD architecture of GPUs, use of which dramatically reduces the training time of
this highly overparameterized family of models with a huge amount of training data (Raina
et al., 2009). Consequently, many eﬀorts have been made to ease GPU programming for
deep learning, resulting in easy-to-use software libraries. Since the sizes of neural networks

2Lee et al. (2017) explored this possibility in image-based regression.

S. Ko, H. Zhou, J. J. Zhou, and J.-H. Won/High-performance statistical computing

8

get ever larger, more HPC capabilities, e.g., support for multiple GPUs and CPU clusters,
have been developed. As we review in the next section, programming with those libraries
gets rid of many hassles with GPUs, close to the level of conventional programming.

3. Easy-to-use Software Libraries for HPC

3.1. Deep learning libraries and HPC

As of revising this article (summer 2020), the two most popular deep learning software
libraries are TensorFlow (Abadi et al., 2016) and PyTorch (Paszke et al., 2019). There are
two common features of these libraries. One is the computation graph that automates the
evaluation of the loss function and its diﬀerentiation required for backpropagation. The other
feature, more relevant to statistical computing, is an eﬃcient and user-friendly interface to
linear algebra and convolution routines that work on both CPU and GPU in a uniﬁed
fashion. A typical pattern of using these libraries is to specify the model and describe how
to ﬁt the model to the training data in a high-level scripting language (mostly Python).
The system on which the model is ﬁtted can be programmed. If the target system is a CPU
node, then the software can be conﬁgured to utilize the OpenBLAS (Zhang et al., 2021) or
Intel Math Kernel Library (Wang et al., 2014), which are optimized implementations of the
Basic Linear Algebra Library (BLAS, Blackford et al., 2002) for shared-memory systems.
If the target system is a workstation with a GPU, then the same script can employ a pair
of host and kernel code that may make use of cuBLAS (NVIDIA, 2021a), a GPU version
of BLAS, and cuSPARSE (NVIDIA, 2021b), GPU-oriented sparse linear algebra routines.
A slight change in the option for device selection — usually a line or two in the script —
can control whether to run the model on a CPU or GPU. From the last paragraph of the
previous section, we see that this “write once, run everywhere” feature of deep learning
libraries can make GPU programming easier for statistical computing as well.

TensorFlow is a successor of Theano (Theano Development Team, 2016), one of the
ﬁrst libraries to support automatic diﬀerentiation based on computational graphs. Unlike
Theano, which generates GPU code on the ﬂy, TensorFlow is equipped with pre-compiled
GPU code for a large class of pre-deﬁned operations. PyTorch inherits Torch (Collobert
et al., 2011), an early machine learning library written in a functional programming language
called Lua, and Caﬀe (Jia et al., 2014), a Python-based deep learning library. PyTorch (and
Torch) can also manage GPU memory eﬃciently. As a result, it is known to be faster than
other deep learning libraries (Bahrampour et al., 2016).

Both libraries support multi-GPU and multi-node computing.3 In TensorFlow, multi-
GPU computation is supported natively on a single node. If data are distributed in multiple
GPUs and one needs data from the other, the GPUs communicate with each other implicitly
and the user does not need to interfere. For multi-node communication, it is recommended
to use MPI through the library called Horovod (Sergeev and Del Balso, 2018) for tightly-
coupled HPC environments. In PyTorch, both multi-GPU and multi-node computing are
enabled by using the interface torch.distributed. This interface deﬁnes MPI-style (but
simpliﬁed) communication primitives (see Section 2.1). Implementations include the bona

3There are other deep learning software libraries with similar HPC supports: Apache MxNet (Chen
et al., 2015) supports multi-node computation via Horovod; multi-GPU computing is also supported at the
interface level. Microsoft Cognitive Toolkit (CNTK, Seide and Agarwal, 2016) supports parallel stochastic
gradient algorithms through MPI.

S. Ko, H. Zhou, J. J. Zhou, and J.-H. Won/High-performance statistical computing

9

ﬁde MPI, Nvidia Collective Communications Library (NCCL), and Gloo (Facebook In-
cubator, 2021). Recent MPI implementations can map multi-GPU communication to the
MPI standard as well as traditional multi-node communication. While NCCL is useful for
a multi-GPU node, Gloo is useful with multiple CPU with Ethernet interconnect.

3.2. Automatic diﬀerentiation

The automatic diﬀerentiation (AD) feature of deep learning software deserves separate at-
tention. AD refers to a collection of techniques that evaluate the derivatives of a function
speciﬁed by a computer program accurately (Griewank and Walther, 2008; Baydin et al.,
2017). Based on AD, complex deep models can be trained with stochastic approximation
(see the next section) on huge data within a hundreds of lines of code and approximate a rich
class of functions eﬃciently; see Schmidt-Hieber et al. (2020); Bauer et al. (2019); Imaizumi
and Fukumizu (2019); Suzuki (2019); Ohn and Kim (2019) for recent theoretical develop-
ments. Most AD techniques rely on decomposition of the target function into elementary
functions (primitives) whose derivatives are known, and the computational graph, either
explicitly or implicitly, that describes the dependency among the primitives. Fig. 1 illus-
trates the computational graph for the bivariate function f (x1, x2) = log(x1 + x2) − x2
2. The
internal nodes represent intermediate variables corresponding to the primitives: z−1 = x1,
z0 = x2, z1 = z−1 + z0, z2 = log z1, z3 = z2

0, and z4 = z2 − z3; y = z4.

˙z0 = ˙x2 = 1,

we see that ˙z−1 = ˙x1 = 0,

˙z3 = 2z0 ˙z0 = (2)(2)(1) = 4,

There are two modes of AD, depending on the order of applying the chain rule. Forward-
mode AD applies the rule from right to left (or from input to output), hence it is straight-
forward to implement. In Fig. 1, if we want to evaluate the partial derivative ∂f
at (3, 2),
∂x2
then by denoting ˙zi ≡ ∂zi
˙z1 = ˙z0 + ˙z1 = 1,
∂x2
˙z2 = ˙z1/z1 = 1/5,
˙z4 = ˙z2 − ˙z3 = 1/5 − 4, and ﬁnally
˙y = ˙z4 = −3.8. While this computation can be conducted in a single pass with evaluation
of the original function f , computing another derivative ∂f
requires a separate pass. Thus,
∂x1
forward mode is ineﬃcient if the whole gradient of a function with many input variables is
needed, e.g., the loss function of a high-dimensional model. Reverse-mode AD applies the
chain rule in the opposite direction. In the ﬁrst pass, the original function and the associ-
ated intermediate variables zi are evaluated from input to output. In the second pass, the
“adjoint” variables ¯zi ≡ ∂y
are initialized to zero and updated from output to input. In
∂zi
Fig. 1, ¯z4 += ∂y
= ¯z3(2z0) = −4,
∂z4
= ¯z2
¯z1 += ¯z2
= 1/5. Here, the ‘+=’
z2
is the C-style increment operator, employed in order to observe the rule of total deriva-
tives. (Note ¯z0 is updated twice.) Finally, ∂f
= ¯x2 = ¯z0 = −3.8.
∂x1
Hence reverse-mode AD generalizes the backpropagation algorithm and computes the whole
gradient ∇f in a single backward pass, at the expense of keeping intermediate variables.

∂z4
= 1, ¯z3 += ¯z4
∂z3
= 1/5, ¯z0 += ¯z1

= −1, ¯z2 += ¯z4
∂z1
∂z0

= ¯x1 = ¯z−1 = 0.2 and ∂f
∂x2

= 1/5, and ¯z−1 += ¯z1

= 1, ¯z0 += ¯z3

∂z1
∂z−1

∂z2
∂z1

∂z3
∂z0

∂z4
∂z2

Deep learning software can be categorized by the way they build computational graphs.
In Theano and TensorFlow, the user needs to construct a static computational graph us-
ing a specialized mini-language before executing the model ﬁtting process, and the graph
cannot be modiﬁed throughout the execution. This static approach has performance advan-
tage since there is room for optimizing the graph structure. Its disadvantage is the limited
expressiveness of computational graphs and AD. On the other hand, PyTorch employs dy-
namic computational graphs, for which the user describes the model as a regular program
for (forward) evaluation of the loss function. Intermediate values and computation trace are
recorded in the forward pass, and the gradient is computed by parsing the recorded compu-

S. Ko, H. Zhou, J. J. Zhou, and J.-H. Won/High-performance statistical computing

10

¯x1 = ¯z−1

¯z1

¯z2

x1

z−1

z1

z2

¯z4

¯z1

¯y

¯z4

z4

y = f (x1, x2)

¯x2 = ¯z0

¯z3

x2

z0

z3

Fig 1: Computational graph for evaluating function f (x1, x2) = log(x1 + x2) − x2
arrows indicate the direction of backpropagation evaluating ∇f (x1, x2).

2. Dashed

tation backwards. The advantage of this dynamic graph construction is the expressiveness
of the model: in particular, recursion is allowed in the loss function deﬁnition. For example,
recursive models such as f (x) = f (x/2) if x > 1 and x otherwise are diﬃcult to describe
using a static graph but easy with a dynamic one. The downside is slower evaluation due to
function call overheads.

3.3. Case study: PyTorch versus TensorFlow

In this section, we illustrate how simple it is to write a statistical computing code on multi-
device HPC environments using modern deep learning libraries. We compare PyTorch and
TensorFlow code written in Python, which computes a Monte Carlo estimate of the constant
π. The emphasis is on readability and ﬂexibility, i.e., how small a modiﬁcation is needed to
run the code written for a single-CPU node on a multi-GPU node and a multi-node system.
Listing 1 shows the code for Monte Carlo estimation of π using PyTorch. Even for those
who are not familiar with Python, the code should be quite readable. The main workhorse
is function mc pi() (Lines 14–21), which generates a sample of size n from the uniform
distribution on the unit square [0, 1]2 and compute the proportion of the points that fall
inside the quarter circle of unit radius centered at the origin. Listing 1 is a fully executable
program. It uses torch.distributed interface with an MPI backend (Line 3). An in-
stance of the program of Listing 1 is attached to a device and is executed as a “process”.
Each process is given its identiﬁer (rank), which is retrieved in Line 6. The total number
of processes is known to each process via Line 7. After the proportion of the points in the
quarter-circle is computed in Line 22, each process gathers the sum of the means computed
from all the processes in Line 25 (this is called the all-reduce operation; see Section 2.1).
Line 27 divides the sum by the number of processes, yielding a Monte Carlo estimate of π
based on the sample size of n × (number of processes).

We have been deliberately ambiguous about the “devices.” Here, a CPU core or a GPU is
referred to as a device. Listing 1 assumes the environment is a workstation with one or more
GPUs, and the backend MPI is CUDA-aware. A CUDA-aware MPI, e.g., OpenMPI (Gabriel
et al., 2004), allows data to be sent directly from a GPU to another GPU through the MPI
protocols. Data transfer between modern GPUs does not go through CPU (Lee et al., 2017).
Lines 9 –10 specify that the devices to use in the program are GPUs. For example, suppose
the workstation has four GPUs, say device 0 through 3. A likely scenario for carrying out the
all-reduce operation in Line 25 is to transfer the estimated π in device 1 (computed in Line

S. Ko, H. Zhou, J. J. Zhou, and J.-H. Won/High-performance statistical computing

11

1 # import packages
2 import torch.distributed as dist
3 import torch
4 dist.init_process_group(’mpi’)

5
6 rank = dist.get_rank()
7 size = dist.get_world_size()

# initialize MPI

# device id
# total number of devices

8
9 # select device
10 device = ’cuda:{}’.format(rank) # or simply ’cpu’ for CPU computing
11 # select GPU based on rank.
12 if device.startswith(’cuda’): torch.cuda.set_device(rank)

13
14 def mc_pi(n):

15

16

17

18

19

20

21

22

23

24

25

26

27

# this code is executed on each device.
# generate n samples from Unif(0, 1) for x and y
x = torch.rand((n), dtype=torch.float64, device=device)
y = torch.rand((n), dtype=torch.float64, device=device)
# compute local estimate of pi in float64.
# type conversion is necessary, because (x ** 2 + y ** 2 < 1)
# results in unsigned 8-bit integer.
pi_hat = torch.mean((x**2 + y**2 <1).to(dtype=torch.float64))*4
# sum of the estimates across processes
#
dist.all_reduce(pi_hat)
# the final estimate of pi, computed on each process
return pi_hat / size

is stored in-place in ’pi_hat’, overwriting its original value.

28
29 if __name__ == ’__main__’:

30

31

32

33

34

n = 10000
pi_hat = mc_pi(n)
print("Pi estimate based on {} Monte Carlo samples across {} processes.".

format(n * size, size))

if rank == 0:

print(pi_hat.item())

Listing 1: Distributed Monte Carlo estimation of π using PyTorch

22, which is parallelized) to device 0, where the two estimates are added. At the same time,
the estimate in device 3 is passed to device 2 and then added with another estimate there.
After this step, the sum in device 2 is sent to device 0 to compute the ﬁnal sum. This sum
is broadcast to all the other devices to replace the local estimates. (The actual behavior
may be slightly diﬀerent from this scenario depending on the speciﬁc implementation of
MPI.) If the environment is a cluster with multiple CPU nodes (or even a single node), then
communication between nodes or CPU cores through high-speed interconnect replaces the
inter-GPU communication. At the code level, all we need to do is change Line 10 to device
= ’cpu’. The resulting code runs on a cluster seamlessly as long as the MPI for the cluster
is properly installed.

In TensorFlow, however, a separate treatment of multi-GPU and cluster settings is almost
necessary. The code for multi-GPU setting is similar to Listing 1 and is given in Appendix
C. In a cluster setting, unfortunately, it is extremely diﬃcult to reuse the multi-GPU code.
If direct access to individual compute nodes is available, that information can be used to

S. Ko, H. Zhou, J. J. Zhou, and J.-H. Won/High-performance statistical computing

12

run the code distributedly, albeit not intuitively. However, in HPC environments where
computing jobs are managed by job schedulers, we often do not have direct access to the
compute nodes. The National Energy Research Scientiﬁc Computing Center (NERSC), the
home of the 16th most powerful supercomputers in the world (as of June 2020), advises
that gRPC, the default inter-node communication method of TensorFlow, is very slow on
tightly-coupled nodes, thus recommends a direct use of MPI (NERSC, 2021). Using MPI
with TensorFlow requires an external library called Horovod and a substantial modiﬁcation
of the code, as shown in Listing 2. This is a sharp contrast to Listing 1, where essentially
the same PyTorch code can be used in both multi-GPU and multi-node settings.

Due to the reasons stated in Section 3.2, we employ PyTorch in the sequel to implement
the highly parallelizable algorithms of Section 4 in a multi-GPU node and a cluster on a
cloud, as it allows simpler code that runs on various HPC environments with a minimal
modiﬁcation. (In fact, this modiﬁcation can be made automatic through a command line
argument.)

1 import tensorflow as tf
2 import horovod.tensorflow as hvd

3
4 # initialize horovod
5 hvd.init()
6 rank = hvd.rank()

7
8 # without this block, all the processes try to allocate
9 # all the memory from each device, causing out of memory error.
10 devices = tf.config.experimental.list_physical_devices("GPU")
11 if len(devices) > 0:

12

13

for d in devices:

tf.config.experimental.set_memory_growth(d, True)

14
15 # select device
16 tf.device("device:gpu:{}".format(rank)) # tf.device("device:cpu:0") for CPU

17
18 # function runs in parallel with (graph computation/lazy-evaluation)
19 # or without (eager execution) the line below
20 @tf.function
21 def mc_pi(n):

22

23

24

25

26

27

28

29

# this code is executed on each device
x = tf.random.uniform((n,), dtype=tf.float64)
y = tf.random.uniform((n,), dtype=tf.float64)
# compute local estimate for pi and save it as ’estim’.
estim = tf.reduce_mean(tf.cast(x**2 + y ** 2 <1, tf.float64))*4
# compute the mean of ’estim’ over all the devices
estim = hvd.allreduce(estim)
return estim

30
31 if __name__ == ’__main__’:

32

33

34

35

36

n = 10000
estim = mc_pi(n)
# print the result on rank zero
if rank == 0:

print(estim.numpy())

Listing 2: Monte Carlo estimation of π for TensorFlow on multiple nodes using Horovod

S. Ko, H. Zhou, J. J. Zhou, and J.-H. Won/High-performance statistical computing

13

4. Highly Parallelizable Algorithms

In this section, we discuss some easily parallelizable optimization algorithms useful for ﬁtting
high-dimensional statistical models, assuming that data are so large that they have to be
stored distributedly. These algorithms can beneﬁt from the distributed-memory environment
by using relatively straightforward operations, via distributed matrix-vector multiplication
and independent update of variables.

4.1. MM algorithms

The MM principle (Lange et al., 2000; Lange, 2016), where “MM” stands for either majorization-
minimization or minorization-maximization, is a useful tool for constructing parallelizable
optimization algorithms. In minimizing an objective function f (x) iteratively, for each iter-
ate we consider a surrogate function g(x|xn) satisfying two conditions: the tangency con-
dition f (xn) = g(xn|xn) and the domination condition f (x) ≤ g(x|xn) for all x. Updating
xn+1 = arg minx g(x|xn) guarantees that {f (xn)} is a nonincreasing sequence:

f (xn+1) ≤ g(xn+1|xn) ≤ g(xn|xn) = f (xn).

In fact, full minimization of g(x|xn) is not necessary for the descent property to hold; merely
decreasing it is suﬃcient. For instance, it can be shown that the EM algorithm (Dempster
et al., 1977) is obtained by applying the MM principle to to the observed-data log likelihood
and Jensen’s inequality. (See Wu and Lange (2010) for more details about the relation
between MM and EM.)

MM updates are usually designed to make a nondiﬀerentiable objective function smooth,
linearize the problem, or avoid matrix inversions by a proper choice of a surrogate function.
MM is naturally well-suited for parallel computing environments, as we can choose a separa-
ble surrogate function and update variables independently. For example, when maximizing
loglikelihoods, a term involving summation inside the logarithm log((cid:80)p
i=1 ui), ui > 0, often
arises. By using Jensen’s inequality, this term can be minorized and separated as

(cid:32) p

(cid:88)

(cid:33)

ui

≥

log

i=1

p
(cid:88)

i=1

un
i
j=1 un
j

(cid:80)p

log

(cid:32) (cid:80)p

j=1 un
j
un
i

(cid:33)

ui

=

(cid:32)

p
(cid:88)

i=1

(cid:33)

un
i
j=1 un
j

(cid:80)p

log ui + cn,

i ’s are constants and cn is a constant only depending on un

where un
i ’s. Parallelization of MM
algorithms on a single GPU using separable surrogate functions is extensively discussed in
Zhou et al. (2010). Separable surrogate functions are especially important in distributed
HPC environments, e.g. multi-GPU systems.

4.2. Proximal gradient method

The proximal gradient method extends the gradient descent method, and deals with mini-
mization of sum of two extended real-valued convex functions, i.e.,

Function f is possibly nondiﬀerentiable, while g is continuously diﬀerentiable.

min
x

f (x) + g(x).

(1)

S. Ko, H. Zhou, J. J. Zhou, and J.-H. Won/High-performance statistical computing

14

We ﬁrst deﬁne the proximity operator of f :

proxλf (y) = arg min

x

(cid:8)f (x) + 1

2λ (cid:107)x − y(cid:107)2

2

(cid:9) , λ > 0

For many functions their proximity operators take closed forms. We call such functions
“proximable”. For example, consider the 0/∞ indicator function δC(x) of a closed convex
set C, i.e., δC(x) = 0 if x ∈ C, and +∞ otherwise. The corresponding proximity operator
is the Euclidean projection onto C: PC(y) = arg minx∈C (cid:107)y − x(cid:107)2. For many sets, e.g.,
nonnegative orthant, PC is simple to compute. Also note that the proximity operator of the
(cid:96)1-norm λ(cid:107) · (cid:107)1 is the soft-thresholding operator: [Sλ(y)]i := sign(yi)(|yi| − λ)+.

Now we proceed with the proximal gradient method for minimization of h(x) = f (x) +
g(x). Assume g is convex and has an L-Lipschitz gradient, i.e., (cid:107)∇g(x) − ∇g(y)(cid:107)2 ≤ L(cid:107)x −
y(cid:107)2 for all x, y in the interior of its domain, and f is lower-semicontinuous, convex, and
proximable. The L-Lipschitz gradients naturally result in the following surrogate function
that majorizes h:

h(x) ≤ f (x) + g(xn) + (cid:104)∇g(xn), x − xn(cid:105) + L

2 (cid:107)x − xn(cid:107)2
L ∇g(xn)(cid:13)
2
2 − 1
(cid:13)
Minimizing p(x|xn) with respect to x results in the iteration:

= f (x) + g(xn) + L
2

(cid:13)
(cid:13)x − xn + 1

2

2L (cid:107)∇g(xn)(cid:107)2

2 =: p(x|xn).

xn+1 = proxγnf (xn − γn∇g(xn)) , γn ∈ (0, 1/L] .

(2)

If f ≡ 0, then iteration (2) reduces to the conventional gradient descent. This iteration
guarantees a nonincreasing sequence of h(xn) by the MM principle. Proximal gradient
method also has an interpretation of forward-backward operator splitting, and the step size
γn ∈ (0, 2/L) guarantees convergence (Combettes and Pesquet, 2011; Combettes, 2018). If
f (x) = δC(x), then the corresponding algorithm is called the projected gradient method.
If f (x) = λ(cid:107)x(cid:107)1, then it is the iterative shrinkage-thresholding algorithm (ISTA, Beck and
Teboulle, 2009).

For many functions f , the update (2) is simple and easily parallelized, thus the algorithm
is suitable for HPC. For example, in the soft-thresholding operator above all the elements
are independent. If f (x) = −a log x, then

proxγf (y) = (cid:0)y +

(cid:112)

y2 + 4γa(cid:1)/2,

(3)

(cid:80)m

[1], AT

i=1 (cid:96)(aT

[2], · · · , AT

which is useful for the PET example in Section 6. The gradient ∇g in update (2) can
also be computed in parallel. In many models the ﬁtting problem takes the form of (1)
i x), where (cid:96) is a loss function and ai ∈ Rp is the ith observation.
with g(x) = 1
m
Collect the latter into a data matrix A ∈ Rm×p. If m (cid:29) p, then split it by the row as A =
[AT
[d]]T , where blocks A[k] are distributed over d devices. If the current iterate
of the parameter xn is known to each device, then the local gradient ∇gi(xn) = (cid:96)(cid:48)(aT
i x)ai can
be computed from A[k] independently. The full gradient ∇g(xn) can be computed then by
averaging ∇gi(xn). In the MPI terminology of Section 2.1, a distributed-memory proximal
gradient update consists of the following steps: 1) broadcast xn; 2) compute the local gradient
∇gi(xn) in each device; 3) reduce the local gradients to compute the full gradient ∇g(xn) in
the master device; 4) update xn+1. If g is not separable in observations, splitting the data
matrix by column may be useful (Section 6.3).

See Parikh and Boyd (2014) for a thorough review and distributed-memory implementa-

tions, and Polson et al. (2015) for a statistically oriented review.

S. Ko, H. Zhou, J. J. Zhou, and J.-H. Won/High-performance statistical computing

15

4.3. Primal-dual methods

Primal-dual methods introduce an additional dual variable y (where x is the primal vari-
able) in order to deal with a larger class of problems. Consider the problems of the form
h(x) = f (Kx) + g(x), where K ∈ Rl×p. We further assume that f and g are lower semi-
continuous, convex, and proper (i.e., not always ∞) functions. Even if f is proximable, the
proximity operator for f (K·) is not easy to compute. The conjugate of f is deﬁned as f ∗(y) =
supx(cid:104)x, y(cid:105) − f (x). It is known that f ∗∗ = f , so f (Kx) = f ∗∗(Kx) = supy(cid:104)Kx, y(cid:105) − f ∗(y).
Then the minimization problem inf x f (Kx) + g(x) is equivalent to the saddle-point problem

inf
x

(cid:104)Kx, y(cid:105) + g(x) − f ∗(y),

sup
y

for which a solution (ˆx, ˆy) exists under mild conditions.

A widely known method for solving this saddle-point problem in the statistical literature
is the ADMM (Xue et al., 2012; Ramdas and Tibshirani, 2016; Zhu, 2017; Lee et al., 2017;
Gu et al., 2018), whose update is given by:

xn+1 = arg min

x

g(x) + (t/2)(cid:107)Kx − ˜xn + (1/t)yn(cid:107)2
2

˜xn+1 = prox(1/t)f (Kxn+1 + (1/t)yn)
yn+1 = yn + t(Kxn+1 − ˜xn+1).

(4a)

(4b)

(4c)

If g is separable, i.e., g(x) = (cid:80)d
k=1 gk(x), then consensus optimization (Boyd et al., 2011,
Chap. 7) applies ADMM to distributed copies of variables xk = x to minimize h(x) =
f (z) + (cid:80)d

k=1 gk(xk) subject to xk = x and Kxk = z for each k:

gk(xk) + t

xn+1
k = arg min
xk

˜xn+1 = prox(dt)−1f
yn+1
k = yn

(cid:0) 1
d
k + t(Kxn+1

k + 1
k − ˜xn+1), wn+1

(cid:80)d

2 (cid:107)Kxk − ˜xn + 1
k=1(Kxn+1

k (cid:107)2
t yn
k )(cid:1)
t yn
k = wn

k + t(xn+1

k − xn+1).

2 + t

2 (cid:107)xk − xn + 1

t wn

k (cid:107)2
2

(5a)

(5b)

(5c)

A distributed-memory implementation will iterate the following steps: 1) for each device k,
solve (5a) in parallel; 2) gather local solutions xn
k in the master device; 3) compute (5b); 4)
broadcast ˜xn+1; 5) compute (5c).

Nonetheless, neither update (4a) nor (5a) results in a proximity operator, since the
quadratic term is not spherical. This inner optimization problem is often nontrivial to solve.
In the simplest case of linear regression, g is quadratic and (4a) involves solving a (large)
linear system whose time complexity is cubic in the dimension p of the primal variable x.

PDHG avoids inner optimization via the following iteration:

yn+1 = proxσf ∗ (yn + σK ¯xn)
xn+1 = proxτ g(xn − τ K T yn+1)
¯xn+1 = 2xn+1 − xn,

(6a)

(6b)

(6c)

where σ and τ are step sizes. If f is proximable, so is f ∗, since proxγf ∗ (x) = x−γproxγ−1f (γ−1x)
by Moreau’s decomposition. This method has been analyzed using monotone operator the-
ory (Condat, 2013; V˜u, 2013; Ko et al., 2019). Convergence of iteration (6) is guaranteed

S. Ko, H. Zhou, J. J. Zhou, and J.-H. Won/High-performance statistical computing

16

if στ (cid:107)K(cid:107)2
gradient, then the proximal step (6b) can be replaced by a gradient step

2 < 1, where (cid:107)M (cid:107)2 is the spectral norm of matrix M . If g has an L-Lipschitz

xn+1 = xn − τ (∇g(xn) + K T yn+1).

PDHG algorithms are also highly parallelizable as long as the involved proximity operators
are easy to compute and separable. No inner optimization is involved in iteration (6) and only
matrix-vector multiplications appear. The distributed computation of gradient in Section
4.2 can be used for the gradient step. A hybrid of PDHG and ADMM has recently been
proposed (Ryu et al., 2020).

4.4. Parallel coordinate descent and stochastic approximation

i

j(cid:54)=i xn

h(x1, . . . , x(cid:48)

Coordinate descent methods apply vector-to-scalar maps Ti : Rp → R : x = (x1, . . . , xi, . . . , xp) (cid:55)→
arg minx(cid:48)
i, . . . , xp) deﬁned for each coordinate i successively to minimize h(x).
The most well-known variant is the cyclic or Gauss-Seidel version. If we denote the jth
elementary unit vector in Rp by ej, then the update rule is xn+1 = (cid:80)
j ej + Ti(x)ei
where i = (n − 1 mod p) + 1, which possesses the descent property. The parallel or Ja-
cobi update reads xn+1 = (cid:80)p
j=1 Tj(x)ej. Obviously, if h is separable in variables, i.e.,
h(x) = (cid:80)p
j=1 hj(xj), this minimization strategy will succeed. Other variants are also pos-
sible, such as randomizing the cyclic order, or updating a subset of coordinates in par-
allel at a time. The “argmin” map Ti can also be relaxed, e.g., by a prox-linear map
i − xi(cid:107)2
2 + f (x) if h has a structure of h = f + g
x (cid:55)→ arg minx(cid:48)
and only g is diﬀerentiable (Tseng and Yun, 2009). See Wright (2015) for a recent review.
If p is much larger than τ , the number of devices, then choosing a subset of coordinates
with size comparable to τ would reduce the complexity of an iteration. Richt´arik and Tak´aˇc
(2016a,b) consider sampling a random subset and study the eﬀect of the sampling distribu-
tion on the performance of parallel prox-linear updates, deriving optimal distributions for
certain cases. In particular, the gain of parallelization is roughly proportional to the degree
of separability p/ω, where ω = maxJ∈J |J| if h(x) = (cid:80)
J∈J hJ (x) for a ﬁnite collection of
nonempty subsets of {1, . . . , p} and hJ depends only on coordinates i ∈ J. For example, if
m]T ∈ Rm×p is the data matrix for ordinary least squares, then ω equals to the
A = [aT
maximum number of nonzero elements in the rows, or equivalently ω = maxi=1,...,n (cid:107)ai(cid:107)0.

i − xi(cid:105) + 1
2γi

1 , . . . , aT

|xi, x(cid:48)

(cid:104) ∂g
∂xi

(cid:107)x(cid:48)

i

(cid:80)m

i=1 (cid:96)(aT

i x), and f ≡ 0, then (cid:96)(cid:48)(aT

For gradient-descent type methods, stochastic approximation (Robbins and Monro, 1951,
see Lai and Yuan (2021) for a recent review) has gained wide popularity under the name of
stochastic gradient descent or SGD. The main idea is to replace the gradient of the expected
loss by its unbiased estimator. For instance, as in the penultimate paragraph of Section 4.2,
if g(x) = 1
i x)ai is an unbiased estimator of ∇g(x)
m
under the uniform distribution on the sample indices {1, . . . , m}. The update rule is then
xn+1 = xn − γn(cid:96)(cid:48)(aT
i xn)ai for some randomly chosen i. SGD and its variants (Defazio et al.,
2014; Johnson and Zhang, 2013) are main training methods in most deep learning software,
since the sample size m needs to be extremely large to properly train deep neural networks.
The idea of using an unbiased estimator of the gradient has been extended to the proximal
gradient (Nitanda, 2014; Xiao and Zhang, 2014; Atchad´e et al., 2017; Rosasco et al., 2019)
and PDHG (Chen et al., 2014; Ko et al., 2019; Ko and Won, 2019) methods. In practice,
it is standard to use a minibatch or a random subset of the sample for each iteration, and
the arbitrary sampling paradigm of Richt´arik and Tak´aˇc (2016a,b) for parallel coordinate
descent has been extended to minibatch SGD (Gower et al., 2019; Qian et al., 2019) and
PDHG (Chambolle et al., 2018).

S. Ko, H. Zhou, J. J. Zhou, and J.-H. Won/High-performance statistical computing

17

5. Distributed matrix data structure for PyTorch

For the forthcoming examples and potential future uses in statistical computing, we propose
the package dist stat built on PyTorch. It consists of two submodules, distmat and
application. The submodule distmat implements a simple distributed matrix data
structure, and the submodule application includes the code for the examples in Section
6 using distmat. In the data structure distmat, each process, enumerated by its rank,
holds a contiguous block of the full data matrix by rows or columns, which may be sparse.
If multiple GPUs are involved, each process controls the GPU whose index matches the
process rank. The blocks are assumed to have equal sizes. For notational simplicity, we
indicate the dimension to split by a pair of square brackets: if a [100] × 100 matrix is split
over four processes, the rank-0 process keeps the ﬁrst 25 rows of the matrix, the rank-1
process takes the next 25 rows, and so on. For the sake of simplicity, we always assume that
the dimension to split is a multiple of the number of processes. The code for dist stat is
available at https://github.com/kose-y/dist_stat. A proper backend setup for a
cloud environment is explained in Appendix B.

In distmat, unary elementwise operations such as exponentiation, square root, absolute
value, and logarithm of matrix entries are implemented in an obvious manner. Binary ele-
mentwise operations such as addition, subtraction, multiplication, division are implemented
in a similar manner to R’s vector recycling: if two matrices of diﬀerent dimensions are to be
added together, say one is 3 × 4 and the other is 4 × 1, the latter matrix is expanded to a
3 × 4 matrix with the column repeated four times. Another example is adding a 1 × 4 matrix
and a 4 × 1 matrix. The former is expanded to a 4 × 3 matrix by repeating the row four
times, and the latter to a 4 × 3 matrix by repeating the column three times. Application of
this recycling rule is in accordance with the broadcast semantics of PyTorch.

Distributed matrix multiplication requires some care. Suppose we multiply a p × r matrix
A and an r × q matrix B. If matrix B is tall and split by row into [B[1], . . . , B[T ]]T and
distributed among T processes, where B[t] is the t-th row block of B. If matrix A is split in the
same manner, a natural way to compute the product AB is for each process t to gather (see
Section 2.1) all B[1], . . . , B[T ] to create a copy of B and compute the row block A[t]B of AB.
On the other hand, if matrix A is wide and split by column into [A[1], . . . , A[T ]], where A[t] is
the t-th column block of A, then each process will compute the local multiplication A[t]B[t].
The product AB = (cid:80)T
t=1 A[t]B[t] is computed by a reduce or all-reduce operation of Section
2.1. These operations are parallelized as outlined in Section 3.3. The distribution scenarios
considered in distmat are collected in Table 1. Each matrix can be either broadcast (p × r
for A), row-distributed ([p] × r), or column-distributed (p × [r]). Since broadcasting both
matrices does not require any distributed treatment in multiplication, there remain eight
possible combinations of the input. For each combination, the output may involve more than
one conﬁgurations. If an outer dimension (either p or q but not both) is distributed, the
p × q output AB is distributed along that dimension (scenarios 4, 8, 11). If both dimensions
are split, then there are two possibilities of [p] × q and p × [q] (scenarios 2, 3). Splitting of
the inner dimension r does not aﬀect the distribution of the output unless it is distributed
in both A and B (scenarios 1, 9, 10). Otherwise, we consider all the possible combinations
in the output: broadcast, split by rows, and split by columns (scenarios 5, 6, 7).

The distmat.mm() function implements the 11 scenarios of Table 1 using the PyTorch
function torch.mm() for within-process matrix multiplication and the collective commu-
nication directives (Section 2.1). Scenarios 3, 6, 8, 10, and 11 are implemented using the
transpositions of input and output matrices for scenarios 2, 7, 1, 9, and 4, respectively. Trans-

1
2

3

4

5

6
7
8

9

S. Ko, H. Zhou, J. J. Zhou, and J.-H. Won/High-performance statistical computing

18

Table 1
Eleven distributed matrix multiplication scenarios of distmat.

A

B

AB

Description

[p] × r
[p] × r

[r] × q
r × [q]

[p] × q A wide matrix times a tall matrix.
[p] × q Outer product, may require a large amount

of memory.

Communication involved
(size of output)
1 all-gather (r × q)
1 all-gather (r × q)

[p] × r

r × [q]

p × [q] Outer product, may require a large amount

1 all-gather (r × p)

[p] × r

r × q

[p] × q A distributed matrix times a small, broad-

None

of memory.

p × [r]

[r] × q

p × [r]
p × [r]
p × [r]

[r] × q
[r] × q
r × [q]

p × q

cast matrix.
Inner product, result broadcast. Suited for
inner product between two tall matrices.
Inner product, result distributed.
[p] × q
p × [q]
Inner product, result distributed.
p × [q] Multiply two column-distributed wide ma-

p × [r]

r × q

p × q

10

p × r

[r] × q

p × q

trices
A distributed matrix times a tall broadcast
matrix. Intended for matrix-vector multi-
plications.
A tall broadcast matrix times a distributed
matrix. Intended for matrix-vector multi-
plications.

1 all-reduce (p × q)

T reductions (p × q/T each)
T reductions (q × p/T each)
1 all-gather (p × r)

1 all-reduce (p × q)

1 all-reduce (p × q)

11

p × r

r × [q]

p × [q] A small, broadcast matrix times a dis-

None

tributed matrix

position costs only a short constant time, as it only ‘tags’ to the original matrix that it is
transposed. The data layout remains intact. A scenario is automatically selected depending
on the distribution of the input matrices. The class distmat has an attribute for determin-
ing if the matrix is distributed by row or column. For scenarios 2, 3; 5, 6, and 7, which share
the same input structure, additional keyword parameters are supplied to distinguish them
and determine the shape of the output matrix. The type of collective communication oper-
ation and the involved matrix block sizes roughly determine the communication cost of the
computation. For example, an all-reduce is more expensive than a reduce. The actual cost
depends on the network latency, number of MPI messages sent, and sizes of the messages
sent between processes, which are all system-dependent.

Listing 3 demonstrates an example usage of distmat. We assume that this program is
run with four processes (size in Line 6 is 4). Line 8 determines the device to use. If multiple
GPUs are involved, the code selects one based on the rank of the process. Line 9 selects the
GPU to use with PyTorch. This code runs on a system in which PyTorch is installed with a
CUDA-aware MPI implementation. The number of processes to be used can be supplied by a
command-line argument (see Appendix B). Line 11 selects the data type and the device used
for matrices. The TType (for “tensor type”) of torch.cuda.FloatTensor indicates that
single-precision GPU arrays are used, while DoubleTensor employs double-precision CPU
arrays. Then Line 12 creates a distributed [4] × 4 matrix and initializes it to uniform (0, 1)
random numbers. This matrix is created once and initialized locally, and then distributed to
all processes. (For large matrices, distmat supports another creation mode that assembles
matrix blocks from distributed processes.) Line 14 multiplies the two such matrices A and
B to form a distributed matrix of size [4] × 2. Scenario 1 in Table 1 is chosen by distmat
to create the output AB. Line 18 computes an elementwise logarithm of 1 + AB, in an
elementwise fashion according to the recycling rule. The local block of data residing in each
process can be accessed by appending .chunk to the name of the distributed matrix, as in

S. Ko, H. Zhou, J. J. Zhou, and J.-H. Won/High-performance statistical computing

19

1 import torch
2 from dist_stat import distmat
3 import torch.distributed as dist
4 dist.init_process_group(’mpi’)
5 rank = dist.get_rank()
6 size = dist.get_world_size()

7
8 device = ’cuda:{}’.format(rank) # or ’cpu’ for CPU computing
9 if device.startswith(’cuda’): torch.cuda.set_device(rank)

DoubleTensor

10
11 TType = torch.cuda.FloatTensor if device.startswith(’cuda’) else torch.
# single precision for GPUs
12 A = distmat.distgen_uniform(4, 4, TType=TType) # create [4] x 4 matrix
13 B = distmat.distgen_uniform(4, 2, TType=TType) # create [4] x 2 matrix
14 AB = distmat.mm(A, B) # A * B, Scenario 1.
15 if rank == 0: # to print this only once

print("AB = ")

16
17 print(rank, AB.chunk) # print the rank’s portion of AB.
18 C = (1 + AB).log() # elementwise logarithm
19 if rank == 0:

print("log(1 + AB) = ")

20
21 print(rank, C.chunk) # print the rank’s portion of C.

Listing 3: An example usage of the module distmat.

Lines 17 and 21.4

Although the present implementation only deals with matrices, distmat can be easily
extended to tensor multiplication, as long as the distributed multiplication scenarios are
carefully examined as in Table 1. Creating communication-eﬃcient parallel strategies that
minimize the amount of communication between computing units is an active area of re-
search (Van De Geijn and Watts, 1997; Ballard et al., 2011; Koanantakool et al., 2016).
Communication-avoiding sparse matrix multiplication has been utilized for sparse inverse
covariance estimation (Koanantakool et al., 2018).

6. Examples

In this section, we compare the performance of the optimization algorithms of Section 4
on various HPC environments for the following four statistical computing examples using
distmat: nonnegative matrix factorization (NMF), positron emission tomography (PET),
multi-dimensional scaling (MDS), all of which were considered in Zhou et al. (2010), and (cid:96)1-
regularized Cox proportional hazards regression for survival analysis. For the former three
examples the focus is on scaling up the size of feasible problems from those about a decade
ago. For the last example, we focus on analyzing a real-world geonomic dataset of size
approximately equal to 200, 000 × 500, 000.

4Lines 17 and 21 do not guarantee printing in order (of ranks). They are printed on a ﬁrst come, ﬁrst

served basis.

S. Ko, H. Zhou, J. J. Zhou, and J.-H. Won/High-performance statistical computing

20

Table 2
HPC environments for experiments

local node

CPU

GPU

Model

Intel Xeon E5-2680 v2 Nvidia GTX 1080

# of cores
Clock

# of entities

Total memory
Total cores

10
2.8 GHz

2

256 GB
20

2560
1.6 GHz

8

64 GB
20,480 (CUDA)

AWS c5.18xlarge
CPU
Intel Xeon
Platinum 8124M
18
3.0GHz
2 (per instance)
× 1-20 (instances)
144 GB × 1–20
36 × 1–20

6.1. Setup

We employed a local multi-GPU workstation and a virtual cluster consisted of multiple AWS
EC2 instances for computing. Table 2 shows the setting of our HPC systems used for the
experiments. For virtual cluster experiments, we utilized 1 to 20 of AWS c5.18xlarge
instances with 36 physical cores with AVX-512 (512-bit advanced vector extension to the
x86 instruction set) enabled in each instance through the CfnCluster resource manager.
Network bandwidth of each c5.18xlarge instance was 25GB/s. A separate c5.18xlarge
instance served as the “master” instance, which did not participate in computation by itself
but managed the computing jobs over the 1 to 20 “worker” instances. Data and software for
the experiments were stored in an Amazon Elastic Block Store (EBS) volume attached to
this instance and shared among the worker instances via the network ﬁle system. Further
details are given in Appendix B. For GPU experiments, we used a local machine with two
CPUs (10 cores per CPU) and eight Nvidia GTX 1080 GPUs. These are desktop GPUs, not
optimized for double-precision. All the experiments were conducted using PyTorch version
0.4 built on the Intel Math Kernel Library (MKL); the released code works for the versions
up to 1.6.

We evaluated the objective function once per 100 iterations. For the comparison of ex-
ecution time, the iteration was run for a ﬁxed number of iterations, regardless of con-
vergence. For comparison of diﬀerent algorithms for the same problem, we iterated until
|f (θn)−f (θn−100)|
|f (θn)|+1

< 10−5.

For all the experiments, single-precision computation results on GPU agreed with double-
precision ones up to six signiﬁcant digits, except for (cid:96)1-regularized Cox regression, where
the PyTorch implementation of the necessary cumulative sum operation caused numerical
instability in some cases. Therefore all the experiments for Cox regression were carried out in
double-precision. Extra eﬀorts for writing a multi-device code were modest with distmat.
Given around 1000 lines of code to implement basic operations for multi-device conﬁguration
in distmat, additional code for our four examples was less than 30 lines for each.

6.2. Scaling up examples in Zhou et al. (2010)

Nonnegative matrix factorization NMF is a procedure that approximates a nonnega-
tive data matrix X ∈ Rm×p by a product of two low-rank nonnegative matrices, V ∈ Rm×r
and W ∈ Rr×p. In a simple setting, NMF minimizes f (V, W ) = (cid:107)X − V W (cid:107)2
F, where (cid:107) · (cid:107)F
denotes the Frobenius norm. Applying the MM principle to recover the famous multiplica-
tive algorithm due to Lee and Seung (1999, 2001) is discussed in Zhou et al. (2010, Sect.

S. Ko, H. Zhou, J. J. Zhou, and J.-H. Won/High-performance statistical computing

21

3.1). Alternatively, the alternating projected gradient (APG) method (Lin, 2007) introduces
ridge penalties to minimize f (V, W ; (cid:15)) = (cid:107)X − V W (cid:107)2
F. Then the APG
iteration is given by

2 (cid:107)W (cid:107)2

2 (cid:107)V (cid:107)2

F + (cid:15)

F + (cid:15)

V n+1 = P+
W n+1 = P+

(cid:0)(1 − σn(cid:15))V n − σn(V nW n(W n)T − X(W n)T )(cid:1)
(cid:0)(1 − τn(cid:15))W n − τn((V n+1)T V n+1W n − (V n+1)T X)(cid:1) ,

where P+ denotes the projection onto the nonnegative orthant; σn and τn are step sizes.
Convergence is guaranteed if (cid:15) > 0, σn ≤ 1/(2(cid:107)W n(W n)T +(cid:15)I(cid:107)2
F), and τn ≤ 1/(2(cid:107)(V n)T V n+
(cid:15)I(cid:107)2
F). APG has an additional advantage of avoiding creation of subnormal numbers over
the multiplicative algorithm (see Appendix D). Table 3 compares the performance of APG
between single-machine multi-GPU and multi-instance virtual cluster settings. Synthetic
datasets of sizes [10,000] × 10,000 and [200,000] × 200,000 were created and distributed.
For reference, the dimension used in Zhou et al. (2010) is 2429 × 361. Multi-GPU setting
achieved up to 4.14x-speedup over a single CPU instance if the dataset was small, but could
not run the larger dataset. The cluster in a cloud was scalable with data, running faster
with more instances, yielding up to 4.10x-speedup over the two-instance cluster.

i=1[yi log((cid:80)p

Positron emission tomography PET reconstruction is essentially a deconvolution prob-
lem of estimating the intensities of radioactive biomarkers from their line integrals, which
can be posed as maximizing the Poisson loglikelihood L(λ) = (cid:80)d
j=1 eijλj) −
(cid:80)p
j=1 eijλj]. Here yi is the observed count of photons arrived coincidentally at detector pair
i. Emission intensities λ = (λ1, · · · , λp) are to be estimated, and eij is the probability that
detector pair i detects an emission form pixel location j, which dependes on the geometry
of the detector conﬁguration. We consider a circular geometry for two-dimensional imaging.
Adding a ridge-type penalty of −(µ/2)(cid:107)Dλ(cid:107)2
2 to enhance spatial contrast and solving the
resulting optimization problem by an MM algorithm is considered in Zhou et al. (2010, Sect.
3.2). Here D is the ﬁnite diﬀerence matrix on the pixel grid. To promote sharper contrast,
we employ the anisotropy total variation (TV) penalty (Rudin et al., 1992) and minimize
−L(λ) + ρ(cid:107)Dλ(cid:107)1. Write E = (eij). Then the PDHG algorithm (Sect. 4.3) can be applied.
Put K = [ET , DT ]T , f (z, w) = (cid:80)
i(−yi log zi) + ρ(cid:107)w(cid:107)1, and g(λ) = 1T Eλ + δ+(λ), where
1 is the all-one vector and δ+ is the 0/∞ indicator function for the nonnegative orthant.
Since f (z, w) is separable in z and w, applying iteration (6) using the proximity operator
(3), we obtain the following iteration:
(cid:104)(cid:0)zn

(cid:1) − (cid:2)(cid:0)zn

i = 1, . . . , d

(cid:3)1/2(cid:105)

(cid:1)2

,

+ 4σyi

= 1
2

i + σ(E¯λn)i

i + σ(E¯λn)i

zn+1
i
wn+1 = P[−ρ,ρ](wn + σD¯λn)
λn+1 = P+(λn − τ (ET zn+1 + DT wn+1 + ET 1))
¯λn+1 = 2λn+1 − λn,

where P[−ρ,ρ] is elementwise projection to the interval [−ρ, ρ]. Convergence is guaranteed if
στ < 1/(cid:107)[ET DT ](cid:107)2
2. Scalability experiments were carried out with large Roland-Varadhan-
Frangakis phantoms (Roland et al., 2007) using grid sizes p = 300 × 300, 400 × 400, and
900×900, with number of detector pairs d = 179, 700. Timing per 1000 iterations is reported
in Table 4. Both matrices E and D were distributed along the columns. For reference, Zhou
et al. (2010) use a 64 × 64 grid with d = 2016. The total elapsed time decreases with more
GPUs or nodes. The multi-GPU node could not run the p = 810, 000 dataset, however, since

S. Ko, H. Zhou, J. J. Zhou, and J.-H. Won/High-performance statistical computing

22

the data size was too big to ﬁt in the GPU memory. Figure 2 illustrates TV reconstructions
of a p = 128 × 128 extended cardiac-torso (XCAT) phantom with d = 8128 (Lim et al.,
2018; Ryu et al., 2020). Results by a stochastic version of PDHG (Chambolle et al., 2018)
are also provided. Each reconstruction was run for 20,000 iterations, which were suﬃcient
for both algorithms to reach similar objective values. Those iterations took 20 to 35 seconds
on a single GPU.

Multi-dimensional scaling The version of MDS considered in Zhou et al. (2010, Sect.
3.3) minimizes the stress function f (θ) = (cid:80)q
j(cid:54)=i wij(yij − (cid:107)θi − θj(cid:107)2)2 to map dissimi-
i=1
larity measures yij between data point pairs (i, j) to points θ = (θ1, . . . , θq)T in an Euclidean
space of low dimension p, where the wij are the weights. Zhou et al. (2010) derive a parallel
MM iteration

(cid:80)

θn+1
ik =

(cid:16)(cid:80)

j(cid:54)=i

(cid:104)

yij
i −θn

j (cid:107)2

(cid:107)θn

(θn

ik − θn

jk) + (θn

ik + θn

jk)

(cid:105)(cid:17) (cid:14) (cid:16)

2 (cid:80)m

j(cid:54)=i wij

(cid:17)

for i = 1, . . . , q and k = 1, . . . , p. We generated a [10,000] × 10,000 and a [100,000] × 100,000
pairwise dissimilarity matrices from samples of the 1,000-dimensional standard normal dis-
tribution. For reference, the dimension of the dissimilarity matrix used in Zhou et al. (2010)
is 401×401. Elapsed time is reported in Table 5. For p = 20, the eight-GPU setting achieved
a 5.32x-speedup compared to the single 36-core CPU AWS instance and a 6.13x-speedup
compared to single GPU. The larger experiment involved storing a distance matrix of size
[100,000] × 100,000, which took 74.5 GB of memory. The multi-GPU node did not scale
to run this experiment due to the memory limit. On the other hand, we observed a 3.78x-
speedup with 20 instances (720 cores) with respect to four instances (144 cores) of CPU
nodes.

Appendix D contains further details on the experiments of this subsection.

6.3. (cid:96)1-regularized Cox proportional hazards regression

We apply the proximal gradient method to (cid:96)1-regularized Cox proportional hazards regres-
sion (Cox, 1972). In this problem, we are given a covariate matrix X ∈ Rm×p, time-to-event
(t1, . . . , tm), and right-censoring time (c1, . . . , cm) for individual i = 1, . . . , m as data. The
“response” is deﬁned by yi = min{ti, ci} for each indivuali i, and whether this individual is
censored is indicated by δi = I{ti≤ci}. The log partial likelihood of the Cox model is then

L(β) =

(cid:104)

δi

m
(cid:88)

i=1

βT xi − log (cid:0) (cid:80)

j:yj ≥yi

exp(βT xj)(cid:1)(cid:105)

.

Coordinate-descent-type approaches to this model are proposed by Suchard et al. (2013)
and Mittal et al. (2014).

To obtain a proximal gradient iteration, we need the gradient ∇L(β) and its Lipschitz

constant. The gradient of the log partial likelihood is

∇L(β) = X T (I − P )δ,

δ = (δ1, . . . , δm)T ,

where we deﬁne the matrix P = (πij) with πij = I(yi ≥ yj)wi/Wj; wi = exp(xT
Wj = (cid:80)
i:yi≥yj
of the Hessian of L(β):

i β),
wi. A Lipschitz constant of ∇L(β) can be found by ﬁnding an upper bound

∇2L(β) = X T (P diag(δ)P T − diag(P δ))X.

S. Ko, H. Zhou, J. J. Zhou, and J.-H. Won/High-performance statistical computing

23

Table 3
Runtime (in seconds) of NMF on simulated data for diﬀerent inner dimensions r. “×” denotes that the
experiment could not run with a single data load to the device.

conﬁguration

10,000 × 10,000
10,000 iterations

200,000 × 200,000
1000 iterations

r = 20

r = 40

r = 60

r = 20

r = 40

r = 60

GPUs

1
2
4
8

164
97
66
57

168
106
78
77

AWS EC2 c5.18xlarge instances

4
5
8
10
20

205
230
328
420
391

310
340
390
559
1094

174
113
90
92

430
481
536
643
1293

×
×
×
×

1493
1326
937
737
693

×
×
×
×

1908
1652
1044
937
818

×
×
×
×

2232
2070
1587
1179
1041

Table 4
Runtime (in seconds) comparison of 1000 iterations of TV-penalized PET. We exploited sparse structures
of E and D. The number of detector pairs d was ﬁxed at 179,700.

conﬁguration

p = 90, 000

p = 160, 000

p = 810, 000

GPUs

1
2
4
8

×
21
19
18

AWS EC2 c5.18xlarge instances

4
5
8
10
20

36
36
33
38
26

×
35
31
28

49
45
39
37
28

×
×
×
×

210
188
178
153
131

Table 5
Runtimes (in seconds) of 1000 iterations for MDS for diﬀerent mapped dimensions q.

conﬁguration

10,000 datapoints
10,000 iterations

100,000 datapoints
1000 iterations

q = 20

q = 40

q = 60

q = 20

q = 40

q = 60

GPUs

1
2
4
8

368
185
100
60

376
190
103
67

AWS EC2 c5.18xlarge instances

4
5
8
10
20

424
364
350
275
319

568
406
425
414
440

384
195
108
73

596
547
520
457
511

×
×
×
×

3103
2634
1580
1490
820

×
×
×
×

3470
2700
1794
1454
958

×
×
×
×

3296
2730
1834
1558
1043

S. Ko, H. Zhou, J. J. Zhou, and J.-H. Won/High-performance statistical computing

24

(a) PDHG, ρ = 0

(b) ρ = 10−3

(c) ρ = 10−2.5

(d) ρ = 10−2

(e) SPDHG, ρ = 0

(f) ρ = 10−3

(g) ρ = 10−2.5

(h) ρ = 10−2

Fig 2: Reconstruction of the XCAT phantom with a TV penalty with regularization param-
eter ρ, using deterministic (top row) and stochastic (bottom) PDHG.

Note (cid:107)P (cid:107)2 ≤ 1, since the sum of each row of P is 1. It follows that (cid:107)∇2L(β)(cid:107)2 ≤ 2(cid:107)X(cid:107)2
2,
and (cid:107)X(cid:107)2 can be quickly computed by using the power iteration (Golub and Van Loan,
2013).

We introduce an (cid:96)1-penalty to the log partial likelihood in order to enforce sparsity in
the regression coeﬃcients and use the proximal gradient descent to estimate β by putting
g(β) = −L(β), f (β) = λ(cid:107)β(cid:107)1. Then the iteration is:

= (cid:80)

i β); W n+1

wn+1
= exp(xT
i
j
/W n+1
ij = I{ti≥tj }wn+1
πn+1
j
∆n+1 = X T (I − P n+1)δ, where P n+1 = (πn+1
βn+1 = Sλ(βn + σ∆n+1).

wn+1
i

i:yi≥yj

ij

i

)

If the data are sorted in descending order of yi, the W n

j can be computed by cumulative
summing (w1, . . . , wm) in the proper order. A CUDA kernel for this operation is readily
available in PyTorch. The soft-thresholding operator Sλ(x) is also implemented in PyTorch.
We can write a simple proximal gradient descent routine for the Cox regression as in Listing
4, assuming no ties in yi’s.

A synthetic data matrix X ∈ Rm×[p], distributed along the columns, was sampled from
the standard normal distribution. The algorithm was designed to keep a copy of the estimand
β in every device. All the numerical experiments were carried out with double precision even
for GPUs, for the following reason. For a very small value of λ (we used λ = 10−5), when
single precision was used in GPUs, the estimate quickly tended to “not a number (NaN)”s
due to numerical instability of the CUDA kernel. Double-precision did not generate such a
problem. Although desktop GPU models such as Nvidia GTX and Titan X are not optimized

S. Ko, H. Zhou, J. J. Zhou, and J.-H. Won/High-performance statistical computing

25

1 import torch
2 import torch.distributed as dist
3 from dist_stat import distmat
4 from dist_stat.distmat import distgen_uniform, distgen_normal
5 dist.init_process_group(’mpi’)
6 rank = dist.get_rank()
7 size = dist.get_world_size()
8 device = ’cuda:{}’.format(rank) # ’cpu’ for CPU computing
9 if device.startswith(’cuda’): torch.cuda.set_device(rank)
10 n = 10000
11 p = 10000
12 max_iter = 10000
13 TType = torch.cuda.FloatTensor if device.startswith(’cuda’) else torch.DoubleTensor
14 X = distgen_normal(p, n, TType=TType).t() # [p] x n transposed => n x [p].
15 delta = torch.multinomial(torch.tensor([1., 1.]), n, replacement=True).float().view(-1, 1).

type(TType) # censoring indicator, n x 1, Bernoulli(0.5).

16 delta_dist = distmat.dist_data(delta, TType=TType) # distribute delta to create [n] x 1 data
17 beta = distmat.dist_data(torch.zeros((p, 1)).type(TType), TType=TType) #[p] x 1
18 Xt = X.t() # transpose. [p] x n
19 sigma = 0.00001 # step size
20 lambd = 0.00001 # penalty parameter
21 soft_threshold = torch.nn.Softshrink(lambd) # soft-thresholding
22
23 # Data points are assumed to be sorted in decreasing order of observed time.
24 y_local = torch.arange(n, 0, step=-1).view(-1, 1).type(TType) # local n x 1
25 y_dist = distmat.dist_data(y_local, TType=TType) # distributed [n] x 1
26 pi_ind = (y_dist - y_local.t() >= 0).type(TType)
27
28 Xbeta = distmat.mm(X, beta) # Scenario 5 (default for n x [p] times [p] x 1)
29
30 for i in range(max_iter):
31

w = Xbeta.exp() # n x 1
W = w.cumsum(0) # n x 1
dist.barrier() # wait until the distributed computation above is finished

w_dist = distmat.dist_data(w, TType=TType) # distribute w. [n] x 1.

pi = (w_dist / W.t()) * pi_ind # [n] x n.
pd = distmat.mm(pi, delta) # Scenario 4.
dmpd = delta_dist - pd # [n] x 1.
grad = distmat.mm(Xt, dmpd) # Scenario 1.
beta = (beta + grad * sigma).apply(soft_threshold) # [p] x 1
Xbeta = distmat.mm(X, beta) # Scenario 5.

expXbeta = (Xbeta).exp() # n x 1
obj = distmat.mm(delta.t(), (Xbeta - (expXbeta.cumsum(0)).log())) \

- lambd * beta.abs().sum() # mm: local computation.

print(i, obj.item())

Listing 4: PyTorch code for the proximal gradient method for (cid:96)1-regularized Cox regression.

for double precision ﬂoating-point operations and is known to be 32 times slower for double
precision operations than single precision operations, this does not necessarily mean that
the total computation time is 32 times slower, since latency takes a signiﬁcant portion of
the total computation time in GPU computing.

In order to demonstrate the scalability of our approach, elapsed times for 10,000 × [10,000]
and 100,000 × [200,000] simulated data are reported in Table 6. We can see 3.92x speedup
from 4 nodes to 20 nodes in the virtual cluster. Even with double-precision arithmetics, eight
GPUs could achieve a 6.30x-speedup over the single 36-core CPU instance. As expected,
virtual clusters in a cloud exhibited better scalability.

32

33

34

35

36

37

38

39

40

41

42

43

44

45

46

47

S. Ko, H. Zhou, J. J. Zhou, and J.-H. Won/High-performance statistical computing

26

Table 6
Runtime comparison of (cid:96)1-regularized Cox regression over multi-node virtual cluster on AWS EC2.
Elapsed time (in seconds) after 1000 iterations.

conﬁguration

GPUs

10, 000 × [10, 000]
10,000 iterations

100, 000 × [200, 000]
1,000 iterations

1
2
4
8

386
204
123
92
AWS EC2 c5.18xlarge instances
580
309
217
170
145
132
148

1
2
4
5
8
10
20

×
×
×
×

×
×
1507
1535
775
617
384

6.4. Genome-wide survival analysis of the UK Biobank dataset

We demonstrate a real-world application of (cid:96)1-regularized Cox proportional hazards regres-
sion to genome-wide survival analysis for Type 2 Diabetes (T2D). We used a UK Biobank
dataset (Sudlow et al., 2015) that contains information on approximately 800,000 single
nucleotide polymorphisms (SNPs) of 500,000 individual subjects recruited from the United
Kingdom. After ﬁltering SNPs for quality control and subjects for the exclusion of Type
1 Diabetes patients, 402,297 subjects including 17,994 T2D patients and 470,189 SNPs
remained. We randomly sampled 200,000 subjects including 8,995 T2D patients for our
analysis. Any missing genotype was imputed with the column mean. Along with the SNPs,
sex and top ten principal components were included as unpenalized covariates to adjust for
population-speciﬁc variations. The resulting dataset was 701 GB with double-precision.

The analysis for this large-scale genome-wide dataset was conducted as follows. Incidence
of T2D was used as the event (δi = 1) and the age of onset was used as survival time yi. For
non-T2D subjects (δi = 0), age at the last visit was used as yi. We chose 63 diﬀerent values
of the regularization parameter λ in the range [0.7 × 10−9, 1.6 × 10−8], with which 0 to 111
SNPs were selected. For each value of λ, the (cid:96)1-regularized Cox regression model of Section
6.3 was ﬁtted. Every run converged after at most 2080 iterations that took less than 2800
seconds using 20 c5.18xlarge instances from AWS EC2.

The SNPs were ranked based on the largest value of λ with which a SNP is selected. (No
variables were removed once selected within the range of λ used. The regularization path
and the full list of the selected SNPs are available in Appendix E.) Among the 111 SNPs
selected, three of the top four selections were located on TCF7L2, whose association with
T2D is well-known (Scott et al., 2007; The Wellcome Trust Case Control Consortium, 2007).
Also prominently selected were SNPs from genes SLC45A2 and HERC2, whose variants are
known to be associated with skin, eye, and hair pigmentation (Cook et al., 2009). This is
possibly due to the dominantly European population in the UK Biobank study. Mapped
genes for 24 SNPs out of the selected 111 were also reported in Mahajan et al. (2018), a
meta-analysis of 32 genome-wide association studies (GWAS) for about 898,130 individuals
of European ancestry; see Tables E.1 and E.2 for details. We then conducted an unpenalized
Cox regression analysis using the 111 selected SNPs. The nine SNPs with p-values less than
0.01 are listed in Table 7. The locations in Table 7 are with respect to the reference genome

S. Ko, H. Zhou, J. J. Zhou, and J.-H. Won/High-performance statistical computing

27

Table 7
SNPs with p-values of less than 0.01 on unpenalized Cox regression with variables selected by
(cid:96)1-penalized Cox regression

Chr.
10
10
15
12
10
8
19
11
6

SNP ID
p-value
rs4506565
<2e-16
rs12243326
0.003467
rs8042680
0.005052
rs343092
0.000400
rs7899137
0.002166
rs8180897
0.000149
rs10416717
0.009474
rs231354
0.001604
2.11e-5
rs9268644
A Minor allele, B Major allele, C Minor allele frequency. The boldface indicates the risk allele
determined by the reference allele and the sign of the regression coeﬃcient.

Location A1A A2B MAFC Mapped Gene Coeﬃcient
2.810e-1
1.963e-1
2.667e-1
−7.204e-2
−4.776e-2
6.361e-2
5.965e-2
4.861e-2
6.589e-2

T
T
C
G
C
G 0.445
G 0.470 CACNA1A
T
A

114756041 A
114788815 C
91521337 A
66250940 T
76668462 A
121699907 A
13521528 A
2706351 C
32408044 C

0.238 TCF7L2
0.249 TCF7L2
0.277
0.463 HMGA2
0.289 KAT6B
SNTB1

0.329 KCNQ1
0.282 HLA-DRA

PRC1

GRCh37 (Church et al., 2011), and mapped genes were predicted by the Ensembl Variant
Eﬀect Predictor (McLaren et al., 2016). Among these nine SNPs, three of them were directly
shown to be associated with T2D (The Wellcome Trust Case Control Consortium (2007)
and Dupuis et al. (2010) for rs4506565, Voight et al. (2010) for rs8042680, Ng et al. (2014)
for rs343092). Three other SNPs have mapped genes reported to be associated with T2D
in Mahajan et al. (2018): rs12243326 on TCF7L2, rs343092 on HMGA2, and rs231354 on
KCNQ1.

Although the interpretation of the results requires additional sub-analysis, the result
shows the promise of joint association analysis using multiple regression models. In GWAS
it is customary to analyze the data on SNP-by-SNP basis. Among the mapped genes har-
boring the 111 SNPs selected by our half-million-variate regression analysis are CPLX3 and
CACNA1A, associated with regulation of insulin secretion, and SEMA7A and HLA-DRA in-
volved with inﬂammatory responses (based on DAVID (Huang et al., 2009a,b)). These genes
might have been missed in conventional univariate analysis of T2D due to nominally mod-
erate statistical signiﬁcance. Joint GWAS may overcome such a limitation and is possible
by combining the computing power of modern HPC and scalable algorithms.

7. Discussion

Abstractions of highly complex computing operations have rapidly evolved over the last
decade. In this article, we have explained how statisticians can beneﬁt from this evolution.
We have seen how deep learning technology is relevant to high-performance statistical com-
puting. We have also demonstrated that many useful tools for incorporating accelerators and
computing clusters have been created. Unfortunately, such developments have been mainly
made in languages other than R, particularly in Python, with which statisticians may not
be familiar with. Although there are libraries that deal with simple parallel computation in
R, there are common issues with these libraries. First, the libraries do not easily incorporate
GPUs that might signiﬁcantly speed up computation. Second, it is hard to write more full-
ﬂedged parallel programs without directly writing code in C or C++. This two-language
problem calls for statisticians to take a second look at Python. Fortunately, this language
is not hard to learn, and younger generations are quite familiar with it. A remedy from the
R side may be either developing more user-friendly interfaces for the distributed-memory
environment, with help from those who are engaged in computer engineering, or writing

S. Ko, H. Zhou, J. J. Zhou, and J.-H. Won/High-performance statistical computing

28

a good wrapper for the important Python libraries. A Python interface to R may be a
good starting point. For example, R package reticulate (Ushey et al., 2021) is a basis of
other interfaces packages to PyTorch (rTorch, Reyes, 2021) and TensorFlow (also called
tensorflow, RStudio, 2021).

By making use of multiple CPU nodes or a multi-GPU workstation, the methods discussed
in the current article can be applied eﬃciently even when the dataset exceeds several tens of
gigabytes. The advantages of engaging multiple compute devices are two-fold. First, we can
take advantage of data parallelism with more computing cores, accelerating the computation.
Second, we can push the limit of the size of the dataset to analyze. As cloud providers now
support virtual clusters better suited for HPC, statisticians can deal with bigger problems
utilizing such services, using up to several thousand cores easily. When the data do not
ﬁt into the GPU memory (e.g., the UK Biobank example), it is still possible to carry
out computation by moving partitions of the data in and out of GPUs. However, this is
impractical because of slow communication between the main and GPU memories. On the
other hand, virtual clusters are scalable with this size of data.

Loss of accuracy due to the default single precision of GPU arithmetic, prominent in our
proportional hazards regression example, can be solved by purchasing scientiﬁcally-oriented
GPUs with better double precision supports. Another option is migrating to the cloud: for
example, the P2 and P3 instances in AWS support scientiﬁc GPUs. Nevertheless, desktop
GPUs with double precision arithmetic turned on could achieve more than 10-fold speedup
over CPU, even though double precision ﬂoating-point operations are 32 times slower than
single precision.

Most of the highly parallelizable algorithms considered in Section 4 require no more than
the ﬁrst-order derivative information, and this feature contributes to their low per-iteration
complexity and parallelizability. As mentioned in Section 1, some second-order methods
for sparse regression (Li et al., 2018; Huang et al., 2018, 2021) maintain the set of active
variables (of nonzero coeﬃcients), and only these are involved in the Newton-Raphson step.
Thus if the solution is sparse, the cost of solving the relevant linear system is moderate. With
distributed matrix computation exempliﬁed with distmat, residual and gradients can be
computed in a distributed fashion and the linear system can be solved after gathering active
variables into the master device.

A major weakness of the present approach is that its eﬀectiveness can be degraded by
the communication cost between the nodes and devices. One way to avoid this issue is by
using high-speed interconnection between the nodes and devices. In multi-CPU clusters,
this can be realized by a high-speed interconnection technology such as InﬁniBand. Even
when such an environment is not aﬀordable, we may still use relatively high-speed connec-
tion equipped with instances from a cloud. The network bandwidth of 25Gbps supported
for c5.18xlarge instances of AWS was quite eﬀective in our experiments. Reducing the
number of communication rounds and iterations with theoretical guarantees, for example,
by one-shot averaging (Zhang et al., 2013; Duchi et al., 2014; Lee et al., 2015), by using
global ﬁrst-order information and local higher-order information (Wang et al., 2017; Jordan
et al., 2019; Fan et al., 2019), or by quantization (Tang et al., 2019; Liu et al., 2020), is an
active area of current research.

Although PyTorch has been advocated throughout this article, it is not the only path
towards easy-to-use programming models in shared- and distributed-memory programming
environments. A possible alternative is Julia (Bezanson et al., 2017), in which data can reside
in a wide variety of environments, such as GPUs (Besard et al., 2019) and multiple CPU
nodes implementing the distributed memory model (JuliaParallel Team, 2021; Janssens,

S. Ko, H. Zhou, J. J. Zhou, and J.-H. Won/High-performance statistical computing

29

2021). While its long-term support release of version 1.0.5 in September 2019 is still fresh,
Julia has the potential to be a powerful tool for statistical HPC once the platforms and user
community mature.

Acknowledgements

by

funded

the Korea

This research was partially funded by the National Research Foundation of Korea (NRF)
grant
JHW;
2020R1A6A3A03037675, SK), the Collaboratory Fellowship program of the UCLA Institute
for Quantitative & Computational Bioscience (SK), AWS Cloud Credit for Research (SK and
JHW), and grants from National Institutes of Health (R35GM141798, HZ; R01HG006139,
HZ and JJZ; K01DK106116, JJZ; R21HL150374, JJZ) and National Science Foundation
(DMS-2054253, HZ and JJZ).

(2019R1A2C1007126,

government

(MSIT)

References

Abadi, M., Agarwal, A., Barham, P., Brevdo, E., Chen, Z., Citro, C., Corrado, G. S., Davis,
A., Dean, J., Devin, M., Ghemawat, S., Goodfellow, I., Harp, A., Irving, G., Isard, M., Jia,
Y., Jozefowicz, R., Kaiser, L., Kudlur, M., Levenberg, J., Man´e, D., Monga, R., Moore,
S., Murray, D., Olah, C., Schuster, M., Shlens, J., Steiner, B., Sutskever, I., Talwar, K.,
Tucker, P., Vanhoucke, V., Vasudevan, V., Vi´egas, F., Vinyals, O., Warden, P., Watten-
berg, M., Wicke, M., Yu, Y. and Zheng, X. (2016), ‘TensorFlow: Large-scale machine
learning on heterogeneous systems’, arXiv preprint arXiv:1603.04467 . Software available
from https://tensorflow.org.

Amazon Web Services (2021), ‘AWS ParallelCluster’, https://aws.amazon.com/ko/

hpc/parallelcluster/. Version 2.11.0. Accessed: 2021-07-03.

Apache Software Foundation (2021), ‘Apache Hadoop’, https://hadoop.apache.org.

Version 3.3.1. Accessed: 2021-07-03.

Armbrust, M., Fox, A., Griﬃth, R., Joseph, A. D., Katz, R., Konwinski, A., Lee, G., Pat-
terson, D., Rabkin, A., Stoica, I. and Zaharia, M. (2010), ‘A view of cloud computing’,
Commun. ACM 53(4), 50–58.

Atchad´e, Y. F., Fort, G. and Moulines, E. (2017), ‘On perturbed proximal gradient algo-

rithms’, J. Mach. Learn. Res. 18(1), 310–342.

Bahrampour, S., Ramakrishnan, N., Schott, L. and Shah, M. (2016), ‘Comparative study of

deep learning software frameworks’, arXiv preprint arXiv:1511.06435 .

Ballard, G., Demmel, J., Holtz, O. and Schwartz, O. (2011), ‘Minimizing communication in

numerical linear algebra’, SIAM J. Matrix Anal. Appl. 32(3), 866–901.

Bauer, B., Kohler, M. et al. (2019), ‘On deep learning as a remedy for the curse of dimen-

sionality in nonparametric regression’, Ann. Statist. 47(4), 2261–2285.

Baydin, A. G., Pearlmutter, B. A., Radul, A. A. and Siskind, J. M. (2017), ‘Automatic
diﬀerentiation in machine learning: a survey’, J. Mach. Learn. Res. 18(1), 5595–5637.

Beck, A. (2017), First-order methods in optimization, SIAM.
Beck, A. and Teboulle, M. (2009), ‘A fast iterative shrinkage-thresholding algorithm for

linear inverse problems’, SIAM J. Imaging Sci. 2(1), 183–202.

Besard, T., Foket, C. and De Sutter, B. (2019), ‘Eﬀective extensible programming: Unleash-

ing Julia on GPUs’, IEEE Trans. Parallel Distrib. Syst. 30(4), 827–841.

Bezanson, J., Edelman, A., Karpinski, S. and Shah, V. B. (2017), ‘Julia: A fresh approach

to numerical computing’, SIAM Review 59(1), 65–98.

S. Ko, H. Zhou, J. J. Zhou, and J.-H. Won/High-performance statistical computing

30

Blackford, L. S., Petitet, A., Pozo, R., Remington, K., Whaley, R. C., Demmel, J., Dongarra,
J., Duﬀ, I., Hammarling, S., Henry, G. et al. (2002), ‘An updated set of basic linear algebra
subprograms (BLAS)’, ACM Trans. Math. Software 28(2), 135–151.

Boyd, S., Parikh, N., Chu, E., Peleato, B. and Eckstein, J. (2011), ‘Distributed optimization
and statistical learning via the alternating direction method of multipliers’, Found. Trends
Mach. Learn. 3(1), 1–122.

Buckner, J., Wilson, J., Seligman, M., Athey, B., Watson, S. and Meng, F. (2009), ‘The

gputools package enables GPU computing in R’, Bioinformatics 26(1), 134–135.

Chambolle, A., Ehrhardt, M. J., Richt´arik, P. and Schonlieb, C.-B. (2018), ‘Stochastic
primal-dual hybrid gradient algorithm with arbitrary sampling and imaging applications’,
SIAM J. Optim. 28(4), 2783–2808.

Chambolle, A. and Pock, T. (2011), ‘A ﬁrst-order primal-dual algorithm for convex problems

with applications to imaging’, J. Math. Imaging Vision 40(1), 120–145.

Chen, T., Li, M., Li, Y., Lin, M., Wang, N., Wang, M., Xiao, T., Xu, B., Zhang, C. and
Zhang, Z. (2015), ‘MXNet: A ﬂexible and eﬃcient machine learning library for heteroge-
neous distributed systems’, arXiv preprint arXiv:1512.01274 .

Chen, Y., Lan, G. and Ouyang, Y. (2014), ‘Optimal primal-dual methods for a class of

saddle point problems’, SIAM J. Optim. 24(4), 1779–1814.

Chi, E. C., Zhou, H. and Lange, K. (2014), ‘Distance majorization and its applications’,

Math. Program. 146(1-2), 409–436.

Chu, D., Zhang, C., Sun, S. and Tao, Q. (2020), Semismooth Newton algorithm for eﬃcient
projections onto (cid:96)1,∞-norm ball, in ‘ICML 2020’, Vol. 119 of Proc. Mach. Learn. Res.,
pp. 1974–1983.

Church, D. M., Schneider, V. A., Graves, T., Auger, K., Cunningham, F., Bouk, N., Chen,
H.-C., Agarwala, R., McLaren, W. M., Ritchie, G. R. et al. (2011), ‘Modernizing reference
genome assemblies’, PLoS Biol. 9(7), e1001091.

Collobert, R., Kavukcuoglu, K. and Farabet, C. (2011), Torch7: A Matlab-like environment

for machine learning, in ‘BigLearn, NeurIPS Workshop’.

Combettes, P. L. (2018), ‘Monotone operator theory in convex optimization’, Math. Pro-

gram. 170, 177–206.

Combettes, P. L. and Pesquet, J.-C. (2011), Proximal splitting methods in signal processing,
in ‘Fixed-point algorithms for inverse problems in science and engineering’, Springer,
pp. 185–212.

Condat, L. (2013), ‘A primal-dual splitting method for convex optimization involving Lip-
schitzian, proximable and linear composite terms’, J. Optim. Theory Appl. 158(2), 460–
479.

Cook, A. L., Chen, W., Thurber, A. E., Smit, D. J., Smith, A. G., Bladen, T. G.,
Brown, D. L., Duﬀy, D. L., Pastorino, L., Bianchi-Scarra, G. et al. (2009), ‘Analysis
of cultured human melanocytes based on polymorphisms within the SLC45A2/MATP,
SLC24A5/NCKX5, and OCA2/P loci’, J. Invest. Dermatol. 129(2), 392–405.

Cox, D. R. (1972), ‘Regression models and life-tables’, J. R. Stat. Soc. Ser. B. Stat. Methodol.

34(2), 187–202.

Dean, J. and Ghemawat, S. (2008), ‘MapReduce: simpliﬁed data processing on large clus-

ters’, Commun. ACM 51(1), 107–113.

Defazio, A., Bach, F. and Lacoste-Julien, S. (2014), SAGA: A fast incremental gradient
method with support for non-strongly convex composite objectives, in ‘NeurIPS 2014’,
Vol. 27 of Adv. Neural Inform. Process. Syst., pp. 1646–1654.

Dempster, A. P., Laird, N. M. and Rubin, D. B. (1977), ‘Maximum likelihood from incom-

S. Ko, H. Zhou, J. J. Zhou, and J.-H. Won/High-performance statistical computing

31

plete data via the EM algorithm’, J. R. Stat. Soc. Ser. B. Stat. Methodol. pp. 1–38.
Donoho, D. (2017), ‘50 years of data science’, J. Comput. Graph. Statist. 26(4), 745–766.
Duchi, J. C., Jordan, M. I., Wainwright, M. J. and Zhang, Y. (2014), ‘Optimality guarantees

for distributed statistical estimation’, arXiv preprint arXiv:1405.0782 .

Dupuis, J., Langenberg, C., Prokopenko, I., Saxena, R., Soranzo, N., Jackson, A. U.,
Wheeler, E., Glazer, N. L., Bouatia-Naji, N., Gloyn, A. L. et al. (2010), ‘New genetic
loci implicated in fasting glucose homeostasis and their impact on type 2 diabetes risk’,
Nat. Genet. 42(2), 105–116.

Eddelbuettel, D.
computing

(2021),
with

‘CRAN Task View: High-performance

paral-
https://cran.r-project.org/web/views/

and

lel
HighPerformanceComputing.html. Version 2021-05-27.

R’,

Eijkhout, V. (2016), Introduction to High Performance Scientiﬁc Computing, 2nd edn,

Lulu.com.

Esser, E., Zhang, X. and Chan, T. F. (2010), ‘A general framework for a class of ﬁrst order
primal-dual algorithms for convex optimization in imaging science’, SIAM J. Imaging Sci.
3(4), 1015–1046.

Evangelinos, C. and Hill, C. N. (2008), Cloud computing for parallel scientiﬁc HPC applica-
tions: Feasibility of running coupled atmosphere-ocean climate models on Amazon’s EC2,
in ‘CCA 2008’, ACM.

Facebook Incubator (2021), ‘Gloo: Collective communications library with various prim-
itives for multi-machine training’, https://github.com/facebookincubator/
gloo. Accessed: 2021-07-03.

Fan, J., Guo, Y. and Wang, K. (2019), ‘Communication-eﬃcient accurate statistical estima-

tion’, arXiv preprint arXiv:1906.04870 .

Fox, A. (2011),

‘Cloud computing—What’s in it for me as a scientist?’, Science

331(6016), 406–407.

Friedman, J., Hastie, T. and Tibshirani, R. (2010), ‘Regularization paths for generalized

linear models via coordinate descent’, J. Stat. Softw. 33(1), 1–22.

Gabay, D. and Mercier, B. (1976), ‘A dual algorithm for the solution of nonlinear variational

problems via ﬁnite element approximation’, Comput. Math. Appl. 2(1), 17–40.

Gabriel, E., Fagg, G. E., Bosilca, G., Angskun, T., Dongarra, J. J., Squyres, J. M., Sahay, V.,
Kambadur, P., Barrett, B., Lumsdaine, A., Castain, R. H., Daniel, D. J., Graham, R. L.
and Woodall, T. S. (2004), Open MPI: Goals, concept, and design of a next generation
MPI implementation, in ‘Proceedings of the 11th European PVM/MPI Users’ Group
Meeting’, Budapest, Hungary, pp. 97–104.

Gentzsch, W. (2001), Sun Grid Engine: Towards creating a compute power grid, in ‘CCGRID

2001’, IEEE, pp. 35–36.

Gittens, A., Devarakonda, A., Racah, E., Ringenburg, M., Gerhardt, L., Kottalam, J., Liu,
J., Maschhoﬀ, K., Canon, S., Chhugani, J. et al. (2016), Matrix factorizations at scale: A
comparison of scientiﬁc data analytics in Spark and C + MPI using three case studies, in
‘2016 IEEE BigData’, IEEE, pp. 204–213.

Golub, G. H. and Van Loan, C. F. (2013), Matrix Computations, Johns Hopkins University

Press, Baltimore, MD.

Gower, R. M., Loizou, N., Qian, X., Sailanbayev, A., Shulgin, E. and Richt´arik, P. (2019),
SGD: General analysis and improved rates, in ‘ICML 2019’, Vol. 97 of Proc. Mach. Learn.
Res., pp. 5200–5209.

Griewank, A. and Walther, A. (2008), Evaluating derivatives: principles and techniques of

algorithmic diﬀerentiation, SIAM.

S. Ko, H. Zhou, J. J. Zhou, and J.-H. Won/High-performance statistical computing

32

Gu, Y., Fan, J., Kong, L., Ma, S. and Zou, H. (2018), ‘ADMM for high-dimensional sparse

penalized quantile regression’, Technometrics 60(3), 319–331.

Hager, G. and Wellein, G. (2010), Introduction to High Performance Computing for Scien-

tists and Engineers, CRC Press.

Huang, D. W., Sherman, B. T. and Lempicki, R. A. (2009a), ‘Bioinformatics enrichment
tools: paths toward the comprehensive functional analysis of large gene lists’, Nucleic
Acids Res. 37(1), 1–13.

Huang, D. W., Sherman, B. T. and Lempicki, R. A. (2009b), ‘Systematic and integrative
analysis of large gene lists using DAVID bioinformatics resources’, Nat. Protoc. 4(1), 44–
57.

Huang, J., Jiao, Y., Jin, B Liu, J., , Lu, X. and Yang, C. (2021), ‘A uniﬁed primal dual

active set algorithm for nonconvex sparse recovery’, Statist. Sci. 36(2), 215–238.

Huang, J., Jiao, Y., Liu, Y. and Lu, X. (2018), ‘A constructive approach to (cid:96)0 penalized

regression’, J. Mach. Learn. Res. 19(1), 403–439.

Hunter, D. and Li, R. (2005), ‘Variable selection using MM algorithms’, Ann. Statist.

33(4), 1617–1642.

Hunter, D. R. and Lange, K. (2004),

‘A tutorial on MM algorithms’, Amer. Statist.

58(1), 30–37.

Hyperion Research (2019), Hyperion Research HPC market update from ISC 2019, Technical

report, Hyperion Research.

Imaizumi, M. and Fukumizu, K. (2019), Deep neural networks learn non-smooth functions

eﬀectively, in ‘AISTATS 2019’, Vol. 89 of Proc. Mach. Learn. Res., pp. 869–878.

Janssens, B. (2021), ‘MPIArrays.jl: Distributed arrays based on MPI onesided communica-

tion’, https://github.com/barche/MPIArrays.jl. Accessed: 2021-07-03.

Jha, S., Qiu, J., Luckow, A., Mantha, P. and Fox, G. C. (2014), A tale of two data-intensive
paradigms: Applications, abstractions, and architectures, in ‘2014 IEEE BigData’, IEEE,
pp. 645–652.

Jia, Y., Shelhamer, E., Donahue, J., Karayev, S., Long, J., Girshick, R., Guadarrama, S.
and Darrell, T. (2014), Caﬀe: Convolutional architecture for fast feature embedding, in
‘MM 2014’, ACM, pp. 675–678.

Johnson, R. and Zhang, T. (2013), Accelerating stochastic gradient descent using predictive
variance reduction, in ‘NeurIPS 2013’, Vol. 26 of Adv. Neural Inform. Process. Syst.,
pp. 315–323.

Jordan, M. I., Lee, J. D. and Yang, Y. (2019), ‘Communication-eﬃcient distributed statis-

tical inference’, J. Amer. Statist. Assoc. 114(526), 668–681.

JuliaParallel Team (2021), ‘DistributedArrays.jl: Distributed arrays in Julia’, https://
github.com/JuliaParallel/DistributedArrays.jl. Accessed: 2021-07-03.
Keys, K. L., Zhou, H. and Lange, K. (2019), ‘Proximal distance algorithms: Theory and

practice.’, J. Mach. Learn. Res. 20(66), 1–38.

Kirk, D. (2007), NVIDIA CUDA software and GPU parallel computing architecture, in

‘ISMM’, Vol. 7, pp. 103–104.

Kl¨ockner, A., Pinto, N., Lee, Y., Catanzaro, B., Ivanov, P. and Fasih, A. (2012), ‘PyCUDA
and PyOpenCL: A scripting-based approach to GPU run-time code generation’, Parallel
Comput. 38(3), 157–174.

Ko, S. (2020), Easily parallelizable statistical computing methods and their applications in
modern high-performance computing environments, PhD thesis, Seoul National Univer-
sity.

Ko, S. and Won, J.-H. (2019), Optimal minimization of the sum of three convex functions

S. Ko, H. Zhou, J. J. Zhou, and J.-H. Won/High-performance statistical computing

33

with a linear operator, in ‘AISTATS 2019’, Vol. 89 of Proc. Mach. Learn. Res., pp. 1185–
1194.

Ko, S., Yu, D. and Won, J.-H. (2019), ‘Easily parallelizable and distributable class of al-
gorithms for structured sparsity, with optimal acceleration’, J. Comput. Graph. Statist.
28(4), 821–833.

Koanantakool, P., Ali, A., Azad, A., Buluc, A., Morozov, D., Oliker, L., Yelick, K. and Oh,
S.-Y. (2018), Communication-avoiding optimization methods for distributed massive-scale
sparse inverse covariance estimation, in ‘AISTATS 2018’, Vol. 84 of Proc. Mach. Learn.
Res., pp. 1376–1386.

Koanantakool, P., Azad, A., Bulu¸c, A., Morozov, D., Oh, S.-Y., Oliker, L. and Yelick, K.
(2016), Communication-avoiding parallel sparse-dense matrix-matrix multiplication, in
‘2016 IEEE IPDPS’, IEEE, pp. 842–853.

Kummer, B. (1988), Newton’s method for non-diﬀerentiable functions, in J. Guddat,
B. Bank, H. Hollatz, P. Kall, D. Karte, B. Kummer, K. Lommatzsch, L. Tammer, M. Vlach
and K. Zimmerman, eds, ‘Advances in Mathematical Optimization’, Vol. 45, Akademie-
Verlag, Berlin, pp. 114–125.

Lai, T. L. and Yuan, H. (2021), ‘Stochastic approximation: from statistical origin to big-data,

multidisciplinary applications’, Statist. Sci. 36(2), 291–302.

Lam, S. K., Pitrou, A. and Seibert, S. (2015), Numba: A LLVM-based Python JIT compiler,

in ‘LLVM 2015’, number 7, ACM, pp. 1–6.

Lange, K. (2016), MM Optimization Algorithms, Vol. 147, SIAM.
Lange, K., Hunter, D. R. and Yang, I. (2000), ‘Optimization transfer using surrogate objec-

tive functions’, J. Comput. Graph. Statist. 9(1), 1–20.

LeCun, Y., Bengio, Y. and Hinton, G. (2015), ‘Deep learning’, Nature 521(7553), 436–444.
Lee, D. D. and Seung, H. S. (1999), ‘Learning the parts of objects by non-negative matrix

factorization’, Nature 401(6755), 788–791.

Lee, D. D. and Seung, H. S. (2001), Algorithms for non-negative matrix factorization, in

‘NeurIPS 2001’, Vol. 14 of Adv. Neural Inform. Process. Syst., pp. 556–562.

Lee, J. D., Sun, Y., Liu, Q. and Taylor, J. E. (2015), ‘Communication-eﬃcient sparse re-

gression: a one-shot approach’, arXiv preprint arXiv:1503.04337 .

Lee, T., Won, J.-H., Lim, J. and Yoon, S. (2017), ‘Large-scale structured sparsity via parallel

fused lasso on multiple GPUs’, J. Comput. Graph. Statist. 26(4), 851–864.

Li, X., Sun, D. and Toh, K.-C. (2018), ‘A highly eﬃcient semismooth newton augmented

lagrangian method for solving lasso problems’, SIAM J. Optim. 28(1), 433–458.

Lim, H., Dewaraja, Y. K. and Fessler, J. A. (2018), ‘A PET reconstruction formulation that
enforces non-negativity in projection space for bias reduction in Y-90 imaging’, Phys.
Med. Biol. 63(3), 035042.

Lin, C.-J. (2007), ‘Projected gradient methods for nonnegative matrix factorization’, Neural

Comput. 19(10), 2756–2779.

Liu, X., Li, Y., Tang, J. and Yan, M. (2020), A double residual compression algorithm for
eﬃcient distributed learning, in ‘AISTATS 2020’, Vol. 108 of Proc. Mach. Learn. Res.,
pp. 133–143.

Mahajan, A., Taliun, D., Thurner, M., Robertson, N. R., Torres, J. M., Rayner, N. W.,
Payne, A. J., Steinthorsdottir, V., Scott, R. A., Grarup, N. et al. (2018), ‘Fine-mapping
type 2 diabetes loci to single-variant resolution using high-density imputation and islet-
speciﬁc epigenome maps’, Nat. Genet. 50(11), 1505–1513.

McLaren, W., Gil, L., Hunt, S. E., Riat, H. S., Ritchie, G. R., Thormann, A., Flicek, P. and
Cunningham, F. (2016), ‘The Ensembl variant eﬀect predictor’, Genome Biol. 17(1), 122.

S. Ko, H. Zhou, J. J. Zhou, and J.-H. Won/High-performance statistical computing

34

Mittal, S., Madigan, D., Burd, R. S. and Suchard, M. A. (2014), ‘High-dimensional, mas-
sive sample-size Cox proportional hazards regression for survival analysis’, Biostatistics
15(2), 207–221.

Munshi, A. (2009), The OpenCL speciﬁcation, in ‘2009 IEEE HCS’, IEEE, pp. 1–314.
Nakano, J. (2012), Parallel computing techniques, in ‘Handbook of Computational Statis-

tics’, Springer, pp. 243–271.

Negahban, S. N., Ravikumar, P., Wainwright, M. J. and Yu, B. (2012), ‘A uniﬁed framework
for high-dimensional analysis of M -estimators with decomposable regularizers’, Statist.
Sci. 27(4), 538–557.

NERSC

(2021),

https://docs.nersc.gov/
machinelearning/tensorflow/#distributed-tensorflow. Accessed: 2021-
07-03.

TensorFlow’,

‘Distributed

Ng, M. C., Shriner, D., Chen, B. H., Li, J., Chen, W.-M., Guo, X., Liu, J., Bielinski, S. J.,
Yanek, L. R., Nalls, M. A. et al. (2014), ‘Meta-analysis of genome-wide association studies
in african americans provides insights into the genetic architecture of type 2 diabetes’,
PLoS Genet. 10(8), e1004517.

Nitanda, A. (2014), Stochastic proximal gradient descent with acceleration techniques, in

‘NeurIPS 2014’, Vol. 27 of Adv. Neural Inform. Process. Syst., pp. 1574–1582.

NVIDIA (2021a), ‘Basic linear algebra subroutines (cuBLAS) library’, http://docs.

nvidia.com/cuda/cublas. Accessed: 2021-07-03.

NVIDIA (2021b),

‘Sparse matrix library (cuSPARSE)’, http://docs.nvidia.com/

cuda/cusparse. Accessed: 2021-07-03.

Ohn, I. and Kim, Y. (2019), ‘Smooth function approximation by deep neural networks with

general activation functions’, Entropy 21(7), 627.

Owens, J. D., Luebke, D., Govindaraju, N., Harris, M., Kr¨uger, J., Lefohn, A. E. and
Purcell, T. J. (2007), A survey of general-purpose computation on graphics hardware, in
‘Computer Graphics Forum’, Vol. 26, Wiley Online Library, pp. 80–113.

Parikh, N. and Boyd, S. (2014), ‘Proximal algorithms’, Found. Trends Optim. 1(3), 127–239.
Paszke, A., Gross, S., Massa, F., Lerer, A., Bradbury, J., Chanan, G., Killeen, T., Lin, Z.,
Gimelshein, N., Antiga, L. et al. (2019), Pytorch: An imperative style, high-performance
deep learning library, in ‘NeurIPS 2019’, Vol. 32 of Adv. Neural Inform. Process. Syst.,
pp. 8026–8037. Software available from https://pytorch.org.

Polson, N. G., Scott, J. G. and Willard, B. T. (2015), ‘Proximal algorithms in statistics and

machine learning’, Statist. Sci. 30(4), 559–581.

Qi, L. and Sun, J. (1993), ‘A nonsmooth version of Newton’s method’, Math. Program.

58(1-3), 353–367.

Qian, X., Qu, Z. and Richt´arik, P. (2019), SAGA with arbitrary sampling, in ‘ICML 2019’,

Vol. 97 of Proc. Mach. Learn. Res., pp. 5190–5199.

R Core Team (2021), R: A Language and Environment for Statistical Computing, R Foun-
dation for Statistical Computing, Vienna, Austria. https://www.R-project.org/.
Version 4.1.0. Accessed: 2021-07-03.

Raina, R., Madhavan, A. and Ng, A. Y. (2009), Large-scale deep unsupervised learning

using graphics processors, in ‘ICML 2009’, ACM, pp. 873–880.

Ramdas, A. and Tibshirani, R. J. (2016), ‘Fast and ﬂexible ADMM algorithms for trend

ﬁltering’, J. Comput. Graph. Statist. 25(3), 839–858.

Reyes, A. R. (2021), ‘rTorch’, https://f0nzie.github.io/rTorch/. Accessed: 2021-

07-03.

Reyes-Ortiz, J. L., Oneto, L. and Anguita, D. (2015), Big data analytics in the cloud: Spark

S. Ko, H. Zhou, J. J. Zhou, and J.-H. Won/High-performance statistical computing

35

on Hadoop vs MPI/OpenMP on Beowulf., in ‘INNS Conference on Big Data’, Vol. 8,
p. 121.

Richt´arik, P. and Tak´aˇc, M. (2016a), ‘On optimal probabilities in stochastic coordinate

descent methods’, Optim. Lett. 10(6), 1233–1243.

Richt´arik, P. and Tak´aˇc, M. (2016b), ‘Parallel coordinate descent methods for big data

optimization’, Math. Program. 156(1-2), 433–484.

Robbins, H. and Monro, S. (1951), ‘A stochastic approximation method’, Ann. Math. Statis-

tics 22, 400–407.

Roland, C., Varadhan, R. and Frangakis, C. (2007), ‘Squared polynomial extrapolation
methods with cycling: an application to the positron emission tomography problem’, Nu-
mer. Algorithms 44(2), 159–172.

Rosasco, L., Villa, S. and V˜u, B. C. (2019), ‘Convergence of stochastic proximal gradient

algorithm’, Appl. Math. Optim. pp. 1–27.

RStudio (2021), ‘R interface to TensorFlow’, https://tensorflow.rstudio.com/.

Version 2.5.0. Accessed: 2021-07-03.

Rudin, L. I., Osher, S. and Fatemi, E. (1992), ‘Nonlinear total variation based noise removal

algorithms’, Physica D 60(1), 259–268.

Rumelhart, D. E., Hinton, G. E. and Williams, R. J. (1986), ‘Learning representations by

back-propagating errors’, Nature 323, 533–536.

Ryu, E. K., Ko, S. and Won, J.-H. (2020), ‘Splitting with near-circulant linear systems:
applications to total variation CT and PET’, SIAM J. Sci. Comput. 42(1), B185–B206.
Schmidt-Hieber, J. et al. (2020), ‘Nonparametric regression using deep neural networks with

ReLU activation function’, Ann. Statist. 48(4), 1875–1897.

Scott, L. J., Mohlke, K. L., Bonnycastle, L. L., Willer, C. J., Li, Y., Duren, W. L., Erdos,
M. R., Stringham, H. M., Chines, P. S., Jackson, A. U. et al. (2007), ‘A genome-wide
association study of type 2 diabetes in ﬁnns detects multiple susceptibility variants’,
Science 316(5829), 1341–1345.

Seide, F. and Agarwal, A. (2016), CNTK: Microsoft’s open-source deep-learning toolkit, in

‘SIGKDD 2016’, ACM, pp. 2135–2135.

Sergeev, A. and Del Balso, M. (2018), ‘Horovod: fast and easy distributed deep learning in

tensorﬂow’, arXiv preprint arXiv:1802.05799 .

Staples, G. (2006), Torque resource manager, in ‘SC 2006’, ACM, p. 8.
Suchard, M. A., Holmes, C. and West, M. (2010), ‘Some of the what?, why?, how?, who?
and where? of graphics processing unit computing for Bayesian analysis’, Bulletin of the
International Society for Bayesian Analysis 17(1), 12–16.

Suchard, M. A., Simpson, S. E., Zorych, I., Ryan, P. and Madigan, D. (2013), ‘Massive
parallelization of serial inference algorithms for a complex generalized linear model’, ACM
Trans. Model. Comput. Simul. 23(1), 1–23.

Suchard, M. A., Wang, Q., Chan, C., Frelinger, J., Cron, A. and West, M. (2010), ‘Un-
derstanding GPU programming for statistical computation: Studies in massively parallel
massive mixtures’, J. Comput. Graph. Statist. 19(2), 419–438.

Sudlow, C., Gallacher, J., Allen, N., Beral, V., Burton, P., Danesh, J., Downey, P., Elliott,
P., Green, J., Landray, M. et al. (2015), ‘UK Biobank: an open access resource for iden-
tifying the causes of a wide range of complex diseases of middle and old age’, PLoS Med.
12(3), e1001779.

Suzuki, T. (2019), Adaptivity of deep ReLU network for learning in Besov and mixed smooth

Besov spaces: optimal rate and curse of dimensionality, in ‘ICLR 2019’.

Tang, H., Yu, C., Lian, X., Zhang, T. and Liu, J. (2019), DoubleSqueeze: Parallel stochas-

S. Ko, H. Zhou, J. J. Zhou, and J.-H. Won/High-performance statistical computing

36

tic gradient descent with double-pass error-compensated compression, in ‘ICML 2019’,
Vol. 97 of Proc. Mach. Learn. Res., pp. 6155–6165.

The Wellcome Trust Case Control Consortium (2007), ‘Genome-wide association study of
14,000 cases of seven common diseases and 3,000 shared controls’, Nature 447, 661–678.
Theano Development Team (2016), ‘Theano: A Python framework for fast computation of

mathematical expressions’, arXiv preprint arXiv:1605.02688 .

Tibshirani, R. (1996), ‘Regression shrinkage and selection via the lasso’, J. R. Stat. Soc.

Ser. B. Stat. Methodol. 58(1), 267–288.

Tibshirani, R. J. and Taylor, J. (2011), ‘The solution path of the generalized lasso’, Ann.

Statist. 39(3), 1335–1371.

Tieleman, T. (2010), Gnumpy: an easy way to use GPU boards in Python, Technical Report

UTML TR 2010–002, Department of Computer Science, University of Toronto.

Tseng, P. and Yun, S. (2009), ‘A coordinate gradient descent method for nonsmooth sepa-

rable minimization’, Math. Program. 117(1-2), 387–423.

University of Zurich (2021), ‘ElastiCluster’, https://elasticluster.readthedocs.

io/en/latest/. Accessed: 2021-07-03.

Ushey, K., Allaire, J. and Tang, Y. (2021), reticulate: Interface to ’Python’. https://
CRAN.R-project.org/package=reticulate. Version 1.20. Accessed: 2021-07-03.
Van De Geijn, R. A. and Watts, J. (1997), ‘SUMMA: Scalable universal matrix multiplica-

tion algorithm’, Concurrency: Practice and Experience 9(4), 255–274.

van Rossum, G. (1995), Python tutorial, Technical Report CS-R9526, Centrum voor

Wiskunde en Informatica (CWI), Amsterdam.

Voight, B. F., Scott, L. J., Steinthorsdottir, V., Morris, A. P., Dina, C., Welch, R. P.,
Zeggini, E., Huth, C., Aulchenko, Y. S., Thorleifsson, G. et al. (2010), ‘Twelve type 2
diabetes susceptibility loci identiﬁed through large-scale association analysis’, Nat. Genet.
42(7), 579.

V˜u, B. C. (2013), ‘A splitting algorithm for dual monotone inclusions involving cocoercive

operators’, Adv. Comput. Math. 38(3), 667–681.

Walker, E. (2008), ‘Benchmarking Amazon EC2 for hig-performance scientiﬁc computing’,

;login:: the Magazine of USENIX & SAGE 33(5), 18–23.

Wang, E., Zhang, Q., Shen, B., Zhang, G., Lu, X., Wu, Q. and Wang, Y. (2014), Intel Math
Kernel library, in ‘High-Performance Computing on the Intel® Xeon Phi™’, Springer,
pp. 167–188.

Wang, J., Kolar, M., Srebro, N. and Zhang, T. (2017), Eﬃcient distributed learning with

sparsity, in ‘ICML 2017’, Vol. 70 of Proc. Mach. Learn. Res., pp. 3636–3645.

Won, J.-H. (2020), Proximity operator of the matrix perspective function and its applica-

tions, in ‘NeurIPS 2020’, Vol. 33 of Adv. Neural Inform. Process. Syst.

Wright, S. J. (2015), ‘Coordinate descent algorithms’, Math. Program. 151(1), 3–34.
Wu, T. T. and Lange, K. (2010), ‘The MM alternative to EM’, Statist. Sci. 25(4), 492–505.
Xiao, L. and Zhang, T. (2014), ‘A proximal stochastic gradient method with progressive

variance reduction’, SIAM J. Optim. 24(4), 2057–2075.

Xue, L., Ma, S. and Zou, H. (2012), ‘Positive-deﬁnite (cid:96)1-penalized estimation of large co-

variance matrices’, J. Amer. Statist. Assoc. 107(500), 1480–1491.

Yoo, A. B., Jette, M. A. and Grondona, M. (2003), Slurm: Simple linux utility for resource

management, in ‘JSSPP 2003’, Springer, pp. 44–60.

Yu, D., Won, J.-H., Lee, T., Lim, J. and Yoon, S. (2015), ‘High-dimensional fused lasso
regression using majorization–minimization and parallel processing’, J. Comput. Graph.
Statist. 24(1), 121–153.

S. Ko, H. Zhou, J. J. Zhou, and J.-H. Won/High-performance statistical computing

37

Zaharia, M., Chowdhury, M., Franklin, M. J., Shenker, S., Stoica, I. et al. (2010), ‘Spark:

Cluster computing with working sets.’, HotCloud 10(10-10), 95.

Zhang, X., Wang, Q. and Chothia, Z. (2021), ‘OpenBLAS: An optimized BLAS library’,

https://www.openblas.net/. Accessed: 2021-07-03.

Zhang, Y., Duchi, J. C. and Wainwright, M. J. (2013), ‘Communication-eﬃcient algorithms

for statistical optimization’, J. Mach. Learn. Res. 14(1), 3321–3363.

Zhou, H., Lange, K. and Suchard, M. A. (2010), ‘Graphics processing units and high-

dimensional optimization’, Statist. Sci. 25(3), 311–324.

Zhu, M. and Chan, T. (2008), An eﬃcient primal-dual hybrid gradient algorithm for total

variation image restoration, Technical Report 08-34, UCLA CAM.

Zhu, Y. (2017), ‘An augmented ADMM algorithm with application to the generalized lasso

problem’, J. Comput. Graph. Statist. 26(1), 195–204.

S. Ko, H. Zhou, J. J. Zhou, and J.-H. Won/High-performance statistical computing

38

Appendix A: A brief introduction to PyTorch

In this section, we introduce simple operations on PyTorch. Note that Python uses 0-based,
row-major ordering, like C and C++ (R is 1-based, column-major ordering). First we import
the PyTorch library. This is equvalent to library() in R.

import torch

Tensor creation

The following is equivalent to set.seed() in R.

torch.manual_seed(100)

One may create an uninitialized tensor. This creates a 3 × 4 tensor (matrix).

torch.empty(3, 4) # uninitialized tensor

tensor([[-393462160144990208.0000,
-393462160144990208.0000,
0.0000,
0.0000,
0.0000,
0.0000,

[

[

0.0000,
0.0000],
0.0000,
0.0000],
0.0000,
0.0000]])

This generates a tensor initialized with random values from (0, 1).

y = torch.rand(3, 4) # from Unif(0, 1)

tensor([[0.1117, 0.8158, 0.2626, 0.4839],
[0.6765, 0.7539, 0.2627, 0.0428],
[0.2080, 0.1180, 0.1217, 0.7356]])

We can also generate a tensor ﬁlled with zeros or ones.

z = torch.ones(3, 4) # torch.zeros(3, 4)

tensor([[1., 1., 1., 1.],
[1., 1., 1., 1.],
[1., 1., 1., 1.]])

A tensor can be created from standard Python data.

w = torch.tensor([3, 4, 5, 6])

tensor([3, 4, 5, 6])

Indexing

The following is the standard method of indexing tensors.

y[2, 3] # indexing: zero-based, returns a 0-dimensional tensor

tensor(0.7356)

The indexing always returns a (sub)tensor, even for scalars (treated as zero-dimensional

tensors). A standard Python number can be returned by using .item().

S. Ko, H. Zhou, J. J. Zhou, and J.-H. Won/High-performance statistical computing

39

y[2, 3].item() # A standard Python floating-point number

0.7355988621711731

To get a column from a tensor, we use the indexing as below. The syntax is similar but

slightly diﬀerent from R.

y[:, 3] # 3rd column. The leftmost column is 0th. cf. y[, 4] in R

tensor([0.4839, 0.0428, 0.7356])

The following is for taking a row.

y[2, :] # 2nd row. The top row is 0th. cf. y[3, ] in R

tensor([0.2080, 0.1180, 0.1217, 0.7356])

Simple operations

Here we provide an example of simple operations on PyTorch. Addition using the operator
‘+’ acts just like anyone can expect:

x = y + z # a simple addition.

tensor([[1.1117, 1.8158, 1.2626, 1.4839],
[1.6765, 1.7539, 1.2627, 1.0428],
[1.2080, 1.1180, 1.1217, 1.7356]])

Here is another form of addition.

x = torch.add(y, z) # another syntax for addition

The operators ending with an underscore ( ) changes the value of the tensor in-place.

y.add_(z) # in-place addition

tensor([[1.1117, 1.8158, 1.2626, 1.4839],
[1.6765, 1.7539, 1.2627, 1.0428],
[1.2080, 1.1180, 1.1217, 1.7356]])

Concatenation

We can concatenate the tensors using the function cat(), which resembles c(), cbind(),
and rbind() in R. The second argument indicates the dimension that the tensors are
concatenated along: zero signiﬁes concatenation by rows while one by columns.

torch.cat((y, z), 0) # along the rows

tensor([[1.1117, 1.8158, 1.2626, 1.4839],
[1.6765, 1.7539, 1.2627, 1.0428],
[1.2080, 1.1180, 1.1217, 1.7356],
[1.0000, 1.0000, 1.0000, 1.0000],
[1.0000, 1.0000, 1.0000, 1.0000],
[1.0000, 1.0000, 1.0000, 1.0000]])

S. Ko, H. Zhou, J. J. Zhou, and J.-H. Won/High-performance statistical computing

40

torch.cat((y, z), 1) # along the columns

tensor([[1.1117, 1.8158, 1.2626, 1.4839, 1.0000, 1.0000, 1.0000, 1.0000],
[1.6765, 1.7539, 1.2627, 1.0428, 1.0000, 1.0000, 1.0000, 1.0000],
[1.2080, 1.1180, 1.1217, 1.7356, 1.0000, 1.0000, 1.0000, 1.0000]])

Reshaping

One can reshape a tensor, like changing the attribute dim in R.

y.view(12) # 1-dimensional array

tensor([1.1117, 1.8158, 1.2626, 1.4839, 1.6765, 1.7539, 1.2627, 1.0428, 1.2080,

1.1180, 1.1217, 1.7356])

Up to one of the arguments of view() can be −1. The size of the reshaped tensor is

inferred from the other dimensions.

# reshape into (6)-by-2 tensor;
# (6) is inferred from the other dimension
y.view(-1, 2)

tensor([[1.1117, 1.8158],
[1.2626, 1.4839],
[1.6765, 1.7539],
[1.2627, 1.0428],
[1.2080, 1.1180],
[1.1217, 1.7356]])

Appendix B: AWS EC2 and ParallelCluster

We used AWS Elastic Compute Cloud (EC2) via CfnCluster throughout our multi CPU-
node experiments, which is updated to ParallelCluster after we had completed the experi-
ments. In this section, we instruct how to use ParallelCluster via Amazon Web Services. This
section is structured into three parts: set up AWS account, conﬁgure, and run a job on Par-
allelCluster. We refer the readers to the oﬃcial documentation5 and an AWS whitepaper6
for further details.

B.1. Overview

A virtual cluster created by ParallelCluster consists of two types of instances in EC2: a
master instance and multiple worker instances. The master instance manages jobs through
a queue on a job scheduler and several AWS services such as Simple Queue Service and Auto
Scaling Group. When a virtual cluster is created, a volume of Elastic Block Store (EBS) is
created and automatically attached to the master instance. It is shared over the workers,
forming a shared ﬁle system. The software necessary for the jobs are installed in this ﬁle
system, and a script to set up the environment variables for the tools is utilized. While the
master instance does not directly take part in the actual computation, the speed of network
on the shared ﬁle system depends on the instance type of the master instance. If the jobs
depend on the shared dataset, the master instance has to allow fast enough network speed.

5https://docs.aws.amazon.com/parallelcluster/index.html
6https://d1.awsstatic.com/Projects/P4114756/deploy-elastic-hpc-cluster_project.

a12a8c61339522e21262da10a6b43a3678099220.pdf

S. Ko, H. Zhou, J. J. Zhou, and J.-H. Won/High-performance statistical computing

41

The actual computation is performed on the worker instances. Each worker has access to the
shared ﬁle system where the necessary tools and data reside. The network speed between
workers depends on the worker instance type.

B.2. Glossary

We brieﬂy introduce some of the key concepts regarding the AWS and cluster computing in
this subsection.

Some of the basic concepts from AWS are shown below:
• Instance: a virtual computer on AWS EC2. There are various types of instances,
which are distinguished by the number of cores, memory size, network speed, etc.
c5.18xlarge is prominently utilized in our experiments.7

• Region: a region (e.g., North Virginia, Ohio, North California, Oregon, Hong. Kong,
Seoul, Tokyo) is completely independent from the other regions, and the data transfer
between regions are charged.

• Availability zone: there are a handful of availability zones in each region. Each avail-
ability zone is isolated, but availability zones in the same region are interconnected
with a low-latency network. Note that a virtual cluster created by ParallelCluster is
tied to a single availability zone.

Listed below are some, but not all, of the AWS services involved in ParallelCluster. They
are all managed automatically through ParallelCluster and can be modiﬁed through the
AWS console.

• Elastic Compute Cloud (EC2): the core service of AWS that allows users to rent virtual

computers. There are three methods of payment available:

– On-demand: hourly charged, without risk of interruption.

– Spot: bid-based charging. Serviced at up to 70%-discounted rate, but is inter-

rupted if the price goes higher than the bid price.

– Reserved: one-time payment at discounted rate.

• Elastic Block Store (EBS): persistent block storage volume for EC2 instances, e.g. a
solid-state drive (SSD). In ParallelCluster, each instance is started with a root EBS
volume exclusive to each instance.

• CloudFormation: An interface that describes and provisions the cloud resources.
• Simple Queue Service: the actual job queue is served through message passing between

EC2 instances.

• CloudWatch: monitors and manages the cloud.
• Auto Scaling Group: a collection of EC2 instances with similar characteristics. The
number of instances is automatically scaled based on criteria deﬁned over CloudWatch.
• Identity and Access Management (IAM): An IAM user is an “entity that [one] creates
in AWS to represent the person or application that uses it to interact with AWS.”8
Each IAM user is granted certain permissions determined by the root user. As there
are many services involved in ParallelCluster, it is recommended to use an IAM user
with full permission.

• Virtual Private Cloud (VPC): a VPC is a dedicated virtual network exclusive to the
user, isolated from any other VPCs, which spans all the availability zones in one region.
A subnet is a subnetwork in VPC exclusive to a single availity zone.9

7See https://aws.amazon.com/en/ec2/instance-types/ for the full list of types of instances.
8https://docs.aws.amazon.com/IAM/latest/UserGuide/id_users.html
9https://docs.aws.amazon.com/vpc/latest/userguide/VPC_Subnets.html

S. Ko, H. Zhou, J. J. Zhou, and J.-H. Won/High-performance statistical computing

42

• Security Group (SG): A security group acts as a “virtural ﬁrewall that controls the

traﬃc for one or more instances.”10

Here are some of the concepts related to cluster computing:
• Shared ﬁle system: for multiple instances to work on the same data, it is convenient to
have a ﬁle system that can be accessed by all the instances involved. In ParallelCluster,
it is implemented as an additional EBS volume attached to the master instance. All
the worker instances can access this volume, and its speed of network depends on the
instance type of the master instance.

• Job: a unit of execution, deﬁned by either a single command or a job script.
• Queue: a data structure containing jobs to run. Jobs in a queue are managed and

prioritized by a job scheduler.

• Master: an instance that manages the job scheduler.
• Worker: an instance that executes the jobs.
• Job scheduler: an application program that controls the execution of jobs over a cluster.
e.g. Sun Grid Engine, Torque, Slurm, etc. The Sun Grid Engine (SGE) was used for
our experiments.

Several SGE commands are as follows:
• qsub: submits a job to the job queue
• qdel: removes a job on the job queue
• qstat: shows the current status of the queue
• qhost: shows the current list of workers

B.3. Prerequisites

The following are needed before we proceed. Most of these might be considered the ﬁrst
steps to use AWS.

• Access keys with administrative privileges: Access keys are credentials for IAM users
and root users. They consist of access key ID (analogous to username) and secret access
key (analogous to passwords). They should be kept conﬁdential. It is recommended to
create a temporary IAM user with administrative privilage and create an access key
ID and a secret access key for the IAM user. They can be created in the AWS console
(or the IAM console for an IAM user).11

• A VPC and a subnet: A VPC for each region and a subnet for each availability zone
is created by default. One may use these default VPC and subnet or newly-created
ones.

• A security group: One may use a default security group or a newly-created one.
• A key pair that allows the user to access the cloud via SSH: Amazon EC2 uses public-
key cryptography for login credentials. Each EC2 instance is conﬁgured with a public
key, and the user has to access this instance using the matching private key. It can be
generated and managed on AWS EC2 console as well as the user’s terminal.12

B.4. Installation

First, we install the ParallelCluster command line interface (CLI) on a local machine. Par-
allelCluster command line interface is distributed through the standard Python Package

10https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/using-network-security.

html

11https://docs.aws.amazon.com/IAM/latest/UserGuide/id_credentials_access-keys.

html

12https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ec2-key-pairs.html

S. Ko, H. Zhou, J. J. Zhou, and J.-H. Won/High-performance statistical computing

43

Index (PyPI), so one may install it through pip, the standard package-installing command
for Python. One may install ParallelCluster by executing the following on the command
line:

sudo pip install aws-parallelcluster

B.5. Conﬁguration

Once ParallelCluster is installed on a local machine, an initial conﬁguration is needed. It
can be done by various ways, but the easiest way is through the command below:

pcluster configure

Then, the interactive dialog to setup ParallelCluster appears:

ParallelCluster Template [default]: <a name desired>
AWS Access Key ID []: <copy and paste the access key>
AWS Secret Access Key ID []: <copy and paste the secret key>
Acceptable Values for AWS Region ID:

eu-north-1
ap-south-1
eu-west-3
eu-west-2
eu-west-1
ap-northeast-2
ap-northeast-1
sa-east-1
ca-central-1
ap-southeast-1
ap-southeast-2
eu-central-1
us-east-1
us-east-2
us-west-1
us-west-2

AWS Region ID [ap-northeast-2]: <the region to use>
VPC Name [<default name>]: <a name desired>
Acceptable Values for Key Name:

<the registered key names appear here>
Key Name []: <enter the EC2 key pair name>
Acceptable Values for VPC ID:

<the list of VPC appears here>
VPC ID []: <enter one of the vpc above>
Acceptable Values for Master Subnet ID:

<the list of subnet ids appears here>

Master Subnet ID [subnet-<default value>]: <enter one of the

master subnet id above>

Now examine the ﬁles in the directory ˜/.parallelcluster (a hidden directory under
the home directory). The ﬁle pcluster-cli.log shows the log and the ﬁle config shows
the conﬁguration. One can modify the ﬁle config to ﬁne-tune the conﬁguration per user’s
need. The following is the config corresponding to our CfnCluster experiments:

[global]
update_check = true
sanity_check = true
cluster_template = test

S. Ko, H. Zhou, J. J. Zhou, and J.-H. Won/High-performance statistical computing

44

[aws]
aws_region_name = ap-northeast-2

[cluster test]
vpc_settings = testcfn
key_name = <key name>
initial_queue_size = 0
max_queue_size = 20
ebs_settings = expr_ebs
scheduler = sge
compute_instance_type = c5.18xlarge
master_instance_type = c5.18xlarge
cluster_type = spot
spot_price = 1.20
base_os = centos7
scaling_settings = custom
extra_json = {"cluster" : { "cfn_scheduler_slots" : "2"} }
master_root_volume_size = 20
compute_root_volume_size = 20

[ebs expr_ebs]
ebs_snapshot_id = < a snapshot id >
volume_size = 40

[vpc testcfn]
master_subnet_id = < a subnet id >
vpc_id = < a vpc id >

[aliases]
ssh = ssh {CFN_USER}@{MASTER_IP} {ARGS}

[scaling custom]
scaling_idletime = 20

In the [global] section, we set global conﬁgurations. The cluster template names

the cluster section to be used for the cluster.
update check check for the updates to ParallelCluster, and sanity check validates that
resources deﬁned in parameters.

In the [aws] section, the region is speciﬁed. AWS access key and secret key may appear

here unless speciﬁed in the base AWS CLI.

In the [cluster] section, we deﬁne the detailed speciﬁcation of the virtual cluster.
The vpc settings names a setting for VPC, detailed in the [vpc] section, and the
ebs settings names the setting for EBS, detailed in [ebs] section. The key name
deﬁnes the key name to use. The initial queue size deﬁnes the number of worker
instances at the launch of the cluster. We used zero for our experiments, as we often needed
to check if the conﬁguration is done properly on master before running actual jobs. The
worker instances are launched upon submission of a new job into the queue, and they are
terminated when the workers stay idle for a while (not exactly deﬁned, but often around
ﬁve to ten minutes).

We set the max queue size, the maximum number of worker instances, to 20. We
used CentOS 7 as the base os for our instances. The master root volume size and
the compute root volume size determine the size of root volume of the master in-
stance and each of the worker instance, respectively. For the scheduler, we used the
Sun Grid Engine (sge). For the compute instance type, we used c5.18xlarge, an
instance with 36 physical cores (72 virtual cores with hyperthreading). It consists of two
non-uniform memory access (NUMA) nodes with 18 physical cores each. In NUMA memory
design, an access to local memory of a processor is faster than an access to non-local memory

S. Ko, H. Zhou, J. J. Zhou, and J.-H. Won/High-performance statistical computing

45

within a shared memory system. master instance type deﬁnes the instance type of the
master. Sometimes it is ﬁne to be as small as t2.micro, a single-core instance, but we
needed an instance with good network performance when many instances simultaneously
accessed a large data ﬁle on shared storage. The cluster type is either ondemand (de-
fault) or spot. For c5.18xlarge in Seoul region (ap-northeast-2), on-demand price
was $3.456 per instance-hour, while the spot price was at $1.0788 per instance-hour through-
out the duration of our experiments. Budget-constrained users may use spot instances for
worker instances. In case of this scenario, the spot prices was set to $1.20 per instance-
hour, so if the actual price went above this value, our worker instances would have been
terminated. Only the on-demand instance could be used as the master instance, so smaller
instance might be desirable for lower cost. The setting extra json = {"cluster" : {
"cfn scheduler slots" : "2"} } sets number of slots that an instance bears to two.
Each computing job is required to declare the number of “slots” to occupy. By default, the
number of slots per instance is the number of virtual cores the instance has. This default
setting is natural, but a problem arises if we intend to utilize shared-memory parallelism
in NUMA node-level, as the number of slots occupied is tied to the number of instances
launched. We assigned one slot per NUMA node that an instance has (i.e., 2 slots per
instance) and utilized all 18 physical cores per NUMA node.

The [ebs] section deﬁnes the conﬁguration for the EBS volume mounted on the master
node and shared via NFS to workers. The ebs snapshot id deﬁnes the ID of the EBS
snapshot to be used. We had datasets and packages necessary for our jobs pre-installed in
an EBS volume and created a snapshot. The size of the volume was 40 GB. By default, the
volume is mounted to the path /shared.

We refer the readers to https://docs.aws.amazon.com/parallelcluster/ for

further details.

B.6. Creating, accessing, and destroying the cluster

We can create a virtual cluster named example by issuing the following command on a
local machine:

pcluster create example

To access the master instance through ssh, one needs the location of the private key

(.pem) ﬁle. The command to use is:

pcluster ssh example -i <private key file>

The default username for instances with CentOS is centos. The default username de-
pends on the Amazon Machine Image (AMI) being used to create a virtual machine, which
is determined by the base os selected on the conﬁguration. The names of the existing clus-
ters can be listed using the command pcluster list, and we may completely remove a
cluster example using the command pcluster delete example.

B.7. Installation of libraries

Now we can access the master node through secure shell(SSH). We have a shared EBS volume
mounted at /shared, and we are to install necessary software there. For our experiments,
we installed anaconda, a portable installation of Python, in the directory /shared. A script
to set up environment variables is also created and saved in /shared:

S. Ko, H. Zhou, J. J. Zhou, and J.-H. Won/High-performance statistical computing

46

# setup.sh
module load mpi/openmpi-x86_64 # loads MPI to the environment
source /shared/conda/etc/profile.d/conda.sh
export PATH=/shared/conda/bin:$PATH
export LD_LIBRARY_PATH=/shared/conda/lib:$LD_LIBRARY_PATH

We issued the command:

source setup.sh

to set up the environment variables. We installed PyTorch from source13, as it is required

to do so in order to incorporate MPI.

To download our code, one can issue the command:

git clone https://github.com/kose-y/dist stat /shared/dist stat

B.8. Running a job

To provide instructions on how to deﬁne the environment to each instance, we need a script
deﬁning each job. The following script mcpi-2.job is for running the program for Monte
Carlo estimation of π in Section 3 (Listing 1) using two instances (four processes using 18
threads each).

#!/bin/sh
#$ -cwd
#$ -N mcpi
#$ -pe mpi 4
#$ -j y
date
source /shared/conda/etc/profile.d/conda.sh
export PATH=/shared/conda/bin:$PATH
export LD_LIBRARY_PATH=/shared/conda/lib:$LD_LIBRARY_PATH
export MKL_NUM_THREADS=18
mpirun -np 4 python /shared/dist stat/examples/mcpi-mpi-pytorch.py

The line -pe mpi 4 tells the scheduler that we are using four slots. Setting the value of
the environment variable MKL NUM THREADS to 18 means that MKL runs with 18 threads or
cores for that process. We launch four processes in the cluster, two per instance, as deﬁned
by our ParallelCluster setup, in parallel using MPI. We can submit this job to the Sun Grid
Engine (the job scheduler) using the command:

qsub mcpi-2.job

When we submit a job, a message similar to the following appears:

Your job 130 ("mcpi") has been submitted

One may see the newly submitted job in the queue using the command qstat.

qstat

job-ID
------------------------------------------------------------------------------

submit/start at

slots

queue

prior

state

user

name

130 0.55500 mcpi

centos

qw

02/28/2019 03:58:54

4

13https://github.com/pytorch/pytorch#from-source

S. Ko, H. Zhou, J. J. Zhou, and J.-H. Won/High-performance statistical computing

47

If we want to delete any job waiting for the queue or running, use the command qdel.

qdel 130

centos has deleted job 130

Once the job is completed, the output is saved as a text ﬁle named such as mcpi.o130.

For example:

Thu Feb 28 04:07:54 UTC 2019
3.148

The scripts for our numerical examples are in /shared/dist stat/jobs.

B.9. Miscellaneous

To keep what is on the EBS volume on the cloud and access later, we need to create a
snapshot for the volume. We can later create a volume based on this snapshot14 and mount
it on any instance15. In ParallelCluster, this is done automatically when we give an ID of a
snapshot in the config ﬁle.

Appendix C: Monte Carlo estimation of π on multi-GPU using TensorFlow

This section shows an implementation of Monte Carlo estimation example in Section 3.3 in
multi-GPU using TensorFlow. Listing C.1 is the implementation that assumes a node with
four GPUs. The code appears more or less the same as Listing 1, except that the list of
devices is pre-speciﬁed. Line 2 indicates that a static computational graph is used (instead of
the eager execution) for the function to run simultaneously on multiple GPUs. It is slightly
shorter than Listing 1, partially because multi-GPU is supported natively in TensorFlow,
and MPI is not used.

Appendix D: Further details on examples

D.1. Nonnegative matrix factorization

The multiplicative algorithm discussed in Zhou et al. (2010, Sect. 3.1) is due to Lee and
Seung (1999, 2001):

V n+1 = V n (cid:12) [X(W n)T ] (cid:11) [V nW n(W n)T ]
W n+1 = W n (cid:12) [(V n+1)T X] (cid:11) [(V n+1)T V n+1W n],

where (cid:12) and (cid:11) respectively denote elementwise multiplication and division. This algorithm
can be interpreted as an MM algorithm minimizing the surrogate function of f (V, W ) =
(cid:107)X − V W (cid:107)2

F based on Jensen’s inequality:

g(V, W |V n, W n) =

(cid:88)

i,j,k

ikwn
vn
kj
k(cid:48) vn
ik(cid:48)wn
k(cid:48)j

(cid:80)

(cid:32)

xij −

(cid:80)

ik(cid:48)wn
k(cid:48)j

k(cid:48) vn
vn
ikwn
kj

(cid:33)2

vikwkj

.

14https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ebs-creating-snapshot.html
15https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ebs-using-volumes.html

S. Ko, H. Zhou, J. J. Zhou, and J.-H. Won/High-performance statistical computing

48

1 import tensorflow as tf

2
3 # Enforce graph computation. With eager execution, the code runs
4 # sequentially w.r.t. GPUs. e.g., computation for ’/gpu:1’ would not
5 # start until the computation for ’/gpu:0’ finishes.
6 @tf.function
7 def mc_pi(n, devices):

8

9

10

11

12

13

14

15

16

17

18

estim = []
for d in devices:

# use device d in this block
with tf.device(d):

x = tf.random.uniform((n,), dtype=tf.float64)
y = tf.random.uniform((n,), dtype=tf.float64)
# compute local estimate of pi
# and save it as an element of ’estim’.
estim.append(tf.reduce_mean(tf.cast(x ** 2 +

y ** 2 < 1, tf.float64)) * 4)

return tf.add_n(estim)/len(devices)

19
20 if __name__ == ’__main__’:

21

22

23

24

n = 10000
devices = [’/gpu:0’, ’/gpu:1’, ’/gpu:2’, ’/gpu:3’]
r = mc_pi(n, devices)
print(r.numpy())

Listing C.1: Monte Carlo estimation of π for TensorFlow on a workstation with multiple
GPUs

S. Ko, H. Zhou, J. J. Zhou, and J.-H. Won/High-performance statistical computing

49

Fig D.1: Three selected bands from the NMF of the Pavia University hyperspectral image
with r = 20

in an alternating fashion.

Figure D.1 shows an example of NMF on a publicly available hyperspectral image, ac-
quired by the reﬂective optics system imaging spectrometer sensor in a ﬂight campaign
over Pavia University in Italy16. The image is essentially a 610 (height) × 340 (width) ×
103 (spactral bands) cube. It is interpreted as a 207, 400 (pixels) × 103 (bands) matrix and
then analyzed using NMF. In the resulting 207, 400 × r matrix V , each column can be inter-
preted as a composite channel from the original 103 bands. In the present experiments, the
rank r was set to 20. Three of the twenty channels exhibiting distinct features were chosen
by hand, and are shown in Figure D.1.

A problem with the multiplicative algorithm is the potential to generate subnormal num-
bers, which signiﬁcantly slows down the algorithm. A subnormal (or denormal) number is a
number smaller (in magnitude) than the smallest positive number that can be represented
by the ﬂoating-point number system of the computer. Subnormal numbers are generated by
the multiplicative algorithm if values smaller than 1 are multiplied repeatedly. Indeed, when
Listing D.1 was run on a CPU with a small synthetic data of size 100 × 100, we observed a
signiﬁcant slowdown. The IEEE ﬂoating-point standard is to deal with subnormal numbers
properly with a special hardware or software implementation (?). In many CPUs, the treat-
ment of subnormal numbers relies on software and hence is very slow. Forcing such values
to zero is potentially dangerous because it is prone to the division-by-zero error. In contrast,
Nvidia GPUs support subnormal numbers at a hardware level since the Fermi architecture,
and simple arithmetic operations do not slow down by subnormal numbers (?). Subnormal
numbers can be completely avoided (especially in CPUs) by using the APG method of the
main text.

Distributing the multiplicative algorithm on a large scale machine is relatively straight-
forward (?). An implementation using distmat is given in Listing D.1. The code for the
APG algorithm of the main text is presented in Listing D.2. In both Listings, X is assumed
to be an [m]×p matrix. The resulting matrix V is distributed as an [m]×r matrix, and W is
distributed as an r × [p] matrix. As discussed in Section 5, distributed matrix multiplication

16Data

available

at

http://www.ehu.eus/ccwintco/index.php?title=

HyperspectralRemoteSensingScenes.

S. Ko, H. Zhou, J. J. Zhou, and J.-H. Won/High-performance statistical computing

50

algorithms are automatically selected from Table 1 based on the arguments.

1 import torch
2 import torch.distributed as dist
3 from dist_stat import distmat
4 from dist_stat.distmat import distgen_uniform

5
6 dist.init_process_group(’mpi’)
7 rank = dist.get_rank()
8 size = dist.get_world_size()
9 device = ’cuda:{}’.format(rank) # or simply ’cpu’ for CPU computing
10 if device.startswith(’cuda’): torch.cuda.set_device(rank)

11
12 # initialize X, W, V in a single device: a CPU or a GPU.
13 p = 10000
14 q = 10000
15 r = 20
16 max_iter=10000
17 TType = torch.cuda.FloatTensor if device.startswith(’cuda’) else torch.

DoubleTensor

18 X = distgen_unifrom(p, q, TType=TType) # [p] x q
19 V = distgen_uniform(p, r, TType=TType) # [p] x r
20 W = distgen_uniform(q, r, TType=TType).t() # r x [q]

21
22 for i in range(max_iter):

23

24

25

26

27

28

29

30

31

32

33

34

35

36

37

38

39

# Update V
XWt =
WWt =
VWWt = distmat.mm(V, WWt)
# V = V * XWˆT / VWWˆT elementwise. In-place operation.
V = V.mul_(XWt).div_(VWWt)

distmat.mm(X, W.t()) # Scenario 1
distmat.mm(W, W.t()) # Scenario 5
# Scenario 4

# Update W
VtX
VtV
VtVW = distmat.mm(VtV, W)
W = W.mul_(VtX).div_(VtVW)

= distmat.mm(V.t(), X, out_sizes=W.sizes) # Scenario 7
# Scenario 5
= distmat.mm(V.t(), V)
# Scenario 11

# compute objective value
outer = distmat.mm(V, W)
# Scenario 2
val = ((X - outer)** 2).all_reduce_sum()
print(i, val.item())

Listing D.1: A PyTorch code for multiplicative NMF update on a distributed system.

Table D.1 shows the performance of the two NMF algorithms on a [10, 000] × 10, 000
input matrix with various values of r. For APG, (cid:15) = 0 was used. While the APG algorithm
required more operations per iteration than the multiplicative algorithm, it was faster on
CPUs, because subnormal numbers were avoided. As they do not slow down with subnormal
numbers, each iteration was faster in the multiplicative algorithm on GPUs. In Table D.2,
APG appears to converge slower early on (10,000 iterations), but eventually catches up the
multiplicative algorithm (100,000 iterations) in terms of objective value. As more GPUs
were used, the algorithms sped up. The only exception was with 8 GPUs with r = 60, where
inter-GPU communication overhead dominates the actual computation.

Additional experiments were conducted to see how the value of (cid:15) aﬀects the convergence.

S. Ko, H. Zhou, J. J. Zhou, and J.-H. Won/High-performance statistical computing

51

1 import torch
2 import torch.distributed as dist
3 from dist_stat import distmat
4 from dist_stat.distmat import distgen_uniform

5
6 dist.init_process_group(’mpi’)
7 p = 10000
8 q = 10000
9 r = 20
10 eps = 1e-6
11 max_iter = 10000
12 TType = torch.cuda.FloatTensor if device.startswith(’cuda’) else torch.

DoubleTensor

13 X = distgen_uniform(p, q, TType=TType) # [p] x q
14 V = distgen_uniform(p, r, TType=TType) # [p] x r
15 W = distgen_uniform(q, r, TType=TType).t() # r x [q]

16
17 for i in range(max_iter):

18

19

20

21

22

23

24

25

26

27

28

29

30

31

32

33

34

distmat.mm(X, W.t()) # Scenario 1
distmat.mm(W, W.t()) # Scenario 5
# Scenario 4

XWt =
WWt =
VWWt = distmat.mm(V, WWt)
sigma_k = 1.0/(2*((WWt**2).sum() + eps * r).sqrt())
V = (V * (1.0 - sigma_k * eps) - (VWWt - XWt) * sigma_k).apply( \

torch.clamp, min=0)

= distmat.mm(V.t(), X, out_sizes=W.sizes) # Scenario 7
# Scenario 5
= distmat.mm(V.t(), V)
# Scenario 11

VtX
VtV
VtVW = distmat.mm(VtV, W)
tau_k = 1.0/(2 * ((VtV ** 2).sum() + eps * r).sqrt())
W = (W * (1.0 - tau_k * eps) - (VtVW - VtX) * tau_k).apply( \

torch.clamp, min=0)

outer = distmat.mm(V, W) # Scenario 2
val = ((X - outer) ** 2).all_reduce_sum()
print(i, val.item())

Listing D.2: A PyTorch code for alternating proximal gradient NMF update on a distributed
system.

S. Ko, H. Zhou, J. J. Zhou, and J.-H. Won/High-performance statistical computing

52

Table D.1
Runtime (in seconds) comparisons for NMF on the simulated [10, 000] × 10, 000 data

10,000 iterations

method
Multiplicative

APG
((cid:15) = 0)

r CPU 1 GPU 2 GPUs
93
102
109
97
106
113

655
978
1355
504
783
1062

160
165
168
164
168
174

20
40
60
20
40
60

4 GPUs
62
73
85
66
78
90

8 GPUs
50
72
86
57
77
92

Table D.2
Comparison of objective function values for simulated [10, 000] × 10, 000 data after 10,000 iterations and
100,000 iterations

method
Multiplicative

APG
((cid:15) = 0)

r
20
40
60
20
40
60

10,000 iterations
8.270667E+06
8.210266E+06
8.155084E+06
8.271248E+06
8.210835E+06
8.155841E+06

100,000 iterations
8.270009E+06
8.208682E+06
8.152358E+06
8.270005E+06
8.208452E+06
8.151794E+06

The results are shown in Table D.3. Convergence was faster for higher values of (cid:15). The
number of iterations to convergence in the multiplicative algorithm was larger than the APG
with (cid:15) = 10 for higher-rank decompositions (r = 40 and 60) due to heavier communication
burden.

In addition to “small,” 10,000 × 10,000 and “large-size,” 200,000 × 200,000 datasets used
for the scalability experiment of the main text, we also considered the “mid-size,” 200,000
× 100,000 dataset. The results are shown in Table D.4, complementing Table 3 of the main
text. As in the large-size dataset, the mid-size dataset did not ﬁt in the memory of the eight
GPUs.

D.2. Positron emission tomography

MM algorithm

Recall that the log likelihood of the PET reconstruction problem is

L(λ) =

d
(cid:88)

(cid:104)

i=1

yi log (cid:0)

p
(cid:88)

j=1

eijλj

(cid:1) −

(cid:105)

.

eijλj

p
(cid:88)

j=1

The MM algorithm considered in Zhou et al. (2010, Sect. 3.2) for ridge-penalized maxi-
mum likelihood estimation of the intensity map λ maximizes L(λ) − (µ/2)(cid:107)Dλ(cid:107)2
2 based on
separation of the penalty function by the minorization

(λj − λk)2 ≥ −

1
2

(2λj − λn

j − λn

k )2 −

1
2

(2λk − λn

j − λn

k )2

(recall that each row of D has one +1 and one −1), yielding iteration
j /(cid:0) (cid:88)

zn+1
ij = eijyiλn

eikλn
k

(cid:1)

k

S. Ko, H. Zhou, J. J. Zhou, and J.-H. Won/High-performance statistical computing

53

d
o
h
t
e
m
e
v
i
t
a
c
i
l
p
i
t
l
u
m
e
h
t

d
n
a
G
P
A
n

i

(cid:15)

f
o

s
e
u

l
a
v

t
n
e
r
e
ﬀ
d

i

r
o
f

s
n
o
s
i
r
a
p
m
o
c

e
m

i
t

e
c
n
e
g
r
e
v
n
o
C

:
3
.
D
e
l
b
a
T

s
U
P
G
4

,
0
6
=
r

s
U
P
G
8

,
0
4
=
r

s
U
P
G
8

,
0
2
=
r

n
o
i
t
c
n
u
f

)
s
(

e
m

i
t

s
n
o
i
t
a
r
e
t
i

n
o
i
t
c
n
u
f

)
s
(

e
m

i
t

s
n
o
i
t
a
r
e
t
i

n
o
i
t
c
n
u
f

)
s
(

e
m

i
t

s
n
o
i
t
a
r
e
t
i

d
o
h
t
e

M

6
0
+
E
9
6
7
2
5
1
.
8

6
0
+
E
8
2
2
2
5
1
.
8

6
0
+
E
0
9
8
3
5
1
.
8

6
0
+
E
3
0
5
8
6
1
.
8

6
0
+
E
8
9
9
8
0
3
.
8

6
4
4

6
3
5

7
3
5

0
6
4

8
4
4

0
0
0
0
5

0
0
5
5
5

0
0
5
5
5

0
0
8
7
4

0
0
4
6
4

6
0
+
E
1
3
0
9
0
2
.
8

6
0
+
E
5
7
8
8
0
2
.
8

6
0
+
E
4
2
3
0
1
2
.
8

6
0
+
E
8
0
1
3
2
2
.
8

6
0
+
E
9
5
8
7
4
3
.
8

9
6
2

0
1
3

2
0
3

7
0
3

7
5
2

0
0
6
6
3

0
0
4
7
3

0
0
7
6
3

0
0
3
7
3

0
0
0
1
3

6
0
+
E
0
3
5
0
7
2
.
8

6
0
+
E
2
0
2
0
7
2
.
8

6
0
+
E
5
8
2
4
7
2
.
8

6
0
+
E
6
4
3
2
8
2
.
8

6
0
+
E
8
1
8
9
8
3
.
8

0
1
1

8
9
1

1
9
1

0
9
1

8
7
1

0
0
2
1
2

0
0
5
1
3

0
0
7
0
3

0
0
5
0
3

0
0
0
8
2

e
v
i
t
a
c
i
l

p
i
t
l
u
M

1
.
0
=
(cid:15)
G
P
A

0
1
=
(cid:15)
G
P
A

1
=
(cid:15)
G
P
A

0
=
(cid:15)
G
P
A

.
s
e
c
n
a
t
s
n
i

e
g
r
a
l
x
8
1
.
5
c

2
C
E
S
W
A
n
o

s
n
o
i
t
a
r
e
t
i

0
0
0
1

r
o
f

a
t
a
d

d
e
t
a
l
u
m

i
s

n
o

F
M
N
r
o
f

m
h
t
i
r
o
g
l
a
G
P
A

f
o

e
m

i
t
n
u
R

:
4
.
D
e
l
b
a
T

×

4
7
0
2

3
3
2
1

2
8
0
1

8
8
6

2
8
6

2
9
5

0
6
=
r

8
9
9

6
9
8

1
4
5

0
4
5

9
8
4

×

9
8
5
1

0
4
=
r

3
6
8

1
6
6

8
4
4

2
2
4

3
6
3

×

7
8
4
1

0
2
=
r

0
0
0
,
0
0
1
×
0
0
0
,
0
0
2

1

2

4

5

8

0
1

0
2

s
e
c
n
a
t
s
n

i

n
o
i
t
a
r
u
g
ﬁ
n
o
c

S. Ko, H. Zhou, J. J. Zhou, and J.-H. Won/High-performance statistical computing

54

(a) µ = 0

(b) µ = 10−6

(c) µ = 10−5

(d) µ = 10−4

Fig D.2: Reconstructed images of the XCAT phantom with a ridge penalty.

Table D.5
Convergence time comparisons for TV-penalized PET with diﬀerent values of ρ. Problem dimension is
p = 10, 000 and d = 16, 110. Eight GPUs were used.

ρ
0
0.01
0.1
1

iterations
6400
4900
5000
2800

time (s)
20.6
15.8
16.1
9.5

function
-2.417200E+05
-2.412787E+05
-2.390336E+05
-2.212579E+05

j = µ(cid:0)njλn
bn+1

j +

(cid:88)

gjkλn
k

(cid:1) − 1

k

(cid:16)

λn+1
j =

− bn+1

j − (cid:2)(bn+1

j

)2 − 4aj

d
(cid:88)

i=1

(cid:3)1/2(cid:17)

zn+1
ij

/(2aj),

wherenj is the degree of the pixel j on the pixel grid, and aj = −2µnj. By using matrix
notation, the distmat implementation of this algorithm can be succinctly written as in
Listing D.3.

Figure D.2 shows the results of applying this iteration to the XCAT phantom of the main
text. Images get smooth as the value of µ increases, but the edges get blurry. Compare
the edge contrast with Figure 2 (panels a–d) of the main text, in which anisotropic TV
penalty was used and the estimation was conducted by using the PDHG algorithm. The
distmat implementation of the latter algorithm is given in Listing D.4. Table D.5 shows
the convergence behavior of PDHG with diﬀerent values of penalty parameters. Observe
that the algorithm converges faster for large values of ρ with which the solution gets sparse.
To complete the comparison with Zhou et al. (2010), we present here Figures D.3 and D.4
that show similar results with the p = 64×64 Roland-Varadhan-Frangakis phantom (Roland
et al., 2007) with d = 2016, used in the cited paper, for various values of regularization
parameters µ and ρ.

Stochastic PDHG

The stochastic PDHG algorithm employed to produce the bottom row of Figure 2 is based
on the dual version of iteration (6) (Condat, 2013; Ko et al., 2019):

xn+1 = proxτ g(xn − τ K T yn+1)
yn+1 = proxσf ∗ (yn + σK ¯xn)
¯yn+1 = 2yn+1 − yn.

S. Ko, H. Zhou, J. J. Zhou, and J.-H. Won/High-performance statistical computing

55

1 import torch
2 import torch.distributed as dist
3 from dist_stat import distmat
4 from dist_stat.distmat import distgen_ones

5
6 dist.init_process_group(’mpi’)
7 rank = dist.get_rank()
8 size = dist.get_world_size()
9 device = ’cuda:{}’.format(rank) # or simply ’cpu’ for CPU computing
10 if device.startswith(’cuda’): torch.cuda.set_device(rank)

11
12 # (Data reading part omitted.)
13 # G: adjacency matrix, sparse [p] x p.
14 # E: detection probability matrix, d x [p].
15 # D: difference matrix, #edges x [p].
16 # y: observed count data, d x 1 broadcast.

17
18 TType = torch.cuda.FloatTensor if device.startswith(’cuda’) else torch.

DoubleTensor

19 eps = 1e-20 # a small number for numerical stability
20 lambd = distmat.distgen_ones(G.shape[0], 1).type(TType) # poisson intensity, [p] x

1.

21 mu = 1e-6 # roughness penalty parameter
22 # compute |N_j|, row-sums of G.
23 N = distmat.mm(G, torch.ones(G.shape[1], 1).type(TType)) # [p] x 1.
24 a = -2 * mu * N # [p] x 1.
25 el = distmat.mm(E, lambd) # Scenario 5. output d x 1.
26 gl = distmat.mm(G, lambd) # Scenario 1. output [p] x 1.

27
28 for i in range(max_iter):

29

30

31

32

33

34

35

36

37

38

39

40

41

42

43

= E * y * lambd.t() / (el + eps) # d x [p].

z
b = mu * (N * lambd + gl) -1 # [p] x 1.
c = z.sum(dim=0).t() # [p] x 1.
# update lambda
if mu != 0:

lambd = (-b - (b ** 2 - 4 * a * c).sqrt()) / (2 * a + eps) # [p] x 1.

else:

lambd = -c / (b + eps) # [p] x 1.

el = distmat.mm(E, lambd) # Scenario 5. output d x 1.
gl = distmat.mm(G, lambd) # Scenario 1. output [p] x 1.

likelihood = (y * torch.log(el + eps) - el).sum()
dl = distmat.mm(D, lambd) # Scenario 5. output #edges x 1.
penalty = - mu / 2.0 * torch.sum(dl ** 2)
print(i, likelihood + penalty)

Listing D.3: PyTorch code for PET with a squared diﬀerence penalty.

S. Ko, H. Zhou, J. J. Zhou, and J.-H. Won/High-performance statistical computing

56

1 import torch
2 import torch.distributed as dist
3 from dist_stat import distmat
4 from dist_stat.distmat import distgen_ones
5
6 dist.init_process_group(’mpi’)
7 rank = dist.get_rank()
8 size = dist.get_world_size()
9 device = ’cuda:{}’.format(rank) # or simply ’cpu’ for CPU computing
10 if device.startswith(’cuda’): torch.cuda.set_device(rank)
11
12 # (Data reading part omitted.)
13 # E: detection probability matrix, d x [p].
14 # D: difference matrix, #edges x [p].
15 # y: observed count data, d x 1 broadcast.
16
17 TType = torch.cuda.FloatTensor if device.startswith(’cuda’) else torch.DoubleTensor
18 Et, Dt = E.t(), D.t() # transpose
19
20 lambd = distmat.distgen_ones(E.shape[1], 1).type(TType) # poisson intensity, [p] x 1.
21 lambd_prev = distmat.disten_ones(E.shape[1], 1).type(TType)
22 lambd_tilde = distmat.disten_ones(E.shape[1], 1).type(TType)
23
24 eps = 1e-20 # a small number for numerical stability
25 rho = 1e-4 # penalty parameter
26 tau = 1/3 # primal step size
27 sig = 1/3 # dual step size
28
29 w = torch.zeros(D.shape[0], 1).type(TType) # #edges x 1.
30 z = - torch.ones(E.shape[0], 1).type(TType) # d x 1.
31 Et1 = - distmat.mm(Et, z) # Scenario 4, [p] x 1.
32
33 for i in range(max_iter):
# dual update
34
el = distmat.mm(E, lambd_tilde) # Scenario 5, d x 1.
z = z + sig * el # d x 1
tmp = (z ** 2 + 4 * sig * y).sqrt() # d x 1
z = 0.5 * (z - tmp) # d x 1
dl = distmat.mm(D, lambd_tilde) # Scenario 5, #edges x 1.
w = w + sig * dl # #edges x 1
w = torch.clamp(w, max=rho, min=-rho) # #edges x 1

37

38

35

36

39

41

40

42

43

44

45

46

47

48

49

50

51

52

53

54

55

56

# primal update
Etz = distmat.mm(Et, z) # Scenario 4, [p] x 1.
Dtw = distmat.mm(Dt, w) # Scenario 4, [p] x 1.
lambd_prev = lambd
lambd = lambd - tau * (Etz + Dtw + Et1) # [p] x 1.
lambd = lambd.apply(torch.clamp, min=0.0) # [p] x 1.
lambd_tilde = 2 * lambd - lambd_prev

# objective
el = distmat.mm(E, lambd) # Scenario 5, d x 1.
dl = distmat.mm(D, lambd) # Scenario 5, #edges x 1.
likelihood = (y * torch.log(el + eps) - el).sum()
penalty = - rho * torch.sum(dl.abs())
print(i, likelihood + penalty)

Listing D.4: PyTorch code for PET with absolute value penalty.

S. Ko, H. Zhou, J. J. Zhou, and J.-H. Won/High-performance statistical computing

57

If we take f (z, w) = (cid:80)d
1 , . . . , ET
K = [ET
iteration reduces to

i=1 fi(zi) + ρ(cid:107)w(cid:107)1 with fi(z) = −yi log z, g(λ) = 1T Eλ + δ+(λ), and
d , DT ]T where Ei is the ith row of the mixing matrix E, then the above

= (cid:2)(zn

λn+1 = P+(λn − τ (ET ¯zn + DT ¯wn + ET 1))
zn+1
i + σEiλn+1) − [(zn
i
wn+1 = P[−ρ,ρ](wn + σDλn+1)
¯zn+1 = 2zn+1 − zn
¯wn+1 = 2wn+1 − wn.

i + σEiλn+1)2 + 4σyi]1/2(cid:3)/2,

i = 1, . . . , d

Chambolle et al. (2018) propose to replace the ˜z-update by sampling and then debias the
z-update accordingly. If uniform sampling is used, then the second and fourth lines are
replaced by:

zn+1
i

=

(cid:26)(cid:2)(zn
˜zn
i ,

i + σEiλn+1) − [(zn

i + σEiλn+1)2 + 4σyi]1/2(cid:3)/2, with probability π

otherwise

¯zn+1 = zn+1 + π−1(zn+1 − zn).

In the experiments for Figure 2 (panels e–h), π=0.2 was used. That is, only 20% of the
coordinates were updated every iteration.

D.3. Multidimensional scaling

The parallel MM iteration of the main text

ik = (cid:0) (cid:88)
θn+1

(cid:2)yij

j(cid:54)=i

θn
ik−θn
jk
i −θn
(cid:107)θn
j (cid:107)2

+ (θn

ik + θn

jk)(cid:3)(cid:1)(cid:14)(cid:0)2 (cid:80)

j(cid:54)=i wij

(cid:1)

is derived from minimizing the surrogate function

g(θ|θn) = 2

q
(cid:88)

(cid:88)

i=1

j(cid:54)=i

(cid:34)

wij

(cid:13)
(cid:13)
(cid:13)
(cid:13)

θi −

1
2

(θn

i + θn
j )

(cid:13)
2
(cid:13)
(cid:13)
(cid:13)
2

−

i − θn
wijyij(θi)T (θn
j )
i − θn
(cid:107)θn
j (cid:107)2

(cid:35)

.

that majorizes the stress function f (θ) = (cid:80)q

(cid:80)

i=1

j(cid:54)=i wij(yij − (cid:107)θi − θj(cid:107)2)2

In PyTorch syntax, this can be computed in parallel using the code in Listing D.5, when all
the data points are equally weighted. In the numerical experiments of Sect. 6.2 of the main
text, the pairwise Euclidean distances between data points were computed distributedly
(?): in each stage, data on one of the processors are broadcast and each processor computes
pairwise distances between the data residing on its memory and the broadcast data. This
is repeated until all the processors broadcast its data. Note that the dimension of the
datapoints does not matter after computing the pairwise distances.

Appendix E: Details of SNPs selected in (cid:96)1-regularized Cox regression

Figure E.1 shows the solution path for SNPs within the range we used for the experiment
in Section 6.4. Tables E.1 and E.2 list the 111 selected SNPs.

S. Ko, H. Zhou, J. J. Zhou, and J.-H. Won/High-performance statistical computing

58

Table E.1
SNPs selected by (cid:96)1-penalized Cox regression: #1-#56

Yes

Yes

Yes
Yes

Yes
Yes

SLC45A2

Location A1B

0.401 CSK, MIR4513
0.416
0.406
0.316

ChrA
10
10
5
10
15
5
15
5
10
15
15
15
15
7
7
11
11
15
5
7
9
2
2
15
9
2
17
3
15
5
5
3
10
9
15
15
9
13
12
9
1
15
15
9
1
9
10
17
5
3
7
9
16
8
5
1

Rank
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
34
35
36
37
38
39
40
41
42
43
44
45
46
47
48
49
50
51
52
53
54
55
56

114756041 A
114788815 C
33951693 G
114808902 T
28365618 G
33958959 C
28356859 C
33951116 T
114781297 C
75047426 T
75044238 A
75077367 A
75101530 T
28223990 C
28180556 C
71167449 T
71168073 G
91521337 A
33969628 T
28189411 T
16659863 T
136407479 G
135837906 A

SNP ID
rs4506565
rs12243326
rs16891982
rs12255372
rs12913832
rs28777
rs1129038
rs35397
rs10787472
rs2470890
rs2472304
rs1378942
rs34862454
rs849335
rs864745
rs12785878
rs4944958
rs8042680
rs35414
rs1635852
rs10962525
rs1446585
rs7570971
rs36074798
rs10962612
rs10962612
rs941444
rs6769511
rs916977
rs35390
rs35391
rs1470579
rs2862954
rs2297174
rs1667394
rs12440952
rs56343038
rs9522149
rs343092
rs10733316
rs823485
rs12910825
rs2959005
rs10756801
rs12072073
rs7039444
rs7899137
rs11078405
rs830532
rs833283
rs10274928
rs13301628
rs885107
rs8180897
rs23282
rs6428460

SignE KnownF
A2C MAFD Mapped genes
+
0.238 TCF7L2
T
+
0.249 TCF7L2
T
−
0.215
C
SLC45A2
+
0.215 TCF7L2
G
−
0.198 HERC2
A
−
0.223
A
−
T
0.343 HERC2
−
SLC45A2
G 0.304
+
0.430 TCF7L2
A
−
C
0.429 CYP1A2
−
G 0.460 CYP1A2
−
C
−
LMAN1L
C
−
JAZF1, JAZF1-AS1
T
−
T
JAZF1
−
G 0.251 NADSYN1, DHCR7
−
0.237 NADSYN1, DHCR7
A
PRC1, PRC1-AS1, Y RNA +
0.277
C
−
SLC45A2
0.188
C
−
0.423
C
JAZF1
+
0.321 BNC2
C
+
0.322 R3HDM1
A
+
0.327 RAB3GAP1
C
PRC1, PRC1-AS1, Y RNA +
0.328
91518800 ACT A
−
0.088 BNC2
T
16804167 G
+
0.097 RAB3GAP1, ZRANB3
C
135911422 T
−
0.073 RAI1
G
17693891 C
+
0.045
C
185530290 T
−
0.044 HERC2
C
28513364 T
−
0.062
A
33955326 C
−
0.374
C
33955673 T
+
0.436
C
185529080 A
−
C
101912064 T
0.488
−
G 0.346 BNC2
16706557 A
−
C
28530182 T
0.274 HERC2
+
A
74615292 G
0.279 CCDC33
−
T
16776792 G
0.318 BNC2, LSM1P1
−
C
111827167 T
0.395 ARHGEF7
−
G
66250940 T
0.463 HMGA2, HMGA2-AS1
−
C
16696626 T
0.436 BNC2
+
LINC01354
0.488
C
234671267 T
PRC1, PRC1-AS1, RCCD1 +
G 0.384
91511260 A
−
C
74618128 T
−
G 0.494 BNC2
16740110 T
0.497
+
T
3130016 C
0.360
+
C
20253425 T
0.289 KAT6B
−
C
76668462 A
0.291 TOM1L2
+
G
17824978 T
0.333 ARHGAP26
+
T
142289541 C
(intergenic variant)
0.352
−
C
181590598 G
0.365
JAZF1
−
G
28142088 A
0.412 BNC2
−
C
16665850 A
+
T
30672719 C
0.353
+
G 0.445
121699907 A
+
A
142270301 G
+
T
198377460 C
A Chromosome, B Minor allele, C Major allele, D Minor allele frequency,
E Sign of the regression coeﬃcient, F Mapped gene included in Mahajan et al. (2018). The boldface indicates the
risk allele determined by the reference allele and the sign of the regression coeﬃcient.

0.225 ARHGAP26
0.229

SLC45A2
SLC45A2
IGF2BP2
ERLIN1

PRDM16
(intergenic variant)

PRR14, FBRS
SNTB1

(intergenic variant)

0.222 CCDC33

IGF2BP2

Yes
Yes

Yes

Yes

Yes

Yes

Yes

S. Ko, H. Zhou, J. J. Zhou, and J.-H. Won/High-performance statistical computing

59

Rank
57
58
59
60
61
62
63
64
65
66
67
68
69
70
71
72
73
74
75
76
77
78
79
80
81
82
83
84
85
86
87
88
89
90
91
92
93
94
95
96
97
98
99
100
101
102
103
104
105
106
107
108
109
110
111

SNP ID
rs11630918
rs7187359
rs2183405
rs2651888
rs2189965
rs12911254
rs757729
rs6495122
rs4944044
rs6856032
rs1375132
rs2451138
rs6430538
rs7651090
rs4918711
rs3861922
rs7917983
rs1781145
rs7170174
rs7164916
rs696859
rs28052
rs1408799
rs10941112
rs11856835
rs4768617
rs8012970
rs4402960
rs1695824
rs934886
rs7083429
rs4918788
rs7219320
rs61822626
rs250414
rs11073964
rs17729876
rs2386584
rs683
rs17344537
rs10416717
rs2644590
rs447923
rs2842895
rs231354
rs4959424
rs2153271
rs12142199
rs2733833
rs1564782
rs9268644
rs271738
rs12907898
rs146900823
rs1635166

ChrA
15
16
9
1
7
15
7
15
11
4
2
8
2
3
10
1
10
1
15
15
1
5
9
5
15
12
14
3
1
15
10
10
17
1
5
15
10
15
9
1
19
1
5
6
11
6
9
1
9
15
6
1
15
3
15

Yes

Yes

Yes

Yes

(intergenic variant)

(intergenic variant)

ZRANB3
SAMD12

SCAMP2
(intergenic variant)

IGF2BP2
(intergenic variant)

0.481 TCF7L2
0.362 ATAD3C
0.246 AC091078.1
0.246 VPS33B, VPS33B-DT
0.430

Table E.2
SNPs selected by (cid:96)1-penalized Cox regression: #57-#111
Location A1B A2C MAFD Mapped genes
75155896 C
30703155 G
16661933 G
3143384 G
28172014 T
75166335 A
28146305 G
75125645 C
71120213 A
38763994 G
135954405 G
119238473 T
135539967 T
185513392 G
113850019 T
198210570 A
114732882 T
1388289 A
94090333 T
91561446 T
234656596 T
142279870 C
12672097 T
34004707 C
74716174 G
45850022 T
101168491 T
185511687 G
1365570 A
55939959 A
69303421 G
114820961 G
17880877 A
205118441 C
33990623 C
91543761 C
101999746 G
91539572 T
12709305 C
205091427 T
13521528 A
156875107 C
142252257 T
7106316 C
2706351 C
7084857 T
16864521 T
1249187 A
12705095 T
74622678 A
32408044 C
234662890 A
75207872 T

0.383
T
0.335
A
0.271 BNC2
A
PRDM16
0.411
T
JAZF1
C
0.340
SCAMP2
G 0.344
0.441
C
JAZF1
0.478 CPLX3, ULK3
A
G 0.426 AP002387.1
0.109 RNA5SP158
C
0.478
A
0.314
C
0.470 CCNT2-AS1
C
0.281
A
C
0.285
G 0.466 NEK7
C
C
C
C
C
G 0.166 ARHGAP26
C
T
A
C
C
T
C
G
T
A
G 0.318 DRC3, AC087163.1, ATPAF2
T
T
T
A
G 0.360 VPS33B, PRC1
A
G
G 0.470 CACNA1A
A
C
G 0.331 RREB1
T
G 0.410
C
G 0.398
G
G 0.283 CCDC33
A
G 0.395
0.391 COX5A
C
149192851 GC G 0.344 TM4SF4
C

SignE KnownF
−
+
+
+
+
−
−
−
−
+
+
−
+
+
−
−
+
+
−
+
+
+
−
−
−
−
−
+
+
−
+
+
+
−
−
+
−
+
−
−
+
−
+
−
+
−
−
INTS11, PUSL1, ACAP3, MIR6727 −
−
−
+
+
−
−
−

0.277
0.355 AMACR, C1QTNF3-AMACR
0.261
0.259
0.179
0.187
0.164
0.360
0.367 CTNNA3
0.348 TCF7L2

0.478 DSTYK
0.361 AMACR, C1QTNF3-AMACR
0.362 VPS33B,PRC1
0.352 CWF19L1, SNORA12

SEMA7A
(intergenic variant)
(intergenic variant)
IGF2BP2
LINC01770, VWA1
PRTG

0.453
0.384 ARHGAP26, ARHGAP26-AS1

0.430 TYRP1, LURAP1L-AS1
0.462 RBBP5

0.282 HLA-DRA
LINC01354

0.272 TYRP1, LURAP1L-AS1

0.329 KCNQ1, KCNQ1OT1

(intergenic variant)

0.118 HERC2

0.411 BNC2

28539834 T

PEAR1

Yes
Yes

Yes

Yes

Yes

Yes

Yes

A Chromosome, B Minor allele, C Major allele, D Minor allele frequency,
E Sign of the regression coeﬃcient, F Mapped gene included in Mahajan et al. (2018). The boldface indicates the risk
allele determined by the reference allele and the sign of the regression coeﬃcient.

S. Ko, H. Zhou, J. J. Zhou, and J.-H. Won/High-performance statistical computing

60

(a) µ = 0

(b) µ = 10−7

(c) µ = 10−6

(d) µ = 10−5

Fig D.3: Reconstructed images of the RVF phantom with a ridge penalty.

(a) ρ = 2−10

(b) ρ = 2−8

(c) ρ = 2−6

(d) ρ = 2−4

Fig D.4: Reconstructed images of the RVF phantom with a TV penalty.

S. Ko, H. Zhou, J. J. Zhou, and J.-H. Won/High-performance statistical computing

61

1 import torch
2 import torch.distributed as dist
3 from dist_stat import distmat
4 from dist_stat.distmat import distgen_normal, distgen_uniform
5 from dist_stat.application.euclidean_distance import euclidean_distance_DistMat
6 from math import inf

7
8 dist.init_process_group(’mpi’)
9 rank = dist.get_rank()
10 size = dist.get_world_size()
11 device = ’cuda:{}’.format(rank) # or simply ’cpu’ for CPU computing
12 if device.startswith(’cuda’): torch.cuda.set_device(rank)

13
14 m = 10000
15 d = 10000
16 q = 20
17 max_iter = 10000
18 TType = torch.cuda.FloatTensor if device.startswith(’cuda’) else torch.

DoubleTensor

19 X = distgen_normal(m, d, TType=TType) # [m] x d
20 y = euclidean_distance_DistMat(X).type(TType) # [m] x m
21 w_sums = float(m - 1.0)
22 theta = distgen_uniform(m, q, lo=-1, hi=1, TType=TType) # [m] x q

23
24 for i in range(max_iter):

25

26

27

28

29

30

31

32

33

34

35

36

37

38

39

40

41

42

d = distmat.mm(theta, theta.t()) # Scenario 2.
TtT_diag_nd = d.diag(distribute=False).view(1, -1) # local
TtT_diag
d = d.mul_(-2.0)
d.add_(TtT_diag_nd)
d.add_(TtT_diag)

= d.diag(distribute=True) # distributed col vector

d.fill_diag_(inf)
Z = distmat.div(y, d)
Z_sums = Z.sum(dim=1) # length-q broadcast vector
WmZ = distmat.sub(1.0, Z)
WmZ.fill_diag_(0.0)
TWmZ = distmat.mm(theta.t(), WmZ.t()) # Scenario 8
theta = (theta * (w_sums + Z_sums) + TWmZ.t()) / (w_sums * 2.0)

distances = euclidean_distance_DistMat(theta)
obj = ((y - distances)**2).sum() / 2.0
print(i, obj.item())

Listing D.5: PyTorch code for MDS.

S. Ko, H. Zhou, J. J. Zhou, and J.-H. Won/High-performance statistical computing

62

Fig E.1: Solution path for (cid:96)1-regularized Cox regression on the UK Biobank dataset. Signs
are with respect to the reference allele: positive value favors alternative allele as the risk
allele.

0.50.60.70.80.91.0-1.0×10-5-5.0×10-605.0×10-61.0×10-51.5×10-5regularizer (relative value)coefficientrs4506565rs12243326rs16891982rs12255372rs12913832rs28777rs1129038rs35397rs10787472