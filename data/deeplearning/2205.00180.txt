2
2
0
2

n
u
J

2

]
E
S
.
s
c
[

2
v
0
8
1
0
0
.
5
0
2
2
:
v
i
X
r
a

Katana: Dual Slicing-Based Context for Learning Bug Fixes

MIFTA SINTAHA, The University of British Columbia, Canada
NOOR NASHID, The University of British Columbia, Canada
ALI MESBAH, The University of British Columbia, Canada

Contextual information plays a vital role for software developers when understanding and fixing a bug.
Consequently, deep learning-based program repair techniques leverage context for bug fixes. However,
existing techniques treat context in an arbitrary manner, by extracting code in close proximity of the buggy
statement within the enclosing file, class, or method, without any analysis to find actual relations with the
bug. To reduce noise, they use a predefined maximum limit on the number of tokens to be used as context.
We present a program slicing-based approach, in which instead of arbitrarily including code as context, we
analyze statements that have a control or data dependency on the buggy statement. We propose a novel
concept called dual slicing, which leverages the context of both buggy and fixed versions of the code to capture
relevant repair ingredients. We present our technique and tool called Katana, the first to apply slicing-based
context for a program repair task. The results show Katana effectively preserves sufficient information for a
model to choose contextual information while reducing noise. We compare against four recent state-of-the-art
context-aware program repair techniques. Our results show Katana fixes between 1.5 to 3.7 times more bugs
than existing techniques.

CCS Concepts: • Software and its engineering → Software maintenance tools.

Additional Key Words and Phrases: program slicing, program repair, deep learning, contextual information,
graph neural networks

1 INTRODUCTION
Traditional automated error detection [45] and program repair [8, 27, 30] techniques rely on a set
of predefined templates and rules that are limited to specific software bug patterns; adding support
for a new type of bug is manual and requires domain-specific knowledge in a given programming
language. Instead of hard-coding error detection and repair patterns, we can automatically learn
them from code examples of developer made mistakes and repairs, through deep learning [10, 15,
32, 38, 39].

To apply learning-based techniques for software analysis, the source code needs to be vectorized.
It is, however, imperative to first delineate what information in the source code should be included
as input for vectorization. While syntactical representation of source code for vectorization has
gained traction in the literature in recent years, semantic information has received less attention.
In particular, the notion of context pertaining to an erroneous statement in the code has been
largely treated in an ad-hoc and arbitrary fashion. For instance, some techniques merely focus on
the buggy statement [19, 43] and ignore context. Others use the enclosing file [11, 18], enclosing
class [10], enclosing function [38, 51], or encapsulating AST subtrees [32] as context, often with a
hard-coded bounding limit on the number of tokens.

Arbitrarily including code as context neither captures the true semantics of the buggy statement
nor encompasses essential fix-ingredients. For developers, context plays a significant role in under-
standing a bug and determining a potential fix. This is true in case of machine learning models,
where too much information in the input introduces noise that can affect the repair accuracy of

Authors’ addresses: Mifta Sintaha, The University of British Columbia, Vancouver, Canada, msintaha@ece.ubc.ca; Noor
Nashid, The University of British Columbia, Vancouver, Canada, nashid@ece.ubc.ca; Ali Mesbah, The University of British
Columbia, Vancouver, Canada, amesbah@ece.ubc.ca.

2022. 1049-331X/2022/6-ART $15.00
https://doi.org/10.1145/nnnnnnn.nnnnnnn

ACM Trans. Softw. Eng. Methodol., Vol. 1, No. 1, Article . Publication date: June 2022.

 
 
 
 
 
 
the model [16] and too little information can lead to overfitting or relevant data loss, leading to
incorrect generation of patches. This poses questions pertaining to what is relevant context, how
to collect it, what role the repair ingredients should play, and how relevant information related to
both the erroneous and fixed code should be represented for deep learning consumption.

Finding adequate context ingredients is vital to overcoming this limitation. In fact, when a
developer tries to fix a buggy line of code, they start from the buggy line and examine all the
ingredients, such as the variables and function calls used in that line. They then go backward in the
code, investigating where such ingredients have been defined, used, and modified throughout the
code to understand the error. Our insight is that such relevant information can help a deep learning
approach better reason about bug fixes. In particular, we propose to adopt backward slicing analysis
for extracting contextual information in the form of code directly related to a buggy statement as
well as its repaired ingredients, which we call dual slicing in this paper. Our approach, implemented
in a tool called Katana, extracts dual slicing-based context from the buggy and fixed files through
inter and intraprocedural control and data flow analysis, transforms the slices to AST-based graph
representations, and uses a Graph Neural Network (GNN) to train a model. To evaluate our dual
slicing approach, we compare against four state-of-the-art repair techniques, and our results show
that Katana outperforms all four.

Our work makes the following contributions:

• A technique for extracting contextual information to enhance deep learning program repair.
Our program slicing uses control and data flow analysis to find code that is relevant and
applies the notion of dual slicing to capture context from both the buggy and fixed code. It
requires no hard-coded bound on the number of tokens to limit the scope of the context.
• An evaluation of our dual slicing technique and comparison with four state-of-the-art
learning-based program repair techniques. We train models on 91,181 pairs of buggy and
fixed JavaScript files. Katana achieves 42% ( 4,781
) repair accuracy within top-3 predictions,
11,397
which is between 1.5 to 3.7 times higher than existing techniques.

• A quantitative analysis of the type and amount of information available for a learning model
as well as a qualitative report on the type of bugs that Katana can fix; 891 bugs are exclusively
fixed by Katana.

• The implementation of our technique, Katana, which is available [4]; it includes our dataset,

model and JavaScript program slicer, compatible with the latest JavaScript ES10.

2 MOTIVATION
Context is essentially the background information related to a particular problem depending on the
domain. The importance of context has been discussed in natural language processing tasks [40]
for establishing semantic similarity between words. For a human developer, context is key in
understanding and creating a fix for a buggy program. Therefore, different types of context in both
machine learning and rule-based approaches have been applied for fixing a bug [10, 32, 38, 55]. In
case of a program repair, the context is typically considered as the surrounding source code relevant
to the buggy statement. Current state-of-the-art learning based program repair approaches employ
context in various ways and many of them use sequence-based and tree/graph based source code
representation for learning program repair. These approaches use the buggy line with enclosing
class [10], enclosing function [38, 51], buggy subtree [32], or the whole file [11] for capturing the
context of a buggy line. However, all these approaches have a bounding limit for processing the
buggy line. The sequence-based approaches use a maximum token limit (e.g., 1,000 tokens [10]),
within which they extract buggy methods or classes to avoid truncated sequences. Similarly, current
graph-based learning techniques use a maximum node limit (e.g., 500 nodes [11]) for program

2

repair. In practice, we cannot expect real world bugs to always be enclosed within small/medium
sized methods or be limited to an arbitrary number of tokens or AST nodes. Therefore, this kind
of quantitative constraint can potentially cause some important information pertaining to the
bug fixes to be truncated and the overall context may even lose semantic meaning. In addition,
increasing the limit on the number of tokens or nodes may add noise to the model.

1 'use strict';
2 import ENV from 'pass-ember/config/environment';

return {

3 import { get } from '@ember/object';
4 import CheckSessionRoute from '../../check-session-route';
5 function service(user) {
6
7
8
9
10
11 }
12 const user = get('currentUser.user');
13 export default CheckSessionRoute.extend({
14 -
15 +

...user,
userToken: get('currentUser.accessKey'),
userSecret:

currentUser: service(),
currentUser: service(user),

get('currentUser.userSecret'),

};

model() {

...

}

...

15
16
17

30

66 });

Listing 1. Sliced program with missing function argument

We will use Listing 1, which shows a real bug and its fix for a JavaScript project on GitHub, as a
running example. This type of bug is common in many languages and falls under Same function
more args pattern as categorized by Karampatsis and Sutton [24]. Let us consider how a developer
would go about fixing the bug here. After localizing the faulty line of code, they would try to
determine the root cause of the bug. To do so, they would analyze how the variables or functions
in the faulty line have been affected in the previous lines. The developer notices that the property
currentUser is invoking a function call to service. After examining the function declaration,
they notice that service expects a required user argument and that the buggy line is not passing
any arguments. The developer identifies the cause of the fault and generates a fix by passing the
missing argument, user declared in line 12, to the function call.

Having too large of a context, especially if the method/class enclosing the buggy line is huge or
has many dependencies can cause significant overhead [28]. By limiting the scope to only lines
relevant to the fault, the developer can avoid information overload during the process of debugging
and repairing the program.

In Listing 1, the highlighted code indicates the portion relevant for the developer to understand
and fix the bug in line 14. The original buggy file in this example contained 66 lines of code and by
focusing only on the relevant lines, the attention decreases to 13 lines. In practice, these kinds of
bugs are pertinent in much more complex settings containing many lines and functions with more
arguments. Furthermore, if we were to follow the approach of some of the current deep learning
models, and extract only the enclosed method or enclosed class of the buggy line, then we would
lose meaningful context outside the enclosing scope like the user variable and service function
that were necessary for the developer in generating the correct fix.

Our insight is that for context, instead of constraining the attention to the enclosing entities or
arbitrary limitations on the number of tokens/nodes, we can constrain the code to those portions
that are relevant to the bug and fix through the notion of program slicing.

3

3 APPROACH

Fig. 1. Overview of our approach.

We hypothesize that slicing-based context can be helpful to a machine learning model that
learns patterns of bug fixes. Our technique, called Katana, is based on static program slicing to
obtain relevant context with respect to the bug and its fix. This obtained slice-based context is then
represented as an AST-based graph augmented with control and data flow edges. This graph is
then fed into a Graph Neural Network (GNN) for training. Figure 1 depicts the overview of our
approach. Our overall approach consists of five main steps, namely, data collection, program slicing,
graph representation, training, and inference. Next, we describe each step.

3.1 Data Collection
To collect data, we chose JavaScript as the target language. According to recent StackOverflow
Developer Surveys [2], JavaScript stands as the most commonly used language in the world as
reported by 68.62% of professional developers. JavaScript was initially limited to client-side code
running in the web browser but has gained popularity in backend development through the
emergence of the Node.js1 framework. Unlike statically typed languages such as Java or C# where
many errors can be discovered at compile time, JavaScript tends to "swallow” errors and silently
continue execution. Therefore, having a good program repair tool for a dynamically typed language
such as JavaScript can help developers with a seamless coding experience.

We collected pairs of buggy and fixed JavaScript files by crawling commits of open-source
JavaScript repositories hosted on GitHub. We narrowed down the search by focusing on commits
containing the keywords "bug", "fix" and "resolve". We collected data from 1,096 github projects.
We exclude duplicates from the test dataset, as well as any data points that appear in both the
test and training datasets [6]. Additionally, we use heuristics to filter out commits that might be
feature additions or refactorings based on the number of AST differences. To this end, we consider a
difference of one AST node to be a bug, similar to existing work [11]. We exclude minified JavaScript
files since our program analysis tool cannot handle those. We collected a total of 113,975 datapoints,
i.e., pairs of buggy and fixed JavaScript files having one AST node difference between the pairs.

1https://nodejs.org

4

Buggy FilesFixed Files1253500...1253500...Program SlicerData CollectionStatic Program Slicingimport { get } from '@ember/object';import CheckSessionRoute from '../../check-session-route';function service(user) {    return { ... };}export default CheckSessionRoute.extend({    currentUser: service(),});  import { get } from '@ember/object';import CheckSessionRoute from '../../check-session-route';function service(user) {    return { ... };}const user = get('currentUser.user');export default CheckSessionRoute.extend({    currentUser: service(user),}); Sliced Buggy FilesSliced Fixed FilesData CollectorCode2Graph GeneratorModuleImpDeclrExportDeclrImpDeclrFuncDeclrget@ember/objectCheckSessionRoute../check-session-routeserviceusercurrentUserModuleImpDeclrExportDeclrImpDeclrFuncDeclrget@ember/objectCheckSessionRoute../check-session-routeserviceusercurrentUserVarDeclruserData Splitting  GNN Model          Node EmbeddingsGraph EmbeddingVarDeclruserImpDeclrGraph Representation80% Training SetBuggy GraphsFixed GraphsValue LinkAST EdgeSucc LinkGNN ModelBuggy GraphsPatch 1Patch 2Patch 3Top k patchesInferenceEvaluateTraining3.2 Static Program Slicing
We propose to use static analysis to examine the buggy line and all the ingredients, such as the
variables and function calls used in that line, and backward program slicing [54], to determine
where such ingredients have been defined, used, and modified throughout the code. We use this
backward slice as context in our work, which is a subset of all statements with data or control
dependencies with respect to the slicing criterion (𝑝, 𝑉 ), which is the set of variables 𝑉 at the buggy
program point 𝑝.

Algorithm 1 Extracting Backward Sliced Context

foreach 𝑟𝑒 𝑓 ∈ 𝑒𝑛𝑡 .𝑟𝑒 𝑓 𝑠 () do

BackwardSlice(𝑒𝑛𝑡𝑖𝑡𝑦, 𝑐𝑜𝑛𝑡𝑒𝑥𝑡𝐿𝑖𝑛𝑒𝑠)

Input: 𝑁 : A given line number; 𝑓 𝑖𝑙𝑒: A given file
Output: List: String: List of statements
1: 𝑒𝑛𝑡𝑖𝑡𝑦𝑆𝑒𝑡 ← 𝑔𝑒𝑡𝐸𝑛𝑡𝑖𝑡𝑖𝑒𝑠 (𝑁 , 𝑓 𝑖𝑙𝑒)
2: 𝑐𝑜𝑛𝑡𝑒𝑥𝑡𝐿𝑖𝑛𝑒𝑠 ← {𝑁 }
3: foreach 𝑒𝑛𝑡𝑖𝑡𝑦 ∈ 𝑒𝑛𝑡𝑖𝑡𝑦𝑆𝑒𝑡 do
4:
5: end for
6: return 𝑔𝑒𝑡𝑆𝑡𝑎𝑡𝑒𝑚𝑒𝑛𝑡𝑠 (𝑐𝑜𝑛𝑡𝑒𝑥𝑡𝐿𝑖𝑛𝑒𝑠)
7: function BackwardSlice(𝑒𝑛𝑡, 𝑐𝑜𝑛𝑡𝑒𝑥𝑡𝐿𝑖𝑛𝑒𝑠)
8:
9:
10:
11:
12:
13:
14:
15:
16:
17:
18: end function

𝑁 ← 𝑟𝑒 𝑓 .𝑙𝑖𝑛𝑒 ()
𝑐𝑜𝑛𝑡𝑒𝑥𝑡𝐿𝑖𝑛𝑒𝑠 ← 𝑐𝑜𝑛𝑡𝑒𝑥𝑡𝐿𝑖𝑛𝑒𝑠 ∪ 𝑁
𝑒𝑛𝑡𝑖𝑡𝑦𝑆𝑒𝑡 ← 𝑔𝑒𝑡𝐸𝑛𝑡𝑖𝑡𝑖𝑒𝑠 (𝑁 , 𝑓 𝑖𝑙𝑒)
foreach 𝑒𝑛𝑡𝑖𝑡𝑦 ∈ 𝑒𝑛𝑡𝑖𝑡𝑦𝑆𝑒𝑡 do

end for

end for

end if

BackwardSlice(𝑒𝑛𝑡𝑖𝑡𝑦, 𝑐𝑜𝑛𝑡𝑒𝑥𝑡𝐿𝑖𝑛𝑒𝑠)

if 𝑟𝑒 𝑓 .𝑒𝑛𝑡 () is control-dependent or data-dependent on 𝑒𝑛𝑡 then

There are two types of program slicing analysis, namely, inter-procedural and intra-procedural
analysis. Intra-procedural analysis is performed within the scope of the function and inter-procedural
analysis is performed within the whole program. We opted for performing both inter-procedural
and intra-procedural analysis but limiting the slicing scope within the JavaScript file. The reason
for such a choice is that only 12% (13,964) of the 113,975 datapoints have the buggy line enclosed
within a function and 3% (22,940) have the buggy line as part of a class. In 85% (97,071), the buggy
line is part of the global scope. In JavaScript, the global code can access all the constructs within a
function, and the global code supports stepwise execution, just like functions do. This is unlike
languages such as Java where the main method is required as the entry point for code execution.
For this reason, we go beyond intra-procedural program analysis to capture a broad spectrum of
datapoints.

We analyze each datapoint in our dataset to collect the necessary information to conduct the
slicing analysis. In order to determine the slicing criterion, we first perform a diff analysis between
the buggy and fixed lines to obtain the buggy line number. We then feed this line number to our
slicing framework and extract the variables and function calls used in that line. For extracting the
contextual statements through static backward slicing, we incorporate both control flow and data
flow analysis and identify the sliced statements where applicable.

Algorithm 1 illustrates a high-level description of the steps used in extracting the context from a
given line number, 𝑁 and 𝑓 𝑖𝑙𝑒. Specifically, Algorithm 1 extracts the entities (i.e. variables, functions,

5

object properties) from the given buggy or fixed line in a file. Algorithm 1 then iterates through
all the entities to obtain all the 𝑐𝑜𝑛𝑡𝑒𝑥𝑡𝐿𝑖𝑛𝑒𝑠 using backward slicing analysis. 𝑐𝑜𝑛𝑡𝑒𝑥𝑡𝐿𝑖𝑛𝑒𝑠 is a
set of line numbers of the sliced context. In the 𝐵𝑎𝑐𝑘𝑤𝑎𝑟𝑑𝑆𝑙𝑖𝑐𝑒 function, Algorithm 1 iterates
through all the references in a given entity; if the referenced entity is control or data-dependent,
then the corresponding line is added to 𝑐𝑜𝑛𝑡𝑒𝑥𝑡𝐿𝑖𝑛𝑒𝑠; this process continues recursively to obtain
the backward slice of the referenced entities. Finally, the algorithm constructs the source code
statements from the 𝑐𝑜𝑛𝑡𝑒𝑥𝑡𝐿𝑖𝑛𝑒𝑠 to return the output. Here, the 𝑔𝑒𝑡𝑆𝑡𝑎𝑡𝑒𝑚𝑒𝑛𝑡𝑠 function ensures
that all the closures of a given context line are extracted correctly, as it is a challenging process in
a dynamic language such as JavaScript. For example, unlike Java, a semicolon is not required in
JavaScript for declaration or expression statements; in such cases, we consider the new line as the
end of a statement. The buggy line is within a loop or conditional statement in some cases. In such
buggy statements, we determine the closure through control flow analysis to avoid an incomplete
context with backward slicing.

return {

1 import { get } from '@ember/object';
2 import CheckSessionRoute from '../../check-session-route';
3 function service(user) {
4
5
6
7
8
9 }
10 export default CheckSessionRoute.extend({
11 -
12 });

...user,
userToken: get('currentUser.accessKey'),
userSecret:

get('currentUser.userSecret'),

currentUser: service(),

};

Listing 2. Sliced Buggy File

return {

1 import { get } from '@ember/object';
2 import CheckSessionRoute from '../../check-session-route';
3 function service(user) {
4
5
6
7
8
9 }
10 export default CheckSessionRoute.extend({
11 +
12 });

...user,
userToken: get('currentUser.accessKey'),
userSecret:

get('currentUser.userSecret'),

currentUser: service(user),

};

Listing 3. Single Sliced Fixed File

Our extracted datapoints of buggy and fixed files contain the whole JavaScript code with only
the bug, and the patch that contains the difference at the same buggy line. We propose two types
of slicing mechanisms, namely single slicing and dual slicing. For each, we generate a separate set
of datasets and conduct experiments to measure their effectiveness in repairing programs. These
two types of slicing mechanisms are described below.
Single Slicing. Single slicing analysis is used for extracting context with respect to the buggy line.
We start off by first taking the buggy line as the slicing criterion to capture the backward slice
statements as context. This generates a buggy file with only the backward sliced statements as
context. We then attach this context of the buggy file to the correct line and generate the fixed
version of the buggy file. We call this single slicing because backward slicing has been applied only
on the buggy file and the context of the buggy file is simply transferred to the fixed file. Listing 2
and 3 demonstrate the snippets of single sliced pairs of buggy and fixed files from the same example
shown in Listing 1. As the slicing criterion was the buggy line, the fixed file contains the same

6

sliced context of the buggy file and contains no information about the variable user that has been
passed as a fix.
Dual Slicing. Our intuition is that, certain repair ingredients are available in the fix context, which
can improve the overall learning process. For instance, if the fix introduces a new identifier that
was not present in the buggy line, then the corresponding context will be different in the fixed
version of the file. This pattern is known as Change Identifier Used, and is commonly observed in
many programming languages [23, 24]. Learning from the context of both the buggy and fixed
code can provide additional semantic information to the model.

Therefore, we hypothesize that extracting slices from both the buggy and fixed files can be
more effective in conveying the contextual information to the training model. We call this notion
of collecting buggy and repair contextual ingredients dual slicing. Dual slicing analysis aims for
extracting context separately from both buggy and fixed files. By using the slicing criterion from
the buggy line and fixed line, we extract the sliced statements from both these lines individually
to generate the sliced buggy and fixed files. Listing 2 and 4 demonstrate the dual sliced pairs of
the buggy and fixed files from the motivating example of Listing 1. As the slicing criteria are both
the buggy and fixed lines in this approach, the fixed file contains an additional line containing the
variable declaration of user in line 10 which is necessary for generating the patch as the function
service expects this param.

return {

1 import { get } from '@ember/object';
2 import CheckSessionRoute from '../../check-session-route';
3 function service(user) {
4
5
6
7
8
9 }
10 const user = get('currentUser.user'); // New context
11 export default CheckSessionRoute.extend({
12 +
13 });

...user,
userToken: get('currentUser.accessKey'),
userSecret:

get('currentUser.userSecret'),

currentUser: service(user),

};

Listing 4. Dual Sliced Fixed File

3.3 Graph Representation
Once we have prepared the single sliced and dual sliced datapoints, we move to the learning phase of
the approach. To represent source code, we opt for graph representation. Graph representations of
source code have gained popularity recently due to their ability to represent semantic information.
These graphs are able to leverage the syntactic and semantic relations between the nodes via
edges and to consider long-range dependencies [7]. For each datapoint, we create graphs for the
buggy and fixed sliced files separately. We extract the AST of each sliced files and convert it to
a graph with the addition of SuccToken edges to connect leaf nodes and ValueLink edges to
connect additional value nodes, following previous work [11]. Figure 1 illustrates the buggy and
fixed graphs for the slices of our example (Listing 1). After representing the source code as a
graph, the resulting graphs are mapped into a vector representation using a Graph Neural Network
(GNN). More specifically, given a graph 𝑔 = (𝑉 , 𝐸) with a set of nodes 𝑉 and edges 𝐸, we determine
the 𝑑-dimensional representation of graph 𝑔 and individual node representations 𝜐 ∈ 𝑉 using a
function 𝑓 (𝑔) ↦→ (R𝑑, R|𝑉 |×𝑑 ). The node embedding is (cid:174)𝑣 = ℎ (𝐿)
𝑣 where 𝐿 denotes the total number
of propagations in the GNN via message passing [59]. The graph representation (cid:174)𝑔 is the aggregation
of node embeddings ℎ𝑙
𝑣 for each 𝑙, max pooling is used, which takes
the average of the 𝐿 + 1 vectors to obtain (cid:174)𝑔.

𝑣, ∀𝑙 ∈ 0, 1, ..., 𝐿. To aggregate ℎ𝑙

7

3.4 Training
In this step, we split the graphs generated from the previous step into training, testing and validation
using random sampling. We fold the dataset into 80% training set, 10% testing set, and 10% validation
set. Table 1 provides the number of datapoints in the training, validation, and test datasets in our
study. To train a model, we adopt a GNN architecture that maps the graph representation into a
fixed dimensional vector space. The program repair is essentially a series of graph transformation
operations, from the buggy graph to the fixed graph, containing operations such as adding or
deleting a node, replacing a node value or node type. For single slicing, the model is trained on a
buggy graph, 𝑔𝑏𝑢𝑔 which is the backward sliced context of only the buggy line. The output is a fixed
graph, 𝑔𝑓 𝑖𝑥 which contains the same sliced context as 𝑔𝑏𝑢𝑔, however, the buggy line is replaced
with the fixed line. On the other hand, for dual slicing, the input remains the same whereas the
output 𝑔𝑓 𝑖𝑥 contains the backward sliced context of the fixed line.

Table 1. Dataset split for training, testing, and validation.

Train

Test

Val

Total

91,181

11,397

11,397

113,975

Hyperparameter Tuning. Hyperparameters are important since they influence a model’s overall
performance. We use a batch size of 10 and tune the hyperparameters of the model for the sliced
datapoints. In order to find the optimum set of hyperparameters, we use random search from a
range of values in GNN layers, learning rate and dropout. We picked 2, 3 and 4 layers for the GNN
model, learning rates of 0.1, 0.01, 0.001 and dropout values of 0.0, 0.1, and 0.2. We trained the model
on 5 epochs with a batch size of 10 for each combination of hyperparameters. Overall, we trained
27 models during the hyperparameter search and found the best performance using 4 GNN layers,
a learning rate of 0.001 and a dropout of 0.1.

3.5 Inference
After the training step, we evaluate each model using the test dataset. Given a buggy file during
inference, we assume that the buggy line has already been identified in a fault localization step,
similar to the current state-of-the-art learning-based repair techniques [10, 32, 38]. Locating the
buggy line is a necessary pre-processing step to extract the backward slice as context. The model
localizes the bug within the buggy graph and generates the corresponding patch. The input and
output to the models during evaluation are as below:

• Training: As described in Section 3.4, we train two separate models with the single slicing

and dual slicing datasets.

• Inference: Input to the models remains the same during inference. For evaluating the trained
single and dual sliced models, input to the model is a graph, 𝑔𝑏𝑢𝑔 which is generated from
the single-sliced buggy JavaScript file containing the buggy line.

• Output: For both cases, the model output is a fixed graph, 𝑔𝑓 𝑖𝑥 , which the model tries to
generate by a sequence of up to 𝑇 steps of transformations. We use a maximum of 20
modification steps in the graph.

For generating the fix for a 𝑔𝑏𝑢𝑔, both single and dual slice models maintain a pool of 492 node
types to predict the type; there are 5,001 values for the single slice model and 5,002 for the dual
slice model, in a global value dictionary to predict node values; these types and values are extracted
from our training datasets of buggy and fixed files. When generating patches to replace a node

8

Table 2. Accuracy of single slicing vs. dual slicing

Approach

Single Slicing
Dual Slicing

Accuracy

Top-1 Top-3
Top-5
20.72% 35.01% 42.90%
28.31% 41.95% 46.96%

value, if a vocabulary is not in the global value dictionary, the patch is replaced with UNKNOWN. The
model generates the top 𝑘 patches for a buggy program as a series of graph edits depending on the
beam size. We consider an inferred patch as correct if the type, location and value of the fix has
been accurately identified.

3.6 Implementation
We implemented our technique in a tool called Katana. Since we were not able to find any existing
program slicing tool that supports the latest JavaScript ES10 features, as required for analyzing our
dataset, we implemented our own JavaScript slicer for Katana. Following previous work [55], we
build on top of the Understand [3] program analysis framework to analyze control flow and data
flow dependencies in JavaScript code with respect to our slicing criterion. The slicer in Katana
takes as input a buggy JavaScript file, and the slicing criterion, which is the buggy line and the
entities (variables, objects or functions) used in that line. It produces as output a sliced JavaScript
file, which is used as context for learning repairs. The deep learning model in Katana is based on
the GNN architecture of Hoppity [11].

4 EVALUATION
To assess the effectiveness of Katana we address the following research questions:
RQ1 How does dual slicing compare to single slicing for learning repairs?
RQ2 How does Katana compare to state-of-the-art learning-based repair techniques?
RQ3 What is the effect of slicing on the information obtained as context?

We discuss the experiments that we designed to answer each of the research questions in the

following subsections and outline the results.

4.1 Single versus dual slicing as context (RQ1)
We compare the effectiveness of single slicing where we extract context from the buggy file with
dual slicing where the context is extracted from both the buggy and fixed files.

Using our initial 91,181 (training) datapoints, we generated two separate datasets using our
single and dual slicing techniques. The processing time to generate slicing is negligible. On average,
single slicing and dual-slicing analysis takes around 330 and 490 milliseconds for a single datapoint,
respectively. With the tuned set of hyperparameters, we trained two models separately with the
generated single sliced and dual sliced datasets. It took 1.7 hours to train each model for 50 epochs.
The average inference time for a datapoint is around 2.54 seconds for each trained model. All our
experimental runs are performed on an Ubuntu Linux 18.04.2 LTS server with 122 GB RAM, 16
vCPUs and 2,000 GB SSD.

We use the same test dataset to evaluate the accuracy of the two models. As the model produces
a series of graph edits to generate the patch, we consider an output to be correct if the predicted
location of the AST node, the type of operation (i.e., replace type or value, add node, delete node)
and the value of the node matches the expected node location, operation type, and value of the
patch in the test dataset, for the entire sequence of graph edits.

9

Table 2 shows the results for single and dual slicing in Katana. Single sliced context yielded
accuracies of 20.72%, 35.01%, and 42.90% with beam sizes of one, three, and five, respectively. Dual
sliced context yielded accuracies of 28.31%, 41.95%, and 46.96%, respectively. Dual slicing achieved
higher accuracies for all beam sizes, with 37, 20, and 9 percent increases for the top-1, top-3, and
top-5 suggestions, respectively, with respect to single slicing.

For instance, if we focus on top-3 suggestions, out of the 11,397 bugs in the test dataset, dual
slicing fixed 4,781 and single slicing fixed 3,990. Out of these, 3,984 were fixed by both single and
dual slicing, however, the dual slicing approach was able to fix 797 more bugs than single slicing.

-
+

cart.push(object);
cart.push(item);

Listing 5. Example of Change Identifier Used bug pattern fixed by dual slicing

Listing 5 shows an example of a bug that was correctly fixed by dual slicing but was not repaired
by single slicing. Through sampling, we verified that Change Identifier Used is one of the recurring
patterns among the correct patches generated by dual slicing. We attribute this pattern to the
availability of repair ingredients, made accessible through dual slicing as context used during
training. Therefore, since the dual sliced context outperforms the single sliced context, we select it
as our default approach in Katana going forward.

4.2 Comparison with state-of-the-art (RQ2)
We compare Katana with the four recent deep learning-based program repair techniques. In
addition, we considered evaluating DLFix [32]. However, DLFix was unavailable at the time of
writing this paper. A brief outline of these state-of-the-art deep learning-based program repair
techniques is described below:

(1) Tufano et al. [51] employs code abstraction on an RNN-based NMT model with attention

mechanism.

(2) SeqenceR [10] uses an RNN-based NMT model equipped with copy mechanism.
(3) Hoppity [11] proposes a GNN model to predict the location of bug and generate a fix

through a sequence of graph edits.

(4) CoCoNuT [38] leverages a CNN-based NMT model with two separate encoders for buggy

line and context.

4.2.1 Characterizing context. To characterize how current techniques treat context, we reason
about five different aspects of context:

• Analysis: Contextual information can be extracted from the source code in various ways.
The analysis can be as simple as naively extracting tokens from the buggy statement’s
surroundings, to more complex program analysis techniques such as data flow, control flow,
or slicing.

• Representation: Once extracted, context can be represented in different ways, e.g., as linear

sequence of tokens, or nodes in a graph.

• Scope: Different levels of granularity can be considered for the scope of context, for example,

by focusing on the entire program, enclosing file, class, or method.

• Proximity: Context can be extracted with respect to the location of the bug/fix, e.g., sur-

rounding, before, or after the buggy line.

• Limit: Context size can be limited by the amount of information it contains, e.g., number of

lines of code, tokens, or AST nodes.

10

Table 3. Accuracy comparison of Katana with other learning-based program repair techniques.

Approach

Tufano et al. [51]
SeqenceR [10]
Hoppity [11]
CoCoNuT [38]
Katana

Analysis
Naive
Naive
Naive
Naive
Dual Slice AST-based graph

Representation
Sequences of tokens
Sequences of tokens
AST-based graph
Sequences of tokens

Context

Scope
Enclosing method
Enclosing class
Enclosing file
Enclosing method
Enclosing file

Proximity
Before/After
Before/After
Before/After
Before/After
Before

Limit
100 tokens
1000 tokens
500 nodes
1,022 tokens
N/A

Top-1
4.90%
7.99%
5.05%
24.67%
28.31%

Accuracy
Top-3
11.33%
13.84%
14.40%
27.90%
41.95%

Top-5
15.84%
18.08%
19.62%
28.97%
46.96%

Table 3 compares how state-of-the-art techniques treat context using these five aspects.

Setup. We trained a separate model using the deep learning framework of each of these

4.2.2
techniques following the dataset split of Table 1 with our unsliced dataset as follows:

Tufano et al. [51] represent source code as a sequence of tokens and use the method enclosing
the bug as the scope of context, which is limited by a maximum of 100 tokens. There is no program
analysis for extracting context. However, abstraction is applied to code to limit the vocabulary
size. We applied the same abstraction technique to our JavaScript dataset. We tokenized the buggy
and fixed JavaScript files and tokenized it using js-tokens2 to discern whether a given token is an
identifier, method, or a literal. Following the same approach as [51], we maintain a dictionary of
frequently occurring tokens and replace the remaining tokens with abstraction depending on the
token type (e.g., METHOD_1, VARIABLE_1). Additional challenges with JavaScript code included
the presence of JSX identifiers and literals. We assigned new abstractions for these types of tokens.
The final vocabulary size became 815, including abstractions, JavaScript keywords, and frequent
tokens.

SeqenceR represents source code as a sequence of tokens. However, they use the enclosing
class as context scope, limited by 1,000 tokens; it delineates the buggy line within <START_BUG>
and <END_BUG> tokens for differentiation within the context. SeqenceR takes more input
tokens from lines that appear before the buggy line. It extracts 2/3rd more tokens from proceeding
statements before the buggy line than the subsequent statements. We applied the same technique
to our JavaScript code corpus, and if the buggy line was not encapsulated in a class, we extracted
tokens from the surrounding lines in the JavaScript file. The vocabulary size is limited to 1,000
tokens.

Hoppity represents context as an AST-based graph and does not apply any program analysis. Its
context scope is the enclosing file of the buggy line, limited to 500 AST nodes. The vocabulary size
on our dataset is 5,003 tokens.

CoCoNuT leverages the enclosed method of the buggy line as the scope of context and represents
the source code as a sequence of tokens limited by 1,022 tokens. Similar to other techniques, context
is taken without any program analysis. They employ abstraction and use two separate encoders
to feed the buggy line and context. We used pre-processing scripts from their artefact to tokenize
source code, and the resulting vocabulary size was 63,499.

We use the same test dataset of 11,397 JavaScript bugs for evaluating all the models. The best
reported hyperparameter settings and epochs were used to train each of these models. To keep the
accuracy comparison equitable across all the models, we use beam sizes of one, three, and five.

4.2.3 Results. Table 3 reports the obtained accuracies for beam sizes of one, three, and five in
the columns Top-1, Top-3 and Top-5. The lowest Top-1 accuracy was yielded by Tufano et al. [51].
Despite using a high level of abstraction and a small vocabulary size, limited contextual information
has not helped their model. As more contextual information was fed into the model, accuracy

2https://www.npmjs.com/package/js-tokens

11

Fig. 2. Overlap of top-3 correct patches generated by context-aware learning based approaches.

improved gradually. Hoppity demonstrated better accuracy for Top-3 and Top-5 predictions. Among
existing techniques, CoCoNuT yielded the highest accuracy. However, Katana outperforms all
existing techniques for all the beam sizes, with accuracies of 28.31% , 41.95%, and 46.96% for the
beam width 1, 3, and 5, respectively.

Considering the top-3, Katana is 270.26%, 203.11%, 191.32%, 50.36% more accurate in fixing buggy
programs than Tufano et al. [51], SeqenceR, Hoppity, and CoCoNuT, respectively. By learning
from the context of the relevant statements in the buggy and fixed file, the dual slicing-based
context in Katana can accurately repair buggy programs by a significant margin.

Figure 2 demonstrates an illustration of the overlapping sets of top-3 correct patches generated by
the repair techniques. Each row in this figure represents a specific approach which is color encoded
for differentiability. The numbers on the right depict the total number of patches generated correctly
by each approach. For example, Katana can fix a total of 4,781 out of 11,397 bugs, CoCoNuT can
fix 3,180 out of 11,397 bugs correctly and so on. The numbers at the bottom indicate the intersecting
set of the correct patches for the specific block. For example, 888 patches of the same set of bugs
were correctly generated by Katana, CoCoNuT, and SeqenceR, as shown at the bottom. As three
approaches overlap, this number is reflected at the top row in this column of 888 patches. All five
techniques could accurately fix 101 instances of bugs as indicated by the presence of all colors in
the leftmost first column in Figure 2. We observe that Katana can fix a wide range of bugs, and
the row for Katana encompasses patches from other approaches. Furthermore, 891 bugs could
only be fixed by Katana as shown in the figure.

4.3 Slicing-based context (RQ3)
We carry out a quantitative analysis of our slicing technique to measure the type and amount of
information considered as context and compare it against other approaches.

We collected statistics over our dataset during the program slicing analysis. We calculated the
number of lines before and after slicing as well as the fraction of the datapoints undergoing control
flow and data flow analysis. We found that 26.96% of the total datapoints (113,975 pairs of buggy
and fixed files) required the use of both control flow and data flow analysis to produce the slices.
The remaining 73.04% were sliced by data flow analysis only. Figure 3 illustrates the number of
lines before and after slicing across the datapoints. Here, we refer to the dataset before slicing as
unsliced.

Katana’s program slicing reduced the average number of lines to consider for context from
39 to 15. The median number is also reduced from 33 to 9. The maximum number of lines before
slicing is 1,087, and after slicing it is reduced to 636, whereas the minimum is 1 in both cases.

12

No. of PatchesApproachesKatanaCoCoNuTHoppitySequenceRTufano et al.3563828881829944138914204333221147813180164115771292Fig. 3. Distribution of number of lines in Sliced and Unsliced data.

Fig. 4. Reduction in number of lines after slicing across datapoints.

Fig. 5. Descriptive statistics of tokens in context-aware learning approaches.

In Figure 4, we can observe the spread of line differences across the datapoints. The difference
from sliced to unsliced is always positive indicating reduction after slicing. The average reduction
is 24 lines. The maximum reduction is 1,073 lines, for a file that was 1,087 lines.

Figure 5 depicts bean plots of the number of tokens in the buggy files for Katana and the other
techniques. We chose the buggy files from our (113,975) JavaScript datapoints because the machine
learning model takes the buggy file or buggy line with context as the input during inference. We
stripped the whitespaces and comments from these files before token analysis. Tokens are the
smallest possible unit that is common between all these approaches. Hence, we examine the increase
or reduction of information used as context through this metric. As discussed before, except for
Katana, all of these context-based approaches use an ad-hoc bounding limit for reducing noise in

13

SlicedUnslicedMd= 9 Mean= 156361Md= 33 Mean= 391087101000200030004000# of Datapoints02004006008001000Line diffsavg. line diffs 24.48TufanoMd=8766897         100Mean=80SequenceRMd=1816993011000Mean=209HoppityMd=25061414113589Mean=290CoCoNuTMd=1606902691019Mean=190KatanaMd=846491602399Mean=132the context. Just to recap, Hoppity uses a maximum limit of 500 nodes whereas SeqenceR, Tufano
et al. and CoCoNuT use a sequence limit of 1,000, 100 and 1,022 tokens, respectively. Among all the
approaches, Hoppity has the highest maximum token count of 3,589 tokens followed by CoCoNuT
(1,019 tokens), SeqenceR (1,000 tokens) and Tufano et al. (100 tokens). The maximum number of
tokens in Katana is 2,399 tokens. Our slicing reduced the number of tokens by 1.5 times compared
to Hoppity. Hoppity has the highest average number of 290 tokens, and with abstraction, Tufano
et al. have the lowest average of 80 tokens. The average number of tokens in SeqenceR and
CoCoNuT are 209 and 190, respectively. Katana has an average of 132 tokens, which is less than
Hoppity, SeqenceR and CoCoNuT. The median number of tokens shows a more profound effect
of program slicing in the reduction of tokens. Katana has the lowest median of 84 tokens among
all the approaches. We can also observe the same trend of token reduction in the lower quartile of
the dataset with Katana. For the third quartile of the dataset, Hoppity, SeqenceR and CoCoNuT
have 411, 301, and 269 tokens, respectively. On the other hand, Tufano et al. and Katana yielded
68 and 160 tokens, which is significantly lower. Without a need for truncating the context, our
approach for extracting relevant context based on control and data flow analysis, has a significant
amount of information reduction on average.

5 DISCUSSION
In this section, we discuss the findings from Section 4.
Role of slicing in learning repairs. The overall accuracy of Katana in repairing programs can
mainly be attributed to the effectiveness of dual slicing-based context. In Table 2, we observe
that dual slicing outperforms single slicing by a significant margin. Dual slicing can derive repair
ingredients from both the buggy and fixed graphs. This additional contextual information allows
the model to learn from buggy context and fixed context. For instance, the availability of vocabulary
from the sliced context in fixed graphs during training equipped the model with repair ingredients
needed for generating fixes during inference. We notice this in Listing 5, where the fixed line
replaces the variable object with item in which, the variable item is part of the vocabulary.

In addition to preserving the repair ingredients, program slicing can help to minimize noise
for the GNN model. The quantitative analysis in Section 4.3 shows the effect of program slicing
in reducing the information scope, without the need for a predefined bound on the number of
tokens or AST nodes. Listing 6 shows an example of a bug pattern that only Katana could fix.
The buggy file contains 550 lines of code. In the sequence-based approaches, although the variable
options was present, the method fetchVoiceRegions was removed from the input due to their
context limit. Even though the method was defined within the enclosing class and enclosing file,
the low proximity between the method call and method definition caused this method definition to
be truncated. While fixing a bug, the method signature provides useful debugging information to
reason about the code. By leveraging slicing, Katana was able to preserve relevant information
pertaining to the buggy and fixed line. This indicates that dual slicing-based context can retain
relevant information while reducing noise, which can benefit the learning process of the model, as
shown in Table 3.

-
+

this.fetchVoiceRegions(options);
this.fetchVoiceRegions();

Listing 6. Example of a bug fixed by Katana

14

Table 4. Qualitative analysis of bugs fixed exclusively by Katana.

Bug Pattern

Description

Buggy Code

Fixed Code

Same Function More Args

Function with missing argument
in the buggy line

return drones[droneUpdate.id] = new

return drones[droneUpdate.id] = new

Drone(droneUpdate);

Drone(droneUpdate, 10);

Same Function Less Args

Function with additional argu-
ment in the buggy line

currentUser: service();

currentUser: service(user);

kittens.pop(name)

kittens.pop()

Incorrect Object Instantia-
tion

Constructor
new keyword

app.get('/api/current_user', (req,
res, next)=> {

app.get('/api/current_user', (req,
res)=> {

invoked without

var componentSchema = Schema({

var componentSchema = new Schema({

const history = mongoose.Schema({

const history = new mongoose.Schema
({

Missing Return Statement

Expression
return keyword

with

missing

await bcrypt.compare(password,
savedPassword);

return await bcrypt.compare(
password, savedPassword);

1
5

Incorrect Variable Decla-
ration

Variable declared with incorrect
keyword

Incorrect For Loop Used

Expression with incorrect usage
of for...in or for...of
used

changeAnimal();

return changeAnimal();

var shuffled = arr.slice(0), i =
arr.length, min = i - count, temp,
index;

let shuffled = arr.slice(0), i =
arr.length, min = i - count, temp,
index;

var garray = array//.shift()

const garray = array//.shift()

for (var file in files){

for (var file of files){

Change Boolean Literal

Expression with incorrect boolean
literal used

app.use(bodyParser.urlencoded({
extended: false }));

app.use(bodyParser.urlencoded({
extended: true }));

for (let slot of Object.values(
character.slots)){

for (let slot in Object.values(
character.slots)){

Change
Literal

String/Numeric

Expression
string/numeric literal used

with

incorrect

Change Identifier Used

Expression with incorrect identi-
fier used

overlap: {type: Boolean, default:
true},

overlap: {type: Boolean, default:
false},

const PORT = process.env.PORT ||
3001;

const PORT = process.env.PORT ||
3000;

router.get('/comment/:id',
CommentController.GetComments);

router.get('/comment/list',
CommentController.GetComments);

componentWillMount: ()=> {

componentDidMount: ()=> {

this.destroy(res, res)

this.destroy(res, err)

Katana is effective across pervasive bug patterns. Table 4 shows a qualitative analysis of
the type and description of bug patterns observed in the test dataset that only Katana was able
to fix. Unlike prior techniques [31, 43, 47] which are trained on specific error types, our model
targets a wide variety of error types as it is trained on real world bugs. We assessed 100 random
samples out of the 891 correct patches generated uniquely by Katana (see Figure 2, second column
from right). As mentioned previously, our dataset has been curated from open-source Github
repositories and hence, they are representative of real bugs that have been fixed by developers.
For the qualitative analysis of the patches, the authors manually analyzed the resulting correct
patches of Katana, categorized them individually and came to a consensus if they agreed on the
bug fixes to be representative of bug patterns. The assessment of the patches and categorization
took approximately 12 person-hours in total. For labelling the bug type, we used ManySStuBs4J [24]
to identify the category of the bug fixes. We present nine bug patterns (two patches per pattern
as examples) fixed by Katana in Table 4 in the test dataset. Among these nine bug patterns, five
represent some common single statement bugs seen in other programming languages such as Java
or Python [23, 24]. The remaining four patterns are specific to JavaScript. These language-specific
bug patterns include Incorrect Object Instantiation, Missing Return Statement, Incorrect Variable
Declaration and Incorrect For Loop Used. Since JavaScript code is interpreted at runtime, bugs such
as Incorrect Object Instantiation and Missing Return Statement may only surface during program
execution. These bugs occur because JavaScript is a dynamically typed language and can silently
propagate these errors. Missing Return Statement has been categorized as a common bug pattern
in JavaScript in BugsJS [17]. Incorrect Variable Instantiation is a common best practice in modern
JavaScript projects that supports the ECMAScript 6 (ES6) syntax, and it indicates a code smell. In
ES6, the keywords let and const were introduced to the language [1] because of scoping issues
caused by the keyword var, which can eventually lead to hard to find bugs in JavaScript [49].
Another bug type, Incorrect For Loop Used, occurs mainly in ES6 compatible JavaScript source code,
where the for...in and for...of loops are incorrectly used in which the former is used for
looping through enumerables in objects and the latter is used for iterating through values of arrays
and objects. Due to their syntactical similarity, developers often tend to use them interchangeably
for the wrong purpose which can lead to bugs that are hard to identify. Thus, Katana can detect
and fix a wide variety of pervasive bug patterns that include both common and language-specific
bugs more effectively than the baselines.

Effect of missing repair ingredients on inference. Katana is most effective when it can incor-
porate relevant repair ingredients by leveraging context using slicing. To understand why Katana
was unable to correctly generate patches for some bugs, we manually examined 200 random samples
from the incorrect patches. Upon careful investigation, we observed that for 16% of the incorrect
patches, the relevant repair ingredients were not present within the file. Since we collect our
datapoints based on buggy commits, our scope is limited to the enclosing file, which often does
not include interdependent files. As such, we found instances of function calls or variables used
in the buggy file that were imported from other files. As a result, relevant syntactic and semantic
information required for such functions or variables was not preserved as context for the model
to generate correct patches. To mitigate this, we plan to extend our slicing technique to perform
interprocedural analysis across files as part of future work.

Furthermore, there were recurring instances (84%) of the Change String/Numeric Literal bug
pattern among the incorrect patches generated by Katana. Upon closer inspection, we noticed
that not all of these string replacements were actual bug fixes—most of them were typo fixes or
ad-hoc textual changes in config files. We believe that bug patterns pertaining to string changes
can further benefit from a larger vocabulary size than the current 5,002 tokens.

16

Limitations. Katana cannot currently generate slices from minified or obfuscated JavaScript code
because of constraints in our slicing framework. The minification step removes whitespaces, obfus-
cates variable names and often produces a single line as an output. In practice, the minification step
is taken prior to deployment in the production environment, and it is not for human consumption.
Therefore, we excluded minified JavaScript files in our dataset as they may introduce noise in the
learning process.

Some JavaScript files can contain templated code (i.e., HTML or JSX3) from front-end frameworks
such as ReactJS4, VueJS5 or Angular6. However, our analysis tool cannot extract slices for bugs
inside front-end templated code. Consequently, Katana could produce invalid code slices because
the files do not contain pure JavaScript code.

JavaScript is a dynamically typed language, which inherently makes it difficult to keep track of
control and data flow dependencies during program slicing. In cases where control and data flow
analysis does not yield any sliced context, we consider the contents of entire file as context scope
for the buggy line.

6 THREATS TO VALIDITY
This section describes how we addressed potential limitations that may have biased our findings.

6.1 Internal Validity
An internal threat to validity is the accurate end-to-end replication of the comparison with state
of the art techniques. Two of the techniques we compared with, namely Tufano et al. [51], and
SeqenceR applied custom abstraction mechanism on the enclosing method or class of bugs in
Java source code. Following Tufano et al. [51], we implemented the same abstraction technique
for JavaScript. SeqenceR derives contextual information from the enclosed class of the buggy
line. However, JavaScript poses unique differences from a compiled language such as, Java. As
JavaScript source code is not always enclosed in a class or function, we take the entire file as the
context scope in such cases to ensure that the same datapoints are preserved for a fair comparison
across all techniques.

6.2 External Validity
We build our dataset from open source GitHub repositories of JavaScript projects. We used specific
keywords to filter buggy commits following prior work [24]. One external validity is that the search
heuristics used cannot ensure that all the datapoints represent actual bug fixes as some commits
may contain refactoring activity or ad-hoc code changes. Nevertheless, we were able to find nine
types of bug patterns pervasive across the test set, and this search criteria is consistent with other
learning-based program repair studies [11, 38, 51].

We implemented our approach for JavaScript, and our experiments cannot conclude how effective
Katana would be for other languages. However, the fundamental challenges Katana addresses,
i.e., how to retain useful contextual information for a repair task, why context selection using a
naive approach is not sufficient, how to select useful information pertaining to the buggy/fixed line,
are language independent and other languages could potentially benefit from slicing-based context.
Static program slicing requires a slicing framework for each programming language, and building
such a framework is an expensive process as it requires high development cost. We therefore used
a single language and built a slicing framework to evaluate our technique.

3https://reactjs.org/docs/introducing-jsx.html
4https://reactjs.org
5https://vuejs.org
6https://angular.io

17

We considered a difference of one AST node for extracting buggy commits, which is similar to
existing work [11]. As a result, currently our approach can handle one-off errors. Existing learning-
based program repair techniques [8, 10, 30, 50] do not support multi-hunk changes (multiple buggy
and fixed statements). However, conceptually Katana could be extended to extract context from
multiple buggy and fixed statements to better encode the repair ingredients, although this requires
further evaluation in the future. In such approaches, using an ad-hoc approach would yield very
large contextual information, severely limiting the effectiveness of a learning model. Therefore, we
believe that fine-grained context extraction using Katana would be even more relevant for the
multi-hunk changes.

Some techniques do not use contextual information at all and others consider context using a
different approach. For instance, Rachet [19] only considers the buggy line without any context,
whereas DLFix [32] selects the buggy subtree as context. Although we did not directly compare with
these in our evaluation, DLFix [32] reported to outperform Rachet, and CoCoNuT [38] reported to
outperform DLFix. Since Katana is directly compared with CoCoNuT, we can deduce, through
transitive closure, that Katana outperforms DLFix and Rachet, although direct experiments are
needed to verify this empirically.

Reproducibility. We have made our dataset, model, comparison framework, and Katana’s im-
plementation available [4] for reproducibility of the results. We further provide instructions for
replicating our experimental setup.

7 RELATED WORK
In this section, we describe related work on (a) how slicing is used in traditional program repair, (b)
the usage of contextual information in learning-based program repair, and (c) the use of slicing on
different learning-based source code processing tasks.

Traditional Automated Program Repair. Automated program repair has received significant
attention from the research community. Many APR techniques have been proposed that could be
classified as search-based [29, 30, 50, 52, 53], semantics-based [25, 41], or pattern-based [20, 26, 35–
37, 46]. From this large body of literature, CapGen [55] is the closest to our work, which employs
program slicing for program repair. CapGen is a search-based program repair technique that
leverages context based on forward and backward slicing of nodes in the AST for mutation operator
selection and ingredient prioritization. However, CapGen requires domain-specific knowledge
about bug and fix types, whereas our approach can learn patterns from sliced data and fix a variety
of bug patterns.

Additionally, there are repair techniques that address specific classes of faults [14]. For exam-
ple, fault specific techniques focus on repairing conditional statements [12, 42, 57], concurrency
bugs [21, 22], string sanitization [5, 58], access control violations [48], or repair memory leaks [13].
FixMeUp [48] targets access control violations in PHP web applications. FixMeUp applied inter-
procedural program slicing on the data, and control dependence graphs to identify the statements
that need to be guarded by an access control check. Instead, we applied program slicing for a
learning-based model which is not bug specific.

Context in Learning-Based Repair. Most current learning-based repair techniques consider
context in an ad-hoc manner. Hoppity [11] is limited by a maximum of 500 nodes in the AST and
treats the whole file as context for the buggy line. On the other hand, SeqenceR [10] uses the
enclosing method and the class surrounding the buggy line to capture long-range dependencies
between the buggy line and context. This technique is limited to 1,000 tokens, which may result in
the truncation of code if the enclosing method is long, as an example. Tufano et al. [51] use buggy
and fix files of small or medium-sized methods with a maximum of 50/100 tokens to learn bug

18

fixes. They employ abstraction in the context of buggy and fixes files to mitigate the vocabulary
limit. DLFix [32] is a deep learning-based program repair approach that uses the context of the
surrounding subtree of both the buggy line and fixed line to generate fixes using a tree-based RNN
model. This context is summarized as a vector and used as weights to generate the patch for the
buggy code. CoCoNuT [38] uses the entire method of the buggy line as context with the buggy
line and context fed as two separate inputs. TFix [9] is a learning-based system that uses a T5
model [44] fine-tuned on a text-to-text patch prediction task that leverages the surrounding two
lines of the buggy line as context for JavaScript errors from ESLint7. All these techniques leverage
context in a limited way and rely on predefined heuristics. In contrast, Katana relies on program
analysis to retrieve relevant code as context, is not bounded by predefined limits or heuristics, and
leverages control and data flow information from both the buggy and fixed versions of the code.

Slicing in Learning-Based Source Code Processing. Program slicing was explored in a learning-
based vulnerability detection task [33, 34, 60]. VulDeePecker [34] trains a neural network with
positive and negative examples to determine whether a code snippet suffers from vulnerability. A
more recent development is µVulDeePecker [60], which extends VulDeePecker to predict multiclass
vulnerabilities. These techniques extract slices of the vulnerable library/API function calls and are
trained on an RNN to predict vulnerabilities in code. In a recent work, Xiao et al. [56] applied program
slicing to a learning-based cryptographic API Suggestion. This approach applied interprocedural
backward slicing from the invocation statement of a cryptography API and trained a modified
LSTM-based model for a neural-network-based API recommendation task. In Katana, we instead
(a) apply program slicing on a generative task, i.e., fix generation, (b) introduce the notion of dual
slicing to learn from both the buggy and fixed-versions of the code, and (c) employ a GNN model to
better capture rich structural and semantic information that is inherent to the source code instead
of using RNNs that treat code as a linear stream of tokens. To the best of our knowledge, we are
the first to employ slicing for learning-based program repair.

8 CONCLUSION
As a developer, context is pivotal while understanding and writing a fix for a buggy piece of
code. Surrounding lines, methods, and class level information provide background contextual
information that is important to devise a patch for a buggy piece of code. Current learning-based
repair techniques adopt predefined heuristics, such as a limited number of tokens or AST nodes
to extract context, which is insufficient for representing contextual information of a bug fix. We
argue that context extraction needs to be directed toward relevant code for a learning-based repair
technique to be effective. We present Katana, the first technique to apply program slicing on a
learning-based program repair task that includes relevant information as context pertaining to the
buggy/fixed code. We show that program slicing through control and data flow analysis effectively
preserves sufficient program repair ingredients to extract recurring patterns. We propose the notion
of dual slicing, a novel approach that leverages context from both the buggy and corrected code.
Our approach is able to fix 4,781 out of 11,397 bugs curated from open-source JavaScript projects.
The results show that Katana can resolve between 1.5 up to 3.7 times more bugs when compared
to the state-of-the-art learning-based repair techniques. Furthermore, Katana is effective across a
wide range of bug patterns. In the future, we plan to expand our interprocedural analysis, which
is now limited to the enclosing file. We will also expand Katana to support other programming
languages such as Java.

7https://eslint.org

19

REFERENCES
[1] 2015. ECMAScript 2015 Language Specification - ECMA-262 6th Edition. https://262.ecma-international.org/6.0.

Accessed: 2022-01-07.

[2] 2021.

StackOverflow Developer Survey 2021. https://insights.stackoverflow.com/survey/2021/#most-popular-

technologies-language-prof. Accessed: 2022-01-26.

[3] 2021. Understand by Scitools. https://www.scitools.com/. Accessed: 2021-12-30.
[4] 2022. Katana. https://anonymous.4open.science/r/Katana-118F/.
[5] Muath Alkhalaf, Abdulbaki Aydin, and Tevfik Bultan. 2014. Semantic Differential Repair for Input Validation and Sani-
tization. In Proceedings of the 2014 International Symposium on Software Testing and Analysis (ISSTA 2014). Association
for Computing Machinery, 225–236.

[6] Miltiadis Allamanis. 2019. The Adverse Effects of Code Duplication in Machine Learning Models of Code. In Proceedings
of the 2019 ACM SIGPLAN International Symposium on New Ideas, New Paradigms, and Reflections on Programming and
Software (Onward! 2019). Association for Computing Machinery, 143–153.

[7] Miltiadis Allamanis, Marc Brockschmidt, and Mahmoud Khademi. 2018. Learning to Represent Programs with Graphs.

In International Conference on Learning Representations (ICLR). 520–524.

[8] Johannes Bader, Andrew Scott, Michael Pradel, and Satish Chandra. 2019. Getafix: Learning to Fix Bugs Automatically.

Proceedings of the ACM on Programming Languages OOPSLA (2019), 27 pages.

[9] Berkay Berabi, Jingxuan He, Veselin Raychev, and Martin Vechev. 2021. Tfix: Learning to fix coding errors with a

text-to-text transformer. In International Conference on Machine Learning. PMLR, 780–791.

[10] Z. Chen, S. J. Kommrusch, M. Tufano, L. Pouchet, D. Poshyvanyk, and M. Monperrus. 2019. SEQUENCER: Sequence-
to-Sequence Learning for End-to-End Program Repair. IEEE Transactions on Software Engineering (2019), 1–1.
[11] Elizabeth Dinella, Hanjun Dai, Ziyang Li, Mayur Naik, Le Song, and Ke Wang. 2020. Hoppity: Learning Graph
Transformations to Detect and Fix Bugs in Programs. In International Conference on Learning Representations (ICLR).
[12] Thomas Durieux and Martin Monperrus. 2016. DynaMoth: Dynamic Code Synthesis for Automatic Program Repair. In
Proceedings of the 11th International Workshop on Automation of Software Test (Austin, Texas) (AST ’16). Association for
Computing Machinery, 85–91.

[13] Qing Gao, Yingfei Xiong, Yaqing Mi, Lu Zhang, Weikun Yang, Zhaoping Zhou, Bing Xie, and Hong Mei. 2015. Safe
Memory-Leak Fixing for C Programs. In Proceedings of the 37th International Conference on Software Engineering -
Volume 1 (ICSE ’15). IEEE Press, 459–470.

[14] Luca Gazzola, Daniela Micucci, and Leonardo Mariani. 2019. Automatic Software Repair: A Survey. IEEE Transactions

on Software Engineering 45, 1 (2019), 34–67.

[15] Rahul Gupta, Soham Pal, Aditya Kanade, and Shirish Shevade. 2017. DeepFix: Fixing Common C Language Errors by

Deep Learning. In Proceedings of the Thirty-First AAAI Conference on Artificial Intelligence. AAAI Press, 1345–1351.

[16] Shivani Gupta and Atul Gupta. 2019. Dealing with Noise Problem in Machine Learning Data-sets: A Systematic Review.

Procedia Computer Science (2019), 466–474.

[17] Péter Gyimesi, Béla Vancsics, Andrea Stocco, Davood Mazinanian, Árpád Beszédes, Rudolf Ferenc, and Ali Mesbah.
2019. BugsJS: a Benchmark of JavaScript Bugs. In 2019 12th IEEE Conference on Software Testing, Validation and
Verification (ICST). 90–101.

[18] Sakib Haque, Alexander LeClair, Lingfei Wu, and Collin McMillan. 2020.

Improved Automatic Summarization of

Subroutines via Attention to File Context. Association for Computing Machinery, 300–310.

[19] Hideaki Hata, Emad Shihab, and Graham Neubig. 2019. Learning to Generate Corrective Patches using Neural Machine

Translation. arXiv preprint arXiv:1812.07170 (2019).

[20] Jiajun Jiang, Yingfei Xiong, Hongyu Zhang, Qing Gao, and Xiangqun Chen. 2018. Shaping Program Repair Space
with Existing Patches and Similar Code. In Proceedings of the 27th ACM SIGSOFT International Symposium on Software
Testing and Analysis (ISSTA). Association for Computing Machinery, 298–309.

[21] Guoliang Jin, Linhai Song, Wei Zhang, Shan Lu, and Ben Liblit. 2011. Automated Atomicity-Violation Fixing. In
Proceedings of the 32nd ACM SIGPLAN Conference on Programming Language Design and Implementation (PLDI ’11).
Association for Computing Machinery, 389–400.

[22] Guoliang Jin, Wei Zhang, Dongdong Deng, Ben Liblit, and Shan Lu. 2012. Automated Concurrency-Bug Fixing.
In Proceedings of the 10th USENIX Conference on Operating Systems Design and Implementation (OSDI’12). USENIX
Association, 221–236.

[23] Arthur V. Kamienski, Luisa Palechor, Cor-Paul Bezemer, and Abram Hindle. 2021. PySStuBs: Characterizing Single-
Statement Bugs in Popular Open-Source Python Projects. In 2021 IEEE/ACM 18th International Conference on Mining
Software Repositories (MSR). 520–524.

[24] Rafael-Michael Karampatsis and Charles Sutton. 2020. How Often Do Single-Statement Bugs Occur? The ManySStuBs4J

Dataset. Association for Computing Machinery, 573–577.

20

[25] Yalin Ke, Kathryn T. Stolee, Claire Le Goues, and Yuriy Brun. 2015. Repairing Programs with Semantic Code Search.
In Proceedings of the 30th IEEE/ACM International Conference on Automated Software Engineering (ASE). IEEE Press,
295–306.

[26] Dongsun Kim, Jaechang Nam, Jaewoo Song, and Sunghun Kim. 2013. Automatic Patch Generation Learned from
Human-Written Patches. In Proceedings of the 2013 International Conference on Software Engineering (ICSE). IEEE Press,
802–811.

[27] Jindae Kim and Sunghun Kim. 2019. Automatic patch generation with context-based change application. Empirical

Software Engineering (2019), 4071–4106.

[28] Andrew J. Ko, Brad A. Myers, Michael J. Coblenz, and Htet Htet Aung. 2006. An Exploratory Study of How Developers
Seek, Relate, and Collect Relevant Information during Software Maintenance Tasks. IEEE Trans. Softw. Eng. 32, 12 (dec
2006), 971–987.

[29] Claire Le Goues, Michael Dewey-Vogt, Stephanie Forrest, and Westley Weimer. 2012. A Systematic Study of Automated
Program Repair: Fixing 55 out of 105 Bugs for $8 Each. In Proceedings of the 34th International Conference on Software
Engineering (ICSE). IEEE Press, 3–13.

[30] Claire Le Goues, ThanhVu Nguyen, Stephanie Forrest, and Westley Weimer. 2012. GenProg: A Generic Method for

Automatic Software Repair. IEEE Transactions on Software Engineering 38 (2012), 54–72.

[31] Guangjie Li, Hui Liu, Jiahao Jin, and Qasim Umer. 2020. Deep Learning Based Identification of Suspicious Return
Statements. In 2020 IEEE 27th International Conference on Software Analysis, Evolution and Reengineering (SANER).
480–491.

[32] Yi Li, Shaohua Wang, and Tien N. Nguyen. 2020. DLFix: Context-Based Code Transformation Learning for Automated
Program Repair. In 2020 IEEE/ACM 42nd International Conference on Software Engineering (ICSE). Association for
Computing Machinery, 602–614.

[33] Zhen Li, Deqing Zou, Shouhuai Xu, Hai Jin, Yawei Zhu, and Zhaoxuan Chen. 2021. SySeVR: A Framework for Using
Deep Learning to Detect Software Vulnerabilities. IEEE Transactions on Dependable and Secure Computing (2021), 1–1.
[34] Zhen Li, Deqing Zou, Shouhuai Xu, Xinyu Ou, Hai Jin, Sujuan Wang, Zhijun Deng, and Yuyi Zhong. 2018. VulDeePecker:
A Deep Learning-Based System for Vulnerability Detection. Proceedings of the Symposium on Network and Distributed
System Security (2018).

[35] Kui Liu, Anil Koyuncu, Dongsun Kim, and Tegawendé F. Bissyandé. 2019. AVATAR: Fixing Semantic Bugs with
Fix Patterns of Static Analysis Violations. In 26th IEEE International Conference on Software Analysis, Evolution and
Reengineering (SANER). 456–467.

[36] Kui Liu, Anil Koyuncu, Dongsun Kim, and Tegawendé F. Bissyandé. 2019. TBar: Revisiting Template-Based Automated

Program Repair. Association for Computing Machinery, 31–42.

[37] Xuliang Liu and Hao Zhong. 2018. Mining stackoverflow for program repair. In 2018 IEEE 25th International Conference

on Software Analysis, Evolution and Reengineering (SANER). 118–129.

[38] Thibaud Lutellier, Hung Viet Pham, Lawrence Pang, Yitong Li, Moshi Wei, and Lin Tan. 2020. CoCoNuT: Combining
Context-Aware Neural Translation Models Using Ensemble for Program Repair. In Proceedings of the 29th ACM SIGSOFT
International Symposium on Software Testing and Analysis (ISSTA). ACM, 101–114.

[39] Ali Mesbah, Andrew Rice, Emily Johnston, Nick Glorioso, and Edward Aftandilian. 2019. DeepDelta: Learning to Repair
Compilation Errors. In Proceedings of the 2019 27th ACM Joint Meeting on European Software Engineering Conference and
Symposium on the Foundations of Software Engineering (ESEC/FSE). Association for Computing Machinery, 925–936.
[40] Simonetta Montemagni and Vito Pirrelli. 1998. Augmenting WordNet-like lexical resources with distributional evidence.
An application-oriented perspective. Proceedings of the COLING/ACL Workshop on Use of WordNet in Natural Language
Processing Systems, 87–93.

[41] Hoang Duong Thien Nguyen, Dawei Qi, Abhik Roychoudhury, and Satish Chandra. 2013. SemFix: Program Repair via
Semantic Analysis. In Proceedings of the 2013 International Conference on Software Engineering (ICSE). 772–781.
[42] A. Jefferson Offutt, Ammei Lee, Gregg Rothermel, Roland H. Untch, and Christian Zapf. 1996. An Experimental

Determination of Sufficient Mutant Operators. ACM Trans. Softw. Eng. Methodol. 5, 2 (1996), 99–118.

[43] Michael Pradel and Koushik Sen. 2018. DeepBugs: A Learning Approach to Name-Based Bug Detection. Proceedings of

the ACM on Programming Languages 2, OOPSLA (2018), 25 pages.

[44] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and
Peter J. Liu. 2020. Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer. Journal of
Machine Learning Research 21, 140 (2020), 1–67.

[45] Caitlin Sadowski, Edward Aftandilian, Alex Eagle, Liam Miller-Cushon, and Ciera Jaspan. 2018. Lessons from Building

Static Analysis Tools at Google. Commun. ACM 61, 4 (2018), 58–66.

[46] Ripon K. Saha, Yingjun Lyu, Hiroaki Yoshida, and Mukul R. Prasad. 2017. ELIXIR: Effective Object Oriented Program
Repair. In Proceedings of the 32nd IEEE/ACM International Conference on Automated Software Engineering (ASE). 648–659.

21

[47] Roger Scott, Joseph Ranieri, Lucja Kot, and Vineeth Kashyap. 2020. Out of Sight, Out of Place: Detecting and Assessing
Swapped Arguments. In 2020 IEEE 20th International Working Conference on Source Code Analysis and Manipulation
(SCAM). 227–237.

[48] Sooel Son, Vitaly Shmatikov, and Kathryn S McKinley. 2013. FixMeUp: Repairing Access-Control Bugs in Web
Applications. In Network and Distributed System Security Symposium (NDSS) (network and distributed system security
symposium (ndss) ed.).

[49] Luis Sotomayor. 2022. Avoiding JavaScript Scoping Pitfalls. https://nearsoft.com/blog/avoiding-javascript-scoping-

pitfalls. Accessed: 2022-01-06.

[50] Shin Hwei Tan and Abhik Roychoudhury. 2015. Relifix: Automated Repair of Software Regressions. In Proceedings of

the 37th International Conference on Software Engineering - Volume 1 (ICSE). 471–482.

[51] Michele Tufano, Cody Watson, Gabriele Bavota, Massimiliano Di Penta, Martin White, and Denys Poshyvanyk. 2019.
An Empirical Study on Learning Bug-Fixing Patches in the Wild via Neural Machine Translation. ACM Transactions
on Software Engineering and Methodology (2019), 29 pages.

[52] Westley Weimer, Zachary P. Fry, and Stephanie Forrest. 2013. Leveraging Program Equivalence for Adaptive Program
Repair: Models and First Results. In Proceedings of the 28th IEEE/ACM International Conference on Automated Software
Engineering (ASE). 356–366.

[53] Westley Weimer, ThanhVu Nguyen, Claire Le Goues, and Stephanie Forrest. 2009. Automatically Finding Patches
Using Genetic Programming. In Proceedings of the 31st International Conference on Software Engineering (ICSE). IEEE
Computer Society, 364–374.

[54] Mark D. Weiser. 1979. Program slices: formal, psychological, and practical investigations of an automatic program
abstraction method. In Proceedings of the 5th International Conference on Software Engineering (ICSE). 439–449.
[55] Ming Wen, Junjie Chen, Rongxin Wu, Dan Hao, and Shing-Chi Cheung. 2018. Context-Aware Patch Generation for
Better Automated Program Repair. In Proceedings of the 40th International Conference on Software Engineering (ICSE).
1–11.

[56] Ya Xiao, Salman Ahmed, Wenjia Song, Xinyang Ge, Bimal Viswanath, and Danfeng Yao. 2021. Embedding Code
Contexts for Cryptographic API Suggestion: New Methodologies and Comparisons. arXiv preprint arXiv:2103.08747
(2021).

[57] Jifeng Xuan, Matias Martinez, Favio DeMarco, Maxime Clement, Sebastian Lamelas Marcote, Thomas Durieux, Daniel
Le Berre, and Martin Monperrus. 2017. Nopol: Automatic Repair of Conditional Statement Bugs in Java Programs.
IEEE Trans. Softw. Eng. 43, 1 (2017), 34–55.

[58] Fang Yu, Muath Alkhalaf, and Tevfik Bultan. 2011. Patching Vulnerabilities with Sanitization Synthesis. In Proceedings
of the 33rd International Conference on Software Engineering (ICSE ’11). Association for Computing Machinery, 251–260.
[59] Jie Zhou, Ganqu Cui, Shengding Hu, Zhengyan Zhang, Cheng Yang, Zhiyuan Liu, Lifeng Wang, Changcheng Li, and

Maosong Sun. 2020. Graph neural networks: A review of methods and applications. AI Open (2020), 57–81.

[60] Deqing Zou, Sujuan Wang, Shouhuai Xu, Zhen Li, and Hai Jin. 2019. 𝜇VulDeePecker: A Deep Learning-Based System

for Multiclass Vulnerability Detection. IEEE Transactions on Dependable and Secure Computing (2019), 1–1.

22

