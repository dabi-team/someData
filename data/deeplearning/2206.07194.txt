2
2
0
2

n
u
J

4
1

]

G
L
.
s
c
[

1
v
4
9
1
7
0
.
6
0
2
2
:
v
i
X
r
a

Machines Explaining Linear Programs

David Steinmann1,† Matej Zeˇcevi´c1 Devendra Singh Dhami1,3 Kristian Kersting1,2,3

1Computer Science Department, TU Darmstadt, 2Centre for Cognitive Science, TU Darmstadt,
3Hessian Center for AI (hessian.AI), †correspondence: david.steinmann@tu-darmstadt.de

Abstract

There has been a recent push in making machine learning models more interpretable
so that their performance can be trusted. Although successful, these methods have
mostly focused on the deep learning methods while the fundamental optimization
methods in machine learning such as linear programs (LP) have been left out.
Even if LPs can be considered as whitebox or clearbox models, they are not easy
to understand in terms of relationships between inputs and outputs. As a linear
program only provides the optimal solution to an optimization problem, further
explanations are often helpful. In this work, we extend the attribution methods
for explaining neural networks to linear programs. These methods explain the
model by providing relevance scores for the model inputs, to show the inﬂuence
of each input on the output. Alongside using classical gradient-based attribution
methods we also propose a way to adapt perturbation-based attribution methods to
LPs. Our evaluations of several different linear and integer problems showed that
attribution methods can generate useful explanations for linear programs. However,
we also demonstrate that using a neural attribution method directly might come
with some drawbacks, as the properties of these methods on neural networks do not
necessarily transfer to linear programs. The methods can also struggle if a linear
program has more than one optimal solution, as a solver just returns one possible
solution. Our results can hopefully be used as a good starting point for further
research in this direction.

1

Introduction

Linear Programs (LP) are a commonly used technique for optimization, widely applied in schedule
optimization in smart grids [Zhu et al., 2012], to solve combinatorial problems [Paulus et al., 2021] and
most recently resource planning during Covid-19 crisis [Petrovi´c, 2020]. Besides their applicability
to a wide range of problems, their efﬁciency is an important reason for their popularity [Spielman
and Teng, 2004]. Although very popular, there have not been many efforts to make their results
understandable, especially for non-experts in optimization. The only major effort in this direction
is sensitivity analysis [Higle and Wallace, 2003], which is commonly used in conjunction with LPs
and provides information about how much the solution changes if the input parameters are modiﬁed
slightly [Ward and Wendell, 1990].

In recent years, explainable artiﬁcial intelligence (XAI) methods have risen in popularity [Adadi
and Berrada, 2018]. Providing explanations for a machine learning (ML) model is useful in many
different ways. For experts, an explanation allows validating if the behavior of the model is correct,
or if it just follows a Clever Hans behavior [Stammer et al., 2021]. Explaining the model’s reasoning
also helps laymen to build trust in the model as it is easier to comprehend and trust the model’s
decision [Gunning et al., 2019]. XAI may also facilitate the knowledge of humans who work with
the ML system. One category of XAI is attribution methods. These provide contribution scores for
all input features of a given ML model thus highlighting important and irrelevant features. There
are various attribution methods such as integrated gradients [Sundararajan et al., 2017], Grad-CAM

 
 
 
 
 
 
Figure 1: The inverted pendulum on a cart problem (left) can be solved as a ReLU network (middle)
and can be explained via attribution methods. It is also possible to encode the ReLU network as a
linear program via deﬁning constraint for each layer (right). However, even if the LP formulation is
equivalent to the ReLU-net, no equivalent explanation methods exist.

[Selvaraju et al., 2017] or LIME [Ribeiro et al., 2016]. They are mostly focused on deep neural
networks and especially image-based tasks, as it is possible to visualize the attributions as an overlay
over the original image, highlighting important segments. As common as XAI, and speciﬁcally
attribution methods, are for deep neural networks, this topic has not been discussed for LPs. As the
integration of optimizers and deep learning has gained signiﬁcant interest [Amos and Kolter, 2017,
Paulus et al., 2021] it is natural to apply these methods to other types of models.

In this work, we extend the attribution methods to obtain explanations for LPs, which we term XLP,
that share equivalences with neural nets (NN) and further justify the transfer. It has been shown that
ReLU NN [Nair and Hinton, 2010] or even recurrent NN [Rumelhart et al., 1986] might be viewed as
LPs [Wang and Chankong, 1992] (see Fig. 1). We note that several attribution methods that require
a certain model architecture cannot be applied to linear programs directly as the structure of an LP
is different from the structure of a neural network. This makes all propagation-based approaches
not feasible for LPs. Additionally, attribution methods that require an error measurement of the
model cannot be applied directly, as an LP does not have an inherent model error. We make use of
four different attribution methods: saliency maps, gradient times input, integrated gradients, and
a modiﬁed version of occlusion to explain LPs, by computing attributions either for the optimal
solution of the linear program or for some function applied to the optimal solution. As an important
result, we show that standard properties of attribution methods, namely sensitivity, completeness, and
implementation invariance do not hold for LPs.

Overall, we make the following key contributions: (1) We show that neural attribution methods can
be extended to explain LPs. (2) We provide an extensive analysis of the theoretical properties of these
methods on LPs and show that these properties do not necessarily translate from NNs to LPs. (3) We
provide an empirical evaluation of the four attribution methods on four different LP use cases. (4)
Furthermore, we provide an empirical evaluation of a real-world optimization problem and a MAP
inference problem.

2 Background and Related Work

Linear Programs. LPs are a special family of optimization programs, restricted to a linear objective
function and linear constraints [Karloff, 2008]. An LP can be formulated as

x∗ = arg maxx
s.t.

wT x
Ax ≤ b

(1)

where x ∈ Rn is the optimization variable and w ∈ Rn is weight vector. The optimization constraints
are speciﬁed by the matrix A ∈ Rm×n and the vector b ∈ Rm. A, b and w are constant parameters
of the LP. x∗ is called the optimal solution of the LP. Integer programs (ILP) add an additional
restriction requiring x to be a valid integer points. For an LP, the solution map S is a function that
maps from the parameters to the optimal solution. It is deﬁned as: S : A, b, w → x∗. S can be
differentiated thereby allowing for the computation of gradients of an LP or ILP [Paulus et al., 2021,
Agrawal et al., 2019]. Similarly, it is possible to deﬁne the objective map O as the function which
maps from the parameters to the objective function value of the optimal solution.

2

xθxxθθLayer 0Layer 1Layer 2Layer 3arg maxqwTqs.t.CLayer1CLayer2CLayer3q1q2Sensitivity Analysis. Sensitivity analysis is a general method to explain the behavior of a mathemati-
cal model to variations in its input parameters [Saltelli and Annoni, 2010]. A problem is sensitive
to an input parameter if changing this parameter slightly also changes the solution of the model. In
terms of LPs, it is investigated how much the parameter values of w or b can be changed such that
the current optimal basis remains optimal [Koltai and Terlaky, 2000]. It is also reported how fast the
optimal solution value changes within these intervals. The results often depend on the used LP solver,
making them difﬁcult to interpret correctly [Jansen et al., 1997]. Sensitivity analysis differs from the
approach of this work in the sense that we aim to ﬁnd the inﬂuence of inputs on the LP output, not
the input sensitivity.

Neural attribution methods. Given a model M with input x = (x1, · · · , xn), an attribution method
provides attribution scores cM(x) = (c1, · · · , cn), such that each ci describes the relevance of xi on
the output of M.

Saliency maps [Simonyan et al., 2014] are one of the simplest and earliest examples of gradient-based
methods where the backpropagated gradients at the input values are used as attributions. Guided
backpropagation [Springenberg et al., 2015] and deconvolutional nets [Zeiler and Fergus, 2014] both
have a slightly different approach by masking out some of the gradient signals. Instead of propagation
the gradients back through the whole network to the input layer, Grad-CAM uses the gradient signals
at the last convolutional layer to combine both high-level information and provide spatially accurate
attributions [Selvaraju et al., 2017]. These methods can be considered local, as they are based solely
on the gradients. On the other hand, global methods describe the effect of a feature on the output
w.r.t. a baseline, by multiplying the gradient with the model input. Examples are gradient times input
[Shrikumar et al., 2016] or integrated gradients [Sundararajan et al., 2017].

Layerwise relevance propagation [Bach et al., 2015], deep Taylor decomposition [Montavon et al.,
2017] and DeepLIFT [Shrikumar et al., 2017] are all examples of propagation-based attribution
methods. They compute the attributions for the input values by propagating relevance backwards
through the network and explicit rules allow for better attribution control [Shrikumar et al., 2017].
Perturbation-based approaches alter, mask or remove some input features of the model. Attributions
are then obtained by comparing the original model output with the output of the changed features
e.g., Occlusion [Zeiler and Fergus, 2014] or LIME [Ribeiro et al., 2016]. They can be applied to any
model without knowledge about the model architecture but tend to get slower as the input size grows
[Ivanovs et al., 2021].

Generating attributions can also be approached from a causal point of view. It is possible to use
the notion of causality [Pearl, 2009] to interpret a neural network as a structural causal model
[Chattopadhyay et al., 2019]. This allows computing the average causal effects of input neurons
on the output neurons. This inherently causal metric can be used as attribution. CXPlain [Schwab
and Karlen, 2019] provides a framework to generate attributions for an arbitrary model based on
the principle of Granger causality [Granger, 1969]. Similar to a perturbation-based method, they
compute the attributions as a difference between the original model output and the model with a
masked input feature.

3 XLP Using Attribution Methods

Although attribution methods have their ﬂaws [Kindermans et al., 2019], they have proven to be
effective, especially for image-based tasks on neural networks. The concept of attributions can be
directly translated to LPs, speciﬁcally to the solution and the objective map functions. For the solution
map, the attributions show the inﬂuence of all parameters on the optimal solution. When computed
for the objective map, they show the inﬂuence of the parameters on the scalar output of the objective
function.

This view of an LP makes it possible to apply attribution methods in general. However, not all neural
attribution methods are suitable for LPs such as those requiring an NN-like structure of the model (e.g.
convolutional layers as for Grad-CAM), as such structures are not available in an LP. Additionally,
propagation-based approaches can also not be applied directly. The solution map of an LP is a single,
non-linear function and it is not possible to propagate attributions step-by-step through this function
as opposed to a NN. We identify and present four different LP-applicable methods.

3

Saliency: Using saliency maps is one of the simplest approaches to generate attributions via gradients.
The attributions for all input parameters are the partial derivatives of that parameter w.r.t. the model
function [Simonyan et al., 2014]. For an LP M, the saliency attributions are:

cM(xi) =

∂M(x)
∂xi

(2)

where xi is one of {A, b, w} and the model can be either S(A, b, w) or O(A, b, w).

Gradient times Input (GxI): There have been arguments that multiplying the partial derivative of a
parameter with the parameter itself increases the quality of the attribution [Shrikumar et al., 2016].
GxI is an extension of the saliency method in this direction. The partial derivatives of the parameters
w.r.t. model function are multiplied with the parameter values themselves, resulting in the following
formulation, with the same elements as Eq. 2:

cM(xi) = xi

∂M(x)
∂xi

(3)

Integrated Gradients (IG): To obtain the IG attributions, it is necessary to select a baseline point
in the input space of the model which acts as a reference to measure the relevance against. In the
context of an LP, the baseline ¯x consists of the values ¯A, ¯b and ¯w. The attributions are obtained by
integrating the model gradient along the path between baseline and input value.

cM(xi) = (xi − ¯xi) ·

(cid:90) 1

α=0

∂S(˜x)
∂ ˜xi

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)˜x=¯x+α(x−¯x)

dα

(4)

This method was speciﬁcally designed so that the attributions fulﬁll several desirable properties,
which encouraged the evaluation of the theoretical capabilities of attribution methods. The behavior
of attribution methods on LPs w.r.t. these properties is analyzed in more detail in section 4.4. When
this method is applied to classiﬁcation tasks with NNs, a reference point should be neutral in terms
of the classiﬁcation output [Bach et al., 2015]. This approach is not directly suitable for LPs, as they
have no classiﬁcation output. Therefore, we use baseline points corresponding to a neutral problem
state, which are selected based on the general knowledge about the problem.

Occlusion (Occ): This is a simple perturbation-based attribution method [Zeiler and Fergus, 2014]and
computes the attributions for a feature xi under the model M as: cM(xi) = M(x) − M(x[xi=0]).
Intuitively, attributions are deﬁned as the difference between the normal model output and the model
output if the feature is set to zero. This assumes that an input feature with a value of zero does not
inﬂuence the model outcome, which allows us to estimate the true inﬂuence of the feature. However,
setting a parameter to zero in an LP does not always result in a zero inﬂuence of that parameter. If
part of b or w, the change can modify the problem signiﬁcantly (Fig. 9 in supplementary).

We approach the problem from a slightly different perspective. Instead of masking out a single
value of a parameter, which has a possibly large inﬂuence on other parameters, certain structures are
masked out. These structures should be meaningful units by themselves and can be part of multiple
parameters. These structures can be selected so that they do not have an undesired inﬂuence on other
parts of the problem, making it possible to compute their inﬂuence.

This approach has two signiﬁcant advantages: (1) The meaningful structures can be selected ﬂexibly,
whereas the other methods always provide attributions for the parameters. (2) It can be easier to
understand the attributions for meaningful structures, as these already have meaning themselves. The
standard parameters of an LP also have a meaning, but it might be more difﬁcult to understand if
one is not familiar with LPs. However, these advantages also come with a drawback. To compute
attributions, it is necessary to specify the meaningful structures beforehand, the other methods can be
directly applied to an LP in a more “exploratory" way. Note that this approach is not restricted to
occlusion, but can also be applied for other perturbation-based methods.

Computation Complexity: In addition to the accuracy the actual runtime of an attribution method is
an important factor to consider. In general, gradient-based attribution methods are known to be fast,
as they require only a single forward and backward pass through the model (e.g. in case of saliency or
GxI) or a ﬁxed number of these cases (e.g. 100 to approximate the integral for IG). On the other hand,
the complexity of Occ is linear in the number of selected meaningful structures, which can limit its
scalability to large problems. However, computing the gradient for a large LP is time-consuming as
well, so Occ with even a moderate number of structures can be faster than e.g. IG.

4

Figure 2: Process to generate attributions for a problem. We start with an LP. This LP is solved and
the partial derivatives w.r.t. the parameters are computed with CVXPY [Diamond and Boyd, 2016].
Then, an attribution method is used to generate gradient-based attributions.

4 Empirical Analysis

We now evaluate the attribution methods on two LPs (maximum ﬂow (MF) and resource optimization
(RO)) and two ILPs (knapsack (KS) and shortest path (SP)), each with several cases. These problems
were selected to cover the typical use case of linear programs, resource optimization, as well as some
typical combinatorial problems. This evaluation should check if the attribution methods can generate
reasonable attributions for LPs and under which circumstances they might fail (Fig. 2).

4.1 Evaluation Structure: Type Categorization

Evaluating attribution methods is not trivial, as there are no ground-truth attributions available to
compare. Additionally, it is possible to argue that different sets of attributions provide reasonable
explanations of a problem making their evaluation notoriously difﬁcult. This problem has been
tackled by deﬁning metrics to measure the quality of attributions [Ancona et al., 2018]. Unfortunately,
these metrics cannot be applied to LPs, as they either depend on removing speciﬁc elements of the
problem or are designed for image-based tasks [Goh et al., 2020]. Therefore, we follow the more
informal approach to compare the produced attributions and decide how reasonable they are as often
done to compare heatmaps of attributions for image-based NN tasks [Shrikumar et al., 2017].

The concrete settings in all problems are kept small so the attributions can be evaluated by hand. The
problem cases are split into three different types: Type I: These cases describe a typical situation
of an LP, where a unique optimal solution exists. Type II: These cases have more than one optimal
solution. Type III: Especially intricate cases, e.g. implicit constraints like x ≥ 0 are deciding factors.

4.2 Exact Problem Descriptions

The ﬁrst problem is one of the earliest use-cases for linear programming, resource optimization
[Dantzig, 1951]. Given the resource costs and the sale prices of several different products, as well as
a limit for each resource, the objective is to maximize the total revenue. The instance of this problem
for our evaluation has two different products and resources. The problem is evaluated in all three
case types with type III being the edge-case in which implicit constraint x ≥ 0 is relevant. The LP
formulation for the problem with example values is:

x∗ = arg maxx

s.t.

( 1
(cid:18) 1
2

2 )x
(cid:19)

1
1

x ≤

(cid:19)

(cid:18) 10
10

(5)

The maximum ﬂow problem is the second LP for the evaluation [Goldberg and Tarjan, 1988]. Given
a directed graph with designated source and target nodes as well as a maximum capacity for each
edge, the objective is to maximize the ﬂow which reaches the target. Flow can be routed along edges
in the graph under two constraints: The ﬂow along an edge cannot exceed that edge’s capacity and
the incoming and outgoing ﬂow has to be the same for all nodes except source and target. The ﬁrst
example of this problem is type I, whereas the second is type II, as it consists of a bottleneck at
the target node which results in multiple solutions of the same quality. Both cases are on the same
general graph structure (Fig. 3), with different edge weights between them. The full mathematical
formulation of the maximum ﬂow problem can be found in Appendix B.

5

Description

Cases

Type I

Unique optimal solution

RO1, RO2, RO3
MF1, KS1, SP1
Type II Multiple optimal solutions RO4, MF2, KS2

Type III Edge cases

SP2
RO5, KS3

Figure 3 & Table 1: Left: Graph layout for the maximum ﬂow problem. The source and target nodes
are highlighted in green and orange respectively. All edges have the ﬂow/capacity for MF1 annotated.
The numbers are unique edge identiﬁers. Right: Evaluation cases of the used LPs and ILPs.

The third problem, an ILP, is the knapsack problem [Salkin and De Kluyver, 1975]. For a set of items
with a weight and a value each, a subset with the highest value should be found which is still lighter
than a maximum weight. This problem is an integer problem, as each item can either be part of the
subset or not, which results in possible values of 0 and 1 for the optimization variable. The three
cases of the problem cover all three types. The ILP formulation of the problem follows Eq. 1. The
parameter A is a vector of the item weights, b is the total capacity of the knapsack, and w is a vector
of all item values. The knapsack problem has just a single constraint.

The last problem is the shortest path problem [Taccari, 2016]. Given a directed graph with a source
and a target node, the objective is to ﬁnd the shortest path from source to target. Each edge in the
graph has an associated cost. As for MF, there are two cases with a unique optimal solution in the ﬁrst
and two different optimal solutions in the latter case. The graph layout is similar to Fig. 3. The LP
formulation and the exact graph layout can be found in Appendix B. Tab. 1 summarizes the settings.

Selecting Baselines for IG. Selecting a suitable baseline for integrated gradients on LPs is different
from NNs. To select a baseline, it is recommended to use a point in the input space where the
classiﬁcation model has a neutral outcome. However, LPs have no classiﬁcation setting. Therefore,
we selected points in the input space which can be considered neutral based on general problem
knowledge. An example is the baseline with equal value and weight for all items in KS, resulting in
no preference for any item at the baseline. More details can be found in Appendix B.

Meaningful Structures for Occ. The method Occ requires the selection of problem parts for which
the attributions should be computed. For the graph-based problems, all edges are considered as such.
For RO, each resource, and for KS each item is used as a meaningful structure.

4.3 Systematic Discussion of Results

For the type I cases, the generated attributions are overall reasonable, with some notable exceptions.
The attributions provided by the saliency method are skewed if the input parameters have largely
different sizes such as in RO1. The parameter inputs and the results are shown in Tab. 2. The input
has b (cid:29) A, which results in attributions for c(b) (cid:28) c(A). However, as both A and b are part of
the same linear constraints, their inﬂuence on the problem should be on a similar level.

In general, attribution methods struggle to capture the inﬂuence of combined parameters, for example,
the parameter A, which encodes the graph structure via the incidence matrix, on MF and SP. Each
element of A is not meaningful by itself, but only in combination with other elements. The gradient-
based methods do not provide reasonable attributions for this parameter (Appendix C). These methods
can only provide reasonable attributions for the edge weights.

Overall type I cases, IG only provides reasonable attributions with a near-zero baseline. The
attributions generated by other baselines were either noisy or they were even misleading as shown
in case MF1 in Tab. 2. At the baseline, all edges have the same capacity. However, the attributions
indicate that only the last two edges are relevant, which is not reasonable, as all edges are saturated
and thus equally important in this example.

For type II cases, the attribution methods provided mixed results. In nearly all cases, the attributions
for the solution map were not able to highlight that there is more than one optimal solution. Instead,
the attributions were similar to the corresponding type I cases. The reason for this behavior is that the
LP solver just returns a single solution and does not provide any hints that more than one optimal

6

124530.2/0.20.4/0.40.5/0.50.1/0.10.6/0.6 0.7/0.8 0.4/0.41245376RO1

Par

Sal

GxI

(cid:19)

A & c(A)
(cid:18) 1.0 1.0
2.0
1.0
(cid:18) 0.0 −16.0
0.0
(cid:18) 0.0 −16.0
0.0

0.0

0.0

b & c(b)
(cid:19)
(cid:18) 8.0
10.0
(cid:19) (cid:18) 2.0
0.0
(cid:19) (cid:18) 16.0
0.0

(cid:19)

(cid:19)

MF1

Grad
GxI
IG-nz
IG-as

c(b)

(0.0, 0.33, 0.33, 0.42, 0.09, 0.67, 0.59)T
(0.0, 0.07, 0.20, 0.04, 0.04, 0.27, 0.29)T
(0.0, 0.07, 0.20, 0.04, 0.04, 0.26, 0.27)T
(0.0, 0.00, 0.00, 0.00, 0.00, 0.39, 0.49)T

(b) Case MF1

(a) Case RO1

Table 2: (a) The value of the parameters (Par) b are 8 times larger than the value of A. This causes the
saliency (Sal) attributions for b to be only 1/8 of the attributions for A. The attributions for gradient
times input (GxI) are more suitable in this case. (b) When comparing the attributions between a
near-zero baseline for IG (IG-nz) and the baseline with the same values for all edges (IG-as), the
latter are rather misleading, as they do not show any inﬂuence for all edges except the last two.

(a) RO4 solutions

(b) RO3 attributions

(c) R04 attributions

Figure 4: (a) All possible optimal solutions for case RO4 with selected solution highlighted in orange.
(b) and (c) Comparison of the attributions for the cases RO3 and RO4. At case RO3, there is a
unique optimal solution, and the attributions provide reasonable information. For the case RO4, the
attributions are still relatively similar to case RO3, even if here multiple optimal solutions are possible.
The attributions are not able to provide information that multiple optimal solution exist.

solution might exist as shown in Fig. 4. This behavior can be observed in all four problems. The
objective map attributions have similar problems, except for RO4. Here, the attributions highlight
that just one constraint is relevant, indicating that multiple optimal solutions exist. The attributions
generated with Occ highlight that there is more than one optimal solution in all cases, however, they
do not provide further insights which makes them not sufﬁcient to explain such a situation. Otherwise,
the observations for the type I cases about the saliency attribution scaling, attributions for combined
parameters, and different baselines for IG are also true for the type II cases.

The type III cases only consist of two different situations. In RO5, both constraints are active at the
optimal solution and the solution is also limited by the implicit constraint x ≥ 0. As the implicit
constraint is not deﬁned by any parameters, the attribution methods fail, resulting in all methods
providing misleading attributions for this case (see Appendix C). The second type III example is KS3.
Here, the normal set of items was extended by one very heavy item which cannot be selected, as its
weight alone exceeds the knapsack limit. However, this item is very valuable. This causes misleading
gradients for the solution map, and therefore also misleading attributions of the gradient-based
methods (Tab. 3). Intuitively, it is expected that the attributions for the weight of all items have a
-ve sign, as a larger weight, in general, corresponds to a reduced total value of the selected weights,
because fewer items can be selected. However, the attributions for items 3 to 7 have a +ve sign,
indicating the opposite connection which is not plausible. The attributions of Occ for this case were
reasonable, as this method is not based on gradients.

Scaling Attributions - Real World Problem of Energy System Design. Real-world problems
are often substantially more complex. This results in larger problems with more parameters, and

7

010203040-202468101214x10-22468101214x2010203040-202468101214x10-22468101214x2RO3RO4KS3

Sal
GxI
IG

Occ

c(A)

(−0.08, −0.19, 0.04, 0.02, 0.04, 0.01, 0.03)T
(−1.65, −1.88, 0.18, 0.06, 0.24, 0.02, 0.10)T
(−1.63, −1.86, 0.17, 0.06, 0.24, 0.02, 0.10)T

Attributions per item
(0.00, 8.00, 0.00, 0.00, 0.00, 0.00, 0.00)T

Table 3: This table shows the gradient-based attributions w.r.t. A for the objective map of case KS3.
As A describes the item weights, there should be a -ve correlation between the values of A and
the total value of the objective function. However, the attributions for some items have a +ve sign,
indicating that increasing the weight of these items has a +ve inﬂuence on the total value, which is
not reasonable. This behavior occurs for all three gradient-based methods. Occ is not affected by this
(the Occ attributions show inﬂuence of a whole item on the result, which is always +ve or zero).

Month
Occ

J F M A M J J A S O N D
1 4 5 7 4 0 0 0 3 4 1 0

Table 4: Occ attributions show the inﬂuence of each month on the battery capacity. Large values
(green) mean high inﬂuence, small values (orange) correspond to low inﬂuence.

therefore more attribution values. The increased number of values can make the attributions harder to
understand. One example is an LP from the PlexPlain program [Frank Jäkel, 2020], which models the
energy level of a small house with a photovoltaic (PV) system. The full LP is shown in the supplement.
As the energy level is modeled on an hourly basis for a year, it has over 40000 different problem
variables. By using Occ, the problem can be split into a small number of high-level meaningful
structures, in this case, the months. In Tab. 4, the inﬂuence of each month on the battery capacity of
the PV system is shown. It can be observed that the months in the spring and autumn have the largest
inﬂuence. This is reasonable, as the battery is the most important during these seasons because it
carries excess energy produced during the day into the evening. To explore the problem in greater
depth, it would now be possible to select more ﬁne-grained meaningful structures to look at.

4.4 LP Attribution Methods Properties

For attribution methods, it is beneﬁcial to have properties that describe the behavior of a method
especially because it might be difﬁcult to evaluate attributions for larger problems by hand. The
following three properties are all proven to hold for Saliency, GxI, and IG (if they apply to that
method). However, it turns out that they do not hold anymore for these methods when applied to LPs.

Sensitivity: describes that the attributions are generated according to relations between input and
output [Sundararajan et al., 2017]. It can be split into two parts: If changing a model feature results
in a change in the model output, that feature should have non-zero attributions. Additionally, if the
output of a model remains the same despite a feature change, this feature should have zero attributions.
This property is relevant for any kind of model and says that basic relations between input and output
should be visible in the attributions. As shown in Tab. 2, the ﬁrst part of sensitivity is violated for
the gradient-based methods. The attributions for the capacity of edge 1 are zero for all methods,
but the edge has a large inﬂuence on the problem as it is responsible for 7/9 of the total ﬂow. The
violation of the second part can be observed in Tab. 3. All methods have non-zero attributions for the
ﬁrst item, even if it does not affect the problem. Occ violates the ﬁrst part of sensitivity in RO5. In
theory, the second part should hold for Occ, as removing a non-relevant item from an LP should not
change its solution. However, if such an item is removed and there were previously multiple optimal
solutions, the solver may choose a different optimal solution, resulting in non-zero attributions. (See
SP2, Appendix C).

Completeness: also known as summation to delta [Shrikumar et al., 2017] only applies to attri-
bution methods having a baseline. The attribution method fulﬁlls completeness, if the condition
(cid:80) cM(x, ¯x) = M(x) − M(¯x) is true i.e., attributions of a model at input x compared to a baseline ¯x

8

Occ X1 X2 X3
E12
E23

-1
-1

-1
0

0
0

Figure 5: Example graph G. Here, X1=1, X2=0
and X3 = equally likely 0 or 1.

Table 5: Occ attributions showing the inﬂu-
ence of the edges on the variable states.

should sum up to the difference between the model output at x and ¯x. Again, IG fulﬁlls this property
for NNs, but not for LPs.

Implementation Invariance: states that the attributions for two functionally equivalent, but differ-
ently implemented models should be the same [Sundararajan et al., 2017]. This is reasonable, as
the attributions should not depend on the speciﬁc implementation of the model, but rather on the
function the model expresses. All considered attribution methods fulﬁll implementation invariance on
NNs. Unfortunately, it can be shown that neither of these methods fulﬁlls implementation invariance
on LPs. The reason behind this is that the solution of an LP, and therefore also its gradients and
the attributions, sometimes depend on the LP solver. If an LP has multiple optimal solutions, the
solver returns any of them. The returned solution depends not only on the type of solver but also
on its implementation. Therefore, it can (and will) happen that two differently implemented solvers
will return different optimal solutions resulting in different gradients leading to different attribution
values for the same problem. One could argue that the solver is technically not part of the model.
Nevertheless, the attributions depend on the LP solver if the LP has more than one optimal solution
and such cases can occur regularly [Koltai and Terlaky, 2000]. Therefore, this ﬁnding is important
when attribution methods are applied to LPs. The solver dependence also prevents other properties of
attributions methods on LPs. For e.g, Occ would fulﬁll the second part of sensitivity if the method
would not be solver-dependent.

Further Properties: There are other proposed properties which are not as relevant for LPs, as for
example input invariance [Kindermans et al., 2019] or linearity. These properties are especially
desirable if the model has an NN-like structure and are therefore not as important for LPs.

5

Implications of XLP

If LPs were to be “properly explainable" (refering to an intuitive, human level understanding), then
this would have incredible implications for science and industry in general (well beyond artiﬁcial in-
telligence (AI) research). Climate-/energy systems researcher could design sustainable infrastructure
to cover long-term energy demand [Schaber et al., 2012]. However, LP explainability also naturally
comes with strong implications within AI research directly due to established equivalences/use-cases
like the discussed relationship between NNs and LPs or for instance in quantifying uncertainty and
probabilistic reasoning as in MAP inference for acquiring the “most probable assignment",

arg max
x∈X

P (x; θθθ) = arg max
x∈X

1
Z(θθθ)

exp((cid:104)θθθ, φ(x)(cid:105)) = arg max
µµµ∈M

(cid:104)θθθ, µµµ(cid:105)

(6)

Here X denotes the space of random variables Xi and P (x; θθθ) is the probability density function
of a Markov Random Field (MRF) with parameters θθθ, sufﬁcient statistic φ. Further M is called
the marginal polytope of the MRF’s graph [Wainwright et al., 2008] (formally, M(G) = {µµµ |
∃θθθ. (µµµ = E[φ(x)])} with graph G, informally, the convex hull of the vectors φ(x)). It is known that
several NP-hard combinatorial optimization problems can solved by exploiting the equivalence in
Eq. 6 [Sontag, 2010], thereby, possibly allowing for a “transportability" of explainability to such
optimization problems (e.g. maximum cuts).

Attributions showcase: Given graph G in Fig. 5 which consists of three different random variables
X1, X2 and X3 each with binary states. X1 is more likely to be 0, X3 is more likely to be 1 and X2
is equally likely in both states. The edges cause connected nodes to have more likely the same state.
This results in all three variables having a zero state in the MAP. The exact values for φ and θθθ are
presented in Appendix E. With the transformation to an LP shown in Eq. 6, it is possible to compute
attributions. Tab. 5 shows the Occ attributions for the edges in terms of the variable states instead of

9

X1X3X2µµµ, as µµµ only encodes the variable state, for a more condensed view. The attributions show that the
edges have a negative inﬂuence on some nodes, causing them to be zero at MAP.

6 Conclusions and Future Work

We showed that generating attributions for linear programs with neural attribution methods is indeed
possible, opening a new possible research area, XLP, to explain linear programs. Evaluation of four
different attribution methods on several different LPs/ILPs showed that the methods have varying
success. GxI and Occ provided overall the most reasonable attributions, especially in cases with a
unique optimal solution. Saliency and IG were not as effective, because the former struggles with
differently scaled input values and the baseline selection for the latter was ineffective. However,
all methods struggled generally when LPs had more than one optimal solution. We showed that
properties of attribution methods that are valid for NNs get violated in LPs. As future work, solving
the dependency of the attribution methods on the solver can make attribution methods for LPs apply
better. The development of evaluation metrics for these methods is also an important task. Finally,
designing better attribution methods speciﬁcally tailored to LPs is an important future direction.

Acknowledgments

This work was supported by the ICT-48 Network of AI Research Excellence Center “TAILOR” (EU
Horizon 2020, GA No 952215), the Nexplore Collaboration Lab “AI in Construction” (AICO) and by
the Federal Ministry of Education and Research (BMBF; project “PlexPlain”, FKZ 01IS19081). It
beneﬁted from the Hessian research priority programme LOEWE within the project WhiteBox and
the HMWK cluster project “The Third Wave of AI” (3AI).

References

Amina Adadi and Mohammed Berrada. Peeking inside the black-box: a survey on explainable

artiﬁcial intelligence (xai). IEEE access, 6:52138–52160, 2018.

Akshay Agrawal, Brandon Amos, Shane T. Barratt, Stephen P. Boyd, Steven Diamond, and J. Zico
Kolter. Differentiable convex optimization layers. CoRR, abs/1910.12430, 2019. URL http:
//arxiv.org/abs/1910.12430.

Brandon Amos and J Zico Kolter. Optnet: Differentiable optimization as a layer in neural networks.

In International Conference on Machine Learning, pages 136–145. PMLR, 2017.

Marco Ancona, Enea Ceolini, Cengiz Öztireli, and Markus Gross. Towards better understanding of
gradient-based attribution methods for deep neural networks. In 6th International Conference on
Learning Representations, ICLR 2018, Vancouver, BC, Canada, April 30 - May 3, 2018, Confer-
ence Track Proceedings. OpenReview.net, 2018. URL https://openreview.net/forum?id=
Sy21R9JAW.

Sebastian Bach, Alexander Binder, Grégoire Montavon, Frederick Klauschen, Klaus-Robert Müller,
and Wojciech Samek. On pixel-wise explanations for non-linear classiﬁer decisions by layer-wise
relevance propagation. PloS one, 10(7):e0130140, 2015.

Aditya Chattopadhyay, Piyushi Manupriya, Anirban Sarkar, and Vineeth N. Balasubramanian. Neural
network attributions: A causal perspective. CoRR, abs/1902.02302, 2019. URL http://arxiv.
org/abs/1902.02302.

George B Dantzig. Maximization of a linear function of variables subject to linear inequalities.

Activity analysis of production and allocation, 13:339–347, 1951.

Steven Diamond and Stephen Boyd. CVXPY: A Python-embedded modeling language for convex

optimization. Journal of Machine Learning Research, 17(83):1–5, 2016.

Florian Steinke Frank Jäkel.

Plexplain - explaining linear programs, 2020.

URL

https://www.softwaresysteme.pt-dlr.de/media/content/Projektblatt_
PlexPlain_01IS19081.pdf.

10

Gary S. W. Goh, Sebastian Lapuschkin, Leander Weber, Wojciech Samek, and Alexander Binder.
Understanding integrated gradients with smoothtaylor for deep neural network attribution. CoRR,
abs/2004.10484, 2020. URL https://arxiv.org/abs/2004.10484.

Andrew V Goldberg and Robert E Tarjan. A new approach to the maximum-ﬂow problem. Journal

of the ACM (JACM), 35(4):921–940, 1988.

Clive WJ Granger. Investigating causal relations by econometric models and cross-spectral methods.

Econometrica: journal of the Econometric Society, pages 424–438, 1969.

David Gunning, Mark Steﬁk, Jaesik Choi, Timothy Miller, Simone Stumpf, and Guang-Zhong Yang.

Xai—explainable artiﬁcial intelligence. Science Robotics, 4(37):eaay7120, 2019.

Julia L Higle and Stein W Wallace. Sensitivity analysis and uncertainty in linear programming.

Interfaces, 33(4):53–60, 2003.

Maksims Ivanovs, Roberts Kadikis, and Kaspars Ozols. Perturbation-based methods for explaining
deep neural networks: A survey. Pattern Recognit. Lett., 150:228–234, 2021. doi: 10.1016/j.patrec.
2021.06.030. URL https://doi.org/10.1016/j.patrec.2021.06.030.

Benjamin Jansen, JJ De Jong, Cornelius Roos, and Tamás Terlaky. Sensitivity analysis in linear
programming: just be careful! European Journal of Operational Research, 101(1):15–28, 1997.

Howard Karloff. Linear programming. Springer Science & Business Media, 2008.

Pieter-Jan Kindermans, Sara Hooker, Julius Adebayo, Maximilian Alber, Kristof T Schütt, Sven
Dähne, Dumitru Erhan, and Been Kim. The (un) reliability of saliency methods. In Explainable
AI: Interpreting, Explaining and Visualizing Deep Learning, pages 267–280. Springer, 2019.

Tamás Koltai and Tamás Terlaky. The difference between the managerial and mathematical interpre-
tation of sensitivity analysis results in linear programming. International Journal of Production
Economics, 65(3):257–274, 2000.

Grégoire Montavon, Sebastian Lapuschkin, Alexander Binder, Wojciech Samek, and Klaus-Robert
Müller. Explaining nonlinear classiﬁcation decisions with deep taylor decomposition. Pattern
Recognit., 65:211–222, 2017. doi: 10.1016/j.patcog.2016.11.008. URL https://doi.org/10.
1016/j.patcog.2016.11.008.

Vinod Nair and Geoffrey E Hinton. Rectiﬁed linear units improve restricted boltzmann machines. In

Icml, 2010.

Anselm Paulus, Michal Rolínek, Vít Musil, Brandon Amos, and Georg Martius. Comboptnet: Fit the
right np-hard problem by learning integer programming constraints. CoRR, abs/2105.02343, 2021.
URL https://arxiv.org/abs/2105.02343.

Judea Pearl. Causality. Cambridge university press, 2009.

Nenad Petrovi´c. Simulation environment for optimal resource planning during covid-19 crisis. In
2020 55th International Scientiﬁc Conference on Information, Communication and Energy Systems
and Technologies (ICEST), pages 23–26. IEEE, 2020.

Marco Túlio Ribeiro, Sameer Singh, and Carlos Guestrin. "why should I trust you?": Explaining
the predictions of any classiﬁer. CoRR, abs/1602.04938, 2016. URL http://arxiv.org/abs/
1602.04938.

David E Rumelhart, Geoffrey E Hinton, and Ronald J Williams. Learning representations by

back-propagating errors. nature, 323(6088):533–536, 1986.

Harvey M Salkin and Cornelis A De Kluyver. The knapsack problem: a survey. Naval Research

Logistics Quarterly, 22(1):127–144, 1975.

Andrea Saltelli and Paola Annoni. How to avoid a perfunctory sensitivity analysis. Environmental

Modelling & Software, 25(12):1508–1517, 2010.

11

Katrin Schaber, Florian Steinke, and Thomas Hamacher. Transmission grid extensions for the
integration of variable renewable energies in europe: Who beneﬁts where? Energy Policy, 43:
123–135, 2012.

Patrick Schwab and Walter Karlen. Cxplain: Causal explanations for model interpretation under

uncertainty. CoRR, abs/1910.12336, 2019. URL http://arxiv.org/abs/1910.12336.

Ramprasaath R. Selvaraju, Michael Cogswell, Abhishek Das, Ramakrishna Vedantam, Devi Parikh,
and Dhruv Batra. Grad-cam: Visual explanations from deep networks via gradient-based localiza-
tion. In IEEE International Conference on Computer Vision, ICCV 2017, Venice, Italy, October
22-29, 2017, pages 618–626. IEEE Computer Society, 2017. doi: 10.1109/ICCV.2017.74. URL
https://doi.org/10.1109/ICCV.2017.74.

Avanti Shrikumar, Peyton Greenside, Anna Shcherbina, and Anshul Kundaje. Not just a black box:
Learning important features through propagating activation differences. CoRR, abs/1605.01713,
2016. URL http://arxiv.org/abs/1605.01713.

Avanti Shrikumar, Peyton Greenside, and Anshul Kundaje. Learning important features through
propagating activation differences. CoRR, abs/1704.02685, 2017. URL http://arxiv.org/
abs/1704.02685.

Karen Simonyan, Andrea Vedaldi, and Andrew Zisserman. Deep inside convolutional networks:
Visualising image classiﬁcation models and saliency maps. In Yoshua Bengio and Yann LeCun,
editors, 2nd International Conference on Learning Representations, ICLR 2014, Banff, AB, Canada,
April 14-16, 2014, Workshop Track Proceedings, 2014. URL http://arxiv.org/abs/1312.
6034.

David Alexander Sontag. Approximate inference in graphical models using LP relaxations. PhD

thesis, Massachusetts Institute of Technology, 2010.

Daniel A. Spielman and Shang-Hua Teng. Smoothed analysis of algorithms: Why the simplex
algorithm usually takes polynomial time. J. ACM, 51(3):385–463, 2004. doi: 10.1145/990308.
990310. URL https://doi.org/10.1145/990308.990310.

Jost Tobias Springenberg, Alexey Dosovitskiy, Thomas Brox, and Martin A. Riedmiller. Striving
In Yoshua Bengio and Yann LeCun, editors, 3rd
for simplicity: The all convolutional net.
International Conference on Learning Representations, ICLR 2015, San Diego, CA, USA, May 7-9,
2015, Workshop Track Proceedings, 2015. URL http://arxiv.org/abs/1412.6806.

Wolfgang Stammer, Patrick Schramowski, and Kristian Kersting. Right for the right concept: Revising
neuro-symbolic concepts by interacting with their explanations. In Proceedings of the IEEE/CVF
Conference on Computer Vision and Pattern Recognition, pages 3619–3629, 2021.

Mukund Sundararajan, Ankur Taly, and Qiqi Yan. Axiomatic attribution for deep networks. CoRR,

abs/1703.01365, 2017. URL http://arxiv.org/abs/1703.01365.

Leonardo Taccari. Integer programming formulations for the elementary shortest path problem.

European Journal of Operational Research, 252(1):122–130, 2016.

Martin J Wainwright, Michael I Jordan, et al. Graphical models, exponential families, and variational

inference. Foundations and Trends in Machine Learning, 1(1–2):1–305, 2008.

Jun Wang and Vira Chankong. Recurrent neural networks for linear programming: Analysis and

design principles. Computers & operations research, 19(3-4):297–311, 1992.

James E Ward and Richard E Wendell. Approaches to sensitivity analysis in linear programming.

Annals of Operations Research, 27(1):3–38, 1990.

Matthew D Zeiler and Rob Fergus. Visualizing and understanding convolutional networks.

In

European conference on computer vision, pages 818–833. Springer, 2014.

Ziming Zhu, Jie Tang, Sangarapillai Lambotharan, Woon Hau Chin, and Zhong Fan. An integer
linear programming based optimization for home demand-side management in smart grid. In 2012
IEEE PES innovative smart grid technologies (ISGT), pages 1–5. IEEE, 2012.

12

A Appendix for “Machines Explaining Linear Programs”

We make use of this appendix following the main paper to provide additional details.

Code

We have made our
Machines-Explaining-Linear-Programs-75D2/README.md

code publicly available

at https://anonymous.4open.science/r/

B Problem speciﬁcations

Maximum Flow Problem: For a given directed graph G = (V, E) with nodes V and edges E, the
goal is to maximize the ﬂow from a selected source node s to the designated target node t. The
problem can be described as follows:

maximize

(cid:88)

f (u, t)

u:(u,t)∈E

subject to f (u, v) ≥ 0

∀u, v ∈ V

(cid:88)

−

f (u, v) +

(cid:88)

= 0 ∀v ∈ V \ {s, t}

u:(u,v)∈E

w:(v,w)∈E

f (u, v) ≤ c(u, v)

∀(u, v) ∈ E

(7)

In this formulation, f (u, v) describes the ﬂow from node u to node v and c(u, v) is the capacity of
the directed edge from u to v. The ﬁrst constraint ensures that the ﬂow is always positive while
the second constraint assures that the incoming and outgoing ﬂow is the same for all nodes except
source and target. The last constraint guarantees that the ﬂow on each edge does not exceed the edge
capacity.

For the evaluation of this problem, the LP formulation is made more concise with the use of the
incidence-matrix. In the problem formulation, the optimization variable x ∈ Rn describes the
ﬂow along each edge. The vector w ∈ Rn is the negation of the target node row of the incidence
matrix and describes which edges are connected to the target. The graph structure is encoded in
A ∈ R(m−2)×n which holds all rows of the incidence matrix except the row for the source and the
target node. The vector b ∈ Rn describes the capacities for all edges. It is necessary that the ordering
of edges in A, b, and w is the same. The full formulation is presented in Eq. 8.

wT x
maximize
subject to Ax = 0
x ≤ b
x ≥ 0

(8)

Shortest Path Problem: Given a directed graph G = (V, E), with designated source node s and
target node t as well as edge costs wij, the objective is to ﬁnd the path which connects s and t with
the lowest combined edge weights. The optimization variables xij for all edges (i, j) ∈ E describe if
the said edge is part of the optimal path or not. The optimization problem can then be formulated as
follows:

minimize

(cid:88)

wijxij

(i,j)∈E

subject to ∀i :

(cid:88)

j

xij −

(cid:88)

j

xij =

xij ∈ {0, 1}






1,
−1,
0,

if i = s;
if i = t;
otherwise

∀(i, j) ∈ E

(9)

13

Figure 6: The graph for the shortest path problem. The source and target node are highlighted in
green and orange respectively. The edge weights and the optimal solution are shown for SP1. The
red numbers are unique edge identiﬁers.

Similar to MF, it is possible to provide a more concise formulation for this problem as well. For this
problem A ∈ Rm×n is the incidence-matrix. Next, b ∈ Rn is a vector of all zeros, except for a 1 at
the position of the source node and a -1 at the target node. Lastly, w ∈ Rn is the vector of all edge
costs. The ordering of the edges in A, b, and w has to be the same. This results in the following
formulation:

maximize
subject to
with

cT x
Ax = b
x ≥ 0
x ∈ {0, 1}n

(10)

The graph for SP is shown in Fig. 6.

PlexPlain Problem: The LP models the energy level of a simple house with a PV system and battery
storage. Given the energy demand of the house over a year and the energy produced by a PV system,
the capacity of the PV and the battery should be optimized. As the model spans a year and the energy
levels are described on an hourly basis, the linear program consists of thousands of variables. The
full LP formulation is given in Eq. 11

minimize

coPV capPV + cobat capbat + cobuy

(cid:88)

i

ebuy,i

subject to

ePV,i ≤ capPV availablePV,i

ebat,i ≤ capbat

(11)

ebat, start = ebat, end − e-outstart + e-instart

ebat,i = ebat,i−1 − e-outi + e-ini ∀i ≥ 1

demandi = ebuy,i + e-outi − e-ini + ePV,i

where “co” describes the cost and “cap” the capacity of an element. All variables with “bat” are
related to the battery storage and “buy” is related to buying energy from the grid. All variables with
“e” describe the energy of the element, for example ePV is the produced energy of the PV-system. The
LP has 24 ∗ 365 = 8760 time steps and if not speciﬁed otherwise, 0 ≤ i ≤ 8760.

Parameter values for all cases: In Tab. 6, 7, 8 and 9, the full parameter values for all cases are
provided. For MF the graph structure is provided in the main paper. The graph structure for SP is
shown in Fig. 6. The Parameters for these problems are given as information about the edges, as the
general graph structure is the same for the different cases of these problems.

Baselines for IG: Several different baselines were tested for IG: As default value, the near-zero
baseline was used. The values of the baseline are all parameter values divided by 100. This is a
relatively close approximation of a zero baseline, which is often not feasible in LPs. The other
baselines were different between problems. For RO, three other baselines were tested, a baseline in
which constraint 1 is the active constraint, one with constraint 2 active, and the last one with both

14

134520.54.22.11.22.01.8215364A

b

w

RO1

RO2

RO3

RO4

RO5

(cid:18) 1.0
2.0
(cid:18) 1.0
2.0
(cid:18) 1.0
2.0
(cid:18) 1.0
2.0
(cid:18) 1.0
2.0

(cid:19)

(cid:19)

(cid:19)

(cid:19)

(cid:19)

1.0
1.0
1.0
1.5
2.25
1.0
1.0
1.0
2.0
1.0

(8.0, 10.0)

(1.0, 2.0)

(10.0, 10.0)

(1.0, 2.0)

(10.0, 10.0)

(1.0, 2.0)

(10.0, 10.0)

(1.0, 2.0)

(10.0, 10.0)

(1.0, 2.0)

Table 6: Parameters for the resource-optimization problem.

Edge

1

MF1
MF2

0.8
0.8

2

0.2
0.2

3

0.6
0.6

4

0.1
0.3

5

0.4
0.4

6

0.4
0.2

7

0.5
0.3

Table 7: Edge capacities for the maximum ﬂow problem cases. Both cases work with the same graph
structure.

constraints active. For MF and SP, a baseline with the same weights for all edges was used in addition
to the near-zero baseline. For KS, a baseline with the average values and weights of all items (divided
by 10) was used as well. The division by 10 is done to ensure that the baseline values are smaller than
the values of the original input, otherwise, the attributions for the parameters would have different
signs.

C Detailed Results

Attributions for the incidence matrix: One of the situations where the gradient-based methods fail
to produce reasonable attributions for parameters with combined meaning is the incidence matrix for
the graph-based problems. The parameter A for the ﬁrst case of the maximum ﬂow problem is the
following:.



A =




e1
n2 −1
n3
n4

e3
e2
0
1
0 −1 −1
0
0

e5
e4
0
1
0
1
0 −1 −1

e6
0
1
0

e7
0
0
1






(12)

This parameter hold the full incidence-matrix, except the rows for the source and target nodes (n1
and n5, respectively). The saliency attributions w.r.t. A for the objective map are:

(cid:32) 0.0

0.0

0.0

0.0
−0.23 −0.07 −0.20 −0.03 −0.13 −0.13 −0.16
−0.29 −0.08 −0.25 −0.04 −0.17 −0.17 −0.21

0.0

0.0

0.0

(cid:33)

(13)

Item

1

2

3

4

5

6

7

b

3.0
KS1 w
KS1 A 5.0
KS2 w
4.0
KS2 A 4.0
KS3 w 50.0 15.0 3.0 2.0 4.0 2.0 3.0
KS3 A 20.0 10.0 5.0 3.0 6.0 2.0 4.0

2.0 4.0 2.0 2.0
3.0 6.0 2.0 4.0
4.0 1.0 3.0
6.0 3.0 3.0

10.0

10.0

10.0

Table 8: Parameters for the knapsack problem.

15

Edge

1

SP1
SP2

0.5
0.5

2

2.0
2.0

3

1.8
1.8

4

4.2
3.9

5

1.2
1.2

6

2.1
2.1

Table 9: Edge weights for the shortest path problem cases. The graph structure is the same in both
cases.

Figure 7: Visualization of RO5. The optimal solution is at the intersection of both constraints. Due to
the limitation of x ≥ 0, the optimal solution is also at the x2-axis.

It does not seem reasonable that the attributions for the second node are all zero, as this node is
responsible for 7/9 of the total ﬂow to the target node. Additionally, the attributions for the other two
nodes are all negative, which is also not plausible. This or similar behavior occurs in all cases when
attributions for the incidence matrix are computed.

More details about RO5: In this case, the problem has a unique optimal solution, which is at the
intersection between both constraints and the x2-axis (Fig. 7). Both constraints and the implicit
constraint x ≥ 0 are relevant. Additionally, the inﬂuence of both constraints should be similar, as
they inﬂuence the optimal solution in the same way.

However, all gradient-based attribution methods show unbalanced inﬂuence for the parameters of both
constraints. The results of Occ were even more misleading, as this method assigns zero attributions
to both constraints. The results for the gradient-based methods are displayed in Tab. 10.

Violations of Sensitivity: Occlusion does violate the sensitivity property on LPs, which can be
observed in two different cases. The ﬁrst part of sensitivity is violated in case RO5, where the method

Method

Grad

GxI

IG

c(A)
(cid:18) 0.0 −16.48
0.0 −3.52
(cid:18) 0.0 −16.48
0.0 −3.52
(cid:18) 0.0 −12.50
0.0 −7.30

c(b)
(cid:19) (cid:18) 1.65
0.35
(cid:19) (cid:18) 16.48
3.52
(cid:19) (cid:18) 12.50
7.30

(cid:19)

(cid:19)

(cid:19)

Table 10: Attributions for the objective map for RO5. It is expected that both constraints have a
similar inﬂuence on the problem, which is not visible in the attributions. For IG, the results with the
near-zero baseline are shown.

16

0102030400-22468101214x2-202468101214x1Figure 8: Graph for SP2. The source node is highlighted in green and the target node in orange. The
ﬁrst optimal path is shown in orange and the second one in blue. The edge numbers are shown in red.

Occ

c(e2)
c(e5)

e1

0
0

e2

none
0

e3

1
1

e4

-1
-1

e5

0
none

e6

1
1

Table 11: Occlusion attributions for case SP2.

provides zero attributions for both constraints, even if both of them are relevant in this case (see Fig.
7).

The second part of sensitivity states, that the attributions for a parameter should be zero if that
parameter does not inﬂuence the problem. One would assume that this property is valid for Occ, as
removing a non-relevant part of the problem should not change the problem outcome, and therefore
result in zero attributions. However, this is unfortunately not true. If a problem does have more than
one optimal solution, the solver returns one of them. The attributions for the problem are based on
this solution. If now a non-relevant part of the problem is removed, the solver may choose a different
optimal solution, resulting in different attributions. This can be observed for SP2. The problem is
shown in Fig. 8, where both optimal paths are highlighted. Neither edge 2 nor edge 5 does inﬂuence
the optimal paths. However, the generated attributions for edges 2 and 5 are not zero, which violates
the second part of sensitivity, as shown in Tab. 11.

D Applying Granger causality to LPs

Suppose there is a set of parameters X and the model produces the output Y. Then, a parameter
xi ∈ X is causing Y by the deﬁnition of Granger, if (cid:15)X < (cid:15)X\{xi} [Granger, 1969]. This means
that the error of the model using the full parameter set X is smaller than the model using the full set
except for xi. This has two assumptions: ﬁrst, X contains all relevant information for Y, and second,
X is available before Y. Both these assumptions are met for LPs.

As an LP does not have an inherent prediction error, the formulation cannot be applied directly.
However, it is possible to use the model output as a ground truth. To get the difference in performance
for an input parameter, it is removed from the LP. The modiﬁed program is solved and the difference
between the output of the original and the modiﬁed model can be used as the Granger causal effect of
the removed parameter. In the following equation, X is the combination of all parameters the linear
program uses, and xi is a certain parameter. The formulation can also be applied for the objective
map, in this case, S has to be replaced by O.

c(xi) = S(X) − S(X \ {xi})

(14)

This formulation is very close to the modiﬁed version of Occ for LPs.

17

134520.53.92.11.22.01.8215364Figure 9: This ﬁgure shows the effects of masking a single element of one parameter from the
problem. Setting a single entry of b to zero also affects the corresponding values of A (marked in
red). If the full constraint is removed at once, no undesired side-effects occur and it is possible to
compute attributions for the constraint.

E MAP assignment details

The example for the MAP is based on a small graph which consists of the nodes V = {X1, X2, X3}
and the edges E = {E12, E23}. Each node stands for one random variable which can have the states
x ∈ {0, 1}. The state of each random variable is inﬂuenced by the potentials φi for the nodes and φij
for the edges.

φX1 (0) = 10 φX1(0) = 2
φX2(0) = 8
φX2 (0) = 8
φX3 (0) = 5
φX3(0) = 10

φE12 (x1, x2) = φE23 (x2, x3) =

(cid:26)20 a = b

1

otherwise

(15)

(16)

The parameters θi = log φi and θij = log φij are not normalized to obtain a proper probability
distribution, because the normalization is not relevant for the LP version of the MAP. The LP
formulation of the MAP problem works on the marginal polytope M of the Markov random ﬁeld.
This polytope is obtained by encoding the variable and edge states into a vector µ [Sontag, 2010].
The resulting LP is then:

max
µ

(cid:88)

(cid:88)

θi(xi)µi(xi) +

i∈V

xi

(cid:88)

(cid:88)

ij∈E

xi,xj

θij(xi, xj)µij(xi, xj)

µi(xi) ∈ [0, 1] ∀i ∈ V, xi

µi(xi) = 1 ∀i ∈ V

(cid:88)

xi

µi(xi) =

µj(xj) =

(cid:88)

xj
(cid:88)

xi

µij(xi, xj) ∀ij ∈ E, xi

µij(xi, xj) ∀ij ∈ E, xj

(17)

In this LP, µi for i ∈ V encodes the state of the random variable i. µij encodes the state of edge ij.
The constraints ensure that the encoding is correct. It is possible to use Occ to obtain attributions
for parts of the problem. In this case, attributions were computed for both edges, by removing the
respective parts of µ and θ from the problem. The presented attributions in the main paper were
obtained by reversing the encoding of the variable states from µ and performing the Occlusion
formula on the variable states directly, which results in easier interpretable attributions.

18

