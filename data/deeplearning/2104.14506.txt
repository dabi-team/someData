1
2
0
2

r
p
A
5
2

]

G
L
.
s
c
[

1
v
6
0
5
4
1
.
4
0
1
2
:
v
i
X
r
a

EXPLAINABLE AI FOR COVID-19 CT CLASSIFIERS: AN INITIAL COMPARISON STUDY

Qinghao Ye 1,2, Jun Xia 3∗ and Guang Yang 4,5∗

1 University of California, San Diego, La Jolla, California, USA
2 Hangzhou Ocean’s Smart Boya Co., Ltd
3 Radiology Department, Shenzhen Second People’s Hospital, Shenzhen, China
4 Royal Brompton Hospital, London, UK
5 National Heart and Lung Institute, Imperial College London, London, UK

ABSTRACT

Artiﬁcial Intelligence (AI) has made leapfrogs in devel-
opment across all the industrial sectors especially when deep
learning has been introduced. Deep learning helps to learn
the behaviour of an entity through methods of recognising
and interpreting patterns. Despite its limitless potential, the
mystery is how deep learning algorithms make a decision in
the ﬁrst place. Explainable AI (XAI) is the key to unlocking
AI and the black-box for deep learning. XAI is an AI model
that is programmed to explain its goals, logic, and decision
making so that the end users can understand. The end users
can be domain experts, regulatory agencies, managers and
executive board members, data scientists, users that use AI,
with or without awareness, or someone who is affected by the
decisions of an AI model. Chest CT has emerged as a valuable
tool for the clinical diagnostic and treatment management of
the lung diseases associated with COVID-19. AI can support
rapid evaluation of CT scans to differentiate COVID-19 ﬁnd-
ings from other lung diseases. However, how these AI tools
or deep learning algorithms reach such a decision and which
are the most inﬂuential features derived from these neural net-
works with typically deep layers are not clear. The aim of this
study is to propose and develop XAI strategies for COVID-19
classiﬁcation models with an investigation of comparison. The
results demonstrate promising quantiﬁcation and qualitative
visualisations that can further enhance the clinician’s under-
standing and decision making with more granular information
from the results given by the learned XAI models.

Index Terms— COVID-19, Explainable AI, Deep Learn-

ing, Classiﬁcation, CT

1. INTRODUCTION

Artiﬁcial intelligence, or AI, which refers to the creation of
intelligent hardware or software that can emulate “human”
behaviours such as learning and problem solving, is a broad

∗Corresponding authors—J Xia: xiajun@email.szu.edu.cn & G Yang:

g.yang@imperial.ac.uk

branch of computer science that focuses on a machine’s ca-
pability to generate rational behaviour. The goal of AI is to
create systems that can perform tasks that would otherwise
require human intelligence. AI manifests itself in everyday
life through virtual assistants, search prediction technology,
and driving automobiles. As AI multiplies in our daily lives,
so does the fear that people will lose control. As a result, the
European Commission, for example, has pushed forward to
create a framework to regulate the use of AI, leading to guide-
lines set out the requirements that AI systems must meet to be
trustworthy.

Deep Learning models such as deep neural networks are
powerful AI models that can learn complex patterns from var-
ious types of data such as images, speech signals or text in
order to predict the associated properties with impressive pre-
cision. However, many view these models as uninterpretable
because the prediction patterns learned are difﬁcult to interpret.
This black-box nature has hindered the widespread application
of deep neural networks to digital healthcare and medicine,
where the interpretation of predictive patterns is of paramount
importance.

Despite the limitless potential of deep learning, the mys-
tery is how the algorithms reach a decision in the ﬁrst place.
Reliability concerns are often raised in deep learning models,
driving questions like, “Which processes did you take over
and at what speed? How did you make such an autonomous
decision?” While the deep learning model helps parse large
amounts of data into intelligent insights for applications that
range from fraud detection to weather forecasting, the human
mind is constantly confused about how to reach such con-
clusions. Additionally, the recurring need to understand the
processes behind decisions becomes increasingly important
when the deep learning model has the potential to make deci-
sions based on incomplete, error-prone, or biased information
that important features may languish inside the black-box.

Explainable AI (XAI) is the key to unlocking AI and the
black-box for deep learning. XAI is an AI model that is pro-
grammed to explain the goals, logic and decision making so
that the common end user can understand it. Here, end users

 
 
 
 
 
 
can be the designers of the AI systems or the people affected by
the decisions of an AI model. According to Arrieta et al. [1],
early AI systems were easy to interpret. For example, decision
trees, Bayesian classiﬁers and other algorithms have a cer-
tain degree of traceability, visibility and transparency in their
decision-making process. But recently, complex and opaque
decision-making systems, such as deep neural networks, have
appeared in AI. The empirical success of deep neural net-
works is based on a combination of efﬁcient algorithms and
their huge parametric space. This parametric space comprises
hundreds of layers and millions of parameters, which means
that deep neural networks are considered complex black-box
models. The opposite of the black-box is the transparent white-
box, which advocates the search for a direct understanding
of the mechanism that how a model works. This demand is
increasing also due to ethical concerns, for example, because
the dataset used to train deep learning systems may not be
justiﬁed, legitimate, or not allow detailed explanations of their
behaviour. Additionally, for the opaque black-box AI decision
making, XAI also addresses the bias inherent in AI systems.
Bias in AI systems can have detrimental effects, especially in
digital healthcare and medicine [2, 3].

This study aims to build up a COVID-19 classiﬁcation
system that incorporates both the latest development of deep
learning with the advances in XAI. In doing so, our data driven
system can provide not only accurate classiﬁcation outcomes
but also explainable results for such a deep learning model.

2. RELATED WORK

COVID-19 (a.k.a. Coronavirus 2019) has become a world-
wide epidemic with rapid development and unprecedented
spread of the disease since December 2019. The virus is
highly contagious and can be asymptomatic and mostly latent,
but it develops rapidly and can also lead to quickly progressive
and frequently fatal pneumonia in 2–8% of those affected [4].
However, the total mortality, prevalence, and severity of viral
infection may be ill-deﬁned partially because of the challenges
associated with SARS-CoV-2 infection, such as an increase in
viral load at symptoms or just beforehand, and misconception
of multi-organ pathophysiology with dominant features and
lung lethality. Differential proliferation complicates healthcare
systems worldwide due to a lack of critical protective equip-
ment and specialised providers, part of missing a cost-effective
testing approach. When the fast reverse transcription poly-
merase chain reaction (RT-PCR) test becomes available, the
challenges are with high false negative values, slow processing,
inconsistency in test methods, and the detection sensitivity is
often registered as low as 60–70 per cent [5].

Computed tomography (CT) is a test that offers a window
to physiology that can explain the different stages of disease
diagnosis and evolution. Although problems exist with the
rapid detection of COVID-19, frontline radiologists ﬁnd com-
mon features such as the ground glass opacity (GGO) in the

lung periphery, rounded opacity, enlarged intra-inﬁltrate ves-
sels, and further consolidations that are signs of developing
a critical illness. Although CT and RT-PCR are most com-
monly concordant, CT may also detect early COVID-19 in
patients with a negative RT-PCR examination, in patients with-
out symptoms, before or after symptoms develop [5]. The
CT evaluation has been an important part of the initial exami-
nation of patients with suspected or conﬁrmed COVID-19 in
various centres in Wuhan China and northern Italy. A new
international expert consensus study advocates the use of chest
CT in COVID-19 patients with declining respiratory status or
resource-restricted conditions for surgical triage of patients
with moderate-severe clinical characteristics and a high pre-
test likelihood of COVID-19 [6]. However, these guidelines
also recommend against the use of chest CT in screening or
diagnostic settings that may be partially due to similar radio-
graphic presentation with other inﬂuenza-associated pneumo-
nia. Techniques for distinguishing between these individuals
can reinforce support for the use of CT in diagnostic settings.
Due to the dramatic rise in the number of newly suspected
COVID-19 cases, the role of AI approaches in the identiﬁca-
tion or characterisation of COVID-19 imaging can be impor-
tant [7]. CT offers a simple yet expeditious window for this
process. Besides, deep learning of large-scale multinational
CT data can provide automated and reproducible imaging
biomarkers for the classiﬁcation and quantiﬁcation of COVID-
19. Previous trials have shown the efﬁcacy of AI for the diagno-
sis of COVID-19 infection or even distinguishing COVID-19
from community-acquired pneumonia. AI models are unavoid-
ably restricted in clinical use due to the homogeneity of data
sources, which in turn restricts their applicability to differ-
ent populations, demographics, or geographies. Moreover,
as mentioned, CT can share some common imaging proper-
ties between COVID-19 and other types of pneumonia and
infection lesions may be confounded at the early stage, mak-
ing automated classiﬁcation difﬁcult. More importantly, AI
techniques, especially deep learning based methods, normally
possess multi-layers of neural networks and the explanation of
how the network components work synergistically and which
are the most important features are commonly vague to the
developers and end-users. This drives us to build up a robust
deep neural network based model for distinguishing COVID-
19 from CT scans with the assistance of both local and global
XAI.

3. METHOD

In this section, we describe a classiﬁer that is designed for
distinguish OVID-19 from CT scan volumes with XAI solu-
tions. Commonly, image-level labels are not easily obtained
since annotating single images are labour-intensive and time-
consuming, which takes a signiﬁcant amount of time for well-
trained radiologists to annotate them. On the other hands,
patient-level annotations are available easily. To overcome

layer leading to the extra computation. Besides, it cannot be
visualised during the training procedure since the weight W
keeps updating. Therefore, to overcome this drawback, we
replace the classiﬁcation layer (fully connection layer) with a
1 convolution layer since they are mathematically equiva-
1
lent. Meanwhile, we reformulate the Equation (1) as

×

sc =

1
HW

H
(cid:88)

W
(cid:88)





C(cid:48)
(cid:88)

i=1

j=1

d=1



WdcF d
ij

 =

1
HW

H
(cid:88)

W
(cid:88)

(Ac)ij,

i=1

j=1

(3)
where Ac is the activation map for class c that is learnt adap-
tively during the training procedure instead of obtained by
post-processing. The workﬂow of CAM and our method are
depicted in Figure 1.

Fig. 1. The workﬂow comparison between class activation
mapping and our proposed explainable module.

3.2. Slice Integration Module

these problems, we propose an explainable COVID-19 classi-
ﬁer for the acquired CT data. The XAI module in the proposed
model can provide radiologists with auxiliary diagnostic infor-
mation. Moreover, we also divide the images into super-pixels
and adopt the LIME method [8] and compute the Sharply val-
ues for the sake of interpreting the individual contribution of
each super-pixel and compare the local and global explainabil-
ity. As a result, these XAI modules can be used to determine
which part of the CT images contains the most predictive
lesion areas leading the model to make the ﬁnal decision.

3.1. Explainable Module

As widely used, Class Activation Map (CAM) [9] can generate
the localisation maps for the prediction through the combina-
tion of classiﬁcation layer and feature maps of the backbone
networks such as ResNet [10]. Denoted the d-th feature maps
of the network as F d
W , and the classiﬁcation score
sc for class c can be obtained by

RH

∈

×

sc =



Wdc



C(cid:48)
(cid:88)

d=1

1
HW

H
(cid:88)

W
(cid:88)

i=1

j=1



F d
ij

 ,

(1)

RC(cid:48)

×

∈

C is the parameter of last classiﬁcation
where W
layer. Therefore, the classiﬁcation score sc can be computed
as the weighted sum of feature maps after the Global Average
Pooling (GAP) operation, and activation map Ac for class c
can be computed as

In intuition, the CT volumes of each patient has a different
severity. Some of the COVID-19 positive patients have only
few infections inside their lungs, which indicates most of the
slices in the 3D volume are normal. In contrast, most of the
patients with severe infections have larger lesions. Thus, the
data would be extremely noisy when directly applying the
patient-level annotation to each CT slice, which can affect
the model performance. To avoid this problem, we develop
a slice integration module, which integrates the probabilities
of slices to reconstruct the probability of the whole CT vol-
ume. Besides, we ﬁrst assume that the lesions are consecutive
and consistently distributed in the CT volume. Under such
assumption, we develop a section based strategy to handle this
In] has
problem. Suppose that the patient
n CT slices, and we divide them into several disjoint sections
is the number of sections deﬁned by

= [

· · ·

=

1,

2,

P

I

I

,

S
|i=1 where
|

S

Si}
{

P

|

|

= max

1,

S
|

|

(cid:18)

(cid:23)(cid:19)

,

(cid:22) n
ls

(4)

where ls is the section length that we empirically set to 8. Then
we compute the probability of the patient

by

P

P (c

|P

) = P (c

|{

S
|i=1) = 1
|

Si}

−

S
|(cid:89)
|

(1

i=1

P (c

Si)),
|

−

(5)

Si) is obtained by the average classiﬁcation score
where P (c
|
of the largest k slices with Sigmoid function. Then, patient
annotation y is used to compute the classiﬁcation loss, which
is formulated as

(Ac)ij =

C(cid:48)
(cid:88)

d=1

WdcF d
ij.

1
N

−

=

L

N
(cid:88)

i=1

(2)

[yi log P (c = 1

|Pi) + (1

−

yi) log P (c = 0

|Pi)] .
(6)

However, generating the activation maps through CAM is
a post-processing procedure, in which the network should be
trained on the dataset then computes the feature maps and acti-
vation maps using the ﬁxed weight W from the classiﬁcation

3.3. Local Importance Explanation

To explicitly highlight which local part of the slices can
improve the model performance, we incorporate the LIME

Global Average PoolingBackboneGlobal Average PoolingExplainable ModuleClass Activation MappingBackboneCTImagesCTImages<latexit sha1_base64="BjtwEGl0Cbw8ClHpcuBNR5FQf4M=">AAACj3icbVFNTxsxEHW2tKXpB0l7rISsRkgcqmi3KoITiopU0XIBSgiIXaKx1wEX27uyZ6tEq73xa7i2f4Z/gzfJgYSOZOnpzZvxzBuWK+kwDO8bwbOV5y9err5qvn7z9t1aq/3+1GWF5aLPM5XZMwZOKGlEHyUqcZZbAZopMWA3e3V+8EdYJzNzgpNcJBqujBxJDuipYWv9+2VKY2lorAGvGSuPq8tyP0aphaODatjqhN1wGvQpiOagQ+ZxOGw3zuM044UWBrkC5y6iMMekBIuSK1E148KJHPgNXIkLDw34j5JyukhFNzyT0lFm/TNIp+zjihK0cxPNvLIe1y3navIz0/9NM88v6q2FyeJACN63pBzPxllUp78LhywbV82Nx/xY7P08WFoLRztJKU1eoDB8ttWoUBQzWp+AptIKjmriAXArvTGUX4MFjv5QvrsT6HtqkKZ2ofx1cgBeVDX9MaJl65+C0y/daKsbHn3t9L7Nz7JKPpJPZJNEZJv0yD45JH3CyS25I3/Jv6AdbAe7QW8mDRrzmg9kIYIfD6SNynI=</latexit>Fd2RH⇥W<latexit sha1_base64="BjtwEGl0Cbw8ClHpcuBNR5FQf4M=">AAACj3icbVFNTxsxEHW2tKXpB0l7rISsRkgcqmi3KoITiopU0XIBSgiIXaKx1wEX27uyZ6tEq73xa7i2f4Z/gzfJgYSOZOnpzZvxzBuWK+kwDO8bwbOV5y9err5qvn7z9t1aq/3+1GWF5aLPM5XZMwZOKGlEHyUqcZZbAZopMWA3e3V+8EdYJzNzgpNcJBqujBxJDuipYWv9+2VKY2lorAGvGSuPq8tyP0aphaODatjqhN1wGvQpiOagQ+ZxOGw3zuM044UWBrkC5y6iMMekBIuSK1E148KJHPgNXIkLDw34j5JyukhFNzyT0lFm/TNIp+zjihK0cxPNvLIe1y3navIz0/9NM88v6q2FyeJACN63pBzPxllUp78LhywbV82Nx/xY7P08WFoLRztJKU1eoDB8ttWoUBQzWp+AptIKjmriAXArvTGUX4MFjv5QvrsT6HtqkKZ2ofx1cgBeVDX9MaJl65+C0y/daKsbHn3t9L7Nz7JKPpJPZJNEZJv0yD45JH3CyS25I3/Jv6AdbAe7QW8mDRrzmg9kIYIfD6SNynI=</latexit>Fd2RH⇥W<latexit sha1_base64="2p2oML0BuA1saXUZTCkWVrt8tJI=">AAACcHicbVHLSgNBEJysrxif0YvgwdUgiEjYFUWPohfRi6Ixii7SO+no6MzsMtMrCUu+wKt+nL/hFzh5HEy0oaGorm66q+NUCktB8FXwxsYnJqeK06WZ2bn5hcXy0o1NMsOxxhOZmNsYLEqhsUaCJN6mBkHFEuvx60m3Xn9DY0Wir6mdYqTgSYum4ECOuqw/LlaCatAL/y8IB6DCBnHxWC7cPTQSninUxCVYex8GKUU5GBJcYqf0kFlMgb/CE947qEGhjfLeph1/0zENv5kYl5r8Hvu7IwdlbVvFTqmAnu1orUvuxOrfcuz4Yb0x0B5eiMAZE+Wt/jrD6sZLZilOWp3S5m++hSdn5yNnUfMwyoVOM0LN+1c1M+lT4nc99hvCICfZdgC4Ec4Ynz+DAU7uE266RXIzFQjddSG/uj4HJ+qU3DPCUev/gpvdarhfDS73KkfHg7cU2SrbYFssZAfsiJ2yC1ZjnCF7Zx/ss/DtrXhr3npf6hUGPctsKLztH0v2v1M=</latexit>W<latexit sha1_base64="2p2oML0BuA1saXUZTCkWVrt8tJI=">AAACcHicbVHLSgNBEJysrxif0YvgwdUgiEjYFUWPohfRi6Ixii7SO+no6MzsMtMrCUu+wKt+nL/hFzh5HEy0oaGorm66q+NUCktB8FXwxsYnJqeK06WZ2bn5hcXy0o1NMsOxxhOZmNsYLEqhsUaCJN6mBkHFEuvx60m3Xn9DY0Wir6mdYqTgSYum4ECOuqw/LlaCatAL/y8IB6DCBnHxWC7cPTQSninUxCVYex8GKUU5GBJcYqf0kFlMgb/CE947qEGhjfLeph1/0zENv5kYl5r8Hvu7IwdlbVvFTqmAnu1orUvuxOrfcuz4Yb0x0B5eiMAZE+Wt/jrD6sZLZilOWp3S5m++hSdn5yNnUfMwyoVOM0LN+1c1M+lT4nc99hvCICfZdgC4Ec4Ynz+DAU7uE266RXIzFQjddSG/uj4HJ+qU3DPCUev/gpvdarhfDS73KkfHg7cU2SrbYFssZAfsiJ2yC1ZjnCF7Zx/ss/DtrXhr3npf6hUGPctsKLztH0v2v1M=</latexit>W<latexit sha1_base64="iu3dnUuZfhIsJzlQXck40OkGaBo=">AAACcnicbVHLTsMwEHTDu7zhBpdAhcQBVQkCwZHHBcEFBOUhiKqNu20NthPZG9Qq6idwhW/jP/gA3JIDLaxkaTQ7Xu3OxKkUloLgs+SNjU9MTk3PlGfn5hcWl5ZXbm2SGY41nsjE3MdgUQqNNRIk8T41CCqWeBe/nPb7d69orEj0DXVTjBS0tGgKDuSo6+M6ry9VgmowKP8vCAtQYUVd1pdLD0+NhGcKNXEJ1j6GQUpRDoYEl9grP2UWU+Av0MJHBzUotFE+2LXnbzmm4TcT454mf8D+/pGDsrarYqdUQG072uuTO7H6tx07flhvDHSHFyJw1kR552edYXXjObMUJ51eees338HT84uRs6h5GOVCpxmh5j9XNTPpU+L3XfYbwiAn2XUAuBHOGJ+3wQAnl4WbbpHcTAVC913Ir28uwIl6ZRdGOGr9X3C7Ww33q8HVXuXopIhlmq2zTbbNQnbAjtgZu2Q1xlmLvbF39lH68ta8Da/I0CsVf1bZUHk73/fQwBM=</latexit>Ac<latexit sha1_base64="iu3dnUuZfhIsJzlQXck40OkGaBo=">AAACcnicbVHLTsMwEHTDu7zhBpdAhcQBVQkCwZHHBcEFBOUhiKqNu20NthPZG9Qq6idwhW/jP/gA3JIDLaxkaTQ7Xu3OxKkUloLgs+SNjU9MTk3PlGfn5hcWl5ZXbm2SGY41nsjE3MdgUQqNNRIk8T41CCqWeBe/nPb7d69orEj0DXVTjBS0tGgKDuSo6+M6ry9VgmowKP8vCAtQYUVd1pdLD0+NhGcKNXEJ1j6GQUpRDoYEl9grP2UWU+Av0MJHBzUotFE+2LXnbzmm4TcT454mf8D+/pGDsrarYqdUQG072uuTO7H6tx07flhvDHSHFyJw1kR552edYXXjObMUJ51eees338HT84uRs6h5GOVCpxmh5j9XNTPpU+L3XfYbwiAn2XUAuBHOGJ+3wQAnl4WbbpHcTAVC913Ir28uwIl6ZRdGOGr9X3C7Ww33q8HVXuXopIhlmq2zTbbNQnbAjtgZu2Q1xlmLvbF39lH68ta8Da/I0CsVf1bZUHk73/fQwBM=</latexit>AcFig. 2. Samples of the activation map Ac generated by our proposed explainable module for classifying the COVID-19 positive
cases. The ﬁrst row contains the original input images, and the second row represents the corresponding activation maps and
bounding boxes. The colour indicates the extent of the attention.

method [8] into our classiﬁer. In details, the input slices are
ﬁrstly divided into several super-pixels, which share simi-
lar visual properties among individual pixels within it. For
trailing the inﬂuence of each super-pixel, each of them is
randomly masked, and we deﬁne the interpretable model
g(x) = (cid:80)N
i=1 wixi, where N is number of super-pixels and
xi = 1 denotes the super-pixel is unmasked. Since we want
to ﬁnd the functionality of each super-pixel, the optimal
interpretable model φ(x) is deﬁned as

φ(x) = arg min

(cid:88)

−(x−x(cid:48))2
σ2

e

(f (x)

G

g

∈

(x,x(cid:48))

∈X

g(x(cid:48)))2 +

x(cid:48)

1,
(cid:107)

(cid:107)

−

φ(x) = arg min

(cid:88)

(7)
where G is a candidate set of possible interpretable models, f
is the trained network with explainable module and slice inte-
gration module, x is the original images, and x(cid:48) is the masked
images. We empirically set σ = 2 for solving the optimal
interpretable model. By applying the Lasso algorithm, we
can solve the Equation 7 for all of the weights wi. Therefore,
the super-pixels with positive weights would account for the
prediction towards a speciﬁc class.

Method
ResNet-50 [10]
COVID-Net [12]
COVNet [13]
VB-Net [14]
Ours

Accuracy (%) Precision (%) Recall (%) AUC (%)

53.72
53.65
67.60
76.73
89.23

61.42
61.35
76.01
85.26
89.98

77.20
77.20
73.18
77.58
93.86

46.30
44.53
66.13
89.48
93.22

Table 1. Comparison results of our method vs. state-of-the-art
architectures on the CC-CCII dataset.

solved using a similar form of Equation (7) as

N
1(N
(cid:107)

1

−
− (cid:107)

x
(cid:107)

x

1)
(cid:107)

(f (x)

−

g(x(cid:48)))2.

g

G

∈

(x,x(cid:48))

∈X

(8)
As a consequence, the SHAP values from game theory can
be computed using the weighted linear regression. In speciﬁc,
the region with negative SHAP value contributes negatively
to the prediction, whereas the remainder of the feature have a
positive contribution to the prediction.

4. EXPERIMENTS

4.1. Datasets

3.4. Global Importance Explanation

The LIME method indeed account for the contribution of each
super-pixel with local explainability; however, it violates the
convention that if the model changes so that the input’s con-
tribution increase, the input’s attribution should not decrease.
In other words, the model’s interpretation should be consis-
tency regardless of changing of the input. Therefore, SHapley
Additive exPlanation (SHAP) value [11] is then incorporated
for the measure of feature importance. It not only preserves
the local accuracy of the super-pixels, but also follows the
consistency assumption. Similarly, the SHAP value wi can be

We collected CT volume data from four hospitals in China
and removed the personal sensitive information to ensure the
privacy. In total, there were 380 CT volumes with COVID-19
positive, and 424 COVID-19 negative CT volumes. For a fair
comparison, we trained the models on these private datasets,
then tested on the open access CC-CCII dataset [15], which is
a publicly available dataset with 2,034 CT volumes.

4.2. Implementation Details

Following the protocol used in [15], we ﬁrstly utilised the
U-Net to segment the CT images, and we randomly cropped

224. Mean-
the volumes and resized each image into 224
while, the images in the input volume were randomly ﬂipped
horizontally with a probability of 0.5. We trained the models
for 200 epochs with the learning rate of 0.001.

×

4.3. Quantitative Results

For comparison, we selected several state-of-the-art methods
[10, 12, 13, 14] that worked well on COVID-19 image clas-
siﬁcation. For those only worked on image-level methods
[10, 12], we directly applied the patient-level annotations as
image labels to train the network. In particular, VBNet [14]
utilised the 3D residual convolutional blocks to train the net-
work on the whole CT volumes rather than processing each
CT slice. Meanwhile, COVNet [13] took the largest probabil-
ity among the those of images in the volume as the volume
probability.

The results are summarised in Table 1. From the table, we
can observe that our method outperformed other methods by a
large margin. In particular, our method surpassed VBNet on
accuracy by 12.5%, which means that our method could be
practical for real-world scenario. The slice integration module
in our method could weight the slices in each section while
preserving the characteristics of each section, leading to better
accuracy. Besides, models (i.e., ResNet-50 and COVID-Net)
trained on single images cannot achieve desirable results due to
a large amount of noise in the training labels. Though COVNet
took the most discriminative slice as the representation of the
whole volume, it would perform poorly on negative examples
since it brings bias towards positive cases.

4.4. Qualitative Results

Apart from robust quantitative performance of our model, the
explainability of the model is also promising. To make the
prediction to be more explainable, we ﬁrstly extracted the acti-
vation maps generated by explainable module of our method
as described above and visualised in Figure 2 . It can be ob-
served that our proposed method would focus on the most
discriminative part of the image, which can be identiﬁed as
the lesion area, and make the positive prediction. Therefore,
the radiologists can make auxiliary judgement based on the
results provided by the activation maps. Meanwhile, the lesion
bounding boxes can be extracted by our method, which tends
to yield on the lesion parts of the generated activation maps,
which further veriﬁed that our method was applicable to be a
valuable auxiliary diagnosis tool for radiologists.

Besides, instead of only showing the most salient parts
of the image, we also focus on local regional contribution to
the prediction. As we described above, we divided the input
images into several super-pixels, in which pixels shared the
similar visual pattern. Figure 7 provides the examples of ex-
plaining the COVID-19 images from the positive CT volumes
via the developed LIME method. For each pair of images, we

Fig. 3. Visualisation of the super-pixels contributed posi-
tively to the model prediction through the developed LIME
paradigm.

Fig. 4. Visualisation of the SHAP values for each super-pixel.
The positive value indicates how much the corresponding
super-pixel would encourage the network to make a positive
prediction.

highlighted those super-pixels that contributed positively to
the prediction, and we can clearly observed that those regions
contains lesions are highlighted for the prediction. Therefore,
it demonstrates that the infected areas are easily to be spotted
by this method.

Furthermore, to overcome the shortcoming of the LIME
method, we also visualised the contribution of each super-
pixel. Figure 4 provides two examples of COVID-19 positive
predictions. We can ﬁnd that the background in the images
tend to negatively contribute to the prediction which are re-
gard as the noise in the images. Besides, most of the lesion
areas in the CT images obtain the large positive SHAP values
indicating they signiﬁcantly positively contribute to the ﬁnal
positive prediction, which are able to explain the prediction of
the model.

5. CONCLUSION

In this study, we have presented an XAI equipped classiﬁer
for COVID-19 detection from CT images. The experimental
studies have demonstrated that our XAI enhanced classiﬁer
model can not only provide promising and robust classiﬁca-
tion results but can also provide convincing explanation of
these results. We have also compared the widely used CAM,
LIME and SHAP value based XAI modules. Although the
study has been carried out for COVID-19 studies, the applica-
tion on other image modalities and clinical questions can be
envisioned.

6. REFERENCES

[1] Alejandro Barredo Arrieta, Natalia D´ıaz-Rodr´ıguez,
Javier Del Ser, Adrien Bennetot, Siham Tabik, Alberto
Barbado, Salvador Garc´ıa, Sergio Gil-L´opez, Daniel
Molina, Richard Benjamins, et al., “Explainable arti-
ﬁcial intelligence (xai): Concepts, taxonomies, opportu-
nities and challenges toward responsible ai,” Information
Fusion, vol. 58, pp. 82–115, 2020.

[2] Guang Yang, Qinghao Ye, and Jun Xia, “Unbox the
black-box for the medical explainable ai via multi-modal
and multi-centre data fusion: A mini-review, two show-
cases and beyond,” arXiv preprint arXiv:2102.01998,
2021.

[3] Qinghao Ye, Daijian Tu, Feiwei Qin, Zizhao Wu, Yong
Peng, and Shuying Shen, “Dual attention based ﬁne-
grained leukocyte recognition for imbalanced micro-
scopic images,” Journal of Intelligent & Fuzzy Systems,
vol. 37, no. 5, pp. 6971–6982, 2019.

[4] Stephanie A Harmon, Thomas H Sanford, Sheng Xu,
Evrim B Turkbey, Holger Roth, Ziyue Xu, Dong Yang,
Andriy Myronenko, Victoria Anderson, Amel Amalou,
et al., “Artiﬁcial intelligence for the detection of covid-
19 pneumonia on chest ct using multinational datasets,”
Nature communications, vol. 11, no. 1, pp. 1–7, 2020.

[5] Shaoping Hu, Yuan Gao, Zhangming Niu, Yinghui Jiang,
Lao Li, Xianglu Xiao, Minhao Wang, Evandro Fei Fang,
Wade Menpes-Smith, Jun Xia, et al., “Weakly super-
vised deep learning for covid-19 infection detection and
classiﬁcation from ct images,” IEEE Access, vol. 8, pp.
118869–118883, 2020.

[6] Geoffrey D Rubin, Christopher J Ryerson, Linda B Hara-
mati, Nicola Sverzellati, Jeffrey P Kanne, Suhail Raoof,
Neil W Schluger, Annalisa Volpi, Jae-Joon Yim, Ian BK
Martin, et al., “The role of chest imaging in patient man-
agement during the covid-19 pandemic: a multinational
consensus statement from the ﬂeischner society,” Chest,
vol. 158, no. 1, pp. 106–116, 2020.

[7] Michael Roberts, Derek Driggs, Matthew Thorpe, Julian
Gilbey, Michael Yeung, Stephan Ursprung, Angelica I
Aviles-Rivero, Christian Etmann, Cathal McCague, Lu-
cian Beer, et al., “Common pitfalls and recommendations
for using machine learning to detect and prognosticate for
covid-19 using chest radiographs and ct scans,” Nature
Machine Intelligence, 2021.

[8] Marco Tulio Ribeiro, Sameer Singh, and Carlos Guestrin,
““Why should I trust you?” Explaining the predictions of
any classiﬁer,” in Proceedings of the 22nd ACM SIGKDD
international conference on knowledge discovery and
data mining, 2016, pp. 1135–1144.

[9] Bolei Zhou, Aditya Khosla, Agata Lapedriza, Aude
Oliva, and Antonio Torralba, “Learning deep features for
discriminative localization,” in Proceedings of the IEEE
conference on computer vision and pattern recognition,
2016, pp. 2921–2929.

[10] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian
Sun, “Deep residual learning for image recognition,” in
Proceedings of the IEEE conference on computer vision
and pattern recognition, 2016, pp. 770–778.

[11] Scott M. Lundberg and Su-In Lee, “A uniﬁed approach
to interpreting model predictions,” in Advances in Neu-
ral Information Processing Systems 30: Annual Confer-
ence on Neural Information Processing Systems, Isabelle
Guyon, Ulrike von Luxburg, Samy Bengio, Hanna M.
Wallach, Rob Fergus, S. V. N. Vishwanathan, and Roman
Garnett, Eds., 2017, pp. 4765–4774.

[12] Shuai Wang, Bo Kang, Jinlu Ma, Xianjun Zeng, Ming-
ming Xiao, Jia Guo, Mengjiao Cai, Jingyi Yang, Yaodong
Li, Xiangfei Meng, et al., “A deep learning algorithm
using ct images to screen for corona virus disease (covid-
19),” MedRxiv, 2020.

[13] Lin Li, Lixin Qin, Zeguo Xu, Youbing Yin, Xin Wang,
Bin Kong, Junjie Bai, Yi Lu, Zhenghan Fang, Qi Song,
et al., “Artiﬁcial intelligence distinguishes covid-19 from
community acquired pneumonia on chest ct,” Radiology,
p. 200905, 2020.

[14] Xi Ouyang, Jiayu Huo, Liming Xia, Fei Shan, Jun Liu,
Zhanhao Mo, Fuhua Yan, Zhongxiang Ding, Qi Yang,
Bin Song, et al., “Dual-sampling attention network for
diagnosis of covid-19 from community acquired pneu-
monia,” arXiv preprint arXiv:2005.02690, 2020.

[15] Kang Zhang, Xiaohong Liu, Jun Shen, Zhihuan Li,
Ye Sang, Xingwang Wu, Yunfei Zha, Wenhua Liang,
Chengdi Wang, Ke Wang, et al., “Clinically applicable
ai system for accurate diagnosis, quantitative measure-
ments, and prognosis of covid-19 pneumonia using com-
puted tomography,” Cell, vol. 181, no. 6, pp. 1423–1433,
2020.

