8
1
0
2

t
c
O
9
1

]

Y
C
.
s
c
[

2
v
8
8
6
2
0
.
0
1
8
1
:
v
i
X
r
a

Wikistat 2.0: Ressources p´edagogiques pour //la
Donn´ees l’Intelligence Artiﬁcielle
Sciences/////des///////////
//////////

Philippe Besse∗

Brendan Guillouet†

B´eatrice Laurent‡

22 octobre 2018

R´esum´e : Big data, science des donn´ees, deep learning, intelligence ar-
tiﬁcielle, sont les mots clefs de battages m´ediatiques intenses en lien avec
un march´e de l’emploi en pleine ´evolution qui impose d’adapter les conte-
nus de nos formations professionnelles universitaires. Quelle intelligence ar-
tiﬁcielle est principalement concern´ee par les oﬀres d’emplois ? Quelles sont
les m´ethodologies et technologies qu’il faut privil´egier dans la formation ?
Quels objectifs, outils et ressources p´edagogiques est-il n´ecessaire de mettre
en place pour r´epondre `a ces besoins pressants ? Nous r´epondons `a ces ques-
tions en d´ecrivant les contenus et ressources op´erationnels dans la sp´ecialit´e
Math´ematiques appliqu´ees, majeure Science des Donn´ees, de l’INSA de Tou-
louse. L’accent est mis sur une formation en Math´ematiques (Optimisation,
Probabilit´es, Statistique) fondamentale ou de base associ´ee `a la mise en œuvre
pratique des algorithmes d’apprentissage statistique les plus performants,
avec les technologies les plus adapt´ees et sur des exemples r´eels. Compte
tenu de la tr`es grande volatilit´e des technologies, il est imp´eratif de former
les ´etudiants `a l’autoformation qui sera leur outil de veille technologique
une fois en poste ; c’est la raison de la structuration du site p´edagogique
github/wikistat en un ensemble de tutoriels. Enﬁn, pour motiver la pra-
tique approfondie de ces tutoriels, un jeu s´erieux est organis´ee chaque ann´ee
sous la forme d’un concours de pr´evision entre ´etudiants de masters de
Math´ematique Appliqu´ees pour l’IA.

Mots-clefs : science des donn´ees, intelligence artiﬁcielle, apprentissage

statistique, donn´ees massives, enseignement, jeux s´erieux.

∗Universit´e de Toulouse – INSA, Institut de Math´ematiques de Toulouse, UMR CNRS
†Universit´e de Toulouse – INSA, IRT Saint Exup´ery
‡Universit´e de Toulouse – INSA, Institut de Math´ematiques de Toulouse, UMR CNRS

1

 
 
 
 
 
 
Wikistat 2.0 : Educational Resources for Artiﬁcial Intelligence

Abstract : Big data, data science, deep learning, artiﬁcial intelligence
are the key words of intense hype related with a job market in full evolution,
that impose to adapt the contents of our university professional trainings.
Which artiﬁcial intelligence is mostly concerned by the job oﬀers ? Which me-
thodologies and technologies should be favored in the training pprograms ?
Which objectives, tools and educational resources do we needed to put in
place to meet these pressing needs ? We answer these questions in descri-
bing the contents and operational ressources in the Data Science orientation
of the speciality Applied Mathematics at INSA Toulouse. We focus on ba-
sic mathematics training (Optimization, Probability, Statistics), associated
with the practical implementation of the most performing statistical learning
algorithms, with the most appropriate technologies and on real examples.
Considering the huge volatility of the technologies, it is imperative to train
students in seft-training, this will be their technological watch tool when they
will be in professional activity. This explains the structuring of the educa-
tional site github/wikistat into a set of tutorials. Finally, to motivate the
thorough practice of these tutorials, a serious game is organized each year
in the form of a prediction contest between students of Master degrees in
Applied Mathematics for IA.

Keywords : Data Science, artiﬁcial intelligence, statistical learning, big

data, teaching, serious game.

2

1

Introduction

1.1 Battage m´ediatique, scientiﬁque et commercial

Big Data Analytics, Data Science, Machine Learning, Deep Learning, In-
telligence Artiﬁcielle, un battage m´ediatique (buzz word) en chasse un autre,
reﬂets ou ´ecumes de disruptions technologiques importantes et surtout conti-
nuelles. Quels objectifs et choix p´edagogiques engager pour anticiper les
contenus de nos formations et les modes d’acquisition des comp´etences aﬁn
de pr´eparer eﬃcacement l’int´egration des nouveaux diplˆom´es ? C’est `a ces
questions que nous tˆachons d’apporter des ´el´ements de r´eponses, non pas
des r´eponses th´eoriques ou des d´eclarations d’intention, mais plutˆot des re-
tours d’exp´eriences et de r´ealisations en constante (r)´evolution au sein de la
sp´ecialit´e Math´ematiques Appliqu´ees de l’INSA de Toulouse.

Il est ´evidemment important de communiquer avec les bons intitul´es et les
´etudiants de l’INSA l’ont bien compris en se d´eclarant data scientist sur leur
CV depuis 2013. Mais les bons choix d’investissement, ceux p´edagogiques qui
prennent du temps et engagent sur la dur´ee, ne peuvent ˆetre pris en s’atta-
chant `a l’´ecume des mots, mˆeme inlassablement soulev´ee par des rapports
oﬃciels, m´edia ou hashtags des r´eseaux sociaux. Il ne suﬃt pas de changer
un intitul´e de cours ou de diplˆome.

Alors que les ressources p´edagogiques, les MOOCs, SPOCs, tutoriels, se
d´eversent `a profusion sur internet, que devient le rˆole d’un enseignant et plus
pr´ecis´ement d’un enseignant / chercheur ? Certes contribuer `a produire de
la connaissance par la recherche mais, en responsabilit´e p´edagogique, une
fonction essentielle consiste `a prioriser des choix. Sous l’´ecume m´ediatique,
quelles sont les m´ethodes, les technologies, les algorithmes, dont les perfor-
mances, donc la diﬀusion, motivent le temps et l’implication n´ecessaires `a
leur int´egration dans un cursus acad´emique inexorablement contraint par le
volume horaire ?

La pression m´ediatique n’est pas seule en jeu, il faut noter aussi celle,
acad´emique, de publication : publish or perish, qui conduit `a la production
de milliers d’articles d´ecrivant l”invention” de centaines de m´ethodes, algo-
rithmes, librairies et de leurs tr`es nombreuses variantes incr´ementales, alors
qu’en pratique, il faut reconnaˆıtre que les diﬀ´erences de performance n’ap-
paraissent pas toujours signiﬁcatives. Lire `a ce sujet les articles de Hand
(2006) et Donoho (2015) tr`es critiques envers les algorithmes d’apprentissage
r´ecents, et pas seulement pour opacit´e et manque d’interpr´etabilit´e.

Notons aussi la pression commerciale ou publicitaire des entreprises, start-
up ou grands groupes, disputant les parts d’un march´e en forte croissance
mais tr`es volatil ou versatile. Chaque ann´ee depuis 2012 et motiv´es par

3

des besoins de visibilit´e ´economique, Matt Turck et ses collaborateurs de la
soci´et´e Firstmark proposent une repr´esentation graphique du paysage ou de
l’´ecosyst`eme, devenu fort complexe, des entreprises traitant de donn´ees mas-
sives ou plutˆot maintenant de donn´ees massives et d’intelligence artiﬁcielle.
Ils tˆachent chaque ann´ee de prendre en compte les cr´eations, disparitions,
fusions, des entreprises du domaine.

1.2 Quelle Intelligence artiﬁcielle ?

Les entreprises ayant appris `a stocker, g´erer massivement leurs donn´ees
depuis 10 ans, la phase suivante concerne leur analyse pour leur valorisation et
l’aide `a la d´ecision, voire de la d´ecision automatique. Apr`es s’ˆetre appel´ee ”big
data analytics” puis ”data science” cette phase fait maintenant r´ef´erence `a
une pratique d’intelligence artiﬁcielle (IA), appellation largement m´ediatis´ee,
notamment depuis les succ`es remarquables en reconnaissance d’images de-
puis 2012, en traduction automatique, d’AlphaGo en 2016 ou autour des
exp´erimentations de v´ehicules autonomes.

L’IA n’est pas une invention r´ecente car cette discipline ou plutˆot
cet ensemble de th´eories et techniques est apparue conjointement avec le
d´eveloppement des tous premiers ordinateurs (ENIAC en 1943), eux-mˆemes
cons´equences des eﬀorts, durant la deuxi`eme guerre mondiale, pour produire
rapidement des abaques de balistique puis r´ealiser les calculs de faisabilit´e de
la premi`ere bombe atomique. L’objectif initial ´etait la simulation des com-
portements du cerveau humain. C’est aussi en 1943 que Mc Culloch (neuro-
physiologiste) et Pitts (logicien) ont propos´e les premi`eres notions de neurone
formel. Notons le d´ebut de la th´eorisation de l’IA avec les travaux pionniers
d’Alan Turing en 1950 et la premi`ere apparition de la notion de perceptron,
le premier r´eseau de neurones formels, par Rosenblatt en 1957. Manque de
moyens de calcul et d’algorithmes pertinents, l’approche connexionniste de
l’IA est mise en veilleuse durant les ann´ees 70 au proﬁt de la logique formelle
(e.g. calcul des pr´edicats du premier ordre) comme outil de simulation du
raisonnement. Les syst`emes experts associant base de connaissance (r`egles
logiques), base de faits et moteur d’inf´erence ont connu un certain succ`es,
notamment avec le d´eveloppement du langage Prolog, mais on butt´e sur la
complexit´e algorithmique explosive des probl`emes NP complets. Ce fut alors,
au d´ebut des ann´ees 80, le retour massif de l’approche connexionniste avec le
d´eveloppement de l’algorithme de r´etropropagation du gradient qui a ouvert
la possibilit´e, en lien avec des moyens de calculs suﬃsamment performants,
de l’apprentissage de r´eseaux de neurones complexes. Le d´eveloppement de
l’IA s’est ensuite focalis´e dans les ann´ees 90 sur des objectifs d’apprentissage
(machine learning), qui devint plus pr´ecis´ement l’apprentissage statistique

4

(statistical learning) en cons´equence de la publication du livre ´eponyme de
Vapnik (1995). C’est toute une farandole d’algorithmes : s´eparateurs `a vaste
marge (SVM), bagging, boosting, random forest... qui provoqua `a nouveau la
mise en retrait des approches connexionnistes consid´er´ees, au mˆeme titre que
bien d’autres algorithmes souvent plus performants. Mais certains chercheurs
dont Yan le Cun, Yoshua Benjio, Geoﬀrey Hinton, ont continu´e `a d´evelopper
des structures connexionnistes sp´eciﬁques dont les fameux r´eseaux int´egrant
des produits de convolution (convolutional neural netwoks) sur des images.
L’accumulation complexe de ces couches fut nomm´ee apprentissage profond
(deep learning) avec un r´eel succ`es marketing.

Actuellement, la pr´esentation m´ediatique de l’IA diverge rapidement vers
des questions philosophiques (transhumanisme), comme celle de singula-
rit´e technologique lorsque les machines deviendront plus ”intelligentes” que
l’homme... Le d´eveloppement de l’IA soul`eve des questions ´egalement an-
xiog`enes de destruction de nombreux emplois qualiﬁ´es (Stiegler et Kourou
2015) et pas seulement des m´etiers manuels absorb´es par la robotisation
des entreprises. D’autres craintes sont li´ees aux menaces concernant la vie
priv´ee ainsi qu’aux questions ´ethiques abord´ees par ailleurs (Besse et al.
2017). Nous oublierons ces aspects pour nous focaliser sur les algorithmes
d’IA en exploitation, ceux qui impactent nos quotidiens professionnels ou
personnels, cons´equences de la dataﬁcation de nos environnements. Ces algo-
rithmes, capables de s’entraˆıner, en surfant sur la vague ou plutˆot le tsu-
nami des donn´ees, aﬁn de construire des d´ecisions automatiques, consti-
tuent le sous-ensemble historique de l’IA appel´e apprentissage automatique
ou machine learning. Plus pr´ecis´ement, nous laisserons de cˆot´e les algo-
rithmes de renforcement ou de d´ecision s´equentielle (e.g. bandit manchot)
qui sont des algorithmes d’optimisation stochastique trouvant leurs appli-
cations dans la gestion des sites de vente en ligne. Il reste alors le prin-
cipal sous-ensemble des algorithmes d’apprentissage statistique au sens de
Vapnik (1995), incluant ´egalement l’apprentissage profond ou deep learning.
Ceux-ci construisent des r`egles de d´ecision ou des pr´evisions par minimisa-
tion d’un risque, g´en´eralement une erreur moyenne de pr´evision. Leur succ`es
et la g´en´eralisation de leur utilisation sont des cons´equences directes de la
dataﬁcation (big data) du quotidien.

Marketing et data mining, ﬁnance et trading algorithmique, traduction
automatique et traitement du langage naturel (sentiment analysis), recon-
naissance faciale et analyse d’images en lien par exemple avec les v´ehicules
autonomes, aide au diagnostic, d´etection d’anomalies, pr´evision de d´efaillance
et maintenance pr´eventive dans l’industrie... sont autant de domaines d’appli-
cation des algorithmes d’apprentissage statistique, sous-ensemble de l’Intelli-
gence Artiﬁcielle b´en´eﬁciant et valorisant les masses de donn´ees en croissance

5

exponentielle.

1.3 Quels choix ?

Au sein de cet environnement, ou de cette jungle, une forme de s´election
naturelle op`ere de fa¸con drastique sur les m´ethodes et technologies associ´ees,
celles qui font leurs preuves, s’adaptent et survivent au ﬁl des mises `a jour
des librairies logicielles et les autres qui s’´eteignent car ﬁnalement ineﬃcaces
ou inadapt´ees au changement environnemental, par exemple au passage aux
´echelles volume, vari´et´e, v´elocit´e, des donn´ees. Trois facteurs ou observatoires
semblent d´eterminants pour suivre cette ´evolution en temps r´eel.

— Le logiciel libre et les librairies associ´ees, qu’elles soient accessibles en
R, Python, Spark... envahissent inexorablement le paysage ; Google ne
s’y est pas tromp´e en ouvrant les codes de Tensor Flow et simpliﬁant
son acc`es avec Keras. Les marges ﬁnanci`eres signiﬁcatives ne sont plus
apport´ees par la vente de logiciels mais par celle de services ; seuls
des logiciels libres autorisent les exp´erimentations indispensables et
permanentes des m´ethodes et algorithmes dans des situations toujours
renouvel´ees.

— Corr´elativement au logiciel

libre,

le travail, et pas seulement le
d´eveloppement de codes, devient agile et surtout collaboratif. L’en-
vironnement git et plus particuli`erement l’expansion du site github
en sont des r´ev´elateurs ; Microsoft ne s’y est pas tromp´e en rache-
tant Github. Le succ`es de structures comme celle des Instituts de
Recherche Technologiques en sont d’autres exemples qui d´epassent
les probl`emes de propri´et´e industrielle. Les donn´ees propri´etaires res-
tent conﬁdentielles mais id´ees, m´ethodes et mˆeme portions de codes
sont partag´ees, co-d´evelopp´ees. Suivre `a ce sujet le d´eveloppement du
projet franco-qu´eb´ecois DEEL (dependable and explainable learning)
pilot´e en France par l’IRT St Exup´ery et associant ing´enieurs, cher-
cheurs industriels et acad´emiques,

— Les suivis et soutenances de stages, de projets industriels, les en-
cadrements de th`eses CIFRE, les premi`eres embauches sont autant
d’exemples d’exp´erimentations en vraie grandeur. Les responsabilit´es,
p´edagogique d’un diplˆome et celle scientiﬁque de projets de recherche,
constituent un poste d’observation de premier plan, mˆeme si biais´e
par la localisation g´eographique. Cette position permet d’identiﬁer ce
qui marche, ou pas, en fonction des domaines d’applications, qu’ils
soient industriels ou publics, a´eronautiques, m´edicaux ou autre.
En d´eﬁnitive, le d´eveloppement des ressources p´edagogiques disponibles
sur le nouveau site github.com/wikistat qui fait suite et vient compl´eter

6

wikistat.fr, est la cons´equence de ces remarques pilotant un ensemble de
choix :

— choix technologiques en perp´etuelles (r)´evolutions ou disruption as-

soci´es `a des

— choix et objectifs p´edagogiques avec en cons´equence des
— moyens p´edagogiques `a mettre en œuvre.
Cette fa¸con sch´ematique de s´equencer le propos n’est ´evidemment pas
repr´esentative de la dynamique de la d´emarche. Elle enchaˆıne sur l’expos´e de
Besse et Laurent (2015) qui tentait d´ej`a de d´eﬁnir ou plutˆot caract´eriser la
fausse nouvelle Science des Donn´ees avant de d´etailler l’´evolution en cours et
`a venir du cursus de la sp´ecialit´e Math´ematiques Appliqu´ees. Revenir `a cet ar-
ticle, pourtant r´ecent, permet d’identiﬁer des technologies d´ej`a abandonn´ees,
au moins dans les cours, comme Mahaout, RHadoop, H2O..., d’autres main-
tenues ou renforc´ees : Spark, et certaines annonc´ees puis eﬀectivement intro-
duites TensorFlow.

2 Choix technologiques

D´etaillons les technologies retenues et les quelques raisons qui ont pr´esid´e

`a ces choix.

2.1 Hadoop

Une architecture de donn´ees distribu´ees associ´ee `a un syst`eme Hadoop
de gestion de ﬁchier est devenue la technologie caract´eristique voire mˆeme
embl´ematique des donn´ees massives. Celle-ci oﬀre des capacit´es de sto-
ckage et de parall´elisation des traitements incontournables mais pose des
contraintes tr`es fortes aux algorithmes susceptibles d’y ˆetre ex´ecut´es en
it´erant n´ecessairement les seules fonctions (cf. ﬁgure 1) : map parall´elisable,
schuﬄe implicite de r´epartition vers celle reduce d’agr´egation des r´esultats.
Un traitement eﬃcace de donn´ees massives est obtenu `a condition de ne pas
les d´eplacer (temps de transfert), ce sont les algorithmes ou codes de cal-
culs qui sont transf´er´es, et de ne les lire qu’une seule fois (temps de lecture
disque). C’est ainsi que certains algorithmes survivent `a ces contraintes (e.g.
k-means) et sont d´evelopp´es dans les librairies aﬀ´erentes, tandis que d’autres
(e.g. k plus proches voisins) ne passent pas `a cette ´echelle et prennent la voie
de l’extinction.

Plus pr´ecis´ement, la contrainte d’une lecture unique impose de conserver
en m´emoire les donn´ees entre deux it´erations d’un algorithme complexe aﬁn
d’´economiser des acc`es disques r´edhibitoires. C’est justement la principale

7

Figure 1 – Organisation sch´ematique des fonctionnalit´es Map-Reduce sous
Hadoop Distributed File System (HDFS).

fonctionnalit´e qui a fait l’originalit´e et le succ`es de Spark avec le principe de
resilient data set.

2.2 Spark

Cette technologie peut ˆetre comprise comme une couche logiciel (frame-
work) au-dessus d’un syst`eme de gestion de ﬁchiers et int´egrant une program-
mation (map/reduce) fonctionnelle. En plus de g´erer des ﬁchiers de donn´ees
r´esilients donc conserv´es en m´emoire entre deux it´erations, Spark propose un
ensemble de fonctionnalit´es (cf. ﬁgure 2) permettant d’adresser tout syst`eme
de gestion ou type de ﬁchiers : Hadoop, MongoDB, MySQL, ElasticSearch,
Cassandra, HBase, csv, JSON... ainsi que des capacit´es de requˆetes SQL. `A
cela s’ajoutent des modules qui oﬀrent des possibilit´es de traiter des donn´ees
en temps r´eel (streaming) ainsi que des graphes. Un dernier module (ML-
lib) propose une librairie d’algorithmes d’apprentissage supervis´es et non-
supervis´es adapt´es au passage `a l’´echelle volume.

Enﬁn, un des atouts de Spark et non des moindres, est l’API PySpark
qui permet de coder en Python toutes les op´erations cit´ees pr´ec´edemment
et, pour le b´eotien qui dispose d’une formation plus ax´ee Math´ematiques
appliqu´ees qu’Informatique, c’est tr`es utile. Il n’a pas `a apprendre encore
d’autres langages de programmation sp´eciﬁques aux donn´ees massives (Hive,
Pig) ou g´en´eralistes (Java, Scala) mais au cœur de Spark pour aborder eﬃ-
cacement ce domaine.

8

Figure 2 – La technologie Spark et son ´ecosyst`eme.

2.3 Python vs. R

La section pr´ec´edente justiﬁe d´ej`a le choix de Python comme langage per-
mettant de g´erer, traiter, analyser des donn´ees massives et distribu´ees. Plus
pr´ecis´ement, Python est un langage interpr´et´e `a tout faire, ce qui pr´esente
de nombreux avantages mais aussi des inconv´enients. Python est un langage
proc´edural comme C ou Java, c’est aussi, grˆace `a la librairie numpy, un lan-
gage matriciel comme Matlab ou R. C’est enﬁn un langage qui int`egre des
commandes fonctionnelles (e.g. map, reduce) comme Lisp. Les inconv´enients
sont de nature p´edagogique car un langage `a tout faire permet aussi d’´ecrire
des programmes n’importe comment et ce, d’autant plus que beaucoup
de d´eclarations sont implicites. N´eanmoins, ceci n’a pas empˆech´e Python
d’ˆetre choisi comme langage p´edagogique dans beaucoup d’´etablissements
et, confront´e `a des donn´ees un peu volumineuses, un ´etudiant comprend vite
ce qu’il faut ´eviter de programmer avec un langage matriciel et / ou un lan-
gage fonctionnel pour obtenir des ex´ecutions eﬃcaces. Enﬁn et cette raison
peut suﬃre, Python remplace Matlab dans beaucoup d’environnements in-
dustriels, notamment dans l’a´eronautique et l’espace, et le syst`eme SAS dans
bien d’autres domaines.

Si Python pr´esente tant d’avantages, pourquoi conserver l’enseignement
et l’utilisation de R dont les capacit´es de parall´elisation, tout du moins avec le

9

noyau actuel, sont plus complexes `a mettre en œuvre surtout sous Windows.
Certes Python se montre g´en´eralement plus eﬃcace avec en plus la possibi-
lit´e de pr´e-traiter les donn´ees en les laissant sur disque lorsqu’elles sont trop
volumineuses pour ˆetre int´egralement charg´ees en m´emoire comme l’impose
R. N´eanmoins, les fonctionnalit´es des librairies Python, notamment en Sta-
tistique et en aides graphiques `a l’interpr´etation, sont largement inf´erieures
`a ce qui est propos´e en R. Aussi, pour aborder des questions statistiques
classiques, d´eployer des m´ethodes d’exploration multidimensionnelles avec
les graphes aﬀ´erents, comprendre et interpr´eter des mod`eles ou des arbres de
d´ecision, R est beaucoup mieux arm´e que Python. Ces deux environnements
apparaissent ﬁnalement comme tr`es compl´ementaires.

Besse et al. (2016) d´eveloppent une comparaison d´etaill´ee de trois en-
vironnements : Python Scikit-learn, R, Spark MLlib, pour l’apprentissage
sur des cas d’usage de donn´ees presque massives : reconnaissance de ca-
ract`eres, syst`eme de recommandation de ﬁlms, traitement de grandes bases
de textes. Celle-ci met clairement en ´evidence la puissance de parall´elisation
de Spark pour l’ex´ecution de certaines ´etapes d’analyse, notamment tout
ce qui concerne la pr´eparation des donn´ees ou data munging. En revanche,
l’ex´ecution de certains algorithmes plus sophistiqu´es de la librairie MLlib
provoque des d´epassements m´emoires et donc des plantages intempestifs.
En eﬀet, l’apprentissage de donn´ees massives peut encourager l’entraˆınement
d’algorithmes fort complexes et l’estimation de tr`es nombreux param`etres.
Le besoin de stocker tous ces param`etres sur tous les nœuds ou calculateurs
d’un nuage Hadoop impose des contraintes m´emoire tr`es fortes et donc de
restreindre la taille des mod`eles et en cons´equence la capacit´e d’ajustement
puis de pr´evision par rapport `a une architecture int´egr´ee qui regrouperait
m´emoire et processeurs dans une seule unit´e. `A ce jour et d’exp´erience, seule
l’impl´ementation de la factorisation non n´egative de matrices (NMF pour les
syst`emes de recommandation) pr´esente un r´eel int´erˆet, pr´ecision et temps de
calcul, par rapport `a une librairie pour l’apprentissage automatique comme
Scikit-learn de Python.

2.4 Jupyter

Le d´eveloppement de codes est plus eﬃcace dans un environnement
adapt´e, un IDE (integrated development environment) comme par exemple
Eclipse. N´eanmoins pour un utilisateur et pas un d´eveloppeur, cet IDE est
trop complexe. Par ailleurs, l’actualit´e est trop souvent d´efray´ee par des
probl`emes de non reproductibilit´e de r´esultats scientiﬁques pourtant publi´es,
voire mˆeme de fraudes scientiﬁques. Elle peuvent ˆetre dues `a une simple fal-
siﬁcation des donn´ees ou `a un usage inad´equat voire malveillant de m´ethodes

10

statistiques. Pour lutter, `a son niveau, contre cette tendance d´esastreuse, le
statisticien / data scientist (cf. Donoho, 2015) se doit d’´ecrire des codes qui
rendent facile la reproductibilit´e de l’analyse parall`element `a la publication des
donn´ees. L’environnement Rmarkdown oﬀre de telles perspectives de mˆeme
que les notebooks ou calepins de Jupyter. Ces derniers permettent d’inclure
codes, commentaires (markdown), formules en LATEXet r´esultats graphiques
au sein d’un mˆeme ﬁchier r´e-ex´ecutable pas `a pas en enchaˆınant les clics.
Ces sortes d’IDE sont tellement pratiques, ouverts `a tout langage interpr´et´e
(Python, R, Julia, Scala...), qu’ils deviennent le cadre de la grande majo-
rit´e des ressources p´edagogiques et tutoriels disponibles sur internet. C’est le
choix op´er´e dans Wikistat 2.0. Noter que Jupyter d´eveloppe ´egalement une
interface Jupyter lab pr´esentant le calepin en association `a d’autres fenˆetres
comme dans RStudio ou Matlab et visualisant une matrice, un ﬁchier csv.
ou une image.

2.5 GitHub

Il est important de former les ´etudiants `a une gestion de projet agile dans
un environnement de travail coop´eratif ad´equat. L’apprentissage et l’utili-
sation des fonctionnalit´es de Git sont donc fortement conseill´ees si ce n’est
impos´ees. Le site oﬀrant un tel service gratuitement, lorsque les d´epˆots sont
publics, et le plus utilis´e est GitHub. Le succ`es de ce site est principale-
ment la cons´equence d’une utilisation massive par une grande majorit´e des
d´eveloppeurs de logiciels libres. Bien ´evidemment le rachat par Microsoft
est assez antinomique et il suﬃrait de peu de choses, de peu de modiﬁca-
tions dans le fonctionnement de ce site pour provoquer la fuite de tous ces
d´eveloppeurs / collaborateurs vers un autre site. Il suﬃra de suivre le mou-
vement mais actuellement, GitHub reste une r´ef´erence et mˆeme une vitrine
pour aﬃcher des comp´etences de d´eveloppeur ou de data scientist sur son
CV.

2.6 GPU et Cloud computing

Les derniers choix technologiques concernent l’environnent mat´eriel de
travail. Il devient assez facile `a un ´etudiant de s’´equiper avec un ordinateur
portable mais il est parfois plus ﬁable de disposer de salles aﬁn d’uniformi-
ser l’environnement de travail. R est tr`es facile `a charger et installer pour
tout syst`eme d’exploitation. En revanche, les diﬀ´erentes versions de Python,
les d´ependances complexes entre les librairies, peuvent faire ´emerger des
probl`emes. Ceci se complique lorsqu’il s’agit d’utiliser des algorithmes plus
sophistiqu´es comme XGBoost ou l’API PySpark. Dans ces derniers cas, un en-

11

vironnement Linux (e.g. Ubuntu) est vivement conseill´e ; c’est plus rarement
le choix des ´etudiants pour leur poste personnel. Enﬁn entraˆıner des algo-
rithmes complexes d’apprentissage profond avec TensorFlow ou optimiser ﬁ-
nement les hyper-param`etres de XGBoost rendent indispensable l’acc`es `a une
carte graphique GPU. Certes quelques ´etudiants gamers en disposent mais
ils sont des exceptions et le coˆut unitaire d’une machine devient cons´equent.
Pour toutes ces raisons, nous g´erons deux salles de travaux pratiques au sein
du d´epartement de Math´ematiques dont la moiti´e des postes viennent d’ˆetre
´equip´es de cartes GPU.

Ils ne sont pas encore op´erationnels mais des tutoriels ou s´eances de
travaux pratiques vont ˆetre mont´es pour initier les ´etudiants `a l’utili-
sation de services de cloud computing. Deux solutions sont en cours de
test et d’´evaluation : Amazon Web Service (AWS) et Google cloud. Le
site rosettaHub oﬀre un acc`es `a AWS en mutualisant, entre un ensemble
d’´etudiants d’un mˆeme ´etablissement, les forfaits d’utilisation gratuite mais
les capacit´es de calcul semblent limit´ees. Les possibilit´es oﬀertes par la
Google Cloud Platform Education sont s´eduisantes. Par ailleurs, l’industrie
a´eronautique locale, ou plutˆot sa composante recherche surtout en analyse
d’image, utilise majoritairement Google cloud tandis que Continental Auto-
motive a fait le choix d’AWS pour la gestion industrielle de ses donn´ees de
suivi de production. L’acquisition de comp´etence vis´ee est la suivante : com-
ment, une fois un projet protoptyp´e sur un poste personnel, le transf´erer sur
un site de cloud computing pour le passage `a une ´echelle op´erationnelle ; une
introduction `a Docker sera envisag´ee dans un proche avenir aﬁn d’automati-
ser au mieux le transfert.

3 Objectifs p´edagogiques

Le facteur essentiel `a prendre en compte est la tr`es forte volatilit´e des
m´ethodes, algorithmes et surtout celle des technologies concern´ees par l’in-
telligence artiﬁcielle. D’une ann´ee sur l’autre il faut ˆetre capable d’int´egrer
de nouveaux avatars d’algorithmes devenus incontournables dans certains do-
maines et les technologies aﬀ´erentes. Toujours plus ﬂexible et v´eloce, il faut
pouvoir adapter, en cours d’ann´ee, les supports p´edagogiques `a la derni`ere
version d’une librairie ; l’environnement collaboratif et agile Git est indispen-
sable `a la satisfaction de ces contraintes.

Attention, ce serait une erreur de limiter la formation aux seuls aspects
technologiques. Le contenu de cet article peut prˆeter `a confusion mais il s’agit
bien d’une formation d’ing´enieurs de sp´ecialit´e Math´ematiques appliqu´ees
int´egrant par ailleurs des cours fondamentaux en Optimisation, Probabi-

12

lit´es, Statistique, Signal (Fourier, spline, ondelettes)... indispensables `a une
compr´ehension ﬁne des algorithmes et m´ethodes d´ecrites, de leurs propri´et´es
th´eoriques, de leurs limites. Contenus fondamentaux qui ne se d´emodent pas
mais diﬃciles `a faire appr´ehender aux ´etudiants en dehors de cours et tra-
vaux dirig´es ﬁnalement plus traditionnels mˆeme si des p´edagogiques actives
par projet ou cours invers´es sont introduites.

3.1 Former `a l’autoformation

Ce besoin de veille et d’adaptation en temps r´eel aux derni`eres m´ethodes
et technologies restera un objectif prioritaire pour les ´etudiants une fois entr´es
dans le monde du travail. Il ne s’agit donc pas de former simplement les
´etudiants `a des m´ethodes et des technologies mais plutˆot de les entraˆıner
`a les apprendre par eux-mˆemes avec les outils ad´equats et eﬃcaces : des
tutoriels sous la forme de calepins (notebooks jupyter) sont du type de ceux
que l’on trouve `a profusion sur internet.

Il s’agit donc de r´epondre `a la question : comment faire acqu´erir des
comp´etences d’auto-formation aux ´etudiants et comment les ´evaluer ? Une
piste possible `a creuser consiste `a mettre les ´etudiants dans la situation que
nous rencontrons nous-mˆemes, enseignants / chercheurs, dans notre recherche
ou pour simplement mettre `a jour nos connaissances et nos comp´etences. Elle
passe n´ecessairement par la recherche des bonnes r´ef´erences bibliographiques
et ressources p´edagogiques. La mise `a disposition d’une s´election soigneuse de
telles ressources sous le format de tutoriels pointant (hyperliens) vers des sup-
ports de cours ou vignettes s’av`ere un choix eﬃcace car oﬀrant un ensemble
coh´erent de connaissances `a d´ecouvrir et mettre en œuvre sur un ensemble de
cas d’usages r´ealistes aﬁn d’acqu´erir les connaissances et comp´etences vis´ees.

3.2 Motivation et jeu s´erieux

Fournir des ressources p´edagogiques adapt´ees aux ´etudiants est un pre-
mier pas. Leur faire utiliser ces ressources, les amener `a collaborer, `a auto-
apprendre avec une grande part d’autonomie, `a s’engager, en bref les motiver
`a travailler, est le deuxi`eme pas. Cette ´etape, au cœur du processus apprendre
`a apprendre, repose depuis l’ann´ee acad´emique 2015-2016 sur un jeu s´erieux
bas´e sur un concours de pr´evision par apprentissage automatique.

Cette notion de probl`eme `a r´esoudre dans le contexte d’un concours n’est
pas originale, elle a ´et´e largement popularis´ee par le concours Netﬂix de re-
commandation de ﬁlms avec un prix d’un million de dollars mais pr´e-existait
dans d’autres contextes : concours d’analyse et de description de donn´ees
par l’Association Am´ericaine de Statistique, concours de pr´evision lors des

13

congr`es de chimiom´etrie. Actuellement le site Kaggle (rachet´e par Google en
mars 2017) aﬃche r´eguli`erement des oﬀres de concours prim´es ou non. Cette
id´ee a ´et´e reprise en France `a des ﬁns p´edagogiques par l’´Ecole Normale
Sup´erieure, l’ENSAE ParisTech ou l’Universit´e Paris-Saclay. Nous testons
ou faisons tester par les ´etudiants (projets tutor´es) ces concours depuis leur
cr´eation aﬁn d’en appr´ecier les int´erˆets p´edagogiques et limites mais la mise
en place d’un concours ou d´eﬁ sp´eciﬁque fut la r´eponse adapt´ee `a notre en-
vironnement et nos objectifs.

En eﬀet, plusieurs raisons :
— maˆıtrise du sujet, de son niveau de complexit´e,
— maˆıtrise du calendrier de d´ebut (octobre) et ﬁn (janvier) de concours,
— ´emulation nettement plus forte entre pairs plutˆot qu’en concurrence

avec des professionnels anonymes,

nous ont pouss´es `a l’organisation locale d’un d´eﬁ toulousain, rejoint succes-
sivement ensuite par des ´equipes de masters des universit´es de Bordeaux,
Pau puis Rennes, Nantes, Paris, Lyon. Ce sont plus de 40 ´equipes de 4 `a 5
´etudiant-e-s qui se sont aﬀront´ees entre octobre 2016 et mi-janvier 2017, 55
pour le d´eﬁ grosses data de 2018. D’autres masters s’associant `a ce projet,
nous attendons la participation de plus de 300 ´etudiants pour le d´eﬁ IA 2019.

Les sujets et jeux de donn´ees aﬀ´erents ´evoluent chaque ann´ee :
— 2016 : construction d’un syst`eme de recommandation de ﬁlms `a partir

des donn´ees publiques du site Movie Lens,

— 2017 : pr´evision du nombre de v´elos sur un ensemble de stations de
v´elos en libre service dans diﬀ´erentes villes : Paris, Lyon, Toulouse
puis New-York,

— 2018 : Collaboration avec M´et´eo France pour la pr´evision de
temp´eratures par adaptation statistique des mod`eles d´eterministes
ARPEGE et AROME,

— 2019 : Collaboration avec AIBUS DS pour d´etecter la pr´esence ou non

d’une ´eolienne sur une photo satellite.

Comme sur les sites de type Kaggle, le concours est organis´e en deux
phases. Dans la premi`ere (3 mois), le classement est public, ´etabli sur une
partie de l’´echantillon test, et chaque ´equipe tente de progresser dans le clas-
sement r´eguli`erement mis `a jour en testant, exp´erimentant, les m´ethodes
accessibles et d´ecrites dans les tutoriels. La deuxi`eme ´etape est le classe-
ment ﬁnal, fourni par la derni`ere solution de chaque ´equipe appliqu´ee `a la
partie conﬁdentielle de l’´echantillon test de fa¸con `a traquer un possible sur-
apprentissage, pi`ege bien connu de ces techniques `a faire exp´erimenter par
les ´etudiants.

La motivation, l’engagement et l’entrain des ´etudiants ont ´et´e au rendez-
vous comme le montre r´eguli`erement la restitution des r´esultats lors d’une

14

journ´ee associant expos´es acad´emiques, industriels et pr´esentations des solu-
tions les plus performantes.

3.3

´Evaluation

L’´evaluation de l’unit´e de formation associ´ee au d´eﬁ d´ecoule facilement
de son organisation. Les performances des solutions propos´ees assorties d’un
expos´e oral permettent de valider ﬁnement quelles sont les comp´etences ac-
quises. Bien entendu tous les ´etudiants ne sont pas des geeks passionn´es consa-
crant leurs soir´ees `a am´eliorer leur solution pour grignoter des places dans
le classement. Pour celles et ceux, moins motiv´es par l’esprit de comp´etition
d’un concours, l’´evaluation de l’UF repose toujours sur la pr´esentation orale
de la d´emarche mise en place et aussi sur un objectif de r´esultat (base line) `a
atteindre a minima. L’objectif de ce r´esultat garantit en eﬀet que le groupe
d’´etudiants maˆıtrise les bases indispensables et n´ecessaires `a la mise en œuvre
des algorithmes d’apprentissage et `a leur optimisation ; la pr´esentation orale
t´emoigne qu’ils sont capables d’en rendre compte, qu’ils maˆıtrisent leur sujet
en expliquant les options et choix qu’ils ont ´et´e amen´es `a engager.

4 Contenus et ressources p´edagogiques

Les projets de contenus p´edagogiques de la sp´ecialit´e Math´ematiques
Appliqu´ees de l’INSA de Toulouse ont d´ej`a ´et´e introduits par Besse et
Laurent (2015) ; ils sont maintenant en place avec ´evidemment quelques
adaptations. Les contenus d´etaill´es (ﬁches ECTS) sont disponibles sur le
site de l’´Etablissement. Nous ne d´etaillerons pas toutes les Unit´es de Forma-
tions (UFs) mais donnerons quelques indications sur l’organisation des plus
sp´eciﬁques `a l’enseignement de l’IA et prenant en compte l’environnement
industriel toulousain, source de tr`es nombreuses oﬀres de stages et d’emplois,
principalement depuis 2017.

4.1 Unit´es de formation en IA

l’ancienne

Historiquement,

et
Mod´elisation, devenue Math´ematiques Appliqu´ees en 2018, avait deux
orientations, l’une d´eterministe ou num´erique, l’autre statistique et stochas-
tique. Ce clivage est h´erit´e de la structuration des ´equipes acad´emiques de

sp´ecialit´e G´enie Math´ematique

15

recherche toulousaines mais c’est un anachronisme pour le monde industriel.
Combler ce foss´e,
lors de la mise en place de la derni`ere maquette a
conduit `a renforcer le tronc commun de 4`eme ann´ee (M1), notamment les
enseignements d’optimisation, analyse fonctionnelle, traitement du signal,
images. Avec la mˆeme ﬁnalit´e, des UFs sont optionnelles en 5`eme ann´ee
(M2) mais celle basique d’apprentissage automatique est choisie par tous les
´etudiants quelque soit leur majeure.

Ces choix sont fortement d´etermin´es par le tropisme a´eronautique tou-
lousain. Jusqu’en 2015, 2016, la majorit´e des emplois de data mining, data
science ´etaient propos´es dans la r´egion parisienne, principalement en lien avec
le marketing et la vente en ligne. Depuis la promotion 2017, nous assistons `a
une tr`es forte demande locale dans le secteur industriel, a´eronautique et sous-
traitance, mais aussi automobile (Continental Automotive, Renault software
lab). Cette demande a largement confort´e les choix du comit´e d’orientation
du d´epartement pour renforcer les th´ematiques pour les applications indus-
trielles plutˆot que la vente en ligne. Cet objectif a converg´e naturellement avec
le rapprochement des deux orientations, d´eterministes et stochastiques du
d´epartement pour apporter des r´eponses concr`etes aux besoins industriels ac-
tuels bien identiﬁ´es : analyse de sensibilit´e, m´eta-mod`eles, d´etection d’anoma-
lies, maintenance pr´edictive, qui sont de plus assortis de structures de donn´ees
de tr`es grande dimension : signaux, courbes, images. Ces sp´eciﬁcit´es locales
ont largement motiv´e les renforcements m´ethodologiques de la 4`eme ann´ee
(optimisation non diﬀ´erentielle, optimisation stochastique et s´equentielle, si-
gnal, image) de mˆeme que ceux technologiques de la 5`eme.

En plus des UFs avec des contenus sp´eciﬁques dont certaines optionnelles :
plan d’exp´erience, ﬁabilit´e, analyse de sensibilit´e, image, calcul stochastique,
assimilation de donn´ees, trois UFs de 5`eme ann´ee (M2) concernent plus par-
ticuli`erement les applications de l’IA. Elles sont structur´ees en trois couches.

4.1.1 Apprentissage automatique ou statistique

La premi`ere couche est suivie par tous les ´etudiants, c’est l’UF de base
qui d´ecrit les propri´et´es de l’ensemble des m´ethodes et algorithmes d’appren-
tissage les plus utilis´es, de la r´egression logistique aux forˆets al´eatoires en
passant par l’analyse discriminante et les k plus proches voisins, les arbres
binaires de d´ecision, le boosting. Une introduction est propos´ee aux supports
`a vaste marge (SVM) et aux r´eseaux de neurones ; les m´ethodes d’imputa-
tion de donn´ees manquantes ainsi que celles de d´etection d’anomalies sont
´egalement trait´ees de mˆeme que les questions ´ethiques : biais et discrimi-
nation, droit `a l’explication, mises en exergue par le d´eploiement du RGPD
(r`eglement g´en´eral sur la protection des donn´ees).

16

4.1.2 Apprentissage en grande dimension

La deuxi`eme couche est suivie par les ´etudiants de l’orientation sto-
chastique. Elle vise `a compl´eter la pr´ec´edente en apportant des ´el´ements
th´eoriques et m´ethodologiques sp´eciﬁques aux donn´ees de grande dimen-
sion : signaux et images. Ce cours aborde les notions de s´election de
mod`eles et s´election de variables dans un mod`ele lin´eaire en grande dimension
(p´enalisation Ridge, Lasso...), les m´ethodes de classiﬁcation lin´eaire et non
lin´eaire, en particulier les SVM. L’agr´egation de classiﬁeurs est ´egalement
introduite. Nous abordons les m´ethodes classiques en r´egression non pa-
ram´etrique : r´egressogramme, estimateurs `a noyau, splines, estimateurs par
projection sur des bases orthonorm´ees (Fourier, ondelettes), estimation par
seuillage sur des bases d’ondelettes, ceci aﬁn de traiter des donn´ees fonc-
tionnelles ou des images. Enﬁn, les r´eseaux de neurones et une introduction
au deep learning, notamment aux r´eseaux convolutionnels sont trait´es. Les
cours sont associ´es `a un volume ´equivalent de travaux pratiques sous forme
de tutoriels en Python ou R. La d´etection d’anomalies dans des donn´ees
fonctionnelles est ´egalement trait´ee lors des travaux pratiques.

En plus d’un contrˆole rapide sur table, ces deux UFs sont ´evalu´ees par
le biais d’un projet r´ealis´e en deux ´etapes par les ´etudiants, une par UF.
La nature ou plutˆot la complexit´e des donn´ees qui sont renouvel´ees chaque
ann´ee justiﬁe de cette organisation. Le sujet 2017-2018 concernait l’analyse
de donn´ees (acc´el´erom`etre, gyroscope) issues d’un smartphone pour identiﬁer
l’activit´e de son porteur. Il s’agissait d’un probl`eme de classiﬁcation super-
vis´ee `a traiter sur la base de donn´ees transform´ees `a l’aide d’algorithmes
d’apprentissage automatique usuels pour la premi`ere UF tandis que l’ana-
lyse des donn´ees brutes `a l’aide de neurones profonds concernait la deuxi`eme
UF. Ce cas d’usage est maintenant un tutoriel qui sert de ﬁl rouge pour
les travaux pratiques ainsi que pour des actions de formation continue. Le
sujet 2018–2019 portera ´egalement sur l’analyse de signaux mais cette fois
physiologiques (EEG) pour de l’aide au diagnostic.

4.1.3 Technologies de l’IA

La troisi`eme couche est sp´eciﬁque aux ´etudiants de la majeure Science
des Donn´ees. Elle apporte les comp´etences n´ecessaires `a l’utilisation des ou-
tils technologiques r´ecents de traitement des donn´ees massives (Spark) mais
aussi de l’apprentissage profond (Keras, TensorFlow) ou du cloud computing
(Google Cloud). L’´etudiant est amen´e `a d´evelopper une d´emarche critique
quant au choix ou non d’utiliser ces technologies, ce qui n’est pas toujours
n´ecessaire. Elle vise ´egalement `a rapprocher le plus possible l’´etudiant de

17

l’exp´erience pratique du m´etier de data scientist et du travail de manipula-
tion et de mise en forme des donn´ees (data munging) qui pr´ec`ede l’application
d’algorithmes d’apprentissage.

Elle est bas´ee sur l’analyse de cas d’usage qui abordent des types de
donn´ees diﬀ´erents et compl´ementaires sur lesquels des algorithmes r´ecents
ont d´ej`a fait leur preuve. La classiﬁcation d’images (CatsVsDogs) et la recon-
naissance de caract`eres (MNIST), `a l’aide de r´eseaux de neurones convolutifs
y sont trait´es. L’´etudiant est ´egalement amen´e `a manipuler des r´eseaux pr´e-
entraˆın´es : ResNet, Inception (transfert d’apprentissage). Nous abordons
´egalement le traitement automatique du langage naturel (NLP) `a travers la
classiﬁcation supervis´ee de descriptions textuelles de produits (Cdiscount)
en pr´esentant les diﬀ´erentes m´ethodes de pr´e-traitement du texte (Racinisa-
tion, Tokenizing), la vectorisation de ces donn´ees (term frequency minus in-
verse document frequency, Word embedding, Word2vec), et l’application d’al-
gorithmes d’apprentissage sur ces donn´ees. Les r´eseaux r´ecurrents (LSTM)
sont appliqu´es sur ces donn´ees pour la g´en´eration automatique de contenu.
La recommandation de Films (movieLens) par ﬁltrage collaboratif `a l’aide de
m´ethodes de factorisation (d´ecomposition en valeurs singuli`eres, factorisation
non n´egative de matrices) est ´egalement ´etudi´ee.

4.2 Wikistat 1.0

Les ressources p´edagogiques de ces trois UFs sont disponibles en ligne
comme celles de l’UF de 4`eme ann´ee : logiciels et exploration statistiques.
Un premier site : wikistat.fr regroupe depuis plusieurs ann´ees des res-
sources mises `a disposition par les intervenants. Elles prennent la forme de
vignettes (ﬁchier pdf issu d’un source LATEX) de cours et travaux pratiques ;
vignettes par m´ethode, algorithme ou famille de m´ethodes. Ce site, sert et ser-
vira toujours de r´ef´erence de cours. Il est largement consult´e par les ´etudiants
locaux ainsi que ceux francophones comme le montrent quelques statistiques
du tableau 1 et de la ﬁgure 3 relatives au mois de mai 2016. Il est actuelle-
ment toujours aussi consult´e mais l’ouverture du deuxi`eme site en biaise les
statistiques.

Les supports p´edagogiques en anglais sont l´egion sur le web mais nette-
ment moins accessibles en fran¸cais et au niveau M2 vis´e ; ce qui explique le
succ`es de ce site.

4.3 Wikistat 2.0

Le site pr´ec´edent wikistat.fr est d´ej`a ancien et bien r´ef´erenc´e par les
moteurs de recherche mais il n´ecessitait une refonte pour ˆetre int´egr´e `a un

18

Table 1 – Nombre de consultations des 5 documents les plus charg´es en
mai 2016 sur le site h´ebergeant physiquement wikistat.fr. Les deux pre-
miers sont des polycopi´es obtenus par simple compilation des vignettes de
wikistat.fr.

Figure 3 – R´epartition g´eographique des consultations de wikistat.fr mon-
trant une pr´esence signiﬁcative de l’Afrique francophone.

19

environnement collaboratif professionnel. Le choix strat´egique, ´economique
en temps de travail, a ´et´e de l’int´egrer ou plutˆot d’int´egrer virtuellement tous
les supports de cours (vignettes) existants comme cibles de liens hypertextes
pr´esents dans les tutoriels (calepins ou jupyter notebooks) ou cas d’usage
du site github.com/wikistat. L’ensemble des ressources p´edagogiques est
donc accessible selon deux entr´ees : `a partir d’un expos´e classique de cours
en pr´esentiel et y faisant r´ef´erence ou `a partir de cas d’usage ex´ecutables
en pr´esentiel (travaux pratiques) ou en autonomie (tutoriels). La mise en
place du site a b´en´eﬁci´e d’une aide `a l’innovation p´edagogique de l’INSA de
Toulouse pour ˆetre rendu rapidement op´erationnel.

Le site est structur´e en 5 saisons d´ecoup´ees en ´episodes d´eroulant la
chronologie classique d’un cours (ﬁcher README) `a travers des tutoriels (ca-
lepins) pouvant ˆetre utilis´es en autoformation ou comme support de tra-
vaux pratiques. Les UFs pr´ec´edentes constituent les saisons 3 `a 5. Le for-
mat des calepins (notebook) permet d’int´egrer, selon les besoins ou n´ecessit´es
p´edagogiques, des exercices ou de simples questions aﬁn de motiver la
r´eﬂexion des imp´etrants au del`a d’une simple ex´ecution.

5 Conclusion

Les retours des ´etudiants `a propos de ces ressources de cours, des tutoriels
et plus particuli`erement du d´eﬁ sont dans l’ensemble tr`es positifs avec des
remarques prises en compte chaque ann´ee aﬁn d’am´eliorer le dispositif. Ainsi,
les ´etudiants, au moins ceux toulousains, se rencontrent d´ebut octobre lors
du lancement du d´eﬁ et lors de la s´eance de restitution. Les pr´esentations
orales de l’organisation g´en´erale, des donn´ees et des objectifs sont ﬁlm´ees et
rendues accessibles aux participants des autres universit´es. Le site aﬃche une
repr´esentation graphique de l’´evolution des classements publics. Les calen-
driers des diﬀ´erents ´etablissements sont coordonn´es au mieux des possibilit´es
et ﬂexibilit´es de chacun.

Le principal objectif de motivation des ´etudiants par le d´eﬁ est atteint ;
celle-ci a largement d´epass´e le seul but trivial de validation d’une unit´e de
formation. Ce n’est pas non plus un classement de sortie ancestral qui pousse
les ´etudiants `a travailler de fa¸con solitaire. C’est plutˆot la r´eussite d’un pro-
jet conduit par une ´equipe de fa¸con agile et collaborative, piment´ee par la
comp´etition entre formations th´ematiquement voisines.

Cette motivation permet d’atteindre l’objectif d’auto-apprentissage. Les
solutions exp´eriment´ees et rendues op´erationnelles ont largement exploit´e
les programmes pr´evus des UFs d’apprentissage statistique. Les ´etudiants
ont spontan´ement approfondi des notions (optimisation, p´enalisation) ou des

20

technologies (Python, PySpark, librairies R, GPU, cloud computing) qui se
sont av´er´ees ou s’av´ereront indispensables au bon d´eroulement des projets
car ils sont directement confront´es aux probl`emes pos´es par un passage aux
´echelles volume et vari´et´e des donn´ees.

Le projet p´edagogique relat´e dans cet article se construit en avan¸cant,
exp´erimentant. Les retours des ´etudiants sont tr`es positifs, leur insertion ac-
tuelle, dans les tr`es nombreux stages faisant appel aux comp´etences vis´ees,
est tr`es bonne. L’insertion professionnelle l’est ´egalement. Ainsi, pour la
sp´ecialit´e Math´ematiques Appliqu´ees de l’INSA de Toulouse et pour les pro-
motions 2016 et 2017, 75 % avaient sign´e un contrat avant la ﬁn de leur stage,
50 % ont un CDI. Tous les retours nous incitent `a poursuivre et d´evelopper
cette exp´erience.

Attention, il faut rester prudent et r´ealiste, les ressources p´edagogiques
mises `a disposition requi`erent une veille technologique permanente, et donc
des moyens humains aﬀ´erents, pour r´esister `a une obsolescence tr`es rapide des
technologies mises en œuvre. C’est la condition incontournable pour pr´eserver
l’excellente insertion actuelle des ´etudiants en anticipant de possibles et tr`es
probables retournements de conjonctures. Des bases math´ematiques solides
et largement immuables ainsi que des comp´etences technologiques de pointe
sont les deux piliers de notre strat´egie pour continuer `a surfer, ou faire surfer
par les ´etudiants, la vague des donn´ees massives en exploitant les m´ethodes
de l’IA.

21

R´ef´erences

[1] Besse P., Laurent B.

(2015). De Statisticien `a Data Scientist ;
d´eveloppements p´edagogiques `a l’INSA de Toulouse, Statistique et En-
seignement, Vol. 7(1), 75-93.

[2] Besse P., Guillouet B., Loubes J.-M. (2016). Apprentissage sur donn´ees
massives, trois cas d’usage avec R, Pyhton, Spark , Apprentissage Statis-
tique et Donn´ees Massives, Maumy-Bertrand M., Saporta G. et Thomas
Agnan C. (eds), Technip.

[3] Besse P., Castets-Renard C., Garivier A. (2017). Loyaut´e des D´ecisions
Algorithmiques, Contribution au D´ebat ”´Ethique et Num´erique” de la
CNIL, hal-0154470.

[4] Donoho, D. (2015). 50 years of data science, in Princeton NJ, Tukey

Cen- tennial Workshop.

[5] Hand, D. (2006). Classiﬁer technology and the illusion of progress, Sta-

tistical Science, 21 (1), 1-15.

[6] Stiegler B., Kyrou A. (2015). L’Emploi est Mort, Vive le Travail !,

Fayard.

[7] Vapnik V. (1995). The Nature of Statistical Learning Theory, Springer.

22

