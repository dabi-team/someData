1
2
0
2

n
a
J

7

]

G
L
.
s
c
[

1
v
9
2
4
2
0
.
1
0
1
2
:
v
i
X
r
a

Neural Spectrahedra and Semideﬁnite Lifts: Global Convex
Optimization of Polynomial Activation Neural Networks in
Fully Polynomial-Time

Burak Bartan
Department of Electrical Engineering
Stanford University
bbartan@stanford.edu

Mert Pilanci
Department of Electrical Engineering
Stanford University
pilanci@stanford.edu

January 8, 2021

Abstract

The training of two-layer neural networks with nonlinear activation functions is an important
non-convex optimization problem with numerous applications and promising performance in layer-
wise deep learning. In this paper, we develop exact convex optimization formulations for two-layer
neural networks with second degree polynomial activations based on semideﬁnite programming.
Remarkably, we show that semideﬁnite lifting is always exact and therefore computational com-
plexity for global optimization is polynomial in the input dimension and sample size for all input
data. The developed convex formulations are proven to achieve the same global optimal solution
set as their non-convex counterparts. More speciﬁcally, the globally optimal two-layer neural
network with polynomial activations can be found by solving a semideﬁnite program (SDP) and
decomposing the solution using a procedure we call Neural Decomposition. Moreover, the choice
of regularizers plays a crucial role in the computational tractability of neural network training. We
show that the standard weight decay regularization formulation is NP-hard, whereas other simple
convex penalties render the problem tractable in polynomial time via convex programming. We
extend the results beyond the fully connected architecture to diﬀerent neural network architec-
tures including networks with vector outputs and convolutional architectures with pooling. We
provide extensive numerical simulations showing that the standard backpropagation approach of-
ten fails to achieve the global optimum of the training loss. The proposed approach is signiﬁcantly
faster to obtain better test accuracy compared to the standard backpropagation procedure.

1

Introduction

We study neural networks from the optimization perspective by deriving equivalent convex optimiza-
tion formulations with identical global optimal solution sets. The derived convex problems have
important theoretical and practical implications concerning the computational complexity of optimal
training of neural network models. Moreover, the convex optimization perspective provides a more
concise parameterization of neural network models that enables further analysis of their interesting
properties.

In non-convex optimization, the choice of optimization method and its internal hyperparameters,
such as initialization, mini-batching and step sizes, have a considerable eﬀect on the quality of the
learned model. This is in sharp contrast to convex optimization problems, where locally optimal
solutions are globally optimal and optimizer parameters have no inﬂuence on the solution and therefore
the model. Moreover, the solutions of convex optimization problems can be obtained in a very robust,

1

 
 
 
 
 
 
Figure 1: ReLU (left) and swish (right) activation functions and their second degree polynomial
approximations. ReLU activation: σ(u) = max(0, u) and its polynomial approximation: σ(u) =
0.09u2 + 0.5u + 0.47. Swish activation: σ(u) = u(1 + e−u)−1 and its polynomial approximation:
σ(u) = 0.1u2 + 0.5u + 0.24.

eﬃcient and reproducible manner thanks to the elegant and extensively studied structure of convex
programs. Therefore, our convex optimization based globally optimal training procedure enables the
study of the neural network model and the optimization procedure in a decoupled way. For instance,
step sizes employed in the optimization can be considered hyperparameters of non-convex models,
which aﬀect the model quality and may require extensive tuning. For a classiﬁcation task, in our
convex optimization formulation, step sizes as well as the choice of the optimizers are no longer
hyperparameters to obtain better classiﬁcation accuracy. Any convex optimization solver can be
applied to the convex problem to obtain a globally optimal model.

Various types of activation functions were proposed in the literature as nonlinearities in neural
network layers. Among the most widely adopted ones is the ReLU (rectiﬁed linear unit) activation
given by σ(u) = max(0, u). A recently proposed alternative is the swish activation σ(u) = u(1+e−u)−1,
which performs comparably well [39]. Another important class is the polynomial activation where the
activation function is a scalar polynomial of a ﬁxed degree. We focus on second degree polynomial
activation functions, i.e., σ(u) = au2 + bu + c. Although polynomial coeﬃcients a, b, c can be regarded
as hyperparameters, it is often suﬃcient to choose the coeﬃcients in order to approximate a target
nonlinear activation function such as the ReLU or swish activation. ReLU and swish activations are
plotted in Figure 1 along with their second degree polynomial approximations.

Our derivation of the convex program for polynomial activations leverages convex duality and the
S-procedure, and can be stated as a simple semideﬁnite program (SDP). We refer the reader to [38] for
a survey of the S-procedure and applications in SDPs. In addition, another commonly used activation
function in the literature, quadratic activation, is a special case of polynomial activations (b = c = 0)
and we devote a separate section to this case (Section 5). The corresponding convex program is an
SDP and takes a simpler form.

Main aspects of our work that diﬀer from others in the literature that study the optimization
landscape of two-layer neural networks (e.g. see section 1.2) are the following: Our results (1) provide
global optimal solutions in fully polynomial time (polynomial in all problem parameters), (2) uncover
an important role of the regularizer in computational tractability, (3) hold for arbitrary loss function
and other network architectures such as vector output, convolutional and pooling, (4) are independent
of the choice of the numerical optimizer and its parameters.

We summarize the types of neural network architectures considered in this work and the corre-
sponding convex problems that we have derived to train them to global optimality in Table 1. The
fourth column of Table 1 shows the upper bounds for critical width m∗, i.e., the optimal number of
neurons that one needs for global optimization of any problems with number of neurons m ≥ m∗. The
ﬁfth column, named ”construction algorithm”, refers to the method for obtaining the optimal neural
network weights from the solution of the associated convex program. The last column contains the

2

−5−4−3−2−1012345u012345σ(u)relu activationpolynomial approximation−5−4−3−2−1012345u024σ(u)swish activationpolynomial approximationreferences to the theorems for each result.

1.1 Overview of Our Contributions

• We show that the standard optimization formulation for training neural networks fθ(x) =
(cid:80)m
j=1 σ(xT uj)αj with trainable parameters θ = (u1, . . . , um, α1, . . . , αm) and degree two poly-
nomial activations σ(u) = au2 + bu + c, training data X = [x1, . . . , xn]T ∈ Rn×d, y ∈ Rn, and
(cid:96)2
2 regularization on the parameters given by

min
θ

(cid:96)(fθ(X), y) + β

m
(cid:88)

j=1

((cid:107)uj(cid:107)2

2 + (cid:107)αj(cid:107)2
2)

(1)

is computationally intractable via a reduction to the NP-hard subset sum problem.

• Surprisingly, for quadratic activation networks, σ(u) = u2, we show that modifying the quadratic

weight decay regularization to cubic regularization

min
θ

(cid:96)(fθ(X), y) + β

m
(cid:88)

j=1

((cid:107)uj(cid:107)3

2 + (cid:107)αj(cid:107)3
2)

(2)

enables global optimization in fully polynomial time via convex semideﬁnite programming. The
computational complexity is polynomial in all problem parameters (n, d, m).

• Furthermore, for any degree two polynomial activation σ, the non-convex neural network training

problem

min
θ s.t. (cid:107)uj (cid:107)2=1 ,∀j∈[m]

(cid:96)(fθ(X), y) + β(cid:107)α(cid:107)1

(3)

can be equivalently stated as a convex semideﬁnite problem and globally solved in fully poly-
nomial time. In fact, the cubic regularization strategy in (2) is a special case of this convex
program. The result holds universally for all input data without any conditions and also holds
when β → 0.

• In deriving the convex formulations, we identify a concise re-parameterization of the neural net-
work parameters that enables exact convexiﬁcation by removing the redundancy in the classical
overparameterized formulation. This is similar in spirit to the semideﬁnite lifting procedure in
relaxations of combinatorial optimization problems. In contrast to these relaxations, we show
that our lifting is always exact as soon as the network width exceeds a critical threshold which
can be eﬃciently determined.

• We develop a matrix decomposition procedure called Neural Decomposition to extract the opti-
mal network parameters from the solution of convex optimization, which is guaranteed to pro-
duce an optimal neural network. Neural Decomposition transforms the convex re-parameterization
to the overparameterized, i.e., redundant, formulation in a similar spirit to (a non-orthogonal
version of) Eigenvalue Decomposition.

• In addition to the fully connected neural network architecture, we derive the equivalent con-
vex programs for various other architectures such as convolutional, pooling and vector output
architectures.

• We provide extensive numerical simulations showing that the standard backpropagation ap-
proach with or without regularization fails to achieve the global optimum of the training loss.
Moreover, the test accuracy of the proposed convex optimization is considerably higher in stan-
dard datasets as well as random planted models. Our convex optimization solver is signiﬁcantly
faster in total computation time to achieve similar or better test accuracy.

3

Non-convex objective

Poly (scalar)

Poly (vector)

Convolutional

Pooling

Quad (scalar,

cubic reg)

Quad (scalar,

quad reg)

(cid:96)

(cid:96)

(cid:96)

(cid:96)

(cid:96)

(cid:96)

(cid:96)

(cid:96)

(cid:17)

j=1 σ(Xuj)αj , y
j=1 σ(Xuj)αT
j , Y
(cid:80)K

(cid:16)(cid:80)m
(cid:16)(cid:80)m
(cid:16)(cid:80)m
j=1
(cid:16)(cid:80)m
j=1

(cid:16)(cid:80)m
(cid:16)(cid:80)m
(cid:16)(cid:80)m
(cid:16)(cid:80)m

j=1 σ(Xuj)αj, y
j=1 σ(Xuj)αj, y
j=1 σ(Xuj)αj, y
j=1 σ(Xuj)αj, y

(cid:17)

(cid:17)

(cid:17)

(cid:17)

+ β (cid:80)m
(cid:17)
+ β (cid:80)m
(cid:17)

j=1 |αj| s.t. (cid:107)uj(cid:107) = 1
j=1 (cid:107)αj(cid:107)1 s.t. (cid:107)uj(cid:107) = 1
+ β (cid:80)m

j=1 (cid:107)αj(cid:107)1 s.t. (cid:107)uj(cid:107) = 1

Eq (72)

Eq (85)

(cid:17)

+ β (cid:80)m

j=1 (cid:107)αj(cid:107)1 Eq (95)

s.t. (cid:107)uj(cid:107) = 1

k=1 σ(Xkuj)αjk, y
(cid:80)P

(cid:80)K/P
k=1

1
P

l=1 σ(X(k−1)P +luj)αjk, y

Tractable convex Upper bound on
critical width m∗
formulation
2(d + 1)

Eq (21)

Construction
algorithm

Thms

Neural decomp Thm 3.1

2(d + 1)C

2(f + 1)K 2
2(f + 1) K2
P 2

Neural decomp Thm 7.1

Neural decomp Thm 8.1

Neural decomp Thm 9.1

+ β (cid:80)m
(cid:80)m
+ β
c
+ β (cid:80)m
(cid:80)m
+ β
c

j=1 |αj| s.t. (cid:107)uj(cid:107) = 1, or
j=1(|αj|3 + (cid:107)uj(cid:107)3
2)
j=1 |αj|2/3 s.t. (cid:107)uj(cid:107) = 1, or
j=1(|αj|2 + (cid:107)uj(cid:107)2
2)

Eq (45)

NP-hard

(intractable)

d

-

Eigen-

Thm 5.1

decomposition

-

Thm 6.1

Table 1: List of the neural network architectures that we have studied in this work and the corre-
sponding convex programs. Abbreviations are as follows. Poly (scalar): Polynomial activation scalar
output, Poly (vector): Polynomial activation vector output, Convolutional: CNN with polynomial
activation, Pooling: CNN with polynomial activation and average pooling, Quad (scalar, cubic reg):
Quadratic activation scalar output with cubic regularization, Quad (scalar, quad reg): Quadratic
activation scalar output with quadratic regularization. K is the number of patches and f is the ﬁlter
size for the convolutional architecture. C is the output dimension for the vector output case. P is
the pool size for average pooling. σ(u) is deﬁned as u2 for quadratic activation, and au2 + bu + c for
polynomial activation.

1.2 Prior Work

A considerable fraction of recent works on the analysis of optimization landscape of neural networks
focuses on explaining why gradient descent performs well. The works [12, 43] consider the opti-
mization landscape of a restricted class of neural networks with quadratic activation and quadratic
regularization where the second layer weights are ﬁxed. They show that when the neural network is
overparameterized, i.e., m ≥ d, the non-convex loss function has benign properties: all local minima
are global and all saddle points have a direction of negative curvature. However, in this paper we
show that training both the ﬁrst and second layer weights with quadratic regularization in fact makes
global optimization NP-hard.
In contrast, we provide a diﬀerent formulation to obtain the global
optimal solution via convex optimization in the more general case when the second layer weights are
also optimized, the activation function is any arbitrary degree two polynomial, and global optimum
is achieved for all values of m. The work in [31] similarly studies two-layer neural networks with
quadratic activation function and squared loss and states results on both optimization and general-
ization properties. The authors in [19] focus on quadratic activation networks from the perspectives
of optimization landscape and generalization performance, where the setting is based on a planted
model with a full rank weight matrix. In [26, 29] it was shown that suﬃciently wide ReLU networks
have a benign landscape when each layer is suﬃciently wide, satisfying m ≥ n + 1.

Another recent work analyzing the training of neural networks with quadratic-like activations for
deeper architectures is [2]. Authors in [2] consider polynomial activation functions and investigate
layerwise training and compare with end-to-end training of layers. It is demonstrated in [2] that the
degree two polynomial activation function performs comparably to ReLU activation in deep networks.
More speciﬁcally, it is reported in [2] that for deep neural networks, ReLU activation achieves a
classiﬁcation accuracy of 0.96 and a degree two polynomial activation yields an accuracy of 0.95
on the Cifar-10 dataset. Similarly for the Cifar-100 dataset, they obtain an accuracy of 0.81 for
ReLU activation and 0.76 for the degree two activation. These numerical results are obtained for the
activation σ(u) = u+0.1u2, which the authors prefer over the standard quadratic activation σ(u) = u2
to make the neural network training stable. Moreover, the performance of layerwise learning with such
activation functions is considerably high, although there is a gap between end-to-end trained models.
These results verify that degree two polynomial activations are promising and worth studying from

4

both theoretical and practical perspectives.

In a recent series of papers, the authors derived convex formulations for training ReLU neural
networks to global optimality [37, 15, 16, 14, 40, 41]. Our work takes a similar convex duality approach
in deriving the convex equivalents of non-convex neural network training problems. In particular, the
previous work in this area deals with ReLU activations while in this work we focus on polynomial
activations. Hence, the mathematical techniques involved in deriving the convex programs and the
resulting convex programs are substantially diﬀerent. The convex program derived for ReLU activation
in [37] has polynomial time trainability for ﬁxed rank data matrices, whereas the convex programs
developed in this work are all polynomial-time trainable with respect to all problem dimensions. More
speciﬁcally, their convex program is given by

min
{vi,wi}P

i=1

s.t.

1
2

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

P
(cid:88)

DiX(vi − wi) − y

(cid:13)
2
(cid:13)
(cid:13)
(cid:13)
(cid:13)
2
(2Di − In)Xvi ≥ 0, (2Di − In)Xwi ≥ 0, ∀i ∈ [P ] ,

((cid:107)vi(cid:107)2 + (cid:107)wi(cid:107)2)

P
(cid:88)

+ β

i=1

i=1

(4)

where the neural network weights are constructed from vi ∈ Rd and wi ∈ Rd, i = 1, . . . , P . The
matrices Di are diagonal matrices whose diagonal entries consist of 1xT
n u≥0 for
all possible u ∈ Rd. The number of distinct Di matrices, denoted by P is the number of hyperplane
(cid:17)r
arrangements corresponding to the data matrix X. It is known that P is bounded by 2r

2 u≥0, . . . , 1xT

1 u≥0, 1xT

(cid:16) e(n−1)
r

where r = rank(X) (see [37] for the details). In particular, convolutional neural networks have a ﬁxed
value of r, for instance m ﬁlters of size 3 × 3 yield r = 9. This is an exponential improvement over
previously known methods that train optimal ReLU networks which are exponential in the number of
neurons m and/or the number of samples n [3, 20, 5].

The work in [6] presents formulations for convex factorization machines with nuclear norm reg-
ularization, which is known to obtain low rank solutions. Vector output extension for factorization
machines and polynomial networks, which are diﬀerent from polynomial activation networks, is devel-
oped in [7]. Polynomial networks are equivalent to quadratic activation networks with an addition of
a linear neuron. In [7], the authors consider learning an inﬁnitely wide quadratic activation layer by
a greedy algorithm. However, this algorithm does not provide optimal ﬁnite width networks even in
the quadratic activation case. Furthermore, [30] presents a greedy algorithm for training polynomial
networks. The algorithm provided in [30] is based on gradually adding neurons to the neural network
to reduce the loss. More recently, [42] considers applying lifting for quadratic activation neural net-
works and presents non-convex algorithms for low rank matrix estimation for two-layer neural network
training.

1.3 Notation

Throughout the text, σ : R → R denotes the activation function of the hidden layer. We refer to the
function σ(u) = u2 as quadratic activation and σ(u) = au2 + bu + c where a, b, c ∈ R as polynomial
activation. We use X ∈ Rn×d to denote the data matrix, where its rows xi ∈ Rd correspond to data
samples and columns are the features. In the text, whenever we have a function mapping from R to R
with a vector argument (e.g., σ(v) or v2 where v is a vector), this means the elementwise application
of that function to all the components of the vector v. We denote a column vector of ones by ¯1 and its
dimension can be understood from the context. vec(·) denotes the vectorized version of its argument.
In writing optimization problems, we use min and max to refer to ”minimize” and ”maximize”. We
use the notations [m] and 1, . . . , m interchangeably.

We use (cid:96)(ˆy, y) for convex loss functions throughout the text for both scalar and vector outputs.
(cid:96)∗(v) = supz(vT z − (cid:96)(z, y)) denotes the Fenchel conjugate of the function (cid:96)(·, y). Furthermore, we
assume (cid:96)∗∗ = (cid:96) which holds when (cid:96) is a convex and closed function [8].

We use Z (cid:23) 0 for positive semideﬁnite matrices (PSD). S refers to the set of symmetric matrices.
tr refers to matrix trace. ⊗ is used for outer product. The operator conv stands for the convex hull

5

of a set.

1.4 Preliminaries on Semideﬁnite Lifting

We defer the discussion of semideﬁnite lifting for two-layer neural networks with polynomial activations
to Section 2. We now brieﬂy discuss a class of problems where SDP relaxations lead to exact solutions
of the original problem and also instances where they fail to be exact. Let us consider the following
quadratic objective problem with a single quadratic constraint:

uT Q1u + bT

min
u
s.t. uT Q2u + bT

1 u + c1

2 u + c2 ≤ 0

(5)

where Q1, Q2 are indeﬁnite, i.e., not assumed to be positive semideﬁnite. Due to the indeﬁnite
quadratics, this is a non-convex optimization problem. By introducing a matrix variable U = uuT ,
one can equivalently state this problem as

min
U,u

s.t.

tr(Q1U ) + bT

1 u + c1

tr(Q2U ) + bT
U = uuT .

2 u + c2 ≤ 0

(6)

This problem can be relaxed by replacing the equality by the matrix inequality U (cid:23) uuT . Re-writing
the expression U (cid:23) uuT as a linear matrix inequality via the Schur complement formula yields the
following SDP

min
U,u

s.t.

tr(Q1U ) + bT

1 u + c1

2 u + c2 ≤ 0

tr(Q2U ) + bT
(cid:20) U u
uT
1

(cid:21)

(cid:23) 0 .

(7)

Remarkably, it can be shown that the original non-convex problem in (5) can be solved exactly by
solving the convex SDP in (7) via duality, under the mild assumption that the original problem is
strictly feasible (see [8]). This shows that the SDP relaxation is exact in this problem, returning a
globally optimal solution when one exists. We note that there are alternative numerical procedures
to compute the global optimum of quadratic programs with one quadratic constraint [8].

We also note that the lifting approach U = uuT and the subsequent relaxation U (cid:23) uuT for
quadratic programs with more than two quadratic constraints is not tight in general [33, 9]. A
notable case with multiple constraints is the NP-hard Max-Cut problem and its SDP relaxation [21]

max
u2
i =1,∀i

uT Qu = max
i =1,∀i

u2

tr(QuuT ) ≤

max
U (cid:23)0, Uii=1,∀i

tr(QU ).

(8)

The SDP relaxation of Max-Cut is not tight since its feasible set contains the cut polytope
conv (cid:8)uuT : ui ∈ {−1, +1} ∀i(cid:9)

and other non-integral extreme points [27]. Nevertheless, an approximation ratio of 0.878 can be
obtained via the Goemans-Williamson randomized rounding procedure [21]. It is conjectured that
this is the best approximation ratio for Max-Cut [24], whereas it can be formally proven to be NP-
hard to approximate within a factor of 16
17 [22, 44]. Hence, in general we cannot expect to obtain exact
solutions to problems of combinatorial nature, such as Max-Cut and variants using SDP relaxations.
It is instructive to note that a naive application of the SDP lifting strategy is not immediately
tractable for two-layer neural networks. For simplicity, consider a scalar output polynomial activation

6

network f (x) = (cid:80)m
j=1 are trainable parameters. The
corresponding training problem for a given loss function (cid:96)(·, y) and its SDP relaxation are as follows

j=1 σ(xT uj)αj where σ(u) = u2 + u, and {uj, αj}m

min
{uj ,αj }m

j=1

(cid:88)

(cid:96)(cid:0)

m
(cid:88)

((xT uj)2 + xT uj)αj, y(cid:1) ≥

x∈X

j=1

min
{Uj (cid:23)uj uT

j ,αj }m

j=1

m
(cid:88)

(cid:88)

(cid:96)(cid:0)

x∈X

j=1

xT Ujxαj + xT ujαj, y(cid:1). (9)

The above problem is non-convex due to the bilinear terms {Ujαj}m
j=1. Moreover, a variable change
ˆUj = Ujαj does not respect semideﬁnite constraints Uj (cid:23) ujuT
j when αj ∈ R. Another limitation
is the prohibitively high number of variables in the lifted space, which is d2m + dm + m as opposed
to dm + m in the original problem. Therefore, a diﬀerent convex analytic formulation is required to
address all these concerns.

Although SDP relaxations are extensively studied for various non-convex problems (see e.g. [45]
for a survey of applications), instances with exact SDP relaxations are exceptionally rare. As will be
discussed in the sequel, our main result for two-layer neural networks is another instance of an SDP
relaxation leading to exact formulations where the semideﬁnite lifting and relaxation is tight.

In convex geometry, a spectrahedron is a convex body that can be represented as a linear matrix
inequality which are the feasible sets of semideﬁnite programs. An example is the elliptope deﬁned
as the feasible set of the Max-Cut relaxation given by U (cid:23) 0, Uii = 1 ∀i, which is a subset of n × n
symmetric positive-deﬁnite matrices. Due to the existence of eﬃcient projection operators and barrier
functions of linear matrix inequalities, optimizing convex objectives over spectrahedra can be eﬃciently
implemented, which renders SDPs tractable. We will show that polynomial activation neural networks
can be represented via a class of simple linear matrix inequalities, dubbed neural spectrahedra (see
Figure 2 for an example), and enables global optimization in fully polynomial time and elucidates
their parameterization in convex analytic terms.

1.5 Paper Organization

Section 2 gives an overview of the theory developed in this work. Section 3 describes the convex opti-
mization formulation via duality and S-procedure for polynomial activation neural networks. Section
4 establishes via the neural decomposition method that the convex problem developed in Section 3 can
be used to train two-layer polynomial activation networks to global optimality. Quadratic activation
neural networks and the hardness result are studied in Section 5 and 6. Vector output and convo-
lutional neural network architectures are studied in Section 7 and 8, respectively and convolutional
networks with average pooling is in Section 9. We discuss the implementation details for solving the
convex programs and give experimental results in Section 10.

2 Lifted Representations of Networks with Polynomial Acti-

vations

Consider the network f (x) = (cid:80)m
polynomial σ(u) = au2 + bu + c. First, we note that the neural network output can be written as

j=1 σ(xT uj)αj where the activation function σ is the degree two

f (x) =

m
(cid:88)

j=1

(cid:0)a(xT uj)2 + bxT uj + c(cid:1) αj =

m
(cid:88)

(cid:0)(cid:104)axxT , ujuT

j (cid:105) + (cid:104)bx, uj(cid:105) + c(cid:1) αj

j=1
(cid:42)






=

axxT
bx
c
= (cid:104)φ(x), ψ({uj, αj}m

 ,



(cid:80)m

j=1 ujuT
j αj
(cid:80)m
j=1 ujαj
(cid:80)m
j=1 αj

j=1)(cid:105) ,



(cid:43)



(10)

7

Figure 2: (Left) The Neural Cone C1

2 described by (u2α, uα, α) ∈ R3 where u, α ∈ R, |u| ≤ 1. (Right)

Neural Spectrahedron M(1) described by (Z11, Z12, Z22) ∈ R3 where Z =



0, Z11 + Z22 = Z33 ≤ 1 (constrained to the slice Z22 = Z11 and Z (cid:48) = 0 in (14)).



Z11 Z12 Z13
Z12 Z22 Z23
Z13 Z23 Z33



 (cid:23)

where φ : Rd → Rd2+d+1 and ψ : Rm(d+1) → Rd2+d+1 are formally deﬁned in the sequel. The above
identity shows that the nonlinear neural network output is linear over the lifted features

φ(x) := (cid:0)axxT , bx, c(cid:1) ∈ Rd2+d+1.

In turn, the nonlinear model f (x) is completely characterized by the lifted parameters which we deﬁne
as the following matrix-vector-scalar triplet

ψ({uj, αj}m

j=1) :=

(cid:16) m
(cid:88)

j=1

ujuT

j αj,

m
(cid:88)

j=1

ujαj,

(cid:17)

αj

m
(cid:88)

j=1

∈ Rd2+d+1.

Optimizing over the lifted parameter space initially appears as hard as the original non-convex neural
network training problem. This is due to the cubic and quadratic terms involving the weights of the
hidden and output layer in the lifted parameters. Nevertheless, one of our main results shows that
the lifted parameters can be exactly described using linear matrix inequalities.

We begin by characterizing the lifted parameter space as a non-convex cone.

Deﬁnition 1 (Neural Cone of degree two). We deﬁne the non-convex cone Cm

Cm
2 :=






(cid:16) m
(cid:88)

j=1

ujuT

j αj,

m
(cid:88)

j=1

ujαj,

(cid:17)

αj

m
(cid:88)

j=1

: uj ∈ Rd, (cid:107)uj(cid:107)2 = 1, αj ∈ R ∀j ∈ [m]

.

(11)

2 ⊆ Rd2+d+1 as





See Figure 2 (left) for a depiction of C1

2 ⊆ R3 corresponding to the case m = 1, d = 1.

Surprisingly, we will show that the original non-convex neural network problem is solved exactly to
global optimality when the optimization is performed over a convex set which we deﬁne as the Neural
Spectrahedron, given by the convex hull of the cone C2. In other words, every element of the convex
hull can be associated with a neural network of the form f (x) = (cid:80)m
j=1 σ(xT uj)αj through a special
matrix decomposition procedure which we introduce in Section 4. Moreover, a Neural Spectrahedron
can be described by a simple linear matrix inequality. Consequently, these two results enable global
optimization of neural networks with polynomial activations of degree two in fully polynomial time
with respect to all problem parameters: dimension d, number of samples n and number of neurons m.
To the best of our knowledge, this is the ﬁrst instance of a method that globally optimizes a standard

8

neural network architecture with computational complexity polynomial in all problem dimensions.
We refer the reader to the recent work [37] for a convex optimization formulation of networks with
ReLU activation, where the worst case computational complexity is O(( n

r )r) with r = rank(X).

It is equally important that our results characterize neural networks as constrained linear learning
methods (cid:104)φ(x), ψ(cid:105) in the lifted feature space φ(x), where the constraints on the lifted parameters ψ are
precisely described by a Neural Spectrahedron via linear matrix inequalities. These constraints can
be easily tackled with convex semideﬁnite programming or closed-form projections onto these sets in
iterative ﬁrst-order algorithms. We also investigate interesting regularization properties of this convex
set, and draw similarities to (cid:96)1 norm and nuclear norm. In contrast, Reproducing Kernel Hilbert Space
methods and Neural Tangent Kernel approximations [23, 10] are linear learning methods over lifted
feature maps where the corresponding parameter constraints are ellipsoids. These approximations
fall short of explaining the extraordinary power of ﬁnite width neural networks employed in practical
applications.

We extend the deﬁnition of the Neural Cone to degree k activations as follows.

Deﬁnition 2 (Neural Cone of degree k). We deﬁne the non-convex cone Cm

k ⊆ R(cid:80)k

i=0 di

as follows

Cm
k :=






(cid:16) m
(cid:88)

j=1

u⊗k
j αj, · · · ,

m
(cid:88)

j=1

uj ⊗ ujαj,

m
(cid:88)

j=1

ujαj,

(cid:17)

αj

m
(cid:88)

j=1

where we use the notation u⊗k := u ⊗ · · · ⊗ u
.
(cid:125)

(cid:124)

(cid:123)(cid:122)
k times

: uj ∈ Rd, (cid:107)uj(cid:107)2 = 1, αj ∈ R ∀j ∈ [m]





(12)

It is easy to see that two-layer neural networks with degree k polynomial activations can be
represented linearly using the lifted parameter space Ck and corresponding lifted features. Taking the
closure of the union {C}∞
k=0, any analytic activation function can be represented in this fashion. In
this paper we limit the analysis to the degree 2 case.

Next, we describe a compact set that we call neural spectrahedron which describes the lifted

parameter space of networks with a constraint on the (cid:96)1 norm of output layer weights.

Deﬁnition 3. A neural spectrahedron S m

2 (t) ⊆ Rd2+d+1 is deﬁned as the compact convex set

S m

2 (t) := conv




(cid:16) m
(cid:88)



j=1

ujuT

j αj,

m
(cid:88)

j=1

ujαj,

(cid:17)

αj

m
(cid:88)

j=1

: (cid:107)uj(cid:107)2 = 1, αj ∈ R, ∀j = 1, . . . , m,

m
(cid:88)

j=1

|αj| ≤ t





(13)

We will show that a neural spectrahedron can be equivalently described as a linear matrix inequality

via deﬁning Sm

2 (t) = (cid:0)M11(t), M12(t), M22(t)(cid:1) for all m ≥ m∗ where

(cid:26)

M(t) =

Z − Z (cid:48) : Z =

(cid:21)

(cid:20) Z1 Z2
Z T
2 Z4

(cid:23) 0, Z (cid:48) =

(cid:21)

(cid:20) Z (cid:48)
1 Z (cid:48)
2
T Z (cid:48)
Z (cid:48)
4
2

(cid:23) 0, tr(Z1) = Z4, tr(Z (cid:48)

1) = Z (cid:48)

4, Z4 + Z (cid:48)

4 ≤ t

(cid:27)

,

1 ∈ Sd×d, Z2, Z (cid:48)

2 ∈ Rd×1 and Z4, Z (cid:48)

4 ∈ R+, and m∗ = m∗(t) is a critical
Z, Z (cid:48) ∈ S(d+1)×(d+1), Z1, Z (cid:48)
number of neurons that satisﬁes m∗(0) = 0 and m∗(t) ≤ 2(d + 1) ∀t, which will be explicitly deﬁned
in the sequel. Therefore, an eﬃcient description of the set M(t) in terms of linear matrix inequalities
enables eﬃcient convex optimization methods in polynomial time. Moreover, it should be noted that
in non-convex optimization, the choice of the optimization algorithm and its internal hyperparameters,
such as initialization, mini-batching and step sizes have a substantial contribution to the quality of
the learned neural network model. This is in stark contrast to convex optimization problems, where
optimizer hyperparameters have no eﬀect, and solutions can be obtained in a very robust, eﬃcient
and reproducible manner.

(14)

9

2.1 A geometric description of the Neural Spectrahedron for the special

case of nonnegative output layer weights

Here we describe a simpler case with the restriction αj ≥ 0 ∀j ∈ [m] in the Neural Cone Cm
2 and we will
suppose that m ≥ d + 1. In this special case, let us deﬁne the one-sided positive Neural Spectrahedron
as

+S m

2 (t) := conv






(cid:16) m
(cid:88)

j=1

ujuT

j αj,

m
(cid:88)

j=1

ujαj,

(cid:17)

αj

m
(cid:88)

j=1

: (cid:107)uj(cid:107)2 = 1, αj ∈ R+, ∀j = 1, . . . , m,

m
(cid:88)

j=1

We observe that +S m

2 (t) is identical to the set (cid:0) +M11, +M12, +M22

(cid:1) ⊆ Rd2+d+1 where

αj ≤ t

.





(15)

+M(t) : = t conv




m
(cid:88)



j=1

(cid:20) uj
1

(cid:21) (cid:20) uj
1

(cid:21)T

αj : uj ∈ Rd, (cid:107)uj(cid:107)2 = 1, αj ∈ R+,

m
(cid:88)

j=1

αj ≤ 1






,

(16)

which is partitioned as +M(t) =

+M22 ⊆ R+.

(cid:20) +M11
+MT
12

(cid:21)

+M12
+M22

where +M11 ⊆ Sd×d, +M12 ⊆ Rd×1 and

Next, we note that as soon as the network width1 satisﬁes m ≥ d + 1, we have

+M(t) : = t conv

(cid:40)(cid:40)(cid:20) u
1

(cid:21) (cid:20) u
1

(cid:21)T

(cid:41)

(cid:41)

: (cid:107)u(cid:107)2 = 1

∪ 0

,

(17)

where 0 is the zero matrix, since (cid:80)m

(cid:21) (cid:20) uj
1
matrix, and hence can be factorized2 as a convex combination of at most d+1 rank-one matrices of the

αj ∈ S(d+1)×(d+1) is a positive semideﬁnite

(cid:20) uj
1

(cid:21)T

j=1

(cid:20) u
1

(cid:21) (cid:20) u
1

(cid:21)T

form

. Note that the zero matrix is included to account for the inequality (cid:80)m

j=1 αj ≤ 1
in (16). This important observation enables us to represent the convex hull of the non-convex Neural
Cone (an example is shown in Figure 2), via the simple convex body +M(t) given in (17).

Most importantly, the positive Neural Spectrahedron set +M(t) provides a representation of the
non-convex Neural Cone Cm
2 via its extreme points. Furthermore, +M(t) has a simple description as
a linear matrix inequality provided in the following lemma (the proof can be found in the appendix).

Lemma 2.1. For m ≥ d + 1, it holds that

+M(t) =

(cid:26)

Z : Z =

(cid:21)

(cid:20) Z1 Z2
Z T
2 Z4

(cid:23) 0, tr(Z1) = Z4 ≤ t

.

(18)

(cid:27)

Therefore the positive Neural Spectrahedron can be represented as the intersection of the positive
semideﬁnite cone and linear inequalities. Moreover, every element of +M(t) can be factorized as
(cid:80)m

for some (cid:107)uj(cid:107)2 = 1, αj ≥ 0, ∀j ∈ [m], (cid:80)m

(cid:105)

(cid:104) ujuT
j αj ujαj
uT
αj
j αj

j=1

j=1 αj ≤ t, which can be identiﬁed
2 and a neural network in the lifted parameter space

as an element of the non-convex Neural Cone Cm
as shown in (10).

The assumption m ≥ d + 1 is not required and only used here to illustrate this simpler special
case. In the more general case of arbitrary output layer weights αj ∈ R, ∀j ∈ [m], we have the more

1This assumption is not required in our later analysis.
2We describe the details of this factorization in Section 4.

10

general linear matrix inequality representation in (14), which is in terms of two positive semideﬁnite
cones and three linear inequalities. In general, such a restriction on the number of neurons m in terms
of the dimension d is not necessary. In the next sections, we only require m ≥ m∗, where m∗ can be
determined via a convex program. Furthermore, the regularization parameter directly controls the
number of neurons m∗. We illustrate the eﬀect of the regularization parameter on m∗ in the numerical
experiments section, and show that m∗ can be made arbitrarily small.

3 Convex Duality for Polynomial Activation Networks

We consider the non-convex training of a two-layer fully connected neural network with polynomial
activation and derive a convex dual optimization problem. The input-output relation for this archi-
tecture is

f (x) =

m
(cid:88)

j=1

σ(xT uj)αj ,

(19)

where σ is the degree two polynomial σ(u) = au2 +bu+c. This neural network has m neurons with the
ﬁrst layer weights uj ∈ Rd and second layer weights αj ∈ R. We refer to this case where f : Rd → R
as the scalar output case. Section 7 extends the results to the vector output case.

It is relatively easy to obtain a weak dual that provides a lower-bound via Lagrangian duality.
However, in non-convex problems, a duality gap may exist since strong duality does not hold in
general. Remarkably, we show that strong duality holds as soon as the network width exceeds a
critical threshold which can be easily determined.

We will assume (cid:96)1 norm regularization on the second layer weights as regularization and include
constraints that the ﬁrst layer weights are unit norm. We note that (cid:96)1 norm regularization on the
second layer weights results in a special dual problem and hence is crucial in the derivations. We show
in Section 5 that this formulation is equivalent to cubic regularization when the activation is quadratic.
For the standard (cid:96)2
2, i.e., weight decay regularization, we will in fact show that the problem is NP-
hard (see Section 6). The training of a network under this setting requires solving the non-convex
optimization problem given by

p∗ =

min

{αj , uj }m

j=1, s.t. (cid:107)uj (cid:107)2=1, ∀j

(cid:96)





m
(cid:88)

j=1



σ(Xuj)αj , y

 + β

m
(cid:88)

j=1

|αj| .

(20)

Theorem 3.1 states the main result for polynomial activation neural networks that the non-convex
optimization problem in (20) can be solved globally optimally via a convex problem. Before we state
Theorem 3.1, we brieﬂy describe the numerical examples shown in Figure 3 and 4 which compare
the solution of the non-convex problem via backpropagation and the solution of the corresponding
convex program via a convex solver (see Section 10 for details on the solver). Figure 3 shows the
training and test costs on a regression task with randomly generated data for the two-layer quadratic
activation neural network. We observe that convex SDP takes a much shorter time to optimize and
obtains a globally optimal solution while the SGD algorithm converges to local minima in some of the
trials where the initialization is diﬀerent. Furthermore, Figure 4 compares the classiﬁcation accuracies
for the two-layer vector output polynomial activation network on a multiclass classiﬁcation problem
with real data. The exact statement of the vector output extension of the main result is provided in
Section 7. In Section 10, we present additional numerical results verifying all of the theoretical results
on various datasets.

Figure 5 compares the accuracy of the non-convex polynomial activation model when it is trained
with diﬀerent optimizers (SGD and Adam) for a range of step sizes. Figure 5 shows that the convex
formulations outperform the non-convex solution via SGD and Adam. The extension of the main
result to convolutional neural networks is discussed in Section 8 and 9.

11

Figure 3: Cost against wall-clock time on the training (left) and test (right) sets for stochastic gradient
descent (SGD) and the convex SDP for quadratic activation networks. The solid lines show the training
curve of the non-convex model with SGD (with learning rate tuned oﬄine) and each line corresponds
to an independent trial. The dotted horizontal line shows the cost for the convex SDP and the cross
indicates the time that it takes to solve the convex SDP. The dataset X is synthetically generated
by sampling from the i.i.d. Gaussian distribution and has dimensions n = 100, d = 10. Labels y are
generated by a teacher network with 10 planted neurons. The regularization coeﬃcient is β = 10−6
and the batch size for SGD is 10.

Figure 4: Classiﬁcation accuracy results on the UCI dataset ”annealing” (n = 638, d = 31) for
polynomial activation networks. This is a multiclass classiﬁcation dataset with C = 5 classes. Both
training (left) and test (right) set accuracies are shown for the gradient descent (GD) and the convex
SDP methods. Legend labels are as follows. GD - tractable: The non-convex problem in (89) solved
via gradient descent, GD - weight decay: Non-convex problem with quadratic regularization on all
weights solved via gradient descent, Convex SDP (optimal): The convex problem in (72). Degree
two polynomial activation with coeﬃcients a = 0.09, b = 0.5, c = 0.47 is used. The regularization
coeﬃcient is β = 1. The learning rate for GD is optimized oﬄine and only the best performing
learning rate is shown. The resulting number of neurons from the convex program is 172.

12

0204060Time (sec)100101102103104Training costBackpropagation (SGD)Convex SDP (optimal)0204060Time (sec)100101102103104Test costBackpropagation (SGD)Convex SDP (optimal)0204060Time (sec)0.000.250.500.75Training accuracyGD - tractableGD - weight decayConvex SDP (optimal)0204060Time (sec)0.000.250.500.75Test accuracyGD - tractableGD - weight decayConvex SDP (optimal)(a) CNN, MNIST, training accuracy

(b) CNN, MNIST, test accuracy

(c) CNN, CIFAR, training accuracy

(d) CNN, CIFAR, test accuracy

(e) Fully connected, oocytes, training accuracy

(f) Fully connected, oocytes, test accuracy

Figure 5: Classiﬁcation accuracy for various learning rates and optimizers are plotted on the same
ﬁgure. SGD and Adam are used in solving the non-convex optimization problem. The solid blue
lines each correspond to a diﬀerent learning rate for SGD and each dashed green line corresponds
to a diﬀerent learning rate for the Adam algorithm. Plots a, b: CNN with degree two polynomial
activations and global average pooling for binary classiﬁcation on the ﬁrst two classes of the MNIST
dataset. Plots c, d: The same architecture as plots a, b and the dataset is the ﬁrst two classes of the
CIFAR-10 dataset. Plots e, f: Fully connected architecture for binary classiﬁcation on the dataset
oocytes-merluccius-nucleus-4d.

13

0100200300400Time (sec)0.50.60.70.80.91.0Training accuracySGDAdamConvex SDP (optimal)0100200300400Time (sec)0.50.60.70.80.91.0Test accuracySGDAdamConvex SDP (optimal)050010001500Time (sec)0.50.60.70.8Training accuracySGDAdamConvex SDP (optimal)050010001500Time (sec)0.50.60.70.8Test accuracySGDAdamConvex SDP (optimal)051015Time (sec)0.40.60.8Training accuracySGDAdamConvex SDP (optimal)051015Time (sec)0.30.40.50.60.70.8Test accuracySGDAdamConvex SDP (optimal)Theorem 3.1 (Globally optimal convex program for polynomial activation networks). The solution
of the convex problem

min
Z=ZT ,Z(cid:48)=Z(cid:48) T

(cid:96)(ˆy, y) + β(Z4 + Z (cid:48)
4)

s.t.

ˆyi = axT
i (Z1 − Z (cid:48)
tr(Z1) = Z4, tr(Z (cid:48)
Z (cid:23) 0, Z (cid:48) (cid:23) 0

1)xi + bxT
1) = Z (cid:48)
4

i (Z2 − Z (cid:48)

2) + c(Z4 − Z (cid:48)

4),

i ∈ [n]

(21)

provides a global optimal solution for the non-convex problem in (20) when the number of neurons
satisﬁes m ≥ m∗ where

m∗ = rank(Z ∗) + rank(Z (cid:48)∗).

(22)

Here Z ∗ and Z (cid:48)∗ denote the solution of (21). The variables Z ∈ S(d+1)×(d+1) and Z (cid:48) ∈ S(d+1)×(d+1)
are deﬁned in (29). It follows that the optimal number of neurons is upper bounded by m∗ ≤ 2(d + 1).

The proof of Theorem 3.1 is established in this section and the next. In this section we show that
the solution of the convex program (21) provides a lower bound for the solution of the non-convex
problem (20). In the next section, we prove, via the method of neural decomposition, that the solution
of the convex problem provides also an upper bound, which concludes the proof of Theorem 3.1.

In proving the lower bound, we leverage duality. Minimizing over ﬁrst αj’s and then uj’s, we can

restate the problem in (20) as

p∗ =

min

{uj }m

j=1 s.t. (cid:107)uj (cid:107)2=1, ∀j

min
{αj }m

j=1,ˆy

(cid:96) (ˆy, y) + β

m
(cid:88)

j=1

|αj|

s.t.

ˆy =

m
(cid:88)

j=1

σ(Xuj)αj .

(23)

The dual problem for the inner minimization problem is given by

max
v

−(cid:96)∗(−v)

s.t.

|vT σ(Xuj)| ≤ β, ∀j .

Next, let us call the optimal solution of the following problem d∗

d∗ =

min

{uj }m

j=1 s.t. (cid:107)uj (cid:107)2=1, ∀j

max
|vT σ(Xuj )|≤β ,∀j

−(cid:96)∗(−v).

(24)

(25)

By changing the order of the minimization and maximization operations, we obtain the following
bound

d∗ ≥

max
|vT σ(Xuj )|≤β ,(cid:107)uj (cid:107)2=1, ∀j

−(cid:96)∗(−v).

(26)

We note that the constraints |vT σ(Xuj)| ≤ β can equivalently be written as two quadratic (in uj)
inequalities for each j = 1, . . . , m,

(cid:32)

uT
j

a

n
(cid:88)

i=1

(cid:33)

xixT

i vi

uj + bvT Xuj + cvT ¯1 ≤ β, −uT
j

(cid:32)

a

n
(cid:88)

i=1

(cid:33)

xixT

i vi

uj − bvT Xuj − cvT ¯1 ≤ β. (27)

Next, we use the S-procedure given in Corollary 3.3 to reformulate the quadratic inequality constraints
as linear matrix inequality constraints. Corollary 3.3 is based on Lemma 3.2 which characterizes the
solvability of a quadratic system. The proof of Corollary 3.3 is given in the appendix.

Lemma 3.2 (Proposition 3.1 from [38]). Let f1 and f2 be quadratic functions where f2 is strictly
concave (or strictly convex) and assume that f2 takes both positive and negative values. Then, the
following two statements are equivalent:

14

1. f1(u) < 0, f2(u) = 0 is not solvable.
2. There exists λ ∈ R such that f1(u) + λf2(u) ≥ 0, ∀u.

Corollary 3.3 (S-procedure with equality). max(cid:107)u(cid:107)2=1 uT Qu + bT u ≤ β if and only if there exists
λ ∈ R such that

(cid:20)λI − Q − 1
2 b
2 bT
β − λ

− 1

(cid:21)

(cid:23) 0.

Corollary 3.3 allows us to write the maximization problem in (26) as the equivalent problem given

by

max − (cid:96)∗(−v)

(cid:20)ρ1I − a (cid:80)n

i=1 xixT

i vi

s.t.

− 1

2 bvT X

(cid:20)ρ2I + a (cid:80)n

i=1 xixT

i vi

1

2 bvT X

− 1

2 bX T v
β − c ¯1T v − ρ1
2 bX T v
β + c ¯1T v − ρ2

1

(cid:21)

(cid:21)

(cid:23) 0

(cid:23) 0 ,

(28)

where we note the two additional variables ρ1, ρ2 ∈ R are introduced. Next, we will ﬁnd the dual of
the problem in (28). Let us ﬁrst deﬁne the following Lagrange multipliers

Z =

(cid:21)

(cid:20)Z1 Z2
Z3 Z4

, Z (cid:48) =

(cid:21)

(cid:20)Z (cid:48)
Z (cid:48)

1 Z (cid:48)
2
3 Z (cid:48)
4

,

(29)

where Z, Z (cid:48) ∈ S(d+1)×(d+1) are symmetric matrices, and the dimensions for each block matrix are
4 ∈ R1×1. We note that because of the symmetry
1 ∈ Sd×d, Z2, Z (cid:48)
Z1, Z (cid:48)
of Z and Z (cid:48), we have Z T

3. The Lagrangian for the problem in (28) is

3 ∈ R1×d, Z4, Z (cid:48)

2 ∈ Rd×1, Z3, Z (cid:48)

2 = Z3 and Z2

(cid:48)T = Z (cid:48)

L(v, ρ1, ρ2, Z, Z (cid:48)) = −(cid:96)∗(−v) + ρ1 tr(Z1) + ρ2 tr(Z (cid:48)

1) − a

n
(cid:88)

i=1

vixT

i (Z1 − Z (cid:48)

1)xi − bvT X(Z2 − Z (cid:48)

2)+

+ (β − ρ1)Z4 + (β − ρ2)Z (cid:48)

4 − c

n
(cid:88)

i=1

vi(Z4 − Z (cid:48)

4).

(30)

Maximizing the Lagrangian with respect to v, ρ1, ρ2, we obtain the problem in (21), which concludes
the lower bound part of the proof. In the next section, we introduce a method for decomposing the
solution of this convex program (i.e. Z ∗ and Z (cid:48)∗) into feasible neural network weights to prove the
upper bound.

4 Neural Decomposition

We have shown that a lower bound on the optimal value of the non-convex problem in (20) is obtained
via the solution of the convex program in (21) that we have derived using Lagrangian duality. Now
we show that this lower bound is in fact identical to the optimal value of the non-convex problem,
thus proving strong duality. Our approach is based on proving an upper bound by constructing neural
network weights from the solution of the convex problem such that the convex objective achieves the
same objective as the non-convex objective. Suppose that (Z ∗, Z (cid:48)∗) is a solution to (21). Let us
denote the rank of Z ∗ by r and the rank of Z (cid:48)∗ by r(cid:48). We will discuss the decomposition for Z ∗ and
then complete the picture by considering the same decomposition for Z (cid:48)∗. We begin by noting that
Z ∗ satisﬁes the constraints of (21), i.e.,

Z ∗ (cid:23) 0 and tr(Z ∗

1 ) = Z ∗

4 , or equivalently tr

(cid:16)

Z ∗

(cid:20)Id
0
0 −1
(cid:123)(cid:122)
G

(cid:124)

(cid:21)

(cid:17)

(cid:125)

= 0.

(31)

15

j G) = pT

Suppose that we have a decomposition of Z ∗ as a sum of rank-1 matrices such that Z ∗ = (cid:80)r
j=1 pjpT
j
where pj ∈ Rd+1 and tr(pjpT
j Gpj = 0 for j = 1, . . . , r. We show how this can always
be done in subsection 4.1 by introducing a new matrix decomposition method, dubbed the neural
decomposition procedure.
Letting pj := (cid:2)cT
dj

with cj ∈ Rd and dj ∈ R, we note that pT
2 = d2
j .
We may assume pj (cid:54)= 0, ∀j in the decomposition (otherwise we can simply remove zero components),
implying (cid:107)cj(cid:107)2
2 > 0, ∀j. Furthermore, this expression for pj’s allows us to establish that

j Gpj = 0 implies (cid:107)cj(cid:107)2

(cid:3)T

j

r
(cid:88)

j=1

pjpT

j =

r
(cid:88)

j=1

(cid:21)

(cid:20)cj
dj

(cid:2)cT
j

(cid:3) =

dj

r
(cid:88)

j=1

(cid:20)cjcT
j
djcT
j

(cid:21)

cjdj
d2
j

=

(cid:21)

(cid:20)Z ∗
Z ∗

1 Z ∗
2
3 Z ∗
4

.

As a result, we have the following decompositions:

ujuT

j (cid:107)cj(cid:107)2

2 =

r
(cid:88)

ujuT

j d2
j

ujdj(cid:107)cj(cid:107)2 =

j=1
r
(cid:88)

ujdj|dj|

j=1

Z ∗

1 =

Z ∗

2 =

Z ∗

4 =

r
(cid:88)

j=1
r
(cid:88)

j=1
r
(cid:88)

j=1

cjcT

j =

cjdj =

r
(cid:88)

j=1
r
(cid:88)

j=1

d2
j ,

(32)

(33)

(34)

(35)

where we have introduced the normalized weights uj = cj
(cid:107)cj (cid:107)2
we redeﬁne the corresponding pj as pj ← −pj, which does not modify the decomposition (cid:80)
and the equality pT
j = 1, . . . , r, which leads to

, j = 1, . . . , r. If dj ≤ 0 for some j,
j pjpT
j
j Gpj = 0. Hence, without loss of generality, we can assume that dj ≥ 0 for all

Z ∗

1 =

r
(cid:88)

j=1

ujuT

j d2

j , Z ∗

2 =

r
(cid:88)

j=1

ujd2

j , Z ∗

4 =

r
(cid:88)

j=1

d2
j .

Similarly for Z (cid:48)∗, we will form the following decompositions:

Z (cid:48)
1

∗ =

r(cid:48)
(cid:88)

j=1

u(cid:48)
ju(cid:48)
j

T d(cid:48)
j

2, Z (cid:48)
2

∗ =

r(cid:48)
(cid:88)

j=1

u(cid:48)
jd(cid:48)
j

2, Z (cid:48)
4

∗ =

r(cid:48)
(cid:88)

j=1

2.

d(cid:48)
j

(36)

(37)

Considering the decompositions for both Z ∗ and Z (cid:48)∗, ﬁnally we obtain a neural network with ﬁrst
2}.
layer weights as {u1, . . . , ur, u(cid:48)
We note that this corresponds to a neural network with r + r(cid:48) neurons. If both Z ∗ and Z (cid:48)∗ are full
rank, then we will have 2(d + 1) neurons, which is the maximum.

r(cid:48)}, and second layer weights as {d2

2, . . . , −d(cid:48)
r(cid:48)

1, . . . , d2

1, . . . , u(cid:48)

r, −d(cid:48)
1

To see why we can use the decompositions of Z ∗ and Z (cid:48)∗ to construct neural network weights, we

plug-in the expressions (36) and (37) in the objective of the convex program in (21):

(cid:96)(ˆy, y) + β

(cid:18) r

(cid:88)

j=1

|d2

j | +

(cid:19)

2|

| − d(cid:48)
j

r(cid:48)
(cid:88)

j=1

, where

ˆyi = axT
i

(cid:18) r

(cid:88)

j=1

ujuT

j d2

j +

ju(cid:48)
u(cid:48)
j

T (−d(cid:48)
j

(cid:19)

2)

xi+

r(cid:48)
(cid:88)

j=1

+bxT
i

(cid:18) r

(cid:88)

j=1

ujd2

j +

j(−d(cid:48)
u(cid:48)
j

(cid:19)

2)

r(cid:48)
(cid:88)

j=1

(cid:18) r

(cid:88)

+ c

r(cid:48)
(cid:88)

(−d(cid:48)
j

(cid:19)

2)

d2
j +

,

i = 1, . . . , n .

(38)

j=1

j=1

We note that this expression exactly matches the optimal value of the non-convex objective in (20)
for a neural network with r + r(cid:48) neurons. Also, the unit norm constraints on the ﬁrst layer weights are

16

satisﬁed (hence feasible) since uj’s and u(cid:48)
j’s are normalized. This establishes that the neural network
weights obtained from the solution of the convex program provide an upper bound for the minimum
value of the original non-convex problem. Consequently, we have shown that the optimal solution of
the convex problem (21) provides a global optimal solution to the non-convex problem (20) and this
concludes the proof of Theorem 3.1.

4.1 Neural Decomposition Procedure
Here we describe the procedure for computing the decomposition Z ∗ = (cid:80)r
j (cid:23) 0 such that
pT
j Gpj = 0, j = 1, . . . , r. This algorithm is inspired by the constructive proof of the S-procedure given
in Lemma 2.4 of [38] with modiﬁcations to account for the equalities pT

j=1 pjpT

j Gpj = 0.

Neural Decomposition for Symmetric Matrices:

0. Compute a rank-1 decomposition Z ∗ = (cid:80)r

j=1 pjpT
j .

This can be done with the eigenvalue decomposition Z ∗ = (cid:80)r
j λj. Since Z ∗ (cid:23) 0,
we have λj > 0, for j = 1, . . . , r. Then we can obtain the desired rank-1 decomposition
Z ∗ = (cid:80)r

j by deﬁning pj = (cid:112)λjqj, j = 1, . . . , r.

j=1 qjqT

j=1 pjpT

1 Gp1 = 0, return y = p1. If not, ﬁnd a j ∈ {2, . . . , r} such that (pT

1 Gp1)(pT

j Gpj) <

1. If pT
0.
We know such j exists since tr(Z ∗G) = (cid:80)r
constraints of the convex program), and pT
j Gpj must have the opposite sign as pT
pT

1 Gp1.

j=1 pT
j Gpj = 0 (this is true since it is one of the
1 Gp1 (cid:54)= 0. Hence, for at least one j ∈ {2, . . . , r},

2. Return y = p1+αpj

√

1+α2 where α ∈ R satisﬁes (p1 + αpj)T G(p1 + αpj) = 0.

We know that such α exists since the quadratic equation

(p1 + αpj)T G(p1 + αpj) = α2pT

j Gpj + 2αpT

1 pj + pT

1 Gp1 = 0

(39)

has real solutions since the discriminant 4(pT
step 1 where we picked j such that (pT
quadratic equation for α.

1 Gp1)(pT

1 pj)2 − 4(pT

j Gpj) is positive due to
j Gpj) < 0. To ﬁnd α, we simply solve the

1 Gp1)(pT

3. Update r ← r − 1, and then the vectors p1, . . . , pr as follows:

Remove p1 and pj and insert u = pj −αp1
√
updated matrix Z ∗ ← Z ∗ − yyT in the next iteration, which is of rank r − 1:

1+α2 . Consequently, we will be dealing with the

Z ∗ − yyT = uuT +

r
(cid:88)

i=2,i(cid:54)=j

pipT
i .

(40)

Note that Step 0 is carried out only once and then steps 1 through 3 are repeated r − 1 times. At the
end of r − 1 iterations, we are left with the rank-1 matrix p1pT
1 Gp1 = 0 since initial
Z ∗ satisﬁes tr(Z ∗G) = 0 and the following r − 1 updates are of the form yyT which satisﬁes yT Gy = 0.
If we denote the returned y vectors as yi for the iteration i and yr is the last one we are left with,
then yi’s satisfy the desired decomposition that Z ∗ = (cid:80)r

1 which satisﬁes pT

i Gyi = 0, i = 1, . . . , r.

i and yT

i=1 yiyT

Figure 6 is an illustration of the neural decomposition procedure for a toy example with d =
2 where the eigenvectors of Z ∗ and the vectors pj are plotted together. Due to the constraints

17

Illustration of the neural decomposition procedure for d = 2 (i.e. Z ∗ ∈ R3×3). The
Figure 6:
dashed red arrows correspond to the eigenvectors of Z ∗ (q1, q2, q3) and the solid blue arrows show
the decomposed vectors p1 and p2. In this example, the rank of Z ∗ is 2 where q1 and q2 are its two
principal eigenvectors. The eigenvalue corresponding to the eigenvector q1 is zero. The light blue
colored surface shows the Lorentz cones z = (cid:112)x2 + y2 and z = −(cid:112)x2 + y2. We observe that the
decomposed vectors p1 and p2 lie on the boundary of Lorentz cones.

j Gpj = 0, j = 1, 2, the vectors pj have to lie on the boundary of Lorentz cones3 z = (cid:112)x2 + y2 and
pT
z = −(cid:112)x2 + y2. Decomposing the solution of the convex problem Z ∗ and Z (cid:48)∗ onto these cones, i.e.,
neural decomposition, enables the construction of neural network weights from Z ∗ and Z (cid:48)∗.

5 Quadratic Activation Networks

In this section, we derive the corresponding convex program when the activation function is quadratic,
i.e., σ(u) = u2. The resulting convex problem takes a simpler form than the polynomial activation
case. We start by noting that the bound in (26) holds for any activation function. The inequalities
|vT σ(Xuj)| ≤ β however lead to diﬀerent constraints than the polynomial activation case. Note that
|vT (Xuj)2| ≤ β is equivalent to the inequalities

uT
j

(cid:32) n
(cid:88)

i=1

(cid:33)

(cid:32)

xixT

i vi

uj ≤ β and uT
j

−

(cid:33)

xixT

i vi

uj ≤ β .

n
(cid:88)

i=1

The constraint maxuj :(cid:107)uj (cid:107)2=1 |vT (Xuj)2| ≤ β can be expressed as largest eigenvalue inequalities

λmax

(cid:32) n
(cid:88)

i=1

(cid:33)

(cid:32)

xixT

i vi

≤ β and λmax

−

(cid:33)

xixT

i vi

≤ β ,

n
(cid:88)

i=1

(41)

(42)

3In special relativity, Lorentz cones describe the path that a ﬂash of light, emanating from a single event traveling

in all directions takes through spacetime (see Figure 1.3.1 in [32]).

18

x1.00.60.20.20.61.0y1.00.60.20.20.61.0z1.00.60.20.20.61.0q1q2q3p1p2eigenvectorsneural decompositionwhere λmax denotes the maximum eigenvalue. Next, representing the largest eigenvalue constraints
as linear matrix inequality constraints, we arrive at the following maximization problem

max
v

s.t.

− (cid:96)∗(−v)

n
(cid:88)

i=1

xixT

i vi − βId (cid:22) 0, −

n
(cid:88)

i=1

xixT

i vi − βId (cid:22) 0.

(43)

Writing the Lagrangian for (43) as L(v, Z1, Z2) = −(cid:96)∗(−v) − (cid:80)n

i=1 vixT

i (Z1 − Z2)xi + β tr(Z1 + Z2)

with Z1, Z2 ∈ Sd×d and maximizing with respect to v, we obtain the following convex problem

min
Z1,Z2(cid:23)0

(cid:96)

(cid:16)(cid:2)xT

1 (Z1 − Z2)x1

. . . xT

n (Z1 − Z2)xn

(cid:3)T

(cid:17)

, y

+ β tr(Z1 + Z2) .

(44)

Replacing Z = Z1−Z2, where Z1 (cid:23) 0, Z2 (cid:23) 0, we recall that any matrix Z can be uniquely decomposed
in this form thanks to the Moreau decomposition onto the cone of positive deﬁnite matrices and
its polar dual, which is the set of negative semideﬁnite matrices.
In particular, suppose that the
eigenvalue decomposition of Z is Z = (cid:80)
j . Then, Z1 and Z2 are uniquely determined by Z1 =
(cid:80)
j:λj >0 λjzjzT
j:λj <0(−λj) =
(cid:80)
j |λj| = (cid:107)Z(cid:107)∗ is the sum of the absolute values of the eigenvalues of Z, which is equivalent to the
nuclear norm for symmetric matrices. Consequently, this leads to the following simpliﬁed problem
with nuclear norm regularization:

j . Note that tr(Z1 + Z2) = (cid:80)

j and Z2 = − (cid:80)

j:λj >0 λj + (cid:80)

j:λj <0 λjzjzT

j λjzjzT

min
Z=ZT
s.t.

(cid:96)(ˆy, y) + β(cid:107)Z(cid:107)∗

ˆyi = xT

i Zxi,

i = 1, . . . , n .

(45)

Theorem 5.1 states the main result for the global optimization of quadratic activation neural

networks. The rest of this section is devoted to the proof and interpretation of Theorem 5.1.

Theorem 5.1 (Globally optimal convex program for quadratic activation cubic regularization net-
works). The solution of the convex problem in (45) provides a global optimal solution to the non-convex
problem for quadratic activation and cubic regularization given in (47) when the number of neurons
satisﬁes m ≥ m∗ where

m∗ = rank(Z ∗).

(46)

The optimal neural network weights are determined from the solution of the convex problem via eigen-
value decomposition of Z ∗ and the rescaling given in (51). The optimal number of neurons is upper
bounded by m∗ ≤ d since rank(Z ∗) ≤ d.

5.1 Strong Duality for Quadratic Activation

We have shown that a lower bound on the non-convex problem for quadratic activation is given
by the nuclear norm regularized convex objective. Now we show that this lower bound is in fact
identical to the non-convex problem. Suppose that Z ∗ is a solution to (45). Let us decompose Z ∗ via
eigenvalue decomposition as Z ∗ = (cid:80)
j . We can generate an upper bound on the non-convex
problem by constructing neural network parameters as αj = λj, and uj = zj with objective value
(cid:16)(cid:80)
j |λj|. Noting that this value exactly matches the optimal value of the convex
(cid:96)
objective in (45), we conclude that the optimal solution of (45) provides a global optimal solution to
the non-convex problem.

j(Xzj)2λj, y

j λjzjzT

+β (cid:80)

(cid:17)

19

5.2 Equivalent Non-convex Problem: Quadratic Activation with Cubic

Regularization

We now show that the non-convex problem with unit norm ﬁrst layer weights and the (cid:96)1 norm regu-
larized second layer weights is in fact equivalent to the non-convex problem with cubic regularization
on all the weights. Let us consider the unconstrained problem with cubic regularization:

p∗ := min
{αj ,uj }m

j=1

(cid:96)





m
(cid:88)

j=1



(Xuj)2αj, y

 +

β
c

m
(cid:88)

(|αj|3 + (cid:107)uj(cid:107)3

2) ,

j=1

(47)

3 + 2− 2
where c = 2 1
j = 1, . . . , m yields

3 ≈ 1.88988. Rescaling the variables uj ← ujt1/2

j

and αj ← αj/tj , ∀j for tj > 0,

p∗ = min
{αj ,uj }m

j=1

(cid:96)





m
(cid:88)

j=1



(Xuj)2αj, y

 +

β
c

m
(cid:88)

(|αj|3/t3

j + (cid:107)uj(cid:107)3

2t3/2

j

j=1

) .

(48)

Noting the regularization term is convex in tj for tj > 0 and optimizing it with respect to tj, we
obtain tj = 22/9 (cid:16) |αj |

. Plugging the expression for tj in yields

(cid:17)2/3

(cid:107)uj (cid:107)2

p∗ = min
{αj ,uj }m

j=1

(cid:96)





m
(cid:88)

j=1



(Xuj)2αj, y

 + β

m
(cid:88)

j=1

|αj|(cid:107)uj(cid:107)2
2 .

(49)

Now we deﬁne the scaled second layer weights α(cid:48)
and deﬁning u(cid:48)
weights as the regularization term

)2α(cid:48)
j
j = uj/(cid:107)uj(cid:107)2, we obtain the equivalent problem with the (cid:96)1 norm of the second layer

2. Noting that (Xuj)2αj = (X uj
(cid:107)uj (cid:107)2

j = αj(cid:107)uj(cid:107)2

{α(cid:48)

j , u(cid:48)

j }m

min
j=1, s.t.(cid:107)u(cid:48)

j (cid:107)2=1, ∀j



(cid:96)



m
(cid:88)

(Xu(cid:48)

j)2α(cid:48)

j, y

j=1



 + β

m
(cid:88)

j=1

|α(cid:48)

j| .

(50)

p∗ =

5.2.1 Rescaling

We note that the weights αj and uj that the eigenvalue decomposition of the solution of (45) gives
are scaled versions of the weights of the problem with cubic regularization in (47). The solution to
the problem in (47) can be constructed by rescaling the weights as

uj ← uj

(cid:113)

t(cid:48)
j, αj ←

αj
t(cid:48)
j

, where

j = 22/9|αj|2/3
t(cid:48)

j = 1, . . . , m.

(51)

This concludes the proof of Theorem 5.1.

5.3 Comparison with Polynomial Activation Networks

In this subsection, we list the important diﬀerences between the results for quadratic activation and
polynomial activation neural networks. The convex program for the quadratic activation network does
not have the equality constraints that appear in the convex program for the polynomial activation.
In addition, for the quadratic activation, the upper bound on the critical width m∗ is d while it is
2(d + 1) for the polynomial activation case.

We note that in the case of quadratic activation, the optimal neural network weights are determined
from eigenvalue decomposition of Z ∗. This results in the ﬁrst layer weights to be orthonormal because

20

they can be chosen as the eigenvectors of the real and symmetric matrix Z ∗. In contrast, we do not
have this property for polynomial activations as the associated optimal weights are determined via
neural decomposition. In this case, the resulting hidden neurons are not necessarily orthogonal, which
shows that the Neural Decomposition is a type of non-orthogonal matrix decomposition. This can
also be seen in Figure 6.

5.4 Constructing Multiple Globally Optimal Solutions in the Neural Net-

work Parameter Space

Once we ﬁnd an optimal Z ∗ using the SDP in (45), we can transform it to the neural network parameter
space with at most d neurons using the eigenvalue decomposition of Z ∗ as Z ∗ = (cid:80)d
j αj.
However, we can also generate a neural network with an arbitrary number of neurons, which is also
optimal. We now describe this construction below for an arbitrary number of neurons m ≥ 2d. Let
us pick an arbitrary m/2 × d matrix H with orthonormal columns, i.e.,

j=1 ujuT

Id = H T H =

m/2
(cid:88)

j=1

hjhT
j ,

(52)

where h1, . . . , hm/2 are the rows of H and we assume m/2 ≥ d. One can generate such matrices using
randomized Haar ensemble, or partial Hadamard matrices. Then, we can represent Z ∗ using

Z ∗ = Z ∗H T H

=

m/2
(cid:88)

j=1

Z ∗hjhT
j .

Since Z ∗ is a symmetric matrix, (cid:80)m/2

j=1 Z ∗hjhT

j is also symmetric, and we can write

Z ∗ =

1
2

m/2
(cid:88)

(Z ∗hjhT

j + hjhT

j Z ∗) .

j=1

Finally, for each term in the above summation, we employ the symmetrization identity

xyT + yxT =

1
2

(cid:0)(x + y)(x + y)T − (x − y)(x − y)T (cid:1) ,

valid for any x, y ∈ Rd. We arrive at the representation

Z ∗ =

1
4

m/2
(cid:88)

((Z ∗hj + hj)(Z ∗hj + hj)T − (Z ∗hj − hj)(Z ∗hj − hj)T )

j=1

=

m
(cid:88)

j=1

ujuT

j αj ,

(53)

(54)

where uj = Z ∗hj + hj, αj = 1/4 for j = 1, . . . , m/2 and uj = Z ∗hj − hj, αj = −1/4 for j =
m/2 + 1, . . . , m.

Since the matrix H is arbitrary, one can map an optimal Z ∗ matrix from the convex semideﬁnite

program to inﬁnitely many optimal solutions in the neural network parameterization space.

21

6 Standard Weight Decay Formulation is NP-Hard

In Section 5, we have studied two-layer neural networks with quadratic activation and cubic regular-
ization and derive a convex program whose solution globally optimizes the non-convex problem. In
this section, we show that if, instead of cubic regularization, we have quadratic regularization (i.e.
weight decay), the resulting optimization problem is an NP-hard problem.

Theorem 6.1. The two-layer neural network optimization problem with quadratic activation and
standard (cid:96)2-squared regularization, i.e., weight decay, in (55) is NP-hard for β → 0.

The remainder of this section breaks down the proof of Theorem 6.1. At the core of the proof is

the polynomial-time reduction of the problem to the NP-hard problem of phase retrieval.

6.1 Reduction to an Equivalent Problem

The optimization problem for training a two-layer fully connected neural network with quadratic
activation and quadratic regularization can be stated as




p∗ := min
{αj ,uj }m

j=1

(cid:96)



m
(cid:88)

j=1

β
c

m
(cid:88)

j=1

(Xuj)2αj, y

 +

(|αj|2 + (cid:107)uj(cid:107)2

2) ,

(55)

where the scaling factor c is the same as before (i.e. c = 2 1
and αj ← αj/tj for tj > 0, j = 1, . . . , m, we obtain the following equivalent optimization problem

3 ≈ 1.88988). Rescaling uj ← ujt1/2

3 + 2− 2

j

p∗ = min
{αj ,uj }m

j=1

(cid:96)





m
(cid:88)

j=1

(Xuj)2αj, y



 +

β
c

m
(cid:88)

j=1

(|αj|2/t2

j + (cid:107)uj(cid:107)2

2tj) .

(56)

Note that the regularization term is convex in tj for tj > 0. Optimizing the regularization term with
respect to tj leads to tj = 21/3 (cid:16) |αj |

and plugging this in yields

(cid:17)2/3

(cid:107)uj (cid:107)2

p∗ = min
{αj ,uj }m

j=1

(cid:96)





m
(cid:88)

j=1



(Xuj)2αj, y

 + β

m
(cid:88)

j=1

|αj|2/3(cid:107)uj(cid:107)4/3

2

.

Deﬁning scaled weights α(cid:48)

j = αj(cid:107)uj(cid:107)2

2 and u(cid:48)

j = uj/(cid:107)uj(cid:107)2, we obtain the equivalent problem

p∗ =

min
j=1 s.t.(cid:107)u(cid:48)

{α(cid:48)

j ,u(cid:48)

j }m

j (cid:107)2=1, ∀j



(cid:96)



m
(cid:88)

j=1

(Xu(cid:48)

j)2α(cid:48)

j, y



 + β

m
(cid:88)

j=1

|α(cid:48)

j|2/3 .

(57)

(58)

This shows that solving the standard weight decay formulation is equivalent to solving a 2/3-norm

penalized problem with unit norm ﬁrst layer weights.

6.2 Hardness Result

We design a data matrix such that the solution coincides with solving the phase retrieval problem
which is NP-hard (see [18]). We consider the equality constrained version of (58), i.e., β → 0, which
is given by

m
(cid:88)

j=1
m
(cid:88)

j=1

|αj|2/3

(Xuj)2αj = y .

(59)

min

{αj ,uj }m

j=1 s.t. (cid:107)uj (cid:107)2=1,∀j

s.t.

22

6.2.1 Addition of a Simplex Constraint

Let the ﬁrst d rows of the data matrix X be eT
the constraint (cid:80)m

j=1(Xuj)2 = y implies

1 , . . . , eT

d and let the ﬁrst d entries of y be 1/d. Then,

m
(cid:88)

j=1

u2
jkαj = 1/d for k = 1, . . . , d .

(60)

Summing the above for all k = 1, . . . , d, and noting that (cid:80)d
(cid:80)m

j=1 αj = 1.

k=1 u2

jk = 1 lead to the constraint

6.2.2 Reduction to the NP-Hard Phase Retrieval and Subset Sum Problem
We let X = [I; ˜X] and y = [ 1
d
previous subsection. In this case, the optimization problem reduces to

¯1; ˜y] to obtain the simplex constraint (cid:80)m

j=1 αj = 1 as shown in the

min

{αj ,uj }m

j=1 s.t. (cid:107)uj (cid:107)2=1,∀j

s.t.

m
(cid:88)

|αj|2/3

j=1
m
(cid:88)

( ˜Xuj)2αj = ˜y

j=1
m
(cid:88)

j=1
m
(cid:88)

j=1

u2
jkαj = 1/d,

k = 1, . . . , d

αj = 1 .

(61)

Suppose that there exists a feasible solution {α∗
and u∗
1
strictly optimal. Consequently, the problem in (61) is equivalent to

j }m
1 = 1
1 = 1 with only one nonzero neuron. Then, it follows from Lemma 6.2 that this solution is

j=1, which satisﬁes (cid:107)α∗(cid:107)0 = 1, where α∗

j , u∗

T u∗

s.t.

ﬁnd u1
(˜xT
i u1)2 = ˜yi,
u2
1k = 1/d,

i = 1, . . . , (n − d)

k = 1, . . . , d .

Lemma 6.2 ((cid:96)p minimization recovers 1-sparse solutions when 0 < p < 1).
Consider the optimization problem

min
α1,...,αm

s.t.

m
(cid:88)

i=1
m
(cid:88)

i=1

|αi|p

αi = 1, α ∈ C ,

(62)

(63)

where C is a convex set and p ∈ (0, 1). Suppose that there exists a feasible solution α∗ ∈ C and
(cid:80)
i = 1 such that (cid:107)α∗(cid:107)0 = 1. Then, α∗ is strictly optimal with objective value 1. More precisely,

i α∗

any solution with cardinality strictly greater than 1 has objective value strictly larger than 1.

6.2.3 NP-hardness Proof

Subset sum problem given in Deﬁnition 4 is a decision problem known to be NP-complete (e.g. [18]).
The decision version of the problem in (62) can be stated as follows: Does there exist a feasible u1?

23

We show that this decision problem is NP-hard via a polynomial-time reduction to the subset sum
problem.

Deﬁnition 4 (Subset sum problem). Given a set of integers A, does there exist a subset AS whose
elements sum to z?

Lemma 6.3 establishes the reduction of the decision version of (62) to the subset sum problem.
The proof is provided in the appendix and follows the same approach used in the proof for the NP-
hardness of phase retrieval in [18], with the main diﬀerence being the additional constraints u2
1k = 1/d,
k = 1, . . . , d in (62). Finally, Lemma 6.3 concludes the proof of Theorem 6.1.
Lemma 6.3. Consider the problem in (62). Let the ﬁrst d samples of ˜X ∈ R(d+1)×d, denoted
˜XD ∈ Rd×d, be any diagonal matrix with −1’s and +1’s on its diagonal, and let the (d + 1)’st sample
. Then, the decision version of the resulting problem returns ’yes’ if and
be ˜xd+1 =
only if the answer for the subset sum problem with A = {a1, . . . , ad} is ’yes’.

d (cid:2)a1

. . .

ad

(cid:3)T

√

Remark 6.1. It follows from Theorem 6.1 that the two-layer neural network training problem with
polynomial activation and unit norm ﬁrst layer weights and (cid:80)
j |αj|p as the regularization term with
p < 1 is also NP-hard for β → 0 since it reduces to the quadratic activation case for the polynomial
coeﬃcients a = 1, b = 0, c = 0.

7 Vector Output Networks

The derivations until this point have been for neural network architectures with scalar outputs, i.e.,
yi ∈ R. In this section, we turn to the vector output case yi ∈ RC where C is the output dimension,
and derive a convex problem that has the same optimal value as the non-convex neural network
optimization problem. We exploit the same techniques described in the scalar output case except for
the part for constructing the vector second layer weights from the solution of the convex program.
In the scalar output case, the convex problem is over the symmetric matrices Z, Z (cid:48) and in the vector
output case, the optimization is over C such matrix pairs Zk, Z (cid:48)

k, k = 1, . . . , C.

We begin our treatment of the vector output case by considering the neural network deﬁned by

f (x) =

m
(cid:88)

j=1

σ(xT uj)αT
j ,

(64)

where αj ∈ RC, j = 1, . . . , m are the vector second layer weights. Note that in the scalar output case,
the second layer weights αj were scalars. Taking the regularization to be the (cid:96)1 norm of the second
layer weights, the neural network training requires solving the following non-convex optimization
problem

p∗ =

min

{uj , αj }m

j=1, s.t. (cid:107)uj (cid:107)2=1, ∀j

(cid:96)





m
(cid:88)

j=1



σ(Xuj)αT

j , Y

 + β

m
(cid:88)

j=1

(cid:107)αj(cid:107)1 ,

(65)

where Y ∈ Rn×C is the output matrix. Equivalently,

p∗ =

min

{uj }m

j=1 s.t. (cid:107)uj (cid:107)2=1, ∀j

(cid:16) ˆY , Y

(cid:96)

(cid:17)

+ β

min
{αj }m

j=1, ˆY

m
(cid:88)

j=1

(cid:107)αj(cid:107)1

s.t.

ˆY =

m
(cid:88)

j=1

σ(Xuj)αT
j .

(66)

The dual problem for the inner minimization problem is given by

max
v

−(cid:96)∗(−v)

s.t.

|vT

k σ(Xuj)| ≤ β , ∀j, k ,

(67)

where v ∈ Rn×C is the dual variable and vk ∈ Rn is the k’th column of v.

Theorem 7.1 gives the main result of this section.

24

Theorem 7.1 (Globally optimal convex program for polynomial activation vector output networks).
The solution of the convex problem in (72) provides a global optimal solution for the vector output
non-convex problem in (65) when the number of neurons satisﬁes m ≥ m∗ where

C
(cid:88)

m∗ =

(rank(Z ∗

k ) + rank(Z (cid:48)
k

∗)).

k=1

(68)

The optimal neural network weights are determined from the solution of the convex problem via the
∗ and the construction given in (75). The optimal
neural decomposition procedure for each Z ∗
number of neurons is upper bounded by m∗ ≤ 2(d + 1)C.

k and Z (cid:48)
k

Proof of Theorem 7.1. Applying the S-procedure for the constraints in the dual problem (67), we
obtain the following maximization problem

max − (cid:96)∗(−v)

s.t.

− 1

(cid:20)ρk,1I − a (cid:80)n
2 bvT
(cid:20)ρk,2I + a (cid:80)n
2 bvT

i=1 xixT
k X
i=1 xixT
k X

1

i vi,k

i vi,k

− 1

2 bX T vk
β − c ¯1T vk − ρk,1
2 bX T vk
β + c ¯1T vk − ρk,2

1

(cid:23) 0,

k = 1, . . . , C

(cid:21)

(cid:21)

(cid:23) 0,

k = 1, . . . , C .

(69)

Next, let us introduce the following Lagrange multipliers

Zk =

(cid:20)Zk,1 Zk,2
Zk,3 Zk,4

(cid:21)

∈ S(d+1)×(d+1), Z (cid:48)

k =

Then, the Lagrangian is

L (cid:0)v, {ρk,1, ρk,2, Zk, Z (cid:48)

k}C

k=1

(cid:1) =

(cid:20)Z (cid:48)
Z (cid:48)

k,1 Z (cid:48)
k,3 Z (cid:48)

k,2

k,4

(cid:21)

∈ S(d+1)×(d+1),

k = 1, . . . , C.

(70)

= −(cid:96)∗(−v) +

C
(cid:88)

k=1

(cid:0)ρk,1 tr(Zk,1) + ρk,2 tr(Z (cid:48)

k,1)(cid:1) − a

C
(cid:88)

n
(cid:88)

k=1

i=1

vi,kxT

i (Zk,1 − Z (cid:48)

k,1)xi − b

+

C
(cid:88)

k=1

(cid:0)(β − ρk,1)Zk,4 + (β − ρk,2)Z (cid:48)

k,4

(cid:1) − c

C
(cid:88)

n
(cid:88)

k=1

i=1

vk,i(Zk,4 − Z (cid:48)

k,4) .

Finally maximizing the Lagrangian leads to the following convex SDP:

min

{Zk=ZT

k ,Z(cid:48)

k=Z(cid:48)
k

T }C

k=1

s.t.

(cid:96)( ˆY , Y ) + β

C
(cid:88)

(Zk,4 + Z (cid:48)

k,4)

k=1
i (Zk,1 − Z (cid:48)

ˆYik = axT
tr(Zk,1) = Zk,4, tr(Z (cid:48)
Zk (cid:23) 0, Z (cid:48)

k,1)xi + bxT
i (Zk,2 − Z (cid:48)
k,1) = Z (cid:48)
k,4,
k = 1, . . . , C .

k (cid:23) 0,

k = 1, . . . , C

k,2) + c(Zk,4 − Z (cid:48)

k,4),

C
(cid:88)

k=1

k X(Zk,2 − Z (cid:48)
vT

k,2)+

(71)

i ∈ [n], k ∈ [C]

(72)

We construct the neural network weights from the optimal solution of the convex program as
follows. We follow the neural decomposition procedure from Section 4 for extracting neurons from
each of the matrices Z ∗

∗, k = 1, . . . , C. The decompositions for Z ∗

k will be of the form

k and Z (cid:48)
k

Z ∗

k,1 =

rk(cid:88)

j=1

uk,juT

k,jd2

k,j, Z ∗

k,2 =

rk(cid:88)

j=1

25

uk,jd2

k,j, Z ∗

k,4 =

rk(cid:88)

j=1

d2
k,j.

(73)

Then, the weights due to Z ∗

k , k = 1, . . . , C are determined as follows:

First layer weights:

Second layer weights:

{u1,1, u1,2, . . . , u1,r1 }, . . . , {uC,1, uC,2, . . . , uC,rC }
C,2eT
1 }, . . . , {d2
eT
{d2

1 , . . . , d2

C,1eT

C, d2

1,1eT

1,2eT

1 , d2

1,r1

C, . . . , d2

C,rC

eT
C} ,

(74)

where ek denotes the k’th C-dimensional unit vector, and rk is the rank of the matrix Z ∗
the matrix Z ∗
k,1eT
weights {d2
k,rk
way. Then, we reach the following neural network construction:

k . In short,
k with rank rk leads to the ﬁrst layer weights {uk,1, uk,2, . . . , uk,rk } and the second layer
∗, k = 1, . . . , C are determined the same
k , d2

k }. The weights due to Z (cid:48)
eT
k

k , . . . , d2

k,2eT

f (X) =

C
(cid:88)

rk(cid:88)

k=1

j=1

σ(Xuk,c)d2

k,jeT

k +

C
(cid:88)

r(cid:48)
k(cid:88)

k=1

j=1

σ(Xu(cid:48)

k,c)d(cid:48)

k,j

2eT
k .

(75)

Finally, the total number of neurons that the convex problem ﬁnds is (cid:80)C
k and Z (cid:48)
k

k). The maximum
∗ are full rank, and this corresponds to a maximum total

k=1(rk+r(cid:48)

number of neurons occurs if all Z ∗
of 2(d + 1)C neurons.

We plug the decomposition expressions given in (73) in the convex program in (72) to conclude
that the optimal value of the convex program is an upper bound for the non-convex optimization
problem (65). The k’th entry of the estimate for the i’th training sample is

ˆYik = axT
i





rk(cid:88)

j=1

uk,juT

k,jd2

k,j +



rk(cid:88)

+ c



r(cid:48)
k(cid:88)

(−d(cid:48)

k,j

d2
k,j +

2)



k,ju(cid:48)
u(cid:48)

k,j

T (−d(cid:48)

k,j


 xi + bxT
i

2)





rk(cid:88)

j=1

uk,jd2

k,j +



k,j(−d(cid:48)
u(cid:48)

k,j

2)

 +

r(cid:48)
k(cid:88)

j=1

r(cid:48)
k(cid:88)

j=1


j=1

j=1

=

rk(cid:88)

j=1

σ(xT

i uk,j)d2

k,j +

r(cid:48)
k(cid:88)

j=1

σ(xT

i u(cid:48)

k,j)(−d(cid:48)

k,j

2) .

It follows that the output vector for the i’th sample is

ˆyi =

C
(cid:88)

rk(cid:88)

k=1

j=1

σ(xT

i uk,j)d2

k,jeT

k +

C
(cid:88)

r(cid:48)
k(cid:88)

k=1

j=1

σ(xT

i u(cid:48)

k,j)(−d(cid:48)

k,j

(76)

(77)

2)eT
k .

We note that this output is of the same form as the non-convex case (66). We also need to check that
the regularization term is equivalent to the sum of (cid:96)1 norms of the second layer weights:

C
(cid:88)

β

(Zk,4 + Z (cid:48)

k,4) = β

k=1

= β

C
(cid:88)

rk(cid:88)

k=1

j=1

C
(cid:88)

rk(cid:88)

k=1

j=1

d2
k,j + β

C
(cid:88)

r(cid:48)
k(cid:88)

k=1

j=1

2

d(cid:48)
k,j

(cid:107)d2

k,jeT

k (cid:107)1 + β

C
(cid:88)

r(cid:48)
k(cid:88)

k=1

j=1

(cid:107) − d(cid:48)

k,j

2eT

k (cid:107)1 ,

(78)

which is of the form β (cid:80)m
j=1 (cid:107)αj(cid:107)1. Hence, the neural network weights that we obtain via the neural
decomposition procedure lead to an upper bound for the original non-convex optimization problem.
This concludes the proof that the optimal solution of the convex problem (72) provides a global
optimal solution to the non-convex problem (65).

26

8 Convolutional Neural Networks

In this section, we consider two-layer convolutional networks with a convolutional ﬁrst layer and a
fully connected second layer. We will denote the ﬁlter size by f . Let us denote the patches of a data
sample x by x1, . . . , xK where the patches have the same dimension as the ﬁlters, i.e., xk ∈ Rf . The
stride and padding do not aﬀect the below derivations as they can be readily handled when forming
the patches. The output of this network is expressed as:

f (x) =

m
(cid:88)

K
(cid:88)

j=1

k=1

σ(xT

k uj)αjk ,

(79)

where uj ∈ Rf denotes the j’th ﬁlter. We will take the regularization to be the (cid:96)1 norm of the second
layer weights αj = (cid:2)αj1

∈ RK, j = 1, . . . , m:

. . . αjK

(cid:3)T

p∗ =

min

{uj }m

j=1 s.t. (cid:107)uj (cid:107)2=1, ∀j

min
{αj }m

j=1,ˆy

(cid:96)(ˆy, y) + β

m
(cid:88)

j=1

(cid:107)αj(cid:107)1

s.t.

ˆy =

m
(cid:88)

K
(cid:88)

j=1

k=1

σ(X T

k uj)αjk

(80)

where we use Xk ∈ Rn×f to denote the matrix with the k’th patch of all the data samples. The dual
for the inner minimization problem is given by

max
v

−(cid:96)∗(−v)

s.t.

|vT σ(Xkuj)| ≤ β , ∀j, k .

(81)

We state the main result of this section in Theorem 8.1.

Theorem 8.1 (Globally optimal convex program for polynomial activation convolutional neural net-
works). The solution of the convex problem in (85) provides a global optimal solution for the non-
convex convolutional neural network problem in (79) when the number of ﬁlters is at least (rank(Z ∗
k ) +
rank(Z (cid:48)
k

∗)) and equivalently, the number of neurons satisﬁes m ≥ m∗ where

m∗ = K

K
(cid:88)

k=1

(rank(Z ∗

k ) + rank(Z (cid:48)
k

∗)).

(82)

The optimal neural network weights are determined from the solution of the convex problem via the
∗. The optimal number of ﬁlters is upper bounded
neural decomposition procedure for each Z ∗
by 2(f + 1)K and the optimal number of neurons is upper bounded by m∗ ≤ 2(f + 1)K 2.

k and Z (cid:48)
k

Proof of Theorem 8.1. We apply the S-procedure to replace the constraints of (81) with equivalent
LMI constraints and this yields

max − (cid:96)∗(−v)

s.t.

(cid:20)ρk,1I − a (cid:80)n

i=1 xi,kxT

i,kvi

− 1

2 bvT Xk

(cid:20)ρk,2I + a (cid:80)n

i=1 xi,kxT

i,kvi

1

2 bvT Xk

− 1

2 bX T
k v
β − c ¯1T v − ρk,1
2 bX T
k v
β + c ¯1T v − ρk,2

1

(cid:23) 0,

k = 1, . . . , K

(cid:21)

(cid:21)

(cid:23) 0,

k = 1, . . . , K ,

(83)

where xi,k ∈ Rf denotes the k’th patch of the i’th data sample. The Lagrangian is as follows
L (cid:0)v, {ρk,1, ρk,2, Zk, Z (cid:48)

(cid:1) =

k}K

k=1

= −(cid:96)∗(−v) +

K
(cid:88)

k=1

(cid:0)ρk,1 tr(Zk,1) + ρk,2 tr(Z (cid:48)

k,1)(cid:1) − a

K
(cid:88)

n
(cid:88)

k=1

i=1

vixT

i,k(Zk,1 − Z (cid:48)

k,1)xi,k − b

+

K
(cid:88)

k=1

(cid:0)(β − ρk,1)Zk,4 + (β − ρk,2)Z (cid:48)

k,4

(cid:1) − c

K
(cid:88)

n
(cid:88)

k=1

i=1

vi(Zk,4 − Z (cid:48)

k,4) ,

27

K
(cid:88)

k=1

vT Xk(Zk,2 − Z (cid:48)

k,2)+

(84)

where Zk, Z (cid:48)
respect to v, ρk,1, ρk,2, k = 1, . . . , K yields the convex SDP

k are (f + 1) × (f + 1) dimensional symmetric matrices. Maximizing the Lagrangian with

min

{Zk=ZT

k ,Z(cid:48)

k=Z(cid:48)
k

(cid:96)(ˆy, y) + β

K
(cid:88)

(Zk,4 + Z (cid:48)

k,4)

k=1

T }K

k=1

s.t.

ˆyi = a

K
(cid:88)

k=1

i,k(Zk,1 − Z (cid:48)
xT

k,1)xi,k + b

K
(cid:88)

i,k(Zk,2 − Z (cid:48)
xT

k,2) + c

k=1
k = 1, . . . , K

tr(Zk,1) = Zk,4, tr(Z (cid:48)
Zk (cid:23) 0, Z (cid:48)

k,1) = Z (cid:48)
k,4,
k = 1, . . . , K .

k (cid:23) 0,

K
(cid:88)

k=1

(Zk,4 − Z (cid:48)

k,4),

i ∈ [n]

(85)

We now show that the convex program in (85) provides an upper bound for the non-convex problem
via the same strategy that we have used for the vector output case in Section 7. We construct the
∗, k = 1, . . . , K via neural decomposition:
neural network weights from each of the matrices Z ∗

Z ∗

k,1 =

rk(cid:88)

j=1

uk,juT

k,jd2

k,j, Z ∗

k,2 =

uk,jd2

k,j, Z ∗

k,4 =

rk(cid:88)

j=1

d2
k,j ,

(86)

k and Z (cid:48)
k
rk(cid:88)

j=1

and the weights due to each Z ∗

k are

First layer ﬁlters:

Second layer weights:

uk,1, uk,2, . . . , uk,rk
{d2

k,1, 0, 0, . . . , 0}, {0, d2

k,2, 0, . . . , 0}, . . . , {0, 0, 0, . . . , d2

k,rk

} .

(87)

To clarify, for each ﬁlter uk,j, we have K (scalar) weights in the second layer because we apply the
same ﬁlter to K diﬀerent patches and the resulting K numbers (after being input to the activation
function) each are multiplied by a diﬀerent second layer weight. The second layer weights associated
with the ﬁlter uk,j will be these K numbers: {0, . . . , 0, d2
k,j, 0 . . . , 0}, where the only nonzero entry
k ) ﬁlters and K rank(Z ∗
is the j’th one. Consequently, each Z ∗
k ) neurons.
k) ﬁlters and K (cid:80)K
Including the weights due to Z (cid:48)
k=1(rk + r(cid:48)
k)
k
neurons in total. The optimal number of ﬁlters is upper bounded by 2(f + 1)K and the optimal
number of neurons is upper bounded by 2(f + 1)K 2.

k matrix produces rank(Z ∗

∗ as well, we will have (cid:80)K

k=1(rk + r(cid:48)

We omit the details of plugging the weights into the convex objective to show that it becomes

equivalent to the non-convex objective. The details are similar to the vector output case.

9 Average Pooling

In this section we will consider convolutional neural networks with average pooling. We will denote
the pool size by P . Let us consider a two-layer neural network where the ﬁrst layer is a convolutional
layer with ﬁlter size f . The convolutional layer is followed by the polynomial activation, average
pooling, and a fully connected layer. We will denote the number of patches per sample by K. The
output of this architecture can be expressed as

f (x) =

m
(cid:88)

K/P
(cid:88)

(cid:32)

j=1

k=1

1
P

P
(cid:88)

l=1

(cid:33)

σ(xT

(k−1)P +luj)

αjk .

(88)

We note that the number of parameters in the second layer (i.e. αjk’s) is equal to m K
mization problem for this architecture can be written as

P . The opti-

p∗ =

min

{uj }m

j=1 s.t. (cid:107)uj (cid:107)2=1, ∀j

min
{αj }m

j=1,ˆy

(cid:96)(ˆy, y) + β

m
(cid:88)

j=1

(cid:107)αj(cid:107)1

s.t.

ˆy =

m
(cid:88)

K/P
(cid:88)

(cid:32)

j=1

k=1

1
P

P
(cid:88)

l=1

(cid:33)

σ(X(k−1)P +luj)

αjk ,

(89)

28

where αj = (cid:2)αj1
by

. . . αj,K/P

(cid:3)T

, j = 1, . . . , m. The dual of the inner minimization problem is given

−(cid:96)∗(−v)

s.t.

max
v

(cid:32)

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

vT

1
P

P
(cid:88)

l=1

σ(X(k−1)P +luj)

(cid:33)(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

≤ β , ∀j, k .

(90)

Theorem 9.1 states our result for CNN with average pooling.

Theorem 9.1 (Globally optimal convex program for polynomial activation convolutional neural net-
works with average pooling). The solution of the convex problem in (95) provides a global optimal
solution for the non-convex problem for the convolutional neural network with average pooling in (89)
when the number of neurons satisﬁes m ≥ m∗ where

m∗ =

K
P

K/P
(cid:88)

k=1

(rank(Z ∗

k ) + rank(Z (cid:48)
k

∗)).

(91)

The optimal neural network weights are determined from the solution of the convex problem via the
∗. The optimal number of neurons is upper bounded
neural decomposition procedure for each Z ∗
by m∗ ≤ 2(f + 1) K2
P 2 .

k and Z (cid:48)
k

Proof of Theorem 9.1. We rewrite the constraints of the dual problem (90) as follows:

−β ≤

1
P

P
(cid:88)

l=1

(cid:32)

(cid:32)

uT
j

a

n
(cid:88)

i=1

xi,(k−1)P +lxT

i,(k−1)P +lvi

uj + bvT X(k−1)P +luj + cvT ¯1

≤ β,

∀j, k .

(cid:33)

(cid:33)

(92)

S-procedure allows us to write this problem equivalently as

max − (cid:96)∗(−v)

(cid:34)
ρk,1I − a 1
P

s.t.

(cid:34)
ρk,2I + a 1
P

l=1

(cid:80)P
(cid:80)n
2P b (cid:80)P
− 1
(cid:80)P
(cid:80)n
2P b (cid:80)P

i=1 xi,(k−1)P +lxT
l=1 vT X(k−1)P +l
i=1 xi,(k−1)P +lxT
l=1 vT X(k−1)P +l

l=1

1

i,(k−1)P +lvi − 1

2P b (cid:80)P
l=1 X T
β − c ¯1T v − ρk,1

(k−1)P +lv

(cid:35)

(cid:23) 0,

k = 1, . . . , K/P

i,(k−1)P +lvi

1

2P b (cid:80)P

l=1 X T
β + c ¯1T v − ρk,2

(k−1)P +lv

(cid:35)

(cid:23) 0,

k = 1, . . . , K/P .

(93)

The Lagrangian is as follows

(cid:16)

L

v, {ρk,1, ρk,2, Zk, Z (cid:48)

k}K/P

k=1

(cid:17)

=

= −(cid:96)∗(−v) +

− b

1
P

K/P
(cid:88)

P
(cid:88)

k=1

l=1

K/P
(cid:88)

k=1

(cid:0)ρk,1 tr(Zk,1) + ρk,2 tr(Z (cid:48)

k,1)(cid:1) − a

1
P

K/P
(cid:88)

P
(cid:88)

n
(cid:88)

k=1

l=1

i=1

vixT

i,(k−1)P +l(Zk,1 − Z (cid:48)

k,1)xi,(k−1)P +l

vT X(k−1)P +l(Zk,2 − Z (cid:48)

k,2) +

K/P
(cid:88)

k=1

(cid:0)(β − ρk,1)Zk,4 + (β − ρk,2)Z (cid:48)

k,4

(cid:1) − c

K/P
(cid:88)

n
(cid:88)

k=1

i=1

vi(Zk,4 − Z (cid:48)

k,4) ,

(94)

where Zk, Z (cid:48)

k are (f + 1) × (f + 1) dimensional symmetric matrices. Maximizing the Lagrangian with

29

respect to v, ρk,1, ρk,2, k = 1, . . . , K/P yields the following convex SDP:

min

{Zk=ZT

k ,Z(cid:48)

k=Z(cid:48)
k

(cid:96)(ˆy, y) + β

T }K/P
k=1

K/P
(cid:88)

k=1

(Zk,4 + Z (cid:48)

k,4)

s.t.

ˆyi = a

1
P

K/P
(cid:88)

P
(cid:88)

k=1

l=1

i,(k−1)P +l(Zk,1 − Z (cid:48)
xT

k,1)xi,(k−1)P +l + b

1
P

K/P
(cid:88)

P
(cid:88)

k=1

l=1

i,(k−1)P +l(Zk,2 − Z (cid:48)
xT

k,2)+

+ c

K/P
(cid:88)

k=1

(Zk,4 − Z (cid:48)

k,4),

i ∈ [n]

tr(Zk,1) = Zk,4, tr(Z (cid:48)
Zk (cid:23) 0, Z (cid:48)

k,1) = Z (cid:48)
k = 1, . . . , K/P .

k,4,

k (cid:23) 0,

k = 1, . . . , K/P

(95)

We omit the details of constructing the neural network weights from the solution of the convex
∗, k = 1, . . . , K/P which follows in a similar fashion as the proof of Theorem 8.1.

SDP Z ∗

k , Z (cid:48)
k

We note that when we pick the pool size as P = 1, this is the same as not having average pooling,
and the corresponding convex program is the same as (85), derived in Section 8. The other extreme
for the pool size is when P = K and this corresponds to what is known as global average pooling in
which case the convex SDP simpliﬁes to

min
Z=ZT ,Z(cid:48)=Z(cid:48) T

(cid:96)(ˆy, y) + β(Z4 + Z (cid:48)
4)

s.t.

ˆyi = a

K
(cid:88)

i,l(Z1 − Z (cid:48)
xT

1)xi,l + b

1
K

l=1
tr(Z1) = Z4, tr(Z (cid:48)
Z (cid:23) 0, Z (cid:48) (cid:23) 0.

1) = Z (cid:48)
4

1
K

K
(cid:88)

l=1

i,l(Z2 − Z (cid:48)
xT

2) + c(Z4 − Z (cid:48)

4),

i ∈ [n]

(96)

We note that the problem (96) has only two variables Z and Z (cid:48). This should be contrasted with the
convolutional architecture with no pooling (85) which has 2K variables.

10 Numerical Results

In this section, we present numerical results that verify the presented theory of the convex formu-
lations along with experiments comparing the test set performance of the derived formulations. All
experiments have been run on a MacBook Pro with 16GB RAM.

Solvers: We have used CVXPY [11, 1] for solving the convex SDPs. In particular, we have used
the open source solver SCS (splitting conic solver) [34, 35] in CVXPY, which is a scalable ﬁrst order
solver for convex cone problems.

Furthermore, we have solved the non-convex problems via backpropagation for which we have used
PyTorch [36]. We have used the SGD algorithm for the non-convex models. For all the experiments
involving SGD in this section, we show only the results corresponding to the best learning rate that
we select via an oﬄine hyperparameter search. The momentum parameter is 0.9. In the plots, the
non-convex models are either labeled as ’Backpropagation (GD)’ or ’Backpropagation (SGD)’. The
ﬁrst one, short for gradient descent, means that the batch size is equal to the number of samples n,
and the second one, short for stochastic gradient descent, means that the batch size is not n and the
exact batch size is explicitly stated in the ﬁgure captions.

Polynomial approximation of activation functions: To obtain the degree-2 polynomial ap-
proximation of a given activation function σ(u) such as the ReLU activation, one way is to select the

30

polynomial coeﬃcients a, b, c that minimize the (cid:96)2 norm objective (cid:107)T (cid:2)a b

c(cid:3)T

− s(cid:107)2 with

T =







1

 ,

t2
1

t1
...
t2
N tN 1

s =






σ(t1)
...
σ(tN )




 ,

(97)

where ti’s are linearly spaced in [L, U ]. The lower and upper limits L and U specify the range in
which we would like to approximate the given activation function. For instance, when L = −5,
U = 5, N = 1000 and σ(u) is the ReLU activation, the optimal polynomial coeﬃcients are a =
0.09, b = 0.5, c = 0.47. When we change the approximation range to a slightly narrower one with
L = −4, U = 4, the coeﬃcients then become a = 0.12, b = 0.5, c = 0.38. Note that the training data
can be normalized appropriately to conﬁne the range of the input to the neurons and control the
approximation error.

10.1 Results for Verifying the Theoretical Formulations

The ﬁrst set of numerical results in Figure 7 is for verifying that the derived convex problems have the
same optimal value as their non-convex counterparts. The plots in Figure 7 show the non-convex cost
against time when 1) the non-convex problem is solved in PyTorch and 2) the corresponding convex
problem (see Table 1) is solved using CVXPY. The number of neurons for the non-convex models in
all of the plots in Figure 7 is set to the optimal number of neurons m∗ found by the convex problem.
Figure 7 demonstrates that solving the convex SDP takes less time than solving the associated
non-convex problem using backpropagation for all of the neural network architectures. Figure 7 also
shows that the training of the non-convex models via the backpropagation algorithm does not always
yield the global optimal but instead may converge to local minima. In addition, we note that the
plots do not reﬂect the time it takes to tune the learning rate for the non-convex models, which was
performed oﬄine.

10.2 Experiments on UCI datasets

We now show how the derived convex programs perform in the context of classiﬁcation datasets.
The datasets used in this subsection are from the UCI machine learning repository [13]. The plots
in Figure 8 show the training and test set costs and classiﬁcation accuracies for binary classiﬁcation
datasets and the plots in Figure 9 are for multiclass classiﬁcation datasets. The convex program used
for solving the binary classiﬁcation problem is the scalar output polynomial activation problem given
in (21) and for the multiclass problem it is the vector output version given in (72).

We note that the training cost plots of Figure 8 and 9 are consistent with the theoretical results.
The accuracy plots show that the convex programs achieve the same ﬁnal accuracy of the non-convex
models or higher accuracies in shorter amounts of time.

Table 2 shows the classiﬁcation accuracies of various fully connected neural network architectures
on binary classiﬁcation UCI datasets. For each dataset, the training and validation partitions are
as pre-processed in [17]. The training and validation partitions are used to select the best hyper-
parameters. The hyperparameter search for the non-convex models includes searching for the best
regularization coeﬃcient β and learning rate. Gradient descent has been used to optimize the non-
convex models and the number of epochs is 1000. After determining the best hyperparameters, we
compute the 4-fold cross validation accuracy and report it in this table. The partitions for the 4-fold
cross validation are also the same as those pre-processed by [17]. Furthermore, for the results shown in
Table 2, the number of neurons for all the non-convex models is set to 2(d + 1), which is the maximum
number of neurons that the polynomial activation convex SDP could output (see Theorem 3.1). Table
2 shows that the convex SDP achieves better or similar accuracy values compared to the non-convex
models on most of the datasets.

31

(a) Quad act (10, 20, 9)

(b) Quad act (100, 20, 20)

(c) Quad act (500, 20, 20)

(d) Poly act (10, 20, 10)

(e) Poly act (100, 20, 23)

(f) Poly act (500, 20, 35)

(g) Vect out C = 3, (10, 20, 61)

(h) Vect out C = 3, (100, 20, 86)

(i) Vect out C = 3, (500, 20, 75)

(j) Convol f = 3, (10, 20, 11)

(k) Convol f = 3, (100, 20, 72)

(l) Convol f = 3, (500, 20, 100)

Figure 7: The numbers in the sub-captions refer to the parameters (n, d, m∗). These ﬁgures show
the training cost against time for backpropagation (blue solid curves) and the convex problem (red
cross shows timing of the convex solver) for the following problems: a,b,c: Quadratic activation scalar
output, d,e,f: Polynomial activation scalar output, g,h,i: Polynomial activation vector output, j,k,l:
Polynomial activation convolutional. The data is artiﬁcially generated with 5 planted neurons and the
data matrix is the element-wise 4’th power of an i.i.d. Gaussian matrix. The regularization coeﬃcient
is β = 0.1 in all of the experiments. The polynomial coeﬃcients for the architectures with polynomial
activation are a = 0.09, b = 0.5, c = 0.47 (i.e. the ReLU approximation coeﬃcients).

32

0246810Time (sec)100101Training costBackpropagation (GD)Convex SDP (optimal)051015Time (sec)101102103Training costBackpropagation (GD)Convex SDP (optimal)0204060Time (sec)102103104Training costBackpropagation (GD)Convex SDP (optimal)0246810Time (sec)100101Training costBackpropagation (GD)Convex SDP (optimal)051015Time (sec)101102103Training costBackpropagation (GD)Convex SDP (optimal)0204060Time (sec)103104Training costBackpropagation (GD)Convex SDP (optimal)0.02.55.07.510.0Time (sec)101102Training costBackpropagation (GD)Convex SDP (optimal)05101520Time (sec)102103Training costBackpropagation (GD)Convex SDP (optimal)0255075100125Time (sec)102103104Training costBackpropagation (GD)Convex SDP (optimal)0510Time (sec)100Training costBackpropagation (GD)Convex SDP (optimal)010203040Time (sec)101102103Training costBackpropagation (GD)Convex SDP (optimal)050100150200250Time (sec)102103Training costBackpropagation (GD)Convex SDP (optimal)(a) DS1, training cost

(b) DS1, test cost

(c) DS1, training accuracy (d) DS1, test accuracy

(e) DS2, training cost

(f) DS2, test cost

(g) DS2, training accuracy (h) DS2, test accuracy

Figure 8: Results on UCI binary classiﬁcation datasets. DS1: dataset 1 is the breast cancer dataset
(n = 228, d = 9), DS2: dataset 2 is the credit approval dataset (n = 552, d = 15). Polynomial
activation with a = 0.09, b = 0.5, c = 0.47 is used. Number of neurons that the convex program found
is 16 and 18 for DS1 and DS2, respectively. The regularization coeﬃcient is β = 0.01 and β = 10 for
DS1 and DS2, respectively.

(a) DS3, training cost

(b) DS3, test cost

(c) DS3, training accuracy (d) DS3, test accuracy

(e) DS4, training cost

(f) DS4, test cost

(g) DS4, training accuracy (h) DS4, test accuracy

Figure 9: Results on UCI multiclass classiﬁcation datasets. DS3: dataset 3 is the annealing dataset
(n = 638, d = 31, C = 5), DS4: dataset 4 is the statlog vehicle dataset (n = 676, d = 18, C = 4).
Polynomial activation with a = 0.09, b = 0.5, c = 0.47 is used. Number of neurons that the convex
program found is 172 and 107 for DS3 and DS4, respectively. The regularization coeﬃcient is β = 1
both for DS3 and DS4.

33

05101520Time (sec)50100150Training costBackpropagation (GD)Convex SDP (optimal)05101520Time (sec)10203040Test costBackpropagation (GD)Convex SDP (optimal)05101520Time (sec)0.40.60.8Training accuracyBackpropagation (GD)Convex SDP (optimal)05101520Time (sec)0.40.50.60.7Test accuracyBackpropagation (GD)Convex SDP (optimal)02040Time (sec)50100150200Training costBackpropagation (GD)Convex SDP (optimal)02040Time (sec)204060Test costBackpropagation (GD)Convex SDP (optimal)02040Time (sec)0.50.60.70.8Training accuracyBackpropagation (GD)Convex SDP (optimal)02040Time (sec)0.60.70.80.9Test accuracyBackpropagation (GD)Convex SDP (optimal)050100Time (sec)102103Training costBackpropagation (GD)Convex SDP (optimal)050100Time (sec)102103Test costBackpropagation (GD)Convex SDP (optimal)050100Time (sec)0.20.40.60.8Training accuracyBackpropagation (GD)Convex SDP (optimal)050100Time (sec)0.20.40.60.8Test accuracyBackpropagation (GD)Convex SDP (optimal)050100Time (sec)1022×1023×1024×102Training costBackpropagation (GD)Convex SDP (optimal)050100Time (sec)1024×1016×101Test costBackpropagation (GD)Convex SDP (optimal)050100Time (sec)0.20.40.60.8Training accuracyBackpropagation (GD)Convex SDP (optimal)050100Time (sec)0.40.60.8Test accuracyBackpropagation (GD)Convex SDP (optimal)dataset
acute-inﬂammation
acute-nephritis
breast-cancer
breast-cancer-wisc-diag
breast-cancer-wisc-prog
congressional-voting
conn-bench-sonar-mines-rocks
cylinder-bands
echocardiogram
fertility
haberman-survival
heart-hungarian
hepatitis
horse-colic
ilpd-indian-liver
molec-biol-promoter
monks-1
parkinsons
pittsburg-bridges-T-OR-D
planning
spect
spectf
statlog-heart
vertebral-column-2clases

n
120
120
286
569
198
435
208
512
131
100
306
294
155
368
583
106
556
195
102
182
265
267
270
310

d
6
6
9
30
33
16
60
35
10
9
3
12
19
25
9
57
6
22
7
12
22
44
13
6

R-Q
100.0
100.0
69.37
79.05
80.1
61.47
79.81
75.59
84.09
89.0
73.03
83.56
80.13
81.67
73.63
77.88
84.68
90.82
88.0
71.67
60.0
72.5
82.46
87.01

P-C
100.0
100.0
73.59
95.95
79.08
61.47
79.33
75.2
83.33
86.0
73.68
83.9
89.1
81.0
72.95
78.85
70.16
87.24
88.0
71.11
75.0
75.0
85.07
85.71

Cvx 111 Cvx r-app Cvx s-app max(Cvx)
100.0
100.0
73.59
95.42
77.55
61.47
81.73
75.59
85.61
88.0
71.38
83.22
80.13
81.67
71.92
72.12
75.81
88.27
82.0
71.67
71.25
58.75
81.72
82.79

100.0
100.0
73.59
96.13
79.59
61.7
81.73
76.95
85.61
88.0
73.36
84.25
80.13
84.0
73.12
82.69
81.45
91.33
87.0
71.67
71.25
77.5
83.58
87.01

100.0
100.0
72.89
96.13
79.59
61.7
79.81
76.95
85.61
88.0
73.36
84.25
77.56
80.33
73.12
82.69
81.45
86.73
87.0
71.11
60.0
60.0
83.58
87.01

100.0
100.0
72.89
96.13
77.55
61.47
75.0
76.37
84.09
88.0
72.04
84.25
80.13
84.0
72.95
78.85
81.45
91.33
87.0
71.11
58.75
77.5
83.21
84.42

Table 2: Classiﬁcation accuracies on binary classiﬁcation UCI datasets. The ﬁrst 3 columns are the
dataset name, the number of samples n in the dataset, and the dimension d of the samples. The
remaining columns show the classiﬁcation accuracies (percentage) for various models. The highest
accuracies for each dataset are shown in bold font. Abbreviations used in the table are as follows:
R-Q: Non-convex two-layer neural network model with ReLU activation and quadratic regularization
(i.e. weight decay), P-C: Non-convex two-layer neural network model with polynomial activation with
coeﬃcients a = 0.09, b = 0.5, c = 0.47 and normalized ﬁrst layer weights and (cid:96)1 norm regularization
on the second layer weights, Cvx 111: Convex SDP with polynomial coeﬃcients a = 1, b = 1, c = 1,
Cvx r-app: Convex SDP with polynomial coeﬃcients a = 0.09, b = 0.5, c = 0.47 (approximating
ReLU activation), Cvx s-app: Convex SDP with polynomial coeﬃcients a = 0.1, b = 0.5, c = 0.24
(approximating swish activation), max(Cvx): The highest accuracy among the convex SDPs.

34

10.3 Comparison with ReLU Networks

We compare the classiﬁcation accuracies for polynomial activation and ReLU activation in Figure 10
on three diﬀerent binary classiﬁcation UCI datasets. The regularization coeﬃcient has been picked
separately for polynomial activation and ReLU activation networks to maximize the accuracy. Figure
10 demonstrates that the convex SDP shows competitive accuracy performance and faster run times
compared to ReLU activation networks.

10.4 CNN Experiments

Figure 11 shows the binary classiﬁcation accuracy performance of the CNN architecture with global
average pooling on MNIST [28], Fashion MNIST [46], and Cifar-10 [25] datasets. Figure 11 compares
the non-convex tractable problem, the corresponding convex formulation, and the non-convex weight
decay formulation. By the weight decay formulation, we mean quadratic regularization on both the
ﬁrst layer ﬁlters and the second layer weights. We observe that the accuracy of the convex SDP is
slightly better or the same as SGD while the run time for the convex SDP solution is consistently
shorter than the time it takes for SGD to converge.

10.5 Regularization Parameter

Figure 12 shows how the accuracy changes as a function of the regularization coeﬃcient β for the
convex problem for two-layer polynomial activation networks. Figure 12 highlights that the choice of
the regularization coeﬃcient is critical in the accuracy performance. In plot a, we see that the value
of β that maximizes the test set accuracy is β = 10 for which the optimal number of neurons m∗ is
near 20. We note that for the dataset in plot a, the optimal number of neurons is upper bounded by
m∗ ≤ 2(d + 1) = 32. Similarly for plot b, the best choice for the regularization coeﬃcient is β = 1
and the optimal number of neurons for β = 1 is near 40. Furthermore, we observe that a higher
value for β tends to translate to a lower optimal number of neurons m∗ (plotted on the right vertical
axis). Even though the convex optimization problem in (21) has a ﬁxed number of variables (in this
case, 2(d + 1)2) for a given dataset, a low number of neurons is still preferable for many reasons such
as inference speed. We observe that the number of neurons can be controlled via the regularization
coeﬃcient β.

10.6 Other Losses

We have so far evaluated the performance of the derived convex programs for squared loss, i.e. (cid:96)(ˆy, y) =
(cid:107)ˆy − y(cid:107)2
2. We reiterate that the derived convex programs are general in the sense the formulations
hold for any convex loss function (cid:96). To verify this numerically, we now present results for additional
loss functions such as Huber loss and (cid:96)1 norm loss in Figure 13. More concretely, Huber loss is deﬁned
as (cid:96)(ˆy, y) = (cid:80)n
i=1 Huber(ˆyi − yi) where Huber(x) = 2|x| − 1 for |x| > 1 and Huber(x) = x2 for |x| ≤ 1.
The (cid:96)1 norm loss is (cid:96)(ˆy, y) = (cid:107)ˆy − y(cid:107)1. We observe that in the case of (cid:96)1 norm loss, backpropagation
takes longer to converge.

10.7 The Eﬀect of Polynomial Coeﬃcients

The plots in Figure 14 show the classiﬁcation accuracy against the polynomial coeﬃcients a, b, c for the
polynomial activation convex problem. In each plot, we vary one of the coeﬃcients and ﬁx the other
two coeﬃcients as 1. We observe that the coeﬃcient of the quadratic term a plays the most important
role in the accuracy performance. The accuracy is not aﬀected by the choice of the coeﬃcient c.

35

(a) DS1, training accuracy

(b) DS1, test accuracy

(c) DS2, training accuracy

(d) DS2, test accuracy

(e) DS3, training accuracy

(f) DS3, test accuracy

Figure 10: Comparison of classiﬁcation accuracies for neural networks with ReLU activation, poly-
nomial activation (a = 0.09, b = 0.5, c = 0.47), and the convex SDP. DS1: dataset 1 is the
oocytes-merluccius-nucleus-4d (n = 817, d = 41), DS2: dataset 2 is the credit approval dataset
(n = 552, d = 15), DS3: dataset 3 is the breast cancer dataset (n = 228, d = 9).

36

0.02.55.07.510.012.5Time (sec)0.40.60.8Training accuracyBackprop - ReLUBackprop - Poly actConvex SDP (optimal)0.02.55.07.510.012.5Time (sec)0.40.60.8Test accuracyBackprop - ReLUBackprop - Poly actConvex SDP (optimal)02468Time (sec)0.50.60.70.80.9Training accuracyBackprop - ReLUBackprop - Poly actConvex SDP (optimal)02468Time (sec)0.50.60.70.80.9Test accuracyBackprop - ReLUBackprop - Poly actConvex SDP (optimal)012345Time (sec)0.650.700.750.80Training accuracyBackprop - ReLUBackprop - Poly actConvex SDP (optimal)012345Time (sec)0.600.650.700.75Test accuracyBackprop - ReLUBackprop - Poly actConvex SDP (optimal)(a) MNIST, training accuracy

(b) MNIST, test accuracy

(c) Fashion-MNIST, training accuracy

(d) Fashion-MNIST, test accuracy

(e) Cifar, training accuracy

(f) Cifar, test accuracy

Figure 11: Binary classiﬁcation with polynomial activation convolutional neural network with pooling
and the corresponding convex SDP. Legend labels are as follows. SGD - tractable: The non-convex
problem in (89), SGD - weight decay: Non-convex problem with quadratic regularization on all weights,
Convex SDP (optimal): The convex problem in (95). Polynomial coeﬃcients are a = 0.09, b = 0.5, c =
0.47. Filter size is f = 3, stride is 1, and no padding is used. Batch size for SGD is 100. The
regularization coeﬃcient is β = 10−6. Constrained least squares form of the convex program was used
for speed (see section A.1) and the pre-computation step, not shown in the plots, takes 2 minutes.
Plots a, b show the binary classiﬁcation accuracy on the ﬁrst two classes of the MNIST dataset where
the classes are the digits 0 and 1 and there are 12600 gray-scale images of size 28 × 28. Plots c, d
show the binary classiﬁcation accuracy on the ﬁrst two classes of Fashion-MNIST dataset where the
classes are ’T-shirt/top’ and ’Trouser’ and there are 12000 gray-scale images of size 28 × 28. For plots
e, f, the dataset is Cifar-2 (the ﬁrst two classes of the Cifar-10 dataset) and has 10000 RGB images
each of size 32 × 32 × 3.

37

0100200300Time (sec)0.800.850.900.951.00Training accuracySGD - tractableSGD - weight decayConvex SDP (optimal)0100200300Time (sec)0.50.60.70.80.91.0Test accuracySGD - tractableSGD - weight decayConvex SDP (optimal)0200400600Time (sec)0.70.80.9Training accuracySGD - tractableSGD - weight decayConvex SDP (optimal)0200400600Time (sec)0.50.60.70.80.91.0Test accuracySGD - tractableSGD - weight decayConvex SDP (optimal)010002000300040005000Time (sec)0.650.700.750.80Training accuracySGD - tractableSGD - weight decayConvex SDP (optimal)010002000300040005000Time (sec)0.50.60.70.8Test accuracySGD - tractableSGD - weight decayConvex SDP (optimal)(a) credit approval (n = 552, d = 15)

(b) ionosphere (n = 280, d = 33)

Figure 12: Accuracy (left vertical axis) and optimal number of neurons (right vertical axis) against
the regularization coeﬃcient β on binary classiﬁcation datasets. These results have been obtained
using the convex program in (21).

(a) Huber loss

(b) (cid:96)1 norm loss

Figure 13: Verifying the theoretical results for other convex loss functions: Huber and (cid:96)1 norm loss. An
artiﬁcially generated dataset with dimensions n = 100, d = 20 is used. The regularization coeﬃcient
is β = 0.1. The number of neurons m∗ is found to be 7 and 9 for plots a and b, respectively.

(a) b = c = 1

(b) a = c = 1

(c) a = b = 1

Figure 14: Training and test set classiﬁcation accuracies against polynomial coeﬃcients a, b, c. The
regularization coeﬃcient is β = 0.1 and the dataset is oocytes-merluccius-nucleus-4d.

38

10−3100103beta0.60.8AccuracyTraining setTest set202530Number of neurons10−410−2100102104beta0.40.60.81.0AccuracyTraining setTest set30405060Number of neurons0510Time (sec)101102Training costBackpropagation (GD)Convex SDP (optimal)050100150Time (sec)101102Training costBackpropagation (GD)Convex SDP (optimal)10−310−210−1100101a0.800.850.900.951.00AccuracyTraining setTest set10−310−210−1100101b0.800.850.900.95AccuracyTraining setTest set10−310−210−1100101c0.850.900.95AccuracyTraining setTest set11 Discussion

In this paper, we have studied the optimization of two-layer neural networks with degree two polyno-
mial activations. We have shown that regularization plays an important role in the tractability of the
problems associated with neural network training. We have developed convex programs for the cases
where the regularization leads to tractable formulations. Convex formulations are useful since they
have many well-known advantages over non-convex optimization such as having to optimize fewer
hyperparameters and no risk of getting stuck at local minima.

The methods presented in this work optimize the neural network parameters in a higher dimen-
sional space in which the problem becomes convex. For fully connected neural networks with quadratic
activation, the standard non-convex problem requires optimizing m neurons (i.e. a d-dimensional ﬁrst
layer weight and a 1-dimensional second layer weight per neuron). The convex program for this neural
network ﬁnds the optimal network parameters in the lifted space Sd×d. For polynomial activations,
convex optimization takes place for Z and Z (cid:48) in S(d+1)×(d+1). We note that the dimensions of the
convex programs are polynomial with respect to all problem dimensions.
In contrast, the convex
program of [37] has 2dP variables where P grows exponentially with respect to the rank of the data
matrix.

We have used the SCS solver with CVXPY for solving the convex problems in the numerical
experiments. It is important to note that there is room for future work in terms of which solvers to
use. Solvers speciﬁcally designed for the presented convex programs could enjoy faster run times.

The scope of this work is limited to two-layer neural networks. We note that it is a promising
direction to consider the use of our convex programs for two-layer neural networks as building blocks
in learning deep neural networks. Many recent works such as [2] and [4] investigate layerwise learning
algorithms for deep neural networks. The training of individual layers in layerwise learning could be
improved by the presented convex programs since the convex programs can be eﬃciently solved and
eliminate much of the hyperparameter tuning involved in standard neural network training.

Acknowledgements

This work was partially supported by the National Science Foundation under grants IIS-1838179,
ECCS-2037304, Facebook Research, Adobe Research and Stanford SystemX Alliance.

References

[1] Akshay Agrawal, Robin Verschueren, Steven Diamond, and Stephen Boyd. A rewriting system

for convex optimization problems. Journal of Control and Decision, 5(1):42–60, 2018.

[2] Zeyuan Allen-Zhu and Yuanzhi Li. Backward feature correction: How deep learning performs

deep learning. arXiv preprint arXiv:2001.04413, 2020.

[3] Raman Arora, Amitabh Basu, Poorya Mianjy, and Anirbit Mukherjee. Understanding deep neural
networks with rectiﬁed linear units. In 6th International Conference on Learning Representations,
ICLR 2018, 2018.

[4] Eugene Belilovsky, Michael Eickenberg, and Edouard Oyallon. Greedy layerwise learning can

scale to imagenet. CoRR, abs/1812.11446, 2018.

[5] Daniel Bienstock, Gonzalo Mu˜noz, and Sebastian Pokutta. Principled deep neural network train-

ing through linear programming, 2018.

[6] Mathieu Blondel, Akinori Fujino, and Naonori Ueda. Convex factorization machines. Euro-
pean Conference on Machine Learning and Principles and Practice of Knowledge Discovery in
Databases (ECML PKDD), 2015.

39

[7] Mathieu Blondel, Vlad Niculae, Takuma Otsuka, and Naonori Ueda. Multi-output polynomial
networks and factorization machines.
In Proceedings of the 31st International Conference on
Neural Information Processing Systems, NIPS’17, page 3351–3361, Red Hook, NY, USA, 2017.
Curran Associates Inc.

[8] Stephen Boyd and Lieven Vandenberghe. Convex optimization. Cambridge university press, 2004.

[9] Samuel Burer. Copositive programming.

In Handbook on semideﬁnite, conic and polynomial

optimization, pages 201–218. Springer, 2012.

[10] Lenaic Chizat, Edouard Oyallon, and Francis Bach. On lazy training in diﬀerentiable program-

ming. In Advances in Neural Information Processing Systems, pages 2937–2947, 2019.

[11] Steven Diamond and Stephen Boyd. CVXPY: A Python-embedded modeling language for convex

optimization. Journal of Machine Learning Research, 17(83):1–5, 2016.

[12] Simon Du and Jason Lee. On the power of over-parametrization in neural networks with quadratic
activation. In Jennifer Dy and Andreas Krause, editors, Proceedings of the 35th International
Conference on Machine Learning, volume 80 of Proceedings of Machine Learning Research, pages
1329–1338, Stockholmsm¨assan, Stockholm Sweden, 10–15 Jul 2018. PMLR.

[13] Dheeru Dua and Casey Graﬀ. UCI machine learning repository, 2017.

[14] Tolga Ergen and Mert Pilanci. Convex geometry of two-layer relu networks: Implicit autoencoding
and interpretable models. In International Conference on Artiﬁcial Intelligence and Statistics,
pages 4024–4033. PMLR, 2020.

[15] Tolga Ergen and Mert Pilanci. Implicit convex regularizers of cnn architectures: Convex opti-
mization of two- and three-layer networks in polynomial time. arXiv preprint arXiv:2006.14798,
2020.

[16] Tolga Ergen and Mert Pilanci. Revealing the structure of deep neural networks via convex duality.

arXiv preprint arXiv:2002.09773, 2020.

[17] Manuel Fern´andez-Delgado, Eva Cernadas, Sen´en Barro, and Dinani Amorim. Do we need
hundreds of classiﬁers to solve real world classiﬁcation problems? Journal of Machine Learning
Research, 15(90):3133–3181, 2014.

[18] Matthew Fickus, Dustin G. Mixon, Aaron A. Nelson, and Yang Wang. Phase retrieval from very

few measurements. arXiv preprint arXiv:1307.7176, 2013.

[19] David Gamarnik, Eren C. Kızılda˘g, and Ilias Zadik. Stationary points of shallow neural networks

with quadratic activation function. arXiv preprint arXiv:1912.01599, 2020.

[20] Surbhi Goel, Varun Kanade, Adam Klivans, and Justin Thaler. Reliably learning the relu in
polynomial time. In Satyen Kale and Ohad Shamir, editors, Proceedings of the 2017 Conference
on Learning Theory, volume 65 of Proceedings of Machine Learning Research, pages 1004–1042,
Amsterdam, Netherlands, 07–10 Jul 2017. PMLR.

[21] M. X. Goemans and D. P. Williamson. Improved approximation algorithms for maximum cut
and satisﬁability problems using semideﬁnite programming. Journal of the ACM, 42:1115–1145,
1995.

[22] Johan H˚astad. Some optimal inapproximability results. Journal of the ACM (JACM), 48(4):798–

859, 2001.

40

[23] Arthur Jacot, Franck Gabriel, and Cl´ement Hongler. Neural tangent kernel: Convergence and
generalization in neural networks. In Advances in neural information processing systems, pages
8571–8580, 2018.

[24] Subhash Khot, Guy Kindler, Elchanan Mossel, and Ryan O’Donnell. Optimal inapproximability
results for max-cut and other 2-variable csps? SIAM Journal on Computing, 37(1):319–357,
2007.

[25] Alex Krizhevsky. Learning multiple layers of features from tiny images, 2009.

[26] Jonathan Lacotte and Mert Pilanci. All local minima are global for two-layer relu neural networks:

The hidden convex optimization landscape. arXiv preprint arXiv:2006.05900, 2020.

[27] Monique Laurent and Svatopluk Poljak. On a positive semideﬁnite relaxation of the cut polytope.

Linear Algebra and its Applications, 223(224):439–461, 1995.

[28] Yann LeCun, Corinna Cortes, and CJ Burges. Mnist handwritten digit database. ATT Labs

[Online]. Available: http://yann.lecun.com/exdb/mnist, 2, 2010.

[29] Johannes Lederer. No spurious local minima: on the optimization landscapes of wide and deep

neural networks, 2020.

[30] Roi Livni, Shai Shalev-Shwartz, and Ohad Shamir. On the computational eﬃciency of training

neural networks. NIPS’14, page 855–863, 2014.

[31] Stefano Sarao Mannelli, Eric Vanden-Eijnden, and Lenka Zdeborov´a. Optimization and gen-
arXiv preprint

eralization of shallow neural networks with quadratic activation functions.
arXiv:2006.15459, 2020.

[32] Gregory L Naber. The geometry of Minkowski spacetime: An introduction to the mathematics of

the special theory of relativity, volume 92. Springer Science & Business Media, 2012.

[33] Yuri Nesterov, Henry Wolkowicz, and Yinyu Ye. Semideﬁnite programming relaxations of noncon-
vex quadratic optimization. In Handbook of semideﬁnite programming, pages 361–419. Springer,
2000.

[34] B. O’Donoghue, E. Chu, N. Parikh, and S. Boyd. Conic optimization via operator splitting and ho-
mogeneous self-dual embedding. Journal of Optimization Theory and Applications, 169(3):1042–
1068, June 2016.

[35] B. O’Donoghue, E. Chu, N. Parikh, and S. Boyd. SCS: Splitting conic solver, version 2.1.2.

https://github.com/cvxgrp/scs, November 2019.

[36] Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan,
Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, Alban Desmaison, Andreas Kopf,
Edward Yang, Zachary DeVito, Martin Raison, Alykhan Tejani, Sasank Chilamkurthy, Benoit
Steiner, Lu Fang, Junjie Bai, and Soumith Chintala. Pytorch: An imperative style, high-
performance deep learning library. Advances in Neural Information Processing Systems 32, pages
8024–8035, 2019.

[37] Mert Pilanci and Tolga Ergen. Neural networks are convex regularizers: Exact polynomial-
time convex optimization formulations for two-layer networks. Proceedings of the International
Conference on Machine Learning (ICML 2020), 2020.

[38] Imre P´olik and Tam´as Terlaky. A survey of the s-lemma. SIAM Review, 49(3):371–418, 2007.

[39] Prajit Ramachandran, Barret Zoph, and Quoc Le. Searching for activation functions. arXiv

preprint arXiv:1710.05941, 2018.

41

[40] Arda Sahiner, Tolga Ergen, John Pauly, and Mert Pilanci. Vector-output relu neural network
problems are copositive programs: Convex analysis of two layer networks and polynomial-time
algorithms. arXiv preprint arXiv:2012.13329, 2020.

[41] Arda Sahiner, Morteza Mardani, Batu Ozturkler, Mert Pilanci, and John Pauly. Convex regu-

larization behind neural reconstruction. arXiv preprint arXiv:2012.05169, 2020.

[42] M. Soltani and C. Hegde. Fast and provable algorithms for learning two-layer polynomial neural

networks. IEEE Transactions on Signal Processing, 67(13):3361–3371, 2019.

[43] Mahdi Soltanolkotabi, Adel Javanmard, and Jason D. Lee. Theoretical insights into the opti-
IEEE Trans. Inf. Theor.,

mization landscape of over-parameterized shallow neural networks.
65(2):742–769, February 2019.

[44] Luca Trevisan, Gregory B Sorkin, Madhu Sudan, and David P Williamson. Gadgets, approxi-

mation, and linear programming. SIAM Journal on Computing, 29(6):2074–2097, 2000.

[45] Henry Wolkowicz, Romesh Saigal, and Lieven Vandenberghe. Handbook of semideﬁnite program-
ming: theory, algorithms, and applications, volume 27. Springer Science & Business Media, 2012.

[46] Han Xiao, Kashif Rasul, and Roland Vollgraf. Fashion-mnist: a novel image dataset for bench-

marking machine learning algorithms, 2017.

42

A Additional Discussion

A.1 Constrained Least Squares Form for the Squared Loss

Let us consider the polynomial activation scalar output case. In the case of squared loss (cid:96)(ˆy, y) =
(cid:107)ˆy − y(cid:107)2

2, the convex program takes the following form:

min
Z=ZT ,Z(cid:48)=Z(cid:48) T

s.t.

n
(cid:88)

(cid:0)axT

i (Z1 − Z (cid:48)

1)xi + bxT

i (Z2 − Z (cid:48)

2) + c(Z4 − Z (cid:48)

4) − yi

i=1
tr(Z1) = Z4, tr(Z (cid:48)
Z (cid:23) 0, Z (cid:48) (cid:23) 0 .

1) = Z (cid:48)
4

(cid:1)2

+ β(Z4 + Z (cid:48)
4)

(98)

Noting that axT

i (Z1 − Z (cid:48)

1)xi = vec(xixT

i )T vec(Z1 − Z (cid:48)

1), we can write the squared loss term as



(cid:2)a vec(xixT

i )T



n
(cid:88)

i=1

=

(cid:13)
(cid:13)
(cid:13)

1 )T






a vec(x1xT
...
a vec(xnxT

n )T

bxT
i

bxT
1

bxT
n

c(cid:3)



c




c









vec(Z1 − Z (cid:48)
1)
Z2 − Z (cid:48)
2
Z4 − Z (cid:48)
4

vec(Z1 − Z (cid:48)
1)
Z2 − Z (cid:48)
2
Z4 − Z (cid:48)
4






2



=

 − yi

 − y

(cid:13)
2
(cid:13)
(cid:13)
2

= (cid:107)XV z − y(cid:107)2
2

V XV z − 2yT XV z + (cid:107)y(cid:107)2
2.
If we pre-compute X T

where we have deﬁned XV ∈ Rn×(d2+d+1) and z ∈ R(d2+d+1). The squared loss term is equal to
zT X T

longer has dependence on the number of samples n. We note that the pre-computation of X T
X T

V y is useful when one is performing hyperparameter tuning for the regularization coeﬃcient β.

V y ∈ R(d2+d+1), then the objective no
V XV and

V XV ∈ R(d2+d+1)×(d2+d+1) and X T

B Proofs

Proof of Lemma 2.1. We will denote the set in (17) as S1 and the set in (18) as S2 to simplify the
notation. We will prove S1 = S2 by showing S1 ⊆ S2 and S2 ⊆ S1.
We ﬁrst show S1 ⊆ S2. Let us take a point S ∈ S1. This implies that S is a matrix of the form

t

m
(cid:88)

j=1

(cid:20) uj
1

(cid:21) (cid:20) uj
1

(cid:21)T

αj = t

m
(cid:88)

j=1

(cid:20)ujuT
j αj ujαj
uT
αj
j αj

(cid:21)

=

(cid:20)t (cid:80)m
t (cid:80)m

j=1 ujuT
j αj
j=1 uT
j αj

(cid:21)

t (cid:80)m
t (cid:80)m

j=1 ujαj
j=1 αj

(99)

with (cid:80)
t (cid:80)m

j=1 tr(uT

j αj ≤ 1 and (cid:107)uj(cid:107)2 = 1 for all j. We note that tr(t (cid:80)m

j αj) =
j=1 αj ≤ t. This shows that S satisﬁes the equality condition in the deﬁnition
(cid:21)T

j=1 tr(ujuT

j=1 ujuT

j uj)αj = t (cid:80)m

j αj) = t (cid:80)m

(18). Now, we show that S is a PSD matrix. Note that each of the rank-1 matrices

(cid:20) uj
1

(cid:21) (cid:20) uj
1

is a PSD matrix and since the coeﬃcients αj’s and t are nonnegative, it follows that S is PSD. This
proves that S ∈ S2.
We next show S2 ⊆ S1. Let us take a point S ∈ S2. This implies that S is PSD and tr(S1) =
S4 = t0 ≤ t. We show in Section 4 that it is possible to decompose S via the neural decomposition
procedure to obtain the expressions given in (36). It follows that we can write S in the following form

S =

t0
(cid:80)m
j=1 d2
j

(cid:20)(cid:80)m
j d2
j=1 ujuT
j
(cid:80)m
j d2
j=1 uT
j

(cid:21)

(cid:80)m
j=1 ujd2
j
(cid:80)m
j=1 d2
j

,

(100)

43

t0
(cid:80)m
j=1 d2
j

where the scaling factor

is to ensure that tr(S1) = S4 = t0 ≤ t. It is obvious to see that S is in
S1 when t0 = t by the deﬁnition of S1 given in (17). When t0 < t, we still have that S is in S1 which
can be seen by noting that S1 is deﬁned as the convex hull of rank-1 matrices and the zero matrix.
We can scale all the rank-1 matrices in the convex combination with t0
t and change the weight of the
zero matrix accordingly.

Proof of Lemma 6.2. Let α1, . . . , αm be any feasible point. First, note that for any s ≥ 0 and α ∈
R, α (cid:54)= 0, we have

(cid:0)s |α|(cid:1)p

≥ s |α|p ,

(101)

where equality holds if and only if s ∈ {0, 1}. The equality condition follows since |α| > 0 and sp = s
implies s ∈ {0, 1} for p ∈ (0, 1). Then, deﬁne si := |αi|
i si = 1, and observe that

(cid:80)

(cid:88)

|αi|p =

(cid:88)

(cid:12)
(cid:12)si

j |αj | , which satisﬁes (cid:80)
(cid:0) (cid:88)

p

|αj|(cid:1)(cid:12)
(cid:12)

i

j
(cid:17)(cid:16) (cid:88)

si

(cid:17)p

|αj|

j

(cid:17)p

|αi|

(cid:17)p

αi

i
(cid:16) (cid:88)

i
(cid:16) (cid:88)

i
(cid:16) (cid:88)

≥

=

≥

i

= 1 ,

where the ﬁrst inequality holds with equality if and only if si ∈ {0, 1}, ∀i. Hence, in order for the
equality to hold, we necessarily have (cid:107)α(cid:107)0 ≤ 1. Since (cid:80)
i αi = 1, the all-zeros vector is infeasible.
This implies that (cid:107)α(cid:107)0 = 1. Finally, note that all feasible vectors which are 1-sparse are of the
form (1, 0, . . . , 0), (0, 1, 0, . . . , 0), . . . , (0, . . . , 1) and achieve an objective value 1. We conclude that all
feasible vectors with cardinality strictly greater than 1 are suboptimal since they achieve objective
value strictly larger than 1.

Proof of Lemma 6.3. Let us deﬁne the set A = {a1, a2, . . . , ad} where ai are integers. We need to
show that the problem (62) ﬁnds a feasible solution u1 if and only if there exists a subset AS of the
set A that satisﬁes (cid:80)

a = z.

We assume n = 2d + 1 and hence ˜X is (d + 1) × d and ˜y is (d + 1) dimensional. Let ˜XD ∈ Rd×d

denote the matrix with the ﬁrst d rows of ˜X, and ˜xd+1 is the last sample in ˜X. Let us deﬁne ˜yi as

a∈AS

˜yi =

(cid:40)

(ai/wi)2,
(2z − (cid:80)d

j=1 aj)2,

i = 1, . . . , d
i = d + 1 ,

(102)

where w = ˜X −T

D ˜xd+1 ∈ Rd.

Direction 1: Suppose there exists u1 ∈ Rd such that (˜xT

1k = 1/d for every k = 1, . . . , d. Then there exists a subset AS with (cid:80)
u2

a∈AS
Proof of direction 1: Assuming ˜XD is invertible, it follows that ˜X ˜X −1

i u1)2 = ˜yi for every i = 1, . . . , d + 1 and
a = z.
D = (cid:2)Id w(cid:3)T
the d × d identity matrix. Let us consider a feasible u1. Then, v = ˜XDu1 satisﬁes (( ˜X −T
for i = 1, . . . , d + 1. Consequently, we have ˜X −T
result, we obtain the following relation between v and ˜y:

where Id is
D ˜xi)T v)2 = ˜yi
D ˜xd+1 = w. As a

D ˜xi = ei for i = 1, . . . , d, and ˜X −T

˜yi =

(cid:40)

v2
i ,
(wT v)2,

i = 1, . . . , d
i = d + 1 .

44

Next, because of (102), we have

|vi| =

(cid:12)
(cid:12)
(cid:12)
(cid:12)

ai
wi

(cid:12)
(cid:12)
(cid:12)
(cid:12)

, i = 1, . . . , d,

and |wT v| =

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

2z −

d
(cid:88)

j=1

aj

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

.

Let us deﬁne εi such that vi = εiai/wi for i = 1, . . . , d. Note that εi ∈ {−1, 1}. Then,

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

2z −

d
(cid:88)

j=1

aj

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

= |wT v| =

(cid:12)
d
(cid:12)
(cid:88)
(cid:12)
(cid:12)
(cid:12)

i=1

εiai

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

=

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

2

(cid:88)

ai −

i:εi=1

d
(cid:88)

i=1

ai

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

=

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

.

(cid:88)

ai −

(cid:88)

i:εi=1

i:εi=−1

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

ai

=

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:88)

ai −

i:εi=1

d
(cid:88)

i=1

ai +

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

ai

(cid:88)

i:εi=1

i:εi=1 ai or z = − (cid:80)

This means we either have z = (cid:80)
i:εi=−1 ai. This shows
that the sum of the elements of AS is equal to z when AS is either equal to {ai|εi = 1} or {ai|εi = −1}.
In proving direction 2, it is straightforward to show the existence of u1 that satisﬁes the constraint
d , we pick ˜X in a certain
i u1)2 = ˜yi. To show that there is a u1 that satisﬁes the constraint u2
D v| = ¯1 1√
is satisﬁed,

(˜xT
way that we discuss now: To prove direction 2, we will need to make sure | ˜X −1
i.e.,

i:εi=1 ai + (cid:80)d

i=1 ai = (cid:80)

1k = 1

d

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

d
(cid:88)

( ˜X −1

D )i,jεj

j=1

aj
wj

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

=

1
√
d

for

i = 1, . . . , d .

We pick ˜XD to be any diagonal matrix with arbitrary −1’s and +1’s on the diagonal and pick
d|ai| for i = 1, . . . , d.

D ˜xd+1, we will have |wi| = |˜xd+1,i| =

d (cid:2)a1

. . .

√

√

(cid:3)T
ad
˜xd+1 =
This choice for ˜XD and ˜xd+1 ensures that | ˜X −1

.
Direction 2: Suppose there is a subset AS with (cid:80)

. Since w = ˜X −T

D v| = ¯1 1√

d

a = z. Then there exists a feasible

a∈AS

u1 ∈ Rd.

Proof of direction 2: Deﬁne εi such that for ai in AS, it is equal to 1, and otherwise it is equal

to −1. Next,

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

d
(cid:88)

i=1

εiai

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

=

=

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:88)

εiai +

(cid:88)

εiai

i:εi=1

i:εi=−1

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

=

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:88)

ai −

(cid:88)

i:εi=1

i:εi=−1

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

ai

=

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

2

(cid:88)

ai −

i:εi=1

d
(cid:88)

i=1

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

ai

2z −

d
(cid:88)

i=1

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

.

ai

(103)

Let us take vi = εiai/wi for i = 1, . . . , d. Now we show that the point deﬁned by ˜X −1
point. First, we check if ˜X −1
D v satisﬁes the constraints (˜xT
i

D v is a feasible
D v)2 = ˜yi for i = 1, . . . , i + 1. Note that

˜X −1

(˜xT
i

(˜xT

d+1

˜X −1

D v)2 = (eT

i v)2 = v2

i =

˜X −1

D v)2 = (wT v)2 = (

d
(cid:88)

j=1

a2
i
w2
i

= ˜yi

for

i = 1, . . . , d,

and

εjaj)2 = (2z −

d
(cid:88)

j=1

aj)2 = ˜yi+1 ,

where the last two equalities follow from (103) and the deﬁnition in (102). This shows that the
constraints (˜xT
i

D v)2 = ˜yi for i = 1, . . . , d + 1 are satisﬁed by ˜X −1

˜X −1

We now check for the other constraint; i.e. does ˜X −1

D v)i| = | (cid:80)d
value is elementwise? This is true because |( ˜X −1
The second equality follows from how we picked ˜XD and ˜xd+1.

D v.
D v satisfy | ˜X −1

D v| = 1√
d
| = 1√
d

aj
wj

j=1( ˜X −1

D )i,jεj

¯1 where the absolute

for i = 1, . . . , d.

45

Proof of Corollary 3.3. Let us deﬁne the quadratic functions f1(u) = −uT Qu − bT u + β and f2(u) =
(cid:107)u(cid:107)2
2 − 1. We note that f2(u) is strictly convex and takes both negative and positive values. Then by
Lemma 3.2, we have that the system −uT Qu − bT u < −β (or uT Qu + bT u > β) and (cid:107)u(cid:107)2 = 1 is not
solvable if and only if there exists λ such that −uT Qu − bT u + β + λ((cid:107)u(cid:107)2

2 − 1) ≥ 0, ∀u.

Equivalently, we have max(cid:107)u(cid:107)2=1 uT Qu + bT u ≤ β if and only if there exists λ such that

uT (λI − Q)u − bT u + β − λ ≥ 0,

∀u.

(104)

We note that if we make the change of variable u ← u

c with c (cid:54)= 0, then (104) implies

1
c2 uT (λI − Q)u −

1
c

bT u + β − λ ≥ 0,

∀u, ∀c (cid:54)= 0

which is the same as

uT (λI − Q)u − cbT u + c2(β − λ) ≥ 0,

∀u, ∀c (cid:54)= 0.

We express this inequality in matrix form as follows

(cid:2)uT

c(cid:3)

(cid:20)λI − Q − 1
2 b
2 bT
β − λ

− 1

(cid:21) (cid:20)u
c

(cid:21)

≥ 0,

∀u, ∀c (cid:54)= 0.

(105)

For the matrix in (105) to be PSD, we ﬁrst need to show that (104) implies the inequality in (105)
for c = 0 as well. We note that (104) implies

uT
(cid:107)u(cid:107)2

(λI − Q)

u
(cid:107)u(cid:107)2

− bT u
(cid:107)u(cid:107)2
2

+

β − λ
(cid:107)u(cid:107)2
2

≥ 0,

∀u s.t. (cid:107)u(cid:107)2 (cid:54)= 0.

Next, taking the norm of u to inﬁnity, we have

lim
(cid:107)u(cid:107)2→∞

(cid:18) uT
(cid:107)u(cid:107)2

(λI − Q)

u
(cid:107)u(cid:107)2

− bT u
(cid:107)u(cid:107)2
2

+

β − λ
(cid:107)u(cid:107)2
2

(cid:19)

= uT

n (λI − Q)un,

where un = u/(cid:107)u(cid:107)2 is unit norm. We note that uT
n (λI − Q)un is non-negative for all unit norm un,
which is the same as the statement that it is non-negative for all un (not necessarily unit norm). This
shows that (104) implies uT (λI − Q)u ≥ 0 for all u, which, we note, is the same as (105) with c = 0.
c(cid:3)T
Hence, because the inequality holds for all (cid:2)uT

, we obtain the matrix inequality

(cid:20)λI − Q − 1
2 b
2 bT
β − λ

− 1

(cid:21)

(cid:23) 0.

(106)

The proof for the other direction of the if and only if statement is straightforward. We note that,
by the deﬁnition of a PSD matrix, (106) implies that uT (λI − Q)u − cbT u + c2(β − λ) ≥ 0, ∀u, c.
Setting c = 0, we obtain the inequality in (104).

C Additional Numerical Results

Figure 15 compares the costs and accuracy performance of the convex formulation with minibatch
SGD.

46

(a) DS1, training cost

(b) DS1, test cost

(c) DS1, training accuracy (d) DS1, test accuracy

(e) DS2, training cost

(f) DS2, test cost

(g) DS2, training accuracy (h) DS2, test accuracy

Figure 15: SGD with minibatch size 13 for two UCI datasets, DS1 is the breast-cancer-wisc-diag
dataset with n = 455, d = 30 and DS2 is parkinsons dataset with d = 156, d = 22. The regularization
coeﬃcient is set to β = 1 and β = 0.1 and the number of neurons m∗ is found as 34 and 27 for DS1
and DS2, respectively.

47

050100150Time (sec)1012×1013×1014×1016×101Training costBackpropagation (SGD)Convex SDP (optimal)050100150Time (sec)101Test costBackpropagation (SGD)Convex SDP (optimal)050100150Time (sec)0.800.850.900.95Training accuracyBackpropagation (SGD)Convex SDP (optimal)050100150Time (sec)0.70.80.91.0Test accuracyBackpropagation (SGD)Convex SDP (optimal)0204060Time (sec)101Training costBackpropagation (SGD)Convex SDP (optimal)0204060Time (sec)1014×1006×100Test costBackpropagation (SGD)Convex SDP (optimal)0204060Time (sec)0.60.70.80.91.0Training accuracyBackpropagation (SGD)Convex SDP (optimal)0204060Time (sec)0.50.60.70.8Test accuracyBackpropagation (SGD)Convex SDP (optimal)