Noname manuscript No.
(will be inserted by the editor)

Model-Free Adaptive Optimal Control of Episodic
Fixed-Horizon Manufacturing Processes using Reinforcement
Learning

Johannes Dornheim · Norbert Link · Peter Gumbsch

9
1
0
2

b
e
F
0
2

]

Y
S
.
s
c
[

3
v
6
4
6
6
0
.
9
0
8
1
:
v
i
X
r
a

Received: date / Accepted: date

Abstract A self-learning optimal control algorithm
for episodic ﬁxed-horizon manufacturing processes with
time-discrete control actions is proposed and evaluated
on a simulated deep drawing process. The control model
is built during consecutive process executions under op-
timal control via reinforcement learning, using the mea-
sured product quality as reward after each process ex-
ecution. Prior model formulation, which is required by
state-of-the-art algorithms from model predictive con-
trol and approximate dynamic programming, is there-
fore obsolete. This avoids several diﬃculties namely in
system identiﬁcation, accurate modelling, and runtime
complexity, that arise when dealing with processes sub-
ject to nonlinear dynamics and stochastic inﬂuences. In-
stead of using pre-created process and observation mod-
els, value function-based reinforcement learning algo-
rithms build functions of expected future reward, which
are used to derive optimal process control decisions.
The expectation functions are learned online, by inter-
acting with the process. The proposed algorithm takes
stochastic variations of the process conditions into ac-
count and is able to cope with partial observability. A
Q-learning-based method for adaptive optimal control
of partially observable episodic ﬁxed-horizon manufac-

Johannes Dornheim
Institute Intelligent Systems Research Group, Karlsruhe Uni-
versity of Applied Sciences
Moltkestr. 30, D-76133 Karlsruhe, Germany
Tel.: +49 721 925-2346
E-mail: johannes.dornheim@hs-karlsruhe.de

Norbert Link
Institute Intelligent Systems Research Group, Karlsruhe Uni-
versity of Applied Sciences

Peter Gumbsch
Institute for Applied Materials (IAM-CMS), Karlsruhe Insti-
tute of Technology

turing processes is developed and studied. The resulting
algorithm is instantiated and evaluated by applying it
to a simulated stochastic optimal control problem in
metal sheet deep drawing.

Keywords Adaptive Optimal Control · Model-
Free Optimal Control · Manufacturing Process
Optimization · Reinforcement Learning

1 Introduction

Series production of parts is a repetition of processes,
transforming each part from an initial state to some
desired end state. Each process is executed by a ﬁ-
nite sequence of – generally irreversible – processing
steps. Examples are processes involving plastic defor-
mation [19] and subtractive or additive manufacturing
processes [14]. The process optimization is formulated
as a Markov decision process with ﬁnite horizon. The
control strategy (policy) can be reﬁned between sub-
sequent process executions, which are called episodes
in reinforcement learning [29]. This allows the adapta-
tion of the policy to changing process conditions. The
proposed reinforcement learning approach does not re-
quire an explicit process model and is therefore highly
adaptive.

The episodic ﬁxed-horizon manufacturing processes
considered here can be regarded as nonlinear stochas-
tic processes. Each episode in the process consists of T
irreversible control steps. The example episodic ﬁxed-
horizon process used throughout this paper, deep draw-
ing, is depicted in ﬁg. 1. Based on the measured quality
of the process episode result, costs are assigned and
transferred to the agent by a reward signal RT at the
end of each execution. The control eﬀort can be re-
ﬂected by intermediate costs. The goal is to ﬁnd a con-

 
 
 
 
 
 
2

Johannes Dornheim et al.

Fig. 1 Deep drawing optimal control as episodic ﬁxed-horizon process. For every control step t, the optimal control task is to
determine the control-action ut that maximizes the expected episode reward RT , based on the observations [o0, ..., ot] in the
current episode and on data from previous episodes. In deep drawing, a planar metal sheet (blank) is pushed by a punch into
the hollow inner part of the die, causing the metal sheet to assume the desired form. The material ﬂow is regulated by blank
holders, which are pressing the blank against the outer part of the die. (source, left image: commons.wikimedia.org)

trol policy that minimizes the cost and thereby opti-
mizes the process performance regarding the resulting
product quality and the process eﬃciency. Processes
considered here are not fully observable. Current sensor
observations are not suﬃcient for deriving optimal con-
trol decisions, therefore historic information has to be
taken into account. In addition, no prior information,
(like reference trajectories, a process model or an obser-
vation model) is given and the optimal control has to
be learned during execution. Model-free adaptive algo-
rithms do not require a-priori models and can be used
if no accurate process model is available or the use of
the given process model for optimization is impractical
[28].

We use the optimal control of a deep drawing pro-
cess as application example and evaluation case. Be-
sides ﬁxed value parameters, like the initial blank shape
and thickness variation, time-variation of blank holder
forces are crucial for the resulting process quality. The
inﬂuence of space and time variation schemes of blank
holder forces on the process results is examined e.g.
in [30], [26] and [33]. The goal in the example applica-
tion is to ﬁnd optimal blank holder forces regarding the
process result and depending on the current, partially
observable, process condition.

For a given process model, observation model and
cost function, the optimal control problem can be solved
by model-based optimal control methods. Oﬄine
optimal control is reached by dynamic programming [4].
Such approaches are subject to the so-called curse of di-
mensionality in high-dimensional state spaces, leading
to diﬃculties in terms of sample size and computational
complexity. In the case of continuous (and thus inﬁnite)
state spaces, the optimal control solution by dynamic
programming requires discretization of the state space,
leading to suboptimal solutions. These problems are ad-

dressed in the ﬁeld of approximate dynamic program-
ming, combining dynamic programming with function
approximation [20].

A family of methods for online optimal control that
is often applied to industrial processes, is model predic-
tive control (MPC). An extensive overview of MPC,
and implementation examples of MPC for industrial
processes can be found in [6]. Qin et al. [21] provide
a survey of industrial grade MPC products. Like ap-
proximate dynamic programming, MPC requires a pro-
cess model, usually determined by linear system iden-
tiﬁcation methods. While MPC with linear models has
been explored extensively, nonlinear MPC is an active
ﬁeld of research [13]. The online computational costs
in MPC depend on the performance of the prediction
model and can be reduced by oﬄine pre-calculations as
in the explicit MPC method [2], or by using artiﬁcial
neural networks to approximate the prediction model
[23], [1].

The application of model-based optimal control meth-

ods is limited by the quality of the underlying process
model. The process identiﬁcation of highly nonlinear
processes from experimental data is very resource and
time-consuming. The experiments can alternatively be
simulated by using the ﬁnite element method (FEM)
with a nonlinear material model and can be used for
simulation-based process identiﬁcation. Published ex-
amples for the use of FEM models for model-based op-
timal control include [24] and [5]. In [24], various ap-
proximate dynamic programming methods are applied
to an optimal control problem of FEM simulated deep
drawing processes. In [5], methods from Nonlinear MPC
in combination with FEM calculations are used for op-
timal control of a glass forming process. However, ac-
curate simulation models of nonlinear manufacturing
processes are usually computationally demanding and

Title Suppressed Due to Excessive Length

3

are thus rarely used in recent work. Simulation eﬀorts
lead to high oﬄine costs when using approximate dy-
namic programming and extensive online costs when
using MPC. From a data-sampling point of view, the
use of a general process model (which may represent
possible process conditions like tool wear, ﬂuctuating
material properties or stochastic outer conditions and
disturbances) is intractable.

Due to the diﬃculties inherent in model-based op-
timal control, we pursue a adaptive model-free opti-
mal control method. Adaptive online learning of opti-
mal control policies without the need for a-priori knowl-
edge (in the form of process or observation models) is
accomplished by reinforcement learning (RL) [29] and
the closely related adaptive dynamic programming [31].
Unlike RL, adaptive dynamic programming is focused
on actor-critic methods for continuous optimal control.
Instead of solving the optimal control problem by
using an existing process model, model-free optimal
control methods optimize the control policy online,
while interacting with the process. Due to online learn-
ing, model-free methods for optimal control are inher-
ently adaptive, whereas, according to [12], approaches
for adaptive MPC focus on robustness and tend to be
conservative. The absence of a process model in RL also
results in online performance advantages over MPC and
in the ability to handle nonlinear systems. The horizon
of costs taken into account in RL is usually unbounded,
while in MPC it is bounded by the chosen prediction
horizon. One advantage of MPC over RL, however, is
the ability to handle state constraints and feasibility
criteria. MPC (unlike ML) has a mature theory of ro-
bustness and stability. These relations between MPC
and RL are studied in depth in [12].

The basis of many successful value function-based
RL algorithms is the use of general function approxi-
mation algorithms (e.g. artiﬁcial neural networks), ap-
proximating the expected value functions based on ex-
perience data, stored in a so-called replay memory. Al-
gorithms following this scheme are Neural Fitted Q-
Iteration [22] and the more recent Deep Q-Learning
[18]. While in Neural Fitted Q-Iteration artiﬁcial neural
networks are retrained from scratch regularly, in Deep
Q-Learning the artiﬁcial neural networks are constantly
redeﬁned based on mini-batch samples uniformly drawn
from the replay memory, enabling the computational
eﬃcient use of deep learning methods.

There is, to the best of our knowledge, no prior
reported application of RL or adaptive dynamic pro-
gramming to episodic ﬁxed-horizon manufacturing pro-
cess optimal control. Several model-free adaptive ap-
proaches for optimal control in the context of continu-
ous inﬁnite-horizon processes can be found in the liter-

ature. Recent examples are [27], where a state-of-the-
art actor-critic method is applied for optimal setpoint
control of linear chemical systems, and [17], where the
same method is applied to a more complex polymeriza-
tion reaction system.

Model predictive control, approximate dynamic pro-
gramming, and reinforcement learning determine the
optimal control action based on the current process
state. Almost all manufacturing processes are only par-
tially observable, the quantities measured in the cur-
rent state do not unambiguously describe the state with
respect to the optimization problem. Observation mod-
els are therefore used in model predictive control and
approximate dynamic programming to reconstruct the
current state from the observation and control history.
When using model-free optimal control methods on
partially observable processes, surrogate state descrip-
tions are derived during control for the current state
from the history of measurement values. In more com-
plex cases, partially observable Markov decision pro-
cesses (POMDPs) can be used to model partially ob-
servable environments. The solution of a POMDP in-
volves reconstructing the observation probability func-
tion and the derived probability distribution over pos-
sible current states, the so-called belief state, as a sur-
rogate state for solving the optimal control problem.
Finding an exact solution of POMDPs is in general in-
tractable. Therefore, approximations of POMDPs, e.g.
point-based solver approaches [25] are used. Alterna-
tively, sequence models such as Recurrent Neural Net-
works (RNNs) are used for deriving surrogate states
from the observation and control history ([15], [3], [25]).
Research on optimal blank holder force con-
trol can be found in the publications of Senn et al.
([24]) and Endelt et al. ([10], [9]). In [24], approximate
dynamic programming is used for the calculation of an
optimal blank holder force controller based on a process
model, which is learned on FEM simulation results. A
proportional optimal feedback control loop, optimizing
blank holder forces regarding a reference blank draw-in,
is presented in [10]. In [9], the control loop is embedded
in an outer iterative learning algorithm control loop.
The iterative learning algorithm transfers information
between deep drawing executions for linear adaption of
non-stationary process behavior.

In [9], a linear relationship between the error signal
and the optimal control and a non-delayed error signal
(the distance to a reference draw-in) is assumed. Both
algorithms are optimizing the control based on previ-
ously sampled data. Our proposed approach aims at
optimizing during control and should thereby be able
to adapt to unexpected changes of process behavior and
conditions.

4

Johannes Dornheim et al.

1.1 Paper Organization

2.2 Optimal Control and the Bellman Equation

In this paper, we develop the optimal online control
of partially observable, nonlinear episodic ﬁxed-horizon
processes with a delayed cost signal based on reinforce-
ment learning, and study it on a deep drawing process.
The model-free optimal control approach proposed is
based on Neural Fitted Q-Iteration [22], adapted for
use in optimal control of stochastic, partially observable
and sequentially executed ﬁxed-horizon manufacturing
processes.

The paper is structured as follows. Chapter 2 in-
troduces the methodical background and algorithms.
In chapter 3, the problem of adaptive optimal con-
trol of episodic ﬁxed-horizon manufacturing processes
is described and speciﬁed formally. Chapter 4 presents
the proposed reinforcement learning approach. In chap-
ter 5, the evaluation case, a simulated stochastic and
partially observable deep drawing optimization prob-
lem, is described and in the ﬁnal chapter, we discuss
quantitative and qualitative results of our approach for
the evaluation case.

2 Background

2.1 Markov Decision Processes

The framework of Markov Decision Processes (MDP)
is used to model time-discrete decision optimization
problems and constitutes the formal basis of algorithms
from dynamic programming, adaptive dynamic pro-
gramming, and reinforcement learning. An MDP is de-
scribed by a 5-tuple (X, U, P, R, γ), where X is the set
of states x, U is the set of control actions u, Pu(x, x(cid:48))
is the probability of a transition to x(cid:48) when applying
the control action u in state x, Ru(x, x(cid:48)) is a reward
signal given for the transition from state x to x(cid:48) due
to action u, and γ is a discount factor representing the
relative weight decrease of future rewards. By deﬁnition
an MDP satisﬁes the Markov property: The probability
of an upcoming state x(cid:48) depends only on the current
state x and on action u and is conditionally indepen-
dent of previous states and actions.

The goal of decision optimization algorithms is to
solve a given MDP by ﬁnding an optimal policy π∗ :
X → U , a function mapping from states to actions
with the property that, when following π∗ starting in
the state x, the expected discounted future reward V (x)
is maximized.

Consider the nonlinear time-discrete system

x(cid:48) = f (x, u, w).

(1)

According to the MDP deﬁnition in 2.1, x is a vec-
tor of system state variables, u is the control vector. x(cid:48)
denotes the system state following x under u and w,
where w ∼ W is a random variable of stochastic pro-
cess conditions, which are independent of previous time
steps.

An optimal control problem for the nonlinear sys-
tem f , for a deﬁned cost-function C(x, u, x(cid:48)), hidden
conditions w and for γ-discounted future reward, can
be solved by calculating the solution V ∗ of the Bell-
man equation (eq. 2), also named cost-to-go function
in dynamic programming.

V ∗(x) = min
u∈U

Ew∼W

(cid:104)

C(x, u, x(cid:48)) + γV ∗(x(cid:48))

(cid:105)

(2)

The term C is a local cost function depending on the
state x(cid:48) and, in some applications, also on the previous
state x and the control vector u that leads to the state.
When eq. (2) is solved, the optimal control law π∗(x)
Ew∼W [V ∗(x(cid:48))] for a given
can be extracted by arg min
u∈U

process model f .

In this paper, following the standard notation of re-
inforcement learning, the system f and the optimiza-
tion problem are modeled as an MDP as introduced
in the previous chapter. In MDPs, instead of the cost-
function C, a reward function R is used, leading to a
maximization problem instead of the minimization in
eq. (2). The Bellman equation is then given by

V ∗(x) = max
u∈U

(cid:104)

(cid:105)
Ru(x, x(cid:48)) + γV ∗(x(cid:48))

,

EP

(3)

where the probability of x(cid:48) is given by the transition
probability function Pu(x, x(cid:48)), capturing stochastic pro-
cess conditions w.

2.3 Q-Learning

The objective of Q-learning [32] is to ﬁnd the optimal
Q-function

Q∗(x, u) = EP

(cid:104)
Ru(x, x(cid:48)) + γ max
u(cid:48)∈U

(cid:105)
Q∗(x(cid:48), u(cid:48))

.

(4)

Unlike the optimal value function in eq. (3), the Q-
function is deﬁned over state, action tuples (x, u). By
taking actions into account, the Q-function implicitly

Title Suppressed Due to Excessive Length

5

Fig. 2 left: Rotationally symmetric deep drawing simulation model with observable values o and actor values u. right: tree-like
manufacturing process MDP for a given friction coeﬃcient of 0.056, with color-coded von Mises stress distribution in the radial
workpiece intersection.

captures the system dynamics, and no additional sys-
tem model is needed for optimal control. Once the op-
timal Q-function has been found, the optimal control
policy π∗ is given by

π∗(x) = arg max
u∈U

Q∗(x, u).

(5)

In Q-learning-based algorithms, Q∗ is found by con-
stantly updating a Q∗-approximation Q(cid:48) by the update
step in eq. 6, using experience tuples (x, u, x(cid:48), R) and a
given learning rate α ∈ [0, 1], while interacting with the
process in an explorative manner.

Q(cid:48)(x, u) = (cid:0)1 − α(cid:1)Q(x, u) + α(cid:0)R + γ max
u(cid:48)∈U

Q(x(cid:48), u(cid:48))(cid:1) (6)

3 Problem Description

In this paper, we consider sequentially executed ﬁxed-
horizon manufacturing processes. The processing of a
single workpiece involves a constant number of discrete
control steps and can be modeled as a Markov Deci-
sion Process (MDP), where for every policy and every
given start state, a terminal state is reached at time step
T . Each processing may be subject to slightly diﬀerent
conditions (e.g. the initial workpiece, lubrication, tool
wear). In reinforcement learning, the control of a ﬁxed-
horizon MDP from the start to the terminal state (here:
single workpiece processing) is denoted as an episode,
and tasks with repeated execution of episodes are de-
noted as episodic tasks. Hereafter, the term episode is

used in reinforcement learning contexts, and the phrase
process execution in manufacturing contexts.

Most processes, such as forming or additive manu-
facturing, are irreversible. The related control decisions
are leading to disjoint sub-state-spaces Xt depending
on the time step t < T . The terminal state reached
(xT ) is assessed, and the main reward is determined by
the ﬁnal product quality, quantiﬁed by a cost function.
At each time step during process execution, a negative
reward can also be assigned according to cost arising
from the execution of a dedicated action in the present
state. For deterministic processes, where the process dy-
namics are solely dependent on the control parameters
u, the structure of the ﬁxed-horizon processing MDP
is represented by a tree graph with the starting state
as root vertex and the terminal states as leaf vertices.
In the manufacturing process optimal control case con-
sidered in this paper, the process dynamics during an
individual process execution are dependent on stochas-
tic per-episode process conditions. In this scenario, the
underlying MDP equals a collection of trees, each rep-
resenting a particular process condition setting. An ex-
emplary MDP tree is depicted on the right-hand side
of ﬁg. 2, relating to the optimal control of the blank
holder force in deep drawing.

3.1 Deep Drawing, Blank Holder Force Optimization

A controlled deep drawing process is used as a proof
of concept, where the optimal variation of the blank-
holder force is determined at processing time. As em-
phasized in chapter 1, the result of the deep drawing

6

Johannes Dornheim et al.

from the processing samples via batch-wise retraining
of the respective function approximation, following the
incremental variant of the neural ﬁtted Q iteration ap-
proach ([22]).

For exploration, an (cid:15)-greedy policy is used, acting
randomly in an (cid:15)-fraction of control actions. To de-
rive the optimal action, the current approximation of
the Q∗-function is used (exploitation). The exploration
factor (cid:15) ∈ [0, 1] is decreased over time to improve the
optimization convergence and to reduce the number of
sub-optimal control trials. We use an exponential decay
over the episodes i according to (cid:15)i = (cid:15)0e−λi, with decay
rate λ.

4.1 Handling Partial Observability

Due to the partial observability of process states, the
current optimal action is potentially dependent on the
whole history of observables and actions. In the case
of ﬁxed-horizon problems, like episodes of the manu-
facturing processes considered here (see 3), the current
history is limited to the current episode only.

The optimal control of a partially observable pro-
cess depends on the given information. If representa-
tive data of observables and corresponding state values
can be drawn from the underlying observation proba-
bility function O (as assumed in [24]) an explicit obser-
vation model can be learned and used to derive state
values and apply regular MDP solution methods. If no
prior information about the state space and observation
probabilities is available, surrogate state descriptions ¯x
have to be derived from the history of observable values
and control actions.

A necessary condition for ﬁnding the optimal con-
trol policy π∗ for the underlying MDP based on ¯X
is, that the π∗ is equivalent for any two states (x1,
x2), whose corresponding action observable histories
are mapped to the same surrogate state ¯x. The fulﬁll-
ment of this condition is restricted by observation (mea-
surement) noise. In the present stochastic ﬁxed-horizon
case, the information about the state contained in the
observable and action history is initially small but in-
creases with every control and observation step along
the episode. The condition is therefore violated, espe-
cially in the beginning of an episode.

In this work, we use the full information about ob-
servables and actions for the current episode by con-
catenating all these values into ¯x. Thus, the dimen-
sion of ¯x ∈ Rn is time-dependent according to nt =
[dim(O) + dim(U )] ∗ t. When using Q-function approx-
imation, the approximation model input dimension is
therefore also dependent on t. If function approxima-

Fig. 3 Scheme of the interaction of the proposed optimal
online control agent with the simulated process environment.

process is strongly dependent on choosing appropri-
ate blank holder force trajectories. In this paper, the
optimization problem is modeled according to the de-
scribed episodic ﬁxed-horizon manufacturing process
MDP framework. Parameterizable FEM models are
used to simulate the online process behavior for method
development and evaluation. The simulated problem
setting (environment) is depicted in ﬁg. 3. The cur-
rent environment state x is calculated by the FEM
model based on the given control action u, the previous
environment-state and process conditions s, which are
randomly sampled from a given distribution. Based on
x, the current reward R is derived by a reward function,
and observable values o are derived by a sensor model.
The (R, o) tuple is ﬁnally reported to the control agent.
A detailed description of the simulation model and the
environment parameters used in this work is provided
in chapter 5.1.

4 Approach

A generic version of the Q-learning control agent is de-
picted in ﬁg. 3. An observer derives surrogate state de-
scriptions ¯x from the observable values o, and previous
control actions u. Control actions are determined based
on a policy π, which itself is derived from a Q-function.
In the approach proposed, the Q-function is learned

Title Suppressed Due to Excessive Length

7

tion methods with ﬁxed input dimensions (like stan-
dard artiﬁcial neural networks) is used, a dedicated
model for each control step is required. This compli-
cates the learning process in cases with higher numbers
of control-steps, especially for retraining and hyperpa-
rameter optimization. These problems can be avoided
by projection to a ﬁxed dimension surrogate state vec-
tor ¯x, e.g. via Recurrent Neural Networks.

ing iteration. The per-example loss used for training is
the squared error

L(cid:0)(x, u), y, Θt

(cid:1) = (cid:0)y − Qt(x, u, Θt)(cid:1)2

,

(7)

where the training target y for a given replay mem-

ory entry is deﬁned by

4.2 Q Function Approximation

The incorporation of function approximation into Q-
learning allows for a generalization of the Q-function
over the (X, U )-space. Thus it becomes possible to
transfer information about local Q-values to newly ob-
served states x. Approximating the Q-function is, there-
fore, increasing the learning speed in general and, fur-
thermore, enables an approximate representation of the
Q-function in cases with a continuous state space.

In this paper, artiﬁcial neural networks are used for
regression, which are retrained every k episodes, based
on data from a so-called replay memory. The replay
memory consists of an increasing set of experience tu-
ples (¯x, u, ¯x(cid:48), R), gathered during processing under con-
trol with the explorative policy described at the begin-
ning of chapter 4. The Q-function is re-trained from
scratch by using the complete replay-memory periodi-
cally. This is in contrast to standard Q-learning, where
the Q-function is re-deﬁned after each control action
u based on the current experience tuple (¯x, u, ¯x(cid:48), R)
only. Due to the non-stationary sampling process in re-
inforcement learning, the use of standard Q-learning
in combination with iteratively trained function ap-
proximation can result in a complete loss of experi-
ence (catastrophic forgetting) [11]. Using a replay mem-
ory for periodic retraining of Q-function approximation
models has shown to be more stable and data-eﬃcient
[22].

As described in 4.1, the dimension of the used
state representation ¯x is dependent on the current con-
trol step t ∈ [0, 1, ..., T ]. Because the input dimen-
sion of feed-forward artiﬁcial neural networks is ﬁxed,
multiple artiﬁcial neural networks Qt(¯x, u, Θt), with
weight parameter values Θt, depending on the cur-
rent control step t are used. For each tuple (¯x, u, ¯x(cid:48), R)
in the replay-memory, the Q-values are updated be-
fore retraining according to eq. (6). The update is
based on the Q-approximation for the current step,
Qt(¯x, u, Θt), and the Q-function approximation for t +
1, Qt+1( ¯x(cid:48), u(cid:48), ¯Θt+1), where Θt are weight values for
time step t from the previous training iteration and ¯Θt
are weight values, already updated in the current train-

y = (1−α)Qt(¯x, u, Θt)+α

(cid:104)
R+γ max
u(cid:48)∈U

(cid:105)
Qt+1( ¯x(cid:48), u(cid:48), ¯Θt+1)

.

(8)

The proposed update mechanism works backward
in the control steps t and thereby, as shown in eq. (8),
incorporates the already updated parameters for Qt+1
when updating Qt. Thereby, the propagation of reward
information is accelerated.

At the beginning of a process, the observable values
only depend on the externally determined initial state,
which in our case does not reﬂect any process inﬂuence.
The Q-function Q0 then has to be deﬁned over the sin-
gle state φ and therefore only depends on the control
action u0. In contrast to other history states, one tran-
sition sample (φ, u0, ¯x1) involving φ is stored in the re-
play memory per episode. By assuming that the stored
transition-samples are representative of the transition
probability function p(¯x1|φ, u0), the expected future re-
ward for the ﬁrst control-step u0 (here denoted as Q0)
is deﬁned in eq. (9) and can be calculated directly by
using the replay memory and the approximation model
for Q1.

(cid:104)
Q0(φ, u0) = E
R + γmaxu1Q1(¯x1, u1)

(cid:105)

(9)

5 Evaluation Environment

The approach presented in the previous chapters is eval-
uated by applying it to optimize the blank holder forces
during deep drawing processes. A controlled evaluation
environment is implemented by using numerical simu-
lations of the process. The digital twin of a deep draw-
ing process is realized via FEM simulation and used
for the online process control assessment. The simula-
tion model, as depicted on the left-hand side of ﬁg. 2,
simpliﬁes the deep drawing process assuming rotational
symmetry and isotropic material behavior. Due to its
simplicity, the FEM model is very eﬃcient with respect
to computing time (about 60 seconds simulation-time
per time step on 2 CPU cores) it, therefore, enables in-
depth methodological investigations such as compara-
tive evaluations of diﬀerent algorithms and parameter
settings.

8

Johannes Dornheim et al.

Abaqus FEM code was used for this work. Extensive
use of the Abaqus scripting interface [8] in combination
with analysis restarts (compare [7], chapter 9.1.1) has
been made for the eﬃcient and reusable simulation of an
online optimal control setting, where the control agent
sets control parameters based on current state observ-
ables.

5.1 FEM Model

The rotationally symmetric FEM model, visualized on
the left-hand side of ﬁg. 2, consists of three rigid parts
interacting with the deformable blank. The circular
blank has a thickness of 25 mm and a diameter of 400
mm. An elastic-plastic material model is used for the
blank, which models the properties of Fe-28Mn-9Al-
0.8C steel [34]. The punch pushes the blank with con-
stant speed into the die, to form a cup with a depth
of 250 mm. Blank holder force values can be set at
the beginning of each of ﬁve consecutive control steps,
where the blank holder force changes linearly in each
control step from its current value to the value given at
the start of the step. Blank holder force can be chosen
from the set {20 kN, 40 kN, ..., 140 kN} in each control
step. The initial blank holder force is 0.0 kN.

5.2 Process Disturbances

Stochastic process disturbances and observation noise
are added to the simulated deep drawing process de-
scribed in 5.1 to create a more realistic process twin.
Process disturbances are introduced by stochastic con-
tact friction. The friction coeﬃcient is modeled as a
beta-distributed random variable, sampled independent-
ly for each deep drawing process execution (episode).
The distribution parameters are chosen to be α =
1.75, β = 5. The distribution is rescaled to the range
[0, 0.14]. For better reusability of simulation results, the
friction coeﬃcient is discretized into 10 bins of equal
size. The resulting distribution is discrete and deﬁned
over 10 equidistant values from the interval [0.014, 0.14].
The distributions mode is 0.028.

5.3 Partial Observability

As depicted in ﬁg. 3, the current process state x is de-
pendent on the previous state, the control action u and
the current friction coeﬃcient s. Neither the state x
nor the underlying beta-distribution is accessible by the
agent. Instead, three observable values o are provided
in each time step:

– the current stamp force
– the current blank infeed in the x-direction
– the current oﬀset of the blank-holder in the y-

direction

The measurement noise of these observables is assumed
to be additive and to follow a normal distribution,
where the standard deviation is chosen to 1% of the
respective value range for both the stamp force and
the blank holder oﬀset and 0.5% of the value range for
the blank infeed. This corresponds to common mea-
surement noise characteristics encountered in analogue
sensors.

5.4 Reward Function

As described in 2.2, the reward function can be com-
posed of local cost (related to the state and action of
each single processing step) and of global cost (assigned
to the process result or ﬁnal state). In the case of zero or
constant local cost, the optimization is determined by
the reward assigned to the terminal state, which is as-
sessed by a quality control station. For our evaluation,
we consider this case. The process goal is to produce
a cup with low internal stress and low material usage,
but with suﬃcient material thickness.

FEM elements e of the blank are disposed in the
form of a discretization grid with n columns and m
rows, depicted in ﬁg. 2. For the reward calculation,
three n × m matrices (S, W y, Dx) of element-wise
simulation results are used. Where Sij is the mean von
Mises Stress of element eij, W y
ij is the width of element
eij in y-direction and Dx
ij is the x-axis displacement of
element eij between time step 0 and T . Using these
matrices, the reward is composed of the following three
terms based on the simulation results: (a) The l2-norm
of the vectorized von Mises stress matrix Ca(xT ) =
||vec(S)||, (b) the minimum Cb(xT ) = −min(si) over
a vector s of column-wise summed blank widths si =
(cid:80)
ij and (c) The summed displacement of elements
in the last column Cc(xT ) = (cid:80)
nj, represent-
ing material consumption during drawing. The reward
function terms Ri are scaled according to eq. 10, with
Ci, i ∈ {a, b, c}, resulting in approximately equally bal-
anced Ri.

nj − dx

j wy

j dx

Ri(xT ) = 10 ∗

1 −

(cid:18)

Ci(xT ) − C min
i
C max
− C min
i
i

(cid:19)

(10)

i

and C max

The extrema C min
are empirically deter-
i
mined per cost term, using 100 data tuples from ter-
minal states, sampled in prior experiments by apply-
ing random blank holder force trajectories. The ﬁnal

Title Suppressed Due to Excessive Length

9

reward is the weighted harmonic mean if all resulting
terms Ri are positive, and 0 otherwise.

R(xT ) =

(cid:40)

H(xT , W ),
0,

if ∀i ∈ {a, b, c} : Ri(xT ) ≥ 0
otherwise

H(xT , W ) =

(cid:80)
i

(cid:80)

i wi
wi
Ri(xT )

(11)

(12)

The harmonic mean was chosen to give preference to
process results with balanced properties regarding the
reward terms. The weights of the harmonic mean can be
used to control the inﬂuence of cost terms to the over-
all reward. Equal weighting is used for the evaluation
experiments of the presented algorithms.

5.5 Q-Function Representation

Fig. 4 Cross-Validation R2 score for Qt models by episode
(quality of the Q-function representation), sorted by t (color)

The Q-function, as introduced in chapter 4.2, is rep-
resented by a set of feedforward artiﬁcial neural net-
works, which are adapted via the backpropagation al-
gorithm. The optimization uses the sum-of-squared-
error loss function combined with an l2 weight regu-
larization term. The limited-memory approximation to
the Broyden-Fletcher-Goldfarb-Shanno (L-BFGS) algo-
rithm [16] is used for optimization because it has been
shown to be very stable, fast converging and a good
choice for learning neural networks when only a small
amount of data is available. Rectiﬁed linear unit func-
tions (ReLU) are used as activation functions in the
hidden layers. Two hidden layers are used for all net-
works, consisting of 10 units each for Q1 and 50 neurons
for Q2, Q3, Q4, respectively.

6 Results

The approach presented in chapter 4 was investigated
with a set of experiments, executed with the simulated
deep drawing process described in chapter 5.1. The pro-
posed adaptive optimal control algorithm is compared
to non-adaptive methods from standard model predic-
tive control and from approximate dynamic program-
ming. To make them comparable, the non-adaptive
methods are based on a given perfect process model
which does not capture stochastic process inﬂuences.
This is reﬂected by a static friction coeﬃcient of 0.028
in the respective simulation model. The non-adaptive
methods are represented by the optimal control trajec-
tory which was determined by a full search over the

trajectory-space for the respective simulation. The ex-
pected baseline reward, which is used for the evaluation
plots, is the expectation value of the reward reached by
the determined trajectory in the stochastic setting.

The model retraining interval k was chosen to be
50 process executions (episodes). The ﬁrst Q-function
training is carried out after 50 random episodes. For
hyperparameter optimization and evaluation, the per-
formance of the Q-function approximators is evaluated
by 5-fold cross-validation during each retraining phase.
In ﬁg. 4 the coeﬃcient of determination (R2-Score), re-
sulting from cross-validation, is plotted over the number
of episodes for the neural network hyperparameters de-
scribed in chapter 5.5 and the following reinforcement
learning parameters: learning-rate α = 0.7, exploration-
rate (cid:15) = 0.3 and (cid:15) decay λ = 10−3.

In this case, for t ∈ {2, 3, 4}, the R2-Score is in-
creasing over time as expected. For the ﬁrst time step
(t = 1) however, a decrease of the R2-Score over time
is observed. No hyperparameters leading to a diﬀerent
convergence behavior for t = 1 were found during opti-
mization. Results from experiments, visualized in ﬁg. 6,
where the current friction-coeﬃcient can be accessed
by the agent, indicate that the decrease is related to
the partial observability setting (see chapter 4.1). A
deeper analysis of the data shows that, due to the early
process stage and measurement noise, the information
about the friction-coeﬃcient in the observable values
is very low for t = 1. This causes the Q1-function to
rely mainly on the previous action u0 and the planned

10

Johannes Dornheim et al.

Fig. 5 Expected reward for the RL approach during op-
timization (blue) and expected reward for the baseline ap-
proach (grey, dashed)

Fig. 6 Cross-Validation R2 score for fully observable envi-
ronment (observable friction coeﬃcient)

t = arg max
ut∈U

action u1. In the ﬁrst episodes, high exploration rates
and low quality Q-functions are leading to very stochas-
tic control decisions and consequently to almost equally
distributed values of u0 and u1 in the replay memory.
In later episodes, optimal control decisions u∗
0 and u∗
1
Q∗(x, ut)) are increasingly dominant in
(u∗
the replay memory. Due to the low information in the
observable values, u∗
1 are independent of the fric-
tion coeﬃcient. The advantage of the Q1 model over the
simple average on the replay memory is decreasing due
to this dominance. Hence, the R2-score is shrinking. In
the fully observable case (ﬁg. 6), u∗
1 are depen-
dent on the friction coeﬃcient, causing the R2 score of
the Q1 model to increase over time.

0 and u∗

0 and u∗

Fig. 5 shows the results of the reinforcement learn-
ing control for 10 independent experimental batches of
1000 deep drawing episodes. The expected reward de-
pends on the given friction-distribution and denotes the
mean reward that would be reached if the agent would
stop to explore and instead exclusively exploit the cur-
rent Q-function. The expected reward reached by the
reinforcement learning agent (with α = 0.7 and (cid:15) = 0.3)
is calculated for each of the 10 independent batches. In
ﬁg. 5, the mean expected reward (blue solid line) is
plotted together with the range between the 10th per-
centile and the 90th percentile with linear interpolation
(shaded area). The baseline (dashed black line) is the
expected reward for non-adaptive methods, determined
as described above.

In order to reveal the eﬀect of the learning parame-
ter variation, experiments were conducted with varying
learning rates α and exploration rates (cid:15) to explore their
inﬂuence on the quality of the online control, reﬂected
by the distribution of the resulting reward. For each
parameter combination, 10 independent experimental
batches were executed under control of the respectively
parameterized reinforcement learning algorithm, each
with 2500 deep drawing episodes.

The control quality decreases slightly with increas-
ing exploration rate. The mean reward µ achieved in the
ﬁrst 250 episodes varies between µ = 4.69 for (cid:15) = 0.4
and µ = 5.03 for (cid:15) = 0.1. This is due to the negative
eﬀect of explorative behavior on the short-term out-
come of the process. In later episodes, the control is
more robust regarding the actuator noise caused by ex-
ploration. The mean reward achieved in the episodes
1250 to 1500 varies between µ = 5.75 for (cid:15) = 0.4
and µ = 5.86 for (cid:15) = 0.1. The mean reward achieved
in the last 250 episodes varies between µ = 5.89 for
(cid:15) = 0.4 and µ = 5.95 for (cid:15) = 0.1. In the deep draw-
ing process considered, even low exploration rates lead
to fast overall convergence, which is not necessarily the
case in more complex applications. The algorithm is
not sensitive to the chosen learning rate, the mean µ
and standard deviation σ of the reward achieved in the
last 250 deep drawing episodes, with a constant explo-
ration rate (cid:15) = 0.3, is (µ = 5.85, σ = 0.69) for α = 0.3,
(µ = 5.87, σ = 0.63) for α = 0.5, (µ = 5.89, σ = 0.78)
for α = 0.7, and (µ = 5.90, σ = 0.69) for α = 0.9,
respectively.

Title Suppressed Due to Excessive Length

11

episodic ﬁxed-horizon manufacturing processes has been
described, to which this generic method can be applied.
The approach has been instantiated for and evaluated
with, the task of blank holder force optimal control in
a deep drawing process. The optimal control goal was
the optimization of the internal stresses, of the wall
thickness and of the material eﬃciency for the resulting
workpiece. The deep drawing processes used to evaluate
the approach were simulated via FEM. The experimen-
tal processes were executed in an automatic virtual lab-
oratory environment, which was used to vary the pro-
cess conditions stochastically and to induce measure-
ment noise. The virtual laboratory is currently special-
ized to the deep drawing context but will be generalized
and published in future work for experimentation and
evaluation of optimal control agents on all types of FEM
simulated manufacturing processes.

Contrary to model-based approaches, no prior model
is needed when using RL for model-free optimal con-
trol. It is therefore applicable in cases where accurate
process models are not feasible or not fast enough for
online prediction. When applied online, the approach
is able to self-optimize, according to the cost function.
The approach is able to adapt to instance-speciﬁc pro-
cess conditions and non-stationary outer conditions. In
a fully blind scenario, with no additional sensory in-
formation, the proposed algorithm reaches the results
of non-adaptive model-based methods from model pre-
dictive control and approximate dynamic programming
in the example use case. The disadvantage of model-
free learning approaches, including the proposed rein-
forcement learning (RL) approach, is the dependence
on data gathered during the optimization (exploration).
Exploration leads to suboptimal behavior of the control
agent in order to learn about the process and to improve
future behavior. When used for optimal control of man-
ufacturing processes, RL can lead to increased product
rejection rates, which decrease during the learning pro-
cess. Optimal manufacturing process control with RL
can be used in applications with high production ﬁg-
ures but is not viable for small individual production
batches.

To overcome these problems, the incorporation of
safe exploration methods from the ﬁeld of safe rein-
forcement learning could directly lead to decreased re-
jection rates. Extending the proposed approach with
methods from transfer learning or multiobjective re-
inforcement learning could enable information trans-
fer between various process instances, diﬀering e.g. in
the process conditions or the production goal, and
could thereby lead to more eﬃcient learning, and con-
sequently to decreased rejection rates, and a wider ﬁeld
of application.

Fig. 7 Reward-distribution by episode for observable fric-
tion (red), partially observable (standard case, blue), control
based on the Reward-signal only, without using observable
values (green)

Besides learning parameter variation, experiments
were made to investigate the eﬀect of process observ-
ability. In ﬁg. 7, the rewards achieved by reinforcement
learning-based control of 10 independent executions in
three diﬀerent observability scenarios are visualized.
The resulting reward distribution for each scenario is
visualized by box plots, each representing a bin of 500
subsequent episodes. In the fully observable scenario
(left), the agent has access to the current friction co-
eﬃcient and thus perfect information about the pro-
cess state. The partially observable scenario (middle)
is equivalent to the scenario described in chapter 4.1
and is used in the previously described experiments.
The ”blind” agent (right) has no access to the process
state, and the optimal control is based on the knowl-
edge about previous control actions and rewards only.
In this scenario, the mean control performance after
1000 episodes is comparable to the baseline.

7 Discussion and Future Work

It has been shown how reinforcement learning-based
methods can be applied for adaptive optimal control
of episodic ﬁxed-horizon manufacturing processes with
varying process conditions. A model-free Q-learning-
based algorithm has been proposed, which enables the
adaptivity to varying process conditions through learn-
ing to modify the Q-function accordingly. A class of

12

Johannes Dornheim et al.

Acknowledgements The authors would like to thank the
DFG and the German Federal Ministry of Education and
Research (BMBF) for funding the presented work carried
out within the Research Training Group 1483 Process chains
in manufacturing (DFG) and under grant #03FH061PX5
(BMBF).

References

1. ˚Akesson, B.M., Toivonen, H.T.: A neural network model
predictive controller. Journal of Process Control 16(9),
937–946 (2006). DOI 10.1016/j.jprocont.2006.06.001
2. Alessio, A., Bemporad, A.: A survey on explicit model
predictive control. In: Nonlinear Model Predictive Con-
trol: Towards New Challenging Applications, pp. 345–
369. Springer (2009)

3. Bakker, B.: Reinforcement Learning with Long Short-
Term Memory. In: Advances in Neural Information Pro-
cessing Systems, vol. 14, pp. 1475–1482 (2002)

4. Bellman, R.: Dynamic programming. Courier Corpora-

tion (2013)

5. Bernard, T., Moghaddam, E.E.: Nonlinear Model Predic-
tive Control of a Glass forming Process based on a Finite
Element Model. Proceedings of the 2006 IEEE Interna-
tional Conference on Control Applications pp. 960–965
(2006)

6. Camacho, E.F., Alba, C.B.: Model predictive control.

Springer Science & Business Media (2013)

7. Dassault Syst`emes: Abaqus 6.14 Analysis User’s Guide:

Analysis, vol. II. Dassault Syst`emes (2014)

8. Dassault Syst`emes: Abaqus 6.14 Scripting User’s Guide.

Dassault Syst`emes (2014)

9. Endelt, B.: Design strategy for optimal iterative learning
control applied on a deep drawing process. The Interna-
tional Journal of Advanced Manufacturing Technology
88(1-4), 3–18 (2017)

10. Endelt, B., Tommerup, S., Danckert, J.: A novel feed-
back control system–controlling the material ﬂow in deep
drawing using distributed blank-holder force. Journal of
Materials Processing Technology 213(1), 36–50 (2013)

11. French, R.M.: Catastrophic forgetting in connectionist
networks. Trends in cognitive sciences 3(4), 128–135
(1999)

12. G¨orges, D.: Relations between Model Predictive Control
and Reinforcement Learning. IFAC-PapersOnLine 50(1),
4920–4928 (2017). DOI 10.1016/j.ifacol.2017.08.747
13. Gr¨une, L., Pannek, J.: Nonlinear model predictive con-
trol. In: Nonlinear Model Predictive Control, pp. 43–66.
Springer (2011)

14. Guo, N., Leu, M.C.: Additive manufacturing: technology,
applications and research needs. Frontiers of Mechanical
Engineering 8(3), 215–243 (2013)

15. Lin, L.J., Mitchell, T.M.: Reinforcement learning with
hidden states. From animals to animats 2, 271–280
(1993)

16. Liu, D.C., Nocedal, J.: On the limited memory bfgs
method for large scale optimization. Mathematical pro-
gramming 45(1-3), 503–528 (1989)

17. Ma, Y., Zhu, W., Benton, M.G., Romagnoli, J.: Contin-
uous control of a polymerization system with deep rein-
forcement learning. Journal of Process Control 75, 40–47
(2019)

18. Mnih, V., Kavukcuoglu, K., Silver, D., Rusu, A.A.,
Veness, J., Bellemare, M.G., Graves, A., Riedmiller,
M., Fidjeland, A.K., Ostrovski, G., et al.: Human-level

control through deep reinforcement learning. Nature
518(7540), 529 (2015)

19. Moran, M.J., Shapiro, H.N., Boettner, D.D., Bailey,
M.B.: Fundamentals of engineering thermodynamics.
John Wiley & Sons (2010)

20. Powell, W.B.: Approximate Dynamic Programming:
Solving the curses of dimensionality. John Wiley & Sons
(2007)

21. Qin, S., Badgwell, T.A.: A survey of industrial model
predictive control technology. Control Engineering Prac-
tice 11(7), 733–764 (2003). DOI 10.1016/S0967-0661(02)
00186-7

22. Riedmiller, M.: Neural Fitted Q Iteration - First Experi-
ences with a Data Eﬃcient Neural Reinforcement Learn-
ing Method. In: European Conference on Machine Learn-
ing, pp. 317–328. Springer (2005)

23. Saint-Donat, J., Bhat, N., McAvoy, T.J.: Neural net
based model predictive control.
International Journal
of Control 54(6), 1453–1468 (1991). DOI 10.1080/
00207179108934221

24. Senn, M., Link, N., Pollak, J., Lee, J.H.: Reducing the
computational eﬀort of optimal process controllers for
continuous state spaces by using incremental learning and
post-decision state formulations. Journal of Process Con-
trol 24(3), 133–143 (2014). DOI 10.1016/j.jprocont.2014.
01.002

25. Shani, G., Pineau, J., Kaplow, R.: A survey of point-
based POMDP solvers. Autonomous Agents and Multi-
Agent Systems 27(1), 1–51 (2013).
DOI 10.1007/
s10458-012-9200-2

26. Singh, C.P., Agnihotri, G.: Study of Deep Drawing Pro-
cess Parameters: A Review. International Journal of Sci-
entiﬁc and Research Publications 5(1), 2250–3153 (2015).
DOI 10.1016/j.matpr.2017.01.091

27. Spielberg, S., Gopaluni, R., Loewen, P.: Deep reinforce-
ment learning approaches for process control.
In: Ad-
vanced Control of Industrial Processes (AdCONIP), 2017
6th International Symposium on, pp. 201–206. IEEE
(2017)

28. Sutton, R., Barto, A., Williams, R.: Reinforcement learn-
ing is direct adaptive optimal control. IEEE Control Sys-
tems 12(2), 19–22 (1992). DOI 10.1109/37.126844
29. Sutton, R.S., Barto, A.G.: Reinforcement learning: An

introduction, vol. 1. MIT press Cambridge (1998)

30. Tommerup, S., Endelt, B.: Experimental veriﬁcation of
a deep drawing tool system for adaptive blank holder
pressure distribution. Journal of Materials Processing
Technology 212(11), 2529–2540 (2012). DOI 10.1016/j.
jmatprotec.2012.06.015

31. Wang, F.y., Zhang, H., Liu, D.: Adaptive Dynamic Pro-
gramming: An Introduction. IEEE computational intel-
ligence magazine 4(2), 39–47 (2009)

32. Watkins, C.J.C.H.: Learning from delayed rewards.

Ph.D. thesis, King’s College, Cambridge (1989)

33. Wiﬁ, a., Mosallam, a.: Some aspects of blank-holder
force schemes in deep drawing process.
Journal of
Achievements in Materials and Manufacturing Engineer-
ing 24(1), 315–323 (2007)

34. Yoo, J.D., Hwang, S.W., Park, K.T.: Factors inﬂuencing
the tensile behavior of a fe–28mn–9al–0.8 c steel. Materi-
als Science and Engineering: A 508(1-2), 234–240 (2009)

