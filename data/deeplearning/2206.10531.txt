Neural Transformers for Intraductal Papillary Mucosal Neoplasms
(IPMN) Classiﬁcation in MRI images

F. Proietto Salanitri1, G. Bellitto1, S. Palazzo1, I. Irmakci2, M. Wallace4, C. Bolan4, M. Engels4,
S. Hoogenboom4, M. Aldinucci3, U. Bagci2, D. Giordano 1 and C. Spampinato1

2
2
0
2

n
u
J

1
2

]

V
C
.
s
c
[

1
v
1
3
5
0
1
.
6
0
2
2
:
v
i
X
r
a

Abstract— Early detection of precancerous cysts or neo-
plasms, i.e., Intraductal Papillary Mucosal Neoplasms (IPMN),
in pancreas is a challenging and complex task, and it may lead
to a more favourable outcome. Once detected, grading IPMNs
accurately is also necessary, since low-risk IPMNs can be under
surveillance program, while high-risk IPMNs have to be surgi-
cally resected before they turn into cancer. Current standards
(Fukuoka and others) for IPMN classiﬁcation show signiﬁcant
intra- and inter-operator variability, beside being error-prone,
making a proper diagnosis unreliable. The established progress
in artiﬁcial intelligence, through the deep learning paradigm,
may provide a key tool for an effective support to medical deci-
sion for pancreatic cancer. In this work, we follow this trend, by
proposing a novel AI-based IPMN classiﬁer that leverages the
recent success of transformer networks in generalizing across a
wide variety of tasks, including vision ones. We speciﬁcally show
that our transformer-based model exploits pre-training better
than standard convolutional neural networks, thus supporting
the sought architectural universalism of transformers in vision,
including the medical image domain and it allows for a better
interpretation of the obtained results.

I. INTRODUCTION

Pancreatic cancer, also known as pancreatic ductal adeno-
carcinoma (PDAC), is a growing public health issue around
the world. In the United States in 2021, an estimated 60,430
new cases of pancreatic cancer will be diagnosed, with
48,220 people dying from the disease [1]. Pre-cancerous
cysts or neoplasms in the pancreatic ducts are known as
Intraductal Papillary Mucosal Neoplasms (IPMN) and can
develop anywhere in the pancreas’ ductal zone. Grading
the severity of IPMNs is an important diagnosis step: most
IPMNs are low-grade, and should be monitored over time;
high-grade IPMNs, however, may turn into invasive cancer
if left untreated. In these cases, surgery is the ﬁrst choice
to prevent them from expanding into malignant pancreatic
tumors. Therefore, there is an unmet for early detection tech-
niques of IPMNs, in order to identify which IPMNs may lead
to cancer. Automated image analysis in radiology imaging
plays a key role in diagnosis, treatment and intervention of
pancreas diseases; thus there is a strong potential for machine
learning tools to support IPMN grade prediction that can
serve better than the current radiographic standards. The most
popular imaging modalities for the pancreas are computed

1PeRCeiVe

Lab,

Department

of

Electrical,

Computer

and
perceive@dieei.unict.it

Engineering,

University

of

Electronic
Italy.

Catania,

2Department of Radiology and BME, Northwestern University, Chicago,

IL, USA

3 Computer Science Department , University of Torino, Torino, Italy
4 Mayo Clinic, Jacksonville, FL, USA

tomography (CT) and magnetic resonance imaging (MRI).
In the last few years,
transformer architectures [2], [3]
have raised as a valid alternative to standard convolutional
networks on a variety of different tasks. More speciﬁcally,
transformers enable learning arbitrary functions and con-
sists of two main operation blocks: ﬁrst, an attention-based
block for modeling inter-element relations; second, a multi-
layer perceptron (MLP) modeling relations intra-element.
A sequence of attention and MLP blocks intertwined with
residual connections and normalization has showed to allow
for generalization over multiple tasks. Following this trend,
herein we propose an automated IPMN classiﬁer based
on transformer architecture. We, in particular, show how
transformers generalize better than standard and state-of-
the-art CNNs (namely, DenseNet, AlexNet, etc.) also for
extremely complex tasks, as IPMN classiﬁcation, while
providing similar accuracy to the state of the art IPMN
classiﬁcation study with deep learning [4].
The major contributions of this study are the following:

1) Our work on IPMN classiﬁcation is an important
application contribution, which is not widely done due
to the difﬁculty nature of the problem, and hence there
is a very limited published research on this task using
MRI data with deep learning. Our method can provide
a signiﬁcant state-of-the-art baseline to be compared
with for further MRI pancreas research just before
critical surgery decision or surveillance.

2) Our study contributes to the recent AI research in
the strive to demonstrate architectural universalism of
Transformers that can be used in a wide variety of tasks
using little inductive bias, beside validating their better
interpretability than CNN counterparts. To our best
of knowledge, transformers have never been tried on
high-risk medical diagnoses tasks before, particularly
for pancreas imaging research.

II. RELATED WORK

While signiﬁcant progress has been made for automated
approaches to segment the pancreas and its cysts [5], the use
of advanced machine learning algorithms to perform fully
automatic risk-stratiﬁcation of IPMNs is still limited.
Some recent works, employing machine learning techniques
for predicting the risk of malignancy in IPMN, have used en-
doscopic ultrasound (EUS) images [6], [7] yielding high ac-
curacy of 94.0%, outperforming both human diagnosis (56%)
and conventional guidelines (40–68%). CT imaging has been

 
 
 
 
 
 
also adopted for investigating IPMN as in [8], [9] where low-
level imaging features, such as texture, strength, and shape,
have extracted from segmented cysts or pancreas for IPMN
classiﬁcation. Recently, deep learning methods based on
standard convolutional neural networks have been proposed
to diagnose IPMN from MRI scans [10], [11], [4]. Sarfaraz
et. al. [10] proposed an architecture for automated IPMN
classiﬁcation based on feature extraction with canonical
correlation using a pre-trained 3D CNN, while [11] propose
a novel CNN for recognizing high grade dysplasia or cancer
on MR-images, yielding promising results. Finally, Rodney
et al. [4] constructed two novel ”inﬂated” CNN architectures,
InceptINN and DenseINN, for the task of diagnosing IPMN
from multisequence (T1 and T2) MRI obtaining an accuracy
of about 73% in grading IPNM into three classes (no risk,
low and high-risk). In this work, we employ transformers
that are speciﬁc neural architectures originally proposed for
machine translation tasks [2]. Transformer-based models in
NLP are generally pre-trained on large corpora and then
ﬁne-tuned for the task at hand [12], [13]. Their increasing
interest to vision tasks starts with Vision Transformers [3]
and Detection Transformer [14]. Recently, several methods
have explored transformer-based architectures for medical
image analysis mainly for segmentation tasks [19], [21], [20].
However, these method employ an hybrid architecture com-
bining both convolutions and transformers. Our approach
builds upon pure vision transformers and employs a strategy
similar to that one employed in NLP (as in [12], [13]), i.e.,
pre-training transformers on natural images and then ﬁne-
tuning them to MRI IPNM images. Experimental results
show that our pre-trained transformers perform signiﬁcantly
better than state-of-the-art CNN classiﬁers.

III. METHOD

In our study, we follow the recently emerging approach
of Transformers [2] for vision tasks. In particular, we use
the ViT [3] setting, in which the encoder of the original
transformer model is used on a sequence of image “patches”.
However, since [3] is trained on natural images, it is neces-
sary to adapt the input representation to be able to process
MRI scans, which are instead composed by an aggregation of
multiple slices, providing anatomical volumetric information.
Fig. 1 describes the proposed procedure in detail. We use
T1- and T2-weighted MRI scans of the same patients in
an early fusion fashion to enrich diagnostic and anatomi-
cal (localization) information. For each modality, we ﬁrst
sample k=9 consecutive slices and use them to create a
k
single image, rearranging the selected slices in a
grid. k can be set differently depending on the memory
availability and z-direction resolution of the MRI scan. In
our experiments, we optimize this number to appreciate full
anatomical information of the pancreas. The two images (one
for each modality) are then concatenated along the channel
kW × 2,
dimension: the resulting tensor, of size
with H ×W being the original size of each slice, is provided
as input to the transformer.

kH ×

k ×

√

√

√

√

Fig. 1: The proposed transformer-based architecture. T1 and
T2 slices are concatenated along the channel dimension
and sequences of 9 consecutive slices are arranged in a
3×3 grid. Patches are then extracted from the resulting
image, and are used as input to the transformer architecture.
After encoding the patch set
through transformer layers
(consisting of a cascade of multihead attention block and
MLP layers), a special classiﬁcation token encodes global
image representation, and is used for ﬁnal classiﬁcation into
three IPMN classes: normal, low risk and high risk.

Without loss of generality, let us assume that H = W .
As in [3], the input image is then divided into N patches
of size P × P , where N = kHW
P 2 . As a result of this
image X ∈ RH×W ×C becomes a
procedure, an input
sequence of 2D patches Xp ∈ RN ×(P ×P ×C) with C being
the channel dimension. The 2D patches are then ﬂattened
into vectors of size P 2C and projected to an embedding
space of size D, obtaining a sequence of token embeddings.
As a last pre-processing step, learnable positional encodings
are summed to token embeddings, producing the actual input
data sequence to the transformer.

We extend the token sequence with a special class token,
whose state at the output of the transformer describes the
image representation for classiﬁcation pur-
input
overall
poses [3], [15], [12].

Formally, the input z0 to the transformer is deﬁned as:

z0 = (cid:2)xclass, x1

pE, . . . , xN

(1)
p ∈ RP 2C is a ﬂattened patch vector, E ∈
where each xi
R(P 2C)×D is the embedding matrix and Epos ∈ R(N +1)×D
is the positional encoding matrix.

p E(cid:3) + Epos,

The transformer encoder [2] alternates multi-head self-
attention and MLP (multilayer perceptron) blocks. These
blocks are then intertwined with layer normalization and
residual connections (see Fig. 1), as follows:

z(cid:48)
l = MSA(LN(zl−1)) + zl−1,
zl = MLP(LN(z(cid:48)

l)) + z(cid:48)
l,

(2)

(3)

where l = 1 · · · L identiﬁes the transformer layer, LN(·)
performs layer normalization, MLP represents a multilayer

perceptor, and MSA(·) computes the standard query-key-
value multi-head self-attention [2].

At the last transformer layer, the output embedding cor-
responding to class token is ﬁnally used for classiﬁcation
into 3 classes, since the MRI dataset includes normal scans,
low-grade and high-grade IPMN lesions:

y = LN(z0

L),

(4)

with y being the vector of output class scores.

IV. EXPERIMENTAL RESULTS

A. Dataset

We evaluate the accuracy of our proposed IPMN risk
assessment method in MRI (with both T1 and T2 modalities).
We use a total of 139 scans from distinct patients, retro-
spectively collected at Mayo Clinic [4]. Patients have either
IPMN cysts detected in their pancreases or they are normal
control cases selected to match the IPMN patients. Out of
139 cases, 58 (42%) were male; mean (standard deviation)
age was 65.3 (11.9) years. 22% had normal pancreas; 34%,
low-grade dysplasia; 14%, high-grade dysplasia; and 29%,
adenocarcinoma [11]. Two expert radiologists graded the
cases in a pathology report after surgery: 0) normal, 1) low-
grade IPMN, and 2) high-grade IPMN. We did not consider
invasive carcinoma in our analysis as they are outside the
scope of IPMN risk stratiﬁcation.

MRI images were resized (in the transverse plane) to
256×256 pixels. Voxel spacing of MRI scans were varying
from 0.468 mm to 1.406 mm. We applied a set of pre-
processing steps: N4 bias ﬁeld correction followed by an
edge-preserving Gaussian smoothing, and intensity standard-
ization procedure to normalize MRI scans across patients,
scanners, and time. All MRIs were performed using Siemens
scanners 1.5 or 3 T (Siemens, Berlin, Germany).

The experimental procedures involving human subjects
described in this paper were approved by the Institutional
Review Board.

B. Training Procedure

We use the Vision-Transformer pre-trained on 300 million
images [16] and released in [3]. During training, we ﬁne-tune
all transformers layers with the training data from the MRI
dataset. MRI slices are cropped around the pancreas areas by
expert physicians for all scans, and each set of 9 consecutive
slices, extracted in a sliding window fashion, is arranged in
a 3 × 3 grid (from top-left to bottom-right), where each cell
of the grid is ﬁlled by a 64×64 MRI slice (see Fig. 1). Input
MRI scans are re-oriented using the RAS axes convention
and normalized, individually, between 0 and 1. Data aug-
mentation is performed through random horizontal ﬂipping
and random 90-degrees rotation (identically applied to all
slices within a grid). We minimize the cross-entropy loss
with gradient descent using the Adam optimizer (learning
rate: 0.003) and batch size of 8, for a total of 3000 epochs.
At inference time, we classify each input MRI by feeding
the sequence of 9 central slices to the model.

We employ the same training and evaluation procedure
for CNN models used as baselines, i.e., DenseNet-121 [22],
AlexNet [23] ResNet18 [24], EfﬁcientNet b5 [25] and Mo-
bileNet v2 [26]. Experiments are performed on a NVIDIA
RTX 3090 GPU.The proposed approach was implemented
in PyTorch and MONAI; all code will be publicly released
upon publication.

TABLE I: Performance of tested models with 10-fold nested
cross-validation. We report results in term of mean ± stan-
dard deviation of metrics computed over all validation folds.

Precision

Method Accuracy
AlexNet 0.42 ± 0.17 0.37 ± 0.15 0.39 ± 0.11
DenseNet 0.51 ± 0.12 0.54 ± 0.14 0.50 ± 0.14
ResNet18 0.53 ± 0.11 0.55 ± 0.23 0.32 ± 0.08
MobileNet v2 0.43 ± 0.11 0.54 ± 0.26 0.35 ± 0.11
EfﬁcientNet b5 0.55 ± 0.10 0.60 ± 0.14 0.36 ± 0.08

Recall

Ours 0.70 ± 0.11 0.67 ± 0.19 0.64 ± 0.12

C. Performance

We perform 10-fold nested cross-validation in order to
estimate the accuracy of the proposed approach and the
methods under comparison. Results are reported in Table I,
where the proposed model largely outperforms the CNN
models, conﬁrming the better generalization capabilities of
transformer-based architectures compared to standard convo-
lutional models.

TABLE II: Performance of our model using different input
data modality with 10-fold nested cross-validation. We report
results in terms of mean ± standard deviation of metrics
computed over all validation folds.

Method

T1
T2

Accuracy
Precision
0.53 ± 0.08 0.60 ± 0.11 0.58 ± 0.14
0.64 ± 0.12 0.64 ± 0.13 0.63 ± 0.11

Recall

T1+T2 modalities

0.61 ± 0.13 0.59 ± 0.11
Late fusion
Early fusion 0.70 ± 0.11 0.67 ± 0.19 0.64 ± 0.12

0.60 ±0.16

We also evaluate the role of early fusion and of com-
bining the T1 and T2 modalities, by assessing classiﬁcation
performance when the model receives only one modality at
a time (either T1 or T2) and when performing late fusion. In
this case, we train two transformer models, one for each
modality, and we then concatenate the two class tokens
before classiﬁcation. Performance is reported in Table II that
demonstrates how using T1 and T2 in an early fusion setting
yields the highest performance.

It has to be noted that, comparing on the same dataset, the
performance achieved by our transformer-based approach is
slightly lower than those obtained in [4], i.e., about 73%.
However, the architecture in [4] was speciﬁcally designed
and tuned for solving the IPMN classiﬁcation problem, while
our transformer architecture is general, designed for natural

(a) AlexNet

(b) DenseNet

(c) MobileNet v2

(d) ResNet18

(e) EfﬁcientNet b5

(f) Our method

Fig. 2: Comparison between the attention maps of AlexNet, DenseNet-121 and our model in case of correct (top row) and
erroneous (bottom row) predictions on a 3×3 grid of MRI images.

image classiﬁcation and applied directly without signiﬁcant
architectural changes to IPMN classiﬁcation problem. This
is remarkable, as we demonstrate that a general architecture
performs similarly to an ad-hoc one for a complex task with
limited training data.

D. Interpretability of results

Transformers allow for a more direct interpretation of
its internal representations through visualizing the attention
weights [3], thus supporting the sought interpretability neces-
sary in safety-critical contexts as the medical domain one. We
apply Attention Rollout [17] to track down the information
propagated from the input layer to the embeddings in the
higher layers. Thus, we average attention weights of all
heads of each transformer layer and then multiply these
averages across all layers. Fig. 3 shows some examples of
interpretabilty maps in cases of correct cyst classiﬁcation.

Fig. 3: Attention maps of our transformer-based classiﬁer on
3×3 grid of MRI images for correct IPMN classiﬁcation.

It can be observed how our transformer-based model
focuses its attention mainly on cysts, thus it provides robust
predictions. Conversely, CNN-based models lead to weak
decisions, as their attention maps (estimated using Grad-
Cam [18]) reveal that features not strictly related to cysts
are used for classiﬁcation (see Fig. 2, top row). Finally,
although our model fails in some cases, as demonstrated by
the classiﬁcation accuracy in Tab. I, its attention maps often
point to the correct cyst regions (see Fig. 2, bottom row);

thus, the wrong prediction is due to either using directly raw
data, rather than a more powerful representation, or lack of
enough training data.

V. CONCLUSION

In this work, our overall goal was to classify pancreas
(IPMN) cysts automatically. We utilized transformers for
time for pancreas risk predictions and obtained
the ﬁrst
promising results that can be used for MRI-based IPMN
risk stratiﬁcation routinely. Compared to the (few) existing
methods, transformers showed higher performance overall.
We found that training transformer for IPMN risk stratiﬁ-
cation is easier than conventional CNN based systems and
generalizes better. Furthermore, the proposed transformer-
based classiﬁer allows for better interpretation of results than
standard CNNs, revealing how it employs cues exclusively
related to cysts, providing more robustness to the automated
diagnosis than the comparing methods. These ﬁndings high-
light the contribution that transformers can give to the future
research in medical image understanding, in general, and
IPMN classiﬁcation, in particular, beside contributing the
recent AI research efforts towards universal architectures.

ACKNOWLEDGMENT

This research was partially supported by the following
grants: 1) Rehastart funded by PO FESR 2014-2020 Si-
cilia, Azione 1.1.5; project number 08ME6201000222, CUP:
G79J18000610007); 2) the “Adaptive Brain-Derived Arti-
ﬁcial Intelligence Methods for Event Detection” – BrAIn
funded by the “Programma per la ricerca di ateneo UNICT
2020–22 linea 3”; 3) NIH grants R01-CA246704-01 and
R01-CA240639-01.

REFERENCES

[1] American Cancer Society, “Cancer facts & ﬁgures,” American Cancer

Society, 2021.

[2] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion
Jones, Aidan N Gomez, Lukasz Kaiser, and Illia Polosukhin, “Atten-
tion is all you need,” arXiv preprint arXiv:1706.03762, 2017.

[3] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weis-
senborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani,
Matthias Minderer, Georg Heigold, Sylvain Gelly, et al., “An image
is worth 16x16 words: Transformers for image recognition at scale,”
arXiv preprint arXiv:2010.11929, 2020.

[4] Rodney LaLonde, Irene Tanner, Katerina Nikiforaki, Georgios Z
Papadakis, Pujan Kandel, Candice W Bolan, Michael B Wallace, and
Ulas Bagci, “Inn: inﬂated neural networks for ipmn diagnosis,” in In-
ternational Conference on Medical Image Computing and Computer-
Assisted Intervention. Springer, 2019, pp. 101–109.

[5] Yuyin Zhou, Lingxi Xie, Elliot K Fishman, and Alan L Yuille, “Deep
supervision for pancreatic cyst segmentation in abdominal ct scans,” in
International conference on medical image computing and computer-
assisted intervention. Springer, 2017, pp. 222–230.

[6] Takamichi Kuwahara, Kazuo Hara, Nobumasa Mizuno, Nozomi
Okuno, Shimpei Matsumoto, Masahiro Obata, Yusuke Kurita, Hiroki
Koda, Kazuhiro Toriyama, Sachiyo Onishi, et al., “Usefulness of deep
learning analysis for the diagnosis of malignancy in intraductal papil-
lary mucinous neoplasms of the pancreas,” Clinical and translational
gastroenterology, vol. 10, no. 5, 2019.

[7] Myrte Gorris, Sanne A Hoogenboom, Michael B Wallace, and Jeanin E
van Hooft, “Artiﬁcial intelligence for the management of pancreatic
diseases,” Digestive Endoscopy, vol. 33, no. 2, pp. 231–241, 2021.
[8] Alexander N Hanania, Leonidas E Bantis, Ziding Feng, Huamin
Wang, Eric P Tamm, Matthew H Katz, Anirban Maitra, and Eugene J
Koay, “Quantitative imaging to evaluate malignant potential of ipmns,”
Oncotarget, vol. 7, no. 52, pp. 85776, 2016.

[9] Lior Gazit, Jayasree Chakraborty, Marc Attiyeh, Liana Langdon-
Embry, Peter J Allen, Richard KG Do, and Amber L Simpson,
“Quantiﬁcation of ct images for the classiﬁcation of high-and low-
in Medical Imaging 2017: Computer-Aided
risk pancreatic cysts,”
Diagnosis. International Society for Optics and Photonics, 2017, vol.
10134, p. 101340X.

[10] Sarfaraz Hussein, Pujan Kandel, Juan E Corral, Candice W Bolan,
Michael B Wallace, and Ulas Bagci, “Deep multi-modal classiﬁcation
of intraductal papillary mucinous neoplasms (ipmn) with canonical
correlation analysis,” in 2018 IEEE 15th International Symposium on
Biomedical Imaging (ISBI 2018). IEEE, 2018, pp. 800–804.

[11] Juan E Corral, Sarfaraz Hussein, Pujan Kandel, Candice W Bolan,
Ulas Bagci, and Michael B Wallace,
“Deep learning to classify
intraductal papillary mucinous neoplasms using magnetic resonance
imaging,” Pancreas, vol. 48, no. 6, pp. 805–810, 2019.

[18] Ramprasaath R Selvaraju, Michael Cogswell, Abhishek Das, Ramakr-
ishna Vedantam, Devi Parikh, and Dhruv Batra, “Grad-cam: Visual
explanations from deep networks via gradient-based localization,” in
Proceedings of the IEEE international conference on computer vision,
2017, pp. 618–626.

[12] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova,
“Bert: Pre-training of deep bidirectional transformers for language
understanding,” arXiv preprint arXiv:1810.04805, 2018.

[13] Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei,
“Language models are unsupervised multitask

and Ilya Sutskever,
learners,” 2018.

[14] Nicolas Carion, Francisco Massa, Gabriel Synnaeve, Nicolas Usunier,
Alexander Kirillov, and Sergey Zagoruyko, “End-to-end object detec-
tion with transformers,” in European Conference on Computer Vision.
Springer, 2020, pp. 213–229.

[15] Hugo Touvron, Matthieu Cord, Matthijs Douze, Francisco Massa,
Alexandre Sablayrolles, and Herv´e J´egou,
“Training data-efﬁcient
image transformers & distillation through attention,” arXiv preprint
arXiv:2012.12877, 2020.

[16] Chen Sun, Abhinav Shrivastava, Saurabh Singh, and Abhinav Gupta,
“Revisiting unreasonable effectiveness of data in deep learning era,” in
Proceedings of the IEEE international conference on computer vision,
2017, pp. 843–852.

[17] Samira Abnar and Willem Zuidema, “Quantifying attention ﬂow in

transformers,” arXiv preprint arXiv:2005.00928, 2020.

[19] Chen, Jieneng and Lu, Yongyi and Yu, Qihang and Luo, Xiangde and
Adeli, Ehsan and Wang, Yan and Lu, Le and Yuille, Alan L and Zhou,
Yuyin “Transunet: Transformers make strong encoders for medical
image segmentation,” arXiv preprint arXiv:2102.04306, 2021.
[20] Hatamizadeh, Ali and Tang, Yucheng and Nath, Vishwesh and Yang,
Dong and Myronenko, Andriy and Landman, Bennett and Roth, Hol-
ger R and Xu, Daguang “Unetr: Transformers for 3d medical image
segmentation,” in Proceedings of the IEEE/CVF Winter Conference
on Applications of Computer Vision,2022, pp. 574–584.

[21] Xie, Yutong and Zhang, Jianpeng and Shen, Chunhua and Xia, Yong
“CoTr: Efﬁciently Bridging CNN and Transformer for 3D Medical
Image Segmentation,” arXiv preprint arXiv:2103.03024, 2021
[22] Huang, Gao and Liu, Zhuang and Van Der Maaten, Laurens and
Weinberger, Kilian Q “Densely connected convolutional networks” in
Proceedings of the IEEE conference on computer vision and pattern
recognition,2017, pp. 4700–4708

[23] Krizhevsky, Alex and Sutskever, Ilya and Hinton, Geoffrey E “Im-
agenet classiﬁcation with deep convolutional neural networks”
in
Advances in neural
information processing systems,2012, 25, pp.
1097–1105

[24] He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian
“Deep residual learning for image recognition” in Proceedings of the
IEEE conference on computer vision and pattern recognition,2016, pp.
770–778

[25] Tan, Mingxing and Le, Quoc “Efﬁcientnet: Rethinking model scaling
for convolutional neural networks” in International Conference on
Machine Learning, 2019, pp. 6105–6114

[26] Sandler, Mark and Howard, Andrew and Zhu, Menglong and Zhmogi-
nov, Andrey and Chen, Liang-Chieh “Mobilenetv2: Inverted residuals
and linear bottlenecks” in Proceedings of the IEEE conference on
computer vision and pattern recognition,2018, pp. 4510–4520

