1
2
0
2

l
u
J

5
1

]
n
y
d
-
u
l
f
.
s
c
i
s
y
h
p
[

1
v
9
5
5
7
0
.
7
0
1
2
:
v
i
X
r
a

LEARNING STABLE GALERKIN MODELS OF TURBULENCE WITH
DIFFERENTIABLE PROGRAMMING

A PREPRINT

Arvind T. Mohan

Computational Physics and Methods Group
Los Alamos National Laboratory
Los Alamos, NM 87545, United States
arvindm@lanl.gov

Kaushik Nagarajan
National Aerospace Laboratories
Bengaluru, Karnataka 560 017, India

Daniel Livescu
Computational Physics and Methods Group
Los Alamos National Laboratory
Los Alamos, NM 87545, United States

July 19, 2021

ABSTRACT

Turbulent ﬂow control has numerous applications and building reduced-order models (ROMs) of
the ﬂow and the associated feedback control laws is extremely challenging. Despite the complexity
of building data-driven ROMs for turbulence, the superior representational capacity of deep neural
networks has demonstrated considerable success in learning ROMs. Nevertheless, these strategies
are typically devoid of physical foundations and often lack interpretability. Conversely, the Proper
Orthogonal Decomposition (POD) based Galerkin projection (GP) approach for ROM has been
popular in many problems owing to its theoretically consistent and explainable physical foundations.
However, a key limitation is that the ordinary differential equations (ODEs) arising from GP ROMs
are highly susceptible to instabilities due to truncation of POD modes and lead to deterioration in
temporal predictions. In this work, we propose a differentiable programming approach that blends the
strengths of both these strategies, by embedding neural networks explicitly into the GP ODE structure,
termed Neural Galerkin projection. We demonstrate this approach on the isentropic Navier-Stokes
equations for compressible ﬂow over a cavity at a moderate Mach number. When provided the
structure of the projected equations, we show that the Neural Galerkin approach implicitly learns
stable ODE coefﬁcients from POD coefﬁcients and demonstrates signiﬁcantly longer and accurate
time horizon predictions, when compared to the classical POD-GP assisted by calibration. We observe
that the key beneﬁts of this differentiable programming-based approach include increased ﬂexibility
in physics-based learning, very low computational costs, and a signiﬁcant increase in interpretability,
when compared to purely data-driven neural networks.

Keywords Reduced Order Modeling · Differentiable Programming · Neural Networks

1

Introduction

Nonlinear dynamical systems pervade several natural phenomena, with the most popular example being turbulence in
ﬂuids. Turbulent ﬂow governs the efﬁciency and performance of several engineering systems where drag reduction,
noise suppression and improved fuel efﬁciency are critical parameters of interest. Naturally, controlling turbulence with
ﬂow control devices has been an active area of research in recent years, to design better aircraft, vehicles, and energy
systems [Gad-el Hak, 2001, Bons et al., 2005]. However, controlling a chaotic and nonlinear system like turbulence
requires making predictions several time instants into the horizon in a fast, cheap and accurate manner. A model

 
 
 
 
 
 
arXiv Template

A PREPRINT

satisfying these characteristics is called a reduced order model or ROM [Noack et al., 2011, Luchtenburg et al., 2010],
since CFD techniques require computational resources which are prohibitive for on-board control hardware in these
applications. ROMs are a crucial component of the overall ﬂow control strategy [Ito and Ravindran, 1998, Ma et al.,
2011, Ravindran, 2000], since such predictions are inputs to the feedback control algorithms that operate the actuator
by the desired amount, over a designated time interval. Therefore, research into ROMs of turbulence has relied on
data-driven strategies and approximations, since speed and efﬁciency of ROMs take precedence over high-ﬁdelity
solutions. One of the most popular strategies in literature involves the Galerkin projection of the Navier-Stokes equations
onto truncated orthogonal basis vectors (hereafter termed GP-ROM), such that the projected equations can be solved
quickly, while capturing the dominant physics of the ﬂow [Rempfer, 2000, Rowley et al., 2004, Lorenzi et al., 2016].
The GP-ROM is based on the Navier-Stokes equations and is fast, efﬁcient, with low computational costs and thus can
be accommodated with feedback control algorithms. However, current GP-ROMs suffer from several issues of accuracy
and stability over long-time horizons, which are required for effective control [Iollo et al., 2000, Balajewicz and Dowell,
2012, Amsallem and Farhat, 2012, Akhtar et al., 2009].

Recent efforts in literature have explored alternatives to GP-ROM with deep learning approaches such as neural
networks (NNs). Several variants of NNs exist, with the LSTM architecture [Hochreiter and Schmidhuber, 1997] being
popular in turbulence [Mohan and Gaitonde, 2018, Ahmed et al., 2020, den, Maulik et al., 2020a, Chattopadhyay et al.,
2020] owing to its strengths in time-series prediction. Although NNs have been reported to have considerable promise,
they face important limitations to practical use due to their black-box nature and lack of interpretability. In this work,
we present a hybrid approach that combines Galerkin Projections with deep learning using Differentiable Programming
(henceforth referred to as DiffProg). DiffProg is a programming paradigm that enables writing software that can be
fully differentiated via automatic differentiation [Griewank et al., 1989, Bartholomew-Biggs et al., 2000, Baydin et al.,
2018], and optimized using gradient-descent based methods. While the idea behind DiffProg is not new [Bischof et al.,
1996, Yu and Blair, 2013, Utke et al., 2008] it has seen renewed interest due to recent efforts in interpreting deep neural
networks (NNs) through the lens of DiffProg [Abadi and Plotkin, 2019, Chizat et al., 2018, Li et al., 2018, Wang and
Rompf, 2018], with the most prominent examples being Neural ODEs [Chen et al., 2018]. The initial successes of NNs
in addressing scientiﬁc problems has made DiffProg an attractive avenue of research [Innes et al., 2019, Hu et al., 2019,
Rackauckas et al., 2020], since NNs can be viewed as an application of DiffProg to speciﬁc learning algorithms. In
this work, we present an approach called Neural Galerkin Projection (NeuralGP) which exploits DiffProg to achieve
superior accuracy and long term stability. This is achieved by casting the traditional GP-ROM as a DiffProg problem
where neural networks are directly embedded inside the equations, and trained on CFD data. The resulting NeuralGP
framework Section 2 outlines the Galerkin Projection approach as applied to ﬂow over a cavity. Section 3 provides
an introduction to Differentiable programming in the context of ordinary differential equations, and our extensions to
incorporate it into Galerkin Projections. The results from this new approach are presented in Section 4 and compared
with the standard GP-ROM approach, where we demonstrate superior accuracy in the predicted POD coefﬁcients and
their stability over longer time horizons. Finally, in Section 5, we discuss the strengths, novelty and future of adopting
DiffProg in ROMs of turbulence.

2 Galerkin Projections for ROMs

The ﬁrst step in GP is to pre-process the ﬂow data with Proper Orthogonal Decomposition (POD). The basic idea of a
POD is to decompose the ﬂow ﬁeld into energy ranked coherent structures represented by mathematical modes [Berkooz
et al., 1993, Smith et al., 2005]. Let H denote any Hilbert space with an inner product denoted by (., .) and the norm
induced by the inner product denoted by (cid:107).(cid:107). In the ﬂuid dynamic context, the space H consists of functions deﬁned on
some spatial domain Ω ⊂ Rd (d is the spatial dimension) on which the ﬂuid evolves, usually H = L2(Ω). Let q(x, t)
be any vector of ﬂow variables, where x and t ∈ [0, T ] denote respectively the spatial variables and the time of the
numerical simulation or experimental measurements. We also assume a discrete sampling of our data in time and space.
We seek an expansion for q of the form

q(x, t) = q(x) +

n
(cid:88)

i=1

ai(t)φi(x),

(1)

where q denotes the average of the n ﬂow snapshots, such that the ﬂow is decomposed into an averaged component
and a ﬂuctuating component. The ﬂuctuating component is further represented by a POD expansion, which contains
n spatial modes φi and their temporal coefﬁcients ai(t). We truncate the number of POD modes and coefﬁcients in
Eqn. 1 by only retaining modes with the highest norm. This truncation serves as a lossy dimensionality reduction
tool, since we represent the ﬂow ﬁeld by just a few modes. These modes are assumed to represent the energetically
dominant ﬂow structures in the data, while the truncated modes account for energetically less important ﬂow features,
and consequently can be neglected. The reader is directed to the Appendix for a brief theoretical background of POD.

2

arXiv Template

A PREPRINT

The ROM is obtained by performing a Galerkin projection of the governing dynamics (in this case the Navier-Stokes
equations) onto the POD spatial modes φi. Two different ROMs for cavities have been proposed in Gloerfelt [2008],
one using the full compressible equations and the other with an isentropic approximation. In this work, we use the
isentropic equations for compressible ﬂows [Rowley et al., 2004], which is a valid physical representation for ﬂows
at moderate Mach number. Scaling the stream-wise and span-wise component of velocities u, v by the free stream
velocity U∞, the local sound speed c by the ambient sound speed c∞, the lengths by the cavity depth D, and time by
D/U∞, the equations are given by

ut + uux + vuy +

vt + uvx + vvy +

1
M 2
1
M 2

2
γ − 1
2
γ − 1

ccx =

(uxx + uyy)

(vxx + vyy)

1
Re
1
Re

ccy =

γ − 1
2

ct + ucx + vcy +

c(ux + vy) = 0

(2)

where M = U∞/a∞ is the Mach number and Re = U∞D/ν is the Reynolds number. If we denote q = (u, v, c) the
vector of ﬂow variables, the above equations can be recast as

˙q =

1
Re

L(q) +

1
M 2 Q1(q, q) + Q2(q, q)

with

(3)

L(q) =

(cid:35)

(cid:34)uxx + uyy
vxx + vyy
0

, Q1(q1, q2) = − 2
γ−1





c1c2
x
c1c2
y
0


 , Q2(q1, q2) = −







u1u2
u1v2
u1c2
2 c1(u2

x + v1u2
y
x + v1v2
y
x + v1c2
y
x + v2
y)







+ γ−1

To obtain the ROM by means of a GP we deﬁne an inner product on the state space as

(q1, q2)Ω =

(cid:90)

Ω

(u1u2 + v1v2 +

2α
γ − 1

c1c2) dΩ

where α is a constant and γ is the ratio of speciﬁc heats. In this work, we choose the value of α = 1, which gives the
deﬁnition of stagnation enthalpy while calculating the norm. The above deﬁnition ensures the stability of the origin of
the attractor as demonstrated by Rowley et al. [2004]. We use expansion in Eqn. 1 to perform the GP of the isentropic
equations onto the ﬁrst n (cid:28) NP OD spatial eigenfunctions, to obtain the ROM as

˙aR
i (t) =

1
Re

C 1

i +

1
M 2 C 2

i + C 3

i +

n
(cid:88)

j=1

(cid:18) 1
Re

L1

ij +

1
M 2 L2

ij + L3
ij

(cid:19)

aR
j (t)

+

n
(cid:88)

j,k=1

= Ci +

(cid:18) 1
M 2 Q1

ijk + Q2

ijk

(cid:19)

j (t)aR
aR

k (t)

n
(cid:88)

j=1

LijaR

j (t) +

n
(cid:88)

j,k=1

QijkaR

j (t)aR

k (t)

, aR(t))
= fi(Ci, Li, Qi
(cid:125)
(cid:124)

(cid:123)(cid:122)
y

= f (y, aR(t))

(4)

where fi is a polynomial of degree 2 in aR and the coefﬁcients of the projection are given by

C 1
i = (φi, L(q))Ω
i = (φi, Q1(q, q))Ω
i = (φi, Q2(q, q))Ω

C 2
C 3

L1

ij = (φi, L(φj))Ω
ij = (φi, Q1(q, φj) + Q1(φj, q))Ω
ij = (φi, Q2(q, φj) + Q2(φj, q))Ω

L2
L3

Q1
Q2

ijk = (φi, Q1(φj, φk))Ω
ijk = (φi, Q2(φj, φk))Ω

3

arXiv Template

A PREPRINT

3 Differentiable Programming for Galerkin ROM

3.1 Overview

In recent years, differentiable programming has taken a center-stage in scientiﬁc ML applications, such as interpreting
neural networks from the perspective of differential equations and physics, especially as applied to dynamical sys-
tems [Cessac, 2010]. A popular example of this is the Neural Ordinary Differential Equations (NODE) [Chen et al.,
2018]. To brieﬂy understand the key idea underpinning NODE (and consequently DiffProg), consider a derivative g that
is deﬁned as a function of some independent variables. Let us assume that the true form of g, denoted by ˆg, is unknown,
since this scenario is commonly observed in several scientiﬁc and real-world problems. When formulating this as a
deep learning problem, g is the function that is represented by a NN, which upon training and convergence should
approximate the true function ˆg. In essence, we are approximating the derivative of an unknown function with a NN.

dy
dx

= g(x, y)

(5)

In order to appreciate the connection between this derivative function and NODE, we brieﬂy review the standard
residual network and recurrent network architectures, which have been popular in deep learning. These NNs are
composed by stacking upon several neural layers known as hidden layers. The outputs of these layers are called hidden
states h, and can be written in the form

ht+1 = ht + f (ht, θt)
(6)
where time t ∈ {0...T } and ht ∈ RD, while θt are the trainable parameters in the NNs. The fundamental premise of
NODE is that the iterative updates to h can be thought of as an Euler discretization of a continuous transformation [Haber
and Ruthotto, 2017, Chang et al., 2018, Behrmann et al., 2019]. This idea is further exploited when we add more layers
and take smaller steps, such that in the limit, the expression above can be written in continuous dynamics as an ODE:

dh(t)
dt

= f (h(t), t, θ).

(7)

This ODE now represents the rate of change of the hidden states (and hence, their dynamics) as a parameterized
function f with parameters t and θ. Since time t ∈ {0...T } in a residual or recurrent NN is “unrolled" through several
hidden layers, solving for h(t) now requires solving an ODE with initial condition h(0), which is input at t = 0.
Consequently, the solution of the ODE at time T is the solution of output at the ﬁnal layer h(T ). Framing the NN
problem as an ODE has a signiﬁcant advantage since solving ODE problems is a mature area of scientiﬁc computing,
with several ODE solvers available that can solve these equations in an efﬁcient, accurate manner. Several adaptations
in modern ODE solvers exist, such as keeping the solution error bounded across time and explicit control over the
level of accuracy by changing the numerical method. Importantly, the computational cost of these ODE solvers is low
enough that they can be quickly solved on commodity computing hardware. This is in contrast to residual and recurrent
NNs, which require progressively higher computational and memory requirements as T increases, since this leads to
a corresponding increase in h(t). Recurrent NNs are especially notorious for their signiﬁcant memory requirements
when unrolling the hidden states for long periods of time, and need dedicated high performance computing resources. A
comparison of accuracy and speed between NODE and recurrent NNs can be found in Maulik et al. [2020b].

Though the beneﬁts are signiﬁcant, a key roadblock exists in practice. Since f is now approximated by a NN,
analogous to Eqn. 5, incorporating a NN inside an ODE solver requires training it with a backpropagation strategy;
while simultaneously solving the ODE with numerical strategies like the Euler method.
It is apparent that this
requires backpropagation through the ODE solver, which amounts to gradient computation via reverse-mode automatic
differentiation (AD). This is an important bottleneck since employing the standard reverse-mode approach commonly
used in NNs to an ODE, results in high memory costs and is slow, in addition to lack of guaranteed accuracy limits. The
key contribution by Chen et al. [2018] circumvents this by computing the gradients via the adjoint sensitivity method
of Boltyanskiy et al. [1961]. To illustrate its operation, consider a scalar-valued loss function L, which is typical in
computing reverse-mode AD with chain rule. The loss is the output of the predictions by the ODE solver such that

(cid:18)

L(z(t0)) = L

z(t0) +

(cid:90) t1

t0

(cid:19)

f (z(t), t, θ)dt

= L(ODESolve(z(t0), f, t0, t1, θ))

(8)

The learning problem requires gradients to be computed with respect to parameters θ, to minimize loss L. Since these
gradients depend on the hidden state z(t), the adjoint to be computed can be written as a(t) = ∂L
∂z(t) , which can then
be represented as another ODE

da(t)
dt

= −a(t)T ∂f (z(t), t, θ)

∂z

4

(9)

arXiv Template

A PREPRINT

Figure 1: Schematic of NeuralGP training with embedded NNs in the Galerkin Projection ODE

Finally, gradients of L with respect to parameters θ are computed via

dL
dθ

= −

(cid:90) t0

t1

a(t)T ∂f (z(t), t, θ)

∂θ

dt

(10)

We conclude this brief description of NeuralODE with its underlying philosophy of blending adjoint methods in ODEs
with backpropagation, for interoperability between neural networks and ODEs. We encourage the reader to the works
of Chen et al. [2018], Rubanova et al. [2019], Zhang et al. [2019], Yan et al. [2019] and Rackauckas et al. [2020] for a
detailed treatment of the mathematical intricacies and implementations of NODE, as it is beyond the scope of this paper.

3.2 Our Approach: Neural Galerkin Projections

With a brief description of the Neural ODE approach above, we now re-direct our attention to the GP formulation
described in Section 2. The equations to compute GP with POD modes can be written in ODE form as

˙aR
i (t) = Ci +

N
(cid:88)

j=1

LijaR

j (t) +

N
(cid:88)

j,k=1

QijkaR

j (t)aR

k (t)

(11)

The GP formulation gives rise to C (Constant), L (Linear) and Q (Quadratic) terms, that are determined via an
optimization and calibration process from the data. We remark that the data-driven nature of Eqn. 11 while being
structured as an ODE, makes it an excellent candidate to be instead cast as a DiffProg problem. With DiffProg, the
“unknown" operators in the equations are represented by a NN of appropriate dimensionality, and the adjoint-based
backpropagation learns this NN from the data. Speciﬁcally, we have seen that C and L are the coefﬁcients that have
signiﬁcant impact on the accuracy and long-time stability of GP [Nagarajan et al., 2013], and need considerable manual
intervention in the form of calibration. Therefore, our intention is to automate discovery of these coefﬁcients with
DiffProg, and directly using the aR
i (t) from the DNS as the training data. This is done by substituting the coefﬁcients C
and L with individual NNs. The NNs take as input a vector f (q), which is dependent on the ﬂow variables q which can
include velocity, density and pressure. The resulting NeuralGP setup is now written as

C θ = N N C
Lθ = N N L

θ (f (q)C)
θ (f (q)L)
N
(cid:88)

j=1

i (t) = C θ
˙aR

i +

Lθ

ijaR

j (t) +

N
(cid:88)

j,k=1

QijkaR

j (t)aR

k (t)

(12)

(13)

(14)

Where C θ refers to the unknown C that is now parameterized with a NN having parameters θ, and similarly for Lθ.
Thus, in the forward pass of the NeuralGP, both NNs are provided their respective inputs f (q) and predict the C and
L coefﬁcients, upon which Eqn. 14 is solved with standard ODE solvers to make predictions. In the backward pass,
the adjoint method described above trains the NNs C θ and Lθ. At this juncture, its useful to discuss some salient
advantages of the NeuralGP formulation by comparing Eqn. 11 to Eqn. 14

• Both Cij and Lij have their own NN models, but they interact in a physical manner governed by the projected

Navier-Stokes equations.

• DiffProg allows for NN models to be backpropagated seamlessly thru the ODE solver via adjoint sensitivity
analysis and a combination of forward/reverse mode Automatic Differentiation (AD). The resulting abstract,

5

arXiv Template

A PREPRINT

Figure 2: Cavity conﬁguration and computational domain.

powerful framework allows for arbitrary neural network models to be tightly integrated within the context of
the well-known GP equations, which are interpretable.

• The choice of the input vector f (q) is ﬂexible and user deﬁned, as it can be an initial guess based on dependency
relationship between the coefﬁcients and the ﬂow physics, or even be random matrix - in which case, it acts as
an “input seed" for the NN to make a prediction.

These inherent theoretical advantages of NeuralGP imply superior capability to the standard GP and we present evidence
of this in the results below.

4 Results

The objective of this work is to demonstrate the performance, accuracy and effectiveness of the proposed NeuralGP
method. The ﬁrst step in this pursuit is to have benchmark results obtained with the standard GP approach, on the
same dataset. In this section, we ﬁrst construct a GP-ROM using the standard approach found in literature [Rowley
et al., 2004, Nagarajan et al., 2013], using a combination of optimization and calibration. The dataset chosen is a
2D DNS dataset of cavity ﬂow as shown in Fig. 2. The cavity is of an Le/D ratio of 2. The ﬂow is initialised by a
laminar boundary layer so as to have a thickness of δ/D = 0.28 at the leading edge of the cavity. The Reynolds number
of the ﬂow based on the cavity depth is 1500 and the ﬂow Mach number is 0.6. The equations of Navier-Stokes are
discretised using a 4th order accurate scheme in time and space. This ﬂow demonstrates vortex shedding from the
edge of the cavity and is a canonical abstraction of engineering applications in aeroacoustics. This vortex shedding
has several instabilities which trigger a strong acoustic response, and are known as Rossiter modes. Suppression and
manipulation of these modes have been a focus of several research efforts in aerodynamics, with a famous example
being the payload bay of an aircraft which has a strong acoustic signature during ﬂight [Stanek et al., 2000, Levasseur
et al., 2008, Williams et al., 2007]. Therefore, this case acts as a good representative of several engineering ﬂows that
can beneﬁt from feedback control problems. The differentiable programming computations are performed in the Julia
programming language [Bezanson et al., 2017] with the DiffEqFlux [Rackauckas et al., 2019] package, as it offers tight
integration between adjoint solvers in ODEs and automatic differentiation required to train neural networks.

4.1 Galerkin Projection ROMs of Cavity Flow

Snapshots are taken once the ﬂow has stabilised and 56 snapshots are uniformly sampled which corresponds to 1 period
of the ﬂow oscillation (2.75 in non dimensional time) corresponding to the ﬁrst Rossiter mode [Delprat, 2006]. Figure 3
demonstrates a degenerate eigen-spectrum showing eigenvalues σj which occur in pairs. Also the ﬁrst 4 eigenmodes
capture around 98.5% of the total ﬂuctuation energy as shown by the Relative Information Content (RIC) deﬁned
as RIC(i) = (cid:80)i
j=1 σj, where M is the total number of eigenvalues. The temporal coefﬁcients for the
dominant 6 POD modes are shown in ﬁgure 4, which displays a phase difference of π/2 between the paired modes. The
representation of the ﬁrst 4 spatial POD modes for the vorticity is shown in ﬁgure 5 where they occur in pairs. While
their values are distinct, we note that their representation is topologically equivalent. These topological features are

j=1 σj/ (cid:80)M

6

bbrdld1d2d3uoxyarXiv Template

A PREPRINT

Figure 3: Real part of the eigen values and Relative Information Content (RIC)

Figure 4: First 6 POD temporal coefﬁcients

representative of the low frequency dynamics of the ﬂow vorticity, which is a hydrodynamic phenomenon of interest.
The key issue plaguing GP ROM in Eqn. 11 is its intrinsic tendency to converge to the wrong attractor when used to
simulate the ﬂow system for long time periods [Amsallem and Farhat, 2012, Barone et al., 2009, Huang et al., 2016].
This is clearly seen in Figure 6 which shows the phase portraits of the GP-ROM predicted POD temporal coefﬁcients a1
with a2, and a1 with a4. We can observe that the phase portraits quickly become unstable, as a result of errors in ai,
thereby creating signiﬁcant barriers in using the GP-ROM for feedback control, since they require stable predictions
over some extended time horizon.

A major reason for the inaccurate behavior of the ROM can be attributed to the truncation of the POD bases where the
dissipative scales of the higher POD modes are neglected. An analogous problem occurs in the Large Eddy Simulation
(LES) of ﬂows where there is lack of dissipation due to the smaller scales. Even including all the modes in Galerkin
projection may still lead to the wrong attractor due to structural instability as has been demonstrated in Rempfer [2000].
Other problems may arise due to the contribution of pressure at the boundaries of the domain, which is usually neglected
[Noack et al., 2005]. The stability properties of the compressible POD-Galerkin approximation has been studied by
Iollo et al. [2000]. Hence, there is a need to identify the temporal coefﬁcients so as to minimize the error between the
actual time coefﬁcients aP
i (t) using a suitable norm for the error.
A typical solution in literature is to add an additional “calibration" step over the GP-ROM. In this step, we have to ﬁnd
the coefﬁcients of the dynamical system such that the error e1 is minimized under the constraints that the coefﬁcients
Ci, Li, Qi (i = 1, · · · , n) satisfy Eqn 4, where e1(f , t) = aP (t) − aR(t). This leads to a non-linear constrained
optimization problem which solved with weighted Tikhonov regularization, the details of which are presented in
Appendix and in Nagarajan et al. [2013]. The phase portraits for the same coefﬁcients in Fig. 6 after calibration
are shown in Fig. 7, demonstrating stable predictions for the individual coefﬁcients. This provides conﬁdence in the
calibration, and we now make predictions for long time horizons, as shown in Fig. 8. Since we are dealing with
statistically stationary ﬂow it is natural to expect the temporal dynamics be valid for a time longer than the period of

i (t) from the POD and that obtained from the ROM aR

7

01020304050x-8-7-6-5-4-3-2-10yleg1405060708090100zleg20123456x-0,2-0,100,10,2ya1a2a3a4a5a6arXiv Template

A PREPRINT

(a) mode 1

(b) mode 2

(c) mode 3

(d) mode 4

Figure 5: Vorticity contours of the ﬁrst 4 POD modes. 15 contours in the range [−5, 1.67] are plotted.

snapshot acquisition i.e. one cycle. As shown by Sirisup and Karniadakis [2004], even if the system is initialized with
correct state, the solution may drift away for a long period of integration despite calibration. Figure 8 shows that the
calibrated model predicts ai(t) for a non-dimensional time t = 11, which corresponds to around 4 cycles of the ﬂow
period. While this is a marked improvement over uncalibrated coefﬁcients, we notice that the predictions still diverge
rapidly after 4 cycles. The reason is that the neglected modes which contribute to the regularization of the system are
not modeled during calibration. The validity of ROM for a longer time of integration is still an open question, since the
coefﬁcients need to be able to model the neglected modes. While performing control, since the time period where the
control law is applicable is much larger than the period of validity of the model, we calibrate the coefﬁcients for more
than one cycle of the ﬂow.

4.2 Neural Galerkin Projection ROMs of Cavity Flow

We now present results for the NeuralGP approach, as applied to the cavity ﬂow problem desribed above. The NeuralGP
ODE is constructed consistent with Eqn. 14, where the C and L terms are represented by fully connected NNs taking
f (q)C and f (q)C respectively, as input. Since the NN learns these terms based on the relationship between f (q) with
aP (t), we have a choice of describing f (q). In this work, we choose f (q)C and f (q)L to be non-zero random matrices.
Using random matrices as input to the NeuralGP is analogous to the generative learning problem, such as the ubiquitous

8

0123 1 0.500.511.52vort: 5 1.6651.670123 1 0.500.511.52vort: 5 1.6651.670123 1 0.500.511.52vort: 5 2.141431.670123 1 0.500.511.52vort: 5 2.141431.67arXiv Template

A PREPRINT

Figure 6: Phase Portraits of mode a1 with mode a2 and mode a1 with mode a3 for GP-ROM predictions without
Calibration. GP-ROM: (solid line), POD: (black o)

Figure 7: Phase Portraits of mode a1 with mode a2 and mode a1 with mode a3 for GP-ROM predictions with Calibrated
Coefﬁcients. GP-ROM: (solid line), POD: (black o)

GANs [Goodfellow et al., 2014] where a latent random matrix serves as a “noise" vector that the NN samples to learn a
probability distribution. As such, it is important we save these matrices, since it will serve as the input seed for the
trained NeuralGP network when making predictions. We note that the input can be any physically motivated quantity as
well, if it is available.
To prototype the validity of the NeuralGP method, we attempt to learn from a training data aP (t) corresponding to one
cycle t ≈ 2.75s and predict the same cycle. On the surface, this seems like a contrived case since the learning process
is considerably easier due to likely over-ﬁtting of the training data. However, the goal of the prototyping phase is to
create a “sanity" test to check if the NN architecture and the learning problem are setup to learn parameters of interest
to making predictions. It is entirely possible to construct NNs that are incapable of accurate predictions due to the
learning problem being set up in an ambiguous and inconsistent manner, irrespective of the computing horsepower
employed. A generally accepted heuristic sanity check for NNs is to attempt deliberate over-ﬁtting of the data, as it
is the simplest learning problem for the NN; compared to the extremely challenging problem of generalization and
prediction on out-of-sample test data. If we ﬁnd that the predictions are accurate, it gives us reason to be optimistic
about out-of-sample prediction by avoiding over-ﬁtting. Conversely, failure in this contrived problem may indicate
fundamental limitations in the NN architecture or learning problem formulation, which need to be addressed before
proceeding to harder problems. In this work, where we aim to predict time series, the training data is one cycle of the
ﬂow in the cavity and the out-of-sample test data is the subsequent cycles in the future. Since the time series is from a

9

a1a2 0.15 0.1 0.0500.050.10.15 0.15 0.1 0.0500.050.1a1a3 0.15 0.1 0.0500.050.10.15 0.04 0.0200.02a1a2 0.1 0.0500.050.1 0.1 0.0500.050.1a1a3 0.1 0.0500.050.1 0.02 0.0100.010.02arXiv Template

A PREPRINT

Figure 8: GP-ROM predictions of ai(t) with Calibrated Coefﬁcients

stationary dynamical system, all cycles are qualitatively similar, but quantitatively different, requiring that the NN learn
its statistical properties to generalize.

Figure 9 shows the results of prototyping sanity test, with the predicted and the training a(t) plotted for the 6 dominant
POD modes. We see that the predictions are extremely good, with an almost perfect match with the training data. This
indicates that the NeuralGP indeed are capable of learning the data with great accuracy, and thus provides conﬁdence
that our architecture and learning problem are appropriate. With this validation, we proceed to the next step which is of
real consequence to engineering problems, namely the prediction of cycles beyond the training horizon. To evaluate
this predictive capability, we again train NeuralGP with one cycle as shown above, and predict 5 cycles ahead. This is
representative of ﬂow control applications where the system ROM predicts several cycles in advance, for the control
algorithms that construct actuation responses to keep the ﬂow in the desired state. The prediction results for the 6
dominant a(t) are shown in Fig. 10 where 5 cycles ≈ 12.75s. Here we notice that the predictions of modes 1-4 are
remarkably stable for almost the entire horizon. In cases of modes 5 and 6, the instability propagates after ≈ 5s, which
corresponds to almost two cycles. It is important to note that unlike the standard GP ROM, we need not perform any
explicit manual calibration of the coefﬁcients. Instead, the NeuralGP automatically learns the best coefﬁcients that
are consistent with both the data and the GP equation structure. To provide further context, GP-ROMs of dynamical
systems place heavy emphasis on the accuracy of the most signiﬁcant a(t) (ranked by eigenvalues) as they have a
greater impact on the overall ROM and control system than the other modes. In fact, a successful GP-ROM based
feedback control system for the cavity problem in this work was previously demonstrated in Nagarajan et al. [2013]
with just POD modes 1-4. From this perspective, we observe that NeuralGP-ROM can easily exceed performance and
accuracy metrics already established for control applications, with minimal intervention.

At this juncture, we remark that turbulence ROMs by virtue of construction - whether data-driven, physics-based or a
hybrid combination (like the NeuralGP) - typically neglect some physics of the dynamical system they seek to represent.
In case of complex, chaotic systems like turbulence, the challenge is to choose which physical features to retain in
the ROM, and the features to neglect/truncate. While this truncation is necessary to construct efﬁcient, parsimonious
ROMs, it causes instabilities in the long time evolution of the dynamical system. As a result, any truncated ROM is
eventually expected to become inaccurate and degenerate into an unstable solution at some point in its evolution. As a
result, a key metric of evaluation is when the ROM destabilizes as this acts as a measure of its quality. We aim to study
this for NeuralGP-ROM, noting from Fig. 8 that the existing GP-ROM with calibration destabilizes at t ≈ 11s. To this
end, the trained NeuralGP is instead used to predict 20 cycles i.e. 55s into the future for all 6 dominant a(t), and the
results are presented in Fig. 11. Interestingly, we observe remarkably stable predictions for POD modes 1 and 2 until
t ≈ 28s, which corresponds to ≈ 10 cycles before rapidly diverging. This trend is also seen in POD modes 3 and 4,
where stability is reached for as long as 9 − 10 cycles before diverging. As expected from the previous results, the
prediction is weaker for modes 5 and 6, where stability is observed for ≈ 3 cycles, after which it destabilizes - ﬁrst
gradually and then exponentially further into the horizon. While this may seem unimpressive at ﬁrst glance, we reiterate

10

arXiv Template

A PREPRINT

that stable ROM based controls have been demonstrated for this problem with only modes 1 − 4 in Nagarajan et al.
[2013] as they capture a signiﬁcant percentage of the ﬂow structures.

Nevertheless, the inclusion of the higher modes 5 and 6 in this work is to illustrate a key challenge in predicting higher
modes with similar levels of accuracy. A major reason is the large variance in amplitudes of these 6 modes relative to
each other. From Fig. 9, we see that the amplitude of mode 6 is almost two orders of magnitude smaller than mode
1, while mode 3 is itself an order of magnitude apart from both of them. This aspect is crucially relevant due to the
construction of the loss function in NNs, since it is an aggregate quantity summed or averaged over all the output
quantities in any given batch. For example, in RMS loss function for N modes (where N = 6), (cid:80)i=N
(t),
the discrepancy between the predicted and training data is summed over to a scalar quantity. This is necessary since the
standard form of reverse mode automatic differentiation; for backpropagation in NNs needs a scalar value to compute
derivatives. As a result, the scalar loss has more contribution from the modes with relatively larger magnitude (modes
1 − 4) than modes 5 − 6 and the NNs learn coefﬁcients that prioritize the dominant modes. Future work will explore
methods to improve accuracy in higher modes by modifying the structure of the Eqn. 14, for applications that require
superior resolution of higher modes. However, since most engineering applications emphasize dominant modes, the
results demonstrate superiority of the NeuralGP approach, since its prediction horizon is stable for three times longer
than even the calibrated GP-ROM without any additional training dataset, or manual parameter tuning.

i (t)−atrain

i=1 aR

i

5 Conclusion

It has been widely reported in literature that predictions from Galerkin-Projection based reduced order models (ROMs)
of turbulence suffer from lack of long-term stability and desired accuracy, despite the approach being theoretically
derived as a data-driven approximation of the Navier-Stokes equations. This can be a serious issue, since these ROMs
are used for a variety of applications ranging from feedback control to surrogate modeling for rapid decision making.
Despite the success of machine learning (ML) based ROMs in the recent years, its black-box nature has raised concerns
about interpretability and conﬁdence in its predictions. In this work, we have proposed a hybrid ROM blending both
ML and Galerkin projection - coined the Neural Galerkin Projection (NeuralGP), by expressing the Galerkin projection
onto the Navier Stokes equations as a Differentiable Programming problem. The NeuralGP learns the coefﬁcients
of the Galerkin projection ODE directly from data by embedding NNs inside the ODE. The NNs can be standard
dense layers with user deﬁned choice of parameters, learning rates and optimizers, while still adhering to the physics
of the Galerkin ODE. This is in contrast to the existing Galerkin ROMs, which require manual calibration of the
coefﬁcients from the data. The results show that in contrast to a fully ML based “black-box" ROM, the NeuralGP-ROM
is extremely interpretable with its strong physics foundations, while being more accurate than traditional Galerkin
ROMs. Speciﬁcally, predictions demonstrate that NeuralGP-ROMs indeed generalize better and can predict as long
as 10 cycles into the future, despite training on only a single cycle. Furthermore, the computational cost of training
is extremely low since retention of the Galerkin ODE structure avoids the need of expensive, complex deep NN
architectures like LSTMs and CNNs as black boxes for the entire ODE which requires several GPUs to train [Mohan
and Gaitonde, 2018, Mohan et al., 2019, 2020]. As a result, training took just over an hour on a desktop grade CPU. An
attractive beneﬁt of hybrid ML ROM; despite its theoretical sophistication, is that the underlying equation is still an
ODE that can be solved with mature, standard ODE solvers. This allows us to leverage and improve existing ODE-based
feedback control infrastructure by seamlessly replacing GP-ROM with NeuralGP-ROM, with almost no changes to
the control system elsewhere. The NeuralGP is extremely ﬂexible as it is based on the differentiable programming
paradigm, which allows us to add arbitrary terms to the ODE based on our partial knowledge of the physics, while
learning the unknown physics with neural networks [Gelbrecht et al., 2021]. In essence, the NeuralGP is an attempt to
bring together the best of both worlds from Galerkin projections and deep learning, and the approach can be further
extended to include additional physics constraints and even effects of truncated modes.

6 Acknowledgements

This work has been co-authored by employees of Triad National Security, LLC which operates Los Alamos National
Laboratory (LANL) under Contract No. 89233218CNA000001 with the U.S. Department of Energy/National Nuclear
Security Administration. A.T.M and D.L. were supported by the LDRD (Laboratory Directed Research and Devel-
opment) program at LANL under project 20190059DR. Computational resources were provided by the Institutional
Computing program at LANL. K.N. has been supported by Aeronautical Research and Development Board, India,
Project number 1912, titled “Development and application of ﬂow control techniques for ﬂow past a rectangular cavity
and circular cylinder". We also thank Chris Rackauckas for assistance with the DiffEqFlux.jl package.

11

arXiv Template

A PREPRINT

Figure 9: NeuralGP-ROM predictions of αi(t) for 1 cycle

7 Appendix

Proper Orthogonal Decomposition

This section expands on the POD representation shown in Eqn. 1, where any given ﬂow variable is expanded into an
average component q of the n ﬂow snapshots and a ﬂuctuating component. The ﬂuctuating component is represented
by its spatial modes φi(x) and the temporal coefﬁcients ai(t), which are determined by solving an eigenvalue problem
involving the time correlation tensor that minimises the average projection error:

E denotes the averaging operator, for instance the ensemble average (E = 1
i=1) and PS denote the projection
n
operator over the space S ⊂ H and dimension m. Note that the problem of minimizing E((cid:107)q − PSq(cid:107)) is equivalent to
maximizing E((cid:107)PS q(cid:107)2), the "energy" of the orthogonal projection since by Pythogoraus theorem we have (cid:107)q(cid:107)2 =
(cid:107)q − PS q(cid:107)2 + (cid:107)P q(cid:107)2. Solving the optimisation problem leads to an eigenvalue problem for the spatial modes φ

(cid:80)n

E ((cid:107)q − PSq(cid:107))

(15)

12

arXiv Template

A PREPRINT

Figure 10: NeuralGP-ROM predictions of αi(t) for 5 cycles

given by

where R : H → H is the linear operator deﬁned as

Rφ = λφ

(16)

R = E(qk ⊗ q∗
k)
Here q∗ ∈ H ∗ denotes the dual of q, given by q∗(.) = (., q) and ⊗ is the usual tensor product 1. One can easily verify
that R is self adjoint i.e. (R x, y) = (x, R y) and hence by the spectral theorem the eigenfunctions φ can be chosen to
be orthonormal, The eigenvalues λ are then determined by taking the inner product of equation (16) with φ to obtain
λ = E (cid:0)| (qk, φ) |2(cid:1)
The eigenvalues λ in the above equation represents the average energy in the projection of the ensemble onto φ
where energy is deﬁned in sense of the induced norm. We also conclude that the R is positive semi-deﬁnite, and the

(17)

(18)

1We have (u ⊗ v∗)(ψ) = u (ψ, v) , ∀u, v, ψ ∈ H.

13

arXiv Template

A PREPRINT

Figure 11: NeuralGP-ROM predictions of αi(t) for 20 cycles, with divergence observed at ≈ 10 cycles

eigenfunctions φj which maximise E((cid:107)PSqk(cid:107)2) are the eigenfunctions corresponding to the largest eigenvalues. Also
the eigenfunctions reproduce almost every member of the ensemble (except on a set of measure zero). One can also
verify that the range of the operator R is contained in the span of the ensemble qk and that any eigenfunction φ can be
written as linear combination of snapshots:

n
(cid:88)

φ =

ckqk

where the coefﬁcients ck ∈ R. If the average operator E over the snapshots qk is given as

k=1

E (f (q)) =

n
(cid:88)

k=1

αkf (qk)

where αk > 0 satisﬁes (cid:80)n
problem (16) may be written in terms of the coefﬁcients ck as

k=1 αk = 1 (usually giving equally weights to snapshots i.e. αk = 1/m). The eigenvalue

R c = λc

14

(21)

(19)

(20)

arXiv Template

A PREPRINT

where c = (c1, · · · , cm) and R is an m × m correlation matrix with Rij = αi (qj, qi). This is the usual method of
snapshots as described Sirovich [1987]. Here we note that the direct problem involves solving an eigenvalue problem
possibly on an inﬁnite dimensional space H which may be very large, on the other hand the method of snapshots
involve solving only an m dimensional eigenvalue problem and the method proves computationally efﬁcient if m is
small compared with the dimension of H.

GP-ROM Algorithm and Calibration

In this section we describe the method of calibration used in constructing the Galerkin Projection ROM, which employs
Tikhonov Regularization. In addition the deﬁnition of error e1(f , t) = aP (t) − aR(t), we can deﬁne the error in two
more forms a) A state calibration error e2 without the dynamical constraint given by

e2(y, t) = aP (t) − aP (0) −

(cid:90) T

0

f (y, aP (τ ))dτ

and, b) The ﬂow calibration error e3(f , t) = ˙aP (t) − f (aP (t)) where f is characterized by the coefﬁcients of Eqn. 4.
As demonstrated in Couplet et al. [2005], minimization of I 3(f ) = (cid:104)(cid:107)e3(f , t)(cid:107)2(cid:105)T leads to the solution of a linear
system for the coefﬁcients y deﬁning f . Unfortunately, this linear system is not well conditioned and leads to numerical
divergence when the calibrated coefﬁcients are used to integrate in time (4). For that purpose, Couplet et al. [2005]
have introduced a new cost functional deﬁned as a weighted sum of a term measuring the normalized error between
the behavior of the model (4) with f and with the coefﬁcients determined directly by Galerkin projection f GP and
another term linked to the distance between f and f GP . The value of the weighting factor which represents the
cost of calibration was chosen rather arbitrarily and hence was user dependent. In Cordier et al. [2010], a Tikhonov
regularisation method was suggested to improve the conditioning of the linear system. The idea of this method is to
seek a regularized solution yρ as the minimizer of the following weighted functional

Φρ(y) = (cid:107)Ay − b(cid:107)2

2 + ρ(cid:107)L (y − y0) (cid:107)2
2,

where the ﬁrst term corresponds to the residual norm, and the second to a side constraint imposed on the solution. L
represents the discrete approximation matrix of a differential operator of order d and y0 a reference solution. The
value ρ is chosen so as to compromise between the minimisation of the norm of the residual for the linear system and
the semi-norm of the solution. Here, the regularisation parameter ρ is determined by the classical L-curve method, as
described in Cordier et al. [2010]. We have the error deﬁned for the minimisation of the functional I i as

I i(y) = (cid:10)(cid:107)ei(y, t)(cid:107)2

Λ

(cid:11)

T

Usually the matrix Λ is chosen as Identity, which means that we give equal weights to all the modes in the deﬁnition
of calibration. However this matrix can be chosen in suitable way so as to include the effect of mode selection in the
deﬁnition. Two ways of deﬁning the weights can be proposed:

1. We consider that the main interest is in modelling the effect of the energetic structures and hence the eigen-
spectra themselves serve as a measure of the relative importance of the modes, which is the most natural choice
of the weights for the deﬁnition of error.

2. The error can be based on an overall sensitivity of the model with respect to a cost functional.

The weight matrix Λ for the deﬁnition of error for the ﬁrst case can be simply written as a diagonal matrix:

Λii =

σi
max σi

for

i = {1, · · · , N }

where σ is the singular value obtained as a solution of the eigenvalue problem of the time correlation matrix. For the
second case consider the state equations

˙aR = f (y, aR)

(22)

Variation of any convex cost functional J with respect to the state variables aR = {aR
of (22) as

i }N

i=1 gives the adjoint equation

(23)
i=1 is the adjoint variable. The overall sensitivity of the cost functional J with respect to aR is

˙ξR = g(y, aR, ξR)

Where ξR(t) = {ξR
obtained as

i }N

S =

dJ
daR = (cid:104)aR(t)ξR(t)(cid:105)

15

(24)

arXiv Template

A PREPRINT

where (cid:104).(cid:105) is any time averaging operator. We can then deﬁne the weight matrix Λ with respect to the sensitivity as

In this study we have taken the cost functional J based on the energy of the temporal modes as

Λii =

Si
max Si

for

i = {1, · · · , N }

J =

1
2

(cid:90) T

N
(cid:88)

0

i=1

(aR

i (t))2dt

(25)

The above functional is minimised subject to the constraint (22), by the method of Lagrange multipliers with the adjoint
state equation of (23) given by

˙ξ(t) = −ai(t) −

(cid:32)

Lij +

N
(cid:88)

i=1

N
(cid:88)

i=1

(Qjik + Qjki) ak(t)

ξj(t)

(26)

(cid:33)

References

Mohamed Gad-el Hak. Flow control: The future. Journal of aircraft, 38(3):402–418, 2001.

Jeffrey P Bons, Laura C Hansen, John P Clark, Peter J Koch, and Rolf Sondergaard. Designing low-pressure turbine
blades with integrated ﬂow control. In Turbo Expo: Power for Land, Sea, and Air, volume 47268, pages 1079–1091,
2005.

Bernd R Noack, Marek Morzynski, and Gilead Tadmor. Reduced-order modelling for ﬂow control, volume 528.

Springer Science & Business Media, 2011.

Dirk M Luchtenburg, Katarina Aleksi´c, Michael Schlegel, Bernd R Noack, Rudibert King, Gilead Tadmor, Bert Günther,
and Frank Thiele. Turbulence control based on reduced-order models and nonlinear control design. In Active Flow
Control II, pages 341–356. Springer, 2010.

Kazufumi Ito and Sivaguru S Ravindran. A reduced-order method for simulation and control of ﬂuid ﬂows. Journal of

computational physics, 143(2):403–425, 1998.

Zhanhua Ma, Sunil Ahuja, and Clarence W Rowley. Reduced-order models for control of ﬂuids using the eigensystem

realization algorithm. Theoretical and Computational Fluid Dynamics, 25(1):233–247, 2011.

Sivaguru S Ravindran. A reduced-order approach for optimal control of ﬂuids using proper orthogonal decomposition.

International journal for numerical methods in ﬂuids, 34(5):425–448, 2000.

D Rempfer. On low-dimensional galerkin models for ﬂuid ﬂow. Theoretical and Computational Fluid Dynamics, 14(2):

75–88, 2000.

Clarence W Rowley, Tim Colonius, and Richard M Murray. Model reduction for compressible ﬂows using pod and

galerkin projection. Physica D: Nonlinear Phenomena, 189(1-2):115–129, 2004.

Stefano Lorenzi, Antonio Cammi, Lelio Luzzi, and Gianluigi Rozza. Pod-galerkin method for ﬁnite volume approxima-
tion of navier–stokes and rans equations. Computer Methods in Applied Mechanics and Engineering, 311:151–179,
2016.

Angelo Iollo, Stéphane Lanteri, and J-A Désidéri. Stability properties of pod–galerkin approximations for the
compressible navier–stokes equations. Theoretical and Computational Fluid Dynamics, 13(6):377–396, 2000.

Maciej Balajewicz and Earl H Dowell. Stabilization of projection-based reduced order models of the navier–stokes.

Nonlinear Dynamics, 70(2):1619–1632, 2012.

David Amsallem and Charbel Farhat. Stabilization of projection-based reduced-order models. International Journal for

Numerical Methods in Engineering, 91(4):358–377, 2012.

Imran Akhtar, Ali H Nayfeh, and Calvin J Ribbens. On the stability and extension of reduced-order galerkin models in

incompressible ﬂows. Theoretical and Computational Fluid Dynamics, 23(3):213–237, 2009.

Sepp Hochreiter and Jürgen Schmidhuber. Long short-term memory. Neural computation, 9(8):1735–1780, 1997.

Arvind T Mohan and Datta V Gaitonde. A deep learning based approach to reduced order modeling for turbulent ﬂow

control using lstm neural networks. arXiv preprint arXiv:1804.09269, 2018.

Shady E Ahmed, Omer San, Adil Rasheed, and Traian Iliescu. A long short-term memory embedding for hybrid

uplifted reduced order models. Physica D: Nonlinear Phenomena, 409:132471, 2020.

16

arXiv Template

A PREPRINT

Romit Maulik, Bethany Lusch, and Prasanna Balaprakash. Non-autoregressive time-series methods for stable parametric

reduced-order models. Physics of Fluids, 32(8):087115, 2020a.

Ashesh Chattopadhyay, Pedram Hassanzadeh, and Devika Subramanian. Data-driven predictions of a multiscale
lorenz 96 chaotic system using machine-learning methods: reservoir computing, artiﬁcial neural network, and long
short-term memory network. Nonlinear Processes in Geophysics, 27(3):373–389, 2020.

Andreas Griewank et al. On automatic differentiation. Mathematical Programming: recent developments and

applications, 6(6):83–107, 1989.

Michael Bartholomew-Biggs, Steven Brown, Bruce Christianson, and Laurence Dixon. Automatic differentiation of

algorithms. Journal of Computational and Applied Mathematics, 124(1-2):171–190, 2000.

Atilim Gunes Baydin, Barak A Pearlmutter, Alexey Andreyevich Radul, and Jeffrey Mark Siskind. Automatic

differentiation in machine learning: a survey. Journal of machine learning research, 18, 2018.

Christian Bischof, Peyvand Khademi, Andrew Mauer, and Alan Carle. Adifor 2.0: Automatic differentiation of fortran

77 programs. IEEE Computational Science and Engineering, 3(3):18–32, 1996.

Wenbin Yu and Maxwell Blair. Dnad, a simple tool for automatic differentiation of fortran codes using dual numbers.

Computer Physics Communications, 184(5):1446–1452, 2013.

Jean Utke, Uwe Naumann, Mike Fagan, Nathan Tallent, Michelle Strout, Patrick Heimbach, Chris Hill, and Carl
Wunsch. Openad/f: A modular open-source tool for automatic differentiation of fortran codes. ACM Transactions on
Mathematical Software (TOMS), 34(4):1–36, 2008.

Martín Abadi and Gordon D Plotkin. A simple differentiable programming language. Proceedings of the ACM on

Programming Languages, 4(POPL):1–28, 2019.

Lenaic Chizat, Edouard Oyallon, and Francis Bach. On lazy training in differentiable programming. arXiv preprint

arXiv:1812.07956, 2018.

Tzu-Mao Li, Michaël Gharbi, Andrew Adams, Frédo Durand, and Jonathan Ragan-Kelley. Differentiable programming

for image processing and deep learning in halide. ACM Transactions on Graphics (TOG), 37(4):1–13, 2018.

Fei Wang and Tiark Rompf. A language and compiler view on differentiable programming. 2018.

Ricky TQ Chen, Yulia Rubanova, Jesse Bettencourt, and David Duvenaud. Neural ordinary differential equations.

arXiv preprint arXiv:1806.07366, 2018.

Mike Innes, Alan Edelman, Keno Fischer, Chris Rackauckas, Elliot Saba, Viral B Shah, and Will Tebbutt. A differen-
tiable programming system to bridge machine learning and scientiﬁc computing. arXiv preprint arXiv:1907.07587,
2019.

Yuanming Hu, Luke Anderson, Tzu-Mao Li, Qi Sun, Nathan Carr, Jonathan Ragan-Kelley, and Frédo Durand. Difftaichi:

Differentiable programming for physical simulation. arXiv preprint arXiv:1910.00935, 2019.

Christopher Rackauckas, Yingbo Ma, Julius Martensen, Collin Warner, Kirill Zubov, Rohit Supekar, Dominic Skinner,
Ali Ramadhan, and Alan Edelman. Universal differential equations for scientiﬁc machine learning. arXiv preprint
arXiv:2001.04385, 2020.

Gal Berkooz, Philip Holmes, and John L Lumley. The proper orthogonal decomposition in the analysis of turbulent

ﬂows. Annual review of ﬂuid mechanics, 25(1):539–575, 1993.

Troy R Smith, Jeff Moehlis, and Philip Holmes. Low-dimensional modelling of turbulence using the proper orthogonal

decomposition: a tutorial. Nonlinear Dynamics, 41(1):275–307, 2005.

Xavier Gloerfelt. Compressible proper orthogonal decomposition/galerkin reduced-order model of self-sustained

oscillations in a cavity. Physics of Fluids, 20(11):115105, 2008.

Bruno Cessac. A view of neural networks as dynamical systems. International Journal of Bifurcation and Chaos, 20

(06):1585–1629, 2010.

Eldad Haber and Lars Ruthotto. Stable architectures for deep neural networks. Inverse Problems, 34(1):014004, 2017.

Bo Chang, Lili Meng, Eldad Haber, Lars Ruthotto, David Begert, and Elliot Holtham. Reversible architectures for
arbitrarily deep residual neural networks. In Proceedings of the AAAI Conference on Artiﬁcial Intelligence, volume 32,
2018.

Jens Behrmann, Will Grathwohl, Ricky TQ Chen, David Duvenaud, and Jörn-Henrik Jacobsen. Invertible residual

networks. In International Conference on Machine Learning, pages 573–582. PMLR, 2019.

17

arXiv Template

A PREPRINT

Romit Maulik, Arvind Mohan, Bethany Lusch, Sandeep Madireddy, Prasanna Balaprakash, and Daniel Livescu. Time-
series learning of latent-space dynamics for reduced-order model closure. Physica D: Nonlinear Phenomena, 405:
132368, 2020b.

VG Boltyanskiy, Revaz Valer’yanovich Gamkrelidze, and Lev Semenovich Pontryagin. Theory of optimal processes.

Technical report, JOINT PUBLICATIONS RESEARCH SERVICE ARLINGTON VA, 1961.

Yulia Rubanova, Ricky TQ Chen, and David Duvenaud. Latent ordinary differential equations for irregularly-sampled

time series. 2019.

Han Zhang, Xi Gao, Jacob Unterman, and Tom Arodz. Approximation capabilities of neural ordinary differential

equations. arXiv preprint arXiv:1907.12998, 2(4), 2019.

Hanshu Yan, Jiawei Du, Vincent YF Tan, and Jiashi Feng. On robustness of neural ordinary differential equations.

arXiv preprint arXiv:1910.05513, 2019.

Kaushik Kumar Nagarajan, Laurent Cordier, and Christophe Airiau. Development and application of a reduced order
model for the control of self-sustained instabilities in cavity ﬂows. Communications in Computational Physics, 14:
pp–186, 2013.

Michael Stanek, Ganesh Raman, Valdis Kibens, John Ross, Jessaji Odedra, and James Peto. Control of cavity resonance

through very high frequency forcing. In 6th Aeroacoustics Conference and Exhibit, page 1905, 2000.

V Levasseur, P Sagaut, M Mallet, and F Chalot. Unstructured large eddy simulation of the passive control of the ﬂow in

a weapon bay. Journal of Fluids and Structures, 24(8):1204–1215, 2008.

David R Williams, Daniel Cornelius, and Clarence W Rowley. Supersonic cavity response to open-loop forcing. In

Active Flow Control, pages 230–243. Springer, 2007.

Jeff Bezanson, Alan Edelman, Stefan Karpinski, and Viral B Shah. Julia: A fresh approach to numerical computing.

SIAM review, 59(1):65–98, 2017.

Chris Rackauckas, Mike Innes, Yingbo Ma, Jesse Bettencourt, Lyndon White, and Vaibhav Dixit. Diffeqﬂux. jl-a julia

library for neural differential equations. arXiv preprint arXiv:1902.02376, 2019.

Nathalie Delprat. Rossiter’s formula: A simple spectral model for a complex amplitude modulation process? Physics of

ﬂuids, 18(7):071703, 2006.

Matthew F Barone, Irina Kalashnikova, Daniel J Segalman, and Heidi K Thornquist. Stable galerkin reduced order

models for linearized compressible ﬂow. Journal of Computational Physics, 228(6):1932–1946, 2009.

Cheng Huang, William E Anderson, Charles L Merkle, and Venkateswaran Sankaran. Investigation of the stability of
pod-galerkin techniques for reduced order model development. Technical report, AFRL/RQR Edwards AFB United
States, 2016.

Bernd R Noack, Paul Papas, and Peter A Monkewitz. The need for a pressure-term representation in empirical galerkin

models of incompressible shear ﬂows. Journal of Fluid Mechanics, 523:339, 2005.

Sirod Sirisup and George E Karniadakis. A spectral viscosity method for correcting the long-term behavior of pod

models. Journal of Computational Physics, 194(1):92–116, 2004.

Ian J Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville,

and Yoshua Bengio. Generative adversarial networks. arXiv preprint arXiv:1406.2661, 2014.

Arvind Mohan, Don Daniel, Michael Chertkov, and Daniel Livescu. Compressed convolutional lstm: An efﬁcient deep

learning framework to model high ﬁdelity 3d turbulence. arXiv preprint arXiv:1903.00033, 2019.

Arvind T Mohan, Dima Tretiak, Misha Chertkov, and Daniel Livescu. Spatio-temporal deep learning models of 3d

turbulence with physics informed diagnostics. Journal of Turbulence, 21(9-10):484–524, 2020.

Maximilian Gelbrecht, Niklas Boers, and Jürgen Kurths. Neural partial differential equations for chaotic systems. New

Journal of Physics, 23(4):043005, 2021.

Lawrence Sirovich. Turbulence and the dynamics of coherent structures. i. coherent structures. Quarterly of applied

mathematics, 45(3):561–571, 1987.

M Couplet, C Basdevant, and P Sagaut. Calibrated reduced-order pod-galerkin system for ﬂuid ﬂow modelling. Journal

of Computational Physics, 207(1):192–220, 2005.

Laurent Cordier, B Abou El Majd, and Julien Favier. Calibration of pod reduced-order models using tikhonov

regularization. International Journal for Numerical Methods in Fluids, 63(2):269–296, 2010.

18

