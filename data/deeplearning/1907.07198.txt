RayTracer.jl: A Differentiable Renderer that
supports Parameter Optimization for Scene
Reconstruction

Avik Pal
Computer Science and Engineering
Indian Institute of Technology Kanpur
avik.pal.2017@gmail.com

9
1
0
2

v
o
N
1

]

R
G
.
s
c
[

3
v
8
9
1
7
0
.
7
0
9
1
:
v
i
X
r
a

Abstract—In this paper, we present RayTracer.jl, a renderer in
Julia that is fully differentiable using source-to-source Automatic
Differentiation (AD). This means that RayTracer not only renders
2D images from 3D scene parameters, but it can be used to
optimize for model parameters that generate a target image in
a Differentiable Programming (DP) pipeline. Our renderer is a
complete general purpose renderer, which means that unlike most
previous work, we do not make any changes to the renderer to
make it differentiable. Additionally, we interface our renderer
with the deep learning framework Flux for use in combination
with neural networks. In this paper, we also demonstrate the use
of this differentiable renderer in rendering tasks and in solving
inverse graphics problems using gradients.

I. INTRODUCTION

Rendering is a technique of generating photo-realistic or
non-photo-realistic 2D projections from 3D objects. As such,
there are several algorithms for rendering complex scenes. One
of the most popular techniques for photo-realistic rendering is
ray tracing [1]. For real-time rendering we use techniques like
rasterization [2] are used.

Ray Tracing is a technique in computer graphics for
rendering 3D graphics with complex light interactions. In this
technique, rays are traced backward from the eye/camera to the
light source(s). The ray can undergo reﬂection and refraction
due to interactions with the objects in its path. This technique,
however, is very computationally expensive and hence difﬁcult
to do in real-time.

Since ray tracing leverages the properties of the materials of
the objects in the scene, a natural extension to the rendering
the exact properties of the
problem would be to extract
materials, lighting, and so on, given an image of a scene.
This task is known as inverse rendering. Calculating analytic
gradients for every single parameter of the scene is a very
tedious process and prone to errors. This has made it a
difﬁcult
task to present a general gradient-based inverse
rendering method. As such, there is only one framework in our
knowledge, redner [3], which has been able to do so by using
analytic gradients. However, we bypass this problem by using
AD. There have also been attempts at making rasterization
differentiable [4]; however, this involves making changes in
the core technique which is against our design principles.

Rendering is a computationally expensive technique, and so
it is generally done in static languages like C++. Developing
software in such languages are incredibly time-consuming.
Also, most languages lack the support of the state of the art
automatic differentiation tools like Zygote [5], Jax [6], which
are generally implemented for high-level languages like Julia
and Python. As such, it is challenging to develop differentiable
renderers in those languages and then interface with popular
deep learning software. Most of the other existing AD softwares,
which the authors are familiar with, like CasADi [7] does not
seamlessly integrate with packages, which means one needs to
rewrite the software to use speciﬁc AD tools.

In this paper, we explore the idea of differentiability through
a renderer, by leveraging the AD in Julia [8]. We present a
fully general renderer capable of handling complex scenes and
able to differentiate through them. We do not rely on analytic
gradients but use source-to-source AD to generate efﬁcient
gradient code in the backward pass. Our renderer contains very
little code for integration with Zygote, and hence, in theory,
we can plug in any other AD software written in Julia.

II. DIFFERENTIABLE RAY TRACING

There are several photo-realistic renderers available which
contain a vast amount of implicit knowledge. Differentiation
allows such renderers to make use of gradients to learn the
inverse mapping from an image to its parameter space. However,
as usual, it is challenging to compute efﬁcient derivatives from
a production-ready renderer, typically written in a performance
language like C++. This provides the primary motivation for
the development of RayTracer.jl. We develop an entire general-
purpose ray tracer in a high-level numerical computation
language. The presence of strong automatic differentiation
libraries like Zygote.jl makes it trivial to compute efﬁcient
derivatives from the renderer. We present the performance gains
we get on using Zygote as compared to Central Differencing
in Section V-B.

RayTracer.jl [9] is a package for Differentiable Ray Tracing
written to solve this particular issue. It relies heavily on the
source-to-source automatic differentiation package, Zygote, for
computing gradients with respect to arbitrary scene parameters.
This package allows the user to conﬁgure the location of objects,

 
 
 
 
 
 
Fig. 1: Utah Teapot Render from three different views. The camera deﬁnition shown in Listing 1 can be easily modiﬁed to
generate all these views.

lights, and a camera in the scene. This scene is then interpreted
by the renderer to generate the image. RayTracer.jl is naturally
interfaced with the deep learning library Flux [10], due to the
common AD backend, for use in more complex differentiable
pipelines.

Ray Tracing is primarily a non-differentiable operation. As
such, any technique that is used to compute gradients for scene
parameters by backpropagating through a ray tracer would
be some approximation of gradients. In order to make our
rendering differentiable, we sample only a single ray for every
pixel on the screen. For better image quality, we should be
sampling multiple rays for a single pixel value, but this would
make differentiation a bit tricky. Since we have only one ray
per pixel, it is bound to intersect with only a single triangle in
the 3D scene. The color of a pixel is a weighted sum of colors
of the intersection points. The computation of point color is a
differentiable operation as it is either a plain color or a texture,
calculated using the barycentric coordinates. Also, checking
the intersection between a ray and a triangle involves solving
a quadratic equation which is again differentiable. Since every
operation is differentiable, we can easily backpropagate the
errors through this. However, in case the ray intersects at a
point close to the intersection of two triangles, the gradients
are not correct. This is because these points in space are non-
differentiable. We discuss this shortcoming in Section VI.

scene, the shape, and materials of the objects and the camera
conﬁguration.
(cid:7)
#
S c r e e n
s c r e e n _ s i z e

= ( w = 5 1 2 , h = 5 1 2 )

S i z e

(cid:4)

C a m e r a

#
c a m =

C a m e r a (

S e t u p

= V e c 3 ( 1 .0 f 0 , 1 0 .0 f 0 , - 1 .0 f 0 ) ,

= V e c 3 ( 0 .0 f 0 ) ,

l o o k f r o m
l o o k a t
v u p = V e c 3 ( 0 .0 f 0 , 1 .0 f 0 , 0 .0 f 0 ) ,
v f o v = 4 5 .0 f 0 ,
= 1 .0 f 0 ,
f o c u s
w i d t h
=
h e i g h t

s c r e e n _ s i z e . w ,
s c r e e n _ s i z e . h

=

)

o r i g i n ,

d i r e c t i o n

=

g e t _ p r i m a r y _ r a y s ( c a m )

#
S c e n e
s c e n e =

l o a d _ o b j ( " t e a p o t . o b j " )

#
L i g h t
l i g h t =

P o s i t i o n
D i s t a n t L i g h t (

c o l o r
i n t e n s i t y
d i r e c t i o n

= V e c 3 ( 1 .0 f 0 ) ,
= 1 0 0 .0 f 0 ,
= V e c 3 ( 0 .0 f 0 , 1 .0 f 0 , 0 .0 f 0 )

)

R e n d e r

#
c o l o r =

t h e

i m a g e

r a y t r a c e (

o r i g i n ,
d i r e c t i o n ,
s c e n e ,
l i g h t ,
o r i g i n ,
2

III. SCENE RENDERING

)(cid:6)

Listing 1: Rendering the Utah Teapot Model

(cid:5)

We ﬁrst create a general-purpose renderer and then make
use of efﬁcient AD tools to make it completely differentiable.
Hence, at its core, RayTracer is a fully-featured renderer. It
contains functionalities for both raytracing and rasterization.
Unlike most prior work in differentiable rendering, we do not
make performance compromises in the forward pass (rendering)
to allow gradient computation.

RayTracer gives much control to the user over the scene
they want to render. The user controls the lighting in the

In this part, we will demonstrate the general pipeline for
deﬁning a 3D scene using RayTracer.jl and then rendering
it. We are going to render the popular Utah Teapot model.
We need to specify the 3D model in the form of a Vector
of Objects. We can do it manually for custom scenes, or we
could load it from a wavefront object (obj) ﬁle (MeshIO.jl
provides support for additional ﬁle formats). Apart from the
scene vector, we need to specify the camera conﬁguration and

(a) Performance Benchmarks

(b) Memory Allocation Benchmarks

Fig. 2: Comparison between scenes rendered with and without BVH

the conﬁguration of light(s). We summarize the entire code to
render the teapot in Listing 1.

IV. INVERSE RENDERING

The rendering problem is to project 3D scene parameters to
form an image in the 2D plane. Inverse Rendering problem is
just the opposite: generating a mapping from the 2D image
back to the parameters of the 3D scene.

RayTracer.jl can be used to solve a variety of inverse graphics
problems. Since the renderer can be used to compute the
gradient with respect to any arbitrary parameter (as long
as it is differentiable), we can then use any gradient-based
optimization technique to optimize on that parameter. This
allows us to propose a generalized Algorithm 1 which is capable
of optimizing any differentiable parameter.

V. EXPERIMENTS

In this section we showcase our differentiable renderer in
some benchmarking and toy inverse rendering problems. Using
the following experiments we demonstrate the use of gradients
obtained via AD to recover the camera, material and lighting
parameters for a scene. In the inverse rendering experiments
we make use of the Adam optimizer as described in [11]. We
interface the raytracer with Flux to use these optimizers. As an
alternative, we have also tested the functioning of our package
with the optimizers present in Optim1 [12].

A. Accelerating the Rendering using Acceleration Structures

To accelerate the rendering process, we support an acceler-
ation structure, Bounding Volume Hierarchy (BVH)[13]. We
follow the same API for ray tracing using these accelerators.
In this case, instead of passing a Vector of Objects, we wrap it
in a BoundingVolumeHierarchy object and pass it. So in order
to use this, we would have to change the scene variable in
Listing 1 to BoundingVolumeHierarchy(...).

1We provide an example at examples/optim_compatibility.jl

Algorithm 1: Gradient Based Optimization of Scene
Parameters
Input: Initial Guess of the Scene Parameters, Maximum
Number of Iterations, Tolerance in Loss

Output: Optimized set of Scene Parameters
1 params ← Initial Guess of Scene Parameters
2 tolerance ← Tolerance in Loss
3 max_iter ← Maximum Number of Iterations
4 converged ← false
5 iter ← 0
6 while not converged or iter < max_iter do
7

loss ← mean_squared_loss (render_image (params),
target_img)
gs ← gradient(loss)
for param in params do

update! (optimizer, param, gs[param])

end
if loss < tolerance then
converged ← true

8

9

10

11

12

13

14

end
iter ← iter + 1

15
16 end
17 return params

In this section, we provide a comparison between the
performance gains and memory allocation beneﬁts of using
BVH. We use a mesh of 137 triangles centered at the origin
as the scene. We increase the screen size and hence increasing
the total number of pixels (and therefore primary rays) in the
scene2.

We present the beneﬁts of using BVH in Figure 2. We are
able to get to reduce the total allocations (Figure 2b) and

2Even though it might seem that increasing the number of objects in the
scene would be a better metric for comparison, it is immensely difﬁcult to
make that comparison. This is primarily because the same scene in a different
conﬁguration will have a different render time

(a) Time taken for the Backward Pass

(b) SpeedUp when using AD vs Numerical Differentiation

Fig. 3: Comparison between Automatic Differentiation and Numerical Differentiation

also get a signiﬁcant performance boost3 (Figure 2a). Note
that we only get exponential beneﬁts in terms of memory and
performance when the number of pixels is of reasonable size,
for example in case of 128 × 128 or better resolution images.
For smaller images, using BVH might end up slowing down
the rendering process.

B. Comparison with Finite Differencing

In this section, we demonstrate the performance gain of
using source-to-source AD backend versus Central Finite
Differencing. We provide a convenience function for ﬁnite
differencing for gradient testing purposes. The δ for calculating
derivatives using the equation df
is ﬁxed
by comparing these values with the values obtained from
Zygote. Through this experiment, we show that our method is
exponentially better than numerical differentiation.

dx = f (x+δ)−f (x−δ)

2δ

For comparing the two differentiation techniques, we run
ﬁve independent trials and present the mean runtimes (with
the standard deviation) in Figure 3. We ﬁx the position of the
camera and light and randomly generate the triangles present
in the scene. Every new triangle added to the scene, adds 20
additional parameters with respect to which we must compute
the derivatives. We use the mean squared loss function to
compute the scalar loss and backpropagate.

Figure 3b shows that we get signiﬁcant speedups for a
reasonable number of parameters. The nature of speedup
shown suggests that it is nearly infeasible to use numerical
differentiation when the number of parameters exceeds 50. In
most practical applications of differentiable mesh rendering
involving neural networks, we will have at least thousands of
parameters. In such cases, our AD-based solution will be able
to compute the derivatives in a reasonable time.

C. Calibration of Camera Parameters

In this experiment we start with the image of a rectangle
(Listing 2) under some conﬁguration of the Camera model

3We provide the code for reproducing the experiment in

examples/performance_benchmarks.jl

(Listing 3). Since RayTracer supports only two primitive shapes
- Spheres and Triangles, we need to triangulate the rectangle.

(cid:7)

s c e n e = [

T r i a n g l e (

V e c 3 ( 2 0 .0 ,
1 0 .0 , 0 .0 ) ,
V e c 3 ( 2 0 .0 , - 1 0 .0 , 0 .0 ) ,
1 0 .0 , 0 .0 ) ,
V e c 3 ( - 2 0 .0 ,
=
M a t e r i a l ( c o l o r _ d i f f u s e

V e c 3 ( 0 .0 , 1 .0 , 0 .0 ) ) ) ,

T r i a n g l e (

V e c 3 ( - 2 0 .0 , - 1 0 .0 , 0 .0 ) ,
V e c 3 ( 2 0 .0 , - 1 0 .0 , 0 .0 ) ,
1 0 .0 , 0 .0 ) ,
V e c 3 ( - 2 0 .0 ,
=
M a t e r i a l ( c o l o r _ d i f f u s e

V e c 3 ( 0 .0 , 1 .0 , 0 .0 ) ) )
]

l i g h t =

P o i n t L i g h t (
V e c 3 ( 1 .0 , 0 .0 , 0 .0 ) ,
1 0 0 0 0 0 .0 ,
V e c 3 ( 0 .0 , 0 .0 , - 1 0 .0 )

)(cid:6)

Listing 2: Conﬁguration of the Scene for Experiment V-C

(cid:7)

c a m e r a _ t a r g e t

=

C a m e r a (

V e c 3 ( 0 .0 , 0 .0 , - 3 0 .0 ) ,
0 .0 ) ,
V e c 3 ( 0 .0 , 0 .0 ,
V e c 3 ( 0 .0 , 1 .0 ,
0 .0 ) ,
9 0 .0 ,
1 .0 ,
s c r e e n _ s i z e . . .

(cid:6)

)

Listing 3: Camera Parameters to be Reconstructed

(cid:4)

(cid:5)

(cid:4)

(cid:5)

(b) Image with uncalibrated
camera

(c) The image to be recon-
structed

(a) Loss Values over the Light Conﬁguration Optimization
Process

(d) Image obtained after the
optimization

(e) Difference between recon-
structed and target image

Fig. 4: Calibration of Camera Parameters to reconstruct Image 4c from Image 4b

(cid:7)

c a m e r a _ g u e s s

=

C a m e r a (

V e c 3 ( 5 .0 , - 4 .0 , - 2 0 .0 ) ,
0 .0 ) ,
V e c 3 ( 0 .0 ,
0 .0 ) ,
V e c 3 ( 0 .0 ,
9 0 .0 ,
3 .0 ,
s c r e e n _ s i z e . . .

0 .0 ,
1 .0 ,

(cid:6)

)

Listing 4: Initial Guess of the Camera Parameters

(cid:7)
(cid:4)

s c r e e n _ s i z e

= ( w = 1 2 8 , h = 1 2 8 )

c a m e r a

=

C a m e r a (

V e c 3 ( 0 .0 f 0 , 6 .0 f 0 , - 1 0 .0 f 0 ) ,
V e c 3 ( 0 .0 f 0 , 2 .0 f 0 ,
V e c 3 ( 0 .0 f 0 , 1 .0 f 0 ,
4 5 .0 f 0 ,
0 .5 f 0 ,
s c r e e n _ s i z e . . .

0 .0 f 0 ) ,
0 .0 f 0 ) ,

(cid:5)

)

We aim to reconstruct the image of this rectangle (Figure 4c)
by modifying the focus and the location of the camera. We
assume that all the other parameters of the model, like the light
conﬁguration (Listing 2) and the position of objects are known
apriori. We use algorithm 1 for optimizing the parameters.
We make an initial guess of the parameters and initialize the
camera (Listing 4).

As our loss function, we use the mean squared difference
between rendered and target images, each with 300 × 400
pixels having fractional RGB values. We minimize loss with
the Adam optimizer, with learning rate 0.1, and declare the
optimization to have converged if loss falls below 100 (where
the initial loss is 5705.98). Figure 4 shows the optimization
steps. We present the various learning rates and optimizers we
experimented with in Figure 4a.

s c e n e =

(cid:6)

l o a d _ o b j ( " t r e e . o b j " )

Listing 5: Conﬁguration of the Scene for Experiment V-D

The object in our scene is a tree. We start with arbitrary
lighting (Listing 7) condition and then iteratively improve the
lighting using Algorithm 1. We present the loss curve and the
images generated during the optimization process in Figure 5.
(cid:7)

l i g h t _ t a r g e t

=

P o i n t L i g h t (

V e c 3 ( 1 .0 f 0 , 1 .0 f 0 , 1 .0 f 0 ) ,
2 0 0 0 0 .0 f 0 ,
V e c 3 ( 1 .0 f 0 , 1 0 .0 f 0 , - 5 0 .0 f 0 )

)(cid:6)

(cid:7)

Listing 6: Target Lighting Conditions

l i g h t _ g u e s s

=

P o i n t L i g h t (

V e c 3 ( 1 .0 f 0 , 1 .0 f 0 , 1 .0 f 0 ) ,
1 .0 f 0 ,
V e c 3 ( - 1 .0 f 0 , - 1 0 .0 f 0 , - 5 0 .0 f 0 )

D. Optimizing the Light Source

)(cid:6)

In this experiment, we describe our solution to the inverse
lighting problem. This problem involved predicting the con-
ﬁguration of the light source(s) in the scene given a target
image (Figure 5b). We know the exact geometry and surface
properties of the objects, as well as the camera conﬁguration.
Listing 5 describes the known conﬁgurations.

E. Retrieving Color of Materials

RayTracer.jl can also be used to recover the properties of the
material of a mesh. For this experiment, we shall use the same
tree mesh from Experiment V-D. We are going to optimize the

Listing 7: Initial Guess of Lighting Conditions

(cid:4)

(cid:5)

(cid:4)

(cid:5)

(cid:4)

(cid:5)

(b) The image to be recon-
structed

(c) Initial Guess of Lighting
Parameters

(a) Loss Values over the Light Conﬁguration Optimization
Process

Fig. 5: Optimization of the lighting conditions to reconstruct Image 5b from Image 5c

(d) Image produced after 10
iterations

Image with converged

(e)
parameters

(a) Image to be reconstructed

(b) Initial Guess of the Materials

loss with Adam optimizer, with a learning rate of 0.05. The
convergence of the model is quite fast, and we get a good
approximation of the parameters just after a single optimizer
step (Figure 6c).
(cid:7)

(cid:4)

s c r e e n _ s i z e

= ( w = 4 0 0 , h = 3 0 0 )

l i g h t =

P o i n t L i g h t (

V e c 3 ( 1 .0 f 0 ) ,
1 0 0 0 0 0 0 .0 f 0 ,
V e c 3 ( 0 .1 5 f 0 , 0 .5 f 0 , - 1 0 .5 f 0 )

)

c a m =

C a m e r a (

V e c 3 ( - 2 .0 f 0 , 2 .0 f 0 , - 5 .0 f 0 ) ,
0 .0 f 0 ) ,
V e c 3 ( 0 .0 f 0 , 1 .7 f 0 ,
V e c 3 ( 0 .0 f 0 , 1 .0 f 0 ,
0 .0 f 0 ) ,
4 5 .0 f 0 ,
1 .0 f 0 ,
s c r e e n _ s i z e . . .

(c) Image produced after just 1
iteration

(d) Image obtained after 35 iter-
ations

Fig. 6: Optimization of the materials of the mesh to reconstruct
Image 6a from Image 6b

diffuse color of the mesh. We use the position of the camera
and the lighting conditions, as mentioned in Listing 8.

The signiﬁcant difference of this optimization from the prior
experiments is that the color can only take values between
0.0 and 1.0. After every iteration, we clamp the value of the
diffuse color. Hence, we update our parameters using projected
gradient descent.

We use the sum squared loss function. We minimize the

)

s c e n e =

(cid:6)

l o a d _ o b j ( " t r e e . o b j " )

(cid:5)

Listing 8: Conﬁguration of the Scene for Experiment V-E

VI. CURRENT LIMITATIONS

Despite the success of our approach in solving a variety
to deal with non-
of inverse graphics problems, we fail
differentiable problems. One such instance would be estimating
the proper geometry of an object given an image. Such
problems are non-differentiable due to a large number of
discrete choices in the position of the triangle vertices. Hence,
trying to optimize such parameters generally causes them to
diverge. The other cases which we cannot handle properly are
secondary lighting (shadows) and global illumination. Most of

these cases can be dealt with, similar to the way proposed by
[3], but that leads to a signiﬁcant slowdown to the rendering
of the scene.

VII. CONCLUSION

In conclusion, we have shown how Julia can be leveraged to
build differentiable systems. We have presented which to the
best of our knowledge is the ﬁrst differentiable renderer which
uses source-to-source AD. We have used a set of toy examples
to demonstrate the ability of our renderer to reconstruct scenes
(which are differentiable) from only a single image. This
also shows that this renderer can be used in differentiable
programming pipelines which involve image generation.

REFERENCES

[1] A. Appel, “Some Techniques for Shading Machine Renderings
the April 30–May 2, 1968, Spring
New
ser. AFIPS ’68 (Spring).
[Online]. Available:

of Solids,” in Proceedings of
Joint Computer Conference,
York, NY, USA: ACM, 1968, pp. 37–45.
http://doi.acm.org/10.1145/1468075.1468082

[2] J. Pineda, “A Parallel Algorithm for Polygon Rasterization,” in
Proceedings of the 15th Annual Conference on Computer Graphics
and Interactive Techniques,
ser. SIGGRAPH ’88. New York,
NY, USA: ACM, 1988, pp. 17–20.
[Online]. Available: http:
//doi.acm.org/10.1145/54852.378457

[3] T.-M. Li, M. Aittala, F. Durand, and J. Lehtinen, “Differentiable Monte
Carlo Ray Tracing Through Edge Sampling,” ACM Trans. Graph.,
vol. 37, no. 6, pp. 222:1–222:11, Dec. 2018. [Online]. Available:
http://doi.acm.org/10.1145/3272127.3275109

[4] S. Liu, T. Li, W. Chen, and H. Li, “Soft rasterizer: A differentiable
renderer for image-based 3d reasoning,” CoRR, vol. abs/1904.01786,
2019. [Online]. Available: http://arxiv.org/abs/1904.01786

[5] M. Innes, “Don’t Unroll Adjoint: Differentiating SSA-Form Programs,”
CoRR, vol. abs/1810.07951, 2018. [Online]. Available: http://arxiv.org/
abs/1810.07951

[6] M. Johnson, R. Frostig, D. Maclaurin, and C. Leary, “JAX: Autograd

and XLA,” https://github.com/google/jax, 2018.

[7] J. A. E. Andersson, J. Gillis, G. Horn, J. B. Rawlings, and
M. Diehl, “Casadi: a software framework for nonlinear optimization
and optimal control,” Mathematical Programming Computation,
vol. 11, no. 1, pp. 1–36, Mar 2019.
[Online]. Available: https:
//doi.org/10.1007/s12532-018-0139-4

[8] J. Bezanson, A. Edelman, S. Karpinski, and V. B. Shah, “Julia: A Fresh
Approach to Numerical Computing,” SIAM Review, vol. 59, no. 1, pp.
65–98, 2017. [Online]. Available: https://doi.org/10.1137/141000671
[9] A. Pal, “RayTracer.jl,” https://github.com/avik-pal/RayTracer.jl, 2019.
[10] M. Innes, E. Saba, K. Fischer, D. Gandhi, M. C. Rudilosso, N. M.
Joy, T. Karmali, A. Pal, and V. Shah, “Fashionable Modelling
with Flux,” CoRR, vol. abs/1811.01457, 2018. [Online]. Available:
http://arxiv.org/abs/1811.01457

[11] D. Kingma and J. Ba, “Adam: A Method for Stochastic Optimization,”
International Conference on Learning Representations, 12 2014.
[12] P. K Mogensen and A. N Riseth, “Optim: A mathematical optimization
package for julia,” Journal of Open Source Software, vol. 3, no. 24, p.
615, 4 2018. [Online]. Available: http://dx.doi.org/10.21105/joss.00615
[13] T. L. Kay and J. T. Kajiya, “Ray Tracing Complex Scenes,”
in Proceedings of
the 13th Annual Conference on Computer
Graphics and Interactive Techniques, ser. SIGGRAPH ’86. New
York, NY, USA: ACM, 1986, pp. 269–278.
[Online]. Available:
http://doi.acm.org/10.1145/15922.15916

