Gated Feedback Recurrent Neural Networks

5
1
0
2

n
u
J

7
1

]
E
N
.
s
c
[

4
v
7
6
3
2
0
.
2
0
5
1
:
v
i
X
r
a

Junyoung Chung
Caglar Gulcehre
Kyunghyun Cho
Yoshua Bengio∗
Dept. IRO, Universit´e de Montr´eal, ∗CIFAR Senior Fellow

JUNYOUNG.CHUNG@UMONTREAL.CA
CAGLAR.GULCEHRE@UMONTREAL.CA
KYUNGHYUN.CHO@UMONTREAL.CA
FIND-ME@THE.WEB

Abstract

In this work, we propose a novel recurrent neu-
ral network (RNN) architecture. The proposed
RNN, gated-feedback RNN (GF-RNN), extends
the existing approach of stacking multiple recur-
rent layers by allowing and controlling signals
ﬂowing from upper recurrent layers to lower lay-
ers using a global gating unit for each pair of
layers. The recurrent signals exchanged between
layers are gated adaptively based on the previous
hidden states and the current input. We evalu-
ated the proposed GF-RNN with different types
of recurrent units, such as tanh, long short-term
memory and gated recurrent units, on the tasks
of character-level language modeling and Python
program evaluation. Our empirical evaluation of
different RNN units, revealed that in both tasks,
the GF-RNN outperforms the conventional ap-
proaches to build deep stacked RNNs. We sug-
gest that the improvement arises because the GF-
RNN can adaptively assign different layers to dif-
ferent timescales and layer-to-layer interactions
(including the top-down ones which are not usu-
ally present in a stacked RNN) by learning to gate
these interactions.

1. Introduction

Recurrent neural networks (RNNs) have been widely stud-
ied and used for various machine learning tasks which in-
volve sequence modeling, especially when the input and
output have variable lengths. Recent studies have revealed
that RNNs using gating units can achieve promising re-
sults in both classiﬁcation and generation tasks (see, e.g.,
Graves, 2013; Bahdanau et al., 2014; Sutskever et al.,
2014).

Although RNNs can theoretically capture any long-term
dependency in an input sequence, it is well-known to be
difﬁcult to train an RNN to actually do so (Hochreiter,

1991; Bengio et al., 1994; Hochreiter, 1998). One of the
most successful and promising approaches to solve this is-
sue is by modifying the RNN architecture e.g., by using a
gated activation function, instead of the usual state-to-state
transition function composing an afﬁne transformation and
a point-wise nonlinearity. A gated activation function,
such as the long short-term memory (LSTM, Hochreiter
& Schmidhuber, 1997) and the gated recurrent unit (GRU,
Cho et al., 2014), is designed to have more persistent mem-
ory so that it can capture long-term dependencies more eas-
ily.

Sequences modeled by an RNN can contain both fast
changing and slow changing components, and these un-
derlying components are often structured in a hierarchical
manner, which, as ﬁrst pointed out by El Hihi & Bengio
(1995) can help to extend the ability of the RNN to learn
to model longer-term dependencies. A conventional way to
encode this hierarchy in an RNN has been to stack multi-
ple levels of recurrent layers (Schmidhuber, 1992; El Hihi
& Bengio, 1995; Graves, 2013; Hermans & Schrauwen,
2013). More recently, Koutn´ık et al. (2014) proposed a
more explicit approach to partition the hidden units in an
RNN into groups such that each group receives the sig-
nal from the input and the other groups at a separate, pre-
deﬁned rate, which allows feedback information between
these partitions to be propagated at multiple timescales.
Stollenga et al. (2014) recently showed the importance of
feedback information across multiple levels of feature hier-
archy, however, with feedforward neural networks.

In this paper, we propose a novel design for RNNs, called a
gated-feedback RNN (GF-RNN), to deal with the issue of
learning multiple adaptive timescales. The proposed RNN
has multiple levels of recurrent layers like stacked RNNs
do. However, it uses gated-feedback connections from up-
per recurrent layers to the lower ones. This makes the hid-
den states across a pair of consecutive timesteps fully con-
nected. To encourage each recurrent layer to work at differ-
ent timescales, the proposed GF-RNN controls the strength
of the temporal (recurrent) connection adaptively. This ef-

 
 
 
 
 
 
Gated Feedback Recurrent Neural Networks

fectively lets the model to adapt its structure based on the
input sequence.

We empirically evaluated the proposed model against the
conventional stacked RNN and the usual, single-layer RNN
on the task of language modeling and Python program eval-
uation (Zaremba & Sutskever, 2014). Our experiments re-
veal that the proposed model signiﬁcantly outperforms the
conventional approaches on two different datasets.

2. Recurrent Neural Network

An RNN is able to process a sequence of arbitrary length
by recursively applying a transition function to its internal
hidden states for each symbol of the input sequence. The
activation of the hidden states at timestep t is computed as a
function f of the current input symbol xt and the previous
hidden states ht−1:

ht =f (xt, ht−1) .

It is common to use the state-to-state transition function f
as the composition of an element-wise nonlinearity with an
afﬁne transformation of both xt and ht−1:

ht =φ (W xt + U ht−1) ,

(1)

where W is the input-to-hidden weight matrix, U is the
state-to-state recurrent weight matrix, and φ is usually a
logistic sigmoid function or a hyperbolic tangent function.

p(x1, · · · , xT ) = p(x1)p(x2 | x1) · · · p(xT | x1, · · · , xT −1).

Then, we can train an RNN to model this distribution by
letting it predict the probability of the next symbol xt+1
given hidden states ht which is a function of all the previ-
ous symbols x1, · · · , xt−1 and current symbol xt:

p(xt+1 | x1, · · · , xt) = g (ht) .

This approach of using a neural network to model a prob-
ability distribution over sequences is widely used, for in-
stance, in language modeling (see, e.g., Bengio et al., 2001;
Mikolov, 2012).

2.1. Gated Recurrent Neural Network

The difﬁculty of training an RNN to capture long-term de-
pendencies has been known for long (Hochreiter, 1991;
Bengio et al., 1994; Hochreiter, 1998). A previously suc-
cessful approaches to this fundamental challenge has been
to modify the state-to-state transition function to encourage
some hidden units to adaptively maintain long-term mem-
ory, creating paths in the time-unfolded RNN, such that
gradients can ﬂow over many timesteps.

Long short-term memory (LSTM) was proposed by
Hochreiter & Schmidhuber (1997) to speciﬁcally address
this issue of learning long-term dependencies. The LSTM
maintains a separate memory cell inside it that updates and
exposes its content only when deemed necessary. More re-
cently, Cho et al. (2014) proposed a gated recurrent unit
(GRU) which adaptively remembers and forgets its state
based on the input signal to the unit. Both of these units are
central to our proposed model, and we will describe them
in more details in the remainder of this section.

2.1.1. LONG SHORT-TERM MEMORY

Since the initial 1997 proposal, several variants of the
LSTM have been introduced (Gers et al., 2000; Zaremba
et al., 2014). Here we follow the implementation provided
by Zaremba et al. (2014).

Such an LSTM unit consists of a memory cell ct, an input
gate it, a forget gate ft, and an output gate ot. The memory
cell carries the memory content of an LSTM unit, while
the gates control the amount of changes to and exposure
of the memory content. The content of the memory cell
cj
t of the j-th LSTM unit at timestep t is updated similar
to the form of a gated leaky neuron, i.e., as the weighted
sum of the new content ˜cj
t and the previous memory content
cj
t and f j
t−1 modulated by the input and forget gates, ij
t ,
respectively:

t = f j
cj

t cj

t−1 + ij

t ˜cj
t ,

˜ct = tanh (Wcxt + Ucht−1) .

(2)

(3)

The input and forget gates control how much new content
should be memorized and how much old content should be
forgotten, respectively. These gates are computed from the
previous hidden states and the current input:

(4)

(5)

it =σ (Wixt + Uiht−1) ,
ft =σ (Wf xt + Uf ht−1) ,
k=1 and ft = (cid:2)f k
(cid:3)p

t

where it = (cid:2)ik
(cid:3)p
k=1 are respectively the
vectors of the input and forget gates in a recurrent layer
composed of p LSTM units. σ(·) is an element-wise logis-
tic sigmoid function. xt and ht−1 are the input vector and
previous hidden states of the LSTM units, respectively.

t

Once the memory content of the LSTM unit is updated, the
hidden state hj

t of the j-th LSTM unit is computed as:
(cid:16)

(cid:17)

t = oj
hj

t tanh

cj
t

.

The output gate oj
t controls to which degree the memory
content is exposed. Similarly to the other gates, the out-
put gate also depends on the current input and the previous

We can factorize the probability of a sequence of arbitrary
length into

where

Gated Feedback Recurrent Neural Networks

hidden states such that

ot = σ (Woxt + Uoht−1) .

(6)

In other words, these gates and the memory cell allow an
LSTM unit to adaptively forget, memorize and expose the
memory content. If the detected feature, i.e., the memory
content, is deemed important, the forget gate will be closed
and carry the memory content across many timesteps,
which is equivalent to capturing a long-term dependency.
On the other hand, the unit may decide to reset the memory
content by opening the forget gate. Since these two modes
of operations can happen simultaneously across different
LSTM units, an RNN with multiple LSTM units may cap-
ture both fast-moving and slow-moving components.

2.1.2. GATED RECURRENT UNIT

The GRU was recently proposed by Cho et al. (2014). Like
the LSTM, it was designed to adaptively reset or update
its memory content. Each GRU thus has a reset gate rj
t
and an update gate zj
t which are reminiscent of the forget
and input gates of the LSTM. However, unlike the LSTM,
the GRU fully exposes its memory content each timestep
and balances between the previous memory content and the
new memory content strictly using leaky integration, albeit
with its adaptive time constant controlled by update gate
zj
t .
At timestep t, the state hj

t of the j-th GRU is computed by
˜hj
t ,

t−1 + zj

t )hj

(7)

t

t = (1 − zj
hj

t−1 and ˜hj

where hj
t respectively correspond to the previ-
ous memory content and the new candidate memory con-
tent. The update gate zj
t controls how much of the previous
memory content is to be forgotten and how much of the
new memory content is to be added. The update gate is
computed based on the previous hidden states ht−1 and the
current input xt:

zt =σ (Wzxt + Uzht−1) ,

(8)

The new memory content ˜hj
conventional transition function in Eq. (1):

t is computed similarly to the

˜ht = tanh (W xt + rt (cid:12) U ht−1) ,

(9)

where (cid:12) is an element-wise multiplication.

One major difference from the traditional transition func-
tion (Eq. (1)) is that the states of the previous step ht−1
is modulated by the reset gates rt. This behavior allows
a GRU to ignore the previous hidden states whenever it is
deemed necessary considering the previous hidden states
and the current input:

rt =σ (Wrxt + Urht−1) .

(10)

The update mechanism helps the GRU to capture long-
term dependencies. Whenever a previously detected fea-
ture, or the memory content is considered to be important
for later use, the update gate will be closed to carry the cur-
rent memory content across multiple timesteps. The reset
mechanism helps the GRU to use the model capacity efﬁ-
ciently by allowing it to reset whenever the detected feature
is not necessary anymore.

3. Gated Feedback Recurrent Neural

Network

Although capturing long-term dependencies in a sequence
is an important and difﬁcult goal of RNNs, it is worth-
while to notice that a sequence often consists of both slow-
moving and fast-moving components, of which only the
former corresponds to long-term dependencies. Ideally, an
RNN needs to capture both long-term and short-term de-
pendencies.

El Hihi & Bengio (1995) ﬁrst showed that an RNN can cap-
ture these dependencies of different timescales more easily
and efﬁciently when the hidden units of the RNN is ex-
plicitly partitioned into groups that correspond to differ-
ent timescales. The clockwork RNN (CW-RNN) (Koutn´ık
et al., 2014) implemented this by allowing the i-th mod-
ule to operate at the rate of 2i−1, where i is a positive
integer, meaning that the module is updated only when
t mod 2i−1 = 0. This makes each module to operate at dif-
ferent rates. In addition, they precisely deﬁned the connec-
tivity pattern between modules by allowing the i-th module
to be affected by j-th module when j > i.

Here, we propose to generalize the CW-RNN by allowing
the model to adaptively adjust the connectivity pattern be-
tween the hidden layers in the consecutive timesteps. Simi-
lar to the CW-RNN, we partition the hidden units into mul-
tiple modules in which each module corresponds to a dif-
ferent layer in a stack of recurrent layers.

Unlike the CW-RNN, however, we do not set an explicit
rate for each module. Instead, we let each module oper-
ate at different timescales by hierarchically stacking them.
Each module is fully connected to all the other modules
across the stack and itself. In other words, we do not de-
ﬁne the connectivity pattern across a pair of consecutive
timesteps. This is contrary to the design of CW-RNN and
the conventional stacked RNN. The recurrent connection
between two modules, instead, is gated by a logistic unit
([0, 1]) which is computed based on the current input and
the previous states of the hidden layers. We call this gating
unit a global reset gate, as opposed to a unit-wise reset gate
which applies only to a single unit (See Eqs. (2) and (9)).

Gated Feedback Recurrent Neural Networks

(a) Conventional stacked RNN

(b) Gated Feedback RNN

Figure 1. Illustrations of (a) conventional stacking approach and (b) gated-feedback approach to form a deep RNN architecture. Bullets
in (b) correspond to global reset gates. Skip connections are omitted to simplify the visualization of networks.

The global reset gate is computed as:

layer is computed by

gi→j = σ

(cid:16)

wi→j
g

hj−1
t + ui→j

g

h∗

t−1

(cid:17)

,

where h∗
t−1 is the concatenation of all the hidden states
from the previous timestep t − 1. The superscript i→j is
an index of associated set of parameters for the transition
from layer i in timestep t − 1 to layer j in timestep t. wi→j
and ui→j
are respectively the weight vectors for the current
input and the previous hidden states. When j = 1, hj−1
is
xt.

g

g

t

In other words, the signal from hi
t is controlled by
a single scalar gi→j which depends on the input xt and all
the previous hidden states h∗
t−1.

t−1 to hj

We call this RNN with a fully-connected recurrent transi-
tions and global reset gates, a gated-feedback RNN (GF-
RNN). Fig. 1 illustrates the difference between the conven-
tional stacked RNN and our proposed GF-RNN. In both
models, information ﬂows from lower recurrent layers to
upper recurrent layers. The GF-RNN, however, further
allows information from the upper recurrent layer, corre-
sponding to coarser timescale, ﬂows back into the lower
recurrent layers, corresponding to ﬁner timescales.

In the remainder of this section, we describe how to use
the previously described LSTM unit, GRU, and more tra-
ditional tanh unit in the GF-RNN.

(cid:32)

hj

t = tanh

W j−1→jhj−1

t +

(cid:33)

gi→jU i→jhi

t−1

,

L
(cid:88)

i=1

where L is the number of hidden layers, W j−1→j and
U i→j are the weight matrices of the current input and
the previous hidden states of the i-th module, respectively.
Compared to Eq. (1), the only difference is that the previ-
ous hidden states are from multiple layers and controlled
by the global reset gates.

Long Short-Term Memory and Gated Recurrent Unit.
In the cases of LSTM and GRU, we do not use the global
reset gates when computing the unit-wise gates. In other
words, Eqs. (4)–(6) for LSTM, and Eqs. (8) and (10) for
GRU are not modiﬁed. We only use the global reset gates
when computing the new state (see Eq. (3) for LSTM, and
Eq. (9) for GRU).

The new memory content of an LSTM at the j-th layer is
computed by

(cid:32)

˜cj
t = tanh

W j−1→j

c

hj−1

t +

L
(cid:88)

i=1

In the case of a GRU, similarly,

gi→jU i→j

c hi

t−1

(cid:33)

.

(cid:32)

˜hj

t = tanh

W j−1→jhj−1

t + rj

t (cid:12)

(cid:33)

gi→jU i→jhi

t−1

.

L
(cid:88)

i=1

4. Experiment Settings

3.1. Practical Implementation of GF-RNN

4.1. Tasks

tanh Unit. For a stacked tanh-RNN, the signal from the
previous timestep is gated. The hidden state of the j-th

We evaluated the proposed GF-RNN on character-level lan-
guage modeling and Python program evaluation. Both

Gated Feedback Recurrent Neural Networks

tasks are representative examples of discrete sequence
modeling, where a model is trained to minimize the neg-
ative log-likelihood of training sequences:

min
θ

1
N

N
(cid:88)

Tn(cid:88)

n=1

t=1

− log p (cid:0)xn

t | xn

1 , . . . , xn

t−1; θ(cid:1) ,

where θ is a set of model parameters.

4.1.1. LANGUAGE MODELING

We used the dataset made available as a part of the human
knowledge compression contest (Hutter, 2012). We refer
to this dataset as the Hutter dataset. The dataset, which
was built from English Wikipedia, contains 100 MBytes of
characters which include Latin alphabets, non-Latin alpha-
bets, XML markups and special characters. Closely follow-
ing the protocols in (Mikolov et al., 2012; Graves, 2013),
we used the ﬁrst 90 MBytes of characters to train a model,
the next 5 MBytes as a validation set, and the remaining
as a test set, with the vocabulary of 205 characters includ-
ing a token for an unknown character. We used the average
number of bits-per-character (BPC, E[− log2 P (xt+1|ht)])
to measure the performance of each model on the Hutter
dataset.

4.1.2. PYTHON PROGRAM EVALUATION

Zaremba & Sutskever (2014) recently showed that an RNN,
more speciﬁcally a stacked LSTM, is able to execute a short
Python script. Here, we compared the proposed architec-
ture against the conventional stacking approach model on
this task, to which refer as Python program evaluation.

Scripts used in this task include addition, multiplication,
subtraction, for-loop, variable assignment, logical compar-
ison and if-else statement. The goal is to generate, or pre-
dict, a correct return value of a given Python script. The
input is a program while the output is the result of a print
statement: every input script ends with a print statement.
Both the input script and the output are sequences of char-
acters, where the input and output vocabularies respectively
consist of 41 and 13 symbols.

The advantage of evaluating the models with this task is
that we can artiﬁcially control the difﬁculty of each sam-
ple (input-output pair). The difﬁculty is determined by
the number of nesting levels in the input sequence and the
length of the target sequence. We can do a ﬁner-grained
analysis of each model by observing its behavior on exam-
ples of different difﬁculty levels.

In Python program evaluation, we closely follow (Zaremba
& Sutskever, 2014) and compute the test accuracy as the
next step symbol prediction given a sequence of correct
preceding symbols.

Table 1. The sizes of the models used in character-level language
modeling. Gated Feedback L is a GF-RNN with a same number
of hidden units as a Stacked RNN (but more parameters). The
number of units is shown as (number of hidden layers)
× (number of hidden units per layer).
# of Units

Architecture

Unit

tanh

GRU

LSTM

Single
Stacked
Gated Feedback

1 × 1000
3 × 390
3 × 303

Single
Stacked
Gated Feedback
Gated Feedback L

Single
Stacked
Gated Feedback
Gated Feedback L

1 × 540
3 × 228
3 × 165
3 × 228

1 × 456
3 × 191
3 × 140
3 × 191

4.2. Models

We compared three different RNN architectures: a single-
layer RNN, a stacked RNN and the proposed GF-RNN. For
each architecture, we evaluated three different transition
functions: tanh + afﬁne, long short-term memory (LSTM)
and gated recurrent unit (GRU). For fair comparison, we
constrained the number of parameters of each model to be
roughly similar to each other.

For each task, in addition to these capacity-controlled ex-
periments, we conducted a few extra experiments to further
test and better understand the properties of the GF-RNN.

4.2.1. LANGUAGE MODELING

For the task of character-level language modeling, we con-
strained the number of parameters of each model to corre-
spond to that of a single-layer RNN with 1000 tanh units
(see Table 1 for more details). Each model is trained for at
most 100 epochs.

We used RMSProp (Hinton, 2012) and momentum to tune
the model parameters (Graves, 2013). According to the
preliminary experiments and their results on the validation
set, we used a learning rate of 0.001 and momentum coef-
ﬁcient of 0.9 when training the models having either GRU
or LSTM units. It was necessary to choose a much smaller
learning rate of 5 × 10−5 in the case of tanh units to ensure
the stability of learning. Whenever the norm of the gradient
explodes, we halve the learning rate.

Each update is done using a minibatch of 100 subsequences
of length 100 each, to avoid memory overﬂow problems
when unfolding in time for backprop. We approximate full
back-propagation by carrying the hidden states computed
at the previous update to initialize the hidden units in the
next update. After every 100-th update, the hidden states

Gated Feedback Recurrent Neural Networks

(a) GRU

(b) LSTM

Figure 2. Validation learning curves of three different RNN architectures; Stacked RNN, GF-RNN with the same number of model
parameters and GF-RNN with the same number of hidden units. The curves represent training up to 100 epochs. Best viewed in colors.

were reset to all zeros.

Table 2. Test set BPC (lower is better) of models trained on the
Hutter dataset for a 100 epochs.
(∗) The gated-feedback RNN
with the global reset gates ﬁxed to 1 (see Sec. 5.1 for details).
Bold indicates statistically signiﬁcant winner over the column
(same type of units, different overall architecture).

Single-layer
Stacked
Gated Feedback
Gated Feedback L
Feedback∗

tanh

1.937
1.892
1.949
–
–

GRU LSTM
1.887
1.883
1.868
1.871
1.842
1.855
1.789
1.813
1.854
–

4.2.2. PYTHON PROGRAM EVALUATION

For the task of Python program evaluation, we used an
RNN encoder-decoder based approach to learn the map-
ping from Python scripts to the corresponding outputs as
done by Cho et al. (2014); Sutskever et al. (2014) for ma-
chine translation. When training the models, Python scripts
are fed into the encoder RNN, and the hidden state of the
encoder RNN is unfolded for 50 timesteps. Prediction is
performed by the decoder RNN whose initial hidden state
is initialized with the last hidden state of the encoder RNN.
The ﬁrst hidden state of encoder RNN h0 is always initial-
ized to a zero vector.

For this task, we used GRU and LSTM units either with
or without the gated-feedback connections. Each encoder
or decoder RNN has three hidden layers. For GRU, each
hidden layer contains 230 units, and for LSTM each hidden
layer contains 200 units.

Following Zaremba & Sutskever (2014), we used the mixed

curriculum strategy for training each model, where each
training example has a random difﬁculty sampled uni-
formly. We generated 320, 000 examples using the script
provided by Zaremba & Sutskever (2014), with the nesting
randomly sampled from [1, 5] and the target length from
(cid:2)1, 1010(cid:3).

We used Adam (Kingma & Ba, 2014) to train our models,
and each update was using a minibatch with 128 sequences.
We used a learning rate of 0.001 and β1 and β2 were both
set to 0.99. We trained each model for 30 epochs, with
early stopping based on the validation set performance to
prevent over-ﬁtting.

At test time, we evaluated each model on multiple sets of
test examples where each set is generated using a ﬁxed tar-
get length and number of nesting levels. Each test set con-
tains 2, 000 examples which are ensured not to overlap with
the training set.

5. Results and Analysis

5.1. Language Modeling

It is clear from Table 2 that the proposed gated-feedback ar-
chitecture outperforms the other baseline architectures that
we have tried when used together with widely used gated
units such as LSTM and GRU. However, the proposed ar-
chitecture failed to improve the performance of a vanilla-
RNN with tanh units.
In addition to the ﬁnal modeling
performance, in Fig. 2, we plotted the learning curves of
some models against wall-clock time (measured in sec-
onds). RNNs that are trained with the proposed gated-
feedback architecture tends to make much faster progress
over time. This behavior is observed both when the number
of parameters is constrained and when the number of hid-

Gated Feedback Recurrent Neural Networks

Table 3. Generated texts with our trained models. Given the seed at the left-most column (bold-faced font), the models predict next
200 ∼ 300 characters. Tabs, spaces and new-line characters are also generated by the models.

Seed

Stacked LSTM

GF-LSTM

[[pl:Icon]]
[[pt:Icon]]
[[ru:Icon]]
[[sv:Programspraket Icon]]</text>

</revision>

</page>
<page>

<title>Iconology</title>
<id>14802</id>
<revi

<revision>
<id>15908383</id>
<timestamp>

2002-07-20T18:33:34Z

</timestamp>
<contributor>

<revision>
<id>41968413</id>
<timestamp>

2006-09-03T11:38:06Z

</timestamp>
<contributor>

<username>The Courseichi</userrand

vehicles in [[enguit]].

<username>Navisb</username>
<id>46264</id>

==The inhibitors and alphabetsy and moral/
hande in===In four [[communications]] and

</contributor>
<comment>The increase from the time

<title>Inherence relation</title>
<id>14807</id>
<revision>

<id>34980694</id>
<timestamp>

2006-01-13T04:19:25Z

</timestamp>
<contributor>

<username>Ro

<username>Robert]]

[[su:20 aves]]
[[vi:10 Februari]]
[[bi:16 agostoferos´ın]]
[[pt:Darenetische]]
[[eo:Hebrew selsowen]]
[[hr:2 febber]]
[[io:21 februari]]
[[it:18 de februari]]

<username>Roma</username>
<id>48</id>
</contributor>
<comment>Vly’’’ and when one hand

is angels and [[ghost]] borted and
’’mask r:centrions]], [[Afghanistan]],
[[Glencoddic tetrahedron]], [[Adjudan]],
[[Dghacn]], for example, in which materials
dangerous (carriers) can only use with one

den units is constrained. This suggests that the proposed
GF-RNN signiﬁcantly facilitates optimization/learning.

Effect of Global Reset Gates

After observing the superiority of the proposed gated-
feedback architecture over the single-layer or conventional
stacked ones, we further trained another GF-RNN with
LSTM units, but this time, after ﬁxing the global reset
gates to 1 to validate the need for the global reset gates.
Without the global reset gates, feedback signals from the
upper recurrent layers inﬂuence the lower recurrent layer
fully without any control. The test set BPC of GF-LSTM
without global reset gates was 1.854 which is in between
the results of conventional stacked LSTM and GF-LSTM
with global reset gates (see the last row of Table 2) which
conﬁrms the importance of adaptively gating the feedback
connections.

Qualitative Analysis: Text Generation

Here we qualitatively evaluate the stacked LSTM and GF-
LSTM trained earlier by generating text. We choose a sub-
sequence of characters from the test set and use it as an
initial seed. Once the model ﬁnishes reading the seed text,
we let the model generate the following characters by sam-
pling a symbol from softmax probabilities of a timestep and
then provide the symbol as next input.

Given two seed snippets selected randomly from the test
set, we generated the sequence of characters ten times for
each model (stacked LSTM and GF-LSTM). We show one
of those ten generated samples per model and per seed snip-
pet in Table 3. We observe that the stacked LSTM failed to
close the tags with </username> and </contributor>
in both trials. However, the GF-LSTM succeeded to close

both of them, which shows that it learned about the struc-
ture of XML tags. This type of behavior could be seen
throughout all ten random generations.

language models trained
Table 4. Test set BPC of neural
on the Hutter dataset, MRNN = multiplicative RNN re-
sults from Sutskever et al. (2011) and Stacked LSTM results
from Graves (2013).

MRNN Stacked LSTM GF-LSTM

1.60

1.67

1.58

Large GF-RNN

We trained a larger GF-RNN that has ﬁve recurrent layers,
each of which has 700 LSTM units. This makes it possible
for us to compare the performance of the proposed archi-
tecture against the previously reported results using other
types of RNNs. In Table 4, we present the test set BPC
by a multiplicative RNN (Sutskever et al., 2011), a stacked
LSTM (Graves, 2013) and the GF-RNN with LSTM units.
The performance of the proposed GF-RNN is comparable
to, or better than, the previously reported best results. Note
that Sutskever et al. (2011) used the vocabulary of 86 char-
acters (removed XML tags and the Wikipedia markups),
and their result is not directly comparable with ours. In this
experiment, we used Adam instead of RMSProp to opti-
mize the RNN. We used learning rate of 0.001 and β1 and
β2 were set to 0.9 and 0.99, respectively.

5.2. Python Program Evaluation

Fig. 3 presents the test results of each model represented in
heatmaps. The accuracy tends to decrease by the growth

Gated Feedback Recurrent Neural Networks

(a) Stacked RNN

(b) Gated Feedback RNN

(c) Gaps between (a) and (b)

Figure 3. Heatmaps of (a) Stacked RNN, (b) GF-RNN, and (c) difference obtained by substracting (a) from (b). The top row is the
heatmaps of models using GRUs, and the bottom row represents the heatmaps of the models using LSTM units. Best viewed in colors.

of the length of target sequences or the number of nesting
levels, where the difﬁculty or complexity of the Python pro-
gram increases. We observed that in most of the test sets,
GF-RNNs are outperforming stacked RNNs, regardless of
the type of units. Fig. 3 (c) represents the gaps between the
test accuracies of stacked RNNs and GF-RNNs which are
computed by subtracting (a) from (b). In Fig. 3 (c), the red
and yellow colors, indicating large gains, are concentrated
on top or right regions (either the number of nesting lev-
els or the length of target sequences increases). From this
we can more easily see that the GF-RNN outperforms the
stacked RNN, especially as the number of nesting levels
grows or the length of target sequences increases.

6. Conclusion

We proposed a novel architecture for deep stacked RNNs
which uses gated-feedback connections between differ-
ent layers. Our experiments focused on challenging se-
quence modeling tasks of character-level language mod-
eling and Python program evaluation. The results were
consistent over different datasets, and clearly demonstrated
that gated-feedback architecture is helpful when the mod-
els are trained on complicated sequences that involve long-
term dependencies. We also showed that gated-feedback
architecture was faster in wall-clock time over the train-
ing and achieved better performance compared to standard

stacked RNN with a same amount of capacity. Large GF-
LSTM was able to outperform the previously reported best
results on character-level language modeling. This sug-
gests that GF-RNNs are also scalable. GF-RNNs were able
to outperform standard stacked RNNs and the best previ-
ous records on Python program evaluation task with vary-
ing difﬁculties.

We noticed a deterioration in performance when the pro-
posed gated-feedback architecture was used together with
a tanh activation function, unlike when it was used with
more sophisticated gated activation functions. More thor-
ough investigation into the interaction between the gated-
feedback connections and the role of recurrent activation
function is required in the future.

Acknowledgments

The authors would like to thank the developers of
Theano (Bastien et al., 2012) and Pylearn2 (Goodfellow
et al., 2013). Also, the authors thank Yann N. Dauphin
and Laurent Dinh for insightful comments and discussion.
We acknowledge the support of the following agencies for
research funding and computing support: NSERC, Sam-
sung, Calcul Qu´ebec, Compute Canada, the Canada Re-
search Chairs and CIFAR.

Gated Feedback Recurrent Neural Networks

References

Bahdanau, Dzmitry, Cho, Kyunghyun, and Bengio,
Yoshua. Neural machine translation by jointly learning
to align and translate. Technical report, arXiv preprint
arXiv:1409.0473, 2014.

Bastien, Fr´ed´eric, Lamblin, Pascal, Pascanu, Razvan,
Bergstra, James, Goodfellow, Ian J., Bergeron, Arnaud,
Bouchard, Nicolas, and Bengio, Yoshua. Theano: new
features and speed improvements. Deep Learning and
Unsupervised Feature Learning NIPS 2012 Workshop,
2012.

Bengio, Yoshua, Simard, Patrice, and Frasconi, Paolo.
Learning long-term dependencies with gradient descent
IEEE Transactions on Neural Networks, 5
is difﬁcult.
(2):157–166, 1994.

Bengio, Yoshua, Ducharme, R´ejean, and Vincent, Pascal.
A neural probabilistic language model. In Adv. Neural
Inf. Proc. Sys. 13, pp. 932–938, 2001.

Cho, Kyunghyun, Van Merri¨enboer, Bart, Gulcehre,
Caglar, Bougares, Fethi, Schwenk, Holger, and Ben-
gio, Yoshua. Learning phrase representations using
rnn encoder-decoder for statistical machine translation.
arXiv preprint arXiv:1406.1078, 2014.

El Hihi, Salah and Bengio, Yoshua. Hierarchical recur-
rent neural networks for long-term dependencies. In Ad-
vances in Neural Information Processing Systems, pp.
493–499. Citeseer, 1995.

Gers, Felix A., Schmidhuber, J¨urgen, and Cummins,
Fred A. Learning to forget: Continual prediction with
LSTM. Neural Computation, 12(10):2451–2471, 2000.

Goodfellow, Ian J., Warde-Farley, David, Lamblin, Pascal,
Dumoulin, Vincent, Mirza, Mehdi, Pascanu, Razvan,
Bergstra, James, Bastien, Fr´ed´eric, and Bengio, Yoshua.
Pylearn2: a machine learning research library. arXiv
preprint arXiv:1308.4214, 2013.

Graves, Alex. Generating sequences with recurrent neural

networks. arXiv preprint arXiv:1308.0850, 2013.

Hermans, Michiel and Schrauwen, Benjamin. Training and
analysing deep recurrent neural networks. In Advances
in Neural Information Processing Systems, pp. 190–198,
2013.

Hinton, Geoffrey. Neural networks for machine learning.

Coursera, video lectures, 2012.

Hochreiter, Sepp.

Untersuchungen zu dynamischen
Institut f¨ur In-
neuronalen Netzen. Diploma thesis,
formatik, Lehrstuhl Prof. Brauer, Technische Uni-
URL http://www7.
versit¨at M¨unchen, 1991.
informatik.tu-muenchen.de/˜Ehochreit.

Hochreiter, Sepp. The vanishing gradient problem dur-
ing learning recurrent neural nets and problem solu-
International Journal of Uncertainty, Fuzziness
tions.
and Knowledge-Based Systems, 6(02):107–116, 1998.

Hochreiter, Sepp and Schmidhuber, J¨urgen. Long short-
term memory. Neural Computation, 9(8):1735–1780,
1997.

Hutter, Marcus. The human knowledge compression con-
test. 2012. URL http://prize.hutter1.net/.

Kingma, Diederik and Ba,

Jimmy.
method for stochastic optimization.
arXiv:1412.6980, 2014.

Adam:

A
arXiv preprint

Koutn´ık, Jan, Greff, Klaus, Gomez, Faustino, and Schmid-
In Proceedings of
huber, J¨urgen. A clockwork rnn.
the 31st International Conference on Machine Learning
(ICML’14), 2014.

Mikolov, Tomas. Statistical Language Models based on
Neural Networks. PhD thesis, Brno University of Tech-
nology, 2012.

Mikolov, Tomas, Sutskever, Ilya, Deoras, Anoop, Le, Hai-
Son, Kombrink, Stefan, and Cernocky, J. Subword lan-
guage modeling with neural networks. Preprint, 2012.

Schmidhuber, J¨urgen. Learning complex, extended se-
quences using the principle of history compression. Neu-
ral Computation, 4(2):234–242, 1992.

Stollenga, Marijn F, Masci, Jonathan, Gomez, Faustino,
and Schmidhuber, J¨urgen. Deep networks with internal
selective attention through feedback connections. In Ad-
vances in Neural Information Processing Systems, pp.
3545–3553, 2014.

Sutskever, Ilya, Martens, James, and Hinton, Geoffrey E.
Generating text with recurrent neural networks. In Pro-
ceedings of the 28th International Conference on Ma-
chine Learning (ICML’11), pp. 1017–1024, 2011.

Sutskever, Ilya, Vinyals, Oriol, and Le, Quoc VV. Se-
quence to sequence learning with neural networks.
In
Advances in Neural Information Processing Systems, pp.
3104–3112, 2014.

Zaremba, Wojciech and Sutskever, Ilya. Learning to exe-

cute. arXiv preprint arXiv:1410.4615, 2014.

Zaremba, Wojciech, Sutskever, Ilya, and Vinyals, Oriol.
Recurrent neural network regularization. arXiv preprint
arXiv:1409.2329, 2014.

