2
2
0
2

b
e
F
7

]

C
O
.
h
t
a
m

[

1
v
6
3
7
6
0
.
2
0
2
2
:
v
i
X
r
a

Minimizing Entropy to Discover Good Solutions
to Recurrent Mixed Integer Programs

Charly Robinson La Rocca1, Emma Frejinger1, and Jean-Fran¸cois Cordeau2

1

Universit´e de Montr´eal, Montr´eal, Canada

{charly.robinson.la.rocca,emma.frejinger}@umontreal.ca
2 HEC Montr´eal, Montr´eal, Canada
jean-francois.cordeau@hec.ca

Abstract. Current state-of-the-art solvers for mixed-integer program-
ming (MIP) problems are designed to perform well on a wide range of
problems. However, for many real-world use cases, problem instances
come from a narrow distribution. This has motivated the development
of specialized methods that can exploit the information in historical
datasets to guide the design of heuristics. Recent works have shown
that machine learning (ML) can be integrated with an MIP solver to
inject domain knowledge and eﬃciently close the optimality gap. This
hybridization is usually done with deep learning (DL), which requires a
large dataset and extensive hyperparameter tuning to perform well. This
paper proposes an online heuristic that uses the notion of entropy to ef-
ﬁciently build a model with minimal training data and tuning. We test
our method on the locomotive assignment problem (LAP), a recurring
real-world problem that is challenging to solve at scale. Experimental
results show a speed up of an order of magnitude compared to a general
purpose solver (CPLEX) with a relative gap of less than 2%. We also
observe that for some instances our method can discover better solutions
than CPLEX within the time limit.

Keywords: Machine Learning · Combinatorial Optimization · Learning
Heuristics · Fleet Management Problem.

1

Introduction

Machine learning (ML) has made major leaps forward with the maturation of
specialized hardware [23] and software [18] stacks. ML enables practitioners to
automatically ﬁnd useful patterns from a dataset without the extensive domain
knowledge often required to make progress on a given problem. In the context
of our research, we are interested in the exploitation of learning algorithms to
eﬃciently solve large combinatorial optimization (CO) problems. The idea to
leverage the knowledge from historical data is not new. Many strategies have
emerged in recent years to take advantage of learning methods and improve
upon standard solvers. [3] provide a tour d’horizon on ML for combinatorial
optimization and give an overview on the diﬀerent ways in which they can be

 
 
 
 
 
 
2

C. Robinson et al.

integrated. The ﬁrst idea uses ML to directly predict the solution of the prob-
lem instance from the input features. One example of this is the application of
the pointer network developed by [22] to learn to predict the optimal permu-
tation for the travelling salesman problem (TSP). The second approach is to
learn to optimize the hyperparameters of the CO algorithm. Third, ML can also
be used alongside the optimization algorithm. In practice, this is often done via
the integration of learning in the branch-and-bound (B&B) framework [15]. Our
approach belongs to the third class of hybrid methods.

This paper proposes an approach that focuses on simplicity and eﬃciency.
Unlike most recent papers on the subject, we avoid using any form of deep learn-
ing (DL). Instead, we exploit the well-known notion of entropy to make decisions
that minimize risk when heuristically adding cuts to the MIP. We favour classic
ML models to ﬁt our data because of their robustness; they require signiﬁcantly
fewer data points and tuning than a deep neural network (DNN). The model is
updated online during the B&B and its predictions are used to guide the deci-
sion of adding cuts to the MIP. The goal is to remove from the search space the
solutions that are the least likely to be optimal. We apply this strategy to the
locomotive assignment problem (LAP), a problem that is not only challenging to
solve but also recurring i.e., similar problem instances are solved repeatedly over
time. Hence, there is a substantial economic incentive to ﬁnd a heuristic that
can solve the LAP quickly. We use the Canadian National Railway Company
(CN) as the subject of our case study for the LAP.

This document ﬁrst presents some related work on the integration of ML
for CO problems. This is followed by an overview on how we use the notion of
entropy to reduce the computational load necessary to solve the LAP. Finally,
we include some preliminary results for the speed up and the gap relative to our
baseline CPLEX.

2 Related works

One deﬁning property of NP-hard CO problems is the fact that there are no algo-
rithms with guarantees that can solve them in polynomial time unless P = NP.
In practice, this means that they may be intractable when the size of the in-
stance is large. This is why eﬃcient heuristics are sought after. In this case, we
pay the penalty of no optimality guarantees to ﬁnd solutions in a reasonable
amount of time. The design of heuristics requires substantial problem-speciﬁc
research and trial-and-error on the part of practitioners. This constitutes the
main motivation to use ML, since it can extract valuable patterns from data
without domain knowledge about the given problem.

The ﬁrst question that arises when integrating ML into CO is how to repre-
sent the input instance. CO problems may comprise both continuous and integer
variables where the relationship is often modelled with graph data structures.

Minimizing Entropy to Discover Good Solutions to Recurrent MIPs

3

Unlike images, audio, and text, which have a clear grid structure, graphs have
irregular structures, making it challenging to generalize some of the basic math-
ematical operations to them. The dominant and fast-growing paradigm for DL
with graph data is the GNN formalism [13]. There have been many successful
attempts at using GNNs to solve CO problems [19,21]. We have also identiﬁed
recent papers on GNNs that focus on how to formally improve their expressive-
ness, since there is a limited understanding of their representational properties
and limitations [24,1]. There is a bottleneck issue where all the information from
the related nodes is compressed into a ﬁxed-length vector and therefore GNNs
fail to propagate messages ﬂowing from distant nodes. By way of contrast, our
method is able to learn from a few features that are designed to be tightly
correlated with the variables that we want to predict. This allows us to use a
non-parametric model that can be trained eﬃciently compared to DNNs.

During the B&B, there are many diﬀerent types of decisions that have to be
made at each node in the tree. The two most common ones are associated with
the branching scheme: variable selection (which fractional variable to branch
on) and node selection (which of the currently open nodes to process next). The
popular framework used to integrate ML within the B&B is RL [11,5,7,2]. This
is expected because there is a natural ﬁt between RL and the iterative algorithm
used to solve CO problems. We can model the branching decisions as actions
made by the agent. The state of the environment can be modelled as the input
instance with the current position in the B&B tree. Nevertheless, RL has limita-
tions that make it challenging to implement. The main concern comes from the
design of the reward function. Learning in the RL framework is mainly driven
by the reward signal which associates a reward value to each transition. Some
use a reward of -1 at each step to motivate exploration but that reward func-
tion contains no guiding signal [6]. Moreover, training an agent using RL may
be unstable and tricks such as replay memory and gradient clipping are often
necessary to achieve good performance. For these reasons, instead of using RL
to train our policy, we design it around the idea of risk minimization. We use
the entropy of the variable to measure how risky it is to add a lazy constraint to
it. The simplicity of our method makes the training process trivial, signiﬁcantly
improving its robustness compared to RL.

In summary, we noticed that GNNs and RL are popular tools used to learn
to solve CO problems. However, they require extensive training and tuning to
yield acceptable performance. In contrast, our method targets the idea of entropy
minimization to directly capture the relevant information from historical data.
As a consequence, we can take advantage of simpler models that are less prone
to over-ﬁtting. Furthermore, the online nature of the model means that most of
the learning happens during the B&B of the test instance. Those design choices
creates the opportunity for an accurate model that requires minimal training
data and manual tuning.

4

C. Robinson et al.

3 Methodology

We start with a general MIP formulation which is described using the notation
arg minx{cT x | Ax ≤ b, x ∈ B ∪ Q ∪ P}, where x is the vector of decision vari-
ables that is partitioned into B, Q and P, the index sets of binary, integer and
continuous variables, respectively. To be compatible with our methodology, the
MIP needs to have at minimum the following constraints

xk
v = 1

∀v ∈ V,

(1)

k∈K
X

where the binary variable xk
v ∈ B is equal to 1 when we assign the class k ∈ K
to the object v ∈ V. The constraints (1) ensure that only one class is possible
per object. We use the notation kvt to refer to the class of v at iteration t during
the B&B. We characterize our methodology as general because it is often trivial
to transform a MIP to ﬁt this formulation. In theory, one can choose to convert
an integer variable into a set of binary variables without changing the problem
deﬁnition. The idea is to use a ML model to predict the class of some objects and
let the MIP solver optimize all the other variables. If we ﬁx the binary variables
to be equal to their optimal value, we expect that the time required to solve the
MIP will be greatly reduced.

Representation embedding. The current method uses an online learn-
ing algorithm that updates its belief dynamically during the B&B as new data
becomes available. This is a signiﬁcant change compared to the standard su-
pervised learning approach where the dataset is generated oﬄine [14,12]. In the
online paradigm, the feature vector needs to be updated at every iteration. Let
φt(v) be the feature vector associated with variable v in the MIP at iteration t
in the B&B. To avoid performance concerns, the complexity of the function that
computes φt(v) given φt−1(v) should be O(1). For this reason, φt(v) is calcu-
lated using statistics that can be easily updated such as the mean, the variance
and the extrema (min and max). These statistics are evaluated on the solutions
collected during B&B up until the current iteration t: Kvt = {kv1, kv2, ..., kvt}.
Formally, the feature vector φt(v) is calculated using the expression

φt(v) = [mean(Kvt), var(Kvt), max(Kvt), min(Kvt)] .

(2)

The next step is to discuss the corresponding label sv associated with v. The
label is binary and informs us about the stability of the variable: sv = 1 when
the variable v is stable and sv = 0 otherwise. The idea of using the concept of
stability as a label was inspired by [9]. We build on this idea by using the notion of
i P (zi) log P (zi)
entropy H to infer the stability [20]. It is deﬁned as H(Z) = −
where P (zi) is the probability associated with the outcome zi for the the random
variable Z. We convert the entropy into a binary variable using the following
function

P

sv =

if H(Kv) > m

0,
1, otherwise,

(

(3)

Minimizing Entropy to Discover Good Solutions to Recurrent MIPs

5

where H(Kv) is the entropy of the visited solutions and m is the median of the
entropy for each variable in the instance. To generate a balanced training set,
we use the median because it reliably creates a 50/50 split between stable and
unstable variables.

Machine learning model. The notable attribute of our model is its on-
line nature. Also, the model is potentially called during each callback of the
solver, which incentivizes a lean design. We decided to use the Fast Random
Forest (FRF) model from the OnlineStats.jl library written in Julia [8]. This
ensemble model has fast processing speed that can be updated in constant time.
Furthermore, the advantage of having the whole software stack written in Julia
signiﬁcantly reduces the overhead that we usually have with libraries in Python
and C++.

Policies. A policy decides which action to take given the predictions gen-
erated with the ML model. In our application, the action space is deﬁned by
the interface of the general purpose solver CPLEX. We can name a few diﬀer-
ent types of actions such as user cuts, lazy constraints and branching rules. We
selected lazy constraints as our main tool to interact with the solver because it
limits the solution directly, which greatly reduces the computational time when
it is done appropriately.

We propose some basic rules and assumptions to signiﬁcantly simplify the
design space of policies. First, we assume that lazy constraints can only be added
after an incumbent solution is found. Second, the policy should be risk averse
to avoid removing the optimal solution from the search space. Third, we pose
that each lazy constraint should have the following form: xk
v = 1 where k is the
predicted class i.e., k = ˜kv. We use these assumptions to design two diﬀerent
types of policies: a scoring policy that uses the entropy directly and a threshold
policy that uses the predictions of the FRF model to select the variables to ﬁx.

The scoring policy (SP) uses a scoring system to measure the risk associated
with ﬁxing each variable in the MIP. At each incumbent, SP computes the score
σv for each variable v as follows: σv = −H(Kvt) where H(Kvt) is the entropy of
the solutions visited up until the current iteration t. We use the negative because
we want to maximize the score. In other words, we are interested in the variables
that are the least likely to change. Once the scores are computed, we select the
top n from the list and add a lazy constraint to each one of them, where ˜kv is
the most common class in Kvt. For example, SP(n = 5) will select ﬁve variables
to ﬁx at each incumbent using the entropy based scoring system.

The threshold policy (TP) is a more aggressive version of SP because every
constraint is added after the ﬁrst incumbent. TP uses a threshold τ to ﬁlter
which variable should be ﬁxed based on the prediction history of the ML model.
A prediction ˜svt is made at every iteration t in the B&B and it is stored in what

6

C. Robinson et al.

we call the prediction history ˜Sv = {˜sv1, ˜sv2, ..., ˜svt}. If the model systematically
predicts that the variable is stable, the policy will add a constraint to it. More
formally, the set of selected variables is {v|mean( ˜Sv) > τ }. For example, TP(τ =
0.5) will select all the variables for which the ML model predicts ˜sv = 1 more
than 50% of the time.

4 Experiments

We apply our methodology to the LAP using the historical schedules of CN. This
problem is diﬃcult to solve without partitioning it into daily sub-problems, be-
cause it contains a large number of binary variables. It follows daily and weekly
cyclical patterns which are useful for learning. The goal of the LAP is to ﬁnd
the cheapest way to send a certain number of locomotives through a network
such that all capacity and demand constraints are satisﬁed. The decisions xk
v
represent the conﬁguration of locomotives k assigned to each train arc v in the
instance. Other variables are required to model ﬂow conservation constraints for
each node in the network. We refer to [17] for the details on the MIP formulation
of the LAP.

Setup. We deﬁne three instance sizes: small (100 to 250 trains), medium
(250 to 450 trains) and large (450 to 800 trains). For each size, we generate 10
consecutive weekly instances that are solved with a time limit of 30 minutes. We
compare the baseline CPLEX with three policies: SP (n = 1), SP (n = 5) and
T P (τ = 0.5). Our software stack is fully written in Julia and we use the JuMP
library to interact with the solver [10]. We run our experiments on a t2.xlarge in-
stance on Amazon Web Services (AWS) with 4 vCPU and 16 GB of memory. The
FRF model is trained on a single small instance to learn to predict the stability
of each decision variable. The training procedure takes 4.54 seconds to complete.

P

Metrics. We consider the work of [4], which is focused on this speciﬁc is-
sue of measuring the impact of primal heuristics. They introduce a performance
measure that takes into account the whole solution process by computing the
integral of the primal gap over time. They deﬁne the primal integral P I of a
i p(ti−1) · (ti − ti−1), where ti is the point in time when the ith
run as P I :=
new incumbent solution is found and p(ti) is the primal gap associated with it.
The smaller P I is, the better the expected quality of the solution if we stop the
solver at an arbitrary point in time. To get a relative measure of performance,
we propose the primal integral ratio (PIR) which is the ratio between the primal
integral of the heuristic and the baseline. To provide a good intuitive under-
standing for how much faster the heuristic is, we also compute a custom version
of the speed up. Most instances reach the time limit of 30 minutes, therefore we
compute its value at every node in the B&B. By leveraging a linear interpola-
tion, we can create a function that returns the speed up for a given gap. From
this, we can evaluate an upper bound on the speed up and its associated gap; we
call them best speed up and gap at best speed up respectively. We should mention

Minimizing Entropy to Discover Good Solutions to Recurrent MIPs

7

that the gap is measured relative to the best feasible solution found by CPLEX
within the time limit.

Results. For the metric PIR we group the results by policy and instance
size. The body on the box plot illustrate the 25th and 75th percentiles of the
distribution. Fig. 1 shows that the 75th pencentile of PIR is 0.969, 0.977 and
1.17 for the policies SP (n = 1), SP (n = 5) and T P (τ = 0.5) respectively.
Since for most instances the PIR is below 1, it means that we converge faster
than CPLEX in general. In Fig. 2, we have a similar pattern for small and
medium instances as their box plot body is well below 1. There is a noticeable
performance drop for large instances (25th and 75th percentiles are 0.972 and
1.15 for the PIR) which is expected given that we train our model on a small
instance. Surprisingly, we measure a negative relative gap for 66.7% (20/30)
of large instances, which suggests that our method can also be useful to ﬁnd a
better feasible solution within a limited time frame. Figs. 3-4 illustrate the speed
up and gap respectively. TP is the most aggressive one and this is noticeable
here, as we observe that the limit for outliers is 38x. The 75th percentile of the
gap is below 2% for all policies, which is reasonable for users that value speed
over quality. The good performance of our method can be explained by the high
action accuracy i.e., the ratio of lazy cuts that preserve the optimal solution. We
measure an action accuracy per instance of 94.4% on average and 80.4% in the
worst case. We ought to mention that we experimented with supervised learning
methods that directly predict the solution of instances using the problem graph
as input. This approach had a substantially worst accuracy (between 50% and
75%) and, as a result, the solution quality was unsatisfactory.

2.25

2.00

1.75

1.50

1.25

R
P

I

1.00

0.75

0.50

0.25

2.25

2.00

1.75

1.50

1.25

R
P

I

1.00

0.75

0.50

0.25

SP(n=1)

SP(n=5)

TP(t=0.5)

small

medium

large

Fig. 1. PIR for diﬀerent policies

Fig. 2. PIR for diﬀerent instance sizes

8

C. Robinson et al.

p
u

d
e
e
p
s

t
s
e
b

70

60

50

40

30

20

10

0

p
u

d
e
e
p
s

t
s
e
b

t
a

p
a
g

0.04

0.03

0.02

0.01

0.00

SP(n=1)

SP(n=5)

TP(t=0.5)

SP(n=1)

SP(n=5)

TP(t=0.5)

Fig. 3. Best speed up for diﬀerent poli-
cies

Fig. 4. Gap at best speed up for diﬀer-
ent policies

5 Perspectives and future work

There are many open questions with respect to the integration of ML into CO
solvers. There is the important question of how to represent the input data of
the problem instance. According to our reading of the literature, GNNs or more
generally DNNs are the most popular tools used for representation learning.
Given enough data, they can learn a high quality embedding of the input space
without requirements for prior domain knowledge from practitioners. However,
they come with practical challenges that impinge upon the research process.
DNNs are more costly and data intensive to train than the model considered in
this work.

The complexity overhead of DL motivated our design of a new method that
decides how to add cuts to a MIP via entropy minimization. Given the online
aspect of the model, most of the learning happens at test time and, therefore,
it requires a negligible amount of resources to train in terms of both data and
time. A method like ours, that takes less than 5 seconds to train, allows us to
iterate faster on the core mechanic of the heuristic. This strategy ended up being
successful, as we were able to ﬁnd feasible solutions for the LAP with a relative
gap below 2% up to 10 times faster than CPLEX. There are still many paths
to explore to validate the methodology. We plan to apply it to other similar CO
problems such as the load planning problem [16]. We identiﬁed the containers-
to-car matching problem as a potential entry point for our learned heuristic.
Once it is validated on multiple problems, the next step is to tackle the scaling
challenges. Future work will be dedicated to issues occurring when solving even
larger instances.

 
 
 
 
 
 
Minimizing Entropy to Discover Good Solutions to Recurrent MIPs

9

References

1. Alon, U., Yahav, E.: On the bottleneck of graph neural networks and its practical
implications. In: International Conference on Learning Representations (2021)
2. Barrett, T., Clements, W., Foerster, J., Lvovsky, A.: Exploratory combinatorial
optimization with reinforcement learning. In: Proceedings of the AAAI Conference
on Artiﬁcial Intelligence. vol. 34, pp. 3243–3250 (2020)

3. Bengio, Y., Lodi, A., Prouvost, A.: Machine learning for combinatorial optimiza-
tion: A methodological tour d’horizon. European Journal of Operational Research
(2020)

4. Berthold, T.: Measuring the impact of primal heuristics. Operations Research Let-

ters 41(6), 611–614 (2013)

5. Cappart, Q., Moisan, T., Rousseau, L.M., Pr´emont-Schwarz, I., Cire, A.: Com-
bining reinforcement learning and constraint programming for combinatorial opti-
mization. arXiv preprint arXiv:2006.01610 (2020)

6. Chalumeau, F., Coulon, I., Cappart, Q., Rousseau, L.M.: Seapearl: A constraint
programming solver guided by reinforcement learning. In: Stuckey, P.J. (ed.) In-
tegration of Constraint Programming, Artiﬁcial Intelligence, and Operations Re-
search. pp. 392–409. Springer International Publishing (2021)

7. Dai, H., Khalil, E.B., Zhang, Y., Dilkina, B., Song, L.: Learning combinatorial op-
timization algorithms over graphs. In: Proceedings of the 31st International Con-
ference on Neural Information Processing Systems. p. 6351–6361 (2017)

8. Day, J., Zhou, H.: Onlinestats.jl: A julia package for statistics on data streams.

Journal of Open Source Software 5(46) (2020)

9. Ding, J.Y., Zhang, C., Shen, L., Li, S., Wang, B., Xu, Y., Song, L.: Accelerating
primal solution ﬁndings for mixed integer programs based on solution prediction.
In: Proceedings of the AAAI Conference on Artiﬁcial Intelligence. vol. 34, pp.
1452–1459 (2020)

10. Dunning, I., Huchette, J., Lubin, M.: Jump: A modeling language for mathematical

optimization. SIAM review 59(2), 295–320 (2017)

11. Gasse, M., Ch´etelat, D., Ferroni, N., Charlin, L., Lodi, A.: Exact combinatorial
optimization with graph convolutional neural networks. In: Advances in Neural
Information Processing Systems. pp. 15580–15592 (2019)

12. Gupta, P., Gasse, M., Khalil, E.B., Kumar, M.P., Lodi, A., Bengio, Y.: Hybrid
models for learning to branch. In: Advances in Neural Information Processing
Systems 33 (2020)

13. Hamilton, W.L.: Graph representation learning. Synthesis Lectures on Artiﬁcal

Intelligence and Machine Learning 14(3), 1–159 (2020)

14. Khalil, E., Le Bodic, P., Song, L., Nemhauser, G., Dilkina, B.: Learning to branch in
mixed integer programming. In: Proceedings of the AAAI Conference on Artiﬁcial
Intelligence. vol. 30 (2016)

15. Lodi, A., Zarpellon, G.: On learning and branching: a survey. Top 25(2), 207–236

(2017)

16. Mantovani, S., Morganti, G., Umang, N., Crainic, T.G., Frejinger, E., Larsen, E.:
The load planning problem for double-stack intermodal trains. European Journal
of Operational Research 267(1), 107–119 (2018)

17. Ortiz-Astorquiza, C., Cordeau, J.F., Frejinger, E.: The Locomotive Assignment
Problem with Distributed Power at the Canadian National Railway Company.
Transportation Science 55(2), 510–531 (2021)

10

C. Robinson et al.

18. Paszke, A., Gross, S., Massa, F., Lerer, A., Bradbury, J., Chanan, G., Killeen,
T., Lin, Z., Gimelshein, N., Antiga, L., et al.: Pytorch: An imperative style, high-
performance deep learning library. Advances in neural information processing sys-
tems 32, 8026–8037 (2019)

19. Peng, Y., Choi, B., Xu, J.: Graph embedding for combinatorial optimization: A

survey. arXiv preprint arXiv:2008.12646 (2020)

20. Shannon, C.E.: A mathematical theory of communication. The Bell system tech-

nical journal 27(3), 379–423 (1948)

21. Vesselinova, N., Steinert, R., Perez-Ramirez, D.F., Boman, M.: Learning combi-
natorial optimization on graphs: A survey with applications to networking. IEEE
Access 8, 120388–120416 (2020)

22. Vinyals, O., Fortunato, M., Jaitly, N.: Pointer networks. In: Advances in Neural

Information Processing Systems. vol. 28 (2015)

23. Wang, Y.E., Wei, G.Y., Brooks, D.: Benchmarking TPU, GPU, and CPU platforms

for deep learning. arXiv preprint arXiv:1907.10701 (2019)

24. Xu, K., Hu, W., Leskovec, J., Jegelka, S.: How powerful are graph neural networks?

In: International Conference on Learning Representations (2019)

