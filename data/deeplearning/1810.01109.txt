8
1
0
2

t
c
O
5
1

]
I

A
.
s
c
[

2
v
9
0
1
1
0
.
0
1
8
1
:
v
i
X
r
a

AI Benchmark: Running Deep Neural Networks
on Android Smartphones

Andrey Ignatov
ETH Zurich

Radu Timofte
ETH Zurich

William Chou
Qualcomm, Inc.

Ke Wang
Huawei, Inc.

andrey@vision.ee.ethz.ch

timofter@vision.ee.ethz.ch

wchou@qti.qualcomm.com

michael.wangke@huawei.com

Max Wu
MediaTek, Inc.

Tim Hartley
Arm, Inc.

Luc Van Gool ∗
ETH Zurich

max.wu@mediatek.com

tim.hartley@arm.com

vangool@vision.ee.ethz.ch

Abstract

Over the last years, the computational power of mobile de-
vices such as smartphones and tablets has grown dramatically,
reaching the level of desktop computers available not long
ago. While standard smartphone apps are no longer a prob-
lem for them, there is still a group of tasks that can easily
challenge even high-end devices, namely running artiﬁcial in-
telligence algorithms.
In this paper, we present a study of
the current state of deep learning in the Android ecosystem
and describe available frameworks, programming models and
the limitations of running AI on smartphones. We give an
overview of the hardware acceleration resources available on
four main mobile chipset platforms: Qualcomm, HiSilicon,
MediaTek and Samsung. Additionally, we present the real-
world performance results of different mobile SoCs collected
with AI Benchmark1 that are covering all main existing hard-
ware conﬁgurations.

1

Introduction

With the recent advances in mobile system-on-chip (SoC)
technologies, the performance of portable Android devices
has increased by a multiple over the past years. With their
multi-core processors, dedicated GPUs, and gigabytes of
RAM, the capabilities of current smartphones have already
gone far beyond running the standard built-in phone applica-
tions or simple mobile games. Whereas their computational
power already signiﬁcantly exceeds the needs of most every-
day use cases, artiﬁcial intelligence algorithms still remain
challenging even for high-end smartphones and tablets. De-
spite the fact that many machine learning solutions are highly
useful when deployed on end-user devices, running them on

∗We also thank Przemyslaw Szczepaniak (pszczepaniak@google.com),

Google Inc., for writing and editing sections 2.7, 3.1 and 3.2.

1http://ai-benchmark.com

mobile platforms is associated with a huge computational
overhead on phone CPUs and a serious drain on battery power.
Many recent developments in deep learning are, however,
tightly connected to tasks meant for mobile devices. One no-
table group of such tasks is concerned with computer vision
problems like image classiﬁcation [1, 2, 3], image enhance-
ment [4, 5, 6] and super-resolution [7, 8, 9], optical character
recognition [10], object tracking [11, 12], visual scene under-
standing [13, 14], face detection and recognition [15, 16], gaze
tracking [17], etc. Another group of tasks encompasses vari-
ous natural language processing problems such as natural lan-
guage translation [18, 19], sentence completion [20, 21], sen-
tence sentiment analysis [22, 23] or interactive chatbots [24].
A separte group deals with on-line sensor data processing for
human activity recognition from accelerometer data [25, 26],
gesture recognition [27] or sleep monitoring [28]. Several
other deep learning problems on smartphones are related to
speech recognition, virtual reality and many other tasks.

Despite the rising interest in deep learning for mobile ap-
plications, the majority of AI algorithms are either not avail-
able on smartphones or are executed on remote servers due
to the aforementioned phones’ hardware limitations. The lat-
ter option is also not ﬂawless, causing: a) privacy issues; b)
dependency on an internet connection; c) delays associated
with network latency; d) bottleneck problems — the number
of possible clients depends on the servers’ computational ca-
pabilities. To overcome these issues, there were a number of
attempts to port separate algorithms or whole machine learn-
ing libraries to mobile platforms with added hardware accel-
eration (HA) using GPUs or DSPs. In [29], the authors imple-
mented a mobile neural network classiﬁcation engine capable
of sensor inference tasks on Qualcomm’s Hexagon DSP [30].
Though they achieved very impressive energy consumption
results, the DSP was able to run only very simple CNN models
due to its small program and memory space. In [31], the au-
thors presented a GPU-accelerated library CNNdroid for par-
allel execution of pre-trained CNNs on mobile GPUs. The

1

 
 
 
 
 
 
Figure 1: Mobile SoCs with potential acceleration support for third-party AI applications.

library was based on the RenderScript framework [32] that
parallelizes computations across CPUs and GPUs, and though
the proposed solution was up to 40 times faster compared to
the baseline naive singe-thread implementation, in reality its
speed was comparable to a CPU-based TensorFlow Mobile
library [33] relying on the Arm NEON [34] instruction set.
Motamedi et al. [35] exploited the same approach of using
RenderScript, but used a CPU’s imprecise computing modes
to lower execution times. Despite the promising results, the
effect inexact arithmetic had on accuracy was not investigated
in depth in this paper, and therefore the applicability of this
approach remains unclear. RSTensorFlow [36] is another at-
tempt to expoit RenderScript for GPU-based acceleration of
matrix operations, and in this case it was used to directly mod-
ify the TensorFlow Mobile library. The results demonstrated
that, while matrix multiplications can be executed up to 3
times faster, it is not possible to speed up the convolutional
operations that take approximately 75% of the total inference
time. Additionally, the experiment revealed that RenderScript
is not always using GPUs on all the devices — sometimes it
is running on a CPU only, leading to slower execution times
even compared to the original TF implementation.

Besides that, some SDKs for running computationally in-
tensive operations were proposed directly by SoC manufac-
turers. In 2016, Qualcomm introduced the Snapdragon Neu-
ral Processing Engine (SNPE) [37] to accelerate the execution
of neural networks with their GPUs and DSPs. The next year
HiSilicon proposed the HiAI platform [38] for running neural
networks on Kirin’s NPU, and later MediaTek presented the
NeuroPilot SDK [39] that can trigger GPUs or APUs to run
deep learning models. The biggest issue is that all these SDKs
were developed for the corresponding chipsets only, i.e., the
application relying on HiAI will not run on Qualcomm SoC,
and vice versa, thus forcing developers to create several ver-
sions of their app for each platform, or to give up on some
of them. This situation changed with the introduction of the
Android Neural Networks API (NNAPI) [40], designed to run
deep learning models on mobile devices. This API is basi-
cally an intermediate layer between the higher-level machine
learning framework and the device’s hardware acceleration re-
sources, and is responsible for their communication and for
scheduling the execution of tasks on the most suitable hard-
ware. NNAPI still requires speciﬁc SoC vendors’ drivers in
order to run the computations on anything but a CPU, and
therefore its default presence in Android 8.1+ does not au-
tomatically guarantee hardware acceleration support.

While there exists a number of common benchmarks test-

ing the CPU and GPU performance of mobile phones, none
of them measure the speed and acceleration of AI operations
that can be achieved due to available AI chips and DSPs. In
this paper, we present an AI Benchmark designed speciﬁcally
to test the machine learning performance, available hardware
AI accelerators, chipset drivers, and memory limitations of the
current Android devices. It consists of a number of computer
vision AI tests that are executed directly on the phones’ hard-
ware and that cover relevant deep learning architectures and
operations. We provide a detailed description of the actual
chipset platforms and popular mobile machine learning frame-
works, and describe the limitations of running deep learning
algorithms on smartphones. Finally, we present the in-the-
wild performance of about 200 Android devices and major
mobile chipsets, as collected with our AI Benchmark, for over
10,000 smartphones and tablets.

The rest of the paper is arranged as follows. In Section 2
we describe the hardware acceleration resources available on
the main chipset platforms, as well as the programming in-
terfaces for accessing them. Section 3 gives an overview of
popular mobile deep learning frameworks. Section 4 provides
a detailed description of the benchmark architecture, its pro-
gramming implementation, and the computer vision tests that
it includes. Section 5 shows the experimental results and in-
ference times for different deep learning architectures, for var-
ious Android devices and chipsets. Section 6 analyzes the ob-
tained results. Finally, Section 7 concludes the paper.

2 Hardware Acceleration

While the ﬁrst consumer computers were mostly equipped
with a single, stand-alone CPU, it soon became clear that
its computational performance is too limited for a number of
multimedia applications. This led to the creation of special
co-processors working in parallel with the main CPU. Their
architecture was optimized for many signal processing tasks.
The era of digital signal processors (DSPs) began in the early
1980s with the introduction of the NEC PD7720 [41], the
AT&T DSP1 [42] and the TI TMS32010 [43] co-processors.
They established general principles of the DSP architecture
used until now [44]: Harvard architecture, hardware block
for multiply-accumulate (MAC) operations, VLIW and SIMD
instruction sets for parallel computations, etc. Though the
ﬁrst DSPs had quite restricted capabilities due to their limited
set of instructions and memory constraints, they were widely
used till the mid 90s of the last century. They were popu-

2

lar for applications related to computer graphics, sound and
video decoding, as mathematical co-processors and accelera-
tors for various photo editing software, and even for running
the ﬁrst deep learning OCR models designed in 1989 [45].
The latter task of classifying handwritten digits using CNNs
reached high speeds at that time (12 images per second) due
to the efﬁcient vector and matrix-based calculations. These
resulted from the highly parallelizable DSP architectures and
the hardware implementation of MAC operations. At the end
of the 90s the popularity of DSPs started to decrease and in
the consumer PC sector they were largely replaced by general-
purpose CPUs with integrated DSP instructions, GPUs for ef-
ﬁcient parallel computations, and FPGAs conﬁgurable for var-
ious speciﬁc problems.

At the beginning of the 1990s, DSPs started to appear in
mobile phones. At ﬁrst, they were used only for voice cod-
ing and compression, as well as for some radio signal pro-
cessing. Later on, with the integration of cameras and many
multimedia features like music and video playback in mobile
devices, the integrated DSPs started to be extensively used for
image, video and sound processing. In contrast to what hap-
pened with desktop computers, DSPs were not displaced here
by CPUs and GPUs because they often offered superior per-
formance at lower power consumption, so critical for portable
devices. In recent years, the computational power of mobile
DSPs and other SoC components has grown drastically, and
now, complemented by GPUs, NPUs and dedicated AI cores,
they enable AI and deep learning-based computations. A de-
tailed description of the current mobile platforms (ﬁg. 1) and
their hardware acceleration resources is provided below.

2.1 Qualcomm chipsets / SNPE SDK

Qualcomm is an American semiconductor and wireless
telecommunications company, founded in 1985. Its ﬁrst Snap-
dragon mobile SoC QSD8250 was released in 2007 and al-
ready featured a dedicated AMD Z430 GPU and the ﬁrst com-
mercial generation of QDSP6 Hexagon DSPs. In 2009, after
the acquisition of AMD’s mobile graphics division, the corre-
sponding GPU series was renamed to Adreno (anagram from
Radeon), and its successors are present under this name in all
current Snapdragon SoCs. Their performance evolved from
2.1 (Adreno 200) to 727 (Adreno 630) GFLOPS. The DSP
architecture has also undergone signiﬁcant changes from the
ﬁrst (2006) to the current sixth generation, and is now support-
ing wide vector extensions (HVX), dynamic multi-threading,
VLIW and SIMD instruction sets. They can also be pro-
grammed by users [30]. The main Snapdragon CPU cores
have an Arm-based architecture and usually feature Qual-
comm’s own customized in-house design, often developed
based on Arm Cortex cores. These three components (CPUs
with the Arm NEON instruction set, GPUs and DSPs) form
Snapdragon’s heterogeneous computing architecture (ﬁg. 2)
well suitable for running various AI algorithms. The Qual-
comm chipsets are now covering around 55% of the smart-
phone SoC market and are installed in many popular smart-
phones, tablets, and wearables.

Figure 2: SoC components integrated into Snapdragon 845 (left)
and Kirin 970 (right) chipsets.

Qualcomm ﬁrst addressed the problem of on-device AI in-
ference hardware acceleration in the Snapdragon 820 in May
2015 and also announced its proprietary Snapdragon Neu-
ral Processing Engine (SNPE) SDK in May 2016, which
offers runtime acceleration across all Snapdragon’s process-
ing components. The SDK supports common deep learning
model frameworks, such as Caffe/Caffe2, TensorFlow, Py-
Torch, Chainer, MxNet, CNTK and PaddlePaddle via ONNX.
It is designed to enable developers to run their own custom
neural network models on various Qualcomm-powered de-
vices. The SDK is supported on 17 Snapdragon mobile pro-
cessors starting from premium (Snapdragon 845, 835, 820),
high tier (Snapdragon 710, 670, 660, 652, 650, 653, 636, 632,
630, 626 and 625) as well as the mid-tier (Snapdragon 450,
439, 429). It also supports the Qualcomm Vision Intelligence
Platform (QCS603 and QCS605), designed for efﬁcient ma-
chine learning on IoT devices.

Qualcomm’s ﬁrst NNAPI driver for running quantized neu-
ral networks on Hexagon DSPs was introduced in the Android
O-MR1, though it was not used in any commercial devices
at that time and ﬁrst appeared only later in the OnePlus 6
and Xiaomi Mi8 with the next Android version. In Android
P, these drivers got additional support for running ﬂoat mod-
els on the Adreno GPU. Yet, they are currently not present
in the market. The considered NNAPI drivers are generally
adopting hardware acceleration principles and implementation
used in SNPE SDK. The differences mainly come from the re-
strictions of the current Android NNAPI speciﬁcations. Qual-
comm delivers these drivers in the software images provided
to its OEM customers, which then in turn determine when and
how to include them to end devices: with their initial release
or later over the air in subsequent software updates. As a re-
sult, their presence and actual version might vary signiﬁcantly
across the phones on the market.

2.2 HiSilicon chipsets / Huawei HiAI SDK

HiSilicon is a Chinese semiconductor company founded in
2004 as a subsidiary of Huawei.
Its ﬁrst mobile processor
(K3V1) was introduced in 2008, but the ﬁrst commercially
successful product used in a number of Android devices was
the next SoC generation (K3V2) released in 2012 and featur-
ing four Arm Cortex-A9 CPU cores and a Vivante GPU. In
2014, a new Kirin SoC family consisting of mid-range (600
Series) and high-end (900 Series) chipsets was launched as a

3

Figure 3: Schematic representation of SNPE, HiAI and NeuroPilot SDKs from Qualcomm, Huawei and MediaTek, respectively.

successor to the K3 series and is used in Huawei devices until
now. Unlike Qualcomm, HiSilicon does not create customized
CPU and GPU designs and all Kirin chipsets are based on
off-the-shelf Arm Cortex CPU cores and various versions of
Mali GPUs. A different approach was also developed for ac-
celerating AI computations: instead of relying on GPUs and
DSPs, HiSilicon introduced a specialized neural processing
unit (NPU) aimed at fast vector and matrix-based computa-
tions widely used in AI and deep learning algorithms. Accord-
ing to Huawei, it delivers up to 25 times better performance
and 50 times greater efﬁciency compared to the standard quad-
core Cortex-A73 CPU cluster. The NPU design was licensed
from the Cambricon Technologies company (Cambricon-1A
chip) and is said to deliver a peak performance of about 1.92
TFLOPs, though this number mainly refers to quantized 8-bit
computations. This NPU ﬁrst appeared in the Kirin 970 SoC,
and later two enhanced NPUs were also integrated into the
subsequent Kirin 980 chipset. It should be noted that other
SoCs apart from Kirin 970/980 do not contain this NPU mod-
ule and are currently unable to provide acceleration for third-
party AI-based applications. The aforementioned chipsets can
be found only inside Huawei devices as they are not sold to
external OEM companies; the current total market share of
HiSilicon SoCs is around 10%.

To give external access to Kirin’s NPU, Huawei released
in late 2017 the HiAI [38] Mobile Computing Platform SDK,
providing APIs for executing deep learning models on hard-
ware resources integrated within Kirin SoC. This SDK is now
supporting only Caffe, Tensorﬂow Mobile and Lite frame-
works, though in future releases it might also offer support for
Caffe2 and ONNX. It provides acceleration for 16-bit ﬂoat, 8-
bit and 1-bit quantized models, and can additionally speed-up
sparse models by skipping multiply-add operations containing
zero variables. Apart from low-level APIs, the HiAI Engine
also provides a ready-to-use implementation of several com-
puter vision algorithms including image categorization, face
and facial attribute detection, document detection and correc-
tion, image super-resolution, QR code detection, etc.

Starting from Android 8.1 (EMUI 8.1), Huawei is including
NNAPI drivers for its Kirin 970/980 chipsets that are generally
based on the HiAI implementation. Currently, they are provid-
ing support only for 16-bit ﬂoat models, quantized networks
will be supported in the future releases.
It should be men-
tioned that all Huawei devices that are based on other chipsets
do not contain NNAPI drivers as they are lacking the above-
mentioned NPU module.

2.3 MediaTek chipsets / NeuroPilot SDK

MediaTek is a Taiwanese semiconductor company spun off
Its
from the United Microelectronics Corporation in 1997.
mobile division was launched in 2004 and soon after this Me-
diaTek released its ﬁrst mobile chipsets that were used in many
entry-level Chinese phones and smartphones produced at that
time. It gained popularity on the global smartphone market in
2013 with the introduction of the MediaTek 657x/658x family
of dual and quad-core SoCs with Mali or PowerVR graph-
ics, and later with the release of 64-bit MediaTek MT67xx
chipsets they became widely used in many Android devices
from various OEMs, getting a market share of about 20%.
Similarly to Huawei, MediaTek is integrating into its SoCs
standard Arm Cortex CPU cores and Mali or PowerVR GPUs.
At the beginning of 2018, MediaTek addressed the problem of
accelerating machine learning-based applications by launch-
ing their Helio P60 platform with embedded AI processing
unit (APU). This APU can deliver the performance of up to
280GMAC/s for 8-bit computations and is primarily used for
accelerating quantized neural networks, while ﬂoat models are
running on four Cortex-A53 CPU cores and Mali-G72 MP3
GPU clocked at 800MHz. Thus, MediaTek’s approach lies in
between the solutions from Huawei and Qualcomm: a dedi-
cated chip for quantized computations (as in Kirin’s SoC) and
CPU/GPU for ﬂoat ones (as in Snapdragon chipsets).

The release of the Helio P60 was accompanied by the in-
troduction of MediaTek’s NeuroPilot SDK [39] constructed
around TensorFlow Lite and Android NNAPI. This SDK con-
sists of four main components: 1) TOCO-based tools for
quantizing ﬂoat TF Lite networks and for converting pre-
trained TensorFlow/Caffe/ONNX models (with supported op-
erations) to TensorFlow Lite format. 2) An extended list
of implemented TF Lite operations and the corresponding
interpreter for loading and running converted .tﬂite models.
3) APU and GPU NNAPI drivers implementing hardware
accelerated operations for MediaTek’s NeuroPilot platform;
the APU drivers currently only support INT8 ops and GPU
drivers — FP16/32 ops. 4) Facilities for proﬁling and debug-
ging neural network-based applications, and an interface for
pinning target operations on a speciﬁc hardware accelerator
like GPU or APU. The SDK is supporting purely MediaTek
NeuroPilot-compatible chipsets (currently Helio P60 only).

There also exists a corresponding stand-alone version
of NNAPI drivers supporting ﬂoat and quantized models.
Nonetheless, except for the P60 developer platform, only one

4

commercial device with MediaTek P60 chipset (Vivo V11) is
known to contain these drivers.

2.4 Samsung chipsets

Samsung Electronics is a South Korean electronics company
founded in 1969.
In 1988, it merged with Samsung Semi-
conductor & Communications and obtained its current name.
That same year it launched its ﬁrst mobile phone, while its ﬁrst
mobile processor (S3C44B0, 66 MHz, Armv4) was presented
only in 2000. Later it signiﬁcantly extended its S3Cxxxx
and S5Pxxxx SoC series that were widely used in many Win-
dows Mobile devices, in the iPhone 2G/3/3GS, and in some
early Android smartphones. With the introduction of the
S5PC110 chipset in 2010, all Samsung SoCs were rebranded
into Exynos and are using this name up to now (Exynos 3-9th
generations). Similarly to Huawei and MediaTek, Samsung is
primarily using Arm Cortex CPU cores and Mali or PowerVR
graphics in its chipsets, though starting from Exynos 8 it is
also integrating its in-house developed Mongoose Arm-based
CPU cores into high-end SoCs. As for speciﬁc AI chips, Sam-
sung introduced in the Exynos 8895 a Vision Processing Unit
(VPU) mainly used by its phones’ cameras. Yet, no drivers,
SDKs or additional details were released, making it inacces-
sible by third-party applications. Only two Samsung devices
(Note 9 and Tab S4) are currently running Android 8.1+ and
are using Google’s default NNAPI drivers utilizing the CPU
only. According to some rumors, the next Exynos chipset
might include a dedicated AI chip, though this information
was not ofﬁcially conﬁrmed by Samsung. The current market
share of Samsung chipsets is around 10%.

2.5 Google Pixel / Pixel Visual Core

Apart from its Android operating system, Google started,
since Android 2.1, to annually release smartphones and tablets
under the Google Nexus brand. These were developed in
collaboration with external OEMs, among which at different
times were HTC, Samsung, LG, Motorola, Huawei and Asus.
These devices were featuring the stock Android operating sys-
tem running on the latest high-end hardware and were the ﬁrst
to receive Android updates (with the possibility of installing
beta versions). In 2016 the Nexus product line was discontin-
ued and all new smartphones started being produced under the
Google Pixel brand, though the aforementioned principles re-
mained the same. The majority of these devices were based on
Qualcomm chipsets, therefore all information from the above
Qualcomm section can be applied to them too. Yet, starting
from Pixel 2 (XL), Google has added to its smartphones a ded-
icated fully-programmable Pixel Visual Core AI chip (ﬁg. 4),
separate from the main Qualcomm SoC and developed in col-
laboration with Intel. The chip contains one Arm Cortex-A53
core for handling communications with the main application
processor, integrated LPDDR4 RAM and eight custom image
processing unit (IPU) cores. Each IPU contains 512 arithmetic
logic units with 256 processing elements arranged as a 16×16

Figure 4: The architecture of the Pixel Visual Core AI Chip.

two-dimensional array and supports a custom VLIW instruc-
tion set. The chip provides native support for 8-bit and 16-bit
integer computations and delivers a performance of up to 3.2
TFLOPS. Although the Pixel Visual Core is generally compli-
ant with TensorFlow (Lite), Google did not release the corre-
sponding SDK and NNAPI drivers, thus it cannot be used by
external developers for accelerating machine learning-based
applications and its present use is mainly limited to Google’s
HDR+ image processing.

2.6 Arm Cortex CPUs / Mali GPUs / NN SDK

Currently, all CPU cores integrated into mobile SoCs are
based on the Arm architecture, and in devices not support-
ing HA for machine learning applications these CPUs are re-
sponsible for running all AI algorithms. To speed-up the com-
putations in this case, Arm has introduced a number of spe-
ciﬁc instruction sets aimed at fast vector- and matrix-based
calculations. The most notable technology here is the Arm
NEON[34] — an advanced SIMD (single instruction multiple
data) architecture extension ﬁrst introduced in Armv7 proces-
sors. NEON basically implements DSP-like instructions for
concurrent computations and allows the simultaneous execu-
tion of up to 16x8-bit, 8x16-bit, 4x32-bit, 2x64-bit integer and
8x16-bit, 4x32-bit, 2x64-bit ﬂoating-point operations. Addi-
tionally, Arm has recently presented its new DynamIQ tech-
nology that is able to efﬁciently utilize all cores within a single
Arm CPU for parallel computations, and a speciﬁc instruction
for calculating dot products in the Armv8.4-A microarchitec-
ture. Many of these optimized instructions are integrated in
Google’s default NNAPI drivers, handling the CPU path when
no other means for acceleration are available.

Apart from that, Arm has also presented the Arm NN
SDK [46] to accelerate machine learning computations on mo-
bile SoCs. It provides both the CPU and GPU paths for ML
workloads, along with parsers for TensorFlow, Caffe, ONNX
and TFLite. On the CPU side it is compatible with any plat-
form with Armv7 and above CPUs (assuming NEON avail-

5

3 Deep Learning Mobile Frameworks

With the widespread use of the Android operating system,
a number of popular deep learning frameworks were ported
to this platform, including Torch [47], Deeplearning4j [48],
TensorFlow (Mobile [33], Lite [49]), Caffe [50], Caffe2 [51],
MXNet [52], NNabla [53], etc. Nowadays, the most com-
monly used are three of them: Tensorﬂow Mobile, Tensorﬂow
Lite and Caffe2 that are described below.

3.1 TensorFlow Mobile

Tensorﬂow [54] is an open-source machine learning library
for research and development released by Google in 2015.
TensorFlow’s programming model can be described as a di-
rected graph that deﬁnes the relation between the input and
output (target) variables. The graph itself consists of a set of
nodes representing various operators applied sequentially to
the input data (e.g., convolutional, pooling, LSTM layers, etc.)
that are deﬁning a deep learning model and the corresponding
dataﬂow computation. After the model is trained, it can be
exported as a .pb graph and executed on mobile devices using
the TensorFlow Mobile library [33], available on Android as
well as iOS platforms. A code snippet of the corresponding
Java inference interface is presented in ﬁgure 6 (a). Note that
there is no need to specify the model architecture in the actual
application code: it is already stored along with pre-trained
weights in the .pb graph, and developers only need to provide
the location of this ﬁle and the input data.

The main advantage of the TensorFlow Mobile library is
that it supports the majority of operations available in the stan-
dard TF version, therefore almost any TensorFlow model can
be converted and executed on a mobile device. Addition-
ally, all current SDKs from SoC manufacturers (SNPE [37],
HiAI [38], NeuroPilot [39] and ArmNN [46]) are providing
(partial) hardware acceleration support for this library. This
said, the development of TensorFlow Mobile is coming to a
close, as Google announced its gradual deprecation in favor
of the TensorFlow Lite library [55]. Particularly, TF Mobile
will not get Android NNAPI support, thus without using spe-
ciﬁc SDKs all models will still be executed on CPUs only.

3.2 TensorFlow Lite

TensorFlow Lite [49] was presented late 2017, as a successor
of the TF Mobile library. According to Google, it provides
better performance and a smaller binary size due to optimized
kernels, pre-fused activations and fewer dependencies. Simi-
larly to TF Mobile, a general TensorFlow pre-trained model
can be in theory converted to .tﬂite format and later used
for inference on Android or iOS platforms, the correspond-
ing Java code snippet is shown in ﬁgure 6 (b). The change of
the ﬁle format (.tﬂite instead of .pb) is caused by the use of
a new FlatBuffers serialization library that allows to access
saved models without a parsing/unpacking step, often cou-
pled with per-object memory allocation. Finally, the new li-
brary is compatible with Android NNAPI and can by default

Figure 5: System architecture for Android Neural Networks API.

ability), with key low level optimizations for speciﬁc architec-
tures. The GPU path will be available on platforms with Arm
Mali GPUs, either from the Midgard family (Mali-T6xx and
onwards when GPGPU was introduced) or the later Bifrost
family (G71 / G51 and onwards), and requires the Mali GPU
and OpenCL drivers to be installed. The Arm NN SDK pro-
vides support for both FP32 and quantized INT8 networks and
can run on Linux or Android platforms in parallel to NNAPI.

2.7 Android NNAPI

While there exist a number of proprietary SDKs for accessing
DSPs, GPUs or NPUs on different mobile platforms, this was
not really solving the problem of using HA for running deep
learning algorithms on mobiles, as all these SDKs are pro-
viding access only to some particular chipsets and are addi-
tionally incompatible with each other. To solve this problem,
Google has recently introduced a uniﬁed Android Neural Net-
works API (NNAPI) that is an Android C API designed for
running computationally intensive machine and deep learn-
ing operations on mobile devices. The system architecture
of NNAPI is presented in the ﬁgure 5. Apps typically would
not use NNAPI directly, instead they will rely on higher-level
machine learning frameworks that in turn could use NNAPI
to run hardware-accelerated inference on supported devices.
To perform computations using NNAPI, the executed model
should be ﬁrst represented as a directed graph that deﬁnes the
computations to perform. This graph, combined with the data
deﬁning the model (e.g., the weights and biases passed down
from a machine learning framework), forms the model for
NNAPI runtime evaluation. Based on the app’s requirements
and device hardware, Android’s neural networks runtime can
efﬁciently distribute the computation workload across avail-
able on-device processors, including dedicated neural network
chips, GPUs and DSPs. NNAPI is available on all devices run-
ning Android 8.1 (API level 27) or higher, but it still requires a
specialized vendor driver for accessing the device’s hardware.
For devices that lack this driver, the NNAPI runtime relies on
optimized code to execute requests on the CPU.

6

(a) TensorFlow Mobile

(b) TensorFlow Lite

Figure 6: Code snippets of TensorFlow Mobile and Lite Android Java interfaces.

run with hardware acceleration on devices with appropriate
chipsets and drivers.

It should be noted, however, that TensorFlow Lite is in
developer preview at the moment and has a number of sub-
stantial limitations. First of all, it supports only a limited
set of operators, lacking the full support of, e.g., image re-
sizing, batch and instance normalization, LSTM units, some
statistical functions or even simple mathematical operations
like exponentiation or argmax. Ofﬁcially, Google guarantees
only three models to work: the Inception-V3, MobileNet and
Smart Reply SSL algorithm, though with some modiﬁcations
it is possible to run a number of other deep learning models.
A second issue concerns the inference time and the amount
of consumed RAM. Since the ByteBuffer format is not sup-
ported for the network’s output, these two values can be up to
2× higher compared to TF Mobile for image-to-image trans-
lation problems. Finally, stability is another concern — the
current ofﬁcial version might not work ﬂawlessly with a num-
ber of models and mobile devices, though some of the issues
are already solved in the nightly TF Lite version. While many
of these problems will probably be overcome in the upcoming
library releases, currently they make the use of TensorFlow
Lite complicated for many existing deep learning problems.

3.3 Caffe2

Caffe [56] is another open-source deep learning framework,
originally developed at UC Berkeley by Yangqing Jia and re-
leased in 2013. Its ﬁrst unofﬁcial Android port appeared the
next year [50], and in 2017, with Facebook’s release of the
successor, Caffe2, its mobile version for iOS and Android
platforms was also presented [51]. Caffe2 is using a pro-
gramming model similar to TensorFlow’s, with static compu-
tational graphs and nodes representing various operators. Ac-
cording to the Caffe2 github repository [57], the speed of its
mobile library is generally comparable to that of TensorFlow
Lite [58] (175ms vs. 158ms for the SqueezeNet model on
Snapdragon 821 SoC). Report [59] additionally claims about
up to a 6x speed-up when using the OpenGL backend for
GPU-based computations, but this feature is not yet available
in the current Caffe2 release. Similarly to TensorFlow, accel-
eration for Caffe2 models is also supported by all proprietary
SDKs (SNPE, HiAI, NeuroPilot and ArmNN), but NNAPI
support is still in development and is not fully integrated yet.

4 AI Benchmark

The AI Benchmark is an Android application designed to
check the performance and the memory limitations associated
with running AI and deep learning algorithms on mobile plat-
forms. It consists of several computer vision tasks performed
by neural networks that are running directly on Android de-
vices. The considered networks represent the most popular
and commonly used architectures that can be currently de-
ployed on smartphones, their detailed description along with
technical details of the application are provided below.

4.1 Deep Learning Tests

The actual benchmark version [2.0.0] consists of the following
nine deep learning tests.

Test 1: Image Recognition. This task represents a con-
ventional ImageNet challenge where the goal is to classify
In the ﬁrst test, classiﬁcation
images into 1000 categories.
is done with a resource-efﬁcient MobileNet-V1 [3] architec-
ture designed speciﬁcally for mobile and embedded vision
applications. The network mainly consists of 1×1 convolu-
tional (75%) and fully connected (24%) layers, where 95%
of the total 569M multiply-add operations happens in the ﬁrst
ones. MobileNet achieves 70.6% accuracy on the ImageNet
dataset, thus outperforming the larger AlexNet, SqueezeNet
and Inception-V1 models. It can be optimized further for mo-
bile usage by quantization [60, 61] — converting its weights
and activations from FLOAT32 to INT8 8-bit ﬁxed point rep-
resentation. Though this leads to an accuracy drop to 69.7%,
the speed is simultaneously more than doubled and the size
is reduced (by a factor of 4) to 4.3MB. The latter quantized
MobileNet-V1 is deployed in the ﬁrst test.

Test 2: Image Recognition. The same ImageNet classiﬁca-
tion problem as above, but in the second test a considerably
larger and more accurate Inception-V3 [2] CNN, presented by
Google in 2015, is used. This network is comprised of 11 in-
ception blocks that mainly consist of 1×1, 1×3 + 3×1, 1×7 +
7×1 and 3×3 convolutional layers. In contrast to MobileNet,
Inception-V3 requires about 5,000M multiply-add operations,
and the size of the saved CNN is around 96MB. The accu-
racy is signiﬁcantly higher too however, — 78% on the same
ImageNet dataset and currently the best result among popular
networks of size below 100MB.

7

Figure 7: Sample result visualizations displayed to the user in the considered deep learning tests.

Test 3: Face Recognition. The goal of this task is to retrieve
the most similar face to a given one from an existing facial
database. To do this, a neural network is ﬁrst trained to pro-
duce a small feature vector for each facial image that encodes
its visual features and is invariant to face scaling, shifts and ro-
tations. In this test, we are using the Inception-Resnet-V1 net-
work [62], presented by Google in 2017. It was trained to min-
imize a triplet loss [16] on the VGGFace2 dataset [63]. After
the network is trained, it is applied to a new facial image and
produces its feature vector that is then used to retrieve the clos-
est vector (and the respective identity) from the database. The
size of the input images in this task is 512×512 pixels, and the
dimensionality of feature vectors is 128. The architecture of
the Inception-ResNet-V1 consists of 20 inception blocks and
is conceptually similar to the previously discussed Inception-
V3 CNN; their size, accuracy on the ImageNet dataset, and
computational cost are very similar as well. The biggest bene-
ﬁt of this network is its training speed — it needs fewer epochs
to achieve the same accuracy than Inception-V3.

We would like to note that the models used in the ﬁrst three
tests currently represent a core set of architectures for clas-
siﬁcation problems that are suitable for mobile deployment.
Networks faster than MobileNet (or its variants) are showing
substantially worse accuracy. Models with better precision
than Inception-V3 or Inception-ResNet-V1 have sizes exceed-
ing 100-150MB [64], which makes their application on mo-
bile devices quite complicated due to the resulting size of the
APK ﬁle. Quantization of these networks can partially solve
the problem, but currently their quantized versions are not yet
publicly available.

Test 4: Image Deblurring. This test is aimed at removing
Gaussian blur from images, which is done using the SRCNN
network [7] — one of the ﬁrst CNNs proposed for the super-
resolution problem that is now widely used as a baseline for
many image-to-image translation tasks. The architecture of
this network is very shallow: three layers with 9×9 and 5×5
ﬁlters, in total 69,162 parameters and around 64B multiply-
add operations for HD-resolution image. As a result, the size
of the saved pre-trained network is only 278KB.

Test 5: Image Super-Resolution. The goal of the super-
resolution task is to reconstruct the original image from its
In this test we consider a downscal-
downscaled version.

ing factor of 3, and image restoration is performed by the
VDSR [65] network, presented in 2015 shortly after SRCNN.
This network features a VGG-based architecture that is com-
posed of 19 convolutional layers with 3×3 ﬁlters, enough
to obtain top quantitative results on many image processing
problems. The VDSR network has 665K parameters and re-
quires around 600B multiply-add operations for HD images;
the size of the network is 2.7MB.

Test 6: Image Super-Resolution. This test solves the same
super-resolution problem, but with a downscaling factor of 4
and using the SRGAN [8] model that consists of two neural
networks. The ﬁrst one is ResNet previously proposed in [66]
that in this implementation consists of 16 residual blocks; this
network performs image restoration. The second one is an
adversarial CNN — it is trained to distinguish between the
real high-resolution images and the images reconstructed by
ResNet. During the training, these networks are playing the
following game: the adversarial CNN is trying to maximize
its classiﬁcation accuracy, while ResNet has the opposite goal
of minimizing it, i.e., to provide reconstructed images that are
indistinguishable from the target ones. In practice, this leads
to much better perceptual results than when using the standard
Euclidean norm or content-based losses. After the model is
trained, the adversarial CNN is removed and inference is per-
formed by ResNet only. The latter network contains 1.5M pa-
rameters and the size of the saved pre-trained model is 6.2MB.

Test 7: Image Semantic Segmentation. In contrast to image
classiﬁcation, the goal of this task is to get a pixel-level image
understanding, meaning that each pixel has to be classiﬁed as
belonging to one of 19 categories: car, pedestrian, road, sky,
vegetation, etc. This is done with an ICNet CNN [67], de-
signed for fast and accurate segmentation on low-performance
devices. The speedup was mainly achieved by downsampling
and shrinking feature maps, though the resulting accuracy on
the Cityscapes dataset remained high — 70.6% mIoU. ICNet
consists of 6.7M parameters and the size of the pre-trained
model is 27MB.

Test 8: Image Enhancement. We consider here a general
image and photo enhancement problem that encompasses var-
ious kinds of improvements including color enhancement, de-
noising, sharpening, texture synthesis, etc. In this formulation
the problem was ﬁrst addressed in the DPED paper [4], where

8

the authors were trying to turn low-quality smartphone pho-
tos into photos as they would be taken with a DSLR camera.
This work adopted a ResNet-like architecture with 4 residual
blocks and proposed speciﬁc losses targeted at various aspects
of image quality. The obtained results demonstrated superior
visual quality compared to the results of manual retouching or
standard automatic algorithms. The main limitation of the ap-
proach was a need of device-speciﬁc training. The network is
parameterized by 400K parameters and has a size of 1.6MB.

Test 9: Memory Limitations. While previous tests were
mainly evaluating the runtime of various deep learning mod-
els, the goal of the last test is to check RAM resources that can
be allocated for running neural networks. In this test we are
using the same SRCNN model as in the fourth task (deblur-
ring), while gradually increasing the size of the input image
until we run into a memory exception, meaning that the de-
vice does not have enough RAM to process larger inputs. The
SRCNN model was chosen here since it consumes an amount
of RAM similar to other models (for images of the same res-
olution), while its runtime is much faster and thus the test re-
quires less time to ﬁnish. It is useful to note that the memory
consumed by a network is primarily determined by the dimen-
sions of its largest (convolutional) layer, which in the case of
SRCNN is the ﬁrst layer with 64 convolutional ﬁlters.

These nine tests represent the current deep learning core of
the benchmark (ﬁg. 7); its technical components and imple-
mentation details are discussed below.

4.2 Technical Description

The current release of the AI Benchmark (2.0.0) is using the
TensorFlow Lite [49] library as a backend for running all em-
bedded deep learning models. Though the previous release
was originally developed based on TF Mobile [33], its lack
of NNAPI support imposed critical constraints on using hard-
ware acceleration resources, and thus was later deprecated.
The actual benchmark version was compiled with the latest
TF Lite nightly build where some issues present in the stable
TensorFlow versions were already solved.

The benchmark consists of nine deep learning tests de-
scribed in the previous section. These can be generally di-
vided into two groups. The ﬁrst group includes tests 1, 2, 4,
5, 8, 9. Those use CNN models fully supported by NNAPI
(i.e., all underlying TensorFlow operations are implemented
in NNAPI introduced in Android 8.1), and therefore they can
run with hardware acceleration on devices with appropriate
chipsets and drivers. NNAPI is always enabled in these tests
to avoid the situation when the system fails to automatically
detect the presence of AI accelerators and performs all com-
putations on CPU. It should also be mentioned that the ﬁrst
test runs a quantized CNN model and is used to check the per-
formance of accelerated INT8-based computations.

The second group contains the other three tests, i.e. 3, 6
and 7, where neural networks are always running entirely on
CPU. They contain at least one TF operation that is not yet
present in NNAPI, and using partial acceleration for supported

Figure 8: Benchmark results displayed after the end of the tests.

ops only is currently not possible. These tests were added
to evaluate the speed of CPU-based execution and the per-
formance of the Arm NEON instruction set [34], present in
all current Arm processors and designed speciﬁcally for high-
performance computing and image processing. In cases where
NNAPI drivers are missing, all computations in the tests from
the ﬁrst group also fall back on CPU and are using the same
instruction set.

The resolution of input images used in the tests was chosen
so that all devices with at least 2GB of RAM and the majority
of devices with 1GB of RAM should have enough memory
to run all tests. The test is considered to be passed when the
network was able to successfully process at least one image
within the allocated time.
In particular, during the internal
testing all devices with 1GB of RAM (e.g., Samsung Galaxy
S2/S3 mini, HTC One X, FiiO X7, etc.) were able to run all
models after a fresh restart.

Each of the ﬁrst eight tests has a predeﬁned time limit: 25,
40, 40, 30, 40, 50, 20 and 25 seconds, respectively. The last
test does not have a time limit — images of increasing res-
olution are processed until the device runs out of memory.
The running time for each test is computed as an average over
the set of images processed within the speciﬁed time. When
more than two images are handled, the processing time for
the ﬁrst two ones is not considered as it might comprise ad-
ditional time expenses associated with network initialization
and memory allocation. The scores for the ﬁrst eight tests are
computed inversely proportional to the corresponding average
runtimes; the score for the memory test is proportionate to the
maximum image size that the network was able to process.
The ﬁnal AI score (ﬁg. 8) is calculated as a weighted sum of
the scores obtained in these nine tests and represents the ag-
gregated AI performance of a particular device. The weight
coefﬁcients for these tests were calibrated based on the results
obtained on Google Pixel 2 running Android P with disabled
NNAPI in all tests.

9

Test

1

2

3

4

5

6

7

8

Task
Architecure
Resolution, px
Parameters
Size, MB
Quantized
NNAPI support
Consumed RAM

Classiﬁcation
MobileNet
224×224
4.2M
4.3
yes
yes
20MB

Classiﬁcation
Inception-V3
346×346
27.1M
96
no
yes
170MB

Face Recognition
Inc-ResNet-V1
512×512
22.8M
92
no
no
240MB

Deblurring
SRCNN
300×300
69K
0.3
no
yes
290MB

Super-Resolution
VGG-19
192×192
665K
2.7
no
yes
110MB

Super-Resolution
SRGAN (ResNet-16)
512×512
1.5M
6.2
no
no
310MB

Segmentation
ICNet
384×576
6.7M
27
no
no
60MB

Enhancement
DPED (ResNet-4)
128×192
400K
1.6
no
yes
120MB

Table 1: Summarized characteristics of deep learning models used in the AI Benchmark

5 Benchmark Results

In this section, we present quantitative benchmark results ob-
tained from over 10,000 mobile devices tested in the wild.
The scores of each device/SoC are presented in tables 2 and 3
that are showing average processing time per one image for
each test/network, maximum possible image resolution that
can be processed by SRCNN model and the total aggregated
AI score. The scores were calculated by averaging all obtained
results of the corresponding devices/SoCs after removing the
outliers. The description of the results is provided below.

5.1 Neural Networks

range as it is primarily deﬁned by the dimensions of the largest
convolutional layer. Finally, the SRCNN model is much faster
than both the VGG-19 and DPED networks, and the amount of
consumed memory here is also quite similar due to the afore-
mentioned reason. The size of the highest image resolution
that can be processed by SRCNN is growing linearly with the
amount of total (free) RAM of the device, though due to a
bug in NNAPI this does not hold true for devices with An-
droid 8.1+ as they are generally consuming much more RAM.
We should also note that all previous conclusions are based on
the results from devices not supporting hardware acceleration,
since it might signiﬁcantly alter the results in tests 1, 2, 4, 5, 8
and 9 that can run with NNAPI on dedicated hardware.

Table 1 summarizes the details of all deep learning architec-
tures included in the benchmark. The results in tables 2 and 3
are quite consistent with the theoretical expectations of the
relative processing time and memory consumed by the net-
works. In particular, the quantized MobileNet CNN from the
ﬁrst test requires about 3-4 times less RAM than the same ﬂoat
model, and its speed on CPU is generally an order of mag-
nitude faster compared to Inception-V3 CNN. The third face
recognition test is dealing with images with a twice larger area
and exhibits around 2x longer inference times than the second
one, meaning that the performances of Inception-ResNet-V1
and Inception-V3 are quite comparable.
In image-to-image
processing tasks, the most efﬁcient model is ICNet since the
computations there are mainly done on the downscaled im-
ages/feature maps. The same approach is used in the SRGAN
model where the original image is downsampled to 128×128
pixels and processed in this resolution till the last two layers
that are performing its upscaling to the original size. There-
fore, despite using 12 residual blocks, the processing time here
still remains reasonable, though the required RAM is quite
high due to the downscaling/upscaling layers working with
512×512px images. The DPED network from the image en-
hancement task contains 4 residual blocks and is processing
images without downsampling, therefore the processing time
here should be roughly 128×128×12
128×192×4 = 2 times faster than in the
previous case, as seen in practice. The VGG-19 model from
the ﬁfth test is the most resource-consuming among all con-
sidered CNNs — since it consists of 19 convolutional layers,
it should be theoretically around 19
12 = 1.6 times slower than
the DPED network (the size of their convolutional layers is
similar), though the RAM consumption should lie in the same

5.2 Smartphones and mobile chipsets

The results in tables 2 and 3 show the performance of sev-
eral selected Android smartphones and chipsets obtained with
the AI Benchmark;
the actual full list is available on the
project website: http://ai-benchmark.com. Before
going into details, we would ﬁrst like to mention several An-
droid NNAPI bugs that are currently affecting some results
presented in the tables. First of all, due to a bug in Android
8.1 with default NNAPI drivers, the performance of (convo-
lutional) operations is twice as slow as when these drivers
are disabled. Therefore, when calculating the average run-
time for different SoCs presented in table 3, we omitted the
results from the phones with this issue. While Huawei phones
with Android 8.1 and the Kirin 970 chipset were using their
own customized NNAPI implementation, it still suffered from
a different bug — after a long standby the clock speed of
Kirin’s NPU drops and does not return back until the phone
is rebooted. The results in both tables represent the scores
obtained from Huawei devices that were recently restarted.
Finally, the RAM consumption on devices using Android
NNAPI might be up to 2× higher in image-to-image process-
ing tests due to the ByteBuffer issue described in Section 3.2;
its consequences can be observed in the last memory test.

Below we summarize the results for each SoC manufacturer
and describe the performance of the corresponding chipsets
present on the market.

• Qualcomm. Snapdragon chipsets can now provide hard-
ware acceleration for quantized neural networks (when Qual-
comm’s NNAPI drivers are present), while ﬂoat models are
not yet supported by existing commercial devices. The ﬁrst

10

Model

SoC

RAM

Android

Test 1,
ms

Test 2,
ms

Test 3,
ms

Test 4,
ms

Test 5,
ms

Test 6,
ms

Test 7,
ms

Test 8,
ms

Test 9,
100 px

AI-Score

Huawei P20 Pro
OnePlus 6
HTC U12+
Samsung Galaxy S9+
Samsung Galaxy S8
Motorola Z2 Force
OnePlus 3T
Lenovo ZUK Z2 Pro
Google Pixel 2
Google Pixel
Nokia 7 plus
Asus Zenfone 5
Google Pixel C
Huawei Honor 8 Pro
Sony XA2 Ultra
Meizu Pro 7 Plus
BlackBerry Keyone
Sony X Compact
Xiaomi Redmi 5
Huawei Nexus 6P
Meizu MX6
HTC U Play
Xiaomi Redmi 4X
Samsung Galaxy J7
LG Nexus 5
Asus Zenfone 2
Motorola Moto C
Samsung Galaxy S3
Fly Nimbus 15
Huawei Ascend P1

HiSilicon Kirin 970
Snapdragon 845/DSP
Snapdragon 845
Exynos 9810 Octa
Exynos 8895 Octa
Snapdragon 835
Snapdragon 821
Snapdragon 820
Snapdragon 835
Snapdragon 821
Snapdragon 660
Snapdragon 636
Nvidia Tegra X1
HiSilicon Kirin 960
Snapdragon 630
Mediatek Helio X30
Snapdragon 625
Snapdragon 650
Snapdragon 450
Snapdragon 810
Mediatek Helio X20
Mediatek Helio P10
Snapdragon 435
Exynos 7870 Octa
Snapdragon 800
Intel Atom Z3580
Mediatek MT6737
Exynos 4412 Quad
Spreadtrum SC9832
TI OMAP 4460

6GB
8GB
6GB
6GB
4GB
6GB
6GB
6GB
4GB
4GB
4GB
4GB
3GB
6GB
4GB
6GB
4GB
3GB
3GB
3GB
4GB
3GB
3GB
3GB
2GB
2GB
1GB
1GB
1GB
1GB

8.1
9.0
8.0
8.0
8.0
8.0
8.0
8.0
9.0
9.0
9.0
8.0
8.0
8.0
8.0
7.0
7.1
8.0
7.1
8.0
7.1
6.0
7.1
7.0
4.4
5.0
7.0
4.3
7.0
4.1

144
24
60
148
134
85
106
115
143
116
136
110
105
121
170
327
160
111
188
106
183
239
246
278
332
1507
414
553
538
482

130
892
620
1208
731
823
776
909
1264
867
944
1055
1064
1720
1653
3357
1695
1804
1753
1962
2217
2061
2640
2092
2182
2433
3394
4640
5103
7613

2634
1365
1433
1572
1512
1894
1937
2099
1953
1838
2132
2405
2585
3163
3424
4550
3525
3566
3707
4113
4981
4303
5428
4648
5080
6188
7761
10321
12618
25105

279
928
1229
958
1197
1513
1707
1747
1168
1287
1320
1910
2104
1943
2638
2215
2780
2469
3020
3389
3906
3563
4155
3881
5732
4337
6356
7587
7594
12667

241
1999
2792
1672
2519
3568
3624
3683
2104
2489
2519
4271
4546
4791
5497
4971
6150
5789
6144
8155
9245
7537
8575
8495
9625
12878
14760
17187
19174
30743

4390
2885
3542
2430
3039
4302
4427
4363
4219
4125
4641
4877
5036
5719
6338
5502
7164
6846
7144
9805
10551
10116
9979
9644
12375
15128
16721
21904
22758
35417

779
303
329
612
428
381
365
313
394
365
475
515
429
1082
685
1666
780
835
751
930
936
989
1229
941
1299
1176
1668
2059
2094
4015

193
1244
1485
1230
1422
1944
1982
2030
1360
1568
1509
2330
2439
2764
3166
2651
3628
3527
3580
4733
4870
4368
5030
4699
5948
6947
7856
9291
9935
18836

6
5
11
8
6
11
10
11
4
4
5
7
6
9
9
10
9
6
8
7
9
7
8
3
3
3
3
2
2
2

6519
2053
1708
1628
1413
1384
1302
1300
1293
1260
1183
1028
980
917
799
785
776
738
706
658
641
561
537
455
387
318
283
216
202
140

Table 2: Benchmark results for several Android devices, a full list is available at: http://ai-benchmark.com/ranking

smartphone to contain these drivers is the OnePlus 6 with
Snapdragon 845 SoC and the latest Android P ﬁrmware. It
can run the quantized MobileNet model under 25ms on the
Hexagon DSP which is considerably faster than the corre-
sponding CPU speed (60-65ms). A similar performance can
be expected from Snapdragon 670/710 chipsets containing the
same Hexagon 685 DSP; Snapdragon 835 with Hexagon 682
and Snapdragon 636/660/820/821 with Hexagon 680 from the
same Qualcomm 68x DSP family should come with a some-
what longer runtime.

While there exist no ofﬁcial tests of Qualcomm’s NNAPI
drivers supporting acceleration for ﬂoat models, the Snap-
dragon 625 SoC, with (presumably) a beta version of these
drivers using the integrated Adreno 506 GPU, can provide up
to 2x speed-up compared to a CPU-based execution. While
the performance of Adreno 506 is around 130 GFLOPs, this
means that Adreno 630 (727 GFLOPs) present in Snapdragon
845 SoC can potentially provide a speed-up by a factor of 3-4,
though the exact number might vary a lot.

As to CPU performance measured in relation to matrix/deep
learning computations, currently the most powerful Qual-
comm core is the Kryo 385 Gold present in the Snapdragon
845 SoC. It exhibits around a 30% improvement over the Kryo
280 cores from Snapdragon 835. Interestingly, the latter ones
demonstrate a similar or slightly degraded performance (per
GHz) compared to the ﬁrst Kryo generation in the Snapdragon
820 SoC with a custom non-Cortex based design, that despite
having only 4 cores is still slightly faster than the Snapdragon
636/660 with newer Kryo 260 cores. The previous Krait mi-
croarchitecture represented by the Snapdragon 800/801 from
2013 is still showing competitive results, outperforming the

majority of SoCs from the 2xx, 4xx and 6xx families or even
subsequently presented 810 and 808 chipsets based on the
Cortex-A57 microarchitecture. We also note that customized
Qualcomm CPU cores are generally showing a better perfor-
mance than the default Arm Cortex architectures.

• Huawei. Though the CPU performance of HiSilicon
SoCs is not as impressive as in Qualcomm’s case, its NPU
integrated into the Kirin 970 provides a dramatic speed-up for
ﬂoat deep learning models.
In particular, depending on the
task it demonstrates 7-21 times faster inference compared to
its CPU and 4-7 times better performance compared to the
overall best CPU results. In tests 2, 4, 5, 8 that are supporting
hardware acceleration, it requires on average 132, 274, 240
and 193 milliseconds to process one image, respectively. The
only main weakness of this NPU is the lack of acceleration
support for quantized models — in the ﬁrst test all computa-
tions are running on CPU with an average processing time of
160ms per image, which is signiﬁcantly higher than the cor-
responding results of the Snapdragon 845 with enabled DSP.
Though this problem can be solved by implementing a quan-
tized mode in Kirin’s NNAPI drivers, at the present time this
functionality is still under development.

Regarding other HiSilicon chipsets, they are now not pro-
viding acceleration for AI apps, and thus all computations are
running on CPUs only. Since all HiSilicon’s SoCs are based
on standard Arm Cortex cores, their performance is also quite
similar to other chipsets with the same Cortex architectures.

• MediaTek. The Helio P60 is the ﬁrst chipset to get
NNAPI drivers for accelerating both ﬂoat and quantized mod-
els. Quantized networks are running on its integrated APU
that is showing a performance similar to that of the Hexagon

11

SoC

Cores

Test 1,
ms

Test 2,
ms

Test 3,
ms

Test 4,
ms

Test 5,
ms

Test 6,
ms

Test 7,
ms

Test 8,
ms

HiSilicon Kirin 970
Mediatek Helio P60 Dev
Exynos 9810 Octa
Snapdragon 845
Exynos 8895 Octa
Snapdragon 835
Snapdragon 820
Nvidia Tegra X1
Snapdragon 660
Snapdragon 636
Exynos 8890 Octa
HiSilicon Kirin 955

CPU (4x2.4 GHz A73 & 4x1.8 GHz A53) + NPU
CPU (4x A73 + 4x A53) + GPU (Mali-G72 MP3) + APU
8 (4x2.7 GHz Mongoose M3 & 4x1.8 GHz Cortex-A55)
8 (4x2.8GHz Kryo 385 Gold & 4x1.8GHz Kryo 385 Silver)
8 (4x2.3 GHz Mongoose M2 & 4x1.7 GHz Cortex-A53)
8 (4x2.45 GHz Kryo 280 & 4x1.9 GHz Kryo 280)
4 (2x2.15 GHz Kryo & 2x1.6 GHz Kryo)
4 (4x1.9 GHz Maxwell)
8 (4x2.2 GHz Kryo 260 & 4x1.8 GHz Kryo 260)
8 (8x1.8 GHz Kryo 260)
8 (4x2.3 GHz Mongoose & 4x1.6 GHz Cortex-A53)
8 (4x2.5 GHz Cortex-A72 & 4x1.8 GHz Cortex A53)

160
21
149
65
135
97
119
102
115
110
139
136

132
439
1247
661
742
855
839
925
1025
1055
1810
1383

2586
2230
1580
1547
1548
2027
2074
2328
2299
2405
3314
2932

274
846
956
1384
1213
1648
1804
1811
1806
1910
1536
2143

240
1419
1661
3108
2576
3771
4015
3824
4072
4271
3594
5132

4848
4499
2450
3744
3181
4375
5055
4437
4695
4877
4717
6202

742
394
613
362
451
439
410
384
547
515
937
751

193
1562
1230
1756
1492
2046
2128
2161
2225
2330
2148
2731

Table 3: Benchmark results for several SoCs, the full list available at: http://ai-benchmark.com/ranking_processors

685 DSP — 21ms for processing one image in the ﬁrst test.
Float networks are executed on the Mali-G72 MP3 GPU that
provides about 2-5 times acceleration compared to its CPU
and 1.5-2x faster runtime than the overall best CPU results.
We should mention that all these numbers were obtained on
MediaTek’s developer phones, while the only Helio P60-based
actual device having NNAPI drivers (Vivo V11) is showing
slightly worse results.

Other MediaTek chipsets are currently not supporting ac-
celeration for AI applications. They run on CPU cores with
standard Arm Cortex designs.

• Samsung. At the time of writing, neither of Samsung’s
SoCs can provide any acceleration for third-party AI apps: all
devices with these chipsets are using default NNAPI drivers.
Since the latest Exynos 9810 SoC has the same Mali-G72
graphics as in the MediaTek P60 chipset (but with 12 instead
of 3 cores), we can expect an additional speed-up factor of 3-4
for ﬂoat neural networks if the Arm NN library was integrated
by Samsung into its NNAPI drivers. Since all recent Samsung
Exynos processors are using Arm Mali GPUs, the same logic
can be applied to them just the same.

Depending on the task, Samsung’s Mongoose M3 CPU
cores can demonstrate signiﬁcantly better or worse perfor-
mance compared to custom Kryo 385 cores in the Snapdragon
845, but their overall performance can be considered quite
comparable. The Mongoose M2 microarchitecture shows a
signiﬁcant 50% boost over the ﬁrst M1 version, while the per-
formance of the second (M2) and third (M3) generations is
rather similar. One notable issue with the latest Exynos 8895
and 9810 SoCs is related to their integrated power manage-
ment system responsible for adjusting the CPU performance.
It is causing very unstable results on the majority of devices:
in particular, several subsequent benchmark runs (with an in-
terval of 10 minutes, “high performance” mode) on the same
Galaxy S9 phone demonstrated up to 50% variation of the
total score, while the results obtained from different devices
showed an even larger variation (e.g., 200-800 ms in the sev-
enth test). Currently, there is no way to have external control
over different performance modes as they are selected auto-
matically based on the integrated logic.

• Others. We have obtained results from a number of other
chipsets that are either not widely used (e.g., Spreadtrum) or

deprecated by their manufacturers (e.g., Intel Atom, Nvidia
Tegra, TI OMAP). Especially interesting in the context of AI
and deep learning are Nvidia Tegra platforms that are support-
ing CUDA [68] and cuDNN [69] GPU-accelerated libraries of
primitives for deep neural networks. Unfortunately, no new
devices using Nvidia SoCs were released since 2015, and the
existing ones are already deprecated and will not get (NNAPI)
drivers for accelerating machine learning mobile frameworks.

6 Discussion

Software and hardware support for machine learning on mo-
bile devices is now evolving extremely fast, with various mile-
stone releases announced each several months. While they are
certainly bringing new possibilities and higher levels of per-
formance, the current lack of standardized requirements and
publicly available speciﬁcations does not always allow for an
objective assessment of their real advantages and limitations.
Below we would like to summarize our experience of work-
ing with mobile machine learning frameworks and chipsets
providing hardware acceleration via NNAPI drivers.

Currently, the easiest way to start using deep learning on
Android is to go for a mature and relatively stable TensorFlow
It was introduced more than two years
Mobile framework.
ago, and all major issues are already solved, while plenty of
information on smaller problems is available on various spe-
cialized websites. If hardware acceleration is one of the crit-
ical problems, TensorFlow Lite can still be an option, but we
would not recommend using it now for anything more compli-
cated than image classiﬁcation with MobileNet or Inception
CNNs as there still might be occasional problems with non-
standard network architectures on some mobile platforms. We
can also mention that migrating from TF Mobile to Lite is
relatively easy since they are using very similar Android pro-
gramming interfaces (the biggest difference will be in con-
verting pre-trained models to .tﬂite instead of .pb format), and
thus can be done later when TF Lite gets better support. If the
application is targeted at some speciﬁc device or SoC, the cor-
responding proprietary SDK can also be used, though in this
case the development might not be so easy and convenient.
Regarding Caffe2 Mobile and other less widespread frame-

12

works, their communities are now very small, which means
that almost no tutorials and problem descriptions are available
on the internet, thus all appearing problems might be primar-
ily solved only by creating new issues in the corresponding
github repositories.

Hardware acceleration for AI algorithms on Android de-
vices is now an even more controversial topic. At the time of
writing, the fastest runtime for conventional ﬂoat neural net-
works is shown by Huawei devices with Kirin 970 chipsets
that at the time of their presentation were signiﬁcantly ahead
of the market. Yet, we prefer to stay neutral regarding the fu-
ture perspectives, as our analysis has demonstrated that almost
all SoC manufacturers have the potential to achieve similar re-
sults in their new chipsets. The real situation will become clear
at the beginning of the next year when the ﬁrst devices with
the Kirin 980, the MediaTek P80 and the next Qualcomm and
Samsung Exynos premium SoCs will appear on the market.
Besides the performance, we would also like to look at their
power efﬁciency since a signiﬁcant battery drain might restrict
their usage to a few standard in-camera processing techniques.
The last topic that we want to address here is the use of
quantized networks. Their current applicability is rather lim-
ited, as there are still no standard and reliable tools for quan-
tizing networks trained even for image classiﬁcation, not to
mention more complex tasks. At the moment we can expect
two different ways of development in this area.
In the ﬁrst
case, the problem of quantization will be largely solved at
some point, and the majority of neural networks deployed on
smartphones will be quantized. In the second case, speciﬁc
NPUs supporting ﬂoat networks will become even more pow-
erful and efﬁcient, and the need for quantization will disappear
as this happened to many optimized solutions developed due
to the lack of computational power in the past. Since we can-
not easily predict the future outcome, we will still be using a
mixture of quantized and ﬂoat models in the benchmark with
predominance of the second ones, though in the future releases
the corresponding ratio might be signiﬁcantly altered.

Since currently there are still many important open ques-
tions that might be answered only with new major software
and hardware releases related to machine learning frameworks
and new dedicated chipsets, we are planning to publish reg-
ular benchmark reports describing the actual state of AI ac-
celeration on mobile devices, as well as changes in the ma-
chine learning ﬁeld and the corresponding adjustments made
in the benchmark to reﬂect them. The latest results obtained
with the AI Benchmark and the description of the actual tests
will also be updated monthly on the project website: http:
//ai-benchmark.com. Additionally, in case of any tech-
nical problems or some additional questions you can always
contact the ﬁrst two authors of this paper.

7 Conclusions

chipsets that can be potentially used for accelerating the ex-
ecution of neural networks on smartphones and other portable
devices, and described popular mobile frameworks for run-
ning AI algorithms on mobile devices. We presented the AI
Benchmark that measures different performance aspects as-
sociated with running deep neural networks on smartphones
and other Android devices, and discussed the real-world re-
sults obtained with this benchmark from over 10,000 mobile
devices and more than 50 different mobile SoCs. Finally, we
discussed future perspectives of software and hardware devel-
opment related to this area and gave our recommendations re-
garding the current deployment of deep learning models on
Android devices.

References

[1] Krizhevsky, A., Sutskever, I., Hinton, G.E.: Imagenet classiﬁcation with
deep convolutional neural networks. In: Advances in neural information
processing systems. (2012) 1097–1105 1

[2] Szegedy, C., Vanhoucke, V., Ioffe, S., Shlens, J., Wojna, Z.: Rethinking
the inception architecture for computer vision. In: Proceedings of the
IEEE conference on computer vision and pattern recognition. (2016)
2818–2826 1, 7

[3] Howard, A.G., Zhu, M., Chen, B., Kalenichenko, D., Wang, W.,
Weyand, T., Andreetto, M., Adam, H.: Mobilenets: Efﬁcient convo-
lutional neural networks for mobile vision applications. arXiv preprint
arXiv:1704.04861 (2017) 1, 7

[4] Ignatov, A., Kobyshev, N., Timofte, R., Vanhoey, K., Van Gool, L.:
Dslr-quality photos on mobile devices with deep convolutional net-
works.
In: the IEEE Int. Conf. on Computer Vision (ICCV). (2017)
1, 8

[5] Ignatov, A., Kobyshev, N., Timofte, R., Vanhoey, K., Van Gool, L.:
Wespe: weakly supervised photo enhancer for digital cameras. arXiv
preprint arXiv:1709.01118 (2017) 1

[6] Ignatov, A., Timofte, R., et al.: Pirm challenge on perceptual image
In: European Conference on

enhancement on smartphones: Report.
Computer Vision Workshops. (2018) 1

[7] Dong, C., Loy, C.C., He, K., Tang, X.:

Image super-resolution using
deep convolutional networks. IEEE transactions on pattern analysis and
machine intelligence 38(2) (2016) 295–307 1, 8

[8] Ledig, C., Theis, L., Husz´ar, F., Caballero, J., Cunningham, A., Acosta,
A., Aitken, A.P., Tejani, A., Totz, J., Wang, Z., et al.: Photo-realistic
single image super-resolution using a generative adversarial network.
In: CVPR. Volume 2. (2017) 4 1, 8

[9] Timofte, R., Gu, S., Wu, J., Van Gool, L., et al.: Ntire 2018 chal-
lenge on single image super-resolution: Methods and results. In: The
IEEE Conference on Computer Vision and Pattern Recognition (CVPR)
Workshops. (June 2018) 1

[10] Netzer, Y., Wang, T., Coates, A., Bissacco, A., Wu, B., Ng, A.Y.: Read-
ing digits in natural images with unsupervised feature learning.
In:
NIPS workshop on deep learning and unsupervised feature learning.
Volume 2011. (2011) 5 1

[11] Wu, Y., Lim, J., Yang, M.H.: Object tracking benchmark.

IEEE
Transactions on Pattern Analysis and Machine Intelligence 37(9) (2015)
1834–1848 1

[12] Huang, J., Rathod, V., Sun, C., Zhu, M., Korattikara, A., Fathi, A.,
Fischer, I., Wojna, Z., Song, Y., Guadarrama, S., et al.: Speed/accuracy
trade-offs for modern convolutional object detectors. In: IEEE CVPR.
Volume 4. (2017) 1

In this paper, we discussed the latest achievements in the area
of machine learning and AI in the Android ecosystem. First,
we presented an overview of all currently existing mobile

[13] Li, L.J., Socher, R., Fei-Fei, L.: Towards total scene understanding:
Classiﬁcation, annotation and segmentation in an automatic framework.
In: Computer Vision and Pattern Recognition, 2009. CVPR 2009. IEEE
Conference on, IEEE (2009) 2036–2043 1

13

[14] Cordts, M., Omran, M., Ramos, S., Rehfeld, T., Enzweiler, M., Benen-
son, R., Franke, U., Roth, S., Schiele, B.: The cityscapes dataset for
semantic urban scene understanding. In: Proceedings of the IEEE con-
ference on computer vision and pattern recognition. (2016) 3213–3223
1

[15] Li, H., Lin, Z., Shen, X., Brandt, J., Hua, G.: A convolutional neural
network cascade for face detection. In: Proceedings of the IEEE Confer-
ence on Computer Vision and Pattern Recognition. (2015) 5325–5334
1

[16] Schroff, F., Kalenichenko, D., Philbin, J.: Facenet: A uniﬁed embed-
ding for face recognition and clustering. In: Proceedings of the IEEE
conference on computer vision and pattern recognition. (2015) 815–823
1, 8

[17] Zhang, X., Sugano, Y., Fritz, M., Bulling, A.: Appearance-based gaze
In: Proceedings of the IEEE Conference on

estimation in the wild.
Computer Vision and Pattern Recognition. (2015) 4511–4520 1

[33] TensorFlow-Mobile: https://www.tensorﬂow.org/mobile/mobile intro.

Retrieved on: 30.09.2018 2, 6, 9

[34] Reddy, V.G.: Neon technology introduction. ARM Corporation (2008)

2, 5, 9

[35] Motamedi, M., Fong, D., Ghiasi, S.: Cappuccino: Efﬁcient cnn infer-
ence software synthesis for mobile system-on-chips. IEEE Embedded
Systems Letters (2018) 2

[36] Alzantot, M., Wang, Y., Ren, Z., Srivastava, M.B.: Rstensorﬂow: Gpu
enabled tensorﬂow for deep learning on commodity android devices.
In: Proceedings of the 1st International Workshop on Deep Learning
for Mobile Systems and Applications, ACM (2017) 7–12 2

[37] SNPE: https://developer.qualcomm.com/docs/snpe/overview.html. Re-

trieved on: 30.09.2018 2, 6

[38] HiAI: https://developer.huawei.com/consumer/en/devservice/doc/2020315.

Retrieved on: 30.09.2018 2, 4, 6

[18] Sutskever, I., Vinyals, O., Le, Q.V.: Sequence to sequence learning
with neural networks. In: Advances in neural information processing
systems. (2014) 3104–3112 1

[39] Lee, Y.L., Tsung, P.K., Wu, M.: Techology trend of edge ai. In: VLSI
Design, Automation and Test (VLSI-DAT), 2018 International Sympo-
sium on, IEEE (2018) 1–2 2, 4, 6

[19] Bahdanau, D., Cho, K., Bengio, Y.: Neural machine translation by
jointly learning to align and translate. arXiv preprint arXiv:1409.0473
(2014) 1

[20] Mikolov, T., Chen, K., Corrado, G., Dean, J.: Efﬁcient estimation of
word representations in vector space. arXiv preprint arXiv:1301.3781
(2013) 1

[21] Hu, B., Lu, Z., Li, H., Chen, Q.: Convolutional neural network architec-
tures for matching natural language sentences. In: Advances in neural
information processing systems. (2014) 2042–2050 1

[22] Socher, R., Perelygin, A., Wu, J., Chuang, J., Manning, C.D., Ng, A.,
Potts, C.: Recursive deep models for semantic compositionality over a
sentiment treebank. In: Proceedings of the 2013 conference on empiri-
cal methods in natural language processing. (2013) 1631–1642 1

[23] Severyn, A., Moschitti, A.: Twitter sentiment analysis with deep con-
volutional neural networks. In: Proceedings of the 38th International
ACM SIGIR Conference on Research and Development in Information
Retrieval, ACM (2015) 959–962 1

[24] Serban, I.V., Sankar, C., Germain, M., Zhang, S., Lin, Z., Subramanian,
S., Kim, T., Pieper, M., Chandar, S., Ke, N.R., et al.: A deep reinforce-
ment learning chatbot. arXiv preprint arXiv:1709.02349 (2017) 1

[25] Kwapisz, J.R., Weiss, G.M., Moore, S.A.: Activity recognition us-
ing cell phone accelerometers. ACM SigKDD Explorations Newsletter
12(2) (2011) 74–82 1

[26] Ignatov, A.: Real-time human activity recognition from accelerometer
data using convolutional neural networks. Applied Soft Computing 62
(2018) 915–922 1

[27] Ord´o˜nez, F.J., Roggen, D.: Deep convolutional and lstm recurrent
neural networks for multimodal wearable activity recognition. Sensors
16(1) (2016) 115 1

[28] Sathyanarayana, A., Joty, S., Fernandez-Luque, L., Oﬂi, F., Srivastava,
J., Elmagarmid, A., Arora, T., Taheri, S.: Sleep quality prediction from
wearable data using deep learning. JMIR mHealth and uHealth 4(4)
(2016) 1

[40] NNAPI:

https://developer.android.com/ndk/guides/neuralnetworks/.

Retrieved on: 30.09.2018 2

[41] Chance, R.: Devices overview. Digital Signal Processing: Principles,

Devices and Applications 42 (1990) 4 2

[42] Hesseldahl, A.: The legacy of dsp1. Electronic News 45(45) (1999)

44–44 2

[43] Guttag, K.: Tms320c8x family architecture and future roadmap.

In:
Digital Signal Processing Technology. Volume 2750., International So-
ciety for Optics and Photonics (1996) 2–12 2

[44] Hays, W.P.: Dsps: Back to the future. Queue 2(1) (2004) 42 2

[45] LeCun, Y., Boser, B., Denker, J.S., Henderson, D., Howard, R.E., Hub-
bard, W., Jackel, L.D.: Backpropagation applied to handwritten zip
code recognition. Neural computation 1(4) (1989) 541–551 3

[46] ArmNN:

https://github.com/arm-software/armnn.

Retrieved on:

30.09.2018 5, 6

[47] Torch-Android: https://github.com/soumith/torch-android. Retrieved

on: 30.09.2018 6

[48] Deeplearning4j: https://deeplearning4j.org/docs/latest/deeplearning4j-

android. Retrieved on: 30.09.2018 6

[49] TensorFlow-Lite: https://www.tensorﬂow.org/mobile/tﬂite/. Retrieved

on: 30.09.2018 6, 9

[50] Caffe-Android: https://github.com/sh1r0/caffe-android-lib. Retrieved

on: 30.09.2018 6, 7

[51] Caffe2-Android:

https://caffe2.ai/docs/mobile-integration.html. Re-

trieved on: 30.09.2018 6, 7

[52] MXNet: https://github.com/leliana/whatsthis. Retrieved on: 30.09.2018

6

[53] NNabla: https://github.com/sony/nnabla. Retrieved on: 30.09.2018 6

[54] Abadi, M., Barham, P., Chen, J., Chen, Z., Davis, A., Dean, J., Devin,
M., Ghemawat, S., Irving, G., Isard, M., et al.: Tensorﬂow: a system for
large-scale machine learning. In: OSDI. Volume 16. (2016) 265–283 6

[29] Lane, N.D., Georgiev, P.: Can deep learning revolutionize mobile sens-
In: Proceedings of the 16th International Workshop on Mobile

ing?
Computing Systems and Applications, ACM (2015) 117–122 1

[55] TensorFlow-Mobile/Lite:
trieved on: 30.09.2018 6

https://www.tensorﬂow.org/mobile/. Re-

[30] Codrescu, L., Anderson, W., Venkumanhanti, S., Zeng, M., Plondke,
E., Koob, C., Ingle, A., Tabony, C., Maule, R.: Hexagon dsp: An ar-
chitecture optimized for mobile multimedia and communications. IEEE
Micro (2) (2014) 34–43 1, 3

[31] Latiﬁ Oskouei, S.S., Golestani, H., Hashemi, M., Ghiasi, S.: Cn-
ndroid: Gpu-accelerated execution of trained deep convolutional neural
networks on android. In: Proceedings of the 2016 ACM on Multimedia
Conference, ACM (2016) 1201–1205 1

[56] Jia, Y., Shelhamer, E., Donahue, J., Karayev, S., Long, J., Girshick,
R., Guadarrama, S., Darrell, T.: Caffe: Convolutional architecture for
fast feature embedding. In: Proceedings of the 22nd ACM international
conference on Multimedia, ACM (2014) 675–678 7

[57] Caffe2-AICamera-Demo:
trieved on: 30.09.2018 7

https://github.com/caffe2/aicamera. Re-

[58] TFLite-Benchmark: https://www.tensorﬂow.org/mobile/tﬂite/performance.

Retrieved on: 30.09.2018 7

[32] Guihot, H.: Renderscript. In: Pro Android Apps Performance Opti-

[59] Caffe2-Presentation:

https://www.slideshare.net/kstan2/caffe2-on-

mization. Springer (2012) 231–263 2

android. Retrieved on: 30.09.2018 7

14

[60] Jacob, B., Kligys, S., Chen, B., Zhu, M., Tang, M., Howard, A.,
Adam, H., Kalenichenko, D.: Quantization and training of neural net-
arXiv preprint
works for efﬁcient integer-arithmetic-only inference.
arXiv:1712.05877 (2017) 7

[61] Sheng, T., Feng, C., Zhuo, S., Zhang, X., Shen, L., Aleksic, M.:
A quantization-friendly separable convolution for mobilenets. arXiv
preprint arXiv:1803.08607 (2018) 7

[62] Szegedy, C., Ioffe, S., Vanhoucke, V., Alemi, A.A.:

Inception-v4,
inception-resnet and the impact of residual connections on learning. In:
AAAI. Volume 4. (2017) 12 8

[63] FaceNet-github: https://github.com/davidsandberg/facenet. Retrieved

on: 30.09.2018 8

[64] TF-Slim: https://github.com/tensorﬂow/models/tree/master/research/slim.

Retrieved on: 30.09.2018 8

[65] Kim, J., Kwon Lee, J., Mu Lee, K.: Accurate image super-resolution
using very deep convolutional networks. In: Proceedings of the IEEE
conference on computer vision and pattern recognition. (2016) 1646–
1654 8

[66] Johnson, J., Alahi, A., Fei-Fei, L.: Perceptual losses for real-time style
transfer and super-resolution. In: European Conference on Computer
Vision, Springer (2016) 694–711 8

[67] Zhao, H., Qi, X., Shen, X., Shi, J., Jia, J.:

semantic segmentation on high-resolution images.
arXiv:1704.08545 (2017) 8

Icnet for real-time
arXiv preprint

[68] Kirk, D., et al.: Nvidia cuda software and gpu parallel computing archi-

tecture. In: ISMM. Volume 7. (2007) 103–104 12

[69] Chetlur, S., Woolley, C., Vandermersch, P., Cohen, J., Tran, J., Catan-
zaro, B., Shelhamer, E.: cudnn: Efﬁcient primitives for deep learning.
arXiv preprint arXiv:1410.0759 (2014) 12

15

