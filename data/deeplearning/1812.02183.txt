Deep Learning at Scale
for the Construction of Galaxy Catalogs
in the Dark Energy Survey

Asad Khan,1, 2 E. A. Huerta,1, 3 Sibo Wang,1 Robert Gruendl,1, 3 Elise Jennings,4 and Huihuo Zheng4
1NCSA, University of Illinois at Urbana-Champaign, Urbana, Illinois 61801, USA
2Department of Physics, University of Illinois at Urbana-Champaign, Urbana, Illinois 61801, USA
3Department of Astronomy, University of Illinois at Urbana-Champaign, Urbana, Illinois 61801, USA
4Argonne National Laboratory, Leadership Computing Facility, Lemont, Illinois 60439, USA
(Dated: July 9, 2019)

The scale of ongoing and future electromagnetic surveys pose formidable challenges to classify as-
tronomical objects. Pioneering eﬀorts on this front include citizen science campaigns adopted by
the Sloan Digital Sky Survey (SDSS). SDSS datasets have been recently used to train neural net-
work models to classify galaxies in the Dark Energy Survey (DES) that overlap the footprint of
both surveys. Herein, we demonstrate that knowledge from deep learning algorithms, pre-trained
with real-object images, can be transferred to classify galaxies that overlap both SDSS and DES
surveys, achieving state-of-the-art accuracy ∼> 99.6%. We demonstrate that this process can be
completed within just eight minutes using distributed training. While this represents a signiﬁcant
step towards the classiﬁcation of DES galaxies that overlap previous surveys, we need to initiate the
characterization of unlabelled DES galaxies in new regions of parameter space. To accelerate this
program, we use our neural network classiﬁer to label over ten thousand unlabelled DES galaxies,
which do not overlap previous surveys. Furthermore, we use our neural network model as a feature
extractor for unsupervised clustering and ﬁnd that unlabeled DES images can be grouped together
in two distinct galaxy classes based on their morphology, which provides a heuristic check that the
learning is successfully transferred to the classiﬁcation of unlabelled DES images. We conclude by
showing that these newly labeled datasets can be combined with unsupervised recursive training to
create large-scale DES galaxy catalogs in preparation for the Large Synoptic Survey Telescope era.

Keywords: Deep Learning, Convolutional Neural Net-
works, Sloan Digital Sky Survey, Dark Energy Survey,
Large Synoptic Survey Telescope, Galaxy Catalogs, Un-
supervised Learning, Data Clustering

I.

INTRODUCTION

Electromagnetic surveys provide key insights into the
large scale structure of the Universe, its geometry and
evolution in cosmic time. As the depth and scale of these
surveys continue to increase in years to come, they will
push back the frontiers of our understanding of dark mat-
ter and dark energy [1–4].

The classiﬁcation of astrophysical objects has been
pursued in the past using a diverse set of tools. For
instance, galaxies have been classiﬁed using their pho-
tometric properties, achieving classiﬁcation accuracies ∼
85% [5]. Other methods to classify galaxies according to
their morphology have taken into account their physical
properties across multiple wavelengths. For instance, the
method introduced in [6], considered a sample of galax-
ies from the Sloan Digital Sky Survey (SDSS) [7], using
the ﬁve SDSS ﬁlters (u, g, r, i, z), and then used a com-
bination of shapelet decomposition and Principal Com-
ponents Analysis (PCA). Other methods for galaxy clas-
siﬁcation include Concentration-Asymmetry-Smoothness
(CAS) [8], and machine learning, including artiﬁcial neu-
ral networks and PCAs [9–11].

In recent years, citizen science campaigns have played

a key role to classify thousands of celestial objects in as-
tronomical surveys. SDSS is an archetypical example of a
successful approach to classify hundreds of thousands of
galaxies. As electromagnetic surveys continue to increase
their depth and coverage, campaigns of this nature may
lack scalability. For instance, within six years of opera-
tion, the Dark Energy Survey (DES) [12] observed over
three hundred million galaxies, a number that will be sur-
passed by the observing capabilities of the Large Synoptic
Survey Telescope (LSST) [13]. In brief, there is a pressing
need to explore new approaches to maximize the science
throughput of next-generation electromagnetic surveys.
A promising paradigm is the convergence of deep learn-
ing and large scale computing to address the imminent
increase in data volume, complexity, and latency of ob-
servations of LSST-type surveys, the theme of this paper.
An innovative idea to accomplish this prospect consists
of leveraging what SDSS has already done, and try to use
it as seed information to classify objects in DES data.
Such idea has been explored in [14], where SDSS galaxies
that overlap the DES footprint were used to train neural
network models to classify DES galaxies that were also
observed by SDSS, reporting classiﬁcation accuracies ∼
95% [14].

While the aforementioned approach provides a way
to classify DES galaxies that overlap previous surveys,
key issues remain: (i) deep learning algorithms for image
classiﬁcation have been trained with hundreds of millions
of images to achieve state-of-the-art classiﬁcation accu-
racy [15]. If one attempts to train a neural network model

9
1
0
2

l
u
J

8

]

M

I
.
h
p
-
o
r
t
s
a
[

2
v
3
8
1
2
0
.
2
1
8
1
:
v
i
X
r
a

 
 
 
 
 
 
from the ground up using just a few tens of thousands
of SDSS galaxies, then the fully trained neural network
model may not achieve state-of-the-art classiﬁcation ac-
curacy, or exhibit overﬁtting [16]; (ii) while training a
neural network model with SDSS galaxies, and then ap-
plying it to classify DES galaxies that overlap the foot-
print of both SDSS and DES is an important validation
study for the applicability of deep learning for classiﬁ-
cation analyses, we also need to demonstrate the appli-
cability of this approach for DES galaxies that have not
yet been observed in previous surveys. This can only be
accomplished once more DES galaxies are labeled; (iii)
newly labelled DES galaxies, that do not overlap previ-
ous surveys, can be used as training datasets to enhance
the classiﬁcation accuracy of deep learning algorithms.
One can easily realize that this approach will lead to the
creation of TB-size training datasets. In this scenario, it
will be essential to design distributed algorithms to re-
duce the training stage at a minimum, while retaining
state-of-the-art classiﬁcation accuracy.

In this article we describe an approach to address the
aforementioned challenges by bringing together several
deep learning methods in an innovative manner. Key
highlights of this study include:
• We transfer knowledge from the state-of-the-art neural
network model for image classiﬁcation, Xception [15],
trained with the ImageNet dataset [17], to classify
SDSS galaxy images, achieving state-of-the-art accura-
cies 99.8%. Note that transfer learning between similar
datasets, such as SDSS and DES, has been tradition-
ally used in the computer science literature [18].
In
stark contrast, we use a pre-trained model for real-
world object recognition, and then transfer its knowl-
edge to classify SDSS and DES galaxies. To the best
of our knowledge this is the ﬁrst application of deep
transfer learning for galaxy classiﬁcation1.

• To streamline and accelerate this method, we intro-
duce the ﬁrst application of deep transfer learning and
distributed training in cosmology, reducing the train-
ing stage of the Xception model with galaxy image
datasets from ﬁve hours to just eight minutes, using
64 K80 GPUs in the Cooley supercomputer.

• We show that our neural network model trained by
transfer learning achieves state-of-the-art accuracy,
99.6%, to classify DES galaxies that overlap the foot-
print of the SDSS survey.

• We use our neural network classiﬁer to label over ten
thousand unlabeled DES galaxies that have not been
observed in previous surveys. We then turn our neural
network model into a feature extractor to show that
these unlabeled datasets can be clustered according to
their morphology, forming two distinct datasets.

1 While this paper was under review, a study on SDSS galaxy clas-
siﬁcation was presented in which disparate datasets for transfer
learning are used [19].

2

• Finally, we use the newly labelled DES images and
do unsupervised recursive training to retrain our deep
transfer learning model, boosting its accuracy to clas-
sify unlabeled DES galaxies in bulk in new regions of
parameter space.
The combination of all the aforementioned deep learn-
ing methods lays the foundations to exploit deep transfer
learning at scale, data clustering and recursive training to
produce large-scale galaxy catalogs in the LSST era [13].
This paper is organized as follows. Section II presents
the approach followed to curate the datasets and deep
learning algorithms designed and trained for our analy-
ses. In section III, we demonstrate the applicability of
our methods to classify galaxies in SDSS, galaxies that
overlap SDSS and DES, and ﬁnally, the applicability of
our approach to correctly classify thousands of unlabelled
DES galaxies. Finally, section IV summarizes our ﬁnd-
ings and future directions of work.

II. METHODS

In this section we describe the SDSS and DES datasets
we have curated for our studies, the neural network model
we use for deep transfer learning, and how we can use
unsupervised recursive training to create galaxy catalogs
at scale.

A. Data Curation for SDSS and DES

We use a subset of SDSS Data Release (DR) 7 images
for which we have high conﬁdence classiﬁcations through
the Galaxy Zoo project, i.e., we only choose galaxies with
debiased probability greater than 0.985 for combined spi-
rals, and 0.926 for ellipticals, respectively, as shown in
[20]. We choose these cutoﬀ thresholds to
Table 2 of
ensure that; (i) the galaxies used for training the neural
network have robust and accurate classiﬁcations; and (ii)
the representation of both classes in the training and test
datasets are balanced. We then divide these images into
three separate datasets for training, validation and test-
ing. The validation set is used to monitor the accuracy
and loss when training and ﬁne-tuning our deep neural
network, and hence serves to optimize hyperparameters,
such as learning rate and number of epochs, for training.
Two test sets are carefully constructed so that the im-
ages in each set lie in both the SDSS and DES footprints.
The ﬁrst test set consists of images with Galazy Zoo clas-
siﬁcation conﬁdence similar to that of the training set,
i.e., a high probability cut-oﬀ is introduced. This test
set is hence labelled High Probability (HP) Test Set, and
there are two versions, one for each survey, i.e., HP SDSS
and HP DES. Just as in the training set, the images for
SDSS are obtained from (DR) 7 and the corresponding
images for DES are obtained from the DES DR1 data re-
lease. Furthermore, a second test set is created without
introducing any probability thresholds on Galaxy Zoo

classiﬁcation conﬁdence. This set consists of almost all
galaxies lying in both the SDSS and DES footprints, and

Dataset
Training set
HP SDSS Test Set
HP DES Test Set
FO SDSS Test Set
FO DES Test Set

Spirals
18,352
516
516
6,677
6,677

Ellipticals
18,268
550
550
5,904
5,904

TABLE I: Summary of the SDSS and DES datasets
used for training and testing.

FIG. 1: Violin Plots of Galaxy Zoo Probability
Distributions for galaxies in each dataset. Probability
threshold cutoﬀs, 98.5% for spiral and 92.6% for
elliptical, are shown as red dashed lines. These cutoﬀs
have been selected to ensure that the datasets of both
galaxy types are balanced.

hence is labelled Full Overlap (FO) Test Set. Again
there are two versions, FO SDSS and FO DES. The mo-
tivation behind creating this second test set is that the
galaxy proﬁles in the unlabelled DES dataset will more
closely match those in FO test sets. Hence FO test set
serves as a good evaluation metric of the performance
of our neural net on the ultimate task of classifying all
unlabelled galaxies in the DES catalogue.

The properties of these datasets are summarized in Ta-
ble I, while their probability distributions are presented
in Fig. 1. A sample of the training SDSS dataset, and the
HP Test set images are presented in the top and bottom
panels of Fig. 2, respectively.
SDSS Dataset We used the de-biased probabilities for
elliptical and combined spiral classes described in Table
2 of [20] to create labels for the two classes of our training
and test sets. After selecting the OBJIDs from Table 2
based on the probability thresholds of 0.985 and 0.926
for spirals and ellipticals respectively, we submit SQL
queries to the SDSS Skyserver [21] to obtain g, r and
i-band images and metadata from the PhotoObj table.

3

Thereafter, each galaxy is ‘cut-out’ from the downloaded
telescope ﬁts ﬁles for each band, and then the bands are
stacked together to create a color image.

We developed the scripts to download and preprocess
data as open source Python software stacks 2. To facili-
tate and streamline these tasks at scale, we incorporated
Message Passing Interface (MPI) [22] to exploit multiple
nodes on supercomputers for a fast parallel computation.
In our case, the data extraction and curation was done
using the Blue Waters Supercomputer [23].
DES Dataset The same steps are repeated to ﬁrst se-
lect the DES DR1 metadata and images from the NCSA
DESaccess web [24], and then to cut-out, preprocess and
stack the ﬁlters together to create RGB color images. Ad-
ditionally, the Astropy package match to catalog sky is
used to crossmatch DES and SDSS catalogues to within
1 arcsec. Finally we pick a random sample of ∼ 10, 000
bright DES galaxies to quantify the classiﬁcation and
clustering performance of our neural network model.

B. Deep Learning: Model and Methodology

We use open source software stacks for our stud-
ies. The deep learning APIs used are Keras [25] and
Tensorflow [26]. For the classiﬁcation problem we do
transfer learning starting with the Xception model [15],
which has been pre-trained with the ImageNet [27]
dataset. We choose this neural network model because it
outperforms many other state-of-the-art neural network
models, including Inception-v3 [28], ResNet-152 [29]
and VGG16 [30] on ImageNet validation dataset, and it has
been suggested that better ImageNet architectures are
capable of learning better transferable representations
[31]. More importantly, we carried out several experi-
ments and found that Xception exhibits either as good
or nominally better performance on our validation and
testing galaxy datasets compared to many other state of
the art architectures (see Fig. 3).
Bearing in mind that the Xception model [15] was orig-
inally trained on the ImageNet [27] dataset, with images
resized to 299 × 299 × 3, we have followed best prac-
tices of neural network training [32], and have resized all
the galaxy sub-images to be 299 × 299 pixels, and then
stacked the three ﬁlters together to create a color image
of size 299 × 299 × 3. Finally, these sub-images are mean
subtracted and normalized to convert the pixel values to
-1 to 1 range centered around 0. These curated datasets
serve as the input tensor into our deep neural network
model.

For training, we ﬁrst extract the feature maps from
the second last layer of the pretrained model for a single
epoch and feed them into a few custom deﬁned fully con-
nected layers added at the end of the pre-trained model

2 The code is publicly available in a github repository at https:

//github.com/khanx169/DL_DES

Train0.920.930.940.950.960.970.980.991.00ProbabilityValidation0.920.930.940.950.960.970.980.991.00HP Test0.920.930.940.950.960.970.980.991.00FO Test0.00.20.40.60.81.0Probability Distribution for each Data SetSpiralElliptical(see Figure 7 in A). Then we progressively unfreeze the
earlier layers of the whole network and ﬁne tune their
weights for a few epochs of training.

The rationale behind this approach is that the earlier
layers of a trained network are very versatile ﬁlters able
to pick up simple abstract features like lines and edges
relevant to any image detection or classiﬁcation prob-
lem. However, deeper into the network, the weights of
the layers become less interpretable and more speciﬁc
to the given problem at hand [33]. Hence, by training
the last layers ﬁrst and then progressively ﬁne tuning
the earlier layers we make sure that the useful weights
learned on millions of ImageNet [17] images are not de-
stroyed while the neural network learns and adapts to the
galaxy classiﬁcation problem [34]. Deep transfer learning
has been explored in physics and astronomy classiﬁcation
problems, including noise anomaly classiﬁcation in gravi-
tational wave data [16], galaxy merger classiﬁcation [35],
and galaxy classiﬁcation [14, 19].
Single-GPU Training We train the network using
Tesla P100 GPUs on XSEDE (Bridges) [36]. The train-
ing process for the dataset of 36500 images is completed
within 5 hours. We use categorical cross entropy as the
loss function together with ADAM optimizer [37]. To
avoid over-ﬁtting, we monitor both training and vali-
dation losses, add a dropout rate of 70% between our
fully connected layers, and also use early-stopping, i.e.,
we stop training once validation loss stops decreasing.
Additionally we use the learning rate scheduler, i.e., we
reduce the learning rate when training loss stops decreas-
ing to do a more ﬁne-grained search of the loss function’s
minima, and data augmentation. For data augmentation
we use random ﬂips, rotations, zooms and shifts as shown
in Figure 8 in B. After training, all the weights are frozen
and saved, and inference on about 10,000 test images is
completed within 10 minutes using a single Tesla P100
GPU.
Distributed Training Figure 4 shows the parallel train-
ing performance for the Xception model using up to 64
K80 GPUs (see Table III for a detailed breakdown of
these results). The code was distributed across multi-
ple GPUs using the Horovod distributed framework for
Keras [38].3 We ﬁnd that distributing the workload de-
creases the time per epoch linearly, and signiﬁcantly re-
duces the training of 36,620 images from ∼ 5 hours using
a single GPU to 8m using 64 GPUs, with similar accu-
racy. 4

The last layer of the network has two softmax nodes,

3 These results were obtained on the Intel Haswell and NVIDIA
K80 based supercomputer, Cooley, at Argonne Leadership Com-
puting Facility using a data parallelization scheme through
Horovod.

4 The base learning rate was 0.0001 and was scaled by the number
of GPUs, N , following [39] while keeping the mini-batch size
the same on each worker.
In addition we used a technique of
“warmup” epochs where we set the learning rate to be the base
learning rate and increase to 0.0001 ∗ N after 2 warmup epochs.

4

which provide the output probability that the input im-
age belongs to a given galaxy class. To quantify the
over-all accuracy of the neural network, we extract the
output probabilities from this last layer for our two HP
and FO test sets, and compare them against the ground
truth labels provided through the Galaxy Zoo project.
While these probabilities can be directly tested for cross-
matched DES sets by comparing to the SDSS-Galaxy Zoo
probabilities, for the rest of the unlabelled DES images
this is not possible. For large-scale galaxy catalogs it
would be unfeasible to inspect individual images to de-
termine what class they belong to and check against the
neural networks out-put probabilities to check for consis-
tency. In practice, we can use the nodes of the second
last layer of the neural network to determine what combi-
nation of them is activated for each galaxy type. In this
approach, the activation vectors of this layer would form
two distinct clusters, for each galaxy type in a 1024-D
space. Checking whether similar combinations of neu-
rons are activated, i.e., similar clusters are formed for
the unlabelled DES data as the FO and HP test sets, will
serve as a heuristic check that the learning is successfully
transferred to the classiﬁcation of unlabelled DES images
for the purpose of constructing a catalog. For example, if
there is a lack of distinct clusters, or more than two clus-
ters are seen, then that would suggest unknown types
that are forced into being classiﬁed as spiral or elliptical
because the output layer has only two nodes.

In order to visualize these 1024-D clusters, we embed
them into a 3-D parameter space using the sklearn li-
brary implementation of t-Distributed Stochastic Neigh-
bor Embedding (t-SNE) [40]. t-SNE is a nonlinear di-
mensionality reduction technique that is particularly apt
for visualizing high-dimensional datasets by ﬁnding a
faithful representation in a low dimensional embedding,
typically 2-D or 3-D. It is important to note that t-SNE
adjusts its notion of distance to regional density varia-
tions in the dataset, and hence bounding boxes of clusters
in the low dimensional representation don’t correspond to
their relative sizes. Similarly, distances between clusters
may not be meaningful since they are aﬀected by a num-
ber of hyper-parameters such as perplexity and number
of iterations.

As a ﬁnal step, we introduce an application of
unsupervised/semi-supervised learning in the form of re-
cursive training, where we introduce into the training
set newly labelled DES galaxies and retrain our model.
It has been suggested in [14] that once trained with a
particular dataset from one survey, neural networks can
quickly adapt to new instrument characteristics (e.g.,
PSF, seeing, depth), reducing by almost one order of
magnitude the necessary training sample from a diﬀerent
survey for morphological classiﬁcation. However instead
of manually labelling new DES images, we extract the
out-put classiﬁcation probabilities for them through our
ﬁne-tuned neural network. We use a sample of 10,000
unlabelled bright DES galaxies and then by introducing
a threshold on the neural networks classiﬁcation conﬁ-

5

FIG. 2: Top panels: labelled images of the SDSS training set. Bottom panels: sample of galaxies from SDSS-DR7
and the corresponding crossmatched galaxies from DES DR1.

FIG. 3: Performance of several diﬀerent ﬁne-tuned architectures pre-trained on ImageNet. Top panels: Receiver
Operating Characteristic (ROC) for galaxies in the FO SDSS test set. Bottom panels: ROC for galaxies in the FO
DES test set. The smaller insets show the diﬀerence in True Positive Rate between Xception and each of the other
four models on a log scale.

SDSS spiralsSDSS ellipticalsSDSSDES0.00.20.40.60.81.0True Positive RateEllipticals - FO SDSSVGG16 (area = 0.95)VGG19 (area = 0.94)resnet50 (area = 0.95)inceptionV3 (area = 0.96)Xception (area = 0.96)0.00.20.40.60.81.0True Positive RateSpirals - FO SDSSVGG16 (area = 0.95)VGG19 (area = 0.94)resnet50 (area = 0.95)inceptionV3 (area = 0.96)Xception (area = 0.96)0.00.20.40.60.81.0False Positive Rate103101Difference0.00.20.40.60.81.0False Positive Rate104102Difference0.00.20.40.60.81.0True Positive RateEllipticals - FO DESVGG16 (area = 0.94)VGG19 (area = 0.93)resnet50 (area = 0.93)inceptionV3 (area = 0.94)Xception (area = 0.94)0.00.20.40.60.81.0True Positive RateSpirals - FO DESVGG16 (area = 0.94)VGG19 (area = 0.93)resnet50 (area = 0.93)inceptionV3 (area = 0.94)Xception (area = 0.94)0.00.20.40.60.81.0False Positive Rate102Difference0.00.20.40.60.81.0False Positive Rate103102Differencedence, we select the 1000 most conﬁdent predictions for
spiral and elliptical respectively to further ﬁne tune our
network on.

summarized in Fig 6, while the Receiver Operating Char-
acteristic (ROC) curves are shown in Fig 10 E.

6

Dataset
Training set
HP SDSS Test Set
HP DES Test Set
FO SDSS Test Set
FO DES Test Set

Precision Recall FPR Accuracy F1 score
99.81% 0.9998
99.81% 0.9980
99.62% 0.9961
96.76% 0.9675
96.32% 0.9685

0.004
0.995 0.002
0.991 0.055
0.946 0.025

0.996
0.998
0.945
0.965

1

FIG. 4: Speed up in training using up to 64 K80 GPUs
for the Xception model.

III. RESULTS

To check the performance of our neural network model
on HP and FO test sets, we use standard evaluation met-
rics: precision, recall, accuracy and F1 score. For binary
classiﬁcation, precision is the number of true positives
divided by the total number of predicted positives, i.e.,
true positives plus false positives. Similarly, recall is the
number of true positives divided by the total number of
actual positives, i.e., true positives plus false negatives.
The F1 score is a single number statistical evaluation
metric that measures the accuracy of binary classiﬁca-
tion by taking a weighted average of precision and recall.
It varies between its worst performance value of 0 and
best performance value of 1, and is given by

F1 score = 2

precision × recall
precision + recall

.

(1)

The performance of these metrics on the various test sets
is summarized in Table II. As can be seen in Table II,
deep transfer learning from everyday object classiﬁca-
tion in the ImageNet dataset to morphological classiﬁ-
cation of galaxies in SDSS and DES leads to state of the
art accuracies and F1 scores. Our ﬁne tuned Xception
model attains accuracies ∼> 99% for the HP SDSS and HP
DES test sets. Unlike HP test sets, the FO test sets do
not entirely consist of galaxies with robust ground truth
classiﬁcations. Hence, instead of applying a threshold
on the ground truth probabilities, we apply a conﬁdence
threshold on the predictions of the neural network.
In
Table II we pick the top half most conﬁdent predictions,
for which the neural network attains ∼> 96% accuracies
and F1 scores. Additionally, the accuracies and F1 scores
obtained by applying various diﬀerent thresholds are also

TABLE II: Classiﬁcation accuracy for each test dataset.

Having quantiﬁed the accuracy of our neural network
model on DES test sets that overlap the SDSS foot-
print, we now feed our model with bright, unlabelled
DES galaxies that do not overlap the SDSS footprint,
and predict their classes, thereby labelling them. A ran-
dom sample of high conﬁdence predictions for these is
shown in Figure 9 in C.
Lastly, using our neural network model as a feature ex-
tractor, we obtain the activation maps of the second last
layer and embed them in 3-D using t-SNE. The left and
middle panels of Figure 5 show the output of t-SNE when
applied to the HP SDSS and HP DES test sets. We la-
belled the points using the ground-truth label of each
galaxy, and found that the points neatly cluster into two
groups with accuracies ∼> 99%. For unlabelled DES set,
shown in the right panel of Figure 5, we ﬁnd again that
two distinct clusters are formed. Based on the accuracy
of the FO DES test set, we heuristically know that these
clusters have accuracies ∼> 96% for the top-half most con-
ﬁdent predictions. These results indicate that the neural
network model has extracted the necessary information
from the training dataset to enable t-SNE to clearly iden-
tify two distinct classes of galaxies based on their mor-
phology. A scientiﬁc visualization of this clustering algo-
rithm for the FO DES test set is presented in [41]. The
astute reader may realize that using t-SNE as a visualiza-
tion tool requires training to prevent common misread-
ings of the visualizations. Furthermore, t-SNE does not
always produce similar outputs on successive runs, and
it requires the user to determine a few hyperparamters
related to the optimization process. Much work has been
presented in the literature to ensure that new users make
a proper use of this tool [42], and to automate hyper-
paramter selection [43].
Recursive training Having labeled about 10,000 DES
galaxies with our neural network classiﬁer, we pick the
top 1000 spiral and top 1000 elliptical galaxies. We then
add them to our original SDSS training dataset, and use
deep transfer learning again to re-train the neural net-
work model. The top- and bottom-left panels in Figure 6
show the initial (0th recursion) accuracy of our classiﬁer,
and the accuracy attained once the newly labelled DES
images are added to the SDSS training dataset (1st re-
cursion). We notice that the accuracy for classiﬁcation
for FO SDSS and DES test sets improves up to 1.5%. In
particular, we notice that the classiﬁcation accuracy for

1248163264Number of GPUs050100150200250Total Training Time in minutesSpeed Up vs. Number of GPUs02004006008001000120014001600Images Processed per second7

FIG. 5: t-SNE visualization of the clustering of HP SDSS and DES test sets, and unlabelled DES test.

FIG. 6: Top panels: SDSS datasets. Bottom panels: DES datasets. Accuracy (left panels) and F1 score (right
panels) vs N high conﬁdence predictions as a fraction of total full overlap test datasets (0th recursion). We also
show the improvement in classiﬁcation accuracy and F1 score after 2000 newly labelled DES images are added to
the SDSS training dataset (1st recursion). These results have been obtained by averaging over ten diﬀerent models.

0.20.30.40.50.60.70.80.91.0Fraction of data set86889092949698100Accuracy (%)0th recursion1st recursion0.10.20.30.40.50.60.70.80.91.0Fraction of data set0.860.880.900.920.940.960.981.00f1 score0th recursion1st recursion0.10.20.30.40.50.60.70.80.91.0Fraction of data set86889092949698100Accuracy (%)0th recursion1st recursion0.10.20.30.40.50.60.70.80.91.0Fraction of data set0.860.880.900.920.940.960.981.00f1 score0th recursion1st recursionthe FO DES test set is now boosted up to 98.5% when
50% of the dataset is considered. These results are rather
signiﬁcant from a machine learning perspective [44], since
these accuracies are already high and this newly labelled
DES dataset represents ∼ 5% of the the original SDSS
training dataset. We have also computed ROC curves
(see Figure 10) to provide an additional metric to quan-
tify the improvement in classiﬁcation accuracy due to
recursive training.

Intuitively, recursive training provides the means to
continually enhance the classiﬁcation accuracy of a neu-
ral network as new data becomes available. We have
found that, averaging over ten models, the mean classiﬁ-
cation accuracies do improve when we retrain the model.
This novel approach provides us with the means to en-
hance SDSS galaxy classiﬁcation, as shown in the top left
panel of Figure 6. More importantly, it provides a way
forward to gradually replace SDSS galaxy images in the
training dataset that we need to construct DES galaxy
catalogs at scale. A DES-only image training dataset will
better capture the nature of images observed by DES,
and would also enable us to better use data augmenta-
tions to model the eﬀects of noise, making our neural net-
work model more resilient to accurately classify galaxies
at higher redshift, or that are contaminated by various
sources of noise.

IV. CONCLUSION

We have presented the ﬁrst application of deep trans-
fer learning combined with distributed training for the
classiﬁcation of DES galaxies that overlap the footprint
of the SDSS survey, achieving state-of-the-art accuracies
∼> 99.6%. We described how to use our neural network
classiﬁer to label over 10,000 unlabelled DES galaxies
that had not been observed in previous surveys. By trun-
cating our neural network model, we used it as a feature
extractor, and once combined with t-SNE, we presented
visualizations which show that, through transfer learn-
ing, the neural network abstracted morphological infor-
mation to clearly identify two distinct classes of galaxies
in the unlabeled DES dataset. To get insights into the
inner workings of our clustering algorithm, we have pre-
sented scientiﬁc visualizations of the clustering of the FO
DES test set, which are available at [41] and [45] .

We have also used t-SNE to inspect seemingly incorrect
labels provided by our neural network model, and have
found that these errors actually correspond to inaccurate
human classiﬁcations in our SDSS testing dataset. We
present an example of this nature in the visualization
available at [41], and in Figure 12.

Furthermore, adding the most conﬁdent predictions
from our newly labeled DES galaxies, we have done re-
cursive training, boosting the classiﬁcation accuracy for
the FO SDSS and DES test datasets. Averaging over ten
models, we ﬁnd improved accuracies as high as 99.5% for
SDSS and 99% for DES.

8

This analysis provides a path forward to construct
galaxy catalogs in DES using actual DES galaxies as
training datasets. The combination of deep transfer
learning with distributed training, and recursive train-
ing presents an alternative to do this analysis at scale in
the LSST era.

V. ACKNOWLEDGEMENTS

This research is part of the Blue Waters sustained-
petascale computing project, which is supported by the
National Science Foundation (awards OCI-0725070 and
ACI-1238993) and the State of Illinois. Blue Waters
is a joint eﬀort of the University of Illinois at Urbana-
Champaign and its National Center for Supercomputing
Applications. We acknowledge support from the NCSA.
We thank the NCSA Gravity Group for useful feedback,
and Vlad Kindratenko for granting us access to state-
of-the-art GPUs and HPC resources at the Innovative
Systems Lab at NCSA. We are grateful to NVIDIA for
donating several Tesla P100 and V100 GPUs that we
used for our analysis. We thank the anonymous referee
for carefully reading this manuscript, and providing con-
structive feedback to improve the presentation of our re-
sults.

This work used the Extreme Science and Engineering
Discovery Environment (XSEDE), which is supported by
National Science Foundation grant number ACI-1548562.
Speciﬁcally, it used the Bridges system, which is sup-
ported by NSF award number ACI-1445606, at the Pitts-
burgh Supercomputing Center (PSC). We gratefully ac-
knowledge grant TG-PHY160053. This research used re-
sources of the Argonne Leadership Computing Facility,
which is a DOE Oﬃce of Science User Facility supported
under Contract DE-AC02-06CH11357.

This project used public archival data from the Dark
Energy Survey (DES). Funding for the DES Projects has
been provided by the U.S. Department of Energy, the
U.S. National Science Foundation, the Ministry of Sci-
ence and Education of Spain, the Science and Technology
Facilities Council of the United Kingdom, the Higher Ed-
ucation Funding Council for England, the National Cen-
ter for Supercomputing Applications at the University
of Illinois at Urbana-Champaign, the Kavli Institute of
Cosmological Physics at the University of Chicago, the
Center for Cosmology and Astro-Particle Physics at the
Ohio State University, the Mitchell Institute for Funda-
mental Physics and Astronomy at Texas A&M Univer-
sity, Financiadora de Estudos e Projetos, Funda¸c˜ao Car-
los Chagas Filho de Amparo `a Pesquisa do Estado do
Rio de Janeiro, Conselho Nacional de Desenvolvimento
Cient´ıﬁco e Tecnol´ogico and the Minist´erio da Ciˆencia,
Tecnologia e Inova¸c˜ao, the Deutsche Forschungsgemein-
schaft and the Collaborating Institutions in the Dark En-
ergy Survey.

The Collaborating Institutions are Argonne National
Laboratory, the University of California at Santa Cruz,

9

the University of Cambridge, Centro de Investigaciones
Energ´eticas, Medioambientales y Tecnol´ogicas–Madrid,
the University of Chicago, University College London,
the DES-Brazil Consortium, the University of Edin-
burgh, the Eidgen¨ossische Technische Hochschule (ETH)
Z¨urich, Fermi National Accelerator Laboratory, the Uni-
versity of Illinois at Urbana-Champaign, the Institut
de Ci`encies de l’Espai (IEEC/CSIC), the Institut de
F´ısica d’Altes Energies, Lawrence Berkeley National Lab-
oratory, the Ludwig-Maximilians Universit¨at M¨unchen
and the associated Excellence Cluster Universe, the

University of Michigan, the National Optical Astron-
omy Observatory, the University of Nottingham, The
Ohio State University, the OzDES Membership Con-
sortium, the University of Pennsylvania, the University
of Portsmouth, SLAC National Accelerator Laboratory,
Stanford University, the University of Sussex, and Texas
A&M University. Based in part on observations at Cerro
Tololo Inter-American Observatory, National Optical As-
tronomy Observatory, which is operated by the Associa-
tion of Universities for Research in Astronomy (AURA)
under a cooperative agreement with the National Science
Foundation.

[1] A. G. Riess, A. V. Filippenko, P. Challis, A. Clocchi-
atti, A. Diercks, P. M. Garnavich, R. L. Gilliland, C. J.
Hogan, S. Jha, R. P. Kirshner, B. Leibundgut, M. M.
Phillips, D. Reiss, B. P. Schmidt, R. A. Schommer, R. C.
Smith, J. Spyromilio, C. Stubbs, N. B. Suntzeﬀ, and
J. Tonry, The Astronomical Journal 116, 1009 (1998),
astro-ph/9805201.

[2] S. Perlmutter, G. Aldering, et al., Astrophys. J. 517, 565

(1999), astro-ph/9812133.

[3] J. L. Tonry, B. P. Schmidt, B. Barris, P. Candia, P. Chal-
lis, A. Clocchiatti, A. L. Coil, A. V. Filippenko, P. Gar-
navich, C. Hogan, S. T. Holland, S. Jha, R. P. Kirsh-
ner, K. Krisciunas, B. Leibundgut, W. Li, T. Matheson,
M. M. Phillips, A. G. Riess, R. Schommer, R. C. Smith,
J. Sollerman, J. Spyromilio, C. W. Stubbs, and N. B.
Suntzeﬀ, Astrophys. J. 594, 1 (2003), astro-ph/0305008.
[4] R. A. Knop, G. Aldering, et al., Astrophys. J. 598, 102

(2003), arXiv:astro-ph/0309368 [astro-ph].

[5] M. Doi, M. Fukugita, and S. Okamura, MNRAS 264,

832 (1993).

[6] D. B. Wijesinghe, A. M. Hopkins, B. C. Kelly, N. We-
likala, and A. J. Connolly, MNRAS 404, 2077 (2010),
arXiv:1001.5322 [astro-ph.GA].

[7] D. J. Eisenstein, D. H. Weinberg, E. Agol, H. Ai-
hara, C. Allende Prieto, S. F. Anderson, J. A. Arns,
´E. Aubourg, S. Bailey, E. Balbinot,
and et al., The
Astronomical Journal 142, 72 (2011), arXiv:1101.1529
[astro-ph.IM].

[8] C. J. Conselice, ApJS 147, 1 (2003), astro-ph/0303065.
[9] O. Lahav, A. Naim, R. J. Buta, H. G. Corwin, G. de Vau-
couleurs, A. Dressler, J. P. Huchra, S. van den Bergh,
S. Raychaudhury, L. Sodre, Jr.,
and M. C. Storrie-
Lombardi, Science 267, 859 (1995), astro-ph/9412027.

[10] O. Lahav, A. Naim, L. Sodr´e, Jr., and M. C. Storrie-
Lombardi, MNRAS 283, 207 (1996), astro-ph/9508012.
[11] M. Banerji, O. Lahav, C. J. Lintott, F. B. Ab-
dalla, K. Schawinski, S. P. Bamford, D. Andreescu,
P. Murray, M. J. Raddick, A. Slosar, A. Szalay,
D. Thomas,
and J. Vandenberg, MNRAS 406, 342
(2010), arXiv:0908.2033.

[12] Dark Energy Survey Collaboration et al., MNRAS 460,

1270 (2016), arXiv:1601.00329.

[13] LSST Dark Energy Science Collaboration, ArXiv e-prints

(2012), arXiv:1211.0310 [astro-ph.CO].

[astro-ph.GA].

[15] F. Chollet, ArXiv e-prints , arXiv:1610.02357 (2016),

arXiv:1610.02357 [cs.CV].

[16] D. George, H. Shen, and E. A. Huerta, Phys. Rev. D 97,

101501 (2018).

[17] J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-

Fei, in Proc. CVPR (2009).

[18] C. Tan, F. Sun, T. Kong, W. Zhang, C. Yang,
and C. Liu, arXiv e-prints , arXiv:1808.01974 (2018),
arXiv:1808.01974 [cs.LG].

[19] P. H. Barchi, R. R. de Carvalho, R. R. Rosa, R. Sautter,
M. Soares-Santos, B. A. D. Marques, and E. Clua, arXiv
e-prints , arXiv:1901.07047 (2019), arXiv:1901.07047
[astro-ph.IM].

[20] C. Lintott, K. Schawinski, S. Bamford, A. Slosar,
K. Land, D. Thomas, E. Edmondson, K. Masters, R. C.
Nichol, M. J. Raddick, A. Szalay, D. Andreescu, P. Mur-
ray,
and J. Vandenberg, MNRAS 410, 166 (2011),
arXiv:1007.3265 [astro-ph.GA].

[21] SDSS, “SDSS Skyserver,” (2018), http://skyserver.

sdss.org/dr7/en/tools/search/sql.asp.

[22] W. Gropp, E. Lusk,

and A. Skjellum, Using MPI:
Portable Programming with the Message-Passing Inter-
face (2nd ed.), by William Gropp, Ewing Lusk and An-
thony Skjellum, Scientiﬁc and Engineering Computation
Series, MIT Press, Cambridge, MA, 1999. (1999).
[23] W. Kramer, M. Butler, G. Bauer, K. Chadalavada, and
C. Mendes, in High Performance Parallel I/O, edited by
Prabhat and Q. Koziol (CRC Publications, Taylor and
Francis Group, 2015) pp. 17–32.
[24] DES, “NCSA DESaccess Web,”
deslabs.ncsa.illinois.edu/.

(2018), https://

[25] Keras, “Keras: The Python Deep Learning library,”

(2018), https://keras.io/.

[26] M. Abadi, A. Agarwal, P. Barham, E. Brevdo, Z. Chen,
C. Citro, G. S. Corrado, A. Davis, J. Dean,
and
M. Devin, arXiv preprint arXiv:1603.04467 (2016).
[27] O. Russakovsky, J. Deng, H. Su, J. Krause, S. Satheesh,
S. Ma, Z. Huang, A. Karpathy, A. Khosla, M. Bern-
stein, A. C. Berg,
and L. Fei-Fei, ArXiv e-prints ,
arXiv:1409.0575 (2014), arXiv:1409.0575 [cs.CV].
[28] C. Szegedy, V. Vanhoucke, S. Ioﬀe, J. Shlens,

and
Z. Wojna, ArXiv e-prints , arXiv:1512.00567 (2015),
arXiv:1512.00567 [cs.CV].

[14] H. Dom´ınguez S´anchez, Huertas-Company, et al., ArXiv
e-prints , arXiv:1807.00807 (2018), arXiv:1807.00807

[29] K. He, X. Zhang, S. Ren, and J. Sun, ArXiv e-prints ,
arXiv:1512.03385 (2015), arXiv:1512.03385 [cs.CV].

[30] K. Simonyan and A. Zisserman, ArXiv e-prints
arXiv:1409.1556 (2014), arXiv:1409.1556 [cs.CV].

,

10

[31] S. Kornblith, J. Shlens,

imagenet models

ter
arXiv:1805.08974.

and Q. V. Le, “Do bet-
(2018),

transfer better?”

[32] Andrej Karpathy, “Convolutional Neural Networks for
(2018), http://cs231n.github.

Visual Recognition,”
io/neural-networks-2/.

[33] M. D. Zeiler and R. Fergus, in ECCV (2014).
[34] J. Yosinski, J. Clune, Y. Bengio, and H. Lipson, in Pro-
ceedings of the 27th International Conference on Neu-
ral Information Processing Systems - Volume 2 , NIPS’14
(MIT Press, Cambridge, MA, USA, 2014) pp. 3320–3328.
[35] S. Ackermann, K. Schawinski, C. Zhang, A. K.
and M. D. Turp, MNRAS 479, 415 (2018),

Weigel,
arXiv:1805.10289 [astro-ph.IM].

[36] XSEDE, “The Approach to Bridges,” (2018), https:

//www.psc.edu/bridges.

[37] D. P. Kingma and J. Ba, CoRR abs/1412.6980 (2014).
preprint
and M. D. Balso,
[38] A.

Sergeev

arXiv

arXiv:1802.05799 (2018).

[39] P. Goyal, P. Doll´ar, R. B. Girshick, P. Noordhuis,
L. Wesolowski, A. Kyrola, A. Tulloch, Y. Jia, and K. He,
CoRR abs/1706.02677 (2017), arXiv:1706.02677.
[40] L. van der Maaten and G. Hinton, Journal of Machine

Learning Research 9, 2579 (2008).

[41] NCSA, “Unsupervised learning and data clustering for
the construction of Galaxy Catalogs in the Dark Energy
Survey,” (2018), https://www.youtube.com/watch?v=
n5rI573i6ws.

[42] M. Wattenberg, F. Vi ˜A c(cid:13)gas, and I. Johnson, Distill

(2016), 10.23915/distill.00002.

[43] Y. Cao and L. Wang, arXiv e-prints , arXiv:1708.03229

(2017), arXiv:1708.03229 [cs.AI].

[44] Benchmarks.AI, “MNIST. Classify handwritten digits,”

https://benchmarks.ai/mnist.

[45] NCSA, “Deep transfer

scale for cos-
mology,” (2018), https://www.youtube.com/watch?v=
1F3q7M8QjTQ.

learning at

Appendix A: Neural network architecture

11

The top panel in Figure 7 presents the architecture of the pre-trained Xception model [15] used in these studies.
The bottom panel of Figure 7 shows the fully connected layers, and classiﬁer added at the second-to-last layer of the
pre-trained Xception model. This is labeled as 2048-dimensional vectors in the Exit flow diagram. Following
this procedure, we truncate our neural network classiﬁer and turn it into a feature extractor, which we can use in
combination with t-SNE to do unsupervised clustering. Note that we use t-SNE just as a visual aid. The labelling of
unlabeled DES datasets is done using our neural network classiﬁer.

FIG. 7: Top panel: Xception model [15]. Bottom panel: fully connected layers, and classiﬁer added at the
bottleneck of the pre-trained Xception model.

Appendix B: Data Augmentation

To expose the neural network to a variety of potential scenarios for classiﬁcation, we augment original galaxy images
with random vertical and horizontal ﬂips, random rotations, height and width shifts and zooms, as shown in Figure 8.
The range for random rotations is set up to a maximum of 45◦, i.e. for each iteration of training the neural net sees
the input image rotated randomly between -45◦ and 45◦. Similarly the maximum range for random height and width
shifts, as well as zooming factor is set to 0.3. Note that for each iteration of training all these image transformations
are applied but with random values within the deﬁned ranges.

This approach not only synthetically increases the training dataset, but also makes the neural network invariant to

rotations, shifts, ﬂips and combinations of these, and also introduces scale invariance.

12

FIG. 8: Data augmentations include random rotations of up to 45 degrees, random ﬂips, height and width shifts and
zooms of up to a factor of 1.3

Appendix C: Classiﬁcation predictions for unlabelled DES galaxies

Figure 9 presents high-conﬁdence neural network predictions for unlabelled DES galaxies. The robustness of these
predictions were tested with our unsupervised clustering algorithm, ﬁnding that these classiﬁcations, based on the mor-
phological features extracted from the DES images in three ﬁlters, are meaningful, as shown in the t-SNE projections
in Figure 5.

FIG. 9: Sample of high conﬁdence predictions for spiral (left panel) and elliptical galaxies (right panel) on an
unlabelled DES set.

Appendix D: Scaling Results

In TableIII the training is comprised of three stages: (1) freeze the base model and train added dense layers; (2)
freeze layers 0-39, and train layers 40+; (3) freeze layers 0-1 and train all layers 2+. The number of epochs for each
stage is given in the third column. The total time only includes the time for the training (all three stages), but not the
time for initialization (launching jobs, loading Python modules, data preparation, etc), which we found was minimal
compared to the training time.

13

GPUs

1

2

4

8

16

32

64

Time per epoch (s)
410
922
1626
231
481
830
119
246
427
64
124
214
35
63
109
20
31
53
13
15
27

# epochs
1
11
4
1
6
4
1
5
7
1
6
8
1
4
17
1
6
12
1
5
15

Total time

Accuracy

Val Accuracy

4h 44 m

0.9992

0.9979

1h 47m

0.9993

0.9990

1h 12m

0.9995

0.9990

42m

0.9991

0.9979

36m

0.9993

0.9980

14m

0.9993

0.9990

8m

0.9993

0.9990

TABLE III: Training results and timings using diﬀerent numbers of K80 GPUs for the Xception model. Validation
Accuracy results are given in the ﬁnal column. The training includes three stages: (1) train the dense layers (base
layers are frozen); (2) train layer 40+ (layers 0-39 are frozen); (3) train Layer 2+ (layer 0-1 are frozen). The number
of epochs shown are for each training stage. The batch size is set to be 16. The benchmarks was done on Cooley
supercomputer at Argonne Leadership Computer Facility (https://www.alcf.anl.gov/user-guides/cooley).

FIG. 10: Top panels: FO SDSS test sets. Bottom panels: FO DES test sets.

Appendix E: Recursive Training

In addition to providing results for recursive training using ten diﬀerent models (see Figure 6), herein we also
provide ROC results for a typical model out of our ten samples. These ROC results indicate that recursive training
indeed leads to an increase in classiﬁcation accuracy both for SDSS and DES.

0.00.20.40.60.81.0False Positive Rate0.00.20.40.60.81.0True Positive RateEllipticals0th Recursion (area = 0.94)1st Recursion (area = 0.95)0.00.20.40.60.81.0False Positive Rate0.00.20.40.60.81.0True Positive RateSpirals0th Recursion (area = 0.94)1st Recursion (area = 0.95)0.00.20.40.60.81.0False Positive Rate0.00.20.40.60.81.0True Positive RateEllipticals0th Recursion (area = 0.94)1st Recursion (area = 0.96)0.00.20.40.60.81.0False Positive Rate0.00.20.40.60.81.0True Positive RateSpirals0th Recursion (area = 0.94)1st Recursion (area = 0.96)14

FIG. 11: There are only four misclassiﬁed examples from HP test sets. Of these four images, one is a noise artifact
in the telescope.

FIG. 12: A sample of misclassiﬁcation on FO test sets. The debiased galaxy zoo probabilities used to produce the
ground truth labels for each image are shown. Notice that the debiased galaxy zoo probabilities for each class are
very low and close to each other, i.e., these samples represent low conﬁdence ground truth labels.

Appendix F: Misclassiﬁed Examples

We present a gallery of misclassiﬁed examples from our high probability (HP) test sets. As shown in Figure 11, we
have only four instances of this nature, all from HP DES test set, and one of these corresponds to a noise artifact in
the telescope.

In Figure 12 we present a sample of inaccurate predictions on the full overlap (FO) test sets.

HP DES. Predicted Class: SpiralP_CS: 0.51  |  P_EL: 0.49P_CS: 0.63  |  P_EL: 0.24P_CS: 0.42  |  P_EL: 0.36P_CS: 0.87  |  P_EL: 0.07P_CS: 0.45  |  P_EL: 0.28P_CS: 0.56  |  P_EL: 0.33FO SDSS. Predicted Class: EllipticalP_EL: 0.37  |  P_CS: 0.33P_EL: 0.23  |  P_CS: 0.07P_EL: 0.55  |  P_CS: 0.45P_EL: 0.1  |  P_CS: 0.07P_EL: 0.36  |  P_CS: 0.1P_EL: 0.43  |  P_CS: 0.32FO SDSS. Predicted Class: SpiralP_CS: 0.48  |  P_EL: 0.47P_CS: 0.47  |  P_EL: 0.44P_CS: 0.51  |  P_EL: 0.37P_CS: 0.5  |  P_EL: 0.37P_CS: 0.55  |  P_EL: 0.27P_CS: 0.52  |  P_EL: 0.41FO DES. Predicted Class: EllipticalP_EL: 0.58  |  P_CS: 0.23P_EL: 0.41  |  P_CS: 0.24P_EL: 0.23  |  P_CS: 0.07P_EL: 0.62  |  P_CS: 0.32P_EL: 0.66  |  P_CS: 0.31P_EL: 0.56  |  P_CS: 0.35FO DES. Predicted Class: Spiral