0
2
0
2

l
u
J

4

]

C
O
.
h
t
a
m

[

2
v
8
1
4
2
0
.
3
0
0
2
:
v
i
X
r
a

A Direct Shooting Method is Equivalent to an Indirect Method

I. M. Ross*
Naval Postgraduate School, Monterey, CA 93943

We show that a direct shooting method is mathematically equivalent to an indirect method in the sense of certain
ﬁrst-order conditions. Speciﬁc mathematical formulas pertaining to the equivalence of a direct shooting method
with an indirect method are derived. We also show that a theoretical equivalence does not necessarily translate
to practical equivalence if the parameterized optimal control problem is simply patched to a nonlinear program-
ming solver. A mathematical explanation is provided for the successes and failures of such patched nonlinear
programming methods. In order to generate the correct solution more consistently, the nonlinear programming
solver used in a traditional direct method must be replaced or augmented by a Hamiltonian programming method.
The theoretical results derived in this paper further strengthen the connections between computational optimal
control, deep learning and automatic differentiation.

I.

Introduction
Starting from ﬁrst principles, we show that a direct shooting
method is mathematically equivalent to an indirect method. The math-
ematical equivalence is established in the sense of certain ﬁrst-order
conditions. Given this equivalence, we revisit some widely-held be-
liefs such as the folklore that an “indirect method is more accurate than
a direct method.” In fact, we show that the apparent differences be-
tween direct and indirect methods can be mathematically reconciled by
realizing that a direct method masks its indirect elements. By unmask-
ing the indirect elements of a direct shooting method, we present new
results on convergence and accuracy. The new results provide mathe-
matical formulas that help explain the historical direct-indirect chasm
while simultaneously generating several prescriptions for closing the
apparent gap.

The main contribution of this paper is a recalibration of certain en-
trenched concepts in trajectory optimization methods. In particular, our
theoretical results provide a mathematical explanation for the successes
and failures of methods that patch discretized optimal control problems
to generic nonlinear programming techniques. Far superior perfor-
mance capabilities can he harnessed if the generic nonlinear program-
ming methods are replaced by mathematical programming techniques
that are centered around the covector mapping principle[1]. A ﬁrst-
principles’ approach combined with a new theory on optimization[2] is
used to present the main ideas.

II. A Signature Problem For Analysis

We consider the following “signature” trajectory optimization

problem,

Minimize
Subject to

J[x(·), u(·)] := E

x(tf )

˙x(t) = f (x(t), u(t))
(cid:0)

(cid:1)
(x0, t0, tf ) = (x0, t0, tf )

(1)

(P )






It turns out that this problem is also a fundamental problem in deep
learning[3]. In Problem (P ), the decision variables are the state and
control trajectories denoted by x(·) and u(·) respectively. The pair
[x(·), u(·)] is the system trajectory. We assume one-dimensional state
and control variables; i.e., x ∈ R and u ∈ R. The one-dimensional as-
sumption allows us to eliminate the unnecessary mathematics of cum-
bersome bookkeeping associated with vectors. While such bookkeep-
ing is indeed necessary for practical problems, it is quite unnecessary to
understand the basic principles of trajectory optimization methods[4].
Given a system trajectory, the cost functional J in Problem (P ) is
computed via the endpoint cost function E : x(tf ) 7→ R. For a system
trajectory to be feasible, it must satisfy the nonlinear dynamical equa-
tions ˙x(t) = f (x(t), u(t)) for all time t ∈ [t0, tf ], where t0 and tf
are speciﬁed clock times. The initial state x(t0) is ﬁxed at x0 while the

*Distinguished Professor and Program Director, Control and Optimization

1

ﬁnal state x(tf ) is free. The problem is then to ﬁnd a system trajectory
that minimizes the cost functional.

Suppose a candidate optimal trajectory is computed by a direct
method. If this trajectory is claimed to be optimal, it must satisfy the
necessary conditions for optimality. This argument is worth empha-
sizing because necessary conditions are not optional; i.e., they are in-
deed necessary, a point that is sometimes lost when optimality claims
are made on solutions computed by classical direct methods.

The necessary conditions for Problem (P ) are easily stated in terms

of the Pontryagin Hamiltonian[1] deﬁned by,

H(λ, x, u) := λf (x, u)

(2)

where, λ ∈ R is a costate variable that satisﬁes the adjoint equation,

− ˙λ(t) = ∂xH(λ(t), x(t), u(t)) = λ(t)∂xf (x(t), u(t))

(3)

and the transversality condition,

λ(tf ) = ∂xE(x(tf ))

(4)

By assumption u is unconstrained; hence, at each point along a candi-
date optimal solution, the ﬁrst-order Hamiltonian minimization condi-
tion,

∂uH(λ(t), x(t), u(t)) = 0

(5)

must be satisﬁed. Collecting all relevant equations, we can state the fol-
lowing: If a system trajectory is claimed to be optimal, it must satisfy
the following ﬁrst-order necessary conditions,

˙x(t) = f (x(t), u(t))

− ˙λ(t) = λ(t)∂xf (x(t), u(t))

∂uH(λ(t), x(t), u(t)) = 0

(x0, t0, tf ) = (x0, t0, tf )
λ(tf ) = ∂xE(x(tf ))

(6)

(P λ) 



Remark 1 Strictly speaking, (P λ) is not a deﬁnition of a problem; it is
simply a statement of the necessary conditions. If (P λ) is reframed in
terms of ﬁnding a costate trajectory λ(·) in addition to ﬁnding a system
trajectory, then it may be viewed as deﬁning a differential-algebraic
boundary value problem.

Conventionally, a direct method is framed in terms of solving (P ) and
an indirect method as solving (P λ) [5, 6]. To reexamine this statement
more carefully, we ﬁrst develop discretizations of (P ) and (P λ).

 
 
 
 
 
 
III. Basic Discretizations for Optimization

a forward/backward Euler discretization of (P λ) may be written as,

2

Analysis

We use forward/backward Euler discretizations to reveal certain
fundamental outcomes in optimization. It will be apparent shortly that
the main aspects of our analysis is portable across alternative discretiza-
tions (e.g., higher-order Runge-Kutta methods) because our use of Eu-
ler methods is centered at a fundamental mathematical level. Euler-
speciﬁc aspects are noted so that a reader can independently gauge the
generality of speciﬁc results and/or statements. To this end, consider a
uniform grid of “diameter” h over the time interval [t0, tf ] deﬁned by

h :=

tf − t0
N

, N ∈ N+

(7)

Time is indexed according to t0 = t0 < t1 := t0 + h < t2 :=
t1 + h < · · · < tN = tf . Discretized versions of continuous-time
variables over this grid will be denoted by the same indexing scheme.
For instance, a discretized state trajectory over (7) will be denoted as
(x0, x1, . . . , xN−1, xN ). Replacing ˙x(t) by its forward difference ap-
proximation results in the following algebraic equations as candidate
approximations for the dynamics:

x1 = x0 + hf (x0, u0)

...

xN = xN−1 + hf (xN−1, uN−1)

λ0 = λ1 + hλ1∂xf (x1, u1)

...

λN−2 = λN−1 + hλN−1∂xf (xN−1, uN−1)

(13)

λN−1 = ∂xE(xN )

0 = ∂uH(λ0, x0, u0)

...

0 = ∂uH(λN−1, xN−1, uN−1)

(P λN )






Remark 3 The adjoint equation over the last interval (tN − tN−1) is
missing in (10) because its inclusion would require uN , which does not
exist; see Remark 2. Consequently, there is no λN in (P λN ). Nonethe-
less, we still have an N -dimensional discretized dual variable given by
(λ0, λ1, . . . , λN−1).

x1 = x0 + hf (x0, u0)

...

xN−1 = xN−2 + hf (xN−2, uN−2)
xN = xN−1 + hf (xN−1, uN−1)

(8)

In a conventional direct method, the objective is to solve Prob-
lem (P N ). This optimization problem has 2N variables and N con-
straints. A conventional indirect method involves solving the system of
equations given by (P λN ). This system has 3N variables and 3N con-
straints. Different discretizations generate different versions of (P N )
and (P λN ).

Thus, a forward Euler discretization of Problem (P ) may be expressed
as,

J N (X N , U N ) := E(xN )

Minimize
Subject to x1 = x0 + hf (x0, u0)
x2 = x1 + hf (x1, u1)

...

xN = xN−1 + hf (xN−1, uN−1)

(9)

(P N )






where, X N ∈ RN and U N ∈ RN are deﬁned by,

X N := (x1, x2, . . . , xN ), U N := (u0, u1, . . . , uN−1)

Remark 2 There is no “uN ” in Problem (P N ) because of our use
of a forward Euler method (Cf. (8)). The absence of uN is simply a
characteristic of Radau-type discretizations.

With foresight in mind, we discretize the adjoint equation by a

backward Euler formula,

λ0 = λ1 + hλ1∂xf (x1, u1)

...

λN−2 = λN−1 + hλN−1∂xf (xN−1, uN−1)

In addition, we discretize (5) according to,

∂uH(λ0, x0, u0) = 0

...

∂uH(λN−1, xN−1, uN−1) = 0

Setting

λN−1 = ∂xE(xN )

(10)

(11)

(12)

IV. Development of the Main Proposition
A direct shooting method attempts to solve a trajectory optimiza-
tion problem using two main components: a simulation function and
an optimization module[6]. The task of the simulation function is to
eliminate the dynamics and provide the necessary data functions for
the optimization module. A basic direct shooting method in terms of
solving (P N ) can be described as follows:

1. For any given U N = (u0, u1, . . . , uN−1), generate xN using
the forward Euler method given by (8). This simulation function
generates xN as a function of U N . By an abuse of notation, we
denote this function as xN (U N ).

2. Starting with i = 0, generate the next iterate U N

i+1 according to

the formula,

U N

i+1 = U N

i − αi M

−1 ∂EN
∂U N
i

,

i = 0, 1, . . .

(14)

where i = 0, 1, . . . is the index of the iteration, αi > 0 is the
step-length of the algorithm, EN is E(xN (U N )) and M deter-
mines at least three algorithmic options given by the following
choices[7, 2],

M = Identity matrix
M = Hessian
M = Approximate Hessian

(gradient method)
(Newton’s method)
(quasi-Newton method)

(15)

The dependence of M on U N
i
paper for enhancing the clarity of exposition of the main ideas.

and N is suppressed throughout this

Step 2 of the basic direct shooting method, encapsulated by (14),

is a more granular expression of the problem,

(QN )

Minimize
U N

E

xN (U N )

(16)

(cid:8)

(cid:0)

(cid:1)

That is, Problem (QN ) is a transformation of (P N ) generated as a re-
sult of Step 1, while (14) and (15) are simply providing some pertinent
details in solving (16).

3

Frequently, a direct shooting method is described more abstractly
as follows: Parameterize the controls in terms of known functions
ξ1(t), ξ2(t), . . . and unknown coefﬁcients c1, c2, . . .. This can be ac-
complished in several different ways. A representative parametrization
is given by,

u(t; C) =

cj ξj(t)

(17)

where, C = (c1, c2, . . .). Then, a more generic direct shooting method
may be described as follows:

j
X

Case k = N − 2
From (8) we have

∂xN
∂uN−2

=

=

∂xN−1
∂uN−2
∂xN−1
∂uN−2

+ h∂xf (xN−1, uN−1)

∂xN−1
∂uN−2

1 + h∂xf (xN−1, uN−1)

(cid:16)
= h∂uf (xN−2, uN−2)

(cid:17)
1 + h∂xf (xN−1, uN−1)

(24)

Substituting (24) in (22) we get,

(cid:16)

(cid:17)

1. For any given C, generate x(tf ) by simulating the differential
equation ˙x = f (x, u). In terms of Step 1 of the basic direct
shooting method described earlier, this process generates “xN ”
as a function of C. Denote this function as xN (U N (C)).

∂EN
∂uN−2

= h∂uf (xN−2, uN−2)

λN−1 + hλN−1∂xf (xN−1, uN−1)

= hλN−2∂uf (xN−2, uN−2)

(cid:16)

(cid:17)
(25)

2. With the dynamic constraints in Problem (P N ) eliminated by
the simulation function of Step 1, solve the resulting optimiza-
tion problem given by,

where the last equality in (25) follows from (10).

The cases for k = (N − 3), (N − 2), . . . , 0 all follow similarly
(cid:3)

with additional steps in chain rule and composition.

(QN
C )

Minimize
C

E

xN (U N (C))

(18)

Remark 6 Lemma 1 requires that the adjoint equation be discretized
by the backward Euler formula; see the statement preceding (10).

If (18) is implemented in a manner analogous to (14), it generates the
family of algorithms parameterized by M according to,

(cid:8)

(cid:0)

(cid:1)

Ci+1 = Ci − αi M

−1 ∂EN
∂Ci

,

i = 0, 1, . . .

(19)

It is evident that the computation of the gradient of EN is central
to all variants of the direct shooting methods discussed in the preced-
ing paragraphs. More speciﬁcally, (14) requires the computation of
the gradient of EN with respect to U N while (19) requires this same
information albeit implicitly because

Lemma 2 The gradient of the cost function is equal to the gradient of
the Hamiltonian scaled by the integration step size h:

∂uH(λ0, x0, u0)
...
∂uH(λN−2, xN−2uN−2)
∂uH(λN−1, xN−1, uN−1)

∂EN
∂U N = h 







Proof. This result follow readily from Lemma 1 and (2).

(cid:3)

Proposition 1 A direct shooting method is ﬁrst-order equivalent to an
indirect method.



(26)

∂EN
∂Ci

=

T

∂U N
i
∂Ci (cid:21)

(cid:20)

∂EN
∂U N
i

(20)

Proof. Consider Step 2 of a direct shooting method. In particular,
consider the computation of the gradient of EN . From Lemmas 1 and
2, it follows that the computation of the last row of the left hand side of
(26) is mathematically equivalent to the following steps:

Remark 4 To avoid an excessive use of new symbols, we have used the
notation EN in (14) to imply the composite function E ◦xN : U N 7→ R
in (16). By the same token, EN in (19) is a different composite function
given by E ◦ xN ◦ U N : C 7→ R.

Remark 5 The versatile and popular Program to Simulate Optimize
Simulated Trajectories II (POST2)[8] uses a direct shooting method
with controls parameterized by various known functions and unknown
coefﬁcients analogous to (17). Step 2 in POST2 (i.e., the equivalent
of solving Problem (QN
C )) is implemented in terms of a gradient or a
quasi-Newton method[8].

Lemma 1 Let EN denote the function E ◦ xN : U N 7→ R. Then, for
k = 0, 1, . . . (N − 1),

∂EN
∂uk

= hλk∂uf (xk, uk)

(21)

Proof. By chain rule, we have,

∂EN
∂uk

=

∂E
∂xN

∂xN
∂uk
∂xN
∂uk

= λN−1

1. Compute the gradient of the cost function ∂xE(xN ). Set it
equal to λN−1. This equality follows from the transversality
condition given by (12).

2. Using the result from Step 1, compute the gradient of the Hamil-

tonian at the grid point tN−1 according to the formula,

∂uH(λN−1, xN−1, uN−1) ← λN−1∂uf (xN−1, uN−1)

3. Multiply the result from Step 2 by the integration step size h.

By similar arguments, it follows that computation of the remainder of
the rows of the left hand side of (26) is equivalent to the following
steps:

1. Evaluate the gradient of the cost function ∂xE(xN ) and set it

equal to λN−1 (Cf. (12)).

2. Using the result from Step 1, back-propagate the costate using
the adjoint equation (Cf. (10)) to determine λN−2, . . . , λ0.

3. Using (2), evaluate the gradient of the Hamiltonian at grid points

tN−2, . . . , t0.

(22)

4. Multiply the result from Step 3 by h to compute the gradient of

the cost function.

where the last equality in (22) follows from (12). The remainder of the
proof is broken down to two cases:

The preceding steps are, in fact, the classical steps of an indirect
(cid:3)
method.

Case k = N − 1
From (8) it follows that,

∂xN
∂uN−1

= h∂uf (xN−1, uN−1)

(23)

Thus, from (22) and (23), it follows that the statement of the lemma
holds for k = N − 1.

Remark 7 Equation (26) is a mathematically factual statement.
It
does not necessarily imply a formula to generate the left-hand side of
(26) via its right-hand side as used in the proof of Proposition 1. The
left-hand side of (26) can be generated independently; for instance,
via numerical ﬁnite differencing as in the case of POST2[8]. Regard-
less of how the left-hand side of (26) is produced, Lemma 2 and hence
Proposition 1 holds.

V. New Results on Convergence and Accuracy
Proposition 1 casts serious doubts on a number of historical and
anecdotal claims on the convergence and accuracy of direct and indirect
methods. In order to place these claims in a proper context, we deﬁne
the following notions of algorithms and convergence.

Remark 9 Analyzing the stability of (27) is an open area of research.
This is because EN is a composite function (see Remark 4) that de-
pends on the data functions E and f as well as the type of discretiza-
tion used in generating EN . Furthermore, if EN is generated using an
adaptive grid, then it is also a function of the grid itself.

4

Deﬁnition 1 (Inner Algorithm and Its Convergence) Let A be an
algorithm for solving Problem (P N ). That is, for a ﬁxed N , A gen-
erates a sequence of vector pairs (X N
0 ), (X N
1 ), . . . by the
i ), i ∈ N. Algorithm A is
iterative map (X N
i+1) = A(X N
i+1, U N
said to converge if

0 , U N
i , U N

1 , U N

lim
n→∞

X N

n := X N
∞,

lim
n→∞

U N

n := U N
∞

is an accumulation point that solves Problem (P N ). Furthermore, A
is called an inner algorithm for Problem (P ).

Deﬁnition 2 (Convergence of a Discretization) Let (X N
∞, U N
solution to Problem (P N ). A discretization is said to converge if

∞) be a

X N

∞ := X

∞
∞ ,

lim
N→∞

U N

∞ := U

∞
∞

lim
N→∞

is an accumulation point that solves Problem (P ).

Deﬁnition 3 (Trajectory Optimization Algorithm) Let
an
sequence
generates
(N0, m0), (N1, m1) · · · such that the sequence

algorithm that

of

a

B
integer

be
pairs

(X N0

m0 , U N0

m0 ), (X N1

m1 , U N1
m1 ), . . .
∞
∞
converges to an accumulation point (X
∞ ) given by Deﬁnition 2.
∞ , U
Then, B is called a trajectory optimization algorithm for solving Prob-
lem (P ).

Note that the preceding deﬁnitions are agnostic to a direct shooting
method. Also, for the purposes of brevity, we have taken the liberty of
avoiding the speciﬁcs of a metric for convergence in Deﬁnitions 1, 2
and 3.

Remark 8 Algorithm B may be designed using A. In this design, it
may be advantageous to have A depend upon N ; i.e., different algo-
rithms for different choices of N . For example, the inner algorithm for
N = N0 may need to be robust rather than fast while for N 6= N0, it
may be advantageous to design A that is fast rather than robust. Fur-
thermore, it is possible to design B with A not even executed through
its completion as implied in Deﬁntion 3. Although it is not a direct
method, the spectral algorithm[9, 10, 11] implemented in DIDO[12]
incorporates such a variable inner algorithm to support its guess-free
execution[13] via an algorithm B as given by Deﬁnition 3.

A. Theoretical Analysis
Consider the convergence of a shooting method that uses a gradient-
based algorithm. The convergence of such algorithms can be better
understood by considering (14) to be a forward Euler discretization of
the “continuous-time” dynamical equation given by,

dU N
dτ

= −M

−1 ∂EN
∂U N

(27)

That is, it is apparent that discretizing (27) by a forward Euler method
using a step size αi generates (14). Consequently, the family of algo-
rithms given by (14) is equivalent to a simple forward propagation of
(27); i.e., an initial value problem. Thus, the convergence of (14) can
be separated into two clear subproblems:

1. Stability of the differential equation given by (27), and

2. Stability of the forward-Euler discretization of (27).

If the dynamical system given by (27) is unstable, then its correspond-
ing algorithm will diverge no matter the step size. Thus, a necessary
condition for the convergence of an inner algorithm (see Deﬁnition 1)
is that (27) be stable. Consequently, the full power of dynamical system
theory can be brought to bear in analyzing the stability of (27).

∂EN
∂U N

∂EN
∂C

Now suppose that (27) is stable; then, the “continuous trajectory of the
algorithm” τ 7→ U N converges to some equilibrium point U N
∞. By
deﬁnition, this equilibrium point satisﬁes the condition dU N /dτ = 0;
hence, by (27), if M −1 is nonsingular, U N
∞ satisﬁes the algebraic equa-
tion,

= 0

(28)

U N
(cid:12)
∞
(cid:12)
(cid:12)
Equation (28) is precisely a ﬁrst-order necessary condition for Prob-
(cid:12)
lem (QN ) which is the optimization problem in Step 2 of the basic
direct shooting method; see (16).

Remark 10 If Step 2 of a direct shooting method is implemented to
solve Problem (QN
C ) deﬁned in (18), then the equation corresponding
to (28) is given by

= 0

(29)

C∞
(cid:12)
(cid:12)
Equation (29) does not necessarily imply (28) unless ∂CU N has full
(cid:12)
(cid:12)
rank; see (20). In other words, to avoid spurious solutions, the con-
trol must be parameterized such that ∂ci u(t) must generate a full rank
condition for ∂C U N .

The preceding discussion illustrates how the convergence of algo-
rithms can be analyzed by using well-established theorems related to
the stability of differential equations. The use of differential equations
to analyze convergence of algorithms is not entirely new; it goes as far
back as 1958 to the pioneering work of Gavurin[14]. Recently, these
ideas have reemerged as a new ﬁeld for designing and analyzing opti-
mization algorithms[2, 15, 16, 17, 18].

Suppose that the inner gradient-based algorithm converges to the

equilibrium point U N

∞; then, by Lemma 2 and (28) we have,

∂uH(λk, xk, uk) = 0 at U N

∞ for k = 0, 1, . . . , (N − 1)

(30)

Thus, if the direct shooting method converges (in the sense of Deﬁni-
tion 1), it converges to a point that satisﬁes the discretized “indirect”
ﬁrst-order necessary conditions given by (P λN ); i.e., the same point as
an indirect method. If the discretization converges (see Deﬁnition 2),
then a solution from the direct shooting method will converge to a so-
lution of (P λ). In this theoretical context, the accuracy of a direct
shooting method is the same as an indirect method.

B. Practical Limits
Despite the favorable theoretical results of the preceding paragraphs,
practical experiences can be dramatically different based on their im-
plementations. This is because practical algorithms satisfy conver-
gence conditions only up to some small tolerances; hence, the theo-
retical results must be reanalyzed in this context. To this end suppose
that (28) is satisﬁed to within some ǫ > 0; i.e.,

0 <

∂EN
∂U N

≤ ǫ

(31)

From Lemma 2, it follows that (31) maps to

(cid:12)
(cid:12)
(cid:12)
(cid:12)
|∂uH(λk, xk, uk)| ≤ ǫ/h

∞
(cid:12)
(cid:12)
(cid:12)
(cid:12)

for k = 0, 1, . . . , (N − 1)

(32)

Equations (31) and (32) generates the following theorem that summa-
rizes the conditions and interplay between convergence and accuracy:

Theorem 1 Suppose that the ﬁrst-order optimality condition given by
(28) is satisﬁed to a tolerance of ǫ > 0 (Cf. (31)). Then, the ﬁrst-order
Hamiltonian optimality condition (Cf. (11)) is satisﬁed to a tolerance
of ǫ/h, where h > 0 is the integration step size.

Proof. The proof of this theorem follows as a direct consequence of
(cid:3)
(31) and (32).

5

VI. Resetting Some Facts and Myths About

Direct and Indirect Methods

Historically, direct and indirect methods have been viewed as sep-
arate computational methods with pros and cons in terms of conver-
gence, accuracy, required human-labor etc.[5, 6]. Proposition 1 and
Theorem 1 suggest otherwise. Interestingly, Proposition 1 and Theo-
rem 1 can also help explain the past computational discrepancies be-
tween direct and indirect methods. These points are summarized in the
following subsections.

A. Accuracy
Classical direct methods have been considered inaccurate with respect
to an indirect method. This “fact” can be easily explained via Theo-
rem 1. For example, if a step size of h = 0.01 is used for integration,
then the accuracy of the ﬁrst-order Hamiltonian minimization condi-
tion is given by ǫ/h = 100ǫ, or one-hundred times worse than the
tolerance on the gradient of the cost function! In fact, as the integration
accuracy is increased (i.e., as h → 0 for convergence in the sense of
Deﬁnition 2), the practical satisfaction of the ﬁrst-order Hamiltonian
optimality condition given by (32) worsens because ǫ/h → ∞ for a
ﬁxed value of ǫ. This explains why a direct shooting method has been
considered to be inaccurate with respect to optimality. One simple rem-
edy for this problem is to coordinate the practical convergence of the
algorithm with that of the discretization in accordance with Theorem 1;
i.e., vary ǫ in concert with h so that ǫ/h is as small as desired.

B. Rate of Convergence
Indirect methods are considered to be superior to direct methods in
terms of their rates of convergence. This perception can be explained as
follows: From Lemma 2 and (14), the inner gradient-based algorithm
corresponding to a direct shooting method can be written as,

U N

i+1 = U N

i − αi h M

−1

∂H
∂U N

i (cid:19)

(cid:18)

,

i = 0, 1, . . .

(33)

Hence, if a classical direct method is implemented with a step size of
say, αi = 1, it follows from (33) that this is equivalent to implementing
an indirect method with a step size of h. This fact explains the obser-
vation that as the integration step size is reduced to enhance accuracy,
the observed convergence of a classical direct method drops quite dra-
matically (assuming h < 1). An obvious remedy for this problem is
adjust the step size of the gradient algorithm such that αih = 1. Al-
ternatively, one could scale the gradient of the direct shooting method
by 1/h; however, we caution that scaling an optimal control problem
must be done in a careful manner that ensures that the dual variables
do not get imbalanced. See [19] for further details.

“Radius”/Region of Convergence

C.
A common argument against an indirect method is that it has a small
“radius” of convergence relative to a direct method. This argument
seems to be at loggerheads with Proposition 1. Nonetheless, Theorem 1
together with the emerging new theory of optimization[2] provides the
answer. As noted in the discussions following (27), an optimization
algorithm may be viewed as a discretization of a differential equation.
A necessary condition for the algorithm to converge is that the Euler
step size of its corresponding differential equation be stable. That is, a
smaller step size promotes global convergence, while a larger step size
contributes to instability. This is precisely what occurs in the direct
shooting method when compared to its indirect counterpart. From (33),
it is clear that the effective step size of the direct shooting method is
less than αi if h < 1; hence, it promotes global convergence at the
price of an observed decreased rate of convergence. A simple remedy
to increase the size of the basin of attraction of an indirect method is to
reduce the algorithm step size to bring it on par with that of its direct
counterpart.

D. Human Labor
A common dissatisfaction associated with an indirect method is the re-
ported human-labor in arriving at the necessary conditions. A quick

examination of (P λN ) shows that the development of the necessary
conditions is equivalent to the computation of the gradients/Jacobians.
These computations can be automated to the point of completely elim-
inating human labor[1, 20]. In fact, the gradients/Jacobians need not
even be computed symbolically/analytically from the point of view of
human-labor intensity. Furthermore, when a software that purports
to be a direct method requires Jacobian information, it is effectively
asking a user to provide the ﬁrst-order necessary conditions. In other
words, the historical dissatisfaction of an indirect method in terms of
human-labor is more a reﬂection of an architecture of the software
(and its sophistication or lack thereof) rather than the fundamentals
of the trajectory optimization method.

E. Challenges and Failures of Adaptive Grids

All of the results presented so far apply to a uniform grid. Extending
the analysis to an arbitrary grid requires new mathematical results that
are beyond the scope of this paper. It is also an open area of research.
Nonetheless, it is possible to show rather quickly why an adaptive grid
causes failures in a direct shooting method. To this end, let πN =
[t0, t1, . . . , tN ] be a nonuniform grid. Then, xN not only depends upon
U N but also on πN ; hence, under the caveat noted in Remark 4, we set

EN ≡ E(xN (U N , πN ))

Under the same notational caveat, we set,

πN ≡ πN (U N )

(34)

(35)

to denote an adaptive grid; i.e., the grid πN is changed over the course
of one or more iteration cycles. Using (34) and (35), the gradient of the
endpoint cost function may be written as,

∂EN
∂U N =

=

∂EN
∂xN  
∂EN
∂xN

∂xN
∂U N +

∂xN
∂U N +

∂πN
∂U N

T

(cid:20)
∂EN
∂xN (cid:20)

(cid:21)
∂πN
∂U N

∂xN
∂πN

!

T

(cid:21)

∂xN
∂πN

(36)

S

{z

|

}

where, S symbolizes the second term on the right-hand-side of (36).
This term is zero in Lemma 1 and Theorem 1. Taking S into account
requires speciﬁc details of adaptation. Even under an assumption of
these details, the computation of Jacobian of πN may not be an easy
task.

If S is not taken into account in a direct shooting method that uses
a variable step size, then an inner algorithm based solely on the ﬁrst
term of the right-hand-side of (36) will have the wrong gradient infor-
mation. This is precisely the source of the problem in shooting meth-
ods based on variable step sizes. In fact, practitioners have long cau-
tioned on the use of variable step size integrators because of “noise” in
the gradients[6]. Equation (36) provides a more precise statement of
“noise” in terms of the second term on the right-hand-side of (36).

Remark 11 Adaptive grids also create new issues such as the intro-
duction of unnecessary dynamics and unaccounted feedback loops that
may destabilize the resulting algorithm for solving even a simple prob-
lem. See [19] for details.

From (36) and the analysis presented in [19], it is evident that a bet-
ter choice for an adaptive grid is to vary it with respect to N as part
of algorithm B while decoupling its variation with respect to the inner
algorithm A (see Deﬁnitions 1 and 3). This point was also observed in
[21] as part of the outcomes of numerical experimentations with adap-
tive pseudospectral grids; however, its cause at that time was unknown.
Evidently, a more rigorous reexamination of several entrenched con-
cepts in trajectory optimization is warranted.

VII. The Case For Hamiltonian Programming

Based on the analysis presented in Section VI, it is clear that if
Theorem 1 is violated, then a direct shooting method exhibits all the
properties reported in the literature. A simple remedy to avoid the re-
ported problems is to not violate Theorem 1. There are several ways to
comply with Theorem 1 some of which are outlined in Section VI. Any
method that complies with Theorem 1 incorporates its indirect counter-
part; hence, we call such methods Hamiltonian programming (even if
a Hamiltonian is not used explicitly).

A.

Introduction to Hamiltonian Programming

We use the term Hamiltonian programming in a much broader sense
than complying with the speciﬁcs of Theorem 1. To elaborate this
point, note that Proposition 1 is essentially a statement of a covec-
tor mapping theorem for a shooting method. That is, Proposition 1
may be construed as an application of the covector mapping principle
(CMP)[1] which basically states that there exists a connection between
a direct and an indirect method. As noted in Ross et al. [19], this con-
nection is a direct consequence of the well-known Hahn-Banach theo-
rem[22]. To illustrate an application of the Hahn-Banach theorem and
the resulting CMP more concretely, consider a linear ordinary differen-
tial equation (ODE),

˙x = Ax

We can always associate with (37) a “shadow” ODE given by,

− ˙ψ = AT ψ

(37)

(38)

That is, (38) exists from the mere fact that (37) exists. If (37) is non-
linear, then (38) still exists with A replaced by the Jacobian of the
right-hand-side of the nonlinear ODE. This follows from the following
“Hahn-Banach-Hamiltonian algorithm:”

1. Given an ODE, ˙x = f (x), construct a scalar function H ac-

cording to the formula,

H(ψ, x) := ψT f (x)

(39)

where ψ ∈ RNx is some auxiliary vector that has the same
dimension as x ∈ RNx .

2. Construct an auxiliary differential equation using H from Step 1

by performing the following operation,

− ˙ψ := ∂xH(ψ, x) =

T

ψ

∂f
∂x

(cid:18)

(cid:19)

(40)

From (39), it follows that the ODE ˙x = f (x) can also be written as
˙x = ∂ψH(ψ, x); hence, any differential equation may be viewed as
“half” of a Hamiltonian system given by:

˙x = ∂ψH(ψ, x)
− ˙ψ = ∂xH(ψ, x)

(⇒ ˙x = f (x))

(shadow ODE)

(41a)

(41b)

Alternatively, any differential equation ˙x = f (x) may be Hamiltoni-
anized according to (41). In this context, Proposition 1 should not be
a surprise in the sense that it is simply providing the “missing” details
of the other half of the Hamiltonian system associated with a direct
shooting method. This is how a CMP equates a direct method with an
equivalent indirect method[1]. Once such a constructive equivalence
is established, an algorithm for a direct method can be modiﬁed (i.e.,
Hamiltonianized) so that it converges to a solution of its indirect coun-
terpart. Consequently, it is unnecessary, or even improper, to clas-
sify trajectory optimization methods as direct or indirect. See also
Sec. VI.D.

6

B. Hamiltonian vs Nonlinear Programming
In constructing numerical methods for solving ODEs, one can, in prin-
ciple, ignore its dual counterpart (i.e., the shadow ODE given by (41b)).
Ignoring the shadow ODE in computational optimal control is tan-
tamount to ignoring 50% of the differential equations that deﬁne the
system! In this context, it should not at all be surprising that if a method
is constructed that takes into account 100% of the differential equations
it will most certainly perform better than one that accounts for only half
the equations. One can go even further and state that if a method is con-
structed that accounts for only 50% of its ODEs, then it may even be
incorrect. A Hamiltonian programming method accounts for 100% of
the ODEs.

A classical indirect method is obviously Hamiltonian: it “directly”
accounts for 100% of the governing ODEs. A classical direct method
only accounts for 50% of the ODEs. Nonetheless, a direct method can
be Hamiltonianized by (“indirectly”) incorporating the missing 50% of
the ODEs (Cf. Theorem 1). In incorporating these ideas with those pre-
sented in Sec. VI.D, it follows that trajectory optimization is fundamen-
tally Hamiltonian while nonlinear programming is Lagrangian. Thus,
the many issues with classical direct methods reported in the literature
(see Sec. VI) can now be framed simply as a result of non-Hamiltonian
programming.

Under this backdrop, we declare Problem (P N ) to be a Hamilto-
nian programming problem rather than a nonlinear programming prob-
lem. Such a declaration at the starting point immediately avoids the
many pitfalls associated with the preconceptions of nonlinear program-
ming. In this context, we note the following:

1. A Hamiltonian is not a Lagrangian. Nonlinear programming
methods are based on Lagrangians. Hamiltonian programming
methods are centered on Hamiltonians.

2. A nonlinear programming method is agnostic to the differences
between (discretized) state and control variables. A Hamilto-
nian programming method treats state and control variables dif-
ferently and in accordance with their respective dependencies
(mapping) to a Hamiltonian.

3. A Hamiltonian programming method treats a dynamic con-
straint differently than a path constraint.
In sharp contrast,
collocation-based nonlinear programming methods for optimal
control treat all constraints the same way (except to exploit lin-
earity/sparsity).

4. The concept of time and its “hidden convexity”[23] is absent in
a nonlinear programming method. For instance, the constancy
of a Hamiltonian cannot be derived from the necessary con-
ditions for Problem (P N ) if it treated as a nonlinear program-
ming problem unless each point on the grid πN is considered an
optimization variable in an open subinterval.

Deﬁning Problem (P N ) to be a Hamiltonian programming problem
is in line with labeling special mathematical programming problems
as subjects deserving their own monikers, theory and algorithms. Ex-
amples are integer programming or linear programming. Furthermore,
even special nonlinear programming problems (e.g. cone and quadratic
programming problems) are treated very differently than generic ones.
Thus, for example, if a convex optimization problem is solved using
generic nonlinear programming methods, it can certainly be successful
albeit in a limited sense. Because the limitations of such an approach
does not imply convex programming problems are hard, the same is
true when generic nonlinear programming methods are used to solve
Problem (P N ). Consequently, the more suitable method for solving
Problem (P N ) is to use Theorem 1 to expose its hidden Hamiltonian
structure and subsequently design Hamiltonian algorithms that exploit
the Hamiltonian conditions. As evident from the proofs of Proposi-
tion 1 and Theorem 1, these aspects of mathematical programming
are well beyond the apparent sparsity or linearity in viewing (P N ) as
a special nonlinear programming problem.

Discretizing a generic optimal control problem naturally gener-
ates a matrix-vector Hamiltonian programming problem rather than
a “sparse” nonlinear programming problem[4].
the
rows of the matrix in this formulation have a separable-programming
property[7] that can be exploited for fast and efﬁcient computation[12].

Furthermore,

7

C. Growing Literature on Hamiltonian Programming
A recognition of the fact that discretizing optimal control problems
may generate mathematical programming problems that are outside
the reach of nonlinear programming theory is not new. It goes back
to the early days of pseudospectral[24, 25] and “orthogonal” collo-
cation methods[26]. Collocation methods for optimal control[26, 27]
are similar to pseudospectral knotting methods[28, 29] although these
two methods were developed independently with applications to prob-
lems in chemical engineering and aerospace guidance respectively.
In the methods advanced by Biegler et al[26, 30, 31] and Ross et
al[11, 12, 25, 24], certain Hamiltonian-type conditions are incorporated
in generating candidate solutions. Emerging new ideas incorporate ad-
ditional Hamiltonian conditions in several different ways whose discus-
sion is beyond the scope of this paper; see [1, 4, 11, 19, 30, 31, 32, 33]
for details. An instantiation of some of these ideas are implemented
in the software packages DynoPC[27] and DIDO[12] which can re-
portedly solve “hard” optimal control problems that cannot be solved
by non-Hamiltonian programming methods[30, 31, 33]. In particular,
DIDO interfaces with a user similar to the technicalities of a traditional
direct method but also outputs all of the additional dual-space infor-
mation (e.g. Hamiltonian) even though a user does not supply any
necessary conditions. This is because, as noted in Section VI.D, all
of the human labor in the development of necessary conditions can
indeed be eliminated by automation. Furthermore, because DIDO im-
plements a sequential Hamiltonian programming algorithm[12] in the
sense of Deﬁnition 3, it is robust (in a new Lyapunov sense[2]) relative
to variations in the staring point of the algorithm; i.e., a guess. Elastic
programming[9, 13] furthers the robustness property of DIDO to the
extent that it runs without a user-supplied guess[13] while continuing
to automatically output necessary Hamiltonian conditions similar to an
indirect method. In other words, DIDO is a “proof of existence” of the
statement that “direct = indirect” methods for trajectory optimization
provided that the generic nonlinear programming method is replaced
or augmented by Hamiltonian programming techniques.

VIII. A Brief Historical Perspective on The

Direct-Indirect Chasm

Although Section VI explains the mathematical origins of the
direct-indirect divide through a narrow prism of direct shooting meth-
ods, it is instructive to view the broader chasm from a historical per-
spective. To set the stage for a brief historical review, we recall from
Section V that an optimization method may be considered as a dis-
cretization of a “continuous-time” dynamical system whose conver-
gence may be analyzed in terms of Lyapunov stability[2, 16]. A tra-
jectory optimization problem generates functions for an intermediate
optimization problem (see Deﬁnition 1). If a trajectory optimization
method that produces these functions generates an unstable dynami-
cal system for optimization, then the resulting inner algorithm will di-
verge. For instance, for the basic direct shooting method, properties of
the generated function EN : U N 7→ R that deﬁne Problem (QN ) (see
(16)) drive the attributes of the inner optimization algorithm.
If the
continuous-time dynamical system for optimization (Cf. (27)) is sta-
ble, then the resulting algorithm may still diverge if the step size of the
“discrete” algorithm is too large. An analysis of trajectory optimiza-
tion methods using these new ideas is an open area of research.

Historically, trajectory optimization methods were generated as di-
rect and direct methods based on continuous-time considerations[5, 6]
and not from the perspective of the arguments presented in this paper.
For example, it is straightforward to show[34, 35] that the ﬁrst variation
δJ for Problem (P ) can be written as,

δJ =

tf

t0 (cid:18)

Z

∂H
∂u

(cid:19)

δu dt

(42)

where δu is the variation of u(·) such that all other constraints are
satisﬁed. Hence, if δu is selected as,

δu = −γ

∂H
∂u

(cid:18)

(cid:19)

,

γ > 0

(43)

it follows that (42) reduces to

tf

δJ = −γ

2

dt ⇒ δJ ≤ 0

(44)

∂H
∂u

t0 (cid:18)
Z

(cid:19)
Thus, δJ decreases with the selection of (43) and vanishes if ∂uH
vanishes. Consequently, (43) generates the variational equivalent of a
gradient method which can be discretized to generate a computational
method according to,

δuk = −γ

∂H
∂u

(cid:18)

(cid:19)k

,

k ∈ N

(45)

Likewise a second-order method may be obtained by pre-multiplying
the gradient by an appropriate Hessian[34]. Although these ideas are
well-founded, they mask the higher-level of granularity offered by (26).
To amplify this statement, consider multiplying both sides of (26) by
δuk to generate,

∂EN
∂uk

k
X

δuk =

h ∂uHkδuk

(46)

k
X

Equation (46) may also be viewed as a discrete analog of (42). Thus,
one can derive (46) by the alternative process of discretizing (42). Us-
ing (46), one can “guess” the result given by (26). That is, although
(46) does not imply (26), the result could have been guessed correctly
starting from (42). This is precisely the reason why Lemma 2 is a more
granular expression of the connection between the ﬁrst variation and
the gradient of the Hamiltonian. Consequently, if (26) were to have
been guessed by discretizing (42), then the problems with the non-
Hamiltonian implementation of a classical direct method could have
been anticipated (as outlined in Section VI) and the historical chasm
between direct and indirect methods might have been averted. Un-
fortunately, once the direct-indirect narrative took hold, the “corporate
knowledge” got passed on through the generations and frequently re-
peated and cited in subsequent papers as “fact.” As noted previously,
a re-analysis of some well-known narratives might be warranted in
light of the arguments presented in this paper and the emerging new
ideas on optimization itself[2].

As a matter of completeness, we note that a special case of (46) is
implied in the textbook by Bryson and Ho[34], page 45, Eq. (2.2.9).
This special case corresponding to a “multi-stage decision process”
governed by the difference equation, x(k + 1) = f (x(k), u(k)) is
equivalent to (8) with an Euler step size of h = 1. Substituting h = 1
in (26), we get,

∂EN
∂uk

=

∂H
∂uk

(47)

Equation (47) is noted in passing in Ref. [34] (see page 45). It is ap-
parent that the ramiﬁcations of a “missing” h in (47) is nontrivial in the
context that (26) is able to explain the myths discussed in Section VI.
Nonetheless, because the special case of h = 1 renders the two gra-
dients in (47) equal to one another, it takes on an extremely impor-
tant role as a foundational equation for both back propagation[36] in
Deep Learning and automatic differentiation in the so-called “reverse
mode”[37].

IX. Conclusions

Historically, trajectory optimization methods have been classiﬁed
as either direct or indirect. Under mild assumptions, a direct shooting
method is mathematically equivalent to an indirect method up to cer-
tain ﬁrst-order conditions. The covector mapping principle provides
a more general mathematical framework for equating a direct method
to some indirect method. Because the equivalence is fundamental, the
purported differences between direct and indirect methods must be rel-
egated to a historical footnote.

The mathematical equivalence between a direct method and some
indirect method does not necessarily translate to computational equiv-
alence if a direct method is implemented without any consideration of
the Hamiltonian structure inherent in a trajectory optimization prob-
lem. In fact, the mathematical results presented in this paper explain

how and why standard nonlinear programming implementations of di-
rect methods can fail in terms of convergence and accuracy. Arguably,
this is part of the reason why nonlinear programming methods for solv-
ing optimal control problems might indeed be hard. A remedy for such
failures is conceptually simple: implement a direct method in a manner
that incorporates the results generated by its corresponding indirect el-
ements. Despite the simplicity of the preceding statement, its practical
realization involves a production of a double-inﬁnite sequence of pri-
mal vectors that must be coordinated with their dual counterparts such
that they converge separately with respect to their discretation and al-
gorithmic maps. In this context, a vast swath of research area in
trajectory optimization remains unexplored and is a wide open area
of research.

The signature trajectory optimization problem considered in this
paper is also a fundamental problem in deep learning (sans the regular-
ization term). If a step size is used as an additional learning parame-
ter, then Theorem 1 (and Hamiltonian programming methods) can be
exploited for enhancing the accuracy, convergence and computational
speed of the learning algorithms.

In recent years, there has been a substantial growth in the num-
ber of ostensibly new trajectory optimization methods and software.
A large number of these tools simply rely on patching discretiza-
tion/integration methods to nonlinear programming (NLP) solvers.
Typically, the NLP solvers are third-party routines that may not allow
an insertion of Hamiltonian programming principles. The purported
success achieved by such patched methods is more of a testament to
the sophistication of the NLP solvers rather than a new approach to
optimizing trajectories. When the patched tools are unable to solve
certain problems, it may not be that these problems are computation-
ally hard; rather, it may be more likely a result of non-Hamiltonian pro-
gramming. Hamiltonian programming techniques are able to overcome
many of the reported difﬁculties in solving previously-troublesome op-
timal control problems. The current challenges in trajectory optimiza-
tion are in entirely new and unexplored areas of research in theory and
computation.

References
[1] I. M. Ross, A Primer on Pontryagin’s Principle in Optimal Con-
trol, Second Edition, Collegiate Publishers, San Francisco, CA,
2015.

[2] I. M. Ross, “An Optimal Control Theory for Nonlinear Optimiza-

tion,” J. Comp. and Appl. Math., 354 (2019) 39–51.

[3] Q. Li, L. Chen, C. Tai and W. E, “Maximum principle based algo-
rithms for deep learning,” Journal of Machine Learning Research,
18/165 (2018), 1–29.

[4] I. M. Ross and R. J. Proulx, “Further Results on Fast Birkhoff
Pseudospectral Optimal Control Programming,” J. Guid. Control
Dyn. 42/9 (2019), 2086–2092.

[5] O. von Stryk and R. J. Bulirsch, “Direct and Indirect Methods
for Trajectory Optimization,” Annals of Operations Research, 37
(1992), 357–373.

[6] J.T. Betts, “Survey of numerical methods for trajectory optimiza-

tion,” J. Guid. Control Dyn. 21 (1998), 193–207.

[7] P. E. Gill, W. Murray and M. H. Wright, Practical Optimization,

Academic Press, London, 1981.

[8] R. A. Lugo et al, “Launch Vehicle Ascent Trajectory Using
The Program to Optimize Simulated Trajectories II (POST2),”
AAS/AIAA Spaceﬂight Mechanics Meeting, San Antonio, TX,
2017, AAS 17-274.

8

[10] I. M. Ross and M. Karpenko, “A Review of Pseudospectral Opti-
mal Control: From Theory to Flight,” Annual Reviews in Control,
Vol.36, No.2, pp.182–197, 2012.

[11] Q. Gong, I. M. Ross and F. Fahroo, “Spectral and Pseudospectral
Optimal Control Over Arbitrary Grids,” Journal of Optimization
Theory and Applications, vol. 169, no. 3, pp. 759-783, 2016.

[12] I. M. Ross, “Enhancements to the DIDO© Optimal Control Tool-

box,” arXiv preprint (2020), arXiv:2004.13112.

[13] I. M. Ross and Q. Gong, “Guess-Free Trajectory Optimization,”
AIAA/AAS Astrodynamics Specialist Conference and Exhibit, 18-
21 August 2008, Honolulu, Hawaii. AIAA 2008-6273.

[14] M. K. Gavurin, “Nonlinear functional equations and continuous
analogues of iteration methods,” Izvestiya Vysshikh Uchebnykh
Zavedenii. Matematika, 5 (1958) 18–31(English Translation).

[15] W. Su, S. Boyd, E. J. Candes, A differential equation for modeling
Nesterov’s accelerated gradient method: theory and insights, J.
machine learning research, 17 (2016) 1–43.

[16] B. Polyak, P. Shcherbakov, “Lyapunov functions: an optimiza-
tion theory perspective,” IFAC PapersOnLine, 50-1 (2017) 7456–
7461.

[17] B. Shi, S. S. Du, M. I. Jordan, and W. J. Su, Understanding the ac-
celeration phenomenon via high-resolution differential equations,
arXiv preprint (2018) arXiv:1810.08907.

[18] I. M. Ross, “An Optimal Control Theory for Accelerated Opti-

mization,” arXiv preprint (2019) arXiv:1902.09004.

[19] I. M. Ross, Q. Gong, M. Karpenko and R. J. Proulx, “Scaling
and Balancing for High-Performance Computation of Optimal
Controls,” Journal of Guidance, Control and Dynamics, Vol. 41,
No. 10, 2018, pp. 2086–2097.

[20] E. Trelat, “Optimal Control and Applications to Aerospace: Some
Results and Challenges,” Journal of Optimization Theory and Ap-
plications, September 2012, Volume 154, Issue 3, pp. 713758.

[21] I. M. Ross, F. Fahroo and J. Strizzi, “Adaptive Grids for Tra-
jectory Optimization by Pseudospectral Methods,” AAS/AIAA
Spaceﬂight Mechanics Conference, Paper No. AAS 03-142,
Ponce, Puerto Rico, 9-13 February 2003.

[22] T. Tao, An Epsilon of Room, Vol. I, American Mathematical So-

ciety, Providence, RI, 2010, Ch. 1.

[23] B. S. Mordukhovich and I. Shvartsman, “The Approximate Maxi-
mum Principle in Constrained Optimal Control,” SIAM Journal of
Control and Optimization, Vol. 43, No. 3, 2004, pp. 1037–1062.

[24] I. M. Ross and F. Fahroo, “Legendre Pseudospectral Approxima-
tions of Optimal Control Problems,” Lecture Notes in Control and
Information Sciences, Vol. 295, SpringerVerlag, New York, 2003,
pp. 327342.

[25] I. M. Ross and F. Fahroo, “Discrete Veriﬁcation of Necessary
Conditions for Switched Nonlinear Optimal Control Systems,”
Proceedings of the American Control Conference, June 2004,
Boston, MA.

[26] L. T. Biegler, A. M. Cervantes and A. W¨achter, “Advances in si-
multaneous strategies for dynamic process optimization,” Chem-
ical Engineering Science, Volume 57, Issue 4, 2002, Pages 575-
593.

[27] Y.-D. Lang, L.T. Biegler, “A software environment for simultane-
ous dynamic optimization,” Computers and Chemical Engineer-
ing 31 (2007) 931942.

[9] Q. Gong, F. Fahroo and I. M. Ross, “Spectral Algorithm for Pseu-
dospectral Methods in Optimal Control,” Journal of Guidance,
Control, and Dynamics, vol. 31 no. 3, pp. 460-471, 2008.

[28] I. M. Ross and F. Fahroo, “Pseudospectral Knotting Methods for
Solving Optimal Control Problems,” Journal of Guidance, Con-
trol and Dynamics, Vol. 27, No. 3, pp. 397-405, 2004.

9

[29] Q. Gong and I. M. Ross, “Autonomous Pseudospectral Knotting
Methods for Space Mission Optimization,” Advances in the As-
tronatuical Sciences, Vol. 124, 2006, AAS 06-151, pp. 779–794.

[30] W. Chen, Z. Shao and L. T. Biegler, “A Bilevel NLP Sensitivity-
based Decomposition for Dynamic Optimization with Moving Fi-
nite Elements,” AIChE J., 2014, 60, 966–979.

[31] W. Chen and L. T. Biegler, “Nested Direct Transcription Op-
timization for Singular Optimal Control Problems,” AIChE J.,
2016, 62, 3611–3627.

[32] L. Ma, Z. Shao, W.Chen and Z. Song, “Trajectory optimization
for lunar soft landing with a Hamiltonian-based adaptive mesh
reﬁnement strategy,” Advances in Engineering Software Volume
100, October 2016, pp. 266–276.

[33] H. Marsh, M. Karpenko and Q. Gong, “A Pseudospectral Ap-
proach to High Index DAE Optimal Control Problems,” arXiv
preprint 2018, arXiv:1811.12582.

[34] A. E. Bryson and Y.-C Ho, Applied Optimal Control, Hemisphere

Publishing Corporation, New York, 1969. Chapter 2.

[35] D. E. Kirk, Optimal Control Theory: An Introduction, Prentice-

Hall Inc, Englewood Cliffs, NJ, 1970. Ch. 6.

[36] S. E. Dreyfus, “Artiﬁcial Neural Networks, Back Propagation,
and the Kelly-Bryson Gradient Procedure,” J. Guidance and Con-
trol, 1990, 13/5, pp. 926–928.

[37] A. Griewank, “Who Invented the Reverse Mode of Differenti-
ation?” Documenta Mathematica, Extra Volume ISMP, 2012,
pp. 389–400.

