A Reinforcement Learning Approach for Dynamic Information Flow
Tracking Games for Detecting Advanced Persistent Threats

Dinuka Sahabandu, Shana Moothedath, Member, IEEE, Joey Allen,
Linda Bushnell, Fellow, IEEE, Wenke Lee, Senior Member, IEEE, and Radha Poovendran, Fellow, IEEE

1
2
0
2

n
u
J

8
2

]

C
O
.
h
t
a
m

[

2
v
6
7
0
0
0
.
7
0
0
2
:
v
i
X
r
a

Abstract—Advanced Persistent Threats (APTs) are stealthy,
sophisticated, and long-term attacks that threaten the security
and privacy of sensitive information. Interactions of APTs with
victim system introduce information ﬂows that are recorded in
the system logs. Dynamic Information Flow Tracking (DIFT)
is a promising detection mechanism for detecting APTs. DIFT
taints information ﬂows originating at system entities that are
susceptible to an attack, tracks the propagation of the tainted
ﬂows, and authenticates the tainted ﬂows at certain system com-
ponents according to a pre-deﬁned security policy. Deployment
of DIFT to defend against APTs in cyber systems is limited
by the heavy resource and performance overhead associated
with DIFT. Effectiveness of detection by DIFT depends on the
false-positives and false-negatives generated due to inadequacy of
DIFT’s pre-deﬁned security policies to detect stealthy behavior
of APTs. In this paper, we propose a resource efﬁcient model for
DIFT by incorporating the security costs, false-positives, and
false-negatives associated with DIFT. Speciﬁcally, we develop
a game-theoretic framework and provide an analytical model
of DIFT that enables the study of trade-off between resource
efﬁciency and the effectiveness of detection. Our game model is
a nonzero-sum, inﬁnite-horizon, average reward stochastic game.
Our model
incorporates the information asymmetry between
players that arises from DIFT’s inability to distinguish malicious
ﬂows from benign ﬂows and APT’s inability to know the locations
where DIFT performs a security analysis. Additionally, the game
has incomplete information as the transition probabilities (false-
positive and false-negative rates) are unknown. We propose a
multiple-time scale stochastic approximation algorithm to learn
an equilibrium solution of the game. We prove that our algorithm
converges to an average reward Nash equilibrium. We evaluated
our proposed model and algorithm on a real-world ransomware
dataset and validated the effectiveness of the proposed approach.

Index Terms—Advanced Persistent Threats (APTs), Dynamic
Information Flow Tracking (DIFT), Stochastic games, Average
reward Nash equilibrium, Reinforcement learning

I. INTRODUCTION

Advanced Persistent Threats (APTs) are emerging class of
cyber threats that victimize governments and organizations
around the world through cyber espionage and sensitive infor-
mation hijacking [1], [2]. Unlike ordinary cyber threats (e.g.,
malware, trojans) that execute quick damaging attacks, APTs
employ sophisticated and stealthy attack strategies that enable
unauthorized operation in the victim system over a prolonged
period of time [3]. End goal of an APT typically aims to

D. Sahabandu, S. Moothedath, L. Bushnell, and R. Poovendran are
with the Department of Electrical and Computer Engineering, University
of Washington, Seattle, WA 98195, USA. {sdinuka, sm15, lb2,
rp3}@uw.edu.

J. Allen and W. Lee are with the College of Computing, Georgia Institute
of Technology, Atlanta, GA 30332 USA. jallen309@gatech.edu,
wenke@cc.gatech.edu.

sabotage critical infrastructures (e.g., Stuxnet [4]) or exﬁltrate
sensitive information (e.g., Operation Aurora, Duqu, Flame,
and Red October [5]). APTs follow a multi-stage stealthy
attack approach to achieve the goal of the attack. Each stage
of an APT is customized to exploit set of vulnerabilities in
the victim to achieve a set of sub-goals (e.g., stealing user
credentials, network reconnaissance) that will eventually lead
to the end goal of the attack [6]. The stealthy, sophisticated and
strategic nature of APTs make detecting and mitigating them
challenging using conventional security mechanisms such as
ﬁrewalls, anti-virus softwares, and intrusion detection systems
that heavily rely on the signatures of malware or anomalies
observed in the benign behavior of the system.

Although APTs operate in stealthy manner without inducing
any suspicious abrupt changes in the victim system,
the
interactions of APTs with the system introduce information
ﬂows in the victim system. Information ﬂows consist of data
and control commands that dictate how data is propagated
between different system entities (e.g., instances of a computer
program, ﬁles, network sockets) [7], [8]. Dynamic Information
Flow Tracking (DIFT) is a mechanism developed to dynam-
ically track the usage of information ﬂows during program
executions [7], [9]. Operation of DIFT is based on three core
steps. i) Taint (tag) all the information ﬂows that originate
from the set of system entities susceptible to cyber threats
[7], [9]. ii) Propagate the tags into the output information ﬂows
based on a set of predeﬁned tag propagation rules which track
the mixing of tagged ﬂows with untagged ﬂows at the different
system entities. iii) Verify the authenticity of the tagged ﬂows
by performing a security analysis at a subset of system entities
using a set of pre-speciﬁed tag check rules. When a tagged
(suspicious) information ﬂow is veriﬁed as malicious through
a security check, DIFT makes use of the tags of the malicious
information ﬂow to identify victimized system entities of
the attack and reset or delete them to protect the system.
Since information ﬂows capture the footprint of APTs in the
victim system and DIFT allows tracking and inspection of
information ﬂows, DIFT has been recently employed as a
defense mechanism against APTs [10], [11].

Tagging and tracking information ﬂows in a system using
DIFT adds additional resource costs to the underlying system
in terms of memory and storage. In addition,
inspecting
information ﬂows demands extra processing power from the
system. Since APTs maintain the characteristics of their
malicious information ﬂows (e.g., data rate, spatio-temporal
patterns of the control commands) close to the characteristics
of benign information ﬂows [12] to avoid detection, pre-
deﬁned security check rules of DIFT can miss the detection

 
 
 
 
 
 
of APTs (false-negatives) or raise false alarms by identifying
benign ﬂows as malicious ﬂows (false-positives). Typically,
the number of benign information ﬂows exceeds the number
of malicious information ﬂows in a system by a large factor.
As a consequence, DIFT incurs a tremendous resource and
performance overhead to the underlying system due to fre-
quent security checks and false-positives. The high cost and
performance degradation of DIFT can be worse in large scale
systems such as servers used in the data centers [11].

There has been software-based design approaches to reduce
the resource and performance cost of DIFT [9], [13]. How-
ever, widespread deployment of DIFT across various cyber
systems and platforms is heavily constrained by the added
resource and performance costs that are inherent to DIFT’s
implementation and due to false-positives and false-negatives
generated by DIFT [14], [15]. An analytical model of DIFT
need to capture the system level interactions between DIFT
and APTs, and cost of resources and performance overhead
due to security checks. Additionally, false-positives and false-
negatives generated by DIFT also need to be considered while
deploying DIFT to detect APTs.

In this paper we consider a computer system equipped with
DIFT that is susceptible to an attack by APT and provide
a game-theoretic model that enables the study of trade-off
between resource efﬁciency and effectiveness of detection
of DIFT. Strategic interactions of an APT to achieve the
malicious objective while evading detection depends on the
effectiveness of the DIFT’s defense policy. On the other
hand, determining a resource-efﬁcient policy for DIFT that
maximize the detection probability depends on the nature of
APT’s interactions with the system. Non-cooperative game
theory provides a rich set of rules that can model the strategic
interactions between two competing agents (DIFT and APT).
The contributions of this paper are the following.

• We model the long-term, stealthy, strategic interactions
between DIFT and APT as a two-player, nonzero-sum,
inﬁnite-horizon stochastic game. The
average reward,
proposed game model captures the resource costs asso-
ciated with DIFT in performing security analysis as well
as the false-positives and false-negatives of DIFT.

• We provide, a reinforcement learning-based algorithm,
RL-ARNE, that learns an average reward Nash Equilib-
rium of the game between DIFT and APT. RL-ARNE is
a multiple-time scale algorithm that extends to K-player,
non-zero sum, average reward, unichain stochastic games.
• We prove the convergence of RL-ARNE algorithm to an

average reward Nash equilibrium of the game.

• We evaluate the performance of our approach via an ex-
perimental analysis on ransomware attack data obtained
from Reﬁnable Attack INvestigation (RAIN) [13].

A. Related Work

Stochastic games introduced by Shapley generalize the
Markov decision processes to model the strategic interactions
between two or more players that occur in a sequence of
stages [16]. Dynamic nature of stochastic games enables
the modeling of competitive market scenarios in economics

[17], competition within and between species for resources in
evolutionary biology [18], resilience of cyber-physical systems
in engineering [19], and secure networks under adversarial
interventions in the ﬁeld of computer/network science [20].

Study of stochastic games is often focused on ﬁnding a
set of Nash Equilibrium (NE) [21] policies for the players
such that no player is able to increase their respective payoffs
by unilaterally deviating from their NE policies. The payoffs
of a stochastic game are usually evaluated under discounted
or limiting average payoff criteria [22], [23]. Discounted
payoff criteria, where future rewards of the players are scaled
down by a factor between zero and one, is widely used in
analyzing stochastic games as an NE is guaranteed to exist
for any discounted stochastic game [24]. Limiting average
payoff criteria considers the time-average of the rewards
received by the players during the game [23]. The existence
of an NE under limiting average payoff criteria for a general
stochastic game is an open problem. When an NE exists, value
iteration, policy iteration, and linear/nonlinear programming
based approaches are proposed in the literature to ﬁnd an NE
[22], [25]. These approaches, however, require the knowledge
of transition structure and the reward structure of the game.
Also, these solution approaches are only guaranteed to ﬁnd an
exact NE only in special classes of stochastic games, such as
zero-sum stochastic games, where rewards of the players sum
up to zero in all the game states [22].

Multi-agent reinforcement learning (MARL) algorithms are
proposed in the literature to obtain NE policies of stochastic
games when the transition probabilities of the game and
reward structure of the players are unknown. In [26] authors
introduced two properties, rationality and convergence, that
are necessary for a learning agent to learn a discounted NE
in MARL setting and proposed a WOLF-policy hill climbing
algorithm which is empirically shown to converge to an NE.
Q-learning based algorithms are proposed to compute an
NE in discounted stochastic games [27] and average reward
stochastic games [28]. Although the convergence of these
approaches are guaranteed in the case of zero-sum games,
convergence in nonzero-sum games require more restrictive
assumptions on the game, such as existence of an unique NE
[27]. Recently, a two-time scale algorithm to compute an NE
of a nonzero-sum discounted stochastic game was proposed in
[29] where authors showed the convergence of algorithm to
an exact NE of the game. However, designing reinforcement
learning (RL)-based algorithms with provable convergence
guarantee for computing average reward Nash-equilibrium in
non-zero sum, stochastic games remains an open problem.

Various game-theoretic models including deterministic,
stochastic, and limited-information security games have been
studied to model the interaction between malicious attackers
and defenders of networked systems in [30]. Stochastic games
have been used to analyze security of computer networks in
the presence of malicious attackers [31], [32]. In [33], authors
modeled an attacker/defender problem as a multi-agent non-
zero sum game and proposed a RL algorithm (friend or foe
Q-learning) to solve the game. Adversarial multi-armed bandit
and Q-learning algorithms were combined to solve a spatial
attacker/defender discounted Stackelberg game in [34]. Game-

theoretic frameworks were proposed in the literature to model
interaction of APTs with the system through a deceptive APT
in [35] and a mimicry attack in [36].

Our prior works used game theory to model

the inter-
action of APTs and a DIFT-based detection system [37],
[38], [39], [40], [41], [42], [43]. The game models in [37],
[38], [39] are non-stochastic as the notions of false alarms
and false-negatives are not considered. Recently, a stochastic
model of DIFT-games was proposed in [40], [41] when the
transition probabilities of the game are known. However,
the transition probabilities, which are the rates of generation
of false alarms and false negatives at the different system
components, are often unknown. In [42], the case of unknown
transition probabilities was analyzed and empirical results
to compute approximate equilibrium policies was presented.
In the conference version of this paper [43] we considered
discounted DIFT-game with unknown transition probabilities
and proposed a two-time scale RL algorithm that converges
to NE of the discounted game.

B. Organization of the Paper

Section II presents the formal deﬁnitions and existing
results. Section III provides system and defender models.
Section IV formulates the stochastic game between DIFT and
APT. Section V analyzes the necessary and sufﬁcient condi-
tions required to characterize the equilibrium of DIFT-APT
game. Section VI presents a RL based algorithm to compute
an equilibrium of DIFT-APT game. Section VII provides an
experimental study of the proposed algorithm on a real-world
attack dataset. Section VIII presents the conclusions.

II. FORMAL DEFINITIONS AND EXISTING RESULTS

A. Stochastic Games

A stochastic game G is deﬁned as a tuple < K, S, A, P, r >,
where K denotes the number of players, S represents the
state space, A := A1 × . . . , ×AK denotes the action space,
P designates the transition probability kernel, and r represents
the reward functions. Here S and A are ﬁnite spaces. Let
Ak := ∪s∈SAk(s) be the action space of the game correspond-
ing to each player k ∈ {1, . . . , K}, where Ak(s) denotes the set
of actions allowed for player k at state s ∈ S. Let πππ k be the set
of stationary policies corresponding to player k ∈ {1, . . . , K}
in G. Then a policy πk ∈ πππ k is said to be a deterministic
stationary policy if πk ∈ {0, 1}|Ak| and said to be a stochastic
stationary policy if πk ∈ [0, 1]|Ak|. Let P(s(cid:48)|s, a1, . . . , aK) be
the probability of transitioning from state s ∈ S to a state
s(cid:48) ∈ S under set of actions (a1, . . . , aK), where ak ∈ Ak(s)
denotes the action chosen by player k at the state s. Further
let rk(s, a1, . . . , aK, s(cid:48)) be the reward received by the player k
when state of the game transitions from states s to s(cid:48) under
set of actions (a1, . . . , aK) of the players at state s.

B. Average Reward Payoff Structure

Let π = (π1, . . . , πK). Then deﬁne ρk(s, π) to be the average
reward payoff of player k when the game starts at an arbitrary
state s ∈ S and the players follow their respective policies π.

Let st and at
player k at time t, respectively. Then ρk(s, π) is deﬁned as

k be the state of game at time t and the action of

(1)

K)],

1, . . . , at

1
T + 1

Es,π [rk(st , at

ρk(s, π) = lim inf
T →∞

T
∑
t=0
where the term Es,π [rk(st , at
1, . . . , at
K)] denotes the expected
reward at time t when the game starts from a state s and
the players draw a set of actions (at
K) at current state
st based on their respective policies from π. All the players in
G aim to maximize their individual payoff values in Eqn. (1).
Let −k be the opponents of a player k ∈ {1, . . . , K} (i.e.,
−k := {1, . . . , K}\k). Then let π−k := {π1, . . . , πK}\πk denotes
a set of stationary policies of the opponents of player k.
Equilibrium of G under average reward criteria is given below.

1, . . . , at

1 , . . . , π ∗

Deﬁnition II.1 (ARNE). A set of stationary policies π ∗ =
(π ∗
K) forms an Average Reward Nash Equilibrium
(ARNE) of G if and only if ρk(s, π ∗
k , π ∗
−k), for
all s ∈ S, πk ∈ πππ k and k ∈ {1, . . . , K}.

−k) ≥ ρk(s, πk, π ∗

1 , . . . , π ∗

A policy π ∗ = (π ∗

K) is referred to as an ARNE policy
of G. When all the players follow ARNE policy, no player k
is able to increase its payoff value by unilaterally deviating
from its respective ARNE policy π ∗
k .

C. Unichain Stochastic Games

Deﬁne P(π) to be the transition probability structure of
G induced by a set of deterministic player policies π. Note
that P(π) is a Markov chain formed in the state space S.
Assumption II.2 presents a condition on P(π).

Assumption II.2. Induced Markov chain (MC) P(π) cor-
responding to every deterministic stationary policy set π
contains exactly one recurrent class of states.

Assumption II.2 imposes a structural constraint on the MC
induced by deterministic stationary policy set. Here, the single
recurrent class need not necessarily contain all s ∈ S. There
may exist some transient states in P(π). Also note that any
G that satisﬁes Assumption II.2 can have multiple recurrent
classes in P(π) under some stochastic stationary policy set
π. Stochastic games that satisfy Assumption II.2 are referred
to as unichain stochastic games. In a special case where the
recurrent class contains all the states in the state space, G is
referred as an irreducible stochastic game [22].

Let Rl and T denote a set of states in the lth recurrent
class of the induced MC P(π) for l ∈ {1, . . . , L}, and a set
of transient states in P(π), respectively, where L denote the
number of recurrent classes. Proposition II.3 gives results on
the average reward values of the states in each Rl and T.

Proposition II.3 ([22], Section 3.2). The following statements
are true for any induced MC P(π) of G.

1) For l ∈ {1, . . . , L} and for all s ∈ Rl, ρk(s, π) = ρ l
k denotes a real-valued constant.

each ρ l

k, where

2) ρk(s, π) =

ql(s)ρ l

k, if s ∈ T, where ql(s) is the proba-

L
∑
l=1

bility of reaching a state in lth recurrent class from s.

1) in Proposition II.3 implies that the average reward payoff
of player k takes the same value ρ l
k for each state in the lth
recurrent class. 2) suggests that the average reward payoff
of a transient state is a convex combination of the average
k , . . . , ρ L
payoff values corresponding to L recurrent classes ρ 1
k .
Proposition II.3 shows that for any G, the average reward
payoffs corresponding to each state solely depends on the
average reward payoffs of the recurrent classes in P(π).

D. ARNE in Unichain Stochastic Games

Existence of an ARNE for nonzero-sum stochastic games
is open. However, the existence of ARNE is shown for some
special classes of stochastic games [22].

Proposition II.4 ([23], Theorem 2). Consider a stochastic
game that satisﬁes Assumption II.2. Then there exists an ARNE
for the stochastic game.

Let πk ∈ πππ k be expressed as πk = [πk(s)]s∈S, where πk(s) =
¯a := (a1, . . . , aK) and a−k :=
P(s(cid:48)|s, ¯a)π−k(s, a−k),

[πk(s, ak)]ak∈Ak(s). Further let
¯a\ak. Deﬁne P(s(cid:48)|s, ak, π−k) = ∑

a−k∈A−k(s)

∑
a−k∈A−k(s)

where P(s(cid:48)|s, ¯a) is the probability of transitioning to a state
s(cid:48) from state s under action set ¯a. Also let rk(s, ak, π−k) =
P(s(cid:48)|s, ¯a)rk(s, ¯a, s(cid:48))π−k(s, a−k), where rk(s, ¯a, s(cid:48))
∑
s(cid:48)∈S
is the reward for player k under action set ¯a when a state
transitions from s to s(cid:48). Then a necessary and sufﬁcient con-
dition for characterizing an ARNE of a stochastic game that
satisﬁes Assumption II.2 is given in the following proposition.

Proposition II.5 ([23], Theorem 4). Under Assumption II.2,
a set of stochastic stationary policies (π1, . . . , πK) forms an
ARNE in G if and only if (π1, . . . , πK) satisﬁes,
ρk(s, π) + vk(s) = rk(s, ak, π−k) +∑
s(cid:48)∈S
for all s ∈ S, ak ∈ Ak(s), k ∈ {1, . . . K},

P(s(cid:48)|s, ak, π−k)vk(s(cid:48)) + λ s,ak

(2a)

k

ρk(s, π) − µ s,ak

k = ∑
s(cid:48)∈S

P(s(cid:48)|s, ak, π−k)ρk(s(cid:48))

∑
s∈S

∑
k∈{1,...,K}
λ s,ak
k ≥ 0, µ s,ak

(λ s,ak

for all s ∈ S, ak ∈ Ak(s), k ∈ {1, . . . K},
∑
ak∈Ak(s)
k ≥ 0, πk(s, ak) ≥ 0

k + µ s,ak

)πk(s, ak) = 0.

k

(2b)

(2c)

for all s ∈ S, ak ∈ Ak(s), k ∈ {1, . . . K},

(2d)

πk(s, ak) = 1 for all s ∈ S, k ∈ {1, . . . K},

∑
ak∈Ak(s)
where vk(s) is the “value” of the game for player k at s ∈ S.

(2e)

E. Stochastic Approximation Algorithms

Let h : Rmz → Rmz be a continuous function of a set of
parameters z ∈ Rmz. Then Stochastic Approximation (SA)
algorithms solve a set of equations of the form h(z) = 0 based
on the noisy measurements of h(z). The classical SA algorithm
takes the following form.

zn+1 = zn + δ n

z [h(zn) + wn

z ], for n ≥ 0

(3)

Here, n denotes the iteration index and zn denote the
estimation of z at nth iteration of the algorithm. The terms
z and δ n
wn
z represent the zero mean measurement noise asso-
ciated with zn and the step-size of the algorithm, respectively.
the stationary points of Eqn. (3) coincide with
Note that
the solutions of h(z) = 0 when the noise term wn
is zero.
z
Convergence analysis of SA algorithms requires investigating
their associated Ordinary Differential Equations (ODEs). The
ODE form of the SA algorithm in Eqn. (3) is given in Eqn. (4).

˙z = h(z)

(4)

Additionally, the following assumptions on δ n
to guarantee the convergence of an SA algorithm.

z are required

Assumption II.6. The step-size δ n
∞
∑
n=0

z )2 = 0.

(δ n

z satisﬁes,

∞
∑
n=0

δ n
z = ∞ and

Few examples of δ n
that satisfy the conditions given in
z
Assumption II.6 are δ n
z = 1/n and δ n
z = 1/n log(n). A con-
vergence result that holds for a more general class of SA
algorithms is given below.

Proposition II.7. Consider an SA algorithm in the following
form deﬁned over a set of parameters z ∈ Rmz and a contin-
uous function h : Rmz → Rmz .

zn+1 = Θ(zn + δ n

z + κ n]), for n ≥ 0,

z [h(zn) + wn
where Θ is a projection operator that projects each zn iterates
onto a compact and convex set Λ ∈ Rmz and κ n denotes a
bounded random sequence. Let the ODE associated with the
iterate in Eqn. (5) is given by,

(5)

˙z = ¯Θ(h(z)),

(6)

Θ(z+ηh(z))−z
η

and ¯Θ denotes a projection
where ¯Θ(h(z)) = lim
η→0
operator that restricts the evolution of ODE in Eqn. (6) to
the set Λ. Let the nonempty compact set Z denotes a set of
asymptotically stable equilibrium points of Eqn. (6).

Then zn converges almost surely to a point in Z as n → ∞

given the following conditions are satisﬁed,

1) δ n

z satisﬁes the conditions in Assumption II.6.

(cid:18)

(cid:12)
¯n
(cid:12)
∑
(cid:12)
(cid:12)
l=n

sup
¯n>n

z wn
δ n
z

(cid:12)
(cid:19)
(cid:12)
(cid:12)
(cid:12)

κ n = 0 almost surely.

2) lim
n→∞
3) lim
n→∞

= 0 almost surely.

Consider a class of SA algorithms that consist of two
interdependent iterates that update on two different time scales
(i.e., step-sizes of two iterates are different in the order of
magnitude). Let x ∈ Rmx and y ∈ Rmy and n ≥ 0. Then the
iterates given in the following equations portray a format of
such two-time scale SA algorithm.

xn+1 = xn + δ n
yn+1 = yn + δ n

x [ f (xn, yn) + wn
x],
y [g(xn, yn) + wn
y].

(7)
(8)

The following proposition provides a convergence result

related to the aforementioned two-time scale SA algorithm.

Proposition II.8 ([44], Theorem 2). Consider xn and yn
iterates given in Eqns. (7) and (8), respectively. Then, given the
iterates in Eqns. (7) and (8) are bounded, {(xt , yt )} converges
to (ψ(y∗), y∗) almost surely under the following conditions.

1) f : Rmx+my → Rmx and g : Rmx+my → Rmy are Lipschitz.
2) Iterates xn and yn are bounded.
3) Let ψ : y → x. For all y ∈ Rmy , the ODE ˙x = f (x, y)
has an asymptotically stable critical point ψ(y) such that
function ψ is Lipschitz.

4) The ODE ˙y = g(ψ(y), y) has a global asymptotically

stable critical point.

, . . . , w0

, . . . , w0

x, wn−1
y

5) Let ξ n be an increasing σ -ﬁeld deﬁned by ξ n :=
y). Fur-
x and
x|ξ n] = 0,
x (cid:107)2 |ξ n] ≤ κx(1+ (cid:107) xn (cid:107) + (cid:107) yn (cid:107)),

σ (xn, . . . , x0, yn, . . . , y0, wn−1
x
ther let κx and κy be two positive constants. Then wn
y are two noise sequences that satisfy, E[wn
wn
E[wn
and E[(cid:107) wn
x and δ n
tionally, lim
n→∞

y (cid:107)2 |ξ n] ≤ κy(1+ (cid:107) xn (cid:107) + (cid:107) yn (cid:107)).
y satisfy conditions in Assumption II.6. Addi-

y|ξ n] = 0, E[(cid:107) wn

6) δ n

= 0.

δ n
y
δ n
x

sup

III. SYSTEM AND DEFENDER MODELS
In this section we detail the concept of information ﬂow

graph and the details on the DIFT defender model.

A. Information Flow Graph

Information Flow Graph (IFG), G = (VG , EG ), is a repre-
sentation of the computer system, where the set of nodes,
VG = {u1, . . . , uN} depicts the N distinct components of the
computer and the set of edges EG ⊂ VG × VG represents
the feasibility of transferring information ﬂows between the
components. Speciﬁcally, an edge ei j ∈ EG indicates that an
information ﬂow can be transferred from a component ui to
another component u j, where i, j ∈ {1, . . . , N} and i (cid:54)= j. Let
E ⊂ VG be the set of entry points used by A to inﬁltrate the
computer system. Consider an APT attack that consists of M
attack stages and let D j ⊂ VG for each j ∈ {1, . . . , M} be the
set of components that are targeted by the APT in the jth
attack stage. Let D j be the set of destinations of stage j.

B. DIFT Defender Model

DIFT tags/taints all the information ﬂows originating from
the set of entry points as suspicious ﬂows. Then DIFT tracks
the propagation of the tainted ﬂows through the system and
initiates security analysis at speciﬁc components of the system
to detect the APT. Performing security analysis incurs memory
and performance overheads to the system which varies across
the system components. The objective of DIFT is to select a set
of system components for performing security analysis while
minimizing the memory and performance overhead. On the
other hand, the objective of APT is to evade detection by DIFT
and successfully complete the attack by sequentially reaching
at least one node from each set D j, for all j = 1, . . . , M.

IV. PROBLEM FORMULATION: DIFT-APT GAME
In this section, we model the interactions between a DIFT-
based defender (D) and an APT adversary (A) as a two-
player stochastic game (DIFT-APT game). The DIFT-APT
game unfolds in the inﬁnite time horizon t ∈ T := {1, 2, . . .}.

A. State Space and Action Space

1, . . . , s j

Let S := {s0} ∪ {VG × {1, . . . , j}} = {s0, s j

N}, for all
j ∈ {1, . . . , M}, represent the ﬁnite state space of DIFT-APT
game. The state s0 represents the reconnaissance stage of
the attack where APT chooses an entry point of the system
to launch the attack. Therefore, at time t = 0, DIFT-APT
game starts from s0. A state s j
i denotes a tagged information
ﬂow at a system component ui ∈ VG corresponding to the
jth attack stage. Also, note that a state s j
i , where ui ∈ D j
and j ∈ {1, . . . , M − 1}, is associated with APT achieving the
intermediate goal of stage j. Moreover, a state sM
i , where
ui ∈ DM, represents APT achieving the ﬁnal goal of the attack.
Let N (s) be the set of out-neighboring states of state
s ∈ S. Let Ak = ∪s∈SAk(s) be the action space of the player
k ∈ {D, A}, where Ak(s) denotes the set of actions allowed
for player k at a state s. The action sets of the players
at any state s ∈ S is given by AD(s) ∈ N (s) ∪ {0} and
AA(s) ∈ N (s) ∪ {∅}. Here, AD(s) ∈ N (s) and AD(s) = 0
denote DIFT deciding to perform security analysis at an
out-neighboring state and deciding not to perform security
analysis, respectively. Also, AA(s) ∈ N (s) represents APT
deciding to transition to an out-neighboring state of s ∈ S and
∅ represents APT quitting the attack. At each step of the game
DIFT and APT simultaneously choose their respective actions.
Speciﬁcally, there are four cases. (i) s = s0, AA(s) = {s1
i :
ui ∈ E } and AD(s) = 0. Here, APT selects an entry point in
the system to initiate the attack. (ii) {s = s j
i : ui (cid:54)∈ D j, j =
1, . . . , M}, AA(s) = {s j
i(cid:48) : (ui, ui(cid:48)) ∈ EG } ∪ {∅} and AD(s) ∈
{s j
i(cid:48) : (ui, ui(cid:48) ) ∈ EG } ∪ {0}. In other words, APT chooses to
transition to one of the out-neighboring node of ui in stage j
or decides to quit the attack (∅) and DIFT decides to perform
security analysis at an out-neighboring node of ui in stage j or
not. (iii) {s = s j
and
AD(s) = 0. That is, APT traverses from stage j of the attack
to stage j + 1 and DIFT does not perform a security analysis.
(iv) {s = sM
: si ∈ DM}. Then, AA(s) = s0 which captures the
i
persistency of the APT attack and AD(s) = 0.

i : ui ∈ D j, j = 1, . . . , M − 1}, AA(s) = s j+1

i

Note that DIFT does not perform security analysis at
the states corresponding to s0 and destinations due to the
following reasons. At the entry points there are not enough
traces to perform security analysis as attack originates at these
system components. The destinations D j, for j ∈ {1, . . . , M},
typically consist of busy processes and/or conﬁdential ﬁles
with restricted access. Performing security analysis at states
corresponding to entry points and destinations is not allowed.

B. Policies and Transition Structure

Let st be the state of the game at time t ∈ T . Consider
stationary policies for DIFT and APT, i.e., decisions made at
a state st ∈ S at any time t only depends on st . Let πππ D and πππ A
be the set of stationary policies of DIFT and APT, respectively.
Then stochastic stationary policies of DIFT and APT are
deﬁned by πk ∈ [0, 1]|Ak|, where πk ∈ πππ k and k ∈ {D, A}.
Moreover, let πk = [πk(s)]s∈S and πk(s) = [πk(s, ak)]ak∈Ak(s),
where πk(s) and πk(s, ak) denote the policy of a player
k ∈ {D, A} at a state s ∈ S and probability of player k choosing
an action ak ∈ Ak(s) at the state s. In what follows, we use

ak = d when k = D and ak = a when k = A to denote an action
of DIFT and APT at a state s, respectively.

Assume state transitions are stationary, i.e., state at time
t + 1, st+1 depends only on the current state st and the actions
at and dt of both players at the state st , for any t ∈ T . Let
P be the transition structure of the DIFT-APT game. Then
P(πD, πA) represents the state transition matrix of the game
resulting from (πD, πA) ∈ (πππ D, πππ A). Then,

P(πD, πA) = (cid:2)P(s(cid:48)|s, πD, πA)(cid:3)

s,s(cid:48)∈S , where

P(s(cid:48)|s, πD, πA)= ∑

d∈AD(s)

∑
a∈AA(s)

P(s(cid:48)|s, d, a)πD(s, d)πA(s, a). (9)

Here P(s(cid:48)|s, d, a) denotes the probability of transitioning to
state s(cid:48) from state s when DIFT chooses an action d ∈ AD(s)
and APT chooses an action a ∈ AA(s). Let FN(s j
i ) denote the
rate of false negatives generated at a system component ui ∈ VG
while analyzing a tagged ﬂow corresponding to stage j of the
attack. Then for a state st , actions dt and at the possible next
state st+1 are as follows,
s j
i , w.p 1,
i , w.p FN(s j
s j
i ),
s0, w.p 1 − FN(s j
s j
i , w.p 1,
s0, w.p 1,

when dt = 0 and at = s j
i
when dt = at = s j
i
i ), when dt = at = s j
when dt (cid:54)= at
when at = ∅.




st+1 =



i

(10)
In the ﬁrst case of Eqn. (10), the next state of the game is
uniquely deﬁned by the action of APT as DIFT does not
perform security analysis. In the second and thrid cases of
Eqn. (10), DIFT decides correctly to perform security analysis
on the malicious ﬂow. Note that the security analysis of DIFT
can not accurately detect a possible attack due to generation of
false negatives. Hence the next state of the game is determined
by the action of APT (in case two) when a false negative is
generated. And the next state of the game is s0 (in case three)
when APT is detected by DIFT and APT starts a new attack.
Case four of Eqn. (10) represents DIFT performing security
analysis on a benign ﬂow. In such a case, the state of the game
is uniquely deﬁned by the action of the adversary. Finally, in
case ﬁve of Eqn. (10), i.e., when APT decides to quit the
attack, the next state of the game is the initial state s0.

False negatives of the DIFT scheme arise from the lim-
itations of the security rules that can be deployed at each
node of the IFG (i.e., processes and objects in the system).
Such limitations are due to variations in the number of rules
and the depth of the security analysis1 (e.g., system call level
trace, CPU instruction level trace) that can be implemented at
each node of the IFG resulting from the resource constraints
including memory, storage and processing power imposed by
the system on each IFG node.

1Detecting an unauthorized use of tagged ﬂow crucially depends on the

path traversed by the information ﬂow [8], [9].

C. Reward Structure

Let rD(s, πD, πA) and rA(s, πD, πA) be the expected reward of
DIFT and APT at a state s ∈ S under policy pair (πD, πA) ∈
(πππ D, πππ A). Then for each k ∈ {D, A},
rk(s, πD, πA) =∑
s(cid:48)∈S

P(s(cid:48)|s, d, a)πD(s, d)πA(s, a)rk(s, d, a, s(cid:48)),

∑
a∈AA(s)
d∈AD(s)



rD(s, d, a, s(cid:48)) =

where rk(s, d, a, s(cid:48)) denotes the reward of player k when state
transition from s to s(cid:48) under actions d ∈ AD(s) and a ∈ AA(s)
of DIFT and APT, respectively. Moreover, rD(s, d, a, s(cid:48)) and
rA(s, d, a, s(cid:48)) are deﬁned as follows.
α j
D + CD(s)
β j
D
σ j
D + CD(s)
σ j
D
CD(s)
0
α j
β j
σ j
0

if d = a, s(cid:48) = s0
if d = 0, s(cid:48) ∈ {s j
if d (cid:54)= 0, a = ∅
if d = 0, a = ∅
if d (cid:54)= a and d (cid:54)= 0
otherwise
if d = a, s(cid:48) = s0
if s(cid:48) ∈ {s j
if a = ∅
otherwise







rA(s, d, a, s(cid:48)) =

i : ui ∈ D j}

i : ui ∈ D j}

A

A

A

The reward structure rD(s, d, a, s(cid:48)) captures the cost of false
positive generation by assigning a cost CD(s) whenever d (cid:54)= a
such that d (cid:54)= 0. Note that, rD(s, d, a, s(cid:48)) consists of four
components (i) reward term α j
D > 0 for DIFT detecting the
APT in jth stage (ii) penalty term β j
D < 0 for APT reaching
a destination of stage j, for j = 1, . . . , M (iii) reward σ j
D > 0
for APT quitting the attack in jth stage and (iv) a security
cost CD(s) < 0 that captures the memory and storage costs
associated with performing a security checks on a tagged
ﬂow at a state s ∈ {s j
i : ui (cid:54)∈ D j ∪ E }. On the other hand
rA(s, d, a, s(cid:48)) consists of three components (i) penalty term
α j
A < 0 if APT is detected by DIFT in the jth stage (ii) reward
term β j
A > 0 for APT reaching a destination of stage j,
A < 0 for APT
for
quitting the attack in jth stage. Since it is not necessary that
rD(s, d, a, s(cid:48)) = −rA(s, d, a, s(cid:48)) for all d ∈ AD(s), a ∈ AA(s) and
s, s(cid:48) ∈ S, DIFT-APT game is a nonzero-sum game.

j = 1, . . . , M and (iii) penalty term σ j

D. Information Structure

Both DIFT and APT are assumed to know the current
state, st of the game, both action sets AD(st ) and AA(st ), and
payoff structure of the DIFT-APT game. But DIFT is unaware
whether a tagged ﬂow at st is malicious or not and APT does
not know the chances of getting detected at st . This results in
an information asymmetry between the players. Hence DIFT-
APT game is an imperfect information game. Furthermore,
both players are unaware of the transition structure P which
depend on the rate of false negatives generated at the different
states st (Eq. (10)). Consequently, the DIFT-APT game is an
incomplete information game.

E. Solution Concept: ARNE

APTs are stealthy attackers whose interactions with the
system span over a long period of time. Hence, players D
and A must consider the rewards they incur over the long-
term time horizon when they decide on their policies πD
and πA, respectively. Therefore, average reward payoff criteria
is used to evaluate the outcome of DIFT-APT game for a
given policy pair (πD, πA) ∈ (πππ D, πππ A). Note that, the DIFT-
APT game originates at s0. Thus the average payoff for player
k ∈ {D, A} with policy pair (πD, πA) is deﬁned as follows.

ρk(s0, πD, πA) = lim inf
T →∞

1
T + 1

T
∑
t=0

Es0,πD,πA [rk(st , dt , at )].

Moreover, a pair of stationary policies (π ∗
ARNE of DIFT-APT game if and only if
ρA(s, π ∗
A) ≥ ρD(s, πD, π ∗
ρD(s, π ∗

D, π ∗

A),

D, π ∗
for all s ∈ S, πk ∈ πππ k.

D, π ∗

A) forms an

A) ≥ ρA(s, π ∗

D, πA)

V. ANALYZING ARNE OF THE DIFT-APT GAME

In this section we ﬁrst show the existence of ARNE in
DIFT-APT game. Then we provide necessary and sufﬁcient
conditions required to characterize an ARNE of DIFT-APT
game. Henceforth we assume the following assumption holds
for the IFG associated with the DIFT-APT game.

Assumption V.1. The IFG is acyclic.

Any IFG with set of cycles can be converted into an acyclic
IFG without
loosing any causal relationships between the
components given in the original IFG. One such dependency
preserving conversion is node versioning given in [45]. Hence
this assumption is not restrictive. Let P(πD, πA) be the MC
induced by a policy pair (πD, πA). The following theorem
presents properties of DIFT-APT game under Assumption V.1.

Theorem V.2. Let
tion V.1. Then, the following properties hold.

the DIFT-APT game satisﬁes Assump-

1) P(πD, πA) corresponding to any (πD, πA) ∈ (πππ D, πππ A) con-
sists of a single recurrent class of states (with possibly
some transient states reaching the recurrent class).
2) The recurrent class of P(πD, πA) includes the state s0.

Proof. Consider a partitioning of the state space such that S =
S1 ∪ S2 and S1 ∩ S2 = ∅. Here S1 denotes the set of states
that are reachable2 from state s0 and S2 denotes the set of
states that are not reachable from s0. We prove 1) and 2) by
showing that S1 forms a single recurrent class of P(πD, πA)
and S2 forms the set of transient states.

We ﬁrst show that in P(πD, πA), state s0 is reachable from
any arbitrary state s ∈ S\{s0}. The proof consists of two steps.
First consider a state s = s j
i , such that ui /∈ DM with j = M.
In other words, the state s is not a state that is corresponding
to a ﬁnal goal of the attack. Let s(cid:48) be an out neighbor of s.
Then s(cid:48) satisﬁes one of the two cases. i) s(cid:48) = s0 and ii) s(cid:48) =
s j(cid:48)
i(cid:48) ∈ S \ {s0}. Case i) happens if the APT decides to dropout

2In a directed graph a state u is said to be reachable from state v, if there

exists a directed path from v to u.

from the game or if DIFT successfully detects the APT. Thus
in case i) s0 is reachable from s.

Case ii) happens when DIFT does not detect APT and
the APT chooses to move to an out neighboring state s(cid:48). By
recursively applying cases i) and ii) at s(cid:48), we get s0 is reachable
when case i) occurs at least once. What is remaining to show
is when only case ii) occurs. In such a case, transitions from
s(cid:48) will eventually reach a state corresponding to a ﬁnal goal
of the attack, i.e., sM
i with ui ∈ DM, due to the acyclic nature
of the IFG imposed by Assumption V.1. Note that at sM
i with
ui ∈ DM the only transition possible is to s0. This proves that
s0 is reachable from any state s ∈ S \ s0.

This along with the deﬁnition of S1 implies that S1 forms
a recurrent class of P(πD, πA). Also as s0 is reachable from
any state in S2 and by the deﬁnition of S2, S2 is the set of
transient states. This completes the proof.

Corollary V.3 below presents the existence of an ARNE in

DIFT-APT using Theorem V.2.

Corollary V.3. Let the DIFT-APT game satisﬁes Assump-
tion V.1. Then, there exits an ARNE for the DIFT-APT game.

Proof. From the condition 1) in Theorem V.2 the DIFT-
APT game has a single recurrent class of states in P(πD, πA)
corresponding to any policy pair (πD, πA). As a result As-
sumption II.2 holds for DIFT-APT game. Therefore by Propo-
sition II.4 there exits an ARNE in DIFT-APT game.

The corollary below gives a necessary and sufﬁcient con-
dition for characterizing an ARNE of DIFT-APT game. Our
algorithm for computing ARNE is based on this condition.

Corollary V.4. The following conditions characterizes the
ARNE of DIFT-APT game.

ρk + vk(s) ≥ rk(s, ak, π−k) + ∑
s(cid:48)∈S

P(s(cid:48)|s, ak, π−k)vk(s(cid:48)),

(11a)

∑
k∈{D,A}

∑
s∈S

∑
ak∈Ak(s)

− ∑
s(cid:48)∈S

(cid:16)

ρk + vk(s) − rk(s, ak, π−k)

(cid:17)
P(s(cid:48)|s, ak, π−k)vk(s(cid:48))

πk(s, ak) = 0,

(11b)

∑
ak∈Ak(s)

πk(s, ak) = 1,

πk(s, ak) ≥ 0,

(11c)

where ρk denotes the average reward value of player k
independent of initial state of the game.

Proof. By Proposition II.5, ARNE of an unichain stochastic
game is characterized by conditions (2a)-(2e). The condition
(2a) reduce to (11a) by substituting λ s,ak
k ≥ 0 from condition
(2d). Below is the argument for condition (2b).

From Theorem V.2, the MC induced by (πD, πA), P(πD, πA),
contains only a single recurrent class. As a consequence, from
Proposition II.3, ρk(s, π) = ρk for all s ∈ S and k ∈ {D, A}.
Thus condition (2b) in Proposition II.5 reduces to
ρk − µ s,ak

P(s(cid:48)|s, ak, π−k) = ρk

P(s(cid:48)|s, ak, π−k)ρk = ρk ∑
s(cid:48)∈S

k = ∑
s(cid:48)∈S

k = 0. Since ρk(s, π) = ρk, condition (2a) in Propo-

Thus, µ s,ak
sition II.5 becomes
λ s,ak
k =ρk +vk(s)−rk(s, ak, π−k)−∑
s(cid:48)∈S
k = 0 and λ s,ak

By substituting µ s,ak
from Eqn. (12), condition
k
(2c) reduces to (11b). Finally, conditions (2d) and (2e) together
reduce to (11c). Thus conditions (11a)-(11c) characterizes an
ARNE in DIFT-APT game.

P(s(cid:48)|s, ak, π−k)vk(s(cid:48)). (12)

VI. DESIGN AND ANALYSIS OF RL-ARNE ALGORITHM

In this section we present a RL algorithm that learns ARNE

in DIFT-APT game.

A. RL-ARNE: Reinforcement Learning Algorithm for Comput-
ing Average Reward Nash Equilibrium

Algorithm VI.1 presents the pseudocode of RL-ARNE, a
stochastic approximation-based algorithm with multiple time
scales that computes an ARNE in DIFT-APT game. The
necessary and sufﬁcient condition given in Corollary V.4 is
used to ﬁnd an ARNE policy pair (π (cid:63)
A ) in Algorithm VI.1.

D, π (cid:63)

Algorithm VI.1 RL-ARNE Algorithm of DIFT-APT game

1: Input: State space (S), transition structure (P), rewards

(rD and rA), number of iterations (I >> 0)

2: Output: ARNE policies, (π (cid:63)
3: Initialization: n ← 0, v0

D, π (cid:63)
k ← 0, ρ 0

A ) ← (πππ I
k ← 0, ε 0

D, πππ I
A)
k ← 0, π 0

k ← πππ k

for k ∈ {D, A} and s ← s0.

4: while n (cid:54) I do
5:
6:
7:
8:
9:

D(s) and a from π n

Draw d from π n
A (s)
Reveal the next state s(cid:48) according to P
Observe the rewards rD(s, d, a, s(cid:48)) and rA(s, d, a, s(cid:48))
for k ∈ {D, A} do
vn+1
(s) = vn
k

k(s(cid:48)) − vn

ρ n+1
k = ρ n
ε n+1
k

k(s)]

k + vn

k(s) + δ n
v [rk(s, d, a, s(cid:48)) − ρ n
(cid:104) nρ n
(cid:105)
k +rk(s,d,a,s(cid:48))
k + δ n
− ρ n
ρ
k
n+1
(cid:2)
k (s, ak) + δ n
(s, ak) = ε n
∑k∈{D,A}(rk(s, d, a, s(cid:48)) −
ε
k (s, ak)(cid:3)
k(s)) − ε n
k + vn
ρ n
k(s(cid:48)) − vn
k (s, ak)(cid:12)
(cid:112)π n
k (s, ak)−δ n
(s, ak)=Γ(π n
(cid:12)rk(s, d, a, s(cid:48))−
π
k(s)(cid:12)
(cid:12)sgn(−ε n
k(s(cid:48)) − vn
k + vn
ρ n
k (s, ak)))

π n+1
k

10:

11:

12:

end for
Update the state of DIFT-APT game: s ← s(cid:48)
n ← n + 1

13:
14:
15:
16: end while

Using stochastic approximation, iterates in lines 9 and 10
compute the value functions vn
k(s), at each state s ∈ S, and
average rewards ρ n
k of DIFT and APT corresponding to policy
D, π n
pair (π n
k (s, ak) in line 11
and π n
k (s, ak) in line 12, are chosen such that Algorithm VI.1
converges to an ARNE of the DIFT-APT game. We present
below the outline of our approach.

A ), respectively. The iterates, ε n

and ∆(π) be deﬁned as

Let Ωs,ak
k,π−k
=ρk+vk(s)−rk(s, ak, π−k)−∑
s(cid:48)∈S

Ωs,ak
k,π−k

∆(π) = ∑

k∈{D, A}

∑
s∈S

∑
ak∈Ak(s)

Ωs,ak
k,π−k

πk(s, ak).

P(s(cid:48)|s, ak, π−k)vk(s(cid:48))(13)

In Theorem VI.13 we prove that all the policies (πD, πA)
such that Ωs,ak
< 0 forms an unstable equilibrium point
k,π−k
of the ODE associated with the iterates π n
k (s, ak). Hence,
Algorithm VI.1 will not converge to such policies. Consider
a policy pair (πD, πA) such that Ωs,ak
≥ 0. Note that, by
k,π−k
Eqn. (14), such a policy pair satisﬁes ∆(π) ≥ 0. When
∆(π) > 0, Algorithm VI.1 updates the policies of players in a
descent direction of ∆(π) to achieve ARNE (i.e., ∆(π) = 0).
Let the gradient of ∆(π) with respect to policies πD and
∂ π , where π = (πD, πA). Then for each k ∈ {D, A},
represents each

πA be ∂ ∆(π)
s ∈ S, and ak ∈ Ak(s),

∂ ∆(π)
∂ πk(s,ak) = ∑

Ωs,ak
¯k,π−k

¯k∈{D,A}

component of ∂ ∆(π)
∂ π . Lemma VIII.1 in Appendix shows the
∂ ∆(π)
∂ ∆(π)
derivation of
∂ πk(s,ak) . Notice that computation of
∂ πk(s,ak)
requires the values of P which is assumed to be unknown
in DIFT-APT game. Therefore the iterate ε n
k (s, ak) in line 11
∂ ∆(π)
∂ πk(s,ak) using stochastic approx-
of Algorithm VI.1 estimates
∂ ∆(π)
imation. Convergence of −ε n
k (s, ak) to
∂ πk(s,ak) is proved in
Theorem VI.8.
Additionally,

the map Γ
projects the policies to probability simplex deﬁned by con-
dition (11c) in Corollary V.4. Here, | · | denotes the absolute
value. The function sgn(χ) denotes the continuous version of
the standard sign function (e.g., sgn(χ) = tanh(cχ) for any
constant c > 1). Lemma VI.10 shows that the policy iterates
in line 12 update in a valid descent direction of ∆(π) and
Theorem VI.12 proves the convergence. Theorem VI.13 then
shows that the converged policies indeed form an ARNE.

in line 12 of Algorithm VI.1,

v and δ n

Note that the value function iterates in line 9 and the gradi-
ent estimate iterates in line 11 of Algorithm VI.1 update in a
same faster time scale δ n
ε , respectively. Policy iterates
in line 12 update in a slower time scale δ n
π . Also average
reward payoff iterates in line 10 update in an intermediate
time scale δ n
ρ . Hence the step-sizes of the proposed algorithm
ε >> δ n
are chosen such that δ n
π . Furthermore, the
step-sizes must also satisfy the conditions in Assumption II.6.
Due to time scale separation, iterations in relatively faster time
scales see iterations in relatively slower times scales as quasi-
static while the latter sees former as nearly equilibrated [46].

ρ >> δ n

v = δ n

k , vn

line 11 of

Remark VI.1. Note that, RL-ARNE algorithm presented in
Algorithm VI.1 must be trained ofﬂine due to the infor-
the algo-
is required at
mation exchange that
rithm. Here, players are required to exchange the information
their respective temporal difference error estimates,
about
˜φk(ρ n
k + vn
k) = rk(s, d, a, s(cid:48)) − ρ n
k(s), as the iter-
ates on each player’s gradient estimation includes the term
∑k∈{D,A} ˜φk(ρ n
k). Since RL-ARNE algorithm is trained of-
ﬂine and the policies found at the end of the training only
depend on their respective actions, players do not require any
information exchange on their respective actions when they
execute their learned policies in real-time.

k(s(cid:48)) − vn

k , vn

B. Convergence Proof of the RL-ARNE Algorithm

(14)

First rewrite iterations in line 9 and line 10 as Eqn. (15)
and Eqn. (16) to show the convergence of value and average

reward payoff iterates in Algorithm VI.1.

vn+1
k

k(s) + wn

v] (15)
(16)

(s) = vn
= ρ n

k(s) + δ n
k + δ n

v [F(vn

k, ρ n
k ) − ρ n

k )(s) − vn
k + wn
ρ ]

ρ n+1
k

ρ [G(ρ n
For brevity we use π(s, d, a) = πD(s, d)πA(s, a) and π to denote
(πD, πA). Then, from Eqn. (9),
P(s(cid:48)|s, π) = ∑
∑
a∈AA(s)
k)(s) and G(ρ n

Two function maps F(vn

π(s, d, a)P(s(cid:48)|s, d, a).

k ) are deﬁned as

d∈AD(s)

F(vn

k, ρ n

k )(s) = ∑
s(cid:48)∈S

G(ρ n

k ) = ∑
s(cid:48)∈S

P(s(cid:48)|s, π)[rk(s, d, a, s(cid:48)) − ρ n

k + vn

k(s(cid:48))], (17)

P(s(cid:48)|s, π)

(cid:104) nρ n

k + rk(s, d, a, s(cid:48))
n + 1

(cid:105)
.

(18)

The zero mean noise parameters wn
v = rk(s, d, a, s(cid:48)) − ρ n
k + vn
wn
k + rk(s, d, a, s(cid:48))
n + 1

v and wn
k(s(cid:48)) − F(vn
− G(ρ n

ρ =

nρ n

k ).

wn

ρ are deﬁned as
k, ρ n

k )(s),

(19)

(20)

(cid:12)vk(s(cid:48)) − ¯vk(s(cid:48))(cid:12)
(cid:12)
(cid:12)

∑
s(cid:48)∈S
(cid:107) vk − ¯vk(cid:107)1 = |S| (cid:107) vk − ¯vk(cid:107)1.

≤ ∑
s∈S
= ∑
s∈S

The inequalities
by the triangle inequality and observing the fact
max{P(s(cid:48)|s, π)} = 1. Then from Eqn. (23),

in the above equations are followed
that

(cid:107) f (vk) − f ( ¯vk)(cid:107)1 ≤ (|S| + 1) (cid:107) vk − ¯vk(cid:107)1.
Hence f (vk) is Lipschitz. Next we prove g(ρk) is Lipschitz.
Let ρk and ¯ρk be two distinct average payoff values. Then,

|g(ρk) − g( ¯ρk)| =

(cid:12)
(cid:12)
(cid:12)
(cid:12)

n
n + 1

(cid:12)
(cid:12)
[ρk − ¯ρk] − [ρk − ¯ρk]
(cid:12)
(cid:12)

= |ρk − ¯ρk| .

Therefore g(ρk) is Lipschitz.

Lemma VI.5 shows the map F(vn

k)(s)]s∈S is a
pseudo-contraction with respect to some weighted sup-norm.
The deﬁnitions of weighted sup-norm and pseudo-contraction
are given below.

k) = [F(vn

Let vk = [vk(s)]s∈S. Then the ODE associated with the
iterates given in Eqn. (15) corresponding to all s ∈ S and the
ODE associated with the iterate in Eqn. (16) are as follows.

Deﬁnition VI.3 (Weighted sup-norm). Let ||b||ε denote the
weighted sup-norm of a vector b ∈ Rmb with respect to the
vecor ε ∈ Rmb . Then,

˙vk = f (vk, ρk)
˙ρk = g(ρk),
where f : R|S| → R|S| is such that f (vk, ρk) = F(vk, ρk) − vk,
where F(vk, ρk) = [F(vk, ρk)(s)]s∈S and g : R → R is deﬁned
as g(ρk) = G(ρk) − ρk.

(21)
(22)

k ). As a consequence, vn

We note that, in Algorithm VI.1, value function iterates
(vn
k(s)) runs in a relatively faster time scale compared to the
average reward iterates (ρ n
k(s) iterates
see ρ n
k as quasi-static. Hence, for brevity, in the proofs of
Lemma VI.2, Lemma VI.5, and Theorem VI.7 we represent
k )(s) as f (vk) and F(vn
f (vk, ρk) and F(vn
k)(s), respectively.
A set of lemmas that are used to prove the convergence of
the iterates in lines 9 and 10 of Algorithm VI.1 are given
below. Lemma VI.2 presents a property of the ODEs in
Eqns. (21) and (22).

k, ρ n

||b||ε = max
q=1,...,n

|b(q)|
ε(q)

,

where |b(q)| represent the absolute value of the qth entry of
vector b.

Deﬁnition VI.4 (Pseudo contraction). Let c, ¯c ∈ Rmc. Then a
function φ : Rmc → Rmc is said to be a pseudo contraction
with respect to the vector γ ∈ Rmc if and only if,

(cid:107) φ (c) − φ ( ¯c) (cid:107)γ ≤ η (cid:107) c − ¯c (cid:107)γ , where 0 ≤ η < 1.

k, ρ n
Lemma VI.5. Consider F(vn
k )(s) deﬁned in Eqn. (17).
Then the function map F(vn
k, ρ n
k ) = [F(vn
k, ρ n
k )(s)]s∈S is a
pseudo-contraction with respect to some weighted sup-norm.

Proof. Consider two distinct value functions vn

(cid:107) F(vn

k)(s) − F( ¯vn

k)(s)(cid:107)1 =(cid:107) ∑
s(cid:48)∈S

P(s(cid:48)|s, π)[vn

k and ¯vn
k(s(cid:48)) − ¯vn

k. Then,
k(s(cid:48))](cid:107)1

Lemma VI.2. Consider the ODEs ˙vk = f (vk, ρk) and ˙ρk =
g(ρk). Then the functions f (vk, ρk) and g(ρk) are Lipschitz.

= (cid:107) ∑
s(cid:48)∈S

∑
d∈AD(s)
a∈AA(s)

π(s, d, a)P(s(cid:48)|s, d, a)[vn

k(s(cid:48)) − ¯vn

k(s(cid:48))](cid:107)1

Proof. First we show f (vk) is Lipschitz. Consider two distinct
value vectors vk and ¯vk. Then,

(cid:107) f (vk) − f ( ¯vk)(cid:107)1 = (cid:107) [F(vk) − F( ¯vk)] − [vk − ¯vk](cid:107)1
≤ (cid:107) F(vk) − F( ¯vk)(cid:107)1+ (cid:107) vk − ¯vk(cid:107)1
(cid:12)
(cid:12)
(cid:12)
(cid:12)
=∑
(cid:12)+(cid:107) vk − ¯vk(cid:107)1.(23)
(cid:12)F(vk)(s) − F( ¯vk)(s)
s∈S

Notice that,

∑
s∈S

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12) = ∑
(cid:12)F(vk)(s) − F( ¯vk)(s)

s∈S
≤ ∑
s∈S

(cid:12)
(cid:12)
P(s(cid:48)|s, π)[vk(s(cid:48)) − ¯vk(s(cid:48))]
(cid:12)
(cid:12)
(cid:12)

P(s(cid:48)|s, π) (cid:12)

(cid:12)vk(s(cid:48)) − ¯vk(s(cid:48))(cid:12)
(cid:12)

(cid:12)
(cid:12)
(cid:12)
∑
(cid:12)
(cid:12)
s(cid:48)∈S
∑
s(cid:48)∈S

≤ ∑

d∈AD(s)
a∈AA(s)

π(s, d, a) ∑
s(cid:48)∈S

P(s(cid:48)|s, d, a) (cid:107) vn

k(s(cid:48)) − ¯vn

k(s(cid:48))(cid:107)1

(24)

Eqn. (24) follows from triangle inequality. To ﬁnd an upper
bound for the term P(s(cid:48)|s, d, a) in Eqn. (24), we construct
a Stochastic Shortest Path Problem (SSPP) with the same
state space and transition probability structure as in DIFT-APT
game, and a player whose action set is given by AD × AA. Fur-
ther set the rewards corresponding to all the state transition in
SSPP to be −1. Then by Proposition 2.2 in [47], the following
holds condition for all s ∈ S and (d, a) ∈ AD(s) × AA(s).

P(s(cid:48)|s, d, a)ε(s(cid:48)) ≤ ηε(s),

∑
s(cid:48)∈S

where ε ∈ [0, 1]|S| and 0 ≤ η < 1. Rewrite Eqn. (24) as

|F(vn

k)(s) − F( ¯vn

k)(s)|

π(s, d, a) ∑
s(cid:48)∈S

P(s(cid:48)|s, d, a)ε(s(cid:48))

|vn

k(s(cid:48)) − ¯vn
ε(s(cid:48))

k(s(cid:48))|

π(s, d, a) ∑
s(cid:48)∈S

P(s(cid:48)|s, d, a)ε(s(cid:48)) max
s(cid:48)∈S

|vn

k(s(cid:48)) − ¯vn
ε(s(cid:48))

k(s(cid:48))|

π(s, d, a) ∑
s(cid:48)∈S

P(s(cid:48)|s, d, a)ε(s(cid:48)) (cid:107) vn

k − ¯vn

k(cid:107)ε

π(s, d, a)ηε(s) (cid:107) vn

k − ¯vn

k(cid:107)ε = ηε(s) (cid:107) vn

k − ¯vn

k(cid:107)ε .

≤ ∑

d∈AD(s)
a∈AA(s)

≤ ∑

d∈AD(s)
a∈AA(s)

≤ ∑

d∈AD(s)
a∈AA(s)

≤ ∑

d∈AD(s)
a∈AA(s)

|F(vn

|F(vn

k)(s)|

k)(s)|

k)(s) − F( ¯vn
ε(s)
k)(s) − F( ¯vn
ε(s)
k) − F( ¯vn

(cid:107) F(vn

max
s∈S

≤ η (cid:107) vn

k − ¯vn

k(cid:107)ε

≤ η (cid:107) vn

k(cid:107)ε

k − ¯vn
k − ¯vn

k(cid:107)ε .

k)(cid:107)ε ≤ η (cid:107) vn

The next result proves the boundedness of the iterates in

Algorithm VI.1.

Lemma VI.6. Consider the RL-ARNE algorithm presented in
Algorithm VI.1. Then, the iterates vn
k , for s ∈ S and
k ∈ {D, A}, in Eqn.(15) and Eqn.(16) are bounded.

k(s) and ρ n

Proof. Lemma VI.5 proved that F(vn
k) is a pseudo-contraction
with respect to some weighted sup-norm. By choosing step-
size, δ n
v to satisfy Assumption II.6 and observing that the noise
parameter, wn
v is zero mean with bounded variance, all the
conditions in Theorem 1 in [48] hold for the DIFT-APT game.
Hence, by Theorem 1 in [48], the iterates vn
k(s) in Eqn. (15)
are bounded for all s ∈ S.

From Proposition II.3, a ﬁxed policy pair (πD, πA) and
n >> 0, the average reward payoff values ρ n
k depend only
on the rewards due to the state transitions that occur within
the recurrent classes of induced MC. Recall that Theorem V.2
showed induced Markov chain,P(πD, πA), in DIFT-APT game
contains only a single recurrent class. Let S1 be the set of
states in the recurrent class of P(πD, πA). Then there exists
a unique stationary distribution p for P(πD, πA) restricted to
states in S1. Thus for n >> 0 and each k ∈ {D, A},
ρ n
k = ∑
s∈S1

p(s)rk(s, π),

(25)

where p(s) is the probability of being at state s ∈ S1 and
π(s, d, a) ∑s(cid:48)∈S P(s(cid:48)|s, d, a)r(s, d, a, s(cid:48)) is the
rk(s, π) = ∑

d∈AD(s)
a∈AA(s)

expected reward at the state s ∈ S1 for player k ∈ {D, A}. Since
S1 has ﬁnite carnality and the rewards, rk are ﬁnite for DIFT-
APT game, ρ n
k converge to a globally asymptotically stable
critical point given in Eqn. (25) and ρ n
k iterates are bounded.

Theorem VI.7 proves the convergence of the iterates vn

k(s),

for all s ∈ S, and ρ n
k .

Theorem VI.7. Consider the RL-ARNE algorithm presented
in Algorithm VI.1. Then the iterates vn
k(s), for all s ∈ S, and
ρ n
k for k ∈ {D, A} in Eqn. (15) and Eqn. (16) converge.
Proof. By Proposition II.8, convergence of
the stochas-
tic approximation-based algorithm by conditions (1)-(6).
Lemma VI.2 and Lemma VI.6 showed that condition (1) and
condition (2) in Proposition II.8 are satisﬁed, respectively.

To show that condition (3) is satisﬁed, we ﬁrst show
k)(s) is a non-expansive map. Consider two distinct value
k. Since P(s(cid:48)|s, πD, πA) ≤ 1, from Eqn. (24),

F(vn
functions vn

k and ¯vn
(cid:107) F(vn

k)(s) − F( ¯vn

k)(s)(cid:107) ≤(cid:107) vn

k(s(cid:48)) − ¯vn

k(s(cid:48))(cid:107).

Thus F(vn
k)(s) is a non-expansive map and hence from Theo-
rem 2.2 in [49] iterates vn
k(s), for all s ∈ S and k ∈ {D, A},
converge to an asymptotically stable critical point. Thus
condition (3) is satisﬁed. Lemma VI.6, showed that ρ n
k , for
k ∈ {D, A}, converge to a globally asymptotically stable critical
point which implies that condition (4) is satisﬁed.

From Eqns. (19) and (20), the noise measures have zero
mean. The variance of these noise measures are bounded
by the ﬁneness of the rewards in DIFT-APT game and the
boundedness of the iterates vn
k . Thus condition (5)
is satisﬁed. Finally, the choice of step-sizes to satisfy condi-
tion (6). Therefore the results follows by Proposition II.8.

k(s) and ρ n

Next theorem proves the convergence of gradient estimates.

Theorem VI.8. Consider Ωs,ak
and ∆(π) given in Eqns. (13)
k,π−k
and (14), respectively. Then gradient estimation iterate,
ε n
k (s, ak) in line 11 corresponding to any k ∈ {D, A}, s ∈ S,
and ak ∈ Ak(s), converge to − ∂ ∆(π)

∂ πk(s,ak) = − ∑

Ωs,ak
¯k,π−k

.

¯k∈{A,D}

Proof. Rewrite gradient estimation in line 11 as follows.
k (s, ak) + wn
ε

k (s, ak) + δ n
ε

(s, ak) = ε n

− ε n

ε n+1
k

(cid:2) −∑
¯k∈{D,A}

Ωs,ak
¯k,π−k

(cid:3),

¯Ωs

where wn

ε = ∑
k(s(cid:48))−vn

k + ∑
k(s). Note that E(wn

¯k∈{D,A}

ρ n
k +vn
with Eqn. (26) is given by,

k∈{D,A}

, and ¯Ωs

(26)
k = rk(s, d, a, s(cid:48))−

Ωs,ak
¯k,π−k
ε ) = 0. Then ODE associated

˙εk(s, ak) = − ∑

Ωs,ak
¯k,π−k

− εk(s, ak).

¯k∈{D,A}

We use Proposition II.7 to prove the convergence of gradient
estimation iterates, ε n
k (s, ak). Step-size δ n
is chosen such
ε
that condition 1) in Proposition II.7 is satisﬁed. Validity of
condition 2) can be shown as follows.





E

 lim
n→∞

sup
¯n>n

(cid:12)
¯n
(cid:12)
(cid:12)
∑
(cid:12)
(cid:12)
l=n

ε wl
δ l
ε

2
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)





 ≤ 4 lim
n→∞

∞
∑
l=n

(δ l

ε )2E(|wl

ε |2) = 0.

(27)
Inequality in Eqn. (27) follows by Doob inequality [50].
Equality in Eqn. (27) follows by choosing δ n
to satisfy
ε
Assumption II.6 and observing E(|wl
k, and

ε |2) < ∞ as rk, vn

ρ n
k are bounded in DIFT-APT game. Comparing Eqn. (26)
with Eqn. (5), κ = 0 in Eqn. (26). Therefore, from Proposi-
tion II.7, as n → ∞, ε n
k (s, ak) → − ∑
∂ πk(s,ak) .
This completes the proof showing the convergence of gradient
estimation iterates ε n

= − ∂ ∆(π)

Ωs,ak
¯k,π−k

¯k∈{A,D}

k (s, ak).

Next, we prove the convergence of the policy iterates. In

order to do so, we proceed in the following manner.

1) We rewrite the conditions in Corollary V.4 that character-
ize ARNE of DIFT-APT game as a non-linear optimiza-
tion problem (Problem VI.9).

2) Then we show the policies are updated in a valid decent
direction, (cid:112)π n
, with respect
to the objective function (or temporal difference error),
∆(π), of Problem VI.9 (Lemma VI.10).

(cid:16) ∂ ∆(πn)
∂ πn
k (s,ak)

k (s, ak)(cid:12)

(cid:12)Ωs,ak
k,π−k

(cid:12)
(cid:12)sgn

(cid:17)

3) Using steps 1) and 2), we characterize the stable and un-
stable equilibrium points associated with the ODE corre-
sponding to the policy iterates in line 12 (Lemma VI.11).
4) Invoking Proposition II.7 we prove the convergence of
policy iterates to stable equilibrium points found in
step 3) (Theorem VI.12).

Below we elaborate steps 1)-4). ARNE of the DIFT-APT game
can be characterized as the following non-linear optimization
problem (step 1)).

Problem VI.9. The necessary and sufﬁcient conditions given
in Corollary V.4 that characterize the ARNE of DIFT-APT can
be reformulated as the following non-linear program using
Ωs,ak
and ∆(π) introduced in Eqns. (13) and (14).
k,π−k

min
v,ρ,π

∆(π) s.t. Ωs,ak
k,π−k

≥ 0; ∑

ak∈Ak(s)

πk(s, ak) = 1; πk(s, ak) ≥ 0,

where v = (vD, vA), vk = [vk(s)]s∈S for k ∈ {D, A}, ρ =
(ρD, ρA), π = (πD, πA), πk = [πk(s)]s∈S, and πk(s) =
[πk(s, ak)]ak∈Ak(s), for k ∈ {D, A}.

In Lemma VI.10, we show policy iterates are updated in a
valid descent direction with respect to the objective function,
∆(π) (step 2)).
Lemma VI.10. Consider Ωs,ak
and ∆(π) given in Eqns. (13)
k,π−k
and (14), respectively. Then for any k ∈ {D, A}, s ∈ S,
and ak ∈ Ak(s), policy iterate, π n
in line 12 of
Algorithm VI.1 is updated in a valid descent direction,
k (s, ak)(cid:12)
(cid:112)π n
≥ 0
and ∆(π) > 0.

(cid:17)
, of ∆(π) when Ωs,ak
k,π−k

(cid:16) ∂ ∆(πn)
∂ πn
k (s,ak)

(cid:12)Ωs,ak
k,π−k

k (s, ak),

(cid:12)
(cid:12)sgn

Proof. First we rewrite policy iteration in line 12 as follows.

π n+1
k

(s, ak)=Γ(π n

k (s, ak)(cid:12)
π n
(cid:19)

(cid:12)Ωs,ak
k,π−k
(cid:19)

(cid:12)
(cid:12)

(28)

+ wn
π

),

(cid:16)(cid:113)

k (s, ak)− δ n
π
(cid:18) ∂ ∆(π n)
∂ π n
k (s, ak)
(cid:105)
(cid:12)
(cid:12)

sgn

(cid:104)(cid:12)
(cid:12) ¯Ωs
k

k (s, ak)

π = (cid:112)π n

(cid:12) − (cid:12)
(cid:12)
k = rk(s, d, a, s(cid:48))−ρk +vk(s(cid:48))−vn

(cid:16) ∂ ∆(πn)
(cid:12)Ωs,ak
where wn
, and
∂ πn
k,π−k
k (s,ak)
¯Ωs
k(s). Note that, policy iterate
time scale when compared to the
updates in the slowest
other iterates. Thus, Eqn. (28) uses the converged values of

sgn

(cid:17)

∂ πn
Consider a policy π n+1

value functions (vk), average reward values (ρk), and gradient
estimates ( ∂ ∆(πn)
D, π n
k (s,ak) ) with respect to policy π n = (π n
A ).
k whose entries are same as π n
k except
(s, ak) which is chosen as in Eqn. (28), for small
k , π n
−k). Also
π and using

−k) and ˆπ = (π n
, π n
π ) = 0. Thus ignoring the term wn

the entry π n+1
0 < δ n
note that E(wn
Taylor series expansion yields,

π << 1. Let ¯π = (π n+1

k

k

(cid:16)

(cid:113)

∆( ¯π) =∆( ˆπ) + δ n
−
π
(cid:18) ∂ ∆( ˆπ)
∂ π n

sgn

k (s, ak)
(cid:18)
(cid:113)

−

=∆( ˆπ) + δ n
π

(cid:12)
(cid:12)

(cid:12)Ωs,ak
k,π−k
(cid:19)

k (s, ak)(cid:12)
π n
(cid:19) ∂ ∆( ˆπ)
∂ π n
k (s, ak)(cid:12)
π n

k (s, ak)
(cid:12)Ωs,ak
k,π−k

+ o(δ n
π )

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

∂ ∆( ˆπ)
k (s, ak)

∂ π n

(cid:19)
(cid:12)
(cid:12)
(cid:12)

,

π . We ignore o(δ n

where o(δ n
π ) represents the higher order terms correspond-
ing to δ n
π ) in the second equality above
since the choice of δ n
is small. Notice that
the term
π
(cid:12)
(cid:12)
k (s, ak)(cid:12)
∂ ∆( ˆπ)
δ n
(cid:12)
is negative. Since ∆(π) >
(cid:12)
(cid:12)
π
k (s,ak)
0 for any π, we get ∆( ¯π) < ∆( ˆπ). This proves policies are
updated in a valid descent direction.

(cid:16)
−(cid:112)π n

(cid:12)Ωs,ak
k,π−k

(cid:12)
(cid:17)
(cid:12)
(cid:12)

∂ πn

Notice that the ODE associated with Eqn. (28) can be

written as,

˙πk(s, ak)= ¯Γ

(cid:18)
−(cid:112)

πk(s, ak)(cid:12)

(cid:12)Ωs,ak
k,π−k

(cid:12)
(cid:12)sgn

(cid:19)(cid:19)

(cid:18) ∂ ∆(π)
∂ πk(s, ak)

, (29)

where ¯Γ is the continuous version of the projection operator
Γ which is deﬁned analogous to the continuous projection
operator in Eqn. (6). Let Π denotes the set of limit points
associated with the system of ODEs in Eqn. (29). Let the
feasible set of Problem VI.9 be
H = {π ∈ L|Ωs,ak
k,π−k

≥ 0, for all ak ∈ Ak(s), s ∈ S, k ∈ {D, A}},
(30)
where the set L = {π| ∑ak∈Ak(s) πk(s, ak) = 1, πk(s, ak) ≥
0, for all ak ∈ Ak(s), s ∈ S}. The set Π can be partitioned
using the set H as Π = Π1 ∪ Π2, where Π1 = Π ∩ H and
Π2 = Π \ Π1. Using these notations and steps 1) and 2), we
characterize the stable and unstable equilibrium points of the
system of ODEs in Eqn. (29) in Lemma VI.11 (step 3)).

Lemma VI.11. The following statements are true for the set
of equilibrium policies π (cid:63) of ODE in Eqn. (29).

1) All π (cid:63) ∈ Π1 form a set of stable equilibrium points.
2) All π (cid:63) ∈ Π2 form a set of unstable equilibrium points.

Proof. First we show statement 1) holds. Since the set Π1 is
in the feasible set H of Problem VI.9 deﬁned in Eqn. (30), for
any π (cid:63) ∈ Π1, there exists some ak ∈ Ak(s), s ∈ S that satisfy
Ωs,ak
≥ 0. Let Bζ (π (cid:63)) = {π ∈ L| (cid:107)π − π (cid:63)(cid:107) < ζ }. Then, for
k,π−k
any π ∈ Bζ (π (cid:63)) \ Π1, there exists a ζ > 0 such that Ωs,ak
> 0
k,π−k
(cid:17)
∂ ∆(π)
∂ πk(s,ak) > 0. This implies sgn
which yields
(cid:16) ∂ ∆(π)
(cid:16)
−(cid:112)πk(s, ak)(cid:12)
(cid:12)Ωs,ak
Hence, ¯Γ
< 0 for any
k,π−k
∂ πk(s,ak)
π ∈ Bζ (π (cid:63))\Π1. This implies that πk(s, ak) will decrease when
moving away from π (cid:63) ∈ Π1. This proves π (cid:63) ∈ Π1 is an stable
equilibrium point of the system of ODEs given in Eqn. (29).
To show statement 2) is true, we ﬁrst note that for any
there exists some ak ∈ Ak(s), s ∈ S such that

(cid:16) ∂ ∆(π)
∂ πk(s,ak)
(cid:17)(cid:17)

π (cid:63) ∈ Π2,

(cid:12)
(cid:12)sgn

> 0.

Ωs,ak
k,π−k
such that Ωs,ak
k,π−k
(cid:17)
(cid:16) ∂ ∆(π)
∂ πk(s,ak)
Therefore, ¯Γ

sgn

< 0. Then, for any π ∈ Bζ (π (cid:63))\Π2, there exists a ζ > 0
∂ ∆(π)
∂ πk(s,ak) < 0. This implies

< 0 which yields

< 0.
(cid:16)
−(cid:112)πk(s, ak)(cid:12)

(cid:12)
(cid:12)sgn

(cid:12)Ωs,ak
> 0 for
k,π−k
any π ∈ Bζ (π (cid:63)) \ Π2. This implies that πk(s, ak) will increase
when moving away from π (cid:63) ∈ Π2. This proves π (cid:63) ∈ Π2 is an
unstable equilibrium point of the system of ODEs in Eqn. (29)
and completes the proof.

(cid:16) ∂ ∆(π)
∂ πk(s,ak)

(cid:17)(cid:17)

Theorem VI.12 gives the convergence of the policy iterates

to the set of stable equilibrium points in step 3) (step 4)).

Theorem VI.12. Consider the RL-ARNE algorithm presented
in Algorithm VI.1. Then the policy iterates π n
k (s, ak) for all
ak ∈ Ak(s), s ∈ S, and k ∈ {D, A} converge to a stable
equilibrium point π (cid:63) = (π (cid:63)

D, π (cid:63)

k (s, ak)

π = (cid:112)π n

(cid:16) ∂ ∆(πn)
Proof. Recall wn
∂ πn
k (s,ak)
We invoke Proposition II.7 to prove the convergence of policy
iterates, π n
π is chosen such that condition
1) in Proposition II.7 is satisﬁed. Validity of condition 2) can
be shown as follows.

k (s, ak). Step-size δ n

(cid:12)Ωs,ak
k,π−k

(cid:105)
(cid:12)
(cid:12)

sgn

A ) ∈ Π1.
(cid:104)(cid:12)
(cid:12) − (cid:12)
(cid:12)
(cid:12) ¯Ωs
k





E

 lim
n→∞

sup
¯n>n

(cid:12)
¯n
(cid:12)
(cid:12)
∑
(cid:12)
(cid:12)
l=n

π wl
δ l
π

2
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)





 ≤ 4 lim
n→∞

∞
∑
l=n

(δ l

π )2E(|wl

π |2) = 0.

(31)
Inequality in Eqn. (31) follows by Doob inequality [50].
Equality in Eqn. (31) follows by choosing δ n
π to satisfy
Assumption II.6 and observing E(|wl
k, and
ρ n
k are bounded in DIFT-APT game. Comparing Eqn. (28)
with Eqn. (5), κ = 0. Therefore, from Proposition II.7, as
n → ∞, the policy iterates π n
k (s, ak) for all ak ∈ Ak(s), s ∈ S,
and k ∈ {D, A} converge to a stable equilibrium point π (cid:63) ∈ Π1.
This completes the proof showing the convergence of policy
iterates π n

π |2) < ∞ as rk, vn

k (s, ak).

Next theorem proves the convergence of π n

k (s, ak) given in
line 12 of Algorithm VI.1, to an ARNE in DIFT-APT game.
Theorem VI.13. Consider Ωs,ak
and ∆(π) given in
k,π−k
Eqns. (13) and (14), respectively. A converged policy (π (cid:63)
D, π (cid:63)
A )
of RL-ARNE algorithm presented in Algorithm VI.1 forms an
ARNE in DIFT-APT game.

D, π (cid:63)

Proof. In the following, we show any converged policy π (cid:63) =
(π (cid:63)
A ) returned by RL-ARNE algorithm presented in Algo-
rithm VI.1 will satisfy conditions (11a)-(11c) in Corollary V.4
and thus π (cid:63) forms an ARNE in DIFT-APT game.

Recall from Theorem VI.12, the policy iterates π n

k (s, ak)
for all ak ∈ Ak(s), s ∈ S, and k ∈ {D, A} converge to a stable
equilibrium point π (cid:63) ∈ Π1. Also, recall Π denotes the set of
limit points associated with the system of ODEs in Eqn. (29)
and L = {π| ∑ak∈Ak(s) πk(s, ak) = 1, πk(s, ak) ≥ 0, for all ak ∈
Ak(s), s ∈ S}. Then, from the deﬁnition of the set Π1, any
converged π (cid:63) will satisfy conditions (11a) and (11c), since
π (cid:63) ∈ Π1 = Π ∩ H yields π (cid:63) ∈ H, where H = {π ∈ L|Ωs,ak
≥
k,π−k
0, for all ak ∈ Ak(s), s ∈ S, k ∈ {D, A}}.

sufﬁces

Then it
(cid:112)πk(s, ak)Ωs,ak
k,π−k
Corollary V.4. We show this by contradiction arguments.

to show any π (cid:63) ∈ Π1 will yield
= 0 since this proves condition (11c) in

(cid:16)
−(cid:112)πk(s, ak)(cid:12)

Note that ¯Γ

(cid:12)Ωs,ak
= 0 as π (cid:63)
k,π−k
forms a set of equilibrium polices associated with the system
of ODEs in Eqn. (29). Then suppose there exists a policy
0 < πk(s, ak) ≤ 1 for some ¯ak ∈ Ak(s), s ∈ S, and k ∈ {D, A}
such that (cid:112)πk(s, ¯ak)Ωs,ak
(cid:54)= 0.
k,π−k

(cid:16) ∂ ∆(π)
∂ πk(s,ak)

(cid:12)
(cid:12)sgn

(cid:17)(cid:17)

Now consider the following two cases.

Case I: πk(s, ¯ak) = 1 and Ωs, ¯ak
k,π−k
Recall F(vk, ρk) = [F(vk, ρk)(s)]s∈S and F(vk, ρk)(s) =
P(s(cid:48)|s, π)[rk(s, d, a, s(cid:48)) − ρ n
k + vk(s(cid:48))]. Then under Case I,

(cid:54)= 0.

∑
s(cid:48)∈S
we obtain the following:

∑
ak∈Ak(s)

πk(s, ak)Ωs,ak
k,π−k

= πk(s, ¯ak)Ωs, ¯ak
k,π−k

= 0,

(cid:17)
.

where the ﬁrst equality is due to πk(s, ¯ak) = 0 and the second
equality is due to the convergence of the value iterates to their
true values (i.e., as n → ∞, vk → F(vk, ρk)) which is proved in
Theorem VI.7.

Further, as πk(s, ¯ak) = 1 this yields Ωs, ¯ak
k,π−k
(cid:54)= 0 in Case I.

contradicts the condition Ωs, ¯ak
k,π−k
Case II: 0 < πk(s, ¯ak) < 1 and Ωs, ¯ak
k,π−k

(cid:54)= 0.

Under this case we get

= 0, which

(cid:18)
−(cid:112)

¯Γ

πk(s, ak)(cid:12)

(cid:12)Ωs,ak
k,π−k

= −(cid:112)

πk(s, ak)(cid:12)

(cid:12)Ωs,ak
k,π−k

(cid:12)
(cid:12)sgn

(cid:18) ∂ ∆(π)
∂ πk(s, ak)
(cid:19)

(cid:12)
(cid:12)sgn
(cid:18) ∂ ∆(π)
∂ πk(s, ak)

(cid:19)(cid:19)

(cid:54)= 0,

due to conditions in given in the Case II and assuming3
sgn(·) (cid:54)= 0. However this contradicts with our initial obser-
vation of ¯Γ

(cid:16)
−(cid:112)πk(s, ak)(cid:12)

= 0.

(cid:12)
(cid:12)sgn

(cid:17)(cid:17)

(cid:16) ∂ ∆(π)
∂ πk(s,ak)

(cid:12)Ωs,ak
k,π−k

Therefore, by contradiction, there does not exist any policy
0 < πk(s, ak) ≤ 1 for some ¯ak ∈ Ak(s), s ∈ S, and k ∈ {D, A}
such that (cid:112)πk(s, ¯ak)Ωs,ak
(cid:54)= 0. This proves condition (11c)
k,π−k
in Corollary V.4 holds.

Since now we have shown conditions (11a)-(11c) in Corol-
lary V.4 hold, a converged policy (π (cid:63)
A ) of RL-ARNE
algorithm presented in Algorithm VI.1 forms an ARNE in
DIFT-APT game.

D, π (cid:63)

Remark VI.14. Note that RL-ARNE algorithm presented in
Algorithm VI and the associated convergence proofs given
in Section VI.B extend to K-player, non-zero sum, average
reward unichain stochastic games. Unichain property is a mild
regularity assumption compared to other regularity conditions
such as ergodicity or irreducibility [51].

VII. SIMULATIONS

In this section we test Algorithm VI.1 on a real-world attack
dataset corresponding to a ransomware attack. We ﬁrst provide
a brief explanation on the dataset and the extraction of the IFG

3This can be achieved by repeating an action in Algorithm VI.1 when
sgn(·) = 0. A similar approach has been proposed in the algorithm that
computes an NE of discounted stochastic games in [29].

from the dataset. Then we explain the choice of parameters
used in our simulations and present the simulation results.

The dataset consists of system logs with both benign and
malicious information ﬂows recorded in a Linux computer
threatened by a ransomware attack. The goal of the ran-
somware attack is to open and read all
the ﬁles in the
./home directory of the victim computer and delete all of
these ﬁles after writing them into an encrypted ﬁle named
ransomware.encrypted. System logs were recorded by RAIN
system [13] and the targets of the ransomware attack (des-
tinations) were annotated in the system logs. Two network
sockets that indicate series of communications with external
IP addresses in the recorded system logs were identiﬁed
as the entry points of the attack. The attack consists of
three stages, where stage 1 correspond to privilege escalation,
stage 2 relate to lateral movement of the attack, and stage 3
represent achieving the goal of encrypting and deleting ./home
directory. Immediate conversion of the system logs resulted in
an information ﬂow graph, ¯G, with 173 nodes and 2426 edges.
The attack related subgraph was extracted from ¯G using the

following graph pruning steps.

1) For each pair of nodes in ¯G (e.g., process and ﬁle,
process and process), collapse any existing multiple edges
between two nodes to a single directed edge representing
the direction of the collapsed edges.

2) Extract all the nodes in ¯G that have at least one infor-
mation ﬂow path from an entry point of the attack to a
destination of stage one of the attack.

3) Extract all the nodes in ¯G that have at least one in-
formation ﬂow path from a destination of stage j to a
destination of a stage j(cid:48), for all
j, j(cid:48) ∈ {1, . . . , M} such
that j (cid:54)= j(cid:48).

4) From ¯G, extract the subgraph corresponding to the entry
points, destinations, and the set of nodes extracted in
steps 2) and 3).

5) Combine all the ﬁle-related nodes in the extracted sub-
graph corresponding to a directory into a single node
(e.g., ./home, ./user) in the victim’s computer.

6) If the resulting subgraph contains any cycles use node
versioning techniques [45] to remove cycles while pre-
serving the information ﬂow dependencies in the graph.

The resulting graph is called as the pruned IFG. The pruned
IFG corresponding to the ransomware attack contains 18 nodes
and 29 edges (Figure 1). Simulations use the following cost,
reward, and penalty parameters. Cost parameters: for all s j
i ∈ S
such that ui /∈ D j, CD(s j
i ) = −1 for j = 1, CD(s j
i ) = −2 for
j = 2, and CD(s j
i ) = −3 for j = 3. For all other states, s ∈ S,
D = 120, β 1
D = 80, α 3
CD(s) = 0. Rewards: α 1
A = 20,
D = 50, and σ 3
A = 40, β 3
β 2
D = 70. Penalties:
D = −60, β 3
D = −30, β 2
A = −20, α 2
α 1
D =
−90, σ 1
A = −70. Learning rates
used in the simulations are: δ n
ε = 0.5 if n < 7000 and
δ n
v = δ n
π = 1, if n < 7000 and
δρ =

A = −60, β 1
A = −50, and σ 3
v = δ n
κ(s,n) , otherwise. δρ = δ n
π = 1

ε = 1.6
1
1+τ(n) log(τ(n)) , δ n

τ(n) , otherwise.

A = −30, σ 2

A = −40, α 3

D = 40, α 2

A = 60, σ 1

D = 30, σ 2

Note that the learning rates remain constant until iteration
7000 and then start decaying. We observed that setting learn-
ing rates in this fashion helps the ﬁnite time convergence of the

Figure 1: IFG of ransomware attack. Nodes of the graph are
color coded to illustrate their respective types (network socket,
ﬁle, and process). Two network sockets are identiﬁed as the en-
try points of the ransomware attack. Destinations of the attack
(/usr/bin/sudo, /bin/bash, /home) are labeled in the graph.

v and δ n

algorithm. Here, the term κ(s, n) in δ n
ε denotes the total
number of times a state s ∈ S is visited from 7000th iteration
onwards in Algorithm VI.1. Hence, in our simulations, the
learning rates δ n
(s, ak)
iterates depend on the iteration n and the state visited at
iteration n. The term τ(n) = n − 6999.

k(s) iterates and δ n

ε of the ε n+1

v of vn

k

A, vn

D, vn

Figure 2: Plots of total Temporal Difference error (TD error),
φT (π n, ρ n, vn), DIFT’s TD error φD(π n, ρ n
D), and APT’s TD error
φA(π n, ρ n
A) evaluated at iterations n = 1, 500, 1000, . . . , 2.5 × 106
of Algorithm VI.1 for ransomware attack. Values of TD errors at
the nth iteration depend on the policies π n, average rewards ρ n,
and value functions vn of DIFT and APT at
the nth iteration.
φD(π n, ρ n
A) are deﬁned in Eqn. (32) and
φT (π n, ρ n, vn) = φD(π n, ρ n

D) and φA(π n, ρ n
D, vn

A, vn
D) + φA(π n, ρ n

D, vn

A, vn

A).

Conditions (11a) and (11b) in Corollary V.4 are used to
validate the convergence of Algorithm VI.1 to an ARNE
of the DIFT-APT game. Let φT (π, ρ, v) = φD(π, ρD, vD) +
φA(π, ρA, vA), where π = (πD, πA), ρ = (ρD, ρA), v = (vD, vA).
Here, φk(π, ρk, vk), for k ∈ {D, A}, is given by

φk(π, ρk, vk) = ∑
s∈S

∑
ak∈Ak(s)

(cid:16)

ρk + vk(s) − rk(s, ak, π−k)

(cid:17)
P(s(cid:48)|s, ak, π−k)vk(s(cid:48))

πk(s, ak) = 0

(32)

− ∑
s(cid:48)∈S

We refer to φT (π, ρ, v), φD(π, ρD, vD), and φA(π, ρA, vA) as the
total Temporal Difference error (TD error) , DIFT’s TD error,
and APT’s TD error, respectively. Then conditions (11a) and
(11b) in Corollary V.4 together imply that a policy pair forms

an ARNE if and only if φD(π, ρD, vD) = φA(π, ρA, vA) = 0.
Consequently, at ARNE φT (π, ρ, v) = 0. Figure 2 plots φT , φD,
and φA corresponding to the policies given by Algorithm VI.1
at iterations n = 1, 500, . . . , 2.5 × 106. The plots show that φT ,
φD and φA converge very close to 0 as n increases.

Figure 3 plots the average reward values of DIFT and APT
in Algorithm VI.1 at n = 1, 500, . . . , 2.5 × 106. Figure 3 shows
that ρ n

A converge as the iteration count n increases.

D and ρ n

Figure 3: Plots of the average rewards of DIFT (ρ n
D) and APT
(ρ n
A) at a iteration n ∈ {1, 500, 1000, . . . 2.5 × 106} of Algorithm VI.1.
Average rewards at the nth iteration depend on the policies (π n) of
DIFT and APT.

d
r
a
w
e
r

e
g
a
r
e
v
A

5

0

−5

−10

7.45

5.48

4.44

−5.87

−6.61

−9.06

DIFT

APT

ARNE policy Uniform policy

Cut policy

Figure 4: Comparison of the average rewards of DIFT and APT
obtained by the converged policies in Algorithm VI.1 (ARNE policy)
against the average rewards of the players obtained by two other
policies of DIFT: uniform policy and cut policy. Uniform policy:
DIFT chooses an action at every state under a uniform distribution.
Cut policy: DIFT performs security analysis at a destination related
state, s j
i : ui ∈ D j, with probability one whenever the state of the
game is an in-neighbor of that destination related state.

Figure 4 compares the average rewards of the players
corresponding to the converged policies in Algorithm VI.1
(ARNE policy) against the average reward values of the play-
ers corresponding to two other policies of DIFT, i) uniform
policy and ii) cut policy. Note that, in i) DIFT chooses an
action at every state under a uniform distribution. Where as
in ii) DIFT performs security analysis at a destination related
state, s j
i : ui ∈ D j, with probability one whenever the state of

the game is an in-neighbor4 of that destination related state.
APT’s policy in both uniform policy and cut policy cases are
maintained to be as same as in the case of ARNE policy case.
The results show that DIFT achieves a higher average reward
under ARNE policy when compared to the uniform and cut
policies. Further, results also suggest that APT gets a lower
reward under the DIFT’s ARNE policy when compared to the
uniform and cut policies.

VIII. CONCLUSION

In this paper we studied the problem of resource efﬁcient
and effective detection of Advanced Persistent Threats (APTs)
using Dynamic Information Flow Tracking (DIFT) detection
mechanism. We modeled the strategic interactions between
DIFT and APT as a nonzero-sum, average reward stochastic
game. Our game model captures the security costs, false-
positives, and false-negatives associated with DIFT to enable
resource efﬁcient and effective defense policies. Our model
also incorporates the information asymmetry between DIFT
and APT that arises from DIFT’s inability to distinguish
malicious ﬂows from benign ﬂows and APT’s inability to
know the locations where DIFT performs a security analysis.
the game has incomplete information as the
Additionally,
transition probabilities (false-positive and false-negative rates)
are unknown. We proposed RL-ARNE to learn an Average
Reward Nash Equilibrium (ARNE) of the DIFT-APT game.
The proposed algorithm is a multiple-time scale stochastic
approximation algorithm. We prove the convergence of RL-
ARNE algorithm to an ARNE of the DIFT-APT game.

We evaluated our game model and algorithm on a real-world
ransomware attack dataset collected using RAIN framework.
Our simulation results showed convergence of the proposed
algorithm on the ransomware attack dataset. Further the results
showed and validated the effectiveness of the proposed game
theoretic framework for devising optimal defense policies to
detect APTs. As future work we plan to investigate and model
APT attacks by multiple attackers with different capabilities.

ACKNOWLEDGMENT

The authors would like to thank Baicen Xiao at Network
Security Lab (NSL) at University of Washington (UW) for the
discussions on reinforcement learning algorithms.

REFERENCES

[1] J. Jang-Jaccard and S. Nepal, “A survey of emerging threats in cyber-
security,” Journal of Computer and System Sciences, vol. 80, no. 5, pp.
973–993, 2014.

[2] B. Watkins, “The impact of cyber attacks on the private sector,” Brieﬁng
Paper, Association for International Affair, vol. 12, pp. 1–11, 2014.
[3] M. Ussath, D. Jaeger, F. Cheng, and C. Meinel, “Advanced persistent
threats: Behind the scenes,” Annual Conference on Information Science
and Systems (CISS), pp. 181–186, 2016.

[4] N. Falliere, L. O. Murchu, and E. Chien, “W32. stuxnet dossier,” White
paper, Symantec Corp., Security Response, vol. 5, no. 6, pp. 1–69, 2011.
[5] B. Bencs´ath, G. P´ek, L. Butty´an, and M. Felegyhazi, “The cousins of
Stuxnet: Duqu, Flame, and Gauss,” Future Internet, vol. 4, no. 4, pp.
971–1003, 2012.

4A vertex ui is said to be an in-neighbor of a vertex ui(cid:48) , if there exists an

edge (ui, ui(cid:48) ) in the directed graph.

[6] T. Yadav and A. M. Rao, “Technical aspects of cyber kill chain,”
International Symposium on Security in Computing and Communication,
pp. 438–452, 2015.

[32] K. C. Nguyen, T. Alpcan, and T. Bas¸ar, “Stochastic games for security
in networks with interdependent nodes,” International Conference on
Game Theory for Networks, pp. 697–703, 2009.

[7] J. Newsome and D. Song, “Dynamic taint analysis: Automatic detection,
analysis, and signature generation of exploit attacks on commodity
software,” Network and Distributed Systems Security Symposium, 2005.
[8] J. Clause, W. Li, and A. Orso, “Dytan: A generic dynamic taint analysis
framework,” International Symposium on Software Testing and Analysis,
pp. 196–206, 2007.

[9] G. E. Suh, J. W. Lee, D. Zhang, and S. Devadas, “Secure program
execution via dynamic information ﬂow tracking,” ACM Sigplan Notices,
vol. 39, no. 11, pp. 85–96, 2004.

[10] G. Brogi and V. V. T. Tong, “TerminAPTor: Highlighting advanced
persistent threats through information ﬂow tracking,” IFIP International
Conference on New Technologies, Mobility and Security, pp. 1–5, 2016.
[11] W. Enck, P. Gilbert, S. Han, V. Tendulkar, B.-G. Chun, L. P. Cox,
J. Jung, P. McDaniel, and A. N. Sheth, “Taintdroid: An information-
ﬂow tracking system for realtime privacy monitoring on smartphones,”
ACM Transactions on Computer Systems, vol. 32, no. 2, pp. 1–5, 2014.
[12] D. Wagner and P. Soto, “Mimicry attacks on host-based intrusion de-
tection systems,” Proceedings of the 9th ACM Conference on Computer
and Communications Security, pp. 255–264, 2002.

[13] Y. Ji, S. Lee, E. Downing, W. Wang, M. Fazzini, T. Kim, A. Orso,
and W. Lee, “RAIN: Reﬁnable attack investigation with on-demand
inter-process information ﬂow tracking,” ACM SIGSAC Conference on
Computer and Communications Security, pp. 377–390, 2017.

[14] K. Jee, V. P. Kemerlis, A. D. Keromytis, and G. Portokalidis, “Shad-
owreplica: Efﬁcient parallelization of dynamic data ﬂow tracking,” ACM
SIGSAC Conference on Computer & Communications Security, pp. 235–
246, 2013.

[15] E. B. Nightingale, D. Peek, P. M. Chen, and J. Flinn, “Parallelizing
security checks on commodity hardware,” ACM Sigplan Notices, vol. 43,
no. 3, pp. 308–318, 2008.

[16] L. S. Shapley, “Stochastic games,” Proceedings of the national academy

of sciences, vol. 39, no. 10, pp. 1095–1100, 1953.

[17] R. Amir, “Stochastic games in economics and related ﬁelds: An
overview,” Stochastic Games and Applications, pp. 455–470, 2003.
[18] D. Foster and P. Young, “Stochastic evolutionary game dynamics,”

Theoretical Population Biology, vol. 38, no. 2, p. 219, 1990.

[19] Q. Zhu and T. Bas¸ar, “Robust and resilient control design for cyber-
physical systems with an application to power systems,” IEEE Decision
and Control and European Control Conference (CDC-ECC), pp. 4066–
4071, 2011.

[20] K.-w. Lye and J. M. Wing, “Game strategies in network security,”
International Journal of Information Security, vol. 4, no. 1-2, pp. 71–86,
2005.

[21] J. F. Nash, “Equilibrium points in n-person games,” Proceedings of the

national academy of sciences, vol. 36, no. 1, pp. 48–49, 1950.

[22] J. Filar and K. Vrieze, Competitive Markov Decision Processes.

Springer Science & Business Media, 2012.

[23] M. Sobel, “Noncooperative stochastic games,” The Annals of Mathemat-

ical Statistics, vol. 42, no. 6, pp. 1930–1935, 1971.

[24] J.-F. Mertens and T. Parthasarathy, “Equilibria for discounted stochastic
games,” Stochastic Games and Applications, pp. 131–172, 2003.
[25] T. Raghavan and J. A. Filar, “Algorithms for stochastic games—A
survey,” Zeitschrift f¨ur Operations Research, vol. 35, no. 6, pp. 437–
472, 1991.

[26] M. Bowling and M. Veloso, “Rational and convergent

learning in
stochastic games,” International Joint Conference on Artiﬁcial Intelli-
gence, vol. 17, no. 1, pp. 1021–1026, 2001.

[27] J. Hu and M. P. Wellman, “Nash Q-learning for general-sum stochastic
games,” Journal of Machine Learning Research, vol. 4, pp. 1039–1069,
2003.

[28] J. Li, “Learning average reward irreducible stochastic games: Analysis
and applications,” Ph.D. dissertation, Dept. Ind. Manage. Syst. Eng.,
Univ. South Florida, Tampa, FL, USA, 2003.

[29] H. L. Prasad, L. A. Prashanth, and S. Bhatnagar, “Two-timescale algo-
rithms for learning Nash equilibria in general-sum stochastic games,”
International Conference on Autonomous Agents and Multiagent Sys-
tems, pp. 1371–1379, 2015.

[30] T. Alpcan and T. Bas¸ar, Network security: A decision and game-theoretic

approach. Cambridge University Press, 2010.

[31] T. Alpcan and T. Bas¸ar, “An intrusion detection game with limited obser-
vations,” International Symposium on Dynamic Games and Applications,
2006.

[33] M. Panﬁli, A. Giuseppi, A. Fiaschetti, H. B. Al-Jibreen, A. Pietrabissa,
and F. D. Priscoli, “A game-theoretical approach to cyber-security of
critical infrastructures based on multi-agent reinforcement learning,”
in 2018 26th Mediterranean Conference on Control and Automation
(MED).

IEEE, 2018, pp. 460–465.

[34] R. Klima, K. Tuyls, and F. Oliehoek, “Markov security games: Learning
in spatial security problems,” in NIPS Workshop on Learning, Inference
and Control of Multi-Agent Systems, 2016, pp. 1–8.

[35] L. Huang and Q. Zhu, “Adaptive strategic cyber defense for advanced
persistent threats in critical infrastructure networks,” ACM SIGMETRICS
Performance Evaluation Review, vol. 46, no. 2, pp. 52–56, 2019.
[36] M. O. Sayin, H. Hosseini, R. Poovendran, and T. Bas¸ar, “A game the-
oretical framework for inter-process adversarial intervention detection,”
International Conference on Decision and Game Theory for Security,
pp. 486–507, 2018.

[37] S. Moothedath, D. Sahabandu, J. Allen, A. Clark, L. Bushnell, W. Lee,
and R. Poovendran, “A game-theoretic approach for dynamic informa-
tion ﬂow tracking to detect multi-stage advanced persistent threats,”
IEEE Transactions on Automatic Control, 2020.

[38] S. Moothedath, D. Sahabandu, A. Clark, S. Lee, W. Lee, and R. Pooven-
dran, “Multi-stage dynamic information ﬂow tracking game,” Confer-
ence on Decision and Game Theory for Security, Lecture Notes in
Computer Science, vol. 11199, pp. 80–101, 2018.

[39] D. Sahabandu, B. Xiao, A. Clark, S. Lee, W. Lee, and R. Poovendran,
“DIFT games: Dynamic information ﬂow tracking games for advanced
persistent threats,” IEEE Conference on Decision and Control (CDC),
pp. 1136–1143, 2018.

[40] D. Sahabandu, S. Moothedath, J. Allen, A. Clark, L. Bushnell, W. Lee,
and R. Poovendran, “A game theoretic approach for dynamic infor-
mation ﬂow tracking with conditional branching,” American Control
Conference (ACC), pp. 2289–2296, 2019.

[41] S. Moothedath, D. Sahabandu, J. Allen, A. Clark, L. Bushnell, W. Lee,
and R. Poovendran, “Dynamic Information Flow Tracking for Detection
of Advanced Persistent Threats: A Stochastic Game Approach,” ArXiv
e-prints, p. arXiv:2006.12327, 2020.

[42] S. Misra, S. Moothedath, H. Hosseini, J. Allen, L. Bushnell, W. Lee,
and R. Poovendran, “Learning equilibria in stochastic information ﬂow
tracking games with partial knowledge,” IEEE Conference on Decision
and Control (CDC), pp. 4053–4060, 2019.

[43] D. Sahabandu, S. Moothedath, J. Allen, L. Bushnell, W. Lee, and
R. Poovendran, “Stochastic dynamic information ﬂow tracking game
with reinforcement learning,” International Conference on Decision and
Game Theory for Security, pp. 417–438, 2019.

[44] V. S. Borkar, “Stochastic approximation with two time scales,” Systems

& Control Letters, vol. 29, no. 5, pp. 291–294, 1997.

[45] S. M. Milajerdi, R. Gjomemo, B. Eshete, R. Sekar, and V. Venkatakrish-
nan, “Holmes: real-time APT detection through correlation of suspicious
information ﬂows,” IEEE Symposium on Security and Privacy (SP), pp.
1137–1152, 2019.

[46] V. S. Borkar, Stochastic approximation: a dynamical systems viewpoint.

Springer, 2009, vol. 48.

[47] D. P. Bertsekas and J. N. Tsitsiklis, Neuro-Dynamic Programming.

Athena Scientiﬁc, 1996.

[48] J. N. Tsitsiklis, “Asynchronous stochastic approximation and Q-
Learning,” Machine learning, vol. 16, no. 3, pp. 185–202, 1994.
[49] K. Soumyanath and V. S. Borkar, “An analog scheme for ﬁxed-point
computation-part ii: Applications,” IEEE Transactions on Circuits and
Systems I: Fundamental Theory and Applications, vol. 46, no. 4, pp.
442–451, 1999.

[50] M. Metivier and P. Priouret, “Applications of a kushner and clark
lemma to general classes of stochastic algorithms,” IEEE Transactions
on Information Theory, vol. 30, no. 2, pp. 140–151, 1984.

[51] S. Bhatnagar, R. S. Sutton, M. Ghavamzadeh, and M. Lee, “Natural
actor–critic algorithms,” Automatica, vol. 45, no. 11, pp. 2471–2482,
2009.

APPENDIX

Lemma VIII.1. Consider Ωs,ak
k,π−k
∂ ∆(π)
and (14), respectively. Then,
∂ πk(s,ak) = ∑

Ωs,ak
¯k,π−k

.

¯k∈{D,A}

and ∆(π) given in Eqns. (13)

Proof. Recall k ∈ {D, A} and −k = {D, A} \ k.

(cid:34)

∆(π) = ∑
s∈S

∑
ak∈Ak(s)

Ωs,ak
k,π−k

πk(s, ak) + ∑

a−k∈A−k(s)

(cid:35)

π−k(s, a−k)

.

Ω

s,a−k
−k,πk

Taking derivative with respect to πk(s, ak) in Eqn. (14) gives,

∂ ∆(π)
∂ πk(s, ak)

= Ωs,ak
k,π−k

+ ∑

a−k∈A−k(s)

s,a−k
∂ Ω
−k,πk
∂ πk(s, ak)

π−k(s, a−k)

From Eqn. (13),

s,a−k
∂ Ω
−k,πk
∂ πk(s, ak)

= ρ−k+v−k(s)−r−k(s, ak, a−k)−∑
s(cid:48)∈S

P(s(cid:48)|s, ak, a−k)v−k(s(cid:48))

Note that,

∑
a−k∈A−k(s)
rk(s, ak, a−k) − ∑
s(cid:48)∈S

s,a−k
−k,πk

∂ Ω
∂ πk(s,ak) π−k(s, a−k) =

∑
a−k∈A−k(s)
P(s(cid:48)|s, ak, a−k)vk(s(cid:48))(cid:3)π−k(s, a−k) = Ωs,ak

(cid:2)ρk + vk(s) −

−k,π−k

Therefore,

∂ ∆(π)
∂ πk(s,ak) = ∑

¯k∈{D,A}

Ωs,ak
¯k,π−k

. This proves the result.

