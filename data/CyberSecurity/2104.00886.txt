1
2
0
2

r
p
A
2

]

B
D
.
s
c
[

1
v
6
8
8
0
0
.
4
0
1
2
:
v
i
X
r
a

Symmetric Continuous Subgraph Matching with Bidirectional
Dynamic Programming

Seunghwan Min
Seoul National University
shmin@theory.snu.ac.kr

Dora Giammarresi†
Università Roma “Tor Vergata”
giammarr@mat.uniroma2.it

Sung Gwan Park
Seoul National University
sgpark@theory.snu.ac.kr

Giuseppe F. Italiano†
LUISS University
gitaliano@luiss.it

Kunsoo Park∗
Seoul National University
kpark@theory.snu.ac.kr

Wook-Shin Han∗
Pohang University of Science and
Technology (POSTECH)
wshan@dblab.postech.ac.kr

ABSTRACT
In many real datasets such as social media streams and cyber data
sources, graphs change over time through a graph update stream
of edge insertions and deletions. Detecting critical patterns in such
dynamic graphs plays an important role in various application do-
mains such as fraud detection, cyber security, and recommendation
systems for social networks. Given a dynamic data graph and a
query graph, the continuous subgraph matching problem is to find
all positive matches for each edge insertion and all negative matches
for each edge deletion. The state-of-the-art algorithm TurboFlux
uses a spanning tree of a query graph for filtering. However, using
the spanning tree may have a low pruning power because it does
not take into account all edges of the query graph. In this paper,
we present a symmetric and much faster algorithm SymBi which
maintains an auxiliary data structure based on a directed acyclic
graph instead of a spanning tree, which maintains the intermediate
results of bidirectional dynamic programming between the query
graph and the dynamic graph. Extensive experiments with real and
synthetic datasets show that SymBi outperforms the state-of-the-
art algorithm by up to three orders of magnitude in terms of the
elapsed time.

1 INTRODUCTION
A dynamic graph is a graph that changes over time through a
graph update stream of edge insertions and deletions. In the last
decade, the topic of massive dynamic graphs has become popular.
Social media streams and cyber data sources, such as computer
network traffic and financial transaction networks, are examples
of dynamic graphs. A social media stream can be modeled as a
graph where vertices represent people, movies, or images, and edges
represent relationship such as friendship, like, post, etc. A computer
network traffic consists of vertices representing IP addresses and
edges representing protocols of network traffic [17].

Extensive research has been done for the efficient analysis of
dynamic graphs [1, 20, 23, 26, 36]. Among them, detecting critical
patterns or events in a dynamic graph is an important issue since
it lies at the core of various application domains such as fraud de-
tection [29, 32], cyber security [6, 7], and recommendation systems

∗ Contact author
† Work partially done while visiting Seoul National University

for social networks [13, 18]. For example, various cyber attacks
such as denial-of-service attack and data exfiltration attack can be
represented as graphs [6]. Moreover, US communications company
Verizon reports that 94% of the cyber security incidents fell into
nine patterns, many of which can be described as graph patterns in
their study, “2020 Data Breach Investigations Report” [38]. Cyber
security applications should detect in real-time that such graph
patterns appear in network traffic, which is one of dynamic graphs
[7].

In this paper, we focus on the problem of detecting and report-
ing such graph patterns in a dynamic graph, called continuous
subgraph matching. Many researchers have developed efficient so-
lutions for continuous subgraph matching [5, 6, 10–12, 18, 19, 28]
and its variants [9, 22, 25, 34, 39, 40] over the past decade. Due
to the NP-hardness of continuous subgraph matching, Chen et al.
[5] and Gao et al. [12] propose algorithms that cannot guarantee
the exact solution for continuous subgraph matching. The results
of these algorithms may include false positive matches, which is
far from being desirable. Since several algorithms such as IncIso-
Mat [10] and Graphflow [18] do not maintain any intermediate
results, these algorithms need to perform subgraph matching for
each graph update even if the update does not incur any match
of the pattern, which leads to significant overhead. Unlike IncI-
soMat and Graphflow, SJ-Tree [6] stores all partial matches for
each subgraph of the pattern to get better performance, but this
method requires expensive storage space. The state-of-the-art algo-
rithm TurboFlux [19] uses the idea of Turboiso [15] which is one
of state-of-the-art algorithms for the subgraph matching problem.
It proposes an auxiliary data structure called data-centric graph
(DCG), which is an updatable graph to store the partial matches for
a spanning tree of the pattern graph. TurboFlux uses less storage
space for the auxiliary data structure than SJ-Tree and outperforms
the other algorithms. According to experimental results, however,
TurboFlux has the disadvantage that processing edge deletions is
much slower than edge insertions due to the asymmetric update
process of DCG.

Previous studies show that what information is stored as interme-
diate results in an auxiliary data structure is important for solving
continuous subgraph matching. An auxiliary data structure should
be designed such that it doesn’t take long time to update while
containing enough information to help detect the pattern quickly
(i.e., balancing update time vs. amount of information to keep). It
was shown in [14] that the weak embedding of a directed acyclic

 
 
 
 
 
 
(a) Query graph 𝑞

(b) Dynamic data graph 𝑔 with an initial
data graph 𝑔0 and two edge insertions

Figure 1: A running example of query graph and data graph
for continuous subgraph matching

graph is more effective in filtering candidates than the embedding
of a spanning tree. In this paper we embed the weak embedding
into our data structure so that the intermediate results (i.e., weak
embeddings of directed acyclic graphs) contain information that
helps detect the pattern quickly and can be updated efficiently. We
propose an algorithm SymBi for continuous subgraph matching
which uses the proposed data structure. Compared to the state-
of-the-art algorithm TurboFlux, this is a substantial benefit since
directed acyclic graphs have better pruning power than spanning
trees due to non-tree edges, while the update of intermediate results
is fast. The contributions of this paper are as follows:

• We propose an auxiliary data structure called dynamic can-
didate space (DCS), which maintains the intermediate results
of bidirectional (i.e., top-down and bottom-up) dynamic pro-
gramming between a directed acyclic graph of the pattern
graph and the dynamic graph. DCS serves as a complete
search space to find all matches of the pattern graph in the
dynamic graph, and it enables us to symmetrically handle
edge insertions and edge deletions. Also, we propose an ef-
ficient algorithm to maintain DCS for each graph update.
Rather than recomputing the entire structure, this algorithm
updates only a small portion of DCS that changes.

• We introduce a new matching algorithm using DCS that
works for both edge insertions and edge deletions. Unlike
the subgraph matching problem, in continuous subgraph
matching we need to find matches that contain the updated
data graph edge. Thus, we propose a new matching order
which is different from the matching orders used in existing
subgraph matching algorithms. This matching order starts
from an edge of the query graph corresponding to the up-
dated data graph edge, and then selects a next query vertex
to match from the neighbors of the matched vertices. When
selecting the next vertex, we use an estimate of the candidate
size of the vertex instead of the exact candidate size [14]
for efficiency. In addition, we introduce the concept of iso-
lated vertices which is an extension of the leaf decomposition
technique from [3].

Experiments show that SymBi outperforms TurboFlux by up to
three orders of magnitude. In particular, when edge deletions are
included in the graph update stream, the performance gap between
the two algorithms becomes larger. In an experiment where all
query graphs are solved within the time limit by both algorithms,

(a) Spanning tree 𝑞𝑇

(b) Query DAG 𝑞ˆ

(c) Path tree of 𝑞ˆ

Figure 2: Spanning tree, DAG, and path tree for the running
example

for example, when the ratio of the number of edge deletions to
the number of edge insertions increases from 0% to 10%, the per-
formance improvement of SymBi over TurboFlux increases from
224.61 times to 309.45 times. While the deletion ratio changes from
0% to 10%, the average elapsed time of SymBi increases only 1.54
times, but TurboFlux increases 2.13 times. This supports the fact
that SymBi handles edge deletions better than TurboFlux.

The remainder of the paper is organized as follows. Section 2
formally defines the problem of continuous subgraph matching and
describes some related work. Section 3 describes a brief overview of
our algorithm. Section 4 introduces DCS and proposes an algorithm
to maintain DCS efficiently. Section 5 presents our matching algo-
rithm. Section 6 presents the results of our performance evaluation.
Finally, we conclude in Section 7.

2 PRELIMINARIES
For simplicity of presentation, we focus on undirected, connected,
and vertex-labeled graphs. Our algorithm can be easily extended to
directed or disconnected graphs with multiple labels on vertices or
edges. A graph 𝑔 is defined as (𝑉 (𝑔), 𝐸 (𝑔), 𝑙𝑔), where 𝑉 (𝑔) is a set
of vertices, 𝐸 (𝑔) is a set of edges, and 𝑙𝑔 : 𝑉 (𝑔) → Σ is a labeling
function, where Σ is a set of labels. Given 𝑆 ⊆ 𝑉 (𝑔), an induced
subgraph 𝑔[𝑆] of 𝑔 is a graph whose vertex set is 𝑆 and whose edge
set consists of all the edges in 𝐸 (𝑔) that have both endpoints in 𝑆.
A directed acyclic graph (DAG) 𝑞ˆ is a directed graph that contains
no cycles. A root (resp., leaf ) of a DAG is a vertex with no incoming
(resp., outgoing) edges. A DAG 𝑞ˆ is a rooted DAG if there is only one
root (e.g., the rooted DAG in Figure 2b can be obtained by directing
edges of 𝑞 in Figure 1a in such a way that 𝑢1 is the root). Its reverse
𝑞ˆ−1 is the same as 𝑞ˆ with all of the edges reversed. We say that 𝑢 is
a parent of 𝑣 (𝑣 is a child of 𝑢) if there exists a directed edge from 𝑢
to 𝑣. An ancestor of a vertex 𝑣 is a vertex which is either a parent
of 𝑣 or an ancestor of a parent of 𝑣. A descendant of a vertex 𝑣 is a
vertex which is either a child of 𝑣 or a descendant of a child of 𝑣. A
sub-DAG of 𝑞ˆ rooted at 𝑢, denoted by 𝑞ˆ𝑢 , is the induced subgraph
of 𝑞ˆ whose vertices set consists of 𝑢 and all the descendants of 𝑢.
The height of a rooted DAG 𝑞ˆ is the maximum distance between
the root and any other vertex in 𝑞ˆ, where the distance between two
vertices is the number of edges in a shortest path connecting them.
Let Child(𝑢), Parent(𝑢), and Nbr(𝑢) denote the children, parents,
and neighbors of 𝑢 in 𝑞ˆ, respectively.
Definition 2.1. Given a query graph 𝑞 = (𝑉 (𝑞), 𝐸 (𝑞), 𝑙𝑞) and a
data graph 𝑔 = (𝑉 (𝑔), 𝐸 (𝑔), 𝑙𝑔), a homomorphism of 𝑞 in 𝑔 is a map-
ping 𝑀 : 𝑉 (𝑞) → 𝑉 (𝑔) such that (1) 𝑙𝑞 (𝑢) = 𝑙𝑔 (𝑀 (𝑢)) for every 𝑢 ∈
𝑉 (𝑞), and (2) (𝑀 (𝑢), 𝑀 (𝑢 ′)) ∈ 𝐸 (𝑔) for every (𝑢, 𝑢 ′) ∈ 𝐸 (𝑞). An

embedding of 𝑞 in 𝑔 is an injective (i.e., ∀𝑢, 𝑢 ′ ∈ 𝑉 (𝑞) such that 𝑢 ≠
𝑢 ′ ⇒ 𝑀 (𝑢) ≠ 𝑀 (𝑢 ′)) homomorphism.

An embedding of an induced subgraph of 𝑞 in 𝑔 is called a partial
embedding of 𝑞 in 𝑔. We say that 𝑞 is subgraph-isomorphic (resp.,
subgraph-homomorphic) to 𝑔, if there is an embedding (resp., homo-
morphism) of 𝑞 in 𝑔. We use subgraph isomorphism as our default
matching semantics. Subgraph homomorphism can be easily ob-
tained by omitting the injective constraint.
Definition 2.2. [14] The path tree of a rooted DAG 𝑞ˆ is defined as
the tree 𝑞ˆ𝑇 such that each root-to-leaf path in 𝑞ˆ𝑇 corresponds to a
distinct root-to-leaf path in 𝑞ˆ, and 𝑞ˆ𝑇 shares common prefixes of its
root-to-leaf paths. Figure 2c shows the path tree of 𝑞ˆ in Figure 2b.
Definition 2.3. [14] For a rooted DAG 𝑞ˆ with root 𝑢, a weak em-
bedding 𝑀 ′ of 𝑞ˆ at 𝑣 ∈ 𝑉 (𝑔) is defined as a homomorphism of the
path tree of 𝑞ˆ in 𝑔 such that 𝑀 ′(𝑢) = 𝑣.
Example 2.1. We will use the query graph and the dynamic data
graph in Figure 1 and the DAG of the query graph in Figure 2b as a
running example throughout this paper. For example, {(𝑢3, 𝑣4), (𝑢 ′
4,
𝑣6), (𝑢 ′
5, 𝑣7), (𝑢 ′′
5 , 𝑣8)} is a weak embedding of 𝑞ˆ𝑢3
(Figure 2b) at 𝑣4 in
𝑔0 (Figure 1b), where 𝑞ˆ𝑢3
is a sub-DAG of 𝑞ˆ rooted at 𝑢3. Note that 𝑢5
in 𝑞ˆ is mapped to two different vertices 𝑣7 and 𝑣8 of 𝑔0 via the path
tree. If Δ𝑜1 is inserted to 𝑔0, {(𝑢3, 𝑣4), (𝑢 ′
5 , 𝑣7)} is
a weak embedding (also an embedding) of 𝑞ˆ𝑢3

5, 𝑣7), (𝑢 ′′

4, 𝑣6), (𝑢 ′

at 𝑣4.

Every embedding of 𝑞 in 𝑔 is a weak embedding of 𝑞ˆ in 𝑔, but
the converse is not true. Hence a weak embedding is a necessary
condition for an embedding. The weak embedding is a key notion
in our filtering.
Definition 2.4. A graph update stream Δ𝑔 is a sequence of update
operations (Δ𝑜1, Δ𝑜2, · · · ), where Δ𝑜𝑖 is a triple (op, 𝑣, 𝑣 ′) such that
𝑣, 𝑣 ′ ∈ 𝑉 (𝑔) and op is the type of the update operation which is one
of edge insertion (denoted by +) or edge deletion (denoted by −) of
an edge (𝑣, 𝑣 ′).

Update operations are defined only as inserting and deleting
edges between existing vertices, but inserting new vertices or delet-
ing existing vertices is also easy to handle. We can insert a new
vertex 𝑣 by putting 𝑣 in 𝑉 (𝑔) and defining a labeling function for 𝑣.
To delete an existing vertex 𝑣, we first delete all edges connected to
𝑣 and then remove 𝑣 from 𝑉 (𝑔) and the labeling function.
Problem Statement. Given an initial data graph 𝑔0, a graph update
stream Δ𝑔, and a query graph 𝑞, the continuous subgraph matching
problem is to find all positive/negative matches for each update
operation in Δ𝑔. For example, given a query graph 𝑞 and an initial
data graph 𝑔0 with two edge insertion operations Δ𝑜1 = (+, 𝑣4, 𝑣7)
and Δ𝑜2 = (+, 𝑣3, 𝑣6) in Figure 1, continuous subgraph matching
finds 200 positive matches when Δ𝑜2 occurs.

2.1 Related Work
Labeled Subgraph Matching. There are many studies for practi-
cal subgraph matching algorithms for labeled graphs [2, 3, 8, 14–
16, 30, 31, 33, 35, 41], which are initiated by Ullmann’s backtracking
algorithm [37]. Given a query graph 𝑞 and a data graph 𝑔, this al-
gorithm finds all embeddings by mapping a query vertex to a data
vertex one by one. Extensive research has been done to improve
the backtracking algorithm. Recently, there are many efficient algo-
rithms solving the subgraph matching problem, such as Turboiso
[15], CFL-Match [3], and DAF [14].

Table 1: Frequently used notations

Symbol

Description

𝑔
Δ𝑔
𝑞
𝑞ˆ
𝐶 (𝑢)
𝑀 (𝑢)
𝐶𝑀 (𝑢)

Data graph
Graph update stream
Query graph
Query DAG
Candidate set for query vertex 𝑢
Mapping of 𝑢 in (partial) embedding 𝑀
Set of extendable candidates of 𝑢 regarding partial
embedding 𝑀

Nbr𝑀 (𝑢) Set of matched neighbors of 𝑢 in 𝑞 regarding partial

embedding 𝑀

Turboiso finds all the embeddings of a spanning tree 𝑞𝑇 of 𝑞 (e.g.,
solid edges in Figure 2a form a spanning tree of 𝑞 in Figure 1a) in
the data graph. Based on the result, it extracts candidate regions
from the data graph that may have embeddings of the query graph,
and decides an effective matching order for each candidate region
by the path-ordering technique. Furthermore, it uses a technique
called neighborhood equivalence class, which compresses equivalent
vertices in the query graph.

CFL-Match also uses a spanning tree for filtering to solve the sub-
graph matching problem, while it proposes additional techniques to
improve Turboiso. It focuses on the fact that Turboiso may check the
non-tree edges of 𝑞 too late, and thus result in a huge search space.
To handle this issue, it proposes the core-forest-leaf decomposition
technique, which decomposes the query graph into a core includ-
ing the non-tree edges, a forest adjacent to the core, and leaves
adjacent to the forest. It is shown in [3] that this technique reduces
the search space effectively.

DAF proposes a new approach to solve the subgraph matching
problem, by building a query DAG instead of a spanning tree. It gives
three techniques to solve the subgraph matching problem using
query DAG, which are dynamic programming on DAG, adaptive
matching order with DAG ordering, and pruning by failing sets. It is
shown in [14] that the query DAG results in the high pruning power
and better matching order. For example, DAF finds that there is no
embeddings of 𝑞 in 𝑔0 in Figure 1 without backtracking process,
while Turboiso and CFL-Match need backtracking.
Continuous Subgraph Matching. Extensive studies have been
done to solve continuous subgraph matching, such as IncIsoMat
[10], Graphflow [18], SJ-Tree [6], and TurboFlux [19].

IncIsoMat finds a subgraph of a data graph that is affected by a
graph update operation, executes subgraph matching to it before
and after a graph update operation, and computes the difference
between them. The affected range within a data graph is computed
based on the diameter of a query graph, where the diameter of
a query graph 𝑞 is defined as the maximum of the length of the
shortest paths between arbitrary two vertices in 𝑞. Since subgraph
matching is an NP-hard problem, it costs a lot of time to execute
subgraph matching for each graph update operation.

Graphflow uses a worst-case optimal join algorithm [24, 27].
Starting from each query edge (𝑢, 𝑢 ′) that matches a graph edge
(𝑣, 𝑣 ′), it solves the subgraph matching starting from partial em-
bedding {(𝑢, 𝑣), (𝑢 ′, 𝑣 ′)} and incrementally joins the other edges

in the query graph until it gets the set of full embeddings of a
query graph. Since it does not maintain any intermediate results, it
starts subgraph matching from scratch every time the graph update
operation occurs.

SJ-Tree decomposes a query graph 𝑞 into smaller graphs recur-
sively until each graph consists of only one edge, and build a tree
structure called SJ-Tree based on them, where each node in the
tree corresponds to a subgraph of 𝑞. For each node, it stores an
intermediate result for subgraph matching between a data graph
and a subgraph of 𝑞 the node represents. When the graph update op-
eration occurs, it updates the intermediate results starting from the
leaves of SJ-Tree and recursively perform join operations between
the neighbors in SJ-Tree, until it reaches the root of the tree. Since
it stores all the intermediate results in an auxiliary data structure,
it may cost an exponential space to the size of the query graph.

TurboFlux uses the idea of Turboiso, and modifies it to solve
continuous subgraph matching efficiently. It maintains an auxiliary
data structure called data-centric graph, or DCG, to maintain the
intermediate results efficiently. For every pair of an edge in the
data graph and an edge in a spanning tree of 𝑞, it stores a filtering
information whether the two edges can be matched or not. For each
graph update operation, it updates whether each pair of edges in
DCG can be used to compose an embedding of a query graph, based
on edge transition model. It is shown in [19] that TurboFlux is more
than two orders of magnitude faster in solving continuous subgraph
matching than the previous results. Note that both Turboiso and
TurboFlux use a spanning tree of the query graph to filter the
candidates, while DAF uses a DAG built from the query graph for
filtering.

3 OVERVIEW OF OUR ALGORITHM
Algorithm 1 shows the overview of SymBi, which takes a data graph
𝑔, a graph update stream Δ𝑔, and a query graph 𝑞 as input, and find
all positive/negative matches of 𝑞 for each update operation in Δ𝑔.
SymBi uses three main procedures below.

1. We first build a rooted DAG 𝑞ˆ from 𝑞. In order to build 𝑞ˆ, we
traverse 𝑞 in a BFS order and direct all edges from earlier to later
visited vertices. In BuildDAG, we select a vertex as root 𝑟 such
that the DAG has the highest height. Figure 2b shows a rooted
DAG 𝑞ˆ built from query graph 𝑞 in Figure 1a when 𝑢1 is the root.
2. BuildDCS is called to build an initial DCS structure by using
bidirectional dynamic programming between the rooted DAG 𝑞ˆ
and the initial data graph 𝑔0 (Section 4.1).

3. For each update operation, we update the data graph 𝑔 and
the DCS structure, and perform continuous subgraph matching.
For insertion of edge 𝑒, we first invoke DCSChangedEdge to
compute a set 𝐸𝐷𝐶𝑆 which consists of updated edges in DCS
due to the inserted edge 𝑒. We also update the data graph by
inserting the edge 𝑒 into 𝑔 and update the DCS structure with
𝐸𝐷𝐶𝑆 (Section 4.2). Finally, we find positive matches from the
updated DCS and 𝐸𝐷𝐶𝑆 by calling the backtracking procedure
(Section 5). For deletion of edge 𝑒, we find negative matches first
and then update data structures because the information related
to 𝑒 is deleted during the update.

Algorithm 1: Continuous Subgraph Matching
Input: A data graph 𝑔, a graph update stream Δ𝑔, and a

query graph 𝑞

Output: all positive/negative matches

1 𝑞ˆ ← BuildDAG(𝑞);
2 DCS ← BuildDCS(𝑔, 𝑞ˆ);
3 foreach Δ𝑜 ∈ Δ𝑔 do
𝑒 ← (Δ𝑜.𝑣, Δ𝑜.𝑣 ′);
4
if Δ𝑜.op = + then

5

6

7

8

9

10

11

12

13

14

𝐸𝐷𝐶𝑆 ← DCSChangedEdge(𝑔, 𝑞, 𝑒);
InsertEdgeToDataGraph(𝑔, 𝑒);
DCSInsertionUpdate(DCS, 𝐸𝐷𝐶𝑆 );
FindMatches(DCS, 𝐸𝐷𝐶𝑆, ∅);

if Δ𝑜.op = − then

𝐸𝐷𝐶𝑆 ← DCSChangedEdge(𝑔, 𝑞, 𝑒);
FindMatches(DCS, 𝐸𝐷𝐶𝑆, ∅);
DeleteEdgeFromDataGraph(𝑔, 𝑒);
DCSDeletionUpdate(DCS, 𝐸𝐷𝐶𝑆 );

4 DCS STRUCTURE
4.1 DCS Structure
To deal with continuous subgraph matching, we introduce an auxil-
iary data structure called the dynamic candidate space (DCS) which
stores weak embeddings of DAGs as intermediate results that help
reduce the search space of backtracking based on the fact that a
weak embedding is a necessary condition for an embedding. These
intermediate results are obtained through top-down and bottom-up
dynamic programming between a DAG of a query graph and a
dynamic data graph. Compared to the auxiliary data structure DCG
used in TurboFlux, DCS has non-tree edge information which DCG
does not have, so it is advantageous in the backtracking process.
The auxiliary data structure CS (Candidate Space) which DAF [14]
uses to solve the subgraph matching problem does not store inter-
mediate results, and thus it cannot respond efficiently to the update
operations.
DCS Structure. Given a rooted DAG 𝑞ˆ from 𝑞 and a data graph 𝑔,
a DCS on 𝑞ˆ and 𝑔 consists of the following.

• For each 𝑢 ∈ 𝑉 (𝑞), a candidate set 𝐶 (𝑢) is a set of vertices
𝑣 ∈ 𝑉 (𝑔) such that 𝑙𝑞 (𝑢) = 𝑙𝑔 (𝑣). Let ⟨𝑢, 𝑣⟩ denote 𝑣 in 𝐶 (𝑢).
• For each 𝑢 ∈ 𝑉 (𝑞) and 𝑣 ∈ 𝐶 (𝑢), 𝐷1 [𝑢, 𝑣] = 1 if there
𝑢 at 𝑣; 𝐷1 [𝑢, 𝑣] = 0

exists a weak embedding of sub-DAG 𝑞ˆ−1
otherwise.

• For each 𝑢 ∈ 𝑉 (𝑞) and 𝑣 ∈ 𝐶 (𝑢), 𝐷2 [𝑢, 𝑣] = 1 if there
exists a weak embedding 𝑀 ′ of sub-DAG 𝑞ˆ𝑢 at 𝑣 such that
𝐷1 [𝑢 ′, 𝑣 ′] = 1 for every mapping (𝑢 ′, 𝑣 ′) ∈ 𝑀 ′; 𝐷2 [𝑢, 𝑣] = 0
otherwise.

• There is an edge (⟨𝑢, 𝑣⟩, ⟨𝑢 ′, 𝑣 ′⟩) between ⟨𝑢, 𝑣⟩ and ⟨𝑢 ′, 𝑣 ′⟩
if and only if (𝑢, 𝑢 ′) ∈ 𝐸 (𝑞) and (𝑣, 𝑣 ′) ∈ 𝐸 (𝑔). We say that
⟨𝑢, 𝑣⟩ is a parent (or child) of ⟨𝑢 ′, 𝑣 ′⟩ if 𝑢 is a parent (or child)
of 𝑢 ′ in 𝑞ˆ.

The DCS structure can be viewed as a labeled graph (labeled
with 𝐷1 and 𝐷2) whose vertices are ⟨𝑢, 𝑣⟩’s and edges are (⟨𝑢, 𝑣⟩,
⟨𝑢 ′, 𝑣 ′⟩)’s. Note that the intermediate results 𝐷1 and 𝐷2 which

(a) Initial DCS0

(b) DCS1 after Δ𝑜1 = (+, 𝑣4, 𝑣7) occurs

(c) DCS2 after Δ𝑜2 = (+, 𝑣3, 𝑣6) occurs

Figure 3: A running example of DCS structure on DAG 𝑞ˆ in Figure 2b and the dynamic data graph 𝑔 in Figure 1b

DCS stores are weak embeddings of sub-DAGs. 𝐷1 and 𝐷2 store
the results of top-down and bottom-up dynamic programming,
respectively, which are used to filter candidates. For any embedding
𝑀 of 𝑞 in 𝑔, 𝐷2 [𝑢, 𝑣] = 1 must hold for every (𝑢, 𝑣) ∈ 𝑀, since a
weak embedding of a sub-DAG of 𝑞 is a necessary condition for
an embedding of 𝑞. From this observation, we need only consider
(𝑢, 𝑣) pairs such that 𝐷2 [𝑢, 𝑣] = 1 when computing an embedding
of 𝑞 in 𝑔.
Example 4.1. Figure 3 shows the DCS structure on the DAG 𝑞ˆ
in Figure 2b and the dynamic data graph 𝑔 in Figure 1b. Figure
3a shows the initial DCS0 on 𝑞ˆ in Figure 2b and 𝑔0 in Figure 1b.
Figure 3b and 3c show DCS after Δ𝑜1 and Δ𝑜2 occur, respectively.
Dashed lines (⟨𝑢1, 𝑣7⟩, ⟨𝑢3, 𝑣4⟩) and (⟨𝑢3, 𝑣4⟩, ⟨𝑢4, 𝑣7⟩) in Figure 3b
represent inserted edges due to Δ𝑜1. Note that multiple edges can
be inserted to DCS by one edge insertion to the data graph. In the
initial DCS0 (Figure 3a), 𝐶 (𝑢2) = {𝑣3, 𝑣5, 𝑣6} because 𝑣3, 𝑣5, and
𝑣6 have the same label as 𝑢2, 𝐷1 [𝑢2, 𝑣3] = 1 since there exists a
weak embedding 𝑀 ′ = {(𝑢2, 𝑣3), (𝑢1, 𝑣1)} of sub-DAG 𝑞ˆ−1
at 𝑣3,
𝑢2
and 𝐷2 [𝑢2, 𝑣3] = 0 because there is no weak embedding of sub-
DAG 𝑞ˆ𝑢2
at 𝑣3. Since there are no (𝑢, 𝑣) pairs such that 𝐷2 [𝑢, 𝑣] = 1
in Figure 3b, SymBi reports that there are no positive matches for
Δ𝑜1 without backtracking. In contrast, TurboFlux, which uses the
spanning tree in Figure 2a, needs to perform backtracking only to
find that there are no positive matches for Δ𝑜1, because there exists
a spanning tree that includes the inserted edge (𝑣4, 𝑣7) in the data
graph.

To compute 𝐷1 and 𝐷2, we use following recurrences which can

be obtained from the definition:

𝐷1 [𝑢, 𝑣] = 1 iff ∃𝑣𝑝 ∈ 𝐶 (𝑢𝑝 ) adjacent to 𝑣 such that 𝐷1 [𝑢𝑝, 𝑣𝑝 ] = 1
(1)

for every parent 𝑢𝑝 of 𝑢 in 𝑞ˆ

𝐷2 [𝑢, 𝑣] = 1 iff 𝐷1 [𝑢, 𝑣] = 1 and ∃𝑣𝑐 ∈ 𝐶 (𝑢𝑐 ) adjacent to 𝑣 such

that 𝐷2 [𝑢𝑐, 𝑣𝑐 ] = 1 for every child 𝑢𝑐 of 𝑢 in 𝑞ˆ

(2)

Based on the above recurrences, we can compute 𝐷1 and 𝐷2 by
dynamic programming in a top-down and bottom-up fashion in

DAG 𝑞ˆ, respectively. Note that we reverse the parent-child relation-
ship in the first recurrence in order to take only one DAG 𝑞ˆ into
account.
Lemma 4.1. Recurrences (1) and (2) correctly compute 𝐷1 and 𝐷2
according to the definition.
Lemma 4.2. Given a query graph 𝑞 and a data graph 𝑔, the space
complexity of the DCS structure and the time complexity of DCS
construction are 𝑂 (|𝐸 (𝑞)| × |𝐸 (𝑔)|).

4.2 DCS Update
In this subsection, we describe how to update the DCS structure
for each update operation. An edge update in a data graph causes
insertion or deletion of a set of edges in DCS, and makes changes
on 𝐷1 and 𝐷2. Since the update algorithm works symmetrically for
edge insertions and edge deletions, we describe how to update 𝐷1
and 𝐷2 when an edge is inserted, and then describe what changes
when an edge is deleted.

We first explain DCSChangedEdge (Lines 6 and 11 in Algorithm
1) which returns a set of inserted/deleted edges in DCS due to the
updated edge 𝑒 = (𝑣, 𝑣 ′). We traverse the query graph and find
an edge (𝑢, 𝑢 ′) such that 𝑙𝑞 (𝑢) = 𝑙𝑔 (𝑣) and 𝑙𝑞 (𝑢 ′) = 𝑙𝑔 (𝑣 ′). We
then insert the edge (⟨𝑢, 𝑣⟩, ⟨𝑢 ′, 𝑣 ′⟩) into the set 𝐸𝐷𝐶𝑆 . In Figure 1,
DCSChangedEdge returns {(⟨𝑢2, 𝑣3⟩, ⟨𝑢4, 𝑣6⟩), (⟨𝑢2, 𝑣6⟩, ⟨𝑢4, 𝑣3⟩)}
when Δ𝑜2 = (+, 𝑣3, 𝑣6) occurs.
Edge Insertion. Now, we focus on updating 𝐷1 for the case of
edge insertion. Obviously, it is inefficient to recompute the entire
process of top-down dynamic programming to update 𝐷1 for each
update. Instead of computing the whole 𝐷1, we want to compute
only the elements of 𝐷1 whose values may change. To update 𝐷1,
we start with an edge (⟨𝑢𝑝, 𝑣𝑝 ⟩, ⟨𝑢, 𝑣⟩) of 𝐸𝐷𝐶𝑆 where ⟨𝑢𝑝, 𝑣𝑝 ⟩ is
a parent of ⟨𝑢, 𝑣⟩. If 𝐷1 [𝑢𝑝, 𝑣𝑝 ] is 0, then the edge (⟨𝑢𝑝, 𝑣𝑝 ⟩, ⟨𝑢, 𝑣⟩)
does not affect 𝐷1 [𝑢, 𝑣] nor its descendants, so we stop the update
and move to the next edge of 𝐸𝐷𝐶𝑆 . On the other hand, if 𝐷1 [𝑢𝑝, 𝑣𝑝 ]
is 1, then 𝐷1 of ⟨𝑢, 𝑣⟩ and its descendants may be changed due to
this edge. First, we compute 𝐷1 [𝑢, 𝑣]. If 𝐷1 [𝑢, 𝑣] changes from 0 to
1, then we repeat this process for the children of ⟨𝑢, 𝑣⟩ until 𝐷1 has
no changes. Next, we try to update 𝐷1 with the next edge in 𝐸𝐷𝐶𝑆 .

Example 4.2. When Δ𝑜2 occurs, we update 𝐷1 in Figure 3b with a
set 𝐸𝐷𝐶𝑆 = {(⟨𝑢2, 𝑣3⟩, ⟨𝑢4, 𝑣6⟩), (⟨𝑢2, 𝑣6⟩, ⟨𝑢4, 𝑣3⟩)}. As mentioned
above, we start with the edge (⟨𝑢2, 𝑣3⟩, ⟨𝑢4, 𝑣6⟩). Since 𝐷1 [𝑢2, 𝑣3] is
1, we recompute 𝐷1 [𝑢4, 𝑣6] and it changes from 0 to 1 because now
every parent of 𝑢4 has a candidate adjacent to ⟨𝑢4, 𝑣6⟩ whose 𝐷1
value is 1. Since 𝐷1 [𝑢4, 𝑣6] becomes 1, we iterate for the children of
⟨𝑢4, 𝑣6⟩ (e.g., ⟨𝑢5, 𝑣7⟩, . . . , ⟨𝑢5, 𝑣106⟩), and then we stop updating 𝐷1
because 𝑢5 has no children. Now, we update 𝐷1 with the next edge
(⟨𝑢2, 𝑣6⟩, ⟨𝑢4, 𝑣3⟩). Similarly to the previous case, we recompute
𝐷1 [𝑢4, 𝑣3], but it remains 0 because there are no edges between
⟨𝑢4, 𝑣3⟩ and ⟨𝑢3, 𝑣4⟩. So, we stop the update with (⟨𝑢2, 𝑣6⟩, ⟨𝑢4, 𝑣3⟩).
Since 𝐸𝐷𝐶𝑆 has no more edges, we finish the update and obtain 𝐷1
in Figure 3c.

We can see that there are two cases that ⟨𝑢𝑝, 𝑣𝑝 ⟩ affects 𝐷1 [𝑢, 𝑣]:
(i) If 𝐷1 [𝑢𝑝, 𝑣𝑝 ] = 1 and an edge between ⟨𝑢𝑝, 𝑣𝑝 ⟩ and ⟨𝑢, 𝑣⟩ is

inserted.

(ii) If 𝐷1 [𝑢𝑝, 𝑣𝑝 ] changes from 0 to 1 and there is an edge between

⟨𝑢𝑝, 𝑣𝑝 ⟩ and ⟨𝑢, 𝑣⟩.

In both of these cases, we say that ⟨𝑢𝑝, 𝑣𝑝 ⟩ is an updated parent of
⟨𝑢, 𝑣⟩. In Example 4.2, ⟨𝑢2, 𝑣3⟩ is an updated parent of ⟨𝑢4, 𝑣6⟩ from
case (i) and ⟨𝑢4, 𝑣6⟩ is an updated parent of its children from case
(ii).

However, the above method has redundant computations in
two aspects. First, if ⟨𝑢, 𝑣⟩ has 𝑛 updated parents then the above
method computes 𝐷1 [𝑢, 𝑣] 𝑛 times in the worst case. Second, to
compute 𝐷1 [𝑢, 𝑣], we need to reference the non-updated parents
of ⟨𝑢, 𝑣⟩ even if they do not change during the update. To handle
these issues, we store additional information between ⟨𝑢, 𝑣⟩ and its
parents. When ⟨𝑢𝑝, 𝑣𝑝 ⟩ becomes an updated parent of ⟨𝑢, 𝑣⟩, instead
of computing 𝐷1 [𝑢, 𝑣] from scratch, we update the information of
⟨𝑢, 𝑣⟩ related to ⟨𝑢𝑝, 𝑣𝑝 ⟩, and then update 𝐷1 [𝑢, 𝑣] using the stored
information.

We store the aforementioned information using two additional

arrays, 𝑁 1
• 𝑁 1

𝑃 [𝑢, 𝑣]:

𝑢,𝑣 [𝑢𝑝 ] and 𝑁 1
𝑢,𝑣 [𝑢𝑝 ] stores the number of candidates 𝑣𝑝 of 𝑢𝑝 such that
there exists an edge (⟨𝑢𝑝, 𝑣𝑝 ⟩, ⟨𝑢, 𝑣⟩) and 𝐷1 [𝑢𝑝, 𝑣𝑝 ] = 1,
where 𝑢𝑝 is a parent of 𝑢. For example, 𝑁 1
𝑢2,𝑣3 [𝑢1] = 2 in
Figure 3b because 𝑣1 and 𝑣2 in 𝐶 (𝑢1) satisfy the condition. By
definition of updated parents and 𝑁 1
𝑢,𝑣 [𝑢𝑝 ], we can easily up-
date 𝑁 1
𝑢,𝑣 [𝑢𝑝 ] while updating DCS: when ⟨𝑢𝑝, 𝑣𝑝 ⟩ becomes
an updated parent of ⟨𝑢, 𝑣⟩, we increase 𝑁 1
• 𝑁 1
𝑃 [𝑢, 𝑣] stores the number of parents 𝑢𝑝 of 𝑢 such that
𝑁 1
𝑢,𝑣 [𝑢𝑝 ] ≠ 0. When 𝑁 1
𝑢,𝑣 [𝑢𝑝 ] changes from 0 to 1 during the
update, we increase 𝑁 1
𝑃 [𝑢, 𝑣] by 1. We can update 𝐷1 [𝑢, 𝑣]
using 𝑁 1
𝑃 [𝑢, 𝑣] from the following equation obtained from
Recurrence (1) in Section 4.1:
𝐷1 [𝑢, 𝑣] = 1 if and only if 𝑁 1

𝑃 [𝑢, 𝑣] = |Parent(𝑢)|.

𝑢,𝑣 [𝑢𝑝 ] by 1.

Back to the situation in Example 4.2, we increase 𝑁 1

𝑢4,𝑣6 [𝑢2] by
1 instead of recomputing 𝐷1 [𝑢4, 𝑣6]. Since 𝑁 1
𝑢4,𝑣6 [𝑢2] becomes 1
from 0, we increase 𝑁 1
𝑃 [𝑢4, 𝑣6] =
|Parent(𝑢4)| = 2 now holds, 𝐷1 [𝑢4, 𝑣6] becomes 1. Thus, we can
update 𝐷1 correctly without redundant computations.

𝑃 [𝑢4, 𝑣6] from 1 to 2. Because 𝑁 1

The revised method solves the two problems described earlier.
The first problem is solved in two aspects. First, the revised method

still performs the update for ⟨𝑢, 𝑣⟩ as many times as the number
of updated parents of ⟨𝑢, 𝑣⟩. However, when there is an updated
parent of ⟨𝑢, 𝑣⟩, we update the corresponding arrays and 𝐷1 [𝑢, 𝑣]
in constant time. So, during the 𝐷1 update, the total computational
cost to update 𝐷1 [𝑢, 𝑣] is proportional to the number of updated
parents of ⟨𝑢, 𝑣⟩. Second, even if we update 𝐷1 [𝑢, 𝑣] more than
once, ⟨𝑢, 𝑣⟩ affects its children at most once because ⟨𝑢, 𝑣⟩ affects
its children only when 𝐷1 [𝑢, 𝑣] changes from 0 to 1. The second
problem is solved because now we update only the information of
⟨𝑢, 𝑣⟩ related to its updated parents (i.e., 𝑁 1
𝑃 [𝑢, 𝑣]) during
the update process.

𝑢,𝑣 and 𝑁 1

Similarly with the 𝐷1 update, we can define updated child, 𝑁 2

𝑢,𝑣,

and 𝑁 2

𝐶 to update 𝐷2 efficiently in a bottom-up fashion.
• 𝑁 2

𝑢,𝑣 [𝑢 ′] stores the number of candidates 𝑣 ′ of 𝑢 ′ such that
there exists an edge (⟨𝑢 ′, 𝑣 ′⟩, ⟨𝑢, 𝑣⟩) and 𝐷2 [𝑢 ′, 𝑣 ′] = 1, where
𝑢 ′ is a neighbor of 𝑢.

• 𝑁 2
𝐶 [𝑢, 𝑣] stores the number of children 𝑢𝑐 of 𝑢 such that
𝑁 2
𝑢,𝑣 [𝑢𝑐 ] ≠ 0.
While we need to define 𝑁 2
𝑢,𝑣 only for the children of 𝑢 in the 𝐷2
update, we define 𝑁 2
𝑢,𝑣 for every neighbor of 𝑢 in order to use it
in the backtracking process (Section 5.2). The difference between
the 𝐷1 update and the 𝐷2 update arises from the condition that
𝐷1 [𝑢, 𝑣] should be 1 in order for 𝐷2 [𝑢, 𝑣] to be 1. There is one
more case where 𝐷2 [𝑢, 𝑣] changes from 0 to 1, except when it
changes due to its updated children. If 𝐷1 [𝑢, 𝑣] becomes 1 and
𝑁 2
𝐶 [𝑢, 𝑣] = |Child(𝑢)| already holds, 𝐷2 [𝑢, 𝑣] changes from 0 to 1.
For example, 𝐷2 [𝑢5, 𝑣7] in Figure 3c changes to 1 after 𝐷1 [𝑢5, 𝑣7]
changes to 1, and then ⟨𝑢5, 𝑣7⟩ becomes an updated child of its
parents.

Algorithm 2 shows the process of updating 𝐷1, 𝐷2 and the addi-
tional arrays for edge insertion. There are two queues 𝑄1 and 𝑄2
which store ⟨𝑢, 𝑣⟩ such that 𝐷1 [𝑢, 𝑣] and 𝐷2 [𝑢, 𝑣] changed from 0 to
1, respectively. DCSInsertionUpdate performs the update process
described above for each inserted edge (⟨𝑢1, 𝑣1⟩, ⟨𝑢2, 𝑣2⟩) in 𝐸𝐷𝐶𝑆 .
Suppose that ⟨𝑢1, 𝑣1⟩ is a parent of ⟨𝑢2, 𝑣2⟩. Lines 5-8 describe the up-
date by case (i) of updated parents and updated children. It invokes
the following two algorithms. InsertionTopDown (Algorithm 3)
updates 𝑁 1
𝑃 [𝑢𝑐, 𝑣𝑐 ], and 𝐷1 [𝑢𝑐, 𝑣𝑐 ] when ⟨𝑢, 𝑣⟩ is an up-
dated parent of ⟨𝑢𝑐, 𝑣𝑐 ⟩. Also, when 𝐷1 [𝑢𝑐, 𝑣𝑐 ] becomes 1, it pushes
⟨𝑢𝑐, 𝑣𝑐 ⟩ into 𝑄1 and check the condition (𝑁 2
𝑐 [𝑢, 𝑣] = |Child(𝑢)|) to
see if 𝐷2 [𝑢𝑐, 𝑣𝑐 ] can change to 1. InsertionBottomUp (Algorithm
4) works similarly. Lines 11-14 (or Lines 15-18) perform the update
process of case (ii) of updated parents (or updated children) until
𝑄1 (or 𝑄2) is not empty.

𝑢𝑐,𝑣𝑐 [𝑢], 𝑁 1

Now we show that Algorithm 2 correctly updates 𝐷1 and 𝐷2 for

the edge insertion.
Lemma 4.3. If we have a correct DCS, and edges in 𝐸𝐷𝐶𝑆 are
inserted into DCS by running Algorithm 2, the DCS structure is
still correct after the insertion.
Edge Deletion. We can update DCS for edge deletion with a small
modification of the previous method. The first case of the updated
parent (or updated child) is changed to when an edge is deleted
and the second case is changed to when 𝐷1 [𝑢𝑝, 𝑣𝑝 ] (or 𝐷2 [𝑢𝑝, 𝑣𝑝 ])
changes from 1 to 0. Next, if 𝐷2 [𝑢, 𝑣] = 1 and 𝐷1 [𝑢, 𝑣] becomes 0
during the 𝐷1 update, then 𝐷2 [𝑢, 𝑣] also changes to 0.

4

5

6

7

8

9

10

11

12

13

14

15

16

17

18

19

20

4

5

6

7

8

Algorithm 2: DCSInsertionUpdate(𝐷𝐶𝑆, 𝐸𝐷𝐶𝑆 )
1 𝑄1, 𝑄2 ← empty queue;
2 foreach (⟨𝑢1, 𝑣1⟩, ⟨𝑢2, 𝑣2⟩) ∈ 𝐸𝐷𝐶𝑆 do
3

if ⟨𝑢2, 𝑣2⟩ is a parent of ⟨𝑢1, 𝑣1⟩ then

swap(⟨𝑢1, 𝑣1⟩, ⟨𝑢2, 𝑣2⟩);

if 𝐷1 [𝑢1, 𝑣1] = 1 then

InsertionTopDown(⟨𝑢1, 𝑣1⟩, ⟨𝑢2, 𝑣2⟩);

if 𝐷2 [𝑢2, 𝑣2] = 1 then

InsertionBottomUp(⟨𝑢2, 𝑣2⟩, ⟨𝑢1, 𝑣1⟩);

if 𝐷2 [𝑢1, 𝑣1] = 1 then
𝑢2,𝑣2 [𝑢1] ← 𝑁 2
𝑁 2

𝑢2,𝑣2 [𝑢1] + 1

while 𝑄1 ≠ ∅ do

⟨𝑢, 𝑣⟩ ← 𝑄1.𝑝𝑜𝑝;
foreach ⟨𝑢𝑐, 𝑣𝑐 ⟩ which is a child of ⟨𝑢, 𝑣⟩ do
InsertionTopDown(⟨𝑢, 𝑣⟩, ⟨𝑢𝑐, 𝑣𝑐 ⟩);

while 𝑄2 ≠ ∅ do

⟨𝑢, 𝑣⟩ ← 𝑄2.𝑝𝑜𝑝;
foreach ⟨𝑢𝑝, 𝑣𝑝 ⟩ which is a parent of ⟨𝑢, 𝑣⟩ do
InsertionBottomUp(⟨𝑢, 𝑣⟩, ⟨𝑢𝑝, 𝑣𝑝 ⟩);
foreach ⟨𝑢𝑐, 𝑣𝑐 ⟩ which is a child of ⟨𝑢, 𝑣⟩ do

𝑁 2
𝑢𝑐,𝑣𝑐 [𝑢] ← 𝑁 2

𝑢𝑐,𝑣𝑐 [𝑢] + 1

Algorithm 3: InsertionTopDown(⟨𝑢, 𝑣⟩, ⟨𝑢𝑐, 𝑣𝑐 ⟩)
1 if 𝑁 1
𝑢𝑐,𝑣𝑐 [𝑢] = 0 then
𝑁 1
𝑃 [𝑢𝑐, 𝑣𝑐 ] ← 𝑁 1
if 𝑁 1

𝑃 [𝑢𝑐, 𝑣𝑐 ] + 1 ;

2

3

𝑃 [𝑢𝑐, 𝑣𝑐 ] = |Parent(𝑢𝑐 )| then
𝐷1 [𝑢𝑐, 𝑣𝑐 ] ← 1;
𝑄1.𝑝𝑢𝑠ℎ(⟨𝑢𝑐, 𝑣𝑐 ⟩);
if 𝑁 2

𝐶 [𝑢𝑐, 𝑣𝑐 ] = |Child(𝑢𝑐 )| then
𝐷2 [𝑢𝑐, 𝑣𝑐 ] ← 1;
𝑄2.𝑝𝑢𝑠ℎ(⟨𝑢𝑐, 𝑣𝑐 ⟩);

9 𝑁 1

𝑢𝑐,𝑣𝑐 [𝑢] ← 𝑁 1

𝑢𝑐,𝑣𝑐 [𝑢] + 1

2

Algorithm 4: InsertionBottomUp(⟨𝑢, 𝑣⟩, ⟨𝑢𝑝, 𝑣𝑝 ⟩)
1 if 𝑁 2
𝑢𝑝,𝑣𝑝 [𝑢] = 0 then
𝑁 2
𝐶 [𝑢𝑝, 𝑣𝑝 ] ← 𝑁 2
if 𝐷1 [𝑢𝑝, 𝑣𝑝 ] = 1 and 𝑁 2
𝐷2 [𝑢𝑝, 𝑣𝑝 ] ← 1;
𝑄2.𝑝𝑢𝑠ℎ(⟨𝑢𝑝, 𝑣𝑝 ⟩);

𝐶 [𝑢𝑝, 𝑣𝑝 ] + 1 ;

4

3

5

𝐶 [𝑢𝑝, 𝑣𝑝 ] = |Child(𝑢𝑝 )| then

6 𝑁 2

𝑢𝑝,𝑣𝑝 [𝑢] ← 𝑁 2

𝑢𝑝,𝑣𝑝 [𝑢] + 1

Lemma 4.4. Let 𝑃 be the set of DCS vertices ⟨𝑢, 𝑣⟩ such that
𝐷1 [𝑢, 𝑣] or 𝐷2 [𝑢, 𝑣] is changed during the update. Then the time
complexity of the DCS update is 𝑂 (∑︁𝑝 ∈𝑃 deg(𝑝) + |𝐸𝐷𝐶𝑆 |), where
deg(𝑝) is the number of edges connected to 𝑝. Also, the space com-
plexity of the DCS update excluding DCS itself is 𝑂 (|𝐸 (𝑞)| × |𝑉 (𝑔)|).

In the worst case, almost all 𝐷1 [𝑢, 𝑣] and 𝐷2 [𝑢, 𝑣] in DCS may
be changed and the time complexity becomes 𝑂 (|𝐸 (𝑞)| × |𝐸 (𝑔)|),
so there is no difference from recomputing DCS from scratch. In
Section 6, however, we will show that there are very few changes
in 𝐷1 [𝑢, 𝑣] or 𝐷2 [𝑢, 𝑣] in practice, so our proposed update method
is efficient.

5 BACKTRACKING
In this section, we present our matching algorithm to find all pos-
itive/negative matches in the DCS structure. Our matching algo-
rithm works regardless of the cases of edge insertion and edge
deletion.

5.1 Backtracking Framework
We find matches by gradually extending a partial embedding until
we get an (full) embedding of 𝑞 in 𝑔. We extend a partial embedding
by matching an extendable vertex of 𝑞, which is defined as below.
Definition 5.1. Given a partial embedding 𝑀, a vertex 𝑢 of query
graph 𝑞 is called extendable if 𝑢 is not mapped to a vertex of 𝑔 in 𝑀
and at least one neighbor of 𝑢 is mapped to a vertex of 𝑔 in 𝑀.

Note that the definition of extendable vertices is different from
that of DAF [14], which requires all parents of 𝑢 to be mapped to a
vertex of 𝑔 while we require only one neighbor of 𝑢. The difference
occurs because DAF has a fixed query DAG and a root vertex for
backtracking, while our algorithm has to start backtracking from
an arbitrary edge.

We start by mapping one edge from 𝐸𝐷𝐶𝑆 , since we need only
find matches including at least one updated edge in 𝐸𝐷𝐶𝑆 . Until
we find a full embedding, we recursively perform the following
steps. First, we find all extendable vertices, and choose one vertex
among them according to the matching order. Once we decide an
extendable vertex 𝑢 to match, we compute its extendable candidates,
which are the vertices in the data graph that can be matched to 𝑢.
Formally, we define an extendable candidate as follows.
Definition 5.2. Given a query vertex 𝑢, a data vertex 𝑣 is its ex-
tendable candidate if 𝑣 satisfies the following conditions:

1. 𝐷2 [𝑢, 𝑣] = 1 (i.e., it is not filtered in the DCS structure)
2. For all matched neighbors 𝑢 ′ of 𝑢, (𝑀 (𝑢 ′), 𝑣) ∈ 𝐸 (𝑔)

The set of extendable candidates of 𝑢 is denoted by 𝐶𝑀 (𝑢). Fi-
nally, we extend the partial embedding by matching 𝑢 to one of its
extendable candidates and continue the process.

Algorithm 5 shows the overall backtracking process. This al-
gorithm is invoked with 𝑀 = ∅ in Algorithm 1. For each edge
(⟨𝑢, 𝑣⟩, ⟨𝑢 ′, 𝑣 ′⟩) in 𝐸𝐷𝐶𝑆 , we start backtracking in Lines 6-11 only
when 𝐷2 [𝑢, 𝑣] = 𝐷2 [𝑢 ′, 𝑣 ′] = 1 (i.e., none of the pairs are filtered).
We recursively extend a partial embedding in Lines 13-19. If we get
a full embedding, we output it as a match in Line 2. UpdateEmbed-
ding and RestoreEmbedding maintain additional values related
to the matching order, every time a new match is augmented to 𝑀
(i.e., 𝑀 is updated) or an existing match is removed from 𝑀 (i.e., 𝑀
is restored). In Section 5.3, we explain what these functions do in
more detail.

Algorithm 5: FindMatches(DCS, 𝐸𝐷𝐶𝑆, 𝑀)
Input: DCS, 𝐸𝐷𝐶𝑆 , and a partial embedding 𝑀
Output: all positive/negative matches including an edge in

𝐸𝐷𝐶𝑆

iterate through 𝑆𝑢𝑚𝑖𝑛 , which has a considerably smaller size than
the number of vertices 𝑣 with 𝐷2 [𝑢, 𝑣] = 1 in usual. Algorithm 6
shows an algorithm to compute 𝐶𝑀 (𝑢).

5

6

7

8

9

10

11

14

15

16

17

18

19

12 else
13

Report 𝑀 as a match;

1 if |𝑀 | = |𝑉 (𝑞)| then
2
3 else if |𝑀 | = 0 then
4

foreach (⟨𝑢, 𝑣⟩, ⟨𝑢 ′, 𝑣 ′⟩) ∈ 𝐸𝐷𝐶𝑆 do

if 𝐷2 [𝑢, 𝑣] = 1 and 𝐷2 [𝑢 ′, 𝑣 ′] = 1 then

𝑀 ← {(𝑢, 𝑣), (𝑢 ′, 𝑣 ′)};
UpdateEmbedding(𝑀, 𝑢);
UpdateEmbedding(𝑀, 𝑢 ′);
FindMatches(DCS, 𝐸𝐷𝐶𝑆, 𝑀);
RestoreEmbedding(𝑀, 𝑢 ′);
RestoreEmbedding(𝑀, 𝑢);

𝑢 ← next vertex according to the matching order;
Compute 𝐶𝑀 (𝑢);
foreach 𝑣 ∈ 𝐶𝑀 (𝑢) do
𝑀 ′ ← 𝑀 ∪ {(𝑢, 𝑣)};
UpdateEmbedding(𝑀, 𝑢);
FindMatches(DCS, 𝐸𝐷𝐶𝑆, 𝑀 ′);
RestoreEmbedding(𝑀, 𝑢);

5.2 Computing Extendable Candidates
According to Definition 5.2, the set of extendable candidates 𝐶𝑀 (𝑢)
of an extendable vertex 𝑢 is defined as follows:

𝐶𝑀 (𝑢) = {𝑣 : 𝐷2 [𝑢, 𝑣] = 1, ∀𝑢 ′ ∈ Nbr𝑀 (𝑢), (𝑣, 𝑀 (𝑢 ′)) ∈ 𝐸 (𝑔)},

where Nbr𝑀 (𝑢) represents the set of matched neighbors of 𝑢.

We can compute the extendable candidates of 𝑢 based on the
above equation. Even though we can compute 𝐶𝑀 (𝑢) by naively
iterating through all data vertices 𝑣 with 𝐷2 [𝑢, 𝑣] = 1 and checking
whether (𝑀 (𝑢 ′), 𝑣) ∈ 𝐸 (𝑔) for all matched neighbors 𝑢 ′ of 𝑢, there
can be a large number of 𝑣’s with 𝐷2 [𝑢, 𝑣] = 1, and thus it costs a
lot of time to iterate through them.

Here we check the conditions in an alternative order to reduce
the number of iterations. Given a vertex 𝑢 ′ ∈ Nbr𝑀 (𝑢), we define
a set 𝑆𝑢′ = {𝑣 ∈ 𝐶 (𝑢) : 𝐷2 [𝑢, 𝑣] = 1, (𝑣, 𝑀 (𝑢 ′)) ∈ 𝐸 (𝑔)}. Among
the vertices 𝑢 ′ ∈ Nbr𝑀 (𝑢), we find a vertex with the smallest |𝑆𝑢′ |
and call it 𝑢𝑚𝑖𝑛. We can see that the definition of |𝑆𝑢′ | matches
the definition of 𝑁 2
[𝑢] in Section 4.2, since the existence
of an edge (⟨𝑢, 𝑣⟩, ⟨𝑢 ′, 𝑀 (𝑢 ′)⟩) is equivalent to the existence of an
edge (𝑣, 𝑀 (𝑢 ′)) if (𝑢, 𝑢 ′) ∈ 𝐸 (𝑞) and 𝑣 ∈ 𝐶 (𝑢). Therefore, we have
|𝑆𝑢′ | = 𝑁 2
[𝑢] and thus 𝑢𝑚𝑖𝑛 is the vertex 𝑢 ′ ∈ Nbr𝑀 (𝑢) with
smallest 𝑁 2

𝑢′,𝑀 (𝑢′)

𝑢′,𝑀 (𝑢′)

[𝑢].

Once we compute 𝑢𝑚𝑖𝑛, we compute 𝑆𝑢𝑚𝑖𝑛 by iterating through
the neighbors 𝑣 of 𝑀 (𝑢𝑚𝑖𝑛) and checking whether 𝑣 ∈ 𝐶 (𝑢) and
𝐷2 [𝑢, 𝑣] = 1. By using 𝑆𝑢𝑚𝑖𝑛 , we rewrite 𝐶𝑀 (𝑢) as follows:
𝐶𝑀 (𝑢) = {𝑣 ∈ 𝑆𝑢𝑚𝑖𝑛 : ∀𝑢 ′ ∈ Nbr𝑀 (𝑢)\{𝑢𝑚𝑖𝑛 }, (𝑣, 𝑀 (𝑢 ′)) ∈ 𝐸 (𝑔)}.
Based on the equation, we compute 𝐶𝑀 (𝑢) by iterating through
the vertices 𝑣 in 𝑆𝑢𝑚𝑖𝑛 and checking whether the edge (𝑀 (𝑢 ′), 𝑣)
exists for every 𝑢 ′ ∈ Nbr𝑀 (𝑢)\{𝑢𝑚𝑖𝑛 }. Note that we need only

𝑢′,𝑀 (𝑢′)

Algorithm 6: Computing 𝐶𝑀 (𝑢)
Input: DCS, a data graph 𝑔, a query graph 𝑞, and an

extendable query vertex 𝑢

Output: A set of extendable candidates 𝐶𝑀 (𝑢)
1 Nbr𝑀 (𝑢) ← a set of matched neighbors of 𝑢 in 𝑞;
2 𝑢𝑚𝑖𝑛 ← 𝑢 ′ ∈ Nbr𝑀 (𝑢) with smallest 𝑁 2
[𝑢];
3 𝑆𝑢𝑚𝑖𝑛 ← {𝑣 ∈ 𝑉 (𝑔) : 𝐷2 [𝑢, 𝑣] = 1, (𝑀 (𝑢𝑚𝑖𝑛), 𝑣) ∈ 𝐸 (𝑔)};
4 𝐶𝑀 (𝑢) ← {𝑣 ∈ 𝑆𝑢𝑚𝑖𝑛 : forall 𝑢 ′ ∈ Nbr𝑀 (𝑢), (𝑀 (𝑢 ′), 𝑣) ∈

𝑢′,𝑀 (𝑢′)

𝐸 (𝑔)};

5.3 Matching Order
We select a matching order that can reduce the search space (and
thus the backtracking time). We choose a matching order based
on the size of extendable candidates, and thus it can be adaptively
changed during the backtracking process.

In the case of the subgraph matching problem, it is known that
the candidate-size order is an efficient way [14], which chooses the
extendable vertex with smallest |𝐶𝑀 (𝑢)|. We basically follow the
candidate-size order with an approximation for speed-up. Even
though we can compute the exact size of extendable candidates
every time we need to decide which extendable vertex to match
as in DAF [14], it costs a high overhead in our case because the
definition of extendable vertices is different. In the case of DAF,
a vertex 𝑢 is extendable only when all parents of 𝑢 are matched.
Therefore, the extendable candidates of an extendable vertex 𝑢 do
not change. In contrast, in our algorithm, a vertex 𝑢 is extendable
if at least one of neighbors of 𝑢 is matched. Therefore, 𝐶𝑀 (𝑢) may
change even if 𝑢 remains extendable, when an unmatched neighbor
of 𝑢 becomes matched. As a result, our algorithm has more frequent
changes in 𝐶𝑀 (𝑢), and a higher overhead of maintaining it when
compared to DAF.

To handle this issue, we use an estimated size of extendable
candidates which can be maintained much faster, and we compute
𝐶𝑀 (𝑢) only when all neighbors of 𝑢 are matched (i.e., when 𝐶𝑀 (𝑢)
no longer changes). Here we use the fact that in Algorithm 6, the
size of 𝐶𝑀 (𝑢) is bounded by |𝑆𝑢𝑚𝑖𝑛 | = 𝑁 2
[𝑢]. We use
this value as an estimated size of extendable candidates, since it
provides an upper bound and approximation of |𝐶𝑀 (𝑢)|, and it can
be easily maintained when we update or restore partial embeddings.
In a more formal way, we define 𝐸 (𝑢), an estimated size of extendable
candidates of 𝑢, as follows.

𝑢𝑚𝑖𝑛,𝑀 (𝑢𝑚𝑖𝑛)

𝐸 (𝑢) =

min
𝑢′ ∈Nbr𝑀 (𝑢)

{𝑁 2

𝑢′,𝑀 (𝑢′) [𝑢]}

Note that for every extendable candidate 𝑢, Nbr𝑀 (𝑢) is not empty
by definition, and thus 𝐸 (𝑢) is well-defined.

Every time match (𝑢, 𝑣) occurs, we iterate through the neighbors
𝑢 ′ of 𝑢 in 𝑞, and update 𝐸 (𝑢 ′) for them. Since we have to restore
a partial embedding later, we store the old 𝐸 (𝑢 ′) every time the
update occurs, and we revert to the old 𝐸 (𝑢 ′) when the partial
embedding is restored. These are done in UpdateEmbedding and
RestoreEmbedding in Algorithm 5.

Example 5.1. Let’s consider an edge insertion operation Δ𝑜2 in
Figure 1. As shown in Figure 3c, we get 𝐸𝐷𝐶𝑆 = {(⟨𝑢2, 𝑣3⟩, ⟨𝑢4, 𝑣6⟩),
(⟨𝑢2, 𝑣6⟩, ⟨𝑢4, 𝑣3⟩)}. We begin with an edge (⟨𝑢2, 𝑣3⟩, ⟨𝑢4, 𝑣6⟩), which
results in a partial embedding 𝑀 = {(𝑢2, 𝑣3), (𝑢4, 𝑣6)}. There are
three extendable vertices at this point, which are 𝑢1, 𝑢3 and 𝑢5.
We compare the estimated sizes of extendable candidates. Since
𝐸 (𝑢3) = 𝑁 2
𝑢4,𝑣6 [𝑢3] = 1 is the smallest compared to 𝐸 (𝑢1) = 2 and
𝐸 (𝑢5) = 100, we choose 𝑢3 as the next query vertex to match. We
get 𝐶𝑀 (𝑢3) = {𝑣4} by Algorithm 6 and extend the partial embed-
ding to 𝑀 = {(𝑢2, 𝑣3), (𝑢4, 𝑣6), (𝑢3, 𝑣4)}. Now we choose the next
extendable vertex between 𝑢1 and 𝑢5. We again compare the esti-
mated size of extendable candidates, and match 𝑢1 first. Finally, we
match 𝑢5 with its extendable candidates, and output the matches.
For DCS edge (⟨𝑢2, 𝑣6⟩, ⟨𝑢4, 𝑣3⟩), we skip finding matches since
𝐷2 [𝑢2, 𝑣6] = 𝐷2 [𝑢4, 𝑣3] = 0.

5.4 Isolated Vertices
In this subsection, we describe the leaf decomposition technique
from [3] and introduce our new idea, called isolated vertices.

For a query graph 𝑞, its leaf vertices are defined as the vertices
that have only one neighbor. The main idea of leaf decomposition
is to postpone matching the leaf vertices of 𝑞 until all other ver-
tices are matched. If we match a non-leaf vertex first, its unmatched
neighbors can be the new extendable vertices (if none of their neigh-
bors were matched before), or have their extendable candidates
pruned (if at least one of their neighbors were matched before),
both of which may lead to a smaller search space. These advantages
do not apply when we match a leaf vertex first, since there are no
unmatched neighbors if the leaf vertex is an extendable vertex.

Based on the above properties, we define isolated vertices as

follows.
Definition 5.3. For a query graph 𝑞, a data graph 𝑔, and a partial
embedding 𝑀, an isolated vertex is an extendable vertex in 𝑞, where
all of its neighbors are mapped in 𝑀.

Note that postponing matching the isolated vertices enjoy the
advantages of leaf decomposition, since isolated vertices have no
unmatched neighbors and thus have the same properties as leaves
in the context of leaf decomposition. Also note that every extend-
able leaf vertex is also an isolated vertex by definition, but the
converse is not true. For example, consider a partial embedding
𝑀 = {(𝑢2, 𝑣3), (𝑢3, 𝑣4), (𝑢4, 𝑣6)} in Figure 1. Even though there are
no leaf vertices in Figure 1a, there are two isolated vertices, 𝑢1 and
𝑢5. Therefore, the notion of isolated vertices fully includes the leaf
decomposition technique, and extends it further.

By combining the discussions in Section 5.3. and 5.4., we use the

following matching order.

1. Backtrack if there exists an isolated vertex 𝑢 such that all data

vertices in 𝐶𝑀 (𝑢) have already matched.

2. If there exists at least one non-isolated extendable vertex in 𝑞,
we choose the non-isolated extendable vertex 𝑢 with smallest
𝐸 (𝑢).

3. If every extendable vertex is isolated, we choose the extendable

vertex 𝑢 with smallest 𝐸 (𝑢).

6 PERFORMANCE EVALUATION
In this section, we present experimental results to show the ef-
fectiveness of our algorithm SymBi. Since TurboFlux [19] outper-
forms the other existing algorithms (e.g., IncIsoMat [10], SJ-Tree
[6], Graphflow [18]), we only compare TurboFlux and SymBi. Ex-
periments are conducted on a machine with two Intel Xeon E5-2680
v3 2.50GHz CPUs and 256GB memory running CentOS Linux. The
executable file of TurboFlux was obtained from the authors.
Datasets. We use two datasets referred to as LSBench and Netflow
which are commonly used in previous works [6, 19]. LSBench is
synthetic RDF social media stream data generated by the Linked
Stream Benchmark data generator [21]. We generate three different
sizes of datasets with 0.1, 0.5, and 2.5 million users with default
settings of Linked Stream Benchmark data generator, and use the
first dataset as a default. This dataset contains 23,317,563 triples.
Netflow is a real dataset (CAIDA Internet Anonymized Traces
2013 Dataset [4]) which contains anonymized passive traffic traces
obtained from CAIDA. Netflow consists of 18,520,759 triples. We
split 90% of the triples of a dataset into an initial graph and 10%
into a graph update stream.
Queries. We generate query graphs by random walk over schema
graphs. To generate various types of queries, we use schema graphs
instead of data graphs to randomly select edge labels regardless of
edge distribution [19]. For each dataset, we set four different query
sizes: 10, 15, 20, 25 (denoted by “G10”, “G15”, “G20”, and “G25”).
The size of a query is defined as the number of triples. We exclude
queries that have no matches for the entire graph update stream.
Also, we use the queries used in [19] (denoted by “T3”, “T6”, “T9”,
“T12”, “G6”, “G9”, and “G12”), where “T” stands for tree and “G”
stands for graph (having cycles). We generate 100 queries for each
dataset and query size. One experiment consists of a dataset and a
query set of 100 query graphs with a same size.
Performance Measurement. We measure the elapsed time of
continuous subgraph matching for a dataset and a query graph for
the entire update stream Δ𝑔. The preprocessing time (e.g., time to
build the initial data graph and the initial auxiliary data structure) is
excluded from the elapsed time. Since continuous subgraph match-
ing is an NP-hard problem, some query graphs may not finish in a
reasonable time. To address this issue, we set a 2-hours time limit
for each query. If some query reaches the time limit, we record
the query processing time of that query as 2 hours. We say that
a query graph is solved if it finishes within 2 hours. To evaluate
an algorithm regarding a query set, we report the average elapsed
time, the number of solved query graphs, and the average peak
memory usage of the program using the “ps” utility.

6.1 Experimental Results
The performance of SymBi was evaluated in several aspects: (1)
varying the query size, (2) varying the insertion rate, (3) varying
the deletion rate, and (4) varying the dataset size. Table 2 shows
the parameters of the experiments. Values in boldface in Table 2
are used as default parameters. The insertion rate is defined as the
ratio of the number of edge insertions to the number of edges in
the original dataset before splitting. Thus, 10% insertion rate means
the entire graph update stream we split. Also, the deletion rate is
defined as the ratio of the number of edge deletions to the number

of edge insertions in the graph update stream. For example, if the
deletion rate is 10%, one edge deletion occurs for every ten edge
insertions. For an edge deletion, we randomly choose an arbitrary
edge in the data graph at the time the edge deletion occurs, and
delete it.

Table 2: Experiment settings

Parameter
Datasets
Query size

Insertion rate
Deletion rate
Dataset size

Value Used
LSBench, Netflow
G10, G15, G20, G25,
T3, T6, T9, T12, G6, G9, G12
2, 4, 6, 8, 10
0, 2, 4, 6, 8, 10
0.1, 0.5, 2.5 million users (LSBench)

Efficiency of DCS Update. Before we compare our results with
TurboFlux, we first show the efficiency of our DCS update algo-
rithm as described in Lemma 4.4. Table 3 shows the number of
updated vertices and the number of visited edges in DCS per up-
date operation for Netflow and LSBench. “DCS |𝑉 |” and “DCS |𝐸|”
denote the average number of vertices and the average number of
edges of the DCS structure. “Updated vertices” denotes the average
number of ⟨𝑢, 𝑣⟩’s such that 𝐷1 [𝑢, 𝑣] or 𝐷2 [𝑢, 𝑣] changes per update
operation and “Visited edges” denotes the average number of DCS
edges visited during the update. This result shows that the portion
of DCS we need to update is extremely small compared to the size
of DCS.

Table 3: The number of updated vertices and visited edges in
DCS per update operation (top: Netflow, bottom: LSBench)

Query set

DCS |𝑉 |

G10
G15
G20
G25
T3
T6
T9
T12
G6
G9
G12

G10
G15
G20
G25
T3
T6
T9
T12
G6
G9
G12

30744014
45975850
58217388
76564119
12459580
21804265
31148950
40493635
18689370
28034055
37378740

51111071
75233829
83517886
125354981
18339548
32198412
46421983
60489250
29332857
42410206
56008565

DCS |𝐸| Updated Visited
edges
vertices
4.634
0.033
9.655
0.052
10.177
0.052
12.396
0.024
36.284
11.219
13.534
5.382
12.951
2.378
15.999
2.596
2.168
0.048
4.048
0.072
6.093
0.069

9725255
16480557
19570587
25310130
3850193
7888574
12044573
16118240
5583268
8868896
12295665

6872525
10584646
10721872
18676312
2104435
4072171
5999443
8021626
2918077
4969639
6764759

0.203
0.225
0.141
0.184
0.104
0.07
0.065
0.055
0.042
0.03
0.029

1.997
3.006
2.536
4.573
0.463
0.773
1.085
1.335
0.675
1.001
1.298

Varying the query size. First, we vary the number of triples in
query graphs. We set the insertion rate to 10% and the deletion rate
to 0% (i.e., no edge deletion in the graph update stream).

Figure 4a shows the average elapsed time for performing contin-
uous subgraph matching for Netflow. When calculating the average
elapsed time, we exclude queries that no algorithms can solve within
the time limit. SymBi outperforms TurboFlux regardless of query
sizes. Specifically, SymBi is 333.13 ∼ 947.02 times faster than Tur-
boFlux in our generated queries (G10 ∼ G25), 4.54 ∼ 16.49 times in
tree queries (T3 ∼ T12), and 516.48 ∼ 1336.24 times in graph queries
(G6 ∼ G12). The performance gap between SymBi and TurboFlux
is larger for graph queries than tree queries. The reason for this
is that TurboFlux does not take into account non-tree edges for
filtering, whereas SymBi consider all edges for filtering.

Figure 4b shows the number of queries solved by two algorithms.
SymBi solves most queries for every query set (except 1 query in
the T6 query set and 1 query in the T9 query set), while TurboFlux
has query sets that contains many unsolved queries. Specifically,
TurboFlux solves only 42 queries while SymBi solves all queries for
G20.

(a) Average elapsed time (in milliseconds)

(b) Solved queries
Figure 4: Varying query size for Netflow

Figure 5 shows the performance results for LSBench. In Figure 5a,
SymBi outperforms TurboFlux by 2.26∼38.35 times in our generated
queries. The reason for the lesser performance gap over Netflow
is that LSBench has 45 edge labels, and thus it is an easier dataset
to solve than Netflow with 8 edge labels. Also, there is almost no
difference in performance for the queries used in [19]. Unlike our
generated queries, most queries from [19] are solved in less than
one second. For theses cases, SymBi takes most of the elapsed time
to update the data graph or auxiliary data structures that takes
polynomial time, which is difficult to improve. In Figure 5b, SymBi
solves all queries within the time limit, while TurboFlux reach the
time limit for 1, 3 and 2 queries in G10, G20 and G25, respectively.
Varying the deletion rate. Next, we vary the deletion rate of
the graph update stream. We fix the query set to G10 and the
insertion rate to 10%, and vary the deletion rate from 0% to 10% in
2% increments.

100101102103104105106107G10G15G20G25T3T6T9T12G6G9G12Elapsed Time (ms)TurboFluxSymBi 0 20 40 60 80 100G10G15G20G25T3T6T9T12G6G9G12# of Solved Queries(a) Average elapsed time (in milliseconds)

(a) Average elapsed time (in mil-
liseconds)

(b) Solved queries

Figure 6: Varying deletion rate for Netflow

(b) Solved queries
Figure 5: Varying query size for LSBench
Figure 6 represents the performance results for Netflow. In Figure
6b, SymBi solves all queries for all deletion rates, but as the deletion
rate increases from 0% to 10%, the number of solved queries of
TurboFlux decreases from 54 to 41. Figure 6a shows that the average
elapsed time of TurboFlux is almost flat, but the average elapsed
time of SymBi increases as the deletion rate increases (i.e., the
performance improvement of SymBi over TurboFlux is 333.13 times
for 0% deletion rate and it decreases to 40.44 times as the deletion
rate increases to 10%). The decrease in the performance gap stems
from queries that TurboFlux cannot solve, but SymBi solves within
the time limit. Figure 7 helps to understand this phenomenon. Figure
7a and 7b show the elapsed time of all queries for each algorithm
with deletion rate 0% and 10%, respectively. Queries on the x-axis of
Figure 7a and 7b are sorted in ascending order based on the elapsed
time of TurboFlux when the deletion rate is 10%. In Figure 7a and 7b,
there are many queries for which TurboFlux reaches the time limit.
As the deletion rate increases from 0% (Figure 7a) to 10% (Figure 7b),
the elapsed time of TurboFlux for these queries does not increase
further beyond the time limit (2 hours), while the elapsed time of
SymBi increases. This reduces the performance gap between two
algorithms.

Considering this issue, we focus on 41 queries that TurboFlux
solves within the time limit in all deletion rates (queries on the
left side of the vertical line in Figure 7). When we measure the
average elapsed time with these 41 queries, the performance ratio
between two algorithms increases from 224.61 times to 309.45 times
as the deletion rate increases from 0% to 10%. While the deletion
rate changes from 0% to 10%, the average elapsed time of SymBi
increases only 1.54 times, but the average elapsed time of TurboFlux
increases 2.13 times. When two algorithms are compared, therefore,
the number of solved queries as well as the average elapsed time
are important measures.

The performance results for LSBench are shown in Figure 8.
Figure 8b shows that SymBi solves all queries while TurboFlux
solves 99 queries for all deletion rates. Figure 8a shows that as
the deletion rate increases, the performance ratio between two
algorithms increases (8.84 to 16.30 times). Similarly to the previous
one, considering only the 99 queries that both algorithms solve,

(a) Deletion rate 0%

(b) Deletion rate 10%

Figure 7: Elapsed time of all queries for each algorithm with
deletion rate 0% and 10%

(a) Average elapsed time (in mil-
liseconds)

(b) Solved queries

Figure 8: Varying deletion rate for LSBench
SymBi only increases 1.52 times when the deletion rate is 10%
compared to 0%, but TurboFlux increases 11.70 times. This shows
that SymBi handles the edge deletion case better than TurboFlux.
In order to further analyze why SymBi processes queries bet-
ter than TurboFlux as the deletion rate increases, we divide the
elapsed time when the deletion rate is 10% into four types: up-
date/backtracking time for edge insertion, and update/backtracking
time for edge deletion. Since the number of insertion operations and
that of deletion operations are different, we measure the elapsed
time per operation by dividing the elapsed time by the number of
operations. Table 4 shows the results for Netflow and LSBench. It is
noteworthy that the update time of TurboFlux for edge deletion is
much slower than that for edge insertion, while those of SymBi are
quite similar. As noted in Section 1, this happens because the DCG
update process of TurboFlux is more complex for edge deletion
than for edge insertion.
Varying the insertion rate. To test the effect of the insertion rate,
we use the G10 query set and vary the insertion rate from 2% to
10% in 2% increments. Note that the size of the initial graph does
not change from 90% of the original dataset.

Figure 9 represents the results using Netflow for varying inser-
tion rates. Figure 9b shows that SymBi solves all queries for all
insertion rates, while the number of solved queries of TurboFlux
decreases from 69 to 54 as the insertion rate increases. In Figure
9a, SymBi outperforms TurboFlux regardless of the insertion rate.
However, as before, the performance gap between two algorithms

100101102103104105106107G10G15G20G25T3T6T9T12G6G9G12Elapsed Time (ms)TurboFluxSymBi 0 20 40 60 80 100G10G15G20G25T3T6T9T12G6G9G12# of Solved Queries1001011021031041051061070246810Elapsed Time (ms)SymBiTurboFlux 0 20 40 60 80 1000246810# of Solved QueriesSymBiTurboFlux100101102103104105106107Elapsed Time (ms)SymBiTurboFlux100101102103104105106107Elapsed Time (ms)SymBiTurboFlux1001011021031041051061070246810Elapsed Time (ms)SymBiTurboFlux 0 20 40 60 80 1000246810# of Solved QueriesSymBiTurboFluxTable 4: Average update and backtracking time per opera-
tion in microseconds (top: Netflow, bottom: LSBench)

decreases. SymBi is consistently faster and solves more queries
than TurboFlux regardless of the dataset sizes.

TurboFlux

SymBi

Update Backtracking Update Backtracking

Ins
Del

Ins
Del

6.44
1867.39

1.13
599.82

202.48
2086.18

4.32
21.72

0.93
1.68

0.47
0.68

0.41
4.20

3.44
17.32

decreases as the insertion rate increases due to the queries that Tur-
boFlux cannot solve. As in Figure 7, when we measure the average
elapsed time with 54 queries that both algorithms solve within the
time limit in all insertion rates, the performance ratio increases
from 95.32 times to 276.60 times as the insertion rate increases from
2% to 10%.

(a) Average elapsed time (in mil-
liseconds)

(b) Solved queries

Figure 11: Varying dataset size

Memory usage. Figure 12 demonstrates the average peak memory
of each program for varying the dataset size (the results for the
other experiments are similar). Here, peak memory is defined as
the maximum of the virtual set size (VSZ) in the “ps” utility output.
This shows that SymBi uses a slightly less memory than TurboFlux
regardless of the dataset sizes.

(a) Average elapsed time (in mil-
liseconds)

(b) Solved queries

Figure 9: Varying insertion rate for Netflow

Figure 10 shows the results for LSBench. The performance ratio
between two algorithms is the largest at 19.30 times when the inser-
tion rate is 4%. As one query reaches the time limit for TurboFlux
at 6% insertion rate, the performance gap starts to decrease from
6% insertion rate. Nevertheless, SymBi is 8.84 times faster than
TurboFlux at 10% insertion rate.

(a) Average elapsed time (in mil-
liseconds)

(b) Solved queries

Figure 10: Varying insertion rate for LSBench

Varying the dataset size. We measure the performance for differ-
ent LSBench dataset sizes: 0.1, 0.5, and 2.5 million users. The size of
the initial data graph increases from 20,988,361 triples (0.1M users)
to 525,446,784 triples (2.5M users). As shown in the experiment
of varying the insertion rate, the number of triples in the graph
update stream affects the elapsed time. To test only the effect of the
dataset size, we set the same number of triples in the three graph
update streams. We fix the number of triples in the graph update
streams as 10% of the triples of the first dataset (0.1M users). In
Figure 11, as the dataset size increases, the elapsed time of both
algorithm generally increases and the number of solved queries

Figure 12: Average peak memory (in MB)

7 CONCLUSION
In this paper, we have studied continuous subgraph matching, and
proposed an auxiliary data structure called dynamic candidate space
(DCS) which stores the intermediate results of bidirectional dy-
namic programming between a query graph and a dynamic data
graph. We further proposed an efficient algorithm to update DCS
for each graph update operation. We then presented a matching
algorithm that uses DCS to find all positive/negative matches. Ex-
tensive experiments on real and synthetic datasets show that SymBi
outperforms the state-of-the-art algorithm by up to several orders of
magnitude. Parallelizing our algorithm including both intra-query
parallelism and inter-query parallelism is an interesting future
work.

8 ACKNOWLEDGMENTS
S. Min, S. G. Park, and K. Park were supported by Institute of In-
formation communications Technology Planning Evaluation (IITP)
grant funded by the Korea government (MSIT) (No. 2018-0-00551,
Framework of Practical Algorithms for NP-hard Graph Problems).
W.-S. Han was supported by Institute of Information communi-
cations Technology Planning Evaluation(IITP) grant funded by
the Korea government(MSIT) (No. 2018-0-01398, Development of
a Conversational, Self-tuning DBMS). G. F. Italiano was partially
supported by MIUR, the Italian Ministry for Education, University
and Research, under PRIN Project AHeAD (Efficient Algorithms
for HArnessing Networked Data).

100101102103104105106107246810Elapsed Time (ms)SymBiTurboFlux 0 20 40 60 80 100246810# of Solved QueriesSymBiTurboFlux100101102103104105106107246810Elapsed Time (ms)SymBiTurboFlux 0 20 40 60 80 100246810# of Solved QueriesSymBiTurboFlux1001011021031041051061070.1M0.5M2.5MElapsed Time (ms)SymBiTurboFlux 0 20 40 60 80 1000.1M0.5M2.5M# of Solved QueriesSymBiTurboFlux1001011021031041050.1M0.5M2.5MPeak Memory (MB)SymBiTurboFluxREFERENCES
[1] Ehab Abdelhamid, Mustafa Canim, Mohammad Sadoghi, Bishwaranjan Bhat-
tacharjee, Yuan-Chi Chang, and Panos Kalnis. 2017. Incremental Frequent Sub-
graph Mining on Large Evolving Graphs. IEEE Transactions on Knowledge and
Data Engineering 29, 12 (2017), 2710–2723.

[2] Bibek Bhattarai, Hang Liu, and H. Howie Huang. 2019. CECI: Compact Embed-
ding Cluster Index for Scalable Subgraph Matching. In Proceedings of the 2019
International Conference on Management of Data. 1447–1462.

[3] Fei Bi, Lijun Chang, Xuemin Lin, Lu Qin, and Wenjie Zhang. 2016. Efficient
Subgraph Matching by Postponing Cartesian Products. In Proceedings of the 2016
International Conference on Management of Data. 1199–1214.

[4] CAIDA. 2013. The CAIDA UCSD Anonymized Internet Traces 2013. Retrieved Sep-
tember 3, 2020 from https://www.caida.org/data/passive/passive_2013_dataset.
xml

[5] Lei Chen and Changliang Wang. 2010. Continuous Subgraph Pattern Search over
Certain and Uncertain Graph Streams. IEEE Transactions on Knowledge and Data
Engineering 22, 8 (2010), 1093–1109.

[6] Sutanay Choudhury, Lawrence Holder, George Chin, Khushbu Agarwal, and
John Feo. 2015. A Selectivity based approach to Continuous Pattern Detection in
Streaming Graphs. In Proceedings of the 18th International Conference on Extending
Database Technology. 157–168.

[7] Sutanay Choudhury, Lawrence Holder, George Chin, Abhik Ray, Sherman Beus,
and John Feo. 2013. StreamWorks: A system for Dynamic Graph Search. In
Proceedings of the 2013 ACM SIGMOD International Conference on Management of
Data. 1101–1104.

[8] Luigi P Cordella, Pasquale Foggia, Carlo Sansone, and Mario Vento. 2004. A
(Sub)Graph Isomorphism Algorithm for Matching Large Graphs. IEEE transactions
on pattern analysis and machine intelligence 26, 10 (2004), 1367–1372.

[9] Grace Fan, Wenfei Fan, Yuanhao Li, Ping Lu, Chao Tian, and Jingren Zhou.
2020. Extending Graph Patterns with Conditions. In Proceedings of the 2020 ACM
SIGMOD International Conference on Management of Data. 715–729.

[10] Wenfei Fan, Xin Wang, and Yinghui Wu. 2013.

Incremental Graph Pattern

Matching. ACM Transactions on Database Systems (TODS) 38, 3 (2013), 1–47.
[11] Jun Gao, Chang Zhou, and Jeffrey Xu Yu. 2016. Toward continuous pattern
detection over evolving large graph with snapshot isolation. The VLDB Journal
25, 2 (2016), 269–290.

[12] Jun Gao, Chang Zhou, Jiashuai Zhou, and Jeffrey Xu Yu. 2014. Continuous Pattern
Detection over Billion-Edge Graph Using Distributed Framework. In 2014 IEEE
30th International Conference on Data Engineering. IEEE, 556–567.

[13] Pankaj Gupta, Venu Satuluri, Ajeet Grewal, Siva Gurumurthy, Volodymyr
Zhabiuk, Quannan Li, and Jimmy Lin. 2014. Real-Time Twitter Recommen-
dation: Online Motif Detection in Large Dynamic Graphs. Proceedings of the
VLDB Endowment 7, 13 (2014), 1379–1380.

[14] Myoungji Han, Hyunjoon Kim, Geonmo Gu, Kunsoo Park, and Wook-Shin Han.
2019. Efficient Subgraph Matching: Harmonizing Dynamic Programming, Ad-
pative Matching Order, and Failing Set Together. In Proceedings of SIGMOD.
1429–1446. https://doi.org/10.1145/3299869.3319880

[15] Wook-Shin Han, Jinsoo Lee, and Jeong-Hoon Lee. 2013. TurboISO: Towards
UltraFast and Robust Subgraph Isomorphism Search in Large Graph Databases.
In Proceedings of the 2013 ACM SIGMOD International Conference on Management
of Data. 337–348.

[16] Huahai He and Ambuj K. Singh. 2008. Graphs-at-a-time: Query Language and
Access Methods for Graph Databases. In Proceedings of the 2008 ACM SIGMOD
international conference on Management of data. 405–418.

[17] Cliff Joslyn, Sutanay Choudhury, David Haglin, Bill Howe, Bill Nickless, and
Bryan Olsen. 2013. Massive Scale Cyber Traffic Analysis: A Driver for Graph
Database Research. In First International Workshop on Graph Data Management
Experiences and Systems. 1–6.

[18] Chathura Kankanamge, Siddhartha Sahu, Amine Mhedbhi, Jeremy Chen, and
Semih Salihoglu. 2017. Graphflow: An Active Graph Database. In Proceedings of
the 2017 ACM International Conference on Management of Data. 1695–1698.
[19] Kyoungmin Kim, In Seo, Wook-Shin Han, Jeong-Hoon Lee, Sungpack Hong,
Hassan Chafi, Hyungyu Shin, and Geonhwa Jeong. 2018. TurboFlux: A Fast
Continuous Subgraph Matching System for Streaming Graph Data. In Proceedings
of the 2018 International Conference on Management of Data. 411–426.

[20] Pradeep Kumar and H. Howie Huang. 2019. GraphOne: A Data Store for Real-
Time Analytics on Evolving Graphs. In 17th USENIX Conference on File and Storage
Technologies (FAST 19). 249–263.

[21] Danh Le-Phuoc, Minh Dao-Tran, Minh-Duc Pham, Peter Boncz, Thomas Eiter,
and Michael Fink. 2012. Linked Stream Data Processing Engines: Facts and
Figures. In International Semantic Web Conference. Springer, 300–312.

[22] Youhuan Li, Lei Zou, M. Tamer Özsu, and Dongyan Zhao. 2019. Time Con-
strained Continuous Subgraph Search Over Streaming Graphs. In 2019 IEEE 35th
International Conference on Data Engineering (ICDE). IEEE, 1082–1093.

[23] Andrew McGregor. 2014. Graph Stream Algorithms: A Survey. ACM SIGMOD

Record 43, 1 (2014), 9–20.

[24] Amine Mhedhbi and Semih Salihoglu. 2019. Optimizing Subgraph Queries by
Combining Binary and Worst-Case Optimal Joins. Proceedings of the VLDB

Endowment 12, 11 (2019), 1692–1704.

[25] Jayanta Mondal and Amol Deshpande. 2014. EAGr: Supporting Continuous
Ego-centric Aggregate Queries over Large Dynamic Graphs. In Proceedings of the
2014 ACM SIGMOD International Conference on Management of data. 1335–1346.
[26] Mohammad Hossein Namaki, Keyvan Sasani, Yinghui Wu, and Tingjian Ge.
2017. BEAMS: Bounded Event Detection in Graph Streams. In 2017 IEEE 33rd
International Conference on Data Engineering (ICDE). IEEE, 1387–1388.

[27] Hung Q. Ngo, Christopher Ré, and Atri Rudra. 2014. Skew Strikes Back: New
Developments in the Theory of Join Algorithms. ACM SIGMOD Record 42, 4
(2014), 5–16.

[28] Andrea Pugliese, Matthias Bröcheler, V. S. Subrahmanian, and Michael Ovelgönne.
2014. Efficient Multiview Maintenance under Insertion in Huge Social Networks.
ACM Transactions on the Web (TWEB) 8, 2 (2014), 1–32.

[29] Xiafei Qiu, Wubin Cen, Zhengping Qian, You Peng, Ying Zhang, Xuemin Lin, and
Jingren Zhou. 2018. Real-time Constrained Cycle Detection in Large Dynamic
Graphs. Proceedings of the VLDB Endowment 11, 12 (2018), 1876–1888.

[30] Xuguang Ren and Junhu Wang. 2015. Exploiting Vertex Relationships in Speed-
ing up Subgraph Isomorphism over Large Graphs. Proceedings of the VLDB
Endowment 8, 5 (2015), 617–628.

[31] Carlos R Rivero and Hasan M Jamil. 2017. Efficient and scalable labeled subgraph
matching using SGMatch. Knowledge and Information Systems 51, 1 (2017), 61–87.
[32] Gorka Sadowski and Philip Rathle. 2014. Fraud Detection: Discovering Connec-
tions with Graph Databases. White Paper-Neo Technology-Graphs are Everywhere
13 (2014).

[33] Haichuan Shang, Ying Zhang, Xuemin Lin, and Jeffrey Xu Yu. 2008. Taming
Verification Hardness: An Efficient Algorithm for Testing Subgraph Isomorphism.
Proceedings of the VLDB Endowment 1, 1 (2008), 364–375.

[34] Chunyao Song, Tingjian Ge, Cindy Chen, and Jie Wang. 2014. Event Pattern
Matching over Graph Streams. Proceedings of the VLDB Endowment 8, 4 (2014),
413–424.

[35] Shixuan Sun and Qiong Luo. 2020.

In-Memory Subgraph Matching: An In-
depth Study. In Proceedings of the 2020 ACM SIGMOD International Conference on
Management of Data. 1083–1098.

[36] Nan Tang, Qing Chen, and Prasenjit Mitra. 2016. Graph Stream Summarization:
From Big Bang to Big Crunch. In Proceedings of the 2016 International Conference
on Management of Data. 1481–1496.

[37] Julian R. Ullmann. 1976. An Algorithm for Subgraph Isomorphism. Journal of

the ACM (JACM) 23, 1 (1976), 31–42.

[38] Verizon. 2020. Data Breach Investigations Report.

Retrieved September 3,
2020 from https://enterprise.verizon.com/resources/reports/2020-data-breach-
investigations-report.pdf

[39] Lefteris Zervakis, Vinay Setty, Christos Tryfonopoulos, and Katja Hose. 2019.
Efficient Continuous Multi-Query Processing over Graph Streams. arXiv preprint
arXiv:1902.05134 (2019).

[40] Qianzhen Zhang, Deke Guo, Xiang Zhao, and Aibo Guo. 2019. On Continuously
Matching of Evolving Graph Patterns. In Proceedings of the 28th ACM International
Conference on Information and Knowledge Management. 2237–2240.

[41] Peixiang Zhao and Jiawei Han. 2010. On Graph Query Optimization in Large

Networks. Proceedings of the VLDB Endowment 3, 1-2 (2010), 340–351.

A APPENDIX
A.1 Proofs of Lemmas
Proof of Lemma 4.1. We prove the lemma for 𝐷1 by induction
in a top-down fashion. For each ⟨𝑢, 𝑣⟩ in DCS, we divide the cases
whether 𝑢 is the root of DAG 𝑞ˆ or not.

When 𝑢 is the root of DAG 𝑞ˆ (i.e., base cases), the lemma holds
trivially since 𝑣 ∈ 𝐶 (𝑢) and thus 𝑢 and 𝑣 both have the same label.
When 𝑢 is not the root of DAG 𝑞ˆ (i.e., inductive case), let’s assume
that 𝐷1 [𝑢𝑝, 𝑣𝑝 ] is correctly computed for every parent 𝑢𝑝 of 𝑢 and
𝑣𝑝 ∈ 𝐶 (𝑢𝑝 ). Now we show that Recurrence (1) correctly computes
𝐷1 [𝑢, 𝑣], i.e., there exists a weak embedding of 𝑞ˆ−1
𝑢 at 𝑣 if and only
if ∃𝑣𝑝 ∈ 𝐶 (𝑢𝑝 ) adjacent to 𝑣 such that 𝐷1 [𝑢𝑝, 𝑣𝑝 ] = 1 for every
parent 𝑢𝑝 of 𝑢 in 𝑞ˆ.

First we show the ‘only if’ part. Let’s assume that there exists a
weak embedding 𝑀 ′ of 𝑞ˆ−1
𝑢 at 𝑣. For each parent 𝑢𝑝 of 𝑢 in 𝑞ˆ (which
is also a child of 𝑢 in 𝑞ˆ−1), we can get a weak embedding of 𝑞ˆ−1
𝑢𝑝
at 𝑀 ′(𝑢𝑝 ) by removing the nodes not in 𝑞ˆ−1
from 𝑀 ′. Therefore,
𝑢𝑝
𝐷1 [𝑢𝑝, 𝑀 ′(𝑢𝑝 )] = 1 holds by inductive hypothesis. Furthermore,

𝑀 ′(𝑢𝑝 ) ∈ 𝐶 (𝑢𝑝 ) and 𝑀 ′(𝑢𝑝 ) is adjacent to 𝑀 ′(𝑢) = 𝑣 because 𝑀 ′
is a weak embedding of 𝑞ˆ−1
𝑢 and 𝑢𝑝 is adjacent to 𝑢. Therefore we
proved the statement.

Now we show the converse. If there exist ∃𝑣𝑝 ∈ 𝐶 (𝑢𝑝 ) adjacent
to 𝑣 such that 𝐷1 [𝑢𝑝, 𝑣𝑝 ] = 1 for every parent 𝑢𝑝 of 𝑢 in 𝑞ˆ, there
is a weak embedding 𝑀 ′
at 𝑣𝑝 for each 𝑢𝑝 by inductive
𝑢𝑝
hypothesis. Now we can build a weak embedding 𝑀 ′ of 𝑞ˆ−1
𝑢 at 𝑣, by
building a tree which has 𝑣 as a root, and 𝑀 ′
’s as subtrees under
𝑢𝑝
𝑣. Therefore we proved the statement.

of 𝑞ˆ−1
𝑢𝑝

We can similarly prove that 𝐷2 is also correctly computed by

Recurrence (2).
Proof of Lemma 4.2. We need 𝑂 (|𝑉 (𝑞)| × |𝑉 (𝑔)|) space to store
⟨𝑢, 𝑣⟩’s in the DCS structure, 𝑂 (|𝑉 (𝑞)| × |𝑉 (𝑔)|) space to store 𝐷1
and 𝐷2, and 𝑂 (|𝐸 (𝑞)| × |𝐸 (𝑔)|) space to store edges. Hence we need
𝑂 (|𝐸 (𝑞)| × |𝐸 (𝑔)|) space for the DCS structure in total.

We can build the vertices and edges in the DCS structure in
𝑂 (|𝐸 (𝑞)| × |𝐸 (𝑔)|) time by traversing through the vertices and
edges in 𝑞 and 𝑔. Now we consider 𝐷1 and 𝐷2. In order to compute
𝐷1 [𝑢, 𝑣], we have to traverse through all parents ⟨𝑢𝑝, 𝑣𝑝 ⟩ of ⟨𝑢, 𝑣⟩.
Since we traverse through all edges once, we need 𝑂 (|𝐸 (𝑞)|×|𝐸 (𝑔)|)
time to compute 𝐷1. Similarly, we can compute 𝐷2 with the same
time complexity.
Proof of Lemma 4.3. We first consider 𝐷1 here, since we can deal
with the case of 𝐷2 similarly to 𝐷1.

In order to prove that 𝐷1 [𝑢, 𝑣] is correctly updated after the
insertion for every ⟨𝑢, 𝑣⟩, we need only prove that 𝑁 1
𝑢,𝑣 [𝑢𝑝 ]’s are
correctly updated for all parents 𝑢𝑝 of 𝑢. It is because every time
𝑁 1
𝑢,𝑣 [𝑢𝑝 ] is updated in Line 9 in Algorithm 3, we update 𝑁 1
𝑃 [𝑢, 𝑣]
and 𝐷1 [𝑢, 𝑣] following their definitions in Lines 1-8 in Algorithm 3.
𝑢,𝑣 [𝑢𝑝 ]’s are correctly updated by induction

Now we prove that 𝑁 1

on 𝑢, in a topological order on 𝑞ˆ.

If 𝑢 is the root of 𝑞ˆ (i.e., base case), the statement is trivial since

since there are no parents 𝑢𝑝 of the root 𝑢.

If 𝑢 is not the root of 𝑞ˆ (i.e., inductive cases), let’s assume that
𝐷1 [𝑢𝑝, 𝑣𝑝 ]’s are correctly updated for every parents of 𝑢𝑝 and
𝑣𝑝 ∈ 𝐶 (𝑢𝑝 ). Now we show that 𝑁 1
𝑢,𝑣 [𝑢𝑝 ]’s are correctly updated.
If 𝑁 1
𝑢,𝑣 [𝑢𝑝 ] has to be updated from 0 to 1, it means that a new edge
(⟨𝑢, 𝑣⟩, ⟨𝑢𝑝, 𝑣𝑝 ⟩) is inserted to DCS for some 𝑣𝑝 , or 𝐷1 [𝑢𝑝, 𝑣𝑝 ] was
updated from 0 to 1 for some 𝑣𝑝 . In the former case, 𝑁 1
𝑢,𝑣 [𝑢 ′] is
updated properly in Line 6 in Algorithm 2. In the latter case, since
𝐷1 [𝑢𝑝, 𝑣𝑝 ] was properly updated from 0 to 1 by inductive hypothe-
sis, Line 4 in Algorithm 3 was called with 𝐷1 [𝑢𝑝, 𝑣𝑝 ], and thus ⟨𝑢, 𝑣⟩
is pushed to 𝑄1 in Line 5. Thus, 𝑁 1
𝑢,𝑣 [𝑢 ′] is updated properly in
Line 14 of Algorithm 2, when ⟨𝑢𝑝, 𝑣𝑝 ⟩ is popped from 𝑄1. Therefore,
Algorithm 2 updates 𝑁 1
𝑢,𝑣 [𝑢 ′] every time it’s necessary, and thus it
has the correct value after Algorithm 2 is finished.
Proof of Lemma 4.4. The DCS update process for edge deletion is
similar to the DCS update process for edge insertion, so we will only
show the time complexity of the update process for edge insertion
(Algorithm 2). First, Lines 3-10 of Algorithm 2 are executed |𝐸𝐷𝐶𝑆 |
times. Since InsertionTopDown (Algorithm 3) and Insertion-
BottomUp (Algorithm 4) take a constant time, the total execution
time of Lines 3-10 is 𝑂 (|𝐸𝐷𝐶𝑆 |). Next, the while loop of Lines 11-14
(or Lines 15-20) takes a time proportional to the number of chil-
dren of ⟨𝑢, 𝑣⟩ (or the number of parent of ⟨𝑢, 𝑣⟩ plus the number
of children of ⟨𝑢, 𝑣⟩), which is equal to or less than the number

We need 𝑂 (|𝐸 (𝑞)| × |𝑉 (𝑔)|) space to store 𝑁 1

of connected edges of ⟨𝑢, 𝑣⟩. Since the while loop is executed for
⟨𝑢, 𝑣⟩ where 𝐷1 [𝑢, 𝑣] or 𝐷2 [𝑢, 𝑣] changes, the total execution time
of Lines 11-20 is proportional to the sum of the number of edges
connected to ⟨𝑢, 𝑣⟩ where 𝐷1 [𝑢, 𝑣] or 𝐷2 [𝑢, 𝑣] changes. Hence, the
time complexity of the DCS update is 𝑂 (∑︁𝑝 ∈𝑃 deg(𝑝) + |𝐸𝐷𝐶𝑆 |).
𝑢,𝑣 [𝑢𝑝 ]’s because
there are |𝐸 (𝑞)| edges for (𝑢, 𝑢𝑝 ) and |𝑉 (𝑔)| vertices for 𝑣. Also, we
need 𝑂 (|𝑉 (𝑞)|×|𝑉 (𝑔)|) space to store 𝑁 1
𝑃 [𝑢, 𝑣]’s. Similarly, we need
𝑂 (|𝐸 (𝑞)| × |𝑉 (𝑔)|) space for 𝑁 2
𝑢,𝑣 [𝑢 ′] and 𝑂 (|𝑉 (𝑞)| × |𝑉 (𝑔)|) for
𝑁 2
𝐶 [𝑢, 𝑣]. Hence the space complexity of the DCS update excluding
DCS itself is 𝑂 (|𝐸 (𝑞)| × |𝑉 (𝑔)|).

A.2 Extensions of Our Algorithm
In this section, we explain how to extend SymBi to handle edge-
labeled graphs and directed graphs.
Edge-labeled Graph. To deal with edge-labeled graphs, our algo-
rithm needs to be modified as follows. First, when constructing
DCS, edge labels should be considered. Specifically, for the edge
(⟨𝑢, 𝑣⟩, ⟨𝑢 ′, 𝑣 ′⟩) in DCS to exist, not only edges (𝑢, 𝑢 ′) and (𝑣, 𝑣 ′)
must exist, but the edge labels of both must also be the same. Also,
when computing 𝐷1 [𝑢, 𝑣] (or 𝐷2 [𝑢, 𝑣]) through recurrences, it is
necessary to verify that the label of (𝑢, 𝑢𝑝 ) (or (𝑢, 𝑢𝑐 )) and the label
of (𝑣, 𝑣𝑝 ) (or (𝑣, 𝑣𝑐 )) should be the same. Next, when computing
𝐸𝐷𝐶𝑆 in DCSChangedEdge, only edges of the query graph with
the same edge label as (𝑣, 𝑣 ′) should be included. Finally, when
computing 𝑆𝑢𝑚𝑖𝑛 in Algorithm 6, we must include vertex 𝑣 such
that the label of (𝑢, 𝑢𝑚𝑖𝑛) and the label of (𝑣, 𝑀 (𝑢𝑚𝑖𝑛)) are the
same. Similarly, the edge label must be considered when computing
𝐶𝑀 (𝑢).
Directed Graph. Suppose that we are given a directed data graph
and a directed query graph. To create a parent-child relationship
of query vertices required in DCS, we regard the directed query
graph as an undirected graph and build a rooted DAG as in the
paper. The query DAG created in this way is independent of the
actual direction of the edge of the query graph. We use the query
DAG to order the query vertices when constructing or updating
DCS in a top-down or bottom-up fashion. When we map an edge
(𝑢, 𝑢 ′) of the query graph and an edge (𝑣, 𝑣 ′) of the data graph, we
need to check that the directions of the two edges match, as in the
edge-labeled graph.

A.3 Distribution of the Number of Matches
Stacked bar charts in Figure 13 represent the distribution of the
number of matches for queries we test. The bar for each query set
is made up of seven sub-bars. The color of a sub-bar represents the
range of the number of matches as shown in the legend of Figure 13,
and the size of a sub-bar represents the number of queries for which
the number of matches is in that range. Sub-bars are stacked in
order from the smallest number of matches to the largest number of
matches from the bottom. For example, the bar for the G10 query set
shows that there are 11 queries with 104 ∼ 106 matches, 18 queries
with 106 ∼ 108 matches, 50 queries with 108 ∼ 1010 matches, and
21 queries with more than 1010 matches. In Figure 13a, two queries
in the T6 query set bar and one in the T9 query set bar are missing
because neither algorithm can solve them within the time limit.
Also, the stacked bars in Figure 13 show different results from

the stacked bars of [19], since [19] uses graph homomorphism as
matching semantics while we use graph isomorphism as matching
semantics.

Figure 13 shows that our generated queries have a larger number
of matches than queries from [19]. Also, Figure 13 shows that the
Netflow queries have more matches than the LSBench queries. This
supports the fact that the LSBench queries are easier to solve than
the Netflow queries as mentioned above.

(a) Netflow query sets

(a) Netflow query sets

(b) LSBench query sets

Figure 13: Distribution of the number of matches for queries

A.4 Effect of Our Techniques
Effect of DCS Update. To see the effect of the proposed DCS up-
date method, we compare the elapsed time of the proposed method
with the elapsed time of recomputing from scratch. We also measure
the number of updated vertices and the number of visited edges
in DCS during the DCS update, which are presented in Section
6.1. Since recomputing from scratch takes a long time, we limit the
number of update operations to 1000.

Figure 14 shows the average DCS update time per update op-
eration (i.e., average DCS update time / 1000) for Netflow and
LSBench. Our proposed update method is faster than recomputing
from scratch by up to four orders of magnitude for Netflow and
five orders of magnitude for LSBench.
Effect of Estimated Size. To evaluate the effect of the estimated
size of extendable candidates, we compare the estimated size of
extendable candidates and the exact size of extendable candidates
through two scores. Also, we compare the elapsed time when using
the estimated size and the elapsed time when using the exact size.

(b) LSBench query sets

Figure 14: Average elapsed time per update operation (in mi-
croseconds)

To define the two scores 𝑆1 and 𝑆2 for a query graph and a dataset,
we consider the query vertex 𝑢 selected to match according to the
estimated candidate size order for each partial embedding 𝑀 in the
search process. To show how similar the estimated size and the
exact size are, we define the first score 𝑆1 as the sum of |𝐶𝑀 (𝑢)|
of the vertices that we consider divided by the sum of 𝐸 (𝑢) of the
same vertices. Next, the second score 𝑆2 is defined as the number of
partial embeddings in which the estimated candidate size order and
the exact candidate size order select the same query vertex 𝑢 (i.e.,
both 𝐸 (𝑢) and |𝐶𝑀 (𝑢)| are the smallest among extendable vertices)
divided by the number of partial embeddings in the search process.
Table 5 shows the average scores of 100 queries in a query set
for Netflow and LSBench. We exclude tree-shaped query sets be-
cause extendable vertices in a tree-shaped query have only one
matched neighbor. The average 𝑆1 score for the 7 query sets is 0.434
for Netflow and 0.874 for LSBench. The average 𝑆2 score is 0.830
for Netflow and 0.966 for LSBench. The 𝑆1 score indicates that
depending on the dataset, there may be differences between the
estimated size and the exact size. However, the 𝑆2 score shows that
in most cases of both datasets, the estimated candidate size order
chooses the same vertex as the exact candidate size order. Although
the computational overhead of the estimated size of extendable
candidates is negligible, the estimated candidate size order works
almost like the exact candidate size order.

Table 5: Estimated size 𝐸 (𝑢) vs. exact size |𝐶𝑀 (𝑢)|. 𝑆1:
∑︁ |𝐶𝑀 (𝑢)|/∑︁ 𝐸 (𝑢), 𝑆2: proportion that the estimated candi-
date size order is the same as the exact candidate size order.
(top: Netflow, bottom: LSBench)

Score G10 G15 G20 G25
0.509
0.535
0.863
0.831

0.567
0.869

0.445
0.811

𝑆1
𝑆2

𝑆1
𝑆2

0.939
0.998

0.950
0.991

0.891
0.899

0.955
0.975

G6
0.308
0.772

0.754
0.969

G9
0.332
0.842

0.807
0.973

G12
0.341
0.822

0.823
0.959

10-1100101102103104105106G10G15G20G25T3T6T9T12G6G9G12Elapsed Time (us)recomputeSymBi10-1100101102103104105106G10G15G20G25T3T6T9T12G6G9G12Elapsed Time (us)recomputeSymBiFigure 15 represents the average elapsed time when using the
estimated size and the exact size. In Figure 15b, the algorithm using
the estimated size outperforms the algorithm using the exact size
for all query sets in LSBench. In particular, the algorithm using the
exact size failed to solve 1 and 2 queries in G10 and G20, respectively,
while the algorithm using the estimated size solves all queries. This
is because the estimated size in LSBench works almost the same as
the exact size, but there is no overhead for updating 𝐶𝑀 (𝑢).

decomposition technique in [3]). Figure 16 shows the results. Since
the isolated vertex technique and the leaf decomposition technique
are the same in a tree-shape query, we exclude tree-shaped query
sets. Due to the characteristics of the datasets, the generated queries
are sparse. Because of this, there are not many situations where a
query vertex becomes an isolated vertex without being a leaf, so
the performance of the two techniques is similar in many query
sets. Nevertheless, when using the isolated vertex technique, it is
1.63, 2.01, 8.07, and 2.59 times faster on query sets G10, G15, G20,
and G25 of LSBench, respectively.

(a) Average elapsed time (in milliseconds) for Netflow query sets

(a) Average elapsed time (in milliseconds) for Netflow query sets

(b) Average elapsed time (in milliseconds) for LSBench query sets

Figure 15: Estimated size vs. exact size

(b) Average elapsed time (in milliseconds) for LSBench query sets

Effect of Isolated Vertex. To test the effect of isolated vertices, we
compare the elapsed time when using the isolated vertex technique
and the elapsed time when not using it (i.e., just using the leaf

Figure 16: Isolated vertex vs. leaf decomposition

100101102103104105106G10G15G20G25G6G9G12Elapsed Time (ms)ExactEstimated100101102103104105106G10G15G20G25G6G9G12Elapsed Time (ms)ExactEstimated100101102103104105106G10G15G20G25G6G9G12Elapsed Time (ms)Leaf decompositionIsolated vertices100101102103104105106G10G15G20G25G6G9G12Elapsed Time (ms)Leaf decompositionIsolated vertices