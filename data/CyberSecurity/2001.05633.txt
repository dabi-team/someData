2
2
0
2

n
u
J

7

]
T
G
.
s
c
[

2
v
3
3
6
5
0
.
1
0
0
2
:
v
i
X
r
a

Master equation of discrete time graphon mean

1

ﬁeld games and teams

Deepanshu Vasal, Rajesh Mishra and Sriram Vishwanath

Abstract

In this paper, we present a sequential decomposition algorithm equivalent of Master equation to compute graphon

mean-ﬁeld equillibrium (GMFE) of graphon mean-ﬁeld games (GMFGs) and graphon optimal Markovian policies

(GOMPs) of graphon mean ﬁeld teams (GMFTs). We consider a large population of players sequentially making

strategic decisions where the actions of each player affect their neighbors which is captured in a graph, generated

by a known graphon. Each player observes a private state and also a common information as a graphon mean-ﬁeld

population state which represents the empirical networked distribution of other players’ types. We consider non-

stationary population state dynamics and present a novel backward recursive algorithm to compute both GMFE and

GOMP that depend on both, a player’s private type, and the current (dynamic) population state determined through

the graphon. Each step in computing GMFE consists of solving a ﬁxed-point equation, while computing GOMP

involves solving for an optimization problem. We provide conditions on model parameters for which there exists

such a GMFE. Using this algorithm, we obtain the GMFE and GOMP for a speciﬁc security setup in cyber physical

systems for different graphons that capture the interactions between the nodes in the system.

Graphon mean-ﬁeld teams and games, Sequential decomposition, Signaling, Optimal Markov strategies

Index Terms

I. INTRODUCTION

Interaction of interconnected agents has been an important topic of study for many decades and its relevance

has been increasing rapidly with the progress of internet penetration and smartphone devices in our society. The

recent decade has seen tremendous technological advancement in the ﬁeld of networking applications that has led

to an unprecedented scale of interaction among people and devices such as in ride sharing platforms, social media

apps, cyber-physical systems, autonomous vehicles and drones, large scale renewable energy, electric vehicles,

cryptocurrencies and smart grid systems. For instance, the inﬂuence of social networks in the decision making of

majority of individuals is a known phenomenon. Most decisions by individuals from which products to buy to

whom to vote for are inﬂuenced by friends and acquaintances. The emerging empirical evidence on these issues

motivates the theoretical study of network effects with strategic and non strategic agents. The analysis, design and

Deepanshu Vasal is with Department of Electrical and Computer Engineering at Northwestern University.

Rajesh Mishra and Sriram Vishwanath are with Department of Electrical and Computer Engineering at University of Texas, Austin.

Part of the paper was presented at [1].

 
 
 
 
 
 
2

control of such systems that involve such interactions embedded in a networked environment could lead to more

intelligent and efﬁcient applications, and can enhance our understanding of the mechanics of such interactions.

Many of the above mentioned applications of interest have following key features: (a) large number of strategic

or non strategic players (b) dynamically evolving incomplete information, and (c) an underlying network. When

the decision makers are non strategic, one can pose such problems as decentralized stochastic control problems on

a network and in general such problems are extremely hard (see [2], [3] and references therein). When it comes to

problems with strategic interactions, game theory is a natural choice to model such interactions where the payoffs

obtained by individuals depend on the action of her neighbors. A shortcoming of the standard approach to solve

dynamic network games with incomplete information is the interdependence of strategies of the players across

time. Moreover, as the number of players become large as is the case in many practical scenarios considered here,

computing Nash equilibrium becomes intractable.

A. Relevant Literature

For the decentralized team problems, Witsenhausen provided a ‘simple’ two stage LQG system [2] where he

showed that linear policies are not optimal and to this day we don’t know the optimal policies for that system

showing how such simple looking decentralized control system could be extremely hard. Decentralized control

systems have been studied extensively in the literature where not too long ago Nayyar et al in [3] (see references

there in) presented a common agent approach where showed that a class of decentralized control problems with

common information can be posed as a single agent partially observed Markov decision problems and thus in

principle can be solved using dynamic programming. Arabneydi and Mahajan posed such a problem with large

number of players as Mean ﬁeld team problems in [4] and provided a dynamic programming approach to ﬁnd

optimal Markovian policies for such problems.

There is a huge literature on studying dynamic decision problems when the users are strategic. Maskin and Tirole

in [5] introduced the concept of Markov perfect equillibrium (MPE) for dynamic games governed by an underlying

Markov decision process (MDP). The strategies thus computed depend on the present state and not on the past

trajectory of the game. In general, there exists a backward recursive methodology to compute MPE of the game.

Some prominent examples of the application of MPE include [6], [7], [8]. Ericson and Pakes in [6] model industry

dynamics for ﬁrms’ entry, exit and investment participation, through a dynamic game with symmetric information,

compute its MPE, and prove ergodicity of the equilibrium process. Bergemann and V¨alim¨aki in [7] study a learning

process in a dynamic oligopoly with strategic sellers and a single buyer, allowing for price competition among

sellers. They study MPE of the game and its convergence behavior. Acemo˘glu and Robinson in [8] develop a

theory of political transitions in a country by modeling it as a repeated game between the elites and the poor, and

study its MPE. When players have private types then an appropriate solution concept is perfect Bayesian equilibrium

(PBE) and sequential equilibirum (SE). Recently authors in [9], [10], [11], [12], [13] presented backward recursive

sequential decomposition methodologies to compute PBE for different classes of dynamic games of incomplete

information.

3

In large population games, computing MPE, PBE and SE with the methods speciﬁed above becomes intractable.

Mean ﬁeld games (MFG) were introduced in Huang, Malham´e,and Caines [14], and Lasry and Lions [15] to model

the strategic interactions with large number of players. In such games, the individual agents have minimal impact

of the overall outcome of the game and so the agents track a mean distribution of states of other agents rather

than their actual states. MFGs is an excellent and a tractable model to study large population dynamic games of

incomplete information, and has been shown to be a good approximation of Nash equilibrium (or MPE) of the

original game as the number of players grow large (for instance see [16], [17], [18], [19], [20] and references

therein).

Parise and Ozdaglar introduced the notion of graphon games [21] to model large population static network

games, where graphon is generative model of a large random graph inroduced by Lov¨asz in [22]. Caines and

Huang in [23] combined the ideas of mean-ﬁeld equillibrium (MFE) and graphon games to deﬁne Graphon Mean

ﬁeld games (GMFGs) where there are a large number of strategic agents with dynamic incomplete information

who interact on an underlying ﬁxed network generated by a known graphon. GMFGs combine the idea of network

games deﬁned through graphons and the mean ﬁeld framework of describing multi agent homogeneous games

and predicting equilibrium in a tractable manner. Large network of nodes interacting with one another can be

represented as graphons and mean ﬁeld games deal with the study of such large interaction among devices and

people as agents to analyze such systems to design and understand the behavior of such large scale interactions and

their impact on our society. The progress in research in the mean ﬁeld domain have been restricted to cases where

the agents interacted in a perfect homogeneous environment and the interactions between the agents were assumed

to be uniform irrespective of the location of the agent in the network. However, in many real world scenarios the

population interaction is not uniform and there is a measure of how the agents interacted with each other or in

other words, the payoff and the transition to the next state is conditional on the relative position of the agent in the

network. Then the mean ﬁeld distribution would be affected by it and so will the optimum policies and the Nash

equilibrium thus generated. The theoretical basis for such a case has been provided in [23] which generalizes the

idea of mean ﬁeld games across the population with different levels of interactions through GMFG.

In this paper, we consider both discounted ﬁnite horizon and inﬁnite-horizon dynamic graphon mean-ﬁeld teams

and games where there is a large population of homogeneous players each having a private type. Each player

sequentially makes decisions and is affected by other players in its neighborhood through a graphon mean-ﬁeld

population state. Each player has a private type that evolves through a controlled Markov process as a function of

the graphon, which only she observes and all players observe a common population state which is the distribution

of other players’ types. In such games, the graphon mean-ﬁeld state evolves through McKean-Vlasov forward

equation given a policy of the players and the graphon function. The equilibrium policy satisﬁes the Bellman

backward equation, given the graphon mean-ﬁeld states. Thus to compute equilibrium, one needs to solve the coupled

backward and forward ﬁxed-point equation in the graphon mean-ﬁeld and the equilibrium policy. We propose a

sequential decomposition algorithm to compute GMFEs and GOMPs by decomposing the problem across time.

This algorithm is equivalent to the Master equation of continuous time mean ﬁeld game [24] that allows one to

4

compute all mean ﬁeld equilibria (MFE) of the game sequentially.1

In order to demonstrate the utility of our algorithm to compute the GMFE and GOMP of a graphon mean ﬁeld

game and a team for varying graphons, we consider a cyber-security example of malware spread problem. A cluster

of nodes in a network of physical servers get infected by an independent random process. For each node, there is

a higher risk of getting infected due to negative externality imposed by other infected players. A graphon function

is deﬁned that quantiﬁes the effect of the effect of the state of other nodes in the network on the concerned node.

At each time t, a node privately observes its own state and publicly observes the population of infected nodes,

based on which it has to make a decision to repair or not. Upon taking an action, the transition of to the next state

is governed by both its individual action and the actions affected by the neighboring agents given by a graphon

function. Using our algorithm, we ﬁnd equilibrium strategies of the players which are observed to be non-decreasing

in the healthy population state. Similarly we ﬁnd optimal Markovian policies for the team problem.

The paper is structured as follows. In Section II, we present a model of the graphon mean ﬁeld game and

team, followed by some preliminary result from our past research regarding MPE in strategic dynamic games.

In section III, we present our main results where we present algorithm to compute MPE for both ﬁnite and

inﬁnite horizon game, and also present existence results. In Section IV we talk about the existence of GMFE. In

Section V, we consider graphon team problem and provide a dynamic program to ﬁnd optimal Markovian policies.

In Section VI, we show the simulation results for the cyber-security example assuming different graphons and

conclude in Section VII.

B. Notation

We use uppercase letters for random variables and lowercase for their realizations. For any variable, subscripts

represent time indices and superscripts represent player identities. We use notation −α to represent all players other

than player α i.e. −α = {1, 2, . . . i − 1, i + 1, . . . , N }. We use notation at:t′ to represent the vector (at, at+1, . . . at′ )
when t′ ≥ t or an empty vector if t′ < t. We use a−α

t

to mean (a1

t , . . . , ai−1
t ). We use the notation
x, and the correct usage is determined depending on the space of x. We remove
t ). We denote the

t , . . . , aN

. . . , aN

, ai+1
t

t , a2

t

superscripts or subscripts if we want to represent the vector, for example at represents (a1
P

P

R

x to represent both

x and

indicator function of any set A by 1{A}. For any ﬁnite set S, P(S) represents space of probability measures on S

and |S| represents its cardinality. We denote by P σ (or Eσ) the probability measure generated by (or expectation
with respect to) strategy proﬁle σ. We denote the set of real numbers by R. For a probabilistic strategy proﬁle

of players (σα
use the short hand notation σ−α

t )i∈[N ] where probability of action aα
(a−α
t

1:t ) to represent
involving random variables are to be interpreted in a.s. sense.

t conditioned on µG
t (aj

1:t, x−α

j6=i σj

|µG

t

Q

1:t, xα
t |µG

1:t is given by σα

t (aα
1:t), we
1:t). All equalities and inequalities

1:t, xα

t |µG

1:t, xj

1Since the publishing an initial version of this paper in [25], authors in [26] have computed a Master’s equation for Linear Quadratic Gaussian

(LQG) GMFG.

5

A. Graphon Mean Field Games and Teams

II. MODEL AND BACKGROUND

Let us consider a discrete-time large population sequential game with N homogeneous players with N → ∞.

The interactions between these N players are captured in a asymptotically inﬁnite network graph represented as
a graphon. Graphons are bounded symmetric Lebesgue measurable functions W : [0, 1]2 → [0, 1] which can be
represented as weighted graphs on the vertex set [0, 1] such that G = {g (α, β) : 0 ≤ α, β ≤ 1} [23]. It is similar to

an adjacency matrix deﬁned over a 2-dimensional plane where each entry in the matrix is the measure of coupling

between the agents concerned.

In each period t ∈ [T ], where [T ] represents the time horizon, a player α ∈ [0, 1] observes a private type xα

t ∈ X

and a common observation µG

t , then takes an action aα

t ∈ A and receives a reward R(xα

t , aα

t , µG

observation is an ensemble of the mean ﬁeld distributions with respect to all agents α ∈ [0, 1] given as µG

t ). The common
t = {µα

t }α

where

with

Nx
i=1 µα

t (i) = 1. Player α’s type evolves as a controlled Markov process,

t (x) = P {xα
µα

t = x}

P

t+1 = ˜f [xα
xα

t , aα

t , µG

t ; gα] + wα
t .

(1)

(2)

The random variables (wα

write the above update of xα

t )α,t are assumed to be mutually independent across players and across time. We also
t ; gα) which depends on the graphon function
t through a kernel, xα

t+1 ∼ Qα(·|xα

t , µG

t , aα

gα = {g(α, β) : 0 ≤ β ≤ 1}.

The dynamics of the MDP are governed both by the local information as well as the global dynamics involving

the effect of the policy action of other players in the system. The idea of graphon is to capture the effect of the

actions of all the other players β ∈ [0, 1] on player α. In prior mean ﬁeld research, it was assumed that there is

a perfect interaction between the players and also that these interactions were uniform. In [23], they provide a set

of differential equations that govern such interactions in the mean ﬁeld setting. The functions below show how the
graphon is used in determining the effect of players on one another. The function ˜f in (2) is given as

˜f [xα

t , aα

t , µG

t ; gα] = f0 (xα

t , aα

t ) + f

t , aα
xα

t , µG

t ; gα

where

f

t , aα
xα

t , µG

t ; gα

=

(cid:2)

(cid:3)

f

t , aα
xα

t , xβ

g (α, β) µβ

t (xβ)d(xβ )dβ

(cid:0)
and f0 represent the local effect of the agent when it takes any action and is independent of the actions taken by

(cid:1)

(cid:2)

(cid:3)

Zβ∈[0,1]

Xxβ∈X

other agents. In the case, when the agents do not interact at all i.e. g(α, β) = 0, the markov process reduces only

to the function f0 ignoring the degenerate case when α = β.
1:t, xα
At instant t, the player α observes the trajectory (µG

strategy σα = (σα
1:t, xα
to be set of observed histories (µG

t )t, where σα
1:t). We denote Z t to be the space of population states µG
1:t) of player α.

t according to a behavioral
t : (µG)t × X t → P(A). We denote the space of such strategies as Kσ. This implies
t = Z t × X t

1:t till time t. We denote Hα

1:t) and takes an action aα

t ∼ σα

t (·|µG

1:t, xα

Aα

(3)

(4)

For ﬁnite time-horizon game, GT , each player wants to maximize its total expected discounted reward over a

time horizon T , discounted by discount factor 0 < δ ≤ 1,

T

6

J α,T
Game := Eσ

δt−1R(X α

t , Aα

t , µG

t ; gα)
#

.

(5)

For the inﬁnite time-horizon game, G∞, each player wants to maximize its total expected discounted reward over

an inﬁnite-time horizon discounted by a discount factor 0 < δ < 1,

∞

J α,∞
Game := Eσ

δt−1R(X α

t , Aα

t , µG

t ; gα)
#

.

(6)

Similarly for ﬁnite time-horizon team, TT , all players wants to maximize their average total expected discounted

"

t=1
X

"

t=1
X

reward over a time horizon T , discounted by discount factor 0 < δ ≤ 1,

T

δt−1





t=1
X

xα
t
X

T eam := Eσ
J T

t (xα
µα

t )R(xα

t , Aα

t , µG

t ; gα)


.

(7)

For the inﬁnite time-horizon team, T∞, each player wants to maximize its total expected discounted reward over



an inﬁnite-time horizon discounted by a discount factor 0 < δ < 1,

T eam := Eσ
J ∞

B. Solution concept: GMFE

∞

δt−1





t=1
X

t (xα
µα

t )R(xα

t , Aα

t , µG

t ; gα)


.

(8)



xα
t
X

For graphon mean ﬁeld games, notion of equilibrium is GMFE [5], which we use in this paper. A GMFE (˜σ)

satisﬁes sequential rationality such that for GT , ∀α ∈ [0, 1] , t ∈ [T ] , µG

1:t, xα

1:t, σα,

T

"

n=t
X

T

E(˜σα ˜σ

−α)

"

n=t
X

δn−tR(X α

n , Aα

n, µG

n ; gα)|µG

1:t, xα
1:t

≥ E(σα ˜σ

−α)

#

δn−tR(X α

n , Aα

n, µG

n ; gα)|µG

1:t, xα
1:t

(9)

,

#

GMFE for G∞ are deﬁned in a similar way where summation in the above equations is taken such that T is

replaced by ∞.

C. Solution concept: Graphon mean ﬁeld team optimal

For graphon mean ﬁeld teams, we use the notion of optimality as follows. A policy (σ∗) is team optimal if for

∀α ∈ [0, 1] , t ∈ [T ] , µG

1:t, σα,

∗

Eσ

T





n=t
X

δn−t

xα
t
X

t (xα
µα

n)R(xα

n, Aα

n, µG

n ; gα)|µG

1:t

≥ Eσ



T

δn−t





n=t
X

xα
t
X

t (xα
µα

n)R(xα

n, Aα

n, µG

n ; gα)|µG

1:t

,



(10)

The notion of optimality for TT are deﬁned in a similar way where summation in the above equations is taken

such that T is replaced by ∞.

III. A METHODOLOGY TO COMPUTE GMFGS

In this section, we will provide a backward recursive methodology to compute GMFGs for both GT and G∞.

We will consider Markovian equilibrium strategies of player α which depend on the common information at time t,

7

t .2 Equivalently, player α takes action of the form Aα
t , and on its current type xα
µG
common agent approach in [3], an alternate and equivalent way of deﬁning the strategies of the players is as follows.

t ). Similar to the

t ∼ σα

t (·|µG

t , xα

We ﬁrst generate partial function γα

t : X → P(A) as a function of µG
t
t = θα

t [µG
t : µG → (X → P(A)) such that γα
θα
t on player α’s current private information xα
γα
t = θα[µG
For a given prescription function γα

t ]. Then action Aα
t , i.e. Aα
t ], the graphon mean-ﬁeld µG

t ∼ γα

t (·|xα

t ). Thus Aα

through an equilibrium generating function

t is generated by applying this prescription function

t ∼ σα

t (·|µG

t , xα

t ) = θα

t [µG

t ](·|xα

t ).

t evolves according to the discrete-time

McKean Vlasov equation, ∀y ∈ X and ∀α ∈ [0, 1]:

which implies

µα

t+1(y) =

µα
t (x)γα

t (a|x)Q

y|x, a, µG

t ; gα

,

a∈A
x∈X X
X

(cid:0)

(cid:1)

t+1 = φ(µα
µα

t , γα

t , µG

t ; gα)

t+1 = φ(µG
µG

t , γt; gα)

(11)

(12)

(13)

A. Backward recursive algorithm for GT

In this subsection, we will provide a methodology to generate GMFE of GT of the form described above. We

deﬁne an equilibrium generating function (θt)t∈[T ], where θt : µG → {X → P(A)}, where for each µG
generate ˜γt = θt[µG

t ]. In addition, we generate a reward-to-go function (V α

t , we
: µG × X → R. These

t )t∈[T ], where V α
t

quantities are generated through a ﬁxed-point equation as follows.

1) Initialize ∀µG

T +1, α, xα

T +1 ∈ X ,

T +1(µG
V α

T +1, xα

T +1)

△
= 0.

(14)

2) For t = T, T − 1, . . . 1, ∀µG

t , let θt[µG
the solution of the following ﬁxed-point equation3, ∀α ∈ [0, 1], xα

t ] be generated as follows. Set ˜γt = (˜γα

t ∈ X ,

t )α∈[0,1] = θt[µG

t ], where ˜γt is

t (·|xα
˜γα

t ) ∈ arg max
t (·|xα
γα
t )

Eγα

t (·|xα
t )

R(X α

t , Aα

t , µG

t ; gα) + δV α

t+1(φ(µG

t , ˜γα

t ; gα), X α

t+1)|µG

t , xα
t

,

(15)

where expectation in (15) is with respect to random variable (Aα

t |xα

t )Qα(xα

t+1|xα
γt(aα
and on the right side in the update of µG

t , µG

t , aα

t , X α
t+1) through the probability measure
t ; gα). We note that the solution of (15), ˜γt, appears both on the left of (15)
t , and is thus unlike the ﬁxed-point equation found in Bayesian Nash

(cid:2)

(cid:3)

equilibrium.

Furthermore, using the quantity ˜γt found above, deﬁne ∀α

t (µG
V α

t , xα
t )

△
= E˜γα

t (·|xα)

R(X α

t , Aα

t , µG

t ; gα) + δV α

t+1(φ(µG

t , ˜γα

t ), X α

t+1)|µG

t , xα
t

(cid:2)
2Note however, that the unilateral deviations of the player are considered in the space of all strategies.

3We discuss the existence of solution of this ﬁxed-point equation in Section IV

(16)

.

(cid:3)

Then, an equilibrium strategy is deﬁned as

˜σα
t (aα

t |µG

1:t, xα

1:t) = ˜γα

t (aα

t |xα

t ),

8

(17)

where ˜γt = θ[µG
t ].

In the following theorem, we show that the strategy thus constructed is a GMFGs of the game.

Theorem 1. A strategy (˜σ) constructed from the above algorithm is an MPE of the game i.e. ∀t, hα

t ∈ Hα

t , σα,

T

E(˜σα ˜σ

−α)

"

n=t
X
E(σα ˜σ

δn−tR(X α

n , Aα

n, µG

n ; gα)|µG

1:t, xα
1:t

≥

#

T

−α)

"

n=t
X

δn−tR(X α

n , Aα

n, µG

n ; gα)|µG

1:t, xα
1:t

#

(18)

Proof. Please see Appendix A.

B. Converse

In the following, we show that every GMFE can be found using the above backward recursion.

Theorem 2 (Converse). Let ˜σ be a GMFE of the graphon mean ﬁeld game. Then there exists an equilibrium

generating function θ that satisﬁes (15) in backward recursion such that ˜σ is deﬁned using θ.

Proof. Please see Appendix C.

C. Backward recursive algorithm for G∞

In this section, we consider the inﬁnite-horizon problem G∞, for which we assume the reward function R to be

absolutely bounded.

We deﬁne an equilibrium generating function θ : µG → {X → P(A)}, where for each µG

t , we generate
t ]. In addition, we generate a reward-to-go function V : µG × X → R. These quantities are generated

˜γt = θ[µG

through a ﬁxed-point equation as follows.

For all µG, set ˜γ = θ[µG]. Then (˜γ, V ) are solution of the following ﬁxed-point equation4, ∀µG, xα ∈ X ,

˜γα(·|xα) ∈ arg max
γα(·|xα)

Eγα(·|xα)

V α(µG, xα) = E˜γ(·|xα)

R(xα, Aα, µG; gα) + δV α(φ(µG, ˜γ; gα), X α
h
R(xα, Aα, µG; gα) + δV α(φ(µG, ˜γ; gα), X α
h

′

′

; gα)|µG, xα

,

)|µG, xα

i

.

i

where expectation in (19) is with respect to random variable (Aα, X α,′) through the measure γ(aα|xα)Qα(xα

Then an equilibrium strategy is deﬁned as

˜σα(aα

t |µG

1:t, xα

1:t) = ˜γ(aα

t |xα

t ),

where ˜γ = θ[µG
t ].

The following theorem shows that the strategy thus constructed is a GMFE of the game.

4We discuss the existence of solution of this ﬁxed-point equation in Section IV

′

(19)

(20)

|xα, aα, µG).

(21)

Theorem 3. A strategy (˜σ) constructed from the above algorithm is a GMFE of the game i.e. ∀t, hα

t ∈ Hα

t , σα,

9

E(˜σα ˜σ

−α)

∞

δn−tR(X α

n , Aα

n, µG

n ; gα)|µG

1:t, xα
1:t

≥

#

δn−tR(X α

n , Aα

n, µG

n ; gα)|µG

1:t, xα
1:t

,

#

(22)

"

n=t
X
E(σα ˜σ

−α)

∞

"

n=t
X

Proof. Please see Appendix D.

D. Converse

In the following, we show that every GMFE can be found using the above backward recursion.

Theorem 4 (Converse). Let ˜σ be a GMFE the graphon mean ﬁeld game. Then there exists an equilibrium generating

function θ that satisﬁes (15) in backward recursion such that ˜σ is deﬁned using θ.

Proof. Please see Appendix F.

In this section, we discuss sufﬁcient conditions for the existence of a solution of the ﬁxed-point equations (15)

IV. EXISTENCE

and (19).

Assumption 1 (A1). The action set A is a compact set.

Assumption 2 (A2). ˜f

t , aα
xα
continuous with respect to aα
t .

(cid:2)

t , µG

t ; gα

(cid:3)

and R(xα

t , aα

t , µG

t ; gα) are Lipschitz continuous in xα

t and uniformly

Assumption 3 (A3). The ﬁrst and second derivatives of ˜f

t , aα
xα

t , µG

t ; gα

and R(xα

t , aα

t , µG

t ; gα) with respect to

xα
t are continuous and bounded.

(cid:2)

(cid:3)

Assumption 4 (A4). ˜f

t , aα
xα

t , µG

t ; gα

are Lipschitz continuous in aα

t and uniformly continuous with respect to

xα
t .

(cid:2)

(cid:3)

Assumption 5 (A5). For any v ∈ R, α ∈ [0, 1] and any probability measure ensemble µG, the set

S (xα

t , v) = arg min
aα
t

[v

˜f

t , aα
xα

t , µG

t ; gα

+ R(xα

t , aα

t , µG

t ; gα)]

(23)

is a singleton and the resulting aα

t as a function of (xα

t , v) is Lipschitz continuous in (xα

t , v) and uniform with

(cid:16)

(cid:2)

(cid:3)(cid:17)

respect to µG

t and gα.

Theorem 5. Under assumptions (A1)-(A5), there exists a solution of the ﬁxed-point equations (15) and (19) for

every t.

Proof. Under the assumption (A1)-(A5), it has been shown in [23] that there exists a solution to the GMFG

equations. Concurrently, Theorem 2 and Theorem 4 show that all GMFE can be found using backward recursion

for the ﬁnite and inﬁnite horizon problems. This proves that under (A1)-(A5), there exists a solution of (15) and (19)

at every t.

V. METHODOLOGY TO COMPUTE GRAPHON MEAN FIELD TEAM OPTIMAL POLICIES

10

In this section, we will provide a common agent based backward recursive dynamic programming methodology to
compute optimal policies for both TT and T∞. As in Section III, we will consider Markovian equilibrium strategies
of player α which depend on the common information at time t, µG

t . Equivalently,

t , and on its current type xα
t ). As before, we ﬁrst generate partial function γα

player α takes action of the form Aα

t (·|µG
through an equilibrium generating function θα

t ∼ σα

t , xα

as a function of µG
t
is generated by applying this prescription function γα

t : µG → (X → P(A)) such that γα

t : X → P(A)
t [µG
t = θα
t ].
t on player α’s current private information

Then action Aα
t
t ∼ γα

t , i.e. Aα
xα

t (·|xα

t ). Thus Aα

t ∼ σα

t (·|µG

t , xα

t ) = θα

t [µG

t ](·|xα

t ).

A. Backward recursive algorithm for TT

In this subsection, we will provide a dynamic programming methodology to generate team optimal strategies of TT
of the form described above. We deﬁne an optimal generating function (θt)t∈[T ], where θt : µG → {X → P(A)},
where for each µG

t ]. In addition, we generate a reward-to-go function (Vt)t∈[T ], where

t , we generate γ∗

t = θt[µG

Vt : µG

t → R. These quantities are generated through a backward recursive optimization equation as follows.

1) Initialize ∀µG

T +1,

VT +1(µG

T +1)

△
= 0.

(24)

2) For t = T, T − 1, . . . 1, ∀µG

t , let θt[µG
of the following optimization equation,

t ] be generated as follows. Set γ∗

t = θt[µG

t ], where γ∗

t is the solution





xα
Xα∈[0,1] X
t





xα
Xα∈[0,1] X
t

γ∗
t ∈ arg max

γt

Eγt

t (xα
µα

t )R(xα

t , Aα

t , µG

t ; gα) + δVt+1(φ(µG

t , γt; gα))|µG

t 

,

(25)

where expectation in (15) is with respect to random variable Aα

t through the probability measure γα

t (aα

t |xα

t ).



Furthermore, using the quantity γ∗

t found above, deﬁne

Vt(µG
t )

△
= E˜γα

t (·|xα)

t (xα
µα

t )R(xα

t , Aα

t , µG

t ; gα) + δVt+1(φ(µG

t , γ∗

t ; gα))|µG

t 

.

(26)

Then, the optimal Markovian strategy is deﬁned as

where γ∗

t = θ[µG
t ].

σ∗,α
t

(aα

t |µG

1:t, xα

1:t) = γ∗,α

t

(aα

t |xα

t ),



(27)

In the following theorem, we show that the strategy thus constructed is an optimal Markovian strategy of the

team problem.

Theorem 6. A strategy (σ∗) constructed from the above algorithm is an optimal Markovian strategy of the team

11

problem i.e. ∀t, µG

1:t, σα,

∗

Eσ

T

δn−t

n=t
X




Eσ

xα
Xα∈[0,1] X
t

T

δn−t





n=t
X

xα
Xα∈[0,1] X
t

µα
t (xα

t )R(xα

n, Aα

n, µG

n ; gα)|µG

1:t

≥


n ; gα)|µG

1:t



µα
t (xα

t )R(xα

n, Aα

n, µG

(28)

Proof. It is easy to see that {µG

t , γt}t is a controlled Markov process for this problem since µG

t+1 = φ(µG

t , γt)

and the current rewards can be written as a function of µG

t , γt. Thus the result is a standard application Markov

decision theory [27].

B. Backward recursive algorithm for T∞

In this section, we consider the inﬁnite-horizon problem T∞, for which we assume the reward function R to be

absolutely bounded.

We deﬁne an optimal generating function θ : µG → {X → P(A)}, where for each µG

t = θ[µG
t ].
In addition, we generate a reward-to-go function V : µG → R. These quantities are generated through a ﬁxed-point

t , we generate γ∗

equation as follows.

For all µG, set γ∗ = θ[µG]. Then (γ∗, V ) are solution of the following ﬁxed-point equation, ∀µG,

γ∗ ∈ arg max

γ

Eγ





xα
Xα∈[0,1] X

t (xα)R(xα, Aα, µG; gα) + δV (φ(µG, γ∗; gα)|µG
µα

,





V (µG) = E˜γ(·|xα)



xα
Xα∈[0,1] X
t

t (xα
µα

t )R(xα, Aα, µG; gα) + δV (φ(µG, γ∗; gα))|µG

.




where expectation in (19) is with respect to random variable (Aα) through the measure γ(aα|xα).



Then the optimal Markovian strategy is deﬁned as

where γ∗

t = φ[µG
t ].

σ∗,α
t

(aα

t |µG

1:t, xα

1:t) = γ∗,α

t

(aα

t |xα

t ),

(29)

(30)

(31)

The following theorem shows that the strategy thus constructed is an optimal Markovian policy of the team

problem.

Theorem 7. A strategy (σ∗) constructed from the above algorithm is an optimal Markovian policy of the team

problem i.e. ∀t, µG

1:t, σα,

∗

Eσ

∞

δn−t

n=t
X




Eσ

xα
Xα∈[0,1] X
t

δn−t

∞





n=t
X

xα
Xα∈[0,1] X
t

t (xα
µα

t )R(xα

n, Aα

n, µG

n ; gα)|µG

1:t

≥

t (xα
µα

t )R(xα

n, Aα

n, µG


n ; gα)|µG

1:t



,

(32)

Proof. By same argument as proof of Theorem 6, since {µG

t , γt}t is a controlled Markov process for this problem

12

as µG

t+1 = φ(µG

t , γt) and the current rewards can be written as a function of µG

t , γt. Also R is absolutely bounded.

Therefore, the result is a standard application Markov decision theory [27].

VI. NUMERICAL EXAMPLE

In this section, we put forth a numerical example to showcase the proposed sequential decomposition in the

context of a system where the relative position of the players with respect to other players in a graph affects the

state of the player as well as their equilibrium strategies. We provide the following deﬁnition.

Deﬁnition 1. Players α and β are statistically equivalent if φα(µ, γ, µG; gα) = φβ(µ, γ, µG; gβ).

Proposition 1. Mean ﬁeld games with statistically equivalent players share the same mean ﬁeld distribution and

µG can be replaced by µα for all α ∈ [0, 1].

For a complete, Erdos R´enyi, symmetric stochastic block model, and random geometric graphon, every player

is statistically equivalent. Thus from proposition 1, the players share the same McKean-Vlasov (MKV) mean ﬁeld

evolution function and so the same mean ﬁeld.

Let n be the total number of statistically different players. Then µG can be replaced by {µ}i=1,...,n. With the

proposition we can represent the graphon mean ﬁeld population state as

µG = {µ}i=1,...,n = µ ∀i

(33)

A. Cybersecurity Example

We consider a cyber-security example where a cluster of nodes, facing a possible malware attack in a network,

do a cost-beneﬁt analysis to determine whether to opt for repairing. The results of this analysis, however, could be

extended to many different cases like the vaccination in a population, entry and exit of ﬁrms, ﬁnancial markets,

demand response in smart-grid and so on. The dynamics of each of the node is affected by the action of the

neighboring nodes connected with different measures captured in a network graph and represented as a graphon

function G. In this example, we assume different graphon functions and obtain the optimal policies using our

sequential decomposition algorithm assuming that the graphs are symmetric with respect to the participating agents.

In the model, the node can have two states xα ∈ X = {0, 1} representing healthy and infected node respectively.

Similarly, there are two actions at their disposal for each of the state aα ∈ A = {0, 1} which says whether the

nodes gets repaired with a cost or takes the risk by not undergoing repair. The chances of a node getting affected by

a malware attack depends on the population as well as the state of the neighboring nodes according to the graphon.

The dynamics of the model are given as

xα
t+1 =

t + (1 − xα
xα

t ) wα
t

if aα

t = 0




0

otherwise

where wα

t ∈ {0, 1} is a binary random variable with



P{wα

t = 1} =

Zβ∈[0,1]

f

t , aα
xα

t , xβ

g (α, β) µβ

t (xβ)d(xβ )dβ

Xxβ∈X

(cid:0)

(cid:1)

(34)

(35)

It is assumed that the value of P{wα

t = 1} is q when the graph is fully connected i.e. g (α, η) = 1 and the mean state
of the neighbors µt(xβ ) = 1. The value of q is assumed to be 0.9 for our game and 0.4 for the team simulations.

The reward function is given as

13

r (xα

t , aα

t , µt) = −kxα

t − λaα
t

(36)

The value k represents the penalty if the node gets infected and λ represents the cost of repair. The values k and λ

are assumed as 0.3 and 0.2 respectively for our simulation. Here we implement our algorithm to derive equilibrium

for this problem by considering three popular network models to capture the interaction between the population.

We consider the following graphons:

1) Fully Connected Graph: The graphon function is given as

2) Erd¨os Renyi Graph: The graphon function is given as

g(α, β) = 1 ∀α, β

g(α, β) = p ∀α, β

We assume a value p = 0.8 for our simulation.

3) Stochastic Block Model: The graphon function is given as

g(α, β) =

p if α, β ≤ 0.5 or α, β ≥ 0.5




q otherwise

(37)

(38)

(39)

Here, p represents the intra-community interaction and is assumed as p = .9 for our simulation. Similarly,



q = .4 represents the inter-community interaction parameter.

4) Random Geometric graph: The graphon function is given as

where f : [0, .5] → [0, 1] is a non-increasing function, and in our simulation we assume it to be f (x) = e

g(α, β) = f (min(β − α, 1 − β + α))

(40)

x

0.5−x .

Figure VI-A shows the equilibrium policy derived for different graphons for the speciﬁc cyber-security example.

The policies differ as the interaction of the agents with their neighbors inﬂuences their strategies. Figure VI-A gives

the relation between µt and µt+1 as presented in the (11). Figure VI-A shows the equilibrium mean ﬁeld or in the

speciﬁc case that we consider when with time, the a mean ﬁeld distribution of 0.5 approaches different mean ﬁeld

states for different graphons but with the same state dynamics. In Figures VI-A, VI-A, VI-A, we plot the policies

and mean ﬁeld equilibrium or different graphons for the speciﬁc cyber-security example when the agents cooperate

as a team.

VII. CONCLUSION

In this paper, we consider both ﬁnite and inﬁnite horizon, large population dynamic game (with individual rewards)

and team(with common rewards) where each player is affected by others through a graphon mean-ﬁeld population

state. We present a novel backward recursive algorithm to compute non-stationary, signaling GMFG and GOMP for

such games, where each player’s strategy depends on its current private type and the current graphon mean-ﬁeld

1

0.5

0

0

1

0.5

0

0

Fully Connected Graph

0.5

1

Statistical Block Graph

0.5

1

Figure 1. Policy action at higher state for all graphons for the game

Fully Connected Graph

14

Erdos-Renyi Graph

0.5

1

Random Geometric Graph

0.5

1

Erdos-Renyi Graph

1

0.5

0

0

1

0.5

0

0

1

0.8

0.6

0.4

1

0.5

0

0

1

0.8

0.6

0.4

0.2

0

0.5

1

0.2

0

0.5

1

Statistical Block Graph

0.5

1

1

0.9

0.8

0.7

0.6

0

Random Geometric Graph

0.5

1

Figure 2. Mean Field Evolution at different mean ﬁelds for the game problem

population state. The non-triviality in the problem is that the update of population state is coupled to the strategies

of the game, and is managed in the algorithm through unique construction of the ﬁxed-point equations (15),(19) for

GMFE and through an optimization problem (25) for the team problem. We proved the existence of the ﬁxed-point

equations (15) under certain conditions. Using this algorithm, we considered a malware propagation problem where

Figure 3. Convergence to GMFGs for all graphons with time for the game problem

15

Fully Connected Graph

Erdos-Renyi Graph

0.45

0.4

0.35

0.3

50

100

0.25

0

50

100

Statistical Block Graph

Random Geometric Graph

0.3

0.2

0.1

0

0

0.6

0.58

0.56

0.54

1

0.8

0.6

0.4

0.2

0

0

1

0.8

0.6

0.4

0.2

0

0

0

50

100

Fully Connected Graph

0.5

1

Statistical Block Graph

1

0.9

0.8

0.7

0

1

0.8

0.6

0.4

0.2

0

0

50

100

Erdos-Renyi Graph

0.5

1

Random Geometric Graph

1

0.8

0.6

0.4

0.2

0.5

1

0

0

0.5

1

Figure 4. Policy action at higher state for all graphons for the team problem

we numerically computed equilibrium and team optimal strategies of the players. In general, this algorithm be could

instrumental in studying non-stationary equilibria and optimal control in a number of applications such as ﬁnancial

markets, social learning, renewable energy and more.

16

0

-0.2

-0.4

-0.6

-0.8

0

0

-0.2

-0.4

-0.6

0

Fully Connected Graph

Erdos-Renyi Graph

0

-0.2

-0.4

-0.6

0.5

1

-0.8

0

0.5

1

Statistical Block Graph

Random Geometric Graph

0

-0.2

-0.4

-0.6

-0.8

0.5

1

-1

0

0.5

1

Figure 5. Mean Field Evolution at different mean ﬁelds for the team problem

2

1.5

1

0.5

0

0

1

0.98

0.96

0.94

0.92

0.9

0

Fully Connected Graph

Erdos-Renyi Graph

1

0.98

0.96

0.94

0.92

50

100

0.9

0

50

100

Statistical Block Graph

Random Geometric Graph

1

0.98

0.96

50

100

0.94

0

50

100

Figure 6. Convergence to GMFGs for all graphons with time for the team problem

Proof. We prove (28) using induction and the results in Lemma 1, and 2 proved in Appendix B. For base case at

APPENDIX A

t = T , ∀(µG

1:T , xα

1:T ) ∈ Hα

T , σα

−α
E˜σα
T ˜σ
T

R(X α

t , Aα

t , µG

t ; gα)

1:T , xα
µG
1:T

= V α

T (µG

T , xα
T )

(cid:8)

(cid:12)
(cid:12)

(cid:9)

−α
≥ Eσα
T ˜σ
T

R(X α

t , Aα

t , µG

t ; gα)

1:T , xα
µG
1:T

,

(41a)

(41b)

(cid:8)

(cid:12)
(cid:12)

(cid:9)

17

(42a)

(42b)

(43a)

(43b)

(43c)

(43d)

(43e)

(43f)

where (41a) follows from Lemma 2 and (41b) follows from Lemma 1 in Appendix B.

Let the induction hypothesis be that for t + 1, ∀i ∈ [N ], µG

1:t+1 ∈ (Hc

t+1), xα

1:t+1 ∈ (X )t+1, σα,

E˜σα

t+1:T ˜σ

−α
t+1:T

δn−t−1R(X α

n , Aα

n, µG

n ; gα)

µG
1:t+1, xα

1:t+1

T

(

n=t+1
X
T

(
n=t+1
X
t , σα, we have

≥ Eσα

t+1:T ˜σ

−α
t+1:T

δn−t−1R(X α

n , Aα

n, µG

n ; gα)

Then ∀i ∈ [N ], (µG

1:t, xα

1:t) ∈ Hα
T

(cid:12)
(cid:12)
µG
1:t+1, xα

1:t+1

)

.

)

(cid:12)
(cid:12)

µG
1:t, xα
1:t

t , Aα
T

n=t+1
X
t , Aα
T

E˜σα

t:T ˜σ

−α
t:T

(

n=t
X
t , xα
t )

= Vt(µG

−α
≥ Eσα
t ˜σ
t

−α
= Eσα
t ˜σ
t

δE˜σα

t+1:T ˜σ

(cid:8)
−α
t+1:T

(

δn−t−1R(X α

n , Aα

n, µG

n ; gα)

(cid:12)
(cid:12)

1:t, xα
µG
1:t

)

R(X α

t , Aα

t , µG

t ; gα) + δV α

t+1(µG

t+1, X α

t+1)

(cid:8)
R(X α

t , µG

t ; gα)+

δn−t−1R(X α

n , Aα

n, µG

n ; gα)

−α
≥ Eσα
t ˜σ
t

R(X α

t , µG

t ; gα)+

δEσα

t+1:T ˜σ

(cid:8)
−α
t+1:T

−α
= Eσα
t ˜σ
t

R(X α

t , µG

t ; gα)+

(

n=t+1
X
t , Aα

δn−t−1R(X α

n , Aα

n, µG

n ; gα)

δEσα

t:T ˜σ

−α
t:T

(cid:8)

(

= Eσα

t:T ˜σ

−α
t:T

T

n=t+1
X
T

(

n=t
X

δn−t−1R(X α

n , Aα

n, µG

n ; gα)

δn−tR(X α

n , Aα

n, µG

n ; gα)

(cid:12)
(cid:12)
1:t, xα
µG
1:t

,

)

(cid:12)
(cid:12)

(cid:9)

1:t, µG
µG

t+1, xα

1:t, X α

t+1

1:t, µG
µG

t+1, xα

1:t, X α

t+1

(cid:12)
(cid:12)

(cid:12)
(cid:12)

1:t, µG
µG

t+1, xα

1:t, X α

t+1

1:t, xα
µG
1:t

1:t, xα
µG
1:t

)

)

)

)

(cid:12)
(cid:12)

(cid:12)
(cid:12)

1:t, xα
µG
1:t

(cid:9)

)

(cid:12)
(cid:12)

(cid:12)
where (43a) follows from Lemma 2, (43b) follows from Lemma 1, (43c) follows from Lemma 2, (43d) follows
(cid:12)

from induction hypothesis in (42b) and (43e) follows since the random variables involved in the right conditional

expectation do not depend on strategies σα
t .

APPENDIX B

Lemma 1. ∀t ∈ [T ], i ∈ [N ], (µG

1:t, xα

1:t) ∈ Hα

t , σα
t

t (µG
V α

t , xα

−α
t ) ≥ Eσα
t ˜σ
t

R(X α

t , Aα

t , µG

t ; gα) + δV α

t+1(µG

t+1, X α

t+1)

1:t, xα
µG
1:t

.

(44)

Proof. We prove this lemma by contradiction.

(cid:8)

(cid:12)
(cid:12)

(cid:9)

Suppose the claim is not true for t. This implies ∃i,

σα
t ,

µG
1:t,

xα
1:t such that

−α
Ebσα
t ˜σ
t

R(X α

t , Aα

t , µG

t ; gα) + δV α

t+1, X α
t+1(µG
b
b
b

t+1)

µG

1:t,

xα
1:t

> Vt(

µG
t ,

xα
t ).

We will show that this leads to a contradiction. Construct

(cid:8)

t (aα
γα

t |xα

t ) =

b

µG
1:t,

t (aα
σα
t |
arbitrary
b
b






(cid:9)

(cid:12)
(cid:12)b
b
xα
xα
t =
t

xα
1:t)

otherwise.
b

b

b

b

(45)

(46)

Then for

µG

1:t,

xα
1:t, we have

V α
t (

µG
t ,
b

xα
t )
b

= max
b
b
γt(·|bxα
t )

Eγt(·|bxα

−α
t )˜σ
t

R(

xα
t , Aα
t ,

µG
t ) + δV α

t+1(φ(

µG
t , ˜γt; gα), X α

t+1)

µG
t ,

xα
t

,

≥ Ebγα

t (·|bxα

−α
t )˜σ
t

(cid:8)
t , µG
t , Aα
R(X α
b

t ; gα) + δV α
b

t+1(φ(

t , ˜γt; gα), X α
µG
b

t+1)

(cid:12)
(cid:12)b
µG
t ,

xα
b
t

(cid:9)

R(

(cid:8)
t , aα
xα
t ,

b
xα
t , aα
t ,

R(

=

=

t ,xα
aα
X

t+1 (cid:8)

t ,xα
aα
t+1 (cid:8)
X
−α
= Ebσα
t ˜σ
t

µG

t ) + δVt+1(φ(

t , ˜γt; gα), xα
µG

t+1)

b

γt(aα
t |

(cid:12)
(cid:9)
t )Qα(xα
xα
t+1|
(cid:12)b
b

t , aα
xα
t ,

t ; gα)
µG

b
µG
t ) + δV α

t+1(φ(

b
µG
t , ˜γt; gα), xα

(cid:9)
t+1)

b
b
σt(aα
µG
t |

1:t,

xα
1:t)Qα(xα

t+1|

b

b
xα
t , aα
t ,

µG
t ; gα)

b

b

b
b
xα
t , aα
µG
t ) + δV α
R(
t ,

b
µG
t , ˜γt), X α
t+1(φ(

> V α
t (

µG
t ,

(cid:8)
xα
t ),
b

b

b

b
µG
1:t,

(cid:9)
t+1; gα)
(cid:12)
(cid:12)b

b
xα
1:t

b

(cid:9)

b

where (47a) follows from deﬁnition of Vt in (16), (47c) follows from deﬁnition of

b

b

γα
t and (47d) follows from (45).

However this leads to a contradiction.

Lemma 2. ∀i ∈ [N ], t ∈ [T ], (µG

1:t, xα

1:t) ∈ Hα
t ,

t (µG
V α

t , xα

t ) = E˜σα

t:T ˜σ

−α
t:T

n=t
X
Proof. We prove the lemma by induction. For t = T ,

(

T

δn−tR(X α

n , Aα

n, µG

n ; gα)

b

1:t, xα
µG
1:t

.

)

(cid:12)
(cid:12)

−α
E˜σα
T ˜σ
T

R(X α

t , Aα

t , µG

t ; gα)

V αertµG

1:T , xα
1:T

=

R(X α

t , Aα

t , µG

t ; gα)˜σα

T (aα

T |µG

T , xα
T )

(cid:8)

(cid:12)
(cid:12)

(cid:9)

aα
T
X
T (µG
= V α

T , xα

T ),

where (49b) follows from the deﬁnition of V α
t

in (16). Suppose the claim is true for t + 1, i.e., ∀i ∈ [N ], t ∈

[T ], (µG

1:t+1, xα

1:t+1) ∈ Hα

t+1

t+1(µG
V α

t+1, xα

t+1) = E˜σα

t+1:T σ

−α
t+1:T

T

(

n=t+1
X

δn−t−1R(X α

n , Aα

n, µG

n ; gα)

(cid:12)
(cid:12)

µG
1:t+1, xα

1:t+1

.

)

(50)

18

(47a)

(47b)

(47c)

(47d)

(48)

(49a)

(49b)

Then ∀i ∈ [N ], t ∈ [T ], (µG

1:t) ∈ Hα

t , we have

E˜σα

t:T ˜σ

−α
t:T

δn−tR(X α

n , Aα

n, µG

n ; gα)

1:t, xα
T

n=t
X

(
−α
t:T

1:t, xα
µG
1:t

)

(cid:12)
(cid:12)

= E˜σα

t:T ˜σ

R(X α

t , Aα

t , µG

t ; gα)

+δE˜σα

t:T ˜σ

−α
t:T

= E˜σα

t:T ˜σ

−α
t:T

(cid:8)

T

(

n=t+1
X
R(X α

+δE˜σα

t+1:T ˜σ

(cid:8)
−α
t+1:T

t , µG

t ; gα)

t , Aα
T

δn−t−1R(X α

n , Aα

n, µG

n ; gα)

1:t, µG
µG

t+1, xα

1:t, X α

t+1

1:t, xα
µG
1:t

)

)

(cid:12)
(cid:12)

(cid:12)
(cid:12)

δn−t−1R(X α

n , Aα

n, µG

n ; gα)

µG
1:t, µG

t+1, xα

1:t, X α

t+1

= E˜σα

t:T ˜σ

−α
t:T

R(X α

t , µG

t ; gα) + δV α

t+1(µG

(cid:12)
(cid:12)
t+1, X α
t+1)

µG
1:t, xα
1:t

(

n=t+1
X
t , Aα

−α
= E˜σα
T ˜σ
T

(cid:8)
R(X α

t , Aα

t , µG

t ; gα) + δV α

t+1(µG

t+1, X α

t+1)

(cid:12)
µG
1:t, xα
(cid:12)
1:t

(cid:9)

= V α

t (µG

(cid:8)
t , xα

t ),

(cid:12)
(cid:12)

(cid:9)

)

µG
1:t, xα
1:t

(cid:12)
(cid:12)

)

19

(51a)

(51b)

(51c)

(51d)

(51e)

(51c) follows from the induction hypothesis in (50), (51d) follows because the random variables involved in

expectation, X α

t , Aα

t , µG

t , µG

t+1, X α

t+1 do not depend on ˜σα

t+1:T σ−α

t+1:T and (51e) follows from the deﬁnition of

V α
t

in (16).

Proof. We prove this by contradiction. Suppose for any equilibrium generating function θ that generates an MPE

APPENDIX C

˜σ, there exists t ∈ [T ], i ∈ [N ], µG

1:t ∈ Hc

t , such that (15) is not satisﬁed for θ i.e. for ˜γt = θt[µG

t ] = ˜σt(·|µG

t , ·),

˜γt(·|xα

t ) 6∈ arg max
γt(·|xα
t )

Eγt(·|xα
t )

Rt(X α

t , Aα

t , µG

t ; gα) + V α

t+1(φ(µG

t , ˜γt; gα), X α

t+1)

t , µG
xα
t

.

(52)

(cid:8)
Let t be the ﬁrst instance in the backward recursion when this happens. This implies ∃

(cid:12)
γt such that
(cid:12)

(cid:9)

Ebγt(·|xα
t )

> E˜γt(·|xα
t )

Rt(X α

t , Aα

t , µG

t ; gα) + Vt+1(φ(µG

t , ˜γt; gα), X α

t+1)

(cid:8)
Rt(X α

t , Aα

t , µG

t ; gα) + V α

t+1(φ(µG

t , ˜γt; gα), X α

(cid:12)
t+1)
(cid:12)

(cid:8)

(cid:12)
(cid:12)

1:t, xα
µG
b
1:t

1:t, xα
µG
1:t

(cid:9)

(cid:9)

(53)

This implies for

σt(·|µG

t , ·) =

γt,

E˜σt:T

T

(

n=t
X

Rn(X α
n , Aα
b

n, µG

n ; gα)
b

(cid:12)
(cid:12)

µG
1:t−1, xα
1:t

)

= E˜σα

−α
t ,˜σ
t

(

Rt(X α

t , Aα

t , µG

t ; gα) + E˜σα

t+1:T ˜σ

−α
t+1:T

T

(

n=t+1
X

Rn(X α

n , Aα

n, µG

n ; gα)

(cid:12)
(cid:12)

t , xα
µG
t

= E˜γt(·|xt)˜γ

−α
t

Rt(X α

t , Aα

t , µG

t ; gα) + V α

t+1(φ(µG

t , ˜γt; gα), X α

t+1)

< Ebσt(·|µG

t ,xα

−α
(cid:8)
t )˜γ
t

Rt(X α

t , Aα

t , µG

t ; gα) + V α

= Ebσt ˜σ

−α
t

(

(cid:8)
t , Aα
Rt(X α

t , µG

t ; gα) + E˜σα

t+1:T ˜σ

−α
t+1:T

(cid:12)
t+1(φ(µG
t , ˜γt; gα), X α
t+1)
(cid:12)
T

µG

(cid:9)
t , xt

Rn(X α

n , Aα

(cid:12)
(cid:12)
n, µG

(cid:9)
n ; gα)

= Ebσt,˜σα

t+1:T ˜σ

−α
t:T

(

n=t
X

T

Rn(X α

n , Aα

n, µG

n ; gα)

(

n=t+1
X

1:t, xα
µG
1:t

,

)

(cid:12)
(cid:12)

20

)

(cid:12)
(cid:12)

(54)

µG
1:t, xα
1:t

)

(55)

(56)

(57)

µG
1:t−1, µG

t+1, xα

1:t, X α

t+1

1:t, µG
µG

t+1, xα

1:t, X α

t+1

)

(cid:12)
(cid:12)

(cid:12)
(cid:12)

1:t, xα
µG
1:t

)

(58)

(59)

σt, (81)

b

where (79) follows from the deﬁnitions of ˜γt and Lemma 2, (80) follows from (76) and the deﬁnition of

follows from Lemma 1. However, this leads to a contradiction since ˜σ is an MPE of the game.

We divide the proof into two parts: ﬁrst we show that the value function V is at least as big as any reward-to-go

APPENDIX D

function; secondly we show that under the strategy ˜σ, reward-to-go is V α. Note that hα

t := (µG

1:t, xα

1:t).

Part 1: For any i ∈ [N ], σα deﬁne the following reward-to-go functions

∞

(

n=t
X
T

δn−tR(X α

n , Aα

n, µG

n ; gα) | hα
t

)

W σα
t

(hα

t ) = Eσα,˜σ

−α

W σα,T

t

(hα

t ) = Eσα,˜σ

−α

(

n=t
X

δn−tR(X α

n , Aα

n, µG

n ; gα) + δT +1−tV α(µG

T +1, X α

T +1) | hα
t

(60a)

.

(60b)

)
t ) is ﬁnite ∀ i, t, σα, hα
t .

Since X , A are ﬁnite sets the reward R is absolutely bounded, the reward-to-go W σα

(hα

t

For any i ∈ [N ], hα

t ∈ Hα
t ,

V α

t , xα
µG
t

− W σα
t

(hα

t ) =

V α

t , xα
µG
t

− W σα,T

t

Combining results from Lemmas 4 and 5 in Appendix D, the term in the ﬁrst bracket in RHS of (61) is non-negative.

(cid:0)

(cid:1)

h

(cid:0)

(cid:1)

+

(hα
t )
i

t

W σα,T
h

(hα

t ) − W σα

t

(hα
t )
i

(61)

Using (60), the term in the second bracket is

δT +1−t

Eσα,˜σ

−α

−

∞

δn−(T +1)R(X α

n , Aα

n, µG

n ; gα) + V α(µG

T +1, X α

T +1) | hα
t

.

(62)

The summation in the expression above is bounded by a convergent geometric series. Also, V α is bounded. Hence

(cid:0)

(cid:1)

n

n=T +1
X

o

the above quantity can be made arbitrarily small by choosing T appropriately large. Since the LHS of (61) does

not depend on T , which implies,

V α

t , xα
µG
t

≥ W σα
t

(hα

t ).

(cid:0)

(cid:1)

(63)

)

o

Part 2: Since the strategy the equilibrium strategy ˜σ generated in (31) is such that ˜σα

t depends on hα

t only

21

through µG

t and xα

t , the reward-to-go W ˜σα

t

, at strategy ˜σ, can be written (with abuse of notation) as

W ˜σα
t

For any hα

t ∈ Hα
t ,

(hα

t ) = W ˜σα

t

(µG

t , xα

t ) = E˜σ

∞

(

n=t
X

δn−tR(X α

n , Aα

n, µG

n ; gα) | µG

t , xα
t

.

)

W ˜σα
t

(µG

t , xα

t ) = E˜σ

R(X α

t , Aα

t , µG

t ; gα) + δW ˜σα

t+1

φ(µG

t , θ[µG

t ], gα)), X α

t+1

| µG

t , xα
t

V α(µG

t , xα

t ) = E˜σ

n
R(X α

t , Aα

t , µG

t ; gα) + δV α

(cid:0)
φ(µG

t , θ[µG

t ], gα)), X α

t+1

n
Repeated application of the above for the ﬁrst n time periods gives

(cid:0)

(cid:1)
| µG

t , xα
t

o

.

(cid:1)

o

W ˜σα
t

(µG

t , xα

t ) = E˜σ

V α(µG

t , xα

t ) = E˜σ

Taking differences results in

t+n−1

m=t
X
t+n−1

m=t
X

(

(

δm−tR(X α

t , Aα

t , µG

t ; gα) + δnW ˜σα

t+n

t+n, X α
µG

t+n

| µG

t , xα
t

δm−tR(X α

t , Aα

t , µG

t ; gα) + δnV α

(cid:0)
t+n, X α
µG

t+n

(cid:1)
| µG

t , xα
t

(cid:0)

(cid:1)

.

)

(64)

(65a)

(65b)

(66a)

(66b)

W ˜σα
t

(µG

t , xα

t ) − V α(µG

t , xα

t ) = δnE˜σ

t+n, X α
µG

t+n

− V

t+n, X α
µG

t+n

| µG

t , xα
t

.

(67)

(cid:0)
Taking absolute value of both sides then using Jensen’s inequality for f (x) = |x| and ﬁnally taking supremum over

(cid:1)

(cid:0)

(cid:1)

W ˜σα
t+n
n

hα
t reduces to

sup
hα
t

(cid:12)
(cid:12)

W ˜σα
t

(µG

t , xα

t ) − V α(µG

t , xα
t )

W ˜σα

t+n(µG

t+n, X α

t+n) − V α(µG

t+n, X α

t+n)

| µG

t , xα
t

.

(68)

Now using the fact that Wt+n, V are bounded and that we can choose n arbitrarily large, we get suphα
V α(µG

t , xα

t )| = 0.

t

(cid:12)
(cid:12)

o
|W ˜σα
t

(µG

t , xα

t )−

≤ δn sup
hα
t

E˜σ

(cid:12)
(cid:12)

n(cid:12)
(cid:12)

APPENDIX E

In this section, we present three lemmas. Lemma 3 is intermediate technical results needed in the proof of

Lemma 4. Then the results in Lemma 4 and 5 are used in Appendix C for the proof of Theorem 7. The proof

for Lemma 3 below isn’t stated as it analogous to the proof of Lemma 1 from Appendix B, used in the proof of

Theorem 6 (the only difference being a non-zero terminal reward in the ﬁnite-horizon model).

Deﬁne the reward-to-go W σα,T

t

for any agent i and strategy σα as

W σα,T

t

(µG

1:t, xα

1:t) = Eσα,˜σ

T

−α

δn−tR(X α

n , Aα

n, µG

n ; gα) + δT +1−tG(µG

T +1, X α

T +1) | µG

1:t, xα
1:t

.

(69)

n=t
X

(cid:2)

Here agent i’s strategy is σα whereas all other agents use strategy ˜σ−α deﬁned above. Since X , A are assumed to

be ﬁnite and G absolutely bounded, the reward-to-go is ﬁnite ∀ i, t, σα, µG

1:t, xα

1:t. In the following, any quantity

with a T in the superscript refers the ﬁnite horizon model with terminal reward G.

Lemma 3. For any t ∈ [T ], i ∈ [N ], µG

1:t, xα

1:t and σα,

V T,α
t

(µG

t , xα

t ) ≥ Eσα,˜σ

−α

(cid:2)

R(X α

t , Aα

t , µG

t ; gα) + δV T,α
t+1

φ(µG

t , θ[µG

t ], gα), X α

t+1

| µG

1:t, xα
1:t

.

(70)

(cid:0)

(cid:1)

(cid:3)

(cid:3)

The result below shows that the value function from the backwards recursive algorithm is higher than any

reward-to-go.

22

Lemma 4. For any t ∈ [T ], i ∈ [N ], µG

1:t, xα

1:t and σα,

V T,α
t

(µG

t , xα

t ) ≥ W σα,T

t

(µG

1:t, xα

1:t).

(71)

Proof. We use backward induction for this. At time T , using the maximization property from (15) (modiﬁed with

terminal reward G),

T , xα
T )

(µG

V T,α
T
= E˜γα,T
△

T

(·|xα

−α,T
T ),˜γ
T

≥ Eγα,T

T

(·|xα

−α,T
T ),˜γ
T

(cid:2)

= W σα,T

T

(hα
T )

(cid:2)

R(X α

t , Aα

t , µG

t ; gα) + δG

φ(µG

T , ˜γT

T )), X α

T +1

R(X α

t , Aα

t , µG

(cid:0)
t ; gα) + δG

φ(µG

T , ˜γT

T )), X α

T +1

(cid:0)

| µG

T , xα
T

| µG

(cid:3)
1:T , xα
1:T

(cid:1)

(cid:1)

(cid:3)

Here the second inequality follows from (15) and (16) and the ﬁnal equality is by deﬁnition in (69).

Assume that the result holds for all n ∈ {t + 1, . . . , T }, then at time t we have

V T,α
t

(µG

t , xα
t )

≥ Eσα

−α
t ,˜σ
t

≥ Eσα

−α
t ,˜σ
t

(cid:2)

R(X α

t , Aα

t , µG

t ; gα) + δV T,α
t+1

φ(µG

t ], gα), X α
t , θ[µG
T

t+1

| µG

1:t, xα
1:t

R(X α

t , Aα

t , µG

t ; gα) + δEσα

(cid:0)
t+1:T ,˜σ

−α
t+1:T

(cid:1)
δn−(t+1)R(X α

(cid:3)
n, µG

n ; gα)

n , Aα

T +1) | µG

1:t, xα

1:t, µG

t+1, X α

t+1

(cid:2)
| µG

n=t+1
X
1:t, xα
1:t

(cid:2)
T +1, X α
+ δT −tG(µG
T

= Eσα

t:T ,˜σ

−α
t:T

δn−tR(X α

n , Aα

n, µG

(cid:3)
n ; gα) + δT +1−tG(µG

(cid:3)

T +1, X α

T +1) | µG

1:t, xα
1:t

= W σα,T

t

n=t
X
(cid:2)
1:t, xα
(µG

1:t)

(cid:3)

(72a)

(72b)

(72c)

(72d)

(73a)

(73b)

(73c)

(73d)

(73e)

Here the ﬁrst inequality follows from Lemma 3, the second inequality from the induction hypothesis, the third

equality follows since the random variables on the right hand side do not depend on σα

t , and the ﬁnal equality by

deﬁnition (69).

The following result highlights the similarities between the ﬁxed-point equation in inﬁnite-horizon and the

backwards recursion in the ﬁnite-horizon.

Lemma 5. Consider the ﬁnite horizon game with G ≡ V α. Then V T,α

t = V α, ∀ i ∈ [N ], t ∈ {1, . . . , T } satisﬁes

the backwards recursive construction stated above (adapted from (15) and (16)).

Proof. Use backward induction for this. Consider the ﬁnite horizon algorithm at time t = T , noting that V T,α
G ≡ V α,

T +1 ≡

˜γT,α
T (· | xα

EγT (·|xα
T )
T ) ∈ arg max
γT (·|xα
T )

R(X α

t , Aα

t , µG

t ; gα) + δV α

φ(µG

T , ˜γT,α

t

), X α

T +1

| µG

T , xα
T

V T,α
T

(µG

T , xα

T ) = E˜γT

T (·|xα
T )

R(X α

(cid:2)
t , µG
t , Aα

t ; gα) + δV

φ(µG

T , ˜γT

(cid:0)
t ), X α

T +1

(cid:2)

(cid:0)

(cid:1)

(cid:3)

| µG

T , xα
T

.

(cid:1)

(cid:3)

(74a)

(74b)

23

Comparing the above set of equations with (19), we can see that the pair (V α, ˜γα) arising out of (19) satisﬁes
n ≡ V α for all n ∈ {t + 1, . . . , T }. At time t, in the ﬁnite horizon construction
t+1 from the induction hypothesis, we get the same set of equations

the above. Now assume that V T,α
from (15), (16), substituting V α in place of V T,α
as (74). Thus V T,α

t ≡ V α satisﬁes it.

Proof. We prove this by contradiction. Suppose for the equilibrium generating function θ that generates MPE ˜σ,

APPENDIX F

there exists t ∈ [T ], α ∈ [0, 1], µG

1:t ∈ Hc

t , such that (15) is not satisﬁed for θ i.e. for ˜γt = θ[µG

t ] = ˜σ(·|µG

t , ·),

˜γα
t 6∈ arg max
t (·|xα
γα
t )

Eγt(·|xα
t )

R(X α

t , Aα

t , µG

t ; gα) + δV α(φ(µG

t , ˜γt), X α

t+1)

xα
t , µG
t

.

(75)

(cid:8)
Let t be the ﬁrst instance in the backward recursion when this happens. This implies ∃

(cid:12)
γα
t such that
(cid:12)

(cid:9)

Ebγα

t (·|xt)

> E˜γα

t (·|xt)

(cid:8)

R(X α

t , Aα

t , µG

t ; gα) + δV α(φ(µG

t , ˜γα

t ; gα), X α

t+1)

R(X α

t , Aα

t , µG

t ; gα) + δV α(φ(µG

t , ˜γα

t ; gα), X α

(cid:12)
t+1)
(cid:12)

This implies for

σα(·|µG

t , ·) =

(cid:8)
γα
t ,

(cid:12)
(cid:12)

E˜σα
b

∞

(

n=t
X

δn−tR(X α
b

n , Aα

n, µG

n ; gα)

(cid:12)
(cid:12)

1:t−1, xα
µG
1:t

)

1:t, xα
µG
b
1:t

µG
1:t, xα
1:t

(cid:9)

(cid:9)

= E˜σα

−α
t ,˜σ
t

R(X α

t , Aα
∞

t , µG

t ; gα)+

E˜σα

t+1:T ˜σ

(cid:8)
−α
t+1:T

(

= E˜γα

−α
t (·|xt)˜γ
t

n=t+1
X
R(X α

δn−tR(X α

n , Aα

n, µG

n ; gα)

1:t−1, µG
µG

t+1, xα

1:t, X α

t+1

1:t, xα
µG
1:t

t , Aα

t , µG

t ; gα) + δV α(φ(µG

t , ˜γα

t ; gα), X α

t+1)

(cid:12)
(cid:12)

)

(cid:12)
(cid:12)
t , xα
µG
t

< Ebσα

t (·|µG

t ,xα

−α
(cid:8)
t )˜γ
t

R(X α

t , Aα

t , µG

t ; gα) + δV α(φ(µG

t , ˜γα

(cid:12)
t ; gα), X α
t+1)
(cid:12)

(cid:9)
t , xα
µG
t

−α
= Ebσα
t ˜σ
t

R(X α

t , µG

t ; gα)+

E˜σα

t+1:T ˜σ

(cid:8)
−α
t+1:T

(

= Ebσα

t ,˜σα

t+1:T ˜σ

−α
t:T

(

(cid:8)
t , Aα
∞

n=t+1
X

∞

n=t
X

δn−tR(X α

n , Aα

n, µG

(cid:12)
(cid:12)
n ; gα)

1:t, xα
µG
1:t

,

)

(cid:12)
(cid:12)

δn−tR(X α

n , Aα

n, µG

n ; gα)

1:t, µG
µG

t+1, xα

1:t, X α

t+1

(cid:9)

1:t, xα
µG
1:t

)

(cid:12)
(cid:12)

)

(cid:12)
(cid:12)

where (79) follows from the deﬁnitions of ˜γt and Appendix D, (80) follows from (76) and the deﬁnition of

(81) follows from Appendix D. However, this leads to a contradiction since ˜σ is a GMFE of the game.

)

(76)

(77)

(78)

(79)

(80)

(81)

(82)

σt,

b

REFERENCES

[1] D. Vasal, R. K. Mishra, and S. Vishwanath, “Sequential decomposition of graphon mean ﬁeld games,” Proceedings of the American

Control Conference, vol. 2021-May, pp. 730–736, jan 2020. [Online]. Available: https://arxiv.org/abs/2001.05633v1

[2] H. Witsenhausen, “A counterexample in stochastic optimum control,” SIAM Journal on Control, vol. 6, no. 1, pp. 131–147, 1968.

[3] A. Nayyar, A. Mahajan, and D. Teneketzis, “Decentralized stochastic control with partial history sharing: A common information approach,”

Automatic Control, IEEE Transactions on, vol. 58, no. 7, pp. 1644–1658, 2013.

24

[4] J. Arabneydi and A. Mahajan, “Team Optimal Control of Coupled Subsystems with Mean-Field Sharing,” dec 2020. [Online]. Available:

https://arxiv.org/abs/2012.01418v1

[5] E. Maskin and J. Tirole, “Markov perfect equilibrium: I. observable actions,” Journal of Economic Theory, vol. 100, no. 2, pp. 191–219,

2001.

[6] R. Ericson and A. Pakes, “Markov-perfect industry dynamics: A framework for empirical work,” The Review of Economic Studies, vol. 62,

no. 1, pp. 53–82, 1995.

[7] D. Bergemann and J. V¨alim¨aki, “Learning and strategic pricing,” Econometrica: Journal of the Econometric Society, pp. 1125–1149, 1996.

[8] D. Acem˘oglu and J. A. Robinson, “A theory of political transitions,” American Economic Review, pp. 938–963, 2001.

[9] D. Vasal, A. Sinha, and A. Anastasopoulos, “A systematic process for evaluating structured perfect bayesian equilibria in dynamic games

with asymmetric information,” IEEE Transactions on Automatic Control, 2018.

[10] D. Vasal and A. Anastasopoulos, “A systematic process for evaluating structured perfect Bayesian equilibria in dynamic games with

asymmetric information,” in American Control Conference, Boston, US, 2016, available on arXiv.

[11] H. T. Jahormi, “On design and analysis of cyber-physical systems with strategic agents,” Ph.D. dissertation, University of Michigan, Ann

Arbor, 2017.

[12] N. Heydaribeni and A. Anastasopoulos, “Structured Equilibria for Dynamic Games with Asymmetric Information and Dependent Types,”

sep 2020. [Online]. Available: https://arxiv.org/abs/2009.04253v1

[13] Y. Ouyang, H. Tavafoghi, and D. Teneketzis, “Dynamic games with asymmetric information: Common information based perfect bayesian

equilibria and sequential decomposition,” IEEE Transactions on Automatic Control, vol. 62, no. 1, pp. 222–237, 2017.

[14] M. Huang, R. P. Malham´e, and P. E. Caines, “Large population stochastic dynamic games: closed-loop mckean-vlasov systems and the

nash certainty equivalence principle,” Communications in Information & Systems, vol. 6, no. 3, pp. 221–252, 2006.

[15] J.-M. Lasry and P.-L. Lions, “Mean ﬁeld games,” Japanese Journal of Mathematics, vol. 2, no. 1, pp. 229–260, 2007.

[16] P. Cardaliaguet, F. Delarue, J.-M. Lasry, and P.-L. Lions, “The master equation and the convergence problem in mean ﬁeld games,” arXiv

preprint arXiv:1509.02505, 2015.

[17] D. Lacker, “A general characterization of the mean ﬁeld limit for stochastic differential games,” Probability Theory and Related Fields,

vol. 165, no. 3-4, pp. 581–648, 2016.

[18] M. Fischer et al., “On the connection between symmetric n-player games and mean ﬁeld games,” The Annals of Applied Probability,

vol. 27, no. 2, pp. 757–810, 2017.

[19] D. Lacker, “On the convergence of closed-loop nash equilibria to the mean ﬁeld game limit,” arXiv preprint arXiv:1808.02745, 2018.

[20] F. Delarue, D. Lacker, and K. Ramanan, “From the master equation to mean ﬁeld game limit theory: a central limit theorem,” Electron.

J. Probab., vol. 24, p. 54 pp., 2019. [Online]. Available: https://doi.org/10.1214/19-EJP298

[21] F. Parise and A. Ozdaglar, “Graphon games,” in Proceedings of the 2019 ACM Conference on Economics and Computation, 2019, pp.

457–458.

[22] L. Lov´asz, Large networks and graph limits. American Mathematical Soc., 2012, vol. 60.

[23] P. E. Caines and M. Huang, “Graphon mean ﬁeld games and the gmfg equations,” in 2018 IEEE Conference on Decision and Control

(CDC).

IEEE, 2018, pp. 4129–4134.

[24] P. Cardaliaguet, F. Delarue, J.-M. Lasry, and P.-L. Lions, “The master equation and the convergence problem in mean ﬁeld games,”

Annals of Mathematics Studies, vol. 2019-Janua, no. 201, pp. 1–222, sep 2015. [Online]. Available: https://arxiv.org/abs/1509.02505v1

[25] R. Mishra, D. Vasal, and S. Vishwanath, “Model-free Reinforcement Learning for Stochastic Stackelberg Security Games,” 2020.

[26] R. F. Tchuendom, P. E. Caines, and M. Huang, “On the Master Equation for Linear Quadratic Graphon Mean Field Games,” Proceedings

of the IEEE Conference on Decision and Control, vol. 2020-Decem, pp. 1026–1031, dec 2020.

[27] P. Kumar and P. Varaiya, “Stochastic systems,” 1986.

REFERENCES

[1] D. Vasal, R. K. Mishra, and S. Vishwanath, “Sequential decomposition of graphon mean ﬁeld games,” Proceedings of the American

Control Conference, vol. 2021-May, pp. 730–736, jan 2020. [Online]. Available: https://arxiv.org/abs/2001.05633v1

[2] H. Witsenhausen, “A counterexample in stochastic optimum control,” SIAM Journal on Control, vol. 6, no. 1, pp. 131–147, 1968.

[3] A. Nayyar, A. Mahajan, and D. Teneketzis, “Decentralized stochastic control with partial history sharing: A common information approach,”

Automatic Control, IEEE Transactions on, vol. 58, no. 7, pp. 1644–1658, 2013.

25

[4] J. Arabneydi and A. Mahajan, “Team Optimal Control of Coupled Subsystems with Mean-Field Sharing,” dec 2020. [Online]. Available:

https://arxiv.org/abs/2012.01418v1

[5] E. Maskin and J. Tirole, “Markov perfect equilibrium: I. observable actions,” Journal of Economic Theory, vol. 100, no. 2, pp. 191–219,

2001.

[6] R. Ericson and A. Pakes, “Markov-perfect industry dynamics: A framework for empirical work,” The Review of Economic Studies, vol. 62,

no. 1, pp. 53–82, 1995.

[7] D. Bergemann and J. V¨alim¨aki, “Learning and strategic pricing,” Econometrica: Journal of the Econometric Society, pp. 1125–1149, 1996.

[8] D. Acem˘oglu and J. A. Robinson, “A theory of political transitions,” American Economic Review, pp. 938–963, 2001.

[9] D. Vasal, A. Sinha, and A. Anastasopoulos, “A systematic process for evaluating structured perfect bayesian equilibria in dynamic games

with asymmetric information,” IEEE Transactions on Automatic Control, 2018.

[10] D. Vasal and A. Anastasopoulos, “A systematic process for evaluating structured perfect Bayesian equilibria in dynamic games with

asymmetric information,” in American Control Conference, Boston, US, 2016, available on arXiv.

[11] H. T. Jahormi, “On design and analysis of cyber-physical systems with strategic agents,” Ph.D. dissertation, University of Michigan, Ann

Arbor, 2017.

[12] N. Heydaribeni and A. Anastasopoulos, “Structured Equilibria for Dynamic Games with Asymmetric Information and Dependent Types,”

sep 2020. [Online]. Available: https://arxiv.org/abs/2009.04253v1

[13] Y. Ouyang, H. Tavafoghi, and D. Teneketzis, “Dynamic games with asymmetric information: Common information based perfect bayesian

equilibria and sequential decomposition,” IEEE Transactions on Automatic Control, vol. 62, no. 1, pp. 222–237, 2017.

[14] M. Huang, R. P. Malham´e, and P. E. Caines, “Large population stochastic dynamic games: closed-loop mckean-vlasov systems and the

nash certainty equivalence principle,” Communications in Information & Systems, vol. 6, no. 3, pp. 221–252, 2006.

[15] J.-M. Lasry and P.-L. Lions, “Mean ﬁeld games,” Japanese Journal of Mathematics, vol. 2, no. 1, pp. 229–260, 2007.

[16] P. Cardaliaguet, F. Delarue, J.-M. Lasry, and P.-L. Lions, “The master equation and the convergence problem in mean ﬁeld games,” arXiv

preprint arXiv:1509.02505, 2015.

[17] D. Lacker, “A general characterization of the mean ﬁeld limit for stochastic differential games,” Probability Theory and Related Fields,

vol. 165, no. 3-4, pp. 581–648, 2016.

[18] M. Fischer et al., “On the connection between symmetric n-player games and mean ﬁeld games,” The Annals of Applied Probability,

vol. 27, no. 2, pp. 757–810, 2017.

[19] D. Lacker, “On the convergence of closed-loop nash equilibria to the mean ﬁeld game limit,” arXiv preprint arXiv:1808.02745, 2018.

[20] F. Delarue, D. Lacker, and K. Ramanan, “From the master equation to mean ﬁeld game limit theory: a central limit theorem,” Electron.

J. Probab., vol. 24, p. 54 pp., 2019. [Online]. Available: https://doi.org/10.1214/19-EJP298

[21] F. Parise and A. Ozdaglar, “Graphon games,” in Proceedings of the 2019 ACM Conference on Economics and Computation, 2019, pp.

457–458.

[22] L. Lov´asz, Large networks and graph limits. American Mathematical Soc., 2012, vol. 60.

[23] P. E. Caines and M. Huang, “Graphon mean ﬁeld games and the gmfg equations,” in 2018 IEEE Conference on Decision and Control

(CDC).

IEEE, 2018, pp. 4129–4134.

[24] P. Cardaliaguet, F. Delarue, J.-M. Lasry, and P.-L. Lions, “The master equation and the convergence problem in mean ﬁeld games,”

Annals of Mathematics Studies, vol. 2019-Janua, no. 201, pp. 1–222, sep 2015. [Online]. Available: https://arxiv.org/abs/1509.02505v1

[25] R. Mishra, D. Vasal, and S. Vishwanath, “Model-free Reinforcement Learning for Stochastic Stackelberg Security Games,” 2020.

[26] R. F. Tchuendom, P. E. Caines, and M. Huang, “On the Master Equation for Linear Quadratic Graphon Mean Field Games,” Proceedings

of the IEEE Conference on Decision and Control, vol. 2020-Decem, pp. 1026–1031, dec 2020.

[27] P. Kumar and P. Varaiya, “Stochastic systems,” 1986.

