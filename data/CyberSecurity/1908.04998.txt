9
1
0
2

g
u
A
4
1

]

R
C
.
s
c
[

1
v
8
9
9
4
0
.
8
0
9
1
:
v
i
X
r
a

Interpretable Encrypted Searchable Neural
Networks

Kai Chen1, Zhongrui Lin2, Jian Wan2, and Chungen Xu1(

)

(cid:12)

1 School of Science, Nanjing University of Science and Technology, Nanjing, CHN
School of Computer Science and Engineering, NJUST, Nanjing, CHN

2

{kaichen,zhongruilin,wanjian,xuchung}@njust.edu.cn

Abstract. In cloud security, traditional searchable encryption (SE) re-
quires high computation and communication overhead for dynamic search
and update. The clever combination of machine learning (ML) and SE
may be a new way to solve this problem. This paper proposes inter-
pretable encrypted searchable neural networks (IESNN) to explore prob-
abilistic query, balanced index tree construction and automatic weight
update in an encrypted cloud environment. In IESNN, probabilistic learn-
ing is used to obtain search ranking for searchable index, and probabilis-
tic query is performed based on ciphertext index, which reduces the com-
putational complexity of query signiﬁcantly. Compared to traditional SE,
it is proposed that adversarial learning and automatic weight update in
response to user’s timely query of the latest data set without expensive
communication overhead. The proposed IESNN performs better than
the previous works, bringing the query complexity closer to O(log N )
and introducing low overhead on computation and communication.

Keywords: Searchable Encryption· Searchable Neural Networks· Prob-
abilistic Learning · Adversarial Learning· Automatic Weight Update

1

Introduction

The frequent and massive disclosure of private data has drawn the growing atten-
tion of the public to the cyberspace security. Meanwhile, cloud storage services
are increasingly attracting individuals and enterprises to outsource data into
cloud server with the rapid development of cloud computing. Unfortunately, out-
sourcing data into cloud server may reveal the privacy of data [10,14]. In cloud
security, searchable encryption (SE) has received widespread attention as it pro-
tects the privacy of outsourced data and prevents sensitive information from
leaking [5]. However, traditional SE [1,3,6,8,9,10,12,13,14] requires high com-
putation and communication overhead to enable dynamic search and dynamic
update, which makes SE still unable to satisfy user’s experience and requirements
of the actual application adequately. Actually, machine learning (ML) can pro-
vide intelligent and eﬃcient means yet the current popular ML only supports
plaintext data training and can not satisfy the special requirements of encrypted
cloud data. Therefore, it is necessary to discuss the cross-fusion problem of ML
and SE, and introduce intelligence and high-eﬃciency into SE.

 
 
 
 
 
 
2

Kai Chen et al.

SE has been continuously developed since it was proposed [8], and multi-
keyword ranked search scheme is recognized as excellent [5]. Cao et al. [1] ﬁrst
discussed privacy-preserving multi-keyword ranked search over encrypted cloud
data (MRSE) for single data owner model, and established strict privacy re-
quirements. They ﬁrst used asymmetric scalar-product preserving encryption
(ASPE) [12] to obtain the similarity score of the query vector and the index
vector. In this way, cloud server can retrieve top-k documents that are most
relevant to the data user’s query request. However, since matrix operations re-
quire high computation overhead, MRSE is not suitable for practical applica-
tion scenario. For the purpose of managing the keyword dictionary dynamically
and improving system performance, Li et al. [6] proposed eﬃcient multi-keyword
ranked query over encrypted data in cloud computing (MKQE) based on MRSE,
which owns a low overhead index construction algorithm and a novel trapdoor
generation algorithm. However, it still has no major breakthrough in improving
search eﬃciency when the data set is large. To achieve dynamic search, Xia et
al. [13] provided a secure and dynamic multi-keyword ranked search scheme over
encrypted cloud data (EDMRS) to support dynamic operation in SE. For tree-
based index structures, search eﬃciency is improved by the greedy depth-ﬁrst
search (GDFS) algorithm and parallel computing. Regrettably, the search eﬃ-
ciency of ordinary balanced binary tree they used gradually decreases and tends
to linear search eﬃciency when migrating to multiple data owners model with
large amount of diﬀerential data. Moreover, maintaining such an index tree is
not ﬂexible and eﬃcient. Guo et al. [3] discussed secure multi-keyword ranked
search over encrypted cloud data for multiple data owners model (MKRS MO)
and designed a heuristic weight generation algorithm based on the relationships
among keywords, documents and owners (KDO). They considered the correla-
tion among documents and the impact of documents’ quality on search results.
Experiments on the real-world data set showed that MKRS MO is better than
the schemes using traditional T F
IDF keyword weight model [9]. However,
the ﬂy in the ointment is that the operations of calculating index similarity in
MKRS MO may lead to “curse of dimensionality”, which limits the availability
of the system. Last but not least, they ignored the secure solution in known
background model [1] (threat model for measuring the ability of cloud server to
evaluate private data and the risk of revealing private information in SE system).

×

For the ﬁrst time, this paper proposes interpretable encrypted searchable neu-
ral networks (IESNN) to explore intelligent SE. Based on the neural network, we
propose sorting network and employ probabilistic learning to obtain the query
ranking for encrypted searchable index. To be speciﬁc, ﬁrstly it performs a suﬃ-
cient amount of random queries (obey uniform distribution) and then calculates
the sum of the inner product of each index vector and all random query vec-
tors. Finally it sorts the index vectors according to the match scores from high
to low. Therefore, the probabilistic ranking of the index is close to the ranking
in the actual query, which reduces the computational complexity of the query
signiﬁcantly. Moreover, probabilistic query with computational complexity close
to O(log N ), is used to retrieve top-k documents. In order to achieve secure

Interpretable Encrypted Searchable Neural Networks

3

weight update without revealing private information to “semi-trusted” cloud
server [10,14], we propose searching adversarial network and weight update net-
work in an encrypted cloud environment. Speciﬁcally, in order to respond to
user’s timely query of the latest data set, we employ adversarial learning [2] and
optimal game equilibrium to make the probabilistic ranking of the index close to
its popular ranking. Furthermore, we combine backpropagation neural network [4]
with discrete Hopﬁeld neural network [7] to enable automatic weight update. It is
worth mentioning that the update operations are done in the cloud, which means
there is no expensive communication overhead. So we can use IESNN for model
training and intelligent system implementation. On the one hand, it introduces
intelligence into the SE system, which improves user’s experience and reduces
system overhead. On the other hand, training data sources for ML can be de-
rived from ciphertext. It means that data mining based on ciphertext analysis
can not only obtain results consistent with plaintext analysis but also strengthen
the intensity of data privacy protection.

Table 1. Comparison of related works.

Item
high-precision query
privacy-preserving query
automatic weight update
high-quality ranked search
eﬃcient multi-keyword search
ﬂexible dynamic maintenance

MRSE [1] MKRS MO [3] MKQE [6] EDMRS [13] IESNN

√
√

×
×
√

×

√

×
×
√
√

×

√
√

×
√
√

×

√
√

×
×
√

×

√
√
√
√
√
√

Our main contributions are summarized as follows:

(1) Towards intelligent SE by combining popular ML with traditional SE eﬀec-

tively;

(2) We employ probabilistic learning method to achieve maximum likelihood

searching and improve search eﬃciency signiﬁcantly;

(3) We use IESNN to implement ﬂexible dynamic operation and maintenance

in an encrypted cloud environment.

The remainder of this paper is organized as follows: Section 2 describes the
SE model. Section 3 describes the details of IESNN and its performance tests.
Section 4 discusses our solution and its implications.

2 Searchable Encryption Model

2.1 System Model

The system model proposed in this paper consists of three parties, is depicted
in Fig. 1, and the speciﬁc description is as follows:

Data owners(DO): DO are responsible for building searchable index and orig-

inal IESNN, encrypting the data and sending them to cloud server.

4

Kai Chen et al.

Data users(DU ): DU are consumers of cloud services. Once the license is

granted, they can retrieve the encrypted cloud data.

Cloud server(CS): CS is considered“semi-trusted”in SE [10,14]. It provides

cloud service, including running authorized access controls, performing searches
for encrypted cloud data based on query requests, returning top-k documents
to DU and enabling dynamic operation and maintenance with IESNN.

Fig. 1. The basic architecture of searchable encryption system

2.2 System Framework

Mi,1, Mi,2}
Vi

×

Setup: Based on privacy requirements in known background model

[1], DOi
determines the size Ni of dictionary Di, the number Ui of pseudo-keyword,
sets the parameter Vi = Ui +Ni. For all data owners DO =
,
, U =
we have V =

DO1,. . . ,DOm

{
N1,. . . ,Nm

U1,. . . ,Um

V1,. . . ,Vm

}

KeyGen(V ): DO generate secret key SK =

Si,
, Mi,1 and Mi,2 are two invertible matrices with the dimension

{

{

}

, N =
}
SK1,. . . ,SKs

{

.
}
, where SKi =

{

{

}

Vi and Si is a random Vi-length vector.

Extended-KeyGen(SKi, Zi): For dynamic search [6], if Zi new keywords are
S′i, M ′i,1,

added into the i-th dictionary Di, DOi generates a new SK ′i =
M ′i,2}
(Vi + Zi), and a new (Vi + Zi)-length vector S′i.

, two invertible matrices M ′i,1 and M ′i,2 with the dimension (Vi + Zi)

×

{

BuildIndex (I, SK): In order to reduce the possibility that “semi-trusted” cloud
server [10,14] evaluates the private data successfully, DO ﬁrst build search-
able indexes for documents and obtain the weighted index vectors, and then
ﬁll index vectors with random pseudo-keywords (obey Gaussian distribution)
and obtain secure index vectors with high privacy protection strength [1].
Finally they use secure index vectors to build IESNN (
to
CS (speciﬁc example: DOi “splits” index vector Ii into two random vectors
. Speciﬁcally, if Si[t] = 0, Ii,1[t] = Ii,2[t] = Ii[t] ; else if Si[t] = 1,
Ii,1, Ii,2}
{
Ii,1[t] is a random value, Ii,2[t] = Ii[t]
Ii =
M T

Ii,1[t]. DOi encrypts Ii as

) and send

with SKi).

i,1Ii,1, M T

−

I

I

{

i,2Ii,2}

e

Interpretable Encrypted Searchable Neural Networks

5

{

}

Q1,. . . ,Qm

Trapdoor (Q, SK): DU send query request (query keywords and k) to DO.
(where Qi is a weighted vector with
DO generate query Q =
dimension Vi) and calculate the trapdoor T =
using SK and
send T to DU (speciﬁc example: DOi “splits” query vector Qi into two
. Speciﬁcally, if Si[t] = 0, Qi,1[t] is a random
random vectors
Qi,1[t]; else if Si[t] = 1, Qi,1[t] = Qi,2[t] = Qi[t].
value, and Qi,2[t] = Qi[t]
i,2 Qi,2}
Finally, DOi encrypts Qi as Ti =

Qi,1, Qi,2}
−

, T, k): DU send trapdoors, query instruction and attribute identiﬁ-
cation to CS. CS performs searches based on the query, and returns top-k
documents to DU .

i,1 Qi,1, M −

with SKi).

T1,. . . ,Tm

Query (

M −

I

{

{

}

{

1

1

3

Interpretable Encrypted Searchable Neural Networks

3.1 Maximum Likelihood Searching

We employ inner product similarity [11] to quantitatively evaluate the eﬀective
similarity between the query vector and the index vector. As illustrated in Fig. 2
(for an intuitive understanding, it shows the unencrypted network), in sorting
network, it performs a suﬃcient amount of random queries (obey uniform dis-
σ√3, σ√3]), and
tribution: X
Qj of each index vector
then calculates the sum of the inner product
and all random query vectors with formula 1. Finally it sorts the index vectors
according to the match scores from high to low. Therefore, the index ranking
obtained by probabilistic learning is close to the ranking in the actual query.

σ√3, σ√3), that is f (x) = 1
2σ√3
m
j=1 I T
i

U (

P

, x

−

−

∼

∈

·

[

Score =

Ii

Ti =

·

{

e

M T

i,1Ii,1, M T

i,2Ii,2} · {

1

M −

i,1 Qi,1, M −

1

i,2 Qi,2}

= I T
i

Qi

·

(1)

Fig. 2. Sorting network

6

Kai Chen et al.

We implement the proposed scheme using Python in Windows 10 operation sys-
tem with Intel Core i5 Processor 2.40GHz and test its eﬃciency on a real-world
document set (IEEE INFOCOM publications, including 400 papers and 2,000
keywords). The probabilistic query algorithm based on the probabilistic ranking
of encrypted searchable index brings the query complexity closer to O(log N ). As
shown in Fig. 3, when retrieving the same number of top-k documents, probabilis-
tic query performs better than the related works that based on tree search [3,13]
and matrix operation [1,6]. As the ordered feature of the balanced binary tree is
not guaranteed in the index tree and the query based on matrix operation needs
to traverse all indexes to retrieve top-k documents, the number of retrieved in-
dexes is far more than the number of retrieved documents.

{

Fig. 3. Performance testing of multiple query algorithms.
The query precision and search
eﬃciency for diﬀerent numbers of retrieved documents with the same document collection (400) and
dictionary (2,000). It requires an average of 100 experimental results to measure performance of the
following subjects: random unordered tree based on plaintext index(RU-Tree-PI) [3,13], probabilistic
ordered tree based on plaintext index(PO-Tree-PI), probabilistic ordered tree based on ciphertext
index(PO-Tree-CI), probabilistic ranking based on ciphertext index(PR-CI), linear traversal based
on ciphertext index(LT-CI) [1,6]. The number of retrieved top-k documents is the factor of 400:
k = 1, 2, 4, 5, 8, 10, 16, 20, 25, 40, 50, 80, 100, 200. Note: the nodes of the tree are also included in the
number of retrieval indexes. According to the experimental results, probabilistic query can signiﬁ-
cantly improve the search eﬃciency. When k takes a speciﬁc interval value, the query precision is
high or low. It is because that the probabilistic ranking of the index vector is not strictly ordered,
and the query is random. Therefore, when the query vector is very “unpopular”, the query precision
will become lower, and when the query vector is “popular”, the query precision and search eﬃciency
will perform well.

}

3.2 Adversarial Learning

Adversarial network works when the probabilistic ranking of the index deviates
from the index ranking in the actual query result. As shown in Fig. 4, it employs
optimal game equilibrium to make the probabilistic ranking of the index close to
its popular ranking (described by formula 2, pi(x) and pq(y) are the probability
distributions of the index ranking and query result, respectively).

min
S

max
A

V (A, S) = Ex

∼

pi(x)[log A(x)] + Ey

pq(y)[log(1

∼

−

A(S(y)))]

(2)

Interpretable Encrypted Searchable Neural Networks

7

Inspired by generative adversarial networks (GAN) [2] and self-attention gener-
ative adversarial networks (SAGAN) [15] but diﬀerent from GAN and SAGAN,
searching adversarial networks (SAN) do not require complex gradient calcu-
lations and extensive iterative training. As a matter of fact, SAN only require
simple residual calculations and index sorting ﬂoating steps. Speciﬁcally, after
completing the query, the ranked search result list is feedback to adversarial net-
work in SAN. Adversarial network calculates the residual before and after the
weight change of the index corresponding to top-k documents, and calculates
the relative ﬂoating of the index ranking of the feedback result(i.e. new index
ranking) and the original index ranking. Finally, SAN send the results of the
calculation (the residual of the weights) and the index ranking changes to the
weight update network as a target for index update (see Fig. 5 for details).

Fig. 4. Searching adversarial networks

3.3 Automatic Weight Update

As illustrated in Fig. 5, in order to achieve automatic weight update and re-
spond to users’ queries for the latest data sets in a timely manner, weight update
network (WUN) combines backpropagation neural network (BPNN) [4] with dis-
crete Hopﬁeld neural network (DHNN) [7]. In WUN, the update of index weights
uses vector and matrix operations to approximate the actual increments, which
has the characteristics of local homomorphism for ciphertext operations and
plaintext operations. For instance, considering index vector Iα = [α1, . . . , αn]T ,
query vector Qγ = [γ1, . . . , γn]T , and two invertible matrices M = (aij )n
n and
×
n. The update principle of ciphertext index is as follows:
M −

1 = (bij )n
×
Matrix and vector multiplication: I T

α M = [

αiai1, . . . ,

αiain], M −

1Qγ =

n

Pj=1

n

[

b1jγj, . . . ,

Pj=1
Qγ =

n

Pi=1

bnjγj];Secure inner product calculation: I T

αiγi; Index vector update: (I T

α +∆I T

α )M = [

n

Pi=1

n

Pi=1
α M

M −

·

1Qγ = I T

α ·

n

n

Pi=1

(αi+∆αi)ai1, . . . ,

(αi+

Pi=1

8

Kai Chen et al.

n

n

αiai1 + ∆β1, . . . ,

αiain + ∆βn] = I T

α M + ∆I T

∆αi)ain]

[

≈

α + ∆I T
uct approximation: (I T
n

Qγ + ∆I T

Qγ =

αiγi +

M −

Pi=1
α )M
·
n
∆βiγi.

1Qγ

≈

Pi=1

β ·

I T
α ·

Pi=1

Pi=1

(I T

α M + ∆I T

β M )

β M ;Inner prod-
1Qγ =

M −

·

Fig. 5. Weight update network

Asynchronous work mode of WUN : The update task from SAN to WUN
is only updating the weight of an index, while other indexes still retain their
original weight. i.e.

Ij(t + 1) =

sgn[netj(t)], j = i
= i
Ij (t),

j

(cid:26)

, Ij(t + 1) =

satlins[netj(t)], j = i
= i
Ij (t),

j

(cid:26)

(3)

Synchronous work mode of WUN : The synchronous work mode is parallel,
i.e. the weights of all indexes are all changed in one update. The adjustment
of the weight is determined according to the current input value. The weight
update is complete and the weight of an index continues to be used for the next
update. When the weight of each index is stabilized, the work ends.

Ij(t + 1) = sgn[netj(t)],
j = 1, 2, . . . , n
Ij(t + 1) = satlins[netj(t)], j = 1, 2, . . . , n

(cid:26)

(4)

When updating an index, the schemes [3,13] employ tree-based index need to
update the index vector itself (leaf node of index tree) and its corresponding
other data (parent node of leaf node). Moreover, in order to achieve dynamic
search, the current schemes [1,3,6,9,13] need to download the ciphertext index
from the cloud, update its plaintext after local decryption, and ﬁnally upload
the new ciphertext index to the cloud. In comparison, our solution only needs
to update the index vector in the cloud with touching a smaller amount of data
and introduce low overhead on computation and communication.

6
6
Interpretable Encrypted Searchable Neural Networks

9

3.4 Overall Operation and Maintenance of IESNN

As shown in Fig. 6, IESNN consist of sorting net, adversarial net, searching net
and weight update net. Except that the initial index weight needs to be generated
by data owners, the rest of automatic update operations (“add, delete, change
and investigate” operations of index) are all completed in an encrypted cloud
environment. The system forms a “query-learning-update-learning-query” self-
attention [15] loop and an “automatic operation and maintenance” mechanism.
Dynamic operation and maintenance of SE system are almost entirely done in
cloud server. On the one hand, implementing dynamic operation and mainte-
nance in an encrypted cloud environment not only improves the usability and
ﬂexibility of SE system, but also enhances the strength of privacy protection.
On the other hand, when it is necessary to update the index in cloud server,
compared with traditional SE [1,3,6,9,13], our solution eliminates the need to
rebuild the index locally and upload a new index to cover the old index stored in
the cloud, which introduces low overhead on computation, communication and
local storage.

Fig. 6. The overall composition of IESNN

4 Discussion

In this paper, we discuss the cross-fusion problem of ML and SE, and pro-
pose IESNN. We creatively combine popular ML with traditional SE, which is
committed to exploring intelligent SE. We employ probabilistic learning method
to generate sorting network that is trained by a suﬃcient amount of random
queries, which makes a contribution to achieve maximum likelihood searching
and bring the query complexity closer to O(log N ). It means that exploiting ML
to optimize the query is eﬀective in an uncertain system, even better than special
construction methods. Obviously, traditional query algorithms based on matrix
operations and tree searching are not optimistic in big data environments be-
cause high dimensional data processing can lead to “curse of dimensionality” and
even system crashes. Implementing ﬂexible dynamic operation and maintenance
in an encrypted cloud environment with IESNN that reduces communication
overhead, protects data privacy and leverages cloud computing well.

10

Kai Chen et al.

Acknowledgment

This work was supported by “the Fundamental Research Funds for the Cen-
tral Universities” (No. 30918012204) and “the National Undergraduate Training
Program for Innovation and Entrepreneurship” (Item number: 201810288061).
NJUST graduate Scientiﬁc Research Training of ‘Hundred, Thousand and Ten
Thousand’ Project “Research on Intelligent Searchable Encryption Technology”.

References

1. Cao, N., Wang, C., Li, M., Ren, K., Lou, W.: Privacy-preserving multi-keyword
ranked search over encrypted cloud data. IEEE Trans. Parallel Distrib. Syst. 25(1),
222–233 (2014)

2. Goodfellow,

I.J., Pouget-Abadie, J., Mirza, M., Xu, B., Warde-Farley, D.,
Ozair, S., Courville, A.C., Bengio, Y.: Generative adversarial networks. CoRR
abs/1406.2661 (2014)

3. Guo, Z., Zhang, H., Sun, C., Wen, Q., Li, W.: Secure multi-keyword ranked search
over encrypted cloud data for multiple data owners. Journal of Systems and Soft-
ware 137(3), 380–395 (2018)

4. Hinton, G.E., Osindero, S., Welling, M., Teh, Y.W.: Unsupervised discovery of
nonlinear structure using contrastive backpropagation. Cognitive Science 30(4),
725–731 (2006)

5. Kumar, D.V.N.S., Thilagam, P.S.: Approaches and challenges of privacy preserving

search over encrypted data. Inf. Syst. 81, 63–81 (2019)

6. Li, R., Xu, Z., Kang, W., Yow, K., Xu, C.: Eﬃcient multi-keyword ranked query
over encrypted data in cloud computing. Future Generation Comp. Syst. 30(1),
179–190 (2014)

7. Park, J.H., Kim, Y.S., Eom, I.K., Lee, K.Y.: Economic load dispatch for piecewise
quadratic cost function using hopﬁeld neural network. IEEE Trans. Power Syst.
8(3), 1030–1038 (1993)

8. Song, D.X., Wagner, D.A., Perrig, A.: Practical techniques for searches on en-
crypted data. In: IEEE S & P 2000. pp. 44–55. IEEE Computer Society (2000)
9. Sun, W., Wang, B., Cao, N., Li, M., Lou, W., Hou, Y.T., Li, H.: Veriﬁable privacy-
preserving multi-keyword text search in the cloud supporting similarity-based rank-
ing. IEEE Trans. Parallel Distrib. Syst. 25(11), 3025–3035 (2014)

10. Wang, C., Wang, Q., Ren, K., Lou, W.: Privacy-preserving public auditing for data
storage security in cloud computing. In: INFOCOM 2010. pp. 525–533 (2010)
11. Witten, I.H., Moﬀat, A., Bell, T.C.: Managing Gigabytes: Compressing and Index-

ing Documents and Images, Second Edition. Morgan Kaufmann (1999)

12. Wong, W.K., Cheung, D.W., Kao, B., Mamoulis, N.: Secure knn computation on

encrypted databases. In: ACM SIGMOD 2009. pp. 139–152. ACM (2009)

13. Xia, Z., Wang, X., Sun, X., Wang, Q.: A secure and dynamic multi-keyword ranked
search scheme over encrypted cloud data. IEEE Trans. Parallel Distrib. Syst. 27(2),
340–352 (2016)

14. Yu, S., Wang, C., Ren, K., Lou, W.: Achieving secure, scalable, and ﬁne-grained
data access control in cloud computing. In: INFOCOM 2010. pp. 534–542. IEEE
(2010)

15. Zhang, H., Goodfellow, I.J., Metaxas, D.N., Odena, A.: Self-attention generative

adversarial networks. In: ICML 2019. pp. 7354–7363. PMLR (2019)

