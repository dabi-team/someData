Less is More: Exploiting Social Trust to Increase
the Effectiveness of a Deception Attack

Shahryar Baki
Computer Science
University of Houston
Houston, Texas
sh.baki@gmail.com

Rakesh M. Verma
Computer Science
University of Houston
Houston, Texas
rmverma2@central.uh.edu

Arjun Mukherjee
Computer Science
University of Houston
Houston, Texas
amukher6@central.uh.edu

Omprakash Gnawali
Computer Science
University of Houston
Houston, Texas
odgnawal@central.uh.edu

0
2
0
2

n
u
J

4
2

]

R
C
.
s
c
[

1
v
9
9
4
3
1
.
6
0
0
2
:
v
i
X
r
a

Abstract—Cyber attacks such as phishing, IRS scams, etc.,
still are successful in fooling Internet users. Users are the last
line of defense against these attacks since attackers seem to
always ﬁnd a way to bypass security systems. Understanding
users’ reason about the scams and frauds can help security
providers to improve users security hygiene practices. In this
work, we study the users’ reasoning and the effectiveness of
several variables within the context of the company representative
fraud. Some of the variables that we study are: 1) the effect of
using LinkedIn as a medium for delivering the phishing message
instead of using email, 2) the effectiveness of natural language
generation techniques in generating phishing emails, and 3) how
some simple customizations, e.g., adding sender’s contact info to
the email, affect participants perception. The results obtained
from the within-subject study show that participants are not
prepared even for a well-known attack - company representative
fraud. Findings include: approximately 65% mean detection rate
and insights into how the success rate changes with the facade and
correspondent (sender/receiver) information. A signiﬁcant ﬁnding
is that a smaller set of well-chosen strategies is better than a large
‘mess’ of strategies. We also ﬁnd signiﬁcant differences in how
males and females approach the same company representative
fraud. Insights from our work could help defenders in developing
better strategies to evaluate their defenses and in devising better
training strategies.

I. INTRODUCTION

So far, security has been an arms race in which defenders
are typically behind the attackers. Researchers have tried to
counter this issue by publishing new attacks or vulnerabilities
in software and hardware, yet people continue to succumb to
“old” attacks. How can defenders leap ahead of the attackers is
the key question. For this purpose, we need to understand: (i)
how people reason about scams and attacks, (ii) what makes
an attack successful, and (iii) how can attackers increase their
chances of success. Armed with this knowledge, defenders can
trace the evolution of attacks and use it to bolster their defenses
further and also devise more effective training regimes.

Given the scale of threat, every IT organization uses two
prong strategy in defense: use of software tools to identify and
ﬁlter phishing emails even before they reach the mailboxes and
educate the users to identify phishing emails. Still, phishing
continues to be effective as it increases in sophistication to
evade tools and training. Understanding what makes phishing
emails effective is critical
to building anti-phishing tools.
Understanding how different aspects of a phishing email

contributes to deception and action by the victim is challeng-
ing. The content, form, and context surrounding the phishing
emails constitute a large number of variables. Over the past
decade, emails have grown in complexity and routinely include
tens of images, tables, formatted paragraphs in different fonts,
sizes, and colors. They now resemble a modern webpage
with the exception of executable scripts, which are disabled
by most email clients. There could be tens to hundreds of
variables controlling the visual elements of the emails that
appear to be sent by sites such as LinkedIn or Facebook
or banks. There are simply too many individual variables
to study in a traditional
laboratory or limited live attack
setting. Many studies arbitrarily select a few variables (font
size, logo tampering, link placement, color, etc.) and study
how participants use these selected visual aspects of email to
identify phishing emails.

Phishing emails oftentimes use visual scaffolding mimick-
ing emails sent from well-known online services to increase
the credibility of the email. An email that convinces a victim
to click on a link to reset the password not only contains the
password reset link but is fully wrapped in the style (logo,
color, text, and other formatting details) used by legitimate
emails sent from the target website. The user thus is led to
believe that the email is legitimate and clicks on the link, go
to the website to provide old password, for example, and fall
victim to phishing.

Selecting a few variables for a deeper study to understand
how and if they are used by users to identify phishing emails
makes practical sense. This practical approach allowed the
researchers to make progress in understanding contributing
factors to successful phishing and building better anti-phishing
tools and training. Fortunately, emails of the past generation
were mostly text with sparse use of images, links, and for-
matting. In that context, the practical approach that studied a
few variables was the right approach. Unfortunately, the emails
have evolved signiﬁcantly in the last 5-10 years to go from
a few to tens to hundreds of objects, but the phishing studies
have not been able to keep pace. Moreover, previous work has
focused too much on phishing attacks and not enough on other
kinds of email scams and attacks.

To gain understanding of the parameters contribution to
deception and action by the victim, we conduct a within-

 
 
 
 
 
 
subjects study in which we take a simple, well-known attack,
viz., company representative fraud, and parameterize it with
signals such as fake logos of varying subtlety, surrounding
context (facade), etc. We also consider automatic generation
of these attacks using natural language generation techniques
such as those employed in the Dada tool. Our goal here
is to study whether such techniques are “mature enough”
so that they can be deployed in credible attack generation.
Being able to synthesize new attacks allows defenders to test
their techniques on such new attacks rather than testing their
methods on classical attacks.

We design the experiment so that the participants have am-
ple opportunity to exhibit their behavior and their reasoning.
Our experiment is divided into two phases. In the ﬁrst, we ask
participants whether they believe the company representative
email is genuine or fake and explain all the reasons for their
judgment. In the second, we embed signals in the email and tell
the participants that the email is fake, but ask them to explain
all the “ﬂags” indicating the email is fake, so that we can
see how good are the participants at detecting them and also
check whether participants are revealing all their reasoning
in the ﬁrst section. Participants took a personality test before
the experiment and a debrieﬁng discussion after. Our ﬁndings
include:

1) We study how complex visual facades can be easily
used to generate realistic and personalized attacks. Our
work suggests that social scaffolding such as a LinkedIn
context increases the effectiveness of the attack more than
email clients such as Gmail (Section IV-A).

2) Our study shows that adding sender’s contact info and
receiver’s name to the email can signiﬁcantly increase
the success rate of the attack (Section IV-D).

3) The more the strategies used by participants the lower
the detection rate. We analyzed the data in different ways
and this ﬁnding persists. Moreover, there are signiﬁcant
differences in the strategies used by males versus those
used by females (Section IV). However, we ﬁnd there is
no statistically signiﬁcant difference in the performance
of the two groups, which suggests that there are multiple
sets of strategies leading to similar performance.

4) 91% of the participants did not notice tampering in a
widely known logo. Speciﬁcally, only two participants
detected our easiest fake logo and only one detected the
next easiest one. No one detected level 3 and level 4
(hardest to detect) fake logos (Section V). This insight
can be used to better train Internet users and employees.
5) We also study whether Natural Language Generation
(NLG) technology can be used to semi-automatically
generate effective attacks (Section IV-B). Our results
showed that participants do a better job in detecting fake
representative offers generated by NLG. We believe that
more complex grammar may be needed than we have
used, since offers tend to be long and complex.

use the signal), while others consider indirect metrics such as
the time taken [2], [3].1 Other researchers [1] have revealed
the limitations of these methods. In our study, we try to go
deeper into the reasoning process employed by the subjects.
This necessitates the use of a lab environment since a user is
unlikely to have the patience to share reasoning strategies for
every decision, even if only from a subset of decisions, made
in the real world. Hence, some studies have tried to use the
mouse movements as a proxy [1] while others have equipped
the subjects with intrusive sensors such as EEG devices [5],
which have their own limitations.

II. FACADE ATTACK EXPLOITING TRUST

The attack that we introduce in this work can be considered
as a deception attack with different ways of exploiting social
trust, which we call scaffolding attack. Although there has
been signiﬁcant work in the context of phishing and spear
phishing [4], to our knowledge, nobody has investigated such
a range of parameters, or how participants behave and reason
about this attack in different scenarios.

A. Representative Offer

The fraudulent company representative offers (representa-
tive offer for short) are mainly looking for representatives
in other countries who will collect money from customers
in those countries. We used an online dataset of company
representative fraud emails to collect them.2. Those offers have
some typos and grammatical mistakes. We do not ﬁx these
issue to discover whether participants pay attention to such
details. Below is a sample representative offer:

Attention,

I am Jeannie U. Ashe manager in Supreme Access Industrial
Limited. I owned a company that sells home storage solution,
home dcor and outdoor dcor products in Turkey. We are searching
for representatives who can help us establish a medium of getting
to our costumers in Europe as well as making payments through
you to us. Please contact us for more information. You will have
to send in your credentials, so we can know your competence.
Subject to your satisfaction you will be given the opportunity to
negotiate your mode of which we will pay for your services as
our representative in Turkey and Europe. If interested forward to
us your phone number/fax and your full contact addresses. We
can assure you that this proposal is 100% legal.
Please
forward
mail:Jeannie Ashe@SAI.com
1) your full names 2) phone number/fax 3) contact adresses, 4)
any form of identity/international passport. 5) age
in this formal application a message will be sent to you as regards
appointment.

interested

you

this

are

to

e-

if

Thanks in advance.

Jeannie U. Ashe
Manager
Supreme Access Industrial Limited

As pointed out in [1], some empirical studies focus on
binary decisions (i.e., whether the subject used or did not

1We also note the use of 5-point Likert scale in some studies, e.g., [4].
2http://www.419scam.org/419representative.htm

B. Natural Language Generators and the Dada Engine

One of the aims of computational linguistics and natural
language generation (NLG) is to facilitate the use of computers
by allowing the machine and their users to communicate using
natural language. There are two main areas of research in this
ﬁeld: understanding and generation. Usually, a generator works
by having a large dataset of knowledge to pull from that is
then manipulated by programmable grammatical rules in order
to generate readable text. This process is usually broken down
into two steps: text planning, which is concerned with deciding
the content of the text, and realization, which is concerned with
the lexigraphy, and syntactic organization of the text. Some of
the variations of these approaches appear in [6]. Moore and
Swartout [7] present a top-down approach to text planning,
which is responsive to some communicative goal a speaker
has in mind. Information is selected from the knowledge
base and organized into coherent text during the planning
process. Paris [8] presents a similar approach with additional
constraints put on the lexical organization of the text. However,
because of the top-down nature of these approaches, the plan-
ning is restricted from incorporating additional elements that
the user might ﬁnd useful. Hovy [9] presents an approach that
allows for top-down and bottom-up approach simultaneously.
Because of this increased ﬂexibility, constraints become even
more important. McCoy and Cheng [10] present constraints
that can be used to control this process. For our purposes,
we used the Dada engine [11], which is capable of both top-
down and bottom-up approaches for email generation which
is appropriate for our use.

The Dada engine has been successfully used to construct
the academic papers on postmodernism [12]. The Dada
engine works by pulling text from the knowledge base that is
speciﬁed as appropriate by the constraints. The Dada engine is
a natural language generator tool that is based on the principle
of recursive transition networks or recursive grammars. A
recursive transition network (RTN) can be thought of as a
schematic diagram of a grammar which shows the various
pathways that different yields of the grammar can take. For
example, in the construction of a sentence, one may choose
to follow the RTN shown in Figure 1.

Fig. 1: Example of Recursive Transition Network

If one follows the RTN in Figure 1 from the start to the
end, one passes through states (boxes) representing the various
elements which make up the sentence in sequence: ﬁrst a
preposition, then an optional adjective, then a noun and then
a verb. After the preposition, the network branches into two
paths, one of which leads to the adjective and the other which
bypasses it and goes straight on to the noun. One may take
either path in the course of constructing a sentence.

The building block of the NLG is a dataset of actual texts
out of which we can extract the grammar rules. We used the
dataset of company representative scams which is available
online3 to create rules for Dada engine and also some of them
were used directly in our representative offers.

C. Hypotheses

There are different clues in an email that can be a signal
for users to decide whether an email is legitimate or fraudu-
lent. Being aware of those signals can help people to detect
legitimate/fraudulent emails more easily. Hence, we pose the
following hypotheses:

H1.a. Paying attention to the higher number of clues in
the email can decrease participants vulnerability to
representative offers.

H1.b. People with higher knowledge about the email pay

attention to more clues.

Some social networks like LinkedIn are designed to be
professional networks. Hence, people may trust message re-
ceived from people in their professional network more than a
email from a random person. This can be used by attackers
to use professional network as a medium for delivering the
message to the victims in order to improve the success rate
of their attacks. Therefore, this study tests the effect of using
professional social networks instead of traditional email-based
phishing attacks by posing the following hypotheses:

H2.a. Using LinkedIn as a medium for delivering phishing
emails improves the effectiveness of the attack com-
pared to using email.

H2.b. People with lower knowledge about the social net-
works are more susceptible to LinkedIn based phish-
ing attack than people who are familiar with them.

Representative offers are mostly used in mass distributions
delivered to random recipient. For example, someone with a
major in electrical engineering can receive a representative
offer from a pharmaceutical company. This can be an obvi-
ous indicator for recipients that the email is fraudulent. In
spear phishing attacks, attackers try to customize the message
exactly for a speciﬁc person, but this necessitates a lot of
knowledge about the victim and also lots of time to gather
this information. In this study, we want to test how much
some simple customization, like adding victims’ name in the
email, can increase the susceptibility of them. So, in this study
we also pose the following hypotheses:

H3.a. Adding recipient’s name in the greeting of the email
decreases the detection rate of a fraudulent represen-
tative offer.

H3.b. Adding the sender’s signature to emails makes recip-
ients feel the offer comes from a legitimate source.
As a result, participants tag emails with signature as
legitimate more frequently.

Creating the representative offer emails is a time-consuming
task. Using NLG tools can help to speed it up. This can

3http://www.419scam.org/419representative.htm

be helpful for both attackers (generating a variety of emails
in a shorter amount of time) and defenders (automatically
generating datasets to improve existing detection systems).
Hence, we present the following hypothesis to evaluate the
effectiveness of the NLG tools:

range of the participants’ scores in each trait. It shows that
the participants’ scores approximately covers from lowest to
highest score for all the traits which means our samples from
the population are not skewed in one or more directions from
the personality point of view.

H4. The detection rate of fraudulent representative offers
generated by NLG is not signiﬁcantly different from
human-generated offers.

D. Our study

In this study, we investigate user behavior and reasoning
with company representation emails. We parameterize the
fraudulent emails with various ways of social trust exploita-
tion.

(a) Level 1

(b) Level 2

(c) Level 3

(d) Level 4

Fig. 2: LinkedIn fake logos with various levels of subtlety

1) Using the context of LinkedIn or a traditional email-based

attack (H2)

2) With varying levels of contact information (sender only,
recipient only, both sender and recipient, and none) (H3)
3) With varying levels of customization using recipient’s
background information (education, work, or both of
them)

4) Obviously fake company names (e.g., Donald Duck and

Mickey Mouse)

5) Fake logos: There are four different fake logos with

varing levels of subtlety (Figure 2).

Some of the attacks are generated semi-automatically using
NLG technology as employed in the DADA tool. We study
whether these attacks are effective and how many participants
are able to identify these “synthetic” attacks (H4).

III. EXPERIMENT SETUP

To evaluate the effectiveness of the attack under different
scenarios, and to investigate the strategies that people use
in their decision-making process, we perform a user study.
Figure 3 shows the diagram of the experiment. Before coming
to the lab, all participants take a personality test. This is
done to reduce the time spent in the lab and the potential
for fatigue. We used the Big Five Personality test [13] for
measuring personality traits of participants. The Big Five
Personality Traits is a well known and widely accepted model
for measuring and describing different aspects of human
personality and psyche [14]. This model describes human
personality from ﬁve broad dimensions: Extraversion, Agree-
ableness, Conscientiousness, Neuroticism, and Openness. We
created the personality test using Google Form4 and asked the
participants to do it before coming to the lab for the actual
experiment. Table I presents the range of possible values and

4https://www.google.com/forms/about/

Fig. 3: Flowchart of the entire experiment

In the lab, we begin by asking participants a few basic
questions about their email experience: approximate number
of emails received each day, years of email use, and the spam
ﬁlter used, if any. Next, we ask them about their education and
computer background. These questions are followed by the
scaffolding attack part of the experiment (gray part of Figure
3). It consists of two sections, the ﬁrst one (Legit/Fraudulent)
is designed to study the effectiveness of scaffolding, and the
second (Reasoning) is intended to investigate in-depth the
strategies employed by participants when the pressure to make
a decision is relaxed.

In the Legit/Fraudulent section, we use two different con-
texts for sending emails to the recipients, Gmail and LinkedIn.
There are two representative offers using Gmail and two using
LinkedIn. One offer in each of them is generated by NLG
(FakeN) and the other one is a ‘real’ representative offer
fraud (Fake). In the Email Delivered by Gmail (EDG), we
show a screenshot of Gmail interface and ask participants to
decide if this a legitimate email or fraudulent one. In the Email
Delivered by LinkedIn (EDL), ﬁrst, we show them a LinkedIn
friend request (screenshot of a friend request email) and ask
them to suppose after receiving the friend request they have
received a message from that person via LinkedIn (screenshot
of a LinkedIn message in their inbox). We also added six more
representative offers consisting of both EDG and EDL. They
are generated by using different combinations of information
about the company that hires and the name of the recipient.
To see how people perform in the case of emails without
any fraudulent signals, we also use two real job offers in this
section, but We change the LinkedIn logo in one of them with
a fake logo (very similar to the actual LinkedIn logo, Figure
2d).

For each attack scenario, we ask (i) “Do you think this
is a legitimate scenario or a fraudulent scenario?” and (ii)
“If you think this scenario is fraudulent, please list ALL the
reasons that made you think the email is fraudulent. Otherwise,
justify why you think this scenario is legitimate.” In addition
to these questions, we also asked participants to indicate their
conﬁdence level, “How conﬁdent are you about your answer?”
to know if they got lucky. Conﬁdence ranges from 1 (least

TABLE I: Range of possible scores in each trait of Big Five Personality test in addition to participants’ scores

Traits

Extraversion
Agreeableness
Conscientiousness
Neuroticism
Openness

Minimum
Possible/Participants
8/14
9/14
9/19
8/11
10/21

Maximum
Possible/Participants
40/39
45/43
45/44
40/38
50/46

Mean
Participants
26.1
32.6
31.4
24.2
35.2

Median
Participants
27.5
33
32
23
35

StdDev
Participants
6.2
6.7
6.4
6.1
5.6

conﬁdent) to 5 (extremely conﬁdent). In order to remove the
effect of the order of representative offers, we randomize them
once at the beginning and then use the same generated order
for all the participants. We do not change the order of questions
for each participant since it affects their response to each
question in a different way.

The Reasoning section consists of nine different represen-
tative offer, eight generated by the NLG and one offer with
“real” representative offer fraud from the Internet. Out of
those eight NLG generated offers, four have fake LinkedIn
logos of different difﬁculty level (see Figure 2), three are
customized offers based on participants background, and one
with an obvious fake company name. For the customized
emails, customization is done based on: participants’ ﬁeld
of study/work and the institution in which they studied. We
gathered this information by asking the participants to send us
their LinkedIn web page or their resume before coming to the
lab. In this section, in contrast to the previous one, we do not
ask participants to decide if the email/scenario is fraudulent or
legitimate. We told them at the beginning that all the emails
in this part are fake and they should just ﬁnd the clues and
factors that made this email/scenario fraudulent.

At the end, we interviewed the participants to evaluate their
knowledge and experience about the Email in more detail.
“What are the different parts of the email” and “How much
do you rely on sender’s email address in order to decide if an
email is fake or not” are some examples of these questions. We
present the answers that we got from the interview in Section
VI.

For running the survey, we utilized Form Maker5 which
is a plugin for Wordpress,6 a free and open source content
management system. It enables us to keep track of the total
time participants spent on reading each representative offer
and the time they spent on answering questions.

A. Participants

Before the experiment, we requested IRB approval. We also
did a small pilot study with three participants before running
the actual survey. During the pilot study, we found and ﬁxed
the following problems:

• Legitimate Emails: We had used two emails from Enron
Email Dataset. Since the topic of these emails did not
match those of the Fake emails (representative offer), we
replaced them with legitimate job offers.

5https://wordpress.org/plugins/form-maker/
6https://wordpress.org/

• Fake Company Name: The fake name that we used at
beginning was hard to detect. So, we changed it to an
obvious one (Donald Duck and Mickey Mouse).

After IRB approval, a recruitment email was sent to all
the students at the College of Natural Sciences and Mathe-
matics, which includes six departments (and over 20 majors):
Biology & Biochemistry, Chemistry, Computer Science, Earth
& Atmospheric Science, Mathematics, and Physics. We also
mentioned in the email that participants will be given $20
Amazon gift card upon ﬁnishing the experiment. To further
diversify the participant pool, we also recruited staff, so
we have some majors from other colleges also. We had 34
participants, of which 15 were female (44%) and 19 male
(56%). The majors of our participants are computer science
(33%), Biology (26%), Chemistry (12%), ﬁnance (9%) and
others (20%). From the academic degree aspect, four of them
are Ph.D. students, two are Masters students, 26 Bachelors
students and two of them have High School Diplomas.

Most of the participants use Gmail as their spam ﬁlter, 30
out of 34 (88%). Kaspersky, Macafee, Yahoo, and Web Of
Trust each is used by one participant. Below are some other
statistics about the participants (since some of the data are
skewed, we also report the median, as well as ﬁrst and third
quartile).

• Age: Range [18 − 64] years (1st quartile = 19, median =

21, mean = 25, 3rd quartile = 23, SD = 10.7).

• Number of emails received daily: Range [3 − 100] (1st
quartile = 7, median = 12, mean = 19.66, 3rd quartile =
20, SD = 19.74).

• Years of email usage: Range [4 − 20] (1st quartile = 8,
median = 10, mean = 10.55, 3rd quartile = 11.75, SD =
3.76).

• Social network usage (number of times checked per
week): Range [0 − 100] (1st quartile = 7, median = 20.5,
mean = 71.81, 3rd quartile = 50, SD = 222.96).

B. Variables

Three categories of variables exist in the study: independent,
dependent, and unmanipulated (extraneous) variables. Figure 4
shows the variables and their relationships. Independent vari-
ables are those that we manipulated on different representa-
tive offers to see how they affect participants’ performance.
Table II lists and explains the independent variables.

Dependent variables are the observed variables and the
goal of this study is to ﬁnd how they depend on the other
variables. Performance of participants on detection legitimate
and fraudulent job offers is the main dependent variable in our

explains unmanipulated variables (age and gender are removed
from the table). For Email Propagation Knowledge, we deﬁne
four levels: (i) zero knowledge, (ii) those who know there is a
server that receives email from the sender and sends it to the
receiver, (iii) those who know there is more than one server
in the middle (sender email server and receiver email server),
and (iv) those who have fairly comprehensive knowledge.

TABLE III: Unmanipulated Variables and Description

Unmanipulated variable
Years using email
Education level

Email propagation knowledge

Email sent each day

Social network

Personality traits

Description
Number of years using email
Participants’ seeking degree
Level of knowledge about how
emails are sent through the Internet
Number of email sent each day
How many times per week
they use any social network
Five personality traits deﬁned
by Big Five Personality Test

IV. PERFORMANCE ON THE LEGITIMATE/FRAUDULENT
SECTION

A common question that needs to be addressed in almost
all user studies is “do the predictor variables have any effect
on dependent variable?” In this section, we present the effec-
tiveness of the attack scenarios that we introduce and whether
any variables have any statistically signiﬁcant effect on the
performance of participants. We take a similar approach that
has been used in [3] to compare the results. We used R [15]
to perform all the signiﬁcance and correlation tests.

First, we study the effect of independent variables which
are the main goal of this experiment, and then the effect
of unmanipulated variables. Table IV shows details of each
representative offer in the ﬁrst section of the experiment with
the performance of participants and their average conﬁdence
level. To cluster together offers that share a common variable,
we changed their order in Table IV (the actual order is in
Column 2).

Participants’ detection rate ranges from 4/10 (40%) to
9/10 (90%) (mean= 0.653, stddev=0.142, var=0.02). Aver-
age conﬁdence level of participants ranges from 2.9 to 5
(mean=3.74, stddev=0.51, var=0.26). In order to have a better
understanding of the effect of each variable, Table V provides
overall performance in each category of representative offers.
The last column in Table IV and second column in Table
V is fraudulent Detection Rate which is the percentage of
participants who detected the offer as fraudulent even if the
offer itself is legitimate. This makes the comparison easier.
So, for the 10th offer and last two rows of Table V, the actual
performance of the participants is one minus the value in the
cell (the value in the parentheses). In the case of Fraud WDR,
last column of Table V, SDR is 1 for offers detected as fake
and -1 for offers detected as real regardless of whether the
offer is real or fake.

The representative offer with fake logo has the least detec-
tion rate (14.7%), and WDR shows that participants are highly
conﬁdent that this is a legitimate offer. It shows that people

Fig. 4: Relationship Between Variables

study. Detection rate (DR) is the proportion of questions that
are answered correctly, legitimate offer detected as legitimate
and fraudulent detected as fraudulent. The conﬁdence level is
the degree of conﬁdence that participants have in their answers
to the questions. In the result sections, we use the average
conﬁdence level (ACL) instead of separate conﬁdence level
for each question/participant. The third dependent variable
is Weighted detection rate which combines detection rate
and conﬁdence level together. DR only considers the ﬁnal
output of participants for each question, either zero or one.
This combination (DR and conﬁdence level) helps us to
differentiate between participants who answered correctly with
high conﬁdence and low conﬁdence, and conversely, those who
answered incorrectly with high conﬁdence and low conﬁdence.

Equation 1 shows the relation of the WDR.

W DR =

SDR =

(cid:80)

all questions SDR ∗ Conf idence
|all questions|

(cid:26) 1 if DR = 1
−1 otherwise

,

(1)

TABLE II: Independent Variables and Description

Independent variable

LinkedIn/Gmail

NLG/Human

Legitimate/Fraud

Attack type

Description
Medium used for sending representative
offer
Representative offer created by
NLG or human
Real job offer and fraudulent
representative offer
Different attacks: Sender info., receiver info.,
both, and none

Unmanipulated variables are variables over which we have
no control. By randomly choosing the participants from the
population, knowing the fact that people reply the recruitment
email randomly, we reduce the effect of these variables on
our study. Nevertheless, we study if there is any relationship
between them and dependent variables. Table III lists and

Independent VariablesLegitimate/Fraud.LinkedIn/GmailAttack TypeUnmanipulated VariablesGenderAgeYears Using EmailEmail Propagation KnowledgeEducation LevelEmail Sent each DaySocial Network UsagePersonality TraitsDependent VariablesWeighted Detection RateDetection RateConfidence Levelin

10

the

offers

representative

TABLE IV:
legiti-
mate/fraudulent section (detection rate and average conﬁdence
level are included). Column. 1 - variable changes in each
offer. n - actual order in the experiment. R/F - correct answer
of the question, FakeN - Fake representative offer created
by NLG, EDG - Email delivered by Gmail, EDL - Email
delivered by LinkedIn.

Variable

LinkedIn/Gmail
NLG/Human

Attack Type

Legitimate

n
1
3
5
7

2

4

6

8

9

10

R/F
F
F
F
F

F

F

F

F

F

R

Feature
EDL/FakeN
EDG/Fake
EDL/Fake
EDG/FakeN
EDG/Fake/
Sender
EDG/Fake/
Recipient
EDG/Fake/
Sender/Recipient
EDG/Fake/None
EDL/No Signal/
Fake Logo
EDG/No Signal

DR % (ACL)
85.22 (3.76)
88.23 (3.7)
61.74 (3.47)
94.11 (4.05)

64.70 (4.17)

70.58 (3.14)

38.23 (3.64)

94.11 (4.14)

14.70 (3.97)

41.17 (3.32)

do not pay attention to the logo of the company. Later, we
check the participants’ reasoning to see if they mentioned fake
logo. Representative offer delivered by Gmail has the highest
detection rate (91.7%) and compared to the LinkedIn delivered
ones, there is a gap of about 20%. We shall see whether the
difference is statistically signiﬁcant.

TABLE V: Performance of participants on each category of
offer (some offers are shared among two or more categories).
n - number of offers in that category

Category (n)
LinkedIn (2)
Gmail (2)
Human (2)
NLG (2)
Fake Offer (8)
Real, No Signal (1)
Real Job Offer (2)

Fraud. DR
73.52
91.17
74.98
89.7
74.63
41.17 (58.83)
27.93 (72.07)

ACL
3.61
3.88
3.58
3.91
3.64
3.32
3.51

Fraud. WDR
1.79
3.38
1.91
3.26
1.98
0.85 (-0.73)
-1.11 (1.11)

A. Gmail and LinkedIn

The top two rows of Table V show the performance of
participants on detecting LinkedIn and Gmail delivered rep-
resentative offers. The results illustrate that success rate of
deception attacks that are delivered by LinkedIn is higher than
those delivered by Gmail. T-test shows a signiﬁcant difference
between performance of participants on LinkedIn and Gmail
delivered representative offers (DR: p=0.009, df=53.183, t=-
2.683, WDR: p=0.001, df=52.117, t=-3.283). To avoid multi-
ple comparison problem, we utilized Benjamini-Hochberg [16]
procedure to ﬁnd the signiﬁcant p-values. Both of them are still
signiﬁcant after applying the Benjamini-Hochberg procedure.
So, there is a statistically signiﬁcant difference between the
effectiveness of attack using LinkedIn versus Gmail.

We go one step deeper to see which personality types
are more vulnerable to this kind of attack. Signiﬁcant tests

show that participants with lower Conscientiousness level (p-
value = 0.005) and lower Extraversion level (p-value = 0.023)
have signiﬁcant difference in their detection rate for LinkedIn
and Gmail delivered representative offers (p-value for partic-
ipants with higher Openness level is also less than 0.05 but
Benjamini-Hochberg procedure made it non-signiﬁcant).

B. NLG and Human

The average performance of participants on representative
offers that are generated by NLG and representative offers that
are generated by scammers (Human) can be found in the third
and fourth rows of Table V. The Detection rate for NLG is
larger than the detection rate for Human, which shows that
NLG generated emails are easier to detect.

Same as the Gmail and LinkedIn comparison, t-test (after
applying Benjamini-Hochberg procedure) shows a signiﬁcant
difference between the performance of participants on NLG
and Human generated representative offers (DR: p=0.01,
df=63.233, t=-2.627, WDR: p=0.002, df=63.763, t=-3.197).
This means our NLG generated offers are not as powerful
as human-generated ones in fooling people. In Section V,
we go into details of participants’ reasoning to have a better
understanding of this difference.

C. Real and Fake

The average performance of participants on fake represen-
tative offers and the legitimate job offer (Real, No Signal) can
be found in the ﬁfth and sixth rows of Table V. The offer that
we used for the fake logo is also a real job offer similar to
Real, No Signal. Since no one detected the fake logo, we also
consider the representative offer with the fake logo as real. So,
we have eight fake representative offers and two real ones. The
last row of Table V (Real) is the average performance for the
real job offers (Real is different from Real, No Signal).

The detection rate of the Fake is larger than the detection
rate for Real, which shows fraudulent emails are easier to de-
tect, but the difference is not signiﬁcant (p=0.076, df=46.731,
t=-1.809). The WDR is also higher for the fake offers which
their answer
means participants are more conﬁdent about
when they choose Fraudulent than the Legitimate. In other
words, participants tend not to choose Legitimate with high
conﬁdence, but again the difference is not signiﬁcant (p=0.075,
df=46.981, t=1.818).

D. Attack Type

We added two kinds of information to our emails, sender
and receiver information. Different combinations of these
variables result in four different emails. Table IV shows these
four offers with participants’ performance for each of them.
Having both information in the email can dramatically reduce
the detection rate (detection rate dropped from 94% to 38%).
We conclude that emails that contain both sender and receiver
information look more legitimate and are more capable of
fooling people.

E. Unmanipulated Variables

Unmanipulated variables are those that unlike independent
variables are neither manipulated nor ﬁxed in the experiment.
So, these variables could affect the dependent variables. Here,
we study the relation between these variables and dependent
variables. We have different kinds of unmanipulated variables,
some of them can be grouped together, e.g., age and sex
are both part of participant demographics. Based on these
similarities, we categorize the unmanipulated variables and
study the correlation for each category separately. We have:

• Personality Traits: Five different traits deﬁned by Big

Five Personality Test

• Demographics: Age, gender
• Knowledge and Background: Participants’ background
knowledge on computer and email, and their education
level. Emails sent each day, Email propagation knowl-
edge, Social network usage and education level are in
this category.

1) Personality Traits: Personality traits reveal internal as-
pects of each person’s mind. Since the performance of par-
ticipants and the personality trait scores are not continuous
variables, it is better not to use the Pearson correlation test
between each trait and the performance value directly. Instead
of using correlation test directly, we calculate the correlation
between participants’ ranks on DR, WDR, and ACL and
their ranks on each of the personality traits, but none of the
correlations are signiﬁcant.

We also group the participants into two groups based on
each personality trait and perform a t-test for comparing
them. For each trait, suppose M is the median. We divide the
participants into two groups based on the value of M; those
who are greater than M and those who are less than or equal
to M. Then we apply t-test to compare their performance.
Table VI presents p-values of t-test on different groups of
participants. The t-test shows no signiﬁcant difference between
participants’ personality and their performance and conﬁdence
level.

TABLE VI: P-value of t-test for comparing the performance
of participants based on each trait

Trait (median)
Extraversion (27)
Agreeableness (33)
Conscientiousness (32)
Neuroticism (23)
Openness (35)

DR
1
0.129
0.989
0.73
0.662

WDR
0.995
0.991
0.242
0.982
0.452

ACL
0.846
0.667
0.209
0.348
0.227

2) Demographics: Demographic features of participants
can also have an effect on their responses. We check the
signiﬁcance of the difference between mean values of depen-
dent variables as a function of age and sex. For the age, we
divided the participants into two groups: older than 21 years
old (median) and younger than 21. Based on the t-test result,
there is no statistically signiﬁcant difference in participants’
average performance with different age and sex.

TABLE VII: Number of participants using each strategy in the
ﬁrst section. N - No. of participants, PG - performance gain

Strategy
Asking for Action/Info
Legality Claim
Phishing Hints
Sender Info
Job Info
Grammar, Capitalization, Punctuation
LinkedIn
Sender’s Email
Receiver Info
Spelling Issue
Proﬁle Picture
Practical Real World Consideration
Company Info
Email Production and Delivery
Over-thinker
Fake Logo

N
32
26
24
21
20
19
13
12
10
10
9
9
8
7
4
0

%
94.1
76.4
70.5
61.7
58.8
55.8
38.2
35.2
29.4
29.4
26.4
26.4
23.5
20.5
11.7
0

PG
-0.209
0.020
0.018
-0.038
-0.080
0
-0.073
0.008
-0.103
-0.046
-0.072
0.033
-0.036
0.077
-0.088
0

3) Knowledge and Background: Users’ background knowl-
edge and experience in working with email and other ap-
plications like social networks may have an effect on their
performance. Hence, we test the correlation between these
variables (email propagation knowledge, email sent each day,
social network usage, education level, and years using email)
and performance indicators. Applying correlation test on them
does not show any signiﬁcant correlation between them.

F. Strategy Analysis

In each representative offer of the legitimate/fraudulent
section, besides asking “Fraudulent/Legitimate?”, we also
asked participants to write down their entire reasoning for
their decision. The reasons used by participants to distinguish
between fraudulent and legitimate offers is another important
aspect of this study. Knowledge of these strategies can be used
by defenders to devise a new generation of attacks and test
their ﬁlters, and also to identify the things that email users do
not pay attention to and need help with.

We categorized participants’ strategies and the signals that
we added to the offers (e.g. fake logo and LinkedIn) into
16 groups as follows (the steps that we took to extract these
strategies are recommended by [17]):

• Asking for action/info: Does the email ask for doing an

action? or ask for any information?

• Legality Claim: The email claims that the offer is legal
• Phishing Hints: The email tries to deceive the reader or

talks about doing payment

• Sender Info: Sender’s contact info (address, phone, and

company name) are provided or not

• Job Info: Enough details about the job are provided or

not

• Grammar, capitalization, and punctuation
• LinkedIn: The message was delivered through LinkedIn,

not through Gmail

• Sender’s Email: All the aspects related to the sender’s
email address, e.g., using public email servers, emails
with different top-level domain than the actual company

(info@chase.opportunity.com), and email whose domain
is same as the company name (info@microsoft.com)
• Receiver’s Info: Sender has some information about the

receiver (his name or educational background)
• Spelling Issue: Misspelled words in the Email
• Proﬁle Picture: Anything suspicious or non-suspicious
about the LinkedIn proﬁle picture, e.g., “proﬁle pic is a
white girl while the company is from India”, “unprofes-
sional proﬁle picture”, etc.

• Practical Real World Consideration: The email asks to
reply to another email or requests for further contact to
share more information

• Company Info: Enough details about the company are

provided or not

• Email Production and Delivery: All the details about
the structure, writing and author of the emails, e.g.,
spacing error, poor formating, CEO sending email, and
using letterhead

• Over-thinkers: Participants who argued that spelling and
grammar issues show that the email is real, since if it was
from an attacker, it would not have such mistakes

• Fake Logo: Fake LinkedIn logo is used
Table VII shows how many participants used each type of
strategy. Asking for action/info, legality claim and phishing
hints are the most used strategies. To have a better under-
standing of the difference between strategies, we deﬁne a
metric called Performance Gain (PG). The idea is similar
to information gain, but for PG, we calculate the difference
between the average performance of participants who used a
speciﬁc strategy s and the average performance of those who
did not use s (P G(s) = AvgDRused(s)−AvgDRnotused(s)).
Performance gain for all of the strategies is very low. This
shows that paying attention to a clue itself without knowing
how to leverage that clue is not enough to correctly identify
the phishing/legitimate emails. There are also some negative
performance gains which mean there are some strategies that
fooled people instead of helping them to detect the phishing
emails. We cannot generalize our conclusion since we have
more than one clue in each representative offer and the priority
of strategies differs for each participant.

These strategies along with their importance (priority) for
users’ decision-making process can be used to improve the
effectiveness of detectors by subjecting them to new, more
effective attacks, or to train people about phishing emails.

Studying the effect of strategies used by participants on
their performance is not straightforward. To achieve this goal
we need to ﬁnd a way to group participants based on their
strategies (similarity of strategies that they used). Once we
group them together, we can compare the performance of users
in each group.

We present two different methods for grouping participants
together (a third based on the number of identiﬁed signals
yielded nothing signiﬁcant). In the ﬁrst method, we use the
16 extracted strategies as boolean features for participants and
clustered participants based on the feature vectors. Since we
do not know the ideal number of clusters to use, hierarchical

TABLE VIII: Average DR, WDR, and ACL for each group.
Group: “cluster number” in Clustering and “number of strat-
egy groups” in Sophistication Level. N - number of participants

Grouping
Method

Clustering

Sophistication
Level

Group

0
1
2
<5
5
6
7
8
>8

N

11
4
19
4
6
8
5
7
4

Average DR

Average WDR

ACL

0.71
0.47
0.65
0.65
0.76
0.67
0.58
0.67
0.5

1.78
0.62
1.26
1.9
1.69
0.98
0.97
1.74
0.87

3.55
3.8
3.83
4.15
3.91
3.46
3.56
3.7
3.92

agglomerative clustering [18] is used. We tried both cosine
similarity and Euclidean distance as distance metrics. The
results with Euclidean distance were more reasonable than
with cosine similarity, so we report the ones for Euclidean
distance. The output of hierarchical clustering is a tree whose
leaves are the individual feature vectors. By cutting the tree
at every height we get different clustering output, some of
them may have a singleton cluster. We try different cutting
points and keep only those clusterings that do not have any
singleton cluster for further analysis. In the second method of
grouping, we use sophistication level to separate participants,
where sophistication level is the total number of groups of
strategies that a participant used.

Table VIII shows average performance of participants for
aforementioned grouping methods. Distribution of the detec-
tion rates is also depicted in Figures 5 and 6. For the Clustering
method, we used one of the clustering outputs that has three
classes as an example here. The results for other cutting points
are similar to this one. In the Clustering method, cluster
zero does perform better than two other clusters. We check
later if the difference is statistically signiﬁcant or not. In the
Sophistication Level, there is a rise and then almost a straight
drop in performance with one exceptional group. Among these
levels, sophistication level ﬁve is the best performing category
(DR=0.76), but if we consider conﬁdence level as well, the
minimum sophistication level (<5) is the best performing
(WDR=1.9). The top performer used the following strategies
Sender Info, Asking for action/info, Grammar, capitalization,
and punctuation, Legality claim, and Practical Real World
Consideration.

Fig. 5: Clusterwise participants DR distribution

We apply ANOVA tests on the two aforementioned grouping
methods to see whether or not the performance of each group

signiﬁcant difference was found between strategies used by
different groups.

G. Linear Regression Model

A linear regression model is another way of analyzing the
relation between the variables. First, we create a model for the
detection rate considering one predictor at a time. Predictor
variables are all unmanipulated and independent variables in
our study. We also consider the number of strategies used by
each participant (Strategy Count) as a variable since it can be
interpreted as a sophistication level of participants.

In all generated models, the p-values for predictors is bigger
than 0.05 except for the Strategy Count (p-value = 0.015,
R2 = 0.171). Since there are several unpredictive variables
in the model, we should only keep those variables that have
a minimum contribution to the model. So, we use 0.04 as a
threshold for the R2 value and only consider variables for
which the corresponding model has the R2 value bigger than
0.04. Gender, age, education level, and Strategy Count are
the variables that passed our R2 criteria.8 We created the
model based on these variables, and Table X displays the
coefﬁcients, standard error, t value, and p values for each of
the ﬁxed effects. The R2 is 0.268 (adjusted R2 = 0.105) and
Strategy Count is the only statistically signiﬁcant predictor.
The coefﬁcient for the Strategy Count
is negative, which
means employing too many strategy groups led to a lower
performance for our participants. We repeated this analysis for
WDR and ACL as response variables but none of the results
are signiﬁcant.

TABLE X: Multiple regression model for predicting detection
rate

Fixed Effect
(Intercept)
Age
Gender (Male)
Education (Diploma)
Education (Master)
Education (PhD)
Strategy Count

Coefﬁcient
0.636
0.012
-0.015
-0.419
-0.121
-0.079
-0.034

Std Error
0.168
0.008
0.055
0.349
0.110
0.089
0.014

t
3.770
1.515
-0.278
-1.199
-1.103
-0.892
-2.340

p
<0.001
0.141
0.782
0.241
0.279
0.380
<0.05

V. REASONING SECTION

The second section of the experiment is about the reasoning
of our participants on nine representative offers involving fake
name, fake logos, NLG generated emails and customization.
Customization is done based on participants’ education and/or
work background. Table XI shows details of the offers in this
section (6th is a company representative fraud email that we
used without any change). Recall that, for this section, we told
participants that all emails are fraudulent and they need to ﬁnd
all indicators that show the emails are fake.

Most of the strategies are similar to those in Section IV-F.
Table XII shows all the strategies used by participants. There
are two new strategies:

8For m = 4 independent variables, we need at least 4(m + 2) = 24
independent observations, m coefﬁcients + intercept + variance is m + 2, and
we have 34 > 24 so we can construct a regression model.

Fig. 6: Participants DR distribution on different sophistication
level

differs statistically signiﬁcantly. Table IX shows the p-value
of ANOVA tests on different grouping method. All the p-
values are bigger than 0.05 except for the detection rate
of the hierarchical clustering method (F(2, 31) = 5.26, p-
value = 0.01).7 A Tukey post-hoc test reveals that the cluster
number zero is signiﬁcantly different from two other clusters.
Participants in this cluster do not pay attention to Sender and
Receiver info. This suggests that there are some strategies that
can protect people better from deception attacks. However,
we hesitate to generalize this conclusion since there are some
features that we did not consider, e.g., IQ, native English
speaker, etc.

TABLE IX: P (F) value of ANOVA tests on different grouping
methods

Method
Clustering
Sophistication

DR
0.01 (5.26)
0.06 (2.37)

WDR
0.148 (2.03)
0.44 (0.98)

ACL
0.35 (1.06)
0.25 (1.4)

Different people pay attention to a subset of all fake
signals that exist in the email. An interesting question here
is “Do people with different characteristics have different
strategies?”, e.g., “Do people with higher conscientiousness
use more signals that are less important for those with lower
conscientiousness?” or “Do women use set of signals that
are less important for men?” Here, we choose the number
of strategies as a response variable and test the effect of other
variables on this response variable.

Comparing the number of strategies used by males and
females reveals that males use more strategies than females
(7.2 versus 5.8) and the difference is signiﬁcant with ninetyﬁve
percent conﬁdence level (p=0.024, df=31.25, t=2.364). Besides
that, there are also some differences in the type of strategies
used by male and females. For example, none of the females
used Email Production and Delivery as their strategy. Also,
females pay more attention to the sender information and
spelling errors than males.

The number of strategies used by participants may vary
for people with different personality. To study the relation
between the number of strategies and the personality traits,
again we divide the participants into two groups based on
their traits (higher and lower than median) and compare the
strategies used by them. We did this for all ﬁve traits, but no

7The p-value of 0.06 for Sophistication is also close to our 0.05 threshold

• Fake Company name: Did participants detect the fake
company name for the representative offer from the
Donald Duck and Mickey Mouse company?

• Constructed: Email created by the human or by the

computer (using NLG tech.).

Seven participants noticed that some emails are generated
by the computer. A deﬁciency in the grammar that we used
for NLG can be the reason for this observation. This can
also justify the higher detection rate of NLG generated emails
compared to human-generated emails in the previous section
of the experiment (legitimate/fraudulent). Also, some of the
participants used LinkedIn as their strategy in the ﬁrst section
of the experiment, but in this part, since we told them that all
the emails are fraudulent, no one mentioned it.

TABLE XI: Nine representative offers in the reasoning section.
n - actual order in the experiment, FakeN - Fake representative
offer created by NLG, EDG - Email delivered by Gmail, EDL
- Email delivered by LinkedIn.

Variable

Customized

Fake Logo

Company Name
N/A

n
1
3

8

2
5
7
9
4
6

Feature
EDL/FakeN/Custom Work Background
EDL/FakeN/Custom Education Background
EDL/FakeN/
Custom Education&Work Background
EDL/FakeN/Fake Logo 1
EDL/FakeN/Fake Logo 2
EDL/Fake/Fake Logo 3
EDL/FakeN/Fake Logo 4
EDL/FakeN/Fake Company
EDG/Fake

TABLE XII: Number of participants (N) using each strategy
in the second section.

Strategy

Legality Claim

Fake Company Name
Sender’s Email
Company Info

Sender Info

Spelling Issue
Job Info
Constructed
Over-thinker

N

27

22
19
19

17

10
9
7
2

Strategy
Grammar, Capitalization,
Punctuation
Email Production and Delivery
Asking for Action/Info
Phishing Hints
Practical Real World
Consideration
Receiver Info
Proﬁle Picture
Fake logo

N

25

19
19
18

11

9
8
3

Only three participants detected the fake logos. Two of them
detected only the easiest one and the third participant detected
the ﬁrst two easy ones. This shows that participants cannot
easily tell the difference between visual cues, so one cannot
rely on this kind of authentication (e.g. banks and ﬁnancial
companies have started using images) and phishers can escape
detection even if their images are not exactly similar. Even
though most of the phishing emails use genuine logos [19],
but this can help the attacker to avoid detection techniques
that rely on image similarity as a feature.

Same as in the Legitimate/Fraudulent part, Legality Claim
has a very high frequency. This shows that most of the
participants know that when something is legal, there is no
need to mention its legality explicitly.

Some participants were suspicious of the location of the
company, e.g. being from India, China, etc. (considered as
Company Info). The relation between the proﬁle picture of
the sender (race) and the location of the company (Proﬁle
Picture) and the relation between the name of the company
and their expertise (Company Info) are two other interesting
points brought up by participants.

In the ﬁrst section of the experiment, we asked participants
to decide whether the representative offer is real or fake, and,
we asked them to write down all of their reasoning. It is
possible that participants ignore the rest of the email after they
have made their decision. In the second section, we told them
that all the emails are fraudulent and they just need to say why
these are fake. Comparing the average number of strategies per
question in the ﬁrst and second sections can help us to see if
this really happened or not. The average number of strategies
used in each question in section two (1.78) is higher than
section one (1.46) and the difference is signiﬁcant (p=0.01,
df=15.581, t=-2.92). So, it is possible that some participants
did not pay attention when we told them to write down all
their strategies. However, note that the content and design of
the emails was not exactly the same for the two parts. So, we
cannot be certain about this.

We also have three customized representative offer in this
part. The goal is to see if the customization can affect the
reasoning of the participants or not. The average number of
strategies in each question can be used as a signal to study the
changes in the reasoning of participants. The average number
of strategies for customized offers (1.62) is a little lower than
for uncustomized ones (1.86), the difference is although not
signiﬁcant, but close (p-value = 0.06).

VI. INTERVIEW

At the end of the experiment, we did a short interview
(about 10 minutes) with the participants. The interview had a
combination of some questions that required verbal answers,
e.g., “What is the difference between CC and BCC in sending
email?”, and some that required demonstrations, e.g., “Can you
show us how you can see full email header?” Table XIII shows
all the questions that we asked during the interview. There are
two types of questions, simple and interactive. simple ones
are those that we just ask a question and the participants give
answers to it. The interactive question, on the other hand, has
two parts. First part is same as the simple one, a question that
interviewer asks from participants. But the participants need to
look at another content (an email for example) to answer the
question. For example, in the ﬁfth question, instead of asking
them “Do you know what is the email full header?”, we ask
them to log into their email client, and then show us the full
header of an email. Someone may have heard about the full
header but still may not know how to check it. This helps
us to differentiate between them. Also in the sixth question,
we show participants an email that someone shares the report
of a project via a Dropbox link (we asked them to suppose
the sender is their colleague that work with them in a same
project), but the actual link is pointing to a fake website (in

the html <a> tag, href attribute is different from the tag text).
Here the goal is to check participants’ knowledge about this
technique without asking them directly.

TABLE XIII: Interview questions

n

1

2

3

4

5

6

7

Question

What is the difference between CC and BCC
in sending email?
How many parts there are in the Email? (You
always see content, what about other parts?)
How Emails are sent through the Internet?
If you think an Email is fake, do you check
its header? Which ﬁelds?
Can you show us how you can see full email
header?
Can you ﬁnd out why this is a suspicious Email?
(an email has been shown to the participant)
When you are going to detect if an Email is real
or fake how much do you rely on: Sender email
address, date/time, and domain of sender

Type

Verbal

Verbal

Verbal

Verbal

Demonstrative

Demonstrative

Verbal

20 participants know the difference between CC and BCC,
eight only know CC and six do not know any of them. There
is no signiﬁcant difference between detection rate of those
participants who know about the BCC and the rest of the
participants. Knowledge about the email transmission (sender
server, receiver server, and relays) is too technical, and as we
expected, only two participants had this knowledge. Both of
them are computer science graduate students.

For the full email header, 25 participants know how to just
check the simple ﬁelds of the header (from, to, subject, and
date), three participants do not know anything about the header
and six participants have the knowledge to show the full header
(we showed them the user interface of their own email client
to remove the effect of the different user interface). Here also
all of these six participants are computer science students (ﬁve
graduate and one bachelor). However, no signiﬁcant difference
was found between detection rate of those six participants and
the rest of participants (p-value = 0.833). In the sixth question,
only ﬁve participants recognized that the actual link is different
from what has been shown (no signiﬁcant difference between
their detection rate, p-value = 0.123).

The last question which is a multiple choice question
consists of three subparts. All three subparts are about the
situation that they receive an email and they want to decide
if it is fake or real. We asked them how much they rely on
the sender’s email address, or the domain of the sender, or
the date/time. In order to make it clear for everyone, during
the interview, the interviewer mentioned to all the participants
that by domain of the sender we mean if the email is from
general email services like @gmail.com, @outlook.com, etc
or it is from companies private domain, e.g. @google.com,
@nyu.edu, etc. For each of the subparts, participants are given
three choices “a lot”, “a little” and “not at all.” Only one
participant answered that he/she does not rely on sender email
at all, 23 participants answered “a lot” and 10 participants
answered “a little”. We checked the reasons given in the ﬁrst
section of the experiment by the person who answered “not
at all”, and he/she did not list sender email as their strategy.

Surprisingly, only eight participants out of 23 that chose “a
lot” explicitly listed sender email as their strategy in the ﬁrst
section of the experiment (and four out of 10 who chose “a
little”). This is interesting that only 36% of participants that
said they pay attention to the sender email address explicitly
mentioned it as one of their strategies in the ﬁrst section of
the experiment. We also compared the detection rate of the
participants who answered “a lot” and the rest of participants
but the difference is not signiﬁcant (p-value = 0.085).

For the domain of the sender, 21 participants chose “a lot”,
nine chose “a little” and four participants chose “not at all”.
Again, the important thing is, do they really explicitly mention
the domain of the sender in the ﬁrst section of the experiment?
In the experiment, we had some emails from LinkedIn and
some from other free email services. Same as for “sender
email”, a few participants who answered “a lot” or “a little”
to relying on “sender domain” mentioned LinkedIn in their
reasoning, nine out of 21, three out of nine. Also, one of
the participants out of four whose answers were “not at all,”
referred to LinkedIn in his/her reasoning. However, we could
not ﬁnd any signiﬁcant difference in their performance (p-
value = 0.228).

Participants answer to date/time question is quite different
from two previous subparts. Only two participants answered
“a lot”, 20 “a little’, and 12 “not at all”. So, most of the
participants think that date/time does not have that much effect
on their decision compared to sender email and domain of the
sender. Unfortunately, we did not have any signal related to
the date/time to check if participants pick on this signal.

VII. LIMITATIONS

This study did not consider several variables in its design

and analysis.

We used the job scams directly from the existing datasets
without any modiﬁcations. So, we did not control the clues
that exist in each offer. For example, one offer can have
grammar/writing issues and another one can be grammatically
correct. This affects participants’ decision and as a result the
average detection rate of participants in each study group.

In the ﬁrst section of the study, we used two legitimate offers
and eight fake offers. This can bias participants decision into
not labeling offers as fake since they might feel they labeled
everything as fake. Adjusting the ratio of classes to not bias
the participants is challenging in a study like ours.

We showed eight fake emails with three different variables
(LinkedIn/Gmail, NLG/Human, and four attack types) to par-
ticipants. This might raise the concern about the learning effect
of showing so many variables to each participant. Using a ﬁxed
ordering for the offers deteriorates the issue. An improvement
to this study would randomize the offers presented to the
participants.

When we checked participants’ reasoning, we realized some
of the changes that we made to the fake offers created some
unexpected clues. For example, for one of the offers, partic-
ipants noticed a gender mismatch between the name of the
sender and his/her proﬁle picture. An overall improvement in

consistency and credibility of the fake offers would strengthen
the study.

VIII. RELATED WORK

Research on phishing and social media can be divided
into three categories: 1) attacks on social media [20]–[22],
2) human interaction with phishing/spam emails and social
networks [23]–[25], and 3) detection and authentication mech-
anism [26]–[32].

Attacks on Social Networks. These attacks are variants
of traditional security threats that leverage social networks
as a new medium. Identity theft attack [20], [22], [33] and
reverse social engineering attack [34] are two types of these
attacks. The social scaffolding attack (using LinkedIn) that
we introduced in this study is a new type of attack on social
networks. Previous studies have analyzed the effect of using
Facebook as a medium for delivering the attack messages
[35], [36] but their ﬁnding contradicts each other. Researchers
in [35] found fraudulent messages delivery by email more
successful in fooling users to click on a link embedded in
the message, while authors in [36] found messages delivered
by Facebook more convincing.

Defense Mechanism. System- and user-oriented are two
categories of research related to the defense mechanism. The
system-oriented approach tries to create different computer
models to stop the existing attacks, e.g. detecting Identity
Clone attacks [37]–[39] or privacy control. On the other hand,
the user-oriented approach tries to increase user awareness
about fraudulent activities. There is a good survey on different
factors that have an impact on individual susceptibility to
malicious online inﬂuence [40]. Understanding these factors
can help to effectively reduce potential vulnerabilities and
develop more effective and targeted mitigations.

User Study. Understanding the relationship between users’
characteristics and their behaviour when they encounter phish-
ing emails/websites has been done previously [5], [41]–[46].
They also analyzed the users’ decision making process when
they decide to respond to an email or not. The relation between
demographic features and phishing susceptibility indicates that
women and participants between 18 to 25 are more susceptible
than other participants [42]. Authors in [46] show that in the
case of message content alone, many users face a hard task to
differentiate between a genuine email and a bogus one. Their
focus is on content, not mimicking the scaffolding to deceive.
Researchers in [19] show that phishing emails with logos are
more capable of fooling people. They did their study with real
phishing emails, which have original logos of the companies,
but they did not do any further investigation to see if there is
any difference between the effectiveness of the attack using
fake or original logos. We showed that people do not pay
attention to the originality of the logos.

Analysis of users’ behavior can be done by using the
strategies that they used. Authors in [43] showed that even
though people are good at managing the risks that that they
are aware of, they do not perform well in cases of unfamiliar
risks. We can conclude that scaffolding can be successful since

most of the people believe that messages in a professional
social network like LinkedIn are legitimate. Our experiment
is similar to this work but they created their own phishing
emails by putting some fake signals together, whereas we used
a combination of existing company representative fraud email
and crafted ones. Since they had a set of speciﬁc clues for each
email, they could not analysis participants’ strategy sets and
their relation to the performance of detecting fake emails. The
relation between habitual use of social network and probability
of getting deceived was also studied in [44]. Their study shows
that both habitual and “inhabitual” users accept the friend
requests, but habitual users are more susceptible to providing
information to the phisher. Interestingly, there is some research
that shows Facebook does not seem to have the same effect as
LinkedIn. Researchers in [35] ﬁnd that users were more likely
to click on a link in email versus in a Facebook message.
LinkedIn and Facebook have different goals, which may
explain the difference. Researchers in [47] used demographics,
anonymity, social capital and risk perception of Facebook
users to predict their susceptibility to phishing attacks. They
used questionnaire approach to test their hypotheses which is
not an accurate method based on our ﬁndings that people do
not have an explicit knowledge of their strategies. In other
words, either people are not aware of the strategies they use,
or they do not explicitly mention some of them.

Although LinkedIn has been used as a mechanism for
spreading phishing emails [48], none of the previous works
studied the scaffolding separately as a mechanism for im-
proving the effectiveness of a deception attack. Besides this,
we also, analyze the strategies that participants used and the
correlation between the demographics and their performance.
Also, the effectiveness of natural language generation in email
masquerade attack has been studied in [49], but here we use it
for generating company representative offers. We believe that
NLG may not have worked as well for offers since they tend
to be longer and more complex.

IX. CONCLUSIONS AND FUTURE WORK

We conducted a study in which we parameterized a de-
ception attack and observed how users deal with it. We built
our study based on the company representative fraud. Using
LinkedIn instead of traditional email-based attacks, different
level of information about sender and receiver, and using fake
logos are some of the parameters that we studied.

Our study showed that putting a simple deception attack
into the context of LinkedIn can increase the attack success
rate signiﬁcantly. People trust messages received from a pro-
fessional network like LinkedIn more than normal emails.
We also analyzed the participants’ strategies among other
variables. Our analyses show that the participants who use
fewer strategies perform better than the other participants.
We also found that the strategies used by females and males
are different from each other. Males use more strategies than
females, and they pay more attention to the formatting of the
emails. Yet, interestingly, the performance is not signiﬁcantly
different between the two groups.

Adding sender’s contact information and also making the
email customized for each particular receiver, by adding
receiver’s name in the greeting, can make the message seem
more legitimate. We also used natural language generation
(NLG) technique to semi-automatically generate the attack, but
this attack was not so successful, and participants performed
better in detecting NLG generated emails.

The interview that we did with participants at the end of the
experiment revealed that people may not use or explicitly men-
tion the strategies that they use. Only 36% of the participants
who said they pay attention to the sender’s email address a lot,
actually mentioned the sender email in the ﬁrst section of the
experiment. Our ﬁndings suggest that training regimes need
to: reinforce that less is more, focus on a few key strategies,
and emphasize that a professional network is not necessarily
more trustworthy. Attackers can inﬁltrate any network.

As with all new works, this represents an initial attempt to
explore the space of social trust exploitation, and even though
it has the above limitations, we believe it could help other
researchers. We plan to improve the work in the future so that
the experimental setup addresses the limitations.

ACKNOWLEDGMENTS

This research was partially supported by NSF grants CNS
1319212, DUE 1241772, DGE 1433817 and CNS 1527364. It
was also partly supported by U. S. Army Research Laboratory
and the U. S. Army Research Ofﬁce under contract/grant
number W911NF-16-1-0422.

REFERENCES

[1] T. Kelley and B. I. Bertenthal, “Real-world decision making: Logging

into secure vs. insecure websites,” in USEC, 2016.

[2] S. E. Schechter, R. Dhamija, A. Ozment, and I. Fischer, “The emperor’s
new security indicators,” in 2007 IEEE Symposium on Security and
Privacy (SP’07).

IEEE, 2007, pp. 51–65.

[3] M. Alsharnouby, F. Alaca, and S. Chiasson, “Why phishing still works:
user strategies for combating phishing attacks,” International Journal of
Human-Computer Studies, vol. 82, pp. 69–82, 2015.

[4] J. Wang, T. Herath, R. Chen, A. Vishwanath, and H. R. Rao, “Phishing
susceptibility: An investigation into the processing of a targeted spear
phishing email,” IEEE Trans. Prof. Communication, vol. 55, no. 4, pp.
345–362, 2012.

[5] A. Neupane, M. L. Rahman, N. Saxena, and L. Hirshﬁeld, “A multi-
modal neuro-physiological study of phishing detection and malware
warnings,” in Proceedings of the 22nd ACM SIGSAC Conference on
Computer and Communications Security. ACM, 2015, pp. 479–491.

[6] D. Galanis, G. Karakatsiotis, G. Lampouras, and I. Androutsopoulos,
“An open-source natural language generator for owl ontologies and its
use in prot´eg´e and second life,” in Proceedings of the 12th Conference of
the European Chapter of the Association for Computational Linguistics:
Demonstrations Session. Association for Computational Linguistics,
2009, pp. 17–20.

[7] J. D. Moore and W. R. Swartout, “A reactive approach to explanation:
taking the users feedback into account,” in Natural language generation
in artiﬁcial intelligence and computational linguistics. Springer, 1991,
pp. 3–48.

[8] C. Paris, W. R. Swartout, and W. C. Mann, Natural language generation
in artiﬁcial intelligence and computational linguistics. Springer Science
& Business Media, 2013, vol. 119.

[9] E. Hovy, “Generating natural language under pragmatic constraints,”

Journal of Pragmatics, vol. 11, no. 6, pp. 689–719, 1987.

[10] K. F. McCoy and J. Cheng, “Focus of attention: Constraining what can
be said next,” in Natural language generation in artiﬁcial intelligence
and computational linguistics. Springer, 1991, pp. 103–124.

[11] A. C. Bulhak, “The dada engine,” Available at dev.null.org/dadaengine/,

1996.

[12] ——, “On the simulation of postmodernism and mental debility using
recursive transition networks,” Monash University Department of Com-
puter Science Technical Report, 1996.

[13] O. P. John and S. Srivastava, “The big ﬁve trait taxonomy: History,
measurement, and theoretical perspectives,” Handbook of personality:
Theory and research, vol. 2, no. 1999, pp. 102–138, 1999.

[14] L. A. Clark, “Assessment and diagnosis of personality disorder: Peren-
nial issues and an emerging reconceptualization,” Annu. Rev. Psychol.,
vol. 58, pp. 227–257, 2007.

[15] R Core Team, R: A Language and Environment

for Statistical
Computing, R Foundation for Statistical Computing, Vienna, Austria,
2016. [Online]. Available: https://www.R-project.org/

[16] Y. Hochberg and Y. Benjamini, “More powerful procedures for multiple
signiﬁcance testing,” Statistics in medicine, vol. 9, no. 7, pp. 811–818,
1990.

[17] Y. D. Eaves, “A synthesis technique for grounded theory data analysis,”
Journal of advanced nursing, vol. 35, no. 5, pp. 654–663, 2001.
[18] F. Murtagh, “A survey of recent advances in hierarchical clustering
algorithms,” The Computer Journal, vol. 26, no. 4, pp. 354–359, 1983.
[19] M. Blythe, H. Petrie, and J. A. Clark, “F for fake: four studies on how
we fall for phish,” in Proceedings of the SIGCHI Conference on Human
Factors in Computing Systems. ACM, 2011, pp. 3469–3478.

[20] L. Bilge, T. Strufe, D. Balzarotti, and E. Kirda, “All your contacts are
belong to us: automated identity theft attacks on social networks,” in
Proceedings of the 18th International Conference on World Wide Web,
WWW 2009, Madrid, Spain, April 20-24, 2009, 2009, pp. 551–560.
[21] S. Mahmood and Y. Desmedt, “Your facebook deactivated friend or a
cloaked spy,” in Pervasive Computing and Communications Workshops
(PERCOM Workshops), 2012 IEEE International Conference on.
IEEE,
2012, pp. 367–373.

[22] M. Huber, M. Mulazzani, E. Weippl, G. Kitzler, and S. Goluch, “Friend-
in-the-middle attacks: Exploiting social networking sites for spam,”
IEEE Internet Computing, vol. 15, no. 3, pp. 28–34, 2011.

[23] C. Tang, K. Ross, N. Saxena, and R. Chen, “Whats in a name: a study
of names, gender inference, and gender behavior in facebook,” in Inter-
national Conference on Database Systems for Advanced Applications.
Springer, 2011, pp. 344–356.

[24] K. Meijdam, W. Pieters, and J. van den Berg, Phishing as a Service:
Designing an ethical way of mimicking targeted phishing attacks to train
employees. TU Delft, 2015.

[25] E. Karavaras, E. Magkos, and A. Tsohou, “Low user awareness against
social malware: An empirical study and design of a security awareness
application,” in 13th European Mediterranean and Middle Eastern
Conference on Information Systems, 2016.

[26] A. E. Aassal, S. Baki, A. Das, and R. M. Verma, “An in-depth
benchmarking and evaluation of phishing detection research for security
needs,” IEEE Access, vol. 8, pp. 22 170–22 192, 2020.

[27] A. Das, S. Baki, A. E. Aassal, R. M. Verma, and A. Dunbar, “SoK:
A comprehensive reexamination of phishing research from the security
perspective,” IEEE Commun. Surv. Tutorials, vol. 22, no. 1, pp. 671–708,
2020.

[28] G. Egozi and R. Verma, “Phishing email detection using robust nlp
techniques,” in 2018 IEEE International Conference on Data Mining
Workshops (ICDMW).
IEEE, 2018, pp. 7–12.

[29] A. Herzberg, “Combining authentication, reputation and classiﬁcation to
make phishing unproﬁtable,” in IFIP International Information Security
Conference. Springer, 2009, pp. 13–24.

[30] R. Oppliger and S. Gajek, “Effective protection against phishing and
web spooﬁng,” in Communications and Multimedia Security, 9th IFIP
TC-6 TC-11 International Conference, CMS 2005, Salzburg, Austria,
September 19-21, 2005, Proceedings, 2005, pp. 32–41.

[31] R. M. Verma and D. J. Marchette, Cybersecurity Analytics. CRC Press,

2019.

[32] X. Zhou and R. Verma, “Phishing sites detection from a web developer’s
perspective using machine learning,” in 53rd Hawaii International
Conference on System Sciences, HICSS 2020, Maui, Hawaii, USA,
January 7-10, 2020. ScholarSpace, 2020, pp. 1–10.

[33] M. Huber, M. Mulazzani, and E. Weippl, “Who on earth is mr. cypher:
automated friend injection attacks on social networking sites,” in IFIP
International Information Security Conference.
Springer, 2010, pp.
80–89.

[34] D. Irani, M. Balduzzi, D. Balzarotti, E. Kirda, and C. Pu, “Reverse
social engineering attacks in online social networks,” in International
Conference on Detection of Intrusions and Malware, and Vulnerability
Assessment. Springer, 2011, pp. 55–74.

[35] Z. Benenson, A. Girard, N. Hintz, and A. Luder, “Susceptibility to url-
based internet attacks: Facebook vs. email,” in Pervasive Computing
and Communications Workshops (PERCOM Workshops), 2014 IEEE
International Conference on.

IEEE, 2014, pp. 604–609.

[36] Z. Benenson, F. Gassmann, and R. Landwirth, “Unpacking spear
phishing susceptibility,” in Financial Cryptography and Data Security,
M. Brenner, K. Rohloff, J. Bonneau, A. Miller, P. Y. Ryan, V. Teague,
A. Bracciali, M. Sala, F. Pintore, and M. Jakobsson, Eds.
Cham:
Springer International Publishing, 2017, pp. 610–627.

[37] L. Jin, H. Takabi, and J. B. Joshi, “Towards active detection of identity
clone attacks on online social networks,” in Proceedings of the ﬁrst ACM
conference on Data and application security and privacy. ACM, 2011,
pp. 27–38.

[38] B.-Z. He, C.-M. Chen, Y.-P. Su, and H.-M. Sun, “A defence scheme
against identity theft attack based on multiple social networks,” Expert
Systems with Applications, vol. 41, no. 5, pp. 2345–2352, 2014.
[39] Z. Yang, C. Wilson, X. Wang, T. Gao, B. Y. Zhao, and Y. Dai,
“Uncovering social network sybils in the wild,” ACM Transactions on
Knowledge Discovery from Data (TKDD), vol. 8, no. 1, p. 2, 2014.
[40] E. J. Williams, A. Beardmore, and A. N. Joinson, “Individual differences
in susceptibility to online inﬂuence: A theoretical review,” Computers
in Human Behavior, vol. 72, pp. 412–421, 2017.

[41] R. Dhamija, J. D. Tygar, and M. Hearst, “Why phishing works,” in
Proceedings of the SIGCHI conference on Human Factors in computing
systems. ACM, 2006.

[42] S. Sheng, M. Holbrook, P. Kumaraguru, L. F. Cranor, and J. Downs,
“Who falls for phish?: a demographic analysis of phishing susceptibility
the SIGCHI
and effectiveness of interventions,” in Proceedings of
Conference on Human Factors in Computing Systems. ACM, 2010,
pp. 373–382.

[43] J. S. Downs, M. B. Holbrook, and L. F. Cranor, “and susceptibility
to phishing,” in Proceedings of the second SOUPS. ACM, 2006, pp.
79–90.

[44] A. Vishwanath, “Habitual facebook use and its impact on getting de-
ceived on social media,” Journal of Computer-Mediated Communication,
vol. 20, no. 1, pp. 83–98, 2015.

[45] S. Egelman, L. F. Cranor, and J. Hong, “You’ve been warned: an em-
pirical study of the effectiveness of web browser phishing warnings,” in
Proceedings of the SIGCHI Conference on Human Factors in Computing
Systems. ACM, 2008, pp. 1065–1074.

[46] A. Karakasiliotis, S. Furnell, and M. Papadaki, “Assessing end-user
awareness of social engineering and phishing,” in AUSTRALIAN IN-
FORMATION WARFARE AND SECURITY CONFERENCE.
School
of Computer and Information Science, Edith Cowan University, Perth,
Western Australia, 2006.

[47] Z. Alqarni, A. Algarni, and Y. Xu, “Toward predicting susceptibility
to phishing victimization on facebook,” in Services Computing (SCC),
2016 IEEE International Conference on.

IEEE, 2016, pp. 419–426.

[48] M. Silic and A. Back, “The dark side of social networking sites:
Understanding phishing risks,” Computers in Human Behavior, vol. 60,
pp. 35–43, 2016.

[49] S. Baki, R. Verma, A. Mukherjee, and O. Gnawali, “Scaling and
effectiveness of email masquerade attacks: Exploiting natural language
generation,” in Proceedings of the ACM on Asia Conference on Com-
puter and Communications Security. ACM, 2017, pp. 469–482.

