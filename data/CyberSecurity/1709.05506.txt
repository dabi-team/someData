1
2
0
2

v
o
N
6
1

]
L
M

.
t
a
t
s
[

5
v
6
0
5
5
0
.
9
0
7
1
:
v
i
X
r
a

A statistical interpretation of spectral embedding:
the generalised random dot product graph

Patrick Rubin-Delanchy1, Joshua Cape2, Minh Tang3, and Carey E. Priebe4

1University of Bristol and Heilbronn Institute for Mathematical Research, U.K.
2University of Pittsburgh, U.S.A.
3North Carolina State University, U.S.A.
4Johns Hopkins University, U.S.A.

Abstract

Spectral embedding is a procedure which can be used to obtain vector representations of
the nodes of a graph. This paper proposes a generalisation of the latent position network
model known as the random dot product graph, to allow interpretation of those vector rep-
resentations as latent position estimates. The generalisation is needed to model heterophilic
connectivity (e.g., ‘opposites attract’) and to cope with negative eigenvalues more generally.
We show that, whether the adjacency or normalised Laplacian matrix is used, spectral embed-
ding produces uniformly consistent latent position estimates with asymptotically Gaussian
error (up to identiﬁability). The standard and mixed membership stochastic block models
are special cases in which the latent positions take only K distinct vector values, represent-
ing communities, or live in the (K − 1)-simplex with those vertices, respectively. Under the
stochastic block model, our theory suggests spectral clustering using a Gaussian mixture model
(rather than K-means) and, under mixed membership, ﬁtting the minimum volume enclosing
simplex, existing recommendations previously only supported under non-negative-deﬁnite as-
sumptions. Empirical improvements in link prediction (over the random dot product graph),
and the potential to uncover richer latent structure (than posited under the standard or mixed
membership stochastic block models) are demonstrated in a cyber-security example.

1 Introduction

While the study of graphs is well-established in Mathematics and Computer Science, it has only
more recently become mainstream in Statistics, a shift driven at least in part by the advent of
the Internet (Newman, 2018). Yet, despite its breadth, translating existing graph theory into
principled statistical procedures has produced many new mathematical challenges.

An example pertinent to this paper is the spectral clustering procedure (Von Luxburg, 2007).
This procedure, which aims to ﬁnd network communities, generally proceeds along the following
steps. Given an undirected graph, the corresponding adjacency or normalised Laplacian matrix is
ﬁrst constructed. Next, the graph is spectrally embedded into d dimensions by computing the d
principal eigenvectors of the matrix — in our case scaled according to eigenvalue — to obtain a
d-dimensional vector representation of each node. The ﬁrst scaled eigenvector can be thought to
provide the x-coordinate of each node, the second the y-coordinate, and so forth. Finally, these
points are input into a clustering algorithm such as K-means (Steinhaus, 1956; Lloyd, 1982) to
obtain communities. The most popular justiﬁcation for this algorithm, put forward by Shi and
Malik (2000) based on earlier work by Donath and Hoﬀman (1973); Fiedler (1973), is its solving
a convex relaxation of the normalised cut problem. A more principled statistical justiﬁcation was
ﬁnally found by Rohe et al. (2011), see also Lei and Rinaldo (2015), showing that the spectral
clustering algorithm provides consistent identiﬁcation of communities under the stochastic block
model (Holland et al., 1983). Their analysis however demands that eigenvectors corresponding
to the largest magnitude eigenvalues are used, countering earlier and contemporary papers which
recommend using only the positive.

1

 
 
 
 
 
 
The random dot product graph is a model which allows statistical interpretation of spectral
embedding as a standalone procedure, i.e., without the subsequent clustering step. Through this
broader view of spectral embedding, one ﬁnds that geometric analyses other than clustering are
also productive. For example, simplex-ﬁtting (Rubin-Delanchy et al., 2017) and spherical clustering
(Qin and Rohe, 2013; Lyzinski et al., 2014; Lei and Rinaldo, 2015; Sanna Passino et al., 2020), re-
spectively, are appropriate under the mixed membership (Airoldi et al., 2008) and degree-corrected
(Karrer and Newman, 2011) stochastic block models, and manifold-ﬁtting is appropriate under sev-
eral other random graph models (Athreya et al., 2021; Trosset et al., 2020; Rubin-Delanchy, 2020;
Whiteley et al., 2021). However, the random dot product graph has an important shortcoming,
addressed in this paper, which is to make a positive-deﬁnite assumption, consistent with the afore-
mentioned practice of retaining only the positive eigenvalues, that is problematic for modelling
several common types of graph connectivity structure.

The limitations of this positive-deﬁnite assumption are easy to identify using a stochastic block
model. In standard form, this model posits that there is a partition of the nodes into K com-
munities, conditional upon which the edges occur independently according to a symmetric inter-
community edge probability matrix B ∈ [0, 1]K×K, known as the block matrix. If the model is
extended to include degree correction (Karrer and Newman, 2011), those probabilities are subject
to nodewise scaling, so that for example a node i, of community 1, and a node j, of community 2,
form an edge with probability wiwjB12. If, and only if, the block matrix is non-negative-deﬁnite,
the random dot product graph can reproduce either model (Lyzinski et al., 2014). It will assign
a latent position Xi to each node i which, in the standard case, is precisely one of K possible
points, each representing a community. Under degree correction, the position instead lives on one
of K rays emanating from the origin, with (cid:107)Xi(cid:107)2 ∝ wi. In this way, nodes with larger magnitude
positions tend to have larger degree.

A positive-deﬁnite block matrix is said to reﬂect homophilic connectivity, in which ‘birds of a
feather ﬂock together’. But to encounter a block matrix that has negative eigenvalues is not un-
usual. With K = 2, negative eigenvalues will occur when B12 > B11, B22, e.g., under heterophilic
connectivity, and may occur when B11 > B12 > B22, e.g., under core-periphery connectivity — a
densely connected core, community 1, with sparsely connected periphery, community 2 (Borgatti
and Everett, 2000). In fact, when K > 2, negative eigenvalues may even occur when Bii > Bij
for each i (cid:54)= j, connectivity which could reasonably be considered homophilic. A reviewer gave the
example

B = 0.1 ×







 ,

9
0
8

0
6
5

8
5
9

which has one negative and two positive eigenvalues.

In a graph following a stochastic block model, the signs of the principal eigenvalues of the
adjacency and normalised Laplacian will correspond to those of B, up to noise.
In this way a
graph from a two-community stochastic block model with B12 > B11, B22 should present two
large-magnitude eigenvalues, one positive and one negative, with the others being close to zero.
To give a light-hearted real data example, consider the graph of enmities between Harry Potter
characters, a publically available dataset (Evans et al., 2014). The same graph was previously
studied by Mara et al. (2020), and more generally several literature studies involve analysis of
character networks (Labatut and Bost, 2019). A plot of the eigenvalues of the graph adjacency
matrix is shown in Figure 1. Two eigenvalues stand out in magnitude, one positive and one
negative, and for the purpose of this example the remaining will be treated as noise.

Figure 2 shows the adjacency spectral embedding of the graph into 2 dimensions (a formal
deﬁnition to follow, Deﬁnition 1), selecting eigenvectors corresponding to those eigenvalues. One
can discern two rays from the origin which, as those familiar with the story will know, distinguish
the good characters from the evil. This geometry is precisely what would be expected under a
two-community degree-corrected stochastic block model, however it falls outside the scope of the
random dot product graph because the second eigenvector, which gives the y-axis, has a negative
eigenvalue. Upon implementing spherical clustering (Lyzinski et al., 2014), we ﬁnd

ˆB ∝

(cid:18) 0.05
1

(cid:19)

,

1
0.09

2

Figure 1: Eigenvalues of the adjacency matrix of the graph of enmities between Harry Potter
characters. Of the two largest-magnitude eigenvalues, the ﬁrst is positive and the second negative,
and the corresponding eigenvectors are used for spectral embedding in Figure 2.

which has one positive and one negative eigenvalue, and the colours of the points in the ﬁgure
reﬂect the node partition thus obtained (community 1 in red, the ‘good’ characters; community 2
in green, the ‘evil’ characters). The block matrix B is not fully identiﬁable due to the presence of
nodewise scaling, but the inter-to-intra community ratios are. In this way, two ‘good’ characters
are estimated as 20 times less likely to be enemies than each would with someone ‘evil’, and a
similarly low level of enmity between ‘evil’ characters is observed (the diﬀerence is not signiﬁcant).
Moving beyond this toy example, a diversity of real-world graphs are surveyed in Section 5.1,
ﬁnding that half present important negative eigenvalues. Following this, a closer study of computer
network data is conducted in Section 5.2 giving physical reasons for heterophilic connectivity struc-
ture, and showing that including negative eigenvalues improves predictions. A model generalising
the random dot product graph to allow for negative eigenvalues is therefore called for.

Our proposed model has the generic structure of a latent position model (Hoﬀ et al., 2002),
in that each node i is posited to have a latent position Xi ∈ Rd, and two nodes i and j form an
edge, conditionally independently, with probability f (Xi, Xj), for some function f . A Generalised
Random Dot Product Graph (GRDPG) is a latent position model with f (x, y) = x(cid:62)Ip,qy, where
Ip,q = diag(1, . . . , 1, −1, . . . , −1), with p ones followed by q minus ones on its diagonal, and where
p ≥ 1 and q ≥ 0 are two integers satisfying p + q = d. When q = 0, the function f becomes
the usual inner product on Rd, and the model reduces to the standard random dot product graph
(Nickel, 2006; Young and Scheinerman, 2007; Athreya et al., 2017).

The core asymptotic ﬁndings of this paper (Theorems 3–7) mirror existing results for the ran-
dom dot product graph (Sussman et al., 2012; Lyzinski et al., 2014; Athreya et al., 2016; Lyzinski
et al., 2017; Cape et al., 2019a,b; Tang et al., 2018): Whether the adjacency or normalised Lapla-
cian matrix is used, the vector representations of nodes obtained by spectral embedding provide
uniformly consistent and asymptotically Gaussian latent position estimates (up to identiﬁability).
In this way, existing recommendations to ﬁt a Gaussian mixture model (rather than K-means
clustering) for spectral clustering under the stochastic block model (Athreya et al., 2016; Tang
et al., 2018), and simplex-ﬁtting under mixed membership (Rubin-Delanchy et al., 2017), previ-
ously justiﬁed only under non-negative-deﬁnite assumptions, now stand in general.

Other than the random dot product graph, there are several precursors to the GRDPG, most
notably the eigenmodel of Hoﬀ (2008) with kernel f (x, y) = Φ(µ+x(cid:62)Λy) (ignoring covariates), and
the proposed random dot product graph generalisation with kernel f (x, y) = x(cid:62)Λy by Rohe et al.
(2018), where Λ ∈ Rd×d is respectively diagonal or symmetric, µ is a scalar and Φ is the normal
distribution function. Negative eigenvalues in Λ allow us to model negative eigenvalues in the
graph adjacency matrix. To our best knowledge those models’ connection to spectral embedding is
unexplored. While the model by Rohe et al. (2018) evidently absorbs ours, the two are equivalent
up to identiﬁability, and we will discuss how our asymptotic results can be adapted to this larger
parameter space in Section 2.3. A contemporaneously written paper, Lei (2018), comes closer to
our work, proposing the kernel f (x, y) = (cid:104)x1, y1(cid:105)1 − (cid:104)x2, y2(cid:105)2, where x = (x1, x2) and y = (y1, y2)
live on the direct sum of two Hilbert spaces with respective inner products (cid:104)·, ·(cid:105)1 and (cid:104)·, ·(cid:105)2, and

3

01020304050−505IndexeigenvalueFigure 2: Adjacency spectral embedding into R2 of the graph of enmities between Harry Potter
characters. This pattern of two rays emanating from the origin is expected under a two-community
degree-corrected stochastic block model, and the points are coloured by their inferred community,
estimated using spherical clustering (Lyzinski et al., 2014). The embedding uses the eigenvectors
corresponding to the two largest-magnitude eigenvalues, of which the ﬁrst is positive and the second
negative, as shown in Figure 1.

4

Regulus Arcturus BlackSirius BlackLavender BrownCho ChangVincent CrabbeBartemius "Barty" Crouch Sr.Bartemius "Barty" Crouch Jr.Fleur DelacourCedric DiggoryAlbus DumbledoreDudley DursleyPetunia DursleyVernon DursleyArgus FilchCornelius FudgeGregory GoyleHermione GrangerRubeus HagridIgor KarkaroffBellatrix LestrangeFrank LongbottomNeville LongbottomRemus LupinDraco MalfoyLucius MalfoyNarcissa MalfoyMinerva McGonagallAlastor "Mad−Eye" MoodyPeter PettigrewHarry PotterJames PotterLily PotterQuirinus QuirrellTom Riddle Sr.Mary RiddleLord VoldemortRita SkeeterSeverus SnapeNymphadora TonksDolores Janes UmbridgeArthur WeasleyBill WeasleyCharlie WeasleyFred WeasleyGeorge WeasleyGinny WeasleyMolly WeasleyRon WeasleyDobbyFluffyAragog−1.0−0.50.00.51.00.00.30.60.91.2X^ (first dimension)X^ (second dimension)proving the consistency of adjacency spectral embedding in a form of Wasserstein distance. The
GRDPG is a special case where the Hilbert spaces are Rp and Rq, equipped with the Euclidean
inner product. The advantage of Lei’s analysis is to handle the inﬁnite-dimensional case; on
the other hand, our ﬁnite-dimensional results are stronger, so much that they lead to concrete
methodological recommendations that could not be made based only on Lei’s results. Those
include to use ﬁt a Gaussian mixture model (rather than K-means clustering) for spectral clustering
under the stochastic block model (calling on Theorems 4 and 7), and minimum volume simplex
ﬁtting under mixed membership (calling on Theorem 3). If the latent positions of the GRDPG
are independent and identically distributed (i.i.d.), as will be assumed in our asymptotic study,
the model also admits an Aldous-Hoover representation (Aldous, 1981; Hoover, 1979), wherein
each node is instead independently assigned a latent position uniformly on the unit interval, and
connections occur conditionally independently according to a kernel g : [0, 1]2 → [0, 1], known as
a graphon (Lov´asz, 2012). Conversely, an Aldous-Hoover graph follows a GRDPG if the integral
operator associated with g has ﬁnite rank (Lei, 2018; Rubin-Delanchy, 2020). If this operator has
negative eigenvalues, it cannot be reproduced by the random dot product graph or any other latent
position model with positive-deﬁnite kernel.

The rest of this article is organised as follows.

In Section 2, we formally describe the data
envisaged, the spectral embedding procedure, and the problem of ﬁnding a model-based rationale
for this approach. Then we propose our solution, a generalisation of the random dot product
graph model, discussing its identiﬁability and alternative parameterisations. Section 3 presents
our asymptotic results which, collected, say: spectral embedding provides uniformly consistent
estimates of the latent positions of a GRDPG, with asymptotically Gaussian error (up to identiﬁa-
bility). In Section 4, the implications of this theory for standard and mixed membership stochastic
block model estimation are discussed, namely, the advantages of ﬁtting a Gaussian mixture model
over K-means for spectral clustering under the stochastic block model, and the consistency of
minimum volume enclosing simplex ﬁtting under mixed membership. In Section 5 we review a
diversity of real-world graphs, showing that many exhibit important negative eigenvalues, before
focussing on a cyber-security application. Section 6 concludes. All proofs are relegated to the
Appendix.

2 The data, spectral embedding, and model

This paper concerns statistical inference based on a single, observed graph on n nodes, labelled
1, . . . , n. In conventional statistical terms, one may view the graph as ‘the data’, and its number
of nodes as a loose substitute for ‘sample size’. The graph is represented by its adjacency matrix
A ∈ {0, 1}n×n, where Aij = 1 if and only if there is an edge between the ith and jth node. The
graph is assumed to be undirected with no-self loops or, equivalently, A is symmetric (A = A(cid:62))
and hollow (Aii = 0 for all i).

To allow statistical analysis using mainstream methods (e.g., clustering), it is common to seek

a vector representation of each node, and spectral embedding is a popular tool for this purpose.

Deﬁnition 1 (Adjacency and Laplacian spectral embedding into Rd). Let ˆS be the d × d diag-
onal matrix containing the d largest eigenvalues of A in magnitude on its diagonal, arranged in
decreasing order (based on their actual, signed, value), and let ˆU ∈ Rn×d be a matrix containing,
as columns, corresponding orthonormal eigenvectors arranged in the same order. Deﬁne the ad-
jacency spectral embedding of the graph into Rd as the matrix ˆX = [ ˆX1, . . . , ˆXn](cid:62) = ˆU|ˆS|1/2 ∈
Rn×d, i.e., ˆX is a matrix whose ith row, transposed into a column vector, is ˆXi. Similarly, let
L = D−1/2AD−1/2 ∈ Rn×n denote the normalised Laplacian of the graph, where D ∈ Rn×n is the
degree matrix, a diagonal matrix with Dii = (cid:80)
j Aij for all i, and let ˘S, ˘U respectively denote the
corresponding matrices of largest-magnitude eigenvalues and associated eigenvectors. Deﬁne the
Laplacian spectral embedding of the graph into Rd by ˘X = [ ˘X1, . . . , ˘Xn](cid:62) = ˘U|˘S|1/2 ∈ Rn×d.

The problem considered in this paper is ﬁnding a model-based rationale for spectral embedding.
We seek a random graph model that deﬁnes true latent positions X1, . . . , Xn ∈ Rd such that
ˆXi provides an estimate of Xi with quantiﬁable error. This search will also yield a suitable
transformation of Xi that may be treated as the estimand of ˘Xi.

5

As alluded to in the introduction, a relatively large body of work exists, comprehensively re-
viewed in Athreya et al. (2017), addressing the same problem with the eigenvalues in Deﬁnition 1
selected by largest (signed) value, in other words, leaving out negative eigenvalues and correspond-
ing eigenvectors. To interpret such embeddings, a latent position model known as the random dot
product graph (Nickel, 2006; Young and Scheinerman, 2007) is put forward and, in this model,
an edge between two nodes occurs with probability given by the inner product of their latent
positions. However, such a model must result in a non-negative-deﬁnite edge probability matrix
Pij = X (cid:62)
i Xj, and cannot explain signiﬁcant negative eigenvalues in A, because the matrices are
related by E(A | P) = P, so that any diﬀerence between their spectra is due to noise.

Our solution is a model which generalises the random dot product graph, in specifying that the
probability of an edge between two nodes is given by the indeﬁnite inner product of their latent
positions. For two vectors x, y ∈ Rd, this product is x(cid:62)Ip,qy, where Ip,q is a diagonal matrix with
p ones followed by q minus ones on its diagonal, and p ≥ 1 and q ≥ 0 are two integers satisfying
p + q = d. A formal model deﬁnition is now given.

Deﬁnition 2 (Generalised random dot product graph model). Let X be a subset of Rd such
that x(cid:62)Ip,qy ∈ [0, 1] for all x, y ∈ X , and F a joint distribution on X n. We say that (X, A) ∼
GRDPG(F), with signature (p, q), if the following hold. First, let (X1, . . . , Xn) ∼ F, to form
the latent position matrix X = [X1, . . . , Xn](cid:62) ∈ Rn×d. Then, the graph adjacency matrix A ∈
{0, 1}n×n is symmetric, hollow and, conditional on X1, . . . , Xn,

Aij

ind∼ Bernoulli (cid:0)X (cid:62)

i Ip,qXj

(cid:1) ,

(1)

for all i < j.

2.1 Special cases

2.1.1 The stochastic block model

A graph follows a stochastic block model if there is a partition of the nodes into K communities,
ind∼ Bernoulli(BZiZj ), for i < j, where B ∈ [0, 1]K×K is symmetric and
conditional upon which Aij
Zi ∈ {1, . . . , K} is an index denoting the community of the ith node.

Let p ≥ 1, q ≥ 0 denote the number of strictly positive and strictly negative eigenvalues of
B respectively, put d = p + q, and choose v1, . . . , vK ∈ Rd such that v(cid:62)
k Ip,qvl = Bkl, for k, l ∈
{1, . . . , K}. We will take as a canonical choice the K rows of UB|ΣB|1/2, where ΣB is diagonal
containing the d non-zero eigenvalues of B, and B has spectral decomposition B = UBΣBU(cid:62)
B. It
may help to remember that p + q = d = rank(B) ≤ K. By letting Xi = vZi, we ﬁnd that the
graph is a GRDPG, and can set X = {v1, . . . , vK}.

2.1.2 The mixed membership stochastic block model

Now, assign (at random or otherwise) to the ith node a probability vector πi ∈ SK−1 where Sm
denotes the standard m-simplex. Conditional on this assignment, let

Aij

ind∼ Bernoulli (cid:0)BZi→j Zj→i

(cid:1) ,

where

Zi→j

ind∼ categorical(πi)

and Zj→i

ind∼ categorical(πj),

for i < j. The resulting graph is said to follow a mixed membership stochastic block model (Airoldi
et al., 2008).

Averaging over Zi→j and Zj→i, we can equivalently write that, conditional on π1, . . . , πn,

Aij

ind∼ Bernoulli (cid:0)π(cid:62)

i Bπj

(cid:1) .

But if p, q, d and v1, . . . , vK are as deﬁned previously, then π(cid:62)
X (cid:62)

i Ip,qXj, where Xi = (cid:80) πikvk. Therefore, conditional on X1, . . . , Xn, Equation (1) holds, and

k=1 πikv(cid:62)

k=1 πjkvk) =

i Bπj = ((cid:80)K

k )Ip,q((cid:80)K

6

Figure 3: Illustration of standard (S) and mixed membership (MM) stochastic block models as
special cases of the GRDPG model (Deﬁnition 2). The models have K = 3 communities and so the
corresponding GRPDG will require d = 3 dimensions (or fewer, if the block matrix has low rank).
The points v1, . . . , vK represent communities. Under the standard stochastic block model, the ith
node is assigned to a single community so that Xi ∈ {v1, . . . , vK}. Under mixed membership, if
the ith node has a community membership probability vector πi, then its position in latent space,
Xi, is the corresponding convex combination of v1, . . . , vK, that is, Xi = (cid:80)K

k=1 πikvk.

the graph is a GRDPG with latent positions X1, . . . , Xn. These live in the convex hull of v1, . . . , vK,
a (K − 1)-simplex if B has full rank (d = K).

The GRDPG model therefore gives the standard and mixed membership stochastic block models
a natural spatial representation in which v1, . . . , vK represent communities, and latent positions
in between them represent nodes with mixed membership. This is illustrated in Figure 3.

Airoldi et al. (2008) set π1, . . . , πn

+ . The corresponding
latent positions X1, . . . , Xn are then a) also i.i.d., and b) fully supported on the convex hull of
v1, . . . , vK. Consistency of simplex-ﬁtting, Algorithm 2 (Section 3) and illustrated in Figure 5e),
relies on these two points, without requiring a Dirichlet distribution assumption.

i.i.d.∼ Dirichlet(α) for some α ∈ RK

2.1.3 The degree-corrected stochastic block model

Instead, assign (at random or otherwise) to the ith node a weight wi ∈ [0, 1]. Conditional on this
assignment, let

ind∼ Bernoulli (cid:0)wiwjBZiZj
for i < j. The resulting graph is said to follow a degree-corrected stochastic block model (Karrer
and Newman, 2011).

Aij

(cid:1) ,

With p, q, d and v1, . . . , vK as deﬁned previously, a corresponding GRDPG is constructed by

letting Xi = wivZi, which lives on one of K rays emanating from the origin.

2.2 Identiﬁability

In the deﬁnition of the GRDPG, it is clear that the conditional distribution of A given X1, . . . , Xn
would be unchanged if X1, . . . , Xn were replaced by QX1, . . . , QXn, for any matrix Q ∈ O(p, q) =
: M(cid:62)Ip,qM = Ip,q}, known as the indeﬁnite orthogonal group. The vectors
{M ∈ Rd×d
X1, . . . , Xn are therefore identiﬁable from A only up to such transformation.

The property of identiﬁability up to orthogonal transformation, that is, by a matrix W ∈
O(d) = {M ∈ Rd×d : M(cid:62)M = I} is encountered in many statistical applications and occurs when
q = 0. This unidentiﬁability property often turns out to be moot since inter-point distances are

7

v1v2v3Xi (S)Xi (MM)Figure 4: Identiﬁability of the latent positions of a GRDPG with signature (1, 2). In each panel,
the three coloured points represent latent positions X1, X2 and X3, which live inside the cone
{x ∈ R3 : x(cid:62)I1,2x = 0} (grey mesh). The positions are only identiﬁable up to transformation
by a matrix in the indeﬁnite orthogonal group O(1, 2). This group includes some rotations (e.g.,
that used to go from the top-left to top-right panel), but also hyberbolic rotations (e.g., going
from top-left to bottom-left and top-right to bottom-right). The observed graph is equally likely
under those four latent position conﬁgurations, and so the conﬁgurations cannot be distinguished.
Some of these transformations aﬀect inter-point distances. In the bottom row, the blue position is
closer to the green on the left, whereas it is closer to the red on the right; the three positions are
equidistant in the top row.

invariant under the action of a common orthogonal transformation, and many statistical analyses
(such as K-means clustering) depend only on distance. When q > 0, the transformation is indeﬁnite
orthogonal and can aﬀect inter-point distances. This is illustrated in Figure 4 with a GRDPG of
signature (1, 2). The group O(1, 2) contains rotation matrices

but also hyperbolic rotations

rt =





1
0
0

0

0

cos t − sin t
cos t
sin t



 ,





ρθ =

cosh θ
sinh θ
0

sinh θ
cosh θ
0



 ,

0
0
1

as can be veriﬁed analytically. A rotation rπ/3 is applied to three GRDPG latent positions to
get from the top-left to the top-right panel in Figure 4. Hyperbolic rotations ρθ (θ = 1.3, chosen
arbitrarily) and ρ−θ take the positions from the top-left to the bottom-left and from the top-right
to the bottom-right panels, respectively. These transformations alter inter-point distances: in the
bottom row, the blue position is closer to the green on the left and closer to the red on the right;
the three positions are equidistant in the top row.

2.3 Alternative parameterisations

In this section we discuss alternative, equivalent parameterisations, explaining why we opted for
the GRDPG without claiming objective superiority.

It may be observed that the only ambiguity in computing the spectral embedding ˆX is how
the principal eigenvectors for A are chosen. If we assume repeated non-zero eigenvalues are rare
in real data, this choice in practice is typically limited to the option of reversing any eigenvector.

8

llllllllllllrp3rqr-qA model for interpreting spectral embedding might have been expected to reﬂect only this kind of
ambiguity in its unidentiﬁability.

The model structure can indeed be brought closer to the spectral decomposition of A by deﬁning
an alternative, ‘spectral’ estimand, ˜X = [ ˜X1, . . . , ˜Xn](cid:62) = U|S|1/2 ∈ Rn×d, where S ∈ Rd×d is a
diagonal matrix containing the non-zero eigenvalues of P = XIp,qX(cid:62), in decreasing order, and
U ∈ Rn×d contains corresponding orthonormal eigenvectors as columns. With the help of a follow-
on paper (Agterberg et al., 2020), the vector ˆXi will be found to estimate ˜Xi up to orthogonal
transformation, uniformly and with asymptotically Gaussian error, or, under a distinct eigenvalue
assumption, up to reﬂection of the axes (see Section 3.1).

The object ˜X has special structure which, for example, precludes its rows ˜X1, . . . , ˜Xn from being
i.i.d.; under a stochastic block model, the K unique vector values taken by ˜X1, . . . , ˜Xn, representing
the communities, cannot be determined from only B, because they depend on Z1, . . . , Zn — so, for
example, those K vectors may change as n grows. These are some reasons why we choose to model
˜X through X (whence P), rather than as a standalone object. However, a reader only interested
in estimating ˜Xi can ignore indeﬁnite orthogonal transformations, which only appear when we try
to relate ˜Xi to Xi.

A convention could be imposed on F to make Xi and ˜Xi converge to each other, up to orthogonal
transformation, and proposals to this eﬀect were made in a follow-on paper (Agterberg et al., 2020).
However, convergence is not fast enough, when it comes to the central limit theorem, to substitute
˜Xi by Xi and avoid indeﬁnite orthogonal transformations (see Section 3.2). Moreover, under
such a convention, the construction of vector representatives of the K communities under the
stochastic block model and extensions is more involved, compared to our canonical construction in
Section 2.1.1, and will vary depending on the distributions of Z1, . . . , Zn, degree-correction weights
w1, . . . , wn, and community membership probabilities π1, . . . , πn.

In the context of graph simulation, Rohe et al. (2018) proposed to generalise the random dot
product graph via a latent position model with kernel f (x, y) = x(cid:62)Λy, where Λ ∈ Rd×d is a
symmetric matrix. Alternatively, in the spirit of the eigenmodel of Hoﬀ (2008), one might consider
enforcing Λ to be diagonal. In either case, the model’s latent positions, Yi say, can be transformed
into the latent positions of an equivalent GRDPG with signature (p, q), via Xi = LYi, given a
decomposition Λ = L(cid:62)Ip,qL for some p, q (such as that proposed for B in Section 2.1.1). If Λ is
only assumed symmetric and the Yi restricted only to give valid probabilities (i.e. to belong to a
set Y in which x(cid:62)Λy ∈ [0, 1] for all x, y ∈ Y), then Yi are identiﬁable only up to invertible linear
transformation, since we can replace Y1, . . . , Yn with MY1, . . . , MYn and Λ with M−(cid:62)ΛM−1, for
any invertible matrix M, without changing the conditional distribution of A. If the model of Rohe
et al. (2018) is preferred, the results of Section 3.1 can be re-interpreted to say that ˆXi estimates
Yi up to invertible linear transformation, uniformly and with asymptotically Gaussian error: in the
theorems of Section 3.1 we would replace Q ˆXi with L−1Q ˆXi (assuming Λ has full rank), Xi with
Yi, updating the covariance matrices accordingly.

3 Asymptotics

This section describes the asymptotic statistical properties of GRDPG latent position estimates
obtained by spectral embedding.

3.1 Results for adjacency spectral embedding
In this section, the spectral estimates ˆX1, . . . , ˆXn are shown to converge to X1, . . . , Xn in two
standard statistical senses: uniformly, and with asymptotically Gaussian error. Analogous results
for Laplacian spectral embedding are given in Section 3.3.

For a given n, the latent positions are assumed to be independent and identically distributed.
As n → ∞, their distribution is either ﬁxed or, to produce a regime in which the average node
degree grows less than linearly in n, it is made to shrink. This is done by letting Xi = ρ1/2
n ξi, where
i.i.d.∼ F , for some distribution F on Rd, and allowing the cases ρn = 1 or ρn → 0 suﬃciently slowly.
ξi
The generic joint distribution F occurring in Deﬁnition 2 is therefore assumed to factorise into a
product of n identical marginal distributions that are equal to F up to scaling. The dimension

9

of the model, d, is assumed to have been chosen ‘economically’ in the sense that, for ξ ∼ F , the
second moment matrix ∆ = E(ξξ(cid:62)) ∈ Rd×d has full rank. Here d is viewed as ﬁxed and known,
so for simplicity we suppress d-dependent factors in the statements of our theorems. Our proofs,
however, keep track of d.

Since the average node degree grows as nρn, the cases ρn = 1 and ρn → 0 can be thought to
respectively produce dense and sparse regimes and ρn is called a sparsity factor. No algorithm
can produce uniformly consistent estimates of X1, . . . , Xn if the average node degree grows less
than logarithmically. Indeed, if one did, it could be used to break the information-theoretic limit
for perfect community recovery under the stochastic block model (Abbe, 2017). Our results hold
under polylogarithmic growth.

Recall that we have also deﬁned a ‘spectral’ estimand, ˜Xi, identiﬁable up to orthogonal rather
than indeﬁnite orthogonal transformation (see Section 2.3). To move between ˆXi, ˜Xi and Xi, we
introduce the transformations:

1. W(cid:63) ∈ O(d) (cid:84) O(p, q): a block orthogonal matrix of which the ﬁrst p × p block (respectively
second q × q block) aligns the ﬁrst p (respectively second q) columns of U with the ﬁrst
p (respectively second q) columns of ˆU, solving the two orthogonal Procrustes problems
independently (explicit construction in the Appendix).

2. QX ∈ O(p, q): an indeﬁnite orthogonal matrix solving X = ˜XQX.

We will ﬁnd that W(cid:63) ˆXi converges to ˜Xi and that Q ˆXi converges to Xi, where Q = Q(cid:62)

XW(cid:63).

Theorem 3 (Uniform consistency of adjacency spectral embedding). There exists a universal
constant c > 1 such that, provided the sparsity factor satisﬁes nρn = ω{log4c n},

max
i∈{1,...,n}

(cid:107)W(cid:63) ˆXi − ˜Xi(cid:107) = OP

(cid:17)

(cid:16) logc n
n1/2

;

max
i∈{1,...,n}

(cid:107)Q ˆXi − Xi(cid:107) = OP

(cid:17)

(cid:16) logc n
n1/2

.

We say that a random variable Y is OP(f (n)) if, for any positive constant c > 0 there exists
an integer n0 and a constant C > 0 (both of which possibly depend on c) such that for all n ≥ n0,
|Y | ≤ Cf (n) with probability at least 1 − n−c. We write that sequences an = ω(bn) when there
exist a positive constant C and an integer n0 such that an ≥ Cbn for all n ≥ n0 and an/bn → ∞.
As the graph grows, we will now look at a ﬁxed, ﬁnite subset of the nodes, indexed 1, . . . , m

without loss of generality, to obtain a central limit theorem on the corresponding errors.

Theorem 4 (Adjacency spectral embedding central limit theorem). Assume the same sparsity
n(Q ˆXi−Xi) =
conditions as Theorem 3. Conditional on Xi, for i = 1, . . . , m, the random vectors
√
X(W(cid:63) ˆXi − ˜Xi) converge in distribution to independent zero-mean Gaussian random vectors

nQ(cid:62)

√

with covariance matrix Σ(ξi) respectively, where

Σ(x) =

(cid:40)

Ip,q∆−1E[(x(cid:62)Ip,qξ)(1 − x(cid:62)Ip,qξ)ξξ(cid:62)]∆−1Ip,q
Ip,q∆−1E[(x(cid:62)Ip,qξ)ξξ(cid:62)]∆−1Ip,q

if ρn = 1,
if ρn → 0.

The matrix QX has, since the ﬁrst edition of this paper, been shown to converge (Agterberg
et al., 2020), in the sense that for each n one can construct an orthogonal matrix W(cid:63)(cid:63) such that
W(cid:63)(cid:63)QX → Q0 almost surely, where Q0 ∈ O(p, q) is a ﬁxed matrix, made explicit in Agterberg
et al. (2020). The second of these observed that this made a central limit theorem “up to orthogonal
nW(cid:63)(cid:63)(W(cid:63) ˆXi − ˜Xi)
transformation” possible: in the above, we can replace
and Σ(ξi) with Q−(cid:62)
0 . Moreover, if the eigenvalues of ∆Ip,q are distinct, the matrices W(cid:63)
and W(cid:63)(cid:63) can be taken to be diagonal with entries 1 or −1, reﬂecting that any eigenvector can be
reversed in the spectral decompositions of P and A.

n(Q ˆXi − Xi) with

0 Σ(ξi)Q−1

√

√

Remark 1 (Proof overview). Theorems 3 and 4 are proved in succession within a uniﬁed frame-
work. The proof begins with a collection of matrix perturbation decompositions which eventually
yield the relation

ˆU|ˆS|1/2 = U|S|1/2W(cid:63) + (A − P)U|S|−1/2W(cid:63)Ip,q + R

10

for some residual matrix R ∈ Rn×d. Appropriately manipulating the above display equation
subsequently yields the important identity

n1/2( ˆXW(cid:62)

(cid:63) QX − X) = n1/2(A − P)X(X(cid:62)X)−1Ip,q + n1/2RW(cid:62)

(cid:63) QX,

Theorem 3 is then established by bounding the maximum Euclidean row norm (equivalently, the
two-to-inﬁnity norm (Cape et al., 2019b)) of the right-hand side of the above display equation
suﬃciently tightly. Theorem 4 is established with respect to the same transformation Q by showing
that, conditional on the ith latent position, i.e., ith row of X, the classical multivariate central
limit theorem can be invoked for the ith row of the matrix n1/2(A − P)X(X(cid:62)X)−1Ip,q, whereas
the remaining residual term has vanishing two-to-inﬁnity norm. The technical tools involved
include a careful matrix perturbation analysis involving an inﬁnite matrix series expansion of ˆU,
probabilistic concentration bounds for (A−P)kU, 1 ≤ k ≤ log n, delicately passing between norms,
and indeﬁnite orthogonal matrix group considerations.

The joint proof of Theorems 3 and 4 captures the novel techniques and necessary additional
considerations for moving beyond random dot product graphs considered in previous work to
generalised random dot product graphs. The proofs of Theorem 6 and Theorem 7 (for Laplacian
spectral embedding), while laborious, follow mutatis mutandis by applying the aforementioned
proof considerations within the earlier work and context of the Laplacian spectral embedding limit
theorems proven in Tang et al. (2018). For this reason, we elect to state those theorems without
proof.

3.2 Discussion

Could we remove all notion of indeﬁnite orthogonal transformation from our results? The answer
is yes if we consider only the ‘spectral’ estimand, ˜Xi, but we have not found a way of describing
ˆXi as asymptotically “Gaussian with mean Xi”, where Xi are i.i.d., regardless of sparsity, without
invoking indeﬁnite orthogonal transformations. Among equivalent latent position distributions —
equal up to indeﬁnite orthogonal transformation (by push-forward) — we can choose F such that
W(cid:63)(cid:63) ˜Xi and Xi are asymptotically equal (Agterberg et al., 2020). However, the matrix QX does
not appear to converge faster than n−1/2. As a result, however we choose F , the error between
√
W(cid:63)(cid:63) ˜Xi and Xi may not vanish when scaled by

n.
It is perhaps remarkable how often the presence of indeﬁnite orthogonal transformation will
turn out not to matter. First, a follow-on inference procedure, e.g. for cluster analysis, may happen
to be invariant to such a transformation of its input data. A key example is ﬁtting a Gaussian
mixture model, which we will shortly discuss in more detail. Second, even if the follow-on procedure
is not invariant, it may still be consistent. Indeed, there is nothing in our results disputing the
consistency of spectral clustering using K-means clustering (Rohe et al., 2011). Our uniform
consistency result allows us to reprove this, and in the same movement prove the consistency
of simplex-ﬁtting (Section 4.2) under the mixed membership stochastic block model, given some
control on the behaviour of Q, which we now provide.

Lemma 5. The matrix QX has bounded spectral norm almost surely.

The same can be said of Q, since W∗ is orthogonal, and of Q−1 = Ip,qQ(cid:62)Ip,q.

Proof of Lemma 5. The matrices S and XIp,qX(cid:62) have common spectrum by deﬁnition which
is further equivalent to the spectrum of X(cid:62)XIp,q, since for any conformable matrices M1, M2,
spec(M1M2) = spec(M2M1), excluding zero-valued eigenvalues. By the law of large numbers,
(nρn)−1(X(cid:62)X) → E(ξξ(cid:62)) almost surely, and so (nρn)−1(X(cid:62)XIp,q) → E(ξξ(cid:62))Ip,q. It follows that
both (nρn)−1(cid:107)X(cid:62)X(cid:107) and (nρn)−1mini|Sii| converge to positive constants almost surely as n → ∞.
X|S|QX,
X(mini|Sii|)QX(cid:107) ≤ (cid:107)X(cid:62)X(cid:107), from which

Now for QX as in the hypothesis, with respect to Loewner order Q(cid:62)

X|S|QX = X(cid:62)X. Hence, mini|Sii|(cid:107)QX(cid:107)2 = (cid:107)Q(cid:62)

X(mini|Sii|I)QX ≤ Q(cid:62)

where Q(cid:62)
the claim follows.

Apart from converging slowly, the limiting QX depends on F . Thus, a typical situation where
the presence of indeﬁnite orthogonal transformation does matter is when comparing two graphs
under a null hypothesis in which the latent position distributions F1 (cid:54)= F2. In the Appendix, we

11

provide two, two-graph examples, following standard and degree-corrected stochastic block models
respectively, where the block matrices are equal, but the community sizes or degree distributions
diﬀer.

3.3 Results for Laplacian spectral embedding

Analogous results are now given for the case of Laplacian spectral embedding. Here, the estimand
is deﬁned as

Xi
j X (cid:62)
i Ip,qXj
a latent position normalised according to its expected degree. As before, the estimate ˘Xi will
only resemble its estimand after indeﬁnite orthogonal transformation by a matrix ˘Q ∈ O(p, q),
constructed in an analogous fashion to Q. To avoid more deﬁnitions and notation, we will forego
deﬁning a ‘spectral’ estimand, as we did with adjacency spectral embedding.

√(cid:80)

,

Theorem 6 (Uniform consistency of Laplacian spectral embedding). Assume the same sparsity
conditions as Theorem 3. Then,

max
i∈{1,...,n}

(cid:13)
(cid:13)
˘Q ˘Xi −
(cid:13)
(cid:13)

√(cid:80)

Xi
j X (cid:62)
i Ip,qXj

(cid:13)
(cid:13)
(cid:13)
(cid:13)

= OP

(cid:16) logc n
nρ1/2
n

(cid:17)

.

Theorem 7 (Laplacian spectral embedding central limit theorem). Assume the same sparsity
conditions as Theorem 3. Conditional on Xi, for i = 1, . . . , m, the random vectors

(cid:18)

˘Q ˘Xi −

nρ1/2
n

√(cid:80)

Xi
j X (cid:62)
i Ip,qXj

(cid:19)

,

converge in distribution to independent zero-mean Gaussian random vectors with covariance matrix
˘Σ(ξi) respectively, where

˘Σ(x) =






Ip,q ˘∆−1E

Ip,q ˘∆−1E

(cid:17) (cid:16)

(cid:26)(cid:16) x(cid:62)Ip,qξ(1−x(cid:62)Ip,qξ)
x(cid:62)Ip,qµ
(cid:17) (cid:16)

(cid:26)(cid:16) x(cid:62)Ip,qξ
x(cid:62)Ip,qµ

ξ
µ(cid:62)Ip,qξ −

ξ
µ(cid:62)Ip,qξ −
˘∆Ip,qx
2µ(cid:62)Ip,qx

(cid:17) (cid:16)

(cid:17) (cid:16)

˘∆Ip,qx
2µ(cid:62)Ip,qx

ξ
µ(cid:62)Ip,qξ −

ξ
µ(cid:62)Ip,qξ −
˘∆Ip,qx
2µ(cid:62)Ip,qx

(cid:17)(cid:62)(cid:27)

(cid:17)(cid:62)(cid:27)

˘∆Ip,qx
2µ(cid:62)Ip,qx

˘∆−1Ip,q,

if ρn = 1,

˘∆−1Ip,q

if ρn → 0,

and µ = E(ξ), ˘∆ = E

(cid:16) ξξ(cid:62)

µ(cid:62)Ip,qξ

(cid:17)

.

4

Implications for stochastic block model estimation

The asymptotic results of Section 3 justify the use of the following high-level algorithms (Athreya
et al., 2016; Tang et al., 2018; Rubin-Delanchy et al., 2017) for standard and mixed membership
stochastic block model estimation, previously only formally supported under non-negative-deﬁnite
assumptions on the block matrix B.

Algorithm 1 Fitting a stochastic block model (spectral clustering)

input adjacency matrix A, dimension d, number of communities K ≥ d

1: compute spectral embedding ˆX1, . . . , ˆXn of the graph into Rd
2: ﬁt a Gaussian mixture model (varying volume, shape, and orientation) with K components

return cluster centres ˆv1, . . . , ˆvK and community memberships ˆZ1, . . . , ˆZn

Where this algorithm diﬀers most signiﬁcantly from Rohe et al. (2011) is in the use of a Gaussian
mixture model over K-means clustering. In Section 4.1, we show why Theorems 4 and 7 would
recommend this modiﬁcation, with a pedagogical example.

To accomplish step 2 we have been employing the mclust algorithm (Fraley and Raftery, 1999),
which has a user-friendly R package. In step 1, either adjacency or Laplacian spectral embedding
can be used (see Deﬁnition 1). If the latter, the resulting node memberships can be interpreted as

12

alternative estimates of Z1, . . . , Zn but the output cluster centres should be treated as estimating
degree-normalised versions of v1, . . . , vK (see Section 3.3).

Algorithm 2 Fitting a mixed membership stochastic block model

input adjacency matrix A, dimension d, number of communities K = d
1: compute adjacency spectral embedding ˆX1, . . . , ˆXn of the graph into Rd
2: project the data onto the (d − 1)-dimensional principal hyperplane, to obtain points ˆY1, . . . , ˆYn,

and ﬁt the minimum volume enclosing K − 1-simplex, with vertices ˆv1, . . . , ˆvK

3: obtain barycentric coordinates ˆYi = (cid:80)K

k=1 ˆπilˆvk, for i = 1, . . . , n

return vertices ˆv1, . . . , ˆvK of the simplex, and estimated community membership probability
vectors ˆπ1, . . . , ˆπn

This algorithm is unchanged from Rubin-Delanchy et al. (2017), but to prove it is consistent
when B has negative eigenvalues requires Theorem 3 and Lemma 5. We provide a pedagogical
example in Section 4.2.

To ﬁt the minimum volume enclosing simplex in step 2, we have been using the algorithm by
Lin et al. (2016) and are grateful to the authors for providing code. Algorithm 2 can be extended
to the case K ≥ d by ﬁtting a minimum volume enclosing convex K-polytope, but for identiﬁability
one must then assume ˆv1, . . . , ˆvK are in convex position.

4.1 A two-community stochastic block model

In this section, we show why the central limit theorems (Theorems 4 and 7) would recommend
ﬁtting a Gaussian mixture model, rather than using K-means, for spectral clustering. To illustrate
ideas, we consider a two-community stochastic block model under which every node is indepen-
dently assigned to the ﬁrst or second community, with respective probabilities 0.2 and 0.8, and the
block matrix is

B(1) =

(cid:20) 0.02
0.03

(cid:21)

,

0.03
0.01

which has one positive and one negative eigenvalue. The two-dimensional adjacency spectral
embedding of a simulated graph on n = 2000 nodes is shown in Figure 5a). As we will see,
Theorem 4 provides a formal sense in which this embedding resembles data from a two-component
Gaussian mixture distribution. Figure 5b) shows how this model ﬁts the embedding, based on the
approximate maximum likelihood parameters found by the mclust algorithm (Fraley and Raftery,
1999). The estimated component assignment of each point is indicated by colouring, the empirical
component centres are shown as small circles and corresponding empirical 95% Gaussian contours
in dashed lines.

Following the construction of Section 2.1.1, the graph is a GRDPG with signature (1, 1), and
its latent positions Xi ∈ R2 are i.i.d. from a distribution F which places all its mass on two distinct
points, v1 and v2, with respective probabilities 0.2 and 0.8. The implication of Theorem 4 for this
example is that, conditional on Zi, the transformed embedding Q ˆXi is approximately distributed
as an independent Gaussian vector with centre vZi and covariance n−1/2Σ(vZi ), where Q ∈ O(1, 1);
or, together, the vectors Q ˆX1, . . . , Q ˆXn approximately follow a two-component Gaussian mixture
model.

As a result, the points ˆX1, . . . , ˆXn resemble data from a two-component Gaussian mixture
model which have been put through a random, data-dependent, linear transformation Q−1. Given
A, the cluster assignments, v1 and v2, we can compute Q (see Section 3.1). For the graph that we
simulated,

Q ≈

(cid:20) 1.05 −0.32
0.32 −1.05

(cid:21)

.

Figure 5c) shows the embedding, the transformed centres Q−1v1, Q−1v2 (crosses), and correspond-
ingly transformed 95% Gaussian contours (solid ellipses) predicted by Theorem 4 for comparison
with the empirical versions (circles and dashed ellipses) obtained from the Gaussian mixture model
ﬁt.

13

Figure 5: Spectral embedding and analysis of simulated graphs from the standard (SBM — top)
and mixed membership (MMSBM — bottom) stochastic block models. a) Adjacency spectral
embedding into R2 of a simulated graph with two communities (n = 2000 nodes); b) two-component
Gaussian mixture model (varying volume, shape, and orientation) ﬁt using the R package mclust,
coloured by estimated component assignment, with estimated component centres as circles and
95% Gaussian contours as dashed ellipses; c) two-component Gaussian mixture model predicted
by the asymptotic theory, with component centres shown as crosses and 95% Gaussian contours
shown as solid ellipses, overlaid onto the empirical centres (circles) and contours (dashed ellipses)
shown in b); d) adjacency spectral embedding into R3 of a simulated graph with 3 communities
(n = 5000 nodes); e) minimum volume enclosing simplex (dashed triangle) enclosing the two
principal components (points); f) theoretical simplex supporting the latent positions (solid triangle)
for comparison with the minimum volume enclosing simplex (dashed triangle). Detailed discussion
in Section 4.

14

a) Spectral embedding (SBM)b) Fitted Gaussian mixture modelc) Theoretical componentsd) Spectral embedding (MMSBM)e) Fitted simplexPC2f) Theoretical simplexIn practice, we typically observe only A and do not have access to Q, and indeed Algorithm 1
is suggesting we ﬁt a Gaussian mixture model to ˆX1, . . . , ˆXn, not Q ˆX1, . . . , Q ˆXn. Remarkably, by
doing the former, we are eﬀectively accomplishing the latter.

This is because, under a Gaussian mixture model, the value of the likelihood is unchanged if,
while the component weights are held ﬁxed, the data, component means and covariances are re-
spectively transformed as X → MX, µ → Mµ, Γ → MΓM(cid:62), where M is any indeﬁnite orthogonal
matrix, for the simple reason that | det(M)| = 1. For the maximum likelihood parameters obtained
from ˆX1, . . . , ˆXn, the maximum a posteriori probability assignment of the data indices to mixture
components, ˆZ1, . . . , ˆZn, is identical to that which would have been obtained from Q ˆX1, . . . , Q ˆXn.
The cluster centres which would have been obtained from Q ˆX1, . . . , Q ˆXn are just Qˆv1 and Qˆv2,
where ˆv1, ˆv2 are those actually obtained from ˆX1, . . . , ˆXn. The pairs provide identical estimates
of B(1) through ˆB
k I1,1ˆvl for k, l ∈ {1, 2}. In practice regularisation
parameters in the clustering method may yield results that are not invariant to indeﬁnite trans-
formation, but such eﬀects should be small for large n, especially taken alongside the additional
result, in Lemma 5, that the spectral norm of Q is almost surely bounded.

(1)
kl = (Qˆvk)(cid:62)I1,1(Qˆvl) = ˆv(cid:62)

As can be seen in Figure 5c), the clusters are elliptical rather than spherical, in theory and in
practice. For this reason, a clustering algorithm which favours spherical solutions might produce
inaccurate results. This is one issue with K-means clustering, often said to be implicitly ﬁtting a
Gaussian mixture model with equal-volume spherical components (Fraley and Raftery, 2002), and
indeed numerical studies (Athreya et al., 2016; Tang et al., 2018) show it has higher misclassiﬁcation
rate on the task of community separation. Another issue, perhaps more a limitation of our own
framework, is that Euclidean distance is not identiﬁable under the GRDPG, and so the empirical
distances (cid:107) ˆXi − ˆXj(cid:107) are not easily understood through our approach. While our results say
enough to prove K-means clustering is consistent (a fact already established by other methods),
the algorithm is not invariant: the clusterings obtained from ˆX1, . . . , ˆXn and from Q ˆX1, . . . , Q ˆXn
are diﬀerent (although they agree asymptotically).

4.2 A three-community mixed membership stochastic block model

In this section, we show how uniform consistency (Theorem 3) guarantees the consistency of
simplex-ﬁtting for mixed membership stochastic block model estimation. To illustrate ideas, we
consider a three-community mixed membership stochastic block model under which every node
is ﬁrst independently assigned a 3-dimensional probability vector πi ∼ Dirichlet(1, 0.5, 0.5), for
i = 1, . . . , n, and the block matrix is

B(2) =





0.6
0.9
0.9



 ,

0.9
0.6
0.9

0.9
0.9
0.3

which has one positive and two negative eigenvalues. The three-dimensional adjacency spectral
embedding of a simulated graph on n = 5000 nodes is shown Figure 5d); this point cloud resembles
a ‘noisy simplex’. Figure 5e) shows the minimum volume 2-simplex (dashed lines) enclosing the
two principal components of the points. As we will see, Theorem 3 provides a formal sense in
which the point cloud becomes denser and sharper as n grows, so that this simplex converges.

Following the construction of Section 2.1.2, the graph is a GRDPG with signature (1, 2) and
its latent positions Xi ∈ R3 are i.i.d. from a distribution F supported on the simplex with vertices
v1, v2, v3 ∈ R3.

Since logc n/n1/2 → 0, Theorem 3 states that the maximum error between any Q ˆXi and Xi
vanishes, across i ∈ {1, . . . , n}, and this has wide-reaching implications for geometric analysis
since, as sets, Q ˆX1, . . . , Q ˆXn and X1, . . . , Xn are asymptotically equal in Hausdorﬀ distance. In
particular, for our example, where Q ∈ O(1, 2), the point set Q ˆX1, . . . , Q ˆXn, like X1, . . . , Xn,
converges in Hausdorﬀ distance to the simplex with vertices v1, v2, v3. This simplex would be
consistently estimated by the minimum volume simplex enclosing Q ˆX1, . . . , Q ˆXn, projected onto
their 2-dimensional principal hyperplane, by the argument presented in Rubin-Delanchy et al.
(2017).

We must just verify that the actual procedure, i.e., applied to ˆX1, . . . , ˆXn, is consistent de-
spite indeﬁnite orthogonal transformation. The bounded spectral norm of Q−1 (Lemma 5 and

15

discussion) guarantees that

max
i∈{1,...,n}

(cid:107) ˆXi − Q−1Xi(cid:107) ≤ (cid:107)Q−1(cid:107) max

i∈{1,...,n}

(cid:107)Q ˆXi − Xi(cid:107) = OP

(cid:17)

(cid:16) logc n
n1/2

,

through which one can replace Xi by Q−1Xi in the argument of Rubin-Delanchy et al. (2017).
The minimum volume simplex enclosing ˆY1, . . . , ˆYn (the points ˆX1, . . . , ˆXn projected onto their
2-dimensional principal hyperplane, see Algorithm 2), is then found to converge to the simplex
with vertices Q−1v1, Q−1v2, Q−1v3. This simplex is shown in Figure 5f) (solid lines).

As with the stochastic block model, the presence of indeﬁnite orthogonal transformation in
the estimated vertices ˆv1, ˆv2, ˆv3 is immaterial for estimating B through ˆB
k I1,2ˆvl for k, l ∈
{1, 2, 3}. Moreover, the community membership probability vectors ˆπ1, . . . , ˆπn, obtained as the
barycentric coordinates of ˆY1, . . . , ˆYn with respect to ˆv1, ˆv2, ˆv3, are the same as those which would
be obtained from Q ˆY1, . . . , Q ˆYn with respect to Qˆv1, Qˆv2, Qˆv3.

(2)
kl = ˆv(cid:62)

5 Real data

5.1 A collection of real-world graphs

Allowing a principled treatment of negative eigenvalues increases the scope of application of spectral
embedding. To gain an impression of the signiﬁcance of our extension, we conduct a survey of
graphs from a variety of application domains. Graphs with about 5,000 nodes were chosen from each
of the domain categories of a comprehensive online network repository (networkrepository.com),
selecting the largest if all graphs in a category were smaller, and rejecting the category if all graphs
were much larger.

For each of the resulting 24 graphs, an estimated embedding dimension ˆd was obtained using
proﬁle likelihood (Zhu and Ghodsi, 2006), and ˆp (respectively, ˆq) were estimated as the number of
positive (respectively, negative) eigenvalues among the largest ˆd in magnitude. Results are shown
in Table 1, and it happens that precisely half of the graphs have ˆq > 0. Moreover, the smallest
negative eigenvalue often ranks among the largest in magnitude.

5.2 Detailed example: link prediction on a computer network

Cyber-security applications often involve data with a network structure, for example, data relating
to computer network traﬃc (Neil et al., 2013a), the underground economy (Li and Chen, 2014),
and the internet-of-things (Hewlett Packard Enterprise research study, 2015). In the ﬁrst example,
a concrete reason to seek to develop an accurate network model is to help identify intrusions on
the basis of anomalous links (Neil et al., 2013b; Heard and Rubin-Delanchy, 2016).

Figure 6 shows, side by side, graphs of the communications made between computers on the
Los Alamos National Laboratory network (Kent, 2016), over a single minute on the left, and ﬁve
minutes on the right. The graphs were extracted from the “network ﬂow events” dataset, by
mapping each IP address to a node, and recording an edge if the corresponding two IP addresses
are observed to communicate at least once over the speciﬁed period.

Neither graph contains a single triangle, i.e., three nodes all connecting to each other. This
is a symptom of a broader property, known as heterophily or disassortivity (Khor, 2010), that
similar nodes are relatively unlikely to connect. In computer networks, such behaviour might be
expected for a number of reasons, including the common server/client networking model and the
physical location of routers (where collection happens) (Rubin-Delanchy et al., 2016). The random
dot product graph is unsuited to modelling heterophilic connectivity patterns. For example, any
two-community stochastic block model with lower on- than oﬀ- diagonal elements is out of scope.
The eigenvalues of the adjacency matrix of the 5-minute graph are plotted in Figure 7, showing an
abundance of negative eigenvalues which, again, cannot be modelled by the random dot product
graph (apart from as noise).

The modelling improvement oﬀered by the GRDPG over the random dot product graph is now
demonstrated empirically, through out-of-sample link prediction. For the observed 5-minute graph,
we estimate the GRDPG latent positions via adjacency spectral embedding, as in Deﬁnition 1, and

16

Graph category

nodes

edges

ˆd

ˆp

ˆq

animal social
benchmark (BHOSLIB)
benchmark (DIMACS 10)
benchmark (DIMACS)
biological
brain
cheminformatics
collaboration
communication
ecology
economic
email
infrastructure
interaction
molecular
power
proximity
retweet
road
router
social (advogato)
social (facebook)
structural mechanics
web

1,686
4,000
4,096
4,000
4,413
1,781
125
4,158
1,899
128
4,008
1,133
4,941
1,266
5,110
5,300
410
5,248
2,642
2,113
6,551
5,372
5,489
4,767

5,324
7,425,226
12,264
4,000,268
108,818
33,641
282
13,422
61,734
2,106
8,188
5,451
6,594
6,451
10,532
13,571
2,765
6,394
3,303
6,632
51,332
279,191
143,300
37,375

9
2
6
3
23
6
12
1
54
47
4
28
2
1
16
22
17
25
11
13
46
6
12
12

9
1
6
2
11
6
12
1
27
47
2
25
1
1
16
22
17
20
9
13
46
5
6
10

0
1
0
1
12
0
0
0
27
0
2
3
1
0
0
0
0
5
2
0
0
1
6
2

Table 1: A collection of real-world graphs. ˆd: the estimated dimension, or approximate rank of
the adjacency matrix, computed using proﬁle likelihood (Zhu and Ghodsi, 2006); ˆp (respectively,
ˆq): the number of positive (respectively, negative) eigenvalues of the adjacency matrix among the
ﬁrst ˆd. The estimate ˆq is non-zero for half of these graphs; these negative components can hold
important signal, that our generalisation of the random dot product graph allows us to model.

Figure 6: Los Alamos National Laboratory computer network. Graphs of the connections made
between diﬀerent computers (IP addresses) over the ﬁrst minute (left) and ﬁrst ﬁve minutes (right)
of the “network ﬂow events” dataset (Kent, 2016). Neither graph contains a single triangle, a motif
which would be expected in abundance under homophily (‘a friend of my friend is my friend’),
suggesting the need to relax this modelling assumption.

17

llllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllFigure 7: Eigenvalues of the adjacency matrix of the ﬁve-minute connection graph of computers on
the Los Alamos National Laboratory network, showing roughly equal contribution, in magnitude,
from the positive and negative eigenvalues.

the random dot product graph latent positions using an analogous procedure that retains instead
only the largest eigenvalues and corresponding eigenvectors.
In both cases, we choose d = 10
(admittedly arbitrarily) as the embedding dimension.

Whereas we have found that certain analysis techniques (e.g. ﬁtting a Gaussian mixture model)
are automatically adapted to the GRDPG signature, a concrete estimate of the signature is required
here, and p, q are respectively estimated to be the number of positive and negative eigenvalues
among the top d, in magnitude.

To compare the models, we then attempt to predict which new edges will occur in the next ﬁve-
minute window, a task known as link prediction, disregarding those involving new nodes. Figure 8
shows the receiver operating characteristic (ROC) curves for each model, treating the prediction
task as a classiﬁcation problem where the presence or absence of an edge is encoded as an instance
of the positive or negative class, respectively, and predicted by thresholding the inner product or
indeﬁnite inner product of the relevant pair of estimated latent positions. By presenting estimated
classiﬁcation performance (true positive versus false positive rate) at all possible thresholds (which
give diﬀerent points along the curve), the ROC allows a direct comparison that is independent of
the potentially diﬀerent ranges and scales of the two inner products. For this prediction problem,
the GRDPG model is far superior.

What does the GRDPG model add over the mixed membership or standard stochastic block
models? For large real-world networks, the latter models are often too simplistic, whereas the
GRDPG model and the statistical investigation thereof, as presented in this paper, provide a more
broadly applicable, principled starting point for analyses when low-dimensional latent generative
structure is supposed. To illustrate this, we construct the full graph of connections between
computers on the Los Alamos National Laboratory network, comprising roughly 12 thousand nodes
and one hundred thousand edges. As before, the nodes are spectrally embedded into R10, but these
are now visualised in two ways. First, we ﬁt a Gaussian mixture model with ten components, as
is consistent with a stochastic block model assumption with K = d = 10. Each of the ten panels
in Figure 9 shows the two principal components of one of the inferred clusters in a faithful aspect
ratio. Because every communication has an associated port number indicating the type of service
being used, for example port 80 corresponds to web activity and port 25 to email, this information
can be used to colour the nodes according to their most commonly employed port. The embedding,
obtained using only connectivity data, is clearly highly associated with port activity, so that the
geometry that can be distinguished appears to be somehow predictive of nodes’ behaviour. At the
same time the clusters, for the most part, do not appear to follow a Gaussian or ﬁnite mixture
of Gaussian distributions, as predicted under a stochastic block model. A diﬀerent view of the
data is obtained using t-distributed stochastic neighbour embedding (Maaten and Hinton, 2008)
in Figure 10, again showing high association with port activity. Taken together, these views of
the data reveal complex structure in low-dimensional pseudo-Euclidean latent space, for which the
GRDPG provides a preferrable starting point for statistical analysis to the random dot product
graph, the mixed membership, or standard stochastic block model.

18

0200400600800100012001400IndexEigenvalue−15015Figure 8: Receiver Operating Characteristic curves comparing the random dot product graph
(RDPG) and GRDPG at the task of link prediction on the Los Alamos National Laboratory
computer network. The nodes are embedded into R10 based on their graph of connections over the
ﬁrst 5-minute window, using either the largest positive (RDPG) or largest magnitude (GRDPG)
eigenvalues. The performance of these embeddings is then evaluated for predicting new edges
over the next 5-minute window, using the matrix of pairwise inner products (RDPG) or pairwise
indeﬁnite inner products as estimated edge probabilities. The ROC curves for each embedding
is presented, with the presence or absence of an edge encoded as an instance of the positive or
negative class, respectively, and thresholding the estimated edge probabilities to obtain diﬀerent
points along the curve.

Figure 9: Visualisation of the full graph of connections between computers on the Los Alamos
National Laboratory network, using adjacency spectral embedding into R10, followed by ﬁtting a
Gaussian mixture model with 10 components. Each of ten clusters obtained is shown in a panel
(two principal components) and the colour of each point represents the corresponding node’s most
commonly employed port (loosely representing the connection purpose, e.g. web, email), showing
association with the structure observed in the embedding. The structure is richer than the standard
or mixed membership stochastic block models would predict.

19

0.00.20.40.60.81.00.00.20.40.60.81.0Link predictionFalse positive rateTrue positive rateGRDPGRDPGllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllPC1lllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllPC1PC2llllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllPC1PC2lllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllPC1PC2llllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllPC1PC2lllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllPC2llllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllPC2llllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllPC2lllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllPC2Figure 10: Alternative visualisation the full graph of connections between computers on the Los
Alamos National Laboratory network, using adjacency spectral embedding into R10 followed by t-
distributed stochastic neighbour embedding. The colour of each point represents the corresponding
node’s most commonly employed port (loosely representing the connection purpose, e.g. web,
email), showing association with the structure observed in the embedding.

6 Conclusion

This paper presents the generalised random dot product graph, a latent position model which
includes the stochastic block model, its extensions, and the random dot product graph as special
cases. The key feature that is added by the generalisation is the possibility of modelling non-
homophilic connectivity behaviour, e.g., where ‘opposites attract’.

This model provides an appropriate statistical framework for interpreting spectral embedding.
This is substantiated in several theoretical results that together show that the vector represen-
tations of nodes obtained by spectral embedding provide uniformly consistent latent position es-
timates with asymptotically Gaussian error. A byproduct of this theory is to add insight and
methodological improvements to the estimation of community structure in networks, and practical
applications are demonstrated in a cyber-security example.

References

Abbe, E. (2017). Community detection and stochastic block models: recent developments. The

Journal of Machine Learning Research, 18(1):6446–6531.

Agterberg, J., Tang, M., and Priebe, C. E. (2020). On two distinct sources of nonidentiﬁability in

latent position random graph models. arXiv preprint arXiv:2003.14250.

Airoldi, E. M., Blei, D. M., Fienberg, S. E., and Xing, E. P. (2008). Mixed membership stochastic

blockmodels. Journal of Machine Learning Research, 9(Sep):1981–2014.

Aldous, D. J. (1981). Representations for partially exchangeable arrays of random variables. Jour-

nal of Multivariate Analysis, 11(4):581–598.

Athreya, A., Fishkind, D. E., Tang, M., Priebe, C. E., Park, Y., Vogelstein, J. T., Levin, K.,
Lyzinski, V., and Qin, Y. (2017). Statistical inference on random dot product graphs: a survey.
The Journal of Machine Learning Research, 18(1):8393–8484.

20

llllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllAthreya, A., Priebe, C. E., Tang, M., Lyzinski, V., Marchette, D. J., and Sussman, D. L. (2016).
A limit theorem for scaled eigenvectors of random dot product graphs. Sankhya A, 78(1):1–18.

Athreya, A., Tang, M., Park, Y., and Priebe, C. E. (2021). On estimation and inference in latent

structure random graphs. Statistical Science, 36(1):68–88.

Bhatia, R. (1997). Matrix Analysis. Springer.

Borgatti, S. P. and Everett, M. G. (2000). Models of core/periphery structures. Social networks,

21(4):375–395.

Cape, J., Tang, M., and Priebe, C. E. (2019a). Signal-plus-noise matrix models: eigenvector

deviations and ﬂuctuations. Biometrika, 106(1):243–250.

Cape, J., Tang, M., Priebe, C. E., et al. (2019b). The two-to-inﬁnity norm and singular subspace
geometry with applications to high-dimensional statistics. The Annals of Statistics, 47(5):2405–
2439.

Donath, W. E. and Hoﬀman, A. J. (1973). Lower bounds for the partitioning of graphs. IBM

Journal of Research and Development, 17(5):420–425.

Erd˝os, L., Knowles, A., Yau, H.-T., and Yin, J. (2013). Spectral statistics of Erd˝os-R´enyi’ graphs

I: Local semicircle law. The Annals of Probability, 41:2279–2375.

Evans, C., Friedman, J., Karakus, E., and Pandey, J. (2014). Potterverse. https://github.com/

efekarakus/potter-network/.

Fiedler, M. (1973). Algebraic connectivity of graphs. Czechoslovak mathematical

journal,

23(2):298–305.

Fraley, C. and Raftery, A. E. (1999). Mclust: Software for model-based cluster analysis. Journal

of classiﬁcation, 16(2):297–306.

Fraley, C. and Raftery, A. E. (2002). Model-based clustering, discriminant analysis, and density

estimation. Journal of the American statistical Association, 97(458):611–631.

Gallier, J. H. (2000). Curves and surfaces in geometric modeling: theory and algorithms. Morgan

Kaufmann.

Heard, N. A. and Rubin-Delanchy, P. (2016). Network-wide anomaly detection via the Dirichlet
process. In Proceedings of IEEE workshop on Big Data Analytics for Cyber-security Computing.

Hewlett Packard Enterprise research study (2015).

Internet of things: research study. http:

//h20195.www2.hpe.com/V4/getpdf.aspx/4aa5-4759enw.

Hoﬀ, P. (2008). Modeling homophily and stochastic equivalence in symmetric relational data. In

Advances in neural information processing systems, volume 20, pages 657–664.

Hoﬀ, P. D., Raftery, A. E., and Handcock, M. S. (2002). Latent space approaches to social network

analysis. Journal of the American Statistical Association, 97(460):1090–1098.

Holland, P. W., Laskey, K. B., and Leinhardt, S. (1983). Stochastic blockmodels: First steps.

Social networks, 5(2):109–137.

Hoover, D. N. (1979). Relations on probability spaces and arrays of random variables. Preprint,

Institute for Advanced Study, Princeton, NJ.

Karrer, B. and Newman, M. E. (2011). Stochastic blockmodels and community structure in net-

works. Physical Review E, 83(1):016107.

Kent, A. D. (2016). Cybersecurity data sources for dynamic network research. In Dynamic Net-

works and Cyber-Security. World Scientiﬁc.

21

Khor, S. (2010). Concurrency and network disassortativity. Artiﬁcial life, 16(3):225–232.

Labatut, V. and Bost, X. (2019). Extraction and analysis of ﬁctional character networks: A survey.

ACM Computing Surveys (CSUR), 52(5):1–40.

Lei, J. (2018).

Network representation using graph root distributions.

arXiv preprint

arXiv:1802.09684 (to appear in the Annals of Statistics).

Lei, J. and Rinaldo, A. (2015). Consistency of spectral clustering in stochastic block models. The

Annals of Statistics, 43(1):215–237.

Li, W. and Chen, H. (2014). Identifying top sellers in underground economy using deep learning-
In Intelligence and Security Informatics Conference (JISIC), 2014

based sentiment analysis.
IEEE Joint, pages 64–67. IEEE.

Lin, C.-H., Chi, C.-Y., Wang, Y.-H., and Chan, T.-H. (2016). A fast hyperplane-based minimum-
volume enclosing simplex algorithm for blind hyperspectral unmixing. IEEE Transactions on
Signal Processing, 64(8):1946–1961.

Lloyd, S. (1982). Least squares quantization in PCM. IEEE Transactions on Information Theory,

28(2):129–137.

Lov´asz, L. (2012). Large networks and graph limits. American Mathematical Society Colloquium

Publications, volume 60. Amer. Math. Soc. Providence, RI.

Lu, L. and Peng, X. (2013). Spectra of edge-independent random graphs. Electronic Journal of

Combinatorics, 20(4).

Lyzinski, V., Sussman, D. L., Tang, M., Athreya, A., and Priebe, C. E. (2014). Perfect clustering
for stochastic blockmodel graphs via adjacency spectral embedding. Electron. J. Stat., 8(2):2905–
2922.

Lyzinski, V., Tang, M., Athreya, A., Park, Y., and Priebe, C. E. (2017). Community detection
and classiﬁcation in hierarchical stochastic blockmodels. IEEE Transactions on Network Science
and Engineering, 4(1):13–26.

Maaten, L. v. d. and Hinton, G. (2008). Visualizing data using t-SNE. Journal of machine learning

research, 9(Nov):2579–2605.

Mao, X., Sarkar, P., and Chakrabarti, D. (2017). Estimating mixed memberships with sharp

eigenvector deviations. Arxiv preprint at http://arxiv.org/abs/1709.00407.

Mara, A., Mashayekhi, Y., Lijﬃjt, J., and De Bie, T. (2020). CSNE: Conditional signed network

embedding. arXiv preprint arXiv:2005.10701.

Neil, J. C., Hash, C., Brugh, A., Fisk, M., and Storlie, C. B. (2013a). Scan statistics for the online

detection of locally anomalous subgraphs. Technometrics, 55(4):403–414.

Neil, J. C., Uphoﬀ, B., Hash, C., and Storlie, C. (2013b). Towards improved detection of attackers in
computer networks: New edges, fast updating, and host agents. In 6th International Symposium
on Resilient Control Systems (ISRCS), pages 218–224. IEEE.

Newman, M. (2018). Networks: an introduction. Oxford university press.

Nickel, C. (2006). Random Dot Product Graphs: A Model for Social Networks. PhD thesis, Johns

Hopkins University.

Qin, T. and Rohe, K. (2013). Regularized spectral clustering under the degree-corrected stochastic

blockmodel. Adv. Neural. Inf. Process. Syst., 26:3120–3128.

Rohe, K., Chatterjee, S., and Yu, B. (2011). Spectral clustering and the high-dimensional stochastic

blockmodel. The Annals of Statistics, 39(4):1878–1915.

22

Rohe, K., Tao, J., Han, X., and Binkiewicz, N. (2018). A note on quickly sampling a sparse matrix

with low rank expectation. The Journal of Machine Learning Research, 19(1):3040–3052.

Rubin-Delanchy, P. (2020). Manifold structure in graph embeddings. In Proceedings of the Thirty-

fourth Conference on Neural Information Processing Systems.

Rubin-Delanchy, P., Adams, N. M., and Heard, N. A. (2016). Disassortivity of computer networks.

In Proceedings of IEEE workshop on Big Data Analytics for Cyber-security Computing.

Rubin-Delanchy, P., Priebe, C. E., and Tang, M. (2017). Consistency of adjacency spectral em-
bedding for the mixed membership stochastic blockmodel. arXiv preprint arXiv:1705.04518.

Sanna Passino, F., Heard, N. A., and Rubin-Delanchy, P. (2020). Spectral clustering on spherical
coordinates under the degree-corrected stochastic blockmodel. arXiv preprint arXiv:2011.04558.

Sarkar, P., Bickel, P. J., et al. (2015). Role of normalization in spectral clustering for stochastic

blockmodels. The Annals of Statistics, 43(3):962–990.

Shi, J. and Malik, J. (2000). Normalized cuts and image segmentation. IEEE Transactions on

pattern analysis and machine intelligence, 22(8):888–905.

Steinhaus, H. (1956). Sur la division des corp mat´eriels en parties. Bulletin L’Acad´emie Polonaise

des Sciences, 1(804):801.

Sussman, D. L., Tang, M., Fishkind, D. E., and Priebe, C. E. (2012). A consistent adjacency
spectral embedding for stochastic blockmodel graphs. Journal of the American Statistical Asso-
ciation, 107(499):1119–1128.

Tang, M., Cape, J., and Priebe, C. E. (2017). Asymptotically eﬃcient estimators for stochastic
blockmodels: the naive MLE, the rank-constrained MLE, and the spectral. arXiv preprint at
http://arxiv.org/abs/1710.10936.

Tang, M., Priebe, C. E., et al. (2018). Limit theorems for eigenvectors of the normalized laplacian

for random graphs. The Annals of Statistics, 46(5):2360–2415.

Trosset, M. W., Gao, M., Tang, M., and Priebe, C. E. (2020). Learning 1-dimensional submanifolds

for subsequent inference on random dot product graphs. arXiv preprint arXiv:2004.07348.

Von Luxburg, U. (2007). A tutorial on spectral clustering. Statistics and Computing, 17(4):395–

416.

Whiteley, N., Gray, A., and Rubin-Delanchy, P. (2021). Matrix factorisation and the interpretation

of geodesic distance. arXiv preprint arXiv:2106.01260.

Young, S. J. and Scheinerman, E. R. (2007). Random dot product graph models for social net-
works. In International Workshop on Algorithms and Models for the Web-Graph, pages 138–149.
Springer.

Yu, Y., Wang, T., and Samworth, R. J. (2015). A useful variant of the Davis–Kahan theorem for

statisticians. Biometrika, 102(2):315–323.

Zhu, M. and Ghodsi, A. (2006). Automatic dimensionality selection from the scree plot via the

use of proﬁle likelihood. Computational Statistics & Data Analysis, 51(2):918–930.

23

A Indeﬁnite orthogonal transformations in practice

In the next two sections, we respond to the possible argument that concerns about distortion by
an indeﬁnite orthogonal Q arise only as an artifact of the GRDPG formalism; or, at least, that
such concerns are of little relevance if sole interest in spectral embedding is to allow inference for
stochastic block models.

A.1 Distortion under the stochastic block model

A practical experiment that reveals the presence of indeﬁnite orthogonal transformation implicit in
spectral embedding is to simulate graphs from two stochastic block models that diﬀer only in their
community proportions. As in Section 4.1 we will use K = 2 and the probability matrix B(1), with
community proportions now set to (0.5, 0.5) and (0.05, 0.95) respectively, as opposed to (0.2, 0.8).
Resulting spectral embeddings on n = 4000 nodes are shown in the left-hand panel of Figure 11,
in orange and purple respectively. While each exhibits two clusters, there is very little overlap
between the orange and purple point clouds. This discrepancy is almost entirely due to distortion
by indeﬁnite orthogonal transformation. Since in simulation Q can be identiﬁed, by inversion the
indeﬁnite orthogonal transformation that takes us from the purple to the orange point cloud can
be found, and those conforming point clouds are shown in the right-hand panel. The centres of
corresponding clusters are now in much closer agreement, and remaining discrepancy is down to
statistical error. One cannot make sense of the geometric relationship between the two point clouds
without the notion of an indeﬁnite orthogonal transformation.

If an indeﬁnite orthogonal transformation is applied to ˆX before clustering using K-means (with
Euclidean distance), a diﬀerent partition of the points is obtained. One might have hoped that the
spectral decomposition would produce an embedding somehow optimally conﬁgured for clustering
using Euclidean K-means, but there is no obvious statistical argument to prefer ˆX. For example,
within-class variance was previously used to compare spectral embeddings under the stochastic
block model (Sarkar et al., 2015), but both the empirical (cluster assignment estimated) and oracle
(cluster assignment known) within-class variances are minimised for a diﬀerent, non-degenerate
conﬁguration. There are no such concerns with Gaussian mixture modelling, which is invariant.
The arguments here support, but go beyond, previous analyses showing the suboptimality of K-
means clustering versus Gaussian mixture modelling under the non-negative-deﬁnite stochastic
block model (Tang et al., 2018), since in that special case K-means clustering is at least invariant.

A.2 Distortion under the degree-corrected stochastic block model

In the ﬁrst case we set wi

ind∼ Beta(1, 5) when Zi = 1 and wi

Two graphs on n = 5000 nodes are now simulated from a two-community degree-corrected stochas-
tic block model, with block matrix B(1), and community proportions (0.5, 0.5), changing only
i.i.d∼ uniform[0, 1], whereas in the second
the degree distributions.
ind∼ Beta(5, 1) when Zi = 2. Resulting spectral embeddings
wi
into R2 are shown in the left-hand panel of Figure 12, in orange and purple respectively. Because
B(1) has one positive and one negative eigenvalue, our theory predicts that each point cloud should
live close to the union of two rays through the origin whose joint conﬁguration is predicted only
up to indeﬁnite orthogonal transformation in O(1, 1). In particular the hyperbolic angle between
the two rays

arcosh(v(cid:62)

1 I1,1v2),

is a population quantity that subject to regularity conditions (e.g. as given in Theorem 3) can
be estimated consistently and is not dependent on the node weights. This would make a natural
measure of distance between the communities and, in general, when p = 1 and q ≥ 1 the point cloud
i I1,q ˆXi) equipped with distance d(x, y) = arcosh(x(cid:62)I1,qy) is an embedding into hyperbolic
ˆXi/( ˆX (cid:62)
space that accounts for degree heterogeneity. Accordingly, the hyperbolic angle between the two
orange rays and that between the two purple rays are equal in the right panel of Figure 12. On
the other hand the Euclidean (or ordinary) angle arcos(u(cid:62)
1 u2), where u1, u2 are unit-norm vectors
on each ray, is visibly diﬀerent between the two point clouds.

24

Figure 11: A practical manifestation of Q. Two graphs are generated from a two-community
stochastic block model with block matrix B(1), n = 4000 nodes, and respective community pro-
portions (0.5, 0.5) (orange) and (0.05, 0.95) (purple). Left: adjacency spectral embedding into R2,
with circles (respectively crosses) indicating the cluster centres of the orange (respectively purple)
point cloud. Right: the purple point cloud is re-conﬁgured to align with the orange point cloud, by
reverting the indeﬁnite orthogonal transformation associated with the former, and then applying
that corresponding to the latter. The dotted lines show the orbits along which the cluster centres
were moved, and the grey crosses their original position. The cluster centres of both point clouds
(black circles and crosses) are now close.

25

0.00.10.20.3−0.2−0.10.00.10.2Spectral embeddingsX^ (first dimension)X^ (second dimension)0.00.10.20.3−0.2−0.10.00.10.2Conforming embeddingsX^ (first dimension)X^ (second dimension)Figure 12: Distortion of angle under the degree-corrected stochastic block model. Two graphs are
generated from a two-community degree-corrected stochastic block model with probability matrix
B(1), n = 5000 nodes, community proportions (0.5, 0.5), and uniform (orange) versus diﬀering
Beta-distributed (purple) weights. Left: adjacency spectral embedding into R2. In each case the
point cloud is a noisy observation of two rays through the origin, shown with orange and purple
lines respectively. Right: Euclidean (dashed) and hyperbolic (dotted) angles between the rays.
The two hyperbolic angles are identical but the two Euclidean angles diﬀer.

26

0.00.10.20.30.40.5−0.20.00.20.4Spectral embeddingsX^ (first dimension)X^ (second dimension)0.00.10.20.30.40.5−0.20.00.20.4Circular versus hyperbolic angleX^ (first dimension)X^ (second dimension)B Uniqueness

There are a number of reasonable alternative latent position models which, broadly described,
assign the nodes to elements X1, . . . , Xn of a set X and, with this assignment held ﬁxed, set

Aij

ind∼ Bernoulli {f (Xi, Xj)} ,

for i < j, where f : X 2 → [0, 1] is some symmetric function. For example, Hoﬀ et al. (2002)
considered the choice f (x, y) = logistic (α − (cid:107)x − y(cid:107)). What is special about the GRDPG?

One argument for considering the GRDPG is that it provides essentially the only way of faith-
fully reproducing mixtures of connectivity probability proﬁles as convex combinations in latent
space. This idea is now made formal.

Property 8 (Reproducing mixtures of connectivity probability proﬁles). Suppose that X is a
convex subset of a real vector space, and that S is a subset of X whose convex hull is X . We say
that a symmetric function f : X 2 → [0, 1] reproduces mixtures of connectivity probability proﬁles
from S if, whenever x = (cid:80)

r αrur, where ur ∈ S, 0 ≤ αr ≤ 1 and (cid:80) αr = 1, we have

f (x, y) =

(cid:88)

r

αrf (ur, y),

for any y in X .

This property helps interpretation of latent space. For example, suppose X1, . . . , X4 ∈ S, and
X1 = 1/2X2 + 1/2X3. In a latent position model where f satisﬁes the above, we can either think
ind∼ Bernoulli {f (X1, X4)}, or by ﬁrst ﬂipping a
of A14 as being directly generated through A14
coin, and generating an edge with probability f (X2, X4) if it comes up heads, or with probability
f (X3, X4) otherwise.

In choosing a latent position model to represent the mixed membership stochastic block model,
it would be natural to restrict attention to kernels satisfying Property 8, since they allow the simplex
representation illustrated in Figure 3, with vertices S = {v1, . . . , vK} representing communities,
and latent positions within it reﬂecting the nodes’ community membership preferences.

We now ﬁnd that in ﬁnite dimension, any such choice amounts to a GRDPG model in at most

one extra dimension:

Theorem 9. Suppose X is a subset of Rl, for some l ∈ N. The function f reproduces mixtures of
connectivity probability proﬁles if and only if there exist integers p ≥ 1, q ≥ 0, d = p + q ≤ l + 1, a
matrix T ∈ Rd×l, and a vector ν ∈ Rd so that f (x, y) = (Tx + ν)(cid:62)Ip,q(Ty + ν), for all x, y ∈ X .

The mixed membership stochastic block model is an example where this additional dimension
is required: in Figure 3 the model is represented as a GRDPG model in d = 3 dimensions, but the
latent positions live on a 2-dimensional subset.

B.1 Proof of Theorem 9

Let aﬀ(C) denote the aﬃne hull of a set C ⊆ Rd,

aﬀ(C) =

(cid:40) n
(cid:88)

i=1

αiui; n ∈ N, ui ∈ C, αi ∈ R,

(cid:41)

αi = 1

.

n
(cid:88)

i=1

We say that a function g : Rd × Rd → R is a bi-aﬃne form if it is an aﬃne function when either
argument is ﬁxed, i.e., g{λx1 + (1 − λ)x2, y} = λg(x1, y) + (1 − λ)g(x2, y) and g{x, λy1 + (1 −
λ)y2} = λg(x, y1) + (1 − λ)g(x, y2), for any x, y, x1, x2, y1, y2 ∈ Rd, λ ∈ R. We say that a function
h : Rd × Rd → R is a bilinear form if it is bi-aﬃne and h(x, y) = 0 if either argument is zero.

The proof of Theorem 9 is a direct consequence of the following two lemmas.

Lemma 10. Suppose X is a convex subset of Rl, for some l ∈ N. Then f reproduces mixtures of
connectivity probability proﬁles on S if and only if it can be extended to a symmetric bi-aﬃne form
g : aﬀ(X ) × aﬀ(X ) → R.

27

Lemma 11. Suppose g : aﬀ(X ) × aﬀ(X ) → R is a bi-aﬃne form. Let (cid:96) = dim{aﬀ(X )} ≤ l. Then
there exist a matrix R ∈ R((cid:96)+1)×l, a vector µ ∈ R(cid:96)+1, and a bilinear form h : R((cid:96)+1) × R((cid:96)+1) → R
such that g(x, y) = h(Rx + µ, Ry + µ), for all x, y ∈ aﬀ(X ).

As is well-known, because h is a symmetric bilinear form on a ﬁnite-dimensional real vector
space, it can be written h(x, y) = x(cid:62)Jy where J ∈ R((cid:96)+1)×((cid:96)+1) is a symmetric matrix. Write
J = VdSdV(cid:62)
d where Vd ∈ R((cid:96)+1)×d has orthonormal columns, Sd ∈ Rd×d is diagonal and has
p ≥ 0 positive followed by q ≥ 0 negative eigenvalues on its diagonal, and d = p + q = rank(J).
Next, deﬁne M = Vd|Sd|1/2. Then,

f (x, y) = g(x, y) = h(Rx + µ, Ry + µ)

= {M(Rx + µ)}(cid:62) Ip,q {M(Ry + µ)} = (Tx + ν)(cid:62)Ip,q(Ty + ν),

where T = MR and ν = Mµ. Since f (x, x) ≥ 0 on X × X , we must have p > 0 unless f is
uniformly zero over X × X .

Proof of Lemma 10. The “if” part of the proof is straightforward. Here, we prove the “only if”.
By deﬁnition, any x, y ∈ aﬀ(X ) = aﬀ(S) can be written x = (cid:80) αrur, y = (cid:80) βrvr where ur, vr ∈ S,
αr, βr ∈ R, and (cid:80) αr = (cid:80) βr = 1. For any such x, y, we deﬁne g(x, y) = (cid:80)

Suppose that (cid:80) αrur = (cid:80) γrtr, (cid:80) βrvr = (cid:80) δrwr where tr, wr ∈ S, γr, δr, ∈ R, and (cid:80) γr =
r = (cid:80) γ(cid:48)
(cid:80) δr = 1. Rearrange the ﬁrst equality to (cid:80) α(cid:48)
rt(cid:48)
r by moving any αrur term where αr < 0
to the right — so that the corresponding new coeﬃcient is α(cid:48)
s = −αr, for some s — and any γrtr
term where γr < 0 to the left, so that the corresponding new coeﬃcient is γ(cid:48)
s = −γr, for some s.
Both linear combinations now involve only non-negative scalars. Furthermore, (cid:80) αr = (cid:80) γr (= 1)
implies (cid:80) α(cid:48)

r,s αrβsf (ur, vs).

ru(cid:48)

r are two convex combinations, therefore,

Then, (cid:80)(α(cid:48)

r = (cid:80) γ(cid:48)
r/c)u(cid:48)

r = c, for some c ≥ 0.
r = (cid:80)(γ(cid:48)
r/c)t(cid:48)
(cid:110)(cid:88)

(cid:88)

(α(cid:48)

r/c)f (u(cid:48)

r, v) = f

(α(cid:48)

r/c)u(cid:48)

r, v

(cid:111)

= f

(cid:110)(cid:88)

(γ(cid:48)

r/c)t(cid:48)

r, v

(cid:111)

=

(cid:88)

(γ(cid:48)

r/c)f (t(cid:48)

r, v),

for any v ∈ S, so that (cid:80) αrf (ur, v) = (cid:80) γrf (tr, v). Therefore,

(cid:88)

r,s

αrβsf (ur, vs) =

(cid:88)

βs

s

(cid:88)

=

γr

(cid:40)

(cid:88)

r

(cid:40)

(cid:88)

γrf (tr, vs)

(cid:41)

(cid:41)

βsf (vs, tr)

=

r

s

(cid:88)

r,s

γrδsf (tr, ws),

so that g is well-deﬁned. The function g is symmetric and it is also clear that g{λx1+(1−λ)x2, y} =
λg(x1, y) + (1 − λ)g(x2, y) for any λ ∈ R, making it bi-aﬃne by symmetry.

The proof technique now used is known as the homogenisation trick in geometry (Gallier, 2000).

Proof of Lemma 11. Let x0, x1, . . . , x(cid:96) ∈ Rl be an aﬃne basis of aﬀ(X ). Then there exists an aﬃne
transformation x → Rx+µ, mapping x0 to z0 = (0, . . . , 0, 1) ∈ R(cid:96)+1, x1 to z1 = (1, 0, . . . , 0, 1), and
so forth, ﬁnally mapping x(cid:96) to z(cid:96) = (0, . . . , 0, 1, 1), where R ∈ R((cid:96)+1)×l and µ ∈ R(cid:96)+1. The vectors
z0, . . . , z(cid:96) form a basis of R(cid:96)+1, so that if we set h(zi, zj) = g(xi, xj) for 0 ≤ i, j ≤ (cid:96), then the value
h is well-deﬁned over R((cid:96)+1) × R((cid:96)+1) by bilinearity and basis expansion. Since any x, y ∈ aﬀ(X )
r=0 βrxr where αr, βr ∈ R, and (cid:80) αr = (cid:80) βr = 1, we have
can be written x = (cid:80)(cid:96)

r=0 αrxr, y = (cid:80)(cid:96)

g(x, y) =

(cid:88)

αrβsg(xr, xs) =

(cid:88)

αrβsh(zr, zs)

r,s
(cid:16)(cid:88)

= h

αrzr,

(cid:88)

βrzr

r,s
(cid:17)

= h(Rx + µ, Ry + µ).

28

C Proof of Theorems 3 and 4

Broadly speaking, extending prior results on adjacency spectral embedding from the random dot
product graph to the GRDPG requires new methods of analysis, that together represent the main
technical contribution of this paper (mainly Theorems 3 and 4). Further extending results to the
case of Laplacian spectral embedding, while mathematically involved, follows mutatis mutandis
the machinery developed in Tang et al. (2018). Analogous Laplacian-based results (Theorems 6
and 7) are therefore stated without proof.

C.1 Preliminaries

This proof synthesizes and adapts both the proof architecture and machinery developed in the
papers Tang et al. (2017); Cape et al. (2019b,a). We invoke the probabilistic concentration phe-
nomena for GRDPGs presented in Lemma 7 of Tang et al. (2018) as well as an eigenvector matrix
series decomposition concisely analyzed in Cape et al. (2019a). This proof is not, however, a trivial
corollary of earlier results, for it requires additional technical considerations and insight.

We recall the setting of Theorems 3 and 4 wherein the rows of X are independent replicates
n ξ, ξ ∼ F , and for i < j the ij-th entries of A are independent Bernoulli
i Ip,qXj. Here we shall allow self-loops for mathematical convenience

of the random vector ρ1/2
random variables with mean X (cid:62)
since (dis)allowing self-loops is immaterial with respect to the asymptotic theory we pursue.

Now for P = XIp,qX(cid:62), denote the low-rank spectral decomposition of P by P = USU(cid:62), where
U ∈ On,d and S ∈ Rd×d. Write U ≡ [U(+)|U(−)] with U(+) ∈ On,p and U(−) ∈ On,q to indicate
the orthonormal eigenvectors corresponding to the p positive and q negative non-zero eigenvalues of
(cid:76) S(−) ∈ R(p+q)×(p+q) ≡ Rd×d. Denote the
P, written in block-diagonal matrix form as S = S(+)
⊥, where ˆU ∈ On,d denotes the matrix
full spectral decomposition of A by A = ˆUˆS ˆU
of leading (orthonormal) eigenvectors of A and ˆS ∈ Rd×d denotes the diagonal matrix containing
(cid:62)
the d largest-in-magnitude eigenvalues of A arranged in decreasing order. Here, the matrix ˆUˆS ˆU
corresponds to the best canonical rank d representation of A. Also above, write ˆU ≡ [ ˆU(+)| ˆU(−)]
such that the columns of ˆU(+) and ˆU(−) consist of orthonormal eigenvectors corresponding to the
largest p positive and q negative non-zero eigenvalues of A, respectively.

+ ˆU⊥ ˆS⊥ ˆU

(cid:62)

(cid:62)

We remark at the onset that for the GRDPG model, asymptotically almost surely (cid:107)U(cid:107)2→∞ =
O(n−1/2) and |Sii|, |ˆSii| = Θ((nρn)) for each i = 1, . . . , d. Simultaneously, (cid:107)A−P(cid:107) = OP((nρn)1/2)
(regarding the latter, see for example Lu and Peng (2013); Lei and Rinaldo (2015)).

Before going into the details of the proof, we ﬁrst show that U(cid:62) ˆU is suﬃciently close to an
orthogonal matrix W∗ with block diagonal structure that is simultaneously an element of O(p, q).
To this end, the matrix U(cid:62) ˆU can be written in block form as

U(cid:62) ˆU =

(cid:104) U(cid:62)
U(cid:62)

(+)

(−)

ˆU(+) U(cid:62)
ˆU(+) U(cid:62)

(+)

(−)

(cid:105)

ˆU(−)
ˆU(−)

∈ Rd×d,

(2)

where U(cid:62)

ˆU(+) ∈ Rp×p, U(cid:62)
(+)

ˆU(−) ∈ Rp×q, U(cid:62)
(−)
Write the singular value decomposition of U(cid:62)

(+)

(+)

and deﬁne the orthogonal matrix W(cid:63)
(+) := W(+),1W(cid:62)
the orthogonal (product) matrix corresponding to U(cid:62)
orthogonal matrix

ˆU(−) ∈ Rq×q.

ˆU(+) ∈ Rq×p, and U(cid:62)
(−)
ˆU(+) ≡ W(+),1Σ(+)W(cid:62)
ˆU(+) ∈ Rp×p as U(cid:62)
(+)
(+),2 ∈ Op. Similarly, let W(cid:63)

(+),2,
(−) ∈ Oq denote
ˆU(−). Now let W(cid:63) denote the structured

(−)

W(cid:63) =

(cid:104) W(cid:63)

0
(+)
0 W(cid:63)

(−)

(cid:105)

∈ Od.

(3)

Observe that W(cid:63)Ip,qW(cid:62)
(cid:63) = Ip,q, hence simultaneously W(cid:63) ∈ O(p, q). Via the triangle inequality,
the spectral norm quantity (cid:107)U(cid:62) ˆU−W(cid:63)(cid:107) is bounded above by four times the largest spectral norm
of its blocks. The main diagonal blocks can be analyzed in a straightforward manner via canonical
angles and satisfy

(cid:107)U(cid:62)

(+)

ˆU(+) − W(cid:63)

(+)(cid:107), (cid:107)U(cid:62)
(−)

ˆU(−) − W(cid:63)

(−)(cid:107) = OP((nρn)−1).

(4)

ˆU(+). Then σi = cos(θi) where
More speciﬁcally, let σ1, σ2, . . . , σp be the singular values of U(cid:62)
(+)
θi are the principal angles between the subspaces spanned by U(+) and ˆU(+). The deﬁnition of

29

W(+) implies

(cid:107)U(cid:62)

(+)

ˆU(+)−W(cid:63)

(+)(cid:107)F = (cid:107)Σ(+)−I(cid:107)F =

(cid:16)

p
(cid:88)

i=1

(1−σi)2(cid:17)1/2

≤

p
(cid:88)

i=1

(1−σ2

i ) = (cid:107)U(+)U(cid:62)

(+)− ˆU(+)

ˆU

(cid:62)
(+)(cid:107)2
F .

By the Davis-Kahan sin Θ theorem (see e.g. Section VII.3 of Bhatia (1997) or Yu et al. (2015)),
we have

(cid:107)U(cid:62)

(+)

ˆU(+) − W(cid:63)

(+)(cid:107)F ≤ (cid:107)U(+)U(cid:62)

(+) − ˆU(+)

ˆU

(cid:62)
(+)(cid:107)2

F ≤

C(cid:107)A − P(cid:107)2

λp(P)2 = OP((nρn)−1),

where λp(P) is the smallest positive eigenvalue of P. The bound (cid:107)U(cid:62)
(−)
is derived similarly.

ˆU(−)−W(cid:63)

(−)(cid:107) = OP((nρn)−1)

We now bound the quantities (cid:107)U(cid:62)

ˆU(−)(cid:107). Let ui,(+) and ˆuj,(−) be arbitrary columns of
U(+) and ˆU(−), respectively. Note that the ij-th entry of U(cid:62)
ˆU(−) is (ui,(+))(cid:62) ˆuj,(−) and that
(+)
λi,(+)(ui,(+))(cid:62) ˆuj,(−) = (ui,(+))(cid:62)P ˆuj,(−), ˆλj,(−)(ui,(+))(cid:62) ˆuj,(−) = (ui,(+))(cid:62)A ˆuj,(−) where λi,(+)
(resp. ˆλj,(−)) is the i-th (resp. j-th) largest in modulus positive eigenvalue (resp. negative eigen-
value) of P (resp. A). We therefore have

(+)

(u(+)
i

)(cid:62) ˆu(−)

j = (ˆλj,(−) − λi,(+))−1(ui,(+))(cid:62)(A − P) ˆuj,(−)

= (ˆλj,(−) − λi,(+))−1(ui,(+))(cid:62)(A − P)U(−)U(cid:62)
+ (ˆλj,(−) − λi,(+))−1(ui,(+))(cid:62)(A − P)(I − U(−)U(cid:62)

(−) ˆuj,(−)

(−)) ˆuj,(−).

The term (ui,(+))(cid:62)(A−P)U(−) is a vector in Rq, and conditional on P, each element of (ui,(+))(cid:62)(A−
P)U(−) can be written as a sum of independent random variables. Hence, by Hoeﬀding’s in-
equality, (cid:107)(ui,(+))(cid:62)(A − P)U(−)(cid:107) = OP(log n). Furthermore, by the Davis-Kahan theorem,
(cid:107)(I − U(−)U(cid:62)

(−)) ˆuj,(−)(cid:107) = OP((nρn)−1/2). We therefore have

(cid:107)(ˆλj,(−) − λi,(+))−1(ui,(+))(cid:62)(A − P)U(−)U(cid:62)
(cid:107)(ˆλj,(−) − λi,(+))−1(ui,(+))(cid:62)(A − P)(I − U(−)U(cid:62)

(−) ˆuj,(−)(cid:107) = OP((nρ−1

n ) log n);
(−)) ˆuj,(−)(cid:107) = OP((nρn)−1).

Equations (5) and (6) together imply

(cid:107)U(cid:62)

(+)

ˆU(−)(cid:107) = OP((nρn)−1(log n)),

thus (cid:107)U(cid:62) ˆU − W(cid:63)(cid:107) = OP((nρn)−1(log n)).

(5)

(6)

(7)

C.2 Proof details
We now proceed with the proof of Theorem 3 and Theorem 4. The matrix relation ˆUˆS = A ˆU =
(P + (A − P)) ˆU yields the matrix equation ˆUˆS − (A − P) ˆU = P ˆU. The spectra of ˆS and A − P
are disjoint asymptotically almost surely, so ˆU can be written as a matrix series of the form (see
e.g. Theorem VII.2.1 and Theorem VII.2.2 of Bhatia (1997))

∞
(cid:88)

ˆU =

(A − P)kP ˆUˆS

−(k+1)

∞
(cid:88)

=

(A − P)kUSU(cid:62) ˆUˆS

−(k+1)

.

(8)

k=0

k=0

30

By scaling the matrix ˆU by |ˆS|1/2, observing that ˆS = Ip,q|ˆS|, and applying a well-thought-out
“plus zero” trick, we arrive at the decomposition

ˆU|ˆS|1/2 =

=

∞
(cid:88)

k=0
∞
(cid:88)

k=0

(A − P)kUSU(cid:62) ˆUIk+1

p,q |ˆS|−k−1/2

(A − P)kUIp,q|S|−k+1/2W(cid:63)Ik+1
p,q

∞
(cid:88)

(A − P)kUIp,q|S|−k+1/2(U(cid:62) ˆU − W(cid:63))Ik+1
p,q

k=0
∞
(cid:88)

(A − P)kUS(U(cid:62) ˆUIk+1

p,q |ˆS|−k−1/2 − |S|−k−1/2U(cid:62) ˆUIk+1
p,q )

+

+

k=0

:= V1 + V2 + V3.

C.2.1 The matrix V1

Diagonal matrices commute, as do the matrices Ip,q and W(cid:63), so V1 can be written as

∞
(cid:88)

V1 ≡

(A − P)kU|S|−k+1/2W(cid:63)Ik+2

p,q = U|S|1/2W(cid:63) + (A − P)U|S|−1/2W(cid:63)Ip,q + RV1 ,

(9)

k=0

where RV1 = (cid:80)∞
Lemma 7.10 from Erd˝os et al. (2013). This result was also noted in Mao et al. (2017).

k=2(A − P)kU|S|−k+1/2W(cid:63)Ik+2

p,q . We now use the following slight restatement of

Lemma 12. Assume the setting and notations in Theorem 3. Let uj be the j-th column of U for
j = 1, 2, . . . , d. Then there exists a (universal) constant c > 1 such that for all k ≤ log n

(cid:107)(A − P)kU(cid:107)2→∞ ≤ d1/2 max
j∈[d]

(cid:107)(A − P)kuj(cid:107)∞ = OP

(cid:16) (nρn)k/2 logkc(n)
n1/2

(cid:17)

.

Thus, for c > 0 as above,

(cid:107)RV1(cid:107)2→∞ ≤

=

log n
(cid:88)

k=2

log n
(cid:88)

k=2

(cid:107)(A − P)kU(cid:107)2→∞(cid:107)|S|−1(cid:107)k−1/2 +

(cid:88)

k>log n

(cid:107)A − P(cid:107)k(cid:107)|S|−1(cid:107)k−1/2

OP

(cid:16) d1/2(log n)kc

(cid:17)

n1/2(nρn)k/2−1/2

+

(cid:88)

(cid:16)

(nρn)−k/2+1/2(cid:17)

OP

k>log n

= OP

= OP

(cid:16) d1/2(log n)2c
n1/2(nρn)1/2
(cid:16) d1/2(log n)2c
n1/2(nρn)1/2

(cid:17)

(cid:17)

+ OP

(cid:16)

(nρn)−(log n)/2(cid:17)

(cid:16)

+ OP

1
n1/2(nρn)1/2

(cid:17)

.

Moving forward, we set forth to make precise the sense in which

ˆU|ˆS|1/2 = U|S|1/2W(cid:63) + (A − P)U|S|−1/2W(cid:63)Ip,q + RV1 + V2 + V3

≈ U|S|1/2W(cid:63) + (A − P)U|S|−1/2W(cid:63)Ip,q.

C.2.2 The matrix V2
For the matrix V2 := (cid:80)∞
that by properties of two-to-inﬁnity norm and the bounds established above,

k=0(A − P)kUIp,q|S|−k+1/2(U(cid:62) ˆU − W(cid:63))Ik+1

p,q , it is suﬃcient to observe

(cid:107)V2(cid:107)2→∞ ≤ (cid:107)U(cid:107)2→∞(cid:107)|S|1/2(cid:107)(cid:107)U(cid:62) ˆU − W(cid:63)(cid:107) + (cid:107)(A − P)U(cid:107)2→∞(cid:107)|S|−1(cid:107)1/2(cid:107)U(cid:62) ˆU − W(cid:63)(cid:107) + (cid:107)RV2(cid:107)2→∞
(cid:17)

(cid:17)

(cid:16)

+ oP((cid:107)RV1(cid:107)2→∞)

= OP

= OP

log n
n1/2(nρn)1/2
(cid:16) d1/2(log n)2c
n1/2(nρn)1/2

+ OP

(cid:16) d1/2(log n)c+1
n1/2(nρn)

(cid:17)

.

31

In the above, we write that the random variable Y ∈ R is oP(f (n)) if for any positive constant
c > 0 and any (cid:15) > 0 there exists an n0 such that for all n ≥ n0, |Y | ≤ (cid:15)f (n) with probability at
least 1 − n−c.

C.2.3 The matrix V3
The matrix V3 is given by V3 = (cid:80)∞
k=0(A − P)kUS(U(cid:62) ˆUIk+1
For each k = 0, 1, 2, . . . , deﬁne the matrix Mk := (U(cid:62) ˆUIk+1
Entry ij of the matrix Mk can be written as

p,q |ˆS|−k−1/2 − |S|−k−1/2U(cid:62) ˆUIk+1
p,q ).
p,q |ˆS|−k−1/2 − |S|−k−1/2U(cid:62) ˆUIk+1
p,q ).

(Mk)ij = (cid:104)ui, ˆuj(cid:105)(Ik+1

p,q )jj

= (cid:104)ui, ˆuj(cid:105)(Ik+1

p,q )jj

= −(cid:104)ui, ˆuj(cid:105)(Ik+1

p,q )jj

|ˆSjj|−k−1/2 − |Sii|−k−1/2(cid:105)
(cid:104)
|ˆSjj|−2k−1 − |Sii|−2k−1(cid:105) (cid:104)
(cid:104)
|ˆSjj|2k+1 − |Sii|2k+1(cid:105) (cid:104)
(cid:104)

|ˆSjj|−k−1/2 + |Sii|−k−1/2(cid:105)−1
|ˆSjj|−k−1/2 + |Sii|−k−1/2(cid:105)−1

|ˆSjj|−2k−1|Sii|−2k−1

= −(cid:104)ui, ˆuj(cid:105)(Ik+1

p,q )jj

(cid:104)

(cid:105)
|ˆSjj| − |Sii|

(cid:35)

|ˆSjj|l|Sii|2k−l

(cid:34) 2k
(cid:88)

l=0

For each k, further deﬁne a matrix Hk ∈ Rd×d entrywise as

|ˆSjj|−k−1/2 + |Sii|−k−1/2(cid:105)−1
(cid:104)

|ˆSjj|−2k−1|Sii|−2k−1.

(Hk)ij :=

(cid:35)

|ˆSjj|l|Sii|2k−l

(cid:34) 2k
(cid:88)

l=0

where it follows that

(cid:104)
|ˆSjj|−k−1/2 + |Sii|−k−1/2(cid:105)−1

|ˆSjj|−2k−1|Sii|−2k−1,

(10)

Letting ◦ denote the Hadamard matrix product, we arrive at the decomposition

(Hk)ij = OP((k + 1)(nρn)−k−3/2).

Mk = −Hk ◦ (U(cid:62) ˆUIk+1

p,q |ˆS| − |S|U(cid:62) ˆUIk+1
p,q ).

The matrices U(cid:62) ˆU and Ip,q approximately commute. More precisely,
(cid:105)

(cid:17)

(cid:16)

(cid:104)

2U(cid:62)

Ip,qU(cid:62) ˆU − U(cid:62) ˆUIp,q

=

0
−2U(cid:62)

(−)

ˆU(+)

ˆU(−)
(+)
0

∈ Rd×d,

(11)

(12)

(13)

so by Eq. (7), the spectral norm of this matrix diﬀerence behaves as OP((nρn)−1(log n)). This
approximate commutativity is important in light of further decomposing the matrix Mk as

Mk = −Hk ◦ (U(cid:62) ˆUIk+1

p,q |ˆS| − |S|U(cid:62) ˆUIk+1
p,q )

(cid:16)(cid:16)

(cid:16)(cid:16)

= −Hk ◦

= −Hk ◦

U(cid:62) ˆUIp,q|ˆS|Ik

p,q − |S|Ip,qU(cid:62) ˆUIk

p,q

(cid:17)

(cid:16)

+ |S|

U(cid:62) ˆUˆS − SU(cid:62) ˆU

(cid:17)

Ik
p,q + |S|

(cid:16)

Ip,qU(cid:62) ˆU − U(cid:62) ˆUIp,q

Ik
p,q

.

Ip,qU(cid:62) ˆU − U(cid:62) ˆUIp,q
(cid:17)

(cid:17)

(cid:17)

(cid:17)

Ik
p,q

We note that U(cid:62) ˆUˆS−SU(cid:62) ˆU = U(cid:62)(A−P) ˆU = U(cid:62)(A−P)UU(cid:62) ˆU+U(cid:62)(A−P)(I−UU(cid:62)) ˆU and
once again, by Hoeﬀding’s inequality and the Davis-Kahan theorem, we have (cid:107)U(cid:62) ˆUˆS − SU(cid:62) ˆU(cid:107) =
OP(log n), so Mk can be bounded as

(cid:104)

(cid:107)Mk(cid:107) ≤ (cid:107)Hk(cid:107)(cid:107)(U(cid:62) ˆUˆS − SU(cid:62) ˆU)Ik

p,q(cid:107)
(cid:105)
(cid:107)U(cid:62) ˆUˆS − SU(cid:62) ˆU(cid:107) + (cid:107)|S|(cid:107)(cid:107)Ip,qU(cid:62) ˆU − U(cid:62) ˆUIp,q(cid:107)

p,q + |S|(Ip,qU(cid:62) ˆU − U(cid:62) ˆUIp,q)Ik

≤ d(cid:107)Hk(cid:107)max
= OP(d(k + 1)(nρn)−k−3/2) (cid:2)OP(log n) + OP((nρn) × (nρn)−1(log n))(cid:3)
= OP(d(k + 1)(log n)(nρn)−k−3/2).

Hence, for the matrix V3,

(cid:107)V3(cid:107)2→∞ ≤ (cid:107)USM0(cid:107)2→∞ + (cid:107)(A − P)USM1(cid:107)2→∞ +

∞
(cid:88)

k=2

(cid:107)(A − P)kUSMk(cid:107)2→∞,

32

where

(cid:107)USM0(cid:107)2→∞ ≤ (cid:107)U(cid:107)2→∞(cid:107)S(cid:107)M0(cid:107) = OP

(cid:107)(A − P)USM1(cid:107)2→∞ ≤ (cid:107)(A − P)U(cid:107)2→∞(cid:107)S(cid:107)(cid:107)M1(cid:107) = OP

(cid:16) d(log n)

(cid:17)

n1/2(nρn)1/2

,
(cid:16) d3/2(log n)c+1
n1/2(nρn)

(cid:17)

,

and

∞
(cid:88)

k=2

(cid:107)(A − P)kUSMk(cid:107)2→∞ ≤

log n
(cid:88)

k=2

(cid:107)(A − P)kUSMk(cid:107)2→∞ +

(cid:88)

k>log n

(cid:107)(A − P)kUSMk(cid:107)2→∞

(cid:16) d(log n)2
nρn

≤

(cid:17)

log n
(cid:88)

k=2

OP

(cid:16) d1/2(log n)kc

n1/2(nρn)k/2−1/2

(cid:17)

(cid:16) d(log n)
nρn

+

(cid:17) (cid:88)

OP(k(nρn)−k/2+1/2)

k>log n

=

=

(cid:16) d(log n)2
nρn
(cid:16) d(log n)2
nρn

(cid:17)

(cid:17)

OP

OP

(cid:16) d1/2(log n)2c
n1/2(nρn)1/2
(cid:16) d1/2(log n)2c
n1/2(nρn)1/2

(cid:17)

(cid:17)

+

+

(cid:17)

(cid:16) d(log n)
nρn
(cid:16) d(log n)2
nρn

OP

(cid:17)

OP

(cid:16)

(log n)(nρn)−(log n)/2(cid:17)
(cid:17)
(cid:16)

1
n1/2(nρn)1/2

.

Since (nρn) = ω(d(log n)4c) for c > 1, we have (nρn) = ω(d(log n)2). It follows that

(cid:107)V3(cid:107)2→∞ = OP

(cid:16) d1/2(log n)2c
n1/2(nρn)1/2

(cid:17)

.

C.2.4 First and second-order characterization

In summary, so far we have shown that

for some (residual) matrix R ∈ Rn×d satisfying (cid:107)R(cid:107)2→∞ = OP

ˆU|ˆS|1/2 = U|S|1/2W(cid:63) + (A − P)U|S|−1/2W(cid:63)Ip,q + R,
(cid:17)
(cid:16) d1/2(log n)2c
n1/2(nρn)1/2

.

(14)

Now let QX be such that X = U|S|1/2QX. Rearranging the terms in Eq. (14) and multiplying

ﬁrst by W(cid:62)

(cid:63) followed by QX yields

ˆU|ˆS|1/2W(cid:62)

(cid:63) QX − U|S|1/2QX = (A − P)U|S|−1/2Ip,qQX + RW(cid:62)

(cid:63) QX
X |S|−1Ip,qQX + RW(cid:62)

(cid:63) QX

= (A − P)U|S|1/2QXQ−1
= (A − P)XQ−1
= (A − P)XQ−1
= (A − P)XQ−1
= (A − P)X(Q(cid:62)

X |S|−1Ip,qQX + RW(cid:62)
X |S|−1Ip,qQXIp,qIp,q + RW(cid:62)
X |S|−1(Q−1
(cid:63) QX
X|S|QX)−1Ip,q + RW(cid:62)

X )(cid:62)Ip,q + RW(cid:62)
(cid:63) QX.

(cid:63) QX

(cid:63) QX

Both ˆX = ˆU|ˆS|1/2 and Q(cid:62)

X|S|QX = X(cid:62)X, yielding the crucial equivalence

ˆXW(cid:62)

(cid:63) QX − X = (A − P)X(X(cid:62)X)−1Ip,q + RW(cid:62)

(cid:63) QX.

(15)

Theorem 3 holds by observing that

(cid:107)RW(cid:62)

(cid:63) QX(cid:107)2→∞ = OP

(cid:16) d1/2(log n)2c
n1/2(nρn)1/2

(cid:17)

and

(cid:107)(A − P)X(Q(cid:62)

X|S|QX)−1Ip,q(cid:107)2→∞ = (cid:107)(A − P)U|S|−1/2Ip,qQX(cid:107)2→∞ = OP

(cid:16) d1/2(log n)c
n1/2

(cid:17)

,

where Lemma 5 was implicitly invoked.

For the purpose of establishing Theorem 4, the i-th row of Eq. 15, when scaled by n1/2, can be

written as

n1/2(Q(cid:62)

XW(cid:63) ˆXi − Xi) = n1/2Ip,q(X(cid:62)X)−1((A − P)X)i + n1/2Q(cid:62)

XW(cid:63)Ri,

33

where the vector n1/2Ip,q(X(cid:62)X)−1((A − P)X)i can be expanded as

Ip,q(n−1X(cid:62)X)−1


n−1/2 (cid:88)

j

(Aij − Pij)Xj


 = Ip,q(n−1ρ−1

n X(cid:62)X)−1


(nρn)−1/2 (cid:88)

(Aij − Pij)ξj





j

by recalling that Xi = ρ1/2
together yield (n−1ρ−1
multivariate central limit theorem gives the (conditional) convergence in distribution

n ξi. The law of large numbers and the continuous mapping theorem
In addition, the classical

n X(cid:62)X)−1 → E(ξξ)−1 ≡ ∆−1 almost surely.

(cid:16)

(nρn)−1/2 (cid:88)

(Aij − Pij)ξj

j

(cid:17)

(cid:12)
(cid:12)
(cid:12)ξi = xi

→ Nd(0, Γρn (xi)),

(16)

i Ip,qξ)ξξ(cid:62)(cid:9). In addition,
with explicit covariance matrix given by Γρn (xi) = E (cid:8)(x(cid:62)
by combining Lemma 5 with our earlier analysis, it follows that the (transformed) residual matrix
satisﬁes

i Ip,qξ)(1 − ρnx(cid:62)

(cid:107)n1/2Q(cid:62)

XW(cid:63)Ri(cid:107)2→∞ ≤ n1/2(cid:107)QX(cid:107)(cid:107)W(cid:63)(cid:107)(cid:107)R(cid:107)2→∞ = OP

(cid:16) d1/2(log n)2c
(nρn)1/2

(cid:17) p

→ 0.

The above observations together with an application of Slutsky’s theorem yield

P

(cid:110)
n1/2(Qn

ˆXi − Xi) ≤ z | Xi = ρ1/2
n x

(cid:111)

→ Φ(z, Σρn (x))

(17)

for Qn := Q(cid:62)
Xn
device yields Theorem 4, concluding the proof.

W(cid:63),n and Σρn(x) = Ip,q∆−1Γρn (x)∆−1Ip,q. Application of the Cram´er-Wold

34

