Linking Common Vulnerabilities and Exposures to the MITRE 
ATT&CK Framework: A Self-Distillation Approach 

Benjamin Ampel  
Department of 
Management Information 
Systems  
University of Arizona  
Tucson, AZ, USA 
bampel@.arizona.edu 

ABSTRACT 

Sagar Samtani  
Department of 
Operations and Decision 
Technologies  
Indiana University 
Bloomington, IN, USA 
ssamtani@iu.edu 

Steven Ullman 
Department of 
Management Information 
Systems  
University of Arizona  
Tucson, AZ, USA 
stevenullman@ 
email.arizona.edu 

Hsinchun Chen  
Department of 
Management Information 
Systems  
University of Arizona  
Tucson, AZ, USA 
hsinchun@arizona.edu 

Due  to  the  ever-increasing  threat  of  cyber-attacks  to  critical 
cyber  infrastructure,  organizations  are  focusing  on  building 
their  cybersecurity  knowledge  base.  A  salient 
list  of 
cybersecurity  knowledge  is  the  Common  Vulnerabilities  and 
Exposures  (CVE)  list,  which  details  vulnerabilities  found  in  a 
wide  range  of  software  and  hardware.  However,  these 
vulnerabilities  often  do  not  have  a  mitigation  strategy  to 
prevent  an  attacker  from  exploiting  them.  A  well-known 
cybersecurity  risk  management  framework,  MITRE  ATT&CK, 
offers mitigation techniques for many malicious tactics. Despite 
the  tremendous  bene�its  that  both  CVEs  and  the  ATT&CK 
framework  can  provide  for  key  cybersecurity  stakeholders 
(e.g., analysts, educators, and managers), the two  entities are 
currently  separate.  We  propose  a  model,  named  the  CVE 
Transformer  (CVET),  to  label  CVEs  with  one  of  ten  MITRE 
ATT&CK  tactics.  The  CVET  model  contains  a  �ine-tuning  and 
self-knowledge  distillation  design  applied  to  the  state-of-the-
art pre-trained language model RoBERTa. Empirical results on 
a  gold-standard  dataset  suggest  that  our  proposed  novelties 
can increase model performance in F1-score. The results of this 
research  can  allow  cybersecurity  stakeholders  to  add 
preliminary  MITRE  ATT&CK  information  to  their  collected 
CVEs. 
CCS CONCEPTS 

Security and privacy
Information systems 
• 

Data mining • 
KEYWORDS 

Computing methodologies

 → Software and application security • 
→  Information systems applications → 
 → Machine learning 

CVE, MITRE ATT&CK, Cybersecurity, Transformer models, Pre-
trained language models, Self-Knowledge Distillation 
1 

INTRODUCTION 

Harmful  cyber-attacks  on  critical  cyber-infrastructure  (e.g., 
large  servers  hosting  confidential  data)  have  cost  on  average 
$7.91 million per breach, leading to over 446,000,000 exposed 
records containing sensitive information in 2019 [22]. Thus, it 

is  imperative  to  build  our  cybersecurity  knowledge  base  to 
combat  new  and  evolving  cyber-threats.  An  essential 
component  of  the  cybersecurity  knowledge  base  is  the 
Common Vulnerability and Exposures (CVE) list, overseen by 
the  MITRE  Corporation.  Cybersecurity 
professionals 
commonly  use  CVEs  to  coordinate  efforts  to  address  the 
vulnerability. A new CVE is created whenever a security flaw is 
discovered in software and hardware and reported to MITRE. 
An example of a recent CVE is shown in Figure 1.  

Figure 1: Example CVE from cve.mitre.org 

it 

     Each CVE includes metadata such as a unique ID, a rich text 
description,  references,  and  the  assigning  CNA.  Despite  their 
massive  value  to  the  cybersecurity  community,  CVEs  often 
provide little information on how to combat the vulnerability 
once 
is  discovered.  Connecting  a  cybersecurity  risk 
management  framework  (CRMF)  with  tangible  mitigation 
strategies  and  additional  context  to  the  CVE  list  to  provide 
mitigation  strategies  could  provide  tremendous  value  to 
cybersecurity analysts and researchers. 
     In 2018,  MITRE created the ATT&CK Matrix for Enterprise 
CRMF  that  models  the  tactics,  techniques,  and  procedures 
(TTP) that an attacker would take when attempting to breach 
cyberinfrastructure [21]. The MITRE ATT&CK framework can 
help reliably predict and mitigate  the tactics, techniques, and 
procedures (TTP) chain that an attacker follows after an initial 

 
 
 
 
 
 
breach  [1].  Example  tactics  include  “initial  access,”  “defense 
evasion,”  and  “exfiltration.”  Each  tactic  in  MITRE  ATT&CK 
comes  with  a  mitigation  strategy  (e.g.,  user  training,  account 
management,  password  policies,  etc.)  to  assist  cybersecurity 
analysts in protecting critical cyber-infrastructure, making it an 
excellent CRMF for our CVE labeling task. Labeling CVEs with 
ATT&CK  tactics  requires  a  computational  model  that  can 
analyze the textual metadata available in CVE descriptions. 
     Despite  the  tremendous  benefits  that  both  CVEs  and  the 
ATT&CK  framework  can  provide  for  key  cybersecurity 
stakeholders  (e.g.,  analysts,  researchers,  educators,  and 
managers), the two entities are currently separate. With over 
158,000 CVEs existing as of the beginning of 2021, it would be 
a  non-trivial  task  to  manually  link  each  one  to  the  ATT&CK 
framework  to  gather  mitigation  strategies  for  every  existing 
CVE.  
     In  this  study,  we  developed  a  novel  framework  that 
leverages  the  textual  features  in  CVEs  currently  linked  to  an 
ATT&CK  tactic  by  prior  research  [11]  to  create  an  ATT&CK 
tactic label for CVEs outside our gold-standard CVE dataset. To 
achieve this goal, we drew upon state-of-the-art methodologies 
in deep learning-based text classification literature to guide the 
development  of  a  novel  cybersecurity  model,  the  CVE 
Transformer  (CVET)  model.  To  ensure  the  value  of  our 
proposed  approach,  we  rigorously  evaluated  CVET  against 
benchmark  models  found  in  related  CVE  data  mining  and 
cybersecurity analytics literature.  
     The rest of this paper is as follows. First, we review related 
literature to CVE data machine learning, transformers for text 
classification,  and  self-knowledge  distillation.  Second,  we 
identify gaps from our literature review and pose our research 
question for study. Third, we detail our proposed method for 
labeling  CVEs  with  MITRE  ATT&CK  tactics.  Fourth,  we 
summarize  the  empirical  results  of  our  work  and  discussed 
their  implications.  Finally,  we  conclude  the  work  with 
important takeaways from the paper. 
2  LITERATURE REVIEW 

2.1  CVE Data Machine Learning 

Large  undertakings  have  been  taken  recently  to  use  CVEs  to 
improve other cybersecurity information systems through the 
use  of  classical  machine  and  deep  learning  architectures. 
Authors have used the convolutional neural network (CNN) to 
predict  CVE  vulnerability  severity  [10]  and  build  knowledge 
graphs with the CVE list, the Common Weakness Enumeration 
(CWE) list, and the Common Attack Pattern Enumeration and 
Classi�ication (CAPEC) list [29]. However, CNNs often struggle 
to  capture  long-term  dependencies  in  textual  passages  [27]. 
Extant literature then applied the more powerful bidirectional 
long short-term memory (BiLSTM) model with a self-attention 
mechanism 
[6]  and 
vulnerability  type  (e.g.,  boundary  condition  error)  [7]  with 
higher accuracy than the CNN. More recently, researchers have 
leveraged the pre-trained transformer model known as BERT 

to  predict  vulnerability  severity 

[23]  to  extract  information  from  the  prominent  vulnerability 
database ExploitDB to enhance the textual descriptions for new 
CVEs.  Building  a  model  that  can  effectively  label  CVEs  with 
ATT&CK  tactics  using  textual  descriptions  requires  an 
algorithm that can effectively represent the long text sequences 
found  in  CVE  descriptions.  The  transformer  model  (and  its 
extensions) 
text 
classi�ication  literature  and  has  proven  to  be  robust  against 
adversarial attacks [14]. We review the transformer model in-
depth to gain a deeper understanding of how it can assist in our 
target task.  
2.2  Transformers for Text Classi�ication

the  state-of-art  within 

is  currently 

Introduced  in  2017,  the  transformer  model  replaces  the 
recurrent  cells  found  in  prominent  text  classi�ication  deep 
learning  models  (e.g.,  BiLSTM,  LSTM)  with  multi-head 
the  original  design 
attention  mechanisms  [26].  While 
incorporates  an  encoder-decoder  structure  (for  machine 
translation tasks), multi-class text classi�ication only requires 
the encoder stack. The encoder transformer model creates an 
embedding  from  the  input,  passes  the  embedding  into  the 
transformer block, and outputs a softmax probability score of 
outputs.  The  embedding  layer  is  a  one-hot  encoding  with 
positional encodings. The transformer block consists of a multi-
head attention mechanism and feed-forward layers, which has 
shown  to  greatly improve  accuracy,  precision,  recall,  and  F1-
score over recurrent models in benchmark tasks [8]. Recently, 
transformers have been used for massive pre-training language 
models  (PTLMs)  that  produce  state-of-the-art  results  in 
various text classi�ication tasks [20]. These models (e.g., BERT, 
GPT-2) are often trained on millions of data points and contain 
hundreds  of  millions  of  trainable  parameters.  While  most 
researchers  do  not  own  the  hardware or  data  to  create  their 
PTLM, these models can be carefully �ine-tuned [4] and distilled 
[12] for improved performance in specialized tasks. Knowledge 
distillation  is  an  emerging  paradigm  that  can  extract  key 
information from the parameters of a PTLM to supplement the 
training of a targeted model [12]. 
2.3  Self-Knowledge Distillation

Knowledge  distillation  (KD)  combines  relational  knowledge 
from a large, pre-trained model (teacher) and a prior untrained 
model (student) [30]. As a result, the trained student model is 
often more generalizable to unseen data than a model without 
knowledge distillation. This design allows researchers that do 
not  have  access  to  the  computing  power  required  to  make  a 
massive  PTLM  to  create  highly  targeted  state-of-the-art 
models.  One 
form  of  knowledge 
distillation is self-knowledge distillation (self-KD). Self-KD uses 
the same architecture for both the student and teacher, where 
knowledge  transfer  occurs  within  the  same  model  [24].  This 
form of distillation creates a new model that often outperforms 
the  original  teacher  model  without  requiring  new  data  [32], 
due to distilling latent features from deeper to shallow sections 
of  the  network  [33],  improved  feature  importance  weighting 

increasingly  popular 

 
 
 
 
 
each ATT&CK tactic category. About 91% of the data is within 
four  tactic  categories:  defense  evasion  (8,452),  discovery 
(6,647), privilege escalation (5,779), and collection (1,748).  

To  pre-process  the  CVE  textual  description,  stop  words 
were  removed,  non-alphanumeric  characters  were  stripped. 
The remaining text was lower-cased, lemmatized, and padded 
to ensure proper lengths for all inputs. This sequence of pre-
processing  steps  is  common  in  deep  learning-based  text 
classi�ication literature [15]. We used the pre-made RoBERTa 
tokenizer [16] to encode the data as input for our CVET model. 
Other  metadata  available  in  CVEs  is  not  used,  as  it  did  not 
provide a bene�it to model performance in preliminary testing. 
4.2  CVET Architecture 
4.2.1 Model Selection

. We adapt a PTLM called RoBERTa [16] (a 
BERT-based model trained on longer sequences) for CVET due 
to the generalizability it has shown in text classification tasks 
[2].  While  there are  dozens  of PTLMs  to  choose  from  for  our 
target task, we chose RoBERTa due to the high performance it 
achieves  while  also  allowing  custom  fine-tuning  and  self-KD 
     4.2.3  Fine-Tuning. 
designs. 

The  standard  fine-tuning  process  of 
RoBERTa  and  related  PTLMs  (e.g.,  BERT,  GPT-2)  is  often 
unstable,  with  different  performances  depending  on  the 
random  seed  and  dataset  size  [4].  Through  a  series  of 
experiments, Mosbach et al. [18] identified that using the Adam 
optimization  algorithm  with  bias  correction  led  to  a  stable 
training  process  with  improved  performance  compared  to 
baseline  fine-tuning  (i.e.,  Adam  without  bias  correction,  see 
[3]).  Therefore,  we  adopt  this  bias  correction  design  for  the 
CVET fine-tuning process, where bias correction is defined as:  

𝑡𝑡
𝛼𝛼𝑡𝑡 ← 𝛼𝛼 ∙ �1 − 𝛽𝛽2

𝑡𝑡
/(1 − 𝛽𝛽1

),

𝜃𝜃𝑡𝑡 ← 𝜃𝜃𝑡𝑡−1 − 𝛼𝛼𝑡𝑡 ∙ 𝑚𝑚𝑡𝑡/(�𝑣𝑣𝑡𝑡 + 𝜖𝜖

(1
(2

𝛼𝛼  is  the  step  size,  𝑚𝑚𝑡𝑡  is  the  first-
In  the  equation  1  and  2,
moment estimate and 𝑣𝑣𝑡𝑡 is the second-moment estimate. From 
equation 1, we see that the goal of bias correction is to reduce
𝑡𝑡
𝛼𝛼  by  the  factor �1 − 𝛽𝛽2
     4.2.2  Self-Knowledge  Distillation
increases. 

),  which  increases  to  1  as 𝑡𝑡 

𝑡𝑡
/(1 − 𝛽𝛽1

.  During  fine-tuning  of  the 
CVET model, we use CVET as both a teacher and student. The 
student model (denoted as 𝐶𝐶𝐶𝐶𝐶𝐶𝐶𝐶𝑆𝑆) is CVET at fine-tuning time 
step 𝑡𝑡 and  the  teacher  model  (denoted  as 𝐶𝐶𝐶𝐶𝐶𝐶𝐶𝐶𝑇𝑇)  is  CVET  at 

[5],  or  modi�ied  regularization  [17].  Generally,  self-KD  for  a 
natural  language  processing  task  with  available  target  labels 
(e.g., a text classi�ication task) uses the weighted sum of cross-
entropy (CE) loss with the correct labels and CE loss with the 
soft target [9].  
3  RESEARCH GAPS AND QUESTIONS 

From the extant literature, we identify several research gaps: 
First,  many  tasks  have  been  undertaken  to  link  CVEs  to 
vulnerabilities, CWEs, and CAPEC, but not directly to the MITRE 
ATT&CK framework. The closest  work to  attempt this task is 
the BRON model [11], which does not link new CVEs, but uses 
existing databases to create a more holistic knowledge graph. 
Second,  the  deep  learning  models  implemented  in  recent 
literature  (e.g.,  CNN,  BiLSTM)  struggle  to  capture  long-term 
dependencies in text, such as the lengthy descriptions within 
CVE listings. These two gaps motivate our research question: 

• 

How can we create a novel and accurate link between 
CVEs  and  ATT&CK  tactics  through  their  textual 
descriptions and long-term dependencies? 

4  PROPOSED METHOD 

Our  proposed  methodology  is  comprised  of  three  major 
components:  (1)  Data  Collection  and  Pre-Processing,  (2)  the 
CVE  Transformer  (CVET)  Architecture,  and  (3)  Benchmark 
Experiments.  Each  component  is  further  detailed  in  the 
subsequent sections. 
4.1  Data Collection and Pre-Processing 

ATT&CK Tactic

Count of CVEs

Defense Evasion
Discovery
Privilege Escalation
Collection
Lateral Movement
Impact
Credential Access
Initial Access
Exfiltration
Total
Execution

8,482
6,647
5,779
1,748
715
594
427
309
137
24,863
25

Table 1: Gold-Standard Dataset CVE Distribution 

For  our  research,  we  use  the  dataset  provided  by  the  BRON 
knowledge graph [11]. As discussed earlier, there are currently 
more than 158,000 CVEs, and our gold-standard dataset only 
captures  a  fraction  of  them,  making  this  linking  task  critical. 
The dataset leverages existing knowledge bases to link 24,863 
CVEs into 10 of the 14 ATT&CK tactics. Many ATT&CK tactics 
do  not  require  a  vulnerability  (e.g.,  “Resource  Development” 
and  “Command  and  Control”).  Thus,  we  cannot  link  CVEs  to 
them. Table 1 provides a distribution of how many CVEs are in 

 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
Model Type 

Classical Machine Learning 

Deep Learning 

Pre-Trained Language Models 

Model 
Random Forest 
SVM 
Naive Bayes 
Logistic Regression 
RNN 
GRU 
LSTM 
BiLSTM 
BiLSTM with Attention 
Transformer 
GPT-2 
XLNet 
BERT 
CVET 
RoBERTa 

Accuracy 
63.70% *** 
65.70% *** 
67.30% *** 
67.10% *** 
68.45% *** 
70.90% *** 
72.75% *** 
72.55% *** 
71.41% *** 
72.45% *** 
70.21% *** 
74.12% *** 
73.93% ** 
76.93% 
74.42% * 

Precision 
35.83% *** 
51.23% *** 
44.22% *** 
41.65% *** 
69.66% *** 
72.55% *** 
74.14% *** 
73.71% *** 
72.52% *** 
74.49% *** 
77.12% *** 
80.12% * 
81.88%  
79.86% * 

Recall 
37.67% *** 
46.34% *** 
33.92% *** 
34.12% *** 
67.30% *** 
71.89%  
69.19% *** 

71.71%  
70.32% * 
70.82% * 
64.56% *** 
68.56% *** 
69.41% * 
68.49% ** 
71.49% 

F1-score 
36.70% *** 
48.66% *** 
38.16% *** 
37.38% *** 
68.46% *** 
70.83% *** 
72.00% *** 
72.70% *** 
71.40% *** 
73.61% *** 
70.27% *** 
73.88% *** 
74.26% * 
76.18% 
74.57% * 

Table 2: Comparing CVET Against Benchmark Classical Machine Learning, Deep Learning, and Pre-Trained Language 
Models (*: p<0.05, **: p<0.01, ***: p<0.001) 

Self-Distillation 

81.38% 

fine-tuning time step 𝑡𝑡 − 1. We distill learned knowledge from 
𝐶𝐶𝐶𝐶𝐶𝐶𝐶𝐶𝑇𝑇 to 𝐶𝐶𝐶𝐶𝐶𝐶𝐶𝐶𝑆𝑆 with equation 3:  

ℒ𝜃𝜃(𝑥𝑥, 𝑦𝑦) = 𝐶𝐶𝐶𝐶(𝐶𝐶𝐶𝐶𝐶𝐶𝐶𝐶𝑆𝑆(𝑥𝑥, 𝜃𝜃), 𝑦𝑦) +
λ𝑀𝑀𝑀𝑀𝐶𝐶(𝐶𝐶𝐶𝐶𝐶𝐶𝐶𝐶𝑆𝑆(𝑥𝑥, 𝜃𝜃), 𝐶𝐶𝐶𝐶𝐶𝐶𝐶𝐶𝑇𝑇(𝑥𝑥, 𝜃𝜃), 
where  𝑥𝑥  is  the  input,  𝑦𝑦  is  the  output,  𝜃𝜃  is  the  model’s 
parameters, 𝜆𝜆 is the distillation weight, CE is the cross-entropy 
loss, and MSE is the mean squared error loss. Simply, the self-
distillation  weight  balances  the  importance  of  both  loss 
functions (CE and MSE) to update the trainable parameters of 
𝐶𝐶𝐶𝐶𝐶𝐶𝐶𝐶𝑆𝑆 based on the loss functions of 𝐶𝐶𝐶𝐶𝐶𝐶𝐶𝐶𝑇𝑇 and 𝐶𝐶𝐶𝐶𝐶𝐶𝐶𝐶𝑆𝑆, which 
in  turn  improves 𝐶𝐶𝐶𝐶𝐶𝐶𝐶𝐶𝑇𝑇 at  the  following  time  step.  Distilling 
BERT-based models like this has shown improvement in many 
benchmark natural language processing (NLP) tasks [31].  
5  EMPIRICAL RESULTS 

The results of the experiment are shown in Table 2, and further 
discussed below. 
5.1  Benchmark Experiments 

We compared the proposed CVET against prevailing and state-
of-the-art  classical  machine  learning,  deep  learning,  and  pre-
trained language models. Each benchmark model is commonly 
used for CVE data machine learning and/or text classification 
Classical Machine Learning
tasks. The models selected in each category are: 

• 

•  Deep Learning

Naïve Bayes, Logistic Regression 

: Random Forest, SVM, 

• 

Pre-Trained Language Models
with attention, Transformer 

: RNN, GRU, LSTM, BiLSTM, BiLSTM 

: GPT-2, XLNet, 

BERT, RoBERTa 

     All  classical  machine  learning  models  were  implemented 
using the Python library scikit-learn. All deep learning models 
were  implemented  with  the  Python  library  Keras.  The  pre-
trained 
implemented  using  the 
Huggingface  Transformers  library.  The  CVET  model  used 

language  models  were 

RoBERTa-large from the Huggingface library [28], and our self-
distillation  and  fine-tuning  designs  were  implemented  in 
PyTorch 1.4 [19].       
     All  models  are  run  with  10-fold  cross-validation.  The 
benchmark models are evaluated with the accuracy, precision, 
recall,  and  F1-score  metrics,  which  is  the  standard  for  multi-
class  text  classification  tasks  [25].  Paired  t-tests  are  used  to 
identify  if  statistically  significant  differences  exist  between 
CVET  and  each  benchmark  method.  Due  to  our  unbalanced 
dataset, the discussion focuses on the F1-score (which is more 
resilient against skewed distributions than the other metrics) 
[13].  
5.2 

 Results and Discussion  

(3

improve 

From Table 2, we make four key observations about the results 
of  our  experiments.  First,  the  four  classical  machine  learning 
models  had  the  lowest  F1-scores,  ranging  from  36.70%  for 
Random Forest  to 48.66% for SVM. Second, all deep learning 
models  reached a  higher  F1-score  than  the  classical  machine 
learning  models.  The  transformer  model  had  the  highest  F1-
learning  models.  The 
score  (73.61%)  among  the  deep 
transformer  does  not  have  an  internal  recurrent  mechanism 
like the other models, which suggests its multi-head attention 
architecture  helped 
classi�ication 
performance.  Third,  all  of  the  PTLMs  marginally  improved 
upon  the  transformer  model  in  F1-score.  Baseline  RoBERTa 
improved  over  the  transformer  by  0.96%  (from  73.61%  to 
74.57%).  These  results  suggest 
the 
transformer can improve the performances of specialized text 
classi�ication  tasks.  Finally,  our  proposed  CVET  model 
outperformed  all  other  PTLMs,  deep  learning  models,  and 
classical  machine  learning  models  in  F1-score  (76.18%).  The 
differences were signi�icant at 𝑝𝑝<0.05 or less in all models. Our 
CVET model also achieved the best accuracy (79.93%) versus 
all other models. These results suggest that our �ine-tuning and 
self-KD  design  assisted  in  performance  improvement  in  the 
target task. 
6 

 CONCLUSION 

that  pre-training 

text 

the 

 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
  
 
 
 
 
can  greatly  assist 

In this study, we developed a novel self-distillation approach to 
automatically label CVEs with their associated ATT&CK tactic. 
The  CVET  model  �ine-tuning  process  included  an  Adam  loss 
function with added bias correction and a self-KD design. Our 
model was evaluated with a series of experiments against state-
of-the-art models in classical machine learning, deep learning, 
and  pre-trained  language  models.  Results  indicated  that  the 
CVET model offers a  signi�icant bene�it to labeling CVEs with 
tactics  over  baseline  non-distillation 
MITRE  ATT&CK 
techniques.  Our  architecture 
the 
cybersecurity  community  by  creating  an  immediate  link 
between  a  heavily  utilized  cybersecurity  risk  management 
framework  and 
can  be 
implemented into key cybersecurity stakeholders' work�low to 
associate vulnerabilities found in their scanners to the MITRE 
ATT&CK  framework  for  additional  information  on  how  to 
combat the vulnerability. 
     We identify two potential future directions for related work 
in this domain. First, we would like to expand the connection of 
CVEs to other prominent CRMFs. Potential options include the 
CAPEC  list  and  the  National  Institute  of  Standards  and 
Technology (NIST) framework. Such connections can broaden 
the mitigation strategies provided by this work when a CVE is 
discovered.  Second,  we  plan  to  look  into  other  more  re�ined 
textual  data  representation  techniques  (e.g.,  novel  word 
embedding  strategies,  synonym/homonym  generation,  POS 
tagging) to improve the features of our textual inputs.  

critical  vulnerabilities.  This 

REFERENCES 

[1] 

[2] 

[3] 

[4] 

[5] 

[6] 

[7] 

[8] 

[9] 

Al-Shaer, R., Al-Shaer, E. and Ahmed, M. 2017. Statistical Learning of 
APT TTP Chains from MITRE ATT&CK. Proc. RSA Conf. (2017), 1–2.
Chalkidis, I., Fergadiotis, M., Kotitsas, S., Malakasiotis, P., Aletras, N. and 
Androutsopoulos,  I.  2020.  An  Empirical  Study  on  Large-Scale  Multi-
Proceedings  of  the  2020  Conference  on  Empirical  Methods  in  Natural 
Label  Text  Classification  Including  Few  and  Zero-Shot  Labels. 
Language  Processing  (EMNLP)

  (Stroudsburg,  PA,  USA,  2020),  7503–

for 

transformers 

. (2018), 4171–4186. 

7515. 
Devlin,  J.,  Chang,  M.W.,  Lee,  K.  and  Toutanova,  K.  2018.  BERT:  Pre-
arXiv
training  of  deep  bidirectional 
language 
understanding. 
Dodge, J., Ilharco, G., Schwartz, R., Farhadi, A., Hajishirzi, H. and Smith, 
N.  2020.  Fine-tuning  pretrained 
language  models:  Weight 
initializations, data orders, and early stopping. 
Furlanello, T., Lipton, Z.C., Tschannen, M., Itti, L. and Anandkumar, A. 
2018. Born-Again Neural Networks. (2018). 
Gong, X., Xing, Z., Li, X., Feng, Z. and Han, Z. 2019. Joint Prediction of 
2019  24th  International  Conference  on  Engineering  of  Complex 
Multiple  Vulnerability  Characteristics  Through  Multi-Task  Learning. 
Computer Systems (ICECCS)

. (2020). 

arXiv

Companion Proceedings of the Web Conference 
Guo,  H.,  Xing,  Z.  and  Li,  X.  2020.  Predicting  Missing  Information  of 
2020
Vulnerability Reports. 

 (Nov. 2019), 31–40. 

 (New York, NY, USA, Apr. 2020), 81–82. 

Proceedings of the AAAI Conference on 
Guo,  Q.,  Qiu,  X.,  Liu,  P.,  Xue,  X.  and  Zhang,  Z.  2020.  Multi-Scale  Self-
Artificial Intelligence
Attention for Text Classification. 

. 34, 05 (Apr. 2020), 7847–7854.  

Proceedings - Natural Language Processing in a 
Hahn,  S.  and  Choi,  H.  2019.  Self-Knowledge  Distillation  in  Natural 
Deep Learning World
Language Processing. 

 (Oct. 2019), 423–430. 

[10] 

Han,  Z.,  Li,  X.,  Xing,  Z.,  Liu,  H.  and  Feng,  Z.  2017.  Learning  to  Predict 
Severity  of  Software  Vulnerability  Using  Only  Vulnerability 

[11] 

[12] 

[13] 

[14] 

[15] 

[16] 

[17] 

[18] 

[19] 

[20] 

[21] 

[22] 

[23] 

[24] 

[25] 

[26] 

[27] 

[28] 

[29] 

[30] 

[31] 

[32] 

[33] 

2017 
Maintenance and Evolution (ICSME)
Description. 

IEEE 

International  Conference  on  Software 

 (Sep. 2017), 125–136. 

arXiv

Hemberg, E., Kelly, J., Shlapentokh-Rothman, M., Reinstadler, B., Xu, K., 
Rutar,  N.  and  O’Reilly,  U.-M.  2020.  BRON  --  Linking  Attack  Tactics, 
Techniques, and Patterns with Defensive Weaknesses, Vulnerabilities 
and Affected Platform Configurations. 
Hinton, G., Vinyals, O. and Dean, J. 2015. Distilling the Knowledge in a 
Neural Network. (2015), 1–9. 
2013 Humaine 
Jeni, L.A., Cohn, J.F. and de La Torre, F. 2013. Facing Imbalanced Data--
Intelligent 
Association  Conference  on  Affective  Computing  and 
Recommendations for the Use of Performance Metrics. 
Interaction

. (Oct. 2020). 

 (Sep. 2013), 245–251. 

Jin, D., Jin, Z., Zhou, J.T. and Szolovits, P. 2020. Is BERT Really Robust? 
Proceedings  of  the  AAAI  Conference  on  Artificial 
A Strong Baseline for Natural Language Attack on Text Classification 
Intelligence
and  Entailment. 

Deep  Learning  for  NLP  and 

. 34, 05 (Apr. 2020), 8018–8025.  

Speech Recognition
Kamath,  U.,  Liu,  J.  and  Whitaker,  J.  2019. 

. Springer International Publishing. 

arXiv

arXiv

arXiv

arXiv

. (2020). 

. 1 (2019). 

. (2020), 1–34. 

Liu, Y., Ott, M., Goyal, N., Du, J., Joshi, M., Chen, D., Levy, O., Lewis, M., 
Zettlemoyer, L. and Stoyanov, V. 2019. RoBERTa: A robustly optimized 
BERT pretraining approach. 
Mobahi,  H.,  Farajtabar,  M.  and  Bartlett,  P.L.  2020.  Self-distillation 
amplifies regularization in Hilbert space. 
Mosbach, M., Andriushchenko, M. and Klakow, D. 2020. On the stability 
of  fine-tuning  BERT:  Misconceptions,  explanations,  and  strong 
baselines. 
Paszke, A. et al. 2019. PyTorch: An imperative style, high-performance 
deep learning library. 
. NeurIPS (2019). 
Qiu, X.P., Sun, T.X., Xu, Y.G., Shao, Y.F., Dai, N. and Huang, X.J. 2020. Pre-
trained models for natural language processing: a survey. 
. 63, 10 
(2020), 1872–1897.  
Strom, B.E., Miller, D.P., Nickels, K.C., Pennington, A.G. and Thomas, C.B. 
2018. MITRE ATT&CK
Sun,  H.,  Xu,  M.  and  Zhao,  P.  2020.  Modeling  Malicious  Hacking  Data 
Breach Risks. 
. 0, 0 (Jul. 2020), 1–19.  
Sun, J., Xing, Z., Guo, H., Ye, D., Li, X., Xu, X. and Zhu, L. 2021. Generating 
Informative  CVE  Description  From  ExploitDB  Posts  by  Extractive 
Summarization. 
Sun, L., Gou, J., Yu, B., Du, L. and Tao, D. 2021. Collaborative Teacher-
Student Learning via Multiple Knowledge Transfer. (2021). 
Information, 
Interdisciplinary 
Thangaraj, M. and Sivakami, M. 2018. Text Classification Techniques: 
Knowledge, and Management
A  Literature  Review. 

 : Design and Philosophy. July (2018). 

North American Actuarial Journal

. (Jan. 2021), 1–36. 

Journal  of 

arXiv

arXiv

TM

Advances 
Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A.N., 
in Neural Information Processing Systems
Kaiser, L. and Polosukhin, I. 2017. Attention Is All You Need. 

. 13, (2018), 117–135.  

2019 International 
Wang,  R.,  Li,  Z.,  Cao,  J.,  Chen,  T.  and  Wang,  L.  2019.  Convolutional 
Joint Conference on Neural Networks (IJCNN)
Recurrent Neural Networks for Text Classification. 

. (Jun. 2017), 5999–6009. 

 (Jul. 2019), 1–6. 

Wolf, T., Debut, L., Sanh, V., Chaumond, J., Delangue, C., Moi, A., Cistac, 
P., Rault, T., Louf, R., Funtowicz, M. and Brew, J. 2019. Transformers: 
Embedding  and  predicting 
State-of-the-art natural language processing. 
software  security  entity  relationships:  A  knowledge  graph  based 
Xiao,  H.,  Xing,  Z.,  Li,  X.  and  Guo,  H.  2019. 
approach

. (2019).  

arXiv

. Springer International Publishing. 
Lecture  Notes 

in  Computer  Science  (including 
Xu, G., Liu, Z., Li,  X. and Loy, C.C. 2020.  Knowledge  Distillation Meets 
subseries  Lecture  Notes  in  Artificial  Intelligence  and  Lecture  Notes  in 
Self-supervision. 
Bioinformatics)

. 588–604. 

arXiv

. (2020). 

Xu, Y., Qiu, X., Zhou, L. and Huang, X. 2020. Improving BERT fine-tuning 
via self-ensemble and self-distillation. 
Yang,  C.,  Xie,  L.,  Qiao,  S.  and  Yuille,  A.L.  2019.  Training  Deep  Neural 
Proceedings of the AAAI Conference on Artificial Intelligence
Networks  in  Generations:  A  More  Tolerant  Teacher  Educates  Better 
Students. 
. 
33, (Jul. 2019), 5628–5635.  
Zhang, L., Song, J., Gao, A., Chen, J., Bao, C. and Ma, K. 2019. Be your own 
teacher: Improve the performance of convolutional  neural networks 
. (2019), 3713–3722. 
via self distillation. 

arXiv

 
 
 
 
 
