Learning to Customize Network Security Rules

Michael Bargury, Roy Levin, Royi Ronen
Microsoft, Israel
{t-mibarg,rolevin,royir}@microsoft.com

7
1
0
2

c
e
D
8
2

]

R
C
.
s
c
[

1
v
5
9
7
9
0
.
2
1
7
1
:
v
i
X
r
a

ABSTRACT
Security is a major concern for organizations who wish to leverage
cloud computing. In order to reduce security vulnerabilities, public
cloud providers offer firewall functionalities. When properly config-
ured, a firewall protects cloud networks from cyber-attacks. How-
ever, proper firewall configuration requires intimate knowledge of
the protected system, high expertise and on-going maintenance.

As a result, many organizations do not use firewalls effectively,
leaving their cloud resources vulnerable. In this paper, we present a
novel supervised learning method, and prototype, which compute
recommendations for firewall rules. Recommendations are based
on sampled network traffic meta-data (NetFlow) collected from a
public cloud provider. Labels are extracted from firewall configura-
tions deemed to be authored by experts. NetFlow is collected from
network routers, avoiding expensive collection from cloud VMs, as
well as relieving privacy concerns.

The proposed method captures network routines and dependen-
cies between resources and firewall configuration. The method
predicts IPs to be allowed by the firewall. A grouping algorithm is
subsequently used to generate a manageable number of IP ranges.
Each range is a parameter for a firewall rule.

We present results of experiments on real data, showing ROC
AUC of 0.92, compared to 0.58 for an unsupervised baseline. The
results prove the hypothesis that firewall rules can be automatically
generated based on router data, and that an automated method can
be effective in blocking a high percentage of malicious traffic.

1 INTRODUCTION
Cloud security introduces unique opportunities as well as chal-
lenges, and is the top concern for organizations who wish to lever-
age the public cloud for their core business infrastructure [7, 15].
While most security breaches can be prevented by proper configu-
ration [5, 9], the need for highly-customized configurations makes
it prohibitively expensive for many organizations to remain pro-
tected over time [4]. This is of increasing importance as large-scale
computing is commoditized by the cloud, enabling organization to
deploy advanced architectures.

A network endpoint which consists of a Virtual Machine (VM)
and an open port, is the gateway to the organization’s virtual net-
work (VNet). Endpoints allow the VNet to communicate with other
resources and users. Unfortunately, endpoints might allow ma-
licious intenders to gain access and compromise network assets.
In response, cloud providers allow control over endpoint access,
using a firewall. The firewall has a list of allowed IP addresses
and protocols. Yet, by default, most public cloud providers allow
endpoints be accessed from any IP [2, 11]. Most of these endpoints
remain with default configuration, because creating and managing
white-lists is difficult: IP addresses change, and a misconfiguration

Figure 1: Recommendation Scenario

Benign and malicious IPs attempt to communicate with a cloud
endpoint. The recommender learns malicious and benign patterns,
and generates a proper configuration.

can result in a broken service. Figure 1 shows an illustration of the
recommendation scenario.

Automatic configuration and maintenance of firewall rules has
been the subject of extensive research. Existing methods focus
on capturing routine network usage with unsupervised methods,
such as association rule mining applied on firewall logs[14], and
network traffic logs[1, 8]. Capturing routine usage is also addressed
in [12, 16], where formulation and methods are proposed in order
to find models which reflect network state. In contrast to our ap-
proach, previous work produces firewall rules from data of a single
resource (losing dependencies between resources). To the best of
our knowledge, this work is the first to introduce a supervised ap-
proach to firewall rules generation. This enables us to automatically
filter out malicious intenders and scanners which might be allowed
by previous methods. Also, we use traffic from network routers,
which enables us to produce recommendations without the need to
collect firewall logs from each machine.

Recommender systems have become very successful at predict-
ing user-item interactions [13]. Despite their significance, few
recommendation systems have been proposed for cyber-security.
Automated recommendations reduce the expertise level and main-
tenance costs required to manage security, while minimizing the
chances of system interruption. One such example is [10], where
the suggested system provides a list of actions to improve network
security, including firewall updates. However, the system does not
generate a full firewall configuration from scratch.

In this paper, we present a novel cyber-security recommenda-
tion system which generates a list of IP ranges as recommenda-
tions for an access white-list for network endpoint firewalls. The
system learns from existing white-list configuration authored by
cyber-security experts, and predicts the IP ranges to be allowed for
unconfigured endpoints.

 
 
 
 
 
 
2 ALGORITHM
2.1 Dataset
Our dataset is sampled network traffic (NetFlow) from a cloud
provider’s routers in IPFIX format [3], collected over three weeks.
The sampling ratio is one every four thousand packets. Each raw
sample represents a traffic flow which consists of: timestamp, source
and destination IPs and ports, direction, protocol and TCP flags.
We refer to IPs that communicate with an endpoint as remote IPs.
Each sample describes communication between an endpoint and a
remote IP. The processing of flow attributes into sample features is
discussed in Section 2.2. In total, we have a little over 10 million
samples corresponding to 250TB of data.

Endpoints are matched to their firewall configuration. We ob-
serve that about 5% of endpoints are manually configured, overrid-
ing the default (allowing all communication). We assume that these
endpoints are configured by domain experts. This assumption was
verified with a sample of authors. Samples corresponding to these
endpoints are considered labeled data, where a sample is labeled
positive if the remote IP is allowed and negative otherwise.

2.2 Feature Extraction
Our model aims to distinguish between benign and malicious net-
work traffic, learning from the labeled traffic described in Section
2.1 (5% of the entire dataset). To capture the characteristics of a
remote IP’s behavior, we use features that fall into three categories:
(1) The distribution of communication over time, which consists of
the percentage of inactive hours, average, maximum and standard
deviation of daily and hourly communication volume; (2) The type
of traffic, which consists of percentage of incoming and outgoing
TCP flags (Syn, Reset, Fin). These flags are sent when a communica-
tion flow has started or ended. Hence, they provide a good indicator
to the number of new connections established; (3) The breadth of
communication, which consists of: number of VMs communicating
with the remote IP, and the number of ports used. We also use a
combination of these features by applying a quadratic polynomial
kernel. Table 1 provides a full feature list.

For every remote IP, each feature is computed at four different
levels: (1) entire cloud; (2) organization; (3) VM; and (4) endpoint.
The different levels allow the model to learn complex dependen-
cies between the communication routine of a remote IP with an
organization’s environment, and its interactions with the cloud.

For intuition on the importance of using different levels, consider
the breadth category. High feature values imply that the remote IP
is communicating with many different VMs, using various ports and
different protocols. For feature levels 2-4, this means the remote IP
is strongly related to an organization’s deployment, thus provides
a good indicator that it should be allowed. In contrast, for level 1 it
means the remote IP communicates with many cloud deployments
and organizations, which is a good indicator of automatic scanning
that should be denied.

2.3 Learning Models
The cost of denying a remote IP due to misclassifying can be high.
If a remote IP communicates heavily with an endpoint, misclassifi-
cation can break a service. Therefore, a key modeling challenge is
weighting misclassification.

Algorithm 1 IP Grouping.
Sub procedures are described in Sub Procedure 2
1: input 1: p1, . . . , pN - authorized IPs
2: input 2: L - max number of prefix sets
3: input 3: S - max prefix set size
4: output: a cover which satisfies definition 3.1
5: procedure FindMinCover(p1, . . . , pN , L, S)
6:

A0, j ← 0 ∀0 ≤ j ≤ L
Ai,0 ← ∞ ∀1 ≤ i ≤ N
for i ← 1 to N do

for j ← 1 to L do

7:

8:

9:

10:

11:

12:

13:

14:

15:

16:

17:

18:

19:

20:

21:

|shared pre f ix set o f pk , pi | ≤ S

s ← arд mink ≤i s.t .

Ai, j ← ∞
for k ← s to i do

I ← shared pre f ix set o f pk , pi
x ← noise(I )
if Ai, j > x + Ak −1, j−1 then

(cid:46) update for i points, j prefix sets
(cid:46) min noise
(cid:46) first authorized IP in I

Ai, j ← x + Ak−1, j−1
Bi, j ← k
Ci, j ← I

PrintCover (A, B, C)

(cid:46) iterate backwards to print results

To overcome this challenge we define a remote IP’s importance,
with respect to an endpoint, as the relative ratio of its communi-
cation with the endpoint. We weight each sample by the remote
IP’s importance, forcing the model to focus its attention on high
importance remote IPs.

We observe that the negative labeled samples (IPs to deny) rep-
resent 3% of labeled samples described in Section 2.1, and only 5%
of the total number of traffic flows accounted for by the labeled
samples. This is due to the noisy nature of IP communication, en-
hanced by the exposure of the public cloud to automatic scanners.
Scanners may be legitimate, like crawlers, or malicious intenders
such as vulnerability scanners. To compensate for the very skewed
nature of our classes, we normalize the weights of samples in each
class by the total class weight.

We devise two recommendation methods, the first is used as a

baseline.

2.3.1 Baseline. The model collects a list of all remote IPs that
communicate with a given endpoint in the three weeks learning
period (the entire dataset), and predicts ”allow” if and only if an IP is
in that list. Due to the underlying sampling of our dataset described
in Section 2.1, it is most likely that these IPs communicated with
the endpoint many times. This model is unlikely to block routine
communication, but it is prone to allow malicious intenders.

2.3.2

SVM Classifier. We use a binary Support Vector Machine
(SVM) with a quadratic polynomial kernel to learn a weight vector
for our features, described in Section 2.2, and predict our labels,
described in Section 2.1. The classifier is then applied on each
instance consisting of an endpoint and remote IP, to predict whether
or not the IP should be allowed access to the endpoint.

2

Figure 2: IP Grouping Example for 3-Bit IP Addresses

Possible recommendations for W = {011, 101, 110, 111}. A cover
with zero noise is presented in blue, and a best possible cover by
two prefix sets is presented in purple.

3 IP GROUPING
When producing a white-list, ranges are easier to comprehend and
maintain than single IPs. Furthermore, ranges provide more gener-
alization which improves our recommendation by allowing small
fluctuations in the IP address. We therefore devise a grouping algo-
rithm using a dynamic programming approach, which transforms
a list of IPs into CIDR formated [6] IP ranges. The CIDR format is
comprised of a base IP address and a number of least significant
bits, that can vary within the range. An IP is covered by the range
if by neglecting the specified number of least significant bits, it
is the same as the base IP. The goal of our algorithm is to find
ranges which cover all IPs classified to be allowed, whilst keeping
the number and size of these ranges as small as possible. Running
the algorithm multiple times with different constraints results in a
configurable recommendation, allowing an organization to decide
whether to emphasize reduced attack surface, or manageability
and generalization. Section 4 describes an experiment using the
algorithm on real data.

Sub Procedure 2 IP Grouping

1: procedure PrintCover(A, B, C)
2:

if AN , L = ∞ then

3:

4:

5:

6:

7:

8:

9:

10:

f ailed

else

i ← N
j ← L
while i > 0 do
print Ci, j
i ← Bi, j − 1
j ← j − 1

(cid:46) impossible constraints

3.1 Problem Statement
Let D be the set of natural numbers from 0 to 232 in their binary
representation. D represents all possible V4 IP addresses. We define
authorized IPs as those classified to be white-listed. Denote D ⊃
W = {p1, . . . , pN } the subset of all N authorized IPs, and pi ∈ W
the i’th authorized IP.

Given a bit string of length ≤ 32 denoted d, we define a prefix
set I ⊂ D to be the set of IPs which share d as their most significant

bit prefix. We say that I is generated from d. The noise of I is |I \W |,
which corresponds to the number of unauthorized IPs that share
d as their significant bit prefix. The prefix notation of a I is its bit
prefix d, followed by * to account for bits that can vary within the
set. Given two prefix sets, their shared prefix is the longest shared
substring of their most significant bit prefixes. Their shared prefix
set is the prefix set generated from their shared prefix. For example,
consider a 5-bit version of D. Let W = {11000, 11001}. Let I1 be the
prefix set of IP addresses with the prefix ”110”, and I2 be the prefix
set of IP addresses with the prefix ”111”. The prefix notation of I1
is ”110*”, its size is 4 and its noise is 2. The shared prefix of I1, I2
is ”11”, and their shared prefix set is the prefix set of IP addresses
with the prefix ”11”.

Finally, we define a cover C to be a set of pairwise disjoint m
Ii . We

prefix sets Ii , i.e C = {I1, . . . , Im }, which satisfies W ⊂

m
(cid:207)
i=1

define it’s noise as

m
(cid:205)
i=1

|Ii \ W |.

Definition 3.1. Given W = {p1, . . . , pN } authorized IPs, L max-
imum number of prefix sets, and S maximum prefix set size, our
objective is to find a cover C which satisfies:

minimize noise(C)
m
(cid:215)

subject to W ⊂

i=1
|Ii | ≤ S
m ≤ L

Ii , ∀Ii ∈ C

, ∀1 ≤ i ≤ m

3.2 The Grouping Algorithm
The IP Grouping method (Algorithm 1) uses dynamic programming
to find the solution to objective function in Definition 3.1. The
key to the algorithm is the following: after each iteration i, j, Ai, j
holds the minimum noise to cover at least the first i authorized
IPs, by cover length ≤ j, with prefix set sizes ≤ S. For each i, j,
the FindMinCover procedure in Algorithm 1 iterates trough the
different possibilities of adding the i’th IP to an existing cover of
j − 1 prefix sets and i − 1 IPs. Hence, by assuming that the smaller
covers that were computed in previous iterations are minimal, we
are guaranteed to find the minimum cover in each new iteration.
Furthermore, after the last iteration we have a minimum cover for
all N authorized IPs, which satisfies our objective function.

The run time complexity of the algorithm is O(N × L × S). In
practice, the number of prefix sets is limited by the number of rules
allowed in the firewall which is typically small [2, 11]. Furthermore,
the maximum prefix set size is also kept small to avoid large ranges
(see Section 4). Hence, in practice the run time complexity of the
algorithm is O(N ).

3.3 A Short Example
Consider a simplified case of a 3-bit version of D:
D = {000, 001, 010, 011, 100, 101, 110, 111}.
Let W = {011, 101, 110, 111}. For simplicity, let L = 4 and S = 8.

3

Figure 3: ROC Curve

Figure 4: Classes separated by the model

Liner SVM, Polynomial SVM and the Baseline ROC curves.

Test set class separation, provided by the polynomial SVM.

Lines 2-19 of FindMinCover (Algorithm 1) produce the matrices:

A =























0
0
0
0
0

0
0
0
2
1

0
0
0
0
0

, B, C =

0
0
∞ 0
∞ 6
∞ 5
∞ 4

.
1, 011
1, ∗
1, ∗
1, ∗

.
1, 011
2, 101
2, 1∗
2, 1∗

.
1, 011
2, 101
3, 110
3, 11∗

.
.




1, 011
.




2, 101
.




3, 110
.




3, 11∗
.




Using procedure PrintCover of Sub Procedure 2 with these matri-
ces, we get a minimum cover {11∗, 101, 011} with noise = 0. By
initializing j to the maximum number of prefix sets wanted L(cid:48) ≤ L
instead of L, one can get every minimum cover for L(cid:48) ≤ L. By
summing over the entries of A, one can get the noise of that cover.
For example, setting i ← 2 we get a minimum cover {1∗, 011} with
noise = 1. Figure 2 shows these covers as leafs of a binary tree. The
reader will notice that the number of rules has become manageable.

4 EXPERIMENTS AND EVALUATION
We start by comparing the results of our baseline and SVM classifier
described in Section 2.3, based on hypothesis validation. We use
60% of our labeled data set for learning, 20% for parameter search
of the SVM regularization term and 20% as a holdout set for final
evaluation. Note that these percentages are as a part of the labeled
data set described in Section 2.1. For each remote IP and endpoint in
the holdout set, the trained model predicts whether a domain expert
would have included the remote IP in the endpoint’s white-list. We
then compare the prediction with the known firewall configuration.
To measure the performance, we use AUC over a weighted ROC
curve. The samples are weighted as described in Section 2.3, and
result in a balance between the classes of IPs that should be allowed
and denied. The firewall generation task requires both a high true
positive rate which correspond to allowing access to authorized IPs,
and a high true negative rate which correspond to denying access
from unauthorized IPs. Hence, the AUC metric over a ROC curve is
a good measure for the success of our model. We measure an AUC
of 0.92 for our polynomial SVM model, compared to 0.58 for our
baseline model. Hence, our model provides an improvement of 0.34
in the AUC metric. The improved AUC is a result of learning the
features of white-listed remote IPs as opposed to allowing access to
remote IPs that historically accessed the endpoint. Figure 3 shows
an ROC curve comparison of the baseline model, a linear SVM and

a polynomial SVM. Figure 4 depicts the separation of classes in the
test set, provided by the score of the polynomial SVM.

For a deeper understanding of the underlying behavior of our
model, we analyze our feature importance. Figure 5 provides in-
sights into the combined importance of features in each level, de-
scribed at the end of Section 2.2. We observe that features in the
entire cloud level are the dominant ones for denying access, while
allowing access can be attributed to a combination of entire cloud,
organization and endpoint level features.

Algorithm 1 is applied to the results of the model, to generate
the final recommendation. We set L = 200, the default number
of firewall rules allowed by major public cloud providers [2, 11].
In addition, we set S = 4096, which can cover the IP range of a
small Internet Service Provider [6], therefore can almost surly cover
organizational network.

Figure 5: Feature Level Importance

Cells represent the percentage of weight a feature level contributes
to the final prediction.

5 CONCLUSIONS AND FUTURE WORK
In this work, we introduced a novel supervised method which rec-
ommends firewall rules for cloud endpoints. Our method combines
learning from firewall configurations created by domain experts
as well as network traffic analysis to generate a recommendation
which excludes malicious intenders. Our experiments show that
allowing all remote IPs that frequently interact with the endpoint is
not likely to provide a good estimation of a firewall configured by a
domain expert. This stresses the importance of using new methods
for firewall rule generation for cloud endpoints. In future research,
we plan to adapt the method suggested in this work to produce
recommendations for other security and maintenance issues such
as VPN configuration and VNET architecture.

4

REFERENCES
[1] Ehab S Al-Shaer and Hazem H Hamed. 2003. Firewall policy advisor for anomaly
discovery and rule editing. In Integrated Network Management, 2003. IFIP/IEEE
Eighth International Symposium on. IEEE, 17–30.

[2] Amazon. 2016. Network ACLs. http://docs.aws.amazon.com/AmazonVPC/latest/

UserGuide/VPC ACLs.html. (2016).

[3] Benoit Claise. 2008. Specification of the IP flow information export (IPFIX)

protocol for the exchange of IP traffic flow information. (2008).

[4] Paul Dourish, Rebecca E Grinter, Jessica Delgado De La Flor, and Melissa Joseph.
2004. Security in the wild: user strategies for managing security as an everyday,
practical problem. Personal and Ubiquitous Computing 8, 6 (2004), 391–401.
[5] Birhanu Eshete, Adolfo Villafiorita, and Komminist Weldemariam. 2011. Early
detection of security misconfiguration vulnerabilities in web applications. In
Availability, Reliability and Security (ARES), 2011 Sixth International Conference
on. IEEE, 169–174.

[6] Vince Fuller and Tony Li. 2006. Classless inter-domain routing (CIDR): The

Internet address assignment and aggregation plan. (2006).

[7] F. Gens. 2009. New IDC IT Cloud Services Survey: Top Benefits and Challenges.

http://blogs.idc.com/ie/?p=730/. (2009).

[8] Korosh Golnabi, Richard K Min, Latifur Khan, and Ehab Al-Shaer. 2006. Analysis
of firewall policy rules using data mining techniques. In Network Operations and
Management Symposium, 2006. NOMS 2006. 10th IEEE/IFIP. IEEE, 305–315.
Issa M Khalil, Abdallah Khreishah, Salah Bouktif, and Azeem Ahmad. 2013. Se-
curity concerns in cloud computing. In Information Technology: New Generations
(ITNG), 2013 Tenth International Conference on. IEEE, 411–416.

[9]

[10] Katherine B Lyons. 2014. A Recommender System in the Cyber Defense Domain.

Technical Report. DTIC Document.

[11] Microsoft. 2016. Network security groups. https://docs.microsoft.com/en-us/

[12]

azure/virtual-network/virtual-networks-nsg. (2016).
Ian Molloy, Youngja Park, and Suresh Chari. 2012. Generative models for access
control policies: applications to role mining over logs with attribution. In Pro-
ceedings of the 17th ACM symposium on Access Control Models and Technologies.
ACM, 45–56.

[13] Francesco Ricci, Lior Rokach, and Bracha Shapira. 2011. Introduction to recom-

mender systems handbook. Springer.

[14] Ehsan Saboori, Shafigh Parsazad, and Yasaman Sanatkhani. 2010. Automatic
firewall rules generator for anomaly detection systems with Apriori algorithm.
In Advanced Computer Theory and Engineering (ICACTE), Vol. 6. IEEE, V6–57.

[15] Kuyoro So. 2011. Cloud computing security issues and challenges. International

Journal of Computer Networks 3, 5 (2011), 247–55.

[16] Zhongyuan Xu and Scott D Stoller. 2012. Algorithms for mining meaningful
roles. In Proceedings of the 17th ACM symposium on Access Control Models and
Technologies. ACM, 57–66.

A APPENDIX

Table 1: Feature List

Distribution of communication over time
% of inactive hours
max over average of hourly # of packets
average of hourly # of packets
std of hourly # of packets
max over average of daily # of packets
average of daily # of packets
std of daily # of packets
Type of traffic
% of TCP packets out of all packets sent
% of SYN TCP packets out of all packets sent
% of RESET TCP packets out of all packets sent
% of FIN TCP packets out of all packets sent
% of TCP packets out of all packets received
% of SYN TCP packets out of all packets received
% of RESET TCP packets out of all packets received
% of FIN TCP packets out of all packets received
Breadth of communication
% of VMs the IP communicates with
# VMs the IP communicates with / # IP packets observed
% of ports the IP uses, out of # of possible ports
% of ports the IP communicates to out of # of possible ports
# ports the IP uses / # IP packets observed
# ports the IP communicates with / # IP packets observed

A list of extracted features. Each feature is computed at four
different levels, as described in Section 2.2.

5

