2
2
0
2

l
u
J

7

]

R
C
.
s
c
[

1
v
9
6
2
3
0
.
7
0
2
2
:
v
i
X
r
a

A Methodology to Support Automatic Cyber
Risk Assessment Review

Marco Angelini

Silvia Bonomi

Alessandro Palma

Abstract

Cyber risk assessment is a fundamental activity for enhancing the pro-
tection of an organization, identifying and evaluating the exposure to cy-
ber threats. Currently, this activity is carried out mainly manually and
the identiﬁcation and correct quantiﬁcation of risks deeply depend on the
experience and conﬁdence of the human assessor. As a consequence, the
process is not completely objective and two parallel assessments of the
same situation may lead to diﬀerent results. This paper takes a step in
the direction of reducing the degree of subjectivity by proposing a method-
ology to support risk assessors with an automatic review of the produced
assessment. Our methodology starts from a controls-based assessment
performed using well-known cybersecurity frameworks (e.g., ISO 27001,
NIST) and maps security controls over infrastructural aspects that can
be assessed automatically (e.g., ICT devices, organization policies). Ex-
ploiting this mapping, the methodology suggests how to identify controls
needing a revision. The approach has been validated through a case study
from the healthcare domain and a set of statistical analyses.

Keywords: Risk Assessment. Security Governance. Cybersecurity

frameworks.

1

Introduction

The growing number of cyber threats puts more and more organizations at risk
of being the victim of a cyber attack.
In this picture, it is thus paramount
to design and implement eﬃcient and eﬀective processes to timely identify and
manage potentially dangerous situations. Cyber risk management is one of those
processes, maybe the most important one. It is challenging due to the peculiar-
ities of cyberspace (e.g., wide attack surface and a high degree of uncertainty)
and its main activities are identiﬁed by several standards and best practices
([12],[11],[20]). Evaluating cyber risk raises several issues: human errors are
always possible in each step of the assessment, as well as diﬀerent sensibilities
from diﬀerent assessors to similar situations that may inﬂuence the evaluation.
Those factors could negatively aﬀect the reliability of the ﬁnal outcome. More-
over, the bias during the assessment could compromise its accuracy. This paper
proposes a methodology to review the results of a risk assessment depending

1

 
 
 
 
 
 
on the actual organization’s infrastructure exposure. We introduce the concept
of comprehensive score to quantify the security of the diﬀerent parts of the
infrastructure at diﬀerent levels (i.e., technical, human-based) and take into ac-
count governance aspects. To perform this comprehensive analysis, we exploit
the multi-layer attack graph (MLAG), over which the results of an assessment
based on well-known frameworks (e.g., ISO 27001, NIST) are mapped. Through
this approach, the assessor has more ﬁtting quantiﬁcation of the cyber risk and
the critical exposures in the infrastructure.

2 Related work

In the current literature, there exist several methodologies (and sometimes
related tools) developed in compliance (sometimes partially) with the stan-
dards, to support the risk identiﬁcation, analysis, and treatment. For example,
MEHARI risk management methodology [5] and EBIOS [2] detail the main steps
that every organization should implement to deal with cyber risk. However, they
leave to the assessor the duty to decide how to quantify the risk by leveraging
on security frameworks (e.g., ISO/IEC 27002 [10] and NIST CSF [20]). Such
frameworks can be used in combination with diﬀerent models of risk quantiﬁca-
tion. They range from the simple two-factor risk model that considers the risk
as the product of the likelihood that an asset is compromised times the impact
of the compromising, to the multi-factor risk model where both likelihood and
impact can be further decomposed into more detailed factors quantiﬁed inde-
pendently and then aggregated. One of these latter models is the OWASP risk
rating methodology [14], in which the likelihood is estimated by considering 4
factors: threat agent, vulnerabilities, technical impact, and business impact.
This rating method is quite simple to adopt and OWASP provides guidelines
about how to score each factor. However, the scoring is done by the human
who needs to spend a big eﬀort in the analysis and in remaining objective and
consistent during it. More objective evidences, coming from the infrastructure
setup, should be considered to address such issues. A suitable approach to do
it is leveraging an attack graph. An attack graph allows representing possible
ways via which an attacker can intrude into a target network by exploiting a
series of vulnerabilities on various hosts. Attack graphs can be computed auto-
matically starting from the data gathered in the system and thus they represent
an objective model over which the risk may be computed [15]. A large body of
literature exists about using attack graphs to compute network security eval-
uation metrics ([21],[24]), perform security risk analysis ([3],[8]) and compute
near-optimal proactive defense measures ([25],[8]). In this scenario, Gonzalez et
al. [8] propose an approach to perform data-driven cyber risk analysis through
an attack graph, with a quantitative risk-aware system to dynamically man-
age risk response by considering likelihood, the induced impact, the cost of the
possible responses, and the negative side-eﬀects of a response. However, the
limitation of using the classical approaches to attack graphs is that almost all of
them can support risk estimation by considering only technical aspects, without

2

considering a comprehensive view of the organization environment (e.g., human
factors, policies, and governance compliance). There exist few proposals try-
ing to address this limitation. Bonomi et al.[4] introduce a novel approach to
attack graphs that is the multi-layer attack graph (MLAG). In the context of
the PANACEA project [17] the architecture introduced by Gonzalez is enriched
with comprehensive data through the MLAG. Although these advancements
represent a good starting point for comprehensive risk analysis, information
about security governance and compliance is still not considered due to the dif-
ﬁculty to integrate them with technical data. We faced such a challenge in [1]
by proposing a methodology towards a comprehensive risk analysis, including
governance aspects.
It consists of the review of a checklist-based cyber risk
assessment, that we include in our methodology as detailed in Section 3.

3 Problem Statement and Background Notions

Before describing our methodology, we introduce two models that are the in-
puts for our solution: a controls-based assessment representing the governance
perspective and a multi-layer attack graph (MLAG) handling the technical per-
spective. We deﬁne as controls-based assessment a list of security controls and
their individual evaluation, as for example the outcome of an assessment per-
formed using ISO 27001 [9]. A security control is everything that may require
the deﬁnition and deployment of a speciﬁc process, policy, device, practice, or
other actions which may contribute to the deﬁnition and quantiﬁcation of risk.
The evaluation for us is performed by assessing the coverage level of each con-
trol. Without loss of generality, we will assume a qualitative evaluation based
on three levels i.e., completely covered (C), partially covered (PC), or not cov-
ered at all (NC) and it can be done by the assessor using data collected through
interviews, workshops or questionnaires. Let us note that compiling a controls-
based assessment is a complex activity and prone to human bias because of the
interpretability of several controls. Thus, our methodology proposes to reduce
such bias. Concerning the MLAG, we will consider the one presented in [4].
This model allows to create an objective representation of the possible paths
that an attacker may follow to compromise an asset and their likelihood can be
quantiﬁed in an unbiased way depending on the severity of the vulnerabilities
involved. The MLAG is based on three interconnected layers: (i) the human
layer modeling how an attacker can compromise individual identities by ex-
ploiting human vulnerabilities of the personnel and their relationships; (ii) the
network layer modeling the ICT part of the company used by individuals rep-
resented in the human layer; (iii) the access layer modeling the credentials that
humans may use to access devices (residing in the network layer). Each layer is
a multi-graph, meaning that multiple edges between the same pair of nodes are
characterized by diﬀerent vulnerabilities.

Using these models, we initially made a step towards their integration in [1],
in which the MLAG is used to validate the controls-based assessment performed
by a security expert, allowing to detect errors due to human bias. Beyond

3

the fact that the approach was completely manual, it just informs about the
trustworthiness of a controls-based assessment. Instead, the methodology we
propose in this paper aims to quantify and re-asses the cyber exposure using
comprehensive data and indicating the speciﬁc parts of the infrastructure they
impact on, as well as to automate the computation steps.

4 Methodology

In this section we present the methodology by ﬁrstly introducing the general
description and then detailing the application of each activity.

4.1 Methodology description

The proposed methodology (see Figure 1) starts taking as input (i) the refer-
ence frameworks, (ii) a controls-based assessment, (iii) domain models (either
extracted from assessment and attack graph or modeled from scratch), and (iv)
a technical assessment (through a MLAG).

Figure 1: Scored assessment review methodology.

Controls classiﬁcation (Fig 1a) is the ﬁrst activity we must complete, which
is labeling each security control and categorizing it according to a taxonomy
that will allow us to map it to speciﬁc elements in the environment. A classiﬁed
set of controls will be the result of this step. This task is needed as security
controls describe sets of activities to be implemented to secure the organization
and are expressed using diﬀerent levels of granularity.

Contextualized assessment mapping (Fig 1b) is the second phase, in which
we map security controls over the technical assessment result to identify the
layers that each control aﬀects. Controls are mapped in one or more layers of
the attack graph to accomplish this. The result is a classiﬁcation of security
controls based on their context (that could be human, access, and/or network
perspective). We denote such classiﬁcation as contextualized controls.

Assessment review (Fig 1c) is performed once we have the classiﬁed and
contextualized controls, and it assigns a reliability evaluation to the assessment
carried out for each control. This task will result in a ﬂagged assessment i.e., a

4

controls-based assessment with additional information indicating the subset of
controls that should be revised, further investigated, or could be implemented
involving diﬀerent aspects. This ﬁrst part of the methodology represents the
evaluation of governance and compliance aspects.

Vulnerability exposure (Fig 1d) is concurrently assessed by using the MLAG
and analyzing attack paths from human to network layer, using an approach
inspired by Gonzalez [8]. Finally, we map controls over the edges of the attack
graph to obtain, for each edge, information about technical vulnerabilities, se-
curity controls, and assessed risk impacting speciﬁc parts of the organization.
After putting this information together, a scored risk assessment (Fig 1e) is
produced, computed with comprehensive and more objective information.

4.2 Methodology application

4.2.1 Controls classiﬁcation.

Given the generality of the security controls, the ﬁrst step of the methodol-
ogy consists of classifying controls based on their features to assess how much
they are clear to be interpreted for implementation. Some controls are more
focused because they refer to speciﬁc aspects of the organization (e.g., login
and logout of devices), while others are more general and impact on diﬀerent
parts of the infrastructure (e.g., design of access control policies impact both
the administrative Board and the ICT department). This information will aﬀect
the evaluation of compliance with governance frameworks. General controls are
more diﬃcult to fully comply with due to the large number of concepts they in-
clude. We consider two criteria to assess these aspects. The ﬁrst is the lifetime,
which is the point of the risk management process in which the controls must
be put in place. It speciﬁes whether the control is implemented at design time
or run time. We assume that design-time controls have a greater impact on
the risk management process because decisions made at design time impact also
their implementation at run time as appearing in controls-based frameworks
([9],[20]). The second criterion is the management level as it is introduced by
Von Solms [22]: it indicates whether the nature of the security controls is op-
erational, meaning that they include practical activities, or compliance, related
to administrative responsibilities.

We use two ontologies to classify lifetime and management levels of each con-
trol: one containing the semantics of security controls inspired by UCO [23] and
the other containing relations between lifetime operations (e.g., process execu-
tion monitoring, communication plans policies) and management levels. Then,
we use ontology alignment between them using the automated tool AML [7].
Ontology alignment is the process of determining correspondences between con-
cepts in diﬀerent ontologies [18]:
for each couple of concepts, such a process
outputs a value, which describes the similarity of two terms of the input on-
tologies. Such a value ranges from 0 (no similarity) to 1 (full similarity). We
compare the concept of each control to the concept of each feature (run time,
design time, operational, and compliance), obtaining an output as shown in

5

table 1.

ID
A.9.4.3

Run time Design time Operational Compliance

0

0.923

0.926

0

Table 1: Example of alignment values for speciﬁcity degree.

The classiﬁcation of the controls labels the lifetime class with the label asso-
ciated to the highest value between run time and design time alignments, while
the management level class with the label of the highest alignment between op-
erational and compliance. If the alignment produces equal values for both run
time and design time, or for both operational and compliance, the related class
is labeled“Not deﬁned”, indicating that lifetime or management level cannot
be distinguished unambiguously. Considering such classiﬁcation, we deﬁne the
speciﬁcity degree of a control (spec(c)) as a function mapping the classiﬁcation
into numerical values. The speciﬁcity degree is higher for run time and opera-
tional controls, while it decreases for design time and compliance ones. This is
according to the assumption that run time and operational controls are easier
to comply with because they are more focused on practical activities and more
clear to interpret, while compliance and design tasks are harder to meet due to
their generality. Thus, we set a maximum value α for run time and operational
controls, then we halve such a value each time that the feature is design time or
compliance, representing that more general is the control, more eﬀort is required
to comply with it. In the “Not deﬁned” case we do not decrease α because the
related control has both operational/run-time and compliance/design-time na-
ture. The speciﬁcity degree values based on controls classiﬁcation are reported
in Table 2.

Lifetime
Runtime DesingTime Not deﬁned

Management

Operational
Compliance
Not deﬁned

α
α/2
α/2

α/2
α/4
α/4

α/2
α/4
α/4

Table 2: Values of speciﬁcity degree.

The parameter α is the percentage of maximum simplicity of interpretation
of the most focused controls and it is assignable by the assessor depending on
the overall generality of the controls. For example, a possible value we can
assign to α is 0.5, indicating that at least half of the controls’ activities can be
completely complied with. Considering such a value for the example in Table 1,
the speciﬁcity degree for control A.9.4.3 is 0.25 since it is classiﬁed as design
time and operational.

6

4.2.2 Contextualized assessment mapping.

This step consists of mapping the controls-based assessment onto the layers of
the attack graph to contextualize each security control to the infrastructure.
This is achieved with automatic ontology alignment [7] between the ontology of
security controls previously designed for controls classiﬁcation and the ontolog-
ical domain of the attack graph obtained by associating the nodes of the attack
graph to ontology classes and the edges to ontology relations. In this way, each
control has an alignment value between 0 (not ﬁtting) and 1 (full ﬁtting) for
each layer (human, access, network), as reported in the example of Table 3.

ID
A.9.4.3

Human Access Network
0.371

0.7

0

Table 3: Example of alignment between controls and layers.

The highest mapping value of each control represents the layer in which it
is mainly contextualized. It is possible for a security control to be mapped on
none of the layers, on one speciﬁc layer, or on multiple layers. Controls that are
mapped in a single layer are focused within the infrastructure of the organization
(i.e., easier to comply due to homogeneity of the impacted resources), while the
ones mapped in multiple layers impact diﬀerent parts of the infrastructure (i.e.,
more eﬀort is required to comply).

Considering this rationale, we deﬁne the ﬁtting degree of a control (f it(c))
to express how much it ﬁts the infrastructure. Let hi, ai, ni be the alignment
values for the control ci in the human, access and network layers respectively.
Let lM = max(hi, ai, ni) be the highest between these values (most ﬁtting layer)
and lm = min(hi, ai, ni) the smallest one (less ﬁtting layer). We deﬁne the
ﬁtting degree of a control as lM − lm, that is the range of the alignment values
representing the spread of the control in the infrastructure. Indeed, the ﬁtting
degree decreases as the control is more spread between layers. Considering the
example in Table 3, the most ﬁtting layer for control A.9.4.3 is human (with
value 0.7), while the layer in which it ﬁts less is network (with value 0).

4.2.3 Assessment review.

Assessment review is performed following the approach we introduced in [1],
which expresses the reliability of a controls-based assessment performed by a
cybersecurity expert, in which each control can be assessed totally (C-Covered),
In such an
partially (PC-Partially Covered), or nothing (NC-Not Covered).
approach the security controls are associated with the layers of the attack graph
and to management levels. Then the retrieved information is averaged to output
a percentage of how much the performed assessment is reliable for each security
control. In particular, it is computed the average of the following four data:
lifetime, management level, mapping value of a control in the layers, and the
coverage factor of the assessment. While the calculation of these data is based
on manual quantiﬁcation, we extend it by automatizing the computation of

7

these values through ontology alignment using the ontologies introduced in the
previous sections. We deﬁne the output we obtain by such a computation as
reliability degree of a control (rel(c)), which represents the trustworthiness of
the assessment of each control.

4.2.4 Vulnerability exposure.

We use the approach of Gonzalez et al. [8] to evaluate the probability that an
attack step over the MLAG is performed, indicating the diﬃculty to exploit
the related vulnerability. They model each attack path as a Markov chain and
then evaluate the exit rates (λ) of each node as the sojourn time in each state:
such a rate is the likelihood that the vulnerability in the related edge of the
attack graph is exploited. Such exit rates are evaluated based on the attributes
of each vulnerability, therefore we need to distinguish the network layer from
the human/access ones, because in the former such information is available
through CVSS [6], while in the latter we retrieve human vulnerabilities from
ISO 27005 [13]. Then the qualitative values of each attribute are normalized in
the range [0,1] such that the higher the values, the higher the severity.

In the network layer we can retrieve the following metrics valued according
to CVSS: Attack Complexity (AC), Attack Vector (AV), Privilege Required
(PR), Exploit Code Maturity (CM) and Report Conﬁdence (RC). The attacker
ability must be considered for each attribute and it can be {Naive, Advanced,
Professional}. Thus, given a network vulnerability vk of the k-th step of the
attack path, assuming attacker with abilities A =< tAC, tAV , tP R, tCM , tRC >
and considering the attributes of network vulnerability as X(vk), the diﬃculty
of exploitation of the edge ek in the network layer is:

λk =

(cid:89)

X∈AC,AV,P R,CM,RC

H(X(vk) − tX ) ∗ X(vk)

where H(z) is the Heaviside step function equal to 0 if z<0 and 1 otherwise.

In human and access layers, we refer to vulnerabilities and their evaluation
guidelines contained in ISO 27005, as “No logout”, “Sharing credentials”, “E-
mail misuse”. We consider the attack complexity (AC) and the access vector
(AV) attributes of each vulnerability, such that AC can assume values {Low,
High} and AV can assume values {Proximity, Knowledge}. We do not consider
the attacker ability because the vulnerability exposures in these layers do not
depend from the attacker. Following this approach, given a vulnerability vk
related to the k-th step of an attack path, with attributes X(vk), then:

λk =

(cid:89)

X(vk)

X∈AC,AV

4.2.5 Scored assessment.

At this point, we have the necessary data to compute a comprehensive score for
each edge of the attack graph including both governance compliance and tech-
nical aspects. On one side we have information about controls (i.e., speciﬁcity

8

degree (spec(c)), ﬁtting degree (f it(c)), reliability degree (rel(c))) and their
classiﬁcation in the attack graph layers based on the information computed in
the contextualized assessment mapping phase of the methodology. We use such
information to introduce a governance factor of a layer expressing the degree
of compliance of the layer with respect to security governance frameworks (i.e.,
security controls). To do so, we group the controls according to the layers they
belong to. Let Ci be the set of controls mapped into layer li, then the governance
factor is:

governanceF actor(li) = gov(li) = avg{∀c ∈ Ci[f (spec(c), f it(c), rel(c))]}

Function f is a generic aggregation function that maps the relation between
speciﬁcity, ﬁtting, and reliability degrees into a value between 0 and 1. Suitable
solutions for such a function could be the average to assign to the three values
the same importance, as well as the minimum (worst-case scenario usually con-
sidered by cyber-risk assessment and management activities). Once aggregated,
the average between the compliance degree of all the controls belonging to the
same layer is computed: this allows to consider the mean compliance of the
governance within each layer.

On the other hand we know the diﬃculty of technical exploitability of each
vulnerability through the computation of λ rates associated with the edges of
the attack graph. The combination of the governance factor with λ values rep-
resents the association of governance and technical knowledge. We associate the
governance factor of a layer to all the edges belonging to that layer: this indi-
cates an equal impact of that factor on each edge. Considering this assumption,
we deﬁne the comprehensive score in the edge ek in layer li of the attack graph
as:

comprehensiveScore(ek) = f (gov(li), λk) ∗ cv

where cv is a constant set by the security expert that takes into account how
rigorous is the assessment evaluation, and it weights the comprehensive score.
It is deﬁned as:

cv =

(cid:80)

x∈X αx ∗ |x|
|X|

where X is the set of possible values used to assess the security controls and |X|
is the number of such diﬀerent values (i.e., 3 if we consider C, PC, and NC); αx
is the percentage of coverage related to each assessment value (e.g., αP C = 0.5
means that the assessor evaluates ”PC” all the controls in which 50% of the
activities have been implemented); |x| is the number of controls that have been
assessed with value “x”.

The aggregation function f

in the comprehensive score calculation can be
chosen depending on the relation between the compliance governance and the
technical exposure (e.g., average, min, max). The comprehensive scores pro-
jected onto the technical assessment represent the cyber exposure of the infras-
tructure in a more ﬁtting way.

9

5 Validation

We illustrate a case study to evaluate a realistic implementation of the method-
ology and its behavior ﬁrst in a controlled ﬁtting assessment and then in two
controlled biased assessments. Then, we relax the hypothesis of controlled bias
by simulating possible scenarios of random human errors. Finally, we test limit
cases to study the bounds of the methodology.

5.1 Case study: controlled assessment ISO/IEC 27001:2013

We consider a healthcare organization (hospital) whose network is reproduced
in the PANACEA emulation environment [17], while the assessment is simu-
lated to capture a realistic scenario. The infrastructure is represented in Fig. 2
through the MLAG. The human layer is reported in green in the left part of

Figure 2: Output of scored assessment review methodology.

the ﬁgure, composed of the IT department, a resident, two doctors, and an em-
ployee simulating a possible internal attacker. The yellow nodes in the middle
part of the graph represent the access layer in which everyone accesses the sys-
tems through username and password and doctor1 has to use a badge to access
the device n9. Blue nodes in the right part of the graph compose the network
layer which is a dense network composed of physical devices and several routers.
The MLAG contains a set of vulnerabilities from CVE [19], namely: 2010-1883,
2019-2018, 2009-2412, 2018-4846, 2018-3110. The vulnerabilities in human and
access layers are the following: (i) sharing credential (i.e., the person shares the
personal credentials with others); (ii) no log-out when leaving the workstation
(i.e., the person leaves unattended device(s) logged in with personal creden-
tial); (iii) poor password management (i.e., the person uses weak passwords
and do not update them). The assessment has been conducted using ISO/IEC
27001:2013 controls because it is common for the organization to be certiﬁed
by ISO and it is such that higher assessment scores correspond to a lower cy-
ber risk. We label the ﬁttest controls-based assessment as ground truth and in
Fig. 2 the assessment values before the review (i.e., considering only technical
perspective) are reported in red, and the comprehensive scores after the review

10

(a)

(b)

Figure 3: Comparative charts of biased assessments with ground truth.

are shown in green. The comprehensive scores are lower than the technical as-
sessment in 94% of the edges with a range of variation between 10-30% (mean
25%). This highlights that while the technical assessment would indicate a cer-
tain level of cyber risk due to the presence of speciﬁc vulnerabilities, this is
not correct. In fact the comprehensive scores (that consider governance factor
across multiple layers) indicate that the organization is overall not compliant
with the standards, resulting in higher cyber exposure. Considering just the
technical perspective would give a distorted overview of the cyber risk.
Then we introduce two controlled biased assessments to study the behavior for
the following cases: (i) the conservative assessment represents the assessment
performed by an expert that gives the certiﬁcation in a meticulous way. The
bias is that if something is only partially covered, the related controls are ﬂagged
as “not covered”; (ii) the not rigorous assessment represents the opposite case:
it simulates the assessment biased by an auditor more oriented to ﬂag partially
covered controls as “covered”. We perform the methodology for the assessments
using the average as aggregation function for governance factor and comprehen-
sive score. We obtain as result the distribution of the scores reported in Fig. 3:
the ground truth has a mean score equals to 0.115 and a standard deviation of
0.0746; the conservative assessment has a mean of 0.0314 and a standard devi-
ation of 0.036; the not rigorous assessment has a mean of 0.206 and a standard
deviation of 0.125. The trend is the same in all the cases; however, they diﬀer
for the values the scores assume: the box plots in Fig. 3b make clear the dif-
ference with the ground truth. The conservative assessment has some outliers
due to the high number of “NC” that appears with respect to the rest of the
assessment. In the not rigorous assessment, the great amplitude of the box un-
derlines the underestimation of cyber exposures. A big diﬀerence with respect
to the ground truth can be dangerous: it either means that the risk coverage is
not suﬃcient, therefore several outliers appear (conservative), or that the risk is
underestimated, therefore the distribution of scores is ampliﬁed (non-rigorous).
This information (i.e., presence of outliers and ampliﬁed distribution) can be
used to detect which parts of the infrastructure have overestimated or under-
estimated exposures by considering the line-chart of Fig. 3a which reports the
edges IDs in the x-axis and the comprehensive scores in the y-axis.

11

5.2 Assessment review sensitivity analysis.

To test the methodology sensitivity we relax the hypothesis of controlled bias
by perturbing the ground truth assessment of a certain percentage of error and
studying how this perturbation aﬀects the methodology scores. We tested 28
cases: seven cases for each percentage of error equal to 15%, 45%, 65%, and
90%. These values are chosen by considering experimental results that help in
diﬀerentiating the diﬀerent cases of errors. Fig. 4 represents the trend of the

Figure 4: Distribution of the 28 error cases and ground truth.

diﬀerent cases. We notice that the scores aﬀected by errors are more variable
for human and access layers (the ﬁrst ones in the x-axis): this is because in
such layers we have fewer edges, therefore the information about controls is
more compacted in each edge and this increases the variability. Instead in the
network layer it is more distributed among the edges, therefore the assessment
scores are more uniform. Also, we can see how the variation of the errors (in
green) diﬀers from the ground truth (in red) in the four cases. In the human and
access layers the errors are detectable because they tend to amplify the value
of the scores, while in the network layer (last edges in the x-axis) the errors are
detectable because they tend to decrease the values of the scores. This means
that it is possible to detect the presence of error with respect to the ground truth
in the vast majority of cases by analyzing the peaks. Thus the methodology
is able to distinguish the degree of bias in the perturbed assessments from the
unbiased one (ground truth).

5.3 Scores distribution validation.

We ﬁnally remove the hypothesis of perturbed biased assessment by studying the
scores the methodology produces independently from the context. It is useful to
ensure adaptability to a generic context and it allows to understand the bounds
of the scores variability. The data are obtained ﬁrstly by considering all the
possible combinations of assessment (Fig. 5) for 114 security controls. Then,

12

Figure 5: Scores trend.

Figure 6: Box plots of borderline cases.

we considered the three borderline cases, where the controls are all covered,
partially covered, or all not covered (Fig. 6).

In Fig. 5 the Gaussian distribution (5a) and the box plot (5b) of the scores
are reported. They have a mean of 0.117 and a standard deviation of 0.120, with
distribution concentrated in the part of the chart with lower values. The outliers
are all located in correspondence with high values: they can be considered the
more secure points of the infrastructure. The presence of a few of those points
indicates that the methodology is oriented to identify the vulnerable parts of the
organization, and this is in line with expectations due to the applicability of this
approach to critical infrastructures. In Fig. 6 the three borderline cases of the
possible bias are reported. For the assessment with all controls covered (Fig. 6a)
the distribution is more ampliﬁed than the other cases because several security
operations are implemented, therefore the organization is compliant from both
governance and technical perspectives and the scores assume higher values. For
some elements they are still low because some controls deal with critical parts
of the organization or they are too general and diﬃcult to be compliant with
(e.g., managing changes to supplier services is a control that depends also on
third parties). Also, we set the parameter at the minimum (e.g., α = 0.5)
because we are considering a critical infrastructure, therefore we tend to be
conservative. In the other cases (Fig. 6b,6c) the distribution is similar to the
case of full coverage, but the mean is halved. This is a good result because
it indicates that the methodology takes the assessment variation into account.
Also, the standard deviation is 0.1 in case (a), 0.078 in case (b), and 0.016 in
case (c), indicating good stability of the computed scores independently from
the context.

6 Conclusion

In this paper we proposed a methodology for obtaining a more ﬁt risk assessment
of a generic organization exploiting a multi-layer attack graph. The methodol-
ogy has been validated through a case study and statistical tests, proving its
capability to correct cyber risk underestimation due to human assessor errors
or biases. Some limitations can be identiﬁed, that will be the object of further

13

research: the aggregation functions between governance and technical factors
could be more complex than the average we used for the validation, as well as
the ontology models for security controls and management levels could be not
exhaustive in terms of modeled information. Indeed, we plan as future work
(i) to provide an aggregation function able to capture with suitable weights the
governance and technical aspects and the reﬁnement of the proposed ontologies;
(ii) to develop a Visual Analytics [16] tool for the proposed approach, to better
support the application of this methodology.

References

[1] Angelini, M., Bonomi, S., Ciccotelli, C., Palma, A.: Toward a
context-aware methodology for information security governance assess-
ment validation. In: Cyber-Physical Security for Critical Infrastructures
Protection. pp. 171–186. Springer (2021), $https://doi.org/10.1007/
978-3-030-69781-5_12$

[2] ANSSI: EBIOS Risk Manager. \https://www.ssi.gouv.fr/en/guide/

ebios-risk-manager-the-method/, online

[3] Beckers, K., Heisel, M., Krautsevich, L., Martinelli, F., Meis, R., Yaut-
siukhin, A.: Determining the probability of smart grid attacks by combin-
ing attack tree and attack graph analysis. In: Cuellar, J. (ed.) Smart Grid
Security. pp. 30–47. Springer International Publishing, Cham (2014)

[4] Bonomi, S., Ciccotelli, C., Laurenza, G., Lenti, S., Palleschi, A., Santucci,
G., Sorella, M., Tanasache, F.D.: Understanding human impact on cyber
security trough multilayer attack graphs. Tech. rep., Department of Com-
puter, Control and Management Engineering, Sapienza University of Rome.
(2020), https://bonomi.diag.uniroma1.it/research/publications

[5] CLUSIF: MEHARI (MEthod for Harmonized Analysis of RIsk). http://

meharipedia.x10host.com/wp/, online; accessed 12 July 2020

[6] CVSS Special Interest Group (SIG): CVSS: Common Vulnerability Scoring

System. https://www.first.org/cvss/

[7] Faria, D., Pesquita, C., Santos, E., Palmonari, M., Cruz, I.F., Couto, F.M.:
The agreementmakerlight ontology matching system. OAEI 2013 - Ontol-
ogy Alignment Evaluation Initiative 1(1), 0–15 (2013)

[8] Gonzalez Granadillo, G., Dubus, S., Motzek, A., Garc´ıa, J., Alvarez, E.,
Merialdo, M., Papillon, S., Debar, H.: Dynamic risk management response
system to handle cyber threats. Future Gener. Comput. Syst. 83, 535–552
(2018), https://doi.org/10.1016/j.future.2017.05.043

[9] ISO Central Secretary: Iso/iec 27001:2013-information security manage-
ment. Standard ISO/IEC TR 27001:2013, International Organization for
Standardization, Geneva, CH (2013)

14

[10] ISO Central Secretary: Iso/iec 27002 - information technology – security
techniques – code of practice for information security controls. Standard
ISO/IEC TR 27002:2013, International Organization for Standardization,
Geneva, CH (2013)

[11] ISO Central Secretary:

Iso/iec 27500 - the human-centred organization
— rationale and general principles. Standard ISO/IEC TR 27500:2016,
International Organization for Standardization, Geneva, CH (2016)

[12] ISO Central Secretary: Iso 31000 - risk management - principles and guide-
lines. Standard ISO TR 31000:2018, International Organization for Stan-
dardization, Geneva, CH (2018)

[13] ISO Central Secretary: Iso/iec 27005- information technology — security
techniques — information security risk management. Standard ISO/IEC
TR 27005:2018, International Organization for Standardization, Geneva,
CH (2018)

[14] Jeﬀ Williams: OWASP Risk Rating Methodology. https://owasp.org/
www-community/OWASP_Risk_Rating_Methodology, online; accessed 12
July 2020

[15] Kaynar, K.: A taxonomy for attack graph generation and usage in network
security. Journal of Information Security and Applications 29, 27–56 (2016)

[16] Keim, D.A., Mansmann, F., Schneidewind, J., Thomas, J., Ziegler, H.:
Visual Analytics: Scope and Challenges, pp. 76–90. Springer Berlin Hei-
delberg, Berlin, Heidelberg (2008). https://doi.org/10.1007/978 − 3 − 540 −
71080 − 66

[17] L. Coventry and D. Branley-Bell and E. Sillence and S. Bonomi and
C. Ciccotelli and S. Lenti and A. Palleschi and L. Querzoni and G.
Santucci and M. Sorella, and F. D. Tanasache: D2.2 - Human Fac-
tors, Threat Models Analysis and Risk Quantiﬁcation. PANACEA Project
https://www.panacearesearch.eu

[18] Li, J., Tang, J., Li, Y., Luo, Q.: Rimom: A dynamic multistrategy on-
tology alignment framework. IEEE Transactions on Knowledge and data
Engineering 21(8), 1218–1232 (2008)

[19] MITRE: CVE: Common Vulnerability and Exposure. https://cve.

mitre.org/

[20] Nist, Aroms, E.: NIST SP 800-100 Information Security Handbook: A

Guide for Managers. CreateSpace, Scotts Valley, CA (2012)

[21] Pamula, J., Jajodia, S., Ammann, P., Swarup, V.: A weakest-adversary
security metric for network conﬁguration security analysis. In: Proceedings
of the 2nd ACM Workshop on Quality of Protection. p. 31–38. QoP ’06,
Association for Computing Machinery, New York, NY, USA (2006)

15

[22] Solms, S.V., Solms, R.V.: Information Security Governance. Springer US,

Springer (2009), https://doi.org/10.1007/978-0-387-79984-1

[23] Syed, Z., Padia, A., Finin, T., Mathews, L., Joshi, A.: Uco: A uniﬁed
cybersecurity ontology. In: UCO: A Uniﬁed Cybersecurity Ontology (02
2016)

[24] Wang, L., Jajodia, S., Singhal, A., Cheng, P., Noel, S.: k-zero day safety: A
network security metric for measuring the risk of unknown vulnerabilities.
IEEE Transactions on Dependable and Secure Computing 11(1), 30–44
(2014)

[25] Wang, L., Albanese, M., Jajodia, S.: Network Hardening - An Automated
Approach to Improving Network Security. Springer Briefs in Computer Sci-
ence, Springer (2014), https://doi.org/10.1007/978-3-319-04612-9

16

