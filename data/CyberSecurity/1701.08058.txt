Optimal Communication Strategies in Networked
Cyber-Physical Systems with Adversarial Elements

Emrah Akyol, Kenneth Rose, Tamer Bas¸ar, and C´edric Langbort

1

7
1
0
2

n
a
J

7
2

]
T
G
.
s
c
[

1
v
8
5
0
8
0
.
1
0
7
1
:
v
i
X
r
a

Abstract

This paper studies optimal communication and coordination strategies in cyber-physical systems for both defender and attacker
within a game-theoretic framework. We model the communication network of a cyber-physical system as a sensor network which
involves one single Gaussian source observed by many sensors, subject to additive independent Gaussian observation noises. The
sensors communicate with the estimator over a coherent Gaussian multiple access channel. The aim of the receiver is to reconstruct
the underlying source with minimum mean squared error. The scenario of interest here is one where some of the sensors are
captured by the attacker and they act as the adversary (jammer): they strive to maximize distortion. The receiver (estimator) knows
the captured sensors but still cannot simply ignore them due to the multiple access channel, i.e., the outputs of all sensors are
summed to generate the estimator input. We show that the ability of transmitter sensors to secretly agree on a random event, that
is “coordination”, plays a key role in the analysis. Depending on the coordination capability of the sensors and the receiver, we
consider three different problem settings. The ﬁrst setting involves transmitters and the receiver with “coordination” capabilities.
Here, all transmitters can use identical realization of randomized encoding for each transmission. In this case, the optimal strategy
for the adversary sensors also exploits coordination, where they all generate the same realization of independent and identically
distributed Gaussian noise. In the second setting, the transmitter sensors are restricted to use deterministic encoders, and this
setting, which corresponds to a Stackelberg game, does not admit a saddle-point solution. We show that the optimal strategy
for all sensors is uncoded communications where encoding functions of adversaries and transmitters are aligned in opposite
directions. In the third, and last, setting where only a subset of the transmitter and/or jammer sensors can coordinate, we show
that the solution radically depends on the fraction of the transmitter sensors that can coordinate. In the second half of the paper,
we extend our analysis to an asymmetric scenario where we remove the assumption of identical power and noise variances for
all sensors. Limiting the optimal strategies to conditionally afﬁne mappings, we derive the optimal power scheduling over the
sensors. We show that optimal power scheduling renders coordination superﬂuous for the attacker, when the transmitter sensors
exploit coordination, as the attacker allocates all adversarial power to one sensor. In the setting where coordination is not allowed,
both the attacker and the transmitter sensors distribute power among all available sensors to utilize the well-known estimation
diversity in distributed settings.

I. INTRODUCTION

Cyber-physical systems (CPSs) are large-scale interconnected systems of heterogeneous, yet collaborating, components
that provide integration of computation with physical processes [1]. The inherent heterogeneity and integration of different
components in CPS pose new security challenges [2]. One such security challenge pertains to the CPS communication network.
Most CPSs rely on the presence of a Wireless Sensor Network (WSN) composed of distributed nodes that communicate
their measurements to a central state estimator (fusion center) with higher computation capabilities. Efﬁcient and reliable
communication of these measurements is a critical aspect of WSN systems that determine usability of the infrastructure.
Consider the architecture shown in Figure 1 where multiple sensors observe the state of the plant and transmit their observations
over a wireless multiple access channel (MAC) to a central estimator (fusion center) which decides on the control action. The
sensors in such architectures are known to be vulnerable to various attacks, see e.g., [3]–[5] and the references therein. For
example, sensors may be captured and analyzed such that the attacker gains insider information about the communication
scheme and networking protocols. The attacker can then reprogram the compromised sensors and use them to launch the
so-called Byzantine attack [6]–[9], where the objective of these adversarial sensors can be i) to distort the estimate made at
the fusion center, which corresponds to a zero-sum game where the transmitting sensors aim to minimize some distortion
associated the state measurements while the objective of the attacker is to maximize it, or ii) strategically craft messages to
deceive the estimator in a way to render its estimate close to a predetermined, biased value [10], as was done in the replay
attacks of StuxNet in SCADA systems [11]. This paper presents an information/communication theoretic approach to Bayesian
optimal sensor fusion in the presence of Byzantine sensors for the ﬁrst setting, while a preliminary analysis of the second case
can be found in [12].

We analyze the communication scenario from the perspective of joint source-channel coding (JSCC) which has certain
advantages over separate source and channel coding for sensor networks; see e.g., [13] and the references therein. In this

E. Akyol, T. Bas¸ar and C. Langbort are with the Coordinated Science Laboratory, University of Illinois at Urbana-Champaign, 1308 West Main Street,
Urbana, IL 61801, USA email: {akyol, basar1, langbort}@illinois.edu. K. Rose is with the Department of Electrical and Computer Engineering, University
of California, Santa Barbara, CA, 93106 USA e-mail: rose @ece.ucsb.edu.

The material in this paper was presented in part at the IEEE International Symposium on Information Theory (ISIT), Turkey, July 2013 and at the Conference

on Decision and Game Theory for Security(GameSec) Nov. 2013, Forth Worth, Texas, USA.

This work was supported in part by NSF under grants CCF-1016861, CCF-1118075, CCF-1111342, CCF-1320599 and also by an Ofﬁce of Naval Research

(ONR) MURI Grant N00014-16-1-2710.

 
 
 
 
 
 
2

Fig. 1. The basic cyber-physical system model. The sensor in red color is a Byzantine sensor, i.e., it is captured by the adversary.

paper, we extend the game theoretic analysis of the Gaussian test channel [14]–[17] to Gaussian sensor networks studied
by [13], [18]–[24]. In [18], the performance of a simple uncoded communication is studied, in conjunction with optimal
power assignment over the sensors given a sum power budget. For a particular symmetric setting, Gastpar showed that indeed
this uncoded scheme is optimal over all encoding/decoding methods that allow arbitrarily high delay [25]. However, it is
well understood that in more realistic asymmetric settings, the uncoded communication scheme is suboptimal, and in fact,
the optimal communication strategies are unknown for these settings [26], [27]. Information-theoretic analysis of the scaling
behavior of such sensor networks, in terms of the number of sensors, is provided in [28].

In this paper, building on our earlier work on the topic [29], [30], we consider three settings for the sensor network model,
which is illustrated in Figure 2 and described in detail in Section II. The ﬁrst M sensors (i.e., the transmitters) and the single
receiver constitute Player 1 (minimizer) and the remaining K sensors (i.e., the adversaries) constitute Player 2 (maximizer).
Formulated as a zero-sum game, this setting does not admit a saddle point in pure strategies (deterministic encoding functions),
but admits one in mixed strategies (randomized functions). In the ﬁrst setting we consider, the transmitter sensors are allowed
to use randomized encoders, i.e., all transmitters and the receiver agree on some (pseudo)random sequence, denoted as
}
in the paper. We coin the term “coordination” for this capability, show that it plays a pivotal role in the analysis and the
implementation of optimal strategies for both the transmitter and the adversarial sensors , and provide the mixed-strategy
saddle-point solution in Theorem 1. In the second setting, we have a hierarchical scheme; it can be viewed as a Stackelberg
game where Player 1 is the leader, restricted to pure strategies, and Player 2 is the follower, who observes Player 1’s choice
of pure strategies and plays accordingly. We present in Theorem 2 the optimal strategies for this Stackelberg game, whose
cost is strictly higher than the cost associated with the ﬁrst setting. The sharp contrast between the two settings underlines
the importance of “coordination” in sensor networks with adversarial nodes. In the third setting, we consider only a given
subset of the transmitters and also the adversarial sensors can coordinate. We show that if the number of transmitter sensors
that can coordinate is sufﬁciently high (compared to ones that cannot), then the problem becomes a zero-sum game with a
saddle-point, where the coordination-capable transmitters use randomized linear strategy and the remaining transmitters are
not used at all. It may at ﬁrst appear to be counter intuitive to forgo utilization of the second set of transmitter sensors but
the gain from coordination (by the ﬁrst set of transmitter sensors) more than compensates for this loss. Coordination is also
important for the adversarial sensors. When transmitters coordinate, adversaries would beneﬁt from coordination to generate
identical realizations of Gaussian jamming noise. In contrast with transmitters, the adversarial sensors which cannot coordinate
are of use: they generate independent copies of identically distributed Gaussian jamming noise. Otherwise, i.e., the number of
coordinating transmitters is not sufﬁciently high, transmitters use deterministic (pure strategies) linear encoding, and optimal
adversarial strategy is also uncoded communications in the opposite direction of the transmitters.

γ
{

In the second part of the paper, we extend the analysis to asymmetric settings where sensing and/or communications channels,
and allowed transmission power of each sensor are different. For this setting, information-theoretically optimal source-channel
coding strategies are unknown (see e.g., [31] for inner and outer bounds of optimal performance). Here, we assume that the
sensors use uncoded (zero-delay) linear communication strategies, which are optimal for the symmetric setting. We also allow
another coordination capability to the sensors to combat with this inherent heterogeneity: we assume a total power limit over
the sensors which allows for power allocation over sensors. We assume this power allocation optimization capability is also
available to the adversarial sensors. We derive optimal power scheduling strategies for the transmitter and the adversarial

Estimator/Controller/FusionCenterPlantSensorSensorSensorSensorWirelessNetworkControlsignal3

Fig. 2. The sensor network model.

sensors for both settings, i.e., with or without coordination1. We show that the power allocation capability renders coordination
superﬂuous for the adversarial sensors, while it is still beneﬁcial to the transmitter sensors.

This paper is organized as follows: In Section II, we formulate the problem. In Section III, we present our results pertaining
to the symmetric setting, and in Section IV, we analyze the asymmetric case. In Section V, we present conclusions and discuss
possible future directions of research.

A. Notation

II. PRELIMINARIES

In general, lowercase letters (e.g., x) denote scalars, boldface lowercase (e.g., x) vectors, uppercase (e.g., U, X) matrices
and random variables, and boldface uppercase (e.g., X) random vectors. The kth element of vector x is denoted by [x]k. E(
),
·
), R, and R+ denote, respectively, the expectation and probability operators, and the sets of real and positive real numbers.
P(
·
Bern(p) denotes the Binary random variable, taking values 1 with probability p and
p. Gaussian
(µ, R). The mutual information of random variables
distribution with mean vector µ and covariance matrix R is denoted as
X and Y is denoted by I(X; Y ).

1 with probability 1

N

−

−

B. Problem Formulation

The sensor network model is depicted in Figure 2. The underlying source

random variables with zero mean and unit variance2. Sensor m

∈

S(i)
}
{
[1 : M +K] observes a sequence

is a sequence of i.i.d. real valued Gaussian
Um(i)

deﬁned as

{

}

Um(i) = S(i) + βmWm(i),

(1)

where
Wm(i)
{
}
and βm ∈
measurable function gN
channel inputs X m = gN

R+ is the deterministic fading coefﬁcient for the sensing channel. Sensor m

is a sequence of i.i.d. Gaussian random variables with zero mean and unit variance, independent of

,
}
[1 : M +K] can apply arbitrary Borel
RN to the observation sequence of length N , U m so as to generate the vector of length N

S(i)
{

∈

m : RN

→

m(U m) under power constraint:

The channel output is then given as

N
(cid:88)

i=1

E
X 2
{

m(i)

} ≤

Pm

Y (i) = Z(i) +

M +K
(cid:88)

m=1

αm Xm(i)

1Here, the term coordination refers to the sensors’ ability on generating identical realization of a (pseudo)random sequence.
2Normalizing the variance to 1 does not lead to any loss of generality.

(2)

(3)

    ......U1UMXMX1Y 1 M↵M↵1Z⇠N(0,1)W1⇠N(0,1)WM⇠N(0,1) ......(Transmitter)Sensor1(Transmitter)SensorM(Adversary)SensorM+1(Adversary)SensorM+KFusionCenterˆSS⇠N(0,1) M+1 M+KUM+KUM+1XM+1↵M+1↵M+KXM+KWM+1⇠N(0,1)WM+K⇠N(0,1)4

where
Z(i)
{
}
and αm ∈
Wm(i)
{
applies a Borel measurable function hN : RN

}

is a sequence of i.i.d. Gaussian random variables of zero mean and unit variance, independent of

and
R+ is the deterministic fading coefﬁcient for the communication channel of the m-th sensor. The receiver

S(i)
}
{

RN to the received length-N vector Y to generate ˆS

→

ˆS = hN (Y )

(4)

that minimize the cost, which is measured as mean squared error (MSE) between the underlying source S and the estimate at
the receiver ˆS as

J(

gN
)
m(
·
{

M +K
m=1 , hN (
}

)) =
·

1
N

N
(cid:88)

i=1

E
{

(S(i)

ˆS(i))2

.
}

−

(5)

{

)
}

)
}
·

Game model: There are two players: transmitter sensors and the receiver constitute Player 1 who seeks to minimize (5)
m=1 and hN (
M
gN
). Player 2 comprises the adversarial sensors whose common objective is to maximize (5) by
over
m(
·
M +K
gN
properly choosing
k=M +1. Since there is a complete conﬂict of interest, this problem constitutes a zero-sum game. We
k (
·
{
primarily consider the Stackelberg solution where Player-1 is the leader and plays ﬁrst as a consequence of being the leader,
and the Player-2 is the follower, responds to the strategies of Player-1. The game proceeds as follows: Player-1 plays ﬁrst and
announces its mappings. Player-2, knowing the mappings of Player-1, determines its own mappings that maximize t (5), given
the strategy of Player 1. Player-1 of course, will anticipate this, and pick its mappings accordingly. The adversarial sensors
have access to the knowledge of the strategy of the transmitter sensors (except the sequence of coordination variables
that
enables Player-1 to use randomized strategies) while the receiver has access to the strategies of all sensors, i.e., the receiver
also knows the statistics of the sensors captured by the adversary. We also note that the statistics of the variables, and the
problem parameters, including the fading coefﬁcients, are common knowledge.

γ
{

}

More formally, we are primarily interested in

JU (cid:44)

which is the upper value of the game.

min
m=1,hN

{gN

m}M

max
k }M +K

k=M +1

{gN

J (cid:0)

gN
m}

{

M
m=1,

gN
k }

{

M +K

k=M +1, hN (cid:1)

(6)

Some of the settings we analyze here admit, a special case of the described Stackelberg solution: a saddle-point solution. A
k , hN ∗) constitutes a saddle-point solution if it satisﬁes the pair of inequalities

transmitter-receiver-adversarial policy (gN ∗

m , gN ∗

J(

gN ∗
m }
{

gN ∗
k }
{
We also show that whenever a saddle-point solution exists, it is essentially unique3. At the saddle point, it is well-known

k=M +1, hN ∗)

k=M +1, hN ∗)

k=M +1, hN )

gN ∗
J(
m }
{

gN ∗
k }
{

gN
k }
{

gN
m}

M
m=1,

M
m=1,

M
m=1,

J(
{

M +K

M +K

M +K

(7)

≤

≤

that the following holds (cf. [32]):

gN ∗
JU = J(
m }
{

M
m=1,

gN ∗
k }
{

M +K

k=M +1, hN ∗) = JL

which we will refer as the saddle-point cost throughout the paper, where

JL = max
k }M +K
{gN

k=M +1

min
m=1,hN

{gN

m}M

J (cid:0)

gN
m}
{

M
m=1,

gN
k }
{

M +K

k=M +1, hN (cid:1)

(8)

(9)

We are primarily concerned with the information-theoretic analysis of fundamental limits, and hence we take N

In this paper, we consider three different problem settings (denoted as settings I, II and III), depending on the “coordination”
capabilities of sensors. A salient encoding strategy that we will frequently encounter in this paper is the uncoded4 linear
communication strategy where the N -letter communication mapping gN

m consist of N identical linear maps:

.
→ ∞

where cm satisﬁes the individual sensor power constraint with equality, i.e., cm =

(cid:113) Pm
1+β2
m

for m = 1, . . . , M .

gm(Um(i)) = cmUm(i)

In this section, we focus on the symmetric scenario. More formally, we have the following symmetry assumption.

III. THE SYMMETRIC SCENARIO

Assumption 1 (Symmetry Assumption). All sensors have identical problem parameters: βm = β, αm = α, and Pm = P for
all m

[1 : M + K].

∈

3In these settings, multiple strategies, that are different only upto a sign change, yield the same cost. To account for such trivially equivalent forms, we use

the term “essentially unique.”

4Throughout this paper, we use “uncoded”, “zero-delay” interchangeably to denote “symbol-by-symbol” coding structure.

A. Problem Setting I

5

The ﬁrst setting is concerned with the situation where the transmitter sensors have the ability to coordinate, i.e., all transmitters
and the receiver can agree on an i.i.d. sequence of random variables
generated, for example, by a side channel, the output
of which is, however, not available to the adversarial sensors5. The ability of coordination allows transmitters and the receiver
to agree on randomized encoding mappings. Perhaps surprisingly, in this setting, the adversarial sensors can also beneﬁt from
coordination, i.e., agree on an i.i.d. random sequence, denoted as

, to generate the optimal jamming strategy.

γ(i)
}
{

The saddle-point solution of this problem is presented in the following theorem.

θ(i)
}

{

Theorem 1. Setting I, and under Assumption 1, admits a saddle-point solution with the following strategies: the strategy of
the transmitter sensors is randomized uncoded transmission

where
∼
adversarial sensors) is to generate the i.i.d. Gaussian output

is an i.i.d. sequence of binary variables γ(i)

γ(i)
}

{

Xm(i) = γ(i) c Um(i), 1

m

M

≤

≤
2 ), and c =

(cid:113) P

1+β2 . The optimal jamming function (for

(10)

Bern( 1

where

Xk(i) = θ(i), M + 1

k

≤

≤

M + K

∼ N
and is independent of the adversarial sensor input Uk(i). The strategy of the receiver is the Bayesian estimator of S given Y ,
i.e.,

θ(i)

(0, P ),

The cost at this saddle-point is

h(Y (i)) =

M c αβ
M 2α2β2c2 + M c2α2 + K 2P + 1

γ(i) Y (i).

Moreover, this saddle-point solution is essentially unique.

J S
C (M, K) =

M c2α2 + K 2P + 1
M 2α2β2c2 + M c2α2 + K 2P + 1

Proof: We start by verifying that the mappings given in the theorem satisfy the pair of saddle-point inequalities (7),

following the approach in [16].

RHS of (7): Suppose the policy of the adversarial sensors is given as in Theorem 1. Then, the communication system at hand
becomes essentially identical to the problem considered in [25], whose solution is uncoded communication with deterministic,
linear encoders, i.e., Xm(i) = cUm(i). Any probabilistic encoder, given in the form of (10) (irrespective of the density of γ) yield
the same cost (12) with deterministic encoders and hence is optimal. Given that the optimal transmitter is in the form of (10), the
optimal decoder is also zero-delay (symbol-by-symbol) mapping given as h(Y (i)) = E
)−1Y (i)
S(i)
|
{
which can be explicitly obtained as in (11) noting that

= E
SY
{

(E
Y 2
{

Y (i)
}

}

}

Y = c α γ(i)

M
(cid:88)

m=1

M +K
(cid:88)

Um(i) + α

Xk(i) + Z(i)

k=M +1
= 1 + K 2P + (M βcα)2 + M α2c2.

The distortion is (observing that σ2

S = 1):

E
SY
{

}

= γ(i) M βcα, E
Y 2
{

}

J = 1

−

(E
)2
SY
}
{
E
Y 2
}
{

=

M c2α2 + K 2P + 1
M 2α2β2c2 + M c2α2 + K 2P + 1

LHS of (7): By the symmetry of the problem, we assume, without any loss of generality, that all adversarial sensors use the
same jamming strategy. Let us derive the overall cost conditioned on the realization of the transmitter mappings (i.e., γ = 1
and γ =

1) used in conjunction with optimal linear decoders. If γ = 1

−

D1 = J1 + ξE
{

SXk}

+ ψE
ZXk}
{

for some constants ξ, ψ, and similarly if γ =

1

−
D2 = J1 −

ξE
{

SXk} −

ψE
{

ZXk}

where the overall cost is

D(i) = P(γ(i) = 1)D1 + P(γ(i) =

1)D2.

−

5An alternative practical method to coordinate is to generate the identical pseudo-random numbers at each sensor, based on pre-determined seed.

(11)

(12)

(13)

(14)

(15)

(16)

(17)

(18)

6

Clearly, for γ(i)
irrespective of the distribution of
this fact, we next show this saddle point is essentially unique.

2 ) the overall cost J1 is only a function of the second-order statistics of the adversarial outputs,
, and hence the solution presented here is indeed a saddle-point. Having established
}

θ(i)
{

∼

Bern( 1

Gaussianity of Xk(i): The choice Xk(i) = θ(i) maximizes (5) since it renders the simple uncoded linear mappings
asymptotically optimal, i.e., the transmitters cannot improve on the zero-delay performance by utilizing asymptotically high
delays. Moreover, the optimal zero-delay performance is always lower bounded by the performance of the linear mappings,
which is imposed by the adversarial choice of Xk(i) = θ(i).
of

= 0
= 0, the transmitter can adjust its Bernoulli parameter to decrease the distortion. Hence, the optimal adversarial

S(i)
{
= E
{
choices will not cancel the cross terms in (16) and (17), i.e., E
{
the adversary to increase the cost, hence optimal strategy for transmitter is to set γ = Bern(1/2).

= 0 which implies independence since all variables are jointly Gaussian.
Choice of Bernoulli parameter: Note that the optimal choice of the Bernoulli parameter for the transmitters is 1

: If the adversarial sensors introduce some correlation, i.e., if E
{

Independence of
Xk(i)
{
}
or E
W Xk} (cid:54)
{
strategy is setting E
SXk}
{

2 since other
. These cross terms can be exploited by

}
W Xk}

and E
{

W Xk}

SXk} (cid:54)

W (i)
}

SXk}

and

{

Corollary 1 (Value of Coordination). Coordination, i.e., the ability of using a common randomized sequence, is beneﬁcial to
adversarial sensors in the case of coordinating transmitters and receiver, in the sense that lack of adversarial coordination
strictly decreases the overall cost.

Proof: Note that coordination, i.e., to be able to generate the same realization of θ(i) enables adversarial sensors to
generate a Gaussian noise with variance K 2PA yielding the cost in (12). However, without coordination, each sensor can only
generate independent Gaussian random variables, yielding an overall Gaussian noise with variance KP and the total cost

M c2α2 + KP + 1
M 2α2β2c2 + M c2α2 + KP + 1

< J S

C (M, K)

(19)

Hence, coordination of adversarial sensors strictly increases the overall cost.

Remark 1. We note that the optimal strategies do not depend on the sensor index m, hence the implementation of the optimal
strategy, for both transmitter and adversarial sensors, requires “coordination” among the sensors. This highlights the need for
coordination in game theoretic settings in sensor networks. Note that this coordination requirement arises purely from the game
theoretic considerations, i.e., the presence of adversarial sensors. In the case where no adversarial node exists, transmitters
do not need to “coordinate”. Moreover, as we will show in Theorem 2 if the transmitters cannot coordinate, then adversarial
sensors do not need to coordinate.

B. Problem Setting II

Here, we address the second setting, where the transmitters do not have the ability to secretly agree on a sequence of i.i.d.
, to generate their transmission functions Xm. This setting does not admit a saddle-point
“coordination” random variables,
}
solution, hence a Stackelberg solution is sought here. We also assume the number of adversarial sensors is less than the number
of transmitter ones, i.e., K < M , otherwise (if M
K) the adversarial sensors can effectively eliminate the output of the
transmitters, and the problem becomes trivial.

γ
{

≥

We show that the essentially unique Stackelberg equilibrium is achieved by a transmitter strategy, which is identical across
all transmitters: uncoded transmission with linear mappings. The equilibrium achieving strategy for the attack sensors, again
identical across all adversarial sensors, is uncoded transmission with linear mappings, but with the opposite sign of the
transmitter. The receiver strategy is symbol-by-symbol optimal estimation of the source from the channel output. A rather
surprising observation is that the adversarial coordination (in the sense of sharing a random sequence that is hidden from
the transmitter sensors and the receiver) of is superﬂuous for this setting, i.e., even if the adversarial sensors are allowed to
cooperate, the optimal mappings and hence, the resulting cost at the saddle point do not change.

The following theorem captures this result.

Theorem 2. For setting II and under Assumption 1, the essentially unique Stackelberg equilibrium is achieved by:

for the transmitter sensors and

Xm(i) = c Um(i), 1

m

≤

≤

M

Xk(i) =

−

c Uk(i), M + 1

k

≤

≤

M + K

for the adversarial sensors. The optimal receiver strategy is the symbol-by-symbol Bayesian estimator of S given Y , i.e.,

h(Y (i)) =

(M

−

(M
K)2α2β2c2 + (M

−

K) c αβ

K)c2α2 + 1

−

Y (i).

(20)

The cost at this Stackelberg solution is

J S
N C(M, K) =

(M

−

(M

K)c2α2 + 1

−
K)2α2β2c2 + (M

K)c2α2 + 1

−

7

(21)

Proof: Let us ﬁrst ﬁnd the cost at the equilibrium, J S

N C, for the given encoding strategies. We start by computing the

expressions used in the MMSE computations,

(cid:32) M
(cid:88)

Y = c α

M +K
(cid:88)

(cid:33)

+ Z

Uk

Um −
m=1
k=M +1
K) c αβ, E
Y 2
{

E
SY
{
Plugging these expressions, we obtain the cost, J S
h(Y (i))E
SY
{

)−1Y (i) as in (20).

(E
Y 2
{

= (M

−

}

}

}

K)2α2β2c2 + (M

= (M

}

−
(E{SY })2
E{Y 2}

−

N C(M, K) = 1

K)c2α2 + 1.

−

(22)

(23)

as given in (21), and the optimal receiver strategy,

We next show that linear mappings are the optimal (in the information-theoretic sense) encoding and decoding strategies.
We ﬁrst note that adversarial sensors have the knowledge of the transmitter encoding functions, and hence the adversarial
encoding functions will be in the same form as the transmitters functions but with a negative sign i.e., since outputs are sent
over an additive channel (see e.g., [16], [17] for a proof of this result). We next proceed to ﬁnd the optimal encoding functions
for the transmitters subject to this restriction. From the data processing theorem, we must have

≤
where we use the notational shorthand U m = [Um(1), Um(2), . . . , Um(N )] (and likewise for X m, Y and ˆS) for length N
sequences of random variables. The left hand side can be lower bounded as:

I(U 1, U 2, . . . , U M +K; ˆS)

I(X 1, X 2, . . . , X M +K; Y )

(24)

where R(D) is the rate-distortion function of the Gaussian CEO problem adopted to our setting, and is derived in Appendix
A. The right hand side can be upper bounded by

I(U 1, U 2, . . . , U M +K; ˆS)

R(D)

≥

(25)

I(X 1, X 2, ..., X M +K; Y )

(a)

≤

N
(cid:88)

i=1

I(X1(i), . . . , XM +K(i); Y (i))

max

N
(cid:88)

i=1

I(X1(i), . . . , XM +K(i); Y (i))

1
2

N
(cid:88)

i=1

log(1 + 1T RX (i)1)

≤

=

where RX (i) is deﬁned as

RX (i)
{

}p,r (cid:44) E
Xp(i)Xr(i)
{

p, r

} ∀

∈

[1 : M +K].

(26)

(27)

(28)

(29)

Note that (a) follows from the memoryless property of the channel and the maximum in (27) is over the joint density
over X1(i), . . . , XM +K(i) given the structural constraints on RX (i) due to the power constraints. It is well known that the
maximum is achieved, uniquely, by the jointly Gaussian density for a given ﬁxed covariance structure [33], yielding (28). Since
logarithm is a monotonically increasing function, the optimal encoding functions gN
[1 : M ] equivalently maximize
m(
·
(cid:80)
p,r

E
Xp(i)Xr(i)
}
{

. Note that

), m

∈

Xm(i) = (cid:2)gN

m(U m)(cid:3)

i

and hence

gN
m(
·
{

)
}

M
m=1 that maximize

p=M +K
(cid:88)

r=M +K
(cid:88)

p=1

r=1

E
{

[gN

p (U p)]i[gN

r (U r)]i}

(30)

(31)

can be found by invoking Witsenhausen’s lemma (given in Appendix B) as [gN
hence gN
sides of (24). The linear mappings in Theorem 2 achieve this outer bound, and hence are optimal.

[1 : N ], and
N C as an outer bound by equating the left and right hand

m(U m)]i = c Um(i) for all i

[1 : M ]. Finally, we obtain J S

m(U m) = c U m for all m

∈

∈

Corollary 2. Source-channel separation, based on digital compression and communications is strictly suboptimal for this
setting.

8

Proof: We ﬁrst note that the optimal adversarial encoding functions must be the negative of that of the transmitters to
achieve the saddle-point solution derived in Theorem 2. But then, the problem at hand becomes equivalent to a problem with
no adversary which was studied in [13], where source-channel separation was shown to be strictly suboptimal. Hence, separate
source-channel coding has to be suboptimal for our problem. A more direct proof follows from the calculation of the separate
source-channel coding performance.

Corollary 3. Coordination is beneﬁcial to transmitter sensors, in the sense that lack of coordination strictly increases the
equilibrium cost.

Proof: Proof follows from the fact that J S

C < J S

N C.

C. Problem Setting III

The focus of this section is the setting between the two extreme scenarios of coordination, namely full, or no coordination.
In the following, we assume that M (cid:15) transmitter sensors can coordinate with the receiver while M (1
(cid:15)) of them cannot
coordinate, where 0 < (cid:15) < 1 and M (cid:15) is integer. Similarly, we consider only Kη of the adversarial sensors can coordinate
while the remaining K(1
η) adversarial sensors cannot, where 0 < η < 1 and Kη is integer. Let us reorder the sensors,
−
without loss of generality, such that the ﬁrst M (cid:15) transmitters and Kη adversaries can coordinate. We again take K < M . Let
us also deﬁne the quantity (cid:15)0 as the unique6 solution to:

−

J S
C (M (cid:15)0,

(cid:112)

K 2η2 + K(1

η)) = J S

N C(M, K)

−

(32)

The following theorem captures our main result.

Theorem 3. For (cid:15) > (cid:15)0, there exists a saddle-point solution with the following strategies: the optimal transmission strategy
(cid:15)) transmitters are not
requires that the M (cid:15) capable transmitters use randomized linear encoding, while the remaining M (1
used.

−

Xm(i) = γ(i) c Um(i),
Xm(i) = 0

1

≤

M (cid:15)

m

≤

m

M (cid:15)

M

≤

≤

(33)

(34)

Bern( 1
where
is an i.i.d. sequence of binary variables γ(i)
capable adversarial sensors) is to generate the identical Gaussian noise

γ(i)
{

∼

}

2 ). The optimal jamming policy (for the coordination-

while the remaining adversarial sensors will generate independent Gaussian noise

Xk(i) = θ(i), M + 1

k

≤

≤

M + Kη

Xk(i) = θk(i), M + Kη

k

≤

≤

M + K

(35)

(36)

(0, P ) are independent of the adversarial sensor input Uk(i). The receiver strategy at this saddle point is

where θk(i)

∼ N

h(Y (i)) =

M 2(cid:15)2α2β2c2 + M (cid:15)c2α2 + (K 2η2 + K(1

M (cid:15) c αβ

γ(i) Y (i).

η))P + 1

(37)

−

If (cid:15) < (cid:15)0, the Stackelberg equilibrium is achieved with the deterministic linear encoding for the transmitter sensors, i.e.,

and the adversarial sensors use identical functional form with opposite sign of the transmitters, i.e.,

Xm(i) = c Um(i), 1

m

≤

≤

M

Xk(i) =

−

c Uk(i), M + 1

k

≤

≤

M + K

and the receiver uses

h(Y (i)) =

(M

(M
K)2α2β2c2 + (M

−

K) c αβ

−

−

K)c2α2 + 1

Y (i).

(38)

(39)

(40)

Proof: The transmitters have two choices: i) All transmitters will choose not to use randomization. Then, the adversarial
sensors do not need to use randomization since the optimal strategy is deterministic, linear coding with the opposite sign, as
shown in Theorem 2. Hence, the cost associated with this option is J S
N C(M, K). ii) Capable transmitters will use randomized
encoding. This choice implies that remaining transmitters do not send information as they do not have access to randomization
, hence they are not used. The adversarial sensors which can coordinate generate identical realization of the
sequence
}
Gaussian noise while, remaining adversaries generate independent realizations. The total effective noise adversarial power will
be ((Kη)2 + (1
η)). Hence, transmitters
will choose between two options depending on their costs, J S
C is a

K 2η2 + K(1
−
N C(M, K). Since, J S
η)) and J S

η)K)P , and the cost associated with this setting is J S

C (M (cid:15),
K 2η2 + K(1

C (M (cid:15),

γ
{

(cid:112)

(cid:112)

−

−

6The fact that J S

C is monotonically decreasing in (cid:15)0 ensures that (32) admits a unique solution.

9

decreasing function in M and hence in (cid:15), whenever (cid:15) > (cid:15)0, transmitters use randomization (and hence so do the adversaries),
otherwise problem setting becomes identical to “no coordination”. The rest of the proof simply follows from the proofs of
Theorems 1 and 2.

Remark 2. Note that in the ﬁrst regime ((cid:15) > (cid:15)0), we have a zero-sum game with saddle-point. In the second regime ((cid:15) < (cid:15)0),
we have a Stackelberg game where all transmitters and receiver constitute the leader and adversaries constitute the follower.

Remark 3. Theorem 3 states a rather interesting observation: depending on the network conditions, the optimal transmission
strategy may not use all of the transmitter sensors. At ﬁrst glance, it might seem that discarding some of the available transmitter
sensors is suboptimal. However, there is no feasible way to use these sensors, which cannot coordinate, without compromising
the beneﬁts of coordination.

IV. THE ASYMMETRIC SCENARIO

In this section, we remove the assumption of identical sensing and channel noise variances and identical transmitter and
adversary average power. Instead, we assume there is a sum-power limit for the set of transmitters and for the set of adversarial
nodes. In this general asymmetric case, the optimal, in information-theoretic sense, communication strategies are unknown in
the absence of adversary. Here, we assume zero-delay linear strategies, in the light of our results in previous section, which
provide an upper bound on the distortion-power performance:

Assumption 2. In Setting-I (where the transmitter sensors can coordinate), the transmission strategies are restricted to

Xm(i) = γ(i)cmUm(i)

(41)

where
transmission strategies are limited to

γ(i)
}

{

is an i.i.d sequence of binary variables γ(i)

Bern( 1

2 ). In Setting-II (where no coordination is allowed), the

∼

The problem we address in this section is two-fold: i) determine the optimal power allocation strategies for a given sum-power

Xm(i) = cmUm(i).

(42)

constraint of the form:

M
(cid:88)

m=1

Pm ≤

PT ,

and ii) determine the optimal adversarial sensor strategies subject to a sum power constraint:

M +K
(cid:88)

k=M +1

Pk ≤

PA,

(43)

(44)

Before deriving our results, we introduce a few variables.

Deﬁnition. We let k∗ be the index of an adversarial sensor having the best communication channel, i.e.,

k∗ (cid:44) argmax

k∈[M +1:M +K]

αk

and P (cid:48)

A is the associated received power P (cid:48)
A

(cid:44) α2

k∗ PA . In the case of multiple k∗s, we pick one arbitrarily.

A. Setting-I

In this setting, the transmitters can coordinate, similar to the setting studied in Section III-A. Following the same steps as

in Section III-A, we conclude that the solution sought here is a saddle point.

Theorem 4. For setting I, and under Assumption 2, an essentially unique saddle-point solution exists with the following
strategies: the communication strategy for the transmitter sensor m is given in (41) where

cm =

,

λ1 =

PT
1 + P (cid:48)
A

,

m)

λ2αmβm

m + λ1α2
2 (1 + β2
(cid:118)
(cid:117)
(cid:117)
(cid:117)
(cid:116)

λ2 =

M(cid:80)
m=1

4PT

(1+β2
(1+β2

m)α2
mβ2
m
m)2
m+λ1α2

The attacker uses only sensor k∗, and it generates i.i.d. Gaussian output

X ∗

k (i) = θ(i), where θ(i)

(0, PA),

∼ N

10

which is independent of the adversarial sensor input Uk∗ (i). The receiver is the Bayesian estimator of S given Y , i.e.,

h(Y (i)) =

The cost at this saddle-point solution is

(cid:18) M(cid:80)

m=1

(cid:19)

βmcmαm

γ(i) Y (i)

1 + P (cid:48)

A +

(cid:18) M(cid:80)

m=1

(cid:19)2

βmcmαm

+

.

M(cid:80)
m=1

α2

mc2
m

(cid:32)

J AS
C =

1 + λ1

M
(cid:88)

m=1

mβ2
α2
m
m + λ1α2
2 (1 + β2

m)

(cid:33)−1

.

(45)

(46)

Proof: The existence of an essentially unique saddle-point solution follows from the same reasoning in Theorem 1. Let us
take the transmission strategy as given in the theorem statement and derive the optimal attack strategy. Note that essentially,
the attacker’s role is limited to adding Gaussian noise subject to attack power PA, the only remaining question here is how to
allocate its power to the sensors. The objective of the attacker is to maximize the effective channel noise, i.e., to maximize:
M +K(cid:80)
k=M
i.e., the sensor with the largest αk, allocates all adversarial power on this sensor.

PA. The solution of this problem is simply: the attacker picks the best attack channel,

αkE
θ2
k}
{

E
θ2
k} ≤
{

M +K(cid:80)
k=M

subject to

Applying the optimal encoding map given in (41), and given adversary strategy we have the following auxillary expressions

for the terms used in standard MMSE estimation.

Y = γ(i)

SY

E
{

}

= γ(i)

M
(cid:88)

m=1

M
(cid:88)

m=1

cm αmUm(i) +

M +K
(cid:88)

k=M +1

αkXk(i) + Z(i)

βmcmαm, E
Y 2
{

}

= 1 + P (cid:48)

A +

(cid:33)2

βmcmαm

+

(cid:32) M
(cid:88)

m=1

M
(cid:88)

m=1

mc2
α2
m.

The distortion is (observing that σ2

S = 1):

J =1

−

)2

(E
SY
}
{
E
Y 2
}
{

(cid:18) M(cid:80)

m=1

(cid:19)2

βmcmαm

1 + P (cid:48)

A +

(cid:18) M(cid:80)

m=1

(cid:19)2

βmcmαm

+

1 + K 2P +

M(cid:80)
m=1

mc2
α2
m

=1

−

=

1 + P (cid:48)

A +

M(cid:80)
m=1

α2

mc2

m +

(cid:18) M(cid:80)

m=1

βmcmαm

(cid:19)2 .

,

M(cid:80)
m=1

α2

mc2
m

(47)

(48)

(49)

(50)

(51)

PT . We ﬁrst
Then, the problem is to determine cm that minimizes (51) subject to the power constraint,
note that this problem is not convex in cm. By changing the variables, we convert this problem into a convex form which
is analytically solvable. First, instead of minimizing the distortion with a power constraint, we can equivalently minimize the
power with a distortion constraint. Since distortion is a convex function of the total power (otherwise it can be converted to a
convex problem by time sharing), there is no duality gap by this modiﬁcation (cf. [34]). The modiﬁed problem is to minimize:

m ≤

(1 + β2

m)c2

M(cid:80)
m=1

M
(cid:88)

(1 + β2

m)c2
m,

m=1

subject to

1 + P (cid:48)

A +

M(cid:80)
m=1

mc2
α2
m
(cid:18) M(cid:80)

m=1

1 + P (cid:48)

A +

M(cid:80)
m=1

α2

mc2

m +

βmcmαm

(52)

(53)

J.

(cid:19)2 ≤

Note that

Next, we introduce a slack variable

The optimization problem is to minimize

subject to

1
J

= 1 +

(cid:18) M(cid:80)

m=1

(cid:19)2

βmcmαm

1 + P (cid:48)

A +

M(cid:80)
m=1

α2

mc2
m

r =

M
(cid:88)

m=1

αmβmcm.

M
(cid:88)

(1 + β2

m)c2
m,

m=1

1 + P (cid:48)

A +

M
(cid:88)

m=1

mc2
α2

m ≤

(J −1

−

1)−1r2,

and (55). This problem is convex in the variables cm and r. Hence, we construct the Lagrangian cost as

11

(54)

(55)

(56)

(57)

M
(cid:88)

J =

(cid:0)1 + β2

m

(cid:1) c2

m +λ1

(cid:32)

1+ P (cid:48)

A +

M
(cid:88)

mc2
α2

m −

r2
(J −1

1)

(cid:33)

(cid:32)
r

+ λ2

M
(cid:88)

(cid:33)

αmβmcm

,

(58)

−

m=1

where λ1 ∈

R+ and λ2 ∈

m=1
R. The ﬁrst-order conditions for stationarity of the Lagrangian yield:

m=1

−

∂J
∂cm

= 2cm(1+β2

m)+2λ1cmα2

m −

λ2αmβm = 0,

∂J
∂r

=

−

2λ1(J −1

−

1)−1r + λ2 = 0,

1 + P (cid:48)

A +

M
(cid:88)

m=1

mc2
α2

m = (cid:0)J −1

1(cid:1)−1

r2.

−

cm =

λ2αmβm

2 (1 + β2

m + λ1α2

m)

.

λ2
2
4λ1

M
(cid:88)

m=1

mβ2
α2
m
m + λ1α2
(1 + β2

m)

= 1 + P (cid:48)

A +

λ2
2
4

M
(cid:88)

m=1

mβ2
α4
m
m + λ1α2
(1 + β2

m)2

and we have (55) and

From (59), we have

Using (62) in (55), we have

which simpliﬁes to

λ1(1 + P (cid:48)

A) =

λ2
2
4

M
(cid:88)

m=1

α2
mβ2
(1 + β2

m(1 + β2
m)
m)2 =
m + λ1α2

M
(cid:88)

m=1

Pm = PT ⇒

λ1 =

PT
1 + P (cid:48)
A

.

(59)

(60)

(61)

(62)

(63)

(64)

We also have

M
(cid:88)

PT =

(1 + β2

m)c2

m =

m=1

λ2
2
4

M
(cid:88)

m=1

(1 + β2

m)

α2
mβ2
m
m + λ1α2
(1 + β2

m)2 ⇒

λ2 =

(cid:118)
(cid:117)
(cid:117)
(cid:117)
(cid:116)

4PT

M(cid:80)
m=1

(1+β2
(1+β2

m)α2
mβ2
m
m+λ1α2
m)2

Plugging the expressions of λ1 and λ2 in (60), we obtain the equilibrium cost.

Remark 4. If Assumption 1 is replaced with Assumption 2 in setting I, coordination becomes redundant for the attacker. This
is because the optimal attack strategy uses only one sensor, and there is no need to coordinate (generate the same realization
of θ(i)).

Remark 5. The optimal strategies can be computed for each sensor in a decentralized manner. The central agent can compute
the optimal values of λ1 and λ2 and then broadcast this information to all sensors. Next, each transmitter sensor can compute
its own mapping based on local parameters αm and βm and the broadcasted global parameters λ1 and λ2.

12

Finally, we analyze the asymmetric setting where the sensors are not allowed to coordinate. We characterize the policies

achieving the Stackelberg equilibrium and associated cost in the following theorem.

Theorem 5. For setting II, and under Assumption 2, the encoding functions for the transmitter and the adversarial sensors at
the Stackelberg equilibrium are:

Xm(i) = cm Um(i), 1

m

≤

≤

M, Xk(i) = ck Uk(i), M + 1

k

≤

≤

M + K

where

and λ1 ∈

R, λ2 ∈

R+, λ3 ∈
2PA + 2λ1

λ2 =

−

and

R, λ4 ∈
(cid:18)
M(cid:80)
1
m=1

−

M(cid:80)
m=1

αmβmcm

cm =

λ4αmβm

,

ck =

λ2αkβk

.

2 (1 + β2

λ1α2
k)
R+ are constants that satisfy the following equations:

m + λ3α2

2 (1 + β2

k −

m)

mc2
α2
m

(cid:19)

,







PA + λ1

(cid:18)
1

M(cid:80)
m=1

−

mc2
α2
m

M(cid:80)
m=1

αmβmcm

(cid:19)

2







M +K
(cid:88)

(1 + β2

k)α2

k=M +1

(1 + β2

k −

kβ2
k
k)2 = PA.
λ1α2

λ4λ1 =

−

λ2λ3,

1 =

PT
λ1

+

PA
λ3

,

λ2
4
4

M
(cid:88)

m=1

mβ2
α2
(1 + β2

m(1 + β2
m)
m)2 = PT .
m + λ3α2

The optimal receiver is the Bayesian estimator of S given Y , i.e.,

h(Y (i)) =

1 +

M +K(cid:80)
m=1

The cost at this Stackelberg equilibrium is

M +K(cid:80)
m=1

βmcmαm

α2

mc2

m +

(cid:18)M +K(cid:80)

m=1

βmcmαm

(cid:19)2 Y (i).

(cid:32)

J AS
N C(M, K) =

1 + λ3

M
(cid:88)

m=1

mβ2
α2
m
m + λ3α2
2 (1 + β2

m) −

λ1

M +K
(cid:88)

k=M +1

kβ2
α2
k
2 (1 + β2

k −

λ1α2
k)

(cid:33)−1

.

(65)

(66)

(67)

Proof: Since Player 1 (the transmitter sensors and the receiver) is the leader of this Stackelberg game and the adversarial
sensors are the followers, we ﬁrst compute the best response of the attacker to the given transmitter strategy and associated
receiver policy. By the reasoning in Theorem 2, we conclude that the best adversary strategy is to use linear maps as given
[M + 1 : M + K] as a
in the theorem statement. In the following, we compute the optimal adversary coefﬁcients, ck, k
function of cm, m

∈
[1 : M ]. We ﬁrst compute the expressions used in the MMSE computations as:

∈

Y =

E
SY
{

}

=

M +K
(cid:88)

m=1

M +K
(cid:88)

m=1

cm αmUm + Z

βmcmαm, E
Y 2
{

}

= 1 +

(cid:32)M +K
(cid:88)

m=1

(cid:33)2

βmcmαm

+

M +K
(cid:88)

m=1

mc2
α2
m.

The objective of the attacker is to maximize

J = 1

−

(E
)2
SY
}
{
E
Y 2
}
{

=

1 +

M +K(cid:80)
m=1

1 +

M +K(cid:80)
m=1

mc2
α2
m
(cid:18)M +K(cid:80)

m=1

α2

mc2

m +

βmcmαm

(cid:19)2 .

over ck, k

∈

[M + 1 : M + K] that satisfy

M +K
(cid:88)

(1 + β2

k)c2

PA.

k ≤

k=M +1

(68)

(69)

(70)

(71)

This problem is again non-convex, hence we follow the approach we used in the proof of Theorem 4: we ﬁrst introduce a
slack variable.

rK =

αkβkck,

(72)

M +K
(cid:88)

k=M +1

and apply the KKT optimality conditions. The stationarity conditions applied to the following Lagrangian cost

M +K
(cid:88)

JA =

(cid:0)1 + β2

k

(cid:1) c2

k +λ1

k=M +1

(cid:32)

(rK +

M
(cid:88)

m=1

αmβmcm)2(J −1

1)−1

1

−

−

−

(cid:33)

mc2
α2
m

+ λ2

M +K
(cid:88)

m=1

(cid:32)
rK −

M +K
(cid:88)

k=M +1

(cid:33)

αkβkck

, (73)

13

where λ1 ∈

R+ and λ2 ∈

R, yield

∂JA
∂ck

= 2ck(1+β2
k)

2λ1ckα2

k −

−

λ2αkβk = 0,

∂JA
∂rK

= 2λ1(J −1

1)−1(rK +

−

M
(cid:88)

m=1

αmβmcm) + λ2 = 0,

1 +

M
(cid:88)

m=1

mc2
α2

m +

M +K
(cid:88)

k=M +1

kc2
α2

k = (cid:0)J −1

1(cid:1)−1

(rK +

−

M
(cid:88)

m=1

αmβmcm)2.

and we have (72) and

From (74), we have

ck =

λ2αkβk

2 (1 + β2

k −

λ1α2
k)

.

Using (77) in (72), we have

λ2
−
2λ1

M
(cid:88)

m=1

αmβmcm −

λ2
2
4λ1

M +K
(cid:88)

k=M +1

kβ2
α2
k
λ1α2
(1 + β2
k)

k −

= 1 +

M
(cid:88)

m=1

mc2
α2

m +

λ2
2
4

M +K
(cid:88)

k=M +1

kβ2
α4
k
λ1α2
(1 + β2

k)2

k −

which simpliﬁes to

or

λ2
−
2λ1

M
(cid:88)

m=1

αmβmcm −

1

−

M
(cid:88)

m=1

mc2
α2

m =

λ2
2
4λ1

M +K
(cid:88)

k=M +1

kβ2
α2
(1 + β2

k(1 + β2
k)
k)2 = PA/λ1
λ1α2

k −

λ2

−

M
(cid:88)

m=1

αmβmcm −

2λ1(1

M
(cid:88)

−

m=1

mc2
α2

m) = 2PA ⇒

λ2 =

−

(cid:18)

2PA + 2λ1

1

M(cid:80)
m=1

−

(cid:19)

mc2
α2
m

M(cid:80)
m=1

αmβmcm

Plugging (77) in (71), we have

λ2
2
4

M +K
(cid:88)

(1 + β2

k)α2

k=M +1

(1 + β2

k −

kβ2
k
k)2 =
λ1α2







PA + λ1

(cid:18)
1

M(cid:80)
m=1

−

mc2
α2
m

M(cid:80)
m=1

αmβmcm

(cid:19)

2







M +K
(cid:88)

(1 + β2

k)α2

k=M +1

(1 + β2

k −

kβ2
k
k)2 = PA
λ1α2

(74)

(75)

(76)

(77)

(78)

(79)

(80)

(81)

The unique positive solution of (81) provides the value of λ1 and by (80), λ2 can be computed, once λ1 is obtained. Having
[1 : M ], we next derive cm that
obtained the optimal ck, k
minimize (70) subject to

[M + 1 : M + K] values as a function of PA and cm, m

∈

∈

M
(cid:88)

m=1

(1 + β2

m)c2

PT .

m ≤

Again, we modify the problem as to minimize

M(cid:80)
m=1

(1 + β2

m)c2

m subject to

and

1 +

M +K
(cid:88)

m=1

(cid:32)

mc2
α2

m ≤

(J −1

−

1)−1

r +

(cid:33)2

αkβkck

,

M +K
(cid:88)

k=M +1

r =

M
(cid:88)

m=1

αmβmcm.

(82)

(83)

(84)

14

The stationarity conditions applied to the following Lagrangian cost

M
(cid:88)

JT =



(cid:0)1 + β2

m

(cid:1) c2

m +λ3

1 +

m=1

M +K
(cid:88)

m=1

mc2
α2

m −

(cid:32)

r +

M +K
(cid:88)

k=M +1

(cid:33)2



αkβkck

(J −1

−

1)−1

 + λ4

(cid:32)
r

M
(cid:88)

−

m=1

(cid:33)

αmβmcm

,

(85)

for λ3 ∈

R+ and λ4 ∈

R yield

∂JT
∂r

=

2λ3(J −1

−

−

(cid:32)

1)−1

r +

M +K
(cid:88)

k=M +1

(cid:33)

αkβkck

+ λ4 = 0,

(86)

∂JT
∂cm

= 2cm(1+β2

m)+2λ3cmα2

m + 2λ3

M +K
(cid:88)

kckc(cid:48)
α2

k −

k=M +1

(cid:32)

2λ3(J −1

−

1)−1

r +

M +K
(cid:88)

αkβkck

(cid:33) M +K
(cid:88)

k=M +1

k=M +1

αkβkc(cid:48)

k −

λ4αmβm

= 2cm(1+β2

m)+2λ3cmα2

m + 2λ3

M +K
(cid:88)

k=M +1

kckc(cid:48)
α2

k −

λ4

M +K
(cid:88)

k=M +1

αkβkc(cid:48)

k −

λ4αmβm = 0

where c(cid:48)

k = ∂ck
∂cm

and (87) follows from (86). We also have (84) and

1 +

M
(cid:88)

m=1

mc2
α2

m +

M +K
(cid:88)

k=M +1

kc2
α2

k = (cid:0)J −1

(cid:32)

r +

1(cid:1)−1

M +K
(cid:88)

(cid:33)2

αkβkck

−
(cid:32)

r +

k=M +1
(cid:33)

αkβkck

M +K
(cid:88)

k=M +1

=

λ4
2λ3

as necessary conditions of optimality. Comparing (89) and (75), we have

We next use (90) to rewrite the terms involving c(cid:48)
k:

λ4
λ3

=

λ2
λ1

.

−

(87)

(88)

(89)

(90)

M +K
(cid:88)

2λ3

k=M +1

kckc(cid:48)
α2

k −

M +K
(cid:88)

λ4

k=M +1

αkβkc(cid:48)

k =

∂
∂cm

(cid:32)

2λ1

M +K
(cid:88)

kc2
α2

k + λ2

M +K
(cid:88)

(cid:33)

αkβkck

=

k=M +1

k=M +1

∂
∂cm

PA = 0

(91)

Using (91) in (87), we obtain

cm =

Plugging (92) and (77) in (89) and using (90), we have

λ4αmβm

2 (1 + β2

m + λ3α2

m)

(92)

1+

λ2
4
4

M
(cid:88)

m=1

α4
mβ2
m
m + λ3α2
(1 + β2

m)2 +

λ2
2
4

M +K
(cid:88)

k=M +1

which yields, after algebraic manipulations,

We also have

kβ2
α4
k
(1 + β2
λ1α2

k)2 =

k −

λ2
4
4λ3

M
(cid:88)

m=1

α2
mβ2
m
(1 + β2
m + λ3α2

m) −

λ2
2
4λ1

M +K
(cid:88)

k=M +1

kβ2
α2
k
(1 + β2
λ1α2
k)

k −

1 =

PT
λ1

+

PA
λ3

λ2
4
4

M
(cid:88)

m=1

α2
mβ2
(1 + β2

m(1 + β2
m)
m)2 = PT .
m + λ3α2

(93)

(94)

The set of equations (80, 81, 90, 93, 94) (essentially) uniquely characterizes the variables λ1, λ2, λ3 and λ4. Plugging these
variables into (86), we obtain the equilibrium cost.

Remark 6. We again observe that, as noted in Remark 5, the optimal power allocation admits a decentralized implementation:
a central agent can compute and broadcast the values of constants λi, i = 1, . . . , 4 and the sensors can implement optimal
communication strategies using the local information αm and βm and these universal constants. The same interpretation also
holds for the Byzantine sensors.

15

V. DISCUSSION AND CONCLUSION

In this paper, we have conducted a game-theoretical analysis of joint source-channel communication over a Gaussian sensor
network with Byzantine sensors. Depending on the coordination capabilities of the sensors, we have analyzed three problem
settings. The ﬁrst setting allows coordination among the transmitter sensors, ﬁrst for the totally symmetric case. Coordination
capability enables the transmitters to use randomized encoders. The saddle-point solution to this problem is randomized uncoded
transmission for the transmitters and the coordinated generation of i.i.d. Gaussian noise for the adversarial sensors. In the second
setting, transmitter sensors cannot coordinate, and hence they use ﬁxed, deterministic mappings. The solution to this problem
is shown to be uncoded communication with linear mappings for both the transmitter and the adversarial sensors, but with
opposite signs. We note that coordination aspect of the problem is entirely due to game-theoretic considerations, i.e., if no
adversarial sensors exist, the transmitters do not need coordination. In the third setting, where only a fraction of sensors can
coordinate, the solution depends on the number of transmitter and adversarial sensors that can coordinate. If the gain from
coordination for the transmitter sensors and the receiver, is sufﬁciently high, only the coordination-capable transmitter sensors
are used. Then, the problem simpliﬁes to an instance of setting I, i.e, there exists a unique saddle-point solution achieved
by randomized linear mappings as the transmitter and the receiver strategy and independent noise as the adversarial strategy.
Otherwise, the transmitters do not utilize coordination, all available transmitter sensors are used, and the problem becomes an
instance of setting II: a saddle-point solution does not exist and the Stackelberg equilibrium is achieved by deterministic linear
strategies.

Our analysis has uncovered an interesting result regarding coordination among the transmitter sensors and the receiver, and
among the adversarial nodes. If the transmitter nodes can coordinate, then the adversaries will beneﬁt from coordination, i.e.,
all will generate the identical realization of an i.i.d. Gaussian noise sequence. If the transmitters cannot coordinate, adversarial
sensors do not beneﬁt from coordination, and the resulting Stackelberg equilibrium is at strictly higher cost than the one when
transmitters can coordinate (setting I).

Finally, we have analyzed the impact of optimal power allocation among both the transmitter (defender) and the adversarial
(attacker) sensors when various parameters that deﬁne the game are not the same for all sensors–the asymmetric case. We have
shown that the optimal attack strategy, when the defender can coordinate, allocates all attack power on the best sensor, where
the criteria of the selection of best sensor pertains to the receiver SNR. Moreover, the ﬂexibility of power allocation renders
coordination superﬂuous for the adversarial sensors, while it remains beneﬁcial for the transmitter sensors. In the absence
of coordination, both the optimal transmitter and the optimal attacker strategies use all available sensors to distribute power
optimally.

Several questions still remain open and are currently under investigation, including extensions of the analysis to vector
sources and channels. The information-theoretic analysis of such a setting requires a vector form of Witsenhausen’s Lemma,
which is an important research question in its own right, see [35] for recent progress in this direction. The investigation of
optimal power allocation strategies for asymmetric settings for vector sources and channels, and the scaling analysis, in terms
of the number sensors, are parts of our current research.

APPENDIX A
THE GAUSSIAN CEO PROBLEM

In the Gaussian CEO problem, an underlying Gaussian source S
∼
(0, RW ) as U = S + W . These noisy observations, i.e., U , must be encoded in such a way that the decoder produces a
N
good approximation to the original underlying source. This problem was proposed in [36] and solved in [37] (see also [38],
[39]). A lower bound for this function for the non-Gaussian sources within the “symmetric” setting where all U ’s have identical
statistics was presented in [40]. Here, we simply extend the results in [38] to our setting, noting

S) is observed under additive noise W

∼ N

(0, σ2

ˆS)2
D = E
(S
}
{
R = min I(U ; ˆS),

−

,

(95)

(96)

where U = βS +W , W
u) that satisfy (95). The MSE distortion can be written as sum of two terms
densities p(ˆs
|

(0, RW ), and RW is an M

∼ N

×

M identity matrix. The minimization in (96) is over all conditional

D =E
{

(S

−

T + T

−

ˆS)2

= E
(S
{

T )2

}

−

+ E
(T
{

}

ˆS)2

,
}

−

where T (cid:44) E
S
{

|

U

. Note that (97) holds since
}

as the estimation error, S
is constant with respect to p(ˆs
|

−

E
(S
{

T )( ˆS

T )

= 0,

}
T is orthogonal to any function7 of the observation, U . The estimation error Dest (cid:44) E
(S
{

T )2
}
u), i.e., a ﬁxed function of U and S. Hence, the minimization is over the densities that satisfy

−

−

−

7Note that ˆS is also a deterministic function of U , since the optimal reconstruction can always be achieved by deterministic codes.

(97)

(98)

16

a distortion constraint of the form E
(T
{

−

ˆS)2

} ≤

Drd and R = min I(U ; ˆS). Hence, we write (97) as

D = Drd + Dest.

(99)

Note that due to their Gaussianity, T is a sufﬁcient statistic of U for S, i.e., S
T ). Hence, R = min I(U ; ˆS) = min I(T ; ˆS) where minimization is over p(ˆs
T
|
where all variables are Gaussian. This is the classical Gaussian rate-distortion problem, and hence:

U forms a Markov chain in that order and
Drd,

t) that satisfy E
(T
{

(0, σ2

∼ N

ˆS)2

} ≤

−

−

−

T

Note that T = RSU R−1

U U , where RSU (cid:44) E
{

which can be written explicitly as:

T 2−2R.
Drd(R) = σ2
and RU (cid:44) E
U U T
{

}

SU T


}
1 + β2
1
β1β2
...
β1βM

RU =






. . .
. . .
. . .

β1β2
1 + β2
2

. . .

β1βM
β2βM
...
1 + β2
M








.

(100)

(101)

Since RU is structured, it can easily be manipulated. In particular, RU admits an eigen-decomposition RU = QT
QU is unitary and Λ is a diagonal matrix with elements 1, . . . , 1, 1 + (cid:80)

m. We compute σ2

T as

U ΛQU where

T = RSU R−1
σ2

U RT

SU = σ2
S

β2
m

m β2
M(cid:80)
m=1
M(cid:80)
m=1

1 +

and using standard linear estimation principles, we obtain

Dest = σ2
S

1
M(cid:80)
m=1

.

β2
m

1 +

Plugging (103) in (100) and using (99) yields







D = σ2
S

1
M(cid:80)
m=1

+

β2
m

1 +

β2
m

M(cid:80)
m=1
M(cid:80)
m=1

1 +

β2
m

,

β2
m







.

2−2R

(102)

(103)

(104)

In this section, we recall Witsenhausen’s lemma [41], which is used in the proof of Theorem 2.

APPENDIX B
WITSENHAUSEN’S LEMMA

Lemma 1. Consider a pair of random variables X and Y , generated from a joint density PX,Y , and two (Borel measurable)
arbitrary functions f, g : R

R satisfying

→

E
f (X)
{
}
f 2(X)
}

E
{

= E
g(Y )
{
}
=E
g2(Y )
{

=0,

=1.
}

Deﬁne

ρ∗ (cid:44) sup
f,g

E
f (X)g(Y )
{

}

Then for any (Borel measurable) functions fN , gN : RN

R satisfying

→
E
fN (X)
{
}
E
f 2
N (X)
{
}
for length N vectors sampled from the independent and identically distributed random sequences
N
i=1, we have
each X(i), Y (i) pair is generated from PX,Y , as X =
Y (i)
{
}
ρ∗.

=E
gN (Y )
{
}
=E
g2
N (Y )
}
{

X(i)
{
}
fN (X)gN (Y )

N
i=1 and Y =

= 1,

= 0,

sup
fN ,gN

E
{

} ≤

(105)

(106)

(107)

(108)

(109)

X(i)
}
{

and

,where
Y (i)
}
{

(110)

Moreover, the supremum and the inﬁmum above are attained by linear mappings, if PX,Y is a bivariate normal density.

17

REFERENCES

[1] K. Kim and P. R. Kumar, “Cyber–physical systems: A perspective at the centennial,” Proceedings of the IEEE, Special Centennial Issue, vol. 100, pp.

1287–1308, 2012.

[2] H. Sandberg, S. Amin, and H. Johansson, “Cyberphysical security in networked control systems: An introduction to the issue,” IEEE Control Systems,

vol. 35, no. 1, pp. 20–23, 2015.

[3] H. Fawzi, P. Tabuada, and S. Diggavi, “Secure estimation and control for cyber-physical systems under adversarial attacks,” IEEE Transactions on

Automatic Control, vol. 59, no. 6, pp. 1454–1467, 2014.

[4] F. Pasqualetti, F. D¨orﬂer, and F. Bullo, “Attack detection and identiﬁcation in cyber-physical systems,” IEEE Transactions on Automatic Control, vol.

58, no. 11, pp. 2715–2729, 2013.

[5] Y. Mo, R. Chabukswar, and B. Sinopoli, “Detecting integrity attacks on SCADA systems,” IEEE Transactions on Control Systems Technology, vol. 22,

no. 4, pp. 1396–1407, 2014.

[6] L. Lamport, R. Shostak, and M. Pease, “The Byzantine generals problem,” ACM Transactions on Programming Languages and Systems (TOPLAS), vol.

4, no. 3, pp. 382–401, 1982.

[7] D. Dolev, “The Byzantine generals strike again,” Journal of Algorithms, vol. 3, no. 1, pp. 14–30, 1982.
[8] A. Vempaty, L. Tong, and P. K. Varshney, “Distributed inference with Byzantine data: State-of-the-art review on data falsiﬁcation attacks,” IEEE Signal

Processing Magazine, vol. 30, no. 5, pp. 65–75, Sept 2013.

[9] O. Kosut and L. Tong, “Distributed source coding in the presence of Byzantine sensors,” IEEE Transactions on Information Theory, vol. 54, no. 6, pp.

2550–2565, 2008.

[10] E. Akyol, C. Langbort, and T. Bas¸ar, “Information-theoretic approach to strategic communication as a hierarchical game,” Proceedings of the IEEE,

vol. 105, no. 2, pp. 205–218, Feb 2017.

[11] R. Langner, “Stuxnet: Dissecting a cyberwarfare weapon,” IEEE Security & Privacy, vol. 9, no. 3, pp. 49–51, 2011.
[12] E. Akyol, C. Langbort, and T. Bas¸ar, “Strategic communication in multi-agent networks,” in Proceedings of the IEEE Asilomar Conference on Signals,

Systems and Computers, 2016. IEEE.

[13] M. Gastpar and M. Vetterli,

“Power, spatio-temporal bandwidth, and distortion in large sensor networks,”

IEEE Journal on Selected Areas in

Communications, vol. 23, no. 4, pp. 745–754, April 2005.

[14] T. Bas¸ar, “The Gaussian test channel with an intelligent jammer,” IEEE Transactions on Information Theory, vol. 29, no. 1, pp. 152–157, 1983.
[15] T. Bas¸ar and Y.W. Wu, “A complete characterization of minimax and maximin encoder-decoder policies for communication channels with incomplete

statistical description,” IEEE Transactions on Information Theory,, vol. 31, no. 4, pp. 482–489, 1985.

[16] T. Bas¸ar and Y.W. Wu, “Solutions to a class of minimax decision problems arising in communication systems,” Journal of Optimization Theory and

Applications, vol. 51, no. 3, pp. 375–404, 1986.

[17] R. Bansal and T. Bas¸ar, “Communication games with partially soft power constraints,” Journal of Optimization Theory and Applications, vol. 61, no.

3, pp. 329–346, 1989.

[18] J. Xiao, S. Cui, Z. Luo, and A. Goldsmith, “Linear coherent decentralized estimation,” IEEE Transactions on Signal Processing, vol. 56, no. 2, pp.

757–770, 2008.

[19] J. Li and G. AlRegib, “Distributed estimation in energy-constrained wireless sensor networks,” IEEE Transactions on Signal Processing, vol. 57, no.

10, pp. 3746–3758, Oct 2009.

[20] A. Ribeiro and G.B. Giannakis, “Bandwidth-constrained distributed estimation for wireless sensor networks-part i: Gaussian case,” IEEE Transactions

on Signal Processing, vol. 54, no. 3, pp. 1131–1143, March 2006.

[21] J. Jin, A. Ribeiro, Luo Z.Q., and G.B. Giannakis, “Distributed compression-estimation using wireless sensor networks,” IEEE Signal Processing

Magazine, vol. 23, no. 4, pp. 27–41, July 2006.

[22] I. Bahceci and A.K. Khandani, “Linear estimation of correlated data in wireless sensor networks with optimum power allocation and analog modulation,”

IEEE Transactions on Communications, vol. 56, no. 7, pp. 1146–1156, July 2008.

[23] F. Jiang, J. Chen, and A.L. Swindlehurst, “Optimal power allocation for parameter tracking in a distributed amplify-and-forward sensor network,” IEEE

Transactions on Signal Processing, vol. 62, no. 9, pp. 2200–2211, May 2014.

[24] H. Behroozi and M.R. Soleymani,

“On the optimal power-distortion tradeoff in asymmetric Gaussian sensor network,”

IEEE Transactions on

Communications, vol. 57, no. 6, pp. 1612–1617, June 2009.

[25] M. Gastpar, “Uncoded transmission is exactly optimal for a simple Gaussian sensor network,” IEEE Transactions on Information Theory, vol. 54, no.

11, pp. 5247–5251, 2008.

[26] A. El Gamal and Y. Kim, Network Information Theory, Cambridge University Press, 2011.
[27] A. Lapidoth and S. Tinguely, “Sending a bivariate Gaussian over a Gaussian MAC,” IEEE Transactions on Information Theory, vol. 56, no. 6, pp.

2714–2752, June 2010.

[28] A. Leong and S. Dey, “On scaling laws of diversity schemes in decentralized estimation,” IEEE Transactions on Information Theory, vol. 57, no. 7,

pp. 4740–4759, 2011.

[29] E. Akyol, K. Rose, and T. Bas¸ar, “Gaussian sensor networks with adversarial nodes,” in Information Theory Proceedings (ISIT), 2013 IEEE International

Symposium on. IEEE, 2013, pp. 539–543.

[30] E. Akyol, K. Rose, and T. Bas¸ar, “On communication over Gaussian sensor networks with adversaries: Further results,” in Decision and Game Theory

for Security, pp. 1–9. Springer, 2013.

[31] E. Akyol and U. Mitra, “Power-distortion metrics for path planning over Gaussian sensor networks,” IEEE Transactions on Communications, vol. 64,

no. 3, pp. 1220–1231, 2016.

[32] T. Bas¸ar and G. Olsder, Dynamic Noncooperative Game Theory, Society for Industrial Mathematics (SIAM) Series in Classics in Applied Mathematics,

1999.

[33] S.N. Diggavi and T.M. Cover, “The worst additive noise under a covariance constraint,” IEEE Transactions on Information Theory, vol. 47, no. 7, pp.

3072–3081, 2001.

[34] S. Boyd and L. Vandenberghe, Convex Optimization, Cambridge University Press, 2004.
[35] C. Tian, J. Chen, S. Diggavi, and S. Shamai, “Matched multiuser Gaussian source-channel communications via uncoded schemes,” in 2015 IEEE

International Symposium on Information Theory (ISIT). IEEE, 2015, pp. 476–480.

[36] H. Viswanathan and T. Berger, “The quadratic Gaussian CEO problem,” IEEE Transactions on Information Theory, vol. 43, no. 5, pp. 1549–1559, 1997.
[37] Y. Oohama, “Rate-distortion theory for Gaussian multiterminal source coding systems with several side informations at the decoder,” IEEE Transactions

on Information Theory, vol. 51, no. 7, pp. 2577–2593, 2005.

[38] Y. Oohama, “The rate-distortion function for the quadratic Gaussian CEO problem,” IEEE Transactions on Information Theory, vol. 44, no. 3, pp.

1057–1070, 1998.

[39] V. Prabhakaran, D. Tse, and K. Ramachandran, “Rate region of the quadratic Gaussian CEO problem,” in Proceedings of the International Symposium

on Information Theory. IEEE, 2004, p. 119.

[40] M. Gastpar, “A lower bound to the AWGN remote rate-distortion function,” in IEEE 13th Workshop on Statistical Signal Processing. IEEE, 2005, pp.

1176–1181.

[41] H.S. Witsenhausen, “On sequences of pairs of dependent random variables,” SIAM Journal on Applied Mathematics, pp. 100–113, 1975.

