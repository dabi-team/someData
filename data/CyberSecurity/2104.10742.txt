Robustness of ML-Enhanced IDS to Stealthy Adversaries

Vance Wong∗

John Emanuello †

April 23, 2021

1
2
0
2

r
p
A
1
2

]

R
C
.
s
c
[

1
v
2
4
7
0
1
.
4
0
1
2
:
v
i
X
r
a

Abstract

Intrusion Detection Systems (IDS) enhanced with Machine
Learning (ML) have demonstrated the capacity to eﬃciently
build a prototype of “normal” cyber behaviors in order
to detect cyber threats’ activity with greater accuracy
than traditional rule-based IDS. Because these are largely
black boxes, their acceptance requires proof of robustness
to stealthy adversaries. Since it is impossible to build a
baseline from activity completely clean of that of malicious
cyber actors (outside of controlled experiments), the training
data for deployed models will be poisoned with examples
of activity that analysts would want to be alerted about.
We train an autoencoder-based anomaly detection system
on network activity with various proportions of malicious
activity mixed in and demonstrate that they are robust to
this sort of poisoning.

1

Introduction

Malicious cyber activity is ubiquitous and the so-
cietal and economic impacts of sophisticated cyber at-
tacks are immense[13, 21]. Given the shortage of cy-
bersecurity professionals, automated tools are required
[3, 6]. However, these so-called intrusion detection sys-
tems (IDS), which are largely rules-based systems, of-
ten fail to detect novel cyber attacks and produce more
alerts than analysts can address.

The success of machine learning architectures in
cybersecurity has created a new generation of IDS which
show promise to be critical components of automated
defense systems[5, 15, 10, 22]. Unlike prior iterations
of these utilities, which relied on frail rules incapable
of adapting to changing cyber threats, these can learn
complex behaviors indicative of normal user activity
and detect those which deviate from the baseline. This
has the potential to enable detection of novel malicious
activity, including zero-day attacks which the model has
never seen[16]. Such techniques, including autoencoder
(AE) anomaly detectors, have been shown to be superior

to rules-based IDS in detecting malicious activity on
ground-truth data1[18].

The ability of sophisticated adversaries to cover
their tracks and make their activities look normal can
potentially degrade the performance of these anomaly
detection schemes if they are not carefully controlled.
Because these are black-box techniques it can be diﬃ-
cult to know if they are learning the right characteristics
of normal activity.

In this work, we introduce a combined feature en-
gineering technique and AE-based anomaly detection
scheme and demonstrate its robustness to stealthy ad-
versaries. The features we engineer are derived from a
technique traditionally applied to natural language pro-
cessing, and, by construction, are directly relevant to
the task of detecting anomalous behaviors using an AE.
We demonstrate that our pipeline is robust to stealthy
adversaries, by training on acceptable levels of malicious
behaviors and demonstrating that performance is main-
tained. We believe this sort of robustness demonstra-
tion to be the ﬁrst of its kind. In a departure from some
research, we conduct experiments on publicly available
data, which enables our technique to serve as a baseline
for others to improve upon.

Our paper is organized as follows: in Section 2 we
discuss an architecture for a neural IDS which is similar
to others seen in the literature, introduce an open-source
data set, and lay out the experiments we conduct; in
Section 3 we present results and discuss the impact
thereof in Section 4.

2 Proposed Model and Experimental Paradigm

In this work we make use of an autoencoder-based
anomaly detection scheme. Autoencoders (AE) are
neural networks, trained in an unsupervised fashion,
wherein the target function to be learned is an approx-
imate identity on the input space[4]. These networks
are usually decomposed into non-linear encoder and de-
coder functions such that the image of the encoder h is

∗Laboratory for Advanced Cybersecurity Research, National

Security Agency

†Laboratory for Advanced Cybersecurity Research, National

Security Agency

1The use of autoencoders for anomaly detection were previ-
ously shown to be eﬀective in image forensics [2] and in manufac-
turing [8].

 
 
 
 
 
 
of much smaller dimension than the input space:

f (x) = g ◦ h(x),

In essence, the AE is a lossy compression-decompression
scheme, with the property that reconstruction will be
poor on data drawn from a diﬀerent distribution than
the one that generated the training data. Thus, an
AE can be used as part of an anomaly detection
(1) Train an AE on data drawn
pipeline[2, 8, 18]:
from a distribution of interest; (2) Feed data potentially
drawn from a diﬀerent distribution, and measure recon-
struction error (cid:107)x − f (x)(cid:107) (errors above a threshold are
deemed anomalous). By construction, AEs learn salient
information that is characteristic of the training data,
and hence are robust to the presence of anomalies in the
training data 2.

The input of an AE must be numeric data, and
given that cybersecurity data is largely nominal, we
employ feature engineering to transform the data into
a meaningful representation that is (1) compatible with
a neural network architecture and (2) directly relevant
to the discovery of anomalous behavior. Rather than
employ an ad-hoc feature engineering technique, we
adapt word2vec to netﬂow as others have demonstrated
in the literature [9, 19, 20].

In essence, word2vec refers to a collection of natural
language processing (NLP) techniques, which embeds
words from a set of documents into a real vector
space, such that semantics are encoded in geometric
properties of the vectors. Training these embeddings
involves associating words with their contexts (i.e. their
neighboring words in the corpus of documents) so
that statistical associations between target words and
their context words are encoded into the embedding
[1, 7, 11, 12].

This methodology is easily adaptable to a cyber-
security context, wherein the “words” are entities ap-
pearing in log data (such as NetFlow or host logs) and
the contexts are the other entities appearing in the log.
Thus the word2vec paradigm results in a vector space
that places entities with similar behaviors nearby (e.g.
IPs are clustered together with other IPs and ﬁle paths
which are loaded in similar processes are neighbors) [20].
When only benign records are used to train entity vec-
tors, we are encoding the data based on what constitutes
benign cyber activities on the network.

2.1 Skip-Gram with Negative Sampling We
adapt the Skip-Gram with Negative Sampling (SGNS)

2However, what constitutes an “anomaly” in the data may not
actually be indicative of an anomalous behavior with respect to
cybersecurity. The feature engineering technique we propose does
ensure this correlation holds.

variant of word2vec, whose goal is to learn to discrimi-
nate between pairs of words that have been observed in
the same context (positive pairs), and word pairs that
have not (negative pairs) [12]. Given a pool of posi-
tive and negative pairs, term vector components (i.e.
the embedding layer of the model) are updated via gra-
dient descent to optimize binary cross entropy (BCE)
loss (with user generated positive or negative labels as
In this
ground truth). See Figure 1 for more detail.
work, positive pairs are sampled (with some acceptance
probability) from pairs of entities that appear in the
same netﬂow record. Negative pairs are generated by
randomly selecting an entity among all entities in the
training set to replace the context word observed in the
positive pair.

2.2 Dual Pipeline with UNSW-NB15 Data Set
We employ the UNSW-NB15 data set to test our im-
plementation of the SGNS methodology. This labeled,
synthetic data set consists of enriched netﬂows contain-
ing examples of normal network traﬃc, as well as ex-
amples of cyber attacks from nine diﬀerent attack cat-
egories, including exploits and fuzzers [14, 17, 15, 16].
While the data contain features from bro and argus,
we consider only 10 ﬁelds:
source/destination IPs,
source/destination ports, type of service, durations, and
source/destination byte and packet counts. To reduce
the complexity of the model, we only consider those
netﬂows which utilized the TCP protocol (which con-
stitutes roughly 1.5M of the 2.5M total netﬂows in the
entire UNSW corpus).

The pipeline is composed of two parts: a word2vec
embedder and an AE anomaly detector, which is similar
to that of Ramstrom [19]. These components are not
trained altogether in end-to-end fashion. Word vectors
are trained, and then used to construct input to train
the AE; reconstruction errors are not back-propagated
to adjust the word vector representation. In contrast to
the implementation of Ramstrom, which feeds numeric
features from the ﬂows directly to the AE, we discretize
numeric ﬁelds (into quantiles) and embed them, thereby
treating them on equal footing with categorical features.
In the ﬁrst (word2vec) phase, the vocabulary con-
sists of all items found among the positive pairs sampled
from the TCP training corpus. In one experiment the
positive pairs come only from benign ﬂows, while in an-
other we poison the training set with malicious ﬂows.
The negative to positive pair ratio is 7:1, in order to
ensure suﬃcient information is propagated to the em-
beddings [12].

To represent a netﬂow record, we concatenate the
SGNS trained embeddings corresponding to the entities
in the record, and use this as the input to an AE

with a single hidden (code) layer. We train an AE
using mean squared error loss (MSE). A hold out set
is then fed through the trained AE and reconstruction
errors for each are measured. When deployed, records
with unusually high reconstruction error would then be
reported as suspicious.

2.3 Experimental Paradigm We propose a simple
set of experiments: (1) train the pipeline (word vectors
and autoencoder) on only benign NetFlows; (2) train
the pipeline from scratch with a suﬃcient number of
records corresponding to the exploits class of attacks so
that roughly 0.75% of the training data is malicious; (3)
a re-run of (2), but with roughly 1.5% of the training
data coming from malicious ﬂows. We compare distri-
butions of reconstruction errors on benign ﬂows and two
classes of malicious traﬃc: exploits and fuzzers. We also
report true positive (malicious) and true negative (be-
nign) rates as well as their harmonic mean, which we
call the “F1”, as a function of MSE cutoﬀ. This will
demonstrate the eﬀect of the poisoning on the ability of
the AE to detect anomalies. Poisoning levels are based
on the widely accepted notion that malicious activity is
rare (below 1%) in the records as compared to normal
activities within an enterprise network, a testament to
strong ﬁrewalls.

3 Results

We ﬁrst present results for an AE trained solely on
benign traﬃc. Figure 3 and Figure 2 show performance
metrics for detecting malicious traﬃc in the test set
from the exploits and fuzzers attack classes (resp.).
As seen in the ﬁgures, the pipeline is largely able to
separate benign traﬃc from both kinds of malicious
traﬃc. As expected, poisoning the training set with
exploits examples leads to greater impacts on model
performance for exploits records than they do for fuzzers
records, and vice-versa. The results show the AE
demonstrates a high degree of resilience even when
presented with uncharacteristically high levels of poison.

4 Discussion

The sizes of some eﬀects we observed were small,
and their statistical signiﬁcance has not yet been as-
sessed. The eﬀects of increased poisoning should also be
investigated. At small levels of poisoning, it is possible
that random factors may obscure relationships we would
like to observe. In addition, the results we have shown
are derived from single train and test runs. Therefore,
they should be considered to be preliminary because of
a number of sources of variation. These include ran-
dom selection of records included in the training set,
positive pair selection (for word vector training), selec-

Figure 1: The SGNS method iteratively updates the 10-
dimensional vector embeddings v1 and v2 of the words
to optimize BCE loss which forces σ(v1 · v2) to be near
1 for a positive pair and near 0 otherwise. Here, σ is
the sigmoid function. The embeddings corresponding
to the entities of a netﬂow record are concatenated (to
form a 100-dimensional vector), which constitutes the
input data for an AE.

Figure 2: Top: With no poisoning the AE easily
distinguishes benign traﬃc from exploits. Middle: With
poisoning at 0.75%, the AE demonstrates a slight
decrease in performance, mostly in a decreasing TPR
at the optimal F1. Bottom: When poison is doubled
performance degrades further, due to benign records in
the tail over the optimal threshold. Notice the support
of the malicious distribution has shifted to the left as
compared to the other scenarios.

Figure 3: Top: With no poisoning the AE is easily dis-
tinguishes benign traﬃc from fuzzers. Middle: With
poisoning at 0.75% with exploits, performance degrades
but this is explained by the skewness of the fuzzers dis-
tribution, which hardly changes. Bottom: When poison
is doubled performance actually improves further, likely
due to stochasticity of the model. Again the support of
the malicious distribution has shifted to the left as com-
pared to the other scenarios.

tion of negative context words in negative sampling, as
well as randomness involved in word vector (embedding
layer) and autoencoder training. We plan to address
these by sampling in future work.

In order to increase the fraction of exploits records
to 1.5% of the poisoned training set, it was necessary
to reduce the number of benign records in the training
set. This resulted in a sizeable reduction in the number
of destination ports included in the training set. This
could be related to the compressed range of observed
MSE for this set compared to MSE distributions for
unpoisoned and 0.75% poisoned data sets.

The ﬁgures demonstrate that poisoning with a
single attack type can aﬀect model performance on
diﬀerent attack types in diﬀerent ways. This seems
reasonable, since the the joint distribution of features
need not be the same for all attack classes. However,
one should also consider the importance of diversity
that may be present within a single attack class. If an
attack class is heterogeneous, it is possible for poisoned
training to improve performance on the attack class
used for poisoning. This counter-intuitive phenomenon
might occur if a small sample from an attack class
is used to poison a model. Model parameters could
adjust to better ﬁt the instances in that sample, while
In
worsening the ﬁt to the other members of class.
other words, a small sample used to poison the training
data may not adequately represent all sub-populations
present in a single attack class. However, if one wishes
to observe such an eﬀect, sampling employed to dampen
other sources of variability must be carefully designed.

References

[1] Yoshua Bengio, Holger Schwenk,

Jean-S´ebastien
Sen´ecal, Fr´ederic Morin, and Jean-Luc Gauvain. Neu-
ral Probabilistic Language Models, pages 137–186.
Springer Berlin Heidelberg, Berlin, Heidelberg, 2006.

[2] D. Cozzolino and L. Verdoliva. Single-image splicing
localization through autoencoder-based anomaly de-
tection. In 2016 IEEE International Workshop on In-
formation Forensics and Security (WIFS), pages 1–6,
2016.

[3] D. E. Denning. An intrusion-detection model. IEEE
Transactions on Software Engineering, SE-13(2):222–
232, 1987.

[4] Ian Goodfellow, Yoshua Bengio, and Aaron Courville.

Deep Learning. The MIT Press, 2016.

[5] N. Heard and P. Rubin-Delanchy. Network-wide
anomaly detection via the dirichlet process.
In 2016
IEEE Conference on Intelligence and Security Infor-
matics (ISI), pages 220–224, 2016.

[6] Aleksandar Lazarevic, Vipin Kumar, and Jaideep Sri-
vastava. Intrusion Detection: A Survey, pages 19–78.
Springer US, Boston, MA, 2005.

[7] Omer Levy and Yoav Goldberg. Neural word embed-
ding as implicit matrix factorization.
In Z. Ghahra-
mani, M. Welling, C. Cortes, N. Lawrence, and K. Q.
Weinberger, editors, Advances in Neural Information
Processing Systems, volume 27. Curran Associates,
Inc., 2014.

[8] D. Liao, C. Chen, W. Tsai, H. Chen, Y. Wu, and
S. Chang. Anomaly detection for semiconductor tools
using stacked autoencoder learning.
In 2018 Inter-
national Symposium on Semiconductor Manufacturing
(ISSM), pages 1–4, 2018.

[9] Fucheng Liu, Yu Wen, Dongxue Zhang, Xihe Jiang,
Xinyu Xing, and Dan Meng. Log2vec: A hetero-
geneous graph embedding based approach for detect-
ing cyber threats within enterprise. In Proceedings of
the 2019 ACM SIGSAC Conference on Computer and
Communications Security, CCS ’19, page 1777–1794,
New York, NY, USA, 2019. Association for Comput-
ing Machinery.

[10] Teresa F. Lunt. Detecting intruders in computer
systems. In Proceedings, Sixth Annual Symposium and
Technical Displays on Physical and Electronic Security,
July 30 - August 2 1990.

[11] Tomas Mikolov, Kai Chen, Greg Corrado, and Jeﬀrey
Dean. Eﬃcient estimation of word representations in
vector space, 2013.

[12] Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg Cor-
rado, and Jeﬀrey Dean. Distributed representations of
words and phrases and their compositionality, 2013.

[13] Steve Morgan. 2017 Cybercrime Report. Technical
report, Cybersecurity Ventures and Herjavec Group,
01 2017.

[14] N. Moustafa and J. Slay. Unsw-nb15: a comprehensive
data set for network intrusion detection systems (unsw-
nb15 network data set). In 2015 Military Communica-
tions and Information Systems Conference (MilCIS),
pages 1–6, 2015.

[15] N. Moustafa, J. Slay, and G. Creech. Novel geometric
area analysis technique for anomaly detection using
trapezoidal area estimation on large-scale networks.
IEEE Transactions on Big Data, 5(4):481–494, 2019.

[16] Nour Moustafa, Gideon Creech, and Jill Slay. Big Data
Analytics for Intrusion Detection System: Statistical
Decision-Making Using Finite Dirichlet Mixture Mod-
els, pages 127–156. Springer International Publishing,
Cham, 2017.

[17] Nour Moustafa and Jill Slay. The evaluation of network
anomaly detection systems: Statistical analysis of the
unsw-nb15 data set and the comparison with the kdd99
data set.
Information Security Journal: A Global
Perspective, 25(1-3):18–31, 2016.

[18] Q. P. Nguyen, K. W. Lim, D. M. Divakaran, K. H.
Low, and M. C. Chan. Gee: A gradient-based explain-
able variational autoencoder for network anomaly de-
tection. In 2019 IEEE Conference on Communications
and Network Security (CNS), pages 91–99, 2019.
[19] Kasper Ramstr¨om. Botnet detection on ﬂow data using
the reconstruction error from Autoencoders trained on

Word2Vec network embeddings. PhD thesis, Uppsala
Universitet, 2019.

[20] M. Ring, A. Dallmann, D. Landes, and A. Hotho.
Ip2vec: Learning similarities between ip addresses. In
2017 IEEE International Conference on Data Mining
Workshops (ICDMW), pages 657–666, 2017.

[21] The Council of Economic Advisors. The Cost of Mali-
cious Cyber Activity to the U.S. Economy. Technical
report, Executive Oﬃce of The President of the United
States, 02 2018.

[22] M. E. Verma and R. A. Bridges. Deﬁning a metric
space of host logs and operational use cases.
In
2018 IEEE International Conference on Big Data (Big
Data), pages 5068–5077, 2018.

